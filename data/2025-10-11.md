<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 36]
- [cs.CL](#cs.CL) [Total: 12]
- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PickStyle: Video-to-Video Style Transfer with Context-Style Adapters](https://arxiv.org/abs/2510.07546)
*Soroush Mehraban, Vida Adeli, Jacob Rommann, Babak Taati, Kyryl Truskovskyi*

#### 🧩 TL;DR
本文提出了PickStyle视频风格迁移框架，通过引入低秩适配器和上下文-风格分类器无引导技术，在保持视频内容一致性的同时实现高效风格迁移，显著优于现有基线方法。

---

#### 📘 Detailed Summary
**Motivation:** 视频风格迁移面临的主要挑战是缺乏成对的视频数据进行监督训练，现有方法难以在保持视频内容一致性的同时实现有效的风格转换，特别是在动态视频场景中维持时间一致性方面存在明显不足。

**Method:** PickStyle框架在预训练视频扩散模型基础上引入低秩适配器插入自注意力层，利用成对静态图像数据构建合成训练片段，通过共享增强模拟相机运动，并提出上下文-风格分类器无引导技术将分类器无引导分解为独立的文本和视频方向。

**Result:** 在多个基准测试上的实验表明，该方法能够生成时间一致、风格忠实且内容保持的视频转换结果，在定性和定量评估中均优于现有基线方法，实现了更好的风格迁移效果。

**Conclusion:** 该研究证明了通过低秩适配器和精心设计的训练策略，可以有效利用静态图像数据解决视频风格迁移问题，为跨模态风格迁移提供了新的技术路径，同时提出的上下文-风格分解方法为控制生成过程提供了更精细的调节机制。

---

#### 📄 Abstract
We address the task of video style transfer with diffusion models, where the
goal is to preserve the context of an input video while rendering it in a
target style specified by a text prompt. A major challenge is the lack of
paired video data for supervision. We propose PickStyle, a video-to-video style
transfer framework that augments pretrained video diffusion backbones with
style adapters and benefits from paired still image data with source-style
correspondences for training. PickStyle inserts low-rank adapters into the
self-attention layers of conditioning modules, enabling efficient
specialization for motion-style transfer while maintaining strong alignment
between video content and style. To bridge the gap between static image
supervision and dynamic video, we construct synthetic training clips from
paired images by applying shared augmentations that simulate camera motion,
ensuring temporal priors are preserved. In addition, we introduce Context-Style
Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free
guidance into independent text (style) and video (context) directions. CS-CFG
ensures that context is preserved in generated video while the style is
effectively transferred. Experiments across benchmarks show that our approach
achieves temporally coherent, style-faithful, and content-preserving video
translations, outperforming existing baselines both qualitatively and
quantitatively.


### [2] [TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility](https://arxiv.org/abs/2510.07550)
*Saman Motamed, Minghao Chen, Luc Van Gool, Iro Laina*

#### 🧩 TL;DR
该研究提出了TRAVL微调方法和ImplausiBench基准，旨在解决视频生成模型中的物理合理性评估问题，通过改进视频语言模型的时空推理能力来检测违反物理定律的视频内容。

---

#### 📘 Detailed Summary
**Motivation:** 现代视频生成模型虽然视觉保真度高，但经常产生违反物理定律的序列，如物体漂浮、瞬移或违反因果关系的变形，目前缺乏定量评估视频物理真实性的稳健方法，而现有视频语言模型在识别物理违规方面存在困难，暴露了其时序和因果推理的根本局限性。

**Method:** 研究引入了TRAVL微调方法，结合平衡训练数据集和轨迹感知注意力模块来改进视频语言模型中的运动编码和判别能力，同时提出了ImplausiBench基准，包含300个视频（150个真实、150个生成），旨在消除语言偏见并隔离视觉时序理解。

**Result:** 性能评估同时采用黄金标准的人类判断和更严格的LLM作为评判指标，TRAVL方法显著提升了视频语言模型对物理违规的检测能力，ImplausiBench基准为物理合理性评估提供了统一的测试框架。

**Conclusion:** TRAVL和ImplausiBench共同构成了一个统一框架，用于探索和改进多模态模型中的物理合理性，揭示了视觉时序理解中具有挑战性且未被充分探索的方面，为视频生成模型的物理真实性评估提供了新的方法和基准。

---

#### 📄 Abstract
Despite impressive visual fidelity, modern video generative models frequently
produce sequences that violate intuitive physical laws, such as objects
floating, teleporting, or morphing in ways that defy causality. While humans
can easily detect such implausibilities, there remains no robust method for
quantitatively assessing physical realism in video. In this work, we explore
whether Video-Language Models (VLMs) can be trained to serve as reliable judges
of physical plausibility. We find that existing VLMs struggle to identify
physics violations, exposing fundamental limitations in their temporal and
causal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe
that combines a balanced training dataset with a trajectory-aware attention
module to improve motion encoding and discrimination in VLMs. To evaluate
physical reasoning more rigorously, we propose ImplausiBench, a benchmark of
300 videos (150 real, 150 generated) that removes linguistic biases and
isolates visual-temporal understanding. Performance is reported both with
gold-standard human judgments and stricter LLM-as-judge metrics. Together,
TRAVL and ImplausiBench offer a unified framework for probing and improving
physical plausibility in multimodal models, shedding light on a challenging and
underexplored aspect of visual-temporal understanding.


### [3] [Label Semantics for Robust Hyperspectral Image Classification](https://arxiv.org/abs/2510.07556)
*Rafin Hassan, Zarin Tasnim Roshni, Rafiqul Bari, Alimul Islam, Nabeel Mohammed, Moshiur Farazi, Shafin Rahman*

#### 🧩 TL;DR
本文提出了一种通用的语义光谱-空间融合网络（S3FN），通过利用大语言模型生成的类别特定文本描述来增强高光谱图像分类性能，解决了传统单模态模型在有限训练样本下的过拟合问题。

---

#### 📘 Detailed Summary
**Motivation:** 高光谱图像分类面临高质量训练样本稀缺和光谱数据高维度的挑战，导致模型容易过拟合且难以平衡精度与计算复杂度。传统单模态模型仅依赖光谱-空间数据学习决策边界，存在性能瓶颈。

**Method:** S3FN利用LLMs为每个类别标签生成全面的文本描述，捕捉其独特特征和光谱行为，然后使用预训练文本编码器（如BERT或RoBERTa）将这些描述嵌入向量空间，提取有意义的标签语义以实现更好的特征-标签对齐。

**Result:** 在Hyperspectral Wood、HyperspectralBlueberries和DeepHS-Fruit三个基准数据集上的评估表明，该方法显著提升了分类性能，证明了文本语义与光谱-空间数据之间的协同效应。

**Conclusion:** 研究揭示了文本语义与光谱-空间数据融合的潜力，为语义增强的高光谱图像分类模型开辟了新方向，展示了多模态方法在解决数据稀缺问题上的优势。

---

#### 📄 Abstract
Hyperspectral imaging (HSI) classification is a critical tool with widespread
applications across diverse fields such as agriculture, environmental
monitoring, medicine, and materials science. Due to the limited availability of
high-quality training samples and the high dimensionality of spectral data, HSI
classification models are prone to overfitting and often face challenges in
balancing accuracy and computational complexity. Furthermore, most of HSI
classification models are monomodal, where it solely relies on spectral-spatial
data to learn decision boundaries in the high dimensional embedding space. To
address this, we propose a general-purpose Semantic Spectral-Spatial Fusion
Network (S3FN) that uses contextual, class specific textual descriptions to
complement the training of an HSI classification model. Specifically, S3FN
leverages LLMs to generate comprehensive textual descriptions for each class
label that captures their unique characteristics and spectral behaviors. These
descriptions are then embedded into a vector space using a pre-trained text
encoder such as BERT or RoBERTa to extract meaningful label semantics which in
turn leads to a better feature-label alignment for improved classification
performance. To demonstrate the effectiveness of our approach, we evaluate our
model on three diverse HSI benchmark datasets - Hyperspectral Wood,
HyperspectralBlueberries, and DeepHS-Fruit and report significant performance
boost. Our results highlight the synergy between textual semantics and
spectral-spatial data, paving the way for further advancements in semantically
augmented HSI classification models. Codes are be available in:
https://github.com/milab-nsu/S3FN


### [4] [Cross-Modal Attention Guided Unlearning in Vision-Language Models](https://arxiv.org/abs/2510.07567)
*Karuna Bhaila, Aneesh Komanduri, Minh-Hao Van, Xintao Wu*

#### 🧩 TL;DR
本文提出了一种轻量级的视觉语言模型遗忘框架CAGUL，通过跨模态注意力引导来防止敏感信息泄露，无需修改预训练模型参数即可实现高效的遗忘效果。

---

#### 📘 Detailed Summary
**Motivation:** 视觉语言模型在训练过程中可能记忆并泄露私有或敏感信息，而现有的遗忘方法主要针对纯文本模型，未能有效处理视觉上下文同样包含敏感信息的复杂场景。

**Method:** 提出跨模态注意力引导遗忘框架CAGUL，利用跨模态注意力分析视觉token对输出的贡献度，通过外部模块对低重要性视觉token进行编码转换，实现敏感信息遗忘。

**Result:** 实验结果表明，CAGUL方法在防止信息泄露方面优于或与基于微调的基线方法相当，同时保持了参考模型的行为特性，且无需重新训练成本。

**Conclusion:** 该研究证明了基于跨模态注意力的轻量级遗忘框架在视觉语言模型中的有效性，为多模态模型隐私保护提供了一种实用且高效的解决方案，避免了传统微调方法的高计算开销。

---

#### 📄 Abstract
Vision-Language Models (VLMs) have demonstrated immense capabilities in
multi-modal understanding and inference tasks such as Visual Question Answering
(VQA), which requires models to infer outputs based on visual and textual
context simultaneously. Such inference abilities of large-scale pretrained
models are often attributed to the massive scale of pre-training data collected
across several domains. However, the models may memorize private and/or
sensitive information during training and regurgitate it in inference.
Recently, machine unlearning has been leveraged to address the leakage of
private data in LLMs. VLMs add a layer of complexity to this process, as the
visual context in the query may also contain sensitive information in addition
to the text. To address this issue, we explore unlearning for vision-language
models, specifically for the VQA task. We explore the role of visual tokens for
output generation in VLMs using cross-modal attention and utilize it to
formulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and
efficient VLM unlearning framework. In contrast to computationally expensive
model finetuning methods, CAGUL utilizes external modules to encode unlearning
information in visual tokens of low importance for relevant queries. We find
that the transformed visual tokens not only prevent leakage but also retain
reference model behavior. Experimental results show that our method performs
better or on par with finetuning-based baselines without altering the
pre-trained model parameters or incurring retraining costs, making it a
practical and effective unlearning solution for VLMs.


### [5] [Rectified-CFG++ for Flow Based Models](https://arxiv.org/abs/2510.07631)
*Shreshth Saini, Shashank Gupta, Alan C. Bovik*

#### 🧩 TL;DR
本文提出了Rectified-CFG++，一种用于校正流模型的适应性预测-校正引导方法，解决了标准分类器无关引导在整流流模型中引起的离流形漂移问题，显著提升了文本到图像生成的质量和稳定性。

---

#### 📘 Detailed Summary
**Motivation:** 标准分类器无关引导在整流流模型中的原生应用会引发严重的离流形漂移，导致视觉伪影、文本对齐失败和脆弱行为，这限制了整流流模型在文本条件生成任务中的有效应用。

**Method:** 提出自适应预测-校正引导方法，每个推理步骤首先执行条件整流流更新将样本锚定在学习到的传输路径附近，然后应用加权条件校正，在条件和无条件速度场之间进行插值，确保轨迹保持在数据流形的有界管状邻域内。

**Result:** 在大规模文本到图像模型上的广泛实验表明，Rectified-CFG++在MS-COCO、LAION-Aesthetic和T2I-CompBench等基准数据集上持续优于标准CFG方法，证明了其在各种引导强度下的稳定性和有效性。

**Conclusion:** 该方法将整流流的确定性效率与几何感知的条件规则相结合，证明了所得速度场的边际一致性，为整流流模型提供了稳定可靠的文本条件引导机制，推动了扩散模型在可控生成任务中的应用。

---

#### 📄 Abstract
Classifier-free guidance (CFG) is the workhorse for steering large diffusion
models toward text-conditioned targets, yet its native application to rectified
flow (RF) based models provokes severe off-manifold drift, yielding visual
artifacts, text misalignment, and brittle behaviour. We present
Rectified-CFG++, an adaptive predictor-corrector guidance that couples the
deterministic efficiency of rectified flows with a geometry-aware conditioning
rule. Each inference step first executes a conditional RF update that anchors
the sample near the learned transport path, then applies a weighted conditional
correction that interpolates between conditional and unconditional velocity
fields. We prove that the resulting velocity field is marginally consistent and
that its trajectories remain within a bounded tubular neighbourhood of the data
manifold, ensuring stability across a wide range of guidance strengths.
Extensive experiments on large-scale text-to-image models (Flux, Stable
Diffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms
standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and
T2I-CompBench. Project page: https://rectified-cfgpp.github.io/


### [6] [PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment](https://arxiv.org/abs/2510.07636)
*Shashank Gupta, Gregoire Phillips, Alan C. Bovik*

#### 🧩 TL;DR
本文提出了PIT-QMM，一种新颖的大型多模态模型，用于无参考点云质量评估，能够端到端处理文本、图像和点云数据，在多个基准测试中显著优于现有最先进方法。

---

#### 📘 Detailed Summary
**Motivation:** 当前大型多模态模型在图像和视频质量评估领域取得了显著进展，但在3D资产质量评估领域尚未得到充分探索。本研究旨在解决无参考点云质量评估的挑战，即在没有参考点云的情况下自动评估点云的感知质量。

**Method:** 研究观察到文本描述、2D投影和3D点云视图等不同模态数据提供了关于点云质量的互补信息。基于此，构建了PIT-QMM这一新颖的大型多模态模型，能够端到端处理文本、图像和点云数据来预测质量分数。

**Result:** 广泛的实验表明，所提出的方法在流行的基准测试中以显著优势超越了现有最先进方法，且需要更少的训练迭代次数。此外，该框架还实现了失真定位和识别功能。

**Conclusion:** 该研究为模型可解释性和交互性开辟了新的途径，证明了多模态融合在3D质量评估中的有效性，并为点云质量评估提供了新的技术方向。

---

#### 📄 Abstract
Large Multimodal Models (LMMs) have recently enabled considerable advances in
the realm of image and video quality assessment, but this progress has yet to
be fully explored in the domain of 3D assets. We are interested in using these
models to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where
the aim is to automatically evaluate the perceptual quality of a point cloud in
absence of a reference. We begin with the observation that different modalities
of data - text descriptions, 2D projections, and 3D point cloud views - provide
complementary information about point cloud quality. We then construct PIT-QMM,
a novel LMM for NR-PCQA that is capable of consuming text, images and point
clouds end-to-end to predict quality scores. Extensive experimentation shows
that our proposed method outperforms the state-of-the-art by significant
margins on popular benchmarks with fewer training iterations. We also
demonstrate that our framework enables distortion localization and
identification, which paves a new way forward for model explainability and
interactivity. Code and datasets are available at
https://www.github.com/shngt/pit-qmm.


### [7] [Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision](https://arxiv.org/abs/2510.07703)
*Xiaoxu Ma, Runhao Li, Zhenyu Weng*

#### 🧩 TL;DR
本文提出了一种新颖的弱到强哈希学习框架MLH，通过互学习机制将基于配对的弱分支的局部相似性知识迁移到基于中心的强分支中，从而在保持全局结构建模优势的同时有效利用局部相似性信息。

---

#### 📘 Detailed Summary
**Motivation:** 基于中心的哈希方法在建模全局数据结构方面表现出色，但往往未能充分利用重要的局部相似性信息，而基于配对的方法虽然能有效保持局部相似关系，但在全局分布捕获方面存在不足。

**Method:** 提出互学习哈希框架MLH，包含基于中心的强分支和基于配对的弱分支，通过迭代互学习过程实现知识迁移，并引入混合哈希专家模块促进分支间有效交互。

**Result:** 在多个基准数据集上的广泛实验表明，MLH始终优于最先进的哈希方法，验证了所提框架在结合全局和局部信息方面的有效性。

**Conclusion:** 该研究证明了通过弱到强互学习框架可以有效整合全局结构和局部相似性信息，为哈希学习提供了新的思路，混合哈希专家模块的设计也为多分支协同优化提供了参考。

---

#### 📄 Abstract
Deep hashing has been widely adopted for large-scale image retrieval, with
numerous strategies proposed to optimize hash function learning. Pairwise-based
methods are effective in learning hash functions that preserve local similarity
relationships, whereas center-based methods typically achieve superior
performance by more effectively capturing global data distributions. However,
the strength of center-based methods in modeling global structures often comes
at the expense of underutilizing important local similarity information. To
address this limitation, we propose Mutual Learning for Hashing (MLH), a novel
weak-to-strong framework that enhances a center-based hashing branch by
transferring knowledge from a weaker pairwise-based branch. MLH consists of two
branches: a strong center-based branch and a weaker pairwise-based branch.
Through an iterative mutual learning process, the center-based branch leverages
local similarity cues learned by the pairwise-based branch. Furthermore,
inspired by the mixture-of-experts paradigm, we introduce a novel
mixture-of-hash-experts module that enables effective cross-branch interaction,
further enhancing the performance of both branches. Extensive experiments
demonstrate that MLH consistently outperforms state-of-the-art hashing methods
across multiple benchmark datasets.


### [8] [GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.07791)
*Qinghongbing Xie, Zhaoyuan Xia, Feng Zhu, Lijun Gong, Ziyue Li, Rui Zhao, Long Zeng*

#### 🧩 TL;DR
本文提出了GTR-Bench基准测试，用于评估视觉语言模型在大型摄像头网络中移动目标的地理时空推理能力，揭示了当前模型在地理时空推理方面的三个主要缺陷。

---

#### 📘 Detailed Summary
**Motivation:** 现有时空基准主要关注以图像/视频为背景的自我中心视角推理或以图形为背景的地理视角推理，但缺乏同时利用图像/视频和图形上下文评估视觉语言模型地理时空推理能力的基准，而这对交通管理和应急响应等领域至关重要。

**Method:** 作者引入了地理时空推理基准GTR-Bench，这是一个在大规模摄像头网络中对移动目标进行地理时空推理的新挑战，要求模型在地图和视频之间进行多视角切换，跨多个非重叠视野视频进行联合推理，并对任何视频上下文未观察到的时空区域进行推断。

**Result:** 对10多个流行视觉语言模型的评估显示，即使最佳专有模型Gemini-2.5-Pro（34.9%）在地理时空推理方面也显著落后于人类表现（78.61%），分析揭示了当前模型在地理时空推理中的三个主要缺陷：时空上下文利用不平衡、时间预测能力弱、地图与多视角视频输入理解对齐能力不足。

**Conclusion:** GTR-Bench为时空智能研究提供了宝贵见解和新的机会，揭示了当前视觉语言模型在地理时空推理方面的关键局限性，特别是在时空上下文平衡利用、时间预测能力以及地图与视频数据融合理解方面的不足。

---

#### 📄 Abstract
Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has
attracted much attention due to its importance for Autonomous Driving, Embodied
AI and General Artificial Intelligence. Existing spatial-temporal benchmarks
mainly focus on egocentric perspective reasoning with images/video context, or
geographic perspective reasoning with graphics context (eg. a map), thus fail
to assess VLMs' geographic spatial-temporal intelligence with both images/video
and graphics context, which is important for areas like traffic management and
emergency response. To address the gaps, we introduce Geo-Temporal Reasoning
benchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of
moving targets in a large-scale camera network. GTR-Bench is more challenging
as it requires multiple perspective switches between maps and videos, joint
reasoning across multiple videos with non-overlapping fields of view, and
inference over spatial-temporal regions that are unobserved by any video
context. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that
even the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags
behind human performance (78.61%) on geo-temporal reasoning. Moreover, our
comprehensive analysis on GTR-Bench reveals three primary deficiencies of
current models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by
an imbalanced utilization of spatial-temporal context. (2) VLMs are weak in
temporal forecasting, which leads to worse performance on temporal-emphasized
tasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to
comprehend or align the map data with multi-view video inputs. We believe
GTR-Bench offers valuable insights and opens up new opportunities for research
and applications in spatial-temporal intelligence. Benchmark and code will be
released at https://github.com/X-Luffy/GTR-Bench.


### [9] [XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method](https://arxiv.org/abs/2510.07856)
*Haochen Yu, Qiankun Liu, Hongyuan Liu, Jianfei Jiang, Juntao Lyu, Jiansheng Chen, Huimin Ma*

#### 🧩 TL;DR
本文提出XYZCylinder，一种基于统一圆柱体提升方法的前馈模型，通过统一相机建模和混合表示提升驾驶场景重建的泛化能力和准确性，在多种评估设置下达到最先进性能。

---

#### 📘 Detailed Summary
**Motivation:** 当前前馈重建范式在驾驶场景重建中存在两个主要问题：固定视角变换在相机配置变化时失效，限制了不同驾驶场景间的泛化能力；稀疏视图间重叠区域小且驾驶场景复杂，增加了学习难度，降低了重建精度。

**Method:** 提出统一圆柱体相机建模策略避免学习视角依赖的空间对应关系，通过可调参数统一不同相机配置；设计基于圆柱平面特征组的混合表示，通过专用模块将2D图像特征提升到3D空间。

**Result:** 实验结果显示XYZCylinder在不同评估设置下均达到最先进性能，并且能够以零样本方式泛化到其他驾驶场景。

**Conclusion:** 该研究证明了统一相机建模和混合表示在提升驾驶场景重建性能方面的有效性，为跨配置场景重建提供了新的解决方案，具有重要的实际应用价值。

---

#### 📄 Abstract
Recently, more attention has been paid to feedforward reconstruction
paradigms, which mainly learn a fixed view transformation implicitly and
reconstruct the scene with a single representation. However, their
generalization capability and reconstruction accuracy are still limited while
reconstructing driving scenes, which results from two aspects: (1) The fixed
view transformation fails when the camera configuration changes, limiting the
generalization capability across different driving scenes equipped with
different camera configurations. (2) The small overlapping regions between
sparse views of the $360^\circ$ panorama and the complexity of driving scenes
increase the learning difficulty, reducing the reconstruction accuracy. To
handle these difficulties, we propose \textbf{XYZCylinder}, a feedforward model
based on a unified cylinder lifting method which involves camera modeling and
feature lifting. Specifically, to improve the generalization capability, we
design a Unified Cylinder Camera Modeling (UCCM) strategy, which avoids the
learning of viewpoint-dependent spatial correspondence and unifies different
camera configurations with adjustable parameters. To improve the reconstruction
accuracy, we propose a hybrid representation with several dedicated modules
based on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D image
features to 3D space. Experimental results show that XYZCylinder achieves
state-of-the-art performance under different evaluation settings, and can be
generalized to other driving scenes in a zero-shot manner. Project page:
\href{https://yuyuyu223.github.io/XYZCYlinder-projectpage/}{here}.


### [10] [MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding](https://arxiv.org/abs/2510.07915)
*Peiran Wu, Zhuorui Yu, Yunze Liu, Chi-Hao Wu, Enmin Zhou, Junxiao Shen*

#### 🧩 TL;DR
本文提出MARC方法，通过记忆增强强化学习实现视觉令牌压缩，在视频理解任务中仅使用单帧令牌即可达到接近基准的准确率，显著降低计算开销。

---

#### 📘 Detailed Summary
**Motivation:** 视觉语言模型从图像扩展到视频时面临高帧率和长持续时间带来的沉重计算成本问题，现有免训练令牌压缩方法导致信息丢失和性能下降，需要更有效的压缩方案。

**Method:** 提出记忆增强强化学习令牌压缩方法，采用检索-压缩策略，使用视觉记忆检索器选择关键片段，并通过压缩组相对策略优化框架将教师模型的推理能力蒸馏到学生模型。

**Result:** 在六个视频基准测试中，MARC仅使用单帧令牌即可达到接近基准准确率，视觉令牌减少95%，GPU内存降低72%，延迟减少23.9%。

**Conclusion:** 该方法展示了在资源受限环境下实现高效实时视频理解的潜力，适用于视频问答、监控和自动驾驶等应用场景，为大规模视频处理提供了可行的解决方案。

---

#### 📄 Abstract
The rapid progress of large language models (LLMs) has laid the foundation
for multimodal models. However, visual language models (VLMs) still face heavy
computational costs when extended from images to videos due to high frame rates
and long durations. Token compression is a promising solution, yet most
existing training-free methods cause information loss and performance
degradation. To overcome this, we propose \textbf{Memory-Augmented
Reinforcement Learning-based Token Compression (MARC)}, which integrates
structured retrieval and RL-based distillation. MARC adopts a
\textit{retrieve-then-compress} strategy using a \textbf{Visual Memory
Retriever (VMR)} to select key clips and a \textbf{Compression Group Relative
Policy Optimization (C-GRPO)} framework to distil reasoning ability from a
teacher to a student model. Experiments on six video benchmarks show that MARC
achieves near-baseline accuracy using only one frame's tokens -- reducing
visual tokens by \textbf{95\%}, GPU memory by \textbf{72\%}, and latency by
\textbf{23.9\%}. This demonstrates its potential for efficient, real-time video
understanding in resource-constrained settings such as video QA, surveillance,
and autonomous driving.


### [11] [TTOM: Test-Time Optimization and Memorization for Compositional Video Generation](https://arxiv.org/abs/2510.07940)
*Leigang Qu, Ziyang Wang, Na Zheng, Wenjie Wang, Liqiang Nie, Tat-Seng Chua*

#### 🧩 TL;DR
本文提出了TTOM框架，一种无需训练的方法，通过在推理时优化参数和对齐时空布局来提升视频基础模型在组合场景下的文本-图像对齐能力，显著改善了运动、数量关系和空间关系等组合性任务的生成质量。

---

#### 📘 Detailed Summary
**Motivation:** 视频基础模型在视觉生成方面表现出色，但在组合性场景（如运动、数量关系和空间关系）中表现不佳，存在文本-图像对齐不足的问题，需要一种能够动态改善跨模态对齐的解决方案。

**Method:** 提出了测试时优化与记忆化框架，通过优化新参数并采用通用布局-注意力目标进行引导，同时引入参数化记忆机制来维护历史优化上下文，支持插入、读取、更新和删除等灵活操作，将视频生成建模为流式处理任务。

**Result:** 在T2V-CompBench和Vbench基准测试上的实验结果表明，TTOM框架能够有效解耦组合性世界知识，展现出强大的可迁移性和泛化能力，成为实现组合性视频生成中跨模态对齐的有效、实用、可扩展且高效的解决方案。

**Conclusion:** 该研究证明了测试时优化结合记忆机制能够显著提升视频基础模型的组合性生成能力，为动态改善跨模态对齐提供了新的技术路径，具有重要的实际应用价值和推广潜力。

---

#### 📄 Abstract
Video Foundation Models (VFMs) exhibit remarkable visual generation
performance, but struggle in compositional scenarios (e.g., motion, numeracy,
and spatial relation). In this work, we introduce Test-Time Optimization and
Memorization (TTOM), a training-free framework that aligns VFM outputs with
spatiotemporal layouts during inference for better text-image alignment. Rather
than direct intervention to latents or attention per-sample in existing work,
we integrate and optimize new parameters guided by a general layout-attention
objective. Furthermore, we formulate video generation within a streaming
setting, and maintain historical optimization contexts with a parametric memory
mechanism that supports flexible operations, such as insert, read, update, and
delete. Notably, we found that TTOM disentangles compositional world knowledge,
showing powerful transferability and generalization. Experimental results on
the T2V-CompBench and Vbench benchmarks establish TTOM as an effective,
practical, scalable, and efficient framework to achieve cross-modal alignment
for compositional video generation on the fly.


### [12] [CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.08003)
*Weihuang Lin, Yiwei Ma, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji*

#### 🧩 TL;DR
本文提出了CIR-CoT，这是首个集成显式思维链推理的端到端检索导向多模态大语言模型，通过生成可解释的推理链来增强跨模态交互理解，在提升检索精度的同时实现决策过程透明化。

---

#### 📘 Detailed Summary
**Motivation:** 当前基于视觉语言模型和多模态大语言模型的组合图像检索方法主要作为'黑箱'运行，这种不透明性不仅阻碍用户理解检索原理，还限制了模型遵循复杂细粒度指令的能力，因此需要开发能够提供可解释推理的检索系统。

**Method:** CIR-CoT通过强制模型首先生成可解释的推理链来增强关键跨模态交互的捕捉能力，采用包含描述、推理和结论的三阶段过程创建结构化思维链标注，并对模型进行微调以产生这种结构化输出，最终将其检索意图编码到专用嵌入中。

**Result:** 综合实验表明，CIR-CoT在领域内数据集（FashionIQ、CIRR）上实现了极具竞争力的性能，并在领域外CIRCO数据集上展现出卓越的泛化能力，为更有效和可信的检索系统开辟了新路径。

**Conclusion:** 该研究确立了通过显式思维链推理实现可解释组合图像检索的新范式，证明了推理透明度与检索性能可以协同提升，为构建更可信的多模态检索系统提供了重要方法论基础。

---

#### 📄 Abstract
Composed Image Retrieval (CIR), which aims to find a target image from a
reference image and a modification text, presents the core challenge of
performing unified reasoning across visual and semantic modalities. While
current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more
recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown
progress, they predominantly function as ``black boxes." This inherent opacity
not only prevents users from understanding the retrieval rationale but also
restricts the models' ability to follow complex, fine-grained instructions. To
overcome these limitations, we introduce CIR-CoT, the first end-to-end
retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT)
reasoning. By compelling the model to first generate an interpretable reasoning
chain, CIR-CoT enhances its ability to capture crucial cross-modal
interactions, leading to more accurate retrieval while making its decision
process transparent. Since existing datasets like FashionIQ and CIRR lack the
necessary reasoning data, a key contribution of our work is the creation of
structured CoT annotations using a three-stage process involving a caption,
reasoning, and conclusion. Our model is then fine-tuned to produce this
structured output before encoding its final retrieval intent into a dedicated
embedding. Comprehensive experiments show that CIR-CoT achieves highly
competitive performance on in-domain datasets (FashionIQ, CIRR) and
demonstrates remarkable generalization on the out-of-domain CIRCO dataset,
establishing a new path toward more effective and trustworthy retrieval
systems.


### [13] [DarkHash: A Data-Free Backdoor Attack Against Deep Hashing](https://arxiv.org/abs/2510.08094)
*Ziqi Zhou, Menghao Deng, Yufei Song, Hangtao Zhang, Wei Wan, Shengshan Hu, Minghui Li, Leo Yu Zhang, Dezhong Yao*

#### 🧩 TL;DR
本文提出了DarkHash，这是首个针对深度哈希的无数据后门攻击方法，通过设计具有双重语义引导的影子后门攻击框架，仅使用替代数据集微调受害者模型的特定层，实现了在不访问训练数据的情况下植入后门并保持原始检索精度。

---

#### 📘 Detailed Summary
**Motivation:** 现有深度哈希后门攻击方法需要访问训练数据集来植入后门，但在现实世界中由于隐私保护和知识产权考虑，获取此类数据往往被禁止，因此开发无需训练数据访问的后门攻击方法成为一个新颖且具有挑战性的问题。

**Method:** 提出了具有双重语义引导的影子后门攻击框架，通过仅使用替代数据集微调受害者模型的特定层来嵌入后门功能并保持原始检索精度；设计了拓扑对齐损失，利用样本与其邻居之间的关系，优化个体和相邻中毒样本朝向目标样本，进一步增强攻击能力。

**Result:** 在四个图像数据集、五种模型架构和两种哈希方法上的实验结果表明，DarkHash具有高有效性，优于现有的最先进后门攻击方法；防御实验显示DarkHash能够抵御现有的主流后门防御方法。

**Conclusion:** DarkHash展示了在无需访问原始训练数据的情况下对深度哈希模型进行有效后门攻击的可行性，揭示了深度哈希模型在现实场景中的安全脆弱性，为未来防御方法的发展提供了重要参考。

---

#### 📄 Abstract
Benefiting from its superior feature learning capabilities and efficiency,
deep hashing has achieved remarkable success in large-scale image retrieval.
Recent studies have demonstrated the vulnerability of deep hashing models to
backdoor attacks. Although these studies have shown promising attack results,
they rely on access to the training dataset to implant the backdoor. In the
real world, obtaining such data (e.g., identity information) is often
prohibited due to privacy protection and intellectual property concerns.
Embedding backdoors into deep hashing models without access to the training
data, while maintaining retrieval accuracy for the original task, presents a
novel and challenging problem. In this paper, we propose DarkHash, the first
data-free backdoor attack against deep hashing. Specifically, we design a novel
shadow backdoor attack framework with dual-semantic guidance. It embeds
backdoor functionality and maintains original retrieval accuracy by fine-tuning
only specific layers of the victim model using a surrogate dataset. We consider
leveraging the relationship between individual samples and their neighbors to
enhance backdoor attacks during training. By designing a topological alignment
loss, we optimize both individual and neighboring poisoned samples toward the
target sample, further enhancing the attack capability. Experimental results on
four image datasets, five model architectures, and two hashing methods
demonstrate the high effectiveness of DarkHash, outperforming existing
state-of-the-art backdoor attack methods. Defense experiments show that
DarkHash can withstand existing mainstream backdoor defense methods.


### [14] [Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement](https://arxiv.org/abs/2510.08138)
*Chengzhi Li, Heyan Huang, Ping Jian, Zhen Yang, Yaning Tian*

#### 🧩 TL;DR
本文提出了一种称为时间条件注意力锐化（TCAS）的方法，通过增强跨模态注意力头的时间分辨能力来解决视频语言模型中存在的响应逻辑不一致问题，显著提升了模型的时间理解逻辑一致性。

---

#### 📘 Detailed Summary
**Motivation:** 大型语言模型经常产生自相矛盾的输出，严重影响其可靠性并阻碍实际应用部署，在视频语言模型中这一现象尤为突出，具体表现为模型无法基于其基础输出来为重新表述的问题提供逻辑一致的响应，但其根本原因尚未得到充分探索。

**Method:** 本文采用可解释性驱动的方法来分析、统计总结并干预该现象的潜在因素，提出了时间条件注意力锐化（TCAS）方法，该方法基于注意力区分度构建增强目标，通过增强模型的时间分辨能力来改善其时间理解逻辑一致性。

**Result:** 实验结果表明，该方法显著提升了视频语言模型的时间逻辑一致性，进一步的可解释性分析显示该方法确实改善了注意力头的时间区分能力，验证了研究结论，同时在通用视频时间定位任务中实现了性能提升。

**Conclusion:** 研究表明时间逻辑一致性是时间理解的关键瓶颈，通过增强一致性可以推动视频时间理解的显著进展，注意力头的时间分辨能力不足是导致响应不一致的主要原因，TCAS方法为解决这一问题提供了有效途径。

---

#### 📄 Abstract
Large language models (LLMs) often generate self-contradictory outputs, which
severely impacts their reliability and hinders their adoption in practical
applications. In video-language models (Video-LLMs), this phenomenon recently
draws the attention of researchers. Specifically, these models fail to provide
logically consistent responses to rephrased questions based on their grounding
outputs. However, the underlying causes of this phenomenon remain
underexplored. In this work, we adopt an interpretability-driven approach to
analyze, statistically summarize, and intervention the potential factors of the
phenomenon. We find that one of the primary reasons for the inconsistency in
responses lies in the inability of cross-modal attention heads to effectively
distinguish video tokens across different timestamps. To address this, we
propose an attention enhancement method called Temporally Conditioned Attention
Sharpening (TCAS), which constructs an enhancement objective based on attention
distinctions to enhance the model's temporal resolution capability, thereby
improving its temporal understanding logic consistency. Experimental results
demonstrate that our method significantly enhances the temporal logic
consistency of Video-LLMs. Further interpretability analyses reveal that our
method indeed improves the temporal discriminability of attention heads,
validating our conclusions. Additionally, our method achieves performance
improvements in general video temporal grounding tasks, highlighting that
temporal logic consistency is a bottleneck in temporal understanding. By
enhancing consistency, our method drives significant progress in video temporal
understanding.


### [15] [UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution](https://arxiv.org/abs/2510.08143)
*Shian Du, Menghan Xia, Chang Liu, Quande Liu, Xintao Wang, Pengfei Wan, Xiangyang Ji*

#### 🧩 TL;DR
本文提出了UniMMVSR，这是首个统一的多模态生成式视频超分辨率框架，能够整合文本、图像和视频等多种生成条件，显著提升了视频生成的质量和条件一致性。

---

#### 📘 Detailed Summary
**Motivation:** 现有级联视频超分辨率方法主要局限于文本到视频任务，未能充分利用文本之外的多种生成条件，而这些条件对于确保多模态视频生成的保真度至关重要。

**Method:** 提出了UniMMVSR统一框架，在潜在视频扩散模型中系统探索了条件注入策略、训练方案和数据混合技术，并设计了针对不同条件类型的数据构建和条件利用方法。

**Result:** 实验表明UniMMVSR显著优于现有方法，生成的视频具有更丰富的细节和更高的多模态条件一致性，并验证了与基础模型结合实现4K视频多模态引导生成的可行性。

**Conclusion:** 该研究证明了整合多模态条件在视频超分辨率中的重要性，为高分辨率视频生成开辟了新途径，突破了现有技术在4K视频生成方面的限制。

---

#### 📄 Abstract
Cascaded video super-resolution has emerged as a promising technique for
decoupling the computational burden associated with generating high-resolution
videos using large foundation models. Existing studies, however, are largely
confined to text-to-video tasks and fail to leverage additional generative
conditions beyond text, which are crucial for ensuring fidelity in multi-modal
video generation. We address this limitation by presenting UniMMVSR, the first
unified generative video super-resolution framework to incorporate hybrid-modal
conditions, including text, images, and videos. We conduct a comprehensive
exploration of condition injection strategies, training schemes, and data
mixture techniques within a latent video diffusion model. A key challenge was
designing distinct data construction and condition utilization methods to
enable the model to precisely utilize all condition types, given their varied
correlations with the target video. Our experiments demonstrate that UniMMVSR
significantly outperforms existing methods, producing videos with superior
detail and a higher degree of conformity to multi-modal conditions. We also
validate the feasibility of combining UniMMVSR with a base model to achieve
multi-modal guided generation of 4K video, a feat previously unattainable with
existing techniques.


### [16] [Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing](https://arxiv.org/abs/2510.08157)
*Zhentao Zou, Zhengrong Yue, Kunpeng Du, Binlei Bao, Hanting Li, Haizhen Xie, Guozheng Xu, Yue Zhou, Yali Wang, Jie Hu, Xue Jiang, Xinghao Chen*

#### 🧩 TL;DR
本文提出了MURE框架，通过多模态链式思维将图像编辑从纯文本推理转变为文本与视觉交替的推理过程，结合深度置信度推理机制解决复杂对象交互和空间关系问题，显著提升了图像编辑的精确度和保真度。

---

#### 📘 Detailed Summary
**Motivation:** 现有图像编辑方法在处理复杂对象交叉和细粒度空间关系时存在困难，主要原因是缺乏显式推理过程，而纯文本链式思维或坐标增强的链式思维在表示复杂视觉布局和引导像素级细节生成方面存在根本性限制。

**Method:** 提出MURE框架，采用原生多模态交替文本-图像链式思维，生成包含文本描述和对应视觉线索的逐步推理链；引入多模态深度置信度推理范式，通过奖励模型的深度置信度分数修剪低质量分支，确保模型沿高质量轨迹生成最终编辑结果。

**Result:** 该方法在三个图像编辑基准测试中取得了显著改进，通过将复杂编辑任务分解为相互依赖的子任务，在每个阶段实现更高精度，产生高保真度的编辑结果，并发布了首个包含14K高质量编辑示例的CoT-Edit-14K数据集。

**Conclusion:** 研究证明了多模态交替推理在复杂图像编辑任务中的有效性，为视觉内容生成提供了新的推理范式，通过显式分解和置信度引导机制解决了大语言模型幻觉问题，为细粒度视觉编辑开辟了新方向。

---

#### 📄 Abstract
Image editing with natural language has gained significant popularity, yet
existing methods struggle with intricate object intersections and fine-grained
spatial relationships due to the lack of an explicit reasoning process. While
Chain-of-Thought (CoT) has been explored to enhance reasoning, purely textual
CoT or CoT augmented with coordinate information is fundamentally limited in
its ability to represent intricate visual layouts and lacks the necessary
visual cues to guide the generation of fine-grained, pixel-level details. To
address these challenges, we propose Multimodal Reasoning Edit (MURE), a novel
framework that shifts the visual editing process from purely text-based
reasoning to a series of interleaved textual and visual rationales. Our
framework performs image editing using a natively multimodal, interleaved
text-image CoT. This approach generates a step-by-step chain of reasoning where
a textual description is followed by a corresponding visual cue, such as a
positional mask that defined intended edited regions or a representation of new
content. Furthermore, to mitigate the hallucination phenomenon of large
language models, we introduce Multimodal Deep Confidence (MMDC) reasoning
paradigm. This paradigm explores a tree of visual reasoning paths at each step.
By pruning low-quality branches using a deep confidence score from a reward
model, it ensures the model consistently follows a high-quality trajectory
towards the final edited result. The proposed method decomposes complex editing
tasks into interdependent sub-tasks, achieving greater precision at each stage
and yielding high-fidelity edited results. We define the formulation for
interleaved text-image chains and release the first CoT-Edit-14K dataset,
comprising 14K high-quality editing examples. Extensive experiments show that
our method yields significant improvements across three image editing
benchmarks.


### [17] [InstructUDrag: Joint Text Instructions and Object Dragging for Interactive Image Editing](https://arxiv.org/abs/2510.08181)
*Haoran Yu, Yi Shi*

#### 🧩 TL;DR
本文提出InstructUDrag框架，将文本指令与物体拖拽相结合，实现了同时进行物体拖拽和基于文本的图像编辑，解决了现有方法在精确定位和语义控制方面的局限性。

---

#### 📘 Detailed Summary
**Motivation:** 当前文本到图像扩散模型在图像编辑中存在两个主要问题：基于文本的方法难以实现精确的物体定位，而物体拖拽方法仅限于静态重定位，无法同时进行语义编辑。

**Method:** 提出双分支协同框架，其中移动重建分支使用基于能量的梯度引导来精确移动物体，通过优化交叉注意力图提升重定位精度；文本驱动编辑分支与重建分支共享梯度信号，确保变换一致性并实现细粒度属性控制；同时采用DDPM反演和先验信息注入来保持移动物体的结构完整性。

**Result:** 大量实验表明，InstructUDrag能够实现灵活、高保真度的图像编辑，在物体重定位精度和图像内容语义控制方面均表现出色。

**Conclusion:** 该研究证明了文本指令与物体拖拽结合的有效性，为扩散模型在图像编辑领域提供了更精确和语义丰富的控制能力，推动了交互式图像编辑技术的发展。

---

#### 📄 Abstract
Text-to-image diffusion models have shown great potential for image editing,
with techniques such as text-based and object-dragging methods emerging as key
approaches. However, each of these methods has inherent limitations: text-based
methods struggle with precise object positioning, while object dragging methods
are confined to static relocation. To address these issues, we propose
InstructUDrag, a diffusion-based framework that combines text instructions with
object dragging, enabling simultaneous object dragging and text-based image
editing. Our framework treats object dragging as an image reconstruction
process, divided into two synergistic branches. The moving-reconstruction
branch utilizes energy-based gradient guidance to move objects accurately,
refining cross-attention maps to enhance relocation precision. The text-driven
editing branch shares gradient signals with the reconstruction branch, ensuring
consistent transformations and allowing fine-grained control over object
attributes. We also employ DDPM inversion and inject prior information into
noise maps to preserve the structure of moved objects. Extensive experiments
demonstrate that InstructUDrag facilitates flexible, high-fidelity image
editing, offering both precision in object relocation and semantic control over
image content.


### [18] [Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception](https://arxiv.org/abs/2510.08352)
*Nikos Theodoridis, Tim Brophy, Reenu Mohandas, Ganesh Sistu, Fiachra Collins, Anthony Scanlan, Ciaran Eising*

#### 🧩 TL;DR
该研究提出了首个专注于交通场景感知的视觉问答基准DTPQA，通过距离标注和排除推理问题来评估小型视觉语言模型的纯感知能力，发现当前模型在远距离感知和方向辨别等基础任务上显著落后于人类表现。

---

#### 📘 Detailed Summary
**Motivation:** 自动驾驶系统需要可靠的感知能力来处理意外情况，但现有视觉语言模型在安全关键应用中的可信度不足，特别是在远距离感知方面存在明显短板，且自动驾驶硬件无法支持大型模型，因此需要专门评估小型模型在交通场景中的纯感知性能。

**Method:** 研究引入了距离标注交通感知问答基准DTPQA，这是首个专注于交通场景感知的视觉问答基准，通过排除需要推理的问题确保仅评估感知能力，并针对多个先进的小型视觉语言模型进行系统性评估。

**Result:** 实验结果显示，表现最佳的小型视觉语言模型平均准确率仅为约60%，远低于人类约85%的表现，特别是在区分左右等基础感知任务上模型表现尤为困难，尽管人类样本量较小存在统计限制。

**Conclusion:** 研究表明当前小型视觉语言模型在交通场景的纯感知任务上仍存在显著差距，特别是在远距离感知和方向辨别方面，这为自动驾驶系统的可靠感知能力发展提供了重要基准和方向指引。

---

#### 📄 Abstract
Vision-Language Models (VLMs) are becoming increasingly powerful,
demonstrating strong performance on a variety of tasks that require both visual
and textual understanding. Their strong generalisation abilities make them a
promising component for automated driving systems, which must handle unexpected
corner cases. However, to be trusted in such safety-critical applications, a
model must first possess a reliable perception system. Moreover, since critical
objects and agents in traffic scenes are often at a distance, we require
systems that are not "shortsighted", i.e., systems with strong perception
capabilities at both close (up to 20 meters) and long (30+ meters) range. With
this in mind, we introduce Distance-Annotated Traffic Perception Question
Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused
solely on perception-based questions in traffic scenes, enriched with distance
annotations. By excluding questions that require reasoning, we ensure that
model performance reflects perception capabilities alone. Since automated
driving hardware has limited processing power and cannot support large VLMs,
our study centers on smaller VLMs. More specifically, we evaluate several
state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the
simplicity of the questions, these models significantly underperform compared
to humans (~60% average accuracy for the best-performing small VLM versus ~85%
human performance). However, it is important to note that the human sample size
was relatively small, which imposes statistical limitations. We also identify
specific perception tasks, such as distinguishing left from right, that remain
particularly challenging for these models.


### [19] [A Multimodal Depth-Aware Method For Embodied Reference Understanding](https://arxiv.org/abs/2510.08278)
*Fevziye Irem Eyiokur, Dogucan Yaman, Hazım Kemal Ekenel, Alexander Waibel*

#### 🧩 TL;DR
本文提出了一种新颖的具身指称理解框架，通过联合利用基于LLM的数据增强、深度图模态和深度感知决策模块，有效解决了多候选对象场景下的歧义问题，显著提升了指称检测的准确性和可靠性。

---

#### 📘 Detailed Summary
**Motivation:** 现有开放词汇对象检测方法在多候选对象存在的模糊场景中经常失败，无法有效处理视觉场景中基于语言指令和指向线索的目标对象识别挑战，特别是在复杂或杂乱环境中的歧义消除问题。

**Method:** 提出的ERU框架整合了基于大语言模型的数据增强技术、深度图模态信息以及深度感知决策模块，实现了语言线索与具身线索的鲁棒融合，专门针对复杂环境中的歧义场景进行优化设计。

**Result:** 在两个数据集上的实验结果表明，该方法显著优于现有基线方法，在指称检测任务中实现了更准确和可靠的性能表现，证明了所提框架在复杂环境中的有效性。

**Conclusion:** 该研究展示了多模态融合在具身指称理解中的重要性，深度信息的引入为歧义场景下的目标识别提供了关键解决方案，为未来具身AI系统的发展提供了有价值的参考方向。

---

#### 📄 Abstract
Embodied Reference Understanding requires identifying a target object in a
visual scene based on both language instructions and pointing cues. While prior
works have shown progress in open-vocabulary object detection, they often fail
in ambiguous scenarios where multiple candidate objects exist in the scene. To
address these challenges, we propose a novel ERU framework that jointly
leverages LLM-based data augmentation, depth-map modality, and a depth-aware
decision module. This design enables robust integration of linguistic and
embodied cues, improving disambiguation in complex or cluttered environments.
Experimental results on two datasets demonstrate that our approach
significantly outperforms existing baselines, achieving more accurate and
reliable referent detection.


### [20] [Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge](https://arxiv.org/abs/2510.08316)
*Yu Huang, Zelin Peng, Changsong Wen, Xiaokang Yang, Wei Shen*

#### 🧩 TL;DR
本文提出了一种语义基础的学习范式，通过跨模态亲和力迁移将大规模2D视觉基础模型的丰富语义知识转移到3D领域，实现了3D功能分割的最先进性能。

---

#### 📘 Detailed Summary
**Motivation:** 现有3D功能分割方法通常依赖点云编码器作为通用特征提取器，但忽视了3D数据固有的稀疏性、噪声和几何模糊性等挑战，导致学习到的3D特征缺乏清晰且语义一致的功能边界。

**Method:** 提出了跨模态亲和力迁移预训练策略，将3D编码器与提升的2D语义对齐，并联合优化重建、亲和力和多样性以产生语义组织的表示；在此基础上设计了跨模态功能分割变换器，集成多模态提示与CMAT预训练特征以生成精确的提示感知分割图。

**Result:** 在标准基准测试上的广泛实验表明，该框架为3D功能分割建立了新的最先进结果，显著提升了分割精度和语义一致性。

**Conclusion:** 该研究证明了将2D视觉基础模型的语义知识迁移到3D领域的有效性，为解决3D数据固有挑战提供了新的学习范式，对机器人操作、具身AI和增强现实等应用具有重要意义。

---

#### 📄 Abstract
Affordance segmentation aims to parse 3D objects into functionally distinct
parts, bridging recognition and interaction for applications in robotic
manipulation, embodied AI, and AR. While recent studies leverage visual or
textual prompts to guide this process, they often rely on point cloud encoders
as generic feature extractors, overlooking the intrinsic challenges of 3D data
such as sparsity, noise, and geometric ambiguity. As a result, 3D features
learned in isolation frequently lack clear and semantically consistent
functional boundaries. To address this bottleneck, we propose a
semantic-grounded learning paradigm that transfers rich semantic knowledge from
large-scale 2D Vision Foundation Models (VFMs) into the 3D domain.
Specifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training
strategy that aligns a 3D encoder with lifted 2D semantics and jointly
optimizes reconstruction, affinity, and diversity to yield semantically
organized representations. Building on this backbone, we further design the
Cross-modal Affordance Segmentation Transformer (CAST), which integrates
multi-modal prompts with CMAT-pretrained features to generate precise,
prompt-aware segmentation maps. Extensive experiments on standard benchmarks
demonstrate that our framework establishes new state-of-the-art results for 3D
affordance segmentation.


### [21] [The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping](https://arxiv.org/abs/2510.08482)
*Onur Keleş, Aslı Özyürek, Gerardo Ortega, Kadir Gökgö, Esam Ghaleb*

#### 🧩 TL;DR
本文提出了视觉象似性挑战基准，用于评估视觉语言模型在手语象似性理解方面的能力。研究发现现有模型在音系形式预测和透明度任务上远低于人类基线，但与人类象似性评分存在中等相关性。

---

#### 📘 Detailed Summary
**Motivation:** 手语中的象似性（语言形式与意义之间的相似性）为视觉基础提供了天然测试平台，但现有视觉语言模型难以从动态人体运动中恢复这种本质映射关系，需要开发专门的评估基准来诊断模型在视觉基础方面的能力。

**Method:** 研究引入视觉象似性挑战基准，将心理语言学测量方法应用于视频数据，包含三个诊断任务：音系手语形式预测（手形、位置）、透明度（从视觉形式推断意义）和分级象似性评分，在零样本和少样本设置下评估了13个最先进的视觉语言模型。

**Result:** 在音系形式预测任务中，模型能够恢复部分手形和位置细节但仍低于人类表现；在透明度任务上远未达到人类基线水平；只有顶级模型与人类象似性评分存在中等相关性。有趣的是，音系形式预测能力更强的模型与人类象似性判断相关性更好。

**Conclusion:** 研究验证了这些诊断任务的有效性，表明模型对视觉基础结构具有共享敏感性，强调了将人类中心信号和具身学习方法纳入多模态模型以改进视觉基础建模的重要性，为理解模型如何从视觉输入中学习语言结构提供了新见解。

---

#### 📄 Abstract
Iconicity, the resemblance between linguistic form and meaning, is pervasive
in signed languages, offering a natural testbed for visual grounding. For
vision-language models (VLMs), the challenge is to recover such essential
mappings from dynamic human motion rather than static context. We introduce the
\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts
psycholinguistic measures to evaluate VLMs on three tasks: (i) phonological
sign-form prediction (e.g., handshape, location), (ii) transparency (inferring
meaning from visual form), and (iii) graded iconicity ratings. We assess $13$
state-of-the-art VLMs in zero- and few-shot settings on Sign Language of the
Netherlands and compare them to human baselines. On \textit{phonological form
prediction}, VLMs recover some handshape and location detail but remain below
human performance; on \textit{transparency}, they are far from human baselines;
and only top models correlate moderately with human \textit{iconicity ratings}.
Interestingly, \textit{models with stronger phonological form prediction
correlate better with human iconicity judgment}, indicating shared sensitivity
to visually grounded structure. Our findings validate these diagnostic tasks
and motivate human-centric signals and embodied learning methods for modelling
iconicity and improving visual grounding in multimodal models.


### [22] [Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning](https://arxiv.org/abs/2510.08442)
*Andrew Lee, Ian Chuang, Dechen Gao, Kai Fukazawa, Iman Soltani*

#### 🧩 TL;DR
本文提出了Gaze on the Prize框架，通过引入可学习的中央凹注意力机制，利用回报差异引导的对比学习来提升视觉强化学习的样本效率，在ManiSkill3基准测试中实现了最高2.4倍的性能提升。

---

#### 📘 Detailed Summary
**Motivation:** 视觉强化学习代理需要处理高维图像数据，但其中只有少量像素与任务相关，导致代理在无关特征上浪费探索和计算资源，造成样本效率低下和学习不稳定的问题。

**Method:** 该框架引入可学习的中央凹注意力机制，通过基于回报差异的自监督信号进行引导，采用回报引导的对比学习方法，将相似视觉表示根据回报差异分组为正负样本，构建对比三元组来训练注意力机制区分不同结果相关的状态表示。

**Result:** 在ManiSkill3基准测试套件的多个操作任务中，该方法实现了最高2.4倍的样本效率提升，并且能够解决基线方法无法学习的任务，且无需修改底层算法或超参数。

**Conclusion:** 研究表明回报差异能够有效揭示任务相关特征，基于对比学习的注意力机制可以显著提升视觉强化学习的效率和稳定性，为处理高维视觉输入提供了新的解决方案。

---

#### 📄 Abstract
Visual Reinforcement Learning (RL) agents must learn to act based on
high-dimensional image data where only a small fraction of the pixels is
task-relevant. This forces agents to waste exploration and computational
resources on irrelevant features, leading to sample-inefficient and unstable
learning. To address this, inspired by human visual foveation, we introduce
Gaze on the Prize. This framework augments visual RL with a learnable foveal
attention mechanism (Gaze), guided by a self-supervised signal derived from the
agent's experience pursuing higher returns (the Prize). Our key insight is that
return differences reveal what matters most: If two similar representations
produce different outcomes, their distinguishing features are likely
task-relevant, and the gaze should focus on them accordingly. This is realized
through return-guided contrastive learning that trains the attention to
distinguish between the features relevant to success and failure. We group
similar visual representations into positives and negatives based on their
return differences and use the resulting labels to construct contrastive
triplets. These triplets provide the training signal that teaches the attention
mechanism to produce distinguishable representations for states associated with
different outcomes. Our method achieves up to 2.4x improvement in sample
efficiency and can solve tasks that the baseline fails to learn, demonstrated
across a suite of manipulation tasks from the ManiSkill3 benchmark, all without
modifying the underlying algorithm or hyperparameters.


### [23] [To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models](https://arxiv.org/abs/2510.08510)
*Jiayun Luo, Wan-Cyuan Fan, Lyuyang Wang, Xiangteng He, Tanzila Rahman, Purang Abolmaesumi, Leonid Sigal*

#### 🧩 TL;DR
该研究发现了视觉变换器中的注意力汇聚点现象，并提出通过显式利用这些高范数视觉标记来增强大视觉语言模型的视觉推理能力，在多个任务上实现了显著性能提升。

---

#### 📘 Detailed Summary
**Motivation:** 现有研究主要关注LLM内部的注意力汇聚点，而忽视了视觉编码器中高范数视觉标记的重要作用，这些ViT注意力汇聚点虽然包含高级语义概念，但在现有LVLM架构中往往被忽略，限制了模型的视觉理解与推理能力。

**Method:** 研究通过定性和定量分析识别ViT中的高范数视觉标记作为注意力汇聚点，并提出无需训练和基于训练的两种方法来优化LLM对这些标记信息的利用程度，从而更有效地传播视觉信号。

**Result:** 实验表明显式利用ViT注意力汇聚点能够在多种LVLM模型和视觉推理任务上带来显著性能提升，证明了这些标记在增强视觉推理方面具有未开发的潜力。

**Conclusion:** ViT注意力汇聚点作为视觉语义信息的关键载体，其有效利用对提升LVLM的视觉理解能力至关重要，这一发现为优化视觉-语言模态交互提供了新的研究方向。

---

#### 📄 Abstract
Large Vision Language Models (LVLMs) have recently emerged as powerful
architectures capable of understanding and reasoning over both visual and
textual information. These models typically rely on two key components: a
Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual
content into a sequence of image tokens and serves as the perceptual front-end
-- the eyes of the model. In contrast, the LLM interprets these tokens to
perform high-level reasoning, generates responses, and functions as the
cognitive core -- the brain of the model. However, it remains unclear which
visual tokens contribute most significantly to understanding and reasoning, and
how effectively these signals are propagated from ViT to the LLM. While most
existing works have focused on identifying attention sinks, low-semantic tokens
receiving disproportionately high attention, within the LLM, we shift the focus
to the vision encoder by identifying a class of high-norm visual tokens from
ViT, referred to as ViT attention sinks -- a problem that has been rarely
studied but is indeed very important for LVLMs. Our findings show that these
ViT sinks encapsulate high-level semantic concepts from images, allowing the
LLM to perform more effective understanding and reasoning. Despite their
importance, these sink tokens are often overlooked in existing LVLM
architectures. To explore their contribution, we present both qualitative and
quantitative analyses of the information embedded in these sink tokens. We also
propose both training-free and training-based approaches to better leverage how
this information is interpreted by the LLM, and to what extent. By explicitly
utilizing these tokens, we demonstrate substantial improvements across a range
of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT
attention sinks in enhancing visual reasoning.


### [24] [UniVideo: Unified Understanding, Generation, and Editing for Videos](https://arxiv.org/abs/2510.08377)
*Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhu Chen*

#### 🧩 TL;DR
UniVideo是一个统一的多模态视频生成与编辑框架，通过双流架构将多模态大语言模型与多模态DiT相结合，实现了对复杂多模态指令的准确理解与视觉一致性的视频生成，在多种视频任务上达到或超越了任务专用模型性能。

---

#### 📘 Detailed Summary
**Motivation:** 当前统一多模态模型主要局限于图像领域，视频领域的统一建模仍存在明显空白，需要开发能够处理多样化视频生成与编辑任务的通用框架。

**Method:** UniVideo采用双流架构设计，结合多模态大语言模型进行指令理解，以及多模态DiT进行视频生成，通过单一多模态指令范式统一多种视频任务并进行联合训练。

**Result:** 实验表明UniVideo在文本/图像到视频生成、上下文视频生成和上下文视频编辑等任务上匹配或超越了最先进的任务专用基线模型，并展现出任务组合和跨任务泛化能力。

**Conclusion:** UniVideo的统一设计实现了任务组合能力和跨领域泛化能力，即使未经专门训练也能处理未见过的编辑指令，为视频多模态研究提供了新的通用框架方向。

---

#### 📄 Abstract
Unified multimodal models have shown promising results in multimodal content
generation and editing but remain largely limited to the image domain. In this
work, we present UniVideo, a versatile framework that extends unified modeling
to the video domain. UniVideo adopts a dual-stream design, combining a
Multimodal Large Language Model (MLLM) for instruction understanding with a
Multimodal DiT (MMDiT) for video generation. This design enables accurate
interpretation of complex multimodal instructions while preserving visual
consistency. Built on this architecture, UniVideo unifies diverse video
generation and editing tasks under a single multimodal instruction paradigm and
is jointly trained across them. Extensive experiments demonstrate that UniVideo
matches or surpasses state-of-the-art task-specific baselines in
text/image-to-video generation, in-context video generation and in-context
video editing. Notably, the unified design of UniVideo enables two forms of
generalization. First, UniVideo supports task composition, such as combining
editing with style transfer, by integrating multiple capabilities within a
single instruction. Second, even without explicit training on free-form video
editing, UniVideo transfers its editing capability from large-scale image
editing data to this setting, handling unseen instructions such as
green-screening characters or changing materials within a video. Beyond these
core capabilities, UniVideo also supports visual-prompt-based video generation,
where the MLLM interprets visual prompts and guides the MMDiT during synthesis.
To foster future research, we will release our model and code.


### [25] [SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.08531)
*Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang*

#### 🧩 TL;DR
本文提出了一种渐进式训练框架SpatialLadder，通过构建SpatialLadder-26k多模态数据集和三级渐进训练方法，在空间推理任务上实现了最先进的性能，相比基础模型提升23.4%，超越GPT-4o和Gemini-2.0-Flash。

---

#### 📘 Detailed Summary
**Motivation:** 当前视觉语言模型在空间推理方面存在根本性挑战，现有方法试图直接学习空间推理而缺乏建立感知和理解的层次化基础，这导致了性能瓶颈。

**Method:** 提出了三级渐进训练框架：首先通过目标定位建立空间感知，然后通过多维度空间任务发展空间理解，最后通过可验证奖励的强化学习加强复杂推理，并构建了包含26,610个样本的SpatialLadder-26k多模态数据集。

**Result:** SpatialLadder模型在空间推理基准测试中达到最先进性能，相比基础模型平均提升23.4%，超越GPT-4o 20.8%和Gemini-2.0-Flash 10.1%，在域外基准上保持7.2%的泛化改进。

**Conclusion:** 研究表明从感知到推理的渐进式训练对于构建稳健的空间智能至关重要，该方法为多模态空间推理提供了系统化的解决方案，并验证了层次化学习框架的有效性。

---

#### 📄 Abstract
Spatial reasoning remains a fundamental challenge for Vision-Language Models
(VLMs), with current approaches struggling to achieve robust performance
despite recent advances. We identify that this limitation stems from a critical
gap: existing methods attempt to learn spatial reasoning directly without
establishing the hierarchical foundations of perception and understanding. To
address this challenge, we present a comprehensive methodology for building
spatial intelligence progressively. We introduce SpatialLadder-26k, a
multimodal dataset containing 26,610 samples spanning object localization,
single image, multi-view, and video spatial reasoning tasks, constructed
through a standardized pipeline that ensures systematic coverage across
modalities. Building on this dataset, we design a three-stage progressive
training framework that (1) establishes spatial perception through object
localization, (2) develops spatial understanding through multi-dimensional
spatial tasks, and (3) strengthens complex reasoning via reinforcement learning
with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter
model that achieves state-of-the-art performance on spatial reasoning
benchmarks, with 23.4% average improvement over the base model, surpassing
GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains
strong generalization with 7.2% improvement on out-of-domain benchmarks,
demonstrating that progressive training from perception to reasoning is
essential for robust spatial intelligence.


### [26] [VideoVerse: How Far is Your T2V Generator from a World Model?](https://arxiv.org/abs/2510.08398)
*Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, Lei Zhang*

#### 🧩 TL;DR
本文提出了VideoVerse基准测试，这是一个专注于评估文本到视频生成模型在理解复杂时间因果关系和世界知识方面能力的综合基准，旨在衡量当前T2V模型与世界模型之间的差距。

---

#### 📘 Detailed Summary
**Motivation:** 现有文本到视频生成基准在评估最先进T2V模型时存在三个主要不足：当前评估维度无法区分最先进的T2V模型，事件级时间因果关系在现有基准中被严重忽视，以及现有基准缺乏对构建世界模型至关重要的世界知识的系统性评估。

**Method:** 研究团队收集了跨多个领域的代表性视频，提取具有内在时间因果关系的事件级描述，并由独立标注者将其重写为文本到视频提示，设计了包含十个精心定义评估维度的二元评估问题套件，并开发了基于现代视觉语言模型的人类偏好对齐QA评估流程。

**Result:** VideoVerse基准包含300个精心策划的提示，涉及815个事件和793个二元评估问题，通过对最先进的开源和闭源T2V模型进行系统性评估，提供了关于当前T2V生成器与世界模型之间差距的深入分析。

**Conclusion:** 该研究揭示了当前文本到视频生成模型在理解复杂时间因果关系和世界知识方面的局限性，为未来开发更接近世界模型的T2V系统提供了重要的评估框架和方向指导，强调了在视频生成中纳入因果推理和常识知识的重要性。

---

#### 📄 Abstract
The recent rapid advancement of Text-to-Video (T2V) generation technologies,
which are critical to build ``world models'', makes the existing benchmarks
increasingly insufficient to evaluate state-of-the-art T2V models. First,
current evaluation dimensions, such as per-frame aesthetic quality and temporal
consistency, are no longer able to differentiate state-of-the-art T2V models.
Second, event-level temporal causality, which not only distinguishes video from
other modalities but also constitutes a crucial component of world models, is
severely underexplored in existing benchmarks. Third, existing benchmarks lack
a systematic assessment of world knowledge, which are essential capabilities
for building world models. To address these issues, we introduce VideoVerse, a
comprehensive benchmark that focuses on evaluating whether a T2V model could
understand complex temporal causality and world knowledge in the real world. We
collect representative videos across diverse domains (e.g., natural landscapes,
sports, indoor scenes, science fiction, chemical and physical experiments) and
extract their event-level descriptions with inherent temporal causality, which
are then rewritten into text-to-video prompts by independent annotators. For
each prompt, we design a suite of binary evaluation questions from the
perspective of dynamic and static properties, with a total of ten carefully
defined evaluation dimensions. In total, our VideoVerse comprises 300 carefully
curated prompts, involving 815 events and 793 binary evaluation questions.
Consequently, a human preference aligned QA-based evaluation pipeline is
developed by using modern vision-language models. Finally, we perform a
systematic evaluation of state-of-the-art open-source and closed-source T2V
models on VideoVerse, providing in-depth analysis on how far the current T2V
generators are from world models.


### [27] [VideoNorms: Benchmarking Cultural Awareness of Video Language Models](https://arxiv.org/abs/2510.08543)
*Nikhil Reddy Varimalla, Yunfei Xu, Arkadiy Saakyan, Meng Fan Wang, Smaranda Muresan*

#### 🧩 TL;DR
本文提出了VideoNorms基准数据集，包含1000多个视频片段与社会文化规范对，用于评估视频大语言模型的文化意识。研究发现现有模型在文化规范理解方面存在显著差距，特别是在中国文化理解、非语言证据识别和规范违反检测方面表现较差。

---

#### 📘 Detailed Summary
**Motivation:** 随着视频大语言模型在全球部署，需要评估其对相关文化背景的理解和基础能力。当前缺乏适当的基准来评估这些模型的文化意识，特别是针对不同文化背景的社会规范理解能力。

**Method:** 采用人机协作框架构建VideoNorms基准，其中使用基于理论指导提示的教师模型提供候选标注，并由训练有素的人类专家验证和修正标注。该数据集包含美中文化的视频片段与规范对，基于言语行为理论标注了社会文化规范、规范遵守与违反标签以及语言和非语言证据。

**Result:** 基准测试显示多个常见趋势：模型在规范违反检测上表现比规范遵守差；对中国文化的理解表现不如美国文化；在提供非语言证据方面比语言证据更困难；难以识别与言语行为对应的确切规范；与人类不同，模型在正式非幽默语境中表现更差。

**Conclusion:** 研究强调了文化基础视频语言模型训练的必要性，现有模型在跨文化规范理解方面存在显著差距。VideoNorms基准和框架为解决这一差距提供了起点，为开发更具文化意识的视频理解模型奠定了基础。

---

#### 📄 Abstract
As Video Large Language Models (VideoLLMs) are deployed globally, they
require understanding of and grounding in the relevant cultural background. To
properly assess these models' cultural awareness, adequate benchmarks are
needed. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm)
pairs from US and Chinese cultures annotated with socio-cultural norms grounded
in speech act theory, norm adherence and violations labels, and verbal and
non-verbal evidence. To build VideoNorms, we use a human-AI collaboration
framework, where a teacher model using theoretically-grounded prompting
provides candidate annotations and a set of trained human experts validate and
correct the annotations. We benchmark a variety of open-weight VideoLLMs on the
new dataset which highlight several common trends: 1) models performs worse on
norm violation than adherence; 2) models perform worse w.r.t Chinese culture
compared to the US culture; 3) models have more difficulty in providing
non-verbal evidence compared to verbal for the norm adhere/violation label and
struggle to identify the exact norm corresponding to a speech-act; and 4)
unlike humans, models perform worse in formal, non-humorous contexts. Our
findings emphasize the need for culturally-grounded video language model
training - a gap our benchmark and framework begin to address.


### [28] [Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency](https://arxiv.org/abs/2510.08431)
*Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang*

#### 🧩 TL;DR
本研究提出了分数正则化连续时间一致性模型（rCM），首次将连续时间一致性蒸馏扩展到大规模文本到图像和视频扩散模型，通过结合分数蒸馏作为长跳跃正则化器，在保持高生成多样性的同时显著提升视觉质量。

---

#### 📘 Detailed Summary
**Motivation:** 连续时间一致性模型（sCM）虽然在学术规模的扩散模型加速中表现出理论优势和实证能力，但其在大规模文本到图像和视频任务中的适用性仍不明确，主要面临雅可比向量积计算的基础设施挑战以及标准评估基准的局限性，特别是在精细细节生成方面存在根本性的质量限制。

**Method:** 研究首先开发了并行兼容的FlashAttention-2 JVP核，支持超过100亿参数模型和高维视频任务的sCM训练；随后提出分数正则化连续时间一致性模型（rCM），通过引入分数蒸馏作为长跳跃正则化器，将sCM的'模式覆盖'前向散度目标与'模式寻求'反向散度相结合。

**Result:** 在高达140亿参数的Cosmos-Predict2、Wan2.1等大规模模型和5秒视频任务上的验证表明，rCM在质量指标上匹配或超越了最先进的蒸馏方法DMD2，同时在多样性方面具有显著优势，仅需1~4步即可生成高保真样本，将扩散采样加速15~50倍，且无需GAN调优或大量超参数搜索。

**Conclusion:** rCM作为一个实用且理论完备的框架，通过解决sCM在精细细节生成中的误差累积和模式覆盖问题，为大尺度扩散蒸馏提供了新的解决方案，在保持生成多样性的同时显著提升了视觉质量，为大规模扩散模型的实际应用铺平了道路。

---

#### 📄 Abstract
This work represents the first effort to scale up continuous-time consistency
distillation to general application-level image and video diffusion models.
Although continuous-time consistency model (sCM) is theoretically principled
and empirically powerful for accelerating academic-scale diffusion, its
applicability to large-scale text-to-image and video tasks remains unclear due
to infrastructure challenges in Jacobian-vector product (JVP) computation and
the limitations of standard evaluation benchmarks. We first develop a
parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on
models with over 10 billion parameters and high-dimensional video tasks. Our
investigation reveals fundamental quality limitations of sCM in fine-detail
generation, which we attribute to error accumulation and the "mode-covering"
nature of its forward-divergence objective. To remedy this, we propose the
score-regularized continuous-time consistency model (rCM), which incorporates
score distillation as a long-skip regularizer. This integration complements sCM
with the "mode-seeking" reverse divergence, effectively improving visual
quality while maintaining high generation diversity. Validated on large-scale
models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM
matches or surpasses the state-of-the-art distillation method DMD2 on quality
metrics while offering notable advantages in diversity, all without GAN tuning
or extensive hyperparameter searches. The distilled models generate
high-fidelity samples in only $1\sim4$ steps, accelerating diffusion sampling
by $15\times\sim50\times$. These results position rCM as a practical and
theoretically grounded framework for advancing large-scale diffusion
distillation.


### [29] [MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning](https://arxiv.org/abs/2510.08567)
*Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan*

#### 🧩 TL;DR
本研究提出了一个视觉中心的智能体调优框架，通过自动合成多模态轨迹和生成逐步偏好对，训练视觉语言模型控制器实现稳健的工具使用推理，在多个基准测试中超越了现有开源和闭源模型。

---

#### 📘 Detailed Summary
**Motivation:** 当前视觉语言模型作为控制器访问外部工具进行复杂推理和决策时，面临高质量多模态轨迹稀缺和人工标注成本高昂的挑战，这限制了其有效性和可扩展性。

**Method:** 开发了视觉中心智能体调优框架，包括构建M-TRACE数据集（28.5K多模态任务和177K已验证轨迹）用于基于模仿的轨迹调优，训练MATRIX Agent控制器进行逐步工具推理，并引入Pref-X偏好对集合（11K自动生成）通过逐步偏好学习实现更精细的对齐。

**Result:** 在Agent-X、GTA和GAIA三个基准测试中，MATRIX控制器持续超越了开源和闭源的视觉语言模型，证明了其可扩展且有效的多模态工具使用能力。

**Conclusion:** 该研究展示了通过自动合成多模态轨迹和偏好学习，可以实现视觉语言模型在工具使用推理方面的显著提升，为构建更强大的多模态智能体提供了可扩展的解决方案，并开源了数据集和代码以促进进一步研究。

---

#### 📄 Abstract
Vision language models (VLMs) are increasingly deployed as controllers with
access to external tools for complex reasoning and decision-making, yet their
effectiveness remains limited by the scarcity of high-quality multimodal
trajectories and the cost of manual annotation. We address this challenge with
a vision-centric agent tuning framework that automatically synthesizes
multimodal trajectories, generates step-wise preference pairs, and trains a VLM
controller for robust tool-use reasoning. Our pipeline first constructs
M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified
trajectories, enabling imitation-based trajectory tuning. Building on this, we
develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool
reasoning. To achieve finer alignment, we further introduce Pref-X, a set of
11K automatically generated preference pairs, and optimize MATRIX on it via
step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,
MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating
scalable and effective multimodal tool use. Our data and code is avaliable at
https://github.com/mbzuai-oryx/MATRIX.


### [30] [SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models](https://arxiv.org/abs/2510.08559)
*Andong Deng, Taojiannan Yang, Shoubin Yu, Lincoln Spencer, Mohit Bansal, Chen Chen, Serena Yeung-Levy, Xiaohan Wang*

#### 🧩 TL;DR
本研究提出了SciVideoBench，一个专门用于评估科学视频推理能力的严格基准，包含1000个从尖端科学实验视频中精心构建的多选题，揭示了当前大型多模态模型在复杂科学视频推理任务上的显著性能缺陷。

---

#### 📘 Detailed Summary
**Motivation:** 当前视频基准主要针对依赖感知/识别的通用场景，推理任务相对简单，导致性能饱和且无法有效评估高级多模态认知技能，科学领域的复杂视频推理仍是一个具有挑战性的前沿问题。

**Method:** 研究构建了SciVideoBench基准，包含1000个从超过25个专业学科的前沿科学实验视频中精心设计的多选题，通过半自动验证系统确保质量，每个问题都需要复杂的领域知识、精确的时空感知和精细的逻辑推理。

**Result:** 评估显示包括Gemini 2.5 Pro和Qwen2.5-VL在内的最先进专有和开源大型多模态模型都存在显著性能缺陷，在科学视频推理能力方面仍有巨大提升空间。

**Conclusion:** 该研究为大型多模态模型的未来发展提供了宝贵见解和明确方向，通过分析推理复杂性和视觉基础等关键因素，推动真正具备能力的多模态AI共同科学家的演进，有望帮助推动前沿AI在更广泛科学领域的边界扩展。

---

#### 📄 Abstract
Large Multimodal Models (LMMs) have achieved remarkable progress across
various capabilities; however, complex video reasoning in the scientific domain
remains a significant and challenging frontier. Current video benchmarks
predominantly target general scenarios where perception/recognition is heavily
relied on, while with relatively simple reasoning tasks, leading to saturation
and thus failing to effectively evaluate advanced multimodal cognitive skills.
To address this critical gap, we introduce SciVideoBench, a rigorous benchmark
specifically designed to assess advanced video reasoning in scientific
contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice
questions derived from cutting-edge scientific experimental videos spanning
over 25 specialized academic subjects and verified by a semi-automatic system.
Each question demands sophisticated domain-specific knowledge, precise
spatiotemporal perception, and intricate logical reasoning, effectively
challenging models' higher-order cognitive abilities. Our evaluation highlights
significant performance deficits in state-of-the-art proprietary and
open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating
substantial room for advancement in video reasoning capabilities. Detailed
analyses of critical factors such as reasoning complexity and visual grounding
provide valuable insights and clear direction for future developments in LMMs,
driving the evolution of truly capable multimodal AI co-scientists. We hope
SciVideoBench could fit the interests of the community and help to push the
boundary of cutting-edge AI for border science.


### [31] [Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools](https://arxiv.org/abs/2510.08480)
*Zhenlong Yuan, Xiangyan Qu, Chengxuan Qian, Rui Chen, Jing Tang, Lei Sun, Xiangxiang Chu, Dapeng Zhang, Yiwei Wang, Yujun Cai, Shuo Li*

#### 🧩 TL;DR
本文提出Video-STAR框架，通过上下文子动作分解与工具增强强化学习的协同机制，解决了多模态大语言模型在开放词汇动作识别中语义相似动作区分困难的问题，显著提升了细粒度动作识别性能并减少了跨模态幻觉。

---

#### 📘 Detailed Summary
**Motivation:** 多模态大语言模型在视觉与文本推理方面展现出巨大潜力，但其对文本中心先验的依赖限制了在开放词汇场景下语义相似动作的区分能力，现有方法将动作视为整体实体处理，缺乏对细粒度动作模式的分解与匹配机制。

**Method:** 提出Video-STAR框架，创新性地将动作分解为判别性子动作进行细粒度匹配，同时动态调用领域特定工具实现跨模态交织，通过设计平衡工具使用效率、子动作相关性和结构连贯性的分层奖励机制，使模型能够自主利用外部工具优先处理子动作模式，实现从文本中心推理到视觉基础推理的转换。

**Result:** 在HMDB-51、UCF-101、SSv2、Kinetics-400和Kinetics-600数据集上的广泛评估表明，该方法在区分细粒度动作和处理跨模态幻觉方面优于现有方法，实现了最先进的性能，验证了其优秀的鲁棒性和泛化能力。

**Conclusion:** 该研究证明了通过子动作分解与工具增强强化学习的协同机制能够有效提升开放词汇动作识别的性能，为多模态推理提供了从文本中心到视觉基础的范式转变，为处理复杂视觉语义任务提供了新的技术路径。

---

#### 📄 Abstract
Multimodal large language models (MLLMs) have demonstrated remarkable
potential in bridging visual and textual reasoning, yet their reliance on
text-centric priors often limits their ability to disentangle semantically
similar actions in open-vocabulary scenarios. To address this, we propose
Video-STAR, a framework that harmonizes contextual sub-motion decomposition
with tool-augmented reinforcement learning for open-vocabulary action
recognition (OVAR). Unlike prior methods that treat actions as monolithic
entities, our approach innovatively decomposes actions into discriminative
sub-motions for fine-grained matching while dynamically invoking
domain-specific tools for cross-modal interleaving, thereby enabling
category-specific reasoning capacity and reducing cross-modal hallucination.
Moreover, by designing a hierarchical reward that balances tool-usage
efficiency, sub-motion relevance, and structural coherence in reasoning, our
method autonomously leverages external tools to prioritize sub-motion patterns
without explicit supervision, transmitting from text-centric reasoning to
visually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2,
Kinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art
performance, outperforming existing methods in distinguishing fine-grained
actions and handling cross-modal hallucination, validating our excellent
robustness and generalization.


### [32] [InstructX: Towards Unified Visual Editing with MLLM Guidance](https://arxiv.org/abs/2510.08485)
*Chong Mou, Qichao Sun, Yanze Wu, Pengze Zhang, Xinghui Li, Fulong Ye, Songtao Zhao, Qian He*

#### 🧩 TL;DR
本文提出了InstructX，一个统一的多模态大语言模型与扩散模型集成框架，用于图像和视频编辑任务。研究表明图像数据训练能够涌现视频编辑能力，并在单一模型中实现了图像和视频编辑任务的统一处理。

---

#### 📘 Detailed Summary
**Motivation:** 当前多模态大语言模型在视觉理解和推理方面取得显著进展，但将其与扩散模型集成用于编辑任务的研究缺乏深入的设计选择分析，特别是在视频编辑等复杂任务中，多模态大语言模型与扩散模型的整合仍面临挑战。

**Method:** 提出了InstructX统一框架，通过全面研究多模态大语言模型与扩散模型在指令驱动编辑任务中的集成策略，分析图像和视频在统一建模中的协作与区别，并引入模态特定的多模态大语言模型特征来实现图像和视频编辑任务的统一处理。

**Result:** 实验表明该方法能够处理广泛的图像和视频编辑任务，在无需显式监督的情况下，仅通过图像数据训练即可涌现视频编辑能力，有效缓解了视频训练数据稀缺的限制，并在性能上达到了最先进水平。

**Conclusion:** 研究揭示了图像训练向视频编辑能力的迁移潜力，证明了单一模型统一处理多模态编辑任务的可行性，为跨模态编辑任务提供了新的技术路径和理论洞见，推动了多模态编辑技术的发展。

---

#### 📄 Abstract
With recent advances in Multimodal Large Language Models (MLLMs) showing
strong visual understanding and reasoning, interest is growing in using them to
improve the editing performance of diffusion models. Despite rapid progress,
most studies lack an in-depth analysis of MLLM design choices. Moreover, the
integration of MLLMs and diffusion models remains an open challenge in some
difficult tasks, such as video editing. In this paper, we present InstructX, a
unified framework for image and video editing. Specifically, we conduct a
comprehensive study on integrating MLLMs and diffusion models for
instruction-driven editing across diverse tasks. Building on this study, we
analyze the cooperation and distinction between images and videos in unified
modeling. (1) We show that training on image data can lead to emergent video
editing capabilities without explicit supervision, thereby alleviating the
constraints imposed by scarce video training data. (2) By incorporating
modality-specific MLLM features, our approach effectively unifies image and
video editing tasks within a single model. Extensive experiments demonstrate
that our method can handle a broad range of image and video editing tasks and
achieves state-of-the-art performance.


### [33] [MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration](https://arxiv.org/abs/2510.08508)
*Lu Liu, Chunlei Cai, Shaocheng Shen, Jianfeng Liang, Weimin Ouyang, Tianxiao Ye, Jian Mao, Huiyu Duan, Jiangchao Yao, Xiaoyun Zhang, Qiang Hu, Guangtao Zhai*

#### 🧩 TL;DR
本文提出了MoA-VR，首个基于智能体混合的视频修复系统，通过三个协同工作的智能体模拟人类专家的推理和处理流程，有效处理现实世界中复杂多样的视频退化问题。

---

#### 📘 Detailed Summary
**Motivation:** 现实世界视频常因采集和传输条件不同而遭受噪声、压缩伪影和低光照失真等复杂退化，现有修复方法需要人工选择专用模型或采用无法泛化处理不同退化的单一架构。

**Method:** 提出MoA-VR系统，包含三个协调智能体：退化识别智能体基于构建的大规模高分辨率视频退化识别基准和视觉语言模型；自适应路由智能体由大语言模型驱动，通过观察工具使用模式自主学习有效修复策略；修复质量评估智能体基于专门构建的Res-VQ数据集和定制的VLM视频质量评估模型。

**Result:** 大量实验表明，MoA-VR能有效处理多样化和复合退化，在客观指标和感知质量方面均持续优于现有基线方法。

**Conclusion:** 研究结果突显了在多模态智能和模块化推理整合方面的潜力，为通用视频修复系统的发展提供了新方向。

---

#### 📄 Abstract
Real-world videos often suffer from complex degradations, such as noise,
compression artifacts, and low-light distortions, due to diverse acquisition
and transmission conditions. Existing restoration methods typically require
professional manual selection of specialized models or rely on monolithic
architectures that fail to generalize across varying degradations. Inspired by
expert experience, we propose MoA-VR, the first
\underline{M}ixture-\underline{o}f-\underline{A}gents \underline{V}ideo
\underline{R}estoration system that mimics the reasoning and processing
procedures of human professionals through three coordinated agents: Degradation
Identification, Routing and Restoration, and Restoration Quality Assessment.
Specifically, we construct a large-scale and high-resolution video degradation
recognition benchmark and build a vision-language model (VLM) driven
degradation identifier. We further introduce a self-adaptive router powered by
large language models (LLMs), which autonomously learns effective restoration
strategies by observing tool usage patterns. To assess intermediate and final
processed video quality, we construct the \underline{Res}tored
\underline{V}ideo \underline{Q}uality (Res-VQ) dataset and design a dedicated
VLM-based video quality assessment (VQA) model tailored for restoration tasks.
Extensive experiments demonstrate that MoA-VR effectively handles diverse and
compound degradations, consistently outperforming existing baselines in terms
of both objective metrics and perceptual quality. These results highlight the
potential of integrating multimodal intelligence and modular reasoning in
general-purpose video restoration systems.


### [34] [MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization](https://arxiv.org/abs/2510.08540)
*Xiangyu Zhao, Junming Lin, Tianhao Liang, Yifan Zhou, Wenhao Chai, Yuzhe Gu, Weiyun Wang, Kai Chen, Gen Luo, Wenwei Zhang, Junchi Yan, Hua Yang, Haodong Duan, Xue Yang*

#### 🧩 TL;DR
本研究提出了MM-HELIX多模态基准和自适应混合策略优化方法，显著提升了多模态大语言模型在长链反思推理任务上的性能，在基准测试中实现了18.6%的准确率提升，并展示了良好的泛化能力。

---

#### 📘 Detailed Summary
**Motivation:** 当前多模态大语言模型虽然在数学和逻辑推理任务上表现出色，但其在长链反思推理能力方面仍存在明显不足，这种能力是解决复杂现实问题的关键前提，因此需要系统性地评估和提升模型的这一能力。

**Method:** 研究首先构建了包含1,260个样本的MM-HELIX多模态基准，然后开发了步骤引导响应生成流水线创建了10万规模的MM-HELIX-100K数据集，并提出了自适应混合策略优化方法，该方法将离线监督学习和在线优化动态统一到单一训练阶段中。

**Result:** 在MM-HELIX基准测试中，所提方法在Qwen2.5-VL-7B基线上实现了18.6%的准确率提升，同时在通用数学和逻辑任务上平均性能提升了5.7%，证明了方法的有效泛化能力。

**Conclusion:** 研究表明多模态大语言模型的反思推理能力可以通过适当的数据生成和学习策略有效学习和泛化，这为开发更强大的多模态大语言模型开辟了新途径，特别是在处理需要迭代思考和回溯的复杂推理任务方面。

---

#### 📄 Abstract
While current Multimodal Large Language Models (MLLMs) have demonstrated
proficiency in reasoning tasks such as mathematics and logic, their capacity
for long-chain reflective reasoning, a prerequisite for solving complex
real-world problems, remains largely underexplored. In this work, we first
conduct an extensive empirical investigation to evaluate this capability.
Leveraging a carefully designed data synthesis engine, we construct MM-HELIX, a
multimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks
that require iterative thinking and backtracking. Empirical results on this
benchmark reveal that existing MLLMs exhibit significant performance deficits
in long-chain reflective reasoning. To address this limitation, we generate
post-training data and further explore learning paradigms for exploiting such
data. We first develop the Step-Elicited Response Generation pipeline to create
MM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning
traces for instruction-tuning stage. Given that standard Reinforcement Learning
fails on complex tasks due to sparse reward signals and catastrophic forgetting
after Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization
(AHPO), a novel training strategy that dynamically unifies offline supervision
and online optimization into a single stage. This strategy enables the model to
learn from expert data when rewards are sparse and conduct independent
exploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our
method achieves a +18.6\% accuracy improvement on MM-HELIX benchmark and
demonstrates strong generalization with a +5.7\% average performance gain on
general mathematic and logic tasks. Our work demonstrate that reflective
reasoning in MLLMs can be effectively learned and generalized, paving the way
for developing more capable MLLMs.


### [35] [MultiCOIN: Multi-Modal COntrollable Video INbetweening](https://arxiv.org/abs/2510.08561)
*Maham Tanveer, Yang Zhou, Simon Niklaus, Ali Mahdavi Amiri, Hao Zhang, Krishna Kumar Singh, Nanxuan Zhao*

#### 🧩 TL;DR
本文提出了一个支持多模态控制的视频插帧框架，通过将各种运动控制映射为统一的稀疏点表示，并采用双分支架构分别处理内容和运动控制，实现了灵活、精确且用户友好的视频过渡生成。

---

#### 📘 Detailed Summary
**Motivation:** 现有视频插帧方法无法生成大规模、复杂或精细的运动，且缺乏对用户意图的多样化支持以及对中间帧细节的精细控制，导致与创作意图不一致。

**Method:** 采用Diffusion Transformer作为视频生成模型，将所有运动控制映射为统一的稀疏点表示作为视频/噪声输入，通过分离内容和运动控制为两个分支分别编码特征，并采用分阶段训练策略确保多模态控制的平滑学习。

**Result:** 广泛的定性和定量实验表明，多模态控制能够实现更动态、可定制且上下文准确的视觉叙事，在视频插帧任务中表现出优越性能。

**Conclusion:** 多模态控制框架为视频编辑和长视频合成提供了更高的灵活性和精确性，平衡了用户友好性与细粒度控制能力，为创意视频制作开辟了新途径。

---

#### 📄 Abstract
Video inbetweening creates smooth and natural transitions between two image
frames, making it an indispensable tool for video editing and long-form video
synthesis. Existing works in this domain are unable to generate large, complex,
or intricate motions. In particular, they cannot accommodate the versatility of
user intents and generally lack fine control over the details of intermediate
frames, leading to misalignment with the creative mind. To fill these gaps, we
introduce \modelname{}, a video inbetweening framework that allows multi-modal
controls, including depth transition and layering, motion trajectories, text
prompts, and target regions for movement localization, while achieving a
balance between flexibility, ease of use, and precision for fine-grained video
interpolation. To achieve this, we adopt the Diffusion Transformer (DiT)
architecture as our video generative model, due to its proven capability to
generate high-quality long videos. To ensure compatibility between DiT and our
multi-modal controls, we map all motion controls into a common sparse and
user-friendly point-based representation as the video/noise input. Further, to
respect the variety of controls which operate at varying levels of granularity
and influence, we separate content controls and motion controls into two
branches to encode the required features before guiding the denoising process,
resulting in two generators, one for motion and the other for content. Finally,
we propose a stage-wise training strategy to ensure that our model learns the
multi-modal controls smoothly. Extensive qualitative and quantitative
experiments demonstrate that multi-modal controls enable a more dynamic,
customizable, and contextually accurate visual narrative.


### [36] [NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints](https://arxiv.org/abs/2510.08565)
*Changyao Tian, Hao Li, Gen Luo, Xizhou Zhu, Weijie Su, Hanming Deng, Jinguo Zhu, Jie Shao, Ziran Zhu, Yunpeng Liu, Lewei Lu, Wenhai Wang, Hongsheng Li, Jifeng Dai*

#### 🧩 TL;DR
本文提出了一种名为NaViL的端到端原生多模态大语言模型，系统研究了在数据约束下原生MLLM的设计空间和扩展特性，发现视觉编码器与LLM之间存在正相关的扩展关系，并在14个多模态基准测试中展现出竞争力。

---

#### 📘 Detailed Summary
**Motivation:** 现有多模态大语言模型通常采用组合式训练范式，即预训练视觉编码器通过连续多模态预训练与预训练LLM连接，但由于分离训练导致多模态扩展特性难以探索，因此需要研究原生端到端训练的MLLM设计空间和扩展特性。

**Method:** 通过系统研究MLLM的各种设计选择，获得了在性能和训练成本之间最佳平衡的元架构，并进一步探索了原生MLLM的扩展特性，提出了结合简单且成本效益高的训练方案的NaViL模型。

**Result:** 在14个多模态基准测试上的实验结果表明，NaViL相对于现有MLLM具有竞争力，同时揭示了视觉编码器与LLM之间正相关的扩展关系，为原生MLLM的未来研究提供了深入见解。

**Conclusion:** 研究发现视觉编码器与LLM之间存在正相关的扩展关系，提出的NaViL模型和训练方案为原生MLLM研究提供了重要参考，实验结果表明该方法在多个基准测试中具有竞争力，为未来MLLM研究指明了方向。

---

#### 📄 Abstract
Compositional training has been the de-facto paradigm in existing Multimodal
Large Language Models (MLLMs), where pre-trained vision encoders are connected
with pre-trained LLMs through continuous multimodal pre-training. However, the
multimodal scaling property of this paradigm remains difficult to explore due
to the separated training. In this paper, we focus on the native training of
MLLMs in an end-to-end manner and systematically study its design space and
scaling property under a practical setting, i.e., data constraint. Through
careful study of various choices in MLLM, we obtain the optimal
meta-architecture that best balances performance and training cost. After that,
we further explore the scaling properties of the native MLLM and indicate the
positively correlated scaling relationship between visual encoders and LLMs.
Based on these findings, we propose a native MLLM called NaViL, combined with a
simple and cost-effective recipe. Experimental results on 14 multimodal
benchmarks confirm the competitive performance of NaViL against existing MLLMs.
Besides that, our findings and results provide in-depth insights for the future
study of native MLLMs.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [37] [Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices](https://arxiv.org/abs/2510.07545)
*Md Tahmid Rahman Laskar, Mohammed Saidul Islam, Ridwan Mahbub, Mizanur Rahman, Amran Bhuiyan, Israt Jahan, Mir Tafseer Nayeem, Shafiq Joty, Enamul Hoque, Jimmy Huang*

#### 🧩 TL;DR
本研究提出了两种成本高效的评估方法：多标准提示和领域自适应迁移学习，成功将2B参数的小型视觉语言模型优化为ChartJudge，在图表理解任务中实现了与7B模型相当的自动评估性能。

---

#### 📘 Detailed Summary
**Motivation:** 当前大型视觉语言模型（7B参数）在图表理解任务中表现出作为自动评估器的潜力，但小型模型（≤2B参数）的评估性能仍然较差，这限制了它们在资源受限环境中的实际应用。

**Method:** 提出了两种方法：多标准提示将多个评估标准整合到单个查询中，以及领域自适应迁移学习，通过在图表数据集上使用合成判断对2B参数的LVLM进行微调，创建了ChartJudge模型。

**Result:** 实验表明多标准提示暴露了鲁棒性差距，导致7B模型性能大幅下降；ChartJudge能够有效在不同数据集间迁移知识，成为更专业的评估模型；细粒度分析揭示了模型大小、提示设计和可迁移性之间的权衡关系。

**Conclusion:** 该研究为图表推理任务提供了可扩展、低成本的评估解决方案，通过系统分析模型大小、提示设计和迁移能力之间的权衡关系，为资源受限环境下的高效评估提供了可行路径。

---

#### 📄 Abstract
Large Vision-Language Models (LVLMs) with only 7B parameters have shown
promise as automated judges in chart comprehension tasks. However, tiny models
(<=2B parameters) still perform poorly as judges, limiting their real-world use
in resource-constrained settings. To address this, we propose two approaches to
ensure cost-efficient evaluation: (i) multi-criteria prompting, which combines
separate evaluation criteria into a single query, and (ii) domain-adaptive
transfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic
judgments in a chart dataset to create the ChartJudge. Experiments show that
multi-criteria prompting exposes robustness gaps, which led to a huge drop in
performance for 7B models, including specialized LVLM judges like LLaVA-Critic.
In addition, we find that our tiny LVLM (ChartJudge) can effectively transfer
knowledge from one dataset to another to make it a more specialized model. Our
fine-grained analysis across chart types and query complexities offers
actionable insights into trade-offs between model size, prompt design, and
transferability, enabling scalable, low-cost evaluation for chart reasoning
tasks. Our code and the data will be made publicly available.


### [38] [ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs](https://arxiv.org/abs/2510.07737)
*Fu Chen, Peng Wang, Xiyin Li, Wen Li, Shichi Lei, Dongdong Xiang*

#### 🧩 TL;DR
本文提出了ToolExpander框架，通过动态多轮硬采样和自示例思维两个关键创新，解决了小规模LLMs在GRPO训练中响应不准确和训练崩溃的问题，显著提升了工具使用能力和训练稳定性。

---

#### 📘 Detailed Summary
**Motivation:** 训练大型语言模型使用GRPO方法时面临显著挑战：模型经常无法产生准确响应，特别是在小规模架构中。这一限制不仅降低了性能改进并削弱了GRPO的潜力，还经常导致训练中期崩溃，严重影响稳定性和最终效果。

**Method:** ToolExpander框架包含两个关键创新：(1)动态多轮硬采样，在训练期间动态替换困难样本（超过10次rollout无正确输出）为高质量少样本演示，并采用指数学习率衰减策略减轻振荡；(2)自示例思维，改进的GRPO框架消除KL散度并调整裁剪系数，通过最小额外奖励（0.01）鼓励模型自主生成和分析少样本示例。

**Result:** 实验结果表明，ToolExpander显著增强了LLMs的工具使用能力，特别是在较弱的小规模模型中，同时改善了训练稳定性和整体性能。

**Conclusion:** 该研究为解决资源受限LLMs在强化学习训练中的稳定性问题提供了有效方案，证明了通过动态样本替换和自示例生成机制可以显著提升小规模模型的工具使用能力和训练鲁棒性。

---

#### 📄 Abstract
Training Large Language Models (LLMs) with Group Relative Policy Optimization
(GRPO) encounters a significant challenge: models often fail to produce
accurate responses, particularly in small-scale architectures. This limitation
not only diminishes performance improvements and undermines the potential of
GRPO but also frequently leads to mid-training collapse, adversely affecting
stability and final efficacy. To address these issues, we propose ToolExpander,
a novel framework that advances tool-oriented reinforcement learning for
resource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round
Hard Sampling, which dynamically substitutes challenging samples(those without
correct outputs over 10 rollouts) with high-quality few-shot demonstrations
during training, coupled with an exponential learning rate decay strategy to
mitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO
framework that eliminates KL divergence and incorporates adjusted clipping
coefficients, encouraging models to autonomously generate and analyze few-shot
examples via a minimal additional reward (0.01).Experimental results
demonstrate that ToolExpander significantly enhances tool-using capabilities in
LLMs, especially in weaker small-scale models, improving both training
stability and overall performance.


### [39] [LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology](https://arxiv.org/abs/2510.07793)
*Sajib Acharjee Dip, Adrika Zafor, Bikash Kumar Paul, Uddip Acharjee Shuvo, Muhit Islam Emon, Xuan Wang, Liqing Zhang*

#### 🧩 TL;DR
LLM4Cell提出了首个统一的大语言模型在单细胞生物学中的综合调查，系统评估了58个基础模型和智能体框架，涵盖了RNA、ATAC、多组学和空间模态，为语言驱动的单细胞智能研究提供了整合视角。

---

#### 📘 Detailed Summary
**Motivation:** 当前大语言模型和智能体框架在单细胞生物学中的应用进展碎片化，缺乏跨数据模态、架构和评估标准的统一视角，阻碍了该领域的系统发展。

**Method:** 研究系统分类了58个模型为五个家族：基础模型、文本桥接、空间模型、多模态、表观基因组和智能体模型，并将其映射到八个关键分析任务，包括细胞注释、轨迹建模、扰动分析和药物反应预测。

**Result:** 基于40多个公共数据集的分析显示，模型在10个领域维度上表现出不同的性能特征，涵盖了生物基础性、多组学对齐、公平性、隐私保护和可解释性等方面，揭示了基准适用性和数据多样性的挑战。

**Conclusion:** 该研究为语言驱动的单细胞智能提供了首个整合视图，指出了在可解释性、标准化和可信模型开发方面的开放挑战，为未来研究方向提供了系统框架。

---

#### 📄 Abstract
Large language models (LLMs) and emerging agentic frameworks are beginning to
transform single-cell biology by enabling natural-language reasoning,
generative annotation, and multimodal data integration. However, progress
remains fragmented across data modalities, architectures, and evaluation
standards. LLM4Cell presents the first unified survey of 58 foundation and
agentic models developed for single-cell research, spanning RNA, ATAC,
multi-omic, and spatial modalities. We categorize these methods into five
families-foundation, text-bridge, spatial, multimodal, epigenomic, and
agentic-and map them to eight key analytical tasks including annotation,
trajectory and perturbation modeling, and drug-response prediction. Drawing on
over 40 public datasets, we analyze benchmark suitability, data diversity, and
ethical or scalability constraints, and evaluate models across 10 domain
dimensions covering biological grounding, multi-omics alignment, fairness,
privacy, and explainability. By linking datasets, models, and evaluation
domains, LLM4Cell provides the first integrated view of language-driven
single-cell intelligence and outlines open challenges in interpretability,
standardization, and trustworthy model development.


### [40] [CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching](https://arxiv.org/abs/2510.07881)
*Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang*

#### 🧩 TL;DR
本研究提出了代码转换语音到语音基准测试CS3-Bench，发现现有多模态大语言模型在语言对齐方面存在严重缺陷，并通过链式识别和关键词高亮方法显著提升了模型的跨语言理解与生成能力。

---

#### 📘 Detailed Summary
**Motivation:** 现有多模态大语言模型虽然在单语言语音交互方面取得进展，但在语言对齐能力上存在明显不足，特别是在代码转换场景下，模型对多语言混合输入的理解和生成能力显著下降，这限制了语音交互系统在真实多语言环境中的应用。

**Method:** 提出了CS3-Bench基准测试来评估模型的语言对齐能力，并开发了数据构建和训练方法，包括使用链式识别增强理解能力，以及关键词高亮技术引导生成过程，专门针对代码转换场景优化模型的跨语言处理性能。

**Result:** 在7个主流模型上的实验显示，在知识密集型问答任务中性能相对下降高达66%，开放对话中存在不同程度的误解。提出的方法将知识准确率从25.14%提升至46.13%，开放理解率从64.5%提升至86.5%，并显著减少了第二语言的发音错误。

**Conclusion:** 该研究揭示了多模态大语言模型在语言对齐方面的系统性缺陷，提出的CS3-Bench基准和训练方法为改进跨语言语音交互系统提供了有效路径，强调了专门针对代码转换场景优化的重要性，对未来多语言AI系统的发展具有重要指导意义。

---

#### 📄 Abstract
The advancement of multimodal large language models has accelerated the
development of speech-to-speech interaction systems. While natural monolingual
interaction has been achieved, we find existing models exhibit deficiencies in
language alignment. In our proposed Code-Switching Speech-to-Speech Benchmark
(CS3-Bench), experiments on 7 mainstream models demonstrate a relative
performance drop of up to 66% in knowledge-intensive question answering and
varying degrees of misunderstanding in open-ended conversations. Starting from
a model with severe performance deterioration, we propose both data
constructions and training approaches to improve the language alignment
capabilities, specifically employing Chain of Recognition (CoR) to enhance
understanding and Keyword Highlighting (KH) to guide generation. Our approach
improves the knowledge accuracy from 25.14% to 46.13%, with open-ended
understanding rate from 64.5% to 86.5%, and significantly reduces pronunciation
errors in the secondary language. CS3-Bench is available at
https://huggingface.co/datasets/VocalNet/CS3-Bench.


### [41] [Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German Dictionaries from the 17th and 18th Centuries](https://arxiv.org/abs/2510.07931)
*Madis Jürviste, Joonatan Jakobson*

#### 🧩 TL;DR
本研究展示了大型语言模型在爱沙尼亚历史词典研究中的应用潜力，通过半自动词典信息增强、哥特体文本识别和跨源数据集构建，显著提升了小语种历史文献数字化的效率和准确性。

---

#### 📘 Detailed Summary
**Motivation:** 该研究旨在解决17-18世纪爱沙尼亚历史词典研究中面临的挑战，包括词典信息不完整、哥特体印刷文本难以识别以及缺乏统一跨源数据集的问题，这些因素限制了小语种历史语言资源的有效利用和数字化进程。

**Method:** 研究采用多模态LLM方法，包括使用Claude 3.7 Sonnet进行词典条目语义增强，基于零样本方法的哥特体文本识别技术，以及通过重叠切片扫描图像处理和双LLM协作（一个负责文本识别，另一个负责结构化输出合并）的数字化流程。

**Result:** 实验结果显示，在充足上下文条件下，Claude 3.7 Sonnet能够为81%的词目条目准确提供现代释义和对应形式；在文本识别实验中，零样本方法成功识别并结构化41%的词目条目为无错误的JSON格式输出；通过重叠切片和双LLM协作实现了Hupel 1780年语法词典的有效数字化。

**Conclusion:** 研究表明即使对于小语种，LLM在历史文献处理中具有显著的时间和经济资源节约潜力，为历史语言学研究和文化遗产数字化提供了高效的技术路径，证明了半自动化方法在小语种资源开发中的可行性和有效性。

---

#### 📄 Abstract
This article presents research conducted at the Institute of the Estonian
Language between 2022 and 2025 on the application of large language models
(LLMs) to the study of 17th and 18th century Estonian dictionaries. The authors
address three main areas: enriching historical dictionaries with modern word
forms and meanings; using vision-enabled LLMs to perform text recognition on
sources printed in Gothic script (Fraktur); and preparing for the creation of a
unified, cross-source dataset. Initial experiments with J. Gutslaff's 1648
dictionary indicate that LLMs have significant potential for semi-automatic
enrichment of dictionary information. When provided with sufficient context,
Claude 3.7 Sonnet accurately provided meanings and modern equivalents for 81%
of headword entries. In a text recognition experiment with A. T. Helle's 1732
dictionary, a zero-shot method successfully identified and structured 41% of
headword entries into error-free JSON-formatted output. For digitising the
Estonian-German dictionary section of A. W. Hupel's 1780 grammar, overlapping
tiling of scanned image files is employed, with one LLM being used for text
recognition and a second for merging the structured output. These findings
demonstrate that even for minor languages LLMs have a significant potential for
saving time and financial resources.


### [42] [Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge](https://arxiv.org/abs/2510.07993)
*Watcharapong Timklaypachara, Monrada Chiewhawan, Nopporn Lekuthai, Titipat Achakulvisut*

#### 🧩 TL;DR
本研究提出了一个面向科学图表标题生成的领域特定系统，通过结合上下文理解和作者特定风格适应，能够生成既科学准确又风格忠实于源论文的图表标题。该系统在SciCap挑战中展示了类别特定提示和风格优化策略的有效性。

---

#### 📘 Detailed Summary
**Motivation:** 科学图表标题需要同时具备准确性和风格一致性以有效传达视觉信息，但现有方法在保持作者特定写作风格方面存在不足。本研究旨在解决科学图表标题生成中上下文理解与风格适应之间的平衡问题。

**Method:** 采用两阶段流水线方法：第一阶段结合上下文过滤、基于DSPy MIPROv2和SIMBA的类别特定提示优化以及标题候选选择；第二阶段应用基于配置文件的少样本提示进行风格精炼，使用LaMP-Cap数据集进行训练和评估。

**Result:** 实验表明类别特定提示在零样本和通用优化方法中表现最佳，将ROUGE-1召回率提升+8.3%，同时限制精度损失至-2.8%和BLEU-4下降至-10.9%。基于配置文件的风格优化在BLEU分数上获得40-48%的提升，在ROUGE指标上获得25-27%的提升。

**Conclusion:** 研究证明结合上下文理解与作者特定风格适应能够生成科学准确且风格忠实的图表标题，为领域特定的自然语言生成任务提供了有效的多阶段优化框架，强调了类别特定提示和风格个性化在科学写作中的重要性。

---

#### 📄 Abstract
Scientific figure captions require both accuracy and stylistic consistency to
convey visual information. Here, we present a domain-specific caption
generation system for the 3rd SciCap Challenge that integrates figure-related
textual context with author-specific writing styles using the LaMP-Cap dataset.
Our approach uses a two-stage pipeline: Stage 1 combines context filtering,
category-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption
candidate selection; Stage 2 applies few-shot prompting with profile figures
for stylistic refinement. Our experiments demonstrate that category-specific
prompts outperform both zero-shot and general optimized approaches, improving
ROUGE-1 recall by +8.3\% while limiting precision loss to -2.8\% and BLEU-4
reduction to -10.9\%. Profile-informed stylistic refinement yields 40--48\%
gains in BLEU scores and 25--27\% in ROUGE. Overall, our system demonstrates
that combining contextual understanding with author-specific stylistic
adaptation can generate captions that are both scientifically accurate and
stylistically faithful to the source paper.


### [43] [Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks](https://arxiv.org/abs/2510.08002)
*Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, Yu Qiao, Haifeng Li*

#### 🧩 TL;DR
本文提出了MUSE，一种新颖的体验驱动自进化AI智能体框架，通过分层记忆模块实现持续学习和自我进化。该框架在长时程生产力基准测试TAC上取得了新的最先进性能，仅使用轻量级Gemini-2.5 Flash模型即可显著超越现有方法。

---

#### 📘 Detailed Summary
**Motivation:** 现有大型语言模型智能体在部署到现实世界长时程任务时存在关键限制：它们是测试时静态的，无法从经验中学习，缺乏积累知识和持续改进的能力。这种局限性阻碍了AI智能体在真实生产力任务中的有效应用和长期性能提升。

**Method:** MUSE框架引入了以分层记忆模块为核心的体验驱动自进化系统，该模块组织不同层级的经验并利用它们来规划和执行跨多个应用的长时程任务。在每个子任务执行后，智能体自主反思其轨迹，将原始轨迹转换为结构化经验并整合回记忆模块，从而超越其静态预训练参数实现持续学习和自我进化。

**Result:** 在长时程生产力基准测试TAC上，MUSE仅使用轻量级Gemini-2.5 Flash模型就取得了新的最先进性能，显著超越了现有方法。实验表明随着智能体自主积累经验，其任务完成能力持续提升，展现出强大的持续学习和自进化能力，且积累的经验具有强泛化性，能够实现新任务的零样本改进。

**Conclusion:** MUSE建立了一种能够实现现实世界生产力任务自动化的AI智能体新范式，证明了体验驱动的自进化机制对于克服静态LLM智能体局限性的有效性。该研究为构建能够从经验中持续学习和改进的AI系统提供了重要见解，推动了智能体在复杂长时程任务中的应用前景。

---

#### 📄 Abstract
Large Language Models have demonstrated remarkable capabilities across
diverse domains, yet significant challenges persist when deploying them as AI
agents for real-world long-horizon tasks. Existing LLM agents suffer from a
critical limitation: they are test-time static and cannot learn from
experience, lacking the ability to accumulate knowledge and continuously
improve on the job. To address this challenge, we propose MUSE, a novel agent
framework that introduces an experience-driven, self-evolving system centered
around a hierarchical Memory Module. MUSE organizes diverse levels of
experience and leverages them to plan and execute long-horizon tasks across
multiple applications. After each sub-task execution, the agent autonomously
reflects on its trajectory, converting the raw trajectory into structured
experience and integrating it back into the Memory Module. This mechanism
enables the agent to evolve beyond its static pretrained parameters, fostering
continuous learning and self-evolution. We evaluate MUSE on the long-horizon
productivity benchmark TAC. It achieves new SOTA performance by a significant
margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments
demonstrate that as the agent autonomously accumulates experience, it exhibits
increasingly superior task completion capabilities, as well as robust
continuous learning and self-evolution capabilities. Moreover, the accumulated
experience from MUSE exhibits strong generalization properties, enabling
zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI
agents capable of real-world productivity task automation.


### [44] [A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models](https://arxiv.org/abs/2510.08049)
*Congming Zheng, Jiachen Zhu, Zhuoying Ou, Yuxiang Chen, Kangning Zhang, Rong Shan, Zeyu Zheng, Mengyue Yang, Jianghao Lin, Yong Yu, Weinan Zhang*

#### 🧩 TL;DR
本文系统综述了过程奖励模型（PRMs）的研究进展，针对传统结果奖励模型仅评估最终答案的局限性，提出了在推理过程中进行细粒度评估和指导的完整技术框架。

---

#### 📘 Detailed Summary
**Motivation:** 尽管大语言模型展现出强大的推理能力，但传统的对齐方法主要依赖仅评估最终答案的结果奖励模型（ORMs），这忽视了推理过程的质量评估。过程奖励模型（PRMs）旨在填补这一空白，通过在步骤或轨迹层面评估和指导推理过程，实现更精细化的对齐。

**Method:** 研究提出了PRMs的完整技术闭环，包括过程数据生成方法、PRMs模型构建技术，以及测试时扩展和强化学习的应用策略。涵盖了从数据准备到模型部署的全流程技术方案。

**Result:** 该综述系统总结了PRMs在数学推理、代码生成、文本理解、多模态推理、机器人控制和智能体等多个领域的应用成果，并整理了新兴的基准测试体系，展示了该技术在不同任务中的广泛适用性。

**Conclusion:** 研究明确了PRMs的设计空间，揭示了当前面临的开放挑战，为未来研究提供了方向性指导，推动推理对齐向更细粒度、更鲁棒的方向发展，有望显著提升AI系统的推理可靠性和可解释性。

---

#### 📄 Abstract
Although Large Language Models (LLMs) exhibit advanced reasoning ability,
conventional alignment remains largely dominated by outcome reward models
(ORMs) that judge only final answers. Process Reward Models(PRMs) address this
gap by evaluating and guiding reasoning at the step or trajectory level. This
survey provides a systematic overview of PRMs through the full loop: how to
generate process data, build PRMs, and use PRMs for test-time scaling and
reinforcement learning. We summarize applications across math, code, text,
multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our
goal is to clarify design spaces, reveal open challenges, and guide future
research toward fine-grained, robust reasoning alignment.


### [45] [DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations](https://arxiv.org/abs/2510.08152)
*Elena Khasanova, Harsh Saini, Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN*

#### 🧩 TL;DR
本文提出了DACIP-RC方法，一种基于阅读理解持续预训练技术，用于增强小型LLMs在业务对话任务中的领域适应性，显著提升零样本泛化能力。

---

#### 📘 Detailed Summary
**Motivation:** 大型语言模型的高推理成本使其部署不切实际，而小型LLMs缺乏跨领域的零样本指令跟随能力，传统微调方法会引发灾难性遗忘问题，限制了模型对动态用户需求的适应性。

**Method:** DACIP-RC通过阅读理解在对话转录本上生成多样化的任务指令和响应，不同于依赖下一词预测的传统预训练方法，该方法能够实现更好的指令泛化能力。

**Result:** 实证评估表明，DACIP-RC在广泛的业务对话任务中显著提升了零样本泛化能力，包括会议摘要、行动项生成和通话目的识别等任务。

**Conclusion:** 这是首个将指令预训练应用于业务对话数据的工作，为行业如何利用专有数据集进行领域适应提供了重要见解，展示了持续预训练在提升小型模型适应性方面的潜力。

---

#### 📄 Abstract
The rapid advancements in Large Language Models (LLMs) have enabled their
adoption in real-world industrial scenarios for various natural language
processing tasks. However, the high inference cost of large-scale LLMs makes
their deployment impractical, necessitating the use of smaller models. Despite
their efficiency, smaller LLMs lack robust zero-shot instruction-following
capabilities across diverse domains, limiting their adaptability to dynamic
user requirements. Traditional fine-tuning approaches exacerbate this issue by
inducing catastrophic forgetting, reducing the model's generalization ability
for unseen tasks. In this paper, we propose Domain Adaptive Continual
Instruction Pre-Training via Reading Comprehension (DACIP-RC), a continual
pre-training technique that enhances smaller LLMs' domain adaptability for
business conversational tasks. Unlike conventional pre-training approaches that
rely on next-token prediction, DACIP-RC generates diverse task instructions and
responses via reading comprehension on conversation transcripts, enabling
better instruction generalization. Our empirical evaluations demonstrate that
DACIP-RC significantly improves zero-shot generalization across a wide range of
business conversational tasks, including meeting summarization, action item
generation, and call purpose identification. To the best of our knowledge, this
is the first work to apply instruction pre-training on business conversational
data, providing insights into how industries can leverage proprietary datasets
for domain adaptation.


### [46] [ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code](https://arxiv.org/abs/2510.08163)
*Jian Xie, Zhendong Chu, Aoxiao Zhong, Kai Zhang, Mingzhe Han, Xin Fang, Jialie Shen, Qingsong Wen*

#### 🧩 TL;DR
本文提出了ARM2，一个通过强化学习框架增强长度感知优化的统一模型，能够自适应平衡多种格式下的推理性能与效率。该模型在保持与传统推理模型相当性能的同时，平均减少70%以上的token使用量。

---

#### 📘 Detailed Summary
**Motivation:** 大型推理模型（LRMs）普遍存在“过度思考”问题，在简单任务上生成不必要的冗长推理。现有缓解策略如长度惩罚或路由机制通常是启发式且任务特定的，缺乏自适应推理的通用框架。

**Method:** ARM2采用强化学习框架增强长度感知优化，统一模型自适应平衡推理性能与效率。该模型不仅整合视觉理解扩展多模态应用，还集成可执行代码到推理过程中，相比长链思维显著降低token成本。

**Result:** 实验表明ARM2在性能上与使用GRPO训练的传统推理模型相当，同时平均减少超过70%的token使用量。广泛分析验证了ARM2的有效性及其设计合理性。

**Conclusion:** ARM2为自适应推理提供了通用框架，通过集成多模态理解和可执行代码，在保持性能的同时显著提升效率。该研究为平衡模型推理能力与计算成本提供了有效解决方案。

---

#### 📄 Abstract
Large Reasoning Models (LRMs) often suffer from the ``over-thinking''
problem, generating unnecessarily long reasoning on simple tasks. Some
strategies have been proposed to mitigate this issue, such as length penalties
or routing mechanisms, but they are typically heuristic and task-specific,
lacking a general framework for adaptive reasoning. In this paper, we present
ARM2, a unified model that adaptively balances reasoning performance and
efficiency across multiple formats through a reinforcement learning framework
augmented with length-aware optimization. Beyond conventional natural language
inference, ARM2 integrates vision understanding, extending its applicability to
multimodal. Moreover, ARM2 integrates executable code into reasoning, enabling
substantial reductions in token cost while preserving task performance compared
to long CoT. Experiments demonstrate that ARM2 achieves performance on par with
traditional reasoning models trained with GRPO, while reducing token usage by
over 70% on average. We further conduct extensive analyses to validate the
effectiveness of ARM2 and the soundness of its design.


### [47] [Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT](https://arxiv.org/abs/2510.08404)
*Noor Ul Zain, Mohsin Raza, Ahsan Adeel*

#### 🧩 TL;DR
本研究提出了一种名为Co⁴的轻量级语言模型，仅包含单层、两个注意力头和800万参数，在近似O(N)计算复杂度下超越了传统O(N²)复杂度的GPT-2和GPT-BERT模型，展示了显著更高的训练效率和样本效率。

---

#### 📘 Detailed Summary
**Motivation:** 当前主流深度学习范式依赖于深层网络架构和O(N²)计算复杂度，这导致了巨大的计算开销和训练成本，本研究旨在探索更高效的轻量级架构来挑战这一传统范式。

**Method:** 采用Co⁴机器架构，仅包含单层Transformer、两个注意力头和800万参数，实现了近似O(N)的计算复杂度，相比传统O(N²)复杂度的模型显著降低了计算需求。

**Result:** 在BabyLM挑战基准测试中，Co⁴仅训练两个周期就超越了训练十个周期的GPT-2和GPT-BERT模型，在1000万token上实现了数量级更高的训练效率，在SuperGLUE任务的零样本和微调评估中，Co⁴在多数指标上优于两个基线模型。

**Conclusion:** 这些结果表明需要重新思考当前主流的深度学习范式和相关的扩展定律，轻量级架构在保持性能的同时可以大幅提升计算效率，为高效预训练模型的发展提供了新的方向。

---

#### 📄 Abstract
We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two
heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$
is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2
(124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two
epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude
greater training efficiency on 10M tokens, demonstrating highly sample
efficient pretraining. Using the BabyLM challenge evaluation pipeline across
complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning
performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out
of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out
of 7 metrics in both cases. These results suggest the need to rethink
prevailing deep learning paradigms and associated scaling laws.


### [48] [ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping](https://arxiv.org/abs/2510.08457)
*Shuang Chen, Yue Guo, Yimeng Ye, Shijue Huang, Wenbo Hu, Haoxi Li, Manyuan Zhang, Jiayu Chen, Song Guo, Nanyun Peng*

#### 🧩 TL;DR
本文提出ARES框架，通过自适应推理机制解决了多模态大推理模型在简单问题上过度推理、在复杂问题上探索不足的问题，实现了基于任务难度的动态探索资源分配。

---

#### 📘 Detailed Summary
**Motivation:** 当前多模态大推理模型存在推理资源分配不平衡的问题：在简单问题上产生不必要的冗长推理轨迹，而在复杂问题上却探索不足导致错失解决方案。这种推理效率的低下限制了模型的实际应用价值。

**Method:** ARES框架采用两阶段训练流程：自适应冷启动阶段通过难度比例配对的推理轨迹数据赋予模型初步难度感知能力；第二阶段提出自适应熵策略优化，使用高窗口熵令牌作为探索触发器，结合分层熵奖励和动态KL控制来决定探索时机和程度。

**Result:** 大量实验表明ARES在多样化数学、逻辑和多模态基准测试中实现了优越的性能和推理效率，同时在显著降低推理成本的情况下缩小了与领先商业系统的差距。

**Conclusion:** 该研究证明了基于任务难度的自适应推理机制的有效性，窗口熵作为推理关键时刻的可靠指标，为构建更高效的推理系统提供了新的技术路径，具有重要的实际应用价值。

---

#### 📄 Abstract
Recent advances in multimodal large reasoning models (MLRMs) have
substantially improved their ability to solve complex textual and visual tasks.
However, these models tend to overthink on simple problems, producing
unnecessarily lengthy reasoning traces, while under-exploring on challenging
ones, leading to missed solutions. To address this imbalance, we propose ARES,
a unified open-source framework for adaptive reasoning that dynamically
allocates exploration effort based on task difficulty. Our approach is
motivated by two key empirical findings: (i) while single-token entropy is
noisy, high window-entropy (HWE) tokens (token-level entropies averaged under a
sliding window) can reliably capture reasoning-critical moments; and (ii)
reducing HWE usage benefits easy problems, while increasing it is essential for
solving hard ones. Building on these insights, ARES introduces a two-stage
training pipeline. In the Adaptive Cold-Start stage, we curate multimodal and
textual data paired with reasoning traces of length proportional to problem
difficulty, equipping the model with initial difficulty awareness. In the
second stage, we develop Adaptive Entropy Policy Optimization (AEPO), which
uses HWE tokens as exploration triggers to decide when to explore, and a
hierarchical entropy reward with dynamic KL control to decide how much to
explore. Extensive experiments demonstrate that ARES achieves superior
performance and reasoning efficiency across diverse mathematical, logical, and
multimodal benchmarks, while closing the gap to leading commercial systems
under significantly lower inference costs.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [49] [TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering](https://arxiv.org/abs/2510.07432)
*Penghang Liu, Elizabeth Fons, Svitlana Vyetrenko, Daniel Borrajo, Vamsi Potluru, Manuela Veloso*

#### 🧩 TL;DR
本文提出TS-Agent，一种时间序列推理智能体，通过将LLMs的推理能力与时间序列分析工具的专业功能相结合，有效解决了LLMs在时间序列推理任务中的幻觉和知识泄漏问题。

---

#### 📘 Detailed Summary
**Motivation:** 大型语言模型在推理和问题解决方面表现出强大能力，但在时间序列推理任务中仍然存在困难，输出经常受到幻觉或知识泄漏的影响，需要专门的方法来克服这些局限性。

**Method:** TS-Agent采用分工策略，让LLMs专注于证据收集和逐步推理合成结论，同时将统计和结构信息提取委托给时间序列分析工具，通过原子操作符与原始数值序列交互，记录显式证据日志，并在自我批评和最终质量门控的指导下迭代优化推理过程。

**Result:** 实验评估显示，TS-Agent在理解基准测试中达到与最先进LLMs相当的性能，在推理任务中实现显著改进，特别是在现有模型依赖记忆且在零样本设置中失败的场景下表现优异。

**Conclusion:** 该研究证明了将LLMs的推理优势与领域专用工具相结合的有效性，避免了多模态对齐训练，保持了时间序列的原始形式，确保了可解释性和可验证性，为时间序列分析提供了新的范式。

---

#### 📄 Abstract
Large language models (LLMs) have shown strong abilities in reasoning and
problem solving, but recent studies reveal that they still struggle with time
series reasoning tasks, where outputs are often affected by hallucination or
knowledge leakage. In this work we propose TS-Agent, a time series reasoning
agent that leverages LLMs strictly for what they excel at, i.e., gathering
evidence and synthesizing it into conclusions through step-by-step reasoning,
while delegating the extraction of statistical and structural information to
time series analytical tools. Instead of mapping time series into text tokens,
images, or embeddings, our agent interacts with raw numeric sequences through
atomic operators, records outputs in an explicit evidence log, and iteratively
refines its reasoning under the guidance of a self-critic and a final quality
gate. This design avoids multi-modal alignment training, preserves the native
form of time series, ensures interpretability and verifiability, and mitigates
knowledge leakage or hallucination. Empirically, we evaluate the agent on
established benchmarks. Our experiments show that TS-Agent achieves performance
comparable to state-of-the-art LLMs on understanding benchmarks, and delivers
significant improvements on reasoning tasks, where existing models often rely
on memorization and fail in zero-shot settings.


### [50] [Evaluation of LLMs for Process Model Analysis and Optimization](https://arxiv.org/abs/2510.07489)
*Akhil Kumar, Jianliang Leon Zhao, Om Dobariya*

#### 🧩 TL;DR
本研究评估了多个大型语言模型在理解BPMN业务流程模型、检测语法逻辑错误以及通过自然语言界面进行深度推理的能力，发现未经训练的LLM在零样本设置下能够有效理解流程模型并智能回答多层次的查询问题。

---

#### 📘 Detailed Summary
**Motivation:** 该研究旨在探索大型语言模型在业务流程建模领域的应用潜力，特别是评估LLMs能否通过交互式对话方式理解流程模型、检测其中的语法和逻辑错误，并通过自然语言界面进行深度推理，填补了LLM在业务流程分析领域系统性评估的研究空白。

**Method:** 研究采用实证分析方法，测试了多个LLM（包括ChatGPT模型o3）在零样本设置下处理BPMN流程模型图像的能力，通过自然语言对话界面评估模型在语法、逻辑和语义层面的理解深度，并分析了不同模型在准确性和有效性方面的性能差异。

**Result:** 实验结果表明，未经训练的LLM在零样本设置下能够有效理解BPMN流程模型图像，并在语法、逻辑和语义层面智能地回答相关查询，不同LLM在准确性和有效性方面表现存在差异，同时研究发现LLMs在流程分析和优化任务中展现出类似人类的推理过程和拟人化特性。

**Conclusion:** 该研究证实了LLMs可以作为业务流程设计者和用户的有价值助手，在流程分析和优化任务中展现出深度推理能力，同时揭示了LLMs在业务流程建模领域具有重要的应用前景，为未来开发更智能的业务流程分析工具提供了理论依据和实践指导。

---

#### 📄 Abstract
In this paper, we report our experience with several LLMs for their ability
to understand a process model in an interactive, conversational style, find
syntactical and logical errors in it, and reason with it in depth through a
natural language (NL) interface. Our findings show that a vanilla, untrained
LLM like ChatGPT (model o3) in a zero-shot setting is effective in
understanding BPMN process models from images and answering queries about them
intelligently at syntactic, logic, and semantic levels of depth. Further,
different LLMs vary in performance in terms of their accuracy and
effectiveness. Nevertheless, our empirical analysis shows that LLMs can play a
valuable role as assistants for business process designers and users. We also
study the LLM's "thought process" and ability to perform deeper reasoning in
the context of process analysis and optimization. We find that the LLMs seem to
exhibit anthropomorphic properties.


### [51] [An Evaluation Study of Hybrid Methods for Multilingual PII Detection](https://arxiv.org/abs/2510.07551)
*Harshit Rajgarhia, Suryam Gupta, Asif Shaik, Gulipalli Praveen Kumar, Y Santhoshraj, Sanka Nithya Tanvy Nishitha, Abhishek Mukherji*

#### 🧩 TL;DR
本文提出RECAP框架，一种结合确定性正则表达式与上下文感知大语言模型的混合方法，用于在13种低资源语言环境中实现可扩展的个人身份信息检测，无需重新训练即可支持300多种实体类型。

---

#### 📘 Detailed Summary
**Motivation:** 当前个人身份信息检测在低资源语言中面临挑战，主要由于语言多样性及标注数据有限，导致隐私合规性难以保障，需要开发能够适应多语言环境且无需大量训练数据的解决方案。

**Method:** RECAP采用模块化设计，结合确定性正则表达式与上下文感知大语言模型，构建三阶段精炼流水线进行消歧和过滤，支持超过300种实体类型检测而无需重新训练模型。

**Result:** 在nervaluate基准测试中，该系统在加权F1分数上比微调NER模型高出82%，比零样本LLMs高出17%，在13种低资源语言环境中均表现出优越性能。

**Conclusion:** 该研究为合规性应用提供了可扩展且适应性强的个人身份信息检测解决方案，展示了混合方法在低资源语言环境中的有效性，为多语言隐私保护技术发展提供了新方向。

---

#### 📄 Abstract
The detection of Personally Identifiable Information (PII) is critical for
privacy compliance but remains challenging in low-resource languages due to
linguistic diversity and limited annotated data. We present RECAP, a hybrid
framework that combines deterministic regular expressions with context-aware
large language models (LLMs) for scalable PII detection across 13 low-resource
locales. RECAP's modular design supports over 300 entity types without
retraining, using a three-phase refinement pipeline for disambiguation and
filtering. Benchmarked with nervaluate, our system outperforms fine-tuned NER
models by 82% and zero-shot LLMs by 17% in weighted F1-score. This work offers
a scalable and adaptable solution for efficient PII detection in
compliance-focused applications.


### [52] [Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models](https://arxiv.org/abs/2510.07632)
*Yinglun Zhu, Jiancheng Zhang, Fuzhi Tang*

#### 🧩 TL;DR
本研究揭示了现有评估指标系统性低估了AI模型在组合推理方面的能力，提出了组匹配分数和测试时匹配算法，显著提升了模型在组合推理基准上的性能表现。

---

#### 📘 Detailed Summary
**Motivation:** 前沿AI模型在组合推理方面表现不佳，现有评估指标系统性低估了模型的实际能力，需要更准确的评估方法和性能提升技术来揭示模型的真实潜力。

**Method:** 提出了组匹配分数以更好地利用组结构揭示隐藏能力，并开发了测试时匹配算法，这是一种无需外部监督的迭代自改进方法，通过测试时优化提升模型性能。

**Result:** 组匹配分数揭示了对比视觉语言模型和多模态大语言模型的显著隐藏能力，测试时匹配使SigLIP-B16超越GPT-4.1在MMVP-VLM上创下新纪录，并在Winoground上首次超越估计的人类性能，在16个数据集变体上实现一致改进。

**Conclusion:** 现有评估指标存在系统性偏差，测试时优化是提升模型组合推理能力的有效途径，该方法在无组结构的数据集上同样有效，为组合推理研究提供了新的评估和改进框架。

---

#### 📄 Abstract
Frontier AI models have achieved remarkable progress, yet recent studies
suggest they struggle with compositional reasoning, often performing at or
below random chance on established benchmarks. We revisit this problem and show
that widely used evaluation metrics systematically underestimate model
capability. To address this, we introduce a group matching score that better
exploits group structure and reveals substantial hidden capability in both
contrastive vision-language models (VLMs) and multimodal large language models
(MLLMs). Moreover, simply overfitting to the induced group matchings at test
time transfers this hidden capability into higher scores under standard
evaluation metrics, closing much of the reported gap. This adjustment enables
SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first
result surpassing estimated human performance on Winoground.
  Building on this insight, we propose Test-Time Matching (TTM), an iterative,
self-improving algorithm that further bootstraps model performance without any
external supervision. TTM delivers additional, non-trivial improvements: for
example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a
new state of the art. Importantly, TTM remains broadly effective even on
benchmarks without metric-induced effects or group structures, achieving
relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16
dataset variants spanning diverse setups, our experiments demonstrate that TTM
consistently improves model performance and advances the frontier of
compositional reasoning.


### [53] [Multimodal Safety Evaluation in Generative Agent Social Simulations](https://arxiv.org/abs/2510.07709)
*Alhim Vera, Karen Sanchez, Carlos Hinojosa, Haidar Bin Hamid, Donghoon Kim, Bernard Ghanem*

#### 🧩 TL;DR
本研究提出了一个可复现的模拟框架来评估多模态环境中生成式智能体的安全性和可信度，发现当前智能体在检测多模态矛盾方面表现良好，但在全局安全对齐方面存在显著局限性，特别是在面对误导性视觉信息时表现出过度信任倾向。

---

#### 📘 Detailed Summary
**Motivation:** 尽管大型语言模型和视觉语言模型使智能体能够在丰富环境中自主行动并追求目标，但它们在跨模态安全性、连贯性和可信度推理方面的能力仍然有限，本研究旨在解决这一研究空白。

**Method:** 研究引入了一个可复现的模拟评估框架，配备分层记忆、动态规划、多模态感知能力，并采用SocialMetrics行为指标套件来量化计划修订、不安全到安全转换以及网络信息扩散。

**Result:** 实验结果显示智能体在多模态矛盾检测方面表现良好，但在全局安全对齐方面成功率仅为55%；三种模型（Claude、GPT-4o mini、Qwen-VL）的不安全到安全转换率分别为75%、55%和58%；在误导性视觉信息配对时，45%的不安全行为被接受。

**Conclusion:** 研究揭示了当前智能体架构在跨模态安全推理方面的关键局限性，特别是对视觉信息的过度信任问题，为研究多模态安全性、连贯性和社交动态提供了可复现平台。

---

#### 📄 Abstract
Can generative agents be trusted in multimodal environments? Despite advances
in large language and vision-language models that enable agents to act
autonomously and pursue goals in rich settings, their ability to reason about
safety, coherence, and trust across modalities remains limited. We introduce a
reproducible simulation framework for evaluating agents along three dimensions:
(1) safety improvement over time, including iterative plan revisions in
text-visual scenarios; (2) detection of unsafe activities across multiple
categories of social situations; and (3) social dynamics, measured as
interaction counts and acceptance ratios of social exchanges. Agents are
equipped with layered memory, dynamic planning, multimodal perception, and are
instrumented with SocialMetrics, a suite of behavioral and structural metrics
that quantifies plan revisions, unsafe-to-safe conversions, and information
diffusion across networks. Experiments show that while agents can detect direct
multimodal contradictions, they often fail to align local revisions with global
safety, reaching only a 55 percent success rate in correcting unsafe plans.
Across eight simulation runs with three models - Claude, GPT-4o mini, and
Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75,
55, and 58 percent, respectively. Overall performance ranged from 20 percent in
multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such
as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted
when paired with misleading visuals, showing a strong tendency to overtrust
images. These findings expose critical limitations in current architectures and
provide a reproducible platform for studying multimodal safety, coherence, and
social dynamics.


### [54] [FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning](https://arxiv.org/abs/2510.07852)
*Shuangyan Deng, Haizhou Peng, Jiachen Xu, Rui Mao, Ciprian Doru Giurcăneanu, Jiamou Liu*

#### 🧩 TL;DR
本文提出了FinMR，一个高质量、知识密集的多模态数据集，专门用于评估专业分析师级别的金融推理能力，填补了金融领域MLLM评估的空白。

---

#### 📘 Detailed Summary
**Motivation:** 当前多模态大语言模型在金融等专业领域的严格评估受到限制，缺乏具有专业知识强度、详细标注和高级推理复杂度的数据集，阻碍了模型在专业金融分析能力方面的准确评估。

**Method:** 研究团队构建了FinMR数据集，包含超过3,200个精心策划和专家标注的问答对，涵盖15个不同金融主题，整合了复杂数学推理、高级金融知识和多类型图像的细微视觉解释任务。

**Result:** 通过对领先的闭源和开源MLLM进行全面基准测试，揭示了这些模型与专业金融分析师之间的显著性能差距，特别是在精确图像分析、复杂金融公式准确应用和深度上下文金融理解等关键领域。

**Conclusion:** FinMR通过提供丰富的视觉内容和详尽的解释性标注，成为评估和推进多模态金融推理向专业分析师水平发展的重要基准工具，为模型改进指明了关键方向。

---

#### 📄 Abstract
Multimodal Large Language Models (MLLMs) have made substantial progress in
recent years. However, their rigorous evaluation within specialized domains
like finance is hindered by the absence of datasets characterized by
professional-level knowledge intensity, detailed annotations, and advanced
reasoning complexity. To address this critical gap, we introduce FinMR, a
high-quality, knowledge-intensive multimodal dataset explicitly designed to
evaluate expert-level financial reasoning capabilities at a professional
analyst's standard. FinMR comprises over 3,200 meticulously curated and
expertly annotated question-answer pairs across 15 diverse financial topics,
ensuring broad domain diversity and integrating sophisticated mathematical
reasoning, advanced financial knowledge, and nuanced visual interpretation
tasks across multiple image types. Through comprehensive benchmarking with
leading closed-source and open-source MLLMs, we highlight significant
performance disparities between these models and professional financial
analysts, uncovering key areas for model advancement, such as precise image
analysis, accurate application of complex financial formulas, and deeper
contextual financial understanding. By providing richly varied visual content
and thorough explanatory annotations, FinMR establishes itself as an essential
benchmark tool for assessing and advancing multimodal financial reasoning
toward professional analyst-level competence.


### [55] [Augur: Modeling Covariate Causal Associations in Time Series via Large Language Models](https://arxiv.org/abs/2510.07858)
*Zhiqing Cui, Binwu Wang, Qingxiang Liu, Yeqiang Wang, Zhengyang Zhou, Yuxuan Liang, Yang Wang*

#### 🧩 TL;DR
本文提出了Augur框架，这是首个完全由大语言模型驱动的时序预测方法，通过利用LLM的因果推理能力发现协变量间的有向因果关联，在保持预测准确性的同时提供可解释的推理过程。

---

#### 📘 Detailed Summary
**Motivation:** 现有基于大语言模型的时序预测方法存在显著局限性，包括在模型架构中的边缘化角色、依赖粗糙的统计文本提示以及缺乏可解释性，这些问题限制了LLM在时序预测中的潜力发挥。

**Method:** Augur采用两阶段师生架构，首先由强大的教师LLM通过启发式搜索和成对因果检验从时序数据中推断有向因果图，然后由轻量级学生代理优化该图并基于高置信度因果关联进行微调，这些关联被编码为丰富的文本提示用于预测任务。

**Result:** 在真实世界数据集上的广泛实验表明，Augur在25个基线方法中取得了具有竞争力的性能表现，并展现出强大的零样本泛化能力，验证了该框架的有效性和鲁棒性。

**Conclusion:** 该研究证明了LLM因果推理在时序预测中的有效性，不仅提升了预测准确性，还提供了透明可追溯的变量交互推理，为可解释AI在时序分析领域的应用开辟了新方向。

---

#### 📄 Abstract
Large language models (LLM) have emerged as a promising avenue for time
series forecasting, offering the potential to integrate multimodal data.
However, existing LLM-based approaches face notable limitations-such as
marginalized role in model architectures, reliance on coarse statistical text
prompts, and lack of interpretability. In this work, we introduce Augur, a
fully LLM driven time series forecasting framework that exploits LLM causal
reasoning to discover and use directed causal associations among covariates.
Augur uses a two stage teacher student architecture where a powerful teacher
LLM infers a directed causal graph from time series using heuristic search
together with pairwise causality testing. A lightweight student agent then
refines the graph and fine tune on high confidence causal associations that are
encoded as rich textual prompts to perform forecasting. This design improves
predictive accuracy while yielding transparent, traceable reasoning about
variable interactions. Extensive experiments on real-world datasets with 25
baselines demonstrate that Augur achieves competitive performance and robust
zero-shot generalization.


### [56] [Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness](https://arxiv.org/abs/2510.08238)
*Jiyang Qiu, Xinbei Ma, Yunqing Xu, Zhuosheng Zhang, Hai Zhao*

#### 🧩 TL;DR
本文提出Chain-of-Trigger Backdoor (CoTri)，一种针对LLM智能体的多步后门攻击方法，能够实现长时程的智能体控制，同时意外地提升了智能体在良性任务上的性能和抗干扰能力。

---

#### 📘 Detailed Summary
**Motivation:** 随着大语言模型智能体在现实应用中的快速部署，其可信赖性问题日益突出，传统后门攻击仅限于单步控制，无法应对智能体在复杂环境中执行多步决策的安全需求。

**Method:** CoTri采用有序触发序列机制，从初始触发开始，后续触发从环境中动态提取，通过多步操作将智能体从预期任务中引导偏离，该方法能够建模环境的随机性特征。

**Result:** 实验结果显示CoTri实现了接近完美的攻击成功率，同时保持接近零的误触发率，由于训练数据对环境随机性的建模，该方法反而提升了智能体在良性任务上的性能和对环境干扰的鲁棒性。

**Conclusion:** CoTri实现了对智能体的稳定多步控制，同时增强了其固有鲁棒性和任务能力，这使得攻击更加隐蔽并带来潜在安全风险，研究还验证了该方法在视觉语言模型上的可扩展性。

---

#### 📄 Abstract
The rapid deployment of large language model (LLM)-based agents in real-world
applications has raised serious concerns about their trustworthiness. In this
work, we reveal the security and robustness vulnerabilities of these agents
through backdoor attacks. Distinct from traditional backdoors limited to
single-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a
multi-step backdoor attack designed for long-horizon agentic control. CoTri
relies on an ordered sequence. It starts with an initial trigger, and
subsequent ones are drawn from the environment, allowing multi-step
manipulation that diverts the agent from its intended task. Experimental
results show that CoTri achieves a near-perfect attack success rate (ASR) while
maintaining a near-zero false trigger rate (FTR). Due to training data modeling
the stochastic nature of the environment, the implantation of CoTri
paradoxically enhances the agent's performance on benign tasks and even
improves its robustness against environmental distractions. We further validate
CoTri on vision-language models (VLMs), confirming its scalability to
multimodal agents. Our work highlights that CoTri achieves stable, multi-step
control within agents, improving their inherent robustness and task
capabilities, which ultimately makes the attack more stealthy and raises
potential safty risks.


### [57] [Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling](https://arxiv.org/abs/2510.08470)
*Bianca-Mihaela Ganescu, Suchir Salhan, Andrew Caines, Paula Buttery*

#### 🧩 TL;DR
本研究提出了一种轻量级解码器架构，通过动态门控机制实现语言和视觉线索的自适应融合，在认知合理的数据量约束下实现了竞争性多模态性能。该方法在多个基准测试中表现优异，且动态门控机制能够无监督地发现可解释的模式。

---

#### 📘 Detailed Summary
**Motivation:** 该研究旨在解决在认知合理数据量约束下训练视觉语言模型的挑战，特别是在BabyLM Challenge 2025视觉赛道中，需要重新思考模型如何有效整合多模态信息。研究关注如何在有限数据条件下最大化视觉信息的效用，同时避免传统方法中的信息瓶颈问题。

**Method:** 提出轻量级解码器架构，包含三个关键技术：基于token的动态门控机制用于语言和视觉线索的自适应融合；特征调制和通道注意力机制以最大化有限视觉信息的效用；辅助对比学习目标用于视觉基础。这些技术共同作用，在严格的数据约束下实现高效的多模态学习。

**Result:** 在五个基准测试（BLiMP、BLiMP Supplement、EWoK、Winoground和VQA）上的评估显示，该方法在多模态基线模型中表现出竞争性或更优的性能。动态门控机制在没有显式监督的情况下发现了可解释的模式：对内容词偏好视觉线索，对功能词偏好语言线索。

**Conclusion:** 研究确立了动态门控作为高效多模态学习的强大工具，即使在严格约束下也能提供可解释性和性能。同时识别了挑战约束中的局限性，如全局图像嵌入造成的信息瓶颈和数据集分割导致的训练不稳定性，为未来研究提供了重要方向。

---

#### 📄 Abstract
Training vision-language models on cognitively-plausible amounts of data
requires rethinking how models integrate multimodal information. Within the
constraints of the Vision track for the BabyLM Challenge 2025, we propose a
lightweight decoder-based architecture with (1) token-wise dynamic gating for
adaptive fusion of linguistic and visual cues, (2) feature modulation and
channel attention to maximise the utility of limited visual information and (3)
auxiliary contrastive objectives for visual grounding. Evaluation on five
benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows
competitive or superior performance to multimodal baselines. More notably, our
dynamic gate discovers interpretable patterns without explicit supervision,
favouring visual cues for content words and linguistic cues for function words.
While we identify limitations in the Challenge constraints, such as the
information bottleneck created by global image embeddings and training
instability from the dataset split, our findings establish dynamic gating as a
powerful tool for efficient multimodal learning, offering both interpretability
and performance even under severe constraints.


### [58] [How to Teach Large Multimodal Models New Skills](https://arxiv.org/abs/2510.08564)
*Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem*

#### 🧩 TL;DR
本研究提出了两种简单有效的参数高效微调方法，通过仅更新自注意力投影层或仅更新MLP Gate&Up层同时冻结Down投影层，解决了大型多模态模型在顺序微调中的灾难性遗忘问题，在保持原有能力的同时有效学习新技能。

---

#### 📘 Detailed Summary
**Motivation:** 本研究旨在解决大型多模态模型在顺序学习新技能时面临的灾难性遗忘问题，即如何在教授模型新能力的同时不损害其原有的通用能力，这是持续学习领域的关键挑战。

**Method:** 研究提出了两种参数高效微调方法：一是仅更新自注意力投影层，二是仅更新MLP Gate&Up层同时冻结Down投影层，这些方法通过限制参数更新范围来减少输出分布的漂移。

**Result:** 实验在三个模型系列上进行，涵盖五个目标技能和八个保留基准测试，结果显示所提方法能够实现强大的目标技能增益，同时显著保留原有性能，并通过计数偏差探针验证了输出分布漂移与遗忘之间的相关性。

**Conclusion:** 研究表明通过选择性更新特定网络组件可以有效缓解灾难性遗忘，为大型多模态模型的持续学习提供了简单而稳健的解决方案，并揭示了输出分布漂移作为遗忘的重要指标。

---

#### 📄 Abstract
How can we teach large multimodal models (LMMs) new skills without erasing
prior abilities? We study sequential fine-tuning on five target skills while
monitoring general ability on eight held-out benchmarks across three model
families. We observe that apparent "forgetting" on held-out tasks after narrow
fine-tuning can partly recover at later stages. We trace this behavior to a
measurable shift in the output token distribution, manifested through a simple
counting-bias probe that co-varies with forgetting. Guided by this picture, we
identify two simple, robust tuning recipes that learn strongly while limiting
drift: (i) updating only the self-attention projection layers, and (ii)
updating only the MLP Gate&Up while freezing the Down projection. Across models
and tasks, these choices deliver strong target gains while largely preserving
held-out performance. Code is available at
https://github.com/jessemelpolio/LMM_CL
