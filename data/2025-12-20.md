<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 16]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song, Jinzhou Li, Rao Fu, Devin Murphy, Kaichen Zhou, Rishi Shiv, Yaqi Li, Haoyu Xiong, Crystal Elaine Owens, Yilun Du, Yiyue Luo, Xianyi Cheng, Antonio Torralba, Wojciech Matusik, Paul Pu Liang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†OpenTouchï¼Œé¦–ä¸ªé‡å¤–ç¯å¢ƒä¸‹çš„ç¬¬ä¸€äººç§°å…¨æ‰‹è§¦è§‰æ•°æ®é›†ï¼ŒåŒ…å«5.1å°æ—¶çš„åŒæ­¥è§†é¢‘-è§¦è§‰-å§¿æ€æ•°æ®åŠ2900ä¸ªå¸¦è¯¦ç»†æ–‡æœ¬æ ‡æ³¨çš„å‰ªè¾‘ç‰‡æ®µï¼Œå¹¶å»ºç«‹äº†æ£€ç´¢å’Œåˆ†ç±»åŸºå‡†ï¼Œæ¢ç´¢è§¦è§‰å¦‚ä½•å¢å¼ºæ„ŸçŸ¥ä¸åŠ¨ä½œçš„å…³è”ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** äººç±»æ‰‹éƒ¨æ˜¯ä¸ç‰©ç†ä¸–ç•Œäº¤äº’çš„ä¸»è¦ç•Œé¢ï¼Œä½†ç°æœ‰ç¬¬ä¸€äººç§°æ„ŸçŸ¥ç³»ç»Ÿå¾ˆå°‘èƒ½å‡†ç¡®è¯†åˆ«æ¥è§¦çš„æ—¶é—´ã€ä½ç½®å’ŒåŠ›åº¦ï¼Œç¼ºä¹é²æ£’çš„ç©¿æˆ´å¼è§¦è§‰ä¼ æ„Ÿå™¨ï¼Œä¸”æ²¡æœ‰é‡å¤–ç¯å¢ƒä¸‹çš„è§†é¢‘-å…¨æ‰‹è§¦è§‰å¯¹é½æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†è§†è§‰æ„ŸçŸ¥ä¸ç‰©ç†äº¤äº’çš„èåˆç ”ç©¶ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†OpenTouchæ•°æ®é›†ï¼ŒåŒ…å«5.1å°æ—¶åŒæ­¥é‡‡é›†çš„è§†é¢‘ã€è§¦è§‰å’Œæ‰‹éƒ¨å§¿æ€æ•°æ®ï¼Œå¹¶ç²¾å¿ƒæ ‡æ³¨äº†2900ä¸ªå‰ªè¾‘ç‰‡æ®µåŠå…¶è¯¦ç»†æ–‡æœ¬æè¿°ï¼›åŸºäºè¯¥æ•°æ®é›†å»ºç«‹äº†æ£€ç´¢å’Œåˆ†ç±»åŸºå‡†ä»»åŠ¡ï¼Œç”¨äºæ¢ç©¶è§¦è§‰ä¿¡å·å¦‚ä½•å¢å¼ºæ„ŸçŸ¥ä¸åŠ¨ä½œçš„å…³è”æ€§ã€‚

**Result:** å®éªŒè¡¨æ˜è§¦è§‰ä¿¡å·ä¸ºæŠ“æ¡ç†è§£æä¾›äº†ç´§å‡‘è€Œå¼ºå¤§çš„çº¿ç´¢ï¼Œèƒ½å¤Ÿæ˜¾è‘—å¢å¼ºè·¨æ¨¡æ€å¯¹é½æ•ˆæœï¼Œå¹¶ä¸”å¯ä»¥ä»é‡å¤–è§†é¢‘æŸ¥è¯¢ä¸­å¯é åœ°æ£€ç´¢åˆ°ç›¸å…³è§¦è§‰ä¿¡æ¯ï¼ŒéªŒè¯äº†è§¦è§‰åœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„å®ç”¨ä»·å€¼ã€‚

**Conclusion:** OpenTouchæ•°æ®é›†å’ŒåŸºå‡†çš„å‘å¸ƒå°†æ¨åŠ¨å¤šæ¨¡æ€ç¬¬ä¸€äººç§°æ„ŸçŸ¥ã€å…·èº«å­¦ä¹ ä»¥åŠæ¥è§¦å¯†é›†å‹æœºå™¨äººæ“ä½œç­‰é¢†åŸŸçš„å‘å±•ï¼Œä¸ºè§†è§‰ä¸è§¦è§‰èåˆç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®èµ„æºå’Œè¯„ä¼°æ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.


### [2] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath, Kai-Wei Chang, Ranjay Krishna, Luke Zettlemoyer, Yushi Hu, Marjan Ghazvininejad*

#### ğŸ§© TL;DR
æœ¬æ–‡æ­ç¤ºäº†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰è¯„ä¼°åŸºå‡†GenEvalå­˜åœ¨çš„åŸºå‡†æ¼‚ç§»é—®é¢˜ï¼Œå³é™æ€è¯„ä¼°å™¨æ— æ³•è·Ÿä¸Šæ–°æ¨¡å‹èƒ½åŠ›çš„å‘å±•ï¼Œå¯¼è‡´ä¸äººç±»åˆ¤æ–­ä¸¥é‡åç¦»ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œä½œè€…æå‡ºäº†æ–°çš„åŸºå‡†GenEval 2å’Œæ”¹è¿›çš„è¯„ä¼°æ–¹æ³•Soft-TIFAï¼Œä»¥æä¾›æ›´å¯é ä¸”ä¸æ˜“æ¼‚ç§»çš„T2Iæ¨¡å‹è¯„ä¼°æ¡†æ¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹è¯„ä¼°é¢ä¸´åŸºå‡†æ¼‚ç§»çš„ä¸¥é‡é—®é¢˜ï¼Œå³é™æ€çš„è¯„ä¼°åŸºå‡†ï¼ˆå¦‚GenEvalï¼‰æ— æ³•é€‚åº”æ–°æ¨¡å‹èƒ½åŠ›çš„å¿«é€Ÿå‘å±•ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸äººç±»åˆ¤æ–­é€æ¸åç¦»ã€‚è¿™ç§æ¼‚ç§»ä½¿å¾—ç°æœ‰åŸºå‡†çš„æœ‰æ•ˆæ€§éšæ—¶é—´ä¸‹é™ï¼Œæ— æ³•å‡†ç¡®åæ˜ æœ€æ–°æ¨¡å‹çš„çœŸå®æ€§èƒ½ï¼Œä»è€Œåœ¨T2Ié¢†åŸŸé€ æˆäº†è¯„ä¼°ç©ºç™½ã€‚

**Method:** ä½œè€…é¦–å…ˆé€šè¿‡å¤§è§„æ¨¡äººç±»ç ”ç©¶éªŒè¯äº†GenEvalåŸºå‡†çš„æ¼‚ç§»é—®é¢˜ï¼Œç„¶åæå‡ºäº†æ–°çš„åŸºå‡†GenEval 2ï¼Œè¯¥åŸºå‡†æ”¹è¿›äº†å¯¹åŸºæœ¬è§†è§‰æ¦‚å¿µçš„è¦†ç›–å¹¶æé«˜äº†ç»„åˆæ€§ç¨‹åº¦ã€‚åŒæ—¶ï¼Œä½œè€…å¼•å…¥äº†Soft-TIFAè¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å¯¹è§†è§‰åŸºæœ¬å…ƒç´ çš„åˆ¤æ–­ï¼Œç›¸æ¯”VQAScoreç­‰æ•´ä½“æ€§è¯„ä¼°å™¨ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¸äººç±»åˆ¤æ–­å¯¹é½ä¸”æ›´ä¸æ˜“å‘ç”Ÿæ¼‚ç§»ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGenEvalåŸºå‡†å·²ç»ä¸¥é‡æ¼‚ç§»ï¼Œä¸äººç±»åˆ¤æ–­çš„ç»å¯¹è¯¯å·®é«˜è¾¾17.7%ï¼Œè¡¨æ˜è¯¥åŸºå‡†åœ¨ç›¸å½“é•¿æ—¶é—´å†…å·²ç»é¥±å’Œã€‚æ–°æå‡ºçš„GenEval 2åŸºå‡†å¯¹å½“å‰æ¨¡å‹æ›´å…·æŒ‘æˆ˜æ€§ï¼Œè€ŒSoft-TIFAè¯„ä¼°æ–¹æ³•åœ¨äººç±»å¯¹é½æ€§æ–¹é¢è¡¨ç°æ›´ä¼˜ï¼Œä¸ºT2Iæ¨¡å‹æä¾›äº†æ›´å¯é çš„è¯„ä¼°æ¡†æ¶ã€‚

**Conclusion:** æœ¬ç ”ç©¶å¼ºè°ƒäº†T2IåŠç›¸å…³è‡ªåŠ¨åŒ–æ¨¡å‹è¯„ä¼°åŸºå‡†éœ€è¦æŒç»­å®¡è®¡å’Œæ”¹è¿›çš„é‡è¦æ€§ï¼Œå› ä¸ºé™æ€åŸºå‡†å®¹æ˜“éšæ—¶é—´æ¼‚ç§»è€Œå¤±æ•ˆã€‚GenEval 2å’ŒSoft-TIFAçš„æå‡ºä¸ºè§£å†³åŸºå‡†æ¼‚ç§»é—®é¢˜æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œä½†é¿å…åŸºå‡†æ¼‚ç§»å¹¶éæ˜“äº‹ï¼Œéœ€è¦ç ”ç©¶ç¤¾åŒºæŒç»­å…³æ³¨å’ŒåŠªåŠ›ã€‚

---

#### ğŸ“„ Abstract
Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.


### [3] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºLinkedOutï¼Œä¸€ç§ä»è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­æå–ä¸–ç•ŒçŸ¥è¯†æ„ŸçŸ¥è¡¨ç¤ºçš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘æ¨èç³»ç»Ÿä¸­å¤šè§†é¢‘è¾“å…¥æ”¯æŒã€ä½å»¶è¿Ÿæ¨ç†å’Œç»†ç²’åº¦è§†è§‰ç»†èŠ‚ä¿ç•™çš„æŒ‘æˆ˜ï¼Œæ— éœ€æ‰‹å·¥æ ‡æ³¨å³å¯åœ¨æ ‡å‡†åŸºå‡†ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ¨èç­‰ä¸‹æ¸¸ä»»åŠ¡éƒ¨ç½²ä¸­å­˜åœ¨ä¸‰ä¸ªä¸»è¦é™åˆ¶ï¼šä»…è§£ç ç”Ÿæˆå¯¼è‡´é¡ºåºæ¨ç†å»¶è¿Ÿé«˜ï¼Œå…¸å‹æ¥å£ä¸æ”¯æŒå¤šè§†é¢‘è¾“å…¥ï¼Œä»¥åŠè¯­è¨€è¾“å‡ºçº¦æŸä¸¢å¼ƒäº†å¯¹ä¸‹æ¸¸è§†è§‰ä»»åŠ¡è‡³å…³é‡è¦çš„ç»†ç²’åº¦è§†è§‰ç»†èŠ‚ã€‚è¿™äº›é™åˆ¶æºäºç¼ºä¹ä¸€ç§æ—¢èƒ½ä¿ç•™åƒç´ çº§ç»†èŠ‚åˆèƒ½åˆ©ç”¨ä¸–ç•ŒçŸ¥è¯†çš„è¡¨ç¤ºæ–¹æ³•ã€‚

**Method:** LinkedOuté€šè¿‡ä»åŸå§‹å¸§ä¸­æå–è¯­ä¹‰åŸºç¡€ã€çŸ¥è¯†æ„ŸçŸ¥çš„ä»¤ç‰Œï¼Œåˆ©ç”¨å¯æç¤ºæŸ¥è¯¢å’Œå¯é€‰è¾…åŠ©æ¨¡æ€å¼•å¯¼VLLMç”Ÿæˆè¡¨ç¤ºã€‚è¯¥æ–¹æ³•å¼•å…¥è·¨å±‚çŸ¥è¯†èåˆæ··åˆä¸“å®¶æ¨¡å‹ï¼Œä»ä¸°å¯Œçš„VLLMç‰¹å¾ä¸­é€‰æ‹©é€‚å½“çš„æŠ½è±¡å±‚æ¬¡ï¼Œå®ç°ä¸ªæ€§åŒ–ã€å¯è§£é‡Šå’Œä½å»¶è¿Ÿçš„æ¨èã€‚

**Result:** LinkedOutåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¯é¦–ä¸ªæ— éœ€æ‰‹å·¥æ ‡æ³¨ã€åŸºäºåŸå§‹å¸§æ“ä½œçš„VLLMè§†é¢‘æ¨èæ–¹æ³•ã€‚å¯è§£é‡Šæ€§ç ”ç©¶å’Œæ¶ˆèå®éªŒè¯å®äº†å±‚å¤šæ ·æ€§å’Œåˆ†å±‚èåˆçš„ä¼˜åŠ¿ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•æœ‰æ•ˆåˆ©ç”¨VLLMä¸–ç•ŒçŸ¥è¯†å…ˆéªŒå’Œè§†è§‰æ¨ç†çš„èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå……åˆ†åˆ©ç”¨VLLMä¸–ç•ŒçŸ¥è¯†å…ˆéªŒå’Œè§†è§‰æ¨ç†è¿›è¡Œä¸‹æ¸¸è§†è§‰ä»»åŠ¡æä¾›äº†ä¸€æ¡å®ç”¨è·¯å¾„ï¼Œå±•ç¤ºäº†è·¨å±‚çŸ¥è¯†èåˆåœ¨å¹³è¡¡æŠ½è±¡å±‚æ¬¡ä¸è®¡ç®—æ•ˆç‡æ–¹é¢çš„ä»·å€¼ï¼Œä¸ºè§†é¢‘æ¨èç³»ç»Ÿçš„é«˜æ•ˆéƒ¨ç½²å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.


### [4] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen, Yifan Wang, Zhengqin Li, Homanga Bharadhwaj, Yujin Chen, Chuan Qin, Ziyi Kou, Yuan Tian, Eric Whitmire, Rajinder Sodhi, Hrvoje Benko, Eli Shlizerman, Yue Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†EgoMANæ•°æ®é›†å’Œæ¨¡å‹ï¼Œç”¨äºè§£å†³3Dæ‰‹éƒ¨è½¨è¿¹é¢„æµ‹ä¸­è¯­ä¹‰ç›‘ç£ä¸è¿åŠ¨è§£è€¦ã€æ¨ç†ä¸åŠ¨ä½œå¼±å…³è”çš„é—®é¢˜ï¼Œé€šè¿‡æ„å»ºå¤§è§„æ¨¡ç¬¬ä¸€äººç§°æ•°æ®é›†å’Œæ¨ç†åˆ°è¿åŠ¨çš„æ¡†æ¶ï¼Œå®ç°äº†å‡†ç¡®ä¸”é˜¶æ®µæ„ŸçŸ¥çš„è½¨è¿¹é¢„æµ‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰3Dæ‰‹éƒ¨è½¨è¿¹é¢„æµ‹ç ”ç©¶å­˜åœ¨ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šæ•°æ®é›†å°†è¿åŠ¨ä¸è¯­ä¹‰ç›‘ç£è§£è€¦ï¼Œæ¨¡å‹ä»…å¼±å…³è”æ¨ç†ä¸åŠ¨ä½œã€‚è¿™å¯¼è‡´é¢„æµ‹è½¨è¿¹ç¼ºä¹è¯­ä¹‰ç†è§£å’Œäº¤äº’é˜¶æ®µæ„ŸçŸ¥èƒ½åŠ›ï¼Œé™åˆ¶äº†åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨æ•ˆæœã€‚

**Method:** ç ”ç©¶é¦–å…ˆæ„å»ºäº†EgoMANæ•°æ®é›†ï¼ŒåŒ…å«219Kä¸ª6DoFè½¨è¿¹å’Œ3Mç»“æ„åŒ–QAå¯¹ï¼Œæ”¯æŒè¯­ä¹‰ã€ç©ºé—´å’Œè¿åŠ¨æ¨ç†ã€‚éšåæå‡ºäº†EgoMANæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨ç†åˆ°è¿åŠ¨çš„æ¡†æ¶ï¼Œé€šè¿‡è½¨è¿¹-ä»¤ç‰Œæ¥å£è¿æ¥è§†è§‰è¯­è¨€æ¨ç†ä¸è¿åŠ¨ç”Ÿæˆï¼Œé‡‡ç”¨æ¸è¿›å¼è®­ç»ƒæ–¹æ³•å¯¹é½æ¨ç†ä¸è¿åŠ¨åŠ¨æ€ã€‚

**Result:** è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå‡†ç¡®ä¸”å…·æœ‰äº¤äº’é˜¶æ®µæ„ŸçŸ¥çš„3Dæ‰‹éƒ¨è½¨è¿¹ï¼Œåœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚EgoMANæ•°æ®é›†çš„å¤§è§„æ¨¡ç‰¹æ€§ï¼ˆ219Kè½¨è¿¹å’Œ3M QAå¯¹ï¼‰ä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†å……åˆ†çš„è¯­ä¹‰å’Œè¿åŠ¨ç›‘ç£ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å°†è¯­ä¹‰æ¨ç†ä¸è¿åŠ¨ç”Ÿæˆç´§å¯†è€¦åˆï¼Œå¯ä»¥æ˜¾è‘—æå‡3Dæ‰‹éƒ¨è½¨è¿¹é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚EgoMANæ¡†æ¶ä¸ºç¬¬ä¸€äººç§°è§†è§’ä¸‹çš„äº¤äº’ç†è§£æä¾›äº†æ–°çš„èŒƒå¼ï¼Œå…¶æ¸è¿›å¼è®­ç»ƒæ–¹æ³•ç¡®ä¿äº†æ¨ç†ä¸è¿åŠ¨åŠ¨æ€çš„æœ‰æ•ˆå¯¹é½ã€‚

---

#### ğŸ“„ Abstract
Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.


### [5] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov, Bernard Ghanem, Peter Wonka, Ashkan Mirzaei*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†EasyV2Væ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„åŸºäºæŒ‡ä»¤çš„è§†é¢‘ç¼–è¾‘ç³»ç»Ÿï¼Œé€šè¿‡æ•°æ®ç»„åˆã€ç®€åŒ–æ¶æ„å’Œç»Ÿä¸€æ§åˆ¶æœºåˆ¶ï¼Œåœ¨è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å›¾åƒç¼–è¾‘æŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œä½†è§†é¢‘ç¼–è¾‘é¢†åŸŸä»é¢ä¸´ä¸€è‡´æ€§ã€æ§åˆ¶å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•°æ®å¤šæ ·æ€§ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶æœºåˆ¶æ–¹é¢å­˜åœ¨å±€é™ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„è§†é¢‘ç¼–è¾‘è§£å†³æ–¹æ¡ˆã€‚

**Method:** EasyV2Væ¡†æ¶ä»æ•°æ®ã€æ¶æ„å’Œæ§åˆ¶ä¸‰ä¸ªç»´åº¦è¿›è¡Œè®¾è®¡ï¼šæ•°æ®æ–¹é¢é€šè¿‡ç»„åˆç°æœ‰ä¸“å®¶æ¨¡å‹ã€å•å¸§ç›‘ç£æå‡è§†é¢‘å¯¹ã€æŒ–æ˜å¯†é›†æ ‡æ³¨è§†é¢‘ç‰‡æ®µä»¥åŠæ·»åŠ è¿‡æ¸¡ç›‘ç£æ¥æ„å»ºå¤šæ ·åŒ–è®­ç»ƒæ•°æ®ï¼›æ¶æ„æ–¹é¢åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„ç¼–è¾‘èƒ½åŠ›ï¼Œé‡‡ç”¨ç®€å•åºåˆ—è¿æ¥å’Œè½»é‡çº§LoRAå¾®è°ƒï¼›æ§åˆ¶æ–¹é¢é€šè¿‡å•ä¸€æ©ç æœºåˆ¶ç»Ÿä¸€æ—¶ç©ºæ§åˆ¶ï¼Œå¹¶æ”¯æŒå¯é€‰å‚è€ƒå›¾åƒè¾“å…¥ã€‚

**Result:** EasyV2Våœ¨è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†åŒæœŸç«äº‰ç³»ç»Ÿå’Œå•†ä¸šç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§çµæ´»è¾“å…¥æ ¼å¼ï¼ŒåŒ…æ‹¬è§†é¢‘+æ–‡æœ¬ã€è§†é¢‘+æ©ç +æ–‡æœ¬ã€è§†é¢‘+æ©ç +å‚è€ƒå›¾åƒ+æ–‡æœ¬ç­‰ç»„åˆã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜é¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹å·²å…·å¤‡ç¼–è¾‘èƒ½åŠ›ï¼Œç®€åŒ–è®¾è®¡ç»“åˆæ•°æ®å¤šæ ·æ€§æ„å»ºå’Œç»Ÿä¸€æ§åˆ¶æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆè§£å†³è§†é¢‘ç¼–è¾‘çš„æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥è§†é¢‘ç¼–è¾‘ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„æ¡†æ¶è®¾è®¡æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/


### [6] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†AuditDMï¼Œä¸€ä¸ªç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è‡ªåŠ¨åŒ–å®¡è®¡æ¡†æ¶ï¼Œé€šè¿‡ä¸»åŠ¨å‘ç°å¹¶çº æ­£æ¨¡å‹å¤±è´¥æ¨¡å¼æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ¡†æ¶è®­ç»ƒä¸€ä¸ªå®¡è®¡æ¨¡å‹ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜å’Œåäº‹å®å›¾åƒï¼Œä»¥æ­ç¤ºç›®æ ‡æ¨¡å‹çš„å¼±ç‚¹å¹¶ç”Ÿæˆæ— éœ€æ ‡æ³¨çš„ä¿®æ­£æ•°æ®ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æ–¹æ³•ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œä¸”å¾€å¾€æ— æ³•å……åˆ†æ­ç¤ºä¸åŒæ¨¡å‹ä¹‹é—´çš„æ˜¾è‘—èƒ½åŠ›å·®è·ã€‚ç°æœ‰è¯„ä¼°æ–¹æ³•é€šå¸¸æ˜¯è¢«åŠ¨å’Œé™æ€çš„ï¼Œéš¾ä»¥ç³»ç»Ÿæ€§åœ°å‘ç°æ¨¡å‹çš„å¤±è´¥æ¨¡å¼ï¼Œé™åˆ¶äº†æ¨¡å‹è¯Šæ–­å’Œæ”¹è¿›çš„æœ‰æ•ˆæ€§ã€‚

**Method:** AuditDMæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸€ä¸ªMLLMä½œä¸ºå®¡è®¡å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæœ€å¤§åŒ–ç›®æ ‡æ¨¡å‹ä¹‹é—´åˆ†æ­§çš„æŒ‘æˆ˜æ€§é—®é¢˜å’Œåäº‹å®å›¾åƒã€‚è®­ç»ƒå®Œæˆåï¼Œå®¡è®¡å™¨èƒ½å¤Ÿå‘ç°å¤šæ ·åŒ–çš„ã€å¯è§£é‡Šçš„ç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹æ—¢æ­ç¤ºäº†æ¨¡å‹å¼±ç‚¹ï¼Œåˆä½œä¸ºæ— éœ€æ ‡æ³¨çš„ä¿®æ­£æ•°æ®ç”¨äºæ¨¡å‹æ”¹è¿›ã€‚

**Result:** åœ¨Gemma-3å’ŒPaliGemma-2ç­‰æœ€å…ˆè¿›æ¨¡å‹ä¸Šåº”ç”¨AuditDMï¼Œå‘ç°äº†è¶…è¿‡20ç§ä¸åŒçš„å¤±è´¥ç±»å‹ã€‚åŸºäºè¿™äº›å‘ç°è¿›è¡Œå¾®è°ƒåï¼Œæ‰€æœ‰æ¨¡å‹åœ¨16ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è·å¾—ä¸€è‡´æå‡ï¼Œå¹¶ä½¿ä¸€ä¸ª3Bå‚æ•°çš„æ¨¡å‹è¶…è¶Šäº†å…¶28Bå‚æ•°çš„å¯¹åº”ç‰ˆæœ¬ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“æ•°æ®æ‰©å±•è¾¾åˆ°æ”¶ç›Šé€’å‡æ—¶ï¼Œæœ‰é’ˆå¯¹æ€§çš„æ¨¡å‹å®¡è®¡ä¸ºæ¨¡å‹è¯Šæ–­å’Œæ”¹è¿›æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚AuditDMæ¡†æ¶ä¸ä»…èƒ½å¤Ÿç³»ç»Ÿæ€§åœ°å‘ç°æ¨¡å‹å¼±ç‚¹ï¼Œè¿˜èƒ½ç”Ÿæˆé«˜è´¨é‡çš„ä¿®æ­£æ•°æ®ï¼Œä¸ºæ¨¡å‹èƒ½åŠ›çš„æŒç»­æå‡æä¾›äº†æ–°çš„æ–¹æ³•è®ºã€‚

---

#### ğŸ“„ Abstract
Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.


### [7] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian, Boyao Han, Chen Shi, Lei Xiao, Long Yang, Shaoshuai Shi, Li Jiang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºGeoPredictï¼Œä¸€ç§å‡ ä½•æ„ŸçŸ¥çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ï¼Œé€šè¿‡é¢„æµ‹æ€§è¿åŠ¨å­¦å’Œå‡ ä½•å…ˆéªŒå¢å¼ºè¿ç»­åŠ¨ä½œç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†VLAæ¨¡å‹åœ¨éœ€è¦ç²¾ç¡®3Dæ¨ç†çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ä¸»è¦å±€é™äºååº”å¼å’Œ2Dä¸­å¿ƒçš„æ–¹æ³•ï¼Œåœ¨éœ€è¦ç²¾ç¡®3Dæ¨ç†çš„ä»»åŠ¡ä¸­å¯é æ€§ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡ ä½•å¯†é›†å’Œç©ºé—´è¦æ±‚é«˜çš„åœºæ™¯ä¸‹å­˜åœ¨æ˜æ˜¾å±€é™ã€‚

**Method:** GeoPredictæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šè½¨è¿¹çº§æ¨¡å—ç¼–ç è¿åŠ¨å†å²å¹¶é¢„æµ‹æœºå™¨äººæ‰‹è‡‚çš„å¤šæ­¥3Då…³é”®ç‚¹è½¨è¿¹ï¼Œä»¥åŠé¢„æµ‹æ€§3Dé«˜æ–¯å‡ ä½•æ¨¡å—é€šè¿‡è½¨è¿¹å¼•å¯¼çš„ç»†åŒ–é¢„æµ‹å·¥ä½œç©ºé—´å‡ ä½•ã€‚è¿™äº›é¢„æµ‹æ¨¡å—ä»…ä½œä¸ºè®­ç»ƒæ—¶çš„ç›‘ç£ä¿¡å·ï¼Œé€šè¿‡åŸºäºæ·±åº¦çš„æ¸²æŸ“å®ç°ï¼Œæ¨ç†æ—¶ä»…éœ€è½»é‡çº§é¢å¤–æŸ¥è¯¢ä»¤ç‰Œè€Œæ— éœ€ä»»ä½•3Dè§£ç ã€‚

**Result:** åœ¨RoboCasa Human-50ã€LIBEROå’ŒçœŸå®ä¸–ç•Œæ“ä½œä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGeoPredictæŒç»­è¶…è¶Šå¼ºå¤§çš„VLAåŸºçº¿æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡ ä½•å¯†é›†å’Œç©ºé—´è¦æ±‚é«˜çš„åœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºï¼ŒéªŒè¯äº†å…¶å‡ ä½•æ„ŸçŸ¥é¢„æµ‹æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å°†é¢„æµ‹æ€§è¿åŠ¨å­¦å’Œå‡ ä½•å…ˆéªŒæ•´åˆåˆ°VLAæ¡†æ¶ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæå‡æœºå™¨äººæ“ä½œä¸­çš„3Dæ¨ç†èƒ½åŠ›æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†æ—¶çš„è®¡ç®—æ•ˆç‡ï¼Œä¸ºæœªæ¥æ›´å¤æ‚çš„å‡ ä½•æ„ŸçŸ¥æœºå™¨äººç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.


### [8] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. MuÃ±iz-De-LeÃ³n, Jorge A. Rosales-de-Golferichs, Ana S. MuÃ±oz-RodrÃ­guez, Alejandro I. Trejo-Castro, Eduardo de Avila-Armenta, Antonio MartÃ­nez-Torteya*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç´§å‡‘çš„èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆæ¶æ„ï¼Œé€šè¿‡å•å¼ æ­£é¢å›¾åƒç”ŸæˆæŠ¥å‘Šå‘ç°éƒ¨åˆ†ï¼Œé‡‡ç”¨å†»ç»“çš„DINOv3è§†è§‰ç¼–ç å™¨å’Œå¢å¼ºçš„GPT-2è§£ç å™¨ï¼Œç»“åˆåˆ†å±‚è§£å‰–æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨å‡å°‘èµ„æºéœ€æ±‚çš„åŒæ—¶æ˜¾è‘—æå‡äº†ä¸´åºŠç›¸å…³åŒºåŸŸçš„ç”Ÿæˆè´¨é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æœ€å…ˆè¿›çš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿå¦‚MAIRA-2å’ŒMedPaLM-Mä¾èµ–äºå¤§è§„æ¨¡å¤šæ¨¡æ€è®­ç»ƒã€ä¸´åºŠå…ƒæ•°æ®å’Œå¤šä¸ªæˆåƒè§†å›¾ï¼Œå¯¼è‡´èµ„æºå¯†é›†ä¸”éš¾ä»¥åœ¨å¤§å¤šæ•°åŒ»ç–—ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§ä»…éœ€å•å¼ æ­£é¢å›¾åƒçš„ç´§å‡‘æ¶æ„æ¥è§£å†³è¿™ä¸€å¯è®¿é—®æ€§é—®é¢˜ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨å†»ç»“çš„DINOv3è§†è§‰Transformerç¼–ç å™¨ä¸GPT-2è§£ç å™¨ç›¸ç»“åˆï¼Œé€šè¿‡å±‚çº§çš„è§£å‰–æ³¨æ„åŠ›æœºåˆ¶æ•´åˆè‚ºéƒ¨å’Œå¿ƒè„åˆ†å‰²æ©ç ï¼Œåˆ©ç”¨åˆ†å±‚é«˜æ–¯å¹³æ»‘æŠ€æœ¯å°†æ³¨æ„åŠ›åå‘ä¸´åºŠç›¸å…³åŒºåŸŸï¼Œè¯¥æœºåˆ¶ä¸å¢åŠ å¯è®­ç»ƒå‚æ•°ï¼Œå®ç°äº†çº¯å›¾åƒæ¡ä»¶çš„æŠ¥å‘Šç”Ÿæˆã€‚

**Result:** åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªå…³é”®ç—…ç†çš„CheXpert Macro-F1æŒ‡æ ‡ä¸Šæå‡äº†168%ï¼ˆ0.083â†’0.238ï¼‰ï¼ŒMicro-F1æå‡äº†146%ï¼ˆ0.137â†’0.337ï¼‰ï¼Œ14ä¸ªè§‚å¯ŸæŒ‡æ ‡çš„æ€»ä½“æ€§èƒ½æå‡äº†86%ï¼ˆ0.170â†’0.316ï¼‰ï¼Œç»“æ„è¿è´¯æ€§æ–¹é¢RadGraph F1æå‡äº†9.7%ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œè§£ç å™¨å±‚çº§çš„è§£å‰–å¼•å¯¼èƒ½å¤Ÿæ”¹å–„ç©ºé—´å®šä½å¹¶å¢å¼ºä¸´åºŠç›¸å…³åŒºåŸŸçš„è¿è´¯æ€§ï¼Œå°½ç®¡æ¨¡å‹è§„æ¨¡è¾ƒå°ä¸”ä»…ä¾èµ–å›¾åƒæ¡ä»¶ï¼Œä½†ä»èƒ½å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.


### [9] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu, Lei Ke, Xiaohang Zhan, Longxiang Tang, Yuqi Liu, Bohao Peng, Bei Yu, Dong Yu, Jiaya Jia*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†RePlanæ¡†æ¶ï¼Œé€šè¿‡åŒºåŸŸå¯¹é½è§„åˆ’æ–¹æ³•è§£å†³æŒ‡ä»¤-è§†è§‰å¤æ‚æ€§åœºæ™¯ä¸‹çš„å›¾åƒç¼–è¾‘é—®é¢˜ï¼Œè¯¥æ¡†æ¶ç»“åˆè§†è§‰è¯­è¨€è§„åˆ’å™¨å’Œæ‰©æ•£ç¼–è¾‘å™¨ï¼Œåœ¨å¤æ‚æŒ‡ä»¤å’Œæ‚ä¹±åœºæ™¯ä¸­å®ç°ç²¾ç¡®çš„å¤šåŒºåŸŸå¹¶è¡Œç¼–è¾‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨å¤„ç†æŒ‡ä»¤-è§†è§‰å¤æ‚æ€§åœºæ™¯æ—¶è¡¨ç°ä¸ä½³ï¼Œå³å½“å¤æ‚æŒ‡ä»¤é‡åˆ°æ‚ä¹±æˆ–æ¨¡ç³Šåœºæ™¯æ—¶ï¼Œæ¨¡å‹éš¾ä»¥å‡†ç¡®ç†è§£å’Œæ‰§è¡Œç¼–è¾‘ä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†å®é™…åº”ç”¨ä¸­çš„ç¼–è¾‘ç²¾åº¦å’Œå¯é æ€§ã€‚

**Method:** RePlané‡‡ç”¨è§„åˆ’-æ‰§è¡Œæ¡†æ¶ï¼ŒåŒ…å«è§†è§‰è¯­è¨€è§„åˆ’å™¨å’Œæ‰©æ•£ç¼–è¾‘å™¨ä¸¤éƒ¨åˆ†ã€‚è§„åˆ’å™¨é€šè¿‡é€æ­¥æ¨ç†åˆ†è§£æŒ‡ä»¤å¹¶æ˜¾å¼åœ°å°†æŒ‡ä»¤å®šä½åˆ°ç›®æ ‡åŒºåŸŸï¼Œç¼–è¾‘å™¨åˆ™ä½¿ç”¨æ— éœ€è®­ç»ƒçš„æ³¨æ„åŠ›åŒºåŸŸæ³¨å…¥æœºåˆ¶è¿›è¡Œä¿®æ”¹ï¼Œæ”¯æŒç²¾ç¡®çš„å¹¶è¡Œå¤šåŒºåŸŸç¼–è¾‘è€Œæ— éœ€è¿­ä»£ä¿®å¤ã€‚ä¸ºå¢å¼ºè§„åˆ’èƒ½åŠ›ï¼Œé‡‡ç”¨åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ ï¼Œä»…ä½¿ç”¨1Kçº¯æŒ‡ä»¤ç¤ºä¾‹å³å¯æ˜¾è‘—æå‡æ¨ç†ä¿çœŸåº¦å’Œæ ¼å¼å¯é æ€§ã€‚

**Result:** RePlanåœ¨IV-EditåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯¥åŸºå‡†ä¸“æ³¨äºç»†ç²’åº¦å®šä½å’ŒçŸ¥è¯†å¯†é›†å‹ç¼–è¾‘ä»»åŠ¡ã€‚åœ¨æŒ‡ä»¤-è§†è§‰å¤æ‚æ€§è®¾ç½®ä¸‹ï¼ŒRePlanæŒç»­ä¼˜äºä½¿ç”¨æ›´å¤§æ•°æ®é›†è®­ç»ƒçš„å¼ºåŸºçº¿æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†åŒºåŸŸç²¾åº¦å’Œæ•´ä½“ä¿çœŸåº¦ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å°†å¤æ‚æŒ‡ä»¤åˆ†è§£ä¸ºé€æ­¥æ¨ç†å¹¶æ˜¾å¼å®šä½åˆ°ç›®æ ‡åŒºåŸŸï¼Œç»“åˆæ— éœ€è®­ç»ƒçš„æ³¨æ„åŠ›æ³¨å…¥æœºåˆ¶ï¼Œå¯ä»¥æ˜¾è‘—æå‡å¤æ‚åœºæ™¯ä¸‹çš„å›¾åƒç¼–è¾‘ç²¾åº¦ã€‚è¯¥æ–¹æ³•ä¸ºå¤„ç†æŒ‡ä»¤-è§†è§‰å¤æ‚æ€§æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºæœªæ¥åŸºäºæŒ‡ä»¤çš„è§†è§‰ä»»åŠ¡ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io


### [10] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele, Mehran Hosseinzadeh, Hendrik P. A. Lensch*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºReMeDI-SAM3ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„å†…å­˜å¢å¼ºå‹SAM3æ‰©å±•ï¼Œé€šè¿‡ç›¸å…³æ€§æ„ŸçŸ¥å†…å­˜è¿‡æ»¤ã€åˆ†æ®µæ’å€¼æ–¹æ¡ˆå’ŒåŸºäºç‰¹å¾çš„é‡æ–°è¯†åˆ«æ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†å†…çª¥é•œè§†é¢‘ä¸­æ‰‹æœ¯å™¨æ¢°åˆ†å‰²çš„å‡†ç¡®æ€§å’Œé®æŒ¡æ¢å¤èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å†…çª¥é•œè§†é¢‘ä¸­æ‰‹æœ¯å™¨æ¢°çš„ç²¾ç¡®åˆ†å‰²å¯¹äºè®¡ç®—æœºè¾…åŠ©å¹²é¢„è‡³å…³é‡è¦ï¼Œä½†ç”±äºé¢‘ç¹é®æŒ¡ã€å¿«é€Ÿè¿åŠ¨ã€é•œé¢ä¼ªå½±å’Œé•¿æœŸå™¨æ¢°é‡æ–°è¿›å…¥ç­‰æŒ‘æˆ˜è€Œéš¾ä»¥å®ç°ã€‚è™½ç„¶SAM3ä¸ºè§†é¢‘å¯¹è±¡åˆ†å‰²æä¾›äº†å¼ºå¤§çš„æ—¶ç©ºæ¡†æ¶ï¼Œä½†å…¶åœ¨æ‰‹æœ¯åœºæ™¯ä¸­çš„æ€§èƒ½å—åˆ°æ— å·®åˆ«å†…å­˜æ›´æ–°ã€å›ºå®šå†…å­˜å®¹é‡ä»¥åŠé®æŒ¡åå¼±èº«ä»½æ¢å¤èƒ½åŠ›çš„é™åˆ¶ã€‚

**Method:** ReMeDI-SAM3é€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶æ‰©å±•SAM3ï¼šç›¸å…³æ€§æ„ŸçŸ¥å†…å­˜è¿‡æ»¤æœºåˆ¶ï¼Œé…å¤‡ä¸“é—¨çš„é®æŒ¡æ„ŸçŸ¥å†…å­˜ç”¨äºå­˜å‚¨é®æŒ¡å‰å¸§ï¼›åˆ†æ®µæ’å€¼æ–¹æ¡ˆï¼Œæ‰©å±•æœ‰æ•ˆå†…å­˜å®¹é‡ï¼›ä»¥åŠåŸºäºç‰¹å¾çš„é‡æ–°è¯†åˆ«æ¨¡å—ï¼Œç»“åˆæ—¶é—´æŠ•ç¥¨æœºåˆ¶å®ç°å¯é çš„é®æŒ¡åèº«ä»½æ¶ˆæ­§ã€‚è¿™äº›ç»„ä»¶å…±åŒå·¥ä½œä»¥å‡è½»é”™è¯¯ç´¯ç§¯å¹¶å®ç°å¯é çš„é®æŒ¡æ¢å¤ã€‚

**Result:** åœ¨EndoVis17å’ŒEndoVis18æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬è¯„ä¼°æ˜¾ç¤ºï¼Œç›¸å¯¹äºåŸå§‹SAM3åˆ†åˆ«å®ç°äº†çº¦7%å’Œ16%çš„ç»å¯¹mcIoUæå‡ï¼Œç”šè‡³è¶…è¶Šäº†å…ˆå‰åŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†æ‰‹æœ¯å™¨æ¢°åˆ†å‰²çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**Conclusion:** ReMeDI-SAM3é€šè¿‡å†…å­˜å¢å¼ºæœºåˆ¶æœ‰æ•ˆè§£å†³äº†æ‰‹æœ¯åœºæ™¯ä¸­è§†é¢‘åˆ†å‰²çš„å…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é®æŒ¡æ¢å¤é—®é¢˜ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†æ— éœ€è®­ç»ƒçš„æ”¹è¿›ç­–ç•¥åœ¨å¤æ‚åŒ»å­¦è§†é¢‘åˆ†æä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯ç³»ç»Ÿæä¾›äº†æ›´å¯é çš„åˆ†å‰²è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¯èƒ½å¯å‘å…¶ä»–è§†é¢‘ç†è§£ä»»åŠ¡çš„å†…å­˜ä¼˜åŒ–è®¾è®¡ã€‚

---

#### ğŸ“„ Abstract
Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.


### [11] [Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection](https://arxiv.org/abs/2512.16905)
*Kaixin Ding, Yang Zhou, Xi Chen, Miao Yang, Jiarong Ou, Rui Chen, Xin Tao, Hengshuang Zhao*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Alchemistï¼Œä¸€ç§åŸºäºå…ƒæ¢¯åº¦çš„æ•°æ®é€‰æ‹©æ¡†æ¶ï¼Œç”¨äºä»å¤§è§„æ¨¡æ–‡æœ¬-å›¾åƒæ•°æ®å¯¹ä¸­è‡ªåŠ¨é€‰æ‹©é«˜è´¨é‡å­é›†ï¼Œä»¥æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œè§†è§‰è´¨é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½å—é™äºè®­ç»ƒæ•°æ®è´¨é‡ï¼Œç½‘ç»œçˆ¬å–å’Œåˆæˆå›¾åƒæ•°æ®é›†å¸¸åŒ…å«ä½è´¨é‡æˆ–å†—ä½™æ ·æœ¬ï¼Œå¯¼è‡´è§†è§‰ä¿çœŸåº¦ä¸‹é™ã€è®­ç»ƒä¸ç¨³å®šå’Œè®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•ä¾èµ–æ˜‚è´µçš„äººå·¥ç­›é€‰æˆ–åŸºäºå•ç»´ç‰¹å¾çš„å¯å‘å¼è¯„åˆ†ï¼Œä¸”å…ƒå­¦ä¹ æ–¹æ³•åœ¨å›¾åƒæ¨¡æ€ä¸­å°šæœªå¾—åˆ°é€‚é…ï¼Œå› æ­¤éœ€è¦ä¸€ç§è‡ªåŠ¨ã€å¯æ‰©å±•çš„æ•°æ®é€‰æ‹©æ¡†æ¶ã€‚

**Method:** Alchemisté‡‡ç”¨åŸºäºå…ƒæ¢¯åº¦çš„æ¡†æ¶ï¼Œé€šè¿‡æ•°æ®ä¸­å¿ƒçš„è§†è§’è¿­ä»£ä¼˜åŒ–æ¨¡å‹ä»¥è¯„ä¼°æ¯ä¸ªæ ·æœ¬çš„å½±å“ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šæ•°æ®è¯„çº§å’Œæ•°æ®å‰ªæã€‚é¦–å…ˆè®­ç»ƒä¸€ä¸ªè½»é‡çº§è¯„çº§å™¨ï¼ŒåŸºäºæ¢¯åº¦ä¿¡æ¯å¹¶å¢å¼ºå¤šç²’åº¦æ„ŸçŸ¥æ¥ä¼°è®¡æ¯ä¸ªæ ·æœ¬çš„å½±å“ï¼›ç„¶åä½¿ç”¨Shift-Gsamplingç­–ç•¥é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„å­é›†è¿›è¡Œé«˜æ•ˆæ¨¡å‹è®­ç»ƒã€‚

**Result:** åœ¨åˆæˆå’Œç½‘ç»œçˆ¬å–æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAlchemistèƒ½æŒç»­æå‡è§†è§‰è´¨é‡å’Œä¸‹æ¸¸æ€§èƒ½ã€‚ä½¿ç”¨Alchemisté€‰æ‹©çš„50%æ•°æ®å­é›†è¿›è¡Œè®­ç»ƒï¼Œå…¶è¡¨ç°å¯è¶…è¶Šä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒçš„ç»“æœï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æ•°æ®æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

**Conclusion:** Alchemistæ˜¯é¦–ä¸ªç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹è®­ç»ƒçš„è‡ªåŠ¨ã€å¯æ‰©å±•ã€åŸºäºå…ƒæ¢¯åº¦çš„æ•°æ®é€‰æ‹©æ¡†æ¶ï¼Œä¸ºè§£å†³è®­ç»ƒæ•°æ®è´¨é‡é—®é¢˜æä¾›äº†ç³»ç»ŸåŒ–è§£å†³æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†æ•°æ®é€‰æ‹©å¯¹ç”Ÿæˆæ¨¡å‹æ€§èƒ½çš„å…³é”®å½±å“ï¼Œå¹¶ä¸ºé«˜æ•ˆè®­ç»ƒå¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.


### [12] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MomaGraphï¼Œä¸€ç§ç”¨äºå…·èº«æ™ºèƒ½ä½“çš„ç»Ÿä¸€åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œé›†æˆäº†ç©ºé—´-åŠŸèƒ½å…³ç³»å’Œéƒ¨ä»¶çº§äº¤äº’å…ƒç´ ï¼Œå¹¶å¼•å…¥äº†é¦–ä¸ªå¤§è§„æ¨¡ä»»åŠ¡é©±åŠ¨åœºæ™¯å›¾æ•°æ®é›†MomaGraph-Sceneså’Œç³»ç»Ÿè¯„ä¼°å¥—ä»¶MomaGraph-Benchï¼Œåœ¨æ­¤åŸºç¡€ä¸Šå¼€å‘äº†MomaGraph-R1æ¨¡å‹ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°71.6%çš„å‡†ç¡®ç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åœºæ™¯å›¾è¡¨ç¤ºæ–¹æ³•é€šå¸¸å°†ç©ºé—´å…³ç³»å’ŒåŠŸèƒ½å…³ç³»åˆ†ç¦»ï¼Œå°†åœºæ™¯è§†ä¸ºé™æ€å¿«ç…§è€Œå¿½ç•¥å¯¹è±¡çŠ¶æ€å’Œæ—¶åºæ›´æ–°ï¼Œå¹¶ä¸”å¿½è§†äº†ä¸å½“å‰ä»»åŠ¡æœ€ç›¸å…³çš„ä¿¡æ¯ï¼Œè¿™é™åˆ¶äº†ç§»åŠ¨æ“ä½œæœºå™¨äººåœ¨å®¶åº­ç¯å¢ƒä¸­çš„å¯¼èˆªå’Œæ“ä½œèƒ½åŠ›ï¼Œéœ€è¦ä¸€ç§ç´§å‡‘ä¸”è¯­ä¹‰ä¸°å¯Œçš„ç»Ÿä¸€åœºæ™¯è¡¨ç¤ºæ–¹æ³•ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†MomaGraphç»Ÿä¸€åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œé›†æˆäº†ç©ºé—´-åŠŸèƒ½å…³ç³»å’Œéƒ¨ä»¶çº§äº¤äº’å…ƒç´ ï¼›åˆ›å»ºäº†MomaGraph-Scenesæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡ä»»åŠ¡é©±åŠ¨çš„å®¶åº­ç¯å¢ƒåœºæ™¯å›¾æ•°æ®é›†ï¼›å¼€å‘äº†MomaGraph-Benchè¯„ä¼°å¥—ä»¶ï¼Œæ¶µç›–ä»é«˜å±‚è§„åˆ’åˆ°ç»†ç²’åº¦åœºæ™¯ç†è§£çš„å…­ç§æ¨ç†èƒ½åŠ›ï¼›å¹¶åŸºäºæ­¤è®­ç»ƒäº†MomaGraph-R1ï¼Œä¸€ä¸ª70äº¿å‚æ•°çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œèƒ½å¤Ÿé¢„æµ‹ä»»åŠ¡å¯¼å‘åœºæ™¯å›¾å¹¶ä½œä¸ºé›¶æ ·æœ¬ä»»åŠ¡è§„åˆ’å™¨ã€‚

**Result:** MomaGraph-R1åœ¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°71.6%çš„å‡†ç¡®ç‡ï¼Œç›¸æ¯”æœ€ä½³åŸºçº¿æå‡äº†11.4%ï¼Œåœ¨å¼€æºæ¨¡å‹ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼›è¯¥æ¨¡å‹åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººå®éªŒä¸­å®ç°äº†æœ‰æ•ˆçš„è¿ç§»ã€‚

**Conclusion:** MomaGraphä¸ºå…·èº«æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§ç»Ÿä¸€çš„åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨ç©ºé—´-åŠŸèƒ½æ•´åˆã€åŠ¨æ€çŠ¶æ€å»ºæ¨¡å’Œä»»åŠ¡ç›¸å…³æ€§æ–¹é¢çš„å±€é™æ€§ï¼›é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†å’Œç³»ç»Ÿè¯„ä¼°å¥—ä»¶çš„å»ºç«‹ï¼Œä¸ºåœºæ™¯å›¾ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºç¡€è®¾æ–½ï¼›æå‡ºçš„Graph-then-Planæ¡†æ¶å±•ç¤ºäº†ä»»åŠ¡å¯¼å‘åœºæ™¯å›¾åœ¨é›¶æ ·æœ¬è§„åˆ’ä¸­çš„æœ‰æ•ˆæ€§ã€‚

---

#### ğŸ“„ Abstract
Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.


### [13] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao, Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSFTokï¼Œä¸€ç§ç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆçš„ç¦»æ•£åˆ†è¯å™¨ï¼Œé€šè¿‡è‡ªå¼ºåˆ¶å¼•å¯¼è§†è§‰é‡å»ºå’Œå»åæ‹Ÿåˆè®­ç»ƒç­–ç•¥ï¼Œè§£å†³äº†å¤šæ­¥è¿­ä»£è¿‡ç¨‹ä¸­çš„è®­ç»ƒ-æ¨ç†ä¸ä¸€è‡´é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒé‡å»ºè´¨é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡ç¦»æ•£åˆ†è¯å™¨ä¸è‡ªå›å½’èŒƒå¼å¤©ç„¶å¥‘åˆï¼Œä½†å…¶æ€§èƒ½ä»è½åäºè¿ç»­åˆ†è¯å™¨ï¼Œé™åˆ¶äº†åœ¨å¤šæ¨¡æ€ç³»ç»Ÿä¸­çš„å¹¿æ³›åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤šæ­¥è¿­ä»£è¿‡ç¨‹ä¸­å­˜åœ¨è®­ç»ƒä¸æ¨ç†ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¯¼è‡´å›¾åƒé‡å»ºè´¨é‡å—é™ã€‚

**Method:** SFToké‡‡ç”¨å¤šæ­¥è¿­ä»£æœºåˆ¶è¿›è¡Œç²¾ç¡®é‡å»ºï¼Œæ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬è‡ªå¼ºåˆ¶å¼•å¯¼è§†è§‰é‡å»ºå’Œå»åæ‹Ÿåˆè®­ç»ƒç­–ç•¥ã€‚è¿™äº›æŠ€æœ¯è§£å†³äº†å¤šæ­¥è¿‡ç¨‹ä¸­çš„è®­ç»ƒ-æ¨ç†ä¸ä¸€è‡´é—®é¢˜ï¼Œé€šè¿‡ä»…ä½¿ç”¨64ä¸ªtokençš„é«˜å‹ç¼©ç‡å®ç°é«˜æ•ˆå›¾åƒè¡¨ç¤ºã€‚

**Result:** åœ¨ImageNetæ•°æ®é›†ä¸Šï¼ŒSFTokå®ç°äº†æœ€å…ˆè¿›çš„å›¾åƒé‡å»ºè´¨é‡ï¼ˆrFID = 1.21ï¼‰ï¼Œåœ¨ç±»åˆ«åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ˆgFID = 2.29ï¼‰ã€‚è¯¥æ¨¡å‹ä»…ä½¿ç”¨æ¯å›¾åƒ64ä¸ªtokençš„é«˜å‹ç¼©ç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰ç¦»æ•£åˆ†è¯å™¨ã€‚

**Conclusion:** SFToké€šè¿‡è§£å†³è®­ç»ƒ-æ¨ç†ä¸ä¸€è‡´é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†ç¦»æ•£åˆ†è¯å™¨çš„æ€§èƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿä¸è¿ç»­åˆ†è¯å™¨ç«äº‰ã€‚è¿™é¡¹å·¥ä½œä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆæä¾›äº†æ›´é«˜æ•ˆçš„ç¦»æ•£è¡¨ç¤ºæ–¹æ³•ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€ç³»ç»Ÿä¸­ç¦»æ•£åˆ†è¯å™¨çš„åº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).


### [14] [Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation](https://arxiv.org/abs/2512.16913)
*Xin Lin, Meixi Song, Dizhe Zhang, Wenxuan Lu, Haodong Li, Bo Du, Ming-Hsuan Yang, Truong Nguyen, Lu Qi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ™¯åº¦é‡æ·±åº¦åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡æ•°æ®é©±åŠ¨çš„èŒƒå¼æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†å¹¶è®¾è®¡åˆ›æ–°æ¡†æ¶ï¼Œå®ç°äº†è·¨å¤šæ ·åŒ–åœºæ™¯è·ç¦»çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ·±åº¦ä¼°è®¡æ¨¡å‹åœ¨å¤„ç†å…¨æ™¯å›¾åƒæ—¶é¢ä¸´è·¨å®¤å†…/å®¤å¤–åœºæ™¯å’Œåˆæˆ/çœŸå®æ•°æ®åŸŸå·®å¼‚çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ ·åŒ–è·ç¦»å°ºåº¦ä¸‹çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ„å»ºä¸€ä¸ªèƒ½å¤Ÿç»Ÿä¸€å¤„ç†ä¸åŒåœºæ™¯è·ç¦»çš„å…¨æ™¯åº¦é‡æ·±åº¦åŸºç¡€æ¨¡å‹ï¼Œè§£å†³ç°æœ‰æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œå¤æ‚ç¯å¢ƒä¸­çš„é²æ£’æ€§é—®é¢˜ã€‚

**Method:** é‡‡ç”¨æ•°æ®é©±åŠ¨èŒƒå¼ï¼Œç»“åˆå…¬å¼€æ•°æ®é›†ã€UE5æ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„é«˜è´¨é‡åˆæˆæ•°æ®ã€æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆæ•°æ®ä»¥åŠç½‘ç»œæ”¶é›†çš„çœŸå®å…¨æ™¯å›¾åƒæ„å»ºå¤§è§„æ¨¡æ•°æ®é›†ã€‚æå‡ºä¸‰é˜¶æ®µä¼ªæ ‡ç­¾ç­›é€‰æµç¨‹ä»¥å‡å°‘åŸŸå·®å¼‚ï¼Œé‡‡ç”¨DINOv3-Largeä½œä¸ºä¸»å¹²ç½‘ç»œï¼Œå¹¶å¼•å…¥å³æ’å³ç”¨çš„èŒƒå›´æ©ç å¤´ã€é”åº¦ä¸­å¿ƒä¼˜åŒ–å’Œå‡ ä½•ä¸­å¿ƒä¼˜åŒ–ç­–ç•¥ï¼Œå¢å¼ºå¯¹å˜åŒ–è·ç¦»çš„é²æ£’æ€§å’Œè·¨è§†å›¾å‡ ä½•ä¸€è‡´æ€§ã€‚

**Result:** åœ¨Stanford2D3Dã€Matterport3Då’ŒDeep360ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½è¡¨ç°å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æ¨¡å‹åœ¨å¤šæ ·åŒ–çœŸå®ä¸–ç•Œåœºæ™¯ä¸­å®ç°äº†é²æ£’ä¸”ç¨³å®šçš„åº¦é‡æ·±åº¦é¢„æµ‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸åŒè·ç¦»å°ºåº¦æ—¶è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ•°æ®é©±åŠ¨èŒƒå¼åœ¨å…¨æ™¯æ·±åº¦ä¼°è®¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ•°æ®æ„å»ºå’Œæ¨¡å‹ä¼˜åŒ–ç­–ç•¥ï¼ŒæˆåŠŸå®ç°äº†è·¨åŸŸæ³›åŒ–ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ºå…¨æ™¯è§†è§‰ç†è§£æä¾›äº†æ–°çš„åŸºç¡€æ¨¡å‹ï¼Œå…¶å³æ’å³ç”¨çš„ä¼˜åŒ–ç»„ä»¶å’Œå‡ ä½•ä¸€è‡´æ€§çº¦æŸä¸ºæœªæ¥ç›¸å…³ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒæ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}


### [15] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li, Sicheng Gao, Meng Meng, Xu Zhou, Manyuan Zhang, Yuzhang Shang, Xiangyu Yue*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†AdaTooler-Vï¼Œä¸€ç§èƒ½å¤Ÿè‡ªé€‚åº”å†³å®šä½•æ—¶ä½¿ç”¨è§†è§‰å·¥å…·çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥AT-GRPOå¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œæ„å»ºå¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†ï¼Œæ˜¾è‘—å‡å°‘äº†ä¸å¿…è¦çš„å·¥å…·è°ƒç”¨å¼€é”€å¹¶æå‡äº†è§†è§‰æ¨ç†æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨ç›²ç›®å·¥å…·ä½¿ç”¨é—®é¢˜ï¼Œå³ä½¿åœ¨ä¸å¿…è¦æ—¶ä¹Ÿä¼šè°ƒç”¨è§†è§‰å·¥å…·ï¼Œè¿™æ˜¾è‘—å¢åŠ äº†æ¨ç†å¼€é”€å¹¶é™ä½äº†æ¨¡å‹æ€§èƒ½ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿè‡ªé€‚åº”å†³å®šä½•æ—¶çœŸæ­£éœ€è¦å·¥å…·çš„æ–¹æ³•ã€‚

**Method:** æå‡ºäº†AdaTooler-Væ¨¡å‹ï¼Œé‡‡ç”¨AT-GRPOå¼ºåŒ–å­¦ä¹ ç®—æ³•æ ¹æ®æ¯ä¸ªæ ·æœ¬çš„å·¥å…·æ•ˆç›Šè¯„åˆ†è‡ªé€‚åº”è°ƒæ•´å¥–åŠ±å°ºåº¦ï¼Œé¼“åŠ±æ¨¡å‹ä»…åœ¨å·¥å…·èƒ½æä¾›çœŸæ­£æ”¹è¿›æ—¶æ‰è°ƒç”¨ï¼›åŒæ—¶æ„å»ºäº†ä¸¤ä¸ªè®­ç»ƒæ•°æ®é›†ï¼šAdaTooler-V-CoT-100kç”¨äºç›‘ç£å¾®è°ƒå†·å¯åŠ¨ï¼ŒAdaTooler-V-300kç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œè¦†ç›–å•å›¾åƒã€å¤šå›¾åƒå’Œè§†é¢‘æ•°æ®ã€‚

**Result:** åœ¨åäºŒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜AdaTooler-Vå…·æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨å¤šæ ·è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼›ç‰¹åˆ«æ˜¯AdaTooler-V-7Båœ¨é«˜åˆ†è¾¨ç‡åŸºå‡†V*ä¸Šè¾¾åˆ°89.8%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å•†ä¸šä¸“æœ‰æ¨¡å‹GPT-4oå’ŒGemini 1.5 Proã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†è‡ªé€‚åº”å·¥å…·ä½¿ç”¨ç­–ç•¥åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„é‡è¦æ€§ï¼Œé€šè¿‡å‡å°‘ä¸å¿…è¦çš„å·¥å…·è°ƒç”¨å¯ä»¥åŒæ—¶æå‡æ€§èƒ½å’Œæ•ˆç‡ï¼›AT-GRPOç®—æ³•ä¸ºå¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°æ€è·¯ï¼Œå¼€æºä»£ç ã€æ¨¡å‹å’Œæ•°æ®å°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.


### [16] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng, Wen Wang, Ka Leong Cheng, Shuailei Ma, Qingyan Bai, Yixuan Li, Cheng Chen, Yanhong Zeng, Xing Zhu, Yujun Shen, Qifeng Chen*

#### ğŸ§© TL;DR
WorldCanvasæ˜¯ä¸€ä¸ªç”¨äºå¯æç¤ºä¸–ç•Œäº‹ä»¶çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬ã€è½¨è¿¹å’Œå‚è€ƒå›¾åƒå®ç°ç”¨æˆ·å¯¼å‘çš„ä¸°å¯Œæ¨¡æ‹Ÿï¼Œå°†ä¸–ç•Œæ¨¡å‹ä»è¢«åŠ¨é¢„æµ‹å™¨è½¬å˜ä¸ºäº¤äº’å¼ã€ç”¨æˆ·å¯å¡‘é€ çš„æ¨¡æ‹Ÿå™¨ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼šçº¯æ–‡æœ¬æ–¹æ³•è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œç°æœ‰è½¨è¿¹æ§åˆ¶çš„å›¾åƒåˆ°è§†é¢‘æ–¹æ³•ç¼ºä¹è¯­ä¹‰æ„å›¾å’Œè§†è§‰åŸºç¡€ã€‚ç ”ç©¶æ—¨åœ¨è§£å†³å¤šæ¨¡æ€ä¸–ç•Œäº‹ä»¶ç”Ÿæˆçš„ç©ºç™½ï¼Œå®ç°æ›´ä¸°å¯Œã€ç”¨æˆ·å¯æ§çš„æ¨¡æ‹Ÿï¼Œæ”¯æŒå¤šæ™ºèƒ½ä½“äº¤äº’ã€å¯¹è±¡è¿›å‡ºã€å‚è€ƒå¼•å¯¼å¤–è§‚å’Œåç›´è§‰äº‹ä»¶ç­‰å¤æ‚åœºæ™¯ã€‚

**Method:** WorldCanvasé‡‡ç”¨å¤šæ¨¡æ€æ–¹æ³•ï¼Œå°†è½¨è¿¹ï¼ˆç¼–ç è¿åŠ¨ã€æ—¶åºå’Œå¯è§æ€§ï¼‰ä¸è‡ªç„¶è¯­è¨€ï¼ˆè¯­ä¹‰æ„å›¾ï¼‰å’Œå‚è€ƒå›¾åƒï¼ˆå¯¹è±¡èº«ä»½çš„è§†è§‰åŸºç¡€ï¼‰ç›¸ç»“åˆã€‚è¯¥æ¡†æ¶é€šè¿‡è½¨è¿¹æ§åˆ¶è¿åŠ¨æ¨¡å¼ï¼Œè¯­è¨€æŒ‡å¯¼è¯­ä¹‰å†…å®¹ï¼Œå‚è€ƒå›¾åƒç¡®ä¿å¯¹è±¡è§†è§‰ä¸€è‡´æ€§ï¼Œä»è€Œç”Ÿæˆè¿è´¯å¯æ§çš„äº‹ä»¶åºåˆ—ã€‚

**Result:** ç”Ÿæˆçš„è§†é¢‘ä¸ä»…å±•ç¤ºæ—¶é—´è¿è´¯æ€§ï¼Œè¿˜è¡¨ç°å‡ºæ¶Œç°ä¸€è‡´æ€§ï¼Œèƒ½å¤Ÿåœ¨å¯¹è±¡æš‚æ—¶æ¶ˆå¤±æ—¶ä¿æŒå¯¹è±¡èº«ä»½å’Œåœºæ™¯è¿ç»­æ€§ã€‚æ¡†æ¶æ”¯æŒå¤šæ™ºèƒ½ä½“äº¤äº’ã€å¯¹è±¡è¿›å…¥/é€€å‡ºã€å‚è€ƒå¼•å¯¼å¤–è§‚å’Œåç›´è§‰äº‹ä»¶ç­‰å¤æ‚ä¸–ç•Œäº‹ä»¶çš„ç”Ÿæˆï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•çš„è¡¨è¾¾èƒ½åŠ›ã€‚

**Conclusion:** WorldCanvasé€šè¿‡æ”¯æŒè¡¨è¾¾æ€§ä¸–ç•Œäº‹ä»¶ç”Ÿæˆï¼Œå°†ä¸–ç•Œæ¨¡å‹ä»è¢«åŠ¨é¢„æµ‹å™¨æ¨è¿›ä¸ºäº¤äº’å¼ã€ç”¨æˆ·å¯å¡‘é€ çš„æ¨¡æ‹Ÿå™¨ã€‚è¯¥ç ”ç©¶ä¸ºå¯æ§è§†é¢‘ç”Ÿæˆå’Œå¤šæ¨¡æ€æ¨¡æ‹Ÿå¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå®ç°äº†æ›´ä¸°å¯Œã€æ›´çµæ´»çš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ï¼Œä¸ºæœªæ¥äº¤äº’å¼AIç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [17] [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899)
*Yushi Hu, Reyhane Askari-Hemmat, Melissa Hall, Emily Dinan, Luke Zettlemoyer, Marjan Ghazvininejad*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†é¦–ä¸ªé’ˆå¯¹å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„ç»¼åˆåŸºå‡†MMRB2ï¼Œæ¶µç›–å›¾åƒç”Ÿæˆã€ç¼–è¾‘ã€äº¤é”™ç”Ÿæˆå’Œæ¨ç†å››å¤§ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨è¯¥åŸºå‡†è¯„ä¼°äº†ç°æœ‰å¤šæ¨¡æ€è¯„åˆ¤ç³»ç»Ÿçš„æ€§èƒ½ï¼Œæ­ç¤ºäº†å½“å‰å¥–åŠ±æ¨¡å‹ä¸äººç±»ä¸“å®¶ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¥–åŠ±æ¨¡å‹å¯¹äºè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†åœ¨å¤„ç†äº¤é”™å›¾åƒå’Œæ–‡æœ¬åºåˆ—çš„å…¨èƒ½æ¨¡å‹é¢†åŸŸä»ç¼ºä¹æ·±å…¥æ¢ç´¢ï¼Œç°æœ‰ç ”ç©¶ç¼ºä¹é’ˆå¯¹å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä»»åŠ¡çš„ç»¼åˆè¯„ä¼°åŸºå‡†ï¼Œæ— æ³•æœ‰æ•ˆè¡¡é‡å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†Multimodal RewardBench 2åŸºå‡†ï¼Œæ¶µç›–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘ã€äº¤é”™ç”Ÿæˆå’Œå¤šæ¨¡æ€æ¨ç†å››å¤§ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«1000ä¸ªä¸“å®¶æ ‡æ³¨çš„åå¥½å¯¹ï¼Œæ•°æ®æ¥è‡ª23ä¸ªæ¨¡å‹å’Œä»£ç†åœ¨21ä¸ªæºä»»åŠ¡ä¸Šçš„å“åº”ï¼Œé‡‡ç”¨é›†æˆè¿‡æ»¤ç­–ç•¥ç¡®ä¿åå¥½å¯¹å…·æœ‰å¼ºäººç±»ä¸“å®¶å…±è¯†ã€‚

**Result:** æœ€æ–°Gemini 3 Proæ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°75-80%å‡†ç¡®ç‡ï¼ŒGPT-5å’ŒGemini 2.5 Proè¾¾åˆ°66-75%ï¼Œæ˜¾è‘—ä¼˜äºå¹¿æ³›ä½¿ç”¨çš„GPT-4oçš„59%ï¼Œæœ€ä½³å¼€æºæ¨¡å‹Qwen3-VL-32Bä¸Gemini 2.5 Flashæ€§èƒ½ç›¸å½“è¾¾åˆ°64%ï¼Œä½†ä¸äººç±»ä¸“å®¶è¶…è¿‡90%çš„å‡†ç¡®ç‡ä»æœ‰æ˜¾è‘—å·®è·ã€‚

**Conclusion:** MMRB2åŸºå‡†æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ä¸äººç±»ä¸“å®¶åˆ¤æ–­ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œå…¶è¡¨ç°ä¸ä¸‹æ¸¸ä»»åŠ¡æˆåŠŸç‡å¼ºç›¸å…³ï¼Œä¸ºæœªæ¥æ”¹è¿›å¥–åŠ±æ¨¡å‹æä¾›äº†å…³é”®æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„åå¥½å»ºæ¨¡æ–¹é¢éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚

---

#### ğŸ“„ Abstract
Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.
