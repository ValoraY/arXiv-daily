<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-11-13.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 21]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 8]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 3]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-rs-net-context-aware-relation-scoring-for-dynamic-scene-graph-generation">[1] <a href="https://arxiv.org/abs/2511.08651">RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation</a></h3>
<p><em>Hae-Won Jo, Yeong-Jun Cho</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºå…³ç³»è¯„åˆ†ç½‘ç»œï¼ˆRS-Netï¼‰ï¼Œä¸€ä¸ªç”¨äºåŠ¨æ€åœºæ™¯å›¾ç”Ÿæˆçš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œé€šè¿‡è¯„ä¼°å¯¹è±¡å¯¹çš„ç©ºé—´å’Œæ—¶é—´ä¸Šä¸‹æ–‡é‡è¦æ€§æ¥å¢å¼ºå…³ç³»é¢„æµ‹ã€‚è¯¥æ¡†æ¶å¯æ— ç¼é›†æˆåˆ°ç°æœ‰DSGGæ¨¡å‹ä¸­ï¼Œåœ¨Action Genomeæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†å¬å›ç‡å’Œç²¾ç¡®ç‡ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŠ¨æ€åœºæ™¯å›¾ç”Ÿæˆæ–¹æ³•ä»…åœ¨æœ‰æ ‡æ³¨çš„å¯¹è±¡å¯¹ä¸Šè®­ç»ƒï¼Œç¼ºä¹å¯¹éç›¸å…³å¯¹è±¡å¯¹çš„æŒ‡å¯¼ï¼Œå¯¼è‡´åœ¨æ¨ç†é˜¶æ®µéš¾ä»¥è¯†åˆ«æœ‰æ„ä¹‰çš„å…³ç³»ã€‚è¿™ç§è®­ç»ƒæ–¹å¼ä½¿å¾—æ¨¡å‹åœ¨å…³ç³»é¢„æµ‹æ—¶å­˜åœ¨æ˜æ˜¾å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> RS-NetåŒ…å«ç©ºé—´ä¸Šä¸‹æ–‡ç¼–ç å™¨å’Œæ—¶é—´ç¼–ç å™¨ï¼Œç©ºé—´ç¼–ç å™¨ä½¿ç”¨å¯å­¦ä¹ çš„ä¸Šä¸‹æ–‡ä»¤ç‰Œæ•è·ç©ºé—´äº¤äº’ï¼Œæ—¶é—´ç¼–ç å™¨èšåˆè§†é¢‘çº§ä¿¡æ¯ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»Ÿä¸€çš„ä¸‰å…ƒç»„è¯„åˆ†æœºåˆ¶å°†å…³ç³»è¯„åˆ†æ•´åˆåˆ°å…³ç³»é¢„æµ‹ä¸­ã€‚</p>
<p><strong>Result:</strong> åœ¨Action Genomeæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRS-Netåœ¨ä¸åŒåŸºçº¿ä¸ŠæŒç»­æå‡äº†å¬å›ç‡å’Œç²¾ç¡®ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹³å‡å¬å›ç‡ä¸Šå–å¾—æ˜¾è‘—å¢ç›Šï¼Œæœ‰æ•ˆåº”å¯¹äº†å…³ç³»çš„é•¿å°¾åˆ†å¸ƒé—®é¢˜ã€‚å°½ç®¡å‚æ•°æ•°é‡å¢åŠ ï¼Œä½†ä¿æŒäº†ç«äº‰åŠ›çš„æ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> RS-Netè¯æ˜äº†é€šè¿‡è¯„ä¼°å¯¹è±¡å¯¹ä¸Šä¸‹æ–‡é‡è¦æ€§å¯ä»¥æœ‰æ•ˆæ”¹å–„åŠ¨æ€åœºæ™¯å›¾ç”Ÿæˆæ€§èƒ½ï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰æ–¹æ³•ä¸­ï¼Œä¸ºè§£å†³å…³ç³»é¢„æµ‹ä¸­çš„é•¿å°¾åˆ†å¸ƒé—®é¢˜æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Dynamic Scene Graph Generation (DSGG) models how object relations evolve over time in videos. However, existing methods are trained only on annotated object pairs and lack guidance for non-related pairs, making it difficult to identify meaningful relations during inference. In this paper, we propose Relation Scoring Network (RS-Net), a modular framework that scores the contextual importance of object pairs using both spatial interactions and long-range temporal context. RS-Net consists of a spatial context encoder with learnable context tokens and a temporal encoder that aggregates video-level information. The resulting relation scores are integrated into a unified triplet scoring mechanism to enhance relation prediction. RS-Net can be easily integrated into existing DSGG models without architectural changes. Experiments on the Action Genome dataset show that RS-Net consistently improves both Recall and Precision across diverse baselines, with notable gains in mean Recall, highlighting its ability to address the long-tailed distribution of relations. Despite the increased number of parameters, RS-Net maintains competitive efficiency, achieving superior performance over state-of-the-art methods.</p>
<h3 id="2-privacy-beyond-pixels-latent-anonymization-for-privacy-preserving-video-understanding">[2] <a href="https://arxiv.org/abs/2511.08666">Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding</a></h3>
<p><em>Joseph Fioresi, Ishan Rajendrakumar Dave, Mubarak Shah</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿è¡Œçš„è§†è§‰éšç§ä¿æŠ¤æ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§åŒ¿ååŒ–é€‚é…å™¨æ¨¡å—ï¼ˆAAMï¼‰ä»è§†é¢‘ç‰¹å¾ä¸­ç§»é™¤éšç§ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒé€šç”¨ä»»åŠ¡æ•ˆç”¨ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°äº†35%çš„éšç§æ³„éœ²å‡å°‘ï¼ŒåŒæ—¶ç»´æŒæ¥è¿‘åŸºçº¿çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘åŸºç¡€æ¨¡å‹æå–çš„æ—¶ç©ºç‰¹å¾åœ¨å…±äº«æˆ–å­˜å‚¨æ—¶ä¼šæ— æ„ä¸­æ³„éœ²æ•æ„Ÿä¸ªäººä¿¡æ¯ï¼Œå¦‚è‚¤è‰²ã€æ€§åˆ«æˆ–æœè£…ç­‰ã€‚ç°æœ‰çš„éšç§ä¿æŠ¤æ–¹æ³•ä¸»è¦å…³æ³¨è¾“å…¥åƒç´ çº§çš„åŒ¿ååŒ–ï¼Œéœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªå®ç”¨è§†é¢‘æ¨¡å‹ï¼Œå¯¼è‡´ä»»åŠ¡ç‰¹å®šçš„åŒ¿ååŒ–ï¼Œä¸é€‚ç”¨äºç°ä»£è§†é¢‘åŸºç¡€æ¨¡å‹ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†è½»é‡çº§åŒ¿ååŒ–é€‚é…å™¨æ¨¡å—ï¼ˆAAMï¼‰ï¼Œå¯åœ¨å†»ç»“çš„è§†é¢‘ç¼–ç å™¨ä¸Šå³æ’å³ç”¨åœ°ç§»é™¤éšç§ä¿¡æ¯ã€‚æ¡†æ¶é‡‡ç”¨ä¸‰ä¸ªæ–°è®¾è®¡çš„è®­ç»ƒç›®æ ‡ï¼šå‰ªè¾‘çº§è‡ªç›‘ç£éšç§ç›®æ ‡ä»¥å‡å°‘é™æ€å‰ªè¾‘é—´çš„äº’ä¿¡æ¯ï¼ŒååŒè®­ç»ƒç›®æ ‡ä»¥ä¿ç•™å·²è§ä»»åŠ¡çš„æ•ˆç”¨ï¼Œä»¥åŠæ½œåœ¨ä¸€è‡´æ€§æŸå¤±ä»¥åœ¨æœªè§ä»»åŠ¡ä¸Šå®ç°æ³›åŒ–ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºéšç§æ³„éœ²æ˜¾è‘—å‡å°‘35%ï¼ŒåŒæ—¶åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ä¿æŒæ¥è¿‘åŸºçº¿çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬åŠ¨ä½œè¯†åˆ«ï¼ˆKinetics400ã€UCF101ã€HMDB51ï¼‰ã€æ—¶åºåŠ¨ä½œæ£€æµ‹ï¼ˆTHUMOS14ï¼‰å’Œå¼‚å¸¸æ£€æµ‹ï¼ˆUCF-Crimeï¼‰ã€‚è¯¥æ–¹æ³•è¿˜èƒ½æœ‰æ•ˆç¼“è§£åŠ¨ä½œè¯†åˆ«æ¨¡å‹ä¸­çš„æ€§åˆ«åè§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œéšç§ä¿æŠ¤çš„å¯è¡Œæ€§ï¼Œä¸ºè§†é¢‘åŸºç¡€æ¨¡å‹æä¾›äº†é«˜æ•ˆä¸”é€šç”¨çš„éšç§ä¿æŠ¤è§£å†³æ–¹æ¡ˆã€‚æå‡ºçš„æ–¹æ³•ä¸ä»…å‡å°‘äº†éšç§æ³„éœ²ï¼Œè¿˜ä¿ƒè¿›äº†æ›´å…¬å¹³çš„è§†é¢‘ç†è§£ï¼Œä¸ºæœªæ¥è§†é¢‘éšç§ä¿æŠ¤ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel-level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug-and-play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.</p>
<h3 id="3-sift-graph-benchmarking-multimodal-defense-against-image-adversarial-attacks-with-robust-feature-graph">[3] <a href="https://arxiv.org/abs/2511.08810">SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph</a></h3>
<p><em>Jingjie He, Weijie Liang, Zihan Shan, Matthew Caesar</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSIFT-Graphå¤šæ¨¡æ€é˜²å¾¡æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå°ºåº¦ä¸å˜ç‰¹å¾å˜æ¢å…³é”®ç‚¹å’Œå›¾æ³¨æ„åŠ›ç½‘ç»œï¼Œæ„å»ºå¯¹å¯¹æŠ—æ”»å‡»å…·æœ‰é²æ£’æ€§çš„ç»“æ„æ„ŸçŸ¥è§†è§‰æ¨¡å‹ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆæå‡äº†ä¼ ç»Ÿè§†è§‰æ¨¡å‹åœ¨æ¢¯åº¦ç™½ç›’æ”»å‡»ä¸‹çš„é²æ£’æ€§ï¼ŒåŒæ—¶ä»…å¸¦æ¥è½»å¾®å¹²å‡€å‡†ç¡®ç‡ä¸‹é™ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¯¹æŠ—æ”»å‡»æš´éœ²äº†ç°ä»£æ·±åº¦è§†è§‰æ¨¡å‹å¯¹å¯†é›†åƒç´ çº§è¡¨ç¤ºçš„ä¾èµ–è„†å¼±æ€§ï¼Œè¿™äº›è¡¨ç¤ºå¯¹å¾®å°æ‰°åŠ¨é«˜åº¦æ•æ„Ÿã€‚ä¼ ç»Ÿé˜²å¾¡ç­–ç•¥é€šå¸¸åœ¨è¿™ç§è„†å¼±çš„åƒç´ åŸŸå†…æ“ä½œï¼Œç¼ºä¹æ•´åˆå›ºæœ‰é²æ£’è§†è§‰ç‰¹å¾çš„æœºåˆ¶ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿåˆ©ç”¨ç»“æ„ä¿¡æ¯çš„é˜²å¾¡æ¡†æ¶æ¥å¢å¼ºæ¨¡å‹é²æ£’æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºSIFT-Graphå¤šæ¨¡æ€é˜²å¾¡æ¡†æ¶ï¼Œæ•´åˆå°ºåº¦ä¸å˜ç‰¹å¾å˜æ¢å…³é”®ç‚¹ä¸å›¾æ³¨æ„åŠ›ç½‘ç»œï¼Œæå–å¯¹å°ºåº¦å’Œæ—‹è½¬ä¸å˜çš„å±€éƒ¨ç»“æ„ç‰¹å¾ã€‚è¿™äº›é²æ£’ç‰¹å¾åµŒå…¥ä¸ä¼ ç»Ÿè§†è§‰æ¨¡å‹èåˆï¼Œå½¢æˆç»Ÿä¸€çš„ç»“æ„æ„ŸçŸ¥é˜²å¾¡æ¨¡å‹ï¼Œæ”¯æŒVision Transformerå’Œå·ç§¯ç¥ç»ç½‘ç»œç­‰å¤šç§æ¶æ„ã€‚</p>
<p><strong>Result:</strong> åˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡è§†è§‰æ¨¡å‹å¯¹åŸºäºæ¢¯åº¦çš„ç™½ç›’å¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§ï¼ŒåŒæ—¶ä»…å¯¼è‡´å¹²å‡€å‡†ç¡®ç‡çš„å°å¹…ä¸‹é™ï¼Œåœ¨ä¿æŒæ¨¡å‹åŸæœ‰æ€§èƒ½çš„åŸºç¡€ä¸Šæ˜¾è‘—å¢å¼ºäº†é˜²å¾¡èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡æ•´åˆæ‰‹å·¥ç‰¹å¾å’Œæ·±åº¦å­¦ä¹ ç‰¹å¾çš„å¤šæ¨¡æ€æ–¹æ³•ï¼Œä¸ºè§†è§‰æ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚ç»“æ„æ„ŸçŸ¥ç‰¹å¾èåˆç­–ç•¥å±•ç¤ºäº†åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶å¢å¼ºå®‰å…¨æ€§çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥é²æ£’è§†è§‰ç³»ç»Ÿè®¾è®¡æä¾›äº†é‡è¦å‚è€ƒæ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Adversarial attacks expose a fundamental vulnerability in modern deep vision models by exploiting their dependence on dense, pixel-level representations that are highly sensitive to imperceptible perturbations. Traditional defense strategies typically operate within this fragile pixel domain, lacking mechanisms to incorporate inherently robust visual features. In this work, we introduce SIFT-Graph, a multimodal defense framework that enhances the robustness of traditional vision models by aggregating structurally meaningful features extracted from raw images using both handcrafted and learned modalities. Specifically, we integrate Scale-Invariant Feature Transform keypoints with a Graph Attention Network to capture scale and rotation invariant local structures that are resilient to perturbations. These robust feature embeddings are then fused with traditional vision model, such as Vision Transformer and Convolutional Neural Network, to form a unified, structure-aware and perturbation defensive model. Preliminary results demonstrate that our method effectively improves the visual model robustness against gradient-based white box adversarial attacks, while incurring only a marginal drop in clean accuracy.</p>
<h3 id="4-improve-contrastive-clustering-performance-by-multiple-fusing-augmenting-vit-blocks">[4] <a href="https://arxiv.org/abs/2511.08883">Improve Contrastive Clustering Performance by Multiple Fusing-Augmenting ViT Blocks</a></h3>
<p><em>Cheng Wang, Shuisheng Zhou, Fengjiao Peng, Jin Sheng, Feng Ye, Yinli Dong</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºVision Transformerçš„å¤šé‡èåˆå¢å¼ºå—ï¼ˆMFAVBsï¼‰ï¼Œé€šè¿‡æ˜¾å¼èåˆæ­£æ ·æœ¬å¯¹çš„ç‰¹å¾æ¥æ”¹è¿›å¯¹æ¯”å­¦ä¹ ç½‘ç»œåœ¨å›¾åƒèšç±»ä¸­çš„æ€§èƒ½ï¼Œåœ¨ä¸ƒä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„èšç±»æ•ˆæœã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¯¹æ¯”å­¦ä¹ ç½‘ç»œé€šè¿‡å‚æ•°å…±äº«æˆ–åŠ¨é‡æ›´æ–°å®ç°ç¼–ç å™¨é—´çš„éšå¼äº¤äº’ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨æ­£æ ·æœ¬å¯¹çš„äº’è¡¥æ€§å’Œç›¸ä¼¼æ€§æ¥æå–èšç±»ç‰¹å¾ï¼Œé™åˆ¶äº†èšç±»æ€§èƒ½çš„è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>Method:</strong> è®¾è®¡åŸºäºVision Transformerçš„å¤šé‡èåˆå¢å¼ºå—ï¼ˆMFAVBsï¼‰ï¼Œå°†ä¸¤ä¸ªå…±äº«æƒé‡çš„ViTè¾“å‡ºçš„ç‰¹å¾èåˆåè¾“å…¥æ›´å¤§çš„ViTï¼Œç„¶åå°†å­¦ä¹ åˆ°çš„ç‰¹å¾åˆ†å‰²ä¸ºæ–°çš„å¢å¼ºæ­£æ ·æœ¬å¯¹ä¼ é€’ç»™åç»­FAVBsï¼Œå®ç°å¤šæ¬¡èåˆå’Œå¢å¼ºï¼Œæœ€åå°†ç‰¹å¾æŠ•å½±åˆ°å®ä¾‹çº§å’Œèšç±»çº§ç©ºé—´è®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸ƒä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå°†MFAVBsä½œä¸ºå¯¹æ¯”èšç±»éª¨å¹²ç½‘ç»œï¼Œåœ¨èšç±»æ€§èƒ½æ–¹é¢è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡æ˜¾å¼èåˆæ­£æ ·æœ¬å¯¹ç‰¹å¾å¹¶åˆ©ç”¨CLIPé¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾æå–èƒ½åŠ›ï¼ŒMFAVBsèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ åŒºåˆ†æ€§èšç±»ç‰¹å¾ï¼Œä¸ºå¯¹æ¯”å­¦ä¹ åœ¨å›¾åƒèšç±»ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>In the field of image clustering, the widely used contrastive learning networks improve clustering performance by maximizing the similarity between positive pairs and the dissimilarity of negative pairs of the inputs. Extant contrastive learning networks, whose two encoders often implicitly interact with each other by parameter sharing or momentum updating, may not fully exploit the complementarity and similarity of the positive pairs to extract clustering features from input data. To explicitly fuse the learned features of positive pairs, we design a novel multiple fusing-augmenting ViT blocks (MFAVBs) based on the excellent feature learning ability of Vision Transformers (ViT). Firstly, two preprocessed augmentions as positive pairs are separately fed into two shared-weight ViTs, then their output features are fused to input into a larger ViT. Secondly, the learned features are split into a pair of new augmented positive samples and passed to the next FAVBs, enabling multiple fusion and augmention through MFAVBs operations. Finally, the learned features are projected into both instance-level and clustering-level spaces to calculate the cross-entropy loss, followed by parameter updates by backpropagation to finalize the training process. To further enhance ability of the model to distinguish between similar images, our input data for the network we propose is preprocessed augmentions with features extracted from the CLIP pretrained model. Our experiments on seven public datasets demonstrate that MFAVBs serving as the backbone for contrastive clustering outperforms the state-of-the-art techniques in terms of clustering performance.</p>
<h3 id="5-asymmetric-cross-modal-knowledge-distillation-bridging-modalities-with-weak-semantic-consistency">[5] <a href="https://arxiv.org/abs/2511.08901">Asymmetric Cross-Modal Knowledge Distillation: Bridging Modalities with Weak Semantic Consistency</a></h3>
<p><em>Riling Wei, Kelu Yao, Chuanguang Yang, Jin Wang, Zhuoyan Gao, Chao Li</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§éå¯¹ç§°è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦æ¡†æ¶SemBridgeï¼Œé€šè¿‡å­¦ç”Ÿå‹å¥½åŒ¹é…æ¨¡å—å’Œè¯­ä¹‰æ„ŸçŸ¥çŸ¥è¯†å¯¹é½æ¨¡å—ï¼Œè§£å†³äº†å¼±è¯­ä¹‰ä¸€è‡´æ€§ä¸‹è·¨æ¨¡æ€çŸ¥è¯†ä¼ è¾“çš„æŒ‘æˆ˜ï¼Œåœ¨é¥æ„Ÿåœºæ™¯åˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿå¯¹ç§°è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ä¾èµ–äºå¼ºè¯­ä¹‰å…³è”çš„é…å¯¹æ¨¡æ€æ•°æ®ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­é…å¯¹æ•°æ®ç¨€ç¼ºï¼Œå› æ­¤éœ€è¦ç ”ç©¶å¼±è¯­ä¹‰ä¸€è‡´æ€§ä¸‹çš„éå¯¹ç§°è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ï¼Œä»¥è¿æ¥è¯­ä¹‰é‡å æœ‰é™çš„æ¨¡æ€ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†SemBridgeæ¡†æ¶ï¼ŒåŒ…å«å­¦ç”Ÿå‹å¥½åŒ¹é…æ¨¡å—å’Œè¯­ä¹‰æ„ŸçŸ¥çŸ¥è¯†å¯¹é½æ¨¡å—ã€‚å‰è€…åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ è·å–è¯­ä¹‰çŸ¥è¯†å¹¶ä¸ºæ¯ä¸ªå­¦ç”Ÿæ ·æœ¬åŠ¨æ€é€‰æ‹©ç›¸å…³æ•™å¸ˆæ ·æœ¬æä¾›ä¸ªæ€§åŒ–æŒ‡å¯¼ï¼Œåè€…é€šè¿‡æ‹‰æ ¼æœ—æ—¥ä¼˜åŒ–å¯»æ‰¾æœ€ä¼˜ä¼ è¾“è·¯å¾„ã€‚</p>
<p><strong>Result:</strong> åœ¨ä»å¤šå…‰è°±å’Œä¸å¯¹ç§°RGBå›¾åƒæ„å»ºçš„é¥æ„Ÿåœºæ™¯åˆ†ç±»åŸºå‡†æ•°æ®é›†ä¸Šï¼Œä¸7ç§ç°æœ‰æ–¹æ³•å’Œ6ç§ä¸åŒæ¨¡å‹æ¶æ„ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨å„ç§æ•°æ®é›†ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åœ¨å¼±è¯­ä¹‰ä¸€è‡´æ€§æ¡ä»¶ä¸‹å®ç°æœ‰æ•ˆè·¨æ¨¡æ€çŸ¥è¯†è’¸é¦çš„å¯è¡Œæ€§ï¼Œä¸ºå®é™…åº”ç”¨ä¸­æ¨¡æ€é…å¯¹å—é™çš„åœºæ™¯æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œå¹¶åŸºäºæœ€ä¼˜ä¼ è¾“ç†è®ºéªŒè¯äº†çŸ¥è¯†ä¼ è¾“æˆæœ¬å¢åŠ çš„æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Cross-modal Knowledge Distillation has demonstrated promising performance on paired modalities with strong semantic connections, referred to as Symmetric Cross-modal Knowledge Distillation (SCKD). However, implementing SCKD becomes exceedingly constrained in real-world scenarios due to the limited availability of paired modalities. To this end, we investigate a general and effective knowledge learning concept under weak semantic consistency, dubbed Asymmetric Cross-modal Knowledge Distillation (ACKD), aiming to bridge modalities with limited semantic overlap. Nevertheless, the shift from strong to weak semantic consistency improves flexibility but exacerbates challenges in knowledge transmission costs, which we rigorously verified based on optimal transport theory. To mitigate the issue, we further propose a framework, namely SemBridge, integrating a Student-Friendly Matching module and a Semantic-aware Knowledge Alignment module. The former leverages self-supervised learning to acquire semantic-based knowledge and provide personalized instruction for each student sample by dynamically selecting the relevant teacher samples. The latter seeks the optimal transport path by employing Lagrangian optimization. To facilitate the research, we curate a benchmark dataset derived from two modalities, namely Multi-Spectral (MS) and asymmetric RGB images, tailored for remote sensing scene classification. Comprehensive experiments exhibit that our framework achieves state-of-the-art performance compared with 7 existing approaches on 6 different model architectures across various datasets.</p>
<h3 id="6-llm-guided-probabilistic-fusion-for-label-efficient-document-layout-analysis">[6] <a href="https://arxiv.org/abs/2511.08903">LLM-Guided Probabilistic Fusion for Label-Efficient Document Layout Analysis</a></h3>
<p><em>Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§èåˆè§†è§‰æ£€æµ‹ä¸LLMç»“æ„å…ˆéªŒçš„åŠç›‘ç£æ–‡æ¡£å¸ƒå±€ç†è§£æ¡†æ¶ï¼Œé€šè¿‡æ¦‚ç‡åŠ æƒæ–¹æ³•å°†OCR-LLMæ¨æ–­çš„å±‚æ¬¡åŒºåŸŸä¸æ•™å¸ˆæ£€æµ‹å™¨è¾“å‡ºç›¸ç»“åˆï¼Œåœ¨è½»é‡çº§å’Œé¢„è®­ç»ƒæ¶æ„ä¸Šå‡å®ç°æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ–‡æ¡£å¸ƒå±€ç†è§£ä»»åŠ¡å°½ç®¡åŠç›‘ç£å­¦ä¹ æœ‰æ‰€è¿›å±•ï¼Œä½†ä»é¢ä¸´æ•°æ®å¯†é›†æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ–‡æœ¬é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹æä¾›çš„ç»“æ„åŒ–å…ˆéªŒçŸ¥è¯†æ¥å¢å¼ºæ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŸºäºæ¦‚ç‡åŠ æƒçš„èåˆæ¡†æ¶ï¼Œé€šè¿‡OCR-LLMæµæ°´çº¿æ¨æ–­æœªæ ‡æ³¨æ–‡æ¡£çš„å±‚æ¬¡åŒºåŸŸç»“æ„ï¼Œé‡‡ç”¨é€†æ–¹å·®èåˆå°†LLMç»“æ„å…ˆéªŒä¸æ•™å¸ˆæ£€æµ‹å™¨è¾“å‡ºç»“åˆç”Ÿæˆç²¾ç‚¼ä¼ªæ ‡ç­¾ï¼Œå¹¶å¼•å…¥å®ä¾‹è‡ªé€‚åº”é—¨æ§æœºåˆ¶ä¼˜åŒ–æƒé‡åˆ†é…ã€‚</p>
<p><strong>Result:</strong> åœ¨PubLayNetæ•°æ®é›†ä¸Šï¼Œè½»é‡çº§SwiftFormeréª¨å¹²ç½‘ç»œä»…ä½¿ç”¨5%æ ‡ç­¾è¾¾åˆ°88.2Â±0.3 APï¼Œæ–‡æ¡£é¢„è®­ç»ƒLayoutLMv3ç»“åˆæœ¬æ¡†æ¶è¾¾åˆ°89.7Â±0.4 APï¼Œè¶…è¶Šæ ‡å‡†åŠç›‘ç£å­¦ä¹ å¹¶åŒ¹é…éœ€è¦äº¿çº§å¤šæ¨¡æ€é¢„è®­ç»ƒçš„UDOPæ€§èƒ½ï¼›LLMæä¾›é’ˆå¯¹æ€§è¯­ä¹‰æ¶ˆæ­§åœ¨18.7%æ¡ˆä¾‹ä¸­å¸¦æ¥3.8 APå¢ç›Šã€‚</p>
<p><strong>Conclusion:</strong> LLMç»“æ„å…ˆéªŒä¸è§†è§‰æ£€æµ‹æ–¹æ³•å…·æœ‰äº’è¡¥æ€§ï¼Œå¯åŒæ—¶æå‡è½»é‡çº§å’Œé¢„è®­ç»ƒæ¶æ„æ€§èƒ½ï¼›å®ä¾‹è‡ªé€‚åº”é—¨æ§æœºåˆ¶ä¼˜äºå›ºå®šæƒé‡ï¼›å¼€æºLLMæ”¯æŒéšç§ä¿æŠ¤éƒ¨ç½²ä¸”æ€§èƒ½æŸå¤±æå°ï¼›ç³»ç»Ÿæˆæœ¬å¯æ§ï¼Œä¸ºæ–‡æ¡£å¸ƒå±€ç†è§£æä¾›äº†é«˜æ•ˆåŠç›‘ç£è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Document layout understanding remains data-intensive despite advances in semi-supervised learning. We present a framework that enhances semi-supervised detection by fusing visual predictions with structural priors from text-pretrained LLMs via principled probabilistic weighting. Given unlabeled documents, an OCR-LLM pipeline infers hierarchical regions which are combined with teacher detector outputs through inverse-variance fusion to generate refined pseudo-labels.Our method demonstrates consistent gains across model scales. With a lightweight SwiftFormer backbone (26M params), we achieve 88.2$\pm$0.3 AP using only 5\% labels on PubLayNet. When applied to document-pretrained LayoutLMv3 (133M params), our fusion framework reaches 89.7$\pm$0.4 AP, surpassing both LayoutLMv3 with standard semi-supervised learning (89.1$\pm$0.4 AP, p=0.02) and matching UDOP~\cite{udop} (89.8 AP) which requires 100M+ pages of multimodal pretraining. This demonstrates that LLM structural priors are complementary to both lightweight and pretrained architectures. Key findings include: (1) learned instance-adaptive gating improves over fixed weights by +0.9 AP with data-dependent PAC bounds correctly predicting convergence; (2) open-source LLMs enable privacy-preserving deployment with minimal loss (Llama-3-70B: 87.1 AP lightweight, 89.4 AP with LayoutLMv3); (3) LLMs provide targeted semantic disambiguation (18.7\% of cases, +3.8 AP gain) beyond simple text heuristics.Total system cost includes \$12 for GPT-4o-mini API or 17 GPU-hours for local Llama-3-70B per 50K pages, amortized across training runs.</p>
<h3 id="7-negative-entity-suppression-for-zero-shot-captioning-with-synthetic-images">[7] <a href="https://arxiv.org/abs/2511.08909">Negative Entity Suppression for Zero-Shot Captioning with Synthetic Images</a></h3>
<p><em>Zimao Lu, Hui Xu, Bing Liu, Ke Wang</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºè´Ÿå®ä½“æŠ‘åˆ¶ï¼ˆNESï¼‰æ–¹æ³•æ¥è§£å†³é›¶æ ·æœ¬å›¾åƒæè¿°ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œé€šè¿‡åˆæˆå›¾åƒç¡®ä¿æ£€ç´¢ä¸€è‡´æ€§ã€è¿‡æ»¤è´Ÿå®ä½“ä»¥åŠæ³¨æ„åŠ›çº§æŠ‘åˆ¶æ¥å‡å°‘è·¨åŸŸæè¿°ä¸­çš„é”™è¯¯å†…å®¹ç”Ÿæˆï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºçº¯æ–‡æœ¬è®­ç»ƒçš„é›¶æ ·æœ¬å›¾åƒæè¿°æ–¹æ³•è™½ç„¶èƒ½é¿å…å›¾åƒ-æ–‡æœ¬é…å¯¹æ•°æ®çš„æ”¶é›†æˆæœ¬ï¼Œä½†åœ¨è·¨åŸŸæ³›åŒ–æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå®¹æ˜“åœ¨é‡åˆ°æ–°è§†è§‰ç¯å¢ƒæ—¶äº§ç”Ÿå¹»è§‰å†…å®¹ã€‚æ£€ç´¢å¼æ–¹æ³•è¯•å›¾é€šè¿‡å¤–éƒ¨çŸ¥è¯†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å½“æ£€ç´¢åˆ°çš„æè¿°åŒ…å«ä¸è¾“å…¥æ— å…³çš„å®ä½“æ—¶åè€Œä¼šåŠ å‰§å¹»è§‰é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„è´Ÿå®ä½“æŠ‘åˆ¶ï¼ˆNESï¼‰æ–¹æ³•åŒ…å«ä¸‰ä¸ªé›†æˆé˜¶æ®µï¼šé¦–å…ˆä½¿ç”¨åˆæˆå›¾åƒç¡®ä¿è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­çš„å›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢ä¸€è‡´æ€§ï¼›å…¶æ¬¡ä»æ£€ç´¢å†…å®¹ä¸­è¿‡æ»¤è´Ÿå®ä½“ä»¥æé«˜å‡†ç¡®æ€§ï¼›æœ€ååˆ©ç”¨è¯†åˆ«å‡ºçš„è´Ÿå®ä½“è¿›è¡Œæ³¨æ„åŠ›çº§æŠ‘åˆ¶ï¼Œè¿›ä¸€æ­¥å‡å°‘æ˜“äº§ç”Ÿå¹»è§‰çš„ç‰¹å¾å½±å“ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒNESåœ¨ä¿æŒç«äº‰æ€§åŸŸå†…æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æ”¹å–„äº†è·¨åŸŸè¿ç§»èƒ½åŠ›å¹¶é™ä½äº†å¹»è§‰ç‡ï¼Œåœ¨é›¶æ ·æœ¬å›¾åƒæè¿°ä»»åŠ¡ä¸­å®ç°äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç³»ç»Ÿæ€§æŠ‘åˆ¶è´Ÿå®ä½“å¯ä»¥æœ‰æ•ˆè§£å†³é›¶æ ·æœ¬å›¾åƒæè¿°ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œä¸ºæ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„è§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶å±•ç¤ºäº†åˆæˆæ•°æ®ä¸æ£€ç´¢æœºåˆ¶ç»“åˆåœ¨è·¨åŸŸæ³›åŒ–ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Text-only training provides an attractive approach to address data scarcity challenges in zero-shot image captioning (ZIC), avoiding the expense of collecting paired image-text annotations. However, although these approaches perform well within training domains, they suffer from poor cross-domain generalization, often producing hallucinated content when encountering novel visual environments. Retrieval-based methods attempt to mitigate this limitation by leveraging external knowledge, but they can paradoxically exacerbate hallucination when retrieved captions contain entities irrelevant to the inputs. We introduce the concept of negative entities--objects that appear in generated caption but are absent from the input--and propose Negative Entity Suppression (NES) to tackle this challenge. NES seamlessly integrates three stages: (1) it employs synthetic images to ensure consistent image-to-text retrieval across both training and inference; (2) it filters negative entities from retrieved content to enhance accuracy; and (3) it applies attention-level suppression using identified negative entities to further minimize the impact of hallucination-prone features. Evaluation across multiple benchmarks demonstrates that NES maintains competitive in-domain performance while improving cross-domain transfer and reducing hallucination rates, achieving new state-of-the-art results in ZIC. Our code is available at https://github.com/nidongpinyinme/NESCap.</p>
<h3 id="8-speed-q-staged-processing-with-enhanced-distillation-towards-efficient-low-bit-on-device-vlm-quantization">[8] <a href="https://arxiv.org/abs/2511.08914">SPEED-Q: Staged Processing with Enhanced Distillation towards Efficient Low-bit On-device VLM Quantization</a></h3>
<p><em>Tianyu Guo, Shanwei Zhao, Shiai Zhu, Chenguang Ma</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>SPEED-Qæå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†é˜¶æ®µå¤„ç†ä¸å¢å¼ºè’¸é¦æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹çš„ä½æ¯”ç‰¹æƒé‡é‡åŒ–ï¼Œè§£å†³äº†å¤šæ¨¡æ€ç»„ä»¶é‡åŒ–æ•æ„Ÿåº¦å·®å¼‚å’Œè®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°äº†å‡†ç¡®ã€ç¨³å®šä¸”æ•°æ®é«˜æ•ˆçš„VLMéƒ¨ç½²ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç ”ç©¶å¾ˆå°‘æ¢ç´¢è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¿€è¿›é‡åŒ–ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ›´é€‚åˆèµ„æºå—é™è¾¹ç¼˜è®¾å¤‡çš„1Bè‡³2Bå‚æ•°æ¨¡å‹ï¼Œä¸”é¢ä¸´è§†è§‰ä¸è¯­è¨€ç»„ä»¶é‡åŒ–æ•æ„Ÿåº¦å·®å¼‚æ˜¾è‘—ä»¥åŠä½æ¯”ç‰¹é‡åŒ–å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šç­‰å…³é”®éšœç¢ã€‚</p>
<p><strong>Method:</strong> SPEED-Qå¼•å…¥äº†åˆ†é˜¶æ®µæ•æ„Ÿåº¦è‡ªé€‚åº”æœºåˆ¶æ¥åè°ƒä¸åŒæ¨¡æ€çš„æ€§èƒ½ï¼Œå¹¶æå‡ºè’¸é¦å¢å¼ºé‡åŒ–ç­–ç•¥ä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶å‡å°‘æ•°æ®ä¾èµ–ï¼Œä»è€Œç³»ç»Ÿæ€§åœ°è§£å†³äº†å¤šæ¨¡æ€é‡åŒ–æ•æ„Ÿåº¦å·®å¼‚å’Œè®­ç»ƒç¨³å®šæ€§é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSPEED-Qåœ¨2æ¯”ç‰¹è®¾ç½®ä¸‹æ¯”ç°æœ‰é‡åŒ–æ–¹æ³•å‡†ç¡®ç‡æå‡é«˜è¾¾6å€ï¼Œåœ¨2æ¯”ç‰¹å’Œ4æ¯”ç‰¹è®¾ç½®ä¸‹å‡æŒç»­ä¼˜äºå…ˆå‰çš„è®¾å¤‡ç«¯VLMæ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é¦–æ¬¡ä¸ºå®Œæ•´çš„å°è§„æ¨¡åäº¿å‚æ•°çº§VLMä½æ¯”ç‰¹é‡åŒ–æä¾›äº†ä¸“é—¨æ¡†æ¶ï¼Œå±•ç¤ºäº†åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¤æ‚VLMçš„å¯è¡Œæ€§ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„å¤šæ¨¡æ€AIåº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Deploying Vision-Language Models (VLMs) on edge devices (e.g., smartphones and robots) is crucial for enabling low-latency and privacy-preserving intelligent applications. Given the resource constraints of these devices, quantization offers a promising solution by improving memory efficiency and reducing bandwidth requirements, thereby facilitating the deployment of VLMs. However, existing research has rarely explored aggressive quantization on VLMs, particularly for the models ranging from 1B to 2B parameters, which are more suitable for resource-constrained edge devices. In this paper, we propose SPEED-Q, a novel Staged Processing with Enhanced Distillation framework for VLM low-bit weight-only quantization that systematically addresses the following two critical obstacles: (1) significant discrepancies in quantization sensitivity between vision (ViT) and language (LLM) components in VLMs; (2) training instability arising from the reduced numerical precision inherent in low-bit quantization. In SPEED-Q, a staged sensitivity adaptive mechanism is introduced to effectively harmonize performance across different modalities. We further propose a distillation-enhanced quantization strategy to stabilize the training process and reduce data dependence. Together, SPEED-Q enables accurate, stable, and data-efficient quantization of complex VLMs. SPEED-Q is the first framework tailored for quantizing entire small-scale billion-parameter VLMs to low bits. Extensive experiments across multiple benchmarks demonstrate that SPEED-Q achieves up to 6x higher accuracy than existing quantization methods under 2-bit settings and consistently outperforms prior on-device VLMs under both 2-bit and 4-bit settings. Our code and models are available at https://github.com/antgroup/SPEED-Q.</p>
<h3 id="9-from-structure-to-detail-hierarchical-distillation-for-efficient-diffusion-model">[9] <a href="https://arxiv.org/abs/2511.08930">From Structure to Detail: Hierarchical Distillation for Efficient Diffusion Model</a></h3>
<p><em>Hanbo Cheng, Peng Wang, Kaixiang Lei, Qi Li, Zhen Zou, Pengfei Hu, Jun Du</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†åˆ†å±‚è’¸é¦ï¼ˆHDï¼‰æ¡†æ¶ï¼Œå°†è½¨è¿¹è’¸é¦å’Œåˆ†å¸ƒè’¸é¦ä»ç‹¬ç«‹èŒƒå¼è½¬å˜ä¸ºååŒç»„ä»¶ï¼Œé€šè¿‡è½¨è¿¹è’¸é¦å»ºç«‹ç»“æ„è‰å›¾ï¼Œå†é€šè¿‡åˆ†å¸ƒè’¸é¦è¿›è¡Œç»†åŒ–ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å•æ­¥æ‰©æ•£æ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ‰©æ•£æ¨¡å‹çš„æ¨ç†å»¶è¿Ÿæ˜¯å®æ—¶åº”ç”¨çš„å…³é”®éšœç¢ï¼Œç°æœ‰è½¨è¿¹è’¸é¦å’Œåˆ†å¸ƒè’¸é¦æ–¹æ³•å­˜åœ¨æ ¹æœ¬æ€§æƒè¡¡ï¼šè½¨è¿¹è’¸é¦ä¿ç•™å…¨å±€ç»“æ„ä½†ç‰ºç‰²é«˜é¢‘ç»†èŠ‚ï¼Œåˆ†å¸ƒè’¸é¦å¯å®ç°æ›´é«˜ä¿çœŸåº¦ä½†å¸¸å‡ºç°æ¨¡å¼å´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åˆ†å±‚è’¸é¦æ¡†æ¶ï¼Œå°†è½¨è¿¹è’¸é¦ä½œä¸ºç»“æ„è‰å›¾ç”Ÿæˆå™¨è€Œéæœ€ç»ˆç”Ÿæˆå™¨ï¼Œä¸ºåç»­åˆ†å¸ƒè’¸é¦é˜¶æ®µæä¾›è¿‘æœ€ä¼˜åˆå§‹åŒ–ï¼›å¼•å…¥è‡ªé€‚åº”åŠ æƒåˆ¤åˆ«å™¨ï¼ˆAWDï¼‰ï¼Œé€šè¿‡åŠ¨æ€åˆ†é…tokenæƒé‡ä¸“æ³¨äºå±€éƒ¨ç¼ºé™·ï¼Œå®ç°é«˜æ•ˆç»†èŠ‚ç»†åŒ–ã€‚</p>
<p><strong>Result:</strong> åœ¨ImageNet 256Ã—256ä¸Šï¼Œå•æ­¥æ¨¡å‹è¾¾åˆ°FID 2.26ï¼Œåª²ç¾å…¶250æ­¥æ•™å¸ˆæ¨¡å‹ï¼›åœ¨é«˜åˆ†è¾¨ç‡æ–‡æœ¬åˆ°å›¾åƒMJHQåŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ‰å‰æ™¯çš„ç»“æœï¼Œè¯æ˜äº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•ä¸ºé«˜ä¿çœŸå•æ­¥æ‰©æ•£æ¨¡å‹å»ºç«‹äº†ç¨³å¥çš„æ–°èŒƒå¼ï¼Œé€šè¿‡ååŒåˆ©ç”¨ä¸¤ç§è’¸é¦æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œå…‹æœäº†å„è‡ªå±€é™æ€§ï¼Œå®ç°äº†æ€§èƒ½ä¸æ•ˆç‡çš„æœ€ä½³å¹³è¡¡ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>The inference latency of diffusion models remains a critical barrier to their real-time application. While trajectory-based and distribution-based step distillation methods offer solutions, they present a fundamental trade-off. Trajectory-based methods preserve global structure but act as a "lossy compressor", sacrificing high-frequency details. Conversely, distribution-based methods can achieve higher fidelity but often suffer from mode collapse and unstable training. This paper recasts them from independent paradigms into synergistic components within our novel Hierarchical Distillation (HD) framework. We leverage trajectory distillation not as a final generator, but to establish a structural ``sketch", providing a near-optimal initialization for the subsequent distribution-based refinement stage. This strategy yields an ideal initial distribution that enhances the ceiling of overall performance. To further improve quality, we introduce and refine the adversarial training process. We find standard discriminator structures are ineffective at refining an already high-quality generator. To overcome this, we introduce the Adaptive Weighted Discriminator (AWD), tailored for the HD pipeline. By dynamically allocating token weights, AWD focuses on local imperfections, enabling efficient detail refinement. Our approach demonstrates state-of-the-art performance across diverse tasks. On ImageNet $256\times256$, our single-step model achieves an FID of 2.26, rivaling its 250-step teacher. It also achieves promising results on the high-resolution text-to-image MJHQ benchmark, proving its generalizability. Our method establishes a robust new paradigm for high-fidelity, single-step diffusion models.</p>
<h3 id="10-t-rex-omni-integrating-negative-visual-prompt-in-generic-object-detection">[10] <a href="https://arxiv.org/abs/2511.08997">T-Rex-Omni: Integrating Negative Visual Prompt in Generic Object Detection</a></h3>
<p><em>Jiazhou Zhou, Qing Jiang, Kanghao Chen, Lutao Jiang, Yuanhuiyi Lyu, Ying-Cong Chen, Lei Zhang</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºT-Rex-Omniæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è´Ÿè§†è§‰æç¤ºæ¥æŠ‘åˆ¶ç¡¬è´Ÿæ ·æœ¬å¹²æ‰°ï¼Œæ˜¾è‘—æå‡å¼€æ”¾é›†ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬æ£€æµ‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿å°¾åœºæ™¯ä¸‹å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¼€æ”¾é›†ç›®æ ‡æ£€æµ‹å™¨ä»…ä¾èµ–åŸºäºæ–‡æœ¬æè¿°æˆ–è§†è§‰ç¤ºä¾‹çš„æ­£å‘æç¤ºï¼Œè¿™ç§ä»…æ­£å‘çš„èŒƒå¼åœ¨é¢å¯¹è§†è§‰ç›¸ä¼¼ä½†è¯­ä¹‰ä¸åŒçš„å¹²æ‰°ç‰©æ—¶å­˜åœ¨æŒç»­è„†å¼±æ€§ã€‚ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œé€šè¿‡æ•´åˆè´Ÿè§†è§‰æç¤ºæ¥å¦å®šç¡¬è´Ÿå¹²æ‰°ç‰©ã€‚</p>
<p><strong>Method:</strong> æå‡ºç»Ÿä¸€è§†è§‰æç¤ºç¼–ç å™¨è”åˆå¤„ç†æ­£è´Ÿè§†è§‰æç¤ºï¼Œè®¾è®¡æ— éœ€è®­ç»ƒçš„è´Ÿå‘å¦å®šè®¡ç®—æ¨¡å—åŠ¨æ€æŠ‘åˆ¶è´Ÿå“åº”ï¼Œå¹¶é€šè¿‡è´Ÿå‘å¦å®šé“°é“¾æŸå¤±åœ¨å¾®è°ƒé˜¶æ®µå¼ºåˆ¶æ­£è´ŸåµŒå…¥ä¹‹é—´çš„åˆ¤åˆ«æ€§è¾¹ç•Œã€‚æ”¯æŒæ­£å‘å’Œè”åˆæ­£è´Ÿæ¨ç†ä¸¤ç§çµæ´»éƒ¨ç½²æ¨¡å¼ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒæ˜¾ç¤ºåœ¨é›¶æ ·æœ¬æ£€æµ‹ä¸­è¡¨ç°å“è¶Šï¼Œæ˜¾è‘—ç¼©å°äº†è§†è§‰æç¤ºä¸æ–‡æœ¬æç¤ºæ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œåœ¨é•¿å°¾åœºæ™¯è¡¨ç°å°¤ä¸ºçªå‡ºï¼ˆLVIS-minivalä¸Šè¾¾åˆ°51.2 AP_rï¼‰ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶ç¡®ç«‹äº†è´Ÿæç¤ºä½œä¸ºæ¨è¿›å¼€æ”¾é›†è§†è§‰è¯†åˆ«ç³»ç»Ÿçš„å…³é”®æ–°ç»´åº¦ï¼Œä¸ºå¤„ç†è§†è§‰ç›¸ä¼¼å¹²æ‰°ç‰©æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºå¼€æ”¾é›†è§†è§‰è¯†åˆ«å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Object detection methods have evolved from closed-set to open-set paradigms over the years. Current open-set object detectors, however, remain constrained by their exclusive reliance on positive indicators based on given prompts like text descriptions or visual exemplars. This positive-only paradigm experiences consistent vulnerability to visually similar but semantically different distractors. We propose T-Rex-Omni, a novel framework that addresses this limitation by incorporating negative visual prompts to negate hard negative distractors. Specifically, we first introduce a unified visual prompt encoder that jointly processes positive and negative visual prompts. Next, a training-free Negating Negative Computing (NNC) module is proposed to dynamically suppress negative responses during the probability computing stage. To further boost performance through fine-tuning, our Negating Negative Hinge (NNH) loss enforces discriminative margins between positive and negative embeddings. T-Rex-Omni supports flexible deployment in both positive-only and joint positive-negative inference modes, accommodating either user-specified or automatically generated negative examples. Extensive experiments demonstrate remarkable zero-shot detection performance, significantly narrowing the performance gap between visual-prompted and text-prompted methods while showing particular strength in long-tailed scenarios (51.2 AP_r on LVIS-minival). This work establishes negative prompts as a crucial new dimension for advancing open-set visual recognition systems.</p>
<h3 id="11-causally-grounded-dual-path-attention-intervention-for-object-hallucination-mitigation-in-lvlms">[11] <a href="https://arxiv.org/abs/2511.09018">Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs</a></h3>
<p><em>Liu Yu, Zhonghao Chen, Ping Kuang, Zhikun Feng, Fan Zhou, Lan Wang, Gillian Dobbie</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Owlæ¡†æ¶ï¼Œä¸€ç§åŸºäºå› æœæ¨ç†çš„åŒæ¨¡æ€æ³¨æ„åŠ›é‡åŠ æƒæ–¹æ³•ï¼Œé€šè¿‡å»ºæ¨¡å¹»è§‰è¿‡ç¨‹çš„ç»“æ„å› æœå›¾ï¼Œå¼•å…¥VTACRæŒ‡æ ‡é‡åŒ–æ¨¡æ€è´¡çŒ®ä¸å¹³è¡¡ï¼Œå¹¶è®¾è®¡ç»†ç²’åº¦æ³¨æ„åŠ›å¹²é¢„æœºåˆ¶ï¼Œæ˜¾è‘—å‡å°‘å¤§è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç‰©ä½“å¹»è§‰é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºè¯­è¨€è§£ç å™¨çš„ç¼“è§£æ–¹æ³•é€šå¸¸ç‹¬ç«‹è°ƒèŠ‚è§†è§‰æˆ–æ–‡æœ¬æ³¨æ„åŠ›ï¼Œå¿½è§†äº†å®ƒä»¬ä½œä¸ºä¸¤ä¸ªå…³é”®å› æœå› ç´ çš„ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´ç‰©ä½“å¹»è§‰é—®é¢˜åœ¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æŒç»­å­˜åœ¨ã€‚</p>
<p><strong>Method:</strong> æå‡ºOwlæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„å› æœå›¾å»ºæ¨¡å¹»è§‰è¿‡ç¨‹ï¼Œå°†åˆ†è§£çš„è§†è§‰å’Œæ–‡æœ¬æ³¨æ„åŠ›è§†ä¸ºä¸­ä»‹å˜é‡ï¼›å¼•å…¥VTACRæŒ‡æ ‡é‡åŒ–è§£ç è¿‡ç¨‹ä¸­çš„æ¨¡æ€è´¡çŒ®ä¸å¹³è¡¡ï¼›è®¾è®¡ç»†ç²’åº¦æ³¨æ„åŠ›å¹²é¢„æœºåˆ¶ï¼Œæ ¹æ®VTACRä¿¡å·åŠ¨æ€è°ƒæ•´tokençº§å’Œå±‚çº§çš„æ³¨æ„åŠ›ï¼›é‡‡ç”¨åŒè·¯å¾„å¯¹æ¯”è§£ç ç­–ç•¥ï¼Œä¸€æ¡è·¯å¾„å¼ºè°ƒè§†è§‰åŸºç¡€é¢„æµ‹ï¼Œå¦ä¸€æ¡æ”¾å¤§å¹»è§‰é¢„æµ‹ã€‚</p>
<p><strong>Result:</strong> åœ¨POPEå’ŒCHAIRåŸºå‡†æµ‹è¯•ä¸­ï¼ŒOwlå®ç°äº†æ˜¾è‘—çš„å¹»è§‰å‡å°‘ï¼Œåœ¨ä¿æŒè§†è§‰è¯­è¨€ç†è§£èƒ½åŠ›çš„åŒæ—¶ï¼Œåœ¨å¿ å®æ€§æ–¹é¢è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡å› æœå»ºæ¨¡å’Œç»†ç²’åº¦æ³¨æ„åŠ›å¹²é¢„å¯ä»¥æœ‰æ•ˆç¼“è§£ç‰©ä½“å¹»è§‰é—®é¢˜ï¼ŒVTACRæŒ‡æ ‡ä¸ºç†è§£å¹»è§‰æœºåˆ¶æä¾›äº†æ–°çš„è§†è§’ï¼ŒåŒè·¯å¾„å¯¹æ¯”è§£ç ç­–ç•¥ä¸ºæœªæ¥å¹»è§‰æ£€æµ‹å’Œç¼“è§£æ–¹æ³•æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Object hallucination remains a critical challenge in Large Vision-Language Models (LVLMs), where models generate content inconsistent with visual inputs. Existing language-decoder based mitigation approaches often regulate visual or textual attention independently, overlooking their interaction as two key causal factors. To address this, we propose Owl (Bi-mOdal attention reWeighting for Layer-wise hallucination mitigation), a causally-grounded framework that models hallucination process via a structural causal graph, treating decomposed visual and textual attentions as mediators. We introduce VTACR (Visual-to-Textual Attention Contribution Ratio), a novel metric that quantifies the modality contribution imbalance during decoding. Our analysis reveals that hallucinations frequently occur in low-VTACR scenarios, where textual priors dominate and visual grounding is weakened. To mitigate this, we design a fine-grained attention intervention mechanism that dynamically adjusts token- and layer-wise attention guided by VTACR signals. Finally, we propose a dual-path contrastive decoding strategy: one path emphasizes visually grounded predictions, while the other amplifies hallucinated ones -- letting visual truth shine and hallucination collapse. Experimental results on the POPE and CHAIR benchmarks show that Owl achieves significant hallucination reduction, setting a new SOTA in faithfulness while preserving vision-language understanding capability. Our code is available at https://github.com/CikZ2023/OWL</p>
<h3 id="12-vietmeagent-culturally-aware-few-shot-multimodal-explanation-for-vietnamese-visual-question-answering">[12] <a href="https://arxiv.org/abs/2511.09058">VietMEAgent: Culturally-Aware Few-Shot Multimodal Explanation for Vietnamese Visual Question Answering</a></h3>
<p><em>Hai-Dang Nguyen, Minh-Anh Dang, Minh-Tan Le, Minh-Tuan Le</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VietMEAgentï¼Œä¸€ä¸ªé’ˆå¯¹è¶Šå—æ–‡åŒ–ç†è§£çš„å¤šæ¨¡æ€å¯è§£é‡Šæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ–‡åŒ–å¯¹è±¡æ£€æµ‹å’Œç»“æ„åŒ–ç¨‹åºç”Ÿæˆï¼Œä¸ºè¶Šå—æ–‡åŒ–ç‰¹å®šçš„è§†è§‰é—®ç­”æä¾›é€æ˜è§£é‡Šã€‚è¯¥æ¡†æ¶é›†æˆäº†æ–‡åŒ–çŸ¥è¯†åº“å’ŒåŒæ¨¡æ€è§£é‡Šæ¨¡å—ï¼Œæ”¯æŒæ–‡åŒ–æ•™è‚²å’Œä¿æŠ¤ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰é—®ç­”ç³»ç»Ÿåœ¨å¤„ç†æ–‡åŒ–ç‰¹å®šå†…å®¹æ—¶å­˜åœ¨å±€é™ï¼Œä¸»è¦å› ä¸ºè®­ç»ƒè¯­æ–™ä¸­æ–‡åŒ–çŸ¥è¯†ä»£è¡¨æ€§ä¸è¶³ä¸”æ¨ç†è¿‡ç¨‹å¯¹ç»ˆç«¯ç”¨æˆ·ä¸å¯è§£é‡Šã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¶Šå—æ–‡åŒ–ç†è§£ä¸­çš„è¿™ä¸€ç¼ºå£ï¼Œå¼ºè°ƒå¯è§£é‡Šæ€§å’Œæ–‡åŒ–æ•æ„Ÿæ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æ•´åˆäº†æ–‡åŒ–å¯¹è±¡æ£€æµ‹ä¸»å¹²ç½‘ç»œä¸ç»“æ„åŒ–ç¨‹åºç”Ÿæˆå±‚ï¼Œæ„å»ºäº†ä¸€ä¸ªç­”æ¡ˆé¢„æµ‹ä¸è§£é‡Šç´§å¯†è€¦åˆçš„æµæ°´çº¿ã€‚ä½¿ç”¨ç²¾å¿ƒç­–åˆ’çš„è¶Šå—æ–‡åŒ–å®ä½“çŸ¥è¯†åº“ä½œä¸ºæ˜¾å¼èƒŒæ™¯ä¿¡æ¯æºï¼Œå¹¶é€šè¿‡åŒæ¨¡æ€è§£é‡Šæ¨¡å—ç»“åˆåŸºäºæ³¨æ„åŠ›çš„è§†è§‰è¯æ®å’Œç»“æ„åŒ–å¯è¯»æ–‡æœ¬æ¨ç†ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶æ„å»ºäº†è¶Šå—æ–‡åŒ–VQAæ•°æ®é›†å¹¶éªŒè¯äº†åŸºäºç¼–ç¨‹æ–¹æ³•åœ¨æ–‡åŒ–AIä¸­çš„å®ç”¨æ€§ã€‚ç³»ç»Ÿèƒ½å¤Ÿæä¾›é€æ˜è§£é‡Šï¼Œæ­ç¤ºè®¡ç®—æ¨ç†è¿‡ç¨‹å’Œæ–‡åŒ–èƒŒæ™¯ï¼Œåœ¨æ•™è‚²å’Œæ–‡åŒ–ä¿æŠ¤åº”ç”¨ä¸­è¡¨ç°å‡ºè‰¯å¥½æ•ˆæœã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†ç¼–ç¨‹åŒ–æ–¹æ³•åœ¨æ–‡åŒ–AIä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ–‡åŒ–æ•æ„Ÿçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†å¯è§£é‡Šæ€§æ¡†æ¶ã€‚ç³»ç»Ÿä¸ä»…æå‡äº†æ–‡åŒ–å†…å®¹ç†è§£èƒ½åŠ›ï¼Œè¿˜é€šè¿‡é€æ˜è§£é‡Šæœºåˆ¶æ”¯æŒæ–‡åŒ–æ•™è‚²å’Œä¿æŠ¤å·¥ä½œï¼Œä¸ºå¤šæ¨¡æ€æ–‡åŒ–ç†è§£ç³»ç»Ÿçš„å‘å±•æä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Contemporary Visual Question Answering (VQA) systems remain constrained when confronted with culturally specific content, largely because cultural knowledge is under-represented in training corpora and the reasoning process is not rendered interpretable to end users. This paper introduces VietMEAgent, a multimodal explainable framework engineered for Vietnamese cultural understanding. The method integrates a cultural object detection backbone with a structured program generation layer, yielding a pipeline in which answer prediction and explanation are tightly coupled. A curated knowledge base of Vietnamese cultural entities serves as an explicit source of background information, while a dual-modality explanation module combines attention-based visual evidence with structured, human-readable textual rationales. We further construct a Vietnamese Cultural VQA dataset sourced from public repositories and use it to demonstrate the practicality of programming-based methodologies for cultural AI. The resulting system provides transparent explanations that disclose both the computational rationale and the underlying cultural context, supporting education and cultural preservation with an emphasis on interpretability and cultural sensitivity.</p>
<h3 id="13-taming-object-hallucinations-with-verified-atomic-confidence-estimation">[13] <a href="https://arxiv.org/abs/2511.09228">Taming Object Hallucinations with Verified Atomic Confidence Estimation</a></h3>
<p><em>Jiarui Liu, Weihao Xuan, Zhijing Jin, Mona Diab</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†TACOæ¡†æ¶ï¼Œé€šè¿‡è‡ªéªŒè¯å’Œç½®ä¿¡åº¦æ ¡å‡†æ¥ç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨è§†è§‰ä¸“å®¶ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¿ å®æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç»å¸¸é­å—å¹»è§‰é—®é¢˜çš„å›°æ‰°ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è±¡å­˜åœ¨æ€§ã€å±æ€§å’Œå…³ç³»æ–¹é¢çš„é”™è¯¯ï¼Œè¿™äº›é—®é¢˜ä¸¥é‡å‰Šå¼±äº†æ¨¡å‹çš„å¯é æ€§ï¼Œå› æ­¤éœ€è¦å¼€å‘æœ‰æ•ˆçš„ç¼“è§£æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> TACOæ¡†æ¶å°†æ¨¡å‹å“åº”åˆ†è§£ä¸ºåŸå­æŸ¥è¯¢ï¼Œé€šè¿‡é‡è¿°æ¥é™ä½å¯¹æªè¾çš„æ•æ„Ÿæ€§ï¼Œä½¿ç”¨è‡ªä¸€è‡´æ€§æˆ–è‡ªç½®ä¿¡åº¦èšåˆæ¥ä¼°è®¡ç½®ä¿¡åº¦ï¼Œæœ€ååˆ©ç”¨è¯­è¨€æ¨¡å‹å¯¹ç­”æ¡ˆè¿›è¡Œç²¾ç‚¼ï¼Œæ•´ä¸ªè¿‡ç¨‹ä¸ä¾èµ–å¤–éƒ¨è§†è§‰ä¸“å®¶ã€‚</p>
<p><strong>Result:</strong> åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTACOåœ¨LLaVA-1.5-7Bå’ŒCogVLM2ä¸¤ä¸ªæ¨¡å‹ä¸Šå‡ä¼˜äºç›´æ¥æç¤ºå’Œè§†è§‰å¯¹æ¯”è§£ç æ–¹æ³•ï¼Œæœ‰æ•ˆå‡å°‘äº†ç³»ç»Ÿæ€§åå·®å¹¶æ”¹å–„äº†ç½®ä¿¡åº¦æ ¡å‡†æ•ˆæœã€‚</p>
<p><strong>Conclusion:</strong> TACOæ¡†æ¶è¯æ˜äº†é€šè¿‡è‡ªéªŒè¯å’Œç½®ä¿¡åº¦æ ¡å‡†å¯ä»¥æœ‰æ•ˆç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜ï¼Œä¸ºæå‡æ¨¡å‹å¿ å®æ€§æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) often suffer from hallucinations, particularly errors in object existence, attributes, or relations, which undermine their reliability. We introduce TACO (Verified Atomic Confidence Estimation), a simple framework that mitigates hallucinations through self-verification and confidence calibration without relying on external vision experts. TACO decomposes responses into atomic queries, paraphrases them to reduce sensitivity to wording, and estimates confidence using self-consistency (black-box) or self-confidence (gray-box) aggregation, before refining answers with a language model. Experiments on five benchmarks (POPE, MME, HallusionBench, AMBER, and MM-Hal Bench) with two MLLMs (\texttt{LLaVA-1.5-7B} and \texttt{CogVLM2}) show that TACO consistently outperforms direct prompting and Visual Contrastive Decoding, reduces systematic biases, and improves confidence calibration, demonstrating its effectiveness in enhancing the faithfulness of MLLMs.</p>
<h3 id="14-diversifying-counterattacks-orthogonal-exploration-for-robust-clip-inference">[14] <a href="https://arxiv.org/abs/2511.09064">Diversifying Counterattacks: Orthogonal Exploration for Robust CLIP Inference</a></h3>
<p><em>Chengze Jiang, Minjing Dong, Xinli Shi, Jie Gui</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºæ–¹å‘æ­£äº¤å¯¹æŠ—é˜²å¾¡ï¼ˆDOCï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ­£äº¤æ¢¯åº¦æ–¹å‘å’ŒåŠ¨é‡æ›´æ–°æ¥å¢å¼ºå¯¹æŠ—æ ·æœ¬çš„å¤šæ ·æ€§å’Œè¦†ç›–èŒƒå›´ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨æµ‹è¯•æ—¶é˜²å¾¡ä¸­çš„å¯¹æŠ—é²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æµ‹è¯•æ—¶å¯¹æŠ—é˜²å¾¡æ–¹æ³•TTCä»…åŸºäºå¯¹æŠ—è¾“å…¥çš„æ¢¯åº¦ç”Ÿæˆå¯¹æŠ—é˜²å¾¡ï¼Œç”±äºå¯¹æŠ—æ”»å‡»ä¸å¯¹æŠ—é˜²å¾¡åœ¨ä¼˜åŒ–ç›®æ ‡ä¸Šçš„æ ¹æœ¬å·®å¼‚ï¼Œå¯¼è‡´æœç´¢ç©ºé—´å—é™ï¼Œå¯¹æŠ—é˜²å¾¡å®¹æ˜“è¿‡æ‹Ÿåˆæœ‰é™çš„å¯¹æŠ—æ¨¡å¼ï¼Œç¼ºä¹è¶³å¤Ÿçš„å¤šæ ·æ€§æ¥å®Œå…¨ä¸­å’Œå¹¿æ³›çš„æ‰°åŠ¨ã€‚</p>
<p><strong>Method:</strong> æå‡ºæ–¹å‘æ­£äº¤å¯¹æŠ—é˜²å¾¡ï¼ˆDOCï¼‰ï¼Œé€šè¿‡æ­£äº¤æ¢¯åº¦æ–¹å‘å’ŒåŠ¨é‡æ›´æ–°æ¥å¢å¼ºå¯¹æŠ—é˜²å¾¡ä¼˜åŒ–ï¼Œæ‰©å±•å¯¹æŠ—é˜²å¾¡ç©ºé—´çš„æ¢ç´¢å¹¶å¢åŠ æ‰°åŠ¨çš„å¤šæ ·æ€§ï¼›åŒæ—¶åŸºäºå¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦æå‡ºæ–¹å‘æ•æ„Ÿåº¦è¯„åˆ†ï¼Œé€šè¿‡æ”¹è¿›æ ·æœ¬åŒºåˆ†åº¦å’Œè‡ªé€‚åº”è°ƒèŠ‚å¯¹æŠ—é˜²å¾¡å¼ºåº¦æ¥æå‡DOCæ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> åœ¨16ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDOCåœ¨å„ç§æ”»å‡»ä¸‹æ˜¾è‘—æå‡äº†å¯¹æŠ—é²æ£’æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„å¹²å‡€å‡†ç¡®ç‡ã€‚</p>
<p><strong>Conclusion:</strong> å¢å¼ºå¯¹æŠ—é˜²å¾¡çš„å¤šæ ·æ€§å’Œè¦†ç›–èŒƒå›´å¯¹äºæå‡æµ‹è¯•æ—¶é˜²å¾¡çš„å¯¹æŠ—é²æ£’æ€§è‡³å…³é‡è¦ï¼ŒDOCé€šè¿‡æ­£äº¤æ¢¯åº¦æ¢ç´¢å’ŒåŠ¨é‡æœºåˆ¶å®ç°äº†æ›´é€šç”¨çš„å¯¹æŠ—é˜²å¾¡ç”Ÿæˆï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é éƒ¨ç½²æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Vision-language pre-training models (VLPs) demonstrate strong multimodal understanding and zero-shot generalization, yet remain vulnerable to adversarial examples, raising concerns about their reliability. Recent work, Test-Time Counterattack (TTC), improves robustness by generating perturbations that maximize the embedding deviation of adversarial inputs using PGD, pushing them away from their adversarial representations. However, due to the fundamental difference in optimization objectives between adversarial attacks and counterattacks, generating counterattacks solely based on gradients with respect to the adversarial input confines the search to a narrow space. As a result, the counterattacks could overfit limited adversarial patterns and lack the diversity to fully neutralize a broad range of perturbations. In this work, we argue that enhancing the diversity and coverage of counterattacks is crucial to improving adversarial robustness in test-time defense. Accordingly, we propose Directional Orthogonal Counterattack (DOC), which augments counterattack optimization by incorporating orthogonal gradient directions and momentum-based updates. This design expands the exploration of the counterattack space and increases the diversity of perturbations, which facilitates the discovery of more generalizable counterattacks and ultimately improves the ability to neutralize adversarial perturbations. Meanwhile, we present a directional sensitivity score based on averaged cosine similarity to boost DOC by improving example discrimination and adaptively modulating the counterattack strength. Extensive experiments on 16 datasets demonstrate that DOC improves adversarial robustness under various attacks while maintaining competitive clean accuracy. Code is available at https://github.com/bookman233/DOC.</p>
<h3 id="15-densicrafter-physically-constrained-generation-and-fabrication-of-self-supporting-hollow-structures">[15] <a href="https://arxiv.org/abs/2511.09298">DensiCrafter: Physically-Constrained Generation and Fabrication of Self-Supporting Hollow Structures</a></h3>
<p><em>Shengqi Dang, Fu Chai, Jiaxin Li, Chao Yuan, Wei Ye, Nan Cao</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DensiCrafteræ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–å¯†åº¦åœºç”Ÿæˆè½»é‡åŒ–ä¸”è‡ªæ”¯æ’‘çš„3Dä¸­ç©ºç»“æ„ï¼Œåœ¨æ–‡æœ¬åˆ°3Dä»»åŠ¡ä¸­å®ç°äº†é«˜è¾¾43%çš„ææ–™è´¨é‡å‡å°‘ï¼ŒåŒæ—¶ä¿æŒé«˜å‡ ä½•ä¿çœŸåº¦å’Œç»“æ„ç¨³å®šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰3Dç”Ÿæˆæ¨¡å‹é€šå¸¸å¿½ç•¥ç‰©ç†çº¦æŸå’Œå¯åˆ¶é€ æ€§è€ƒè™‘ï¼Œæ— æ³•ç”Ÿæˆæ—¢è½»é‡åŒ–åˆè‡ªæ”¯æ’‘çš„3Dè®¾è®¡ï¼Œè¿™é™åˆ¶äº†å®é™…åˆ¶é€ åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> ä»Trellisç”Ÿæˆçš„ç²—ä½“ç´ ç½‘æ ¼å‡ºå‘ï¼Œå°†å…¶è§£é‡Šä¸ºè¿ç»­å¯†åº¦åœºè¿›è¡Œä¼˜åŒ–ï¼Œå¼•å…¥äº†ä¸‰ä¸ªå¯å¾®åˆ†ã€ç‰©ç†çº¦æŸä¸”æ— éœ€æ¨¡æ‹Ÿçš„æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬è´¨é‡æ­£åˆ™åŒ–æƒ©ç½šå¤šä½™ææ–™ï¼Œä»¥åŠå—é™ä¼˜åŒ–åŸŸä¿æŒå¤–è¡¨é¢å®Œæ•´æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨æ–‡æœ¬åˆ°3Dä»»åŠ¡ä¸­å®ç°äº†é«˜è¾¾43%çš„ææ–™è´¨é‡å‡å°‘ï¼Œç›¸æ¯”ç°æœ‰æœ€ä¼˜æ–¹æ³•æå‡äº†ç»“æ„ç¨³å®šæ€§å¹¶ä¿æŒé«˜å‡ ä½•ä¿çœŸåº¦ï¼ŒçœŸå®3Dæ‰“å°å®éªŒéªŒè¯äº†ä¸­ç©ºè®¾è®¡çš„å¯é åˆ¶é€ å’Œè‡ªæ”¯æ’‘èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥å·¥ä½œå±•ç¤ºäº†å°†ç‰©ç†çº¦æŸæ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹ä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºå¯åˆ¶é€ 3Dè®¾è®¡ç”Ÿæˆå¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—æå‡ç»“æ„æ•ˆç‡å’Œå®ç”¨æ€§ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>The rise of 3D generative models has enabled automatic 3D geometry and texture synthesis from multimodal inputs (e.g., text or images). However, these methods often ignore physical constraints and manufacturability considerations. In this work, we address the challenge of producing 3D designs that are both lightweight and self-supporting. We present DensiCrafter, a framework for generating lightweight, self-supporting 3D hollow structures by optimizing the density field. Starting from coarse voxel grids produced by Trellis, we interpret these as continuous density fields to optimize and introduce three differentiable, physically constrained, and simulation-free loss terms. Additionally, a mass regularization penalizes unnecessary material, while a restricted optimization domain preserves the outer surface. Our method seamlessly integrates with pretrained Trellis-based models (e.g., Trellis, DSO) without any architectural changes. In extensive evaluations, we achieve up to 43% reduction in material mass on the text-to-3D task. Compared to state-of-the-art baselines, our method could improve the stability and maintain high geometric fidelity. Real-world 3D-printing experiments confirm that our hollow designs can be reliably fabricated and could be self-supporting.</p>
<h3 id="16-composition-incremental-learning-for-compositional-generalization">[16] <a href="https://arxiv.org/abs/2511.09082">Composition-Incremental Learning for Compositional Generalization</a></h3>
<p><em>Zhen Li, Yuwei Wu, Chenchen Jing, Che Sun, Chuanhao Li, Yunde Jia</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç»„åˆé›¶æ ·æœ¬å­¦ä¹ çš„ç»„åˆå¢é‡å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰åˆæˆå™¨å’Œè¯­è¨€åŸºå…ƒè’¸é¦æœºåˆ¶æ¥è§£å†³ç°å®ä¸–ç•Œä¸­ç»„åˆæ— é™ä¸”é•¿å°¾åˆ†å¸ƒçš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨è¿ç»­å­¦ä¹ æ–°ç»„åˆæ—¶çš„ç»„åˆæ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°å®ä¸–ç•Œä¸­çš„æ•°æ®æŒç»­æ¶Œç°ï¼Œå¯èƒ½çš„ç»„åˆå‡ ä¹æ˜¯æ— é™çš„ã€é•¿å°¾åˆ†å¸ƒçš„ä¸”ä¸å®Œå…¨å¯è§ï¼Œç°æœ‰æ–¹æ³•åœ¨é¢„æ”¶é›†çš„è®­ç»ƒæ•°æ®ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†ç¼ºä¹åœ¨å¢é‡å­¦ä¹ ç¯å¢ƒä¸­é€æ­¥æå‡ç»„åˆæ³›åŒ–èƒ½åŠ›çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ä¸ªä¼ªé‡æ”¾æ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰åˆæˆå™¨åˆæˆå·²å­¦ä¹ ç»„åˆçš„è§†è§‰è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨è¯­è¨€åŸºå…ƒè’¸é¦æœºåˆ¶åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¿æŒå¯¹é½çš„åŸºå…ƒè¡¨ç¤ºï¼ŒåŒæ—¶æ„å»ºäº†MIT-States-CompILå’ŒC-GQA-CompILä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ç”¨äºå®šé‡è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¯æ˜äº†æ‰€æå‡ºæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œåœ¨ç»„åˆå¢é‡å­¦ä¹ ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç»„åˆæ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿å°¾åˆ†å¸ƒå’Œæ— é™ç»„åˆåœºæ™¯æ—¶è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºç»„åˆå¢é‡å­¦ä¹ æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡è§†è§‰åˆæˆå’Œè¯­è¨€è’¸é¦æœºåˆ¶è§£å†³äº†è¿ç»­å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä¸ºç°å®ä¸–ç•Œä¸­æ— é™ç»„åˆçš„å­¦ä¹ é—®é¢˜æŒ‡æ˜äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Compositional generalization has achieved substantial progress in computer vision on pre-collected training data. Nonetheless, real-world data continually emerges, with possible compositions being nearly infinite, long-tailed, and not entirely visible. Thus, an ideal model is supposed to gradually improve the capability of compositional generalization in an incremental manner. In this paper, we explore Composition-Incremental Learning for Compositional Generalization (CompIL) in the context of the compositional zero-shot learning (CZSL) task, where models need to continually learn new compositions, intending to improve their compositional generalization capability progressively. To quantitatively evaluate CompIL, we develop a benchmark construction pipeline leveraging existing datasets, yielding MIT-States-CompIL and C-GQA-CompIL. Furthermore, we propose a pseudo-replay framework utilizing a visual synthesizer to synthesize visual representations of learned compositions and a linguistic primitive distillation mechanism to maintain aligned primitive representations across the learning process. Extensive experiments demonstrate the effectiveness of the proposed framework.</p>
<h3 id="17-ultra-light-test-time-adaptation-for-vision-language-models">[17] <a href="https://arxiv.org/abs/2511.09101">Ultra-Light Test-Time Adaptation for Vision--Language Models</a></h3>
<p><em>Byunghyun Kim</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºUL-TTAï¼Œä¸€ç§è¶…è½»é‡çº§æµ‹è¯•æ—¶é€‚åº”æ¡†æ¶ï¼Œé€šè¿‡ä»…è°ƒæ•´logitçº§å‚æ•°ï¼ˆç±»åˆ«åŸå‹ã€å…ˆéªŒå’Œæ¸©åº¦ï¼‰æ¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸŸåç§»ä¸‹çš„ç‰¹å¾æ¼‚ç§»å’Œæ ¡å‡†é—®é¢˜ï¼Œæ— éœ€æ›´æ–°ä¸»å¹²ç½‘ç»œå‚æ•°å³å¯å®ç°æœ€å…ˆè¿›çš„ç²¾åº¦-æ ¡å‡†æƒè¡¡ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPåœ¨åŸŸåç§»ä¸‹å­˜åœ¨ç‰¹å¾æ¼‚ç§»ã€ç±»åˆ«å…ˆéªŒä¸åŒ¹é…å’Œä¸¥é‡æ ¡å‡†é”™è¯¯çš„é—®é¢˜ï¼Œç°æœ‰æµ‹è¯•æ—¶é€‚åº”æ–¹æ³•éœ€è¦ä¸»å¹²ç½‘ç»œçš„åå‘ä¼ æ’­ã€åæ–¹å·®ä¼°è®¡æˆ–å¤§é‡å†…å­˜çŠ¶æ€ï¼Œè¿™åœ¨æµå¼å’Œè¾¹ç¼˜åœºæ™¯ä¸­å­˜åœ¨æ˜¾è‘—é™åˆ¶ã€‚</p>
<p><strong>Method:</strong> UL-TTAé‡‡ç”¨å®Œå…¨å…è®­ç»ƒå’Œå…åå‘ä¼ æ’­çš„æ¡†æ¶ï¼Œå†»ç»“ä¸»å¹²ç½‘ç»œä»…åœ¨çº¿è°ƒæ•´logitçº§å‚æ•°ï¼ŒåŒ…æ‹¬é€‰æ‹©æ€§æ ·æœ¬è¿‡æ»¤ã€åŸºäºæ–‡æœ¬å’Œç‹„åˆ©å…‹é›·å…ˆéªŒçš„é—­å¼è´å¶æ–¯æ›´æ–°åŸå‹å’Œå…ˆéªŒã€è§£è€¦çš„é¢„æµ‹ä¸æ ¡å‡†æ¸©åº¦ï¼Œä»¥åŠè½»é‡çº§é˜²æŠ¤æœºåˆ¶é˜²æ­¢é•¿æœŸæµä¸­çš„æ¼‚ç§»ã€‚</p>
<p><strong>Result:</strong> åœ¨è·¨åŸŸå’Œåˆ†å¸ƒå¤–åŸºå‡†æµ‹è¯•ä¸­ï¼ŒUL-TTAç›¸æ¯”é›¶æ ·æœ¬CLIPå¹³å‡æå‡4.7ä¸ªç™¾åˆ†ç‚¹çš„top-1å‡†ç¡®ç‡ï¼ŒåŒæ—¶å°†ECEé™ä½20-30%ï¼Œå»¶è¿Ÿå¼€é”€å°äº8%ï¼Œåœ¨é•¿è¾¾20ä¸‡æ ·æœ¬çš„æµå¼å®éªŒä¸­æœªå‡ºç°å´©æºƒç°è±¡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜logitçº§è´å¶æ–¯é€‚åº”è¶³ä»¥åœ¨åŸŸåç§»ä¸‹ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹å®ç°æœ€å…ˆè¿›çš„ç²¾åº¦-æ ¡å‡†æƒè¡¡ï¼Œæ— éœ€æ›´æ–°ä»»ä½•ä¸»å¹²ç½‘ç»œå‚æ•°ï¼Œä¸ºæµå¼å’Œè¾¹ç¼˜éƒ¨ç½²æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) such as CLIP achieve strong zero-shot recognition by comparing image embeddings to text-derived class prototypes. However, under domain shift, they suffer from feature drift, class-prior mismatch, and severe miscalibration. Existing test-time adaptation (TTA) methods often require backpropagation through large backbones, covariance estimation, or heavy memory/state, which is problematic for streaming and edge scenarios. We propose Ultra-Light Test-Time Adaptation (UL-TTA), a fully training-free and backprop-free framework that freezes the backbone and adapts only logit-level parameters: class prototypes, class priors, and temperature. UL-TTA performs an online EM-style procedure with (i) selective sample filtering to use only confident predictions, (ii) closed-form Bayesian updates for prototypes and priors anchored by text and Dirichlet priors, (iii) decoupled temperatures for prediction vs. calibration, and (iv) lightweight guards (norm clipping, prior KL constraints, smoothed temperature) to prevent drift in long streams. Across large-scale cross-domain and OOD benchmarks (PACS, Office-Home, DomainNet, Terra Incognita, ImageNet-R/A/V2/Sketch; ~726K test samples) and strong TTA baselines including Tent, T3A, CoTTA, SAR, Tip-Adapter, and FreeTTA, UL-TTA consistently improves top-1 accuracy (e.g., +4.7 points over zero-shot CLIP on average) while reducing ECE by 20-30%, with less than 8% latency overhead. Long-stream experiments up to 200K samples show no collapse. Our results demonstrate that logit-level Bayesian adaptation is sufficient to obtain state-of-the-art accuracy-calibration trade-offs for VLMs under domain shift, without updating any backbone parameters.</p>
<h3 id="18-towards-trustworthy-dermatology-mllms-a-benchmark-and-multimodal-evaluator-for-diagnostic-narratives">[18] <a href="https://arxiv.org/abs/2511.09195">Towards Trustworthy Dermatology MLLMs: A Benchmark and Multimodal Evaluator for Diagnostic Narratives</a></h3>
<p><em>Yuhao Shen, Jiahe Qian, Shuping Zhang, Zhangtianyi Chen, Tao Lu, Juexiao Zhou</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»“åˆDermBenchåŸºå‡†æµ‹è¯•å’ŒDermEvalè‡ªåŠ¨è¯„ä¼°å™¨çš„åˆ›æ–°è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå¯é è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨çš®è‚¤ç—…å­¦è¯Šæ–­ä¸­çš„è¡¨ç°ï¼Œè§£å†³äº†ä¸´åºŠéƒ¨ç½²ä¸­çš„è¯„ä¼°ç“¶é¢ˆé—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°ç”¨äºç›´æ¥ä»å›¾åƒç”Ÿæˆçš®è‚¤ç—…å­¦è¯Šæ–­å™è¿°ï¼Œä½†å¯é çš„è¯„ä¼°ä»ç„¶æ˜¯è´Ÿè´£ä»»ä¸´åºŠéƒ¨ç½²çš„ä¸»è¦ç“¶é¢ˆï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿæä¾›ä¸´åºŠæ„ä¹‰ã€å¯é‡å¤å’Œå¯æ‰©å±•è¯„ä¼°çš„æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†DermBenchåŸºå‡†æµ‹è¯•ï¼Œå°†4000å¼ çœŸå®çš®è‚¤ç—…å›¾åƒä¸ä¸“å®¶è®¤è¯çš„è¯Šæ–­å™è¿°é…å¯¹ï¼Œå¹¶ä½¿ç”¨åŸºäºLLMçš„è¯„åˆ¤å™¨åœ¨ä¸´åºŠåŸºç¡€ç»´åº¦ä¸Šå¯¹å€™é€‰å™è¿°è¿›è¡Œè¯„åˆ†ï¼›åŒæ—¶è®­ç»ƒäº†DermEvalæ— å‚è€ƒå¤šæ¨¡æ€è¯„ä¼°å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®å›¾åƒå’Œç”Ÿæˆå™è¿°äº§ç”Ÿç»“æ„åŒ–æ‰¹è¯„ä»¥åŠæ€»ä½“åˆ†æ•°å’Œç»´åº¦è¯„çº§ã€‚</p>
<p><strong>Result:</strong> åœ¨4500ä¸ªå¤šæ ·åŒ–ç—…ä¾‹çš„å®éªŒè¡¨æ˜ï¼ŒDermBenchå’ŒDermEvalä¸ä¸“å®¶è¯„åˆ†é«˜åº¦ä¸€è‡´ï¼Œå¹³å‡åå·®åˆ†åˆ«ä¸º0.251å’Œ0.117ï¼ˆæ»¡åˆ†5åˆ†ï¼‰ï¼Œèƒ½å¤Ÿå¯é æµ‹é‡ä¸åŒå¤šæ¨¡æ€LLMçš„è¯Šæ–­èƒ½åŠ›å’Œå¯ä¿¡åº¦ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥è¯„ä¼°æ¡†æ¶ä¸ºå¤šæ¨¡æ€çš®è‚¤ç—…å­¦AIç³»ç»Ÿæä¾›äº†ç»†ç²’åº¦çš„é€ç—…ä¾‹åˆ†æèƒ½åŠ›ï¼Œè¿™å¯¹äºè¯†åˆ«æ¨¡å‹å±€é™æ€§å’Œåè§è‡³å…³é‡è¦ï¼Œä¸ºå®ç°å¯é çš„ä¸´åºŠéƒ¨ç½²å»ºç«‹äº†æ ‡å‡†åŒ–è¯„ä¼°åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (LLMs) are increasingly used to generate dermatology diagnostic narratives directly from images. However, reliable evaluation remains the primary bottleneck for responsible clinical deployment. We introduce a novel evaluation framework that combines DermBench, a meticulously curated benchmark, with DermEval, a robust automatic evaluator, to enable clinically meaningful, reproducible, and scalable assessment. We build DermBench, which pairs 4,000 real-world dermatology images with expert-certified diagnostic narratives and uses an LLM-based judge to score candidate narratives across clinically grounded dimensions, enabling consistent and comprehensive evaluation of multimodal models. For individual case assessment, we train DermEval, a reference-free multimodal evaluator. Given an image and a generated narrative, DermEval produces a structured critique along with an overall score and per-dimension ratings. This capability enables fine-grained, per-case analysis, which is critical for identifying model limitations and biases. Experiments on a diverse dataset of 4,500 cases demonstrate that DermBench and DermEval achieve close alignment with expert ratings, with mean deviations of 0.251 and 0.117 (out of 5), respectively, providing reliable measurement of diagnostic ability and trustworthiness across different multimodal LLMs.</p>
<h3 id="19-enriching-knowledge-distillation-with-cross-modal-teacher-fusion">[19] <a href="https://arxiv.org/abs/2511.09286">Enriching Knowledge Distillation with Cross-Modal Teacher Fusion</a></h3>
<p><em>Amir M. Mansourian, Amir Mohammad Babaei, Shohreh Kasaei</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºRichKDæ¡†æ¶ï¼Œé€šè¿‡èåˆä¼ ç»Ÿæ•™å¸ˆæ¨¡å‹ä¸CLIPçš„è·¨æ¨¡æ€çŸ¥è¯†æ¥å¢å¼ºçŸ¥è¯†è’¸é¦ï¼Œåˆ©ç”¨å¤šæç¤ºæ–‡æœ¬æŒ‡å¯¼å®ç°è¯­ä¹‰ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•å¹¶å±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦æ–¹æ³•ä¸»è¦ä¾èµ–å•æ¨¡æ€è§†è§‰ä¿¡æ¯ï¼Œç¼ºä¹çŸ¥è¯†å¤šæ ·æ€§ï¼Œå¿½è§†äº†è·¨æ¨¡æ€è¡¨ç¤ºçš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯CLIPçš„è§†è§‰-è¯­è¨€çŸ¥è¯†ä½œä¸ºè¡¥å……ç›‘ç£æºçš„ç ”ç©¶å°šæœªå……åˆ†æ¢ç´¢ã€‚</p>
<p><strong>Method:</strong> æå‡ºç®€å•è€Œæœ‰æ•ˆçš„RichKDæ¡†æ¶ï¼Œå°†ä¼ ç»Ÿæ•™å¸ˆæ¨¡å‹çš„logitså’Œç‰¹å¾ä¸CLIPçš„è¾“å‡ºè¿›è¡Œèåˆï¼Œé€šè¿‡æ•´åˆCLIPçš„å¤šæç¤ºæ–‡æœ¬æŒ‡å¯¼ï¼ŒåŒæ—¶æ•è·æ•°æ®é›†ç‰¹å®šä¿¡æ¯å’Œè¯­ä¹‰å¢å¼ºçš„è§†è§‰çº¿ç´¢ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºç°æœ‰åŸºçº¿ï¼Œèåˆç›‘ç£äº§ç”Ÿæ›´è‡ªä¿¡å¯é çš„é¢„æµ‹ï¼Œæ˜¾è‘—å¢åŠ è‡ªä¿¡æ­£ç¡®æ¡ˆä¾‹å¹¶å‡å°‘è‡ªä¿¡é”™è¯¯æ¡ˆä¾‹ï¼ŒåŒæ—¶æ”¹å–„æ•´ä¸ªlogitåˆ†å¸ƒï¼Œæå‡ç±»é—´ä¸€è‡´æ€§å’Œè’¸é¦è´¨é‡ï¼Œåœ¨åˆ†å¸ƒåç§»å’Œè¾“å…¥æŸåä¸‹å±•ç°å‡ºæ›´å¼ºé²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> èåˆè·¨æ¨¡æ€çŸ¥è¯†å¯æ˜¾è‘—æå‡çŸ¥è¯†è’¸é¦æ•ˆæœï¼ŒCLIPçš„è§†è§‰-è¯­è¨€è¡¨ç¤ºæä¾›äº†è¯­ä¹‰ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ï¼Œè¯¥æ–¹æ³•ä¸ºçŸ¥è¯†è’¸é¦å¼€è¾Ÿäº†åˆ©ç”¨é¢„è®­ç»ƒè·¨æ¨¡æ€æ¨¡å‹çš„æ–°æ–¹å‘ï¼Œè¯æ˜äº†ç®€å•è€Œæœ‰æ•ˆçš„èåˆç­–ç•¥çš„å®ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Multi-teacher knowledge distillation (KD), a more effective technique than traditional single-teacher methods, transfers knowledge from expert teachers to a compact student model using logit or feature matching. However, most existing approaches lack knowledge diversity, as they rely solely on unimodal visual information, overlooking the potential of cross-modal representations. In this work, we explore the use of CLIP's vision-language knowledge as a complementary source of supervision for KD, an area that remains largely underexplored. We propose a simple yet effective framework that fuses the logits and features of a conventional teacher with those from CLIP. By incorporating CLIP's multi-prompt textual guidance, the fused supervision captures both dataset-specific and semantically enriched visual cues. Beyond accuracy, analysis shows that the fused teacher yields more confident and reliable predictions, significantly increasing confident-correct cases while reducing confidently wrong ones. Moreover, fusion with CLIP refines the entire logit distribution, producing semantically meaningful probabilities for non-target classes, thereby improving inter-class consistency and distillation quality. Despite its simplicity, the proposed method, Enriching Knowledge Distillation (RichKD), consistently outperforms most existing baselines across multiple benchmarks and exhibits stronger robustness under distribution shifts and input corruptions.</p>
<h3 id="20-fq-petr-fully-quantized-position-embedding-transformation-for-multi-view-3d-object-detection">[20] <a href="https://arxiv.org/abs/2511.09347">FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection</a></h3>
<p><em>Jiangyong Yu, Changyong Shu, Sifan Zhou, Zichen Yu, Xing Hu, Yan Chen, Dawei Yang</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FQ-PETRï¼Œä¸€ç§é’ˆå¯¹PETRç³»åˆ—3Dæ£€æµ‹æ¨¡å‹çš„å®Œå…¨é‡åŒ–æ¡†æ¶ï¼Œé€šè¿‡é‡åŒ–å‹å¥½çš„ä½ç½®åµŒå…¥ã€åŒæŸ¥æ‰¾è¡¨æŠ€æœ¯å’Œé‡åŒ–åæ•°å€¼ç¨³å®šåŒ–æ–¹æ³•ï¼Œåœ¨W8A8é‡åŒ–ä¸‹å®ç°äº†æ¥è¿‘æµ®ç‚¹ç²¾åº¦çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å»¶è¿Ÿã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> PETRç³»åˆ—æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶å¤šè§†å›¾3Dæ£€æµ‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œå†…å­˜å ç”¨çš„æŒ‘æˆ˜ã€‚ç›´æ¥åº”ç”¨ç°æœ‰é‡åŒ–æ–¹æ³•ä¼šå¯¼è‡´ä¸¥é‡çš„ç²¾åº¦ä¸‹é™ï¼Œä¸»è¦é—®é¢˜æºäºå¤šæ¨¡æ€ç‰¹å¾é—´æ˜¾è‘—çš„é‡çº§å·®å¼‚ä»¥åŠéçº¿æ€§ç®—å­é‡åŒ–ä¸­çš„æ•ˆç‡ä½ä¸‹å’Œè¿‘ä¼¼è¯¯å·®ã€‚</p>
<p><strong>Method:</strong> FQ-PETRæ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®æŠ€æœ¯ï¼šé‡åŒ–å‹å¥½çš„LiDARå°„çº¿ä½ç½®åµŒå…¥ï¼ˆQFPEï¼‰é€šè¿‡å•ç‚¹é‡‡æ ·å’ŒåŸºäºé”šç‚¹çš„åµŒå…¥æ¶ˆé™¤éçº¿æ€§æ“ä½œï¼›åŒæŸ¥æ‰¾è¡¨ï¼ˆDULUTï¼‰ä½¿ç”¨ä¸¤ä¸ªçº§è”çº¿æ€§æŸ¥æ‰¾è¡¨é«˜ç²¾åº¦è¿‘ä¼¼å¤æ‚éçº¿æ€§å‡½æ•°ï¼›é‡åŒ–åæ•°å€¼ç¨³å®šåŒ–ï¼ˆQANSï¼‰åœ¨softmaxæ•°å€¼ç¨³å®šåŒ–åæ‰§è¡Œé‡åŒ–ä»¥å‡å°‘æ³¨æ„åŠ›å¤±çœŸã€‚</p>
<p><strong>Result:</strong> åœ¨PETRã€StreamPETRã€PETRv2ã€MV2dç­‰æ¨¡å‹ä¸Šï¼ŒFQ-PETRåœ¨W8A8é‡åŒ–ä¸‹å®ç°äº†æ¥è¿‘æµ®ç‚¹ç²¾åº¦ï¼ˆä»…1%æ€§èƒ½ä¸‹é™ï¼‰ï¼ŒåŒæ—¶å°†å»¶è¿Ÿé™ä½é«˜è¾¾75%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„PTQå’ŒQATåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„é‡åŒ–ç­–ç•¥å¯ä»¥æœ‰æ•ˆè§£å†³å¤šæ¨¡æ€ç‰¹å¾é‡çº§å·®å¼‚å’Œéçº¿æ€§ç®—å­é‡åŒ–éš¾é¢˜ï¼Œä¸º3Dæ£€æµ‹æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼ŒåŒæ—¶è¯æ˜äº†åœ¨ä¿æŒç²¾åº¦çš„å‰æä¸‹å®ç°æ˜¾è‘—è®¡ç®—ä¼˜åŒ–çš„å¯èƒ½æ€§ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) significant magnitude disparity between multi-modal features-specifically, image features and camera-ray positional embeddings (PE), and (2) the inefficiency and approximation error of quantizing non-linear operators, which commonly rely on hardware-unfriendly computations. In this paper, we propose FQ-PETR, a fully quantized framework for PETRs, featuring three key innovations: (1) Quantization-Friendly LiDAR-ray Position Embedding (QFPE): Replacing multi-point sampling with LiDAR-prior-guided single-point sampling and anchor-based embedding eliminates problematic non-linearities (e.g., inverse-sigmoid) and aligns PE scale with image features, preserving accuracy. (2) Dual-Lookup Table (DULUT): This algorithm approximates complex non-linear functions using two cascaded linear LUTs, achieving high fidelity with minimal entries and no specialized hardware. (3) Quantization After Numerical Stabilization (QANS): Performing quantization after softmax numerical stabilization mitigates attention distortion from large inputs. On PETRs (e.g. PETR, StreamPETR, PETRv2, MV2d), FQ-PETR under W8A8 achieves near-floating-point accuracy (1% degradation) while reducing latency by up to 75%, significantly outperforming existing PTQ and QAT baselines.</p>
<h3 id="21-learning-by-neighbor-aware-semantics-deciding-by-open-form-flows-towards-robust-zero-shot-skeleton-action-recognition">[21] <a href="https://arxiv.org/abs/2511.09388">Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition</a></h3>
<p><em>Yang Chen, Miaoge Li, Zhijie Rao, Deze Zeng, Song Guo, Jingcai Guo</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFloraçš„é›¶æ ·æœ¬éª¨æ¶åŠ¨ä½œè¯†åˆ«æ–¹æ³•ï¼Œé€šè¿‡çµæ´»çš„é‚»å±…æ„ŸçŸ¥è¯­ä¹‰è°ƒè°å’Œå¼€æ”¾å½¢å¼åˆ†å¸ƒæ„ŸçŸ¥æµåˆ†ç±»å™¨ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­çš„ç‚¹å¯¹ç‚¹å¯¹é½è„†å¼±æ€§å’Œåˆ†ç±»å™¨å†³ç­–è¾¹ç•ŒåƒµåŒ–é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é›¶æ ·æœ¬éª¨æ¶åŠ¨ä½œè¯†åˆ«æ–¹æ³•éµå¾ªâ€œå¯¹é½ååˆ†ç±»â€èŒƒå¼ï¼Œä½†é¢ä¸´ä¸¤ä¸ªåŸºæœ¬é—®é¢˜ï¼šç”±äºä¸å®Œç¾è¯­ä¹‰å¯¼è‡´çš„è„†å¼±ç‚¹å¯¹ç‚¹å¯¹é½ï¼Œä»¥åŠå—é™äºé™æ€å†³ç­–è¾¹ç•Œå’Œç²—ç²’åº¦é”šç‚¹çš„åƒµåŒ–åˆ†ç±»å™¨ã€‚</p>
<p><strong>Method:</strong> Floraæ–¹æ³•åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šçµæ´»çš„é‚»å±…æ„ŸçŸ¥è¯­ä¹‰è°ƒè°é€šè¿‡æ•´åˆç›¸é‚»ç±»é—´ä¸Šä¸‹æ–‡çº¿ç´¢å½¢æˆæ–¹å‘æ„ŸçŸ¥åŒºåŸŸè¯­ä¹‰ï¼Œç»“åˆè·¨æ¨¡æ€å‡ ä½•ä¸€è‡´æ€§ç›®æ ‡ç¡®ä¿ç¨³å®šçš„ç‚¹å¯¹åŒºåŸŸå¯¹é½ï¼›é‡‡ç”¨æ— å™ªå£°æµåŒ¹é…æ¥å¼¥åˆè¯­ä¹‰ä¸éª¨æ¶æ½œåœ¨åµŒå…¥ä¹‹é—´çš„æ¨¡æ€åˆ†å¸ƒå·®è·ï¼ŒåŒæ—¶é€šè¿‡æ— æ¡ä»¶å¯¹æ¯”æ­£åˆ™åŒ–å¢å¼ºåŒºåˆ†æ€§ï¼Œå®ç°åŸºäºä»¤ç‰Œçº§é€Ÿåº¦é¢„æµ‹çš„ç»†ç²’åº¦å†³ç­–è¾¹ç•Œåˆ†å¸ƒæ„ŸçŸ¥åˆ†ç±»å™¨ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå³ä½¿åœ¨ä»…ä½¿ç”¨10%å¯è§æ•°æ®è®­ç»ƒçš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºç‰¹åˆ«ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡çµæ´»çš„è¯­ä¹‰å¯¹é½å’Œåˆ†å¸ƒæ„ŸçŸ¥åˆ†ç±»å™¨è®¾è®¡ï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³é›¶æ ·æœ¬éª¨æ¶åŠ¨ä½œè¯†åˆ«ä¸­çš„å¯¹é½è„†å¼±æ€§å’Œåˆ†ç±»å™¨åƒµåŒ–é—®é¢˜ï¼Œä¸ºè·¨æ¨¡æ€åŠ¨ä½œè¯†åˆ«æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Recognizing unseen skeleton action categories remains highly challenging due to the absence of corresponding skeletal priors. Existing approaches generally follow an "align-then-classify" paradigm but face two fundamental issues, i.e., (i) fragile point-to-point alignment arising from imperfect semantics, and (ii) rigid classifiers restricted by static decision boundaries and coarse-grained anchors. To address these issues, we propose a novel method for zero-shot skeleton action recognition, termed $\texttt{$\textbf{Flora}$}$, which builds upon $\textbf{F}$lexib$\textbf{L}$e neighb$\textbf{O}$r-aware semantic attunement and open-form dist$\textbf{R}$ibution-aware flow cl$\textbf{A}$ssifier. Specifically, we flexibly attune textual semantics by incorporating neighboring inter-class contextual cues to form direction-aware regional semantics, coupled with a cross-modal geometric consistency objective that ensures stable and robust point-to-region alignment. Furthermore, we employ noise-free flow matching to bridge the modality distribution gap between semantic and skeleton latent embeddings, while a condition-free contrastive regularization enhances discriminability, leading to a distribution-aware classifier with fine-grained decision boundaries achieved through token-level velocity predictions. Extensive experiments on three benchmark datasets validate the effectiveness of our method, showing particularly impressive performance even when trained with only 10\% of the seen data. Code is available at https://github.com/cseeyangchen/Flora.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="22-self-harmllm-can-large-language-model-harm-itself">[22] <a href="https://arxiv.org/abs/2511.08597">Self-HarmLLM: Can Large Language Model Harm Itself?</a></h3>
<p><em>Heehwan Kim, Sungjune Park, Daeseon Choi</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†Self-HarmLLMæ”»å‡»åœºæ™¯ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«ç”Ÿæˆçš„æ¨¡ç³Šæœ‰å®³æŸ¥è¯¢ä½œä¸ºæ–°è¾“å…¥æ¥ç»•è¿‡å®‰å…¨é˜²æŠ¤ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ¡ä»¶ä¸‹åˆ†åˆ«è¾¾åˆ°æœ€é«˜33%å’Œ41%çš„è¶Šç‹±æˆåŠŸç‡ï¼Œæ­ç¤ºäº†ç°æœ‰é˜²æŠ¤æœºåˆ¶çš„æ½œåœ¨æ¼æ´ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰LLMå®‰å…¨é˜²æŠ¤æœºåˆ¶ä¸»è¦å‡è®¾æ”»å‡»æ¥è‡ªå¤–éƒ¨æ¶æ„æŸ¥è¯¢ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹è‡ªèº«è¾“å‡ºå¯èƒ½æˆä¸ºæ–°æ”»å‡»å‘é‡çš„å¯èƒ½æ€§ï¼Œè¿™ç§è‡ªæˆ‘æ”»å‡»åœºæ™¯å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</p>
<p><strong>Method:</strong> æå‡ºSelf-HarmLLMæ”»å‡»æ–¹æ³•ï¼Œä½¿ç”¨åŒä¸€æ¨¡å‹ç”Ÿæˆçš„Mitigated Harmful Queryä½œä¸ºæ–°è¾“å…¥ï¼Œè¿™äº›æŸ¥è¯¢ä¿æŒåŸå§‹æ„å›¾ä½†éšè—äº†æœ‰å®³æ€§è´¨ï¼Œå¹¶åœ¨GPT-3.5-turboã€LLaMA3-8B-instructå’ŒDeepSeek-R1-Distill-Qwen-7Bæ¨¡å‹ä¸Šè¿›è¡ŒBaseã€Zero-shotå’ŒFew-shotæ¡ä»¶ä¸‹çš„å®éªŒéªŒè¯ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨Zero-shotæ¡ä»¶ä¸‹æœ€é«˜è¾¾åˆ°52%çš„è½¬æ¢æˆåŠŸç‡å’Œ33%çš„è¶Šç‹±æˆåŠŸç‡ï¼ŒFew-shotæ¡ä»¶ä¸‹æœ€é«˜è¾¾åˆ°65%çš„è½¬æ¢æˆåŠŸç‡å’Œ41%çš„è¶Šç‹±æˆåŠŸç‡ï¼ŒåŒæ—¶å‘ç°è‡ªåŠ¨è¯„ä¼°ä¼šç³»ç»Ÿæ€§é«˜ä¼°è¶Šç‹±æˆåŠŸç‡ï¼Œå¹³å‡å·®å¼‚è¾¾52%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜åŸºäºæ¨¡å‹è‡ªèº«è¾“å‡ºçš„æ”»å‡»æ˜¯æœ‰æ•ˆçš„æ”»å‡»åœºæ™¯ï¼Œè¡¨æ˜éœ€è¦ä»æ ¹æœ¬ä¸Šé‡æ–°è€ƒè™‘é˜²æŠ¤æœºåˆ¶è®¾è®¡å¹¶å»ºç«‹æ›´é²æ£’çš„è¯„ä¼°æ–¹æ³•ï¼ŒåŒæ—¶å¼ºè°ƒè‡ªåŠ¨è¯„ä¼°å•ç‹¬ä½¿ç”¨ä¸è¶³ä»¥å‡†ç¡®åˆ¤æ–­æœ‰å®³æ€§ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Large Language Models (LLMs) are generally equipped with guardrails to block the generation of harmful responses. However, existing defenses always assume that an external attacker crafts the harmful query, and the possibility of a model's own output becoming a new attack vector has not been sufficiently explored. In this study, we propose the Self-HarmLLM scenario, which uses a Mitigated Harmful Query (MHQ) generated by the same model as a new input. An MHQ is an ambiguous query whose original intent is preserved while its harmful nature is not directly exposed. We verified whether a jailbreak occurs when this MHQ is re-entered into a separate session of the same model. We conducted experiments on GPT-3.5-turbo, LLaMA3-8B-instruct, and DeepSeek-R1-Distill-Qwen-7B under Base, Zero-shot, and Few-shot conditions. The results showed up to 52% transformation success rate and up to 33% jailbreak success rate in the Zero-shot condition, and up to 65% transformation success rate and up to 41% jailbreak success rate in the Few-shot condition. By performing both prefix-based automated evaluation and human evaluation, we found that the automated evaluation consistently overestimated jailbreak success, with an average difference of 52%. This indicates that automated evaluation alone is not accurate for determining harmfulness. While this study is a toy-level study based on a limited query set and evaluators, it proves that our method can still be a valid attack scenario. These results suggest the need for a fundamental reconsideration of guardrail design and the establishment of a more robust evaluation methodology.</p>
<h3 id="23-halluclean-a-unified-framework-to-combat-hallucinations-in-llms">[23] <a href="https://arxiv.org/abs/2511.08916">HalluClean: A Unified Framework to Combat Hallucinations in LLMs</a></h3>
<p><em>Yaxin Zhao, Yu Zhang</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>HalluCleanæ˜¯ä¸€ä¸ªè½»é‡çº§ã€ä»»åŠ¡æ— å…³çš„æ¡†æ¶ï¼Œé€šè¿‡æ¨ç†å¢å¼ºèŒƒå¼æ£€æµ‹å’Œçº æ­£å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ä¸­çš„å¹»è§‰å†…å®¹ï¼Œæ˜¾è‘—æå‡äº‹å®ä¸€è‡´æ€§ä¸”æ— éœ€å¤–éƒ¨çŸ¥è¯†æºæˆ–ç›‘ç£æ£€æµ‹å™¨ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§è¯­è¨€æ¨¡å‹åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç»å¸¸äº§ç”Ÿå¹»è§‰å†…å®¹ï¼Œè¿™ä¸¥é‡æŸå®³äº†äº‹å®å¯é æ€§ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿæ£€æµ‹å’Œçº æ­£è¿™äº›å¹»è§‰çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> HalluCleané‡‡ç”¨æ¨ç†å¢å¼ºèŒƒå¼ï¼Œå°†è¿‡ç¨‹æ˜ç¡®åˆ†è§£ä¸ºè§„åˆ’ã€æ‰§è¡Œå’Œä¿®è®¢ä¸‰ä¸ªé˜¶æ®µæ¥è¯†åˆ«å’Œä¼˜åŒ–æ— ä¾æ®çš„ä¸»å¼ ï¼Œä½¿ç”¨æœ€å°ä»»åŠ¡è·¯ç”±æç¤ºå®ç°è·¨é¢†åŸŸçš„é›¶æ ·æœ¬æ³›åŒ–ã€‚</p>
<p><strong>Result:</strong> åœ¨äº”ä¸ªä»£è¡¨æ€§ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒHalluCleanæ˜¾è‘—æ”¹å–„äº†äº‹å®ä¸€è‡´æ€§ï¼Œå¹¶åœ¨é—®ç­”ã€å¯¹è¯ã€æ‘˜è¦ã€æ•°å­¦æ–‡å­—é—®é¢˜å’ŒçŸ›ç›¾æ£€æµ‹ä»»åŠ¡ä¸­ä¼˜äºç«äº‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†HalluCleanåœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹è¾“å‡ºå¯ä¿¡åº¦æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„å¹»è§‰é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è½»é‡çº§è§£å†³æ–¹æ¡ˆï¼Œä¸”æ— éœ€ä¾èµ–å¤–éƒ¨èµ„æºæˆ–ç›‘ç£è®­ç»ƒã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Large language models (LLMs) have achieved impressive performance across a wide range of natural language processing tasks, yet they often produce hallucinated content that undermines factual reliability. To address this challenge, we introduce HalluClean, a lightweight and task-agnostic framework for detecting and correcting hallucinations in LLM-generated text. HalluClean adopts a reasoning-enhanced paradigm, explicitly decomposing the process into planning, execution, and revision stages to identify and refine unsupported claims. It employs minimal task-routing prompts to enable zero-shot generalization across diverse domains, without relying on external knowledge sources or supervised detectors. We conduct extensive evaluations on five representative tasks-question answering, dialogue, summarization, math word problems, and contradiction detection. Experimental results show that HalluClean significantly improves factual consistency and outperforms competitive baselines, demonstrating its potential to enhance the trustworthiness of LLM outputs in real-world applications.</p>
<h3 id="24-mm-critic-a-holistic-evaluation-of-large-multimodal-models-as-multimodal-critique">[24] <a href="https://arxiv.org/abs/2511.09067">MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique</a></h3>
<p><em>Gailun Zeng, Ziyang Luo, Hongzhan Lin, Yuchen Tian, Kaixin Li, Ziyang Gong, Jianxiong Guo, Jing Ma</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MM-CRITICåŸºå‡†ï¼Œè¿™æ˜¯é¦–ä¸ªå…¨é¢è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ‰¹åˆ¤èƒ½åŠ›çš„åŸºå‡†ï¼Œæ¶µç›–8ç§ä¸»è¦ä»»åŠ¡ç±»å‹å’Œ500å¤šä¸ªä»»åŠ¡ï¼ŒåŒ…å«4471ä¸ªæ ·æœ¬ï¼Œä¸ºå¤šæ¨¡æ€æ‰¹åˆ¤èƒ½åŠ›ç ”ç©¶æä¾›äº†ç³»ç»Ÿè¯„ä¼°æ¡†æ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒæè¿°å’Œè§†è§‰æ¨ç†ç­‰ä»»åŠ¡ä¸­èƒ½åŠ›ä¸æ–­å¢å¼ºï¼Œä½†å¤šæ¨¡æ€æ‰¹åˆ¤èƒ½åŠ›çš„ç ”ç©¶ä»ç›¸å¯¹ä¸è¶³ï¼Œè€Œæ‰¹åˆ¤èƒ½åŠ›å¯¹äºæ¨¡å‹è‡ªæˆ‘æ”¹è¿›å’Œæˆä¸ºå¯é AIåŠ©æ‰‹è‡³å…³é‡è¦ï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨çº¯è¯­è¨€ç¯å¢ƒä¸‹çš„æ‰¹åˆ¤èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†MM-CRITICåŸºå‡†ï¼Œæ¶µç›–åŸºæœ¬ã€ä¿®æ­£å’Œæ¯”è¾ƒä¸‰ä¸ªç»´åº¦çš„æ‰¹åˆ¤èƒ½åŠ›è¯„ä¼°ï¼Œæ•´åˆä¸“å®¶æŒ‡å¯¼çš„å‚è€ƒç­”æ¡ˆåˆ°è¯„åˆ†æ ‡å‡†ä¸­ï¼Œä½¿ç”¨GPT-4oæ ‡æ³¨å“åº”å¹¶ç”Ÿæˆå‚è€ƒæ‰¹åˆ¤ä½œä¸ºå¯é åˆ¤æ–­çš„é”šç‚¹ï¼Œè¦†ç›–ä¸åŒæ¨¡å‹å¤§å°çš„å¤šç§LMMå“åº”ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›å®éªŒéªŒè¯äº†MM-CRITICçš„æœ‰æ•ˆæ€§ï¼Œå¯¹ä¸»æµLMMsçš„æ‰¹åˆ¤èƒ½åŠ›è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œåˆ†ææ­ç¤ºäº†å“åº”è´¨é‡ä¸æ‰¹åˆ¤èƒ½åŠ›ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»¥åŠä¸åŒè¯„ä¼°ç»´åº¦ä¸‹æ‰¹åˆ¤éš¾åº¦çš„å˜åŒ–è§„å¾‹ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ä¸ºå¤šæ¨¡æ€æ‰¹åˆ¤èƒ½åŠ›è¯„ä¼°æä¾›äº†æ ‡å‡†åŒ–åŸºå‡†ï¼Œæ­ç¤ºäº†LMMsæ‰¹åˆ¤èƒ½åŠ›çš„ç°çŠ¶å’ŒæŒ‘æˆ˜ï¼Œä¸ºæœªæ¥æ¨¡å‹æ”¹è¿›å’Œæ‰¹åˆ¤èƒ½åŠ›ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ï¼Œä»£ç å·²å¼€æºä¾›ç¤¾åŒºä½¿ç”¨ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic.</p>
<h3 id="25-a-hybrid-search-for-complex-table-question-answering-in-securities-report">[25] <a href="https://arxiv.org/abs/2511.09179">A Hybrid Search for Complex Table Question Answering in Securities Report</a></h3>
<p><em>Daiki Shirafuji, Koji Tanaka, Tatsuhiko Saito</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€äººå·¥è¯†åˆ«çš„è¡¨æ ¼é—®ç­”ç»†èƒæå–æ–¹æ³•ï¼Œé€šè¿‡æ··åˆæ£€ç´¢æœºåˆ¶è®¡ç®—é—®é¢˜ä¸å•å…ƒæ ¼ç›¸ä¼¼åº¦æ¥è‡ªåŠ¨è¯†åˆ«å¤æ‚è¡¨å¤´ï¼Œåœ¨NTCIR-18 U4å…±äº«ä»»åŠ¡çš„TQAæ•°æ®é›†ä¸Šè¾¾åˆ°74.6%çš„å‡†ç¡®ç‡ï¼Œä¼˜äºGPT-4o miniç­‰ç°æœ‰LLMã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡¨æ ¼é—®ç­”ä»»åŠ¡ä¸­ç›´æ¥å°†æ•´ä¸ªè¡¨æ ¼ä½œä¸ºé•¿æ–‡æœ¬è¾“å…¥ä¼šå¯¼è‡´é”™è¯¯ç­”æ¡ˆï¼Œå› ä¸ºå¤§å¤šæ•°LLMæ— æ³•æœ‰æ•ˆæ•æ‰å¤æ‚çš„è¡¨æ ¼ç»“æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚è¡¨å¤´æ—¶ç¼ºä¹è‡ªåŠ¨è¯†åˆ«èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŸºäºæ··åˆæ£€ç´¢çš„ç»†èƒæå–æ–¹æ³•ï¼Œç»“åˆè¯­è¨€æ¨¡å‹å’ŒTF-IDFè®¡ç®—ç»™å®šé—®é¢˜ä¸å•ä¸ªå•å…ƒæ ¼çš„ç›¸ä¼¼åº¦æ¥ä¼°è®¡è¡¨å¤´ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ åœ¨å°è§„æ¨¡é—®é¢˜-è¡¨å¤´å¯¹æ•°æ®é›†ä¸Šè®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæœ€ç»ˆé€‰æ‹©æœ€ç›¸å…³è¡Œå’Œåˆ—äº¤å‰ç‚¹çš„å•å…ƒæ ¼ä½œä¸ºç­”æ¡ˆã€‚</p>
<p><strong>Result:</strong> åœ¨NTCIR-18 U4å…±äº«ä»»åŠ¡çš„TQAæ•°æ®é›†è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°74.6%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºGPT-4o miniçš„63.9%ï¼Œè¯æ˜äº†æ··åˆæ£€ç´¢æœºåˆ¶åœ¨è¡¨æ ¼ç»“æ„ç†è§£ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•å±•ç¤ºäº†æ— éœ€äººå·¥å¹²é¢„å³å¯å¤„ç†å¤æ‚è¡¨æ ¼ç»“æ„çš„èƒ½åŠ›ï¼Œè™½ç„¶å½“å‰ä½¿ç”¨ä¼ ç»Ÿç¼–ç å™¨æ¨¡å‹è¿›è¡Œæ£€ç´¢ï¼Œæœªæ¥è®¡åˆ’æ•´åˆæ›´é«˜æ•ˆçš„æ–‡æœ¬æœç´¢æ¨¡å‹ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½å¹¶ç¼©å°ä¸äººå·¥è¯„ä¼°ç»“æœçš„å·®è·ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Recently, Large Language Models (LLMs) are gaining increased attention in the domain of Table Question Answering (TQA), particularly for extracting information from tables in documents. However, directly entering entire tables as long text into LLMs often leads to incorrect answers because most LLMs cannot inherently capture complex table structures. In this paper, we propose a cell extraction method for TQA without manual identification, even for complex table headers. Our approach estimates table headers by computing similarities between a given question and individual cells via a hybrid retrieval mechanism that integrates a language model and TF-IDF. We then select as the answer the cells at the intersection of the most relevant row and column. Furthermore, the language model is trained using contrastive learning on a small dataset of question-header pairs to enhance performance. We evaluated our approach in the TQA dataset from the U4 shared task at NTCIR-18. The experimental results show that our pipeline achieves an accuracy of 74.6\%, outperforming existing LLMs such as GPT-4o mini~(63.9\%). In the future, although we used traditional encoder models for retrieval in this study, we plan to incorporate more efficient text-search models to improve performance and narrow the gap with human evaluation results.</p>
<h3 id="26-context-is-enough-empirical-validation-of-textitsequentiality-on-essays">[26] <a href="https://arxiv.org/abs/2511.09185">Context is Enough: Empirical Validation of $\textit{Sequentiality}$ on Essays</a></h3>
<p><em>Amal Sunny, Advay Gupta, Vishnu Sreekumar</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶å®è¯éªŒè¯äº†åŸºäºä¸Šä¸‹æ–‡çš„åºåˆ—æ€§åº¦é‡åœ¨è‡ªåŠ¨ä½œæ–‡è¯„åˆ†ä¸­çš„æœ‰æ•ˆæ€§ï¼Œè¯¥åº¦é‡æ¯”åŸå§‹çš„ä¸»é¢˜-ä¸Šä¸‹æ–‡æ··åˆç‰ˆæœ¬æ›´ç¬¦åˆäººç±»å¯¹ç¯‡ç« è¿è´¯æ€§çš„è¯„ä¼°ï¼Œå¹¶ä¸æ ‡å‡†è¯­è¨€ç‰¹å¾ç»“åˆæ—¶ä¼˜äºé›¶æ ·æœ¬LLMé¢„æµ‹ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å…ˆå‰ç ”ç©¶æå‡ºä½¿ç”¨LLMsé€šè¿‡åºåˆ—æ€§åº¦é‡æ¥é‡åŒ–å™äº‹æµï¼Œä½†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å—åˆ°è´¨ç–‘ï¼Œç‰¹åˆ«æ˜¯ä¸»é¢˜é€‰æ‹©æ–¹å¼å¯èƒ½æ··æ·†ç»“æœï¼Œä¸”ç¼ºä¹ä¸çœŸå®æµç•…åº¦è¯„ä¼°çš„éªŒè¯ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å®è¯éªŒè¯ä»…ä½¿ç”¨ä¸Šä¸‹æ–‡é¡¹çš„åºåˆ—æ€§åº¦é‡ä½œä¸ºæ›´æ¦‚å¿µæœ‰æ•ˆå’Œå¯è§£é‡Šçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> ä½¿ç”¨ä¸¤ä¸ªå¸¦æœ‰äººå·¥æ ‡æ³¨ç‰¹è´¨åˆ†æ•°çš„ä½œæ–‡æ•°æ®é›†ASAP++å’ŒELLIPSEï¼Œæ¯”è¾ƒä¸Šä¸‹æ–‡ç‰ˆæœ¬åºåˆ—æ€§ä¸äººç±»å¯¹ç»„ç»‡å’Œè¿è´¯æ€§ç­‰ç¯‡ç« å±‚é¢ç‰¹è´¨çš„è¯„ä¼°ã€‚å°†ä¸Šä¸‹æ–‡åº¦é‡ä¸æ ‡å‡†è¯­è¨€ç‰¹å¾ç»“åˆï¼Œå¹¶ä¸é›¶æ ·æœ¬æç¤ºLLMsçš„é¢„æµ‹è¿›è¡Œå¯¹æ¯”åˆ†æã€‚</p>
<p><strong>Result:</strong> ä¸Šä¸‹æ–‡åºåˆ—æ€§ç‰ˆæœ¬ä¸äººç±»å¯¹ç»„ç»‡å’Œè¿è´¯æ€§çš„è¯„ä¼°æ›´ä¸€è‡´ã€‚è™½ç„¶é›¶æ ·æœ¬æç¤ºLLMsåœ¨é¢„æµ‹ç‰¹è´¨åˆ†æ•°ä¸Šæ¯”å•ç‹¬ä½¿ç”¨ä¸Šä¸‹æ–‡åº¦é‡æ›´å‡†ç¡®ï¼Œä½†ä¸Šä¸‹æ–‡åº¦é‡ä¸æ ‡å‡†è¯­è¨€ç‰¹å¾ç»“åˆæ—¶æ¯”ä»…ä¸»é¢˜ç‰ˆæœ¬å’ŒåŸå§‹åºåˆ—æ€§å…¬å¼å¢åŠ æ›´å¤šé¢„æµ‹ä»·å€¼ï¼Œä¸”è¿™ç§ç»„åˆä¹Ÿä¼˜äºé›¶æ ·æœ¬LLMé¢„æµ‹ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœæ”¯æŒä½¿ç”¨åŸºäºä¸Šä¸‹æ–‡çš„åºåˆ—æ€§ä½œä¸ºç»è¿‡éªŒè¯ã€å¯è§£é‡Šä¸”äº’è¡¥çš„ç‰¹å¾ï¼Œç”¨äºè‡ªåŠ¨ä½œæ–‡è¯„åˆ†å’Œç›¸å…³NLPä»»åŠ¡ã€‚æ˜ç¡®å»ºæ¨¡å¥å­é—´æµåŠ¨æ€§çš„æ–¹æ³•å…·æœ‰é‡è¦ä»·å€¼ï¼Œä¸Šä¸‹æ–‡åºåˆ—æ€§æä¾›äº†æ¯”æ··åˆæ–¹æ³•æ›´å¯é å’Œå¯è§£é‡Šçš„ç¯‡ç« è¿è´¯æ€§åº¦é‡ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Recent work has proposed using Large Language Models (LLMs) to quantify narrative flow through a measure called sequentiality, which combines topic and contextual terms. A recent critique argued that the original results were confounded by how topics were selected for the topic-based component, and noted that the metric had not been validated against ground-truth measures of flow. That work proposed using only the contextual term as a more conceptually valid and interpretable alternative. In this paper, we empirically validate that proposal. Using two essay datasets with human-annotated trait scores, ASAP++ and ELLIPSE, we show that the contextual version of sequentiality aligns more closely with human assessments of discourse-level traits such as Organization and Cohesion. While zero-shot prompted LLMs predict trait scores more accurately than the contextual measure alone, the contextual measure adds more predictive value than both the topic-only and original sequentiality formulations when combined with standard linguistic features. Notably, this combination also outperforms the zero-shot LLM predictions, highlighting the value of explicitly modeling sentence-to-sentence flow. Our findings support the use of context-based sequentiality as a validated, interpretable, and complementary feature for automated essay scoring and related NLP tasks.</p>
<h3 id="27-potsa-a-cross-lingual-speech-alignment-framework-for-low-resource-speech-to-text-translation">[27] <a href="https://arxiv.org/abs/2511.09232">POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation</a></h3>
<p><em>Xuanchen Li, Chenrui Cui, Tianrui Wang, Meng Ge, Zikang Huang, Jin Li, Yizhou Peng, Longbiao Wang, Jianwu Dang, Nyima Tashi</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†POTSAæ¡†æ¶ï¼ŒåŸºäºè·¨è¯­è¨€å¹³è¡Œè¯­éŸ³å¯¹å’Œæœ€ä¼˜ä¼ è¾“ç†è®ºï¼Œé€šè¿‡åç½®è¡¥å¿æ¨¡å—å’Œtokençº§OTçº¦æŸæ¥å¼¥åˆé«˜ä½èµ„æºè¯­è¨€é—´çš„ç¿»è¯‘å·®è·ï¼Œåœ¨FLEURSæ•°æ®é›†ä¸Šå®ç°äº†SOTAæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€è¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ä¸­å¾€å¾€å¿½ç•¥è·¨æºè¯­è¨€çš„è¯­ä¹‰å…±æ€§ï¼Œå¯¼è‡´ç¿»è¯‘æ€§èƒ½å­˜åœ¨åå·®ï¼Œç‰¹åˆ«æ˜¯é«˜ä½èµ„æºè¯­è¨€ä¹‹é—´çš„ç¿»è¯‘å·®è·é—®é¢˜æ²¡æœ‰å¾—åˆ°æœ‰æ•ˆè§£å†³ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†POTSAæ¡†æ¶ï¼Œé¦–å…ˆå¼•å…¥åç½®è¡¥å¿æ¨¡å—å¯¹åˆå§‹è¯­éŸ³è¡¨å¾è¿›è¡Œç²—ç²’åº¦è·¨è¯­è¨€å¯¹é½ï¼Œç„¶ååœ¨Q-Formerä¸Šæ–½åŠ åŸºäºå¹³è¡Œè¯­éŸ³å¯¹çš„tokençº§æœ€ä¼˜ä¼ è¾“çº¦æŸä»¥å»ºç«‹ç»†ç²’åº¦è¡¨å¾ä¸€è‡´æ€§ï¼Œå¹¶é‡‡ç”¨å±‚è°ƒåº¦ç­–ç•¥å°†OTçº¦æŸèšç„¦äºæœ€å…·è¯­ä¹‰ä»·å€¼çš„å±‚ã€‚</p>
<p><strong>Result:</strong> åœ¨FLEURSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ç§å¸¸ç”¨è¯­è¨€ä¸Šå¹³å‡æå‡0.93 BLEUï¼Œåœ¨é›¶æ ·æœ¬è¯­è¨€ä¸Šæå‡5.05 BLEUï¼Œä¸”æ¯ä¸ªæºè¯­è¨€ä»…éœ€10å°æ—¶çš„å¹³è¡Œè¯­éŸ³æ•°æ®ã€‚</p>
<p><strong>Conclusion:</strong> POTSAæ¡†æ¶é€šè¿‡ç»“åˆåç½®è¡¥å¿å’Œæœ€ä¼˜ä¼ è¾“çº¦æŸï¼Œæœ‰æ•ˆè§£å†³äº†è·¨è¯­è¨€è¯­éŸ³ç¿»è¯‘ä¸­çš„è¯­ä¹‰å¯¹é½é—®é¢˜ï¼Œä¸ºé«˜ä½èµ„æºè¯­è¨€é—´çš„ç¿»è¯‘æ€§èƒ½å‡è¡¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Speech Large Language Models (SpeechLLMs) have achieved breakthroughs in multilingual speech-to-text translation (S2TT). However, existing approaches often overlook semantic commonalities across source languages, leading to biased translation performance. In this work, we propose \textbf{POTSA} (Parallel Optimal Transport for Speech Alignment), a new framework based on cross-lingual parallel speech pairs and Optimal Transport (OT), designed to bridge high- and low-resource translation gaps. First, we introduce a Bias Compensation module to coarsely align initial speech representations across languages. Second, we impose token-level OT constraints on a Q-Former using parallel speech pairs to establish fine-grained consistency of representations. Then, we apply a layer scheduling strategy to focus OT constraints on the most semantically beneficial layers. Experiments on the FLEURS dataset show that our method achieves SOTA performance, with +0.93 BLEU on average over five common languages and +5.05 BLEU on zero-shot languages, using only 10 hours of parallel speech per source language.</p>
<h3 id="28-mmjee-eval-a-bilingual-multimodal-benchmark-for-evaluating-scientific-reasoning-in-vision-language-models">[28] <a href="https://arxiv.org/abs/2511.09339">mmJEE-Eval: A Bilingual Multimodal Benchmark for Evaluating Scientific Reasoning in Vision-Language Models</a></h3>
<p><em>Arka Mukherjee, Shreya Ghosh</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†mmJEE-Evalï¼Œä¸€ä¸ªå¤šæ¨¡æ€åŒè¯­åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„ç§‘å­¦æ¨ç†èƒ½åŠ›ï¼Œæ­ç¤ºäº†å‰æ²¿é—­æºæ¨¡å‹ä¸å¼€æºæ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ˜¾è‘—æ€§èƒ½å·®è·ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†è¿™äº›ç»“æœæ— æ³•æœ‰æ•ˆåŒºåˆ†çœŸæ­£çš„ç§‘å­¦æ¨ç†èƒ½åŠ›ä¸æ¨¡å¼åŒ¹é…ï¼Œå­˜åœ¨è¯„ä¼°ä¸è¶³çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†åŒ…å«1,460ä¸ªæ¥è‡ªå°åº¦JEE Advancedè€ƒè¯•é—®é¢˜çš„å¤šæ¨¡æ€åŒè¯­åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ç‰©ç†ã€åŒ–å­¦å’Œæ•°å­¦é¢†åŸŸï¼Œå¹¶ç³»ç»Ÿè¯„ä¼°äº†17ä¸ªæœ€å…ˆè¿›æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> å‰æ²¿é—­æºæ¨¡å‹åœ¨2025å¹´é¢˜ç›®ä¸Šè¾¾åˆ°77-84%å‡†ç¡®ç‡ï¼Œè€Œå¼€æºæ¨¡å‹ä»…è¾¾åˆ°37-45%ï¼Œä¸”åœ¨å…ƒè®¤çŸ¥æ¨ç†è´Ÿè½½å¢åŠ æ—¶æ€§èƒ½å®Œå…¨å´©æºƒã€‚</p>
<p><strong>Conclusion:</strong> è¯¥åŸºå‡†æµ‹è¯•èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†ä¸åŒè®­ç»ƒå’Œæ¨ç†æ–¹æ³•çš„è´¨é‡ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤æ‚ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦è¯„ä¼°å·¥å…·ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>Contemporary vision-language models (VLMs) perform well on existing multimodal reasoning benchmarks (78-85\% accuracy on MMMU, MathVista). Yet, these results fail to sufficiently distinguish true scientific reasoning articulation capabilities from pattern-matching. To address this gap, we introduce \textbf{mmJEE-Eval}, a multimodal bilingual (English and Hindi) benchmark comprising 1,460 questions from India's JEE Advanced examination (2019-2025) spanning pre-college Physics, Chemistry, and Mathematics domains. Our evaluation of 17 state-of-the-art models reveals that while frontier VLMs (GPT-5, Gemini 2.5 Pro/Flash) achieve 77-84\% accuracy on held-out 2025 questions, open-source models plateau at 37-45\% despite scaling to 400B parameters, a significant difference not observed on existing benchmarks. While closed frontiers from Google and OpenAI show high problem-solving accuracies (up to 100\% pass@3 scores), they fully collapse when the reasoning load is increased meta-cognitively (GPT-5 fixes just 5.2\% errors). Systematic ablations show mmJEE-Eval's difficulty stems from complexity and reasoning depth rather than memorization. Effectively, our benchmark segregates superior training and reasoning methodologies where alternatives fail. We publicly release our code and data: https://mmjee-eval.github.io</p>
<h3 id="29-multimodal-large-language-models-for-low-resource-languages-a-case-study-for-basque">[29] <a href="https://arxiv.org/abs/2511.09396">Multimodal Large Language Models for Low-Resource Languages: A Case Study for Basque</a></h3>
<p><em>Lukas Arana, Julen Etxaniz, Ander Salaberria, Gorka Azkune</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶å¼€å‘äº†é’ˆå¯¹ä½èµ„æºå·´æ–¯å…‹è¯­çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ„å»ºä¸“ç”¨è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ï¼Œè¯æ˜äº†ä»…éœ€20%å·´æ–¯å…‹å¤šæ¨¡æ€æ•°æ®å³å¯è·å¾—è‰¯å¥½æ€§èƒ½ï¼Œä¸”æ— éœ€å·´æ–¯å…‹æŒ‡ä»¤è°ƒä¼˜çš„éª¨å¹²æ¨¡å‹ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸»æµè¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å¼€æºç¤¾åŒºåœ¨ä½èµ„æºè¯­è¨€æ–¹é¢ä»æ— æ³•è¾¾åˆ°å•†ä¸šæ¨¡å‹çš„æ€§èƒ½æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯å·´æ–¯å…‹è¯­è¿™ç±»èµ„æºç¨€ç¼ºè¯­è¨€ç¼ºä¹é«˜è´¨é‡çš„å¤šæ¨¡æ€æ¨¡å‹æ”¯æŒã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨Llama-3.1-Instructå’Œå·´æ–¯å…‹è¯­é€‚é…å˜ä½“Latxaä½œä¸ºéª¨å¹²æ¨¡å‹ï¼Œæ„å»ºäº†ä¸“ç”¨çš„å›¾åƒ-æ–‡æœ¬è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ï¼Œæ¢ç´¢äº†ä¸åŒæ•°æ®æ··åˆæ¯”ä¾‹çš„è®­ç»ƒç­–ç•¥ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ä»…éœ€çº¦20%çš„å·´æ–¯å…‹å¤šæ¨¡æ€æ•°æ®å³å¯åœ¨å·´æ–¯å…‹åŸºå‡†æµ‹è¯•ä¸­è·å¾—ç¨³å®šç»“æœï¼Œä¸”æ„å¤–å‘ç°æ— éœ€å·´æ–¯å…‹æŒ‡ä»¤è°ƒä¼˜çš„éª¨å¹²æ¨¡å‹ä¹Ÿèƒ½æ„å»ºå¼ºå¤§çš„å·´æ–¯å…‹å¤šæ¨¡æ€æ¨¡å‹ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¼€å‘å…¶ä»–ä½èµ„æºè¯­è¨€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œé€šè¿‡å…¬å¼€é‡Šæ”¾èµ„æºä¿ƒè¿›äº†å¼€æºç¤¾åŒºåœ¨ä½èµ„æºè¯­è¨€å¤šæ¨¡æ€å»ºæ¨¡æ–¹é¢çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Current Multimodal Large Language Models exhibit very strong performance for several demanding tasks. While commercial MLLMs deliver acceptable performance in low-resource languages, comparable results remain unattained within the open science community. In this paper, we aim to develop a strong MLLM for a low-resource language, namely Basque. For that purpose, we develop our own training and evaluation image-text datasets. Using two different Large Language Models as backbones, the Llama-3.1-Instruct model and a Basque-adapted variant called Latxa, we explore several data mixtures for training. We show that: i) low ratios of Basque multimodal data (around 20%) are already enough to obtain solid results on Basque benchmarks, and ii) contrary to expected, a Basque instructed backbone LLM is not required to obtain a strong MLLM in Basque. Our results pave the way to develop MLLMs for other low-resource languages by openly releasing our resources.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="30-lumine-an-open-recipe-for-building-generalist-agents-in-3d-open-worlds">[30] <a href="https://arxiv.org/abs/2511.08892">Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds</a></h3>
<p><em>Weihao Tan, Xiangyang Li, Yunhao Fang, Heyuan Yao, Shi Yan, Hao Luo, Tenglong Ao, Huihui Li, Hongbin Ren, Bairen Yi, Yujia Qin, Bo An, Libin Liu, Guang Shi</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>Lumineæ˜¯é¦–ä¸ªèƒ½å¤Ÿå®Œæˆæ•°å°æ—¶å¤æ‚ä»»åŠ¡çš„é€šç”¨æ™ºèƒ½ä½“é…æ–¹ï¼Œåœ¨3Då¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å®ç°å®æ—¶äº¤äº’ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„è§†è§‰è¯­è¨€æ¨¡å‹ç»Ÿä¸€æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ï¼Œå¹¶åœ¨å¤šä¸ªæ¸¸æˆä¸­å±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³é€šç”¨æ™ºèƒ½ä½“åœ¨å¤æ‚3Då¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­æ‰§è¡Œé•¿æ—¶é—´ä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨å®æ—¶äº¤äº’ä¸­ç»Ÿä¸€æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ï¼Œå¹¶ä¸”ç¼ºä¹è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> Lumineé‡‡ç”¨ç±»äººäº¤äº’èŒƒå¼ï¼ŒåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹å®ç°ç«¯åˆ°ç«¯å¤„ç†ï¼Œä»¥5Hzå¤„ç†åŸå§‹åƒç´ å¹¶ç”Ÿæˆ30Hzçš„é”®ç›˜é¼ æ ‡åŠ¨ä½œï¼Œä»…åœ¨å¿…è¦æ—¶è‡ªé€‚åº”è°ƒç”¨æ¨ç†æ¨¡å—ï¼Œåœ¨ã€ŠåŸç¥ã€‹ä¸­è¿›è¡Œè®­ç»ƒã€‚</p>
<p><strong>Result:</strong> LumineæˆåŠŸå®Œæˆäº†ã€ŠåŸç¥ã€‹ä¸­äº”å°æ—¶è’™å¾·ä¸»çº¿å‰§æƒ…ï¼Œæ•ˆç‡è¾¾åˆ°äººç±»æ°´å¹³ï¼Œå¹¶åœ¨ã€Šé¸£æ½®ã€‹å’Œã€Šå´©åï¼šæ˜Ÿç©¹é“é“ã€‹ä¸­å®ç°é›¶æ ·æœ¬æ³›åŒ–ï¼Œåˆ†åˆ«å®Œæˆ100åˆ†é’Ÿä»»åŠ¡å’Œäº”å°æ—¶ç¬¬ä¸€ç« å†…å®¹ã€‚</p>
<p><strong>Conclusion:</strong> Lumineå±•ç¤ºäº†åœ¨å¼€æ”¾ç¯å¢ƒä¸­é€šç”¨æ™ºèƒ½ä½“çš„å¯è¡Œæ€§ï¼Œå…¶è·¨æ¸¸æˆæ³›åŒ–èƒ½åŠ›è¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›é€‚ç”¨æ€§ï¼Œä¸ºå¼€å‘çœŸæ­£é€šç”¨çš„å¼€æ”¾ä¸–ç•Œæ™ºèƒ½ä½“è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.</p>
<h3 id="31-history-aware-reasoning-for-gui-agents">[31] <a href="https://arxiv.org/abs/2511.09127">History-Aware Reasoning for GUI Agents</a></h3>
<p><em>Ziwei Wang, Leyang Yang, Xiaoxuan Tang, Sheng Zhou, Dajun Chen, Wei Jiang, Yong Li</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å†å²æ„ŸçŸ¥æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡åæ€å­¦ä¹ æœºåˆ¶å¢å¼ºGUIä»£ç†çš„çŸ­æœŸè®°å¿†èƒ½åŠ›ï¼Œå°†æ¨ç†æ¨¡å¼ä»å†å²æ— å…³è½¬å˜ä¸ºå†å²æ„ŸçŸ¥ï¼Œæ˜¾è‘—æå‡äº†é•¿æ—¶ç¨‹GUIä»»åŠ¡çš„æ‰§è¡Œæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸç”ŸGUIä»£ç†åœ¨æ˜¾å¼æ¨ç†ä¸­å­˜åœ¨çŸ­æœŸè®°å¿†è–„å¼±çš„é—®é¢˜ï¼Œå°†é“¾å¼äº¤äº’ç†è§£ä¸ºç¦»æ•£çš„å±å¹•ç†è§£ï¼Œç¼ºä¹å¯¹å†å²äº¤äº’çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œè¿™ç§å†å²æ— å…³çš„æ¨ç†æ¨¡å¼ä¸¥é‡å½±å“äº†GUIè‡ªåŠ¨åŒ–çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†å†å²æ„ŸçŸ¥æ¨ç†æ¡†æ¶ï¼Œä¸»è¦åŒ…æ‹¬æ„å»ºåæ€å­¦ä¹ åœºæ™¯ã€åˆæˆå®šåˆ¶åŒ–ä¿®æ­£æŒ‡å—ä»¥åŠè®¾è®¡æ··åˆå¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œé€šè¿‡è®©ä»£ç†åæ€è‡ªèº«é”™è¯¯å¹¶ä»é”™è¯¯ä¸­è·å–æƒ…æ™¯æ¨ç†çŸ¥è¯†æ¥å¢å¼ºé•¿æœŸäº¤äº’ä¸­çš„çŸ­æœŸè®°å¿†ã€‚</p>
<p><strong>Result:</strong> åŸºäºè¯¥æ¡†æ¶å¼€å‘çš„HAR-GUI-3Bæ¨¡å‹åœ¨å¤šä¸ªGUIç›¸å…³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†GUIä»£ç†çš„çŸ­æœŸè®°å¿†ç¨³å®šæ€§å’Œå±å¹•ç»†èŠ‚æ„ŸçŸ¥å¯é æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡å†å²æ„ŸçŸ¥æ¨ç†æ¡†æ¶æˆåŠŸè§£å†³äº†GUIä»£ç†çš„çŸ­æœŸè®°å¿†ç“¶é¢ˆï¼Œä¸ºé•¿æ—¶ç¨‹GUIä»»åŠ¡è‡ªåŠ¨åŒ–æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†åæ€å­¦ä¹ åœ¨å¢å¼ºæ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Advances in Multimodal Large Language Models have significantly enhanced Graphical User Interface (GUI) automation. Equipping GUI agents with reliable episodic reasoning capabilities is essential for bridging the gap between users' concise task descriptions and the complexities of real-world execution. Current methods integrate Reinforcement Learning (RL) with System-2 Chain-of-Thought, yielding notable gains in reasoning enhancement. For long-horizon GUI tasks, historical interactions connect each screen to the goal-oriented episode chain, and effectively leveraging these clues is crucial for the current decision. However, existing native GUI agents exhibit weak short-term memory in their explicit reasoning, interpreting the chained interactions as discrete screen understanding, i.e., unawareness of the historical interactions within the episode. This history-agnostic reasoning challenges their performance in GUI automation. To alleviate this weakness, we propose a History-Aware Reasoning (HAR) framework, which encourages an agent to reflect on its own errors and acquire episodic reasoning knowledge from them via tailored strategies that enhance short-term memory in long-horizon interaction. The framework mainly comprises constructing a reflective learning scenario, synthesizing tailored correction guidelines, and designing a hybrid RL reward function. Using the HAR framework, we develop a native end-to-end model, HAR-GUI-3B, which alters the inherent reasoning mode from history-agnostic to history-aware, equipping the GUI agent with stable short-term memory and reliable perception of screen details. Comprehensive evaluations across a range of GUI-related benchmarks demonstrate the effectiveness and generalization of our method.</p>
<h3 id="32-crochetbench-can-vision-language-models-move-from-describing-to-doing-in-crochet-domain">[32] <a href="https://arxiv.org/abs/2511.09483">CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?</a></h3>
<p><em>Peiyu Li, Xiaobao Huang, Nitesh V. Chawla</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CrochetBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é’©é’ˆç¼–ç»‡é¢†åŸŸæ‰§è¡Œç»†ç²’åº¦ã€ä½å±‚æ¬¡ç¨‹åºæ¨ç†çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†å°†è¯„ä¼°é‡ç‚¹ä»æè¿°è½¬å‘æ‰§è¡Œï¼Œè¦æ±‚æ¨¡å‹è¯†åˆ«é’ˆæ³•ã€é€‰æ‹©ç»“æ„é€‚å½“çš„æŒ‡ä»¤å¹¶ç”Ÿæˆå¯ç¼–è¯‘çš„é’©é’ˆç¨‹åºã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨é«˜å±‚æ¬¡æè¿°æˆ–è§†è§‰é—®ç­”ï¼Œè€Œç¼ºä¹å¯¹å¤šæ¨¡æ€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåˆ›é€ æ€§é¢†åŸŸä¸­ç¨‹åºæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ã€‚CrochetBenchæ—¨åœ¨è§£å†³è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œç‰¹åˆ«å…³æ³¨ä»è¡¨é¢ç†è§£åˆ°å¯æ‰§è¡Œç²¾åº¦çš„è½¬å˜ï¼Œå¡«è¡¥äº†å¤šæ¨¡æ€æ¨¡å‹åœ¨ä½å±‚æ¬¡ç¨‹åºæ¨ç†èƒ½åŠ›è¯„ä¼°æ–¹é¢çš„ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨CrochetPARADEé¢†åŸŸç‰¹å®šè¯­è¨€ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œæ”¯æŒç»“æ„éªŒè¯å’Œé€šè¿‡æ‰§è¡Œçš„åŠŸèƒ½è¯„ä¼°ã€‚åŸºå‡†æ¶µç›–é’ˆæ³•åˆ†ç±»ã€æŒ‡ä»¤æ¥åœ°ä»¥åŠè‡ªç„¶è¯­è¨€å’Œå›¾åƒåˆ°DSLçš„ç¿»è¯‘ä»»åŠ¡ï¼Œé€šè¿‡å¯æ‰§è¡Œæ€§éªŒè¯æ¥è¯„ä¼°æ¨¡å‹çš„ç¨‹åºæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ï¼Œéšç€è¯„ä¼°ä»è¡¨é¢ç›¸ä¼¼æ€§è½¬å‘å¯æ‰§è¡Œæ­£ç¡®æ€§ï¼Œæ¨¡å‹æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚è¿™æš´éœ²äº†å¤šæ¨¡æ€æ¨¡å‹åœ¨é•¿è·ç¦»ç¬¦å·æ¨ç†å’Œ3Dæ„ŸçŸ¥ç¨‹åºåˆæˆæ–¹é¢çš„å±€é™æ€§ï¼Œæ˜¾ç¤ºå‡ºè¡¨é¢ç†è§£ä¸å¯æ‰§è¡Œç²¾åº¦ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</p>
<p><strong>Conclusion:</strong> CrochetBenchä¸ºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„ç¨‹åºèƒ½åŠ›æä¾›äº†æ–°è§†è§’ï¼Œå¼ºè°ƒäº†åœ¨çœŸå®ä¸–ç•Œåˆ›é€ æ€§é¢†åŸŸä¸­è¡¨é¢ç†è§£ä¸å¯æ‰§è¡Œç²¾åº¦ä¹‹é—´çš„å…³é”®å·®è·ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤æ‚ç¨‹åºæ¨ç†ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥æ”¹è¿›å¤šæ¨¡æ€æ¨¡å‹çš„ç¨‹åºåˆæˆèƒ½åŠ›æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>We present CrochetBench, a benchmark for evaluating the ability of multimodal large language models to perform fine-grained, low-level procedural reasoning in the domain of crochet. Unlike prior benchmarks that focus on high-level description or visual question answering, CrochetBench shifts the emphasis from describing to doing: models are required to recognize stitches, select structurally appropriate instructions, and generate compilable crochet procedures. We adopt the CrochetPARADE DSL as our intermediate representation, enabling structural validation and functional evaluation via execution. The benchmark covers tasks including stitch classification, instruction grounding, and both natural language and image-to-DSL translation. Across all tasks, performance sharply declines as the evaluation shifts from surface-level similarity to executable correctness, exposing limitations in long-range symbolic reasoning and 3D-aware procedural synthesis. CrochetBench offers a new lens for assessing procedural competence in multimodal models and highlights the gap between surface-level understanding and executable precision in real-world creative domains. Code is available at https://github.com/Peiyu-Georgia-Li/crochetBench.</p>
  </article>
</body>
</html>
