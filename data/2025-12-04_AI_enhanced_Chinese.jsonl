{"id": "2512.03494", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03494", "abs": "https://arxiv.org/abs/2512.03494", "authors": ["Di Xiu", "Hongyin Tang", "Bolin Rong", "Lizhi Yan", "Jingang Wang", "Yifan Lu", "Xunliang Cai"], "title": "A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention", "comment": null, "summary": "Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.", "AI": {"tldr": "\u8be5\u7814\u7a76\u62a5\u544a\u5bf9Top-k\u6ce8\u610f\u529b\u673a\u5236\u5728\u89e3\u7801\u548c\u8bad\u7ec3\u9636\u6bb5\u7684\u6709\u6548\u6027\u4e0e\u7406\u8bba\u673a\u5236\u8fdb\u884c\u4e86\u521d\u6b65\u63a2\u7d22\uff0c\u9a8c\u8bc1\u4e86\u7cbe\u786eTop-k\u89e3\u7801\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u63d0\u51fa\u4e86\u8bad\u7ec3-\u63a8\u7406\u4e00\u81f4\u7684Top-k\u6ce8\u610f\u529b\u7b56\u7565\u4ee5\u8fdb\u4e00\u6b65\u91ca\u653e\u6a21\u578b\u6f5c\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u5df2\u6210\u4e3a\u963b\u788d\u667a\u80fd\u4f53\u548c\u591a\u6a21\u6001\u5e94\u7528\u53d1\u5c55\u7684\u5173\u952e\u74f6\u9888\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u66f4\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7cbe\u786eTop-k\u89e3\u7801\u673a\u5236\uff0c\u5728\u89e3\u7801\u9636\u6bb5\u4ec5\u4fdd\u7559\u4e0e\u67e5\u8be2\u76f8\u4f3c\u5ea6\u6700\u9ad8\u7684\u5173\u952e\u952e\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5e76\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e86\u539f\u751fTop-k\u6ce8\u610f\u529b\u8bad\u7ec3\u7b56\u7565\uff0c\u786e\u4fdd\u8bad\u7ec3\u4e0e\u63a8\u7406\u9636\u6bb5\u64cd\u4f5c\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u7814\u7a76\u4e86\u8fd1\u4f3cTop-k\u7b97\u6cd5\u7cbe\u5ea6\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7cbe\u786eTop-k\u89e3\u7801\u5728HELMET\u548cLongBench v2\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u5168\u6ce8\u610f\u529b\u6027\u80fd\uff0c\u8bad\u7ec3-\u63a8\u7406\u4e00\u81f4\u7684Top-k\u6ce8\u610f\u529b\u7b56\u7565\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff0c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0e\u8fd1\u4f3c\u7b97\u6cd5\u4fdd\u771f\u5ea6\u5448\u6b63\u76f8\u5173\uff0c\u4e14Top-k\u6ce8\u610f\u529bSFT\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u660e\u663e\u7684\u71b5\u51cf\u73b0\u8c61\u3002", "conclusion": "\u7814\u7a76\u4ece\u71b5\u7684\u89d2\u5ea6\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u9a8c\u8bc1\u4e86\u4f4e\u71b5\u72b6\u6001\u66f4\u9002\u5e94Top-k\u89e3\u7801\u7684\u5047\u8bbe\uff0c\u4e3a\u964d\u4f4eLLM\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8bad\u7ec3\u4e0e\u63a8\u7406\u673a\u5236\u4e00\u81f4\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u4f18\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.03704", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03704", "abs": "https://arxiv.org/abs/2512.03704", "authors": ["Yijun Liao"], "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue", "comment": "22 pages, 2 figures, 13 tables. Code available at https://github.com/lyj20071013/DZ-TDPO", "summary": "Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a \"Capacity-Stability Trade-off\": while smaller models incur an \"alignment tax\" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDZ-TDPO\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u7834\u574f\u6027\u5bf9\u9f50\u65b9\u6cd5\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u72b6\u6001\u60ef\u6027\u95ee\u9898\uff0c\u7ed3\u5408\u51b2\u7a81\u611f\u77e5\u52a8\u6001KL\u7ea6\u675f\u548c\u53ef\u5b66\u4e60\u65f6\u5e8f\u6ce8\u610f\u529b\u504f\u7f6e\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u5353\u8d8a\u7684\u5bf9\u8bdd\u6027\u80fd\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u7cfb\u7edf\u5b58\u5728\u72b6\u6001\u60ef\u6027\u95ee\u9898\uff0c\u5373\u9759\u6001\u7ea6\u675f\u963b\u788d\u6a21\u578b\u89e3\u51b3\u7528\u6237\u610f\u56fe\u6f14\u5316\u4e0e\u5386\u53f2\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u5728\u52a8\u6001\u4ea4\u4e92\u4e2d\u7684\u9002\u5e94\u6027\u548c\u54cd\u5e94\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faDZ-TDPO\u975e\u7834\u574f\u6027\u5bf9\u9f50\u6846\u67b6\uff0c\u7ed3\u5408\u51b2\u7a81\u611f\u77e5\u52a8\u6001KL\u7ea6\u675f\u548c\u53ef\u5b66\u4e60\u65f6\u5e8f\u6ce8\u610f\u529b\u504f\u7f6e\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u6ce8\u610f\u529b\u8c03\u8282\u800c\u975e\u7834\u574f\u6027\u6743\u91cd\u66f4\u65b0\u6765\u7f13\u89e3\u72b6\u6001\u60ef\u6027\u95ee\u9898\u3002", "result": "\u5728Multi-Session Chat\u6570\u636e\u96c6\u4e0a\uff0cDZ-TDPO\u5b9e\u73b0\u4e8686.2%\u7684\u80dc\u7387\uff08Phi-3.5\u6a21\u578b\uff09\uff0c\u800cQwen2.5-7B\u6a21\u578b\u8fbe\u523099.4%\u7684\u8fd1\u4e4e\u5b8c\u7f8e\u5bf9\u9f50\u4e14\u56f0\u60d1\u5ea6\u5f00\u9500\u53ef\u5ffd\u7565\uff0c\u540c\u65f6\u4fdd\u6301MMLU\u7b49\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\"\u5bb9\u91cf-\u7a33\u5b9a\u6027\u6743\u8861\"\u73b0\u8c61\uff1a\u5c0f\u6a21\u578b\u9700\u4ed8\u51fa\"\u5bf9\u9f50\u7a0e\"\u6765\u514b\u670d\u5386\u53f2\u60ef\u6027\uff0c\u800c\u5927\u6a21\u578b\u53ef\u901a\u8fc7\u7cbe\u786e\u6ce8\u610f\u529b\u8c03\u8282\u5b9e\u73b0\u5b8c\u7f8e\u5bf9\u9f50\uff0c\u8fd9\u4e3a\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2512.03818", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03818", "abs": "https://arxiv.org/abs/2512.03818", "authors": ["Kylie L. Anglin", "Stephanie Milan", "Brittney Hernandez", "Claudia Ventura"], "title": "Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology", "comment": "22 pages, 2 figures", "summary": "Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5b66\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7ed3\u5408\u4eba\u5de5\u6307\u5bfc\u4e0e\u81ea\u52a8\u751f\u6210\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u4e0e\u4e13\u5bb6\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8f93\u51fa\u9ad8\u5ea6\u4f9d\u8d56\u63d0\u793a\u7684\u63aa\u8f9e\uff0c\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u5173\u6ce8\u5fc3\u7406\u5b66\u7b49\u4e13\u4e1a\u9886\u57df\uff0c\u8fd9\u4e9b\u9886\u57df\u7684\u6784\u5ff5\u5177\u6709\u7cbe\u786e\u7684\u7406\u8bba\u5b9a\u4e49\u4e14\u53ef\u80fd\u672a\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u5145\u5206\u4f53\u73b0\uff0c\u5bfc\u81f4\u6a21\u578b\u4e0e\u4e13\u5bb6\u5224\u65ad\u5b58\u5728\u504f\u5dee\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u8bc1\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e94\u79cd\u63d0\u793a\u7b56\u7565\uff1a\u57fa\u4e8e\u4ee3\u7801\u672c\u7684\u7ecf\u9a8c\u63d0\u793a\u9009\u62e9\u3001\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u3001\u89d2\u8272\u63d0\u793a\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u89e3\u91ca\u6027\u63d0\u793a\uff0c\u5e76\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5206\u7c7b\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u91cd\u70b9\u5173\u6ce8\u6784\u5ff5\u5b9a\u4e49\u3001\u4efb\u52a1\u6846\u67b6\u548c\u793a\u4f8b\u9009\u62e9\u7b49\u5173\u952e\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u89d2\u8272\u3001\u601d\u7ef4\u94fe\u548c\u89e3\u91ca\u6027\u63d0\u793a\u65e0\u6cd5\u5b8c\u5168\u5f25\u8865\u4e0d\u826f\u63aa\u8f9e\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\uff0c\u6700\u5177\u5f71\u54cd\u529b\u7684\u63d0\u793a\u7279\u5f81\u662f\u6784\u5ff5\u5b9a\u4e49\u3001\u4efb\u52a1\u6846\u67b6\u548c\u793a\u4f8b\u9009\u62e9\uff0c\u5728\u4e09\u4e2a\u6784\u5ff5\u548c\u4e24\u4e2a\u6a21\u578b\u4e0a\uff0c\u7ed3\u5408\u4ee3\u7801\u672c\u6307\u5bfc\u7684\u7ecf\u9a8c\u63d0\u793a\u9009\u62e9\u4e0e\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u7684\u5c11\u6837\u672c\u63d0\u793a\u4ea7\u751f\u4e86\u4e0e\u4e13\u5bb6\u5224\u65ad\u6700\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u5efa\u8bae\u7814\u7a76\u4eba\u5458\u5e94\u5c3d\u53ef\u80fd\u751f\u6210\u548c\u8bc4\u4f30\u591a\u79cd\u63d0\u793a\u53d8\u4f53\uff0c\u5305\u62ec\u4eba\u5de5\u8bbe\u8ba1\u548c\u81ea\u52a8\u751f\u6210\u7684\uff0c\u5e76\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u5b9e\u8bc1\u6027\u80fd\u9009\u62e9\u63d0\u793a\u548c\u793a\u4f8b\uff0c\u5728\u4fdd\u7559\u96c6\u4e0a\u9a8c\u8bc1\u6700\u7ec8\u65b9\u6cd5\uff0c\u8fd9\u4e3a\u9700\u8981\u4e0e\u4e13\u5bb6\u5224\u65ad\u4fdd\u6301\u4e00\u81f4\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u7cfb\u7edf\u4e14\u7406\u8bba\u9a71\u52a8\u7684LLM\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2512.03838", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03838", "abs": "https://arxiv.org/abs/2512.03838", "authors": ["Michael Staniek", "Artem Sokolov", "Stefan Riezler"], "title": "Training and Evaluation of Guideline-Based Medical Reasoning in LLMs", "comment": null, "summary": "Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5fae\u8c03LLMs\u9075\u5faa\u533b\u5b66\u5171\u8bc6\u6307\u5357\u8fdb\u884c\u9010\u6b65\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u533b\u7597AI\u4e2d\u89e3\u91ca\u53ef\u4fe1\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u7279\u5b9a\u533b\u7597\u9886\u57df\uff08\u5982\u8113\u6bd2\u75c7-3\u5b9a\u4e49\uff09\u4e0a\u5fae\u8c03\u7684\u5c0f\u6a21\u578b\u5728\u63a8\u7406\u6b63\u786e\u6027\u548c\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u4f7f\u7528\u663e\u5f0f\u5b9a\u4e49\u63d0\u793a\u7684\u5927\u578bLLMs\u3002", "motivation": "\u533b\u7597\u65e9\u671f\u9884\u6d4b\u7684\u673a\u5668\u5b66\u4e60\u7814\u7a76\u867d\u7136\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u6027\u80fd\uff0c\u4f46\u8fc7\u5ea6\u5173\u6ce8\u9884\u6d4b\u51c6\u786e\u6027\u5bfc\u81f4\u5ffd\u89c6\u4e86\u83b7\u5f97\u533b\u7597\u4ece\u4e1a\u8005\u4fe1\u4efb\u6240\u9700\u7684\u53ef\u4fe1\u89e3\u91ca\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u6574\u5408\u533b\u5b66\u4e2d\u666e\u904d\u5b58\u5728\u7684\u5171\u8bc6\u6307\u5357\uff0c\u8fd9\u4e9b\u6307\u5357\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u63a8\u7406\u6b65\u9aa4\u548c\u4f8b\u5916\u60c5\u51b5\u5904\u7406\uff0c\u5bf9\u4e8e\u786e\u4fdd\u6a21\u578b\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u5c06\u533b\u5b66\u5171\u8bc6\u6307\u5357\u5b9e\u4f8b\u5316\u4e3a\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u8bed\u8a00\u5316\u63a8\u7406\u89c4\u5219\uff0c\u5e76\u4ee5\u6b64\u4f5c\u4e3a\u5fae\u8c03\u6570\u636e\u6765\u8bad\u7ec3LLMs\u5b66\u4e60\u5171\u8bc6\u89c4\u5219\u53ca\u5176\u4f8b\u5916\u60c5\u51b5\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u4e24\u6b65\u8bc4\u4f30\u6846\u67b6\uff1a\u63a8\u5bfc\u6b63\u786e\u6027\u8bc4\u4f30\u6a21\u578b\u4ece\u524d\u63d0\u6b63\u786e\u63a8\u5bfc\u7ed3\u8bba\u7684\u80fd\u529b\uff0c\u4ef7\u503c\u6b63\u786e\u6027\u8bc4\u4f30\u9884\u6d4b\u503c\u4e0e\u5b9e\u9645\u6d4b\u91cf\u503c\u7684\u4e00\u81f4\u6027\u3002\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6311\u6218\uff0c\u7814\u7a76\u8fdb\u4e00\u6b65\u63d0\u51fa\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u8f93\u51fa\u8868\u793a\u4e0eLLM\u96c6\u6210\u3002", "result": "\u5728\u8113\u6bd2\u75c7-3\u5171\u8bc6\u5b9a\u4e49\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u7279\u5b9a\u533b\u7597\u9886\u57df\u89c4\u5219\u5b9e\u4f8b\u5fae\u8c03\u7684\u5c0f\u578b\u6a21\u578b\u5728\u672a\u89c1\u60a3\u8005\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u63a8\u5bfc\u6b63\u786e\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u4f7f\u7528\u663e\u5f0f\u5b9a\u4e49\u8fdb\u884c\u5355\u6837\u672c\u5b66\u4e60\u7684\u5927\u578bLLMs\u4ee5\u53ca\u8bad\u7ec3\u65f6\u5305\u542b\u5171\u8bc6\u5b9a\u4e49\u7684\u533b\u5b66\u6587\u672c\u6a21\u578b\u3002\u591a\u6a21\u6001\u96c6\u6210\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6539\u5584\u4e86\u7a00\u758f\u3001\u4e0d\u89c4\u5219\u91c7\u6837\u4e34\u5e8a\u53d8\u91cf\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5411\u672a\u6765\u6cdb\u5316\u7684\u6b63\u4ea4\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u65e9\u671f\u9884\u6d4b\u7684\u4e3b\u8981\u74f6\u9888\u5e76\u975e\u5206\u5e03\u5916\u6cdb\u5316\uff0c\u800c\u662f\u5411\u672a\u6765\u65f6\u95f4\u6cdb\u5316\u7684\u6b63\u4ea4\u95ee\u9898\u3002\u901a\u8fc7\u8bed\u8a00\u5316\u89c4\u5219\u5b9e\u4f8b\u8fdb\u884c\u5fae\u8c03\u80fd\u591f\u6709\u6548\u6559\u6388LLMs\u9075\u5faa\u533b\u5b66\u5171\u8bc6\u6307\u5357\uff0c\u5b9e\u73b0\u5fe0\u5b9e\u4e14\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u591a\u6a21\u6001\u96c6\u6210\u65b9\u6cd5\u4e3a\u89e3\u51b3\u4e34\u5e8a\u53d8\u91cf\u9884\u6d4b\u7684\u65f6\u95f4\u5e8f\u5217\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u533b\u7597AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\u3002"}}
{"id": "2512.03233", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03233", "abs": "https://arxiv.org/abs/2512.03233", "authors": ["Richard F\u00fczess\u00e9ry", "Kaziwa Saleh", "S\u00e1ndor Sz\u00e9n\u00e1si", "Zolt\u00e1n V\u00e1mossy"], "title": "Object Counting with GPT-4o and GPT-5: A Comparative Study", "comment": "5 pages, 3 figures", "summary": "Zero-shot object counting attempts to estimate the number of object instances belonging to novel categories that the vision model performing the counting has never encountered during training. Existing methods typically require large amount of annotated data and often require visual exemplars to guide the counting process. However, large language models (LLMs) are powerful tools with remarkable reasoning and data understanding abilities, which suggest the possibility of utilizing them for counting tasks without any supervision. In this work we aim to leverage the visual capabilities of two multi-modal LLMs, GPT-4o and GPT-5, to perform object counting in a zero-shot manner using only textual prompts. We evaluate both models on the FSC-147 and CARPK datasets and provide a comparative analysis. Our findings show that the models achieve performance comparable to the state-of-the-art zero-shot approaches on FSC-147, in some cases, even surpass them.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\u548cGPT-5\uff09\u8fdb\u884c\u96f6\u6837\u672c\u7269\u4f53\u8ba1\u6570\uff0c\u4ec5\u901a\u8fc7\u6587\u672c\u63d0\u793a\u5b9e\u73b0\u65e0\u9700\u76d1\u7763\u7684\u8ba1\u6570\u4efb\u52a1\uff0c\u5728FSC-147\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u96f6\u6837\u672c\u7269\u4f53\u8ba1\u6570\u65e8\u5728\u4f30\u8ba1\u89c6\u89c9\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ece\u672a\u89c1\u8fc7\u7684\u65b0\u7c7b\u522b\u7269\u4f53\u5b9e\u4f8b\u6570\u91cf\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u89c6\u89c9\u793a\u4f8b\u5f15\u5bfc\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u63a8\u7406\u548c\u6570\u636e\u7406\u89e3\u80fd\u529b\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u5229\u7528\u591a\u6a21\u6001LLMs\u5b9e\u73b0\u5b8c\u5168\u65e0\u9700\u76d1\u7763\u7684\u96f6\u6837\u672c\u8ba1\u6570\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u5229\u7528\u4e24\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\u548cGPT-5\uff09\u7684\u89c6\u89c9\u80fd\u529b\uff0c\u4ec5\u901a\u8fc7\u6587\u672c\u63d0\u793a\u8fdb\u884c\u96f6\u6837\u672c\u7269\u4f53\u8ba1\u6570\uff0c\u65e0\u9700\u4efb\u4f55\u76d1\u7763\u8bad\u7ec3\u6216\u89c6\u89c9\u793a\u4f8b\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u57fa\u4e8e\u6587\u672c\u5f15\u5bfc\u7684\u8ba1\u6570\u65b9\u6cd5\u3002", "result": "\u5728FSC-147\u548cCARPK\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u4e24\u79cd\u6a21\u578b\u5728FSC-147\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001LLMs\u5728\u96f6\u6837\u672c\u8ba1\u6570\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u6267\u884c\u96f6\u6837\u672c\u7269\u4f53\u8ba1\u6570\u4efb\u52a1\uff0c\u4ec5\u901a\u8fc7\u6587\u672c\u63d0\u793a\u5373\u53ef\u5b9e\u73b0\u4e0e\u4e13\u95e8\u8bbe\u8ba1\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e3a\u65e0\u9700\u76d1\u7763\u7684\u89c6\u89c9\u8ba1\u6570\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86LLMs\u5728\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2512.03318", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03318", "abs": "https://arxiv.org/abs/2512.03318", "authors": ["Chandler Smith", "Marwa Abdulhai", "Manfred Diaz", "Marko Tesic", "Rakshit S. Trivedi", "Alexander Sasha Vezhnevets", "Lewis Hammond", "Jesse Clifton", "Minsuk Chang", "Edgar A. Du\u00e9\u00f1ez-Guzm\u00e1n", "John P. Agapiou", "Jayd Matyas", "Danny Karmon", "Akash Kundu", "Aliaksei Korshuk", "Ananya Ananya", "Arrasy Rahman", "Avinaash Anand Kulandaivel", "Bain McHale", "Beining Zhang", "Buyantuev Alexander", "Carlos Saith Rodriguez Rojas", "Caroline Wang", "Chetan Talele", "Chenao Liu", "Chichen Lin", "Diana Riazi", "Di Yang Shi", "Emanuel Tewolde", "Elizaveta Tennant", "Fangwei Zhong", "Fuyang Cui", "Gang Zhao", "Gema Parre\u00f1o Piqueras", "Hyeonggeun Yun", "Ilya Makarov", "Jiaxun Cui", "Jebish Purbey", "Jim Dilkes", "Jord Nguyen", "Lingyun Xiao", "Luis Felipe Giraldo", "Manuela Chacon-Chamorro", "Manuel Sebastian Rios Beltran", "Marta Emili Garc\u00eda Segura", "Mengmeng Wang", "Mogtaba Alim", "Nicanor Quijano", "Nico Schiavone", "Olivia Macmillan-Scott", "Oswaldo Pe\u00f1a", "Peter Stone", "Ram Mohan Rao Kadiyala", "Rolando Fernandez", "Ruben Manrique", "Sunjia Lu", "Sheila A. McIlraith", "Shamika Dhuri", "Shuqing Shi", "Siddhant Gupta", "Sneheel Sarangi", "Sriram Ganapathi Subramanian", "Taehun Cha", "Toryn Q. Klassen", "Wenming Tu", "Weijian Fan", "Wu Ruiyang", "Xue Feng", "Yali Du", "Yang Liu", "Yiding Wang", "Yipeng Kang", "Yoonchang Sung", "Yuxuan Chen", "Zhaowei Zhang", "Zhihan Wang", "Zhiqiang Wu", "Ziang Chen", "Zilong Zheng", "Zixia Jia", "Ziyan Wang", "Dylan Hadfield-Menell", "Natasha Jaques", "Tim Baarslag", "Jose Hernandez-Orallo", "Joel Z. Leibo"], "title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia", "comment": "Published at NeurIPS Datasets and Benchmarks 2025, 10 pages", "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u96f6\u6837\u672c\u6df7\u5408\u52a8\u673a\u73af\u5883\u4e2d\u5408\u4f5c\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528Concordia\u81ea\u7136\u8bed\u8a00\u591a\u667a\u80fd\u4f53\u4eff\u771f\u73af\u5883\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u667a\u80fd\u4f53\u5728\u9700\u8981\u8bf4\u670d\u548c\u89c4\u8303\u6267\u884c\u7684\u573a\u666f\u4e2d\u5b58\u5728\u663e\u8457\u6cdb\u5316\u5dee\u8ddd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5728\u793e\u4ea4\u4e92\u52a8\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u5e76\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u4e0e\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u573a\u666f\u4e2d\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u8861\u91cf\u8fd9\u4e9b\u80fd\u529b\u5728\u65b0\u9896\u793e\u4ea4\u60c5\u5883\u4e2d\u7684\u6cdb\u5316\u8868\u73b0\uff0c\u8fd9\u6784\u6210\u4e86LLM\u667a\u80fd\u4f53\u53d1\u5c55\u7684\u5173\u952e\u524d\u6cbf\u6311\u6218\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u57fa\u4e8eConcordia\u81ea\u7136\u8bed\u8a00\u591a\u667a\u80fd\u4f53\u4eff\u771f\u73af\u5883\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u8bd5\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u5408\u4f5c\u4f19\u4f34\u548c\u60c5\u5883\u4e2d\u8bc6\u522b\u5e76\u5229\u7528\u4e92\u5229\u673a\u4f1a\u7684\u80fd\u529b\uff0c\u6765\u8861\u91cf\u5176\u4e00\u822c\u5408\u4f5c\u667a\u80fd\uff0c\u8be5\u65b9\u6cd5\u7279\u522b\u5173\u6ce8\u96f6\u6837\u672c\u6df7\u5408\u52a8\u673a\u73af\u5883\u4e2d\u7684\u5408\u4f5c\u8868\u73b0\u8bc4\u4f30\u3002", "result": "\u57fa\u4e8eNeurIPS 2024 Concordia\u7ade\u8d5b\u7684\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u667a\u80fd\u4f53\u80fd\u529b\u4e0e\u7a33\u5065\u6cdb\u5316\u6240\u9700\u6c34\u5e73\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u8bf4\u670d\u548c\u89c4\u8303\u6267\u884c\u7684\u573a\u666f\u4e2d\uff0c\u667a\u80fd\u4f53\u5728\u4ece\u8c08\u5224\u5230\u96c6\u4f53\u884c\u52a8\u95ee\u9898\u7684\u591a\u6837\u5316\u60c5\u5883\u4e2d\u5b9e\u73b0\u4e92\u5229\u7684\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u5408\u4f5c\u667a\u80fd\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u5e76\u6307\u51fa\u4e86\u5728\u8bf4\u670d\u548c\u89c4\u8303\u6267\u884c\u7b49\u5173\u952e\u793e\u4ea4\u80fd\u529b\u65b9\u9762\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2512.04032", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04032", "abs": "https://arxiv.org/abs/2512.04032", "authors": ["Andreas Koukounas", "Georgios Mastrapas", "Florian H\u00f6nicke", "Sedigheh Eslami", "Guillaume Roncari", "Scott Martens", "Han Xiao"], "title": "Jina-VLM: Small Multilingual Vision Language Model", "comment": "18 pages, 1-7 main content", "summary": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Jina-VLM\uff0c\u4e00\u4e2a24\u4ebf\u53c2\u6570\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u5f00\u653e2B\u89c4\u6a21VLM\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u6ce8\u610f\u529b\u6c60\u5316\u8fde\u63a5\u5668\u5c06SigLIP2\u89c6\u89c9\u7f16\u7801\u5668\u4e0eQwen3\u8bed\u8a00\u4e3b\u5e72\u8026\u5408\uff0c\u652f\u6301\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u4ee4\u724c\u9ad8\u6548\u5904\u7406\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u67092B\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u7eaf\u6587\u672c\u6027\u80fd\u7684\u540c\u65f6\uff0c\u9700\u8981\u63d0\u5347\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002\u5f53\u524d\u5f00\u653e2B\u89c4\u6a21VLM\u5728\u591a\u8bed\u8a00\u8bc4\u4f30\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u6765\u5904\u7406\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "method": "Jina-VLM\u91c7\u7528SigLIP2\u89c6\u89c9\u7f16\u7801\u5668\u4e0eQwen3\u8bed\u8a00\u4e3b\u5e72\u76f8\u7ed3\u5408\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u6c60\u5316\u8fde\u63a5\u5668\u5b9e\u73b0\u89c6\u89c9\u4e0e\u8bed\u8a00\u6a21\u6001\u7684\u878d\u5408\u3002\u8be5\u8fde\u63a5\u5668\u8bbe\u8ba1\u652f\u6301\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u4ee4\u724c\u9ad8\u6548\u5904\u7406\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u4e3a24\u4ebf\u3002", "result": "\u5728\u6807\u51c6VQA\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u8bed\u8a00\u8bc4\u4f30\u4e2d\uff0cJina-VLM\u8d85\u8d8a\u4e86\u540c\u7c7b\u53ef\u6bd4\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u7eaf\u6587\u672c\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u80fd\u529b\uff0c\u57282B\u89c4\u6a21\u5f00\u653eVLM\u4e2d\u786e\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u6746\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6ce8\u610f\u529b\u6c60\u5316\u8fde\u63a5\u5668\u5c06\u5148\u8fdb\u7684\u89c6\u89c9\u7f16\u7801\u5668\u4e0e\u8bed\u8a00\u4e3b\u5e72\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u5728\u4e2d\u7b49\u53c2\u6570\u89c4\u6a21\u4e0b\u5b9e\u73b0\u5353\u8d8a\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002\u8fd9\u4e00\u67b6\u6784\u4e3a\u5f00\u53d1\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u521b\u9020\u4e86\u6761\u4ef6\u3002"}}
{"id": "2512.03237", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.03237", "abs": "https://arxiv.org/abs/2512.03237", "authors": ["Nafiseh Izadyar", "Teseo Schneider"], "title": "LLM-Guided Material Inference for 3D Point Clouds", "comment": null, "summary": "Most existing 3D shape datasets and models focus solely on geometry, overlooking the material properties that determine how objects appear. We introduce a two-stage large language model (LLM) based method for inferring material composition directly from 3D point clouds with coarse segmentations. Our key insight is to decouple reasoning about what an object is from what it is made of. In the first stage, an LLM predicts the object's semantic; in the second stage, it assigns plausible materials to each geometric segment, conditioned on the inferred semantics. Both stages operate in a zero-shot manner, without task-specific training. Because existing datasets lack reliable material annotations, we evaluate our method using an LLM-as-a-Judge implemented in DeepEval. Across 1,000 shapes from Fusion/ABS and ShapeNet, our method achieves high semantic and material plausibility. These results demonstrate that language models can serve as general-purpose priors for bridging geometric reasoning and material understanding in 3D data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5e26\u6709\u7c97\u5206\u5272\u7684\u4e09\u7ef4\u70b9\u4e91\u76f4\u63a5\u63a8\u65ad\u6750\u6599\u7ec4\u6210\uff0c\u901a\u8fc7\u5c06\u7269\u4f53\u8bc6\u522b\u4e0e\u6750\u6599\u63a8\u7406\u89e3\u8026\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684\u6750\u6599\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u4e09\u7ef4\u5f62\u72b6\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u51e0\u4f55\u7279\u5f81\uff0c\u5ffd\u89c6\u4e86\u51b3\u5b9a\u7269\u4f53\u5916\u89c2\u7684\u5173\u952e\u6750\u6599\u5c5e\u6027\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u4ece\u4e09\u7ef4\u6570\u636e\u4e2d\u63a8\u65ad\u6750\u6599\u7ec4\u6210\u7684\u53ef\u9760\u65b9\u6cd5\uff0c\u8fd9\u9650\u5236\u4e86\u4e09\u7ef4\u5f62\u72b6\u7684\u5b8c\u6574\u7269\u7406\u7406\u89e3\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff1a\u7b2c\u4e00\u9636\u6bb5LLM\u9884\u6d4b\u7269\u4f53\u7684\u8bed\u4e49\u7c7b\u522b\uff0c\u7b2c\u4e8c\u9636\u6bb5LLM\u6839\u636e\u63a8\u65ad\u7684\u8bed\u4e49\u4e3a\u6bcf\u4e2a\u51e0\u4f55\u5206\u5272\u5206\u914d\u5408\u7406\u7684\u6750\u6599\uff0c\u4e24\u4e2a\u9636\u6bb5\u5747\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u8fd0\u884c\uff0c\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u3002", "result": "\u5728Fusion/ABS\u548cShapeNet\u6570\u636e\u96c6\u76841000\u4e2a\u5f62\u72b6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u8bed\u4e49\u548c\u6750\u6599\u5408\u7406\u6027\uff0c\u901a\u8fc7DeepEval\u5b9e\u73b0\u7684LLM-as-a-Judge\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u96f6\u6837\u672c\u6750\u6599\u63a8\u7406\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u901a\u7528\u5148\u9a8c\u77e5\u8bc6\uff0c\u6709\u6548\u8fde\u63a5\u4e09\u7ef4\u6570\u636e\u4e2d\u7684\u51e0\u4f55\u63a8\u7406\u4e0e\u6750\u6599\u7406\u89e3\uff0c\u4e3a\u96f6\u6837\u672c\u6750\u6599\u63a8\u65ad\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5e76\u4e3a\u4e09\u7ef4\u5f62\u72b6\u7684\u5b8c\u6574\u7269\u7406\u5c5e\u6027\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.03438", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03438", "abs": "https://arxiv.org/abs/2512.03438", "authors": ["Reuben Tan", "Baolin Peng", "Zhengyuan Yang", "Hao Cheng", "Oier Mees", "Theodore Zhao", "Andrea Tupini", "Isar Meijier", "Qianhui Wu", "Yuncong Yang", "Lars Liden", "Yu Gu", "Sheng Zhang", "Xiaodong Liu", "Lijuan Wang", "Marc Pollefeys", "Yong Jae Lee", "Jianfeng Gao"], "title": "Multimodal Reinforcement Learning with Agentic Verifier for AI Agents", "comment": null, "summary": "Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Argos\uff08Agentic Reward for Grounded & Objective Scoring\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u8bad\u7ec3\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u7684\u5956\u52b1\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6559\u5e08\u6a21\u578b\u548c\u89c4\u5219\u8bc4\u5206\u51fd\u6570\u6765\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\uff0c\u4ece\u800c\u89e3\u51b3\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u4e2d\u7a00\u758f\u5956\u52b1\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u667a\u80fd\u4f53\u63a8\u7406\u6a21\u578b\u901a\u5e38\u4ec5\u4f9d\u8d56\u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u7684\u7a00\u758f\u5956\u52b1\u8fdb\u884c\u4f18\u5316\uff0c\u8fd9\u79cd\u5956\u52b1\u673a\u5236\u65e0\u6cd5\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u5b66\u4e60\u6307\u5bfc\u3002\u4e0d\u540c\u6837\u672c\u9700\u8981\u4e0d\u540c\u7684\u8bc4\u5206\u51fd\u6570\uff0c\u800c\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u7684\u5956\u52b1\u4fe1\u53f7\u53ef\u80fd\u5b58\u5728\u566a\u58f0\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u63d0\u5347\u3002", "method": "Argos\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u6837\u672c\u4ece\u6559\u5e08\u6a21\u578b\u6d3e\u751f\u7684\u8bc4\u5206\u51fd\u6570\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u8bc4\u5206\u51fd\u6570\u6c60\u4e2d\u52a8\u6001\u9009\u62e9\uff0c\u540c\u65f6\u8bc4\u4f30\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u6700\u7ec8\u54cd\u5e94\u51c6\u786e\u6027\u3001\u5f15\u7528\u5b9e\u4f53\u548c\u52a8\u4f5c\u7684\u65f6\u7a7a\u5b9a\u4f4d\u8d28\u91cf\uff0c\u4ee5\u53ca\u63a8\u7406\u8fc7\u7a0b\u7684\u8d28\u91cf\u3002\u8be5\u65b9\u6cd5\u5728\u76d1\u7763\u5fae\u8c03\u6570\u636e\u7b5b\u9009\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9636\u6bb5\u5747\u5e94\u7528\u667a\u80fd\u4f53\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u4f7f\u7528Argos\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u3001\u89c6\u89c9\u5e7b\u89c9\u4ee5\u53ca\u673a\u5668\u4eba\u548c\u5177\u8eabAI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4f9d\u8d56\u9ad8\u5ea6\u7b5b\u9009\u63a8\u7406\u6570\u636e\u7684\u76d1\u7763\u5fae\u8c03\u540e\u8bad\u7ec3\u662f\u4e0d\u591f\u7684\uff0c\u6ca1\u6709\u5728\u7ebf\u9a8c\u8bc1\u7684\u667a\u80fd\u4f53\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f1a\u5d29\u6e83\u4e3a\u672a\u63a5\u5730\u6c14\u7684\u89e3\u51b3\u65b9\u6848\u3002Argos\u8fd8\u80fd\u6709\u6548\u51cf\u5c11\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u73b0\u8c61\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ec6\u7c92\u5ea6\u3001\u591a\u7ef4\u5ea6\u5956\u52b1\u673a\u5236\u5bf9\u591a\u6a21\u6001\u63a8\u7406\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\u91cd\u8981\u6027\uff0cArgos\u901a\u8fc7\u5e15\u7d2f\u6258\u6700\u4f18\u6027\u6982\u5ff5\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u7ebf\u9a8c\u8bc1\u5728\u9632\u6b62\u667a\u80fd\u4f53\u5d29\u6e83\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2512.03463", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03463", "abs": "https://arxiv.org/abs/2512.03463", "authors": ["Shojiro Yamabe", "Futa Waseda", "Daiki Shiono", "Tsubasa Takahashi"], "title": "Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models", "comment": null, "summary": "Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6587\u672c\u6253\u5370\u56fe\u50cf\uff08TPI\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6587\u672c\u63cf\u8ff0\u76f4\u63a5\u6e32\u67d3\u5230\u7a7a\u767d\u753b\u5e03\u4e0a\u751f\u6210\u5408\u6210\u56fe\u50cf\uff0c\u4ee5\u89e3\u51b3\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\u548c\u6536\u96c6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u6587\u672c\u4e2d\u5fc3\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728VQA\u4efb\u52a1\u4e2d\u9700\u8981\u5927\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u4f46\u56fe\u50cf\u6570\u636e\u6536\u96c6\u53d7\u9690\u79c1\u9650\u5236\u548c\u9886\u57df\u7a00\u7f3a\u6027\u5236\u7ea6\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u800c\u6587\u672c\u6570\u636e\u5e7f\u6cdb\u53ef\u7528\u4e14\u6613\u4e8e\u7f16\u8f91\u6269\u5c55\uff0c\u7136\u800c\u4ec5\u4f7f\u7528\u539f\u59cb\u6587\u672c\u8bad\u7ec3\u4f1a\u56e0\u6a21\u6001\u9e3f\u6c9f\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002", "method": "\u672c\u6587\u63d0\u51fa\u6587\u672c\u6253\u5370\u56fe\u50cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7ed9\u5b9a\u6587\u672c\u63cf\u8ff0\u76f4\u63a5\u6e32\u67d3\u5230\u7eaf\u767d\u8272\u753b\u5e03\u4e0a\u751f\u6210\u5408\u6210\u56fe\u50cf\uff0c\u8fd9\u79cd\u7b80\u5355\u6e32\u67d3\u5c06\u6587\u672c\u6295\u5f71\u5230\u56fe\u50cf\u6a21\u6001\uff0c\u53ef\u4f4e\u6210\u672c\u96c6\u6210\u5230\u73b0\u6709LVLM\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6587\u672c\u8bed\u4e49\uff0c\u907f\u514d\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5e38\u51fa\u73b0\u7684\u8bed\u4e49\u5931\u771f\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u6a21\u578b\u548c\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u7cfb\u7edf\u5b9e\u9a8c\u4e2d\uff0cTPI\u65b9\u6cd5\u5728\u6587\u672c\u4e2d\u5fc3\u8bad\u7ec3\u65b9\u9762\u6bd4\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u66f4\u6709\u6548\uff0c\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e86TPI\u4f5c\u4e3a\u4f4e\u6210\u672c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7684\u5b9e\u9645\u6548\u7528\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cdVQA\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u6587\u672c\u4e2d\u5fc3\u8bad\u7ec3\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0cTPI\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5168\u81ea\u52a8\u6570\u636e\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u5e7f\u6cdb\u53ef\u7528\u7684\u6587\u672c\u8d44\u6e90\u6765\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2512.03284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03284", "abs": "https://arxiv.org/abs/2512.03284", "authors": ["Hongpei Zheng", "Shijie Li", "Yanran Li", "Hujun Yin"], "title": "SpatialReasoner: Active Perception for Large-Scale 3D Scene Understanding", "comment": null, "summary": "Spatial reasoning in large-scale 3D environments remains challenging for current vision-language models, which are typically constrained to room-scale scenarios. We introduce H$^2$U3D (Holistic House Understanding in 3D), a 3D visual question answering dataset designed for house-scale scene understanding. H$^2$U3D features multi-floor environments spanning up to three floors and 10-20 rooms, covering more than 300 m$^2$. Through an automated annotation pipeline, it constructs hierarchical coarse-to-fine visual representations and generates diverse question-answer pairs with chain-of-thought annotations. We further propose SpatialReasoner, an active perception framework that autonomously invokes spatial tools to explore 3D scenes based on textual queries. SpatialReasoner is trained through a two-stage strategy: a supervised cold start followed by reinforcement learning with an adaptive exploration reward that promotes efficient exploration while discouraging redundant operations. Extensive experiments demonstrate that SpatialReasoner achieves state-of-the-art performance on H$^2$U3D, outperforming strong baselines including GPT-4o and Gemini-2.5-Pro. Notably, our method attains superior results while using only 3-4 images in total on average, compared to baselines requiring 16+ images, highlighting the effectiveness of our coarse-to-fine active exploration paradigm.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86H\u00b2U3D\u623f\u5c4b\u5c3a\u5ea63D\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u548cSpatialReasoner\u4e3b\u52a8\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u5c42\u6b21\u5316\u8868\u793a\u548c\u81ea\u9002\u5e94\u63a2\u7d22\u5956\u52b1\u673a\u5236\uff0c\u5728\u5927\u89c4\u6a213D\u73af\u5883\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u6240\u9700\u7684\u56fe\u50cf\u6570\u91cf\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5927\u89c4\u6a213D\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u5b58\u5728\u5c40\u9650\uff0c\u4e3b\u8981\u5c40\u9650\u4e8e\u623f\u95f4\u5c3a\u5ea6\u573a\u666f\uff0c\u7f3a\u4e4f\u9488\u5bf9\u623f\u5c4b\u5c3a\u5ea6\u591a\u697c\u5c42\u590d\u6742\u73af\u5883\u7684\u7cfb\u7edf\u8bc4\u4f30\u57fa\u51c6\u548c\u6709\u6548\u63a2\u7d22\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86H\u00b2U3D\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\u6784\u5efa\u4ece\u7c97\u5230\u7ec6\u7684\u5c42\u6b21\u5316\u89c6\u89c9\u8868\u793a\u5e76\u751f\u6210\u591a\u6837\u5316\u95ee\u7b54\u5bf9\uff1b\u540c\u65f6\u5f00\u53d1\u4e86SpatialReasoner\u4e3b\u52a8\u611f\u77e5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u6587\u672c\u67e5\u8be2\u81ea\u4e3b\u8c03\u7528\u7a7a\u95f4\u5de5\u5177\u63a2\u7d223D\u573a\u666f\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u76d1\u7763\u5f0f\u51b7\u542f\u52a8\u540e\u63a5\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u63a2\u7d22\u5956\u52b1\u673a\u5236\u4ee5\u4fc3\u8fdb\u9ad8\u6548\u63a2\u7d22\u540c\u65f6\u51cf\u5c11\u5197\u4f59\u64cd\u4f5c\u3002", "result": "SpatialReasoner\u5728H\u00b2U3D\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u4e86GPT-4o\u548cGemini-2.5-Pro\u7b49\u5f3a\u57fa\u7ebf\u6a21\u578b\uff1b\u8be5\u65b9\u6cd5\u5e73\u5747\u4ec5\u97003-4\u5f20\u56fe\u50cf\u5373\u53ef\u8fbe\u5230\u4f18\u5f02\u6548\u679c\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u9700\u898116\u5f20\u4ee5\u4e0a\u56fe\u50cf\uff0c\u8bc1\u660e\u4e86\u7c97\u5230\u7ec6\u4e3b\u52a8\u63a2\u7d22\u8303\u5f0f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u623f\u5c4b\u5c3a\u5ea63D\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u548c\u4e3b\u52a8\u611f\u77e5\u6846\u67b6\uff0c\u4e3a\u5927\u89c4\u6a213D\u73af\u5883\u7a7a\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\uff1b\u81ea\u9002\u5e94\u63a2\u7d22\u5956\u52b1\u673a\u5236\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u63a2\u7d22\u6548\u7387\uff0c\u4e3a\u672a\u67653D\u573a\u666f\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u8bba\u6307\u5bfc\u3002"}}
{"id": "2512.03627", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03627", "abs": "https://arxiv.org/abs/2512.03627", "authors": ["Junming Liu", "Yifei Sun", "Weihua Cheng", "Haodong Lei", "Yirong Chen", "Licheng Wen", "Xuemeng Yang", "Daocheng Fu", "Pinlong Cai", "Nianchen Deng", "Yi Yu", "Shuyue Hu", "Botian Shi", "Ding Wang"], "title": "MemVerse: Multimodal Memory for Lifelong Learning Agents", "comment": "11 pages, 2 figures, 2 tables", "summary": "Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember. Without reliable memory, agents catastrophically forget past experiences, struggle with long-horizon reasoning, and fail to operate coherently in multimodal or interactive environments. We introduce MemVerse, a model-agnostic, plug-and-play memory framework that bridges fast parametric recall with hierarchical retrieval-based memory, enabling scalable and adaptive multimodal intelligence. MemVerse maintains short-term memory for recent context while transforming raw multimodal experiences into structured long-term memories organized as hierarchical knowledge graphs. This design supports continual consolidation, adaptive forgetting, and bounded memory growth. To handle real-time demands, MemVerse introduces a periodic distillation mechanism that compresses essential knowledge from long-term memory into the parametric model, allowing fast, differentiable recall while preserving interpretability. Extensive experiments demonstrate that MemVerse significantly improves multimodal reasoning and continual learning efficiency, empowering agents to remember, adapt, and reason coherently across extended interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MemVerse\uff0c\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u5373\u63d2\u5373\u7528\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u6865\u63a5\u5feb\u901f\u53c2\u6570\u5316\u56de\u5fc6\u4e0e\u5206\u5c42\u68c0\u7d22\u5f0f\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u9002\u5e94\u591a\u6a21\u6001\u667a\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c4\u6a21\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46AI\u667a\u80fd\u4f53\u4ecd\u9762\u4e34\u6839\u672c\u6027\u9650\u5236\uff1a\u65e0\u6cd5\u6709\u6548\u8bb0\u5fc6\u3002\u7f3a\u4e4f\u53ef\u9760\u8bb0\u5fc6\u5bfc\u81f4\u667a\u80fd\u4f53\u707e\u96be\u6027\u9057\u5fd8\u8fc7\u53bb\u7ecf\u9a8c\uff0c\u96be\u4ee5\u8fdb\u884c\u957f\u65f6\u7a0b\u63a8\u7406\uff0c\u5728\u591a\u6a21\u6001\u6216\u4ea4\u4e92\u73af\u5883\u4e2d\u65e0\u6cd5\u4fdd\u6301\u8fde\u8d2f\u64cd\u4f5c\u3002", "method": "MemVerse\u91c7\u7528\u6a21\u578b\u65e0\u5173\u7684\u5373\u63d2\u5373\u7528\u67b6\u6784\uff0c\u5c06\u5feb\u901f\u53c2\u6570\u5316\u56de\u5fc6\u4e0e\u5206\u5c42\u68c0\u7d22\u5f0f\u8bb0\u5fc6\u76f8\u7ed3\u5408\u3002\u8be5\u6846\u67b6\u7ef4\u62a4\u77ed\u671f\u8bb0\u5fc6\u5904\u7406\u8fd1\u671f\u4e0a\u4e0b\u6587\uff0c\u540c\u65f6\u5c06\u539f\u59cb\u591a\u6a21\u6001\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u957f\u671f\u8bb0\u5fc6\uff0c\u7ec4\u7ec7\u4e3a\u5206\u5c42\u77e5\u8bc6\u56fe\u8c31\u3002\u901a\u8fc7\u5468\u671f\u6027\u84b8\u998f\u673a\u5236\u5c06\u957f\u671f\u8bb0\u5fc6\u4e2d\u7684\u5173\u952e\u77e5\u8bc6\u538b\u7f29\u5230\u53c2\u6570\u5316\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u5feb\u901f\u53ef\u5fae\u56de\u5fc6\u5e76\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMemVerse\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u7406\u548c\u6301\u7eed\u5b66\u4e60\u6548\u7387\u3002\u8be5\u6846\u67b6\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u6269\u5c55\u4ea4\u4e92\u4e2d\u8bb0\u5fc6\u3001\u9002\u5e94\u548c\u8fdb\u884c\u8fde\u8d2f\u63a8\u7406\uff0c\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "MemVerse\u4e3a\u89e3\u51b3\u667a\u80fd\u4f53\u8bb0\u5fc6\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8bb0\u5fc6\u7ed3\u6784\u548c\u5468\u671f\u6027\u84b8\u998f\u673a\u5236\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u9002\u5e94\u8bb0\u5fc6\u7cfb\u7edf\u3002\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u5177\u6709\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\u7684AI\u667a\u80fd\u4f53\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5bf9\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u6301\u7eed\u5b66\u4e60\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2512.03558", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03558", "abs": "https://arxiv.org/abs/2512.03558", "authors": ["Huy Quang Ung", "Guillaume Habault", "Yasutaka Nishimura", "Hao Niu", "Roberto Legaspi", "Tomoki Oya", "Ryoichi Kojima", "Masato Taya", "Chihiro Ono", "Atsunori Minamikawa", "Yan Liu"], "title": "CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding", "comment": "Accepted at SIGSPATIAL 2025 (Best paper candidates), 15 pages", "summary": "The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CartoMapQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u8bfb\u5236\u56fe\u5730\u56fe\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5730\u56fe\u8bed\u4e49\u7406\u89e3\u548c\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u663e\u8457\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9-\u6587\u672c\u878d\u5408\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u5236\u56fe\u5730\u56fe\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u5730\u56fe\u89e3\u8bfb\u4efb\u52a1\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u5bfc\u822a\u3001\u5730\u7406\u641c\u7d22\u548c\u57ce\u5e02\u89c4\u5212\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86CartoMapQA\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc72000\u4e2a\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u7531\u5236\u56fe\u5730\u56fe\u3001\u95ee\u9898\uff08\u5f00\u653e\u5f0f\u6216\u591a\u9009\u9898\uff09\u548c\u771f\u5b9e\u7b54\u6848\u7ec4\u6210\u3002\u8be5\u57fa\u51c6\u6db5\u76d6\u4e86\u4ece\u4f4e\u5c42\u5230\u9ad8\u5c42\u7684\u591a\u79cd\u5730\u56fe\u89e3\u8bfb\u6280\u80fd\uff0c\u5305\u62ec\u7b26\u53f7\u8bc6\u522b\u3001\u5d4c\u5165\u5f0f\u4fe1\u606f\u63d0\u53d6\u3001\u6bd4\u4f8b\u5c3a\u89e3\u8bfb\u548c\u57fa\u4e8e\u8def\u5f84\u7684\u63a8\u7406\u3002", "result": "\u5bf9\u5f00\u6e90\u548c\u4e13\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6a21\u578b\u5728\u5730\u56fe\u7279\u5b9a\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u5b58\u5728\u6301\u7eed\u6311\u6218\uff0c\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u5e76\u4e14\u5bb9\u6613\u53d7\u5230\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u76f8\u5173\u9519\u8bef\u7684\u5f71\u54cd\u3002\u8fd9\u4e9b\u5f31\u70b9\u5728\u7b26\u53f7\u8bc6\u522b\u3001\u6bd4\u4f8b\u5c3a\u89e3\u8bfb\u548c\u590d\u6742\u8def\u5f84\u63a8\u7406\u4efb\u52a1\u4e2d\u5c24\u4e3a\u660e\u663e\u3002", "conclusion": "CartoMapQA\u57fa\u51c6\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5730\u56fe\u7406\u89e3\u65b9\u9762\u7684\u5f31\u70b9\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u67b6\u6784\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u5de5\u5177\u3002\u8be5\u7814\u7a76\u652f\u6301\u5f00\u53d1\u66f4\u9002\u7528\u4e8e\u4f9d\u8d56\u53ef\u9760\u5730\u56fe\u7406\u89e3\u7684\u5b9e\u9645\u5e94\u7528\u7684\u6a21\u578b\uff0c\u5e76\u516c\u5f00\u4e86\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u7814\u7a76\u793e\u533a\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.03335", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03335", "abs": "https://arxiv.org/abs/2512.03335", "authors": ["Faizan Farooq Khan", "K J Joseph", "Koustava Goswami", "Mohamed Elhoseiny", "Balaji Vasan Srinivasan"], "title": "Step-by-step Layered Design Generation", "comment": null, "summary": "Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9010\u6b65\u5206\u5c42\u8bbe\u8ba1\u751f\u6210\u95ee\u9898\u8bbe\u5b9a\uff0c\u5e76\u5f00\u53d1\u4e86SLEDGE\u6a21\u578b\u6765\u6a21\u62df\u8bbe\u8ba1\u5e08\u9010\u6b65\u4fee\u6539\u8bbe\u8ba1\u7684\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5c06\u6bcf\u6b21\u66f4\u65b0\u5efa\u6a21\u4e3a\u539f\u5b50\u5316\u7684\u5206\u5c42\u53d8\u5316\u6765\u5b9e\u73b0\u6307\u4ee4\u9a71\u52a8\u7684\u8bbe\u8ba1\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5c06\u8bbe\u8ba1\u5408\u6210\u89c6\u4e3a\u5355\u6b65\u751f\u6210\u95ee\u9898\uff0c\u4e25\u91cd\u4f4e\u4f30\u4e86\u521b\u9020\u6027\u8fc7\u7a0b\u7684\u56fa\u6709\u590d\u6742\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u8bbe\u8ba1\u5e08\u9010\u6b65\u7ec6\u5316\u548c\u589e\u5f3a\u5de5\u4f5c\u7684\u672c\u8d28\u7279\u5f81\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6a21\u62df\u9010\u6b65\u8bbe\u8ba1\u8fc7\u7a0b\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u9010\u6b65\u5206\u5c42\u8bbe\u8ba1\u751f\u6210\u7684\u65b0\u95ee\u9898\u8bbe\u5b9a\uff0c\u5e76\u5f00\u53d1\u4e86SLEDGE\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u6bcf\u6b21\u8bbe\u8ba1\u66f4\u65b0\u5efa\u6a21\u4e3a\u57fa\u4e8e\u5148\u524d\u72b6\u6001\u7684\u539f\u5b50\u5316\u5206\u5c42\u53d8\u5316\uff0c\u540c\u65f6\u786e\u4fdd\u4e0e\u8bbe\u8ba1\u6307\u4ee4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u5b8c\u6574\u8bc4\u4f30\u5957\u4ef6\uff0c\u901a\u8fc7\u8be6\u5c3d\u7684\u5b9e\u9a8c\u5206\u6790\u8868\u660e\uff0c\u4e0e\u9488\u5bf9\u65b0\u8bbe\u5b9a\u5b9a\u5236\u7684\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9010\u6b65\u8bbe\u8ba1\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u9010\u6b65\u8bbe\u8ba1\u751f\u6210\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u5438\u5f15\u66f4\u591a\u7814\u7a76\u5173\u6ce8\u8fd9\u4e00\u88ab\u4f4e\u4f30\u4f46\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c\u7684\u7814\u7a76\u9886\u57df\uff0c\u63a8\u52a8\u8bbe\u8ba1\u751f\u6210\u65b9\u6cd5\u5411\u66f4\u7b26\u5408\u4eba\u7c7b\u521b\u4f5c\u8fc7\u7a0b\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2512.03783", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.03783", "abs": "https://arxiv.org/abs/2512.03783", "authors": ["Dongchao Yang", "Songxiang Liu", "Disong Wang", "Yuanyuan Wang", "Guanglu Wan", "Helen Meng"], "title": "Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning", "comment": null, "summary": "Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOmni-AutoThink\uff0c\u4e00\u79cd\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u6765\u89e3\u51b3\u73b0\u6709Omni\u6a21\u578b\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u8fc7\u5ea6\u63a8\u7406\u6216\u590d\u6742\u95ee\u9898\u4e0a\u63a8\u7406\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u81ea\u9002\u5e94\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709Omni\u6a21\u578b\u5728\u591a\u6a21\u6001\u611f\u77e5\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u63a8\u7406\u884c\u4e3a\u4ecd\u7136\u50f5\u5316\uff0c\u8981\u4e48\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u8fc7\u5ea6\u63a8\u7406\uff0c\u8981\u4e48\u5728\u9700\u8981\u63a8\u7406\u65f6\u63a8\u7406\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u91c7\u7528\u81ea\u9002\u5e94\u76d1\u7763\u5fae\u8c03\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u63a8\u7406\u589e\u5f3a\u6570\u636e\u8d4b\u4e88\u6a21\u578b\u57fa\u7840\u63a8\u7406\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\uff0c\u57fa\u4e8e\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u5956\u52b1\u53cd\u9988\u4f18\u5316\u63a8\u7406\u884c\u4e3a\uff0c\u5e76\u6784\u5efa\u4e86\u6db5\u76d6\u6587\u672c\u3001\u6587\u672c-\u97f3\u9891\u3001\u6587\u672c-\u89c6\u89c9\u3001\u6587\u672c-\u97f3\u9891-\u89c6\u89c9\u6a21\u6001\u7684\u5168\u9762\u81ea\u9002\u5e94\u63a8\u7406\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u81ea\u9002\u5e94\u63a8\u7406\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u5148\u524d\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6784\u5efa\u7684\u57fa\u51c6\u63d0\u4f9b\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\u5206\u5272\uff0c\u652f\u6301\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\u5728\u63d0\u5347Omni\u6a21\u578b\u63a8\u7406\u6548\u7387\u548c\u6548\u679c\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u63a8\u7406\u673a\u5236\uff0c\u6240\u6709\u57fa\u51c6\u6570\u636e\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u91ca\u653e\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2512.03746", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03746", "abs": "https://arxiv.org/abs/2512.03746", "authors": ["Zirun Guo", "Minjie Hong", "Feng Zhang", "Kai Jia", "Tao Jin"], "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images", "comment": null, "summary": "Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCodeVision\uff0c\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7801\u5373\u5de5\u5177\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u4ee3\u7801\u4f5c\u4e3a\u901a\u7528\u63a5\u53e3\u6765\u8c03\u7528\u4efb\u610f\u56fe\u50cf\u64cd\u4f5c\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u63a8\u7406\u4e2d\u7684\u8106\u5f31\u6027\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u9762\u5bf9\u7b80\u5355\u65b9\u5411\u53d8\u5316\u6216\u81ea\u7136\u635f\u574f\u7684\u56fe\u50cf\u65f6\u4e5f\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u8106\u5f31\u6027\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u540c\u65f6\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u7684\u5de5\u5177\u96c6\uff0c\u7f3a\u4e4f\u73b0\u5b9e\u5fc5\u8981\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faCodeVision\u6846\u67b6\uff0c\u5c06\u4ee3\u7801\u4f5c\u4e3a\u901a\u7528\u63a5\u53e3\u6765\u8c03\u7528\u4efb\u610f\u56fe\u50cf\u64cd\u4f5c\uff0c\u8d85\u8d8a\u56fa\u5b9a\u7684\u5de5\u5177\u6ce8\u518c\u8868\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9996\u5148\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u8be5\u6570\u636e\u96c6\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u591a\u8f6e\u5de5\u5177\u7ec4\u5408\u548c\u9519\u8bef\u6062\u590d\u800c\u6784\u5efa\uff0c\u968f\u540e\u4f7f\u7528\u5177\u6709\u65b0\u9896\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u6765\u9f13\u52b1\u6218\u7565\u6027\u548c\u9ad8\u6548\u7684\u5de5\u5177\u4f7f\u7528\u3002", "result": "\u5728Qwen2.5-VL\u548cQwen3-VL\u7cfb\u5217\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4fc3\u8fdb\u4e86\u65b0\u5174\u80fd\u529b\u7684\u53d1\u5c55\uff0c\u5305\u62ec\u7075\u6d3b\u7684\u5de5\u5177\u7ec4\u5408\u3001\u9ad8\u6548\u7684\u94fe\u5f0f\u6267\u884c\u4ee5\u53ca\u4ece\u8fd0\u884c\u65f6\u53cd\u9988\u4e2d\u8fdb\u884c\u7a33\u5065\u7684\u9519\u8bef\u6062\u590d\uff1b\u540c\u65f6\u6784\u5efa\u4e86\u65b0\u7684SFT\u548cRL\u6570\u636e\u96c6\u4ee5\u53ca\u6311\u6218\u6027\u57fa\u51c6\u5957\u4ef6\uff0c\u7528\u4e8e\u4e25\u683c\u8bc4\u4f30\u65b9\u5411\u53d8\u5316\u548c\u591a\u5de5\u5177\u63a8\u7406\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u63a8\u7406\u4e2d\u7684\u5173\u952e\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff1bCodeVision\u6846\u67b6\u901a\u8fc7\u4ee3\u7801\u5373\u5de5\u5177\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u548c\u9c81\u68d2\u7684\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4e3a\u672a\u6765\u5de5\u5177\u589e\u5f3a\u578b\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2512.03369", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03369", "abs": "https://arxiv.org/abs/2512.03369", "authors": ["Nan Zhou", "Huandong Wang", "Jiahao Li", "Han Li", "Yali Song", "Qiuhua Wang", "Yong Li", "Xinlei Chen"], "title": "FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting", "comment": null, "summary": "Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86FireSentry\u6570\u636e\u96c6\u548cFiReDiff\u8303\u5f0f\uff0c\u524d\u8005\u662f\u9996\u4e2a\u7701\u7ea7\u89c4\u6a21\u3001\u4e9a\u7c73\u7ea7\u7a7a\u95f4\u548c\u4e9a\u79d2\u7ea7\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u91ce\u706b\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u540e\u8005\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u6a21\u6001\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9996\u5148\u751f\u6210\u7ea2\u5916\u89c6\u9891\u5e8f\u5217\u518d\u7cbe\u786e\u5206\u5272\u706b\u573a\u63a9\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u91ce\u706b\u8513\u5ef6\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u91ce\u706b\u8513\u5ef6\u9884\u6d4b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7c97\u65f6\u7a7a\u5c3a\u5ea6\u5e76\u4f9d\u8d56\u4f4e\u5206\u8fa8\u7387\u536b\u661f\u6570\u636e\uff0c\u4ec5\u80fd\u6355\u6349\u5b8f\u89c2\u706b\u60c5\u72b6\u6001\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u9ad8\u7cbe\u5ea6\u5c40\u90e8\u706b\u52bf\u52a8\u6001\u5efa\u6a21\u80fd\u529b\uff0c\u4e9f\u9700\u7ec6\u7c92\u5ea6\u9884\u6d4b\u65b9\u6cd5\u4ee5\u63d0\u5347\u5e94\u6025\u54cd\u5e94\u6548\u80fd\u548c\u51b3\u7b56\u7cbe\u5ea6\u3002", "method": "\u7814\u7a76\u9996\u5148\u6784\u5efa\u4e86FireSentry\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u540c\u6b65\u65e0\u4eba\u673a\u5e73\u53f0\u91c7\u96c6\u4e9a\u7c73\u7ea7\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u4e9a\u79d2\u7ea7\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u53ef\u89c1\u5149\u4e0e\u7ea2\u5916\u89c6\u9891\u6d41\u3001\u73b0\u573a\u73af\u5883\u6d4b\u91cf\u6570\u636e\u53ca\u4eba\u5de5\u9a8c\u8bc1\u7684\u706b\u573a\u63a9\u7801\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u5efa\u7acb\u4e86\u6db5\u76d6\u7269\u7406\u6a21\u578b\u3001\u6570\u636e\u9a71\u52a8\u6a21\u578b\u548c\u751f\u6210\u6a21\u578b\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff1b\u63d0\u51fa\u4e86FiReDiff\u53cc\u6a21\u6001\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u5148\u5728\u7ea2\u5916\u6a21\u6001\u4e2d\u9884\u6d4b\u672a\u6765\u89c6\u9891\u5e8f\u5217\uff0c\u518d\u57fa\u4e8e\u751f\u6210\u7684\u52a8\u6001\u4fe1\u606f\u5728\u63a9\u7801\u6a21\u6001\u4e2d\u7cbe\u786e\u5206\u5272\u706b\u573a\u63a9\u7801\u3002", "result": "FiReDiff\u5728\u751f\u6210\u6a21\u578b\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\uff0c\u89c6\u9891\u8d28\u91cf\u65b9\u9762PSNR\u63d0\u534739.2%\u3001SSIM\u63d0\u534736.1%\u3001LPIPS\u63d0\u534750.0%\u3001FVD\u63d0\u534729.4%\uff0c\u63a9\u7801\u7cbe\u5ea6\u65b9\u9762AUPRC\u63d0\u53473.3%\u3001F1\u5206\u6570\u63d0\u534759.1%\u3001IoU\u63d0\u534742.9%\u3001MSE\u63d0\u534762.5%\uff0c\u5168\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u4ec5\u63a9\u7801\u65b9\u6cd5\u3002", "conclusion": "FireSentry\u57fa\u51c6\u6570\u636e\u96c6\u548cFiReDiff\u8303\u5f0f\u5171\u540c\u63a8\u8fdb\u4e86\u7ec6\u7c92\u5ea6\u91ce\u706b\u9884\u6d4b\u548c\u52a8\u6001\u707e\u5bb3\u6a21\u62df\u9886\u57df\u7684\u53d1\u5c55\uff0c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u65f6\u5e8f\u52a8\u6001\u5efa\u6a21\u5728\u7cbe\u51c6\u706b\u52bf\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u672a\u6765\u9ad8\u7cbe\u5ea6\u5e94\u6025\u54cd\u5e94\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u57fa\u7840\u548c\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2512.03794", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03794", "abs": "https://arxiv.org/abs/2512.03794", "authors": ["Zichuan Lin", "Yicheng Liu", "Yang Yang", "Lvfang Tao", "Deheng Ye"], "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition", "comment": "15 pages, 9 figures", "summary": "Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAdaptVision\uff0c\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u4e3b\u52a8\u89c6\u89c9\u673a\u5236\u7684\u9ad8\u6548\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8303\u5f0f\uff0c\u901a\u8fc7\u4ece\u7c97\u5230\u7ec6\u7684\u81ea\u9002\u5e94\u89c6\u89c9\u6807\u8bb0\u83b7\u53d6\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u89e3\u8026\u56de\u5408\u7b56\u7565\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u6839\u636e\u4efb\u52a1\u9700\u6c42\u52a8\u6001\u8c03\u6574\u89c6\u89c9\u4fe1\u606f\u83b7\u53d6\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u89c6\u89c9\u6807\u8bb0\u5bfc\u81f4\u663e\u8457\u8ba1\u7b97\u5f00\u9500\uff0c\u800c\u73b0\u6709\u9ad8\u6548\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u6bd4\u4f8b\u538b\u7f29\u7f3a\u4e4f\u9002\u5e94\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3VLMs\u80fd\u5426\u81ea\u4e3b\u786e\u5b9a\u6bcf\u4e2a\u6837\u672c\u6240\u9700\u6700\u5c0f\u89c6\u89c9\u6807\u8bb0\u6570\u7684\u95ee\u9898\uff0c\u53d7\u4eba\u7c7b\u4e3b\u52a8\u89c6\u89c9\u673a\u5236\u542f\u53d1\uff0c\u63a2\u7d22\u81ea\u9002\u5e94\u89c6\u89c9\u6807\u8bb0\u83b7\u53d6\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u63d0\u51faAdaptVision\u8303\u5f0f\uff0c\u91c7\u7528\u4ece\u7c97\u5230\u7ec6\u7684\u81ea\u9002\u5e94\u89c6\u89c9\u6807\u8bb0\u83b7\u53d6\u65b9\u6cd5\uff1a\u9996\u5148\u5904\u7406\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u538b\u7f29\u89c6\u89c9\u6807\u8bb0\uff0c\u5fc5\u8981\u65f6\u8c03\u7528\u8fb9\u754c\u6846\u5de5\u5177\u88c1\u526a\u5173\u952e\u533a\u57df\u83b7\u53d6\u989d\u5916\u89c6\u89c9\u4fe1\u606f\u3002\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\uff0c\u6838\u5fc3\u662f\u89e3\u8026\u56de\u5408\u7b56\u7565\u4f18\u5316\uff0c\u5c06\u5b66\u4e60\u76ee\u6807\u5206\u89e3\u4e3a\u5de5\u5177\u5b66\u4e60\u548c\u51c6\u786e\u6027\u6539\u8fdb\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u5e76\u8fdb\u4e00\u6b65\u89e3\u8026\u4f18\u52bf\u4f30\u8ba1\u4e3a\u6bcf\u4e2a\u76ee\u6807\u8ba1\u7b97\u72ec\u7acb\u4f18\u52bf\u3002", "result": "\u5728\u591a\u4e2aVQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cAdaptVision\u5728\u6d88\u8017\u663e\u8457\u5c11\u4e8e\u73b0\u6709\u9ad8\u6548VLM\u65b9\u6cd5\u7684\u89c6\u89c9\u6807\u8bb0\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002\u4e0e\u6807\u51c6GRPO\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u7684\u4f18\u5316\u6548\u679c\u66f4\u4e3a\u6709\u6548\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86VLMs\u80fd\u591f\u81ea\u4e3b\u786e\u5b9a\u6240\u9700\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u9002\u5e94\u89c6\u89c9\u4fe1\u606f\u83b7\u53d6\u8303\u5f0f\u3002\u89e3\u8026\u56de\u5408\u7b56\u7565\u4f18\u5316\u4e3a\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u7684\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.03370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03370", "abs": "https://arxiv.org/abs/2512.03370", "authors": ["Lingjun Zhao", "Yandong Luo", "James Hay", "Lu Gan"], "title": "ShelfGaussian: Shelf-Supervised Open-Vocabulary Gaussian-based 3D Scene Understanding", "comment": null, "summary": "We introduce ShelfGaussian, an open-vocabulary multi-modal Gaussian-based 3D scene understanding framework supervised by off-the-shelf vision foundation models (VFMs). Gaussian-based methods have demonstrated superior performance and computational efficiency across a wide range of scene understanding tasks. However, existing methods either model objects as closed-set semantic Gaussians supervised by annotated 3D labels, neglecting their rendering ability, or learn open-set Gaussian representations via purely 2D self-supervision, leading to degraded geometry and limited to camera-only settings. To fully exploit the potential of Gaussians, we propose a Multi-Modal Gaussian Transformer that enables Gaussians to query features from diverse sensor modalities, and a Shelf-Supervised Learning Paradigm that efficiently optimizes Gaussians with VFM features jointly at 2D image and 3D scene levels. We evaluate ShelfGaussian on various perception and planning tasks. Experiments on Occ3D-nuScenes demonstrate its state-of-the-art zero-shot semantic occupancy prediction performance. ShelfGaussian is further evaluated on an unmanned ground vehicle (UGV) to assess its in the-wild performance across diverse urban scenarios. Project website: https://lunarlab-gatech.github.io/ShelfGaussian/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ShelfGaussian\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f00\u653e\u8bcd\u6c47\u591a\u6a21\u6001\u9ad8\u65af\u5206\u5e03\u76843D\u573a\u666f\u7406\u89e3\u6846\u67b6\uff0c\u5229\u7528\u73b0\u6210\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u76d1\u7763\uff0c\u5b9e\u73b0\u4e86\u5728\u96f6\u6837\u672c\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u4e00\u662f\u5c06\u7269\u4f53\u5efa\u6a21\u4e3a\u5c01\u95ed\u96c6\u8bed\u4e49\u9ad8\u65af\u5206\u5e03\uff0c\u4f9d\u8d56\u4e8e\u6807\u6ce8\u76843D\u6807\u7b7e\u4e14\u5ffd\u7565\u4e86\u6e32\u67d3\u80fd\u529b\uff1b\u4e8c\u662f\u901a\u8fc7\u7eaf2D\u81ea\u76d1\u7763\u5b66\u4e60\u5f00\u653e\u96c6\u9ad8\u65af\u8868\u793a\uff0c\u5bfc\u81f4\u51e0\u4f55\u8d28\u91cf\u4e0b\u964d\u4e14\u4ec5\u9650\u4e8e\u76f8\u673a\u8bbe\u7f6e\u3002\u672c\u7814\u7a76\u65e8\u5728\u5145\u5206\u6316\u6398\u9ad8\u65af\u5206\u5e03\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u9ad8\u65af\u53d8\u6362\u5668\uff0c\u4f7f\u9ad8\u65af\u5206\u5e03\u80fd\u591f\u4ece\u591a\u79cd\u4f20\u611f\u5668\u6a21\u6001\u4e2d\u67e5\u8be2\u7279\u5f81\uff1b\u8bbe\u8ba1\u4e86\u8d27\u67b6\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff0c\u57282D\u56fe\u50cf\u548c3D\u573a\u666f\u5c42\u9762\u8054\u5408\u4f18\u5316\u9ad8\u65af\u5206\u5e03\u4e0e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7279\u5f81\uff1b\u6784\u5efa\u4e86\u5f00\u653e\u8bcd\u6c47\u591a\u6a21\u6001\u9ad8\u65af\u6846\u67b6\uff0c\u5229\u7528\u73b0\u6210\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u76d1\u7763\u3002", "result": "\u5728Occ3D-nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u6027\u80fd\uff1b\u5728\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u4e0a\u8fdb\u884c\u4e86\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u6837\u5316\u57ce\u5e02\u573a\u666f\u4e2d\u7684\u91ce\u5916\u6027\u80fd\uff1b\u5c55\u793a\u4e86\u6846\u67b6\u5728\u5404\u79cd\u611f\u77e5\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u591a\u6a21\u6001\u4f20\u611f\u5668\u4fe1\u606f\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u76d1\u7763\u80fd\u591f\u663e\u8457\u63d0\u5347\u9ad8\u65af\u5206\u5e03\u65b9\u6cd5\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.03445", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03445", "abs": "https://arxiv.org/abs/2512.03445", "authors": ["Xieji Li", "Siyuan Yan", "Yingsheng Liu", "H. Peter Soyer", "Monika Janda", "Victoria Mar", "Zongyuan Ge"], "title": "Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation", "comment": "10 pages. Under Review", "summary": "Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations. However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts. To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining. First, MAGEN enhances data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline. Second, O-MAKE addresses the difficulty of learning from long, unstructured texts by decomposing them into distinct knowledge aspects. This facilitates fine-grained alignment at both global and patch levels, while explicitly modeling medical concept relationships through ontology-guided mechanisms. We validate our framework in the field of dermatology, where comprehensive experiments demonstrate the effectiveness of each component. Our approach achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight datasets. Our code and the augmented dataset Derm1M-AgentAug, comprising over 400k skin-image-text pairs, will be released at https://github.com/SiyuanYan1/Derm1M.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6570\u636e\u751f\u6210\u7cfb\u7edf\u548c\u57fa\u4e8e\u672c\u4f53\u7684\u591a\u7ef4\u5ea6\u77e5\u8bc6\u589e\u5f3a\u9884\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u6570\u636e\u566a\u58f0\u548c\u957f\u6587\u672c\u590d\u6742\u6027\u7684\u6311\u6218\uff0c\u5728\u76ae\u80a4\u75c5\u5b66\u9886\u57df\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u7f51\u7edc\u6536\u96c6\u6570\u636e\u56fa\u6709\u7684\u566a\u58f0\u95ee\u9898\uff0c\u4ee5\u53ca\u975e\u7ed3\u6784\u5316\u957f\u533b\u5b66\u6587\u672c\u7684\u590d\u6742\u6027\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86\u4ece\u5927\u89c4\u6a21\u56fe\u50cf-\u6587\u672c\u5bf9\u4e2d\u5b66\u4e60\u9ad8\u8d28\u91cf\u8868\u793a\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7cbe\u786e\u533b\u5b66\u77e5\u8bc6\u7684\u9886\u57df\u5982\u76ae\u80a4\u75c5\u5b66\u4e2d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u591a\u667a\u80fd\u4f53\u6570\u636e\u751f\u6210\u7cfb\u7edf\u548c\u57fa\u4e8e\u672c\u4f53\u7684\u591a\u7ef4\u5ea6\u77e5\u8bc6\u589e\u5f3a\u9884\u8bad\u7ec3\u6846\u67b6\u3002MAGEN\u7cfb\u7edf\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u8f85\u52a9\u7684\u6807\u6ce8\u548c\u57fa\u4e8e\u68c0\u7d22\u7684\u9a8c\u8bc1\u6d41\u7a0b\u5408\u6210\u77e5\u8bc6\u4e30\u5bcc\u7684\u63cf\u8ff0\u4ee5\u589e\u5f3a\u6570\u636e\u8d28\u91cf\uff1bO-MAKE\u65b9\u6cd5\u5c06\u957f\u975e\u7ed3\u6784\u5316\u6587\u672c\u5206\u89e3\u4e3a\u4e0d\u540c\u7684\u77e5\u8bc6\u7ef4\u5ea6\uff0c\u5b9e\u73b0\u5168\u5c40\u548c\u5c40\u90e8\u5c42\u9762\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u672c\u4f53\u5f15\u5bfc\u673a\u5236\u663e\u5f0f\u5efa\u6a21\u533b\u5b66\u6982\u5ff5\u5173\u7cfb\u3002", "result": "\u5728\u76ae\u80a4\u75c5\u5b66\u9886\u57df\u7684\u7efc\u5408\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u75be\u75c5\u5206\u7c7b\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002\u7814\u7a76\u56e2\u961f\u8fd8\u53d1\u5e03\u4e86\u5305\u542b\u8d85\u8fc740\u4e07\u76ae\u80a4\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u589e\u5f3a\u6570\u636e\u96c6Derm1M-AgentAug\uff0c\u9a8c\u8bc1\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u667a\u80fd\u6570\u636e\u589e\u5f3a\u548c\u672c\u4f53\u77e5\u8bc6\u6574\u5408\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002\u6846\u67b6\u7684\u6210\u529f\u5e94\u7528\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u8d28\u91cf\u6709\u9650\u548c\u6587\u672c\u590d\u6742\u6027\u9ad8\u7684\u573a\u666f\u4e0b\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.03404", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03404", "abs": "https://arxiv.org/abs/2512.03404", "authors": ["Yujian Zhao", "Hankun Liu", "Guanglin Niu"], "title": "MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification", "comment": null, "summary": "Cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) imagery has recently emerged as a critical yet underexplored task in maritime intelligence and surveillance. However, the substantial modality gap between optical and SAR images poses a major challenge for robust identification. To address this issue, we propose MOS, a novel framework designed to mitigate the optical-SAR modality gap and achieve modality-consistent feature learning for optical-SAR cross-modal ship ReID. MOS consists of two core components: (1) Modality-Consistent Representation Learning (MCRL) applies denoise SAR image procession and a class-wise modality alignment loss to align intra-identity feature distributions across modalities. (2) Cross-modal Data Generation and Feature fusion (CDGF) leverages a brownian bridge diffusion model to synthesize cross-modal samples, which are subsequently fused with original features during inference to enhance alignment and discriminability. Extensive experiments on the HOSS ReID dataset demonstrate that MOS significantly surpasses state-of-the-art methods across all evaluation protocols, achieving notable improvements of +3.0%, +6.2%, and +16.4% in R1 accuracy under the ALL to ALL, Optical to SAR, and SAR to Optical settings, respectively. The code and trained models will be released upon publication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMOS\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u6001\u4e00\u81f4\u8868\u793a\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u6570\u636e\u751f\u6210\u4e0e\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u7f13\u89e3\u5149\u5b66\u4e0eSAR\u56fe\u50cf\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u5f02\uff0c\u663e\u8457\u63d0\u5347\u8de8\u6a21\u6001\u8239\u8236\u91cd\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u5149\u5b66\u4e0e\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u56fe\u50cf\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u8239\u8236\u91cd\u8bc6\u522b\u662f\u6d77\u4e8b\u60c5\u62a5\u4e0e\u76d1\u89c6\u4e2d\u7684\u5173\u952e\u4efb\u52a1\uff0c\u4f46\u4e24\u79cd\u6a21\u6001\u95f4\u7684\u663e\u8457\u5dee\u5f02\u6784\u6210\u4e86\u9c81\u68d2\u8bc6\u522b\u7684\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u6b64\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "MOS\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u6a21\u6001\u4e00\u81f4\u8868\u793a\u5b66\u4e60\u901a\u8fc7SAR\u56fe\u50cf\u53bb\u566a\u5904\u7406\u548c\u7c7b\u7ea7\u6a21\u6001\u5bf9\u9f50\u635f\u5931\u6765\u5bf9\u9f50\u8de8\u6a21\u6001\u7684\u7c7b\u5185\u7279\u5f81\u5206\u5e03\uff1b\u8de8\u6a21\u6001\u6570\u636e\u751f\u6210\u4e0e\u7279\u5f81\u878d\u5408\u5229\u7528\u5e03\u6717\u6865\u6269\u6563\u6a21\u578b\u5408\u6210\u8de8\u6a21\u6001\u6837\u672c\uff0c\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u5c06\u5408\u6210\u7279\u5f81\u4e0e\u539f\u59cb\u7279\u5f81\u878d\u5408\u4ee5\u589e\u5f3a\u5bf9\u9f50\u6027\u548c\u5224\u522b\u6027\u3002", "result": "\u5728HOSS ReID\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMOS\u5728\u6240\u6709\u8bc4\u4f30\u534f\u8bae\u4e0b\u5747\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5728ALL to ALL\u3001Optical to SAR\u548cSAR to Optical\u8bbe\u7f6e\u4e0b\u5206\u522b\u5b9e\u73b0\u4e86R1\u51c6\u786e\u7387+3.0%\u3001+6.2%\u548c+16.4%\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u6a21\u6001\u5bf9\u9f50\u548c\u8de8\u6a21\u6001\u6570\u636e\u751f\u6210\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u5149\u5b66-SAR\u6a21\u6001\u5dee\u5f02\uff0c\u4e3a\u8de8\u6a21\u6001\u8239\u8236\u91cd\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u8de8\u6a21\u6001\u7279\u5f81\u589e\u5f3a\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.03454", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03454", "abs": "https://arxiv.org/abs/2512.03454", "authors": ["Haicheng Liao", "Huanming Shen", "Bonan Wang", "Yongkang Li", "Yihong Tang", "Chengyue Wang", "Dingyi Zhuang", "Kehua Chen", "Hai Yang", "Chengzhong Xu", "Zhenning Li"], "title": "Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles", "comment": null, "summary": "Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ThinkDeeper\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u4e16\u754c\u6a21\u578b\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u8fdb\u884c\u524d\u77bb\u6027\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u7cca\u6307\u4ee4\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u5728\u5904\u7406\u6a21\u7cca\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u6307\u4ee4\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u5bf9\u4e09\u7ef4\u7a7a\u95f4\u5173\u7cfb\u548c\u573a\u666f\u6f14\u53d8\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5bf9\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u7684\u51c6\u786e\u7406\u89e3\u548c\u76ee\u6807\u5b9a\u4f4d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ThinkDeeper\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u7a7a\u95f4\u611f\u77e5\u4e16\u754c\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u5f53\u524d\u573a\u666f\u84b8\u998f\u4e3a\u6307\u4ee4\u611f\u77e5\u7684\u6f5c\u5728\u72b6\u6001\uff0c\u5e76\u63a8\u6f14\u51fa\u4e00\u7cfb\u5217\u672a\u6765\u6f5c\u5728\u72b6\u6001\u4ee5\u63d0\u4f9b\u524d\u77bb\u6027\u7ebf\u7d22\u3002\u6b64\u5916\uff0c\u91c7\u7528\u8d85\u56fe\u5f15\u5bfc\u7684\u89e3\u7801\u5668\u5c42\u6b21\u5316\u878d\u5408\u8fd9\u4e9b\u72b6\u6001\u4e0e\u591a\u6a21\u6001\u8f93\u5165\uff0c\u6355\u83b7\u9ad8\u9636\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\u4ee5\u5b9e\u73b0\u9c81\u68d2\u5b9a\u4f4d\u3002\u540c\u65f6\uff0c\u8fd8\u6784\u5efa\u4e86DrivePilot\u6570\u636e\u96c6\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7ba1\u9053\u751f\u6210\u8bed\u4e49\u6807\u6ce8\u3002", "result": "ThinkDeeper\u5728Talk2Car\u6392\u884c\u699c\u4e0a\u6392\u540d\u7b2c\u4e00\uff0c\u5e76\u5728DrivePilot\u3001MoCAD\u548cRefCOCO/+/g\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u5373\u4f7f\u5728\u4ec5\u4f7f\u752850%\u6570\u636e\u8bad\u7ec3\u65f6\u4ecd\u80fd\u4fdd\u6301\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u524d\u77bb\u6027\u7a7a\u95f4\u63a8\u7406\u80fd\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6a21\u7cca\u6307\u4ee4\u548c\u590d\u6742\u573a\u666f\u65f6\u3002\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u7a7a\u95f4\u611f\u77e5\u63a8\u7406\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.03405", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03405", "abs": "https://arxiv.org/abs/2512.03405", "authors": ["Jiangtao Wu", "Shihao Li", "Zhaozhou Bian", "Yuanxing Zhang", "Jialu Chen", "Runzhe Wen", "An Ping", "Yiwen He", "Jiakai Wang", "Jiaheng Liu"], "title": "ViDiC: Video Difference Captioning", "comment": null, "summary": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u89c6\u9891\u5dee\u5f02\u63cf\u8ff0\uff08ViDiC\uff09\u4efb\u52a1\u53ca\u76f8\u5e94\u7684ViDiC-1K\u6570\u636e\u96c6\uff0c\u65e8\u5728\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u5bf9\u4e4b\u95f4\u8fdb\u884c\u7ec6\u7c92\u5ea6\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u63cf\u8ff0\u7684\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u5728\u52a8\u6001\u573a\u666f\u6bd4\u8f83\u611f\u77e5\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u5728\u7406\u89e3\u52a8\u6001\u573a\u666f\u4e4b\u95f4\u7684\u89c6\u89c9\u5dee\u5f02\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u7ec4\u5408\u6027\u3001\u7a7a\u95f4\u6027\u548c\u65f6\u95f4\u6027\u53d8\u5316\u7684\u6bd4\u8f83\u611f\u77e5\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u867d\u7136\u56fe\u50cf\u5dee\u5f02\u63cf\u8ff0\uff08IDC\uff09\u7814\u7a76\u5df2\u4f7f\u6a21\u578b\u80fd\u591f\u63cf\u8ff0\u9759\u6001\u56fe\u50cf\u4e4b\u95f4\u7684\u8bed\u4e49\u53d8\u5316\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u8fd0\u52a8\u8fde\u7eed\u6027\u3001\u4e8b\u4ef6\u6f14\u53d8\u6216\u65f6\u95f4\u4e0a\u7684\u7f16\u8f91\u4e00\u81f4\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u89c6\u9891\u5dee\u5f02\u63cf\u8ff0\uff08ViDiC\uff09\u4efb\u52a1\u5e76\u6784\u5efa\u4e86ViDiC-1K\u6570\u636e\u96c6\uff0c\u5305\u542b1,000\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u89c6\u9891\u5bf9\uff0c\u6807\u6ce8\u4e86\u8d85\u8fc74,000\u4e2a\u6bd4\u8f83\u68c0\u67e5\u9879\uff0c\u6db5\u76d6\u4e3b\u4f53\u3001\u98ce\u683c\u3001\u80cc\u666f\u3001\u6444\u5f71\u3001\u8fd0\u52a8\u3001\u4f4d\u7f6e\u548c\u64ad\u653e\u6280\u672f\u7b49\u4e03\u4e2a\u7c7b\u522b\u3002\u4e3a\u786e\u4fdd\u53ef\u9760\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eLLM-as-a-Judge\u534f\u8bae\u7684\u53cc\u68c0\u67e5\u8868\u6846\u67b6\uff0c\u5206\u522b\u6d4b\u91cf\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u7684\u51c6\u786e\u6027\u3002", "result": "\u572819\u4e2a\u4ee3\u8868\u6027\u591a\u6a21\u6001\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u6bd4\u8f83\u63cf\u8ff0\u548c\u5dee\u5f02\u611f\u77e5\u80fd\u529b\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002ViDiC-1K\u6570\u636e\u96c6\u4f5c\u4e3a\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u8bc4\u4f30\u89c6\u9891\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u52a8\u6001\u573a\u666f\u6bd4\u8f83\u5206\u6790\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "ViDiC-1K\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u667a\u80fd\u4e2d\u7684\u89c6\u9891\u7406\u89e3\u3001\u7f16\u8f91\u611f\u77e5\u548c\u6bd4\u8f83\u63a8\u7406\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u52a8\u6001\u573a\u666f\u6bd4\u8f83\u5206\u6790\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u6709\u671b\u63a8\u52a8\u89c6\u9891\u5dee\u5f02\u7406\u89e3\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2512.03427", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03427", "abs": "https://arxiv.org/abs/2512.03427", "authors": ["Yida Lin", "Bing Xue", "Mengjie Zhang", "Sam Schofield", "Richard Green"], "title": "Generalization Evaluation of Deep Stereo Matching Methods for UAV-Based Forestry Applications", "comment": null, "summary": "Autonomous UAV forestry operations require robust depth estimation methods with strong cross-domain generalization. However, existing evaluations focus on urban and indoor scenarios, leaving a critical gap for specialized vegetation-dense environments. We present the first systematic zero-shot evaluation of eight state-of-the-art stereo methods--RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM (plus baseline methods ACVNet, PSMNet, TCstereo)--spanning iterative refinement, foundation model, and zero-shot adaptation paradigms. All methods are trained exclusively on Scene Flow and evaluated without fine-tuning on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury forestry dataset captured with ZED Mini camera (1920x1080). Performance reveals scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D, 0.83-1.07 px on KITTI; DEFOM: 0.35-4.65 px across benchmarks), while iterative methods maintain cross-domain robustness (IGEV++: 0.36-6.77 px; IGEV: 0.33-21.91 px). Critical finding: RAFT-Stereo exhibits catastrophic ETH3D failure (26.23 px EPE, 98 percent error rate) due to negative disparity predictions, while performing normally on KITTI (0.90-1.11 px). Qualitative evaluation on Canterbury forestry dataset identifies DEFOM as the optimal gold-standard baseline for vegetation depth estimation, exhibiting superior depth smoothness, occlusion handling, and cross-domain consistency compared to IGEV++, despite IGEV++'s finer detail preservation.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5bf9\u516b\u79cd\u6700\u5148\u8fdb\u7684\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u6797\u4e1a\u573a\u666f\u4e2d\u8fdb\u884c\u7cfb\u7edf\u6027\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u57ce\u5e02\u548c\u5ba4\u5185\u73af\u5883\u7684\u7a7a\u767d\uff0c\u5e76\u8bc6\u522b\u51faDEFOM\u4f5c\u4e3a\u690d\u88ab\u5bc6\u96c6\u73af\u5883\u6df1\u5ea6\u4f30\u8ba1\u7684\u6700\u4f73\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u81ea\u4e3b\u65e0\u4eba\u673a\u6797\u4e1a\u4f5c\u4e1a\u9700\u8981\u5177\u6709\u5f3a\u5927\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u7684\u9c81\u68d2\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u4e8e\u57ce\u5e02\u548c\u5ba4\u5185\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u690d\u88ab\u5bc6\u96c6\u4e13\u4e1a\u73af\u5883\u7684\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u8fd9\u6784\u6210\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u5bf9\u516b\u79cd\u6700\u5148\u8fdb\u7684\u7acb\u4f53\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u6db5\u76d6\u8fed\u4ee3\u7ec6\u5316\u3001\u57fa\u7840\u6a21\u578b\u548c\u96f6\u6837\u672c\u9002\u5e94\u8303\u5f0f\uff0c\u5305\u62ecRAFT-Stereo\u3001IGEV\u3001IGEV++\u3001BridgeDepth\u3001StereoAnywhere\u3001DEFOM\u4ee5\u53ca\u57fa\u7ebf\u65b9\u6cd5ACVNet\u3001PSMNet\u548cTCstereo\u3002\u6240\u6709\u65b9\u6cd5\u4ec5\u5728Scene Flow\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728\u56db\u4e2a\u6807\u51c6\u57fa\u51c6\uff08ETH3D\u3001KITTI 2012/2015\u3001Middlebury\uff09\u548c\u4e00\u4e2a\u5305\u542b5,313\u5bf9\u56fe\u50cf\u7684\u65b0\u578bCanterbury\u6797\u4e1a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u65e0\u5fae\u8c03\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u573a\u666f\u4f9d\u8d56\u7684\u6027\u80fd\u6a21\u5f0f\uff1a\u57fa\u7840\u6a21\u578b\u5728\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff08BridgeDepth\u5728ETH3D\u4e0a\u4e3a0.23 px\uff0c\u5728KITTI\u4e0a\u4e3a0.83-1.07 px\uff1bDEFOM\u5728\u5404\u57fa\u51c6\u4e0a\u4e3a0.35-4.65 px\uff09\uff0c\u800c\u8fed\u4ee3\u65b9\u6cd5\u4fdd\u6301\u8de8\u57df\u9c81\u68d2\u6027\uff08IGEV++\u4e3a0.36-6.77 px\uff1bIGEV\u4e3a0.33-21.91 px\uff09\u3002\u5173\u952e\u53d1\u73b0\u662fRAFT-Stereo\u5728ETH3D\u4e0a\u51fa\u73b0\u707e\u96be\u6027\u5931\u8d25\uff0826.23 px EPE\uff0c98%\u9519\u8bef\u7387\uff09\uff0c\u4f46\u5728KITTI\u4e0a\u8868\u73b0\u6b63\u5e38\uff080.90-1.11 px\uff09\u3002\u5728Canterbury\u6797\u4e1a\u6570\u636e\u96c6\u4e0a\uff0cDEFOM\u88ab\u8bc6\u522b\u4e3a\u690d\u88ab\u6df1\u5ea6\u4f30\u8ba1\u7684\u6700\u4f73\u9ec4\u91d1\u6807\u51c6\u57fa\u7ebf\uff0c\u5c55\u73b0\u51fa\u4f18\u4e8eIGEV++\u7684\u6df1\u5ea6\u5e73\u6ed1\u6027\u3001\u906e\u6321\u5904\u7406\u548c\u8de8\u57df\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u7684\u573a\u666f\u4f9d\u8d56\u6027\uff0c\u5f3a\u8c03\u4e86\u5728\u4e13\u4e1a\u9886\u57df\uff08\u5982\u6797\u4e1a\uff09\u8fdb\u884c\u9488\u5bf9\u6027\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002DEFOM\u88ab\u786e\u7acb\u4e3a\u690d\u88ab\u5bc6\u96c6\u73af\u5883\u6df1\u5ea6\u4f30\u8ba1\u7684\u63a8\u8350\u57fa\u51c6\u65b9\u6cd5\uff0c\u4e3a\u81ea\u4e3b\u65e0\u4eba\u673a\u6797\u4e1a\u4f5c\u4e1a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6280\u672f\u6307\u5bfc\u3002\u7814\u7a76\u8fd8\u6307\u51fa\u4e86RAFT-Stereo\u7b49\u65b9\u6cd5\u7684\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u672a\u6765\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.03534", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03534", "abs": "https://arxiv.org/abs/2512.03534", "authors": ["Subin Kim", "Sangwoo Mo", "Mamshad Nayeem Rizve", "Yiran Xu", "Difan Liu", "Jinwoo Shin", "Tobias Hinz"], "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation", "comment": "Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS", "summary": "Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPRIS\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u5730\u4fee\u8ba2\u63d0\u793a\u8bcd\u6765\u6539\u8fdb\u6587\u672c\u5230\u89c6\u89c9\u751f\u6210\u7684\u5bf9\u9f50\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u5143\u7d20\u7ea7\u4e8b\u5b9e\u6821\u6b63\u9a8c\u8bc1\u5668\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u751f\u6210\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6587\u672c\u5230\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u662f\u7528\u6237\u610f\u56fe\u4e0e\u751f\u6210\u89c6\u89c9\u5185\u5bb9\u4e4b\u95f4\u7684\u7cbe\u786e\u5bf9\u9f50\uff0c\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u6269\u5c55\u89c6\u89c9\u751f\u6210\u8fc7\u7a0b\uff08\u5982\u589e\u52a0\u91c7\u6837\u6b65\u6570\u6216\u79cd\u5b50\u6570\uff09\u6765\u89e3\u51b3\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5f88\u5feb\u8fbe\u5230\u8d28\u91cf\u74f6\u9888\uff0c\u56e0\u4e3a\u63d0\u793a\u8bcd\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u56fa\u5b9a\uff0c\u65e0\u6cd5\u6839\u636e\u751f\u6210\u7ed3\u679c\u8fdb\u884c\u9002\u5e94\u6027\u8c03\u6574\u3002", "method": "\u672c\u6587\u63d0\u51faPRIS\u6846\u67b6\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u5730\u4fee\u8ba2\u63d0\u793a\u8bcd\u4ee5\u54cd\u5e94\u6269\u5c55\u7684\u89c6\u89c9\u751f\u6210\u3002\u6838\u5fc3\u601d\u60f3\u662f\u5ba1\u67e5\u751f\u6210\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u8bc6\u522b\u8de8\u89c6\u89c9\u7684\u91cd\u590d\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u76f8\u5e94\u5730\u91cd\u65b0\u8bbe\u8ba1\u63d0\u793a\u8bcd\uff0c\u7136\u540e\u4f7f\u7528\u4fee\u8ba2\u540e\u7684\u63d0\u793a\u8bcd\u91cd\u65b0\u751f\u6210\u89c6\u89c9\u5185\u5bb9\u3002\u4e3a\u63d0\u4f9b\u7cbe\u786e\u7684\u5bf9\u9f50\u53cd\u9988\uff0c\u5f15\u5165\u4e86\u5143\u7d20\u7ea7\u4e8b\u5b9e\u6821\u6b63\u9a8c\u8bc1\u5668\uff0c\u5728\u7ec6\u7c92\u5ea6\u7ea7\u522b\u8bc4\u4f30\u63d0\u793a\u5c5e\u6027\u4e0e\u751f\u6210\u89c6\u89c9\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u76f8\u6bd4\u6574\u4f53\u8bc4\u4f30\u65b9\u6cd5\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u548c\u6587\u672c\u5230\u89c6\u9891\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u5728VBench 2.0\u4e0a\u5b9e\u73b0\u4e8615%\u7684\u6027\u80fd\u589e\u76ca\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u8054\u5408\u6269\u5c55\u63d0\u793a\u8bcd\u548c\u89c6\u89c9\u5185\u5bb9\u662f\u5145\u5206\u5229\u7528\u63a8\u7406\u65f6\u6269\u5c55\u5b9a\u5f8b\u7684\u5173\u952e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8054\u5408\u6269\u5c55\u63d0\u793a\u8bcd\u548c\u89c6\u89c9\u5185\u5bb9\u5bf9\u4e8e\u5b9e\u73b0\u6587\u672c\u5230\u89c6\u89c9\u751f\u6210\u7684\u7cbe\u786e\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002PRIS\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u4fee\u8ba2\u548c\u7ec6\u7c92\u5ea6\u9a8c\u8bc1\u673a\u5236\uff0c\u4e3a\u89e3\u51b3\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u63d0\u793a\u8bcd\u52a8\u6001\u8c03\u6574\u5728\u63d0\u5347\u751f\u6210\u8d28\u91cf\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.03430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03430", "abs": "https://arxiv.org/abs/2512.03430", "authors": ["Yuzhen Hu", "Biplab Banerjee", "Saurabh Prasad"], "title": "Label-Efficient Hyperspectral Image Classification via Spectral FiLM Modulation of Low-Level Pretrained Diffusion Features", "comment": "Accepted to the ICML 2025 TerraBytes Workshop (June 9, 2025)", "summary": "Hyperspectral imaging (HSI) enables detailed land cover classification, yet low spatial resolution and sparse annotations pose significant challenges. We present a label-efficient framework that leverages spatial features from a frozen diffusion model pretrained on natural images. Our approach extracts low-level representations from high-resolution decoder layers at early denoising timesteps, which transfer effectively to the low-texture structure of HSI. To integrate spectral and spatial information, we introduce a lightweight FiLM-based fusion module that adaptively modulates frozen spatial features using spectral cues, enabling robust multimodal learning under sparse supervision. Experiments on two recent hyperspectral datasets demonstrate that our method outperforms state-of-the-art approaches using only the provided sparse training labels. Ablation studies further highlight the benefits of diffusion-derived features and spectral-aware fusion. Overall, our results indicate that pretrained diffusion models can support domain-agnostic, label-efficient representation learning for remote sensing and broader scientific imaging tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u7a7a\u95f4\u7279\u5f81\u8fdb\u884c\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7FiLM\u878d\u5408\u6a21\u5757\u6574\u5408\u5149\u8c31\u4e0e\u7a7a\u95f4\u4fe1\u606f\uff0c\u5728\u7a00\u758f\u6807\u6ce8\u4e0b\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u9ad8\u5149\u8c31\u6210\u50cf\u867d\u7136\u80fd\u5b9e\u73b0\u8be6\u7ec6\u7684\u5730\u7269\u5206\u7c7b\uff0c\u4f46\u9762\u4e34\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\u548c\u6807\u6ce8\u7a00\u758f\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u9650\u6807\u6ce8\u4e0b\u96be\u4ee5\u6709\u6548\u6574\u5408\u5149\u8c31\u4e0e\u7a7a\u95f4\u4fe1\u606f\uff0c\u9700\u8981\u5f00\u53d1\u6807\u7b7e\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684\u51bb\u7ed3\u6269\u6563\u6a21\u578b\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\uff0c\u4ece\u65e9\u671f\u53bb\u566a\u65f6\u95f4\u6b65\u7684\u9ad8\u5206\u8fa8\u7387\u89e3\u7801\u5668\u5c42\u83b7\u53d6\u4f4e\u5c42\u8868\u793a\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7FiLM\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u5149\u8c31\u7ebf\u7d22\u81ea\u9002\u5e94\u8c03\u5236\u51bb\u7ed3\u7684\u7a7a\u95f4\u7279\u5f81\uff0c\u5b9e\u73b0\u7a00\u758f\u76d1\u7763\u4e0b\u7684\u9c81\u68d2\u591a\u6a21\u6001\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u8fd1\u671f\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u63d0\u4f9b\u7684\u7a00\u758f\u8bad\u7ec3\u6807\u7b7e\u5c31\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6269\u6563\u6a21\u578b\u7279\u5f81\u548c\u5149\u8c31\u611f\u77e5\u878d\u5408\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u5728\u6807\u7b7e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u80fd\u591f\u652f\u6301\u9886\u57df\u65e0\u5173\u7684\u6807\u7b7e\u9ad8\u6548\u8868\u793a\u5b66\u4e60\uff0c\u4e3a\u9065\u611f\u53ca\u66f4\u5e7f\u6cdb\u7684\u79d1\u5b66\u6210\u50cf\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u8bc1\u660e\u4e86\u51bb\u7ed3\u6269\u6563\u7279\u5f81\u5728\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002"}}
{"id": "2512.03540", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03540", "abs": "https://arxiv.org/abs/2512.03540", "authors": ["Ruoxuan Zhang", "Bin Wen", "Hongxia Xie", "Yi Yao", "Songhan Zuo", "Jian-Yu Jiang-Lin", "Hong-Han Shuai", "Wen-Huang Cheng"], "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation", "comment": "Accepted by ACM Multimedia 2025", "summary": "Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CookAnything\uff0c\u4e00\u4e2a\u7075\u6d3b\u7684\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4efb\u610f\u957f\u5ea6\u7684\u6587\u672c\u70f9\u996a\u6307\u4ee4\u751f\u6210\u8fde\u8d2f\u3001\u8bed\u4e49\u5206\u660e\u7684\u56fe\u50cf\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u7ed3\u6784\u5316\u591a\u6b65\u9aa4\u573a\u666f\u548c\u53ef\u53d8\u6307\u4ee4\u957f\u5ea6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u7ed3\u6784\u5316\u591a\u6b65\u9aa4\u573a\u666f\uff08\u5982\u83dc\u8c31\u56fe\u89e3\uff09\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u5f53\u524d\u83dc\u8c31\u56fe\u89e3\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u83dc\u8c31\u957f\u5ea6\u7684\u81ea\u7136\u53d8\u5316\uff0c\u65e0\u8bba\u5b9e\u9645\u6307\u4ee4\u7ed3\u6784\u5982\u4f55\u90fd\u751f\u6210\u56fa\u5b9a\u6570\u91cf\u7684\u56fe\u50cf\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u7a0b\u5e8f\u6027\u5185\u5bb9\u521b\u4f5c\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u6b65\u9aa4\u7ea7\u533a\u57df\u63a7\u5236\uff08SRC\uff09\u5728\u5355\u4e2a\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5c06\u6587\u672c\u6b65\u9aa4\u4e0e\u5bf9\u5e94\u56fe\u50cf\u533a\u57df\u5bf9\u9f50\uff1b\u7075\u6d3b\u7684RoPE\u4f5c\u4e3a\u6b65\u9aa4\u611f\u77e5\u7684\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\uff0c\u589e\u5f3a\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u7a7a\u95f4\u591a\u6837\u6027\uff1b\u8de8\u6b65\u9aa4\u4e00\u81f4\u6027\u63a7\u5236\uff08CSCC\uff09\u4fdd\u6301\u6b65\u9aa4\u95f4\u7ec6\u7c92\u5ea6\u6210\u5206\u4e00\u81f4\u6027\u3002", "result": "\u5728\u83dc\u8c31\u56fe\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCookAnything\u5728\u57fa\u4e8e\u8bad\u7ec3\u548c\u65e0\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u652f\u6301\u590d\u6742\u591a\u6b65\u9aa4\u6307\u4ee4\u7684\u53ef\u6269\u5c55\u9ad8\u8d28\u91cf\u89c6\u89c9\u5408\u6210\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u5728\u7a0b\u5e8f\u6027\u5185\u5bb9\u521b\u4f5c\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6559\u5b66\u5a92\u4f53\u548c\u7ed3\u6784\u5316\u89c6\u89c9\u5185\u5bb9\u751f\u6210\u65b9\u9762\uff0c\u4e3a\u5904\u7406\u53ef\u53d8\u957f\u5ea6\u591a\u6b65\u9aa4\u6307\u4ee4\u7684\u89c6\u89c9\u5408\u6210\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03542", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03542", "abs": "https://arxiv.org/abs/2512.03542", "authors": ["Nan Sun", "Zhenyu Zhang", "Xixun Lin", "Kun Wang", "Yanmin Shang", "Naibin Gu", "Shuohuan Wang", "Yu Sun", "Hua Wu", "Haifeng Wang", "Yanan Cao"], "title": "V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on \"how to intervene\" but overlooking the prerequisite \"when to intervene\", which leads to the \"over-intervention\" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faV-ITI\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9\u63a8\u7406\u65f6\u5e72\u9884\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u5934\u7ea7\u6fc0\u6d3b\u6a21\u5f0f\u8bc6\u522b\u89c6\u89c9\u5ffd\u89c6\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u8fdb\u884c\u5e72\u9884\uff0c\u6709\u6548\u7f13\u89e3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f17\u591a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u751f\u6210\u4e0e\u8f93\u5165\u89c6\u89c9\u5185\u5bb9\u4e0d\u4e00\u81f4\u7684\u8f93\u51fa\uff0c\u8fd9\u5728\u7cbe\u5ea6\u654f\u611f\u9886\u57df\u4e25\u91cd\u635f\u5bb3\u53ef\u9760\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u5e72\u9884\u6ce8\u610f\u529b\u5206\u6570\u6216\u8f93\u51falogits\u6765\u7f13\u89e3\u5e7b\u89c9\uff0c\u4f46\u4e3b\u8981\u5173\u6ce8\"\u5982\u4f55\u5e72\u9884\"\u800c\u5ffd\u7565\u4e86\"\u4f55\u65f6\u5e72\u9884\"\u8fd9\u4e00\u524d\u63d0\uff0c\u5bfc\u81f4\"\u8fc7\u5ea6\u5e72\u9884\"\u95ee\u9898\uff0c\u8fdb\u800c\u5f15\u5165\u65b0\u7684\u5e7b\u89c9\u548c\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u672c\u6587\u9996\u5148\u7814\u7a76\u4e86\u89c6\u89c9\u5ffd\u89c6\u673a\u5236\uff0c\u53d1\u73b0\u53ef\u4ee5\u901a\u8fc7MLLMs\u4e2d\u7684\u5934\u7ea7\u6fc0\u6d3b\u6a21\u5f0f\u51c6\u786e\u68c0\u6d4b\u89c6\u89c9\u5ffd\u89c6\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86V-ITI\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u89c6\u89c9\u5ffd\u89c6\u68c0\u6d4b\u5668\u901a\u8fc7\u5934\u7ea7\u5224\u522b\u6027\u63a2\u9488\u8bc6\u522b\u89c6\u89c9\u5ffd\u89c6\uff1b\u89c6\u89c9\u56de\u5fc6\u5e72\u9884\u5668\u4ec5\u5728\u68c0\u6d4b\u5230\u89c6\u89c9\u5ffd\u89c6\u65f6\uff0c\u4f7f\u7528\u9884\u5b58\u50a8\u7684\u89c6\u89c9\u6fc0\u6d3b\u4fe1\u606f\u6765\u8c03\u5236\u6fc0\u6d3b\uff0c\u5b9e\u73b0\u7cbe\u51c6\u5e72\u9884\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e0d\u540cMLLM\u5bb6\u65cf\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cV-ITI\u80fd\u591f\u6301\u7eed\u7f13\u89e3\u89c6\u89c9\u76f8\u5173\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u4efb\u52a1\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u89c9\u5ffd\u89c6\u53ef\u4ee5\u901a\u8fc7\u5934\u7ea7\u6fc0\u6d3b\u6a21\u5f0f\u51c6\u786e\u68c0\u6d4b\uff0c\u5e76\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u7684\u63a8\u7406\u65f6\u5e72\u9884\u6846\u67b6V-ITI\uff0c\u901a\u8fc7\"\u4f55\u65f6\u5e72\u9884\"\u7684\u7cbe\u786e\u63a7\u5236\u89e3\u51b3\u4e86\u8fc7\u5ea6\u5e72\u9884\u95ee\u9898\uff0c\u4e3a\u7f13\u89e3MLLMs\u5e7b\u89c9\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u53ef\u9760\u6027\u3002"}}
{"id": "2512.03449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03449", "abs": "https://arxiv.org/abs/2512.03449", "authors": ["Tongxu Zhang"], "title": "LM-CartSeg: Automated Segmentation of Lateral and Medial Cartilage and Subchondral Bone for Radiomics Analysis", "comment": null, "summary": "Background and Objective: Radiomics of knee MRI requires robust, anatomically meaningful regions of interest (ROIs) that jointly capture cartilage and subchondral bone. Most existing work relies on manual ROIs and rarely reports quality control (QC). We present LM-CartSeg, a fully automatic pipeline for cartilage/bone segmentation, geometric lateral/medial (L/M) compartmentalisation and radiomics analysis. Methods: Two 3D nnU-Net models were trained on SKM-TEA (138 knees) and OAIZIB-CM (404 knees). At test time, zero-shot predictions were fused and refined by simple geometric rules: connected-component cleaning, construction of 10 mm subchondral bone bands in physical space, and a data-driven tibial L/M split based on PCA and k-means. Segmentation was evaluated on an OAIZIB-CM test set (103 knees) and on SKI-10 (100 knees). QC used volume and thickness signatures. From 10 ROIs we extracted 4 650 non-shape radiomic features to study inter-compartment similarity, dependence on ROI size, and OA vs. non-OA classification on OAIZIB-CM Results: Post-processing improved macro ASSD on OAIZIB-CM from 2.63 to 0.36 mm and HD95 from 25.2 to 3.35 mm, with DSC 0.91; zero-shot DSC on SKI-10 was 0.80. The geometric L/M rule produced stable compartments across datasets, whereas a direct L/M nnU-Net showed domain-dependent side swaps. Only 6 to 12 percent of features per ROI were strongly correlated with volume or thickness. Radiomics-based models models restricted to size-linked features. Conclusions: LM-CartSeg yields automatic, QCd ROIs and radiomic features that carry discriminative information beyond simple morphometry, providing a practical foundation for multi-centre knee OA radiomics studies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faLM-CartSeg\uff0c\u4e00\u79cd\u5168\u81ea\u52a8\u7684\u819d\u5173\u8282MRI\u5206\u5272\u4e0e\u5f71\u50cf\u7ec4\u5b66\u5206\u6790\u6d41\u7a0b\uff0c\u901a\u8fc7\u51e0\u4f55\u540e\u5904\u7406\u89c4\u5219\u548c\u96f6\u6837\u672c\u9884\u6d4b\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u8f6f\u9aa8/\u9aa8\u5206\u5272\u4e0e\u5185\u5916\u4fa7\u5206\u5ba4\uff0c\u4e3a\u591a\u4e2d\u5fc3\u9aa8\u5173\u8282\u708e\u5f71\u50cf\u7ec4\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002", "motivation": "\u819d\u5173\u8282MRI\u5f71\u50cf\u7ec4\u5b66\u7814\u7a76\u9700\u8981\u7a33\u5065\u4e14\u5177\u6709\u89e3\u5256\u5b66\u610f\u4e49\u7684\u611f\u5174\u8da3\u533a\u57df\u6765\u540c\u65f6\u6355\u83b7\u8f6f\u9aa8\u548c\u8f6f\u9aa8\u4e0b\u9aa8\uff0c\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u624b\u52a8\u6807\u6ce8\u4e14\u7f3a\u4e4f\u8d28\u91cf\u63a7\u5236\u62a5\u544a\uff0c\u8fd9\u9650\u5236\u4e86\u591a\u4e2d\u5fc3\u7814\u7a76\u7684\u53ef\u9760\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u4e24\u4e2a3D nnU-Net\u6a21\u578b\u5206\u522b\u5728SKM-TEA\u548cOAIZIB-CM\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6d4b\u8bd5\u65f6\u878d\u5408\u96f6\u6837\u672c\u9884\u6d4b\u5e76\u901a\u8fc7\u51e0\u4f55\u89c4\u5219\u8fdb\u884c\u540e\u5904\u7406\uff0c\u5305\u62ec\u8fde\u901a\u5206\u91cf\u6e05\u6d17\u3001\u5728\u7269\u7406\u7a7a\u95f4\u6784\u5efa10\u6beb\u7c73\u8f6f\u9aa8\u4e0b\u9aa8\u5e26\uff0c\u4ee5\u53ca\u57fa\u4e8ePCA\u548ck-means\u7684\u6570\u636e\u9a71\u52a8\u80eb\u9aa8\u5185\u5916\u4fa7\u5206\u5272\u3002", "result": "\u540e\u5904\u7406\u663e\u8457\u6539\u5584\u4e86\u5206\u5272\u6027\u80fd\uff0c\u5728OAIZIB-CM\u6d4b\u8bd5\u96c6\u4e0a\u5b8f\u89c2ASSD\u4ece2.63\u6beb\u7c73\u964d\u81f30.36\u6beb\u7c73\uff0cHD95\u4ece25.2\u6beb\u7c73\u964d\u81f33.35\u6beb\u7c73\uff0cDSC\u8fbe\u52300.91\uff1b\u5728SKI-10\u6570\u636e\u96c6\u4e0a\u96f6\u6837\u672cDSC\u4e3a0.80\u3002\u51e0\u4f55\u5185\u5916\u4fa7\u5206\u5272\u89c4\u5219\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u4ea7\u751f\u7a33\u5b9a\u5206\u5ba4\uff0c\u800c\u76f4\u63a5\u4f7f\u7528\u5185\u5916\u4fa7nnU-Net\u6a21\u578b\u5219\u51fa\u73b0\u57df\u4f9d\u8d56\u7684\u4fa7\u5411\u4ea4\u6362\u95ee\u9898\u3002", "conclusion": "LM-CartSeg\u80fd\u591f\u751f\u6210\u81ea\u52a8\u5316\u7684\u8d28\u91cf\u63a7\u5236\u611f\u5174\u8da3\u533a\u57df\u548c\u5f71\u50cf\u7ec4\u5b66\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u643a\u5e26\u4e86\u8d85\u8d8a\u7b80\u5355\u5f62\u6001\u6d4b\u91cf\u7684\u5224\u522b\u4fe1\u606f\uff0c\u4e3a\u591a\u4e2d\u5fc3\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\u5f71\u50cf\u7ec4\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u540c\u65f6\u53d1\u73b0\u4ec56-12%\u7684\u7279\u5f81\u4e0e\u4f53\u79ef\u6216\u539a\u5ea6\u5f3a\u76f8\u5173\u3002"}}
{"id": "2512.03553", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03553", "abs": "https://arxiv.org/abs/2512.03553", "authors": ["Wei Chee Yew", "Hailun Xu", "Sanjay Saha", "Xiaotian Fan", "Hiok Hian Ong", "David Yuchen Wang", "Kanchan Sarkar", "Zhenheng Yang", "Danhui Guan"], "title": "Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching", "comment": "Accepted at KDD 2026", "summary": "Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u7528\u6237\u751f\u6210\u89c6\u9891\u5e73\u53f0\u7684\u6df7\u5408\u5185\u5bb9\u5ba1\u6838\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u76d1\u7763\u5206\u7c7b\u548c\u57fa\u4e8e\u53c2\u8003\u7684\u76f8\u4f3c\u6027\u5339\u914d\uff0c\u4ee5\u5e94\u5bf9\u76f4\u64ad\u73af\u5883\u4e2d\u591a\u6a21\u6001\u3001\u53ca\u65f6\u4e14\u9700\u9002\u5e94\u4e0d\u65ad\u6f14\u5316\u7684\u8fdd\u89c4\u5185\u5bb9\u68c0\u6d4b\u6311\u6218\u3002", "motivation": "\u5927\u89c4\u6a21\u7528\u6237\u751f\u6210\u89c6\u9891\u5e73\u53f0\u7684\u5185\u5bb9\u5ba1\u6838\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u5c24\u5176\u5728\u76f4\u64ad\u73af\u5883\u4e2d\u9700\u8981\u5b9e\u73b0\u53ca\u65f6\u3001\u591a\u6a21\u6001\u4e14\u5bf9\u4e0d\u65ad\u6f14\u5316\u7684\u8fdd\u89c4\u5185\u5bb9\u5177\u6709\u9c81\u68d2\u6027\u7684\u68c0\u6d4b\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u65b0\u578b\u6216\u5fae\u5999\u7684\u8fdd\u89c4\u6848\u4f8b\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5ba1\u6838\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5206\u7c7b\u7ba1\u9053\u5904\u7406\u5df2\u77e5\u8fdd\u89c4\u5185\u5bb9\uff0c\u4ee5\u53ca\u57fa\u4e8e\u53c2\u8003\u7684\u76f8\u4f3c\u6027\u5339\u914d\u7ba1\u9053\u68c0\u6d4b\u65b0\u578b\u6216\u5fae\u5999\u6848\u4f8b\uff0c\u591a\u6a21\u6001\u8f93\u5165\u901a\u8fc7\u4e24\u4e2a\u7ba1\u9053\u5904\u7406\uff0c\u5e76\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u77e5\u8bc6\u84b8\u998f\u5230\u6bcf\u4e2a\u7ba1\u9053\u4e2d\u4ee5\u63d0\u5347\u51c6\u786e\u6027\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8f7b\u91cf\u7ea7\u3002", "result": "\u5728\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u5206\u7c7b\u7ba1\u9053\u572880%\u7cbe\u786e\u7387\u4e0b\u8fbe\u523067%\u53ec\u56de\u7387\uff0c\u76f8\u4f3c\u6027\u7ba1\u9053\u572880%\u7cbe\u786e\u7387\u4e0b\u8fbe\u523076%\u53ec\u56de\u7387\uff0c\u5927\u89c4\u6a21A/B\u6d4b\u8bd5\u663e\u793a\u7528\u6237\u89c2\u770b\u4e0d\u826f\u76f4\u64ad\u7684\u6bd4\u4f8b\u51cf\u5c11\u4e866-8%\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u591a\u6a21\u6001\u5185\u5bb9\u6cbb\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u663e\u6027\u8fdd\u89c4\u548c\u65b0\u5174\u5bf9\u6297\u884c\u4e3a\uff0c\u4e3a\u5927\u89c4\u6a21\u89c6\u9891\u5e73\u53f0\u63d0\u4f9b\u4e86\u517c\u987e\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u7cfb\u7edf\u6548\u7387\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03453", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03453", "abs": "https://arxiv.org/abs/2512.03453", "authors": ["Yunpeng Bai", "Shaoheng Fang", "Chaohui Yu", "Fan Wang", "Qixing Huang"], "title": "GeoVideo: Introducing Geometric Regularization into Video Generation Model", "comment": "Project Page: https://geovideo.github.io/GeoVideo/", "summary": "Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6df1\u5ea6\u9884\u6d4b\u589e\u5f3a\u89c6\u9891\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0c\u5c06\u591a\u89c6\u89d2\u51e0\u4f55\u635f\u5931\u5f15\u5165\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u7684\u65f6\u7a7a\u8fde\u8d2f\u6027\u548c\u7ed3\u6784\u5408\u7406\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u57282D\u50cf\u7d20\u7a7a\u95f4\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u660e\u786e\u76843D\u7ed3\u6784\u5efa\u6a21\u673a\u5236\uff0c\u5bfc\u81f4\u65f6\u95f4\u51e0\u4f55\u4e0d\u4e00\u81f4\u3001\u8fd0\u52a8\u4e0d\u81ea\u7136\u548c\u7ed3\u6784\u4f2a\u5f71\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5c06\u51e0\u4f55\u6b63\u5219\u5316\u5f15\u5165\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u4ee5\u63d0\u5347\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e3a\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6dfb\u52a0\u9010\u5e27\u6df1\u5ea6\u9884\u6d4b\u6765\u5b9e\u73b0\u51e0\u4f55\u6b63\u5219\u5316\uff0c\u91c7\u7528\u6df1\u5ea6\u4f5c\u4e3a\u51e0\u4f55\u8868\u793a\u56e0\u5176\u4e0e\u56fe\u50cf\u7f16\u7801\u5668\u7684\u517c\u5bb9\u6027\uff0c\u5e76\u63d0\u51fa\u591a\u89c6\u89d2\u51e0\u4f55\u635f\u5931\u5728\u5171\u4eab3D\u5750\u6807\u7cfb\u4e2d\u5bf9\u9f50\u8de8\u5e27\u6df1\u5ea6\u56fe\u4ee5\u589e\u5f3a\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u80fd\u4ea7\u751f\u663e\u8457\u66f4\u7a33\u5b9a\u548c\u51e0\u4f55\u4e00\u81f4\u7684\u7ed3\u679c\uff0c\u5728\u65f6\u7a7a\u8fde\u8d2f\u6027\u3001\u5f62\u72b6\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u5747\u6709\u660e\u663e\u6539\u5584\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5f25\u5408\u4e86\u5916\u89c2\u751f\u6210\u4e0e3D\u7ed3\u6784\u5efa\u6a21\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u51e0\u4f55\u6b63\u5219\u5316\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u8868\u793a\u5728\u63d0\u5347\u751f\u6210\u89c6\u9891\u8d28\u91cf\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7ed3\u5408\u66f4\u4e30\u5bcc3D\u5148\u9a8c\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.03666", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03666", "abs": "https://arxiv.org/abs/2512.03666", "authors": ["Qi'ao Xu", "Tianwen Qian", "Yuqian Fu", "Kailing Li", "Yang Jiao", "Jiacheng Zhang", "Xiaoling Wang", "Liang He"], "title": "ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos", "comment": "26 pages", "summary": "A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \\textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \\textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \\textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \\textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \\href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ToG-Bench\uff0c\u9996\u4e2a\u9762\u5411\u4efb\u52a1\u7684\u65f6\u7a7a\u89c6\u9891\u5b9a\u4f4d\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8c61\u5b9a\u4f4d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u63cf\u8ff0\u6027\u6307\u4ee4\u800c\u7f3a\u4e4f\u4efb\u52a1\u63a8\u7406\u80fd\u529b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65f6\u7a7a\u89c6\u9891\u5b9a\u4f4d\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u5bf9\u8c61\u4e2d\u5fc3\u548c\u63cf\u8ff0\u6027\u6307\u4ee4\uff0c\u5ffd\u89c6\u4e86\u4efb\u52a1\u5bfc\u5411\u63a8\u7406\u5bf9\u4e8e\u5177\u8eab\u667a\u80fd\u4f53\u5b9e\u73b0\u76ee\u6807\u5bfc\u5411\u4ea4\u4e92\u7684\u5173\u952e\u4f5c\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u5177\u8eab\u573a\u666f\u4e2d\u7684\u5e94\u7528\u80fd\u529b\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u57fa\u4e8eScanNet\u89c6\u9891\u7684ToG-Bench\u57fa\u51c6\uff0c\u5305\u542b100\u4e2a\u6807\u6ce8\u7247\u6bb5\u548c2,704\u4e2a\u4efb\u52a1\u5bfc\u5411\u5b9a\u4f4d\u6307\u4ee4\uff0c\u91c7\u7528\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u6807\u6ce8\u548c\u4eba\u5de5\u7ec6\u5316\u7684\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9488\u5bf9\u591a\u5bf9\u8c61\u548c\u663e\u9690\u5f0f\u5bf9\u8c61\u5b9a\u4f4d\u7684\u4efb\u52a1\u7ea7\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u4e03\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u4efb\u52a1\u5bfc\u5411\u65f6\u7a7a\u89c6\u9891\u5b9a\u4f4d\u5b58\u5728\u56fa\u6709\u6311\u6218\uff0c\u5728\u663e\u9690\u5f0f\u548c\u591a\u5bf9\u8c61\u5b9a\u4f4d\u65b9\u9762\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u63ed\u793a\u4e86\u611f\u77e5\u4e0e\u4ea4\u4e92\u5728\u5177\u8eab\u573a\u666f\u4e2d\u7684\u878d\u5408\u96be\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u4efb\u52a1\u5bfc\u5411\u63a8\u7406\u5728\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e3a\u8fde\u63a5\u89c6\u89c9\u611f\u77e5\u4e0e\u7269\u7406\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7406\u89e3\u4e0a\u4e0b\u6587\u548c\u4efb\u52a1\u610f\u56fe\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.03848", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03848", "abs": "https://arxiv.org/abs/2512.03848", "authors": ["Hania Ghouse", "Maryam Alsharqi", "Farhad R. Nezami", "Muzammil Behzad"], "title": "PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation", "comment": null, "summary": "Cardiac image analysis remains fragmented across tasks: anatomical segmentation, disease classification, and grounded clinical report generation are typically handled by separate networks trained under different data regimes. No existing framework unifies these objectives within a single architecture while retaining generalization across imaging modalities and datasets. We introduce PULSE, a multi-task vision-language framework built on self-supervised representations and optimized through a composite supervision strategy that balances region overlap learning, pixel wise classification fidelity, and boundary aware IoU refinement. A multi-scale token reconstruction decoder enables anatomical segmentation, while shared global representations support disease classification and clinically grounded text output allowing the model to transition from pixels to structures and finally clinical reasoning within one architecture. Unlike prior task-specific pipelines, PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision. This moves the field closer to a scalable, foundation style cardiac analysis framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PULSE\uff0c\u4e00\u79cd\u591a\u4efb\u52a1\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\uff0c\u65e8\u5728\u7edf\u4e00\u5fc3\u810f\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u89e3\u5256\u5206\u5272\u3001\u75be\u75c5\u5206\u7c7b\u548c\u4e34\u5e8a\u62a5\u544a\u751f\u6210\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u8868\u793a\u548c\u590d\u5408\u76d1\u7763\u7b56\u7565\u5b9e\u73b0\u8de8\u6a21\u6001\u548c\u8de8\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5fc3\u810f\u56fe\u50cf\u5206\u6790\u76ee\u524d\u5b58\u5728\u4efb\u52a1\u788e\u7247\u5316\u95ee\u9898\uff0c\u89e3\u5256\u5206\u5272\u3001\u75be\u75c5\u5206\u7c7b\u548c\u4e34\u5e8a\u62a5\u544a\u751f\u6210\u901a\u5e38\u7531\u5728\u4e0d\u540c\u6570\u636e\u673a\u5236\u4e0b\u8bad\u7ec3\u7684\u72ec\u7acb\u7f51\u7edc\u5904\u7406\uff0c\u7f3a\u4e4f\u80fd\u591f\u5c06\u8fd9\u4e9b\u76ee\u6807\u7edf\u4e00\u5728\u5355\u4e00\u67b6\u6784\u4e2d\u5e76\u4fdd\u6301\u8de8\u6210\u50cf\u6a21\u6001\u548c\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u7684\u73b0\u6709\u6846\u67b6\u3002", "method": "PULSE\u6846\u67b6\u57fa\u4e8e\u81ea\u76d1\u7763\u8868\u793a\u6784\u5efa\uff0c\u91c7\u7528\u590d\u5408\u76d1\u7763\u7b56\u7565\u5e73\u8861\u533a\u57df\u91cd\u53e0\u5b66\u4e60\u3001\u50cf\u7d20\u7ea7\u5206\u7c7b\u4fdd\u771f\u5ea6\u548c\u8fb9\u754c\u611f\u77e5IoU\u7ec6\u5316\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u4ee4\u724c\u91cd\u5efa\u89e3\u7801\u5668\u5b9e\u73b0\u89e3\u5256\u5206\u5272\uff0c\u5171\u4eab\u7684\u5168\u5c40\u8868\u793a\u652f\u6301\u75be\u75c5\u5206\u7c7b\u548c\u4e34\u5e8a\u6587\u672c\u8f93\u51fa\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u5355\u4e00\u67b6\u6784\u5185\u4ece\u50cf\u7d20\u5230\u7ed3\u6784\u518d\u5230\u4e34\u5e8a\u63a8\u7406\u8fdb\u884c\u8fc7\u6e21\u3002", "result": "\u4e0e\u5148\u524d\u4efb\u52a1\u7279\u5b9a\u7ba1\u9053\u4e0d\u540c\uff0cPULSE\u80fd\u591f\u5b66\u4e60\u4efb\u52a1\u4e0d\u53d8\u7684\u5fc3\u810f\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u8de8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u80fd\u591f\u4ee5\u6700\u5c0f\u76d1\u7763\u9002\u5e94\u65b0\u7684\u6210\u50cf\u6a21\u6001\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u57fa\u7840\u98ce\u683c\u5fc3\u810f\u5206\u6790\u6846\u67b6\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u5fc3\u810f\u56fe\u50cf\u5206\u6790\u9886\u57df\u5411\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u6846\u67b6\u53d1\u5c55\uff0c\u901a\u8fc7\u5355\u4e00\u67b6\u6784\u6574\u5408\u591a\u4e2a\u5206\u6790\u4efb\u52a1\u5e76\u5b9e\u73b0\u8de8\u6a21\u6001\u6cdb\u5316\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u4e00\u81f4\u7684\u5206\u6790\u5de5\u5177\uff0c\u51cf\u5c11\u4e86\u4efb\u52a1\u7279\u5b9a\u7ba1\u9053\u5e26\u6765\u7684\u590d\u6742\u6027\u548c\u6570\u636e\u9700\u6c42\u3002"}}
{"id": "2512.03474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03474", "abs": "https://arxiv.org/abs/2512.03474", "authors": ["Wenliang Guo", "Yujiang Pu", "Yu Kong"], "title": "Procedural Mistake Detection via Action Effect Modeling", "comment": null, "summary": "Mistake detection in procedural tasks is essential for building intelligent systems that support learning and task execution. Existing approaches primarily analyze how an action is performed, while overlooking what it produces, i.e., the \\textbf{action effect}. Yet many errors manifest not in the execution itself but in the resulting outcome, such as an unintended object state or incorrect spatial arrangement. To address this gap, we propose Action Effect Modeling (AEM), a unified framework that jointly captures action execution and its outcomes through a probabilistic formulation. AEM first identifies the outcome of an action by selecting the most informative effect frame based on semantic relevance and visual quality. It then extracts complementary cues from visual grounding and symbolic scene graphs, aligning them in a shared latent space to form robust effect-aware representations. To detect mistakes, we further design a prompt-based detector that incorporates task-specific prompts and aligns each action segment with its intended execution semantics. Our approach achieves state-of-the-art performance on the EgoPER and CaptainCook4D benchmarks under the challenging one-class classification (OCC) setting. These results demonstrate that modeling both execution and outcome yields more reliable mistake detection, and highlight the potential of effect-aware representations to benefit a broader range of downstream applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u52a8\u4f5c\u6548\u679c\u5efa\u6a21\uff08AEM\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u52a8\u4f5c\u6267\u884c\u53ca\u5176\u7ed3\u679c\u6765\u6539\u8fdb\u7a0b\u5e8f\u6027\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u68c0\u6d4b\u3002\u8be5\u6846\u67b6\u5728\u5355\u7c7b\u5206\u7c7b\u8bbe\u7f6e\u4e0b\u5728EgoPER\u548cCaptainCook4D\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7a0b\u5e8f\u6027\u4efb\u52a1\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5206\u6790\u52a8\u4f5c\u6267\u884c\u65b9\u5f0f\uff0c\u800c\u5ffd\u89c6\u4e86\u52a8\u4f5c\u4ea7\u751f\u7684\u7ed3\u679c\uff08\u5373\u52a8\u4f5c\u6548\u679c\uff09\u3002\u8bb8\u591a\u9519\u8bef\u5e76\u975e\u4f53\u73b0\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u800c\u662f\u4f53\u73b0\u5728\u7ed3\u679c\u72b6\u6001\u4e2d\uff0c\u5982\u610f\u5916\u7684\u7269\u4f53\u72b6\u6001\u6216\u4e0d\u6b63\u786e\u7684\u7a7a\u95f4\u6392\u5217\uff0c\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u9700\u8981\u88ab\u586b\u8865\u3002", "method": "\u672c\u6587\u63d0\u51fa\u52a8\u4f5c\u6548\u679c\u5efa\u6a21\uff08AEM\uff09\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u516c\u5f0f\u8054\u5408\u6355\u6349\u52a8\u4f5c\u6267\u884c\u53ca\u5176\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u57fa\u4e8e\u8bed\u4e49\u76f8\u5173\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u6548\u679c\u5e27\u6765\u8bc6\u522b\u52a8\u4f5c\u7ed3\u679c\uff0c\u7136\u540e\u4ece\u89c6\u89c9\u5b9a\u4f4d\u548c\u7b26\u53f7\u573a\u666f\u56fe\u4e2d\u63d0\u53d6\u4e92\u8865\u7ebf\u7d22\uff0c\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u4ee5\u5f62\u6210\u9c81\u68d2\u7684\u6548\u679c\u611f\u77e5\u8868\u793a\u3002\u4e3a\u68c0\u6d4b\u9519\u8bef\uff0c\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u68c0\u6d4b\u5668\uff0c\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u5e76\u5c06\u6bcf\u4e2a\u52a8\u4f5c\u7247\u6bb5\u4e0e\u5176\u9884\u671f\u6267\u884c\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5355\u7c7b\u5206\u7c7b\uff08OCC\uff09\u8bbe\u7f6e\u4e0b\uff0c\u5728EgoPER\u548cCaptainCook4D\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8054\u5408\u5efa\u6a21\u6267\u884c\u548c\u7ed3\u679c\u80fd\u4ea7\u751f\u66f4\u53ef\u9760\u7684\u9519\u8bef\u68c0\u6d4b\uff0c\u7a81\u663e\u4e86\u6548\u679c\u611f\u77e5\u8868\u793a\u5728\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5efa\u6a21\u52a8\u4f5c\u6267\u884c\u548c\u7ed3\u679c\u80fd\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u9519\u8bef\u68c0\u6d4b\uff0c\u6548\u679c\u611f\u77e5\u8868\u793a\u5177\u6709\u6f5c\u529b\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u4e0b\u6e38\u4efb\u52a1\u3002\u8be5\u6846\u67b6\u4e3a\u89e3\u51b3\u7a0b\u5e8f\u6027\u4efb\u52a1\u4e2d\u57fa\u4e8e\u7ed3\u679c\u7684\u9519\u8bef\u68c0\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5f3a\u8c03\u4e86\u5728\u667a\u80fd\u7cfb\u7edf\u4e2d\u540c\u65f6\u8003\u8651\u6267\u884c\u8fc7\u7a0b\u548c\u7ed3\u679c\u72b6\u6001\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.03992", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03992", "abs": "https://arxiv.org/abs/2512.03992", "authors": ["Zexin Lin", "Hawen Wan", "Yebin Zhong", "Xiaoqiang"], "title": "DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation", "comment": null, "summary": "Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DIQ-H\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u662f\u9996\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u89c6\u89c9\u9000\u5316\u65f6\u5e8f\u5e8f\u5217\u4e2d\u9c81\u68d2\u6027\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u7269\u7406\u6a21\u62df\u7684\u89c6\u89c9\u9000\u5316\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u7684\u4e25\u91cd\u53ef\u9760\u6027\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u89c6\u89c9\u9000\u5316\u4e0e\u9519\u8bef\u4f20\u64ad\u95ee\u9898\uff0c\u8fd9\u4e9b\u662f\u5b89\u5168\u5173\u952e\u5e94\u7528\u5982\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5173\u952e\u5931\u6548\u6a21\u5f0f\uff0c\u5176\u4e2d\u77ac\u6001\u89c6\u89c9\u635f\u574f\u4f1a\u5f15\u53d1\u8de8\u5e27\u6301\u7eed\u5b58\u5728\u7684\u5e7b\u89c9\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86DIQ-H\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e94\u7528\u57fa\u4e8e\u7269\u7406\u7684\u89c6\u89c9\u9000\u5316\u5305\u62ec\u8fd0\u52a8\u6a21\u7cca\u3001\u4f20\u611f\u5668\u566a\u58f0\u548c\u538b\u7f29\u4f2a\u5f71\uff0c\u5e76\u901a\u8fc7\u591a\u8f6e\u95ee\u7b54\u4efb\u52a1\u8bc4\u4f30\u5e7b\u89c9\u6301\u7eed\u6027\u3001\u9519\u8bef\u6062\u590d\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u8fed\u4ee3\u7cbe\u70bc\u65b9\u6cd5\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u8fc7\u6ee4\u751f\u6210\u53ef\u9760\u7684\u4f2a\u771f\u503c\u6807\u6ce8\u3002", "result": "\u572816\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u63ed\u793a\u4e86\u663e\u8457\u7684\u9c81\u68d2\u6027\u5dee\u8ddd\uff1a\u5373\u4f7f\u662fGPT-4o\u7b49\u5148\u8fdb\u6a21\u578b\u4e5f\u4ec5\u8fbe\u523078.5%\u7684\u6062\u590d\u7387\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u8868\u73b0\u4f4e\u4e8e60%\uff1b\u63d0\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u8fed\u4ee3\u7cbe\u70bc\u65b9\u6cd5\u5b9e\u73b0\u4e8615.3%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "DIQ-H\u57fa\u51c6\u4e3a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5168\u9762\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u52a8\u6001\u89c6\u89c9\u9000\u5316\u6761\u4ef6\u4e0b\u7684\u4e25\u91cd\u9c81\u68d2\u6027\u7f3a\u9677\uff0c\u5f3a\u8c03\u4e86\u65f6\u95f4\u5e8f\u5217\u8bc4\u4f30\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.03477", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03477", "abs": "https://arxiv.org/abs/2512.03477", "authors": ["Zijian Gu", "Yuxi Liu", "Zhenhao Zhang", "Song Wang"], "title": "Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis", "comment": "10 pages, 3 tables", "summary": "Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u611f\u77e5\u7684\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u7684MaxAccGap\u635f\u5931\u51fd\u6570\u4f18\u5316\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u7684\u8bca\u65ad\u51c6\u786e\u6027\u5dee\u5f02\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e8669%\u7684\u516c\u5e73\u6027\u5dee\u8ddd\u3002", "motivation": "\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u5f71\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e13\u5bb6\u7ea7\u6027\u80fd\uff0c\u4f46\u5728\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u5b58\u5728\u663e\u8457\u7684\u8bca\u65ad\u51c6\u786e\u6027\u5dee\u5f02\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u516c\u5e73\u5e94\u7528\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u53c2\u6570\u6548\u7387\u53c8\u80fd\u4f18\u5316\u516c\u5e73\u6027\u7684\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u516c\u5e73\u611f\u77e5\u7684\u4f4e\u79e9\u9002\u5e94\u6846\u67b6\uff0c\u6838\u5fc3\u8d21\u732e\u662f\u53ef\u5fae\u5206\u7684MaxAccGap\u635f\u5931\u51fd\u6570\uff0c\u80fd\u591f\u7aef\u5230\u7aef\u4f18\u5316\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u7684\u51c6\u786e\u6027\u5747\u8861\uff1b\u5177\u4f53\u5305\u62ec\u4e09\u79cd\u65b9\u6cd5\uff1aFR-LoRA\u5c06MaxAccGap\u6b63\u5219\u5316\u6574\u5408\u5230\u8bad\u7ec3\u76ee\u6807\u4e2d\uff0cGR-LoRA\u5e94\u7528\u9006\u9891\u7387\u52a0\u6743\u5e73\u8861\u68af\u5ea6\u8d21\u732e\uff0cHybrid-LoRA\u7ed3\u5408\u4e24\u79cd\u673a\u5236\uff0c\u6574\u4e2a\u65b9\u6cd5\u4ec5\u97000.24%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "result": "\u572810,000\u5f20\u9752\u5149\u773c\u773c\u5e95\u56fe\u50cf\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cGR-LoRA\u5c06\u8bca\u65ad\u51c6\u786e\u6027\u5dee\u5f02\u51cf\u5c11\u4e8669%\uff0c\u540c\u65f6\u4fdd\u630153.15%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff1b\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u5f3a\u6b63\u5219\u5316\u5f3a\u5ea6\u5728\u6700\u5c0f\u5316\u51c6\u786e\u6027\u6298\u8877\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6700\u4f18\u516c\u5e73\u6027\uff0c\u9488\u5bf9\u7279\u5b9a\u79cd\u65cf\u7684\u4f18\u5316\u5b9e\u73b0\u4e8660%\u7684\u5dee\u5f02\u51cf\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u516c\u5e73\u6027\u4f18\u5316\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u533b\u7597\u73af\u5883\u4e2d\u90e8\u7f72\u516c\u5e73\u7684\u533b\u5b66AI\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff1b\u7814\u7a76\u63ed\u793a\u4e86\u5f3a\u6b63\u5219\u5316\u5728\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u516c\u5e73\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u4eba\u53e3\u7fa4\u4f53\u7279\u5b9a\u4f18\u5316\u7b56\u7565\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u533b\u7597AI\u5411\u66f4\u516c\u5e73\u3001\u53ef\u90e8\u7f72\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2512.03996", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03996", "abs": "https://arxiv.org/abs/2512.03996", "authors": ["Hang Xu", "Linjiang Huang", "Feng Zhao"], "title": "Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation", "comment": null, "summary": "Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \\href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6587\u672c\u5d4c\u5165\u6270\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u6027\u80fd\uff0c\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u566a\u58f0\u548c\u6587\u672c\u5d4c\u5165\u6270\u52a8\u5728\u9891\u57df\u4e0a\u7684\u4e92\u8865\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u641c\u7d22\u7b56\u7565\u548c\u5956\u52b1\u6a21\u578b\uff0c\u4f46\u5ffd\u7565\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u566a\u58f0\u968f\u673a\u6027\u5bf9\u65b9\u6cd5\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u7a7a\u95f4\u566a\u58f0\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5728\u9ad8\u9891\u7ec6\u8282\u5904\u7406\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\u4e00\u662f\u5f15\u5165\u57fa\u4e8e\u6b65\u9aa4\u7684\u6587\u672c\u5d4c\u5165\u6270\u52a8\uff0c\u7ed3\u5408\u9891\u7387\u5f15\u5bfc\u7684\u566a\u58f0\u8c03\u5ea6\u4e0e\u7a7a\u95f4\u566a\u58f0\u6270\u52a8\uff1b\u4e8c\u662f\u6839\u636e\u9891\u57df\u8d21\u732e\u548c\u6270\u52a8\u5bb9\u5fcd\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u6270\u52a8\u5f3a\u5ea6\uff0c\u5229\u7528\u6587\u672c\u5d4c\u5165\u6270\u52a8\u4e0eSDE\u6ce8\u5165\u566a\u58f0\u5728\u9891\u57df\u4e0a\u7684\u4e92\u8865\u7279\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u4e2d\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5c55\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u4e14\u51e0\u4e4e\u4e0d\u589e\u52a0\u989d\u5916\u8ba1\u7b97\u5f00\u9500\uff0c\u901a\u8fc7\u9891\u57df\u5206\u6790\u9a8c\u8bc1\u4e86\u7a7a\u95f4\u566a\u58f0\u504f\u597d\u4f4e\u9891\u6210\u5206\u800c\u6587\u672c\u5d4c\u5165\u6270\u52a8\u589e\u5f3a\u9ad8\u9891\u7ec6\u8282\u7684\u4e92\u8865\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6587\u672c\u5d4c\u5165\u6270\u52a8\u662f\u4e00\u79cd\u6709\u6548\u7684\u968f\u673a\u6027\u5f62\u5f0f\uff0c\u80fd\u591f\u4e0e\u7a7a\u95f4\u566a\u58f0\u8026\u5408\u4ee5\u589e\u5f3a\u751f\u6210\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u968f\u673a\u6027\u5229\u7528\u89c6\u89d2\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u7ef4\u5ea6\u5bf9\u6270\u52a8\u5177\u6709\u4e0d\u540c\u5bb9\u5fcd\u5ea6\u7684\u7279\u6027\u3002"}}
{"id": "2512.03479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03479", "abs": "https://arxiv.org/abs/2512.03479", "authors": ["Wenliang Guo", "Yu Kong"], "title": "Towards Object-centric Understanding for Instructional Videos", "comment": null, "summary": "Understanding procedural activities is crucial for developing future assistive AI that can reason about complex real-world tasks. Existing action-centric methods struggle with the flexibility of real procedures, where step order varies depending on object states. In this work, we propose to shift the focus to an object-centric paradigm by regarding actions as mechanisms that drive state transitions. To advance this direction, we introduce Object-IVQA, a long-form instructional video benchmark with 107 videos and 514 open-ended question-answer pairs annotated with temporally grounded evidence. The benchmark evaluates four dimensions of object-centric reasoning, including state evolution, precondition verification, counterfactual reasoning and mistake recognition. We further propose an agent framework that orchestrates object-centric planning, perception, analysis and generation tools, enabling explicit evidence retrieval and multi-hop reasoning across disjoint segments. Experiments show that existing large vision-language models struggle in object-level recognition and reasoning, whereas our framework achieves substantially improvement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5bf9\u8c61\u7684\u89c6\u9891\u7406\u89e3\u8303\u5f0f\uff0c\u5c06\u52a8\u4f5c\u89c6\u4e3a\u9a71\u52a8\u72b6\u6001\u8f6c\u6362\u7684\u673a\u5236\uff0c\u5e76\u5f15\u5165\u4e86Object-IVQA\u57fa\u51c6\u6d4b\u8bd5\u548c\u4ee3\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a0b\u5e8f\u6027\u6d3b\u52a8\u7406\u89e3\u4e2d\u7684\u5bf9\u8c61\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4ee5\u52a8\u4f5c\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u73b0\u5b9e\u7a0b\u5e8f\u4e2d\u7684\u7075\u6d3b\u6027\uff0c\u5176\u4e2d\u6b65\u9aa4\u987a\u5e8f\u4f1a\u6839\u636e\u5bf9\u8c61\u72b6\u6001\u800c\u53d8\u5316\u3002\u672c\u6587\u65e8\u5728\u5c06\u7814\u7a76\u7126\u70b9\u8f6c\u5411\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\uff0c\u5c06\u52a8\u4f5c\u89c6\u4e3a\u9a71\u52a8\u72b6\u6001\u8f6c\u6362\u7684\u673a\u5236\uff0c\u4ee5\u89e3\u51b3\u7a0b\u5e8f\u6027\u6d3b\u52a8\u7406\u89e3\u4e2d\u7684\u8fd9\u4e00\u5173\u952e\u9650\u5236\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86Object-IVQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b107\u4e2a\u957f\u683c\u5f0f\u6559\u5b66\u89c6\u9891\u548c514\u4e2a\u5e26\u6709\u65f6\u95f4\u951a\u5b9a\u8bc1\u636e\u7684\u5f00\u653e\u5f0f\u95ee\u7b54\u5bf9\uff0c\u8bc4\u4f30\u5bf9\u8c61\u4e2d\u5fc3\u63a8\u7406\u7684\u56db\u4e2a\u7ef4\u5ea6\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee3\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u534f\u8c03\u5bf9\u8c61\u4e2d\u5fc3\u89c4\u5212\u3001\u611f\u77e5\u3001\u5206\u6790\u548c\u751f\u6210\u5de5\u5177\uff0c\u652f\u6301\u663e\u5f0f\u8bc1\u636e\u68c0\u7d22\u548c\u8de8\u4e0d\u8fde\u7eed\u7247\u6bb5\u7684\u591a\u6b21\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8c61\u7ea7\u8bc6\u522b\u548c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u3002Object-IVQA\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e86\u72b6\u6001\u6f14\u5316\u3001\u524d\u63d0\u6761\u4ef6\u9a8c\u8bc1\u3001\u53cd\u4e8b\u5b9e\u63a8\u7406\u548c\u9519\u8bef\u8bc6\u522b\u56db\u4e2a\u63a8\u7406\u7ef4\u5ea6\uff0c\u4e3a\u5bf9\u8c61\u4e2d\u5fc3\u7406\u89e3\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5bf9\u8c61\u4e2d\u5fc3\u8303\u5f0f\u5728\u7a0b\u5e8f\u6027\u6d3b\u52a8\u7406\u89e3\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u8bc1\u636e\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002Object-IVQA\u57fa\u51c6\u4e3a\u672a\u6765\u8f85\u52a9AI\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.04000", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.04000", "abs": "https://arxiv.org/abs/2512.04000", "authors": ["Jialuo Li", "Bin Li", "Jiahao Li", "Yan Lu"], "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding", "comment": null, "summary": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDIG\u6846\u67b6\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u5e27\u9009\u62e9\u65b9\u6cd5\uff0c\u6839\u636e\u67e5\u8be2\u7c7b\u578b\u81ea\u9002\u5e94\u5730\u91c7\u7528\u5747\u5300\u91c7\u6837\u6216\u67e5\u8be2\u611f\u77e5\u9009\u62e9\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u9762\u4e34\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u548c\u5bc6\u96c6\u89c6\u9891\u6807\u8bb0\u5904\u7406\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u73b0\u6709\u67e5\u8be2\u611f\u77e5\u5e27\u9009\u62e9\u65b9\u6cd5\u867d\u7136\u6709\u6548\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u590d\u6742\u641c\u7d22\u673a\u5236\u662f\u5426\u5728\u6240\u6709\u67e5\u8be2\u573a\u666f\u4e0b\u90fd\u5fc5\u8981\uff0c\u5e76\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u5e27\u9009\u62e9\u7b56\u7565\u3002", "method": "\u672c\u6587\u9996\u5148\u5efa\u7acb\u5e76\u9a8c\u8bc1\u4e86\u67e5\u8be2\u7c7b\u578b\u5b66\uff0c\u533a\u5206\u5168\u5c40\u67e5\u8be2\u548c\u5c40\u90e8\u5316\u67e5\u8be2\uff0c\u57fa\u4e8e\u6b64\u63d0\u51faDIG\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u9488\u5bf9\u5168\u5c40\u67e5\u8be2\u91c7\u7528\u9ad8\u6548\u7684\u5747\u5300\u91c7\u6837\u7b56\u7565\uff0c\u9488\u5bf9\u5c40\u90e8\u5316\u67e5\u8be2\u5219\u6fc0\u6d3b\u4e13\u95e8\u7684\u6d41\u6c34\u7ebf\u63d0\u53d6\u67e5\u8be2\u76f8\u5173\u5e27\uff0c\u6574\u4e2a\u6846\u67b6\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u81ea\u9002\u5e94\u5e27\u9009\u62e9\u3002", "result": "\u5728\u4e09\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDIG\u6846\u67b6\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u8f93\u5165\u5e27\u6570\u6269\u5c55\u5230256\u5e27\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u7a33\u5065\u5730\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u67e5\u8be2\u7c7b\u578b\u5bf9\u5e27\u9009\u62e9\u7b56\u7565\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u8868\u660e\u590d\u6742\u641c\u7d22\u673a\u5236\u5e76\u975e\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u5fc5\u8981\uff0cDIG\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.03500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03500", "abs": "https://arxiv.org/abs/2512.03500", "authors": ["Te Yang", "Xiangyu Zhu", "Bo Wang", "Quan Chen", "Peng Jiang", "Zhen Lei"], "title": "EEA: Exploration-Exploitation Agent for Long Video Understanding", "comment": null, "summary": "Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEEA\u6846\u67b6\uff0c\u4e00\u79cd\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u7684\u5c42\u6b21\u6811\u641c\u7d22\u8fc7\u7a0b\u5b9e\u73b0\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u7684\u65b0\u578b\u89c6\u9891\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u4e3b\u53d1\u73b0\u5e76\u52a8\u6001\u66f4\u65b0\u4efb\u52a1\u76f8\u5173\u7684\u8bed\u4e49\u67e5\u8be2\uff0c\u540c\u65f6\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u5956\u52b1\u4e0e\u8bed\u4e49\u5148\u9a8c\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u957f\u89c6\u9891\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u957f\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5bc6\u96c6\u9884\u5904\u7406\u5bfc\u81f4\u4e25\u91cd\u8ba1\u7b97\u5f00\u9500\uff0c\u4ee5\u53ca\u63a2\u7d22\u4e0e\u5229\u7528\u5e73\u8861\u4e0d\u5f53\u5bfc\u81f4\u4fe1\u606f\u8986\u76d6\u4e0d\u5b8c\u6574\u548c\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u8981\u4e48\u65e0\u6cd5\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u9650\u5236\u4e86\u957f\u89c6\u9891\u5206\u6790\u7684\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002", "method": "EEA\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u7684\u5c42\u6b21\u6811\u641c\u7d22\u8fc7\u7a0b\u5b9e\u73b0\u63a2\u7d22-\u5229\u7528\u5e73\u8861\uff0c\u81ea\u4e3b\u53d1\u73b0\u5e76\u52a8\u6001\u66f4\u65b0\u4efb\u52a1\u76f8\u5173\u8bed\u4e49\u67e5\u8be2\uff0c\u6536\u96c6\u4e0e\u8fd9\u4e9b\u67e5\u8be2\u7d27\u5bc6\u5339\u914d\u7684\u89c6\u9891\u5e27\u4f5c\u4e3a\u8bed\u4e49\u951a\u70b9\u3002\u5728\u6811\u641c\u7d22\u8fc7\u7a0b\u4e2d\uff0cEEA\u4f18\u5148\u63a2\u7d22\u8bed\u4e49\u76f8\u5173\u5e27\uff0c\u540c\u65f6\u786e\u4fdd\u672a\u77e5\u6bb5\u7684\u5145\u5206\u8986\u76d6\uff0c\u5e76\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u5956\u52b1\u4e0e\u8bed\u4e49\u5148\u9a8c\u81ea\u9002\u5e94\u7ed3\u5408\uff0c\u5b9e\u73b0\u5bf9\u89c6\u9891\u6bb5\u7684\u7a33\u5b9a\u7cbe\u786e\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u4e2a\u957f\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002EEA\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u6548\u8ba1\u7b97\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u7684\u5c42\u6b21\u6811\u641c\u7d22\u8fc7\u7a0b\u80fd\u591f\u6709\u6548\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u7684\u957f\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002EEA\u6846\u67b6\u7684\u81ea\u9002\u5e94\u8bed\u4e49\u67e5\u8be2\u673a\u5236\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u6cd5\u4e3a\u672a\u6765\u89c6\u9891\u667a\u80fd\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5927\u89c4\u6a21\u89c6\u89c9\u6570\u636e\u65f6\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u4e0e\u4fe1\u606f\u8986\u76d6\u7684\u5e73\u8861\u3002"}}
{"id": "2512.04025", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.04025", "abs": "https://arxiv.org/abs/2512.04025", "authors": ["Xiaolong Li", "Youping Gu", "Xi Lin", "Weijie Wang", "Bohan Zhuang"], "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation", "comment": "Tech report", "summary": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u91d1\u5b57\u5854\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u591a\u7ea7\u6c60\u5316\u952e\u503c\u8868\u793a\u66ff\u4ee3\u4f20\u7edf\u4e8c\u503c\u63a9\u7801\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u9ad8\u7a00\u758f\u5ea6\u4e0b\u7684\u4fe1\u606f\u635f\u5931\uff0c\u9002\u7528\u4e8e\u89c6\u9891\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u6ce8\u610f\u529b\u673a\u5236\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u6838\u5fc3\uff0c\u5176\u4e8c\u6b21\u590d\u6742\u5ea6\u662f\u6269\u5c55\u7684\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u9ad8\u6548\u6ce8\u610f\u529b\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u7a00\u758f\u5316\u8303\u5f0f\uff0c\u4f46\u5f53\u524d\u65b9\u6cd5\u4f7f\u7528\u4e8c\u503c\u63a9\u7801\u4fdd\u7559\u6216\u4e22\u5f03\u6574\u4e2a\u952e\u503c\u5757\uff0c\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u4fe1\u606f\u635f\u5931\uff0c\u8fd9\u4e00\u7f3a\u9677\u4e9f\u5f85\u89e3\u51b3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u91d1\u5b57\u5854\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u91c7\u7528\u591a\u7ea7\u6c60\u5316\u952e\u503c\u8868\u793a\u66ff\u4ee3\u4f20\u7edf\u4e8c\u503c\u63a9\u7801\u3002\u6bcf\u4e2a\u67e5\u8be2\u5757\u52a8\u6001\u5206\u914d\u8f83\u4f4e\u7684\u6c60\u5316\u7ea7\u522b\u7ed9\u5173\u952e\u952e\u503c\u5757\uff0c\u8f83\u9ad8\u7684\u6c60\u5316\u7ea7\u522b\u7ed9\u6b21\u8981\u952e\u503c\u5757\uff0c\u5728\u5b8c\u5168\u4fdd\u7559\u548c\u5b8c\u5168\u526a\u679d\u4e4b\u95f4\u521b\u5efa\u4fe1\u606f\u6027\u63d2\u503c\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u89e3\u8026\u7684\u5757-\u74e6\u7247\u8bbe\u8ba1\uff0c\u786e\u4fdd\u786c\u4ef6\u53cb\u597d\u7684\u9ad8\u6548\u6267\u884c\u3002", "result": "\u5728\u89c6\u9891\u7406\u89e3\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPSA\u5728\u4fdd\u6301\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u59cb\u7ec8\u4f18\u4e8e\u6216\u8fbe\u5230\u4e0e\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u6548\u7387-\u8d28\u91cf\u6743\u8861\u3002\u8be5\u65b9\u6cd5\u5728\u4f4e\u8ba1\u7b97\u9884\u7b97\u4e0b\u6709\u6548\u7f13\u89e3\u4fe1\u606f\u635f\u5931\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u91d1\u5b57\u5854\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u901a\u8fc7\u591a\u7ea7\u6c60\u5316\u8868\u793a\u63d0\u4f9b\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u8bbe\u8ba1\u7684\u65b0\u8303\u5f0f\uff0c\u7c7b\u6bd4\u4e8e\u5b9a\u70b9\u91cf\u5316\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u7ecf\u5178\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u9ad8\u7a00\u758f\u5ea6\u4e0b\u7684\u4fe1\u606f\u635f\u5931\uff0c\u4e3a\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2512.03508", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03508", "abs": "https://arxiv.org/abs/2512.03508", "authors": ["Seogkyu Jeon", "Kibeom Hong", "Hyeran Byun"], "title": "Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation", "comment": "ICCV 2025 (poster)", "summary": "Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDPMFormer\uff0c\u4e00\u79cd\u7528\u4e8e\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u548c\u4e00\u81f4\u6027\u5b66\u4e60\u89e3\u51b3\u89c6\u89c9-\u6587\u672c\u8bed\u4e49\u9519\u914d\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u5ffd\u89c6\u4e86\u89c6\u89c9\u4e0e\u6587\u672c\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u8bed\u4e49\u9519\u914d\u95ee\u9898\uff0c\u8fd9\u79cd\u9519\u914d\u6e90\u4e8e\u5728\u5355\u4e00\u6e90\u57df\u4e0a\u5b66\u4e60\u7684\u56fa\u5b9a\u4e0a\u4e0b\u6587\u63d0\u793a\u7684\u521a\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u8de8\u57df\u6cdb\u5316\u65f6\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u9886\u57df\u611f\u77e5\u63d0\u793a\u9a71\u52a8\u7684\u63a9\u7801\u53d8\u6362\u5668\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u9886\u57df\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u4ee5\u4fc3\u8fdb\u89c6\u89c9\u4e0e\u6587\u672c\u7ebf\u7d22\u7684\u8bed\u4e49\u5bf9\u9f50\uff1b\u7ed3\u5408\u7eb9\u7406\u6270\u52a8\u7684\u9886\u57df\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u4ee5\u591a\u6837\u5316\u53ef\u89c2\u6d4b\u9886\u57df\uff1b\u4ee5\u53ca\u9886\u57df\u9c81\u68d2\u4e00\u81f4\u6027\u5b66\u4e60\u4ee5\u6700\u5c0f\u5316\u539f\u59cb\u56fe\u50cf\u4e0e\u589e\u5f3a\u56fe\u50cf\u9884\u6d4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u5bf9\u63d0\u5347\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u89e3\u51b3\u89c6\u89c9-\u6587\u672c\u8bed\u4e49\u9519\u914d\u5bf9\u9886\u57df\u6cdb\u5316\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u591a\u7ec4\u4ef6\u6846\u67b6\u4e3a\u6784\u5efa\u5bf9\u73af\u5883\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u7684\u8bed\u4e49\u5206\u5272\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5e76\u4e3a\u672a\u6765\u8de8\u57df\u89c6\u89c9\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.03532", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03532", "abs": "https://arxiv.org/abs/2512.03532", "authors": ["Zhishan Zhou", "Siyuan Wei", "Zengran Wang", "Chunjie Wang", "Xiaosheng Yan", "Xiao Liu"], "title": "OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation", "comment": null, "summary": "Generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments is crucial for robotics and AR/VR, yet remains a significant challenge. We attribute this to two key limitations of existing methods: (1) proposal generation relies on dataset-specific proposal networks or mesh-based superpoints, rendering them inapplicable in mesh-free scenarios and limiting generalization to novel scenes; and (2) the weak textual reasoning of CLIP-based classifiers, which struggle to recognize compositional and functional user queries. To address these issues, we introduce OpenTrack3D, a generalizable and accurate framework. Unlike methods that rely on pre-generated proposals, OpenTrack3D employs a novel visual-spatial tracker to construct cross-view consistent object proposals online. Given an RGB-D stream, our pipeline first leverages a 2D open-vocabulary segmenter to generate masks, which are lifted to 3D point clouds using depth. Mask-guided instance features are then extracted using DINO feature maps, and our tracker fuses visual and spatial cues to maintain instance consistency. The core pipeline is entirely mesh-free, yet we also provide an optional superpoints refinement module to further enhance performance when scene mesh is available. Finally, we replace CLIP with a multi-modal large language model (MLLM), significantly enhancing compositional reasoning for complex user queries. Extensive experiments on diverse benchmarks, including ScanNet200, Replica, ScanNet++, and SceneFun3D, demonstrate state-of-the-art performance and strong generalization capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OpenTrack3D\uff0c\u4e00\u4e2a\u7528\u4e8e\u5f00\u653e\u8bcd\u6c473D\u5b9e\u4f8b\u5206\u5272\u7684\u901a\u7528\u4e14\u51c6\u786e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u89c6\u89c9-\u7a7a\u95f4\u8ddf\u8e2a\u5668\u6784\u5efa\u8de8\u89c6\u89d2\u4e00\u81f4\u7684\u5bf9\u8c61\u63d0\u8bae\uff0c\u5e76\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c473D\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u4e00\u662f\u63d0\u8bae\u751f\u6210\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u7279\u5b9a\u7684\u63d0\u8bae\u7f51\u7edc\u6216\u57fa\u4e8e\u7f51\u683c\u7684\u8d85\u70b9\uff0c\u4f7f\u5176\u5728\u65e0\u7f51\u683c\u573a\u666f\u4e2d\u4e0d\u9002\u7528\u4e14\u9650\u5236\u4e86\u5411\u65b0\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\uff1b\u4e8c\u662f\u57fa\u4e8eCLIP\u7684\u5206\u7c7b\u5668\u6587\u672c\u63a8\u7406\u80fd\u529b\u8f83\u5f31\uff0c\u96be\u4ee5\u5904\u7406\u7ec4\u5408\u6027\u548c\u529f\u80fd\u6027\u7684\u7528\u6237\u67e5\u8be2\u3002", "method": "OpenTrack3D\u91c7\u7528\u5728\u7ebf\u89c6\u89c9-\u7a7a\u95f4\u8ddf\u8e2a\u5668\u6784\u5efa\u8de8\u89c6\u89d2\u4e00\u81f4\u7684\u5bf9\u8c61\u63d0\u8bae\uff0c\u9996\u5148\u5229\u75282D\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u5668\u751f\u6210\u63a9\u7801\u5e76\u901a\u8fc7\u6df1\u5ea6\u4fe1\u606f\u63d0\u5347\u52303D\u70b9\u4e91\uff0c\u7136\u540e\u4f7f\u7528DINO\u7279\u5f81\u56fe\u63d0\u53d6\u63a9\u7801\u5f15\u5bfc\u7684\u5b9e\u4f8b\u7279\u5f81\uff0c\u8ddf\u8e2a\u5668\u878d\u5408\u89c6\u89c9\u548c\u7a7a\u95f4\u7ebf\u7d22\u4ee5\u4fdd\u6301\u5b9e\u4f8b\u4e00\u81f4\u6027\u3002\u8be5\u6838\u5fc3\u6d41\u7a0b\u5b8c\u5168\u65e0\u7f51\u683c\uff0c\u4f46\u63d0\u4f9b\u4e86\u53ef\u9009\u7684\u8d85\u70b9\u7ec6\u5316\u6a21\u5757\u4ee5\u5728\u573a\u666f\u7f51\u683c\u53ef\u7528\u65f6\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u66ff\u4ee3CLIP\u4ee5\u589e\u5f3a\u590d\u6742\u67e5\u8be2\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728ScanNet200\u3001Replica\u3001ScanNet++\u548cSceneFun3D\u7b49\u591a\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cOpenTrack3D\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u65e0\u7f51\u683c\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u548c\u5bf9\u590d\u6742\u7528\u6237\u67e5\u8be2\u7684\u51c6\u786e\u7406\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u7ebf\u89c6\u89c9-\u7a7a\u95f4\u8ddf\u8e2a\u5668\u5728\u6784\u5efa\u8de8\u89c6\u89d2\u4e00\u81f4\u5bf9\u8c61\u63d0\u8bae\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u589e\u5f3a\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u548cAR/VR\u5e94\u7528\u4e2d\u7684\u5f00\u653e\u8bcd\u6c473D\u5b9e\u4f8b\u5206\u5272\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5728\u65e0\u7f51\u683c\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2512.03575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03575", "abs": "https://arxiv.org/abs/2512.03575", "authors": ["Chao Yuan", "Shimin Chen", "Minliang Lin", "Limeng Qiao", "Guanglu Wan", "Lin Ma"], "title": "UniComp: Rethinking Video Compression Through Informational Uniqueness", "comment": null, "summary": "Distinct from attention-based compression methods, this paper presents an information uniqueness driven video compression framework, termed UniComp, which aims to maximize the information fidelity of video representations under constrained computational budgets. Starting from the information-theoretic perspective, we formulate the vision compression as an optimization problem that minimizes conditional entropy (reconstruction error) between retained and full tokens. To achieve this, we introduce the notion of information uniqueness to measure intrinsic redundancy among tokens to link with reconstruction error. Based on uniqueness, we design three modules-Frame Group Fusion, Token Allocation, and Spatial Dynamic Compression-that progressively perform semantic frame grouping, adaptive resource allocation, and fine-grained spatial compression. Extensive experiments demonstrate that UniComp consistently outperforms existing compression methods in preserving essential visual tokens under limited computational budgets, highlighting the pivotal role of information uniqueness in token compression efficacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u72ec\u7279\u6027\u7684\u89c6\u9891\u538b\u7f29\u6846\u67b6UniComp\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4fdd\u7559\u4ee4\u724c\u4e0e\u5b8c\u6574\u4ee4\u724c\u4e4b\u95f4\u7684\u6761\u4ef6\u71b5\u6765\u4f18\u5316\u538b\u7f29\u8fc7\u7a0b\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u4ee4\u724c\u7684\u4fdd\u7559\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672c\u6587\u65e8\u5728\u4ece\u4fe1\u606f\u8bba\u89c6\u89d2\u51fa\u53d1\uff0c\u5728\u53d7\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u6700\u5927\u5316\u89c6\u9891\u8868\u793a\u7684\u4fe1\u606f\u4fdd\u771f\u5ea6\uff0c\u89e3\u51b3\u89c6\u89c9\u538b\u7f29\u4e2d\u5982\u4f55\u6709\u6548\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4fe1\u606f\u72ec\u7279\u6027\u6982\u5ff5\u6765\u8861\u91cf\u4ee4\u724c\u95f4\u7684\u5185\u5728\u5197\u4f59\u5ea6\uff0c\u5e76\u8bbe\u8ba1\u4e09\u4e2a\u6e10\u8fdb\u5f0f\u6a21\u5757\uff1a\u5e27\u7ec4\u878d\u5408\u5b9e\u73b0\u8bed\u4e49\u5e27\u5206\u7ec4\uff0c\u4ee4\u724c\u5206\u914d\u8fdb\u884c\u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\uff0c\u7a7a\u95f4\u52a8\u6001\u538b\u7f29\u6267\u884c\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u538b\u7f29\uff0c\u5171\u540c\u6784\u6210UniComp\u6846\u67b6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUniComp\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\uff0c\u5728\u4fdd\u7559\u5173\u952e\u89c6\u89c9\u4ee4\u724c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u4fe1\u606f\u72ec\u7279\u6027\u5728\u4ee4\u724c\u538b\u7f29\u6548\u679c\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u4fe1\u606f\u72ec\u7279\u6027\u5728\u89c6\u89c9\u538b\u7f29\u4e2d\u7684\u6838\u5fc3\u4ef7\u503c\uff0c\u4e3a\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u538b\u7f29\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c55\u793a\u4e86\u6e10\u8fdb\u5f0f\u8bed\u4e49\u538b\u7f29\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2512.03577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03577", "abs": "https://arxiv.org/abs/2512.03577", "authors": ["Yizhi Zhang", "Lei Fan", "Zhulin Tao", "Donglin Di", "Yang Song", "Sidong Liu", "Cong Cong"], "title": "Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning", "comment": "6 pages, 2 figures. Camera-ready version accepted for IEEE BIBM 2025", "summary": "Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&E enriches H&E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&E, HER2, KI67, ER, PGR) to enable paired H&E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u8de8\u67d3\u8272\u5bf9\u6bd4\u5b66\u4e60\uff08CSCL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u65b0\u6784\u5efa\u7684\u4e94\u67d3\u8272\u5bf9\u9f50\u6570\u636e\u96c6\uff0c\u589e\u5f3aH&E\u5168\u5207\u7247\u56fe\u50cf\u7684\u901a\u7528\u8868\u793a\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u6709\u6548\u6574\u5408\u591a\u67d3\u8272\u751f\u7269\u6807\u5fd7\u7269\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u5347\u8ba1\u7b97\u75c5\u7406\u5b66\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\uff0c\u901a\u7528\u4e14\u53ef\u8fc1\u79fb\u7684\u5168\u5207\u7247\u56fe\u50cf\u8868\u793a\u81f3\u5173\u91cd\u8981\uff0c\u5c06\u514d\u75ab\u7ec4\u5316\u7b49\u591a\u67d3\u8272\u4fe1\u606f\u4e0eH&E\u7ed3\u5408\u53ef\u4e30\u5bcc\u7279\u5f81\u8868\u8fbe\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u5bf9\u9f50\u826f\u597d\u7684\u591a\u67d3\u8272\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u67d3\u8272\u95f4\u9519\u4f4d\u5bfc\u81f4\u7ec4\u7ec7\u5bf9\u5e94\u5173\u7cfb\u4e0d\u4e00\u81f4\uff0c\u963b\u788d\u4e86\u7a33\u5b9a\u7684\u8865\u4e01\u7ea7\u7279\u5f81\u63d0\u53d6\u548c\u5207\u7247\u7ea7\u5d4c\u5165\u8d28\u91cf\u3002", "method": "\u7814\u7a76\u9996\u5148\u6784\u5efa\u4e86\u5207\u7247\u7ea7\u5bf9\u9f50\u7684\u4e94\u67d3\u8272\u6570\u636e\u96c6\uff08H&E\u3001HER2\u3001KI67\u3001ER\u3001PGR\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u6846\u67b6CSCL\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u8f7b\u91cf\u9002\u914d\u5668\u901a\u8fc7\u8865\u4e01\u7ea7\u5bf9\u6bd4\u5bf9\u9f50\u589e\u5f3aH&E\u7279\u5f81\u4e0e\u5bf9\u5e94IHC\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7684\u517c\u5bb9\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u591a\u793a\u4f8b\u5b66\u4e60\u8fdb\u884c\u5207\u7247\u7ea7\u8868\u793a\u5b66\u4e60\uff0c\u5305\u542b\u8de8\u67d3\u8272\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u6574\u5408\u67d3\u8272\u7279\u5f02\u6027\u8865\u4e01\u7279\u5f81\uff0c\u4ee5\u53ca\u8de8\u67d3\u8272\u5168\u5c40\u5bf9\u9f50\u6a21\u5757\u5f3a\u5236\u4e0d\u540c\u67d3\u8272\u95f4\u5207\u7247\u7ea7\u5d4c\u5165\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u764c\u75c7\u4e9a\u578b\u5206\u7c7b\u3001IHC\u751f\u7269\u6807\u5fd7\u7269\u72b6\u6001\u5206\u7c7b\u548c\u751f\u5b58\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u6301\u7eed\u63d0\u5347\uff0c\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u8fc1\u79fb\u7684H&E\u5207\u7247\u7ea7\u8868\u793a\uff0c\u9a8c\u8bc1\u4e86\u8de8\u67d3\u8272\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5bf9\u9f50\u7684\u591a\u67d3\u8272\u6570\u636e\u96c6\u548c\u521b\u65b0\u7684\u8de8\u67d3\u8272\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u591a\u67d3\u8272\u4fe1\u606f\u6574\u5408\u7684\u6311\u6218\uff0c\u4e3a\u751f\u6210\u901a\u7528\u4e14\u751f\u7269\u5b66\u610f\u4e49\u4e30\u5bcc\u7684H&E\u8868\u793a\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u75c5\u7406\u56fe\u50cf\u5206\u6790\u7684\u53d1\u5c55\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.03580", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.03580", "abs": "https://arxiv.org/abs/2512.03580", "authors": ["Malte Bleeker", "Mauro Gotsch"], "title": "Dynamic Optical Test for Bot Identification (DOT-BI): A simple check to identify bots in surveys and online processes", "comment": null, "summary": "We propose the Dynamic Optical Test for Bot Identification (DOT-BI): a quick and easy method that uses human perception of motion to differentiate between human respondents and automated systems in surveys and online processes. In DOT-BI, a 'hidden' number is displayed with the same random black-and-white pixel texture as its background. Only the difference in motion and scale between the number and the background makes the number perceptible to humans across frames, while frame-by-frame algorithmic processing yields no meaningful signal. We conducted two preliminary assessments. Firstly, state-of-the-art, video-capable, multimodal models (GPT-5-Thinking and Gemini 2.5 Pro) fail to extract the correct value, even when given explicit instructions about the mechanism. Secondly, in an online survey (n=182), 99.5% (181/182) of participants solved the task, with an average end-to-end completion time of 10.7 seconds; a supervised lab study (n=39) found no negative effects on perceived ease-of-use or completion time relative to a control. We release code to generate tests and 100+ pre-rendered variants to facilitate adoption in surveys and online processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DOT-BI\uff08\u52a8\u6001\u5149\u5b66\u6d4b\u8bd5\u673a\u5668\u4eba\u8bc6\u522b\uff09\uff0c\u4e00\u79cd\u5229\u7528\u4eba\u7c7b\u8fd0\u52a8\u611f\u77e5\u80fd\u529b\u533a\u5206\u4eba\u7c7b\u4e0e\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u5feb\u901f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u52a8\u6001\u80cc\u666f\u7eb9\u7406\u4e2d\u9690\u85cf\u6570\u5b57\uff0c\u4f7f\u5176\u4ec5\u5bf9\u4eba\u7c7b\u53ef\u89c1\u800c\u5bf9\u7b97\u6cd5\u4e0d\u53ef\u89c1\u3002", "motivation": "\u5f53\u524d\u5728\u7ebf\u8c03\u67e5\u548c\u6d41\u7a0b\u4e2d\u7f3a\u4e4f\u6709\u6548\u533a\u5206\u4eba\u7c7b\u53d7\u8bbf\u8005\u4e0e\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u73b0\u6709\u9a8c\u8bc1\u673a\u5236\u5bb9\u6613\u88ab\u5148\u8fdbAI\u6a21\u578b\u7ed5\u8fc7\uff0c\u9700\u8981\u5f00\u53d1\u57fa\u4e8e\u4eba\u7c7b\u72ec\u7279\u611f\u77e5\u80fd\u529b\u7684\u9a8c\u8bc1\u6280\u672f\u3002", "method": "DOT-BI\u91c7\u7528\u52a8\u6001\u5149\u5b66\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5c06\u6570\u5b57\u4ee5\u4e0e\u80cc\u666f\u76f8\u540c\u7684\u968f\u673a\u9ed1\u767d\u50cf\u7d20\u7eb9\u7406\u663e\u793a\uff0c\u4ec5\u901a\u8fc7\u6570\u5b57\u4e0e\u80cc\u666f\u4e4b\u95f4\u7684\u8fd0\u52a8\u548c\u5c3a\u5ea6\u5dee\u5f02\u4f7f\u6570\u5b57\u5bf9\u4eba\u7c7b\u53ef\u89c1\uff0c\u800c\u9010\u5e27\u7b97\u6cd5\u5904\u7406\u65e0\u6cd5\u63d0\u53d6\u6709\u610f\u4e49\u4fe1\u53f7\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\uff08GPT-5-Thinking\u548cGemini 2.5 Pro\uff09\u65e0\u6cd5\u6b63\u786e\u63d0\u53d6\u6570\u503c\uff1b\u5728\u7ebf\u8c03\u67e5\u4e2d99.5%\u53c2\u4e0e\u8005\u6210\u529f\u5b8c\u6210\u4efb\u52a1\uff0c\u5e73\u5747\u5b8c\u6210\u65f6\u95f410.7\u79d2\uff1b\u5b9e\u9a8c\u5ba4\u7814\u7a76\u672a\u53d1\u73b0\u76f8\u5bf9\u4e8e\u5bf9\u7167\u7ec4\u7684\u6613\u7528\u6027\u6216\u5b8c\u6210\u65f6\u95f4\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "DOT-BI\u901a\u8fc7\u5229\u7528\u4eba\u7c7b\u8fd0\u52a8\u611f\u77e5\u7684\u72ec\u7279\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u673a\u5668\u4eba\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6cd5\u5bf9\u7528\u6237\u53cb\u597d\u4e14\u80fd\u62b5\u6297\u5148\u8fdbAI\u653b\u51fb\uff0c\u4e3a\u5728\u7ebf\u9a8c\u8bc1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u5b9e\u8df5\u5de5\u5177\u3002"}}
{"id": "2512.03590", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03590", "abs": "https://arxiv.org/abs/2512.03590", "authors": ["Yuchen Deng", "Xiuyang Wu", "Hai-Tao Zheng", "Jie Wang", "Feidiao Yang", "Yuxing Han"], "title": "Beyond Boundary Frames: Audio-Visual Semantic Guidance for Context-Aware Video Interpolation", "comment": null, "summary": "Handling fast, complex, and highly non-linear motion patterns has long posed challenges for video frame interpolation. Although recent diffusion-based approaches improve upon traditional optical-flow-based methods, they still struggle to cover diverse application scenarios and often fail to produce sharp, temporally consistent frames in fine-grained motion tasks such as audio-visual synchronized interpolation. To address these limitations, we introduce BBF (Beyond Boundary Frames), a context-aware video frame interpolation framework, which could be guided by audio/visual semantics. First, we enhance the input design of the interpolation model so that it can flexibly handle multiple conditional modalities, including text, audio, images, and video. Second, we propose a decoupled multimodal fusion mechanism that sequentially injects different conditional signals into a DiT backbone. Finally, to maintain the generation abilities of the foundation model, we adopt a progressive multi-stage training paradigm, where the start-end frame difference embedding is used to dynamically adjust both the data sampling and the loss weighting. Extensive experimental results demonstrate that BBF outperforms specialized state-of-the-art methods on both generic interpolation and audio-visual synchronized interpolation tasks, establishing a unified framework for video frame interpolation under coordinated multi-channel conditioning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BBF\uff08Beyond Boundary Frames\uff09\uff0c\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89c6\u9891\u5e27\u63d2\u503c\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u97f3\u9891/\u89c6\u89c9\u8bed\u4e49\u8fdb\u884c\u5f15\u5bfc\uff0c\u5728\u901a\u7528\u63d2\u503c\u548c\u97f3\u9891\u89c6\u89c9\u540c\u6b65\u63d2\u503c\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u4e86\u4e13\u95e8\u7684\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5e27\u63d2\u503c\u65b9\u6cd5\u5728\u5904\u7406\u5feb\u901f\u3001\u590d\u6742\u4e14\u9ad8\u5ea6\u975e\u7ebf\u6027\u7684\u8fd0\u52a8\u6a21\u5f0f\u65f6\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u6269\u6563\u57fa\u65b9\u6cd5\u867d\u7136\u6539\u8fdb\u4e86\u4f20\u7edf\u5149\u6d41\u65b9\u6cd5\uff0c\u4f46\u4ecd\u96be\u4ee5\u8986\u76d6\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\uff0c\u4e14\u5728\u97f3\u9891\u89c6\u89c9\u540c\u6b65\u63d2\u503c\u7b49\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u4efb\u52a1\u4e2d\u7ecf\u5e38\u65e0\u6cd5\u751f\u6210\u6e05\u6670\u3001\u65f6\u95f4\u4e00\u81f4\u7684\u5e27\u3002", "method": "BBF\u6846\u67b6\u91c7\u7528\u589e\u5f3a\u7684\u8f93\u5165\u8bbe\u8ba1\uff0c\u80fd\u591f\u7075\u6d3b\u5904\u7406\u6587\u672c\u3001\u97f3\u9891\u3001\u56fe\u50cf\u548c\u89c6\u9891\u7b49\u591a\u79cd\u6761\u4ef6\u6a21\u6001\uff1b\u63d0\u51fa\u89e3\u8026\u7684\u591a\u6a21\u6001\u878d\u5408\u673a\u5236\uff0c\u5c06\u4e0d\u540c\u6761\u4ef6\u4fe1\u53f7\u987a\u5e8f\u6ce8\u5165DiT\u9aa8\u5e72\u7f51\u7edc\uff1b\u91c7\u7528\u6e10\u8fdb\u591a\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u5229\u7528\u8d77\u59cb-\u7ed3\u675f\u5e27\u5dee\u5f02\u5d4c\u5165\u52a8\u6001\u8c03\u6574\u6570\u636e\u91c7\u6837\u548c\u635f\u5931\u6743\u91cd\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBBF\u5728\u901a\u7528\u63d2\u503c\u548c\u97f3\u9891\u89c6\u89c9\u540c\u6b65\u63d2\u503c\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u4e86\u4e13\u95e8\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u534f\u8c03\u591a\u901a\u9053\u6761\u4ef6\u4e0b\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u89c6\u9891\u5e27\u63d2\u503c\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u66f4\u6e05\u6670\u3001\u65f6\u95f4\u4e00\u81f4\u7684\u5e27\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u591a\u6a21\u6001\u5f15\u5bfc\u5728\u89c6\u9891\u5e27\u63d2\u503c\u4e2d\u7684\u91cd\u8981\u6027\uff0cBBF\u6846\u67b6\u901a\u8fc7\u7075\u6d3b\u7684\u8f93\u5165\u8bbe\u8ba1\u3001\u89e3\u8026\u878d\u5408\u673a\u5236\u548c\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u4e0b\u7684\u63d2\u503c\u6311\u6218\uff0c\u4e3a\u591a\u6761\u4ef6\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03667", "abs": "https://arxiv.org/abs/2512.03667", "authors": ["Ge-Peng Ji", "Jingyi Liu", "Deng-Ping Fan", "Nick Barnes"], "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning", "comment": "Technical report", "summary": "In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Colon-X\u5f00\u653e\u8ba1\u5212\uff0c\u6784\u5efa\u4e86\u6700\u5168\u9762\u7684\u7ed3\u80a0\u955c\u591a\u6a21\u6001\u6570\u636e\u96c6ColonVQA\uff0c\u5e76\u5f00\u53d1\u4e86\u9996\u4e2aR1\u98ce\u683c\u6a21\u578bColonR1\uff0c\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u5956\u52b1\u548c\u68af\u5ea6\u7a33\u5b9a\u4f18\u5316\u6280\u672f\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u4ece\u591a\u6a21\u6001\u7406\u89e3\u5230\u4e34\u5e8a\u63a8\u7406\u7684\u8f6c\u53d8\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u7ed3\u80a0\u955c\u591a\u6a21\u6001\u667a\u80fd\u4ece\u7406\u89e3\u5230\u4e34\u5e8a\u63a8\u7406\u7684\u5173\u952e\u8fc7\u6e21\u95ee\u9898\uff0c\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u8f93\u51fa\u65b9\u9762\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u7ed3\u80a0\u955c\u7684\u63a8\u7406\u4e2d\u5fc3\u667a\u80fd\u7cfb\u7edf\u3002", "method": "\u7814\u7a76\u9996\u5148\u6784\u5efa\u4e86\u5305\u542b110\u4e07+\u89c6\u89c9\u95ee\u7b54\u6761\u76ee\u7684ColonVQA\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6db5\u76d676\u4e2a\u4e34\u5e8a\u53d1\u73b0\u548c18\u4e2a\u591a\u6a21\u6001\u4efb\u52a1\uff1b\u968f\u540e\u901a\u8fc7\u591a\u4e13\u5bb6\u8fa9\u8bba\u6d41\u7a0b\u6807\u6ce8\u4e86ColonReason\u4e34\u5e8a\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86ColonR1\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u91c7\u7528\u4efb\u52a1\u81ea\u9002\u5e94\u5956\u52b1\u548c\u68af\u5ea6\u7a33\u5b9a\u4f18\u5316\u6280\u672f\uff0c\u662f\u9996\u4e2aR1\u98ce\u683c\u7684\u7ed3\u80a0\u955c\u63a8\u7406\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u8bc4\u4f30\u4e8622\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6297\u5e72\u6270\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u7684\u4e34\u5e8a\u8f93\u51fa\u8fdc\u672a\u8fbe\u5230\u9c81\u68d2\u53ef\u4fe1\uff1b\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\uff0cColonR1\u6a21\u578b\u5b9e\u73b0\u4e8656.61%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u6bd4\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u63d0\u5347\u4e8625.22%\uff0c\u4e3a\u591a\u6a21\u6001\u7ed3\u80a0\u955c\u5206\u6790\u8bbe\u7acb\u4e86\u65b0\u7684\u63a8\u7406\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u591a\u6a21\u6001\u7406\u89e3\u5230\u4e34\u5e8a\u63a8\u7406\u7684\u8f6c\u53d8\uff0c\u4e3a\u7ed3\u80a0\u955c\u667a\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u6570\u636e\u57fa\u7840\u548c\u6a21\u578b\u57fa\u51c6\uff0cColonR1\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u7684\u4f18\u5f02\u8868\u73b0\u5c55\u793a\u4e86\u4efb\u52a1\u81ea\u9002\u5e94\u5956\u52b1\u548c\u68af\u5ea6\u7a33\u5b9a\u4f18\u5316\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u6240\u6709\u6570\u636e\u548c\u6a21\u578b\u8d44\u6e90\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2512.03683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03683", "abs": "https://arxiv.org/abs/2512.03683", "authors": ["Melis Ocal", "Xiaoyan Xing", "Yue Li", "Ngo Anh Vien", "Sezer Karaoglu", "Theo Gevers"], "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces", "comment": null, "summary": "3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GaussianBlender\uff0c\u4e00\u79cd\u7528\u4e8e\u6587\u672c\u9a71\u52a83D\u98ce\u683c\u5316\u7684\u524d\u9988\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u5373\u65f6\u63a8\u7406\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9010\u8d44\u4ea7\u4f18\u5316\u548c\u5b58\u5728\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u52303D\u98ce\u683c\u5316\u65b9\u6cd5\u901a\u5e38\u4ece2D\u56fe\u50cf\u7f16\u8f91\u5668\u84b8\u998f\u800c\u6765\uff0c\u9700\u8981\u8017\u65f6\u7684\u9010\u8d44\u4ea7\u4f18\u5316\uff0c\u5e76\u4e14\u7531\u4e8e\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5c40\u9650\u6027\u800c\u8868\u73b0\u51fa\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u5728\u5927\u89c4\u6a21\u751f\u4ea7\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u8be5\u65b9\u6cd5\u4ece\u7a7a\u95f4\u5206\u7ec4\u76843D\u9ad8\u65af\u4e2d\u5b66\u4e60\u7ed3\u6784\u5316\u3001\u89e3\u8026\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9e\u73b0\u51e0\u4f55\u548c\u5916\u89c2\u7684\u53d7\u63a7\u4fe1\u606f\u5171\u4eab\uff0c\u7136\u540e\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u8fd9\u4e9b\u5b66\u4e60\u5230\u7684\u8868\u793a\u4e0a\u5e94\u7528\u6587\u672c\u6761\u4ef6\u7f16\u8f91\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cGaussianBlender\u4e0d\u4ec5\u80fd\u591f\u5b9e\u73b0\u5373\u65f6\u3001\u9ad8\u4fdd\u771f\u3001\u51e0\u4f55\u4fdd\u6301\u3001\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u98ce\u683c\u5316\uff0c\u800c\u4e14\u8d85\u8d8a\u4e86\u9700\u8981\u9010\u5b9e\u4f8b\u6d4b\u8bd5\u65f6\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u89e3\u9501\u4e86\u5b9e\u7528\u7684\u5927\u89c4\u6a21\u6c11\u4e3b\u53163D\u98ce\u683c\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u524d\u9988\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u5373\u65f63D\u98ce\u683c\u5316\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u751f\u4ea7\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6e38\u620f\u5f00\u53d1\u3001\u865a\u62df\u73b0\u5b9e\u548c\u6570\u5b57\u827a\u672f\u9886\u57df\u76843D\u8d44\u4ea7\u521b\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03687", "abs": "https://arxiv.org/abs/2512.03687", "authors": ["Yian Li", "Xiaoyu Guo", "Hao Zhang", "Shuiwang Li", "Xiaowei Dai"], "title": "Active Visual Perception: Opportunities and Challenges", "comment": null, "summary": "Active visual perception refers to the ability of a system to dynamically engage with its environment through sensing and action, allowing it to modify its behavior in response to specific goals or uncertainties. Unlike passive systems that rely solely on visual data, active visual perception systems can direct attention, move sensors, or interact with objects to acquire more informative data. This approach is particularly powerful in complex environments where static sensing methods may not provide sufficient information. Active visual perception plays a critical role in numerous applications, including robotics, autonomous vehicles, human-computer interaction, and surveillance systems. However, despite its significant promise, there are several challenges that need to be addressed, including real-time processing of complex visual data, decision-making in dynamic environments, and integrating multimodal sensory inputs. This paper explores both the opportunities and challenges inherent in active visual perception, providing a comprehensive overview of its potential, current research, and the obstacles that must be overcome for broader adoption.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e3b\u52a8\u89c6\u89c9\u611f\u77e5\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u901a\u8fc7\u611f\u77e5\u4e0e\u884c\u52a8\u4ea4\u4e92\u83b7\u53d6\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u8be5\u9886\u57df\u7684\u673a\u9047\u3001\u6311\u6218\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u4e3b\u52a8\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u52a8\u6001\u611f\u77e5\u548c\u884c\u52a8\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u4f46\u9762\u4e34\u5b9e\u65f6\u5904\u7406\u590d\u6742\u89c6\u89c9\u6570\u636e\u3001\u52a8\u6001\u73af\u5883\u51b3\u7b56\u5236\u5b9a\u548c\u591a\u6a21\u6001\u611f\u77e5\u878d\u5408\u7b49\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u68b3\u7406\u8be5\u9886\u57df\u7684\u673a\u9047\u4e0e\u969c\u788d\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u672c\u6587\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u4e3b\u52a8\u89c6\u89c9\u611f\u77e5\u7684\u6838\u5fc3\u6982\u5ff5\u3001\u6280\u672f\u6846\u67b6\u548c\u5e94\u7528\u573a\u666f\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u6ce8\u610f\u529b\u5f15\u5bfc\u3001\u4f20\u611f\u5668\u79fb\u52a8\u548c\u7269\u4f53\u4ea4\u4e92\u7b49\u5173\u952e\u6280\u672f\uff0c\u4ee5\u53ca\u5b83\u4eec\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u73b0\u673a\u5236\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e3b\u52a8\u89c6\u89c9\u611f\u77e5\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u4eba\u673a\u4ea4\u4e92\u548c\u76d1\u63a7\u7cfb\u7edf\u7b49\u9886\u57df\u7684\u5e94\u7528\u5168\u666f\uff0c\u8bc6\u522b\u4e86\u5b9e\u65f6\u6570\u636e\u5904\u7406\u3001\u52a8\u6001\u51b3\u7b56\u548c\u591a\u6a21\u6001\u878d\u5408\u7b49\u5173\u952e\u6280\u672f\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u4e3b\u52a8\u89c6\u89c9\u611f\u77e5\u4ee3\u8868\u4e86\u4ece\u88ab\u52a8\u89c2\u5bdf\u5230\u4e3b\u52a8\u4ea4\u4e92\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u867d\u7136\u9762\u4e34\u6280\u672f\u6311\u6218\uff0c\u4f46\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5b9e\u65f6\u7b97\u6cd5\u3001\u51b3\u7b56\u6846\u67b6\u548c\u8de8\u6a21\u6001\u96c6\u6210\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2512.03724", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03724", "abs": "https://arxiv.org/abs/2512.03724", "authors": ["Ziwen Li", "Xin Wang", "Hanlue Zhang", "Runnan Chen", "Runqi Lin", "Xiao He", "Han Huang", "Yandong Guo", "Fakhri Karray", "Tongliang Liu", "Mingming Gong"], "title": "PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention", "comment": null, "summary": "The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684PosA-VLA\u6846\u67b6\uff0c\u901a\u8fc7\u59ff\u6001\u6761\u4ef6\u76d1\u7763\u951a\u5b9a\u89c6\u89c9\u6ce8\u610f\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u4ea7\u751f\u5197\u4f59\u52a8\u4f5c\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u4f5c\u751f\u6210\u7684\u7cbe\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5177\u8eab\u4efb\u52a1\u4e2d\u4ecd\u96be\u4ee5\u4ea7\u751f\u4e00\u81f4\u4e14\u7cbe\u786e\u7684\u76ee\u6807\u5bfc\u5411\u52a8\u4f5c\uff0c\u7ecf\u5e38\u5728\u8f68\u8ff9\u4e2d\u751f\u6210\u5197\u4f59\u6216\u4e0d\u7a33\u5b9a\u7684\u8fd0\u52a8\uff0c\u9650\u5236\u4e86\u5176\u5728\u65f6\u95f4\u654f\u611f\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u4f5c\u8005\u5c06\u8fd9\u4e9b\u5197\u4f59\u52a8\u4f5c\u5f52\u56e0\u4e8e\u73b0\u6709VLA\u6a21\u578b\u7684\u7a7a\u95f4\u5747\u5300\u611f\u77e5\u573a\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bb9\u6613\u88ab\u76ee\u6807\u65e0\u5173\u7269\u4f53\u5206\u6563\u6ce8\u610f\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u9ad8\u6548\u7684PosA-VLA\u6846\u67b6\uff0c\u901a\u8fc7\u59ff\u6001\u6761\u4ef6\u76d1\u7763\u951a\u5b9a\u89c6\u89c9\u6ce8\u610f\u529b\uff0c\u6301\u7eed\u5f15\u5bfc\u6a21\u578b\u611f\u77e5\u671d\u5411\u4efb\u52a1\u76f8\u5173\u533a\u57df\u3002\u8be5\u59ff\u6001\u6761\u4ef6\u951a\u5b9a\u6ce8\u610f\u529b\u673a\u5236\u4f7f\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5bf9\u9f50\u6307\u4ee4\u8bed\u4e49\u4e0e\u53ef\u64cd\u4f5c\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ece\u800c\u63d0\u5347\u52a8\u4f5c\u751f\u6210\u7cbe\u5ea6\u548c\u6548\u7387\u3002\u8be5\u6846\u67b6\u91c7\u7528\u8f7b\u91cf\u7ea7\u67b6\u6784\uff0c\u65e0\u9700\u8f85\u52a9\u611f\u77e5\u6a21\u5757\uff0c\u786e\u4fdd\u4e86\u9ad8\u6548\u63a8\u7406\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u80fd\u591f\u4ee5\u7cbe\u786e\u4e14\u65f6\u95f4\u9ad8\u6548\u7684\u65b9\u5f0f\u6267\u884c\u5177\u8eab\u4efb\u52a1\uff0c\u5e76\u5728\u5404\u79cd\u6311\u6218\u6027\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\u3002\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cPosA-VLA\u5728\u52a8\u4f5c\u751f\u6210\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u59ff\u6001\u6761\u4ef6\u76d1\u7763\u951a\u5b9a\u89c6\u89c9\u6ce8\u610f\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5197\u4f59\u52a8\u4f5c\u95ee\u9898\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u9ad8\u6548\u7684\u52a8\u4f5c\u751f\u6210\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u611f\u77e5\u6a21\u5757\u7684\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u4f7f\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u65f6\u95f4\u654f\u611f\u573a\u666f\u4e0b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03745", "abs": "https://arxiv.org/abs/2512.03745", "authors": ["Jiaze Li", "Yan Lu", "Bin Liu", "Guojun Yin", "Mang Ye"], "title": "Dual-level Modality Debiasing Learning for Unsupervised Visible-Infrared Person Re-Identification", "comment": null, "summary": "Two-stage learning pipeline has achieved promising results in unsupervised visible-infrared person re-identification (USL-VI-ReID). It first performs single-modality learning and then operates cross-modality learning to tackle the modality discrepancy. Although promising, this pipeline inevitably introduces modality bias: modality-specific cues learned in the single-modality training naturally propagate into the following cross-modality learning, impairing identity discrimination and generalization. To address this issue, we propose a Dual-level Modality Debiasing Learning (DMDL) framework that implements debiasing at both the model and optimization levels. At the model level, we propose a Causality-inspired Adjustment Intervention (CAI) module that replaces likelihood-based modeling with causal modeling, preventing modality-induced spurious patterns from being introduced, leading to a low-biased model. At the optimization level, a Collaborative Bias-free Training (CBT) strategy is introduced to interrupt the propagation of modality bias across data, labels, and features by integrating modality-specific augmentation, label refinement, and feature alignment. Extensive experiments on benchmark datasets demonstrate that DMDL could enable modality-invariant feature learning and a more generalized model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7ea7\u6a21\u6001\u53bb\u504f\u5b66\u4e60\uff08DMDL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u7ea7\u548c\u4f18\u5316\u7ea7\u7684\u53cc\u91cd\u5e72\u9884\u6765\u89e3\u51b3\u65e0\u76d1\u7763\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u7684\u6a21\u6001\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u5e7f\u4e49\u7684\u6a21\u6001\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7684\u4e24\u9636\u6bb5\u65e0\u76d1\u7763\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u65b9\u6cd5\u5728\u5355\u6a21\u6001\u5b66\u4e60\u9636\u6bb5\u4f1a\u5f15\u5165\u6a21\u6001\u7279\u5b9a\u7ebf\u7d22\uff0c\u8fd9\u4e9b\u504f\u5dee\u4f1a\u4f20\u64ad\u5230\u8de8\u6a21\u6001\u5b66\u4e60\u9636\u6bb5\uff0c\u635f\u5bb3\u8eab\u4efd\u5224\u522b\u80fd\u529b\u548c\u6a21\u578b\u6cdb\u5316\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u6a21\u6001\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u7ea7\u6a21\u6001\u53bb\u504f\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u6a21\u578b\u7ea7\u7684\u56e0\u679c\u542f\u53d1\u8c03\u6574\u5e72\u9884\u6a21\u5757\uff0c\u7528\u56e0\u679c\u5efa\u6a21\u66ff\u4ee3\u57fa\u4e8e\u4f3c\u7136\u7684\u5efa\u6a21\u4ee5\u9632\u6b62\u6a21\u6001\u8bf1\u5bfc\u7684\u865a\u5047\u6a21\u5f0f\uff1b\u4ee5\u53ca\u4f18\u5316\u7ea7\u7684\u534f\u4f5c\u65e0\u504f\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u589e\u5f3a\u3001\u6807\u7b7e\u7ec6\u5316\u548c\u7279\u5f81\u5bf9\u9f50\u6765\u4e2d\u65ad\u6a21\u6001\u504f\u5dee\u5728\u6570\u636e\u3001\u6807\u7b7e\u548c\u7279\u5f81\u95f4\u7684\u4f20\u64ad\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDMDL\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u6a21\u6001\u4e0d\u53d8\u7684\u7279\u5f81\u5b66\u4e60\uff0c\u83b7\u5f97\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u6a21\u578b\uff0c\u5728\u65e0\u76d1\u7763\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6a21\u578b\u7ea7\u548c\u4f18\u5316\u7ea7\u7684\u53cc\u91cd\u53bb\u504f\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u8de8\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u53bb\u504f\u6846\u67b6\uff0c\u8868\u660e\u540c\u65f6\u5904\u7406\u6a21\u578b\u7ed3\u6784\u548c\u8bad\u7ec3\u8fc7\u7a0b\u7684\u504f\u5dee\u4f20\u64ad\u662f\u5b9e\u73b0\u5e7f\u4e49\u6a21\u6001\u4e0d\u53d8\u8868\u793a\u7684\u5173\u952e\u3002"}}
{"id": "2512.03749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03749", "abs": "https://arxiv.org/abs/2512.03749", "authors": ["Korada Sri Vardhana", "Shrikrishna Lolla", "Soma Biswas"], "title": "Fully Unsupervised Self-debiasing of Text-to-Image Diffusion Models", "comment": "Accepted at WACV 2026", "summary": "Text-to-image (T2I) diffusion models have achieved widespread success due to their ability to generate high-resolution, photorealistic images. These models are trained on large-scale datasets, like LAION-5B, often scraped from the internet. However, since this data contains numerous biases, the models inherently learn and reproduce them, resulting in stereotypical outputs. We introduce SelfDebias, a fully unsupervised test-time debiasing method applicable to any diffusion model that uses a UNet as its noise predictor. SelfDebias identifies semantic clusters in an image encoder's embedding space and uses these clusters to guide the diffusion process during inference, minimizing the KL divergence between the output distribution and the uniform distribution. Unlike supervised approaches, SelfDebias does not require human-annotated datasets or external classifiers trained for each generated concept. Instead, it is designed to automatically identify semantic modes. Extensive experiments show that SelfDebias generalizes across prompts and diffusion model architectures, including both conditional and unconditional models. It not only effectively debiases images along key demographic dimensions while maintaining the visual fidelity of the generated images, but also more abstract concepts for which identifying biases is also challenging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSelfDebias\uff0c\u4e00\u79cd\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u6d4b\u8bd5\u65f6\u53bb\u504f\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u4f7f\u7528UNet\u4f5c\u4e3a\u566a\u58f0\u9884\u6d4b\u5668\u7684\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u8bed\u4e49\u6a21\u5f0f\u5e76\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\u4ee5\u51cf\u5c11\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5927\u578b\u4e92\u8054\u7f51\u6570\u636e\u96c6\uff08\u5982LAION-5B\uff09\u4e0a\u8bad\u7ec3\u65f6\uff0c\u4f1a\u5b66\u4e60\u5e76\u518d\u73b0\u6570\u636e\u4e2d\u5b58\u5728\u7684\u4f17\u591a\u504f\u89c1\uff0c\u5bfc\u81f4\u751f\u6210\u523b\u677f\u5370\u8c61\u5316\u7684\u8f93\u51fa\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u6216\u9488\u5bf9\u6bcf\u4e2a\u751f\u6210\u6982\u5ff5\u8bad\u7ec3\u5916\u90e8\u5206\u7c7b\u5668\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "SelfDebias\u662f\u4e00\u79cd\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u6d4b\u8bd5\u65f6\u53bb\u504f\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u56fe\u50cf\u7f16\u7801\u5668\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u805a\u7c7b\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u8fd9\u4e9b\u805a\u7c7b\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u6700\u5c0f\u5316\u8f93\u51fa\u5206\u5e03\u4e0e\u5747\u5300\u5206\u5e03\u4e4b\u95f4\u7684KL\u6563\u5ea6\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u6216\u5916\u90e8\u5206\u7c7b\u5668\uff0c\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u8bed\u4e49\u6a21\u5f0f\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSelfDebias\u5728\u591a\u79cd\u63d0\u793a\u548c\u6269\u6563\u6a21\u578b\u67b6\u6784\uff08\u5305\u62ec\u6761\u4ef6\u6a21\u578b\u548c\u65e0\u6761\u4ef6\u6a21\u578b\uff09\u4e0a\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u4e0d\u4ec5\u80fd\u6709\u6548\u51cf\u5c11\u5173\u952e\u4eba\u53e3\u7edf\u8ba1\u7ef4\u5ea6\u4e0a\u7684\u504f\u89c1\u540c\u65f6\u4fdd\u6301\u751f\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u8fd8\u80fd\u5904\u7406\u8bc6\u522b\u504f\u89c1\u66f4\u5177\u6311\u6218\u6027\u7684\u62bd\u8c61\u6982\u5ff5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u76d1\u7763\u7684\u53bb\u504f\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u9002\u5e94\u4e0d\u540c\u6982\u5ff5\u548c\u6a21\u578b\u67b6\u6784\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u516c\u5e73\u6027\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u8868\u660e\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u8bc6\u522b\u548c\u51cf\u8f7b\u590d\u6742\u504f\u89c1\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u4e0d\u53d7\u5f71\u54cd\u3002"}}
{"id": "2512.03837", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03837", "abs": "https://arxiv.org/abs/2512.03837", "authors": ["Mengyuan Liu", "Jinfu Liu", "Yongkang Jiang", "Bin He"], "title": "Heatmap Pooling Network for Action Recognition from RGB Videos", "comment": "Final Version of IEEE Transactions on Pattern Analysis and Machine Intelligence", "summary": "Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u70ed\u56fe\u6c60\u5316\u7f51\u7edc\uff08HP-Net\uff09\u7528\u4e8e\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\uff0c\u901a\u8fc7\u53cd\u9988\u6c60\u5316\u6a21\u5757\u63d0\u53d6\u4fe1\u606f\u4e30\u5bcc\u3001\u9c81\u68d2\u4e14\u7b80\u6d01\u7684\u4eba\u4f53\u6c60\u5316\u7279\u5f81\uff0c\u5e76\u8bbe\u8ba1\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u52a8\u4f5c\u8bc6\u522b\u3002", "motivation": "\u73b0\u6709RGB\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u5728\u63d0\u53d6\u6df1\u5ea6\u7279\u5f81\u65f6\u9762\u4e34\u4fe1\u606f\u5197\u4f59\u3001\u6613\u53d7\u566a\u58f0\u5e72\u6270\u548c\u9ad8\u5b58\u50a8\u6210\u672c\u7b49\u6311\u6218\uff0c\u9700\u8981\u5145\u5206\u5229\u7528\u89c6\u9891\u4e2d\u7684\u6709\u7528\u4fe1\u606f\u5e76\u63d0\u9ad8\u7279\u5f81\u63d0\u53d6\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u70ed\u56fe\u6c60\u5316\u7f51\u7edc\uff08HP-Net\uff09\uff0c\u5305\u542b\u53cd\u9988\u6c60\u5316\u6a21\u5757\u63d0\u53d6\u4fe1\u606f\u4e30\u5bcc\u4e14\u9c81\u68d2\u7684\u4eba\u4f53\u6c60\u5316\u7279\u5f81\uff0c\u5e76\u8bbe\u8ba1\u7a7a\u95f4-\u8fd0\u52a8\u534f\u540c\u5b66\u4e60\u6a21\u5757\u548c\u6587\u672c\u7ec6\u5316\u8c03\u5236\u6a21\u5757\u6765\u6574\u5408\u63d0\u53d6\u7684\u6c60\u5316\u7279\u5f81\u4e0e\u5176\u4ed6\u591a\u6a21\u6001\u6570\u636e\u3002", "result": "\u5728NTU RGB+D 60\u3001NTU RGB+D 120\u3001Toyota-Smarthome\u548cUAV-Human\u7b49\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86HP-Net\u7684\u6709\u6548\u6027\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u3002", "conclusion": "HP-Net\u901a\u8fc7\u521b\u65b0\u7684\u6c60\u5316\u7279\u5f81\u63d0\u53d6\u548c\u591a\u6a21\u6001\u878d\u5408\u673a\u5236\uff0c\u4e3a\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6240\u63d0\u53d6\u7684\u6c60\u5316\u7279\u5f81\u76f8\u6bd4\u4f20\u7edf\u59ff\u6001\u6570\u636e\u548c\u70ed\u56fe\u7279\u5f81\u5177\u6709\u660e\u663e\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2512.03844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03844", "abs": "https://arxiv.org/abs/2512.03844", "authors": ["Letian Zhou", "Songhua Liu", "Xinchao Wang"], "title": "CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation", "comment": "34 pages, 24 figures", "summary": "Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the \"intrinsic core distribution\" of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4% at the 50-images-per-class (IPC) setup on ImageNet-1K. Our code is available on the project webpage: https://github.com/zzzlt422/CoDA", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Core Distribution Alignment (CoDA)\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u73b0\u6210\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5b9e\u73b0\u6570\u636e\u96c6\u84b8\u998f\uff0c\u65e0\u9700\u5728\u76ee\u6807\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u901a\u7528\u751f\u6210\u5148\u9a8c\u4e0e\u76ee\u6807\u8bed\u4e49\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u57fa\u672c\u9650\u5236\uff1a\u5927\u591a\u6570\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u9700\u8981\u5728\u5b8c\u6574\u76ee\u6807\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8fd9\u8fdd\u80cc\u4e86\u6570\u636e\u96c6\u84b8\u998f\u7684\u521d\u8877\u4e14\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff1b\u800c\u4f9d\u8d56\u901a\u7528\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u65b9\u6cd5\u5219\u5b58\u5728\u663e\u8457\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u56e0\u4e3a\u7f51\u7edc\u89c4\u6a21\u7684\u5148\u9a8c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u76ee\u6807\u7279\u5b9a\u7684\u8bed\u4e49\uff0c\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\u3002", "method": "\u63d0\u51fa\u7684Core Distribution Alignment (CoDA)\u6846\u67b6\u9996\u5148\u901a\u8fc7\u9c81\u68d2\u7684\u57fa\u4e8e\u5bc6\u5ea6\u7684\u53d1\u73b0\u673a\u5236\u8bc6\u522b\u76ee\u6807\u6570\u636e\u96c6\u7684\"\u5185\u5728\u6838\u5fc3\u5206\u5e03\"\uff0c\u7136\u540e\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\u4f7f\u751f\u6210\u7684\u6837\u672c\u4e0e\u8be5\u6838\u5fc3\u5206\u5e03\u5bf9\u9f50\uff0c\u4ece\u800c\u6709\u6548\u5f25\u5408\u901a\u7528\u751f\u6210\u5148\u9a8c\u4e0e\u76ee\u6807\u8bed\u4e49\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u751f\u6210\u5177\u6709\u9ad8\u5ea6\u4ee3\u8868\u6027\u7684\u84b8\u998f\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoDA\u5728\u4e0d\u4f9d\u8d56\u76ee\u6807\u6570\u636e\u96c6\u7279\u5b9a\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5305\u62ecImageNet-1K\u53ca\u5176\u5b50\u96c6\u5728\u5185\u7684\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u4e86\u5148\u524d\u4f9d\u8d56\u6b64\u7c7b\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5728ImageNet-1K\u7684\u6bcf\u7c7b50\u56fe\u50cf\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u4e8660.4%\u7684\u6700\u65b0\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5229\u7528\u73b0\u6210\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u96c6\u84b8\u998f\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6838\u5fc3\u5206\u5e03\u5bf9\u9f50\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u901a\u7528\u751f\u6210\u5148\u9a8c\u4e0e\u76ee\u6807\u8bed\u4e49\u7684\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u6570\u636e\u96c6\u84b8\u998f\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u4e14\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u7279\u5b9a\u6570\u636e\u96c6\u6a21\u578b\u8bad\u7ec3\u9700\u6c42\u3002"}}
{"id": "2512.03905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03905", "abs": "https://arxiv.org/abs/2512.03905", "authors": ["Shuai Yang", "Junxin Lin", "Yifan Zhou", "Ziwei Liu", "Chen Change Loy"], "title": "Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence", "comment": "Code: https://github.com/Sunnycookies/FRESCO-v2, Project: https://williamyang1991.github.io/projects/FRESCOv2/", "summary": "The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FRESCO\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5e27\u5185\u4e0e\u5e27\u95f4\u5bf9\u5e94\u5173\u7cfb\u6765\u589e\u5f3a\u65f6\u7a7a\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u89c6\u9891\u7f16\u8f91\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u5728\u89c6\u9891\u5230\u89c6\u9891\u8f6c\u6362\u548c\u6587\u672c\u5f15\u5bfc\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u96f6\u6837\u672c\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u6574\u5408\u5e27\u95f4\u5bf9\u5e94\u5173\u7cfb\uff0c\u4f46\u5176\u8f6f\u7ea6\u675f\u5728\u8bc6\u522b\u6709\u6548\u7279\u5f81\u65b9\u9762\u4e0d\u8db3\uff0c\u5bb9\u6613\u5bfc\u81f4\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u65f6\u7a7a\u7ea6\u675f\u6765\u786e\u4fdd\u89c6\u9891\u7f16\u8f91\u7684\u89c6\u89c9\u8fde\u8d2f\u6027\u3002", "method": "FRESCO\u6846\u67b6\u6574\u5408\u4e86\u5e27\u5185\u5bf9\u5e94\u5173\u7cfb\u4e0e\u5e27\u95f4\u5bf9\u5e94\u5173\u7cfb\uff0c\u5f62\u6210\u4e86\u66f4\u9c81\u68d2\u7684\u65f6\u7a7a\u7ea6\u675f\u673a\u5236\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u6ce8\u610f\u529b\u5f15\u5bfc\uff0c\u8fd8\u901a\u8fc7\u663e\u5f0f\u4f18\u5316\u7279\u5f81\u6765\u786e\u4fdd\u8bed\u4e49\u76f8\u4f3c\u5185\u5bb9\u5728\u5e27\u95f4\u7684\u4e00\u81f4\u6027\u8f6c\u6362\uff0c\u4ece\u800c\u63d0\u5347\u89c6\u9891\u7f16\u8f91\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "result": "\u5728\u89c6\u9891\u5230\u89c6\u9891\u8f6c\u6362\u548c\u6587\u672c\u5f15\u5bfc\u89c6\u9891\u7f16\u8f91\u4e24\u4e2a\u96f6\u6837\u672c\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cFRESCO\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8fde\u8d2f\u7684\u89c6\u9891\u5185\u5bb9\uff0c\u76f8\u6bd4\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u63d0\u5347\u89c6\u89c9\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6574\u5408\u5e27\u5185\u4e0e\u5e27\u95f4\u5bf9\u5e94\u5173\u7cfb\u6784\u5efa\u4e86\u66f4\u9c81\u68d2\u7684\u65f6\u7a7a\u7ea6\u675f\u673a\u5236\uff0c\u4e3a\u63d0\u5347\u96f6\u6837\u672c\u89c6\u9891\u7f16\u8f91\u7684\u89c6\u89c9\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u8868\u4e86\u5f53\u524d\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u5e76\u4e3a\u672a\u6765\u89c6\u9891\u7f16\u8f91\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.03918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03918", "abs": "https://arxiv.org/abs/2512.03918", "authors": ["Youxin Pang", "Yong Zhang", "Ruizhi Shao", "Xiang Deng", "Feng Gao", "Xu Xiaoming", "Xiaoming Wei", "Yebin Liu"], "title": "UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework", "comment": "https://carlyx.github.io/UniMo/", "summary": "We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UniMo\uff0c\u4e00\u79cd\u521b\u65b0\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u9996\u6b21\u5728\u7edf\u4e00\u6846\u67b6\u5185\u5b9e\u73b0\u4e862D\u4eba\u4f53\u89c6\u9891\u548c3D\u4eba\u4f53\u8fd0\u52a8\u7684\u8054\u5408\u5efa\u6a21\uff0c\u80fd\u591f\u540c\u65f6\u751f\u6210\u548c\u7406\u89e3\u8fd9\u4e24\u79cd\u6a21\u6001\uff0c\u4e3a\u4eba\u7c7b\u4e2d\u5fc3\u4fe1\u606f\u7684\u591a\u6a21\u6001\u878d\u5408\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4ee5\u53e6\u4e00\u79cd\u6a21\u6001\u4e3a\u6761\u4ef6\u751f\u6210\u5355\u4e00\u6a21\u6001\uff0c\u6216\u5c06\u5176\u4e2d\u4e00\u79cd\u6a21\u6001\u4e0e\u6587\u672c\u3001\u97f3\u9891\u7b49\u5176\u4ed6\u6a21\u6001\u96c6\u6210\uff0c\u800c\u5c062D\u89c6\u9891\u548c3D\u8fd0\u52a8\u7edf\u4e00\u8fdb\u884c\u540c\u65f6\u4f18\u5316\u548c\u751f\u6210\u7684\u7814\u7a76\u4ecd\u7136\u5f88\u5c11\u63a2\u7d22\uff0c\u8fd9\u9762\u4e34\u7740\u7531\u4e8e\u5b83\u4eec\u5728\u7ed3\u6784\u548c\u5206\u5e03\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u6240\u5e26\u6765\u7684\u91cd\u5927\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u89c6\u9891\u548c3D\u8fd0\u52a8\u5efa\u6a21\u4e3a\u7edf\u4e00\u7684\u4ee4\u724c\u5e8f\u5217\uff0c\u5229\u7528\u5355\u72ec\u7684\u5d4c\u5165\u5c42\u6765\u7f13\u89e3\u5206\u5e03\u5dee\u5f02\uff0c\u5e76\u8bbe\u8ba1\u4e86\u96c6\u6210\u4e24\u79cd\u4e0d\u540c\u4efb\u52a1\u7684\u5e8f\u5217\u5efa\u6a21\u7b56\u7565\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u6709\u6548\u5bf9\u9f50\u89c6\u89c9\u4ee4\u724c\u5e76\u4fdd\u75593D\u7a7a\u95f4\u4fe1\u606f\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5177\u6709\u65f6\u95f4\u6269\u5c55\u7b56\u7565\u7684\u65b0\u578b3D\u8fd0\u52a8\u5206\u8bcd\u5668\uff0c\u4f7f\u7528\u5355\u4e2aVQ-VAE\u751f\u6210\u91cf\u5316\u8fd0\u52a8\u4ee4\u724c\uff0c\u8be5\u5206\u8bcd\u5668\u5305\u542b\u591a\u4e2a\u4e13\u5bb6\u89e3\u7801\u5668\uff0c\u5206\u522b\u5904\u7406\u8eab\u4f53\u5f62\u72b6\u3001\u5e73\u79fb\u3001\u5168\u5c40\u65b9\u5411\u548c\u8eab\u4f53\u59ff\u6001\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9760\u76843D\u8fd0\u52a8\u91cd\u5efa\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u751f\u6210\u76f8\u5e94\u7684\u89c6\u9891\u548c\u8fd0\u52a8\uff0c\u540c\u65f6\u6267\u884c\u7cbe\u786e\u7684\u8fd0\u52a8\u6355\u6349\uff0c\u8bc1\u660e\u4e86\u7edf\u4e00\u5efa\u6a21\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6316\u6398\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u878d\u5408\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u7684\u80fd\u529b\uff0c\u4e3a\u5c06\u4eba\u7c7b\u4e2d\u5fc3\u4fe1\u606f\u96c6\u6210\u5230\u73b0\u6709\u6a21\u578b\u4e2d\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5e76\u53ef\u80fd\u5b9e\u73b0\u4eba\u7c7b\u3001\u7269\u4f53\u548c\u573a\u666f\u7684\u591a\u6a21\u6001\u53ef\u63a7\u8054\u5408\u5efa\u6a21\uff0c\u4ee3\u8868\u4e86\u8de8\u6a21\u6001\u4eba\u7c7b\u8868\u793a\u5b66\u4e60\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2512.03963", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03963", "abs": "https://arxiv.org/abs/2512.03963", "authors": ["Tao Wu", "Li Yang", "Gen Zhan", "Yiting Liao", "Junlin Li", "Deliang Fu", "Li Zhang", "Limin Wang"], "title": "TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning", "comment": null, "summary": "Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TempR1\uff0c\u4e00\u79cd\u9762\u5411\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u611f\u77e5\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u591a\u4efb\u52a1\u4f18\u5316\u663e\u8457\u589e\u5f3a\u6a21\u578b\u5bf9\u89c6\u9891\u65f6\u5e8f\u7ed3\u6784\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u5e8f\u7406\u89e3\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u7c7b\u578b\u548c\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u6837\u5316\u65f6\u5e8f\u7406\u89e3\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u7cfb\u7edf\u5316\u63d0\u5347\u6a21\u578b\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\u7684\u901a\u7528\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86TempR1\u6846\u67b6\uff0c\u57fa\u4e8eGroup Relative Policy Optimization\u7b97\u6cd5\u6784\u5efa\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b\u591a\u6837\u5316\u65f6\u5e8f\u7ed3\u6784\u548c\u8bed\u4e49\u7684\u591a\u4efb\u52a1\u8bed\u6599\u5e93\uff0c\u5e76\u5c06\u65f6\u5e8f\u4efb\u52a1\u5212\u5206\u4e3a\u4e09\u79cd\u9884\u6d4b\u533a\u95f4\u4e0e\u771f\u5b9e\u5b9e\u4f8b\u5bf9\u5e94\u7c7b\u578b\uff0c\u4e3a\u6bcf\u79cd\u7c7b\u578b\u8bbe\u8ba1\u5b9a\u5236\u5316\u7684\u5b9a\u4f4d\u5956\u52b1\u51fd\u6570\uff0c\u5b9e\u73b0\u7a33\u5b9a\u6709\u6548\u7684\u8de8\u4efb\u52a1\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTempR1\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u5176\u8054\u5408\u4f18\u5316\u4e92\u8865\u4efb\u52a1\u4ea7\u751f\u4e86\u663e\u8457\u7684\u534f\u540c\u6548\u5e94\uff0c\u65e2\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e5f\u63d0\u5347\u4e86\u5355\u4efb\u52a1\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u5e8f\u63a8\u7406\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u539f\u5219\u6027\u7684\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u5728\u7cfb\u7edf\u5316\u589e\u5f3a\u65f6\u5e8f\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u957f\u89c6\u9891\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2512.04019", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.04019", "abs": "https://arxiv.org/abs/2512.04019", "authors": ["Ho Man Kwan", "Tianhao Peng", "Ge Gao", "Fan Zhang", "Mike Nilsson", "Andrew Gower", "David Bull"], "title": "Ultra-lightweight Neural Video Representation Compression", "comment": null, "summary": "Recent works have demonstrated the viability of utilizing over-fitted implicit neural representations (INRs) as alternatives to autoencoder-based models for neural video compression. Among these INR-based video codecs, Neural Video Representation Compression (NVRC) was the first to adopt a fully end-to-end compression framework that compresses INRs, achieving state-of-the-art performance. Moreover, some recently proposed lightweight INRs have shown comparable performance to their baseline codecs with computational complexity lower than 10kMACs/pixel. In this work, we extend NVRC toward lightweight representations, and propose NVRC-Lite, which incorporates two key changes. Firstly, we integrated multi-scale feature grids into our lightweight neural representation, and the use of higher resolution grids significantly improves the performance of INRs at low complexity. Secondly, we address the issue that existing INRs typically leverage autoregressive models for entropy coding: these are effective but impractical due to their slow coding speed. In this work, we propose an octree-based context model for entropy coding high-dimensional feature grids, which accelerates the entropy coding module of the model. Our experimental results demonstrate that NVRC-Lite outperforms C3, one of the best lightweight INR-based video codecs, with up to 21.03% and 23.06% BD-rate savings when measured in PSNR and MS-SSIM, respectively, while achieving 8.4x encoding and 2.5x decoding speedup. The implementation of NVRC-Lite will be made available.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faNVRC-Lite\uff0c\u4e00\u79cd\u8f7b\u91cf\u5316\u7684\u795e\u7ecf\u89c6\u9891\u8868\u793a\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u5c3a\u5ea6\u7279\u5f81\u7f51\u683c\u548c\u516b\u53c9\u6811\u4e0a\u4e0b\u6587\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6027\u80fd\u5e76\u52a0\u901f\u4e86\u71b5\u7f16\u7801\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684\u89c6\u9891\u538b\u7f29\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4e00\u662f\u8f7b\u91cf\u5316INR\u5728\u4f4e\u590d\u6742\u5ea6\u4e0b\u7684\u6027\u80fd\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u4e8c\u662f\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u81ea\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u71b5\u7f16\u7801\uff0c\u867d\u7136\u6709\u6548\u4f46\u7f16\u7801\u901f\u5ea6\u7f13\u6162\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "NVRC-Lite\u91c7\u7528\u4e24\u79cd\u5173\u952e\u6280\u672f\u6539\u8fdb\uff1a\u9996\u5148\uff0c\u5c06\u591a\u5c3a\u5ea6\u7279\u5f81\u7f51\u683c\u96c6\u6210\u5230\u8f7b\u91cf\u5316\u795e\u7ecf\u8868\u793a\u4e2d\uff0c\u901a\u8fc7\u4f7f\u7528\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u7f51\u683c\u663e\u8457\u63d0\u5347\u4f4e\u590d\u6742\u5ea6\u4e0bINR\u7684\u6027\u80fd\uff1b\u5176\u6b21\uff0c\u63d0\u51fa\u57fa\u4e8e\u516b\u53c9\u6811\u7684\u4e0a\u4e0b\u6587\u6a21\u578b\u7528\u4e8e\u9ad8\u7ef4\u7279\u5f81\u7f51\u683c\u7684\u71b5\u7f16\u7801\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u4ece\u800c\u52a0\u901f\u6574\u4e2a\u71b5\u7f16\u7801\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cNVRC-Lite\u5728PSNR\u548cMS-SSIM\u6307\u6807\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad821.03%\u548c23.06%\u7684BD-rate\u8282\u7701\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u8f7b\u91cf\u5316INR\u89c6\u9891\u7f16\u89e3\u7801\u5668C3\uff0c\u540c\u65f6\u5b9e\u73b0\u4e868.4\u500d\u7684\u7f16\u7801\u52a0\u901f\u548c2.5\u500d\u7684\u89e3\u7801\u52a0\u901f\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4fdd\u6301\u572810kMACs/\u50cf\u7d20\u4ee5\u4e0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u7f51\u683c\u548c\u9ad8\u6548\u71b5\u7f16\u7801\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8f7b\u91cf\u5316INR\u89c6\u9891\u538b\u7f29\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u516b\u53c9\u6811\u4e0a\u4e0b\u6587\u6a21\u578b\u5728\u52a0\u901f\u71b5\u7f16\u7801\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.04082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04082", "abs": "https://arxiv.org/abs/2512.04082", "authors": ["Jiazhe Wei", "Ken Li", "Tianyu Lao", "Haofan Wang", "Liang Wang", "Caifeng Shan", "Chenyang Si"], "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "comment": "Project page: https://postercopilot.github.io/", "summary": "Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PosterCopilot\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u51e0\u4f55\u7406\u89e3\u548c\u7f8e\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u4e13\u4e1a\u7ea7\u56fe\u5f62\u8bbe\u8ba1\u7684\u53ef\u63a7\u8fed\u4ee3\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u56fe\u5f62\u8bbe\u8ba1\u81ea\u52a8\u5316\u65b9\u6cd5\u5b58\u5728\u51e0\u4f55\u5e03\u5c40\u4e0d\u51c6\u786e\u548c\u7f3a\u4e4f\u4e13\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u6240\u9700\u7684\u8fed\u4ee3\u3001\u5206\u5c42\u7f16\u8f91\u80fd\u529b\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u4e13\u4e1a\u8bbe\u8ba1\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "PosterCopilot\u6846\u67b6\u91c7\u7528\u6e10\u8fdb\u5f0f\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u6270\u52a8\u76d1\u7763\u5fae\u8c03\u3001\u89c6\u89c9-\u73b0\u5b9e\u5bf9\u9f50\u7684\u5f3a\u5316\u5b66\u4e60\u4ee5\u53ca\u7f8e\u5b66\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u589e\u5f3aLMM\u7684\u51e0\u4f55\u7406\u89e3\u548c\u7f8e\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u5206\u5c42\u53ef\u63a7\u7684\u8fed\u4ee3\u7f16\u8f91\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660ePosterCopilot\u80fd\u591f\u751f\u6210\u51e0\u4f55\u51c6\u786e\u4e14\u7f8e\u5b66\u4f18\u8d8a\u7684\u5e03\u5c40\uff0c\u5728\u4e13\u4e1a\u8fed\u4ee3\u8bbe\u8ba1\u4e2d\u63d0\u4f9b\u524d\u6240\u672a\u6709\u7684\u53ef\u63a7\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u589e\u5f3aLMM\u7684\u51e0\u4f55\u7406\u89e3\u548c\u7f8e\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u5408\u53ef\u63a7\u7f16\u8f91\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4e3a\u4e13\u4e1a\u56fe\u5f62\u8bbe\u8ba1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u51e0\u4f55\u51c6\u786e\u6027\u548c\u8bbe\u8ba1\u53ef\u63a7\u6027\u7684\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2512.04085", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04085", "abs": "https://arxiv.org/abs/2512.04085", "authors": ["Tengda Han", "Sayna Ebrahimi", "Dilara Gokay", "Li Yang Ku", "Maks Ovsjanikov", "Iva Babukova", "Daniel Zoran", "Viorica Patraucean", "Joao Carreira", "Andrew Zisserman", "Dima Damen"], "title": "Unique Lives, Shared World: Learning from Single-Life Videos", "comment": null, "summary": "We introduce the \"single-life\" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual. We leverage the multiple viewpoints naturally captured within a single life to learn a visual encoder in a self-supervised manner. Our experiments demonstrate three key findings. First, models trained independently on different lives develop a highly aligned geometric understanding. We demonstrate this by training visual encoders on distinct datasets each capturing a different life, both indoors and outdoors, as well as introducing a novel cross-attention-based metric to quantify the functional alignment of the internal representations developed by different models. Second, we show that single-life models learn generalizable geometric representations that effectively transfer to downstream tasks, such as depth estimation, in unseen environments. Third, we demonstrate that training on up to 30 hours from one week of the same person's life leads to comparable performance to training on 30 hours of diverse web data, highlighting the strength of single-life representation learning. Overall, our results establish that the shared structure of the world, both leads to consistency in models trained on individual lives, and provides a powerful signal for visual representation learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa'\u5355\u4e00\u751f\u6daf'\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u5355\u4e00\u4e2a\u4f53\u91c7\u96c6\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u8bad\u7ec3\u72ec\u7acb\u7684\u89c6\u89c9\u6a21\u578b\uff0c\u5229\u7528\u81ea\u7136\u6355\u83b7\u7684\u591a\u89c6\u89d2\u4fe1\u606f\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u8bc1\u660e\u4e86\u8be5\u8303\u5f0f\u80fd\u591f\u5b66\u4e60\u5230\u9ad8\u5ea6\u5bf9\u9f50\u4e14\u53ef\u6cdb\u5316\u7684\u51e0\u4f55\u8868\u793a\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22'\u5355\u4e00\u751f\u6daf'\u5b66\u4e60\u8303\u5f0f\uff0c\u5373\u4ec5\u4f7f\u7528\u5355\u4e00\u4e2a\u4f53\u91c7\u96c6\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff0c\u7814\u7a76\u8fd9\u79cd\u53d7\u9650\u6570\u636e\u6e90\u662f\u5426\u80fd\u591f\u5b66\u4e60\u5230\u6709\u6548\u7684\u89c6\u89c9\u8868\u793a\uff0c\u4ee5\u53ca\u4e0d\u540c\u4e2a\u4f53\u6a21\u578b\u4e4b\u95f4\u662f\u5426\u80fd\u591f\u53d1\u5c55\u51fa\u5bf9\u9f50\u7684\u51e0\u4f55\u7406\u89e3\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u5355\u4e00\u4e2a\u4f53\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u81ea\u7136\u6355\u83b7\u7684\u591a\u89c6\u89d2\u4fe1\u606f\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u65b0\u5ea6\u91cf\u6807\u51c6\u6765\u91cf\u5316\u4e0d\u540c\u6a21\u578b\u5185\u90e8\u8868\u793a\u7684\u529f\u80fd\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u540c\u65f6\u5728\u5ba4\u5185\u5916\u4e0d\u540c\u573a\u666f\u4e0b\u5bf9\u591a\u4e2a\u72ec\u7acb\u4e2a\u4f53\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\uff1a\u4e0d\u540c\u4e2a\u4f53\u72ec\u7acb\u8bad\u7ec3\u7684\u6a21\u578b\u53d1\u5c55\u51fa\u9ad8\u5ea6\u5bf9\u9f50\u7684\u51e0\u4f55\u7406\u89e3\uff1b\u5355\u4e00\u751f\u6daf\u6a21\u578b\u5b66\u4e60\u5230\u7684\u51e0\u4f55\u8868\u793a\u80fd\u591f\u6709\u6548\u8fc1\u79fb\u5230\u6df1\u5ea6\u4f30\u8ba1\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff1b\u4ec5\u4f7f\u7528\u540c\u4e00\u4eba\u4e00\u5468\u518530\u5c0f\u65f6\u6570\u636e\u8bad\u7ec3\u7684\u6027\u80fd\u4e0e\u4f7f\u752830\u5c0f\u65f6\u591a\u6837\u5316\u7f51\u7edc\u6570\u636e\u8bad\u7ec3\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u786e\u7acb\u4e86\u5355\u4e00\u751f\u6daf\u8868\u793a\u5b66\u4e60\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5171\u4eab\u7684\u4e16\u754c\u7ed3\u6784\u4e0d\u4ec5\u5bfc\u81f4\u4e2a\u4f53\u6a21\u578b\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u8fd8\u4e3a\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u4fe1\u53f7\uff0c\u4e3a\u4e2a\u6027\u5316\u89c6\u89c9\u6a21\u578b\u548c\u53d7\u9650\u6570\u636e\u573a\u666f\u4e0b\u7684\u8868\u793a\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
