<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 73]
- [cs.CL](#cs.CL) [Total: 16]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang, Amish Sethi, Matthew Kuo, Mayank Keoliya, Neelay Velingker, JungHo Jung, Ser-Nam Lim, Ziyang Li, Mayur Naik*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ESCAæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–æ—¶ç©ºç†è§£æ¥æƒ…å¢ƒåŒ–å…·èº«æ™ºèƒ½ä½“ï¼Œå…¶æ ¸å¿ƒæ˜¯SGClipæ¨¡å‹â€”â€”ä¸€ç§åŸºäºCLIPçš„å¼€æ”¾åŸŸå¯æç¤ºåœºæ™¯å›¾ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å…·èº«ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸»è¦ä¾èµ–é«˜å±‚è§†è§‰-å£°éŸ³-æ–‡æœ¬å¯¹ï¼Œç¼ºä¹åƒç´ çº§è§†è§‰å†…å®¹ä¸æ–‡æœ¬è¯­ä¹‰ä¹‹é—´çš„ç»†ç²’åº¦ç»“æ„åŒ–å¯¹é½ï¼Œè¿™é™åˆ¶äº†å…·èº«æ™ºèƒ½ä½“çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚

**Method:** æå‡ºäº†ESCAæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯SGClipæ¨¡å‹â€”â€”ä¸€ç§åŸºäºCLIPçš„å¼€æ”¾åŸŸå¯æç¤ºåœºæ™¯å›¾ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ç¥ç»ç¬¦å·å­¦ä¹ ç®¡é“åœ¨87K+å¼€æ”¾åŸŸè§†é¢‘ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨è§†é¢‘-å­—å¹•å¯¹ä¸­çš„æ¨¡å‹é©±åŠ¨è‡ªç›‘ç£å’Œç»“æ„åŒ–æ¨ç†ï¼Œæ— éœ€äººå·¥æ ‡æ³¨çš„åœºæ™¯å›¾æ³¨é‡Šã€‚

**Result:** SGClipåœ¨åœºæ™¯å›¾ç”Ÿæˆå’ŒåŠ¨ä½œå®šä½åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒESCAæ¡†æ¶æŒç»­æå‡äº†å¼€æºå’Œå•†ä¸šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨ä¸¤ä¸ªå…·èº«ç¯å¢ƒä¸­å®ç°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼Œæ˜¾è‘—å‡å°‘äº†æ™ºèƒ½ä½“æ„ŸçŸ¥é”™è¯¯å¹¶ä½¿å¼€æºæ¨¡å‹è¶…è¶Šä¸“æœ‰åŸºçº¿ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ç»“æ„åŒ–ç©ºé—´-æ—¶é—´ç†è§£å¯¹äºå…·èº«æ™ºèƒ½ä½“çš„é‡è¦æ€§ï¼Œç¥ç»ç¬¦å·å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£å†³ç¼ºä¹æ ‡æ³¨æ•°æ®çš„é—®é¢˜ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„é€šç”¨å…·èº«æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.


### [2] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CrossRay3Dç¨€ç–å¤šæ¨¡æ€æ£€æµ‹å™¨ï¼Œé€šè¿‡å°„çº¿æ„ŸçŸ¥ç›‘ç£å’Œç±»åˆ«å¹³è¡¡ç›‘ç£æœºåˆ¶æå‡tokenè¡¨ç¤ºè´¨é‡ï¼Œåœ¨nuScenesåŸºå‡†ä¸Šå®ç°72.4 mAPå’Œ74.7 NDSçš„SOTAæ€§èƒ½ï¼ŒåŒæ—¶è¿è¡Œé€Ÿåº¦æå‡1.84å€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç¨€ç–æ£€æµ‹å™¨å¿½è§†äº†tokenè¡¨ç¤ºè´¨é‡çš„é—®é¢˜ï¼Œå¯¼è‡´å‰æ™¯è´¨é‡æ¬¡ä¼˜ä¸”æ€§èƒ½å—é™ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹å¯¹å‡ ä½•ç»“æ„ä¿æŒå’Œç±»åˆ«åˆ†å¸ƒçš„å…³é”®å…³æ³¨ã€‚

**Method:** æå‡ºç¨€ç–é€‰æ‹©å™¨ï¼ˆSSï¼‰æ ¸å¿ƒæ¨¡å—ï¼ŒåŒ…æ‹¬å°„çº¿æ„ŸçŸ¥ç›‘ç£ï¼ˆRASï¼‰åœ¨è®­ç»ƒé˜¶æ®µä¿æŒä¸°å¯Œå‡ ä½•ä¿¡æ¯ï¼Œç±»åˆ«å¹³è¡¡ç›‘ç£è‡ªé€‚åº”é‡åŠ æƒç±»åˆ«è¯­ä¹‰æ˜¾è‘—æ€§ï¼Œä»¥åŠå°„çº¿ä½ç½®ç¼–ç ï¼ˆRay PEï¼‰è§£å†³LiDARä¸å›¾åƒæ¨¡æ€é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚

**Result:** åœ¨nuScenesåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCrossRay3Dè¾¾åˆ°72.4 mAPå’Œ74.7 NDSçš„SOTAæ€§èƒ½ï¼Œè¿è¡Œé€Ÿåº¦æ¯”å…¶ä»–é¢†å…ˆæ–¹æ³•å¿«1.84å€ï¼Œä¸”åœ¨LiDARæˆ–ç›¸æœºæ•°æ®éƒ¨åˆ†æˆ–å®Œå…¨ç¼ºå¤±çš„åœºæ™¯ä¸‹è¡¨ç°å‡ºå¼ºé²æ£’æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜å‡ ä½•ç»“æ„ä¿æŒå’Œç±»åˆ«åˆ†å¸ƒå¹³è¡¡æ˜¯æå‡ç¨€ç–æ£€æµ‹å™¨æ€§èƒ½çš„å…³é”®ï¼Œæå‡ºçš„æ–¹æ³•åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ£€æµ‹ç²¾åº¦å’Œé²æ£’æ€§ï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›äº†æ›´ä¼˜çš„é€‚åº”æ€§ã€‚

---

#### ğŸ“„ Abstract
The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.


### [3] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed, Abdullah Yahya Abdullah Omaisan*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªåŸºäºCCTVç›‘æ§è§†é¢‘çš„æ™ºèƒ½åŸå¸‚åŸºç¡€è®¾æ–½ç¼ºé™·æ£€æµ‹ä¸ä¿®å¤è§„åˆ’ç³»ç»Ÿï¼Œç»“åˆYOLOç›®æ ‡æ£€æµ‹å™¨å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å¤šç§ç¼ºé™·å¹¶ç”Ÿæˆç»“æ„åŒ–çš„ç»´æŠ¤è¡ŒåŠ¨è®¡åˆ’ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ™ºèƒ½åŸå¸‚åŸºç¡€è®¾æ–½ç›‘æ§é¢ä¸´æ‰‹åŠ¨æ£€æŸ¥æˆæœ¬é«˜ã€å±é™©æ€§å¤§ï¼Œç°æœ‰è‡ªåŠ¨ç³»ç»Ÿé€šå¸¸åªèƒ½å¤„ç†å•ä¸€ç¼ºé™·ç±»å‹æˆ–è¾“å‡ºéç»“æ„åŒ–ç»“æœï¼Œæ— æ³•ç›´æ¥æŒ‡å¯¼ç»´æŠ¤å›¢é˜Ÿè¿›è¡Œä¿®å¤å·¥ä½œï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿå…¨é¢æ£€æµ‹å¤šç§ç¼ºé™·å¹¶ç”Ÿæˆç»“æ„åŒ–ç»´æŠ¤è®¡åˆ’çš„ä¸€ä½“åŒ–è§£å†³æ–¹æ¡ˆã€‚

**Method:** é‡‡ç”¨YOLOç³»åˆ—ç›®æ ‡æ£€æµ‹å™¨è¿›è¡Œå¤šç¼ºé™·æ£€æµ‹å’Œåˆ†å‰²ï¼Œç„¶åå°†æ£€æµ‹ç»“æœä¼ é€’ç»™è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œåœºæ™¯æ„ŸçŸ¥æ€»ç»“ï¼Œç”ŸæˆåŒ…å«äº‹ä»¶æè¿°ã€æ¨èå·¥å…·ã€å°ºå¯¸ã€ä¿®å¤è®¡åˆ’å’Œç´§æ€¥è­¦æŠ¥çš„JSONæ ¼å¼ç»“æ„åŒ–è¡ŒåŠ¨è®¡åˆ’ï¼Œæ•´åˆäº†QwenVLå’ŒLLaVAç­‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å…ˆè¿›æŠ€æœ¯ã€‚

**Result:** åœ¨å…¬å…±æ•°æ®é›†å’Œæ•è·çš„CCTVè§†é¢‘ç‰‡æ®µä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å¤šç§åŸºç¡€è®¾æ–½ç¼ºé™·ï¼Œå¹¶ç”Ÿæˆè¿è´¯çš„ç»“æ„åŒ–æ€»ç»“ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

**Conclusion:** è¯¥ç³»ç»Ÿä¸ºæ™ºèƒ½åŸå¸‚åŸºç¡€è®¾æ–½ç»´æŠ¤æä¾›äº†ç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æ£€æµ‹æ•ˆç‡å’Œç»´æŠ¤å“åº”é€Ÿåº¦ï¼ŒåŒæ—¶è®¨è®ºäº†å°†ç³»ç»Ÿæ‰©å±•åˆ°åŸå¸‚èŒƒå›´éƒ¨ç½²æ—¶é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.


### [4] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li, Zitong Yu, Qilang Ye, Weicheng Xie, Wei Zhuo, Linlin Shen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºIAD-GPTï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ–°èŒƒå¼ï¼Œé€šè¿‡å¼‚å¸¸æç¤ºç”Ÿæˆå™¨å’Œæ–‡æœ¬å¼•å¯¼å¢å¼ºå™¨æ¿€æ´»é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„æ£€æµ‹ä¸åˆ†å‰²èƒ½åŠ›ï¼Œåœ¨MVTec-ADå’ŒVisAæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿå·¥ä¸šå¼‚å¸¸æ£€æµ‹æ–¹æ³•ç¼ºä¹å¤šè½®äººæœºå¯¹è¯å’Œè¯¦ç»†æè¿°èƒ½åŠ›ï¼Œè€ŒåŸºäºå¤§é¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•å°šæœªå……åˆ†æ¿€å‘å¤§æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“åˆæ–‡æœ¬è¯­ä¹‰ä¸å›¾åƒçº§ã€åƒç´ çº§ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚

**Method:** é‡‡ç”¨å¼‚å¸¸æç¤ºç”Ÿæˆå™¨ä¸ºç‰¹å®šå¯¹è±¡ç”Ÿæˆè¯¦ç»†å¼‚å¸¸æç¤ºï¼Œé€šè¿‡æ–‡æœ¬å¼•å¯¼å¢å¼ºå™¨ä½¿å›¾åƒç‰¹å¾ä¸æ­£å¸¸/å¼‚å¸¸æ–‡æœ¬æç¤ºäº¤äº’ä»¥åŠ¨æ€é€‰æ‹©å¢å¼ºè·¯å¾„ï¼Œå¹¶è®¾è®¡å¤šæ©ç èåˆæ¨¡å—å°†æ©ç ä½œä¸ºä¸“å®¶çŸ¥è¯†èå…¥ï¼Œå¢å¼ºå¤§è¯­è¨€æ¨¡å‹å¯¹åƒç´ çº§å¼‚å¸¸çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

**Result:** åœ¨MVTec-ADå’ŒVisAæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªç›‘ç£å’Œå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä¸åˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å·¥ä¸šå¼‚å¸¸æ£€æµ‹ä¸­çš„å¼ºå¤§æ½œåŠ›ï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬è¯­ä¹‰ä¸å¤šçº§è§†è§‰ä¿¡æ¯çš„æ–°èŒƒå¼ï¼Œä¸ºæ™ºèƒ½å·¥ä¸šæ£€æµ‹ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¼€è¾Ÿäº†å¤§æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸåº”ç”¨çš„æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.


### [5] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong, James Fort, Aria Kang, Jonathan Wittmer, Simon Green, Tianwei Shen, Yipu Zhao, Cheng Peng, Gustavo Solaira, Andrew Berkovich, Nikhil Raina, Vijay Baiyya, Evgeniy Oleinik, Eric Huang, Fan Zhang, Julian Straub, Mark Schwesinger, Luis Pesqueira, Xiaqing Pan, Jakob Julian Engel, Carl Ren, Mingfei Yan, Richard Newcombe*

#### ğŸ§© TL;DR
Aria Gen 2 Pilot Dataset (A2PD) æ˜¯ä¸€ä¸ªä½¿ç”¨å…ˆè¿›Aria Gen 2çœ¼é•œé‡‡é›†çš„è‡ªæˆ‘ä¸­å¿ƒå¤šæ¨¡æ€å¼€æ”¾æ•°æ®é›†ï¼Œé€šè¿‡å¢é‡å‘å¸ƒæ–¹å¼æä¾›åŒ…å«å¤šç§æ—¥å¸¸åœºæ™¯çš„åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®å’Œæœºå™¨æ„ŸçŸ¥ç®—æ³•è¾“å‡ºï¼Œæ”¯æŒå¯¹ä½©æˆ´è€…ã€ç¯å¢ƒå’Œäº¤äº’çš„å…¨é¢æ„ŸçŸ¥ç ”ç©¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è‡ªæˆ‘ä¸­å¿ƒè§†è§‰å’Œå¤šæ¨¡æ€æ„ŸçŸ¥é¢†åŸŸç¼ºä¹å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çœŸå®åœºæ™¯æ•°æ®é›†çš„é—®é¢˜ï¼Œé€šè¿‡æä¾›åŒ…å«æ¸…æ´ã€çƒ¹é¥ªã€è¿›é£Ÿã€æ¸¸æˆå’Œæˆ·å¤–æ­¥è¡Œç­‰äº”ç§ä¸»è¦åœºæ™¯çš„å…¨é¢æ•°æ®ï¼Œå¡«è¡¥äº†ç°æœ‰æ•°æ®é›†åœ¨ç”¨æˆ·å¤šæ ·æ€§å’Œç¯å¢ƒå¤æ‚æ€§æ–¹é¢çš„ä¸è¶³ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨Aria Gen 2çœ¼é•œè®¾å¤‡é‡‡é›†å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®ï¼Œé€šè¿‡å¢é‡å‘å¸ƒç­–ç•¥æŒç»­æ‰©å±•æ•°æ®é›†è§„æ¨¡ï¼ŒåŒ…å«åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®å’Œå¤šç§æœºå™¨æ„ŸçŸ¥ç®—æ³•çš„è¾“å‡ºç»“æœï¼Œæ¶µç›–äº†ä½©æˆ´è€…çŠ¶æ€ã€ç¯å¢ƒæ„ŸçŸ¥ä»¥åŠäººæœºäº¤äº’ç­‰å¤šä¸ªç»´åº¦çš„ä¿¡æ¯ã€‚

**Result:** æ•°æ®é›†å±•ç¤ºäº†è®¾å¤‡åœ¨ä¸åŒç”¨æˆ·å’Œæ¡ä»¶ä¸‹ä¿æŒé²æ£’æ€§èƒ½çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ„ŸçŸ¥ä½©æˆ´è€…çŠ¶æ€ã€å‘¨å›´ç¯å¢ƒä»¥åŠä½©æˆ´è€…ä¸ç¯å¢ƒä¹‹é—´çš„äº¤äº’å…³ç³»ï¼Œä¸ºå¤šæ¨¡æ€æ„ŸçŸ¥ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„çœŸå®ä¸–ç•ŒåŸºå‡†æ•°æ®ã€‚

**Conclusion:** A2PDæ•°æ®é›†ä¸ºè‡ªæˆ‘ä¸­å¿ƒè§†è§‰å’Œå¤šæ¨¡æ€æ„ŸçŸ¥ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†èµ„æºï¼Œå…¶å¼€æ”¾è·å–ç­–ç•¥å’Œé…å¥—å·¥å…·å°†ä¿ƒè¿›ç›¸å…³é¢†åŸŸçš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨çœŸå®ç¯å¢ƒä¸‹çš„æ„ŸçŸ¥ç®—æ³•è¯„ä¼°å’Œè·¨ç”¨æˆ·æ³›åŒ–èƒ½åŠ›ç ”ç©¶æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.


### [6] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui, Rohit Gupta, Sirnam Swetha, Mubarak Shah*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§çµæ´»çš„æ—¶ç©ºè‡ªé€‚åº”è®­ç»ƒæ–¹æ³•StretchySnakeï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡è®­ç»ƒè§†é¢‘çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œè§£å†³äº†ä¼ ç»Ÿè§†é¢‘æ¨¡å‹åœ¨æœªè§æ—¶ç©ºåˆ†è¾¨ç‡ä¸‹çš„æ€§èƒ½é€€åŒ–é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŠ¨ä½œè¯†åˆ«åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºTransformerå’ŒSSMåŸºçº¿ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†é¢‘ç†è§£è®­ç»ƒæ–¹æ³•ä¸»è¦é’ˆå¯¹Transformerè®¾è®¡ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹(SSMs)çš„ç‹¬ç‰¹å±æ€§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹è®­ç»ƒæ—¶æœªè§è¿‡çš„ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡æ—¶å‡ºç°æ€§èƒ½é€€åŒ–ï¼Œè¿™ç§æ—¶ç©ºä¸çµæ´»æ€§ä¸¥é‡é™åˆ¶äº†æ¨¡å‹åœ¨é•¿çŸ­è§†é¢‘ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

**Method:** æå‡ºçµæ´»è®­ç»ƒæ–¹æ³•ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡æ ·ä¸åŒæ—¶ç©ºåˆ†è¾¨ç‡çš„è§†é¢‘ï¼Œå¹¶åŠ¨æ€æ’å€¼æ¨¡å‹æƒé‡ä»¥é€‚åº”ä»»æ„æ—¶ç©ºå°ºåº¦ï¼Œå¼€å‘äº†äº”ç§çµæ´»è®­ç»ƒå˜ä½“å¹¶ç¡®å®šäº†æœ€é€‚åˆè§†é¢‘SSMçš„ç­–ç•¥ï¼Œæ„å»ºäº†åä¸ºStretchySnakeçš„æ—¶ç©ºè‡ªé€‚åº”SSMæ¨¡å‹ã€‚

**Result:** åœ¨çŸ­åŠ¨ä½œåŸºå‡†(UCF-101ã€HMDB-51)å’Œé•¿åŠ¨ä½œåŸºå‡†(COINã€Breakfast)ä¸Šï¼ŒStretchySnakeæ¯”Transformerå’ŒSSMåŸºçº¿æ€§èƒ½æå‡é«˜è¾¾28%ï¼Œåœ¨ç»†ç²’åº¦åŠ¨ä½œæ•°æ®é›†(SSV2ã€Diving-48)ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§ã€‚

**Conclusion:** è¯¥æ–¹æ³•æä¾›äº†ä¸€ä¸ªç®€å•çš„å³æ’å³ç”¨è®­ç»ƒæ–¹æ¡ˆï¼Œä½¿è§†é¢‘SSMåœ¨å„ç§åŠ¨ä½œè¯†åˆ«åœºæ™¯ä¸­æ›´åŠ é²æ£’ã€åˆ†è¾¨ç‡æ— å…³ä¸”é«˜æ•ˆï¼Œä¸ºè§†é¢‘ç†è§£æ¨¡å‹æä¾›äº†æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œå……åˆ†åˆ©ç”¨äº†SSMçš„çº¿æ€§å¤æ‚åº¦å’Œéšè—çŠ¶æ€é€’å½’ä¼˜åŠ¿ã€‚

---

#### ğŸ“„ Abstract
State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.


### [7] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean, Makenzie Meendering, Tristan Swartz, Orri Gabbay, Alexandra Olsen, Rachel Jacobs, Nicholas Rosen, Philippe de Bree, Tony Garcia, Gadsden Merrill, Jake Sandakly, Julia Buffalini, Neham Jain, Steven Krenn, Moneish Kumar, Dejan Markovic, Evonne Ng, Fabian Prada, Andrew Saba, Siwei Zhang, Vasu Agrawal, Tim Godisart, Alexander Richard, Michael Zollhoefer*

#### ğŸ§© TL;DR
Embody 3Dæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€3Dè¿åŠ¨æ•°æ®é›†ï¼ŒåŒ…å«500å°æ—¶æ¥è‡ª439åå‚ä¸è€…çš„3Dè¿åŠ¨æ•°æ®ï¼Œæ¶µç›–å•äººå¤šäººå’Œäº¤äº’è¡Œä¸ºï¼Œä¸ºäººä½“è¿åŠ¨åˆ†æå’Œè¡Œä¸ºç†è§£ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å¤šæ¨¡æ€3Däººä½“è¿åŠ¨æ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šäººäº¤äº’å’Œè¡Œä¸ºåˆ†ææ–¹é¢å­˜åœ¨æ•°æ®ç©ºç™½ï¼Œé™åˆ¶äº†ç›¸å…³ç®—æ³•çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚

**Method:** é€šè¿‡å¤šç›¸æœºé‡‡é›†ç³»ç»Ÿæ”¶é›†439åå‚ä¸è€…çš„3Dè¿åŠ¨æ•°æ®ï¼ŒåŒ…æ‹¬å•äººè¿åŠ¨ã€æ‰‹åŠ¿ã€ç§»åŠ¨ä»¥åŠå¤šäººå¯¹è¯ã€åä½œæ´»åŠ¨å’Œå…±åŒç”Ÿæ´»åœºæ™¯ï¼Œå¹¶æä¾›è¿åŠ¨è¿½è¸ªã€èº«ä½“å½¢æ€ã€æ–‡æœ¬æ ‡æ³¨å’Œç‹¬ç«‹éŸ³é¢‘è½¨é“ã€‚

**Result:** æ„å»ºäº†åŒ…å«500å°æ—¶æ•°æ®ã€è¶…è¿‡5400ä¸‡å¸§3Dè¿åŠ¨è¿½è¸ªçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–å¹¿æ³›çš„è¿åŠ¨ç±»å‹å’Œäº¤äº’åœºæ™¯ï¼Œä¸ºå¤šæ¨¡æ€è¡Œä¸ºåˆ†ææä¾›äº†å…¨é¢åŸºå‡†ã€‚

**Conclusion:** Embody 3Dæ•°æ®é›†å¡«è¡¥äº†3Däººä½“è¿åŠ¨æ•°æ®åœ¨è§„æ¨¡å’Œå¤šæ ·æ€§æ–¹é¢çš„ç©ºç™½ï¼Œä¸ºè®¡ç®—æœºè§†è§‰ã€äººæœºäº¤äº’å’Œè¡Œä¸ºåˆ†æç­‰é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€è®¾æ–½ï¼Œå°†æ¨åŠ¨ç›¸å…³ç®—æ³•çš„å‘å±•å’Œåº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.


### [8] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng, Xiufang Shi, Jiming Chen, Yuanchao Shu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Cerberusï¼Œä¸€ç§ç”¨äºå®æ—¶è§†é¢‘å¼‚å¸¸æ£€æµ‹çš„ä¸¤çº§çº§è”ç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆè½»é‡çº§è¿‡æ»¤å’Œç»†ç²’åº¦è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶å®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•è™½ç„¶å…·æœ‰ä¼˜è¶Šçš„é›¶æ ·æœ¬æ£€æµ‹èƒ½åŠ›ï¼Œä½†å…¶å·¨å¤§çš„è®¡ç®—æˆæœ¬å’Œä¸ç¨³å®šçš„è§†è§‰å®šä½æ€§èƒ½é˜»ç¢äº†å®æ—¶éƒ¨ç½²åº”ç”¨ã€‚

**Method:** Cerberusé‡‡ç”¨ä¸¤çº§çº§è”æ¶æ„ï¼ŒåŒ…æ‹¬ç¦»çº¿å­¦ä¹ æ­£å¸¸è¡Œä¸ºè§„åˆ™å’Œåœ¨çº¿æ¨ç†æ—¶çš„è½»é‡çº§è¿‡æ»¤ä¸ç»†ç²’åº¦VLMæ¨ç†ç›¸ç»“åˆï¼Œå…³é”®åˆ›æ–°åŒ…æ‹¬è¿åŠ¨æ©ç æç¤ºå’ŒåŸºäºè§„åˆ™çš„åå·®æ£€æµ‹ã€‚

**Result:** åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒCerberusåœ¨NVIDIA L40S GPUä¸Šå¹³å‡è¾¾åˆ°57.68 fpsï¼Œå®ç°äº†151.79å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒ97.2%çš„å‡†ç¡®ç‡ï¼Œä¸æœ€å…ˆè¿›çš„VLM-based VADæ–¹æ³•ç›¸å½“ã€‚

**Conclusion:** Cerberusé€šè¿‡åˆ›æ–°çš„è¿åŠ¨æ³¨æ„åŠ›å¼•å¯¼å’Œè§„åˆ™åå·®æ£€æµ‹æœºåˆ¶ï¼Œè¯æ˜äº†åœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶å®ç°å®æ—¶è§†é¢‘å¼‚å¸¸æ£€æµ‹çš„å¯è¡Œæ€§ï¼Œä¸ºå®é™…è§†é¢‘åˆ†æåº”ç”¨æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.


### [9] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto, Xin Fan, Fuyuko Kido, Tsuneo Matsumoto, Hayato Yamana*

#### ğŸ§© TL;DR
OpenLVLM-MIAæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†è¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æˆå‘˜æ¨ç†æ”»å‡»æ—¶å­˜åœ¨çš„æ ¹æœ¬æ€§æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°å…ˆå‰æŠ¥é“çš„é«˜æ”»å‡»æˆåŠŸç‡ä¸»è¦æºäºæ•°æ®é›†æ„å»ºå¼•å…¥çš„åˆ†å¸ƒåå·®æ£€æµ‹ï¼Œè€ŒéçœŸå®çš„æˆå‘˜çŠ¶æ€è¯†åˆ«ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æˆå‘˜æ¨ç†æ”»å‡»æ—¶å­˜åœ¨çš„æ ¹æœ¬é—®é¢˜ã€‚å…ˆå‰å·¥ä½œæŠ¥å‘Šçš„é«˜æ”»å‡»æˆåŠŸç‡å¯èƒ½æºäºæ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­å¼•å…¥çš„åˆ†å¸ƒåå·®ï¼Œè€ŒéçœŸå®çš„æˆå‘˜çŠ¶æ€è¯†åˆ«èƒ½åŠ›ï¼Œè¿™å¯¼è‡´äº†å¯¹æ”»å‡»æ–¹æ³•çœŸå®æ€§èƒ½çš„è¯¯è§£ã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†åŒ…å«6,000å¼ å›¾åƒçš„å—æ§åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­æˆå‘˜å’Œéæˆå‘˜æ ·æœ¬çš„åˆ†å¸ƒç»è¿‡ç²¾å¿ƒå¹³è¡¡ï¼Œå¹¶åœ¨ä¸‰ä¸ªä¸åŒçš„è®­ç»ƒé˜¶æ®µæä¾›äº†çœŸå®æˆå‘˜æ ‡ç­¾ã€‚è¯¥åŸºå‡†é€šè¿‡æ¶ˆé™¤åˆ†å¸ƒåå·®æ¥ç¡®ä¿è¯„ä¼°çš„å…¬æ­£æ€§ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ— åæ¡ä»¶ä¸‹ï¼Œæœ€å…ˆè¿›çš„æˆå‘˜æ¨ç†æ”»å‡»æ–¹æ³•çš„æ€§èƒ½æ”¶æ•›äºéšæœºçŒœæµ‹æ°´å¹³ã€‚è¿™è¯å®äº†å…ˆå‰æŠ¥é“çš„é«˜æ”»å‡»æˆåŠŸç‡ä¸»è¦æºäºæ•°æ®é›†åå·®è€ŒéçœŸå®çš„æ”»å‡»èƒ½åŠ›ã€‚

**Conclusion:** OpenLVLM-MIAåŸºå‡†æ­ç¤ºäº†å½“å‰LVLMæˆå‘˜æ¨ç†æ”»å‡»ç ”ç©¶çš„å±€é™æ€§ï¼Œä¸ºå¼€å‘æ›´å¼ºçš„éšç§ä¿æŠ¤æŠ€æœ¯æä¾›äº†åšå®åŸºç¡€ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨è¯„ä¼°éšç§æ”»å‡»æ–¹æ³•æ—¶æ¶ˆé™¤æ•°æ®é›†åå·®çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.


### [10] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang, Yu-Wing Tai*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Scale-DiTï¼Œä¸€ç§æ–°çš„æ‰©æ•£æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚å±€éƒ¨æ³¨æ„åŠ›ä¸ä½åˆ†è¾¨ç‡å…¨å±€å¼•å¯¼ç›¸ç»“åˆï¼Œå®ç°äº†è¶…é«˜æ¸…å›¾åƒçš„é«˜æ•ˆç”Ÿæˆï¼Œæ— éœ€é¢å¤–çš„é«˜åˆ†è¾¨ç‡è®­ç»ƒæ•°æ®å³å¯æ‰©å±•åˆ°4Kåˆ†è¾¨ç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ‰©æ•£æ¨¡å‹å—é™äºæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§å’ŒåŸç”Ÿ4Kè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºï¼Œæ— æ³•å®ç°è¶…é«˜æ¸…æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨ç»†ç²’åº¦çº¹ç†åˆæˆå’Œå…¨å±€ç»“æ„ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚

**Method:** Scale-DiTé‡‡ç”¨åˆ†å±‚å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†é«˜åˆ†è¾¨ç‡æ½œå˜é‡åˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„å±€éƒ¨çª—å£ä»¥é™ä½æ³¨æ„åŠ›å¤æ‚åº¦ï¼ŒåŒæ—¶ä½¿ç”¨é…å¤‡ç¼©æ”¾ä½ç½®é”šç‚¹çš„ä½åˆ†è¾¨ç‡æ½œå˜é‡æ³¨å…¥å…¨å±€è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é€šè¿‡è½»é‡çº§LoRAé€‚é…å™¨åœ¨å»å™ªè¿‡ç¨‹ä¸­æ¡¥æ¥å…¨å±€å’Œå±€éƒ¨è·¯å¾„ã€‚

**Result:** å®éªŒè¡¨æ˜Scale-DiTç›¸æ¯”å¯†é›†æ³¨æ„åŠ›åŸºçº¿å®ç°äº†2å€ä»¥ä¸Šçš„æ¨ç†åŠ é€Ÿå’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ï¼Œèƒ½å¤Ÿåœ¨4KÃ—4Kåˆ†è¾¨ç‡ä¸‹ç”Ÿæˆå…·æœ‰ä¼˜è¶Šå…¨å±€ä¸€è‡´æ€§å’Œæ›´æ¸…æ™°å±€éƒ¨ç»†èŠ‚çš„å›¾åƒï¼Œåœ¨FIDã€ISå’ŒCLIP Scoreç­‰å®šé‡æŒ‡æ ‡ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šä¾èµ–åŸç”Ÿ4Kè®­ç»ƒçš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜åˆ†å±‚å±€éƒ¨æ³¨æ„åŠ›ä¸å¼•å¯¼æ€§ä½åˆ†è¾¨ç‡é”šç‚¹ç›¸ç»“åˆæ˜¯æ¨è¿›è¶…é«˜æ¸…å›¾åƒç”Ÿæˆçš„æœ‰æ•ˆæ–¹æ³•ï¼Œä¸ºæ— éœ€é¢å¤–é«˜åˆ†è¾¨ç‡è®­ç»ƒæ•°æ®çš„å¤§è§„æ¨¡å›¾åƒåˆæˆæä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.


### [11] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song, Sangdoo Yun, Dongyoon Han, Jaegul Choo, Byeongho Heo*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æ­ç¤ºäº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒç­–ç•¥èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­è§†è§‰ç¼–ç å™¨çš„è¡¨å¾èƒ½åŠ›ï¼Œæå‡ºäº†PIVOTæ–¹æ³•ï¼Œä»…éœ€æ ‡å‡†è§†è§‰é¢„è®­ç»ƒ1%çš„è®¡ç®—æˆæœ¬å³å¯æ„å»ºæ›´å¼ºå¤§çš„è§†è§‰ç¼–ç å™¨ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ç ”ç©¶æ™®éå‡è®¾æ€§èƒ½ä¸»è¦ç»§æ‰¿è‡ªLLMéª¨å¹²ç½‘ç»œï¼Œå¯¼è‡´å¯¹è§†è§‰ç¼–ç å™¨çš„ç†è§£å­˜åœ¨ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒèŒƒå¼ä»ç›‘ç£å¾®è°ƒè½¬å‘å¼ºåŒ–å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œç¼ºä¹å¯¹è®­ç»ƒç­–ç•¥å¦‚ä½•é‡å¡‘è§†è§‰ç¼–ç å™¨çš„ç³»ç»Ÿåˆ†æã€‚

**Method:** é€šè¿‡å¤šæ ·åŒ–çš„æ·±åº¦å®éªŒåˆ†æè®­ç»ƒç­–ç•¥å¯¹è§†è§‰ç¼–ç å™¨çš„å½±å“ï¼ŒåŒ…æ‹¬ImageNetåˆ†ç±»ã€åˆ†å‰²ä»»åŠ¡å’Œæ¢¯åº¦å¯è§†åŒ–ï¼Œå¹¶åŸºäºç ”ç©¶å‘ç°æå‡ºäº†Preference-Instructed Vision OpTimization (PIVOT)æ–¹æ³•ã€‚

**Result:** å¼ºåŒ–å­¦ä¹ ç›¸æ¯”ç›‘ç£å¾®è°ƒåœ¨å¼ºè§†è§‰ç›¸å…³çš„VQAåŸºå‡†ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ›´å¼ºä¸”ç²¾ç¡®å®šä½çš„è§†è§‰è¡¨å¾ï¼ŒPIVOTè®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å³ä½¿è®¡ç®—æˆæœ¬ä¸è¶³æ ‡å‡†é¢„è®­ç»ƒçš„1%ï¼Œä¹Ÿèƒ½è¶…è¶Šæ›´å¤§è§„æ¨¡è®­ç»ƒçš„å¯¹ç­‰æ¨¡å‹ã€‚

**Conclusion:** è®­ç»ƒç­–ç•¥ä¸ä»…å½±å“å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œè¿˜ä»æ ¹æœ¬ä¸Šé‡å¡‘å…¶åº•å±‚è§†è§‰è¡¨å¾ï¼ŒPIVOTæ–¹æ³•ä¸ºæ¨è¿›å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„è§†è§‰éª¨å¹²ç½‘ç»œæä¾›äº†ä¸€æ¡é«˜æ•ˆè·¯å¾„ï¼ŒæŒ‘æˆ˜äº†å½“å‰å¯¹è§†è§‰ç¼–ç å™¨ä½œç”¨çš„ä½ä¼°è®¤çŸ¥ã€‚

---

#### ğŸ“„ Abstract
A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/


### [12] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng, Jie Lu, Guangquan Zhang, Zhen Fang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†GradNormè¿™ä¸€åŸºäºæ¢¯åº¦çš„æ¡†æ¶æ¥è§£å†³è¯­è¨€è¾…åŠ©å›¾åƒèšç±»ä¸­çš„æ­£åè¯ç­›é€‰é—®é¢˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰ç†è®ºä¿è¯å¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„èšç±»æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯­è¨€è¾…åŠ©å›¾åƒèšç±»ï¼ˆLaICï¼‰é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯å¦‚ä½•ä»æœªæ ‡æ³¨çš„é‡ç”Ÿè¯­æ–™æ•°æ®ä¸­ç­›é€‰å‡ºä¸ç›®æ ‡å›¾åƒè¯­ä¹‰ç›¸è¿‘çš„æ­£åè¯ï¼Œç°æœ‰åŸºäºCLIPç‰¹å¾ç©ºé—´çš„ç­›é€‰ç­–ç•¥è™½ç„¶ç›´è§‚ä½†ç¼ºä¹ä¸¥æ ¼çš„ç†è®ºåŸºç¡€ã€‚

**Method:** æå‡ºäº†åä¸ºGradNormçš„æ¢¯åº¦æ¡†æ¶ï¼Œé€šè¿‡è®¡ç®—é¢„æµ‹ç›®æ ‡åˆ†å¸ƒä¸softmaxè¾“å‡ºä¹‹é—´çš„äº¤å‰ç†µåå‘ä¼ æ’­æ¢¯åº¦å¤§å°æ¥è¡¡é‡æ¯ä¸ªåè¯çš„æ­£æ€§ç¨‹åº¦ï¼Œè¯¥æ–¹æ³•åœ¨ç†è®ºä¸Šèƒ½å¤ŸåŒ…å«ç°æœ‰ç­›é€‰ç­–ç•¥ä½œä¸ºå…¶ç‰¹ä¾‹ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜GradNormåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„èšç±»æ€§èƒ½ï¼Œç†è®ºåˆ†ææä¾›äº†æ­£åè¯å¯åˆ†æ€§çš„ä¸¥æ ¼è¯¯å·®ç•Œè¯æ˜ã€‚

**Conclusion:** GradNormä¸ä»…ä¸ºè¯­è¨€è¾…åŠ©å›¾åƒèšç±»æä¾›äº†ç†è®ºä¸¥è°¨çš„è§£å†³æ–¹æ¡ˆï¼Œè¿˜è¯æ˜äº†ç°æœ‰æ–¹æ³•åªæ˜¯è¯¥æ¡†æ¶çš„æç«¯ç‰¹ä¾‹ï¼Œä¸ºæœªæ¥ç›¸å…³ç ”ç©¶å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.


### [13] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li, Guocheng Wu, Li Yin, Yuxin Zheng, Wei Zhang, Yanjie Zhou*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªä¸“é—¨é’ˆå¯¹ç¤¾ä¼šåˆ¶é€ åœºæ™¯çš„å¼‚å¸¸æ£€æµ‹åŸºå‡†æ•°æ®é›†MIRADï¼Œè¯¥æ•°æ®é›†æ•æ‰äº†ä¸ªæ€§åŒ–ç”Ÿäº§ä¸­çš„ä¸‰ä¸ªå…³é”®ç»´åº¦æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡å¹¿æ³›å®éªŒæ­ç¤ºäº†ç°æœ‰å…ˆè¿›æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œä¸ªæ€§åŒ–ç”Ÿäº§ç¼ºé™·æ£€æµ‹ä¸­çš„æ˜¾è‘—æ€§èƒ½ä¸‹é™ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç¤¾ä¼šåˆ¶é€ èŒƒå¼é€šè¿‡ç¤¾åŒºåä½œå’Œåˆ†æ•£èµ„æºå®ç°å¤§è§„æ¨¡ä¸ªæ€§åŒ–ç”Ÿäº§ï¼Œä½†è¿™ä¸€è½¬å˜åœ¨è´¨é‡æ§åˆ¶ç‰¹åˆ«æ˜¯ç¼ºé™·æ£€æµ‹æ–¹é¢å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦å›°éš¾æºäºäº§å“é«˜åº¦å®šåˆ¶åŒ–é…ç½®ã€ç”Ÿäº§æ¶‰åŠç¢ç‰‡åŒ–å°æ‰¹é‡è®¢å•ä»¥åŠåˆ†å¸ƒå¼ç«™ç‚¹æˆåƒç¯å¢ƒå·®å¼‚æ˜¾è‘—ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹é’ˆå¯¹æ€§çš„çœŸå®ä¸–ç•Œæ•°æ®é›†å’Œç®—æ³•ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†Mass Individualization Robust Anomaly Detection (MIRAD)æ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ä¸ºç¤¾ä¼šåˆ¶é€ å¼‚å¸¸æ£€æµ‹è®¾è®¡çš„åŸºå‡†æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ•æ‰äº†ä¸ªæ€§åŒ–ç”Ÿäº§ä¸­çš„ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šå…·æœ‰å¤§ç±»å†…å˜å¼‚çš„å¤šæ ·åŒ–ä¸ªæ€§åŒ–äº§å“ã€æ¥è‡ªå…­ä¸ªåœ°ç†åˆ†æ•£åˆ¶é€ èŠ‚ç‚¹çš„æ•°æ®æ”¶é›†ä»¥åŠåŒ…å«å…‰ç…§ã€èƒŒæ™¯å’Œè¿åŠ¨æ¡ä»¶å˜åŒ–çš„æ˜¾è‘—æˆåƒå¼‚è´¨æ€§ã€‚

**Result:** åœ¨MIRADæ•°æ®é›†ä¸Šå¯¹åŒ…æ‹¬å•ç±»ã€å¤šç±»å’Œé›¶æ ·æœ¬æ–¹æ³•åœ¨å†…çš„æœ€å…ˆè¿›å¼‚å¸¸æ£€æµ‹æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹ç›¸æ¯”ä¼ ç»ŸåŸºå‡†éƒ½å‡ºç°äº†æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œè¿™çªæ˜¾äº†çœŸå®ä¸–ç•Œä¸ªæ€§åŒ–ç”Ÿäº§ä¸­ç¼ºé™·æ£€æµ‹å°šæœªè§£å†³çš„å¤æ‚æ€§æŒ‘æˆ˜ã€‚

**Conclusion:** MIRADæ•°æ®é›†é€šè¿‡è¿æ¥å·¥ä¸šéœ€æ±‚å’Œå­¦æœ¯ç ”ç©¶ï¼Œä¸ºå¼€å‘å·¥ä¸š5.0æ‰€éœ€çš„é²æ£’è´¨é‡æ§åˆ¶è§£å†³æ–¹æ¡ˆæä¾›äº†ç°å®åŸºç¡€ï¼Œæ­ç¤ºäº†ç°æœ‰å¼‚å¸¸æ£€æµ‹æ–¹æ³•åœ¨åº”å¯¹ä¸ªæ€§åŒ–ç”Ÿäº§åœºæ™¯æ—¶çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ”¹è¿›æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.


### [14] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi, Minghao Chen, Yiping Mao, Chuxiao Yang, Xinyuan Hu, Jiajun Ding, Zhou Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºREALMæ¡†æ¶ï¼Œé€šè¿‡å°†3Dé«˜æ–¯æ³¼æº…è¡¨ç¤ºä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œå®ç°äº†æ— éœ€å¤§é‡3Dç‰¹å®šåè®­ç»ƒå³å¯è¿›è¡Œå¼€æ”¾ä¸–ç•Œæ¨ç†åˆ†å‰²çš„åˆ›æ–°æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰3Dåˆ†å‰²æ–¹æ³•éš¾ä»¥å¤„ç†æ¨¡ç³Šçš„æ¨ç†å¼æŒ‡ä»¤ï¼Œè€Œæ“…é•¿æ­¤ç±»æ¨ç†çš„2Dè§†è§‰è¯­è¨€æ¨¡å‹åˆç¼ºä¹å†…åœ¨çš„3Dç©ºé—´ç†è§£èƒ½åŠ›ï¼Œè¿™æ„æˆäº†å¤æ‚äººç±»æŒ‡ä»¤ä¸ç²¾ç¡®3Dç‰©ä½“å®šä½ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚

**Method:** REALMç›´æ¥åœ¨3Dé«˜æ–¯æ³¼æº…è¡¨ç¤ºä¸Šæ‰§è¡Œåˆ†å‰²ï¼Œåˆ©ç”¨å…¶æ¸²æŸ“é€¼çœŸæ–°è§†è§’çš„èƒ½åŠ›ï¼Œå¹¶æå‡ºå…¨å±€åˆ°å±€éƒ¨ç©ºé—´å®šä½ç­–ç•¥ï¼šé¦–å…ˆå¹¶è¡Œè¾“å…¥å¤šä¸ªå…¨å±€è§†å›¾è¿›è¡Œç²—ç²’åº¦å®šä½ï¼Œç„¶ååˆæˆå¤šä¸ªç‰©ä½“ç‰¹å†™è§†å›¾è¿›è¡Œç»†ç²’åº¦å±€éƒ¨åˆ†å‰²ã€‚

**Result:** åœ¨LERFã€3D-OVSå’Œæ–°å¼•å…¥çš„REALM3DåŸºå‡†æµ‹è¯•ä¸­ï¼ŒREALMåœ¨è§£é‡Šæ˜¾å¼å’Œéšå¼æŒ‡ä»¤æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶è¯¥æ¡†æ¶æ— ç¼æ”¯æŒç‰©ä½“ç§»é™¤ã€æ›¿æ¢å’Œé£æ ¼è½¬æ¢ç­‰å¤šç§3Däº¤äº’ä»»åŠ¡ã€‚

**Conclusion:** REALMå±•ç¤ºäº†å°†å…ˆè¿›2Dè§†è§‰è¯­è¨€æ¨¡å‹èƒ½åŠ›æœ‰æ•ˆè¿ç§»åˆ°3Dç©ºé—´çš„å¯è¡Œæ€§ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œ3Dåœºæ™¯ç†è§£å’Œäº¤äº’æä¾›äº†å®ç”¨ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.


### [15] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo, Runyu Zhou, Yifei Wang, Qi Zhang, Chenheng Zhang, Stefanie Jegelka, Xiaohan Wang, Jiajun Chai, Guojun Yin, Wei Lin, Yisen Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSSL4RLæ¡†æ¶ï¼Œé€šè¿‡å°†è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡è½¬åŒ–ä¸ºå¯éªŒè¯çš„å¥–åŠ±ä¿¡å·ï¼Œè§£å†³äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­ç¼ºä¹å¯æ‰©å±•å¯é å¥–åŠ±æœºåˆ¶çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨è§†è§‰ä¸­å¿ƒå’Œè§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åˆ©ç”¨è§†è§‰è¯æ®æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¦ä¹ˆè¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒï¼Œè¦ä¹ˆä½¿ç”¨æ–‡æœ¬æ·å¾„è¿›è¡Œæ¨ç†ï¼Œè€Œå¼ºåŒ–å­¦ä¹ åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨å› ç¼ºä¹å¯æ‰©å±•å¯é çš„å¥–åŠ±æœºåˆ¶è€Œå—åˆ°é™åˆ¶ã€‚

**Method:** æå‡ºSSL4RLæ¡†æ¶ï¼Œå°†è‡ªç›‘ç£å­¦ä¹ ç›®æ ‡ï¼ˆå¦‚å›¾åƒæ—‹è½¬é¢„æµ‹ã€æ©ç è¡¥ä¸é‡å»ºï¼‰é‡æ–°è¡¨è¿°ä¸ºå¯†é›†çš„è‡ªåŠ¨å¥–åŠ±ä¿¡å·ï¼Œæ— éœ€äººå·¥åå¥½æ•°æ®æˆ–ä¸å¯é çš„AIè¯„ä¼°å™¨ï¼Œä¸ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæä¾›å¯éªŒè¯çš„å¥–åŠ±æ¥æºã€‚

**Result:** å®éªŒè¡¨æ˜SSL4RLåœ¨è§†è§‰ä¸­å¿ƒå’Œè§†è§‰è¯­è¨€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œé€šè¿‡ç³»ç»Ÿæ¶ˆèç ”ç©¶è¯†åˆ«å‡ºä»»åŠ¡éš¾åº¦ã€æ¨¡å‹è§„æ¨¡å’Œä¸ç›®æ ‡é¢†åŸŸè¯­ä¹‰å¯¹é½ç­‰å…³é”®å½±å“å› ç´ ï¼Œå¹¶åœ¨å›¾å­¦ä¹ é¢†åŸŸéªŒè¯äº†æ¡†æ¶çš„é€šç”¨æ€§ã€‚

**Conclusion:** SSL4RLå»ºç«‹äº†ä¸€ä¸ªé€šç”¨æœ‰æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹å¯¹é½èŒƒå¼ï¼Œä½¿ç”¨å¯éªŒè¯çš„è‡ªç›‘ç£ç›®æ ‡ï¼Œä¸ºæœªæ¥å·¥ä½œæä¾›äº†æ–°çš„è®¾è®¡åŸåˆ™ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨ä¸åŒé¢†åŸŸçš„é€‚ç”¨æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.


### [16] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun, Chen Cai, Huiping Zhuang, Kong Aik Lee, Lap-Pui Chau, Yi Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºå¯è§£é‡Šæ·±åº¦ä¼ªé€ è§†é¢‘æ£€æµ‹ä»»åŠ¡EDVDï¼Œå¹¶è®¾è®¡EDVD-LLaMAå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æä¾›å‡†ç¡®æ£€æµ‹ç»“æœçš„åŒæ—¶ï¼Œæä¾›å¯è¿½æº¯çš„æ¨ç†è¿‡ç¨‹å’Œå¯ä¿¡çš„è§£é‡Šã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿæ·±åº¦ä¼ªé€ è§†é¢‘æ£€æµ‹æ–¹æ³•å­˜åœ¨åŸç†é€æ˜åº¦ä¸è¶³å’Œå¯¹ä¸æ–­å‘å±•çš„ä¼ªé€ æŠ€æœ¯æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œè¿«åˆ‡éœ€è¦èƒ½å¤Ÿè¯†åˆ«ä¼ªé€ å†…å®¹å¹¶æä¾›å¯éªŒè¯æ¨ç†è§£é‡Šçš„æ£€æµ‹å™¨ã€‚

**Method:** æå‡ºEDVD-LLaMAå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ï¼ŒåŒ…å«æ—¶ç©ºç»†å¾®ä¿¡æ¯æ ‡è®°åŒ–ST-SITæ¥æå–å’Œèåˆå…¨å±€å±€éƒ¨è·¨å¸§æ·±åº¦ä¼ªé€ ç‰¹å¾ï¼Œä»¥åŠç»†ç²’åº¦å¤šæ¨¡æ€æ€ç»´é“¾Fg-MCoTæœºåˆ¶ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥é¢éƒ¨ç‰¹å¾æ•°æ®ä½œä¸ºç¡¬çº¦æŸä»¥å®ç°åƒç´ çº§æ—¶ç©ºè§†é¢‘å®šä½ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜EDVD-LLaMAåœ¨æ£€æµ‹å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§ä»¥åŠå¤„ç†è·¨ä¼ªé€ æ–¹æ³•å’Œè·¨æ•°æ®é›†åœºæ™¯æ–¹é¢å®ç°äº†å‡ºè‰²çš„æ€§èƒ½å’Œé²æ£’æ€§ï¼Œç›¸æ¯”å…ˆå‰DVDæ–¹æ³•æä¾›äº†æ›´å¯è§£é‡Šä¸”ä¼˜è¶Šçš„è§£å†³æ–¹æ¡ˆã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºæ·±åº¦ä¼ªé€ æ£€æµ‹é¢†åŸŸæä¾›äº†æ›´é€æ˜å’Œå¯ä¿¡çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å¤šæ¨¡æ€æ¨ç†æ¡†æ¶å®ç°äº†æ£€æµ‹ç»“æœçš„å¯è¿½æº¯è§£é‡Šï¼Œå¢å¼ºäº†æ£€æµ‹ç³»ç»Ÿçš„å¯é æ€§å’Œå®ç”¨æ€§ã€‚

---

#### ğŸ“„ Abstract
The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.


### [17] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng, Di Wen, Jia Fu, Jiamin Wu, Kailun Yang, Junwei Zheng, Ruiping Liu, Yufan Chen, Yuqian Fu, Danda Pani Paudel, Luc Van Gool, Rainer Stiefelhagen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†RefAtomNet++æ¡†æ¶å’ŒRefAVA++æ•°æ®é›†ï¼Œç”¨äºè§£å†³åŸºäºè¯­è¨€æè¿°çš„åŸå­çº§è§†é¢‘åŠ¨ä½œè¯†åˆ«é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šå±‚çº§è¯­ä¹‰å¯¹é½äº¤å‰æ³¨æ„åŠ›å’Œå¤šè½¨è¿¹Mambaå»ºæ¨¡ï¼Œåœ¨ç»†ç²’åº¦åŠ¨ä½œè¯†åˆ«å’Œäººç‰©å®šä½æ–¹é¢å®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰çš„åŸå­è§†é¢‘åŠ¨ä½œè¯†åˆ«æ–¹æ³•åœ¨è·¨æ¨¡æ€ä¿¡æ¯å¯¹é½å’Œæ£€ç´¢æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¯¼è‡´ç›®æ ‡äººç‰©å®šä½å’Œç»†ç²’åº¦åŠ¨ä½œé¢„æµ‹æ€§èƒ½ä¸ä½³ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤æ‚å¤šäººåœºæ™¯ä¸­ï¼Œç²¾ç¡®çš„è¯­è¨€å¼•å¯¼åŠ¨ä½œç†è§£èƒ½åŠ›ä¸è¶³ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ æœºåˆ¶ã€‚

**Method:** æå‡ºäº†RefAtomNet++æ¡†æ¶ï¼Œé‡‡ç”¨å¤šå±‚çº§è¯­ä¹‰å¯¹é½äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆéƒ¨åˆ†å…³é”®è¯ã€åœºæ™¯å±æ€§å’Œæ•´ä½“å¥å­ä¸‰ä¸ªå±‚æ¬¡çš„å¤šè½¨è¿¹Mambaå»ºæ¨¡ã€‚é€šè¿‡åŠ¨æ€é€‰æ‹©æœ€è¿‘è§†è§‰ç©ºé—´tokenæ„å»ºæ‰«æè½¨è¿¹ï¼Œå®ç°è·¨ä¸åŒè¯­ä¹‰å±‚çº§çš„æ—¶ç©ºtokenæœ‰æ•ˆèšåˆã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜RefAtomNet++åœ¨RefAVA++æ•°æ®é›†ä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡290ä¸‡å¸§è§†é¢‘å’Œ7.51ä¸‡ä¸ªæ ‡æ³¨äººç‰©ï¼Œä¸ºç»†ç²’åº¦åŠ¨ä½œè¯†åˆ«æä¾›äº†å¤§è§„æ¨¡åŸºå‡†ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å¤šå±‚çº§è¯­ä¹‰å¯¹é½å’ŒåŠ¨æ€è½¨è¿¹å»ºæ¨¡åœ¨è·¨æ¨¡æ€åŠ¨ä½œè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤æ‚åœºæ™¯ä¸‹çš„äº¤äº’å¼äººç±»åŠ¨ä½œåˆ†ææä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä¸­çš„ç»†ç²’åº¦è¯­ä¹‰å¯¹é½æ˜¯æå‡æ€§èƒ½çš„å…³é”®å› ç´ ã€‚

---

#### ğŸ“„ Abstract
Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.


### [18] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong, Jiabao Chen, Ye Wang, Jialin Peng*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç”µå­æ˜¾å¾®é•œå›¾åƒä¸­çº¿ç²’ä½“åˆ†å‰²çš„å¼±ç›‘ç£åŸŸè‡ªé€‚åº”æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆç¨€ç–ç‚¹æ ‡æ³¨å’Œå¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†åˆ†å‰²æ€§èƒ½å¹¶å‡å°‘äº†æ ‡æ³¨æˆæœ¬ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç”µå­æ˜¾å¾®é•œå›¾åƒä¸­çº¿ç²’ä½“å®ä¾‹åˆ†å‰²çš„æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œç°æœ‰çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­æ€§èƒ½æœ‰é™ï¼Œå› æ­¤ç ”ç©¶åˆ©ç”¨ç›®æ ‡åŸŸä¸Šç¨€ç–ç‚¹æ ‡æ³¨çš„å¼±ç›‘ç£åŸŸè‡ªé€‚åº”æ–¹æ³•ï¼Œä»¥æœ€å°åŒ–æ ‡æ³¨å·¥ä½œé‡å¹¶é™ä½ä¸“å®¶çŸ¥è¯†éœ€æ±‚ã€‚

**Method:** æå‡ºäº†å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œè”åˆè¿›è¡Œåˆ†å‰²å’Œä¸­å¿ƒç‚¹æ£€æµ‹ï¼Œé‡‡ç”¨äº¤å‰æ•™å­¦æœºåˆ¶å’Œç±»èšç„¦è·¨åŸŸå¯¹æ¯”å­¦ä¹ ï¼Œå¹¶å¼•å…¥å…·æœ‰å®ä¾‹æ„ŸçŸ¥ä¼ªæ ‡ç­¾é€‰æ‹©ç­–ç•¥çš„åˆ†å‰²è‡ªè®­ç»ƒæ–¹æ³•ï¼Œè¯­ä¹‰æ€§åœ°é€‰æ‹©å¯é ä¸”å¤šæ ·åŒ–çš„ä¼ªæ ‡ç­¾ã€‚

**Result:** åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šçš„ç»¼åˆéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”å’Œå¼±ç›‘ç£åŸŸè‡ªé€‚åº”æ–¹æ³•ï¼Œæ˜¾è‘—ç¼©å°äº†ä¸ç›‘ç£ä¸Šç•Œçš„æ€§èƒ½å·®è·ï¼Œåœ¨æ— ç›‘ç£åŸŸè‡ªé€‚åº”è®¾ç½®ä¸‹ä¹Ÿç›¸æ¯”å…¶ä»–æŠ€æœ¯å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚

**Conclusion:** è¯¥æ–¹æ³•é€šè¿‡æœ‰æ•ˆåˆ©ç”¨ç¨€ç–ç‚¹æ ‡æ³¨å’Œå¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…é™ä½äº†æ ‡æ³¨æˆæœ¬ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.


### [19] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu, Xicheng Gong, Yadong MU*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘ç›®æ ‡è§†è§‰è¯­è¨€å¯¼èˆªçš„å‰ç»æ€§æ™ºèƒ½ä½“ï¼Œé€šè¿‡Qå­¦ä¹ ä»å¤§è§„æ¨¡æ— æ ‡ç­¾è½¨è¿¹æ•°æ®ä¸­å­¦ä¹ åœºæ™¯å¸ƒå±€å’Œç‰©ä½“å…³ç³»çŸ¥è¯†ï¼Œç»“åˆA*æœç´¢ç­–ç•¥æœ‰æ•ˆæ¢ç´¢å¯èƒ½é€šå¾€ç›®çš„åœ°çš„åŒºåŸŸã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰è¯­è¨€å¯¼èˆªæ–¹æ³•é€šå¸¸åŸºäºå†å²ä¿¡æ¯è¿›è¡Œå†³ç­–ï¼Œå¿½ç•¥äº†åŠ¨ä½œçš„æœªæ¥å½±å“å’Œé•¿æœŸç»“æœï¼Œå¯¼è‡´å¯¼èˆªæ•ˆç‡å—é™ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œå¼€å‘èƒ½å¤Ÿé¢„è§æœªæ¥çŠ¶æ€çš„å‰ç»æ€§å¯¼èˆªæ™ºèƒ½ä½“ã€‚

**Method:** é‡‡ç”¨Qå­¦ä¹ æ¡†æ¶åœ¨å¤§è§„æ¨¡æ— æ ‡ç­¾è½¨è¿¹æ•°æ®ä¸Šè®­ç»ƒQæ¨¡å‹ï¼Œå­¦ä¹ å®¤å†…åœºæ™¯çš„å¸ƒå±€å’Œç‰©ä½“å…³ç³»çŸ¥è¯†ï¼Œç”Ÿæˆæè¿°å€™é€‰åŠ¨ä½œæ½œåœ¨æœªæ¥ä¿¡æ¯çš„Qç‰¹å¾ã€‚é€šè¿‡è·¨æ¨¡æ€æœªæ¥ç¼–ç å™¨å°†ä»»åŠ¡æ— å…³çš„Qç‰¹å¾ä¸å¯¼èˆªæŒ‡ä»¤ç»“åˆï¼Œäº§ç”Ÿåæ˜ æœªæ¥å‰æ™¯çš„åŠ¨ä½œè¯„åˆ†ï¼Œå¹¶ä¸åŸºäºå†å²çš„åŸå§‹è¯„åˆ†ç»“åˆï¼Œå®ç°A*é£æ ¼çš„æœç´¢ç­–ç•¥ã€‚

**Result:** åœ¨å¹¿æ³›ä½¿ç”¨çš„ç›®æ ‡å¯¼å‘è§†è§‰è¯­è¨€å¯¼èˆªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡å¯¼èˆªæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¢ç´¢å¯èƒ½é€šå¾€ç›®çš„åœ°çš„åŒºåŸŸæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å°†æœªæ¥ä¿¡æ¯çº³å…¥è§†è§‰è¯­è¨€å¯¼èˆªå†³ç­–è¿‡ç¨‹çš„é‡è¦æ€§ï¼Œé€šè¿‡ç»“åˆä»»åŠ¡æ— å…³çš„åœºæ™¯çŸ¥è¯†å’Œä»»åŠ¡ç‰¹å®šçš„å¯¼èˆªæŒ‡ä»¤ï¼Œå®ç°äº†æ›´æœ‰æ•ˆçš„è·¯å¾„è§„åˆ’å’Œç›®æ ‡è¾¾æˆã€‚è¿™ä¸€æ–¹æ³•ä¸ºå¼€å‘æ›´æ™ºèƒ½çš„å¯¼èˆªç³»ç»Ÿæä¾›äº†æ–°çš„æ€è·¯ï¼Œå¼ºè°ƒäº†é•¿æœŸè§„åˆ’åœ¨å¤æ‚ç¯å¢ƒå¯¼èˆªä¸­çš„å…³é”®ä½œç”¨ã€‚

---

#### ğŸ“„ Abstract
In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.


### [20] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch, Yufang Hou, M. Jehanzeb Mirza, Sivan Doveh, James Glass, Rogerio Feris, Wei Lin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PRISMM-Benchï¼Œé¦–ä¸ªåŸºäºçœŸå®ç§‘å­¦è®ºæ–‡ä¸­å®¡ç¨¿äººæ ‡è®°ä¸ä¸€è‡´æ€§çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†ä¸­çš„å¯é æ€§ã€‚ç ”ç©¶å‘ç°ç°æœ‰æ¨¡å‹åœ¨æ£€æµ‹å’Œè§£å†³è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§æ–¹é¢è¡¨ç°è¾ƒå·®ï¼Œæ­ç¤ºäº†ç§‘å­¦å¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œä½†å…¶èƒ½å¦å¯é ç†è§£å’Œæ¨ç†è®ºæ–‡ä¸­çš„å¤šæ¨¡æ€å¤æ‚æ€§ä»ä¸æ˜ç¡®ã€‚æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæ£€æµ‹å’Œè§£å†³æ–‡æœ¬ã€å›¾è¡¨ã€è¡¨æ ¼å’Œæ–¹ç¨‹ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼Œè¿™äº›é—®é¢˜é€šå¸¸å¾ˆå¾®å¦™ä¸”é¢†åŸŸç‰¹å®šï¼Œä¼šå‰Šå¼±æ¸…æ™°åº¦ã€å¯é‡å¤æ€§å’Œå¯ä¿¡åº¦ã€‚ç°æœ‰åŸºå‡†è¦ä¹ˆå­¤ç«‹å•ä¸€æ¨¡æ€ï¼Œè¦ä¹ˆä¾èµ–æ— æ³•æ•æ‰çœŸå®ä¸–ç•Œå¤æ‚æ€§çš„åˆæˆé”™è¯¯ã€‚

**Method:** é€šè¿‡å¤šé˜¶æ®µæµç¨‹æ„å»ºPRISMM-BenchåŸºå‡†ï¼ŒåŒ…æ‹¬å®¡ç¨¿æ„è§æŒ–æ˜ã€LLMè¾…åŠ©è¿‡æ»¤å’Œäººå·¥éªŒè¯ï¼Œä»242ç¯‡è®ºæ–‡ä¸­æ”¶é›†äº†262ä¸ªä¸ä¸€è‡´æ€§ã€‚è®¾è®¡äº†ä¸‰ä¸ªä»»åŠ¡ï¼šä¸ä¸€è‡´æ€§è¯†åˆ«ã€ä¿®æ­£å’Œé…å¯¹åŒ¹é…ï¼Œè¯„ä¼°æ¨¡å‹è·¨æ¨¡æ€æ£€æµ‹ã€çº æ­£å’Œæ¨ç†èƒ½åŠ›ã€‚å¼•å…¥ç»“æ„åŒ–JSONç­”æ¡ˆè¡¨ç¤ºä»¥æœ€å°åŒ–è¯­è¨€åè§ï¼Œå‡å°‘å¯¹è¡¨é¢é£æ ¼çº¿ç´¢çš„ä¾èµ–ã€‚

**Result:** å¯¹21ä¸ªé¢†å…ˆLMMæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¤§å‹å¼€æ”¾æƒé‡æ¨¡å‹å’Œä¸“æœ‰æ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºæ€§èƒ½æä½ï¼ˆ26.1-54.2%ï¼‰ï¼Œçªæ˜¾äº†å¤šæ¨¡æ€ç§‘å­¦æ¨ç†çš„æŒ‘æˆ˜æ€§ã€‚å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ£€æµ‹å’Œè§£å†³çœŸå®ç§‘å­¦è®ºæ–‡ä¸­çš„ä¸ä¸€è‡´æ€§æ–¹é¢ä¹Ÿè¡¨ç°ä¸ä½³ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æ˜¾è‘—å±€é™æ€§ï¼Œå¼ºè°ƒäº†å¼€å‘å¯ä¿¡ç§‘å­¦åŠ©æ‰‹çš„å¿…è¦æ€§ã€‚PRISMM-Benchä¸ºè¯„ä¼°å’Œæ”¹è¿›æ¨¡å‹çš„å¤šæ¨¡æ€ç§‘å­¦ç†è§£èƒ½åŠ›æä¾›äº†é‡è¦åŸºå‡†ï¼Œæ¨åŠ¨äº†æ›´å¯é ç§‘å­¦AIç³»ç»Ÿçš„å‘å±•æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.


### [21] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon, Kyle Min, Jy-yong Sohn*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†READæ–¹æ³•ï¼Œé€šè¿‡æ·»åŠ é‡æ„å’Œå¯¹é½ä¸¤ä¸ªè¾…åŠ©ç›®æ ‡æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚READ-CLIPåœ¨äº”ä¸ªä¸»è¦ç»„åˆæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¯”ä¼ ç»Ÿå¾®è°ƒåŸºçº¿æå‡é«˜è¾¾4.1%ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡è¿‘æœŸå–å¾—è¿›å±•ï¼Œä½†ä½¿ç”¨æ ‡å‡†å¯¹æ¯”ç›®æ ‡è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç»„åˆæ¨ç†æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œå³ç†è§£è§†è§‰å’Œè¯­è¨€å…ƒç´ ä¹‹é—´çš„ç»“æ„åŒ–å…³ç³»ã€‚è¿™ä¸€ä¸è¶³ä¸»è¦æºäºæ–‡æœ¬ç¼–ç å™¨å€¾å‘äºå…³æ³¨å•ä¸ªå•è¯è€Œéå®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼Œè¿™ç§å±€é™æ€§è¢«ä¸»è¦å°†å•è¯ä¸è§†è§‰å¯¹è±¡å¯¹é½çš„å¯¹æ¯”è®­ç»ƒæ‰€å¼ºåŒ–ã€‚

**Method:** æˆ‘ä»¬å¼•å…¥äº†READæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å‘å¯¹æ¯”å­¦ä¹ æ·»åŠ ä¸¤ä¸ªè¾…åŠ©ç›®æ ‡æ¥å¢å¼ºç»„åˆæ¨ç†çš„å¾®è°ƒæ–¹æ³•ï¼š(1) ä»¤ç‰Œçº§é‡æ„ç›®æ ‡ï¼Œå…¶ä¸­å†»ç»“çš„é¢„è®­ç»ƒè§£ç å™¨åŸºäºåŸå§‹æ ‡é¢˜çš„åµŒå…¥é‡æ„æ›¿ä»£æ ‡é¢˜ï¼›(2) å¥å­çº§å¯¹é½ç›®æ ‡ï¼Œæ˜¾å¼åœ°åœ¨åµŒå…¥ç©ºé—´ä¸­å¯¹é½é‡Šä¹‰å¥å­ã€‚

**Result:** READ-CLIPåœ¨äº”ä¸ªä¸»è¦ç»„åˆæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¯”æœ€å¼ºçš„ä¼ ç»Ÿå¾®è°ƒåŸºçº¿æå‡é«˜è¾¾4.1%ã€‚å°†READåº”ç”¨äºç°æœ‰CLIPå˜ä½“ï¼ˆåŒ…æ‹¬NegCLIPå’ŒFSC-CLIPï¼‰ä¹Ÿèƒ½æé«˜è¿™äº›åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚å®šé‡å’Œå®šæ€§åˆ†æè¡¨æ˜ï¼Œé‡æ„å’Œå¯¹é½ç›®æ ‡æä¾›äº†äº’è¡¥çš„ç›Šå¤„ã€‚

**Conclusion:** æœ¬ç ”ç©¶æå‡ºçš„é‡æ„å’Œå¯¹é½ç›®æ ‡æä¾›äº†äº’è¡¥çš„ç›Šå¤„ï¼šå‰è€…é¼“åŠ±ç¼–ç å™¨æ•è·æ ‡é¢˜å†…å•è¯ä¹‹é—´çš„å…³ç³»ï¼Œè€Œåè€…ç¡®ä¿ä½¿ç”¨ä¸åŒæªè¾è¡¨è¾¾çš„é‡Šä¹‰å…·æœ‰ä¸€è‡´çš„è¡¨ç¤ºã€‚è¯¥æ–¹æ³•ä¸ºå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç»„åˆæ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥ã€‚

---

#### ğŸ“„ Abstract
Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.


### [22] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin, Li Lin, Christina P. Walker, Daniel S. Schiff, Shu Hu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªåŸºäºçœŸå®ä¸–ç•Œæ”¿æ²»æ·±åº¦ä¼ªé€ äº‹ä»¶çš„ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†ç°æœ‰æ£€æµ‹å™¨åœ¨çœŸå®æ”¿æ²»æ·±åº¦ä¼ªé€ å†…å®¹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå‘ç°ç°æœ‰æ£€æµ‹å™¨åœ¨çœŸå®æ”¿æ²»åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ä¸”æ˜“å—ç®€å•æ”»å‡»ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤§å¤šæ•°æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹åœ¨å®éªŒå®¤æ§åˆ¶çš„åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒå’ŒéªŒè¯ï¼Œé™åˆ¶äº†å®ƒä»¬å¯¹ç¤¾äº¤åª’ä½“ä¸Šä¼ æ’­çš„çœŸå®ä¸–ç•Œæ”¿æ²»æ·±åº¦ä¼ªé€ çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œè¿™ç±»å†…å®¹å¯¹å…¬ä¼—ä¿¡ä»»å’Œæ°‘ä¸»åˆ¶åº¦æ„æˆä¸¥é‡å¨èƒã€‚

**Method:** ç ”ç©¶åŸºäºæ”¿æ²»æ·±åº¦ä¼ªé€ äº‹ä»¶æ•°æ®åº“æ„å»ºäº†é¦–ä¸ªç³»ç»Ÿæ€§åŸºå‡†ï¼Œè¯¥æ•°æ®åº“æ”¶é›†äº†2018å¹´ä»¥æ¥ç¤¾äº¤åª’ä½“ä¸Šä¼ æ’­çš„çœŸå®æ”¿æ²»æ·±åº¦ä¼ªé€ å†…å®¹ï¼Œå¹¶å¯¹å­¦æœ¯ç•Œã€æ”¿åºœå’Œå·¥ä¸šç•Œçš„æœ€å…ˆè¿›æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨è¿›è¡Œäº†ç³»ç»Ÿæ€§è¯„ä¼°ã€‚

**Result:** è¯„ä¼°å‘ç°å­¦æœ¯ç•Œå’Œæ”¿åºœå¼€å‘çš„æ£€æµ‹å™¨è¡¨ç°ç›¸å¯¹è¾ƒå·®ï¼Œä»˜è´¹æ£€æµ‹å·¥å…·è™½ç„¶æ€§èƒ½ä¼˜äºå…è´¹æ¨¡å‹ï¼Œä½†æ‰€æœ‰æ£€æµ‹å™¨éƒ½éš¾ä»¥æœ‰æ•ˆæ³›åŒ–åˆ°çœŸå®æ”¿æ²»æ·±åº¦ä¼ªé€ å†…å®¹ï¼Œä¸”åœ¨è§†é¢‘é¢†åŸŸç‰¹åˆ«å®¹æ˜“å—åˆ°ç®€å•æ“ä½œçš„æ”»å‡»ã€‚

**Conclusion:** ç ”ç©¶ç»“æœå¼ºè°ƒäº†å¼€å‘æ”¿æ²»æƒ…å¢ƒåŒ–çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ¡†æ¶çš„è¿«åˆ‡éœ€æ±‚ï¼Œä»¥åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­æ›´å¥½åœ°ä¿æŠ¤å…¬ä¼—å…å—æ”¿æ²»æ·±åº¦ä¼ªé€ çš„å¨èƒï¼Œéœ€è¦é’ˆå¯¹æ”¿æ²»å†…å®¹çš„ç‰¹æ®Šæ€§è®¾è®¡æ›´å…·é²æ£’æ€§çš„æ£€æµ‹æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.


### [23] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang, Liang Shi, Yitian Zhang, Yi Xu, Yun Fu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSHIELDæ¡†æ¶ï¼Œé¦–æ¬¡å°†LVLMä¸­çš„ç‰©ä½“å¹»è§‰é—®é¢˜æº¯æºè‡³è§†è§‰ç¼–ç å™¨ï¼Œå¹¶é€šè¿‡ä¸‰ç§è®­ç»ƒæ— å…³ç­–ç•¥æœ‰æ•ˆç¼“è§£äº†ç»Ÿè®¡åå·®ã€å›ºæœ‰åå·®å’Œè„†å¼±æ€§å¯¼è‡´çš„å¹»è§‰ç°è±¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç‰©ä½“å¹»è§‰é—®é¢˜ï¼ˆæ¨¡å‹ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†ä¸å‡†ç¡®çš„ç‰©ä½“æè¿°ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œä¸ä»¥å¾€å…³æ³¨LLMç»„ä»¶çš„ç ”ç©¶ä¸åŒï¼Œæœ¬æ–‡é¦–æ¬¡å°†LVLMå¹»è§‰é—®é¢˜æº¯æºè‡³è§†è§‰ç¼–ç å™¨ã€‚

**Method:** æå‡ºSHIELDè®­ç»ƒæ— å…³æ¡†æ¶ï¼Œé‡‡ç”¨ä¸‰ç§ç­–ç•¥ï¼šé‡åŠ æƒè§†è§‰tokenä»¥å‡å°‘ç»Ÿè®¡åå·®ï¼Œå¼•å…¥å™ªå£°è¡ç”Ÿtokenä»¥å¯¹æŠ—å›ºæœ‰åå·®ï¼Œåº”ç”¨å¯¹æŠ—æ”»å‡»ä¸å¯¹æ¯”è§£ç æ¥è§£å†³è„†å¼±æ€§é—®é¢˜ã€‚

**Result:** å®éªŒè¡¨æ˜SHIELDåœ¨å¤šç§åŸºå‡†æµ‹è¯•å’ŒLVLMå®¶æ—ä¸­æœ‰æ•ˆç¼“è§£äº†ç‰©ä½“å¹»è§‰ï¼ŒåŒæ—¶åœ¨é€šç”¨LVLMåŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶å¹¿æ³›é€‚ç”¨æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†è§†è§‰ç¼–ç å™¨åœ¨LVLMå¹»è§‰é—®é¢˜ä¸­çš„å…³é”®ä½œç”¨ï¼Œæå‡ºçš„SHIELDæ¡†æ¶ä¸ä»…è§£å†³äº†ç‰¹å®šå¹»è§‰é—®é¢˜ï¼Œè¿˜å±•ç¤ºäº†åœ¨ä¿æŒé€šç”¨æ€§èƒ½çš„åŒæ—¶æå‡æ¨¡å‹é²æ£’æ€§çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.


### [24] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu, Yurui Zhu, Xin Lu, Wenrui Yan, Dong Li, Kunlin Liu, Xueyang Fu, Zheng-Jun Zha*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºVisionSelectorï¼Œä¸€ç§è½»é‡çº§å³æ’å³ç”¨çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è§†è§‰ä»¤ç‰Œå‹ç¼©æ¡†æ¶ï¼Œé€šè¿‡å¯å­¦ä¹ çš„å†³ç­–è¿‡ç¨‹å®ç°é«˜æ•ˆä¸”è‡ªé€‚åº”çš„ä»¤ç‰Œé€‰æ‹©ï¼Œæ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡åŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæˆ–å¤šå›¾åƒè¾“å…¥æ—¶é¢ä¸´å¤§é‡è§†è§‰ä»¤ç‰Œå¸¦æ¥çš„è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆï¼Œç°æœ‰ä»¤ç‰Œå‹ç¼©æŠ€æœ¯å—é™äºå¯å‘å¼è§„åˆ™ï¼Œå­˜åœ¨ä¸¢å¼ƒå…³é”®ä¿¡æ¯å’Œæ³¨æ„åŠ›åå·®é—®é¢˜ï¼Œå¯¼è‡´åœ¨æ¿€è¿›å‹ç¼©ç‡ä¸‹æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚

**Method:** æå‡ºVisionSelectoræ¡†æ¶ï¼Œå°†ä»¤ç‰Œå‹ç¼©é‡æ–°è¡¨è¿°ä¸ºç«¯åˆ°ç«¯å¯å­¦ä¹ çš„å†³ç­–è¿‡ç¨‹ï¼ŒåŒ…å«è§£è€¦äºMLLMéª¨å¹²ç½‘ç»œçš„è¯„åˆ†å™¨æ¨¡å—ï¼Œé‡‡ç”¨å¯å¾®åˆ†Top-Kæœºåˆ¶å’Œè¯¾ç¨‹é€€ç«ç­–ç•¥æ¥å¼¥åˆè®­ç»ƒ-æ¨ç†å·®è·ï¼Œæ”¯æŒä»»æ„å‹ç¼©ç‡ä¸‹çš„é«˜æ•ˆè‡ªé€‚åº”ä»¤ç‰Œé€‰æ‹©ã€‚

**Result:** VisionSelectorä»…éœ€12.85Må¯è®­ç»ƒå‚æ•°ï¼Œåœ¨MMEåŸºå‡†ä¸Šä»¥30%ä¿ç•™é¢„ç®—ä¿æŒ100%å‡†ç¡®ç‡ï¼Œåœ¨10%ä¿ç•™é¢„ç®—ä¸‹æ¯”å…ˆå‰æ–¹æ³•æå‡12.14%ï¼Œé¢„å¡«å……é€Ÿåº¦æå‡ä¸¤å€ï¼Œå±•ç°å‡ºè·¨ä¸åŒå‹ç¼©é¢„ç®—çš„ä¼˜è¶Šæ€§èƒ½å’Œè‡ªé€‚åº”è¯†åˆ«å…³é”®ä»¤ç‰Œçš„èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡å¯å­¦ä¹ å†³ç­–è¿‡ç¨‹å®ç°ä»¤ç‰Œå‹ç¼©çš„æœ‰æ•ˆæ€§ï¼ŒVisionSelectorçš„è½»é‡çº§è®¾è®¡å’Œæ³›åŒ–èƒ½åŠ›ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¸ºè‡ªé€‚åº”ä»¤ç‰Œé€‰æ‹©æœºåˆ¶çš„æœªæ¥å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .


### [25] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji, Zihui Song*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„è§†é¢‘ç†è§£æ¡†æ¶ï¼Œé€šè¿‡å°†é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰å…ˆéªŒä¸ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•ç›¸ç»“åˆï¼Œå°†è§†é¢‘ç†è§£é‡æ–°å®šä¹‰ä¸ºé«˜ç»´è¯­ä¹‰ç‰¹å¾ç©ºé—´ä¸­çš„è‡ªç›‘ç£æ—¶ç©ºèšç±»é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é™æ€å›¾åƒä¸Šè¡¨ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™ç§èƒ½åŠ›å°šæœªå……åˆ†è¿ç§»åˆ°è§†é¢‘é¢†åŸŸï¼Œä¼ ç»Ÿè§†é¢‘ç†è§£æ¨¡å‹ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®å’Œç‰¹å®šä»»åŠ¡è®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ä¸”å¯æ‰©å±•æ€§æœ‰é™ã€‚

**Method:** è¯¥æ¡†æ¶é¦–å…ˆä½¿ç”¨é¢„è®­ç»ƒVLMçš„å†»ç»“è§†è§‰ç¼–ç å™¨å°†è§†é¢‘æµè½¬æ¢ä¸ºè¯­ä¹‰ç‰¹å¾è½¨è¿¹ï¼Œç„¶åé‡‡ç”¨æ ¸æ—¶é—´åˆ†å‰²ç®—æ³•å°†è¿ç»­ç‰¹å¾æµåˆ’åˆ†ä¸ºç¦»æ•£çš„è¯­ä¹‰è¿è´¯äº‹ä»¶ç‰‡æ®µï¼Œæœ€åé€šè¿‡æ— ç›‘ç£å¯†åº¦èšç±»è¯†åˆ«è§†é¢‘ä¸­é‡å¤å‡ºç°çš„å®è§‚åœºæ™¯å’Œä¸»é¢˜ã€‚

**Result:** é€šè¿‡ä»æ¯ä¸ªå‘ç°çš„èšç±»ä¸­é€‰æ‹©ä»£è¡¨æ€§å…³é”®å¸§å¹¶åˆ©ç”¨VLMçš„ç”Ÿæˆèƒ½åŠ›è¿›è¡Œæ–‡æœ¬æè¿°ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆè§†é¢‘å†…å®¹çš„ç»“æ„åŒ–å¤šæ¨¡æ€æ‘˜è¦ï¼Œå®ç°äº†é›¶æ ·æœ¬çš„è‡ªåŠ¨åŒ–è§†é¢‘ç»“æ„åˆ†æã€‚

**Conclusion:** è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§æœ‰æ•ˆã€å¯è§£é‡Šä¸”æ¨¡å‹æ— å…³çš„è·¯å¾„ï¼Œç”¨äºè§†é¢‘å†…å®¹çš„é›¶æ ·æœ¬è‡ªåŠ¨ç»“æ„åˆ†æï¼Œä¸ºè§†é¢‘ç†è§£å¼€è¾Ÿäº†æ— éœ€ç«¯åˆ°ç«¯è®­ç»ƒçš„æ–°èŒƒå¼ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.


### [26] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo, Yunzhong Hou, Wufei Ma, Meng Tang, Ming-Hsuan Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MSSRï¼ˆæœ€å°å……åˆ†ç©ºé—´æ¨ç†å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºæœ€å°å……åˆ†ä¿¡æ¯é›†æ¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨3Dç©ºé—´æ¨ç†ä¸­çš„æŒ‘æˆ˜ï¼Œæ˜¾è‘—æå‡äº†ç©ºé—´æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨ä¸¤ä¸ªåŸºæœ¬ç“¶é¢ˆï¼šç”±äº2Dä¸­å¿ƒé¢„è®­ç»ƒå¯¼è‡´çš„3Dç†è§£èƒ½åŠ›ä¸è¶³ï¼Œä»¥åŠå†—ä½™3Dä¿¡æ¯å¼•å‘çš„æ¨ç†å¤±è´¥é—®é¢˜ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨3Dåœºæ™¯ä¸­çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚

**Method:** MSSRé‡‡ç”¨åŒæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå…¶ä¸­æ„ŸçŸ¥æ™ºèƒ½ä½“é€šè¿‡ç¨‹åºåŒ–æŸ¥è¯¢3Dåœºæ™¯å¹¶ä½¿ç”¨å¤šåŠŸèƒ½æ„ŸçŸ¥å·¥å…·ç®±æå–å……åˆ†ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–°é¢–çš„SOGï¼ˆæƒ…å¢ƒåŒ–æ–¹å‘å®šä½ï¼‰æ¨¡å—ï¼›æ¨ç†æ™ºèƒ½ä½“åˆ™è¿­ä»£ä¼˜åŒ–è¿™äº›ä¿¡æ¯ä»¥è¿½æ±‚æœ€å°åŒ–ï¼Œåœ¨é—­ç¯ä¸­ä¿®å‰ªå†—ä½™ç»†èŠ‚å¹¶è¯·æ±‚ç¼ºå¤±ä¿¡æ¯ï¼Œç›´åˆ°æ„å»ºå‡ºæœ€å°å……åˆ†ä¿¡æ¯é›†ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ˜¾å¼è¿½æ±‚å……åˆ†æ€§å’Œæœ€å°æ€§ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œåœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ¡†æ¶äº§ç”Ÿäº†å¯è§£é‡Šçš„æ¨ç†è·¯å¾„ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸º3Dç©ºé—´æ¨ç†æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æœ€å°å……åˆ†ä¿¡æ¯é›†åŸåˆ™æœ‰æ•ˆè§£å†³äº†ä¿¡æ¯å†—ä½™å’Œä¸è¶³çš„é—®é¢˜ï¼ŒåŒæ—¶äº§ç”Ÿçš„å¯è§£é‡Šæ¨ç†è·¯å¾„ä¸ºæœªæ¥æ¨¡å‹è®­ç»ƒæä¾›äº†é«˜è´¨é‡æ•°æ®æºï¼Œæ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨3Dç†è§£æ–¹é¢çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.


### [27] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei, Yifan Chen, Xinrui He, Wenxuan Bao, Jingrui He*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢†åŸŸè¿æ¥å¯¹æ¯”å­¦ä¹ èŒƒå¼DCCLï¼Œé€šè¿‡å¢å¼ºè·¨é¢†åŸŸçš„æ¦‚å¿µè¿é€šæ€§æ¥è§£å†³é¢†åŸŸæ³›åŒ–ä¸­ç›´æ¥åº”ç”¨å¯¹æ¯”å­¦ä¹ å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œåœ¨äº”ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æœ€å…ˆè¿›æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é¢†åŸŸæ³›åŒ–ä¸­è®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬ä¹‹é—´çš„åˆ†å¸ƒåç§»ä¸¥é‡é˜»ç¢æ¨¡å‹æ³›åŒ–æ€§èƒ½ï¼Œè™½ç„¶å¯¹æ¯”å­¦ä¹ ç†è®ºä¸Šåº”èƒ½é€šè¿‡ç±»åˆ«åˆ†ç¦»è¡¨ç¤ºæ”¹å–„é¢†åŸŸæ³›åŒ–ï¼Œä½†å®é™…åº”ç”¨ä¸­ç›´æ¥ä½¿ç”¨å¯¹æ¯”å­¦ä¹ åè€Œä¼šé™ä½æ€§èƒ½ï¼Œç ”ç©¶å‘ç°è¿™æ˜¯ç”±äºé¢†åŸŸæ³›åŒ–è®¾ç½®ä¸­ç¼ºä¹ç±»å†…è¿é€šæ€§å¯¼è‡´çš„ã€‚

**Method:** æå‡ºäº†é¢†åŸŸè¿æ¥å¯¹æ¯”å­¦ä¹ DCCLèŒƒå¼ï¼Œåœ¨æ•°æ®å±‚é¢é‡‡ç”¨æ›´æ¿€è¿›çš„æ•°æ®å¢å¼ºå’Œè·¨é¢†åŸŸæ­£æ ·æœ¬æ¥å¢å¼ºç±»å†…è¿é€šæ€§ï¼Œåœ¨æ¨¡å‹å±‚é¢æå‡ºæ¨¡å‹é”šå®šæŠ€æœ¯åˆ©ç”¨é¢„è®­ç»ƒè¡¨ç¤ºä¸­çš„ç±»å†…è¿é€šæ€§ï¼Œå¹¶è¾…ä»¥ç”Ÿæˆå˜æ¢æŸå¤±æ¥æ›´å¥½åœ°åµŒå…¥æœªè§æµ‹è¯•é¢†åŸŸã€‚

**Result:** åœ¨äº”ä¸ªæ ‡å‡†é¢†åŸŸæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†DCCLä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œå³ä½¿åœ¨æ²¡æœ‰é¢†åŸŸç›‘ç£çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å–å¾—ä¼˜è¶Šæ€§èƒ½ï¼Œä»£ç å®ç°å·²åœ¨GitHubä¸Šå¼€æºæä¾›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜å¢å¼ºè·¨é¢†åŸŸçš„æ¦‚å¿µè¿é€šæ€§å¯¹äºé¢†åŸŸæ³›åŒ–è‡³å…³é‡è¦ï¼ŒDCCLé€šè¿‡æ•°æ®å¢å¼ºå’Œæ¨¡å‹é”šå®šçš„åŒé‡ç­–ç•¥æœ‰æ•ˆè§£å†³äº†å¯¹æ¯”å­¦ä¹ åœ¨é¢†åŸŸæ³›åŒ–ä¸­çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œä¸ºæ— ç›‘ç£é¢†åŸŸæ³›åŒ–æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL


### [28] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li, Zhicai Wang, Yuan Sheng, Xingyu Zhu, Yanbin Hao, Xiang Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Res-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè¾“å…¥åˆ†è¾¨ç‡ä¸‹çš„é²æ£’æ€§ï¼Œå¹¶å¼•å…¥æ–°çš„è¯„ä¼°æ¡†æ¶æ¥é‡åŒ–æ¨¡å‹æ€§èƒ½éšåˆ†è¾¨ç‡å˜åŒ–çš„ç¨³å®šæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ä¸»è¦å…³æ³¨è¯­ä¹‰æ€§èƒ½ï¼Œè€Œå¿½è§†äº†åˆ†è¾¨ç‡é²æ£’æ€§è¿™ä¸€å…³é”®é—®é¢˜ï¼Œå³æ¨¡å‹æ€§èƒ½åœ¨ä¸åŒè¾“å…¥åˆ†è¾¨ç‡ä¸‹æ˜¯å¦ä¿æŒç¨³å®šï¼Œè¿™ä¸€ç ”ç©¶ç©ºç™½éœ€è¦ç³»ç»Ÿæ€§çš„è¯„ä¼°æ¡†æ¶æ¥è§£å†³ã€‚

**Method:** ç ”ç©¶è®¾è®¡äº†Res-BenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«14,400ä¸ªæ ·æœ¬è¦†ç›–12ä¸ªåˆ†è¾¨ç‡çº§åˆ«å’Œ6ä¸ªæ ¸å¿ƒèƒ½åŠ›ç»´åº¦ï¼Œå¹¶æå‡ºäº†åŒ…å«Spearmanç›¸å…³æ€§åˆ†æå’Œç»å¯¹/ç›¸å¯¹è¿ç»­è¯¯å·®åœ¨å†…çš„å¤šç§é²æ£’æ€§è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡æ€§èƒ½ç¨³å®šæ€§ã€‚

**Result:** é€šè¿‡å¤§è§„æ¨¡è¯„ä¼°ä¸»æµMLLMsï¼Œç ”ç©¶ä»æ¨¡å‹ä¸­å¿ƒåŒ–ã€ä»»åŠ¡ä¸­å¿ƒåŒ–è§’åº¦åˆ†æäº†é²æ£’æ€§ï¼Œè€ƒå¯Ÿäº†å¡«å……å’Œè¶…åˆ†è¾¨ç‡ç­‰é¢„å¤„ç†ç­–ç•¥ï¼Œå¹¶æ¢ç´¢äº†å¾®è°ƒå¯¹ç¨³å®šæ€§æå‡çš„æ•ˆæœã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†åˆ†è¾¨ç‡é²æ£’æ€§è¯„ä¼°çš„é‡è¦æ€§ï¼Œä¸ºMLLMsçš„ç¨³å¥æ€§å‘å±•æä¾›äº†ç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ï¼Œå¹¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ä¸åŒåˆ†è¾¨ç‡ä¸‹çš„æ€§èƒ½æ³¢åŠ¨é—®é¢˜ï¼Œä¸ºæœªæ¥æ¨¡å‹ä¼˜åŒ–æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.


### [29] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu, Jiangyong Huang, Ziyu Zhu, Baoxiong Jia, Siyuan Huang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªåº”ç”¨äº3Dåœºæ™¯ç†è§£çš„æ€ç»´é“¾æ¨ç†æ¡†æ¶SCENECOTï¼Œé€šè¿‡æ„å»ºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†SCENECOT-185Kï¼Œå®ç°äº†åŸºäºå¤šæ¨¡æ€ä¸“å®¶æ¨¡å—çš„æ¸è¿›å¼åœºæ™¯-å¯¹è±¡æ¥åœ°æ¨ç†ï¼Œåœ¨å¤šä¸ªå¤æ‚3Dåœºæ™¯æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰3Då¤§è¯­è¨€æ¨¡å‹åœ¨æ¥åœ°é—®ç­”æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹å¯¹äººç±»åœºæ™¯-å¯¹è±¡æ¥åœ°æ¨ç†æœºåˆ¶çš„æ·±å…¥æ¢ç´¢ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œè§£å†³å¤æ‚3Dåœºæ™¯æ¨ç†ä¸­çš„æ¥åœ°é—®é¢˜ã€‚

**Method:** æå‡ºäº†åŸºäº3Dåœºæ™¯çš„æ¥åœ°æ€ç»´é“¾æ¨ç†æ–¹æ³•SCENECOTï¼Œå°†å¤æ‚æ¨ç†ä»»åŠ¡åˆ†è§£ä¸ºæ›´ç®€å•çš„å­é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€ä¸“å®¶æ¨¡å—æ„å»ºç›¸åº”çš„è§†è§‰çº¿ç´¢ï¼ŒåŒæ—¶å¼€å‘äº†é¦–ä¸ªå¤§è§„æ¨¡æ¥åœ°CoTæ¨ç†æ•°æ®é›†SCENECOT-185Kï¼ŒåŒ…å«18.5ä¸‡ä¸ªé«˜è´¨é‡å®ä¾‹ã€‚

**Result:** åœ¨å¤šä¸ªå¤æ‚3Dåœºæ™¯æ¨ç†åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ–°æ¡†æ¶å®ç°äº†å¼ºå¤§çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶å…·æœ‰é«˜åº¦çš„æ¥åœ°é—®ç­”ä¸€è‡´æ€§ï¼ŒéªŒè¯äº†CoTæ¨ç†åœ¨3Dåœºæ™¯ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¿™æ˜¯æ€ç»´é“¾æ¨ç†åœ¨3Dåœºæ™¯ç†è§£ä¸­çš„é¦–æ¬¡æˆåŠŸåº”ç”¨ï¼Œå®ç°äº†ç±»ä¼¼äººç±»çš„é€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œæ˜¾ç¤ºå‡ºæ‰©å±•åˆ°æ›´å¹¿æ³›3Dåœºæ™¯ç†è§£åœºæ™¯çš„æ½œåŠ›ï¼Œä¸º3Dåœºæ™¯æ¨ç†ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.


### [30] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan, Anhao Zhao, Jinlan Fu, Junlong Tong, Hui Su, Yijie Pan, Wei Zhang, Xiaoyu Shen*

#### ğŸ§© TL;DR
æœ¬æ–‡é€šè¿‡ç³»ç»Ÿåˆ†ææ­ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä¸‰é˜¶æ®µè·¨æ¨¡æ€äº¤äº’è¿‡ç¨‹ï¼Œå¹¶åŸºäºæ­¤æå‡ºäº†æ— éœ€è®­ç»ƒçš„å‰ªææ¡†æ¶VisiPrunerï¼Œèƒ½å¤Ÿæ˜¾è‘—å‡å°‘è§†è§‰ç›¸å…³æ³¨æ„åŠ›è®¡ç®—å’ŒFLOPsï¼ŒåŒæ—¶è¶…è¶Šç°æœ‰æ–¹æ³•å¹¶é€‚ç”¨äºå¤šç§MLLMæ¶æ„ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è§†è§‰è¯­è¨€ä»»åŠ¡æ—¶é¢ä¸´è®¡ç®—å¼€é”€è¿‡å¤§çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ³¨æ„åŠ›è®¡ç®—éšå¤šæ¨¡æ€tokenæ•°é‡å‘ˆäºŒæ¬¡å¢é•¿ï¼Œè€Œç°æœ‰çš„tokenå‰ªææ–¹æ³•ç¼ºä¹å¯¹MLLMå¦‚ä½•å¤„ç†å’Œèåˆå¤šæ¨¡æ€ä¿¡æ¯çš„åŸºæœ¬ç†è§£ã€‚

**Method:** é€šè¿‡ç³»ç»Ÿåˆ†æå‘ç°MLLMå­˜åœ¨ä¸‰é˜¶æ®µè·¨æ¨¡æ€äº¤äº’è¿‡ç¨‹ï¼šæµ…å±‚è¯†åˆ«ä»»åŠ¡æ„å›¾ã€ä¸­å±‚å…³é”®è§†è§‰tokené©±åŠ¨è·¨æ¨¡æ€èåˆã€æ·±å±‚ä»…å…³æ³¨è¯­è¨€ç²¾ç‚¼ï¼ŒåŸºäºæ­¤æå‡ºäº†æ— éœ€è®­ç»ƒçš„å‰ªææ¡†æ¶VisiPrunerï¼Œé’ˆå¯¹æ€§åœ°åœ¨ä¸åŒé˜¶æ®µè¿›è¡Œè§†è§‰tokenå‰ªæã€‚

**Result:** VisiPruneråœ¨LLaVA-v1.5 7Bæ¨¡å‹ä¸Šå®ç°äº†é«˜è¾¾99%çš„è§†è§‰ç›¸å…³æ³¨æ„åŠ›è®¡ç®—å‡å°‘å’Œ53.9%çš„FLOPsé™ä½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰tokenå‰ªææ–¹æ³•ï¼Œå¹¶åœ¨å¤šç§MLLMæ¶æ„ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ä»…æä¾›äº†é«˜æ•ˆçš„å‰ªæè§£å†³æ–¹æ¡ˆï¼Œæ›´é‡è¦çš„æ˜¯æ­ç¤ºäº†MLLMå†…åœ¨çš„å±‚æ¬¡å¤„ç†åŠ¨æ€ï¼Œä¸ºè®­ç»ƒé«˜æ•ˆMLLMæä¾›äº†å¯æ“ä½œçš„æŒ‡å¯¼åŸåˆ™ï¼Œå³é€šè¿‡ä½¿æ¨¡å‹æ¶æ„ä¸å…¶å†…åœ¨å¤„ç†åŠ¨æ€å¯¹é½æ¥ä¼˜åŒ–æ€§èƒ½ã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.


### [31] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise, Mehmet Onurcan Kaya, Dim P. Papadopoulos*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯æ˜ç¦»æ•£è‡ªå›å½’è§†è§‰æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æŸæœç´¢è¿›è¡Œæ¨ç†æ—¶ä¼˜åŒ–ï¼Œä½¿2Bå‚æ•°çš„è‡ªå›å½’æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¶…è¶Š12Bå‚æ•°çš„æ‰©æ•£æ¨¡å‹ï¼Œæ­ç¤ºäº†æ¨¡å‹æ¶æ„å¯¹æ¨ç†æ—¶ä¼˜åŒ–çš„å…³é”®ä½œç”¨ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡æ¨ç†æ—¶æœç´¢ç­–ç•¥åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å–å¾—äº†é©å‘½æ€§æˆåŠŸï¼Œä½†åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„åº”ç”¨æ•ˆæœæœ‰é™ï¼Œè¿ç»­æ‰©æ•£æ¨¡å‹ä¸­çš„æœç´¢ç­–ç•¥æ”¶ç›Šç”šå¾®ï¼Œç®€å•éšæœºé‡‡æ ·å¾€å¾€è¡¨ç°æœ€ä½³ï¼Œè¿™ä¿ƒä½¿ç ”ç©¶è€…æ¢ç´¢ä¸åŒæ¨¡å‹æ¶æ„å¯¹æ¨ç†æ—¶ä¼˜åŒ–çš„é€‚åº”æ€§ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨ç¦»æ•£è‡ªå›å½’è§†è§‰æ¨¡å‹æ¶æ„ï¼Œåˆ©ç”¨æŸæœç´¢ç­–ç•¥è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œé€šè¿‡ç¦»æ•£æ ‡è®°ç©ºé—´çš„ç‰¹æ€§å®ç°æ—©æœŸå‰ªæå’Œè®¡ç®—é‡ç”¨ï¼Œå¹¶ç³»ç»Ÿåˆ†æäº†éªŒè¯å™¨åœ¨é€Ÿåº¦ä¸æ¨ç†èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚

**Result:** å®éªŒè¡¨æ˜æŸæœç´¢æ˜¾è‘—æå‡äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè´¨é‡ï¼Œ2Bå‚æ•°çš„è‡ªå›å½’æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†12Bå‚æ•°çš„æ‰©æ•£æ¨¡å‹ï¼Œç³»ç»Ÿæ¶ˆèç ”ç©¶è¯å®ç¦»æ•£æ ‡è®°ç©ºé—´æ˜¯å®ç°è¿™ä¸€ä¼˜åŠ¿çš„å…³é”®å› ç´ ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜æ¨¡å‹æ¶æ„è€Œä¸ä»…ä»…æ˜¯è§„æ¨¡å¯¹äºè§†è§‰ç”Ÿæˆä¸­çš„æ¨ç†æ—¶ä¼˜åŒ–è‡³å…³é‡è¦ï¼Œç¦»æ•£è‡ªå›å½’æ¨¡å‹å› å…¶æ”¯æŒæœ‰æ•ˆæœç´¢ç­–ç•¥è€Œå±•ç°å‡ºç‹¬ç‰¹ä¼˜åŠ¿ï¼Œè¿™ä¸ºæœªæ¥è§†è§‰ç”Ÿæˆç³»ç»Ÿçš„è®¾è®¡æä¾›äº†é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.


### [32] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu, Dinh-Cuong Hoang, Minhhuy Le, Phan Xuan Tan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Region in Contextæ¡†æ¶ï¼Œé€šè¿‡å¤šå±‚çº§è¯­ä¹‰å¯¹é½å®ç°æ–‡æœ¬æ¡ä»¶å›¾åƒç¼–è¾‘ï¼Œä½¿å±€éƒ¨åŒºåŸŸåœ¨å…¨å±€å›¾åƒä¸Šä¸‹æ–‡ä¸­è¿›è¡Œåè°ƒä¸€è‡´çš„ç¼–è¾‘ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨åŒºåŸŸç¼–è¾‘ä¸­ç¼ºä¹æ•´ä½“è¯­ä¹‰ä¸€è‡´æ€§çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–‡æœ¬æ¡ä»¶å›¾åƒç¼–è¾‘æ–¹æ³•é€šå¸¸å°†å›¾åƒåŒºåŸŸè§†ä¸ºå­¤ç«‹å•å…ƒï¼Œä»…ä¾èµ–å±€éƒ¨çº¿ç´¢è€Œå¿½ç•¥äº†å„éƒ¨åˆ†å¯¹æ•´ä½“è§†è§‰å’Œè¯­ä¹‰æ„æˆçš„è´¡çŒ®ï¼Œå¯¼è‡´ç¼–è¾‘ç»“æœå‡ºç°ä¸ä¸€è‡´ã€ä¸è‡ªç„¶è¿‡æ¸¡æˆ–å›¾åƒæ•´ä½“è¿è´¯æ€§ä¸§å¤±çš„é—®é¢˜ã€‚

**Method:** è¯¥æ¡†æ¶å¼•å…¥äº†åŒå±‚çº§å¼•å¯¼æœºåˆ¶ï¼šåŒºåŸŸåœ¨å®Œæ•´å›¾åƒä¸Šä¸‹æ–‡ä¸­è¿›è¡Œè¡¨ç¤ºå¹¶ä¸è¯¦ç»†åŒºåŸŸçº§æè¿°å¯¹é½ï¼ŒåŒæ—¶æ•´ä¸ªå›¾åƒä¸å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å…¨é¢åœºæ™¯çº§æè¿°è¿›è¡ŒåŒ¹é…ï¼Œè¿™äº›æè¿°ä½œä¸ºæ˜ç¡®çš„è¯­è¨€å‚è€ƒæŒ‡å¯¼å±€éƒ¨ä¿®æ”¹å’Œå…¨å±€ç»“æ„ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæ›´åŠ è¿è´¯ä¸”ä¸æŒ‡ä»¤å¯¹é½çš„ç¼–è¾‘ç»“æœï¼Œåœ¨ä¿æŒå±€éƒ¨ç²¾ç¡®æ€§çš„åŒæ—¶ç¡®ä¿äº†æ•´ä½“å›¾åƒçš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨å›¾åƒç¼–è¾‘ä¸­è€ƒè™‘å…¨å±€ä¸Šä¸‹æ–‡çš„é‡è¦æ€§ï¼Œé€šè¿‡å¤šå±‚çº§è¯­ä¹‰å¯¹é½å®ç°äº†å±€éƒ¨ç¼–è¾‘ä¸æ•´ä½“åœºæ™¯çš„åè°ƒç»Ÿä¸€ï¼Œä¸ºæ–‡æœ¬é©±åŠ¨çš„å›¾åƒç¼–è¾‘æä¾›äº†æ–°çš„æ€è·¯å’Œè§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git


### [33] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang, Zhen Yang, Zi-Yi Dou, Anh Nguyen, Keen You, Omar Attia, Andrew Szot, Michael Feng, Ram Ramrakhya, Alexander Toshev, Chao Huang, Yinfei Yang, Zhe Gan*

#### ğŸ§© TL;DR
UltraCUAæå‡ºäº†ä¸€ç§æ··åˆè¡ŒåŠ¨çš„åŸºç¡€æ¨¡å‹ï¼Œå°†GUIåŸè¯­æ“ä½œä¸é«˜çº§ç¨‹åºåŒ–å·¥å…·è°ƒç”¨æ— ç¼é›†æˆï¼Œè§£å†³äº†è®¡ç®—æœºä½¿ç”¨ä»£ç†ä»…ä¾èµ–åŸå§‹æ“ä½œå¯¼è‡´çš„çº§è”å¤±è´¥å’Œæ‰§è¡Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è®¡ç®—æœºä½¿ç”¨ä»£ç†ä»…ä¾èµ–ç‚¹å‡»ã€è¾“å…¥ã€æ»šåŠ¨ç­‰åŸå§‹æ“ä½œï¼Œéœ€è¦ç²¾ç¡®çš„è§†è§‰å®šä½å’Œå†—é•¿çš„æ‰§è¡Œé“¾ï¼Œå¯¼è‡´çº§è”å¤±è´¥å’Œæ€§èƒ½ç“¶é¢ˆï¼ŒåŒæ—¶è¿™äº›ä»£ç†æ— æ³•åˆ©ç”¨ä¸°å¯Œçš„ç¨‹åºåŒ–æ¥å£ï¼ˆå¦‚APIã€MCPæœåŠ¡å™¨ã€å·¥å…·ï¼‰ç­‰èƒ½åŠ›ã€‚

**Method:** è¯¥æ–¹æ³•åŒ…å«å››ä¸ªå…³é”®ç»„ä»¶ï¼šä»è½¯ä»¶æ–‡æ¡£ã€å¼€æºä»“åº“å’Œä»£ç ç”Ÿæˆä¸­æ‰©å±•ç¨‹åºåŒ–å·¥å…·çš„è‡ªåŠ¨åŒ–æµæ°´çº¿ï¼›ç”Ÿæˆè¶…è¿‡17,000ä¸ªå¯éªŒè¯ä»»åŠ¡çš„åˆæˆæ•°æ®å¼•æ“ï¼›åŒ…å«ä½çº§GUIæ“ä½œå’Œé«˜çº§ç¨‹åºåŒ–å·¥å…·è°ƒç”¨çš„å¤§è§„æ¨¡é«˜è´¨é‡æ··åˆè¡ŒåŠ¨è½¨è¿¹æ”¶é›†ï¼›ç»“åˆç›‘ç£å¾®è°ƒå’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œå®ç°ä½çº§å’Œé«˜çº§è¡ŒåŠ¨ä¹‹é—´çš„ç­–ç•¥æ€§äº¤æ›¿ã€‚

**Result:** åœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUltraCUAæ¨¡å‹ç›¸æ¯”åŸºçº¿æ¨¡å‹å®ç°äº†22%çš„ç›¸å¯¹æ”¹è¿›ï¼Œæ‰§è¡Œæ­¥éª¤å‡å°‘11%ï¼›åœ¨WindowsAgentArenaçš„åŸŸå¤–è¯„ä¼°ä¸­è¾¾åˆ°21.7%çš„æˆåŠŸç‡ï¼Œä¼˜äºåœ¨Windowsæ•°æ®ä¸Šè®­ç»ƒçš„åŸºçº¿æ¨¡å‹ã€‚

**Conclusion:** æ··åˆè¡ŒåŠ¨æœºåˆ¶è¢«è¯æ˜è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿå‡å°‘é”™è¯¯ä¼ æ’­åŒæ—¶ä¿æŒæ‰§è¡Œæ•ˆç‡ï¼Œä¸ºè®¡ç®—æœºä½¿ç”¨ä»£ç†æä¾›äº†æ›´å¼ºå¤§å’Œçµæ´»çš„è¡ŒåŠ¨èƒ½åŠ›ï¼Œçªç ´äº†ä¼ ç»Ÿä»…ä¾èµ–åŸå§‹æ“ä½œçš„æ€§èƒ½é™åˆ¶ã€‚

---

#### ğŸ“„ Abstract
Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.


### [34] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang, Jinfeng Gao, Dan Xu, Jiangrui Yu, Yuhan Qiao, Lan Chen, Jin Tang, Xiao Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºEMRRGæ¡†æ¶ï¼Œé€šè¿‡å‚æ•°é«˜æ•ˆæ–¹æ³•å¾®è°ƒé¢„è®­ç»ƒçš„Mambaç½‘ç»œï¼Œå®ç°äº†Xå°„çº¿åŒ»å­¦æŠ¥å‘Šç”Ÿæˆçš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŒ»å­¦æŠ¥å‘Šç”Ÿæˆæ¨¡å‹ä¸»è¦ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯¹é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹å’Œå…ˆè¿›å¾®è°ƒæŠ€æœ¯æ¢ç´¢æœ‰é™ï¼Œä¸»æµæ¡†æ¶è¦ä¹ˆé¿å…å¾®è°ƒè¦ä¹ˆä½¿ç”¨ç®€å•æ–¹æ³•å¦‚LoRAï¼ŒåŒæ—¶å¿½ç•¥äº†éTransformeræ¶æ„å¦‚Mambaç½‘ç»œåœ¨åŒ»å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚

**Method:** æå‡ºçš„EMRRGæ¡†æ¶å°†Xå°„çº¿å›¾åƒåˆ†å‰²ä¸ºè¡¥ä¸å¹¶æ ‡è®°åŒ–ï¼Œä½¿ç”¨åŸºäºSSMçš„è§†è§‰éª¨å¹²ç½‘ç»œè¿›è¡Œç‰¹å¾æå–ï¼Œå…¶ä¸­Partial LoRAæ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œç»“åˆå…·æœ‰æ··åˆè§£ç å™¨çš„LLMç”ŸæˆåŒ»å­¦æŠ¥å‘Šï¼Œå®ç°ç«¯åˆ°ç«¯è®­ç»ƒã€‚

**Result:** åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒå……åˆ†éªŒè¯äº†æ‰€æå‡ºç­–ç•¥å¯¹Xå°„çº¿åŒ»å­¦æŠ¥å‘Šç”Ÿæˆçš„æœ‰æ•ˆæ€§ï¼Œå–å¾—äº†å¼ºåŠ²çš„ç»“æœã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†Mambaç½‘ç»œåœ¨åŒ»å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œè¯æ˜äº†å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥åŒ»å­¦è§†è§‰è¯­è¨€ä»»åŠ¡ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.


### [35] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong, Ruiliang Lyu, Weihan Wang, Zhe Su, Xiaotao Gu, Xiao Liu, Yushi Bai, Jie Tang, Hongning Wang, Minlie Huang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºGlyphæ¡†æ¶ï¼Œé€šè¿‡å°†é•¿æ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒå¹¶ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¤„ç†ï¼Œå®ç°3-4å€æ–‡æœ¬å‹ç¼©ï¼Œåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶æ˜¾è‘—æå‡é•¿ä¸Šä¸‹æ–‡å¤„ç†æ•ˆç‡ã€‚è¯¥æ–¹æ³•çªç ´äº†ä¼ ç»ŸLLMåœ¨ç™¾ä¸‡çº§tokenä¸Šä¸‹æ–‡æ‰©å±•ä¸­çš„è®¡ç®—ç“¶é¢ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€å¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡æ¡£ç†è§£ã€ä»£ç åˆ†æå’Œå¤šæ­¥æ¨ç†ç­‰ä»»åŠ¡ä¸­å¯¹é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡éœ€æ±‚çš„å¢é•¿ï¼Œå°†ä¸Šä¸‹æ–‡çª—å£æ‰©å±•åˆ°ç™¾ä¸‡tokençº§åˆ«å¸¦æ¥äº†é«˜æ˜‚çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬ï¼Œé™åˆ¶äº†é•¿ä¸Šä¸‹æ–‡LLMçš„å®é™…åº”ç”¨ä»·å€¼ã€‚

**Method:** æå‡ºGlyphæ¡†æ¶ï¼Œé‡‡ç”¨è§†è§‰ä¸Šä¸‹æ–‡æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå°†é•¿æ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒå¹¶é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹å¤„ç†ï¼ŒåŒæ—¶è®¾è®¡åŸºäºLLMé©±åŠ¨çš„é—ä¼ æœç´¢ç®—æ³•æ¥ä¼˜åŒ–è§†è§‰æ¸²æŸ“é…ç½®ï¼Œä»¥å¹³è¡¡ç²¾åº¦å’Œå‹ç¼©ç‡ã€‚

**Result:** å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šç§é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­å®ç°3-4å€tokenå‹ç¼©ï¼Œç²¾åº¦ä¸Qwen3-8Bç­‰é¢†å…ˆLLMç›¸å½“ï¼Œé¢„å¡«å……å’Œè§£ç é€Ÿåº¦æå‡çº¦4å€ï¼ŒSFTè®­ç»ƒé€Ÿåº¦æå‡çº¦2å€ï¼Œ128Kä¸Šä¸‹æ–‡VLMå¯æ‰©å±•å¤„ç†ç™¾ä¸‡tokençº§æ–‡æœ¬ä»»åŠ¡ã€‚

**Conclusion:** è§†è§‰ä¸Šä¸‹æ–‡æ‰©å±•ä¸ºé•¿æ–‡æœ¬å¤„ç†æä¾›äº†é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œçªç ´äº†ä¼ ç»Ÿtokenæ‰©å±•çš„è®¡ç®—ç“¶é¢ˆï¼Œæ¸²æŸ“çš„æ–‡æœ¬æ•°æ®è¿˜èƒ½å—ç›Šäºç°å®å¤šæ¨¡æ€ä»»åŠ¡å¦‚æ–‡æ¡£ç†è§£ï¼Œä¸ºå¤§è§„æ¨¡é•¿ä¸Šä¸‹æ–‡åº”ç”¨å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.


### [36] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu, Long Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºLENSæ–¹æ³•ï¼Œä¸€ç§æ— éœ€å¾®è°ƒå³å¯ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ·»åŠ åƒç´ çº§åˆ†å‰²èƒ½åŠ›çš„å³æ’å³ç”¨è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡æå–æ³¨æ„åŠ›å›¾ä¸­çš„å…³é”®ç‚¹ç‰¹å¾ï¼Œåœ¨ä¿æŒæ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶å®ç°äº†ç«äº‰æ€§çš„åˆ†å‰²æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ·»åŠ åˆ†å‰²èƒ½åŠ›çš„æ–¹æ³•é€šå¸¸éœ€è¦å¾®è°ƒæ¨¡å‹ä»¥äº§ç”Ÿä¸æ©ç è§£ç å™¨å…¼å®¹çš„è¾“å‡ºï¼Œè¿™ä¼šæ”¹å˜æ¨¡å‹çš„è¾“å‡ºç©ºé—´å¹¶æŸå®³å…¶å†…åœ¨çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œè¿èƒŒäº†æ„å»ºç»Ÿä¸€æ¨¡å‹çš„ç›®æ ‡ã€‚

**Method:** LENSæ–¹æ³•åœ¨å®Œå…¨å†»ç»“çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸Šé™„åŠ ä¸€ä¸ªè½»é‡çº§çš„å¯è®­ç»ƒå¤´éƒ¨ï¼Œé€šè¿‡ç²¾ç‚¼æ³¨æ„åŠ›å›¾ä¸­åµŒå…¥çš„ç©ºé—´çº¿ç´¢æ¥æå–å…³é”®ç‚¹ï¼Œå¹¶å°†å…¶æè¿°ä¸ºä¸æ©ç è§£ç å™¨ç›´æ¥å…¼å®¹çš„ç‚¹çº§ç‰¹å¾ã€‚

**Result:** å¤§é‡å®éªŒéªŒè¯äº†LENSæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼šåœ¨åˆ†å‰²æ€§èƒ½ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†åŸºäºé‡è®­ç»ƒçš„æ–¹æ³•ï¼ŒåŒæ—¶å®Œå…¨ä¿ç•™äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œå¾®è°ƒæ–¹æ³•ä¼šæ˜¾è‘—é™ä½è¿™ç§èƒ½åŠ›ã€‚

**Conclusion:** LENSçš„å¯é™„åŠ è®¾è®¡ä¸ºæ‰©å±•å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å»ºç«‹äº†ä¸€ä¸ªé«˜æ•ˆä¸”å¼ºå¤§çš„èŒƒå¼ï¼Œä¸ºå®ç°çœŸæ­£å¤šæ‰å¤šè‰ºçš„ç»Ÿä¸€æ¨¡å‹é“ºå¹³äº†é“è·¯ï¼ŒåŒæ—¶è§£å†³äº†å¾®è°ƒæ–¹æ³•æŸå®³æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å…³é”®é—®é¢˜ã€‚

---

#### ğŸ“„ Abstract
Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.


### [37] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu, Shuchen Weng, Jiacong Fang, Peixuan Zhang, Si Li, Chao Xu, Boxin Shi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸ªæ€§åŒ–å›¾åƒæ»¤é•œï¼ˆPIFï¼‰æ–¹æ³•ï¼ŒåŸºäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ–‡æœ¬åæ¼”æŠ€æœ¯å­¦ä¹ å‚è€ƒå›¾åƒçš„æ‘„å½±é£æ ¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå–å’Œè½¬ç§»å¤šç§æ‘„å½±é£æ ¼ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•åœ¨ä»å‚è€ƒå›¾åƒä¸­å­¦ä¹ æœ‰æ„ä¹‰çš„æ‘„å½±æ¦‚å¿µæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¦ä¹ˆæ— æ³•å­¦ä¹ åˆ°æœ‰æ„ä¹‰çš„æ‘„å½±æ¦‚å¿µï¼Œè¦ä¹ˆæ— æ³•ä¿æŒå†…å®¹å›¾åƒçš„å†…å®¹å®Œæ•´æ€§ã€‚æ‘„å½±é£æ ¼ä½œä¸ºç‰¹å®šæ‘„å½±æ¦‚å¿µçš„ç»„åˆï¼Œæ˜¯è‘—åæ‘„å½±å¸ˆä½œå“é­…åŠ›çš„å…³é”®æ‰€åœ¨ã€‚

**Method:** åŸºäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨ç”Ÿæˆå…ˆéªŒå­¦ä¹ æ‘„å½±æ¦‚å¿µçš„å¹³å‡å¤–è§‚ä»¥åŠå¦‚ä½•æ ¹æ®æ–‡æœ¬æç¤ºè°ƒæ•´è¿™äº›æ¦‚å¿µã€‚é€šè¿‡æ–‡æœ¬åæ¼”æŠ€æœ¯ä¼˜åŒ–æ‘„å½±æ¦‚å¿µçš„æç¤ºè¯æ¥å­¦ä¹ å‚è€ƒå›¾åƒçš„æ‘„å½±é£æ ¼ã€‚

**Result:** PIFåœ¨æå–å’Œè½¬ç§»å„ç§æ‘„å½±é£æ ¼æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ æ‘„å½±æ¦‚å¿µçš„å¹³å‡å¤–è§‚å¹¶æ ¹æ®æ–‡æœ¬æç¤ºè¿›è¡Œç›¸åº”è°ƒæ•´ã€‚

**Conclusion:** è¯¥æ–¹æ³•é€šè¿‡ç»“åˆé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œæ–‡æœ¬åæ¼”æŠ€æœ¯ï¼Œä¸ºæ‘„å½±é£æ ¼å­¦ä¹ å’Œè½¬ç§»æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†åœ¨é£æ ¼æå–å’Œè½¬ç§»ä»»åŠ¡ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚

---

#### ğŸ“„ Abstract
Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/


### [38] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach, Abdulwahab Felemban, Faizan Farooq Khan, Yousef A. Radwan, Xiang Li, Fabio Marchese, Sara Beery, Burton H. Jones, Francesca Benzoni, Mohamed Elhoseiny*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ReefNetï¼Œä¸€ä¸ªå¤§è§„æ¨¡å…¬å¼€çŠç‘šç¤å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«çº¦925,000ä¸ªç»ä¸“å®¶éªŒè¯çš„å±çº§ç¡¬çŠç‘šæ ‡æ³¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡ã€åœ°ç†è¦†ç›–å’Œç²¾ç»†æ ‡æ³¨æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºçŠç‘šç¤è‡ªåŠ¨ç›‘æµ‹æä¾›å…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸæ³›åŒ–åŸºå‡†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** çŠç‘šç¤å› æ°”å€™å˜åŒ–ç­‰äººç±»å‹åŠ›è€Œè¿…é€Ÿè¡°é€€ï¼Œè¿«åˆ‡éœ€è¦å¯æ‰©å±•çš„è‡ªåŠ¨ç›‘æµ‹æ–¹æ³•ï¼Œä½†ç°æœ‰æ•°æ®é›†å¾€å¾€å—é™äºè§„æ¨¡ã€åœ°ç†è¦†ç›–èŒƒå›´æˆ–ç²—ç²’åº¦æ ‡æ³¨ï¼Œä¸”ä¸ç¬¦åˆæœºå™¨å­¦ä¹ å°±ç»ªè¦æ±‚ï¼Œæ— æ³•æ”¯æŒå…¨çƒå°ºåº¦çš„ç²¾ç»†çŠç‘šåˆ†ç±»ç ”ç©¶ã€‚

**Method:** ReefNetæ•´åˆäº†æ¥è‡ª76ä¸ªCoralNetæ¥æºå’Œçº¢æµ·Al Wajhç«™ç‚¹çš„å›¾åƒæ•°æ®ï¼Œæä¾›æ˜ å°„è‡³ä¸–ç•Œæµ·æ´‹ç‰©ç§åå½•çš„ç²¾ç»†åˆ†ç±»æ ‡æ³¨ï¼Œå¹¶è®¾è®¡äº†ä¸¤ç§è¯„ä¼°è®¾ç½®ï¼šæºå†…åŸºå‡†å°†å„æ¥æºå›¾åƒåˆ†åŒºè¿›è¡Œå±€éƒ¨è¯„ä¼°ï¼Œè·¨æºåŸºå‡†åˆ™å®Œå…¨ä¿ç•™æŸäº›æ¥æºä»¥æµ‹è¯•é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚

**Result:** å®éªŒåˆ†ææ˜¾ç¤ºï¼Œç›‘ç£å­¦ä¹ åœ¨æºå†…åŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è·¨åŸŸæƒ…å†µä¸‹æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œé›¶æ ·æœ¬æ¨¡å‹åœ¨æ‰€æœ‰æƒ…å†µä¸‹è¡¨ç°å‡è¾ƒå·®ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç¨€æœ‰å’Œè§†è§‰ç›¸ä¼¼å±ç§çš„åˆ†ç±»æ•ˆæœä¸ä½³ï¼Œè¿™ä¸ºé¢†åŸŸæ³›åŒ–ç ”ç©¶æä¾›äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†çŠç‘šç¤ç›‘æµ‹ä¸­é¢†åŸŸæ³›åŒ–çš„é‡è¦æ€§ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è·¨åŸŸè¯†åˆ«å’Œç¨€æœ‰ç‰©ç§åˆ†ç±»æ–¹é¢çš„å±€é™æ€§ï¼Œé€šè¿‡å‘å¸ƒæ•°æ®é›†ã€åŸºå‡†ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ—¨åœ¨æ¨åŠ¨é²æ£’ã€é¢†åŸŸè‡ªé€‚åº”çš„å…¨çƒçŠç‘šç¤ç›‘æµ‹ä¸ä¿æŠ¤æŠ€æœ¯çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.


### [39] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren, Xinlong Wang, Kexin Wang, Tian Xia, Zihan Ma, Zhaowei Li, Xiangrong Bi, Xiao Li, Xiaowei He*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç¥ç»å…ƒçº§åˆ«çš„åˆ†ææ¡†æ¶ï¼Œé€šè¿‡ç»“åˆäººå·¥ç¥ç»å…ƒåˆ†æå’ŒåŸºäºfMRIçš„ä½“ç´ ç¼–ç ï¼Œæ­ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹ä¸äººç±»å¤§è„‘åœ¨å¤šæ¨¡æ€ä¿¡æ¯å¤„ç†æœºåˆ¶ä¸Šçš„ç›¸ä¼¼æ€§ï¼Œä¸ºç†è§£äººå·¥ç¥ç»ç½‘ç»œä¸ç”Ÿç‰©ç¥ç»ç³»ç»Ÿçš„å¯¹åº”å…³ç³»æä¾›äº†æ–°çš„è¯æ®ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¯¹äººå·¥ç¥ç»ç½‘ç»œä¸äººç±»å¤§è„‘å¤„ç†æœºåˆ¶çš„ç†è§£å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šå•æ¨¡æ€ANNç ”ç©¶æ— æ³•æ•æ‰å¤§è„‘å›ºæœ‰çš„å¤šæ¨¡æ€å¤„ç†èƒ½åŠ›ï¼Œè€Œå¤šæ¨¡æ€ANNç ”ç©¶ä¸»è¦å…³æ³¨é«˜å±‚æ¨¡å‹è¾“å‡ºï¼Œå¿½è§†äº†å•ä¸ªç¥ç»å…ƒçš„å…³é”®ä½œç”¨ã€‚

**Method:** æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ç¥ç»å…ƒçº§åˆ«åˆ†ææ¡†æ¶ï¼Œé€šè¿‡ç»“åˆç²¾ç»†çš„äººå·¥ç¥ç»å…ƒåˆ†æå’ŒåŸºäºfMRIçš„ä½“ç´ ç¼–ç ï¼Œå¯¹ä¸¤ç§æ¶æ„ä¸åŒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆCLIPå’ŒMETERï¼‰è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ã€‚

**Result:** ç ”ç©¶å‘ç°äººå·¥ç¥ç»å…ƒèƒ½å¤ŸæˆåŠŸé¢„æµ‹å¤šä¸ªåŠŸèƒ½ç½‘ç»œä¸­çš„ç”Ÿç‰©ç¥ç»å…ƒæ´»åŠ¨ï¼Œä¸¤è€…éƒ½è¡¨ç°å‡ºåŠŸèƒ½å†—ä½™æ€§ï¼Œäººå·¥ç¥ç»å…ƒå±•ç°å‡ºä¸ç”Ÿç‰©ç¥ç»å…ƒç›¸ä¼¼çš„ææ€§æ¨¡å¼ï¼Œå¹¶ä¸”ä¸åŒVLMæ¶æ„é©±åŠ¨ä¸åŒçš„ç”Ÿç‰©ç¥ç»å…ƒæ¿€æ´»æ¨¡å¼ã€‚

**Conclusion:** è¿™äº›ç»“æœä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨ç±»è„‘å±‚æ¬¡å¤„ç†æä¾›äº†æœ‰åŠ›è¯æ®ï¼Œæ­ç¤ºäº†äººå·¥ç¥ç»ç½‘ç»œä¸ç”Ÿç‰©ç¥ç»ç³»ç»Ÿåœ¨ç¥ç»å…ƒçº§åˆ«çš„å…±äº«è¡¨å¾æœºåˆ¶ï¼Œä¸ºç†è§£AIç³»ç»Ÿçš„ç±»è„‘ç‰¹æ€§åŠå…¶ä¸äººç±»è®¤çŸ¥çš„å¯¹åº”å…³ç³»å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.


### [40] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, Li Yuan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Edit-R1ï¼Œä¸€ç§åŸºäºç­–ç•¥ä¼˜åŒ–çš„åè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡DiffusionNFTæ–¹æ³•å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¥–åŠ±æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æŒ‡ä»¤å¼å›¾åƒç¼–è¾‘æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºç›‘ç£å¾®è°ƒçš„æŒ‡ä»¤å¼å›¾åƒç¼–è¾‘æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆåˆ°æ ‡æ³¨æ¨¡å¼ï¼Œé™åˆ¶äº†å…¶åœ¨è®­ç»ƒåˆ†å¸ƒä¹‹å¤–çš„æ¢ç´¢å’Œæ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿçªç ´è¿™ä¸€é™åˆ¶çš„åè®­ç»ƒæ¡†æ¶ã€‚

**Method:** é‡‡ç”¨Diffusion Negative-aware Finetuningç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸æµåŒ¹é…å‰å‘è¿‡ç¨‹ä¸€è‡´ï¼Œæ”¯æŒé«˜é˜¶é‡‡æ ·å™¨å’Œé«˜æ•ˆè®­ç»ƒï¼›åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºæ— éœ€è®­ç»ƒçš„ç»Ÿä¸€å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡è¾“å‡ºlogitsæä¾›ç»†ç²’åº¦åé¦ˆï¼›è®¾è®¡äº†ä½æ–¹å·®ç»„è¿‡æ»¤æœºåˆ¶æ¥å‡å°‘è¯„åˆ†å™ªå£°å¹¶ç¨³å®šä¼˜åŒ–ã€‚

**Result:** UniWorld-V2åœ¨ImgEditå’ŒGEdit-BenchåŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«è·å¾—4.49å’Œ7.83çš„åˆ†æ•°ï¼Œè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼›è¯¥æ¡†æ¶å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œåœ¨Qwen-Image-Editå’ŒFLUX-Kontextç­‰ä¸åŒåŸºç¡€æ¨¡å‹ä¸Šå‡èƒ½å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

**Conclusion:** Edit-R1æ¡†æ¶é€šè¿‡ç­–ç•¥ä¼˜åŒ–å’ŒMLLMå¥–åŠ±æœºåˆ¶æœ‰æ•ˆè§£å†³äº†æŒ‡ä»¤å¼å›¾åƒç¼–è¾‘ä¸­çš„æ³›åŒ–é—®é¢˜ï¼Œå…¶æ¨¡å‹æ— å…³ç‰¹æ€§å±•ç¤ºäº†å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œä¸ºåè®­ç»ƒä¼˜åŒ–æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.


### [41] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran, Mojtaba Safari, Wenfeng He, Mingzhe Hu, Shansong Wang, Jun Zhou, Xiaofeng Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡å¯¹åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åŸºç¡€æ¨¡å‹è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œç³»ç»Ÿæ€§åœ°åˆ†ç±»äº†è§†è§‰ä¸“ç”¨å’Œè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œåˆ†æäº†å…¶æ¶æ„æ¼”è¿›ã€è®­ç»ƒç­–ç•¥å’Œä¸´åºŠåº”ç”¨ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ä»¥åŠ é€Ÿä¸´åºŠè½¬åŒ–ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå¿«é€Ÿå‘å±•ï¼Œä½†è¯¥é¢†åŸŸä»ç¼ºä¹å¯¹è·¨æ¨¡æ€æ¶æ„æ¼”è¿›ã€è®­ç»ƒèŒƒå¼å’Œä¸´åºŠåº”ç”¨çš„ç³»ç»Ÿæ€§ç»¼åˆï¼Œéœ€è¦ç»Ÿä¸€çš„æ¡†æ¶æ¥æ•´åˆç°æœ‰ç ”ç©¶æˆæœå¹¶æŒ‡å¯¼æœªæ¥å‘å±•ã€‚

**Method:** é‡‡ç”¨ç³»ç»Ÿæ€§åˆ†ç±»æ–¹æ³•å°†ç ”ç©¶åˆ†ä¸ºè§†è§‰ä¸“ç”¨å’Œè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œåˆ†æå…¶æ¶æ„åŸºç¡€ã€è®­ç»ƒç­–ç•¥å’Œä¸‹æ¸¸ä¸´åºŠä»»åŠ¡ï¼Œå¹¶è¿›è¡Œå®šé‡å…ƒåˆ†æä»¥è¡¨å¾æ•°æ®é›†åˆ©ç”¨å’Œåº”ç”¨é¢†åŸŸçš„æ—¶é—´è¶‹åŠ¿ã€‚

**Result:** é€šè¿‡ç³»ç»Ÿæ€§ç»¼è¿°å’Œå®šé‡åˆ†ææ­ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æ¶æ„æ¼”è¿›æ¨¡å¼ã€è®­ç»ƒç­–ç•¥å‘å±•è¶‹åŠ¿ä»¥åŠä¸´åºŠåº”ç”¨åˆ†å¸ƒç‰¹å¾ï¼Œè¯†åˆ«äº†é¢†åŸŸé€‚åº”ã€é«˜æ•ˆå¾®è°ƒå’Œè®¡ç®—çº¦æŸç­‰å…³é”®æŒ‘æˆ˜ã€‚

**Conclusion:** åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†éœ€è¦è§£å†³é¢†åŸŸé€‚åº”ã€å¯è§£é‡Šæ€§å’Œä¸´åºŠé›†æˆç­‰æŒ‘æˆ˜ï¼Œæœªæ¥ç ”ç©¶æ–¹å‘åº”èšç„¦äºå¢å¼ºæ¨¡å‹é²æ£’æ€§ã€å¯è§£é‡Šæ€§å’Œä¸´åºŠå®ç”¨æ€§ä»¥åŠ é€Ÿå®é™…åŒ»ç–—åº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.


### [42] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu, Eleftherios Tsonis, Lucas Degeorge, Vicky Kalogeiton*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Di-Bregmanæ¡†æ¶ï¼Œé€šè¿‡Bregmanæ•£åº¦å¯†åº¦æ¯”åŒ¹é…ç»Ÿä¸€äº†æ‰©æ•£æ¨¡å‹è’¸é¦æ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¸€æ­¥ç”Ÿæˆï¼Œåœ¨CIFAR-10å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜äºåå‘KLè’¸é¦çš„FIDæŒ‡æ ‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ‰©æ•£å’Œæµæ¨¡å‹è™½ç„¶ç”Ÿæˆè´¨é‡é«˜ï¼Œä½†ç”±äºå¤šæ­¥é‡‡æ ·è¿‡ç¨‹å¯¼è‡´è®¡ç®—æˆæœ¬æ˜‚è´µï¼Œç°æœ‰è’¸é¦æ–¹æ³•ç¼ºä¹ç»Ÿä¸€çš„ç†è®ºåŸºç¡€ï¼Œéœ€è¦å»ºç«‹æ›´ç³»ç»ŸåŒ–çš„è’¸é¦æ¡†æ¶æ¥åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚

**Method:** æå‡ºäº†Di-Bregmanæ¡†æ¶ï¼Œå°†æ‰©æ•£è’¸é¦å»ºæ¨¡ä¸ºåŸºäºBregmanæ•£åº¦çš„å¯†åº¦æ¯”åŒ¹é…é—®é¢˜ï¼Œä»å‡¸åˆ†æè§†è§’ç»Ÿä¸€äº†å¤šç§ç°æœ‰ç›®æ ‡å‡½æ•°ï¼Œæä¾›äº†ç†è®ºä¸Šçš„å…±åŒåŸºç¡€ã€‚

**Result:** åœ¨CIFAR-10å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå®éªŒä¸­ï¼ŒDi-Bregmanåœ¨ä¸€æ­¥ç”ŸæˆFIDæŒ‡æ ‡ä¸Šä¼˜äºåå‘KLè’¸é¦æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†ä¸æ•™å¸ˆæ¨¡å‹ç›¸å½“çš„é«˜è§†è§‰ä¿çœŸåº¦ã€‚

**Conclusion:** Bregmanå¯†åº¦æ¯”åŒ¹é…ä¸ºé«˜æ•ˆä¸€æ­¥æ‰©æ•£ç”Ÿæˆæä¾›äº†å®ç”¨ä¸”ç†è®ºåŸºç¡€åšå®çš„è·¯å¾„ï¼Œç»Ÿä¸€äº†å¤šç§è’¸é¦ç›®æ ‡ï¼Œä¸ºæœªæ¥æ‰©æ•£æ¨¡å‹åŠ é€Ÿç ”ç©¶æä¾›äº†æ–°çš„ç†è®ºè§†è§’ã€‚

---

#### ğŸ“„ Abstract
Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.


### [43] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar, Kartikeya Bhardwaj, Jeya Pradha Jeyaraj, Nuno Vasconcelos, Ankita Nayak, Harris Teague*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºV-Reasonæ–¹æ³•ï¼Œé€šè¿‡åŸºäºç†µçš„æ§åˆ¶å™¨ä¼˜åŒ–å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†é¢‘æ¨ç†ä¸­çš„å¾®æ¢ç´¢å’Œå¾®åˆ©ç”¨è¡Œä¸ºï¼Œæ— éœ€å¼ºåŒ–å­¦ä¹ æˆ–ç›‘ç£å¾®è°ƒå³å¯æ˜¾è‘—æå‡æ¨ç†æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†é¢‘æ¨ç†æ•°æ®é›†ä¸Šæ¥è¿‘RLè®­ç»ƒæ¨¡å‹çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘è®¡ç®—å¼€é”€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„è§†é¢‘æ¨ç†æ–¹æ³•ä¾èµ–æ˜‚è´µçš„å¼ºåŒ–å­¦ä¹ å’Œå†—é•¿çš„æ€ç»´é“¾ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†é˜¶æ®µçš„è®¡ç®—å¼€é”€å·¨å¤§ï¼Œä¸”æ¨ç†è¿‡ç¨‹çš„æ§åˆ¶æœºåˆ¶éå¸¸æœ‰é™ã€‚

**Method:** åˆ©ç”¨æ¨¡å‹è¾“å‡ºç†µä½œä¸ºä¿¡å·ï¼Œå‘ç°é«˜è´¨é‡æ¨¡å‹ç»å†å¾®æ¢ç´¢å’Œå¾®åˆ©ç”¨åºåˆ—ä»¥ä¿æŒæ¨ç†è¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œå¹¶åŸºäºæ­¤æå‡ºV-Reasonæ–¹æ³•ï¼šåœ¨æ¨ç†æ—¶é€šè¿‡åŸºäºç†µçš„ç›®æ ‡å‡½æ•°å¯¹å°å‹å¯è®­ç»ƒæ§åˆ¶å™¨è¿›è¡Œå°‘é‡ä¼˜åŒ–æ­¥éª¤ï¼Œè‡ªé€‚åº”è°ƒæ•´LMMçš„å€¼ç¼“å­˜ã€‚

**Result:** åœ¨å¤šä¸ªè§†é¢‘æ¨ç†æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸æ¯”åŸºç¡€æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹å–å¾—æ˜¾è‘—æ”¹è¿›ï¼Œä¸RLè®­ç»ƒæ¨¡å‹çš„å‡†ç¡®ç‡å·®è·ç¼©å°è‡³0.6%ä»¥å†…ï¼ŒåŒæ—¶è¾“å‡ºtokenå‡å°‘58.6%ï¼Œæä¾›å·¨å¤§çš„æ•ˆç‡ä¼˜åŠ¿ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜é€šè¿‡ç†µä¿¡å·ç›´æ¥ä¼˜åŒ–æ¨ç†è¡Œä¸ºå¯æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæ•°æ®æˆ–å¼ºåŒ–å­¦ä¹ ï¼Œä¸ºé«˜æ•ˆè§†é¢‘æ¨ç†æä¾›äº†æ–°çš„ç†è®ºæŒ‡å¯¼å’Œå®ç”¨æ–¹æ³•ã€‚

---

#### ğŸ“„ Abstract
Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.


### [44] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao, Jiyao Liu, Guanghao Li, Yueming Lyu, Jianxiong Gao, Weichen Yu, Ningsheng Xu, Liang Wang, Caifeng Shan, Ziwei Liu, Chenyang Si*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºGOODæ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨ç°æˆçš„åˆ†å¸ƒå†…åˆ†ç±»å™¨ç›´æ¥å¼•å¯¼æ‰©æ•£é‡‡æ ·è½¨è¿¹ç”Ÿæˆåˆ†å¸ƒå¤–æ ·æœ¬ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•è¯­ä¹‰ä¸ç¨³å®šå’Œå¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†åˆ†å¸ƒå¤–æ£€æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒå¤–æ ·æœ¬ç”Ÿæˆæ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰°åŠ¨æ–‡æœ¬æ¡ä»¶åµŒå…¥ï¼Œå¯¼è‡´è¯­ä¹‰ä¸ç¨³å®šå’Œåç§»å¤šæ ·æ€§ä¸è¶³ï¼Œé™åˆ¶äº†åœ¨çœŸå®åˆ†å¸ƒå¤–åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Method:** GOODæ¡†æ¶é‡‡ç”¨åŒå±‚çº§å¼•å¯¼æœºåˆ¶ï¼šå›¾åƒçº§å¼•å¯¼åŸºäºå¯¹æ•°åˆ†å‰²æ¢¯åº¦é™ä½è¾“å…¥ä¼¼ç„¶ï¼Œé©±åŠ¨æ ·æœ¬å‘åƒç´ ç©ºé—´ä½å¯†åº¦åŒºåŸŸç§»åŠ¨ï¼›ç‰¹å¾çº§å¼•å¯¼åŸºäºåˆ†ç±»å™¨æ½œåœ¨ç©ºé—´ä¸­k-NNè·ç¦»ï¼Œä¿ƒè¿›åœ¨ç‰¹å¾ç¨€ç–åŒºåŸŸé‡‡æ ·ï¼Œä»è€Œå®ç°æ›´å¯æ§å’Œå¤šæ ·åŒ–çš„åˆ†å¸ƒå¤–æ ·æœ¬ç”Ÿæˆã€‚

**Result:** å…¨é¢çš„å®šé‡å’Œå®šæ€§åˆ†æè¡¨æ˜ï¼Œä½¿ç”¨GOODç”Ÿæˆçš„æ ·æœ¬è¿›è¡Œè®­ç»ƒå¯ä»¥æ˜¾è‘—æå‡åˆ†å¸ƒå¤–æ£€æµ‹æ€§èƒ½ï¼ŒåŒæ—¶å¼•å…¥äº†ç»Ÿä¸€çš„è‡ªé€‚åº”ç»“åˆå›¾åƒå’Œç‰¹å¾å·®å¼‚çš„åˆ†å¸ƒå¤–è¯„åˆ†æœºåˆ¶ï¼Œå¢å¼ºäº†æ£€æµ‹é²æ£’æ€§ã€‚

**Conclusion:** GOODæ¡†æ¶é€šè¿‡ç›´æ¥å¼•å¯¼æ‰©æ•£é‡‡æ ·è½¨è¿¹çš„åˆ›æ–°æ–¹æ³•ï¼Œä¸ºåˆ†å¸ƒå¤–æ£€æµ‹æä¾›äº†æ›´æœ‰æ•ˆå’Œå¯æ§çš„æ ·æœ¬ç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œå…¶åŒå±‚çº§å¼•å¯¼è®¾è®¡ç¡®ä¿äº†è¯­ä¹‰ç¨³å®šæ€§å’Œå¤šæ ·æ€§ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„åˆ†å¸ƒå¤–æ£€æµ‹æ€§èƒ½æå‡æä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.


### [45] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella, Massimiliano Mancini, Yiming Wang, Alessio Tonioni, Elisa Ricci*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€åœ¨çº¿æ‰§è¡Œçš„è§†é¢‘æ­¥éª¤å®šä½æ–¹æ³•BaGLMï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œé€šè¿‡è´å¶æ–¯æ»¤æ³¢æ•´åˆå†å²å¸§ä¿¡æ¯ï¼Œåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†éœ€è¦è®­ç»ƒçš„ä¼ ç»Ÿç¦»çº¿æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿçš„è§†é¢‘æ­¥éª¤å®šä½æ–¹æ³•éœ€è¦å¸¦æ ‡æ³¨çš„è®­ç»ƒæ•°æ®å’Œç¦»çº¿å¤„ç†å®Œæ•´è§†é¢‘ï¼Œè¿™å¯¼è‡´æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”æ— æ³•åº”ç”¨äºéœ€è¦åœ¨çº¿å†³ç­–çš„åœºæ™¯ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢æ— éœ€è®­ç»ƒå’Œåœ¨çº¿æ‰§è¡Œçš„è§†é¢‘æ­¥éª¤å®šä½æ–¹æ³•ã€‚

**Method:** æœ¬æ–‡åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼ŒåŸºäºæœ‰é™å¸§é›†é¢„æµ‹æ­¥éª¤ï¼Œå¹¶å¼€å‘äº†BaGLMæ–¹æ³•ï¼Œé€šè¿‡è´å¶æ–¯æ»¤æ³¢åŸåˆ™æ•´åˆå†å²å¸§ä¿¡æ¯ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–çš„ä¾èµ–çŸ©é˜µå’Œæ­¥éª¤è¿›åº¦ä¼°è®¡æ¥å»ºæ¨¡æ­¥éª¤è½¬æ¢ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ— éœ€ä»»åŠ¡ç‰¹å®šè°ƒä¼˜çš„åœ¨çº¿ç­–ç•¥ä¼˜äºä¼ ç»Ÿçš„ç¦»çº¿å’ŒåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„æµ‹è¯•æ˜¾ç¤ºï¼ŒBaGLMåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„åŸºäºè®­ç»ƒçš„ç¦»çº¿æ–¹æ³•ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›èƒ½å¤Ÿæœ‰æ•ˆè§£å†³è§†é¢‘æ­¥éª¤å®šä½é—®é¢˜ï¼Œè´å¶æ–¯æ»¤æ³¢æ¡†æ¶çš„å¼•å…¥è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ï¼Œä¸ºæ— éœ€è®­ç»ƒçš„è§†é¢‘ç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆå’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.


### [46] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang, Xinyu Zhang, Peng Du*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºGACO-CADï¼Œä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µåè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ·±åº¦å’Œè¡¨é¢æ³•çº¿å›¾ä½œä¸ºå‡ ä½•å…ˆéªŒï¼Œå¹¶å¼•å…¥ç»„é•¿åº¦å¥–åŠ±æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†ä»å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘å‚æ•°åŒ–CADæ¨¡å‹çš„å‡ ä½•ç²¾åº¦å’Œå»ºæ¨¡ç®€æ´æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä»2Då›¾åƒå‡†ç¡®æ¨æ–­3Då‡ ä½•æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä¸»è¦å—é™äºç©ºé—´æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œè¿™é˜»ç¢äº†å·¥ä¸šæ¦‚å¿µè®¾è®¡ä¸­ä»å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘å‚æ•°åŒ–CADæ¨¡å‹çš„å®é™…åº”ç”¨ã€‚

**Method:** é‡‡ç”¨ä¸¤é˜¶æ®µåè®­ç»ƒæ¡†æ¶ï¼šç›‘ç£å¾®è°ƒé˜¶æ®µåˆ©ç”¨æ·±åº¦å’Œè¡¨é¢æ³•çº¿å›¾ä½œä¸ºå¯†é›†å‡ ä½•å…ˆéªŒï¼Œä¸RGBå›¾åƒå½¢æˆå¤šé€šé“è¾“å…¥ï¼›å¼ºåŒ–å­¦ä¹ é˜¶æ®µå¼•å…¥ç»„é•¿åº¦å¥–åŠ±æœºåˆ¶ï¼Œåœ¨ä¿æŒé«˜å‡ ä½•ä¿çœŸåº¦çš„åŒæ—¶ä¿ƒè¿›ç”Ÿæˆæ›´ç´§å‡‘çš„å»ºæ¨¡åºåˆ—ï¼Œå¹¶é‡‡ç”¨åŠ¨æ€åŠ æƒç­–ç•¥ç¨³å®šè®­ç»ƒã€‚

**Result:** åœ¨DeepCADå’ŒFusion360æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGACO-CADåœ¨ç›¸åŒMLLMéª¨å¹²ç½‘ç»œä¸‹è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œåœ¨ä»£ç æœ‰æ•ˆæ€§ã€å‡ ä½•ç²¾åº¦å’Œå»ºæ¨¡ç®€æ´æ€§æ–¹é¢æŒç»­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆå‡ ä½•å…ˆéªŒå’Œç®€æ´æ€§å¥–åŠ±çš„åŒé‡ä¼˜åŒ–ç­–ç•¥èƒ½æœ‰æ•ˆæå‡CADæ¨¡å‹ç”Ÿæˆè´¨é‡ï¼Œä¸ºå·¥ä¸šè®¾è®¡è‡ªåŠ¨åŒ–æä¾›äº†æ›´å¯é çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†å¤šæ¨¡æ€å­¦ä¹ åœ¨å‡ ä½•æ¨ç†ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.


### [47] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara, Cristian Rodriguez-Opazo, Edison Marrese-Taylor, Felipe Bravo-Marquez*

#### ğŸ§© TL;DR
æœ¬æ–‡é€šè¿‡å®è¯ç ”ç©¶æ¢è®¨äº†ä¸åŒè§†é¢‘ç‰¹å¾å¯¹æ—¶åºè§†é¢‘å®šä½ä»»åŠ¡çš„å½±å“ï¼Œå‘ç°åœ¨ç»å…¸æ¶æ„ä¸­ä»…æ”¹å˜è§†é¢‘ç¼–ç å™¨å³å¯å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ï¼ŒåŒæ—¶æ­ç¤ºäº†ä¸åŒç‰¹å¾ä¹‹é—´çš„æ½œåœ¨äº’è¡¥æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ—¶åºè§†é¢‘å®šä½ç ”ç©¶ä¸»è¦é›†ä¸­äºå°‘æ•°å‡ ç§è§†é¢‘è¡¨ç¤ºæ–¹æ³•ï¼Œè¿™å¯èƒ½å¯¼è‡´é•¿æœŸçš„æ¶æ„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå› æ­¤éœ€è¦ç³»ç»Ÿç ”ç©¶ä¸åŒè§†é¢‘ç‰¹å¾å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**Method:** ç ”ç©¶ä½¿ç”¨åŸºäºCNNã€æ—¶åºæ¨ç†å’ŒTransformerçš„è§†é¢‘ç¼–ç å™¨æå–ç‰¹å¾ï¼Œå¹¶åœ¨Charades-STAã€ActivityNet-Captionså’ŒYouCookIIä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œé‡‡ç”¨ç»å…¸æ¶æ„è¯„ä¼°ä¸åŒè§†é¢‘ç‰¹å¾çš„å½±å“ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œä»…æ”¹å˜è§†é¢‘ç¼–ç å™¨å°±èƒ½åœ¨æ¨¡å‹æ€§èƒ½ä¸Šäº§ç”Ÿæ˜¾è‘—å·®å¼‚ï¼ŒåŒæ—¶æ­ç¤ºäº†ä½¿ç”¨ç‰¹å®šç‰¹å¾æ—¶äº§ç”Ÿçš„æ˜æ˜¾æ¨¡å¼å’Œé”™è¯¯ï¼Œè¡¨æ˜ä¸åŒç‰¹å¾ä¹‹é—´å­˜åœ¨æ½œåœ¨çš„äº’è¡¥æ€§ã€‚

**Conclusion:** ä¸åŒè§†é¢‘ç‰¹å¾å¯¹æ—¶åºè§†é¢‘å®šä½ä»»åŠ¡å…·æœ‰æ˜¾è‘—å½±å“ï¼Œç‰¹å¾é€‰æ‹©ä¸åº”å±€é™äºå°‘æ•°å‡ ç§è¡¨ç¤ºæ–¹æ³•ï¼Œå¤šç§ç‰¹å¾çš„ç»„åˆå¯èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½è¡¨ç°ï¼Œè¿™ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„è®¾è®¡æŒ‡å¯¼ã€‚

---

#### ğŸ“„ Abstract
Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.


### [48] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang, Yuwei Li, Xingyuan Xian, Guoming Tang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬çš„è§†è§‰ä»¤ç‰Œå‰ªææ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æç¤ºæ„ŸçŸ¥è§†è§’åœ¨ä»»åŠ¡ç›¸å…³æ€§å’Œä¿¡æ¯å¤šæ ·æ€§ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œèƒ½å¤Ÿåœ¨å‰ªé™¤é«˜è¾¾90%è§†è§‰ä»¤ç‰Œçš„åŒæ—¶ä¿æŒæ€§èƒ½ï¼Œæ˜¾è‘—é™ä½æ¨ç†æˆæœ¬ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€è§†è§‰è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„æå‡ï¼Œå¤„ç†å¤§è§„æ¨¡è¾“å…¥æ—¶ä¼šäº§ç”Ÿæ˜¾è‘—çš„è§†è§‰ä»¤ç‰Œå†—ä½™ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬è¿‡é«˜ã€‚ç°æœ‰åŸºäºæ³¨æ„åŠ›æˆ–å¤šæ ·æ€§çš„å‰ªææ–¹æ³•é€šå¸¸å¿½ç•¥æ–‡æœ¬æç¤ºçš„æŒ‡å¯¼ï¼Œæ— æ³•æœ‰æ•ˆä¼˜å…ˆè€ƒè™‘ä»»åŠ¡ç›¸å…³æ€§ã€‚

**Method:** æå‡ºäº†ä¸€ç§å±‚æ¬¡åŒ–çš„é›¶æ ·æœ¬æ–¹æ³•ï¼Œé¦–å…ˆé€‰æ‹©ä»»åŠ¡ç›¸å…³çš„æ ¸å¿ƒè§†è§‰ä»¤ç‰Œé›†åˆï¼Œç„¶åè¡¥å……å¤šæ ·æ€§ä»¤ç‰Œä»¥ä¿ç•™æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ˜ç¡®å»ºæ¨¡è§†è§‰ä»¤ç‰Œå‰ªæåœ¨ä»»åŠ¡ç›¸å…³æ€§å’Œä¿¡æ¯å¤šæ ·æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚

**Result:** åœ¨å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‰ªé™¤é«˜è¾¾90%ä»¤ç‰Œçš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æœ€ä¼˜æ–¹æ³•ï¼Œä»…å¸¦æ¥æœ€å°ç²¾åº¦æŸå¤±ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†GPUå†…å­˜å ç”¨å’Œæ¨ç†å»¶è¿Ÿã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æç¤ºæ„ŸçŸ¥çš„è§†è§‰ä»¤ç‰Œå‰ªæç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä¸ºé™ä½è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†æˆæœ¬æä¾›äº†æ–°æ€è·¯ï¼ŒåŒæ—¶å¼ºè°ƒäº†åœ¨å‰ªæè¿‡ç¨‹ä¸­å¹³è¡¡ä»»åŠ¡ç›¸å…³æ€§å’Œä¿¡æ¯å¤šæ ·æ€§çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.


### [49] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick, Effrosyni Mavroudi, Yale Song, Rama Chellappa, Lorenzo Torresani, Triantafyllos Afouras*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ED-VTGæ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œç»†ç²’åº¦è§†é¢‘æ—¶åºå®šä½ï¼Œé€šè¿‡ä¸¤é˜¶æ®µå¤„ç†å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬åŒ–ä¸ºå¯Œå«ç»†èŠ‚çš„å¢å¼ºå¥å­ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§è§£ç å™¨å®ç°ç²¾ç¡®è¾¹ç•Œé¢„æµ‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†é¢‘æ—¶åºå®šä½æ–¹æ³•åœ¨å¤„ç†è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ—¶é¢ä¸´ç»†èŠ‚ç¼ºå¤±å’Œå¹»è§‰å™ªå£°çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯æ¥æå‡å®šä½ç²¾åº¦å’Œé²æ£’æ€§ã€‚

**Method:** é‡‡ç”¨ä¸¤é˜¶æ®µå¤„ç†æµç¨‹ï¼šé¦–å…ˆå°†è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºåŒ…å«ç¼ºå¤±ç»†èŠ‚çš„å¢å¼ºå¥å­ï¼Œç„¶åä½¿ç”¨è½»é‡çº§è§£ç å™¨åŸºäºå¢å¼ºæŸ¥è¯¢çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºé¢„æµ‹å‡†ç¡®è¾¹ç•Œï¼Œå¹¶é€šè¿‡å¤šå®ä¾‹å­¦ä¹ ç›®æ ‡åŠ¨æ€é€‰æ‹©æœ€ä¼˜æŸ¥è¯¢ç‰ˆæœ¬ä»¥å‡è½»å™ªå£°å½±å“ã€‚

**Result:** åœ¨å¤šä¸ªè§†é¢‘æ—¶åºå®šä½å’Œæ®µè½å®šä½åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰å…ˆå‰æå‡ºçš„åŸºäºLLMçš„æ—¶åºå®šä½æ–¹æ³•ï¼Œåœ¨é›¶æ ·æœ¬è¯„ä¼°åœºæ™¯ä¸­ç›¸å¯¹äºä¸“ç”¨æ¨¡å‹ä¿æŒæ˜æ˜¾ä¼˜åŠ¿ã€‚

**Conclusion:** è¯¥æ–¹æ³•è¯æ˜äº†å¤šæ¨¡æ€LLMåœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡æŸ¥è¯¢å¢å¼ºå’ŒåŠ¨æ€é€‰æ‹©æœºåˆ¶æœ‰æ•ˆç¼“è§£äº†å¹»è§‰é—®é¢˜ï¼Œä¸ºç»†ç²’åº¦è§†é¢‘ç†è§£æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.


### [50] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao, Heming Du, Bingqing Zhang, Xin Yu, Xue Li, Sen Wang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†QV-MÂ²å¤šæ—¶åˆ»æ£€ç´¢æ•°æ®é›†å’ŒFlashMMRæ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰è§†é¢‘æ—¶åˆ»æ£€ç´¢æ–¹æ³•å±€é™äºå•æ—¶åˆ»æ£€ç´¢çš„é—®é¢˜ï¼Œåœ¨QV-MÂ²æ•°æ®é›†ä¸Šç›¸æ¯”å…ˆå‰æœ€ä¼˜æ–¹æ³•å®ç°äº†3.00%çš„G-mAPæå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ—¶åˆ»æ£€ç´¢æ–¹æ³•ä¸»è¦å…³æ³¨å•æ—¶åˆ»æ£€ç´¢ï¼Œä½†ç°å®åº”ç”¨ä¸­ä¸€ä¸ªæŸ¥è¯¢å¯èƒ½å¯¹åº”å¤šä¸ªç›¸å…³æ—¶åˆ»ï¼Œè¿™ä½¿å¾—ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•åœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ã€‚

**Method:** æå‡ºäº†FlashMMRæ¡†æ¶ï¼ŒåŒ…å«å¤šæ—¶åˆ»åéªŒè¯æ¨¡å—æ¥ç²¾ç‚¼æ—¶åˆ»è¾¹ç•Œï¼Œé€šè¿‡çº¦æŸæ€§æ—¶åºè°ƒæ•´å’ŒéªŒè¯æ¨¡å—é‡æ–°è¯„ä¼°å€™é€‰ç‰‡æ®µï¼Œé€šè¿‡ç²¾ç»†çš„è¿‡æ»¤ç®¡é“ä¿®å‰ªä½ç½®ä¿¡åº¦æè®®å¹¶å®ç°é²æ£’çš„å¤šæ—¶åˆ»å¯¹é½ã€‚

**Result:** åœ¨QV-MÂ²æ•°æ®é›†ä¸Šï¼ŒFlashMMRç›¸æ¯”å…ˆå‰æœ€ä¼˜æ–¹æ³•åœ¨G-mAPä¸Šæå‡3.00%ï¼Œåœ¨mAP@3+tgtä¸Šæå‡2.70%ï¼Œåœ¨mR@3ä¸Šæå‡2.56%ã€‚QV-MÂ²åŒ…å«2,212ä¸ªæ ‡æ³¨å’Œ6,384ä¸ªè§†é¢‘ç‰‡æ®µã€‚

**Conclusion:** QV-MÂ²æ•°æ®é›†å’ŒFlashMMRæ–¹æ³•ä¸ºæ¨è¿›æ›´ç°å®å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†é¢‘æ—¶åºå®šä½åœºæ™¯ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œæå‡ºçš„åŸºå‡†å’Œæ–¹æ³•åœ¨MMRè®¾ç½®ä¸‹æä¾›äº†æœ‰æ•ˆçš„è®­ç»ƒå’Œè¯„ä¼°å¹³å°ã€‚

---

#### ğŸ“„ Abstract
Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.


### [51] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºW2R2è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡è§£è€¦è¡¨å¾å­¦ä¹ å’Œé’ˆå¯¹æ€§æ·å¾„æŠ‘åˆ¶æ¥è§£å†³å¤šæ¨¡æ€3Då®šä½ä¸­çš„2Dè¯­ä¹‰åå·®é—®é¢˜ï¼Œåœ¨ä¸ä¿®æ”¹æ¨ç†æ¶æ„çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡3Då®šä½ç²¾åº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€3Då®šä½æ¨¡å‹å­˜åœ¨ä¸¥é‡çš„2Dè¯­ä¹‰åå·®é—®é¢˜ï¼Œå³è¿‡åº¦ä¾èµ–2Då›¾åƒç‰¹å¾è¿›è¡Œç²—ç•¥å®šä½è€Œå¿½è§†3Då‡ ä½•è¾“å…¥ï¼Œå¯¼è‡´èåˆæ€§èƒ½æ¬ ä½³ã€‚

**Method:** æå‡ºWhat-Whereè¡¨å¾é‡æ„æ¡†æ¶ï¼Œå°†2Dç‰¹å¾ä½œä¸ºè¯­ä¹‰ä¿¡æ ‡ç”¨äºç‰©ä½“è¯†åˆ«ï¼Œ3Dç‰¹å¾ä½œä¸ºç©ºé—´é”šç‚¹ç”¨äºå®šä½ï¼Œé‡‡ç”¨åŒç›®æ ‡æŸå¤±å‡½æ•°åŒ…æ‹¬ç›‘ç£èåˆé¢„æµ‹çš„å¯¹é½æŸå¤±å’Œæƒ©ç½š2Dä¸»å¯¼ä¼ªè¾“å‡ºçš„è¾¹ç•Œæœºåˆ¶ä¼ªæ ‡ç­¾æŸå¤±ã€‚

**Result:** åœ¨ScanReferå’ŒScanQAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒW2R2åœ¨å®šä½ç²¾åº¦å’Œé²æ£’æ€§æ–¹é¢å–å¾—æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å®¤å¤–åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è§£è€¦è¡¨å¾å­¦ä¹ å’Œé’ˆå¯¹æ€§æ·å¾„æŠ‘åˆ¶å¯æœ‰æ•ˆç¼“è§£2Dè¯­ä¹‰åå·®ï¼Œä¸ºå¤šæ¨¡æ€3Då®šä½æä¾›äº†æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.


### [52] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann, Orr Zohar, Amir Mahla, Xiaohan Wang, Rui Li, Thibaud Frere, Leandro von Werra, Aritra Roy Gosthipaty, AndrÃ©s Marafioti*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†FineVisionï¼Œä¸€ä¸ªç»è¿‡ç²¾å¿ƒæ”¶é›†ã€æ•´ç†å’Œç»Ÿä¸€çš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«2400ä¸‡ä¸ªæ ·æœ¬ï¼Œæ˜¯ç›®å‰åŒç±»ä¸­æœ€å¤§çš„å¼€æ”¾èµ„æºã€‚è¯¥æ•°æ®é›†é€šè¿‡åŠè‡ªåŠ¨åŒ–äººå·¥å‚ä¸æµç¨‹æ•´åˆäº†200å¤šä¸ªæ¥æºï¼Œå¹¶ç»è¿‡ä¸¥æ ¼å»é‡å’Œå»æ±¡æŸ“å¤„ç†ï¼Œåœ¨å¹¿æ³›è¯„ä¼°ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰å¼€æ”¾æ··åˆæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹çš„å‘å±•å—åˆ°ç¢ç‰‡åŒ–ã€ä¸ä¸€è‡´å’Œå—æ±¡æŸ“å…¬å…±æ•°æ®é›†çš„é˜»ç¢ï¼Œè¿™äº›æ•°æ®é›†ç¼ºä¹ç»Ÿä¸€æ ‡å‡†å’Œä¸¥æ ¼çš„è´¨é‡æ§åˆ¶ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„è¿›ä¸€æ­¥æå‡å’Œå¯é è¯„ä¼°ã€‚

**Method:** é‡‡ç”¨åŠè‡ªåŠ¨åŒ–äººå·¥å‚ä¸æµç¨‹ï¼Œè‡ªåŠ¨åŒ–å¤„ç†æ‰¹é‡æ•°æ®æ‘„å…¥å’Œæ¨¡å¼æ˜ å°„ï¼ŒåŒæ—¶ç”±å®¡æ ¸äººå‘˜å®¡æŸ¥æ˜ å°„ç»“æœå¹¶æŠ½æ ·æ£€æŸ¥è¾“å‡ºè´¨é‡ï¼Œç¡®ä¿æ³¨é‡Šçš„å¿ å®è½¬æ¢ã€æ ¼å¼é€‚å½“æ€§ã€å¤šæ ·æ€§å’Œå®‰å…¨æ€§ï¼›è¯¥æµç¨‹è¿˜åº”ç”¨äº†ä¸¥æ ¼çš„å†…éƒ¨å’Œè·¨æºå»é‡ï¼Œä»¥åŠé’ˆå¯¹66ä¸ªå…¬å…±åŸºå‡†çš„å»æ±¡æŸ“å¤„ç†ã€‚

**Result:** åœ¨FineVisionä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å¹¿æ³›çš„è¯„ä¼°å¥—ä»¶ä¸­æŒç»­ä¼˜äºåŸºäºç°æœ‰å¼€æ”¾æ··åˆæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ï¼Œè¯æ˜äº†æ•°æ®é›†è§„æ¨¡ã€æ•°æ®å«ç”Ÿä»¥åŠå¹³è¡¡è‡ªåŠ¨åŒ–ä¸äººå·¥ç›‘ç£çš„ç»¼åˆæ•ˆç›Šã€‚

**Conclusion:** ç ”ç©¶å¼ºè°ƒäº†å¤§è§„æ¨¡ã€é«˜è´¨é‡æ•°æ®é›†å¯¹è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½æå‡çš„é‡è¦æ€§ï¼Œå±•ç¤ºäº†åŠè‡ªåŠ¨åŒ–äººå·¥å‚ä¸æµç¨‹åœ¨æ•°æ®é›†æ„å»ºä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶å‘å¸ƒçš„è¯­æ–™åº“å’Œæ•´ç†å·¥å…·å°†åŠ é€Ÿä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ç ”ç©¶å‘å±•ã€‚

---

#### ğŸ“„ Abstract
The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.


### [53] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani, Alon Harell, Hyomin Choi, Runyu Yang, Fabien Racape, Ivan V. Bajic*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡æ¯”è¾ƒé€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹Hieraä¸ä¸“ç”¨åˆ†å‰²æ¨¡å‹SAM2çš„ç‰¹å¾é€šç”¨æ€§ï¼Œé‡åŒ–äº†æ¨¡å‹ä¸“ä¸šåŒ–å¸¦æ¥çš„ä¿¡æ¯è®ºä»£ä»·ï¼Œæ­ç¤ºäº†ä¸“ç”¨æ¨¡å‹åœ¨ç©ºé—´ç›¸å…³ä»»åŠ¡ä¸Šçš„ä¼˜åŠ¿ä¸è¯­ä¹‰ä¿¡æ¯æŸå¤±ä¹‹é—´çš„æƒè¡¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç ”ç©¶å°šæœªå……åˆ†ç†è§£é€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹ä¸å…¶ä¸“ç”¨å¯¹åº”æ¨¡å‹ä¹‹é—´çš„æƒè¡¡å…³ç³»ï¼Œè¿™å¯¹äºé«˜æ•ˆç‰¹å¾ç¼–ç è®¾è®¡è‡³å…³é‡è¦ï¼Œéœ€è¦é‡åŒ–ä¸“ä¸šåŒ–å¸¦æ¥çš„ä¿¡æ¯è®ºä»£ä»·å’Œç‰¹å¾é€šç”¨æ€§æŸå¤±ã€‚

**Method:** é‡‡ç”¨è½»é‡çº§å¯è®­ç»ƒé¢ˆéƒ¨ç½‘ç»œæ¥æ¢æµ‹å†»ç»“ç‰¹å¾çš„å¯é€‚åº”æ€§ï¼Œé€šè¿‡ä¿¡æ¯è®ºæ–¹æ³•é‡åŒ–ä¸“ä¸šåŒ–æˆæœ¬ï¼Œå¹¶å¯¹SAM2è¿›è¡Œè·¨é¢ˆéƒ¨åˆ†æä»¥æ­ç¤ºä¸åŒå±‚çº§é€‚åº”çš„è¡¨å¾ç“¶é¢ˆæ•ˆåº”ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºSAM2åœ¨æ·±åº¦ä¼°è®¡ç­‰ç©ºé—´ç›¸å…³ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å§¿æ€ä¼°è®¡å’Œå›¾åƒæè¿°ç­‰æ¦‚å¿µè·ç¦»è¾ƒè¿œçš„ä»»åŠ¡ä¸Šæ˜¾è‘—è½åäºé€šç”¨æ¨¡å‹Hieraï¼Œè¡¨æ˜ä¸“ç”¨ç¼–ç å™¨å­˜åœ¨æ›´å¹¿æ³›çš„è¯­ä¹‰ä¿¡æ¯æŸå¤±ï¼Œä¸”æ¯ä¸ªé€‚åº”å±‚çº§éƒ½ä¼šäº§ç”Ÿé¢å¤–çš„è¡¨å¾ç“¶é¢ˆã€‚

**Conclusion:** ç ”ç©¶é˜æ˜äº†ç‰¹å¾é€šç”¨æ€§ä¸ä¸“ä¸šåŒ–ä¹‹é—´çš„é‡åŒ–æƒè¡¡å…³ç³»ï¼Œä¸ºé’ˆå¯¹ä¸åŒä¸‹æ¸¸åº”ç”¨è®¾è®¡é«˜æ•ˆç‰¹å¾ç¼–ç å’Œé€‚åº”ç­–ç•¥æä¾›äº†ç†è®ºåŸºç¡€ï¼Œå¼ºè°ƒäº†åœ¨æ¨¡å‹è®¾è®¡ä¸­å¹³è¡¡ä¸“ä¸šåŒ–ä¸é€šç”¨æ€§çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.


### [54] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi, Yoann Dupas, Christophe C'erin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†FMCAFï¼ˆæ»¤æ³¢å¤šæ¨¡æ€äº¤å‰æ³¨æ„åŠ›èåˆï¼‰æ¶æ„ï¼Œé€šè¿‡é¢‘åŸŸæ»¤æ³¢å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºRGBä¸çº¢å¤–å›¾åƒçš„ç‰¹å¾èåˆï¼Œåœ¨å¤šç§å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å®ç°äº†æ€§èƒ½æå‡ï¼Œæ— éœ€ç‰¹å®šæ•°æ®é›†è°ƒä¼˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹åœ¨æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹é€šè¿‡èåˆå¤šç§ä¼ æ„Ÿå™¨æ¨¡æ€çš„äº’è¡¥ä¿¡æ¯æ¥æé«˜é²æ£’æ€§ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€é’ˆå¯¹ç‰¹å®šæ•°æ®é›†è®¾è®¡ï¼Œç¼ºä¹é€šç”¨æ€§ã€‚æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ç§é€šç”¨çš„é¢„å¤„ç†æ¶æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆèåˆRGBå’Œçº¢å¤–è¾“å…¥ï¼Œæå‡å¤šæ¨¡æ€æ£€æµ‹æ€§èƒ½è€Œä¸éœ€è¦æ•°æ®é›†ç‰¹å®šçš„è°ƒä¼˜ã€‚

**Method:** FMCAFæ¶æ„åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¢‘åŸŸæ»¤æ³¢æ¨¡å—ï¼ˆFreq-Filterï¼‰ç”¨äºæŠ‘åˆ¶å†—ä½™å…‰è°±ç‰¹å¾ï¼Œä»¥åŠåŸºäºäº¤å‰æ³¨æ„åŠ›çš„èåˆæ¨¡å—ï¼ˆMCAFï¼‰ç”¨äºæ”¹å–„æ¨¡æ€é—´ç‰¹å¾å…±äº«ã€‚è¯¥æ¶æ„ä½œä¸ºé¢„å¤„ç†æ¨¡å—è®¾è®¡ï¼Œå¯ä¸ç°æœ‰æ£€æµ‹å™¨ç»“åˆä½¿ç”¨ï¼Œä¸“æ³¨äºå¢å¼ºè¾“å…¥ç‰¹å¾çš„è´¨é‡å’Œäº’è¡¥æ€§ã€‚

**Result:** åœ¨LLVIPï¼ˆä½å…‰è¡Œäººæ£€æµ‹ï¼‰å’ŒVEDAIï¼ˆèˆªç©ºè½¦è¾†æ£€æµ‹ï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFMCAFæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æ‹¼æ¥èåˆæ–¹æ³•ï¼Œåœ¨VEDAIä¸Šå®ç°äº†+13.9%çš„mAP@50æå‡ï¼Œåœ¨LLVIPä¸Šå®ç°äº†+1.1%çš„æå‡ã€‚è¿™äº›ç»“æœéªŒè¯äº†FMCAFåœ¨ä¸åŒå¤šæ¨¡æ€æŒ‘æˆ˜ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** FMCAFå±•ç¤ºäº†ä½œä¸ºé€šç”¨å¤šæ¨¡æ€èåˆåŸºç¡€æ¶æ„çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ£€æµ‹ä»»åŠ¡è€Œæ— éœ€ç‰¹å®šè°ƒä¼˜ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´é²æ£’çš„å¤šæ¨¡æ€æ£€æµ‹ç³»ç»Ÿæä¾›äº†çµæ´»çš„åŸºç¡€ï¼Œæœªæ¥å¯æ‰©å±•åˆ°å…¶ä»–ä¼ æ„Ÿå™¨æ¨¡æ€ç»„åˆï¼Œæ¨åŠ¨å¤šæ¨¡æ€æ„ŸçŸ¥æŠ€æœ¯çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.


### [55] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi, Tomoya Nakamura, Hiroaki Santo, SaiKiran Kumar Tedla, Trevor Dalton Canham, Yasushi Yagi, Michael S. Brown*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨LEDç¯å¢ƒç…§æ˜ä¸ºæ¶ˆè´¹çº§ç›¸æœºç”Ÿæˆè§†è§‰ä¸å¯è§æ°´å°çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–LEDå…‰æºçš„å…‰è°±ç‰¹æ€§ï¼Œä½¿å…¶å¯¹äººçœ¼å‡ ä¹ä¸å¯è§ä½†å¯¹ç›¸æœºé«˜åº¦å¯æ£€æµ‹ï¼Œå®ç°äº†åœ¨æ ‡å‡†å¸§ç‡ä¸‹æå–æ°´å°çš„èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰éœ€è¦ä¸€ç§èƒ½å¤Ÿåœ¨æ¶ˆè´¹çº§ç›¸æœºæ‹æ‘„çš„è§†é¢‘ä¸­åµŒå…¥ä¸å¯è§æ°´å°çš„æŠ€æœ¯ï¼Œä»¥æ”¯æŒéšç§ä¿æŠ¤å’Œå†…å®¹éªŒè¯ç­‰åº”ç”¨ï¼Œä½†ä¼ ç»Ÿå¯è§å…‰é€šä¿¡æ–¹æ³•é€šå¸¸éœ€è¦é«˜å¸§ç‡ä¸”å¯èƒ½è¢«äººçœ¼å¯Ÿè§‰ã€‚

**Method:** è¯¥æ–¹æ³•è”åˆè€ƒè™‘äº†äººçœ¼è§†è§‰ç³»ç»Ÿå¯¹å¯è§å…‰è°±çš„æ•æ„Ÿæ€§ã€ç°ä»£æ¶ˆè´¹ç›¸æœºä¼ æ„Ÿå™¨çš„å…‰è°±çµæ•åº¦ä»¥åŠçª„å¸¦LEDç”Ÿæˆå®½å¸¦å…‰è°±çš„èƒ½åŠ›ï¼Œé‡‡ç”¨å…‰è°±è°ƒåˆ¶è€Œéå¼ºåº¦è°ƒåˆ¶æ¥ç¡®ä¿ä¸å¯æ„ŸçŸ¥æ€§ï¼Œå¹¶ä¼˜åŒ–LEDå…‰æºçš„å…‰è°±è½®å»“ä½¿å…¶åœ¨D65ç…§æ˜ä¸‹å‘ˆç°ä¸º"ç™½å…‰"ã€‚

**Result:** è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ ‡å‡†ä½å¸§ç‡ï¼ˆ30-60 fpsï¼‰ä¸‹æå–æ°´å°ï¼Œä¿¡æ¯ä¼ è¾“é€Ÿç‡é€‚ä¸­ï¼Œåœ¨10ç§’è§†é¢‘ç‰‡æ®µä¸­å¯åµŒå…¥128ä½æ•°æ®ï¼Œè¿™ä¸€å®¹é‡è¶³ä»¥æ”¯æŒåŸºæœ¬çš„å…ƒæ•°æ®éœ€æ±‚ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†åˆ©ç”¨ç¯å¢ƒç…§æ˜å®ç°è§†è§‰ä¸å¯è§æ°´å°çš„å¯è¡Œæ€§ï¼Œä¸ºéšç§ä¿æŠ¤å’Œå†…å®¹éªŒè¯åº”ç”¨æä¾›äº†ä¸€ç§å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒäº†ä¸ç°æœ‰æ¶ˆè´¹çº§ç›¸æœºç³»ç»Ÿçš„å…¼å®¹æ€§ã€‚

---

#### ğŸ“„ Abstract
This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.


### [56] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu, Long Zhang, Yue Du, Bin Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯„åˆ†æ ‡å‡†å¼•å¯¼çš„ä¼ªæ ‡ç­¾æç¤ºæ¡†æ¶ï¼Œé€šè¿‡å°†å°‘é‡çœŸå®æ ‡æ³¨è½¬åŒ–ä¸ºé«˜ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾å¹¶èšåˆä¸ºç»“æ„åŒ–è¯„åˆ†æ ‡å‡†ï¼Œå®ç°äº†æ— éœ€å‚æ•°è°ƒä¼˜çš„é›¶æ ·æœ¬è§†é¢‘æ‘˜è¦æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨SumMeå’ŒTVSumæ•°æ®é›†ä¸Šè¶…è¶Šäº†æ— ç›‘ç£å’Œå…ˆå‰é›¶æ ·æœ¬åŸºçº¿ï¼Œæ¥è¿‘ç›‘ç£æ–¹æ³•æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç›‘ç£æ–¹æ³•ä¾èµ–å¯†é›†æ ‡æ³¨å¯¼è‡´é«˜æ˜‚æ ‡æ³¨æˆæœ¬å’Œæœ‰é™è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œæ— ç›‘ç£æ–¹æ³•éš¾ä»¥æ•æ‰é«˜å±‚æ¬¡è¯­ä¹‰å’Œç»†ç²’åº¦å™äº‹çº¿ç´¢ï¼Œè€Œé›¶æ ·æœ¬æç¤ºæ–¹æ³•å¯¹äººå·¥è®¾è®¡æç¤ºæ¨¡æ¿å’Œæ•°æ®é›†ç‰¹å®šåˆ†æ•°å½’ä¸€åŒ–é«˜åº¦æ•æ„Ÿã€‚æœ¬æ–‡æ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶ï¼Œå¼€å‘ä¸€ç§ç¨³å®šä¸”å¯è§£é‡Šçš„é›¶æ ·æœ¬è§†é¢‘æ‘˜è¦æ–¹æ³•ã€‚

**Method:** æå‡ºè¯„åˆ†æ ‡å‡†å¼•å¯¼çš„ä¼ªæ ‡ç­¾æç¤ºæ¡†æ¶ï¼Œå°†å°‘é‡çœŸå®æ ‡æ³¨è½¬åŒ–ä¸ºé«˜ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾å¹¶èšåˆä¸ºç»“æ„åŒ–ã€æ•°æ®é›†è‡ªé€‚åº”çš„è¯„åˆ†æ ‡å‡†ã€‚æ¨ç†æ—¶é¦–å°¾ç‰‡æ®µä»…åŸºäºæè¿°è¯„åˆ†ï¼Œä¸­é—´ç‰‡æ®µåˆ™ç»“åˆç›¸é‚»åœºæ™¯çš„ä¸Šä¸‹æ–‡æ‘˜è¦æ¥è¯„ä¼°å™äº‹è¿›å±•å’Œå†—ä½™åº¦ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æç¤ºä½¿LLMåœ¨æ— éœ€å‚æ•°è°ƒä¼˜ä¸‹å¹³è¡¡å±€éƒ¨æ˜¾è‘—æ€§å’Œå…¨å±€è¿è´¯æ€§ã€‚

**Result:** åœ¨SumMeå’ŒTVSumæ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°57.58å’Œ63.05çš„F1åˆ†æ•°ï¼Œè¶…è¶Šäº†æ— ç›‘ç£æ–¹æ³•å’Œå…ˆå‰é›¶æ ·æœ¬åŸºçº¿ï¼Œæ¥è¿‘ç›‘ç£æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆç¨³å®šåŸºäºLLMçš„è¯„åˆ†è¿‡ç¨‹ã€‚

**Conclusion:** è¯„åˆ†æ ‡å‡†å¼•å¯¼çš„ä¼ªæ ‡ç­¾æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆç¨³å®šLLMåŸºè¯„åˆ†è¿‡ç¨‹ï¼Œä¸ºè§†é¢‘æ‘˜è¦å»ºç«‹äº†ä¸€ä¸ªé€šç”¨ä¸”å¯è§£é‡Šçš„é›¶æ ·æœ¬èŒƒå¼ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç»“æ„åŒ–è¯„åˆ†æ ‡å‡†å’Œä¸Šä¸‹æ–‡æç¤ºå®ç°æ— éœ€è®­ç»ƒçš„é«˜è´¨é‡è§†é¢‘æ‘˜è¦ï¼Œä¸ºåç»­é›¶æ ·æœ¬è§†é¢‘ç†è§£ç ”ç©¶æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.


### [57] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang, Zhongyi Fan, Yonghang Zhang, Zhangzikang Li, Weifeng Chen, Zhongwei Feng, Chaoyue Wang, Peng Hou, Anxiang Zeng*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¼˜åŒ–å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹è®­ç»ƒçš„å››æ”¯æŸ±æ¡†æ¶ï¼Œæ¶µç›–æ•°æ®å¤„ç†ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒç­–ç•¥å’ŒåŸºç¡€è®¾æ–½ï¼Œå¼€å‘å‡ºçš„MUG-V 10Bæ¨¡å‹åœ¨æ•´ä½“æ€§èƒ½ä¸Šè¾¾åˆ°å½“å‰æœ€ä¼˜æ°´å¹³ï¼Œå¹¶åœ¨ç”µå•†è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­è¶…è¶Šé¢†å…ˆå¼€æºåŸºçº¿ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆæ¨¡å‹è®­ç»ƒé¢ä¸´è·¨æ¨¡æ€æ–‡æœ¬-è§†é¢‘å¯¹é½ã€é•¿åºåˆ—å¤„ç†å’Œå¤æ‚æ—¶ç©ºä¾èµ–ç­‰æŒ‘æˆ˜ï¼Œå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ç‰¹åˆ«å›°éš¾å’Œèµ„æºå¯†é›†ï¼Œéœ€è¦ç³»ç»Ÿæ€§çš„ä¼˜åŒ–æ–¹æ¡ˆæ¥æå‡è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

**Method:** æå‡ºçš„è®­ç»ƒæ¡†æ¶ç³»ç»Ÿä¼˜åŒ–äº†å››ä¸ªå…³é”®æ”¯æŸ±ï¼šæ•°æ®å¤„ç†ï¼ˆåŒ…æ‹¬æ•°æ®é¢„å¤„ç†å’Œè§†é¢‘å‹ç¼©ï¼‰ã€æ¨¡å‹æ¶æ„ï¼ˆå‚æ•°ç¼©æ”¾è®¾è®¡ï¼‰ã€è®­ç»ƒç­–ç•¥ï¼ˆè¯¾ç¨‹å¼é¢„è®­ç»ƒå’Œå¯¹é½å¯¼å‘çš„åè®­ç»ƒï¼‰ä»¥åŠåŸºç¡€è®¾æ–½ï¼ˆåŸºäºMegatron-Coreçš„å¤§è§„æ¨¡è®­ç»ƒå®ç°ï¼‰ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„è®­ç»ƒæ•ˆç‡æå‡ã€‚

**Result:** MUG-V 10Bæ¨¡å‹åœ¨æ•´ä½“è§†é¢‘ç”Ÿæˆè´¨é‡ä¸ŠåŒ¹é…å½“å‰æœ€ä¼˜æ¨¡å‹ï¼Œåœ¨ç”µå•†å¯¼å‘çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œé€šè¿‡äººå·¥è¯„ä¼°è¶…è¶Šäº†é¢†å…ˆçš„å¼€æºåŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶è®­ç»ƒæ¡†æ¶å®ç°äº†æ˜¾è‘—æ•ˆç‡å¢ç›Šå’Œæ€§èƒ½æ”¹è¿›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç³»ç»Ÿæ€§ä¼˜åŒ–è®­ç»ƒæ¡†æ¶çš„å››ä¸ªæ”¯æŸ±å¯ä»¥æ˜¾è‘—æå‡å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆçš„æ•ˆç‡å’Œæ€§èƒ½ï¼Œå¼€æºå‘å¸ƒçš„å®Œæ•´æŠ€æœ¯æ ˆä¸ºç¤¾åŒºæä¾›äº†é¦–ä¸ªåŸºäºMegatron-Coreçš„å¤§è§„æ¨¡è§†é¢‘ç”Ÿæˆè®­ç»ƒä»£ç ï¼Œå®ç°äº†é«˜æ•ˆçš„è¿‘çº¿æ€§å¤šèŠ‚ç‚¹æ‰©å±•èƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.


### [58] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan, Peiru Wang, Yao Zhu, Kaiyu Pang, Qingyan Wei, Huiqi Li, Linfeng Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†GtRï¼ˆGeneration then Reconstructionï¼‰ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„åˆ†å±‚é‡‡æ ·ç­–ç•¥ï¼Œé€šè¿‡å°†å›¾åƒç”Ÿæˆåˆ†è§£ä¸ºç»“æ„ç”Ÿæˆå’Œç»†èŠ‚é‡å»ºä¸¤ä¸ªé˜¶æ®µï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å®ç°äº†3.72å€çš„åŠ é€Ÿï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŠ é€Ÿæ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ©ç è‡ªå›å½’æ¨¡å‹è™½ç„¶å…·å¤‡å¹¶è¡Œç”Ÿæˆèƒ½åŠ›ï¼Œä½†å…¶åŠ é€Ÿæ½œåŠ›å—åˆ°å•æ­¥ä¸­ç©ºé—´ç›¸å…³è§†è§‰æ ‡è®°å»ºæ¨¡å¤æ‚åº¦çš„é™åˆ¶ï¼Œéœ€è¦è§£å†³ç”Ÿæˆæ•ˆç‡ä¸è´¨é‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚

**Method:** æå‡ºäº†GtRåˆ†å±‚é‡‡æ ·ç­–ç•¥ï¼Œå°†ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºç»“æ„ç”Ÿæˆå’Œç»†èŠ‚é‡å»ºä¸¤ä¸ªé˜¶æ®µï¼šç»“æ„ç”Ÿæˆå»ºç«‹å…¨å±€è¯­ä¹‰æ¡†æ¶ï¼Œç»†èŠ‚é‡å»ºé«˜æ•ˆå®Œæˆå‰©ä½™æ ‡è®°ï¼›åŒæ—¶å¼•å…¥é¢‘ç‡åŠ æƒæ ‡è®°é€‰æ‹©æœºåˆ¶ï¼ŒåŸºäºé«˜é¢‘ä¿¡æ¯èƒ½é‡ä¸ºå›¾åƒç»†èŠ‚åŒºåŸŸçš„æ ‡è®°åˆ†é…æ›´å¤šè®¡ç®—èµ„æºã€‚

**Result:** åœ¨ImageNetç±»åˆ«æ¡ä»¶ç”Ÿæˆå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGtRåœ¨MAR-Hæ¨¡å‹ä¸Šå®ç°äº†3.72å€åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒå¯æ¯”è¾ƒçš„ç”Ÿæˆè´¨é‡ï¼ˆFIDï¼š1.59ï¼ŒISï¼š304.4 vs åŸå§‹1.59ï¼Œ299.1ï¼‰ï¼Œåœ¨å„ç§æ¨¡å‹è§„æ¨¡å’Œç”Ÿæˆä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŠ é€Ÿæ–¹æ³•ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡åˆ†å±‚ç”Ÿæˆç­–ç•¥å¯ä»¥æœ‰æ•ˆå¹³è¡¡ç”Ÿæˆæ•ˆç‡ä¸è´¨é‡ï¼Œé¢‘ç‡æ„ŸçŸ¥çš„æ ‡è®°é€‰æ‹©æœºåˆ¶èƒ½å¤Ÿä¼˜åŒ–è®¡ç®—èµ„æºåˆ†é…ï¼Œä¸ºé«˜æ•ˆè§†è§‰ç”Ÿæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

---

#### ğŸ“„ Abstract
Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.


### [59] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*SÃ©bastien Thuau, Siba Haidar, Ayush Bajracharya, Rachid Chelouah*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æ¯”è¾ƒäº†ä¸¤ç§èŠ‚ä¿­è”é‚¦å­¦ä¹ æ–¹æ³•ç”¨äºæš´åŠ›æ£€æµ‹ï¼šåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬ä¸è”é‚¦å¾®è°ƒç­–ç•¥ï¼Œä»¥åŠç´§å‡‘3Då·ç§¯ç¥ç»ç½‘ç»œçš„ä¸ªæ€§åŒ–è®­ç»ƒæ–¹æ³•ï¼Œé‡ç‚¹å…³æ³¨èƒ½æºæ•ˆç‡å’Œç¯å¢ƒæŒ‡æ ‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æš´åŠ›æ£€æµ‹ç³»ç»Ÿåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹éƒ¨ç½²æ—¶é¢ä¸´èƒ½æºæ¶ˆè€—å’Œè®¡ç®—æ•ˆç‡çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®åœºæ™¯ä¸‹ï¼Œéœ€è¦æ¢ç´¢æ—¢èƒ½ä¿æŒé«˜æ€§èƒ½åˆå…·å¤‡èƒ½æºæ•ˆç‡çš„è§£å†³æ–¹æ¡ˆã€‚

**Method:** é‡‡ç”¨ä¸¤ç§äº’è¡¥ç­–ç•¥ï¼šä½¿ç”¨LLaVA-7Bè¿›è¡Œé›¶æ ·æœ¬å’Œè”é‚¦å¾®è°ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹æ–¹æ³•ï¼Œä»¥åŠåŸºäº65.8Må‚æ•°ç´§å‡‘3Då·ç§¯ç¥ç»ç½‘ç»œçš„ä¸ªæ€§åŒ–è®­ç»ƒæ–¹æ³•ï¼Œå¹¶åœ¨éç‹¬ç«‹åŒåˆ†å¸ƒè®¾ç½®ä¸‹è¯„ä¼°å‡†ç¡®ç‡ã€æ ¡å‡†åº¦å’Œèƒ½æºä½¿ç”¨æƒ…å†µã€‚

**Result:** ä¸¤ç§æ–¹æ³•å‡è¶…è¿‡90%çš„å‡†ç¡®ç‡ï¼ŒCNN3Dåœ¨ROC AUCå’Œlog lossæŒ‡æ ‡ä¸Šç•¥ä¼˜äºä½ç§©è‡ªé€‚åº”å¾®è°ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶èƒ½è€—æ›´ä½ï¼›è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡æ¨ç†å’Œå¤šæ¨¡æ€æ¨æ–­æ–¹é¢ä»å…·ä¼˜åŠ¿ã€‚

**Conclusion:** ç ”ç©¶æ”¯æŒæ··åˆæ¨¡å‹æ¶æ„ï¼šè½»é‡çº§CNNç”¨äºå¸¸è§„åˆ†ç±»ä»»åŠ¡ï¼Œé€‰æ‹©æ€§æ¿€æ´»è§†è§‰è¯­è¨€æ¨¡å‹å¤„ç†å¤æ‚æˆ–æè¿°æ€§åœºæ™¯ï¼Œä¸ºè§†é¢‘ç›‘æ§é¢†åŸŸæä¾›å¯å¤ç°çš„èµ„æºæ„ŸçŸ¥AIåŸºå‡†æ¡†æ¶ï¼Œå¯æ‰©å±•è‡³å®æ—¶å¤šæ¨¡æ€å’Œç”Ÿå‘½å‘¨æœŸæ„ŸçŸ¥ç³»ç»Ÿã€‚

---

#### ğŸ“„ Abstract
We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings.
  Both approaches exceed 90% accuracy. CNN3D slightly outperforms Low-Rank
Adaptation(LoRA)-tuned VLMs in ROC AUC and log loss, while using less energy.
VLMs remain favorable for contextual reasoning and multimodal inference. We
quantify energy and CO$_2$ emissions across training and inference, and analyze
sustainability trade-offs for deployment.
  To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics.
  These findings support a hybrid model: lightweight CNNs for routine
classification, with selective VLM activation for complex or descriptive
scenarios. The resulting framework offers a reproducible baseline for
responsible, resource-aware AI in video surveillance, with extensions toward
real-time, multimodal, and lifecycle-aware systems.


### [60] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang, Hu Chen, Zhe Yuan, Sukun Tian, Peng Feng*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºIC-MoEæ¨¡å‹ï¼Œé€šè¿‡æ··åˆä¸“å®¶æ¶æ„å’Œè¯­ä¹‰å¼•å¯¼å¯¹æ¯”å­¦ä¹ ï¼Œè§£å†³äº†åŒ»å­¦å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹è‡ªé€‚åº”å¾®è°ƒä¸­çš„é«˜å±‚ç‰¹å¾è¡¨ç¤ºä¸è¶³å’Œé¢„è®­ç»ƒæƒé‡ç»“æ„å®Œæ•´æ€§ç ´åé—®é¢˜ï¼Œåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹è‡ªé€‚åº”å¾®è°ƒæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šé«˜å±‚ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ä¸è¶³ï¼Œä»¥åŠå¾®è°ƒè¿‡ç¨‹ä¼šç ´åé¢„è®­ç»ƒæƒé‡çš„ç»“æ„å®Œæ•´æ€§ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Method:** æå‡ºIC-MoEæ¨¡å‹ï¼Œæ„å»ºåŸºç¡€ä¸“å®¶ã€è¯­ä¹‰ä¸“å®¶å’Œè‡ªé€‚åº”ä¸“å®¶ï¼Œé‡‡ç”¨åƒç´ æ¦‚ç‡è‡ªé€‚åº”æŠ•ç¥¨ç­–ç•¥å®ç°ä¸“å®¶é€‰æ‹©å’Œèåˆï¼›åŒæ—¶æå‡ºè¯­ä¹‰å¼•å¯¼å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ ‡ç­¾ä¸€è‡´æ€§å’Œè´Ÿè½½å¹³è¡¡å¢å¼ºé«˜å±‚ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒé¢„è®­ç»ƒæƒé‡ç»“æ„å®Œæ•´æ€§ã€‚

**Result:** åœ¨ä¸‰ä¸ªå…¬å¼€åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒIC-MoEæ¨¡å‹è¶…è¶Šäº†å…¶ä»–æœ€å…ˆè¿›æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ ·åŒ–åŒ»å­¦å›¾åƒåˆ†å‰²åœºæ™¯ä¸­çš„ä¼˜è¶Šæ³›åŒ–æ€§èƒ½ã€‚

**Conclusion:** IC-MoEæ¨¡å‹æœ‰æ•ˆè¡¥å……äº†åŸºç¡€åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„é«˜å±‚ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›å’Œé¢„è®­ç»ƒç»“æ„å®Œæ•´æ€§ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡æä¾›äº†ä¸€ç§é«˜æ•ˆçš„è‡ªé€‚åº”å¾®è°ƒè§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.


### [61] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao, Xinyu Zhou, Ding Jiang, Bo Du, Mang Ye, Min Zhang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªå¤šè¯­è¨€æ–‡æœ¬åˆ°å›¾åƒäººç‰©æ£€ç´¢ä»»åŠ¡ï¼Œå¹¶å¼€å‘äº†Bi-IRRAæ¡†æ¶ï¼Œé€šè¿‡åŒå‘éšå¼å…³ç³»æ¨ç†å’Œå¤šç»´å…¨å±€å¯¹é½æ¥å­¦ä¹ è·¨è¯­è¨€å’Œè·¨æ¨¡æ€çš„å¯¹é½ï¼Œåœ¨æ‰€æœ‰å¤šè¯­è¨€TIPRæ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ–‡æœ¬åˆ°å›¾åƒäººç‰©æ£€ç´¢é¢ä¸´æ¨¡æ€å¼‚è´¨æ€§çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼šå…¨å±€æ–¹æ³•å¿½ç•¥ç»†ç²’åº¦è·¨æ¨¡æ€å·®å¼‚ï¼Œå±€éƒ¨æ–¹æ³•éœ€è¦å…ˆéªŒä¿¡æ¯è¿›è¡Œæ˜¾å¼éƒ¨åˆ†å¯¹é½ï¼Œä¸”å½“å‰æ–¹æ³•ä¸»è¦é’ˆå¯¹è‹±è¯­è®¾è®¡ï¼Œé™åˆ¶äº†åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

**Method:** æå‡ºäº†Bi-IRRAæ¡†æ¶ï¼ŒåŒ…å«åŒå‘éšå¼å…³ç³»æ¨ç†æ¨¡å—ï¼Œé€šè¿‡åŒå‘é¢„æµ‹æ©ç å›¾åƒå’Œæ–‡æœ¬æ¥éšå¼å¢å¼ºè·¨è¯­è¨€å’Œè·¨æ¨¡æ€çš„å±€éƒ¨å…³ç³»å»ºæ¨¡ï¼ŒåŒæ—¶é›†æˆäº†å¤šç»´å…¨å±€å¯¹é½æ¨¡å—æ¥å¼¥åˆæ¨¡æ€å¼‚è´¨æ€§ï¼Œå¹¶æ„å»ºäº†å¤šè¯­è¨€TIPRåŸºå‡†æ•°æ®é›†ã€‚

**Result:** æ‰€æå‡ºçš„æ–¹æ³•åœ¨æ‰€æœ‰å¤šè¯­è¨€TIPRæ•°æ®é›†ä¸Šå‡å–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼ŒéªŒè¯äº†æ¡†æ¶åœ¨è·¨è¯­è¨€å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ä»…æ‰©å±•äº†TIPRä»»åŠ¡åˆ°å¤šè¯­è¨€åœºæ™¯ï¼Œè¿˜é€šè¿‡éšå¼å…³ç³»æ¨ç†å’Œå…¨å±€å¯¹é½çš„ç»“åˆæä¾›äº†æ›´æœ‰æ•ˆçš„è·¨æ¨¡æ€å­¦ä¹ æ–¹æ³•ï¼Œä¸ºå¤šè¯­è¨€è§†è§‰è¯­è¨€ç†è§£å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.


### [62] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan, Zekun Wang, Qianqian Xie, Yongqian Wen, Yuanxing Zhang, Guohui Zhang, Haoxuan Hu, Zhiyu Pan, Yibing Huang, Zhidong Gan, Yonghong Lin, An Ping, Tianhao Peng, Jiaheng Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MT-Video-Benchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­è§†é¢‘ç†è§£èƒ½åŠ›çš„ç»¼åˆåŸºå‡†ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šè½®è§†é¢‘å¯¹è¯æ—¶çš„æ˜¾è‘—æ€§èƒ½å·®è·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è¯„ä¼°åŸºå‡†ä¸»è¦å±€é™äºå•è½®é—®ç­”ä»»åŠ¡ï¼Œæœªèƒ½å……åˆ†åæ˜ ç°å®åœºæ™¯ä¸­å¤šè½®å¯¹è¯çš„å¤æ‚æ€§ï¼Œè¿™é™åˆ¶äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®åº”ç”¨ä¸­çš„æœ‰æ•ˆè¯„ä¼°å’Œå‘å±•ã€‚

**Method:** æ„å»ºäº†åŒ…å«987ä¸ªç²¾å¿ƒç­–åˆ’çš„å¤šè½®å¯¹è¯çš„MT-Video-BenchåŸºå‡†ï¼Œä¸»è¦è¯„ä¼°æ„ŸçŸ¥æ€§å’Œäº¤äº’æ€§ä¸¤å¤§ç»´åº¦çš„å…­é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼Œæ¶µç›–ä½“è‚²åˆ†æå’Œè§†é¢‘æ•™å­¦ç­‰å¤šä¸ªå®é™…åº”ç”¨é¢†åŸŸã€‚

**Result:** å¯¹å¤šç§å…ˆè¿›å¼€æºå’Œé—­æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†å¤šè½®è§†é¢‘å¯¹è¯æ—¶å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚å’Œå±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸­è¡¨ç°ä¸è¶³ã€‚

**Conclusion:** è¯¥åŸºå‡†æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè½®è§†é¢‘å¯¹è¯ç†è§£æ–¹é¢çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„è¯„ä¼°å·¥å…·å’Œå‘å±•æ–¹å‘ï¼Œå°†ä¿ƒè¿›æ›´é²æ£’çš„è§†é¢‘å¯¹è¯ç³»ç»Ÿå¼€å‘ã€‚

---

#### ğŸ“„ Abstract
The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.


### [63] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo, Jingwei Ji, Tong He, Runsheng Xu, Yichen Xie, Dragomir Anguelov, Mingxing Tan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºPlug-and-Forecastï¼ˆPnFï¼‰ï¼Œä¸€ç§å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¢å¼ºç°æœ‰è¿åŠ¨é¢„æµ‹æ¨¡å‹ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°å¤æ‚åœºæ™¯å®ç°å¿«é€Ÿé€‚åº”ï¼Œæ— éœ€å¾®è°ƒå³å¯æ˜¾è‘—æå‡è¿åŠ¨é¢„æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¾èµ–ä¸“ç”¨æ¨¡å‹è¿›è¡Œæ„ŸçŸ¥å’Œè¿åŠ¨é¢„æµ‹ï¼Œåœ¨æ ‡å‡†æ¡ä»¶ä¸‹è¡¨ç°å¯é ï¼Œä½†éš¾ä»¥ç»æµé«˜æ•ˆåœ°æ³›åŒ–åˆ°å¤šæ ·åŒ–ç°å®åœºæ™¯ï¼Œè¿™æˆä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿé¢ä¸´çš„é‡è¦æŒ‘æˆ˜ã€‚

**Method:** PnFæ–¹æ³•è®¾è®¡æç¤ºè¯ä»å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­æå–ç»“æ„åŒ–åœºæ™¯ç†è§£ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯æç‚¼ä¸ºå¯å­¦ä¹ çš„åµŒå…¥å‘é‡æ¥å¢å¼ºç°æœ‰è¡Œä¸ºé¢„æµ‹æ¨¡å‹ï¼Œå……åˆ†åˆ©ç”¨MLLMsçš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›ã€‚

**Result:** åœ¨Waymo Open Motion Datasetå’ŒnuScenes Datasetä¸Šå¯¹ä¸¤ç§æœ€å…ˆè¿›çš„è¿åŠ¨é¢„æµ‹æ¨¡å‹è¿›è¡ŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºåœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è·å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

**Conclusion:** è¯¥æ–¹æ³•è¯æ˜äº†è‡ªç„¶è¯­è¨€åœ¨æè¿°å’Œå¤„ç†å¤æ‚é©¾é©¶åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºé›¶æ ·æœ¬é€‚åº”ç‰¹å®šè¡Œä¸ºæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œä¸”æ— éœ€å¾®è°ƒçš„ç‰¹æ€§ä½¿å…¶å…·æœ‰å®é™…éƒ¨ç½²çš„å¯è¡Œæ€§ã€‚

---

#### ğŸ“„ Abstract
Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.


### [64] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han, Qihan Lin, Hao Liang, Bowen Chen, Zhou Liu, Wentao Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªä¸“æ³¨äºé•¿è§†é¢‘ç†è§£çš„åŸºå‡†æµ‹è¯•LongInsightBenchï¼Œé€šè¿‡æ•´åˆè§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬æ¨¡æ€ï¼Œè¯„ä¼°æ¨¡å‹åœ¨äººç±»è¯­è¨€ã€è§‚ç‚¹ã€åŠ¨ä½œç­‰ä¸Šä¸‹æ–‡å…ƒç´ ä¸Šçš„ç†è§£èƒ½åŠ›ï¼Œæ­ç¤ºäº†å½“å‰å…¨æ¨¡æ€æ¨¡å‹åœ¨æ—¶é—´å®šä½å’Œé•¿è·ç¦»å› æœæ¨ç†ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç¼ºä¹ä¸“é—¨é’ˆå¯¹é•¿è§†é¢‘ç†è§£çš„åŸºå‡†æµ‹è¯•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ï¼ˆè§†è§‰ã€éŸ³é¢‘ã€æ–‡æœ¬ï¼‰æ–¹é¢å­˜åœ¨ç ”ç©¶ç©ºç™½ï¼Œéœ€è¦è¯„ä¼°æ¨¡å‹å¯¹äººç±»è¯­è¨€ã€è§‚ç‚¹ã€åŠ¨ä½œç­‰ä¸Šä¸‹æ–‡å…ƒç´ çš„æ·±åº¦ç†è§£èƒ½åŠ›ã€‚

**Method:** æ„å»ºäº†åŒ…å«çº¦1000ä¸ªé•¿æ—¶é•¿ã€ä¿¡æ¯å¯†é›†è§†é¢‘çš„æ•°æ®é›†ï¼Œæ¶µç›–è®²åº§ã€è®¿è°ˆå’Œvlogç­‰å¯Œå«è¯­è¨€å…ƒç´ çš„å†…å®¹ï¼›è®¾è®¡äº†å…­ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡åœºæ™¯ï¼ŒåŒ…æ‹¬äº‹ä»¶å†…å’Œäº‹ä»¶é—´ä»»åŠ¡ï¼›å¼€å‘äº†ä¸‰æ­¥åŠè‡ªåŠ¨æ•°æ®è´¨é‡ä¿è¯æµç¨‹ç¡®ä¿é—®é¢˜éš¾åº¦å’Œæœ‰æ•ˆæ€§ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œå…¨æ¨¡æ€æ¨¡å‹åœ¨éœ€è¦ç²¾ç¡®æ—¶é—´å®šä½å’Œé•¿è·ç¦»å› æœæ¨ç†çš„ä»»åŠ¡ä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼›æ‰©å±•å®éªŒæ­ç¤ºäº†å…¨æ¨¡æ€æ¨¡å‹åœ¨å¤šæ¨¡æ€èåˆä¸­å­˜åœ¨ä¿¡æ¯ä¸¢å¤±å’Œå¤„ç†åå·®çš„é—®é¢˜ã€‚

**Conclusion:** LongInsightBenchä¸ºé•¿è§†é¢‘ç†è§£ç ”ç©¶æä¾›äº†é¦–ä¸ªç»¼åˆæ€§åŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰å…¨æ¨¡æ€æ¨¡å‹çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„ä¸è¶³ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æ¨¡å‹çš„å‘å±•æŒ‡æ˜äº†æ”¹è¿›æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.


### [65] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao, Xinli Yue, Jianhui Sun, Yuhao Xie, Tao Shao, Liangchao Yao, Fan Xia, Yuetang Deng*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†iDETEXï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶æ‰§è¡Œè´¨é‡å®šä½ã€æ„ŸçŸ¥å’Œæè¿°ä¸‰ä¸ªå…³é”®ä»»åŠ¡ï¼Œè§£å†³äº†è¯¦ç»†å¯è§£é‡Šå›¾åƒè´¨é‡è¯„ä¼°çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åœ¨ViDA-UGCåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ICCV MIPI 2025è¯¦ç»†å›¾åƒè´¨é‡è¯„ä¼°æŒ‘æˆ˜èµ›ä¸­æ’åç¬¬ä¸€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å›¾åƒè´¨é‡è¯„ä¼°å·²ä»æ ‡é‡è´¨é‡é¢„æµ‹å‘å±•åˆ°æ›´å¯è§£é‡Šã€ä¸äººç±»å¯¹é½çš„è¯„ä¼°èŒƒå¼ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³æ–°å…´çš„è¯¦ç»†å¯è§£é‡Šå›¾åƒè´¨é‡è¯„ä¼°æŒ‘æˆ˜ï¼Œé€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åŒæ—¶å¤„ç†è´¨é‡å®šä½ã€æ„ŸçŸ¥å’Œæè¿°ä¸‰ä¸ªå¼‚æ„å­ä»»åŠ¡ã€‚

**Method:** æå‡ºäº†iDETEXç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œè®¾è®¡äº†ä¸€å¥—ä»»åŠ¡ç‰¹å®šçš„ç¦»çº¿å¢å¼ºæ¨¡å—å’Œæ•°æ®æ··åˆç­–ç•¥ï¼Œè¾…ä»¥åœ¨çº¿å¢å¼ºç­–ç•¥ä»¥å……åˆ†åˆ©ç”¨å¤šæºç›‘ç£ï¼Œå®ç°ä¸‰ä¸ªå¼‚æ„å­ä»»åŠ¡çš„é«˜æ•ˆå’Œæ³›åŒ–è®­ç»ƒã€‚

**Result:** åœ¨å¤§å‹ViDA-UGCåŸºå‡†æµ‹è¯•ä¸­ï¼ŒiDETEXåœ¨æ‰€æœ‰å­ä»»åŠ¡ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ICCV MIPI 2025è¯¦ç»†å›¾åƒè´¨é‡è¯„ä¼°æŒ‘æˆ˜èµ›ä¸­æ’åç¬¬ä¸€ï¼Œè¯æ˜äº†å…¶åœ¨æä¾›å‡†ç¡®å’Œå¯è§£é‡Šè´¨é‡è¯„ä¼°æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¯¦ç»†å¯è§£é‡Šå›¾åƒè´¨é‡è¯„ä¼°ä¸­çš„æ½œåŠ›ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒç­–ç•¥å’Œæ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªå¼‚æ„ä»»åŠ¡ï¼Œä¸ºå›¾åƒè´¨é‡è¯„ä¼°æä¾›äº†æ›´åŠ å…¨é¢å’Œäººç±»å¯¹é½çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.


### [66] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu, Shengpeng Xu, Yunbo Jia, Edmund Y. Lam*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSemantic-E2VIDæ¡†æ¶ï¼Œé€šè¿‡å°†è§†è§‰è¯­ä¹‰çŸ¥è¯†ä»å¸§æ¨¡æ€è¿ç§»åˆ°äº‹ä»¶æ¨¡æ€ï¼Œæ˜¾è‘—æå‡äº†äº‹ä»¶åˆ°è§†é¢‘é‡å»ºçš„è´¨é‡å’Œè¯­ä¹‰ä¿¡æ¯æ¢å¤èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„äº‹ä»¶åˆ°è§†é¢‘é‡å»ºæ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** äº‹ä»¶ç›¸æœºä»…æ•è·å¼ºåº¦å˜åŒ–è€Œå¿½ç•¥é™æ€ç‰©ä½“å’ŒèƒŒæ™¯ï¼Œå¯¼è‡´äº‹ä»¶æ¨¡æ€ç¼ºä¹è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œç°æœ‰äº‹ä»¶åˆ°è§†é¢‘é‡å»ºæ–¹æ³•å¾€å¾€å¿½è§†äº†è¯­ä¹‰ä¿¡æ¯åœ¨è§†é¢‘é‡å»ºä¸­çš„å…³é”®ä½œç”¨ï¼Œè¿™é™åˆ¶äº†é‡å»ºè§†é¢‘çš„è¯­ä¹‰è´¨é‡å’Œä¿¡æ¯å®Œæ•´æ€§ã€‚

**Method:** æå‡ºSemantic-E2VIDæ¡†æ¶ï¼ŒåŒ…å«è·¨æ¨¡æ€ç‰¹å¾å¯¹é½æ¨¡å—å°†Segment Anything Modelçš„è§†è§‰è¯­ä¹‰çŸ¥è¯†è¿ç§»åˆ°äº‹ä»¶ç¼–ç å™¨ï¼Œè¯­ä¹‰æ„ŸçŸ¥ç‰¹å¾èåˆå—é›†æˆå­¦ä¹ åˆ°çš„è¯­ä¹‰ç‰¹å¾å½¢æˆå¯Œå«è¯­ä¹‰çš„äº‹ä»¶è¡¨ç¤ºï¼Œä»¥åŠè¯­ä¹‰æ„ŸçŸ¥E2Vç›‘ç£æœºåˆ¶åˆ©ç”¨SAMç”Ÿæˆçš„ç±»åˆ«æ ‡ç­¾ä¿ƒè¿›è¯­ä¹‰ç»†èŠ‚é‡å»ºã€‚

**Result:** åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSemantic-E2VIDæ˜¾è‘—æå‡äº†å¸§è´¨é‡ï¼Œåœ¨äº‹ä»¶åˆ°è§†é¢‘é‡å»ºä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¯æ˜äº†è¯­ä¹‰ä¿¡æ¯è¿ç§»å¯¹é‡å»ºè´¨é‡çš„é‡è¦æ”¹è¿›ä½œç”¨ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å°†è§†è§‰è¯­ä¹‰çŸ¥è¯†ä»å¸§æ¨¡æ€è¿ç§»åˆ°äº‹ä»¶æ¨¡æ€çš„æœ‰æ•ˆæ€§ï¼Œä¸ºäº‹ä»¶è§†è§‰ä»»åŠ¡æä¾›äº†æ–°çš„è¯­ä¹‰å¢å¼ºèŒƒå¼ï¼Œæœªæ¥å¯æ‰©å±•åˆ°å…¶ä»–äº‹ä»¶è§†è§‰ä»»åŠ¡ä¸­æå‡è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.


### [67] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas, Soroush Seifi, Gunshi Gupta, Rahaf Aljundi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡LLMå¼•å¯¼çš„è§†è§‰ä»¤ç‰Œé€‰æ‹©ã€å¾ªç¯å¤„ç†å’ŒåŸºäºæè¿°çš„é—®ç­”ï¼Œè§£å†³äº†è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨æµå¼è§†é¢‘å¤„ç†ä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡äº†å¤„ç†æ•ˆç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†é¢‘å¤§è¯­è¨€æ¨¡å‹è™½ç„¶åœ¨ä¸Šä¸‹æ–‡è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æµå¼å¤„ç†åœºæ™¯ä¸‹é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å½“éœ€è¦å®æ—¶å¤„ç†å°æ—¶çº§é•¿åº¦è§†é¢‘å¹¶ç»™å‡ºåŠæ—¶å“åº”æ—¶ï¼Œç°æœ‰æ–¹æ³•æ— æ³•æ»¡è¶³æ•ˆç‡è¦æ±‚ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨ä¸‰ä¸ªå…³é”®æŠ€æœ¯ï¼šåŸºäºLLMæ³¨æ„åŠ›çš„è§†è§‰ä»¤ç‰Œé€‰æ‹©æœºåˆ¶è¯†åˆ«å¯¹ç†è§£æ¯ä¸ªçŸ­ç‰‡æ®µé‡è¦çš„è§†è§‰ä»¤ç‰Œï¼›å¾ªç¯å¤„ç†è¿‡å»é€‰å®šçš„ä»¤ç‰Œä»¥ç”Ÿæˆæ—¶é—´è¿è´¯çš„è§†é¢‘ç†è§£ï¼›åŸºäºæè¿°çš„è½»é‡çº§é—®ç­”ç³»ç»Ÿå®ç°å‡†ç¡®å“åº”ã€‚

**Result:** è¯¥æ–¹æ³•åœ¨æµå¼è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œèƒ½å¤Ÿä¸¢å¼ƒçº¦95%çš„ä¸é‡è¦è§†è§‰ä»¤ç‰Œè€Œä»…å¸¦æ¥æœ€å°æ€§èƒ½æŸå¤±ï¼Œåœ¨æ•ˆç‡å’Œæ•ˆæœä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜é€šè¿‡æ™ºèƒ½ä»¤ç‰Œé€‰æ‹©å’Œå¾ªç¯å¤„ç†æœºåˆ¶ï¼Œå¯ä»¥åœ¨ä¸ç‰ºç‰²ç†è§£è´¨é‡çš„å‰æä¸‹æ˜¾è‘—æå‡è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„æµå¼å¤„ç†æ•ˆç‡ï¼Œä¸ºå®æ—¶é•¿è§†é¢‘åˆ†ææä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.


### [68] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang, Zhengxuan Wei, Ge Zheng, Sibei Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†LoopTransï¼Œä¸€ç§æ–°é¢–çš„é—­ç¯æ¡†æ¶ï¼Œç”¨äºå¼±ç›‘ç£å¯æ‰¿å—æ€§å®šä½ï¼Œä¸ä»…ä»å¤–ä¸­å¿ƒè§†è§’å‘è‡ªæˆ‘ä¸­å¿ƒè§†è§’ä¼ é€’çŸ¥è¯†ï¼Œè¿˜é€šè¿‡åå‘ä¼ é€’å¢å¼ºå¤–ä¸­å¿ƒçŸ¥è¯†æå–ï¼Œåœ¨å›¾åƒå’Œè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å…¨é¢æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¼±ç›‘ç£å¯æ‰¿å—æ€§å®šä½æ–¹æ³•ä»…ä»å¤–ä¸­å¿ƒäº¤äº’å›¾åƒå•å‘ä¼ é€’çŸ¥è¯†åˆ°è‡ªæˆ‘ä¸­å¿ƒå›¾åƒï¼Œé™åˆ¶äº†åœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸­çš„é€‚ç”¨æ€§ï¼Œéœ€è¦è§£å†³é¢†åŸŸå·®è·å’ŒçŸ¥è¯†ä¼ é€’æ•ˆç‡é—®é¢˜ã€‚

**Method:** æå‡ºäº†LoopTransé—­ç¯æ¡†æ¶ï¼ŒåŒ…å«ç»Ÿä¸€è·¨æ¨¡æ€å®šä½å’Œå»å™ªçŸ¥è¯†è’¸é¦æœºåˆ¶ï¼Œé€šè¿‡åŒå‘çŸ¥è¯†ä¼ é€’æ¡¥æ¥ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è‡ªæˆ‘ä¸­å¿ƒå›¾åƒå’Œä»¥äº¤äº’ä¸ºä¸­å¿ƒçš„å¤–ä¸­å¿ƒå›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚

**Result:** å®éªŒè¡¨æ˜LoopTransåœ¨å›¾åƒå’Œè§†é¢‘åŸºå‡†æµ‹è¯•çš„æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½å®ç°äº†æŒç»­æ”¹è¿›ï¼Œå³ä½¿åœ¨äººç±»èº«ä½“å®Œå…¨é®æŒ¡å¯¹è±¡äº¤äº’åŒºåŸŸçš„æŒ‘æˆ˜æ€§åœºæ™¯ä¸­ä¹Ÿèƒ½æœ‰æ•ˆå¤„ç†ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åŒå‘çŸ¥è¯†ä¼ é€’åœ¨å¯æ‰¿å—æ€§å®šä½ä¸­çš„é‡è¦æ€§ï¼Œé—­ç¯æ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºçŸ¥è¯†æå–å’Œä¼ é€’æ•ˆæœï¼Œä¸ºå¤æ‚äº¤äº’åœºæ™¯ä¸‹çš„è§†è§‰ç†è§£æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.


### [69] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov, Viacheslav Vyshegorodtsev, Ivan Nezhivykh*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰çš„é©¬åŒ¹è¡Œä¸ºè‡ªåŠ¨ç›‘æµ‹ç³»ç»ŸåŸå‹ï¼Œåˆ©ç”¨ç›®æ ‡æ£€æµ‹å’Œå¤šç›®æ ‡è·Ÿè¸ªæŠ€æœ¯å®ç°é©¬å©å†…é©¬åŒ¹å’Œäººå‘˜çš„è‡ªåŠ¨åŒ–ç›‘æ§ã€‚è¯¥ç³»ç»Ÿé€šè¿‡YOLOv11å’ŒBoT-SORTç®—æ³•ç»“åˆç©ºé—´å…³ç³»æ¨ç†ï¼Œä¸ºé©¬åŒ¹ç¦åˆ©ç›‘æµ‹æä¾›äº†å®æ—¶è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰é©¬åŒ¹è¡Œä¸ºç›‘æµ‹ä¸»è¦ä¾èµ–äººå·¥è§‚å¯Ÿï¼Œå­˜åœ¨åŠ³åŠ¨å¯†é›†å’Œæ—¶é—´æ¶ˆè€—å¤§çš„é—®é¢˜ï¼Œéš¾ä»¥å®ç°æ—©æœŸå¥åº·é—®é¢˜çš„åŠæ—¶æ£€æµ‹ã€‚ä¼ ç»Ÿæ–¹æ³•æ— æ³•æ»¡è¶³é©¬å©ç¯å¢ƒä¸­æŒç»­ã€è‡ªåŠ¨åŒ–çš„è¡Œä¸ºç›‘æ§éœ€æ±‚ï¼ŒäºŸéœ€å¼€å‘æ™ºèƒ½åŒ–çš„ç›‘æµ‹è§£å†³æ–¹æ¡ˆã€‚

**Method:** ç³»ç»Ÿé‡‡ç”¨YOLOv11è¿›è¡Œç›®æ ‡æ£€æµ‹å’ŒBoT-SORTè¿›è¡Œå¤šç›®æ ‡è·Ÿè¸ªï¼ŒåŸºäºç›®æ ‡è½¨è¿¹å’Œç©ºé—´ä½ç½®å…³ç³»æ¨æ–­äº‹ä»¶çŠ¶æ€ã€‚ä¸ºæ”¯æŒå¼€å‘ï¼Œæ„å»ºäº†è‡ªå®šä¹‰æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨CLIPå’ŒGroundingDINOç­‰åŸºç¡€æ¨¡å‹è¾…åŠ©æ•°æ®æ ‡æ³¨ã€‚ç³»ç»Ÿèƒ½å¤Ÿè¯†åˆ«äº”ç§äº‹ä»¶ç±»å‹å¹¶è€ƒè™‘ç›¸æœºç›²åŒºã€‚

**Result:** å®šæ€§è¯„ä¼°æ˜¾ç¤ºç³»ç»Ÿåœ¨é©¬åŒ¹ç›¸å…³äº‹ä»¶æ£€æµ‹æ–¹é¢è¡¨ç°å¯é ï¼Œä½†ç”±äºæ•°æ®ç¨€ç¼ºï¼Œåœ¨äººå‘˜æ£€æµ‹æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ç³»ç»ŸæˆåŠŸåŒºåˆ†äº†ä¸åŒäº‹ä»¶ç±»å‹ï¼Œå¹¶åœ¨å®é™…é©¬å©ç¯å¢ƒä¸­éªŒè¯äº†å…¶å¯è¡Œæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºé©¬åŒ¹è®¾æ–½ä¸­çš„å®æ—¶è¡Œä¸ºç›‘æµ‹å¥ å®šäº†åŸºç¡€ï¼Œå¯¹åŠ¨ç‰©ç¦åˆ©å’Œå©èˆç®¡ç†å…·æœ‰é‡è¦æ„ä¹‰ã€‚æœªæ¥å·¥ä½œåº”ç€é‡è§£å†³äººå‘˜æ£€æµ‹çš„æ•°æ®ä¸è¶³é—®é¢˜ï¼Œå¹¶è¿›ä¸€æ­¥ä¼˜åŒ–ç³»ç»Ÿçš„å®æ—¶æ€§èƒ½å’Œé²æ£’æ€§ã€‚

---

#### ğŸ“„ Abstract
Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.


### [70] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein, Hossein Javidnia, FranÃ§ois PitiÃ©, Anil Kokaram*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºåˆ©ç”¨AV1è¿åŠ¨å‘é‡ç”Ÿæˆå¯†é›†äºšåƒç´ å¯¹åº”å…³ç³»å’Œä½™å¼¦ä¸€è‡´æ€§è¿‡æ»¤çš„çŸ­è½¨è¿¹ï¼Œä½œä¸ºå‹ç¼©åŸŸè§†è§‰å‰ç«¯ã€‚è¯¥æ–¹æ³•åœ¨çŸ­è§†é¢‘ä¸Šå®ç°äº†ä¸é¡ºåºSIFTç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½CPUä½¿ç”¨é‡ï¼Œä¸ºå®Œæ•´è§†è§‰ç®¡é“æä¾›äº†èµ„æºé«˜æ•ˆçš„å‰ç«¯è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿè§†è§‰å‰ç«¯æ–¹æ³•å¦‚SIFTåœ¨è®¡ç®—èµ„æºæ¶ˆè€—æ–¹é¢æ•ˆç‡è¾ƒä½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è§†é¢‘åºåˆ—æ—¶ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å‹ç¼©åŸŸå¯¹åº”å…³ç³»ä½œä¸ºèµ„æºé«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆçš„å¯è¡Œæ€§ï¼Œè§£å†³ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„å±€é™æ€§ã€‚

**Method:** è¯¥æ–¹æ³•é‡æ–°åˆ©ç”¨AV1è§†é¢‘ç¼–ç æ ‡å‡†ä¸­çš„è¿åŠ¨å‘é‡æ¥ç”Ÿæˆå¯†é›†äºšåƒç´ å¯¹åº”å…³ç³»ï¼Œå¹¶é€šè¿‡ä½™å¼¦ä¸€è‡´æ€§è¿‡æ»¤çŸ­è½¨è¿¹ã€‚è¿™ç§å‹ç¼©åŸŸå‰ç«¯é¿å…äº†ä¼ ç»Ÿç‰¹å¾æå–å’ŒåŒ¹é…çš„æ˜‚è´µè®¡ç®—è¿‡ç¨‹ï¼Œç›´æ¥ä»å‹ç¼©è§†é¢‘æ•°æ®ä¸­æå–å‡ ä½•ä¿¡æ¯ã€‚

**Result:** åœ¨117å¸§è§†é¢‘ç‰‡æ®µä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¿åŠ¨å‘é‡åŒ¹é…æˆåŠŸé…å‡†æ‰€æœ‰å›¾åƒå¹¶é‡å»ºäº†46-62ä¸‡ä¸ªä¸‰ç»´ç‚¹ï¼Œé‡æŠ•å½±è¯¯å·®ä¸º0.51-0.53åƒç´ ã€‚è¯¥æ–¹æ³•åœ¨çŸ­è§†é¢‘ä¸Šè¿è¡Œæ€§èƒ½ä¸é¡ºåºSIFTç›¸å½“ï¼Œä½†CPUä½¿ç”¨é‡æ˜¾è‘—é™ä½ï¼Œä¸”èƒ½äº§ç”Ÿæ›´å¯†é›†çš„åŒ¹é…ç‚¹ã€‚

**Conclusion:** å‹ç¼©åŸŸå¯¹åº”å…³ç³»è¢«è¯æ˜æ˜¯å®ç”¨ä¸”èµ„æºé«˜æ•ˆçš„è§†è§‰å‰ç«¯è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰æ¸…æ™°çš„æ‰©å±•è·¯å¾„ã€‚è¯¥æ–¹æ³•ä¸ºå®Œæ•´è§†è§‰ç®¡é“æä¾›äº†å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—èµ„æºå—é™æˆ–éœ€è¦å¤„ç†å¤§è§„æ¨¡è§†é¢‘æ•°æ®çš„åœºæ™¯ä¸­å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.


### [71] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo, Shuai Lu, Lei Fan, Zelin Li, Donglin Di, Yang Song, Weihang Zhang, Wenbing Zhu, Hong Yan, Fang Chen, Huiqi Li, Hongen Liao*

#### ğŸ§© TL;DR
Dinomaly2æå‡ºäº†é¦–ä¸ªå…¨è°±å›¾åƒæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡äº”ä¸ªç®€å•å…ƒç´ çš„ååŒè®¾è®¡åœ¨å¤šç±»æ£€æµ‹ä¸­å®ç°çªç ´æ€§æ€§èƒ½ï¼ŒåŒæ—¶æ— ç¼æ‰©å±•åˆ°å¤šç§æ•°æ®æ¨¡æ€å’Œä»»åŠ¡è®¾ç½®ï¼Œè¯æ˜äº†ç®€å•æ€§æ˜¯çœŸæ­£é€šç”¨æ€§çš„åŸºç¡€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹é¢†åŸŸå­˜åœ¨å¤šç±»æ¨¡å‹æ€§èƒ½æ˜¾è‘—è½åäºä¸€å¯¹ä¸€ä¸“ç”¨æ¨¡å‹çš„é—®é¢˜ï¼Œä¸”è¯¥é¢†åŸŸå·²åˆ†è£‚ä¸ºé’ˆå¯¹ç‰¹å®šåœºæ™¯çš„ä¸“é—¨åŒ–æ–¹æ³•ï¼Œè¿™é€ æˆäº†éƒ¨ç½²éšœç¢å¹¶å‡¸æ˜¾äº†å¯¹ç»Ÿä¸€è§£å†³æ–¹æ¡ˆçš„éœ€æ±‚ã€‚

**Method:** Dinomaly2é‡‡ç”¨åŸºäºé‡æ„çš„æ ‡å‡†æ¡†æ¶ï¼Œé€šè¿‡äº”ä¸ªç®€å•å…ƒç´ çš„ç²¾å¿ƒç¼–æ’å®ç°å“è¶Šæ€§èƒ½ï¼Œè¿™ç§æ–¹æ³•è®ºä¸Šçš„æç®€ä¸»ä¹‰ä½¿å…¶èƒ½å¤Ÿæ— éœ€ä¿®æ”¹å³å¯è‡ªç„¶æ‰©å±•åˆ°å¤šæ ·åŒ–ä»»åŠ¡ä¸­ã€‚

**Result:** åœ¨12ä¸ªæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDinomaly2åœ¨å¤šæ¨¡æ€ã€ä»»åŠ¡è®¾ç½®å’Œåº”ç”¨é¢†åŸŸå‡å±•ç°å‡ºå…¨è°±ä¼˜åŠ¿ï¼Œå¤šç±»æ¨¡å‹åœ¨MVTec-ADå’ŒVisAä¸Šåˆ†åˆ«è¾¾åˆ°å‰æ‰€æœªæœ‰çš„99.9%å’Œ99.3%å›¾åƒçº§AUROCï¼Œä»…ä½¿ç”¨æ¯ç±»8ä¸ªæ­£å¸¸æ ·æœ¬å³å¯è¶…è¶Šå…ˆå‰å…¨æ ·æœ¬æ¨¡å‹ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æç®€è®¾è®¡ã€è®¡ç®—å¯æ‰©å±•æ€§å’Œé€šç”¨é€‚ç”¨æ€§çš„ç»“åˆä½¿Dinomaly2æˆä¸ºç°å®ä¸–ç•Œå¼‚å¸¸æ£€æµ‹åº”ç”¨å…¨è°±çš„ç»Ÿä¸€è§£å†³æ–¹æ¡ˆï¼Œç¡®ç«‹äº†ç®€å•æ€§ä½œä¸ºçœŸæ­£é€šç”¨æ€§åŸºç¡€çš„é‡è¦åŸåˆ™ã€‚

---

#### ğŸ“„ Abstract
Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.


### [72] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu, Zhenyu Wang, Ruofeng Liu, Guang Wang, Desheng Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºOP3Detï¼Œä¸€ç§æ— éœ€æ–‡æœ¬æç¤ºçš„å¼€æ”¾ä¸–ç•Œ3Dæ£€æµ‹å™¨ï¼Œé€šè¿‡èåˆ2Dè¯­ä¹‰å…ˆéªŒå’Œ3Då‡ ä½•å…ˆéªŒå®ç°é€šç”¨3Dç‰©ä½“å‘ç°ï¼Œåœ¨å¼€æ”¾ä¸–ç•Œ3Dæ£€æµ‹ä¸­æ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰3Dç‰©ä½“æ£€æµ‹å’Œæ–°å‹ç±»åˆ«æ£€æµ‹è™½å–å¾—è¿›å±•ï¼Œä½†é€šç”¨3Dç‰©ä½“æ€§å­¦ä¹ ç ”ç©¶ä¸è¶³ã€‚ä¼ ç»Ÿé—­é›†3Dæ£€æµ‹å™¨éš¾ä»¥æ³›åŒ–åˆ°å¼€æ”¾ä¸–ç•Œåœºæ™¯ï¼Œè€Œç›´æ¥å¼•å…¥3Då¼€æ”¾è¯æ±‡æ¨¡å‹åˆé¢ä¸´è¯æ±‡æ‰©å±•å’Œè¯­ä¹‰é‡å é—®é¢˜ï¼Œéœ€è¦è§£å†³å¼€æ”¾ä¸–ç•Œ3Dç‰©ä½“æ€§å­¦ä¹ çš„ç ”ç©¶ç©ºç™½ã€‚

**Method:** æå‡ºOP3Detæ¡†æ¶ï¼Œåˆ©ç”¨2DåŸºç¡€æ¨¡å‹çš„å¼ºæ³›åŒ–èƒ½åŠ›å’Œé›¶æ ·æœ¬èƒ½åŠ›ï¼Œç»“åˆ2Dè¯­ä¹‰å…ˆéªŒå’Œ3Då‡ ä½•å…ˆéªŒç”Ÿæˆç±»åˆ«æ— å…³çš„ç‰©ä½“æè®®ã€‚é€šè¿‡è·¨æ¨¡æ€ä¸“å®¶æ··åˆæœºåˆ¶ï¼ŒåŠ¨æ€è·¯ç”±å•æ¨¡æ€å’Œå¤šæ¨¡æ€ç‰¹å¾ï¼Œä»ç‚¹äº‘å’ŒRGBå›¾åƒä¸­å­¦ä¹ é€šç”¨3Dç‰©ä½“æ€§ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜OP3Detè¡¨ç°å“è¶Šï¼Œåœ¨ARæŒ‡æ ‡ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰å¼€æ”¾ä¸–ç•Œ3Dæ£€æµ‹å™¨è¾¾16.0%ï¼Œç›¸æ¯”é—­ä¸–ç•Œ3Dæ£€æµ‹å™¨æå‡13.5%ï¼Œè¯æ˜äº†å…¶åœ¨å¼€æ”¾ä¸–ç•Œ3Dç‰©ä½“å‘ç°ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†èåˆ2DåŸºç¡€æ¨¡å‹ä¸3Då‡ ä½•ä¿¡æ¯åœ¨å¼€æ”¾ä¸–ç•Œ3Dæ£€æµ‹ä¸­çš„æ½œåŠ›ï¼Œä¸ºé€šç”¨3Dç‰©ä½“æ€§å­¦ä¹ æä¾›äº†æ–°èŒƒå¼ï¼Œæ¨åŠ¨äº†3Dåœºæ™¯ç†è§£å‘æ›´é€šç”¨çš„æ–¹å‘å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.


### [73] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki, Junxian Guo, Jiaming Tang, Shang Yang, Yukang Chen, Konstantinos N. Plataniotis, Yao Lu, Song Han, Zhijian Liu*

#### ğŸ§© TL;DR
SparseVILAæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†æ–°èŒƒå¼ï¼Œé€šè¿‡åœ¨é¢„å¡«å……é˜¶æ®µå‰ªæå†—ä½™è§†è§‰ä»¤ç‰Œå¹¶åœ¨è§£ç é˜¶æ®µæ£€ç´¢æŸ¥è¯¢ç›¸å…³ä»¤ç‰Œï¼Œå®ç°äº†è®­ç»ƒæ— å…³ã€æ¶æ„æ— å…³çš„åŠ é€Ÿæ¡†æ¶ï¼Œåœ¨ä¿æŒæ¨¡å‹èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯æ‰©å±•æ€§å—åˆ°è§†è§‰ä»¤ç‰Œæ•°é‡å¿«é€Ÿå¢é•¿çš„é™åˆ¶ï¼Œè¿™äº›è§†è§‰ä»¤ç‰Œä¸»å¯¼äº†æ¨ç†å»¶è¿Ÿï¼Œé˜»ç¢äº†æ¨¡å‹åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£ã€é•¿è§†é¢‘åˆ†æå’Œå¤šè½®å¯¹è¯ç­‰åº”ç”¨ä¸­çš„å®é™…éƒ¨ç½²æ•ˆç‡ã€‚

**Method:** SparseVILAé‡‡ç”¨è§£è€¦è§†è§‰ç¨€ç–æ€§çš„è®¾è®¡ï¼Œåœ¨é¢„å¡«å……é˜¶æ®µè¿›è¡ŒæŸ¥è¯¢æ— å…³çš„ä»¤ç‰Œå‰ªæï¼Œåœ¨è§£ç é˜¶æ®µè¿›è¡ŒæŸ¥è¯¢æ„ŸçŸ¥çš„ä»¤ç‰Œæ£€ç´¢ï¼ŒåŸºäºAWQä¼˜åŒ–çš„æ¨ç†ç®¡é“å®ç°ï¼Œä¿ç•™å¤§éƒ¨åˆ†è§†è§‰ç¼“å­˜ä»¥ç¡®ä¿å¤šè½®å¯¹è¯çš„ä¿çœŸåº¦ã€‚

**Result:** åœ¨é•¿ä¸Šä¸‹æ–‡è§†é¢‘ä»»åŠ¡ä¸­å®ç°äº†4.0å€é¢„å¡«å……åŠ é€Ÿã€2.5å€è§£ç åŠ é€Ÿå’Œ2.6å€ç«¯åˆ°ç«¯åŠ é€Ÿï¼ŒåŒæ—¶åœ¨æ–‡æ¡£ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸Šæå‡äº†å‡†ç¡®æ€§ï¼Œè¯æ˜äº†æ•ˆç‡ä¸æ€§èƒ½çš„ååŒæ”¹è¿›ã€‚

**Conclusion:** é€šè¿‡è§£è€¦æŸ¥è¯¢æ— å…³å‰ªæå’ŒæŸ¥è¯¢æ„ŸçŸ¥æ£€ç´¢ï¼ŒSparseVILAä¸ºé«˜æ•ˆå¤šæ¨¡æ€æ¨ç†å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œæä¾›äº†æ— éœ€è®­ç»ƒã€æ¶æ„æ— å…³çš„åŠ é€Ÿæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå¤§æ¨¡å‹èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚

---

#### ğŸ“„ Abstract
Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [74] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam, Md Jobair Hossain Faruk, Jerry Q. Cheng, Huanying Gu*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºChatGPTå’ŒClaudeçš„å¤šæ¨¡å‹èåˆæ¡†æ¶ï¼Œé€šè¿‡è¾“å‡ºç›¸ä¼¼åº¦å…±è¯†æœºåˆ¶æ˜¾è‘—æå‡äº†èƒ¸éƒ¨Xå…‰ç‰‡è¯Šæ–­çš„å‡†ç¡®æ€§ï¼Œåœ¨CheXpertæ•°æ®é›†ä¸Šå®ç°äº†91.3%çš„æœ€é«˜å‡†ç¡®ç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰AIè¾…åŠ©æ”¾å°„å­¦è¯Šæ–­å­˜åœ¨å¯é æ€§ä¸è¶³çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å•ä¸€æ¨¡å‹è¯Šæ–­å‡†ç¡®æ€§æœ‰é™çš„æƒ…å†µä¸‹ï¼Œéœ€è¦æ¢ç´¢å¤šæ¨¡å‹èåˆç­–ç•¥æ¥æå‡è¯Šæ–­ä¿¡ä»»åº¦å’Œä¸´åºŠå®ç”¨æ€§ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œç»“åˆChatGPTå’ŒClaudeä¸¤ä¸ªå…ˆè¿›å¤§è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨åŸºäº95%è¾“å‡ºç›¸ä¼¼åº¦é˜ˆå€¼çš„å…±è¯†æ–¹æ³•ï¼Œå¹¶è¯„ä¼°äº†çº¯å›¾åƒæç¤ºä¸å›¾åƒåŠ åˆæˆä¸´åºŠç¬”è®°çš„å¤šæ¨¡æ€è¾“å…¥æ•ˆæœã€‚

**Result:** åœ¨234ä¾‹çº¯å›¾åƒæµ‹è¯•ä¸­ï¼ŒChatGPTå’ŒClaudeåˆ†åˆ«è¾¾åˆ°62.8%å’Œ76.9%çš„å‡†ç¡®ç‡ï¼Œå…±è¯†æ–¹æ³•æå‡è‡³77.6%ï¼›åœ¨50ä¾‹å¤šæ¨¡æ€æµ‹è¯•ä¸­ï¼Œå‡†ç¡®ç‡åˆ†åˆ«æå‡è‡³84%å’Œ76%ï¼Œå…±è¯†å‡†ç¡®ç‡è¾¾åˆ°91.3%ï¼Œèåˆç­–ç•¥å§‹ç»ˆä¼˜äºå•ä¸€æ¨¡å‹ã€‚

**Conclusion:** ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€è¾“å…¥å’Œè¾“å‡ºçº§å…±è¯†æœºåˆ¶åœ¨æå‡AIè¾…åŠ©æ”¾å°„è¯Šæ–­å¯é æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå‡å°‘è¯Šæ–­é”™è¯¯æä¾›äº†è®¡ç®—å¼€é”€æœ€å°çš„å®ç”¨è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠè½¬åŒ–ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.


### [75] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han, Anuradha Mathrani, Teo Susnjak*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶ç³»ç»Ÿé‡åŒ–äº†æç¤ºç­–ç•¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–ç³»ç»Ÿæ–‡çŒ®ç»¼è¿°ç­›é€‰é˜¶æ®µçš„äº¤äº’æ•ˆåº”ï¼Œå‘ç°CoTå°‘æ ·æœ¬æç¤ºæä¾›æœ€å¯é çš„ç²¾ç¡®ç‡-å¬å›ç‡å¹³è¡¡ï¼Œå¹¶æå‡ºäº†åŸºäºæˆæœ¬æ•ˆç›Šçš„åˆ†é˜¶æ®µå·¥ä½œæµç¨‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç³»ç»Ÿæ–‡çŒ®ç»¼è¿°è‡ªåŠ¨åŒ–ç­›é€‰é˜¶æ®µä¸­ï¼Œä¸åŒæç¤ºç­–ç•¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„äº¤äº’æ•ˆåº”å°šæœªè¢«ç³»ç»Ÿé‡åŒ–çš„é—®é¢˜ï¼Œæ¢ç´¢å¦‚ä½•é€šè¿‡ä¼˜åŒ–æç¤º-æ¨¡å‹ç»„åˆæ¥æé«˜ç­›é€‰æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**Method:** ç ”ç©¶è¯„ä¼°äº†å…­ç§LLMåœ¨äº”ç§æç¤ºç±»å‹ä¸‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€æ€ç»´é“¾ã€æ€ç»´é“¾å°‘æ ·æœ¬å’Œè‡ªåæ€æç¤ºï¼Œä½¿ç”¨å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°ç­‰æŒ‡æ ‡ï¼Œå¯¹ç›¸å…³æ€§åˆ†ç±»å’Œå…­ä¸ªäºŒçº§ä»»åŠ¡è¿›è¡Œå…¨é¢åˆ†æã€‚

**Result:** ç»“æœæ˜¾ç¤ºæ˜¾è‘—çš„æ¨¡å‹-æç¤ºäº¤äº’æ•ˆåº”ï¼šCoTå°‘æ ·æœ¬æç¤ºæä¾›æœ€å¯é çš„ç²¾ç¡®ç‡-å¬å›ç‡å¹³è¡¡ï¼›é›¶æ ·æœ¬æç¤ºåœ¨é«˜æ•æ„Ÿåº¦ç­›é€‰æ—¶æœ€å¤§åŒ–å¬å›ç‡ï¼›è‡ªåæ€æç¤ºå› è¿‡åº¦åŒ…å®¹æ€§å’Œä¸ç¨³å®šæ€§è¡¨ç°ä¸ä½³ã€‚GPT-4oå’ŒDeepSeekæä¾›ç¨³å¥çš„æ•´ä½“æ€§èƒ½ï¼Œè€ŒGPT-4o-miniåœ¨æ˜¾è‘—é™ä½æˆæœ¬çš„åŒæ—¶ä¿æŒç«äº‰åŠ›ã€‚

**Conclusion:** ç ”ç©¶æ¨èé‡‡ç”¨åˆ†é˜¶æ®µå·¥ä½œæµç¨‹ï¼šé¦–å…ˆä½¿ç”¨ä½æˆæœ¬æ¨¡å‹é…åˆç»“æ„åŒ–æç¤ºè¿›è¡Œåˆç­›ï¼Œä»…å°†è¾¹ç•Œæ¡ˆä¾‹å‡çº§åˆ°é«˜å®¹é‡æ¨¡å‹å¤„ç†ã€‚è¿™äº›å‘ç°çªæ˜¾äº†LLMåœ¨è‡ªåŠ¨åŒ–æ–‡çŒ®ç­›é€‰æ–¹é¢ä¸å‡è¡¡ä½†å…·æœ‰å‰æ™¯çš„æ½œåŠ›ï¼Œä¸ºä»»åŠ¡è‡ªé€‚åº”LLMéƒ¨ç½²æä¾›äº†æ¯”è¾ƒåŸºå‡†å’Œå®è·µæŒ‡å¯¼ã€‚

---

#### ğŸ“„ Abstract
This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.


### [76] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil, Abdelrahman Elsayed, Abdelrahman Lila, Ahmed Gad, Hesham Abdelgawad, Mohamed Aref, Ahmed Fares*

#### ğŸ§© TL;DR
æœ¬æ–‡ä»‹ç»äº†EgMM-Corpusï¼Œä¸€ä¸ªä¸“é—¨é’ˆå¯¹åŸƒåŠæ–‡åŒ–çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«3,000å¤šå¼ å›¾åƒè¦†ç›–313ä¸ªæ–‡åŒ–æ¦‚å¿µï¼Œå¹¶é€šè¿‡è¯„ä¼°CLIPæ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šçš„è¡¨ç°æ­ç¤ºäº†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡åŒ–åè§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰AIé¢†åŸŸç¼ºä¹é’ˆå¯¹ä¸­ä¸œå’Œéæ´²åœ°åŒºçš„å¤šæ¨¡æ€æ–‡åŒ–å¤šæ ·æ€§æ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯åŸƒåŠæ–‡åŒ–èƒŒæ™¯ä¸‹çš„èµ„æºä¸¥é‡ä¸è¶³ï¼Œè¿™é™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è·¨æ–‡åŒ–åœºæ™¯ä¸­çš„è¯„ä¼°å’Œå‘å±•ã€‚

**Method:** ç ”ç©¶è®¾è®¡å¹¶è¿è¡Œäº†æ–°çš„æ•°æ®æ”¶é›†æµç¨‹ï¼Œæ”¶é›†äº†æ¶µç›–åœ°æ ‡ã€é£Ÿç‰©å’Œæ°‘é—´ä¼ è¯´ç­‰313ä¸ªæ¦‚å¿µçš„3,000å¤šå¼ å›¾åƒï¼Œæ¯ä¸ªæ¡ç›®éƒ½ç»è¿‡äººå·¥éªŒè¯ä»¥ç¡®ä¿æ–‡åŒ–çœŸå®æ€§å’Œå¤šæ¨¡æ€ä¸€è‡´æ€§ã€‚

**Result:** åœ¨EgMM-Corpusä¸Šè¯„ä¼°CLIPæ¨¡å‹çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œç»“æœæ˜¾ç¤ºTop-1å‡†ç¡®ç‡ä¸º21.2%ï¼ŒTop-5å‡†ç¡®ç‡ä¸º36.4%ï¼Œæ˜¾è‘—ä½äºåœ¨ä¸»æµæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¯å®äº†å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„æ–‡åŒ–åè§ï¼Œå¼ºè°ƒäº†EgMM-Corpusä½œä¸ºå¼€å‘æ–‡åŒ–æ„ŸçŸ¥æ¨¡å‹åŸºå‡†çš„é‡è¦æ€§ï¼Œä¸ºä¿ƒè¿›AIç³»ç»Ÿçš„æ–‡åŒ–å¤šæ ·æ€§æä¾›äº†å…³é”®èµ„æºã€‚

---

#### ğŸ“„ Abstract
Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.


### [77] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo, Samuel Dahan, Xiaodan Zhu*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸¤ç§ä½èµ„æºè®¾ç½®ä¸‹çš„å¤šå…ƒåŒ–å¯¹é½æ–¹æ³•â€”â€”å¤šå…ƒåŒ–è§£ç å’Œæ¨¡å‹å¼•å¯¼ï¼Œé€šè¿‡ä»…50ä¸ªæ ‡æ³¨æ ·æœ¬å³å¯åœ¨å¤šä¸ªé«˜é£é™©ä»»åŠ¡ä¸­æ˜¾è‘—æå‡è¯­è¨€æ¨¡å‹çš„å¤šå…ƒåŒ–å¯¹é½èƒ½åŠ›ï¼Œå‡å°‘è¯¯åˆ¤å¹¶æ”¹å–„äººç±»ä»·å€¼è§‚åˆ†å¸ƒå¯¹é½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€è¯­è¨€æ¨¡å‹å¯¹ç¤¾ä¼šå½±å“æ—¥ç›Šå¢å¤§ï¼Œéœ€è¦ç¡®ä¿å®ƒä»¬èƒ½å¤Ÿä¸å¤šæ ·åŒ–è§†è§’å¯¹é½å¹¶åæ˜ äººç±»ä»·å€¼è§‚çš„ç»†å¾®å·®åˆ«ï¼Œç„¶è€Œå½“å‰ä¸»æµè®­ç»ƒèŒƒå¼å‡è®¾æ¯ä¸ªæŸ¥è¯¢å­˜åœ¨å”¯ä¸€æœ€ä¼˜ç­”æ¡ˆï¼Œå¯¼è‡´ç”Ÿæˆå“åº”è¿‡äºæ³›åŒ–ä¸”å¯¹é½æ•ˆæœä¸ä½³ã€‚

**Method:** æå‡ºäº†ä¸¤ç§ä½èµ„æºè®¾ç½®ä¸‹çš„å¤šå…ƒåŒ–å¯¹é½æ–¹æ³•ï¼šå¤šå…ƒåŒ–è§£ç é€šè¿‡å¼•å…¥å¤šæ ·æ€§æœºåˆ¶ç”Ÿæˆå¤šä¸ªå“åº”å˜ä½“ï¼Œæ¨¡å‹å¼•å¯¼åˆ™åˆ©ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬ç›´æ¥è°ƒæ•´æ¨¡å‹è¡Œä¸ºä»¥æ›´å¥½åœ°æ•æ‰ä¸åŒè§‚ç‚¹ã€‚

**Result:** å®éªŒè¡¨æ˜æ¨¡å‹å¼•å¯¼æ–¹æ³•åœ¨ä»…ä½¿ç”¨50ä¸ªæ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œç›¸æ¯”é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åŸºçº¿å–å¾—ä¸€è‡´æ”¹è¿›ï¼Œåœ¨ä»‡æ¨è¨€è®ºæ£€æµ‹å’Œé”™è¯¯ä¿¡æ¯æ£€æµ‹ç­‰é«˜é£é™©ä»»åŠ¡ä¸­æ˜¾è‘—é™ä½è¯¯æŠ¥ç‡ï¼Œå¹¶åœ¨GlobalOpinionQAåŸºå‡†ä¸Šæ”¹å–„äº†ä¸äººç±»ä»·å€¼è§‚çš„åˆ†å¸ƒå¯¹é½ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†å¤šå…ƒåŒ–å¯¹é½çš„é‡è¦æ€§ï¼Œè¯æ˜äº†è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡è½»é‡çº§æ–¹æ³•æœ‰æ•ˆé€‚åº”ä¸åŒè§†è§’ï¼Œä¸ºæ„å»ºæ›´å…·åŒ…å®¹æ€§å’Œç»†è‡´ç†è§£èƒ½åŠ›çš„AIç³»ç»Ÿæä¾›äº†å¯è¡Œè·¯å¾„ï¼Œæ¨åŠ¨äº†ä»·å€¼è§‚æ•æ„Ÿåœºæ™¯ä¸‹è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.


### [78] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao, Bi-Cheng Yan, Berlin Chen*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æ¢ç´¢äº†Whisperè¯­éŸ³è¯†åˆ«åŸºç¡€æ¨¡å‹åœ¨ç¬¬äºŒè¯­è¨€å£è¯­è¯„ä¼°ä¸­çš„æ½œåŠ›ï¼Œé€šè¿‡æå–å…¶éšè—è¡¨ç¤ºä¸­çš„å£°å­¦å’Œè¯­è¨€ç‰¹å¾ï¼Œä»…éœ€è®­ç»ƒè½»é‡çº§åˆ†ç±»å™¨å³å¯åœ¨GEPTæ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰å…ˆè¿›åŸºçº¿ï¼Œå¹¶æ­ç¤ºäº†è¯¥æ¨¡å‹å†…åœ¨ç¼–ç äº†å£è¯­èƒ½åŠ›ç­‰çº§å’Œè¯­ä¹‰ä¿¡æ¯ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç ”ç©¶ä¸»è¦ä»å¤–éƒ¨åˆ†æWhisperäº§ç”Ÿçš„è½¬å½•æ–‡æœ¬ï¼Œæœªèƒ½å……åˆ†æŒ–æ˜å…¶æ½œåœ¨èƒ½åŠ›ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢Whisperåœ¨ç¬¬äºŒè¯­è¨€å£è¯­è¯„ä¼°ä¸­çš„éšè—è¡¨å¾èƒ½åŠ›ï¼Œå¡«è¡¥äº†ç›´æ¥åˆ©ç”¨åŸºç¡€æ¨¡å‹å†…éƒ¨è¡¨ç¤ºè¿›è¡Œå£è¯­èƒ½åŠ›è¯„ä¼°çš„ç ”ç©¶ç©ºç™½ã€‚

**Method:** é€šè¿‡æå–Whisperéšè—è¡¨ç¤ºä¸­çš„å£°å­¦å’Œè¯­è¨€ç‰¹å¾ï¼Œä»…è®­ç»ƒè½»é‡çº§åˆ†ç±»å™¨äºå…¶ä¸­é—´å’Œæœ€ç»ˆè¾“å‡ºä¹‹ä¸Šï¼Œå¹¶å¼•å…¥å›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯ä½œä¸ºè¾…åŠ©ç›¸å…³æ€§çº¿ç´¢æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚

**Result:** åœ¨GEPTå›¾ç‰‡æè¿°æ•°æ®é›†ä¸Šå–å¾—äº†å¼ºåŠ²æ€§èƒ½ï¼Œè¶…è¶Šäº†åŒ…æ‹¬å¤šæ¨¡æ€æ–¹æ³•åœ¨å†…çš„ç°æœ‰å…ˆè¿›åŸºçº¿ï¼Œé€šè¿‡æ•´åˆå›¾åƒå’Œæ–‡æœ¬æç¤ºä¿¡æ¯è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½è¡¨ç°ï¼Œæ·±å…¥åˆ†ææ˜¾ç¤ºWhisperåµŒå…¥å†…åœ¨ç¼–ç äº†å£è¯­èƒ½åŠ›ç­‰çº§æ¨¡å¼å’Œè¯­éŸ³è¯­ä¹‰æ–¹é¢ã€‚

**Conclusion:** å³ä½¿æ²¡æœ‰ä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒï¼ŒWhisperæ¨¡å‹æœ¬è´¨ä¸Šç¼–ç äº†å£è¯­èƒ½åŠ›çš„åºæ•°æ¨¡å¼å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œçªæ˜¾äº†å…¶ä½œä¸ºå£è¯­è¯„ä¼°åŠå…¶ä»–å£è¯­ç†è§£ä»»åŠ¡çš„å¼ºå¤§åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œä¸ºåŸºäºåŸºç¡€æ¨¡å‹çš„å£è¯­èƒ½åŠ›è¯„ä¼°æä¾›äº†æ–°èŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.


### [79] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji, Yuekui Yang, Haiyang Wu, Shaoping Ma, Tianrun Chen, Lanyun Zhu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºRAVENæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œè§£å†³äº†å¹¿å‘Šè§†é¢‘è¿è§„æ£€æµ‹ä¸­çš„æ—¶åºå®šä½ä¸ç²¾ç¡®ã€æ ‡æ³¨å™ªå£°å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™ç­‰é—®é¢˜ï¼Œåœ¨å·¥ä¸šæ•°æ®é›†å’Œå…¬å¼€åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¹¿å‘Šè§†é¢‘è¿è§„æ£€æµ‹æ–¹æ³•åœ¨ç²¾ç¡®æ—¶åºå®šä½ã€å™ªå£°æ ‡æ³¨å¤„ç†å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³å¹³å°åˆè§„æ€§è¦æ±‚çš„å®é™…åº”ç”¨éœ€æ±‚ã€‚

**Method:** RAVENæ¡†æ¶æ•´åˆè¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ¸è¿›å¼è®­ç»ƒç­–ç•¥ç»“åˆç²¾ç¡®å’Œç²—ç•¥æ ‡æ³¨æ•°æ®ï¼Œåˆ©ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å¼€å‘æ¶Œç°æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¤šå±‚å¤æ‚å¥–åŠ±æœºåˆ¶ç¡®ä¿ç²¾ç¡®æ—¶åºå®šä½å’Œä¸€è‡´ç±»åˆ«é¢„æµ‹ã€‚

**Result:** åœ¨å·¥ä¸šæ•°æ®é›†å’Œå…¬å¼€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRAVENåœ¨è¿è§„ç±»åˆ«å‡†ç¡®ç‡å’Œæ—¶åºåŒºé—´å®šä½æ–¹é¢å‡å–å¾—ä¼˜è¶Šæ€§èƒ½ï¼Œåœ¨çº¿A/Bæµ‹è¯•è¿›ä¸€æ­¥éªŒè¯äº†å…¶å®é™…é€‚ç”¨æ€§ï¼Œåœ¨ç²¾ç¡®ç‡å’Œå¬å›ç‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼ŒåŒæ—¶å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** RAVENæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ååŒè®¾è®¡ï¼Œæœ‰æ•ˆè§£å†³äº†å¹¿å‘Šè¿è§„æ£€æµ‹çš„å…³é”®æŒ‘æˆ˜ï¼Œå…¶åœ¨çº¿éƒ¨ç½²éªŒè¯äº†å®é™…åº”ç”¨ä»·å€¼ï¼Œå¹¶ä¸ºç±»ä¼¼æ—¶åºå¤šæ¨¡æ€ä»»åŠ¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.


### [80] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova, Mohammed Makhlouf*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MOSAICæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¥å­åµŒå…¥æ¨¡å‹é¢†åŸŸé€‚åº”çš„å¤šé˜¶æ®µæ–¹æ³•ï¼Œé€šè¿‡è”åˆä¼˜åŒ–æ©ç è¯­è¨€å»ºæ¨¡å’Œå¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼Œåœ¨ä¿æŒåŸå§‹æ¨¡å‹é²æ£’è¯­ä¹‰åŒºåˆ†èƒ½åŠ›çš„åŒæ—¶æœ‰æ•ˆå­¦ä¹ é¢†åŸŸç›¸å…³è¡¨ç¤ºã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å°†å¤§è§„æ¨¡é€šç”¨é¢†åŸŸå¥å­åµŒå…¥æ¨¡å‹é€‚åº”åˆ°ä¸“ä¸šé¢†åŸŸæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åœ¨ä¿æŒæ¨¡å‹åŸæœ‰è¯­ä¹‰åŒºåˆ†èƒ½åŠ›çš„åŒæ—¶æœ‰æ•ˆå­¦ä¹ é¢†åŸŸç‰¹å®šçŸ¥è¯†ã€‚

**Method:** MOSAICæ¡†æ¶é‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡è”åˆä¼˜åŒ–æ©ç è¯­è¨€å»ºæ¨¡å’Œå¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼Œç»“åˆé€‰æ‹©æ€§é€‚åº”æœºåˆ¶ï¼Œåœ¨ç»Ÿä¸€è®­ç»ƒæµç¨‹ä¸­å®ç°é¢†åŸŸç›¸å…³è¡¨ç¤ºå­¦ä¹ ã€‚

**Result:** åœ¨é«˜ä½èµ„æºé¢†åŸŸçš„å®è¯éªŒè¯ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨NDCG@10æŒ‡æ ‡ä¸Šç›¸æ¯”å¼ºé€šç”¨é¢†åŸŸåŸºçº¿æå‡äº†é«˜è¾¾13.4%ï¼Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†å„ç»„ä»¶çš„é‡è¦æ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜å¹³è¡¡çš„è”åˆç›‘ç£å’Œåˆ†é˜¶æ®µé€‚åº”ç­–ç•¥å¯¹äºé¢†åŸŸé€‚åº”è‡³å…³é‡è¦ï¼Œä¸ºå¥å­åµŒå…¥æ¨¡å‹çš„é¢†åŸŸä¸“ä¸šåŒ–æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶å¼ºè°ƒäº†å¤šç›®æ ‡è”åˆä¼˜åŒ–çš„ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.


### [81] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin, Yuyao Zhang, Yimeng Xu, Hongjin Qian, Yutao Zhu, Zhicheng Dou*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†FinSightï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆé«˜è´¨é‡å¤šæ¨¡æ€è´¢åŠ¡æŠ¥å‘Šçš„æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡å¯ç¼–ç¨‹å˜é‡ç©ºé—´å’Œè¿­ä»£è§†è§‰å¢å¼ºæœºåˆ¶ï¼Œåœ¨äº‹å®å‡†ç¡®æ€§ã€åˆ†ææ·±åº¦å’Œå‘ˆç°è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ç³»ç»Ÿã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰AIç³»ç»Ÿéš¾ä»¥å®Œå…¨è‡ªåŠ¨åŒ–ç”Ÿæˆä¸“ä¸šè´¢åŠ¡æŠ¥å‘Šï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢è€—æ—¶åˆéœ€è¦ä¸“ä¸šçŸ¥è¯†ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿçµæ´»æ”¶é›†æ•°æ®ã€è¿›è¡Œåˆ†æå¹¶ç”Ÿæˆé«˜è´¨é‡å¤šæ¨¡æ€æŠ¥å‘Šçš„æ™ºèƒ½ç³»ç»Ÿã€‚

**Method:** FinSighté‡‡ç”¨å¸¦å¯å˜å†…å­˜çš„ä»£ç æ™ºèƒ½ä½“æ¶æ„ï¼Œå°†å¤–éƒ¨æ•°æ®ã€è®¾è®¡å·¥å…·å’Œæ™ºèƒ½ä½“ç»Ÿä¸€åˆ°å¯ç¼–ç¨‹å˜é‡ç©ºé—´ä¸­ï¼›æå‡ºè¿­ä»£è§†è§‰å¢å¼ºæœºåˆ¶é€æ­¥ä¼˜åŒ–åŸå§‹è§†è§‰è¾“å‡ºä¸ºä¸“ä¸šè´¢åŠ¡å›¾è¡¨ï¼›å¹¶é‡‡ç”¨ä¸¤é˜¶æ®µå†™ä½œæ¡†æ¶å°†ç®€æ´çš„åˆ†æé“¾æ‰©å±•ä¸ºè¿è´¯ã€å¼•ç”¨æ„ŸçŸ¥çš„å¤šæ¨¡æ€æŠ¥å‘Šã€‚

**Result:** åœ¨å¤šä¸ªå…¬å¸å’Œè¡Œä¸šçº§ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFinSightåœ¨äº‹å®å‡†ç¡®æ€§ã€åˆ†ææ·±åº¦å’Œå‘ˆç°è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿ç³»ç»Ÿï¼ŒåŒ…æ‹¬é¢†å…ˆçš„æ·±åº¦ç ”ç©¶ç³»ç»Ÿï¼Œæ˜¾ç¤ºå‡ºæ¥è¿‘äººç±»ä¸“å®¶æ°´å¹³æŠ¥å‘Šç”Ÿæˆçš„æ½œåŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†ç”Ÿæˆæ¥è¿‘äººç±»ä¸“å®¶è´¨é‡è´¢åŠ¡æŠ¥å‘Šçš„å¯è¡Œè·¯å¾„ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“æ¡†æ¶å’Œè¿­ä»£ä¼˜åŒ–æœºåˆ¶å®ç°äº†ä¸“ä¸šçº§è´¢åŠ¡åˆ†æå’Œå¯è§†åŒ–ï¼Œä¸ºè‡ªåŠ¨åŒ–é‡‘èåˆ†æç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚

---

#### ğŸ“„ Abstract
Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.


### [82] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang, Yupei Wang, Kaijie Mo, Zhe Zhao, Renfen Hu*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†åŸºäºå¿ƒç†å­¦æ„ŸçŸ¥ç†è®ºçš„å¤šæ¨¡æ€çŸ¥è¯†ç†è§£åŸºå‡†ï¼Œé€šè¿‡æ¯”è¾ƒ30ç§å…ˆè¿›è¯­è¨€æ¨¡å‹å‘ç°ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å…·èº«çŸ¥è¯†ç†è§£æ–¹é¢å¹¶æœªè¶…è¶Šçº¯æ–‡æœ¬æ¨¡å‹ï¼Œä¸”æ¨¡å‹åœ¨è§†è§‰ç»´åº¦çš„è¡¨ç°æ˜¾è‘—å¼±äºå…¶ä»–æ„Ÿå®˜ç»´åº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°šä¸æ¸…æ¥šè§†è§‰åŸºç¡€æ˜¯å¦æ¯”çº¯æ–‡æœ¬æ¨¡å‹æ›´èƒ½å¢å¼ºå…¶å¯¹å…·èº«çŸ¥è¯†çš„ç†è§£ï¼Œå› æ­¤éœ€è¦ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ„Ÿå®˜æ¨¡æ€ä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

**Method:** åŸºäºå¿ƒç†å­¦æ„ŸçŸ¥ç†è®ºæ„å»ºäº†åŒ…å«è§†è§‰ã€å¬è§‰ã€è§¦è§‰ã€å‘³è§‰ã€å—…è§‰å¤–éƒ¨æ„Ÿå®˜åŠå†…æ„Ÿå—çš„å…·èº«çŸ¥è¯†ç†è§£åŸºå‡†ï¼Œé€šè¿‡å‘é‡æ¯”è¾ƒå’Œé—®ç­”ä»»åŠ¡è¯„ä¼°æ¨¡å‹æ„ŸçŸ¥èƒ½åŠ›ï¼Œæ¶µç›–è¶…è¿‡1700ä¸ªé—®é¢˜ã€‚

**Result:** æ¯”è¾ƒ30ç§å…ˆè¿›è¯­è¨€æ¨¡å‹åå‘ç°ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸¤é¡¹ä»»åŠ¡ä¸­å‡æœªè¶…è¶Šçº¯æ–‡æœ¬æ¨¡å‹ï¼Œä¸”æ¨¡å‹åœ¨è§†è§‰ç»´åº¦çš„è¡¨ç°æ˜¾è‘—å·®äºå…¶ä»–æ„Ÿå®˜ç»´åº¦ï¼Œå‘é‡è¡¨ç¤ºæ˜“å—è¯å½¢å’Œé¢‘ç‡å½±å“ï¼Œæ¨¡å‹åœ¨æ¶‰åŠç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†çš„é—®é¢˜ä¸Šè¡¨ç°å›°éš¾ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜å½“å‰è¯­è¨€æ¨¡å‹åœ¨å…·èº«çŸ¥è¯†æ•´åˆæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦æ›´æœ‰æ•ˆåœ°å°†å…·èº«çŸ¥è¯†èå…¥è¯­è¨€æ¨¡å‹ä»¥å¢å¼ºå…¶å¯¹ç‰©ç†ä¸–ç•Œçš„ç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.


### [83] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen, Akif Islam, Abu Saleh Musa Miah, Ayesha Siddiqua, Jungpil Shin*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡å¤§è§„æ¨¡æƒ…æ„Ÿåˆ†ææ­ç¤ºäº†å­ŸåŠ æ‹‰è¯­æ–°é—»ä¸­è´Ÿé¢æƒ…ç»ªçš„ä¸»å¯¼åœ°ä½ï¼Œå¹¶åŸºäºæ­¤æå‡ºäº†å¯è§†åŒ–æƒ…æ„Ÿçº¿ç´¢çš„æ–°é—»èšåˆå™¨è®¾è®¡ç†å¿µï¼Œå¸®åŠ©è¯»è€…è¯†åˆ«æ–°é—»æŠ¥é“ä¸­çš„æƒ…æ„Ÿæ¡†æ¶åè§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ–°é—»åª’ä½“é€šè¿‡æƒ…æ„Ÿæ¡†æ¶å½±å“å…¬ä¼—æƒ…ç»ªï¼Œè´Ÿé¢æˆ–æƒ…ç»ªåŒ–æ ‡é¢˜å¾€å¾€è·å¾—æ›´å¤šå…³æ³¨å¹¶ä¼ æ’­æ›´å¿«ï¼Œå¯¼è‡´åª’ä½“å€¾å‘äºä½¿ç”¨å¼•å‘å¼ºçƒˆååº”çš„æŠ¥é“æ–¹å¼ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å­ŸåŠ æ‹‰è¯­æ–°é—»ä¸­çš„è¿™ç§æƒ…æ„Ÿåè§ç°è±¡ã€‚

**Method:** ä½¿ç”¨Gemma-3 4Bæ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬æ¨ç†ï¼Œåˆ†æäº†30ä¸‡æ¡å­ŸåŠ æ‹‰è¯­æ–°é—»æ ‡é¢˜åŠå…¶å†…å®¹ï¼Œè¯†åˆ«æ¯æ¡æ–°é—»çš„ä¸»å¯¼æƒ…ç»ªå’Œæ•´ä½“æƒ…æ„ŸåŸºè°ƒã€‚

**Result:** ç ”ç©¶å‘ç°è´Ÿé¢æƒ…ç»ªæ˜æ˜¾å ä¸»å¯¼åœ°ä½ï¼Œç‰¹åˆ«æ˜¯æ„¤æ€’ã€ææƒ§å’Œå¤±æœ›æƒ…ç»ªï¼ŒåŒæ—¶ä¸åŒåª’ä½“å¯¹ç›¸ä¼¼æ–°é—»äº‹ä»¶çš„æƒ…æ„Ÿå‘ˆç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚

**Conclusion:** åŸºäºåˆ†æç»“æœæå‡ºäº†ä»¥äººä¸ºæœ¬çš„æ–°é—»èšåˆå™¨è®¾è®¡ç†å¿µï¼Œé€šè¿‡å¯è§†åŒ–æƒ…æ„Ÿçº¿ç´¢å¸®åŠ©è¯»è€…è¯†åˆ«æ—¥å¸¸æ–°é—»ä¸­éšè—çš„æƒ…æ„Ÿæ¡†æ¶ï¼Œå¢å¼ºå¯¹åª’ä½“æŠ¥é“åè§çš„è®¤çŸ¥èƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.


### [84] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou, Mohamed Sinane El Messoussi, AnaÃ¯s Ollagnier*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“å¤šè½®å¯¹è¯ä¸­çš„åç¤¾ä¼šè¡Œä¸ºæ£€æµ‹ï¼Œæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€èåˆæ–¹æ³•ï¼Œåœ¨æ³•è¯­æ•°æ®é›†CyberAgressionAdo-Largeä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†éšå¼æ”»å‡»å’Œè§’è‰²è½¬æ¢ç­‰å¤æ‚ç°è±¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç¤¾äº¤åª’ä½“åç¤¾ä¼šè¡Œä¸ºç ”ç©¶ä¸»è¦é›†ä¸­äºXå’ŒRedditç­‰å¹³å°ï¼Œè€Œå¤šè½®å¯¹è¯åœºæ™¯ç”±äºæ•°æ®ç¨€ç¼ºå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚

**Method:** ç ”ç©¶è¯„ä¼°äº†å…­ç§åŸºäºæ–‡æœ¬å’Œå…«ç§åŸºäºå›¾çš„è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œåˆ†æäº†è¯æ±‡çº¿ç´¢å’Œäº¤äº’åŠ¨æ€ï¼Œå¹¶æ¢ç´¢äº†å¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œé‡ç‚¹å…³æ³¨æ™šæœŸèåˆæ¨¡å‹mBERT + WD-SGCNã€‚

**Result:** å¤šæ¨¡æ€æ¨¡å‹æ˜¾è‘—ä¼˜äºå•æ¨¡æ€åŸºçº¿ï¼ŒmBERT + WD-SGCNåœ¨æ»¥ç”¨æ£€æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ°0.718çš„æœ€ä½³æ€§èƒ½ï¼Œåœ¨åŒä¼´ç¾¤ä½“è¯†åˆ«å’Œæ¬ºå‡Œè¡Œä¸ºåˆ†æä»»åŠ¡ä¸Šåˆ†åˆ«è·å¾—0.286å’Œ0.606çš„ç«äº‰æ€§åˆ†æ•°ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜å¤šæ¨¡æ€èåˆèƒ½æœ‰æ•ˆå¤„ç†å¤æ‚åç¤¾ä¼šè¡Œä¸ºç°è±¡ï¼ŒåŒ…æ‹¬éšå¼æ”»å‡»ã€è§’è‰²è½¬æ¢å’Œä¸Šä¸‹æ–‡ä¾èµ–çš„æ•Œæ„è¡Œä¸ºï¼Œä¸ºç¤¾äº¤åª’ä½“å®‰å…¨ç›‘æ§æä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.


### [85] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang, Guanting Dong, Xinyu Yang, Zhicheng Dou*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºNyxï¼Œä¸€ç§é’ˆå¯¹é€šç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆURAGï¼‰çš„ç»Ÿä¸€æ··åˆæ¨¡æ€æ£€ç´¢å™¨ï¼Œé€šè¿‡æ„å»ºNyxQAæ•°æ®é›†å’Œä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€æ£€ç´¢å’Œæ¨ç†çš„æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰RAGç³»ç»Ÿä¸»è¦å…³æ³¨å•æ¨¡æ€æ–‡æœ¬æ£€ç´¢ï¼Œåœ¨ç°å®åœºæ™¯ä¸­å½“æŸ¥è¯¢å’Œæ–‡æ¡£éƒ½åŒ…å«æ··åˆæ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬å’Œå›¾åƒï¼‰æ—¶è¡¨ç°ä¸è¶³ï¼Œéœ€è¦è§£å†³é€šç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆURAGï¼‰ä¸­æ··åˆæ¨¡æ€ä¿¡æ¯çš„æ£€ç´¢å’Œæ¨ç†é—®é¢˜ã€‚

**Method:** æå‡ºNyxç»Ÿä¸€æ··åˆæ¨¡æ€æ£€ç´¢å™¨ï¼Œæ„å»ºå››é˜¶æ®µè‡ªåŠ¨æµæ°´çº¿ç”Ÿæˆå’Œè¿‡æ»¤NyxQAæ•°æ®é›†ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šå…ˆåœ¨NyxQAå’Œå¼€æºæ£€ç´¢æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååˆ©ç”¨ä¸‹æ¸¸è§†è§‰è¯­è¨€æ¨¡å‹çš„åé¦ˆè¿›è¡Œç›‘ç£å¾®è°ƒä»¥å¯¹é½æ£€ç´¢è¾“å‡ºä¸ç”Ÿæˆåå¥½ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜Nyxä¸ä»…åœ¨æ ‡å‡†æ–‡æœ¬RAGåŸºå‡†ä¸Šè¡¨ç°ç«äº‰åŠ›ï¼Œåœ¨æ›´é€šç”¨å’Œç°å®çš„URAGè®¾ç½®ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„ç”Ÿæˆè´¨é‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ··åˆæ¨¡æ€æ£€ç´¢åœ¨å¢å¼ºè§†è§‰è¯­è¨€ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤„ç†ç°å®ä¸–ç•Œå¤šæ¨¡æ€ä¿¡æ¯éœ€æ±‚æä¾›äº†å¯è¡Œè§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºé€šç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆå¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.


### [86] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim, Kwan Hui Lim*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯„ä¼°äº†20ä¸ªæŒ‡ä»¤è°ƒä¼˜å¤§è¯­è¨€æ¨¡å‹åœ¨åŸå­æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼Œå‘ç°æ¨¡å‹å¯¹é€‰é¡¹æ ‡ç­¾æ ¼å¼å­˜åœ¨æ˜¾è‘—åè§ä¸”æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸è¶³ï¼Œæ­ç¤ºäº†å½“å‰æŒ‡ä»¤è°ƒä¼˜èŒƒå¼çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡æŒ‡ä»¤è°ƒä¼˜å¤§è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶æ‰§è¡Œç®€å•ã€è‡ªåŒ…å«æŒ‡ä»¤çš„åŸºæœ¬èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè€Œè¿™æ­£æ˜¯å¤æ‚æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„åŸºç¡€ã€‚

**Method:** ç ”ç©¶åœ¨ä¿®æ”¹åçš„MMLUå’ŒMMLU-ProåŸºå‡†ä¸Šç³»ç»Ÿæ€§åœ°æ”¹å˜é€‰é¡¹æ ‡ç­¾æ ¼å¼ï¼ˆå­—æ¯ã€æ•°å­—ã€ç½—é©¬æ•°å­—ï¼‰ï¼Œåœ¨å››ç§å®éªŒèŒƒå¼ä¸‹è¯„ä¼°20ä¸ªIT-LLMï¼šæ˜¾å¼æŒ‡ä»¤ã€æ— æŒ‡ä»¤ã€ç§»é™¤é€‰é¡¹å†…å®¹å’Œä¸‰æ ·æœ¬ç¤ºä¾‹ã€‚

**Result:** æ ‡ç­¾æ ¼å¼å˜åŒ–å¯¼è‡´æ˜¾è‘—æ€§èƒ½æ³¢åŠ¨ï¼ˆç½—é©¬vsæ•°å­—ä¸‹é™30.45%ï¼‰ï¼Œæ— æŒ‡ä»¤æ—¶æ€§èƒ½è¿›ä¸€æ­¥ä¸‹é™10.84%ï¼Œç§»é™¤é€‰é¡¹å†…å®¹åé™¤æ•°å­—æ ‡ç­¾å¤–å‡æ— æ³•è¶…è¶ŠéšæœºåŸºçº¿ï¼Œä¸‰æ ·æœ¬ç¤ºä¾‹æœªèƒ½æ˜¾è‘—æå‡é²æ£’æ€§ï¼Œå¤§æ¨¡å‹å‡†ç¡®ç‡æ›´é«˜ä½†æŒ‡ä»¤éµå¾ªä¸€è‡´æ€§ä»ä¸è¶³ã€‚

**Conclusion:** ç ”ç©¶ç»“æœæš´éœ²äº†å½“å‰æŒ‡ä»¤è°ƒä¼˜èŒƒå¼çš„ä¸è¶³ï¼Œå¼ºè°ƒéœ€è¦å¼€å‘ä¸“é—¨é’ˆå¯¹åŸå­æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„è¯„ä¼°æ–¹æ³•å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥æå‡æ¨¡å‹çš„åŸºç¡€æŒ‡ä»¤ç†è§£ä¸æ‰§è¡Œèƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.


### [87] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole, Prince Mireku, Fatimo Adebanjo, Oluwatosin Olajide, Mahi Aminu Aliyu, Jekaterina Novikova*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†AfriCaptionæ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹20ç§éæ´²è¯­è¨€çš„å¤šè¯­è¨€å›¾åƒæè¿°ç³»ç»Ÿï¼Œé€šè¿‡ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ã€åŠ¨æ€è´¨é‡ä¿è¯æµç¨‹å’Œ0.5Bå‚æ•°æ¨¡å‹ï¼Œä¸ºèµ„æºåŒ®ä¹è¯­è¨€å»ºç«‹äº†å¯æ‰©å±•çš„å¤šæ¨¡æ€AIåŸºç¡€è®¾æ–½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€AIç ”ç©¶è¿‡åº¦é›†ä¸­äºé«˜èµ„æºè¯­è¨€ï¼Œé˜»ç¢äº†è¯¥é¢†åŸŸè¿›æ­¥çš„æ°‘ä¸»åŒ–ï¼Œç‰¹åˆ«æ˜¯éæ´²è¯­è¨€åœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸­ä¸¥é‡ç¼ºä¹ä»£è¡¨æ€§èµ„æºå’ŒæŠ€æœ¯æ”¯æŒã€‚

**Method:** æ„å»ºäº†åŸºäºFlickr8kçš„è¯­ä¹‰å¯¹é½å¤šè¯­è¨€æ•°æ®é›†ï¼Œé‡‡ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥é€‰æ‹©å’Œç¿»è¯‘æµç¨‹ï¼›å¼€å‘äº†åŠ¨æ€ä¸Šä¸‹æ–‡ä¿æŒç®¡é“ï¼Œé€šè¿‡æ¨¡å‹é›†æˆå’Œè‡ªé€‚åº”æ›¿æ¢ç¡®ä¿æŒç»­æ•°æ®è´¨é‡ï¼›è®¾è®¡äº†0.5Bå‚æ•°çš„AfriCaptionæ¨¡å‹ï¼Œæ•´åˆSigLIPå’ŒNLLB200å®ç°è·¨è¯­è¨€å›¾åƒæè¿°ç”Ÿæˆã€‚

**Result:** è¯¥æ¡†æ¶å»ºç«‹äº†é¦–ä¸ªé’ˆå¯¹éæ´²ä½èµ„æºè¯­è¨€çš„å¯æ‰©å±•å›¾åƒæè¿°èµ„æºï¼Œé€šè¿‡ç»Ÿä¸€æ¶æ„ç¡®ä¿äº†æŒç»­çš„æ•°æ®è´¨é‡ï¼Œä¸º20ç§ä»£è¡¨æ€§ä¸è¶³çš„éæ´²è¯­è¨€æä¾›äº†å¤šæ¨¡æ€AIæ”¯æŒã€‚

**Conclusion:** AfriCaptionä¸ºçœŸæ­£åŒ…å®¹æ€§å¤šæ¨¡æ€AIå¥ å®šäº†åŸºç¡€ï¼Œé€šè¿‡ç³»ç»ŸåŒ–æ–¹æ³•è§£å†³äº†è¯­è¨€å¤šæ ·æ€§é¸¿æ²Ÿï¼Œå±•ç¤ºäº†åœ¨èµ„æºåŒ®ä¹è¯­è¨€ç¯å¢ƒä¸­æ„å»ºé«˜è´¨é‡å¤šæ¨¡æ€ç³»ç»Ÿçš„å¯è¡Œæ€§ã€‚

---

#### ğŸ“„ Abstract
Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.


### [88] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie, Yang Yu, Yibo Chen, Hanyao Zhang, Lening Zhao, Jiaxuan He, Lei Jiang, Xiaoting Tang, Guanghui An, Dong Xu*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶å¼€å‘äº†BenCaoï¼Œä¸€ä¸ªåŸºäºChatGPTçš„ä¸­åŒ»å¤šæ¨¡æ€åŠ©æ‰‹ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è°ƒä¼˜è€Œéå‚æ•°é‡è®­ç»ƒçš„æ–¹å¼ï¼Œæ•´åˆäº†ç»“æ„åŒ–çŸ¥è¯†åº“ã€è¯Šæ–­æ•°æ®å’Œä¸“å®¶åé¦ˆï¼Œåœ¨ä¸­åŒ»é—®ç­”å’Œè¯Šæ–­ä»»åŠ¡ä¸­è¶…è¶Šäº†é€šç”¨é¢†åŸŸå’Œä¸­åŒ»é¢†åŸŸæ¨¡å‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿä¸­åŒ»ä¾èµ–æ•´ä½“æ¨ç†ã€éšå«é€»è¾‘å’Œå¤šæ¨¡æ€è¯Šæ–­çº¿ç´¢ï¼Œç°æœ‰ä¸­åŒ»é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬ç†è§£æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†ç¼ºä¹å¤šæ¨¡æ€æ•´åˆã€å¯è§£é‡Šæ€§å’Œä¸´åºŠé€‚ç”¨æ€§ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯å¹¶ç¬¦åˆä¸­åŒ»ä¸“ä¸šæ¨ç†å’Œä¼¦ç†è§„èŒƒçš„ç³»ç»Ÿã€‚

**Method:** ç³»ç»Ÿæ•´åˆäº†è¶…è¿‡1000éƒ¨ç»å…¸å’Œç°ä»£æ–‡çŒ®çš„å…¨é¢çŸ¥è¯†åº“ï¼Œé‡‡ç”¨åŸºäºåœºæ™¯çš„æŒ‡ä»¤æ¡†æ¶æ”¯æŒå¤šæ ·åŒ–äº¤äº’ï¼ŒåŒ…å«ç”¨äºå¯è§£é‡Šæ¨ç†çš„æ€ç»´é“¾æ¨¡æ‹Ÿæœºåˆ¶ï¼Œä»¥åŠç”±æ‰§ä¸šä¸­åŒ»å¸ˆå‚ä¸çš„åé¦ˆç²¾ç‚¼è¿‡ç¨‹ï¼Œå¹¶è¿æ¥å¤–éƒ¨APIå®ç°èˆŒè±¡åˆ†ç±»å’Œå¤šæ¨¡æ€æ•°æ®åº“æ£€ç´¢ã€‚

**Result:** åœ¨å•é¡¹é€‰æ‹©é—®ç­”åŸºå‡†å’Œå¤šæ¨¡æ€åˆ†ç±»ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒBenCaoåœ¨è¯Šæ–­ã€è‰è¯è¯†åˆ«å’Œä½“è´¨åˆ†ç±»æ–¹é¢å®ç°äº†ä¼˜äºé€šç”¨é¢†åŸŸå’Œä¸­åŒ»é¢†åŸŸæ¨¡å‹çš„å‡†ç¡®ç‡ï¼Œè¯¥ç³»ç»Ÿå·²åœ¨OpenAI GPTså•†åº—éƒ¨ç½²ä¸ºäº¤äº’å¼åº”ç”¨ï¼Œæˆªè‡³2025å¹´10æœˆå·²æœ‰è¿‘1000åå…¨çƒç”¨æˆ·è®¿é—®ã€‚

**Conclusion:** æœ¬ç ”ç©¶è¯æ˜äº†é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è°ƒä¼˜å’Œå¤šæ¨¡æ€é›†æˆå¼€å‘ä¸­åŒ»é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œä¸ºç”Ÿæˆå¼AIä¸ä¼ ç»ŸåŒ»å­¦æ¨ç†çš„å¯¹é½æä¾›äº†å®ç”¨æ¡†æ¶ï¼Œå¹¶ä¸ºç°å®ä¸–ç•Œéƒ¨ç½²æä¾›äº†å¯æ‰©å±•è·¯å¾„ï¼Œå±•ç¤ºäº†å°†AIæŠ€æœ¯ä¸ä¼ ç»ŸåŒ»å­¦å®è·µç›¸ç»“åˆçš„å®é™…ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.


### [89] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan, Jun Yan, Chun Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§å°‘æ ·æœ¬å‘½åå®ä½“è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡åˆ›æ–°çš„æŒ‡ä»¤è°ƒä¼˜æ¨¡æ¿å’Œä¿ç•™å®ä½“ä¿¡æ¯çš„ç­–ç•¥æ€§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œåœ¨ä½èµ„æºåœºæ™¯ä¸‹å®ç°äº†ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä½†åœ¨ä½èµ„æºåœºæ™¯ä¸­æ ‡ç­¾è·å–æˆæœ¬é«˜æ˜‚ã€‚ç°æœ‰çš„é›¶æ ·æœ¬å’ŒæŒ‡ä»¤è°ƒä¼˜æ–¹æ³•éš¾ä»¥æ³›åŒ–åˆ°é¢†åŸŸç‰¹å®šå®ä½“ï¼Œä¸”æ— æ³•æœ‰æ•ˆåˆ©ç”¨æœ‰é™çš„å¯ç”¨æ•°æ®ï¼Œè¿™æˆä¸ºå½“å‰ç ”ç©¶çš„ä¸»è¦æŒ‘æˆ˜ã€‚

**Method:** è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šä¸€æ˜¯è®¾è®¡æ–°çš„æŒ‡ä»¤è°ƒä¼˜æ¨¡æ¿ï¼Œé‡‡ç”¨ç®€åŒ–çš„è¾“å‡ºæ ¼å¼ç»“åˆå…ˆå‰ITæ–¹æ³•çš„åŸç†ï¼Œå……åˆ†åˆ©ç”¨æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤§ä¸Šä¸‹æ–‡çª—å£ï¼›äºŒæ˜¯å¼•å…¥ç­–ç•¥æ€§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œåœ¨ä¿æŒå®ä½“ä¿¡æ¯çš„åŒæ—¶å¯¹å‘¨å›´ä¸Šä¸‹æ–‡è¿›è¡Œæ”¹å†™ï¼Œä»è€Œæ‰©å±•è®­ç»ƒæ•°æ®è€Œä¸æŸå®³è¯­ä¹‰å…³ç³»ã€‚

**Result:** åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬ä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå…¶ä¸­å°‘æ ·æœ¬æ–¹æ³•åœ¨CrossNERæ•°æ®é›†ä¸Šå¹³å‡F1å¾—åˆ†ä¸º80.1ã€‚ä½¿ç”¨æ”¹å†™æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”åŸºçº¿ç‰ˆæœ¬åœ¨F1åˆ†æ•°ä¸Šå®ç°äº†é«˜è¾¾17ä¸ªç‚¹çš„æŒç»­æ”¹è¿›ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºæ‹¥æœ‰æœ‰é™NERè®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºçš„ç¾¤ä½“æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œè¯æ˜äº†è½»é‡çº§æ¡†æ¶åœ¨ä½èµ„æºåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºå°‘æ ·æœ¬NERä»»åŠ¡çš„å‘å±•æŒ‡æ˜äº†æ–°çš„æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [90] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li, Guangze Zhao, Sichen Liu*

#### ğŸ§© TL;DR
VisuoAlignæ˜¯ä¸€ä¸ªé€šè¿‡æç¤ºå¼•å¯¼æ ‘æœç´¢å®ç°å¤šæ¨¡æ€å®‰å…¨å¯¹é½çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡å°†å®‰å…¨çº¦æŸåµŒå…¥æ¨ç†è¿‡ç¨‹ã€ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢æ„å»ºå®‰å…¨å…³é”®æç¤ºè½¨è¿¹ï¼Œä»¥åŠå¼•å…¥åŸºäºæç¤ºçš„ç¼©æ”¾æ¥æ˜¾è‘—æå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¯¹å¤æ‚è·¨æ¨¡æ€å¨èƒçš„é²æ£’æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ„ŸçŸ¥å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å®‰å…¨å¯¹é½ä»é¢ä¸´å…³é”®æŒ‘æˆ˜ï¼Œç°æœ‰é˜²å¾¡æ–¹æ³•å¯¹å¤šæ¨¡æ€è¶Šç‹±æ”»å‡»å­˜åœ¨è„†å¼±æ€§ï¼Œè§†è§‰è¾“å…¥å¼•å…¥äº†æ–°çš„æ”»å‡»é¢ï¼Œæ¨ç†é“¾ç¼ºä¹å®‰å…¨ç›‘ç£ï¼Œä¸”æ¨¡æ€èåˆå¸¸å¸¸å¯¼è‡´å¯¹é½æ€§èƒ½ä¸‹é™ã€‚

**Method:** VisuoAligné€šè¿‡è§†è§‰-æ–‡æœ¬äº¤äº’æç¤ºå°†å®‰å…¨çº¦æŸåµŒå…¥æ¨ç†è¿‡ç¨‹ï¼Œé‡‡ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ç³»ç»Ÿæ€§åœ°æ„å»ºå¤šæ ·åŒ–çš„å®‰å…¨å…³é”®æç¤ºè½¨è¿¹ï¼Œå¹¶å¼•å…¥åŸºäºæç¤ºçš„ç¼©æ”¾æœºåˆ¶ä»¥ç¡®ä¿å®æ—¶é£é™©æ£€æµ‹å’Œåˆè§„å“åº”ã€‚

**Result:** å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVisuoAlignèƒ½å¤Ÿä¸»åŠ¨æš´éœ²é£é™©ï¼Œå®ç°å…¨é¢çš„æ•°æ®é›†ç”Ÿæˆï¼Œå¹¶æ˜¾è‘—æå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¯¹å¤æ‚è·¨æ¨¡æ€å¨èƒçš„é²æ£’æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€å®‰å…¨å¯¹é½æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç³»ç»ŸåŒ–çš„æç¤ºå¼•å¯¼æ ‘æœç´¢æ¡†æ¶å¢å¼ºäº†æ¨¡å‹çš„å®‰å…¨æ¨ç†èƒ½åŠ›ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€ç³»ç»Ÿçš„å®‰å…¨éƒ¨ç½²å¥ å®šäº†é‡è¦åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.


### [91] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu, Ying Long, Sophia Zuoqiu, Di Liu, Kang Li, Yiting Lin, Hanyi Liu, Rong Yin, Tian Tang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶éªŒè¯äº†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºç­–ç•¥ï¼ˆé€‰æ‹©æ€§å°‘æ ·æœ¬å­¦ä¹ ï¼‰å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ä¸´åºŠæ€ç»´é“¾ï¼Œæ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬å’Œéšæœºå°‘æ ·æœ¬æ–¹æ³•ï¼Œå¹¶æå‡ºäº†åŸºäº"é»„é‡‘æ ‡å‡†æ·±åº¦"å’Œ"ä»£è¡¨æ€§å¤šæ ·æ€§"çš„åŒåŸåˆ™æ¡†æ¶æ¥è§£å†³åŒ»ç–—AIä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŒ»ç–—AIä¸­é«˜è´¨é‡ä¸´åºŠæ€ç»´é“¾çš„åˆ›å»ºé¢ä¸´æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œè™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤ŸåˆæˆåŒ»ç–—æ•°æ®ï¼Œä½†å…¶ä¸´åºŠå¯é æ€§å°šæœªå¾—åˆ°éªŒè¯ï¼Œå› æ­¤éœ€è¦è¯„ä¼°LLMç”Ÿæˆçš„æ€ç»´é“¾å¯é æ€§å¹¶æ¢ç´¢æå‡å…¶è´¨é‡çš„æç¤ºç­–ç•¥ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨ç›²æ³•æ¯”è¾ƒç ”ç©¶è®¾è®¡ï¼Œç”±è¾…åŠ©ç”Ÿæ®–æŠ€æœ¯é¢†åŸŸçš„é«˜çº§ä¸´åºŠåŒ»ç”Ÿè¯„ä¼°ä¸‰ç§ä¸åŒæç¤ºç­–ç•¥ç”Ÿæˆçš„æ€ç»´é“¾ï¼šé›¶æ ·æœ¬å­¦ä¹ ã€éšæœºå°‘æ ·æœ¬å­¦ä¹ ï¼ˆä½¿ç”¨æµ…å±‚ç¤ºä¾‹ï¼‰å’Œé€‰æ‹©æ€§å°‘æ ·æœ¬å­¦ä¹ ï¼ˆä½¿ç”¨å¤šæ ·åŒ–é«˜è´¨é‡ç¤ºä¾‹ï¼‰ï¼Œå¹¶å°†ä¸“å®¶è¯„åˆ†ä¸æœ€å…ˆè¿›çš„AIæ¨¡å‹ï¼ˆGPT-4oï¼‰è¯„ä¼°ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚

**Result:** é€‰æ‹©æ€§å°‘æ ·æœ¬ç­–ç•¥åœ¨æ‰€æœ‰äººç±»è¯„ä¼°æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºå…¶ä»–ç­–ç•¥ï¼ˆp < .001ï¼‰ï¼Œè€Œéšæœºå°‘æ ·æœ¬ç­–ç•¥ç›¸æ¯”é›¶æ ·æœ¬åŸºçº¿æ²¡æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œè¡¨æ˜ä½è´¨é‡ç¤ºä¾‹ä¸æ— ç¤ºä¾‹åŒæ ·æ— æ•ˆï¼›AIè¯„ä¼°å™¨æœªèƒ½è¯†åˆ«è¿™äº›å…³é”®æ€§èƒ½å·®å¼‚ï¼Œé€‰æ‹©æ€§ç­–ç•¥çš„æˆåŠŸå½’å› äº"é»„é‡‘æ ‡å‡†æ·±åº¦"å’Œ"ä»£è¡¨æ€§å¤šæ ·æ€§"ä¸¤ä¸ªåŸåˆ™ã€‚

**Conclusion:** åˆæˆæ€ç»´é“¾çš„ä¸´åºŠå¯é æ€§å–å†³äºç­–ç•¥æ€§æç¤ºè®¾è®¡è€Œéå•çº¯ç¤ºä¾‹å­˜åœ¨ï¼Œæå‡ºçš„"åŒåŸåˆ™"æ¡†æ¶ä¸ºå¤§è§„æ¨¡ç”Ÿæˆå¯ä¿¡æ•°æ®æä¾›äº†åŸºç¡€æ–¹æ³•å­¦ï¼Œè¿™é¡¹å·¥ä½œä¸ºè§£å†³æ•°æ®ç“¶é¢ˆæä¾›äº†éªŒè¯æ–¹æ¡ˆï¼Œå¹¶ç¡®è®¤äº†äººç±»ä¸“ä¸šçŸ¥è¯†åœ¨è¯„ä¼°é«˜é£é™©ä¸´åºŠAIä¸­ä¸å¯æˆ–ç¼ºçš„ä½œç”¨ã€‚

---

#### ğŸ“„ Abstract
Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.


### [92] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang, Ru Zhang, Jianyi Liu, Zhen Yang, Gongshen Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†SELECTæ¡†æ¶ï¼Œä¸€ç§åŠ¨æ€é”šç‚¹é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºè§£å†³æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­æ¦‚å¿µæ“¦é™¤çš„é”šç‚¹æ•æ„Ÿæ€§é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å› æœè¿½è¸ªåˆ†ææ­ç¤ºäº†å›ºå®šé”šç‚¹ç­–ç•¥çš„å±€é™æ€§ï¼Œå¹¶å¼•å…¥å…„å¼Ÿæ’ä»–æ€§æ¦‚å¿µä½œä¸ºæ›´ä¼˜çš„é”šç‚¹ç±»åˆ«ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¦‚å¿µæ“¦é™¤æ–¹æ³•é€šå¸¸ä¾èµ–å›ºå®šé”šç‚¹ç­–ç•¥ï¼Œè¿™ä¼šå¯¼è‡´æ¦‚å¿µé‡æ–°å‡ºç°å’Œä¾µèš€ç­‰å…³é”®é—®é¢˜ã€‚é€šè¿‡å› æœè¿½è¸ªåˆ†æï¼Œç ”ç©¶å‘ç°æ“¦é™¤æ•ˆæœå¯¹é”šç‚¹é€‰æ‹©å…·æœ‰å†…åœ¨æ•æ„Ÿæ€§ï¼Œéœ€è¦å¼€å‘æ›´æ™ºèƒ½çš„é”šç‚¹é€‰æ‹©æœºåˆ¶æ¥å…‹æœè¿™äº›é™åˆ¶ã€‚

**Method:** æå‡ºäº†SELECTæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§åŠ¨æ€é”šç‚¹é€‰æ‹©æ–¹æ³•ï¼Œé‡‡ç”¨æ–°é¢–çš„ä¸¤é˜¶æ®µè¯„ä¼°æœºåˆ¶ã€‚è¯¥æ¡†æ¶è‡ªåŠ¨å‘ç°ç”¨äºç²¾ç¡®æ“¦é™¤çš„æœ€ä¼˜é”šç‚¹ï¼ŒåŒæ—¶è¯†åˆ«å…³é”®è¾¹ç•Œé”šç‚¹ä»¥ä¿ç•™ç›¸å…³æ¦‚å¿µï¼Œå°†å…„å¼Ÿæ’ä»–æ€§æ¦‚å¿µå®šä¹‰ä¸ºä¸€ç±»æ›´ä¼˜çš„é”šç‚¹ç±»åˆ«ã€‚

**Result:** å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSELECTä½œä¸ºä¸€ç§é€šç”¨é”šç‚¹è§£å†³æ–¹æ¡ˆï¼Œä¸ä»…èƒ½å¤Ÿé«˜æ•ˆé€‚é…å¤šç§æ“¦é™¤æ¡†æ¶ï¼Œè€Œä¸”åœ¨å…³é”®æ€§èƒ½æŒ‡æ ‡ä¸ŠæŒç»­ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚å¯¹äºå•ä¸ªæ¦‚å¿µçš„é”šç‚¹æŒ–æ˜ï¼Œå¹³å‡ä»…éœ€4ç§’æ—¶é—´ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†æ¦‚å¿µæ“¦é™¤ä¸­é”šç‚¹é€‰æ‹©çš„å…³é”®ä½œç”¨ï¼Œè¯æ˜äº†åŠ¨æ€é”šç‚¹ç­–ç•¥ç›¸å¯¹äºå›ºå®šæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚SELECTæ¡†æ¶ä¸ºè§£å†³æ¦‚å¿µæ“¦é™¤ä¸­çš„æ•æ„Ÿæ€§é—®é¢˜æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºæœªæ¥ç›¸å…³ç ”ç©¶æä¾›äº†é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.


### [93] [Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence](https://arxiv.org/abs/2510.16555)
*Qiongyan Wang, Xingchen Zou, Yutian Jiang, Haomin Wen, Jiaheng Wei, Qingsong Wen, Yuxuan Liang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Urban-R1ï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡åœ°ç†åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å’ŒåŸå¸‚åŒºåŸŸç”»åƒä»»åŠ¡æ¥ç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŸå¸‚é€šç”¨æ™ºèƒ½ä¸­çš„åœ°ç†åè§é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†è·¨åŒºåŸŸæ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¿«é€ŸåŸå¸‚åŒ–åŠ å‰§äº†å¯¹åŸå¸‚é€šç”¨æ™ºèƒ½çš„éœ€æ±‚ï¼Œä½†ç°æœ‰åŸºäºç›‘ç£å¾®è°ƒçš„åŸå¸‚åŸºç¡€æ¨¡å‹å­˜åœ¨æŒç»­çš„åœ°ç†åè§é—®é¢˜ï¼Œå¯¼è‡´åŒºåŸŸé¢„æµ‹åå·®å’Œæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿç†è§£å¤æ‚åŸå¸‚ç¯å¢ƒçš„å…¬å¹³AIç³»ç»Ÿã€‚

**Method:** Urban-R1é‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ¡†æ¶ï¼Œä½¿ç”¨åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¥ä¼˜åŒ–è·¨åœ°ç†ç¾¤ä½“çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä»¥åŸå¸‚åŒºåŸŸç”»åƒä½œä¸ºä»£ç†ä»»åŠ¡ï¼Œä»å¤šæ¨¡æ€åŸå¸‚æ•°æ®ä¸­æä¾›å¯æµ‹é‡çš„å¥–åŠ±ä¿¡å·ã€‚

**Result:** è·¨å¤šä¸ªåŒºåŸŸå’Œä»»åŠ¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒUrban-R1æœ‰æ•ˆç¼“è§£äº†åœ°ç†åè§å¹¶æ˜¾è‘—æ”¹å–„äº†è·¨åŒºåŸŸæ³›åŒ–æ€§èƒ½ï¼Œåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºç›‘ç£å¾®è°ƒè®­ç»ƒæ¨¡å‹å’Œé—­æºæ¨¡å‹ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜å¼ºåŒ–å­¦ä¹ å¯¹é½æ˜¯å®ç°å…¬å¹³å¯ä¿¡åŸå¸‚æ™ºèƒ½çš„æœ‰å‰æ™¯è·¯å¾„ï¼Œä¸ºæ„å»ºæ— åœ°ç†åè§çš„åŸå¸‚AIç³»ç»Ÿæä¾›äº†é‡è¦æŠ€æœ¯æ¡†æ¶å’ŒéªŒè¯ã€‚

---

#### ğŸ“„ Abstract
Rapid urbanization intensifies the demand for Urban General Intelligence
(UGI), referring to AI systems that can understand and reason about complex
urban environments. Recent studies have built urban foundation models using
supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit
persistent geospatial bias, producing regionally skewed predictions and limited
generalization. To this end, we propose Urban-R1, a reinforcement
learning-based post-training framework that aligns MLLMs with the objectives of
UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize
reasoning across geographic groups and employs urban region profiling as a
proxy task to provide measurable rewards from multimodal urban data. Extensive
experiments across diverse regions and tasks show that Urban-R1 effectively
mitigates geo-bias and improves cross-region generalization, outperforming both
SFT-trained and closed-source models. Our results highlight reinforcement
learning alignment as a promising pathway toward equitable and trustworthy
urban intelligence.


### [94] [Ripple Effect Protocol: Coordinating Agent Populations](https://arxiv.org/abs/2510.16572)
*Ayush Chopra, Aman Sharma, Feroz Ahmad, Luca Muscariello, Vijoy Pandey, Ramesh Raskar*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Ripple Effect Protocol (REP)ï¼Œä¸€ç§åè°ƒåè®®ï¼Œé€šè¿‡è®©æ™ºèƒ½ä½“å…±äº«å†³ç­–å’Œè½»é‡çº§æ•æ„Ÿæ€§ä¿¡å·æ¥æ”¹å–„ç¾¤ä½“åè°ƒæ•ˆç‡ï¼Œç›¸æ¯”ä¼ ç»ŸA2Aåè®®åœ¨å¤šä¸ªé¢†åŸŸå®ç°äº†41%è‡³100%çš„åè°ƒå‡†ç¡®æ€§å’Œæ•ˆç‡æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°ä»£AIæ™ºèƒ½ä½“ä½¿ç”¨A2Aå’ŒACPç­‰åè®®è¿›è¡Œé€šä¿¡ï¼Œä½†è¿™äº›æœºåˆ¶å¼ºè°ƒé€šä¿¡è€Œéåè°ƒï¼Œéšç€æ™ºèƒ½ä½“ç¾¤ä½“è§„æ¨¡æ‰©å¤§ï¼Œè¿™ç§å±€é™æ€§å¯¼è‡´è„†å¼±çš„é›†ä½“è¡Œä¸ºï¼Œå³ä½¿ä¸ªä½“æ™ºèƒ½ä½“è¡¨ç°ä¼˜ç§€ä¹Ÿä¼šäº§ç”Ÿä¸è‰¯ç¾¤ä½“ç»“æœã€‚

**Method:** REPåè®®è®©æ™ºèƒ½ä½“ä¸ä»…å…±äº«å†³ç­–ï¼Œè¿˜å…±äº«è½»é‡çº§æ•æ„Ÿæ€§ä¿¡å·â€”â€”è¡¨è¾¾å…³é”®ç¯å¢ƒå˜é‡å˜åŒ–æ—¶å…¶é€‰æ‹©å°†å¦‚ä½•æ”¹å˜çš„ä¿¡å·ï¼Œè¿™äº›æ•æ„Ÿæ€§åœ¨å±€éƒ¨ç½‘ç»œä¸­ä¼ æ’­ï¼Œä½¿ç¾¤ä½“èƒ½å¤Ÿæ¯”å•ç‹¬ä½¿ç”¨æ™ºèƒ½ä½“ä¸­å¿ƒé€šä¿¡æ›´å¿«æ›´ç¨³å®šåœ°å¯¹é½ã€‚

**Result:** åœ¨ä¸‰ä¸ªé¢†åŸŸçš„åŸºå‡†æµ‹è¯•ä¸­ï¼šä¾›åº”é“¾çº§è”ï¼ˆå•¤é…’æ¸¸æˆï¼‰ã€ç¨€ç–ç½‘ç»œä¸­çš„åå¥½èšåˆï¼ˆç”µå½±è°ƒåº¦ï¼‰å’Œå¯æŒç»­èµ„æºåˆ†é…ï¼ˆæ¸”ä¸šé“¶è¡Œï¼‰ï¼ŒREPç›¸æ¯”A2Aåè®®å°†åè°ƒå‡†ç¡®æ€§å’Œæ•ˆç‡æé«˜äº†41%è‡³100%ï¼Œå¹¶èƒ½çµæ´»å¤„ç†æ¥è‡ªLLMçš„å¤šæ¨¡æ€æ•æ„Ÿæ€§ä¿¡å·ã€‚

**Conclusion:** é€šè¿‡å°†åè°ƒä½œä¸ºåè®®çº§èƒ½åŠ›ï¼ŒREPä¸ºæ–°å…´çš„æ™ºèƒ½ä½“äº’è”ç½‘æä¾›äº†å¯æ‰©å±•çš„åŸºç¡€è®¾æ–½ï¼Œä½¿æ™ºèƒ½ä½“ç¾¤ä½“èƒ½å¤Ÿå®ç°æ›´é«˜æ•ˆç¨³å®šçš„é›†ä½“å†³ç­–å’Œèµ„æºåˆ†é…ã€‚

---

#### ğŸ“„ Abstract
Modern AI agents can exchange messages using protocols such as A2A and ACP,
yet these mechanisms emphasize communication over coordination. As agent
populations grow, this limitation produces brittle collective behavior, where
individually smart agents converge on poor group outcomes. We introduce the
Ripple Effect Protocol (REP), a coordination protocol in which agents share not
only their decisions but also lightweight sensitivities - signals expressing
how their choices would change if key environmental variables shifted. These
sensitivities ripple through local networks, enabling groups to align faster
and more stably than with agent-centric communication alone. We formalize REP's
protocol specification, separating required message schemas from optional
aggregation rules, and evaluate it across scenarios with varying incentives and
network topologies. Benchmarks across three domains: (i) supply chain cascades
(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),
and (iii) sustainable resource allocation (Fishbanks) show that REP improves
coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly
handling multimodal sensitivity signals from LLMs. By making coordination a
protocol-level capability, REP provides scalable infrastructure for the
emerging Internet of Agents


### [95] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang, Xiying Huang, Danilo Bernardo, Jun-En Ding, Andrew Michael, Jingmei Yang, Patrick Kwan, Ashish Raj, Feng Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡ç»¼è¿°äº†å¤§è§„æ¨¡AIæ¨¡å‹åœ¨ç¥ç»ç§‘å­¦é¢†åŸŸçš„å˜é©æ€§å½±å“ï¼Œå±•ç¤ºäº†è¿™äº›æ¨¡å‹å¦‚ä½•é€šè¿‡ç«¯åˆ°ç«¯å­¦ä¹ ä»åŸå§‹è„‘ä¿¡å·ä¸­è§£å†³å¤šæ¨¡æ€ç¥ç»æ•°æ®æ•´åˆã€æ—¶ç©ºæ¨¡å¼è§£é‡Šç­‰å…³é”®è®¡ç®—ç¥ç»ç§‘å­¦æŒ‘æˆ˜ï¼Œå¹¶æ¢è®¨äº†ç¥ç»ç§‘å­¦ä¸AIä¹‹é—´çš„åŒå‘äº’åŠ¨å…³ç³»ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿè®¡ç®—æ–¹æ³•åœ¨å¤„ç†ç¥ç»ç§‘å­¦æ•°æ®æ—¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è§£å†³å¤šæ¨¡æ€ç¥ç»æ•°æ®æ•´åˆã€æ—¶ç©ºæ¨¡å¼è§£é‡Šä»¥åŠä¸´åºŠéƒ¨ç½²çš„è½¬åŒ–æ¡†æ¶ç­‰ä¸»è¦è®¡ç®—ç¥ç»ç§‘å­¦æŒ‘æˆ˜ï¼Œè€Œå¤§è§„æ¨¡AIæ¨¡å‹çš„å‡ºç°ä¸ºè¿™äº›æŒ‘æˆ˜æä¾›äº†æ–°çš„è§£å†³é€”å¾„ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨å¤§è§„æ¨¡AIæ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯å­¦ä¹ ï¼Œç›´æ¥ä»åŸå§‹è„‘ä¿¡å·å’Œç¥ç»æ•°æ®ä¸­æå–ç‰¹å¾ï¼Œå¹¶æ•´åˆç”Ÿç‰©å­¦å¯å‘çš„æ¶æ„çº¦æŸæ¥å¼€å‘æ›´å¯è§£é‡Šå’Œè®¡ç®—æ•ˆç‡æ›´é«˜çš„æ¨¡å‹ï¼Œè¦†ç›–ç¥ç»å½±åƒæ•°æ®å¤„ç†ã€è„‘æœºæ¥å£ã€åˆ†å­ç¥ç»ç§‘å­¦ç­‰å¤šä¸ªé¢†åŸŸã€‚

**Result:** å¤§è§„æ¨¡AIæ¨¡å‹åœ¨äº”ä¸ªä¸»è¦ç¥ç»ç§‘å­¦é¢†åŸŸå±•ç°å‡ºæ˜¾è‘—æ•ˆæœï¼šç¥ç»å½±åƒä¸æ•°æ®å¤„ç†ã€è„‘æœºæ¥å£ä¸ç¥ç»è§£ç ã€åˆ†å­ç¥ç»ç§‘å­¦ä¸åŸºå› ç»„å»ºæ¨¡ã€ä¸´åºŠè¾…åŠ©ä¸è½¬åŒ–æ¡†æ¶ï¼Œä»¥åŠè·¨ç¥ç»å’Œç²¾ç¥ç–¾ç—…çš„ç–¾ç—…ç‰¹å¼‚æ€§åº”ç”¨ï¼ŒæˆåŠŸè§£å†³äº†å¤šæ¨¡æ€æ•°æ®æ•´åˆå’Œæ—¶ç©ºæ¨¡å¼è§£é‡Šç­‰å…³é”®é—®é¢˜ã€‚

**Conclusion:** å¤§è§„æ¨¡AIæ¨¡å‹ä¸ºç¥ç»ç§‘å­¦ç ”ç©¶å¸¦æ¥äº†èŒƒå¼è½¬å˜ï¼Œä½†æˆåŠŸå®æ–½éœ€è¦å¼ºè°ƒä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ã€æœ‰æ•ˆçš„é¢†åŸŸçŸ¥è¯†æ•´åˆä»¥åŠå…¨é¢çš„ä¸´åºŠä½¿ç”¨ä¼¦ç†æŒ‡å—ï¼ŒåŒæ—¶ç¥ç»ç§‘å­¦ä¸AIçš„äº’åŠ¨æ—¥ç›ŠåŒå‘åŒ–ï¼Œç”Ÿç‰©å­¦çº¦æŸçš„æ•´åˆä¿ƒè¿›äº†æ›´å¯è§£é‡Šæ¨¡å‹çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.


### [96] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang, Peining Li, Meiyu Liang, Xu Hou, Junping Du, Yingxia Shao, Guanhua Ye, Wu Liu, Kangkang Lu, Yang Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºELMMæ–¹æ³•ï¼Œé€šè¿‡å¤šè§†è§’è§†è§‰ä»¤ç‰Œå‹ç¼©å™¨å’Œæ³¨æ„åŠ›å‰ªæç­–ç•¥ï¼Œè§£å†³å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±è¡¥å…¨ä¸­å›¾åƒä»¤ç‰Œå†—ä½™å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±å­˜åœ¨ä¸å®Œæ•´æ€§é—®é¢˜ï¼Œè€Œç°æœ‰æ–¹æ³•åœ¨åº”ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¡¥å…¨æ—¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå›¾åƒä»¤ç‰Œæ•°é‡è¿‡å¤šå¯¼è‡´è¯­ä¹‰å™ªå£°å’Œæ¨¡æ€å†²çªï¼Œä»¥åŠå¤„ç†å¤§é‡ä»¤ç‰Œè¾“å…¥å¸¦æ¥çš„é«˜è®¡ç®—æˆæœ¬ã€‚

**Method:** æå‡ºELMMæ¡†æ¶ï¼ŒåŒ…å«åŸºäºå¤šå¤´æ³¨æ„åŠ›çš„å¤šè§†è§’è§†è§‰ä»¤ç‰Œå‹ç¼©å™¨ï¼Œä»æ–‡æœ¬å’Œè§†è§‰è§†è§’è‡ªé€‚åº”å‹ç¼©å›¾åƒä»¤ç‰Œä»¥å‡å°‘å†—ä½™å¹¶é¿å…æ¨¡æ€å†²çªï¼›åŒæ—¶è®¾è®¡æ³¨æ„åŠ›å‰ªæç­–ç•¥ç§»é™¤å†—ä½™æ³¨æ„åŠ›å±‚ï¼Œå¹¶é€šè¿‡çº¿æ€§æŠ•å½±è¡¥å¿å‰ªæå¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚

**Result:** åœ¨FB15k-237-IMGå’ŒWN18-IMGåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒELMMåœ¨å®ç°æœ€å…ˆè¿›æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œå»ºç«‹äº†å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±è¡¥å…¨çš„æ–°èŒƒå¼ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡æœ‰æ•ˆçš„ä»¤ç‰Œå‹ç¼©å’Œæ¨¡å‹å‰ªæç­–ç•¥ï¼Œå¯ä»¥åœ¨ä¿æŒå¤šæ¨¡æ€çŸ¥è¯†å›¾è°±è¡¥å…¨æ€§èƒ½çš„åŒæ—¶å¤§å¹…é™ä½è®¡ç®—å¼€é”€ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„æ•ˆç‡ä¼˜åŒ–æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.


### [97] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang, Wenyi Yu, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Lu Lu, Chao Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ELLSAï¼Œè¿™æ˜¯é¦–ä¸ªå…¨åŒå·¥ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¶æ„ä¸­åŒæ—¶æ„ŸçŸ¥å’Œç”Ÿæˆè§†è§‰ã€æ–‡æœ¬ã€è¯­éŸ³å’ŒåŠ¨ä½œæ¨¡æ€ï¼Œå®ç°äº†æ›´è‡ªç„¶çš„äººæœºäº¤äº’è¡Œä¸ºã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** äººç±»äº¤äº’æœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€å’Œå…¨åŒå·¥çš„ï¼Œç°æœ‰æ¨¡å‹éš¾ä»¥åŒæ—¶å¤„ç†æ„ŸçŸ¥å’Œç”Ÿæˆä»»åŠ¡ï¼Œæ— æ³•å®ç°æµç•…çš„å¯¹è¯è½®æ¢å’Œä¸­æ–­ç­‰è‡ªç„¶äº¤äº’æ¨¡å¼ã€‚

**Method:** æå‡ºSA-MoEæ¶æ„ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æ··åˆä¸“å®¶æ¨¡å‹å°†å„æ¨¡æ€è·¯ç”±åˆ°ä¸“ç”¨ä¸“å®¶ï¼Œå¹¶é€šè¿‡ç»Ÿä¸€æ³¨æ„åŠ›éª¨å¹²ç½‘ç»œè¿›è¡Œèåˆï¼Œå®ç°è”åˆå¤šæ¨¡æ€æ„ŸçŸ¥å’Œå¹¶å‘ç”Ÿæˆã€‚

**Result:** åœ¨è¯­éŸ³äº¤äº’å’Œæœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­ï¼ŒELLSAè¾¾åˆ°æ¨¡æ€ä¸“ç”¨åŸºçº¿çš„æ€§èƒ½ï¼ŒåŒæ—¶æ”¯æŒé«˜çº§å¤šæ¨¡æ€å’Œå…¨åŒå·¥è¡Œä¸ºï¼ŒåŒ…æ‹¬å¯¹è¯è½®æ¢ã€ç¼ºé™·æŒ‡ä»¤æ‹’ç»ã€è¾¹è¯´è¾¹åšç­‰èƒ½åŠ›ã€‚

**Conclusion:** ELLSAä»£è¡¨äº†å‘æ›´è‡ªç„¶å’Œé€šç”¨äº¤äº’æ™ºèƒ½è¿ˆå‡ºçš„ä¸€æ­¥ï¼Œä¸ºäººå·¥é€šç”¨æ™ºèƒ½çš„è¿½æ±‚åšå‡ºè´¡çŒ®ï¼Œå…¶ç»Ÿä¸€æ¶æ„ä¸ºå¤šæ¨¡æ€ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.


### [98] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han, Yukun Cao, Zezhong Ding, Zengyi Gao, S Kevin Zhou, Xike Xie*

#### ğŸ§© TL;DR
GraphVistaæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚å›¾ä¿¡æ¯ç»„ç»‡å’Œæ¨¡æ€åè°ƒè§„åˆ’ä»£ç†ï¼Œè§£å†³äº†å›¾ç†è§£ä¸­çš„å¯æ‰©å±•æ€§å’Œå¤šæ¨¡æ€åè°ƒé—®é¢˜ï¼Œèƒ½å¤Ÿå¤„ç†æ¯”ç°æœ‰åŸºå‡†å¤§200å€çš„å›¾å¹¶å®ç°4.4å€çš„æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾ç†è§£ä»»åŠ¡ä¸­å­˜åœ¨è¾“å…¥ä»¤ç‰Œé™åˆ¶å¯¼è‡´çš„å¯æ‰©å±•æ€§ç“¶é¢ˆï¼Œä»¥åŠç¼ºä¹æœ‰æ•ˆçš„æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€åè°ƒæœºåˆ¶ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡å›¾æ•°æ®ä¸Šçš„åº”ç”¨æ•ˆæœã€‚

**Method:** GraphVistaé‡‡ç”¨åˆ†å±‚å›¾ä¿¡æ¯ç»„ç»‡æ–¹æ³•æ„å»ºè½»é‡çº§GraphRAGåŸºç¡€ï¼Œä»…æ£€ç´¢ä»»åŠ¡ç›¸å…³çš„æ–‡æœ¬æè¿°å’Œé«˜åˆ†è¾¨ç‡è§†è§‰å­å›¾ï¼ŒåŒæ—¶å¼•å…¥è§„åˆ’ä»£ç†æ ¹æ®ä»»åŠ¡å¤æ‚åº¦å°†æ¨ç†è·¯ç”±åˆ°æœ€é€‚åˆçš„æ¨¡æ€ï¼šç®€å•å±æ€§æ¨ç†ä½¿ç”¨æ–‡æœ¬æ¨¡æ€ï¼Œå±€éƒ¨å’Œç»“æ„å¤æ‚æ¨ç†ä½¿ç”¨åŸºäºæ˜¾å¼æ‹“æ‰‘çš„è§†è§‰æ¨¡æ€ã€‚

**Result:** å®éªŒè¡¨æ˜GraphVistaèƒ½å¤Ÿæ‰©å±•åˆ°æ¯”ç°æœ‰åŸºå‡†å¤§200å€çš„å¤§å‹å›¾ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºç°æœ‰çš„æ–‡æœ¬ã€è§†è§‰å’Œèåˆæ–¹æ³•ï¼Œç›¸æ¯”æœ€å…ˆè¿›åŸºçº¿å®ç°äº†é«˜è¾¾4.4å€çš„è´¨é‡æå‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡åˆ†å±‚ä¿¡æ¯å‹ç¼©å’Œæ™ºèƒ½æ¨¡æ€è·¯ç”±å¯ä»¥æœ‰æ•ˆè§£å†³å›¾ç†è§£ä¸­çš„å¯æ‰©å±•æ€§å’Œæ¨¡æ€åè°ƒæŒ‘æˆ˜ï¼Œå……åˆ†åˆ©ç”¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€çš„äº’è¡¥ä¼˜åŠ¿ä¸ºå¤§è§„æ¨¡å›¾åˆ†ææä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.


### [99] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang, Pingyue Zhang, Zihan Wang, Yaning Gao, Linjie Li, Qineng Wang, Hanyang Chen, Chi Wan, Yiping Lu, Zhengyuan Yang, Lijuan Wang, Ranjay Krishna, Jiajun Wu, Li Fei-Fei, Yejin Choi, Manling Li*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¶æ„æ€§åœ°å¼ºåˆ¶å’Œå¥–åŠ±è§†è§‰è¯­è¨€æ¨¡å‹ä»£ç†è¿›è¡Œæ˜¾å¼è§†è§‰çŠ¶æ€æ¨ç†ï¼Œæ„å»ºå†…éƒ¨ä¸–ç•Œæ¨¡å‹ï¼Œåœ¨äº”ä¸ªå¤šæ ·åŒ–ä»£ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†3å€æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†GPT-5ã€Gemini 2.5 Proå’ŒClaude 4.5ç­‰ä¸“æœ‰æ¨ç†æ¨¡å‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ä»£ç†é¢ä¸´çš„å…³é”®æŒ‘æˆ˜æ˜¯ä»æ–‡æœ¬çŠ¶æ€å‘å¤æ‚è§†è§‰è§‚å¯Ÿçš„è½¬å˜ï¼Œè¿™å¼•å…¥äº†éƒ¨åˆ†å¯è§‚æµ‹æ€§å¹¶éœ€è¦å¼ºå¤§çš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ï¼Œç ”ç©¶æ—¨åœ¨æ¢ç´¢VLMä»£ç†æ˜¯å¦èƒ½å¤Ÿé€šè¿‡æ˜¾å¼è§†è§‰çŠ¶æ€æ¨ç†æ„å»ºå†…éƒ¨ä¸–ç•Œæ¨¡å‹ã€‚

**Method:** å°†ä»£ç†æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸ºéƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå°†æ¨ç†åˆ†è§£ä¸ºçŠ¶æ€ä¼°è®¡å’Œè½¬ç§»å»ºæ¨¡ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œè®¾è®¡äº†ä¸–ç•Œå»ºæ¨¡å¥–åŠ±æä¾›å¯†é›†çš„å›åˆçº§ç›‘ç£ï¼Œå¹¶å¼•å…¥äº†åŒå±‚é€šç”¨ä¼˜åŠ¿ä¼°è®¡è¿›è¡Œå›åˆæ„ŸçŸ¥çš„ä¿¡ç”¨åˆ†é…ã€‚

**Result:** é€šè¿‡è§†è§‰çŠ¶æ€æ¨ç†ï¼Œä¸€ä¸ª30äº¿å‚æ•°çš„æ¨¡å‹åœ¨äº”ä¸ªå¤šæ ·åŒ–ä»£ç†åŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†0.82çš„åˆ†æ•°ï¼Œç›¸æ¯”æœªè®­ç»ƒå¯¹åº”æ¨¡å‹ï¼ˆ0.21ï¼‰å®ç°äº†3å€æ”¹è¿›ï¼Œè¶…è¶Šäº†GPT-5ï¼ˆ0.75ï¼‰ã€Gemini 2.5 Proï¼ˆ0.67ï¼‰å’ŒClaude 4.5ï¼ˆ0.62ï¼‰ç­‰ä¸“æœ‰æ¨ç†æ¨¡å‹ã€‚

**Conclusion:** ç ”ç©¶å‘ç°ä»£ç†è¡¨ç¤ºå†…éƒ¨ä¿¡å¿µçš„æœ€ä½³æ–¹å¼æ˜¯ä»»åŠ¡ä¾èµ–çš„ï¼šè‡ªç„¶è¯­è¨€åœ¨é€šç”¨ä»»åŠ¡ä¸­æ“…é•¿æ•æ‰è¯­ä¹‰å…³ç³»ï¼Œè€Œç»“æ„åŒ–æ ¼å¼å¯¹äºç²¾ç¡®æ“ä½œå’Œæ§åˆ¶ä¸å¯æˆ–ç¼ºï¼Œè¿™äº›å‘ç°ä¸ºVLMä»£ç†çš„ä¸–ç•Œå»ºæ¨¡æä¾›äº†é‡è¦è®¾è®¡åŸåˆ™ã€‚

---

#### ğŸ“„ Abstract
A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.


### [100] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad, Yingru Xu, Liang Zhao, Wenbo Yan, Narendra Gyanchandani*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ToolCriticè¯Šæ–­æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å’Œæ”¹è¿›å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å·¥å…·å¢å¼ºå¯¹è¯ä¸­çš„è¡Œä¸ºã€‚è¯¥æ¡†æ¶é€šè¿‡æ£€æµ‹å…«ç§ç‰¹å®šå·¥å…·è°ƒç”¨é”™è¯¯å¹¶æä¾›é’ˆå¯¹æ€§åé¦ˆï¼Œåœ¨SGDæ•°æ®é›†ä¸Šå°†å·¥å…·è°ƒç”¨å‡†ç¡®ç‡æå‡äº†13%ï¼Œæ˜¾è‘—å¢å¼ºäº†LLMä¸å¤–éƒ¨å·¥å…·é›†æˆçš„é²æ£’æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å·¥å…·å¢å¼ºçš„å¤§è¯­è¨€æ¨¡å‹åœ¨ç°å®åº”ç”¨ä¸­æ—¥ç›Šæ™®åŠï¼Œä½†å·¥å…·ä½¿ç”¨é”™è¯¯ä»ç„¶é˜»ç¢å…¶å¯é æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨æ£€æµ‹å’Œçº æ­£å·¥å…·è°ƒç”¨è¿‡ç¨‹ä¸­çš„ç‰¹å®šé”™è¯¯ç±»å‹æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè½®å¯¹è¯åœºæ™¯ä¸­ï¼Œéœ€è¦ç³»ç»ŸåŒ–çš„è¯Šæ–­æ¡†æ¶æ¥æå‡LLMä¸å¤–éƒ¨å·¥å…·äº¤äº’çš„é²æ£’æ€§ã€‚

**Method:** ToolCriticæ¡†æ¶ç³»ç»Ÿæ€§åœ°å®šä¹‰äº†å…«ç§å·¥å…·è°ƒç”¨é”™è¯¯ç±»å‹ï¼ŒåŒ…æ‹¬è¿‡æ—©è°ƒç”¨ã€å‚æ•°ä¸å¯¹é½å’Œå·¥å…·è¾“å‡ºè¯¯è§£ç­‰ï¼Œå¹¶æ„å»ºåˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¡†æ¶é€šè¿‡æ£€æµ‹è¿™äº›ç‰¹å®šé”™è¯¯å¹¶æä¾›é’ˆå¯¹æ€§åé¦ˆï¼Œä½¿å…·æœ‰å¼ºå¤§æ¨ç†èƒ½åŠ›çš„ä¸»LLMèƒ½å¤ŸåŸºäºåé¦ˆä¿®æ­£å…¶å“åº”ã€‚

**Result:** åœ¨Schema-Guided Dialogueæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒToolCriticç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼ˆåŒ…æ‹¬é›¶æ ·æœ¬æç¤ºå’Œè‡ªæ ¡æ­£æŠ€æœ¯ï¼‰å°†å·¥å…·è°ƒç”¨å‡†ç¡®ç‡æå‡äº†é«˜è¾¾13%ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆå‡å°‘äº†å·¥å…·è°ƒç”¨è¿‡ç¨‹ä¸­çš„å„ç±»é”™è¯¯ï¼Œæ˜¾è‘—æ”¹å–„äº†å¤šè½®å·¥å…·å¢å¼ºå¯¹è¯çš„æ€§èƒ½ã€‚

**Conclusion:** ToolCriticä»£è¡¨äº†å‘æ›´é²æ£’çš„LLMä¸å¤–éƒ¨å·¥å…·é›†æˆè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ï¼Œä¸ºç°å®å¯¹è¯åº”ç”¨æä¾›äº†å¯é çš„è¯Šæ–­å’Œæ ¡æ­£æœºåˆ¶ã€‚è¯¥æ¡†æ¶çš„ç³»ç»ŸåŒ–é”™è¯¯åˆ†ç±»å’Œåé¦ˆæœºåˆ¶ä¸ºæœªæ¥å·¥å…·å¢å¼ºLLMçš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒæ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè½®äº¤äº’åœºæ™¯çš„å¯é æ€§æå‡æ–¹é¢ã€‚

---

#### ğŸ“„ Abstract
Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.


### [101] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil, Sharad Duwal, Abhishek Tyagi, Adiba Mahbub Proma*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MIRAGEï¼Œä¸€ç§æ¨ç†æ—¶ã€æ¨¡å‹å¯æ’æ‹”çš„ä»£ç†æ¡†æ¶ï¼Œé€šè¿‡å°†å¤šæ¨¡æ€éªŒè¯åˆ†è§£ä¸ºå››ä¸ªé¡ºåºæ¨¡å—æ¥æ£€æµ‹ç½‘ç»œè™šå‡ä¿¡æ¯ï¼Œåœ¨MMFakeBenchéªŒè¯é›†ä¸Šè¾¾åˆ°81.65% F1åˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºæœ€å¼ºé›¶æ ·æœ¬åŸºçº¿7.65ä¸ªç‚¹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç½‘ç»œå¹³å°ä¸Šæ¯å¤©é€šè¿‡æ•°åäº¿ç»“åˆæ–‡æœ¬å’Œå›¾åƒçš„å¤šæ¨¡æ€å¸–å­ä¼ æ’­è™šå‡ä¿¡æ¯ï¼Œè¶…å‡ºäº†äººå·¥äº‹å®æ ¸æŸ¥èƒ½åŠ›ï¼Œè€Œç›‘ç£æ£€æµ‹æ¨¡å‹éœ€è¦é¢†åŸŸç‰¹å®šçš„è®­ç»ƒæ•°æ®ä¸”æ— æ³•æ³›åŒ–åˆ°ä¸åŒçš„æ“çºµç­–ç•¥ã€‚

**Method:** MIRAGEæ¡†æ¶å°†å¤šæ¨¡æ€éªŒè¯åˆ†è§£ä¸ºå››ä¸ªé¡ºåºæ¨¡å—ï¼šè§†è§‰çœŸå®æ€§è¯„ä¼°æ£€æµ‹AIç”Ÿæˆå›¾åƒï¼Œè·¨æ¨¡æ€ä¸€è‡´æ€§åˆ†æè¯†åˆ«ä¸Šä¸‹æ–‡ä¸å½“é‡ç”¨é€”ï¼Œæ£€ç´¢å¢å¼ºçš„äº‹å®æ£€æŸ¥é€šè¿‡è¿­ä»£é—®é¢˜ç”Ÿæˆå°†å£°æ˜åŸºäºç½‘ç»œè¯æ®ï¼Œæ ¡å‡†åˆ¤æ–­æ¨¡å—æ•´åˆæ‰€æœ‰ä¿¡å·ï¼Œåè°ƒè§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†ä¸å®šå‘ç½‘ç»œæ£€ç´¢ï¼Œè¾“å‡ºç»“æ„åŒ–ä¸”å¸¦å¼•ç”¨çš„æ¨ç†è¿‡ç¨‹ã€‚

**Result:** åœ¨MMFakeBenchéªŒè¯é›†ï¼ˆ1,000æ ·æœ¬ï¼‰ä¸Šï¼ŒMIRAGEä¸GPT-4o-miniç»„åˆè¾¾åˆ°81.65% F1åˆ†æ•°å’Œ75.1%å‡†ç¡®ç‡ï¼Œä¼˜äºæœ€å¼ºé›¶æ ·æœ¬åŸºçº¿ï¼ˆGPT-4Vä¸MMD-Agentçš„74.0% F1ï¼‰7.65ä¸ªç‚¹ï¼ŒåŒæ—¶ä¿æŒ34.3%çš„è¯¯æŠ¥ç‡ï¼Œè€Œä»…åˆ¤æ–­åŸºçº¿çš„è¯¯æŠ¥ç‡ä¸º97.3%ï¼›æµ‹è¯•é›†ç»“æœï¼ˆ5,000æ ·æœ¬ï¼‰ç¡®è®¤æ³›åŒ–èƒ½åŠ›ï¼Œè¾¾åˆ°81.44% F1å’Œ75.08%å‡†ç¡®ç‡ï¼›æ¶ˆèç ”ç©¶æ˜¾ç¤ºè§†è§‰éªŒè¯è´¡çŒ®5.18 F1ç‚¹ï¼Œæ£€ç´¢å¢å¼ºæ¨ç†è´¡çŒ®2.97 F1ç‚¹ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“åˆç½‘ç»œæ£€ç´¢çš„åˆ†è§£ä»£ç†æ¨ç†å¯ä»¥åœ¨æ²¡æœ‰é¢†åŸŸç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹åŒ¹é…ç›‘ç£æ£€æµ‹å™¨æ€§èƒ½ï¼Œåœ¨æ ‡è®°æ•°æ®ç¨€ç¼ºçš„å¤šæ¨¡æ€åœºæ™¯ä¸­å®ç°è™šå‡ä¿¡æ¯æ£€æµ‹ï¼Œä¸ºåº”å¯¹å¤§è§„æ¨¡å¤šæ¨¡æ€è™šå‡ä¿¡æ¯ä¼ æ’­æä¾›äº†æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.


### [102] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu, Ziyi Chen, Hui Liu, Chen Luo, Xianfeng Tang, Suhang Wang, Joy Zeng, Zhenwei Dai, Zhan Shi, Tianxin Wei, Benoit Dumoulin, Hanghang Tong*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æ­ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹å­˜åœ¨'çœ‹è§ä½†ä¸ç›¸ä¿¡'ç°è±¡ï¼Œå³æ¨¡å‹åœ¨è¾“å‡ºé”™è¯¯ç­”æ¡ˆæ—¶ä»èƒ½æ„ŸçŸ¥è§†è§‰è¯æ®ã€‚æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨ç†æ—¶å¹²é¢„æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§æ³¨æ„åŠ›æ©ç æ˜¾è‘—æå‡äº†å¤šä¸ªVLMå®¶æ—çš„å‡†ç¡®æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å³ä½¿å­˜åœ¨æ­£ç¡®çš„è§†è§‰è¯æ®æ—¶ä»ä¼šå¤±è´¥ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ç³»ç»Ÿæ€§åœ°æ¢ç©¶è¿™äº›å¤±è´¥æ˜¯ç”±äºæœªèƒ½æ„ŸçŸ¥è¯æ®è¿˜æ˜¯æœªèƒ½æœ‰æ•ˆåˆ©ç”¨è¯æ®ï¼Œä»¥ç†è§£VLMå†…éƒ¨æ„ŸçŸ¥ä¸æ¨ç†ä¹‹é—´çš„å·®è·ã€‚

**Method:** é€šè¿‡åˆ†æå±‚é—´æ³¨æ„åŠ›åŠ¨æ€ï¼Œå‘ç°æµ…å±‚ä¸»è¦å…³æ³¨æ–‡æœ¬è€Œæ·±å±‚ç¨€ç–ä½†å¯é åœ°å…³æ³¨å±€éƒ¨è¯æ®åŒºåŸŸã€‚åŸºäºæ­¤æå‡ºæ¨ç†æ—¶å¹²é¢„æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§æ³¨æ„åŠ›æ©ç çªå‡ºæ·±å±‚è¯æ®åŒºåŸŸï¼Œè¯¥æ–¹æ³•æ— éœ€è®­ç»ƒå³å¯åº”ç”¨ã€‚

**Result:** ç ”ç©¶å‘ç°VLMsåœ¨è¾“å‡ºé”™è¯¯ç­”æ¡ˆæ—¶é€šå¸¸ä»èƒ½æ„ŸçŸ¥è§†è§‰è¯æ®ï¼Œè¿™ç§ç°è±¡å¹¿æ³›å­˜åœ¨äºä¸»è¦VLMå®¶æ—ä¸­ã€‚æå‡ºçš„å¹²é¢„æ–¹æ³•åœ¨LLaVAã€Qwenã€Gemmaå’ŒInternVLç­‰å¤šä¸ªæ¨¡å‹ä¸Šä¸€è‡´æå‡äº†å‡†ç¡®æ€§ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜VLMså†…éƒ¨ç¼–ç äº†å¯é çš„è¯æ®ä½†æœªèƒ½å……åˆ†åˆ©ç”¨ï¼Œé€šè¿‡ä½¿è¿™äº›ä¿¡å·æ˜¾å¼åŒ–å¯ä»¥å¼¥åˆæ„ŸçŸ¥ä¸æ¨ç†ä¹‹é—´çš„å·®è·ã€‚è¿™ä¸€å‘ç°æ¨è¿›äº†å¯¹VLMçš„è¯Šæ–­ç†è§£ï¼Œä¸ºæé«˜æ¨¡å‹å¯é æ€§æä¾›äº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.
