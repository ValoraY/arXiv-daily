<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-01-08.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 30]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 8]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 2]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-hyperclova-x-32b-think">[1] <a href="https://arxiv.org/abs/2601.03286">HyperCLOVA X 32B Think</a></h3>
<p><em>NAVER Cloud HyperCLOVA X Team</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†HyperCLOVA X 32B Thinkï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹éŸ©è¯­è¯­è¨€æ–‡åŒ–ç¯å¢ƒè®¾è®¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«å¼ºè°ƒæ¨ç†èƒ½åŠ›å’Œæ™ºèƒ½ä½“èƒ½åŠ›ï¼Œå¹¶åœ¨å¼€æºåæ—¨åœ¨ä¿ƒè¿›å­¦æœ¯å’Œå·¥ä¸šç•Œçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨éŸ©è¯­è¯­è¨€æ–‡åŒ–ç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ä¸è¶³é—®é¢˜ï¼Œä»¥åŠç¼ºä¹ä¸“é—¨é’ˆå¯¹éŸ©è¯­ç¯å¢ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œæ™ºèƒ½ä½“èƒ½åŠ›æ”¯æŒçš„ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼šé¦–å…ˆè¿›è¡Œä»¥æ¨ç†èƒ½åŠ›ä¸ºé‡ç‚¹çš„é¢„è®­ç»ƒï¼Œéšåè¿›è¡Œåè®­ç»ƒä»¥æ”¯æŒå¤šæ¨¡æ€ç†è§£ã€å¢å¼ºæ¨ç†ã€æ™ºèƒ½ä½“è¡Œä¸ºå’Œäººç±»åå¥½å¯¹é½ï¼Œä¸“é—¨é’ˆå¯¹éŸ©è¯­è¯­è¨€æ–‡åŒ–ç¯å¢ƒè¿›è¡Œä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸åŒç±»è§„æ¨¡æ¨¡å‹ç›¸æ¯”ï¼ŒHyperCLOVA X 32B Thinkåœ¨éŸ©è¯­æ–‡æœ¬åˆ°æ–‡æœ¬å’Œè§†è§‰åˆ°æ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨é¢å‘æ™ºèƒ½ä½“çš„è¯„ä¼°ä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†å¼ºåŠ²æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†ä¸“é—¨é’ˆå¯¹ç‰¹å®šè¯­è¨€æ–‡åŒ–ç¯å¢ƒå®šåˆ¶è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œé€šè¿‡å¼€æºæ¨¡å‹æ”¯æŒæ›´å¹¿æ³›çš„é‡‡ç”¨ï¼Œå¹¶ä¸ºå­¦æœ¯å’Œå·¥ä¸šç•Œåœ¨å¤šæ¨¡æ€æ¨ç†å’Œæ™ºèƒ½ä½“èƒ½åŠ›æ–¹é¢çš„ç ”ç©¶åˆ›æ–°æä¾›äº†é‡è¦èµ„æºã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>In this report, we present HyperCLOVA X 32B Think, a vision-language model designed with particular emphasis on reasoning within the Korean linguistic and cultural context, as well as agentic ability. HyperCLOVA X 32B Think is pre-trained with a strong focus on reasoning capabilities and subsequently post-trained to support multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences. Experimental evaluations against comparably sized models demonstrate that our model achieves strong performance on Korean text-to-text and vision-to-text benchmarks, as well as on agent-oriented evaluation tasks. By open-sourcing HyperCLOVA X 32B Think, we aim to support broader adoption and facilitate further research and innovation across both academic and industrial communities.</p>
<h3 id="2-vlm4vla-revisiting-vision-language-models-in-vision-language-action-models">[2] <a href="https://arxiv.org/abs/2601.03309">VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models</a></h3>
<p><em>Jianke Zhang, Xiaoyu Chen, Qiuyue Wang, Mingsheng Li, Yanjiang Guo, Yucheng Hu, Jiajun Zhang, Shuai Bai, Junyang Lin, Jianyu Chen</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡é€šè¿‡ç³»ç»Ÿç ”ç©¶è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€‰æ‹©ä¸èƒ½åŠ›å¦‚ä½•å½±å“ä¸‹æ¸¸è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥æ€§èƒ½ï¼ŒæŒ‘æˆ˜äº†å…³äºVLMé€šç”¨èƒ½åŠ›å¯é¢„æµ‹ä¸‹æ¸¸æ§åˆ¶æ€§èƒ½çš„å¸¸è§å‡è®¾ï¼Œå¹¶å‘ç°è§†è§‰æ¨¡å—æ˜¯ä¸»è¦æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºé€šè¿‡æ³¨å…¥æ§åˆ¶ç›¸å…³ç›‘ç£å¯æå‡æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ä¸€ä¸ªåŸºç¡€ä½†é²œæœ‰ç³»ç»Ÿç ”ç©¶çš„é—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é€‰æ‹©å’Œèƒ½åŠ›å¦‚ä½•è½¬åŒ–ä¸ºä¸‹æ¸¸è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥çš„æ€§èƒ½ã€‚å½“å‰ç ”ç©¶ç¼ºä¹å¯¹VLMé€šç”¨èƒ½åŠ›ä¸å…·ä½“å…·èº«æ§åˆ¶ä»»åŠ¡æ€§èƒ½ä¹‹é—´å…³ç³»çš„æ·±å…¥ç†è§£ï¼Œéœ€è¦æ¢ç©¶æ ‡å‡†VLMèƒ½åŠ›æ˜¯å¦è¶³ä»¥æ”¯æŒæœ‰æ•ˆçš„å…·èº«æ§åˆ¶ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶è€…æå‡ºäº†VLM4VLAï¼Œè¿™æ˜¯ä¸€ä¸ªæœ€å°åŒ–é€‚åº”ç®¡é“ï¼Œä»…ä½¿ç”¨å°‘é‡æ–°çš„å¯å­¦ä¹ å‚æ•°å°†é€šç”¨VLMè½¬æ¢ä¸ºVLAç­–ç•¥ï¼Œä»¥å®ç°å…¬å¹³é«˜æ•ˆçš„æ¯”è¾ƒã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¹¿æ³›å®è¯ç ”ç©¶ï¼Œå¹¶è¿›ä¸€æ­¥é€šè¿‡åœ¨ä¸ƒä¸ªè¾…åŠ©å…·èº«ä»»åŠ¡ä¸Šå¾®è°ƒVLMæ¥æ¢ç©¶å…·ä½“å…·èº«èƒ½åŠ›çš„å½±å“ï¼ŒåŒæ—¶è¿›è¡Œæ¨¡æ€çº§æ¶ˆèå®éªŒä»¥è¯†åˆ«æ€§èƒ½ç“¶é¢ˆã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°ï¼Œå°½ç®¡VLMåˆå§‹åŒ–ç›¸æ¯”ä»å¤´è®­ç»ƒå§‹ç»ˆå…·æœ‰ä¼˜åŠ¿ï¼Œä½†VLMçš„é€šç”¨èƒ½åŠ›æ˜¯ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„è¾ƒå·®é¢„æµ‹æŒ‡æ ‡ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæå‡VLMåœ¨ç‰¹å®šå…·èº«æŠ€èƒ½ä¸Šçš„è¡¨ç°å¹¶ä¸èƒ½ä¿è¯æ›´å¥½çš„ä¸‹æ¸¸æ§åˆ¶æ€§èƒ½ã€‚æ¨¡æ€çº§æ¶ˆèå®éªŒç¡®å®šVLMä¸­çš„è§†è§‰æ¨¡å—è€Œéè¯­è¨€ç»„ä»¶æ˜¯ä¸»è¦æ€§èƒ½ç“¶é¢ˆï¼Œå‘è§†è§‰ç¼–ç å™¨æ³¨å…¥æ§åˆ¶ç›¸å…³ç›‘ç£å³ä½¿åœ¨ä¸‹æ¸¸å¾®è°ƒæœŸé—´ç¼–ç å™¨ä¿æŒå†»ç»“ä¹Ÿèƒ½å¸¦æ¥æŒç»­å¢ç›Šã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æŒ‘æˆ˜äº†å…³äºVLMé€šç”¨èƒ½åŠ›å¯é¢„æµ‹ä¸‹æ¸¸æ§åˆ¶æ€§èƒ½çš„å¸¸è§å‡è®¾ï¼Œè¡¨æ˜æ ‡å‡†VLMèƒ½åŠ›å¯¹äºæœ‰æ•ˆå…·èº«æ§åˆ¶æ˜¯å¿…è¦ä½†ä¸å……åˆ†çš„ã€‚ç ”ç©¶æ­ç¤ºäº†å½“å‰VLMé¢„è®­ç»ƒç›®æ ‡ä¸å…·èº«åŠ¨ä½œè§„åˆ’éœ€æ±‚ä¹‹é—´å­˜åœ¨æŒç»­çš„é¢†åŸŸå·®è·ï¼Œå¹¶è¯æ˜é€šè¿‡å‘è§†è§‰ç¼–ç å™¨æ³¨å…¥æ§åˆ¶ç›¸å…³ç›‘ç£å¯ä»¥ç¼“è§£è¿™ä¸€å·®è·ï¼Œä¸ºæœªæ¥VLAæ¨¡å‹è®¾è®¡æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Vision-Language-Action (VLA) models, which integrate pretrained large Vision-Language Models (VLM) into their policy backbone, are gaining significant attention for their promising generalization capabilities. This paper revisits a fundamental yet seldom systematically studied question: how VLM choice and competence translate to downstream VLA policies performance? We introduce VLM4VLA, a minimal adaptation pipeline that converts general-purpose VLMs into VLA policies using only a small set of new learnable parameters for fair and efficient comparison. Despite its simplicity, VLM4VLA proves surprisingly competitive with more sophisticated network designs. Through extensive empirical studies on various downstream tasks across three benchmarks, we find that while VLM initialization offers a consistent benefit over training from scratch, a VLM's general capabilities are poor predictors of its downstream task performance. This challenges common assumptions, indicating that standard VLM competence is necessary but insufficient for effective embodied control. We further investigate the impact of specific embodied capabilities by fine-tuning VLMs on seven auxiliary embodied tasks (e.g., embodied QA, visual pointing, depth estimation). Contrary to intuition, improving a VLM's performance on specific embodied skills does not guarantee better downstream control performance. Finally, modality-level ablations identify the visual module in VLM, rather than the language component, as the primary performance bottleneck. We demonstrate that injecting control-relevant supervision into the vision encoder of the VLM yields consistent gains, even when the encoder remains frozen during downstream fine-tuning. This isolates a persistent domain gap between current VLM pretraining objectives and the requirements of embodied action-planning.</p>
<h3 id="3-mmerror-a-benchmark-for-erroneous-reasoning-in-vision-language-models">[3] <a href="https://arxiv.org/abs/2601.03331">MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models</a></h3>
<p><em>Yang Shi, Yifeng Xie, Minzhe Guo, Liangsi Lu, Mingxuan Huang, Jingchao Wang, Zhihong Zhu, Boyan Xu, Zhiqi Huang</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MMErroRåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2,013ä¸ªæ ·æœ¬çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹æ£€æµ‹å’Œåˆ†ç±»æ¨ç†é”™è¯¯çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†å½“å‰å…ˆè¿›æ¨¡å‹åœ¨è¯†åˆ«é”™è¯¯æ¨ç†æ–¹é¢ä»é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ€§èƒ½æå‡ï¼Œç ”ç©¶å…³æ³¨è¿™äº›æ¨¡å‹æ˜¯å¦çœŸæ­£ç†è§£å…¶å¤„ç†çš„å†…å®¹ï¼Œç‰¹åˆ«æ˜¯èƒ½å¦æ£€æµ‹æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯å¹¶è¯†åˆ«é”™è¯¯ç±»å‹ï¼Œç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨ç­”æ¡ˆæ­£ç¡®æ€§è€Œéè¿‡ç¨‹å±‚é¢çš„é”™è¯¯ä¸­å¿ƒè¯„ä¼°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†MMErroRå¤šæ¨¡æ€åŸºå‡†ï¼ŒåŒ…å«2,013ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åµŒå…¥å•ä¸€è¿è´¯çš„æ¨ç†é”™è¯¯ï¼Œæ¶µç›–å…­ä¸ªé¡¶çº§é¢†åŸŸä¸‹çš„24ä¸ªå­é¢†åŸŸï¼Œé‡‡ç”¨è¿‡ç¨‹å±‚é¢çš„é”™è¯¯ä¸­å¿ƒè¯„ä¼°æ–¹æ³•ï¼Œè¦æ±‚æ¨¡å‹åœ¨è§†è§‰å’Œè¯­è¨€ä¸Šä¸‹æ–‡ä¸­æ£€æµ‹é”™è¯¯æ¨ç†å¹¶è¿›è¡Œé”™è¯¯ç±»å‹åˆ†ç±»ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°äº†20ä¸ªå…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæœ€ä½³æ¨¡å‹Gemini-3.0-Proçš„é”™è¯¯åˆ†ç±»å‡†ç¡®ç‡ä»…ä¸º66.47%ï¼Œè¡¨æ˜è¯†åˆ«é”™è¯¯æ¨ç†å…·æœ‰æ˜¾è‘—æŒ‘æˆ˜æ€§ï¼ŒåŸºå‡†çš„å¹¿æ³›è¦†ç›–å’Œåˆ†ç±»ä¸°å¯Œæ€§ç¡®ä¿äº†è¯„ä¼°çš„å…¨é¢æ€§å’Œä»£è¡¨æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹å’Œåˆ†ç±»æ¨ç†é”™è¯¯æ–¹é¢çš„å±€é™æ€§ï¼Œé”™è¯¯è¯†åˆ«èƒ½åŠ›ä¸ºç†è§£å¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„çœŸå®èƒ½åŠ›æä¾›äº†å®è´µè§è§£ï¼ŒåŸºå‡†æµ‹è¯•ä¸ºæœªæ¥æ¨¡å‹è¯„ä¼°å’Œå¼€å‘æä¾›äº†é‡è¦å·¥å…·å’Œæ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Recent advances in Vision-Language Models (VLMs) have improved performance in multi-modal learning, raising the question of whether these models truly understand the content they process. Crucially, can VLMs detect when a reasoning process is wrong and identify its error type? To answer this, we present MMErroR, a multi-modal benchmark of 2,013 samples, each embedding a single coherent reasoning error. These samples span 24 subdomains across six top-level domains, ensuring broad coverage and taxonomic richness. Unlike existing benchmarks that focus on answer correctness, MMErroR targets a process-level, error-centric evaluation that requires models to detect incorrect reasoning and classify the error type within both visual and linguistic contexts. We evaluate 20 advanced VLMs, even the best model (Gemini-3.0-Pro) classifies the error in only 66.47\% of cases, underscoring the challenge of identifying erroneous reasoning. Furthermore, the ability to accurately identify errors offers valuable insights into the capabilities of multi-modal reasoning models. Project Page: https://mmerror-benchmark.github.io</p>
<h3 id="4-riskcuebench-benchmarking-anticipatory-reasoning-from-early-risk-cues-in-video-language-models">[4] <a href="https://arxiv.org/abs/2601.03369">RiskCueBench: Benchmarking Anticipatory Reasoning from Early Risk Cues in Video-Language Models</a></h3>
<p><em>Sha Luo, Yogesh Prabhu, Tim Ossowski, Kaiping Chen, Junjie Hu</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†RiskCueBenchè§†é¢‘ç†è§£åŸºå‡†ï¼Œé€šè¿‡è¯†åˆ«é£é™©ä¿¡å·ç‰‡æ®µæ¥è¯„ä¼°æ¨¡å‹ä»æ—©æœŸè§†è§‰çº¿ç´¢é¢„æµ‹æœªæ¥å±é™©äº‹ä»¶çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†å½“å‰ç³»ç»Ÿåœ¨è§£é‡ŠåŠ¨æ€åœºæ™¯å’Œé£é™©é¢„æµ‹æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘é£é™©è¯„ä¼°ç ”ç©¶å¤§å¤šå…è®¸æ¨¡å‹è®¿é—®åŒ…å«äº‹æ•…æœ¬èº«çš„å®Œæ•´è§†é¢‘åºåˆ—ï¼Œè¿™å¤§å¤§é™ä½äº†ä»»åŠ¡éš¾åº¦ï¼Œæ— æ³•åæ˜ çœŸå®ä¸–ç•Œæ¡ä»¶ã€‚ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œé€šè¿‡è¯†åˆ«é£é™©ä¿¡å·ç‰‡æ®µæ¥æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹ä»æ—©æœŸè§†è§‰çº¿ç´¢é¢„æµ‹æœªæ¥å±é™©äº‹ä»¶çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†æ–°çš„è§†é¢‘ç†è§£åŸºå‡†RiskCueBenchï¼Œå…¶ä¸­è§†é¢‘ç»è¿‡ç²¾å¿ƒæ ‡æ³¨ä»¥è¯†åˆ«é£é™©ä¿¡å·ç‰‡æ®µï¼Œå®šä¹‰ä¸ºæŒ‡ç¤ºæ½œåœ¨å®‰å…¨å…³æ³¨çš„æœ€æ—©æ—¶åˆ»ã€‚è¯¥æ–¹æ³•è¦æ±‚æ¨¡å‹ä»…åŸºäºæ—©æœŸè§†è§‰ä¿¡å·è¿›è¡Œé¢„æµ‹ï¼Œè€Œä¸æ˜¯è®¿é—®åŒ…å«äº‹æ•…ç»“æœçš„å®Œæ•´è§†é¢‘åºåˆ—ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºå½“å‰ç³»ç»Ÿåœ¨è§£é‡ŠåŠ¨æ€åœºæ™¯å’Œä»æ—©æœŸè§†è§‰ä¿¡å·é¢„æµ‹æœªæ¥å±é™©äº‹ä»¶æ–¹é¢å­˜åœ¨æ˜¾è‘—èƒ½åŠ›å·®è·ã€‚RiskCueBenchåŸºå‡†æ­ç¤ºäº†ç°æœ‰è§†é¢‘é£é™©é¢„æµ‹æ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´çš„é‡è¦æŒ‘æˆ˜ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†è§†é¢‘é£é™©é¢„æµ‹æ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œå³éœ€è¦ä»æ—©æœŸè§†è§‰çº¿ç´¢å‡†ç¡®é¢„æµ‹æœªæ¥å±é™©äº‹ä»¶ã€‚RiskCueBenchåŸºå‡†ä¸ºè¯„ä¼°æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹çš„é£é™©é¢„æµ‹èƒ½åŠ›æä¾›äº†æ›´ç°å®çš„æµ‹è¯•å¹³å°ï¼Œæ¨åŠ¨äº†è§†é¢‘ç†è§£é¢†åŸŸå‘æ›´å…·å‰ç»æ€§çš„é£é™©è¯„ä¼°æ–¹å‘å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>With the rapid growth of video centered social media, the ability to anticipate risky events from visual data is a promising direction for ensuring public safety and preventing real world accidents. Prior work has extensively studied supervised video risk assessment across domains such as driving, protests, and natural disasters. However, many existing datasets provide models with access to the full video sequence, including the accident itself, which substantially reduces the difficulty of the task. To better reflect real world conditions, we introduce a new video understanding benchmark RiskCueBench in which videos are carefully annotated to identify a risk signal clip, defined as the earliest moment that indicates a potential safety concern. Experimental results reveal a significant gap in current systems ability to interpret evolving situations and anticipate future risky events from early visual signals, highlighting important challenges for deploying video risk prediction models in practice.</p>
<h3 id="5-eye-q-a-multilingual-benchmark-for-visual-word-puzzle-solving-and-image-to-phrase-reasoning">[5] <a href="https://arxiv.org/abs/2601.03400">Eye-Q: A Multilingual Benchmark for Visual Word Puzzle Solving and Image-to-Phrase Reasoning</a></h3>
<p><em>Ali Najar, Alireza Mirrokni, Arshia Izadyari, Sadegh Mohammadian, Amir Homayoon Sharifizade, Asal Meskin, Mobin Bagherian, Ehsaneddin Asgari</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Eye-QåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œè¯¥åŸºå‡†åŒ…å«å¤šè¯­è¨€è§†è§‰å­—è°œï¼Œè¦æ±‚æ¨¡å‹è¿›è¡Œå‡è®¾ç”Ÿæˆå’Œä¿®è®¢ç­‰æ·±å±‚æ¨ç†ï¼Œè€Œéè¡¨é¢è¯†åˆ«ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†ä¸»è¦ä¾èµ–è¡¨é¢è¯†åˆ«è€Œéæ·±å±‚æ¨ç†ï¼Œç°æœ‰è¯„ä¼°æ–¹æ³•å®¹æ˜“é€šè¿‡å­—é¢åŒ¹é…ã€OCRæ·å¾„æˆ–ç®€å•æ£€ç´¢å¼åŒ¹é…è§£å†³ï¼Œç¼ºä¹å¯¹å¤æ‚è§†è§‰ç†è§£èƒ½åŠ›çš„è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯éœ€è¦å‘ç°éšå«è§†è§‰çº¿ç´¢ã€ç”Ÿæˆå’Œä¿®è®¢å‡è®¾ä»¥åŠå°†æ„ŸçŸ¥è¯æ®æ˜ å°„åˆ°éå­—é¢æ¦‚å¿µçš„æ¨ç†ä»»åŠ¡ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†Eye-Qå¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1,343ä¸ªè§†è§‰å­—è°œï¼Œæ¯ä¸ªè°œé¢˜å‘ˆç°æ¦‚å¿µå¯†é›†çš„åœºæ™¯å’Œç®€çŸ­æè¿°ï¼Œè¦æ±‚æ¨¡å‹æ¨æ–­ç‰¹å®šç›®æ ‡è¯æˆ–çŸ­è¯­ï¼Œè¿™äº›è°œé¢˜æ•…æ„è®¾è®¡ä¸ºéç»“æ„åŒ–å’Œçº¿ç´¢éšå«ï¼ŒåŒ…å«å¹²æ‰°é¡¹å’Œä¸Šä¸‹æ–‡å…³ç³»ï¼Œéœ€è¦é€‰æ‹©æ€§æ³¨æ„ã€æŠ½è±¡å’Œå…³è”æ¨ç†ï¼Œè¯„ä¼°é‡‡ç”¨å¼€æ”¾å¼ã€ä¸äººç±»å¯¹é½çš„åè®®ï¼Œåœ¨è½»åº¦è¾…åŠ©ä¸‹æ¢æµ‹å‡è®¾å½¢æˆå’Œä¿®è®¢è¿‡ç¨‹ï¼Œæ¶µç›–è‹±è¯­ã€æ³¢æ–¯è¯­ã€é˜¿æ‹‰ä¼¯è¯­å’Œè·¨è¯­è¨€è°œé¢˜ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹æ˜¾ç¤ºå­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨æŠ½è±¡å’Œè·¨è¯­è¨€è°œé¢˜ä¸Šï¼Œæœ€å¤§å‡†ç¡®ç‡ä»…ä¸º60.27%ï¼Œè¿™è¡¨æ˜å½“å‰æ¨¡å‹åœ¨æ„å»ºå’Œæœç´¢é€‚å½“æ¦‚å¿µè¡¨ç¤ºä»¥è¿›è¡Œçµæ´»å›¾åƒåˆ°çŸ­è¯­æ¨ç†çš„èƒ½åŠ›å­˜åœ¨å±€é™æ€§ï¼Œæ¨¡å‹éš¾ä»¥å¤„ç†éœ€è¦æ·±å±‚æ¨ç†çš„å¤æ‚è§†è§‰ç†è§£ä»»åŠ¡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„å®è´¨æ€§é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯æŠ½è±¡å’Œè·¨è¯­è¨€åœºæ™¯ä¸‹çš„è¡¨ç°ä¸è¶³ï¼ŒEye-QåŸºå‡†ä¸ºè¯„ä¼°æ¨¡å‹æ·±å±‚è§†è§‰ç†è§£èƒ½åŠ›æä¾›äº†é‡è¦å·¥å…·ï¼Œå¼ºè°ƒäº†æœªæ¥ç ”ç©¶éœ€è¦å…³æ³¨æ¨¡å‹æ„å»ºå’Œæœç´¢æ¦‚å¿µè¡¨ç¤ºçš„èƒ½åŠ›ï¼Œä»¥æ”¯æŒæ›´çµæ´»çš„è§†è§‰è¯­è¨€æ¨ç†ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) have achieved strong performance on standard vision-language benchmarks, yet often rely on surface-level recognition rather than deeper reasoning. We propose visual word puzzles as a challenging alternative, as they require discovering implicit visual cues, generating and revising hypotheses, and mapping perceptual evidence to non-literal concepts in ways that are difficult to solve via literal grounding, OCR-heavy shortcuts, or simple retrieval-style matching. We introduce Eye-Q, a multilingual benchmark designed to assess this form of complex visual understanding. Eye-Q contains 1,343 puzzles in which a model observes a conceptually dense scene with a brief description and must infer a specific target word or phrase. The puzzles are intentionally unstructured and cue-implicit, with distractors and contextual relationships that demand selective attention, abstraction, and associative inference. The benchmark spans English, Persian, Arabic, and cross-lingual puzzles. We evaluate state-of-the-art VLMs using an open-ended, human-aligned protocol that probes hypothesis formation and revision under lightweight assistance. Results reveal substantial performance gaps, especially on abstract and cross-lingual puzzles, highlighting limitations in current models' ability to construct and search over appropriate conceptual representations for flexible image-to-phrase inference; maximum accuracy reaches only 60.27%.</p>
<h3 id="6-gambit-a-gamified-jailbreak-framework-for-multimodal-large-language-models">[6] <a href="https://arxiv.org/abs/2601.03416">GAMBIT: A Gamified Jailbreak Framework for Multimodal Large Language Models</a></h3>
<p><em>Xiangdong Hu, Yangyang Jiang, Qin Hu, Xiaojun Jia</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGAMBITæ¡†æ¶ï¼Œä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€è¶Šç‹±æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºæ¸¸æˆåŒ–åœºæ™¯é©±åŠ¨æ¨¡å‹ä¸»åŠ¨æ¢ç´¢å¹¶é‡æ„æ¶æ„æ„å›¾ï¼Œä»è€Œæœ‰æ•ˆç»•è¿‡æ¨ç†å‹MLLMsçš„å®‰å…¨å¯¹é½æœºåˆ¶ï¼Œæ˜¾è‘—æå‡æ”»å‡»æˆåŠŸç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨å¯¹é½æœºåˆ¶åœ¨å¯¹æŠ—æ€§è¾“å…¥ä¸‹ä»ç„¶è„†å¼±ï¼Œç°æœ‰æ”»å‡»æ–¹æ³•ä¸»è¦å…³æ³¨å¢åŠ è§†è§‰ä»»åŠ¡å¤æ‚åº¦ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æ¨ç†æ¿€åŠ±æœºåˆ¶ï¼Œå¯¼è‡´åœ¨æ¨ç†å‹æ¨¡å‹ä¸Šçš„æ”»å‡»æ•ˆæœè¿œä½äºéæ¨ç†å‹æ¨¡å‹ï¼Œå› æ­¤éœ€è¦æ¢ç´¢å¦‚ä½•é€šè¿‡å½±å“æ¨¡å‹çš„è®¤çŸ¥é˜¶æ®µå†³ç­–æ¥ä¸»åŠ¨å®Œæˆè¶Šç‹±ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºGAMBITæ¡†æ¶ï¼Œé¦–å…ˆåˆ†è§£å¹¶é‡ç»„æœ‰å®³è§†è§‰è¯­ä¹‰ï¼Œç„¶åæ„å»ºæ¸¸æˆåŒ–åœºæ™¯é©±åŠ¨æ¨¡å‹è¿›è¡Œæ¢ç´¢å’Œæ„å›¾é‡æ„ï¼Œé€šè¿‡ç»“æ„åŒ–æ¨ç†é“¾åŒæ—¶å¢åŠ è§†è§‰å’Œæ–‡æœ¬ä»»åŠ¡å¤æ‚åº¦ï¼Œä½¿æ¨¡å‹æˆä¸ºæ¸¸æˆå‚ä¸è€…ï¼Œå…¶ç›®æ ‡è¿½æ±‚è¿‡ç¨‹ä¼šé™ä½å®‰å…¨æ³¨æ„åŠ›å¹¶è¯±å¯¼å…¶å›ç­”é‡æ„åçš„æ¶æ„æŸ¥è¯¢ã€‚</p>
<p><strong>Result:</strong> åœ¨æµè¡Œçš„æ¨ç†å‹å’Œéæ¨ç†å‹MLLMsä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGAMBITå®ç°äº†é«˜æ”»å‡»æˆåŠŸç‡ï¼Œåœ¨Gemini 2.5 Flashä¸Šè¾¾åˆ°92.13%ï¼Œåœ¨QvQ-MAXä¸Šè¾¾åˆ°91.20%ï¼Œåœ¨GPT-4oä¸Šè¾¾åˆ°85.87%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡æ¸¸æˆåŒ–åœºæ™¯è®¾è®¡å¯ä»¥å½±å“æ¨ç†å‹MLLMsçš„è®¤çŸ¥å†³ç­–è¿‡ç¨‹ï¼Œæœ‰æ•ˆç»•è¿‡å®‰å…¨å¯¹é½æœºåˆ¶ï¼Œæ­ç¤ºäº†å½“å‰å®‰å…¨é˜²æŠ¤åœ¨ç»“æ„åŒ–æ¨ç†æ”»å‡»ä¸‹çš„è„†å¼±æ€§ï¼Œä¸ºæœªæ¥æ›´é²æ£’çš„å®‰å…¨å¯¹é½æ–¹æ³•æä¾›äº†é‡è¦å¯ç¤ºã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) have become widely deployed, yet their safety alignment remains fragile under adversarial inputs. Previous work has shown that increasing inference steps can disrupt safety mechanisms and lead MLLMs to generate attacker-desired harmful content. However, most existing attacks focus on increasing the complexity of the modified visual task itself and do not explicitly leverage the model's own reasoning incentives. This leads to them underperforming on reasoning models (Models with Chain-of-Thoughts) compared to non-reasoning ones (Models without Chain-of-Thoughts). If a model can think like a human, can we influence its cognitive-stage decisions so that it proactively completes a jailbreak? To validate this idea, we propose GAMBI} (Gamified Adversarial Multimodal Breakout via Instructional Traps), a novel multimodal jailbreak framework that decomposes and reassembles harmful visual semantics, then constructs a gamified scene that drives the model to explore, reconstruct intent, and answer as part of winning the game. The resulting structured reasoning chain increases task complexity in both vision and text, positioning the model as a participant whose goal pursuit reduces safety attention and induces it to answer the reconstructed malicious query. Extensive experiments on popular reasoning and non-reasoning MLLMs demonstrate that GAMBIT achieves high Attack Success Rates (ASR), reaching 92.13% on Gemini 2.5 Flash, 91.20% on QvQ-MAX, and 85.87% on GPT-4o, significantly outperforming baselines.</p>
<h3 id="7-frost-drive-scalable-and-efficient-end-to-end-driving-with-a-frozen-vision-encoder">[7] <a href="https://arxiv.org/abs/2601.03460">FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder</a></h3>
<p><em>Zeyu Dong, Yimin Zhu, Yu Wu, Yu Sun</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FROST-Driveï¼Œä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¶æ„ï¼Œé€šè¿‡å†»ç»“é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰ç¼–ç å™¨æƒé‡ï¼Œç›´æ¥å°†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›è¿ç§»åˆ°é©¾é©¶ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—æå‡äº†åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¨¡å‹åœ¨æ³›åŒ–åˆ°æ–°é¢–å¤æ‚åœºæ™¯æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•å¯¹è§†è§‰ç¼–ç å™¨è¿›è¡Œå®Œå…¨å¾®è°ƒå¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡åº¦ä¸“æ³¨äºè®­ç»ƒæ•°æ®è€Œé™åˆ¶æ³›åŒ–èƒ½åŠ›ï¼Œæœ¬æ–‡è´¨ç–‘è¿™ç§è®­ç»ƒèŒƒå¼çš„å¿…è¦æ€§ï¼Œæ—¨åœ¨æ¢ç´¢å¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å¹¿æ³›ä¸–ç•ŒçŸ¥è¯†ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºFROST-Driveæ¶æ„ï¼Œä¿æŒé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ä¸­è§†è§‰ç¼–ç å™¨çš„æƒé‡å†»ç»“ï¼Œç›´æ¥å°†å…¶ä¸°å¯Œçš„æ³›åŒ–çŸ¥è¯†è¿ç§»åˆ°é©¾é©¶ä»»åŠ¡ï¼›è¯¥æ¶æ„ç»“åˆå†»ç»“ç¼–ç å™¨ã€åŸºäºTransformerçš„å¤šæ¨¡æ€èåˆé€‚é…å™¨ä»¥åŠåŸºäºGRUçš„è§£ç å™¨ç”¨äºå¹³æ»‘è·¯å¾„ç‚¹ç”Ÿæˆï¼›åŒæ—¶å¼•å…¥ä¸“é—¨è®¾è®¡çš„æŸå¤±å‡½æ•°æ¥ç›´æ¥ä¼˜åŒ–Rater Feedback Scoreï¼Œè¯¥æŒ‡æ ‡ä¼˜å…ˆè€ƒè™‘é²æ£’çš„è½¨è¿¹è§„åˆ’ã€‚</p>
<p><strong>Result:</strong> åœ¨Waymo Open E2Eæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥å†»ç»“ç¼–ç å™¨æ–¹æ³•æ˜¾è‘—ä¼˜äºé‡‡ç”¨å®Œå…¨å¾®è°ƒçš„æ¨¡å‹ï¼›è¯¥æ•°æ®é›†ä¸“é—¨è®¾è®¡ç”¨äºæ•æ‰é•¿å°¾åœºæ™¯ï¼Œå®éªŒç»“æœæä¾›äº†æœ‰åŠ›è¯æ®ï¼Œè¡¨æ˜ä¿ç•™å¼ºå¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„å¹¿æ³›çŸ¥è¯†æ¯”å¯†é›†çš„é¢†åŸŸç‰¹å®šé€‚åº”æ›´æœ‰æ•ˆã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶è¯æ˜ä¿ç•™é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å¹¿æ³›çŸ¥è¯†æ˜¯å®ç°é²æ£’ã€å¯æ³›åŒ–é©¾é©¶æ€§èƒ½çš„æ›´æœ‰æ•ˆç­–ç•¥ï¼Œä¸ºå¼€å‘èƒ½å¤Ÿæ›´å¥½å¤„ç†ç°å®ä¸–ç•Œåº”ç”¨å¤æ‚æ€§çš„è§†è§‰æ¨¡å‹æä¾›äº†æ–°é€”å¾„ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿå®Œå…¨å¾®è°ƒèŒƒå¼çš„å¿…è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>End-to-end (E2E) models in autonomous driving aim to directly map sensor inputs to control commands, but their ability to generalize to novel and complex scenarios remains a key challenge. The common practice of fully fine-tuning the vision encoder on driving datasets potentially limits its generalization by causing the model to specialize too heavily in the training data. This work challenges the necessity of this training paradigm. We propose FROST-Drive, a novel E2E architecture designed to preserve and leverage the powerful generalization capabilities of a pretrained vision encoder from a Vision-Language Model (VLM). By keeping the encoder's weights frozen, our approach directly transfers the rich, generalized world knowledge from the VLM to the driving task. Our model architecture combines this frozen encoder with a transformer-based adapter for multimodal fusion and a GRU-based decoder for smooth waypoint generation. Furthermore, we introduce a custom loss function designed to directly optimize for Rater Feedback Score (RFS), a metric that prioritizes robust trajectory planning. We conduct extensive experiments on Waymo Open E2E Dataset, a large-scale datasets deliberately curated to capture the long-tail scenarios, demonstrating that our frozen-encoder approach significantly outperforms models that employ full fine-tuning. Our results provide substantial evidence that preserving the broad knowledge of a capable VLM is a more effective strategy for achieving robust, generalizable driving performance than intensive domain-specific adaptation. This offers a new pathway for developing vision-based models that can better handle the complexities of real-world application domains.</p>
<h3 id="8-thinkrl-edit-thinking-in-reinforcement-learning-for-reasoning-centric-image-editing">[8] <a href="https://arxiv.org/abs/2601.03467">ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing</a></h3>
<p><em>Hengjia Li, Liming Jiang, Qing Yan, Yizhi Song, Hao Kang, Zichuan Liu, Xin Lu, Boxi Wu, Deng Cai</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºThinkRL-Editï¼Œä¸€ç§é¢å‘æ¨ç†çš„å›¾åƒç¼–è¾‘å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è§£è€¦è§†è§‰æ¨ç†ä¸å›¾åƒåˆæˆï¼Œå¹¶å¼•å…¥æ€ç»´é“¾é‡‡æ ·æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æŒ‡ä»¤é©±åŠ¨å›¾åƒç¼–è¾‘åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºå¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„æŒ‡ä»¤é©±åŠ¨å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨è§†è§‰æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™ï¼Œå¯¼è‡´åœ¨æ¨ç†å¯†é›†å‹ç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼›ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ¨ç†æ¢ç´¢å—é™ã€å¥–åŠ±èåˆåå·®ä»¥åŠä¸ç¨³å®šçš„è§†è§‰è¯­è¨€æ¨¡å‹æŒ‡ä»¤å¥–åŠ±ã€‚</p>
<p><strong>Method:</strong> æå‡ºThinkRL-Editæ¡†æ¶ï¼Œå°†è§†è§‰æ¨ç†ä¸å›¾åƒåˆæˆè§£è€¦ï¼Œå¹¶è¶…è¶Šå»å™ªéšæœºæ€§æ‰©å±•æ¨ç†æ¢ç´¢ï¼›å¼•å…¥åŸºäºæ€ç»´é“¾çš„æ¨ç†é‡‡æ ·æœºåˆ¶ï¼ŒåŒ…å«ç”Ÿæˆå‰çš„è§„åˆ’å’Œåæ€é˜¶æ®µï¼Œä¿ƒä½¿æ¨¡å‹æ¢ç´¢å¤šä¸ªè¯­ä¹‰å‡è®¾å¹¶éªŒè¯å…¶åˆç†æ€§ï¼›æå‡ºæ— åé“¾åå¥½åˆ†ç»„ç­–ç•¥ä»¥é¿å…åŠ æƒèšåˆå¤±è´¥ï¼Œå¹¶ç”¨äºŒå…ƒæ£€æŸ¥è¡¨æ›¿ä»£åŸºäºåŒºé—´çš„è§†è§‰è¯­è¨€æ¨¡å‹è¯„åˆ†ï¼Œæä¾›æ›´ç²¾ç¡®ã€ä½æ–¹å·®ä¸”å¯è§£é‡Šçš„å¤æ‚æ¨ç†å¥–åŠ±ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†å¯†é›†å‹å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå…ˆå‰å·¥ä½œï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å¿ å®äºæŒ‡ä»¤ã€è§†è§‰è¿è´¯ä¸”è¯­ä¹‰åŸºç¡€æ‰å®çš„ç¼–è¾‘ç»“æœï¼Œåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è§£è€¦æ¨ç†ä¸åˆæˆã€æ‰©å±•æ¨ç†æ¢ç´¢ç©ºé—´ä»¥åŠæ”¹è¿›å¥–åŠ±æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤æ‚å›¾åƒç¼–è¾‘ä»»åŠ¡çš„æ€§èƒ½ï¼›æå‡ºçš„æ¡†æ¶ä¸ºå¼ºåŒ–å­¦ä¹ åœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°æ€è·¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜å±‚æ¬¡è¯­ä¹‰æ¨ç†çš„åœºæ™¯ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)-based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.</p>
<h3 id="9-crobim-u-uncertainty-driven-referring-remote-sensing-image-segmentation">[9] <a href="https://arxiv.org/abs/2601.03490">CroBIM-U: Uncertainty-Driven Referring Remote Sensing Image Segmentation</a></h3>
<p><em>Yuzhe Sun, Zhe Dong, Haochen Jiang, Tianzhu Liu, Yanfeng Gu</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸ç¡®å®šæ€§å¼•å¯¼çš„é¥æ„Ÿå›¾åƒæŒ‡ä»£åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡æŒ‡ä»£ä¸ç¡®å®šæ€§ç©ºé—´åˆ†å¸ƒæ¥æŒ‡å¯¼è‡ªé€‚åº”æ¨ç†ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è·¨æ¨¡æ€å¯¹é½ä¸­ç©ºé—´éå‡åŒ€æ€§çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚é¥æ„Ÿåœºæ™¯ä¸‹çš„åˆ†å‰²é²æ£’æ€§å’Œå‡ ä½•ä¿çœŸåº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é¥æ„Ÿå›¾åƒæŒ‡ä»£åˆ†å‰²é¢ä¸´æç«¯å°ºåº¦å˜åŒ–ã€å¯†é›†ç›¸ä¼¼å¹²æ‰°ç‰©å’Œå¤æ‚è¾¹ç•Œç»“æ„ç­‰æŒ‘æˆ˜ï¼Œå¯¼è‡´è·¨æ¨¡æ€å¯¹é½å­˜åœ¨æ˜¾è‘—çš„ç©ºé—´éå‡åŒ€æ€§ã€‚ç°æœ‰æ–¹æ³•é‡‡ç”¨å‡åŒ€èåˆå’Œç»†åŒ–ç­–ç•¥ï¼Œåœ¨è§†è§‰æ¸…æ™°åŒºåŸŸå¼•å…¥ä¸å¿…è¦çš„è¯­è¨€æ‰°åŠ¨ï¼Œè€Œåœ¨æ··æ·†åŒºåŸŸåˆæ— æ³•æä¾›è¶³å¤Ÿçš„æ¶ˆæ­§èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½å¤Ÿè‡ªé€‚åº”å¤„ç†ä¸åŒåŒºåŸŸä¸ç¡®å®šæ€§çš„æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºä¸ç¡®å®šæ€§å¼•å¯¼æ¡†æ¶ï¼Œæ ¸å¿ƒæ˜¯å¼•å…¥å¯æ’æ‹”çš„æŒ‡ä»£ä¸ç¡®å®šæ€§è¯„åˆ†å™¨ï¼Œé€šè¿‡åœ¨çº¿è¯¯å·®ä¸€è‡´æ€§ç›‘ç£ç­–ç•¥è®­ç»ƒæ¥é¢„æµ‹æŒ‡ä»£æ­§ä¹‰çš„ç©ºé—´åˆ†å¸ƒã€‚åŸºäºæ­¤å…ˆéªŒè®¾è®¡ä¸¤ä¸ªå¯æ’æ‹”æ¨¡å—ï¼šä¸ç¡®å®šæ€§é—¨æ§èåˆåŠ¨æ€è°ƒèŠ‚è¯­è¨€æ³¨å…¥å¼ºåº¦ï¼Œåœ¨é«˜ä¸ç¡®å®šæ€§åŒºåŸŸå¢å¼ºçº¦æŸè€Œåœ¨ä½ä¸ç¡®å®šæ€§åŒºåŸŸæŠ‘åˆ¶å™ªå£°ï¼›ä¸ç¡®å®šæ€§é©±åŠ¨å±€éƒ¨ç»†åŒ–åˆ©ç”¨ä¸ç¡®å®šæ€§è¡ç”Ÿçš„è½¯æ©ç èšç„¦äºæ˜“é”™è¾¹ç•Œå’Œç²¾ç»†ç»†èŠ‚çš„ä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½œä¸ºç»Ÿä¸€çš„å³æ’å³ç”¨è§£å†³æ–¹æ¡ˆï¼Œåœ¨ä¸æ”¹å˜éª¨å¹²ç½‘ç»œæ¶æ„çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚é¥æ„Ÿåœºæ™¯ä¸­çš„é²æ£’æ€§å’Œå‡ ä½•ä¿çœŸåº¦ã€‚å…·ä½“è¡¨ç°ä¸ºåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å°ºåº¦å˜åŒ–ã€ç›¸ä¼¼å¹²æ‰°å’Œå¤æ‚è¾¹ç•Œç­‰æŒ‘æˆ˜æ€§åœºæ™¯æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†æ˜¾å¼å»ºæ¨¡æŒ‡ä»£ä¸ç¡®å®šæ€§ç©ºé—´åˆ†å¸ƒå¯¹äºé¥æ„Ÿå›¾åƒåˆ†å‰²çš„é‡è¦æ€§ï¼Œæå‡ºçš„ä¸ç¡®å®šæ€§å¼•å¯¼æ¡†æ¶ä¸ºè·¨æ¨¡æ€å¯¹é½ä¸­çš„ç©ºé—´éå‡åŒ€æ€§é—®é¢˜æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä½œä¸ºå³æ’å³ç”¨æ¨¡å—å…·æœ‰å¹¿æ³›é€‚ç”¨æ€§ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ï¼Œå³é€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥æœºåˆ¶æ¥å¢å¼ºè§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„è‡ªé€‚åº”æ¨ç†èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Referring remote sensing image segmentation aims to localize specific targets described by natural language within complex overhead imagery. However, due to extreme scale variations, dense similar distractors, and intricate boundary structures, the reliability of cross-modal alignment exhibits significant \textbf{spatial non-uniformity}. Existing methods typically employ uniform fusion and refinement strategies across the entire image, which often introduces unnecessary linguistic perturbations in visually clear regions while failing to provide sufficient disambiguation in confused areas. To address this, we propose an \textbf{uncertainty-guided framework} that explicitly leverages a pixel-wise \textbf{referring uncertainty map} as a spatial prior to orchestrate adaptive inference. Specifically, we introduce a plug-and-play \textbf{Referring Uncertainty Scorer (RUS)}, which is trained via an online error-consistency supervision strategy to interpretably predict the spatial distribution of referential ambiguity. Building on this prior, we design two plug-and-play modules: 1) \textbf{Uncertainty-Gated Fusion (UGF)}, which dynamically modulates language injection strength to enhance constraints in high-uncertainty regions while suppressing noise in low-uncertainty ones; and 2) \textbf{Uncertainty-Driven Local Refinement (UDLR)}, which utilizes uncertainty-derived soft masks to focus refinement on error-prone boundaries and fine details. Extensive experiments demonstrate that our method functions as a unified, plug-and-play solution that significantly improves robustness and geometric fidelity in complex remote sensing scenes without altering the backbone architecture.</p>
<h3 id="10-understanding-reward-hacking-in-text-to-image-reinforcement-learning">[10] <a href="https://arxiv.org/abs/2601.03468">Understanding Reward Hacking in Text-to-Image Reinforcement Learning</a></h3>
<p><em>Yunqi Hong, Kuei-Chun Kao, Hengguang Zhou, Cho-Jui Hsieh</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†æ–‡æœ¬åˆ°å›¾åƒå¼ºåŒ–å­¦ä¹ åè®­ç»ƒä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§è½»é‡çº§è‡ªé€‚åº”ä¼ªå½±å¥–åŠ±æ¨¡å‹ä½œä¸ºæœ‰æ•ˆçš„æ­£åˆ™åŒ–å™¨ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒçš„è§†è§‰çœŸå®æ€§å’Œå¯¹é½æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒå¼ºåŒ–å­¦ä¹ åè®­ç»ƒä¸­ä½¿ç”¨çš„å¥–åŠ±å‡½æ•°å¾€å¾€æ˜¯ä¸å®Œç¾çš„äººç±»åˆ¤æ–­ä»£ç†ï¼Œå¯¼è‡´æ¨¡å‹å®¹æ˜“äº§ç”Ÿå¥–åŠ±é»‘å®¢è¡Œä¸ºâ€”â€”ç”Ÿæˆä¸ç°å®æˆ–ä½è´¨é‡çš„å›¾åƒå´èƒ½è·å¾—é«˜å¥–åŠ±åˆ†æ•°ï¼Œè¿™ä¸¥é‡å½±å“äº†ç”Ÿæˆè´¨é‡å’Œäººç±»åå¥½å¯¹é½ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†å®¡ç¾/äººç±»åå¥½å¥–åŠ±å’Œæç¤º-å›¾åƒä¸€è‡´æ€§å¥–åŠ±å„è‡ªå¯¹å¥–åŠ±é»‘å®¢çš„è´¡çŒ®ï¼Œå¹¶ç ”ç©¶äº†å¤šå¥–åŠ±é›†æˆæ–¹æ³•çš„å±€é™æ€§ã€‚ä¸ºåº”å¯¹ä¼ªå½±ç”Ÿæˆè¿™ä¸€å¸¸è§å¤±æ•ˆæ¨¡å¼ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§è‡ªé€‚åº”ä¼ªå½±å¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å°è§„æ¨¡ç²¾å¿ƒç­–åˆ’çš„æ— ä¼ªå½±å’Œå«ä¼ªå½±æ ·æœ¬æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¯ä½œä¸ºç°æœ‰å¼ºåŒ–å­¦ä¹ æµç¨‹çš„æœ‰æ•ˆæ­£åˆ™åŒ–å™¨ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œé›†æˆæå‡ºçš„ä¼ªå½±å¥–åŠ±æ¨¡å‹èƒ½æ˜¾è‘—æå‡è§†è§‰çœŸå®æ€§å¹¶å‡å°‘å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œåœ¨å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒå¼ºåŒ–å­¦ä¹ è®¾ç½®ä¸­å‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•è¯æ˜äº†è½»é‡çº§å¥–åŠ±å¢å¼ºä½œä¸ºå¯¹æŠ—å¥–åŠ±é»‘å®¢çš„å®‰å…¨ä¿éšœæœºåˆ¶çš„å®é™…æ•ˆç”¨ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å¥–åŠ±é»‘å®¢æ˜¯æ–‡æœ¬åˆ°å›¾åƒå¼ºåŒ–å­¦ä¹ åè®­ç»ƒä¸­çš„ç³»ç»Ÿæ€§æŒ‘æˆ˜ï¼Œè€Œè½»é‡çº§è‡ªé€‚åº”å¥–åŠ±æ¨¡å‹å¯ä½œä¸ºæœ‰æ•ˆçš„æ­£åˆ™åŒ–è§£å†³æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œä¸ºæ”¹è¿›ç”Ÿæˆæ¨¡å‹çš„å¥–åŠ±è®¾è®¡æä¾›äº†æ–°æ€è·¯ï¼Œå¼ºè°ƒäº†é’ˆå¯¹ç‰¹å®šå¤±æ•ˆæ¨¡å¼å¼€å‘ä¸“é—¨å¥–åŠ±ç»„ä»¶çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Reinforcement learning (RL) has become a standard approach for post-training large language models and, more recently, for improving image generation models, which uses reward functions to enhance generation quality and human preference alignment. However, existing reward designs are often imperfect proxies for true human judgment, making models prone to reward hacking--producing unrealistic or low-quality images that nevertheless achieve high reward scores. In this work, we systematically analyze reward hacking behaviors in text-to-image (T2I) RL post-training. We investigate how both aesthetic/human preference rewards and prompt-image consistency rewards individually contribute to reward hacking and further show that ensembling multiple rewards can only partially mitigate this issue. Across diverse reward models, we identify a common failure mode: the generation of artifact-prone images. To address this, we propose a lightweight and adaptive artifact reward model, trained on a small curated dataset of artifact-free and artifact-containing samples. This model can be integrated into existing RL pipelines as an effective regularizer for commonly used reward models. Experiments demonstrate that incorporating our artifact reward significantly improves visual realism and reduces reward hacking across multiple T2I RL setups, demonstrating the effectiveness of lightweight reward augment serving as a safeguard against reward hacking.</p>
<h3 id="11-sdcd-structure-disrupted-contrastive-decoding-for-mitigating-hallucinations-in-large-vision-language-models">[11] <a href="https://arxiv.org/abs/2601.03500">SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models</a></h3>
<p><em>Yuxuan Xia, Siheng Wang, Peng Li</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç»“æ„æ‰°ä¹±å¯¹æ¯”è§£ç ç®—æ³•ï¼Œé€šè¿‡å¼•å…¥ç»“æ„æ‰°ä¹±çš„è§†è§‰è§†å›¾æ¥æ ¡å‡†è¾“å‡ºåˆ†å¸ƒï¼Œæœ‰æ•ˆç¼“è§£å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç‰©ä½“å¹»è§‰é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç‰©ä½“å¹»è§‰é—®é¢˜ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ç¼“è§£è¯­è¨€å…ˆéªŒæˆ–é«˜å±‚ç»Ÿè®¡åå·®ï¼Œå¾€å¾€å¿½è§†äº†è§†è§‰ç¼–ç è¿‡ç¨‹ä¸­çš„å†…éƒ¨å¤æ‚æ€§ã€‚æœ¬æ–‡å‘ç°è§†è§‰ç»Ÿè®¡åå·®æºäºè§†è§‰ç¼–ç å™¨åœ¨å¼±ç»“æ„ç›‘ç£ä¸‹å›ºæœ‰çš„Bag-of-Patchesè¡Œä¸ºï¼Œè¿™ç§åå·®å¯¼è‡´æ¨¡å‹ä¼˜å…ˆè€ƒè™‘å•ä¸ªè¡¥ä¸å†…çš„å±€éƒ¨çº¹ç†ç‰¹å¾è€Œéæ•´ä½“å‡ ä½•ç»“æ„ï¼Œä»è€Œå¯èƒ½å¼•å‘è™šå‡çš„è§†è§‰ç½®ä¿¡åº¦å¹¶å¯¼è‡´å¹»è§‰ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç»“æ„æ‰°ä¹±å¯¹æ¯”è§£ç ç®—æ³•ï¼Œé€šè¿‡å¼•å…¥ç»è¿‡æ´—ç‰Œçš„ç»“æ„æ‰°ä¹±è§†å›¾æ¥å¯¹æ¯”æ ¡å‡†è¾“å‡ºåˆ†å¸ƒã€‚è¯¥æ–¹æ³•æƒ©ç½šåœ¨ç»“æ„ç¼ºå¤±è§†å›¾ä¸‹ä»ä¿æŒé«˜ç½®ä¿¡åº¦çš„æ ‡è®°ï¼Œä»è€Œæœ‰æ•ˆæŠ‘åˆ¶çº¹ç†é©±åŠ¨çš„åå·®ã€‚SDCDç®—æ³•ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç›´æ¥ä½œç”¨äºæ¨ç†é˜¶æ®µï¼Œé€šè¿‡å¯¹æ¯”åŸå§‹è§†å›¾å’Œç»“æ„æ‰°ä¹±è§†å›¾çš„ç½®ä¿¡åº¦å·®å¼‚æ¥è¯†åˆ«å’Œå‡å°‘å¹»è§‰å€¾å‘ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒSDCDç®—æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ç¼“è§£äº†å¹»è§‰ç°è±¡ï¼Œæœ‰æ•ˆæå‡äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ•´ä½“å¤šæ¨¡æ€èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹æ¶æ„å’Œæ•°æ®é›†ä¸Šéƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œè¯æ˜äº†é€šè¿‡ç»“æ„æ‰°ä¹±å¯¹æ¯”æœºåˆ¶æŠ‘åˆ¶è§†è§‰ç»Ÿè®¡åå·®çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶æ­ç¤ºäº†è§†è§‰ç¼–ç è¿‡ç¨‹ä¸­çš„ç»Ÿè®¡åå·®æ˜¯å¯¼è‡´ç‰©ä½“å¹»è§‰çš„é‡è¦å› ç´ ï¼Œç‰¹åˆ«æ˜¯Bag-of-Patchesè¡Œä¸ºåœ¨å¼±ç»“æ„ç›‘ç£ä¸‹å¼•å‘çš„çº¹ç†åå¥½é—®é¢˜ã€‚æå‡ºçš„SDCDæ–¹æ³•ä¸ºç¼“è§£å¹»è§‰é—®é¢˜æä¾›äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¼ºè°ƒäº†åœ¨è§†è§‰ç†è§£ä¸­è€ƒè™‘å‡ ä½•ç»“æ„ä¿¡æ¯çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æ¨¡å‹çš„è®¾è®¡å’Œä¼˜åŒ–æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process. We identify that visual statistical bias, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations. To address this, we introduce a training-free algorithm called Structure-Disrupted Contrastive Decoding (SDCD), which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.</p>
<h3 id="12-easlt-emotion-aware-sign-language-translation">[12] <a href="https://arxiv.org/abs/2601.03549">EASLT: Emotion-Aware Sign Language Translation</a></h3>
<p><em>Guobin Tu, Di Weng</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºEASLTæ¡†æ¶ï¼Œå°†é¢éƒ¨æƒ…æ„Ÿä½œä¸ºè¯­ä¹‰é”šç‚¹è€Œéè¾…åŠ©ä¿¡æ¯ï¼Œé€šè¿‡æƒ…æ„Ÿæ„ŸçŸ¥èåˆæ¨¡å—è§£å†³æ‰‹è¯­ç¿»è¯‘ä¸­å› å¿½ç•¥é¢éƒ¨è¡¨æƒ…å¯¼è‡´çš„è¯­ä¹‰æ­§ä¹‰é—®é¢˜ï¼Œåœ¨æ— è¯æ±‡æ³¨é‡Šæ–¹æ³•ä¸­å®ç°äº†å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ— è¯æ±‡æ³¨é‡Šçš„æ‰‹è¯­ç¿»è¯‘æ–¹æ³•ä¸»è¦å…³æ³¨æ‰‹åŠ¨ä¿¡å·è€Œå¿½è§†éæ‰‹åŠ¨ä¿¡å·ï¼Œç‰¹åˆ«æ˜¯é¢éƒ¨è¡¨æƒ…çš„è¯­ä¹‰é‡è¦æ€§ï¼Œå¯¼è‡´å½“ä¸åŒæ¦‚å¿µå…·æœ‰ç›¸åŒæ‰‹åŠ¨è¡¨è¾¾æ—¶äº§ç”Ÿè¯­ä¹‰æ­§ä¹‰ï¼Œéœ€è¦å°†é¢éƒ¨æƒ…æ„Ÿä½œä¸ºæ ¸å¿ƒè¯­ä¹‰é”šç‚¹è€Œéè¾…åŠ©ä¿¡æ¯æ¥è§£å†³è¿™ä¸€å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> EASLTæ¡†æ¶åŒ…å«ä¸“é—¨çš„æƒ…æ„Ÿç¼–ç å™¨æ¥æ•æ‰è¿ç»­çš„æƒ…æ„ŸåŠ¨æ€ï¼Œå¹¶é€šè¿‡æ–°é¢–çš„æƒ…æ„Ÿæ„ŸçŸ¥èåˆæ¨¡å—è‡ªé€‚åº”åœ°åŸºäºæƒ…æ„Ÿä¸Šä¸‹æ–‡é‡æ–°æ ¡å‡†æ—¶ç©ºæ‰‹è¯­ç‰¹å¾ï¼Œå°†é¢éƒ¨æƒ…æ„Ÿè¡¨ç¤ºä½œä¸ºè¯­ä¹‰é”šç‚¹æ•´åˆåˆ°ç¿»è¯‘è¿‡ç¨‹ä¸­ä»¥è§£å†³æ­§ä¹‰é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨PHOENIX14Tå’ŒCSL-DailyåŸºå‡†æµ‹è¯•ä¸­ï¼ŒEASLTåœ¨æ— è¯æ±‡æ³¨é‡Šæ–¹æ³•ä¸­å®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œåˆ†åˆ«è·å¾—26.15å’Œ22.80çš„BLEU-4åˆ†æ•°ï¼Œä»¥åŠ61.0å’Œ57.8çš„BLEURTåˆ†æ•°ï¼Œæ¶ˆèç ”ç©¶è¯å®æ˜¾å¼å»ºæ¨¡æƒ…æ„Ÿèƒ½æœ‰æ•ˆå°†æƒ…æ„Ÿè¯­ä¹‰ä¸æ‰‹åŠ¨åŠ¨æ€è§£è€¦ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜å°†é¢éƒ¨æƒ…æ„Ÿä½œä¸ºè¯­ä¹‰é”šç‚¹è€Œéè¾…åŠ©ä¿¡æ¯èƒ½æ˜¾è‘—æå‡æ‰‹è¯­ç¿»è¯‘çš„ä¿çœŸåº¦ï¼Œæƒ…æ„Ÿæ„ŸçŸ¥èåˆæœºåˆ¶èƒ½æœ‰æ•ˆè§£å†³è·¨æ¨¡æ€ç¿»è¯‘ä¸­çš„è¯­ä¹‰æ­§ä¹‰é—®é¢˜ï¼Œä¸ºæœªæ¥æ•´åˆå¤šæ¨¡æ€ä¿¡å·çš„æ‰‹è¯­ç¿»è¯‘ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Sign Language Translation (SLT) is a complex cross-modal task requiring the integration of Manual Signals (MS) and Non-Manual Signals (NMS). While recent gloss-free SLT methods have made strides in translating manual gestures, they frequently overlook the semantic criticality of facial expressions, resulting in ambiguity when distinct concepts share identical manual articulations. To address this, we present <strong>EASLT</strong> (<strong>E</strong>motion-<strong>A</strong>ware <strong>S</strong>ign <strong>L</strong>anguage <strong>T</strong>ranslation), a framework that treats facial affect not as auxiliary information, but as a robust semantic anchor. Unlike methods that relegate facial expressions to a secondary role, EASLT incorporates a dedicated emotional encoder to capture continuous affective dynamics. These representations are integrated via a novel <em>Emotion-Aware Fusion</em> (EAF) module, which adaptively recalibrates spatio-temporal sign features based on affective context to resolve semantic ambiguities. Extensive evaluations on the PHOENIX14T and CSL-Daily benchmarks demonstrate that EASLT establishes advanced performance among gloss-free methods, achieving BLEU-4 scores of 26.15 and 22.80, and BLEURT scores of 61.0 and 57.8, respectively. Ablation studies confirm that explicitly modeling emotion effectively decouples affective semantics from manual dynamics, significantly enhancing translation fidelity. Code is available at https://github.com/TuGuobin/EASLT.</p>
<h3 id="13-physics-constrained-cross-resolution-enhancement-network-for-optics-guided-thermal-uav-image-super-resolution">[13] <a href="https://arxiv.org/abs/2601.03526">Physics-Constrained Cross-Resolution Enhancement Network for Optics-Guided Thermal UAV Image Super-Resolution</a></h3>
<p><em>Zhicheng Zhao, Fengjiao Peng, Jinquan Yan, Wei Lu, Chenglong Li, Jin Tang</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºPCNetï¼Œä¸€ç§ç”¨äºçƒ­æˆåƒæ— äººæœºå›¾åƒè¶…åˆ†è¾¨ç‡çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡è·¨åˆ†è¾¨ç‡ç›¸äº’å¢å¼ºæ¨¡å—å’Œç‰©ç†é©±åŠ¨çš„çƒ­ä¼ å¯¼æ¨¡å—ï¼Œåœ¨ä¿ç•™é«˜é¢‘å…‰å­¦å…ˆéªŒçš„åŒæ—¶é˜²æ­¢ç‰©ç†ä¸ä¸€è‡´çš„ä¼ªå½±ï¼Œæ˜¾è‘—æå‡äº†çƒ­æˆåƒè¶…åˆ†è¾¨ç‡çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å…‰å­¦å¼•å¯¼çš„çƒ­æˆåƒæ— äººæœºå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•é€šå¸¸å‹ç¼©å…‰å­¦ç‰¹å¾ä»¥åŒ¹é…çƒ­ç‰¹å¾ç»´åº¦ï¼Œè¿™ä¼šå¯¼è‡´å¯¹çƒ­è¶…åˆ†è¾¨ç‡æœ‰ç›Šçš„é«˜é¢‘ä¿¡æ¯ä¸¢å¤±ï¼Œå¹¶å› å¿½ç•¥æ¨¡æ€é—´æˆåƒç‰©ç†å·®å¼‚è€Œå¼•å…¥çº¹ç†å¤±çœŸå’Œè¾¹ç¼˜æ¨¡ç³Šç­‰ç‰©ç†ä¸ä¸€è‡´çš„ä¼ªå½±ã€‚</p>
<p><strong>Method:</strong> PCNetåŒ…å«è·¨åˆ†è¾¨ç‡ç›¸äº’å¢å¼ºæ¨¡å—ï¼ˆCRMEï¼‰å’Œç‰©ç†é©±åŠ¨çš„çƒ­ä¼ å¯¼æ¨¡å—ï¼ˆPDTMï¼‰ã€‚CRMEè”åˆä¼˜åŒ–çƒ­å›¾åƒè¶…åˆ†è¾¨ç‡å’Œå…‰å­¦åˆ°çƒ­æ¨¡æ€è½¬æ¢ï¼Œå®ç°è·¨åˆ†è¾¨ç‡çš„åŒå‘ç‰¹å¾äº¤äº’å¹¶ä¿ç•™é«˜é¢‘å…‰å­¦å…ˆéªŒï¼›PDTMå°†äºŒç»´çƒ­ä¼ å¯¼èå…¥å…‰å­¦å¼•å¯¼è¿‡ç¨‹ï¼Œå»ºæ¨¡ç©ºé—´å˜åŒ–çš„çƒ­ä¼ å¯¼ç‰¹æ€§ä»¥é˜²æ­¢ä¸ä¸€è‡´ä¼ªå½±ï¼›æ­¤å¤–è¿˜å¼•å…¥äº†æ¸©åº¦ä¸€è‡´æ€§æŸå¤±æ¥å¼ºåˆ¶åŒºåŸŸåˆ†å¸ƒä¸€è‡´æ€§å’Œè¾¹ç•Œæ¢¯åº¦å¹³æ»‘æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨VGTSR2.0å’ŒDroneVehicleæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPCNetåœ¨é‡å»ºè´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡ï¼ˆåŒ…æ‹¬è¯­ä¹‰åˆ†å‰²å’Œç›®æ ‡æ£€æµ‹ï¼‰æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ç‰©ç†çº¦æŸçš„å…‰å­¦å¼•å¯¼å’Œè·¨åˆ†è¾¨ç‡ç›¸äº’å¢å¼ºæœºåˆ¶ï¼Œè§£å†³äº†çƒ­æˆåƒè¶…åˆ†è¾¨ç‡ä¸­çš„ä¿¡æ¯æŸå¤±å’Œç‰©ç†ä¸ä¸€è‡´é—®é¢˜ï¼Œä¸ºå…¨å¤©å€™ç›‘æµ‹åº”ç”¨æä¾›äº†æ›´é²æ£’çš„çƒ­æˆåƒè¶…åˆ†è¾¨ç‡è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨è¯­ä¹‰åˆ†å‰²å’Œç›®æ ‡æ£€æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Optics-guided thermal UAV image super-resolution has attracted significant research interest due to its potential in all-weather monitoring applications. However, existing methods typically compress optical features to match thermal feature dimensions for cross-modal alignment and fusion, which not only causes the loss of high-frequency information that is beneficial for thermal super-resolution, but also introduces physically inconsistent artifacts such as texture distortions and edge blurring by overlooking differences in the imaging physics between modalities. To address these challenges, we propose PCNet to achieve cross-resolution mutual enhancement between optical and thermal modalities, while physically constraining the optical guidance process via thermal conduction to enable robust thermal UAV image super-resolution. In particular, we design a Cross-Resolution Mutual Enhancement Module (CRME) to jointly optimize thermal image super-resolution and optical-to-thermal modality conversion, facilitating effective bidirectional feature interaction across resolutions while preserving high-frequency optical priors. Moreover, we propose a Physics-Driven Thermal Conduction Module (PDTM) that incorporates two-dimensional heat conduction into optical guidance, modeling spatially-varying heat conduction properties to prevent inconsistent artifacts. In addition, we introduce a temperature consistency loss that enforces regional distribution consistency and boundary gradient smoothness to ensure generated thermal images align with real-world thermal radiation principles. Extensive experiments on VGTSR2.0 and DroneVehicle datasets demonstrate that PCNet significantly outperforms state-of-the-art methods on both reconstruction quality and downstream tasks including semantic segmentation and object detection.</p>
<h3 id="14-csmcir-cot-enhanced-symmetric-alignment-with-memory-bank-for-composed-image-retrieval">[14] <a href="https://arxiv.org/abs/2601.03728">CSMCIR: CoT-Enhanced Symmetric Alignment with Memory Bank for Composed Image Retrieval</a></h3>
<p><em>Zhipeng Qian, Zihan Liang, Yufei Ma, Ben Chen, Huangyu Dai, Yiwei Ma, Jiayi Ji, Chenyi Lei, Han Li, Xiaoshuai Sun</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCSMCIRï¼Œä¸€ç§ç”¨äºç»„åˆå›¾åƒæ£€ç´¢çš„ç»Ÿä¸€è¡¨ç¤ºæ¡†æ¶ï¼Œé€šè¿‡å¤šçº§æ€ç»´é“¾æç¤ºã€å¯¹ç§°åŒå¡”æ¶æ„å’ŒåŠ¨æ€è®°å¿†åº“ç­–ç•¥ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­è¡¨ç¤ºç©ºé—´ç¢ç‰‡åŒ–çš„é—®é¢˜ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ£€ç´¢æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç»„åˆå›¾åƒæ£€ç´¢æ–¹æ³•å­˜åœ¨è¡¨ç¤ºç©ºé—´ç¢ç‰‡åŒ–é—®é¢˜ï¼ŒæŸ¥è¯¢å’Œç›®æ ‡ç”±å¼‚æ„æ¨¡æ€ç»„æˆå¹¶ç”±ä¸åŒç¼–ç å™¨å¤„ç†ï¼Œè¿«ä½¿æ¨¡å‹ä»…é€šè¿‡äº‹åå¯¹é½æ¥æ¡¥æ¥æœªå¯¹é½çš„è¡¨ç¤ºç©ºé—´ï¼Œè¿™ä»æ ¹æœ¬ä¸Šé™åˆ¶äº†æ£€ç´¢æ€§èƒ½ã€‚è¿™ç§æ¶æ„ä¸å¯¹ç§°æ€§åœ¨ç‰¹å¾ç©ºé—´ä¸­è¡¨ç°ä¸ºä¸‰ä¸ªæ˜æ˜¾åˆ†ç¦»çš„èšç±»ï¼Œç›´æ¥å±•ç¤ºäº†å¼‚æ„æ¨¡æ€å¦‚ä½•ä»åˆå§‹åŒ–å¼€å§‹å°±åˆ›å»ºæ ¹æœ¬æœªå¯¹é½çš„è¡¨ç¤ºç©ºé—´ã€‚</p>
<p><strong>Method:</strong> CSMCIRæ¡†æ¶åŒ…å«ä¸‰ä¸ªååŒç»„ä»¶ï¼šé¦–å…ˆå¼•å…¥å¤šçº§æ€ç»´é“¾æç¤ºç­–ç•¥ï¼ŒæŒ‡å¯¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸ºç›®æ ‡å›¾åƒç”Ÿæˆå…·æœ‰åŒºåˆ†æ€§ä¸”è¯­ä¹‰å…¼å®¹çš„æ ‡é¢˜ï¼Œå»ºç«‹æ¨¡æ€å¯¹ç§°æ€§ï¼›å…¶æ¬¡è®¾è®¡å¯¹ç§°åŒå¡”æ¶æ„ï¼ŒæŸ¥è¯¢å’Œç›®æ ‡ä¸¤ä¾§ä½¿ç”¨ç›¸åŒçš„å…±äº«å‚æ•°Q-Formerè¿›è¡Œè·¨æ¨¡æ€ç¼–ç ï¼Œç¡®ä¿ä¸€è‡´çš„ç‰¹å¾è¡¨ç¤ºï¼›æœ€ååˆ©ç”¨æ¶æ„å¯¹ç§°æ€§å®ç°åŸºäºç†µçš„æ—¶åºåŠ¨æ€è®°å¿†åº“ç­–ç•¥ï¼Œæä¾›é«˜è´¨é‡è´Ÿæ ·æœ¬åŒæ—¶ä¿æŒä¸æ¼”åŒ–æ¨¡å‹çŠ¶æ€çš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCSMCIRå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½å¹¶å…·æœ‰ä¼˜è¶Šçš„è®­ç»ƒæ•ˆç‡ã€‚ç»¼åˆæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¯ä¸ªæå‡ºç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨ç»„åˆå›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ç»Ÿä¸€è¡¨ç¤ºæ¡†æ¶è§£å†³äº†ç»„åˆå›¾åƒæ£€ç´¢ä¸­çš„è¡¨ç¤ºç©ºé—´ç¢ç‰‡åŒ–é—®é¢˜ï¼Œå±•ç¤ºäº†æ¨¡æ€å¯¹ç§°æ€§æ¶æ„å’ŒåŠ¨æ€è´Ÿé‡‡æ ·ç­–ç•¥çš„é‡è¦æ€§ã€‚CSMCIRçš„æˆåŠŸè¡¨æ˜ï¼Œé€šè¿‡æ¶ˆé™¤å¼‚æ„ç¼–ç å™¨å¸¦æ¥çš„è¡¨ç¤ºç©ºé—´ä¸åŒ¹é…ï¼Œå¯ä»¥æ˜¾è‘—æå‡è·¨æ¨¡æ€æ£€ç´¢æ€§èƒ½ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿè®¾è®¡æä¾›äº†é‡è¦è§è§£ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Composed Image Retrieval (CIR) enables users to search for target images using both a reference image and manipulation text, offering substantial advantages over single-modality retrieval systems. However, existing CIR methods suffer from representation space fragmentation: queries and targets comprise heterogeneous modalities and are processed by distinct encoders, forcing models to bridge misaligned representation spaces only through post-hoc alignment, which fundamentally limits retrieval performance. This architectural asymmetry manifests as three distinct, well-separated clusters in the feature space, directly demonstrating how heterogeneous modalities create fundamentally misaligned representation spaces from initialization. In this work, we propose CSMCIR, a unified representation framework that achieves efficient query-target alignment through three synergistic components. First, we introduce a Multi-level Chain-of-Thought (MCoT) prompting strategy that guides Multimodal Large Language Models to generate discriminative, semantically compatible captions for target images, establishing modal symmetry. Building upon this, we design a symmetric dual-tower architecture where both query and target sides utilize the identical shared-parameter Q-Former for cross-modal encoding, ensuring consistent feature representations and further reducing the alignment gap. Finally, this architectural symmetry enables an entropy-based, temporally dynamic Memory Bank strategy that provides high-quality negative samples while maintaining consistency with the evolving model state. Extensive experiments on four benchmark datasets demonstrate that our CSMCIR achieves state-of-the-art performance with superior training efficiency. Comprehensive ablation studies further validate the effectiveness of each proposed component.</p>
<h3 id="15-spatialoc-leveraging-multi-level-spatial-enhanced-descriptors-for-cross-modal-localization">[15] <a href="https://arxiv.org/abs/2601.03579">SpatiaLoc: Leveraging Multi-Level Spatial Enhanced Descriptors for Cross-Modal Localization</a></h3>
<p><em>Tianyi Shang, Pengjie Xu, Zhaojun Deng, Zhenyu Li, Zhicong Chen, Lijun Wu</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SpatiaLocæ¡†æ¶ï¼Œé€šè¿‡ç²—åˆ°ç»†çš„ç­–ç•¥å¼ºè°ƒå®ä¾‹çº§å’Œå…¨å±€çº§çš„ç©ºé—´å…³ç³»ï¼Œç”¨äºåŸºäºæ–‡æœ¬å’Œç‚¹äº‘çš„è·¨æ¨¡æ€å®šä½ä»»åŠ¡ï¼Œåœ¨KITTI360Poseæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è·¨æ¨¡æ€å®šä½ä»»åŠ¡ä¸­ï¼Œæ–‡æœ¬å’Œç‚¹äº‘ä¹‹é—´çš„ç‰©ä½“ç»å¸¸é‡å¤å‡ºç°ï¼Œä½¿å¾—ç©ºé—´å…³ç³»æˆä¸ºæœ€å…·æœ‰åŒºåˆ†æ€§çš„å®šä½çº¿ç´¢ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æŒ–æ˜è¿™ä¸€ç‰¹æ€§ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„ç©ºé—´å…³ç³»å»ºæ¨¡æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> SpatiaLocé‡‡ç”¨ç²—åˆ°ç»†çš„åŒé˜¶æ®µç­–ç•¥ï¼Œç²—é˜¶æ®µåŒ…å«Bezierå¢å¼ºç‰©ä½“ç©ºé—´ç¼–ç å™¨ï¼ˆBEOSEï¼‰ä½¿ç”¨äºŒæ¬¡è´å¡å°”æ›²çº¿å»ºæ¨¡å®ä¾‹çº§ç©ºé—´å…³ç³»ï¼Œä»¥åŠé¢‘ç‡æ„ŸçŸ¥ç¼–ç å™¨ï¼ˆFAEï¼‰åœ¨é¢‘åŸŸç”Ÿæˆå…¨å±€çº§ç©ºé—´è¡¨ç¤ºï¼›ç»†é˜¶æ®µé‡‡ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥é«˜æ–¯ç²¾ç»†å®šä½å™¨ï¼ˆUGFLï¼‰ï¼Œå°†é¢„æµ‹å»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒå¹¶ä½¿ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥æŸå¤±å‡½æ•°å›å½’2Dä½ç½®ã€‚</p>
<p><strong>Result:</strong> åœ¨KITTI360Poseæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSpatiaLocæ¡†æ¶æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶ç©ºé—´å…³ç³»å»ºæ¨¡ç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åœ¨è·¨æ¨¡æ€å®šä½ä»»åŠ¡ä¸­ï¼ŒåŒæ—¶å…³æ³¨å®ä¾‹çº§å’Œå…¨å±€çº§çš„ç©ºé—´å…³ç³»å»ºæ¨¡è‡³å…³é‡è¦ï¼Œæå‡ºçš„ç²—åˆ°ç»†ç­–ç•¥å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥æ–¹æ³•ä¸ºæœºå™¨äººå¯¼èˆªå’Œäººæœºäº¤äº’ä¸­çš„è‡ªç„¶è¯­è¨€å®šä½æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Cross-modal localization using text and point clouds enables robots to localize themselves via natural language descriptions, with applications in autonomous navigation and interaction between humans and robots. In this task, objects often recur across text and point clouds, making spatial relationships the most discriminative cues for localization. Given this characteristic, we present SpatiaLoc, a framework utilizing a coarse-to-fine strategy that emphasizes spatial relationships at both the instance and global levels. In the coarse stage, we introduce a Bezier Enhanced Object Spatial Encoder (BEOSE) that models spatial relationships at the instance level using quadratic Bezier curves. Additionally, a Frequency Aware Encoder (FAE) generates spatial representations in the frequency domain at the global level. In the fine stage, an Uncertainty Aware Gaussian Fine Localizer (UGFL) regresses 2D positions by modeling predictions as Gaussian distributions with a loss function aware of uncertainty. Extensive experiments on KITTI360Pose demonstrate that SpatiaLoc significantly outperforms existing state-of-the-art (SOTA) methods.</p>
<h3 id="16-focusui-efficient-ui-grounding-via-position-preserving-visual-token-selection">[16] <a href="https://arxiv.org/abs/2601.03928">FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection</a></h3>
<p><em>Mingyu Ouyang, Kevin Qinghong Lin, Mike Zheng Shou, Hwee Tou Ng</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºFocusUIï¼Œä¸€ç§é«˜æ•ˆçš„UI groundingæ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©æ€§ä¿ç•™ä¸æŒ‡ä»¤ç›¸å…³çš„è§†è§‰tokenå¹¶ä¿æŒä½ç½®è¿ç»­æ€§ï¼Œæ˜¾è‘—é™ä½äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨UI groundingä»»åŠ¡ä¸­çš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†é«˜ç²¾åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡UIæˆªå›¾æ—¶ä¼šäº§ç”Ÿæ•°åƒä¸ªè§†è§‰tokenï¼Œå¯¼è‡´æ˜¾è‘—çš„è®¡ç®—å¼€é”€å’Œæ³¨æ„åŠ›ç¨€é‡Šï¼Œè€Œäººç±»é€šå¸¸åªå…³æ³¨æ„Ÿå…´è¶£åŒºåŸŸï¼Œå› æ­¤éœ€è¦å¼€å‘é«˜æ•ˆçš„UI groundingæ–¹æ³•æ¥è§£å†³è¿™ä¸€æ•ˆç‡é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> FocusUIæ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼šé¦–å…ˆé€šè¿‡èåˆæŒ‡ä»¤æ¡ä»¶åˆ†æ•°å’ŒåŸºäºè§„åˆ™çš„UIå›¾åˆ†æ•°æ„å»ºè¡¥ä¸çº§ç›‘ç£ï¼Œé€‰æ‹©å…·æœ‰åŒºåˆ†æ€§å’ŒæŒ‡ä»¤ç›¸å…³æ€§çš„è§†è§‰tokenï¼›å…¶æ¬¡æå‡ºPosPadç­–ç•¥ï¼Œå°†è¿ç»­ä¸¢å¼ƒçš„è§†è§‰tokenåºåˆ—å‹ç¼©ä¸ºæ”¾ç½®åœ¨åºåˆ—æœ€åç´¢å¼•çš„ç‰¹æ®Šæ ‡è®°ï¼Œä»¥ä¿æŒä½ç½®è¿ç»­æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªgroundingåŸºå‡†æµ‹è¯•ä¸­ï¼ŒFocusUIè¶…è¶Šäº†GUIç‰¹å®šåŸºçº¿æ–¹æ³•ï¼Œåœ¨ScreenSpot-ProåŸºå‡†ä¸Šï¼ŒFocusUI-7Bç›¸æ¯”GUI-Actor-7Bå®ç°äº†3.7%çš„æ€§èƒ½æå‡ï¼›å³ä½¿ä»…ä¿ç•™30%è§†è§‰tokenï¼Œæ€§èƒ½ä»…ä¸‹é™3.2%ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æå‡1.44å€ï¼Œå³°å€¼GPUå†…å­˜é™ä½17%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡é€‰æ‹©æ€§è§†è§‰tokenä¿ç•™å’Œä½ç½®è¿ç»­æ€§ä¿æŒï¼Œå¯ä»¥åœ¨UI groundingä»»åŠ¡ä¸­å®ç°æ˜¾è‘—çš„æ•ˆç‡æå‡è€Œä¸ç‰ºç‰²ç²¾åº¦ï¼Œä¸ºé«˜æ•ˆè§†è§‰è¯­è¨€æ¨¡å‹åº”ç”¨æä¾›äº†æ–°æ€è·¯ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦å¤„ç†é«˜åˆ†è¾¨ç‡UIç•Œé¢çš„å®é™…åº”ç”¨åœºæ™¯ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence's last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.</p>
<h3 id="17-raddiff-describing-differences-in-radiology-image-sets-with-natural-language">[17] <a href="https://arxiv.org/abs/2601.03733">RadDiff: Describing Differences in Radiology Image Sets with Natural Language</a></h3>
<p><em>Xiaoxian Shen, Yuhui Zhang, Sahithi Ankireddy, Xiaohan Wang, Maya Varma, Henry Guo, Curtis Langlotz, Serena Yeung-Levy</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†RadDiffï¼Œä¸€ç§å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿï¼Œèƒ½å¤Ÿæ‰§è¡Œæ”¾å°„ç§‘åŒ»ç”Ÿé£æ ¼çš„æ¯”è¾ƒæ¨ç†æ¥æè¿°æˆå¯¹æ”¾å°„å­¦ç ”ç©¶ä¹‹é—´çš„ä¸´åºŠæ„ä¹‰å·®å¼‚ï¼Œå¹¶æ„å»ºäº†RadDiffBenchåŸºå‡†æ¥ç³»ç»Ÿè¯„ä¼°æ”¾å°„å­¦å·®å¼‚å‘ç°æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç†è§£ä¸¤ç»„æ”¾å°„å­¦å›¾åƒä¹‹é—´çš„å·®å¼‚å¯¹äºç”Ÿæˆä¸´åºŠè§è§£å’Œè§£é‡ŠåŒ»å­¦AIç³»ç»Ÿè‡³å…³é‡è¦ï¼Œå½“å‰ç¼ºä¹èƒ½å¤Ÿç³»ç»Ÿå‘ç°æ”¾å°„å­¦æ•°æ®ä¸­æœ‰æ„ä¹‰å·®å¼‚çš„æ–¹æ³•å’ŒåŸºå‡†ã€‚</p>
<p><strong>Method:</strong> RadDiffåŸºäºVisDiffçš„æè®®è€…-æ’åºè€…æ¡†æ¶æ„å»ºï¼Œèåˆäº†å››é¡¹åˆ›æ–°ï¼šé€šè¿‡é¢†åŸŸé€‚åº”çš„è§†è§‰è¯­è¨€æ¨¡å‹æ³¨å…¥åŒ»å­¦çŸ¥è¯†ï¼›æ•´åˆå›¾åƒä¸ä¸´åºŠæŠ¥å‘Šçš„å¤šæ¨¡æ€æ¨ç†ï¼›è·¨å¤šè½®æ¨ç†çš„è¿­ä»£å‡è®¾ç»†åŒ–ï¼›ä»¥åŠå®šä½å¹¶æ”¾å¤§æ˜¾è‘—åŒºåŸŸä»¥æ•æ‰ç»†å¾®å‘ç°çš„é’ˆå¯¹æ€§è§†è§‰æœç´¢ã€‚</p>
<p><strong>Result:</strong> åœ¨åŒ…å«57ä¸ªä¸“å®¶éªŒè¯æ”¾å°„å­¦ç ”ç©¶å¯¹çš„RadDiffBenchåŸºå‡†ä¸Šï¼ŒRadDiffå®ç°äº†47%çš„å‡†ç¡®ç‡ï¼Œåœ¨çœŸå®æŠ¥å‘ŠæŒ‡å¯¼ä¸‹è¾¾åˆ°50%å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºé€šç”¨é¢†åŸŸVisDiffåŸºçº¿ï¼Œå¹¶å±•ç¤ºäº†åœ¨COVID-19è¡¨å‹æ¯”è¾ƒã€ç§æ—äºšç»„åˆ†æå’Œç”Ÿå­˜ç›¸å…³å½±åƒç‰¹å¾å‘ç°ç­‰å¤šæ ·åŒ–ä¸´åºŠä»»åŠ¡ä¸­çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Conclusion:</strong> RadDiffå’ŒRadDiffBenchå…±åŒä¸ºç³»ç»Ÿæ­ç¤ºæ”¾å°„å­¦æ•°æ®ä¸­æœ‰æ„ä¹‰çš„å·®å¼‚æä¾›äº†é¦–ä¸ªæ–¹æ³•-åŸºå‡†åŸºç¡€ï¼Œå±•ç¤ºäº†å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿåœ¨æ”¾å°„å­¦æ¯”è¾ƒåˆ†æä¸­çš„æ½œåŠ›ï¼Œä¸ºä¸´åºŠè§è§£ç”Ÿæˆå’ŒåŒ»å­¦AIç³»ç»Ÿè§£é‡Šå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff's versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.</p>
<h3 id="18-detecting-ai-generated-images-via-distributional-deviations-from-real-images">[18] <a href="https://arxiv.org/abs/2601.03586">Detecting AI-Generated Images via Distributional Deviations from Real Images</a></h3>
<p><em>Yakun Niu, Yingjian Chen, Lei Zhang</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ©ç é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒï¼ˆMPFTï¼‰çš„ç­–ç•¥ï¼Œé€šè¿‡çº¹ç†æ„ŸçŸ¥æ©ç ï¼ˆTAMï¼‰æœºåˆ¶åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ©ç åŒ…å«ç”Ÿæˆæ¨¡å‹ç‰¹å®šæ¨¡å¼çš„çº¹ç†åŒºåŸŸï¼Œä»è€Œå¢å¼ºCLIP-ViTå¯¹AIç”Ÿæˆå›¾åƒçš„æ£€æµ‹æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€ç”Ÿæˆæ¨¡å‹å¿«é€Ÿå‘å±•ï¼ŒAIç”Ÿæˆå›¾åƒè´¨é‡æ˜¾è‘—æå‡ï¼Œå¼•å‘äº†å…³äºé”™è¯¯ä¿¡æ¯å’Œå…¬ä¼—ä¿¡ä»»ä¾µèš€çš„æ‹…å¿§ã€‚æ£€æµ‹AIç”Ÿæˆå›¾åƒå·²æˆä¸ºå…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³›åŒ–åˆ°æœªè§ç”Ÿæˆæ¨¡å‹æ–¹é¢ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨å†»ç»“é¢„è®­ç»ƒCLIPæ¨¡å‹è™½åœ¨æ³›åŒ–æ–¹é¢æœ‰æ½œåŠ›ï¼Œä½†ä»…å°†å›¾åƒç¼–ç å™¨è§†ä¸ºåŸºæœ¬ç‰¹å¾æå–å™¨ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å…¶æ½œåŠ›ï¼Œä¸”ç¼ºä¹å¯¹çœŸå®ä¸AIç”Ÿæˆå›¾åƒçš„æœ‰æ•ˆåŒºåˆ†èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é¦–å…ˆå¯¹å†»ç»“CLIPå›¾åƒç¼–ç å™¨ï¼ˆCLIP-ViTï¼‰è¿›è¡Œæ·±å…¥åˆ†æï¼Œå‘ç°å…¶èƒ½åœ¨é«˜çº§æŠ½è±¡ç‰¹å¾ç©ºé—´ä¸­æœ‰æ•ˆèšç±»çœŸå®å›¾åƒï¼Œä½†ç¼ºä¹åŒºåˆ†çœŸå®ä¸AIç”Ÿæˆå›¾åƒçš„èƒ½åŠ›ã€‚åŸºäºæ­¤åˆ†æï¼Œæå‡ºäº†æ©ç é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒï¼ˆMPFTï¼‰ç­–ç•¥ï¼Œå¼•å…¥çº¹ç†æ„ŸçŸ¥æ©ç ï¼ˆTAMï¼‰æœºåˆ¶ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ©ç åŒ…å«ç”Ÿæˆæ¨¡å‹ç‰¹å®šæ¨¡å¼çš„çº¹ç†åŒºåŸŸã€‚è¿™ç§æ–¹æ³•è¿«ä½¿CLIP-ViTå…³æ³¨AIç”Ÿæˆå›¾åƒç›¸å¯¹äºçœŸå®å›¾åƒçš„"åˆ†å¸ƒåå·®"ï¼Œä»è€Œå®ç°å¢å¼ºçš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> åœ¨GenImageå’ŒUniversalFakeDetectæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä»…éœ€å°‘é‡å›¾åƒè¿›è¡Œå¾®è°ƒå³å¯æ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°98.2%å’Œ94.6%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶å“è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œæ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†CLIP-ViTåœ¨AIç”Ÿæˆå›¾åƒæ£€æµ‹ä¸­çš„æ½œåŠ›ä¸å±€é™ï¼Œå¹¶æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥æ¥å¢å¼ºå…¶æ³›åŒ–èƒ½åŠ›ã€‚çº¹ç†æ„ŸçŸ¥æ©ç æœºåˆ¶é€šè¿‡å…³æ³¨ç”Ÿæˆæ¨¡å‹ç‰¹å®šæ¨¡å¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´é²æ£’çš„åŒºåˆ†ç‰¹å¾ï¼Œä¸ºAIç”Ÿæˆå›¾åƒæ£€æµ‹æä¾›äº†æ–°çš„æŠ€æœ¯æ–¹å‘ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>The rapid advancement of generative models has significantly enhanced the quality of AI-generated images, raising concerns about misinformation and the erosion of public trust. Detecting AI-generated images has thus become a critical challenge, particularly in terms of generalizing to unseen generative models. Existing methods using frozen pre-trained CLIP models show promise in generalization but treat the image encoder as a basic feature extractor, failing to fully exploit its potential. In this paper, we perform an in-depth analysis of the frozen CLIP image encoder (CLIP-ViT), revealing that it effectively clusters real images in a high-level, abstract feature space. However, it does not truly possess the ability to distinguish between real and AI-generated images. Based on this analysis, we propose a Masking-based Pre-trained model Fine-Tuning (MPFT) strategy, which introduces a Texture-Aware Masking (TAM) mechanism to mask textured areas containing generative model-specific patterns during fine-tuning. This approach compels CLIP-ViT to attend to the "distributional deviations"from authentic images for AI-generated image detection, thereby achieving enhanced generalization performance. Extensive experiments on the GenImage and UniversalFakeDetect datasets demonstrate that our method, fine-tuned with only a minimal number of images, significantly outperforms existing approaches, achieving up to 98.2% and 94.6% average accuracy on the two datasets, respectively.</p>
<h3 id="19-analyzing-reasoning-consistency-in-large-multimodal-models-under-cross-modal-conflicts">[19] <a href="https://arxiv.org/abs/2601.04073">Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts</a></h3>
<p><em>Zhihao Zhu, Jiafeng Liang, Shixin Jiang, Jinlan Fu, Ming Liu, Guanglu Sun, See-Kiong Ng, Bing Qin</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºActive Visual-Context Refinementæ–¹æ³•ï¼Œé€šè¿‡ä¸»åŠ¨è§†è§‰é‡å®šä½æœºåˆ¶å’Œè‡ªé€‚åº”ä¸Šä¸‹æ–‡ç²¾ç‚¼ç­–ç•¥ï¼Œæœ‰æ•ˆç¼“è§£å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†é¢‘æ¨ç†ä¸­å‡ºç°çš„æ–‡æœ¬æƒ¯æ€§é—®é¢˜ï¼Œæ˜¾è‘—æŠ‘åˆ¶å¹»è§‰ä¼ æ’­å¹¶å¢å¼ºæ¨ç†é²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†é¢‘æ¨ç†ä¸­å±•ç°å‡ºå¼ºå¤§çš„é“¾å¼æ€ç»´èƒ½åŠ›ï¼Œä½†å…¶æ¨ç†é“¾çš„é²æ£’æ€§å­˜åœ¨ä¸¥é‡é—®é¢˜ã€‚ç ”ç©¶è¯†åˆ«å‡ºæ–‡æœ¬æƒ¯æ€§è¿™ä¸€å…³é”®å¤±æ•ˆæ¨¡å¼ï¼Œå³ä¸€æ—¦æ€ç»´è¿‡ç¨‹ä¸­å‡ºç°æ–‡æœ¬å¹»è§‰ï¼Œæ¨¡å‹å€¾å‘äºç›²ç›®éµå¾ªé”™è¯¯æ–‡æœ¬è€Œå¿½ç•¥å†²çªçš„è§†è§‰è¯æ®ï¼Œè¿™å¯¼è‡´é”™è¯¯åœ¨æ¨ç†é“¾ä¸­æŒç»­ä¼ æ’­ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é¦–å…ˆæå‡ºLogicGraph Perturbation Protocolï¼Œé€šè¿‡åœ¨å¤šæ ·åŒ–LMMçš„æ¨ç†é“¾ä¸­ç»“æ„æ€§æ³¨å…¥æ‰°åŠ¨æ¥ç³»ç»Ÿè¯„ä¼°å…¶è‡ªæˆ‘åæ€èƒ½åŠ›ã€‚ä¸ºç¼“è§£æ–‡æœ¬æƒ¯æ€§é—®é¢˜ï¼Œæå‡ºActive Visual-Context Refinementè¿™ä¸€æ— éœ€è®­ç»ƒçš„æ¨ç†èŒƒå¼ï¼Œè¯¥æ–¹æ³•åè°ƒä¸»åŠ¨è§†è§‰é‡å®šä½æœºåˆ¶ä»¥æ‰§è¡Œç»†ç²’åº¦éªŒè¯ï¼Œå¹¶ç»“åˆè‡ªé€‚åº”ä¸Šä¸‹æ–‡ç²¾ç‚¼ç­–ç•¥æ¥æ€»ç»“å’Œå»å™ªæ¨ç†å†å²ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨æ–‡æœ¬æƒ¯æ€§æƒ…å†µä¸‹æˆåŠŸè‡ªæˆ‘çº æ­£çš„æ¯”ä¾‹ä½äº10%ï¼Œä¸»è¦è¡¨ç°å‡ºç›²ç›®çš„æ–‡æœ¬é”™è¯¯ä¼ æ’­ã€‚æå‡ºçš„Active Visual-Context Refinementæ–¹æ³•æ˜¾è‘—æŠ‘åˆ¶äº†å¹»è§‰ä¼ æ’­ï¼Œæœ‰æ•ˆå¢å¼ºäº†æ¨ç†é²æ£’æ€§ï¼Œåœ¨å¤šç§LMMæ¶æ„å’Œæç¤ºé©±åŠ¨èŒƒå¼ä¸Šå‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ¨ç†é“¾ä¸­å­˜åœ¨çš„æ–‡æœ¬æƒ¯æ€§é—®é¢˜åŠå…¶ä¸¥é‡æ€§ï¼Œæå‡ºçš„è®­ç»ƒå…è´¹æ¨ç†èŒƒå¼ä¸ºè§£å†³è¯¥é—®é¢˜æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚ç ”ç©¶å¼ºè°ƒäº†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è§†è§‰è¯æ®éªŒè¯çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥å¼€å‘æ›´é²æ£’çš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæä¾›äº†é‡è¦è§è§£å’Œæ–¹æ³•è®ºåŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Large Multimodal Models (LMMs) have demonstrated impressive capabilities in video reasoning via Chain-of-Thought (CoT). However, the robustness of their reasoning chains remains questionable. In this paper, we identify a critical failure mode termed textual inertia, where once a textual hallucination occurs in the thinking process, models tend to blindly adhere to the erroneous text while neglecting conflicting visual evidence. To systematically investigate this, we propose the LogicGraph Perturbation Protocol that structurally injects perturbations into the reasoning chains of diverse LMMs spanning both native reasoning architectures and prompt-driven paradigms to evaluate their self-reflection capabilities. The results reveal that models successfully self-correct in less than 10% of cases and predominantly succumb to blind textual error propagation. To mitigate this, we introduce Active Visual-Context Refinement, a training-free inference paradigm which orchestrates an active visual re-grounding mechanism to enforce fine-grained verification coupled with an adaptive context refinement strategy to summarize and denoise the reasoning history. Experiments demonstrate that our approach significantly stifles hallucination propagation and enhances reasoning robustness.</p>
<h3 id="20-can-llms-see-without-pixels-benchmarking-spatial-intelligence-from-textual-descriptions">[20] <a href="https://arxiv.org/abs/2601.03590">Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions</a></h3>
<p><em>Zhongbin Guo, Zhen Yang, Yushan Li, Xinyue Zhang, Wenyu Gao, Jiacheng Wang, Chengzhi Li, Xiangrui Liu, Ping Jian</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†SiT-BenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°çº¯æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ™ºèƒ½èƒ½åŠ›ï¼Œé€šè¿‡å°†è§†è§‰åœºæ™¯è½¬æ¢ä¸ºåæ ‡æ„ŸçŸ¥çš„æ–‡æœ¬æè¿°ï¼Œæ­ç¤ºäº†å½“å‰LLMsåœ¨ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨çš„æ˜¾è‘—å·®è·ï¼Œå¹¶è¯æ˜äº†æ˜¾å¼ç©ºé—´æ¨ç†èƒ½æ˜¾è‘—æå‡æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç©ºé—´æ™ºèƒ½ç ”ç©¶ä¸»è¦ä¾èµ–è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä½†ä¸€ä¸ªå…³é”®é—®é¢˜å°šæœªè§£å†³ï¼šç©ºé—´ç†è§£èƒ½åŠ›ç©¶ç«Ÿæºäºè§†è§‰ç¼–ç å™¨è¿˜æ˜¯åŸºç¡€æ¨ç†æ¶æ„ï¼Ÿæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹åœ¨ç¼ºä¹åƒç´ çº§è¾“å…¥çš„æƒ…å†µä¸‹æ˜¯å¦å…·å¤‡ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶é‡åŒ–è¯„ä¼°å…¶ç©ºé—´æ™ºèƒ½è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†SiT-BenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¶…è¿‡3,800ä¸ªä¸“å®¶æ ‡æ³¨é¡¹ç›®ï¼Œæ¶µç›–5ä¸ªä¸»è¦ç±»åˆ«å’Œ17ä¸ªå­ä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªæˆ‘ä¸­å¿ƒå¯¼èˆªã€è§†è§’è½¬æ¢å’Œç²¾ç»†æœºå™¨äººæ“ä½œç­‰ã€‚é€šè¿‡å°†å•/å¤šè§†è§’åœºæ™¯è½¬æ¢ä¸ºé«˜ä¿çœŸã€åæ ‡æ„ŸçŸ¥çš„æ–‡æœ¬æè¿°ï¼Œè¿«ä½¿LLMsè¿›è¡Œç¬¦å·æ–‡æœ¬æ¨ç†è€Œéè§†è§‰æ¨¡å¼åŒ¹é…ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹å‘ç°ï¼Œè™½ç„¶æ¨¡å‹åœ¨å±€éƒ¨è¯­ä¹‰ä»»åŠ¡ä¸Šè¡¨ç°ç†Ÿç»ƒï¼Œä½†åœ¨å…¨å±€ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„"ç©ºé—´å·®è·"ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ˜¾å¼ç©ºé—´æ¨ç†èƒ½æ˜¾è‘—æå‡æ€§èƒ½ï¼Œè¡¨æ˜LLMså…·å¤‡æ½œåœ¨çš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> SiT-BenchåŸºå‡†æµ‹è¯•ä¸ºæœªæ¥è§†è§‰è¯­è¨€æ¨¡å‹å’Œå…·èº«æ™ºèƒ½ä½“çš„ç©ºé—´åŸºç¡€LLMæ¶æ„å¼€å‘æä¾›äº†åŸºç¡€èµ„æºã€‚ç ”ç©¶è¡¨æ˜å¤§è¯­è¨€æ¨¡å‹å…·æœ‰æ½œåœ¨çš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ï¼Œæ˜¾å¼ç©ºé—´æ¨ç†èƒ½æœ‰æ•ˆæå‡æ€§èƒ½ï¼Œä¸ºå¼€å‘ä¸ä¾èµ–è§†è§‰è¾“å…¥çš„ç©ºé—´æ™ºèƒ½ç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant "spatial gap" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .</p>
<h3 id="21-unveiling-text-in-challenging-stone-inscriptions-a-character-context-aware-patching-strategy-for-binarization">[21] <a href="https://arxiv.org/abs/2601.03609">Unveiling Text in Challenging Stone Inscriptions: A Character-Context-Aware Patching Strategy for Binarization</a></h3>
<p><em>Pratyush Jena, Amal Joseph, Arnav Sharma, Ravi Kiran Sarvadevabhatla</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå†å²çŸ³ç¢‘é“­æ–‡äºŒå€¼åŒ–çš„é²æ£’è‡ªé€‚åº”åˆ†å—ç­–ç•¥ï¼Œç»“åˆæ³¨æ„åŠ›U-Netæ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†åœ¨ä½å¯¹æ¯”åº¦ã€è¡¨é¢é€€åŒ–ä¸¥é‡æƒ…å†µä¸‹çš„äºŒå€¼åŒ–æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†è·¨æ–‡å­—çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> çŸ³ç¢‘é“­æ–‡å›¾åƒç”±äºèš€åˆ»å­—ç¬¦ä¸çŸ³æèƒŒæ™¯å¯¹æ¯”åº¦ä½ã€è¡¨é¢é€€åŒ–ä¸å‡åŒ€ã€å­˜åœ¨å¹²æ‰°ä¼ªå½±ä»¥åŠæ–‡å­—å¯†åº¦å’Œå¸ƒå±€é«˜åº¦å¯å˜ï¼Œç»™äºŒå€¼åŒ–å¸¦æ¥äº†ä¸¥å³»æŒ‘æˆ˜ï¼Œå¯¼è‡´ç°æœ‰äºŒå€¼åŒ–æŠ€æœ¯ç»å¸¸å¤±è´¥ä¸”éš¾ä»¥åˆ†ç¦»è¿è´¯çš„å­—ç¬¦åŒºåŸŸã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§é²æ£’çš„è‡ªé€‚åº”åˆ†å—ç­–ç•¥ç”¨äºå¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„å°åº¦æ–‡å­—é“­æ–‡ï¼Œåˆ©ç”¨åŠ¨æ€é‡‡æ ·å’Œåˆ†å—é€‰æ‹©æ–¹æ³•ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå…‹æœè¡¨é¢å™ªå£°å’Œå¸ƒå±€ä¸è§„åˆ™æ€§ï¼Œå¹¶ä½¿ç”¨è¿™äº›åˆ†å—è®­ç»ƒæ³¨æ„åŠ›U-Netæ¨¡å‹ï¼Œå…¶ä¸­æ³¨æ„åŠ›æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿèšç„¦äºç»†å¾®çš„ç»“æ„çº¿ç´¢ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œæ–°é¢–çš„åˆ†å—æœºåˆ¶æ˜¾è‘—æå‡äº†äºŒå€¼åŒ–æ€§èƒ½ï¼Œåœ¨ç»å…¸å’Œæ·±åº¦å­¦ä¹ åŸºçº¿ä¸Šå‡æœ‰æ˜æ˜¾æ”¹è¿›ï¼›å°½ç®¡ä»…åœ¨å•ä¸€æ–‡å­—çš„å°åº¦æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ¨¡å‹å±•ç°å‡ºå¯¹å…¶ä»–å°åº¦æ–‡å­—å’Œéå°åº¦æ–‡å­—çš„å¼ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå‡¸æ˜¾äº†å…¶é²æ£’æ€§å’Œæ–‡å­—æ— å…³çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå¹²å‡€ã€ç»“æ„åŒ–çš„é“­æ–‡å†…å®¹è¡¨ç¤ºï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡å¦‚æ–‡å­—è¯†åˆ«ã€OCRå’Œå†å²æ–‡æœ¬åˆ†æå¥ å®šäº†åŸºç¡€ï¼›ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªåœ¨å­—ç¬¦ç‰‡æ®µçº§åˆ«ç²¾å¿ƒæ ‡æ³¨çš„åƒç´ çº§ç²¾ç¡®å°åº¦çŸ³ç¢‘é“­æ–‡æ•°æ®é›†ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Binarization is a popular first step towards text extraction in historical artifacts. Stone inscription images pose severe challenges for binarization due to poor contrast between etched characters and the stone background, non-uniform surface degradation, distracting artifacts, and highly variable text density and layouts. These conditions frequently cause existing binarization techniques to fail and struggle to isolate coherent character regions. Many approaches sub-divide the image into patches to improve text fragment resolution and improve binarization performance. With this in mind, we present a robust and adaptive patching strategy to binarize challenging Indic inscriptions. The patches from our approach are used to train an Attention U-Net for binarization. The attention mechanism allows the model to focus on subtle structural cues, while our dynamic sampling and patch selection method ensures that the model learns to overcome surface noise and layout irregularities. We also introduce a carefully annotated, pixel-precise dataset of Indic stone inscriptions at the character-fragment level. We demonstrate that our novel patching mechanism significantly boosts binarization performance across classical and deep learning baselines. Despite training only on single script Indic dataset, our model exhibits strong zero-shot generalization to other Indic and non-indic scripts, highlighting its robustness and script-agnostic generalization capabilities. By producing clean, structured representations of inscription content, our method lays the foundation for downstream tasks such as script identification, OCR, and historical text analysis. Project page: https://ihdia.iiit.ac.in/shilalekhya-binarization/</p>
<h3 id="22-pixel-wise-multimodal-contrastive-learning-for-remote-sensing-images">[22] <a href="https://arxiv.org/abs/2601.04127">Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images</a></h3>
<p><em>Leandro Stival, Ricardo da Silva Torres, Helio Pedrini</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€è‡ªç›‘ç£æ–¹æ³•PIMCï¼Œé€šè¿‡å°†å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ—è½¬æ¢ä¸ºäºŒç»´é€’å½’å›¾è¡¨ç¤ºï¼Œå¹¶ç»“åˆé¥æ„Ÿå½±åƒè¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†åœ°çƒè§‚æµ‹ä»»åŠ¡ä¸­ç‰¹å¾æå–çš„æ•ˆæœã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å«æ˜ŸæŒç»­äº§ç”Ÿæµ·é‡åœ°çƒè§‚æµ‹æ•°æ®ï¼Œç‰¹åˆ«æ˜¯å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ—ï¼Œä½†ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹å¤§å¤šè®¾è®¡ç”¨äºå¤„ç†å®Œæ•´å›¾åƒæˆ–æ—¶é—´åºåˆ—ï¼Œç¼ºä¹å¯¹åƒç´ çº§æ—¶é—´åŠ¨æ€çš„æœ‰æ•ˆç¼–ç æ–¹æ³•ï¼Œè¿™é™åˆ¶äº†ä»SITSä¸­æå–æœ‰æ„ä¹‰çš„ç‰¹å¾ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºåƒç´ çº§äºŒç»´è¡¨ç¤ºçš„å¤šæ¨¡æ€æ–¹æ³•ï¼Œé¦–å…ˆå°†åŸºäºåƒç´ çš„æ¤è¢«æŒ‡æ•°æ—¶é—´åºåˆ—è½¬æ¢ä¸ºé€’å½’å›¾ä½œä¸ºåŸå§‹åƒç´ å€¼çš„æ›¿ä»£è¡¨ç¤ºï¼Œç„¶åå¼•å…¥PIxel-wise Multimodal Contrastiveè‡ªç›‘ç£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäºŒç»´åƒç´ æ—¶é—´åºåˆ—è¡¨ç¤ºå’Œé¥æ„Ÿå½±åƒï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ è®­ç»ƒæœ‰æ•ˆçš„ç¼–ç å™¨ã€‚</p>
<p><strong>Result:</strong> å®éªŒåœ¨ä¸‰ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼šä½¿ç”¨PASTISæ•°æ®é›†è¿›è¡Œåƒç´ çº§é¢„æµ‹å’Œåˆ†ç±»ï¼Œä»¥åŠåœ¨EuroSATæ•°æ®é›†ä¸Šè¿›è¡ŒåœŸåœ°è¦†ç›–åˆ†ç±»ã€‚ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒäºŒç»´è¡¨ç¤ºæ˜¾è‘—å¢å¼ºäº†ä»SITSä¸­æå–ç‰¹å¾çš„èƒ½åŠ›ï¼Œå¯¹æ¯”å­¦ä¹ æé«˜äº†åƒç´ æ—¶é—´åºåˆ—å’ŒRSIè¡¨ç¤ºçš„è´¨é‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€æ–¹æ³•åœ¨å¤„ç†å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ—å’Œé¥æ„Ÿå½±åƒæ–¹é¢çš„ä¼˜åŠ¿ï¼Œå»ºç«‹äº†ä¸€ä¸ªå¼ºå¤§çš„è‡ªç›‘ç£æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä¸¤ç§æ•°æ®ç±»å‹ï¼Œä¸ºåœ°çƒè§‚æµ‹ä»»åŠ¡æä¾›äº†æ›´æœ‰æ•ˆçš„ç‰¹å¾æå–è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on</p>
<h3 id="23-videomemory-toward-consistent-video-generation-via-memory-integration">[23] <a href="https://arxiv.org/abs/2601.03655">VideoMemory: Toward Consistent Video Generation via Memory Integration</a></h3>
<p><em>Jinsong Zhou, Yihua Du, Xinli Xu, Luozhou Wang, Zijie Zhuang, Yehang Zhang, Shuaibo Li, Xiaojun Hu, Bolan Su, Ying-cong Chen</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºVideoMemoryï¼Œä¸€ç§é¢å‘å®ä½“çš„è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è®°å¿†åº“å®ç°è·¨å¤šé•œå¤´å™äº‹è§†é¢‘ä¸­è§’è‰²ã€é“å…·å’ŒèƒŒæ™¯çš„ä¸€è‡´æ€§ä¿æŒï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨é•¿æ—¶ç¨‹ç”Ÿæˆä¸­å®ä½“èº«ä»½ä¸å¤–è§‚ä¿æŒçš„éš¾é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å™äº‹è§†é¢‘ç”Ÿæˆä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯åœ¨å¤šä¸ªé•œå¤´é—´ä¿æŒè§’è‰²ã€é“å…·å’Œç¯å¢ƒçš„èº«ä»½ä¸å¤–è§‚ä¸€è‡´æ€§ï¼Œç°æœ‰æ¨¡å‹è™½èƒ½ç”Ÿæˆé«˜è´¨é‡çŸ­è§†é¢‘ç‰‡æ®µï¼Œä½†åœ¨åœºæ™¯å˜åŒ–æˆ–å®ä½“ç»è¿‡é•¿æ—¶é—´é—´éš”åé‡æ–°å‡ºç°æ—¶ï¼Œå¾€å¾€æ— æ³•ç»´æŒå®ä½“çš„ä¸€è‡´è¡¨å¾ã€‚</p>
<p><strong>Method:</strong> VideoMemoryé‡‡ç”¨å®ä½“ä¸­å¿ƒåŒ–æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è®°å¿†åº“æ•´åˆå™äº‹è§„åˆ’ä¸è§†è§‰ç”Ÿæˆï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿå°†å™äº‹åˆ†è§£ä¸ºé•œå¤´åºåˆ—ï¼Œä»è®°å¿†åº“æ£€ç´¢å®ä½“è¡¨å¾ï¼Œå¹¶åŸºäºè¿™äº›æ£€ç´¢çŠ¶æ€åˆæˆå…³é”®å¸§å’Œè§†é¢‘ï¼Œè®°å¿†åº“å­˜å‚¨è§’è‰²ã€é“å…·å’ŒèƒŒæ™¯çš„æ˜¾å¼è§†è§‰ä¸è¯­ä¹‰æè¿°ç¬¦ï¼Œå¹¶åœ¨æ¯ä¸ªé•œå¤´åæ›´æ–°ä»¥åæ˜ æ•…äº‹é©±åŠ¨å˜åŒ–åŒæ—¶ä¿æŒèº«ä»½ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶æ„å»ºäº†åŒ…å«54ä¸ªæ¡ˆä¾‹çš„å¤šé•œå¤´ä¸€è‡´æ€§åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–è§’è‰²ã€é“å…·å’ŒèƒŒæ™¯æŒç»­æ€§åœºæ™¯ï¼Œå¤§é‡å®éªŒè¡¨æ˜VideoMemoryåœ¨å¤šæ ·åŒ–å™äº‹åºåˆ—ä¸­å®ç°äº†å¼ºå¤§çš„å®ä½“çº§è¿è´¯æ€§å’Œé«˜æ„ŸçŸ¥è´¨é‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡æ£€ç´¢-æ›´æ–°æœºåˆ¶å®ç°äº†è·¨è¿œè·ç¦»é•œå¤´çš„å®ä½“ä¸€è‡´æ€§æç»˜ï¼Œæ”¯æŒè¿è´¯çš„é•¿æ—¶ç¨‹è§†é¢‘ç”Ÿæˆï¼Œä¸ºå™äº‹è§†é¢‘ç”Ÿæˆä¸­çš„å®ä½“ä¸€è‡´æ€§ä¿æŒæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶è®¾ç«‹äº†æ–°çš„è¯„ä¼°åŸºå‡†ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Maintaining consistent characters, props, and environments across multiple shots is a central challenge in narrative video generation. Existing models can produce high-quality short clips but often fail to preserve entity identity and appearance when scenes change or when entities reappear after long temporal gaps. We present VideoMemory, an entity-centric framework that integrates narrative planning with visual generation through a Dynamic Memory Bank. Given a structured script, a multi-agent system decomposes the narrative into shots, retrieves entity representations from memory, and synthesizes keyframes and videos conditioned on these retrieved states. The Dynamic Memory Bank stores explicit visual and semantic descriptors for characters, props, and backgrounds, and is updated after each shot to reflect story-driven changes while preserving identity. This retrieval-update mechanism enables consistent portrayal of entities across distant shots and supports coherent long-form generation. To evaluate this setting, we construct a 54-case multi-shot consistency benchmark covering character-, prop-, and background-persistent scenarios. Extensive experiments show that VideoMemory achieves strong entity-level coherence and high perceptual quality across diverse narrative sequences.</p>
<h3 id="24-mgpc-multimodal-network-for-generalizable-point-cloud-completion-with-modality-dropout-and-progressive-decoding">[24] <a href="https://arxiv.org/abs/2601.03660">MGPC: Multimodal Network for Generalizable Point Cloud Completion With Modality Dropout and Progressive Decoding</a></h3>
<p><em>Jiangyuan Liu, Hongxuan Ma, Yuhao Zhao, Zhe Liu, Jian Wang, Wei Zou</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMGPCï¼Œä¸€ç§å¯æ³›åŒ–çš„å¤šæ¨¡æ€ç‚¹äº‘è¡¥å…¨æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆç‚¹äº‘ã€RGBå›¾åƒå’Œæ–‡æœ¬ï¼Œåœ¨ç»Ÿä¸€æ¶æ„ä¸­è§£å†³ç°æœ‰æ–¹æ³•åœ¨æ³›åŒ–åˆ°æ–°ç‰©ä½“å’ŒçœŸå®åœºæ™¯æ—¶çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ¨¡æ€ä¸¢å¼ƒç­–ç•¥ã€Transformerèåˆæ¨¡å—å’Œæ¸è¿›ç”Ÿæˆå™¨ï¼Œå¹¶åœ¨æ„å»ºçš„å¤§è§„æ¨¡æ•°æ®é›†MGPC-1Mä¸ŠéªŒè¯äº†å…¶ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºå­¦ä¹ çš„ç‚¹äº‘è¡¥å…¨æ–¹æ³•ï¼ˆåŒ…æ‹¬3D CNNã€åŸºäºç‚¹çš„æ–¹æ³•å’ŒTransformeræ–¹æ³•ï¼‰åœ¨åˆæˆåŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†ç”±äºæ¨¡æ€é™åˆ¶ã€å¯æ‰©å±•æ€§å’Œç”Ÿæˆèƒ½åŠ›ä¸è¶³ï¼Œå®ƒä»¬åœ¨æ–°ç‰©ä½“å’ŒçœŸå®åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€æ³›åŒ–æŒ‘æˆ˜ï¼Œå¼€å‘ä¸€ä¸ªèƒ½å¤Ÿé€‚åº”å¤šæ ·åŒ–çœŸå®ä¸–ç•Œæ¡ä»¶çš„é€šç”¨ç‚¹äº‘è¡¥å…¨æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> MGPCæ¡†æ¶æ•´åˆäº†ç‚¹äº‘ã€RGBå›¾åƒå’Œæ–‡æœ¬ä¸‰ç§æ¨¡æ€ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„æ¶æ„è®¾è®¡ã€‚å…³é”®æŠ€æœ¯åŒ…æ‹¬åˆ›æ–°çš„æ¨¡æ€ä¸¢å¼ƒç­–ç•¥ä»¥æé«˜é²æ£’æ€§ï¼ŒåŸºäºTransformerçš„èåˆæ¨¡å—å®ç°å¤šæ¨¡æ€ä¿¡æ¯æœ‰æ•ˆæ•´åˆï¼Œä»¥åŠæ–°é¢–çš„æ¸è¿›ç”Ÿæˆå™¨å¢å¼ºå‡ ä½•å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†è‡ªåŠ¨æ•°æ®ç”Ÿæˆæµç¨‹ï¼Œæ„å»ºäº†åŒ…å«è¶…è¿‡1000ä¸ªç±»åˆ«ã€100ä¸‡è®­ç»ƒå¯¹çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†MGPC-1Mã€‚</p>
<p><strong>Result:</strong> åœ¨MGPC-1Mæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•æŒç»­ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–ç±»åˆ«å’Œå¤æ‚åœºæ™¯ä¸­å‡è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€æ•´åˆå’Œæ‰€ææŠ€æœ¯ç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€æ•´åˆå¯¹äºç‚¹äº‘è¡¥å…¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›çš„é‡è¦æ€§ï¼Œæå‡ºçš„MGPCæ¡†æ¶ä¸ºè§£å†³çœŸå®ä¸–ç•Œç‚¹äº‘è¡¥å…¨æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚æ¨¡æ€ä¸¢å¼ƒç­–ç•¥å’Œæ¸è¿›ç”Ÿæˆå™¨ç­‰åˆ›æ–°æŠ€æœ¯ä¸ºæœªæ¥å¤šæ¨¡æ€3Dç†è§£ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒæ–¹å‘ï¼Œå¤§è§„æ¨¡æ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºè¯¥é¢†åŸŸçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Point cloud completion aims to recover complete 3D geometry from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and Transformer-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalization to novel objects and real-world scenarios remains challenging. In this paper, we propose MGPC, a generalizable multimodal point cloud completion framework that integrates point clouds, RGB images, and text within a unified architecture. MGPC introduces an innovative modality dropout strategy, a Transformer-based fusion module, and a novel progressive generator to improve robustness, scalability, and geometric modeling capability. We further develop an automatic data generation pipeline and construct MGPC-1M, a large-scale benchmark with over 1,000 categories and one million training pairs. Extensive experiments on MGPC-1M and in-the-wild data demonstrate that the proposed method consistently outperforms prior baselines and exhibits strong generalization under real-world conditions.</p>
<h3 id="25-breath-vl-vision-language-guided-6-dof-bronchoscopy-localization-via-semantic-geometric-fusion">[25] <a href="https://arxiv.org/abs/2601.03713">BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion</a></h3>
<p><em>Qingyao Tian, Bingyu Yang, Huai Liao, Xinyan Huang, Junyong Li, Dong Yi, Hongbin Liu</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†BREATH-VLï¼Œä¸€ç§å°†è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£ä¸è§†è§‰é…å‡†æ–¹æ³•çš„å‡ ä½•ä¿¡æ¯ç›¸ç»“åˆçš„æ··åˆæ¡†æ¶ï¼Œç”¨äºå†…çª¥é•œ6-DoFç›¸æœºå®šä½ï¼Œå¹¶åœ¨æ–°æ„å»ºçš„BREATHæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°†è§†è§‰è¯­è¨€æ¨¡å‹åº”ç”¨äº6-DoFå†…çª¥é•œç›¸æœºå®šä½é¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡ã€å¯†é›†æ ‡æ³¨ä¸”é¢å‘å®šä½çš„çœŸå®åŒ»ç–—åœºæ™¯è§†è§‰è¯­è¨€æ•°æ®é›†ï¼›ç»†ç²’åº¦å§¿æ€å›å½’èƒ½åŠ›æœ‰é™ï¼›ä»¥åŠä»å†å²å¸§æå–æ—¶åºç‰¹å¾æ—¶è®¡ç®—å»¶è¿Ÿé«˜ã€‚</p>
<p><strong>Method:</strong> é¦–å…ˆæ„å»ºäº†BREATHæ•°æ®é›†ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§è§„æ¨¡çš„ä½“å†…å†…çª¥é•œå®šä½æ•°æ®é›†ï¼Œé‡‡é›†è‡ªå¤æ‚çš„äººä½“æ°”é“ç¯å¢ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†BREATH-VLæ··åˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰çº¿ç´¢ä¸è§†è§‰é…å‡†æ–¹æ³•çš„å‡ ä½•ä¿¡æ¯ç›¸ç»“åˆè¿›è¡Œ6-DoFå§¿æ€ä¼°è®¡ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§è½»é‡çº§ä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶ï¼Œå°†è¿åŠ¨å†å²ç¼–ç ä¸ºè¯­è¨€æç¤ºï¼Œå®ç°é«˜æ•ˆçš„æ—¶åºæ¨ç†è€Œæ— éœ€æ˜‚è´µçš„è§†é¢‘çº§è®¡ç®—ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è§†è§‰è¯­è¨€æ¨¡å—åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ‰‹æœ¯åœºæ™¯ä¸­å®ç°äº†é²æ£’çš„è¯­ä¹‰å®šä½ã€‚BREATH-VLåœ¨å‡†ç¡®æ€§å’Œæ³›åŒ–æ€§æ–¹é¢å‡ä¼˜äºæœ€å…ˆè¿›çš„çº¯è§†è§‰å®šä½æ–¹æ³•ï¼Œä¸æœ€ä½³åŸºçº¿ç›¸æ¯”å¹³ç§»è¯¯å·®é™ä½äº†25.5%ï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„è®¡ç®—å»¶è¿Ÿã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†è§†è§‰è¯­è¨€æ¨¡å‹ä¸å‡ ä½•é…å‡†æ–¹æ³•åœ¨å†…çª¥é•œå®šä½ä¸­çš„äº’è¡¥ä¼˜åŠ¿ï¼šè§†è§‰è¯­è¨€æ¨¡å‹æä¾›å¯æ³›åŒ–çš„è¯­ä¹‰ç†è§£ï¼Œè€Œé…å‡†æ–¹æ³•æä¾›ç²¾ç¡®çš„å‡ ä½•å¯¹é½ã€‚æ‰€æå‡ºçš„è½»é‡çº§ä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶ä¸ºé«˜æ•ˆæ—¶åºæ¨ç†æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä¸ºåŒ»ç–—å¯¼èˆªå’Œå®šä½ä»»åŠ¡å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Vision-language models (VLMs) have recently shown remarkable performance in navigation and localization tasks by leveraging large-scale pretraining for semantic understanding. However, applying VLMs to 6-DoF endoscopic camera localization presents several challenges: 1) the lack of large-scale, high-quality, densely annotated, and localization-oriented vision-language datasets in real-world medical settings; 2) limited capability for fine-grained pose regression; and 3) high computational latency when extracting temporal features from past frames. To address these issues, we first construct BREATH dataset, the largest in-vivo endoscopic localization dataset to date, collected in the complex human airway. Building on this dataset, we propose BREATH-VL, a hybrid framework that integrates semantic cues from VLMs with geometric information from vision-based registration methods for accurate 6-DoF pose estimation. Our motivation lies in the complementary strengths of both approaches: VLMs offer generalizable semantic understanding, while registration methods provide precise geometric alignment. To further enhance the VLM's ability to capture temporal context, we introduce a lightweight context-learning mechanism that encodes motion history as linguistic prompts, enabling efficient temporal reasoning without expensive video-level computation. Extensive experiments demonstrate that the vision-language module delivers robust semantic localization in challenging surgical scenes. Building on this, our BREATH-VL outperforms state-of-the-art vision-only localization methods in both accuracy and generalization, reducing translational error by 25.5% compared with the best-performing baseline, while achieving competitive computational latency.</p>
<h3 id="26-i2e-from-image-pixels-to-actionable-interactive-environments-for-text-guided-image-editing">[26] <a href="https://arxiv.org/abs/2601.03741">I2E: From Image Pixels to Actionable Interactive Environments for Text-Guided Image Editing</a></h3>
<p><em>Jinghan Yu, Junhao Xiao, Chenyu Zhu, Jiaming Li, Jia Li, HanMing Deng, Xirui Wang, Guoli Jia, Jianjun Li, Zhiyuan Ma, Xiang Bai, Bowen Zhou</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºI2Eï¼Œä¸€ç§æ–°é¢–çš„"åˆ†è§£-è¡ŒåŠ¨"èŒƒå¼ï¼Œå°†å›¾åƒç¼–è¾‘é‡æ–°å®šä¹‰ä¸ºç»“æ„åŒ–ç¯å¢ƒä¸­çš„å¯æ“ä½œäº¤äº’è¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚ç»„åˆç¼–è¾‘ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºæ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ–¹æ³•ä¸»è¦ä¾èµ–ç«¯åˆ°ç«¯çš„åƒç´ çº§ä¿®å¤èŒƒå¼ï¼Œåœ¨éœ€è¦ç²¾ç¡®å±€éƒ¨æ§åˆ¶å’Œå¤æ‚å¤šå¯¹è±¡ç©ºé—´æ¨ç†çš„ç»„åˆç¼–è¾‘ä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå…·ä½“è¡¨ç°ä¸ºè§„åˆ’ä¸æ‰§è¡Œçš„éšå¼è€¦åˆã€ç¼ºä¹å¯¹è±¡çº§æ§åˆ¶ç²’åº¦ä»¥åŠå¯¹éç»“æ„åŒ–åƒç´ ä¸­å¿ƒå»ºæ¨¡çš„ä¾èµ–ã€‚</p>
<p><strong>Method:</strong> I2Eé‡‡ç”¨"åˆ†è§£-è¡ŒåŠ¨"èŒƒå¼ï¼Œé¦–å…ˆä½¿ç”¨åˆ†è§£å™¨å°†éç»“æ„åŒ–å›¾åƒè½¬æ¢ä¸ºç¦»æ•£å¯æ“ä½œçš„å¯¹è±¡å±‚ï¼Œç„¶åå¼•å…¥ç‰©ç†æ„ŸçŸ¥çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ä»£ç†ï¼Œé€šè¿‡æ€ç»´é“¾æ¨ç†å°†å¤æ‚æŒ‡ä»¤è§£æä¸ºä¸€ç³»åˆ—åŸå­æ“ä½œï¼Œå®ç°äº†è§„åˆ’ä¸æ‰§è¡Œçš„è§£è€¦ã€‚</p>
<p><strong>Result:</strong> åœ¨I2E-BenchåŸºå‡†æµ‹è¯•å’Œå¤šä¸ªå…¬å…±åŸºå‡†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒI2Eåœ¨å¤„ç†å¤æ‚ç»„åˆæŒ‡ä»¤ã€ä¿æŒç‰©ç†åˆç†æ€§å’Œç¡®ä¿å¤šè½®ç¼–è¾‘ç¨³å®šæ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥èŒƒå¼åœ¨å¤æ‚ç¼–è¾‘ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ç»“æ„åŒ–åˆ†è§£å’Œå¯æ“ä½œäº¤äº’çš„èŒƒå¼è½¬å˜ï¼Œä¸ºå¤æ‚å›¾åƒç¼–è¾‘æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¼ºè°ƒäº†å¯¹è±¡çº§æ§åˆ¶ã€ç‰©ç†æ„ŸçŸ¥æ¨ç†å’Œè§„åˆ’-æ‰§è¡Œè§£è€¦çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥å¯è§£é‡Šå’Œå¯æ§çš„è§†è§‰ç¼–è¾‘ç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Existing text-guided image editing methods primarily rely on end-to-end pixel-level inpainting paradigm. Despite its success in simple scenarios, this paradigm still significantly struggles with compositional editing tasks that require precise local control and complex multi-object spatial reasoning. This paradigm is severely limited by 1) the implicit coupling of planning and execution, 2) the lack of object-level control granularity, and 3) the reliance on unstructured, pixel-centric modeling. To address these limitations, we propose I2E, a novel "Decompose-then-Action" paradigm that revisits image editing as an actionable interaction process within a structured environment. I2E utilizes a Decomposer to transform unstructured images into discrete, manipulable object layers and then introduces a physics-aware Vision-Language-Action Agent to parse complex instructions into a series of atomic actions via Chain-of-Thought reasoning. Further, we also construct I2E-Bench, a benchmark designed for multi-instance spatial reasoning and high-precision editing. Experimental results on I2E-Bench and multiple public benchmarks demonstrate that I2E significantly outperforms state-of-the-art methods in handling complex compositional instructions, maintaining physical plausibility, and ensuring multi-turn editing stability.</p>
<h3 id="27-hemblip-a-vision-language-model-for-interpretable-leukemia-cell-morphology-analysis">[27] <a href="https://arxiv.org/abs/2601.03915">HemBLIP: A Vision-Language Model for Interpretable Leukemia Cell Morphology Analysis</a></h3>
<p><em>Julie van Logtestijn, Petru Manescu</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†HemBLIPï¼Œä¸€ç§ç”¨äºç”Ÿæˆå¤–å‘¨è¡€ç»†èƒå¯è§£é‡Šå½¢æ€æè¿°çš„å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç™½è¡€ç—…è¯Šæ–­ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹é»‘ç®±é—®é¢˜å¹¶æå‡ä¸´åºŠä¿¡ä»»åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç”¨äºç™½è¡€ç—…è¯Šæ–­çš„ç™½ç»†èƒå½¢æ€å­¦è¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸ä½œä¸ºé»‘ç®±è¿è¡Œï¼Œè¿™é™åˆ¶äº†ä¸´åºŠä¿¡ä»»å’Œå®é™…åº”ç”¨ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šã€å½¢æ€æ„ŸçŸ¥æè¿°çš„ç³»ç»Ÿæ¥æå‡è¯Šæ–­é€æ˜åº¦ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†åŒ…å«14,000ä¸ªå¥åº·ä¸ç™½è¡€ç—…ç»†èƒåŠå…¶ä¸“å®¶æ ‡æ³¨å±æ€§æè¿°çš„æ–°æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ä¸¤ç§ç­–ç•¥å¾®è°ƒé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼šå®Œæ•´å¾®è°ƒå’ŒåŸºäºLoRAçš„å‚æ•°é«˜æ•ˆè®­ç»ƒï¼ŒåŒæ—¶ä»¥ç”Ÿç‰©åŒ»å­¦åŸºç¡€æ¨¡å‹MedGEMMAä½œä¸ºåŸºå‡†è¿›è¡Œå¯¹æ¯”è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> HemBLIPåœ¨æè¿°è´¨é‡å’Œå½¢æ€å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºåŸºå‡†æ¨¡å‹MedGEMMAï¼Œè€ŒLoRAé€‚é…æ–¹æ³•åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½è¡¨ç°ï¼Œå®ç°äº†è®¡ç®—æ•ˆç‡ä¸æ¨¡å‹æ€§èƒ½çš„å¹³è¡¡ä¼˜åŒ–ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¡€æ¶²å­¦è¯Šæ–­ä¸­å®ç°é€æ˜åŒ–å’Œå¯æ‰©å±•åº”ç”¨çš„æ½œåŠ›ï¼Œä¸ºå¼€å‘ä¸´åºŠå¯ä¿¡çš„AIè¾…åŠ©è¯Šæ–­å·¥å…·æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å°¤å…¶é€‚ç”¨äºåŒ»ç–—é¢†åŸŸçš„å®é™…éƒ¨ç½²éœ€æ±‚ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Microscopic evaluation of white blood cell morphology is central to leukemia diagnosis, yet current deep learning models often act as black boxes, limiting clinical trust and adoption. We introduce HemBLIP, a vision language model designed to generate interpretable, morphology aware descriptions of peripheral blood cells. Using a newly constructed dataset of 14k healthy and leukemic cells paired with expert-derived attribute captions, we adapt a general-purpose VLM via both full fine-tuning and LoRA based parameter efficient training, and benchmark against the biomedical foundation model MedGEMMA. HemBLIP achieves higher caption quality and morphological accuracy, while LoRA adaptation provides further gains with significantly reduced computational cost. These results highlight the promise of vision language models for transparent and scalable hematological diagnostics.</p>
<h3 id="28-georeason-aligning-thinking-and-answering-in-remote-sensing-vision-language-models-via-logical-consistency-reinforcement-learning">[28] <a href="https://arxiv.org/abs/2601.04118">GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning</a></h3>
<p><em>Wenshuai Li, Xiantai Xiang, Zixiao Wen, Guangyao Zhou, Ben Niu, Feng Wang, Lijia Huang, Qiantong Wang, Yuxin Hu</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†GeoReasonæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é¥æ„Ÿè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„é€»è¾‘å¹»è§‰é—®é¢˜ï¼Œé€šè¿‡åŒæ­¥å†…éƒ¨æ¨ç†ä¸æœ€ç»ˆå†³ç­–æ¥å¢å¼ºç©ºé—´ä»»åŠ¡ä¸­çš„è®¤çŸ¥å¯é æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç»“åˆç›‘ç£çŸ¥è¯†åˆå§‹åŒ–å’Œä¸€è‡´æ€§æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰é¥æ„Ÿè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç©ºé—´ä»»åŠ¡ä¸­å­˜åœ¨é€»è¾‘å¹»è§‰é—®é¢˜ï¼Œå³æ­£ç¡®ç­”æ¡ˆæºäºé”™è¯¯æ¨ç†é“¾æˆ–ä¾èµ–ä½ç½®æ·å¾„è€Œéç©ºé—´é€»è¾‘ï¼Œè¿™ç§æ¨ç†ä¸å†³ç­–çš„è„±é’©ä¸¥é‡å½±å“äº†æˆ˜ç•¥ç©ºé—´å†³ç­–çš„å¯é æ€§ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤ŸåŒæ­¥å†…éƒ¨æ€è€ƒä¸æœ€ç»ˆå†³ç­–çš„æ¡†æ¶æ¥å¢å¼ºè®¤çŸ¥å¯é æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†GeoReasonæ¡†æ¶ï¼Œé¦–å…ˆæ„å»ºäº†åŒ…å«4000ä¸ªæ¨ç†è½¨è¿¹çš„é€»è¾‘é©±åŠ¨æ•°æ®é›†GeoReason-Benchï¼Œè¯¥æ•°æ®é›†åŸºäºå‡ ä½•åŸºå…ƒå’Œä¸“å®¶çŸ¥è¯†åˆæˆï¼›ç„¶åè®¾è®¡äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µä¸ºç›‘ç£çŸ¥è¯†åˆå§‹åŒ–ï¼Œä½¿æ¨¡å‹æŒæ¡æ¨ç†è¯­æ³•å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼›ç¬¬äºŒé˜¶æ®µä¸ºä¸€è‡´æ€§æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡æ–°é¢–çš„é€»è¾‘ä¸€è‡´æ€§å¥–åŠ±æœºåˆ¶ï¼Œé‡‡ç”¨é€‰é¡¹æ’åˆ—ç­–ç•¥æƒ©ç½šé€»è¾‘æ¼‚ç§»ï¼Œå°†å†³ç­–é”šå®šåœ¨å¯éªŒè¯çš„æ¨ç†è½¨è¿¹ä¸Šã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒGeoReasonæ¡†æ¶æ˜¾è‘—æå‡äº†é¥æ„Ÿè§†è§‰è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥å¯é æ€§å’Œå¯è§£é‡Šæ€§ï¼Œåœ¨å„é¡¹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œç›¸æ¯”å…¶ä»–å…ˆè¿›æ–¹æ³•å±•ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼ŒéªŒè¯äº†åŒæ­¥æ¨ç†ä¸å†³ç­–ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†ä»æ„ŸçŸ¥ä¸ºä¸­å¿ƒè¯†åˆ«å‘é«˜çº§æ¼”ç»æ¨ç†è½¬å˜çš„é‡è¦æ€§ï¼Œæå‡ºçš„GeoReasonæ¡†æ¶é€šè¿‡é€»è¾‘ä¸€è‡´æ€§çº¦æŸæœ‰æ•ˆè§£å†³äº†é¥æ„Ÿè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†-å†³ç­–è„±é’©é—®é¢˜ï¼Œä¸ºå¤æ‚ç©ºé—´ä»»åŠ¡ä¸­çš„å¯é å†³ç­–æä¾›äº†æ–°èŒƒå¼ï¼Œå¹¶ä¸ºæœªæ¥æ›´å¯é çš„ç©ºé—´è®¤çŸ¥ç³»ç»Ÿå¼€å‘æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>The evolution of Remote Sensing Vision-Language Models(RS-VLMs) emphasizes the importance of transitioning from perception-centric recognition toward high-level deductive reasoning to enhance cognitive reliability in complex spatial tasks. However, current models often suffer from logical hallucinations, where correct answers are derived from flawed reasoning chains or rely on positional shortcuts rather than spatial logic. This decoupling undermines reliability in strategic spatial decision-making. To address this, we present GeoReason, a framework designed to synchronize internal thinking with final decisions. We first construct GeoReason-Bench, a logic-driven dataset containing 4,000 reasoning trajectories synthesized from geometric primitives and expert knowledge. We then formulate a two-stage training strategy: (1) Supervised Knowledge Initialization to equip the model with reasoning syntax and domain expertise, and (2) Consistency-Aware Reinforcement Learning to refine deductive reliability. This second stage integrates a novel Logical Consistency Reward, which penalizes logical drift via an option permutation strategy to anchor decisions in verifiable reasoning traces. Experimental results demonstrate that our framework significantly enhances the cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.</p>
<h3 id="29-diffusion-drf-differentiable-reward-flow-for-video-diffusion-fine-tuning">[29] <a href="https://arxiv.org/abs/2601.04153">Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning</a></h3>
<p><em>Yifan Wang, Yanyu Li, Sergey Tulyakov, Yun Fu, Anil Kag</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDiffusion-DRFï¼Œä¸€ç§ç”¨äºè§†é¢‘æ‰©æ•£æ¨¡å‹å¾®åˆ†çš„å¥–åŠ±æµæ–¹æ³•ï¼Œé€šè¿‡å†»ç»“çš„è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºå…è®­ç»ƒæ‰¹è¯„å™¨ï¼Œç›´æ¥åå‘ä¼ æ’­åé¦ˆä»¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡ï¼Œæ— éœ€é¢å¤–å¥–åŠ±æ¨¡å‹æˆ–åå¥½æ•°æ®é›†ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºç›´æ¥åå¥½ä¼˜åŒ–çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•ä¾èµ–äºä¸å¯å¾®åˆ†çš„äººç±»æ ‡æ³¨æˆ–å­¦ä¹ å‹å¥–åŠ±æ¨¡å‹ä¿¡å·ï¼Œå¯¼è‡´è®­ç»ƒæ ‡ç­¾å¯†é›†ã€æ˜“äº§ç”Ÿåå·®ä¸”å®¹æ˜“è§¦å‘å¥–åŠ±æ”»å‡»å’Œä¸ç¨³å®šè®­ç»ƒï¼Œéœ€è¦ä¸€ç§æ›´ç¨³å®šä¸”å…è®­ç»ƒå¥–åŠ±çš„ä¼˜åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> Diffusion-DRFä½¿ç”¨å†»ç»“çš„ç°æˆè§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºå…è®­ç»ƒæ‰¹è¯„å™¨ï¼Œé€šè¿‡æ‰©æ•£å»å™ªé“¾ç›´æ¥åå‘ä¼ æ’­VLMåé¦ˆï¼Œå°†logitçº§å“åº”è½¬æ¢ä¸ºtokenæ„ŸçŸ¥æ¢¯åº¦è¿›è¡Œä¼˜åŒ–ï¼›æå‡ºè‡ªåŠ¨åŒ–ã€é¢å‘æ–¹é¢çš„æç¤ºç®¡é“è·å–å¯é çš„å¤šç»´VLMåé¦ˆï¼ŒåŒæ—¶ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯å®ç°æœ€ç»ˆå»å™ªæ­¥éª¤çš„é«˜æ•ˆæ›´æ–°ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•æé«˜äº†è§†é¢‘è´¨é‡å’Œè¯­ä¹‰å¯¹é½ï¼ŒåŒæ—¶ç¼“è§£äº†å¥–åŠ±æ”»å‡»å’Œå´©æºƒé—®é¢˜ï¼Œæ— éœ€é¢å¤–çš„å¥–åŠ±æ¨¡å‹æˆ–åå¥½æ•°æ®é›†ï¼›å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå¯è½»æ¾æ¨å¹¿åˆ°å…¶ä»–åŸºäºæ‰©æ•£çš„ç”Ÿæˆä»»åŠ¡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å±•ç¤ºäº†ä½¿ç”¨å†»ç»“VLMä½œä¸ºå…è®­ç»ƒæ‰¹è¯„å™¨çš„å¯è¡Œæ€§ï¼Œä¸ºæ‰©æ•£æ¨¡å‹ä¼˜åŒ–æä¾›äº†æ›´ç¨³å®šå’Œé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œé¿å…äº†ä¼ ç»Ÿåå¥½ä¼˜åŒ–æ–¹æ³•ä¸­çš„æ ‡ç­¾ä¾èµ–å’Œåå·®é—®é¢˜ï¼Œä¸ºç”Ÿæˆæ¨¡å‹å¯¹é½å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment. However, current methods rely on non-differentiable preference signals from human annotations or learned reward models. This reliance makes training label-intensive, bias-prone, and easy-to-game, which often triggers reward hacking and unstable training. We propose Diffusion-DRF, a differentiable reward flow for fine-tuning video diffusion models using a frozen, off-the-shelf Vision-Language Model (VLM) as a training-free critic. Diffusion-DRF directly backpropagates VLM feedback through the diffusion denoising chain, converting logit-level responses into token-aware gradients for optimization. We propose an automated, aspect-structured prompting pipeline to obtain reliable multi-dimensional VLM feedback, while gradient checkpointing enables efficient updates through the final denoising steps. Diffusion-DRF improves video quality and semantic alignment while mitigating reward hacking and collapse -- without additional reward models or preference datasets. It is model-agnostic and readily generalizes to other diffusion-based generative tasks.</p>
<h3 id="30-totmnet-fft-accelerated-toeplitz-temporal-mixing-network-for-lightweight-remote-photoplethysmography">[30] <a href="https://arxiv.org/abs/2601.04159">ToTMNet: FFT-Accelerated Toeplitz Temporal Mixing Network for Lightweight Remote Photoplethysmography</a></h3>
<p><em>Vladimir Frants, Sos Agaian, Karen Panetta</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºToTMNetï¼Œä¸€ç§è½»é‡çº§è¿œç¨‹å…‰ç”µå®¹ç§¯æè®°ï¼ˆrPPGï¼‰æ¶æ„ï¼Œä½¿ç”¨FFTåŠ é€Ÿçš„Toeplitzæ—¶åºæ··åˆå±‚æ›¿ä»£æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦å’Œå‚æ•°é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡æ·±åº¦å­¦ä¹ æ–¹æ³•æé«˜äº†è¿œç¨‹å…‰ç”µå®¹ç§¯æè®°ï¼ˆrPPGï¼‰çš„é²æ£’æ€§ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸è®¡ç®—æˆæœ¬é«˜ã€å‚æ•°é‡å¤§ï¼Œä¸”åŸºäºæ³¨æ„åŠ›çš„æ—¶åºå»ºæ¨¡å­˜åœ¨ä¸æ—¶é—´é•¿åº¦å¹³æ–¹ç›¸å…³çš„è®¡ç®—å¤æ‚åº¦é—®é¢˜ï¼Œéœ€è¦æ›´é«˜æ•ˆçš„æ—¶åºå»ºæ¨¡æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> ToTMNeté‡‡ç”¨FFTåŠ é€Ÿçš„Toeplitzæ—¶åºæ··åˆå±‚æ›¿ä»£ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡å¾ªç¯åµŒå…¥å’ŒFFTå·ç§¯å®ç°è¿‘çº¿æ€§æ—¶é—´è®¡ç®—ï¼›è¯¥æ¶æ„æ•´åˆå…¨å±€Toeplitzæ—¶åºç®—å­åˆ°ç´§å‡‘çš„é—¨æ§æ—¶åºæ··åˆå™¨ä¸­ï¼Œç»“åˆå±€éƒ¨æ·±åº¦æ—¶åºå·ç§¯åˆ†æ”¯ä¸é—¨æ§å…¨å±€Toeplitzæ··åˆï¼Œä»…éœ€63kå‚æ•°å³å¯å®ç°é«˜æ•ˆé•¿ç¨‹æ—¶åºæ»¤æ³¢ã€‚</p>
<p><strong>Result:</strong> åœ¨UBFC-rPPGæ•°æ®é›†ä¸Šï¼ŒToTMNetè¾¾åˆ°1.055 bpmçš„å¹³å‡ç»å¯¹è¯¯å·®å’Œ0.996çš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼›åœ¨åˆæˆåˆ°çœŸå®åœºæ™¯è¿ç§»ï¼ˆSCAMPSåˆ°UBFC-rPPGï¼‰ä¸­ï¼Œè¾¾åˆ°1.582 bpm MAEå’Œ0.994ç›¸å…³ç³»æ•°ï¼›æ¶ˆèå®éªŒè¯å®é—¨æ§æœºåˆ¶å¯¹æœ‰æ•ˆåˆ©ç”¨å…¨å±€Toeplitzæ··åˆè‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨é¢†åŸŸåç§»æƒ…å†µä¸‹ã€‚</p>
<p><strong>Conclusion:</strong> Toeplitzç»“æ„çš„æ—¶åºæ··åˆæ˜¯rPPGä¸­æ³¨æ„åŠ›æœºåˆ¶çš„é«˜æ•ˆå®ç”¨æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦ï¼›é—¨æ§æœºåˆ¶å¯¹è·¨é¢†åŸŸæ³›åŒ–è‡³å…³é‡è¦ï¼Œä¸ºè½»é‡çº§æ—¶åºå»ºæ¨¡æä¾›äº†æ–°æ€è·¯ï¼Œä½†ç ”ç©¶å—é™äºä»…ä½¿ç”¨ä¸¤ä¸ªæ•°æ®é›†ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Remote photoplethysmography (rPPG) estimates a blood volume pulse (BVP) waveform from facial videos captured by commodity cameras. Although recent deep models improve robustness compared to classical signal-processing approaches, many methods increase computational cost and parameter count, and attention-based temporal modeling introduces quadratic scaling with respect to the temporal length. This paper proposes ToTMNet, a lightweight rPPG architecture that replaces temporal attention with an FFT-accelerated Toeplitz temporal mixing layer. The Toeplitz operator provides full-sequence temporal receptive field using a linear number of parameters in the clip length and can be applied in near-linear time using circulant embedding and FFT-based convolution. ToTMNet integrates the global Toeplitz temporal operator into a compact gated temporal mixer that combines a local depthwise temporal convolution branch with gated global Toeplitz mixing, enabling efficient long-range temporal filtering while only having 63k parameters. Experiments on two datasets, UBFC-rPPG (real videos) and SCAMPS (synthetic videos), show that ToTMNet achieves strong heart-rate estimation accuracy with a compact design. On UBFC-rPPG intra-dataset evaluation, ToTMNet reaches 1.055 bpm MAE with Pearson correlation 0.996. In a synthetic-to-real setting (SCAMPS to UBFC-rPPG), ToTMNet reaches 1.582 bpm MAE with Pearson correlation 0.994. Ablation results confirm that the gating mechanism is important for effectively using global Toeplitz mixing, especially under domain shift. The main limitation of this preprint study is the use of only two datasets; nevertheless, the results indicate that Toeplitz-structured temporal mixing is a practical and efficient alternative to attention for rPPG.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="31-advances-and-challenges-in-semantic-textual-similarity-a-comprehensive-survey">[31] <a href="https://arxiv.org/abs/2601.03270">Advances and Challenges in Semantic Textual Similarity: A Comprehensive Survey</a></h3>
<p><em>Lokendra Kumar, Neelesh S. Upadhye, Kannan Piedy</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æ˜¯ä¸€ç¯‡å…³äºè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ï¼ˆSTSï¼‰ç ”ç©¶çš„ç»¼è¿°æ€§è®ºæ–‡ï¼Œç³»ç»Ÿå›é¡¾äº†è‡ª2021å¹´ä»¥æ¥è¯¥é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œæ¶µç›–äº†åŸºäºTransformerçš„æ¨¡å‹ã€å¯¹æ¯”å­¦ä¹ ã€é¢†åŸŸç‰¹å®šæ–¹æ³•ã€å¤šæ¨¡æ€æ–¹æ³•ã€å›¾åŸºæ–¹æ³•å’ŒçŸ¥è¯†å¢å¼ºæŠ€æœ¯ç­‰å…­ä¸ªå…³é”®é¢†åŸŸã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç»¼è¿°æ—¨åœ¨è§£å†³è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ç ”ç©¶é¢†åŸŸè‡ª2021å¹´ä»¥æ¥å¿«é€Ÿæ‰©å¼ ä½†ç¼ºä¹ç³»ç»Ÿæ€§æ•´ç†çš„é—®é¢˜ï¼Œé€šè¿‡å…¨é¢å›é¡¾å’Œåˆ†ç±»æœ€æ–°è¿›å±•ï¼Œä¸ºç ”ç©¶è€…å’Œå®è·µè€…æä¾›å¯¼èˆªï¼Œå¸®åŠ©ä»–ä»¬ç†è§£å½“å‰æ–¹æ³•ã€å®é™…åº”ç”¨å’Œå‰©ä½™æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> è¯¥ç»¼è¿°é‡‡ç”¨ç³»ç»Ÿæ€§æ–‡çŒ®å›é¡¾æ–¹æ³•ï¼Œå°†STSç ”ç©¶è¿›å±•ç»„ç»‡ä¸ºå…­ä¸ªå…³é”®é¢†åŸŸï¼šåŸºäºTransformerçš„æ¨¡å‹ï¼ˆå¦‚FarSSiBERTå’ŒDeBERTa-v3ï¼‰ã€å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚AspectCSEï¼‰ã€é¢†åŸŸç‰¹å®šè§£å†³æ–¹æ¡ˆï¼ˆå¦‚CXR-BERTç”¨äºåŒ»å­¦æ–‡æœ¬å’ŒFinancial-STSç”¨äºé‡‘èï¼‰ã€å¤šæ¨¡æ€æ–¹æ³•ã€å›¾åŸºæ–¹æ³•ä»¥åŠçŸ¥è¯†å¢å¼ºæŠ€æœ¯ã€‚</p>
<p><strong>Result:</strong> ç»¼è¿°å‘ç°æœ€è¿‘çš„Transformeræ¨¡å‹å¦‚FarSSiBERTå’ŒDeBERTa-v3å–å¾—äº†æ˜¾è‘—å‡†ç¡®ç‡ï¼Œè€Œå¯¹æ¯”å­¦ä¹ æ–¹æ³•å¦‚AspectCSEå»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œé¢†åŸŸé€‚åº”æ¨¡å‹å¦‚CXR-BERTå’ŒFinancial-STSå±•ç¤ºäº†STSåœ¨ä¸“ä¸šé¢†åŸŸä¸­çš„æœ‰æ•ˆå®šåˆ¶èƒ½åŠ›ï¼Œå¤šæ¨¡æ€ã€å›¾åŸºå’ŒçŸ¥è¯†é›†æˆæ¨¡å‹è¿›ä¸€æ­¥å¢å¼ºäº†è¯­ä¹‰ç†è§£å’Œè¡¨ç¤ºèƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç»¼è¿°ä¸ºç ”ç©¶è€…å’Œå®è·µè€…æä¾›äº†å½“å‰STSæ–¹æ³•çš„å®è´µè§è§£ï¼Œçªå‡ºäº†æ–°å…´è¶‹åŠ¿å’Œæœªæ¥æœºä¼šï¼Œé€šè¿‡ç³»ç»Ÿç»„ç»‡å’Œåˆ†æè¿™äº›å‘å±•ï¼Œå¸®åŠ©å¯¼èˆªå¿«é€Ÿè¿›å±•çš„STSé¢†åŸŸï¼Œå¹¶è¯†åˆ«äº†å®é™…åº”ç”¨å’Œå‰©ä½™æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Semantic Textual Similarity (STS) research has expanded rapidly since 2021, driven by advances in transformer architectures, contrastive learning, and domain-specific techniques. This survey reviews progress across six key areas: transformer-based models, contrastive learning, domain-focused solutions, multi-modal methods, graph-based approaches, and knowledge-enhanced techniques. Recent transformer models such as FarSSiBERT and DeBERTa-v3 have achieved remarkable accuracy, while contrastive methods like AspectCSE have established new benchmarks. Domain-adapted models, including CXR-BERT for medical texts and Financial-STS for finance, demonstrate how STS can be effectively customized for specialized fields. Moreover, multi-modal, graph-based, and knowledge-integrated models further enhance semantic understanding and representation. By organizing and analyzing these developments, the survey provides valuable insights into current methods, practical applications, and remaining challenges. It aims to guide researchers and practitioners alike in navigating rapid advancements, highlighting emerging trends and future opportunities in the evolving field of STS.</p>
<h3 id="32-prompting-underestimates-llm-capability-for-time-series-classification">[32] <a href="https://arxiv.org/abs/2601.03464">Prompting Underestimates LLM Capability for Time Series Classification</a></h3>
<p><em>Dan Schumacher, Erfan Nourbakhsh, Rocky Slavin, Anthony Rios</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„åŸå› åœ¨äºæç¤ºè¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ï¼Œè€Œéæ¨¡å‹æœ¬èº«ç¼ºä¹æ—¶é—´ç»“æ„è¡¨å¾èƒ½åŠ›ã€‚é€šè¿‡çº¿æ€§æ¢é’ˆåˆ†æå‘ç°ï¼ŒLLMså†…éƒ¨ç¡®å®ç¼–ç äº†æœ‰æ„ä¹‰çš„æ—¶é—´åºåˆ—ä¿¡æ¯ï¼Œå…¶æ€§èƒ½å¯ä¸ä¸“ç”¨æ—¶é—´åºåˆ—æ¨¡å‹ç›¸åª²ç¾ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºæç¤ºçš„è¯„ä¼°æ–¹æ³•æ˜¾ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè¿™å¼•å‘äº†å…³äºLLMsæ˜¯å¦çœŸæ­£ç¼–ç æœ‰æ„ä¹‰æ—¶é—´ç»“æ„çš„ç–‘é—®ã€‚è¯¥ç ”ç©¶æ—¨åœ¨æ¢ç©¶è¿™ç§è¡¨ç°ä¸ä½³æ˜¯ç”±äºæ¨¡å‹è¡¨å¾èƒ½åŠ›ä¸è¶³ï¼Œè¿˜æ˜¯æç¤ºè¯„ä¼°æ–¹æ³•æœ¬èº«çš„å±€é™æ€§æ‰€è‡´ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨çº¿æ€§æ¢é’ˆæ–¹æ³•ç›´æ¥åˆ†æLLMså†…éƒ¨è¡¨å¾ï¼Œå¹¶ä¸åŸºäºæç¤ºçš„ç”Ÿæˆæ–¹æ³•è¿›è¡Œå¯¹æ¯”ã€‚é€šè¿‡å±‚é—´åˆ†ææŠ€æœ¯è¿½è¸ªæ—¶é—´åºåˆ—ä¿¡æ¯åœ¨ä¸åŒTransformerå±‚ä¸­çš„æ¼”åŒ–è¿‡ç¨‹ï¼Œå¹¶è€ƒå¯Ÿè§†è§‰å’Œå¤šæ¨¡æ€è¾“å…¥å¯¹æ—¶é—´åºåˆ—è¡¨å¾çš„å¢å¼ºæ•ˆåº”ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œé›¶æ ·æœ¬æç¤ºæ–¹æ³•çš„å¹³å‡F1åˆ†æ•°ä»…ä¸º0.15-0.26ï¼Œæ¥è¿‘éšæœºæ°´å¹³ï¼Œè€Œçº¿æ€§æ¢é’ˆæ–¹æ³•å°†å¹³å‡F1æ˜¾è‘—æå‡è‡³0.61-0.67ã€‚å±‚é—´åˆ†æè¡¨æ˜ï¼Œç±»åˆ«åŒºåˆ†æ€§çš„æ—¶é—´åºåˆ—ä¿¡æ¯åœ¨æ—©æœŸTransformerå±‚ä¸­å·²å¼€å§‹å‡ºç°ï¼Œä¸”è§†è§‰å’Œå¤šæ¨¡æ€è¾“å…¥èƒ½å¤Ÿè¿›ä¸€æ­¥æ”¾å¤§è¿™ç§ä¿¡æ¯è¡¨å¾ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰åŸºäºæç¤ºçš„è¯„ä¼°æ–¹æ³•ä¸LLMså†…éƒ¨è¡¨å¾èƒ½åŠ›ä¹‹é—´å­˜åœ¨ç³»ç»Ÿæ€§ä¸åŒ¹é…ï¼Œå¯¼è‡´ç°æœ‰è¯„ä¼°ä¸¥é‡ä½ä¼°äº†æ¨¡å‹å¯¹æ—¶é—´åºåˆ—çš„ç†è§£èƒ½åŠ›ã€‚è¿™ä¸€å‘ç°å¯¹å¦‚ä½•å‡†ç¡®è¯„ä¼°LLMsçš„æ—¶åºç†è§£èƒ½åŠ›å…·æœ‰é‡è¦å¯ç¤ºï¼Œå¹¶è¡¨æ˜éœ€è¦å¼€å‘æ›´æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•æ¥æ­ç¤ºæ¨¡å‹çš„å®é™…è¡¨å¾èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Prompt-based evaluations suggest that large language models (LLMs) perform poorly on time series classification, raising doubts about whether they encode meaningful temporal structure. We show that this conclusion reflects limitations of prompt-based generation rather than the model's representational capacity by directly comparing prompt outputs with linear probes over the same internal representations. While zero-shot prompting performs near chance, linear probes improve average F1 from 0.15-0.26 to 0.61-0.67, often matching or exceeding specialized time series models. Layer-wise analyses further show that class-discriminative time series information emerges in early transformer layers and is amplified by visual and multimodal inputs. Together, these results demonstrate a systematic mismatch between what LLMs internally represent and what prompt-based evaluation reveals, leading current evaluations to underestimate their time series understanding.</p>
<h3 id="33-mem-gallery-benchmarking-multimodal-long-term-conversational-memory-for-mllm-agents">[33] <a href="https://arxiv.org/abs/2601.03515">Mem-Gallery: Benchmarking Multimodal Long-Term Conversational Memory for MLLM Agents</a></h3>
<p><em>Yuanchen Bei, Tianxin Wei, Xuying Ning, Yanjun Zhao, Zhining Liu, Xiao Lin, Yada Zhu, Hendrik Hamann, Jingrui He, Hanghang Tong</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Mem-Galleryï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»£ç†åœ¨å¤šè½®å¯¹è¯ä¸­é•¿æ—¶è®°å¿†èƒ½åŠ›çš„æ–°åŸºå‡†ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„è¯„ä¼°æ¡†æ¶æ¥é‡åŒ–è®°å¿†æå–ã€æ¨ç†å’ŒçŸ¥è¯†ç®¡ç†ä¸‰ä¸ªç»´åº¦çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºå‡†è¦ä¹ˆè¯„ä¼°çº¯æ–‡æœ¬å¯¹è¯ä¸­çš„å¤šä¼šè¯è®°å¿†ï¼Œè¦ä¹ˆè¯„ä¼°å±€éƒ¨ä¸Šä¸‹æ–‡ä¸­çš„å¤šæ¨¡æ€ç†è§£ï¼Œç¼ºä¹å¯¹å¤šæ¨¡æ€è®°å¿†åœ¨é•¿æœŸå¯¹è¯è½¨è¿¹ä¸­å¦‚ä½•è¢«ä¿å­˜ã€ç»„ç»‡å’Œæ¼”åŒ–çš„ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œè¿™é™åˆ¶äº†MLLMä»£ç†åœ¨çœŸå®ä¸–ç•Œäº¤äº’ä¸­çš„é•¿æœŸè®°å¿†èƒ½åŠ›å‘å±•ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶è€…æ„å»ºäº†Mem-GalleryåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«åŸºäºè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„é«˜è´¨é‡å¤šä¼šè¯å¯¹è¯ï¼Œå…·æœ‰é•¿äº¤äº’è§†é‡å’Œä¸°å¯Œçš„å¤šæ¨¡æ€ä¾èµ–å…³ç³»ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ï¼Œä»è®°å¿†æå–ä¸æµ‹è¯•æ—¶é€‚åº”ã€è®°å¿†æ¨ç†ä»¥åŠè®°å¿†çŸ¥è¯†ç®¡ç†ä¸‰ä¸ªåŠŸèƒ½ç»´åº¦è¯„ä¼°å…³é”®è®°å¿†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨åä¸‰ä¸ªè®°å¿†ç³»ç»Ÿä¸Šçš„å¹¿æ³›åŸºå‡†æµ‹è¯•æ­ç¤ºäº†å‡ ä¸ªå…³é”®å‘ç°ï¼šæ˜ç¡®çš„å¤šæ¨¡æ€ä¿¡æ¯ä¿ç•™å’Œè®°å¿†ç»„ç»‡çš„å¿…è¦æ€§ã€è®°å¿†æ¨ç†å’ŒçŸ¥è¯†ç®¡ç†æ–¹é¢çš„æŒç»­å±€é™æ€§ï¼Œä»¥åŠå½“å‰æ¨¡å‹çš„æ•ˆç‡ç“¶é¢ˆï¼Œè¿™äº›å‘ç°é‡åŒ–äº†ç°æœ‰MLLMåœ¨é•¿æ—¶å¤šæ¨¡æ€è®°å¿†æ–¹é¢çš„èƒ½åŠ›å·®è·ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘èƒ½å¤Ÿæœ‰æ•ˆä¿ç•™ã€ç»„ç»‡å’Œæ¼”åŒ–å¤šæ¨¡æ€è®°å¿†çš„MLLMä»£ç†çš„é‡è¦æ€§ï¼ŒMem-GalleryåŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„è¯„ä¼°å·¥å…·ï¼Œæ­ç¤ºäº†å½“å‰ç³»ç»Ÿåœ¨è®°å¿†æ¨ç†å’ŒçŸ¥è¯†ç®¡ç†æ–¹é¢çš„ä¸è¶³ï¼Œä¸ºæ”¹è¿›é•¿æ—¶å¤šæ¨¡æ€è®°å¿†èƒ½åŠ›æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Long-term memory is a critical capability for multimodal large language model (MLLM) agents, particularly in conversational settings where information accumulates and evolves over time. However, existing benchmarks either evaluate multi-session memory in text-only conversations or assess multimodal understanding within localized contexts, failing to evaluate how multimodal memory is preserved, organized, and evolved across long-term conversational trajectories. Thus, we introduce Mem-Gallery, a new benchmark for evaluating multimodal long-term conversational memory in MLLM agents. Mem-Gallery features high-quality multi-session conversations grounded in both visual and textual information, with long interaction horizons and rich multimodal dependencies. Building on this dataset, we propose a systematic evaluation framework that assesses key memory capabilities along three functional dimensions: memory extraction and test-time adaptation, memory reasoning, and memory knowledge management. Extensive benchmarking across thirteen memory systems reveals several key findings, highlighting the necessity of explicit multimodal information retention and memory organization, the persistent limitations in memory reasoning and knowledge management, as well as the efficiency bottleneck of current models.</p>
<h3 id="34-e5-omni-explicit-cross-modal-alignment-for-omni-modal-embeddings">[34] <a href="https://arxiv.org/abs/2601.03666">e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings</a></h3>
<p><em>Haonan Chen, Sicheng Gao, Radu Timofte, Tetsuya Sakai, Zhicheng Dou</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºe5-omniï¼Œä¸€ç§è½»é‡çº§æ˜¾å¼å¯¹é½æ–¹æ³•ï¼Œå°†ç°æˆçš„è§†è§‰è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºç¨³å¥çš„å…¨æ¨¡æ€åµŒå…¥æ¨¡å‹ï¼Œé€šè¿‡æ¸©åº¦æ ¡å‡†ã€å¯æ§è´Ÿæ ·æœ¬è¯¾ç¨‹å’Œæ‰¹ç™½åŒ–æŠ€æœ¯è§£å†³è·¨æ¨¡æ€å¯¹é½ä¸­çš„ä¸‰ä¸ªå¸¸è§é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å…¨æ¨¡æ€åµŒå…¥æ¨¡å‹ä¸¥é‡ä¾èµ–é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„éšå¼å¯¹é½ï¼Œå¯¼è‡´ä¸‰ä¸ªå®é™…é—®é¢˜ï¼šç›¸ä¼¼åº¦åˆ†æ•°å­˜åœ¨æ¨¡æ€ä¾èµ–çš„é”åº¦å·®å¼‚ï¼Œè·¨æ¨¡æ€æ‰¹æ¬¡ä¸­è´Ÿæ ·æœ¬ç¡¬åº¦åˆ†å¸ƒä¸å‡è¡¡ä½¿å¾—è®¸å¤šè´Ÿæ ·æœ¬è¿…é€Ÿå˜å¾—æ— å…³ç´§è¦ï¼Œä»¥åŠè·¨æ¨¡æ€åµŒå…¥çš„ä¸€é˜¶å’ŒäºŒé˜¶ç»Ÿè®¡é‡ä¸åŒ¹é…å¯¼è‡´æ’åºä¸ç¨³å®šã€‚</p>
<p><strong>Method:</strong> e5-omniåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæ¨¡æ€æ„ŸçŸ¥æ¸©åº¦æ ¡å‡†ä»¥å¯¹é½ç›¸ä¼¼åº¦å°ºåº¦ï¼Œå¯æ§è´Ÿæ ·æœ¬è¯¾ç¨‹ä¸å»åæŠ€æœ¯ä»¥èšç„¦æ··æ·†è´Ÿæ ·æœ¬åŒæ—¶å‡å°‘å‡è´Ÿæ ·æœ¬å½±å“ï¼Œä»¥åŠåæ–¹å·®æ­£åˆ™åŒ–çš„æ‰¹ç™½åŒ–æŠ€æœ¯ä»¥æ›´å¥½åœ°åŒ¹é…å…±äº«åµŒå…¥ç©ºé—´ä¸­çš„è·¨æ¨¡æ€å‡ ä½•ç»“æ„ã€‚</p>
<p><strong>Result:</strong> åœ¨MMEB-V2å’ŒAudioCapsåŸºå‡†æµ‹è¯•ä¸­ï¼Œe5-omniç›¸æ¯”å¼ºåŒæ¨¡æ€å’Œå…¨æ¨¡æ€åŸºçº¿æ¨¡å‹å–å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œä¸”è¯¥é…æ–¹èƒ½å¤Ÿè‰¯å¥½åœ°è¿ç§»åˆ°å…¶ä»–è§†è§‰è¯­è¨€æ¨¡å‹éª¨å¹²ç½‘ç»œä¸Šï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜é€šè¿‡è½»é‡çº§æ˜¾å¼å¯¹é½æŠ€æœ¯å¯ä»¥æœ‰æ•ˆè§£å†³å…¨æ¨¡æ€åµŒå…¥ä¸­çš„è·¨æ¨¡æ€ä¸ä¸€è‡´é—®é¢˜ï¼Œä¸ºå°†ç°æˆè§†è§‰è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºç¨³å¥çš„å…¨æ¨¡æ€æ£€ç´¢ç³»ç»Ÿæä¾›äº†å®ç”¨æ–¹æ¡ˆï¼ŒåŒæ—¶é‡Šæ”¾çš„æ¨¡å‹æ£€æŸ¥ç‚¹ä¸ºåç»­ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„åŸºå‡†ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.</p>
<h3 id="35-airnav-a-large-scale-real-world-uav-vision-and-language-navigation-dataset-with-natural-and-diverse-instructions">[35] <a href="https://arxiv.org/abs/2601.03707">AirNav: A Large-Scale Real-World UAV Vision-and-Language Navigation Dataset with Natural and Diverse Instructions</a></h3>
<p><em>Hengxing Cai, Yijie Rao, Ligang Huang, Zanyang Zhong, Jinhan Dong, Jingjun Tan, Wenhao Lu, Renxin Zhong</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†AirNavâ€”â€”ä¸€ä¸ªåŸºäºçœŸå®åŸå¸‚èˆªæ‹æ•°æ®æ„å»ºçš„å¤§è§„æ¨¡æ— äººæœºè§†è§‰è¯­è¨€å¯¼èˆªåŸºå‡†æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†AirVLN-R1æ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å¾®è°ƒç›¸ç»“åˆçš„æ–¹æ³•æå‡äº†æ— äººæœºè§†è§‰è¯­è¨€å¯¼èˆªçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ— äººæœºè§†è§‰è¯­è¨€å¯¼èˆªæ•°æ®é›†å­˜åœ¨ä¾èµ–è™šæ‹Ÿç¯å¢ƒã€æŒ‡ä»¤ç¼ºä¹è‡ªç„¶æ€§ä»¥åŠè§„æ¨¡æœ‰é™ç­‰é—®é¢˜ï¼Œè¿™é™åˆ¶äº†æ— äººæœºåœ¨çœŸå®åœºæ™¯ä¸­çš„å¯¼èˆªèƒ½åŠ›ç ”ç©¶å’Œåº”ç”¨ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†AirNavåŸºå‡†æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŸºäºçœŸå®åŸå¸‚èˆªæ‹æ•°æ®è€Œéåˆæˆç¯å¢ƒï¼ŒåŒ…å«è‡ªç„¶å¤šæ ·çš„æŒ‡ä»¤ï¼›åŒæ—¶æå‡ºäº†AirVLN-R1æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å¾®è°ƒä¸¤ç§è®­ç»ƒç­–ç•¥æ¥æå‡æ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡çœŸå®ä¸–ç•Œæµ‹è¯•å¯¹æ¨¡å‹å¯è¡Œæ€§è¿›è¡Œäº†åˆæ­¥è¯„ä¼°ï¼Œç ”ç©¶å›¢é˜Ÿå…¬å¼€äº†æ•°æ®é›†å’Œä»£ç ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†å¯å¤ç°çš„åŸºç¡€ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡çœŸå®æ•°æ®æ„å»ºçš„å¤§è§„æ¨¡åŸºå‡†å’Œæ··åˆè®­ç»ƒç­–ç•¥ï¼Œä¸ºè§£å†³æ— äººæœºè§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„ç¯å¢ƒçœŸå®æ€§å’ŒæŒ‡ä»¤è‡ªç„¶æ€§é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸå‘çœŸå®åœºæ™¯åº”ç”¨çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Existing Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) datasets face issues such as dependence on virtual environments, lack of naturalness in instructions, and limited scale. To address these challenges, we propose AirNav, a large-scale UAV VLN benchmark constructed from real urban aerial data, rather than synthetic environments, with natural and diverse instructions. Additionally, we introduce the AirVLN-R1, which combines Supervised Fine-Tuning and Reinforcement Fine-Tuning to enhance performance and generalization. The feasibility of the model is preliminarily evaluated through real-world tests. Our dataset and code are publicly available.</p>
<h3 id="36-when-helpers-become-hazards-a-benchmark-for-analyzing-multimodal-llm-powered-safety-in-daily-life">[36] <a href="https://arxiv.org/abs/2601.04043">When Helpers Become Hazards: A Benchmark for Analyzing Multimodal LLM-Powered Safety in Daily Life</a></h3>
<p><em>Xinyue Lou, Jinan Xu, Jingyi Yin, Xiaolong Wang, Zhaolu Kang, Youwei Liao, Yixuan Wang, Xiangyu Shi, Fengran Mo, Su Yao, Kaiyu Huang</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SaLADï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«2,013ä¸ªçœŸå®ä¸–ç•Œå›¾åƒ-æ–‡æœ¬æ ·æœ¬ï¼Œè¦†ç›–10ä¸ªå¸¸è§ç±»åˆ«ï¼Œå¹¶æå‡ºäº†åŸºäºå®‰å…¨è­¦å‘Šçš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ›¿ä»£é€šç”¨çš„æ‹’ç»å“åº”ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æˆä¸ºäººç±»ç”Ÿæ´»ä¸­ä¸å¯æˆ–ç¼ºçš„åŠ©æ‰‹ï¼Œå…¶ç”Ÿæˆçš„ä¸å®‰å…¨å†…å®¹å¯¹äººç±»è¡Œä¸ºæ„æˆå¨èƒï¼Œä½†ç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹MLLMsåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å®‰å…¨å½±å“è¿›è¡Œç³»ç»Ÿè¯„ä¼°çš„åŸºå‡†ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹å¼ºè°ƒçœŸå®é£é™©æš´éœ²ã€çœŸå®è§†è§‰è¾“å…¥å’Œç»†ç²’åº¦è·¨æ¨¡æ€æ¨ç†çš„è¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†SaLADå¤šæ¨¡æ€å®‰å…¨åŸºå‡†ï¼ŒåŒ…å«2,013ä¸ªçœŸå®ä¸–ç•Œå›¾åƒ-æ–‡æœ¬æ ·æœ¬ï¼Œè¦†ç›–10ä¸ªå¸¸è§ç±»åˆ«ï¼Œé‡‡ç”¨å¹³è¡¡è®¾è®¡åŒæ—¶åŒ…å«ä¸å®‰å…¨åœºæ™¯å’Œè¿‡åº¦æ•æ„Ÿæ¡ˆä¾‹ï¼›ç‰¹åˆ«å¼ºè°ƒå®‰å…¨é£é™©ä¸èƒ½ä»…ä»æ–‡æœ¬æ¨æ–­ï¼Œéœ€è¦è·¨æ¨¡æ€æ¨ç†ï¼›è¿›ä¸€æ­¥æå‡ºäº†åŸºäºå®‰å…¨è­¦å‘Šçš„è¯„ä¼°æ¡†æ¶ï¼Œé¼“åŠ±æ¨¡å‹æä¾›æ¸…æ™°ä¸”ä¿¡æ¯ä¸°å¯Œçš„å®‰å…¨è­¦å‘Šè€Œéé€šç”¨æ‹’ç»ã€‚</p>
<p><strong>Result:</strong> åœ¨18ä¸ªMLLMsä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹åœ¨ä¸å®‰å…¨æŸ¥è¯¢ä¸Šçš„å®‰å…¨å“åº”ç‡ä»…ä¸º57.2%ï¼›å³ä½¿æµè¡Œçš„å®‰å…¨å¯¹é½æ–¹æ³•åœ¨è¯¥åœºæ™¯ä¸‹æ•ˆæœä¹Ÿæœ‰é™ï¼Œæ­ç¤ºäº†å½“å‰MLLMsåœ¨è¯†åˆ«æ—¥å¸¸ç”Ÿæ´»ä¸­å±é™©è¡Œä¸ºæ–¹é¢çš„è„†å¼±æ€§ï¼›åŸºå‡†æ•°æ®é›†å·²å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å®‰å…¨é£é™©æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå³ä½¿ç»è¿‡å®‰å…¨å¯¹é½çš„æ¨¡å‹ä¹Ÿéš¾ä»¥æœ‰æ•ˆå¤„ç†çœŸå®ä¸–ç•Œçš„å¤šæ¨¡æ€å®‰å…¨åœºæ™¯ï¼›æå‡ºçš„SaLADåŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ä¸ºMLLMså®‰å…¨è¯„ä¼°æä¾›äº†é‡è¦å·¥å…·ï¼Œæ­ç¤ºäº†éœ€è¦æ›´ç²¾ç»†çš„å®‰å…¨å¯¹é½ç­–ç•¥æ¥åº”å¯¹å¤æ‚çš„è·¨æ¨¡æ€å®‰å…¨æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>As Multimodal Large Language Models (MLLMs) become an indispensable assistant in human life, the unsafe content generated by MLLMs poses a danger to human behavior, perpetually overhanging human society like a sword of Damocles. To investigate and evaluate the safety impact of MLLMs responses on human behavior in daily life, we introduce SaLAD, a multimodal safety benchmark which contains 2,013 real-world image-text samples across 10 common categories, with a balanced design covering both unsafe scenarios and cases of oversensitivity. It emphasizes realistic risk exposure, authentic visual inputs, and fine-grained cross-modal reasoning, ensuring that safety risks cannot be inferred from text alone. We further propose a safety-warning-based evaluation framework that encourages models to provide clear and informative safety warnings, rather than generic refusals. Results on 18 MLLMs demonstrate that the top-performing models achieve a safe response rate of only 57.2% on unsafe queries. Moreover, even popular safety alignment methods limit effectiveness of the models in our scenario, revealing the vulnerabilities of current MLLMs in identifying dangerous behaviors in daily life. Our dataset is available at https://github.com/xinyuelou/SaLAD.</p>
<h3 id="37-bridging-the-discrete-continuous-gap-unified-multimodal-generation-via-coupled-manifold-discrete-absorbing-diffusion">[37] <a href="https://arxiv.org/abs/2601.04056">Bridging the Discrete-Continuous Gap: Unified Multimodal Generation via Coupled Manifold Discrete Absorbing Diffusion</a></h3>
<p><em>Yuanfeng Xu, Yuhao Chen, Liang Lin, Guangrun Wang</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CoM-DADï¼ˆè€¦åˆæµå½¢ç¦»æ•£å¸æ”¶æ‰©æ•£ï¼‰ï¼Œä¸€ç§æ–°é¢–çš„æ¦‚ç‡æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚åŒè¿‡ç¨‹å°†å¤šæ¨¡æ€ç”Ÿæˆé‡æ–°è¡¨è¿°ä¸ºè¯­ä¹‰è§„åˆ’å’Œä»¤ç‰Œåˆæˆçš„è§£è€¦ï¼Œä¸ºç»Ÿä¸€æ–‡æœ¬-å›¾åƒç”Ÿæˆå»ºç«‹äº†æ–°èŒƒå¼ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç”Ÿæˆå»ºæ¨¡åœ¨ç¦»æ•£æ•°æ®ï¼ˆæ–‡æœ¬ï¼‰çš„è‡ªå›å½’æ–¹æ³•å’Œè¿ç»­æ•°æ®ï¼ˆå›¾åƒï¼‰çš„æ‰©æ•£æ–¹æ³•ä¹‹é—´å­˜åœ¨åˆ†æ­§ï¼Œé˜»ç¢äº†çœŸæ­£ç»Ÿä¸€å¤šæ¨¡æ€ç³»ç»Ÿçš„å‘å±•ã€‚æ©ç è¯­è¨€æ¨¡å‹è™½ç„¶æä¾›é«˜æ•ˆåŒå‘ä¸Šä¸‹æ–‡ï¼Œä½†ç¼ºä¹è‡ªå›å½’æ¨¡å‹çš„ç”Ÿæˆä¿çœŸåº¦å’Œæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰è¿ç»­æ€§ï¼Œä¸”åœ¨æ‰©å±•åˆ°å¤šæ¨¡æ€è®¾ç½®æ—¶é¢ä¸´ä¸¥é‡çš„å¯¹é½æŒ‘æˆ˜å’Œè®­ç»ƒä¸ç¨³å®šæ€§ã€‚</p>
<p><strong>Method:</strong> CoM-DADæ¡†æ¶å°†å¤šæ¨¡æ€ç”Ÿæˆé‡æ–°è¡¨è¿°ä¸ºåˆ†å±‚åŒè¿‡ç¨‹ï¼Œè§£è€¦é«˜å±‚è¯­ä¹‰è§„åˆ’å’Œä½å±‚ä»¤ç‰Œåˆæˆã€‚é¦–å…ˆé€šè¿‡è¿ç»­æ½œåœ¨æ‰©æ•£è¿‡ç¨‹å»ºæ¨¡è¯­ä¹‰æµå½¢ï¼Œç„¶åå°†ä»¤ç‰Œç”Ÿæˆå¤„ç†ä¸ºç¦»æ•£å¸æ”¶æ‰©æ•£è¿‡ç¨‹ï¼Œç”±å¯å˜é€Ÿç‡å™ªå£°è°ƒåº¦è°ƒèŠ‚å¹¶åŸºäºè¿™äº›æ¼”åŒ–çš„è¯­ä¹‰å…ˆéªŒè¿›è¡Œæ¡ä»¶åŒ–ã€‚å…³é”®åˆ›æ–°æ˜¯å¼•å…¥éšæœºæ··åˆæ¨¡æ€ä¼ è¾“ç­–ç•¥ï¼Œæ— éœ€ç¹é‡çš„å¯¹æ¯”åŒç¼–ç å™¨å³å¯å¯¹é½ä¸åŒæ¨¡æ€ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨è®­ç»ƒç¨³å®šæ€§æ–¹é¢è¡¨ç°å‡ºä¼˜äºæ ‡å‡†æ©ç å»ºæ¨¡çš„ä¼˜è¶Šæ€§èƒ½ï¼Œä¸ºå¯æ‰©å±•çš„ç»Ÿä¸€æ–‡æœ¬-å›¾åƒç”Ÿæˆå»ºç«‹äº†æ–°èŒƒå¼ã€‚å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ–‡æœ¬å’Œå›¾åƒå¯¹é½æŒ‘æˆ˜æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</p>
<p><strong>Conclusion:</strong> CoM-DADé€šè¿‡è§£è€¦è¯­ä¹‰è§„åˆ’å’Œä»¤ç‰Œåˆæˆï¼Œä¸ºç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆæä¾›äº†åˆ›æ–°æ¡†æ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ¨¡æ€å¯¹é½å’Œè®­ç»ƒç¨³å®šæ€§æ–¹é¢çš„å…³é”®é™åˆ¶ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘æ›´é«˜æ•ˆã€æ›´ç¨³å®šçš„è·¨æ¨¡æ€ç”Ÿæˆç³»ç»Ÿå¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå…·æœ‰æ¨åŠ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½å‘å±•çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>The bifurcation of generative modeling into autoregressive approaches for discrete data (text) and diffusion approaches for continuous data (images) hinders the development of truly unified multimodal systems. While Masked Language Models (MLMs) offer efficient bidirectional context, they traditionally lack the generative fidelity of autoregressive models and the semantic continuity of diffusion models. Furthermore, extending masked generation to multimodal settings introduces severe alignment challenges and training instability. In this work, we propose \textbf{CoM-DAD} (\textbf{Co}upled \textbf{M}anifold \textbf{D}iscrete \textbf{A}bsorbing \textbf{D}iffusion), a novel probabilistic framework that reformulates multimodal generation as a hierarchical dual-process. CoM-DAD decouples high-level semantic planning from low-level token synthesis. First, we model the semantic manifold via a continuous latent diffusion process; second, we treat token generation as a discrete absorbing diffusion process, regulated by a \textbf{Variable-Rate Noise Schedule}, conditioned on these evolving semantic priors. Crucially, we introduce a \textbf{Stochastic Mixed-Modal Transport} strategy that aligns disparate modalities without requiring heavy contrastive dual-encoders. Our method demonstrates superior stability over standard masked modeling, establishing a new paradigm for scalable, unified text-image generation.</p>
<h3 id="38-persona-aware-and-explainable-bikeability-assessment-a-vision-language-model-approach">[38] <a href="https://arxiv.org/abs/2601.03534">Persona-aware and Explainable Bikeability Assessment: A Vision-Language Model Approach</a></h3>
<p><em>Yilong Dai, Ziyi Wang, Chenguang Wang, Kexin Zhou, Yiheng Qian, Susu Xu, Xiang Yan</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§’è‰²æ„ŸçŸ¥çš„è§†è§‰è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œç”¨äºè‡ªè¡Œè½¦é“å¯éª‘è¡Œæ€§è¯„ä¼°ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç†è®ºé©±åŠ¨çš„è§’è‰²æ¡ä»¶åŒ–ã€å¤šç²’åº¦ç›‘ç£å¾®è°ƒå’ŒAIå¢å¼ºæ•°æ®ç”Ÿæˆï¼Œå®ç°äº†å¯è§£é‡Šçš„è¯„ä¼°ä¸é¢„æµ‹ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºæ„ŸçŸ¥çš„è‡ªè¡Œè½¦é“å¯éª‘è¡Œæ€§è¯„ä¼°æ–¹æ³•åœ¨æ•æ‰å¤æ‚é“è·¯ç¯å¢ƒæ–¹é¢å­˜åœ¨å±€é™ï¼Œä¸”æœªèƒ½å……åˆ†è€ƒè™‘ç”¨æˆ·ä¸»è§‚æ„ŸçŸ¥çš„å¼‚è´¨æ€§ï¼Œè¿™é™åˆ¶äº†è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æå‡ºäº†ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼šåŸºäºæ—¢å®šè‡ªè¡Œè½¦æ‰‹ç±»å‹å­¦çš„ç†è®ºé©±åŠ¨è§’è‰²æ¡ä»¶åŒ–ï¼Œé€šè¿‡æ€ç»´é“¾æ¨ç†ç”Ÿæˆè§’è‰²ç‰¹å®šè§£é‡Šï¼›ç»“åˆç¨€ç¼ºä¸“å®¶æ ‡æ³¨æ¨ç†ä¸ä¸°å¯Œç”¨æˆ·è¯„åˆ†çš„å¤šç²’åº¦ç›‘ç£å¾®è°ƒï¼Œå®ç°è”åˆé¢„æµ‹ä¸å¯è§£é‡Šè¯„ä¼°ï¼›ä»¥åŠAIé©±åŠ¨çš„æ•°æ®å¢å¼ºï¼Œåˆ›å»ºå—æ§é…å¯¹æ•°æ®ä»¥éš”ç¦»åŸºç¡€è®¾æ–½å˜é‡å½±å“ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡å¼€å‘å…¨æ™¯å›¾åƒä¼—åŒ…ç³»ç»Ÿæ”¶é›†äº†æ¥è‡ª427åè‡ªè¡Œè½¦æ‰‹çš„12,400æ¡è§’è‰²æ¡ä»¶åŒ–è¯„ä¼°æ•°æ®ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶åœ¨è‡ªè¡Œè½¦é“å¯éª‘è¡Œæ€§è¯„åˆ†é¢„æµ‹æ–¹é¢å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶ç‹¬ç‰¹åœ°å®ç°äº†å¯è§£é‡Šçš„å› å­å½’å› ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè‡ªè¡Œè½¦é“å¯éª‘è¡Œæ€§è¯„ä¼°æä¾›äº†ä¸€ç§èåˆç†è®ºæŒ‡å¯¼ä¸æ•°æ®é©±åŠ¨çš„æ–°èŒƒå¼ï¼Œä¸ä»…æå‡äº†é¢„æµ‹æ€§èƒ½ï¼Œæ›´é‡è¦çš„æ˜¯å®ç°äº†è¯„ä¼°è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ï¼Œä¸ºåŸå¸‚è§„åˆ’è€…æä¾›äº†æ›´æ·±å…¥çš„å†³ç­–æ”¯æŒã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>Bikeability assessment is essential for advancing sustainable urban transportation and creating cyclist-friendly cities, and it requires incorporating users' perceptions of safety and comfort. Yet existing perception-based bikeability assessment approaches face key limitations in capturing the complexity of road environments and adequately accounting for heterogeneity in subjective user perceptions. This paper proposes a persona-aware Vision-Language Model framework for bikeability assessment with three novel contributions: (i) theory-grounded persona conditioning based on established cyclist typology that generates persona-specific explanations via chain-of-thought reasoning; (ii) multi-granularity supervised fine-tuning that combines scarce expert-annotated reasoning with abundant user ratings for joint prediction and explainable assessment; and (iii) AI-enabled data augmentation that creates controlled paired data to isolate infrastructure variable impacts. To test and validate this framework, we developed a panoramic image-based crowdsourcing system and collected 12,400 persona-conditioned assessments from 427 cyclists. Experiment results show that the proposed framework offers competitive bikeability rating prediction while uniquely enabling explainable factor attribution.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="39-personalization-of-large-foundation-models-for-health-interventions">[39] <a href="https://arxiv.org/abs/2601.03482">Personalization of Large Foundation Models for Health Interventions</a></h3>
<p><em>Stefan Konigorski, Johannes E. Vedder, Babajide Alamu Owoyele, Ä°brahim Ã–zkan</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æ¢è®¨å¤§å‹åŸºç¡€æ¨¡å‹åœ¨ä¸ªæ€§åŒ–åŒ»ç–—ä¸­çš„å±€é™æ€§ï¼Œæå‡ºLFMsæ— æ³•æ›¿ä»£N-of-1è¯•éªŒï¼Œä½†ä¸¤è€…äº’è¡¥ï¼šLFMsæ“…é•¿ä»ç¾¤ä½“æ•°æ®ä¸­ç”Ÿæˆå‡è®¾ï¼Œè€ŒN-of-1è¯•éªŒæä¾›ä¸ªä½“å› æœéªŒè¯ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç»“åˆä¸¤è€…ä¼˜åŠ¿çš„æ··åˆæ¡†æ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹åŸºç¡€æ¨¡å‹åœ¨åŒ»ç–—AIä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶èƒ½å¦æä¾›çœŸæ­£ä¸ªæ€§åŒ–çš„æ²»ç–—å»ºè®®ä»å­˜ç–‘é—®ã€‚ç ”ç©¶å‘ç°ä¸ªæ€§åŒ–é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ³›åŒ–æ€§æ‚–è®ºï¼ˆæ¨¡å‹åœ¨ä¸€ä¸ªä¸´åºŠç ”ç©¶ä¸­è¡¨ç°ä¼˜å¼‚å´åœ¨å…¶ä»–ç ”ç©¶ä¸­è¡¨ç°éšæœºï¼‰ã€éšç§-æ€§èƒ½æ‚–è®ºã€è§„æ¨¡-ç‰¹å¼‚æ€§æ‚–è®ºå’Œè‡ªåŠ¨åŒ–-å…±æƒ…æ‚–è®ºã€‚æ­¤å¤–ï¼Œä¸ªæ€§åŒ–æ¨èæ‰€éœ€çš„å› æœç†è§£ç¨‹åº¦ä¸LFMsçš„é¢„æµ‹èƒ½åŠ›ä¹‹é—´çš„ç•Œé™ä¹Ÿä¸æ˜ç¡®ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºä¸€ä¸ªæ··åˆæ¡†æ¶ï¼Œç»“åˆå¤§å‹åŸºç¡€æ¨¡å‹å’ŒN-of-1è¯•éªŒçš„ä¼˜åŠ¿ã€‚LFMsåˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ä»ç¾¤ä½“æ¨¡å¼ä¸­å¿«é€Ÿç”Ÿæˆå¹²é¢„å€™é€‰å‡è®¾å¹¶ç»™å‡ºä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œè€ŒN-of-1è¯•éªŒä½œä¸ºä¸ªæ€§åŒ–åŒ»å­¦ä¸­ä¸ªä½“å› æœæ¨æ–­çš„é‡‘æ ‡å‡†ï¼Œé€šè¿‡äº¤å‰è‡ªæˆ‘å®éªŒæä¾›ä¸ªä½“å†…å› æœè¯æ®å¹¶ä¿æŠ¤éšç§ã€‚è¯¥æ¡†æ¶è®©LFMsç”Ÿæˆæ’åºçš„å¹²é¢„å€™é€‰è§¦å‘åç»­çš„N-of-1è¯•éªŒéªŒè¯ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶è®ºè¯LFMsæ— æ³•æ›¿ä»£N-of-1è¯•éªŒï¼Œä½†ä¸¤è€…å…·æœ‰äº’è¡¥æ€§ï¼šLFMsæ“…é•¿ä»ç¾¤ä½“æ¨¡å¼ä¸­å¿«é€Ÿç”Ÿæˆå‡è®¾ï¼Œè€ŒN-of-1è¯•éªŒæ“…é•¿ä¸ºç‰¹å®šä¸ªä½“æä¾›å› æœéªŒè¯ã€‚é€šè¿‡æ˜ç¡®é¢„æµ‹ä¸å› æœä¹‹é—´çš„ç•Œé™ï¼Œå¹¶ç›´æ¥è§£å†³ä¸ªæ€§åŒ–åŒ»ç–—ä¸­çš„å„ç§æ‚–è®ºï¼Œè¯¥æ··åˆæ¡†æ¶ä¸ºè´Ÿè´£ä»»åœ°æ•´åˆAIåˆ°ä¸ªæ€§åŒ–åŒ»å­¦æä¾›äº†è·¯å¾„ã€‚</p>
<p><strong>Conclusion:</strong> ä¸ªæ€§åŒ–åŒ»ç–—éœ€è¦æ˜ç¡®åŒºåˆ†é¢„æµ‹èƒ½åŠ›å’Œå› æœç†è§£ï¼ŒLFMså’ŒN-of-1è¯•éªŒçš„äº’è¡¥ç»“åˆä¸ºè§£å†³åŒ»ç–—AIä¸­çš„æ‚–è®ºæä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚æ˜ç¡®é¢„æµ‹ä¸å› æœçš„è¾¹ç•Œã€ç›´æ¥åº”å¯¹å„ç§æ‚–è®ºå¯¹äºè´Ÿè´£ä»»åœ°å°†AIæ•´åˆåˆ°ä¸ªæ€§åŒ–åŒ»å­¦è‡³å…³é‡è¦ã€‚è¯¥æ¡†æ¶ä¸ºæœªæ¥åŒ»ç–—AIç³»ç»Ÿè®¾è®¡æä¾›äº†ç†è®ºæŒ‡å¯¼ï¼Œå¼ºè°ƒå› æœéªŒè¯åœ¨ä¸ªæ€§åŒ–æ²»ç–—ä¸­çš„æ ¸å¿ƒåœ°ä½ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using multimodal data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.</p>
<h3 id="40-current-agents-fail-to-leverage-world-model-as-tool-for-foresight">[40] <a href="https://arxiv.org/abs/2601.03905">Current Agents Fail to Leverage World Model as Tool for Foresight</a></h3>
<p><em>Cheng Qian, Emre Can Acikgoz, Bingxuan Li, Xiusi Chen, Yuji Zhang, Bingxiang He, Qinyu Luo, Dilek Hakkani-TÃ¼r, Gokhan Tur, Yunzhu Li, Heng Ji, Heng Ji</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼Œå½“å‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨ç”Ÿæˆä¸–ç•Œæ¨¡å‹ä½œä¸ºå¤–éƒ¨æ¨¡æ‹Ÿå™¨æ¥å¢å¼ºå…¶è®¤çŸ¥èƒ½åŠ›ï¼Œä¸»è¦ç“¶é¢ˆåœ¨äºæ™ºèƒ½ä½“æ— æ³•å†³å®šä½•æ—¶è¿›è¡Œæ¨¡æ‹Ÿã€å¦‚ä½•è§£é‡Šé¢„æµ‹ç»“æœä»¥åŠå¦‚ä½•å°†é¢„è§æ•´åˆåˆ°ä¸‹æ¸¸æ¨ç†ä¸­ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€æ™ºèƒ½ä½“é¢ä¸´è¶Šæ¥è¶Šå¤šéœ€è¦é¢„æµ‹æœªæ¥çŠ¶æ€è€Œéä¾èµ–çŸ­è§†æ¨ç†çš„ä»»åŠ¡ï¼Œç”Ÿæˆä¸–ç•Œæ¨¡å‹è¢«è§†ä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä½œä¸ºå¤–éƒ¨æ¨¡æ‹Ÿå™¨å¸®åŠ©æ™ºèƒ½ä½“åœ¨è¡ŒåŠ¨å‰é¢„è§ç»“æœã€‚ç„¶è€Œï¼Œå½“å‰æ™ºèƒ½ä½“æ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨è¿™äº›ä¸–ç•Œæ¨¡å‹ä½œä¸ºå·¥å…·æ¥å¢å¼ºå…¶è®¤çŸ¥èƒ½åŠ›ï¼Œè¿™ä¸€é—®é¢˜å°šæœªå¾—åˆ°å®è¯æ£€éªŒã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶é‡‡ç”¨å®è¯åˆ†ææ–¹æ³•ï¼Œåœ¨å¤šæ ·åŒ–çš„æ™ºèƒ½ä½“ä»»åŠ¡å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šï¼Œç³»ç»Ÿè¯„ä¼°äº†å½“å‰æ™ºèƒ½ä½“åˆ©ç”¨ç”Ÿæˆä¸–ç•Œæ¨¡å‹ä½œä¸ºå¤–éƒ¨æ¨¡æ‹Ÿå™¨çš„èƒ½åŠ›ã€‚é€šè¿‡åˆ†ææ™ºèƒ½ä½“è°ƒç”¨æ¨¡æ‹Ÿçš„é¢‘ç‡ã€ä½¿ç”¨é¢„æµ‹æ¨æ¼”çš„æ–¹å¼ä»¥åŠæ€§èƒ½å˜åŒ–ï¼Œå¹¶ç»“åˆå½’å› åˆ†æè¯†åˆ«å…·ä½“ç“¶é¢ˆã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼ŒæŸäº›æ™ºèƒ½ä½“æå°‘è°ƒç”¨æ¨¡æ‹Ÿï¼ˆå°‘äº1%ï¼‰ï¼Œç»å¸¸è¯¯ç”¨é¢„æµ‹æ¨æ¼”ï¼ˆçº¦15%ï¼‰ï¼Œå¹¶ä¸”åœ¨æ¨¡æ‹Ÿå¯ç”¨æˆ–å¼ºåˆ¶ä½¿ç”¨æ—¶ç»å¸¸è¡¨ç°å‡ºä¸ä¸€è‡´ç”šè‡³æ€§èƒ½ä¸‹é™ï¼ˆé«˜è¾¾5%ï¼‰ã€‚å½’å› åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œä¸»è¦ç“¶é¢ˆåœ¨äºæ™ºèƒ½ä½“å†³å®šä½•æ—¶æ¨¡æ‹Ÿã€å¦‚ä½•è§£é‡Šé¢„æµ‹ç»“æœä»¥åŠå¦‚ä½•å°†é¢„è§æ•´åˆåˆ°ä¸‹æ¸¸æ¨ç†çš„èƒ½åŠ›ä¸è¶³ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å‘ç°å½“å‰æ™ºèƒ½ä½“éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨ä¸–ç•Œæ¨¡å‹ä½œä¸ºè®¤çŸ¥å¢å¼ºå·¥å…·ï¼Œè¿™å‡¸æ˜¾äº†éœ€è¦å¼€å‘èƒ½å¤Ÿä¿ƒè¿›ä¸ä¸–ç•Œæ¨¡å‹è¿›è¡Œæ ¡å‡†ã€æˆ˜ç•¥æ€§äº¤äº’çš„æœºåˆ¶ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥æ„å»ºæ›´å¯é çš„é¢„è§æ€§è®¤çŸ¥æ™ºèƒ½ä½“ç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ï¼Œå¼ºè°ƒäº†æ™ºèƒ½ä½“ä¸ä¸–ç•Œæ¨¡å‹äº¤äº’ç­–ç•¥çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.</p>
  </article>
</body>
</html>
