<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-11.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 36]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 12]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 10]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-pickstyle-video-to-video-style-transfer-with-context-style-adapters">[1] <a href="https://arxiv.org/abs/2510.07546">PickStyle: Video-to-Video Style Transfer with Context-Style Adapters</a></h3>
<p><em>Soroush Mehraban, Vida Adeli, Jacob Rommann, Babak Taati, Kyryl Truskovskyi</em></p>
<h4 id="tldr">🧩 TL;DR</h4>
<p>本文提出了PickStyle框架，通过低秩适配器和上下文-风格分类器自由引导技术，解决了视频风格迁移中缺乏配对监督数据的问题，实现了时间一致、风格忠实且内容保持的视频转换。</p>
<hr />
<h4 id="detailed-summary">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 视频风格迁移面临的主要挑战是缺乏配对的视频数据进行监督训练，现有方法难以在保持视频内容的同时有效传递目标风格，且难以保证时间一致性。</p>
<p><strong>Method:</strong> 提出PickStyle框架，在预训练视频扩散模型中插入低秩适配器到自注意力层，利用配对静态图像数据构建合成训练片段，并引入上下文-风格分类器自由引导技术将文本风格和视频上下文引导方向独立分解。</p>
<p><strong>Result:</strong> 在多个基准测试上的实验表明，该方法实现了时间一致、风格忠实且内容保持的视频转换效果，在定性和定量评估中均优于现有基线方法。</p>
<p><strong>Conclusion:</strong> 该研究证明了通过低秩适配器和精心设计的训练策略，可以利用静态图像数据有效解决视频风格迁移问题，为视频编辑任务提供了新的技术路径和理论指导。</p>
<hr />
<h4 id="abstract">📄 Abstract</h4>
<p>We address the task of video style transfer with diffusion models, where the
goal is to preserve the context of an input video while rendering it in a
target style specified by a text prompt. A major challenge is the lack of
paired video data for supervision. We propose PickStyle, a video-to-video style
transfer framework that augments pretrained video diffusion backbones with
style adapters and benefits from paired still image data with source-style
correspondences for training. PickStyle inserts low-rank adapters into the
self-attention layers of conditioning modules, enabling efficient
specialization for motion-style transfer while maintaining strong alignment
between video content and style. To bridge the gap between static image
supervision and dynamic video, we construct synthetic training clips from
paired images by applying shared augmentations that simulate camera motion,
ensuring temporal priors are preserved. In addition, we introduce Context-Style
Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free
guidance into independent text (style) and video (context) directions. CS-CFG
ensures that context is preserved in generated video while the style is
effectively transferred. Experiments across benchmarks show that our approach
achieves temporally coherent, style-faithful, and content-preserving video
translations, outperforming existing baselines both qualitatively and
quantitatively.</p>
<h3 id="2-travl-a-recipe-for-making-video-language-models-better-judges-of-physics-implausibility">[2] <a href="https://arxiv.org/abs/2510.07550">TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility</a></h3>
<p><em>Saman Motamed, Minghao Chen, Luc Van Gool, Iro Laina</em></p>
<h4 id="tldr_1">🧩 TL;DR</h4>
<p>本研究提出了TRAVL微调方法和ImplausiBench基准，用于提升视频语言模型在物理合理性评估方面的能力，为解决视频生成模型违反物理规律的问题提供了统一框架。</p>
<hr />
<h4 id="detailed-summary_1">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现代视频生成模型经常产生违反物理规律的序列，如物体漂浮、瞬移或违背因果关系的形变，但目前缺乏定量评估物理真实性的鲁棒方法，现有视频语言模型在识别物理违规方面存在困难，暴露了其时序和因果推理的根本局限性。</p>
<p><strong>Method:</strong> 提出了TRAVL微调方法，结合平衡训练数据集和轨迹感知注意力模块来改进视频语言模型中的运动编码和判别能力，同时引入了ImplausiBench基准，包含300个视频（150个真实、150个生成），旨在消除语言偏见并隔离视觉-时序理解。</p>
<p><strong>Result:</strong> 性能评估同时采用黄金标准的人类判断和更严格的LLM-as-judge指标，TRAVL方法显著提升了模型对物理违规的识别能力，ImplausiBench为物理推理提供了更严格的评估框架。</p>
<p><strong>Conclusion:</strong> TRAVL和ImplausiBench共同为探索和改进多模态模型中的物理合理性提供了统一框架，揭示了视觉-时序理解中具有挑战性且未充分探索的方面，为视频生成模型的物理真实性评估开辟了新途径。</p>
<hr />
<h4 id="abstract_1">📄 Abstract</h4>
<p>Despite impressive visual fidelity, modern video generative models frequently
produce sequences that violate intuitive physical laws, such as objects
floating, teleporting, or morphing in ways that defy causality. While humans
can easily detect such implausibilities, there remains no robust method for
quantitatively assessing physical realism in video. In this work, we explore
whether Video-Language Models (VLMs) can be trained to serve as reliable judges
of physical plausibility. We find that existing VLMs struggle to identify
physics violations, exposing fundamental limitations in their temporal and
causal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe
that combines a balanced training dataset with a trajectory-aware attention
module to improve motion encoding and discrimination in VLMs. To evaluate
physical reasoning more rigorously, we propose ImplausiBench, a benchmark of
300 videos (150 real, 150 generated) that removes linguistic biases and
isolates visual-temporal understanding. Performance is reported both with
gold-standard human judgments and stricter LLM-as-judge metrics. Together,
TRAVL and ImplausiBench offer a unified framework for probing and improving
physical plausibility in multimodal models, shedding light on a challenging and
underexplored aspect of visual-temporal understanding.</p>
<h3 id="3-label-semantics-for-robust-hyperspectral-image-classification">[3] <a href="https://arxiv.org/abs/2510.07556">Label Semantics for Robust Hyperspectral Image Classification</a></h3>
<p><em>Rafin Hassan, Zarin Tasnim Roshni, Rafiqul Bari, Alimul Islam, Nabeel Mohammed, Moshiur Farazi, Shafin Rahman</em></p>
<h4 id="tldr_2">🧩 TL;DR</h4>
<p>本文提出了一种通用的语义光谱-空间融合网络（S3FN），通过利用LLM生成的类别文本描述来增强高光谱图像分类性能，解决了传统单模态方法仅依赖光谱空间数据的局限性。</p>
<hr />
<h4 id="detailed-summary_2">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 高光谱图像分类面临高质量训练样本稀缺、数据维度高导致模型易过拟合的问题，且现有方法多为单模态，仅依赖光谱空间数据在高维嵌入空间中学习决策边界，缺乏语义信息的补充。</p>
<p><strong>Method:</strong> S3FN利用大型语言模型为每个类别标签生成全面的文本描述，捕捉其独特特征和光谱行为，然后使用预训练文本编码器（如BERT或RoBERTa）将这些描述嵌入向量空间，提取有意义的标签语义以实现更好的特征-标签对齐。</p>
<p><strong>Result:</strong> 在三个不同的高光谱基准数据集（Hyperspectral Wood、HyperspectralBlueberries和DeepHS-Fruit）上的评估表明，该方法显著提升了分类性能，证明了文本语义与光谱空间数据之间的协同效应。</p>
<p><strong>Conclusion:</strong> 该研究展示了语义增强在高光谱图像分类中的潜力，为开发更先进的语义增强HSI分类模型开辟了新途径，强调了多模态融合在解决复杂视觉任务中的重要性。</p>
<hr />
<h4 id="abstract_2">📄 Abstract</h4>
<p>Hyperspectral imaging (HSI) classification is a critical tool with widespread
applications across diverse fields such as agriculture, environmental
monitoring, medicine, and materials science. Due to the limited availability of
high-quality training samples and the high dimensionality of spectral data, HSI
classification models are prone to overfitting and often face challenges in
balancing accuracy and computational complexity. Furthermore, most of HSI
classification models are monomodal, where it solely relies on spectral-spatial
data to learn decision boundaries in the high dimensional embedding space. To
address this, we propose a general-purpose Semantic Spectral-Spatial Fusion
Network (S3FN) that uses contextual, class specific textual descriptions to
complement the training of an HSI classification model. Specifically, S3FN
leverages LLMs to generate comprehensive textual descriptions for each class
label that captures their unique characteristics and spectral behaviors. These
descriptions are then embedded into a vector space using a pre-trained text
encoder such as BERT or RoBERTa to extract meaningful label semantics which in
turn leads to a better feature-label alignment for improved classification
performance. To demonstrate the effectiveness of our approach, we evaluate our
model on three diverse HSI benchmark datasets - Hyperspectral Wood,
HyperspectralBlueberries, and DeepHS-Fruit and report significant performance
boost. Our results highlight the synergy between textual semantics and
spectral-spatial data, paving the way for further advancements in semantically
augmented HSI classification models. Codes are be available in:
https://github.com/milab-nsu/S3FN</p>
<h3 id="4-cross-modal-attention-guided-unlearning-in-vision-language-models">[4] <a href="https://arxiv.org/abs/2510.07567">Cross-Modal Attention Guided Unlearning in Vision-Language Models</a></h3>
<p><em>Karuna Bhaila, Aneesh Komanduri, Minh-Hao Van, Xintao Wu</em></p>
<h4 id="tldr_3">🧩 TL;DR</h4>
<p>本文提出了CAGUL，一种轻量级高效的视觉语言模型遗忘框架，通过利用跨模态注意力引导的视觉标记转换来防止敏感信息泄露，同时保持参考模型行为，无需修改预训练模型参数或产生重新训练成本。</p>
<hr />
<h4 id="detailed-summary_3">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 视觉语言模型在训练过程中可能记忆并泄露私有或敏感信息，而现有的机器遗忘方法主要针对语言模型，视觉语言模型由于视觉上下文也可能包含敏感信息而增加了复杂性，需要专门针对VQA任务的遗忘解决方案。</p>
<p><strong>Method:</strong> 本文探索了跨模态注意力在视觉语言模型输出生成中的作用，并基于此提出了跨模态注意力引导遗忘框架CAGUL，该框架利用外部模块在相关查询的低重要性视觉标记中编码遗忘信息，避免了对预训练模型参数的修改。</p>
<p><strong>Result:</strong> 实验结果表明，CAGUL方法在防止信息泄露方面表现优于或与基于微调的基线方法相当，同时保持了参考模型的行为特性，且不产生重新训练的计算成本。</p>
<p><strong>Conclusion:</strong> CAGUL为视觉语言模型提供了一种实用有效的遗忘解决方案，通过轻量级的外部模块实现了敏感信息保护，同时维持了模型性能，为多模态模型的隐私保护开辟了新的研究方向。</p>
<hr />
<h4 id="abstract_3">📄 Abstract</h4>
<p>Vision-Language Models (VLMs) have demonstrated immense capabilities in
multi-modal understanding and inference tasks such as Visual Question Answering
(VQA), which requires models to infer outputs based on visual and textual
context simultaneously. Such inference abilities of large-scale pretrained
models are often attributed to the massive scale of pre-training data collected
across several domains. However, the models may memorize private and/or
sensitive information during training and regurgitate it in inference.
Recently, machine unlearning has been leveraged to address the leakage of
private data in LLMs. VLMs add a layer of complexity to this process, as the
visual context in the query may also contain sensitive information in addition
to the text. To address this issue, we explore unlearning for vision-language
models, specifically for the VQA task. We explore the role of visual tokens for
output generation in VLMs using cross-modal attention and utilize it to
formulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and
efficient VLM unlearning framework. In contrast to computationally expensive
model finetuning methods, CAGUL utilizes external modules to encode unlearning
information in visual tokens of low importance for relevant queries. We find
that the transformed visual tokens not only prevent leakage but also retain
reference model behavior. Experimental results show that our method performs
better or on par with finetuning-based baselines without altering the
pre-trained model parameters or incurring retraining costs, making it a
practical and effective unlearning solution for VLMs.</p>
<h3 id="5-rectified-cfg-for-flow-based-models">[5] <a href="https://arxiv.org/abs/2510.07631">Rectified-CFG++ for Flow Based Models</a></h3>
<p><em>Shreshth Saini, Shashank Gupta, Alan C. Bovik</em></p>
<h4 id="tldr_4">🧩 TL;DR</h4>
<p>本文提出了Rectified-CFG++，一种用于整流流模型的适应性预测-校正引导方法，解决了标准分类器无关引导在整流流模型中引起的离流形漂移问题，在多个大规模文本到图像模型上实现了稳定且对齐的生成效果。</p>
<hr />
<h4 id="detailed-summary_4">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 标准分类器无关引导在整流流模型中的直接应用会导致严重的离流形漂移，产生视觉伪影、文本对齐失败和脆弱行为，这限制了整流流模型在文本条件生成中的实际应用效果。</p>
<p><strong>Method:</strong> 提出自适应预测-校正引导方法，每个推理步骤首先执行条件整流流更新将样本锚定在学习的传输路径附近，然后应用加权条件校正，在条件和无条件速度场之间进行插值，确保速度场的边际一致性。</p>
<p><strong>Result:</strong> 在Flux、Stable Diffusion 3/3.5、Lumina等大规模文本到图像模型上的广泛实验表明，Rectified-CFG++在MS-COCO、LAION-Aesthetic和T2I-CompBench等基准数据集上持续优于标准分类器无关引导方法。</p>
<p><strong>Conclusion:</strong> 该方法证明了整流流轨迹能够保持在数据流形的有界管状邻域内，确保了在各种引导强度下的稳定性，为整流流模型的实际部署提供了可靠的引导机制。</p>
<hr />
<h4 id="abstract_4">📄 Abstract</h4>
<p>Classifier-free guidance (CFG) is the workhorse for steering large diffusion
models toward text-conditioned targets, yet its native application to rectified
flow (RF) based models provokes severe off-manifold drift, yielding visual
artifacts, text misalignment, and brittle behaviour. We present
Rectified-CFG++, an adaptive predictor-corrector guidance that couples the
deterministic efficiency of rectified flows with a geometry-aware conditioning
rule. Each inference step first executes a conditional RF update that anchors
the sample near the learned transport path, then applies a weighted conditional
correction that interpolates between conditional and unconditional velocity
fields. We prove that the resulting velocity field is marginally consistent and
that its trajectories remain within a bounded tubular neighbourhood of the data
manifold, ensuring stability across a wide range of guidance strengths.
Extensive experiments on large-scale text-to-image models (Flux, Stable
Diffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms
standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and
T2I-CompBench. Project page: https://rectified-cfgpp.github.io/</p>
<h3 id="6-pit-qmm-a-large-multimodal-model-for-no-reference-point-cloud-quality-assessment">[6] <a href="https://arxiv.org/abs/2510.07636">PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment</a></h3>
<p><em>Shashank Gupta, Gregoire Phillips, Alan C. Bovik</em></p>
<h4 id="tldr_5">🧩 TL;DR</h4>
<p>本文提出了PIT-QMM，一种新颖的大型多模态模型，用于无参考点云质量评估，通过端到端处理文本、图像和点云数据，在多个基准测试中显著优于现有最先进方法。</p>
<hr />
<h4 id="detailed-summary_5">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前大型多模态模型在图像和视频质量评估方面取得了显著进展，但在3D资产领域的应用尚未充分探索，特别是在无参考点云质量评估方面存在研究空白。</p>
<p><strong>Method:</strong> 作者构建了PIT-QMM模型，该模型能够端到端地处理文本描述、2D投影和3D点云视图等多种模态数据，利用不同模态提供的互补信息来预测点云质量分数。</p>
<p><strong>Result:</strong> 大量实验表明，所提方法在流行基准测试中以显著优势超越现有最先进技术，且需要更少的训练迭代次数，同时框架还支持失真定位和识别功能。</p>
<p><strong>Conclusion:</strong> 该研究为点云质量评估开辟了新途径，增强了模型的可解释性和交互性，为3D资产质量评估提供了有效的多模态解决方案。</p>
<hr />
<h4 id="abstract_5">📄 Abstract</h4>
<p>Large Multimodal Models (LMMs) have recently enabled considerable advances in
the realm of image and video quality assessment, but this progress has yet to
be fully explored in the domain of 3D assets. We are interested in using these
models to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where
the aim is to automatically evaluate the perceptual quality of a point cloud in
absence of a reference. We begin with the observation that different modalities
of data - text descriptions, 2D projections, and 3D point cloud views - provide
complementary information about point cloud quality. We then construct PIT-QMM,
a novel LMM for NR-PCQA that is capable of consuming text, images and point
clouds end-to-end to predict quality scores. Extensive experimentation shows
that our proposed method outperforms the state-of-the-art by significant
margins on popular benchmarks with fewer training iterations. We also
demonstrate that our framework enables distortion localization and
identification, which paves a new way forward for model explainability and
interactivity. Code and datasets are available at
https://www.github.com/shngt/pit-qmm.</p>
<h3 id="7-mutual-learning-for-hashing-unlocking-strong-hash-functions-from-weak-supervision">[7] <a href="https://arxiv.org/abs/2510.07703">Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision</a></h3>
<p><em>Xiaoxu Ma, Runhao Li, Zhenyu Weng</em></p>
<h4 id="tldr_6">🧩 TL;DR</h4>
<p>本文提出了MLH（Mutual Learning for Hashing），一种新颖的弱到强框架，通过从基于成对相似性的弱分支向基于中心的强分支转移知识，有效解决了中心哈希方法在建模全局结构时忽略局部相似性信息的问题。</p>
<hr />
<h4 id="detailed-summary_6">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有深度哈希方法中，基于成对相似性的方法能够有效保持局部相似关系，而基于中心的方法在捕获全局数据分布方面表现更优，但后者在建模全局结构时往往未能充分利用重要的局部相似性信息，这一局限性限制了哈希检索性能的进一步提升。</p>
<p><strong>Method:</strong> MLH框架包含两个分支：一个基于中心的强分支和一个基于成对相似性的弱分支，通过迭代式相互学习过程，强分支能够利用弱分支学到的局部相似性线索；此外，受混合专家范式启发，引入了混合哈希专家模块，实现有效的跨分支交互，进一步提升两个分支的性能。</p>
<p><strong>Result:</strong> 在多个基准数据集上的广泛实验表明，MLH方法在图像检索任务中一致性地超越了现有最先进的哈希方法，验证了所提框架在结合全局结构和局部相似性信息方面的有效性。</p>
<p><strong>Conclusion:</strong> 该研究证明了通过弱到强的相互学习框架能够有效整合不同哈希方法的优势，混合哈希专家模块的引入为跨分支知识转移提供了有效机制，为未来哈希方法设计提供了新的思路，即通过多分支协作来平衡全局和局部信息的利用。</p>
<hr />
<h4 id="abstract_6">📄 Abstract</h4>
<p>Deep hashing has been widely adopted for large-scale image retrieval, with
numerous strategies proposed to optimize hash function learning. Pairwise-based
methods are effective in learning hash functions that preserve local similarity
relationships, whereas center-based methods typically achieve superior
performance by more effectively capturing global data distributions. However,
the strength of center-based methods in modeling global structures often comes
at the expense of underutilizing important local similarity information. To
address this limitation, we propose Mutual Learning for Hashing (MLH), a novel
weak-to-strong framework that enhances a center-based hashing branch by
transferring knowledge from a weaker pairwise-based branch. MLH consists of two
branches: a strong center-based branch and a weaker pairwise-based branch.
Through an iterative mutual learning process, the center-based branch leverages
local similarity cues learned by the pairwise-based branch. Furthermore,
inspired by the mixture-of-experts paradigm, we introduce a novel
mixture-of-hash-experts module that enables effective cross-branch interaction,
further enhancing the performance of both branches. Extensive experiments
demonstrate that MLH consistently outperforms state-of-the-art hashing methods
across multiple benchmark datasets.</p>
<h3 id="8-gtr-bench-evaluating-geo-temporal-reasoning-in-vision-language-models">[8] <a href="https://arxiv.org/abs/2510.07791">GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models</a></h3>
<p><em>Qinghongbing Xie, Zhaoyuan Xia, Feng Zhu, Lijun Gong, Ziyue Li, Rui Zhao, Long Zeng</em></p>
<h4 id="tldr_7">🧩 TL;DR</h4>
<p>本文提出了Geo-Temporal Reasoning基准（GTR-Bench），用于评估视觉语言模型在大规模摄像头网络中移动目标的地理时空推理能力，填补了现有基准在融合图像/视频与图形上下文进行地理时空智能评估方面的空白。</p>
<hr />
<h4 id="detailed-summary_7">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有时空基准主要关注以图像/视频为上下文的自我中心视角推理，或以图形为上下文的地理视角推理，但缺乏同时利用图像/视频和图形上下文评估视觉语言模型地理时空智能的基准，这种能力对于交通管理和应急响应等领域至关重要。</p>
<p><strong>Method:</strong> 研究团队开发了GTR-Bench基准，该基准要求模型在地图和视频之间进行多视角切换，对具有非重叠视野的多个视频进行联合推理，并在视频上下文未观测到的时空区域进行推断，从而全面评估地理时空推理能力。</p>
<p><strong>Result:</strong> 对10多个流行视觉语言模型的评估显示，即使最佳专有模型Gemini-2.5-Pro（34.9%）也显著落后于人类表现（78.61%），分析揭示了当前模型在地理时空推理中的三个主要缺陷：时空上下文利用不平衡、时间预测能力弱、地图与多视角视频输入理解对齐能力不足。</p>
<p><strong>Conclusion:</strong> GTR-Bench为时空智能研究提供了宝贵见解和新机遇，揭示了当前视觉语言模型在地理时空推理方面的关键局限性，特别是时空上下文利用不平衡、时间预测能力不足以及地图与视频数据融合困难等问题，为未来模型改进指明了方向。</p>
<hr />
<h4 id="abstract_7">📄 Abstract</h4>
<p>Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has
attracted much attention due to its importance for Autonomous Driving, Embodied
AI and General Artificial Intelligence. Existing spatial-temporal benchmarks
mainly focus on egocentric perspective reasoning with images/video context, or
geographic perspective reasoning with graphics context (eg. a map), thus fail
to assess VLMs' geographic spatial-temporal intelligence with both images/video
and graphics context, which is important for areas like traffic management and
emergency response. To address the gaps, we introduce Geo-Temporal Reasoning
benchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of
moving targets in a large-scale camera network. GTR-Bench is more challenging
as it requires multiple perspective switches between maps and videos, joint
reasoning across multiple videos with non-overlapping fields of view, and
inference over spatial-temporal regions that are unobserved by any video
context. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that
even the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags
behind human performance (78.61%) on geo-temporal reasoning. Moreover, our
comprehensive analysis on GTR-Bench reveals three primary deficiencies of
current models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by
an imbalanced utilization of spatial-temporal context. (2) VLMs are weak in
temporal forecasting, which leads to worse performance on temporal-emphasized
tasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to
comprehend or align the map data with multi-view video inputs. We believe
GTR-Bench offers valuable insights and opens up new opportunities for research
and applications in spatial-temporal intelligence. Benchmark and code will be
released at https://github.com/X-Luffy/GTR-Bench.</p>
<h3 id="9-xyzcylinder-feedforward-reconstruction-for-driving-scenes-based-on-a-unified-cylinder-lifting-method">[9] <a href="https://arxiv.org/abs/2510.07856">XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method</a></h3>
<p><em>Haochen Yu, Qiankun Liu, Hongyuan Liu, Jianfei Jiang, Juntao Lyu, Jiansheng Chen, Huimin Ma</em></p>
<h4 id="tldr_8">🧩 TL;DR</h4>
<p>本文提出XYZCylinder，一种基于统一圆柱体提升方法的前馈模型，通过统一相机建模和混合表示设计，解决了驾驶场景重建中因相机配置变化导致的泛化能力不足和稀疏视图重建精度受限的问题。</p>
<hr />
<h4 id="detailed-summary_8">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前前馈重建范式在驾驶场景重建中存在两个主要限制：固定视图变换在相机配置变化时失效，限制了不同驾驶场景间的泛化能力；360度全景稀疏视图间重叠区域小且驾驶场景复杂，增加了学习难度并降低了重建精度。</p>
<p><strong>Method:</strong> 提出统一圆柱体相机建模策略避免学习视点依赖的空间对应关系，使用可调参数统一不同相机配置；设计基于圆柱平面特征组的混合表示，通过专用模块将2D图像特征提升到3D空间。</p>
<p><strong>Result:</strong> 实验结果表明，XYZCylinder在不同评估设置下均达到最先进性能，并能够以零样本方式泛化到其他驾驶场景。</p>
<p><strong>Conclusion:</strong> 该研究证明了统一相机建模和混合表示在提升驾驶场景重建泛化能力和精度方面的有效性，为处理复杂多相机配置场景提供了新的解决方案。</p>
<hr />
<h4 id="abstract_8">📄 Abstract</h4>
<p>Recently, more attention has been paid to feedforward reconstruction
paradigms, which mainly learn a fixed view transformation implicitly and
reconstruct the scene with a single representation. However, their
generalization capability and reconstruction accuracy are still limited while
reconstructing driving scenes, which results from two aspects: (1) The fixed
view transformation fails when the camera configuration changes, limiting the
generalization capability across different driving scenes equipped with
different camera configurations. (2) The small overlapping regions between
sparse views of the $360^\circ$ panorama and the complexity of driving scenes
increase the learning difficulty, reducing the reconstruction accuracy. To
handle these difficulties, we propose \textbf{XYZCylinder}, a feedforward model
based on a unified cylinder lifting method which involves camera modeling and
feature lifting. Specifically, to improve the generalization capability, we
design a Unified Cylinder Camera Modeling (UCCM) strategy, which avoids the
learning of viewpoint-dependent spatial correspondence and unifies different
camera configurations with adjustable parameters. To improve the reconstruction
accuracy, we propose a hybrid representation with several dedicated modules
based on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D image
features to 3D space. Experimental results show that XYZCylinder achieves
state-of-the-art performance under different evaluation settings, and can be
generalized to other driving scenes in a zero-shot manner. Project page:
\href{https://yuyuyu223.github.io/XYZCYlinder-projectpage/}{here}.</p>
<h3 id="10-marc-memory-augmented-rl-token-compression-for-efficient-video-understanding">[10] <a href="https://arxiv.org/abs/2510.07915">MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding</a></h3>
<p><em>Peiran Wu, Zhuorui Yu, Yunze Liu, Chi-Hao Wu, Enmin Zhou, Junxiao Shen</em></p>
<h4 id="tldr_9">🧩 TL;DR</h4>
<p>本文提出MARC方法，通过结构化检索和强化学习蒸馏实现视频token压缩，在仅使用单帧token的情况下达到接近基线准确率，显著降低计算开销。</p>
<hr />
<h4 id="detailed-summary_9">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 视觉语言模型从图像扩展到视频时面临高帧率和长持续时间带来的沉重计算成本问题，现有免训练token压缩方法会导致信息丢失和性能下降。</p>
<p><strong>Method:</strong> 提出MARC方法，采用检索-压缩策略，包含视觉记忆检索器选择关键片段，以及压缩组相对策略优化框架从教师模型向学生模型蒸馏推理能力。</p>
<p><strong>Result:</strong> 在六个视频基准测试中，MARC仅使用单帧token即可达到接近基线准确率，视觉token减少95%，GPU内存降低72%，延迟减少23.9%。</p>
<p><strong>Conclusion:</strong> 该方法展示了在资源受限场景下实现高效实时视频理解的潜力，适用于视频问答、监控和自动驾驶等应用领域。</p>
<hr />
<h4 id="abstract_9">📄 Abstract</h4>
<p>The rapid progress of large language models (LLMs) has laid the foundation
for multimodal models. However, visual language models (VLMs) still face heavy
computational costs when extended from images to videos due to high frame rates
and long durations. Token compression is a promising solution, yet most
existing training-free methods cause information loss and performance
degradation. To overcome this, we propose \textbf{Memory-Augmented
Reinforcement Learning-based Token Compression (MARC)}, which integrates
structured retrieval and RL-based distillation. MARC adopts a
\textit{retrieve-then-compress} strategy using a \textbf{Visual Memory
Retriever (VMR)} to select key clips and a \textbf{Compression Group Relative
Policy Optimization (C-GRPO)} framework to distil reasoning ability from a
teacher to a student model. Experiments on six video benchmarks show that MARC
achieves near-baseline accuracy using only one frame's tokens -- reducing
visual tokens by \textbf{95\%}, GPU memory by \textbf{72\%}, and latency by
\textbf{23.9\%}. This demonstrates its potential for efficient, real-time video
understanding in resource-constrained settings such as video QA, surveillance,
and autonomous driving.</p>
<h3 id="11-ttom-test-time-optimization-and-memorization-for-compositional-video-generation">[11] <a href="https://arxiv.org/abs/2510.07940">TTOM: Test-Time Optimization and Memorization for Compositional Video Generation</a></h3>
<p><em>Leigang Qu, Ziyang Wang, Na Zheng, Wenjie Wang, Liqiang Nie, Tat-Seng Chua</em></p>
<h4 id="tldr_10">🧩 TL;DR</h4>
<p>本文提出了TTOM（测试时优化与记忆化）框架，一种无需训练的解决方案，通过布局引导的注意力优化和参数化记忆机制，显著提升了视频基础模型在组合场景下的跨模态对齐能力。</p>
<hr />
<h4 id="detailed-summary_10">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 视频基础模型在视觉生成方面表现出色，但在组合场景（如运动、数量关系和空间关系）中表现不佳，存在文本-图像对齐问题，需要一种能够在推理过程中动态优化模型输出的方法。</p>
<p><strong>Method:</strong> 提出了TTOM框架，包含测试时优化和记忆化机制，通过布局引导的注意力目标函数优化新参数，并采用参数化记忆机制支持流式视频生成中的历史上下文管理，支持插入、读取、更新和删除等灵活操作。</p>
<p><strong>Result:</strong> 在T2V-CompBench和Vbench基准测试上的实验结果表明，TTOM能够有效解耦组合性世界知识，展现出强大的可迁移性和泛化能力，成为实现组合视频生成中跨模态对齐的有效、实用、可扩展且高效的框架。</p>
<p><strong>Conclusion:</strong> TTOM框架证明了在推理阶段通过布局引导优化和记忆机制能够显著提升视频基础模型的组合能力，为动态视频生成中的跨模态对齐问题提供了新的解决方案，具有重要的实际应用价值。</p>
<hr />
<h4 id="abstract_10">📄 Abstract</h4>
<p>Video Foundation Models (VFMs) exhibit remarkable visual generation
performance, but struggle in compositional scenarios (e.g., motion, numeracy,
and spatial relation). In this work, we introduce Test-Time Optimization and
Memorization (TTOM), a training-free framework that aligns VFM outputs with
spatiotemporal layouts during inference for better text-image alignment. Rather
than direct intervention to latents or attention per-sample in existing work,
we integrate and optimize new parameters guided by a general layout-attention
objective. Furthermore, we formulate video generation within a streaming
setting, and maintain historical optimization contexts with a parametric memory
mechanism that supports flexible operations, such as insert, read, update, and
delete. Notably, we found that TTOM disentangles compositional world knowledge,
showing powerful transferability and generalization. Experimental results on
the T2V-CompBench and Vbench benchmarks establish TTOM as an effective,
practical, scalable, and efficient framework to achieve cross-modal alignment
for compositional video generation on the fly.</p>
<h3 id="12-cir-cot-towards-interpretable-composed-image-retrieval-via-end-to-end-chain-of-thought-reasoning">[12] <a href="https://arxiv.org/abs/2510.08003">CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning</a></h3>
<p><em>Weihuang Lin, Yiwei Ma, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji</em></p>
<h4 id="tldr_11">🧩 TL;DR</h4>
<p>本文提出了CIR-CoT，这是首个集成显式思维链推理的端到端检索导向多模态大语言模型，通过生成可解释的推理链来增强跨模态交互理解，在保持竞争力的检索性能的同时实现决策过程透明化。</p>
<hr />
<h4 id="detailed-summary_11">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前基于视觉语言模型和多模态大语言模型的组合图像检索方法主要作为黑箱运行，这种不透明性不仅阻碍用户理解检索原理，还限制了模型遵循复杂细粒度指令的能力，需要解决这一可解释性和推理能力不足的问题。</p>
<p><strong>Method:</strong> CIR-CoT通过强制模型首先生成可解释的推理链来增强跨模态交互捕获能力，采用三阶段标注流程创建结构化思维链注释，包括描述、推理和结论，然后对模型进行微调以产生这种结构化输出，最后将其检索意图编码到专用嵌入中。</p>
<p><strong>Result:</strong> 综合实验表明，CIR-CoT在领域内数据集（FashionIQ、CIRR）上实现了高度竞争力的性能，并在领域外CIRCO数据集上展现出卓越的泛化能力，为更有效和可信的检索系统开辟了新路径。</p>
<p><strong>Conclusion:</strong> 该研究确立了通过显式思维链推理增强多模态检索模型的新范式，不仅提升了检索准确性，更重要的是实现了决策过程的可解释性，为构建更透明和可信的检索系统提供了重要方向。</p>
<hr />
<h4 id="abstract_11">📄 Abstract</h4>
<p>Composed Image Retrieval (CIR), which aims to find a target image from a
reference image and a modification text, presents the core challenge of
performing unified reasoning across visual and semantic modalities. While
current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more
recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown
progress, they predominantly function as ``black boxes." This inherent opacity
not only prevents users from understanding the retrieval rationale but also
restricts the models' ability to follow complex, fine-grained instructions. To
overcome these limitations, we introduce CIR-CoT, the first end-to-end
retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT)
reasoning. By compelling the model to first generate an interpretable reasoning
chain, CIR-CoT enhances its ability to capture crucial cross-modal
interactions, leading to more accurate retrieval while making its decision
process transparent. Since existing datasets like FashionIQ and CIRR lack the
necessary reasoning data, a key contribution of our work is the creation of
structured CoT annotations using a three-stage process involving a caption,
reasoning, and conclusion. Our model is then fine-tuned to produce this
structured output before encoding its final retrieval intent into a dedicated
embedding. Comprehensive experiments show that CIR-CoT achieves highly
competitive performance on in-domain datasets (FashionIQ, CIRR) and
demonstrates remarkable generalization on the out-of-domain CIRCO dataset,
establishing a new path toward more effective and trustworthy retrieval
systems.</p>
<h3 id="13-darkhash-a-data-free-backdoor-attack-against-deep-hashing">[13] <a href="https://arxiv.org/abs/2510.08094">DarkHash: A Data-Free Backdoor Attack Against Deep Hashing</a></h3>
<p><em>Ziqi Zhou, Menghao Deng, Yufei Song, Hangtao Zhang, Wei Wan, Shengshan Hu, Minghui Li, Leo Yu Zhang, Dezhong Yao</em></p>
<h4 id="tldr_12">🧩 TL;DR</h4>
<p>本文提出了DarkHash，这是首个针对深度哈希模型的无数据后门攻击方法，通过设计具有双语义指导的影子后门攻击框架，仅使用代理数据集微调特定层即可在保持原始检索精度的同时嵌入后门功能。</p>
<hr />
<h4 id="detailed-summary_12">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有深度哈希后门攻击方法需要访问训练数据集来植入后门，但在现实世界中由于隐私保护和知识产权考虑，获取此类数据往往被禁止，因此开发无需训练数据即可嵌入后门并保持原始检索精度的攻击方法成为一个新颖且具有挑战性的问题。</p>
<p><strong>Method:</strong> 提出了一种新颖的影子后门攻击框架，采用双语义指导机制，通过仅微调受害者模型的特定层并使用代理数据集来嵌入后门功能，同时设计了拓扑对齐损失函数，利用样本与其邻居之间的关系来优化个体和相邻中毒样本朝向目标样本，进一步增强攻击能力。</p>
<p><strong>Result:</strong> 在四个图像数据集、五种模型架构和两种哈希方法上的实验结果表明，DarkHash具有高度有效性，优于现有的最先进后门攻击方法，并且防御实验显示DarkHash能够抵御现有的主流后门防御方法。</p>
<p><strong>Conclusion:</strong> 该研究证明了深度哈希模型在无数据场景下仍然容易受到后门攻击，提出了首个有效的无数据后门攻击框架，揭示了深度哈希模型在现实部署中的安全风险，并为未来防御机制的设计提供了重要参考。</p>
<hr />
<h4 id="abstract_12">📄 Abstract</h4>
<p>Benefiting from its superior feature learning capabilities and efficiency,
deep hashing has achieved remarkable success in large-scale image retrieval.
Recent studies have demonstrated the vulnerability of deep hashing models to
backdoor attacks. Although these studies have shown promising attack results,
they rely on access to the training dataset to implant the backdoor. In the
real world, obtaining such data (e.g., identity information) is often
prohibited due to privacy protection and intellectual property concerns.
Embedding backdoors into deep hashing models without access to the training
data, while maintaining retrieval accuracy for the original task, presents a
novel and challenging problem. In this paper, we propose DarkHash, the first
data-free backdoor attack against deep hashing. Specifically, we design a novel
shadow backdoor attack framework with dual-semantic guidance. It embeds
backdoor functionality and maintains original retrieval accuracy by fine-tuning
only specific layers of the victim model using a surrogate dataset. We consider
leveraging the relationship between individual samples and their neighbors to
enhance backdoor attacks during training. By designing a topological alignment
loss, we optimize both individual and neighboring poisoned samples toward the
target sample, further enhancing the attack capability. Experimental results on
four image datasets, five model architectures, and two hashing methods
demonstrate the high effectiveness of DarkHash, outperforming existing
state-of-the-art backdoor attack methods. Defense experiments show that
DarkHash can withstand existing mainstream backdoor defense methods.</p>
<h3 id="14-improving-temporal-understanding-logic-consistency-in-video-language-models-via-attention-enhancement">[14] <a href="https://arxiv.org/abs/2510.08138">Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement</a></h3>
<p><em>Chengzhi Li, Heyan Huang, Ping Jian, Zhen Yang, Yaning Tian</em></p>
<h4 id="tldr_13">🧩 TL;DR</h4>
<p>本文提出了一种称为时间条件注意力锐化（TCAS）的方法，通过增强跨模态注意力头的时间分辨能力来解决视频语言模型中的时间逻辑不一致问题，显著提升了模型的时间理解一致性。</p>
<hr />
<h4 id="detailed-summary_13">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 大型语言模型在视频语言模型中存在输出自相矛盾的问题，特别是在对基于其基础输出的重述问题时无法提供逻辑一致的响应，这种现象严重影响了模型的可靠性并阻碍了实际应用，但其根本原因尚未得到充分探索。</p>
<p><strong>Method:</strong> 采用可解释性驱动的方法分析、统计总结并干预该现象的可能因素，发现跨模态注意力头无法有效区分不同时间戳的视频标记是响应不一致的主要原因，为此提出了时间条件注意力锐化方法，通过构建基于注意力差异的增强目标来提升模型的时间分辨能力。</p>
<p><strong>Result:</strong> 实验结果表明该方法显著提升了视频语言模型的时间逻辑一致性，可解释性分析进一步证实了该方法确实改善了注意力头的时间区分能力，同时在通用视频时间定位任务中实现了性能提升，表明时间逻辑一致性是时间理解的瓶颈。</p>
<p><strong>Conclusion:</strong> 通过增强时间逻辑一致性，该方法推动了视频时间理解的显著进展，证明了时间分辨能力是视频语言模型一致性的关键因素，为改进多模态模型的时间理解能力提供了有效途径。</p>
<hr />
<h4 id="abstract_13">📄 Abstract</h4>
<p>Large language models (LLMs) often generate self-contradictory outputs, which
severely impacts their reliability and hinders their adoption in practical
applications. In video-language models (Video-LLMs), this phenomenon recently
draws the attention of researchers. Specifically, these models fail to provide
logically consistent responses to rephrased questions based on their grounding
outputs. However, the underlying causes of this phenomenon remain
underexplored. In this work, we adopt an interpretability-driven approach to
analyze, statistically summarize, and intervention the potential factors of the
phenomenon. We find that one of the primary reasons for the inconsistency in
responses lies in the inability of cross-modal attention heads to effectively
distinguish video tokens across different timestamps. To address this, we
propose an attention enhancement method called Temporally Conditioned Attention
Sharpening (TCAS), which constructs an enhancement objective based on attention
distinctions to enhance the model's temporal resolution capability, thereby
improving its temporal understanding logic consistency. Experimental results
demonstrate that our method significantly enhances the temporal logic
consistency of Video-LLMs. Further interpretability analyses reveal that our
method indeed improves the temporal discriminability of attention heads,
validating our conclusions. Additionally, our method achieves performance
improvements in general video temporal grounding tasks, highlighting that
temporal logic consistency is a bottleneck in temporal understanding. By
enhancing consistency, our method drives significant progress in video temporal
understanding.</p>
<h3 id="15-unimmvsr-a-unified-multi-modal-framework-for-cascaded-video-super-resolution">[15] <a href="https://arxiv.org/abs/2510.08143">UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution</a></h3>
<p><em>Shian Du, Menghan Xia, Chang Liu, Quande Liu, Xintao Wang, Pengfei Wan, Xiangyang Ji</em></p>
<h4 id="tldr_14">🧩 TL;DR</h4>
<p>本文提出了UniMMVSR，这是首个统一的多模态生成式视频超分辨率框架，能够整合文本、图像和视频等多种生成条件，显著提升了视频生成的质量和条件一致性。</p>
<hr />
<h4 id="detailed-summary_14">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有级联视频超分辨率方法主要局限于文本到视频任务，未能充分利用文本之外的多模态生成条件，这在确保多模态视频生成保真度方面存在明显不足。</p>
<p><strong>Method:</strong> 提出了UniMMVSR统一框架，在潜在视频扩散模型中系统探索了条件注入策略、训练方案和数据混合技术，针对不同条件类型设计了专门的数据构建和条件利用方法。</p>
<p><strong>Result:</strong> 实验表明UniMMVSR显著优于现有方法，生成的视频具有更丰富的细节和更高的多模态条件一致性，并验证了与基础模型结合实现4K视频多模态引导生成的可行性。</p>
<p><strong>Conclusion:</strong> 该研究证明了多模态条件在视频超分辨率中的重要性，为高质量视频生成提供了新的技术路径，突破了现有方法在4K视频生成方面的限制。</p>
<hr />
<h4 id="abstract_14">📄 Abstract</h4>
<p>Cascaded video super-resolution has emerged as a promising technique for
decoupling the computational burden associated with generating high-resolution
videos using large foundation models. Existing studies, however, are largely
confined to text-to-video tasks and fail to leverage additional generative
conditions beyond text, which are crucial for ensuring fidelity in multi-modal
video generation. We address this limitation by presenting UniMMVSR, the first
unified generative video super-resolution framework to incorporate hybrid-modal
conditions, including text, images, and videos. We conduct a comprehensive
exploration of condition injection strategies, training schemes, and data
mixture techniques within a latent video diffusion model. A key challenge was
designing distinct data construction and condition utilization methods to
enable the model to precisely utilize all condition types, given their varied
correlations with the target video. Our experiments demonstrate that UniMMVSR
significantly outperforms existing methods, producing videos with superior
detail and a higher degree of conformity to multi-modal conditions. We also
validate the feasibility of combining UniMMVSR with a base model to achieve
multi-modal guided generation of 4K video, a feat previously unattainable with
existing techniques.</p>
<h3 id="16-beyond-textual-cot-interleaved-text-image-chains-with-deep-confidence-reasoning-for-image-editing">[16] <a href="https://arxiv.org/abs/2510.08157">Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing</a></h3>
<p><em>Zhentao Zou, Zhengrong Yue, Kunpeng Du, Binlei Bao, Hanting Li, Haizhen Xie, Guozheng Xu, Yue Zhou, Yali Wang, Jie Hu, Xue Jiang, Xinghao Chen</em></p>
<h4 id="tldr_15">🧩 TL;DR</h4>
<p>本文提出了MURE框架，通过引入多模态交错文本-图像思维链，将视觉编辑过程从纯文本推理转向文本与视觉理性交替的推理过程，显著提升了复杂图像编辑任务的精度和保真度。</p>
<hr />
<h4 id="detailed-summary_15">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有基于自然语言的图像编辑方法在处理复杂对象交叉和细粒度空间关系时存在困难，主要原因是缺乏显式推理过程。纯文本思维链或坐标增强思维链在表示复杂视觉布局方面存在根本限制，且缺乏足够的视觉线索来指导像素级细节生成。</p>
<p><strong>Method:</strong> 提出了多模态推理编辑框架，采用原生多模态交错文本-图像思维链，在推理链的每一步中，文本描述后跟随相应的视觉线索，如位置掩码或新内容表示。同时引入了多模态深度置信推理范式，通过奖励模型的深度置信分数修剪低质量分支，确保模型始终沿着高质量轨迹进行编辑。</p>
<p><strong>Result:</strong> 该方法在三个图像编辑基准测试中均取得了显著改进，通过将复杂编辑任务分解为相互依赖的子任务，在每个阶段实现了更高的精度，并产生了高保真度的编辑结果。同时发布了首个CoT-Edit-14K数据集，包含14K个高质量编辑示例。</p>
<p><strong>Conclusion:</strong> 该研究证明了从纯文本推理转向多模态交错推理在视觉编辑任务中的有效性，通过显式分解复杂编辑任务和视觉线索引导，显著提升了编辑精度和结果质量，为多模态推理在视觉任务中的应用开辟了新方向。</p>
<hr />
<h4 id="abstract_15">📄 Abstract</h4>
<p>Image editing with natural language has gained significant popularity, yet
existing methods struggle with intricate object intersections and fine-grained
spatial relationships due to the lack of an explicit reasoning process. While
Chain-of-Thought (CoT) has been explored to enhance reasoning, purely textual
CoT or CoT augmented with coordinate information is fundamentally limited in
its ability to represent intricate visual layouts and lacks the necessary
visual cues to guide the generation of fine-grained, pixel-level details. To
address these challenges, we propose Multimodal Reasoning Edit (MURE), a novel
framework that shifts the visual editing process from purely text-based
reasoning to a series of interleaved textual and visual rationales. Our
framework performs image editing using a natively multimodal, interleaved
text-image CoT. This approach generates a step-by-step chain of reasoning where
a textual description is followed by a corresponding visual cue, such as a
positional mask that defined intended edited regions or a representation of new
content. Furthermore, to mitigate the hallucination phenomenon of large
language models, we introduce Multimodal Deep Confidence (MMDC) reasoning
paradigm. This paradigm explores a tree of visual reasoning paths at each step.
By pruning low-quality branches using a deep confidence score from a reward
model, it ensures the model consistently follows a high-quality trajectory
towards the final edited result. The proposed method decomposes complex editing
tasks into interdependent sub-tasks, achieving greater precision at each stage
and yielding high-fidelity edited results. We define the formulation for
interleaved text-image chains and release the first CoT-Edit-14K dataset,
comprising 14K high-quality editing examples. Extensive experiments show that
our method yields significant improvements across three image editing
benchmarks.</p>
<h3 id="17-instructudrag-joint-text-instructions-and-object-dragging-for-interactive-image-editing">[17] <a href="https://arxiv.org/abs/2510.08181">InstructUDrag: Joint Text Instructions and Object Dragging for Interactive Image Editing</a></h3>
<p><em>Haoran Yu, Yi Shi</em></p>
<h4 id="tldr_16">🧩 TL;DR</h4>
<p>本文提出了InstructUDrag框架，将文本指令与对象拖拽相结合，实现了同时进行对象拖拽和基于文本的图像编辑，解决了现有方法在精确定位和语义控制方面的局限性。</p>
<hr />
<h4 id="detailed-summary_16">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有的文本到图像扩散模型编辑方法存在固有局限：基于文本的方法难以实现精确的对象定位，而对象拖拽方法仅限于静态重定位，无法同时实现精确移动和语义属性控制。</p>
<p><strong>Method:</strong> 提出InstructUDrag框架，将对象拖拽视为图像重建过程，分为两个协同分支：移动重建分支使用基于能量的梯度引导精确定位对象，文本驱动编辑分支共享梯度信号确保一致变换；采用DDPM反演和先验信息注入以保持移动对象的结构完整性。</p>
<p><strong>Result:</strong> 大量实验表明，InstructUDrag实现了灵活、高保真度的图像编辑，在对象重定位精度和图像内容语义控制方面均表现出色，能够同时满足精确定位和细粒度属性控制的需求。</p>
<p><strong>Conclusion:</strong> 该研究证明了结合文本指令和对象拖拽的协同框架在图像编辑中的有效性，为扩散模型提供了更灵活、精确的编辑能力，推动了多模态交互在图像生成和编辑中的应用发展。</p>
<hr />
<h4 id="abstract_16">📄 Abstract</h4>
<p>Text-to-image diffusion models have shown great potential for image editing,
with techniques such as text-based and object-dragging methods emerging as key
approaches. However, each of these methods has inherent limitations: text-based
methods struggle with precise object positioning, while object dragging methods
are confined to static relocation. To address these issues, we propose
InstructUDrag, a diffusion-based framework that combines text instructions with
object dragging, enabling simultaneous object dragging and text-based image
editing. Our framework treats object dragging as an image reconstruction
process, divided into two synergistic branches. The moving-reconstruction
branch utilizes energy-based gradient guidance to move objects accurately,
refining cross-attention maps to enhance relocation precision. The text-driven
editing branch shares gradient signals with the reconstruction branch, ensuring
consistent transformations and allowing fine-grained control over object
attributes. We also employ DDPM inversion and inject prior information into
noise maps to preserve the structure of moved objects. Extensive experiments
demonstrate that InstructUDrag facilitates flexible, high-fidelity image
editing, offering both precision in object relocation and semantic control over
image content.</p>
<h3 id="18-a-multimodal-depth-aware-method-for-embodied-reference-understanding">[18] <a href="https://arxiv.org/abs/2510.08278">A Multimodal Depth-Aware Method For Embodied Reference Understanding</a></h3>
<p><em>Fevziye Irem Eyiokur, Dogucan Yaman, Hazım Kemal Ekenel, Alexander Waibel</em></p>
<h4 id="tldr_17">🧩 TL;DR</h4>
<p>本文提出了一种新颖的具身参考理解框架，通过联合利用LLM数据增强、深度图模态和深度感知决策模块，显著提升了在复杂场景中的目标物体识别准确性。</p>
<hr />
<h4 id="detailed-summary_17">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有开放词汇目标检测方法在场景中存在多个候选物体的模糊情况下往往失效，无法有效处理具身参考理解中的歧义性问题，这限制了在复杂或杂乱环境中的实际应用效果。</p>
<p><strong>Method:</strong> 提出的ERU框架整合了基于LLM的数据增强技术、深度图模态信息以及深度感知决策模块，实现了语言指令和具身线索的鲁棒融合，特别增强了在复杂环境中的消歧能力。</p>
<p><strong>Result:</strong> 在两个数据集上的实验结果表明，该方法显著优于现有基线方法，实现了更准确和可靠的参考目标检测性能，在复杂场景中表现出色。</p>
<p><strong>Conclusion:</strong> 该研究证明了多模态信息融合和深度感知机制在具身参考理解中的重要性，为处理复杂环境中的视觉语言任务提供了有效的解决方案，具有重要的实际应用价值。</p>
<hr />
<h4 id="abstract_17">📄 Abstract</h4>
<p>Embodied Reference Understanding requires identifying a target object in a
visual scene based on both language instructions and pointing cues. While prior
works have shown progress in open-vocabulary object detection, they often fail
in ambiguous scenarios where multiple candidate objects exist in the scene. To
address these challenges, we propose a novel ERU framework that jointly
leverages LLM-based data augmentation, depth-map modality, and a depth-aware
decision module. This design enables robust integration of linguistic and
embodied cues, improving disambiguation in complex or cluttered environments.
Experimental results on two datasets demonstrate that our approach
significantly outperforms existing baselines, achieving more accurate and
reliable referent detection.</p>
<h3 id="19-unlocking-3d-affordance-segmentation-with-2d-semantic-knowledge">[19] <a href="https://arxiv.org/abs/2510.08316">Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge</a></h3>
<p><em>Yu Huang, Zelin Peng, Changsong Wen, Xiaokang Yang, Wei Shen</em></p>
<h4 id="tldr_18">🧩 TL;DR</h4>
<p>本文提出了一种语义基础的学习范式，通过跨模态亲和力迁移将大规模2D视觉基础模型的丰富语义知识转移到3D领域，在3D功能分割任务上实现了新的最先进性能。</p>
<hr />
<h4 id="detailed-summary_18">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有3D功能分割方法通常依赖点云编码器作为通用特征提取器，但忽视了3D数据固有的稀疏性、噪声和几何模糊性等挑战，导致学习到的3D特征缺乏清晰且语义一致的功能边界。</p>
<p><strong>Method:</strong> 提出了跨模态亲和力迁移预训练策略，将3D编码器与提升的2D语义对齐，并联合优化重构、亲和力和多样性以产生语义组织的表示；在此基础上设计了跨模态功能分割变换器，集成多模态提示与CMAT预训练特征以生成精确的提示感知分割图。</p>
<p><strong>Result:</strong> 在标准基准测试上的广泛实验表明，该框架在3D功能分割任务上建立了新的最先进结果，显著提升了分割精度和语义一致性。</p>
<p><strong>Conclusion:</strong> 该研究证明了将2D视觉基础模型的语义知识迁移到3D领域的有效性，为3D功能分割提供了一种新的语义基础学习范式，对机器人操作、具身AI和增强现实等应用具有重要意义。</p>
<hr />
<h4 id="abstract_18">📄 Abstract</h4>
<p>Affordance segmentation aims to parse 3D objects into functionally distinct
parts, bridging recognition and interaction for applications in robotic
manipulation, embodied AI, and AR. While recent studies leverage visual or
textual prompts to guide this process, they often rely on point cloud encoders
as generic feature extractors, overlooking the intrinsic challenges of 3D data
such as sparsity, noise, and geometric ambiguity. As a result, 3D features
learned in isolation frequently lack clear and semantically consistent
functional boundaries. To address this bottleneck, we propose a
semantic-grounded learning paradigm that transfers rich semantic knowledge from
large-scale 2D Vision Foundation Models (VFMs) into the 3D domain.
Specifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training
strategy that aligns a 3D encoder with lifted 2D semantics and jointly
optimizes reconstruction, affinity, and diversity to yield semantically
organized representations. Building on this backbone, we further design the
Cross-modal Affordance Segmentation Transformer (CAST), which integrates
multi-modal prompts with CMAT-pretrained features to generate precise,
prompt-aware segmentation maps. Extensive experiments on standard benchmarks
demonstrate that our framework establishes new state-of-the-art results for 3D
affordance segmentation.</p>
<h3 id="20-evaluating-small-vision-language-models-on-distance-dependent-traffic-perception">[20] <a href="https://arxiv.org/abs/2510.08352">Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception</a></h3>
<p><em>Nikos Theodoridis, Tim Brophy, Reenu Mohandas, Ganesh Sistu, Fiachra Collins, Anthony Scanlan, Ciaran Eising</em></p>
<h4 id="tldr_19">🧩 TL;DR</h4>
<p>本文提出了首个专注于交通场景感知的距离标注视觉问答基准DTPQA，通过评估小型视觉语言模型发现其在感知任务上显著落后于人类表现，特别是在远距离感知和方向辨别等关键任务上存在明显不足。</p>
<hr />
<h4 id="detailed-summary_19">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前视觉语言模型在自动驾驶等安全关键应用中缺乏可靠的感知系统评估，特别是对于远距离（30米以上）交通场景的感知能力不足，现有基准往往包含推理任务而无法准确衡量纯感知性能，且自动驾驶硬件限制无法支持大型模型部署。</p>
<p><strong>Method:</strong> 提出了距离标注交通感知问答基准DTPQA，该基准专门针对交通场景设计，排除需要推理的问题以确保仅评估感知能力，通过距离标注区分近距离（20米内）和远距离（30米以上）感知任务，并重点评估了多个最先进的小型视觉语言模型。</p>
<p><strong>Result:</strong> 实验结果显示最佳小型视觉语言模型在DTPQA上的平均准确率仅为约60%，远低于人类约85%的表现，模型在远距离感知任务上表现显著下降，特别是在区分左右方向等基本感知任务上存在明显困难，尽管人类样本数量有限存在统计限制。</p>
<p><strong>Conclusion:</strong> 研究表明当前小型视觉语言模型在交通场景感知任务上仍存在显著差距，特别是在远距离感知和方向辨别等关键能力上，这为自动驾驶系统的安全部署提出了重要挑战，未来需要开发更强大的小型模型和专门的感知优化技术。</p>
<hr />
<h4 id="abstract_19">📄 Abstract</h4>
<p>Vision-Language Models (VLMs) are becoming increasingly powerful,
demonstrating strong performance on a variety of tasks that require both visual
and textual understanding. Their strong generalisation abilities make them a
promising component for automated driving systems, which must handle unexpected
corner cases. However, to be trusted in such safety-critical applications, a
model must first possess a reliable perception system. Moreover, since critical
objects and agents in traffic scenes are often at a distance, we require
systems that are not "shortsighted", i.e., systems with strong perception
capabilities at both close (up to 20 meters) and long (30+ meters) range. With
this in mind, we introduce Distance-Annotated Traffic Perception Question
Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused
solely on perception-based questions in traffic scenes, enriched with distance
annotations. By excluding questions that require reasoning, we ensure that
model performance reflects perception capabilities alone. Since automated
driving hardware has limited processing power and cannot support large VLMs,
our study centers on smaller VLMs. More specifically, we evaluate several
state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the
simplicity of the questions, these models significantly underperform compared
to humans (~60% average accuracy for the best-performing small VLM versus ~85%
human performance). However, it is important to note that the human sample size
was relatively small, which imposes statistical limitations. We also identify
specific perception tasks, such as distinguishing left from right, that remain
particularly challenging for these models.</p>
<h3 id="21-univideo-unified-understanding-generation-and-editing-for-videos">[21] <a href="https://arxiv.org/abs/2510.08377">UniVideo: Unified Understanding, Generation, and Editing for Videos</a></h3>
<p><em>Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhu Chen</em></p>
<h4 id="tldr_20">🧩 TL;DR</h4>
<p>本文提出了UniVideo，一个统一的多模态视频生成与编辑框架，通过双流架构将多模态大语言模型与多模态DiT相结合，实现了在单一模型中处理多种视频任务的统一范式。</p>
<hr />
<h4 id="detailed-summary_20">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有的统一多模态模型主要局限于图像领域，视频领域的统一建模仍存在空白，无法有效处理复杂的多模态指令和保持视觉一致性。</p>
<p><strong>Method:</strong> UniVideo采用双流设计，结合多模态大语言模型进行指令理解，以及多模态DiT进行视频生成，通过联合训练统一处理文本/图像到视频生成、上下文视频生成和编辑等多种任务。</p>
<p><strong>Result:</strong> 实验表明UniVideo在多个视频生成和编辑任务上达到或超越了最先进的任务专用基线模型，并展现出任务组合和零样本泛化能力。</p>
<p><strong>Conclusion:</strong> UniVideo的统一设计实现了任务组合和跨模态能力迁移，为视频领域的统一多模态建模开辟了新方向，其框架将公开发布以促进未来研究。</p>
<hr />
<h4 id="abstract_20">📄 Abstract</h4>
<p>Unified multimodal models have shown promising results in multimodal content
generation and editing but remain largely limited to the image domain. In this
work, we present UniVideo, a versatile framework that extends unified modeling
to the video domain. UniVideo adopts a dual-stream design, combining a
Multimodal Large Language Model (MLLM) for instruction understanding with a
Multimodal DiT (MMDiT) for video generation. This design enables accurate
interpretation of complex multimodal instructions while preserving visual
consistency. Built on this architecture, UniVideo unifies diverse video
generation and editing tasks under a single multimodal instruction paradigm and
is jointly trained across them. Extensive experiments demonstrate that UniVideo
matches or surpasses state-of-the-art task-specific baselines in
text/image-to-video generation, in-context video generation and in-context
video editing. Notably, the unified design of UniVideo enables two forms of
generalization. First, UniVideo supports task composition, such as combining
editing with style transfer, by integrating multiple capabilities within a
single instruction. Second, even without explicit training on free-form video
editing, UniVideo transfers its editing capability from large-scale image
editing data to this setting, handling unseen instructions such as
green-screening characters or changing materials within a video. Beyond these
core capabilities, UniVideo also supports visual-prompt-based video generation,
where the MLLM interprets visual prompts and guides the MMDiT during synthesis.
To foster future research, we will release our model and code.</p>
<h3 id="22-the-visual-iconicity-challenge-evaluating-vision-language-models-on-sign-language-form-meaning-mapping">[22] <a href="https://arxiv.org/abs/2510.08482">The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping</a></h3>
<p><em>Onur Keleş, Aslı Özyürek, Gerardo Ortega, Kadir Gökgö, Esam Ghaleb</em></p>
<h4 id="tldr_21">🧩 TL;DR</h4>
<p>本文提出了视觉象似性挑战基准，通过评估13种最先进的视觉语言模型在手语象似性任务上的表现，发现模型在视觉基础能力方面仍远低于人类水平，但更强的语音形式预测能力与人类象似性判断存在相关性。</p>
<hr />
<h4 id="detailed-summary_21">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 该研究旨在解决视觉语言模型从动态人体运动而非静态上下文中恢复语言形式与意义之间本质映射的挑战，手语中的象似性为视觉基础提供了自然测试平台。</p>
<p><strong>Method:</strong> 研究引入了视觉象似性挑战基准，采用心理语言学测量方法评估视觉语言模型在三个任务上的表现：语音符号形式预测、透明度推理和分级象似性评分，在零样本和少样本设置下对13种最先进模型进行评估。</p>
<p><strong>Result:</strong> 在语音形式预测任务中，视觉语言模型能够恢复部分手形和位置细节但低于人类表现；在透明度任务中远未达到人类基线；只有顶级模型与人类象似性评分呈中等相关性，且语音形式预测能力更强的模型与人类象似性判断相关性更高。</p>
<p><strong>Conclusion:</strong> 研究验证了这些诊断任务的有效性，表明模型对视觉基础结构具有共享敏感性，强调了采用以人为本的信号和具身学习方法对于建模象似性和改进多模态模型中视觉基础的重要性。</p>
<hr />
<h4 id="abstract_21">📄 Abstract</h4>
<p>Iconicity, the resemblance between linguistic form and meaning, is pervasive
in signed languages, offering a natural testbed for visual grounding. For
vision-language models (VLMs), the challenge is to recover such essential
mappings from dynamic human motion rather than static context. We introduce the
\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts
psycholinguistic measures to evaluate VLMs on three tasks: (i) phonological
sign-form prediction (e.g., handshape, location), (ii) transparency (inferring
meaning from visual form), and (iii) graded iconicity ratings. We assess $13$
state-of-the-art VLMs in zero- and few-shot settings on Sign Language of the
Netherlands and compare them to human baselines. On \textit{phonological form
prediction}, VLMs recover some handshape and location detail but remain below
human performance; on \textit{transparency}, they are far from human baselines;
and only top models correlate moderately with human \textit{iconicity ratings}.
Interestingly, \textit{models with stronger phonological form prediction
correlate better with human iconicity judgment}, indicating shared sensitivity
to visually grounded structure. Our findings validate these diagnostic tasks
and motivate human-centric signals and embodied learning methods for modelling
iconicity and improving visual grounding in multimodal models.</p>
<h3 id="23-videoverse-how-far-is-your-t2v-generator-from-a-world-model">[23] <a href="https://arxiv.org/abs/2510.08398">VideoVerse: How Far is Your T2V Generator from a World Model?</a></h3>
<p><em>Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, Lei Zhang</em></p>
<h4 id="tldr_22">🧩 TL;DR</h4>
<p>本文提出了VideoVerse基准测试，旨在解决现有文本到视频生成评估方法的不足，通过关注事件级时间因果关系和世界知识来评估T2V模型构建世界模型的能力。</p>
<hr />
<h4 id="detailed-summary_22">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有文本到视频生成基准测试存在三个主要问题：当前评估维度无法区分最先进的T2V模型；事件级时间因果关系这一关键维度被严重忽视；缺乏对构建世界模型所需世界知识的系统性评估。</p>
<p><strong>Method:</strong> 研究团队收集跨领域代表性视频并提取具有内在时间因果关系的事件级描述，通过独立标注者重写为文本到视频提示，设计了包含十个评估维度的二元评估问题，并开发了基于现代视觉语言模型的人类偏好对齐QA评估流程。</p>
<p><strong>Result:</strong> VideoVerse基准包含300个精心策划的提示，涉及815个事件和793个二元评估问题，对最先进的开源和闭源T2V模型进行了系统性评估，深入分析了当前T2V生成器与世界模型之间的差距。</p>
<p><strong>Conclusion:</strong> 研究表明当前T2V生成器在理解复杂时间因果关系和世界知识方面仍存在显著不足，VideoVerse为评估T2V模型构建世界模型的能力提供了全面基准，揭示了未来研究方向。</p>
<hr />
<h4 id="abstract_22">📄 Abstract</h4>
<p>The recent rapid advancement of Text-to-Video (T2V) generation technologies,
which are critical to build ``world models'', makes the existing benchmarks
increasingly insufficient to evaluate state-of-the-art T2V models. First,
current evaluation dimensions, such as per-frame aesthetic quality and temporal
consistency, are no longer able to differentiate state-of-the-art T2V models.
Second, event-level temporal causality, which not only distinguishes video from
other modalities but also constitutes a crucial component of world models, is
severely underexplored in existing benchmarks. Third, existing benchmarks lack
a systematic assessment of world knowledge, which are essential capabilities
for building world models. To address these issues, we introduce VideoVerse, a
comprehensive benchmark that focuses on evaluating whether a T2V model could
understand complex temporal causality and world knowledge in the real world. We
collect representative videos across diverse domains (e.g., natural landscapes,
sports, indoor scenes, science fiction, chemical and physical experiments) and
extract their event-level descriptions with inherent temporal causality, which
are then rewritten into text-to-video prompts by independent annotators. For
each prompt, we design a suite of binary evaluation questions from the
perspective of dynamic and static properties, with a total of ten carefully
defined evaluation dimensions. In total, our VideoVerse comprises 300 carefully
curated prompts, involving 815 events and 793 binary evaluation questions.
Consequently, a human preference aligned QA-based evaluation pipeline is
developed by using modern vision-language models. Finally, we perform a
systematic evaluation of state-of-the-art open-source and closed-source T2V
models on VideoVerse, providing in-depth analysis on how far the current T2V
generators are from world models.</p>
<h3 id="24-to-sink-or-not-to-sink-visual-information-pathways-in-large-vision-language-models">[24] <a href="https://arxiv.org/abs/2510.08510">To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models</a></h3>
<p><em>Jiayun Luo, Wan-Cyuan Fan, Lyuyang Wang, Xiangteng He, Tanzila Rahman, Purang Abolmaesumi, Leonid Sigal</em></p>
<h4 id="tldr_23">🧩 TL;DR</h4>
<p>本文发现并研究了ViT注意力汇聚点——高范数视觉标记，这些标记包含丰富的语义信息但常被现有LVLM架构忽略。通过显式利用这些标记，研究显著提升了多种LVLM在视觉推理任务上的性能。</p>
<hr />
<h4 id="detailed-summary_23">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有研究主要关注LLM内部的注意力汇聚点，而忽略了视觉编码器ViT中高范数视觉标记的重要作用。这些ViT注意力汇聚点包含高层次语义概念，但当前LVLM架构未能有效利用这些关键信息，限制了视觉理解与推理能力。</p>
<p><strong>Method:</strong> 研究提出定性和定量分析ViT汇聚点标记中嵌入的信息，并开发了无需训练和基于训练的两种方法来优化LLM对这些标记的解读。方法通过显式利用ViT注意力汇聚点来增强视觉信息向语言模型的传播效果。</p>
<p><strong>Result:</strong> 实验表明显式利用ViT注意力汇聚点可在多种LVLM架构和视觉推理任务上带来显著性能提升。这些高范数视觉标记被证实包含关键语义信息，能够有效增强模型的视觉理解与推理能力。</p>
<p><strong>Conclusion:</strong> ViT注意力汇聚点是LVLM中未被充分挖掘的重要资源，其包含的丰富语义信息对视觉推理至关重要。未来LVLM设计应更关注视觉编码器中的关键标记，优化视觉信息向语言模型的传播机制。</p>
<hr />
<h4 id="abstract_23">📄 Abstract</h4>
<p>Large Vision Language Models (LVLMs) have recently emerged as powerful
architectures capable of understanding and reasoning over both visual and
textual information. These models typically rely on two key components: a
Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual
content into a sequence of image tokens and serves as the perceptual front-end
-- the eyes of the model. In contrast, the LLM interprets these tokens to
perform high-level reasoning, generates responses, and functions as the
cognitive core -- the brain of the model. However, it remains unclear which
visual tokens contribute most significantly to understanding and reasoning, and
how effectively these signals are propagated from ViT to the LLM. While most
existing works have focused on identifying attention sinks, low-semantic tokens
receiving disproportionately high attention, within the LLM, we shift the focus
to the vision encoder by identifying a class of high-norm visual tokens from
ViT, referred to as ViT attention sinks -- a problem that has been rarely
studied but is indeed very important for LVLMs. Our findings show that these
ViT sinks encapsulate high-level semantic concepts from images, allowing the
LLM to perform more effective understanding and reasoning. Despite their
importance, these sink tokens are often overlooked in existing LVLM
architectures. To explore their contribution, we present both qualitative and
quantitative analyses of the information embedded in these sink tokens. We also
propose both training-free and training-based approaches to better leverage how
this information is interpreted by the LLM, and to what extent. By explicitly
utilizing these tokens, we demonstrate substantial improvements across a range
of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT
attention sinks in enhancing visual reasoning.</p>
<h3 id="25-large-scale-diffusion-distillation-via-score-regularized-continuous-time-consistency">[25] <a href="https://arxiv.org/abs/2510.08431">Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency</a></h3>
<p><em>Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang</em></p>
<h4 id="tldr_24">🧩 TL;DR</h4>
<p>本研究提出了分数正则化连续时间一致性模型（rCM），首次将连续时间一致性蒸馏扩展到大规模文本到图像和视频扩散模型，通过结合分数蒸馏作为长跳跃正则化器，在保持高生成多样性的同时显著提升视觉质量。</p>
<hr />
<h4 id="detailed-summary_24">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 尽管连续时间一致性模型在学术规模扩散加速方面具有理论优势和实证效果，但其在大规模文本到图像和视频任务中的适用性仍不明确，主要面临雅可比向量积计算的基础设施挑战以及标准评估基准的局限性，特别是在精细细节生成方面存在根本性的质量限制。</p>
<p><strong>Method:</strong> 开发了并行兼容的FlashAttention-2雅可比向量积计算内核，支持超过100亿参数模型和高维视频任务的训练；提出分数正则化连续时间一致性模型，将分数蒸馏作为长跳跃正则化器集成到sCM中，通过结合模式覆盖的前向散度和模式寻求的反向散度来提升视觉质量。</p>
<p><strong>Result:</strong> 在高达140亿参数的Cosmos-Predict2、Wan2.1等大规模模型和5秒视频任务上验证，rCM在质量指标上匹配或超越了最先进的蒸馏方法DMD2，同时在多样性方面具有显著优势，无需GAN调优或大量超参数搜索；蒸馏后的模型仅需1-4步即可生成高保真样本，将扩散采样加速15-50倍。</p>
<p><strong>Conclusion:</strong> rCM作为一个实用且理论基础的框架，为推进大规模扩散蒸馏提供了有效解决方案，通过互补的模式覆盖和模式寻求目标函数，在保持多样性的同时解决了精细细节生成的质量限制问题，为大模型的实际部署提供了高效推理方案。</p>
<hr />
<h4 id="abstract_24">📄 Abstract</h4>
<p>This work represents the first effort to scale up continuous-time consistency
distillation to general application-level image and video diffusion models.
Although continuous-time consistency model (sCM) is theoretically principled
and empirically powerful for accelerating academic-scale diffusion, its
applicability to large-scale text-to-image and video tasks remains unclear due
to infrastructure challenges in Jacobian-vector product (JVP) computation and
the limitations of standard evaluation benchmarks. We first develop a
parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on
models with over 10 billion parameters and high-dimensional video tasks. Our
investigation reveals fundamental quality limitations of sCM in fine-detail
generation, which we attribute to error accumulation and the "mode-covering"
nature of its forward-divergence objective. To remedy this, we propose the
score-regularized continuous-time consistency model (rCM), which incorporates
score distillation as a long-skip regularizer. This integration complements sCM
with the "mode-seeking" reverse divergence, effectively improving visual
quality while maintaining high generation diversity. Validated on large-scale
models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM
matches or surpasses the state-of-the-art distillation method DMD2 on quality
metrics while offering notable advantages in diversity, all without GAN tuning
or extensive hyperparameter searches. The distilled models generate
high-fidelity samples in only $1\sim4$ steps, accelerating diffusion sampling
by $15\times\sim50\times$. These results position rCM as a practical and
theoretically grounded framework for advancing large-scale diffusion
distillation.</p>
<h3 id="26-spatialladder-progressive-training-for-spatial-reasoning-in-vision-language-models">[26] <a href="https://arxiv.org/abs/2510.08531">SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models</a></h3>
<p><em>Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang</em></p>
<h4 id="tldr_25">🧩 TL;DR</h4>
<p>本文提出了一种渐进式训练框架SpatialLadder，通过建立空间感知、空间理解和复杂推理的三阶段训练方法，显著提升了视觉语言模型的空间推理能力，在多个基准测试中超越了GPT-4o和Gemini-2.0-Flash等先进模型。</p>
<hr />
<h4 id="detailed-summary_25">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前视觉语言模型在空间推理方面存在根本性挑战，现有方法试图直接学习空间推理而缺乏建立感知与理解的层次化基础，导致性能受限。</p>
<p><strong>Method:</strong> 构建了包含26,610个样本的多模态数据集SpatialLadder-26k，涵盖物体定位、单图像、多视角和视频空间推理任务，并设计了渐进式三阶段训练框架：通过物体定位建立空间感知，通过多维空间任务发展空间理解，通过可验证奖励的强化学习加强复杂推理。</p>
<p><strong>Result:</strong> SpatialLadder模型在空间推理基准测试中达到最先进性能，相比基础模型平均提升23.4%，超越GPT-4o 20.8%和Gemini-2.0-Flash 10.1%，在领域外基准测试中保持强泛化能力，提升7.2%。</p>
<p><strong>Conclusion:</strong> 研究表明从感知到推理的渐进式训练对于构建鲁棒的空间智能至关重要，该方法为视觉语言模型的空间推理能力提供了系统性的解决方案，具有重要的实际应用价值。</p>
<hr />
<h4 id="abstract_25">📄 Abstract</h4>
<p>Spatial reasoning remains a fundamental challenge for Vision-Language Models
(VLMs), with current approaches struggling to achieve robust performance
despite recent advances. We identify that this limitation stems from a critical
gap: existing methods attempt to learn spatial reasoning directly without
establishing the hierarchical foundations of perception and understanding. To
address this challenge, we present a comprehensive methodology for building
spatial intelligence progressively. We introduce SpatialLadder-26k, a
multimodal dataset containing 26,610 samples spanning object localization,
single image, multi-view, and video spatial reasoning tasks, constructed
through a standardized pipeline that ensures systematic coverage across
modalities. Building on this dataset, we design a three-stage progressive
training framework that (1) establishes spatial perception through object
localization, (2) develops spatial understanding through multi-dimensional
spatial tasks, and (3) strengthens complex reasoning via reinforcement learning
with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter
model that achieves state-of-the-art performance on spatial reasoning
benchmarks, with 23.4% average improvement over the base model, surpassing
GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains
strong generalization with 7.2% improvement on out-of-domain benchmarks,
demonstrating that progressive training from perception to reasoning is
essential for robust spatial intelligence.</p>
<h3 id="27-gaze-on-the-prize-shaping-visual-attention-with-return-guided-contrastive-learning">[27] <a href="https://arxiv.org/abs/2510.08442">Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning</a></h3>
<p><em>Andrew Lee, Ian Chuang, Dechen Gao, Kai Fukazawa, Iman Soltani</em></p>
<h4 id="tldr_26">🧩 TL;DR</h4>
<p>本文提出了Gaze on the Prize框架，通过可学习的中央凹注意力机制增强视觉强化学习，利用回报差异引导对比学习来识别任务相关特征，在ManiSkill3基准测试中实现了最高2.4倍的样本效率提升。</p>
<hr />
<h4 id="detailed-summary_26">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 视觉强化学习代理必须基于高维图像数据进行学习，但只有少量像素与任务相关，这导致代理在无关特征上浪费探索和计算资源，造成样本效率低下和学习不稳定。</p>
<p><strong>Method:</strong> 该框架引入可学习的中央凹注意力机制，通过基于回报差异的自监督信号进行引导，采用回报引导的对比学习方法，将相似视觉表示根据回报差异分组为正负样本，构建对比三元组来训练注意力机制产生可区分的状态表示。</p>
<p><strong>Result:</strong> 在ManiSkill3基准测试的一系列操作任务中，该方法实现了最高2.4倍的样本效率提升，并且能够解决基线方法无法学习的任务，所有这些改进都无需修改底层算法或超参数。</p>
<p><strong>Conclusion:</strong> 研究表明回报差异能够有效揭示任务相关特征，基于回报引导的对比学习可以显著提升视觉强化学习的样本效率和稳定性，为处理高维视觉输入中的注意力分配问题提供了新的解决方案。</p>
<hr />
<h4 id="abstract_26">📄 Abstract</h4>
<p>Visual Reinforcement Learning (RL) agents must learn to act based on
high-dimensional image data where only a small fraction of the pixels is
task-relevant. This forces agents to waste exploration and computational
resources on irrelevant features, leading to sample-inefficient and unstable
learning. To address this, inspired by human visual foveation, we introduce
Gaze on the Prize. This framework augments visual RL with a learnable foveal
attention mechanism (Gaze), guided by a self-supervised signal derived from the
agent's experience pursuing higher returns (the Prize). Our key insight is that
return differences reveal what matters most: If two similar representations
produce different outcomes, their distinguishing features are likely
task-relevant, and the gaze should focus on them accordingly. This is realized
through return-guided contrastive learning that trains the attention to
distinguish between the features relevant to success and failure. We group
similar visual representations into positives and negatives based on their
return differences and use the resulting labels to construct contrastive
triplets. These triplets provide the training signal that teaches the attention
mechanism to produce distinguishable representations for states associated with
different outcomes. Our method achieves up to 2.4x improvement in sample
efficiency and can solve tasks that the baseline fails to learn, demonstrated
across a suite of manipulation tasks from the ManiSkill3 benchmark, all without
modifying the underlying algorithm or hyperparameters.</p>
<h3 id="28-videonorms-benchmarking-cultural-awareness-of-video-language-models">[28] <a href="https://arxiv.org/abs/2510.08543">VideoNorms: Benchmarking Cultural Awareness of Video Language Models</a></h3>
<p><em>Nikhil Reddy Varimalla, Yunfei Xu, Arkadiy Saakyan, Meng Fan Wang, Smaranda Muresan</em></p>
<h4 id="tldr_27">🧩 TL;DR</h4>
<p>本文提出了VideoNorms基准数据集，包含1000多个视频片段与社会文化规范配对，用于评估视频大语言模型的文化意识能力。研究发现现有模型在文化规范理解方面存在显著差距，特别是在中国文化理解、非语言证据识别和规范违反检测方面表现较差。</p>
<hr />
<h4 id="detailed-summary_27">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 随着视频大语言模型在全球部署，它们需要理解和融入相关文化背景，但目前缺乏适当的基准来评估这些模型的文化意识能力。现有评估方法未能充分捕捉模型对文化规范的理解和接地能力，特别是在跨文化场景下的表现。</p>
<p><strong>Method:</strong> 采用人机协作框架构建VideoNorms数据集，其中使用基于言语行为理论的理论驱动提示的教师模型提供候选标注，然后由训练有素的人类专家验证和修正标注。数据集包含来自美国和中国文化的视频片段与社会文化规范配对，标注了规范遵守与违反标签以及语言和非语言证据。</p>
<p><strong>Result:</strong> 基准测试显示多个开放权重的视频大语言模型存在共同趋势：在规范违反检测上表现较差；对中国文化的理解不如美国文化；提供非语言证据比语言证据更困难；难以准确识别与言语行为对应的具体规范；在正式、非幽默语境中表现不如人类。</p>
<p><strong>Conclusion:</strong> 研究强调了文化接地视频语言模型训练的必要性，现有模型在跨文化理解和规范识别方面存在显著不足。VideoNorms基准和框架为填补这一空白提供了起点，为开发更具文化意识的视频理解模型奠定了基础，并揭示了当前模型在真实世界文化场景应用中的局限性。</p>
<hr />
<h4 id="abstract_27">📄 Abstract</h4>
<p>As Video Large Language Models (VideoLLMs) are deployed globally, they
require understanding of and grounding in the relevant cultural background. To
properly assess these models' cultural awareness, adequate benchmarks are
needed. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm)
pairs from US and Chinese cultures annotated with socio-cultural norms grounded
in speech act theory, norm adherence and violations labels, and verbal and
non-verbal evidence. To build VideoNorms, we use a human-AI collaboration
framework, where a teacher model using theoretically-grounded prompting
provides candidate annotations and a set of trained human experts validate and
correct the annotations. We benchmark a variety of open-weight VideoLLMs on the
new dataset which highlight several common trends: 1) models performs worse on
norm violation than adherence; 2) models perform worse w.r.t Chinese culture
compared to the US culture; 3) models have more difficulty in providing
non-verbal evidence compared to verbal for the norm adhere/violation label and
struggle to identify the exact norm corresponding to a speech-act; and 4)
unlike humans, models perform worse in formal, non-humorous contexts. Our
findings emphasize the need for culturally-grounded video language model
training - a gap our benchmark and framework begin to address.</p>
<h3 id="29-video-star-reinforcing-open-vocabulary-action-recognition-with-tools">[29] <a href="https://arxiv.org/abs/2510.08480">Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools</a></h3>
<p><em>Zhenlong Yuan, Xiangyan Qu, Chengxuan Qian, Rui Chen, Jing Tang, Lei Sun, Xiangxiang Chu, Dapeng Zhang, Yiwei Wang, Yujun Cai, Shuo Li</em></p>
<h4 id="tldr_28">🧩 TL;DR</h4>
<p>本文提出Video-STAR框架，通过上下文子动作分解与工具增强强化学习的协同机制，解决多模态大语言模型在开放词汇动作识别中的语义混淆和跨模态幻觉问题，实现了细粒度动作匹配和视觉推理能力的提升。</p>
<hr />
<h4 id="detailed-summary_28">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 多模态大语言模型在视觉-文本推理方面展现出显著潜力，但其对文本中心先验的依赖限制了在开放词汇场景下区分语义相似动作的能力，特别是在处理细粒度动作识别和减少跨模态幻觉方面存在明显局限。</p>
<p><strong>Method:</strong> 提出Video-STAR框架，创新性地将动作分解为具有区分性的子动作以实现细粒度匹配，同时动态调用领域特定工具进行跨模态交织，通过设计平衡工具使用效率、子动作相关性和结构连贯性的分层奖励机制，实现无监督地优先考虑子动作模式。</p>
<p><strong>Result:</strong> 在HMDB-51、UCF-101、SSv2、Kinetics-400和Kinetics-600数据集上的广泛评估表明，该方法在区分细粒度动作和处理跨模态幻觉方面优于现有方法，实现了最先进的性能，验证了其优秀的鲁棒性和泛化能力。</p>
<p><strong>Conclusion:</strong> 该研究通过子动作分解和工具增强强化学习的协同机制，成功实现了从文本中心推理向视觉基础推理的转变，为开放词汇动作识别提供了新的技术路径，在减少跨模态幻觉和提升类别特定推理能力方面具有重要价值。</p>
<hr />
<h4 id="abstract_28">📄 Abstract</h4>
<p>Multimodal large language models (MLLMs) have demonstrated remarkable
potential in bridging visual and textual reasoning, yet their reliance on
text-centric priors often limits their ability to disentangle semantically
similar actions in open-vocabulary scenarios. To address this, we propose
Video-STAR, a framework that harmonizes contextual sub-motion decomposition
with tool-augmented reinforcement learning for open-vocabulary action
recognition (OVAR). Unlike prior methods that treat actions as monolithic
entities, our approach innovatively decomposes actions into discriminative
sub-motions for fine-grained matching while dynamically invoking
domain-specific tools for cross-modal interleaving, thereby enabling
category-specific reasoning capacity and reducing cross-modal hallucination.
Moreover, by designing a hierarchical reward that balances tool-usage
efficiency, sub-motion relevance, and structural coherence in reasoning, our
method autonomously leverages external tools to prioritize sub-motion patterns
without explicit supervision, transmitting from text-centric reasoning to
visually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2,
Kinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art
performance, outperforming existing methods in distinguishing fine-grained
actions and handling cross-modal hallucination, validating our excellent
robustness and generalization.</p>
<h3 id="30-matrix-multimodal-agent-tuning-for-robust-tool-use-reasoning">[30] <a href="https://arxiv.org/abs/2510.08567">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a></h3>
<p><em>Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan</em></p>
<h4 id="tldr_29">🧩 TL;DR</h4>
<p>本文提出了一个视觉中心的多模态智能体调优框架，通过自动合成多模态轨迹和生成逐步偏好对，训练视觉语言模型控制器以实现鲁棒的工具使用推理。该方法在多个基准测试中超越了现有开源和闭源视觉语言模型。</p>
<hr />
<h4 id="detailed-summary_29">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前视觉语言模型作为控制器访问外部工具进行复杂推理和决策时，面临高质量多模态轨迹稀缺和手动标注成本高昂的限制，这阻碍了模型在工具使用推理方面的有效性。</p>
<p><strong>Method:</strong> 提出了一个视觉中心的多模态智能体调优框架，包括构建包含28.5K多模态任务和177K已验证轨迹的M-TRACE数据集，开发基于模仿学习的MATRIX Agent控制器，以及通过Pref-X生成的11K自动偏好对进行逐步偏好学习优化。</p>
<p><strong>Result:</strong> 在Agent-X、GTA和GAIA三个基准测试中，MATRIX控制器持续超越了开源和闭源的视觉语言模型，证明了该方法在多模态工具使用方面的可扩展性和有效性。</p>
<p><strong>Conclusion:</strong> 该研究展示了通过自动合成多模态轨迹和偏好学习，可以有效解决高质量训练数据稀缺的问题，为构建更强大的多模态工具使用智能体提供了可扩展的解决方案，推动了视觉语言模型在复杂推理任务中的应用。</p>
<hr />
<h4 id="abstract_29">📄 Abstract</h4>
<p>Vision language models (VLMs) are increasingly deployed as controllers with
access to external tools for complex reasoning and decision-making, yet their
effectiveness remains limited by the scarcity of high-quality multimodal
trajectories and the cost of manual annotation. We address this challenge with
a vision-centric agent tuning framework that automatically synthesizes
multimodal trajectories, generates step-wise preference pairs, and trains a VLM
controller for robust tool-use reasoning. Our pipeline first constructs
M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified
trajectories, enabling imitation-based trajectory tuning. Building on this, we
develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool
reasoning. To achieve finer alignment, we further introduce Pref-X, a set of
11K automatically generated preference pairs, and optimize MATRIX on it via
step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,
MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating
scalable and effective multimodal tool use. Our data and code is avaliable at
https://github.com/mbzuai-oryx/MATRIX.</p>
<h3 id="31-instructx-towards-unified-visual-editing-with-mllm-guidance">[31] <a href="https://arxiv.org/abs/2510.08485">InstructX: Towards Unified Visual Editing with MLLM Guidance</a></h3>
<p><em>Chong Mou, Qichao Sun, Yanze Wu, Pengze Zhang, Xinghui Li, Fulong Ye, Songtao Zhao, Qian He</em></p>
<h4 id="tldr_30">🧩 TL;DR</h4>
<p>本文提出了InstructX框架，通过系统研究多模态大语言模型与扩散模型的集成策略，实现了图像和视频编辑任务的统一建模，在无需显式监督的情况下涌现出视频编辑能力，并在广泛任务中达到最先进性能。</p>
<hr />
<h4 id="detailed-summary_30">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前多模态大语言模型在视觉理解和推理方面展现出强大能力，但将其与扩散模型集成用于编辑任务的研究缺乏深入的设计选择分析，特别是在视频编辑等困难任务中，多模态大语言模型与扩散模型的集成仍是一个开放挑战。</p>
<p><strong>Method:</strong> 提出了InstructX统一框架，通过全面研究多模态大语言模型与扩散模型在指令驱动编辑任务中的集成策略，分析图像和视频在统一建模中的协同与差异关系，并引入模态特定的多模态大语言模型特征来统一处理图像和视频编辑任务。</p>
<p><strong>Result:</strong> 实验表明，仅使用图像数据进行训练即可在没有显式监督的情况下涌现出视频编辑能力，有效缓解了视频训练数据稀缺的限制，同时该方法能够处理广泛的图像和视频编辑任务，并实现了最先进的性能表现。</p>
<p><strong>Conclusion:</strong> 该研究揭示了在图像数据上训练可以自然获得视频编辑能力的涌现现象，为统一建模图像和视频任务提供了有效途径，通过模态特定的特征集成策略成功实现了单一模型对多种编辑任务的处理，为多模态编辑系统的发展提供了重要见解。</p>
<hr />
<h4 id="abstract_30">📄 Abstract</h4>
<p>With recent advances in Multimodal Large Language Models (MLLMs) showing
strong visual understanding and reasoning, interest is growing in using them to
improve the editing performance of diffusion models. Despite rapid progress,
most studies lack an in-depth analysis of MLLM design choices. Moreover, the
integration of MLLMs and diffusion models remains an open challenge in some
difficult tasks, such as video editing. In this paper, we present InstructX, a
unified framework for image and video editing. Specifically, we conduct a
comprehensive study on integrating MLLMs and diffusion models for
instruction-driven editing across diverse tasks. Building on this study, we
analyze the cooperation and distinction between images and videos in unified
modeling. (1) We show that training on image data can lead to emergent video
editing capabilities without explicit supervision, thereby alleviating the
constraints imposed by scarce video training data. (2) By incorporating
modality-specific MLLM features, our approach effectively unifies image and
video editing tasks within a single model. Extensive experiments demonstrate
that our method can handle a broad range of image and video editing tasks and
achieves state-of-the-art performance.</p>
<h3 id="32-moa-vr-a-mixture-of-agents-system-towards-all-in-one-video-restoration">[32] <a href="https://arxiv.org/abs/2510.08508">MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration</a></h3>
<p><em>Lu Liu, Chunlei Cai, Shaocheng Shen, Jianfeng Liang, Weimin Ouyang, Tianxiao Ye, Jian Mao, Huiyu Duan, Jiangchao Yao, Xiaoyun Zhang, Qiang Hu, Guangtao Zhai</em></p>
<h4 id="tldr_31">🧩 TL;DR</h4>
<p>本文提出了MoA-VR，首个基于智能体混合架构的视频修复系统，通过三个协同工作的智能体模拟人类专家的推理过程，有效处理复杂多样的视频退化问题，在客观指标和感知质量上均优于现有基线方法。</p>
<hr />
<h4 id="detailed-summary_31">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现实世界视频常因采集和传输条件不同而遭受噪声、压缩伪影和低光失真等复杂退化，现有修复方法通常需要人工选择专用模型或采用单一架构难以泛化处理各种退化类型，存在通用性和适应性不足的问题。</p>
<p><strong>Method:</strong> 提出MoA-VR系统，包含三个协调智能体：基于视觉语言模型的退化识别器、大型语言模型驱动的自适应路由器和专门为修复任务设计的视频质量评估模型，通过构建大规模视频退化识别基准和恢复视频质量数据集来支持系统训练。</p>
<p><strong>Result:</strong> 大量实验表明MoA-VR能有效处理多样化和复合退化，在客观指标和感知质量方面持续优于现有基线方法，验证了系统在处理复杂视频退化问题上的优越性能。</p>
<p><strong>Conclusion:</strong> 该研究展示了多模态智能和模块化推理在通用视频修复系统中的潜力，为构建更智能、自适应的视频处理系统提供了新的范式，推动了基于智能体协作的视频修复技术发展。</p>
<hr />
<h4 id="abstract_31">📄 Abstract</h4>
<p>Real-world videos often suffer from complex degradations, such as noise,
compression artifacts, and low-light distortions, due to diverse acquisition
and transmission conditions. Existing restoration methods typically require
professional manual selection of specialized models or rely on monolithic
architectures that fail to generalize across varying degradations. Inspired by
expert experience, we propose MoA-VR, the first
\underline{M}ixture-\underline{o}f-\underline{A}gents \underline{V}ideo
\underline{R}estoration system that mimics the reasoning and processing
procedures of human professionals through three coordinated agents: Degradation
Identification, Routing and Restoration, and Restoration Quality Assessment.
Specifically, we construct a large-scale and high-resolution video degradation
recognition benchmark and build a vision-language model (VLM) driven
degradation identifier. We further introduce a self-adaptive router powered by
large language models (LLMs), which autonomously learns effective restoration
strategies by observing tool usage patterns. To assess intermediate and final
processed video quality, we construct the \underline{Res}tored
\underline{V}ideo \underline{Q}uality (Res-VQ) dataset and design a dedicated
VLM-based video quality assessment (VQA) model tailored for restoration tasks.
Extensive experiments demonstrate that MoA-VR effectively handles diverse and
compound degradations, consistently outperforming existing baselines in terms
of both objective metrics and perceptual quality. These results highlight the
potential of integrating multimodal intelligence and modular reasoning in
general-purpose video restoration systems.</p>
<h3 id="33-scivideobench-benchmarking-scientific-video-reasoning-in-large-multimodal-models">[33] <a href="https://arxiv.org/abs/2510.08559">SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models</a></h3>
<p><em>Andong Deng, Taojiannan Yang, Shoubin Yu, Lincoln Spencer, Mohit Bansal, Chen Chen, Serena Yeung-Levy, Xiaohan Wang</em></p>
<h4 id="tldr_32">🧩 TL;DR</h4>
<p>本研究提出了SciVideoBench，一个专门用于评估科学视频推理能力的严格基准，包含1000个精心设计的多选题，覆盖25个专业学科，旨在推动多模态模型在复杂科学推理方面的发展。</p>
<hr />
<h4 id="detailed-summary_32">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前视频基准主要针对通用场景，依赖感知识别而推理任务相对简单，导致性能饱和，无法有效评估先进的多模态认知技能，特别是在科学领域的复杂视频推理仍是一个重要且具有挑战性的前沿问题。</p>
<p><strong>Method:</strong> 通过半自动系统验证，从尖端科学实验视频中精心构建了1000个多选题，涵盖超过25个专业学术领域，每个问题都需要复杂的领域专业知识、精确的时空感知和精细的逻辑推理能力。</p>
<p><strong>Result:</strong> 评估显示包括Gemini 2.5 Pro和Qwen2.5-VL在内的最先进专有和开源大语言模型存在显著的性能缺陷，表明在视频推理能力方面仍有巨大的提升空间。</p>
<p><strong>Conclusion:</strong> 详细分析推理复杂性和视觉基础等关键因素为未来大语言模型的发展提供了有价值的见解和明确方向，推动真正有能力的多模态AI共同科学家的演进，该基准有望满足社区兴趣并帮助推动前沿AI在更广泛科学领域的边界扩展。</p>
<hr />
<h4 id="abstract_32">📄 Abstract</h4>
<p>Large Multimodal Models (LMMs) have achieved remarkable progress across
various capabilities; however, complex video reasoning in the scientific domain
remains a significant and challenging frontier. Current video benchmarks
predominantly target general scenarios where perception/recognition is heavily
relied on, while with relatively simple reasoning tasks, leading to saturation
and thus failing to effectively evaluate advanced multimodal cognitive skills.
To address this critical gap, we introduce SciVideoBench, a rigorous benchmark
specifically designed to assess advanced video reasoning in scientific
contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice
questions derived from cutting-edge scientific experimental videos spanning
over 25 specialized academic subjects and verified by a semi-automatic system.
Each question demands sophisticated domain-specific knowledge, precise
spatiotemporal perception, and intricate logical reasoning, effectively
challenging models' higher-order cognitive abilities. Our evaluation highlights
significant performance deficits in state-of-the-art proprietary and
open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating
substantial room for advancement in video reasoning capabilities. Detailed
analyses of critical factors such as reasoning complexity and visual grounding
provide valuable insights and clear direction for future developments in LMMs,
driving the evolution of truly capable multimodal AI co-scientists. We hope
SciVideoBench could fit the interests of the community and help to push the
boundary of cutting-edge AI for border science.</p>
<h3 id="34-mm-helix-boosting-multimodal-long-chain-reflective-reasoning-with-holistic-platform-and-adaptive-hybrid-policy-optimization">[34] <a href="https://arxiv.org/abs/2510.08540">MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization</a></h3>
<p><em>Xiangyu Zhao, Junming Lin, Tianhao Liang, Yifan Zhou, Wenhao Chai, Yuzhe Gu, Weiyun Wang, Kai Chen, Gen Luo, Wenwei Zhang, Junchi Yan, Hua Yang, Haodong Duan, Xue Yang</em></p>
<h4 id="tldr_33">🧩 TL;DR</h4>
<p>本研究提出了MM-HELIX基准和自适应混合策略优化方法，显著提升了多模态大语言模型的长链反思推理能力，在基准测试中实现了18.6%的准确率提升，并展现出良好的泛化性能。</p>
<hr />
<h4 id="detailed-summary_33">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前多模态大语言模型在数学和逻辑推理任务中表现出色，但其长链反思推理能力——解决复杂现实问题的关键前提——仍然未被充分探索，本研究旨在填补这一研究空白。</p>
<p><strong>Method:</strong> 研究首先构建了包含1,260个样本的MM-HELIX多模态基准，随后开发了步骤引导响应生成流水线创建MM-HELIX-100K数据集，并提出了自适应混合策略优化方法，该方法将离线监督和在线优化动态统一到单一训练阶段中。</p>
<p><strong>Result:</strong> 在MM-HELIX基准测试中，所提方法在Qwen2.5-VL-7B基线上实现了18.6%的准确率提升，同时在通用数学和逻辑任务上获得了5.7%的平均性能增益，展现出强大的泛化能力。</p>
<p><strong>Conclusion:</strong> 研究表明多模态大语言模型的反思推理能力可以通过有效学习获得并实现泛化，这为开发更强大的多模态大语言模型开辟了新途径，证明了复杂推理任务中动态训练策略的重要性。</p>
<hr />
<h4 id="abstract_33">📄 Abstract</h4>
<p>While current Multimodal Large Language Models (MLLMs) have demonstrated
proficiency in reasoning tasks such as mathematics and logic, their capacity
for long-chain reflective reasoning, a prerequisite for solving complex
real-world problems, remains largely underexplored. In this work, we first
conduct an extensive empirical investigation to evaluate this capability.
Leveraging a carefully designed data synthesis engine, we construct MM-HELIX, a
multimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks
that require iterative thinking and backtracking. Empirical results on this
benchmark reveal that existing MLLMs exhibit significant performance deficits
in long-chain reflective reasoning. To address this limitation, we generate
post-training data and further explore learning paradigms for exploiting such
data. We first develop the Step-Elicited Response Generation pipeline to create
MM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning
traces for instruction-tuning stage. Given that standard Reinforcement Learning
fails on complex tasks due to sparse reward signals and catastrophic forgetting
after Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization
(AHPO), a novel training strategy that dynamically unifies offline supervision
and online optimization into a single stage. This strategy enables the model to
learn from expert data when rewards are sparse and conduct independent
exploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our
method achieves a +18.6\% accuracy improvement on MM-HELIX benchmark and
demonstrates strong generalization with a +5.7\% average performance gain on
general mathematic and logic tasks. Our work demonstrate that reflective
reasoning in MLLMs can be effectively learned and generalized, paving the way
for developing more capable MLLMs.</p>
<h3 id="35-multicoin-multi-modal-controllable-video-inbetweening">[35] <a href="https://arxiv.org/abs/2510.08561">MultiCOIN: Multi-Modal COntrollable Video INbetweening</a></h3>
<p><em>Maham Tanveer, Yang Zhou, Simon Niklaus, Ali Mahdavi Amiri, Hao Zhang, Krishna Kumar Singh, Nanxuan Zhao</em></p>
<h4 id="tldr_34">🧩 TL;DR</h4>
<p>本文提出了一个支持多模态控制的视频插帧框架，通过将运动控制映射为统一的点基表示，并采用双分支生成器分别处理内容和运动控制，实现了灵活、精确的视频过渡生成。</p>
<hr />
<h4 id="detailed-summary_34">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有视频插帧方法无法生成大规模、复杂或精细的运动，难以适应多样化的用户意图，并且缺乏对中间帧细节的精细控制，导致与创意构思不一致。</p>
<p><strong>Method:</strong> 采用扩散变换器作为视频生成模型，将所有运动控制映射为统一的稀疏点基表示作为视频/噪声输入，将内容控制和运动控制分离为两个分支进行特征编码，并采用分阶段训练策略确保多模态控制的平滑学习。</p>
<p><strong>Result:</strong> 广泛的定性和定量实验表明，多模态控制能够实现更动态、可定制且上下文准确的视觉叙事效果。</p>
<p><strong>Conclusion:</strong> 多模态控制为视频插帧提供了更高的灵活性和精确性，通过统一的点基表示和双分支架构实现了对复杂运动的有效建模，为视频编辑和长视频合成开辟了新方向。</p>
<hr />
<h4 id="abstract_34">📄 Abstract</h4>
<p>Video inbetweening creates smooth and natural transitions between two image
frames, making it an indispensable tool for video editing and long-form video
synthesis. Existing works in this domain are unable to generate large, complex,
or intricate motions. In particular, they cannot accommodate the versatility of
user intents and generally lack fine control over the details of intermediate
frames, leading to misalignment with the creative mind. To fill these gaps, we
introduce \modelname{}, a video inbetweening framework that allows multi-modal
controls, including depth transition and layering, motion trajectories, text
prompts, and target regions for movement localization, while achieving a
balance between flexibility, ease of use, and precision for fine-grained video
interpolation. To achieve this, we adopt the Diffusion Transformer (DiT)
architecture as our video generative model, due to its proven capability to
generate high-quality long videos. To ensure compatibility between DiT and our
multi-modal controls, we map all motion controls into a common sparse and
user-friendly point-based representation as the video/noise input. Further, to
respect the variety of controls which operate at varying levels of granularity
and influence, we separate content controls and motion controls into two
branches to encode the required features before guiding the denoising process,
resulting in two generators, one for motion and the other for content. Finally,
we propose a stage-wise training strategy to ensure that our model learns the
multi-modal controls smoothly. Extensive qualitative and quantitative
experiments demonstrate that multi-modal controls enable a more dynamic,
customizable, and contextually accurate visual narrative.</p>
<h3 id="36-navil-rethinking-scaling-properties-of-native-multimodal-large-language-models-under-data-constraints">[36] <a href="https://arxiv.org/abs/2510.08565">NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints</a></h3>
<p><em>Changyao Tian, Hao Li, Gen Luo, Xizhou Zhu, Weijie Su, Hanming Deng, Jinguo Zhu, Jie Shao, Ziran Zhu, Yunpeng Liu, Lewei Lu, Wenhai Wang, Hongsheng Li, Jifeng Dai</em></p>
<h4 id="tldr_35">🧩 TL;DR</h4>
<p>本文提出了一种名为NaViL的原生多模态大语言模型，通过端到端训练系统研究了MLLM的设计空间和扩展特性，在14个多模态基准测试中展现出竞争力，并为未来原生MLLM研究提供了深入见解。</p>
<hr />
<h4 id="detailed-summary_35">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有多模态大语言模型普遍采用组合式训练范式，其中预训练视觉编码器通过连续多模态预训练与预训练LLM连接，但这种分离训练方式使得多模态扩展特性难以探索，本研究旨在解决这一问题。</p>
<p><strong>Method:</strong> 研究聚焦于原生端到端训练MLLM，在数据约束的实际设置下系统探索其设计空间和扩展特性，通过仔细研究MLLM中的各种选择，获得了在性能和训练成本之间最佳平衡的元架构。</p>
<p><strong>Result:</strong> 实验结果表明，在14个多模态基准测试中，NaViL模型相对于现有MLLM展现出竞争力，同时发现了视觉编码器与LLM之间正相关的扩展关系。</p>
<p><strong>Conclusion:</strong> 研究为未来原生MLLM研究提供了深入见解，提出的简单且成本效益高的训练方案以及发现的扩展规律对后续多模态模型发展具有重要指导意义。</p>
<hr />
<h4 id="abstract_35">📄 Abstract</h4>
<p>Compositional training has been the de-facto paradigm in existing Multimodal
Large Language Models (MLLMs), where pre-trained vision encoders are connected
with pre-trained LLMs through continuous multimodal pre-training. However, the
multimodal scaling property of this paradigm remains difficult to explore due
to the separated training. In this paper, we focus on the native training of
MLLMs in an end-to-end manner and systematically study its design space and
scaling property under a practical setting, i.e., data constraint. Through
careful study of various choices in MLLM, we obtain the optimal
meta-architecture that best balances performance and training cost. After that,
we further explore the scaling properties of the native MLLM and indicate the
positively correlated scaling relationship between visual encoders and LLMs.
Based on these findings, we propose a native MLLM called NaViL, combined with a
simple and cost-effective recipe. Experimental results on 14 multimodal
benchmarks confirm the competitive performance of NaViL against existing MLLMs.
Besides that, our findings and results provide in-depth insights for the future
study of native MLLMs.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="37-deploying-tiny-lvlm-judges-for-real-world-evaluation-of-chart-models-lessons-learned-and-best-practices">[37] <a href="https://arxiv.org/abs/2510.07545">Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices</a></h3>
<p><em>Md Tahmid Rahman Laskar, Mohammed Saidul Islam, Ridwan Mahbub, Mizanur Rahman, Amran Bhuiyan, Israt Jahan, Mir Tafseer Nayeem, Shafiq Joty, Enamul Hoque, Jimmy Huang</em></p>
<h4 id="tldr_36">🧩 TL;DR</h4>
<p>本研究提出了两种成本高效的评估方法：多标准提示和领域自适应迁移学习，成功将2B参数的小型视觉语言模型训练为专门的图表理解评估器ChartJudge，在资源受限环境中实现了可扩展的低成本评估。</p>
<hr />
<h4 id="detailed-summary_36">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 尽管7B参数的大型视觉语言模型在图表理解任务中展现出作为自动化评估器的潜力，但2B参数及以下的微型模型表现仍然较差，这限制了它们在资源受限环境中的实际应用，因此需要开发成本高效的评估解决方案。</p>
<p><strong>Method:</strong> 提出了两种核心方法：多标准提示将多个评估标准整合到单个查询中，以及领域自适应迁移学习，通过在图表数据集上使用合成判断对2B参数的LVLM进行微调，创建专门的ChartJudge模型。</p>
<p><strong>Result:</strong> 实验表明多标准提示暴露了鲁棒性差距，导致包括LLaVA-Critic在内的7B模型性能大幅下降；同时，ChartJudge能够有效地将知识从一个数据集迁移到另一个数据集，成为更专业化的模型。</p>
<p><strong>Conclusion:</strong> 通过对图表类型和查询复杂度的细粒度分析，本研究提供了关于模型大小、提示设计和可迁移性之间权衡的可操作见解，为图表推理任务实现了可扩展的低成本评估方案。</p>
<hr />
<h4 id="abstract_36">📄 Abstract</h4>
<p>Large Vision-Language Models (LVLMs) with only 7B parameters have shown
promise as automated judges in chart comprehension tasks. However, tiny models
(&lt;=2B parameters) still perform poorly as judges, limiting their real-world use
in resource-constrained settings. To address this, we propose two approaches to
ensure cost-efficient evaluation: (i) multi-criteria prompting, which combines
separate evaluation criteria into a single query, and (ii) domain-adaptive
transfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic
judgments in a chart dataset to create the ChartJudge. Experiments show that
multi-criteria prompting exposes robustness gaps, which led to a huge drop in
performance for 7B models, including specialized LVLM judges like LLaVA-Critic.
In addition, we find that our tiny LVLM (ChartJudge) can effectively transfer
knowledge from one dataset to another to make it a more specialized model. Our
fine-grained analysis across chart types and query complexities offers
actionable insights into trade-offs between model size, prompt design, and
transferability, enabling scalable, low-cost evaluation for chart reasoning
tasks. Our code and the data will be made publicly available.</p>
<h3 id="38-toolexpander-extending-the-frontiers-of-tool-using-reinforcement-learning-to-weak-llms">[38] <a href="https://arxiv.org/abs/2510.07737">ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs</a></h3>
<p><em>Fu Chen, Peng Wang, Xiyin Li, Wen Li, Shichi Lei, Dongdong Xiang</em></p>
<h4 id="tldr_37">🧩 TL;DR</h4>
<p>本文提出了ToolExpander框架，通过动态多轮硬采样和自示例思维两项创新技术，解决了资源受限LLM在GRPO训练中响应不准确和训练崩溃的问题，显著提升了小规模模型使用工具的能力。</p>
<hr />
<h4 id="detailed-summary_37">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 训练大型语言模型使用GRPO方法时面临显著挑战：模型经常无法产生准确响应，特别是在小规模架构中，这不仅降低了性能改进并削弱了GRPO的潜力，还经常导致训练中期崩溃，严重影响稳定性和最终效果。</p>
<p><strong>Method:</strong> ToolExpander框架包含两项关键技术：动态多轮硬采样在训练期间动态替换困难样本（10次rollout无正确输出的样本）为高质量少样本演示，并采用指数学习率衰减策略缓解震荡；自示例思维是增强的GRPO框架，消除了KL散度并整合调整后的裁剪系数，通过最小额外奖励（0.01）鼓励模型自主生成和分析少样本示例。</p>
<p><strong>Result:</strong> 实验结果表明，ToolExpander显著增强了LLM使用工具的能力，特别是在较弱的小规模模型中，同时改善了训练稳定性和整体性能。</p>
<p><strong>Conclusion:</strong> 该研究证明了通过动态样本替换和自主示例生成机制，可以有效解决资源受限LLM在强化学习训练中的稳定性问题，为小规模模型的工具使用能力提升提供了有效解决方案，具有重要的实际应用价值。</p>
<hr />
<h4 id="abstract_37">📄 Abstract</h4>
<p>Training Large Language Models (LLMs) with Group Relative Policy Optimization
(GRPO) encounters a significant challenge: models often fail to produce
accurate responses, particularly in small-scale architectures. This limitation
not only diminishes performance improvements and undermines the potential of
GRPO but also frequently leads to mid-training collapse, adversely affecting
stability and final efficacy. To address these issues, we propose ToolExpander,
a novel framework that advances tool-oriented reinforcement learning for
resource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round
Hard Sampling, which dynamically substitutes challenging samples(those without
correct outputs over 10 rollouts) with high-quality few-shot demonstrations
during training, coupled with an exponential learning rate decay strategy to
mitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO
framework that eliminates KL divergence and incorporates adjusted clipping
coefficients, encouraging models to autonomously generate and analyze few-shot
examples via a minimal additional reward (0.01).Experimental results
demonstrate that ToolExpander significantly enhances tool-using capabilities in
LLMs, especially in weaker small-scale models, improving both training
stability and overall performance.</p>
<h3 id="39-llm4cell-a-survey-of-large-language-and-agentic-models-for-single-cell-biology">[39] <a href="https://arxiv.org/abs/2510.07793">LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology</a></h3>
<p><em>Sajib Acharjee Dip, Adrika Zafor, Bikash Kumar Paul, Uddip Acharjee Shuvo, Muhit Islam Emon, Xuan Wang, Liqing Zhang</em></p>
<h4 id="tldr_38">🧩 TL;DR</h4>
<p>LLM4Cell提出了首个统一的大语言模型和智能体框架在单细胞生物学领域的综述，系统分类了58个基础模型和智能体模型，涵盖了RNA、ATAC、多组学和空间模态，为语言驱动的单细胞智能研究提供了综合视图。</p>
<hr />
<h4 id="detailed-summary_38">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前大语言模型和新兴智能体框架正在通过自然语言推理、生成式注释和多模态数据集成改变单细胞生物学研究，但进展在数据模态、架构和评估标准方面仍然碎片化，缺乏统一的分类和评估框架。</p>
<p><strong>Method:</strong> 该研究系统综述了58个为单细胞研究开发的基础模型和智能体模型，将其分为基础模型、文本桥接、空间模型、多模态、表观基因组和智能体六个家族，并映射到八个关键分析任务，包括细胞注释、轨迹建模、扰动建模和药物反应预测等。</p>
<p><strong>Result:</strong> 基于40多个公共数据集的分析显示，该研究评估了模型在10个领域维度的表现，包括生物学基础性、多组学对齐、公平性、隐私保护和可解释性，同时分析了基准测试的适用性、数据多样性以及伦理和可扩展性约束。</p>
<p><strong>Conclusion:</strong> LLM4Cell通过连接数据集、模型和评估领域，为语言驱动的单细胞智能提供了首个集成视图，并指出了在可解释性、标准化和可信模型开发方面的开放挑战，为未来研究提供了系统框架和方向指引。</p>
<hr />
<h4 id="abstract_38">📄 Abstract</h4>
<p>Large language models (LLMs) and emerging agentic frameworks are beginning to
transform single-cell biology by enabling natural-language reasoning,
generative annotation, and multimodal data integration. However, progress
remains fragmented across data modalities, architectures, and evaluation
standards. LLM4Cell presents the first unified survey of 58 foundation and
agentic models developed for single-cell research, spanning RNA, ATAC,
multi-omic, and spatial modalities. We categorize these methods into five
families-foundation, text-bridge, spatial, multimodal, epigenomic, and
agentic-and map them to eight key analytical tasks including annotation,
trajectory and perturbation modeling, and drug-response prediction. Drawing on
over 40 public datasets, we analyze benchmark suitability, data diversity, and
ethical or scalability constraints, and evaluate models across 10 domain
dimensions covering biological grounding, multi-omics alignment, fairness,
privacy, and explainability. By linking datasets, models, and evaluation
domains, LLM4Cell provides the first integrated view of language-driven
single-cell intelligence and outlines open challenges in interpretability,
standardization, and trustworthy model development.</p>
<h3 id="40-cs3-bench-evaluating-and-enhancing-speech-to-speech-llms-for-mandarin-english-code-switching">[40] <a href="https://arxiv.org/abs/2510.07881">CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching</a></h3>
<p><em>Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang</em></p>
<h4 id="tldr_39">🧩 TL;DR</h4>
<p>该研究提出了代码切换语音到语音基准（CS3-Bench），揭示了现有多模态大语言模型在语言对齐方面的严重缺陷，并提出通过链式识别和关键词高亮方法显著提升了模型的跨语言理解和生成能力。</p>
<hr />
<h4 id="detailed-summary_39">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有多模态大语言模型在语音到语音交互系统中虽然实现了自然单语交互，但在语言对齐方面存在明显不足，特别是在代码切换场景下表现出严重的性能下降和误解问题。</p>
<p><strong>Method:</strong> 研究提出了CS3-Bench基准用于评估语言对齐能力，并开发了两种改进方法：使用链式识别（CoR）增强理解能力，以及采用关键词高亮（KH）技术指导生成过程。</p>
<p><strong>Result:</strong> 在7个主流模型上的实验显示知识密集型问答性能相对下降高达66%，而提出的方法将知识准确率从25.14%提升至46.13%，开放理解率从64.5%提升至86.5%，并显著减少了第二语言的发音错误。</p>
<p><strong>Conclusion:</strong> 该研究强调了多模态大语言模型中语言对齐的重要性，提出的数据构建和训练方法为改善跨语言交互系统提供了有效解决方案，并为未来多语言语音交互研究建立了重要基准。</p>
<hr />
<h4 id="abstract_39">📄 Abstract</h4>
<p>The advancement of multimodal large language models has accelerated the
development of speech-to-speech interaction systems. While natural monolingual
interaction has been achieved, we find existing models exhibit deficiencies in
language alignment. In our proposed Code-Switching Speech-to-Speech Benchmark
(CS3-Bench), experiments on 7 mainstream models demonstrate a relative
performance drop of up to 66% in knowledge-intensive question answering and
varying degrees of misunderstanding in open-ended conversations. Starting from
a model with severe performance deterioration, we propose both data
constructions and training approaches to improve the language alignment
capabilities, specifically employing Chain of Recognition (CoR) to enhance
understanding and Keyword Highlighting (KH) to guide generation. Our approach
improves the knowledge accuracy from 25.14% to 46.13%, with open-ended
understanding rate from 64.5% to 86.5%, and significantly reduces pronunciation
errors in the secondary language. CS3-Bench is available at
https://huggingface.co/datasets/VocalNet/CS3-Bench.</p>
<h3 id="41-vision-enabled-llms-in-historical-lexicography-digitising-and-enriching-estonian-german-dictionaries-from-the-17th-and-18th-centuries">[41] <a href="https://arxiv.org/abs/2510.07931">Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German Dictionaries from the 17th and 18th Centuries</a></h3>
<p><em>Madis Jürviste, Joonatan Jakobson</em></p>
<h4 id="tldr_40">🧩 TL;DR</h4>
<p>本研究探讨了大型语言模型在17-18世纪爱沙尼亚语历史词典研究中的应用，展示了LLMs在词典信息半自动丰富、哥特体文本识别和跨源数据集构建方面的显著潜力。</p>
<hr />
<h4 id="detailed-summary_40">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 该研究旨在解决历史词典研究中面临的关键挑战：如何高效丰富历史词典的现代词形和词义，如何自动识别哥特体印刷文本，以及如何为构建统一跨源数据集做准备，特别是针对资源相对匮乏的小语种研究。</p>
<p><strong>Method:</strong> 研究采用多模态方法，包括使用Claude 3.7 Sonnet进行词典信息的半自动丰富，运用视觉增强LLMs进行哥特体文本的零样本识别，以及采用重叠分块扫描图像处理技术结合多个LLM进行文本识别和结构化输出合并。</p>
<p><strong>Result:</strong> 实验结果显示，在充足上下文条件下，Claude 3.7 Sonnet能够为81%的词目准确提供词义和现代对应词；在文本识别实验中，零样本方法成功识别并结构化41%的词目为无错误的JSON格式输出；重叠分块技术有效支持了词典的数字化处理。</p>
<p><strong>Conclusion:</strong> 研究表明，即使是对于小语种，大型语言模型也具有显著的时间和资源节省潜力，为历史语言学研究和文化遗产数字化提供了高效的技术路径，同时证明了多LLM协同工作流程在复杂文本处理任务中的可行性。</p>
<hr />
<h4 id="abstract_40">📄 Abstract</h4>
<p>This article presents research conducted at the Institute of the Estonian
Language between 2022 and 2025 on the application of large language models
(LLMs) to the study of 17th and 18th century Estonian dictionaries. The authors
address three main areas: enriching historical dictionaries with modern word
forms and meanings; using vision-enabled LLMs to perform text recognition on
sources printed in Gothic script (Fraktur); and preparing for the creation of a
unified, cross-source dataset. Initial experiments with J. Gutslaff's 1648
dictionary indicate that LLMs have significant potential for semi-automatic
enrichment of dictionary information. When provided with sufficient context,
Claude 3.7 Sonnet accurately provided meanings and modern equivalents for 81%
of headword entries. In a text recognition experiment with A. T. Helle's 1732
dictionary, a zero-shot method successfully identified and structured 41% of
headword entries into error-free JSON-formatted output. For digitising the
Estonian-German dictionary section of A. W. Hupel's 1780 grammar, overlapping
tiling of scanned image files is employed, with one LLM being used for text
recognition and a second for merging the structured output. These findings
demonstrate that even for minor languages LLMs have a significant potential for
saving time and financial resources.</p>
<h3 id="42-leveraging-author-specific-context-for-scientific-figure-caption-generation-3rd-scicap-challenge">[42] <a href="https://arxiv.org/abs/2510.07993">Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge</a></h3>
<p><em>Watcharapong Timklaypachara, Monrada Chiewhawan, Nopporn Lekuthai, Titipat Achakulvisut</em></p>
<h4 id="tldr_41">🧩 TL;DR</h4>
<p>本研究提出了一个针对科学图表标题生成的领域特定系统，通过结合上下文过滤和作者特定写作风格优化，实现了既科学准确又风格忠实于源论文的标题生成。该系统采用两阶段流程，在SciCap挑战中证明了类别特定提示和风格化精炼的有效性。</p>
<hr />
<h4 id="detailed-summary_41">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 科学图表标题需要同时具备准确性和风格一致性以传达视觉信息，当前方法在保持作者特定写作风格方面存在不足。本研究旨在解决科学图表标题生成中上下文理解与风格适应性之间的平衡问题，特别是在领域特定的SciCap挑战背景下。</p>
<p><strong>Method:</strong> 系统采用两阶段流程：第一阶段结合上下文过滤、基于DSPy的MIPROv2和SIMBA进行类别特定提示优化以及标题候选选择；第二阶段应用少量样本提示结合配置文件进行风格化精炼。该方法整合了图表相关文本上下文与作者特定写作风格。</p>
<p><strong>Result:</strong> 实验表明类别特定提示在零样本和通用优化方法中表现最优，将ROUGE-1召回率提升+8.3%，同时将精度损失限制在-2.8%，BLEU-4减少控制在-10.9%。基于配置文件的风格化精炼使BLEU得分提升40-48%，ROUGE得分提升25-27%。</p>
<p><strong>Conclusion:</strong> 研究表明结合上下文理解与作者特定风格适应能够生成既科学准确又风格忠实的标题。该方法为领域特定的科学内容生成提供了有效框架，强调了类别特定优化和个性化风格调整在技术文档生成中的重要性。</p>
<hr />
<h4 id="abstract_41">📄 Abstract</h4>
<p>Scientific figure captions require both accuracy and stylistic consistency to
convey visual information. Here, we present a domain-specific caption
generation system for the 3rd SciCap Challenge that integrates figure-related
textual context with author-specific writing styles using the LaMP-Cap dataset.
Our approach uses a two-stage pipeline: Stage 1 combines context filtering,
category-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption
candidate selection; Stage 2 applies few-shot prompting with profile figures
for stylistic refinement. Our experiments demonstrate that category-specific
prompts outperform both zero-shot and general optimized approaches, improving
ROUGE-1 recall by +8.3\% while limiting precision loss to -2.8\% and BLEU-4
reduction to -10.9\%. Profile-informed stylistic refinement yields 40--48\%
gains in BLEU scores and 25--27\% in ROUGE. Overall, our system demonstrates
that combining contextual understanding with author-specific stylistic
adaptation can generate captions that are both scientifically accurate and
stylistically faithful to the source paper.</p>
<h3 id="43-learning-on-the-job-an-experience-driven-self-evolving-agent-for-long-horizon-tasks">[43] <a href="https://arxiv.org/abs/2510.08002">Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks</a></h3>
<p><em>Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, Yu Qiao, Haifeng Li</em></p>
<h4 id="tldr_42">🧩 TL;DR</h4>
<p>本文提出了MUSE，一种经验驱动的自进化AI智能体框架，通过分层记忆模块实现持续学习和自我进化，在长时程任务中显著超越了现有方法。</p>
<hr />
<h4 id="detailed-summary_42">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有LLM智能体存在测试时静态化的关键局限，无法从经验中学习，缺乏知识积累和持续改进能力，这限制了它们在现实世界长时程任务中的部署效果。</p>
<p><strong>Method:</strong> MUSE框架引入了以分层记忆模块为核心的经验驱动自进化系统，将原始轨迹转化为结构化经验并整合回记忆模块，使智能体能够超越其静态预训练参数实现持续进化。</p>
<p><strong>Result:</strong> 在长时程生产力基准TAC上，MUSE仅使用轻量级Gemini-2.5 Flash模型就实现了新的SOTA性能，显著优于现有方法，并且随着经验积累展现出越来越强的任务完成能力和泛化能力。</p>
<p><strong>Conclusion:</strong> MUSE建立了一个能够进行现实世界生产力任务自动化的AI智能体新范式，其积累的经验展现出强大的泛化特性，能够实现新任务的零样本改进，为持续学习智能体提供了可行路径。</p>
<hr />
<h4 id="abstract_42">📄 Abstract</h4>
<p>Large Language Models have demonstrated remarkable capabilities across
diverse domains, yet significant challenges persist when deploying them as AI
agents for real-world long-horizon tasks. Existing LLM agents suffer from a
critical limitation: they are test-time static and cannot learn from
experience, lacking the ability to accumulate knowledge and continuously
improve on the job. To address this challenge, we propose MUSE, a novel agent
framework that introduces an experience-driven, self-evolving system centered
around a hierarchical Memory Module. MUSE organizes diverse levels of
experience and leverages them to plan and execute long-horizon tasks across
multiple applications. After each sub-task execution, the agent autonomously
reflects on its trajectory, converting the raw trajectory into structured
experience and integrating it back into the Memory Module. This mechanism
enables the agent to evolve beyond its static pretrained parameters, fostering
continuous learning and self-evolution. We evaluate MUSE on the long-horizon
productivity benchmark TAC. It achieves new SOTA performance by a significant
margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments
demonstrate that as the agent autonomously accumulates experience, it exhibits
increasingly superior task completion capabilities, as well as robust
continuous learning and self-evolution capabilities. Moreover, the accumulated
experience from MUSE exhibits strong generalization properties, enabling
zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI
agents capable of real-world productivity task automation.</p>
<h3 id="44-a-survey-of-process-reward-models-from-outcome-signals-to-process-supervisions-for-large-language-models">[44] <a href="https://arxiv.org/abs/2510.08049">A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models</a></h3>
<p><em>Congming Zheng, Jiachen Zhu, Zhuoying Ou, Yuxiang Chen, Kangning Zhang, Rong Shan, Zeyu Zheng, Mengyue Yang, Jianghao Lin, Yong Yu, Weinan Zhang</em></p>
<h4 id="tldr_43">🧩 TL;DR</h4>
<p>本文对过程奖励模型（PRMs）进行了系统性综述，通过完整流程展示了如何生成过程数据、构建PRMs以及将其用于测试时扩展和强化学习，旨在推动细粒度、鲁棒的推理对齐研究。</p>
<hr />
<h4 id="detailed-summary_43">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 尽管大型语言模型展现出先进的推理能力，传统对齐方法主要被仅评判最终答案的结果奖励模型（ORMs）主导，过程奖励模型（PRMs）通过评估和指导步骤或轨迹级别的推理来弥补这一差距。</p>
<p><strong>Method:</strong> 该综述系统性地概述了PRMs的完整循环：包括过程数据生成方法、PRMs构建技术以及将PRMs用于测试时扩展和强化学习的应用策略。</p>
<p><strong>Result:</strong> 研究总结了PRMs在数学、代码、文本、多模态推理、机器人技术和智能体等领域的应用，并回顾了新兴的基准测试体系，揭示了设计空间和开放挑战。</p>
<p><strong>Conclusion:</strong> 该研究阐明了PRMs的设计空间，揭示了当前面临的开放挑战，并为未来研究提供了指导方向，推动向更细粒度、鲁棒的推理对齐方法发展。</p>
<hr />
<h4 id="abstract_43">📄 Abstract</h4>
<p>Although Large Language Models (LLMs) exhibit advanced reasoning ability,
conventional alignment remains largely dominated by outcome reward models
(ORMs) that judge only final answers. Process Reward Models(PRMs) address this
gap by evaluating and guiding reasoning at the step or trajectory level. This
survey provides a systematic overview of PRMs through the full loop: how to
generate process data, build PRMs, and use PRMs for test-time scaling and
reinforcement learning. We summarize applications across math, code, text,
multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our
goal is to clarify design spaces, reveal open challenges, and guide future
research toward fine-grained, robust reasoning alignment.</p>
<h3 id="45-dacip-rc-domain-adaptive-continual-instruction-pre-training-via-reading-comprehension-on-business-conversations">[45] <a href="https://arxiv.org/abs/2510.08152">DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations</a></h3>
<p><em>Elena Khasanova, Harsh Saini, Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN</em></p>
<h4 id="tldr_44">🧩 TL;DR</h4>
<p>本文提出DACIP-RC方法，通过阅读理解式持续预训练增强小型LLM在商业对话任务中的领域适应性和零样本泛化能力，解决了传统微调导致的灾难性遗忘问题。</p>
<hr />
<h4 id="detailed-summary_44">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 大规模LLM的高推理成本使其在工业部署中不切实际，而小型LLM缺乏跨领域的零样本指令跟随能力，传统微调方法会引发灾难性遗忘并降低模型对未见任务的泛化能力。</p>
<p><strong>Method:</strong> 提出领域自适应持续指令预训练方法DACIP-RC，通过阅读理解对话记录生成多样化任务指令和响应，替代传统的下一词预测预训练方式，实现更好的指令泛化。</p>
<p><strong>Result:</strong> 实证评估表明DACIP-RC在多种商业对话任务中显著提升零样本泛化能力，包括会议摘要、行动项生成和通话目的识别等任务。</p>
<p><strong>Conclusion:</strong> 这是首个在商业对话数据上应用指令预训练的工作，为行业如何利用专有数据集进行领域适应提供了实践见解，证明了持续预训练在保持模型泛化能力方面的有效性。</p>
<hr />
<h4 id="abstract_44">📄 Abstract</h4>
<p>The rapid advancements in Large Language Models (LLMs) have enabled their
adoption in real-world industrial scenarios for various natural language
processing tasks. However, the high inference cost of large-scale LLMs makes
their deployment impractical, necessitating the use of smaller models. Despite
their efficiency, smaller LLMs lack robust zero-shot instruction-following
capabilities across diverse domains, limiting their adaptability to dynamic
user requirements. Traditional fine-tuning approaches exacerbate this issue by
inducing catastrophic forgetting, reducing the model's generalization ability
for unseen tasks. In this paper, we propose Domain Adaptive Continual
Instruction Pre-Training via Reading Comprehension (DACIP-RC), a continual
pre-training technique that enhances smaller LLMs' domain adaptability for
business conversational tasks. Unlike conventional pre-training approaches that
rely on next-token prediction, DACIP-RC generates diverse task instructions and
responses via reading comprehension on conversation transcripts, enabling
better instruction generalization. Our empirical evaluations demonstrate that
DACIP-RC significantly improves zero-shot generalization across a wide range of
business conversational tasks, including meeting summarization, action item
generation, and call purpose identification. To the best of our knowledge, this
is the first work to apply instruction pre-training on business conversational
data, providing insights into how industries can leverage proprietary datasets
for domain adaptation.</p>
<h3 id="46-arm2-adaptive-reasoning-model-with-vision-understanding-and-executable-code">[46] <a href="https://arxiv.org/abs/2510.08163">ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code</a></h3>
<p><em>Jian Xie, Zhendong Chu, Aoxiao Zhong, Kai Zhang, Mingzhe Han, Xin Fang, Jialie Shen, Qingsong Wen</em></p>
<h4 id="tldr_45">🧩 TL;DR</h4>
<p>本文提出了ARM2，一种通过强化学习和长度感知优化的统一模型，能够自适应平衡推理性能与效率。该模型在保持与传统推理模型相当性能的同时，平均减少超过70%的token使用量。</p>
<hr />
<h4 id="detailed-summary_45">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 大型推理模型普遍存在"过度思考"问题，在简单任务上生成不必要的冗长推理。现有解决方案如长度惩罚或路由机制通常是启发式且任务特定的，缺乏自适应推理的通用框架。</p>
<p><strong>Method:</strong> ARM2采用强化学习框架结合长度感知优化，统一平衡多种格式的推理性能与效率。该模型不仅支持自然语言推理，还整合了视觉理解和可执行代码，扩展了多模态应用能力。</p>
<p><strong>Result:</strong> 实验表明ARM2在性能上与使用GRPO训练的传统推理模型相当，同时平均减少超过70%的token使用量。广泛的消融分析验证了ARM2的有效性和设计合理性。</p>
<p><strong>Conclusion:</strong> ARM2为自适应推理提供了通用解决方案，通过整合代码执行显著降低计算成本，同时保持任务性能。该研究为高效多模态推理系统的发展提供了重要方向。</p>
<hr />
<h4 id="abstract_45">📄 Abstract</h4>
<p>Large Reasoning Models (LRMs) often suffer from the ``over-thinking''
problem, generating unnecessarily long reasoning on simple tasks. Some
strategies have been proposed to mitigate this issue, such as length penalties
or routing mechanisms, but they are typically heuristic and task-specific,
lacking a general framework for adaptive reasoning. In this paper, we present
ARM2, a unified model that adaptively balances reasoning performance and
efficiency across multiple formats through a reinforcement learning framework
augmented with length-aware optimization. Beyond conventional natural language
inference, ARM2 integrates vision understanding, extending its applicability to
multimodal. Moreover, ARM2 integrates executable code into reasoning, enabling
substantial reductions in token cost while preserving task performance compared
to long CoT. Experiments demonstrate that ARM2 achieves performance on par with
traditional reasoning models trained with GRPO, while reducing token usage by
over 70% on average. We further conduct extensive analyses to validate the
effectiveness of ARM2 and the soundness of its design.</p>
<h3 id="47-single-layer-tiny-co4-outpaces-gpt-2-and-gpt-bert">[47] <a href="https://arxiv.org/abs/2510.08404">Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT</a></h3>
<p><em>Noor Ul Zain, Mohsin Raza, Ahsan Adeel</em></p>
<h4 id="tldr_46">🧩 TL;DR</h4>
<p>本文提出了一种名为Co⁴的极小模型，仅包含单层、双头和8M参数，以近似O(N)的计算成本在BabyLM挑战中超越了GPT-2和GPT-BERT基线模型，展示了极高的训练效率和样本效率。</p>
<hr />
<h4 id="detailed-summary_46">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前深度学习范式普遍依赖深层网络和O(N²)计算复杂度，但这种方法在训练效率和样本效率方面存在显著局限性，需要探索更高效的模型架构和训练方法。</p>
<p><strong>Method:</strong> 采用Co⁴机器架构，仅包含单层transformer、两个注意力头和800万参数，通过近似O(N)的计算复杂度实现高效训练，相比传统O(N²)复杂度模型具有显著计算优势。</p>
<p><strong>Result:</strong> 在仅训练两个周期后，Co⁴在10M tokens上实现了数量级更高的训练效率，在BabyLM评估中零样本性能超越GPT-2的5/7指标和GPT-BERT的4/7指标，微调性能超越GPT-2的6/7任务和GPT-BERT的4/7指标。</p>
<p><strong>Conclusion:</strong> 研究结果表明需要重新思考当前主流的深度学习范式和扩展定律，极小模型通过高效架构设计可以在显著降低计算成本的同时实现竞争性性能，为高效AI模型开发提供了新方向。</p>
<hr />
<h4 id="abstract_46">📄 Abstract</h4>
<p>We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two
heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$
is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2
(124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two
epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude
greater training efficiency on 10M tokens, demonstrating highly sample
efficient pretraining. Using the BabyLM challenge evaluation pipeline across
complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning
performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out
of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out
of 7 metrics in both cases. These results suggest the need to rethink
prevailing deep learning paradigms and associated scaling laws.</p>
<h3 id="48-ares-multimodal-adaptive-reasoning-via-difficulty-aware-token-level-entropy-shaping">[48] <a href="https://arxiv.org/abs/2510.08457">ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping</a></h3>
<p><em>Shuang Chen, Yue Guo, Yimeng Ye, Shijue Huang, Wenbo Hu, Haoxi Li, Manyuan Zhang, Jiayu Chen, Song Guo, Nanyun Peng</em></p>
<h4 id="tldr_47">🧩 TL;DR</h4>
<p>本文提出ARES框架，通过自适应推理机制动态分配探索努力来解决多模态大推理模型在简单问题上过度思考、在困难问题上探索不足的问题。该框架利用高窗口熵令牌作为探索触发器，在多个数学、逻辑和多模态基准上实现了优越性能和推理效率。</p>
<hr />
<h4 id="detailed-summary_47">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前多模态大推理模型存在推理努力分配不平衡的问题：在简单问题上产生不必要的冗长推理轨迹（过度思考），而在困难问题上探索不足导致错失解决方案。这种不平衡限制了模型的实际应用效率和性能。</p>
<p><strong>Method:</strong> ARES框架采用两阶段训练流程：自适应冷启动阶段通过按问题难度比例配对的推理轨迹数据赋予模型初始难度感知能力；第二阶段提出自适应熵策略优化，使用高窗口熵令牌作为探索触发器决定何时探索，并采用带动态KL控制的分层熵奖励决定探索程度。</p>
<p><strong>Result:</strong> 广泛实验表明，ARES在多样化数学、逻辑和多模态基准上实现了优越性能和推理效率，同时在显著降低推理成本的情况下缩小了与领先商业系统的性能差距。</p>
<p><strong>Conclusion:</strong> 该研究表明基于高窗口熵令牌的自适应推理机制能有效平衡模型在不同难度问题上的探索努力，为构建更高效、更具成本效益的多模态推理系统提供了重要方向，同时证明了动态难度感知训练策略的可行性。</p>
<hr />
<h4 id="abstract_47">📄 Abstract</h4>
<p>Recent advances in multimodal large reasoning models (MLRMs) have
substantially improved their ability to solve complex textual and visual tasks.
However, these models tend to overthink on simple problems, producing
unnecessarily lengthy reasoning traces, while under-exploring on challenging
ones, leading to missed solutions. To address this imbalance, we propose ARES,
a unified open-source framework for adaptive reasoning that dynamically
allocates exploration effort based on task difficulty. Our approach is
motivated by two key empirical findings: (i) while single-token entropy is
noisy, high window-entropy (HWE) tokens (token-level entropies averaged under a
sliding window) can reliably capture reasoning-critical moments; and (ii)
reducing HWE usage benefits easy problems, while increasing it is essential for
solving hard ones. Building on these insights, ARES introduces a two-stage
training pipeline. In the Adaptive Cold-Start stage, we curate multimodal and
textual data paired with reasoning traces of length proportional to problem
difficulty, equipping the model with initial difficulty awareness. In the
second stage, we develop Adaptive Entropy Policy Optimization (AEPO), which
uses HWE tokens as exploration triggers to decide when to explore, and a
hierarchical entropy reward with dynamic KL control to decide how much to
explore. Extensive experiments demonstrate that ARES achieves superior
performance and reasoning efficiency across diverse mathematical, logical, and
multimodal benchmarks, while closing the gap to leading commercial systems
under significantly lower inference costs.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="49-ts-agent-a-time-series-reasoning-agent-with-iterative-statistical-insight-gathering">[49] <a href="https://arxiv.org/abs/2510.07432">TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering</a></h3>
<p><em>Penghang Liu, Elizabeth Fons, Svitlana Vyetrenko, Daniel Borrajo, Vamsi Potluru, Manuela Veloso</em></p>
<h4 id="tldr_48">🧩 TL;DR</h4>
<p>本文提出了TS-Agent，一种时间序列推理智能体，通过将LLMs的推理能力与时间序列分析工具相结合，专门解决LLMs在时间序列推理任务中的幻觉和知识泄露问题。</p>
<hr />
<h4 id="detailed-summary_48">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 大型语言模型在推理和问题解决方面表现出强大能力，但最近研究发现它们在时间序列推理任务中仍然存在困难，输出经常受到幻觉或知识泄露的影响，这限制了LLMs在时间序列分析领域的实际应用价值。</p>
<p><strong>Method:</strong> TS-Agent采用模块化设计，严格利用LLMs擅长证据收集和结论合成的能力，同时将统计和结构信息提取委托给时间序列分析工具；智能体通过原子操作符与原始数值序列交互，在显式证据日志中记录输出，并在自我批评和最终质量门的指导下迭代优化推理过程。</p>
<p><strong>Result:</strong> 在标准基准测试上的实验表明，TS-Agent在理解基准测试中达到了与最先进LLMs相当的性能，在推理任务上实现了显著改进，特别是在零样本设置下，现有模型通常依赖记忆而失败的情况下表现优异。</p>
<p><strong>Conclusion:</strong> 该研究证明了通过将LLMs严格限定在其优势领域，同时将专业任务委托给专用工具，可以有效避免多模态对齐训练，保持时间序列的原始形式，确保可解释性和可验证性，并为时间序列推理任务提供了新的解决方案框架。</p>
<hr />
<h4 id="abstract_48">📄 Abstract</h4>
<p>Large language models (LLMs) have shown strong abilities in reasoning and
problem solving, but recent studies reveal that they still struggle with time
series reasoning tasks, where outputs are often affected by hallucination or
knowledge leakage. In this work we propose TS-Agent, a time series reasoning
agent that leverages LLMs strictly for what they excel at, i.e., gathering
evidence and synthesizing it into conclusions through step-by-step reasoning,
while delegating the extraction of statistical and structural information to
time series analytical tools. Instead of mapping time series into text tokens,
images, or embeddings, our agent interacts with raw numeric sequences through
atomic operators, records outputs in an explicit evidence log, and iteratively
refines its reasoning under the guidance of a self-critic and a final quality
gate. This design avoids multi-modal alignment training, preserves the native
form of time series, ensures interpretability and verifiability, and mitigates
knowledge leakage or hallucination. Empirically, we evaluate the agent on
established benchmarks. Our experiments show that TS-Agent achieves performance
comparable to state-of-the-art LLMs on understanding benchmarks, and delivers
significant improvements on reasoning tasks, where existing models often rely
on memorization and fail in zero-shot settings.</p>
<h3 id="50-evaluation-of-llms-for-process-model-analysis-and-optimization">[50] <a href="https://arxiv.org/abs/2510.07489">Evaluation of LLMs for Process Model Analysis and Optimization</a></h3>
<p><em>Akhil Kumar, Jianliang Leon Zhao, Om Dobariya</em></p>
<h4 id="tldr_49">🧩 TL;DR</h4>
<p>本研究评估了大型语言模型在零样本设置下理解BPMN业务流程模型、检测语法逻辑错误以及进行深度推理的能力，发现未经专门训练的LLM能够有效分析流程模型并在语法、逻辑和语义层面提供智能回答。</p>
<hr />
<h4 id="detailed-summary_49">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 该研究旨在探索大型语言模型是否能够通过自然语言交互方式理解业务流程模型，检测其中的语法和逻辑错误，并进行深度推理，填补了LLM在业务流程分析领域应用的研究空白。</p>
<p><strong>Method:</strong> 研究采用零样本设置下的多种LLM（包括ChatGPT o3模型），通过自然语言接口对BPMN流程模型图像进行理解，评估模型在语法、逻辑和语义层面的分析能力，并研究其"思维过程"和深度推理机制。</p>
<p><strong>Result:</strong> 实验结果表明，未经专门训练的LLM能够有效理解BPMN流程模型图像，在语法、逻辑和语义层面提供准确回答，不同LLM在准确性和有效性方面存在性能差异，但总体上展现出作为业务流程设计助手的重要价值。</p>
<p><strong>Conclusion:</strong> LLM在业务流程分析中展现出类人化的推理特性，能够作为业务流程设计者和用户的有效助手，为流程分析和优化提供深度推理支持，揭示了LLM在专业领域应用的潜力。</p>
<hr />
<h4 id="abstract_49">📄 Abstract</h4>
<p>In this paper, we report our experience with several LLMs for their ability
to understand a process model in an interactive, conversational style, find
syntactical and logical errors in it, and reason with it in depth through a
natural language (NL) interface. Our findings show that a vanilla, untrained
LLM like ChatGPT (model o3) in a zero-shot setting is effective in
understanding BPMN process models from images and answering queries about them
intelligently at syntactic, logic, and semantic levels of depth. Further,
different LLMs vary in performance in terms of their accuracy and
effectiveness. Nevertheless, our empirical analysis shows that LLMs can play a
valuable role as assistants for business process designers and users. We also
study the LLM's "thought process" and ability to perform deeper reasoning in
the context of process analysis and optimization. We find that the LLMs seem to
exhibit anthropomorphic properties.</p>
<h3 id="51-an-evaluation-study-of-hybrid-methods-for-multilingual-pii-detection">[51] <a href="https://arxiv.org/abs/2510.07551">An Evaluation Study of Hybrid Methods for Multilingual PII Detection</a></h3>
<p><em>Harshit Rajgarhia, Suryam Gupta, Asif Shaik, Gulipalli Praveen Kumar, Y Santhoshraj, Sanka Nithya Tanvy Nishitha, Abhishek Mukherji</em></p>
<h4 id="tldr_50">🧩 TL;DR</h4>
<p>RECAP是一个混合框架，结合确定性正则表达式和上下文感知大语言模型，用于在13种低资源语言中实现可扩展的PII检测。该系统无需重新训练即可支持300多种实体类型，在加权F1分数上显著优于现有方法。</p>
<hr />
<h4 id="detailed-summary_50">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前在低资源语言中检测个人身份信息面临语言多样性和标注数据有限的挑战，这给隐私合规带来了困难，需要开发能够适应多种语言环境的解决方案。</p>
<p><strong>Method:</strong> RECAP采用混合框架设计，将确定性正则表达式与上下文感知LLMs相结合，通过模块化架构支持超过300种实体类型而无需重新训练，并采用三阶段精炼流程进行消歧和过滤处理。</p>
<p><strong>Result:</strong> 使用nervaluate基准测试表明，该系统在加权F1分数上比微调NER模型高出82%，比零样本LLMs高出17%，在13种低资源语言环境中表现出卓越性能。</p>
<p><strong>Conclusion:</strong> 该研究提供了一个可扩展且适应性强的解决方案，能够有效支持合规导向应用中的PII检测需求，为多语言环境下的隐私保护提供了实用框架。</p>
<hr />
<h4 id="abstract_50">📄 Abstract</h4>
<p>The detection of Personally Identifiable Information (PII) is critical for
privacy compliance but remains challenging in low-resource languages due to
linguistic diversity and limited annotated data. We present RECAP, a hybrid
framework that combines deterministic regular expressions with context-aware
large language models (LLMs) for scalable PII detection across 13 low-resource
locales. RECAP's modular design supports over 300 entity types without
retraining, using a three-phase refinement pipeline for disambiguation and
filtering. Benchmarked with nervaluate, our system outperforms fine-tuned NER
models by 82% and zero-shot LLMs by 17% in weighted F1-score. This work offers
a scalable and adaptable solution for efficient PII detection in
compliance-focused applications.</p>
<h3 id="52-test-time-matching-unlocking-compositional-reasoning-in-multimodal-models">[52] <a href="https://arxiv.org/abs/2510.07632">Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models</a></h3>
<p><em>Yinglun Zhu, Jiancheng Zhang, Fuzhi Tang</em></p>
<h4 id="tldr_51">🧩 TL;DR</h4>
<p>本研究揭示了现有评估指标系统性低估了前沿AI模型的组合推理能力，提出了组匹配分数和测试时匹配算法，显著提升了模型在组合推理基准上的表现，甚至超越了人类估计性能。</p>
<hr />
<h4 id="detailed-summary_51">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 前沿AI模型在组合推理方面表现不佳，现有评估指标系统性低估了模型的实际能力，导致在标准基准测试中性能被严重低估，需要更准确的评估方法和性能提升技术。</p>
<p><strong>Method:</strong> 提出了组匹配分数来更好地利用组结构揭示模型的隐藏能力，并开发了测试时匹配算法，这是一种无需外部监督的迭代自改进方法，通过在测试时过拟合诱导的组匹配来提升标准评估指标下的性能。</p>
<p><strong>Result:</strong> 方法显著提升了模型性能：SigLIP-B16超越所有先前结果，GPT-4.1在Winoground上首次超越估计的人类性能，TTM使SigLIP-B16在MMVP-VLM上超越GPT-4.1建立新SOTA，在16个数据集变体上均实现一致改进，在WhatsUp等挑战性数据集上相对增益高达85.7%。</p>
<p><strong>Conclusion:</strong> 研究表明现有评估指标存在系统性偏差，通过适当的测试时调整可以显著释放模型的隐藏组合推理能力，测试时匹配算法为提升模型性能提供了有效且通用的解决方案，推动了组合推理研究的前沿发展。</p>
<hr />
<h4 id="abstract_51">📄 Abstract</h4>
<p>Frontier AI models have achieved remarkable progress, yet recent studies
suggest they struggle with compositional reasoning, often performing at or
below random chance on established benchmarks. We revisit this problem and show
that widely used evaluation metrics systematically underestimate model
capability. To address this, we introduce a group matching score that better
exploits group structure and reveals substantial hidden capability in both
contrastive vision-language models (VLMs) and multimodal large language models
(MLLMs). Moreover, simply overfitting to the induced group matchings at test
time transfers this hidden capability into higher scores under standard
evaluation metrics, closing much of the reported gap. This adjustment enables
SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first
result surpassing estimated human performance on Winoground.
  Building on this insight, we propose Test-Time Matching (TTM), an iterative,
self-improving algorithm that further bootstraps model performance without any
external supervision. TTM delivers additional, non-trivial improvements: for
example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a
new state of the art. Importantly, TTM remains broadly effective even on
benchmarks without metric-induced effects or group structures, achieving
relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16
dataset variants spanning diverse setups, our experiments demonstrate that TTM
consistently improves model performance and advances the frontier of
compositional reasoning.</p>
<h3 id="53-multimodal-safety-evaluation-in-generative-agent-social-simulations">[53] <a href="https://arxiv.org/abs/2510.07709">Multimodal Safety Evaluation in Generative Agent Social Simulations</a></h3>
<p><em>Alhim Vera, Karen Sanchez, Carlos Hinojosa, Haidar Bin Hamid, Donghoon Kim, Bernard Ghanem</em></p>
<h4 id="tldr_52">🧩 TL;DR</h4>
<p>本研究提出了一个可复现的多模态智能体仿真框架，通过社会行为指标评估生成式智能体在安全、一致性和信任方面的表现，揭示了当前模型在多模态环境中存在严重的安全对齐缺陷。</p>
<hr />
<h4 id="detailed-summary_52">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 尽管大型语言模型和视觉语言模型使智能体能够在丰富环境中自主行动并追求目标，但它们在跨模态安全、一致性和信任推理方面的能力仍然有限，需要系统评估其在多模态环境中的可信度。</p>
<p><strong>Method:</strong> 研究引入了可复现的仿真框架，配备分层记忆、动态规划和多模态感知能力的智能体，并采用SocialMetrics行为指标套件来量化计划修订、不安全到安全转换以及网络中的信息扩散。</p>
<p><strong>Result:</strong> 实验显示智能体在纠正不安全计划方面仅达到55%成功率，三个模型（Claude、GPT-4o mini、Qwen-VL）的平均不安全到安全转换率分别为75%、55%和58%，在误导性视觉信息下45%的不安全行为被接受。</p>
<p><strong>Conclusion:</strong> 研究结果揭示了当前架构在多模态安全对齐方面的关键局限性，为研究多模态安全、一致性和社会动态提供了可复现平台，强调了智能体对图像信息的过度信任问题。</p>
<hr />
<h4 id="abstract_52">📄 Abstract</h4>
<p>Can generative agents be trusted in multimodal environments? Despite advances
in large language and vision-language models that enable agents to act
autonomously and pursue goals in rich settings, their ability to reason about
safety, coherence, and trust across modalities remains limited. We introduce a
reproducible simulation framework for evaluating agents along three dimensions:
(1) safety improvement over time, including iterative plan revisions in
text-visual scenarios; (2) detection of unsafe activities across multiple
categories of social situations; and (3) social dynamics, measured as
interaction counts and acceptance ratios of social exchanges. Agents are
equipped with layered memory, dynamic planning, multimodal perception, and are
instrumented with SocialMetrics, a suite of behavioral and structural metrics
that quantifies plan revisions, unsafe-to-safe conversions, and information
diffusion across networks. Experiments show that while agents can detect direct
multimodal contradictions, they often fail to align local revisions with global
safety, reaching only a 55 percent success rate in correcting unsafe plans.
Across eight simulation runs with three models - Claude, GPT-4o mini, and
Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75,
55, and 58 percent, respectively. Overall performance ranged from 20 percent in
multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such
as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted
when paired with misleading visuals, showing a strong tendency to overtrust
images. These findings expose critical limitations in current architectures and
provide a reproducible platform for studying multimodal safety, coherence, and
social dynamics.</p>
<h3 id="54-finmr-a-knowledge-intensive-multimodal-benchmark-for-advanced-financial-reasoning">[54] <a href="https://arxiv.org/abs/2510.07852">FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning</a></h3>
<p><em>Shuangyan Deng, Haizhou Peng, Jiachen Xu, Rui Mao, Ciprian Doru Giurcăneanu, Jiamou Liu</em></p>
<h4 id="tldr_53">🧩 TL;DR</h4>
<p>本文提出了FinMR，一个高质量、知识密集的多模态数据集，专门用于评估专业分析师级别的金融推理能力。该数据集包含3200多个精心策划的专家标注问答对，涵盖15个金融主题，为多模态大语言模型在金融领域的专业级评估提供了关键基准。</p>
<hr />
<h4 id="detailed-summary_53">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 当前多模态大语言模型在金融等专业领域的严格评估受到限制，主要原因是缺乏具有专业级知识强度、详细标注和高级推理复杂度的数据集。现有数据集无法充分评估模型在专业金融分析师标准下的推理能力，这阻碍了模型在金融领域的实际应用和发展。</p>
<p><strong>Method:</strong> 研究团队构建了FinMR数据集，包含超过3200个精心策划和专家标注的问答对，涵盖15个不同的金融主题。该数据集整合了复杂的数学推理、高级金融知识和细微的图像解释任务，涉及多种图像类型，确保广泛的领域多样性和专业级的知识强度。</p>
<p><strong>Result:</strong> 通过对领先的闭源和开源多模态大语言模型进行全面基准测试，研究发现这些模型与专业金融分析师之间存在显著的性能差距。测试揭示了模型在精确图像分析、复杂金融公式的准确应用以及更深层次上下文金融理解等关键领域需要改进。</p>
<p><strong>Conclusion:</strong> FinMR通过提供丰富多样的视觉内容和详尽的解释性标注，确立了作为评估和推进多模态金融推理向专业分析师水平发展的关键基准工具。该研究为多模态大语言模型在金融领域的专业级能力评估提供了重要基础，并指明了模型改进的关键方向。</p>
<hr />
<h4 id="abstract_53">📄 Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) have made substantial progress in
recent years. However, their rigorous evaluation within specialized domains
like finance is hindered by the absence of datasets characterized by
professional-level knowledge intensity, detailed annotations, and advanced
reasoning complexity. To address this critical gap, we introduce FinMR, a
high-quality, knowledge-intensive multimodal dataset explicitly designed to
evaluate expert-level financial reasoning capabilities at a professional
analyst's standard. FinMR comprises over 3,200 meticulously curated and
expertly annotated question-answer pairs across 15 diverse financial topics,
ensuring broad domain diversity and integrating sophisticated mathematical
reasoning, advanced financial knowledge, and nuanced visual interpretation
tasks across multiple image types. Through comprehensive benchmarking with
leading closed-source and open-source MLLMs, we highlight significant
performance disparities between these models and professional financial
analysts, uncovering key areas for model advancement, such as precise image
analysis, accurate application of complex financial formulas, and deeper
contextual financial understanding. By providing richly varied visual content
and thorough explanatory annotations, FinMR establishes itself as an essential
benchmark tool for assessing and advancing multimodal financial reasoning
toward professional analyst-level competence.</p>
<h3 id="55-augur-modeling-covariate-causal-associations-in-time-series-via-large-language-models">[55] <a href="https://arxiv.org/abs/2510.07858">Augur: Modeling Covariate Causal Associations in Time Series via Large Language Models</a></h3>
<p><em>Zhiqing Cui, Binwu Wang, Qingxiang Liu, Yeqiang Wang, Zhengyang Zhou, Yuxuan Liang, Yang Wang</em></p>
<h4 id="tldr_54">🧩 TL;DR</h4>
<p>本文提出Augur框架，首次将大型语言模型完全驱动时间序列预测，通过因果推理发现协变量间的有向因果关系，在保持预测准确性的同时提供可解释的推理过程。</p>
<hr />
<h4 id="detailed-summary_54">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 现有基于LLM的时间序列预测方法存在显著局限性，包括在模型架构中的边缘化角色、依赖粗糙统计文本提示以及缺乏可解释性，这些问题限制了LLM在时间序列分析中的潜力发挥。</p>
<p><strong>Method:</strong> Augur采用两阶段师生架构，首先由强大的教师LLM通过启发式搜索和成对因果检验从时间序列推断有向因果图，然后由轻量级学生代理精炼该图并基于高置信度因果关联进行微调，这些关联被编码为丰富文本提示用于预测。</p>
<p><strong>Result:</strong> 在真实世界数据集上进行的广泛实验表明，Augur在25个基线方法中实现了竞争性性能，并展现出强大的零样本泛化能力，验证了该框架的有效性和鲁棒性。</p>
<p><strong>Conclusion:</strong> 该研究证明了LLM因果推理在时间序列预测中的有效性，不仅提升了预测精度，还提供了透明可追溯的变量交互推理，为可解释AI在时间序列分析中的应用开辟了新方向。</p>
<hr />
<h4 id="abstract_54">📄 Abstract</h4>
<p>Large language models (LLM) have emerged as a promising avenue for time
series forecasting, offering the potential to integrate multimodal data.
However, existing LLM-based approaches face notable limitations-such as
marginalized role in model architectures, reliance on coarse statistical text
prompts, and lack of interpretability. In this work, we introduce Augur, a
fully LLM driven time series forecasting framework that exploits LLM causal
reasoning to discover and use directed causal associations among covariates.
Augur uses a two stage teacher student architecture where a powerful teacher
LLM infers a directed causal graph from time series using heuristic search
together with pairwise causality testing. A lightweight student agent then
refines the graph and fine tune on high confidence causal associations that are
encoded as rich textual prompts to perform forecasting. This design improves
predictive accuracy while yielding transparent, traceable reasoning about
variable interactions. Extensive experiments on real-world datasets with 25
baselines demonstrate that Augur achieves competitive performance and robust
zero-shot generalization.</p>
<h3 id="56-chain-of-trigger-an-agentic-backdoor-that-paradoxically-enhances-agentic-robustness">[56] <a href="https://arxiv.org/abs/2510.08238">Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness</a></h3>
<p><em>Jiyang Qiu, Xinbei Ma, Yunqing Xu, Zhuosheng Zhang, Hai Zhao</em></p>
<h4 id="tldr_55">🧩 TL;DR</h4>
<p>本文提出Chain-of-Trigger Backdoor (CoTri)攻击方法，针对LLM智能体实现多步骤后门控制，在保持高攻击成功率的同时增强智能体在良性任务上的性能，揭示了智能体安全性的潜在风险。</p>
<hr />
<h4 id="detailed-summary_55">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 随着基于大语言模型的智能体在现实应用中的快速部署，其可信赖性引发严重担忧。传统后门攻击仅限于单步控制，无法应对智能体在长时程任务中的多步骤决策过程，存在安全性和鲁棒性漏洞需要深入探索。</p>
<p><strong>Method:</strong> 提出Chain-of-Trigger Backdoor (CoTri)多步骤后门攻击框架，采用有序触发序列策略，初始触发后从环境中提取后续触发条件，实现对智能体的多步骤操控，使其偏离原始任务目标。该方法通过训练数据建模环境的随机性特性来增强攻击的隐蔽性。</p>
<p><strong>Result:</strong> 实验结果显示CoTri达到接近完美的攻击成功率(ASR)，同时保持接近零的误触发率(FTR)。由于训练数据对环境随机性的建模，CoTri植入反而提升了智能体在良性任务上的性能表现，并增强了其对抗环境干扰的鲁棒性。在视觉语言模型上的验证进一步证实了该方法对多模态智能体的可扩展性。</p>
<p><strong>Conclusion:</strong> CoTri实现了智能体内稳定的多步骤控制，同时增强了其固有鲁棒性和任务能力，这使得攻击更加隐蔽并带来潜在安全风险。研究揭示了智能体安全性的深层脆弱性，强调了在部署前进行严格安全评估的必要性，为未来防御机制设计提供了重要参考。</p>
<hr />
<h4 id="abstract_55">📄 Abstract</h4>
<p>The rapid deployment of large language model (LLM)-based agents in real-world
applications has raised serious concerns about their trustworthiness. In this
work, we reveal the security and robustness vulnerabilities of these agents
through backdoor attacks. Distinct from traditional backdoors limited to
single-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a
multi-step backdoor attack designed for long-horizon agentic control. CoTri
relies on an ordered sequence. It starts with an initial trigger, and
subsequent ones are drawn from the environment, allowing multi-step
manipulation that diverts the agent from its intended task. Experimental
results show that CoTri achieves a near-perfect attack success rate (ASR) while
maintaining a near-zero false trigger rate (FTR). Due to training data modeling
the stochastic nature of the environment, the implantation of CoTri
paradoxically enhances the agent's performance on benign tasks and even
improves its robustness against environmental distractions. We further validate
CoTri on vision-language models (VLMs), confirming its scalability to
multimodal agents. Our work highlights that CoTri achieves stable, multi-step
control within agents, improving their inherent robustness and task
capabilities, which ultimately makes the attack more stealthy and raises
potential safty risks.</p>
<h3 id="57-looking-to-learn-token-wise-dynamic-gating-for-low-resource-vision-language-modelling">[57] <a href="https://arxiv.org/abs/2510.08470">Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling</a></h3>
<p><em>Bianca-Mihaela Ganescu, Suchir Salhan, Andrew Caines, Paula Buttery</em></p>
<h4 id="tldr_56">🧩 TL;DR</h4>
<p>本文提出了一种轻量级解码器架构，通过动态门控机制实现语言和视觉信息的自适应融合，在认知合理的数据量约束下实现了竞争性多模态学习性能。该方法在五个基准测试中表现优异，且动态门控机制无需显式监督即可发现可解释的模式。</p>
<hr />
<h4 id="detailed-summary_56">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 本研究旨在解决在认知合理数据量约束下训练视觉语言模型的核心挑战，即在有限的多模态信息下重新思考模型如何有效整合语言和视觉线索。BabyLM Challenge 2025的视觉赛道约束条件促使研究者开发能够在严重数据限制下仍能高效学习的多模态架构。</p>
<p><strong>Method:</strong> 提出轻量级解码器架构，包含三个关键技术：令牌级动态门控用于语言和视觉线索的自适应融合；特征调制和通道注意力机制以最大化有限视觉信息的效用；辅助对比学习目标用于视觉基础。这些组件共同作用在严格的数据约束下实现高效多模态学习。</p>
<p><strong>Result:</strong> 在五个基准测试（BLiMP、BLiMP Supplement、EWoK、Winoground和VQA）上评估显示，该方法在多模态基线中达到竞争性或更优性能。动态门控机制无需显式监督即可发现可解释模式：对内容词偏好视觉线索，对功能词偏好语言线索。同时识别了挑战约束下的局限性，如全局图像嵌入造成的信息瓶颈和数据集分割导致的训练不稳定性。</p>
<p><strong>Conclusion:</strong> 研究确立了动态门控作为高效多模态学习的强大工具，即使在严格约束下也能提供可解释性和性能优势。动态门控的自适应特性为轻量级多模态模型设计提供了新思路，其发现的模式与认知语言学原理一致，为未来在资源受限环境下的多模态学习研究奠定了基础。</p>
<hr />
<h4 id="abstract_56">📄 Abstract</h4>
<p>Training vision-language models on cognitively-plausible amounts of data
requires rethinking how models integrate multimodal information. Within the
constraints of the Vision track for the BabyLM Challenge 2025, we propose a
lightweight decoder-based architecture with (1) token-wise dynamic gating for
adaptive fusion of linguistic and visual cues, (2) feature modulation and
channel attention to maximise the utility of limited visual information and (3)
auxiliary contrastive objectives for visual grounding. Evaluation on five
benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows
competitive or superior performance to multimodal baselines. More notably, our
dynamic gate discovers interpretable patterns without explicit supervision,
favouring visual cues for content words and linguistic cues for function words.
While we identify limitations in the Challenge constraints, such as the
information bottleneck created by global image embeddings and training
instability from the dataset split, our findings establish dynamic gating as a
powerful tool for efficient multimodal learning, offering both interpretability
and performance even under severe constraints.</p>
<h3 id="58-how-to-teach-large-multimodal-models-new-skills">[58] <a href="https://arxiv.org/abs/2510.08564">How to Teach Large Multimodal Models New Skills</a></h3>
<p><em>Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem</em></p>
<h4 id="tldr_57">🧩 TL;DR</h4>
<p>本研究提出了两种简单的微调方法，通过仅更新自注意力投影层或仅更新MLP Gate&amp;Up投影层，有效解决了大型多模态模型在顺序微调中的灾难性遗忘问题，实现了目标技能学习与通用能力保持的良好平衡。</p>
<hr />
<h4 id="detailed-summary_57">📘 Detailed Summary</h4>
<p><strong>Motivation:</strong> 本研究旨在解决大型多模态模型在顺序微调过程中面临的灾难性遗忘问题，即学习新技能时可能擦除先前获得的能力，这限制了模型在实际应用中的持续学习能力。</p>
<p><strong>Method:</strong> 研究提出了两种简单而鲁棒的微调方法：一是仅更新自注意力投影层，二是仅更新MLP中的Gate&amp;Up投影层同时冻结Down投影层，这些方法通过限制参数更新范围来减少输出分布的漂移。</p>
<p><strong>Result:</strong> 实验在三个模型家族上进行，涵盖五个目标技能和八个保持基准测试，结果显示所提出的微调方法在获得强大目标技能提升的同时，显著保持了保持任务的性能，有效缓解了遗忘现象。</p>
<p><strong>Conclusion:</strong> 研究揭示了模型遗忘行为与输出标记分布漂移之间的关联，并证明了通过选择性参数更新策略可以实现高效持续学习，为大型多模态模型的顺序微调提供了实用的解决方案。</p>
<hr />
<h4 id="abstract_57">📄 Abstract</h4>
<p>How can we teach large multimodal models (LMMs) new skills without erasing
prior abilities? We study sequential fine-tuning on five target skills while
monitoring general ability on eight held-out benchmarks across three model
families. We observe that apparent "forgetting" on held-out tasks after narrow
fine-tuning can partly recover at later stages. We trace this behavior to a
measurable shift in the output token distribution, manifested through a simple
counting-bias probe that co-varies with forgetting. Guided by this picture, we
identify two simple, robust tuning recipes that learn strongly while limiting
drift: (i) updating only the self-attention projection layers, and (ii)
updating only the MLP Gate&amp;Up while freezing the Down projection. Across models
and tasks, these choices deliver strong target gains while largely preserving
held-out performance. Code is available at
https://github.com/jessemelpolio/LMM_CL</p>
  </article>
</body>
</html>
