{"id": "2510.19954", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19954", "abs": "https://arxiv.org/abs/2510.19954", "authors": ["Joseph Meyer", "Divyansha Lachi", "Reza Mohammadi", "Roshan Reddy Upendra", "Eva L. Dyer", "Mark Li", "Tom Palczewski"], "title": "RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs", "comment": "6 pages", "summary": "Relational multi-table data is common in domains such as e-commerce,\nhealthcare, and scientific research, and can be naturally represented as\nheterogeneous temporal graphs with multi-modal node attributes. Existing graph\nneural networks (GNNs) rely on schema-specific feature encoders, requiring\nseparate modules for each node type and feature column, which hinders\nscalability and parameter sharing. We introduce RELATE (Relational Encoder for\nLatent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature\nencoder that can be used with any general purpose GNN. RELATE employs shared\nmodality-specific encoders for categorical, numerical, textual, and temporal\nattributes, followed by a Perceiver-style cross-attention module that\naggregates features into a fixed-size, permutation-invariant node\nrepresentation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,\nwhere it achieves performance within 3% of schema-specific encoders while\nreducing parameter counts by up to 5x. This design supports varying schemas and\nenables multi-dataset pretraining for general-purpose GNNs, paving the way\ntoward foundation models for relational graph data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RELATE\uff0c\u4e00\u79cd\u6a21\u5f0f\u65e0\u5173\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5171\u4eab\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u5f02\u6784\u65f6\u5e8f\u56fe\u7684\u7edf\u4e00\u8868\u793a\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u5173\u7cfb\u578b\u591a\u8868\u6570\u636e\u65f6\u4f9d\u8d56\u6a21\u5f0f\u7279\u5b9a\u7684\u7279\u5f81\u7f16\u7801\u5668\uff0c\u9700\u8981\u4e3a\u6bcf\u79cd\u8282\u70b9\u7c7b\u578b\u548c\u7279\u5f81\u5217\u8bbe\u8ba1\u72ec\u7acb\u6a21\u5757\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u53c2\u6570\u5171\u4eab\u80fd\u529b\uff0c\u963b\u788d\u4e86\u901a\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u53d1\u5c55\u3002", "method": "RELATE\u91c7\u7528\u5171\u4eab\u7684\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u5904\u7406\u5206\u7c7b\u3001\u6570\u503c\u3001\u6587\u672c\u548c\u65f6\u95f4\u5c5e\u6027\uff0c\u7136\u540e\u901a\u8fc7Perceiver\u98ce\u683c\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u5c06\u7279\u5f81\u805a\u5408\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u7f6e\u6362\u4e0d\u53d8\u8282\u70b9\u8868\u793a\uff0c\u53ef\u4e0e\u4efb\u4f55\u901a\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u914d\u5408\u4f7f\u7528\u3002", "result": "\u5728RelBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRELATE\u4e0eReLGNN\u548cHGT\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u6027\u80fd\u8fbe\u5230\u6a21\u5f0f\u7279\u5b9a\u7f16\u7801\u5668\u768497%\u4ee5\u4e0a\uff0c\u540c\u65f6\u5c06\u53c2\u6570\u6570\u91cf\u51cf\u5c11\u9ad8\u8fbe5\u500d\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "RELATE\u7684\u8bbe\u8ba1\u652f\u6301\u4e0d\u540c\u6a21\u5f0f\u7684\u6570\u636e\u5904\u7406\uff0c\u5e76\u4e3a\u5173\u7cfb\u56fe\u6570\u636e\u7684\u591a\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u4e3a\u5b9e\u73b0\u5173\u7cfb\u56fe\u6570\u636e\u7684\u57fa\u7840\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u63a8\u52a8\u4e86\u901a\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.20310", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20310", "abs": "https://arxiv.org/abs/2510.20310", "authors": ["Mingliang Zhai", "Hansheng Liang", "Xiaomeng Fan", "Zhi Gao", "Chuanhao Li", "Che Sun", "Xu Bin", "Yuwei Wu", "Yunde Jia"], "title": "Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation", "comment": "16 pages, 7 figures, 8 tables", "summary": "Embodied Question Answering (EQA) requires agents to explore 3D environments\nto obtain observations and answer questions related to the scene. Existing\nmethods leverage VLMs to directly explore the environment and answer questions\nwithout explicit thinking or planning, which limits their reasoning ability and\nresults in excessive or inefficient exploration as well as ineffective\nresponses. In this paper, we introduce ToolEQA, an agent that integrates\nexternal tools with multi-step reasoning, where external tools can provide more\nuseful information for completing the task, helping the model derive better\nexploration directions in the next step of reasoning and thus obtaining\nadditional effective information. This enables ToolEQA to generate more\naccurate responses with a shorter exploration distance. To enhance the model's\nability for tool-usage and multi-step reasoning, we further design a novel EQA\ndata generation pipeline that automatically constructs large-scale EQA tasks\nwith reasoning trajectories and corresponding answers. Based on the pipeline,\nwe collect the EQA-RT dataset that contains about 18K tasks, divided into a\ntraining set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping\nwith the training set) and EQA-RT-Unseen (novel scenes). Experiments on\nEQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by\n9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot\nToolEQA by 10% in success rate. In addition, ToolEQA also achieves\nstate-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench\ndatasets, demonstrating its generality. Our homepage see\nhttps://tooleqa.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ToolEQA\uff0c\u4e00\u79cd\u5c06\u5916\u90e8\u5de5\u5177\u4e0e\u591a\u6b65\u63a8\u7406\u76f8\u7ed3\u5408\u7684\u5177\u8eab\u95ee\u7b54\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5de5\u5177\u4f7f\u7528\u83b7\u53d6\u989d\u5916\u6709\u6548\u4fe1\u606f\u6765\u6539\u5584\u63a2\u7d22\u65b9\u5411\uff0c\u4ece\u800c\u5728\u66f4\u77ed\u63a2\u7d22\u8ddd\u79bb\u5185\u751f\u6210\u66f4\u51c6\u786e\u56de\u7b54\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6210\u529f\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53479.2-20.2%\u3002", "motivation": "\u73b0\u6709\u5177\u8eab\u95ee\u7b54\u65b9\u6cd5\u76f4\u63a5\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a2\u7d22\u73af\u5883\u5e76\u56de\u7b54\u95ee\u9898\uff0c\u7f3a\u4e4f\u663e\u5f0f\u601d\u8003\u548c\u89c4\u5212\uff0c\u8fd9\u9650\u5236\u4e86\u63a8\u7406\u80fd\u529b\u5e76\u5bfc\u81f4\u8fc7\u5ea6\u6216\u4f4e\u6548\u63a2\u7d22\u4ee5\u53ca\u65e0\u6548\u54cd\u5e94\u3002", "method": "ToolEQA\u901a\u8fc7\u96c6\u6210\u5916\u90e8\u5de5\u5177\u4e0e\u591a\u6b65\u63a8\u7406\uff0c\u4f7f\u5916\u90e8\u5de5\u5177\u80fd\u591f\u4e3a\u4efb\u52a1\u5b8c\u6210\u63d0\u4f9b\u66f4\u6709\u7528\u4fe1\u606f\uff0c\u5e2e\u52a9\u6a21\u578b\u5728\u4e0b\u4e00\u6b65\u63a8\u7406\u4e2d\u63a8\u5bfc\u66f4\u597d\u7684\u63a2\u7d22\u65b9\u5411\uff0c\u4ece\u800c\u83b7\u5f97\u989d\u5916\u6709\u6548\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684EQA\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u81ea\u52a8\u6784\u5efa\u5177\u6709\u63a8\u7406\u8f68\u8ff9\u548c\u76f8\u5e94\u7b54\u6848\u7684\u5927\u89c4\u6a21EQA\u4efb\u52a1\uff0c\u5e76\u57fa\u4e8e\u6b64\u6536\u96c6\u4e86\u5305\u542b\u7ea618K\u4efb\u52a1\u7684EQA-RT\u6570\u636e\u96c6\u3002", "result": "\u5728EQA-RT-Seen\u548cEQA-RT-Unseen\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cToolEQA\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u53479.2-20.2%\uff0c\u540c\u65f6\u6bd4\u96f6\u6837\u672cToolEQA\u6210\u529f\u7387\u9ad8\u51fa10%\u3002\u6b64\u5916\uff0cToolEQA\u5728HM-EQA\u3001OpenEQA\u548cEXPRESS-Bench\u6570\u636e\u96c6\u4e0a\u4e5f\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ToolEQA\u901a\u8fc7\u5de5\u5177\u4f7f\u7528\u548c\u591a\u6b65\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u5177\u8eab\u95ee\u7b54\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u5916\u90e8\u5de5\u5177\u96c6\u6210\u5728\u589e\u5f3a\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u5177\u8eab\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u8f68\u8ff9\u5728\u590d\u6742\u73af\u5883\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.20345", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20345", "abs": "https://arxiv.org/abs/2510.20345", "authors": ["Haonan Bian"], "title": "LLM-empowered knowledge graph construction: A survey", "comment": null, "summary": "Knowledge Graphs (KGs) have long served as a fundamental infrastructure for\nstructured knowledge representation and reasoning. With the advent of Large\nLanguage Models (LLMs), the construction of KGs has entered a new\nparadigm-shifting from rule-based and statistical pipelines to language-driven\nand generative frameworks. This survey provides a comprehensive overview of\nrecent progress in LLM-empowered knowledge graph construction, systematically\nanalyzing how LLMs reshape the classical three-layered pipeline of ontology\nengineering, knowledge extraction, and knowledge fusion.\n  We first revisit traditional KG methodologies to establish conceptual\nfoundations, and then review emerging LLM-driven approaches from two\ncomplementary perspectives: schema-based paradigms, which emphasize structure,\nnormalization, and consistency; and schema-free paradigms, which highlight\nflexibility, adaptability, and open discovery. Across each stage, we synthesize\nrepresentative frameworks, analyze their technical mechanisms, and identify\ntheir limitations.\n  Finally, the survey outlines key trends and future research directions,\nincluding KG-based reasoning for LLMs, dynamic knowledge memory for agentic\nsystems, and multimodal KG construction. Through this systematic review, we aim\nto clarify the evolving interplay between LLMs and knowledge graphs, bridging\nsymbolic knowledge engineering and neural semantic understanding toward the\ndevelopment of adaptive, explainable, and intelligent knowledge systems.", "AI": {"tldr": "\u672c\u8c03\u67e5\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d4b\u80fd\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5206\u6790\u4e86LLM\u5982\u4f55\u91cd\u5851\u4f20\u7edf\u7684\u672c\u4f53\u5de5\u7a0b\u3001\u77e5\u8bc6\u62bd\u53d6\u548c\u77e5\u8bc6\u878d\u5408\u4e09\u5c42\u6d41\u6c34\u7ebf\uff0c\u4e3a\u7b26\u53f7\u5316\u77e5\u8bc6\u5de5\u7a0b\u4e0e\u795e\u7ecf\u8bed\u4e49\u7406\u89e3\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6b63\u7ecf\u5386\u4ece\u57fa\u4e8e\u89c4\u5219\u548c\u7edf\u8ba1\u7684\u6d41\u6c34\u7ebf\u5411\u8bed\u8a00\u9a71\u52a8\u548c\u751f\u6210\u6846\u67b6\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406LLM\u5982\u4f55\u91cd\u5851\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7684\u4e09\u5c42\u6d41\u6c34\u7ebf\uff0c\u5e76\u6f84\u6e05LLM\u4e0e\u77e5\u8bc6\u56fe\u8c31\u4e4b\u95f4\u4e0d\u65ad\u6f14\u53d8\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u8c03\u67e5\u4ece\u4e24\u4e2a\u4e92\u8865\u89c6\u89d2\u56de\u987e\u65b0\u5174\u7684LLM\u9a71\u52a8\u65b9\u6cd5\uff1a\u5f3a\u8c03\u7ed3\u6784\u3001\u89c4\u8303\u5316\u548c\u4e00\u81f4\u6027\u7684\u57fa\u4e8e\u6a21\u5f0f\u7684\u8303\u5f0f\uff0c\u4ee5\u53ca\u5f3a\u8c03\u7075\u6d3b\u6027\u3001\u9002\u5e94\u6027\u548c\u5f00\u653e\u53d1\u73b0\u7684\u514d\u6a21\u5f0f\u8303\u5f0f\uff0c\u5e76\u5728\u6bcf\u4e2a\u9636\u6bb5\u7efc\u5408\u4ee3\u8868\u6027\u6846\u67b6\u5e76\u5206\u6790\u5176\u6280\u672f\u673a\u5236\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790LLM\u8d4b\u80fd\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7684\u8fdb\u5c55\uff0c\u8c03\u67e5\u8bc6\u522b\u4e86\u5404\u9636\u6bb5\u4ee3\u8868\u6027\u6846\u67b6\u7684\u6280\u672f\u673a\u5236\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u7406\u89e3LLM\u5982\u4f55\u6539\u53d8\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u65b9\u6cd5\u8bba\u63d0\u4f9b\u4e86\u5168\u9762\u89c6\u89d2\u3002", "conclusion": "\u8be5\u8c03\u67e5\u4e3a\u5f00\u53d1\u81ea\u9002\u5e94\u3001\u53ef\u89e3\u91ca\u548c\u667a\u80fd\u77e5\u8bc6\u7cfb\u7edf\u6307\u660e\u4e86\u5173\u952e\u8d8b\u52bf\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684LLM\u63a8\u7406\u3001\u9762\u5411\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u52a8\u6001\u77e5\u8bc6\u8bb0\u5fc6\u4ee5\u53ca\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\uff0c\u6709\u6548\u6865\u63a5\u4e86\u7b26\u53f7\u5316\u77e5\u8bc6\u5de5\u7a0b\u4e0e\u795e\u7ecf\u8bed\u4e49\u7406\u89e3\u3002"}}
{"id": "2510.20632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20632", "abs": "https://arxiv.org/abs/2510.20632", "authors": ["Shuyi Xie", "Ziqin Liew", "Hailing Zhang", "Haibo Zhang", "Ling Hu", "Zhiqiang Zhou", "Shuman Liu", "Anxiang Zeng"], "title": "Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications", "comment": null, "summary": "Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet\ntheir capabilities in specialized domains remain underexplored. In e-commerce,\nexisting evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping\nMMLU-suffer from limited task diversity (e.g., lacking product guidance and\nafter-sales issues), limited task modalities (e.g., absence of multimodal\ndata), synthetic or curated data, and a narrow focus on English and Chinese,\nleaving practitioners without reliable tools to assess models on complex,\nreal-world shopping scenarios. We introduce EcomEval, a comprehensive\nmultilingual and multimodal benchmark for evaluating LLMs in e-commerce.\nEcomEval covers six categories and 37 tasks (including 8 multimodal tasks),\nsourced primarily from authentic customer queries and transaction logs,\nreflecting the noisy and heterogeneous nature of real business interactions. To\nensure both quality and scalability of reference answers, we adopt a\nsemi-automatic pipeline in which large models draft candidate responses\nsubsequently reviewed and modified by over 50 expert annotators with strong\ne-commerce and multilingual expertise. We define difficulty levels for each\nquestion and task category by averaging evaluation scores across models with\ndifferent sizes and capabilities, enabling challenge-oriented and fine-grained\nassessment. EcomEval also spans seven languages-including five low-resource\nSoutheast Asian languages-offering a multilingual perspective absent from prior\nwork.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EcomEval\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u591a\u8bed\u8a00\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7535\u5b50\u5546\u52a1\u9886\u57df\u7684\u6027\u80fd\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u5728\u4efb\u52a1\u591a\u6837\u6027\u3001\u6a21\u6001\u8986\u76d6\u548c\u8bed\u8a00\u8303\u56f4\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7535\u5b50\u5546\u52a1\u8bc4\u4f30\u57fa\u51c6\u5982EcomInstruct\u3001ChineseEcomQA\u7b49\u5b58\u5728\u4efb\u52a1\u591a\u6837\u6027\u4e0d\u8db3\uff08\u7f3a\u5c11\u4ea7\u54c1\u6307\u5bfc\u548c\u552e\u540e\u95ee\u9898\uff09\u3001\u6a21\u6001\u8986\u76d6\u6709\u9650\uff08\u7f3a\u4e4f\u591a\u6a21\u6001\u6570\u636e\uff09\u3001\u6570\u636e\u5408\u6210\u6216\u4eba\u5de5\u6574\u7406\u3001\u4ee5\u53ca\u8bed\u8a00\u8986\u76d6\u72ed\u7a84\uff08\u4ec5\u9650\u82f1\u8bed\u548c\u4e2d\u6587\uff09\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u53ef\u9760\u5de5\u5177\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u590d\u6742\u771f\u5b9e\u8d2d\u7269\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4e86\u6db5\u76d66\u4e2a\u7c7b\u522b\u548c37\u4e2a\u4efb\u52a1\uff08\u5305\u62ec8\u4e2a\u591a\u6a21\u6001\u4efb\u52a1\uff09\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u4e3b\u8981\u6765\u6e90\u4e8e\u771f\u5b9e\u5ba2\u6237\u67e5\u8be2\u548c\u4ea4\u6613\u65e5\u5fd7\uff0c\u91c7\u7528\u534a\u81ea\u52a8\u6d41\u7a0b\u7531\u5927\u6a21\u578b\u751f\u6210\u5019\u9009\u56de\u7b54\u5e76\u753150\u591a\u540d\u7535\u5b50\u5546\u52a1\u548c\u591a\u8bed\u8a00\u4e13\u5bb6\u5ba1\u6838\u4fee\u6539\uff0c\u4e3a\u6bcf\u4e2a\u95ee\u9898\u548c\u4efb\u52a1\u7c7b\u522b\u5b9a\u4e49\u96be\u5ea6\u7ea7\u522b\uff0c\u5e76\u8986\u76d6\u5305\u62ec5\u79cd\u4e1c\u5357\u4e9a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u5185\u76847\u79cd\u8bed\u8a00\u3002", "result": "EcomEval\u57fa\u51c6\u53cd\u6620\u4e86\u771f\u5b9e\u5546\u4e1a\u4ea4\u4e92\u7684\u566a\u58f0\u548c\u5f02\u6784\u7279\u6027\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u89c4\u6a21\u548c\u80fd\u529b\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u5206\u6570\u5e73\u5747\u6765\u5b9a\u4e49\u96be\u5ea6\u7ea7\u522b\uff0c\u5b9e\u73b0\u4e86\u9762\u5411\u6311\u6218\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u4e3a\u7535\u5b50\u5546\u52a1\u9886\u57df\u7684\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u591a\u8bed\u8a00\u591a\u6a21\u6001\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7535\u5b50\u5546\u52a1\u9886\u57df\u7684LLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u7684\u591a\u8bed\u8a00\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u590d\u6742\u5546\u4e1a\u573a\u666f\u4e2d\u7684\u80fd\u529b\u5efa\u7acb\u4e86\u65b0\u7684\u6807\u51c6\uff0c\u7279\u522b\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u652f\u6301\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.19955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19955", "abs": "https://arxiv.org/abs/2510.19955", "authors": ["M\u00e1rcus Vin\u00edcius Lobo Costa", "Sherlon Almeida da Silva", "B\u00e1rbara Caroline Benato", "Leo Sampaio Ferraz Ribeiro", "Moacir Antonelli Ponti"], "title": "Transformed Multi-view 3D Shape Features with Contrastive Learning", "comment": null, "summary": "This paper addresses the challenges in representation learning of 3D shape\nfeatures by investigating state-of-the-art backbones paired with both\ncontrastive supervised and self-supervised learning objectives. Computer vision\nmethods struggle with recognizing 3D objects from 2D images, often requiring\nextensive labeled data and relying on Convolutional Neural Networks (CNNs) that\nmay overlook crucial shape relationships. Our work demonstrates that Vision\nTransformers (ViTs) based architectures, when paired with modern contrastive\nobjectives, achieve promising results in multi-view 3D analysis on our\ndownstream tasks, unifying contrastive and 3D shape understanding pipelines.\nFor example, supervised contrastive losses reached about 90.6% accuracy on\nModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' ability\nto understand overall shapes and contrastive learning's effectiveness,\novercomes the need for extensive labeled data and the limitations of CNNs in\ncapturing crucial shape relationships. The success stems from capturing global\nshape semantics via ViTs and refining local discriminative features through\ncontrastive optimization. Importantly, our approach is empirical, as it is\ngrounded on extensive experimental evaluation to validate the effectiveness of\ncombining ViTs with contrastive objectives for 3D representation learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5c06\u89c6\u89c9Transformer\u67b6\u6784\u4e0e\u73b0\u4ee3\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4e863D\u5f62\u72b6\u7279\u5f81\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5728ModelNet10\u4e0a\u8fbe\u523090.6%\u7684\u51c6\u786e\u7387\uff0c\u7edf\u4e00\u4e86\u5bf9\u6bd4\u5b66\u4e60\u548c3D\u5f62\u72b6\u7406\u89e3\u6d41\u7a0b\u3002", "motivation": "\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u5728\u4ece2D\u56fe\u50cf\u8bc6\u522b3D\u7269\u4f53\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u4f9d\u8d56\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u8fd9\u53ef\u80fd\u5ffd\u7565\u5173\u952e\u7684\u5f62\u72b6\u5173\u7cfb\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u63a2\u7d22\u66f4\u6709\u6548\u76843D\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u89c6\u89c9Transformer\u67b6\u6784\u4e0e\u73b0\u4ee3\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u6709\u76d1\u7763\u548c\u65e0\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u5229\u7528ViT\u7406\u89e3\u6574\u4f53\u5f62\u72b6\u7684\u80fd\u529b\u548c\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u5c40\u90e8\u5224\u522b\u7279\u5f81\u7684\u4f18\u52bf\u3002", "result": "\u5728ModelNet10\u6570\u636e\u96c6\u4e0a\uff0c\u6709\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u8fbe\u5230\u4e86\u7ea690.6%\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86ViT\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u7ed3\u5408\u5728\u591a\u89c6\u89d23D\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "ViT\u901a\u8fc7\u6355\u6349\u5168\u5c40\u5f62\u72b6\u8bed\u4e49\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u5c40\u90e8\u7279\u5f81\u7684\u7ed3\u5408\uff0c\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edfCNN\u5728\u6355\u83b7\u5f62\u72b6\u5173\u7cfb\u65b9\u9762\u7684\u9650\u5236\uff0c\u4e3a3D\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u8bc1\u57fa\u7840\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.19871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19871", "abs": "https://arxiv.org/abs/2510.19871", "authors": ["Yatai Ji", "Teng Wang", "Yuying Ge", "Zhiheng Liu", "Sidi Yang", "Ying Shan", "Ping Luo"], "title": "From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model", "comment": null, "summary": "Discrete diffusion models have emerged as a promising direction for\nvision-language tasks, offering bidirectional context modeling and theoretical\nparallelization. However, their practical application is severely hindered by a\ntrain-inference discrepancy, which leads to catastrophic error cascades:\ninitial token errors during parallel decoding pollute the generation context,\ntriggering a chain reaction of compounding errors and leading to syntactic\nerrors and semantic hallucinations. To address this fundamental challenge, we\nreframe the generation process from passive denoising to active refining. We\nintroduce ReDiff, a refining-enhanced diffusion framework that teaches the\nmodel to identify and correct its own errors. Our approach features a two-stage\ntraining process: first, we instill a foundational revision capability by\ntraining the model to revise synthetic errors; second, we implement a novel\nonline self-correction loop where the model is explicitly trained to revise its\nown flawed drafts by learning from an expert's corrections. This mistake-driven\nlearning endows the model with the crucial ability to revisit and refine its\nalready generated output, effectively breaking the error cascade. Extensive\nexperiments demonstrate that ReDiff significantly improves the coherence and\nfactual accuracy of generated content, enabling stable and efficient parallel\ngeneration far superior to traditional denoising methods. Our codes and models\nare available at https://rediff-hku.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReDiff\u6846\u67b6\uff0c\u5c06\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\u4ece\u88ab\u52a8\u53bb\u566a\u91cd\u6784\u4e3a\u4e3b\u52a8\u7cbe\u70bc\uff0c\u901a\u8fc7\u6559\u5bfc\u6a21\u578b\u8bc6\u522b\u548c\u4fee\u6b63\u81ea\u8eab\u9519\u8bef\u6765\u89e3\u51b3\u5e76\u884c\u89e3\u7801\u4e2d\u7684\u9519\u8bef\u7ea7\u8054\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5185\u5bb9\u7684\u8fde\u8d2f\u6027\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u8bad\u7ec3-\u63a8\u7406\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u884c\u89e3\u7801\u8fc7\u7a0b\u4e2d\u7684\u521d\u59cb\u4ee4\u724c\u9519\u8bef\u4f1a\u6c61\u67d3\u751f\u6210\u4e0a\u4e0b\u6587\uff0c\u5f15\u53d1\u9519\u8bef\u7ea7\u8054\u6548\u5e94\uff0c\u5bfc\u81f4\u8bed\u6cd5\u9519\u8bef\u548c\u8bed\u4e49\u5e7b\u89c9\uff0c\u8fd9\u4e25\u91cd\u963b\u788d\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "ReDiff\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\uff1a\u9996\u5148\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u4fee\u6b63\u5408\u6210\u9519\u8bef\u6765\u5efa\u7acb\u57fa\u7840\u4fee\u8ba2\u80fd\u529b\uff0c\u7136\u540e\u5b9e\u73b0\u65b0\u9896\u7684\u5728\u7ebf\u81ea\u6821\u6b63\u5faa\u73af\uff0c\u6a21\u578b\u901a\u8fc7\u4ece\u4e13\u5bb6\u4fee\u6b63\u4e2d\u5b66\u4e60\u6765\u663e\u5f0f\u8bad\u7ec3\u4fee\u6b63\u81ea\u8eab\u6709\u7f3a\u9677\u7684\u8349\u7a3f\uff0c\u8fd9\u79cd\u9519\u8bef\u9a71\u52a8\u5b66\u4e60\u8d4b\u4e88\u6a21\u578b\u91cd\u65b0\u5ba1\u89c6\u548c\u7cbe\u70bc\u5df2\u751f\u6210\u8f93\u51fa\u7684\u5173\u952e\u80fd\u529b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eReDiff\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5185\u5bb9\u7684\u8fde\u8d2f\u6027\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u4e86\u8fdc\u4f18\u4e8e\u4f20\u7edf\u53bb\u566a\u65b9\u6cd5\u7684\u7a33\u5b9a\u9ad8\u6548\u5e76\u884c\u751f\u6210\uff0c\u6709\u6548\u6253\u7834\u4e86\u9519\u8bef\u7ea7\u8054\u6548\u5e94\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u751f\u6210\u8fc7\u7a0b\u4ece\u88ab\u52a8\u53bb\u566a\u8f6c\u5411\u4e3b\u52a8\u7cbe\u70bc\u7684\u6709\u6548\u6027\uff0c\u9519\u8bef\u9a71\u52a8\u5b66\u4e60\u65b9\u6cd5\u4e3a\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4e3a\u7a33\u5b9a\u9ad8\u6548\u7684\u5e76\u884c\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.19892", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19892", "abs": "https://arxiv.org/abs/2510.19892", "authors": ["Nishant Balepur", "Dang Nguyen", "Dayeon Ki"], "title": "Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities", "comment": "Accepted as a Spotlight paper at the EMNLP 2025 Wordplay Workshop", "summary": "Multi-modal large language models (MLMs) are often assessed on static,\nindividual benchmarks -- which cannot jointly assess MLM capabilities in a\nsingle task -- or rely on human or model pairwise comparisons -- which is\nhighly subjective, expensive, and allows models to exploit superficial\nshortcuts (e.g., verbosity) to inflate their win-rates. To overcome these\nissues, we propose game-based evaluations to holistically assess MLM\ncapabilities. Games require multiple abilities for players to win, are\ninherently competitive, and are governed by fix, objective rules, and makes\nevaluation more engaging, providing a robust framework to address the\naforementioned challenges. We manifest this evaluation specifically through\nDixit, a fantasy card game where players must generate captions for a card that\ntrick some, but not all players, into selecting the played card. Our\nquantitative experiments with five MLMs show Dixit win-rate rankings are\nperfectly correlated with those on popular MLM benchmarks, while games between\nhuman and MLM players in Dixit reveal several differences between agent\nstrategies and areas of improvement for MLM reasoning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u6e38\u620f\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7Dixit\u5e7b\u60f3\u5361\u724c\u6e38\u620f\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u6d4b\u8bd5\u591a\u79cd\u80fd\u529b\uff0c\u63d0\u4f9b\u5ba2\u89c2\u4e14\u5177\u5438\u5f15\u529b\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u6216\u4e3b\u89c2\u7684\u4eba\u5de5\u6bd4\u8f83\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3001\u6210\u672c\u9ad8\u6602\u4e14\u5bb9\u6613\u88ab\u6a21\u578b\u5229\u7528\u8868\u9762\u7279\u5f81\uff08\u5982\u5197\u957f\u6027\uff09\u6765\u865a\u589e\u80dc\u7387\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6e38\u620f\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5177\u4f53\u5b9e\u73b0\u4e3aDixit\u5e7b\u60f3\u5361\u724c\u6e38\u620f\uff0c\u8981\u6c42\u73a9\u5bb6\u4e3a\u5361\u724c\u751f\u6210\u80fd\u591f\u6b3a\u9a97\u90e8\u5206\u800c\u975e\u5168\u90e8\u73a9\u5bb6\u7684\u63cf\u8ff0\uff0c\u4ece\u800c\u540c\u65f6\u6d4b\u8bd5\u6a21\u578b\u7684\u591a\u79cd\u63a8\u7406\u80fd\u529b\u3002", "result": "\u4e94\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728Dixit\u6e38\u620f\u4e2d\u7684\u80dc\u7387\u6392\u540d\u4e0e\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u5b8c\u5168\u4e00\u81f4\uff0c\u540c\u65f6\u4eba\u673a\u5bf9\u6218\u63ed\u793a\u4e86\u6a21\u578b\u7b56\u7565\u4e0e\u4eba\u7c7b\u7b56\u7565\u7684\u5dee\u5f02\u4ee5\u53ca\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u6e38\u620f\u5316\u8bc4\u4f30\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u5ba2\u89c2\u4e14\u5177\u5438\u5f15\u529b\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u63ed\u793a\u6a21\u578b\u5728\u771f\u5b9e\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u80fd\u529b\u5c40\u9650\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.19981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19981", "abs": "https://arxiv.org/abs/2510.19981", "authors": ["Martha Teiko Teye", "Ori Maoz", "Matthias Rottmann"], "title": "FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking", "comment": null, "summary": "We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework\nthat builds on existing 3D detectors by introducing a transformer-based\nsmoother and a fusion-driven tracker. Inspired by query-based tracking\nframeworks, FutrTrack employs a multimodal two-stage transformer refinement and\ntracking pipeline. Our fusion tracker integrates bounding boxes with multimodal\nbird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without\nthe need for an explicit motion model. The tracker assigns and propagates\nidentities across frames, leveraging both geometric and semantic cues for\nrobust re-identification under occlusion and viewpoint changes. Prior to\ntracking, we refine sequences of bounding boxes with a temporal smoother over a\nmoving window to refine trajectories, reduce jitter, and improve spatial\nconsistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that\nquery-based transformer tracking methods benefit significantly from multimodal\nsensor features compared with previous single-sensor approaches. With an aMOTA\nof 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D\nMOT benchmarks, reducing identity switches while maintaining competitive\naccuracy. Our approach provides an efficient framework for improving\ntransformer-based trackers to compete with other neural-network-based methods\neven with limited data and without pretraining.", "AI": {"tldr": "FutrTrack\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u76f8\u673a-LiDAR\u591a\u76ee\u6807\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8eTransformer\u7684\u5e73\u6ed1\u5668\u548c\u878d\u5408\u9a71\u52a8\u7684\u8ddf\u8e2a\u5668\uff0c\u5728nuScenes\u548cKITTI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8674.7 aMOTA\u7684\u5f3a\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8eab\u4efd\u5207\u6362\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u67093D\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u5728\u906e\u6321\u548c\u89c6\u89d2\u53d8\u5316\u4e0b\u8eab\u4efd\u91cd\u8bc6\u522b\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5355\u4f20\u611f\u5668\u65b9\u6cd5\u5728\u7279\u5f81\u8868\u793a\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u5982\u4f55\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u4f20\u611f\u5668\u7279\u5f81\u6765\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd\u3002", "method": "FutrTrack\u91c7\u7528\u57fa\u4e8e\u67e5\u8be2\u7684\u8ddf\u8e2a\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u591a\u6a21\u6001\u4e24\u9636\u6bb5Transformer\u7cbe\u70bc\u548c\u8ddf\u8e2a\u6d41\u6c34\u7ebf\uff0c\u5305\u62ec\u57fa\u4e8e\u79fb\u52a8\u7a97\u53e3\u7684\u65f6\u95f4\u5e73\u6ed1\u5668\u6765\u4f18\u5316\u8f68\u8ff9\u548c\u51cf\u5c11\u6296\u52a8\uff0c\u4ee5\u53ca\u878d\u5408\u8ddf\u8e2a\u5668\u96c6\u6210\u8fb9\u754c\u6846\u4e0e\u591a\u6a21\u6001BEV\u878d\u5408\u7279\u5f81\uff0c\u65e0\u9700\u663e\u5f0f\u8fd0\u52a8\u6a21\u578b\u5373\u53ef\u8de8\u5e27\u5206\u914d\u548c\u4f20\u64ad\u8eab\u4efd\u3002", "result": "\u5728nuScenes\u548cKITTI\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cFutrTrack\u5728nuScenes\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523074.7 aMOTA\uff0c\u57283D MOT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8eab\u4efd\u5207\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u4f20\u611f\u5668\u7279\u5f81\u76f8\u6bd4\u5355\u4f20\u611f\u5668\u65b9\u6cd5\u7684\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u57fa\u4e8e\u67e5\u8be2\u7684Transformer\u8ddf\u8e2a\u65b9\u6cd5\u80fd\u591f\u4ece\u591a\u6a21\u6001\u4f20\u611f\u5668\u7279\u5f81\u4e2d\u663e\u8457\u83b7\u76ca\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u6846\u67b6\u6765\u6539\u8fdb\u57fa\u4e8eTransformer\u7684\u8ddf\u8e2a\u5668\uff0c\u4f7f\u5176\u5373\u4f7f\u5728\u6709\u9650\u6570\u636e\u548c\u65e0\u9700\u9884\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4e0e\u5176\u4ed6\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u7ade\u4e89\u3002"}}
{"id": "2510.20093", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20093", "abs": "https://arxiv.org/abs/2510.20093", "authors": ["Jiho Park", "Sieun Choi", "Jaeyoon Seo", "Jihie Kim"], "title": "StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback", "comment": "Under review at IEEE Access. Author-submitted preprint. Not the\n  IEEE-published version", "summary": "Although recent advancements in diffusion models have significantly enriched\nthe quality of generated images, challenges remain in synthesizing pixel-based\nhuman-drawn sketches, a representative example of abstract expression. To\ncombat these challenges, we propose StableSketcher, a novel framework that\nempowers diffusion models to generate hand-drawn sketches with high prompt\nfidelity. Within this framework, we fine-tune the variational autoencoder to\noptimize latent decoding, enabling it to better capture the characteristics of\nsketches. In parallel, we integrate a new reward function for reinforcement\nlearning based on visual question answering, which improves text-image\nalignment and semantic consistency. Extensive experiments demonstrate that\nStableSketcher generates sketches with improved stylistic fidelity, achieving\nbetter alignment with prompts compared to the Stable Diffusion baseline.\nAdditionally, we introduce SketchDUO, to the best of our knowledge, the first\ndataset comprising instance-level sketches paired with captions and\nquestion-answer pairs, thereby addressing the limitations of existing datasets\nthat rely on image-label pairs. Our code and dataset will be made publicly\navailable upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86StableSketcher\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u89e3\u7801\u548c\u96c6\u6210\u57fa\u4e8e\u89c6\u89c9\u95ee\u7b54\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u751f\u6210\u624b\u7ed8\u8349\u56fe\u7684\u8d28\u91cf\u548c\u6587\u672c\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u751f\u6210\u57fa\u4e8e\u50cf\u7d20\u7684\u624b\u7ed8\u8349\u56fe\uff08\u62bd\u8c61\u8868\u8fbe\u7684\u4ee3\u8868\u6027\u793a\u4f8b\uff09\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u6355\u6349\u8349\u56fe\u7684\u98ce\u683c\u7279\u5f81\u5e76\u786e\u4fdd\u6587\u672c-\u56fe\u50cf\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u9996\u5148\u5bf9\u53d8\u5206\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u5fae\u8c03\u4ee5\u4f18\u5316\u6f5c\u5728\u89e3\u7801\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u6355\u6349\u8349\u56fe\u7279\u5f81\uff1b\u5176\u6b21\u96c6\u6210\u57fa\u4e8e\u89c6\u89c9\u95ee\u7b54\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\uff0c\u4e13\u95e8\u7528\u4e8e\u63d0\u5347\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cStableSketcher\u751f\u6210\u7684\u8349\u56fe\u5728\u98ce\u683c\u4fdd\u771f\u5ea6\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u4e0e\u63d0\u793a\u8bcd\u7684\u5bf9\u9f50\u6548\u679c\u4f18\u4e8eStable Diffusion\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u9996\u4e2a\u5305\u542b\u5b9e\u4f8b\u7ea7\u8349\u56fe\u4e0e\u6807\u9898\u53ca\u95ee\u7b54\u5bf9\u7684\u6570\u636e\u96c6SketchDUO\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63d0\u51fa\u4e86\u6709\u6548\u7684\u8349\u56fe\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u8fd8\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4f9d\u8d56\u56fe\u50cf-\u6807\u7b7e\u5bf9\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u62bd\u8c61\u827a\u672f\u8868\u8fbe\u7684\u751f\u6210\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8d44\u6e90\u548c\u65b9\u5411\u6307\u5f15\u3002"}}
{"id": "2510.20042", "categories": ["cs.CV", "I.2.10; I.2.6; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.20042", "abs": "https://arxiv.org/abs/2510.20042", "authors": ["Huichan Seo", "Sieun Choi", "Minki Hong", "Yi Zhou", "Junseo Kim", "Lukman Ismaila", "Naome Etori", "Mehul Agarwal", "Zhixuan Liu", "Jihie Kim", "Jean Oh"], "title": "Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models", "comment": "28 pages, 8 figures. Submitted to the Second Conference of the\n  International Association for Safe and Ethical Artificial Intelligence\n  (IASEAI '26)", "summary": "Generative image models produce striking visuals yet often misrepresent\nculture. Prior work has examined cultural bias mainly in text-to-image (T2I)\nsystems, leaving image-to-image (I2I) editors underexplored. We bridge this gap\nwith a unified evaluation across six countries, an 8-category/36-subcategory\nschema, and era-aware prompts, auditing both T2I generation and I2I editing\nunder a standardized protocol that yields comparable diagnostics. Using open\nmodels with fixed settings, we derive cross-country, cross-era, and\ncross-category evaluations. Our framework combines standard automatic metrics,\na culture-aware retrieval-augmented VQA, and expert human judgments collected\nfrom native reviewers. To enable reproducibility, we release the complete image\ncorpus, prompts, and configurations. Our study reveals three findings: (1)\nunder country-agnostic prompts, models default to Global-North, modern-leaning\ndepictions that flatten cross-country distinctions; (2) iterative I2I editing\nerodes cultural fidelity even when conventional metrics remain flat or improve;\nand (3) I2I models apply superficial cues (palette shifts, generic props)\nrather than era-consistent, context-aware changes, often retaining source\nidentity for Global-South targets. These results highlight that\nculture-sensitive edits remain unreliable in current systems. By releasing\nstandardized data, prompts, and human evaluation protocols, we provide a\nreproducible, culture-centered benchmark for diagnosing and tracking cultural\nbias in generative image models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u6846\u67b6\u6765\u8bc4\u4f30\u751f\u6210\u5f0f\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u6587\u5316\u504f\u89c1\uff0c\u901a\u8fc7\u8de8\u56fd\u5bb6\u3001\u8de8\u65f6\u4ee3\u548c\u8de8\u7c7b\u522b\u7684\u7edf\u4e00\u8bc4\u4f30\u63ed\u793a\u4e86T2I\u751f\u6210\u548cI2I\u7f16\u8f91\u4e2d\u7684\u6587\u5316\u5931\u771f\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86\u53ef\u590d\u73b0\u7684\u6587\u5316\u4e2d\u5fc3\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u7684\u6587\u5316\u504f\u89c1\uff0c\u800c\u56fe\u50cf\u5230\u56fe\u50cf\u7f16\u8f91\u5668\u7684\u6587\u5316\u504f\u5dee\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u534f\u8bae\u5bf9T2I\u751f\u6210\u548cI2I\u7f16\u8f91\u8fdb\u884c\u53ef\u6bd4\u6027\u8bca\u65ad\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u8986\u76d6\u516d\u4e2a\u56fd\u5bb6\uff0c\u6784\u5efa\u5305\u542b8\u4e2a\u7c7b\u522b\u548c36\u4e2a\u5b50\u7c7b\u522b\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u7ed3\u5408\u65f6\u4ee3\u611f\u77e5\u63d0\u793a\u8bcd\uff0c\u4f7f\u7528\u56fa\u5b9a\u8bbe\u7f6e\u7684\u5f00\u653e\u6a21\u578b\u8fdb\u884c\u8de8\u56fd\u5bb6\u3001\u8de8\u65f6\u4ee3\u548c\u8de8\u7c7b\u522b\u8bc4\u4f30\uff0c\u6574\u5408\u6807\u51c6\u81ea\u52a8\u6307\u6807\u3001\u6587\u5316\u611f\u77e5\u68c0\u7d22\u589e\u5f3aVQA\u548c\u672c\u5730\u8bc4\u5ba1\u4e13\u5bb6\u7684\u4e13\u4e1a\u4eba\u5de5\u5224\u65ad\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u5728\u56fd\u5bb6\u65e0\u5173\u63d0\u793a\u4e0b\u6a21\u578b\u9ed8\u8ba4\u751f\u6210\u504f\u5411\u5168\u7403\u5317\u65b9\u548c\u73b0\u4ee3\u98ce\u683c\u7684\u63cf\u7ed8\uff0c\u62b9\u5e73\u4e86\u8de8\u56fd\u5dee\u5f02\uff1b\u8fed\u4ee3\u5f0fI2I\u7f16\u8f91\u4f1a\u4fb5\u8680\u6587\u5316\u4fdd\u771f\u5ea6\uff0c\u5373\u4f7f\u4f20\u7edf\u6307\u6807\u4fdd\u6301\u7a33\u5b9a\u6216\u6539\u5584\uff1bI2I\u6a21\u578b\u4ec5\u5e94\u7528\u8868\u9762\u7ebf\u7d22\u800c\u975e\u65f6\u4ee3\u4e00\u81f4\u7684\u60c5\u5883\u611f\u77e5\u53d8\u5316\uff0c\u5bf9\u5168\u7403\u5357\u65b9\u76ee\u6807\u5e38\u4fdd\u7559\u6e90\u8eab\u4efd\u7279\u5f81\u3002", "conclusion": "\u5f53\u524d\u7cfb\u7edf\u4e2d\u7684\u6587\u5316\u654f\u611f\u7f16\u8f91\u4ecd\u7136\u4e0d\u53ef\u9760\uff0c\u7814\u7a76\u901a\u8fc7\u53d1\u5e03\u6807\u51c6\u5316\u6570\u636e\u3001\u63d0\u793a\u8bcd\u548c\u4eba\u5de5\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3a\u8bca\u65ad\u548c\u8ffd\u8e2a\u751f\u6210\u5f0f\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u6587\u5316\u504f\u89c1\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u6587\u5316\u4e2d\u5fc3\u57fa\u51c6\uff0c\u5f3a\u8c03\u4e86\u6539\u8fdb\u6587\u5316\u8868\u793a\u51c6\u786e\u6027\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.20154", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20154", "abs": "https://arxiv.org/abs/2510.20154", "authors": ["Anthony Dubreuil", "Antoine Gourru", "Christine Largeron", "Amine Trabelsi"], "title": "Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?", "comment": "Accepted in EMNLP 2025 (Main)", "summary": "Large Language Models inherit stereotypes from their pretraining data,\nleading to biased behavior toward certain social groups in many Natural\nLanguage Processing tasks, such as hateful speech detection or sentiment\nanalysis. Surprisingly, the evaluation of this kind of bias in stance detection\nmethods has been largely overlooked by the community. Stance Detection involves\nlabeling a statement as being against, in favor, or neutral towards a specific\ntarget and is among the most sensitive NLP tasks, as it often relates to\npolitical leanings. In this paper, we focus on the bias of Large Language\nModels when performing stance detection in a zero-shot setting. We\nautomatically annotate posts in pre-existing stance detection datasets with two\nattributes: dialect or vernacular of a specific group and text\ncomplexity/readability, to investigate whether these attributes influence the\nmodel's stance detection decisions. Our results show that LLMs exhibit\nsignificant stereotypes in stance detection tasks, such as incorrectly\nassociating pro-marijuana views with low text complexity and African American\ndialect with opposition to Donald Trump.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u7acb\u573a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u4f1a\u9519\u8bef\u5730\u5c06\u7279\u5b9a\u89c2\u70b9\u4e0e\u7279\u5b9a\u793e\u4f1a\u7fa4\u4f53\u7684\u8bed\u8a00\u7279\u5f81\u76f8\u5173\u8054\uff0c\u5982\u5c06\u652f\u6301\u5927\u9ebb\u5408\u6cd5\u5316\u7684\u7acb\u573a\u4e0e\u4f4e\u6587\u672c\u590d\u6742\u5ea6\u53ca\u975e\u88d4\u7f8e\u56fd\u4eba\u65b9\u8a00\u8054\u7cfb\u8d77\u6765\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7ee7\u627f\u4e86\u523b\u677f\u5370\u8c61\uff0c\u5bfc\u81f4\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u5bf9\u67d0\u4e9b\u793e\u4f1a\u7fa4\u4f53\u4ea7\u751f\u504f\u89c1\u884c\u4e3a\uff0c\u7136\u800c\u7acb\u573a\u68c0\u6d4b\u65b9\u6cd5\u4e2d\u7684\u6b64\u7c7b\u504f\u89c1\u8bc4\u4f30\u4e00\u76f4\u88ab\u7814\u7a76\u793e\u533a\u6240\u5ffd\u89c6\u3002\u7acb\u573a\u68c0\u6d4b\u4f5c\u4e3a\u6700\u654f\u611f\u7684NLP\u4efb\u52a1\u4e4b\u4e00\uff0c\u5e38\u6d89\u53ca\u653f\u6cbb\u503e\u5411\u5224\u65ad\uff0c\u56e0\u6b64\u8bc4\u4f30LLMs\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u504f\u89c1\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u5728\u73b0\u6709\u7acb\u573a\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u81ea\u52a8\u6807\u6ce8\u4e86\u4e24\u4e2a\u5c5e\u6027\uff1a\u7279\u5b9a\u7fa4\u4f53\u7684\u65b9\u8a00\u6216\u8bed\u8a00\u53d8\u4f53\uff0c\u4ee5\u53ca\u6587\u672c\u590d\u6742\u5ea6/\u53ef\u8bfb\u6027\uff0c\u4ee5\u63a2\u7a76\u8fd9\u4e9b\u5c5e\u6027\u662f\u5426\u5f71\u54cd\u6a21\u578b\u7684\u7acb\u573a\u68c0\u6d4b\u51b3\u7b56\u3002\u7814\u7a76\u91c7\u7528\u96f6\u6837\u672c\u8bbe\u7f6e\u8bc4\u4f30LLMs\u5728\u7acb\u573a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLMs\u5728\u7acb\u573a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u523b\u677f\u5370\u8c61\uff0c\u4f8b\u5982\u9519\u8bef\u5730\u5c06\u652f\u6301\u5927\u9ebb\u7684\u89c2\u70b9\u4e0e\u4f4e\u6587\u672c\u590d\u6742\u5ea6\u76f8\u5173\u8054\uff0c\u5e76\u5c06\u975e\u88d4\u7f8e\u56fd\u4eba\u65b9\u8a00\u4e0e\u53cd\u5bf9\u5510\u7eb3\u5fb7\u00b7\u7279\u6717\u666e\u7684\u7acb\u573a\u9519\u8bef\u5730\u8054\u7cfb\u8d77\u6765\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u654f\u611fNLP\u4efb\u52a1\u4e2d\u7cfb\u7edf\u8bc4\u4f30\u548c\u7f13\u89e3LLMs\u504f\u89c1\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u7acb\u573a\u68c0\u6d4b\u8fd9\u7c7b\u6d89\u53ca\u653f\u6cbb\u5224\u65ad\u7684\u4efb\u52a1\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u516c\u5e73\u7684\u7acb\u573a\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u547c\u5401\u793e\u533a\u5173\u6ce8LLMs\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u504f\u89c1\u95ee\u9898\u3002"}}
{"id": "2510.20381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20381", "abs": "https://arxiv.org/abs/2510.20381", "authors": ["Son T. Luu", "Trung Vo", "Hiep Nguyen", "Khanh Quoc Tran", "Kiet Van Nguyen", "Vu Tran", "Ngan Luu-Thuy Nguyen", "Le-Minh Nguyen"], "title": "VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation", "comment": "VLSP 2025 MLQA-TSR Share Task", "summary": "This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question\nanswering on traffic sign regulation shared task at VLSP 2025. VLSP 2025\nMLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal\nquestion answering. The goal is to advance research on Vietnamese multimodal\nlegal text processing and to provide a benchmark dataset for building and\nevaluating intelligent systems in multimodal legal domains, with a focus on\ntraffic sign regulation in Vietnam. The best-reported results on VLSP 2025\nMLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an\naccuracy of 86.30% for multimodal question answering.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86VLSP 2025 MLQA-TSR\u591a\u6a21\u6001\u4ea4\u901a\u6807\u5fd7\u6cd5\u89c4\u95ee\u7b54\u5171\u4eab\u4efb\u52a1\uff0c\u5305\u542b\u591a\u6a21\u6001\u6cd5\u5f8b\u68c0\u7d22\u548c\u591a\u6a21\u6001\u95ee\u7b54\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u65e8\u5728\u63a8\u8fdb\u8d8a\u5357\u591a\u6a21\u6001\u6cd5\u5f8b\u6587\u672c\u5904\u7406\u7814\u7a76\u5e76\u5efa\u7acb\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8d8a\u5357\u591a\u6a21\u6001\u6cd5\u5f8b\u6587\u672c\u5904\u7406\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u7279\u522b\u662f\u4ea4\u901a\u6807\u5fd7\u6cd5\u89c4\u65b9\u9762\u7684\u667a\u80fd\u7cfb\u7edf\u5f00\u53d1\u9700\u6c42\uff0c\u901a\u8fc7\u5efa\u7acb\u57fa\u51c6\u6570\u636e\u96c6\u6765\u4fc3\u8fdb\u591a\u6a21\u6001\u6cd5\u5f8b\u9886\u57df\u667a\u80fd\u7cfb\u7edf\u7684\u6784\u5efa\u4e0e\u8bc4\u4f30\u3002", "method": "\u8be5\u4efb\u52a1\u91c7\u7528\u591a\u6a21\u6001\u6cd5\u5f8b\u68c0\u7d22\u548c\u591a\u6a21\u6001\u95ee\u7b54\u4e24\u4e2a\u5b50\u4efb\u52a1\u7684\u6846\u67b6\u8bbe\u8ba1\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u5904\u7406\u4ea4\u901a\u6807\u5fd7\u6cd5\u89c4\u76f8\u5173\u7684\u95ee\u9898\uff0c\u4e3a\u53c2\u4e0e\u8005\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u5e73\u53f0\u548c\u65b9\u6cd5\u8bba\u6307\u5bfc\u3002", "result": "\u5728VLSP 2025 MLQA-TSR\u4efb\u52a1\u4e2d\uff0c\u591a\u6a21\u6001\u6cd5\u5f8b\u68c0\u7d22\u7684\u6700\u4f73F2\u5206\u6570\u8fbe\u523064.55%\uff0c\u591a\u6a21\u6001\u95ee\u7b54\u7684\u51c6\u786e\u7387\u8fbe\u523086.30%\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u8bbe\u5b9a\u4e86\u6027\u80fd\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8d8a\u5357\u591a\u6a21\u6001\u6cd5\u5f8b\u6587\u672c\u5904\u7406\u5efa\u7acb\u4e86\u91cd\u8981\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u7279\u522b\u5728\u4ea4\u901a\u6807\u5fd7\u6cd5\u89c4\u9886\u57df\u63a8\u52a8\u4e86\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6cd5\u5f8bAI\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u652f\u6491\u3002"}}
{"id": "2510.20229", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20229", "abs": "https://arxiv.org/abs/2510.20229", "authors": ["Ge Zheng", "Jiaye Qian", "Jiajin Tang", "Sibei Yang"], "title": "Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have made significant progress in recent\nyears but are also prone to hallucination issues. They exhibit more\nhallucinations in longer, free-form responses, often attributed to accumulated\nuncertainties. In this paper, we ask: Does increased hallucination result\nsolely from length-induced errors, or is there a deeper underlying mechanism?\nAfter a series of preliminary experiments and findings, we suggest that the\nrisk of hallucinations is not caused by length itself but by the increased\nreliance on context for coherence and completeness in longer responses.\nBuilding on these insights, we propose a novel \"induce-detect-suppress\"\nframework that actively induces hallucinations through deliberately designed\ncontexts, leverages induced instances for early detection of high-risk cases,\nand ultimately suppresses potential object-level hallucinations during actual\ndecoding. Our approach achieves consistent, significant improvements across all\nbenchmarks, demonstrating its efficacy. The strong detection and improved\nhallucination mitigation not only validate our framework but, more importantly,\nre-validate our hypothesis on context. Rather than solely pursuing performance\ngains, this study aims to provide new insights and serves as a first step\ntoward a deeper exploration of hallucinations in LVLMs' longer responses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684'\u8bf1\u5bfc-\u68c0\u6d4b-\u6291\u5236'\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u8bf1\u5bfc\u5e7b\u89c9\u6765\u68c0\u6d4b\u9ad8\u98ce\u9669\u60c5\u51b5\u5e76\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u6291\u5236\u5bf9\u8c61\u7ea7\u5e7b\u89c9\uff0c\u663e\u8457\u6539\u5584\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u957f\u6587\u672c\u54cd\u5e94\u65f6\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u95ee\u9898\uff0c\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u8fd9\u4ec5\u7531\u957f\u5ea6\u5bfc\u81f4\u7684\u9519\u8bef\u7d2f\u79ef\u5f15\u8d77\uff0c\u4f46\u672c\u6587\u7814\u7a76\u53d1\u73b0\u5e7b\u89c9\u98ce\u9669\u5b9e\u9645\u4e0a\u6e90\u4e8e\u957f\u6587\u672c\u5bf9\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u548c\u5b8c\u6574\u6027\u7684\u66f4\u5f3a\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa'\u8bf1\u5bfc-\u68c0\u6d4b-\u6291\u5236'\u4e09\u5c42\u6846\u67b6\uff1a\u9996\u5148\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4e0a\u4e0b\u6587\u4e3b\u52a8\u8bf1\u5bfc\u5e7b\u89c9\uff0c\u7136\u540e\u5229\u7528\u8bf1\u5bfc\u5b9e\u4f8b\u8fdb\u884c\u65e9\u671f\u9ad8\u98ce\u9669\u68c0\u6d4b\uff0c\u6700\u540e\u5728\u5b9e\u9645\u89e3\u7801\u8fc7\u7a0b\u4e2d\u6291\u5236\u6f5c\u5728\u7684\u5bf9\u8c61\u7ea7\u5e7b\u89c9\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u663e\u8457\u6539\u8fdb\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u68c0\u6d4b\u80fd\u529b\u548c\u5e7b\u89c9\u7f13\u89e3\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u66f4\u91cd\u8981\u7684\u662f\u91cd\u65b0\u9a8c\u8bc1\u4e86\u4e0a\u4e0b\u6587\u4f9d\u8d56\u662f\u957f\u6587\u672c\u5e7b\u89c9\u7684\u6838\u5fc3\u673a\u5236\uff0c\u4e3a\u6df1\u5165\u63a2\u7d22LVLMs\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u6d1e\u89c1\u548c\u521d\u6b65\u63a2\u7d22\u65b9\u5411\u3002"}}
{"id": "2510.20095", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20095", "abs": "https://arxiv.org/abs/2510.20095", "authors": ["Ziheng Zhang", "Xinyue Ma", "Arpita Chowdhury", "Elizabeth G. Campolongo", "Matthew J. Thompson", "Net Zhang", "Samuel Stevens", "Hilmar Lapp", "Tanya Berger-Wolf", "Yu Su", "Wei-Lun Chao", "Jianyang Gu"], "title": "BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models", "comment": "Project page: https://imageomics.github.io/biocap/", "summary": "This work investigates descriptive captions as an additional source of\nsupervision for biological multimodal foundation models. Images and captions\ncan be viewed as complementary samples from the latent morphospace of a\nspecies, each capturing certain biological traits. Incorporating captions\nduring training encourages alignment with this shared latent structure,\nemphasizing potentially diagnostic characters while suppressing spurious\ncorrelations. The main challenge, however, lies in obtaining faithful,\ninstance-specific captions at scale. This requirement has limited the\nutilization of natural language supervision in organismal biology compared with\nmany other scientific domains. We complement this gap by generating synthetic\ncaptions with multimodal large language models (MLLMs), guided by\nWikipedia-derived visual information and taxon-tailored format examples. These\ndomain-specific contexts help reduce hallucination and yield accurate,\ninstance-based descriptive captions. Using these captions, we train BIOCAP\n(i.e., BIOCLIP with Captions), a biological foundation model that captures rich\nsemantics and achieves strong performance in species classification and\ntext-image retrieval. These results demonstrate the value of descriptive\ncaptions beyond labels in bridging biological images with multimodal foundation\nmodels.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faBIOCAP\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u63cf\u8ff0\u6027\u6807\u6ce8\uff0c\u5c06\u751f\u7269\u56fe\u50cf\u4e0e\u6587\u672c\u63cf\u8ff0\u5bf9\u9f50\uff0c\u5728\u7269\u79cd\u5206\u7c7b\u548c\u56fe\u6587\u68c0\u7d22\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u63cf\u8ff0\u6027\u6807\u6ce8\u5728\u751f\u7269\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u4ef7\u503c\u3002", "motivation": "\u751f\u7269\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u5b9e\u4f8b\u7279\u5b9a\u7684\u63cf\u8ff0\u6027\u6807\u6ce8\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u8fd9\u9650\u5236\u4e86\u81ea\u7136\u8bed\u8a00\u76d1\u7763\u5728\u751f\u7269\u9886\u57df\u7684\u5e94\u7528\uff0c\u800c\u56fe\u50cf\u548c\u63cf\u8ff0\u6027\u6807\u6ce8\u53ef\u4ee5\u89c6\u4e3a\u7269\u79cd\u6f5c\u5728\u5f62\u6001\u7a7a\u95f4\u7684\u4e92\u8865\u6837\u672c\uff0c\u6355\u83b7\u4e0d\u540c\u7684\u751f\u7269\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u63cf\u8ff0\u6027\u6807\u6ce8\uff0c\u7ed3\u5408\u7ef4\u57fa\u767e\u79d1\u7684\u89c6\u89c9\u4fe1\u606f\u548c\u9488\u5bf9\u7279\u5b9a\u5206\u7c7b\u7fa4\u5b9a\u5236\u7684\u683c\u5f0f\u793a\u4f8b\uff0c\u8fd9\u4e9b\u9886\u57df\u7279\u5b9a\u4e0a\u4e0b\u6587\u6709\u52a9\u4e8e\u51cf\u5c11\u5e7b\u89c9\u5e76\u4ea7\u751f\u51c6\u786e\u7684\u5b9e\u4f8b\u63cf\u8ff0\u6027\u6807\u6ce8\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u6807\u6ce8\u8bad\u7ec3BIOCAP\u6a21\u578b\u3002", "result": "BIOCAP\u6a21\u578b\u80fd\u591f\u6355\u83b7\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5728\u7269\u79cd\u5206\u7c7b\u548c\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u63cf\u8ff0\u6027\u6807\u6ce8\u5728\u751f\u7269\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63cf\u8ff0\u6027\u6807\u6ce8\u8d85\u8d8a\u4e86\u4f20\u7edf\u6807\u7b7e\u7684\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u6865\u63a5\u751f\u7269\u56fe\u50cf\u4e0e\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u8c03\u6f5c\u5728\u8bca\u65ad\u7279\u5f81\u5e76\u6291\u5236\u865a\u5047\u76f8\u5173\u6027\uff0c\u4e3a\u751f\u7269\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u76d1\u7763\u8303\u5f0f\u3002"}}
{"id": "2510.20727", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20727", "abs": "https://arxiv.org/abs/2510.20727", "authors": ["Xizhi Wu", "Madeline S. Kreider", "Philip E. Empey", "Chenyu Li", "Yanshan Wang"], "title": "Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing", "comment": null, "summary": "Objective: Fluoropyrimidines are widely prescribed for colorectal and breast\ncancers, but are associated with toxicities such as hand-foot syndrome and\ncardiotoxicity. Since toxicity documentation is often embedded in clinical\nnotes, we aimed to develop and evaluate natural language processing (NLP)\nmethods to extract treatment and toxicity information.\n  Materials and Methods: We constructed a gold-standard dataset of 236 clinical\nnotes from 204,165 adult oncology patients. Domain experts annotated categories\nrelated to treatment regimens and toxicities. We developed rule-based, machine\nlearning-based (Random Forest, Support Vector Machine [SVM], Logistic\nRegression [LR]), deep learning-based (BERT, ClinicalBERT), and large language\nmodels (LLM)-based NLP approaches (zero-shot and error-analysis prompting).\nModels used an 80:20 train-test split.\n  Results: Sufficient data existed to train and evaluate 5 annotated\ncategories. Error-analysis prompting achieved optimal precision, recall, and F1\nscores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot\nprompting reached F1=1.000 for treatment and F1=0.876 for toxicities\nextraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning\nunderperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and\nClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods\nserved as our baseline with F1 scores of 0.857 in treatment and 0.858 in\ntoxicities.\n  Discussion: LMM-based approaches outperformed all others, followed by machine\nlearning methods. Machine and deep learning approaches were limited by small\ntraining data and showed limited generalizability, particularly for rare\ncategories.\n  Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine\ntreatment and toxicity information from clinical notes, and has strong\npotential to support oncology research and pharmacovigilance.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u63d0\u53d6\u6c1f\u5627\u5576\u7c7b\u836f\u7269\u6cbb\u7597\u548c\u6bd2\u6027\u4fe1\u606f\u3002\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9519\u8bef\u5206\u6790\u63d0\u793a\u65b9\u6cd5\u5728\u63d0\u53d6\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u65b9\u9762\u8868\u73b0\u6700\u4f18\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u6c1f\u5627\u5576\u7c7b\u836f\u7269\u5e7f\u6cdb\u7528\u4e8e\u7ed3\u76f4\u80a0\u764c\u548c\u4e73\u817a\u764c\u6cbb\u7597\uff0c\u4f46\u5e38\u4f34\u968f\u624b\u8db3\u7efc\u5408\u5f81\u548c\u5fc3\u810f\u6bd2\u6027\u7b49\u4e0d\u826f\u53cd\u5e94\u3002\u7531\u4e8e\u6bd2\u6027\u8bb0\u5f55\u901a\u5e38\u5d4c\u5165\u5728\u4e34\u5e8a\u7b14\u8bb0\u4e2d\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u6709\u6548\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u6cd5\u6765\u81ea\u52a8\u63d0\u53d6\u6cbb\u7597\u548c\u6bd2\u6027\u4fe1\u606f\uff0c\u4ee5\u652f\u6301\u80bf\u7624\u5b66\u7814\u7a76\u548c\u836f\u7269\u8b66\u6212\u5de5\u4f5c\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b236\u4efd\u4e34\u5e8a\u7b14\u8bb0\u7684\u91d1\u6807\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u89c4\u5219\u3001\u673a\u5668\u5b66\u4e60\uff08\u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\u3001\u903b\u8f91\u56de\u5f52\uff09\u3001\u6df1\u5ea6\u5b66\u4e60\uff08BERT\u3001ClinicalBERT\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08\u96f6\u6837\u672c\u63d0\u793a\u548c\u9519\u8bef\u5206\u6790\u63d0\u793a\uff09\u7684\u591a\u7c7b\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u6cd5\u3002\u6240\u6709\u6a21\u578b\u91c7\u752880:20\u7684\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u5272\u7b56\u7565\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u9519\u8bef\u5206\u6790\u63d0\u793a\u65b9\u6cd5\u5728\u6cbb\u7597\u548c\u6bd2\u6027\u63d0\u53d6\u65b9\u9762\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff08F1=1.000\uff09\uff0c\u96f6\u6837\u672c\u63d0\u793a\u5728\u6cbb\u7597\u63d0\u53d6\u4e0a\u540c\u6837\u8fbe\u5230F1=1.000\uff0c\u6bd2\u6027\u63d0\u53d6\u4e3aF1=0.876\u3002\u903b\u8f91\u56de\u5f52\u548c\u652f\u6301\u5411\u91cf\u673a\u5728\u6bd2\u6027\u63d0\u53d6\u4e2d\u6392\u540d\u7b2c\u4e8c\uff08F1=0.937\uff09\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u8f83\u5dee\uff0cBERT\u548cClinicalBERT\u7684F1\u5206\u6570\u5206\u522b\u4e3a0.873/0.839\u548c0.873/0.886\u3002\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u4f5c\u4e3a\u57fa\u7ebf\uff0cF1\u5206\u6570\u4e3a0.857\u548c0.858\u3002", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u6240\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5176\u6b21\u4e3a\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7f55\u89c1\u7c7b\u522b\u65f6\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u80fd\u591f\u6709\u6548\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u63d0\u53d6\u6c1f\u5627\u5576\u6cbb\u7597\u548c\u6bd2\u6027\u4fe1\u606f\uff0c\u5177\u6709\u652f\u6301\u80bf\u7624\u5b66\u7814\u7a76\u548c\u836f\u7269\u8b66\u6212\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.20287", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20287", "abs": "https://arxiv.org/abs/2510.20287", "authors": ["Sauptik Dhar", "Naveen Ramakrishnan", "Michelle Munson"], "title": "Breakdance Video classification in the age of Generative AI", "comment": "11 pages", "summary": "Large Vision Language models have seen huge application in several sports\nuse-cases recently. Most of these works have been targeted towards a limited\nsubset of popular sports like soccer, cricket, basketball etc; focusing on\ngenerative tasks like visual question answering, highlight generation. This\nwork analyzes the applicability of the modern video foundation models (both\nencoder and decoder) for a very niche but hugely popular dance sports -\nbreakdance. Our results show that Video Encoder models continue to outperform\nstate-of-the-art Video Language Models for prediction tasks. We provide\ninsights on how to choose the encoder model and provide a thorough analysis\ninto the workings of a finetuned decoder model for breakdance video\nclassification.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u73b0\u4ee3\u89c6\u9891\u57fa\u7840\u6a21\u578b\u5728\u8857\u821e\u8fd0\u52a8\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u89c6\u9891\u7f16\u7801\u5668\u6a21\u578b\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u4e3a\u8857\u821e\u89c6\u9891\u5206\u7c7b\u63d0\u4f9b\u4e86\u6a21\u578b\u9009\u62e9\u548c\u5fae\u8c03\u7b56\u7565\u7684\u6df1\u5165\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5e94\u7528\u4e8e\u4e3b\u6d41\u4f53\u80b2\u9879\u76ee\u5982\u8db3\u7403\u3001\u677f\u7403\u3001\u7bee\u7403\u7b49\uff0c\u4e13\u6ce8\u4e8e\u751f\u6210\u5f0f\u4efb\u52a1\u5982\u89c6\u89c9\u95ee\u7b54\u548c\u9ad8\u5149\u751f\u6210\uff0c\u800c\u9488\u5bf9\u8857\u821e\u7b49\u5c0f\u4f17\u4f46\u6d41\u884c\u7684\u821e\u8e48\u4f53\u80b2\u5e94\u7528\u7814\u7a76\u76f8\u5bf9\u7f3a\u4e4f\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u5206\u6790\u89c6\u9891\u57fa\u7840\u6a21\u578b\u5728\u8857\u821e\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u73b0\u4ee3\u89c6\u9891\u57fa\u7840\u6a21\u578b\uff0c\u5305\u62ec\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e24\u79cd\u67b6\u6784\uff0c\u5bf9\u8857\u821e\u89c6\u9891\u5206\u7c7b\u4efb\u52a1\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u7f16\u7801\u5668\u6a21\u578b\u7684\u9009\u62e9\u7b56\u7565\u4ee5\u53ca\u5fae\u8c03\u89e3\u7801\u5668\u6a21\u578b\u5728\u8857\u821e\u89c6\u9891\u5206\u7c7b\u4e2d\u7684\u5de5\u4f5c\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u89c6\u9891\u7f16\u7801\u5668\u6a21\u578b\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u7f16\u7801\u5668\u6a21\u578b\u9009\u62e9\u7684\u6307\u5bfc\u539f\u5219\uff0c\u5e76\u5bf9\u5fae\u8c03\u89e3\u7801\u5668\u6a21\u578b\u5728\u8857\u821e\u89c6\u9891\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5c0f\u4f17\u4f53\u80b2\u9886\u57df\u7684\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5f3a\u8c03\u7f16\u7801\u5668\u6a21\u578b\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u5730\u4f4d\uff0c\u5e76\u4e3a\u8857\u821e\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u89c6\u9891\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6a21\u578b\u9009\u62e9\u548c\u5fae\u8c03\u7b56\u7565\u6307\u5bfc\uff0c\u63a8\u52a8\u4e86\u89c6\u9891\u57fa\u7840\u6a21\u578b\u5728\u4e13\u4e1a\u4f53\u80b2\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.20134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20134", "abs": "https://arxiv.org/abs/2510.20134", "authors": ["Jiachen Liang", "Ruibing Hou", "Minyang Hu", "Hong Chang", "Shiguang Shan", "Xilin Chen"], "title": "Revisiting Logit Distributions for Reliable Out-of-Distribution Detection", "comment": "Accepted by NeurIPS 2025", "summary": "Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof deep learning models in open-world applications. While post-hoc methods are\nfavored for their efficiency and ease of deployment, existing approaches often\nunderexploit the rich information embedded in the model's logits space. In this\npaper, we propose LogitGap, a novel post-hoc OOD detection method that\nexplicitly exploits the relationship between the maximum logit and the\nremaining logits to enhance the separability between in-distribution (ID) and\nOOD samples. To further improve its effectiveness, we refine LogitGap by\nfocusing on a more compact and informative subset of the logit space.\nSpecifically, we introduce a training-free strategy that automatically\nidentifies the most informative logits for scoring. We provide both theoretical\nanalysis and empirical evidence to validate the effectiveness of our approach.\nExtensive experiments on both vision-language and vision-only models\ndemonstrate that LogitGap consistently achieves state-of-the-art performance\nacross diverse OOD detection scenarios and benchmarks. Code is available at\nhttps://github.com/GIT-LJc/LogitGap.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLogitGap\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5229\u7528\u6700\u5927logit\u4e0e\u5176\u4f59logits\u4e4b\u95f4\u7684\u5173\u7cfb\u6765\u589e\u5f3a\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u6837\u672c\u7684\u53ef\u5206\u79bb\u6027\uff0c\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u540e\u5904\u7406\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u68c0\u6d4b\u4e2d\u5f80\u5f80\u672a\u5145\u5206\u5229\u7528\u6a21\u578blogits\u7a7a\u95f4\u4e2d\u4e30\u5bcc\u7684\u5d4c\u5165\u4fe1\u606f\uff0c\u8fd9\u9650\u5236\u4e86\u68c0\u6d4b\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "LogitGap\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u6700\u5927logit\u4e0e\u5176\u4f59logits\u7684\u5173\u7cfb\u6765\u589e\u5f3a\u53ef\u5206\u79bb\u6027\uff0c\u5e76\u5f15\u5165\u65e0\u9700\u8bad\u7ec3\u7684\u7b56\u7565\u81ea\u52a8\u8bc6\u522blogits\u7a7a\u95f4\u4e2d\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5b50\u96c6\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u5728\u89c6\u89c9\u8bed\u8a00\u548c\u7eaf\u89c6\u89c9\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLogitGap\u5728\u591a\u79cd\u5206\u5e03\u5916\u68c0\u6d4b\u573a\u666f\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86logits\u7a7a\u95f4\u4e2d\u6700\u5927logit\u4e0e\u5176\u4f59logits\u5173\u7cfb\u7684\u6709\u6548\u5229\u7528\u80fd\u591f\u663e\u8457\u63d0\u5347\u5206\u5e03\u5916\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u540e\u5904\u7406\u65b9\u6cd5\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.20291", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20291", "abs": "https://arxiv.org/abs/2510.20291", "authors": ["LinFeng Li", "Jian Zhao", "Zepeng Yang", "Yuhang Song", "Bojun Lin", "Tianle Zhang", "Yuchen Yuan", "Chi Zhang", "Xuelong Li"], "title": "A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization", "comment": null, "summary": "We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone\nNavigation. The task retrieves the most relevant geo-referenced image from a\nlarge multi-platform corpus (satellite/drone/ground) given a natural-language\nquery. Two obstacles are severe inter-platform heterogeneity and a domain gap\nbetween generic training descriptions and platform-specific test queries. We\nmitigate these with a domain-aligned preprocessing pipeline and a\nMixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite\naugmentation, and removal of orientation words; (ii) an LLM-based caption\nrefinement pipeline to align textual semantics with the distinct visual\ncharacteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we\ntrain three platform experts using a progressive two-stage, hard-negative\nmining strategy to enhance discriminative power, and fuse their scores at\ninference. The system tops the official leaderboard, demonstrating robust\ncross-modal geo-localization under heterogeneous viewpoints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8de8\u6a21\u6001\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u83b7\u80dc\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u9886\u57df\u5bf9\u9f50\u9884\u5904\u7406\u6d41\u7a0b\u548c\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u89e3\u51b3\u4e86\u591a\u5e73\u53f0\u5f02\u6784\u6027\u548c\u9886\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u5728RoboSense 2025 Track 4\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8de8\u6a21\u6001\u5730\u7406\u5b9a\u4f4d\u4e2d\u7684\u4e24\u4e2a\u4e3b\u8981\u969c\u788d\uff1a\u4e25\u91cd\u7684\u5e73\u53f0\u95f4\u5f02\u6784\u6027\uff08\u536b\u661f/\u65e0\u4eba\u673a/\u5730\u9762\u5e73\u53f0\uff09\u4ee5\u53ca\u901a\u7528\u8bad\u7ec3\u63cf\u8ff0\u4e0e\u5e73\u53f0\u7279\u5b9a\u6d4b\u8bd5\u67e5\u8be2\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u8fd9\u4e9b\u56e0\u7d20\u9650\u5236\u4e86\u591a\u5e73\u53f0\u56fe\u50cf\u68c0\u7d22\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u9886\u57df\u5bf9\u9f50\u9884\u5904\u7406\u6d41\u7a0b\uff08\u5e73\u53f0\u5212\u5206\u3001\u536b\u661f\u589e\u5f3a\u3001\u65b9\u5411\u8bcd\u79fb\u9664\uff09\u548c\u57fa\u4e8eLLM\u7684\u6807\u9898\u7cbe\u70bc\u7ba1\u9053\uff0c\u4f7f\u7528BGE-M3\u548cEVA-CLIP\u5206\u522b\u5904\u7406\u6587\u672c\u548c\u56fe\u50cf\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u4e24\u9636\u6bb5\u786c\u8d1f\u6837\u672c\u6316\u6398\u7b56\u7565\u8bad\u7ec3\u4e09\u4e2a\u5e73\u53f0\u4e13\u5bb6\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u878d\u5408\u5176\u5f97\u5206\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u5b98\u65b9\u6392\u884c\u699c\u4e0a\u4f4d\u5c45\u9996\u4f4d\uff0c\u8bc1\u660e\u4e86\u5728\u5f02\u6784\u89c6\u89d2\u4e0b\u5177\u6709\u9c81\u68d2\u7684\u8de8\u6a21\u6001\u5730\u7406\u5b9a\u4f4d\u80fd\u529b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u5e73\u53f0\u56fe\u50cf\u68c0\u7d22\u4e2d\u7684\u9886\u57df\u9002\u5e94\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u9886\u57df\u5bf9\u9f50\u9884\u5904\u7406\u548c\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u80fd\u6709\u6548\u7f13\u89e3\u8de8\u5e73\u53f0\u5f02\u6784\u6027\u548c\u9886\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u5730\u7406\u5b9a\u4f4d\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u80fd\u3002"}}
{"id": "2510.20162", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20162", "abs": "https://arxiv.org/abs/2510.20162", "authors": ["Xudong Yan", "Songhe Feng"], "title": "TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel\nattribute-object compositions based on the knowledge learned from seen ones.\nExisting methods suffer from performance degradation caused by the distribution\nshift of label space at test time, which stems from the inclusion of unseen\ncompositions recombined from attributes and objects. To overcome the challenge,\nwe propose a novel approach that accumulates comprehensive knowledge in both\ntextual and visual modalities from unsupervised data to update multimodal\nprototypes at test time. Building on this, we further design an adaptive update\nweight to control the degree of prototype adjustment, enabling the model to\nflexibly adapt to distribution shift during testing. Moreover, a dynamic\npriority queue is introduced that stores high-confidence images to acquire\nvisual knowledge from historical images for inference. Considering the semantic\nconsistency of multimodal knowledge, we align textual and visual prototypes by\nmultimodal collaborative representation learning. Extensive experiments\nindicate that our approach achieves state-of-the-art performance on four\nbenchmark datasets under both closed-world and open-world settings. Code will\nbe available at https://github.com/xud-yan/TOMCAT .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u65e0\u76d1\u7763\u6570\u636e\u4e2d\u79ef\u7d2f\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u5168\u9762\u77e5\u8bc6\u6765\u66f4\u65b0\u591a\u6a21\u6001\u539f\u578b\uff0c\u89e3\u51b3\u4e86\u6d4b\u8bd5\u65f6\u6807\u7b7e\u7a7a\u95f4\u5206\u5e03\u504f\u79fb\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u95ed\u4e16\u754c\u548c\u5f00\u4e16\u754c\u8bbe\u7f6e\u4e0b\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u9762\u4e34\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u8fd9\u6e90\u4e8e\u4ece\u672a\u89c1\u8fc7\u7684\u5c5e\u6027-\u5bf9\u8c61\u7ec4\u5408\u91cd\u65b0\u7ec4\u5408\u5bfc\u81f4\u7684\u6807\u7b7e\u7a7a\u95f4\u5206\u5e03\u504f\u79fb\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u5206\u5e03\u53d8\u5316\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u65b0\u9896\u7ec4\u5408\u7684\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6a21\u6001\u539f\u578b\u66f4\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u66f4\u65b0\u6743\u91cd\u63a7\u5236\u539f\u578b\u8c03\u6574\u7a0b\u5ea6\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u4f18\u5148\u7ea7\u961f\u5217\u5b58\u50a8\u9ad8\u7f6e\u4fe1\u5ea6\u56fe\u50cf\u4ee5\u83b7\u53d6\u5386\u53f2\u89c6\u89c9\u77e5\u8bc6\u3002\u91c7\u7528\u591a\u6a21\u6001\u534f\u540c\u8868\u793a\u5b66\u4e60\u5bf9\u9f50\u6587\u672c\u548c\u89c6\u89c9\u539f\u578b\uff0c\u786e\u4fdd\u591a\u6a21\u6001\u77e5\u8bc6\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u95ed\u4e16\u754c\u548c\u5f00\u4e16\u754c\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u6709\u6548\u8d21\u732e\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5229\u7528\u65e0\u76d1\u7763\u6570\u636e\u79ef\u7d2f\u591a\u6a21\u6001\u77e5\u8bc6\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002\u81ea\u9002\u5e94\u539f\u578b\u66f4\u65b0\u673a\u5236\u548c\u591a\u6a21\u6001\u534f\u540c\u5b66\u4e60\u4e3a\u5904\u7406\u52a8\u6001\u6d4b\u8bd5\u73af\u5883\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.20189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20189", "abs": "https://arxiv.org/abs/2510.20189", "authors": ["Xinyi Hu", "Yuran Wang", "Yue Li", "Wenxuan Liu", "Zheng Wang"], "title": "SPAN: Continuous Modeling of Suspicion Progression for Temporal Intention Localization", "comment": null, "summary": "Temporal Intention Localization (TIL) is crucial for video surveillance,\nfocusing on identifying varying levels of suspicious intentions to improve\nsecurity monitoring. However, existing discrete classification methods fail to\ncapture the continuous nature of suspicious intentions, limiting early\nintervention and explainability. In this paper, we propose the Suspicion\nProgression Analysis Network (SPAN), which shifts from discrete classification\nto continuous regression, enabling the capture of fluctuating and evolving\nsuspicious intentions. We reveal that suspicion exhibits long-term dependencies\nand cumulative effects, similar to Temporal Point Process (TPP) theory. Based\non these insights, we define a suspicion score formula that models continuous\nchanges while accounting for temporal characteristics. We also introduce\nSuspicion Coefficient Modulation, which adjusts suspicion coefficients using\nmultimodal information to reflect the varying impacts of suspicious actions.\nAdditionally, the Concept-Anchored Mapping method is proposed to link\nsuspicious actions to predefined intention concepts, offering insights into\nboth the actions and their potential underlying intentions. Extensive\nexperiments on the HAI dataset show that SPAN significantly outperforms\nexisting methods, reducing MSE by 19.8% and improving average mAP by 1.78%.\nNotably, SPAN achieves a 2.74% mAP gain in low-frequency cases, demonstrating\nits superior ability to capture subtle behavioral changes. Compared to discrete\nclassification systems, our continuous suspicion modeling approach enables\nearlier detection and proactive intervention, greatly enhancing system\nexplainability and practical utility in security applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u53ef\u7591\u8fdb\u5c55\u5206\u6790\u7f51\u7edc\uff08SPAN\uff09\uff0c\u5c06\u65f6\u5e8f\u610f\u56fe\u5b9a\u4f4d\u4ece\u79bb\u6563\u5206\u7c7b\u8f6c\u53d8\u4e3a\u8fde\u7eed\u56de\u5f52\uff0c\u80fd\u591f\u6355\u6349\u6ce2\u52a8\u6f14\u5316\u7684\u53ef\u7591\u610f\u56fe\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728HAI\u6570\u636e\u96c6\u4e0a\u964d\u4f4eMSE 19.8%\uff0c\u63d0\u5347\u5e73\u5747mAP 1.78%\u3002", "motivation": "\u73b0\u6709\u79bb\u6563\u5206\u7c7b\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u53ef\u7591\u610f\u56fe\u7684\u8fde\u7eed\u7279\u6027\uff0c\u9650\u5236\u4e86\u65e9\u671f\u5e72\u9884\u548c\u53ef\u89e3\u91ca\u6027\u3002\u65f6\u5e8f\u610f\u56fe\u5b9a\u4f4d\u9700\u8981\u8bc6\u522b\u4e0d\u540c\u7ea7\u522b\u7684\u53ef\u7591\u610f\u56fe\u4ee5\u63d0\u5347\u89c6\u9891\u76d1\u63a7\u5b89\u5168\u6027\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u610f\u56fe\u7684\u6ce2\u52a8\u548c\u6f14\u5316\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u53ef\u7591\u8fdb\u5c55\u5206\u6790\u7f51\u7edc\uff08SPAN\uff09\uff0c\u57fa\u4e8e\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u7406\u8bba\u5efa\u6a21\u53ef\u7591\u610f\u56fe\u7684\u957f\u671f\u4f9d\u8d56\u6027\u548c\u7d2f\u79ef\u6548\u5e94\u3002\u5f15\u5165\u53ef\u7591\u7cfb\u6570\u8c03\u5236\u673a\u5236\uff0c\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u8c03\u6574\u53ef\u7591\u7cfb\u6570\u4ee5\u53cd\u6620\u4e0d\u540c\u53ef\u7591\u52a8\u4f5c\u7684\u5f71\u54cd\u5dee\u5f02\u3002\u91c7\u7528\u6982\u5ff5\u951a\u5b9a\u6620\u5c04\u65b9\u6cd5\u5c06\u53ef\u7591\u52a8\u4f5c\u4e0e\u9884\u5b9a\u4e49\u610f\u56fe\u6982\u5ff5\u5173\u8054\u3002", "result": "\u5728HAI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSPAN\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cMSE\u964d\u4f4e19.8%\uff0c\u5e73\u5747mAP\u63d0\u53471.78%\u3002\u5728\u4f4e\u9891\u6848\u4f8b\u4e2dmAP\u589e\u76ca\u8fbe2.74%\uff0c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u6355\u6349\u7ec6\u5fae\u884c\u4e3a\u53d8\u5316\u3002\u8fde\u7eed\u53ef\u7591\u5efa\u6a21\u65b9\u6cd5\u76f8\u6bd4\u79bb\u6563\u5206\u7c7b\u7cfb\u7edf\u80fd\u5b9e\u73b0\u66f4\u65e9\u68c0\u6d4b\u548c\u4e3b\u52a8\u5e72\u9884\u3002", "conclusion": "\u8fde\u7eed\u53ef\u7591\u5efa\u6a21\u65b9\u6cd5\u6781\u5927\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u65e9\u68c0\u6d4b\u53ef\u7591\u884c\u4e3a\u5e76\u5b9e\u73b0\u4e3b\u52a8\u5e72\u9884\uff0c\u4e3a\u5b89\u5168\u76d1\u63a7\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6982\u5ff5\u951a\u5b9a\u6620\u5c04\u65b9\u6cd5\u540c\u65f6\u63d0\u4f9b\u4e86\u5bf9\u52a8\u4f5c\u53ca\u5176\u6f5c\u5728\u610f\u56fe\u7684\u6df1\u5165\u7406\u89e3\u3002"}}
{"id": "2510.20256", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.20256", "abs": "https://arxiv.org/abs/2510.20256", "authors": ["Guowei Zhong", "Junjie Li", "Huaiyu Zhu", "Ruohong Huan", "Yun Pan"], "title": "Calibrating Multimodal Consensus for Emotion Recognition", "comment": null, "summary": "In recent years, Multimodal Emotion Recognition (MER) has made substantial\nprogress. Nevertheless, most existing approaches neglect the semantic\ninconsistencies that may arise across modalities, such as conflicting emotional\ncues between text and visual inputs. Besides, current methods are often\ndominated by the text modality due to its strong representational capacity,\nwhich can compromise recognition accuracy. To address these challenges, we\npropose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a\nPseudo Label Generation Module (PLGM) to produce pseudo unimodal labels,\nenabling unimodal pretraining in a self-supervised fashion. It then employs a\nParameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for\nmultimodal finetuning, thereby mitigating text dominance and guiding the fusion\nprocess toward a more reliable consensus. Experimental results demonstrate that\nCMC achieves performance on par with or superior to state-of-the-art methods\nacross four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and\nexhibits notable advantages in scenarios with semantic inconsistencies on\nCH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible\nat https://github.com/gw-zhong/CMC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6821\u51c6\u591a\u6a21\u6001\u5171\u8bc6\uff08CMC\uff09\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u751f\u6210\u6a21\u5757\u5b9e\u73b0\u81ea\u76d1\u7763\u5355\u6a21\u6001\u9884\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u53c2\u6570\u65e0\u5173\u878d\u5408\u6a21\u5757\u548c\u591a\u6a21\u6001\u5171\u8bc6\u8def\u7531\u5668\u6765\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u8bed\u4e49\u4e0d\u4e00\u81f4\u6027\u548c\u6587\u672c\u6a21\u6001\u4e3b\u5bfc\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u666e\u904d\u5ffd\u89c6\u6a21\u6001\u95f4\u7684\u8bed\u4e49\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4f8b\u5982\u6587\u672c\u4e0e\u89c6\u89c9\u8f93\u5165\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u51b2\u7a81\u7684\u60c5\u611f\u7ebf\u7d22\uff0c\u540c\u65f6\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u6587\u672c\u6a21\u6001\u7684\u5f3a\u5927\u8868\u793a\u80fd\u529b\u800c\u5f80\u5f80\u88ab\u5176\u4e3b\u5bfc\uff0c\u8fd9\u4f1a\u635f\u5bb3\u8bc6\u522b\u51c6\u786e\u6027\u3002", "method": "CMC\u6a21\u578b\u5305\u542b\u4f2a\u6807\u7b7e\u751f\u6210\u6a21\u5757\uff08PLGM\uff09\u7528\u4e8e\u751f\u6210\u4f2a\u5355\u6a21\u6001\u6807\u7b7e\u4ee5\u5b9e\u73b0\u81ea\u76d1\u7763\u5355\u6a21\u6001\u9884\u8bad\u7ec3\uff0c\u53c2\u6570\u65e0\u5173\u878d\u5408\u6a21\u5757\uff08PFM\uff09\u7528\u4e8e\u591a\u6a21\u6001\u5fae\u8c03\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u5171\u8bc6\u8def\u7531\u5668\uff08MCR\uff09\u6765\u5f15\u5bfc\u878d\u5408\u8fc7\u7a0b\u8fbe\u6210\u66f4\u53ef\u9760\u7684\u5171\u8bc6\uff0c\u4ece\u800c\u7f13\u89e3\u6587\u672c\u4e3b\u5bfc\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eCMC\u5728\u56db\u4e2a\u6570\u636e\u96c6\uff08CH-SIMS\u3001CH-SIMS v2\u3001CMU-MOSI\u548cCMU-MOSEI\uff09\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5728CH-SIMS\u548cCH-SIMS v2\u6570\u636e\u96c6\u4e0a\u5bf9\u8bed\u4e49\u4e0d\u4e00\u81f4\u573a\u666f\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u81ea\u76d1\u7763\u5355\u6a21\u6001\u9884\u8bad\u7ec3\u548c\u5171\u8bc6\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u878d\u5408\u673a\u5236\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u8bed\u4e49\u4e0d\u4e00\u81f4\u548c\u6a21\u6001\u4e3b\u5bfc\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2510.20519", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20519", "abs": "https://arxiv.org/abs/2510.20519", "authors": ["Xiaohan Lan", "Fanfan Liu", "Haibo Qiu", "Siqi Yang", "Delian Ruan", "Peng Shi", "Lin Ma"], "title": "Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning", "comment": null, "summary": "Inspired by recent advancements in LLM reasoning, the field of multimodal\nreasoning has seen remarkable progress, achieving significant performance gains\non intricate tasks such as mathematical problem-solving. Despite this progress,\ncurrent multimodal large reasoning models exhibit two key limitations. They\ntend to employ computationally expensive reasoning even for simple queries,\nleading to inefficiency. Furthermore, this focus on specialized reasoning often\nimpairs their broader, more general understanding capabilities. In this paper,\nwe propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed\nto address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by\nstructuring the original dense model into two distinct expert branches: a\nthinking branch tailored for complex, multi-step reasoning, and a non-thinking\nbranch optimized for rapid, direct inference on tasks like general VQA and OCR.\nA lightweight, trainable router dynamically allocates queries to the most\nsuitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into\nan MoE architecture. Comprehensive evaluations reveal that our approach not\nonly substantially enhances complex reasoning abilities but also improves the\nmodel's general capabilities, reversing the degradation trend observed in other\nreasoning-specialized models. Our work establishes a new paradigm for building\npowerful and versatile MLLMs, effectively resolving the prevalent\nreasoning-vs-generalization dilemma.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMetis-HOME\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4f18\u5316\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u5b9e\u73b0\"\u6df7\u5408\u601d\u7ef4\"\u8303\u5f0f\uff0c\u5c06\u5bc6\u96c6\u6a21\u578b\u5206\u89e3\u4e3a\u601d\u8003\u5206\u652f\u548c\u975e\u601d\u8003\u5206\u652f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u63a8\u7406\u6548\u7387\u4e0e\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u63a8\u7406\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u5bf9\u7b80\u5355\u67e5\u8be2\u4e5f\u91c7\u7528\u8ba1\u7b97\u6602\u8d35\u7684\u63a8\u7406\u8fc7\u7a0b\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u4ee5\u53ca\u4e13\u6ce8\u4e8e\u4e13\u95e8\u5316\u63a8\u7406\u5f80\u5f80\u635f\u5bb3\u5176\u66f4\u5e7f\u6cdb\u7684\u901a\u7528\u7406\u89e3\u80fd\u529b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u79cd\u63a8\u7406\u4e0e\u6cdb\u5316\u4e4b\u95f4\u7684\u6743\u8861\u56f0\u5883\u3002", "method": "\u63d0\u51faMetis-HOME\u6df7\u5408\u4f18\u5316\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\uff0c\u5c06\u539f\u59cb\u5bc6\u96c6\u6a21\u578b\u7ed3\u6784\u5316\u5206\u4e3a\u4e24\u4e2a\u4e13\u5bb6\u5206\u652f\uff1a\u4e13\u4e3a\u590d\u6742\u591a\u6b65\u63a8\u7406\u8bbe\u8ba1\u7684\u601d\u8003\u5206\u652f\u548c\u9488\u5bf9\u901a\u7528VQA\u53caOCR\u7b49\u4efb\u52a1\u4f18\u5316\u7684\u5feb\u901f\u76f4\u63a5\u63a8\u7406\u975e\u601d\u8003\u5206\u652f\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u8def\u7531\u5668\u52a8\u6001\u5206\u914d\u67e5\u8be2\u5230\u6700\u5408\u9002\u7684\u4e13\u5bb6\uff0c\u57fa\u4e8eQwen2.5-VL-7B\u5b9e\u4f8b\u5316\u4e3aMoE\u67b6\u6784\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8868\u660e\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u663e\u8457\u589e\u5f3a\u4e86\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u6539\u5584\u4e86\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\uff0c\u9006\u8f6c\u4e86\u5176\u4ed6\u63a8\u7406\u4e13\u95e8\u5316\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u9000\u5316\u8d8b\u52bf\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u5f3a\u5927\u4e14\u901a\u7528\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u666e\u904d\u5b58\u5728\u7684\u63a8\u7406\u4e0e\u6cdb\u5316\u56f0\u5883\uff0c\u8bc1\u660e\u4e86\u6df7\u5408\u601d\u7ef4\u67b6\u6784\u5728\u5e73\u8861\u4e13\u95e8\u5316\u63a8\u7406\u4e0e\u901a\u7528\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.20212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20212", "abs": "https://arxiv.org/abs/2510.20212", "authors": ["Yanghao Wang", "Zhen Wang", "Long Chen"], "title": "FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing", "comment": null, "summary": "Recent advances in pre-trained text-to-image flow models have enabled\nremarkable progress in text-based image editing. Mainstream approaches always\nadopt a corruption-then-restoration paradigm, where the source image is first\ncorrupted into an ``intermediate state'' and then restored to the target image\nunder the prompt guidance. However, current methods construct this intermediate\nstate in a target-agnostic manner, i.e., they primarily focus on realizing\nsource image reconstruction while neglecting the semantic gaps towards the\nspecific editing target. This design inherently results in limited editability\nor inconsistency when the desired modifications substantially deviate from the\nsource. In this paper, we argue that the intermediate state should be\ntarget-aware, i.e., selectively corrupting editing-relevant contents while\npreserving editing-irrelevant ones. To this end, we propose FlowCycle, a novel\ninversion-free and flow-based editing framework that parameterizes corruption\nwith learnable noises and optimizes them through a cycle-consistent process. By\niteratively editing the source to the target and recovering back to the source\nwith dual consistency constraints, FlowCycle learns to produce a target-aware\nintermediate state, enabling faithful modifications while preserving source\nconsistency. Extensive ablations have demonstrated that FlowCycle achieves\nsuperior editing quality and consistency over state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFlowCycle\uff0c\u4e00\u79cd\u57fa\u4e8e\u6d41\u7684\u514d\u53cd\u6f14\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u76ee\u6807\u611f\u77e5\u7684\u4e2d\u95f4\u72b6\u6001\u6784\u5efa\u548c\u5faa\u73af\u4e00\u81f4\u6027\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u76ee\u6807\u65e0\u5173\u7684\u4e2d\u95f4\u72b6\u6001\u6784\u5efa\u5bfc\u81f4\u7684\u7f16\u8f91\u9650\u5236\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u91c7\u7528\u76ee\u6807\u65e0\u5173\u7684\u4e2d\u95f4\u72b6\u6001\u6784\u5efa\u65b9\u5f0f\uff0c\u4e3b\u8981\u5173\u6ce8\u6e90\u56fe\u50cf\u91cd\u5efa\u800c\u5ffd\u7565\u4e86\u4e0e\u7279\u5b9a\u7f16\u8f91\u76ee\u6807\u4e4b\u95f4\u7684\u8bed\u4e49\u5dee\u8ddd\uff0c\u8fd9\u5bfc\u81f4\u5f53\u671f\u671b\u4fee\u6539\u4e0e\u6e90\u56fe\u50cf\u663e\u8457\u504f\u79bb\u65f6\u51fa\u73b0\u6709\u9650\u7684\u7f16\u8f91\u80fd\u529b\u6216\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "FlowCycle\u6846\u67b6\u901a\u8fc7\u53c2\u6570\u5316\u53ef\u5b66\u4e60\u566a\u58f0\u6765\u6784\u5efa\u76ee\u6807\u611f\u77e5\u7684\u4e2d\u95f4\u72b6\u6001\uff0c\u91c7\u7528\u5faa\u73af\u4e00\u81f4\u6027\u8fc7\u7a0b\u8fdb\u884c\u4f18\u5316\uff0c\u901a\u8fc7\u4ece\u6e90\u5230\u76ee\u6807\u7684\u8fed\u4ee3\u7f16\u8f91\u548c\u4ece\u76ee\u6807\u56de\u6e90\u7684\u53cc\u5411\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u5b66\u4e60\u751f\u6210\u76ee\u6807\u611f\u77e5\u7684\u4e2d\u95f4\u72b6\u6001\u3002", "result": "\u5e7f\u6cdb\u7684\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0cFlowCycle\u5728\u7f16\u8f91\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5fe0\u5b9e\u7684\u4fee\u6539\u540c\u65f6\u4fdd\u6301\u6e90\u56fe\u50cf\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u76ee\u6807\u611f\u77e5\u4e2d\u95f4\u72b6\u6001\u6784\u5efa\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u8303\u5f0f\uff0c\u901a\u8fc7\u5faa\u73af\u4e00\u81f4\u6027\u5b66\u4e60\u5b9e\u73b0\u4e86\u7f16\u8f91\u76f8\u5173\u5185\u5bb9\u7684\u667a\u80fd\u9009\u62e9\u4fdd\u7559\u548c\u4fee\u6539\u3002"}}
{"id": "2510.20812", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20812", "abs": "https://arxiv.org/abs/2510.20812", "authors": ["Yuhan Liu", "Lianhui Qin", "Shengjie Wang"], "title": "Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation", "comment": null, "summary": "Large Vision-Language Models (VLMs) have achieved remarkable progress in\nmultimodal understanding, yet they struggle when reasoning over\ninformation-intensive images that densely interleave textual annotations with\nfine-grained graphical elements. The main challenges lie in precisely\nlocalizing critical cues in dense layouts and multi-hop reasoning to integrate\ndispersed evidence. We propose Speculative Verdict (SV), a training-free\nframework inspired by speculative decoding that combines multiple lightweight\ndraft experts with a large verdict model. In the draft stage, small VLMs act as\ndraft experts to generate reasoning paths that provide diverse localization\ncandidates; in the verdict stage, a strong VLM synthesizes these paths to\nproduce the final answer, minimizing computational cost while recovering\ncorrect answers. To further improve efficiency and accuracy, SV introduces a\nconsensus expert selection mechanism that forwards only high-agreement\nreasoning paths to the verdict. Empirically, SV achieves consistent gains on\nchallenging information-intensive and high-resolution visual question answering\nbenchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.\nBy synthesizing correct insights from multiple partially accurate reasoning\npaths, SV achieves both error correction and cost-efficiency compared to large\nproprietary models or training pipelines. Code is available at\nhttps://github.com/Tinaliu0123/speculative-verdict", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSpeculative Verdict (SV)\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u8f7b\u91cf\u7ea7\u8349\u7a3f\u4e13\u5bb6\u548c\u5927\u578b\u88c1\u51b3\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u5bc6\u96c6\u578b\u56fe\u50cf\u4e0a\u7684\u63a8\u7406\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9519\u8bef\u6821\u6b63\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u5bc6\u96c6\u578b\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u4e9b\u56fe\u50cf\u5bc6\u96c6\u4ea4\u7ec7\u6587\u672c\u6807\u6ce8\u4e0e\u7ec6\u7c92\u5ea6\u56fe\u5f62\u5143\u7d20\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u7cbe\u786e\u5b9a\u4f4d\u5bc6\u96c6\u5e03\u5c40\u4e2d\u7684\u5173\u952e\u7ebf\u7d22\u4ee5\u53ca\u6574\u5408\u5206\u6563\u8bc1\u636e\u7684\u591a\u8df3\u63a8\u7406\u3002", "method": "SV\u6846\u67b6\u91c7\u7528\u8bad\u7ec3\u514d\u8d39\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u5728\u8349\u7a3f\u9636\u6bb5\u4f7f\u7528\u5c0f\u578bVLM\u4f5c\u4e3a\u8349\u7a3f\u4e13\u5bb6\u751f\u6210\u591a\u6837\u5316\u7684\u5b9a\u4f4d\u5019\u9009\u63a8\u7406\u8def\u5f84\uff0c\u5728\u88c1\u51b3\u9636\u6bb5\u7531\u5f3a\u5927VLM\u5408\u6210\u8fd9\u4e9b\u8def\u5f84\u4ea7\u751f\u6700\u7ec8\u7b54\u6848\uff0c\u5e76\u5f15\u5165\u5171\u8bc6\u4e13\u5bb6\u9009\u62e9\u673a\u5236\u4ec5\u8f6c\u53d1\u9ad8\u4e00\u81f4\u6027\u63a8\u7406\u8def\u5f84\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4fe1\u606f\u5bc6\u96c6\u548c\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5305\u62ecInfographicVQA\u3001ChartMuseum\u3001ChartQAPro\u548cHR-Bench 4K\uff0cSV\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\uff0c\u901a\u8fc7\u5408\u6210\u591a\u4e2a\u90e8\u5206\u51c6\u786e\u63a8\u7406\u8def\u5f84\u4e2d\u7684\u6b63\u786e\u89c1\u89e3\uff0c\u76f8\u6bd4\u5927\u578b\u4e13\u6709\u6a21\u578b\u6216\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\u4e86\u9519\u8bef\u6821\u6b63\u548c\u6210\u672c\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u5408\u6210\u591a\u4e2a\u90e8\u5206\u51c6\u786e\u63a8\u7406\u8def\u5f84\u53ef\u4ee5\u6709\u6548\u5730\u8fdb\u884c\u9519\u8bef\u6821\u6b63\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5904\u7406\u4fe1\u606f\u5bc6\u96c6\u578b\u89c6\u89c9\u5185\u5bb9\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e0e\u5927\u578b\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.20531", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20531", "abs": "https://arxiv.org/abs/2510.20531", "authors": ["Lixiong Qin", "Yang Zhang", "Mei Wang", "Jiani Hu", "Weihong Deng", "Weiran Xu"], "title": "Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis", "comment": "25 pages, 9 figures, 17 tables", "summary": "The advancement of Multimodal Large Language Models (MLLMs) has bridged the\ngap between vision and language tasks, enabling the implementation of\nExplainable DeepFake Analysis (XDFA). However, current methods suffer from a\nlack of fine-grained awareness: the description of artifacts in data annotation\nis unreliable and coarse-grained, and the models fail to support the output of\nconnections between textual forgery explanations and the visual evidence of\nartifacts, as well as the input of queries for arbitrary facial regions. As a\nresult, their responses are not sufficiently grounded in Face Visual Context\n(Facext). To address this limitation, we propose the Fake-in-Facext (FiFa)\nframework, with contributions focusing on data annotation and model\nconstruction. We first define a Facial Image Concept Tree (FICT) to divide\nfacial images into fine-grained regional concepts, thereby obtaining a more\nreliable data annotation pipeline, FiFa-Annotator, for forgery explanation.\nBased on this dedicated data annotation, we introduce a novel\nArtifact-Grounding Explanation (AGE) task, which generates textual forgery\nexplanations interleaved with segmentation masks of manipulated artifacts. We\npropose a unified multi-task learning architecture, FiFa-MLLM, to\nsimultaneously support abundant multimodal inputs and outputs for fine-grained\nExplainable DeepFake Analysis. With multiple auxiliary supervision tasks,\nFiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA\nperformance on existing XDFA datasets. The code and data will be made\nopen-source at https://github.com/lxq1000/Fake-in-Facext.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Fake-in-Facext\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u7ec6\u7c92\u5ea6\u9762\u90e8\u6982\u5ff5\u6811\u548c\u6784\u5efa\u591a\u4efb\u52a1\u5b66\u4e60\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6df1\u5ea6\u4f2a\u9020\u5206\u6790\u4e2d\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u7684\u95ee\u9898\uff0c\u5728\u4f2a\u9020\u89e3\u91ca\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6df1\u5ea6\u4f2a\u9020\u5206\u6790\u4e2d\u5b58\u5728\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u6570\u636e\u6807\u6ce8\u4e2d\u5bf9\u4f2a\u9020\u75d5\u8ff9\u7684\u63cf\u8ff0\u4e0d\u53ef\u9760\u4e14\u7c92\u5ea6\u7c97\u7cd9\uff0c\u6a21\u578b\u65e0\u6cd5\u8f93\u51fa\u6587\u672c\u4f2a\u9020\u89e3\u91ca\u4e0e\u89c6\u89c9\u4f2a\u9020\u8bc1\u636e\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u4e5f\u4e0d\u652f\u6301\u5bf9\u4efb\u610f\u9762\u90e8\u533a\u57df\u7684\u67e5\u8be2\u8f93\u5165\uff0c\u5bfc\u81f4\u5176\u54cd\u5e94\u7f3a\u4e4f\u9762\u90e8\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u5145\u5206\u652f\u6491\u3002", "method": "\u63d0\u51fa\u4e86Fake-in-Facext\u6846\u67b6\uff0c\u9996\u5148\u5b9a\u4e49\u4e86\u9762\u90e8\u56fe\u50cf\u6982\u5ff5\u6811\u5c06\u9762\u90e8\u56fe\u50cf\u5212\u5206\u4e3a\u7ec6\u7c92\u5ea6\u533a\u57df\u6982\u5ff5\uff0c\u6784\u5efa\u4e86\u66f4\u53ef\u9760\u7684\u6570\u636e\u6807\u6ce8\u6d41\u7a0bFiFa-Annotator\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165\u4e86\u65b0\u7684\u4f2a\u9020\u5b9a\u4f4d\u89e3\u91ca\u4efb\u52a1\uff0c\u751f\u6210\u4e0e\u5206\u5272\u63a9\u7801\u4ea4\u7ec7\u7684\u6587\u672c\u4f2a\u9020\u89e3\u91ca\uff1b\u5f00\u53d1\u4e86\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u67b6\u6784FiFa-MLLM\uff0c\u540c\u65f6\u652f\u6301\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u8f93\u5165\u8f93\u51fa\u3002", "result": "\u901a\u8fc7\u591a\u4e2a\u8f85\u52a9\u76d1\u7763\u4efb\u52a1\uff0cFiFa-MLLM\u5728\u4f2a\u9020\u5b9a\u4f4d\u89e3\u91ca\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5728\u73b0\u6709\u53ef\u89e3\u91ca\u6df1\u5ea6\u4f2a\u9020\u5206\u6790\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u7ec6\u7c92\u5ea6\u9762\u90e8\u6982\u5ff5\u5212\u5206\u548c\u7edf\u4e00\u591a\u4efb\u52a1\u67b6\u6784\u5728\u63d0\u5347\u6df1\u5ea6\u4f2a\u9020\u5206\u6790\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u53ef\u89e3\u91caAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2510.20214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20214", "abs": "https://arxiv.org/abs/2510.20214", "authors": ["Talha Ilyas", "Duong Nhu", "Allison Thomas", "Arie Levin", "Lim Wei Yap", "Shu Gong", "David Vera Anaya", "Yiwen Jiang", "Deval Mehta", "Ritesh Warty", "Vinayak Smith", "Maya Reddy", "Euan Wallace", "Wenlong Cheng", "Zongyuan Ge", "Faezeh Marzbanrad"], "title": "Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection", "comment": "This is the preprint version of the manuscript submitted to IEEE\n  Journal of Biomedical and Health Informatics (JBHI) for review", "summary": "Accurate fetal movement (FM) detection is essential for assessing prenatal\nhealth, as abnormal movement patterns can indicate underlying complications\nsuch as placental dysfunction or fetal distress. Traditional methods, including\nmaternal perception and cardiotocography (CTG), suffer from subjectivity and\nlimited accuracy. To address these challenges, we propose Contrastive\nUltrasound Video Representation Learning (CURL), a novel self-supervised\nlearning framework for FM detection from extended fetal ultrasound video\nrecordings. Our approach leverages a dual-contrastive loss, incorporating both\nspatial and temporal contrastive learning, to learn robust motion\nrepresentations. Additionally, we introduce a task-specific sampling strategy,\nensuring the effective separation of movement and non-movement segments during\nself-supervised training, while enabling flexible inference on arbitrarily long\nultrasound recordings through a probabilistic fine-tuning approach. Evaluated\non an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions,\nCURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating its\npotential for reliable and objective FM analysis. These results highlight the\npotential of self-supervised contrastive learning for fetal movement analysis,\npaving the way for improved prenatal monitoring and clinical decision-making.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86CURL\uff08\u5bf9\u6bd4\u8d85\u58f0\u89c6\u9891\u8868\u793a\u5b66\u4e60\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u4ece\u80ce\u513f\u8d85\u58f0\u89c6\u9891\u4e2d\u68c0\u6d4b\u80ce\u513f\u8fd0\u52a8\uff0c\u572892\u540d\u53d7\u8bd5\u8005\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8678.01%\u7684\u654f\u611f\u5ea6\u548c81.60%\u7684AUROC\uff0c\u4e3a\u4ea7\u524d\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5ba2\u89c2\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u80ce\u513f\u8fd0\u52a8\u68c0\u6d4b\u65b9\u6cd5\u5982\u6bcd\u4f53\u611f\u77e5\u548c\u80ce\u5fc3\u76d1\u62a4\u5b58\u5728\u4e3b\u89c2\u6027\u5f3a\u548c\u51c6\u786e\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u5f02\u5e38\u8fd0\u52a8\u6a21\u5f0f\u53ef\u80fd\u6307\u793a\u80ce\u76d8\u529f\u80fd\u969c\u788d\u6216\u80ce\u513f\u7a98\u8feb\u7b49\u5e76\u53d1\u75c7\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u5ba2\u89c2\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86CURL\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u91cd\u5bf9\u6bd4\u635f\u5931\u7ed3\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u5bf9\u6bd4\u5b66\u4e60\u6765\u5b66\u4e60\u9c81\u68d2\u7684\u8fd0\u52a8\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u91c7\u6837\u7b56\u7565\u6709\u6548\u5206\u79bb\u8fd0\u52a8\u548c\u975e\u8fd0\u52a8\u7247\u6bb5\uff0c\u901a\u8fc7\u6982\u7387\u5fae\u8c03\u65b9\u6cd5\u5b9e\u73b0\u4efb\u610f\u957f\u5ea6\u8d85\u58f0\u8bb0\u5f55\u4e0a\u7684\u7075\u6d3b\u63a8\u7406\u3002", "result": "\u5728\u5305\u542b92\u540d\u53d7\u8bd5\u8005\u3001\u6bcf\u4f8b30\u5206\u949f\u8d85\u58f0\u4f1a\u8bdd\u7684\u5185\u90e8\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cCURL\u5b9e\u73b0\u4e8678.01%\u7684\u654f\u611f\u5ea6\u548c81.60%\u7684AUROC\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u80ce\u513f\u8fd0\u52a8\u5206\u6790\u4e2d\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u5728\u80ce\u513f\u8fd0\u52a8\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u6539\u8fdb\u4ea7\u524d\u76d1\u6d4b\u548c\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u5ba2\u89c2\u53ef\u9760\u7684\u80ce\u513f\u8fd0\u52a8\u8bc4\u4f30\uff0c\u6709\u671b\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2510.20579", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.20579", "abs": "https://arxiv.org/abs/2510.20579", "authors": ["Jiahao Meng", "Xiangtai Li", "Haochen Wang", "Yue Tan", "Tao Zhang", "Lingdong Kong", "Yunhai Tong", "Anran Wang", "Zhiyang Teng", "Yujing Wang", "Zhuochen Wang"], "title": "Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence", "comment": null, "summary": "Most video reasoning models only generate textual reasoning traces without\nindicating when and where key evidence appears. Recent models such as OpenAI-o3\nhave sparked wide interest in evidence-centered reasoning for images, yet\nextending this ability to videos is more challenging, as it requires joint\ntemporal tracking and spatial localization across dynamic scenes. We introduce\nOpen-o3 Video, a non-agent framework that integrates explicit spatio-temporal\nevidence into video reasoning, and carefully collect training data and design\ntraining strategies to address the aforementioned challenges. The model\nhighlights key timestamps, objects, and bounding boxes alongside its answers,\nallowing reasoning to be grounded in concrete visual observations. To enable\nthis functionality, we first curate and build two high-quality datasets,\nSTGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed\ntemporal and spatial annotations, since most existing datasets offer either\ntemporal spans for videos or spatial boxes on images, lacking unified\nspatio-temporal supervision and reasoning traces. Then, we adopt a cold-start\nreinforcement learning strategy with multiple specially designed rewards that\njointly encourage answer accuracy, temporal alignment, and spatial precision.\nOn V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,\nraising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent\nimprovements are also observed on a broad range of video understanding\nbenchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond\naccuracy, the reasoning traces produced by Open-o3 Video also provide valuable\nsignals for test-time scaling, enabling confidence-aware verification and\nimproving answer reliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Open-o3 Video\u6846\u67b6\uff0c\u5c06\u663e\u5f0f\u65f6\u7a7a\u8bc1\u636e\u6574\u5408\u5230\u89c6\u9891\u63a8\u7406\u4e2d\uff0c\u901a\u8fc7\u7cbe\u5fc3\u6784\u5efa\u7684\u6570\u636e\u96c6\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5728V-STAR\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u63a8\u7406\u6a21\u578b\u4ec5\u751f\u6210\u6587\u672c\u63a8\u7406\u8f68\u8ff9\uff0c\u65e0\u6cd5\u6307\u793a\u5173\u952e\u8bc1\u636e\u51fa\u73b0\u7684\u65f6\u95f4\u548c\u4f4d\u7f6e\uff0c\u800c\u5c06\u8bc1\u636e\u4e2d\u5fc3\u63a8\u7406\u80fd\u529b\u6269\u5c55\u5230\u89c6\u9891\u9762\u4e34\u8054\u5408\u65f6\u95f4\u8ddf\u8e2a\u548c\u7a7a\u95f4\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u7edf\u4e00\u7684\u65f6\u7a7a\u76d1\u7763\u548c\u63a8\u7406\u8f68\u8ff9\u3002", "method": "\u63d0\u51fa\u975e\u667a\u80fd\u4f53\u6846\u67b6Open-o3 Video\uff0c\u6574\u5408\u663e\u5f0f\u65f6\u7a7a\u8bc1\u636e\u5230\u89c6\u9891\u63a8\u7406\u4e2d\uff1b\u6784\u5efa\u4e24\u4e2a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6STGR-CoT-30k\u7528\u4e8eSFT\u548cSTGR-RL-36k\u7528\u4e8eRL\uff0c\u5305\u542b\u7cbe\u5fc3\u6784\u5efa\u7684\u65f6\u7a7a\u6807\u6ce8\uff1b\u91c7\u7528\u51b7\u542f\u52a8\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u8bbe\u8ba1\u591a\u4e2a\u4e13\u95e8\u5956\u52b1\u51fd\u6570\u8054\u5408\u4fc3\u8fdb\u7b54\u6848\u51c6\u786e\u6027\u3001\u65f6\u95f4\u5bf9\u9f50\u548c\u7a7a\u95f4\u7cbe\u5ea6\u3002", "result": "\u5728V-STAR\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4Qwen2.5-VL\u57fa\u7ebf\u5c06mAM\u63d0\u534714.4%\uff0cmLGM\u63d0\u534724.2%\uff1b\u5728VideoMME\u3001WorldSense\u3001VideoMMMU\u548cTVGBench\u7b49\u591a\u4e2a\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u89c2\u5bdf\u5230\u4e00\u81f4\u6539\u8fdb\uff1b\u63a8\u7406\u8f68\u8ff9\u4e3a\u6d4b\u8bd5\u65f6\u7f29\u653e\u63d0\u4f9b\u6709\u4ef7\u503c\u4fe1\u53f7\uff0c\u5b9e\u73b0\u7f6e\u4fe1\u5ea6\u611f\u77e5\u9a8c\u8bc1\u5e76\u63d0\u9ad8\u7b54\u6848\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u663e\u5f0f\u65f6\u7a7a\u8bc1\u636e\u6574\u5408\u5230\u89c6\u9891\u63a8\u7406\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u6027\u80fd\u8fd8\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8fc7\u7a0b\uff1b\u63d0\u51fa\u7684\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4e3a\u89e3\u51b3\u89c6\u9891\u65f6\u7a7a\u63a8\u7406\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff1b\u63a8\u7406\u8f68\u8ff9\u7684\u7f6e\u4fe1\u5ea6\u611f\u77e5\u80fd\u529b\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u9a8c\u8bc1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.20596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20596", "abs": "https://arxiv.org/abs/2510.20596", "authors": ["Ziyu Ye", "Chen Ju", "Chaofan Ma", "Xiaoyun Zhang"], "title": "Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation", "comment": "MICCAI 2021", "summary": "Deep learning models have achieved great success on various vision\nchallenges, but a well-trained model would face drastic performance degradation\nwhen applied to unseen data. Since the model is sensitive to domain shift,\nunsupervised domain adaptation attempts to reduce the domain gap and avoid\ncostly annotation of unseen domains. This paper proposes a novel framework for\ncross-modality segmentation via similarity-based prototypes. In specific, we\nlearn class-wise prototypes within an embedding space, then introduce a\nsimilarity constraint to make these prototypes representative for each semantic\nclass while separable from different classes. Moreover, we use dictionaries to\nstore prototypes extracted from different images, which prevents the\nclass-missing problem and enables the contrastive learning of prototypes, and\nfurther improves performance. Extensive experiments show that our method\nachieves better results than other state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u539f\u578b\u7684\u8de8\u6a21\u6001\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u7c7b\u522b\u539f\u578b\u5e76\u5f15\u5165\u76f8\u4f3c\u6027\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9886\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u7c7b\u522b\u7f3a\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5f53\u5e94\u7528\u4e8e\u672a\u89c1\u6570\u636e\u65f6\u4f1a\u51fa\u73b0\u6027\u80fd\u6025\u5267\u4e0b\u964d\u7684\u95ee\u9898\u3002\u7531\u4e8e\u6a21\u578b\u5bf9\u9886\u57df\u504f\u79fb\u654f\u611f\uff0c\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u65e8\u5728\u51cf\u5c11\u9886\u57df\u5dee\u8ddd\u5e76\u907f\u514d\u5bf9\u65b0\u9886\u57df\u8fdb\u884c\u6602\u8d35\u7684\u6807\u6ce8\u5de5\u4f5c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u539f\u578b\u7684\u8de8\u6a21\u6001\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u7c7b\u522b\u539f\u578b\u5e76\u5f15\u5165\u76f8\u4f3c\u6027\u7ea6\u675f\uff0c\u4f7f\u8fd9\u4e9b\u539f\u578b\u80fd\u591f\u4ee3\u8868\u6bcf\u4e2a\u8bed\u4e49\u7c7b\u522b\u540c\u65f6\u4e0e\u4e0d\u540c\u7c7b\u522b\u4fdd\u6301\u5206\u79bb\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u5b57\u5178\u5b58\u50a8\u4ece\u4e0d\u540c\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u539f\u578b\uff0c\u9632\u6b62\u7c7b\u522b\u7f3a\u5931\u95ee\u9898\u5e76\u5b9e\u73b0\u539f\u578b\u7684\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u5206\u5272\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6bd4\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u5728\u63d0\u5347\u5206\u5272\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8e\u76f8\u4f3c\u6027\u539f\u578b\u7684\u6846\u67b6\u5728\u8de8\u6a21\u6001\u5206\u5272\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u539f\u578b\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u9886\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.20244", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20244", "abs": "https://arxiv.org/abs/2510.20244", "authors": ["Minseok Kang", "Minhyeok Lee", "Minjung Kim", "Donghyeong Kim", "Sangyoun Lee"], "title": "Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding", "comment": "Comments: 28 pages, including appendix. 5 figures. Full version of\n  the NeurIPS 2025 paper", "summary": "Video Temporal Grounding (VTG) aims to localize temporal segments in long,\nuntrimmed videos that align with a given natural language query. This task\ntypically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection\n(HD). While recent advances have been progressed by powerful pretrained\nvision-language models such as CLIP and InternVideo2, existing approaches\ncommonly treat all text tokens uniformly during crossmodal attention,\ndisregarding their distinct semantic roles. To validate the limitations of this\napproach, we conduct controlled experiments demonstrating that VTG models\noverly rely on [EOS]-driven global semantics while failing to effectively\nutilize word-level signals, which limits their ability to achieve fine-grained\ntemporal alignment. Motivated by this limitation, we propose DualGround, a\ndual-branch architecture that explicitly separates global and local semantics\nby routing the [EOS] token through a sentence-level path and clustering word\ntokens into phrase-level units for localized grounding. Our method introduces\n(1) tokenrole- aware cross modal interaction strategies that align video\nfeatures with sentence-level and phrase-level semantics in a structurally\ndisentangled manner, and (2) a joint modeling framework that not only improves\nglobal sentence-level alignment but also enhances finegrained temporal\ngrounding by leveraging structured phrase-aware context. This design allows the\nmodel to capture both coarse and localized semantics, enabling more expressive\nand context-aware video grounding. DualGround achieves state-of-the-art\nperformance on both Moment Retrieval and Highlight Detection tasks across\nQVHighlights and Charades- STA benchmarks, demonstrating the effectiveness of\ndisentangled semantic modeling in video-language alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDualGround\uff0c\u4e00\u79cd\u53cc\u5206\u652f\u67b6\u6784\u7684\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06[EOS]\u6807\u8bb0\u4e0e\u8bcd\u6807\u8bb0\u5206\u522b\u8def\u7531\u5230\u53e5\u5b50\u7ea7\u548c\u77ed\u8bed\u7ea7\u8def\u5f84\uff0c\u5b9e\u73b0\u5168\u5c40\u4e0e\u5c40\u90e8\u8bed\u4e49\u7684\u663e\u5f0f\u5206\u79bb\uff0c\u4ece\u800c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u6587\u672c\u6807\u8bb0\u8bed\u4e49\u89d2\u8272\u5dee\u5f02\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u65b9\u6cd5\u901a\u5e38\u5c06\u6240\u6709\u6587\u672c\u6807\u8bb0\u5728\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u4e2d\u7edf\u4e00\u5904\u7406\uff0c\u5ffd\u89c6\u4e86\u5b83\u4eec\u4e0d\u540c\u7684\u8bed\u4e49\u89d2\u8272\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56[EOS]\u9a71\u52a8\u7684\u5168\u5c40\u8bed\u4e49\u800c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u8bcd\u7ea7\u4fe1\u53f7\uff0c\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u5bf9\u9f50\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u53cc\u5206\u652f\u67b6\u6784DualGround\uff0c\u901a\u8fc7\u5c06[EOS]\u6807\u8bb0\u8def\u7531\u5230\u53e5\u5b50\u7ea7\u8def\u5f84\u5e76\u5c06\u8bcd\u6807\u8bb0\u805a\u7c7b\u4e3a\u77ed\u8bed\u7ea7\u5355\u5143\uff0c\u5b9e\u73b0\u5168\u5c40\u4e0e\u5c40\u90e8\u8bed\u4e49\u7684\u663e\u5f0f\u5206\u79bb\uff1b\u5f15\u5165\u6807\u8bb0\u89d2\u8272\u611f\u77e5\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u7b56\u7565\uff0c\u4ee5\u7ed3\u6784\u89e3\u8026\u7684\u65b9\u5f0f\u5bf9\u9f50\u89c6\u9891\u7279\u5f81\u4e0e\u53e5\u5b50\u7ea7\u548c\u77ed\u8bed\u7ea7\u8bed\u4e49\uff1b\u91c7\u7528\u8054\u5408\u5efa\u6a21\u6846\u67b6\u540c\u65f6\u63d0\u5347\u5168\u5c40\u53e5\u5b50\u7ea7\u5bf9\u9f50\u548c\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u5b9a\u4f4d\u80fd\u529b\u3002", "result": "DualGround\u5728QVHighlights\u548cCharades-STA\u57fa\u51c6\u6d4b\u8bd5\u7684Moment Retrieval\u548cHighlight Detection\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8bed\u4e49\u89e3\u8026\u5efa\u6a21\u5728\u89c6\u9891-\u8bed\u8a00\u5bf9\u9f50\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u663e\u5f0f\u5206\u79bb\u5168\u5c40\u548c\u5c40\u90e8\u8bed\u4e49\u53ef\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\uff0c\u89e3\u8026\u7684\u8bed\u4e49\u5efa\u6a21\u80fd\u591f\u6709\u6548\u63d0\u5347\u89c6\u9891-\u8bed\u8a00\u5bf9\u9f50\u7684\u8868\u8fbe\u80fd\u529b\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\u3002"}}
{"id": "2510.20268", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.20268", "abs": "https://arxiv.org/abs/2510.20268", "authors": ["Guangyu Dai", "Dong Chen", "Siliang Tang", "Yueting Zhuang"], "title": "GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection", "comment": null, "summary": "Video anomaly detection (VAD) is a challenging task that detects anomalous\nframes in continuous surveillance videos. Most previous work utilizes the\nspatio-temporal correlation of visual features to distinguish whether there are\nabnormalities in video snippets. Recently, some works attempt to introduce\nmulti-modal information, like text feature, to enhance the results of video\nanomaly detection. However, these works merely incorporate text features into\nvideo snippets in a coarse manner, overlooking the significant amount of\nredundant information that may exist within the video snippets. Therefore, we\npropose to leverage the diversity among multi-modal information to further\nrefine the extracted features, reducing the redundancy in visual features, and\nwe propose Grained Multi-modal Feature for Video Anomaly Detection (GMFVAD).\nSpecifically, we generate more grained multi-modal feature based on the video\nsnippet, which summarizes the main content, and text features based on the\ncaptions of original video will be introduced to further enhance the visual\nfeatures of highlighted portions. Experiments show that the proposed GMFVAD\nachieves state-of-the-art performance on four mainly datasets. Ablation\nexperiments also validate that the improvement of GMFVAD is due to the\nreduction of redundant information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGMFVAD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u7684\u591a\u6837\u6027\u6765\u7ec6\u5316\u63d0\u53d6\u7279\u5f81\uff0c\u51cf\u5c11\u89c6\u89c9\u7279\u5f81\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u5347\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u56db\u4e2a\u4e3b\u8981\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u867d\u7136\u5c1d\u8bd5\u5f15\u5165\u6587\u672c\u7b49\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u4f46\u4ec5\u4ee5\u7c97\u7565\u65b9\u5f0f\u5c06\u6587\u672c\u7279\u5f81\u6574\u5408\u5230\u89c6\u9891\u7247\u6bb5\u4e2d\uff0c\u5ffd\u7565\u4e86\u89c6\u9891\u7247\u6bb5\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u5927\u91cf\u5197\u4f59\u4fe1\u606f\uff0c\u8fd9\u9650\u5236\u4e86\u68c0\u6d4b\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u63d0\u51faGrained Multi-modal Feature for Video Anomaly Detection (GMFVAD)\uff0c\u57fa\u4e8e\u89c6\u9891\u7247\u6bb5\u751f\u6210\u66f4\u7ec6\u7c92\u5ea6\u7684\u591a\u6a21\u6001\u7279\u5f81\uff0c\u901a\u8fc7\u603b\u7ed3\u4e3b\u8981\u5185\u5bb9\u5e76\u5f15\u5165\u57fa\u4e8e\u539f\u59cb\u89c6\u9891\u5b57\u5e55\u7684\u6587\u672c\u7279\u5f81\u6765\u8fdb\u4e00\u6b65\u589e\u5f3a\u7a81\u51fa\u90e8\u5206\u7684\u89c6\u89c9\u7279\u5f81\u3002", "result": "GMFVAD\u5728\u56db\u4e2a\u4e3b\u8981\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6027\u80fd\u63d0\u5347\u786e\u5b9e\u6e90\u4e8e\u5197\u4f59\u4fe1\u606f\u7684\u51cf\u5c11\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u591a\u6a21\u6001\u4fe1\u606f\u7684\u7ec6\u7c92\u5ea6\u878d\u5408\u80fd\u591f\u6709\u6548\u51cf\u5c11\u89c6\u89c9\u7279\u5f81\u5197\u4f59\uff0c\u4e3a\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u7279\u5f81\u589e\u5f3a\u601d\u8def\uff0c\u5f3a\u8c03\u4e86\u5197\u4f59\u4fe1\u606f\u6d88\u9664\u5728\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.20285", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.20285", "abs": "https://arxiv.org/abs/2510.20285", "authors": ["Jiayi Zou", "Chaofan Chen", "Bing-Kun Bao", "Changsheng Xu"], "title": "DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering", "comment": null, "summary": "Egocentric Video Question Answering (Egocentric VideoQA) plays an important\nrole in egocentric video understanding, which refers to answering questions\nbased on first-person videos. Although existing methods have made progress\nthrough the paradigm of pre-training and fine-tuning, they ignore the unique\nchallenges posed by the first-person perspective, such as understanding\nmultiple events and recognizing hand-object interactions. To deal with these\nchallenges, we propose a Dual-Modal Counterfactual Contrastive Construction\n(DMC$^3$) framework, which contains an egocentric videoqa baseline, a\ncounterfactual sample construction module and a counterfactual sample-involved\ncontrastive optimization. Specifically, We first develop a counterfactual\nsample construction module to generate positive and negative samples for\ntextual and visual modalities through event description paraphrasing and core\ninteraction mining, respectively. Then, We feed these samples together with the\noriginal samples into the baseline. Finally, in the counterfactual\nsample-involved contrastive optimization module, we apply contrastive loss to\nminimize the distance between the original sample features and the positive\nsample features, while maximizing the distance from the negative samples.\nExperiments show that our method achieve 52.51\\% and 46.04\\% on the\n\\textit{normal} and \\textit{indirect} splits of EgoTaskQA, and 13.2\\% on\nQAEGO4D, both reaching the state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u53cc\u6a21\u6001\u53cd\u4e8b\u5b9e\u5bf9\u6bd4\u6784\u5efa\uff08DMC\u00b3\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6837\u672c\u6784\u5efa\u548c\u5bf9\u6bd4\u4f18\u5316\u6765\u89e3\u51b3\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u591a\u4e8b\u4ef6\u7406\u89e3\u548c\u624b\u7269\u4ea4\u4e92\u8bc6\u522b\u6311\u6218\uff0c\u5728EgoTaskQA\u548cQAEGO4D\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u95ee\u7b54\u4e2d\u867d\u7136\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u8303\u5f0f\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5ffd\u7565\u4e86\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5e26\u6765\u7684\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u7406\u89e3\u591a\u4e2a\u4e8b\u4ef6\u548c\u8bc6\u522b\u624b\u7269\u4ea4\u4e92\uff0c\u8fd9\u4e9b\u9650\u5236\u5f71\u54cd\u4e86\u6a21\u578b\u5bf9\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7684\u6df1\u5ea6\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7684DMC\u00b3\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u95ee\u7b54\u57fa\u7ebf\u6a21\u578b\u3001\u53cd\u4e8b\u5b9e\u6837\u672c\u6784\u5efa\u6a21\u5757\u548c\u53cd\u4e8b\u5b9e\u6837\u672c\u53c2\u4e0e\u7684\u5bf9\u6bd4\u4f18\u5316\u6a21\u5757\uff0c\u5176\u4e2d\u53cd\u4e8b\u5b9e\u6837\u672c\u6784\u5efa\u901a\u8fc7\u4e8b\u4ef6\u63cf\u8ff0\u6539\u5199\u548c\u6838\u5fc3\u4ea4\u4e92\u6316\u6398\u5206\u522b\u751f\u6210\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u6b63\u8d1f\u6837\u672c\uff0c\u7136\u540e\u4e0e\u539f\u59cb\u6837\u672c\u4e00\u8d77\u8f93\u5165\u57fa\u7ebf\u6a21\u578b\uff0c\u6700\u540e\u901a\u8fc7\u5bf9\u6bd4\u635f\u5931\u6700\u5c0f\u5316\u539f\u59cb\u6837\u672c\u4e0e\u6b63\u6837\u672c\u7279\u5f81\u8ddd\u79bb\uff0c\u540c\u65f6\u6700\u5927\u5316\u4e0e\u8d1f\u6837\u672c\u8ddd\u79bb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728EgoTaskQA\u7684normal\u548cindirect\u5206\u5272\u4e0a\u5206\u522b\u8fbe\u523052.51%\u548c46.04%\u7684\u51c6\u786e\u7387\uff0c\u5728QAEGO4D\u4e0a\u8fbe\u523013.2%\u7684\u51c6\u786e\u7387\uff0c\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u53cd\u4e8b\u5b9e\u5bf9\u6bd4\u5b66\u4e60\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u591a\u4e8b\u4ef6\u7406\u89e3\u548c\u624b\u7269\u4ea4\u4e92\u8bc6\u522b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u548c\u4f18\u5316\u7b56\u7565\u3002"}}
{"id": "2510.20322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20322", "abs": "https://arxiv.org/abs/2510.20322", "authors": ["Zelin Peng", "Zhengqin Xu", "Qingyang Liu", "Xiaokang Yang", "Wei Shen"], "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models", "comment": "Accepted by NeurIPS2025", "summary": "Multi-modal large language models (MLLMs) have emerged as a transformative\napproach for aligning visual and textual understanding. They typically require\nextremely high computational resources (e.g., thousands of GPUs) for training\nto achieve cross-modal alignment at multi-granularity levels. We argue that a\nkey source of this inefficiency lies in the vision encoders they widely equip\nwith, e.g., CLIP and SAM, which lack the alignment with language at\nmulti-granularity levels. To address this issue, in this paper, we leverage\nhyperbolic space, which inherently models hierarchical levels and thus provides\na principled framework for bridging the granularity gap between visual and\ntextual modalities at an arbitrary granularity level. Concretely, we propose an\nefficient training paradigm for MLLMs, dubbed as HyperET, which can optimize\nvisual representations to align with their textual counterparts at an arbitrary\ngranularity level through dynamic hyperbolic radius adjustment in hyperbolic\nspace. HyperET employs learnable matrices with M\\\"{o}bius multiplication\noperations, implemented via three effective configurations: diagonal scaling\nmatrices, block-diagonal matrices, and banded matrices, providing a flexible\nyet efficient parametrization strategy. Comprehensive experiments across\nmultiple MLLM benchmarks demonstrate that HyperET consistently improves both\nexisting pre-training and fine-tuning MLLMs clearly with less than 1\\%\nadditional parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHyperET\uff0c\u4e00\u79cd\u57fa\u4e8e\u53cc\u66f2\u7a7a\u95f4\u7684\u9ad8\u6548\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u53cc\u66f2\u534a\u5f84\u5b9e\u73b0\u89c6\u89c9\u4e0e\u6587\u672c\u8868\u5f81\u5728\u4efb\u610f\u7c92\u5ea6\u7ea7\u522b\u7684\u5bf9\u9f50\uff0c\u4ec5\u9700\u589e\u52a0\u4e0d\u52301%\u7684\u53c2\u6570\u5373\u53ef\u663e\u8457\u63d0\u5347\u73b0\u6709MLLM\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6781\u9ad8\u7684\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u8bad\u7ec3\uff0c\u5176\u6839\u672c\u539f\u56e0\u5728\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff08\u5982CLIP\u548cSAM\uff09\u7f3a\u4e4f\u4e0e\u8bed\u8a00\u5728\u591a\u4e2a\u7c92\u5ea6\u7ea7\u522b\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u5bfc\u81f4\u8de8\u6a21\u6001\u5bf9\u9f50\u6548\u7387\u4f4e\u4e0b\u3002", "method": "HyperET\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u5929\u7136\u5efa\u6a21\u5c42\u6b21\u7ed3\u6784\u7684\u7279\u70b9\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u77e9\u9635\u4e0eM\u00f6bius\u4e58\u6cd5\u64cd\u4f5c\u5b9e\u73b0\u52a8\u6001\u53cc\u66f2\u534a\u5f84\u8c03\u6574\uff0c\u91c7\u7528\u5bf9\u89d2\u7f29\u653e\u77e9\u9635\u3001\u5757\u5bf9\u89d2\u77e9\u9635\u548c\u5e26\u72b6\u77e9\u9635\u4e09\u79cd\u9ad8\u6548\u53c2\u6570\u5316\u7b56\u7565\uff0c\u4f18\u5316\u89c6\u89c9\u8868\u5f81\u4e0e\u6587\u672c\u8868\u5f81\u5728\u4efb\u610f\u7c92\u5ea6\u7ea7\u522b\u7684\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2aMLLM\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cHyperET\u80fd\u591f\u6301\u7eed\u663e\u8457\u63d0\u5347\u73b0\u6709\u9884\u8bad\u7ec3\u548c\u5fae\u8c03MLLM\u7684\u6027\u80fd\uff0c\u4e14\u4ec5\u9700\u589e\u52a0\u4e0d\u52301%\u7684\u989d\u5916\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u8fd9\u4e00\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u53cc\u66f2\u7a7a\u95f4\u5728\u89e3\u51b3\u89c6\u89c9-\u6587\u672c\u7c92\u5ea6\u5bf9\u9f50\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u6548\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u8868\u660e\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u51e0\u4f55\u7a7a\u95f4\u5efa\u6a21\u53ef\u4ee5\u5927\u5e45\u964d\u4f4eMLLM\u8bad\u7ec3\u7684\u8ba1\u7b97\u9700\u6c42\u3002"}}
{"id": "2510.20393", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.20393", "abs": "https://arxiv.org/abs/2510.20393", "authors": ["Qing Wang", "Chong-Wah Ngo", "Yu Cao", "Ee-Peng Lim"], "title": "Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval", "comment": "ACM Multimedia 2025", "summary": "Existing approaches for image-to-recipe retrieval have the implicit\nassumption that a food image can fully capture the details textually documented\nin its recipe. However, a food image only reflects the visual outcome of a\ncooked dish and not the underlying cooking process. Consequently, learning\ncross-modal representations to bridge the modality gap between images and\nrecipes tends to ignore subtle, recipe-specific details that are not visually\napparent but are crucial for recipe retrieval. Specifically, the\nrepresentations are biased to capture the dominant visual elements, resulting\nin difficulty in ranking similar recipes with subtle differences in use of\ningredients and cooking methods. The bias in representation learning is\nexpected to be more severe when the training data is mixed of images and\nrecipes sourced from different cuisines. This paper proposes a novel causal\napproach that predicts the culinary elements potentially overlooked in images,\nwhile explicitly injecting these elements into cross-modal representation\nlearning to mitigate biases. Experiments are conducted on the standard\nmonolingual Recipe1M dataset and a newly curated multilingual multicultural\ncuisine dataset. The results indicate that the proposed causal representation\nlearning is capable of uncovering subtle ingredients and cooking actions and\nachieves impressive retrieval performance on both monolingual and multilingual\nmulticultural datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56e0\u679c\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u56fe\u50cf\u4e2d\u53ef\u80fd\u88ab\u5ffd\u7565\u7684\u70f9\u996a\u5143\u7d20\u5e76\u5c06\u5176\u663e\u5f0f\u6ce8\u5165\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6765\u7f13\u89e3\u56fe\u50cf-\u98df\u8c31\u68c0\u7d22\u4e2d\u7684\u8868\u793a\u504f\u5dee\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u63ed\u793a\u7ec6\u5fae\u7684\u98df\u6750\u548c\u70f9\u996a\u52a8\u4f5c\uff0c\u5728\u5355\u8bed\u548c\u591a\u8bed\u8a00\u591a\u6587\u5316\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf-\u98df\u8c31\u68c0\u7d22\u65b9\u6cd5\u9690\u542b\u5047\u8bbe\u98df\u7269\u56fe\u50cf\u80fd\u591f\u5b8c\u5168\u6355\u6349\u98df\u8c31\u4e2d\u6587\u672c\u8bb0\u5f55\u7684\u6240\u6709\u7ec6\u8282\uff0c\u4f46\u5b9e\u9645\u4e0a\u98df\u7269\u56fe\u50cf\u4ec5\u53cd\u6620\u70f9\u996a\u5b8c\u6210\u540e\u7684\u89c6\u89c9\u7ed3\u679c\u800c\u975e\u5e95\u5c42\u70f9\u996a\u8fc7\u7a0b\u3002\u8fd9\u5bfc\u81f4\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u503e\u5411\u4e8e\u5ffd\u7565\u90a3\u4e9b\u89c6\u89c9\u4e0a\u4e0d\u660e\u663e\u4f46\u5bf9\u98df\u8c31\u68c0\u7d22\u81f3\u5173\u91cd\u8981\u7684\u7ec6\u5fae\u3001\u98df\u8c31\u7279\u5b9a\u7684\u7ec6\u8282\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6570\u636e\u6df7\u5408\u4e86\u6765\u81ea\u4e0d\u540c\u83dc\u7cfb\u7684\u56fe\u50cf\u548c\u98df\u8c31\u65f6\uff0c\u8868\u793a\u5b66\u4e60\u7684\u504f\u5dee\u95ee\u9898\u4f1a\u66f4\u52a0\u4e25\u91cd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56e0\u679c\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9884\u6d4b\u56fe\u50cf\u4e2d\u53ef\u80fd\u88ab\u5ffd\u7565\u7684\u70f9\u996a\u5143\u7d20\uff0c\u5e76\u663e\u5f0f\u5730\u5c06\u8fd9\u4e9b\u5143\u7d20\u6ce8\u5165\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4ee5\u7f13\u89e3\u504f\u5dee\u3002\u8be5\u65b9\u6cd5\u7279\u522b\u5173\u6ce8\u63ed\u793a\u7ec6\u5fae\u7684\u98df\u6750\u4f7f\u7528\u548c\u70f9\u996a\u65b9\u6cd5\u5dee\u5f02\uff0c\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u673a\u5236\u6765\u589e\u5f3a\u8868\u793a\u5b66\u4e60\u5bf9\u975e\u89c6\u89c9\u660e\u663e\u4f46\u5173\u952e\u98df\u8c31\u7ec6\u8282\u7684\u6355\u6349\u80fd\u529b\u3002", "result": "\u5728\u6807\u51c6\u7684\u5355\u8bedRecipe1M\u6570\u636e\u96c6\u548c\u65b0\u6784\u5efa\u7684\u591a\u8bed\u8a00\u591a\u6587\u5316\u83dc\u7cfb\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u56e0\u679c\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63ed\u793a\u7ec6\u5fae\u7684\u98df\u6750\u548c\u70f9\u996a\u52a8\u4f5c\u3002\u8be5\u65b9\u6cd5\u5728\u5355\u8bed\u548c\u591a\u8bed\u8a00\u591a\u6587\u5316\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u56e0\u679c\u8868\u793a\u5b66\u4e60\u80fd\u591f\u6709\u6548\u7f13\u89e3\u56fe\u50cf-\u98df\u8c31\u68c0\u7d22\u4e2d\u7684\u8868\u793a\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u548c\u6ce8\u5165\u88ab\u56fe\u50cf\u5ffd\u7565\u7684\u70f9\u996a\u5143\u7d20\u6765\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u3002\u8fd9\u4e00\u65b9\u6cd5\u4e3a\u5904\u7406\u591a\u6587\u5316\u591a\u8bed\u8a00\u98df\u8c31\u68c0\u7d22\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u5c55\u793a\u4e86\u56e0\u679c\u63a8\u7406\u5728\u8de8\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u5728\u66f4\u590d\u6742\u70f9\u996a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.20470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20470", "abs": "https://arxiv.org/abs/2510.20470", "authors": ["Kun Ouyang", "Yuanxin Liu", "Linli Yao", "Yishuo Cai", "Hao Zhou", "Jie Zhou", "Fandong Meng", "Xu Sun"], "title": "Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence", "comment": null, "summary": "Video reasoning, which requires multi-step deduction across frames, remains a\nmajor challenge for multimodal large language models (MLLMs). While\nreinforcement learning (RL)-based methods enhance reasoning capabilities, they\noften rely on text-only chains that yield ungrounded or hallucinated\nconclusions. Conversely, frame-retrieval approaches introduce visual grounding\nbut still struggle with inaccurate evidence localization. To address these\nchallenges, we present Conan, a framework for evidence-grounded multi-step\nvideo reasoning. Conan identifies contextual and evidence frames, reasons over\ncross-frame clues, and adaptively decides when to conclude or explore further.\nTo achieve this, we (1) construct Conan-91K, a large-scale dataset of\nautomatically generated reasoning traces that includes frame identification,\nevidence reasoning, and action decision, and (2) design a multi-stage\nprogressive cold-start strategy combined with an\nIdentification-Reasoning-Action (AIR) RLVR training framework to jointly\nenhance multi-step visual reasoning. Extensive experiments on six multi-step\nreasoning benchmarks demonstrate that Conan surpasses the baseline\nQwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving\nstate-of-the-art performance. Furthermore, Conan generalizes effectively to\nlong-video understanding tasks, validating its strong scalability and\nrobustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faConan\u6846\u67b6\uff0c\u901a\u8fc7\u8bc1\u636e\u63a5\u5730\u7684\u591a\u6b65\u89c6\u9891\u63a8\u7406\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u548c\u8bc1\u636e\u5e27\u8bc6\u522b\u3001\u8de8\u5e27\u7ebf\u7d22\u63a8\u7406\u4ee5\u53ca\u81ea\u9002\u5e94\u51b3\u7b56\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u9762\u4e34\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u5e27\u63a8\u7406\u65b9\u9762\u7684\u6311\u6218\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u4f46\u5e38\u4ea7\u751f\u65e0\u6839\u636e\u7684\u6587\u672c\u94fe\u7ed3\u8bba\uff0c\u800c\u5e27\u68c0\u7d22\u65b9\u6cd5\u867d\u7136\u5f15\u5165\u89c6\u89c9\u63a5\u5730\u4f46\u4ecd\u5b58\u5728\u8bc1\u636e\u5b9a\u4f4d\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "method": "Conan\u6846\u67b6\u91c7\u7528\u8bc6\u522b-\u63a8\u7406-\u884c\u52a8\u7684\u4e09\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u89c6\u9891\u63a8\u7406\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u5305\u542b91K\u81ea\u52a8\u751f\u6210\u63a8\u7406\u8f68\u8ff9\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6Conan-91K\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u9636\u6bb5\u6e10\u8fdb\u5f0f\u51b7\u542f\u52a8\u7b56\u7565\u6765\u8054\u5408\u589e\u5f3a\u591a\u6b65\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u516d\u4e2a\u591a\u6b65\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cConan\u76f8\u6bd4\u57fa\u7ebfQwen2.5-VL-7B-Instruct\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc710%\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u5e76\u5728\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Conan\u6846\u67b6\u9a8c\u8bc1\u4e86\u8bc1\u636e\u63a5\u5730\u591a\u6b65\u89c6\u9891\u63a8\u7406\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5f3a\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u548c\u6570\u636e\u96c6\u8d44\u6e90\u3002"}}
{"id": "2510.20512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20512", "abs": "https://arxiv.org/abs/2510.20512", "authors": ["Yixiong Yang", "Tao Wu", "Senmao Li", "Shiqi Yang", "Yaxing Wang", "Joost van de Weijer", "Kai Wang"], "title": "EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization", "comment": "Project page available at\n  https://liulisixin.github.io/EchoDistill-page/", "summary": "Recent advances in accelerating text-to-image (T2I) diffusion models have\nenabled the synthesis of high-fidelity images even in a single step. However,\npersonalizing these models to incorporate novel concepts remains a challenge\ndue to the limited capacity of one-step models to capture new concept\ndistributions effectively. We propose a bidirectional concept distillation\nframework, EchoDistill, to enable one-step diffusion personalization (1-SDP).\nOur approach involves an end-to-end training process where a multi-step\ndiffusion model (teacher) and a one-step diffusion model (student) are trained\nsimultaneously. The concept is first distilled from the teacher model to the\nstudent, and then echoed back from the student to the teacher. During the\nEchoDistill, we share the text encoder between the two models to ensure\nconsistent semantic understanding. Following this, the student model is\noptimized with adversarial losses to align with the real image distribution and\nwith alignment losses to maintain consistency with the teacher's output.\nFurthermore, we introduce the bidirectional echoing refinement strategy,\nwherein the student model leverages its faster generation capability to\nfeedback to the teacher model. This bidirectional concept distillation\nmechanism not only enhances the student ability to personalize novel concepts\nbut also improves the generative quality of the teacher model. Our experiments\ndemonstrate that this collaborative framework significantly outperforms\nexisting personalization methods over the 1-SDP setup, establishing a novel\nparadigm for rapid and effective personalization in T2I diffusion models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEchoDistill\u53cc\u5411\u6982\u5ff5\u84b8\u998f\u6846\u67b6\uff0c\u5b9e\u73b0\u5355\u6b65\u6269\u6563\u4e2a\u6027\u5316\uff081-SDP\uff09\uff0c\u901a\u8fc7\u5e08\u751f\u6a21\u578b\u534f\u540c\u8bad\u7ec3\u673a\u5236\uff0c\u5728\u4fdd\u6301\u5feb\u901f\u751f\u6210\u7684\u540c\u65f6\u6709\u6548\u6355\u6349\u65b0\u6982\u5ff5\u5206\u5e03\u3002", "motivation": "\u5f53\u524d\u5355\u6b65\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u867d\u7136\u5b9e\u73b0\u4e86\u52a0\u901f\u751f\u6210\uff0c\u4f46\u5728\u4e2a\u6027\u5316\u65b0\u6982\u5ff5\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u4e3a\u5355\u6b65\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6355\u6349\u65b0\u6982\u5ff5\u7684\u5206\u5e03\u7279\u5f81\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6982\u5ff5\u5b9a\u5236\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u53cc\u5411\u6982\u5ff5\u84b8\u998f\u6846\u67b6EchoDistill\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u5f0f\u540c\u65f6\u4f18\u5316\u591a\u6b65\u6559\u5e08\u6a21\u578b\u548c\u5355\u6b65\u5b66\u751f\u6a21\u578b\uff0c\u901a\u8fc7\u6982\u5ff5\u4ece\u6559\u5e08\u5230\u5b66\u751f\u7684\u84b8\u998f\u4ee5\u53ca\u4ece\u5b66\u751f\u5230\u6559\u5e08\u7684\u56de\u4f20\u5b9e\u73b0\u53cc\u5411\u77e5\u8bc6\u4f20\u9012\uff0c\u5e76\u5171\u4eab\u6587\u672c\u7f16\u7801\u5668\u4fdd\u8bc1\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u7ed3\u5408\u5bf9\u6297\u635f\u5931\u548c\u5bf9\u9f50\u635f\u5931\u4f18\u5316\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u534f\u4f5c\u6846\u67b6\u57281-SDP\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u5bf9\u65b0\u6982\u5ff5\u7684\u4e2a\u6027\u5316\u80fd\u529b\uff0c\u8fd8\u6539\u5584\u4e86\u6559\u5e08\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u5feb\u901f\u6709\u6548\u4e2a\u6027\u5316T2I\u6269\u6563\u6a21\u578b\u7684\u65b0\u8303\u5f0f\uff0c\u53cc\u5411\u6982\u5ff5\u84b8\u998f\u673a\u5236\u8bc1\u660e\u4e86\u5e08\u751f\u6a21\u578b\u534f\u540c\u8bad\u7ec3\u5728\u5e73\u8861\u751f\u6210\u901f\u5ea6\u4e0e\u6982\u5ff5\u6355\u6349\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u65f6\u4e2a\u6027\u5316\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.20578", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20578", "abs": "https://arxiv.org/abs/2510.20578", "authors": ["Ding Zou", "Feifan Wang", "Mengyu Ge", "Siyuan Fan", "Zongbing Zhang", "Wei Chen", "Lingfeng Wang", "Zhongyou Hu", "Wenrui Yan", "Zhengwei Gao", "Hao Wang", "Weizhao Jin", "Yu Zhang", "Hainan Zhao", "Mingliang Zhang", "Xianxian Xi", "Yaru Zhang", "Wenyuan Li", "Zhengguang Gao", "Yurui Zhu"], "title": "EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence", "comment": null, "summary": "The realization of Artificial General Intelligence (AGI) necessitates\nEmbodied AI agents capable of robust spatial perception, effective task\nplanning, and adaptive execution in physical environments. However, current\nlarge language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks\nsuffer from key limitations, including a significant gap between model design\nand agent requirements, an unavoidable trade-off between real-time latency and\nperformance, and the use of unauthentic, offline evaluation metrics. To address\nthese challenges, we propose EmbodiedBrain, a novel vision-language foundation\nmodel available in both 7B and 32B parameter sizes. Our framework features an\nagent-aligned data structure and employs a powerful training methodology that\nintegrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group\nRelative Policy Optimization (Step-GRPO), which boosts long-horizon task\nsuccess by integrating preceding steps as Guided Precursors. Furthermore, we\nincorporate a comprehensive reward system, including a Generative Reward Model\n(GRM) accelerated at the infrastructure level, to improve training efficiency.\nFor enable thorough validation, we establish a three-part evaluation system\nencompassing General, Planning, and End-to-End Simulation Benchmarks,\nhighlighted by the proposal and open-sourcing of a novel, challenging\nsimulation environment. Experimental results demonstrate that EmbodiedBrain\nachieves superior performance across all metrics, establishing a new\nstate-of-the-art for embodied foundation models. Towards paving the way for the\nnext generation of generalist embodied agents, we open-source all of our data,\nmodel weight, and evaluating methods, which are available at\nhttps://zterobot.github.io/EmbodiedBrain.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EmbodiedBrain\uff0c\u4e00\u79cd\u65b0\u578b\u7684\u5177\u8eab\u667a\u80fd\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u8bc4\u4f30\u4f53\u7cfb\u89e3\u51b3\u4e86\u5f53\u524dLLM\u548cMLLM\u5728\u5177\u8eab\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u5728\u5404\u9879\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u5177\u8eab\u4efb\u52a1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u6a21\u578b\u8bbe\u8ba1\u4e0e\u667a\u80fd\u4f53\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3001\u5b9e\u65f6\u5ef6\u8fdf\u4e0e\u6027\u80fd\u4e4b\u95f4\u4e0d\u53ef\u907f\u514d\u7684\u6743\u8861\u3001\u4ee5\u53ca\u4f7f\u7528\u4e0d\u771f\u5b9e\u7684\u79bb\u7ebf\u8bc4\u4f30\u6307\u6807\uff0c\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86\u901a\u7528\u4eba\u5de5\u667a\u80fd\u5728\u5177\u8eab\u667a\u80fd\u9886\u57df\u7684\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u4e86EmbodiedBrain\u6846\u67b6\uff0c\u91c7\u75287B\u548c32B\u4e24\u79cd\u53c2\u6570\u89c4\u6a21\uff0c\u8bbe\u8ba1\u4e86\u667a\u80fd\u4f53\u5bf9\u9f50\u7684\u6570\u636e\u7ed3\u6784\uff0c\u5e76\u91c7\u7528\u5927\u89c4\u6a21\u76d1\u7763\u5fae\u8c03\u4e0eStep-Augmented Group Relative Policy Optimization\u76f8\u7ed3\u5408\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u524d\u5e8f\u6b65\u9aa4\u4f5c\u4e3a\u5f15\u5bfc\u524d\u4f53\u6765\u63d0\u5347\u957f\u65f6\u7a0b\u4efb\u52a1\u6210\u529f\u7387\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u5305\u542b\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\u7684\u7efc\u5408\u5956\u52b1\u7cfb\u7edf\u4ee5\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEmbodiedBrain\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u5728\u901a\u7528\u57fa\u51c6\u3001\u89c4\u5212\u57fa\u51c6\u548c\u7aef\u5230\u7aef\u4eff\u771f\u57fa\u51c6\u7684\u4e09\u90e8\u5206\u8bc4\u4f30\u4f53\u7cfb\u4e2d\u5747\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u7279\u522b\u662f\u5728\u63d0\u51fa\u7684\u65b0\u578b\u6311\u6218\u6027\u4eff\u771f\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e0b\u4e00\u4ee3\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u901a\u8fc7\u5f00\u6e90\u6240\u6709\u6570\u636e\u3001\u6a21\u578b\u6743\u91cd\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u5177\u8eab\u57fa\u7840\u6a21\u578b\u7684\u65b0\u6807\u51c6\uff0c\u5f3a\u8c03\u4e86\u7efc\u5408\u8bad\u7ec3\u65b9\u6cd5\u548c\u771f\u5b9e\u8bc4\u4f30\u73af\u5883\u5bf9\u4e8e\u5b9e\u73b0\u7a33\u5065\u7a7a\u95f4\u611f\u77e5\u548c\u81ea\u9002\u5e94\u4efb\u52a1\u6267\u884c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.20586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20586", "abs": "https://arxiv.org/abs/2510.20586", "authors": ["Muhammad Atif Butt", "Alexandra Gomez-Villa", "Tao Wu", "Javier Vazquez-Corral", "Joost Van De Weijer", "Kai Wang"], "title": "GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models", "comment": null, "summary": "Recent years have seen impressive advances in text-to-image generation, with\nimage generative or unified models producing high-quality images from text. Yet\nthese models still struggle with fine-grained color controllability, often\nfailing to accurately match colors specified in text prompts. While existing\nbenchmarks evaluate compositional reasoning and prompt adherence, none\nsystematically assess color precision. Color is fundamental to human visual\nperception and communication, critical for applications from art to design\nworkflows requiring brand consistency. However, current benchmarks either\nneglect color or rely on coarse assessments, missing key capabilities such as\ninterpreting RGB values or aligning with human expectations. To this end, we\npropose GenColorBench, the first comprehensive benchmark for text-to-image\ncolor generation, grounded in color systems like ISCC-NBS and CSS3/X11,\nincluding numerical colors which are absent elsewhere. With 44K color-focused\nprompts covering 400+ colors, it reveals models' true capabilities via\nperceptual and automated assessments. Evaluations of popular text-to-image\nmodels using GenColorBench show performance variations, highlighting which\ncolor conventions models understand best and identifying failure modes. Our\nGenColorBench assessments will guide improvements in precise color generation.\nThe benchmark will be made public upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GenColorBench\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u989c\u8272\u751f\u6210\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8eISCC-NBS\u548cCSS3/X11\u989c\u8272\u7cfb\u7edf\u6784\u5efa\uff0c\u5305\u542b44K\u4e2a\u989c\u8272\u7126\u70b9\u63d0\u793a\uff0c\u63ed\u793a\u4e86\u4e3b\u6d41\u6a21\u578b\u5728\u7cbe\u7ec6\u989c\u8272\u63a7\u5236\u65b9\u9762\u7684\u6027\u80fd\u5dee\u5f02\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u7cbe\u7ec6\u989c\u8272\u53ef\u63a7\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u65e0\u6cd5\u51c6\u786e\u5339\u914d\u6587\u672c\u63d0\u793a\u4e2d\u6307\u5b9a\u7684\u989c\u8272\uff0c\u800c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8981\u4e48\u5ffd\u7565\u989c\u8272\u8bc4\u4f30\uff0c\u8981\u4e48\u4f9d\u8d56\u7c97\u7cd9\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9RGB\u6570\u503c\u89e3\u91ca\u548c\u4eba\u7c7b\u671f\u671b\u5bf9\u9f50\u7b49\u5173\u952e\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86GenColorBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8eISCC-NBS\u548cCSS3/X11\u989c\u8272\u7cfb\u7edf\u6784\u5efa\uff0c\u5305\u542b44K\u4e2a\u989c\u8272\u7126\u70b9\u63d0\u793a\uff0c\u8986\u76d6400\u591a\u79cd\u989c\u8272\uff0c\u9996\u6b21\u5f15\u5165\u6570\u503c\u989c\u8272\u8bc4\u4f30\uff0c\u901a\u8fc7\u611f\u77e5\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u5168\u9762\u5206\u6790\u6a21\u578b\u7684\u989c\u8272\u751f\u6210\u80fd\u529b\u3002", "result": "\u5bf9\u4e3b\u6d41\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u4e0d\u540c\u989c\u8272\u7ea6\u5b9a\u7684\u7406\u89e3\u7a0b\u5ea6\uff0c\u8bc6\u522b\u4e86\u5177\u4f53\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u7cbe\u786e\u989c\u8272\u751f\u6210\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6027\u80fd\u57fa\u51c6\u3002", "conclusion": "GenColorBench\u57fa\u51c6\u6d4b\u8bd5\u5c06\u6307\u5bfc\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7cbe\u786e\u989c\u8272\u751f\u6210\u65b9\u9762\u7684\u6539\u8fdb\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u4f53\u7cfb\u7684\u7a7a\u767d\uff0c\u4e3a\u989c\u8272\u53ef\u63a7\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u65b9\u5411\u6307\u5f15\u3002"}}
{"id": "2510.20622", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20622", "abs": "https://arxiv.org/abs/2510.20622", "authors": ["Yuan Sheng", "Yanbin Hao", "Chenxu Li", "Shuo Wang", "Xiangnan He"], "title": "SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding", "comment": null, "summary": "Long video understanding remains challenging due to its complex, diverse, and\ntemporally scattered content. Although video large language models (Video-LLMs)\ncan process videos lasting tens of minutes, applying them to truly long\nsequences is computationally prohibitive and often leads to unfocused or\ninconsistent reasoning. A promising solution is to select only the most\ninformative frames, yet existing approaches typically ignore temporal\ndependencies or rely on unimodal evidence, limiting their ability to provide\ncomplete and query-relevant context. We propose a Semantic-Visual Consensus\nEvidence Selection (SeViCES) framework for effective and reliable long video\nunderstanding. SeViCES is training-free and model-agnostic, and introduces two\nkey components. The Semantic-Visual Consensus Frame Selection (SVCFS) module\nselects frames through (1) a temporal-aware semantic branch that leverages LLM\nreasoning over captions, and (2) a cluster-guided visual branch that aligns\nembeddings with semantic scores via mutual information. The Answer Consensus\nRefinement (ACR) module further resolves inconsistencies between semantic- and\nvisual-based predictions by fusing evidence and constraining the answer space.\nExtensive experiments on long video understanding benchmarks show that SeViCES\nconsistently outperforms state-of-the-art methods in both accuracy and\nrobustness, demonstrating the importance of consensus-driven evidence selection\nfor Video-LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SeViCES\u6846\u67b6\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u4e14\u6a21\u578b\u65e0\u5173\u7684\u957f\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49-\u89c6\u89c9\u5171\u8bc6\u8bc1\u636e\u9009\u62e9\u673a\u5236\uff0c\u5728\u591a\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u957f\u89c6\u9891\u7406\u89e3\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u548c\u63a8\u7406\u4e0d\u4e00\u81f4\u7684\u6311\u6218\uff0c\u73b0\u6709\u5e27\u9009\u62e9\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u65f6\u95f4\u4f9d\u8d56\u6027\u6216\u4f9d\u8d56\u5355\u6a21\u6001\u8bc1\u636e\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5b8c\u6574\u4e14\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "SeViCES\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u8bed\u4e49-\u89c6\u89c9\u5171\u8bc6\u5e27\u9009\u62e9\u6a21\u5757\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u7684\u8bed\u4e49\u5206\u652f\u548c\u805a\u7c7b\u5f15\u5bfc\u7684\u89c6\u89c9\u5206\u652f\u8fdb\u884c\u5e27\u9009\u62e9\uff0c\u7b54\u6848\u5171\u8bc6\u7cbe\u70bc\u6a21\u5757\u901a\u8fc7\u8bc1\u636e\u878d\u5408\u548c\u7b54\u6848\u7a7a\u95f4\u7ea6\u675f\u6765\u89e3\u51b3\u8bed\u4e49\u4e0e\u89c6\u89c9\u9884\u6d4b\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSeViCES\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5171\u8bc6\u9a71\u52a8\u8bc1\u636e\u9009\u62e9\u5bf9\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u8bed\u4e49\u4e0e\u89c6\u89c9\u8bc1\u636e\u4e4b\u95f4\u8fbe\u6210\u5171\u8bc6\u5bf9\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63d0\u51fa\u7684\u8bad\u7ec3\u65e0\u5173\u6846\u67b6\u4e3a\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bc1\u636e\u9009\u62e9\u673a\u5236\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.20639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20639", "abs": "https://arxiv.org/abs/2510.20639", "authors": ["Ibrahim Ethem Hamamci", "Sezgin Er", "Suprosanna Shit", "Hadrien Reynaud", "Dong Yang", "Pengfei Guo", "Marc Edgar", "Daguang Xu", "Bernhard Kainz", "Bjoern Menze"], "title": "Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging", "comment": "NeurIPS 2025", "summary": "Recent progress in vision-language modeling for 3D medical imaging has been\nfueled by large-scale computed tomography (CT) corpora with paired free-text\nreports, stronger architectures, and powerful pretrained models. This has\nenabled applications such as automated report generation and text-conditioned\n3D image synthesis. Yet, current approaches struggle with high-resolution,\nlong-sequence volumes: contrastive pretraining often yields vision encoders\nthat are misaligned with clinical language, and slice-wise tokenization blurs\nfine anatomy, reducing diagnostic performance on downstream tasks. We introduce\nBTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder\nthat unifies 2D and 3D training and inference while producing compact,\nfrequency-aware volumetric tokens. A three-stage training curriculum enables\n(i) local reconstruction, (ii) overlapping-window tiling, and (iii)\nlong-context decoder refinement, during which the model learns from short slice\nexcerpts yet generalizes to scans exceeding 300 slices without additional\nmemory overhead. BTB3D sets a new state-of-the-art on two key tasks: it\nimproves BLEU scores and increases clinical F1 by 40% over CT2Rep, CT-CHAT, and\nMerlin for report generation; and it reduces FID by 75% and halves FVD compared\nto GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically\nconsistent 512*512*241 volumes. These results confirm that precise\nthree-dimensional tokenization, rather than larger language backbones alone, is\nessential for scalable vision-language modeling in 3D medical imaging. The\ncodebase is available at: https://github.com/ibrahimethemhamamci/BTB3D", "AI": {"tldr": "BTB3D\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u5377\u79ef\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u9891\u7387\u611f\u77e5\u7684\u4f53\u7d20\u6807\u8bb0\u5316\u548c\u4e09\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u89e3\u51b3\u4e863D\u533b\u5b66\u5f71\u50cf\u4e2d\u9ad8\u5206\u8fa8\u7387\u957f\u5e8f\u5217\u5904\u7406\u96be\u9898\uff0c\u5728\u62a5\u544a\u751f\u6210\u548c\u6587\u672c\u5230CT\u5408\u6210\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d3D\u533b\u5b66\u5f71\u50cf\u7684\u89c6\u89c9\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u957f\u5e8f\u5217\u4f53\u79ef\u65f6\u9762\u4e34\u6311\u6218\uff1a\u5bf9\u6bd4\u9884\u8bad\u7ec3\u4ea7\u751f\u7684\u89c6\u89c9\u7f16\u7801\u5668\u4e0e\u4e34\u5e8a\u8bed\u8a00\u5b58\u5728\u9519\u4f4d\uff0c\u5207\u7247\u7ea7\u6807\u8bb0\u5316\u4f1a\u6a21\u7cca\u7cbe\u7ec6\u89e3\u5256\u7ed3\u6784\uff0c\u4ece\u800c\u964d\u4f4e\u4e0b\u6e38\u4efb\u52a1\u7684\u8bca\u65ad\u6027\u80fd\u3002", "method": "BTB3D\u91c7\u7528\u56e0\u679c\u5377\u79ef\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7edf\u4e002D\u548c3D\u8bad\u7ec3\u4e0e\u63a8\u7406\uff0c\u751f\u6210\u7d27\u51d1\u7684\u9891\u7387\u611f\u77e5\u4f53\u7d20\u6807\u8bb0\u3002\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\u5b9e\u73b0\u5c40\u90e8\u91cd\u5efa\u3001\u91cd\u53e0\u7a97\u53e3\u5e73\u94fa\u548c\u957f\u4e0a\u4e0b\u6587\u89e3\u7801\u5668\u7cbe\u70bc\uff0c\u6a21\u578b\u4ece\u77ed\u5207\u7247\u6458\u5f55\u4e2d\u5b66\u4e60\u4f46\u80fd\u6cdb\u5316\u5230\u8d85\u8fc7300\u5207\u7247\u7684\u626b\u63cf\u800c\u4e0d\u589e\u52a0\u5185\u5b58\u5f00\u9500\u3002", "result": "BTB3D\u5728\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e0a\u6bd4CT2Rep\u3001CT-CHAT\u548cMerlin\u63d0\u9ad8\u4e86BLEU\u5206\u6570\uff0c\u4e34\u5e8aF1\u5206\u6570\u589e\u52a0\u4e8640%\uff1b\u5728\u6587\u672c\u5230CT\u5408\u6210\u4efb\u52a1\u4e0a\u6bd4GenerateCT\u548cMedSyn\u5c06FID\u964d\u4f4e\u4e8675%\uff0cFVD\u51cf\u534a\uff0c\u751f\u6210\u4e86\u89e3\u5256\u5b66\u4e00\u81f4\u7684512*512*241\u4f53\u79ef\u56fe\u50cf\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u7cbe\u786e\u7684\u4e09\u7ef4\u6807\u8bb0\u5316\u800c\u975e\u4ec5\u4f9d\u8d56\u66f4\u5927\u7684\u8bed\u8a00\u9aa8\u5e72\u7f51\u7edc\uff0c\u5bf9\u4e8e3D\u533b\u5b66\u5f71\u50cf\u4e2d\u53ef\u6269\u5c55\u7684\u89c6\u89c9\u8bed\u8a00\u5efa\u6a21\u81f3\u5173\u91cd\u8981\u3002\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u5206\u8fa8\u7387\u957f\u5e8f\u5217\u533b\u5b66\u5f71\u50cf\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u533b\u5b66\u5f71\u50cf\u5206\u6790\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.20661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20661", "abs": "https://arxiv.org/abs/2510.20661", "authors": ["Chen Zhao", "En Ci", "Yunzhe Xu", "Tiehan Fan", "Shanyan Guan", "Yanhao Ge", "Jian Yang", "Ying Tai"], "title": "UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset", "comment": "Accepted by NeurIPS 2025", "summary": "Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable\nprogress. However, two key challenges remain : 1) the absence of a large-scale\nhigh-quality UHR T2I dataset, and (2) the neglect of tailored training\nstrategies for fine-grained detail synthesis in UHR scenarios. To tackle the\nfirst challenge, we introduce \\textbf{UltraHR-100K}, a high-quality dataset of\n100K UHR images with rich captions, offering diverse content and strong visual\nfidelity. Each image exceeds 3K resolution and is rigorously curated based on\ndetail richness, content complexity, and aesthetic quality. To tackle the\nsecond challenge, we propose a frequency-aware post-training method that\nenhances fine-detail generation in T2I diffusion models. Specifically, we\ndesign (i) \\textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning\non detail-critical denoising steps, and (ii) \\textit{Soft-Weighting Frequency\nRegularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to\nsoftly constrain frequency components, encouraging high-frequency detail\npreservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks\ndemonstrate that our approach significantly improves the fine-grained detail\nquality and overall fidelity of UHR image generation. The code is available at\n\\href{https://github.com/NJU-PCALab/UltraHR-100k}{here}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UltraHR-100K\u8d85\u9ad8\u6e05\u6587\u672c\u5230\u56fe\u50cf\u6570\u636e\u96c6\u548c\u9891\u7387\u611f\u77e5\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u8282\u5bfc\u5411\u65f6\u95f4\u6b65\u91c7\u6837\u548c\u8f6f\u52a0\u6743\u9891\u7387\u6b63\u5219\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u9ad8\u6e05\u56fe\u50cf\u751f\u6210\u7684\u7ec6\u8282\u8d28\u91cf\u548c\u6574\u4f53\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5f53\u524d\u8d85\u9ad8\u6e05\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u8d85\u9ad8\u6e05\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u9488\u5bf9\u8d85\u9ad8\u6e05\u573a\u666f\u4e0b\u7ec6\u7c92\u5ea6\u7ec6\u8282\u5408\u6210\u7684\u5b9a\u5236\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u8d85\u9ad8\u5206\u8fa8\u7387\u4e0b\u751f\u6210\u7cbe\u7ec6\u7ec6\u8282\u7684\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6784\u5efaUltraHR-100K\u6570\u636e\u96c6\uff08\u5305\u542b10\u4e07\u5f20\u8d85\u8fc73K\u5206\u8fa8\u7387\u7684\u7cbe\u9009\u56fe\u50cf\uff09\u548c\u63d0\u51fa\u9891\u7387\u611f\u77e5\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u7ec6\u8282\u5bfc\u5411\u65f6\u95f4\u6b65\u91c7\u6837\u805a\u7126\u4e8e\u7ec6\u8282\u5173\u952e\u7684\u53bb\u566a\u6b65\u9aa4\uff0c\u4ee5\u53ca\u8f6f\u52a0\u6743\u9891\u7387\u6b63\u5219\u5316\u5229\u7528\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u8f6f\u7ea6\u675f\u9891\u7387\u5206\u91cf\u4ee5\u4fc3\u8fdb\u9ad8\u9891\u7ec6\u8282\u4fdd\u7559\u3002", "result": "\u5728\u63d0\u51fa\u7684UltraHR-eval4K\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8d85\u9ad8\u6e05\u56fe\u50cf\u751f\u6210\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\u8d28\u91cf\u548c\u6574\u4f53\u4fdd\u771f\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8d85\u9ad8\u6e05\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6570\u636e\u96c6\u8d44\u6e90\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u9488\u5bf9\u9ad8\u9891\u7ec6\u8282\u7684\u4e13\u95e8\u4f18\u5316\u7b56\u7565\u5728\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u672a\u6765\u8d85\u9ad8\u6e05\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.20696", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20696", "abs": "https://arxiv.org/abs/2510.20696", "authors": ["Jing Bi", "Guangyu Sun", "Ali Vosoughi", "Chen Chen", "Chenliang Xu"], "title": "Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward", "comment": "5 pages", "summary": "Multimodal large language models (MLLMs) that integrate visual and textual\nreasoning leverage chain-of-thought (CoT) prompting to tackle complex visual\ntasks, yet continue to exhibit visual hallucinations and an over-reliance on\ntextual priors. We present a systematic diagnosis of state-of-the-art\nvision-language models using a three-stage evaluation framework, uncovering key\nfailure modes. To address these, we propose an agent-based architecture that\ncombines LLM reasoning with lightweight visual modules, enabling fine-grained\nanalysis and iterative refinement of reasoning chains. Our results highlight\nfuture visual reasoning models should focus on integrating a broader set of\nspecialized tools for analyzing visual content. Our system achieves significant\ngains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or\nsurpassing much larger models. We will release our framework and evaluation\nsuite to facilitate future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u67b6\u6784\uff0c\u5c06LLM\u63a8\u7406\u4e0e\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u5757\u76f8\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u89c6\u89c9\u5e7b\u89c9\u548c\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u5148\u9a8c\u95ee\u9898\u3002\u8be5\u7cfb\u7edf\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u63a8\u7406\u65f6\uff0c\u867d\u7136\u5229\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u5904\u7406\u590d\u6742\u89c6\u89c9\u4efb\u52a1\uff0c\u4f46\u4ecd\u5b58\u5728\u89c6\u89c9\u5e7b\u89c9\u548c\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u5148\u9a8c\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u8bca\u65ad\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63ed\u793a\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u5e76\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u67b6\u6784\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u5757\u76f8\u7ed3\u5408\uff0c\u652f\u6301\u5bf9\u63a8\u7406\u94fe\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\u548c\u8fed\u4ee3\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e09\u9636\u6bb5\u8bc4\u4f30\u6846\u67b6\u5bf9\u73b0\u6709\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6027\u8bca\u65ad\uff0c\u5e76\u5f00\u53d1\u4e13\u95e8\u7684\u89c6\u89c9\u5185\u5bb9\u5206\u6790\u5de5\u5177\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5728MMMU\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86+10.3\u5206\u7684\u663e\u8457\u63d0\u5347\uff0c\u5728MathVista\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86+6.0\u5206\u7684\u63d0\u5347\uff0c\u8d85\u8d8a\u4e867B\u53c2\u6570\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u6027\u80fd\u3002\u7814\u7a76\u56e2\u961f\u5c06\u53d1\u5e03\u6846\u67b6\u548c\u8bc4\u4f30\u5957\u4ef6\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u672a\u6765\u7684\u89c6\u89c9\u63a8\u7406\u6a21\u578b\u5e94\u4e13\u6ce8\u4e8e\u6574\u5408\u66f4\u5e7f\u6cdb\u7684\u4e13\u95e8\u5316\u5de5\u5177\u6765\u5206\u6790\u89c6\u89c9\u5185\u5bb9\u3002\u57fa\u4e8e\u4ee3\u7406\u7684\u67b6\u6784\u7ed3\u5408\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u5757\u7684\u65b9\u6cd5\u4e3a\u89e3\u51b3\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.20707", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20707", "abs": "https://arxiv.org/abs/2510.20707", "authors": ["Xuyang Liu", "Xiyan Gui", "Yuchao Zhang", "Linfeng Zhang"], "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models", "comment": "Our code is available at https://github.com/xuyang-liu16/MixKV", "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MixKV\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u91cd\u8981\u6027\u548c\u591a\u6837\u6027\u6765\u4f18\u5316\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684KV\u7f13\u5b58\u538b\u7f29\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u91cd\u8981\u6027\u800c\u5ffd\u7565\u6a21\u6001\u7279\u5b9a\u8bed\u4e49\u5197\u4f59\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u5728\u6781\u7aef\u538b\u7f29\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6269\u5c55\u591a\u6a21\u6001\u5e8f\u5217\u65f6\u9762\u4e34KV\u7f13\u5b58\u81a8\u80c0\u5bfc\u81f4\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4fdd\u7559\u9ad8\u91cd\u8981\u6027KV\u5bf9\u4ee5\u6700\u5c0f\u5316\u5b58\u50a8\uff0c\u5374\u5ffd\u7565\u4e86\u591a\u6a21\u6001KV\u7f13\u5b58\u4e2d\u51fa\u73b0\u7684\u6a21\u6001\u7279\u5b9a\u8bed\u4e49\u5197\u4f59\u6a21\u5f0f\uff0c\u4ec5\u4f9d\u8d56\u91cd\u8981\u6027\u53ea\u80fd\u8986\u76d6KV\u7f13\u5b58\u4fe1\u606f\u5206\u5e03\u7684\u5b50\u96c6\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bed\u4e49\u8986\u76d6\u635f\u5931\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MixKV\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6790LVLMs\u4e2dKV\u7f13\u5b58\u5728\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u95f4\u8868\u73b0\u51fa\u7684\u5197\u4f59\u5ea6\u53d8\u5316\uff0c\u81ea\u9002\u5e94\u5730\u9002\u5e94\u5934\u7ea7\u8bed\u4e49\u5197\u4f59\uff0c\u5728\u538b\u7f29KV\u5bf9\u65f6\u9009\u62e9\u6027\u5e73\u8861\u591a\u6837\u6027\u548c\u91cd\u8981\u6027\uff0c\u4ece\u800c\u4f18\u5316KV\u7f13\u5b58\u538b\u7f29\u6548\u679c\u3002", "result": "\u5728\u6781\u7aef\u538b\u7f29\u6761\u4ef6\u4e0b\uff08\u9884\u7b97=64\uff09\uff0cMixKV\u5728\u4e94\u4e2a\u591a\u6a21\u6001\u7406\u89e3\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u5347\u57fa\u7ebf\u65b9\u6cd55.1%\uff0c\u5728GUI grounding\u4efb\u52a1\u4e0a\u5bf9SnapKV\u548cAdaKV\u5206\u522b\u5b9e\u73b0\u4e868.0%\u548c9.0%\u7684\u663e\u8457\u589e\u76ca\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u63a8\u7406\u6548\u7387\uff0c\u5e76\u4e14\u80fd\u591f\u65e0\u7f1d\u6269\u5c55\u5230LLMs\u5e76\u83b7\u5f97\u53ef\u6bd4\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "MixKV\u901a\u8fc7\u6df7\u5408\u91cd\u8981\u6027\u548c\u591a\u6837\u6027\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001KV\u7f13\u5b58\u538b\u7f29\u4e2d\u7684\u8bed\u4e49\u5197\u4f59\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u8003\u8651\u5934\u7ea7\u8bed\u4e49\u5197\u4f59\u6a21\u5f0f\u5bf9\u4e8e\u4f18\u5316\u538b\u7f29\u6027\u80fd\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u90e8\u7f72\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5411\u7eaf\u8bed\u8a00\u6a21\u578b\u7684\u826f\u597d\u6269\u5c55\u6027\u3002"}}
{"id": "2510.20803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20803", "abs": "https://arxiv.org/abs/2510.20803", "authors": ["Xiaolong Wang", "Lixiang Ru", "Ziyuan Huang", "Kaixiang Ji", "Dandan Zheng", "Jingdong Chen", "Jun Zhou"], "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model", "comment": "Accepted to NeurIPS 2025, 18 pages", "summary": "We propose a novel AutoRegressive Generation-based paradigm for image\nSegmentation (ARGenSeg), achieving multimodal understanding and pixel-level\nperception within a unified framework. Prior works integrating image\nsegmentation into multimodal large language models (MLLMs) typically employ\neither boundary points representation or dedicated segmentation heads. These\nmethods rely on discrete representations or semantic prompts fed into\ntask-specific decoders, which limits the ability of the MLLM to capture\nfine-grained visual details. To address these challenges, we introduce a\nsegmentation framework for MLLM based on image generation, which naturally\nproduces dense masks for target objects. We leverage MLLM to output visual\ntokens and detokenize them into images using an universal VQ-VAE, making the\nsegmentation fully dependent on the pixel-level understanding of the MLLM. To\nreduce inference latency, we employ a next-scale-prediction strategy to\ngenerate required visual tokens in parallel. Extensive experiments demonstrate\nthat our method surpasses prior state-of-the-art approaches on multiple\nsegmentation datasets with a remarkable boost in inference speed, while\nmaintaining strong understanding capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u751f\u6210\u7684\u56fe\u50cf\u5206\u5272\u8303\u5f0fARGenSeg\uff0c\u901a\u8fc7\u56fe\u50cf\u751f\u6210\u65b9\u5f0f\u5b9e\u73b0\u591a\u6a21\u6001\u7406\u89e3\u548c\u50cf\u7d20\u7ea7\u611f\u77e5\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u5206\u5272\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5e76\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u5c06\u56fe\u50cf\u5206\u5272\u96c6\u6210\u5230\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u8fb9\u754c\u70b9\u8868\u793a\u6216\u4e13\u7528\u5206\u5272\u5934\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u79bb\u6563\u8868\u793a\u6216\u8f93\u5165\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5668\u7684\u8bed\u4e49\u63d0\u793a\uff0c\u9650\u5236\u4e86MLLM\u6355\u6349\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u5206\u5272\u6846\u67b6\uff0c\u5229\u7528MLLM\u8f93\u51fa\u89c6\u89c9\u6807\u8bb0\u5e76\u901a\u8fc7\u901a\u7528VQ-VAE\u5c06\u5176\u89e3\u7801\u4e3a\u56fe\u50cf\uff0c\u4f7f\u5206\u5272\u5b8c\u5168\u4f9d\u8d56\u4e8eMLLM\u7684\u50cf\u7d20\u7ea7\u7406\u89e3\uff1b\u91c7\u7528\u4e0b\u4e00\u5c3a\u5ea6\u9884\u6d4b\u7b56\u7565\u5e76\u884c\u751f\u6210\u6240\u9700\u89c6\u89c9\u6807\u8bb0\u4ee5\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u5728\u591a\u4e2a\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u5148\u524d\u7684state-of-the-art\u65b9\u6cd5\uff0c\u63a8\u7406\u901f\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u5206\u5272\u8303\u5f0f\u80fd\u591f\u6709\u6548\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u548c\u50cf\u7d20\u7ea7\u611f\u77e5\uff0c\u4e3aMLLM\u5728\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u540c\u65f6\u901a\u8fc7\u5e76\u884c\u5316\u7b56\u7565\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u65b9\u6cd5\u7684\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2510.20820", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20820", "abs": "https://arxiv.org/abs/2510.20820", "authors": ["Guocheng Gordon Qian", "Ruihang Zhang", "Tsai-Shien Chen", "Yusuf Dalva", "Anujraaj Argo Goyal", "Willi Menapace", "Ivan Skorokhodov", "Meng Dong", "Arpit Sahni", "Daniil Ostashev", "Ju Hu", "Sergey Tulyakov", "Kuan-Chieh Jackson Wang"], "title": "LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas", "comment": "9 pages, preprint", "summary": "Despite their impressive visual fidelity, existing personalized generative\nmodels lack interactive control over spatial composition and scale poorly to\nmultiple subjects. To address these limitations, we present LayerComposer, an\ninteractive framework for personalized, multi-subject text-to-image generation.\nOur approach introduces two main contributions: (1) a layered canvas, a novel\nrepresentation in which each subject is placed on a distinct layer, enabling\nocclusion-free composition; and (2) a locking mechanism that preserves selected\nlayers with high fidelity while allowing the remaining layers to adapt flexibly\nto the surrounding context. Similar to professional image-editing software, the\nproposed layered canvas allows users to place, resize, or lock input subjects\nthrough intuitive layer manipulation. Our versatile locking mechanism requires\nno architectural changes, relying instead on inherent positional embeddings\ncombined with a new complementary data sampling strategy. Extensive experiments\ndemonstrate that LayerComposer achieves superior spatial control and identity\npreservation compared to the state-of-the-art methods in multi-subject\npersonalized image generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LayerComposer\uff0c\u4e00\u4e2a\u7528\u4e8e\u4e2a\u6027\u5316\u591a\u4e3b\u4f53\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u753b\u5e03\u8868\u793a\u548c\u9501\u5b9a\u673a\u5236\u5b9e\u73b0\u4e86\u5bf9\u7a7a\u95f4\u7ec4\u5408\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u5e76\u5728\u591a\u4e3b\u4f53\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4e2a\u6027\u5316\u751f\u6210\u6a21\u578b\u867d\u7136\u89c6\u89c9\u4fdd\u771f\u5ea6\u9ad8\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u7ec4\u5408\u7684\u4ea4\u4e92\u63a7\u5236\uff0c\u5e76\u4e14\u5728\u5904\u7406\u591a\u4e2a\u4e3b\u4f53\u65f6\u6269\u5c55\u6027\u5dee\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u7528\u6237\u63a7\u5236\u80fd\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u5206\u5c42\u753b\u5e03\u8868\u793a\uff0c\u5c06\u6bcf\u4e2a\u4e3b\u4f53\u7f6e\u4e8e\u72ec\u7acb\u5c42\u4e2d\u5b9e\u73b0\u65e0\u906e\u6321\u7ec4\u5408\uff0c\u5e76\u63d0\u51fa\u4e86\u9501\u5b9a\u673a\u5236\u6765\u4fdd\u6301\u9009\u5b9a\u5c42\u7684\u9ad8\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u5141\u8bb8\u5176\u4ed6\u5c42\u7075\u6d3b\u9002\u5e94\u4e0a\u4e0b\u6587\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u67b6\u6784\u4fee\u6539\uff0c\u4f9d\u8d56\u4f4d\u7f6e\u5d4c\u5165\u548c\u4e92\u8865\u6570\u636e\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLayerComposer\u5728\u591a\u4e3b\u4f53\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u4e2d\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u7a7a\u95f4\u63a7\u5236\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u4ea4\u4e92\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5206\u5c42\u8868\u793a\u548c\u9501\u5b9a\u673a\u5236\u5728\u4e2a\u6027\u5316\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u4e3b\u4f53\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u4ea4\u4e92\u8303\u5f0f\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u573a\u666f\u7ec4\u5408\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2510.20822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20822", "abs": "https://arxiv.org/abs/2510.20822", "authors": ["Yihao Meng", "Hao Ouyang", "Yue Yu", "Qiuyu Wang", "Wen Wang", "Ka Leong Cheng", "Hanlin Wang", "Yixuan Li", "Cheng Chen", "Yanhong Zeng", "Yujun Shen", "Huamin Qu"], "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives", "comment": "Project page and code: https://holo-cine.github.io/", "summary": "State-of-the-art text-to-video models excel at generating isolated clips but\nfall short of creating the coherent, multi-shot narratives, which are the\nessence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model\nthat generates entire scenes holistically to ensure global consistency from the\nfirst shot to the last. Our architecture achieves precise directorial control\nthrough a Window Cross-Attention mechanism that localizes text prompts to\nspecific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within\nshots but sparse between them) ensures the efficiency required for minute-scale\ngeneration. Beyond setting a new state-of-the-art in narrative coherence,\nHoloCine develops remarkable emergent abilities: a persistent memory for\ncharacters and scenes, and an intuitive grasp of cinematic techniques. Our work\nmarks a pivotal shift from clip synthesis towards automated filmmaking, making\nend-to-end cinematic creation a tangible future. Our code is available at:\nhttps://holo-cine.github.io/.", "AI": {"tldr": "HoloCine\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u4f53\u751f\u6210\u8fde\u8d2f\u591a\u955c\u5934\u53d9\u4e8b\u89c6\u9891\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u7a97\u53e3\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u7a00\u758f\u955c\u5934\u95f4\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u5728\u53d9\u4e8b\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u7535\u5f71\u5236\u4f5c\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u64c5\u957f\u751f\u6210\u5b64\u7acb\u7247\u6bb5\uff0c\u4f46\u5728\u521b\u5efa\u8fde\u8d2f\u7684\u591a\u955c\u5934\u53d9\u4e8b\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u8fd9\u79cd\"\u53d9\u4e8b\u9e3f\u6c9f\"\u9650\u5236\u4e86\u771f\u6b63\u7684\u6545\u4e8b\u8bb2\u8ff0\u80fd\u529b\u3002", "method": "HoloCine\u91c7\u7528\u7a97\u53e3\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5c06\u6587\u672c\u63d0\u793a\u5b9a\u4f4d\u5230\u7279\u5b9a\u955c\u5934\uff0c\u540c\u65f6\u4f7f\u7528\u7a00\u758f\u955c\u5934\u95f4\u81ea\u6ce8\u610f\u529b\u6a21\u5f0f\uff08\u955c\u5934\u5185\u5bc6\u96c6\u4f46\u955c\u5934\u95f4\u7a00\u758f\uff09\uff0c\u786e\u4fdd\u5206\u949f\u7ea7\u751f\u6210\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "HoloCine\u5728\u53d9\u4e8b\u8fde\u8d2f\u6027\u65b9\u9762\u8bbe\u7acb\u4e86\u65b0\u7684\u6280\u672f\u6807\u51c6\uff0c\u5e76\u5c55\u73b0\u51fa\u663e\u8457\u7684\u65b0\u5174\u80fd\u529b\uff1a\u5bf9\u89d2\u8272\u548c\u573a\u666f\u7684\u6301\u4e45\u8bb0\u5fc6\uff0c\u4ee5\u53ca\u5bf9\u7535\u5f71\u6280\u672f\u7684\u76f4\u89c2\u7406\u89e3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6807\u5fd7\u7740\u4ece\u7247\u6bb5\u5408\u6210\u5230\u81ea\u52a8\u5316\u7535\u5f71\u5236\u4f5c\u7684\u5173\u952e\u8f6c\u53d8\uff0c\u4f7f\u7aef\u5230\u7aef\u7684\u7535\u5f71\u521b\u4f5c\u6210\u4e3a\u53ef\u5b9e\u73b0\u7684\u672a\u6765\uff0c\u4e3a\u8fde\u8d2f\u53d9\u4e8b\u89c6\u9891\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
