{"id": "2511.02358", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.02358", "abs": "https://arxiv.org/abs/2511.02358", "authors": ["Wongyu Kim", "Hochang Lee", "Sanghak Lee", "Yoonsung Kim", "Jaehyun Park"], "title": "Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation", "comment": "Accepted to MMGenSR Workshop (CIKM 2025)", "summary": "Query augmentation makes queries more meaningful by appending further\ninformation to the queries to find relevant documents. Current studies have\nproposed Large Language Model (LLM)-based embedders, which learn representation\nfor embedding and generation for query augmentation in a multi-task manner by\nleveraging the generative capabilities of LLM. During inference, these jointly\ntrained embedders have conducted query augmentation followed by embedding,\nshowing effective results. However, augmenting every query leads to substantial\nembedding latency and query augmentation can be detrimental to performance for\nsome queries. Also, previous methods have not been explored in multimodal\nenvironments. To tackle these problems, we propose M-Solomon, a universal\nmultimodal embedder that can adaptively determine when to augment queries. Our\napproach first divides the queries of the training datasets into two groups at\nthe dataset level. One includes queries that require augmentation and the other\nincludes queries that do not. Then, we introduces a synthesis process that\ngenerates appropriate augmentations for queries that require them by leveraging\na powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.\nThrough this step, M-Solomon can conduct query augmentation only when necessary\nby learning to generate synthetic augmentations with the prefix /augment for\nqueries that demand them and to generate the simple string /embed for others.\nExperimental results showed that M-Solomon not only surpassed the baseline\nwithout augmentation by a large margin but also outperformed the baseline that\nalways used augmentation, providing much faster embedding latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86M-Solomon\uff0c\u4e00\u79cd\u901a\u7528\u591a\u6a21\u6001\u5d4c\u5165\u5668\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u51b3\u5b9a\u4f55\u65f6\u8fdb\u884c\u67e5\u8be2\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u5d4c\u5165\u5668\u4e2d\u76f2\u76ee\u589e\u5f3a\u6240\u6709\u67e5\u8be2\u5bfc\u81f4\u7684\u5ef6\u8fdf\u95ee\u9898\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u5d4c\u5165\u5668\u5bf9\u6240\u6709\u67e5\u8be2\u8fdb\u884c\u589e\u5f3a\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u5d4c\u5165\u5ef6\u8fdf\uff0c\u4e14\u67d0\u4e9b\u67e5\u8be2\u7684\u589e\u5f3a\u53cd\u800c\u4f1a\u635f\u5bb3\u6027\u80fd\uff0c\u540c\u65f6\u5148\u524d\u65b9\u6cd5\u672a\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u63a2\u7d22\uff0c\u8fd9\u4e9b\u5c40\u9650\u6027\u4fc3\u4f7f\u672c\u7814\u7a76\u5f00\u53d1\u80fd\u591f\u81ea\u9002\u5e94\u51b3\u5b9a\u589e\u5f3a\u65f6\u673a\u7684\u591a\u6a21\u6001\u5d4c\u5165\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u5728\u6570\u636e\u96c6\u5c42\u9762\u5c06\u8bad\u7ec3\u67e5\u8be2\u5206\u4e3a\u9700\u8981\u589e\u5f3a\u548c\u4e0d\u9700\u8981\u589e\u5f3a\u4e24\u7ec4\uff0c\u5229\u7528\u5f3a\u5927\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u9700\u8981\u589e\u5f3a\u7684\u67e5\u8be2\u751f\u6210\u9002\u5f53\u7684\u589e\u5f3a\u5185\u5bb9\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u9002\u5e94\u67e5\u8be2\u589e\u5f3a\u673a\u5236\uff0c\u4ec5\u5bf9\u9700\u8981\u589e\u5f3a\u7684\u67e5\u8be2\u751f\u6210\u5e26\u6709/augment\u524d\u7f00\u7684\u5408\u6210\u589e\u5f3a\uff0c\u5bf9\u5176\u4ed6\u67e5\u8be2\u751f\u6210\u7b80\u5355\u7684/embed\u5b57\u7b26\u4e32\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cM-Solomon\u4e0d\u4ec5\u5927\u5e45\u8d85\u8d8a\u4e86\u65e0\u589e\u5f3a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e5f\u4f18\u4e8e\u59cb\u7ec8\u4f7f\u7528\u589e\u5f3a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u5feb\u7684\u5d4c\u5165\u5ef6\u8fdf\uff0c\u5728\u591a\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u81ea\u9002\u5e94\u67e5\u8be2\u589e\u5f3a\u7b56\u7565\u80fd\u591f\u6709\u6548\u5e73\u8861\u68c0\u7d22\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u7684\u67e5\u8be2\u5904\u7406\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u9009\u62e9\u6027\u589e\u5f3a\u6bd4\u76f2\u76ee\u589e\u5f3a\u6240\u6709\u67e5\u8be2\u66f4\u5177\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u5d4c\u5165\u65b9\u6cd5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2511.02366", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.02366", "abs": "https://arxiv.org/abs/2511.02366", "authors": ["Yudong Li", "Zhongliang Yang", "Kejiang Chen", "Wenxuan Wang", "Tianxin Zhang", "Sifang Wan", "Kecheng Wang", "Haitian Li", "Xu Wang", "Lefan Cheng", "Youdan Yang", "Baocheng Chen", "Ziyu Liu", "Yufei Sun", "Liyan Wu", "Wenya Wen", "Xingchi Gu", "Peiru Yang"], "title": "LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context", "comment": null, "summary": "In this work, we propose LiveSecBench, a dynamic and continuously updated\nsafety benchmark specifically for Chinese-language LLM application scenarios.\nLiveSecBench evaluates models across six critical dimensions (Legality, Ethics,\nFactuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in\nthe Chinese legal and social frameworks. This benchmark maintains relevance\nthrough a dynamic update schedule that incorporates new threat vectors, such as\nthe planned inclusion of Text-to-Image Generation Safety and Agentic Safety in\nthe next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,\nproviding a landscape of AI safety in the context of Chinese language. The\nleaderboard is publicly accessible at https://livesecbench.intokentech.cn/.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86LiveSecBench\uff0c\u4e00\u4e2a\u9488\u5bf9\u4e2d\u6587LLM\u5e94\u7528\u573a\u666f\u7684\u52a8\u6001\u6301\u7eed\u66f4\u65b0\u5b89\u5168\u57fa\u51c6\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u516d\u4e2a\u5173\u952e\u5b89\u5168\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5df2\u5bf918\u4e2a\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u4e2d\u6587\u8bed\u8a00\u73af\u5883\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u573a\u666f\u7684\u52a8\u6001\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u80fd\u591f\u6301\u7eed\u66f4\u65b0\u3001\u53cd\u6620\u4e2d\u6587\u6cd5\u5f8b\u548c\u793e\u4f1a\u6846\u67b6\u7684\u5b89\u5168\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86LiveSecBench\u57fa\u51c6\uff0c\u5305\u542b\u5408\u6cd5\u6027\u3001\u4f26\u7406\u3001\u4e8b\u5b9e\u6027\u3001\u9690\u79c1\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u63a8\u7406\u5b89\u5168\u516d\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u91c7\u7528\u52a8\u6001\u66f4\u65b0\u673a\u5236\u7eb3\u5165\u65b0\u7684\u5a01\u80c1\u5411\u91cf\uff0c\u5982\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5b89\u5168\u548c\u667a\u80fd\u4f53\u5b89\u5168\u3002", "result": "LiveSecBench\uff08v251030\uff09\u5df2\u8bc4\u4f30\u4e8618\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u4e2d\u6587\u8bed\u5883\u4e0bAI\u5b89\u5168\u7684\u5168\u666f\u56fe\uff0c\u57fa\u51c6\u6392\u884c\u699c\u5df2\u516c\u5f00\u53ef\u8bbf\u95ee\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u4e2d\u6587LLM\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u673a\u5236\u786e\u4fdd\u8bc4\u4f30\u7684\u65f6\u6548\u6027\uff0c\u4e3aAI\u5b89\u5168\u7814\u7a76\u548c\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u4f9d\u636e\u3002"}}
{"id": "2511.02770", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.02770", "abs": "https://arxiv.org/abs/2511.02770", "authors": ["Hung-Ting Chen", "Xiang Liu", "Shauli Ravfogel", "Eunsol Choi"], "title": "Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval", "comment": null, "summary": "Most text retrievers generate \\emph{one} query vector to retrieve relevant\ndocuments. Yet, the conditional distribution of relevant documents for the\nquery may be multimodal, e.g., representing different interpretations of the\nquery. We first quantify the limitations of existing retrievers. All retrievers\nwe evaluate struggle more as the distance between target document embeddings\ngrows. To address this limitation, we develop a new retriever architecture,\n\\emph{A}utoregressive \\emph{M}ulti-\\emph{E}mbedding \\emph{R}etriever (AMER).\nOur model autoregressively generates multiple query vectors, and all the\npredicted query vectors are used to retrieve documents from the corpus. We show\nthat on the synthetic vectorized data, the proposed method could capture\nmultiple target distributions perfectly, showing 4x better performance than\nsingle embedding model. We also fine-tune our model on real-world multi-answer\nretrieval datasets and evaluate in-domain. AMER presents 4 and 21\\% relative\ngains over single-embedding baselines on two datasets we evaluate on.\nFurthermore, we consistently observe larger gains on the subset of dataset\nwhere the embeddings of the target documents are less similar to each other. We\ndemonstrate the potential of using a multi-query vector retriever and open up a\nnew direction for future work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52\u591a\u5d4c\u5165\u68c0\u7d22\u5668\uff08AMER\uff09\uff0c\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u67e5\u8be2\u5411\u91cf\u6765\u6355\u83b7\u76f8\u5173\u6587\u6863\u7684\u591a\u6a21\u6001\u5206\u5e03\uff0c\u76f8\u6bd4\u4f20\u7edf\u5355\u5411\u91cf\u68c0\u7d22\u5668\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u68c0\u7d22\u5668\u901a\u5e38\u53ea\u751f\u6210\u5355\u4e2a\u67e5\u8be2\u5411\u91cf\u6765\u68c0\u7d22\u76f8\u5173\u6587\u6863\uff0c\u4f46\u76f8\u5173\u6587\u6863\u7684\u6761\u4ef6\u5206\u5e03\u53ef\u80fd\u662f\u591a\u6a21\u6001\u7684\uff0c\u4ee3\u8868\u67e5\u8be2\u7684\u4e0d\u540c\u89e3\u91ca\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u68c0\u7d22\u5668\u5728\u76ee\u6807\u6587\u6863\u5d4c\u5165\u8ddd\u79bb\u8f83\u5927\u65f6\u8868\u73b0\u8f83\u5dee\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5904\u7406\u590d\u6742\u67e5\u8be2\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u68c0\u7d22\u5668\u67b6\u6784\u2014\u2014\u81ea\u56de\u5f52\u591a\u5d4c\u5165\u68c0\u7d22\u5668\uff08AMER\uff09\uff0c\u8be5\u6a21\u578b\u81ea\u56de\u5f52\u5730\u751f\u6210\u591a\u4e2a\u67e5\u8be2\u5411\u91cf\uff0c\u6240\u6709\u9884\u6d4b\u7684\u67e5\u8be2\u5411\u91cf\u90fd\u7528\u4e8e\u4ece\u8bed\u6599\u5e93\u4e2d\u68c0\u7d22\u6587\u6863\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u6355\u83b7\u591a\u6a21\u6001\u7684\u76ee\u6807\u5206\u5e03\u3002", "result": "\u5728\u5408\u6210\u5411\u91cf\u5316\u6570\u636e\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u5b8c\u7f8e\u6355\u83b7\u591a\u4e2a\u76ee\u6807\u5206\u5e03\uff0c\u6027\u80fd\u6bd4\u5355\u5d4c\u5165\u6a21\u578b\u63d0\u53474\u500d\u3002\u5728\u771f\u5b9e\u4e16\u754c\u591a\u7b54\u6848\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cAMER\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5206\u522b\u6bd4\u5355\u5d4c\u5165\u57fa\u7ebf\u76f8\u5bf9\u63d0\u53474%\u548c21%\u3002\u5728\u76ee\u6807\u6587\u6863\u5d4c\u5165\u76f8\u4f3c\u5ea6\u8f83\u4f4e\u7684\u6570\u636e\u5b50\u96c6\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u4f7f\u7528\u591a\u67e5\u8be2\u5411\u91cf\u68c0\u7d22\u5668\u7684\u6f5c\u529b\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u67e5\u8be2\u7684\u591a\u6a21\u6001\u5206\u5e03\u95ee\u9898\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u76ee\u6807\u6587\u6863\u5d4c\u5165\u5dee\u5f02\u8f83\u5927\u7684\u573a\u666f\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff0c\u4e3a\u672a\u6765\u68c0\u7d22\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2511.02280", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.02280", "abs": "https://arxiv.org/abs/2511.02280", "authors": ["Fangxun Shu", "Yongjie Ye", "Yue Liao", "Zijian Kang", "Weijie Yin", "Jiacong Wang", "Xiao Liang", "Shuicheng Yan", "Chao Feng"], "title": "SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning", "comment": null, "summary": "We introduce SAIL-RL, a reinforcement learning (RL) post-training framework\nthat enhances the reasoning capabilities of multimodal large language models\n(MLLMs) by teaching them when and how to think. Existing approaches are limited\nby outcome-only supervision, which rewards correct answers without ensuring\nsound reasoning, and by uniform thinking strategies, which often lead to\noverthinking on simple tasks and underthinking on complex ones. SAIL-RL\naddresses these challenges with a dual reward system: the Thinking Reward,\nwhich evaluates reasoning quality through factual grounding, logical coherence,\nand answer consistency, and the Judging Reward, which adaptively determines\nwhether deep reasoning or direct answering is appropriate. Experiments on the\nstate-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal\nunderstanding benchmarks at both 4B and 8B scales, achieving competitive\nperformance against commercial closed-source models such as GPT-4o, and\nsubstantially reduces hallucinations, establishing it as a principled framework\nfor building more reliable and adaptive MLLMs. The code will be available at\nhttps://github.com/BytedanceDouyinContent/SAIL-RL.", "AI": {"tldr": "SAIL-RL\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5bfc\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f55\u65f6\u601d\u8003\u4ee5\u53ca\u5982\u4f55\u601d\u8003\u6765\u589e\u5f3a\u5176\u63a8\u7406\u80fd\u529b\u3002\u8be5\u6846\u67b6\u91c7\u7528\u53cc\u5956\u52b1\u7cfb\u7edf\u8bc4\u4f30\u63a8\u7406\u8d28\u91cf\u5e76\u81ea\u9002\u5e94\u5730\u51b3\u5b9a\u6df1\u5ea6\u63a8\u7406\u6216\u76f4\u63a5\u56de\u7b54\u7684\u9002\u7528\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u76d1\u7763\u673a\u5236\uff0c\u4ec5\u5956\u52b1\u6b63\u786e\u7b54\u6848\u800c\u4e0d\u786e\u4fdd\u63a8\u7406\u8fc7\u7a0b\u7684\u5408\u7406\u6027\uff0c\u540c\u65f6\u91c7\u7528\u7edf\u4e00\u7684\u601d\u8003\u7b56\u7565\u5bfc\u81f4\u7b80\u5355\u4efb\u52a1\u8fc7\u5ea6\u601d\u8003\u800c\u590d\u6742\u4efb\u52a1\u601d\u8003\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "SAIL-RL\u63d0\u51fa\u53cc\u5956\u52b1\u7cfb\u7edf\uff1a\u601d\u8003\u5956\u52b1\u901a\u8fc7\u4e8b\u5b9e\u57fa\u7840\u3001\u903b\u8f91\u4e00\u81f4\u6027\u548c\u7b54\u6848\u4e00\u81f4\u6027\u8bc4\u4f30\u63a8\u7406\u8d28\u91cf\uff0c\u5224\u65ad\u5956\u52b1\u81ea\u9002\u5e94\u5730\u51b3\u5b9a\u4f55\u65f6\u9700\u8981\u6df1\u5ea6\u63a8\u7406\u6216\u76f4\u63a5\u56de\u7b54\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u540e\u8bad\u7ec3\u4f18\u5316\u3002", "result": "\u5728SAIL-VL2\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAIL-RL\u57284B\u548c8B\u89c4\u6a21\u4e0a\u5747\u63d0\u5347\u4e86\u63a8\u7406\u548c\u591a\u6a21\u6001\u7406\u89e3\u57fa\u51c6\u6027\u80fd\uff0c\u8fbe\u5230\u4e0eGPT-4o\u7b49\u5546\u4e1a\u95ed\u6e90\u6a21\u578b\u7ade\u4e89\u7684\u6c34\u5e73\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\u3002", "conclusion": "SAIL-RL\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u548c\u81ea\u9002\u5e94\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5bfc\u6a21\u578b\u4f55\u65f6\u601d\u8003\u4ee5\u53ca\u5982\u4f55\u601d\u8003\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.01914", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01914", "abs": "https://arxiv.org/abs/2511.01914", "authors": ["Yuan Zhang", "Chenyu Xue", "Wenjie Xu", "Chao Ji", "Jiajia wu", "Jia Pan"], "title": "iFlyBot-VLA Technical Report", "comment": null, "summary": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model\ntrained under a novel framework. The main contributions are listed as follows:\n(1) a latent action model thoroughly trained on large-scale human and robotic\nmanipulation videos; (2) a dual-level action representation framework that\njointly supervises both the Vision-Language Model (VLM) and the action expert\nduring training; (3) a mixed training strategy that combines robot trajectory\ndata with general QA and spatial QA datasets, effectively enhancing the 3D\nperceptual and reasoning capabilities of the VLM backbone. Specifically, the\nVLM is trained to predict two complementary forms of actions: latent actions,\nderived from our latent action model pretrained on cross-embodiment\nmanipulation data, which capture implicit high-level intentions; and structured\ndiscrete action tokens, obtained through frequency-domain transformations of\ncontinuous control signals, which encode explicit low-level dynamics. This dual\nsupervision aligns the representation spaces of language, vision, and action,\nenabling the VLM to directly contribute to action generation. Experimental\nresults on the LIBERO Franka benchmark demonstrate the superiority of our\nframe-work, while real-world evaluations further show that iFlyBot-VLA achieves\ncompetitive success rates across diverse and challenging manipulation tasks.\nFurthermore, we plan to open-source a portion of our self-constructed dataset\nto support future research in the community", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86iFlyBot-VLA\uff0c\u8fd9\u662f\u4e00\u4e2a\u5728\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\u4e0b\u8bad\u7ec3\u7684\u5927\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u7ea7\u52a8\u4f5c\u8868\u793a\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b58\u5728\u52a8\u4f5c\u8868\u793a\u4e0e\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u7a7a\u95f4\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u6355\u6349\u9ad8\u5c42\u610f\u56fe\u548c\u4f4e\u5c42\u52a8\u6001\u7684\u52a8\u4f5c\u8868\u793a\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u53cc\u7ea7\u52a8\u4f5c\u8868\u793a\u6846\u67b6\uff0c\u7ed3\u5408\u6f5c\u5728\u52a8\u4f5c\u6a21\u578b\u548c\u7ed3\u6784\u5316\u79bb\u6563\u52a8\u4f5c\u6807\u8bb0\uff0c\u6f5c\u5728\u52a8\u4f5c\u4ece\u8de8\u5177\u8eab\u64cd\u4f5c\u6570\u636e\u4e2d\u5b66\u4e60\u9690\u542b\u9ad8\u5c42\u610f\u56fe\uff0c\u79bb\u6563\u52a8\u4f5c\u6807\u8bb0\u901a\u8fc7\u9891\u57df\u53d8\u6362\u7f16\u7801\u663e\u5f0f\u4f4e\u5c42\u52a8\u6001\uff0c\u91c7\u7528\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u7ed3\u5408\u673a\u5668\u4eba\u8f68\u8ff9\u6570\u636e\u4e0e\u901a\u7528\u95ee\u7b54\u6570\u636e\u96c6\u3002", "result": "\u5728LIBERO Franka\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u663e\u793a\u5728\u591a\u6837\u5316\u6311\u6218\u6027\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u53cc\u7ea7\u52a8\u4f5c\u8868\u793a\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u80fd\u591f\u6709\u6548\u5bf9\u9f50\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u52a8\u4f5c\u8868\u793a\u7a7a\u95f4\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u8ba1\u5212\u5f00\u6e90\u90e8\u5206\u81ea\u5efa\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u793e\u533a\u7814\u7a76\u3002"}}
{"id": "2511.02071", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02071", "abs": "https://arxiv.org/abs/2511.02071", "authors": ["Xinyi Lin", "Yuyang Zhang", "Yuanhang Gan", "Juntao Chen", "Hao Shen", "Yichun He", "Lijun Li", "Ze Yuan", "Shuang Wang", "Chaohao Wang", "Rui Zhang", "Na Li", "Jia Liu"], "title": "Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing", "comment": null, "summary": "Scientific experiment and manufacture rely on complex, multi-step procedures\nthat demand continuous human expertise for precise execution and\ndecision-making. Despite advances in machine learning and automation,\nconventional models remain confined to virtual domains, while real-world\nexperiment and manufacture still rely on human supervision and expertise. This\ngap between machine intelligence and physical execution limits reproducibility,\nscalability, and accessibility across scientific and manufacture workflows.\nHere, we introduce human-AI co-embodied intelligence, a new form of physical AI\nthat unites human users, agentic AI, and wearable hardware into an integrated\nsystem for real-world experiment and intelligent manufacture. In this paradigm,\nhumans provide precise execution and control, while agentic AI contributes\nmemory, contextual reasoning, adaptive planning, and real-time feedback. The\nwearable interface continuously captures the experimental and manufacture\nprocesses, facilitates seamless communication between humans and AI for\ncorrective guidance and interpretable collaboration. As a demonstration, we\npresent Agentic-Physical Experimentation (APEX) system, coupling agentic\nreasoning with physical execution through mixed-reality. APEX observes and\ninterprets human actions, aligns them with standard operating procedures,\nprovides 3D visual guidance, and analyzes every step. Implemented in a\ncleanroom for flexible electronics fabrication, APEX system achieves\ncontext-aware reasoning with accuracy exceeding general multimodal large\nlanguage models, corrects errors in real time, and transfers expertise to\nbeginners. These results establish a new class of agentic-physical-human\nintelligence that extends agentic reasoning beyond computation into the\nphysical domain, transforming scientific research and manufacturing into\nautonomous, traceable, interpretable, and scalable processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u540c\u5177\u8eab\u667a\u80fd\u65b0\u8303\u5f0f\uff0c\u5c06\u4eba\u7c7b\u7528\u6237\u3001\u667a\u80fd\u4f53AI\u548c\u53ef\u7a7f\u6234\u786c\u4ef6\u96c6\u6210\u5230\u7edf\u4e00\u7cfb\u7edf\u4e2d\uff0c\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\u548c\u667a\u80fd\u5236\u9020\u3002\u901a\u8fc7APEX\u7cfb\u7edf\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u63a8\u7406\u4e0e\u7269\u7406\u6267\u884c\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u8d85\u8d8a\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u79d1\u5b66\u5b9e\u9a8c\u548c\u5236\u9020\u4f9d\u8d56\u590d\u6742\u591a\u6b65\u9aa4\u7a0b\u5e8f\uff0c\u9700\u8981\u6301\u7eed\u7684\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u8fdb\u884c\u7cbe\u786e\u6267\u884c\u548c\u51b3\u7b56\u3002\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u548c\u81ea\u52a8\u5316\u6280\u672f\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f20\u7edf\u6a21\u578b\u4ecd\u5c40\u9650\u4e8e\u865a\u62df\u9886\u57df\uff0c\u800c\u73b0\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\u548c\u5236\u9020\u4ecd\u4f9d\u8d56\u4eba\u7c7b\u76d1\u7763\u548c\u4e13\u4e1a\u77e5\u8bc6\u3002\u673a\u5668\u667a\u80fd\u4e0e\u7269\u7406\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\u9650\u5236\u4e86\u79d1\u5b66\u548c\u5236\u9020\u5de5\u4f5c\u6d41\u7a0b\u7684\u53ef\u91cd\u590d\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u63d0\u51fa\u4eba\u673a\u534f\u540c\u5177\u8eab\u667a\u80fd\u8303\u5f0f\uff0c\u5c06\u4eba\u7c7b\u7528\u6237\u3001\u667a\u80fd\u4f53AI\u548c\u53ef\u7a7f\u6234\u786c\u4ef6\u96c6\u6210\u5230\u7edf\u4e00\u7cfb\u7edf\u4e2d\u3002\u5f00\u53d1\u4e86APEX\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df7\u5408\u73b0\u5b9e\u6280\u672f\u5c06\u667a\u80fd\u4f53\u63a8\u7406\u4e0e\u7269\u7406\u6267\u884c\u76f8\u7ed3\u5408\uff0c\u7cfb\u7edf\u80fd\u591f\u89c2\u5bdf\u548c\u89e3\u91ca\u4eba\u7c7b\u52a8\u4f5c\uff0c\u4e0e\u6807\u51c6\u64cd\u4f5c\u7a0b\u5e8f\u5bf9\u9f50\uff0c\u63d0\u4f9b3D\u89c6\u89c9\u6307\u5bfc\uff0c\u5e76\u5206\u6790\u6bcf\u4e2a\u6b65\u9aa4\u3002", "result": "\u5728\u6d01\u51c0\u5ba4\u67d4\u6027\u7535\u5b50\u5236\u9020\u73af\u5883\u4e2d\u5b9e\u73b0\u7684APEX\u7cfb\u7edf\uff0c\u5176\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u51c6\u786e\u7387\u8d85\u8fc7\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u5b9e\u65f6\u7ea0\u6b63\u9519\u8bef\uff0c\u5e76\u5c06\u4e13\u4e1a\u77e5\u8bc6\u4f20\u9012\u7ed9\u521d\u5b66\u8005\u3002\u7cfb\u7edf\u5b9e\u73b0\u4e86\u81ea\u4e3b\u3001\u53ef\u8ffd\u6eaf\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u5b9e\u9a8c\u548c\u5236\u9020\u6d41\u7a0b\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u7c7b\u65b0\u7684\u667a\u80fd\u4f53-\u7269\u7406-\u4eba\u7c7b\u667a\u80fd\u7cfb\u7edf\uff0c\u5c06\u667a\u80fd\u4f53\u63a8\u7406\u4ece\u8ba1\u7b97\u9886\u57df\u6269\u5c55\u5230\u7269\u7406\u9886\u57df\u3002\u8be5\u8303\u5f0f\u5c06\u79d1\u5b66\u7814\u7a76\u548c\u5236\u9020\u8f6c\u53d8\u4e3a\u81ea\u4e3b\u3001\u53ef\u8ffd\u6eaf\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u8fc7\u7a0b\uff0c\u4e3a\u4eba\u673a\u534f\u540c\u667a\u80fd\u5728\u7269\u7406\u4e16\u754c\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.02360", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.02360", "abs": "https://arxiv.org/abs/2511.02360", "authors": ["Jizheng Ma", "Xiaofei Zhou", "Yanlong Song", "Han Yan"], "title": "CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning", "comment": null, "summary": "In human cognition, there exist numerous thought processes that are tacit and\nbeyond verbal expression, enabling us to understand and interact with the world\nin multiple ways. However, contemporary Vision-Language Models (VLMs) remain\nconstrained to reasoning within the discrete and rigid space of linguistic\ntokens, thereby bottlenecking the rich, high-dimensional nature of visual\nperception. To bridge this gap, we propose CoCoVa (Chain of Continuous\nVision-Language Thought), a novel framework for vision-language model that\nleverages continuous cross-modal reasoning for diverse vision-language tasks.\nThe core of CoCoVa is an iterative reasoning cycle, where a novel Latent\nQ-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a\nchain of latent thought vectors through cross-modal fusion. To focus this\nprocess, a token selection mechanism dynamically identifies salient visual\nregions, mimicking attentional focus. To ensure these latent thoughts remain\ngrounded, we train the model with a multi-task objective that combines\ncontrastive learning and diffusion-based reconstruction, enforcing alignment\nbetween latent representations and both visual and textual modalities.\nEvaluations show CoCoVa improves accuracy and token efficiency over strong\nbaselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B\nmodels on almost all benchmarks. When scaled to 7B LLM backbones, it remains\ncompetitive with state-of-the-art models. Qualitative analysis validates that\nlearned latent space captures interpretable and structured reasoning patterns,\nhighlighting the potential of CoCoVa to bridge the representational gap between\ndiscrete language processing and the continuous nature of visual understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CoCoVa\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8fde\u7eed\u8de8\u6a21\u6001\u63a8\u7406\u673a\u5236\u6765\u5f25\u5408\u89c6\u89c9\u611f\u77e5\u7684\u4e30\u5bcc\u9ad8\u7ef4\u7279\u6027\u4e0e\u8bed\u8a00\u6a21\u578b\u79bb\u6563\u63a8\u7406\u7a7a\u95f4\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5728\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u5728\u79bb\u6563\u8bed\u8a00\u6807\u8bb0\u7a7a\u95f4\u4e2d\u8fdb\u884c\u63a8\u7406\uff0c\u65e0\u6cd5\u5145\u5206\u8868\u8fbe\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u90a3\u4e9b\u96be\u4ee5\u8a00\u4f20\u7684\u601d\u7ef4\u8fc7\u7a0b\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u4e30\u5bcc\u9ad8\u7ef4\u89c6\u89c9\u4fe1\u606f\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "CoCoVa\u6846\u67b6\u91c7\u7528\u8fed\u4ee3\u63a8\u7406\u5faa\u73af\u673a\u5236\uff0c\u5176\u4e2d\u65b0\u578bLatent Q-Former\u4f5c\u4e3a\u52a8\u6001\u63a8\u7406\u5f15\u64ce\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u878d\u5408\u8fed\u4ee3\u4f18\u5316\u6f5c\u5728\u601d\u7ef4\u5411\u91cf\u94fe\uff0c\u5e76\u914d\u5408\u52a8\u6001\u6807\u8bb0\u9009\u62e9\u673a\u5236\u805a\u7126\u5173\u952e\u89c6\u89c9\u533a\u57df\uff0c\u540c\u65f6\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u57fa\u4e8e\u6269\u6563\u7684\u91cd\u6784\u591a\u4efb\u52a1\u76ee\u6807\u6765\u786e\u4fdd\u6f5c\u5728\u8868\u793a\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCoCoVa\u5728\u51c6\u786e\u6027\u548c\u6807\u8bb0\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u4f7f\u75281.5B\u9aa8\u5e72\u7f51\u7edc\u65f6\u5728\u51e0\u4e4e\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a7B-9B\u5927\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5f53\u6269\u5c55\u52307B\u9aa8\u5e72\u65f6\u4ecd\u80fd\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u5b66\u4e60\u7684\u6f5c\u5728\u7a7a\u95f4\u80fd\u591f\u6355\u83b7\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u5316\u63a8\u7406\u6a21\u5f0f\uff0c\u8bc1\u660e\u4e86CoCoVa\u5728\u5f25\u5408\u79bb\u6563\u8bed\u8a00\u5904\u7406\u4e0e\u8fde\u7eed\u89c6\u89c9\u7406\u89e3\u4e4b\u95f4\u8868\u5f81\u9e3f\u6c9f\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.02014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02014", "abs": "https://arxiv.org/abs/2511.02014", "authors": ["Tuan Truong", "Guillermo Jimenez Perez", "Pedro Osorio", "Matthias Lenga"], "title": "Towards Selection of Large Multimodal Models as Engines for Burned-in Protected Health Information Detection in Medical Images", "comment": "Submitted to ISBI 2026", "summary": "The detection of Protected Health Information (PHI) in medical imaging is\ncritical for safeguarding patient privacy and ensuring compliance with\nregulatory frameworks. Traditional detection methodologies predominantly\nutilize Optical Character Recognition (OCR) models in conjunction with named\nentity recognition. However, recent advancements in Large Multimodal Model\n(LMM) present new opportunities for enhanced text extraction and semantic\nanalysis. In this study, we systematically benchmark three prominent closed and\nopen-sourced LMMs, namely GPT-4o, Gemini 2.5 Flash, and Qwen 2.5 7B, utilizing\ntwo distinct pipeline configurations: one dedicated to text analysis alone and\nanother integrating both OCR and semantic analysis. Our results indicate that\nLMM exhibits superior OCR efficacy (WER: 0.03-0.05, CER: 0.02-0.03) compared to\nconventional models like EasyOCR. However, this improvement in OCR performance\ndoes not consistently correlate with enhanced overall PHI detection accuracy.\nThe strongest performance gains are observed on test cases with complex imprint\npatterns. In scenarios where text regions are well readable with sufficient\ncontrast, and strong LMMs are employed for text analysis after OCR, different\npipeline configurations yield similar results. Furthermore, we provide\nempirically grounded recommendations for LMM selection tailored to specific\noperational constraints and propose a deployment strategy that leverages\nscalable and modular infrastructure.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u533b\u7597\u56fe\u50cf\u4e2d\u53d7\u4fdd\u62a4\u5065\u5eb7\u4fe1\u606f\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0LMM\u5728OCR\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4f46\u6574\u4f53PHI\u68c0\u6d4b\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u5e76\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u6a21\u578b\u9009\u62e9\u548c\u90e8\u7f72\u7b56\u7565\u5efa\u8bae\u3002", "motivation": "\u4f20\u7edf\u533b\u7597\u56fe\u50cfPHI\u68c0\u6d4b\u4e3b\u8981\u4f9d\u8d56OCR\u6a21\u578b\u7ed3\u5408\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u65b9\u6cd5\uff0c\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u800c\u65b0\u5174\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4e3a\u6587\u672c\u63d0\u53d6\u548c\u8bed\u4e49\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u673a\u4f1a\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86GPT-4o\u3001Gemini 2.5 Flash\u548cQwen 2.5 7B\u4e09\u4e2a\u4e3b\u6d41LMM\uff0c\u91c7\u7528\u4e24\u79cd\u5904\u7406\u6d41\u7a0b\uff1a\u7eaf\u6587\u672c\u5206\u6790\u6d41\u7a0b\u4ee5\u53caOCR\u4e0e\u8bed\u4e49\u5206\u6790\u7ed3\u5408\u7684\u6df7\u5408\u6d41\u7a0b\uff0c\u4e0e\u4f20\u7edfEasyOCR\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "LMM\u5728OCR\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff08WER\uff1a0.03-0.05\uff0cCER\uff1a0.02-0.03\uff09\uff0c\u4f46\u5728\u590d\u6742\u5370\u8bb0\u6a21\u5f0f\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5bf9\u4e8e\u9ad8\u5bf9\u6bd4\u5ea6\u53ef\u8bfb\u6587\u672c\u533a\u57df\uff0c\u4e0d\u540c\u6d41\u7a0b\u914d\u7f6e\u6548\u679c\u76f8\u8fd1\uff0cOCR\u6027\u80fd\u63d0\u5347\u5e76\u672a\u4e00\u81f4\u8f6c\u5316\u4e3a\u6574\u4f53PHI\u68c0\u6d4b\u51c6\u786e\u7387\u7684\u63d0\u9ad8\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LMM\u5728\u533b\u7597\u56fe\u50cfPHI\u68c0\u6d4b\u4e2d\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\uff0c\u4e3a\u4e0d\u540c\u64cd\u4f5c\u7ea6\u675f\u4e0b\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u53ef\u6269\u5c55\u6a21\u5757\u5316\u57fa\u7840\u8bbe\u65bd\u7684\u90e8\u7f72\u7b56\u7565\uff0c\u5bf9\u5b9e\u9645\u533b\u7597\u9690\u79c1\u4fdd\u62a4\u5e94\u7528\u5177\u6709\u91cd\u8981\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2511.02243", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02243", "abs": "https://arxiv.org/abs/2511.02243", "authors": ["Zhuoran Zhang", "Tengyue Wang", "Xilin Gong", "Yang Shi", "Haotian Wang", "Di Wang", "Lijie Hu"], "title": "When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs", "comment": "19 pages", "summary": "Multimodal large language models (MLLMs) must resolve conflicts when\ndifferent modalities provide contradictory information, a process we term\nmodality following. Prior work measured this behavior only with coarse\ndataset-level statistics, overlooking the influence of model's confidence in\nunimodal reasoning. In this paper, we introduce a new framework that decomposes\nmodality following into two fundamental factors: relative reasoning uncertainty\n(the case-specific confidence gap between unimodal predictions) and inherent\nmodality preference( a model's stable bias when uncertainties are balanced). To\nvalidate this framework, we construct a controllable dataset that\nsystematically varies the reasoning difficulty of visual and textual inputs.\nUsing entropy as a fine-grained uncertainty metric, we uncover a universal law:\nthe probability of following a modality decreases monotonically as its relative\nuncertainty increases. At the relative difficulty level where the model tends\nto follow both modalities with comparable probability what we call the balance\npoint, a practical indicator of the model's inherent preference. Unlike\ntraditional macro-level ratios, this measure offers a more principled and less\nconfounded way to characterize modality bias, disentangling it from unimodal\ncapabilities and dataset artifacts. Further, by probing layer-wise predictions,\nwe reveal the internal mechanism of oscillation: in ambiguous regions near the\nbalance point, models vacillate between modalities across layers, explaining\nexternally observed indecision. Together, these findings establish relative\nuncertainty and inherent preference as the two governing principles of modality\nfollowing, offering both a quantitative framework and mechanistic insight into\nhow MLLMs resolve conflicting information.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u89e3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6a21\u6001\u8ddf\u968f\u884c\u4e3a\u7684\u65b0\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u76f8\u5bf9\u63a8\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u5185\u5728\u6a21\u6001\u504f\u597d\u8fd9\u4e24\u4e2a\u57fa\u672c\u56e0\u7d20\u5982\u4f55\u5171\u540c\u51b3\u5b9a\u6a21\u578b\u5728\u51b2\u7a81\u4fe1\u606f\u4e0b\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\u901a\u8fc7\u6784\u5efa\u53ef\u63a7\u6570\u636e\u96c6\u548c\u5f15\u5165\u71b5\u4f5c\u4e3a\u7ec6\u7c92\u5ea6\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u53d1\u73b0\u4e86\u6a21\u6001\u8ddf\u968f\u6982\u7387\u968f\u76f8\u5bf9\u4e0d\u786e\u5b9a\u6027\u5355\u8c03\u4e0b\u964d\u7684\u666e\u904d\u89c4\u5f8b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u4f7f\u7528\u7c97\u7c92\u5ea6\u7684\u6570\u636e\u96c6\u7ea7\u7edf\u8ba1\u6765\u6d4b\u91cf\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u8ddf\u968f\u884c\u4e3a\uff0c\u5ffd\u7565\u4e86\u6a21\u578b\u5728\u5355\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u5f71\u54cd\u3002\u8fd9\u79cd\u7b80\u5316\u65b9\u6cd5\u65e0\u6cd5\u63ed\u793a\u6a21\u6001\u51b2\u7a81\u89e3\u51b3\u7684\u5185\u5728\u673a\u5236\uff0c\u7279\u522b\u662f\u5f53\u4e0d\u540c\u6a21\u6001\u63d0\u4f9b\u77db\u76fe\u4fe1\u606f\u65f6\u6a21\u578b\u51b3\u7b56\u7684\u6df1\u5c42\u539f\u7406\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u63a7\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\u7684\u63a8\u7406\u96be\u5ea6\uff0c\u5e76\u4f7f\u7528\u71b5\u4f5c\u4e3a\u7ec6\u7c92\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u6307\u6807\u3002\u901a\u8fc7\u5206\u6790\u5c42\u95f4\u9884\u6d4b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u5e73\u8861\u70b9\u9644\u8fd1\u533a\u57df\u5185\u7684\u5185\u90e8\u632f\u8361\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u4e86\u4e00\u4e2a\u666e\u904d\u89c4\u5f8b\uff1a\u6a21\u6001\u8ddf\u968f\u6982\u7387\u968f\u5176\u76f8\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u589e\u52a0\u800c\u5355\u8c03\u4e0b\u964d\u3002\u5728\u5e73\u8861\u70b9\u5904\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u4ee5\u53ef\u6bd4\u8f83\u7684\u6982\u7387\u8ddf\u968f\u4e24\u79cd\u6a21\u6001\uff0c\u8fd9\u4e3a\u5185\u5728\u6a21\u6001\u504f\u597d\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u6807\u3002\u5c42\u95f4\u5206\u6790\u663e\u793a\uff0c\u5728\u5e73\u8861\u70b9\u9644\u8fd1\u7684\u6a21\u7cca\u533a\u57df\uff0c\u6a21\u578b\u4f1a\u5728\u4e0d\u540c\u5c42\u95f4\u5728\u6a21\u6001\u4e4b\u95f4\u632f\u8361\u3002", "conclusion": "\u76f8\u5bf9\u4e0d\u786e\u5b9a\u6027\u548c\u5185\u5728\u504f\u597d\u662f\u6a21\u6001\u8ddf\u968f\u7684\u4e24\u4e2a\u4e3b\u5bfc\u539f\u5219\uff0c\u4e3a\u7406\u89e3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u89e3\u51b3\u51b2\u7a81\u4fe1\u606f\u63d0\u4f9b\u4e86\u5b9a\u91cf\u6846\u67b6\u548c\u673a\u5236\u6027\u6d1e\u89c1\u3002\u8fd9\u4e00\u53d1\u73b0\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u5b8f\u89c2\u6bd4\u7387\u66f4\u539f\u5219\u5316\u3001\u66f4\u5c11\u6df7\u6dc6\u7684\u65b9\u5f0f\u6765\u8868\u5f81\u6a21\u6001\u504f\u5dee\uff0c\u5c06\u5176\u4e0e\u5355\u6a21\u6001\u80fd\u529b\u548c\u6570\u636e\u96c6\u4f2a\u5f71\u5206\u79bb\u5f00\u6765\u3002"}}
{"id": "2511.02495", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.02495", "abs": "https://arxiv.org/abs/2511.02495", "authors": ["Zixuan Liu", "Siavash H. Khajavi", "Guangkai Jiang"], "title": "DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding", "comment": "Advances in Neural Information Processing Systems 2025 (NeurIPS\n  2025), Poster, https://neurips.cc/virtual/2025/loc/san-diego/poster/121400", "summary": "Recent advances in multi-modal models have demonstrated strong performance in\ntasks such as image generation and reasoning. However, applying these models to\nthe fire domain remains challenging due to the lack of publicly available\ndatasets with high-quality fire domain annotations. To address this gap, we\nintroduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k\nhigh-resolution fire-related images and 2.5k real-world fire-related videos\ncovering a wide range of fire types, environments, and risk levels. The data\nare annotated with both traditional computer vision labels (e.g., bounding\nboxes) and detailed textual prompts describing the scene, enabling applications\nsuch as synthetic data generation and fire risk reasoning. DetectiumFire offers\nclear advantages over existing benchmarks in scale, diversity, and data\nquality, significantly reducing redundancy and enhancing coverage of real-world\nscenarios. We validate the utility of DetectiumFire across multiple tasks,\nincluding object detection, diffusion-based image generation, and\nvision-language reasoning. Our results highlight the potential of this dataset\nto advance fire-related research and support the development of intelligent\nsafety systems. We release DetectiumFire to promote broader exploration of fire\nunderstanding in the AI community. The dataset is available at\nhttps://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DetectiumFire\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u706b\u707e\u6570\u636e\u96c6\uff0c\u5305\u542b22.5K\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c2.5K\u771f\u5b9e\u4e16\u754c\u89c6\u9891\uff0c\u65e8\u5728\u89e3\u51b3\u706b\u707e\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u706b\u707e\u76f8\u5173AI\u7814\u7a76\u7684\u53d1\u5c55\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u706b\u707e\u9886\u57df\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u516c\u5f00\u53ef\u7528\u7684\u9ad8\u8d28\u91cf\u706b\u707e\u9886\u57df\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u706b\u707e\u76f8\u5173AI\u6280\u672f\u7684\u5f00\u53d1\u548c\u5e94\u7528\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86DetectiumFire\u6570\u636e\u96c6\uff0c\u5305\u542b22.5K\u9ad8\u5206\u8fa8\u7387\u706b\u707e\u56fe\u50cf\u548c2.5K\u771f\u5b9e\u4e16\u754c\u706b\u707e\u89c6\u9891\uff0c\u8986\u76d6\u591a\u79cd\u706b\u707e\u7c7b\u578b\u3001\u73af\u5883\u548c\u98ce\u9669\u7b49\u7ea7\uff0c\u6570\u636e\u6807\u6ce8\u5305\u62ec\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u6807\u7b7e\uff08\u5982\u8fb9\u754c\u6846\uff09\u548c\u8be6\u7ec6\u6587\u672c\u63d0\u793a\u63cf\u8ff0\u573a\u666f\uff0c\u652f\u6301\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u706b\u707e\u98ce\u9669\u63a8\u7406\u7b49\u5e94\u7528\u3002", "result": "DetectiumFire\u5728\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u6570\u636e\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u6570\u636e\u5197\u4f59\u5e76\u589e\u5f3a\u4e86\u771f\u5b9e\u573a\u666f\u8986\u76d6\u5ea6\uff0c\u5728\u76ee\u6807\u68c0\u6d4b\u3001\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u751f\u6210\u548c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u7b49\u591a\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u5177\u6709\u63a8\u52a8\u706b\u707e\u76f8\u5173\u7814\u7a76\u548c\u667a\u80fd\u5b89\u5168\u7cfb\u7edf\u5f00\u53d1\u7684\u6f5c\u529b\uff0c\u5176\u516c\u5f00\u53d1\u5e03\u5c06\u4fc3\u8fdbAI\u793e\u533a\u5bf9\u706b\u707e\u7406\u89e3\u7814\u7a76\u7684\u5e7f\u6cdb\u63a2\u7d22\uff0c\u4e3a\u706b\u707e\u9884\u9632\u548c\u5e94\u6025\u54cd\u5e94\u63d0\u4f9b\u91cd\u8981\u6570\u636e\u652f\u6491\u3002"}}
{"id": "2511.02046", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02046", "abs": "https://arxiv.org/abs/2511.02046", "authors": ["Soham Joshi", "Shwet Kamal Mishra", "Viswanath Gopalakrishnan"], "title": "Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis", "comment": "First two authors contributed equally", "summary": "Creation of large-scale databases for Visual Question Answering tasks\npertaining to the text data in a scene (text-VQA) involves skilful human\nannotation, which is tedious and challenging. With the advent of foundation\nmodels that handle vision and language modalities, and with the maturity of OCR\nsystems, it is the need of the hour to establish an end-to-end pipeline that\ncan synthesize Question-Answer (QA) pairs based on scene-text from a given\nimage. We propose a pipeline for automated synthesis for text-VQA dataset that\ncan produce faithful QA pairs, and which scales up with the availability of\nscene text data. Our proposed method harnesses the capabilities of multiple\nmodels and algorithms involving OCR detection and recognition (text spotting),\nregion of interest (ROI) detection, caption generation, and question\ngeneration. These components are streamlined into a cohesive pipeline to\nautomate the synthesis and validation of QA pairs. To the best of our\nknowledge, this is the first pipeline proposed to automatically synthesize and\nvalidate a large-scale text-VQA dataset comprising around 72K QA pairs based on\naround 44K images.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u81ea\u52a8\u5316\u5408\u6210\u6587\u672c\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u901a\u8fc7\u6574\u5408OCR\u68c0\u6d4b\u3001\u533a\u57df\u8bc6\u522b\u3001\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u548c\u95ee\u9898\u751f\u6210\u7b49\u6a21\u578b\uff0c\u6210\u529f\u6784\u5efa\u4e86\u5305\u542b\u7ea672K\u95ee\u7b54\u5bf9\u7684\u5927\u89c4\u6a21\u6587\u672c-VQA\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u9762\u5411\u573a\u666f\u6587\u672c\u7684\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\uff0c\u8fc7\u7a0b\u7e41\u7410\u4e14\u5177\u6709\u6311\u6218\u6027\uff0c\u4e9f\u9700\u5efa\u7acb\u80fd\u591f\u57fa\u4e8e\u56fe\u50cf\u4e2d\u573a\u666f\u6587\u672c\u81ea\u52a8\u5408\u6210\u95ee\u7b54\u5bf9\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\u3002", "method": "\u8be5\u65b9\u6cd5\u6574\u5408\u4e86OCR\u68c0\u6d4b\u4e0e\u8bc6\u522b\u3001\u611f\u5174\u8da3\u533a\u57df\u68c0\u6d4b\u3001\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u548c\u95ee\u9898\u751f\u6210\u7b49\u591a\u4e2a\u6a21\u578b\u4e0e\u7b97\u6cd5\uff0c\u5c06\u8fd9\u4e9b\u7ec4\u4ef6\u6d41\u7ebf\u5316\u6574\u5408\u4e3a\u7edf\u4e00\u7684\u81ea\u52a8\u5316\u95ee\u7b54\u5bf9\u5408\u6210\u4e0e\u9a8c\u8bc1\u6d41\u7a0b\u3002", "result": "\u8be5\u6d41\u7a0b\u6210\u529f\u6784\u5efa\u4e86\u5305\u542b\u7ea644K\u56fe\u50cf\u548c72K\u95ee\u7b54\u5bf9\u7684\u5927\u89c4\u6a21\u6587\u672c-VQA\u6570\u636e\u96c6\uff0c\u636e\u6211\u4eec\u6240\u77e5\u8fd9\u662f\u9996\u4e2a\u80fd\u591f\u81ea\u52a8\u5408\u6210\u548c\u9a8c\u8bc1\u5927\u89c4\u6a21\u6587\u672c-VQA\u6570\u636e\u96c6\u7684\u5b8c\u6574\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5229\u7528\u57fa\u7840\u6a21\u578b\u548c\u6210\u719fOCR\u6280\u672f\u6784\u5efa\u81ea\u52a8\u5316\u6587\u672c-VQA\u6570\u636e\u96c6\u5408\u6210\u6d41\u7a0b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u6570\u636e\u96c6\u6784\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.02340", "categories": ["cs.AI", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2511.02340", "abs": "https://arxiv.org/abs/2511.02340", "authors": ["Yohan Lee", "DongGyun Kang", "SeHoon Park", "Sa-Yoon Park", "Kwangsoo Kim"], "title": "Chronic Kidney Disease Prognosis Prediction Using Transformer", "comment": "5 pages, 2 figures, 2 tables", "summary": "Chronic Kidney Disease (CKD) affects nearly 10\\% of the global population and\noften progresses to end-stage renal failure. Accurate prognosis prediction is\nvital for timely interventions and resource optimization. We present a\ntransformer-based framework for predicting CKD progression using multi-modal\nelectronic health records (EHR) from the Seoul National University Hospital\nOMOP Common Data Model. Our approach (\\textbf{ProQ-BERT}) integrates\ndemographic, clinical, and laboratory data, employing quantization-based\ntokenization for continuous lab values and attention mechanisms for\ninterpretability. The model was pretrained with masked language modeling and\nfine-tuned for binary classification tasks predicting progression from stage 3a\nto stage 5 across varying follow-up and assessment periods. Evaluated on a\ncohort of 91,816 patients, our model consistently outperformed CEHR-BERT,\nachieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.\nThese results highlight the effectiveness of transformer architectures and\ntemporal design choices in clinical prognosis modeling, offering a promising\ndirection for personalized CKD care.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6846\u67b6ProQ-BERT\uff0c\u7528\u4e8e\u9884\u6d4b\u6162\u6027\u80be\u810f\u75c5\u7684\u8fdb\u5c55\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff0c\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe0.995\u7684ROC-AUC\u6027\u80fd\u3002", "motivation": "\u6162\u6027\u80be\u810f\u75c5\u5f71\u54cd\u5168\u7403\u8fd110%\u7684\u4eba\u53e3\uff0c\u5e38\u8fdb\u5c55\u81f3\u7ec8\u672b\u671f\u80be\u8870\u7aed\uff0c\u51c6\u786e\u7684\u9884\u540e\u9884\u6d4b\u5bf9\u4e8e\u53ca\u65f6\u5e72\u9884\u548c\u8d44\u6e90\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528\u591a\u6a21\u6001\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u8fdb\u884c\u7cbe\u786e\u9884\u6d4b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u4e86ProQ-BERT\u6846\u67b6\uff0c\u57fa\u4e8eTransformer\u67b6\u6784\u6574\u5408\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u4e34\u5e8a\u548c\u5b9e\u9a8c\u5ba4\u6570\u636e\uff0c\u91c7\u7528\u57fa\u4e8e\u91cf\u5316\u7684\u6807\u8bb0\u5316\u65b9\u6cd5\u5904\u7406\u8fde\u7eed\u5b9e\u9a8c\u5ba4\u6570\u503c\uff0c\u5e76\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002\u6a21\u578b\u901a\u8fc7\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u9488\u5bf9\u4ece3a\u671f\u52305\u671f\u7684\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u572891,816\u540d\u60a3\u8005\u961f\u5217\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6a21\u578b\u6301\u7eed\u4f18\u4e8eCEHR-BERT\uff0c\u5728\u77ed\u671f\u9884\u6d4b\u4e2dROC-AUC\u9ad8\u8fbe0.995\uff0cPR-AUC\u9ad8\u8fbe0.989\uff0c\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660eTransformer\u67b6\u6784\u548c\u65f6\u95f4\u8bbe\u8ba1\u9009\u62e9\u5728\u4e34\u5e8a\u9884\u540e\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4e2a\u6027\u5316\u6162\u6027\u80be\u810f\u75c5\u62a4\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u548c\u6ce8\u610f\u529b\u673a\u5236\u5728\u533b\u7597\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.02607", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.02607", "abs": "https://arxiv.org/abs/2511.02607", "authors": ["Xu Zhang", "Danyang Li", "Xiaohang Dong", "Tianhao Wu", "Hualong Yu", "Jianye Wang", "Qicheng Li", "Xiang Li"], "title": "UniChange: Unifying Change Detection with Multimodal Large Language Model", "comment": null, "summary": "Change detection (CD) is a fundamental task for monitoring and analyzing land\ncover dynamics. While recent high performance models and high quality datasets\nhave significantly advanced the field, a critical limitation persists. Current\nmodels typically acquire limited knowledge from single-type annotated data and\ncannot concurrently leverage diverse binary change detection (BCD) and semantic\nchange detection (SCD) datasets. This constraint leads to poor generalization\nand limited versatility. The recent advancements in Multimodal Large Language\nModels (MLLMs) introduce new possibilities for a unified CD framework. We\nleverage the language priors and unification capabilities of MLLMs to develop\nUniChange, the first MLLM-based unified change detection model. UniChange\nintegrates generative language abilities with specialized CD functionalities.\nOur model successfully unifies both BCD and SCD tasks through the introduction\nof three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange\nutilizes text prompts to guide the identification of change categories,\neliminating the reliance on predefined classification heads. This design allows\nUniChange to effectively acquire knowledge from multi-source datasets, even\nwhen their class definitions conflict. Experiments on four public benchmarks\n(WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance,\nachieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively,\nsurpassing all previous methods. The code is available at\nhttps://github.com/Erxucomeon/UniChange.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UniChange\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7279\u6b8a\u4ee4\u724c\u548c\u6587\u672c\u63d0\u793a\u673a\u5236\uff0c\u6210\u529f\u7edf\u4e00\u4e86\u4e8c\u5143\u53d8\u5316\u68c0\u6d4b\u548c\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u53d8\u5316\u68c0\u6d4b\u6a21\u578b\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\uff0c\u901a\u5e38\u53ea\u80fd\u4ece\u5355\u4e00\u7c7b\u578b\u6807\u6ce8\u6570\u636e\u4e2d\u83b7\u53d6\u6709\u9650\u77e5\u8bc6\uff0c\u65e0\u6cd5\u540c\u65f6\u5229\u7528\u591a\u6837\u7684\u4e8c\u5143\u53d8\u5316\u68c0\u6d4b\u548c\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u8fd9\u5bfc\u81f4\u4e86\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u529f\u80fd\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "UniChange\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u5148\u9a8c\u548c\u7edf\u4e00\u80fd\u529b\uff0c\u5c06\u751f\u6210\u5f0f\u8bed\u8a00\u80fd\u529b\u4e0e\u4e13\u95e8\u7684\u53d8\u5316\u68c0\u6d4b\u529f\u80fd\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u5f15\u5165\u4e09\u4e2a\u7279\u6b8a\u4ee4\u724c[T1]\u3001[T2]\u548c[CHANGE]\u6765\u7edf\u4e00\u4e8c\u5143\u53d8\u5316\u68c0\u6d4b\u548c\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u4f7f\u7528\u6587\u672c\u63d0\u793a\u6765\u6307\u5bfc\u53d8\u5316\u7c7b\u522b\u7684\u8bc6\u522b\uff0c\u6d88\u9664\u4e86\u5bf9\u9884\u5b9a\u4e49\u5206\u7c7b\u5934\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u57fa\u51c6\uff08WHU-CD\u3001S2Looking\u3001LEVIR-CD+\u548cSECOND\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUniChange\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5206\u522b\u83b7\u5f97\u4e8690.41\u300153.04\u300178.87\u548c57.62\u7684IoU\u5206\u6570\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u5148\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u7edf\u4e00\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0cUniChange\u7684\u8bbe\u8ba1\u4f7f\u5176\u80fd\u591f\u6709\u6548\u4ece\u591a\u6e90\u6570\u636e\u96c6\u4e2d\u83b7\u53d6\u77e5\u8bc6\uff0c\u5373\u4f7f\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u7c7b\u522b\u5b9a\u4e49\u5b58\u5728\u51b2\u7a81\uff0c\u8fd9\u4e3a\u53d8\u5316\u68c0\u6d4b\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.02182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02182", "abs": "https://arxiv.org/abs/2511.02182", "authors": ["Jinhwan Seo", "Yoonki Cho", "Junhyug Noh", "Sung-eui Yoon"], "title": "Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models", "comment": "1st place winner of Grounded Videoqa track at the ICCV2025 Perception\n  Test", "summary": "In this technical report, we introduce a framework to address Grounded Video\nQuestion Answering (GVQA) task for the ICCV 2025 Perception Test Challenge. The\nGVQA task demands robust multimodal models capable of complex reasoning over\nvideo content, grounding the resulting answers visually, and tracking the\nreferenced objects temporally. To achieve this capability, our proposed\napproach decomposes the GVQA task into a three-stage pipeline: (1) Video\nReasoning \\& QA, (2) Spatio-temporal Grounding and (3) Tracking. Our key\ncontribution is the introduction of a trigger moment, derived from our proposed\nCORTEX prompt, which pinpoints the single most visible frame of a target object\nto serve as a robust anchor for grounding and tracking. To this end, we achieve\nthe HOTA score of 0.4968, which marks a significant improvement over the\nprevious year's winning score of 0.2704 on GVQA task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u89e3\u51b3\u63a5\u5730\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u89e6\u53d1\u65f6\u523b\u6982\u5ff5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u7a7a\u63a5\u5730\u548c\u8ddf\u8e2a\u6027\u80fd\uff0c\u5728GVQA\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86HOTA\u5206\u65700.4968\u7684\u91cd\u5927\u6539\u8fdb\u3002", "motivation": "GVQA\u4efb\u52a1\u9700\u8981\u5f3a\u5927\u7684\u591a\u6a21\u6001\u6a21\u578b\u6765\u5904\u7406\u89c6\u9891\u5185\u5bb9\u7684\u590d\u6742\u63a8\u7406\uff0c\u5c06\u7b54\u6848\u7ed3\u679c\u8fdb\u884c\u89c6\u89c9\u63a5\u5730\uff0c\u5e76\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u8ddf\u8e2a\u53c2\u8003\u5bf9\u8c61\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e9b\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06GVQA\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a\u89c6\u9891\u63a8\u7406\u4e0e\u95ee\u7b54\u3001\u65f6\u7a7a\u63a5\u5730\u548c\u8ddf\u8e2a\uff0c\u5173\u952e\u8d21\u732e\u662f\u5f15\u5165\u57fa\u4e8eCORTEX\u63d0\u793a\u7684\u89e6\u53d1\u65f6\u523b\u6982\u5ff5\uff0c\u8be5\u65f6\u523b\u786e\u5b9a\u76ee\u6807\u5bf9\u8c61\u6700\u53ef\u89c1\u7684\u5355\u4e00\u5e27\u4f5c\u4e3a\u63a5\u5730\u548c\u8ddf\u8e2a\u7684\u9c81\u68d2\u951a\u70b9\u3002", "result": "\u5728GVQA\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86HOTA\u5206\u65700.4968\uff0c\u76f8\u6bd4\u524d\u4e00\u5e74\u83b7\u80dc\u5206\u65700.2704\u6709\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u89e6\u53d1\u65f6\u523b\u7684\u5f15\u5165\uff0c\u8be5\u7814\u7a76\u4e3a\u590d\u6742\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u591a\u9636\u6bb5\u65b9\u6cd5\u548c\u7cbe\u786e\u65f6\u523b\u5b9a\u4f4d\u5728\u63d0\u5347\u89c6\u89c9\u63a5\u5730\u6027\u80fd\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.02532", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02532", "abs": "https://arxiv.org/abs/2511.02532", "authors": ["Jorge Pellejero", "Luis A. Hern\u00e1ndez G\u00f3mez", "Luis Mendo Tom\u00e1s", "Zoraida Frias Barroso"], "title": "Agentic AI for Mobile Network RAN Management and Optimization", "comment": null, "summary": "Agentic AI represents a new paradigm for automating complex systems by using\nLarge AI Models (LAMs) to provide human-level cognitive abilities with\nmultimodal perception, planning, memory, and reasoning capabilities. This will\nlead to a new generation of AI systems that autonomously decompose goals,\nretain context over time, learn continuously, operate across tools and\nenvironments, and adapt dynamically. The complexity of 5G and upcoming 6G\nnetworks renders manual optimization ineffective, pointing to Agentic AI as a\nmethod for automating decisions in dynamic RAN environments. However, despite\nits rapid advances, there is no established framework outlining the\nfoundational components and operational principles of Agentic AI systems nor a\nuniversally accepted definition.\n  This paper contributes to ongoing research on Agentic AI in 5G and 6G\nnetworks by outlining its core concepts and then proposing a practical use case\nthat applies Agentic principles to RAN optimization. We first introduce Agentic\nAI, tracing its evolution from classical agents and discussing the progress\nfrom workflows and simple AI agents to Agentic AI. Core design\npatterns-reflection, planning, tool use, and multi-agent collaboration-are then\ndescribed to illustrate how intelligent behaviors are orchestrated. These\ntheorical concepts are grounded in the context of mobile networks, with a focus\non RAN management and optimization. A practical 5G RAN case study shows how\ntime-series analytics and LAM-driven agents collaborate for KPI-based\nautonomous decision-making.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u54115G\u548c6G\u7f51\u7edc\u7684Agentic AI\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5927\u578bAI\u6a21\u578b\u4e0e\u6838\u5fc3\u8bbe\u8ba1\u6a21\u5f0f\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\u7684\u81ea\u4e3b\u4f18\u5316\u51b3\u7b56\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6846\u67b6\u7684\u7a7a\u767d\u3002", "motivation": "5G\u53ca\u672a\u67656G\u7f51\u7edc\u7684\u590d\u6742\u6027\u4f7f\u5f97\u4eba\u5de5\u4f18\u5316\u53d8\u5f97\u4f4e\u6548\uff0c\u800cAgentic AI\u867d\u7136\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u5b9a\u4e49\u548c\u7cfb\u7edf\u6027\u6846\u67b6\u6765\u6307\u5bfc\u5176\u5728\u52a8\u6001\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u81ea\u52a8\u5316\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u578bAI\u6a21\u578b\u7684Agentic AI\u7cfb\u7edf\uff0c\u91c7\u7528\u53cd\u601d\u3001\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7b49\u6838\u5fc3\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u548cKPI\u9a71\u52a8\u7684\u81ea\u4e3b\u51b3\u7b56\u673a\u5236\u6765\u534f\u8c03\u667a\u80fd\u884c\u4e3a\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u5b9e\u9645\u76845G RAN\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e0eLAM\u9a71\u52a8\u667a\u80fd\u4f53\u5982\u4f55\u534f\u4f5c\u5b9e\u73b0\u57fa\u4e8eKPI\u7684\u81ea\u4e3b\u51b3\u7b56\uff0c\u9a8c\u8bc1\u4e86Agentic AI\u5728\u7f51\u7edc\u4f18\u5316\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "Agentic AI\u4e3a5G/6G\u7f51\u7edc\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u611f\u77e5\u3001\u89c4\u5212\u3001\u8bb0\u5fc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u7f51\u7edc\u76ee\u6807\u7684\u81ea\u4e3b\u5206\u89e3\u3001\u4e0a\u4e0b\u6587\u4fdd\u6301\u548c\u52a8\u6001\u9002\u5e94\uff0c\u4e3a\u672a\u6765\u7f51\u7edc\u667a\u80fd\u5316\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.02778", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.02778", "abs": "https://arxiv.org/abs/2511.02778", "authors": ["Kevin Qinghong Lin", "Yuhao Zheng", "Hangyu Ran", "Dantong Zhu", "Dongxing Mao", "Linjie Li", "Philip Torr", "Alex Jinpeng Wang"], "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation", "comment": "Project page: https://csu-jpg.github.io/VCode Github:\n  https://github.com/CSU-JPG/VCode", "summary": "Code has emerged as a precise and executable medium for reasoning and action\nin the agent era. Yet, progress has largely focused on language-centric tasks\nsuch as program synthesis and debugging, leaving visual-centric coding\nunderexplored. Inspired by how humans reason over sketches, we advocate SVG\ncode as a compact, interpretable, and executable visual representation. We\nintroduce VCode, a benchmark that reframes multimodal understanding as code\ngeneration: given an image, a model must produce SVG that preserves symbolic\nmeaning for downstream reasoning. VCode covers three domains - general\ncommonsense (MM-Vet), professional disciplines (MMMU), and visual-centric\nperception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel\nevaluation protocol in which a policy model answers questions over rendered\nSVGs; correct answers indicate faithful symbolic preservation. Empirically,\nfrontier VLMs struggle to generate faithful SVGs, revealing a persistent gap\nbetween language-centric and visual-centric coding. To close this gap, we\nintroduce VCoder, an agentic framework that augments VLMs along two axes: (i)\nThinking with Revision, which iteratively analyzes discrepancies and refines\nSVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply\nstructured cues such as objects, shapes, and text beyond the model's intrinsic\ncapacity. Across benchmarks, frontier VLMs with strong reasoning capabilities\nscore well overall yet remain limited in professional knowledge and 3D\nreasoning. VCoder delivers a 12.3-point overall gain over the top-performing\nClaude-4-Opus. Human studies show that both humans and VLMs perform worse on\nrendered SVGs, their consistency reveals the promise of symbolic visual\nrepresentation. The benchmark and code are available at\nhttps://github.com/CSU-JPG/VCode.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVCode\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c06\u591a\u6a21\u6001\u7406\u89e3\u91cd\u65b0\u5b9a\u4e49\u4e3aSVG\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1VCoder\u4ee3\u7406\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u4fee\u8ba2\u548c\u89c6\u89c9\u5de5\u5177\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4e2d\u5fc3\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dAI\u7814\u7a76\u5728\u4ee3\u7801\u4f5c\u4e3a\u63a8\u7406\u548c\u6267\u884c\u5a92\u4ecb\u65b9\u9762\u4e3b\u8981\u5173\u6ce8\u8bed\u8a00\u4e2d\u5fc3\u4efb\u52a1\u5982\u7a0b\u5e8f\u5408\u6210\u548c\u8c03\u8bd5\uff0c\u800c\u89c6\u89c9\u4e2d\u5fc3\u7f16\u7801\u9886\u57df\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u80fd\u591f\u4fdd\u7559\u7b26\u53f7\u610f\u4e49\u8fdb\u884c\u4e0b\u6e38\u63a8\u7406\u7684\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u89c6\u89c9\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faVCode\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5c06\u591a\u6a21\u6001\u7406\u89e3\u91cd\u6784\u4e3aSVG\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1VCoder\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u4fee\u8ba2\u7684\u8fed\u4ee3\u5206\u6790\u673a\u5236\u548c\u5229\u7528\u68c0\u6d4b\u5668\u4e0e\u89e3\u6790\u5668\u63d0\u4f9b\u7ed3\u6784\u5316\u89c6\u89c9\u7ebf\u7d22\u7684\u89c6\u89c9\u5de5\u5177\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5fe0\u5b9eSVG\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0cVCoder\u6846\u67b6\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u8868\u73b0\u6700\u4f73\u7684Claude-4-Opus\u5b9e\u73b0\u4e8612.3\u5206\u7684\u603b\u4f53\u63d0\u5347\uff0c\u4eba\u7c7b\u7814\u7a76\u663e\u793a\u6e32\u67d3SVG\u5bf9\u4eba\u548c\u6a21\u578b\u90fd\u5177\u6709\u6311\u6218\u6027\u4f46\u4e00\u81f4\u6027\u63ed\u793a\u4e86\u7b26\u53f7\u89c6\u89c9\u8868\u793a\u7684\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8bed\u8a00\u4e2d\u5fc3\u4e0e\u89c6\u89c9\u4e2d\u5fc3\u7f16\u7801\u4e4b\u95f4\u7684\u6301\u7eed\u5dee\u8ddd\uff0c\u8bc1\u660e\u4e86\u7b26\u53f7\u89c6\u89c9\u8868\u793a\u5728\u63a8\u7406\u4e2d\u7684\u4ef7\u503c\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u8303\u5f0f\u548c\u6539\u8fdb\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u4e13\u4e1a\u77e5\u8bc6\u548c3D\u63a8\u7406\u65b9\u9762\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2511.02206", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02206", "abs": "https://arxiv.org/abs/2511.02206", "authors": ["Zhengjie Zhang", "Xiaoxie Mao", "Qihao Guo", "Shaoting Zhang", "Qi Huang", "Mu Zhou", "Fang Xie", "Mianxin Liu"], "title": "Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers", "comment": "31 pages, 8 figures", "summary": "Background: Alzheimer's disease (AD) diagnosis heavily relies on amyloid-beta\npositron emission tomography (Abeta-PET), which is limited by high cost and\nlimited accessibility. This study explores whether Abeta-PET spatial patterns\ncan be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We\ncollected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566\nparticipants. A language-enhanced generative model, driven by a large language\nmodel (LLM) and multimodal information fusion, was developed to synthesize PET\nimages. Synthesized images were evaluated for image quality, diagnostic\nconsistency, and clinical applicability within a fully automated diagnostic\npipeline. Findings: The synthetic PET images closely resemble real PET scans in\nboth structural details (SSIM = 0.920 +/- 0.003) and regional patterns\n(Pearson's r = 0.955 +/- 0.007). Diagnostic outcomes using synthetic PET show\nhigh agreement with real PET-based diagnoses (accuracy = 0.80). Using synthetic\nPET, we developed a fully automatic AD diagnostic pipeline integrating PET\nsynthesis and classification. The synthetic PET-based model (AUC = 0.78)\noutperforms T1-based (AUC = 0.68) and BBM-based (AUC = 0.73) models, while\ncombining synthetic PET and BBMs further improved performance (AUC = 0.79).\nAblation analysis supports the advantages of LLM integration and prompt\nengineering. Interpretation: Our language-enhanced generative model synthesizes\nrealistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial\npattern assessment and improving the diagnostic workflow for Alzheimer's\ndisease.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u7684\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u8840\u6db2\u751f\u7269\u6807\u5fd7\u7269\u548cMRI\u626b\u63cf\u4e2d\u5408\u6210\u6dc0\u7c89\u6837\u86cb\u767dPET\u56fe\u50cf\uff0c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u66f4\u4f4e\u3001\u66f4\u6613\u83b7\u53d6\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e3b\u8981\u4f9d\u8d56\u6dc0\u7c89\u6837\u86cb\u767d\u6b63\u7535\u5b50\u53d1\u5c04\u65ad\u5c42\u626b\u63cf\uff0c\u4f46\u8be5\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u4e14\u53ef\u53ca\u6027\u6709\u9650\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u662f\u5426\u80fd\u591f\u4ece\u8840\u6db2\u751f\u7269\u6807\u5fd7\u7269\u548cMRI\u626b\u63cf\u4e2d\u9884\u6d4b\u6dc0\u7c89\u6837\u86cb\u767dPET\u7684\u7a7a\u95f4\u5206\u5e03\u6a21\u5f0f\u3002", "method": "\u7814\u7a76\u6536\u96c6\u4e86566\u540d\u53c2\u4e0e\u8005\u7684\u6dc0\u7c89\u6837\u86cb\u767dPET\u56fe\u50cf\u3001T1\u52a0\u6743MRI\u626b\u63cf\u548c\u8840\u6db2\u751f\u7269\u6807\u5fd7\u7269\u6570\u636e\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u7684\u8bed\u8a00\u589e\u5f3a\u751f\u6210\u6a21\u578b\u6765\u5408\u6210PET\u56fe\u50cf\uff0c\u5e76\u6784\u5efa\u4e86\u5168\u81ea\u52a8\u8bca\u65ad\u6d41\u7a0b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5408\u6210\u7684PET\u56fe\u50cf\u5728\u7ed3\u6784\u7ec6\u8282\u4e0a\u4e0e\u771f\u5b9ePET\u626b\u63cf\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u8fbe\u52300.920\u00b10.003\uff0c\u533a\u57df\u6a21\u5f0f\u76f8\u5173\u6027\u4e3a0.955\u00b10.007\uff0c\u57fa\u4e8e\u5408\u6210PET\u7684\u8bca\u65ad\u4e0e\u771f\u5b9ePET\u8bca\u65ad\u7684\u4e00\u81f4\u6027\u51c6\u786e\u7387\u8fbe\u52300.80\uff0c\u5408\u6210PET\u6a21\u578b\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u7684AUC\u4e3a0.78\uff0c\u4f18\u4e8e\u57fa\u4e8eT1\u7684\u6a21\u578b\u548c\u57fa\u4e8e\u8840\u6db2\u751f\u7269\u6807\u5fd7\u7269\u7684\u6a21\u578b\u3002", "conclusion": "\u8bed\u8a00\u589e\u5f3a\u751f\u6210\u6a21\u578b\u80fd\u591f\u5408\u6210\u903c\u771f\u7684PET\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86MRI\u548c\u8840\u6db2\u751f\u7269\u6807\u5fd7\u7269\u5728\u6dc0\u7c89\u6837\u86cb\u767d\u7a7a\u95f4\u6a21\u5f0f\u8bc4\u4f30\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u6539\u5584\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u8bca\u65ad\u6d41\u7a0b\uff0c\u4e3a\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.02794", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.02794", "abs": "https://arxiv.org/abs/2511.02794", "authors": ["Chenyu Zhang", "Minsol Kim", "Shohreh Ghorbani", "Jingyao Wu", "Rosalind Picard", "Patricia Maes", "Paul Pu Liang"], "title": "When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning", "comment": "Accepted at the Multimodal Algorithmic Reasoning (MAR) Workshop,\n  NeurIPS 2025", "summary": "Despite rapid growth in multimodal large language models (MLLMs), their\nreasoning traces remain opaque: it is often unclear which modality drives a\nprediction, how conflicts are resolved, or when one stream dominates. In this\npaper, we introduce modality sabotage, a diagnostic failure mode in which a\nhigh-confidence unimodal error overrides other evidence and misleads the fused\nresult. To analyze such dynamics, we propose a lightweight, model-agnostic\nevaluation layer that treats each modality as an agent, producing candidate\nlabels and a brief self-assessment used for auditing. A simple fusion mechanism\naggregates these outputs, exposing contributors (modalities supporting correct\noutcomes) and saboteurs (modalities that mislead). Applying our diagnostic\nlayer in a case study on multimodal emotion recognition benchmarks with\nfoundation models revealed systematic reliability profiles, providing insight\ninto whether failures may arise from dataset artifacts or model limitations.\nMore broadly, our framework offers a diagnostic scaffold for multimodal\nreasoning, supporting principled auditing of fusion dynamics and informing\npossible interventions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6a21\u6001\u7834\u574f\u8fd9\u4e00\u8bca\u65ad\u6027\u5931\u6548\u6a21\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u8bc4\u4f30\u5c42\u6765\u63ed\u793a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5404\u6a21\u6001\u7684\u63a8\u7406\u52a8\u6001\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u6a21\u6001\u89c6\u4e3a\u667a\u80fd\u4f53\u6765\u8bc6\u522b\u8d21\u732e\u8005\u548c\u7834\u574f\u8005\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u4ecd\u7136\u4e0d\u900f\u660e\uff1a\u96be\u4ee5\u786e\u5b9a\u54ea\u4e2a\u6a21\u6001\u9a71\u52a8\u9884\u6d4b\u3001\u5982\u4f55\u89e3\u51b3\u6a21\u6001\u95f4\u51b2\u7a81\u3001\u4ee5\u53ca\u4f55\u65f6\u67d0\u4e2a\u6a21\u6001\u4e3b\u5bfc\u51b3\u7b56\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u8bca\u65ad\u80fd\u529b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u8bc4\u4f30\u5c42\uff0c\u5c06\u6bcf\u4e2a\u6a21\u6001\u89c6\u4e3a\u667a\u80fd\u4f53\uff0c\u751f\u6210\u5019\u9009\u6807\u7b7e\u548c\u7b80\u8981\u7684\u81ea\u6211\u8bc4\u4f30\u7528\u4e8e\u5ba1\u8ba1\u3002\u91c7\u7528\u7b80\u5355\u7684\u878d\u5408\u673a\u5236\u805a\u5408\u8fd9\u4e9b\u8f93\u51fa\uff0c\u4ece\u800c\u66b4\u9732\u8d21\u732e\u8005\uff08\u652f\u6301\u6b63\u786e\u7ed3\u679c\u7684\u6a21\u6001\uff09\u548c\u7834\u574f\u8005\uff08\u8bef\u5bfc\u51b3\u7b56\u7684\u6a21\u6001\uff09\u3002", "result": "\u5728\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u5e94\u7528\u8be5\u8bca\u65ad\u5c42\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u7684\u53ef\u9760\u6027\u7279\u5f81\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u5931\u8d25\u662f\u6e90\u4e8e\u6570\u636e\u96c6\u4f2a\u5f71\u8fd8\u662f\u6a21\u578b\u5c40\u9650\u6027\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u8bca\u65ad\u6027\u652f\u67b6\uff0c\u652f\u6301\u5bf9\u878d\u5408\u52a8\u6001\u7684\u539f\u5219\u6027\u5ba1\u8ba1\uff0c\u5e76\u4e3a\u53ef\u80fd\u7684\u5e72\u9884\u63aa\u65bd\u63d0\u4f9b\u4e86\u4fe1\u606f\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u51b3\u7b56\u8fc7\u7a0b\u548c\u5931\u6548\u673a\u5236\u3002"}}
{"id": "2511.02834", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02834", "abs": "https://arxiv.org/abs/2511.02834", "authors": ["Huawei Lin", "Yunzhi Shi", "Tong Geng", "Weijie Zhao", "Wei Wang", "Ravender Pal Singh"], "title": "Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything", "comment": "16 pages, 7 figures, 14 tables. Under Review", "summary": "Multimodal large language models (MLLMs) have shown strong capabilities but\nremain limited to fixed modality pairs and require costly fine-tuning with\nlarge aligned datasets. Building fully omni-capable models that can integrate\ntext, images, audio, and video remains impractical and lacks robust reasoning\nsupport. In this paper, we propose an Agent-Omni framework that coordinates\nexisting foundation models through a master-agent system, enabling flexible\nmultimodal reasoning without retraining. The master agent interprets user\nintent, delegates subtasks to modality-specific agents, and integrates their\noutputs into coherent responses. Extensive experiments across text, image,\naudio, video, and omni benchmarks show that Agent-Omni consistently achieves\nstate-of-the-art performance, particularly on tasks requiring complex\ncross-modal reasoning. Its agent-based design enables seamless integration of\nspecialized foundation models, ensuring adaptability to diverse inputs while\nmaintaining transparency and interpretability. In addition, the framework is\nmodular and easily extensible, allowing future improvements as stronger models\nbecome available. %We release an open-source implementation to support\ncontinued research on scalable and reliable omni-modal reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Agent-Omni\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u4ee3\u7406\u7cfb\u7edf\u534f\u8c03\u73b0\u6709\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u63a8\u7406\uff0c\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5c40\u9650\u4e8e\u56fa\u5b9a\u6a21\u6001\u7ec4\u5408\uff0c\u9700\u8981\u5927\u91cf\u5bf9\u9f50\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u6784\u5efa\u80fd\u591f\u96c6\u6210\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7684\u5168\u80fd\u6a21\u578b\u4ecd\u4e0d\u5b9e\u7528\u4e14\u7f3a\u4e4f\u5f3a\u5927\u7684\u63a8\u7406\u652f\u6301\u3002", "method": "\u91c7\u7528\u4e3b\u4ee3\u7406\u7cfb\u7edf\u6846\u67b6\uff0c\u4e3b\u4ee3\u7406\u89e3\u91ca\u7528\u6237\u610f\u56fe\uff0c\u5c06\u5b50\u4efb\u52a1\u59d4\u6258\u7ed9\u7279\u5b9a\u6a21\u6001\u4ee3\u7406\uff0c\u5e76\u6574\u5408\u5b83\u4eec\u7684\u8f93\u51fa\u5f62\u6210\u8fde\u8d2f\u54cd\u5e94\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u591a\u6a21\u6001\u63a8\u7406\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u548c\u5168\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAgent-Omni\u59cb\u7ec8\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u590d\u6742\u8de8\u6a21\u6001\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u57fa\u4e8e\u4ee3\u7406\u7684\u8bbe\u8ba1\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u4e13\u95e8\u7684\u57fa\u7840\u6a21\u578b\uff0c\u786e\u4fdd\u5bf9\u591a\u6837\u5316\u8f93\u5165\u7684\u9002\u5e94\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff1b\u6846\u67b6\u5177\u6709\u6a21\u5757\u5316\u548c\u6613\u6269\u5c55\u6027\uff0c\u80fd\u591f\u968f\u7740\u66f4\u5f3a\u6a21\u578b\u7684\u53ef\u7528\u6027\u8fdb\u884c\u672a\u6765\u6539\u8fdb\u3002"}}
{"id": "2511.02228", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02228", "abs": "https://arxiv.org/abs/2511.02228", "authors": ["Delin Ma", "Menghui Zhou", "Jun Qi", "Yun Yang", "Po Yang"], "title": "Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's Disease Diagnosis", "comment": null, "summary": "Alzheimer's disease (AD) is the most prevalent form of dementia, and its\nearly diagnosis is essential for slowing disease progression. Recent studies on\nmultimodal neuroimaging fusion using MRI and PET have achieved promising\nresults by integrating multi-scale complementary features. However, most\nexisting approaches primarily emphasize cross-modal complementarity while\noverlooking the diagnostic importance of modality-specific features. In\naddition, the inherent distributional differences between modalities often lead\nto biased and noisy representations, degrading classification performance. To\naddress these challenges, we propose a Collaborative Attention and\nConsistent-Guided Fusion framework for MRI and PET based AD diagnosis. The\nproposed model introduces a learnable parameter representation (LPR) block to\ncompensate for missing modality information, followed by a shared encoder and\nmodality-independent encoders to preserve both shared and specific\nrepresentations. Furthermore, a consistency-guided mechanism is employed to\nexplicitly align the latent distributions across modalities. Experimental\nresults on the ADNI dataset demonstrate that our method achieves superior\ndiagnostic performance compared with existing fusion strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u7684\u534f\u4f5c\u6ce8\u610f\u529b\u548c\u4e00\u81f4\u6027\u5f15\u5bfc\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408MRI\u548cPET\u591a\u6a21\u6001\u795e\u7ecf\u5f71\u50cf\u6570\u636e\uff0c\u5728ADNI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u878d\u5408\u65b9\u6cd5\u7684\u8bca\u65ad\u6027\u80fd\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u53ef\u5b66\u4e60\u53c2\u6570\u8868\u793a\u5757\u8865\u507f\u7f3a\u5931\u6a21\u6001\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u4e00\u81f4\u6027\u5f15\u5bfc\u673a\u5236\u663e\u5f0f\u5bf9\u9f50\u8de8\u6a21\u6001\u6f5c\u5728\u5206\u5e03\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u795e\u7ecf\u5f71\u50cf\u878d\u5408\u65b9\u6cd5\u4e3b\u8981\u5f3a\u8c03\u8de8\u6a21\u6001\u4e92\u8865\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u6a21\u6001\u7279\u5f02\u6027\u7279\u5f81\u5728\u8bca\u65ad\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e14\u6a21\u6001\u95f4\u56fa\u6709\u7684\u5206\u5e03\u5dee\u5f02\u5f80\u5f80\u5bfc\u81f4\u6709\u504f\u548c\u566a\u58f0\u8868\u793a\uff0c\u4ece\u800c\u964d\u4f4e\u5206\u7c7b\u6027\u80fd\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u5347\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u6ce8\u610f\u529b\u548c\u4e00\u81f4\u6027\u5f15\u5bfc\u878d\u5408\u6846\u67b6\uff0c\u5305\u542b\u53ef\u5b66\u4e60\u53c2\u6570\u8868\u793a\u5757\u7528\u4e8e\u8865\u507f\u7f3a\u5931\u6a21\u6001\u4fe1\u606f\uff0c\u5171\u4eab\u7f16\u7801\u5668\u548c\u6a21\u6001\u72ec\u7acb\u7f16\u7801\u5668\u4ee5\u4fdd\u7559\u5171\u4eab\u548c\u7279\u5b9a\u8868\u793a\uff0c\u5e76\u91c7\u7528\u4e00\u81f4\u6027\u5f15\u5bfc\u673a\u5236\u663e\u5f0f\u5bf9\u9f50\u8de8\u6a21\u6001\u6f5c\u5728\u5206\u5e03\u3002\u8be5\u6a21\u578b\u6709\u6548\u6574\u5408\u4e86MRI\u548cPET\u6570\u636e\u7684\u591a\u5c3a\u5ea6\u4e92\u8865\u7279\u5f81\u3002", "result": "\u5728ADNI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u878d\u5408\u7b56\u7565\u5b9e\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u8bca\u65ad\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002\u6a21\u578b\u901a\u8fc7\u540c\u65f6\u8003\u8651\u8de8\u6a21\u6001\u4e92\u8865\u6027\u548c\u6a21\u6001\u7279\u5f02\u6027\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u540c\u65f6\u4fdd\u7559\u5171\u4eab\u548c\u7279\u5b9a\u6a21\u6001\u8868\u793a\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u663e\u5f0f\u5bf9\u9f50\u8de8\u6a21\u6001\u5206\u5e03\u5bf9\u63d0\u5347\u8bca\u65ad\u6027\u80fd\u7684\u5173\u952e\u4f5c\u7528\u3002\u6240\u63d0\u51fa\u7684\u878d\u5408\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u795e\u7ecf\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5bf9\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u8bca\u65ad\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u610f\u4e49\u3002"}}
{"id": "2511.02271", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02271", "abs": "https://arxiv.org/abs/2511.02271", "authors": ["Yucheng Song", "Yifan Ge", "Junhao Li", "Zhining Liao", "Zhifang Liao"], "title": "Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework", "comment": null, "summary": "Medical Report Generation (MRG) is a key part of modern medical diagnostics,\nas it automatically generates reports from radiological images to reduce\nradiologists' burden. However, reliable MRG models for lesion description face\nthree main challenges: insufficient domain knowledge understanding, poor\ntext-visual entity embedding alignment, and spurious correlations from\ncross-modal biases. Previous work only addresses single challenges, while this\npaper tackles all three via a novel hierarchical task decomposition approach,\nproposing the HTSC-CIF framework. HTSC-CIF classifies the three challenges into\nlow-, mid-, and high-level tasks: 1) Low-level: align medical entity features\nwith spatial locations to enhance domain knowledge for visual encoders; 2)\nMid-level: use Prefix Language Modeling (text) and Masked Image Modeling\n(images) to boost cross-modal alignment via mutual guidance; 3) High-level: a\ncross-modal causal intervention module (via front-door intervention) to reduce\nconfounders and improve interpretability. Extensive experiments confirm\nHTSC-CIF's effectiveness, significantly outperforming state-of-the-art (SOTA)\nMRG methods. Code will be made public upon paper acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHTSC-CIF\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u4efb\u52a1\u5206\u89e3\u65b9\u6cd5\u89e3\u51b3\u533b\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u7684\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u9886\u57df\u77e5\u8bc6\u7406\u89e3\u4e0d\u8db3\u3001\u6587\u672c-\u89c6\u89c9\u5b9e\u4f53\u5d4c\u5165\u5bf9\u9f50\u4e0d\u4f73\u4ee5\u53ca\u8de8\u6a21\u6001\u504f\u5dee\u5bfc\u81f4\u7684\u4f2a\u76f8\u5173\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u62a5\u544a\u751f\u6210\u6a21\u578b\u5728\u75c5\u7076\u63cf\u8ff0\u65b9\u9762\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u9886\u57df\u77e5\u8bc6\u7406\u89e3\u4e0d\u8db3\u3001\u6587\u672c-\u89c6\u89c9\u5b9e\u4f53\u5d4c\u5165\u5bf9\u9f50\u4e0d\u4f73\u4ee5\u53ca\u8de8\u6a21\u6001\u504f\u5dee\u5bfc\u81f4\u7684\u4f2a\u76f8\u5173\u6027\uff0c\u73b0\u6709\u5de5\u4f5c\u4ec5\u80fd\u89e3\u51b3\u5355\u4e2a\u6311\u6218\u800c\u65e0\u6cd5\u5168\u9762\u5e94\u5bf9\u6240\u6709\u95ee\u9898\u3002", "method": "\u63d0\u51faHTSC-CIF\u6846\u67b6\u91c7\u7528\u5206\u5c42\u4efb\u52a1\u5206\u89e3\u65b9\u6cd5\uff1a\u4f4e\u5c42\u4efb\u52a1\u5c06\u533b\u5b66\u5b9e\u4f53\u7279\u5f81\u4e0e\u7a7a\u95f4\u4f4d\u7f6e\u5bf9\u9f50\u4ee5\u589e\u5f3a\u89c6\u89c9\u7f16\u7801\u5668\u7684\u9886\u57df\u77e5\u8bc6\uff1b\u4e2d\u5c42\u4efb\u52a1\u4f7f\u7528\u524d\u7f00\u8bed\u8a00\u5efa\u6a21\u548c\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u901a\u8fc7\u76f8\u4e92\u6307\u5bfc\u63d0\u5347\u8de8\u6a21\u6001\u5bf9\u9f50\uff1b\u9ad8\u5c42\u4efb\u52a1\u901a\u8fc7\u524d\u95e8\u5e72\u9884\u7684\u8de8\u6a21\u6001\u56e0\u679c\u5e72\u9884\u6a21\u5757\u51cf\u5c11\u6df7\u6742\u56e0\u7d20\u5e76\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86HTSC-CIF\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5728\u533b\u5b66\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5206\u5c42\u4efb\u52a1\u5206\u89e3\u65b9\u6cd5\u5728\u89e3\u51b3\u533b\u5b66\u62a5\u544a\u751f\u6210\u591a\u6311\u6218\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8de8\u6a21\u6001\u533b\u5b66AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6846\u67b6\u8bbe\u8ba1\u601d\u8def\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.02384", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02384", "abs": "https://arxiv.org/abs/2511.02384", "authors": ["Jiahe Song", "Chuang Wang", "Bowen Jiang", "Yinfan Wang", "Hao Zheng", "Xingjian Wei", "Chengjin Liu", "Junyuan Gao", "Yubin Wang", "Lijun Wu", "Jiang Wu", "Qian Yu", "Conghui He"], "title": "RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning", "comment": null, "summary": "Large-scale chemical reaction datasets are crucial for AI research in\nchemistry. However, existing chemical reaction data often exist as images\nwithin papers, making them not machine-readable and unusable for training\nmachine learning models. In response to this challenge, we propose the\nRxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP).\nOur framework reformulates the traditional coordinate prediction driven parsing\nprocess into an image captioning problem, which Large Vision-Language Models\n(LVLMs) handle naturally. We introduce a strategy termed \"BBox and Index as\nVisual Prompt\" (BIVP), which uses our state-of-the-art molecular detector,\nMolYOLO, to pre-draw molecular bounding boxes and indices directly onto the\ninput image. This turns the downstream parsing into a natural-language\ndescription problem. Extensive experiments show that the BIVP strategy\nsignificantly improves structural extraction quality while simplifying model\ndesign. We further construct the RxnCaption-11k dataset, an order of magnitude\nlarger than prior real-world literature benchmarks, with a balanced test subset\nacross four layout archetypes. Experiments demonstrate that RxnCaption-VL\nachieves state-of-the-art performance on multiple metrics. We believe our\nmethod, dataset, and models will advance structured information extraction from\nchemical literature and catalyze broader AI applications in chemistry. We will\nrelease data, models, and code on GitHub.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86RxnCaption\u6846\u67b6\uff0c\u5c06\u5316\u5b66\u53cd\u5e94\u56fe\u89e3\u6790\u91cd\u65b0\u6784\u5efa\u4e3a\u56fe\u50cf\u63cf\u8ff0\u95ee\u9898\uff0c\u901a\u8fc7BIVP\u7b56\u7565\u548cMolYOLO\u5206\u5b50\u68c0\u6d4b\u5668\u663e\u8457\u63d0\u5347\u4e86\u7ed3\u6784\u63d0\u53d6\u8d28\u91cf\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21RxnCaption-11k\u6570\u636e\u96c6\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5316\u5b66\u53cd\u5e94\u6570\u636e\u901a\u5e38\u4ee5\u8bba\u6587\u4e2d\u7684\u56fe\u50cf\u5f62\u5f0f\u5b58\u5728\uff0c\u8fd9\u4e9b\u6570\u636e\u65e0\u6cd5\u88ab\u673a\u5668\u8bfb\u53d6\uff0c\u4e5f\u65e0\u6cd5\u7528\u4e8e\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86AI\u5728\u5316\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86RxnCaption\u6846\u67b6\uff0c\u5c06\u4f20\u7edf\u7684\u5750\u6807\u9884\u6d4b\u9a71\u52a8\u89e3\u6790\u8fc7\u7a0b\u91cd\u65b0\u6784\u5efa\u4e3a\u56fe\u50cf\u63cf\u8ff0\u95ee\u9898\uff0c\u91c7\u7528BIVP\u7b56\u7565\uff0c\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u5206\u5b50\u68c0\u6d4b\u5668MolYOLO\u5728\u8f93\u5165\u56fe\u50cf\u4e0a\u9884\u7ed8\u5236\u5206\u5b50\u8fb9\u754c\u6846\u548c\u7d22\u5f15\uff0c\u5c06\u4e0b\u6e38\u89e3\u6790\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u95ee\u9898\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660eBIVP\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u7ed3\u6784\u63d0\u53d6\u8d28\u91cf\u5e76\u7b80\u5316\u4e86\u6a21\u578b\u8bbe\u8ba1\uff0c\u6784\u5efa\u7684RxnCaption-11k\u6570\u636e\u96c6\u6bd4\u5148\u524d\u771f\u5b9e\u4e16\u754c\u6587\u732e\u57fa\u51c6\u5927\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0cRxnCaption-VL\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u5c06\u63a8\u52a8\u4ece\u5316\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u8fdb\u5c55\uff0c\u5e76\u50ac\u5316\u66f4\u5e7f\u6cdb\u7684AI\u5728\u5316\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u6570\u636e\u3001\u6a21\u578b\u548c\u4ee3\u7801\u5c06\u5728GitHub\u4e0a\u53d1\u5e03\u3002"}}
{"id": "2511.02415", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02415", "abs": "https://arxiv.org/abs/2511.02415", "authors": ["Duo Xu", "Hao Cheng", "Xin Lin", "Zhen Xie", "Hao Wang"], "title": "ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension", "comment": "23 pages, EMNLP25 Accepted", "summary": "Complex chart understanding tasks demand advanced visual recognition and\nreasoning capabilities from multimodal large language models (MLLMs). However,\ncurrent research provides limited coverage of complex chart scenarios and\ncomputation-intensive reasoning tasks prevalent in real-world applications.\nThis study proposes an automated multi-stage code-driven pipeline for\nsystematically generating visual reasoning datasets to address these\nlimitations. The pipeline integrates retrieval-augmented generation (RAG) to\nretrieve professional chart templates and employs chain-of-thought (CoT)\nstrategies to generate reasoning codes that simulate real data distributions,\nthereby driving chart rendering and question-related statistical computations.\nThrough model-based evaluation, the pipeline enhances chart diversity and data\nquality. Using this framework, we construct ChartM$^3$, a multi-dimensional and\nmulti-step dataset containing 38K charts and 142K Q&A pairs for training, along\nwith 2,871 high-quality evaluation samples for enabling practical performance\nassessment. Supervised fine-tuning (SFT) and reinforcement learning (RL)\nexperiments demonstrate that our dataset significantly improves reasoning\ncapabilities and cross-domain generalization performance, enabling smaller\nmodels to achieve performance comparable to larger-scale models in complex\nchart comprehension.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u591a\u9636\u6bb5\u4ee3\u7801\u9a71\u52a8\u7ba1\u9053\uff0c\u7528\u4e8e\u7cfb\u7edf\u751f\u6210\u590d\u6742\u56fe\u8868\u7406\u89e3\u6570\u636e\u96c6ChartM\u00b3\uff0c\u8be5\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4f7f\u8f83\u5c0f\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u4e0e\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u76f8\u5f53\u7684\u590d\u6742\u56fe\u8868\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u8986\u76d6\u573a\u666f\u6709\u9650\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u63a8\u7406\u4efb\u52a1\u652f\u6301\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u5bf9\u9ad8\u7ea7\u89c6\u89c9\u8bc6\u522b\u548c\u63a8\u7406\u80fd\u529b\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u5316\u591a\u9636\u6bb5\u4ee3\u7801\u9a71\u52a8\u7ba1\u9053\uff0c\u96c6\u6210\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u83b7\u53d6\u4e13\u4e1a\u56fe\u8868\u6a21\u677f\uff0c\u5e76\u5229\u7528\u601d\u7ef4\u94fe\u7b56\u7565\u751f\u6210\u6a21\u62df\u771f\u5b9e\u6570\u636e\u5206\u5e03\u7684\u63a8\u7406\u4ee3\u7801\uff0c\u9a71\u52a8\u56fe\u8868\u6e32\u67d3\u548c\u95ee\u9898\u76f8\u5173\u7edf\u8ba1\u8ba1\u7b97\uff0c\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u7684\u8bc4\u4f30\u63d0\u5347\u56fe\u8868\u591a\u6837\u6027\u548c\u6570\u636e\u8d28\u91cf\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b38K\u56fe\u8868\u548c142K\u95ee\u7b54\u5bf9\u7684ChartM\u00b3\u6570\u636e\u96c6\uff0c\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u8868\u660e\u8be5\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u548c\u8de8\u9886\u57df\u6cdb\u5316\u6027\u80fd\uff0c\u4f7f\u8f83\u5c0f\u6a21\u578b\u5728\u590d\u6742\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u80fd\u591f\u8fbe\u5230\u4e0e\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u76f8\u5f53\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u7ba1\u9053\u5728\u6784\u5efa\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u96c6\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u6570\u636e\u8d28\u91cf\u5bf9\u6a21\u578b\u80fd\u529b\u63d0\u5347\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2511.02565", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02565", "abs": "https://arxiv.org/abs/2511.02565", "authors": ["Jingyu Lu", "Haonan Wang", "Qixiang Zhang", "Xiaomeng Li"], "title": "A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding", "comment": "9 pages main text with 6 figures (excluding references),\n  supplementary material included", "summary": "Subject-agnostic brain decoding, which aims to reconstruct continuous visual\nexperiences from fMRI without subject-specific training, holds great potential\nfor clinical applications. However, this direction remains underexplored due to\nchallenges in cross-subject generalization and the complex nature of brain\nsignals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a\nnovel hierarchical decoding framework that explicitly models the ventral-dorsal\narchitecture of the human visual system to learn multi-dimensional\nrepresentations. By disentangling and leveraging features from early visual\ncortex, ventral, and dorsal streams, VCFlow captures diverse and complementary\ncognitive information essential for visual reconstruction. Furthermore, we\nintroduce a feature-level contrastive learning strategy to enhance the\nextraction of subject-invariant semantic representations, thereby enhancing\nsubject-agnostic applicability to previously unseen subjects. Unlike\nconventional pipelines that need more than 12 hours of per-subject data and\nheavy computation, VCFlow sacrifices only 7\\% accuracy on average yet generates\neach reconstructed video in 10 seconds without any retraining, offering a fast\nand clinically scalable solution. The source code will be released upon\nacceptance of the paper.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVCFlow\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u5c42\u6b21\u5316\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u8179\u4fa7-\u80cc\u4fa7\u67b6\u6784\u6765\u5b66\u4e60\u591a\u7ef4\u8868\u5f81\uff0c\u5b9e\u73b0\u65e0\u9700\u7279\u5b9a\u53d7\u8bd5\u8005\u8bad\u7ec3\u7684\u8fde\u7eed\u89c6\u89c9\u4f53\u9a8c\u91cd\u5efa\uff0c\u5728\u4ec5\u727a\u72727%\u51c6\u786e\u7387\u7684\u540c\u65f6\u5c06\u91cd\u5efa\u901f\u5ea6\u63d0\u5347\u81f310\u79d2\u6bcf\u89c6\u9891\u3002", "motivation": "\u4e3b\u9898\u65e0\u5173\u7684\u8111\u89e3\u7801\u65e8\u5728\u65e0\u9700\u7279\u5b9a\u53d7\u8bd5\u8005\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4ecefMRI\u91cd\u5efa\u8fde\u7eed\u89c6\u89c9\u4f53\u9a8c\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u8de8\u53d7\u8bd5\u8005\u6cdb\u5316\u6311\u6218\u548c\u8111\u4fe1\u53f7\u7684\u590d\u6742\u6027\uff0c\u8be5\u65b9\u5411\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u76ae\u5c42\u6d41\u67b6\u6784VCFlow\uff0c\u901a\u8fc7\u89e3\u8026\u548c\u5229\u7528\u65e9\u671f\u89c6\u89c9\u76ae\u5c42\u3001\u8179\u4fa7\u6d41\u548c\u80cc\u4fa7\u6d41\u7684\u7279\u5f81\u6765\u5b66\u4e60\u591a\u7ef4\u8868\u5f81\uff0c\u5e76\u5f15\u5165\u7279\u5f81\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u589e\u5f3a\u4e3b\u9898\u4e0d\u53d8\u8bed\u4e49\u8868\u5f81\u7684\u63d0\u53d6\u3002", "result": "\u4e0e\u4f20\u7edf\u9700\u8981\u8d85\u8fc712\u5c0f\u65f6\u6bcf\u53d7\u8bd5\u8005\u6570\u636e\u548c\u5927\u91cf\u8ba1\u7b97\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cVCFlow\u5e73\u5747\u4ec5\u727a\u72727%\u51c6\u786e\u7387\uff0c\u5374\u80fd\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4ee5\u6bcf\u89c6\u989110\u79d2\u7684\u901f\u5ea6\u751f\u6210\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "VCFlow\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u4e14\u4e34\u5e8a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u89c6\u89c9\u7cfb\u7edf\u5c42\u6b21\u7ed3\u6784\u6210\u529f\u5b9e\u73b0\u4e86\u8de8\u53d7\u8bd5\u8005\u7684\u8111\u4fe1\u53f7\u89e3\u7801\uff0c\u4e3a\u4e34\u5e8a\u8111\u673a\u63a5\u53e3\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.02427", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02427", "abs": "https://arxiv.org/abs/2511.02427", "authors": ["Nicolas Schuler", "Lea Dewald", "Nick Baldig", "J\u00fcrgen Graf"], "title": "From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics", "comment": "15 pages, 6 figures, 1 table; accepted for AI-2025 Forty-fifth SGAI\n  International Conference on Artificial Intelligence CAMBRIDGE, ENGLAND 16-18\n  DECEMBER 2025", "summary": "Video Understanding, Scene Interpretation and Commonsense Reasoning are\nhighly challenging tasks enabling the interpretation of visual information,\nallowing agents to perceive, interact with and make rational decisions in its\nenvironment. Large Language Models (LLMs) and Visual Language Models (VLMs)\nhave shown remarkable advancements in these areas in recent years, enabling\ndomain-specific applications as well as zero-shot open vocabulary tasks,\ncombining multiple domains. However, the required computational complexity\nposes challenges for their application on edge devices and in the context of\nMobile Robotics, especially considering the trade-off between accuracy and\ninference time. In this paper, we investigate the capabilities of\nstate-of-the-art VLMs for the task of Scene Interpretation and Action\nRecognition, with special regard to small VLMs capable of being deployed to\nedge devices in the context of Mobile Robotics. The proposed pipeline is\nevaluated on a diverse dataset consisting of various real-world cityscape,\non-campus and indoor scenarios. The experimental evaluation discusses the\npotential of these small models on edge devices, with particular emphasis on\nchallenges, weaknesses, inherent model biases and the application of the gained\ninformation. Supplementary material is provided via the following repository:\nhttps://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u7684\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u573a\u666f\u7406\u89e3\u548c\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u79fb\u52a8\u673a\u5668\u4eba\u5e94\u7528\u573a\u666f\u4e0b\u7684\u51c6\u786e\u6027\u4e0e\u63a8\u7406\u65f6\u95f4\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u3001\u573a\u666f\u89e3\u91ca\u548c\u5e38\u8bc6\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u8ba1\u7b97\u590d\u6742\u6027\u5bf9\u8fb9\u7f18\u8bbe\u5907\u548c\u79fb\u52a8\u673a\u5668\u4eba\u5e94\u7528\u6784\u6210\u4e86\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u51c6\u786e\u6027\u4e0e\u63a8\u7406\u65f6\u95f4\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u4e0a\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4e13\u95e8\u7814\u7a76\u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u573a\u666f\u89e3\u91ca\u548c\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u5728\u5305\u542b\u771f\u5b9e\u4e16\u754c\u57ce\u5e02\u666f\u89c2\u3001\u6821\u56ed\u548c\u5ba4\u5185\u573a\u666f\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u6df1\u5165\u63a2\u8ba8\u4e86\u5c0f\u578b\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6f5c\u529b\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u8fd9\u4e9b\u6a21\u578b\u9762\u4e34\u7684\u6311\u6218\u3001\u5f31\u70b9\u3001\u56fa\u6709\u504f\u89c1\u4ee5\u53ca\u6240\u83b7\u4fe1\u606f\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u5c0f\u578b\u6a21\u578b\u5728\u79fb\u52a8\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u53ef\u884c\u6027\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u667a\u80fd\u611f\u77e5\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2511.02580", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02580", "abs": "https://arxiv.org/abs/2511.02580", "authors": ["Daichi Nagai", "Ryugo Morita", "Shunsuke Kitada", "Hitoshi Iyatomi"], "title": "TAUE: Training-free Noise Transplant and Cultivation Diffusion Model", "comment": "13 pages, 8 figures, 3 tables. The first two authors contributed\n  equally. Project Page: https://iyatomilab.github.io/TAUE", "summary": "Despite the remarkable success of text-to-image diffusion models, their\noutput of a single, flattened image remains a critical bottleneck for\nprofessional applications requiring layer-wise control. Existing solutions\neither rely on fine-tuning with large, inaccessible datasets or are\ntraining-free yet limited to generating isolated foreground elements, failing\nto produce a complete and coherent scene. To address this, we introduce the\nTraining-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a\nnovel framework for zero-shot, layer-wise image generation. Our core technique,\nNoise Transplantation and Cultivation (NTC), extracts intermediate latent\nrepresentations from both foreground and composite generation processes,\ntransplanting them into the initial noise for subsequent layers. This ensures\nsemantic and structural coherence across foreground, background, and composite\nlayers, enabling consistent, multi-layered outputs without requiring\nfine-tuning or auxiliary datasets. Extensive experiments show that our\ntraining-free method achieves performance comparable to fine-tuned methods,\nenhancing layer-wise consistency while maintaining high image quality and\nfidelity. TAUE not only eliminates costly training and dataset requirements but\nalso unlocks novel downstream applications, such as complex compositional\nediting, paving the way for more accessible and controllable generative\nworkflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TAUE\u6846\u67b6\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5c42\u5f0f\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u79fb\u690d\u4e0e\u57f9\u80b2\u6280\u672f\u5b9e\u73b0\u524d\u666f\u3001\u80cc\u666f\u548c\u5408\u6210\u5c42\u7684\u8bed\u4e49\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5c42\u5f0f\u63a7\u5236\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4ec5\u80fd\u751f\u6210\u5355\u4e00\u5e73\u9762\u56fe\u50cf\uff0c\u8fd9\u6210\u4e3a\u4e13\u4e1a\u5e94\u7528\u4e2d\u9700\u8981\u5c42\u5f0f\u63a7\u5236\u7684\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u4f9d\u8d56\u5927\u89c4\u6a21\u4e0d\u53ef\u5f97\u6570\u636e\u96c6\u7684\u5fae\u8c03\uff0c\u8981\u4e48\u662f\u65e0\u9700\u8bad\u7ec3\u4f46\u4ec5\u9650\u4e8e\u751f\u6210\u5b64\u7acb\u524d\u666f\u5143\u7d20\uff0c\u65e0\u6cd5\u4ea7\u751f\u5b8c\u6574\u8fde\u8d2f\u7684\u573a\u666f\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u566a\u58f0\u79fb\u690d\u4e0e\u57f9\u80b2\u6280\u672f\uff0c\u4ece\u524d\u666f\u548c\u5408\u6210\u751f\u6210\u8fc7\u7a0b\u4e2d\u63d0\u53d6\u4e2d\u95f4\u6f5c\u5728\u8868\u793a\uff0c\u5c06\u5176\u79fb\u690d\u5230\u540e\u7eed\u5c42\u7684\u521d\u59cb\u566a\u58f0\u4e2d\u3002\u8fd9\u79cd\u65b9\u6cd5\u786e\u4fdd\u4e86\u524d\u666f\u3001\u80cc\u666f\u548c\u5408\u6210\u5c42\u4e4b\u95f4\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u8f85\u52a9\u6570\u636e\u96c6\u5373\u53ef\u5b9e\u73b0\u4e00\u81f4\u7684\u5c42\u5f0f\u8f93\u51fa\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65e0\u9700\u8bad\u7ec3\u65b9\u6cd5\u8fbe\u5230\u4e86\u4e0e\u5fae\u8c03\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u9ad8\u56fe\u50cf\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u5c42\u5f0f\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u6602\u8d35\u7684\u8bad\u7ec3\u548c\u6570\u636e\u96c6\u9700\u6c42\uff0c\u540c\u65f6\u89e3\u9501\u4e86\u590d\u6742\u7ec4\u5408\u7f16\u8f91\u7b49\u65b0\u9896\u4e0b\u6e38\u5e94\u7528\u3002", "conclusion": "TAUE\u4e0d\u4ec5\u6d88\u9664\u4e86\u6210\u672c\u9ad8\u6602\u7684\u8bad\u7ec3\u548c\u6570\u636e\u96c6\u9700\u6c42\uff0c\u8fd8\u4e3a\u66f4\u6613\u8bbf\u95ee\u548c\u53ef\u63a7\u7684\u751f\u6210\u5de5\u4f5c\u6d41\u7a0b\u5f00\u8f9f\u4e86\u9053\u8def\u3002\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u4e13\u4e1a\u7ea7\u5c42\u5f0f\u56fe\u50cf\u751f\u6210\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u751f\u6210\u5f0fAI\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.02720", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02720", "abs": "https://arxiv.org/abs/2511.02720", "authors": ["Vojt\u011bch K\u016fr", "Adam Bajger", "Adam Kuku\u010dka", "Marek Hradil", "V\u00edt Musil", "Tom\u00e1\u0161 Br\u00e1zdil"], "title": "LLEXICORP: End-user Explainability of Convolutional Neural Networks", "comment": null, "summary": "Convolutional neural networks (CNNs) underpin many modern computer vision\nsystems. With applications ranging from common to critical areas, a need to\nexplain and understand the model and its decisions (XAI) emerged. Prior works\nsuggest that in the top layers of CNNs, the individual channels can be\nattributed to classifying human-understandable concepts. Concept relevance\npropagation (CRP) methods can backtrack predictions to these channels and find\nimages that most activate these channels. However, current CRP workflows are\nlargely manual: experts must inspect activation images to name the discovered\nconcepts and must synthesize verbose explanations from relevance maps, limiting\nthe accessibility of the explanations and their scalability.\n  To address these issues, we introduce Large Language model EXplaIns COncept\nRelevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a\nmultimodal large language model. Our approach automatically assigns descriptive\nnames to concept prototypes and generates natural-language explanations that\ntranslate quantitative relevance distributions into intuitive narratives. To\nensure faithfulness, we craft prompts that teach the language model the\nsemantics of CRP through examples and enforce a separation between naming and\nexplanation tasks. The resulting text can be tailored to different audiences,\noffering low-level technical descriptions for experts and high-level summaries\nfor non-technical stakeholders.\n  We qualitatively evaluate our method on various images from ImageNet on a\nVGG16 model. Our findings suggest that integrating concept-based attribution\nmethods with large language models can significantly lower the barrier to\ninterpreting deep neural networks, paving the way for more transparent AI\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLLEXICORP\uff0c\u4e00\u79cd\u5c06\u6982\u5ff5\u76f8\u5173\u6027\u4f20\u64ad\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u6a21\u5757\u5316\u7ba1\u9053\uff0c\u80fd\u591f\u81ea\u52a8\u4e3a\u6982\u5ff5\u539f\u578b\u5206\u914d\u63cf\u8ff0\u6027\u540d\u79f0\u5e76\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u89e3\u91ca\u7684\u95e8\u69db\u3002", "motivation": "\u5f53\u524d\u6982\u5ff5\u76f8\u5173\u6027\u4f20\u64ad\u5de5\u4f5c\u6d41\u7a0b\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\uff0c\u4e13\u5bb6\u9700\u8981\u68c0\u67e5\u6fc0\u6d3b\u56fe\u50cf\u6765\u547d\u540d\u53d1\u73b0\u7684\u6982\u5ff5\uff0c\u5e76\u4ece\u76f8\u5173\u6027\u56fe\u4e2d\u5408\u6210\u5197\u957f\u7684\u89e3\u91ca\uff0c\u8fd9\u9650\u5236\u4e86\u89e3\u91ca\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u6982\u5ff5\u76f8\u5173\u6027\u4f20\u64ad\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8026\u5408\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\u6559\u7ed9\u8bed\u8a00\u6a21\u578bCRP\u7684\u8bed\u4e49\uff0c\u5e76\u5f3a\u5236\u5206\u79bb\u547d\u540d\u548c\u89e3\u91ca\u4efb\u52a1\uff0c\u751f\u6210\u7684\u6587\u672c\u53ef\u4ee5\u6839\u636e\u4e0d\u540c\u53d7\u4f17\u8fdb\u884c\u5b9a\u5236\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u7684VGG16\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9a\u6027\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u751f\u6210\u63cf\u8ff0\u6027\u6982\u5ff5\u540d\u79f0\u548c\u76f4\u89c2\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u5c06\u5b9a\u91cf\u76f8\u5173\u6027\u5206\u5e03\u8f6c\u5316\u4e3a\u6613\u4e8e\u7406\u89e3\u7684\u53d9\u8ff0\u3002", "conclusion": "\u5c06\u57fa\u4e8e\u6982\u5ff5\u7684\u53ef\u5f52\u56e0\u65b9\u6cd5\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u89e3\u91ca\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u95e8\u69db\uff0c\u4e3a\u66f4\u900f\u660e\u7684AI\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\uff0c\u751f\u6210\u7684\u89e3\u91ca\u53ef\u4ee5\u6839\u636e\u6280\u672f\u80cc\u666f\u4e3a\u4e0d\u540c\u53d7\u4f17\u63d0\u4f9b\u9002\u5f53\u8be6\u7ec6\u7a0b\u5ea6\u7684\u4fe1\u606f\u3002"}}
{"id": "2511.02503", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02503", "abs": "https://arxiv.org/abs/2511.02503", "authors": ["Robinson Umeike", "Neil Getty", "Yin Xiangyu", "Yi Jiang"], "title": "Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes", "comment": null, "summary": "The automation of workflows in advanced microscopy is a key goal where\nfoundation models like Language Models (LLMs) and Vision-Language Models (VLMs)\nshow great potential. However, adapting these general-purpose models for\nspecialized scientific tasks is critical, and the optimal domain adaptation\nstrategy is often unclear. To address this, we introduce PtychoBench, a new\nmulti-modal, multi-task benchmark for ptychographic analysis. Using this\nbenchmark, we systematically compare two specialization strategies: Supervised\nFine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies\non a visual artifact detection task with VLMs and a textual parameter\nrecommendation task with LLMs in a data-scarce regime. Our findings reveal that\nthe optimal specialization pathway is task-dependent. For the visual task, SFT\nand ICL are highly complementary, with a fine-tuned model guided by\ncontext-aware examples achieving the highest mean performance (Micro-F1 of\n0.728). Conversely, for the textual task, ICL on a large base model is the\nsuperior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a\npowerful \"super-expert\" SFT model (0-shot Micro-F1 of 0.839). We also confirm\nthe superiority of context-aware prompting and identify a consistent contextual\ninterference phenomenon in fine-tuned models. These results, benchmarked\nagainst strong baselines including GPT-4o and a DINOv3-based classifier, offer\nkey observations for AI in science: the optimal specialization path in our\nbenchmark is dependent on the task modality, offering a clear framework for\ndeveloping more effective science-based agentic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PtychoBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u4e24\u79cd\u9886\u57df\u9002\u5e94\u7b56\u7565\u5728\u79d1\u5b66\u663e\u5fae\u955c\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u6700\u4f18\u7b56\u7565\u53d6\u51b3\u4e8e\u4efb\u52a1\u6a21\u6001\uff1a\u89c6\u89c9\u4efb\u52a1\u4e2dSFT\u4e0eICL\u4e92\u8865\uff0c\u6587\u672c\u4efb\u52a1\u4e2dICL\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5148\u8fdb\u663e\u5fae\u955c\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u662f\u91cd\u8981\u76ee\u6807\uff0c\u57fa\u7840\u6a21\u578b\u5982LLMs\u548cVLMs\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5c06\u8fd9\u4e9b\u901a\u7528\u6a21\u578b\u9002\u5e94\u4e13\u4e1a\u79d1\u5b66\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u800c\u6700\u4f18\u9886\u57df\u9002\u5e94\u7b56\u7565\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5f15\u5165PtychoBench\u591a\u6a21\u6001\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u6bd4\u8f83\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u4e24\u79cd\u4e13\u4e1a\u5316\u7b56\u7565\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u8bc4\u4f30VLMs\u7684\u89c6\u89c9\u4f2a\u5f71\u68c0\u6d4b\u4efb\u52a1\u548cLLMs\u7684\u6587\u672c\u53c2\u6570\u63a8\u8350\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6700\u4f18\u4e13\u4e1a\u5316\u8def\u5f84\u5177\u6709\u4efb\u52a1\u4f9d\u8d56\u6027\uff1a\u89c6\u89c9\u4efb\u52a1\u4e2dSFT\u548cICL\u9ad8\u5ea6\u4e92\u8865\uff0c\u5fae\u8c03\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u611f\u77e5\u793a\u4f8b\u6307\u5bfc\u4e0b\u8fbe\u5230\u6700\u9ad8\u6027\u80fd\uff08Micro-F1 0.728\uff09\uff1b\u6587\u672c\u4efb\u52a1\u4e2d\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684ICL\u662f\u66f4\u4f18\u7b56\u7565\uff0c\u8fbe\u5230\u5cf0\u503cMicro-F1 0.847\uff0c\u4f18\u4e8e\u5f3a\u5927\u7684\"\u8d85\u7ea7\u4e13\u5bb6\"SFT\u6a21\u578b\uff080-shot Micro-F1 0.839\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u79d1\u5b66AI\u63d0\u4f9b\u4e86\u5173\u952e\u89c2\u5bdf\uff1a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6700\u4f18\u4e13\u4e1a\u5316\u8def\u5f84\u53d6\u51b3\u4e8e\u4efb\u52a1\u6a21\u6001\uff0c\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u57fa\u4e8e\u79d1\u5b66\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6e05\u6670\u6846\u67b6\uff0c\u540c\u65f6\u786e\u8ba4\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\u7684\u4f18\u8d8a\u6027\u5e76\u8bc6\u522b\u4e86\u5fae\u8c03\u6a21\u578b\u4e2d\u4e00\u81f4\u7684\u4e0a\u4e0b\u6587\u5e72\u6270\u73b0\u8c61\u3002"}}
{"id": "2511.02507", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02507", "abs": "https://arxiv.org/abs/2511.02507", "authors": ["Nicolas Schuler", "Lea Dewald", "J\u00fcrgen Graf"], "title": "Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems", "comment": "6 pages, 4 figures, 1 table; accepted for MECATRONICS-REM 2025\n  International Conference, PARIS, FRANCE December 3-5 2025", "summary": "Recent advancements in Deep Learning enable hardware-based cognitive systems,\nthat is, mechatronic systems in general and robotics in particular with\nintegrated Artificial Intelligence, to interact with dynamic and unstructured\nenvironments. While the results are impressive, the application of such systems\nto critical tasks like autonomous driving as well as service and care robotics\nnecessitate the evaluation of large amount of heterogeneous data. Automated\nreport generation for Mobile Robotics can play a crucial role in facilitating\nthe evaluation and acceptance of such systems in various domains. In this\npaper, we propose a pipeline for generating automated reports in natural\nlanguage utilizing various multi-modal sensors that solely relies on local\nmodels capable of being deployed on edge computing devices, thus preserving the\nprivacy of all actors involved and eliminating the need for external services.\nIn particular, we evaluate our implementation on a diverse dataset spanning\nmultiple domains including indoor, outdoor and urban environments, providing\nquantitative as well as qualitative evaluation results. Various generated\nexample reports and other supplementary materials are available via a public\nrepository.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u7684\u81ea\u52a8\u5316\u62a5\u544a\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u8be5\u65b9\u6848\u5b8c\u5168\u4f9d\u8d56\u672c\u5730\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u5e76\u65e0\u9700\u5916\u90e8\u670d\u52a1\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u751f\u6210\u81ea\u7136\u8bed\u8a00\u62a5\u544a\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u5728\u786c\u4ef6\u8ba4\u77e5\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u670d\u52a1\u673a\u5668\u4eba\u7b49\u5173\u952e\u4efb\u52a1\u4e2d\uff0c\u9700\u8981\u8bc4\u4f30\u5927\u91cf\u5f02\u6784\u6570\u636e\u3002\u81ea\u52a8\u5316\u62a5\u544a\u751f\u6210\u5bf9\u4e8e\u4fc3\u8fdb\u6b64\u7c7b\u7cfb\u7edf\u5728\u5404\u4e2a\u9886\u57df\u7684\u8bc4\u4f30\u548c\u63a5\u53d7\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u4f9d\u8d56\u5916\u90e8\u670d\u52a1\u4e14\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u4f20\u611f\u5668\u7684\u81ea\u52a8\u5316\u62a5\u544a\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u8be5\u65b9\u6848\u5b8c\u5168\u91c7\u7528\u672c\u5730\u6a21\u578b\u90e8\u7f72\u5728\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u5907\u4e0a\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u4e91\u670d\u52a1\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5404\u79cd\u4f20\u611f\u5668\u6570\u636e\u751f\u6210\u81ea\u7136\u8bed\u8a00\u62a5\u544a\uff0c\u786e\u4fdd\u6240\u6709\u53c2\u4e0e\u8005\u7684\u9690\u79c1\u4fdd\u62a4\u3002", "result": "\u5728\u6db5\u76d6\u5ba4\u5185\u3001\u5ba4\u5916\u548c\u57ce\u5e02\u73af\u5883\u7b49\u591a\u79cd\u9886\u57df\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u7ed3\u679c\u3002\u751f\u6210\u7684\u793a\u4f8b\u62a5\u544a\u548c\u8865\u5145\u6750\u6599\u901a\u8fc7\u516c\u5171\u5b58\u50a8\u5e93\u63d0\u4f9b\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5b8c\u5168\u57fa\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u7684\u81ea\u52a8\u5316\u62a5\u544a\u751f\u6210\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5728\u5173\u952e\u4efb\u52a1\u5e94\u7528\u4e2d\u63a8\u5e7f\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u672c\u5730\u5316AI\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2511.02564", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02564", "abs": "https://arxiv.org/abs/2511.02564", "authors": ["Md Rashidunnabi", "Kailash A. Hambarde", "Vasco Lopes", "Joao C. Neves", "Hugo Proenca"], "title": "Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification", "comment": null, "summary": "Video-based person re-identification (ReID) in cross-view domains (for\nexample, aerial-ground surveillance) remains an open problem because of extreme\nviewpoint shifts, scale disparities, and temporal inconsistencies. To address\nthese challenges, we propose MTF-CVReID, a parameter-efficient framework that\nintroduces seven complementary modules over a ViT-B/16 backbone. Specifically,\nwe include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera and\nview biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scale\nstabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) to\nreinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) for\nmotion-aware short-term temporal encoding; (5) Inter-View Feature Alignment\n(IVFA) for perspective-invariant representation alignment; (6) Hierarchical\nTemporal Pattern Learning (HTPL) to capture multi-scale temporal regularities;\nand (7) Multi-View Identity Consistency Learning (MVICL) that enforces\ncross-view identity coherence using a contrastive learning paradigm. Despite\nadding only about 2 million parameters and 0.7 GFLOPs over the baseline,\nMTF-CVReID maintains real-time efficiency (189 FPS) and achieves\nstate-of-the-art performance on the AG-VPReID benchmark across all altitude\nlevels, with strong cross-dataset generalization to G2A-VReID and MARS\ndatasets. These results show that carefully designed adapter-based modules can\nsubstantially enhance cross-view robustness and temporal consistency without\ncompromising computational efficiency. The source code is available at\nhttps://github.com/MdRashidunnabi/MTF-CVReID", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MTF-CVReID\u6846\u67b6\uff0c\u901a\u8fc7\u4e03\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u6a21\u5757\u589e\u5f3aViT-B/16\u9aa8\u5e72\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u89c6\u89d2\u89c6\u9891\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u8de8\u89c6\u89d2\u89c6\u9891\u884c\u4eba\u91cd\u8bc6\u522b\u9762\u4e34\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u3001\u5c3a\u5ea6\u5dee\u5f02\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u7a7a\u4e2d-\u5730\u9762\u76d1\u63a7\u7b49\u573a\u666f\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u9c81\u68d2\u6846\u67b6\u3002", "method": "\u57fa\u4e8eViT-B/16\u9aa8\u5e72\u7f51\u7edc\uff0c\u5f15\u5165\u4e86\u4e03\u4e2a\u4e92\u8865\u6a21\u5757\uff1a\u8de8\u6d41\u7279\u5f81\u5f52\u4e00\u5316\u7528\u4e8e\u7ea0\u6b63\u76f8\u673a\u548c\u89c6\u89d2\u504f\u5dee\uff0c\u591a\u5206\u8fa8\u7387\u7279\u5f81\u534f\u8c03\u7528\u4e8e\u5c3a\u5ea6\u7a33\u5b9a\uff0c\u8eab\u4efd\u611f\u77e5\u8bb0\u5fc6\u6a21\u5757\u5f3a\u5316\u8eab\u4efd\u7279\u5f81\uff0c\u65f6\u95f4\u52a8\u6001\u5efa\u6a21\u8fdb\u884c\u8fd0\u52a8\u611f\u77e5\u7f16\u7801\uff0c\u8de8\u89c6\u89d2\u7279\u5f81\u5bf9\u9f50\u5b9e\u73b0\u89c6\u89d2\u4e0d\u53d8\u8868\u793a\uff0c\u5206\u5c42\u65f6\u95f4\u6a21\u5f0f\u5b66\u4e60\u6355\u83b7\u591a\u5c3a\u5ea6\u65f6\u95f4\u89c4\u5f8b\uff0c\u591a\u89c6\u89d2\u8eab\u4efd\u4e00\u81f4\u6027\u5b66\u4e60\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5f3a\u5236\u8de8\u89c6\u89d2\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "result": "MTF-CVReID\u4ec5\u589e\u52a0\u7ea6200\u4e07\u53c2\u6570\u548c0.7 GFLOPs\uff0c\u4fdd\u6301\u5b9e\u65f6\u6548\u7387\uff08189 FPS\uff09\uff0c\u5728AG-VPReID\u57fa\u51c6\u6d4b\u8bd5\u7684\u6240\u6709\u9ad8\u5ea6\u7ea7\u522b\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728G2A-VReID\u548cMARS\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u57fa\u4e8e\u9002\u914d\u5668\u7684\u6a21\u5757\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u8ba1\u7b97\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u589e\u5f3a\u8de8\u89c6\u89d2\u9c81\u68d2\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u8bc1\u660e\u4e86\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u5728\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02591", "abs": "https://arxiv.org/abs/2511.02591", "authors": ["Jan Frederik Meier", "Timo L\u00fcddecke"], "title": "Zero-Shot Multi-Animal Tracking in the Wild", "comment": null, "summary": "Multi-animal tracking is crucial for understanding animal ecology and\nbehavior. However, it remains a challenging task due to variations in habitat,\nmotion patterns, and species appearance. Traditional approaches typically\nrequire extensive model fine-tuning and heuristic design for each application\nscenario. In this work, we explore the potential of recent vision foundation\nmodels for zero-shot multi-animal tracking. By combining a Grounding Dino\nobject detector with the Segment Anything Model 2 (SAM 2) tracker and carefully\ndesigned heuristics, we develop a tracking framework that can be applied to new\ndatasets without any retraining or hyperparameter adaptation. Evaluations on\nChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstrate\nstrong and consistent performance across diverse species and environments. The\ncode is available at https://github.com/ecker-lab/SAM2-Animal-Tracking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u591a\u52a8\u7269\u8ffd\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408Grounding Dino\u76ee\u6807\u68c0\u6d4b\u5668\u548cSAM 2\u8ddf\u8e2a\u5668\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u8d85\u53c2\u6570\u8c03\u6574\u5373\u53ef\u5e94\u7528\u4e8e\u65b0\u6570\u636e\u96c6\u7684\u52a8\u7269\u8ffd\u8e2a\u3002", "motivation": "\u591a\u52a8\u7269\u8ffd\u8e2a\u5728\u7406\u89e3\u52a8\u7269\u751f\u6001\u548c\u884c\u4e3a\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u6816\u606f\u5730\u53d8\u5316\u3001\u8fd0\u52a8\u6a21\u5f0f\u548c\u7269\u79cd\u5916\u89c2\u7684\u5dee\u5f02\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e3a\u6bcf\u4e2a\u5e94\u7528\u573a\u666f\u8fdb\u884c\u5927\u91cf\u6a21\u578b\u5fae\u8c03\u548c\u542f\u53d1\u5f0f\u8bbe\u8ba1\uff0c\u8fd9\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u6548\u7387\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06Grounding Dino\u76ee\u6807\u68c0\u6d4b\u5668\u4e0eSegment Anything Model 2\u8ddf\u8e2a\u5668\u76f8\u7ed3\u5408\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7cbe\u5fc3\u4f18\u5316\u7684\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u8d85\u53c2\u6570\u9002\u5e94\u7684\u96f6\u6837\u672c\u8ffd\u8e2a\u6846\u67b6\u3002", "result": "\u5728ChimpAct\u3001Bird Flock Tracking\u3001AnimalTrack\u548cGMOT-40\u5b50\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7269\u79cd\u548c\u73af\u5883\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u4e14\u4e00\u81f4\u7684\u8ffd\u8e2a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u591a\u52a8\u7269\u8ffd\u8e2a\u4efb\u52a1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u751f\u6001\u5b66\u548c\u52a8\u7269\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u573a\u666f\u7279\u5b9a\u8c03\u6574\u7684\u4f9d\u8d56\u3002"}}
{"id": "2511.02650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02650", "abs": "https://arxiv.org/abs/2511.02650", "authors": ["Tianfan Peng", "Yuntao Du", "Pengzhou Ji", "Shijie Dong", "Kailin Jiang", "Mingchuan Ma", "Yijun Tian", "Jinhe Bi", "Qian Li", "Wei Du", "Feng Xiao", "Lizhen Cui"], "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models", "comment": null, "summary": "Large multimodal models (LMMs) often suffer from severe inference\ninefficiency due to the large number of visual tokens introduced by image\nencoders. While recent token compression methods, such as pruning and merging,\nhave shown promise in reducing redundancy, their evaluation remains fragmented\nand inconsistent. In this work, we present UniPruneBench, a unified and\nextensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench\nprovides standardized protocols across six ability dimensions and ten datasets,\ncovering ten representative compression algorithms and three families of LMMs\n(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates\nsystem-level metrics such as runtime and prefilling latency to provide a\nholistic view. Our experiments uncover several key findings: (1) random pruning\nis a surprisingly strong baseline, (2) no single method consistently\noutperforms others across scenarios, (3) pruning sensitivity varies\nsignificantly across tasks, with OCR being most vulnerable, and (4) pruning\nratio is the dominant factor governing performance degradation. We believe\nUniPruneBench will serve as a reliable foundation for future research on\nefficient multimodal modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UniPruneBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e2d\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u7684\u7edf\u4e00\u53ef\u6269\u5c55\u57fa\u51c6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u63ed\u793a\u4e86\u526a\u679d\u65b9\u6cd5\u7684\u5173\u952e\u53d1\u73b0\uff0c\u5305\u62ec\u968f\u673a\u526a\u679d\u4f5c\u4e3a\u5f3a\u57fa\u7ebf\u3001\u65b9\u6cd5\u6027\u80fd\u4e0d\u4e00\u81f4\u6027\u4ee5\u53ca\u526a\u679d\u6bd4\u4f8b\u7684\u4e3b\u5bfc\u4f5c\u7528\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u6a21\u578b\u7531\u4e8e\u56fe\u50cf\u7f16\u7801\u5668\u5f15\u5165\u7684\u5927\u91cf\u89c6\u89c9\u4ee4\u724c\u5bfc\u81f4\u4e25\u91cd\u7684\u63a8\u7406\u6548\u7387\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u4ee4\u724c\u538b\u7f29\u65b9\u6cd5\u8bc4\u4f30\u5b58\u5728\u788e\u7247\u5316\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u57fa\u51c6\u6765\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u526a\u679d\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "method": "UniPruneBench\u63d0\u4f9b\u4e86\u8de8\u516d\u4e2a\u80fd\u529b\u7ef4\u5ea6\u548c\u5341\u4e2a\u6570\u636e\u96c6\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\uff0c\u6db5\u76d6\u5341\u79cd\u4ee3\u8868\u6027\u538b\u7f29\u7b97\u6cd5\u548c\u4e09\u4e2aLMM\u5bb6\u65cf\uff08LLaVA-v1.5\u3001Intern-VL3\u548cQwen2.5-VL\uff09\uff0c\u9664\u4e86\u4efb\u52a1\u7cbe\u5ea6\u5916\u8fd8\u6574\u5408\u4e86\u8fd0\u884c\u65f6\u548c\u524d\u586b\u5145\u5ef6\u8fdf\u7b49\u7cfb\u7edf\u7ea7\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u968f\u673a\u526a\u679d\u662f\u4e00\u4e2a\u4ee4\u4eba\u60ca\u8bb6\u7684\u5f3a\u57fa\u7ebf\uff0c\u6ca1\u6709\u5355\u4e00\u65b9\u6cd5\u5728\u6240\u6709\u573a\u666f\u4e2d\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u526a\u679d\u654f\u611f\u6027\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u5dee\u5f02\u663e\u8457\uff08OCR\u6700\u8106\u5f31\uff09\uff0c\u4e14\u526a\u679d\u6bd4\u4f8b\u662f\u6027\u80fd\u4e0b\u964d\u7684\u4e3b\u5bfc\u56e0\u7d20\u3002", "conclusion": "UniPruneBench\u4e3a\u9ad8\u6548\u591a\u6a21\u6001\u5efa\u6a21\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u526a\u679d\u65b9\u6cd5\u9009\u62e9\u9700\u8981\u6839\u636e\u5177\u4f53\u4efb\u52a1\u573a\u666f\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u5f3a\u8c03\u4e86\u526a\u679d\u6bd4\u4f8b\u63a7\u5236\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.02685", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02685", "abs": "https://arxiv.org/abs/2511.02685", "authors": ["Chao Yuan", "Zanwu Liu", "Guiwei Zhang", "Haoxuan Xu", "Yujian Zhao", "Guanglin Niu", "Bo Li"], "title": "Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification", "comment": null, "summary": "Visible-infrared person re-identification (VI-ReID) technique could associate\nthe pedestrian images across visible and infrared modalities in the practical\nscenarios of background illumination changes. However, a substantial gap\ninherently exists between these two modalities. Besides, existing methods\nprimarily rely on intermediate representations to align cross-modal features of\nthe same person. The intermediate feature representations are usually create by\ngenerating intermediate images (kind of data enhancement), or fusing\nintermediate features (more parameters, lack of interpretability), and they do\nnot make good use of the intermediate features. Thus, we propose a novel\nVI-ReID framework via Modality-Transition Representation Learning (MTRL) with a\nmiddle generated image as a transmitter from visible to infrared modals, which\nare fully aligned with the original visible images and similar to the infrared\nmodality. After that, using a modality-transition contrastive loss and a\nmodality-query regularization loss for training, which could align the\ncross-modal features more effectively. Notably, our proposed framework does not\nneed any additional parameters, which achieves the same inference speed to the\nbackbone while improving its performance on VI-ReID task. Extensive\nexperimental results illustrate that our model significantly and consistently\noutperforms existing SOTAs on three typical VI-ReID datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u6001\u8f6c\u6362\u8868\u793a\u5b66\u4e60\uff08MTRL\uff09\u7684\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u4e2d\u95f4\u56fe\u50cf\u4f5c\u4e3a\u6a21\u6001\u8f6c\u6362\u5668\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u5373\u53ef\u6709\u6548\u5bf9\u9f50\u8de8\u6a21\u6001\u7279\u5f81\uff0c\u5728\u4e09\u4e2a\u5178\u578b\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e2d\u95f4\u8868\u793a\u6765\u5bf9\u9f50\u8de8\u6a21\u6001\u7279\u5f81\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u751f\u6210\u4e2d\u95f4\u56fe\u50cf\u6216\u878d\u5408\u4e2d\u95f4\u7279\u5f81\u5b9e\u73b0\uff0c\u5b58\u5728\u53c2\u6570\u8fc7\u591a\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u4e2d\u95f4\u7279\u5f81\u7684\u6709\u6548\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u6a21\u6001\u8f6c\u6362\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u751f\u6210\u4e2d\u95f4\u56fe\u50cf\u4f5c\u4e3a\u53ef\u89c1\u5149\u5230\u7ea2\u5916\u6a21\u6001\u7684\u8f6c\u6362\u5668\uff0c\u8be5\u56fe\u50cf\u4e0e\u539f\u59cb\u53ef\u89c1\u5149\u56fe\u50cf\u5b8c\u5168\u5bf9\u9f50\u4e14\u4e0e\u7ea2\u5916\u6a21\u6001\u76f8\u4f3c\uff1b\u91c7\u7528\u6a21\u6001\u8f6c\u6362\u5bf9\u6bd4\u635f\u5931\u548c\u6a21\u6001\u67e5\u8be2\u6b63\u5219\u5316\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\uff0c\u6709\u6548\u5bf9\u9f50\u8de8\u6a21\u6001\u7279\u5f81\u4e14\u65e0\u9700\u989d\u5916\u53c2\u6570\u3002", "result": "\u5728\u4e09\u4e2a\u5178\u578b\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u6a21\u578b\u663e\u8457\u4e14\u6301\u7eed\u5730\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u9aa8\u5e72\u7f51\u7edc\u76f8\u540c\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6a21\u6001\u8f6c\u6362\u8868\u793a\u5b66\u4e60\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u53ef\u89c1\u5149-\u7ea2\u5916\u6a21\u6001\u95f4\u7684\u7279\u5f81\u5bf9\u9f50\u95ee\u9898\uff0c\u65e0\u9700\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u5373\u53ef\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u8de8\u6a21\u6001\u884c\u4eba\u91cd\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02767", "abs": "https://arxiv.org/abs/2511.02767", "authors": ["Tyler Zhu", "Tengda Han", "Leonidas Guibas", "Viorica P\u0103tr\u0103ucean", "Maks Ovsjanikov"], "title": "Dynamic Reflections: Probing Video Representations with Text Alignment", "comment": "21 pages, 12 figures", "summary": "The alignment of representations from different modalities has recently been\nshown to provide insights on the structural similarities and downstream\ncapabilities of different encoders across diverse data types. While significant\nprogress has been made in aligning images with text, the temporal nature of\nvideo data remains largely unexplored in this context. In this work, we conduct\nthe first comprehensive study of video-text representation alignment, probing\nthe capabilities of modern video and language encoders. Our findings reveal\nseveral key insights. First, we demonstrate that cross-modal alignment highly\ndepends on the richness of both visual (static images vs. multi-frame videos)\nand text (single caption vs. a collection) data provided at test time,\nespecially when using state-of-the-art video encoders. We propose parametric\ntest-time scaling laws that capture this behavior and show remarkable\npredictive power against empirical observations. Secondly, we investigate the\ncorrelation between semantic alignment and performance on both semantic and\nnon-semantic downstream tasks, providing initial evidence that strong alignment\nagainst text encoders may be linked to general-purpose video representation and\nunderstanding. Finally, we correlate temporal reasoning with cross-modal\nalignment providing a challenging test-bed for vision and language models.\nOverall, our work introduces video-text alignment as an informative zero-shot\nway to probe the representation power of different encoders for spatio-temporal\ndata. Project page can be found at https://video-prh.github.io/", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u89c6\u9891-\u6587\u672c\u8868\u793a\u5bf9\u9f50\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u53c2\u6570\u5316\u6d4b\u8bd5\u65f6\u7f29\u653e\u5b9a\u5f8b\u6765\u6355\u6349\u8de8\u6a21\u6001\u5bf9\u9f50\u884c\u4e3a\uff0c\u5e76\u63ed\u793a\u4e86\u8bed\u4e49\u5bf9\u9f50\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u4e3a\u65f6\u7a7a\u6570\u636e\u7684\u8868\u793a\u80fd\u529b\u63d0\u4f9b\u4e86\u96f6\u6837\u672c\u63a2\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u56fe\u50cf\u4e0e\u6587\u672c\u7684\u5bf9\u9f50\u7814\u7a76\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u89c6\u9891\u6570\u636e\u7684\u65f6\u5e8f\u7279\u6027\u5728\u6b64\u80cc\u666f\u4e0b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u89c6\u9891-\u6587\u672c\u8868\u793a\u5bf9\u9f50\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u7cfb\u7edf\u8bc4\u4f30\u73b0\u4ee3\u89c6\u9891\u548c\u8bed\u8a00\u7f16\u7801\u5668\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u53c2\u6570\u5316\u6d4b\u8bd5\u65f6\u7f29\u653e\u5b9a\u5f8b\u6765\u6355\u6349\u8de8\u6a21\u6001\u5bf9\u9f50\u884c\u4e3a\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u89c6\u89c9\u6570\u636e\uff08\u9759\u6001\u56fe\u50cfvs\u591a\u5e27\u89c6\u9891\uff09\u548c\u6587\u672c\u6570\u636e\uff08\u5355\u5b57\u5e55vs\u96c6\u5408\uff09\u4e30\u5bcc\u5ea6\u5bf9\u5bf9\u9f50\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bed\u4e49\u5bf9\u9f50\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5173\u8054\u6027\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6d4b\u8bd5\u65f6\u63d0\u4f9b\u7684\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u4e30\u5bcc\u5ea6\uff0c\u63d0\u51fa\u7684\u7f29\u653e\u5b9a\u5f8b\u5c55\u73b0\u51fa\u4e0e\u7ecf\u9a8c\u89c2\u5bdf\u7684\u663e\u8457\u9884\u6d4b\u80fd\u529b\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f3a\u6587\u672c\u7f16\u7801\u5668\u5bf9\u9f50\u53ef\u80fd\u4e0e\u901a\u7528\u89c6\u9891\u8868\u793a\u548c\u7406\u89e3\u80fd\u529b\u76f8\u5173\u3002", "conclusion": "\u672c\u7814\u7a76\u786e\u7acb\u4e86\u89c6\u9891-\u6587\u672c\u5bf9\u9f50\u4f5c\u4e3a\u63a2\u6d4b\u65f6\u7a7a\u6570\u636e\u8868\u793a\u80fd\u529b\u7684\u6709\u6548\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u4e3a\u7406\u89e3\u89c6\u9891\u7f16\u7801\u5668\u7684\u8868\u793a\u80fd\u529b\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u673a\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u5efa\u7acb\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u57fa\u51c6\u3002"}}
{"id": "2511.02779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02779", "abs": "https://arxiv.org/abs/2511.02779", "authors": ["Yiyang Zhou", "Haoqin Tu", "Zijun Wang", "Zeyu Wang", "Niklas Muennighoff", "Fan Nie", "Yejin Choi", "James Zou", "Chaorui Deng", "Shen Yan", "Haoqi Fan", "Cihang Xie", "Huaxiu Yao", "Qinghao Ye"], "title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought", "comment": "28 pages, 15 figures", "summary": "We propose MIRA, a new benchmark designed to evaluate models in scenarios\nwhere generating intermediate visual images is essential for successful\nreasoning. Unlike traditional CoT methods that rely solely on text, tasks in\nMIRA require models to generate and utilize intermediate images - such as\nsketches, structural diagrams, or path drawings - to guide their reasoning\nprocess. This setup closely mirrors how humans solve complex problems through\n\"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically\nchallenging and involve complex structures, spatial relationships, or reasoning\nsteps that are difficult to express through language alone. To ensure that our\nevaluation data is of high-quality, we include 546 multimodal problems,\nannotated with intermediate visual images and final answers. We also propose a\nunified evaluation protocol for MIRA that spans three levels of evaluation\ninput: direct input with image and question only, text-only CoT input with\nimage and thinking prompts, and Visual-CoT input with both annotated image\nclues and textual thinking prompts. To probe the upper bound of model capacity\non our benchmark, we also report pass@k and majority voting accuracies under\ndifferent k settings. Experimental results show that existing multimodal large\nlanguage models, including strongest private models as well as strong\nopen-weight models, perform poorly when relying solely on textual prompts.\nHowever, when intermediate visual cues are provided, model performance improves\nconsistently, yielding an average relative gain of 33.7% across all models and\ntasks. We also probe the upper bound by expanding the search space and\ndesigning textual prompts aligned with Visual-CoT, but both yield only limited\nimprovements compared to our Visual-CoT setting. These results underscore the\ncritical role of imagined visual information in enabling successful reasoning\non MIRA.", "AI": {"tldr": "MIRA\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u6a21\u578b\u5728\u9700\u8981\u751f\u6210\u4e2d\u95f4\u89c6\u89c9\u56fe\u50cf\u4ee5\u8fdb\u884c\u6210\u529f\u63a8\u7406\u7684\u573a\u666f\u4e2d\u7684\u80fd\u529b\uff0c\u5f3a\u8c03\u89c6\u89c9\u601d\u7ef4\u5728\u590d\u6742\u63a8\u7406\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u601d\u7ef4\u94fe\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6587\u672c\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u5728\u9700\u8981\u751f\u6210\u548c\u5229\u7528\u4e2d\u95f4\u89c6\u89c9\u56fe\u50cf\uff08\u5982\u8349\u56fe\u3001\u7ed3\u6784\u56fe\u6216\u8def\u5f84\u56fe\uff09\u6765\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd9\u79cd\u8bbe\u7f6e\u66f4\u8d34\u8fd1\u4eba\u7c7b\u901a\u8fc7\"\u7ed8\u56fe\u601d\u8003\"\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u65b9\u5f0f\u3002", "method": "MIRA\u57fa\u51c6\u5305\u542b546\u4e2a\u591a\u6a21\u6001\u95ee\u9898\uff0c\u5e76\u6807\u6ce8\u4e86\u4e2d\u95f4\u89c6\u89c9\u56fe\u50cf\u548c\u6700\u7ec8\u7b54\u6848\uff1b\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u6db5\u76d6\u4e09\u4e2a\u8bc4\u4f30\u8f93\u5165\u7ea7\u522b\uff1a\u4ec5\u56fe\u50cf\u548c\u95ee\u9898\u7684\u76f4\u63a5\u8f93\u5165\u3001\u5e26\u56fe\u50cf\u548c\u601d\u7ef4\u63d0\u793a\u7684\u7eaf\u6587\u672cCoT\u8f93\u5165\uff0c\u4ee5\u53ca\u5e26\u6ce8\u91ca\u56fe\u50cf\u7ebf\u7d22\u548c\u6587\u672c\u601d\u7ef4\u63d0\u793a\u7684Visual-CoT\u8f93\u5165\uff1b\u8fd8\u62a5\u544a\u4e86\u4e0d\u540ck\u8bbe\u7f6e\u4e0b\u7684pass@k\u548c\u591a\u6570\u6295\u7968\u51c6\u786e\u7387\u4ee5\u63a2\u6d4b\u6a21\u578b\u80fd\u529b\u4e0a\u9650\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ec5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u65f6\u8868\u73b0\u8f83\u5dee\uff0c\u4f46\u5f53\u63d0\u4f9b\u4e2d\u95f4\u89c6\u89c9\u7ebf\u7d22\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u4e00\u81f4\u63d0\u5347\uff0c\u5728\u6240\u6709\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u5e73\u5747\u76f8\u5bf9\u589e\u76ca\u8fbe\u523033.7%\uff1b\u901a\u8fc7\u6269\u5927\u641c\u7d22\u7a7a\u95f4\u548c\u8bbe\u8ba1\u4e0eVisual-CoT\u5bf9\u9f50\u7684\u6587\u672c\u63d0\u793a\u6765\u63a2\u6d4b\u4e0a\u9650\uff0c\u4f46\u76f8\u6bd4Visual-CoT\u8bbe\u7f6e\u4ec5\u83b7\u5f97\u6709\u9650\u6539\u8fdb\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u60f3\u8c61\u89c6\u89c9\u4fe1\u606f\u5728MIRA\u4e0a\u5b9e\u73b0\u6210\u529f\u63a8\u7406\u7684\u5173\u952e\u4f5c\u7528\uff0c\u8868\u660e\u89c6\u89c9\u601d\u7ef4\u5bf9\u4e8e\u5904\u7406\u6d89\u53ca\u590d\u6742\u7ed3\u6784\u3001\u7a7a\u95f4\u5173\u7cfb\u6216\u96be\u4ee5\u4ec5\u901a\u8fc7\u8bed\u8a00\u8868\u8fbe\u7684\u63a8\u7406\u6b65\u9aa4\u7684\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u57fa\u51c6\u3002"}}
