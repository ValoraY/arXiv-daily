<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 35]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging Text Guidance for Enhancing Demographic Fairness in Gender Classification](https://arxiv.org/abs/2512.11015)
*Anoop Krishnan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºä¸¤ç§åŸºäºæ–‡æœ¬å¼•å¯¼çš„æ–¹æ³•æ¥æå‡é¢éƒ¨å›¾åƒæ€§åˆ«åˆ†ç±»ç®—æ³•çš„å…¬å¹³æ€§ï¼šå›¾åƒæ–‡æœ¬åŒ¹é…å¼•å¯¼å’Œå›¾åƒæ–‡æœ¬èåˆï¼Œè¿™äº›æ–¹æ³•åˆ©ç”¨å›¾åƒæ ‡é¢˜çš„è¯­ä¹‰ä¿¡æ¯æ¥å¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›å¹¶å‡å°‘äººå£ç»Ÿè®¡åå·®ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³é¢éƒ¨å›¾åƒæ€§åˆ«åˆ†ç±»ç®—æ³•ä¸­å­˜åœ¨çš„å…¬å¹³æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯äººå£ç»Ÿè®¡åå·®é—®é¢˜ï¼Œå½“å‰æ–¹æ³•åœ¨è·¨æ€§åˆ«å’Œç§æ—ç¾¤ä½“ä¸Šå­˜åœ¨æ€§èƒ½å·®å¼‚ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿå‡å°‘è¿™äº›å·®å¼‚å¹¶æé«˜ç®—æ³•å…¬å¹³æ€§çš„æ–°æ–¹æ³•ã€‚

**Method:** æœ¬æ–‡æå‡ºä¸¤ç§æ ¸å¿ƒæ–¹æ³•ï¼šå›¾åƒæ–‡æœ¬åŒ¹é…å¼•å¯¼è®­ç»ƒæ¨¡å‹å­¦ä¹ å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„ç»†ç²’åº¦å¯¹é½ä»¥è·å¾—å¢å¼ºçš„å¤šæ¨¡æ€è¡¨ç¤ºï¼›å›¾åƒæ–‡æœ¬èåˆå°†ä¸¤ç§æ¨¡æ€ç»“åˆä¸ºç»¼åˆè¡¨ç¤ºä»¥æ”¹å–„å…¬å¹³æ€§ï¼Œè¿™äº›æ–¹æ³•æ— éœ€äººå£ç»Ÿè®¡æ ‡ç­¾ä¸”å…·æœ‰åº”ç”¨æ— å…³æ€§ã€‚

**Result:** åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æœ‰æ•ˆå‡è½»äº†åè§ï¼Œå¹¶ç›¸æ¯”ç°æœ‰æ–¹æ³•åœ¨è·¨æ€§åˆ«å’Œç§æ—ç¾¤ä½“ä¸Šæé«˜äº†åˆ†ç±»å‡†ç¡®æ€§ï¼ŒåŒæ—¶éªŒè¯äº†è¯­ä¹‰ä¿¡æ¯åœ¨å‡å°‘æ€§èƒ½å·®å¼‚æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå¼€å‘æ›´å…¬å¹³çš„é¢éƒ¨åˆ†æç®—æ³•æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå±•ç¤ºäº†æ–‡æœ¬å¼•å¯¼æ–¹æ³•åœ¨æé«˜è®¡ç®—æœºè§†è§‰ç³»ç»Ÿå…¬å¹³æ€§æ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¸ºå¯è§£é‡Šå’Œç›´è§‚çš„è®­ç»ƒèŒƒå¼å¥ å®šäº†åŸºç¡€ï¼Œä¸ºè§£å†³é¢éƒ¨å›¾åƒæ€§åˆ«åˆ†ç±»ä¸­çš„äººå£ç»Ÿè®¡åå·®æŒ‘æˆ˜åšå‡ºäº†è´¡çŒ®ã€‚

---

#### ğŸ“„ Abstract
In the quest for fairness in artificial intelligence, novel approaches to enhance it in facial image based gender classification algorithms using text guided methodologies are presented. The core methodology involves leveraging semantic information from image captions during model training to improve generalization capabilities. Two key strategies are presented: Image Text Matching (ITM) guidance and Image Text fusion. ITM guidance trains the model to discern fine grained alignments between images and texts to obtain enhanced multimodal representations. Image text fusion combines both modalities into comprehensive representations for improved fairness. Exensive experiments conducted on benchmark datasets demonstrate these approaches effectively mitigate bias and improve accuracy across gender racial groups compared to existing methods. Additionally, the unique integration of textual guidance underscores an interpretable and intuitive training paradigm for computer vision systems. By scrutinizing the extent to which semantic information reduces disparities, this research offers valuable insights into cultivating more equitable facial analysis algorithms. The proposed methodologies contribute to addressing the pivotal challenge of demographic bias in gender classification from facial images. Furthermore, this technique operates in the absence of demographic labels and is application agnostic.


### [2] [Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context](https://arxiv.org/abs/2512.11167)
*Anatole Jacquin de Margerie, Alexis Roger, Irina Rish*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶å¯¹CVPR24å‘è¡¨çš„Monkeyè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¤ç°ä¸æ‰©å±•åˆ†æï¼ŒéªŒè¯äº†å›¾åƒåˆ†å—ç­–ç•¥åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ¢è®¨äº†å…¨å±€ä¸Šä¸‹æ–‡æ•´åˆçš„å½±å“ï¼Œä¸ºé«˜åˆ†è¾¨ç‡å¤šæ¨¡æ€å»ºæ¨¡æä¾›äº†å®ç”¨è§è§£ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¯å¤ç°æ€§æ˜¯ç§‘å­¦è¿›æ­¥çš„åŸºç¡€ï¼Œä½†å¤æ‚çš„å¤šæ¨¡æ€æ¨¡å‹å¾€å¾€ç¼ºä¹é€æ˜çš„å®ç°ç»†èŠ‚å’Œå¯è®¿é—®çš„è®­ç»ƒåŸºç¡€è®¾æ–½ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¯¹Monkeyè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè¯¦ç»†å¤ç°å’Œæ‰¹åˆ¤æ€§åˆ†æï¼Œä»¥éªŒè¯å…¶é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£æ–¹æ³•å¹¶æ¢ç´¢æ”¹è¿›æ–¹å‘ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨å¼€æ”¾æ£€æŸ¥ç‚¹å¤ç°äº†Monkeyè§†è§‰è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæµç¨‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å›¾åƒåˆ†å—ç­–ç•¥å°†å¤§å›¾åƒåˆ†å‰²ä¸ºå¤šä¸ªå›¾å—ä»¥æ¢å¤ç»†ç²’åº¦è§†è§‰ç»†èŠ‚ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ‰©å±•ç ”ç©¶äº†å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ•´åˆæ•ˆæœã€‚

**Result:** ç ”ç©¶ç¡®è®¤äº†åŸå§‹Monkey VLMå·¥ä½œçš„å…³é”®å‘ç°ï¼Œå³åˆ†å—ç­–ç•¥èƒ½æœ‰æ•ˆæ¢å¤å±€éƒ¨ç»†èŠ‚ï¼Œä½†åŒæ—¶ä¹ŸæŠ¥å‘Šäº†ç»“æœåå·®ï¼Œè¿™äº›åå·®çš„å¹…åº¦ä¸¥é‡ä¾èµ–äºä»»åŠ¡ç±»å‹å’Œåˆ†å—ç²’åº¦ï¼Œå…¨å±€ä¸Šä¸‹æ–‡çš„æ•´åˆæ•ˆæœä¸ºé«˜åˆ†è¾¨ç‡å¤šæ¨¡æ€å»ºæ¨¡æä¾›äº†å®ç”¨è§è§£ã€‚

**Conclusion:** å›¾åƒåˆ†å—ç­–ç•¥æ˜¯é«˜åˆ†è¾¨ç‡è§†è§‰ç†è§£çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†æ€§èƒ½è¡¨ç°å¯¹ä»»åŠ¡ç‰¹æ€§å’Œåˆ†å—å‚æ•°æ•æ„Ÿï¼Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ•´åˆå…·æœ‰é‡è¦å½±å“ï¼Œè¿™ä¸ºæœªæ¥é«˜åˆ†è¾¨ç‡å¤šæ¨¡æ€æ¨¡å‹è®¾è®¡æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼å’Œç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Reproducibility remains a cornerstone of scientific progress, yet complex multimodal models often lack transparent implementation details and accessible training infrastructure. In this work, we present a detailed reproduction and critical analysis of the Monkey Vision-Language Model (VLM) (Li et al. 2023b) published in CVPR24, a recent approach to high-resolution image understanding via image tiling. The original paper proposed splitting large images into tiles to recover fine-grained visual details while maintaining computational efficiency. Our study replicates this strategy using open checkpoints and reimplements the training pipeline. We confirm the key finding of the original Monkey VLM work, namely that tiling effectively recovers local details. We then extend this work further, by investigating the effect of the inclusion of the global context, which provide practical insights for future high-resolution multimodal modeling. However, we also report deviations in the results, with the magnitude of these effects depending heavily on task type and tile granularity.


### [3] [Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining](https://arxiv.org/abs/2512.11296)
*Yasaman Hashem Pour, Nazanin Mahjourian, Vinh Nguyen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å°‘æ ·æœ¬éªŒè¯æ–¹æ³•ï¼Œç”¨äºåŒæ—¶è¯„ä¼°æ•°æ§æœºåºŠçš„Gä»£ç å’ŒHMIæ˜¾ç¤ºç•Œé¢ä¸­çš„é”™è¯¯ä¸å®‰å…¨çŠ¶æ€ï¼Œè§£å†³äº†ä¼ ç»ŸLLMæ–¹æ³•æ— æ³•å¤„ç†è§†è§‰æ¨¡æ€ä¿¡æ¯çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»ŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„Gä»£ç éªŒè¯æ–¹æ³•ä¸»è¦å…³æ³¨ç¼–ç¨‹é”™è¯¯æ£€æµ‹ï¼Œä½†æ•°æ§åŠ å·¥éœ€è¦å¹¿æ³›ä½¿ç”¨å’Œäº†è§£äººæœºç•Œé¢ï¼Œè¯¥ç•Œé¢æ˜¾ç¤ºæœºå™¨çŠ¶æ€å’Œé”™è¯¯ä¿¡æ¯ã€‚ç”±äºLLMæ— æ³•è®¿é—®è§†è§‰æ¨¡æ€ï¼Œå½“å‰æ–¹æ³•ç¼ºä¹åˆ©ç”¨HMIçŸ¥è¯†çš„èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†Gä»£ç éªŒè¯çš„å…¨é¢æ€§ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ç§å°‘æ ·æœ¬VLMéªŒè¯æ–¹æ³•ï¼Œè¾“å…¥æ•°æ®é›†åŒ…å«æ¥è‡ª15-slant-PROè½¦åºŠçš„é…å¯¹Gä»£ç æ–‡æœ¬å’Œç›¸å…³HMIæˆªå›¾ï¼ŒåŒ…æ‹¬æ­£ç¡®å’Œæ˜“å‡ºé”™æ¡ˆä¾‹ã€‚ä¸ºå®ç°å°‘æ ·æœ¬å­¦ä¹ ï¼ŒVLMé…å¤‡äº†åŸºäºå…ˆéªŒå¯å‘å¼çŸ¥è¯†çš„ç»“æ„åŒ–JSONæ¨¡å¼ï¼Œå¹¶ä½¿ç”¨åŒ…å«é”™è¯¯å’Œæ— é”™è¯¯çš„Gä»£ç åŠHMIå®ä¾‹ä½œä¸ºå°‘æ ·æœ¬ç¤ºä¾‹æ¥æŒ‡å¯¼æ¨¡å‹ã€‚

**Result:** ä¸é›¶æ ·æœ¬VLMç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªé”™è¯¯Gä»£ç å’ŒHMIé”™è¯¯åœºæ™¯ä¸‹é€šè¿‡æ¯æ§½å‡†ç¡®ç‡è¿›è¡Œè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œå°‘æ ·æœ¬æç¤ºæ˜¾è‘—å¢å¼ºäº†HMIé”™è¯¯æ£€æµ‹èƒ½åŠ›ï¼Œå¹¶æ”¹å–„äº†Gä»£ç ä¸HMIæ˜¾ç¤ºä¹‹é—´ä¸ä¸€è‡´æ€§çš„è¯†åˆ«ï¼Œå®ç°äº†æ›´å…¨é¢çš„è°ƒè¯•åŠŸèƒ½ã€‚

**Conclusion:** æ‰€æå‡ºçš„æ¡†æ¶è¢«è¯æ˜é€‚ç”¨äºéªŒè¯é€šå¸¸åœ¨CNCåŸ¹è®­ä¸­å¼€å‘çš„æ‰‹åŠ¨ç”ŸæˆGä»£ç ï¼Œä¸ºæ•°æ§æœºåºŠæ“ä½œå­¦ä¹ æä¾›äº†æ›´å…¨é¢çš„éªŒè¯æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆè§†è§‰æ¨¡æ€ä¿¡æ¯ï¼Œè§£å†³äº†ä¼ ç»Ÿçº¯æ–‡æœ¬æ–¹æ³•çš„å±€é™æ€§ï¼Œä¸ºå·¥ä¸šåŸ¹è®­å’Œå®‰å…¨éªŒè¯å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.


### [4] [Synthetic Vasculature and Pathology Enhance Vision-Language Model Reasoning](https://arxiv.org/abs/2512.11060)
*Chenjun Li, Cheng Wan, Laurin Lux, Alexander Berger, Richard B. Rosen, Martin J. Menten, Johannes C. Paetzold*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†Synthetic Vasculature Reasoning (SVR)æ¡†æ¶ï¼Œé€šè¿‡å¯æ§åˆæˆè§†ç½‘è†œè¡€ç®¡å›¾åƒå’Œå¯¹åº”æ–‡æœ¬ï¼Œè§£å†³äº†åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­é«˜è´¨é‡æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œå¹¶æ„å»ºäº†OCTA-100K-SVRæ•°æ®é›†ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨OCTAå›¾åƒä¸Šçš„è¯Šæ–­æ€§èƒ½å’Œè§£é‡Šèƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦è¯Šæ–­ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†è®­ç»ƒéœ€è¦å¤§è§„æ¨¡é«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®ï¼Œè€Œåœ¨è®¸å¤šä¸“ä¸šé¢†åŸŸå¦‚å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æè¡€ç®¡æˆåƒä¸­ï¼ŒåŒ…å«ç—…ç†ç»†èŠ‚çš„ç²¾ç¡®æ–‡æœ¬æ ‡æ³¨éå¸¸ç¨€ç¼ºç”šè‡³ä¸å­˜åœ¨ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨ä¸´åºŠè§£é‡Šå’Œè·¨æ¨¡æ€æ¨ç†æ–¹é¢çš„å‘å±•ã€‚

**Method:** ç ”ç©¶æå‡ºäº†Synthetic Vasculature Reasoningæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå¯æ§åœ°åˆæˆå…·æœ‰ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ç‰¹å¾çš„è§†ç½‘è†œè¡€ç®¡å›¾åƒï¼ŒåŒ…æ‹¬æ¯›ç»†è¡€ç®¡è„±è½ã€å¾®åŠ¨è„‰ç˜¤ã€æ–°ç”Ÿè¡€ç®¡å’Œè¡€ç®¡è¿‚æ›²ç­‰ç—…ç†ç‰¹å¾ï¼ŒåŒæ—¶è‡ªåŠ¨ç”Ÿæˆç»†ç²’åº¦çš„æ¨ç†æ–‡æœ¬ï¼Œå¹¶åŸºäºæ­¤æ„å»ºäº†åŒ…å«10ä¸‡å¯¹å›¾åƒçš„OCTA-100K-SVRæ•°æ®é›†ã€‚

**Result:** åœ¨OCTA-100K-SVRæ•°æ®é›†ä¸Šè®­ç»ƒçš„é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®OCTAå›¾åƒä¸Šå®ç°äº†89.67%çš„é›¶æ ·æœ¬å¹³è¡¡åˆ†ç±»å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ç›‘ç£åŸºçº¿æ–¹æ³•ï¼Œé€šè¿‡äººç±»ä¸“å®¶è¯„ä¼°è¯å®ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—æå‡äº†ä¸´åºŠæ•°æ®çš„è§£é‡Šè´¨é‡å’Œç—…ç†å®šä½èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¯æ§åˆæˆæ–¹æ³•ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®æ˜¯è§£å†³åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹æ•°æ®ç¨€ç¼ºé—®é¢˜çš„æœ‰æ•ˆé€”å¾„ï¼ŒSVRæ¡†æ¶ä¸ä»…æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†ä¸´åºŠè§£é‡Šçš„å¯ä¿¡åº¦ï¼Œä¸ºä¸“ä¸šåŒ»å­¦é¢†åŸŸçš„AIè¯Šæ–­ç³»ç»Ÿå¼€å‘æä¾›äº†æ–°çš„æ•°æ®ç”ŸæˆèŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
Vision-Language Models (VLMs) offer a promising path toward interpretable medical diagnosis by allowing users to ask about clinical explanations alongside predictions and across different modalities. However, training VLMs for detailed reasoning requires large-scale image-text datasets. In many specialized domains, for example in reading Optical Coherence Tomography Angiography (OCTA) images, such precise text with grounded description of pathologies is scarce or even non-existent. To overcome this bottleneck, we introduce Synthetic Vasculature Reasoning (SVR), a framework that controllably synthesizes images and corresponding text, specifically: realistic retinal vasculature with Diabetic Retinopathy (DR) features: capillary dropout, microaneurysms, neovascularization, and tortuosity, while automatically generating granular reasoning texts. Based on this we curate OCTA-100K-SVR, an OCTA image-reasoning dataset with 100,000 pairs. Our experiments show that a general-purpose VLM (Qwen3-VL-8b) trained on the dataset achieves a zero-shot balanced classification accuracy of 89.67% on real OCTA images, outperforming supervised baselines. Through human expert evaluation we also demonstrate that it significantly enhances explanation quality and pathology localization on clinical data.


### [5] [Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation](https://arxiv.org/abs/2512.11458)
*Jingmin Zhu, Anqi Zhu, Hossein Rahmani, Jun Liu, Mohammed Bennamoun, Qiuhong Ke*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Skeleton-Cacheï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºåŸºäºéª¨æ¶çš„é›¶æ ·æœ¬åŠ¨ä½œè¯†åˆ«çš„å…è®­ç»ƒæµ‹è¯•æ—¶è‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡å°†æ¨ç†é‡æ„ä¸ºè½»é‡çº§æ£€ç´¢è¿‡ç¨‹å¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹æœªè§åŠ¨ä½œçš„æ³›åŒ–æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºéª¨æ¶çš„é›¶æ ·æœ¬åŠ¨ä½œè¯†åˆ«æ–¹æ³•åœ¨æ¨ç†æ—¶éš¾ä»¥é€‚åº”æœªè§åŠ¨ä½œï¼Œç¼ºä¹æœ‰æ•ˆçš„æµ‹è¯•æ—¶è‡ªé€‚åº”æœºåˆ¶ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæå‡ºé¦–ä¸ªå…è®­ç»ƒçš„æµ‹è¯•æ—¶è‡ªé€‚åº”æ¡†æ¶ï¼Œä»¥æå‡æ¨¡å‹å¯¹æœªè§åŠ¨ä½œçš„è¯†åˆ«èƒ½åŠ›ã€‚

**Method:** Skeleton-Cacheå°†æ¨ç†é‡æ„ä¸ºè½»é‡çº§æ£€ç´¢è¿‡ç¨‹ï¼Œé€šè¿‡éå‚æ•°ç¼“å­˜å­˜å‚¨ç»“æ„åŒ–çš„éª¨æ¶è¡¨ç¤ºï¼Œç»“åˆå…¨å±€å’Œç»†ç²’åº¦å±€éƒ¨æè¿°ç¬¦ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ä¸ºç±»åˆ«åˆ†é…é‡è¦æ€§æƒé‡ï¼ŒæŒ‡å¯¼æè¿°ç¬¦é¢„æµ‹çš„èåˆï¼Œå®ç°åŠ¨æ€é€‚åº”æœªè§åŠ¨ä½œè€Œæ— éœ€é¢å¤–è®­ç»ƒæˆ–è®¿é—®è®­ç»ƒæ•°æ®ã€‚

**Result:** åœ¨NTU RGB+D 60/120å’ŒPKU-MMD IIæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSkeleton-Cacheåœ¨é›¶æ ·æœ¬å’Œå¹¿ä¹‰é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œèƒ½å¤ŸæŒç»­æå‡å¤šç§SZARéª¨å¹²ç½‘ç»œçš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹æœªè§åŠ¨ä½œçš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

**Conclusion:** Skeleton-Cacheä¸ºåŸºäºéª¨æ¶çš„é›¶æ ·æœ¬åŠ¨ä½œè¯†åˆ«æä¾›äº†é¦–ä¸ªå…è®­ç»ƒçš„æµ‹è¯•æ—¶è‡ªé€‚åº”è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç»“åˆç»“æ„åŒ–éª¨æ¶è¡¨ç¤ºå’ŒLLMå¼•å¯¼çš„è¯­ä¹‰å…ˆéªŒï¼Œå®ç°äº†å¯¹æœªè§åŠ¨ä½œçš„æœ‰æ•ˆé€‚åº”ã€‚è¯¥æ¡†æ¶ä¸ºåŠ¨ä½œè¯†åˆ«é¢†åŸŸçš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µæ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.


### [6] [VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation](https://arxiv.org/abs/2512.11061)
*Felix O'Mahony, Roberto Cipolla, Ayush Tewari*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†VDAWorldæ¡†æ¶ï¼Œé€šè¿‡å°†å›¾åƒ-æ–‡æœ¬å¯¹è’¸é¦ä¸ºå¯å¤„ç†çš„æŠ½è±¡è¡¨ç¤ºå¹¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºæ™ºèƒ½ä»£ç†åè°ƒè§†è§‰å·¥å…·å’Œç‰©ç†æ¨¡æ‹Ÿå™¨ï¼Œæ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡åŠ¨æ€åœºæ™¯æ¨¡æ‹Ÿçš„é€šç”¨ä¸–ç•Œæ¨¡å‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç”Ÿæˆå¼è§†é¢‘æ¨¡å‹ä½œä¸ºä¸–ç•Œå»ºæ¨¡çš„ä¸»è¦æ–¹æ³•å­˜åœ¨æ ¹æœ¬æ€§å±€é™ï¼ŒåŒ…æ‹¬è¿åç‰©ç†å’Œé€»è¾‘è§„åˆ™ã€ç¼ºä¹äº¤äº’æ€§ä»¥åŠä½œä¸ºä¸é€æ˜é»‘ç®±éš¾ä»¥æ„å»ºç»“æ„åŒ–å¯æŸ¥è¯¢ä¸–ç•Œã€‚æœ¬ç ”ç©¶æ—¨åœ¨å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæ¢ç´¢æ–°çš„ä¸–ç•Œå»ºæ¨¡èŒƒå¼ã€‚

**Method:** æå‡ºVDAWorldæ¡†æ¶ï¼Œå°†å›¾åƒ-æ–‡æœ¬å¯¹è’¸é¦ä¸ºé’ˆå¯¹æ¨¡æ‹Ÿä¼˜åŒ–çš„å¯å¤„ç†æŠ½è±¡è¡¨ç¤ºã€‚é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºæ™ºèƒ½ä»£ç†åè°ƒæ•´ä¸ªè¿‡ç¨‹ï¼Œè‡ªä¸»æ„å»ºåŸºäºè§†è§‰å·¥å…·å¥—ä»¶çš„æ¥åœ°ï¼ˆ2Dæˆ–3Dï¼‰åœºæ™¯è¡¨ç¤ºï¼Œå¹¶ç›¸åº”é€‰æ‹©å…¼å®¹çš„ç‰©ç†æ¨¡æ‹Ÿå™¨ï¼ˆå¦‚åˆšä½“ã€æµä½“ï¼‰ä½œç”¨äºåœºæ™¯ï¼Œèƒ½å¤Ÿä»é™æ€åœºæ™¯æ¨æ–­æ½œåœ¨åŠ¨æ€ä»¥é¢„æµ‹åˆç†æœªæ¥çŠ¶æ€ã€‚

**Result:** å®éªŒè¡¨æ˜ï¼Œæ™ºèƒ½æŠ½è±¡ä¸è‡ªé€‚åº”æ¨¡æ‹Ÿçš„ç»“åˆäº§ç”Ÿäº†èƒ½å¤Ÿè·¨å¹¿æ³›åŠ¨æ€åœºæ™¯ç”Ÿæˆé«˜è´¨é‡æ¨¡æ‹Ÿçš„é€šç”¨ä¸–ç•Œæ¨¡å‹ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§åŠ¨æ€åœºæ™¯ä¸­å±•ç°å‡ºå“è¶Šçš„æ¨¡æ‹Ÿèƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†æ™ºèƒ½æŠ½è±¡ä¸è‡ªé€‚åº”ç‰©ç†æ¨¡æ‹Ÿç›¸ç»“åˆçš„æ–°èŒƒå¼æ½œåŠ›ï¼Œä¸ºæ„å»ºç»“æ„åŒ–ã€å¯æŸ¥è¯¢ä¸”éµå¾ªç‰©ç†è§„åˆ™çš„ä¸–ç•Œæ¨¡å‹æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œæ¨åŠ¨äº†ä»é»‘ç®±ç”Ÿæˆæ¨¡å‹å‘å¯è§£é‡Šã€å¯äº¤äº’ä¸–ç•Œè¡¨ç¤ºçš„è½¬å˜ã€‚

---

#### ğŸ“„ Abstract
Generative video models, a leading approach to world modeling, face fundamental limitations. They often violate physical and logical rules, lack interactivity, and operate as opaque black boxes ill-suited for building structured, queryable worlds. To overcome these challenges, we propose a new paradigm focused on distilling an image caption pair into a tractable, abstract representation optimized for simulation. We introduce VDAWorld, a framework where a Vision-Language Model (VLM) acts as an intelligent agent to orchestrate this process. The VLM autonomously constructs a grounded (2D or 3D) scene representation by selecting from a suite of vision tools, and accordingly chooses a compatible physics simulator (e.g., rigid body, fluid) to act upon it. VDAWorld can then infer latent dynamics from the static scene to predict plausible future states. Our experiments show that this combination of intelligent abstraction and adaptive simulation results in a versatile world model capable of producing high quality simulations across a wide range of dynamic scenarios.


### [7] [Exploring MLLM-Diffusion Information Transfer with MetaCanvas](https://arxiv.org/abs/2512.11464)
*Han Lin, Xichen Pan, Ziqi Huang, Ji Hou, Jialiang Wang, Weifeng Chen, Zecheng He, Felix Juefei-Xu, Junzhe Sun, Zhipeng Fan, Ali Thabet, Mohit Bansal, Chu Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMetaCanvasæ¡†æ¶ï¼Œé€šè¿‡å°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºæ½œåœ¨ç©ºé—´è§„åˆ’å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç©ºé—´å’Œæ—¶ç©ºæ½œåœ¨ç©ºé—´ä¸­ç›´æ¥è¿›è¡Œæ¨ç†å’Œè§„åˆ’ï¼Œä»è€Œå¼¥åˆå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„èƒ½åŠ›å·®è·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­é€šå¸¸è¢«ç®€åŒ–ä¸ºæ‰©æ•£æ¨¡å‹çš„å…¨å±€æ–‡æœ¬ç¼–ç å™¨ï¼Œå…¶å¼ºå¤§çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›æœªè¢«å……åˆ†åˆ©ç”¨ï¼Œå¯¼è‡´æ¨¡å‹èƒ½å¤Ÿè§£æå¤æ‚å¸ƒå±€å’ŒçŸ¥è¯†å¯†é›†å‹åœºæ™¯ï¼Œå´éš¾ä»¥ç”Ÿæˆå…·æœ‰åŒç­‰ç²¾ç¡®å’Œç»“æ„åŒ–æ§åˆ¶çš„å›¾åƒæˆ–è§†é¢‘ï¼Œå½¢æˆäº†å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„èƒ½åŠ›å·®è·ã€‚

**Method:** æå‡ºMetaCanvasè½»é‡çº§æ¡†æ¶ï¼Œä½¿å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨ç©ºé—´å’Œæ—¶ç©ºæ½œåœ¨ç©ºé—´ä¸­ç›´æ¥è¿›è¡Œæ¨ç†å’Œè§„åˆ’ï¼Œå¹¶ä¸æ‰©æ•£ç”Ÿæˆå™¨ç´§å¯†æ¥å£ï¼›è¯¥æ¡†æ¶åœ¨ä¸‰ç§ä¸åŒçš„æ‰©æ•£æ¨¡å‹éª¨å¹²ä¸Šè¿›è¡Œäº†å®è¯å®ç°ï¼Œæ”¯æŒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ–‡æœ¬/å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€å›¾åƒ/è§†é¢‘ç¼–è¾‘ä»¥åŠä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆç­‰å¤šç§ä»»åŠ¡ã€‚

**Result:** MetaCanvasåœ¨å…­ä¸ªéœ€è¦ç²¾ç¡®å¸ƒå±€ã€é²æ£’å±æ€§ç»‘å®šå’Œæ¨ç†å¯†é›†å‹æ§åˆ¶çš„ä»»åŠ¡ä¸­æŒç»­ä¼˜äºå…¨å±€æ¡ä»¶åŸºå‡†æ–¹æ³•ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ–‡æœ¬/å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€å›¾åƒ/è§†é¢‘ç¼–è¾‘å’Œä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆç­‰ä»»åŠ¡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** å°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è§†ä¸ºæ½œåœ¨ç©ºé—´è§„åˆ’å™¨æ˜¯å¼¥åˆå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¹‹é—´å·®è·çš„æœ‰å‰æ™¯æ–¹å‘ï¼ŒMetaCanvasæ¡†æ¶é€šè¿‡å……åˆ†åˆ©ç”¨æ¨¡å‹çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œä¸ºç²¾ç¡®å’Œç»“æ„åŒ–è§†è§‰ç”Ÿæˆæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.


### [8] [HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning](https://arxiv.org/abs/2512.11534)
*Yiqing Yang, Kin-Man Lam*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ä»»åŠ¡è‡ªé€‚åº”å…³é”®å¸§é€‰æ‹©æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€æŸ¥è¯¢ç”Ÿæˆã€é›†åˆçº§ä¼˜åŒ–å’Œå¸ˆç”Ÿäº’å­¦ä¹ æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘ç†è§£ä¸­çš„å¸§é€‰æ‹©æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ¶ˆé™¤äº†å¯¹é™æ€ä¼ªæ ‡ç­¾çš„ä¾èµ–ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿè§†é¢‘å…³é”®å¸§é€‰æ‹©æ–¹æ³•å­˜åœ¨ä¸¤å¤§å±€é™ï¼šä¸€æ˜¯åŸºäºç‹¬ç«‹è¯„åˆ†çš„top-Ké€‰æ‹©æ–¹æ³•æ— æ³•ä»æ•´ä½“ä¸Šä¼˜åŒ–å¸§é€‰æ‹©ï¼Œå¯¼è‡´æ‰€é€‰å¸§åœ¨æ—¶é—´ä¸Šèšé›†ä¸”è§†è§‰å†—ä½™ï¼›äºŒæ˜¯ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç¦»çº¿ç”Ÿæˆçš„ä¼ªæ ‡ç­¾è®­ç»ƒè½»é‡çº§é€‰æ‹©å™¨ï¼Œä½¿å¾—ç›‘ç£ä¿¡å·æ— æ³•åŠ¨æ€é€‚åº”ä»»åŠ¡ç›®æ ‡ï¼Œé™åˆ¶äº†é€‰æ‹©å™¨çš„æ€§èƒ½æå‡ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ä»»åŠ¡è‡ªé€‚åº”å¸§é€‰æ‹©æ¡†æ¶ã€‚é¦–å…ˆé‡‡ç”¨æ€ç»´é“¾æ–¹æ³•å¼•å¯¼å°å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä»»åŠ¡ç‰¹å®šçš„éšå¼æŸ¥è¯¢å‘é‡ï¼Œä¸å¤šæ¨¡æ€ç‰¹å¾ç»“åˆå®ç°åŠ¨æ€å¸§è¯„åˆ†ã€‚å…¶æ¬¡å®šä¹‰äº†åŒ…å«ç›¸å…³æ€§ã€è¦†ç›–åº¦å’Œå†—ä½™åº¦çš„è¿ç»­é›†åˆçº§ç›®æ ‡å‡½æ•°ï¼Œé€šè¿‡Gumbel-Softmaxå®ç°å¯å¾®åˆ†ä¼˜åŒ–ä»¥é€‰æ‹©æœ€ä¼˜å¸§ç»„åˆã€‚æœ€åé‡‡ç”¨å¸ˆç”Ÿäº’å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡KLæ•£åº¦å¯¹é½å­¦ç”Ÿé€‰æ‹©å™¨ï¼ˆSLMï¼‰å’Œæ•™å¸ˆæ¨ç†å™¨ï¼ˆMLLMï¼‰çš„å¸§é‡è¦æ€§åˆ†å¸ƒï¼Œç»“åˆäº¤å‰ç†µæŸå¤±å®ç°ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œæ¶ˆé™¤å¯¹é™æ€ä¼ªæ ‡ç­¾çš„ä¾èµ–ã€‚

**Result:** åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…·ä½“åœ¨Video-MMEã€LongVideoBenchã€MLVUå’ŒNExT-QAç­‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†ä»»åŠ¡è‡ªé€‚åº”æ¡†æ¶åœ¨å…³é”®å¸§é€‰æ‹©ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ç«¯åˆ°ç«¯ä»»åŠ¡è‡ªé€‚åº”å…³é”®å¸§é€‰æ‹©æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€æŸ¥è¯¢ç”Ÿæˆã€é›†åˆçº§ä¼˜åŒ–å’Œå¸ˆç”Ÿäº’å­¦ä¹ æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•ä¸ä»…æ¶ˆé™¤äº†å¯¹é™æ€ä¼ªæ ‡ç­¾çš„ä¾èµ–ï¼Œè¿˜èƒ½æ ¹æ®å…·ä½“ä»»åŠ¡ç›®æ ‡è‡ªé€‚åº”åœ°é€‰æ‹©æœ€ä¼˜å¸§ç»„åˆï¼Œä¸ºè§†é¢‘ç†è§£ä¸­çš„å¸§é€‰æ‹©é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.


### [9] [Vision-Language Models for Infrared Industrial Sensing in Additive Manufacturing Scene Description](https://arxiv.org/abs/2512.11098)
*Nazanin Mahjourian, Vinh Nguyen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†VLM-IRISæ¡†æ¶ï¼Œé€šè¿‡å°†çº¢å¤–å›¾åƒé¢„å¤„ç†ä¸ºRGBå…¼å®¹è¡¨ç¤ºï¼Œä½¿åŸºäºCLIPçš„è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå®ç°é›¶æ ·æœ¬çº¢å¤–å·¥ä¸šæ„ŸçŸ¥ï¼Œæ— éœ€æ¨¡å‹é‡æ–°è®­ç»ƒå³å¯åœ¨çƒ­æˆåƒåº”ç”¨ä¸­å®ç°é«˜ç²¾åº¦å·¥ä»¶æ£€æµ‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åœ¨ä½å…‰ç…§æˆ–å°é—­æœºå™¨ç¯å¢ƒç­‰åˆ¶é€ åœºæ™¯ä¸­ï¼Œä¼ ç»Ÿè§†è§‰ç³»ç»Ÿéš¾ä»¥æœ‰æ•ˆå·¥ä½œï¼Œè€Œçº¢å¤–ç›¸æœºå…·æœ‰äº’è¡¥ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå½“å‰è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ä»…é’ˆå¯¹RGBæ•°æ®è®­ç»ƒï¼Œæ— æ³•ç†è§£çº¢å¤–ç›¸æœºæ•°æ®ï¼Œä¸”ç›‘ç£AIç³»ç»Ÿéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™ä½¿å¾—é›¶æ ·æœ¬å­¦ä¹ æ¡†æ¶åœ¨çº¢å¤–ç›¸æœºåº”ç”¨ä¸­æ›´å…·å®ç”¨æ€§ã€‚

**Method:** æœ¬ç ”ç©¶æå‡ºäº†VLM-IRISæ¡†æ¶ï¼Œé€šè¿‡é¢„å¤„ç†FLIR Bosonä¼ æ„Ÿå™¨æ•è·çš„çº¢å¤–å›¾åƒï¼Œå°†å…¶è½¬æ¢ä¸ºé€‚åˆCLIPç¼–ç å™¨çš„RGBå…¼å®¹è¾“å…¥ã€‚å…·ä½“æ–¹æ³•åŒ…æ‹¬å°†çº¢å¤–å›¾åƒè½¬æ¢ä¸ºå²©æµ†è¡¨ç¤ºï¼Œå¹¶åº”ç”¨è´¨å¿ƒæç¤ºé›†æˆæŠ€æœ¯ä¸CLIP ViT-B/32ç¼–ç å™¨ç›¸ç»“åˆï¼Œå®ç°åœ¨æ— éœ€æ¨¡å‹é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å¯¹çº¢å¤–å›¾åƒè¿›è¡Œé›¶æ ·æœ¬é¢„æµ‹ã€‚

**Result:** VLM-IRISåœ¨3Dæ‰“å°æœºåºŠçš„å·¥ä»¶å­˜åœ¨æ£€æµ‹ä»»åŠ¡ä¸­å±•ç¤ºäº†é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œè¯¥ä»»åŠ¡åˆ©ç”¨æ„å»ºæ¿ä¸å·¥ä»¶ä¹‹é—´çš„æ¸©åº¦å·®å¼‚ï¼Œéå¸¸é€‚åˆçƒ­æˆåƒåº”ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡æ‰€æå‡ºçš„é¢„å¤„ç†å’Œæç¤ºé›†æˆæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨çº¢å¤–å›¾åƒä¸Šå®ç°é«˜ç²¾åº¦æ£€æµ‹ï¼Œæ— éœ€ä»»ä½•æ¨¡å‹é‡æ–°è®­ç»ƒæˆ–æ ‡æ³¨æ•°æ®ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡é€‚å½“çš„é¢„å¤„ç†æŠ€æœ¯ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ€§å¯ä»¥æ‰©å±•åˆ°çƒ­æˆåƒåº”ç”¨é¢†åŸŸï¼Œä¸ºæ— æ ‡ç­¾ç›‘æ§æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚è¿™ä¸€å‘ç°ä¸ºå·¥ä¸šç¯å¢ƒä¸­çº¢å¤–æ„ŸçŸ¥çš„é›¶æ ·æœ¬å­¦ä¹ å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå±•ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨éRGBæ¨¡æ€æ•°æ®ä¸Šçš„é€‚åº”æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Many manufacturing environments operate in low-light conditions or within enclosed machines where conventional vision systems struggle. Infrared cameras provide complementary advantages in such environments. Simultaneously, supervised AI systems require large labeled datasets, which makes zero-shot learning frameworks more practical for applications including infrared cameras. Recent advances in vision-language foundation models (VLMs) offer a new path in zero-shot predictions from paired image-text representations. However, current VLMs cannot understand infrared camera data since they are trained on RGB data. This work introduces VLM-IRIS (Vision-Language Models for InfraRed Industrial Sensing), a zero-shot framework that adapts VLMs to infrared data by preprocessing infrared images captured by a FLIR Boson sensor into RGB-compatible inputs suitable for CLIP-based encoders. We demonstrate zero-shot workpiece presence detection on a 3D printer bed where temperature differences between the build plate and workpieces make the task well-suited for thermal imaging. VLM-IRIS converts the infrared images to magma representation and applies centroid prompt ensembling with a CLIP ViT-B/32 encoder to achieve high accuracy on infrared images without any model retraining. These findings demonstrate that the proposed improvements to VLMs can be effectively extended to thermal applications for label-free monitoring.


### [10] [DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry](https://arxiv.org/abs/2512.11558)
*Zhenyang Cai, Jiaming Zhang, Junjie Zhao, Ziyi Zeng, Yanchao Li, Jingyi Liang, Junying Chen, Yunjin Yang, Jiajun You, Shuzhi Deng, Tongfei Wang, Wanting Chen, Chunxiu Hao, Ruiqi Xie, Zhenwei Wen, Xiangyi Feng, Zou Ting, Jin Zou Lin, Jianquan Li, Guangjun Yu, Liangyi Chen, Junwen Wang, Shan Jiang, Benyou Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†DentalGPTï¼Œä¸€ç§ä¸“é—¨ç”¨äºç‰™ç§‘é¢†åŸŸçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡é«˜è´¨é‡é¢†åŸŸçŸ¥è¯†æ³¨å…¥å’Œå¼ºåŒ–å­¦ä¹ è§£å†³äº†ç°æœ‰MLLMsåœ¨ç‰™ç§‘è§†è§‰ç»†èŠ‚æ•æ‰å’Œè¯Šæ–­æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰™ç§‘é¢†åŸŸå­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šéš¾ä»¥æ•æ‰ç»†ç²’åº¦çš„ç‰™ç§‘è§†è§‰ç»†èŠ‚ï¼Œä»¥åŠç¼ºä¹è¿›è¡Œç²¾ç¡®è¯Šæ–­æ‰€éœ€çš„å……åˆ†æ¨ç†èƒ½åŠ›ï¼Œè¿™é˜»ç¢äº†è‡ªåŠ¨åŒ–å£è…”åŒ»ç–—ä¿å¥ä¸­å¤šæ¨¡æ€æ•°æ®çš„å¯é è§£é‡Šã€‚

**Method:** ç ”ç©¶é€šè¿‡æ„å»ºè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç‰™ç§‘å¤šæ¨¡æ€æ ‡æ³¨æ•°æ®é›†ï¼ˆåŒ…å«è¶…è¿‡12ä¸‡å¼ ç‰™ç§‘å›¾åƒåŠå…¶è¯¦ç»†æè¿°ï¼‰ï¼Œå¹¶é‡‡ç”¨é«˜è´¨é‡é¢†åŸŸçŸ¥è¯†æ³¨å…¥å’Œå¼ºåŒ–å­¦ä¹ ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥å¼€å‘DentalGPTï¼Œå…¶ä¸­æ•°æ®é›†ç‰¹åˆ«å¼ºè°ƒäº†è¯Šæ–­ç›¸å…³çš„è§†è§‰ç‰¹å¾ã€‚

**Result:** DentalGPTåœ¨å£è…”å†…å’Œå…¨æ™¯Xå…‰ç‰‡åŸºå‡†æµ‹è¯•ä»¥åŠåŒ»å­¦VQAåŸºå‡†çš„ç‰™ç§‘å­é›†ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨ç–¾ç—…åˆ†ç±»å’Œç‰™ç§‘è§†è§‰é—®ç­”ä»»åŠ¡ä¸­è¶…è¶Šäº†è®¸å¤šæœ€å…ˆè¿›çš„MLLMsï¼Œå°½ç®¡æ¨¡å‹ä»…æœ‰70äº¿å‚æ•°ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œé«˜è´¨é‡ç‰™ç§‘æ•°æ®ä¸åˆ†é˜¶æ®µé€‚åº”ç­–ç•¥ç›¸ç»“åˆï¼Œä¸ºæ„å»ºèƒ½åŠ›å¼ºä¸”é¢†åŸŸä¸“ä¸šåŒ–çš„ç‰™ç§‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œè¯æ˜äº†é¢†åŸŸä¸“ä¸šåŒ–æ¨¡å‹åœ¨åŒ»ç–—åº”ç”¨ä¸­çš„ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.


### [11] [VGent: Visual Grounding via Modular Design for Disentangling Reasoning and Prediction](https://arxiv.org/abs/2512.11099)
*Weitai Kang, Jason Kuen, Mengwei Ren, Zijun Wei, Yan Yan, Kangning Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºVGentï¼Œä¸€ç§æ¨¡å—åŒ–çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œé€šè¿‡è§£è€¦é«˜å±‚æ¨ç†ä¸ä½å±‚è¾¹ç•Œæ¡†é¢„æµ‹æ¥è§£å†³è§†è§‰å®šä½ä¸­çš„è‡ªå›å½’è§£ç é€Ÿåº¦æ…¢ã€å¹»è§‰é£é™©ä»¥åŠé‡æ–°å¯¹é½LLMæŸå®³é¢„è®­ç»ƒæ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ–°çš„æœ€ä¼˜æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰å®šä½æ¨¡å‹å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’è§£ç é€Ÿåº¦æ…¢ä¸”å­˜åœ¨å¹»è§‰é£é™©ï¼Œè€Œé€šè¿‡é‡æ–°å¯¹é½LLMä¸è§†è§‰ç‰¹å¾å­¦ä¹ æ–°ç‰¹æ®Šæ ‡è®°çš„æ–¹æ³•å¯èƒ½æŸå®³LLMçš„é¢„è®­ç»ƒæ¨ç†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™äº›é™åˆ¶ï¼Œæå‡ºä¸€ç§æ—¢èƒ½ä¿æŒå¼ºå¤§æ¨ç†èƒ½åŠ›åˆèƒ½å®ç°å¿«é€Ÿå‡†ç¡®è¾¹ç•Œæ¡†é¢„æµ‹çš„è§£å†³æ–¹æ¡ˆã€‚

**Method:** VGenté‡‡ç”¨æ¨¡å—åŒ–ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œä½¿ç”¨å†»ç»“çš„MLLMä½œä¸ºç¼–ç å™¨ä»¥ä¿æŒå…¶å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œè§£ç å™¨åˆ™æ¥æ”¶æ£€æµ‹å™¨æå‡ºçš„é«˜è´¨é‡è¾¹ç•Œæ¡†ä½œä¸ºæŸ¥è¯¢ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶åœ¨ç¼–ç å™¨éšè—çŠ¶æ€ä¸Šé€‰æ‹©ç›®æ ‡æ¡†ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†QuadThinkerï¼ˆåŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒèŒƒå¼å¢å¼ºç¼–ç å™¨çš„å¤šç›®æ ‡æ¨ç†èƒ½åŠ›ï¼‰ã€æ©ç æ„ŸçŸ¥æ ‡ç­¾è§£å†³æ£€æµ‹-åˆ†å‰²æ­§ä¹‰ï¼Œä»¥åŠå…¨å±€ç›®æ ‡è¯†åˆ«æ”¹è¿›æ‰€æœ‰ç›®æ ‡çš„è¯†åˆ«èƒ½åŠ›ã€‚

**Result:** åœ¨å¤šç›®æ ‡è§†è§‰å®šä½åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVGentç›¸æ¯”å…ˆå‰æ–¹æ³•å®ç°äº†+20.6%çš„F1åˆ†æ•°æå‡ï¼Œåœ¨è§†è§‰å‚è€ƒæŒ‘æˆ˜ä¸‹è¿›ä¸€æ­¥å°†gIoUæå‡+8.2%ã€cIoUæå‡+5.8%ï¼ŒåŒæ—¶ä¿æŒäº†æ’å®šä¸”å¿«é€Ÿçš„æ¨ç†å»¶è¿Ÿï¼Œè¾¾åˆ°äº†æ–°çš„æœ€ä¼˜æ€§èƒ½æ°´å¹³ã€‚

**Conclusion:** VGenté€šè¿‡è§£è€¦é«˜å±‚æ¨ç†ä¸ä½å±‚è¾¹ç•Œæ¡†é¢„æµ‹ï¼Œå……åˆ†åˆ©ç”¨äº†ç›®æ ‡æ£€æµ‹å’ŒMLLMçš„æœ€æ–°è¿›å±•ï¼Œé¿å…äº†è‡ªå›å½’è§£ç çš„ç¼ºé™·ï¼Œæ”¯æŒç¼–ç å™¨å’Œè§£ç å™¨çš„æ¨¡å—åŒ–å‡çº§ï¼Œä¸ºè§†è§‰å®šä½ä»»åŠ¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å’Œå¿«é€Ÿæ¨ç†é€Ÿåº¦ã€‚

---

#### ğŸ“„ Abstract
Current visual grounding models are either based on a Multimodal Large Language Model (MLLM) that performs auto-regressive decoding, which is slow and risks hallucinations, or on re-aligning an LLM with vision features to learn new special or object tokens for grounding, which may undermine the LLM's pretrained reasoning ability. In contrast, we propose VGent, a modular encoder-decoder architecture that explicitly disentangles high-level reasoning and low-level bounding box prediction. Specifically, a frozen MLLM serves as the encoder to provide untouched powerful reasoning capabilities, while a decoder takes high-quality boxes proposed by detectors as queries and selects target box(es) via cross-attending on encoder's hidden states. This design fully leverages advances in both object detection and MLLM, avoids the pitfalls of auto-regressive decoding, and enables fast inference. Moreover, it supports modular upgrades of both the encoder and decoder to benefit the whole system: we introduce (i) QuadThinker, an RL-based training paradigm for enhancing multi-target reasoning ability of the encoder; (ii) mask-aware label for resolving detection-segmentation ambiguity; and (iii) global target recognition to improve the recognition of all the targets which benefits the selection among augmented proposals. Experiments on multi-target visual grounding benchmarks show that VGent achieves a new state-of-the-art with +20.6% F1 improvement over prior methods, and further boosts gIoU by +8.2% and cIoU by +5.8% under visual reference challenges, while maintaining constant, fast inference latency.


### [12] [Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching](https://arxiv.org/abs/2512.11130)
*Bowen Wen, Shaurya Dewan, Stan Birchfield*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Fast-FoundationStereoï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿåœ¨ä¿æŒå¼ºå¤§é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶å®ç°å®æ—¶å¸§ç‡çš„ç«‹ä½“è§†è§‰åŸºç¡€æ¨¡å‹å®¶æ—ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦ã€ç¥ç»æ¶æ„æœç´¢å’Œç»“æ„åŒ–å‰ªæç­‰åŠ é€Ÿç­–ç•¥ï¼Œåœ¨é€Ÿåº¦ä¸Šæ¯”FoundationStereoå¿«10å€ä»¥ä¸Šã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç«‹ä½“è§†è§‰åŸºç¡€æ¨¡å‹è™½ç„¶å…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä½†è®¡ç®—æˆæœ¬è¿‡é«˜æ— æ³•æ»¡è¶³å®æ—¶åº”ç”¨éœ€æ±‚ï¼›è€Œé«˜æ•ˆçš„ç«‹ä½“è§†è§‰æ¶æ„åˆ™ä¸ºäº†é€Ÿåº¦ç‰ºç‰²äº†é²æ£’æ€§ï¼Œå¹¶ä¸”éœ€è¦æ˜‚è´µçš„é€é¢†åŸŸå¾®è°ƒã€‚æœ¬æ–‡æ—¨åœ¨å¼¥åˆè¿™ä¸€å·®è·ï¼Œå¼€å‘èƒ½å¤Ÿåœ¨ä¿æŒé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶å®ç°å®æ—¶æ€§èƒ½çš„ç«‹ä½“è§†è§‰æ¨¡å‹ã€‚

**Method:** æœ¬æ–‡é‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„åŠ é€Ÿç­–ç•¥ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä½¿ç”¨çŸ¥è¯†è’¸é¦å°†æ··åˆéª¨å¹²ç½‘ç»œå‹ç¼©ä¸ºå•ä¸ªé«˜æ•ˆå­¦ç”Ÿæ¨¡å‹ï¼›é‡‡ç”¨åˆ†å—ç¥ç»æ¶æ„æœç´¢è‡ªåŠ¨å‘ç°å»¶è¿Ÿé¢„ç®—ä¸‹çš„æœ€ä¼˜æˆæœ¬æ»¤æ³¢è®¾è®¡ï¼Œå°†æœç´¢å¤æ‚åº¦æŒ‡æ•°çº§é™ä½ï¼›ä»¥åŠé€šè¿‡ç»“æ„åŒ–å‰ªææ¶ˆé™¤è¿­ä»£ç»†åŒ–æ¨¡å—ä¸­çš„å†—ä½™ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è‡ªåŠ¨ä¼ªæ ‡æ³¨æµç¨‹ï¼Œæ”¶é›†äº†140ä¸‡å¼ çœŸå®ä¸–ç•Œç«‹ä½“å›¾åƒå¯¹æ¥è¡¥å……åˆæˆè®­ç»ƒæ•°æ®å¹¶ä¿ƒè¿›çŸ¥è¯†è’¸é¦ã€‚

**Result:** Fast-FoundationStereoæ¨¡å‹èƒ½å¤Ÿä»¥è¶…è¿‡10å€çš„é€Ÿåº¦è¿è¡ŒäºFoundationStereoï¼ŒåŒæ—¶ç´§å¯†åŒ¹é…å…¶é›¶æ ·æœ¬ç²¾åº¦ï¼Œä»è€Œåœ¨å®æ—¶æ–¹æ³•ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚è¯¥æ¨¡å‹é¦–æ¬¡å®ç°äº†åœ¨å®æ—¶å¸§ç‡ä¸‹çš„å¼ºå¤§é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨é€Ÿåº¦ä¸æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚

**Conclusion:** æœ¬ç ”ç©¶è¯æ˜äº†é€šè¿‡ç³»ç»ŸåŒ–çš„åŠ é€Ÿç­–ç•¥ï¼Œå¯ä»¥åœ¨ä¸ç‰ºç‰²é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›çš„å‰æä¸‹å®ç°ç«‹ä½“è§†è§‰åŸºç¡€æ¨¡å‹çš„å®æ—¶æ€§èƒ½ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„é«˜æ•ˆç«‹ä½“è§†è§‰ç³»ç»Ÿæä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä¸ºå…¶ä»–è®¡ç®—å¯†é›†å‹åŸºç¡€æ¨¡å‹çš„åŠ é€Ÿæä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒæ¡†æ¶ï¼Œå±•ç¤ºäº†çŸ¥è¯†è’¸é¦ã€ç¥ç»æ¶æ„æœç´¢å’Œç»“æ„åŒ–å‰ªæåœ¨æ¨¡å‹ä¼˜åŒ–ä¸­çš„ååŒä½œç”¨ã€‚

---

#### ğŸ“„ Abstract
Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications. Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly per-domain fine-tuning. To bridge this gap, we present Fast-FoundationStereo, a family of architectures that achieve, for the first time, strong zero-shot generalization at real-time frame rate. We employ a divide-and-conquer acceleration strategy with three components: (1) knowledge distillation to compress the hybrid backbone into a single efficient student; (2) blockwise neural architecture search for automatically discovering optimal cost filtering designs under latency budgets, reducing search complexity exponentially; and (3) structured pruning for eliminating redundancy in the iterative refinement module. Furthermore, we introduce an automatic pseudo-labeling pipeline used to curate 1.4M in-the-wild stereo pairs to supplement synthetic training data and facilitate knowledge distillation. The resulting model can run over 10x faster than FoundationStereo while closely matching its zero-shot accuracy, thus establishing a new state-of-the-art among real-time methods. Project page: https://nvlabs.github.io/Fast-FoundationStereo/


### [13] [Learning complete and explainable visual representations from itemized text supervision](https://arxiv.org/abs/2512.11141)
*Yiwei Lyu, Chenhui Zhao, Soumyanil Banerjee, Shixuan Liu, Akshay Rao, Akhil Kondepudi, Honglak Lee, Todd C. Hollon*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ItemizedCLIPæ¡†æ¶ï¼Œç”¨äºä»é¡¹ç›®åŒ–æ–‡æœ¬ç›‘ç£ä¸­å­¦ä¹ å®Œæ•´ä¸”å¯è§£é‡Šçš„è§†è§‰è¡¨ç¤ºï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æ¨¡å—å’Œå®šåˆ¶åŒ–ç›®æ ‡å‡½æ•°ï¼Œåœ¨åŒ»å­¦å½±åƒå’Œé¥æ„Ÿç­‰å¤šä¸ªé¢†åŸŸå®ç°äº†æ˜¾è‘—çš„é›¶æ ·æœ¬æ€§èƒ½æå‡å’Œç»†ç²’åº¦å¯è§£é‡Šæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è®¸å¤šè§†è§‰é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åŒ»å­¦å½±åƒå’Œé¥æ„Ÿç­‰éä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„é¢†åŸŸï¼ŒåŒ…å«é¡¹ç›®åŒ–æ–‡æœ¬æ ‡æ³¨ï¼šå•ä¸ªå›¾åƒä¸­æœ‰å¤šä¸ªæ–‡æœ¬é¡¹ç›®æè¿°ç‹¬ç«‹ä¸”è¯­ä¹‰ä¸åŒçš„å‘ç°ã€‚è¿™ç§ç›‘ç£ä¸æ ‡å‡†çš„å¤šæ ‡é¢˜ç›‘ç£ä¸åŒï¼Œåè€…é€šå¸¸æ˜¯å†—ä½™æˆ–é«˜åº¦é‡å çš„ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†è¿™ç§é¡¹ç›®åŒ–ç›‘ç£ç»“æ„ã€‚

**Method:** ItemizedCLIPé‡‡ç”¨è·¨æ³¨æ„åŠ›æ¨¡å—ç”Ÿæˆæ–‡æœ¬é¡¹ç›®æ¡ä»¶åŒ–çš„è§†è§‰åµŒå…¥ï¼Œå¹¶è®¾è®¡äº†ä¸€å¥—å®šåˆ¶åŒ–ç›®æ ‡å‡½æ•°ï¼Œè”åˆå¼ºåˆ¶æ‰§è¡Œé¡¹ç›®ç‹¬ç«‹æ€§ï¼ˆä¸åŒé¡¹ç›®å¯¹åº”ä¸åŒåŒºåŸŸï¼‰å’Œè¡¨ç¤ºå®Œæ•´æ€§ï¼ˆè¦†ç›–æ‰€æœ‰é¡¹ç›®ï¼‰ã€‚è¯¥æ¡†æ¶ä¸“é—¨é’ˆå¯¹é¡¹ç›®åŒ–æ–‡æœ¬ç›‘ç£çš„ç»“æ„ç‰¹ç‚¹è¿›è¡Œä¼˜åŒ–ã€‚

**Result:** åœ¨å››ä¸ªå…·æœ‰è‡ªç„¶é¡¹ç›®åŒ–æ–‡æœ¬ç›‘ç£çš„é¢†åŸŸï¼ˆè„‘éƒ¨MRIã€å¤´éƒ¨CTã€èƒ¸éƒ¨CTã€é¥æ„Ÿï¼‰å’Œä¸€ä¸ªåˆæˆé¡¹ç›®åŒ–æ•°æ®é›†ä¸Šï¼ŒItemizedCLIPåœ¨é›¶æ ·æœ¬æ€§èƒ½å’Œç»†ç²’åº¦å¯è§£é‡Šæ€§æ–¹é¢ç›¸æ¯”åŸºçº¿æ–¹æ³•å®ç°äº†æ˜¾è‘—æå‡ã€‚ç”Ÿæˆçš„è¡¨ç¤ºå…·æœ‰è¯­ä¹‰åŸºç¡€ã€é¡¹ç›®å¯åŒºåˆ†æ€§ã€å®Œæ•´æ€§å’Œè§†è§‰å¯è§£é‡Šæ€§ã€‚

**Conclusion:** ItemizedCLIPä¸ºå¤„ç†é¡¹ç›®åŒ–æ–‡æœ¬ç›‘ç£æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå­¦ä¹ åˆ°è¯­ä¹‰åŸºç¡€ä¸”å¯è§£é‡Šçš„è§†è§‰è¡¨ç¤ºã€‚è¯¥æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºåŒ»å­¦å½±åƒå’Œé¥æ„Ÿç­‰éœ€è¦ç»†ç²’åº¦è§£é‡Šçš„é¢†åŸŸï¼Œä¸ºè·¨æ¨¡æ€å­¦ä¹ ä¸­çš„ç»“æ„åŒ–ç›‘ç£å¤„ç†å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Training vision models with language supervision enables general and transferable representations. However, many visual domains, especially non-object-centric domains such as medical imaging and remote sensing, contain itemized text annotations: multiple text items describing distinct and semantically independent findings within a single image. Such supervision differs from standard multi-caption supervision, where captions are redundant or highly overlapping. Here, we introduce ItemizedCLIP, a framework for learning complete and explainable visual representations from itemized text supervision. ItemizedCLIP employs a cross-attention module to produce text item-conditioned visual embeddings and a set of tailored objectives that jointly enforce item independence (distinct regions for distinct items) and representation completeness (coverage of all items). Across four domains with naturally itemized text supervision (brain MRI, head CT, chest CT, remote sensing) and one additional synthetically itemized dataset, ItemizedCLIP achieves substantial improvements in zero-shot performance and fine-grained interpretability over baselines. The resulting ItemizedCLIP representations are semantically grounded, item-differentiable, complete, and visually interpretable. Our code is available at https://github.com/MLNeurosurg/ItemizedCLIP.


### [14] [Multi-task Learning with Extended Temporal Shift Module for Temporal Action Localization](https://arxiv.org/abs/2512.11189)
*Anh-Kiet Duong, Petra Gomez-KrÃ¤mer*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é’ˆå¯¹BinEgo-360æŒ‘æˆ˜èµ›çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ‰©å±•æ—¶åºç§»ä½æ¨¡å—(TSM)å¤„ç†æ—¶åºåŠ¨ä½œå®šä½ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ å’Œé›†æˆç­–ç•¥ï¼Œåœ¨æ¯”èµ›ä¸­å–å¾—äº†ç¬¬ä¸€åæˆç»©ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤šè§†è§’å¤šæ¨¡æ€è§†é¢‘ä¸­çš„æ—¶åºåŠ¨ä½œå®šä½é—®é¢˜ï¼Œå…·ä½“é’ˆå¯¹BinEgo-360æŒ‘æˆ˜èµ›æä¾›çš„åŒ…å«å…¨æ™¯ã€ç¬¬ä¸‰äººç§°å’Œç¬¬ä¸€äººç§°è§†è§’çš„å¤æ‚è§†é¢‘æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ ‡æ³¨äº†ç»†ç²’åº¦åŠ¨ä½œç±»åˆ«ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è¿™ç§å¤šæ¨¡æ€æ—¶åºæ•°æ®çš„å®šä½æ–¹æ³•ã€‚

**Method:** æ–¹æ³•åŸºäºæ—¶åºç§»ä½æ¨¡å—(TSM)è¿›è¡Œæ‰©å±•ï¼Œé€šè¿‡å¼•å…¥èƒŒæ™¯ç±»åˆ«å’Œå¯¹å›ºå®šé•¿åº¦éé‡å åŒºé—´è¿›è¡Œåˆ†ç±»æ¥å¤„ç†æ—¶åºåŠ¨ä½œå®šä½ä»»åŠ¡ï¼›é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶è”åˆä¼˜åŒ–åœºæ™¯åˆ†ç±»å’Œæ—¶åºåŠ¨ä½œå®šä½ï¼Œåˆ©ç”¨åŠ¨ä½œä¸ç¯å¢ƒä¹‹é—´çš„ä¸Šä¸‹æ–‡çº¿ç´¢ï¼›æœ€åé€šè¿‡åŠ æƒé›†æˆç­–ç•¥æ•´åˆå¤šä¸ªæ¨¡å‹ï¼Œæå‡é¢„æµ‹çš„é²æ£’æ€§å’Œä¸€è‡´æ€§ã€‚

**Result:** è¯¥æ–¹æ³•åœ¨ICCV 2025 BinEgo-360æŒ‘æˆ˜èµ›çš„åˆå§‹è½®å’Œæ‰©å±•è½®ä¸­å‡æ’åç¬¬ä¸€ï¼Œè¯æ˜äº†å¤šä»»åŠ¡å­¦ä¹ ã€é«˜æ•ˆéª¨å¹²ç½‘ç»œå’Œé›†æˆå­¦ä¹ ç›¸ç»“åˆåœ¨æ—¶åºåŠ¨ä½œå®šä½ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šè§†è§’å¤šæ¨¡æ€è§†é¢‘æ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œå°†å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ã€é«˜æ•ˆçš„æ—¶åºå»ºæ¨¡éª¨å¹²ç½‘ç»œå’Œé›†æˆç­–ç•¥ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³å¤æ‚å¤šè§†è§’è§†é¢‘ä¸­çš„æ—¶åºåŠ¨ä½œå®šä½é—®é¢˜ï¼›è¯¥æ–¹æ³•ä¸ºå¤„ç†å¤šæ¨¡æ€æ—¶åºæ•°æ®æä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ï¼Œå±•ç¤ºäº†ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ©ç”¨å’Œæ¨¡å‹é›†æˆåœ¨æå‡å®šä½æ€§èƒ½æ–¹é¢çš„é‡è¦ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
We present our solution to the BinEgo-360 Challenge at ICCV 2025, which focuses on temporal action localization (TAL) in multi-perspective and multi-modal video settings. The challenge provides a dataset containing panoramic, third-person, and egocentric recordings, annotated with fine-grained action classes. Our approach is built on the Temporal Shift Module (TSM), which we extend to handle TAL by introducing a background class and classifying fixed-length non-overlapping intervals. We employ a multi-task learning framework that jointly optimizes for scene classification and TAL, leveraging contextual cues between actions and environments. Finally, we integrate multiple models through a weighted ensemble strategy, which improves robustness and consistency of predictions. Our method is ranked first in both the initial and extended rounds of the competition, demonstrating the effectiveness of combining multi-task learning, an efficient backbone, and ensemble learning for TAL.


### [15] [AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path](https://arxiv.org/abs/2512.11203)
*Zhengyang Yu, Akio Hayakawa, Masato Ishii, Qingtao Yu, Takashi Shibuya, Jing Zhang, Yuki Mitsufuji*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºAutoRefinerï¼Œä¸€ç§ä¸“ä¸ºè‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹è®¾è®¡çš„å™ªå£°ä¼˜åŒ–å™¨ï¼Œé€šè¿‡è·¯å¾„å¼å™ªå£°ä¼˜åŒ–å’Œåå°„KVç¼“å­˜æœºåˆ¶ï¼Œåœ¨ä¸æ›´æ–°æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡ç”Ÿæˆæ ·æœ¬çš„ä¿çœŸåº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºåŒå‘è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å¯æ‰©å±•æ›¿ä»£æ–¹æ¡ˆå±•ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶æ ·æœ¬ä¿çœŸåº¦ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚ç°æœ‰åŸºäºä¼˜åŒ–çš„æ¨ç†æ—¶å¯¹é½æ–¹æ³•è®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œè€Œæ–‡æœ¬åˆ°å›¾åƒé¢†åŸŸçš„å™ªå£°ä¼˜åŒ–å™¨æ— æ³•ç›´æ¥åº”ç”¨äºè§†é¢‘æ¨¡å‹ï¼Œéœ€è¦ä¸“é—¨é’ˆå¯¹è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹è®¾è®¡é«˜æ•ˆçš„å™ªå£°ä¼˜åŒ–æ–¹æ¡ˆã€‚

**Method:** æœ¬æ–‡æå‡ºAutoRefinerå™ªå£°ä¼˜åŒ–å™¨ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®è®¾è®¡ï¼šè·¯å¾„å¼å™ªå£°ä¼˜åŒ–æ²¿ç€éšæœºå»å™ªè·¯å¾„ä¼˜åŒ–å™ªå£°ï¼Œä»¥åŠåå°„KVç¼“å­˜æœºåˆ¶æœ‰æ•ˆç®¡ç†è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­çš„é”®å€¼ç¼“å­˜ã€‚è¯¥æ–¹æ³•ä½œä¸ºå³æ’å³ç”¨æ¨¡å—ï¼Œé€šè¿‡å•æ¬¡å‰å‘ä¼ æ’­è°ƒåˆ¶é‡‡æ ·å™ªå£°ï¼Œé¿å…å‚æ•°æ›´æ–°ã€‚

**Result:** å®éªŒè¡¨æ˜AutoRefinerèƒ½æœ‰æ•ˆæå‡è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ ·æœ¬ä¿çœŸåº¦ï¼Œç›¸æ¯”æœ´ç´ æ‰©å±•çš„æ–‡æœ¬åˆ°å›¾åƒå™ªå£°ä¼˜åŒ–å™¨è¡¨ç°æ›´ä¼˜ã€‚è¯¥æ–¹æ³•è®¡ç®—é«˜æ•ˆï¼Œå¯ä½œä¸ºç°æœ‰æ¨¡å‹çš„å¢å¼ºæ¨¡å—ï¼Œåœ¨ä¿æŒå®æ—¶å’Œäº¤äº’åº”ç”¨ç‰¹æ€§çš„åŒæ—¶æ”¹å–„ç”Ÿæˆè´¨é‡ã€‚

**Conclusion:** ç ”ç©¶è¯å®äº†ä¸ºè‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹è®¾è®¡ä¸“ç”¨å™ªå£°ä¼˜åŒ–å™¨çš„å¿…è¦æ€§ï¼ŒAutoRefineré€šè¿‡è·¯å¾„å¼ä¼˜åŒ–å’Œç¼“å­˜ç®¡ç†è§£å†³äº†è§†é¢‘ç”Ÿæˆä¸­çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ä¸ºæå‡è§†é¢‘ç”Ÿæˆè´¨é‡æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒäº†è‡ªå›å½’æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œå®æ—¶æ€§ä¼˜åŠ¿ã€‚

---

#### ğŸ“„ Abstract
Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.


### [16] [SmokeBench: Evaluating Multimodal Large Language Models for Wildfire Smoke Detection](https://arxiv.org/abs/2512.11215)
*Tianye Qi, Weihao Li, Nick Barnes*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†SmokeBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é‡ç«çƒŸé›¾è¯†åˆ«ä¸å®šä½æ–¹é¢çš„èƒ½åŠ›ï¼Œå‘ç°ç°æœ‰æ¨¡å‹åœ¨æ—©æœŸçƒŸé›¾å®šä½æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é‡ç«çƒŸé›¾å…·æœ‰é€æ˜ã€æ— å®šå½¢ä¸”å¸¸ä¸äº‘å±‚æ··æ·†çš„è§†è§‰ç‰¹æ€§ï¼Œä½¿å¾—æ—©æœŸæ£€æµ‹å°¤ä¸ºå›°éš¾ï¼Œå½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨å…³é”®çš„é‡ç«ç›‘æµ‹åº”ç”¨ä¸­çš„èƒ½åŠ›å°šæœªå¾—åˆ°ç³»ç»Ÿè¯„ä¼°ã€‚

**Method:** ç ”ç©¶æå‡ºäº†SmokeBenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«çƒŸé›¾åˆ†ç±»ã€åŸºäºç“¦ç‰‡çš„çƒŸé›¾å®šä½ã€åŸºäºç½‘æ ¼çš„çƒŸé›¾å®šä½å’ŒçƒŸé›¾æ£€æµ‹å››é¡¹ä»»åŠ¡ï¼Œå¹¶è¯„ä¼°äº†Idefics2ã€Qwen2.5-VLã€InternVL3ã€Unified-IO 2ã€Grounding DINOã€GPT-4oå’ŒGemini-2.5 Proç­‰å¤šä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶éƒ¨åˆ†æ¨¡å‹èƒ½å¤Ÿåœ¨çƒŸé›¾è¦†ç›–å¤§é¢ç§¯æ—¶è¿›è¡Œåˆ†ç±»ï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨ç²¾ç¡®å®šä½æ–¹é¢å‡è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ—©æœŸé˜¶æ®µï¼›çƒŸé›¾ä½“ç§¯ä¸æ¨¡å‹æ€§èƒ½å¼ºç›¸å…³ï¼Œè€Œå¯¹æ¯”åº¦çš„å½±å“ç›¸å¯¹è¾ƒå°ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨å…³é”®é‡ç«ç›‘æµ‹ä¸­çš„å…³é”®å±€é™æ€§ï¼Œå¼ºè°ƒäº†éœ€è¦å¼€å‘æ”¹è¿›æ—©æœŸçƒŸé›¾å®šä½èƒ½åŠ›çš„æ–°æ–¹æ³•ï¼Œä¸ºæœªæ¥æ¨¡å‹ä¼˜åŒ–æä¾›äº†é‡è¦æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Wildfire smoke is transparent, amorphous, and often visually confounded with clouds, making early-stage detection particularly challenging. In this work, we introduce a benchmark, called SmokeBench, to evaluate the ability of multimodal large language models (MLLMs) to recognize and localize wildfire smoke in images. The benchmark consists of four tasks: (1) smoke classification, (2) tile-based smoke localization, (3) grid-based smoke localization, and (4) smoke detection. We evaluate several MLLMs, including Idefics2, Qwen2.5-VL, InternVL3, Unified-IO 2, Grounding DINO, GPT-4o, and Gemini-2.5 Pro. Our results show that while some models can classify the presence of smoke when it covers a large area, all models struggle with accurate localization, especially in the early stages. Further analysis reveals that smoke volume is strongly correlated with model performance, whereas contrast plays a comparatively minor role. These findings highlight critical limitations of current MLLMs for safety-critical wildfire monitoring and underscore the need for methods that improve early-stage smoke localization.


### [17] [RoomPilot: Controllable Synthesis of Interactive Indoor Environments via Multimodal Semantic Parsing](https://arxiv.org/abs/2512.11234)
*Wentang Chen, Shougao Zhang, Yiman Zhang, Tianhao Zhou, Ruihui Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†RoomPilotï¼Œä¸€ä¸ªç”¨äºå¯æ§å®¤å†…åœºæ™¯ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†æ–‡æœ¬æè¿°æˆ–CADå¹³é¢å›¾ç­‰å¤šæ¨¡æ€è¾“å…¥è§£æä¸ºå®¤å†…é¢†åŸŸç‰¹å®šè¯­è¨€ï¼Œä»è€Œç”Ÿæˆå…·æœ‰äº¤äº’è¯­ä¹‰çš„ç»“æ„åŒ–åœºæ™¯ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å®¤å†…åœºæ™¯ç”Ÿæˆæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šä¸€æ˜¯ä»…æ”¯æŒæœ‰é™çš„è¾“å…¥æ¨¡æ€èŒƒå›´ï¼ŒäºŒæ˜¯ä¾èµ–éšæœºè¿‡ç¨‹å¯¼è‡´å¯æ§æ€§ä¸è¶³ã€‚è¿™é™åˆ¶äº†åœ¨æ¸¸æˆå¼€å‘ã€å»ºç­‘å¯è§†åŒ–å’Œå…·èº«AIè®­ç»ƒç­‰åº”ç”¨ä¸­çš„å®ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯æ— æ³•ç”Ÿæˆå…·æœ‰çœŸå®äº¤äº’è¡Œä¸ºçš„åœºæ™¯ã€‚

**Method:** RoomPilotçš„æ ¸å¿ƒæ–¹æ³•æ˜¯å¼•å…¥å®¤å†…é¢†åŸŸç‰¹å®šè¯­è¨€ä½œä¸ºå…±äº«è¯­ä¹‰è¡¨ç¤ºï¼Œå°†æ–‡æœ¬æè¿°æˆ–CADå¹³é¢å›¾ç­‰å¤šæ¨¡æ€è¾“å…¥ç»Ÿä¸€è§£æä¸ºIDSLã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŒ…å«äº¤äº’æ ‡æ³¨çš„èµ„äº§æ•°æ®é›†ï¼Œç”Ÿæˆå…·æœ‰çœŸå®å¯¹è±¡è¡Œä¸ºçš„åœºæ™¯ï¼Œé¿å…äº†ä¼ ç»Ÿç¨‹åºåŒ–æ–¹æ³•äº§ç”Ÿçš„è§†è§‰åˆç†ä½†åŠŸèƒ½åƒµåŒ–çš„å¸ƒå±€é—®é¢˜ã€‚

**Result:** å¤§é‡å®éªŒéªŒè¯äº†RoomPilotåœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œå±•ç¤ºäº†åœ¨åœºæ™¯ç”Ÿæˆä¸­çš„ç»†ç²’åº¦å¯æ§æ€§ï¼Œä»¥åŠåœ¨ç‰©ç†ä¸€è‡´æ€§å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢çš„ä¼˜è¶Šè¡¨ç°ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰äº¤äº’è¯­ä¹‰çš„å®¤å†…åœºæ™¯æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ï¼Œç²¾å¿ƒè®¾è®¡çš„é¢†åŸŸç‰¹å®šè¯­è¨€å¯ä»¥ä½œä¸ºå…±äº«è¯­ä¹‰è¡¨ç¤ºï¼Œå®ç°ä»å•ä¸€æ¨¡æ€åˆ°é«˜è´¨é‡åœºæ™¯çš„è¿è´¯åˆæˆï¼ŒåŒæ—¶ä¿æŒäº¤äº’è¯­ä¹‰ã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€å‘é€šç”¨å¯æ§3Då®¤å†…åœºæ™¯ç”Ÿæˆè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œä¸ºå¤šæ¨¡æ€è¾“å…¥çš„ç»Ÿä¸€å¤„ç†æä¾›äº†æœ‰æ•ˆæ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
Generating controllable and interactive indoor scenes is fundamental to applications in game development, architectural visualization, and embodied AI training. Yet existing approaches either handle a narrow range of input modalities or rely on stochastic processes that hinder controllability. To overcome these limitations, we introduce RoomPilot, a unified framework that parses diverse multi-modal inputs--textual descriptions or CAD floor plans--into an Indoor Domain-Specific Language (IDSL) for indoor structured scene generation. The key insight is that a well-designed IDSL can act as a shared semantic representation, enabling coherent, high-quality scene synthesis from any single modality while maintaining interaction semantics. In contrast to conventional procedural methods that produce visually plausible but functionally inert layouts, RoomPilot leverages a curated dataset of interaction-annotated assets to synthesize environments exhibiting realistic object behaviors. Extensive experiments further validate its strong multi-modal understanding, fine-grained controllability in scene generation, and superior physical consistency and visual fidelity, marking a significant step toward general-purpose controllable 3D indoor scene generation.


### [18] [Cross-modal Prompting for Balanced Incomplete Multi-modal Emotion Recognition](https://arxiv.org/abs/2512.11239)
*Wen-Jue He, Xiaofeng Zhu, Zheng Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€æç¤ºï¼ˆComPï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¢å¼ºæ¨¡æ€ç‰¹å®šç‰¹å¾å’Œæå‡å„æ¨¡æ€æ€§èƒ½æ¥è§£å†³ä¸å®Œå…¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„æ€§èƒ½å·®è·å’Œæ¨¡æ€æ¬ ä¼˜åŒ–é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¸å®Œå…¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«é¢ä¸´å¤šæ¨¡æ€æ•°æ®éƒ¨åˆ†ç¼ºå¤±çš„æŒ‘æˆ˜ï¼Œå…¶ä¸­æ€§èƒ½å·®è·å’Œæ¨¡æ€æ¬ ä¼˜åŒ–é—®é¢˜åœ¨å®è·µä¸­é˜»ç¢äº†æœ‰æ•ˆçš„å¤šæ¨¡æ€å­¦ä¹ ï¼Œå¹¶åœ¨æ•°æ®ç¼ºå¤±æƒ…å†µä¸‹è¿›ä¸€æ­¥åŠ å‰§ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥å¢å¼ºæ¨¡æ€ç‰¹å®šç‰¹å¾å¹¶æå‡æ•´ä½“è¯†åˆ«ç²¾åº¦ã€‚

**Method:** è¯¥æ–¹æ³•è®¾è®¡äº†è·¨æ¨¡æ€æç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬å…·æœ‰åŠ¨æ€æ¢¯åº¦è°ƒåˆ¶å™¨çš„æ¸è¿›æç¤ºç”Ÿæˆæ¨¡å—æ¥äº§ç”Ÿç®€æ´ä¸€è‡´çš„æ¨¡æ€è¯­ä¹‰çº¿ç´¢ï¼Œè·¨æ¨¡æ€çŸ¥è¯†ä¼ æ’­é€šè¿‡ä¼ é€’çš„æç¤ºé€‰æ‹©æ€§åœ°æ”¾å¤§æ¨¡æ€ç‰¹å¾ä¸­çš„ä¸€è‡´ä¿¡æ¯ä»¥å¢å¼ºæ¨¡æ€ç‰¹å®šè¾“å‡ºçš„åŒºåˆ†åº¦ï¼Œä»¥åŠè®¾è®¡åè°ƒå™¨åŠ¨æ€é‡æ–°åŠ æƒæ¨¡æ€è¾“å‡ºä½œä¸ºå¹³è¡¡ç­–ç•¥çš„è¡¥å……ã€‚

**Result:** åœ¨4ä¸ªæ•°æ®é›†ä¸Šå¯¹7ç§æœ€å…ˆè¿›æ–¹æ³•åœ¨ä¸åŒç¼ºå¤±ç‡ä¸‹è¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯¥æ–¹æ³•åœ¨æå‡å„æ¨¡æ€æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æ”¹å–„äº†ä¸å®Œå…¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«çš„æ•´ä½“å‡†ç¡®ç‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡è·¨æ¨¡æ€æç¤ºæœºåˆ¶æœ‰æ•ˆè§£å†³äº†ä¸å®Œå…¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œæå‡ºçš„æ¸è¿›æç¤ºç”Ÿæˆå’ŒåŠ¨æ€åè°ƒç­–ç•¥ä¸ºå¤„ç†å¤šæ¨¡æ€æ•°æ®ç¼ºå¤±å’Œæ€§èƒ½ä¸å¹³è¡¡é—®é¢˜æä¾›äº†æ–°çš„æŠ€æœ¯é€”å¾„ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Incomplete multi-modal emotion recognition (IMER) aims at understanding human intentions and sentiments by comprehensively exploring the partially observed multi-source data. Although the multi-modal data is expected to provide more abundant information, the performance gap and modality under-optimization problem hinder effective multi-modal learning in practice, and are exacerbated in the confrontation of the missing data. To address this issue, we devise a novel Cross-modal Prompting (ComP) method, which emphasizes coherent information by enhancing modality-specific features and improves the overall recognition accuracy by boosting each modality's performance. Specifically, a progressive prompt generation module with a dynamic gradient modulator is proposed to produce concise and consistent modality semantic cues. Meanwhile, cross-modal knowledge propagation selectively amplifies the consistent information in modality features with the delivered prompts to enhance the discrimination of the modality-specific output. Additionally, a coordinator is designed to dynamically re-weight the modality outputs as a complement to the balance strategy to improve the model's efficacy. Extensive experiments on 4 datasets with 7 SOTA methods under different missing rates validate the effectiveness of our proposed method.


### [19] [KeyframeFace: From Text to Expressive Facial Keyframes](https://arxiv.org/abs/2512.11321)
*Jingchao Wu, Zejian Kang, Haibo Liu, Yuanchen Fei, Xiangru Huang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†KeyframeFaceæ•°æ®é›†å’ŒåŸºäºLLMçš„æ–‡æœ¬åˆ°åŠ¨ç”»æ¡†æ¶ï¼Œé€šè¿‡å…³é”®å¸§çº§ç›‘ç£å’ŒLLMå…ˆéªŒçŸ¥è¯†ï¼Œè§£å†³äº†ä»è‡ªç„¶è¯­è¨€ç”ŸæˆåŠ¨æ€3Dé¢éƒ¨åŠ¨ç”»æ—¶ç¼ºä¹è¯­ä¹‰åŸºç¡€å’Œæ—¶åºç»“æ„çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦å…³æ³¨è¯­éŸ³é©±åŠ¨åŠ¨ç”»æˆ–æ— ç»“æ„è¡¨æƒ…åºåˆ—ï¼Œç¼ºä¹è¯­ä¹‰åŸºç¡€å’Œæ—¶åºç»“æ„ï¼Œéš¾ä»¥ç”Ÿæˆå…·æœ‰è¡¨ç°åŠ›çš„äººç±»è¡¨æ¼”åŠ¨ç”»ï¼Œå› æ­¤éœ€è¦è§£å†³ä»è‡ªç„¶è¯­è¨€ç”ŸæˆåŠ¨æ€3Dé¢éƒ¨åŠ¨ç”»æ—¶ç†è§£æ—¶åºè¯­ä¹‰å’Œç»†ç²’åº¦è¡¨æƒ…å˜åŒ–çš„é—®é¢˜ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†KeyframeFaceå¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«2100ä¸ªè¡¨ç°åŠ›è„šæœ¬ã€å•ç›®è§†é¢‘ã€é€å¸§ARKitç³»æ•°ã€ä¸Šä¸‹æ–‡èƒŒæ™¯ã€å¤æ‚æƒ…æ„Ÿå’Œæ‰‹åŠ¨å®šä¹‰å…³é”®å¸§ï¼Œå¹¶é€šè¿‡LLMå’ŒMLLMè¿›è¡Œå¤šè§†è§’æ ‡æ³¨ï¼›åŒæ—¶æå‡ºäº†é¦–ä¸ªåˆ©ç”¨LLMå…ˆéªŒè¿›è¡Œå¯è§£é‡Šé¢éƒ¨è¿åŠ¨åˆæˆçš„æ–‡æœ¬åˆ°åŠ¨ç”»æ¡†æ¶ï¼Œå°†LLMçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ä¸ARKitç³»æ•°çš„å¯è§£é‡Šç»“æ„å¯¹é½ã€‚

**Result:** KeyframeFaceæ•°æ®é›†æä¾›äº†å…¨é¢çš„å¤šæ¨¡æ€æ ‡æ³¨èµ„æºï¼ŒåŸºäºLLMçš„æ¡†æ¶å®ç°äº†é«˜ä¿çœŸè¡¨ç°åŠ›åŠ¨ç”»ç”Ÿæˆï¼Œå…±åŒå»ºç«‹äº†å¯è§£é‡Šã€å…³é”®å¸§å¼•å¯¼å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ–‡æœ¬åˆ°åŠ¨ç”»æ–°åŸºç¡€ï¼Œä»£ç å’Œæ•°æ®å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡æ•°æ®é›†å’Œæ¡†æ¶çš„ç»“åˆï¼Œä¸ºå¯è§£é‡Šçš„é¢éƒ¨åŠ¨ç”»ç”Ÿæˆå»ºç«‹äº†æ–°èŒƒå¼ï¼Œå°†LLMçš„è¯­ä¹‰ç†è§£ä¸å‚æ•°åŒ–é¢éƒ¨æ¨¡å‹çš„ç»“æ„åŒ–è¡¨ç¤ºç›¸ç»“åˆï¼Œä¸ºæœªæ¥æ–‡æœ¬é©±åŠ¨åŠ¨ç”»ç ”ç©¶æä¾›äº†å…³é”®åŸºç¡€è®¾æ–½å’Œæ–¹æ³•è®ºåŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Generating dynamic 3D facial animation from natural language requires understanding both temporally structured semantics and fine-grained expression changes. Existing datasets and methods mainly focus on speech-driven animation or unstructured expression sequences and therefore lack the semantic grounding and temporal structures needed for expressive human performance generation. In this work, we introduce KeyframeFace, a large-scale multimodal dataset designed for text-to-animation research through keyframe-level supervision. KeyframeFace provides 2,100 expressive scripts paired with monocular videos, per-frame ARKit coefficients, contextual backgrounds, complex emotions, manually defined keyframes, and multi-perspective annotations based on ARKit coefficients and images via Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Beyond the dataset, we propose the first text-to-animation framework that explicitly leverages LLM priors for interpretable facial motion synthesis. This design aligns the semantic understanding capabilities of LLMs with the interpretable structure of ARKit's coefficients, enabling high-fidelity expressive animation. KeyframeFace and our LLM-based framework together establish a new foundation for interpretable, keyframe-guided, and context-aware text-to-animation. Code and data are available at https://github.com/wjc12345123/KeyframeFace.


### [20] [UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models](https://arxiv.org/abs/2512.11336)
*Hewen Pan, Cong Wei, Dashuang Liang, Zepeng Huang, Pengfei Gao, Ziqi Zhou, Lulu Xue, Pengfei Yan, Xiaoming Wei, Minghui Li, Shengshan Hu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†UFVideoï¼Œé¦–ä¸ªå…·å¤‡ç»Ÿä¸€å¤šç²’åº¦ååŒç†è§£èƒ½åŠ›çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€å¼•å¯¼å¯¹é½æœºåˆ¶ï¼Œåœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°äº†å…¨å±€ã€åƒç´ å’Œæ—¶é—´å°ºåº¦çš„è§†é¢‘ç†è§£ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸»è¦å±€é™äºç‰¹å®šè§†é¢‘ç†è§£ä»»åŠ¡ï¼Œç¼ºä¹å…¨é¢ä¸”å¤šç²’åº¦çš„è§†é¢‘æ„ŸçŸ¥èƒ½åŠ›ï¼Œæ— æ³•å®ç°è·¨ä¸åŒå°ºåº¦çš„ç»Ÿä¸€è§†é¢‘ç†è§£ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚

**Method:** è®¾è®¡äº†ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€å¼•å¯¼å¯¹é½æœºåˆ¶ï¼Œèƒ½å¤Ÿçµæ´»å¤„ç†å…¨å±€ã€åƒç´ å’Œæ—¶é—´å°ºåº¦çš„è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œæ¨¡å‹åŠ¨æ€ç¼–ç ä¸åŒä»»åŠ¡çš„è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶ç”Ÿæˆæ–‡æœ¬å“åº”ã€æ—¶é—´å®šä½æˆ–åŸºç¡€æ©ç ã€‚

**Result:** æ„å»ºäº†UFVideo-BenchåŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«ä¸‰ä¸ªä¸åŒå°ºåº¦çš„åä½œä»»åŠ¡ï¼Œå®éªŒè¡¨æ˜UFVideoåœ¨çµæ´»æ€§æ–¹é¢ä¼˜äºGPT-4oï¼ŒåŒæ—¶åœ¨9ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†æ¨¡å‹åœ¨å„ç§å¸¸è§è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** UFVideoå±•ç¤ºäº†ç»Ÿä¸€å¤šç²’åº¦è§†é¢‘ç†è§£çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œè¯æ˜äº†å•ä¸€æ¨¡å‹èƒ½å¤ŸåŒæ—¶å¤„ç†ä¸åŒå°ºåº¦çš„è§†é¢‘ç†è§£ä»»åŠ¡ã€‚

---

#### ğŸ“„ Abstract
With the advancement of multi-modal Large Language Models (LLMs), Video LLMs have been further developed to perform on holistic and specialized video understanding. However, existing works are limited to specialized video understanding tasks, failing to achieve a comprehensive and multi-grained video perception. To bridge this gap, we introduce UFVideo, the first Video LLM with unified multi-grained cooperative understanding capabilities. Specifically, we design unified visual-language guided alignment to flexibly handle video understanding across global, pixel and temporal scales within a single model. UFVideo dynamically encodes the visual and text inputs of different tasks and generates the textual response, temporal localization, or grounded mask. Additionally, to evaluate challenging multi-grained video understanding tasks, we construct the UFVideo-Bench consisting of three distinct collaborative tasks within the scales, which demonstrates UFVideo's flexibility and advantages over GPT-4o. Furthermore, we validate the effectiveness of our model across 9 public benchmarks covering various common video understanding tasks, providing valuable insights for future Video LLMs.


### [21] [Task-Specific Distance Correlation Matching for Few-Shot Action Recognition](https://arxiv.org/abs/2512.11340)
*Fei Long, Yao Zhang, Jiaming Lv, Jiangtao Xie, Peihua Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºTS-FSARæ¡†æ¶ï¼Œé€šè¿‡è§†è§‰é˜¶æ¢¯ä¾§ç½‘ç»œå®ç°CLIPçš„é«˜æ•ˆå¾®è°ƒï¼Œå¹¶å¼•å…¥ä»»åŠ¡ç‰¹å®šçš„è·ç¦»ç›¸å…³åŒ¹é…åº¦é‡ï¼Œä»¥è§£å†³å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ä¸­ç°æœ‰æ–¹æ³•åœ¨éçº¿æ€§å…³ç³»å»ºæ¨¡å’Œæœ‰é™æ•°æ®ä¸‹ä¼˜åŒ–å›°éš¾çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®é™åˆ¶ï¼šç°æœ‰é›†åˆåŒ¹é…åº¦é‡é€šå¸¸ä¾èµ–ä½™å¼¦ç›¸ä¼¼åº¦æµ‹é‡å¸§é—´çº¿æ€§ä¾èµ–ï¼Œä»…åˆ©ç”¨å®ä¾‹çº§ä¿¡æ¯è¿›è¡ŒåŒ¹é…ï¼Œæ— æ³•æ•æ‰éçº¿æ€§å…³ç³»ç­‰å¤æ‚æ¨¡å¼ä¸”å¿½è§†ä»»åŠ¡ç‰¹å®šçº¿ç´¢ï¼›åŒæ—¶ï¼Œé€šè¿‡è·³è·ƒèåˆå±‚å¯¹CLIPè¿›è¡Œé«˜æ•ˆå¾®è°ƒçš„æ–¹æ³•åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹æ–°å¼•å…¥çš„ä¾§å±‚éš¾ä»¥ä¼˜åŒ–ã€‚

**Method:** TS-FSARæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè§†è§‰é˜¶æ¢¯ä¾§ç½‘ç»œç”¨äºCLIPçš„é«˜æ•ˆå¾®è°ƒï¼›ä»»åŠ¡ç‰¹å®šè·ç¦»ç›¸å…³åŒ¹é…åº¦é‡ä½¿ç”¨Î±-è·ç¦»ç›¸å…³å»ºæ¨¡çº¿æ€§å’Œéçº¿æ€§å¸§é—´ä¾èµ–ï¼Œå¹¶åˆ©ç”¨ä»»åŠ¡åŸå‹å®ç°ä»»åŠ¡ç‰¹å®šåŒ¹é…ï¼›å¼•å¯¼LSNä¸é€‚åº”CLIPæ¨¡å—é€šè¿‡é€‚åº”åçš„å†»ç»“CLIPæ­£åˆ™åŒ–LSNï¼Œä»¥åœ¨æœ‰é™ç›‘ç£ä¸‹æ”¹è¿›Î±-è·ç¦»ç›¸å…³ä¼°è®¡çš„è®­ç»ƒã€‚

**Result:** åœ¨äº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTS-FSARç›¸æ¯”å…ˆå‰çš„æœ€å…ˆè¿›æ–¹æ³•å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯äº†æ‰€ææ¡†æ¶åœ¨å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡ç»“åˆé«˜æ•ˆå¾®è°ƒç­–ç•¥å’Œä»»åŠ¡ç‰¹å®šçš„éçº¿æ€§åŒ¹é…åº¦é‡ï¼Œæ˜¾è‘—æå‡äº†å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«çš„æ€§èƒ½ï¼Œä¸ºè§£å†³æœ‰é™æ•°æ®æ¡ä»¶ä¸‹çš„å¤æ‚æ¨¡å¼å»ºæ¨¡å’Œæ¨¡å‹ä¼˜åŒ–å›°éš¾æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Few-shot action recognition (FSAR) has recently made notable progress through set matching and efficient adaptation of large-scale pre-trained models. However, two key limitations persist. First, existing set matching metrics typically rely on cosine similarity to measure inter-frame linear dependencies and then perform matching with only instance-level information, thus failing to capture more complex patterns such as nonlinear relationships and overlooking task-specific cues. Second, for efficient adaptation of CLIP to FSAR, recent work performing fine-tuning via skip-fusion layers (which we refer to as side layers) has significantly reduced memory cost. However, the newly introduced side layers are often difficult to optimize under limited data conditions. To address these limitations, we propose TS-FSAR, a framework comprising three components: (1) a visual Ladder Side Network (LSN) for efficient CLIP fine-tuning; (2) a metric called Task-Specific Distance Correlation Matching (TS-DCM), which uses $Î±$-distance correlation to model both linear and nonlinear inter-frame dependencies and leverages a task prototype to enable task-specific matching; and (3) a Guiding LSN with Adapted CLIP (GLAC) module, which regularizes LSN using the adapted frozen CLIP to improve training for better $Î±$-distance correlation estimation under limited supervision. Extensive experiments on five widely-used benchmarks demonstrate that our TS-FSAR yields superior performance compared to prior state-of-the-arts.


### [22] [The N-Body Problem: Parallel Execution from Single-Person Egocentric Video](https://arxiv.org/abs/2512.11393)
*Zhifan Zhu, Yifei Huang, Yoichi Sato, Dima Damen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†N-Bodyé—®é¢˜ï¼Œæ—¨åœ¨ä»å•è§†è§’è§†é¢‘ä¸­å­¦ä¹ äººç±»æ´»åŠ¨çš„å¹¶è¡ŒåŒ–æ‰§è¡Œï¼Œé€šè¿‡ç»“æ„åŒ–æç¤ºç­–ç•¥å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†3Dç¯å¢ƒã€ç‰©ä½“ä½¿ç”¨å’Œæ—¶åºä¾èµ–ï¼Œç”Ÿæˆå¯è¡Œçš„å¹¶è¡Œæ‰§è¡Œæ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** äººç±»èƒ½å¤Ÿç›´è§‚åœ°å¹¶è¡ŒåŒ–å¤æ‚æ´»åŠ¨ï¼Œä½†æ¨¡å‹èƒ½å¦ä»è§‚å¯Ÿå•ä¸ªäººçš„è§†é¢‘ä¸­å­¦ä¹ è¿™ç§èƒ½åŠ›ï¼Ÿç»™å®šä¸€ä¸ªç¬¬ä¸€äººç§°è§†è§’è§†é¢‘ï¼Œç ”ç©¶N-Bodyé—®é¢˜ï¼šNä¸ªä¸ªä½“å¦‚ä½•å‡è®¾æ€§åœ°æ‰§è¡Œè§†é¢‘ä¸­è§‚å¯Ÿåˆ°çš„ç›¸åŒä»»åŠ¡é›†åˆï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–åŠ é€Ÿï¼Œä½†ç®€å•çš„è§†é¢‘ç‰‡æ®µåˆ†é…å¸¸è¿åç°å®çº¦æŸï¼Œå¯¼è‡´ç‰©ç†ä¸Šä¸å¯èƒ½çš„åœºæ™¯ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿå½¢å¼åŒ–äº†N-Bodyé—®é¢˜å¹¶æå‡ºäº†ä¸€å¥—è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡æ€§èƒ½ï¼ˆåŠ é€Ÿã€ä»»åŠ¡è¦†ç›–ç‡ï¼‰å’Œå¯è¡Œæ€§ï¼ˆç©ºé—´ç¢°æ’ã€ç‰©ä½“å†²çªå’Œå› æœçº¦æŸï¼‰ã€‚å¼•å…¥ç»“æ„åŒ–æç¤ºç­–ç•¥ï¼Œå¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†3Dç¯å¢ƒã€ç‰©ä½“ä½¿ç”¨å’Œæ—¶åºä¾èµ–ï¼Œä»¥ç”Ÿæˆå¯è¡Œçš„å¹¶è¡Œæ‰§è¡Œæ–¹æ¡ˆã€‚

**Result:** åœ¨EPIC-Kitchenså’ŒHD-EPICçš„100ä¸ªè§†é¢‘ä¸Šï¼Œå½“N=2æ—¶ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”Gemini 2.5 Proçš„åŸºçº¿æç¤ºå°†åŠ¨ä½œè¦†ç›–ç‡æé«˜äº†45%ï¼ŒåŒæ—¶å°†ç¢°æ’ç‡ã€ç‰©ä½“å†²çªå’Œå› æœå†²çªåˆ†åˆ«é™ä½äº†55%ã€45%å’Œ55%ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†ä»å•è§†è§’è§†é¢‘å­¦ä¹ æ´»åŠ¨å¹¶è¡ŒåŒ–çš„å¯è¡Œæ€§ï¼Œæå‡ºçš„ç»“æ„åŒ–æç¤ºç­–ç•¥æœ‰æ•ˆè§£å†³äº†ç°å®çº¦æŸé—®é¢˜ï¼Œä¸ºå¤šæ™ºèƒ½ä½“åä½œã€æœºå™¨äººä»»åŠ¡è§„åˆ’å’Œæ•ˆç‡ä¼˜åŒ–ç­‰åº”ç”¨æä¾›äº†æ–°æ–¹æ³•ï¼ŒåŒæ—¶å½¢å¼åŒ–çš„è¯„ä¼°æ¡†æ¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†åŸºå‡†ã€‚

---

#### ğŸ“„ Abstract
Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.


### [23] [FlowDC: Flow-Based Decoupling-Decay for Complex Image Editing](https://arxiv.org/abs/2512.11395)
*Yilei Jiang, Zhen Wang, Yanghao Wang, Jun Yu, Yueting Zhuang, Jun Xiao, Long Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºFlowDCæ–¹æ³•ï¼Œé€šè¿‡å°†å¤æ‚å›¾åƒç¼–è¾‘ä»»åŠ¡è§£è€¦ä¸ºå¤šä¸ªå­ç¼–è¾‘æ•ˆæœå¹¶è¿›è¡Œå¹¶è¡Œå åŠ ï¼ŒåŒæ—¶åˆ†è§£é€Ÿåº¦åœºå¹¶è¡°å‡æ­£äº¤åˆ†é‡ï¼Œä»¥è§£å†³å¤šç›®æ ‡å¤æ‚ç¼–è¾‘ä¸­çš„è¯­ä¹‰å¯¹é½ä¸æºä¸€è‡´æ€§å¹³è¡¡é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæµåŒ¹é…æ¨¡å‹çš„å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨å¤„ç†ç®€å•ç¼–è¾‘ä»»åŠ¡æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤„ç†åŒ…å«å¤šä¸ªç¼–è¾‘ç›®æ ‡çš„å¤æ‚ç¼–è¾‘ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„å•è½®ç¼–è¾‘å’Œå¤šè½®ç¼–è¾‘æ–¹æ³•åˆ†åˆ«å—åˆ°é•¿æ–‡æœ¬è·Ÿéšå’Œç´¯ç§¯ä¸ä¸€è‡´æ€§çš„é™åˆ¶ï¼Œéš¾ä»¥åœ¨è¯­ä¹‰å¯¹é½å’Œæºä¸€è‡´æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**Method:** FlowDCæ–¹æ³•çš„æ ¸å¿ƒæ˜¯å°†å¤æ‚ç¼–è¾‘ä»»åŠ¡è§£è€¦ä¸ºå¤šä¸ªå­ç¼–è¾‘æ•ˆæœï¼Œå¹¶åœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­å¹¶è¡Œå åŠ è¿™äº›æ•ˆæœã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•è§‚å¯Ÿåˆ°ä¸ç¼–è¾‘ä½ç§»æ­£äº¤çš„é€Ÿåº¦åˆ†é‡ä¼šæŸå®³æºç»“æ„ä¿æŒï¼Œå› æ­¤å¯¹é€Ÿåº¦åœºè¿›è¡Œåˆ†è§£å¹¶è¡°å‡æ­£äº¤éƒ¨åˆ†ä»¥æ›´å¥½åœ°ä¿æŒæºä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ„å»ºäº†å¤æ‚ç¼–è¾‘åŸºå‡†Complex-PIE-Benchç”¨äºè¯„ä¼°æ–¹æ³•æ€§èƒ½ã€‚

**Result:** åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒFlowDCæ–¹æ³•ç›¸æ¯”ç°æœ‰æ–¹æ³•å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚è¯¥æ–¹æ³•åœ¨å¤æ‚ç¼–è¾‘ä»»åŠ¡ä¸­å®ç°äº†è¯­ä¹‰å¯¹é½å’Œæºä¸€è‡´æ€§ä¹‹é—´çš„æ›´å¥½å¹³è¡¡ï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒè¯¦ç»†éªŒè¯äº†å„æ¨¡å—è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** FlowDCé€šè¿‡è§£è€¦å¤æ‚ç¼–è¾‘ä»»åŠ¡å’Œä¼˜åŒ–é€Ÿåº¦åœºåˆ†è§£ï¼Œä¸ºå¤šç›®æ ‡å›¾åƒç¼–è¾‘æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä¸ä»…æå‡äº†å¤æ‚ç¼–è¾‘çš„æ€§èƒ½ï¼Œè¿˜ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„æŠ€æœ¯æ€è·¯å’Œè¯„ä¼°åŸºå‡†ï¼Œæ¨åŠ¨äº†æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘é¢†åŸŸçš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
With the surge of pre-trained text-to-image flow matching models, text-based image editing performance has gained remarkable improvement, especially for \underline{simple editing} that only contains a single editing target. To satisfy the exploding editing requirements, the \underline{complex editing} which contains multiple editing targets has posed as a more challenging task. However, current complex editing solutions: single-round and multi-round editing are limited by long text following and cumulative inconsistency, respectively. Thus, they struggle to strike a balance between semantic alignment and source consistency. In this paper, we propose \textbf{FlowDC}, which decouples the complex editing into multiple sub-editing effects and superposes them in parallel during the editing process. Meanwhile, we observed that the velocity quantity that is orthogonal to the editing displacement harms the source structure preserving. Thus, we decompose the velocity and decay the orthogonal part for better source consistency. To evaluate the effectiveness of complex editing settings, we construct a complex editing benchmark: Complex-PIE-Bench. On two benchmarks, FlowDC shows superior results compared with existing methods. We also detail the ablations of our module designs.


### [24] [VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing](https://arxiv.org/abs/2512.11490)
*Emanuel SÃ¡nchez Aimar, Gulnaz Zhambulova, Fahad Shahbaz Khan, Yonghao Xu, Michael Felsberg*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºVLM2GeoVecï¼Œä¸€ç§æŒ‡ä»¤è·Ÿéšçš„å•ç¼–ç å™¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†äº¤é”™è¾“å…¥ï¼ˆå›¾åƒã€æ–‡æœ¬ã€è¾¹ç•Œæ¡†å’Œåœ°ç†åæ ‡ï¼‰åµŒå…¥ç»Ÿä¸€å‘é‡ç©ºé—´ï¼Œè§£å†³äº†é¥æ„Ÿé¢†åŸŸä¸­æ£€ç´¢æ¨¡å‹ä¸ç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„å‰²è£‚é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å«æ˜Ÿå›¾åƒä¸è‡ªç„¶å›¾åƒå­˜åœ¨æ ¹æœ¬å·®å¼‚ï¼Œå…¶èˆªæ‹è§†è§’ã€è¶…é«˜åˆ†è¾¨ç‡ã€å°ºåº¦å˜åŒ–å¤šæ ·å’Œå°ç‰©ä½“ä¸°å¯Œç­‰ç‰¹ç‚¹éœ€è¦åŒºåŸŸçº§ç©ºé—´æ¨ç†å’Œæ•´ä½“åœºæ™¯ç†è§£ã€‚å½“å‰é¥æ„Ÿæ–¹æ³•åœ¨åŒç¼–ç å™¨æ£€ç´¢æ¨¡å‹ï¼ˆæ“…é•¿å¤§è§„æ¨¡è·¨æ¨¡æ€æœç´¢ä½†æ— æ³•äº¤é”™æ¨¡æ€ï¼‰å’Œç”ŸæˆåŠ©æ‰‹ï¼ˆæ”¯æŒåŒºåŸŸçº§è§£é‡Šä½†ç¼ºä¹å¯æ‰©å±•æ£€ç´¢èƒ½åŠ›ï¼‰ä¹‹é—´å­˜åœ¨å‰²è£‚ï¼Œéœ€è¦ç»Ÿä¸€è§£å†³æ–¹æ¡ˆã€‚

**Method:** æå‡ºVLM2GeoVecæ¨¡å‹ï¼Œé‡‡ç”¨æŒ‡ä»¤è·Ÿéšçš„å•ç¼–ç å™¨æ¶æ„ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†äº¤é”™è¾“å…¥ï¼ˆå›¾åƒã€æ–‡æœ¬ã€è¾¹ç•Œæ¡†å’Œåœ°ç†åæ ‡ï¼‰åµŒå…¥ç»Ÿä¸€å‘é‡ç©ºé—´ã€‚è¯¥æ–¹æ³•æ¶ˆé™¤äº†å¤šé˜¶æ®µæµæ°´çº¿å’Œä»»åŠ¡ç‰¹å®šæ¨¡å—ï¼Œå¹¶å¼•å…¥RSMEBåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–åœºæ™¯åˆ†ç±»ã€è·¨æ¨¡æ€æœç´¢ã€ç»„åˆæ£€ç´¢ã€è§†è§‰é—®ç­”ã€è§†è§‰å®šä½å’ŒåŒºåŸŸçº§æ¨ç†ä»¥åŠè¯­ä¹‰åœ°ç†ç©ºé—´æ£€ç´¢ç­‰å…³é”®é¥æ„ŸåµŒå…¥åº”ç”¨ã€‚

**Result:** åœ¨RSMEBåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVLM2GeoVecåœ¨åŒºåŸŸ-å­—å¹•æ£€ç´¢ä¸Šè¾¾åˆ°26.6%çš„P@1ï¼ˆæ¯”åŒç¼–ç å™¨åŸºçº¿æå‡25ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œåœ¨æŒ‡ä»£è¡¨è¾¾æ£€ç´¢ä¸Šè¾¾åˆ°32.5%çš„P@1ï¼ˆæå‡19ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œåœ¨è¯­ä¹‰åœ°ç†å®šä½æ£€ç´¢ä¸Šè¾¾åˆ°17.8%çš„P@1ï¼ˆè¶…è¿‡å…ˆå‰æœ€ä½³ç»“æœ3å€ä»¥ä¸Šï¼‰ï¼ŒåŒæ—¶åœ¨åœºæ™¯åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ç­‰ä¼ ç»Ÿä»»åŠ¡ä¸ŠåŒ¹é…æˆ–è¶…è¶Šä¸“ç”¨åŸºçº¿ã€‚

**Conclusion:** VLM2GeoVecç»Ÿä¸€äº†å¯æ‰©å±•æ£€ç´¢ä¸åŒºåŸŸçº§ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå®ç°äº†é¥æ„Ÿä¸­è¿è´¯çš„å¤šæ¨¡æ€åˆ†æã€‚è¯¥ç ”ç©¶ä¸ºé¥æ„Ÿé¢†åŸŸæä¾›äº†é¦–ä¸ªèƒ½å¤ŸåŒæ—¶å¤„ç†äº¤é”™æ¨¡æ€è¾“å…¥å’Œç»Ÿä¸€åµŒå…¥çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†åŒºåŸŸçº§ä»»åŠ¡çš„æ€§èƒ½ï¼Œä¸ºæœªæ¥é¥æ„Ÿæ™ºèƒ½åˆ†æç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.


### [25] [Reconstruction as a Bridge for Event-Based Visual Question Answering](https://arxiv.org/abs/2512.11510)
*Hanyue Lou, Jiayi Zhou, Yang Zhang, Boyu Li, Yi Wang, Guangnan Ye, Boxin Shi*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†ä¸¤ç§å°†äº‹ä»¶ç›¸æœºæ•°æ®ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é›†æˆçš„æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†é¦–ä¸ªå®¢è§‚çš„ã€çœŸå®ä¸–ç•Œçš„äº‹ä»¶MLLMåŸºå‡†EvQAï¼Œåœ¨äº‹ä»¶è§†è§‰ç†è§£æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°†äº‹ä»¶ç›¸æœºä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é›†æˆæœ‰æœ›åœ¨æŒ‘æˆ˜æ€§è§†è§‰æ¡ä»¶ä¸‹å®ç°é€šç”¨åœºæ™¯ç†è§£ï¼Œä½†éœ€è¦åœ¨ä¿ç•™äº‹ä»¶æ•°æ®ç‹¬ç‰¹ä¼˜åŠ¿ä¸ç¡®ä¿ä¸åŸºäºå¸§çš„æ¨¡å‹å…¼å®¹æ€§ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå½“å‰ç¼ºä¹å®¢è§‚çš„ã€çœŸå®ä¸–ç•Œçš„äº‹ä»¶MLLMè¯„ä¼°åŸºå‡†ã€‚

**Method:** æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼šåŸºäºå¸§çš„é‡å»ºä¸æ ‡è®°åŒ–æ–¹æ³•ï¼Œä»¥åŠåˆ©ç”¨äº‹ä»¶ç¨€ç–æ€§çš„è‡ªé€‚åº”é‡å»ºä¸æ ‡è®°åŒ–æ–¹æ³•ï¼›åŒæ—¶è®¾è®¡äº†EvQAåŸºå‡†ï¼ŒåŒ…å«æ¥è‡ª22ä¸ªå…¬å…±æ•°æ®é›†çš„1000ä¸ªäº‹ä»¶-Q&Aå¯¹ï¼Œç”¨äºå®¢è§‚è¯„ä¼°äº‹ä»¶MLLMæ€§èƒ½ã€‚

**Result:** å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨EvQAåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†äº‹ä»¶ç¨€ç–æ€§åˆ©ç”¨çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†MLLMåœ¨äº‹ä»¶è§†è§‰ç†è§£æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡é‡å»ºä½œä¸ºæ¡¥æ¢ï¼ŒæˆåŠŸè§£å†³äº†äº‹ä»¶æ•°æ®ä¸å¸§åŸºæ¨¡å‹å…¼å®¹æ€§çš„æŒ‘æˆ˜ï¼Œæå‡ºçš„EvQAåŸºå‡†ä¸ºäº‹ä»¶MLLMè¯„ä¼°æä¾›äº†å®¢è§‚æ ‡å‡†ï¼Œè¯æ˜äº†MLLMåœ¨äº‹ä»¶è§†è§‰é¢†åŸŸçš„åº”ç”¨å‰æ™¯ã€‚

---

#### ğŸ“„ Abstract
Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.


### [26] [Infinity and Beyond: Compositional Alignment in VAR and Diffusion T2I Models](https://arxiv.org/abs/2512.11542)
*Hossein Shahabadi, Niki Sepasian, Arash Marioriyad, Ali Sharifi-Zarchi, Mahdieh Soleymani Baghshah*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ¯”è¾ƒäº†è§†è§‰è‡ªå›å½’æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹åœ¨ç»„åˆå¯¹é½èƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼Œé€šè¿‡å…¨é¢åŸºå‡†æµ‹è¯•å‘ç°Infinity-8Båœ¨ç»„åˆå¯¹é½æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä¸ºT2Iæ¨¡å‹çš„æœªæ¥å‘å±•å»ºç«‹äº†ç»Ÿä¸€åŸºå‡†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°ä»£æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å®ç°æ–‡æœ¬æè¿°ä¸ç”Ÿæˆå›¾åƒä¹‹é—´çš„ç»„åˆå¯¹é½ï¼ˆæ¶µç›–å¯¹è±¡ã€å±æ€§å’Œç©ºé—´å…³ç³»ï¼‰æ–¹é¢ä»é¢ä¸´æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå°½ç®¡æ‰©æ•£æ¨¡å‹å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†æ–°å…´è§†è§‰è‡ªå›å½’æ¨¡å‹çš„ç»„åˆè¡Œä¸ºå°šæœªå¾—åˆ°å……åˆ†æ£€éªŒï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚

**Method:** ç ”ç©¶å¯¹å…­ç§ä¸åŒçš„T2Iç³»ç»Ÿè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬SDXLã€PixArt-Î±ã€Flux-Devã€Flux-Schnellã€Infinity-2Bå’ŒInfinity-8Bï¼Œè¯„ä¼°è¦†ç›–äº†å®Œæ•´çš„T2I-CompBench++å’ŒGenEvalæµ‹è¯•å¥—ä»¶ï¼Œé‡ç‚¹è€ƒå¯Ÿé¢œè‰²ä¸å±æ€§ç»‘å®šã€ç©ºé—´å…³ç³»ã€æ•°å€¼ç†è§£å’Œå¤æ‚å¤šå¯¹è±¡æç¤ºç­‰ç»„åˆå¯¹é½èƒ½åŠ›ã€‚

**Result:** åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒInfinity-8Bå®ç°äº†æœ€å¼ºçš„æ•´ä½“ç»„åˆå¯¹é½æ€§èƒ½ï¼Œè€ŒInfinity-2Båœ¨å¤šä¸ªç±»åˆ«ä¸­ä¹ŸåŒ¹é…æˆ–è¶…è¶Šäº†æ›´å¤§çš„æ‰©æ•£æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºä¼˜è¶Šçš„æ•ˆç‡-æ€§èƒ½æƒè¡¡ï¼›ç›¸æ¯”ä¹‹ä¸‹ï¼ŒSDXLå’ŒPixArt-Î±åœ¨å±æ€§æ•æ„Ÿå’Œç©ºé—´ä»»åŠ¡ä¸­è¡¨ç°å‡ºæŒç»­çš„å¼±ç‚¹ã€‚

**Conclusion:** è¿™é¡¹ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ¯”è¾ƒäº†VARå’Œæ‰©æ•£æ–¹æ³•åœ¨ç»„åˆå¯¹é½æ–¹é¢çš„è¡¨ç°ï¼Œä¸ºT2Iæ¨¡å‹çš„æœªæ¥å‘å±•å»ºç«‹äº†ç»Ÿä¸€åŸºå‡†ï¼Œç»“æœè¡¨æ˜è§†è§‰è‡ªå›å½’æ¨¡å‹åœ¨ç»„åˆå¯¹é½ä»»åŠ¡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯Infinityç³»åˆ—æ¨¡å‹åœ¨æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ã€‚

---

#### ğŸ“„ Abstract
Achieving compositional alignment between textual descriptions and generated images - covering objects, attributes, and spatial relationships - remains a core challenge for modern text-to-image (T2I) models. Although diffusion-based architectures have been widely studied, the compositional behavior of emerging Visual Autoregressive (VAR) models is still largely unexamined. We benchmark six diverse T2I systems - SDXL, PixArt-$Î±$, Flux-Dev, Flux-Schnell, Infinity-2B, and Infinity-8B - across the full T2I-CompBench++ and GenEval suites, evaluating alignment in color and attribute binding, spatial relations, numeracy, and complex multi-object prompts. Across both benchmarks, Infinity-8B achieves the strongest overall compositional alignment, while Infinity-2B also matches or exceeds larger diffusion models in several categories, highlighting favorable efficiency-performance trade-offs. In contrast, SDXL and PixArt-$Î±$ show persistent weaknesses in attribute-sensitive and spatial tasks. These results provide the first systematic comparison of VAR and diffusion approaches to compositional alignment and establish unified baselines for the future development of the T2I model.


### [27] [Embodied Image Compression](https://arxiv.org/abs/2512.11612)
*Chunyi Li, Rui Qing, Jianbo Zhang, Yuan Tian, Xiangyang Zhu, Zicheng Zhang, Xiaohong Liu, Weisi Lin, Guangtao Zhai*

#### ğŸ§© TL;DR
æœ¬æ–‡é¦–æ¬¡æå‡ºå…·èº«å›¾åƒå‹ç¼©è¿™ä¸€ç§‘å­¦é—®é¢˜ï¼Œé’ˆå¯¹å…·èº«æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œç¯å¢ƒä¸­çš„é€šä¿¡çº¦æŸï¼Œå»ºç«‹äº†æ ‡å‡†åŒ–åŸºå‡†EmbodiedCompï¼Œå¹¶è¯æ˜ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨ä½äºå…·èº«æ¯”ç‰¹ç‡é˜ˆå€¼æ—¶æ— æ³•å¯é æ‰§è¡Œç®€å•æ“ä½œä»»åŠ¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€æœºå™¨æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ï¼Œå‹ç¼©ç›®æ ‡å·²ä»ä»»åŠ¡ç‰¹å®šçš„è™šæ‹Ÿæ¨¡å‹è½¬å‘åœ¨ç°å®ä¸–ç•Œç¯å¢ƒä¸­æ“ä½œçš„å…·èº«æ™ºèƒ½ä½“ï¼Œä½†ç°æœ‰å›¾åƒå‹ç¼©æ–¹æ³•æ— æ³•æ»¡è¶³å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­å…·èº«AIçš„é€šä¿¡çº¦æŸå’Œå®æ—¶ä»»åŠ¡æ‰§è¡Œéœ€æ±‚ï¼Œè¿™ä¿ƒä½¿ç ”ç©¶è€…é¦–æ¬¡æå‡ºå…·èº«å›¾åƒå‹ç¼©è¿™ä¸€ç§‘å­¦é—®é¢˜ã€‚

**Method:** æœ¬æ–‡å»ºç«‹äº†æ ‡å‡†åŒ–åŸºå‡†EmbodiedCompï¼Œç”¨äºåœ¨é—­ç¯è®¾ç½®ä¸‹å¯¹è¶…ä½æ¯”ç‰¹ç‡æ¡ä»¶è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œé€šè¿‡æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œç¯å¢ƒä¸­çš„å¹¿æ³›å®è¯ç ”ç©¶ï¼Œè¯„ä¼°ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å‹ç¼©æ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

**Result:** å®éªŒç ”ç©¶è¡¨æ˜ï¼Œå½“å‹ç¼©æ¯”ç‰¹ç‡ä½äºå…·èº«æ¯”ç‰¹ç‡é˜ˆå€¼æ—¶ï¼Œç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ— æ³•å¯é æ‰§è¡Œç®€å•çš„æ“ä½œä»»åŠ¡ï¼Œè¿™æ­ç¤ºäº†å½“å‰æ–¹æ³•åœ¨æ»¡è¶³å…·èº«æ™ºèƒ½ä½“é€šä¿¡éœ€æ±‚æ–¹é¢çš„ä¸¥é‡ä¸è¶³ã€‚

**Conclusion:** EmbodiedCompåŸºå‡†å°†æ¨åŠ¨é¢å‘å…·èº«æ™ºèƒ½ä½“çš„é¢†åŸŸç‰¹å®šå‹ç¼©æŠ€æœ¯çš„å‘å±•ï¼ŒåŠ é€Ÿå…·èº«AIåœ¨ç°å®ä¸–ç•Œä¸­çš„éƒ¨ç½²åº”ç”¨ï¼Œä¸ºè§£å†³å…·èº«æ™ºèƒ½ä½“åœ¨å—é™é€šä¿¡ç¯å¢ƒä¸‹çš„è§†è§‰æ•°æ®å¤„ç†é—®é¢˜æä¾›äº†é‡è¦çš„è¯„ä¼°æ¡†æ¶å’Œç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.


### [28] [Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation](https://arxiv.org/abs/2512.11654)
*Luca Cazzola, Ahed Alboody*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºKineMICæ¡†æ¶ï¼Œé€šè¿‡è¿ç§»å­¦ä¹ æ–¹æ³•å°†é€šç”¨æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆæ¨¡å‹é€‚é…åˆ°äººä½“åŠ¨ä½œè¯†åˆ«é¢†åŸŸï¼Œåˆ©ç”¨CLIPæ–‡æœ¬åµŒå…¥å»ºç«‹è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œå®ç°å°‘æ ·æœ¬åŠ¨ä½œåˆæˆï¼Œæ˜¾è‘—æå‡åŠ¨ä½œè¯†åˆ«æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§è§„æ¨¡æ ‡æ³¨è¿åŠ¨æ•°æ®é›†çš„è·å–æˆæœ¬æ˜¯éª¨éª¼åŸºäººä½“åŠ¨ä½œè¯†åˆ«çš„å…³é”®ç“¶é¢ˆï¼Œå°½ç®¡æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆæ¨¡å‹æä¾›äº†å¯æ‰©å±•çš„åˆæˆæ•°æ®æºï¼Œä½†å…¶è®­ç»ƒç›®æ ‡å’Œæ•°æ®é›†ç»“æ„ä¸åŠ¨ä½œè¯†åˆ«æ‰€éœ€çš„è¿åŠ¨ç²¾åº¦å’Œç±»åˆ«åŒºåˆ†æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¯¼è‡´é€šç”¨T2Mæ¨¡å‹æ— æ³•ç”Ÿæˆé€‚åˆåŠ¨ä½œè¯†åˆ«åˆ†ç±»å™¨çš„è¿åŠ¨æ•°æ®ã€‚

**Method:** æœ¬æ–‡æå‡ºKineMICè¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å‡è®¾æ–‡æœ¬ç¼–ç ç©ºé—´ä¸­çš„è¯­ä¹‰å¯¹åº”å…³ç³»å¯ä»¥ä¸ºè¿åŠ¨å­¦è’¸é¦æä¾›è½¯ç›‘ç£ï¼Œé‡‡ç”¨åŠ¨åŠ›å­¦æŒ–æ˜ç­–ç•¥åˆ©ç”¨CLIPæ–‡æœ¬åµŒå…¥å»ºç«‹ç¨€ç–åŠ¨ä½œè¯†åˆ«æ ‡ç­¾ä¸T2Mæºæ•°æ®ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼ŒæŒ‡å¯¼å¾®è°ƒè¿‡ç¨‹å°†é€šç”¨T2Mä¸»å¹²è½¬æ¢ä¸ºä¸“é—¨çš„å°‘æ ·æœ¬åŠ¨ä½œåˆ°è¿åŠ¨ç”Ÿæˆå™¨ã€‚

**Result:** åœ¨HumanML3Dä½œä¸ºæºT2Mæ•°æ®é›†å’ŒNTU RGB+D 120å­é›†ä½œä¸ºç›®æ ‡åŠ¨ä½œè¯†åˆ«é¢†åŸŸçš„éªŒè¯ä¸­ï¼Œä»…ä½¿ç”¨æ¯åŠ¨ä½œç±»åˆ«10ä¸ªæ ·æœ¬ï¼ŒKineMICç”Ÿæˆçš„è¿åŠ¨æ•°æ®æ˜¾è‘—æ›´è¿è´¯ï¼Œä½œä¸ºæ•°æ®å¢å¼ºæºæä¾›äº†+23.1%å‡†ç¡®ç‡æå‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è¯­ä¹‰å¯¹åº”å…³ç³»è¿›è¡Œè¿åŠ¨å­¦è’¸é¦çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå°‘æ ·æœ¬åŠ¨ä½œåˆæˆæä¾›äº†å®ç”¨æ¡†æ¶ï¼Œæ˜¾è‘—ç¼©å°äº†é€šç”¨æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆæ¨¡å‹ä¸åŠ¨ä½œè¯†åˆ«éœ€æ±‚ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œä¸ºæ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„åŠ¨ä½œè¯†åˆ«æä¾›äº†å¯è¡Œçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).


### [29] [Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing](https://arxiv.org/abs/2512.11680)
*Xu Zhang, Jiabin Fang, Zhuoming Ding, Jin Yuan, Xuan Liu, Qianjun Zhang, Zhiyong Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºCLV-Netï¼Œä¸€ç§ç”¨äºè§†è§‰æç¤ºå¼•å¯¼çš„å¤šæ¨¡æ€é¥æ„Ÿå›¾åƒç†è§£æ¡†æ¶ï¼Œé€šè¿‡ç”¨æˆ·æä¾›çš„ç®€å•è§†è§‰æç¤ºï¼ˆè¾¹ç•Œæ¡†ï¼‰æ¥ç”Ÿæˆç›¸å…³çš„åˆ†å‰²æ©ç å’Œæè¿°ï¼Œæœ‰æ•ˆæ•æ‰ç”¨æˆ·æ„å›¾å¹¶å»ºæ¨¡å¯¹è±¡é—´å…³ç³»ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€é¥æ„Ÿå›¾åƒç†è§£æ–¹æ³•åœ¨ä»…æœ‰ç®€å•æ–‡æœ¬æç¤ºæ—¶éš¾ä»¥å¼•å¯¼æ¨¡å‹å…³æ³¨ç”¨æˆ·ç›¸å…³åŒºåŸŸï¼Œä¸”å¤§è§„æ¨¡èˆªç©ºå½±åƒä¸­è®¸å¤šå¯¹è±¡å…·æœ‰é«˜åº¦ç›¸ä¼¼çš„è§†è§‰å¤–è§‚å’Œä¸°å¯Œçš„å¯¹è±¡é—´å…³ç³»ï¼Œè¿™è¿›ä¸€æ­¥é˜»ç¢äº†å‡†ç¡®è¯†åˆ«ã€‚

**Method:** CLV-Netçš„æ ¸å¿ƒè®¾è®¡åŒ…æ‹¬ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ©ç è§£ç å™¨ï¼Œç”¨äºå»ºæ¨¡å’Œæ•´åˆå¯¹è±¡é—´å…³ç³»ä»¥å¢å¼ºç›®æ ‡è¡¨ç¤ºï¼›ä»¥åŠè¯­ä¹‰ä¸å…³ç³»å¯¹é½æ¨¡å—ï¼ŒåŒ…å«è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§æŸå¤±ä»¥å¢å¼ºè§†è§‰ç›¸ä¼¼ç›®æ ‡çš„ç»†ç²’åº¦åŒºåˆ†ï¼Œä»¥åŠå…³ç³»ä¸€è‡´æ€§æŸå¤±ä»¥ç¡®ä¿æ–‡æœ¬å…³ç³»ä¸è§†è§‰äº¤äº’çš„å¯¹é½ã€‚

**Result:** åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒCLV-Netä¼˜äºç°æœ‰æ–¹æ³•å¹¶å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œæ¨¡å‹èƒ½æœ‰æ•ˆæ•æ‰ç”¨æˆ·æ„å›¾å¹¶äº§ç”Ÿç²¾ç¡®ã€æ„å›¾å¯¹é½çš„å¤šæ¨¡æ€è¾“å‡ºã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†è§†è§‰æç¤ºå¼•å¯¼çš„å¤šæ¨¡æ€ç†è§£åœ¨é¥æ„Ÿé¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡å»ºæ¨¡å¯¹è±¡é—´å…³ç³»å’Œè·¨æ¨¡æ€å¯¹é½æ˜¾è‘—æå‡äº†æ„å›¾æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºå¤æ‚åœºæ™¯ä¸‹çš„äº¤äº’å¼å›¾åƒåˆ†ææä¾›äº†æ–°èŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.


### [30] [Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection](https://arxiv.org/abs/2512.11683)
*Qiushi Guo*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºDepth Copy Pasteï¼Œä¸€ç§å¤šæ¨¡æ€æ·±åº¦æ„ŸçŸ¥æ•°æ®å¢å¼ºæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè¯­ä¹‰åŒ¹é…ã€ç²¾ç¡®åˆ†å‰²å’Œæ·±åº¦å¼•å¯¼æ”¾ç½®ï¼Œç”Ÿæˆç‰©ç†ä¸€è‡´ä¸”è§†è§‰é€¼çœŸçš„äººè„¸æ£€æµ‹è®­ç»ƒæ ·æœ¬ï¼Œæ˜¾è‘—æå‡æ£€æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿå¤åˆ¶ç²˜è´´å¢å¼ºæ–¹æ³•åœ¨ç”Ÿæˆäººè„¸æ£€æµ‹è®­ç»ƒæ•°æ®æ—¶å­˜åœ¨æ˜æ˜¾ç¼ºé™·ï¼ŒåŒ…æ‹¬å‰æ™¯æå–ä¸å‡†ç¡®ã€åœºæ™¯å‡ ä½•ä¸ä¸€è‡´å’ŒèƒŒæ™¯è¯­ä¹‰ä¸åŒ¹é…ï¼Œå¯¼è‡´åˆæˆæ ·æœ¬ä¸çœŸå®ä¸”ç‰©ç†ä¸ä¸€è‡´ï¼Œé™åˆ¶äº†æ£€æµ‹ç³»ç»Ÿçš„é²æ£’æ€§æå‡ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨å¤šæ¨¡æ€æ·±åº¦æ„ŸçŸ¥å¢å¼ºæ¡†æ¶ï¼Œé¦–å…ˆåˆ©ç”¨BLIPå’ŒCLIPè”åˆè¯„ä¼°è¯­ä¹‰ä¸è§†è§‰ä¸€è‡´æ€§ï¼Œè‡ªåŠ¨æ£€ç´¢æœ€åŒ¹é…çš„èƒŒæ™¯å›¾åƒï¼›é›†æˆSAM3è¿›è¡Œç²¾ç¡®åˆ†å‰²å’ŒDepth-Anythingæå–éé®æŒ¡å¯è§åŒºåŸŸï¼›å¼•å…¥æ·±åº¦å¼•å¯¼æ»‘åŠ¨çª—å£æ”¾ç½®æœºåˆ¶ï¼Œåœ¨èƒŒæ™¯æ·±åº¦å›¾ä¸Šæœç´¢å…·æœ‰æœ€ä½³æ·±åº¦è¿ç»­æ€§å’Œå°ºåº¦å¯¹é½çš„ç²˜è´´ä½ç½®ã€‚

**Result:** å®éªŒè¡¨æ˜Depth Copy Pasteç”Ÿæˆçš„åˆæˆæ ·æœ¬åœ¨æ·±åº¦å…³ç³»å’Œè§†è§‰é€¼çœŸåº¦æ–¹é¢æ˜¾è‘—æ”¹å–„ï¼Œä¸ºä¸‹æ¸¸äººè„¸æ£€æµ‹ä»»åŠ¡æä¾›äº†æ›´å¤šæ ·åŒ–å’ŒçœŸå®çš„è®­ç»ƒæ•°æ®ï¼Œç›¸æ¯”ä¼ ç»Ÿå¤åˆ¶ç²˜è´´å’Œæ— æ·±åº¦å¢å¼ºæ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆè¯­ä¹‰åŒ¹é…ã€ç²¾ç¡®åˆ†å‰²å’Œæ·±åº¦æ„ŸçŸ¥çš„å¢å¼ºæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆç‰©ç†ä¸€è‡´çš„è®­ç»ƒæ ·æœ¬ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„äººè„¸æ£€æµ‹ç³»ç»Ÿæä¾›äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®å¢å¼ºæ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€æ•°æ®å¢å¼ºæŠ€æœ¯çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.


### [31] [EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing](https://arxiv.org/abs/2512.11715)
*Wei Chow, Linfeng Li, Lingdong Kong, Zefeng Li, Qi Xu, Hang Song, Tian Ye, Xian Wang, Jinbin Bai, Shilin Xu, Xiangtai Li, Junting Pan, Shaoteng Liu, Ran Zhou, Tianshu Yang, Songhua Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†EditMGTï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºæ©ç ç”Ÿæˆå˜æ¢å™¨ï¼ˆMGTï¼‰çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨MGTçš„å±€éƒ¨è§£ç ç‰¹æ€§æ¥è§£å†³æ‰©æ•£æ¨¡å‹åœ¨å±€éƒ¨ç¼–è¾‘æ—¶å¯¹éç›®æ ‡åŒºåŸŸçš„æ„å¤–ä¿®æ”¹é—®é¢˜ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„åŒºåŸŸæ§åˆ¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­è™½ç„¶å–å¾—äº†ä¼˜å¼‚çš„è§†è§‰è´¨é‡ï¼Œä½†å…¶å…¨å±€å»å™ªåŠ¨æ€æœ¬è´¨ä¸Šå°†å±€éƒ¨ç¼–è¾‘ç›®æ ‡ä¸å…¨å›¾ä¸Šä¸‹æ–‡æ··ä¸ºä¸€è°ˆï¼Œå¯¼è‡´éç›®æ ‡åŒºåŸŸå‡ºç°æ„å¤–ä¿®æ”¹ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢æ©ç ç”Ÿæˆå˜æ¢å™¨ä½œä¸ºæ›¿ä»£æ–¹æ³•æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œåˆ©ç”¨å…¶å±€éƒ¨è§£ç èŒƒå¼æ¥æ˜¾å¼ä¿æŠ¤ç¼–è¾‘è¿‡ç¨‹ä¸­çš„éç›¸å…³åŒºåŸŸã€‚

**Method:** æœ¬æ–‡æå‡ºäº†EditMGTæ¡†æ¶ï¼Œé¦–å…ˆåˆ©ç”¨MGTçš„äº¤å‰æ³¨æ„åŠ›å›¾æä¾›ä¿¡æ¯æ€§å®šä½ä¿¡å·ï¼Œå¹¶è®¾è®¡äº†å¤šå±‚æ³¨æ„åŠ›æ•´åˆæ–¹æ¡ˆæ¥ç»†åŒ–è¿™äº›å›¾ä»¥å®ç°ç»†ç²’åº¦ç²¾ç¡®å®šä½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šå¼•å…¥äº†åŒºåŸŸä¿æŒé‡‡æ ·æŠ€æœ¯ï¼Œé™åˆ¶åœ¨ä½æ³¨æ„åŠ›åŒºåŸŸå†…è¿›è¡Œä»¤ç‰Œç¿»è½¬ä»¥æŠ‘åˆ¶è™šå‡ç¼–è¾‘ï¼Œä»è€Œå°†ä¿®æ”¹é™åˆ¶åœ¨ç›®æ ‡åŒºåŸŸå†…å¹¶ä¿æŠ¤å‘¨å›´éç›®æ ‡åŒºåŸŸçš„å®Œæ•´æ€§ã€‚é€šè¿‡æ„å»ºCrispEdit-2Mé«˜åˆ†è¾¨ç‡æ•°æ®é›†ï¼Œé‡‡ç”¨æ³¨æ„åŠ›æ³¨å…¥æ–¹æ³•å°†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒMGTé€‚é…ä¸ºå›¾åƒç¼–è¾‘æ¨¡å‹ã€‚

**Result:** åœ¨å››ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒEditMGTåœ¨å‚æ•°å°‘äº10äº¿çš„æƒ…å†µä¸‹å®ç°äº†ç›¸ä¼¼çš„ç›¸ä¼¼æ€§æ€§èƒ½ï¼ŒåŒæ—¶ç¼–è¾‘é€Ÿåº¦æå‡äº†6å€ã€‚åœ¨é£æ ¼å˜åŒ–å’Œé£æ ¼è¿ç§»ä»»åŠ¡ä¸Šåˆ†åˆ«å–å¾—äº†3.6%å’Œ17.6%çš„æ”¹è¿›ï¼Œæä¾›äº†å¯æ¯”æˆ–æ›´ä¼˜çš„ç¼–è¾‘è´¨é‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ©ç ç”Ÿæˆå˜æ¢å™¨åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œå…¶å±€éƒ¨è§£ç èŒƒå¼ä¸ºè§£å†³æ‰©æ•£æ¨¡å‹çš„å…¨å±€ç¼–è¾‘é™åˆ¶æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚EditMGTæ¡†æ¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨MGTçš„æ³¨æ„åŠ›æœºåˆ¶å®ç°ç²¾ç¡®çš„åŒºåŸŸæ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆçš„ç¼–è¾‘æ€§èƒ½ï¼Œä¸ºæœªæ¥çš„å±€éƒ¨æ„ŸçŸ¥å›¾åƒç¼–è¾‘æ–¹æ³•å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.


### [32] [Referring Change Detection in Remote Sensing Imagery](https://arxiv.org/abs/2512.11719)
*Yilmaz Korkmaz, Jay N. Paranjape, Celso M. de Melo, Vishal M. Patel*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Referring Change Detection (RCD)æ¡†æ¶ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°é¥æ„Ÿå›¾åƒä¸­ç‰¹å®šç±»åˆ«å˜åŒ–çš„æ£€æµ‹ï¼Œå¹¶å¼•å…¥RCDNetè·¨æ¨¡æ€èåˆç½‘ç»œå’ŒRCDGenæ‰©æ•£åˆæˆæ•°æ®ç”Ÿæˆç®¡é“æ¥è§£å†³æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿå˜åŒ–æ£€æµ‹æ–¹æ³•æ— æ³•åŒºåˆ†å˜åŒ–ç±»å‹ï¼Œè€Œè¯­ä¹‰å˜åŒ–æ£€æµ‹æ–¹æ³•ä¾èµ–åˆšæ€§ç±»åˆ«å®šä¹‰å’Œå›ºå®šæ¨¡å‹æ¶æ„ï¼Œéš¾ä»¥æ··åˆä¸åŒæ ‡ç­¾é›†çš„æ•°æ®é›†æˆ–è·¨ä»»åŠ¡é‡ç”¨æ¨¡å‹ï¼Œå› ä¸ºè¾“å‡ºé€šé“ä¸è¯­ä¹‰ç±»åˆ«æ•°é‡å’Œç±»å‹ç´§å¯†è€¦åˆã€‚ç°æœ‰æ–¹æ³•æ— æ³•æ»¡è¶³ç”¨æˆ·å¯¹ç‰¹å®šå˜åŒ–ç±»å‹çš„æ£€æµ‹éœ€æ±‚ã€‚

**Method:** æœ¬æ–‡æå‡ºä¸¤é˜¶æ®µæ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µæ˜¯RCDNetï¼Œä¸€ä¸ªç”¨äºæŒ‡ä»£å˜åŒ–æ£€æµ‹çš„è·¨æ¨¡æ€èåˆç½‘ç»œï¼Œå°†è‡ªç„¶è¯­è¨€ç†è§£ä¸è§†è§‰åˆ†æç›¸ç»“åˆï¼›ç¬¬äºŒé˜¶æ®µæ˜¯RCDGenï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„åˆæˆæ•°æ®ç”Ÿæˆç®¡é“ï¼Œä»…ä½¿ç”¨å˜åŒ–å‰å›¾åƒå³å¯ç”ŸæˆæŒ‡å®šç±»åˆ«çš„çœŸå®å˜åŒ–åå›¾åƒå’Œå˜åŒ–å›¾ï¼Œæ— éœ€ä¾èµ–è¯­ä¹‰åˆ†å‰²æ©ç ã€‚

**Result:** åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å®ç°äº†å¯æ‰©å±•å’Œæœ‰é’ˆå¯¹æ€§çš„å˜åŒ–æ£€æµ‹ã€‚RCDGenæ˜¾è‘—é™ä½äº†å¤§è§„æ¨¡æ•°æ®åˆ›å»ºçš„éšœç¢ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œæœ‰æ•ˆç¼“è§£äº†æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥è‡ªç„¶è¯­è¨€æç¤ºå’Œåˆæˆæ•°æ®ç”Ÿæˆï¼Œä¸ºé¥æ„Ÿå˜åŒ–æ£€æµ‹æä¾›äº†æ›´çµæ´»å’Œå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚RCDæ¡†æ¶å…è®¸ç”¨æˆ·æŒ‡å®šæ„Ÿå…´è¶£çš„å…·ä½“å˜åŒ–ç±»å‹ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•åœ¨ç±»åˆ«å®šä¹‰å’Œæ¨¡å‹é‡ç”¨æ–¹é¢çš„é™åˆ¶ï¼Œä¸ºå®é™…åº”ç”¨åœºæ™¯æä¾›äº†æ›´å¼ºçš„é€‚åº”æ€§ã€‚

---

#### ğŸ“„ Abstract
Change detection in remote sensing imagery is essential for applications such as urban planning, environmental monitoring, and disaster management. Traditional change detection methods typically identify all changes between two temporal images without distinguishing the types of transitions, which can lead to results that may not align with specific user needs. Although semantic change detection methods have attempted to address this by categorizing changes into predefined classes, these methods rely on rigid class definitions and fixed model architectures, making it difficult to mix datasets with different label sets or reuse models across tasks, as the output channels are tightly coupled with the number and type of semantic classes. To overcome these limitations, we introduce Referring Change Detection (RCD), which leverages natural language prompts to detect specific classes of changes in remote sensing images. By integrating language understanding with visual analysis, our approach allows users to specify the exact type of change they are interested in. However, training models for RCD is challenging due to the limited availability of annotated data and severe class imbalance in existing datasets. To address this, we propose a two-stage framework consisting of (I) \textbf{RCDNet}, a cross-modal fusion network designed for referring change detection, and (II) \textbf{RCDGen}, a diffusion-based synthetic data generation pipeline that produces realistic post-change images and change maps for a specified category using only pre-change image, without relying on semantic segmentation masks and thereby significantly lowering the barrier to scalable data creation. Experiments across multiple datasets show that our framework enables scalable and targeted change detection. Project website is here: https://yilmazkorkmaz1.github.io/RCD.


### [33] [Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation](https://arxiv.org/abs/2512.11720)
*Yan Zhang, Han Zou, Lincong Feng, Cong Xie, Ruiqi Yu, Zhenpeng Zhan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„éŸ³ä¹åˆ°èˆè¹ˆç”Ÿæˆæ–¹æ³•ï¼Œå°†2Då§¿æ€åºåˆ—ç¼–ç ä¸ºå•çƒ­å›¾åƒï¼Œåˆ©ç”¨é¢„è®­ç»ƒå›¾åƒVAEè¿›è¡Œå‹ç¼©ï¼Œå¹¶é‡‡ç”¨DiTé£æ ¼ä¸»å¹²ç½‘ç»œè¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰é«˜æ–¹å·®å§¿æ€åˆ†å¸ƒã€‚è¯¥æ–¹æ³•é€šè¿‡æ—¶é—´å…±äº«ç´¢å¼•æ–¹æ¡ˆå’Œå‚è€ƒå§¿æ€æ¡ä»¶ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆèˆè¹ˆå§¿æ€çš„æ—¶é—´ä¸€è‡´æ€§å’ŒèŠ‚å¥å¯¹é½æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å§¿æ€åˆ°è§†é¢‘æ¨¡å‹èƒ½å¤Ÿå°†2Då§¿æ€åºåˆ—è½¬æ¢ä¸ºé€¼çœŸçš„èº«ä»½ä¿æŒèˆè¹ˆè§†é¢‘ï¼Œä½†å…³é”®æŒ‘æˆ˜åœ¨äºä»éŸ³ä¹ç”Ÿæˆæ—¶é—´ä¸€è‡´ã€èŠ‚å¥å¯¹é½çš„2Då§¿æ€ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ã€é«˜æ–¹å·®çš„é‡å¤–åˆ†å¸ƒåœºæ™¯ä¸‹ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³éŸ³ä¹åˆ°èˆè¹ˆç”Ÿæˆä¸­å§¿æ€åºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§å’ŒéŸ³ä¹èŠ‚å¥å¯¹é½é—®é¢˜ã€‚

**Method:** è¯¥æ–¹æ³•å°†éŸ³ä¹åˆ°èˆè¹ˆç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºéŸ³ä¹æ ‡è®°æ¡ä»¶çš„å¤šé€šé“å›¾åƒåˆæˆé—®é¢˜ï¼šå°†2Då§¿æ€åºåˆ—ç¼–ç ä¸ºå•çƒ­å›¾åƒï¼Œä½¿ç”¨é¢„è®­ç»ƒå›¾åƒVAEè¿›è¡Œå‹ç¼©ï¼Œå¹¶é‡‡ç”¨DiTé£æ ¼ä¸»å¹²ç½‘ç»œè¿›è¡Œå»ºæ¨¡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†æ—¶é—´å…±äº«çš„æ—¶é—´ç´¢å¼•æ–¹æ¡ˆï¼Œæ˜¾å¼åŒæ­¥éŸ³ä¹æ ‡è®°å’Œå§¿æ€æ½œåœ¨è¡¨ç¤ºï¼›ä»¥åŠå‚è€ƒå§¿æ€æ¡ä»¶ç­–ç•¥ï¼Œä¿æŒä¸»ä½“ç‰¹å®šèº«ä½“æ¯”ä¾‹å’Œå±å¹•å°ºåº¦ï¼ŒåŒæ—¶æ”¯æŒé•¿æ—¶åŸŸåˆ†æ®µæ‹¼æ¥ç”Ÿæˆã€‚

**Result:** åœ¨å¤§å‹é‡å¤–2Dèˆè¹ˆè¯­æ–™åº“å’Œæ ¡å‡†çš„AIST++2DåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å§¿æ€ç©ºé—´å’Œè§†é¢‘ç©ºé—´æŒ‡æ ‡ä»¥åŠäººç±»åå¥½æ–¹é¢å‡ä¼˜äºä»£è¡¨æ€§éŸ³ä¹åˆ°èˆè¹ˆæ–¹æ³•ã€‚æ¶ˆèå®éªŒéªŒè¯äº†è¡¨ç¤ºæ–¹æ³•ã€æ—¶é—´ç´¢å¼•å’Œå‚è€ƒæ¡ä»¶ç­–ç•¥çš„æœ‰æ•ˆè´¡çŒ®ï¼Œå±•ç¤ºäº†åœ¨å¤æ‚åˆ†å¸ƒä¸‹ç”Ÿæˆé«˜è´¨é‡èˆè¹ˆå§¿æ€çš„èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡å°†éŸ³ä¹åˆ°èˆè¹ˆç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºå›¾åƒåˆæˆé—®é¢˜ï¼ŒæˆåŠŸç»§æ‰¿äº†ç°ä»£æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ¶æ„å’Œè®­ç»ƒä¼˜åŠ¿ï¼Œæ˜¾è‘—æå‡äº†å§¿æ€ç”Ÿæˆçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚æå‡ºçš„æ—¶é—´ç´¢å¼•å’Œå‚è€ƒæ¡ä»¶ç­–ç•¥ä¸ºè§£å†³é•¿æ—¶åŸŸç”Ÿæˆå’Œä¸»ä½“ä¿æŒé—®é¢˜æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä¸ºéŸ³ä¹é©±åŠ¨çš„èˆè¹ˆç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io


### [34] [SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder](https://arxiv.org/abs/2512.11749)
*Minglei Shi, Haolin Wang, Borui Zhang, Wenzhao Zheng, Bohan Zeng, Ziyang Yuan, Xiaoshi Wu, Yuanxing Zhang, Huan Yang, Xintao Wang, Pengfei Wan, Kun Gai, Jie Zhou, Jiwen Lu*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†SVG-T2Iæ¡†æ¶ï¼Œå°†è‡ªç›‘ç£è§†è§‰åŸºç¡€æ¨¡å‹è¡¨ç¤ºæ‰©å±•åˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå®ç°äº†åœ¨VFMç‰¹å¾ç©ºé—´ä¸­çš„é«˜è´¨é‡å›¾åƒåˆæˆï¼Œå¹¶å¼€æºäº†å®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†æµç¨‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡åŸºäºè§†è§‰åŸºç¡€æ¨¡å‹è¡¨ç¤ºçš„å¯è§†åŒ–ç”Ÿæˆä¸ºæ•´åˆè§†è§‰ç†è§£ã€æ„ŸçŸ¥å’Œç”Ÿæˆæä¾›äº†ç»Ÿä¸€è·¯å¾„ï¼Œä½†åœ¨VFMè¡¨ç¤ºç©ºé—´ä¸­å®Œå…¨è®­ç»ƒå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç ”ç©¶ä»å¤„äºæ¢ç´¢ä¸è¶³çš„çŠ¶æ€ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚

**Method:** ç ”ç©¶æ‰©å±•äº†SVGæ¡†æ¶ï¼Œæå‡ºSVG-T2Iæ–¹æ³•ï¼Œé‡‡ç”¨æ ‡å‡†çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ç®¡é“ç›´æ¥åœ¨VFMç‰¹å¾åŸŸä¸­è¿›è¡Œå›¾åƒåˆæˆï¼Œé€šè¿‡è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ å®ç°è§†è§‰ç”Ÿæˆä»»åŠ¡ã€‚

**Result:** SVG-T2Iåœ¨GenEvalåŸºå‡†ä¸Šè¾¾åˆ°0.75åˆ†ï¼Œåœ¨DPG-Benchä¸Šè·å¾—85.78åˆ†ï¼Œè¡¨ç°å‡ºä¸ç°æœ‰æ–¹æ³•ç«äº‰çš„æ€§èƒ½ï¼ŒéªŒè¯äº†VFMè¡¨ç¤ºåœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„å†…åœ¨è¡¨å¾èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯å®äº†è§†è§‰åŸºç¡€æ¨¡å‹è¡¨ç¤ºåœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡å¼€æºå®Œæ•´çš„é¡¹ç›®åŒ…æ‹¬è‡ªç¼–ç å™¨ã€ç”Ÿæˆæ¨¡å‹ã€è®­ç»ƒæ¨ç†æµç¨‹å’Œé¢„è®­ç»ƒæƒé‡ï¼Œä¸ºè¡¨ç¤ºé©±åŠ¨çš„è§†è§‰ç”Ÿæˆç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.


### [35] [MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator](https://arxiv.org/abs/2512.11782)
*Peiqing Yang, Shangchen Zhou, Kai Hao, Qingyi Tao*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€çœŸå®æ ‡æ³¨çš„æŠ å›¾è´¨é‡è¯„ä¼°å™¨ï¼ˆMQEï¼‰ï¼Œé€šè¿‡è¯¥è¯„ä¼°å™¨æ„å»ºäº†å¤§è§„æ¨¡çœŸå®ä¸–ç•Œè§†é¢‘æŠ å›¾æ•°æ®é›†VMRealï¼Œå¹¶å¼€å‘äº†MatAnyone 2æ¨¡å‹ï¼Œåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†é¢‘æŠ å›¾é¢†åŸŸå—åˆ°ç°æœ‰æ•°æ®é›†è§„æ¨¡å’ŒçœŸå®æ€§çš„é™åˆ¶ï¼Œè™½ç„¶åˆ©ç”¨åˆ†å‰²æ•°æ®å¯ä»¥å¢å¼ºè¯­ä¹‰ç¨³å®šæ€§ï¼Œä½†ç¼ºä¹æœ‰æ•ˆçš„è¾¹ç•Œç›‘ç£é€šå¸¸å¯¼è‡´æŠ å›¾ç»“æœå‘ˆç°åˆ†å‰²çŠ¶è€Œç¼ºä¹ç²¾ç»†ç»†èŠ‚ï¼Œè¿™é˜»ç¢äº†é«˜è´¨é‡è§†é¢‘æŠ å›¾æ¨¡å‹çš„å‘å±•ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†æ— éœ€çœŸå®æ ‡æ³¨çš„æŠ å›¾è´¨é‡è¯„ä¼°å™¨ï¼ˆMQEï¼‰ï¼Œè¯¥è¯„ä¼°å™¨èƒ½å¤Ÿè¯„ä¼°alphaæŠ å›¾çš„è¯­ä¹‰å’Œè¾¹ç•Œè´¨é‡ï¼Œå¹¶ç”Ÿæˆåƒç´ çº§è¯„ä¼°å›¾ä»¥è¯†åˆ«å¯é å’Œé”™è¯¯åŒºåŸŸã€‚MQEé€šè¿‡ä¸¤ç§æ–¹å¼æ‰©å±•è§†é¢‘æŠ å›¾ï¼šä½œä¸ºè®­ç»ƒæœŸé—´çš„åœ¨çº¿è´¨é‡åé¦ˆæœºåˆ¶æ¥æŠ‘åˆ¶é”™è¯¯åŒºåŸŸï¼Œä»¥åŠä½œä¸ºç¦»çº¿æ•°æ®ç­›é€‰æ¨¡å—ç”¨äºæ•°æ®æ•´ç†ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜å¼•å…¥äº†å‚è€ƒå¸§è®­ç»ƒç­–ç•¥æ¥å¤„ç†é•¿è§†é¢‘ä¸­çš„å¤–è§‚å˜åŒ–ï¼Œå¹¶æ„å»ºäº†åŒ…å«28Kä¸ªè§†é¢‘ç‰‡æ®µå’Œ240ä¸‡å¸§çš„å¤§è§„æ¨¡çœŸå®ä¸–ç•Œè§†é¢‘æŠ å›¾æ•°æ®é›†VMRealã€‚

**Result:** åŸºäºMQEå’ŒVMRealæ•°æ®é›†å¼€å‘çš„MatAnyone 2æ¨¡å‹åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šè¶…è¶Šäº†å…ˆå‰çš„æ–¹æ³•ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿è§†é¢‘ä¸­çš„å¤–è§‚å˜åŒ–ï¼Œå¹¶ç”Ÿæˆå…·æœ‰ç²¾ç»†ç»†èŠ‚çš„é«˜è´¨é‡æŠ å›¾ç»“æœã€‚

**Conclusion:** æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥æ— éœ€çœŸå®æ ‡æ³¨çš„æŠ å›¾è´¨é‡è¯„ä¼°å™¨å’Œæ„å»ºå¤§è§„æ¨¡çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘æŠ å›¾çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¸ºè§†é¢‘æŠ å›¾æä¾›äº†æœ‰æ•ˆçš„è´¨é‡ç›‘ç£æœºåˆ¶ï¼Œè¿˜ä¸ºæ„å»ºé«˜è´¨é‡è®­ç»ƒæ•°æ®å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [36] [MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data](https://arxiv.org/abs/2512.11074)
*Christopher Driggers-Ellis, Detravious Brinkley, Ray Chen, Aashish Dhawan, Daisy Zhe Wang, Christan Grant*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MultiScript30kï¼Œè¿™æ˜¯Multi30kæ•°æ®é›†çš„ä¸€ä¸ªæ–°æ‰©å±•ï¼Œé€šè¿‡ä½¿ç”¨NLLB200-3.3Bæ¨¡å‹å°†è‹±æ–‡ç‰ˆMulti30kç¿»è¯‘æˆå¤šç§è„šæœ¬è¯­è¨€ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘æ•°æ®é›†ä»…é™äºå°‘æ•°æ¬§æ´²è¯­è¨€å’Œæ‹‰ä¸è„šæœ¬çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰Multi30kæ•°æ®é›†ä»…æ”¯æŒæ·å…‹è¯­ã€è‹±è¯­ã€æ³•è¯­å’Œå¾·è¯­è¿™å››ç§æ¬§æ´²è¯­è¨€ï¼Œä¸”å‡ä¸ºæ‹‰ä¸è„šæœ¬ï¼Œè¿™é™åˆ¶äº†å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ç ”ç©¶åœ¨å¤šæ ·åŒ–è¯­è¨€ä¸Šçš„å‘å±•ã€‚ç”±äºå®˜æ–¹æ•°æ®é›†ä»…ä»£è¡¨æ¬§æ´²è¯­è¨€ï¼Œå¯¼è‡´å¯¹éæ‹‰ä¸è„šæœ¬å’Œå…¨çƒå¤šæ ·åŒ–è¯­è¨€çš„ç ”ç©¶è¿›å±•å—é˜»ï¼Œå…ˆå‰æ‰©å±•å°è¯•æ”¯æŒçš„è¯­è¨€ç§ç±»ã€è¯­ç³»å’Œè„šæœ¬ä»ç„¶éå¸¸æœ‰é™ã€‚

**Method:** ç ”ç©¶æå‡ºMultiScript30kæ•°æ®é›†ï¼Œé€šè¿‡ä½¿ç”¨NLLB200-3.3Bç¿»è¯‘æ¨¡å‹å°†è‹±æ–‡ç‰ˆMulti30kæ•°æ®é›†ç¿»è¯‘æˆå¤šç§å…¨çƒè¯­è¨€ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡30000ä¸ªå¥å­ï¼Œæä¾›äº†Multi30k-Enä¸­æ‰€æœ‰å¥å­åˆ°é˜¿æ‹‰ä¼¯è¯­ã€è¥¿ç­ç‰™è¯­ã€ä¹Œå…‹å…°è¯­ã€ç®€ä½“ä¸­æ–‡å’Œç¹ä½“ä¸­æ–‡çš„ç¿»è¯‘ï¼Œè¦†ç›–äº†å¤šç§è„šæœ¬ç³»ç»Ÿã€‚

**Result:** ç›¸ä¼¼æ€§åˆ†ææ˜¾ç¤ºï¼Œé™¤ç¹ä½“ä¸­æ–‡å¤–ï¼ŒMultiScript30kæ‰©å±•åœ¨æ‰€æœ‰æ”¯æŒè¯­è¨€ä¸Šå‡å®ç°äº†å¤§äº0.8çš„ä½™å¼¦ç›¸ä¼¼åº¦å’Œå°äº0.000251çš„å¯¹ç§°KLæ•£åº¦ï¼Œä¸å…ˆå‰æ‰©å±•ArEnMulti30kå’ŒMulti30k-Ukç›¸å½“ã€‚COMETKiwiè¯„ä¼°æ˜¾ç¤ºæ··åˆç»“æœï¼šArEnMulti30kä¸MultiScript30k-Arå¾—åˆ†ç›¸è¿‘ï¼Œä½†Multi30k-Ukæ¯”MultiScript30k-Uké«˜å‡º6.4%ã€‚

**Conclusion:** MultiScript30kä¸ºå¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ç ”ç©¶æä¾›äº†æ›´å¹¿æ³›çš„è¯­è¨€è¦†ç›–ï¼Œç‰¹åˆ«æ˜¯éæ‹‰ä¸è„šæœ¬è¯­è¨€ï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸåœ¨å¤šæ ·åŒ–è¯­è¨€ä¸Šçš„å‘å±•ã€‚å°½ç®¡åœ¨æŸäº›è¯­è¨€ä¸Šä¸ç°æœ‰æ‰©å±•ç›¸æ¯”å­˜åœ¨æ€§èƒ½å·®å¼‚ï¼Œä½†è¯¥æ•°æ®é›†ä¸ºç ”ç©¶å…¨çƒè¯­è¨€çš„å¤šæ¨¡æ€ç¿»è¯‘æä¾›äº†é‡è¦èµ„æºï¼Œå¹¶å±•ç¤ºäº†ä½¿ç”¨å¤§è§„æ¨¡ç¿»è¯‘æ¨¡å‹æ‰©å±•ç°æœ‰æ•°æ®é›†çš„å¯è¡Œæ€§ã€‚

---

#### ğŸ“„ Abstract
Multi30k is frequently cited in the multimodal machine translation (MMT) literature, offering parallel text data for training and fine-tuning deep learning models. However, it is limited to four languages: Czech, English, French, and German. This restriction has led many researchers to focus their investigations only on these languages. As a result, MMT research on diverse languages has been stalled because the official Multi30k dataset only represents European languages in Latin scripts. Previous efforts to extend Multi30k exist, but the list of supported languages, represented language families, and scripts is still very short. To address these issues, we propose MultiScript30k, a new Multi30k dataset extension for global languages in various scripts, created by translating the English version of Multi30k (Multi30k-En) using NLLB200-3.3B. The dataset consists of over \(30000\) sentences and provides translations of all sentences in Multi30k-En into Ar, Es, Uk, Zh\_Hans and Zh\_Hant. Similarity analysis shows that Multi30k extension consistently achieves greater than \(0.8\) cosine similarity and symmetric KL divergence less than \(0.000251\) for all languages supported except Zh\_Hant which is comparable to the previous Multi30k extensions ArEnMulti30k and Multi30k-Uk. COMETKiwi scores reveal mixed assessments of MultiScript30k as a translation of Multi30k-En in comparison to the related work. ArEnMulti30k scores nearly equal MultiScript30k-Ar, but Multi30k-Uk scores $6.4\%$ greater than MultiScript30k-Uk per split.


### [37] [Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction](https://arxiv.org/abs/2512.11399)
*Galann Pennec, Zhengyuan Liu, Nicholas Asher, Philippe Muller, Nancy F. Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹é•¿è§†é¢‘çš„å¤šæ¨¡æ€æ‘˜è¦ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§è§†é¢‘æè¿°æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹åä½œé€‰æ‹©å…³é”®è§†é¢‘ç‰‡æ®µï¼Œåœ¨ä¿æŒä½è®¡ç®—æˆæœ¬çš„åŒæ—¶æ˜¾è‘—æå‡æ‘˜è¦è´¨é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€è§†è§‰è¯­è¨€æ¨¡å‹å¤„ç†è§†é¢‘é•¿åº¦çš„å¢åŠ ï¼Œé‡è¦è§†è§‰ä¿¡æ¯å®¹æ˜“åœ¨æ•´ä¸ªä¸Šä¸‹æ–‡ä¸­ä¸¢å¤±ï¼ŒåŒæ—¶éœ€è¦è®¾è®¡èƒ½å¤Ÿç»æµé«˜æ•ˆåˆ†æé•¿è§†é¢‘å†…å®¹çš„å·¥å…·ï¼Œä»¥è§£å†³ç°æœ‰æ–¹æ³•åœ¨é•¿è§†é¢‘æ‘˜è¦ä¸­ä¿¡æ¯é—æ¼å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚

**Method:** è¯¥æ–¹æ³•å°†è§†é¢‘åˆ†å‰²ä¸ºçŸ­ç‰‡æ®µï¼Œä½¿ç”¨è½»é‡çº§è§†é¢‘æè¿°æ¨¡å‹ä¸ºæ¯ä¸ªç‰‡æ®µç”Ÿæˆç´§å‡‘çš„è§†è§‰æè¿°ï¼Œç„¶åå°†è¿™äº›æè¿°è¾“å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”±LLMé€‰æ‹©åŒ…å«æœ€ç›¸å…³è§†è§‰ä¿¡æ¯çš„Kä¸ªç‰‡æ®µç”¨äºæ„å»ºå¤šæ¨¡æ€æ‘˜è¦ï¼Œå®ç°äº†è®¡ç®—æ•ˆç‡ä¸æ‘˜è¦è´¨é‡çš„å¹³è¡¡ã€‚

**Result:** åœ¨MovieSumæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„æ‘˜è¦æ€§èƒ½æ¥è¿‘äººå·¥æ ‡æ³¨çš„å‚è€ƒç‰‡æ®µï¼ˆä»…å ç”µå½±æ€»æ—¶é•¿ä¸åˆ°6%ï¼‰ï¼ŒåŒæ—¶æ¯”éšæœºç‰‡æ®µé€‰æ‹©æ•è·äº†æ›´å¤šç›¸å…³è§†é¢‘ä¿¡æ¯ï¼ŒéªŒè¯äº†è½»é‡çº§æè¿°æ¨¡å‹ä¸LLMåä½œçš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ç‰‡æ®µé€‰æ‹©æœºåˆ¶ï¼Œä»…éœ€å°‘é‡å…³é”®è§†é¢‘ç‰‡æ®µå³å¯æ„å»ºå®Œæ•´çš„ç”µå½±å¤šæ¨¡æ€æ‘˜è¦ï¼Œä¸ºé•¿è§†é¢‘å†…å®¹åˆ†ææä¾›äº†é«˜æ•ˆå®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶è¯æ˜äº†è½»é‡çº§æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ååŒå·¥ä½œçš„å¯è¡Œæ€§ã€‚

---

#### ğŸ“„ Abstract
Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.


### [38] [Extending a Parliamentary Corpus with MPs' Tweets: Automatic Annotation and Evaluation Using MultiParTweet](https://arxiv.org/abs/2512.11567)
*MevlÃ¼t Bagci, Ali Abusaleh, Daniel Baumartz, Giueseppe Abrami, Maxim Konca, Alexander Mehler*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†MultiParTweetï¼Œä¸€ä¸ªè¿æ¥å¾·å›½è®®ä¼šè¯­æ–™åº“çš„å¤šè¯­è¨€æ¨ç‰¹è¯­æ–™åº“ï¼Œä»¥åŠTTLABTweetCrawleræ•°æ®æ”¶é›†å·¥å…·ï¼Œé€šè¿‡ä¹ç§æ–‡æœ¬æ¨¡å‹å’Œä¸€ç§è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ï¼ŒéªŒè¯äº†æ¨¡å‹é—´çš„ç›¸äº’å¯é¢„æµ‹æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç¤¾äº¤åª’ä½“åœ¨ç°ä»£æ”¿æ²»ä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œä½†ç¼ºä¹è¿æ¥æ”¿æ²»å®¶åœ¨çº¿è¯è¯­ä¸è®®ä¼šè¾©è®ºçš„ç³»ç»Ÿæ€§å¤šè¯­è¨€è¯­æ–™åº“ï¼Œé™åˆ¶äº†æ¯”è¾ƒåˆ†æå’Œè·¨å¹³å°æ”¿æ²»æ²Ÿé€šç ”ç©¶ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†åŒ…å«39,546æ¡æ¨æ–‡ï¼ˆå«19,056ä¸ªåª’ä½“é¡¹ç›®ï¼‰çš„MultiParTweetè¯­æ–™åº“ï¼Œè¿æ¥å¾·å›½æ”¿æ²»è¯­æ–™åº“GerParCorï¼Œä½¿ç”¨ä¹ç§æ–‡æœ¬æ¨¡å‹å’Œä¸€ç§è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿã€æƒ…ç»ªå’Œä¸»é¢˜è‡ªåŠ¨æ ‡æ³¨ï¼Œå¹¶å¼€å‘äº†TTLABTweetCrawlerå·¥å…·ç”¨äºXå¹³å°æ•°æ®æ”¶é›†ã€‚

**Result:** å®éªŒè¡¨æ˜ä¸åŒæ¨¡å‹è¾“å‡ºä¹‹é—´å­˜åœ¨ç›¸äº’å¯é¢„æµ‹æ€§ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹æ ‡æ³¨ç»“æœä¸äººå·¥æ ‡æ³¨è€…åå¥½æ›´ä¸€è‡´ï¼Œè‡ªåŠ¨æ ‡æ³¨ä¸äººå·¥æ ‡æ³¨å­é›†è¿›è¡Œäº†éªŒè¯è¯„ä¼°ï¼Œè¯æ˜äº†å¤šæ¨¡æ€è¡¨ç¤ºä¸äººç±»è§£é‡Šæ›´å¥‘åˆã€‚

**Conclusion:** ç ”ç©¶æä¾›äº†æ•´åˆè‡ªåŠ¨æ–‡æœ¬ä¸åª’ä½“æ ‡æ³¨çš„æ ‡å‡†åŒ–èµ„æºï¼Œå±•ç¤ºäº†æ¨¡å‹é—´å¯é¢„æµ‹æ€§çš„åˆ†ææ–¹æ³•ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ”¿æ²»è¯è¯­åˆ†æä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œä¸ºè·¨å¹³å°æ”¿æ²»æ²Ÿé€šç ”ç©¶æä¾›äº†æ–¹æ³•è®ºæ¡†æ¶å’Œæ•°æ®åŸºç¡€è®¾æ–½ã€‚

---

#### ğŸ“„ Abstract
Social media serves as a critical medium in modern politics because it both reflects politicians' ideologies and facilitates communication with younger generations. We present MultiParTweet, a multilingual tweet corpus from X that connects politicians' social media discourse with German political corpus GerParCor, thereby enabling comparative analyses between online communication and parliamentary debates. MultiParTweet contains 39 546 tweets, including 19 056 media items. Furthermore, we enriched the annotation with nine text-based models and one vision-language model (VLM) to annotate MultiParTweet with emotion, sentiment, and topic annotations. Moreover, the automated annotations are evaluated against a manually annotated subset. MultiParTweet can be reconstructed using our tool, TTLABTweetCrawler, which provides a framework for collecting data from X. To demonstrate a methodological demonstration, we examine whether the models can predict each other using the outputs of the remaining models. In summary, we provide MultiParTweet, a resource integrating automatic text and media-based annotations validated with human annotations, and TTLABTweetCrawler, a general-purpose X data collection tool. Our analysis shows that the models are mutually predictable. In addition, VLM-based annotation were preferred by human annotators, suggesting that multimodal representations align more with human interpretation.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving](https://arxiv.org/abs/2512.11323)
*Jianyi Zhang, Ziyin Zhou, Xu Ji, Shizhao Liu, Zhangchi Zhao*

#### ğŸ§© TL;DR
æœ¬æ–‡é¦–æ¬¡ä¸ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªåä¸ºCAPTUREçš„CAPTCHAåŸºå‡†æµ‹è¯•ï¼Œè¯¥åŸºå‡†æ¶µç›–4ç§ä¸»è¦ç±»å‹å’Œ25ç§å­ç±»å‹ï¼Œæ¥è‡ª31ä¸ªä¾›åº”å•†ï¼Œç”¨äºå…¨é¢è¯„ä¼°LVLMåœ¨è§£å†³éªŒè¯ç æ–¹é¢çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºè§†è§‰éªŒè¯ç çš„åŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ï¼Œå…ˆå‰ç ”ç©¶æ ¹æ®ç‰¹å®šç›®æ ‡å®šåˆ¶çš„åŸºå‡†æ— æ³•å…¨é¢è¦†ç›–æ‰€æœ‰éªŒè¯ç ç±»å‹ï¼Œä¸”ç¼ºä¹ä¸“é—¨é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸“ç”¨åŸºå‡†ï¼Œè¿™é˜»ç¢äº†å¯¹LVLMè§£å†³éªŒè¯ç èƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚

**Method:** ç ”ç©¶æå‡ºäº†åä¸ºCAPTUREï¼ˆCAPTCHA for Testing Under Real-world Experimentsï¼‰çš„æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œè¯¥åŸºå‡†ç³»ç»Ÿæ€§åœ°æ¶µç›–äº†4ç§ä¸»è¦éªŒè¯ç ç±»å‹å’Œ25ç§å­ç±»å‹ï¼Œæ”¶é›†è‡ª31ä¸ªä¸åŒä¾›åº”å•†ï¼Œå…·æœ‰å¹¿æ³›çš„ç±»åˆ«å¤šæ ·æ€§ã€å¤§è§„æ¨¡æ•°æ®å’Œä¸“é—¨ä¸ºLVLMå®šåˆ¶çš„æ ‡ç­¾ä½“ç³»ã€‚

**Result:** ä½¿ç”¨CAPTUREåŸºå‡†å¯¹å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°æ—¶ï¼Œç»“æœæ˜¾ç¤ºè¿™äº›æ¨¡å‹åœ¨è§£å†³éªŒè¯ç æ–¹é¢è¡¨ç°ä¸ä½³ï¼ŒéªŒè¯äº†ç°æœ‰LVLMåœ¨åº”å¯¹å¤šæ ·åŒ–çœŸå®ä¸–ç•ŒéªŒè¯ç æŒ‘æˆ˜æ—¶çš„å±€é™æ€§ã€‚

**Conclusion:** CAPTUREåŸºå‡†å¡«è¡¥äº†å…ˆå‰ç ”ç©¶åœ¨æ•°æ®å…¨é¢æ€§å’Œæ ‡ç­¾é’ˆå¯¹æ€§æ–¹é¢çš„ç©ºç™½ï¼Œä¸ºLVLMçš„éªŒè¯ç è§£å†³èƒ½åŠ›æä¾›äº†å¤šç»´åº¦çš„å…¨é¢è¯„ä¼°æ¡†æ¶ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­çš„æ€§èƒ½ç¼ºé™·ï¼Œä¸ºæœªæ¥æ¨¡å‹æ”¹è¿›æä¾›äº†é‡è¦å‚è€ƒä¾æ®ã€‚

---

#### ğŸ“„ Abstract
Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.
