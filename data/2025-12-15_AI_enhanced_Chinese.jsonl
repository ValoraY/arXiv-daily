{"id": "2512.11323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11323", "abs": "https://arxiv.org/abs/2512.11323", "authors": ["Jianyi Zhang", "Ziyin Zhou", "Xu Ji", "Shizhao Liu", "Zhangchi Zhao"], "title": "CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving", "comment": null, "summary": "Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4e3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aCAPTURE\u7684CAPTCHA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u57fa\u51c6\u6db5\u76d64\u79cd\u4e3b\u8981\u7c7b\u578b\u548c25\u79cd\u5b50\u7c7b\u578b\uff0c\u6765\u81ea31\u4e2a\u4f9b\u5e94\u5546\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30LVLM\u5728\u89e3\u51b3\u9a8c\u8bc1\u7801\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u9a8c\u8bc1\u7801\u7684\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5148\u524d\u7814\u7a76\u6839\u636e\u7279\u5b9a\u76ee\u6807\u5b9a\u5236\u7684\u57fa\u51c6\u65e0\u6cd5\u5168\u9762\u8986\u76d6\u6240\u6709\u9a8c\u8bc1\u7801\u7c7b\u578b\uff0c\u4e14\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e13\u7528\u57fa\u51c6\uff0c\u8fd9\u963b\u788d\u4e86\u5bf9LVLM\u89e3\u51b3\u9a8c\u8bc1\u7801\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u540d\u4e3aCAPTURE\uff08CAPTCHA for Testing Under Real-world Experiments\uff09\u7684\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u57fa\u51c6\u7cfb\u7edf\u6027\u5730\u6db5\u76d6\u4e864\u79cd\u4e3b\u8981\u9a8c\u8bc1\u7801\u7c7b\u578b\u548c25\u79cd\u5b50\u7c7b\u578b\uff0c\u6536\u96c6\u81ea31\u4e2a\u4e0d\u540c\u4f9b\u5e94\u5546\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u7c7b\u522b\u591a\u6837\u6027\u3001\u5927\u89c4\u6a21\u6570\u636e\u548c\u4e13\u95e8\u4e3aLVLM\u5b9a\u5236\u7684\u6807\u7b7e\u4f53\u7cfb\u3002", "result": "\u4f7f\u7528CAPTURE\u57fa\u51c6\u5bf9\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u65f6\uff0c\u7ed3\u679c\u663e\u793a\u8fd9\u4e9b\u6a21\u578b\u5728\u89e3\u51b3\u9a8c\u8bc1\u7801\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9a8c\u8bc1\u4e86\u73b0\u6709LVLM\u5728\u5e94\u5bf9\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u7801\u6311\u6218\u65f6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "CAPTURE\u57fa\u51c6\u586b\u8865\u4e86\u5148\u524d\u7814\u7a76\u5728\u6570\u636e\u5168\u9762\u6027\u548c\u6807\u7b7e\u9488\u5bf9\u6027\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3aLVLM\u7684\u9a8c\u8bc1\u7801\u89e3\u51b3\u80fd\u529b\u63d0\u4f9b\u4e86\u591a\u7ef4\u5ea6\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u7684\u6027\u80fd\u7f3a\u9677\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u4f9d\u636e\u3002"}}
{"id": "2512.11015", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11015", "abs": "https://arxiv.org/abs/2512.11015", "authors": ["Anoop Krishnan"], "title": "Leveraging Text Guidance for Enhancing Demographic Fairness in Gender Classification", "comment": null, "summary": "In the quest for fairness in artificial intelligence, novel approaches to enhance it in facial image based gender classification algorithms using text guided methodologies are presented. The core methodology involves leveraging semantic information from image captions during model training to improve generalization capabilities. Two key strategies are presented: Image Text Matching (ITM) guidance and Image Text fusion. ITM guidance trains the model to discern fine grained alignments between images and texts to obtain enhanced multimodal representations. Image text fusion combines both modalities into comprehensive representations for improved fairness. Exensive experiments conducted on benchmark datasets demonstrate these approaches effectively mitigate bias and improve accuracy across gender racial groups compared to existing methods. Additionally, the unique integration of textual guidance underscores an interpretable and intuitive training paradigm for computer vision systems. By scrutinizing the extent to which semantic information reduces disparities, this research offers valuable insights into cultivating more equitable facial analysis algorithms. The proposed methodologies contribute to addressing the pivotal challenge of demographic bias in gender classification from facial images. Furthermore, this technique operates in the absence of demographic labels and is application agnostic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6587\u672c\u5f15\u5bfc\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u9762\u90e8\u56fe\u50cf\u6027\u522b\u5206\u7c7b\u7b97\u6cd5\u7684\u516c\u5e73\u6027\uff1a\u56fe\u50cf\u6587\u672c\u5339\u914d\u5f15\u5bfc\u548c\u56fe\u50cf\u6587\u672c\u878d\u5408\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5229\u7528\u56fe\u50cf\u6807\u9898\u7684\u8bed\u4e49\u4fe1\u606f\u6765\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5e76\u51cf\u5c11\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u9762\u90e8\u56fe\u50cf\u6027\u522b\u5206\u7c7b\u7b97\u6cd5\u4e2d\u5b58\u5728\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\u95ee\u9898\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u8de8\u6027\u522b\u548c\u79cd\u65cf\u7fa4\u4f53\u4e0a\u5b58\u5728\u6027\u80fd\u5dee\u5f02\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u51cf\u5c11\u8fd9\u4e9b\u5dee\u5f02\u5e76\u63d0\u9ad8\u7b97\u6cd5\u516c\u5e73\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u6838\u5fc3\u65b9\u6cd5\uff1a\u56fe\u50cf\u6587\u672c\u5339\u914d\u5f15\u5bfc\u8bad\u7ec3\u6a21\u578b\u5b66\u4e60\u56fe\u50cf\u4e0e\u6587\u672c\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u4ee5\u83b7\u5f97\u589e\u5f3a\u7684\u591a\u6a21\u6001\u8868\u793a\uff1b\u56fe\u50cf\u6587\u672c\u878d\u5408\u5c06\u4e24\u79cd\u6a21\u6001\u7ed3\u5408\u4e3a\u7efc\u5408\u8868\u793a\u4ee5\u6539\u5584\u516c\u5e73\u6027\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u9700\u4eba\u53e3\u7edf\u8ba1\u6807\u7b7e\u4e14\u5177\u6709\u5e94\u7528\u65e0\u5173\u6027\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u51cf\u8f7b\u4e86\u504f\u89c1\uff0c\u5e76\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u6027\u522b\u548c\u79cd\u65cf\u7fa4\u4f53\u4e0a\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u8bed\u4e49\u4fe1\u606f\u5728\u51cf\u5c11\u6027\u80fd\u5dee\u5f02\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u516c\u5e73\u7684\u9762\u90e8\u5206\u6790\u7b97\u6cd5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u6587\u672c\u5f15\u5bfc\u65b9\u6cd5\u5728\u63d0\u9ad8\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u516c\u5e73\u6027\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u4e3a\u53ef\u89e3\u91ca\u548c\u76f4\u89c2\u7684\u8bad\u7ec3\u8303\u5f0f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u89e3\u51b3\u9762\u90e8\u56fe\u50cf\u6027\u522b\u5206\u7c7b\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\u6311\u6218\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2512.11074", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.11074", "abs": "https://arxiv.org/abs/2512.11074", "authors": ["Christopher Driggers-Ellis", "Detravious Brinkley", "Ray Chen", "Aashish Dhawan", "Daisy Zhe Wang", "Christan Grant"], "title": "MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data", "comment": "7 pages, 2 figures, 5 tables. Not published at any conference at this time", "summary": "Multi30k is frequently cited in the multimodal machine translation (MMT) literature, offering parallel text data for training and fine-tuning deep learning models. However, it is limited to four languages: Czech, English, French, and German. This restriction has led many researchers to focus their investigations only on these languages. As a result, MMT research on diverse languages has been stalled because the official Multi30k dataset only represents European languages in Latin scripts. Previous efforts to extend Multi30k exist, but the list of supported languages, represented language families, and scripts is still very short. To address these issues, we propose MultiScript30k, a new Multi30k dataset extension for global languages in various scripts, created by translating the English version of Multi30k (Multi30k-En) using NLLB200-3.3B. The dataset consists of over \\(30000\\) sentences and provides translations of all sentences in Multi30k-En into Ar, Es, Uk, Zh\\_Hans and Zh\\_Hant. Similarity analysis shows that Multi30k extension consistently achieves greater than \\(0.8\\) cosine similarity and symmetric KL divergence less than \\(0.000251\\) for all languages supported except Zh\\_Hant which is comparable to the previous Multi30k extensions ArEnMulti30k and Multi30k-Uk. COMETKiwi scores reveal mixed assessments of MultiScript30k as a translation of Multi30k-En in comparison to the related work. ArEnMulti30k scores nearly equal MultiScript30k-Ar, but Multi30k-Uk scores $6.4\\%$ greater than MultiScript30k-Uk per split.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MultiScript30k\uff0c\u8fd9\u662fMulti30k\u6570\u636e\u96c6\u7684\u4e00\u4e2a\u65b0\u6269\u5c55\uff0c\u901a\u8fc7\u4f7f\u7528NLLB200-3.3B\u6a21\u578b\u5c06\u82f1\u6587\u7248Multi30k\u7ffb\u8bd1\u6210\u591a\u79cd\u811a\u672c\u8bed\u8a00\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u96c6\u4ec5\u9650\u4e8e\u5c11\u6570\u6b27\u6d32\u8bed\u8a00\u548c\u62c9\u4e01\u811a\u672c\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709Multi30k\u6570\u636e\u96c6\u4ec5\u652f\u6301\u6377\u514b\u8bed\u3001\u82f1\u8bed\u3001\u6cd5\u8bed\u548c\u5fb7\u8bed\u8fd9\u56db\u79cd\u6b27\u6d32\u8bed\u8a00\uff0c\u4e14\u5747\u4e3a\u62c9\u4e01\u811a\u672c\uff0c\u8fd9\u9650\u5236\u4e86\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\u7814\u7a76\u5728\u591a\u6837\u5316\u8bed\u8a00\u4e0a\u7684\u53d1\u5c55\u3002\u7531\u4e8e\u5b98\u65b9\u6570\u636e\u96c6\u4ec5\u4ee3\u8868\u6b27\u6d32\u8bed\u8a00\uff0c\u5bfc\u81f4\u5bf9\u975e\u62c9\u4e01\u811a\u672c\u548c\u5168\u7403\u591a\u6837\u5316\u8bed\u8a00\u7684\u7814\u7a76\u8fdb\u5c55\u53d7\u963b\uff0c\u5148\u524d\u6269\u5c55\u5c1d\u8bd5\u652f\u6301\u7684\u8bed\u8a00\u79cd\u7c7b\u3001\u8bed\u7cfb\u548c\u811a\u672c\u4ecd\u7136\u975e\u5e38\u6709\u9650\u3002", "method": "\u7814\u7a76\u63d0\u51faMultiScript30k\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4f7f\u7528NLLB200-3.3B\u7ffb\u8bd1\u6a21\u578b\u5c06\u82f1\u6587\u7248Multi30k\u6570\u636e\u96c6\u7ffb\u8bd1\u6210\u591a\u79cd\u5168\u7403\u8bed\u8a00\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc730000\u4e2a\u53e5\u5b50\uff0c\u63d0\u4f9b\u4e86Multi30k-En\u4e2d\u6240\u6709\u53e5\u5b50\u5230\u963f\u62c9\u4f2f\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u4e4c\u514b\u5170\u8bed\u3001\u7b80\u4f53\u4e2d\u6587\u548c\u7e41\u4f53\u4e2d\u6587\u7684\u7ffb\u8bd1\uff0c\u8986\u76d6\u4e86\u591a\u79cd\u811a\u672c\u7cfb\u7edf\u3002", "result": "\u76f8\u4f3c\u6027\u5206\u6790\u663e\u793a\uff0c\u9664\u7e41\u4f53\u4e2d\u6587\u5916\uff0cMultiScript30k\u6269\u5c55\u5728\u6240\u6709\u652f\u6301\u8bed\u8a00\u4e0a\u5747\u5b9e\u73b0\u4e86\u5927\u4e8e0.8\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u5c0f\u4e8e0.000251\u7684\u5bf9\u79f0KL\u6563\u5ea6\uff0c\u4e0e\u5148\u524d\u6269\u5c55ArEnMulti30k\u548cMulti30k-Uk\u76f8\u5f53\u3002COMETKiwi\u8bc4\u4f30\u663e\u793a\u6df7\u5408\u7ed3\u679c\uff1aArEnMulti30k\u4e0eMultiScript30k-Ar\u5f97\u5206\u76f8\u8fd1\uff0c\u4f46Multi30k-Uk\u6bd4MultiScript30k-Uk\u9ad8\u51fa6.4%\u3002", "conclusion": "MultiScript30k\u4e3a\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u8bed\u8a00\u8986\u76d6\uff0c\u7279\u522b\u662f\u975e\u62c9\u4e01\u811a\u672c\u8bed\u8a00\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u5728\u591a\u6837\u5316\u8bed\u8a00\u4e0a\u7684\u53d1\u5c55\u3002\u5c3d\u7ba1\u5728\u67d0\u4e9b\u8bed\u8a00\u4e0a\u4e0e\u73b0\u6709\u6269\u5c55\u76f8\u6bd4\u5b58\u5728\u6027\u80fd\u5dee\u5f02\uff0c\u4f46\u8be5\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u5168\u7403\u8bed\u8a00\u7684\u591a\u6a21\u6001\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u5c55\u793a\u4e86\u4f7f\u7528\u5927\u89c4\u6a21\u7ffb\u8bd1\u6a21\u578b\u6269\u5c55\u73b0\u6709\u6570\u636e\u96c6\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.11167", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11167", "abs": "https://arxiv.org/abs/2512.11167", "authors": ["Anatole Jacquin de Margerie", "Alexis Roger", "Irina Rish"], "title": "Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context", "comment": "Accepted in AAAI 2025 Workshop on Reproducible AI", "summary": "Reproducibility remains a cornerstone of scientific progress, yet complex multimodal models often lack transparent implementation details and accessible training infrastructure. In this work, we present a detailed reproduction and critical analysis of the Monkey Vision-Language Model (VLM) (Li et al. 2023b) published in CVPR24, a recent approach to high-resolution image understanding via image tiling. The original paper proposed splitting large images into tiles to recover fine-grained visual details while maintaining computational efficiency. Our study replicates this strategy using open checkpoints and reimplements the training pipeline. We confirm the key finding of the original Monkey VLM work, namely that tiling effectively recovers local details. We then extend this work further, by investigating the effect of the inclusion of the global context, which provide practical insights for future high-resolution multimodal modeling. However, we also report deviations in the results, with the magnitude of these effects depending heavily on task type and tile granularity.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9CVPR24\u53d1\u8868\u7684Monkey\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u590d\u73b0\u4e0e\u6269\u5c55\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u56fe\u50cf\u5206\u5757\u7b56\u7565\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7406\u89e3\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5168\u5c40\u4e0a\u4e0b\u6587\u6574\u5408\u7684\u5f71\u54cd\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u591a\u6a21\u6001\u5efa\u6a21\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002", "motivation": "\u5c3d\u7ba1\u53ef\u590d\u73b0\u6027\u662f\u79d1\u5b66\u8fdb\u6b65\u7684\u57fa\u7840\uff0c\u4f46\u590d\u6742\u7684\u591a\u6a21\u6001\u6a21\u578b\u5f80\u5f80\u7f3a\u4e4f\u900f\u660e\u7684\u5b9e\u73b0\u7ec6\u8282\u548c\u53ef\u8bbf\u95ee\u7684\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5bf9Monkey\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8be6\u7ec6\u590d\u73b0\u548c\u6279\u5224\u6027\u5206\u6790\uff0c\u4ee5\u9a8c\u8bc1\u5176\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7406\u89e3\u65b9\u6cd5\u5e76\u63a2\u7d22\u6539\u8fdb\u65b9\u5411\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5f00\u653e\u68c0\u67e5\u70b9\u590d\u73b0\u4e86Monkey\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u56fe\u50cf\u5206\u5757\u7b56\u7565\u5c06\u5927\u56fe\u50cf\u5206\u5272\u4e3a\u591a\u4e2a\u56fe\u5757\u4ee5\u6062\u590d\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6269\u5c55\u7814\u7a76\u4e86\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6574\u5408\u6548\u679c\u3002", "result": "\u7814\u7a76\u786e\u8ba4\u4e86\u539f\u59cbMonkey VLM\u5de5\u4f5c\u7684\u5173\u952e\u53d1\u73b0\uff0c\u5373\u5206\u5757\u7b56\u7565\u80fd\u6709\u6548\u6062\u590d\u5c40\u90e8\u7ec6\u8282\uff0c\u4f46\u540c\u65f6\u4e5f\u62a5\u544a\u4e86\u7ed3\u679c\u504f\u5dee\uff0c\u8fd9\u4e9b\u504f\u5dee\u7684\u5e45\u5ea6\u4e25\u91cd\u4f9d\u8d56\u4e8e\u4efb\u52a1\u7c7b\u578b\u548c\u5206\u5757\u7c92\u5ea6\uff0c\u5168\u5c40\u4e0a\u4e0b\u6587\u7684\u6574\u5408\u6548\u679c\u4e3a\u9ad8\u5206\u8fa8\u7387\u591a\u6a21\u6001\u5efa\u6a21\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002", "conclusion": "\u56fe\u50cf\u5206\u5757\u7b56\u7565\u662f\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u7406\u89e3\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u6027\u80fd\u8868\u73b0\u5bf9\u4efb\u52a1\u7279\u6027\u548c\u5206\u5757\u53c2\u6570\u654f\u611f\uff0c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6574\u5408\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u8fd9\u4e3a\u672a\u6765\u9ad8\u5206\u8fa8\u7387\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5b9e\u8df5\u6307\u5bfc\u548c\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.11296", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11296", "abs": "https://arxiv.org/abs/2512.11296", "authors": ["Yasaman Hashem Pour", "Nazanin Mahjourian", "Vinh Nguyen"], "title": "Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining", "comment": null, "summary": "Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5c11\u6837\u672c\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u4e8e\u540c\u65f6\u8bc4\u4f30\u6570\u63a7\u673a\u5e8a\u7684G\u4ee3\u7801\u548cHMI\u663e\u793a\u754c\u9762\u4e2d\u7684\u9519\u8bef\u4e0e\u5b89\u5168\u72b6\u6001\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfLLM\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u89c6\u89c9\u6a21\u6001\u4fe1\u606f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684G\u4ee3\u7801\u9a8c\u8bc1\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7f16\u7a0b\u9519\u8bef\u68c0\u6d4b\uff0c\u4f46\u6570\u63a7\u52a0\u5de5\u9700\u8981\u5e7f\u6cdb\u4f7f\u7528\u548c\u4e86\u89e3\u4eba\u673a\u754c\u9762\uff0c\u8be5\u754c\u9762\u663e\u793a\u673a\u5668\u72b6\u6001\u548c\u9519\u8bef\u4fe1\u606f\u3002\u7531\u4e8eLLM\u65e0\u6cd5\u8bbf\u95ee\u89c6\u89c9\u6a21\u6001\uff0c\u5f53\u524d\u65b9\u6cd5\u7f3a\u4e4f\u5229\u7528HMI\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86G\u4ee3\u7801\u9a8c\u8bc1\u7684\u5168\u9762\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c11\u6837\u672cVLM\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u8f93\u5165\u6570\u636e\u96c6\u5305\u542b\u6765\u81ea15-slant-PRO\u8f66\u5e8a\u7684\u914d\u5bf9G\u4ee3\u7801\u6587\u672c\u548c\u76f8\u5173HMI\u622a\u56fe\uff0c\u5305\u62ec\u6b63\u786e\u548c\u6613\u51fa\u9519\u6848\u4f8b\u3002\u4e3a\u5b9e\u73b0\u5c11\u6837\u672c\u5b66\u4e60\uff0cVLM\u914d\u5907\u4e86\u57fa\u4e8e\u5148\u9a8c\u542f\u53d1\u5f0f\u77e5\u8bc6\u7684\u7ed3\u6784\u5316JSON\u6a21\u5f0f\uff0c\u5e76\u4f7f\u7528\u5305\u542b\u9519\u8bef\u548c\u65e0\u9519\u8bef\u7684G\u4ee3\u7801\u53caHMI\u5b9e\u4f8b\u4f5c\u4e3a\u5c11\u6837\u672c\u793a\u4f8b\u6765\u6307\u5bfc\u6a21\u578b\u3002", "result": "\u4e0e\u96f6\u6837\u672cVLM\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u9519\u8befG\u4ee3\u7801\u548cHMI\u9519\u8bef\u573a\u666f\u4e0b\u901a\u8fc7\u6bcf\u69fd\u51c6\u786e\u7387\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c11\u6837\u672c\u63d0\u793a\u663e\u8457\u589e\u5f3a\u4e86HMI\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u6539\u5584\u4e86G\u4ee3\u7801\u4e0eHMI\u663e\u793a\u4e4b\u95f4\u4e0d\u4e00\u81f4\u6027\u7684\u8bc6\u522b\uff0c\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u7684\u8c03\u8bd5\u529f\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u88ab\u8bc1\u660e\u9002\u7528\u4e8e\u9a8c\u8bc1\u901a\u5e38\u5728CNC\u57f9\u8bad\u4e2d\u5f00\u53d1\u7684\u624b\u52a8\u751f\u6210G\u4ee3\u7801\uff0c\u4e3a\u6570\u63a7\u673a\u5e8a\u64cd\u4f5c\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u6a21\u6001\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7eaf\u6587\u672c\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5de5\u4e1a\u57f9\u8bad\u548c\u5b89\u5168\u9a8c\u8bc1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.11399", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11399", "abs": "https://arxiv.org/abs/2512.11399", "authors": ["Galann Pennec", "Zhengyuan Liu", "Nicholas Asher", "Philippe Muller", "Nancy F. Chen"], "title": "Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction", "comment": null, "summary": "Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u957f\u89c6\u9891\u7684\u591a\u6a21\u6001\u6458\u8981\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u89c6\u9891\u63cf\u8ff0\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u534f\u4f5c\u9009\u62e9\u5173\u952e\u89c6\u9891\u7247\u6bb5\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6458\u8981\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u89c6\u9891\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u91cd\u8981\u89c6\u89c9\u4fe1\u606f\u5bb9\u6613\u5728\u6574\u4e2a\u4e0a\u4e0b\u6587\u4e2d\u4e22\u5931\uff0c\u540c\u65f6\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u7ecf\u6d4e\u9ad8\u6548\u5206\u6790\u957f\u89c6\u9891\u5185\u5bb9\u7684\u5de5\u5177\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u6458\u8981\u4e2d\u4fe1\u606f\u9057\u6f0f\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u89c6\u9891\u5206\u5272\u4e3a\u77ed\u7247\u6bb5\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u89c6\u9891\u63cf\u8ff0\u6a21\u578b\u4e3a\u6bcf\u4e2a\u7247\u6bb5\u751f\u6210\u7d27\u51d1\u7684\u89c6\u89c9\u63cf\u8ff0\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u63cf\u8ff0\u8f93\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7531LLM\u9009\u62e9\u5305\u542b\u6700\u76f8\u5173\u89c6\u89c9\u4fe1\u606f\u7684K\u4e2a\u7247\u6bb5\u7528\u4e8e\u6784\u5efa\u591a\u6a21\u6001\u6458\u8981\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u6458\u8981\u8d28\u91cf\u7684\u5e73\u8861\u3002", "result": "\u5728MovieSum\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u6458\u8981\u6027\u80fd\u63a5\u8fd1\u4eba\u5de5\u6807\u6ce8\u7684\u53c2\u8003\u7247\u6bb5\uff08\u4ec5\u5360\u7535\u5f71\u603b\u65f6\u957f\u4e0d\u52306%\uff09\uff0c\u540c\u65f6\u6bd4\u968f\u673a\u7247\u6bb5\u9009\u62e9\u6355\u83b7\u4e86\u66f4\u591a\u76f8\u5173\u89c6\u9891\u4fe1\u606f\uff0c\u9a8c\u8bc1\u4e86\u8f7b\u91cf\u7ea7\u63cf\u8ff0\u6a21\u578b\u4e0eLLM\u534f\u4f5c\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7247\u6bb5\u9009\u62e9\u673a\u5236\uff0c\u4ec5\u9700\u5c11\u91cf\u5173\u952e\u89c6\u9891\u7247\u6bb5\u5373\u53ef\u6784\u5efa\u5b8c\u6574\u7684\u7535\u5f71\u591a\u6a21\u6001\u6458\u8981\uff0c\u4e3a\u957f\u89c6\u9891\u5185\u5bb9\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.11060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11060", "abs": "https://arxiv.org/abs/2512.11060", "authors": ["Chenjun Li", "Cheng Wan", "Laurin Lux", "Alexander Berger", "Richard B. Rosen", "Martin J. Menten", "Johannes C. Paetzold"], "title": "Synthetic Vasculature and Pathology Enhance Vision-Language Model Reasoning", "comment": "23 pages, 8 figures, 6 tables. Full paper under review for MIDL 2026 (Medical Imaging with Deep Learning)", "summary": "Vision-Language Models (VLMs) offer a promising path toward interpretable medical diagnosis by allowing users to ask about clinical explanations alongside predictions and across different modalities. However, training VLMs for detailed reasoning requires large-scale image-text datasets. In many specialized domains, for example in reading Optical Coherence Tomography Angiography (OCTA) images, such precise text with grounded description of pathologies is scarce or even non-existent. To overcome this bottleneck, we introduce Synthetic Vasculature Reasoning (SVR), a framework that controllably synthesizes images and corresponding text, specifically: realistic retinal vasculature with Diabetic Retinopathy (DR) features: capillary dropout, microaneurysms, neovascularization, and tortuosity, while automatically generating granular reasoning texts. Based on this we curate OCTA-100K-SVR, an OCTA image-reasoning dataset with 100,000 pairs. Our experiments show that a general-purpose VLM (Qwen3-VL-8b) trained on the dataset achieves a zero-shot balanced classification accuracy of 89.67% on real OCTA images, outperforming supervised baselines. Through human expert evaluation we also demonstrate that it significantly enhances explanation quality and pathology localization on clinical data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Synthetic Vasculature Reasoning (SVR)\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u63a7\u5408\u6210\u89c6\u7f51\u819c\u8840\u7ba1\u56fe\u50cf\u548c\u5bf9\u5e94\u6587\u672c\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86OCTA-100K-SVR\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728OCTA\u56fe\u50cf\u4e0a\u7684\u8bca\u65ad\u6027\u80fd\u548c\u89e3\u91ca\u80fd\u529b\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u8bad\u7ec3\u9700\u8981\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u6570\u636e\uff0c\u800c\u5728\u8bb8\u591a\u4e13\u4e1a\u9886\u57df\u5982\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\u8840\u7ba1\u6210\u50cf\u4e2d\uff0c\u5305\u542b\u75c5\u7406\u7ec6\u8282\u7684\u7cbe\u786e\u6587\u672c\u6807\u6ce8\u975e\u5e38\u7a00\u7f3a\u751a\u81f3\u4e0d\u5b58\u5728\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u4e34\u5e8a\u89e3\u91ca\u548c\u8de8\u6a21\u6001\u63a8\u7406\u65b9\u9762\u7684\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86Synthetic Vasculature Reasoning\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u53ef\u63a7\u5730\u5408\u6210\u5177\u6709\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u7279\u5f81\u7684\u89c6\u7f51\u819c\u8840\u7ba1\u56fe\u50cf\uff0c\u5305\u62ec\u6bdb\u7ec6\u8840\u7ba1\u8131\u843d\u3001\u5fae\u52a8\u8109\u7624\u3001\u65b0\u751f\u8840\u7ba1\u548c\u8840\u7ba1\u8fc2\u66f2\u7b49\u75c5\u7406\u7279\u5f81\uff0c\u540c\u65f6\u81ea\u52a8\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u63a8\u7406\u6587\u672c\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86\u5305\u542b10\u4e07\u5bf9\u56fe\u50cf\u7684OCTA-100K-SVR\u6570\u636e\u96c6\u3002", "result": "\u5728OCTA-100K-SVR\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9eOCTA\u56fe\u50cf\u4e0a\u5b9e\u73b0\u4e8689.67%\u7684\u96f6\u6837\u672c\u5e73\u8861\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u8bc1\u5b9e\uff0c\u8be5\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u4e34\u5e8a\u6570\u636e\u7684\u89e3\u91ca\u8d28\u91cf\u548c\u75c5\u7406\u5b9a\u4f4d\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u53ef\u63a7\u5408\u6210\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u662f\u89e3\u51b3\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u7684\u6709\u6548\u9014\u5f84\uff0cSVR\u6846\u67b6\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u589e\u5f3a\u4e86\u4e34\u5e8a\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u4e13\u4e1a\u533b\u5b66\u9886\u57df\u7684AI\u8bca\u65ad\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u751f\u6210\u8303\u5f0f\u3002"}}
{"id": "2512.11458", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11458", "abs": "https://arxiv.org/abs/2512.11458", "authors": ["Jingmin Zhu", "Anqi Zhu", "Hossein Rahmani", "Jun Liu", "Mohammed Bennamoun", "Qiuhong Ke"], "title": "Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation", "comment": null, "summary": "We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Skeleton-Cache\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u57fa\u4e8e\u9aa8\u67b6\u7684\u96f6\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u7684\u514d\u8bad\u7ec3\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u91cd\u6784\u4e3a\u8f7b\u91cf\u7ea7\u68c0\u7d22\u8fc7\u7a0b\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u52a8\u4f5c\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9aa8\u67b6\u7684\u96f6\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u96be\u4ee5\u9002\u5e94\u672a\u89c1\u52a8\u4f5c\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u9996\u4e2a\u514d\u8bad\u7ec3\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5bf9\u672a\u89c1\u52a8\u4f5c\u7684\u8bc6\u522b\u80fd\u529b\u3002", "method": "Skeleton-Cache\u5c06\u63a8\u7406\u91cd\u6784\u4e3a\u8f7b\u91cf\u7ea7\u68c0\u7d22\u8fc7\u7a0b\uff0c\u901a\u8fc7\u975e\u53c2\u6570\u7f13\u5b58\u5b58\u50a8\u7ed3\u6784\u5316\u7684\u9aa8\u67b6\u8868\u793a\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u7ec6\u7c92\u5ea6\u5c40\u90e8\u63cf\u8ff0\u7b26\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u4e3a\u7c7b\u522b\u5206\u914d\u91cd\u8981\u6027\u6743\u91cd\uff0c\u6307\u5bfc\u63cf\u8ff0\u7b26\u9884\u6d4b\u7684\u878d\u5408\uff0c\u5b9e\u73b0\u52a8\u6001\u9002\u5e94\u672a\u89c1\u52a8\u4f5c\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u8bbf\u95ee\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728NTU RGB+D 60/120\u548cPKU-MMD II\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSkeleton-Cache\u5728\u96f6\u6837\u672c\u548c\u5e7f\u4e49\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u80fd\u591f\u6301\u7eed\u63d0\u5347\u591a\u79cdSZAR\u9aa8\u5e72\u7f51\u7edc\u7684\u6027\u80fd\u3002\u8be5\u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u52a8\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Skeleton-Cache\u4e3a\u57fa\u4e8e\u9aa8\u67b6\u7684\u96f6\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u63d0\u4f9b\u4e86\u9996\u4e2a\u514d\u8bad\u7ec3\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u9aa8\u67b6\u8868\u793a\u548cLLM\u5f15\u5bfc\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u52a8\u4f5c\u7684\u6709\u6548\u9002\u5e94\u3002\u8be5\u6846\u67b6\u4e3a\u52a8\u4f5c\u8bc6\u522b\u9886\u57df\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2512.11567", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.11567", "abs": "https://arxiv.org/abs/2512.11567", "authors": ["Mevl\u00fct Bagci", "Ali Abusaleh", "Daniel Baumartz", "Giueseppe Abrami", "Maxim Konca", "Alexander Mehler"], "title": "Extending a Parliamentary Corpus with MPs' Tweets: Automatic Annotation and Evaluation Using MultiParTweet", "comment": "Submitted to LREC 2026", "summary": "Social media serves as a critical medium in modern politics because it both reflects politicians' ideologies and facilitates communication with younger generations. We present MultiParTweet, a multilingual tweet corpus from X that connects politicians' social media discourse with German political corpus GerParCor, thereby enabling comparative analyses between online communication and parliamentary debates. MultiParTweet contains 39 546 tweets, including 19 056 media items. Furthermore, we enriched the annotation with nine text-based models and one vision-language model (VLM) to annotate MultiParTweet with emotion, sentiment, and topic annotations. Moreover, the automated annotations are evaluated against a manually annotated subset. MultiParTweet can be reconstructed using our tool, TTLABTweetCrawler, which provides a framework for collecting data from X. To demonstrate a methodological demonstration, we examine whether the models can predict each other using the outputs of the remaining models. In summary, we provide MultiParTweet, a resource integrating automatic text and media-based annotations validated with human annotations, and TTLABTweetCrawler, a general-purpose X data collection tool. Our analysis shows that the models are mutually predictable. In addition, VLM-based annotation were preferred by human annotators, suggesting that multimodal representations align more with human interpretation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MultiParTweet\uff0c\u4e00\u4e2a\u8fde\u63a5\u5fb7\u56fd\u8bae\u4f1a\u8bed\u6599\u5e93\u7684\u591a\u8bed\u8a00\u63a8\u7279\u8bed\u6599\u5e93\uff0c\u4ee5\u53caTTLABTweetCrawler\u6570\u636e\u6536\u96c6\u5de5\u5177\uff0c\u901a\u8fc7\u4e5d\u79cd\u6587\u672c\u6a21\u578b\u548c\u4e00\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u6807\u6ce8\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u95f4\u7684\u76f8\u4e92\u53ef\u9884\u6d4b\u6027\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5728\u73b0\u4ee3\u653f\u6cbb\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0c\u4f46\u7f3a\u4e4f\u8fde\u63a5\u653f\u6cbb\u5bb6\u5728\u7ebf\u8bdd\u8bed\u4e0e\u8bae\u4f1a\u8fa9\u8bba\u7684\u7cfb\u7edf\u6027\u591a\u8bed\u8a00\u8bed\u6599\u5e93\uff0c\u9650\u5236\u4e86\u6bd4\u8f83\u5206\u6790\u548c\u8de8\u5e73\u53f0\u653f\u6cbb\u6c9f\u901a\u7814\u7a76\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b39,546\u6761\u63a8\u6587\uff08\u542b19,056\u4e2a\u5a92\u4f53\u9879\u76ee\uff09\u7684MultiParTweet\u8bed\u6599\u5e93\uff0c\u8fde\u63a5\u5fb7\u56fd\u653f\u6cbb\u8bed\u6599\u5e93GerParCor\uff0c\u4f7f\u7528\u4e5d\u79cd\u6587\u672c\u6a21\u578b\u548c\u4e00\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u60c5\u611f\u3001\u60c5\u7eea\u548c\u4e3b\u9898\u81ea\u52a8\u6807\u6ce8\uff0c\u5e76\u5f00\u53d1\u4e86TTLABTweetCrawler\u5de5\u5177\u7528\u4e8eX\u5e73\u53f0\u6570\u636e\u6536\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u4e0d\u540c\u6a21\u578b\u8f93\u51fa\u4e4b\u95f4\u5b58\u5728\u76f8\u4e92\u53ef\u9884\u6d4b\u6027\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6807\u6ce8\u7ed3\u679c\u4e0e\u4eba\u5de5\u6807\u6ce8\u8005\u504f\u597d\u66f4\u4e00\u81f4\uff0c\u81ea\u52a8\u6807\u6ce8\u4e0e\u4eba\u5de5\u6807\u6ce8\u5b50\u96c6\u8fdb\u884c\u4e86\u9a8c\u8bc1\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u8868\u793a\u4e0e\u4eba\u7c7b\u89e3\u91ca\u66f4\u5951\u5408\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u6574\u5408\u81ea\u52a8\u6587\u672c\u4e0e\u5a92\u4f53\u6807\u6ce8\u7684\u6807\u51c6\u5316\u8d44\u6e90\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u95f4\u53ef\u9884\u6d4b\u6027\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u653f\u6cbb\u8bdd\u8bed\u5206\u6790\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u8de8\u5e73\u53f0\u653f\u6cbb\u6c9f\u901a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u6846\u67b6\u548c\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2512.11061", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11061", "abs": "https://arxiv.org/abs/2512.11061", "authors": ["Felix O'Mahony", "Roberto Cipolla", "Ayush Tewari"], "title": "VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation", "comment": "Website: https://felixomahony.github.io/vdaworld/", "summary": "Generative video models, a leading approach to world modeling, face fundamental limitations. They often violate physical and logical rules, lack interactivity, and operate as opaque black boxes ill-suited for building structured, queryable worlds. To overcome these challenges, we propose a new paradigm focused on distilling an image caption pair into a tractable, abstract representation optimized for simulation. We introduce VDAWorld, a framework where a Vision-Language Model (VLM) acts as an intelligent agent to orchestrate this process. The VLM autonomously constructs a grounded (2D or 3D) scene representation by selecting from a suite of vision tools, and accordingly chooses a compatible physics simulator (e.g., rigid body, fluid) to act upon it. VDAWorld can then infer latent dynamics from the static scene to predict plausible future states. Our experiments show that this combination of intelligent abstraction and adaptive simulation results in a versatile world model capable of producing high quality simulations across a wide range of dynamic scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VDAWorld\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf-\u6587\u672c\u5bf9\u84b8\u998f\u4e3a\u53ef\u5904\u7406\u7684\u62bd\u8c61\u8868\u793a\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4ee3\u7406\u534f\u8c03\u89c6\u89c9\u5de5\u5177\u548c\u7269\u7406\u6a21\u62df\u5668\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u4ea7\u751f\u9ad8\u8d28\u91cf\u52a8\u6001\u573a\u666f\u6a21\u62df\u7684\u901a\u7528\u4e16\u754c\u6a21\u578b\u3002", "motivation": "\u751f\u6210\u5f0f\u89c6\u9891\u6a21\u578b\u4f5c\u4e3a\u4e16\u754c\u5efa\u6a21\u7684\u4e3b\u8981\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u5305\u62ec\u8fdd\u53cd\u7269\u7406\u548c\u903b\u8f91\u89c4\u5219\u3001\u7f3a\u4e4f\u4ea4\u4e92\u6027\u4ee5\u53ca\u4f5c\u4e3a\u4e0d\u900f\u660e\u9ed1\u7bb1\u96be\u4ee5\u6784\u5efa\u7ed3\u6784\u5316\u53ef\u67e5\u8be2\u4e16\u754c\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u63a2\u7d22\u65b0\u7684\u4e16\u754c\u5efa\u6a21\u8303\u5f0f\u3002", "method": "\u63d0\u51faVDAWorld\u6846\u67b6\uff0c\u5c06\u56fe\u50cf-\u6587\u672c\u5bf9\u84b8\u998f\u4e3a\u9488\u5bf9\u6a21\u62df\u4f18\u5316\u7684\u53ef\u5904\u7406\u62bd\u8c61\u8868\u793a\u3002\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4ee3\u7406\u534f\u8c03\u6574\u4e2a\u8fc7\u7a0b\uff0c\u81ea\u4e3b\u6784\u5efa\u57fa\u4e8e\u89c6\u89c9\u5de5\u5177\u5957\u4ef6\u7684\u63a5\u5730\uff082D\u62163D\uff09\u573a\u666f\u8868\u793a\uff0c\u5e76\u76f8\u5e94\u9009\u62e9\u517c\u5bb9\u7684\u7269\u7406\u6a21\u62df\u5668\uff08\u5982\u521a\u4f53\u3001\u6d41\u4f53\uff09\u4f5c\u7528\u4e8e\u573a\u666f\uff0c\u80fd\u591f\u4ece\u9759\u6001\u573a\u666f\u63a8\u65ad\u6f5c\u5728\u52a8\u6001\u4ee5\u9884\u6d4b\u5408\u7406\u672a\u6765\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u667a\u80fd\u62bd\u8c61\u4e0e\u81ea\u9002\u5e94\u6a21\u62df\u7684\u7ed3\u5408\u4ea7\u751f\u4e86\u80fd\u591f\u8de8\u5e7f\u6cdb\u52a8\u6001\u573a\u666f\u751f\u6210\u9ad8\u8d28\u91cf\u6a21\u62df\u7684\u901a\u7528\u4e16\u754c\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u52a8\u6001\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6a21\u62df\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u667a\u80fd\u62bd\u8c61\u4e0e\u81ea\u9002\u5e94\u7269\u7406\u6a21\u62df\u76f8\u7ed3\u5408\u7684\u65b0\u8303\u5f0f\u6f5c\u529b\uff0c\u4e3a\u6784\u5efa\u7ed3\u6784\u5316\u3001\u53ef\u67e5\u8be2\u4e14\u9075\u5faa\u7269\u7406\u89c4\u5219\u7684\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u4ece\u9ed1\u7bb1\u751f\u6210\u6a21\u578b\u5411\u53ef\u89e3\u91ca\u3001\u53ef\u4ea4\u4e92\u4e16\u754c\u8868\u793a\u7684\u8f6c\u53d8\u3002"}}
{"id": "2512.11464", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11464", "abs": "https://arxiv.org/abs/2512.11464", "authors": ["Han Lin", "Xichen Pan", "Ziqi Huang", "Ji Hou", "Jialiang Wang", "Weifeng Chen", "Zecheng He", "Felix Juefei-Xu", "Junzhe Sun", "Zhipeng Fan", "Ali Thabet", "Mohit Bansal", "Chu Wang"], "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas", "comment": "Project page: https://metacanvas.github.io", "summary": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMetaCanvas\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6f5c\u5728\u7a7a\u95f4\u89c4\u5212\u5668\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u7a7a\u95f4\u548c\u65f6\u7a7a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u76f4\u63a5\u8fdb\u884c\u63a8\u7406\u548c\u89c4\u5212\uff0c\u4ece\u800c\u5f25\u5408\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u4e4b\u95f4\u7684\u80fd\u529b\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e2d\u901a\u5e38\u88ab\u7b80\u5316\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5168\u5c40\u6587\u672c\u7f16\u7801\u5668\uff0c\u5176\u5f3a\u5927\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u5bfc\u81f4\u6a21\u578b\u80fd\u591f\u89e3\u6790\u590d\u6742\u5e03\u5c40\u548c\u77e5\u8bc6\u5bc6\u96c6\u578b\u573a\u666f\uff0c\u5374\u96be\u4ee5\u751f\u6210\u5177\u6709\u540c\u7b49\u7cbe\u786e\u548c\u7ed3\u6784\u5316\u63a7\u5236\u7684\u56fe\u50cf\u6216\u89c6\u9891\uff0c\u5f62\u6210\u4e86\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u4e4b\u95f4\u7684\u80fd\u529b\u5dee\u8ddd\u3002", "method": "\u63d0\u51faMetaCanvas\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u4f7f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u7a7a\u95f4\u548c\u65f6\u7a7a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u76f4\u63a5\u8fdb\u884c\u63a8\u7406\u548c\u89c4\u5212\uff0c\u5e76\u4e0e\u6269\u6563\u751f\u6210\u5668\u7d27\u5bc6\u63a5\u53e3\uff1b\u8be5\u6846\u67b6\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u6269\u6563\u6a21\u578b\u9aa8\u5e72\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5b9e\u73b0\uff0c\u652f\u6301\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u6587\u672c/\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u3001\u56fe\u50cf/\u89c6\u9891\u7f16\u8f91\u4ee5\u53ca\u4e0a\u4e0b\u6587\u89c6\u9891\u751f\u6210\u7b49\u591a\u79cd\u4efb\u52a1\u3002", "result": "MetaCanvas\u5728\u516d\u4e2a\u9700\u8981\u7cbe\u786e\u5e03\u5c40\u3001\u9c81\u68d2\u5c5e\u6027\u7ed1\u5b9a\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u63a7\u5236\u7684\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u5168\u5c40\u6761\u4ef6\u57fa\u51c6\u65b9\u6cd5\uff0c\u5305\u62ec\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u6587\u672c/\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u3001\u56fe\u50cf/\u89c6\u9891\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u89c6\u9891\u751f\u6210\u7b49\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u6f5c\u5728\u7a7a\u95f4\u89c4\u5212\u5668\u662f\u5f25\u5408\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u4e4b\u95f4\u5dee\u8ddd\u7684\u6709\u524d\u666f\u65b9\u5411\uff0cMetaCanvas\u6846\u67b6\u901a\u8fc7\u5145\u5206\u5229\u7528\u6a21\u578b\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\uff0c\u4e3a\u7cbe\u786e\u548c\u7ed3\u6784\u5316\u89c6\u89c9\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.11534", "categories": ["cs.CV", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.11534", "abs": "https://arxiv.org/abs/2512.11534", "authors": ["Yiqing Yang", "Kin-Man Lam"], "title": "HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning", "comment": "18 pages, 8 figures", "summary": "Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7684\u4efb\u52a1\u81ea\u9002\u5e94\u5173\u952e\u5e27\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u67e5\u8be2\u751f\u6210\u3001\u96c6\u5408\u7ea7\u4f18\u5316\u548c\u5e08\u751f\u4e92\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5e27\u9009\u62e9\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6d88\u9664\u4e86\u5bf9\u9759\u6001\u4f2a\u6807\u7b7e\u7684\u4f9d\u8d56\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u4e00\u662f\u57fa\u4e8e\u72ec\u7acb\u8bc4\u5206\u7684top-K\u9009\u62e9\u65b9\u6cd5\u65e0\u6cd5\u4ece\u6574\u4f53\u4e0a\u4f18\u5316\u5e27\u9009\u62e9\uff0c\u5bfc\u81f4\u6240\u9009\u5e27\u5728\u65f6\u95f4\u4e0a\u805a\u96c6\u4e14\u89c6\u89c9\u5197\u4f59\uff1b\u4e8c\u662f\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u79bb\u7ebf\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9009\u62e9\u5668\uff0c\u4f7f\u5f97\u76d1\u7763\u4fe1\u53f7\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u4efb\u52a1\u76ee\u6807\uff0c\u9650\u5236\u4e86\u9009\u62e9\u5668\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7684\u4efb\u52a1\u81ea\u9002\u5e94\u5e27\u9009\u62e9\u6846\u67b6\u3002\u9996\u5148\u91c7\u7528\u601d\u7ef4\u94fe\u65b9\u6cd5\u5f15\u5bfc\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7684\u9690\u5f0f\u67e5\u8be2\u5411\u91cf\uff0c\u4e0e\u591a\u6a21\u6001\u7279\u5f81\u7ed3\u5408\u5b9e\u73b0\u52a8\u6001\u5e27\u8bc4\u5206\u3002\u5176\u6b21\u5b9a\u4e49\u4e86\u5305\u542b\u76f8\u5173\u6027\u3001\u8986\u76d6\u5ea6\u548c\u5197\u4f59\u5ea6\u7684\u8fde\u7eed\u96c6\u5408\u7ea7\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7Gumbel-Softmax\u5b9e\u73b0\u53ef\u5fae\u5206\u4f18\u5316\u4ee5\u9009\u62e9\u6700\u4f18\u5e27\u7ec4\u5408\u3002\u6700\u540e\u91c7\u7528\u5e08\u751f\u4e92\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7KL\u6563\u5ea6\u5bf9\u9f50\u5b66\u751f\u9009\u62e9\u5668\uff08SLM\uff09\u548c\u6559\u5e08\u63a8\u7406\u5668\uff08MLLM\uff09\u7684\u5e27\u91cd\u8981\u6027\u5206\u5e03\uff0c\u7ed3\u5408\u4ea4\u53c9\u71b5\u635f\u5931\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u6d88\u9664\u5bf9\u9759\u6001\u4f2a\u6807\u7b7e\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5177\u4f53\u5728Video-MME\u3001LongVideoBench\u3001MLVU\u548cNExT-QA\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u4efb\u52a1\u81ea\u9002\u5e94\u6846\u67b6\u5728\u5173\u952e\u5e27\u9009\u62e9\u4efb\u52a1\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u7aef\u5230\u7aef\u4efb\u52a1\u81ea\u9002\u5e94\u5173\u952e\u5e27\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u67e5\u8be2\u751f\u6210\u3001\u96c6\u5408\u7ea7\u4f18\u5316\u548c\u5e08\u751f\u4e92\u5b66\u4e60\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6d88\u9664\u4e86\u5bf9\u9759\u6001\u4f2a\u6807\u7b7e\u7684\u4f9d\u8d56\uff0c\u8fd8\u80fd\u6839\u636e\u5177\u4f53\u4efb\u52a1\u76ee\u6807\u81ea\u9002\u5e94\u5730\u9009\u62e9\u6700\u4f18\u5e27\u7ec4\u5408\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5e27\u9009\u62e9\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.11098", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11098", "abs": "https://arxiv.org/abs/2512.11098", "authors": ["Nazanin Mahjourian", "Vinh Nguyen"], "title": "Vision-Language Models for Infrared Industrial Sensing in Additive Manufacturing Scene Description", "comment": null, "summary": "Many manufacturing environments operate in low-light conditions or within enclosed machines where conventional vision systems struggle. Infrared cameras provide complementary advantages in such environments. Simultaneously, supervised AI systems require large labeled datasets, which makes zero-shot learning frameworks more practical for applications including infrared cameras. Recent advances in vision-language foundation models (VLMs) offer a new path in zero-shot predictions from paired image-text representations. However, current VLMs cannot understand infrared camera data since they are trained on RGB data. This work introduces VLM-IRIS (Vision-Language Models for InfraRed Industrial Sensing), a zero-shot framework that adapts VLMs to infrared data by preprocessing infrared images captured by a FLIR Boson sensor into RGB-compatible inputs suitable for CLIP-based encoders. We demonstrate zero-shot workpiece presence detection on a 3D printer bed where temperature differences between the build plate and workpieces make the task well-suited for thermal imaging. VLM-IRIS converts the infrared images to magma representation and applies centroid prompt ensembling with a CLIP ViT-B/32 encoder to achieve high accuracy on infrared images without any model retraining. These findings demonstrate that the proposed improvements to VLMs can be effectively extended to thermal applications for label-free monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VLM-IRIS\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7ea2\u5916\u56fe\u50cf\u9884\u5904\u7406\u4e3aRGB\u517c\u5bb9\u8868\u793a\uff0c\u4f7f\u57fa\u4e8eCLIP\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u96f6\u6837\u672c\u7ea2\u5916\u5de5\u4e1a\u611f\u77e5\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5728\u70ed\u6210\u50cf\u5e94\u7528\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5de5\u4ef6\u68c0\u6d4b\u3002", "motivation": "\u5728\u4f4e\u5149\u7167\u6216\u5c01\u95ed\u673a\u5668\u73af\u5883\u7b49\u5236\u9020\u573a\u666f\u4e2d\uff0c\u4f20\u7edf\u89c6\u89c9\u7cfb\u7edf\u96be\u4ee5\u6709\u6548\u5de5\u4f5c\uff0c\u800c\u7ea2\u5916\u76f8\u673a\u5177\u6709\u4e92\u8865\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u4ec5\u9488\u5bf9RGB\u6570\u636e\u8bad\u7ec3\uff0c\u65e0\u6cd5\u7406\u89e3\u7ea2\u5916\u76f8\u673a\u6570\u636e\uff0c\u4e14\u76d1\u7763AI\u7cfb\u7edf\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u8fd9\u4f7f\u5f97\u96f6\u6837\u672c\u5b66\u4e60\u6846\u67b6\u5728\u7ea2\u5916\u76f8\u673a\u5e94\u7528\u4e2d\u66f4\u5177\u5b9e\u7528\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86VLM-IRIS\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u5904\u7406FLIR Boson\u4f20\u611f\u5668\u6355\u83b7\u7684\u7ea2\u5916\u56fe\u50cf\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u9002\u5408CLIP\u7f16\u7801\u5668\u7684RGB\u517c\u5bb9\u8f93\u5165\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u5c06\u7ea2\u5916\u56fe\u50cf\u8f6c\u6362\u4e3a\u5ca9\u6d46\u8868\u793a\uff0c\u5e76\u5e94\u7528\u8d28\u5fc3\u63d0\u793a\u96c6\u6210\u6280\u672f\u4e0eCLIP ViT-B/32\u7f16\u7801\u5668\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u5728\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5bf9\u7ea2\u5916\u56fe\u50cf\u8fdb\u884c\u96f6\u6837\u672c\u9884\u6d4b\u3002", "result": "VLM-IRIS\u57283D\u6253\u5370\u673a\u5e8a\u7684\u5de5\u4ef6\u5b58\u5728\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u8be5\u4efb\u52a1\u5229\u7528\u6784\u5efa\u677f\u4e0e\u5de5\u4ef6\u4e4b\u95f4\u7684\u6e29\u5ea6\u5dee\u5f02\uff0c\u975e\u5e38\u9002\u5408\u70ed\u6210\u50cf\u5e94\u7528\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u9884\u5904\u7406\u548c\u63d0\u793a\u96c6\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u7ea2\u5916\u56fe\u50cf\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\uff0c\u65e0\u9700\u4efb\u4f55\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u6216\u6807\u6ce8\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u9002\u5f53\u7684\u9884\u5904\u7406\u6280\u672f\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u6027\u53ef\u4ee5\u6269\u5c55\u5230\u70ed\u6210\u50cf\u5e94\u7528\u9886\u57df\uff0c\u4e3a\u65e0\u6807\u7b7e\u76d1\u63a7\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u5de5\u4e1a\u73af\u5883\u4e2d\u7ea2\u5916\u611f\u77e5\u7684\u96f6\u6837\u672c\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u975eRGB\u6a21\u6001\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6f5c\u529b\u3002"}}
{"id": "2512.11558", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11558", "abs": "https://arxiv.org/abs/2512.11558", "authors": ["Zhenyang Cai", "Jiaming Zhang", "Junjie Zhao", "Ziyi Zeng", "Yanchao Li", "Jingyi Liang", "Junying Chen", "Yunjin Yang", "Jiajun You", "Shuzhi Deng", "Tongfei Wang", "Wanting Chen", "Chunxiu Hao", "Ruiqi Xie", "Zhenwei Wen", "Xiangyi Feng", "Zou Ting", "Jin Zou Lin", "Jianquan Li", "Guangjun Yu", "Liangyi Chen", "Junwen Wang", "Shan Jiang", "Benyou Wang"], "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry", "comment": null, "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DentalGPT\uff0c\u4e00\u79cd\u4e13\u95e8\u7528\u4e8e\u7259\u79d1\u9886\u57df\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u9886\u57df\u77e5\u8bc6\u6ce8\u5165\u548c\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u4e86\u73b0\u6709MLLMs\u5728\u7259\u79d1\u89c6\u89c9\u7ec6\u8282\u6355\u6349\u548c\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7259\u79d1\u9886\u57df\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u7259\u79d1\u89c6\u89c9\u7ec6\u8282\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u8fdb\u884c\u7cbe\u786e\u8bca\u65ad\u6240\u9700\u7684\u5145\u5206\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u963b\u788d\u4e86\u81ea\u52a8\u5316\u53e3\u8154\u533b\u7597\u4fdd\u5065\u4e2d\u591a\u6a21\u6001\u6570\u636e\u7684\u53ef\u9760\u89e3\u91ca\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u7259\u79d1\u591a\u6a21\u6001\u6807\u6ce8\u6570\u636e\u96c6\uff08\u5305\u542b\u8d85\u8fc712\u4e07\u5f20\u7259\u79d1\u56fe\u50cf\u53ca\u5176\u8be6\u7ec6\u63cf\u8ff0\uff09\uff0c\u5e76\u91c7\u7528\u9ad8\u8d28\u91cf\u9886\u57df\u77e5\u8bc6\u6ce8\u5165\u548c\u5f3a\u5316\u5b66\u4e60\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u6765\u5f00\u53d1DentalGPT\uff0c\u5176\u4e2d\u6570\u636e\u96c6\u7279\u522b\u5f3a\u8c03\u4e86\u8bca\u65ad\u76f8\u5173\u7684\u89c6\u89c9\u7279\u5f81\u3002", "result": "DentalGPT\u5728\u53e3\u8154\u5185\u548c\u5168\u666fX\u5149\u7247\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u533b\u5b66VQA\u57fa\u51c6\u7684\u7259\u79d1\u5b50\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u5728\u75be\u75c5\u5206\u7c7b\u548c\u7259\u79d1\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u8bb8\u591a\u6700\u5148\u8fdb\u7684MLLMs\uff0c\u5c3d\u7ba1\u6a21\u578b\u4ec5\u670970\u4ebf\u53c2\u6570\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9ad8\u8d28\u91cf\u7259\u79d1\u6570\u636e\u4e0e\u5206\u9636\u6bb5\u9002\u5e94\u7b56\u7565\u76f8\u7ed3\u5408\uff0c\u4e3a\u6784\u5efa\u80fd\u529b\u5f3a\u4e14\u9886\u57df\u4e13\u4e1a\u5316\u7684\u7259\u79d1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u8bc1\u660e\u4e86\u9886\u57df\u4e13\u4e1a\u5316\u6a21\u578b\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2512.11099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11099", "abs": "https://arxiv.org/abs/2512.11099", "authors": ["Weitai Kang", "Jason Kuen", "Mengwei Ren", "Zijun Wei", "Yan Yan", "Kangning Liu"], "title": "VGent: Visual Grounding via Modular Design for Disentangling Reasoning and Prediction", "comment": "8 pages", "summary": "Current visual grounding models are either based on a Multimodal Large Language Model (MLLM) that performs auto-regressive decoding, which is slow and risks hallucinations, or on re-aligning an LLM with vision features to learn new special or object tokens for grounding, which may undermine the LLM's pretrained reasoning ability. In contrast, we propose VGent, a modular encoder-decoder architecture that explicitly disentangles high-level reasoning and low-level bounding box prediction. Specifically, a frozen MLLM serves as the encoder to provide untouched powerful reasoning capabilities, while a decoder takes high-quality boxes proposed by detectors as queries and selects target box(es) via cross-attending on encoder's hidden states. This design fully leverages advances in both object detection and MLLM, avoids the pitfalls of auto-regressive decoding, and enables fast inference. Moreover, it supports modular upgrades of both the encoder and decoder to benefit the whole system: we introduce (i) QuadThinker, an RL-based training paradigm for enhancing multi-target reasoning ability of the encoder; (ii) mask-aware label for resolving detection-segmentation ambiguity; and (iii) global target recognition to improve the recognition of all the targets which benefits the selection among augmented proposals. Experiments on multi-target visual grounding benchmarks show that VGent achieves a new state-of-the-art with +20.6% F1 improvement over prior methods, and further boosts gIoU by +8.2% and cIoU by +5.8% under visual reference challenges, while maintaining constant, fast inference latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVGent\uff0c\u4e00\u79cd\u6a21\u5757\u5316\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u89e3\u8026\u9ad8\u5c42\u63a8\u7406\u4e0e\u4f4e\u5c42\u8fb9\u754c\u6846\u9884\u6d4b\u6765\u89e3\u51b3\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u81ea\u56de\u5f52\u89e3\u7801\u901f\u5ea6\u6162\u3001\u5e7b\u89c9\u98ce\u9669\u4ee5\u53ca\u91cd\u65b0\u5bf9\u9f50LLM\u635f\u5bb3\u9884\u8bad\u7ec3\u63a8\u7406\u80fd\u529b\u7684\u95ee\u9898\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u5b9a\u4f4d\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u56de\u5f52\u89e3\u7801\u901f\u5ea6\u6162\u4e14\u5b58\u5728\u5e7b\u89c9\u98ce\u9669\uff0c\u800c\u901a\u8fc7\u91cd\u65b0\u5bf9\u9f50LLM\u4e0e\u89c6\u89c9\u7279\u5f81\u5b66\u4e60\u65b0\u7279\u6b8a\u6807\u8bb0\u7684\u65b9\u6cd5\u53ef\u80fd\u635f\u5bb3LLM\u7684\u9884\u8bad\u7ec3\u63a8\u7406\u80fd\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u51fa\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u53c8\u80fd\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u8fb9\u754c\u6846\u9884\u6d4b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "VGent\u91c7\u7528\u6a21\u5757\u5316\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u4f7f\u7528\u51bb\u7ed3\u7684MLLM\u4f5c\u4e3a\u7f16\u7801\u5668\u4ee5\u4fdd\u6301\u5176\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u7801\u5668\u5219\u63a5\u6536\u68c0\u6d4b\u5668\u63d0\u51fa\u7684\u9ad8\u8d28\u91cf\u8fb9\u754c\u6846\u4f5c\u4e3a\u67e5\u8be2\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5728\u7f16\u7801\u5668\u9690\u85cf\u72b6\u6001\u4e0a\u9009\u62e9\u76ee\u6807\u6846\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86QuadThinker\uff08\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u8303\u5f0f\u589e\u5f3a\u7f16\u7801\u5668\u7684\u591a\u76ee\u6807\u63a8\u7406\u80fd\u529b\uff09\u3001\u63a9\u7801\u611f\u77e5\u6807\u7b7e\u89e3\u51b3\u68c0\u6d4b-\u5206\u5272\u6b67\u4e49\uff0c\u4ee5\u53ca\u5168\u5c40\u76ee\u6807\u8bc6\u522b\u6539\u8fdb\u6240\u6709\u76ee\u6807\u7684\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u5728\u591a\u76ee\u6807\u89c6\u89c9\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVGent\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5b9e\u73b0\u4e86+20.6%\u7684F1\u5206\u6570\u63d0\u5347\uff0c\u5728\u89c6\u89c9\u53c2\u8003\u6311\u6218\u4e0b\u8fdb\u4e00\u6b65\u5c06gIoU\u63d0\u5347+8.2%\u3001cIoU\u63d0\u5347+5.8%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6052\u5b9a\u4e14\u5feb\u901f\u7684\u63a8\u7406\u5ef6\u8fdf\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "VGent\u901a\u8fc7\u89e3\u8026\u9ad8\u5c42\u63a8\u7406\u4e0e\u4f4e\u5c42\u8fb9\u754c\u6846\u9884\u6d4b\uff0c\u5145\u5206\u5229\u7528\u4e86\u76ee\u6807\u68c0\u6d4b\u548cMLLM\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u907f\u514d\u4e86\u81ea\u56de\u5f52\u89e3\u7801\u7684\u7f3a\u9677\uff0c\u652f\u6301\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u6a21\u5757\u5316\u5347\u7ea7\uff0c\u4e3a\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u548c\u5feb\u901f\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2512.11130", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11130", "abs": "https://arxiv.org/abs/2512.11130", "authors": ["Bowen Wen", "Shaurya Dewan", "Stan Birchfield"], "title": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching", "comment": null, "summary": "Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications. Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly per-domain fine-tuning. To bridge this gap, we present Fast-FoundationStereo, a family of architectures that achieve, for the first time, strong zero-shot generalization at real-time frame rate. We employ a divide-and-conquer acceleration strategy with three components: (1) knowledge distillation to compress the hybrid backbone into a single efficient student; (2) blockwise neural architecture search for automatically discovering optimal cost filtering designs under latency budgets, reducing search complexity exponentially; and (3) structured pruning for eliminating redundancy in the iterative refinement module. Furthermore, we introduce an automatic pseudo-labeling pipeline used to curate 1.4M in-the-wild stereo pairs to supplement synthetic training data and facilitate knowledge distillation. The resulting model can run over 10x faster than FoundationStereo while closely matching its zero-shot accuracy, thus establishing a new state-of-the-art among real-time methods. Project page: https://nvlabs.github.io/Fast-FoundationStereo/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Fast-FoundationStereo\uff0c\u8fd9\u662f\u4e00\u4e2a\u80fd\u591f\u5728\u4fdd\u6301\u5f3a\u5927\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u5e27\u7387\u7684\u7acb\u4f53\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u3001\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u548c\u7ed3\u6784\u5316\u526a\u679d\u7b49\u52a0\u901f\u7b56\u7565\uff0c\u5728\u901f\u5ea6\u4e0a\u6bd4FoundationStereo\u5feb10\u500d\u4ee5\u4e0a\u3002", "motivation": "\u7acb\u4f53\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\uff1b\u800c\u9ad8\u6548\u7684\u7acb\u4f53\u89c6\u89c9\u67b6\u6784\u5219\u4e3a\u4e86\u901f\u5ea6\u727a\u7272\u4e86\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u9700\u8981\u6602\u8d35\u7684\u9010\u9886\u57df\u5fae\u8c03\u3002\u672c\u6587\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5f00\u53d1\u80fd\u591f\u5728\u4fdd\u6301\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u7684\u7acb\u4f53\u89c6\u89c9\u6a21\u578b\u3002", "method": "\u672c\u6587\u91c7\u7528\u5206\u800c\u6cbb\u4e4b\u7684\u52a0\u901f\u7b56\u7565\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u5c06\u6df7\u5408\u9aa8\u5e72\u7f51\u7edc\u538b\u7f29\u4e3a\u5355\u4e2a\u9ad8\u6548\u5b66\u751f\u6a21\u578b\uff1b\u91c7\u7528\u5206\u5757\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u81ea\u52a8\u53d1\u73b0\u5ef6\u8fdf\u9884\u7b97\u4e0b\u7684\u6700\u4f18\u6210\u672c\u6ee4\u6ce2\u8bbe\u8ba1\uff0c\u5c06\u641c\u7d22\u590d\u6742\u5ea6\u6307\u6570\u7ea7\u964d\u4f4e\uff1b\u4ee5\u53ca\u901a\u8fc7\u7ed3\u6784\u5316\u526a\u679d\u6d88\u9664\u8fed\u4ee3\u7ec6\u5316\u6a21\u5757\u4e2d\u7684\u5197\u4f59\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u81ea\u52a8\u4f2a\u6807\u6ce8\u6d41\u7a0b\uff0c\u6536\u96c6\u4e86140\u4e07\u5f20\u771f\u5b9e\u4e16\u754c\u7acb\u4f53\u56fe\u50cf\u5bf9\u6765\u8865\u5145\u5408\u6210\u8bad\u7ec3\u6570\u636e\u5e76\u4fc3\u8fdb\u77e5\u8bc6\u84b8\u998f\u3002", "result": "Fast-FoundationStereo\u6a21\u578b\u80fd\u591f\u4ee5\u8d85\u8fc710\u500d\u7684\u901f\u5ea6\u8fd0\u884c\u4e8eFoundationStereo\uff0c\u540c\u65f6\u7d27\u5bc6\u5339\u914d\u5176\u96f6\u6837\u672c\u7cbe\u5ea6\uff0c\u4ece\u800c\u5728\u5b9e\u65f6\u65b9\u6cd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u8be5\u6a21\u578b\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u5b9e\u65f6\u5e27\u7387\u4e0b\u7684\u5f3a\u5927\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u901f\u5ea6\u4e0e\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u52a0\u901f\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u7acb\u4f53\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5b9e\u65f6\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u6548\u7acb\u4f53\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u4e3a\u5176\u4ed6\u8ba1\u7b97\u5bc6\u96c6\u578b\u57fa\u7840\u6a21\u578b\u7684\u52a0\u901f\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u77e5\u8bc6\u84b8\u998f\u3001\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u548c\u7ed3\u6784\u5316\u526a\u679d\u5728\u6a21\u578b\u4f18\u5316\u4e2d\u7684\u534f\u540c\u4f5c\u7528\u3002"}}
{"id": "2512.11141", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11141", "abs": "https://arxiv.org/abs/2512.11141", "authors": ["Yiwei Lyu", "Chenhui Zhao", "Soumyanil Banerjee", "Shixuan Liu", "Akshay Rao", "Akhil Kondepudi", "Honglak Lee", "Todd C. Hollon"], "title": "Learning complete and explainable visual representations from itemized text supervision", "comment": null, "summary": "Training vision models with language supervision enables general and transferable representations. However, many visual domains, especially non-object-centric domains such as medical imaging and remote sensing, contain itemized text annotations: multiple text items describing distinct and semantically independent findings within a single image. Such supervision differs from standard multi-caption supervision, where captions are redundant or highly overlapping. Here, we introduce ItemizedCLIP, a framework for learning complete and explainable visual representations from itemized text supervision. ItemizedCLIP employs a cross-attention module to produce text item-conditioned visual embeddings and a set of tailored objectives that jointly enforce item independence (distinct regions for distinct items) and representation completeness (coverage of all items). Across four domains with naturally itemized text supervision (brain MRI, head CT, chest CT, remote sensing) and one additional synthetically itemized dataset, ItemizedCLIP achieves substantial improvements in zero-shot performance and fine-grained interpretability over baselines. The resulting ItemizedCLIP representations are semantically grounded, item-differentiable, complete, and visually interpretable. Our code is available at https://github.com/MLNeurosurg/ItemizedCLIP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ItemizedCLIP\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u9879\u76ee\u5316\u6587\u672c\u76d1\u7763\u4e2d\u5b66\u4e60\u5b8c\u6574\u4e14\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u8868\u793a\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u6a21\u5757\u548c\u5b9a\u5236\u5316\u76ee\u6807\u51fd\u6570\uff0c\u5728\u533b\u5b66\u5f71\u50cf\u548c\u9065\u611f\u7b49\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u96f6\u6837\u672c\u6027\u80fd\u63d0\u5347\u548c\u7ec6\u7c92\u5ea6\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u8bb8\u591a\u89c6\u89c9\u9886\u57df\uff0c\u7279\u522b\u662f\u533b\u5b66\u5f71\u50cf\u548c\u9065\u611f\u7b49\u975e\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u9886\u57df\uff0c\u5305\u542b\u9879\u76ee\u5316\u6587\u672c\u6807\u6ce8\uff1a\u5355\u4e2a\u56fe\u50cf\u4e2d\u6709\u591a\u4e2a\u6587\u672c\u9879\u76ee\u63cf\u8ff0\u72ec\u7acb\u4e14\u8bed\u4e49\u4e0d\u540c\u7684\u53d1\u73b0\u3002\u8fd9\u79cd\u76d1\u7763\u4e0e\u6807\u51c6\u7684\u591a\u6807\u9898\u76d1\u7763\u4e0d\u540c\uff0c\u540e\u8005\u901a\u5e38\u662f\u5197\u4f59\u6216\u9ad8\u5ea6\u91cd\u53e0\u7684\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u79cd\u9879\u76ee\u5316\u76d1\u7763\u7ed3\u6784\u3002", "method": "ItemizedCLIP\u91c7\u7528\u8de8\u6ce8\u610f\u529b\u6a21\u5757\u751f\u6210\u6587\u672c\u9879\u76ee\u6761\u4ef6\u5316\u7684\u89c6\u89c9\u5d4c\u5165\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u5957\u5b9a\u5236\u5316\u76ee\u6807\u51fd\u6570\uff0c\u8054\u5408\u5f3a\u5236\u6267\u884c\u9879\u76ee\u72ec\u7acb\u6027\uff08\u4e0d\u540c\u9879\u76ee\u5bf9\u5e94\u4e0d\u540c\u533a\u57df\uff09\u548c\u8868\u793a\u5b8c\u6574\u6027\uff08\u8986\u76d6\u6240\u6709\u9879\u76ee\uff09\u3002\u8be5\u6846\u67b6\u4e13\u95e8\u9488\u5bf9\u9879\u76ee\u5316\u6587\u672c\u76d1\u7763\u7684\u7ed3\u6784\u7279\u70b9\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u81ea\u7136\u9879\u76ee\u5316\u6587\u672c\u76d1\u7763\u7684\u9886\u57df\uff08\u8111\u90e8MRI\u3001\u5934\u90e8CT\u3001\u80f8\u90e8CT\u3001\u9065\u611f\uff09\u548c\u4e00\u4e2a\u5408\u6210\u9879\u76ee\u5316\u6570\u636e\u96c6\u4e0a\uff0cItemizedCLIP\u5728\u96f6\u6837\u672c\u6027\u80fd\u548c\u7ec6\u7c92\u5ea6\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\u3002\u751f\u6210\u7684\u8868\u793a\u5177\u6709\u8bed\u4e49\u57fa\u7840\u3001\u9879\u76ee\u53ef\u533a\u5206\u6027\u3001\u5b8c\u6574\u6027\u548c\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "ItemizedCLIP\u4e3a\u5904\u7406\u9879\u76ee\u5316\u6587\u672c\u76d1\u7763\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5b66\u4e60\u5230\u8bed\u4e49\u57fa\u7840\u4e14\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u548c\u9065\u611f\u7b49\u9700\u8981\u7ec6\u7c92\u5ea6\u89e3\u91ca\u7684\u9886\u57df\uff0c\u4e3a\u8de8\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u7ed3\u6784\u5316\u76d1\u7763\u5904\u7406\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.11189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11189", "abs": "https://arxiv.org/abs/2512.11189", "authors": ["Anh-Kiet Duong", "Petra Gomez-Kr\u00e4mer"], "title": "Multi-task Learning with Extended Temporal Shift Module for Temporal Action Localization", "comment": "BinEgo360@ICCV25", "summary": "We present our solution to the BinEgo-360 Challenge at ICCV 2025, which focuses on temporal action localization (TAL) in multi-perspective and multi-modal video settings. The challenge provides a dataset containing panoramic, third-person, and egocentric recordings, annotated with fine-grained action classes. Our approach is built on the Temporal Shift Module (TSM), which we extend to handle TAL by introducing a background class and classifying fixed-length non-overlapping intervals. We employ a multi-task learning framework that jointly optimizes for scene classification and TAL, leveraging contextual cues between actions and environments. Finally, we integrate multiple models through a weighted ensemble strategy, which improves robustness and consistency of predictions. Our method is ranked first in both the initial and extended rounds of the competition, demonstrating the effectiveness of combining multi-task learning, an efficient backbone, and ensemble learning for TAL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9BinEgo-360\u6311\u6218\u8d5b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6269\u5c55\u65f6\u5e8f\u79fb\u4f4d\u6a21\u5757(TSM)\u5904\u7406\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u96c6\u6210\u7b56\u7565\uff0c\u5728\u6bd4\u8d5b\u4e2d\u53d6\u5f97\u4e86\u7b2c\u4e00\u540d\u6210\u7ee9\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u591a\u89c6\u89d2\u591a\u6a21\u6001\u89c6\u9891\u4e2d\u7684\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u95ee\u9898\uff0c\u5177\u4f53\u9488\u5bf9BinEgo-360\u6311\u6218\u8d5b\u63d0\u4f9b\u7684\u5305\u542b\u5168\u666f\u3001\u7b2c\u4e09\u4eba\u79f0\u548c\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u7684\u590d\u6742\u89c6\u9891\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6807\u6ce8\u4e86\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u7c7b\u522b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5904\u7406\u8fd9\u79cd\u591a\u6a21\u6001\u65f6\u5e8f\u6570\u636e\u7684\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8e\u65f6\u5e8f\u79fb\u4f4d\u6a21\u5757(TSM)\u8fdb\u884c\u6269\u5c55\uff0c\u901a\u8fc7\u5f15\u5165\u80cc\u666f\u7c7b\u522b\u548c\u5bf9\u56fa\u5b9a\u957f\u5ea6\u975e\u91cd\u53e0\u533a\u95f4\u8fdb\u884c\u5206\u7c7b\u6765\u5904\u7406\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u4efb\u52a1\uff1b\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u8054\u5408\u4f18\u5316\u573a\u666f\u5206\u7c7b\u548c\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\uff0c\u5229\u7528\u52a8\u4f5c\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff1b\u6700\u540e\u901a\u8fc7\u52a0\u6743\u96c6\u6210\u7b56\u7565\u6574\u5408\u591a\u4e2a\u6a21\u578b\uff0c\u63d0\u5347\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728ICCV 2025 BinEgo-360\u6311\u6218\u8d5b\u7684\u521d\u59cb\u8f6e\u548c\u6269\u5c55\u8f6e\u4e2d\u5747\u6392\u540d\u7b2c\u4e00\uff0c\u8bc1\u660e\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u3001\u9ad8\u6548\u9aa8\u5e72\u7f51\u7edc\u548c\u96c6\u6210\u5b66\u4e60\u76f8\u7ed3\u5408\u5728\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u89c6\u89d2\u591a\u6a21\u6001\u89c6\u9891\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u3001\u9ad8\u6548\u7684\u65f6\u5e8f\u5efa\u6a21\u9aa8\u5e72\u7f51\u7edc\u548c\u96c6\u6210\u7b56\u7565\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u590d\u6742\u591a\u89c6\u89d2\u89c6\u9891\u4e2d\u7684\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u95ee\u9898\uff1b\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u591a\u6a21\u6001\u65f6\u5e8f\u6570\u636e\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\uff0c\u5c55\u793a\u4e86\u4e0a\u4e0b\u6587\u4fe1\u606f\u5229\u7528\u548c\u6a21\u578b\u96c6\u6210\u5728\u63d0\u5347\u5b9a\u4f4d\u6027\u80fd\u65b9\u9762\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2512.11203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11203", "abs": "https://arxiv.org/abs/2512.11203", "authors": ["Zhengyang Yu", "Akio Hayakawa", "Masato Ishii", "Qingtao Yu", "Takashi Shibuya", "Jing Zhang", "Yuki Mitsufuji"], "title": "AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path", "comment": null, "summary": "Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAutoRefiner\uff0c\u4e00\u79cd\u4e13\u4e3a\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\u7684\u566a\u58f0\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u8def\u5f84\u5f0f\u566a\u58f0\u4f18\u5316\u548c\u53cd\u5c04KV\u7f13\u5b58\u673a\u5236\uff0c\u5728\u4e0d\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u751f\u6210\u6837\u672c\u7684\u4fdd\u771f\u5ea6\u3002", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u53cc\u5411\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u6837\u672c\u4fdd\u771f\u5ea6\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u73b0\u6709\u57fa\u4e8e\u4f18\u5316\u7684\u63a8\u7406\u65f6\u5bf9\u9f50\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u6587\u672c\u5230\u56fe\u50cf\u9886\u57df\u7684\u566a\u58f0\u4f18\u5316\u5668\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u89c6\u9891\u6a21\u578b\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\u9ad8\u6548\u7684\u566a\u58f0\u4f18\u5316\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51faAutoRefiner\u566a\u58f0\u4f18\u5316\u5668\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\u8def\u5f84\u5f0f\u566a\u58f0\u4f18\u5316\u6cbf\u7740\u968f\u673a\u53bb\u566a\u8def\u5f84\u4f18\u5316\u566a\u58f0\uff0c\u4ee5\u53ca\u53cd\u5c04KV\u7f13\u5b58\u673a\u5236\u6709\u6548\u7ba1\u7406\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u952e\u503c\u7f13\u5b58\u3002\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u8c03\u5236\u91c7\u6837\u566a\u58f0\uff0c\u907f\u514d\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAutoRefiner\u80fd\u6709\u6548\u63d0\u5347\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u6837\u672c\u4fdd\u771f\u5ea6\uff0c\u76f8\u6bd4\u6734\u7d20\u6269\u5c55\u7684\u6587\u672c\u5230\u56fe\u50cf\u566a\u58f0\u4f18\u5316\u5668\u8868\u73b0\u66f4\u4f18\u3002\u8be5\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\uff0c\u53ef\u4f5c\u4e3a\u73b0\u6709\u6a21\u578b\u7684\u589e\u5f3a\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u548c\u4ea4\u4e92\u5e94\u7528\u7279\u6027\u7684\u540c\u65f6\u6539\u5584\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u4e3a\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\u4e13\u7528\u566a\u58f0\u4f18\u5316\u5668\u7684\u5fc5\u8981\u6027\uff0cAutoRefiner\u901a\u8fc7\u8def\u5f84\u5f0f\u4f18\u5316\u548c\u7f13\u5b58\u7ba1\u7406\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u7684\u72ec\u7279\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u4e3a\u63d0\u5347\u89c6\u9891\u751f\u6210\u8d28\u91cf\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u65f6\u6027\u4f18\u52bf\u3002"}}
{"id": "2512.11215", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11215", "abs": "https://arxiv.org/abs/2512.11215", "authors": ["Tianye Qi", "Weihao Li", "Nick Barnes"], "title": "SmokeBench: Evaluating Multimodal Large Language Models for Wildfire Smoke Detection", "comment": "Accepted to WACV 2026", "summary": "Wildfire smoke is transparent, amorphous, and often visually confounded with clouds, making early-stage detection particularly challenging. In this work, we introduce a benchmark, called SmokeBench, to evaluate the ability of multimodal large language models (MLLMs) to recognize and localize wildfire smoke in images. The benchmark consists of four tasks: (1) smoke classification, (2) tile-based smoke localization, (3) grid-based smoke localization, and (4) smoke detection. We evaluate several MLLMs, including Idefics2, Qwen2.5-VL, InternVL3, Unified-IO 2, Grounding DINO, GPT-4o, and Gemini-2.5 Pro. Our results show that while some models can classify the presence of smoke when it covers a large area, all models struggle with accurate localization, especially in the early stages. Further analysis reveals that smoke volume is strongly correlated with model performance, whereas contrast plays a comparatively minor role. These findings highlight critical limitations of current MLLMs for safety-critical wildfire monitoring and underscore the need for methods that improve early-stage smoke localization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SmokeBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91ce\u706b\u70df\u96fe\u8bc6\u522b\u4e0e\u5b9a\u4f4d\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u65e9\u671f\u70df\u96fe\u5b9a\u4f4d\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002", "motivation": "\u91ce\u706b\u70df\u96fe\u5177\u6709\u900f\u660e\u3001\u65e0\u5b9a\u5f62\u4e14\u5e38\u4e0e\u4e91\u5c42\u6df7\u6dc6\u7684\u89c6\u89c9\u7279\u6027\uff0c\u4f7f\u5f97\u65e9\u671f\u68c0\u6d4b\u5c24\u4e3a\u56f0\u96be\uff0c\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u7684\u91ce\u706b\u76d1\u6d4b\u5e94\u7528\u4e2d\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86SmokeBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u70df\u96fe\u5206\u7c7b\u3001\u57fa\u4e8e\u74e6\u7247\u7684\u70df\u96fe\u5b9a\u4f4d\u3001\u57fa\u4e8e\u7f51\u683c\u7684\u70df\u96fe\u5b9a\u4f4d\u548c\u70df\u96fe\u68c0\u6d4b\u56db\u9879\u4efb\u52a1\uff0c\u5e76\u8bc4\u4f30\u4e86Idefics2\u3001Qwen2.5-VL\u3001InternVL3\u3001Unified-IO 2\u3001Grounding DINO\u3001GPT-4o\u548cGemini-2.5 Pro\u7b49\u591a\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u90e8\u5206\u6a21\u578b\u80fd\u591f\u5728\u70df\u96fe\u8986\u76d6\u5927\u9762\u79ef\u65f6\u8fdb\u884c\u5206\u7c7b\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u7cbe\u786e\u5b9a\u4f4d\u65b9\u9762\u5747\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u65e9\u671f\u9636\u6bb5\uff1b\u70df\u96fe\u4f53\u79ef\u4e0e\u6a21\u578b\u6027\u80fd\u5f3a\u76f8\u5173\uff0c\u800c\u5bf9\u6bd4\u5ea6\u7684\u5f71\u54cd\u76f8\u5bf9\u8f83\u5c0f\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u91ce\u706b\u76d1\u6d4b\u4e2d\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u5f00\u53d1\u6539\u8fdb\u65e9\u671f\u70df\u96fe\u5b9a\u4f4d\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2512.11234", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11234", "abs": "https://arxiv.org/abs/2512.11234", "authors": ["Wentang Chen", "Shougao Zhang", "Yiman Zhang", "Tianhao Zhou", "Ruihui Li"], "title": "RoomPilot: Controllable Synthesis of Interactive Indoor Environments via Multimodal Semantic Parsing", "comment": "20 pages, 6 figures", "summary": "Generating controllable and interactive indoor scenes is fundamental to applications in game development, architectural visualization, and embodied AI training. Yet existing approaches either handle a narrow range of input modalities or rely on stochastic processes that hinder controllability. To overcome these limitations, we introduce RoomPilot, a unified framework that parses diverse multi-modal inputs--textual descriptions or CAD floor plans--into an Indoor Domain-Specific Language (IDSL) for indoor structured scene generation. The key insight is that a well-designed IDSL can act as a shared semantic representation, enabling coherent, high-quality scene synthesis from any single modality while maintaining interaction semantics. In contrast to conventional procedural methods that produce visually plausible but functionally inert layouts, RoomPilot leverages a curated dataset of interaction-annotated assets to synthesize environments exhibiting realistic object behaviors. Extensive experiments further validate its strong multi-modal understanding, fine-grained controllability in scene generation, and superior physical consistency and visual fidelity, marking a significant step toward general-purpose controllable 3D indoor scene generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RoomPilot\uff0c\u4e00\u4e2a\u7528\u4e8e\u53ef\u63a7\u5ba4\u5185\u573a\u666f\u751f\u6210\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u6587\u672c\u63cf\u8ff0\u6216CAD\u5e73\u9762\u56fe\u7b49\u591a\u6a21\u6001\u8f93\u5165\u89e3\u6790\u4e3a\u5ba4\u5185\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u4ece\u800c\u751f\u6210\u5177\u6709\u4ea4\u4e92\u8bed\u4e49\u7684\u7ed3\u6784\u5316\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u5ba4\u5185\u573a\u666f\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u4e00\u662f\u4ec5\u652f\u6301\u6709\u9650\u7684\u8f93\u5165\u6a21\u6001\u8303\u56f4\uff0c\u4e8c\u662f\u4f9d\u8d56\u968f\u673a\u8fc7\u7a0b\u5bfc\u81f4\u53ef\u63a7\u6027\u4e0d\u8db3\u3002\u8fd9\u9650\u5236\u4e86\u5728\u6e38\u620f\u5f00\u53d1\u3001\u5efa\u7b51\u53ef\u89c6\u5316\u548c\u5177\u8eabAI\u8bad\u7ec3\u7b49\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u7279\u522b\u662f\u65e0\u6cd5\u751f\u6210\u5177\u6709\u771f\u5b9e\u4ea4\u4e92\u884c\u4e3a\u7684\u573a\u666f\u3002", "method": "RoomPilot\u7684\u6838\u5fc3\u65b9\u6cd5\u662f\u5f15\u5165\u5ba4\u5185\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u4f5c\u4e3a\u5171\u4eab\u8bed\u4e49\u8868\u793a\uff0c\u5c06\u6587\u672c\u63cf\u8ff0\u6216CAD\u5e73\u9762\u56fe\u7b49\u591a\u6a21\u6001\u8f93\u5165\u7edf\u4e00\u89e3\u6790\u4e3aIDSL\u3002\u8be5\u6846\u67b6\u5229\u7528\u5305\u542b\u4ea4\u4e92\u6807\u6ce8\u7684\u8d44\u4ea7\u6570\u636e\u96c6\uff0c\u751f\u6210\u5177\u6709\u771f\u5b9e\u5bf9\u8c61\u884c\u4e3a\u7684\u573a\u666f\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7a0b\u5e8f\u5316\u65b9\u6cd5\u4ea7\u751f\u7684\u89c6\u89c9\u5408\u7406\u4f46\u529f\u80fd\u50f5\u5316\u7684\u5e03\u5c40\u95ee\u9898\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RoomPilot\u5728\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5728\u573a\u666f\u751f\u6210\u4e2d\u7684\u7ec6\u7c92\u5ea6\u53ef\u63a7\u6027\uff0c\u4ee5\u53ca\u5728\u7269\u7406\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u7684\u4f18\u8d8a\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u5177\u6709\u4ea4\u4e92\u8bed\u4e49\u7684\u5ba4\u5185\u573a\u666f\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u53ef\u4ee5\u4f5c\u4e3a\u5171\u4eab\u8bed\u4e49\u8868\u793a\uff0c\u5b9e\u73b0\u4ece\u5355\u4e00\u6a21\u6001\u5230\u9ad8\u8d28\u91cf\u573a\u666f\u7684\u8fde\u8d2f\u5408\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u4ea4\u4e92\u8bed\u4e49\u3002\u8fd9\u9879\u5de5\u4f5c\u6807\u5fd7\u7740\u5411\u901a\u7528\u53ef\u63a73D\u5ba4\u5185\u573a\u666f\u751f\u6210\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u591a\u6a21\u6001\u8f93\u5165\u7684\u7edf\u4e00\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2512.11239", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11239", "abs": "https://arxiv.org/abs/2512.11239", "authors": ["Wen-Jue He", "Xiaofeng Zhu", "Zheng Zhang"], "title": "Cross-modal Prompting for Balanced Incomplete Multi-modal Emotion Recognition", "comment": "Accepted by AAAI 2026", "summary": "Incomplete multi-modal emotion recognition (IMER) aims at understanding human intentions and sentiments by comprehensively exploring the partially observed multi-source data. Although the multi-modal data is expected to provide more abundant information, the performance gap and modality under-optimization problem hinder effective multi-modal learning in practice, and are exacerbated in the confrontation of the missing data. To address this issue, we devise a novel Cross-modal Prompting (ComP) method, which emphasizes coherent information by enhancing modality-specific features and improves the overall recognition accuracy by boosting each modality's performance. Specifically, a progressive prompt generation module with a dynamic gradient modulator is proposed to produce concise and consistent modality semantic cues. Meanwhile, cross-modal knowledge propagation selectively amplifies the consistent information in modality features with the delivered prompts to enhance the discrimination of the modality-specific output. Additionally, a coordinator is designed to dynamically re-weight the modality outputs as a complement to the balance strategy to improve the model's efficacy. Extensive experiments on 4 datasets with 7 SOTA methods under different missing rates validate the effectiveness of our proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u6a21\u6001\u63d0\u793a\uff08ComP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u548c\u63d0\u5347\u5404\u6a21\u6001\u6027\u80fd\u6765\u89e3\u51b3\u4e0d\u5b8c\u5168\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u6027\u80fd\u5dee\u8ddd\u548c\u6a21\u6001\u6b20\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u4e0d\u5b8c\u5168\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u9762\u4e34\u591a\u6a21\u6001\u6570\u636e\u90e8\u5206\u7f3a\u5931\u7684\u6311\u6218\uff0c\u5176\u4e2d\u6027\u80fd\u5dee\u8ddd\u548c\u6a21\u6001\u6b20\u4f18\u5316\u95ee\u9898\u5728\u5b9e\u8df5\u4e2d\u963b\u788d\u4e86\u6709\u6548\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u5e76\u5728\u6570\u636e\u7f3a\u5931\u60c5\u51b5\u4e0b\u8fdb\u4e00\u6b65\u52a0\u5267\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u5e76\u63d0\u5347\u6574\u4f53\u8bc6\u522b\u7cbe\u5ea6\u3002", "method": "\u8be5\u65b9\u6cd5\u8bbe\u8ba1\u4e86\u8de8\u6a21\u6001\u63d0\u793a\u65b9\u6cd5\uff0c\u5305\u62ec\u5177\u6709\u52a8\u6001\u68af\u5ea6\u8c03\u5236\u5668\u7684\u6e10\u8fdb\u63d0\u793a\u751f\u6210\u6a21\u5757\u6765\u4ea7\u751f\u7b80\u6d01\u4e00\u81f4\u7684\u6a21\u6001\u8bed\u4e49\u7ebf\u7d22\uff0c\u8de8\u6a21\u6001\u77e5\u8bc6\u4f20\u64ad\u901a\u8fc7\u4f20\u9012\u7684\u63d0\u793a\u9009\u62e9\u6027\u5730\u653e\u5927\u6a21\u6001\u7279\u5f81\u4e2d\u7684\u4e00\u81f4\u4fe1\u606f\u4ee5\u589e\u5f3a\u6a21\u6001\u7279\u5b9a\u8f93\u51fa\u7684\u533a\u5206\u5ea6\uff0c\u4ee5\u53ca\u8bbe\u8ba1\u534f\u8c03\u5668\u52a8\u6001\u91cd\u65b0\u52a0\u6743\u6a21\u6001\u8f93\u51fa\u4f5c\u4e3a\u5e73\u8861\u7b56\u7565\u7684\u8865\u5145\u3002", "result": "\u57284\u4e2a\u6570\u636e\u96c6\u4e0a\u5bf97\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u4e0d\u540c\u7f3a\u5931\u7387\u4e0b\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u5404\u6a21\u6001\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u4e86\u4e0d\u5b8c\u5168\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u8de8\u6a21\u6001\u63d0\u793a\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5b8c\u5168\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u51fa\u7684\u6e10\u8fdb\u63d0\u793a\u751f\u6210\u548c\u52a8\u6001\u534f\u8c03\u7b56\u7565\u4e3a\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u7f3a\u5931\u548c\u6027\u80fd\u4e0d\u5e73\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.11321", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11321", "abs": "https://arxiv.org/abs/2512.11321", "authors": ["Jingchao Wu", "Zejian Kang", "Haibo Liu", "Yuanchen Fei", "Xiangru Huang"], "title": "KeyframeFace: From Text to Expressive Facial Keyframes", "comment": null, "summary": "Generating dynamic 3D facial animation from natural language requires understanding both temporally structured semantics and fine-grained expression changes. Existing datasets and methods mainly focus on speech-driven animation or unstructured expression sequences and therefore lack the semantic grounding and temporal structures needed for expressive human performance generation. In this work, we introduce KeyframeFace, a large-scale multimodal dataset designed for text-to-animation research through keyframe-level supervision. KeyframeFace provides 2,100 expressive scripts paired with monocular videos, per-frame ARKit coefficients, contextual backgrounds, complex emotions, manually defined keyframes, and multi-perspective annotations based on ARKit coefficients and images via Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Beyond the dataset, we propose the first text-to-animation framework that explicitly leverages LLM priors for interpretable facial motion synthesis. This design aligns the semantic understanding capabilities of LLMs with the interpretable structure of ARKit's coefficients, enabling high-fidelity expressive animation. KeyframeFace and our LLM-based framework together establish a new foundation for interpretable, keyframe-guided, and context-aware text-to-animation. Code and data are available at https://github.com/wjc12345123/KeyframeFace.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86KeyframeFace\u6570\u636e\u96c6\u548c\u57fa\u4e8eLLM\u7684\u6587\u672c\u5230\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u952e\u5e27\u7ea7\u76d1\u7763\u548cLLM\u5148\u9a8c\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u52a8\u60013D\u9762\u90e8\u52a8\u753b\u65f6\u7f3a\u4e4f\u8bed\u4e49\u57fa\u7840\u548c\u65f6\u5e8f\u7ed3\u6784\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bed\u97f3\u9a71\u52a8\u52a8\u753b\u6216\u65e0\u7ed3\u6784\u8868\u60c5\u5e8f\u5217\uff0c\u7f3a\u4e4f\u8bed\u4e49\u57fa\u7840\u548c\u65f6\u5e8f\u7ed3\u6784\uff0c\u96be\u4ee5\u751f\u6210\u5177\u6709\u8868\u73b0\u529b\u7684\u4eba\u7c7b\u8868\u6f14\u52a8\u753b\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u52a8\u60013D\u9762\u90e8\u52a8\u753b\u65f6\u7406\u89e3\u65f6\u5e8f\u8bed\u4e49\u548c\u7ec6\u7c92\u5ea6\u8868\u60c5\u53d8\u5316\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86KeyframeFace\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b2100\u4e2a\u8868\u73b0\u529b\u811a\u672c\u3001\u5355\u76ee\u89c6\u9891\u3001\u9010\u5e27ARKit\u7cfb\u6570\u3001\u4e0a\u4e0b\u6587\u80cc\u666f\u3001\u590d\u6742\u60c5\u611f\u548c\u624b\u52a8\u5b9a\u4e49\u5173\u952e\u5e27\uff0c\u5e76\u901a\u8fc7LLM\u548cMLLM\u8fdb\u884c\u591a\u89c6\u89d2\u6807\u6ce8\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u9996\u4e2a\u5229\u7528LLM\u5148\u9a8c\u8fdb\u884c\u53ef\u89e3\u91ca\u9762\u90e8\u8fd0\u52a8\u5408\u6210\u7684\u6587\u672c\u5230\u52a8\u753b\u6846\u67b6\uff0c\u5c06LLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u4e0eARKit\u7cfb\u6570\u7684\u53ef\u89e3\u91ca\u7ed3\u6784\u5bf9\u9f50\u3002", "result": "KeyframeFace\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u591a\u6a21\u6001\u6807\u6ce8\u8d44\u6e90\uff0c\u57fa\u4e8eLLM\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u8868\u73b0\u529b\u52a8\u753b\u751f\u6210\uff0c\u5171\u540c\u5efa\u7acb\u4e86\u53ef\u89e3\u91ca\u3001\u5173\u952e\u5e27\u5f15\u5bfc\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6587\u672c\u5230\u52a8\u753b\u65b0\u57fa\u7840\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5728GitHub\u4e0a\u516c\u5f00\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6570\u636e\u96c6\u548c\u6846\u67b6\u7684\u7ed3\u5408\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u9762\u90e8\u52a8\u753b\u751f\u6210\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u5c06LLM\u7684\u8bed\u4e49\u7406\u89e3\u4e0e\u53c2\u6570\u5316\u9762\u90e8\u6a21\u578b\u7684\u7ed3\u6784\u5316\u8868\u793a\u76f8\u7ed3\u5408\uff0c\u4e3a\u672a\u6765\u6587\u672c\u9a71\u52a8\u52a8\u753b\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u548c\u65b9\u6cd5\u8bba\u57fa\u7840\u3002"}}
{"id": "2512.11336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11336", "abs": "https://arxiv.org/abs/2512.11336", "authors": ["Hewen Pan", "Cong Wei", "Dashuang Liang", "Zepeng Huang", "Pengfei Gao", "Ziqi Zhou", "Lulu Xue", "Pengfei Yan", "Xiaoming Wei", "Minghui Li", "Shengshan Hu"], "title": "UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models", "comment": "22 pages, 13 figures, technical report", "summary": "With the advancement of multi-modal Large Language Models (LLMs), Video LLMs have been further developed to perform on holistic and specialized video understanding. However, existing works are limited to specialized video understanding tasks, failing to achieve a comprehensive and multi-grained video perception. To bridge this gap, we introduce UFVideo, the first Video LLM with unified multi-grained cooperative understanding capabilities. Specifically, we design unified visual-language guided alignment to flexibly handle video understanding across global, pixel and temporal scales within a single model. UFVideo dynamically encodes the visual and text inputs of different tasks and generates the textual response, temporal localization, or grounded mask. Additionally, to evaluate challenging multi-grained video understanding tasks, we construct the UFVideo-Bench consisting of three distinct collaborative tasks within the scales, which demonstrates UFVideo's flexibility and advantages over GPT-4o. Furthermore, we validate the effectiveness of our model across 9 public benchmarks covering various common video understanding tasks, providing valuable insights for future Video LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UFVideo\uff0c\u9996\u4e2a\u5177\u5907\u7edf\u4e00\u591a\u7c92\u5ea6\u534f\u540c\u7406\u89e3\u80fd\u529b\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00\u5f15\u5bfc\u5bf9\u9f50\u673a\u5236\uff0c\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u5168\u5c40\u3001\u50cf\u7d20\u548c\u65f6\u95f4\u5c3a\u5ea6\u7684\u89c6\u9891\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u7279\u5b9a\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5168\u9762\u4e14\u591a\u7c92\u5ea6\u7684\u89c6\u9891\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u5b9e\u73b0\u8de8\u4e0d\u540c\u5c3a\u5ea6\u7684\u7edf\u4e00\u89c6\u9891\u7406\u89e3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00\u5f15\u5bfc\u5bf9\u9f50\u673a\u5236\uff0c\u80fd\u591f\u7075\u6d3b\u5904\u7406\u5168\u5c40\u3001\u50cf\u7d20\u548c\u65f6\u95f4\u5c3a\u5ea6\u7684\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u6a21\u578b\u52a8\u6001\u7f16\u7801\u4e0d\u540c\u4efb\u52a1\u7684\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\uff0c\u5e76\u751f\u6210\u6587\u672c\u54cd\u5e94\u3001\u65f6\u95f4\u5b9a\u4f4d\u6216\u57fa\u7840\u63a9\u7801\u3002", "result": "\u6784\u5efa\u4e86UFVideo-Bench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b\u4e09\u4e2a\u4e0d\u540c\u5c3a\u5ea6\u7684\u534f\u4f5c\u4efb\u52a1\uff0c\u5b9e\u9a8c\u8868\u660eUFVideo\u5728\u7075\u6d3b\u6027\u65b9\u9762\u4f18\u4e8eGPT-4o\uff0c\u540c\u65f6\u57289\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u5404\u79cd\u5e38\u89c1\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "UFVideo\u5c55\u793a\u4e86\u7edf\u4e00\u591a\u7c92\u5ea6\u89c6\u9891\u7406\u89e3\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u8bc1\u660e\u4e86\u5355\u4e00\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5904\u7406\u4e0d\u540c\u5c3a\u5ea6\u7684\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u3002"}}
{"id": "2512.11340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11340", "abs": "https://arxiv.org/abs/2512.11340", "authors": ["Fei Long", "Yao Zhang", "Jiaming Lv", "Jiangtao Xie", "Peihua Li"], "title": "Task-Specific Distance Correlation Matching for Few-Shot Action Recognition", "comment": "9 pages. 4 figures, conference", "summary": "Few-shot action recognition (FSAR) has recently made notable progress through set matching and efficient adaptation of large-scale pre-trained models. However, two key limitations persist. First, existing set matching metrics typically rely on cosine similarity to measure inter-frame linear dependencies and then perform matching with only instance-level information, thus failing to capture more complex patterns such as nonlinear relationships and overlooking task-specific cues. Second, for efficient adaptation of CLIP to FSAR, recent work performing fine-tuning via skip-fusion layers (which we refer to as side layers) has significantly reduced memory cost. However, the newly introduced side layers are often difficult to optimize under limited data conditions. To address these limitations, we propose TS-FSAR, a framework comprising three components: (1) a visual Ladder Side Network (LSN) for efficient CLIP fine-tuning; (2) a metric called Task-Specific Distance Correlation Matching (TS-DCM), which uses $\u03b1$-distance correlation to model both linear and nonlinear inter-frame dependencies and leverages a task prototype to enable task-specific matching; and (3) a Guiding LSN with Adapted CLIP (GLAC) module, which regularizes LSN using the adapted frozen CLIP to improve training for better $\u03b1$-distance correlation estimation under limited supervision. Extensive experiments on five widely-used benchmarks demonstrate that our TS-FSAR yields superior performance compared to prior state-of-the-arts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTS-FSAR\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u9636\u68af\u4fa7\u7f51\u7edc\u5b9e\u73b0CLIP\u7684\u9ad8\u6548\u5fae\u8c03\uff0c\u5e76\u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u7684\u8ddd\u79bb\u76f8\u5173\u5339\u914d\u5ea6\u91cf\uff0c\u4ee5\u89e3\u51b3\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u4e2d\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u5173\u7cfb\u5efa\u6a21\u548c\u6709\u9650\u6570\u636e\u4e0b\u4f18\u5316\u56f0\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u73b0\u6709\u96c6\u5408\u5339\u914d\u5ea6\u91cf\u901a\u5e38\u4f9d\u8d56\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u5e27\u95f4\u7ebf\u6027\u4f9d\u8d56\uff0c\u4ec5\u5229\u7528\u5b9e\u4f8b\u7ea7\u4fe1\u606f\u8fdb\u884c\u5339\u914d\uff0c\u65e0\u6cd5\u6355\u6349\u975e\u7ebf\u6027\u5173\u7cfb\u7b49\u590d\u6742\u6a21\u5f0f\u4e14\u5ffd\u89c6\u4efb\u52a1\u7279\u5b9a\u7ebf\u7d22\uff1b\u540c\u65f6\uff0c\u901a\u8fc7\u8df3\u8dc3\u878d\u5408\u5c42\u5bf9CLIP\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\u7684\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u65b0\u5f15\u5165\u7684\u4fa7\u5c42\u96be\u4ee5\u4f18\u5316\u3002", "method": "TS-FSAR\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u89c6\u89c9\u9636\u68af\u4fa7\u7f51\u7edc\u7528\u4e8eCLIP\u7684\u9ad8\u6548\u5fae\u8c03\uff1b\u4efb\u52a1\u7279\u5b9a\u8ddd\u79bb\u76f8\u5173\u5339\u914d\u5ea6\u91cf\u4f7f\u7528\u03b1-\u8ddd\u79bb\u76f8\u5173\u5efa\u6a21\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u5e27\u95f4\u4f9d\u8d56\uff0c\u5e76\u5229\u7528\u4efb\u52a1\u539f\u578b\u5b9e\u73b0\u4efb\u52a1\u7279\u5b9a\u5339\u914d\uff1b\u5f15\u5bfcLSN\u4e0e\u9002\u5e94CLIP\u6a21\u5757\u901a\u8fc7\u9002\u5e94\u540e\u7684\u51bb\u7ed3CLIP\u6b63\u5219\u5316LSN\uff0c\u4ee5\u5728\u6709\u9650\u76d1\u7763\u4e0b\u6539\u8fdb\u03b1-\u8ddd\u79bb\u76f8\u5173\u4f30\u8ba1\u7684\u8bad\u7ec3\u3002", "result": "\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTS-FSAR\u76f8\u6bd4\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u5728\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u975e\u7ebf\u6027\u5339\u914d\u5ea6\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u590d\u6742\u6a21\u5f0f\u5efa\u6a21\u548c\u6a21\u578b\u4f18\u5316\u56f0\u96be\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.11393", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11393", "abs": "https://arxiv.org/abs/2512.11393", "authors": ["Zhifan Zhu", "Yifei Huang", "Yoichi Sato", "Dima Damen"], "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video", "comment": "project webpage: https://zhifanzhu.github.io/ego-nbody", "summary": "Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86N-Body\u95ee\u9898\uff0c\u65e8\u5728\u4ece\u5355\u89c6\u89d2\u89c6\u9891\u4e2d\u5b66\u4e60\u4eba\u7c7b\u6d3b\u52a8\u7684\u5e76\u884c\u5316\u6267\u884c\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\u5f15\u5bfc\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u74063D\u73af\u5883\u3001\u7269\u4f53\u4f7f\u7528\u548c\u65f6\u5e8f\u4f9d\u8d56\uff0c\u751f\u6210\u53ef\u884c\u7684\u5e76\u884c\u6267\u884c\u65b9\u6848\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u76f4\u89c2\u5730\u5e76\u884c\u5316\u590d\u6742\u6d3b\u52a8\uff0c\u4f46\u6a21\u578b\u80fd\u5426\u4ece\u89c2\u5bdf\u5355\u4e2a\u4eba\u7684\u89c6\u9891\u4e2d\u5b66\u4e60\u8fd9\u79cd\u80fd\u529b\uff1f\u7ed9\u5b9a\u4e00\u4e2a\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\uff0c\u7814\u7a76N-Body\u95ee\u9898\uff1aN\u4e2a\u4e2a\u4f53\u5982\u4f55\u5047\u8bbe\u6027\u5730\u6267\u884c\u89c6\u9891\u4e2d\u89c2\u5bdf\u5230\u7684\u76f8\u540c\u4efb\u52a1\u96c6\u5408\uff0c\u76ee\u6807\u662f\u6700\u5927\u5316\u52a0\u901f\uff0c\u4f46\u7b80\u5355\u7684\u89c6\u9891\u7247\u6bb5\u5206\u914d\u5e38\u8fdd\u53cd\u73b0\u5b9e\u7ea6\u675f\uff0c\u5bfc\u81f4\u7269\u7406\u4e0a\u4e0d\u53ef\u80fd\u7684\u573a\u666f\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f62\u5f0f\u5316\u4e86N-Body\u95ee\u9898\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u8bc4\u4f30\u6307\u6807\u6765\u8861\u91cf\u6027\u80fd\uff08\u52a0\u901f\u3001\u4efb\u52a1\u8986\u76d6\u7387\uff09\u548c\u53ef\u884c\u6027\uff08\u7a7a\u95f4\u78b0\u649e\u3001\u7269\u4f53\u51b2\u7a81\u548c\u56e0\u679c\u7ea6\u675f\uff09\u3002\u5f15\u5165\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\uff0c\u5f15\u5bfc\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u74063D\u73af\u5883\u3001\u7269\u4f53\u4f7f\u7528\u548c\u65f6\u5e8f\u4f9d\u8d56\uff0c\u4ee5\u751f\u6210\u53ef\u884c\u7684\u5e76\u884c\u6267\u884c\u65b9\u6848\u3002", "result": "\u5728EPIC-Kitchens\u548cHD-EPIC\u7684100\u4e2a\u89c6\u9891\u4e0a\uff0c\u5f53N=2\u65f6\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4Gemini 2.5 Pro\u7684\u57fa\u7ebf\u63d0\u793a\u5c06\u52a8\u4f5c\u8986\u76d6\u7387\u63d0\u9ad8\u4e8645%\uff0c\u540c\u65f6\u5c06\u78b0\u649e\u7387\u3001\u7269\u4f53\u51b2\u7a81\u548c\u56e0\u679c\u51b2\u7a81\u5206\u522b\u964d\u4f4e\u4e8655%\u300145%\u548c55%\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4ece\u5355\u89c6\u89d2\u89c6\u9891\u5b66\u4e60\u6d3b\u52a8\u5e76\u884c\u5316\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u7684\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u5b9e\u7ea6\u675f\u95ee\u9898\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u548c\u6548\u7387\u4f18\u5316\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u540c\u65f6\u5f62\u5f0f\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2512.11395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11395", "abs": "https://arxiv.org/abs/2512.11395", "authors": ["Yilei Jiang", "Zhen Wang", "Yanghao Wang", "Jun Yu", "Yueting Zhuang", "Jun Xiao", "Long Chen"], "title": "FlowDC: Flow-Based Decoupling-Decay for Complex Image Editing", "comment": null, "summary": "With the surge of pre-trained text-to-image flow matching models, text-based image editing performance has gained remarkable improvement, especially for \\underline{simple editing} that only contains a single editing target. To satisfy the exploding editing requirements, the \\underline{complex editing} which contains multiple editing targets has posed as a more challenging task. However, current complex editing solutions: single-round and multi-round editing are limited by long text following and cumulative inconsistency, respectively. Thus, they struggle to strike a balance between semantic alignment and source consistency. In this paper, we propose \\textbf{FlowDC}, which decouples the complex editing into multiple sub-editing effects and superposes them in parallel during the editing process. Meanwhile, we observed that the velocity quantity that is orthogonal to the editing displacement harms the source structure preserving. Thus, we decompose the velocity and decay the orthogonal part for better source consistency. To evaluate the effectiveness of complex editing settings, we construct a complex editing benchmark: Complex-PIE-Bench. On two benchmarks, FlowDC shows superior results compared with existing methods. We also detail the ablations of our module designs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFlowDC\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u89e3\u8026\u4e3a\u591a\u4e2a\u5b50\u7f16\u8f91\u6548\u679c\u5e76\u8fdb\u884c\u5e76\u884c\u53e0\u52a0\uff0c\u540c\u65f6\u5206\u89e3\u901f\u5ea6\u573a\u5e76\u8870\u51cf\u6b63\u4ea4\u5206\u91cf\uff0c\u4ee5\u89e3\u51b3\u591a\u76ee\u6807\u590d\u6742\u7f16\u8f91\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u4e0e\u6e90\u4e00\u81f4\u6027\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6d41\u5339\u914d\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u5904\u7406\u7b80\u5355\u7f16\u8f91\u4efb\u52a1\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5904\u7406\u5305\u542b\u591a\u4e2a\u7f16\u8f91\u76ee\u6807\u7684\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u65f6\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7684\u5355\u8f6e\u7f16\u8f91\u548c\u591a\u8f6e\u7f16\u8f91\u65b9\u6cd5\u5206\u522b\u53d7\u5230\u957f\u6587\u672c\u8ddf\u968f\u548c\u7d2f\u79ef\u4e0d\u4e00\u81f4\u6027\u7684\u9650\u5236\uff0c\u96be\u4ee5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u6e90\u4e00\u81f4\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "FlowDC\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u5c06\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u89e3\u8026\u4e3a\u591a\u4e2a\u5b50\u7f16\u8f91\u6548\u679c\uff0c\u5e76\u5728\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u5e76\u884c\u53e0\u52a0\u8fd9\u4e9b\u6548\u679c\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u89c2\u5bdf\u5230\u4e0e\u7f16\u8f91\u4f4d\u79fb\u6b63\u4ea4\u7684\u901f\u5ea6\u5206\u91cf\u4f1a\u635f\u5bb3\u6e90\u7ed3\u6784\u4fdd\u6301\uff0c\u56e0\u6b64\u5bf9\u901f\u5ea6\u573a\u8fdb\u884c\u5206\u89e3\u5e76\u8870\u51cf\u6b63\u4ea4\u90e8\u5206\u4ee5\u66f4\u597d\u5730\u4fdd\u6301\u6e90\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u6784\u5efa\u4e86\u590d\u6742\u7f16\u8f91\u57fa\u51c6Complex-PIE-Bench\u7528\u4e8e\u8bc4\u4f30\u65b9\u6cd5\u6027\u80fd\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlowDC\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u8bed\u4e49\u5bf9\u9f50\u548c\u6e90\u4e00\u81f4\u6027\u4e4b\u95f4\u7684\u66f4\u597d\u5e73\u8861\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u8be6\u7ec6\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "FlowDC\u901a\u8fc7\u89e3\u8026\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u548c\u4f18\u5316\u901f\u5ea6\u573a\u5206\u89e3\uff0c\u4e3a\u591a\u76ee\u6807\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u590d\u6742\u7f16\u8f91\u7684\u6027\u80fd\uff0c\u8fd8\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u601d\u8def\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.11490", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.11490", "abs": "https://arxiv.org/abs/2512.11490", "authors": ["Emanuel S\u00e1nchez Aimar", "Gulnaz Zhambulova", "Fahad Shahbaz Khan", "Yonghao Xu", "Michael Felsberg"], "title": "VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing", "comment": "21 pages, 7 figures, under review", "summary": "Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVLM2GeoVec\uff0c\u4e00\u79cd\u6307\u4ee4\u8ddf\u968f\u7684\u5355\u7f16\u7801\u5668\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06\u4ea4\u9519\u8f93\u5165\uff08\u56fe\u50cf\u3001\u6587\u672c\u3001\u8fb9\u754c\u6846\u548c\u5730\u7406\u5750\u6807\uff09\u5d4c\u5165\u7edf\u4e00\u5411\u91cf\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u9065\u611f\u9886\u57df\u4e2d\u68c0\u7d22\u6a21\u578b\u4e0e\u751f\u6210\u6a21\u578b\u4e4b\u95f4\u7684\u5272\u88c2\u95ee\u9898\u3002", "motivation": "\u536b\u661f\u56fe\u50cf\u4e0e\u81ea\u7136\u56fe\u50cf\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff0c\u5176\u822a\u62cd\u89c6\u89d2\u3001\u8d85\u9ad8\u5206\u8fa8\u7387\u3001\u5c3a\u5ea6\u53d8\u5316\u591a\u6837\u548c\u5c0f\u7269\u4f53\u4e30\u5bcc\u7b49\u7279\u70b9\u9700\u8981\u533a\u57df\u7ea7\u7a7a\u95f4\u63a8\u7406\u548c\u6574\u4f53\u573a\u666f\u7406\u89e3\u3002\u5f53\u524d\u9065\u611f\u65b9\u6cd5\u5728\u53cc\u7f16\u7801\u5668\u68c0\u7d22\u6a21\u578b\uff08\u64c5\u957f\u5927\u89c4\u6a21\u8de8\u6a21\u6001\u641c\u7d22\u4f46\u65e0\u6cd5\u4ea4\u9519\u6a21\u6001\uff09\u548c\u751f\u6210\u52a9\u624b\uff08\u652f\u6301\u533a\u57df\u7ea7\u89e3\u91ca\u4f46\u7f3a\u4e4f\u53ef\u6269\u5c55\u68c0\u7d22\u80fd\u529b\uff09\u4e4b\u95f4\u5b58\u5728\u5272\u88c2\uff0c\u9700\u8981\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faVLM2GeoVec\u6a21\u578b\uff0c\u91c7\u7528\u6307\u4ee4\u8ddf\u968f\u7684\u5355\u7f16\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06\u4ea4\u9519\u8f93\u5165\uff08\u56fe\u50cf\u3001\u6587\u672c\u3001\u8fb9\u754c\u6846\u548c\u5730\u7406\u5750\u6807\uff09\u5d4c\u5165\u7edf\u4e00\u5411\u91cf\u7a7a\u95f4\u3002\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\u548c\u4efb\u52a1\u7279\u5b9a\u6a21\u5757\uff0c\u5e76\u5f15\u5165RSMEB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u573a\u666f\u5206\u7c7b\u3001\u8de8\u6a21\u6001\u641c\u7d22\u3001\u7ec4\u5408\u68c0\u7d22\u3001\u89c6\u89c9\u95ee\u7b54\u3001\u89c6\u89c9\u5b9a\u4f4d\u548c\u533a\u57df\u7ea7\u63a8\u7406\u4ee5\u53ca\u8bed\u4e49\u5730\u7406\u7a7a\u95f4\u68c0\u7d22\u7b49\u5173\u952e\u9065\u611f\u5d4c\u5165\u5e94\u7528\u3002", "result": "\u5728RSMEB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVLM2GeoVec\u5728\u533a\u57df-\u5b57\u5e55\u68c0\u7d22\u4e0a\u8fbe\u523026.6%\u7684P@1\uff08\u6bd4\u53cc\u7f16\u7801\u5668\u57fa\u7ebf\u63d0\u534725\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u5728\u6307\u4ee3\u8868\u8fbe\u68c0\u7d22\u4e0a\u8fbe\u523032.5%\u7684P@1\uff08\u63d0\u534719\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u5728\u8bed\u4e49\u5730\u7406\u5b9a\u4f4d\u68c0\u7d22\u4e0a\u8fbe\u523017.8%\u7684P@1\uff08\u8d85\u8fc7\u5148\u524d\u6700\u4f73\u7ed3\u679c3\u500d\u4ee5\u4e0a\uff09\uff0c\u540c\u65f6\u5728\u573a\u666f\u5206\u7c7b\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u7b49\u4f20\u7edf\u4efb\u52a1\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u4e13\u7528\u57fa\u7ebf\u3002", "conclusion": "VLM2GeoVec\u7edf\u4e00\u4e86\u53ef\u6269\u5c55\u68c0\u7d22\u4e0e\u533a\u57df\u7ea7\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9065\u611f\u4e2d\u8fde\u8d2f\u7684\u591a\u6a21\u6001\u5206\u6790\u3002\u8be5\u7814\u7a76\u4e3a\u9065\u611f\u9886\u57df\u63d0\u4f9b\u4e86\u9996\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406\u4ea4\u9519\u6a21\u6001\u8f93\u5165\u548c\u7edf\u4e00\u5d4c\u5165\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533a\u57df\u7ea7\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u9065\u611f\u667a\u80fd\u5206\u6790\u7cfb\u7edf\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.11510", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11510", "abs": "https://arxiv.org/abs/2512.11510", "authors": ["Hanyue Lou", "Jiayi Zhou", "Yang Zhang", "Boyu Li", "Yi Wang", "Guangnan Ye", "Boxin Shi"], "title": "Reconstruction as a Bridge for Event-Based Visual Question Answering", "comment": null, "summary": "Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u5c06\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u9996\u4e2a\u5ba2\u89c2\u7684\u3001\u771f\u5b9e\u4e16\u754c\u7684\u4e8b\u4ef6MLLM\u57fa\u51c6EvQA\uff0c\u5728\u4e8b\u4ef6\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5c06\u4e8b\u4ef6\u76f8\u673a\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u6709\u671b\u5728\u6311\u6218\u6027\u89c6\u89c9\u6761\u4ef6\u4e0b\u5b9e\u73b0\u901a\u7528\u573a\u666f\u7406\u89e3\uff0c\u4f46\u9700\u8981\u5728\u4fdd\u7559\u4e8b\u4ef6\u6570\u636e\u72ec\u7279\u4f18\u52bf\u4e0e\u786e\u4fdd\u4e0e\u57fa\u4e8e\u5e27\u7684\u6a21\u578b\u517c\u5bb9\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5f53\u524d\u7f3a\u4e4f\u5ba2\u89c2\u7684\u3001\u771f\u5b9e\u4e16\u754c\u7684\u4e8b\u4ef6MLLM\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5e27\u7684\u91cd\u5efa\u4e0e\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5229\u7528\u4e8b\u4ef6\u7a00\u758f\u6027\u7684\u81ea\u9002\u5e94\u91cd\u5efa\u4e0e\u6807\u8bb0\u5316\u65b9\u6cd5\uff1b\u540c\u65f6\u8bbe\u8ba1\u4e86EvQA\u57fa\u51c6\uff0c\u5305\u542b\u6765\u81ea22\u4e2a\u516c\u5171\u6570\u636e\u96c6\u76841000\u4e2a\u4e8b\u4ef6-Q&A\u5bf9\uff0c\u7528\u4e8e\u5ba2\u89c2\u8bc4\u4f30\u4e8b\u4ef6MLLM\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728EvQA\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u4e8b\u4ef6\u7a00\u758f\u6027\u5229\u7528\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86MLLM\u5728\u4e8b\u4ef6\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u7684\u663e\u8457\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u91cd\u5efa\u4f5c\u4e3a\u6865\u6881\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e8b\u4ef6\u6570\u636e\u4e0e\u5e27\u57fa\u6a21\u578b\u517c\u5bb9\u6027\u7684\u6311\u6218\uff0c\u63d0\u51fa\u7684EvQA\u57fa\u51c6\u4e3a\u4e8b\u4ef6MLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5ba2\u89c2\u6807\u51c6\uff0c\u8bc1\u660e\u4e86MLLM\u5728\u4e8b\u4ef6\u89c6\u89c9\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.11542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11542", "abs": "https://arxiv.org/abs/2512.11542", "authors": ["Hossein Shahabadi", "Niki Sepasian", "Arash Marioriyad", "Ali Sharifi-Zarchi", "Mahdieh Soleymani Baghshah"], "title": "Infinity and Beyond: Compositional Alignment in VAR and Diffusion T2I Models", "comment": null, "summary": "Achieving compositional alignment between textual descriptions and generated images - covering objects, attributes, and spatial relationships - remains a core challenge for modern text-to-image (T2I) models. Although diffusion-based architectures have been widely studied, the compositional behavior of emerging Visual Autoregressive (VAR) models is still largely unexamined. We benchmark six diverse T2I systems - SDXL, PixArt-$\u03b1$, Flux-Dev, Flux-Schnell, Infinity-2B, and Infinity-8B - across the full T2I-CompBench++ and GenEval suites, evaluating alignment in color and attribute binding, spatial relations, numeracy, and complex multi-object prompts. Across both benchmarks, Infinity-8B achieves the strongest overall compositional alignment, while Infinity-2B also matches or exceeds larger diffusion models in several categories, highlighting favorable efficiency-performance trade-offs. In contrast, SDXL and PixArt-$\u03b1$ show persistent weaknesses in attribute-sensitive and spatial tasks. These results provide the first systematic comparison of VAR and diffusion approaches to compositional alignment and establish unified baselines for the future development of the T2I model.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e86\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u4e0e\u6269\u6563\u6a21\u578b\u5728\u7ec4\u5408\u5bf9\u9f50\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0Infinity-8B\u5728\u7ec4\u5408\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4e3aT2I\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u5efa\u7acb\u4e86\u7edf\u4e00\u57fa\u51c6\u3002", "motivation": "\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u5b9e\u73b0\u6587\u672c\u63cf\u8ff0\u4e0e\u751f\u6210\u56fe\u50cf\u4e4b\u95f4\u7684\u7ec4\u5408\u5bf9\u9f50\uff08\u6db5\u76d6\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\uff09\u65b9\u9762\u4ecd\u9762\u4e34\u6838\u5fc3\u6311\u6218\uff0c\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u65b0\u5174\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u7ec4\u5408\u884c\u4e3a\u5c1a\u672a\u5f97\u5230\u5145\u5206\u68c0\u9a8c\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u5bf9\u516d\u79cd\u4e0d\u540c\u7684T2I\u7cfb\u7edf\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ecSDXL\u3001PixArt-\u03b1\u3001Flux-Dev\u3001Flux-Schnell\u3001Infinity-2B\u548cInfinity-8B\uff0c\u8bc4\u4f30\u8986\u76d6\u4e86\u5b8c\u6574\u7684T2I-CompBench++\u548cGenEval\u6d4b\u8bd5\u5957\u4ef6\uff0c\u91cd\u70b9\u8003\u5bdf\u989c\u8272\u4e0e\u5c5e\u6027\u7ed1\u5b9a\u3001\u7a7a\u95f4\u5173\u7cfb\u3001\u6570\u503c\u7406\u89e3\u548c\u590d\u6742\u591a\u5bf9\u8c61\u63d0\u793a\u7b49\u7ec4\u5408\u5bf9\u9f50\u80fd\u529b\u3002", "result": "\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInfinity-8B\u5b9e\u73b0\u4e86\u6700\u5f3a\u7684\u6574\u4f53\u7ec4\u5408\u5bf9\u9f50\u6027\u80fd\uff0c\u800cInfinity-2B\u5728\u591a\u4e2a\u7c7b\u522b\u4e2d\u4e5f\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u6269\u6563\u6a21\u578b\uff0c\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u6548\u7387-\u6027\u80fd\u6743\u8861\uff1b\u76f8\u6bd4\u4e4b\u4e0b\uff0cSDXL\u548cPixArt-\u03b1\u5728\u5c5e\u6027\u654f\u611f\u548c\u7a7a\u95f4\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6301\u7eed\u7684\u5f31\u70b9\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86VAR\u548c\u6269\u6563\u65b9\u6cd5\u5728\u7ec4\u5408\u5bf9\u9f50\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4e3aT2I\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u5efa\u7acb\u4e86\u7edf\u4e00\u57fa\u51c6\uff0c\u7ed3\u679c\u8868\u660e\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u5728\u7ec4\u5408\u5bf9\u9f50\u4efb\u52a1\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u662fInfinity\u7cfb\u5217\u6a21\u578b\u5728\u6027\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2512.11612", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.11612", "abs": "https://arxiv.org/abs/2512.11612", "authors": ["Chunyi Li", "Rui Qing", "Jianbo Zhang", "Yuan Tian", "Xiangyang Zhu", "Zicheng Zhang", "Xiaohong Liu", "Weisi Lin", "Guangtao Zhai"], "title": "Embodied Image Compression", "comment": "15 pages, 12 figures, 3 tables", "summary": "Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63d0\u51fa\u5177\u8eab\u56fe\u50cf\u538b\u7f29\u8fd9\u4e00\u79d1\u5b66\u95ee\u9898\uff0c\u9488\u5bf9\u5177\u8eab\u667a\u80fd\u4f53\u5728\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u901a\u4fe1\u7ea6\u675f\uff0c\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u57fa\u51c6EmbodiedComp\uff0c\u5e76\u8bc1\u660e\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u4f4e\u4e8e\u5177\u8eab\u6bd4\u7279\u7387\u9608\u503c\u65f6\u65e0\u6cd5\u53ef\u9760\u6267\u884c\u7b80\u5355\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u968f\u7740\u673a\u5668\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u538b\u7f29\u76ee\u6807\u5df2\u4ece\u4efb\u52a1\u7279\u5b9a\u7684\u865a\u62df\u6a21\u578b\u8f6c\u5411\u5728\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u64cd\u4f5c\u7684\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u4f46\u73b0\u6709\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5177\u8eabAI\u7684\u901a\u4fe1\u7ea6\u675f\u548c\u5b9e\u65f6\u4efb\u52a1\u6267\u884c\u9700\u6c42\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u9996\u6b21\u63d0\u51fa\u5177\u8eab\u56fe\u50cf\u538b\u7f29\u8fd9\u4e00\u79d1\u5b66\u95ee\u9898\u3002", "method": "\u672c\u6587\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u57fa\u51c6EmbodiedComp\uff0c\u7528\u4e8e\u5728\u95ed\u73af\u8bbe\u7f6e\u4e0b\u5bf9\u8d85\u4f4e\u6bd4\u7279\u7387\u6761\u4ef6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u538b\u7f29\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7814\u7a76\u8868\u660e\uff0c\u5f53\u538b\u7f29\u6bd4\u7279\u7387\u4f4e\u4e8e\u5177\u8eab\u6bd4\u7279\u7387\u9608\u503c\u65f6\uff0c\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u65e0\u6cd5\u53ef\u9760\u6267\u884c\u7b80\u5355\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u8fd9\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u6ee1\u8db3\u5177\u8eab\u667a\u80fd\u4f53\u901a\u4fe1\u9700\u6c42\u65b9\u9762\u7684\u4e25\u91cd\u4e0d\u8db3\u3002", "conclusion": "EmbodiedComp\u57fa\u51c6\u5c06\u63a8\u52a8\u9762\u5411\u5177\u8eab\u667a\u80fd\u4f53\u7684\u9886\u57df\u7279\u5b9a\u538b\u7f29\u6280\u672f\u7684\u53d1\u5c55\uff0c\u52a0\u901f\u5177\u8eabAI\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u90e8\u7f72\u5e94\u7528\uff0c\u4e3a\u89e3\u51b3\u5177\u8eab\u667a\u80fd\u4f53\u5728\u53d7\u9650\u901a\u4fe1\u73af\u5883\u4e0b\u7684\u89c6\u89c9\u6570\u636e\u5904\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.11654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11654", "abs": "https://arxiv.org/abs/2512.11654", "authors": ["Luca Cazzola", "Ahed Alboody"], "title": "Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation", "comment": null, "summary": "The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).", "AI": {"tldr": "\u672c\u6587\u63d0\u51faKineMIC\u6846\u67b6\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u5c06\u901a\u7528\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6a21\u578b\u9002\u914d\u5230\u4eba\u4f53\u52a8\u4f5c\u8bc6\u522b\u9886\u57df\uff0c\u5229\u7528CLIP\u6587\u672c\u5d4c\u5165\u5efa\u7acb\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u52a8\u4f5c\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u6807\u6ce8\u8fd0\u52a8\u6570\u636e\u96c6\u7684\u83b7\u53d6\u6210\u672c\u662f\u9aa8\u9abc\u57fa\u4eba\u4f53\u52a8\u4f5c\u8bc6\u522b\u7684\u5173\u952e\u74f6\u9888\uff0c\u5c3d\u7ba1\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u6e90\uff0c\u4f46\u5176\u8bad\u7ec3\u76ee\u6807\u548c\u6570\u636e\u96c6\u7ed3\u6784\u4e0e\u52a8\u4f5c\u8bc6\u522b\u6240\u9700\u7684\u8fd0\u52a8\u7cbe\u5ea6\u548c\u7c7b\u522b\u533a\u5206\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5bfc\u81f4\u901a\u7528T2M\u6a21\u578b\u65e0\u6cd5\u751f\u6210\u9002\u5408\u52a8\u4f5c\u8bc6\u522b\u5206\u7c7b\u5668\u7684\u8fd0\u52a8\u6570\u636e\u3002", "method": "\u672c\u6587\u63d0\u51faKineMIC\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5047\u8bbe\u6587\u672c\u7f16\u7801\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\u53ef\u4ee5\u4e3a\u8fd0\u52a8\u5b66\u84b8\u998f\u63d0\u4f9b\u8f6f\u76d1\u7763\uff0c\u91c7\u7528\u52a8\u529b\u5b66\u6316\u6398\u7b56\u7565\u5229\u7528CLIP\u6587\u672c\u5d4c\u5165\u5efa\u7acb\u7a00\u758f\u52a8\u4f5c\u8bc6\u522b\u6807\u7b7e\u4e0eT2M\u6e90\u6570\u636e\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u6307\u5bfc\u5fae\u8c03\u8fc7\u7a0b\u5c06\u901a\u7528T2M\u4e3b\u5e72\u8f6c\u6362\u4e3a\u4e13\u95e8\u7684\u5c11\u6837\u672c\u52a8\u4f5c\u5230\u8fd0\u52a8\u751f\u6210\u5668\u3002", "result": "\u5728HumanML3D\u4f5c\u4e3a\u6e90T2M\u6570\u636e\u96c6\u548cNTU RGB+D 120\u5b50\u96c6\u4f5c\u4e3a\u76ee\u6807\u52a8\u4f5c\u8bc6\u522b\u9886\u57df\u7684\u9a8c\u8bc1\u4e2d\uff0c\u4ec5\u4f7f\u7528\u6bcf\u52a8\u4f5c\u7c7b\u522b10\u4e2a\u6837\u672c\uff0cKineMIC\u751f\u6210\u7684\u8fd0\u52a8\u6570\u636e\u663e\u8457\u66f4\u8fde\u8d2f\uff0c\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u6e90\u63d0\u4f9b\u4e86+23.1%\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\u8fdb\u884c\u8fd0\u52a8\u5b66\u84b8\u998f\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5c11\u6837\u672c\u52a8\u4f5c\u5408\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u901a\u7528\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6a21\u578b\u4e0e\u52a8\u4f5c\u8bc6\u522b\u9700\u6c42\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u52a8\u4f5c\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.11680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11680", "abs": "https://arxiv.org/abs/2512.11680", "authors": ["Xu Zhang", "Jiabin Fang", "Zhuoming Ding", "Jin Yuan", "Xuan Liu", "Qianjun Zhang", "Zhiyong Li"], "title": "Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing", "comment": "12 pages, 5 figures", "summary": "Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCLV-Net\uff0c\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u63d0\u793a\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u63d0\u4f9b\u7684\u7b80\u5355\u89c6\u89c9\u63d0\u793a\uff08\u8fb9\u754c\u6846\uff09\u6765\u751f\u6210\u76f8\u5173\u7684\u5206\u5272\u63a9\u7801\u548c\u63cf\u8ff0\uff0c\u6709\u6548\u6355\u6349\u7528\u6237\u610f\u56fe\u5e76\u5efa\u6a21\u5bf9\u8c61\u95f4\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u7406\u89e3\u65b9\u6cd5\u5728\u4ec5\u6709\u7b80\u5355\u6587\u672c\u63d0\u793a\u65f6\u96be\u4ee5\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u7528\u6237\u76f8\u5173\u533a\u57df\uff0c\u4e14\u5927\u89c4\u6a21\u822a\u7a7a\u5f71\u50cf\u4e2d\u8bb8\u591a\u5bf9\u8c61\u5177\u6709\u9ad8\u5ea6\u76f8\u4f3c\u7684\u89c6\u89c9\u5916\u89c2\u548c\u4e30\u5bcc\u7684\u5bf9\u8c61\u95f4\u5173\u7cfb\uff0c\u8fd9\u8fdb\u4e00\u6b65\u963b\u788d\u4e86\u51c6\u786e\u8bc6\u522b\u3002", "method": "CLV-Net\u7684\u6838\u5fc3\u8bbe\u8ba1\u5305\u62ec\u4e0a\u4e0b\u6587\u611f\u77e5\u63a9\u7801\u89e3\u7801\u5668\uff0c\u7528\u4e8e\u5efa\u6a21\u548c\u6574\u5408\u5bf9\u8c61\u95f4\u5173\u7cfb\u4ee5\u589e\u5f3a\u76ee\u6807\u8868\u793a\uff1b\u4ee5\u53ca\u8bed\u4e49\u4e0e\u5173\u7cfb\u5bf9\u9f50\u6a21\u5757\uff0c\u5305\u542b\u8de8\u6a21\u6001\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931\u4ee5\u589e\u5f3a\u89c6\u89c9\u76f8\u4f3c\u76ee\u6807\u7684\u7ec6\u7c92\u5ea6\u533a\u5206\uff0c\u4ee5\u53ca\u5173\u7cfb\u4e00\u81f4\u6027\u635f\u5931\u4ee5\u786e\u4fdd\u6587\u672c\u5173\u7cfb\u4e0e\u89c6\u89c9\u4ea4\u4e92\u7684\u5bf9\u9f50\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cCLV-Net\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u7528\u6237\u610f\u56fe\u5e76\u4ea7\u751f\u7cbe\u786e\u3001\u610f\u56fe\u5bf9\u9f50\u7684\u591a\u6a21\u6001\u8f93\u51fa\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u89c6\u89c9\u63d0\u793a\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u7406\u89e3\u5728\u9065\u611f\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5efa\u6a21\u5bf9\u8c61\u95f4\u5173\u7cfb\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u610f\u56fe\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u4ea4\u4e92\u5f0f\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2512.11683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11683", "abs": "https://arxiv.org/abs/2512.11683", "authors": ["Qiushi Guo"], "title": "Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection", "comment": null, "summary": "Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDepth Copy Paste\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u611f\u77e5\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5339\u914d\u3001\u7cbe\u786e\u5206\u5272\u548c\u6df1\u5ea6\u5f15\u5bfc\u653e\u7f6e\uff0c\u751f\u6210\u7269\u7406\u4e00\u81f4\u4e14\u89c6\u89c9\u903c\u771f\u7684\u4eba\u8138\u68c0\u6d4b\u8bad\u7ec3\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u590d\u5236\u7c98\u8d34\u589e\u5f3a\u65b9\u6cd5\u5728\u751f\u6210\u4eba\u8138\u68c0\u6d4b\u8bad\u7ec3\u6570\u636e\u65f6\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u5305\u62ec\u524d\u666f\u63d0\u53d6\u4e0d\u51c6\u786e\u3001\u573a\u666f\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u80cc\u666f\u8bed\u4e49\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u5408\u6210\u6837\u672c\u4e0d\u771f\u5b9e\u4e14\u7269\u7406\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u63d0\u5347\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u591a\u6a21\u6001\u6df1\u5ea6\u611f\u77e5\u589e\u5f3a\u6846\u67b6\uff0c\u9996\u5148\u5229\u7528BLIP\u548cCLIP\u8054\u5408\u8bc4\u4f30\u8bed\u4e49\u4e0e\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u81ea\u52a8\u68c0\u7d22\u6700\u5339\u914d\u7684\u80cc\u666f\u56fe\u50cf\uff1b\u96c6\u6210SAM3\u8fdb\u884c\u7cbe\u786e\u5206\u5272\u548cDepth-Anything\u63d0\u53d6\u975e\u906e\u6321\u53ef\u89c1\u533a\u57df\uff1b\u5f15\u5165\u6df1\u5ea6\u5f15\u5bfc\u6ed1\u52a8\u7a97\u53e3\u653e\u7f6e\u673a\u5236\uff0c\u5728\u80cc\u666f\u6df1\u5ea6\u56fe\u4e0a\u641c\u7d22\u5177\u6709\u6700\u4f73\u6df1\u5ea6\u8fde\u7eed\u6027\u548c\u5c3a\u5ea6\u5bf9\u9f50\u7684\u7c98\u8d34\u4f4d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDepth Copy Paste\u751f\u6210\u7684\u5408\u6210\u6837\u672c\u5728\u6df1\u5ea6\u5173\u7cfb\u548c\u89c6\u89c9\u903c\u771f\u5ea6\u65b9\u9762\u663e\u8457\u6539\u5584\uff0c\u4e3a\u4e0b\u6e38\u4eba\u8138\u68c0\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u591a\u6837\u5316\u548c\u771f\u5b9e\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u76f8\u6bd4\u4f20\u7edf\u590d\u5236\u7c98\u8d34\u548c\u65e0\u6df1\u5ea6\u589e\u5f3a\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u8bed\u4e49\u5339\u914d\u3001\u7cbe\u786e\u5206\u5272\u548c\u6df1\u5ea6\u611f\u77e5\u7684\u589e\u5f3a\u6846\u67b6\u80fd\u591f\u6709\u6548\u751f\u6210\u7269\u7406\u4e00\u81f4\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u4eba\u8138\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u6570\u636e\u589e\u5f3a\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u6570\u636e\u589e\u5f3a\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.11715", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.11715", "abs": "https://arxiv.org/abs/2512.11715", "authors": ["Wei Chow", "Linfeng Li", "Lingdong Kong", "Zefeng Li", "Qi Xu", "Hang Song", "Tian Ye", "Xian Wang", "Jinbin Bai", "Shilin Xu", "Xiangtai Li", "Junting Pan", "Shaoteng Liu", "Ran Zhou", "Tianshu Yang", "Songhua Liu"], "title": "EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing", "comment": null, "summary": "Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EditMGT\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u63a9\u7801\u751f\u6210\u53d8\u6362\u5668\uff08MGT\uff09\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528MGT\u7684\u5c40\u90e8\u89e3\u7801\u7279\u6027\u6765\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u5c40\u90e8\u7f16\u8f91\u65f6\u5bf9\u975e\u76ee\u6807\u533a\u57df\u7684\u610f\u5916\u4fee\u6539\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u533a\u57df\u63a7\u5236\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u867d\u7136\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u4f46\u5176\u5168\u5c40\u53bb\u566a\u52a8\u6001\u672c\u8d28\u4e0a\u5c06\u5c40\u90e8\u7f16\u8f91\u76ee\u6807\u4e0e\u5168\u56fe\u4e0a\u4e0b\u6587\u6df7\u4e3a\u4e00\u8c08\uff0c\u5bfc\u81f4\u975e\u76ee\u6807\u533a\u57df\u51fa\u73b0\u610f\u5916\u4fee\u6539\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u63a9\u7801\u751f\u6210\u53d8\u6362\u5668\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u5229\u7528\u5176\u5c40\u90e8\u89e3\u7801\u8303\u5f0f\u6765\u663e\u5f0f\u4fdd\u62a4\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u975e\u76f8\u5173\u533a\u57df\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86EditMGT\u6846\u67b6\uff0c\u9996\u5148\u5229\u7528MGT\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\u63d0\u4f9b\u4fe1\u606f\u6027\u5b9a\u4f4d\u4fe1\u53f7\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u5c42\u6ce8\u610f\u529b\u6574\u5408\u65b9\u6848\u6765\u7ec6\u5316\u8fd9\u4e9b\u56fe\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7cbe\u786e\u5b9a\u4f4d\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165\u4e86\u533a\u57df\u4fdd\u6301\u91c7\u6837\u6280\u672f\uff0c\u9650\u5236\u5728\u4f4e\u6ce8\u610f\u529b\u533a\u57df\u5185\u8fdb\u884c\u4ee4\u724c\u7ffb\u8f6c\u4ee5\u6291\u5236\u865a\u5047\u7f16\u8f91\uff0c\u4ece\u800c\u5c06\u4fee\u6539\u9650\u5236\u5728\u76ee\u6807\u533a\u57df\u5185\u5e76\u4fdd\u62a4\u5468\u56f4\u975e\u76ee\u6807\u533a\u57df\u7684\u5b8c\u6574\u6027\u3002\u901a\u8fc7\u6784\u5efaCrispEdit-2M\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\uff0c\u91c7\u7528\u6ce8\u610f\u529b\u6ce8\u5165\u65b9\u6cd5\u5c06\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cfMGT\u9002\u914d\u4e3a\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u3002", "result": "\u5728\u56db\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEditMGT\u5728\u53c2\u6570\u5c11\u4e8e10\u4ebf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u76f8\u4f3c\u7684\u76f8\u4f3c\u6027\u6027\u80fd\uff0c\u540c\u65f6\u7f16\u8f91\u901f\u5ea6\u63d0\u5347\u4e866\u500d\u3002\u5728\u98ce\u683c\u53d8\u5316\u548c\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e0a\u5206\u522b\u53d6\u5f97\u4e863.6%\u548c17.6%\u7684\u6539\u8fdb\uff0c\u63d0\u4f9b\u4e86\u53ef\u6bd4\u6216\u66f4\u4f18\u7684\u7f16\u8f91\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u63a9\u7801\u751f\u6210\u53d8\u6362\u5668\u5728\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5176\u5c40\u90e8\u89e3\u7801\u8303\u5f0f\u4e3a\u89e3\u51b3\u6269\u6563\u6a21\u578b\u7684\u5168\u5c40\u7f16\u8f91\u9650\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002EditMGT\u6846\u67b6\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528MGT\u7684\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7cbe\u786e\u7684\u533a\u57df\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7684\u7f16\u8f91\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7684\u5c40\u90e8\u611f\u77e5\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.11719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11719", "abs": "https://arxiv.org/abs/2512.11719", "authors": ["Yilmaz Korkmaz", "Jay N. Paranjape", "Celso M. de Melo", "Vishal M. Patel"], "title": "Referring Change Detection in Remote Sensing Imagery", "comment": "2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)", "summary": "Change detection in remote sensing imagery is essential for applications such as urban planning, environmental monitoring, and disaster management. Traditional change detection methods typically identify all changes between two temporal images without distinguishing the types of transitions, which can lead to results that may not align with specific user needs. Although semantic change detection methods have attempted to address this by categorizing changes into predefined classes, these methods rely on rigid class definitions and fixed model architectures, making it difficult to mix datasets with different label sets or reuse models across tasks, as the output channels are tightly coupled with the number and type of semantic classes. To overcome these limitations, we introduce Referring Change Detection (RCD), which leverages natural language prompts to detect specific classes of changes in remote sensing images. By integrating language understanding with visual analysis, our approach allows users to specify the exact type of change they are interested in. However, training models for RCD is challenging due to the limited availability of annotated data and severe class imbalance in existing datasets. To address this, we propose a two-stage framework consisting of (I) \\textbf{RCDNet}, a cross-modal fusion network designed for referring change detection, and (II) \\textbf{RCDGen}, a diffusion-based synthetic data generation pipeline that produces realistic post-change images and change maps for a specified category using only pre-change image, without relying on semantic segmentation masks and thereby significantly lowering the barrier to scalable data creation. Experiments across multiple datasets show that our framework enables scalable and targeted change detection. Project website is here: https://yilmazkorkmaz1.github.io/RCD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Referring Change Detection (RCD)\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5b9e\u73b0\u9065\u611f\u56fe\u50cf\u4e2d\u7279\u5b9a\u7c7b\u522b\u53d8\u5316\u7684\u68c0\u6d4b\uff0c\u5e76\u5f15\u5165RCDNet\u8de8\u6a21\u6001\u878d\u5408\u7f51\u7edc\u548cRCDGen\u6269\u6563\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\u6765\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u533a\u5206\u53d8\u5316\u7c7b\u578b\uff0c\u800c\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u521a\u6027\u7c7b\u522b\u5b9a\u4e49\u548c\u56fa\u5b9a\u6a21\u578b\u67b6\u6784\uff0c\u96be\u4ee5\u6df7\u5408\u4e0d\u540c\u6807\u7b7e\u96c6\u7684\u6570\u636e\u96c6\u6216\u8de8\u4efb\u52a1\u91cd\u7528\u6a21\u578b\uff0c\u56e0\u4e3a\u8f93\u51fa\u901a\u9053\u4e0e\u8bed\u4e49\u7c7b\u522b\u6570\u91cf\u548c\u7c7b\u578b\u7d27\u5bc6\u8026\u5408\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u5bf9\u7279\u5b9a\u53d8\u5316\u7c7b\u578b\u7684\u68c0\u6d4b\u9700\u6c42\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u662fRCDNet\uff0c\u4e00\u4e2a\u7528\u4e8e\u6307\u4ee3\u53d8\u5316\u68c0\u6d4b\u7684\u8de8\u6a21\u6001\u878d\u5408\u7f51\u7edc\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e0e\u89c6\u89c9\u5206\u6790\u76f8\u7ed3\u5408\uff1b\u7b2c\u4e8c\u9636\u6bb5\u662fRCDGen\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u4ec5\u4f7f\u7528\u53d8\u5316\u524d\u56fe\u50cf\u5373\u53ef\u751f\u6210\u6307\u5b9a\u7c7b\u522b\u7684\u771f\u5b9e\u53d8\u5316\u540e\u56fe\u50cf\u548c\u53d8\u5316\u56fe\uff0c\u65e0\u9700\u4f9d\u8d56\u8bed\u4e49\u5206\u5272\u63a9\u7801\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u548c\u6709\u9488\u5bf9\u6027\u7684\u53d8\u5316\u68c0\u6d4b\u3002RCDGen\u663e\u8457\u964d\u4f4e\u4e86\u5927\u89c4\u6a21\u6570\u636e\u521b\u5efa\u7684\u969c\u788d\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u548c\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u4e3a\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002RCD\u6846\u67b6\u5141\u8bb8\u7528\u6237\u6307\u5b9a\u611f\u5174\u8da3\u7684\u5177\u4f53\u53d8\u5316\u7c7b\u578b\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7c7b\u522b\u5b9a\u4e49\u548c\u6a21\u578b\u91cd\u7528\u65b9\u9762\u7684\u9650\u5236\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2512.11720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11720", "abs": "https://arxiv.org/abs/2512.11720", "authors": ["Yan Zhang", "Han Zou", "Lincong Feng", "Cong Xie", "Ruiqi Yu", "Zhenpeng Zhan"], "title": "Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation", "comment": null, "summary": "Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u97f3\u4e50\u5230\u821e\u8e48\u751f\u6210\u65b9\u6cd5\uff0c\u5c062D\u59ff\u6001\u5e8f\u5217\u7f16\u7801\u4e3a\u5355\u70ed\u56fe\u50cf\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u56fe\u50cfVAE\u8fdb\u884c\u538b\u7f29\uff0c\u5e76\u91c7\u7528DiT\u98ce\u683c\u4e3b\u5e72\u7f51\u7edc\u8fdb\u884c\u5efa\u6a21\uff0c\u4ece\u800c\u66f4\u597d\u5730\u6355\u6349\u9ad8\u65b9\u5dee\u59ff\u6001\u5206\u5e03\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u65f6\u95f4\u5171\u4eab\u7d22\u5f15\u65b9\u6848\u548c\u53c2\u8003\u59ff\u6001\u6761\u4ef6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u821e\u8e48\u59ff\u6001\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8282\u594f\u5bf9\u9f50\u6027\u3002", "motivation": "\u73b0\u6709\u59ff\u6001\u5230\u89c6\u9891\u6a21\u578b\u80fd\u591f\u5c062D\u59ff\u6001\u5e8f\u5217\u8f6c\u6362\u4e3a\u903c\u771f\u7684\u8eab\u4efd\u4fdd\u6301\u821e\u8e48\u89c6\u9891\uff0c\u4f46\u5173\u952e\u6311\u6218\u5728\u4e8e\u4ece\u97f3\u4e50\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u3001\u8282\u594f\u5bf9\u9f50\u76842D\u59ff\u6001\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u3001\u9ad8\u65b9\u5dee\u7684\u91ce\u5916\u5206\u5e03\u573a\u666f\u4e0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u97f3\u4e50\u5230\u821e\u8e48\u751f\u6210\u4e2d\u59ff\u6001\u5e8f\u5217\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u97f3\u4e50\u8282\u594f\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u97f3\u4e50\u5230\u821e\u8e48\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u97f3\u4e50\u6807\u8bb0\u6761\u4ef6\u7684\u591a\u901a\u9053\u56fe\u50cf\u5408\u6210\u95ee\u9898\uff1a\u5c062D\u59ff\u6001\u5e8f\u5217\u7f16\u7801\u4e3a\u5355\u70ed\u56fe\u50cf\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u56fe\u50cfVAE\u8fdb\u884c\u538b\u7f29\uff0c\u5e76\u91c7\u7528DiT\u98ce\u683c\u4e3b\u5e72\u7f51\u7edc\u8fdb\u884c\u5efa\u6a21\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e86\u65f6\u95f4\u5171\u4eab\u7684\u65f6\u95f4\u7d22\u5f15\u65b9\u6848\uff0c\u663e\u5f0f\u540c\u6b65\u97f3\u4e50\u6807\u8bb0\u548c\u59ff\u6001\u6f5c\u5728\u8868\u793a\uff1b\u4ee5\u53ca\u53c2\u8003\u59ff\u6001\u6761\u4ef6\u7b56\u7565\uff0c\u4fdd\u6301\u4e3b\u4f53\u7279\u5b9a\u8eab\u4f53\u6bd4\u4f8b\u548c\u5c4f\u5e55\u5c3a\u5ea6\uff0c\u540c\u65f6\u652f\u6301\u957f\u65f6\u57df\u5206\u6bb5\u62fc\u63a5\u751f\u6210\u3002", "result": "\u5728\u5927\u578b\u91ce\u59162D\u821e\u8e48\u8bed\u6599\u5e93\u548c\u6821\u51c6\u7684AIST++2D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u59ff\u6001\u7a7a\u95f4\u548c\u89c6\u9891\u7a7a\u95f4\u6307\u6807\u4ee5\u53ca\u4eba\u7c7b\u504f\u597d\u65b9\u9762\u5747\u4f18\u4e8e\u4ee3\u8868\u6027\u97f3\u4e50\u5230\u821e\u8e48\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8868\u793a\u65b9\u6cd5\u3001\u65f6\u95f4\u7d22\u5f15\u548c\u53c2\u8003\u6761\u4ef6\u7b56\u7565\u7684\u6709\u6548\u8d21\u732e\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u5206\u5e03\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u821e\u8e48\u59ff\u6001\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u97f3\u4e50\u5230\u821e\u8e48\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u56fe\u50cf\u5408\u6210\u95ee\u9898\uff0c\u6210\u529f\u7ee7\u627f\u4e86\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u59ff\u6001\u751f\u6210\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002\u63d0\u51fa\u7684\u65f6\u95f4\u7d22\u5f15\u548c\u53c2\u8003\u6761\u4ef6\u7b56\u7565\u4e3a\u89e3\u51b3\u957f\u65f6\u57df\u751f\u6210\u548c\u4e3b\u4f53\u4fdd\u6301\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u97f3\u4e50\u9a71\u52a8\u7684\u821e\u8e48\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2512.11749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11749", "abs": "https://arxiv.org/abs/2512.11749", "authors": ["Minglei Shi", "Haolin Wang", "Borui Zhang", "Wenzhao Zheng", "Bohan Zeng", "Ziyang Yuan", "Xiaoshi Wu", "Yuanxing Zhang", "Huan Yang", "Xintao Wang", "Pengfei Wan", "Kun Gai", "Jie Zhou", "Jiwen Lu"], "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "comment": "Code Repository: https://github.com/KlingTeam/SVG-T2I; Model Weights: https://huggingface.co/KlingTeam/SVG-T2I", "summary": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86SVG-T2I\u6846\u67b6\uff0c\u5c06\u81ea\u76d1\u7763\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8868\u793a\u6269\u5c55\u5230\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u5728VFM\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u5408\u6210\uff0c\u5e76\u5f00\u6e90\u4e86\u5b8c\u6574\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6d41\u7a0b\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8868\u793a\u7684\u53ef\u89c6\u5316\u751f\u6210\u4e3a\u6574\u5408\u89c6\u89c9\u7406\u89e3\u3001\u611f\u77e5\u548c\u751f\u6210\u63d0\u4f9b\u4e86\u7edf\u4e00\u8def\u5f84\uff0c\u4f46\u5728VFM\u8868\u793a\u7a7a\u95f4\u4e2d\u5b8c\u5168\u8bad\u7ec3\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u7814\u7a76\u4ecd\u5904\u4e8e\u63a2\u7d22\u4e0d\u8db3\u7684\u72b6\u6001\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u6269\u5c55\u4e86SVG\u6846\u67b6\uff0c\u63d0\u51faSVG-T2I\u65b9\u6cd5\uff0c\u91c7\u7528\u6807\u51c6\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u7ba1\u9053\u76f4\u63a5\u5728VFM\u7279\u5f81\u57df\u4e2d\u8fdb\u884c\u56fe\u50cf\u5408\u6210\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u5b9e\u73b0\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u3002", "result": "SVG-T2I\u5728GenEval\u57fa\u51c6\u4e0a\u8fbe\u52300.75\u5206\uff0c\u5728DPG-Bench\u4e0a\u83b7\u5f9785.78\u5206\uff0c\u8868\u73b0\u51fa\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86VFM\u8868\u793a\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5185\u5728\u8868\u5f81\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u5b9e\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8868\u793a\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5f00\u6e90\u5b8c\u6574\u7684\u9879\u76ee\u5305\u62ec\u81ea\u7f16\u7801\u5668\u3001\u751f\u6210\u6a21\u578b\u3001\u8bad\u7ec3\u63a8\u7406\u6d41\u7a0b\u548c\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u4e3a\u8868\u793a\u9a71\u52a8\u7684\u89c6\u89c9\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2512.11782", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11782", "abs": "https://arxiv.org/abs/2512.11782", "authors": ["Peiqing Yang", "Shangchen Zhou", "Kai Hao", "Qingyi Tao"], "title": "MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator", "comment": "Project page: https://pq-yang.github.io/projects/MatAnyone2/", "summary": "Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u6807\u6ce8\u7684\u62a0\u56fe\u8d28\u91cf\u8bc4\u4f30\u5668\uff08MQE\uff09\uff0c\u901a\u8fc7\u8be5\u8bc4\u4f30\u5668\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u62a0\u56fe\u6570\u636e\u96c6VMReal\uff0c\u5e76\u5f00\u53d1\u4e86MatAnyone 2\u6a21\u578b\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u62a0\u56fe\u9886\u57df\u53d7\u5230\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u548c\u771f\u5b9e\u6027\u7684\u9650\u5236\uff0c\u867d\u7136\u5229\u7528\u5206\u5272\u6570\u636e\u53ef\u4ee5\u589e\u5f3a\u8bed\u4e49\u7a33\u5b9a\u6027\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u8fb9\u754c\u76d1\u7763\u901a\u5e38\u5bfc\u81f4\u62a0\u56fe\u7ed3\u679c\u5448\u73b0\u5206\u5272\u72b6\u800c\u7f3a\u4e4f\u7cbe\u7ec6\u7ec6\u8282\uff0c\u8fd9\u963b\u788d\u4e86\u9ad8\u8d28\u91cf\u89c6\u9891\u62a0\u56fe\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u65e0\u9700\u771f\u5b9e\u6807\u6ce8\u7684\u62a0\u56fe\u8d28\u91cf\u8bc4\u4f30\u5668\uff08MQE\uff09\uff0c\u8be5\u8bc4\u4f30\u5668\u80fd\u591f\u8bc4\u4f30alpha\u62a0\u56fe\u7684\u8bed\u4e49\u548c\u8fb9\u754c\u8d28\u91cf\uff0c\u5e76\u751f\u6210\u50cf\u7d20\u7ea7\u8bc4\u4f30\u56fe\u4ee5\u8bc6\u522b\u53ef\u9760\u548c\u9519\u8bef\u533a\u57df\u3002MQE\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u6269\u5c55\u89c6\u9891\u62a0\u56fe\uff1a\u4f5c\u4e3a\u8bad\u7ec3\u671f\u95f4\u7684\u5728\u7ebf\u8d28\u91cf\u53cd\u9988\u673a\u5236\u6765\u6291\u5236\u9519\u8bef\u533a\u57df\uff0c\u4ee5\u53ca\u4f5c\u4e3a\u79bb\u7ebf\u6570\u636e\u7b5b\u9009\u6a21\u5757\u7528\u4e8e\u6570\u636e\u6574\u7406\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u5f15\u5165\u4e86\u53c2\u8003\u5e27\u8bad\u7ec3\u7b56\u7565\u6765\u5904\u7406\u957f\u89c6\u9891\u4e2d\u7684\u5916\u89c2\u53d8\u5316\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b28K\u4e2a\u89c6\u9891\u7247\u6bb5\u548c240\u4e07\u5e27\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u62a0\u56fe\u6570\u636e\u96c6VMReal\u3002", "result": "\u57fa\u4e8eMQE\u548cVMReal\u6570\u636e\u96c6\u5f00\u53d1\u7684MatAnyone 2\u6a21\u578b\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u65b9\u6cd5\u3002\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u5904\u7406\u957f\u89c6\u9891\u4e2d\u7684\u5916\u89c2\u53d8\u5316\uff0c\u5e76\u751f\u6210\u5177\u6709\u7cbe\u7ec6\u7ec6\u8282\u7684\u9ad8\u8d28\u91cf\u62a0\u56fe\u7ed3\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u65e0\u9700\u771f\u5b9e\u6807\u6ce8\u7684\u62a0\u56fe\u8d28\u91cf\u8bc4\u4f30\u5668\u548c\u6784\u5efa\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u62a0\u56fe\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4e3a\u89c6\u9891\u62a0\u56fe\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8d28\u91cf\u76d1\u7763\u673a\u5236\uff0c\u8fd8\u4e3a\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
