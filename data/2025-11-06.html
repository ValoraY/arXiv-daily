<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-11-06.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 14]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 6]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 3]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-prom3e-probabilistic-masked-multimodal-embedding-model-for-ecology">[1] <a href="https://arxiv.org/abs/2511.02946">ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology</a></h3>
<p><em>Srikumar Sastry, Subash Khanal, Aayush Dhakal, Jiayu Lin, Dan Cher, Phoenix Jarosz, Nathan Jacobs</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>ProM3Eæ˜¯ä¸€ç§åŸºäºæ¦‚ç‡æ©ç å¤šæ¨¡æ€åµŒå…¥çš„ç”Ÿæ€å­¦è¡¨ç¤ºå­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡åµŒå…¥ç©ºé—´çš„æ¨¡æ€é‡æ„æ”¯æŒä»»æ„æ¨¡æ€é—´çš„ç”Ÿæˆä¸æ£€ç´¢ï¼Œå¹¶æå‡ºæ–°é¢–çš„è·¨æ¨¡æ€æ£€ç´¢æ–¹æ³•å®ç°ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ç”Ÿæ€å­¦ä¸­å¤šæ¨¡æ€æ•°æ®èåˆçš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•ä»éƒ¨åˆ†ä¸Šä¸‹æ–‡æ¨¡æ€æ¨æ–­ç¼ºå¤±æ¨¡æ€ï¼Œå¹¶åˆ†æä¸åŒæ¨¡æ€èåˆåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¯è¡Œæ€§ï¼Œæœ¬è´¨ä¸Šå­¦ä¹ ä½•æ—¶èåˆä½•ç§æ¨¡æ€ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¨¡å‹é‡‡ç”¨åŸºäºåµŒå…¥ç©ºé—´çš„æ©ç æ¨¡æ€é‡æ„æ–¹æ³•ï¼Œå­¦ä¹ ç»™å®šå°‘é‡ä¸Šä¸‹æ–‡æ¨¡æ€æ—¶æ¨æ–­ç¼ºå¤±æ¨¡æ€çš„èƒ½åŠ›ï¼Œå…¶æ¦‚ç‡æ€§è´¨æ”¯æŒæ¨¡æ€åµŒå…¥ç©ºé—´çš„åè½¬ï¼Œå¹¶æå‡ºæ··åˆæ¨¡æ€é—´å’Œæ¨¡æ€å†…ç›¸ä¼¼åº¦çš„æ–°å‹è·¨æ¨¡æ€æ£€ç´¢æ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è¯¥æ¨¡å‹åœ¨æ‰€æœ‰æ£€ç´¢ä»»åŠ¡ä¸­å‡å®ç°ä¼˜è¶Šæ€§èƒ½ï¼Œå…¶éšè—è¡¨ç¤ºåœ¨çº¿æ€§æ¢æµ‹ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„è¡¨å¾å­¦ä¹ èƒ½åŠ›ï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€èåˆå’Œè¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºç”Ÿæ€å­¦å¤šæ¨¡æ€æ•°æ®åˆ†ææä¾›äº†æœ‰æ•ˆçš„è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œå…¶æ¦‚ç‡å»ºæ¨¡å’Œæ¨¡æ€åè½¬èƒ½åŠ›ä¸ºç†è§£æ¨¡æ€èåˆå¯è¡Œæ€§æä¾›äº†æ–°è§†è§’ï¼Œæå‡ºçš„è·¨æ¨¡æ€æ£€ç´¢æ–¹æ³•å…·æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>We introduce ProM3E, a probabilistic masked multimodal embedding model for
any-to-any generation of multimodal representations for ecology. ProM3E is
based on masked modality reconstruction in the embedding space, learning to
infer missing modalities given a few context modalities. By design, our model
supports modality inversion in the embedding space. The probabilistic nature of
our model allows us to analyse the feasibility of fusing various modalities for
given downstream tasks, essentially learning what to fuse. Using these features
of our model, we propose a novel cross-modal retrieval approach that mixes
inter-modal and intra-modal similarities to achieve superior performance across
all retrieval tasks. We further leverage the hidden representation from our
model to perform linear probing tasks and demonstrate the superior
representation learning capability of our model. All our code, datasets and
model will be released at https://vishu26.github.io/prom3e.</p>
<h3 id="2-scale-vlp-soft-weighted-contrastive-volumetric-vision-language-pre-training-with-spatial-knowledge-semantics">[2] <a href="https://arxiv.org/abs/2511.02996">SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics</a></h3>
<p><em>Ailar Mahdizadeh, Puria Azadi Moghadam, Xiangteng He, Shahriar Mirabbasi, Panos Nasiopoulos, Leonid Sigal</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>SCALE-VLPæå‡ºäº†ä¸€ç§è½¯åŠ æƒå¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆä½“ç§¯ç©ºé—´è¯­ä¹‰å’Œé¢†åŸŸæ„ŸçŸ¥çŸ¥è¯†æ³¨å…¥è¯­ä¹‰ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä½“ç§¯æ•°æ®æ—¶å¿½ç•¥è¿ç»­ç»“æ„åŒ–ä¾èµ–å…³ç³»çš„é—®é¢˜ï¼Œåœ¨CTå›¾åƒ-æŠ¥å‘Šæ£€ç´¢ã€å¼‚å¸¸åˆ†ç±»å’ŒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ä¸»è¦å±€é™äº2Dæ•°æ®å¹¶é‡‡ç”¨äºŒå…ƒç›‘ç£ï¼Œå¿½ç•¥äº†CTç­‰ä½“ç§¯æ•°æ®ä¸­å­˜åœ¨çš„è¿ç»­ç»“æ„åŒ–ä¾èµ–å…³ç³»ï¼ŒåŒæ—¶ç°æœ‰æ–¹æ³•å°†ä½“ç§¯æ‰«æè§†ä¸ºç‹¬ç«‹2Dåˆ‡ç‰‡å¤„ç†ï¼ŒæŸå®³äº†ç©ºé—´ä¸€è‡´æ€§å¹¶æœªèƒ½å……åˆ†åˆ©ç”¨ä¸°å¯Œçš„ä¸´åºŠè¯­ä¹‰ä¿¡æ¯ã€‚</p>
<p><strong>Method:</strong> SCALE-VLPæ¡†æ¶æ•´åˆäº†ä½“ç§¯ç©ºé—´è¯­ä¹‰ä»¥ä¿æŒè§£å‰–ç»“æ„ï¼Œä»¥åŠé¢†åŸŸæ„ŸçŸ¥çš„çŸ¥è¯†æ³¨å…¥è¯­ä¹‰ï¼ˆå¦‚æ”¾å°„å­¦æœ¬ä½“ï¼‰ï¼Œé€šè¿‡è½¯åŠ æƒå¯¹æ¯”å­¦ä¹ åœ¨æœ‰é™ç›‘ç£ä¸‹ç”Ÿæˆç»“æ„ä¸€è‡´ä¸”è¯­ä¹‰åŸºç¡€çš„è¡¨å¾ã€‚</p>
<p><strong>Result:</strong> ç›¸æ¯”å…ˆå‰æœ€ä¼˜æ–¹æ³•ï¼ŒSCALE-VLPåœ¨CT-æŠ¥å‘Šæ£€ç´¢ä¸­å®ç°äº†æœ€é«˜4.3å€çš„top-1æ€§èƒ½æå‡ï¼Œå¼‚å¸¸åˆ†ç±»å‡†ç¡®ç‡æé«˜10ä¸ªç™¾åˆ†ç‚¹ï¼ŒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡è¾¾åˆ°ROUGE-L 0.44å’ŒBERT-F1 0.89ï¼Œåœ¨é›¶æ ·æœ¬è·¨åŸŸè¯„ä¼°ä¸­ä¹Ÿè§‚å¯Ÿåˆ°ä¸€è‡´çš„æ€§èƒ½å¢ç›Šã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜SCALE-VLPå…·æœ‰å¼ºå¤§çš„è·¨ä»»åŠ¡å¯è¿ç§»æ€§å’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒå³å¯å®ç°ä¸€è‡´æ€§èƒ½æå‡ï¼Œä¸ºå¤„ç†ä½“ç§¯åŒ»å­¦æ•°æ®æä¾›äº†ç»“æ„ä¸€è‡´ä¸”è¯­ä¹‰åŸºç¡€çš„è¡¨å¾å­¦ä¹ æ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Vision-language models (VLMs) have demonstrated strong cross-modal
capabilities, yet most work remains limited to 2D data and assumes binary
supervision (i.e., positive vs. negative pairs), overlooking the continuous and
structured dependencies present in volumetric data such as CT. Existing
approaches often treat volumetric scans as independent 2D slices, compromising
spatial coherence and underutilizing rich clinical semantics. We propose
SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework
that integrates (i) volumetric spatial semantics to preserve anatomical
structure and (ii) domain-aware, knowledge-infused semantics (e.g.,
radiological ontologies) to guide alignment. This yields structurally
consistent and semantically grounded representations under limited supervision,
demonstrating strong cross-task transferability (retrieval, report generation,
and classification), and cross-domain generalizability with consistent gains
without further fine-tuning. In particular, compared to the previous state of
the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval,
improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and
BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an
out-of-domain external dataset, we observe consistent gains, indicating the
cross-task and cross-domain generalization ability of SCALE-VLP.</p>
<h3 id="3-slip-structural-aware-language-image-pretraining-for-vision-language-alignment">[3] <a href="https://arxiv.org/abs/2511.03019">SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment</a></h3>
<p><em>Wenbo Lu</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºç»“æ„æ„ŸçŸ¥è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆSLIPï¼‰ï¼Œé€šè¿‡å¼•å…¥ç»“æ„åŒ–å¯¹æ¯”æŸå¤±æ¥å»ºæ¨¡å›¾åƒ-æ–‡æœ¬å¯¹ä¹‹é—´çš„å…³è”å…³ç³»ï¼Œåœ¨è·¨æ¨¡æ€æ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºCLIPæ¨¡å‹ï¼Œè¯æ˜äº†å…³ç³»ç›‘ç£å¯¹è·¨æ¨¡æ€å¯¹é½çš„é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ–¹æ³•å°†å›¾åƒ-æ–‡æœ¬å¯¹è§†ä¸ºå­¤ç«‹è®­ç»ƒæ ·æœ¬ï¼Œå¿½ç•¥äº†ç°å®é¢†åŸŸä¸­ä¸°å¯Œçš„å…³è”ç»“æ„ï¼Œå¦‚ç”µå•†äº§å“å…±è´­å›¾å’Œç¤¾ä¼šæ¨èç½‘ç»œï¼Œè€Œç¥ç»ç§‘å­¦è¯æ®è¡¨æ˜äººç±»å°†çŸ¥è¯†ç¼–ç ä¸ºå…³ç³»è®¤çŸ¥å›¾è°±ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿå»ºæ¨¡ç»“æ„åŒ–å…³ç³»çš„è·¨æ¨¡æ€å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> SLIPæ–¹æ³•æ•´åˆäº†ç»“æ„åŒ–å¯¹æ¯”æŸå¤±ï¼Œåœ¨ä¿æŒæ¨¡æ€å¯¹é½çš„åŒæ—¶å»ºæ¨¡ç»“æ„åŒ–å›¾ä¸­ç›¸é‚»å®ä½“ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶æ„å»ºäº†å¤§è§„æ¨¡äºšé©¬é€Šäº§å“å…±è´­å¤šæ¨¡æ€å›¾æ•°æ®é›†æ¥æ”¯æŒç»“æ„åŒ–è·¨æ¨¡æ€ç›‘ç£å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒSLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹ï¼Œåœ¨è·¨æ¨¡æ€æ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡ä¸ŠæŒç»­ä¼˜äºCLIPæ¨¡å‹ï¼ŒéªŒè¯äº†å…³ç³»ç›‘ç£å¯¹è·¨æ¨¡æ€å¯¹é½çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“æ„åŒ–å…³ç³»ç›‘ç£åœ¨è·¨æ¨¡æ€å­¦ä¹ ä¸­çš„é‡è¦æ€§ï¼Œä¸ºåˆ©ç”¨é¢†åŸŸç‰¹å®šå…³ç³»ç»“æ„æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½æä¾›äº†æ–°èŒƒå¼ï¼Œå¹¶å±•ç¤ºäº†å…³ç³»è®¤çŸ¥å›¾è°±åœ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Vision-Language Pretraining (VLP) has achieved remarkable success across
various downstream tasks, but such gains are largely driven by scaling up on
training data. Yet, literature methods treat image-text pairs as isolated
training examples; this neglects the rich relational structure naturally
present in many domains, such as e-commerce product co-purchase graphs and
social recommendation networks. Inspired by neuroscientific evidence that human
encodes knowledge as relationship cognitive maps, we introduce Structure-aware
Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive
loss to align modalities while also modeling relationships between neighboring
entities in a structured graph. To support this paradigm, we construct a
large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling
structured cross-modality supervision at scale. Experiment results show that
SLIP consistently outperforms CLIP on cross-modal retrieval and classification
tasks in both zero-shot and few-shot settings, showing the value of relational
supervision for cross-modal alignment.</p>
<h3 id="4-finetuning-free-personalization-of-text-to-image-generation-via-hypernetworks">[4] <a href="https://arxiv.org/abs/2511.03156">Finetuning-Free Personalization of Text to Image Generation via Hypernetworks</a></h3>
<p><em>Sagar Shrestha, Gopal Sharma, Luowei Zhou, Suren Kumar</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¶…ç½‘ç»œçš„å…å¾®è°ƒä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒç›®æ ‡é¢„æµ‹LoRAé€‚é…æƒé‡ï¼Œæ¶ˆé™¤äº†ä¼ ç»Ÿæ–¹æ³•ä¸­æ¯ä¸ªä¸»ä½“éƒ½éœ€è¦ä¼˜åŒ–çš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†ä¸»ä½“ä¿çœŸåº¦å’Œæç¤ºå¯¹é½ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸ªæ€§åŒ–æ–¹æ³•å¦‚DreamBoothä¾èµ–äºä¸»ä½“ç‰¹å®šçš„å¾®è°ƒï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”æ¨ç†é€Ÿåº¦æ…¢ã€‚ç°æœ‰çš„é€‚é…å™¨å’Œç¼–ç å™¨æ–¹æ³•è™½ç„¶å°è¯•å‡å°‘å¼€é”€ï¼Œä½†ä»éœ€è¦é¢å¤–å¾®è°ƒæˆ–å¤§å‹éª¨å¹²æ¨¡å‹ã€‚è¶…ç½‘ç»œæ–¹æ³•è™½ç„¶é¿å…äº†é€ä¸»ä½“ä¼˜åŒ–ï¼Œä½†é¢ä¸´æ•°æ®ç”Ÿæˆæˆæœ¬é«˜å’Œä¼˜åŒ–è½¨è¿¹ä¸ç¨³å®šçš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯è®­ç»ƒç›®æ ‡ï¼Œé€šè¿‡ç®€å•çš„è¾“å‡ºæ­£åˆ™åŒ–å®ç°ç¨³å®šåŒ–ï¼Œæ„å»ºå¯é çš„è¶…ç½‘ç»œç›´æ¥ä»ä¸»ä½“å›¾åƒé¢„æµ‹LoRAé€‚é…æƒé‡ã€‚åœ¨æ¨ç†é˜¶æ®µå¼•å…¥äº†æ··åˆæ¨¡å‹åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆHM-CFGï¼‰ï¼Œç»“åˆåŸºç¡€æ‰©æ•£æ¨¡å‹çš„ç»„åˆèƒ½åŠ›å’Œä¸ªæ€§åŒ–æ¨¡å‹çš„ä¸»ä½“ä¿çœŸåº¦è¿›è¡Œé‡‡æ ·ã€‚</p>
<p><strong>Result:</strong> åœ¨CelebA-HQã€AFHQ-v2å’ŒDreamBenchä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†å¼ºå¤§çš„ä¸ªæ€§åŒ–æ€§èƒ½ï¼Œè¯æ˜äº†è¶…ç½‘ç»œä½œä¸ºå¯æ‰©å±•å’Œæœ‰æ•ˆçš„å¼€æ”¾ç±»åˆ«ä¸ªæ€§åŒ–æ–¹å‘çš„æ½œåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•æ¶ˆé™¤äº†æµ‹è¯•æ—¶é€ä¸»ä½“ä¼˜åŒ–çš„éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒäº†ä¸»ä½“ä¿çœŸåº¦å’Œæç¤ºå¯¹é½ã€‚ç ”ç©¶è¡¨æ˜è¶…ç½‘ç»œæ˜¯æ„å»ºå¯æ‰©å±•ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç³»ç»Ÿçš„æœ‰å‰æ™¯æ–¹å‘ï¼Œä¸ºå¼€æ”¾ç±»åˆ«ä¸ªæ€§åŒ–æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Personalizing text-to-image diffusion models has traditionally relied on
subject-specific fine-tuning approaches such as
DreamBooth~\cite{ruiz2023dreambooth}, which are computationally expensive and
slow at inference. Recent adapter- and encoder-based methods attempt to reduce
this overhead but still depend on additional fine-tuning or large backbone
models for satisfactory results. In this work, we revisit an orthogonal
direction: fine-tuning-free personalization via Hypernetworks that predict
LoRA-adapted weights directly from subject images. Prior hypernetwork-based
approaches, however, suffer from costly data generation or unstable attempts to
mimic base model optimization trajectories. We address these limitations with
an end-to-end training objective, stabilized by a simple output regularization,
yielding reliable and effective hypernetworks. Our method removes the need for
per-subject optimization at test time while preserving both subject fidelity
and prompt alignment. To further enhance compositional generalization at
inference time, we introduce Hybrid-Model Classifier-Free Guidance (HM-CFG),
which combines the compositional strengths of the base diffusion model with the
subject fidelity of personalized models during sampling. Extensive experiments
on CelebA-HQ, AFHQ-v2, and DreamBench demonstrate that our approach achieves
strong personalization performance and highlights the promise of hypernetworks
as a scalable and effective direction for open-category personalization.</p>
<h3 id="5-surgant-vivqa-learning-to-anticipate-surgical-events-through-gru-driven-temporal-cross-attention">[5] <a href="https://arxiv.org/abs/2511.03178">SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention</a></h3>
<p><em>Shreyas C. Dhake, Jiayuan Huang, Runlong He, Danyal Z. Khan, Evangelos B. Mazomenos, Sophia Bano, Hani J. Marcus, Danail Stoyanov, Matthew J. Clarkson, Mobarak I. Hoque</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†PitVQA-Anticipationæ•°æ®é›†å’ŒSurgAnt-ViVQAæ¨¡å‹ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹æ‰‹æœ¯é¢„è§æ€§æ¨ç†çš„è§†è§‰é—®ç­”ç³»ç»Ÿï¼Œé€šè¿‡æ—¶åºå»ºæ¨¡å’Œé—¨æ§èåˆæœºåˆ¶å®ç°äº†ä»å›é¡¾æ€§æè¿°å‘å‰ç»æ€§é¢„æµ‹çš„è½¬å˜ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ‰‹æœ¯è§†è§‰é—®ç­”ç³»ç»Ÿä¸»è¦åŸºäºå­¤ç«‹å¸§çš„é™æ€è§†è§‰è¯­è¨€å¯¹é½ï¼Œç¼ºä¹å¯¹æœªæ¥æ­¥éª¤æˆ–å™¨æ¢°éœ€æ±‚çš„é¢„æµ‹èƒ½åŠ›ï¼Œè€Œç°æœ‰æ‰‹æœ¯VQAæ•°æ®é›†ä¹Ÿé›†ä¸­äºå½“å‰åœºæ™¯è€Œéè¿‘æœŸæœªæ¥ï¼Œæ— æ³•æ»¡è¶³å†…çª¥é•œç»é¼»è¶å‚ä½“æ‰‹æœ¯ç­‰è§†é‡å—é™ã€å·¥ä½œæµå¿«é€Ÿå˜åŒ–åœºæ™¯çš„å®æ—¶è¾…åŠ©éœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†SurgAnt-ViVQAè§†é¢‘è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨GRUé—¨æ§æ—¶åºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼ŒåŒå‘GRUç¼–ç å¸§é—´åŠ¨æ€ï¼Œè‡ªé€‚åº”é—¨æ§åœ¨tokençº§åˆ«å°†è§†è§‰ä¸Šä¸‹æ–‡æ³¨å…¥è¯­è¨€æµï¼Œå¹¶é€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒå®šåˆ¶è¯­è¨€éª¨å¹²ç½‘ç»œä»¥é€‚åº”æ‰‹æœ¯é¢†åŸŸã€‚</p>
<p><strong>Result:</strong> åœ¨PitVQA-Anticipationå’ŒEndoVisæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒSurgAnt-ViVQAè¶…è¶Šäº†å¼ºå›¾åƒå’Œè§†é¢‘åŸºçº¿ï¼Œæ¶ˆèç ”ç©¶æ˜¾ç¤ºæ—¶åºå¾ªç¯å’Œé—¨æ§èåˆè´¡çŒ®äº†ä¸»è¦æ€§èƒ½æå‡ï¼Œå¸§é¢„ç®—ç ”ç©¶å‘ç°8å¸§æœ€å¤§åŒ–æµç•…æ€§ï¼Œ32å¸§ç•¥å¾®é™ä½BLEUä½†æ”¹å–„æ•°å€¼æ—¶é—´ä¼°è®¡ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡å°†æ—¶åºæ„ŸçŸ¥ç¼–ç å™¨ä¸ç»†ç²’åº¦é—¨æ§äº¤å‰æ³¨æ„åŠ›ç»“åˆï¼ŒSurgAnt-ViVQAå°†æ‰‹æœ¯VQAä»å›é¡¾æ€§æè¿°æ¨è¿›åˆ°å‰ç»æ€§é¢„æµ‹ï¼ŒPitVQA-Anticipationä¸ºè¯¥é¢†åŸŸæä¾›äº†å…¨é¢åŸºå‡†ï¼Œå¹¶å¼ºè°ƒäº†é’ˆå¯¹æ€§æ—¶åºå»ºæ¨¡å¯¹äºå¯é ã€æœªæ¥æ„ŸçŸ¥æ‰‹æœ¯è¾…åŠ©çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Anticipating forthcoming surgical events is vital for real-time assistance in
endonasal transsphenoidal pituitary surgery, where visibility is limited and
workflow changes rapidly. Most visual question answering (VQA) systems reason
on isolated frames with static vision language alignment, providing little
support for forecasting next steps or instrument needs. Existing surgical VQA
datasets likewise center on the current scene rather than the near future. We
introduce PitVQA-Anticipation, the first VQA dataset designed for forward
looking surgical reasoning. It comprises 33.5 hours of operative video and
734,769 question answer pairs built from temporally grouped clips and expert
annotations across four tasks: predicting the future phase, next step, upcoming
instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video
language model that adapts a large language model using a GRU Gated Temporal
Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics,
while an adaptive gate injects visual context into the language stream at the
token level. Parameter efficient fine tuning customizes the language backbone
to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and
EndoVis datasets, surpassing strong image and video based baselines. Ablations
show that temporal recurrence and gated fusion drive most of the gains. A frame
budget study indicates a trade-off: 8 frames maximize fluency, whereas 32
frames slightly reduce BLEU but improve numeric time estimation. By pairing a
temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA
advances surgical VQA from retrospective description to proactive anticipation.
PitVQA-Anticipation offers a comprehensive benchmark for this setting and
highlights the importance of targeted temporal modeling for reliable, future
aware surgical assistance.</p>
<h3 id="6-petwb-rep-a-multi-cancer-whole-body-fdg-petct-and-radiology-report-dataset-for-medical-imaging-research">[6] <a href="https://arxiv.org/abs/2511.03194">PETWB-REP: A Multi-Cancer Whole-Body FDG PET/CT and Radiology Report Dataset for Medical Imaging Research</a></h3>
<p><em>Le Xue, Gang Feng, Wenbo Zhang, Yichi Zhang, Lanlan Li, Shuqi Wang, Liling Peng, Sisi Peng, Xin Gao</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†PETWB-REPæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«490åå¤šç§ç™Œç—‡æ‚£è€…å…¨èº«FDG PET/CTæ‰«æå’Œç›¸åº”æ”¾å°„å­¦æŠ¥å‘Šçš„å…¬å¼€æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒåŒ»å­¦å½±åƒã€æ”¾å°„ç»„å­¦å’Œå¤šæ¨¡æ€å­¦ä¹ ç ”ç©¶ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç¼ºä¹ç»“åˆåŠŸèƒ½ä¸è§£å‰–æˆåƒåŠè¯¦ç»†ä¸´åºŠæŠ¥å‘Šçš„å¤šç™Œç§å…¬å¼€åŒ»å­¦å½±åƒæ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†äººå·¥æ™ºèƒ½æ¨¡å‹çš„å¼€å‘å’ŒéªŒè¯ä»¥åŠå›é¡¾æ€§ä¸´åºŠç ”ç©¶çš„å¼€å±•ã€‚</p>
<p><strong>Method:</strong> è¯¥ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ï¼ŒåŒ…å«å…¨èº«18F-FDG PET/CTæ‰«æã€é…å¯¹çš„PETå’ŒCTå›¾åƒã€å»æ ‡è¯†åŒ–çš„æ–‡æœ¬æŠ¥å‘Šä»¥åŠç»“æ„åŒ–çš„ä¸´åºŠå…ƒæ•°æ®ï¼Œæ¶µç›–è‚ºç™Œã€è‚ç™Œã€ä¹³è…ºç™Œã€å‰åˆ—è…ºç™Œå’Œåµå·¢ç™Œç­‰å¸¸è§ç™Œç—‡ç±»å‹ã€‚</p>
<p><strong>Result:</strong> æ•°æ®é›†æˆåŠŸæ•´åˆäº†490åæ‚£è€…çš„å¤šç§æ¶æ€§è‚¿ç˜¤å½±åƒæ•°æ®å’Œä¸´åºŠä¿¡æ¯ï¼Œæä¾›äº†é…å¯¹çš„PET-CTå›¾åƒã€æ”¾å°„å­¦æŠ¥å‘Šå’Œç»“æ„åŒ–å…ƒæ•°æ®ï¼Œä¸ºå¤šæ¨¡æ€åŒ»å­¦AIç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚</p>
<p><strong>Conclusion:</strong> PETWB-REPæ•°æ®é›†å¡«è¡¥äº†å¤šç™Œç§å¤šæ¨¡æ€åŒ»å­¦å½±åƒæ•°æ®çš„ç©ºç™½ï¼Œä¸ºåŒ»å­¦å½±åƒåˆ†æã€æ”¾å°„ç»„å­¦ç ”ç©¶ã€äººå·¥æ™ºèƒ½ç®—æ³•å¼€å‘å’Œå¤šæ¨¡æ€å­¦ä¹ æä¾›äº†å®è´µçš„åŸºå‡†æ•°æ®é›†ï¼Œå°†æ¨åŠ¨ç²¾å‡†åŒ»ç–—å’Œä¸´åºŠç ”ç©¶çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Publicly available, large-scale medical imaging datasets are crucial for
developing and validating artificial intelligence models and conducting
retrospective clinical research. However, datasets that combine functional and
anatomical imaging with detailed clinical reports across multiple cancer types
remain scarce. Here, we present PETWB-REP, a curated dataset comprising
whole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography/Computed
Tomography (PET/CT) scans and corresponding radiology reports from 490 patients
diagnosed with various malignancies. The dataset primarily includes common
cancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and
ovarian cancer. This dataset includes paired PET and CT images, de-identified
textual reports, and structured clinical metadata. It is designed to support
research in medical imaging, radiomics, artificial intelligence, and
multi-modal learning.</p>
<h3 id="7-qg-coc-question-guided-chain-of-captions-for-large-multimodal-models">[7] <a href="https://arxiv.org/abs/2511.03206">QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models</a></h3>
<p><em>Kuei-Chun Kao, Hsu Tzu-Yin, Yunqi Hong, Ruochen Wang, Cho-Jui Hsieh</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é›¶æ ·æœ¬æç¤ºæ–¹æ³•QG-CoCï¼Œé€šè¿‡é—®é¢˜å¼•å¯¼çš„æ ‡é¢˜é“¾æœºåˆ¶æœ‰æ•ˆè§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šå›¾åƒåœºæ™¯ä¸‹çš„ç»†ç²’åº¦æ„ŸçŸ¥å’Œè·¨å›¾åƒæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºç«äº‰ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šå›¾åƒç¯å¢ƒä¸­å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šç¼ºä¹å¯¹åˆ†æ•£å›¾åƒçš„ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»¥åŠåœ¨å¤šè§†è§‰è¾“å…¥ä¸Šè¿›è¡Œæœ‰æ•ˆæ¨ç†å’Œä¿¡æ¯åˆæˆçš„èƒ½åŠ›ä¸‹é™ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨å•å›¾åƒè®¾ç½®æˆ–ç‰¹å®šå—é™åœºæ™¯ï¼Œç¼ºä¹å¯¹é€šç”¨å¤æ‚å¤šå›¾åƒæ¨ç†ä»»åŠ¡çš„ç†è§£å’Œè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬æç¤ºæ–¹æ³•QG-CoCï¼Œè¯¥æ–¹æ³•é€šè¿‡é—®é¢˜å¼•å¯¼çš„æ ‡é¢˜é“¾æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä»»æ„æ•°é‡å›¾åƒçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆå¯¹å¤šå›¾åƒåœºæ™¯è¿›è¡Œç³»ç»Ÿæ€§è°ƒæŸ¥ï¼Œå‘ç°ç°æœ‰æç¤ºæ–¹æ³•åœ¨å…³æ³¨æ‰€éœ€çº¿ç´¢å’Œæ— ç¼æ•´åˆæ„ŸçŸ¥ä¸æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¿›è€Œè®¾è®¡å‡ºè¿™ç§é€šç”¨çš„æç¤ºæ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šç§å¼€æºå’Œé—­æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ¶µç›–å¤šå›¾åƒå’Œå•å›¾åƒåŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQG-CoCåœ¨ä¸åŒä»»åŠ¡ä¸­å±•ç°å‡ºç«äº‰ä¼˜åŠ¿ï¼Œå¹¶åœ¨ç°æœ‰æç¤ºæ–¹æ³•å¤±æ•ˆçš„æŒ‘æˆ˜æ€§åœºæ™¯ä¸­è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Conclusion:</strong> QG-CoCæ–¹æ³•ä¸ºè§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šå›¾åƒæ¨ç†ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œå±•ç¤ºäº†åœ¨å¤æ‚å¤šå›¾åƒåœºæ™¯ä¸‹æå‡æ¨¡å‹æ€§èƒ½çš„æ½œåŠ›ã€‚è¯¥æ–¹æ³•ä¸ºæœªæ¥å¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¼ºè°ƒäº†ç»†ç²’åº¦æ„ŸçŸ¥ä¸æ¨ç†è¿‡ç¨‹æ— ç¼æ•´åˆçš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Recently, Multimodal Large Language Models (MLLMs) encounter two key issues
in multi-image contexts: (1) a lack of fine-grained perception across disparate
images, and (2) a diminished capability to effectively reason over and
synthesize information from multiple visual inputs. However, while various
prompting methods aim to describe visual content, many existing studies focus
primarily on single-image settings or specific, constrained scenarios. This
leaves a critical gap in understanding and addressing how MLLMs tackle more
general and complex multi-image reasoning tasks. Thus, we first extensively
investigate how current prompting methods perceive fine-grained visual details
and process visual information when dealing with multiple images. Our findings
reveal that existing prompting methods fall short in attending to needed clues
and seamlessly integrating perception and reasoning. Inspired by the findings,
we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions
(QG-CoC), a generalized prompting approach that effectively handles problems
with an arbitrary number of images. We evaluate our method on various
open-source and closed-source MLLMs for multi-image and single-image
benchmarks. Experimental results indicate that QG-CoC demonstrates competitive
performance across tasks and exhibits robust improvements in the challenging
scenarios where existing prompting methods fail.</p>
<h3 id="8-enhancing-medical-image-segmentation-via-heat-conduction-equation">[8] <a href="https://arxiv.org/abs/2511.03260">Enhancing Medical Image Segmentation via Heat Conduction Equation</a></h3>
<p><em>Rong Wu, Yim-Sang Yu</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆU-Mambaä¸çƒ­ä¼ å¯¼æ–¹ç¨‹çš„æ··åˆæ¶æ„ï¼Œé€šè¿‡çŠ¶æ€ç©ºé—´æ¨¡å—å®ç°é«˜æ•ˆé•¿ç¨‹æ¨ç†ï¼Œå¹¶åœ¨ç“¶é¢ˆå±‚å¼•å…¥çƒ­ä¼ å¯¼ç®—å­æ¨¡æ‹Ÿé¢‘åŸŸçƒ­æ‰©æ•£ï¼Œæ˜¾è‘—æå‡äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨å®ç”¨è®¡ç®—é¢„ç®—ä¸‹éš¾ä»¥åŒæ—¶å®ç°é«˜æ•ˆçš„å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œé•¿ç¨‹ä¾èµ–æ¨ç†ï¼Œç‰¹åˆ«æ˜¯U-Netå˜ä½“åœ¨å…¨å±€ä¿¡æ¯æ•è·æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§æ··åˆæ¶æ„ï¼Œç»“åˆMambaçŠ¶æ€ç©ºé—´æ¨¡å—è¿›è¡Œé«˜æ•ˆé•¿ç¨‹æ¨ç†ï¼Œå¹¶åœ¨ç“¶é¢ˆå±‚å¼•å…¥çƒ­ä¼ å¯¼ç®—å­æ¨¡æ‹Ÿé¢‘åŸŸçƒ­æ‰©æ•£è¿‡ç¨‹ï¼Œå¢å¼ºè¯­ä¹‰æŠ½è±¡èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šæ¨¡æ€è…¹éƒ¨CTå’ŒMRIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æŒç»­ä¼˜äºå¼ºåŸºçº¿æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> å°†çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦ä¸åŸºäºçƒ­åŠ›å­¦çš„å…¨å±€æ‰©æ•£ç›¸ç»“åˆï¼Œä¸ºåŒ»å­¦åˆ†å‰²ä»»åŠ¡æä¾›äº†å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†æ··åˆå»ºæ¨¡ç­–ç•¥çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Medical image segmentation has been significantly advanced by deep learning
architectures, notably U-Net variants. However, existing models struggle to
achieve efficient global context modeling and long-range dependency reasoning
under practical computational budgets simultaneously. In this work, we propose
a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation.
Our model combines Mamba-based state-space modules for efficient long-range
reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers,
simulating frequency-domain thermal diffusion for enhanced semantic
abstraction. Experimental results on multimodal abdominal CT and MRI datasets
demonstrate that the proposed model consistently outperforms strong baselines,
validating its effectiveness and generalizability. It suggest that blending
state-space dynamics with heat-based global diffusion offers a scalable and
interpretable solution for medical segmentation tasks.</p>
<h3 id="9-unified-long-video-inpainting-and-outpainting-via-overlapping-high-order-co-denoising">[9] <a href="https://arxiv.org/abs/2511.03272">Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising</a></h3>
<p><em>Shuangquan Lyu, Steven Mao, Yue Ma</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„é•¿è§†é¢‘ä¿®å¤ä¸å¤–å»¶æ–¹æ³•ï¼Œé€šè¿‡æ‰©å±•æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹å®ç°ä»»æ„é•¿åº¦çš„é«˜ä¿çœŸç©ºé—´ç¼–è¾‘è§†é¢‘ç”Ÿæˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨LoRAé«˜æ•ˆå¾®è°ƒé¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨é‡å æ··åˆæ—¶é—´ååŒå»å™ªç­–ç•¥ä¿æŒé•¿åºåˆ—ä¸€è‡´æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é•¿è§†é¢‘ç”Ÿæˆå­˜åœ¨æ ¹æœ¬æ€§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘ä¿®å¤å’Œå¤–å»¶ä»»åŠ¡ä¸­å®ç°é«˜å¯æ§æ€§å°¤ä¸ºå›°éš¾ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å›ºå®šé•¿åº¦ç‰‡æ®µæ—¶å­˜åœ¨å±€é™æ€§ï¼Œå®¹æ˜“å‡ºç°æ‹¼æ¥ä¼ªå½±æˆ–ä¸€è‡´æ€§æ¼‚ç§»é—®é¢˜ï¼Œéš¾ä»¥å®ç°ä»»æ„é•¿åº¦çš„æ— ç¼è§†é¢‘ç¼–è¾‘ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨LoRAå¯¹é˜¿é‡Œå·´å·´Wan 2.1ç­‰å¤§å‹é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼Œä¸“é—¨é’ˆå¯¹æ©ç åŒºåŸŸè§†é¢‘åˆæˆä»»åŠ¡ã€‚é€šè¿‡é‡å æ··åˆæ—¶é—´ååŒå»å™ªç­–ç•¥ç»“åˆé«˜é˜¶æ±‚è§£å™¨ï¼Œç¡®ä¿é•¿åºåˆ—ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œé¿å…å‡ºç°å¯è§çš„æ¥ç¼æˆ–æ¼‚ç§»ç°è±¡ã€‚</p>
<p><strong>Result:</strong> åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¿®å¤å’Œå¤–å»¶ä»»åŠ¡ä¸ŠéªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æ•°ç™¾å¸§çš„å¯¹è±¡ç¼–è¾‘å’Œæ·»åŠ ã€‚åœ¨è´¨é‡æŒ‡æ ‡ï¼ˆPSNR/SSIMï¼‰å’Œæ„ŸçŸ¥çœŸå®æ€§ï¼ˆLPIPSï¼‰æ–¹é¢å‡ä¼˜äºWan 2.1æ¨¡å‹å’ŒVACEç­‰åŸºçº¿æ–¹æ³•ï¼Œå®ç°äº†å‚æ•°æ•ˆç‡ä¸æ€§èƒ½ä¼˜è¶Šæ€§çš„å¹³è¡¡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•ä¸ºå®é™…é•¿èŒƒå›´è§†é¢‘ç¼–è¾‘æä¾›äº†å¯è¡Œè§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰æœ€å°è®¡ç®—å¼€é”€ã€‚ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•æœ‰æ•ˆæ‰©å±•æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œï¼Œä¸ºä»»æ„é•¿åº¦è§†é¢‘çš„ç©ºé—´ç¼–è¾‘ä»»åŠ¡å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œåœ¨å‚æ•°æ•ˆç‡å’Œç”Ÿæˆè´¨é‡ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Generating long videos remains a fundamental challenge, and achieving high
controllability in video inpainting and outpainting is particularly demanding.
To address both of these challenges simultaneously and achieve controllable
video inpainting and outpainting for long video clips, we introduce a novel and
unified approach for long video inpainting and outpainting that extends
text-to-video diffusion models to generate arbitrarily long, spatially edited
videos with high fidelity. Our method leverages LoRA to efficiently fine-tune a
large pre-trained video diffusion model like Alibaba's Wan 2.1 for masked
region video synthesis, and employs an overlap-and-blend temporal co-denoising
strategy with high-order solvers to maintain consistency across long sequences.
In contrast to prior work that struggles with fixed-length clips or exhibits
stitching artifacts, our system enables arbitrarily long video generation and
editing without noticeable seams or drift. We validate our approach on
challenging inpainting/outpainting tasks including editing or adding objects
over hundreds of frames and demonstrate superior performance to baseline
methods like Wan 2.1 model and VACE in terms of quality (PSNR/SSIM), and
perceptual realism (LPIPS). Our method enables practical long-range video
editing with minimal overhead, achieved a balance between parameter efficient
and superior performance.</p>
<h3 id="10-diffusion-sdpo-safeguarded-direct-preference-optimization-for-diffusion-models">[10] <a href="https://arxiv.org/abs/2511.03317">Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models</a></h3>
<p><em>Minghao Fu, Guo-Hua Wang, Tianyu Cui, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Diffusion-SDPOæ–¹æ³•ï¼Œè§£å†³äº†æ‰©æ•£æ¨¡å‹ç›´æ¥åå¥½ä¼˜åŒ–ä¸­çš„å…³é”®ç—…ç†é—®é¢˜ï¼Œé€šè¿‡è‡ªé€‚åº”ç¼©æ”¾å¤±è´¥è€…æ¢¯åº¦æ¥ä¿æŠ¤è·èƒœè€…è´¨é‡ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¼˜äºåŸºçº¿çš„æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºæ‰©æ•£çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•å­˜åœ¨ä¸€ä¸ªå…³é”®ç—…ç†ï¼šå¢å¤§åå¥½è¾¹ç•Œå¹¶ä¸æ€»èƒ½æå‡ç”Ÿæˆè´¨é‡ï¼Œæ ‡å‡†Diffusion-DPOç›®æ ‡ä¼šå¢åŠ è·èƒœè€…å’Œå¤±è´¥è€…åˆ†æ”¯çš„é‡æ„è¯¯å·®ï¼Œå¯¼è‡´å³ä½¿åå¥½è¾¹ç•Œæ‰©å¤§ï¼Œåå¥½åˆ†æ”¯ä¹Ÿä¼šå—åˆ°ä¸åˆ©å½±å“ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†Diffusion-SDPOæ–¹æ³•ï¼Œé‡‡ç”¨ä¿æŠ¤æ€§æ›´æ–°è§„åˆ™ï¼Œé€šè¿‡è‡ªé€‚åº”ç¼©æ”¾å¤±è´¥è€…æ¢¯åº¦æ¥ä¿æŠ¤è·èƒœè€…ï¼Œä¸€é˜¶åˆ†æå¾—å‡ºé—­å¼ç¼©æ”¾ç³»æ•°ï¼Œç¡®ä¿æ¯ä¸ªä¼˜åŒ–æ­¥éª¤ä¸­åå¥½è¾“å‡ºçš„è¯¯å·®éé€’å¢ï¼Œè¯¥æ–¹æ³•ç®€å•ã€æ¨¡å‹æ— å…³ä¸”ä¸ç°æœ‰DPOé£æ ¼å¯¹é½æ¡†æ¶å¹¿æ³›å…¼å®¹ã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†æ–‡æœ¬åˆ°å›¾åƒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDiffusion-SDPOåœ¨è‡ªåŠ¨åŒ–åå¥½ã€ç¾å­¦å’Œæç¤ºå¯¹é½æŒ‡æ ‡ä¸Šç›¸æ¯”åå¥½å­¦ä¹ åŸºçº¿å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä»…å¢åŠ äº†è¾¹é™…è®¡ç®—å¼€é”€ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†æ‰©æ•£æ¨¡å‹åå¥½ä¼˜åŒ–ä¸­çš„å…³é”®ç—…ç†æœºåˆ¶ï¼Œæå‡ºçš„ä¿æŠ¤æ€§æ›´æ–°æ–¹æ³•ä¸ºæ‰©æ•£æ¨¡å‹å¯¹é½æä¾›äº†æ›´å¯é çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§å’Œå®é™…éƒ¨ç½²ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Text-to-image diffusion models deliver high-quality images, yet aligning them
with human preferences remains challenging. We revisit diffusion-based Direct
Preference Optimization (DPO) for these models and identify a critical
pathology: enlarging the preference margin does not necessarily improve
generation quality. In particular, the standard Diffusion-DPO objective can
increase the reconstruction error of both winner and loser branches.
Consequently, degradation of the less-preferred outputs can become sufficiently
severe that the preferred branch is also adversely affected even as the margin
grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule
that preserves the winner by adaptively scaling the loser gradient according to
its alignment with the winner gradient. A first-order analysis yields a
closed-form scaling coefficient that guarantees the error of the preferred
output is non-increasing at each optimization step. Our method is simple,
model-agnostic, broadly compatible with existing DPO-style alignment frameworks
and adds only marginal computational overhead. Across standard text-to-image
benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning
baselines on automated preference, aesthetic, and prompt alignment metrics.
Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.</p>
<h3 id="11-decoupling-augmentation-bias-in-prompt-learning-for-vision-language-models">[11] <a href="https://arxiv.org/abs/2511.03367">Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models</a></h3>
<p><em>Gahyeon Kim, Sohee Kim, Seokju Lee</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºAAPLæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å¯¹æŠ—æ€§tokenåµŒå…¥æ¥è§£è€¦å›¾åƒå¢å¼ºå¼•å…¥çš„è¡¨å±‚è§†è§‰å˜åŒ–ä¸ç±»åˆ«ç›¸å…³è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œå¢å¼ºæç¤ºå­¦ä¹ åœ¨é›¶æ ·æœ¬å­¦ä¹ ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨11ä¸ªåŸºå‡†æ•°æ®é›†ä¸ŠæŒç»­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å°‘æ ·æœ¬ã€é›¶æ ·æœ¬ã€è·¨æ•°æ®é›†å’Œé¢†åŸŸæ³›åŒ–æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æç¤ºå­¦ä¹ æ–¹æ³•å¦‚CoOpå’ŒCoCoOpè™½ç„¶é€šè¿‡å¯å­¦ä¹ å‘é‡æ›¿ä»£æ‰‹å·¥æç¤ºæå‡äº†é›¶æ ·æœ¬å­¦ä¹ æ€§èƒ½ï¼Œä½†åœ¨å¤„ç†å®Œå…¨æœªè§ç±»åˆ«æ—¶æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ä¼ ç»Ÿé›¶æ ·æœ¬å­¦ä¹ å—ç›Šäºå¤šç§æ•°æ®å¢å¼ºç­–ç•¥ï¼Œä½†æç¤ºå­¦ä¹ ä¸»è¦å…³æ³¨æ–‡æœ¬å±‚é¢çš„ä¿®æ”¹ï¼Œå›¾åƒçº§å¢å¼ºçš„æ½œåŠ›å°šæœªå……åˆ†æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹å¯¹è¯­ä¹‰ç›¸å…³è§†è§‰ç‰¹å¾å­¦ä¹ çš„æ˜¾å¼æŒ‡å¯¼ã€‚</p>
<p><strong>Method:</strong> æå‡ºAAPLæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å¯¹æŠ—æ€§tokenåµŒå…¥æ¥è§£è€¦å›¾åƒå¢å¼ºå¼•å…¥çš„è¡¨å±‚è§†è§‰å˜åŒ–ä¸ç±»åˆ«ç›¸å…³è¯­ä¹‰è¡¨ç¤ºã€‚è¯¥æ–¹æ³•åˆ©ç”¨å±æ€§ç‰¹å®šçš„å›¾åƒçº§å¢å¼ºï¼Œä½¿å­¦ä¹ çš„æç¤ºèƒ½å¤Ÿä¸“æ³¨äºä¸ç›®æ ‡ç±»åˆ«å¯¹é½çš„è§†è§‰åˆ¤åˆ«æ€§ç‰¹å¾ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹è¯­ä¹‰ç›¸å…³è§†è§‰ç‰¹å¾æ˜¾å¼æŒ‡å¯¼çš„å±€é™æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨11ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒAAPLåœ¨å°‘æ ·æœ¬ã€é›¶æ ·æœ¬ã€è·¨æ•°æ®é›†å’Œé¢†åŸŸæ³›åŒ–è®¾ç½®ä¸­æŒç»­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å„ç§å­¦ä¹ åœºæ™¯ä¸‹çš„æ³›åŒ–æ€§èƒ½ï¼Œè¯æ˜äº†å›¾åƒçº§å¢å¼ºä¸è½¯æç¤ºæ¡†æ¶ååŒä½œç”¨çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å›¾åƒçº§å¢å¼ºç‰¹åˆ«æ˜¯å±æ€§ç‰¹å®šå˜åŒ–èƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒæç¤ºå­¦ä¹ ï¼Œå¯¹æŠ—æ€§tokenåµŒå…¥æœºåˆ¶æˆåŠŸè§£è€¦äº†è¡¨å±‚å˜åŒ–ä¸è¯­ä¹‰è¡¨ç¤ºã€‚è¿™é¡¹å·¥ä½œä¸ºæç¤ºå­¦ä¹ æä¾›äº†æ–°çš„å¢å¼ºç­–ç•¥æ–¹å‘ï¼Œå¼ºè°ƒäº†è§†è§‰ç‰¹å¾ä¸è¯­ä¹‰å¯¹é½åœ¨é›¶æ ·æœ¬å­¦ä¹ ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶å¼€è¾Ÿäº†ç»“åˆè§†è§‰å’Œæ–‡æœ¬å¢å¼ºçš„æ··åˆæ–¹æ³•è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Recent advances in large-scale vision and language models have led to
significant progress in zero-shot learning tasks. Methods such as CoOp and
CoCoOp have shown that replacing handcrafted prompts with learnable vectors,
known as prompt learning, can result in improved performance. However, these
models often struggle to generalize to entirely unseen categories. While
traditional zero-shot learning techniques benefit from various data
augmentation strategies, prompt learning has primarily focused on text-based
modifications, leaving the potential of image-based augmentation largely
unexplored. In this work, we explore how image-level augmentations,
particularly those that introduce attribute-specific variations, can support
and enhance prompt learning. Our analysis examines the interaction between
these augmentations and soft prompt frameworks, revealing their potential to
improve generalization. We also identify a limitation in existing methods, such
as CoCoOp, which do not provide explicit guidance for learning prompts that
focus on semantically meaningful visual features. To address this, we propose
Adding Attributes to Prompt Learning, AAPL, a novel method that introduces
adversarial token embeddings to decouple superficial visual variations
introduced by augmentation from class-relevant semantic representations. This
decoupling enables the learned prompts to concentrate on visually
discriminative features that align with the target categories. We conduct
comprehensive experiments on eleven benchmark datasets, and AAPL consistently
outperforms existing methods across few-shot, zero-shot, cross-dataset, and
domain generalization settings. Our source code is publicly available at:
https://github.com/Gahyeonkim09/AAPL</p>
<h3 id="12-surgvivqa-temporally-grounded-video-question-answering-for-surgical-scene-understanding">[12] <a href="https://arxiv.org/abs/2511.03325">SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding</a></h3>
<p><em>Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena, Cesare Hassan, Danail Stoyanov, Elena De Momi, Sophia Bano, Mobarak I. Hoque</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SurgViVQAï¼Œä¸€ç§ç”¨äºæ‰‹æœ¯è§†é¢‘é—®ç­”çš„æ¨¡å‹ï¼Œé€šè¿‡æ©ç è§†é¢‘-æ–‡æœ¬ç¼–ç å™¨èåˆè§†é¢‘å’Œé—®é¢˜ç‰¹å¾ï¼Œæ•æ‰æ‰‹æœ¯åœºæ™¯ä¸­çš„æ—¶é—´åŠ¨æ€ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†æ‰‹æœ¯VideoQAçš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ‰‹æœ¯è§†é¢‘é—®ç­”æ–¹æ³•å±€é™äºé™æ€å›¾åƒç‰¹å¾ï¼Œå¯ç”¨æ•°æ®é›†ç¼ºä¹æ—¶é—´æ ‡æ³¨ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰æ‰‹æœ¯è¿‡ç¨‹ä¸­å…³é”®çš„è¿åŠ¨åŠ¨æ€å’Œå·¥å…·-ç»„ç»‡äº¤äº’ï¼Œé™åˆ¶äº†AIæ¨¡å‹å¯¹åŠ¨æ€æ‰‹æœ¯åœºæ™¯çš„å‡†ç¡®ç†è§£ã€‚</p>
<p><strong>Method:</strong> SurgViVQAé‡‡ç”¨æ©ç è§†é¢‘-æ–‡æœ¬ç¼–ç å™¨èåˆè§†é¢‘å’Œé—®é¢˜ç‰¹å¾ï¼Œæ•æ‰è¿åŠ¨åŠ¨æ€å’Œå·¥å…·-ç»„ç»‡äº¤äº’ç­‰æ—¶é—´çº¿ç´¢ï¼Œç„¶åé€šè¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è§£ç ç”Ÿæˆè¿è´¯ç­”æ¡ˆï¼›åŒæ—¶æ„å»ºäº†REAL-Colon-VQAæ•°æ®é›†ï¼ŒåŒ…å«è¿åŠ¨ç›¸å…³é—®é¢˜å’Œè¯Šæ–­å±æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨REAL-Colon-VQAå’ŒEndoVis18-VQAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSurgViVQAåœ¨å…³é”®è¯å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰å›¾åƒåŸºå‡†æ¨¡å‹ï¼Œåˆ†åˆ«æ¯”PitVQAæå‡11%å’Œ9%ï¼›æ‰°åŠ¨ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æ¨¡å‹å¯¹é—®é¢˜è¡¨è¿°å˜åŒ–çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> SurgViVQAå’ŒREAL-Colon-VQAæ•°æ®é›†ä¸ºæ‰‹æœ¯è§†é¢‘é—®ç­”æä¾›äº†æ—¶é—´æ„ŸçŸ¥ç†è§£æ¡†æ¶ï¼Œä½¿AIæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£é‡ŠåŠ¨æ€æ‰‹æœ¯è¿‡ç¨‹ï¼›è¯¥ç ”ç©¶æ¨åŠ¨äº†æ‰‹æœ¯AIä»é™æ€å›¾åƒåˆ†æå‘åŠ¨æ€åœºæ™¯ç†è§£çš„è½¬å˜ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Video Question Answering (VideoQA) in the surgical domain aims to enhance
intraoperative understanding by enabling AI models to reason over temporally
coherent events rather than isolated frames. Current approaches are limited to
static image features, and available datasets often lack temporal annotations,
ignoring the dynamics critical for accurate procedural interpretation. We
propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from
static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder
to fuse video and question features, capturing temporal cues such as motion and
tool--tissue interactions, which a fine-tuned large language model (LLM) then
decodes into coherent answers. To evaluate its performance, we curated
REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related
questions and diagnostic attributes, as well as out-of-template questions with
rephrased or semantically altered formulations to assess model robustness.
Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset
shows that SurgViVQA outperforms existing image-based VQA benchmark models,
particularly in keyword accuracy, improving over PitVQA by +11\% on
REAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questions
further confirms improved generalizability and robustness to variations in
question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework
for temporally-aware understanding in surgical VideoQA, enabling AI models to
interpret dynamic procedural contexts more effectively. Code and dataset
available at https://github.com/madratak/SurgViVQA.</p>
<h3 id="13-multi-object-tracking-retrieval-with-llava-video-a-training-free-solution-to-mot25-stag-challenge">[13] <a href="https://arxiv.org/abs/2511.03332">Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge</a></h3>
<p><em>Yi Yang, Yiming Xu, Timo Kaiser, Hao Cheng, Bodo Rosenhahn, Michael Ying Yang</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºMOT25æ—¶ç©ºåŠ¨ä½œå®šä½æŒ‘æˆ˜çš„ä¸¤é˜¶æ®µé›¶æ ·æœ¬æ–¹æ³•ï¼Œå°†æœ€å…ˆè¿›çš„è·Ÿè¸ªæ¨¡å‹FastTrackerä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹LLaVA-Videoç›¸ç»“åˆï¼Œåœ¨å¤æ‚çœŸå®åœºæ™¯ä¸­å®ç°äº†åŸºäºè¯­è¨€æŸ¥è¯¢çš„å¤šç›®æ ‡å®šä½ä¸è·Ÿè¸ªï¼Œæœ€ç»ˆåœ¨æŒ‘æˆ˜èµ›ä¸­è·å¾—äº†ç¬¬äºŒåã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³MOT25æ—¶ç©ºåŠ¨ä½œå®šä½æŒ‘æˆ˜ä¸­çš„æ ¸å¿ƒé—®é¢˜ï¼Œå³åœ¨å¤æ‚çœŸå®ä¸–ç•Œåœºæ™¯çš„è§†é¢‘æ•°æ®ä¸­ï¼Œå‡†ç¡®åœ°å¯¹ç¬¦åˆç‰¹å®šå’Œè‡ªç”±å½¢å¼è¯­è¨€æŸ¥è¯¢çš„å¤šä¸ªç›®æ ‡è¿›è¡Œå®šä½å’Œè·Ÿè¸ªï¼Œè¿™éœ€è¦åŒæ—¶å¤„ç†è§†è§‰è·Ÿè¸ªå’Œè¯­è¨€ç†è§£çš„å¤šæ¨¡æ€ä»»åŠ¡ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•å°†ä»»åŠ¡å»ºæ¨¡ä¸ºè§†é¢‘æ£€ç´¢é—®é¢˜ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µé›¶æ ·æœ¬æ–¹æ³•ï¼Œç»“åˆäº†æœ€å…ˆè¿›çš„è·Ÿè¸ªæ¨¡å‹FastTrackerå’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹LLaVA-Videoçš„ä¼˜åŠ¿ï¼Œé€šè¿‡é›†æˆè§†è§‰è·Ÿè¸ªèƒ½åŠ›å’Œè¯­è¨€ç†è§£èƒ½åŠ›æ¥å®ç°åŸºäºæŸ¥è¯¢çš„ç›®æ ‡å®šä½ä¸è·Ÿè¸ªã€‚</p>
<p><strong>Result:</strong> åœ¨MOT25-StAGæµ‹è¯•é›†ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†m-HIoUå¾—åˆ†20.68å’ŒHOTAå¾—åˆ†10.73çš„ä¼˜å¼‚è¡¨ç°ï¼Œè¿™ä¸€æˆç»©åœ¨æŒ‘æˆ˜èµ›ä¸­è·å¾—äº†ç¬¬äºŒåï¼Œè¯æ˜äº†æ‰€ææ–¹æ³•åœ¨å¤æ‚å¤šç›®æ ‡æ—¶ç©ºåŠ¨ä½œå®šä½ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ï¼Œå°†å…ˆè¿›çš„è·Ÿè¸ªæ¨¡å‹ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç›¸ç»“åˆçš„ä¸¤é˜¶æ®µé›¶æ ·æœ¬æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³åŸºäºè¯­è¨€æŸ¥è¯¢çš„å¤šç›®æ ‡æ—¶ç©ºå®šä½é—®é¢˜ï¼Œä¸ºè§†é¢‘ç†è§£ä¸å¤šæ¨¡æ€ä»»åŠ¡æä¾›äº†æœ‰å‰æ™¯çš„æŠ€æœ¯è·¯å¾„ï¼Œå±•ç¤ºäº†åœ¨å¤æ‚çœŸå®åœºæ™¯ä¸­å®ç°ç²¾ç¡®åŠ¨ä½œå®šä½çš„å¯è¡Œæ€§ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>In this report, we present our solution to the MOT25-Spatiotemporal Action
Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately
localize and track multiple objects that match specific and free-form language
queries, using video data of complex real-world scenes as input. We model the
underlying task as a video retrieval problem and present a two-stage, zero-shot
approach, combining the advantages of the SOTA tracking model FastTracker and
Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our
method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which
won second place in the challenge.</p>
<h3 id="14-uniavgen-unified-audio-and-video-generation-with-asymmetric-cross-modal-interactions">[14] <a href="https://arxiv.org/abs/2511.03334">UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions</a></h3>
<p><em>Guozhen Zhang, Zixiang Zhou, Teng Hu, Ziqiao Peng, Youliang Zhang, Yi Chen, Yuan Zhou, Qinglin Lu, Limin Wang</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UniAVGenï¼Œä¸€ç§ç»Ÿä¸€çš„éŸ³é¢‘-è§†é¢‘è”åˆç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡åŒåˆ†æ”¯æ‰©æ•£å˜æ¢å™¨æ¶æ„å’Œéå¯¹ç§°è·¨æ¨¡æ€äº¤äº’æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†å”‡éƒ¨åŒæ­¥å’Œè¯­ä¹‰ä¸€è‡´æ€§ï¼Œåœ¨å‡å°‘è®­ç»ƒæ•°æ®é‡çš„åŒæ—¶å®ç°äº†å¤šä»»åŠ¡ç»Ÿä¸€å»ºæ¨¡ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¼€æºéŸ³é¢‘-è§†é¢‘ç”Ÿæˆæ–¹æ³•ç”±äºç¼ºä¹æœ‰æ•ˆçš„è·¨æ¨¡æ€å»ºæ¨¡ï¼Œå¾€å¾€å­˜åœ¨å”‡éƒ¨åŒæ­¥æ•ˆæœä¸ä½³å’Œè¯­ä¹‰ä¸€è‡´æ€§ä¸è¶³çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†ç”Ÿæˆå†…å®¹çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Method:</strong> UniAVGené‡‡ç”¨åŒåˆ†æ”¯è”åˆåˆæˆæ¶æ„ï¼ŒåŒ…å«ä¸¤ä¸ªå¹¶è¡Œçš„æ‰©æ•£å˜æ¢å™¨æ„å»ºç»Ÿä¸€çš„è·¨æ¨¡æ€æ½œåœ¨ç©ºé—´ï¼Œæ ¸å¿ƒæ˜¯éå¯¹ç§°è·¨æ¨¡æ€äº¤äº’æœºåˆ¶å®ç°åŒå‘æ—¶é—´å¯¹é½çš„è·¨æ³¨æ„åŠ›ï¼Œå¹¶è¾…ä»¥é¢éƒ¨æ„ŸçŸ¥è°ƒåˆ¶æ¨¡å—åŠ¨æ€ä¼˜åŒ–äº¤äº’è¿‡ç¨‹ï¼ŒåŒæ—¶æå‡ºæ¨¡æ€æ„ŸçŸ¥çš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥å¢å¼ºæ¨ç†é˜¶æ®µçš„ç”Ÿæˆä¿çœŸåº¦ã€‚</p>
<p><strong>Result:</strong> ç»¼åˆå®éªŒéªŒè¯è¡¨æ˜ï¼ŒUniAVGenåœ¨ä»…ä½¿ç”¨1.3Mè®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œç›¸æ¯”éœ€è¦30.1Mæ ·æœ¬çš„åŸºçº¿æ–¹æ³•ï¼Œåœ¨éŸ³é¢‘-è§†é¢‘åŒæ­¥ã€éŸ³è‰²ä¸€è‡´æ€§å’Œæƒ…æ„Ÿä¸€è‡´æ€§æ–¹é¢å‡è¡¨ç°å‡ºæ•´ä½“ä¼˜åŠ¿ï¼Œå®ç°äº†æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Conclusion:</strong> UniAVGençš„é²æ£’è”åˆåˆæˆè®¾è®¡å®ç°äº†å…³é”®éŸ³é¢‘-è§†é¢‘ä»»åŠ¡åœ¨å•ä¸€æ¨¡å‹ä¸­çš„æ— ç¼ç»Ÿä¸€ï¼ŒåŒ…æ‹¬è”åˆéŸ³é¢‘-è§†é¢‘ç”Ÿæˆä¸å»¶ç»­ã€è§†é¢‘åˆ°éŸ³é¢‘é…éŸ³ä»¥åŠéŸ³é¢‘é©±åŠ¨è§†é¢‘åˆæˆï¼Œä¸ºè·¨æ¨¡æ€ç”Ÿæˆæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Due to the lack of effective cross-modal modeling, existing open-source
audio-video generation methods often exhibit compromised lip synchronization
and insufficient semantic consistency. To mitigate these drawbacks, we propose
UniAVGen, a unified framework for joint audio and video generation. UniAVGen is
anchored in a dual-branch joint synthesis architecture, incorporating two
parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent
space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which
enables bidirectional, temporally aligned cross-attention, thus ensuring
precise spatiotemporal synchronization and semantic consistency. Furthermore,
this cross-modal interaction is augmented by a Face-Aware Modulation module,
which dynamically prioritizes salient regions in the interaction process. To
enhance generative fidelity during inference, we additionally introduce
Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly
amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint
synthesis design enables seamless unification of pivotal audio-video tasks
within a single model, such as joint audio-video generation and continuation,
video-to-audio dubbing, and audio-driven video synthesis. Comprehensive
experiments validate that, with far fewer training samples (1.3M vs. 30.1M),
UniAVGen delivers overall advantages in audio-video synchronization, timbre
consistency, and emotion consistency.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="15-lego-eval-towards-fine-grained-evaluation-on-synthesizing-3d-embodied-environments-with-tool-augmentation">[15] <a href="https://arxiv.org/abs/2511.03001">LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation</a></h3>
<p><em>Gyeom Hwangbo, Hyungjoo Chae, Minseok Kang, Hyeonjong Ju, Soohyun Oh, Jinyoung Yeo</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†LEGO-Evalè¯„ä¼°æ¡†æ¶å’ŒLEGO-BenchåŸºå‡†ï¼Œç”¨äºè§£å†³å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆ3Dåœºæ™¯æ—¶ç¼ºä¹çœŸå®ç©ºé—´å¸ƒå±€å’Œå¯¹è±¡å±æ€§çš„é—®é¢˜ï¼Œé€šè¿‡æ˜¾å¼åœ°æ¥åœ°åœºæ™¯ç»„ä»¶æ¥æ›´å‡†ç¡®åœ°è¯„ä¼°åœºæ™¯ä¸ç»†ç²’åº¦æŒ‡ä»¤çš„å¯¹é½ç¨‹åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„3Dåœºæ™¯ç”Ÿæˆæ–¹æ³•å­˜åœ¨ç©ºé—´å¸ƒå±€å’Œå¯¹è±¡å±æ€§ä¸çœŸå®çš„é—®é¢˜ï¼Œè¿™æºäºæŒ‡ä»¤ç²’åº¦ä¸å¤Ÿç»†ï¼Œå¯¼è‡´ç”Ÿæˆçš„åœºæ™¯ä¸ç°å®ä¸–ç•Œç¯å¢ƒå­˜åœ¨åå·®ï¼Œè¿›è€Œå½±å“åœ¨ä»¿çœŸç¯å¢ƒä¸­è®­ç»ƒçš„å…·èº«æ™ºèƒ½ä½“çš„æ€§èƒ½ï¼Œè€Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¦‚CLIPScoreå’Œè§†è§‰è¯­è¨€æ¨¡å‹å¯¹3Dåœºæ™¯ç†è§£ä¸è¶³ï¼Œæ— æ³•å¯é è¯„ä¼°åœºæ™¯ä¸ç»†ç²’åº¦æŒ‡ä»¤çš„å¯¹é½ç¨‹åº¦ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†LEGO-Evalè¯„ä¼°æ¡†æ¶ï¼Œé…å¤‡å¤šæ ·åŒ–å·¥å…·æ¥æ˜¾å¼åœ°æ¥åœ°åœºæ™¯ç»„ä»¶ï¼Œå®ç°æ›´å‡†ç¡®çš„å¯¹é½è¯„ä¼°ï¼›åŒæ—¶æ„å»ºäº†LEGO-BenchåŸºå‡†ï¼ŒåŒ…å«è¯¦ç»†æŒ‡ä»¤ä»¥æŒ‡å®šç°å®ä¸–ç•Œç¯å¢ƒçš„å¤æ‚å¸ƒå±€å’Œå±æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜LEGO-Evalåœ¨è¯„ä¼°åœºæ™¯-æŒ‡ä»¤å¯¹é½æ–¹é¢æ¯”VLM-as-a-judgeæ–¹æ³•é«˜å‡º0.41 F1åˆ†æ•°ï¼›ä½¿ç”¨LEGO-Benchè¿›è¡ŒåŸºå‡†æµ‹è¯•æ˜¾ç¤ºå½“å‰ç”Ÿæˆæ–¹æ³•å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œæ‰€æœ‰è¯„ä¼°æ–¹æ³•åœ¨ç”Ÿæˆå®Œå…¨ç¬¦åˆç»†ç²’åº¦æŒ‡ä»¤çš„åœºæ™¯æ—¶æˆåŠŸç‡æœ€é«˜ä»…ä¸º10%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰3Dåœºæ™¯ç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†ç»†ç²’åº¦æŒ‡ä»¤æ–¹é¢çš„ä¸¥é‡ä¸è¶³ï¼Œæå‡ºçš„è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†ä¸ºæ”¹è¿›3Dåœºæ™¯ç”Ÿæˆè´¨é‡æä¾›äº†é‡è¦å·¥å…·ï¼Œå¼ºè°ƒäº†éœ€è¦å¼€å‘æ›´å…ˆè¿›çš„ç”Ÿæˆæ–¹æ³•æ¥æ»¡è¶³ç°å®ä¸–ç•Œç¯å¢ƒå»ºæ¨¡çš„éœ€æ±‚ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Despite recent progress in using Large Language Models (LLMs) for
automatically generating 3D scenes, generated scenes often lack realistic
spatial layouts and object attributes found in real-world environments. As this
problem stems from insufficiently detailed, coarse-grained instructions,
advancing 3D scene synthesis guided by more detailed, fine-grained instructions
that reflect real-world environments becomes crucial. Without such realistic
scenes, training embodied agents in unrealistic environments can lead them to
learn priors that diverge significantly from real-world physics and semantics,
degrading their performance when deployed. Thus, verifying the alignment
between the fine-grained instruction and the generated scene is essential for
effective learning. However, current evaluation methods, such as CLIPScore and
vision-language models (VLMs), often fail to reliably assess such alignment.
This shortcoming arises primarily from their shallow understanding of 3D
scenes, which often leads to improperly grounded scene components. To address
this, we introduce LEGO-Eval, an evaluation framework equipped with diverse
tools designed to explicitly ground scene components, enabling more accurate
alignment assessments. We also present LEGO-Bench, a benchmark of detailed
instructions that specify complex layouts and attributes of real-world
environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge
by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with
LEGO-Bench reveals significant limitations in current generation methods.
Across all evaluated approaches, success rates reached at most 10% in
generating scenes that fully align with fine-grained instructions.</p>
<h3 id="16-mme-cc-a-challenging-multi-modal-evaluation-benchmark-of-cognitive-capacity">[16] <a href="https://arxiv.org/abs/2511.03146">MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity</a></h3>
<p><em>Kaiyuan Zhang, Chenghao Yang, Zhoufutu Wen, Sihang Yuan, Qiuyue Wang, Chaoyi Huang, Guosheng Zhu, He Wang, Huawenyu Lu, Jianing Wen, Jianpeng Jiao, Lishu Luo, Longxiang Liu, Sijin Wu, Xiaolei Zhu, Xuanliang Zhang, Ge Zhang, Yi Lin, Guang Shi, Chaoyou Fu, Wenhao Huang</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†MME-CCå¤šæ¨¡æ€è®¤çŸ¥èƒ½åŠ›è¯„ä¼°åŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ä¸­å¿ƒè®¤çŸ¥è¡Œä¸ºä¸Šçš„è¡¨ç°ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨è§†è§‰æ¨ç†èƒ½åŠ›è¯„ä¼°æ–¹é¢çš„ä¸è¶³ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤šæ¨¡æ€åŸºå‡†è¿‡åº¦å¼ºè°ƒæ–‡æœ¬æ¨ç†ï¼Œæœªèƒ½ç³»ç»Ÿæ•æ‰è§†è§‰ä¸­å¿ƒè®¤çŸ¥è¡Œä¸ºï¼Œå¯¼è‡´å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›è¯„ä¼°ä¸è¶³ï¼Œéœ€è¦å¼€å‘ä¸“é—¨é’ˆå¯¹è§†è§‰ä¿¡æ¯å¤„ç†çš„è¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†MME-CCåŸºå‡†ï¼Œå°†11ä¸ªä»£è¡¨æ€§æ¨ç†ä»»åŠ¡ç»„ç»‡ä¸ºç©ºé—´æ¨ç†ã€å‡ ä½•æ¨ç†å’ŒçŸ¥è¯†æ¨ç†ä¸‰ä¸ªåŸºæœ¬ç±»åˆ«ï¼Œå¯¹16ä¸ªä»£è¡¨æ€§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> é—­æºæ¨¡å‹è¡¨ç°é¢†å…ˆï¼ˆå¦‚Gemini-2.5-Proå¾—åˆ†ä¸º42.66ï¼ŒGLM-4.5Vä¸º30.45ï¼‰ï¼Œç©ºé—´å’Œå‡ ä½•æ¨ç†èƒ½åŠ›æ™®éè¾ƒå¼±ï¼ˆâ‰¤30%ï¼‰ï¼Œè¯†åˆ«å‡ºæ–¹å‘é”™è¯¯ã€è·¨è§†å›¾èº«ä»½æŒç»­æ€§è„†å¼±ã€åäº‹å®æŒ‡ä»¤éµå¾ªå·®ç­‰å¸¸è§é”™è¯¯æ¨¡å¼ã€‚</p>
<p><strong>Conclusion:</strong> æ€ç»´é“¾é€šå¸¸éµå¾ªæå–-æ¨ç†-éªŒè¯çš„ä¸‰é˜¶æ®µè¿‡ç¨‹ä¸”ä¸¥é‡ä¾èµ–è§†è§‰æå–ï¼Œç ”ç©¶å‘¼åå°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ä½œä¸ºè¯„ä¼°å’Œæ¨¡å‹è®¾è®¡çš„æ ¸å¿ƒè€ƒé‡ï¼Œæ¨åŠ¨è®¤çŸ¥èƒ½åŠ›å¯¼å‘çš„æ¨¡å‹å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>As reasoning models scale rapidly, the essential role of multimodality in
human cognition has come into sharp relief, driving a growing need to probe
vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either
overemphasize textual reasoning or fall short of systematically capturing
vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs
insufficiently assessed. To address this limitation, we introduce MME-CC
(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded
benchmark that organizes 11 representative reasoning tasks into three
fundamental categories of visual information: spatial, geometric, and
knowledge-based reasoning, and provides fine-grained analyses of MLLMs'
cognitive capacity across these dimensions. Based on MME-CC, we conduct
extensive experiments over 16 representative MLLMs. Our study reveals that
closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.
30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak
(less than or equal to 30%). We further identify common error patterns,
including orientation mistakes, fragile cross-view identity persistence, and
poor adherence to counterfactual instructions, and observe that
Chain-of-Thought typically follows a three-stage process (extract -&gt; reason -&gt;
verify) with heavy reliance on visual extraction. We hope this work catalyzes a
shift toward treating the cognitive capacity of MLLMs as central to both
evaluation and model design.</p>
<h3 id="17-bengalimoralbench-a-benchmark-for-auditing-moral-reasoning-in-large-language-models-within-bengali-language-and-culture">[17] <a href="https://arxiv.org/abs/2511.03180">BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture</a></h3>
<p><em>Shahriyar Zaman Ridoy, Azmine Toushik Wasi, Koushik Ahamed Tonmoy</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†BengaliMoralBenchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å­ŸåŠ æ‹‰è¯­çš„å¤§è§„æ¨¡ä¼¦ç†åŸºå‡†ï¼Œå¡«è¡¥äº†å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡åŒ–ä¼¦ç†å¯¹é½æ–¹é¢çš„ç ”ç©¶ç©ºç™½ï¼Œä¸ºå—äºšåœ°åŒºçš„è´Ÿè´£ä»»AIéƒ¨ç½²æä¾›äº†è¯„ä¼°åŸºç¡€ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹åœ¨å—äºšåœ°åŒºçš„æ™®åŠï¼Œè¿™äº›æ¨¡å‹ä¸å½“åœ°ä¼¦ç†è§„èŒƒçš„å¥‘åˆåº¦ç ”ç©¶ä»ç„¶ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…¨çƒä½¿ç”¨äººæ•°æ’åç¬¬å…­çš„å­ŸåŠ æ‹‰è¯­ã€‚ç°æœ‰çš„ä¼¦ç†åŸºå‡†ä¸»è¦åŸºäºè‹±è¯­å’Œè¥¿æ–¹æ¡†æ¶ï¼Œå¿½è§†äº†æ–‡åŒ–ç»†å¾®å·®åˆ«å¯¹å®é™…éƒ¨ç½²çš„å…³é”®å½±å“ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†æ¶µç›–äº”ä¸ªé“å¾·é¢†åŸŸçš„å¤§è§„æ¨¡ä¼¦ç†åŸºå‡†ï¼ŒåŒ…æ‹¬æ—¥å¸¸æ´»åŠ¨ã€ä¹ æƒ¯ã€è‚²å„¿ã€å®¶åº­å…³ç³»å’Œå®—æ•™æ´»åŠ¨ï¼Œç»†åˆ†ä¸º50ä¸ªæ–‡åŒ–ç›¸å…³å­ä¸»é¢˜ã€‚æ¯ä¸ªåœºæ™¯é€šè¿‡æ¯è¯­è€…å…±è¯†è¿›è¡Œæ ‡æ³¨ï¼Œé‡‡ç”¨ç¾å¾·ä¼¦ç†ã€å¸¸è¯†ä¼¦ç†å’Œæ­£ä¹‰ä¼¦ç†ä¸‰ç§ä¼¦ç†è§†è§’ï¼Œå¹¶å¯¹ä¸»æµå¤šè¯­è¨€LLMè¿›è¡Œç³»ç»Ÿæ€§é›¶æ ·æœ¬è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> ä¸åŒæ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œå‡†ç¡®ç‡èŒƒå›´ä¸º50-91%ã€‚å®šæ€§åˆ†ææ­ç¤ºäº†æ¨¡å‹åœ¨æ–‡åŒ–åŸºç¡€ã€å¸¸è¯†æ¨ç†å’Œé“å¾·å…¬å¹³æ€§æ–¹é¢å­˜åœ¨ä¸€è‡´çš„å¼±ç‚¹ï¼Œè¡¨æ˜å½“å‰æ¨¡å‹å¯¹å­ŸåŠ æ‹‰æ–‡åŒ–èƒŒæ™¯çš„ç†è§£ä»æœ‰æ˜æ˜¾ä¸è¶³ã€‚</p>
<p><strong>Conclusion:</strong> BengaliMoralBenchä¸ºè´Ÿè´£ä»»çš„æœ¬åœŸåŒ–æä¾›äº†åŸºç¡€æ¡†æ¶ï¼Œæ”¯æŒåœ¨å¤šæ ·åŒ–ã€ä½èµ„æºå¤šè¯­è¨€ç¯å¢ƒä¸­è¿›è¡Œæ–‡åŒ–å¯¹é½è¯„ä¼°ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘æ–‡åŒ–æ•æ„Ÿçš„ä¼¦ç†åŸºå‡†å¯¹äºåœ¨éè¥¿æ–¹ç¯å¢ƒä¸­éƒ¨ç½²ç¨³å¥AIç³»ç»Ÿçš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥å¤šè¯­è¨€ä¼¦ç†å¯¹é½ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>As multilingual Large Language Models (LLMs) gain traction across South Asia,
their alignment with local ethical norms, particularly for Bengali, which is
spoken by over 285 million people and ranked 6th globally, remains
underexplored. Existing ethics benchmarks are largely English-centric and
shaped by Western frameworks, overlooking cultural nuances critical for
real-world deployment. To address this, we introduce BengaliMoralBench, the
first large-scale ethics benchmark for the Bengali language and socio-cultural
contexts. It covers five moral domains, Daily Activities, Habits, Parenting,
Family Relationships, and Religious Activities, subdivided into 50 culturally
relevant subtopics. Each scenario is annotated via native-speaker consensus
using three ethical lenses: Virtue, Commonsense, and Justice ethics. We conduct
systematic zero-shot evaluation of prominent multilingual LLMs, including
Llama, Gemma, Qwen, and DeepSeek, using a unified prompting protocol and
standard metrics. Performance varies widely (50-91% accuracy), with qualitative
analysis revealing consistent weaknesses in cultural grounding, commonsense
reasoning, and moral fairness. BengaliMoralBench provides a foundation for
responsible localization, enabling culturally aligned evaluation and supporting
the deployment of ethically robust AI in diverse, low-resource multilingual
settings such as Bangladesh.</p>
<h3 id="18-benchmarking-the-thinking-mode-of-multimodal-large-language-models-in-clinical-tasks">[18] <a href="https://arxiv.org/abs/2511.03328">Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks</a></h3>
<p><em>Jindong Hong, Tianjie Chen, Lingjie Luo, Chuanyang Zheng, Ting Xu, Haibao Yu, Jianing Qiu, Qianzhong Chen, Suning Huang, Yan Xu, Yong Gui, Yijun He, Jiankai Sun</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠä»»åŠ¡ä¸­æ¿€æ´»æ€ç»´æ¨¡å¼å¯¹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°æ€ç»´æ¨¡å¼ç›¸æ¯”æ ‡å‡†æ¨¡å¼ä»…å¸¦æ¥è¾¹é™…æ”¹è¿›ï¼Œåœ¨å¤æ‚åŒ»ç–—ä»»åŠ¡ä¸­è¡¨ç°ä»ä¸ç†æƒ³ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€å…·å¤‡'åŒçŠ¶æ€'èƒ½åŠ›çš„æ¨ç†å‹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¿«é€Ÿå‘å±•ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨ä¸¥æ ¼è¯„ä¼°è¿™äº›æ¨¡å‹å¢å¼ºçš„æ¨ç†è¿‡ç¨‹å¦‚ä½•å½±å“å…¶åœ¨ä¸´åºŠä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œç‰¹åˆ«å…³æ³¨æ€ç»´æ¨¡å¼æ¿€æ´»å¯¹åŒ»ç–—åº”ç”¨çš„å®è´¨æ€§æå‡ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶è¯„ä¼°äº†Seed1.5-VLå’ŒGemini-2.5-Flashä¸¤ä¸ªé¢†å…ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä¸»åŠ¨æ€ç»´æ¨¡å¼èƒ½åŠ›ï¼Œåœ¨è§†è§‰åŒ»ç–—ä»»åŠ¡ä¸­ä½¿ç”¨VQA-RADå’ŒROCOv2æ•°æ®é›†è¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°ï¼Œæ¶µç›–å››ä¸ªä¸åŒçš„è§†è§‰åŒ»ç–—ä»»åŠ¡ç±»å‹ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå¯¹äºå¤§å¤šæ•°ä»»åŠ¡è€Œè¨€ï¼Œæ¿€æ´»æ€ç»´æ¨¡å¼ç›¸æ¯”æ ‡å‡†éæ€ç»´æ¨¡å¼ä»…å¸¦æ¥è¾¹é™…æ€§èƒ½æ”¹è¿›ï¼Œåœ¨å¼€æ”¾å¼è§†è§‰é—®ç­”å’ŒåŒ»å­¦å›¾åƒè§£é‡Šç­‰å¤æ‚åŒ»ç–—ä»»åŠ¡ä¸­è¡¨ç°ä»ç„¶æ¬ ä½³ï¼Œæœªèƒ½è¾¾åˆ°ç†æƒ³æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸéœ€è¦é¢†åŸŸç‰¹å®šçš„åŒ»å­¦æ•°æ®å’Œæ›´å…ˆè¿›çš„åŒ»å­¦çŸ¥è¯†é›†æˆæ–¹æ³•ï¼Œå½“å‰æ€ç»´æ¨¡å¼åœ¨å¤æ‚ä¸´åºŠæ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§æœ‰é™ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–å’Œæ”¹è¿›ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>A recent advancement in Multimodal Large Language Models (MLLMs) research is
the emergence of "reasoning MLLMs" that offer explicit control over their
internal thinking processes (normally referred as the "thinking mode")
alongside the standard "non-thinking mode". This capability allows these models
to engage in a step-by-step process of internal deliberation before generating
a final response. With the rapid transition to and adoption of these
"dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning
processes of these MLLMs impact model performance and reliability in clinical
tasks. This paper evaluates the active "thinking mode" capabilities of two
leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We
assessed their performance on four visual medical tasks using VQA-RAD and
ROCOv2 datasets. Our findings reveal that the improvement from activating the
thinking mode remains marginal compared to the standard non-thinking mode for
the majority of the tasks. Their performance on complex medical tasks such as
open-ended VQA and medical image interpretation remains suboptimal,
highlighting the need for domain-specific medical data and more advanced
methods for medical knowledge integration.</p>
<h3 id="19-step-audio-editx-technical-report">[19] <a href="https://arxiv.org/abs/2511.03601">Step-Audio-EditX Technical Report</a></h3>
<p><em>Chao Yan, Boyong Wu, Peng Yang, Pengfei Tan, Guoqiang Hu, Yuxin Zhang, Xiangyu, Zhang, Fei Tian, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, Gang Yu</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>Step-Audio-EditXæ˜¯é¦–ä¸ªåŸºäºLLMçš„å¼€æºéŸ³é¢‘æ¨¡å‹ï¼Œåœ¨è¡¨è¾¾æ€§éŸ³é¢‘ç¼–è¾‘å’Œé›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³æ–¹é¢è¡¨ç°å“è¶Šï¼Œé€šè¿‡å¤§è¾¹ç•Œåˆæˆæ•°æ®æ–¹æ³•å®ç°äº†æƒ…æ„Ÿã€è¯´è¯é£æ ¼å’Œå‰¯è¯­è¨€ç‰¹å¾çš„è¿­ä»£æ§åˆ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»ŸéŸ³é¢‘ç¼–è¾‘æ¨¡å‹åœ¨è¡¨è¾¾æ€§æ§åˆ¶å’Œè¿­ä»£ç¼–è¾‘æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯æƒ…æ„Ÿã€è¯´è¯é£æ ¼å’Œå‰¯è¯­è¨€ç‰¹å¾ç­‰ç»†ç²’åº¦æ§åˆ¶èƒ½åŠ›çš„ä¸è¶³ï¼Œä»¥åŠä¼ ç»Ÿæ–¹æ³•å¯¹åµŒå…¥å…ˆéªŒæˆ–è¾…åŠ©æ¨¡å—çš„ä¾èµ–é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æ ¸å¿ƒåˆ›æ–°åœ¨äºé‡‡ç”¨ä»…ä½¿ç”¨å¤§è¾¹ç•Œåˆæˆæ•°æ®çš„æ–¹æ³•ï¼Œé¿å…äº†åŸºäºåµŒå…¥çš„å…ˆéªŒæˆ–è¾…åŠ©æ¨¡å—çš„éœ€æ±‚ï¼Œé€šè¿‡å¤§è¾¹ç•Œå­¦ä¹ å®ç°è·¨å£°éŸ³çš„è¿­ä»£æ§åˆ¶å’Œé«˜è¡¨è¾¾æ€§ï¼Œä»£è¡¨äº†ä»ä¼ ç»Ÿè¡¨ç¤ºçº§è§£çº ç¼ æ–¹æ³•çš„æ ¹æœ¬æ€§è½¬å˜ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒStep-Audio-EditXåœ¨æƒ…æ„Ÿç¼–è¾‘å’Œå…¶ä»–ç»†ç²’åº¦æ§åˆ¶ä»»åŠ¡ä¸­è¶…è¶Šäº†MiniMax-2.6-hdå’ŒDoubao-Seed-TTS-2.0ï¼Œè¯æ˜äº†å…¶åœ¨è¡¨è¾¾æ€§éŸ³é¢‘ç¼–è¾‘æ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†ä»…ä½¿ç”¨åˆæˆæ•°æ®å®ç°é«˜è´¨é‡éŸ³é¢‘ç¼–è¾‘çš„å¯è¡Œæ€§ï¼Œä¸ºéŸ³é¢‘ç”Ÿæˆé¢†åŸŸæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œè¡¨æ˜å¤§è¾¹ç•Œå­¦ä¹ æ–¹æ³•å¯ä»¥æ›¿ä»£ä¼ ç»Ÿçš„è¡¨ç¤ºçº§è§£çº ç¼ æ–¹æ³•ï¼Œä¸ºæœªæ¥éŸ³é¢‘ç¼–è¾‘ç³»ç»Ÿçš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>We present Step-Audio-EditX, the first open-source LLM-based audio model
excelling at expressive and iterative audio editing encompassing emotion,
speaking style, and paralinguistics alongside robust zero-shot text-to-speech
(TTS) capabilities.Our core innovation lies in leveraging only large-margin
synthetic data, which circumvents the need for embedding-based priors or
auxiliary modules. This large-margin learning approach enables both iterative
control and high expressivity across voices, and represents a fundamental pivot
from the conventional focus on representation-level disentanglement. Evaluation
results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and
Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.</p>
<h3 id="20-towards-transparent-stance-detection-a-zero-shot-approach-using-implicit-and-explicit-interpretability">[20] <a href="https://arxiv.org/abs/2511.03635">Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability</a></h3>
<p><em>Apoorva Upadhyaya, Wolfgang Nejdl, Marco Fisichella</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯è§£é‡Šé›¶æ ·æœ¬ç«‹åœºæ£€æµ‹æ¡†æ¶IRISï¼Œé€šè¿‡ç»“åˆéšå¼æ¨ç†ä¾æ®å’Œæ˜¾å¼è¯­è¨€å­¦ç‰¹å¾ï¼Œåœ¨æ— éœ€çœŸå®æ¨ç†ä¾æ®æ ‡æ³¨çš„æƒ…å†µä¸‹å®ç°ç«‹åœºæ£€æµ‹ï¼ŒåŒæ—¶æä¾›å†…åœ¨çš„å¯è§£é‡Šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é›¶æ ·æœ¬ç«‹åœºæ£€æµ‹æ–¹æ³•å­˜åœ¨æ³›åŒ–æ€§ä¸è¶³ã€æ–‡æœ¬ä¸ç›®æ ‡ä¹‹é—´ç¼ºä¹è¿è´¯æ€§ç­‰é—®é¢˜ï¼Œä¸”å¤§å¤šæ•°åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•è¿‡åº¦ä¾èµ–æ˜¾å¼æ¨ç†ã€æä¾›çš„è§£é‡Šç¼ºä¹ç»†å¾®å·®åˆ«ã€æœªèƒ½æ˜¾å¼å»ºæ¨¡æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´æ¨¡å‹é¢„æµ‹éš¾ä»¥è§£é‡Šã€‚</p>
<p><strong>Method:</strong> IRISæ¡†æ¶å°†ç«‹åœºæ£€æµ‹è§†ä¸ºä¿¡æ¯æ£€ç´¢æ’åºä»»åŠ¡ï¼ŒåŸºäºæ–‡æœ¬ä¸­çš„åºåˆ—ï¼ˆéšå¼æ¨ç†ä¾æ®ï¼‰å’Œè¯­è¨€å­¦åº¦é‡ï¼ˆæ˜¾å¼æ¨ç†ä¾æ®ï¼‰åˆ†åˆ«æä¾›éšå¼å’Œæ˜¾å¼çš„ç«‹åœºç†è§£ï¼Œé€šè¿‡ç†è§£ä¸åŒç«‹åœºéšå¼æ¨ç†ä¾æ®çš„ç›¸å…³æ€§æ¥æŒ‡å¯¼æ¨¡å‹é¢„æµ‹ï¼Œæ— éœ€çœŸå®æ¨ç†ä¾æ®æ ‡æ³¨å³å¯æä¾›å†…åœ¨å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨VASTã€EZ-STANCEã€P-Stanceå’ŒRFDç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œå³ä½¿ä»…ä½¿ç”¨50%ã€30%ç”šè‡³10%çš„è®­ç»ƒæ•°æ®ï¼Œè¯¥æ¨¡å‹ä»è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™å¾—ç›Šäºæ‰€æå‡ºçš„æ¶æ„å’Œå¯è§£é‡Šè®¾è®¡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ç»“åˆéšå¼å’Œæ˜¾å¼æ¨ç†ä¾æ®ï¼Œä¸ä»…æé«˜äº†é›¶æ ·æœ¬ç«‹åœºæ£€æµ‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè¿˜æä¾›äº†å¯¹ä½œè€…æ€åº¦æƒ…æ„Ÿå’Œè®¤çŸ¥ç»´åº¦çš„å¯è§£é‡Šç†è§£ï¼Œä¸ºå¯è§£é‡ŠAIåœ¨ç«‹åœºæ£€æµ‹é¢†åŸŸçš„åº”ç”¨å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward
unseen targets. Existing research using contrastive, meta-learning, or data
augmentation suffers from generalizability issues or lack of coherence between
text and target. Recent works leveraging large language models (LLMs) for ZSSD
focus either on improving unseen target-specific knowledge or generating
explanations for stance analysis. However, most of these works are limited by
their over-reliance on explicit reasoning, provide coarse explanations that
lack nuance, and do not explicitly model the reasoning process, making it
difficult to interpret the model's predictions. To address these issues, in our
study, we develop a novel interpretable ZSSD framework, IRIS. We provide an
interpretable understanding of the attitude of the input towards the target
implicitly based on sequences within the text (implicit rationales) and
explicitly based on linguistic measures (explicit rationales). IRIS considers
stance detection as an information retrieval ranking task, understanding the
relevance of implicit rationales for different stances to guide the model
towards correct predictions without requiring the ground-truth of rationales,
thus providing inherent interpretability. In addition, explicit rationales
based on communicative features help decode the emotional and cognitive
dimensions of stance, offering an interpretable understanding of the author's
attitude towards the given target. Extensive experiments on the benchmark
datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%
training data prove the generalizability of our model, benefiting from the
proposed architecture and interpretable design.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="21-using-multi-modal-large-language-model-to-boost-fireworks-algorithms-ability-in-settling-challenging-optimization-tasks">[21] <a href="https://arxiv.org/abs/2511.03137">Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks</a></h3>
<p><em>Shipeng Cen, Ying Tan</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©çš„çƒŸèŠ±ç®—æ³•ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å…³é”®éƒ¨ä»¶æ¦‚å¿µæ‰©å±•çƒŸèŠ±ç®—æ³•å¤„ç†å¤æ‚é«˜ç»´ä¼˜åŒ–ä»»åŠ¡çš„èƒ½åŠ›ï¼Œåœ¨æ—…è¡Œå•†é—®é¢˜å’Œç”µå­è®¾è®¡è‡ªåŠ¨åŒ–é—®é¢˜ä¸Šå–å¾—äº†ä¼˜äºæˆ–è¾¾åˆ°å½“å‰æœ€ä¼˜æ°´å¹³çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿé›¶é˜¶æˆ–ä¸€é˜¶ä¼˜åŒ–æ–¹æ³•åœ¨å¤„ç†éå‡¸ã€é«˜ç»´ã€é»‘ç®±ç­‰å¤æ‚ä¼˜åŒ–é—®é¢˜æ—¶å­˜åœ¨æ•ˆç‡ä½ä¸‹ã€æ¢¯åº¦ä¿¡æ¯ä¸å‡†ç¡®å’Œä¼˜åŒ–ä¿¡æ¯åˆ©ç”¨ä¸è¶³ç­‰å±€é™æ€§ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹ç°ä»£ä¼˜åŒ–é—®é¢˜çš„æŒ‘æˆ˜ï¼Œè€Œå¤§è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€ç†è§£å’Œä»£ç ç”Ÿæˆèƒ½åŠ›ä¸Šçš„æ˜¾è‘—æå‡ä¸ºä¼˜åŒ–ç®—æ³•è®¾è®¡æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶ä»¥çƒŸèŠ±ç®—æ³•ä¸ºåŸºç¡€ä¼˜åŒ–å™¨ï¼Œç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æå‡ºå…³é”®éƒ¨ä»¶æ¦‚å¿µï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€ç‰¹æ€§å……åˆ†æŒ–æ˜ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ï¼Œå°†çƒŸèŠ±ç®—æ³•æ‰©å±•åˆ°å¤æ‚é«˜ç»´ä»»åŠ¡ä¸­ï¼Œç‰¹åˆ«é’ˆå¯¹æ—…è¡Œå•†é—®é¢˜å’Œç”µå­è®¾è®¡è‡ªåŠ¨åŒ–é—®é¢˜è¿›è¡Œäº†ç®—æ³•è®¾è®¡ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ–°æ¡†æ¶ä¸‹ç”Ÿæˆçš„çƒŸèŠ±ç®—æ³•åœ¨å¤šä¸ªé—®é¢˜å®ä¾‹ä¸Šå–å¾—äº†ä¼˜äºæˆ–è¾¾åˆ°å½“å‰æœ€ä¼˜æ°´å¹³çš„æ€§èƒ½ï¼Œè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•åœ¨å¤æ‚ä¼˜åŒ–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¼˜åŒ–ç®—æ³•è®¾è®¡ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºå¤„ç†å¤æ‚é«˜ç»´ä¼˜åŒ–é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶è¯æ˜äº†ç»“åˆä¼ ç»Ÿä¼˜åŒ–ç®—æ³•ä¸å…ˆè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯èƒ½å¤Ÿäº§ç”ŸååŒæ•ˆåº”ï¼Œä¸ºæœªæ¥ä¼˜åŒ–ç®—æ³•ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>As optimization problems grow increasingly complex and diverse, advancements
in optimization techniques and paradigm innovations hold significant
importance. The challenges posed by optimization problems are primarily
manifested in their non-convexity, high-dimensionality, black-box nature, and
other unfavorable characteristics. Traditional zero-order or first-order
methods, which are often characterized by low efficiency, inaccurate gradient
information, and insufficient utilization of optimization information, are
ill-equipped to address these challenges effectively. In recent years, the
rapid development of large language models (LLM) has led to substantial
improvements in their language understanding and code generation capabilities.
Consequently, the design of optimization algorithms leveraging large language
models has garnered increasing attention from researchers. In this study, we
choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel
approach to assist the design of the FWA by incorporating multi-modal large
language model(MLLM). To put it simply, we propose the concept of Critical
Part(CP), which extends FWA to complex high-dimensional tasks, and further
utilizes the information in the optimization process with the help of the
multi-modal characteristics of large language models. We focus on two specific
tasks: the \textit{traveling salesman problem }(TSP) and \textit{electronic
design automation problem} (EDA). The experimental results show that FWAs
generated under our new framework have achieved or surpassed SOTA results on
many problem instances.</p>
<h3 id="22-from-five-dimensions-to-many-large-language-models-as-precise-and-interpretable-psychological-profilers">[22] <a href="https://arxiv.org/abs/2511.03235">From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers</a></h3>
<p><em>Yi-Fei Liu, Yi-Long Lu, Di He, Hang Zhang</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶è¯æ˜å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿä»å°‘é‡äººæ ¼é‡è¡¨è¾“å…¥ä¸­å‡†ç¡®å»ºæ¨¡äººç±»å¿ƒç†ç‰¹è´¨çš„å…³è”ç»“æ„ï¼Œé€šè¿‡æŠ½è±¡å’Œæ¨ç†è¿‡ç¨‹å®ç°é›¶æ ·æœ¬å¿ƒç†æ¨¡æ‹Ÿï¼Œå…¶æ€§èƒ½æ¥è¿‘åœ¨æ•°æ®é›†ä¸Šç›´æ¥è®­ç»ƒçš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿä»æœ€å°åŒ–çš„å®šé‡è¾“å…¥ä¸­å»ºæ¨¡äººç±»å¿ƒç†ç‰¹è´¨çš„å…³è”ç»“æ„ï¼Œè§£å†³ä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®å’Œä¸“ä¸šçŸ¥è¯†çš„å±€é™æ€§ï¼ŒåŒæ—¶æ­ç¤ºLLMsåœ¨å¿ƒç†æ¨¡æ‹Ÿæ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨é›¶æ ·æœ¬æç¤ºæ–¹æ³•ï¼Œè®©å¤šç§LLMsåŸºäº816åäººç±»ä¸ªä½“çš„äº”å¤§æ€§æ ¼é‡è¡¨å“åº”ï¼Œåœ¨ä¹ä¸ªå…¶ä»–å¿ƒç†é‡è¡¨ä¸Šè¿›è¡Œè§’è‰²æ‰®æ¼”å“åº”ç”Ÿæˆï¼Œå¹¶é€šè¿‡åˆ†ææ¨ç†è½¨è¿¹æ­ç¤ºLLMsä½¿ç”¨çš„ä¸¤é˜¶æ®µå¤„ç†è¿‡ç¨‹ï¼šä¿¡æ¯é€‰æ‹©å‹ç¼©å’ŒåŸºäºæ‘˜è¦çš„æ¨ç†ã€‚</p>
<p><strong>Result:</strong> LLMsåœ¨æ•æ‰äººç±»å¿ƒç†ç»“æ„æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç”Ÿæˆå“åº”ä¸äººç±»æ•°æ®é—´çš„é‡è¡¨é—´ç›¸å…³æ€§æ¨¡å¼é«˜åº¦ä¸€è‡´ï¼ˆRÂ² &gt; 0.89ï¼‰ï¼Œé›¶æ ·æœ¬æ€§èƒ½æ˜¾è‘—è¶…è¿‡åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„é¢„æµ‹ï¼Œæ¥è¿‘ç›´æ¥åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒçš„æœºå™¨å­¦ä¹ ç®—æ³•ç²¾åº¦ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å‘ç°LLMsé€šè¿‡æŠ½è±¡å’Œæ¨ç†è¿‡ç¨‹èƒ½å¤Ÿç²¾ç¡®é¢„æµ‹ä¸ªä½“å¿ƒç†ç‰¹è´¨ï¼Œå…¶ç”Ÿæˆçš„å‹ç¼©æ‘˜è¦ä¸ä»…æ•è·äº†ååŒä¿¡æ¯ï¼Œè¿˜ç¼–ç äº†ç‰¹è´¨äº’åŠ¨çš„æ¶Œç°äºŒé˜¶æ¨¡å¼ï¼Œä¸ºå¿ƒç†æ¨¡æ‹Ÿæä¾›äº†å¼ºå¤§å·¥å…·ï¼ŒåŒæ—¶æ­ç¤ºäº†LLMsçš„æ¶Œç°æ¨ç†èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Psychological constructs within individuals are widely believed to be
interconnected. We investigated whether and how Large Language Models (LLMs)
can model the correlational structure of human psychological traits from
minimal quantitative inputs. We prompted various LLMs with Big Five Personality
Scale responses from 816 human individuals to role-play their responses on nine
other psychological scales. LLMs demonstrated remarkable accuracy in capturing
human psychological structure, with the inter-scale correlation patterns from
LLM-generated responses strongly aligning with those from human data $(R^2 &gt;
0.89)$. This zero-shot performance substantially exceeded predictions based on
semantic similarity and approached the accuracy of machine learning algorithms
trained directly on the dataset. Analysis of reasoning traces revealed that
LLMs use a systematic two-stage process: First, they transform raw Big Five
responses into natural language personality summaries through information
selection and compression, analogous to generating sufficient statistics.
Second, they generate target scale responses based on reasoning from these
summaries. For information selection, LLMs identify the same key personality
factors as trained algorithms, though they fail to differentiate item
importance within factors. The resulting compressed summaries are not merely
redundant representations but capture synergistic information--adding them to
original scores enhances prediction alignment, suggesting they encode emergent,
second-order patterns of trait interplay. Our findings demonstrate that LLMs
can precisely predict individual participants' psychological traits from
minimal data through a process of abstraction and reasoning, offering both a
powerful tool for psychological simulation and valuable insights into their
emergent reasoning capabilities.</p>
<h3 id="23-towards-scalable-web-accessibility-audit-with-mllms-as-copilots">[23] <a href="https://arxiv.org/abs/2511.03471">Towards Scalable Web Accessibility Audit with MLLMs as Copilots</a></h3>
<p><em>Ming Gu, Ziwei Wang, Sicen Lai, Zirui Gao, Sheng Zhou, Jiajun Bu</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†AAAæ¡†æ¶ï¼Œé€šè¿‡äººæœºåä½œæ¨¡å¼å®ç°å¯æ‰©å±•çš„Webå¯è®¿é—®æ€§å®¡è®¡ï¼Œæ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬åŸºäºå›¾çš„å¤šæ¨¡æ€é‡‡æ ·æ–¹æ³•GRASPå’ŒåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åŠ©æ‰‹MaCï¼Œèƒ½å¤Ÿç«¯åˆ°ç«¯åœ°æ”¯æŒå¤§è§„æ¨¡ç½‘ç«™å¯è®¿é—®æ€§è¯„ä¼°ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç½‘ç«™ç”¨æˆ·ç•Œé¢çš„å¯è®¿é—®æ€§åˆè§„æ€§æ™®éä¸è¶³ï¼Œä¸»è¦ç”±äºç°æœ‰å®¡è®¡æ–¹æ³•èµ„æºå¯†é›†ä¸”éš¾ä»¥æ‰©å±•ï¼ŒWCAG-EMæ ‡å‡†è™½ç„¶æä¾›äº†ç»“æ„åŒ–è¯„ä¼°æ–¹æ³•ï¼Œä½†éœ€è¦å¤§é‡äººå·¥æŠ•å…¥ä¸”ç¼ºä¹è§„æ¨¡åŒ–æ‰§è¡Œçš„å®é™…æ”¯æŒã€‚</p>
<p><strong>Method:</strong> AAAæ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šGRASPåŸºäºå›¾çš„å¤šæ¨¡æ€é‡‡æ ·æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ è§†è§‰ã€æ–‡æœ¬å’Œå…³ç³»çº¿ç´¢çš„åµŒå…¥è¡¨ç¤ºç¡®ä¿ä»£è¡¨æ€§é¡µé¢è¦†ç›–ï¼›MaCåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¾…åŠ©ç³»ç»Ÿï¼Œé€šè¿‡è·¨æ¨¡æ€æ¨ç†ä¸ºå®¡è®¡å‘˜æä¾›æ™ºèƒ½ååŠ©ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºå®¡è®¡æµç¨‹çš„æ ¸å¿ƒé˜¶æ®µè´¡çŒ®äº†å››ä¸ªæ–°é¢–çš„æ•°æ®é›†ç”¨äºåŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶è¿˜å‘ç°ç»è¿‡å¾®è°ƒçš„å°å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿèƒœä»»ä¸“å®¶è§’è‰²ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†äººæœºåä½œæ¨¡å¼åœ¨Webå¯è®¿é—®æ€§å®¡è®¡ä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºå¤§è§„æ¨¡å¯è®¿é—®æ€§è¯„ä¼°æä¾›äº†ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æ­ç¤ºäº†å°å‹è¯­è¨€æ¨¡å‹åœ¨ä¸“é—¨ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Ensuring web accessibility is crucial for advancing social welfare, justice,
and equality in digital spaces, yet the vast majority of website user
interfaces remain non-compliant, due in part to the resource-intensive and
unscalable nature of current auditing practices. While WCAG-EM offers a
structured methodology for site-wise conformance evaluation, it involves great
human efforts and lacks practical support for execution at scale. In this work,
we present an auditing framework, AAA, which operationalizes WCAG-EM through a
human-AI partnership model. AAA is anchored by two key innovations: GRASP, a
graph-based multimodal sampling method that ensures representative page
coverage via learned embeddings of visual, textual, and relational cues; and
MaC, a multimodal large language model-based copilot that supports auditors
through cross-modal reasoning and intelligent assistance in high-effort tasks.
Together, these components enable scalable, end-to-end web accessibility
auditing, empowering human auditors with AI-enhanced assistance for real-world
impact. We further contribute four novel datasets designed for benchmarking
core stages of the audit pipeline. Extensive experiments demonstrate the
effectiveness of our methods, providing insights that small-scale language
models can serve as capable experts when fine-tuned.</p>
  </article>
</body>
</html>
