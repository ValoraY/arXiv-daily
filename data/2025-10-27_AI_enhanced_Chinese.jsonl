{"id": "2510.21049", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21049", "abs": "https://arxiv.org/abs/2510.21049", "authors": ["Atoosa Chegini", "Hamid Kazemi", "Garrett Souza", "Maria Safi", "Yang Song", "Samy Bengio", "Sinead Williamson", "Mehrdad Farajtabar"], "title": "Reasoning's Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical Operating Points in Safety and Hallucination Detection", "comment": null, "summary": "Reasoning has become a central paradigm for large language models (LLMs),\nconsistently boosting accuracy across diverse benchmarks. Yet its suitability\nfor precision-sensitive tasks remains unclear. We present the first systematic\nstudy of reasoning for classification tasks under strict low false positive\nrate (FPR) regimes. Our analysis covers two tasks--safety detection and\nhallucination detection--evaluated in both fine-tuned and zero-shot settings,\nusing standard LLMs and Large Reasoning Models (LRMs). Our results reveal a\nclear trade-off: Think On (reasoning-augmented) generation improves overall\naccuracy, but underperforms at the low-FPR thresholds essential for practical\nuse. In contrast, Think Off (no reasoning during inference) dominates in these\nprecision-sensitive regimes, with Think On surpassing only when higher FPRs are\nacceptable. In addition, we find token-based scoring substantially outperforms\nself-verbalized confidence for precision-sensitive deployments. Finally, a\nsimple ensemble of the two modes recovers the strengths of each. Taken\ntogether, our findings position reasoning as a double-edged tool: beneficial\nfor average accuracy, but often ill-suited for applications requiring strict\nprecision.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u5728\u4f4e\u8bef\u62a5\u7387\u8981\u6c42\u4e0b\u7684\u63a8\u7406\u589e\u5f3a\u5206\u7c7b\u4efb\u52a1\uff0c\u53d1\u73b0\u63a8\u7406\u867d\u7136\u63d0\u5347\u6574\u4f53\u51c6\u786e\u7387\uff0c\u4f46\u5728\u4e25\u683c\u7cbe\u5ea6\u654f\u611f\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u5728\u7cbe\u5ea6\u654f\u611f\u5e94\u7528\u4e2d\u7684\u53cc\u5203\u5251\u7279\u6027\u3002", "motivation": "\u5c3d\u7ba1\u63a8\u7406\u5df2\u6210\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u8303\u5f0f\u5e76\u663e\u8457\u63d0\u5347\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u7684\u51c6\u786e\u7387\uff0c\u4f46\u5176\u5728\u7cbe\u5ea6\u654f\u611f\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u4ecd\u4e0d\u660e\u786e\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u7279\u522b\u662f\u5728\u4e25\u683c\u4f4e\u8bef\u62a5\u7387\u8981\u6c42\u4e0b\u7684\u5206\u7c7b\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u7814\u7a76\u8986\u76d6\u5b89\u5168\u68c0\u6d4b\u548c\u5e7b\u89c9\u68c0\u6d4b\u4e24\u4e2a\u4efb\u52a1\uff0c\u5728\u5fae\u8c03\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u6807\u51c6LLM\u548c\u5927\u578b\u63a8\u7406\u6a21\u578b\uff0c\u6bd4\u8f83\u63a8\u7406\u5f00\u542f\u4e0e\u63a8\u7406\u5173\u95ed\u4e24\u79cd\u6a21\u5f0f\uff0c\u5e76\u5206\u6790\u57fa\u4e8etoken\u7684\u8bc4\u5206\u4e0e\u81ea\u6211\u53e3\u5934\u5316\u7f6e\u4fe1\u5ea6\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u63a8\u7406\u5f00\u542f\u6a21\u5f0f\u867d\u63d0\u5347\u6574\u4f53\u51c6\u786e\u7387\uff0c\u4f46\u5728\u4f4e\u8bef\u62a5\u7387\u9608\u503c\u4e0b\u8868\u73b0\u4e0d\u4f73\uff1b\u63a8\u7406\u5173\u95ed\u6a21\u5f0f\u5728\u7cbe\u5ea6\u654f\u611f\u573a\u666f\u4e2d\u5360\u4f18\uff1b\u57fa\u4e8etoken\u7684\u8bc4\u5206\u663e\u8457\u4f18\u4e8e\u81ea\u6211\u53e3\u5934\u5316\u7f6e\u4fe1\u5ea6\uff1b\u4e24\u79cd\u6a21\u5f0f\u7684\u7b80\u5355\u96c6\u6210\u53ef\u6062\u590d\u5404\u81ea\u4f18\u52bf\u3002", "conclusion": "\u63a8\u7406\u5728\u7cbe\u5ea6\u654f\u611f\u5e94\u7528\u4e2d\u5177\u6709\u53cc\u5203\u5251\u7279\u6027\uff1a\u6709\u76ca\u4e8e\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4f46\u5f80\u5f80\u4e0d\u9002\u7528\u4e8e\u8981\u6c42\u4e25\u683c\u7cbe\u5ea6\u7684\u5b9e\u9645\u90e8\u7f72\u573a\u666f\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2510.21445", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21445", "abs": "https://arxiv.org/abs/2510.21445", "authors": ["Thanh Cong Ho", "Farah Kharrat", "Abderrazek Abid", "Fakhri Karray"], "title": "REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring", "comment": null, "summary": "With the widespread adoption of wearable devices in our daily lives, the\ndemand and appeal for remote patient monitoring have significantly increased.\nMost research in this field has concentrated on collecting sensor data,\nvisualizing it, and analyzing it to detect anomalies in specific diseases such\nas diabetes, heart disease and depression. However, this domain has a notable\ngap in the aspect of human-machine interaction. This paper proposes REMONI, an\nautonomous REmote health MONItoring system that integrates multimodal large\nlanguage models (MLLMs), the Internet of Things (IoT), and wearable devices.\nThe system automatically and continuously collects vital signs, accelerometer\ndata from a special wearable (such as a smartwatch), and visual data in patient\nvideo clips collected from cameras. This data is processed by an anomaly\ndetection module, which includes a fall detection model and algorithms to\nidentify and alert caregivers of the patient's emergency conditions. A\ndistinctive feature of our proposed system is the natural language processing\ncomponent, developed with MLLMs capable of detecting and recognizing a\npatient's activity and emotion while responding to healthcare worker's\ninquiries. Additionally, prompt engineering is employed to integrate all\npatient information seamlessly. As a result, doctors and nurses can access\nreal-time vital signs and the patient's current state and mood by interacting\nwith an intelligent agent through a user-friendly web application. Our\nexperiments demonstrate that our system is implementable and scalable for\nreal-life scenarios, potentially reducing the workload of medical professionals\nand healthcare costs. A full-fledged prototype illustrating the functionalities\nof the system has been developed and being tested to demonstrate the robustness\nof its various capabilities.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faREMONI\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u7269\u8054\u7f51\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u76d1\u6d4b\u60a3\u8005\u751f\u547d\u4f53\u5f81\u3001\u68c0\u6d4b\u5f02\u5e38\u72b6\u6001\u5e76\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u7684\u8fdc\u7a0b\u5065\u5eb7\u76d1\u62a4\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u80fd\u591f\u5b9e\u65f6\u5206\u6790\u60a3\u8005\u6d3b\u52a8\u3001\u60c5\u7eea\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u4e3a\u533b\u62a4\u4eba\u5458\u63d0\u4f9b\u76f4\u89c2\u7684\u5065\u5eb7\u72b6\u6001\u67e5\u8be2\u754c\u9762\u3002", "motivation": "\u5f53\u524d\u8fdc\u7a0b\u60a3\u8005\u76d1\u62a4\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u4f20\u611f\u5668\u6570\u636e\u6536\u96c6\u3001\u53ef\u89c6\u5316\u548c\u7279\u5b9a\u75be\u75c5\u5f02\u5e38\u68c0\u6d4b\uff0c\u4f46\u5728\u4eba\u673a\u4ea4\u4e92\u65b9\u9762\u5b58\u5728\u663e\u8457\u7a7a\u767d\u3002\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u60a3\u8005\u6d3b\u52a8\u72b6\u6001\u3001\u60c5\u7eea\u53d8\u5316\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u53ca\u533b\u62a4\u4eba\u5458\u4e0e\u76d1\u62a4\u7cfb\u7edf\u4e4b\u95f4\u7684\u667a\u80fd\u4ea4\u4e92\u673a\u5236\u3002", "method": "\u7cfb\u7edf\u6574\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u7269\u8054\u7f51\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u81ea\u52a8\u6301\u7eed\u6536\u96c6\u751f\u547d\u4f53\u5f81\u3001\u52a0\u901f\u5ea6\u8ba1\u6570\u636e\u548c\u60a3\u8005\u89c6\u9891\u7247\u6bb5\u3002\u91c7\u7528\u5f02\u5e38\u68c0\u6d4b\u6a21\u5757\u5305\u62ec\u8dcc\u5012\u68c0\u6d4b\u6a21\u578b\u548c\u7d27\u6025\u72b6\u51b5\u8bc6\u522b\u7b97\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u65e0\u7f1d\u6574\u5408\u6240\u6709\u60a3\u8005\u4fe1\u606f\uff0c\u5e76\u5f00\u53d1\u4e86\u80fd\u591f\u8bc6\u522b\u60a3\u8005\u6d3b\u52a8\u548c\u60c5\u7eea\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u7cfb\u7edf\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u53ef\u5b9e\u65bd\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5f00\u53d1\u4e86\u5b8c\u6574\u529f\u80fd\u539f\u578b\u5e76\u6b63\u5728\u8fdb\u884c\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5404\u9879\u80fd\u529b\u7684\u7a33\u5065\u6027\u3002\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u51cf\u5c11\u533b\u62a4\u4eba\u5458\u5de5\u4f5c\u8d1f\u62c5\u548c\u533b\u7597\u6210\u672c\uff0c\u901a\u8fc7\u7528\u6237\u53cb\u597d\u7684Web\u5e94\u7528\u4e3a\u533b\u751f\u62a4\u58eb\u63d0\u4f9b\u5b9e\u65f6\u751f\u547d\u4f53\u5f81\u548c\u60a3\u8005\u72b6\u6001\u4fe1\u606f\u3002", "conclusion": "REMONI\u7cfb\u7edf\u5c55\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fdc\u7a0b\u5065\u5eb7\u76d1\u62a4\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u667a\u80fd\u533b\u7597\u76d1\u62a4\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8303\u5f0f\u3002\u8be5\u7cfb\u7edf\u6709\u671b\u901a\u8fc7\u81ea\u52a8\u5316\u76d1\u6d4b\u548c\u667a\u80fd\u4ea4\u4e92\u663e\u8457\u63d0\u5347\u533b\u7597\u76d1\u62a4\u6548\u7387\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u533b\u7597\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.21553", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21553", "abs": "https://arxiv.org/abs/2510.21553", "authors": ["Jared Claypoole", "Yunye Gong", "Noson S. Yanofsky", "Ajay Divakaran"], "title": "Document Understanding, Measurement, and Manipulation Using Category Theory", "comment": null, "summary": "We apply category theory to extract multimodal document structure which leads\nus to develop information theoretic measures, content summarization and\nextension, and self-supervised improvement of large pretrained models. We first\ndevelop a mathematical representation of a document as a category of\nquestion-answer pairs. Second, we develop an orthogonalization procedure to\ndivide the information contained in one or more documents into non-overlapping\npieces. The structures extracted in the first and second steps lead us to\ndevelop methods to measure and enumerate the information contained in a\ndocument. We also build on those steps to develop new summarization techniques,\nas well as to develop a solution to a new problem viz. exegesis resulting in an\nextension of the original document. Our question-answer pair methodology\nenables a novel rate distortion analysis of summarization techniques. We\nimplement our techniques using large pretrained models, and we propose a\nmultimodal extension of our overall mathematical framework. Finally, we develop\na novel self-supervised method using RLVR to improve large pretrained models\nusing consistency constraints such as composability and closure under certain\noperations that stem naturally from our category theoretic framework.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8303\u7574\u8bba\u7684\u6587\u6863\u7ed3\u6784\u5206\u6790\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u4fe1\u606f\u7406\u8bba\u5ea6\u91cf\u3001\u5185\u5bb9\u6458\u8981\u4e0e\u6269\u5c55\u6280\u672f\uff0c\u4ee5\u53ca\u5229\u7528RLVR\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u6765\u6539\u8fdb\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u6587\u6863\u8868\u793a\u4e3a\u95ee\u7b54\u5bf9\u7684\u8303\u7574\uff0c\u5b9e\u73b0\u4e86\u6587\u6863\u4fe1\u606f\u7684\u6b63\u4ea4\u5316\u5206\u89e3\u548c\u65b0\u578b\u6458\u8981\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6587\u6863\u7406\u89e3\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u6587\u6863\u5185\u5728\u7ed3\u6784\u7684\u6570\u5b66\u5f62\u5f0f\u5316\u8868\u793a\uff0c\u96be\u4ee5\u7cfb\u7edf\u6027\u5730\u63d0\u53d6\u3001\u5ea6\u91cf\u548c\u64cd\u4f5c\u6587\u6863\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8303\u7574\u8bba\u5efa\u7acb\u6587\u6863\u7684\u6570\u5b66\u8868\u793a\u6846\u67b6\uff0c\u89e3\u51b3\u6587\u6863\u4fe1\u606f\u5206\u89e3\u3001\u6458\u8981\u751f\u6210\u548c\u5185\u5bb9\u6269\u5c55\u7b49\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5c06\u6587\u6863\u5efa\u6a21\u4e3a\u95ee\u7b54\u5bf9\u8303\u7574\u7684\u6570\u5b66\u8868\u793a\uff0c\u5f00\u53d1\u4e86\u4fe1\u606f\u6b63\u4ea4\u5316\u7a0b\u5e8f\u5c06\u6587\u6863\u4fe1\u606f\u5206\u89e3\u4e3a\u4e0d\u91cd\u53e0\u7684\u90e8\u5206\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\u6784\u5efa\u4e86\u4fe1\u606f\u5ea6\u91cf\u65b9\u6cd5\u3001\u65b0\u578b\u6458\u8981\u6280\u672f\uff0c\u5e76\u5229\u7528RLVR\u5f00\u53d1\u4e86\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u5408\u6027\u548c\u95ed\u5305\u7b49\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u6539\u8fdb\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u4e86\u6587\u6863\u4fe1\u606f\u7684\u7cfb\u7edf\u5206\u89e3\u548c\u91cf\u5316\u5ea6\u91cf\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u7387\u5931\u771f\u5206\u6790\u7684\u6458\u8981\u8bc4\u4f30\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u6587\u6863\u6ce8\u91ca\u6269\u5c55\u7684\u65b0\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u6784\u5efa\u4e86\u591a\u6a21\u6001\u6269\u5c55\u7684\u6570\u5b66\u6846\u67b6\u3002", "conclusion": "\u8303\u7574\u8bba\u4e3a\u6587\u6863\u7406\u89e3\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6570\u5b66\u57fa\u7840\uff0c\u652f\u6301\u7cfb\u7edf\u5316\u7684\u4fe1\u606f\u63d0\u53d6\u548c\u64cd\u4f5c\u3002\u95ee\u7b54\u5bf9\u8868\u793a\u548c\u6b63\u4ea4\u5316\u5206\u89e3\u4e3a\u6587\u6863\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u81ea\u76d1\u7763\u65b9\u6cd5\u5c55\u793a\u4e86\u5229\u7528\u7ed3\u6784\u7ea6\u675f\u6539\u8fdb\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u6587\u6863\u5904\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.21182", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21182", "abs": "https://arxiv.org/abs/2510.21182", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xiaojun Wan"], "title": "KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution", "comment": "submitting to ICLR2026", "summary": "The rapid progress of multimodal large language models (MLLMs) calls for more\nreliable evaluation protocols. Existing static benchmarks suffer from the\npotential risk of data contamination and saturation, leading to inflated or\nmisleading performance evaluations. To address these issues, we first apply\nGraph formulation to represent a static or dynamic VQA sample. With the\nformulation, we propose Knowledge-enhanced Benchmark Evolution(KBE), a dynamic\nmultimodal evaluation framework. KBE first analyzes the original static\nbenchmark, then expands it by integrating multimodal knowledge, transforming\nthe static benchmark into a controllable, dynamic evolving version. Crucially,\nKBE can both reconstruct questions by Re-selecting visual information in the\noriginal image and expand existing questions with external textual knowledge.\nIt enables difficulty-controllable evaluation by adjusting the degree of\nquestion exploration. Extensive experiments demonstrate that KBE alleviates the\nrisk of data contamination, data saturation, and provides a more comprehensive\nassessment of MLLM capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u77e5\u8bc6\u589e\u5f3a\u57fa\u51c6\u6f14\u5316\uff08KBE\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u8868\u793aVQA\u6837\u672c\u5e76\u6574\u5408\u591a\u6a21\u6001\u77e5\u8bc6\uff0c\u5c06\u9759\u6001\u57fa\u51c6\u8f6c\u5316\u4e3a\u53ef\u63a7\u7684\u52a8\u6001\u6f14\u5316\u7248\u672c\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u6570\u636e\u6c61\u67d3\u548c\u9971\u548c\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u57fa\u51c6\u5b58\u5728\u6570\u636e\u6c61\u67d3\u548c\u9971\u548c\u98ce\u9669\uff0c\u5bfc\u81f4\u6027\u80fd\u8bc4\u4f30\u5931\u771f\u6216\u8bef\u5bfc\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u534f\u8bae\u6765\u51c6\u786e\u8861\u91cf\u6a21\u578b\u80fd\u529b\u3002", "method": "\u91c7\u7528\u56fe\u7ed3\u6784\u8868\u793a\u9759\u6001\u6216\u52a8\u6001VQA\u6837\u672c\uff0c\u63d0\u51fa\u77e5\u8bc6\u589e\u5f3a\u57fa\u51c6\u6f14\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u9009\u62e9\u539f\u59cb\u56fe\u50cf\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\u548c\u6574\u5408\u5916\u90e8\u6587\u672c\u77e5\u8bc6\u6765\u91cd\u6784\u548c\u6269\u5c55\u95ee\u9898\uff0c\u5b9e\u73b0\u96be\u5ea6\u53ef\u63a7\u7684\u8bc4\u4f30\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eKBE\u6709\u6548\u7f13\u89e3\u4e86\u6570\u636e\u6c61\u67d3\u548c\u9971\u548c\u98ce\u9669\uff0c\u63d0\u4f9b\u4e86\u5bf9MLLM\u80fd\u529b\u66f4\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u901a\u8fc7\u8c03\u6574\u95ee\u9898\u63a2\u7d22\u7a0b\u5ea6\u5b9e\u73b0\u96be\u5ea6\u53ef\u63a7\u7684\u57fa\u51c6\u6f14\u5316\u3002", "conclusion": "KBE\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u52a8\u6001\u3001\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u53cd\u6620\u6a21\u578b\u771f\u5b9e\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u8bc4\u4f30\u65b9\u6cd5\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2510.20887", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20887", "abs": "https://arxiv.org/abs/2510.20887", "authors": ["Anujraaj Argo Goyal", "Guocheng Gordon Qian", "Huseyin Coskun", "Aarush Gupta", "Himmy Tam", "Daniil Ostashev", "Ju Hu", "Dhritiman Sagar", "Sergey Tulyakov", "Kfir Aberman", "Kuan-Chieh Jackson Wang"], "title": "Preventing Shortcuts in Adapter Training via Providing the Shortcuts", "comment": "Accepted to NeurIPS 2025, webpage:\n  https://snap-research.github.io/shortcut-rerouting/", "summary": "Adapter-based training has emerged as a key mechanism for extending the\ncapabilities of powerful foundation image generators, enabling personalized and\nstylized text-to-image synthesis. These adapters are typically trained to\ncapture a specific target attribute, such as subject identity, using\nsingle-image reconstruction objectives. However, because the input image\ninevitably contains a mixture of visual factors, adapters are prone to entangle\nthe target attribute with incidental ones, such as pose, expression, and\nlighting. This spurious correlation problem limits generalization and obstructs\nthe model's ability to adhere to the input text prompt. In this work, we\nuncover a simple yet effective solution: provide the very shortcuts we wish to\neliminate during adapter training. In Shortcut-Rerouted Adapter Training,\nconfounding factors are routed through auxiliary modules, such as ControlNet or\nLoRA, eliminating the incentive for the adapter to internalize them. The\nauxiliary modules are then removed during inference. When applied to tasks like\nfacial and full-body identity injection, our approach improves generation\nquality, diversity, and prompt adherence. These results point to a general\ndesign principle in the era of large models: when seeking disentangled\nrepresentations, the most effective path may be to establish shortcuts for what\nshould NOT be learned.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6377\u5f84\u91cd\u5b9a\u5411\u9002\u914d\u5668\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u9002\u914d\u5668\u8bad\u7ec3\u671f\u95f4\u4e3a\u6df7\u6dc6\u56e0\u7d20\u5efa\u7acb\u4e13\u7528\u8def\u5f84\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9002\u914d\u5668\u7ea0\u7f20\u76ee\u6807\u5c5e\u6027\u4e0e\u5076\u7136\u56e0\u7d20\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u6587\u672c\u63d0\u793a\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u57fa\u4e8e\u9002\u914d\u5668\u7684\u8bad\u7ec3\u5728\u6269\u5c55\u57fa\u7840\u56fe\u50cf\u751f\u6210\u5668\u80fd\u529b\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u76ee\u6807\u5c5e\u6027\u4e0e\u5076\u7136\u56e0\u7d20\uff08\u5982\u59ff\u6001\u3001\u8868\u60c5\u3001\u5149\u7167\uff09\u7684\u7ea0\u7f20\u95ee\u9898\uff0c\u8fd9\u79cd\u4f2a\u76f8\u5173\u6027\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u963b\u788d\u4e86\u5bf9\u8f93\u5165\u6587\u672c\u63d0\u793a\u7684\u9075\u5faa\u3002", "method": "\u63d0\u51fa\u6377\u5f84\u91cd\u5b9a\u5411\u9002\u914d\u5668\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7ControlNet\u6216LoRA\u7b49\u8f85\u52a9\u6a21\u5757\u4e3a\u6df7\u6dc6\u56e0\u7d20\u5efa\u7acb\u4e13\u7528\u8def\u5f84\uff0c\u6d88\u9664\u9002\u914d\u5668\u5185\u90e8\u5316\u8fd9\u4e9b\u56e0\u7d20\u7684\u52a8\u673a\uff0c\u8bad\u7ec3\u5b8c\u6210\u540e\u5728\u63a8\u7406\u9636\u6bb5\u79fb\u9664\u8f85\u52a9\u6a21\u5757\u3002", "result": "\u5728\u9762\u90e8\u548c\u5168\u8eab\u8eab\u4efd\u6ce8\u5165\u7b49\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u63d0\u793a\u9075\u5faa\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u8026\u76ee\u6807\u5c5e\u6027\u4e0e\u5076\u7136\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u4e2a\u901a\u7528\u8bbe\u8ba1\u539f\u5219\uff1a\u5728\u8ffd\u6c42\u89e3\u8026\u8868\u793a\u65f6\uff0c\u6700\u6709\u6548\u7684\u9014\u5f84\u53ef\u80fd\u662f\u4e3a\u4e0d\u5e94\u5b66\u4e60\u7684\u5185\u5bb9\u5efa\u7acb\u6377\u5f84\uff0c\u8fd9\u4e3a\u5927\u578b\u6a21\u578b\u65f6\u4ee3\u7684\u9002\u914d\u5668\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u8303\u5f0f\u3002"}}
{"id": "2510.20838", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.20838", "abs": "https://arxiv.org/abs/2510.20838", "authors": ["Abir Khan Ratul", "Sanjay Acharjee", "Somin Park", "Md Nazmus Sakib"], "title": "Sketch2BIM: A Multi-Agent Human-AI Collaborative Pipeline to Convert Hand-Drawn Floor Plans to 3D BIM", "comment": null, "summary": "This study introduces a human-in-the-loop pipeline that converts unscaled,\nhand-drawn floor plan sketches into semantically consistent 3D BIM models. The\nworkflow leverages multimodal large language models (MLLMs) within a\nmulti-agent framework, combining perceptual extraction, human feedback, schema\nvalidation, and automated BIM scripting. Initially, sketches are iteratively\nrefined into a structured JSON layout of walls, doors, and windows. Later,\nthese layouts are transformed into executable scripts that generate 3D BIM\nmodels. Experiments on ten diverse floor plans demonstrate strong convergence:\nopenings (doors, windows) are captured with high reliability in the initial\npass, while wall detection begins around 83% and achieves near-perfect\nalignment after a few feedback iterations. Across all categories, precision,\nrecall, and F1 scores remain above 0.83, and geometric errors (RMSE, MAE)\nprogressively decrease to zero through feedback corrections. This study\ndemonstrates how MLLM-driven multi-agent reasoning can make BIM creation\naccessible to both experts and non-experts using only freehand sketches.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u540c\u7ba1\u9053\uff0c\u5c06\u672a\u7f29\u653e\u624b\u7ed8\u5e73\u9762\u56fe\u8f6c\u6362\u4e3a\u8bed\u4e49\u4e00\u81f4\u76843D BIM\u6a21\u578b\u3002\u8be5\u5de5\u4f5c\u6d41\u7a0b\u5728\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4e2d\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u611f\u77e5\u63d0\u53d6\u3001\u4eba\u5de5\u53cd\u9988\u3001\u6a21\u5f0f\u9a8c\u8bc1\u548c\u81ea\u52a8\u5316BIM\u811a\u672c\u751f\u6210\u5b9e\u73b0\u9ad8\u6548\u8f6c\u6362\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edfBIM\u521b\u5efa\u8fc7\u7a0b\u5bf9\u4e13\u4e1a\u77e5\u8bc6\u548c\u590d\u6742\u8f6f\u4ef6\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u4f7f\u5f97\u975e\u4e13\u4e1a\u4eba\u58eb\u4e5f\u80fd\u901a\u8fc7\u7b80\u5355\u7684\u624b\u7ed8\u8349\u56fe\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u5efa\u7b51\u4fe1\u606f\u6a21\u578b\uff0c\u964d\u4f4eBIM\u6280\u672f\u7684\u4f7f\u7528\u95e8\u69db\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u542b\u611f\u77e5\u63d0\u53d6\u3001\u4eba\u5de5\u53cd\u9988\u8fed\u4ee3\u3001\u6a21\u5f0f\u9a8c\u8bc1\u548c\u81ea\u52a8\u5316BIM\u811a\u672c\u751f\u6210\u56db\u4e2a\u5173\u952e\u9636\u6bb5\u3002\u9996\u5148\u5c06\u8349\u56fe\u8fed\u4ee3\u4f18\u5316\u4e3a\u7ed3\u6784\u5316\u7684JSON\u5e03\u5c40\uff0c\u7136\u540e\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u811a\u672c\u751f\u62103D BIM\u6a21\u578b\u3002", "result": "\u5728\u5341\u4e2a\u4e0d\u540c\u5e73\u9762\u56fe\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u95e8\u7a97\u7b49\u5f00\u53e3\u5143\u7d20\u5728\u521d\u59cb\u9636\u6bb5\u5373\u8fbe\u5230\u9ad8\u53ef\u9760\u6027\u68c0\u6d4b\uff0c\u5899\u4f53\u68c0\u6d4b\u4ece83%\u5f00\u59cb\u5e76\u901a\u8fc7\u51e0\u6b21\u53cd\u9988\u8fed\u4ee3\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u5bf9\u9f50\u3002\u6240\u6709\u7c7b\u522b\u7684\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u4fdd\u6301\u57280.83\u4ee5\u4e0a\uff0c\u51e0\u4f55\u8bef\u5dee\u901a\u8fc7\u53cd\u9988\u4fee\u6b63\u9010\u6b65\u964d\u81f3\u96f6\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660eMLLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u591f\u4f7fBIM\u521b\u5efa\u5bf9\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u90fd\u53d8\u5f97\u53ef\u8bbf\u95ee\uff0c\u4ec5\u9700\u4f7f\u7528\u624b\u7ed8\u8349\u56fe\u5373\u53ef\u5b8c\u6210\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5efa\u7b51\u884c\u4e1a\u7684\u6570\u5b57\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u4f4e\u95e8\u69db\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001AI\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.21518", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21518", "abs": "https://arxiv.org/abs/2510.21518", "authors": ["Lorenzo Basile", "Valentino Maiorca", "Diego Doimo", "Francesco Locatello", "Alberto Cazzaniga"], "title": "Head Pursuit: Probing Attention Specialization in Multimodal Transformers", "comment": "Accepted at NeurIPS 2025 (spotlight)", "summary": "Language and vision-language models have shown impressive performance across\na wide range of tasks, but their internal mechanisms remain only partly\nunderstood. In this work, we study how individual attention heads in\ntext-generative models specialize in specific semantic or visual attributes.\nBuilding on an established interpretability method, we reinterpret the practice\nof probing intermediate activations with the final decoding layer through the\nlens of signal processing. This lets us analyze multiple samples in a\nprincipled way and rank attention heads based on their relevance to target\nconcepts. Our results show consistent patterns of specialization at the head\nlevel across both unimodal and multimodal transformers. Remarkably, we find\nthat editing as few as 1% of the heads, selected using our method, can reliably\nsuppress or enhance targeted concepts in the model output. We validate our\napproach on language tasks such as question answering and toxicity mitigation,\nas well as vision-language tasks including image classification and captioning.\nOur findings highlight an interpretable and controllable structure within\nattention layers, offering simple tools for understanding and editing\nlarge-scale generative models.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4fe1\u53f7\u5904\u7406\u89c6\u89d2\u91cd\u65b0\u89e3\u91ca\u6ce8\u610f\u529b\u5934\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6587\u672c\u751f\u6210\u6a21\u578b\u4e2d\u4e2a\u4f53\u6ce8\u610f\u529b\u5934\u5728\u8bed\u4e49\u548c\u89c6\u89c9\u5c5e\u6027\u4e0a\u7684\u4e13\u4e1a\u5316\u6a21\u5f0f\uff0c\u5e76\u8bc1\u660e\u4ec5\u7f16\u8f911%\u7684\u6ce8\u610f\u529b\u5934\u5373\u53ef\u53ef\u9760\u5730\u6291\u5236\u6216\u589e\u5f3a\u76ee\u6807\u6982\u5ff5\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u4ecd\u672a\u88ab\u5b8c\u5168\u7406\u89e3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6587\u672c\u751f\u6210\u6a21\u578b\u4e2d\u4e2a\u4f53\u6ce8\u610f\u529b\u5934\u5982\u4f55\u4e13\u95e8\u5316\u5904\u7406\u7279\u5b9a\u8bed\u4e49\u6216\u89c6\u89c9\u5c5e\u6027\uff0c\u4ee5\u586b\u8865\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u5df2\u6709\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u672c\u7814\u7a76\u4ece\u4fe1\u53f7\u5904\u7406\u89d2\u5ea6\u91cd\u65b0\u89e3\u91ca\u4e86\u4f7f\u7528\u6700\u7ec8\u89e3\u7801\u5c42\u63a2\u6d4b\u4e2d\u95f4\u6fc0\u6d3b\u7684\u5b9e\u8df5\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\u6765\u5206\u6790\u591a\u4e2a\u6837\u672c\u5e76\u6839\u636e\u6ce8\u610f\u529b\u5934\u4e0e\u76ee\u6807\u6982\u5ff5\u7684\u76f8\u5173\u6027\u8fdb\u884c\u6392\u5e8f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001Transformer\u4e2d\u90fd\u5b58\u5728\u4e00\u81f4\u7684\u6ce8\u610f\u529b\u5934\u7ea7\u4e13\u4e1a\u5316\u6a21\u5f0f\uff0c\u901a\u8fc7\u8be5\u65b9\u6cd5\u9009\u62e9\u7684\u4ec51%\u6ce8\u610f\u529b\u5934\u7f16\u8f91\u5373\u53ef\u53ef\u9760\u5730\u6291\u5236\u6216\u589e\u5f3a\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u76ee\u6807\u6982\u5ff5\uff0c\u5e76\u5728\u8bed\u8a00\u95ee\u7b54\u3001\u6bd2\u6027\u7f13\u89e3\u4ee5\u53ca\u89c6\u89c9\u8bed\u8a00\u56fe\u50cf\u5206\u7c7b\u548c\u63cf\u8ff0\u4efb\u52a1\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u5c42\u5185\u5b58\u5728\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\u7684\u7ed3\u6784\uff0c\u4e3a\u7406\u89e3\u548c\u7f16\u8f91\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u5de5\u5177\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u6a21\u578b\u5185\u90e8\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u6982\u5ff5\u4e13\u4e1a\u5316\u673a\u5236\u3002"}}
{"id": "2510.20888", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20888", "abs": "https://arxiv.org/abs/2510.20888", "authors": ["Yuxuan Bian", "Xin Chen", "Zenan Li", "Tiancheng Zhi", "Shen Sang", "Linjie Luo", "Qiang Xu"], "title": "Video-As-Prompt: Unified Semantic Control for Video Generation", "comment": "Website: https://bytedance.github.io/Video-As-Prompt", "summary": "Unified, generalizable semantic control in video generation remains a\ncritical open challenge. Existing methods either introduce artifacts by\nenforcing inappropriate pixel-wise priors from structure-based controls, or\nrely on non-generalizable, condition-specific finetuning or task-specific\narchitectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes\nthis problem as in-context generation. VAP leverages a reference video as a\ndirect semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via\na plug-and-play Mixture-of-Transformers (MoT) expert. This architecture\nprevents catastrophic forgetting and is guided by a temporally biased position\nembedding that eliminates spurious mapping priors for robust context retrieval.\nTo power this approach and catalyze future research, we built VAP-Data, the\nlargest dataset for semantic-controlled video generation with over 100K paired\nvideos across 100 semantic conditions. As a single unified model, VAP sets a\nnew state-of-the-art for open-source methods, achieving a 38.7% user preference\nrate that rivals leading condition-specific commercial models. VAP's strong\nzero-shot generalization and support for various downstream applications mark a\nsignificant advance toward general-purpose, controllable video generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Video-As-Prompt (VAP)\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u53c2\u8003\u89c6\u9891\u4f5c\u4e3a\u8bed\u4e49\u63d0\u793a\u6765\u5f15\u5bfc\u51bb\u7ed3\u7684\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\uff0c\u5b9e\u73b0\u4e86\u7edf\u4e00\u3001\u53ef\u6cdb\u5316\u7684\u8bed\u4e49\u63a7\u5236\u89c6\u9891\u751f\u6210\uff0c\u5728\u5f00\u6e90\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u4e2d\u7684\u7edf\u4e00\u8bed\u4e49\u63a7\u5236\u9762\u4e34\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u901a\u8fc7\u7ed3\u6784\u63a7\u5236\u65bd\u52a0\u4e0d\u9002\u5f53\u7684\u50cf\u7d20\u7ea7\u5148\u9a8c\u5f15\u5165\u4f2a\u5f71\uff0c\u8981\u4e48\u4f9d\u8d56\u4e0d\u53ef\u6cdb\u5316\u7684\u6761\u4ef6\u7279\u5b9a\u5fae\u8c03\u6216\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\uff0c\u7f3a\u4e4f\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "VAP\u91c7\u7528\u4e0a\u4e0b\u6587\u751f\u6210\u8303\u5f0f\uff0c\u5229\u7528\u53c2\u8003\u89c6\u9891\u4f5c\u4e3a\u76f4\u63a5\u8bed\u4e49\u63d0\u793a\uff0c\u901a\u8fc7\u5373\u63d2\u5373\u7528\u7684\u6df7\u5408\u53d8\u6362\u5668\u4e13\u5bb6\u67b6\u6784\u5f15\u5bfc\u51bb\u7ed3\u7684Video Diffusion Transformer\uff0c\u5e76\u91c7\u7528\u65f6\u95f4\u504f\u7f6e\u4f4d\u7f6e\u5d4c\u5165\u6d88\u9664\u865a\u5047\u6620\u5c04\u5148\u9a8c\u4ee5\u5b9e\u73b0\u9c81\u68d2\u7684\u4e0a\u4e0b\u6587\u68c0\u7d22\u3002", "result": "VAP\u4f5c\u4e3a\u5355\u4e00\u7edf\u4e00\u6a21\u578b\uff0c\u5728\u5f00\u6e90\u65b9\u6cd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u83b7\u5f9738.7%\u7684\u7528\u6237\u504f\u597d\u7387\uff0c\u53ef\u4e0e\u9886\u5148\u7684\u6761\u4ef6\u7279\u5b9a\u5546\u4e1a\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u5305\u542b10\u4e07\u5bf9\u89c6\u9891\u7684VAP-Data\u6570\u636e\u96c6\u652f\u6301\u8be5\u65b9\u6cd5\u3002", "conclusion": "VAP\u7684\u5f3a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u5404\u79cd\u4e0b\u6e38\u5e94\u7528\u7684\u652f\u6301\u6807\u5fd7\u7740\u5411\u901a\u7528\u53ef\u63a7\u89c6\u9891\u751f\u6210\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u5176\u5373\u63d2\u5373\u7528\u67b6\u6784\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2510.21093", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21093", "abs": "https://arxiv.org/abs/2510.21093", "authors": ["Siyong Chen", "Jinbo Wen", "Jiawen Kang", "Tenghui Huang", "Xumin Huang", "Yuanjia Su", "Hudan Pan", "Zishao Zhong", "Dusit Niyato", "Shengli Xie", "Dong In Kim"], "title": "MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning", "comment": null, "summary": "Recently, large models have shown significant potential for smart healthcare.\nHowever, the deployment of Large Vision-Language Models (LVLMs) for clinical\nservices is currently hindered by three critical challenges: a tendency to\nhallucinate answers not grounded in visual evidence, the inefficiency of\nfixed-depth reasoning, and the difficulty of multi-institutional collaboration.\nTo address these challenges, in this paper, we develop MedAlign, a novel\nframework to ensure visually accurate LVLM responses for Medical Visual\nQuestion Answering (Med-VQA). Specifically, we first propose a multimodal\nDirect Preference Optimization (mDPO) objective to explicitly align preference\nlearning with visual context. We then design a Retrieval-Aware\nMixture-of-Experts (RA-MoE) architecture that utilizes image and text\nsimilarity to route queries to a specialized and context-augmented LVLM (i.e.,\nan expert), thereby mitigating hallucinations in LVLMs. To achieve adaptive\nreasoning and facilitate multi-institutional collaboration, we propose a\nfederated governance mechanism, where the selected expert, fine-tuned on\nclinical datasets based on mDPO, locally performs iterative Chain-of-Thought\n(CoT) reasoning via the local meta-cognitive uncertainty estimator. Extensive\nexperiments on three representative Med-VQA datasets demonstrate that MedAlign\nachieves state-of-the-art performance, outperforming strong retrieval-augmented\nbaselines by up to $11.85\\%$ in F1-score, and simultaneously reducing the\naverage reasoning length by $51.60\\%$ compared with fixed-depth CoT approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MedAlign\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u76f4\u63a5\u504f\u597d\u4f18\u5316\u548c\u68c0\u7d22\u611f\u77e5\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u6765\u89e3\u51b3\u533b\u7597\u89c6\u89c9\u95ee\u7b54\u4e2dLVLM\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406\u548c\u591a\u673a\u6784\u534f\u4f5c\uff0c\u5728\u4e09\u4e2aMed-VQA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u670d\u52a1\u90e8\u7f72\u4e2d\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u57fa\u4e8e\u89c6\u89c9\u8bc1\u636e\u7684\u5e7b\u89c9\u503e\u5411\u3001\u56fa\u5b9a\u6df1\u5ea6\u63a8\u7406\u7684\u4f4e\u6548\u6027\u4ee5\u53ca\u591a\u673a\u6784\u534f\u4f5c\u7684\u56f0\u96be\uff0c\u8fd9\u4e9b\u9650\u5236\u4e86LVLM\u5728\u667a\u80fd\u533b\u7597\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u76f4\u63a5\u504f\u597d\u4f18\u5316\u76ee\u6807\u5c06\u504f\u597d\u5b66\u4e60\u4e0e\u89c6\u89c9\u4e0a\u4e0b\u6587\u663e\u5f0f\u5bf9\u9f50\uff0c\u8bbe\u8ba1\u68c0\u7d22\u611f\u77e5\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u5229\u7528\u56fe\u50cf\u548c\u6587\u672c\u76f8\u4f3c\u6027\u5c06\u67e5\u8be2\u8def\u7531\u5230\u4e13\u95e8\u7684\u4e0a\u4e0b\u6587\u589e\u5f3aLVLM\u4e13\u5bb6\uff0c\u5e76\u91c7\u7528\u8054\u90a6\u6cbb\u7406\u673a\u5236\u5b9e\u73b0\u57fa\u4e8e\u672c\u5730\u5143\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5668\u7684\u81ea\u9002\u5e94\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027Med-VQA\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMedAlign\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6bd4\u5f3a\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\u5728F1\u5206\u6570\u4e0a\u63d0\u5347\u9ad8\u8fbe11.85%\uff0c\u540c\u65f6\u4e0e\u56fa\u5b9a\u6df1\u5ea6CoT\u65b9\u6cd5\u76f8\u6bd4\u5e73\u5747\u63a8\u7406\u957f\u5ea6\u51cf\u5c1151.60%\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u89c6\u89c9\u4e0a\u4e0b\u6587\u5bf9\u9f50\u3001\u4e13\u5bb6\u8def\u7531\u548c\u8054\u90a6\u6cbb\u7406\u673a\u5236\u53ef\u4ee5\u6709\u6548\u89e3\u51b3LVLM\u5728\u533b\u7597\u9886\u57df\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u591a\u673a\u6784\u4e34\u5e8a\u534f\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u667a\u80fd\u533b\u7597\u4e2d\u5927\u578b\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2510.20951", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20951", "abs": "https://arxiv.org/abs/2510.20951", "authors": ["Mattie Tesfaldet", "Adam W. Harley", "Konstantinos G. Derpanis", "Derek Nowrouzezahrai", "Christopher Pal"], "title": "Generative Point Tracking with Flow Matching", "comment": "Project page: https://mtesfaldet.net/genpt_projpage/", "summary": "Tracking a point through a video can be a challenging task due to uncertainty\narising from visual obfuscations, such as appearance changes and occlusions.\nAlthough current state-of-the-art discriminative models excel in regressing\nlong-term point trajectory estimates -- even through occlusions -- they are\nlimited to regressing to a mean (or mode) in the presence of uncertainty, and\nfail to capture multi-modality. To overcome this limitation, we introduce\nGenerative Point Tracker (GenPT), a generative framework for modelling\nmulti-modal trajectories. GenPT is trained with a novel flow matching\nformulation that combines the iterative refinement of discriminative trackers,\na window-dependent prior for cross-window consistency, and a variance schedule\ntuned specifically for point coordinates. We show how our model's generative\ncapabilities can be leveraged to improve point trajectory estimates by\nutilizing a best-first search strategy on generated samples during inference,\nguided by the model's own confidence of its predictions. Empirically, we\nevaluate GenPT against the current state of the art on the standard\nPointOdyssey, Dynamic Replica, and TAP-Vid benchmarks. Further, we introduce a\nTAP-Vid variant with additional occlusions to assess occluded point tracking\nperformance and highlight our model's ability to capture multi-modality. GenPT\nis capable of capturing the multi-modality in point trajectories, which\ntranslates to state-of-the-art tracking accuracy on occluded points, while\nmaintaining competitive tracking accuracy on visible points compared to extant\ndiscriminative point trackers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u751f\u6210\u5f0f\u70b9\u8f68\u8ff9\u8ddf\u8e2a\u5668\uff08GenPT\uff09\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u5efa\u6a21\u65b9\u6cd5\u89e3\u51b3\u89c6\u9891\u70b9\u8ddf\u8e2a\u4e2d\u7684\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u906e\u6321\u70b9\u8ddf\u8e2a\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5728\u53ef\u89c1\u70b9\u8ddf\u8e2a\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5224\u522b\u5f0f\u70b9\u8ddf\u8e2a\u6a21\u578b\u5728\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u65f6\u53ea\u80fd\u56de\u5f52\u5230\u5747\u503c\u6216\u4f17\u6570\uff0c\u65e0\u6cd5\u6355\u6349\u8f68\u8ff9\u7684\u591a\u6a21\u6001\u7279\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u906e\u6321\u548c\u5916\u89c2\u53d8\u5316\u5bfc\u81f4\u7684\u4e0d\u786e\u5b9a\u6027\u60c5\u51b5\u4e0b\u5b58\u5728\u5c40\u9650\u3002", "method": "GenPT\u91c7\u7528\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u751f\u6210\u5f0f\u6846\u67b6\uff0c\u7ed3\u5408\u5224\u522b\u5f0f\u8ddf\u8e2a\u5668\u7684\u8fed\u4ee3\u4f18\u5316\u3001\u8de8\u7a97\u53e3\u4e00\u81f4\u6027\u7684\u7a97\u53e3\u76f8\u5173\u5148\u9a8c\uff0c\u4ee5\u53ca\u4e13\u95e8\u4e3a\u70b9\u5750\u6807\u8c03\u6574\u7684\u65b9\u5dee\u8c03\u5ea6\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u57fa\u4e8e\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7684\u6700\u4f73\u4f18\u5148\u641c\u7d22\u7b56\u7565\u3002", "result": "\u5728PointOdyssey\u3001Dynamic Replica\u548cTAP-Vid\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGenPT\u5728\u906e\u6321\u70b9\u8ddf\u8e2a\u7cbe\u5ea6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u540c\u65f6\u5728\u53ef\u89c1\u70b9\u8ddf\u8e2a\u4e0a\u4fdd\u6301\u4e0e\u73b0\u6709\u5224\u522b\u5f0f\u8ddf\u8e2a\u5668\u76f8\u5f53\u7684\u7ade\u4e89\u529b\uff0c\u5e76\u5728\u4e13\u95e8\u8bbe\u8ba1\u7684\u906e\u6321\u589e\u5f3a\u7248TAP-Vid\u4e0a\u5c55\u793a\u4e86\u591a\u6a21\u6001\u6355\u6349\u80fd\u529b\u3002", "conclusion": "\u751f\u6210\u5f0f\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5efa\u6a21\u70b9\u8f68\u8ff9\u7684\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u906e\u6321\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd\uff0c\u4e3a\u89c6\u9891\u70b9\u8ddf\u8e2a\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u8868\u660e\u751f\u6210\u5f0f\u6846\u67b6\u5728\u89e3\u51b3\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u4e0a\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.21175", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21175", "abs": "https://arxiv.org/abs/2510.21175", "authors": ["Yujin Jo", "Taesup Kim"], "title": "Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models", "comment": null, "summary": "Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated\nremarkable zero-shot generalization, enabling deployment in a wide range of\nreal-world tasks without additional task-specific training. However, in real\ndeployment scenarios with evolving environments or emerging classes, these\nmodels inevitably face distributional shifts and novel tasks. In such contexts,\nstatic zero-shot capabilities are insufficient, and there is a growing need for\ncontinual learning methods that allow models to adapt over time while avoiding\ncatastrophic forgetting. We introduce NuSA-CL (Null Space Adaptation for\nContinual Learning), a lightweight memory-free continual learning framework\ndesigned to address this challenge. NuSA-CL employs low-rank adaptation and\nconstrains task-specific weight updates to lie within an approximate null space\nof the model's current parameters. This strategy minimizes interference with\npreviously acquired knowledge, effectively preserving the zero-shot\ncapabilities of the original model. Unlike methods relying on replay buffers or\ncostly distillation, NuSA-CL imposes minimal computational and memory overhead,\nmaking it practical for deployment in resource-constrained, real-world\ncontinual learning environments. Experiments show that our framework not only\neffectively preserves zero-shot transfer capabilities but also achieves highly\ncompetitive performance on continual learning benchmarks. These results\nposition NuSA-CL as a practical and scalable solution for continually evolving\nzero-shot VLMs in real-world applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NuSA-CL\u6846\u67b6\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65e0\u5185\u5b58\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u96f6\u7a7a\u95f4\u9002\u5e94\u7b56\u7565\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96f6\u6837\u672c\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u5206\u5e03\u504f\u79fb\u548c\u65b0\u4efb\u52a1\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u9762\u4e34\u73af\u5883\u6f14\u53d8\u548c\u65b0\u5174\u7c7b\u522b\u5e26\u6765\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u9759\u6001\u96f6\u6837\u672c\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u4e0d\u5f15\u53d1\u707e\u96be\u6027\u9057\u5fd8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6a21\u578b\u9002\u5e94\u3002", "method": "NuSA-CL\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\u6280\u672f\uff0c\u5c06\u4efb\u52a1\u7279\u5b9a\u7684\u6743\u91cd\u66f4\u65b0\u7ea6\u675f\u5728\u6a21\u578b\u5f53\u524d\u53c2\u6570\u7684\u8fd1\u4f3c\u96f6\u7a7a\u95f4\u5185\uff0c\u8fd9\u79cd\u7b56\u7565\u6700\u5c0f\u5316\u5bf9\u5df2\u83b7\u53d6\u77e5\u8bc6\u7684\u5e72\u6270\uff0c\u6709\u6548\u4fdd\u7559\u539f\u59cb\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u4e0d\u4ec5\u6709\u6548\u4fdd\u6301\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\uff0c\u8fd8\u5728\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "NuSA-CL\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u73b0\u5b9e\u5e94\u7528\u4e2d\u6301\u7eed\u6f14\u5316\u7684\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u5177\u6709\u8f83\u4f4e\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u4f18\u52bf\u3002"}}
{"id": "2510.20967", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20967", "abs": "https://arxiv.org/abs/2510.20967", "authors": ["Sraavya Sambara", "Sung Eun Kim", "Xiaoman Zhang", "Luyang Luo", "Shreya Johri", "Mohammed Baharoon", "Du Hyun Ro", "Pranav Rajpurkar"], "title": "3DReasonKnee: Advancing Grounded Reasoning in Medical Vision Language Models", "comment": null, "summary": "Current Vision-Language Models (VLMs) struggle to ground anatomical regions\nin 3D medical images and reason about them in a step-by-step manner, a key\nrequirement of real-world diagnostic assessment. This ability is essential for\naligning model outputs with the diagnostic workflows clinicians use in\npractice, enabling trustworthy clinician-AI collaboration. Existing 3D datasets\nprovide localization labels, but none support this \"grounded reasoning\"\nability. To address this gap, we introduce 3DReasonKnee, the first 3D grounded\nreasoning dataset for medical images, which provides 494k high-quality\nquintuples derived from 7,970 3D knee MRI volumes. Each quintuple includes: (1)\nthe 3D MRI volume, (2) a diagnostic question targeting a specific anatomical\nregion (3) a 3D bounding box localizing the relevant anatomical structures, (4)\nclinician-generated diagnostic reasoning steps that explicitly detail the 3D\nreasoning process, and (5) structured severity assessments for the relevant\nanatomical region. The creation and validation of 3DReasonKnee, involving over\n450 hours of expert clinician time for manually segmenting MRIs and generating\nreasoning chains, ensures its superior quality and clinical relevance. We\nestablish ReasonKnee-Bench to evaluate localization and diagnostic accuracy,\nproviding insight into VLM ability to perform grounding and severity assessment\nacross anatomical regions and diagnostic inquiries. We benchmark five\nstate-of-the-art VLMs, providing baseline performance for ReasonKnee-Bench. By\nproviding this unique resource of expert-annotated 3D reasoning pathways,\n3DReasonKnee serves as a repository of orthopedic surgeons' diagnostic\nexpertise and offers a vital testbed for advancing multimodal medical AI\nsystems towards 3D, clinically aligned, localized decision-making capabilities.\nThe dataset can be found in:\nhttps://huggingface.co/datasets/rajpurkarlab/3DReasonKnee", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e863DReasonKnee\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u76843D\u57fa\u7840\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea7,970\u4e2a3D\u819d\u5173\u8282MRI\u4f53\u79ef\u7684494k\u9ad8\u8d28\u91cf\u4e94\u5143\u7ec4\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u533b\u5b66\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u89e3\u5256\u533a\u57df\u5e76\u8fdb\u884c\u9010\u6b65\u63a8\u7406\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u533b\u5b66\u56fe\u50cf\u4e2d\u96be\u4ee5\u5b9a\u4f4d\u89e3\u5256\u533a\u57df\u5e76\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0c\u8fd9\u662f\u771f\u5b9e\u4e16\u754c\u8bca\u65ad\u8bc4\u4f30\u7684\u5173\u952e\u8981\u6c42\u3002\u73b0\u67093D\u6570\u636e\u96c6\u63d0\u4f9b\u5b9a\u4f4d\u6807\u7b7e\uff0c\u4f46\u90fd\u4e0d\u652f\u6301\u8fd9\u79cd\"\u57fa\u7840\u63a8\u7406\"\u80fd\u529b\uff0c\u65e0\u6cd5\u4e0e\u4e34\u5e8a\u533b\u751f\u7684\u8bca\u65ad\u5de5\u4f5c\u6d41\u7a0b\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u53ef\u4fe1\u8d56\u7684\u4e34\u5e8a\u533b\u751f-AI\u534f\u4f5c\u3002", "method": "\u7814\u7a76\u56e2\u961f\u521b\u5efa\u4e863DReasonKnee\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u4e94\u5143\u7ec4\u5305\u542b3D MRI\u4f53\u79ef\u3001\u9488\u5bf9\u7279\u5b9a\u89e3\u5256\u533a\u57df\u7684\u8bca\u65ad\u95ee\u9898\u3001\u5b9a\u4f4d\u76f8\u5173\u89e3\u5256\u7ed3\u6784\u76843D\u8fb9\u754c\u6846\u3001\u4e34\u5e8a\u533b\u751f\u751f\u6210\u7684\u8be6\u7ec63D\u63a8\u7406\u8fc7\u7a0b\u7684\u8bca\u65ad\u63a8\u7406\u6b65\u9aa4\uff0c\u4ee5\u53ca\u76f8\u5173\u89e3\u5256\u533a\u57df\u7684\u7ed3\u6784\u5316\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u4f30\u3002\u6570\u636e\u96c6\u521b\u5efa\u6d89\u53ca450\u591a\u4e2a\u5c0f\u65f6\u7684\u4e13\u5bb6\u4e34\u5e8a\u533b\u751f\u65f6\u95f4\u8fdb\u884c\u624b\u52a8\u5206\u5272\u548c\u751f\u6210\u63a8\u7406\u94fe\u3002", "result": "\u5efa\u7acb\u4e86ReasonKnee-Bench\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30\u5b9a\u4f4d\u548c\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u4e3a\u4e94\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u51c6\u6027\u80fd\uff0c\u4f5c\u4e3aReasonKnee-Bench\u7684\u57fa\u7ebf\u8868\u73b0\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8eVLM\u5728\u4e0d\u540c\u89e3\u5256\u533a\u57df\u548c\u8bca\u65ad\u67e5\u8be2\u4e2d\u6267\u884c\u57fa\u7840\u548c\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u4f30\u80fd\u529b\u7684\u6df1\u5165\u89c1\u89e3\u3002", "conclusion": "3DReasonKnee\u4f5c\u4e3a\u9aa8\u79d1\u5916\u79d1\u533b\u751f\u8bca\u65ad\u4e13\u4e1a\u77e5\u8bc6\u7684\u72ec\u7279\u8d44\u6e90\u5e93\uff0c\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u533b\u5b66AI\u7cfb\u7edf\u54113D\u3001\u4e34\u5e8a\u5bf9\u9f50\u7684\u5c40\u90e8\u51b3\u7b56\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u63d0\u4f9b\u4e13\u5bb6\u6ce8\u91ca\u76843D\u63a8\u7406\u8def\u5f84\uff0c\u4fc3\u8fdb\u4e86\u4e34\u5e8a\u533b\u751f-AI\u534f\u4f5c\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2510.21679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21679", "abs": "https://arxiv.org/abs/2510.21679", "authors": ["Gaku Morio", "Harri Rowlands", "Dominik Stammbach", "Christopher D. Manning", "Peter Henderson"], "title": "A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection", "comment": "Forthcoming in NeurIPS 2025 Datasets and Benchmarks Track", "summary": "Companies spend large amounts of money on public relations campaigns to\nproject a positive brand image. However, sometimes there is a mismatch between\nwhat they say and what they do. Oil & gas companies, for example, are accused\nof \"greenwashing\" with imagery of climate-friendly initiatives. Understanding\nthe framing, and changes in framing, at scale can help better understand the\ngoals and nature of public relations campaigns. To address this, we introduce a\nbenchmark dataset of expert-annotated video ads obtained from Facebook and\nYouTube. The dataset provides annotations for 13 framing types for more than 50\ncompanies or advocacy groups across 20 countries. Our dataset is especially\ndesigned for the evaluation of vision-language models (VLMs), distinguishing it\nfrom past text-only framing datasets. Baseline experiments show some promising\nresults, while leaving room for improvement for future work: GPT-4.1 can detect\nenvironmental messages with 79% F1 score, while our best model only achieves\n46% F1 score on identifying framing around green innovation. We also identify\nchallenges that VLMs must address, such as implicit framing, handling videos of\nvarious lengths, or implicit cultural backgrounds. Our dataset contributes to\nresearch in multimodal analysis of strategic communication in the energy\nsector.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u591a\u6a21\u6001\u89c6\u9891\u5e7f\u544a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u80fd\u6e90\u884c\u4e1a\u6218\u7565\u4f20\u64ad\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7eaf\u6587\u672c\u6846\u67b6\u5206\u6790\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u4f01\u4e1a\u516c\u5173\u6d3b\u52a8\u4e2d\u5b58\u5728\u8a00\u884c\u4e0d\u4e00\u7684\"\u6f02\u7eff\"\u73b0\u8c61\uff0c\u7279\u522b\u662f\u5728\u77f3\u6cb9\u5929\u7136\u6c14\u884c\u4e1a\uff0c\u9700\u8981\u5927\u89c4\u6a21\u5206\u6790\u6846\u67b6\u53d8\u5316\u6765\u7406\u89e3\u516c\u5173\u6d3b\u52a8\u7684\u76ee\u6807\u548c\u6027\u8d28\uff0c\u800c\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u7eaf\u6587\u672c\u5f62\u5f0f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u591a\u6a21\u6001\u5206\u6790\u9700\u6c42\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u89c6\u9891\u5e7f\u544a\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81eaFacebook\u548cYouTube\u7684\u5e7f\u544a\u89c6\u9891\uff0c\u6db5\u76d613\u79cd\u6846\u67b6\u7c7b\u578b\u300150\u591a\u5bb6\u516c\u53f8\u6216\u5021\u5bfc\u56e2\u4f53\u300120\u4e2a\u56fd\u5bb6\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u3002", "result": "\u57fa\u7ebf\u5b9e\u9a8c\u663e\u793aGPT-4.1\u5728\u68c0\u6d4b\u73af\u5883\u4fe1\u606f\u65b9\u9762\u8fbe\u523079%\u7684F1\u5206\u6570\uff0c\u800c\u6700\u4f73\u6a21\u578b\u5728\u8bc6\u522b\u7eff\u8272\u521b\u65b0\u6846\u67b6\u65b9\u9762\u4ec5\u8fbe\u523046%\u7684F1\u5206\u6570\uff0c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u6846\u67b6\u5206\u6790\u65b9\u9762\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u80fd\u6e90\u884c\u4e1a\u6218\u7565\u4f20\u64ad\u7684\u591a\u6a21\u6001\u5206\u6790\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9690\u5f0f\u6846\u67b6\u3001\u4e0d\u540c\u957f\u5ea6\u89c6\u9891\u548c\u9690\u542b\u6587\u5316\u80cc\u666f\u7b49\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.21069", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21069", "abs": "https://arxiv.org/abs/2510.21069", "authors": ["Pranav Saxena", "Jimmy Chiun"], "title": "ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models", "comment": null, "summary": "Understanding and reasoning about complex 3D environments requires structured\nscene representations that capture not only objects but also their semantic and\nspatial relationships. While recent works on 3D scene graph generation have\nleveraged pretrained VLMs without task-specific fine-tuning, they are largely\nconfined to single-view settings, fail to support incremental updates as new\nobservations arrive and lack explicit geometric grounding in 3D space, all of\nwhich are essential for embodied scenarios. In this paper, we propose, ZING-3D,\na framework that leverages the vast knowledge of pretrained foundation models\nto enable open-vocabulary recognition and generate a rich semantic\nrepresentation of the scene in a zero-shot manner while also enabling\nincremental updates and geometric grounding in 3D space, making it suitable for\ndownstream robotics applications. Our approach leverages VLM reasoning to\ngenerate a rich 2D scene graph, which is grounded in 3D using depth\ninformation. Nodes represent open-vocabulary objects with features, 3D\nlocations, and semantic context, while edges capture spatial and semantic\nrelations with inter-object distances. Our experiments on scenes from the\nReplica and HM3D dataset show that ZING-3D is effective at capturing spatial\nand relational knowledge without the need of task-specific training.", "AI": {"tldr": "ZING-3D\u662f\u4e00\u4e2a\u96f6\u6837\u672c3D\u573a\u666f\u56fe\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u8bc6\u522b\uff0c\u652f\u6301\u589e\u91cf\u66f4\u65b0\u548c3D\u51e0\u4f55\u63a5\u5730\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u4e0b\u6e38\u5e94\u7528\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u56fe\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u89c6\u89d2\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u652f\u6301\u65b0\u89c2\u6d4b\u7684\u589e\u91cf\u66f4\u65b0\uff0c\u4e14\u7f3a\u4e4f3D\u7a7a\u95f4\u7684\u663e\u5f0f\u51e0\u4f55\u63a5\u5730\uff0c\u8fd9\u4e9b\u9650\u5236\u5bf9\u4e8e\u5177\u8eab\u573a\u666f\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528VLM\u63a8\u7406\u751f\u6210\u4e30\u5bcc\u76842D\u573a\u666f\u56fe\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u4fe1\u606f\u5c06\u5176\u63a5\u5730\u52303D\u7a7a\u95f4\uff0c\u8282\u70b9\u8868\u793a\u5177\u6709\u7279\u5f81\u30013D\u4f4d\u7f6e\u548c\u8bed\u4e49\u4e0a\u4e0b\u6587\u7684\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\uff0c\u8fb9\u6355\u83b7\u7a7a\u95f4\u548c\u8bed\u4e49\u5173\u7cfb\u53ca\u7269\u4f53\u95f4\u8ddd\u79bb\u3002", "result": "\u5728Replica\u548cHM3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cZING-3D\u80fd\u591f\u6709\u6548\u6355\u83b7\u7a7a\u95f4\u548c\u5173\u7cfb\u77e5\u8bc6\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u573a\u666f\u7406\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u57283D\u573a\u666f\u7406\u89e3\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u4e3a\u96f6\u6837\u672c3D\u573a\u666f\u56fe\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6\u73af\u5883\u7406\u89e3\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.21083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21083", "abs": "https://arxiv.org/abs/2510.21083", "authors": ["Youssef Megahed", "Atallah Madi", "Dina El Demellawy", "Adrian D. C. Chan"], "title": "Knowledge-Driven Vision-Language Model for Plexus Detection in Hirschsprung's Disease", "comment": "Accepted into the ICAAI 2025 - The 9th International Conference on\n  Advances in Artificial Intelligence", "summary": "Hirschsprung's disease is defined as the congenital absence of ganglion cells\nin some segment(s) of the colon. The muscle cannot make coordinated movements\nto propel stool in that section, most commonly leading to obstruction. The\ndiagnosis and treatment for this disease require a clear identification of\ndifferent region(s) of the myenteric plexus, where ganglion cells should be\npresent, on the microscopic view of the tissue slide. While deep learning\napproaches, such as Convolutional Neural Networks, have performed very well in\nthis task, they are often treated as black boxes, with minimal understanding\ngained from them, and may not conform to how a physician makes decisions. In\nthis study, we propose a novel framework that integrates expert-derived textual\nconcepts into a Contrastive Language-Image Pre-training-based vision-language\nmodel to guide plexus classification. Using prompts derived from expert sources\n(e.g., medical textbooks and papers) generated by large language models and\nreviewed by our team before being encoded with QuiltNet, our approach aligns\nclinically relevant semantic cues with visual features. Experimental results\nshow that the proposed model demonstrated superior discriminative capability\nacross different classification metrics as it outperformed CNN-based models,\nincluding VGG-19, ResNet-18, and ResNet-50; achieving an accuracy of 83.9%, a\nprecision of 86.6%, and a specificity of 87.6%. These findings highlight the\npotential of multi-modal learning in histopathology and underscore the value of\nincorporating expert knowledge for more clinically relevant model outputs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u4e13\u5bb6\u77e5\u8bc6\u9a71\u52a8\u7684\u6587\u672c\u6982\u5ff5\u96c6\u6210\u5230\u57fa\u4e8e\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u7528\u4e8e\u6307\u5bfcHirschsprung\u75c5\u80a0\u808c\u5c42\u795e\u7ecf\u4e1b\u5206\u7c7b\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5206\u7c7b\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfCNN\u6a21\u578b\uff0c\u5b9e\u73b0\u4e8683.9%\u7684\u51c6\u786e\u7387\u300186.6%\u7684\u7cbe\u786e\u7387\u548c87.6%\u7684\u7279\u5f02\u6027\u3002", "motivation": "Hirschsprung\u75c5\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u9700\u8981\u6e05\u6670\u8bc6\u522b\u7ec4\u7ec7\u5207\u7247\u4e2d\u808c\u5c42\u795e\u7ecf\u4e1b\u7684\u4e0d\u540c\u533a\u57df\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5982\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u901a\u5e38\u88ab\u89c6\u4e3a\u9ed1\u7bb1\u6a21\u578b\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u4e0d\u7b26\u5408\u533b\u751f\u7684\u51b3\u7b56\u65b9\u5f0f\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e13\u5bb6\u6765\u6e90\uff08\u5982\u533b\u5b66\u6559\u79d1\u4e66\u548c\u8bba\u6587\uff09\u7684\u63d0\u793a\u8bcd\uff0c\u7ecf\u56e2\u961f\u5ba1\u6838\u540e\u4f7f\u7528QuiltNet\u7f16\u7801\uff0c\u5c06\u4e34\u5e8a\u76f8\u5173\u8bed\u4e49\u7ebf\u7d22\u4e0e\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\uff0c\u96c6\u6210\u5230\u57fa\u4e8e\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u6307\u5bfc\u795e\u7ecf\u4e1b\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u4e0d\u540c\u5206\u7c7b\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5224\u522b\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eCNN\u7684\u6a21\u578b\uff08\u5305\u62ecVGG-19\u3001ResNet-18\u548cResNet-50\uff09\uff0c\u5b9e\u73b0\u4e8683.9%\u7684\u51c6\u786e\u7387\u300186.6%\u7684\u7cbe\u786e\u7387\u548c87.6%\u7684\u7279\u5f02\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u4e86\u6574\u5408\u4e13\u5bb6\u77e5\u8bc6\u4ee5\u83b7\u5f97\u66f4\u5177\u4e34\u5e8a\u76f8\u5173\u6027\u7684\u6a21\u578b\u8f93\u51fa\u7684\u4ef7\u503c\uff0c\u4e3a\u5f00\u53d1\u66f4\u7b26\u5408\u4e34\u5e8a\u5b9e\u8df5\u7684\u53ef\u89e3\u91caAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2510.21111", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21111", "abs": "https://arxiv.org/abs/2510.21111", "authors": ["Weijie Zhou", "Xuantang Xiong", "Yi Peng", "Manli Tao", "Chaoyang Zhao", "Honghui Dong", "Ming Tang", "Jinqiao Wang"], "title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments", "comment": "39th Conference on Neural Information Processing Systemss (NeurIPS\n  2025)", "summary": "Visual reasoning in multimodal large language models (MLLMs) has primarily\nbeen studied in static, fully observable settings, limiting their effectiveness\nin real-world environments where information is often incomplete due to\nocclusion or limited field of view. Humans, in contrast, actively explore and\ninteract with their environment-moving, examining, and manipulating objects-to\ngather information through a closed-loop process integrating perception,\nreasoning, and action. Inspired by this human capability, we introduce the\nActive Visual Reasoning (AVR) task, extending visual reasoning to partially\nobservable, interactive environments. AVR necessitates agents to: (1) actively\nacquire information via sequential physical actions, (2) integrate observations\nacross multiple steps for coherent reasoning, and (3) dynamically adjust\ndecisions based on evolving visual feedback. To rigorously evaluate AVR, we\nintroduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive\nenvironments designed to assess both reasoning correctness and\ninformation-gathering efficiency. We present AVR-152k, a large-scale dataset\nthat offers rich Chain-of-Thought (CoT) annotations detailing iterative\nreasoning for uncertainty identification, action-conditioned information gain\nprediction, and information-maximizing action selection, crucial for training\nagents in a higher-order Markov Decision Process. Building on this, we develop\nPhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR,\nembodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath,\nGeometry30K). Our analysis also reveals that current embodied MLLMs, despite\ndetecting information incompleteness, struggle to actively acquire and\nintegrate new information through interaction, highlighting a fundamental gap\nin active reasoning capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e3b\u52a8\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\uff0c\u5c06\u89c6\u89c9\u63a8\u7406\u6269\u5c55\u5230\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u4ea4\u4e92\u73af\u5883\u4e2d\uff0c\u5e76\u5f00\u53d1\u4e86PhysVLM-AVR\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u52a8\u83b7\u53d6\u548c\u6574\u5408\u4fe1\u606f\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u9759\u6001\u3001\u5b8c\u5168\u53ef\u89c2\u5bdf\u7684\u73af\u5883\uff0c\u8fd9\u4e0e\u73b0\u5b9e\u4e16\u754c\u4e2d\u4fe1\u606f\u5e38\u56e0\u906e\u6321\u6216\u89c6\u91ce\u9650\u5236\u800c\u4e0d\u5b8c\u6574\u7684\u5b9e\u9645\u60c5\u51b5\u5b58\u5728\u5dee\u8ddd\u3002\u4eba\u7c7b\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u548c\u4ea4\u4e92\u6765\u6536\u96c6\u4fe1\u606f\u7684\u95ed\u73af\u8fc7\u7a0b\u542f\u53d1\u4e86\u672c\u7814\u7a76\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u8fdb\u884c\u4e3b\u52a8\u63a8\u7406\u7684\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e3b\u52a8\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86CLEVR-AVR\u4eff\u771f\u57fa\u51c6\u6765\u8bc4\u4f30\u63a8\u7406\u6b63\u786e\u6027\u548c\u4fe1\u606f\u6536\u96c6\u6548\u7387\u3002\u6784\u5efa\u4e86\u5305\u542b15.2\u4e07\u6837\u672c\u7684AVR-152k\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u601d\u7ef4\u94fe\u6807\u6ce8\uff0c\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u8bc6\u522b\u3001\u52a8\u4f5c\u6761\u4ef6\u4fe1\u606f\u589e\u76ca\u9884\u6d4b\u548c\u4fe1\u606f\u6700\u5927\u5316\u52a8\u4f5c\u9009\u62e9\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86PhysVLM-AVR\u6a21\u578b\uff0c\u5728\u9ad8\u7ea7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u8bad\u7ec3\u667a\u80fd\u4f53\u3002", "result": "PhysVLM-AVR\u5728CLEVR-AVR\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5177\u8eab\u63a8\u7406\u548c\u88ab\u52a8\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\u3002\u5206\u6790\u53d1\u73b0\u5f53\u524d\u5177\u8eab\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u591f\u68c0\u6d4b\u4fe1\u606f\u4e0d\u5b8c\u6574\u6027\uff0c\u4f46\u5728\u901a\u8fc7\u4ea4\u4e92\u4e3b\u52a8\u83b7\u53d6\u548c\u6574\u5408\u65b0\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u663e\u8457\u56f0\u96be\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u52a8\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u6839\u672c\u6027\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u8fdb\u884c\u95ed\u73af\u611f\u77e5-\u63a8\u7406-\u884c\u52a8\u96c6\u6210\u7684\u91cd\u8981\u6027\u3002\u63d0\u51fa\u7684\u4e3b\u52a8\u89c6\u89c9\u63a8\u7406\u6846\u67b6\u4e3a\u5f00\u53d1\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8ba4\u77e5\u80fd\u529b\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2510.21120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21120", "abs": "https://arxiv.org/abs/2510.21120", "authors": ["Alec Helbling", "Shruti Palaskar", "Kundan Krishna", "Polo Chau", "Leon Gatys", "Joseph Yitan Cheng"], "title": "SafetyPairs: Isolating Safety Critical Image Features with Counterfactual Image Generation", "comment": null, "summary": "What exactly makes a particular image unsafe? Systematically differentiating\nbetween benign and problematic images is a challenging problem, as subtle\nchanges to an image, such as an insulting gesture or symbol, can drastically\nalter its safety implications. However, existing image safety datasets are\ncoarse and ambiguous, offering only broad safety labels without isolating the\nspecific features that drive these differences. We introduce SafetyPairs, a\nscalable framework for generating counterfactual pairs of images, that differ\nonly in the features relevant to the given safety policy, thus flipping their\nsafety label. By leveraging image editing models, we make targeted changes to\nimages that alter their safety labels while leaving safety-irrelevant details\nunchanged. Using SafetyPairs, we construct a new safety benchmark, which serves\nas a powerful source of evaluation data that highlights weaknesses in\nvision-language models' abilities to distinguish between subtly different\nimages. Beyond evaluation, we find our pipeline serves as an effective data\naugmentation strategy that improves the sample efficiency of training\nlightweight guard models. We release a benchmark containing over 3,020\nSafetyPair images spanning a diverse taxonomy of 9 safety categories, providing\nthe first systematic resource for studying fine-grained image safety\ndistinctions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SafetyPairs\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u4ec5\u5b89\u5168\u76f8\u5173\u7279\u5f81\u4e0d\u540c\u7684\u53cd\u4e8b\u5b9e\u56fe\u50cf\u5bf9\u6765\u7cfb\u7edf\u7814\u7a76\u56fe\u50cf\u5b89\u5168\u6027\uff0c\u6784\u5efa\u4e86\u5305\u542b3020\u5bf9\u56fe\u50cf\u7684\u5b89\u5168\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u5fae\u5b89\u5168\u533a\u5206\u4e0a\u7684\u5f31\u70b9\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5b89\u5168\u6570\u636e\u96c6\u5b58\u5728\u7c97\u7cd9\u548c\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u4ec5\u63d0\u4f9b\u5e7f\u6cdb\u7684\u5b89\u5168\u6807\u7b7e\u800c\u672a\u9694\u79bb\u9a71\u52a8\u5b89\u5168\u5dee\u5f02\u7684\u5177\u4f53\u7279\u5f81\uff0c\u96be\u4ee5\u7cfb\u7edf\u533a\u5206\u826f\u6027\u56fe\u50cf\u548c\u95ee\u9898\u56fe\u50cf\uff0c\u7279\u522b\u662f\u5f53\u56fe\u50cf\u7684\u7ec6\u5fae\u53d8\u5316\uff08\u5982\u4fae\u8fb1\u6027\u624b\u52bf\u6216\u7b26\u53f7\uff09\u4f1a\u5f7b\u5e95\u6539\u53d8\u5176\u5b89\u5168\u542b\u4e49\u65f6\u3002", "method": "\u63d0\u51fa\u4e86SafetyPairs\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5229\u7528\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5bf9\u56fe\u50cf\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u6539\uff0c\u4ec5\u6539\u53d8\u4e0e\u7ed9\u5b9a\u5b89\u5168\u7b56\u7565\u76f8\u5173\u7684\u7279\u5f81\u4ece\u800c\u7ffb\u8f6c\u5b89\u5168\u6807\u7b7e\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u65e0\u5173\u7ec6\u8282\u4e0d\u53d8\uff0c\u6784\u5efa\u4e86\u6db5\u76d69\u4e2a\u5b89\u5168\u7c7b\u522b\u7684\u591a\u6837\u5316\u5206\u7c7b\u4f53\u7cfb\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b3020\u5bf9SafetyPair\u56fe\u50cf\u7684\u65b0\u5b89\u5168\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u4f5c\u4e3a\u5f3a\u5927\u7684\u8bc4\u4f30\u6570\u636e\u6e90\uff0c\u7a81\u663e\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533a\u5206\u7ec6\u5fae\u4e0d\u540c\u56fe\u50cf\u80fd\u529b\u4e0a\u7684\u5f31\u70b9\uff1b\u540c\u65f6\u53d1\u73b0\u8be5\u6d41\u6c34\u7ebf\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u7b56\u7565\u80fd\u6709\u6548\u63d0\u9ad8\u8f7b\u91cf\u7ea7\u9632\u62a4\u6a21\u578b\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u7814\u7a76\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5b89\u5168\u533a\u5206\u7684\u8d44\u6e90\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5b89\u5168\u654f\u611f\u7279\u5f81\u8bc6\u522b\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u4e0d\u4ec5\u53ef\u7528\u4e8e\u6a21\u578b\u8bc4\u4f30\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u63d0\u5347\u5b89\u5168\u9632\u62a4\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002"}}
{"id": "2510.21122", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21122", "abs": "https://arxiv.org/abs/2510.21122", "authors": ["Longtian Qiu", "Shan Ning", "Jiaxuan Sun", "Xuming He"], "title": "NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation", "comment": "Accepted by Neurips2025, Project page at at\n  https://artanic30.github.io/project_pages/NoisyGRPO/", "summary": "Reinforcement learning (RL) has shown promise in enhancing the general\nChain-of-Thought (CoT) reasoning capabilities of multimodal large language\nmodels (MLLMs). However, when applied to improve general CoT reasoning,\nexisting RL frameworks often struggle to generalize beyond the training\ndistribution. To address this, we propose NoisyGRPO, a systematic multimodal RL\nframework that introduces controllable noise into visual inputs for enhanced\nexploration and explicitly models the advantage estimation process via a\nBayesian framework. Specifically, NoisyGRPO improves RL training by: (1)\n\\textbf{Noise-Injected Exploration Policy}: Perturbing visual inputs with\nGaussian noise to encourage exploration across a wider range of visual\nscenarios; and (2) \\textbf{Bayesian Advantage Estimation}: Formulating\nadvantage estimation as a principled Bayesian inference problem, where the\ninjected noise level serves as a prior and the observed trajectory reward as\nthe likelihood. This Bayesian modeling fuses both sources of information to\ncompute a robust posterior estimate of trajectory advantage, effectively\nguiding MLLMs to prefer visually grounded trajectories over noisy ones.\nExperiments on standard CoT quality, general capability, and hallucination\nbenchmarks demonstrate that NoisyGRPO substantially improves generalization and\nrobustness, especially in RL settings with small-scale MLLMs such as Qwen2.5-VL\n3B. The project page is available at\n\\href{https://artanic30.github.io/project_pages/NoisyGRPO/}{\\texttt{https://artanic30.github.io/project\\_pages/NoisyGRPO}}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NoisyGRPO\uff0c\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5411\u89c6\u89c9\u8f93\u5165\u6ce8\u5165\u53ef\u63a7\u566a\u58f0\u589e\u5f3a\u63a2\u7d22\uff0c\u5e76\u91c7\u7528\u8d1d\u53f6\u65af\u6846\u67b6\u663e\u5f0f\u5efa\u6a21\u4f18\u52bf\u4f30\u8ba1\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u80fd\u529b\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u8d85\u8d8a\u8bad\u7ec3\u5206\u5e03\u7684\u6cdb\u5316\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u590d\u6742\u89c6\u89c9\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6548\u679c\u548c\u7a33\u5b9a\u6027\u3002", "method": "NoisyGRPO\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6280\u672f\uff1a\u566a\u58f0\u6ce8\u5165\u63a2\u7d22\u7b56\u7565\u901a\u8fc7\u5728\u89c6\u89c9\u8f93\u5165\u4e2d\u6dfb\u52a0\u9ad8\u65af\u566a\u58f0\u6765\u9f13\u52b1\u6a21\u578b\u5728\u66f4\u5e7f\u6cdb\u7684\u89c6\u89c9\u573a\u666f\u4e2d\u8fdb\u884c\u63a2\u7d22\uff1b\u8d1d\u53f6\u65af\u4f18\u52bf\u4f30\u8ba1\u5c06\u4f18\u52bf\u4f30\u8ba1\u5efa\u6a21\u4e3a\u8d1d\u53f6\u65af\u63a8\u7406\u95ee\u9898\uff0c\u5176\u4e2d\u6ce8\u5165\u7684\u566a\u58f0\u6c34\u5e73\u4f5c\u4e3a\u5148\u9a8c\uff0c\u89c2\u5bdf\u5230\u7684\u8f68\u8ff9\u5956\u52b1\u4f5c\u4e3a\u4f3c\u7136\uff0c\u901a\u8fc7\u878d\u5408\u4e24\u79cd\u4fe1\u606f\u6e90\u8ba1\u7b97\u8f68\u8ff9\u4f18\u52bf\u7684\u7a33\u5065\u540e\u9a8c\u4f30\u8ba1\u3002", "result": "\u5728\u6807\u51c6\u94fe\u5f0f\u601d\u7ef4\u8d28\u91cf\u3001\u901a\u7528\u80fd\u529b\u548c\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNoisyGRPO\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u5c0f\u89c4\u6a21\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982Qwen2.5-VL 3B\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u7ed3\u5408\u566a\u58f0\u6ce8\u5165\u548c\u8d1d\u53f6\u65af\u5efa\u6a21\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u6311\u6218\uff0c\u4e3a\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u548c\u7406\u8bba\u652f\u6491\u3002"}}
{"id": "2510.21346", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21346", "abs": "https://arxiv.org/abs/2510.21346", "authors": ["Lemin Liu", "Fangchao Hu", "Honghua Jiang", "Yaru Chen", "Limin Liu", "Yongliang Qiao"], "title": "CT-CLIP: A Multi-modal Fusion Framework for Robust Apple Leaf Disease Recognition in Complex Environments", "comment": null, "summary": "In complex orchard environments, the phenotypic heterogeneity of different\napple leaf diseases, characterized by significant variation among lesions,\nposes a challenge to traditional multi-scale feature fusion methods. These\nmethods only integrate multi-layer features extracted by convolutional neural\nnetworks (CNNs) and fail to adequately account for the relationships between\nlocal and global features. Therefore, this study proposes a multi-branch\nrecognition framework named CNN-Transformer-CLIP (CT-CLIP). The framework\nsynergistically employs a CNN to extract local lesion detail features and a\nVision Transformer to capture global structural relationships. An Adaptive\nFeature Fusion Module (AFFM) then dynamically fuses these features, achieving\noptimal coupling of local and global information and effectively addressing the\ndiversity in lesion morphology and distribution. Additionally, to mitigate\ninterference from complex backgrounds and significantly enhance recognition\naccuracy under few-shot conditions, this study proposes a multimodal image-text\nlearning approach. By leveraging pre-trained CLIP weights, it achieves deep\nalignment between visual features and disease semantic descriptions.\nExperimental results show that CT-CLIP achieves accuracies of 97.38% and 96.12%\non a publicly available apple disease and a self-built dataset, outperforming\nseveral baseline methods. The proposed CT-CLIP demonstrates strong capabilities\nin recognizing agricultural diseases, significantly enhances identification\naccuracy under complex environmental conditions, provides an innovative and\npractical solution for automated disease recognition in agricultural\napplications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCT-CLIP\u7684\u591a\u5206\u652f\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u4f7f\u7528CNN\u548cVision Transformer\u5e76\u7ed3\u5408CLIP\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u679c\u56ed\u73af\u5883\u4e2d\u82f9\u679c\u53f6\u90e8\u75c5\u5bb3\u8868\u578b\u5f02\u8d28\u6027\u5e26\u6765\u7684\u8bc6\u522b\u6311\u6218\u3002", "motivation": "\u5728\u590d\u6742\u679c\u56ed\u73af\u5883\u4e2d\uff0c\u4e0d\u540c\u82f9\u679c\u53f6\u90e8\u75c5\u5bb3\u7684\u8868\u578b\u5f02\u8d28\u6027\u8868\u73b0\u4e3a\u75c5\u53d8\u533a\u57df\u7684\u663e\u8457\u53d8\u5f02\uff0c\u8fd9\u5bf9\u4f20\u7edf\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u6784\u6210\u4e86\u6311\u6218\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4ec5\u6574\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u7684\u591a\u5c42\u7279\u5f81\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86CNN-Transformer-CLIP\uff08CT-CLIP\uff09\u591a\u5206\u652f\u8bc6\u522b\u6846\u67b6\uff0c\u534f\u540c\u4f7f\u7528CNN\u63d0\u53d6\u5c40\u90e8\u75c5\u53d8\u7ec6\u8282\u7279\u5f81\u548cVision Transformer\u6355\u6349\u5168\u5c40\u7ed3\u6784\u5173\u7cfb\u3002\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08AFFM\uff09\u52a8\u6001\u878d\u5408\u8fd9\u4e9b\u7279\u5f81\uff0c\u5b9e\u73b0\u5c40\u90e8\u4e0e\u5168\u5c40\u4fe1\u606f\u7684\u6700\u4f18\u8026\u5408\u3002\u540c\u65f6\u91c7\u7528\u591a\u6a21\u6001\u56fe\u6587\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684CLIP\u6743\u91cd\u5b9e\u73b0\u89c6\u89c9\u7279\u5f81\u4e0e\u75c5\u5bb3\u8bed\u4e49\u63cf\u8ff0\u7684\u6df1\u5ea6\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCT-CLIP\u5728\u516c\u5f00\u82f9\u679c\u75c5\u5bb3\u6570\u636e\u96c6\u548c\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523097.38%\u548c96.12%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u5728\u590d\u6742\u73af\u5883\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684CT-CLIP\u5728\u519c\u4e1a\u75c5\u5bb3\u8bc6\u522b\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u4e3a\u519c\u4e1a\u5e94\u7528\u4e2d\u7684\u81ea\u52a8\u5316\u75c5\u5bb3\u8bc6\u522b\u63d0\u4f9b\u4e86\u521b\u65b0\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u7814\u7a76\u4e3a\u5904\u7406\u8868\u578b\u5f02\u8d28\u6027\u75c5\u5bb3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u3002"}}
{"id": "2510.21160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21160", "abs": "https://arxiv.org/abs/2510.21160", "authors": ["Guanlin Wu", "Boyan Su", "Yang Zhao", "Pu Wang", "Yichen Lin", "Hao Frank Yang"], "title": "Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study", "comment": "NeurIPS 2025 (Spotlight)", "summary": "How to integrate and verify spatial intelligence in foundation models remains\nan open challenge. Current practice often proxies Visual-Spatial Intelligence\n(VSI) with purely textual prompts and VQA-style scoring, which obscures\ngeometry, invites linguistic shortcuts, and weakens attribution to genuinely\nspatial skills. We introduce Spatial Intelligence Grid (SIG): a structured,\ngrid-based schema that explicitly encodes object layouts, inter-object\nrelations, and physically grounded priors. As a complementary channel to text,\nSIG provides a faithful, compositional representation of scene structure for\nfoundation-model reasoning. Building on SIG, we derive SIG-informed evaluation\nmetrics that quantify a model's intrinsic VSI, which separates spatial\ncapability from language priors. In few-shot in-context learning with\nstate-of-the-art multimodal LLMs (e.g. GPT- and Gemini-family models), SIG\nyields consistently larger, more stable, and more comprehensive gains across\nall VSI metrics compared to VQA-only representations, indicating its promise as\na data-labeling and training schema for learning VSI. We also release SIGBench,\na benchmark of 1.4K driving frames annotated with ground-truth SIG labels and\nhuman gaze traces, supporting both grid-based machine VSI tasks and\nattention-driven, human-like VSI tasks in autonomous-driving scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7a7a\u95f4\u667a\u80fd\u7f51\u683c\uff08SIG\uff09\uff0c\u4e00\u79cd\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u7f51\u683c\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u663e\u5f0f\u7f16\u7801\u7a7a\u95f4\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u4e86SIG-informed\u8bc4\u4f30\u6307\u6807\u6765\u91cf\u5316\u6a21\u578b\u7684\u5185\u5728\u89c6\u89c9\u7a7a\u95f4\u667a\u80fd\uff0c\u6709\u6548\u5206\u79bb\u7a7a\u95f4\u80fd\u529b\u4e0e\u8bed\u8a00\u5148\u9a8c\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u4e2d\u7a7a\u95f4\u667a\u80fd\u7684\u96c6\u6210\u4e0e\u9a8c\u8bc1\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u7eaf\u6587\u672c\u63d0\u793a\u548cVQA\u5f0f\u8bc4\u5206\u6765\u4ee3\u7406\u89c6\u89c9\u7a7a\u95f4\u667a\u80fd\uff0c\u8fd9\u63a9\u76d6\u4e86\u51e0\u4f55\u4fe1\u606f\u3001\u5f15\u5165\u4e86\u8bed\u8a00\u6377\u5f84\uff0c\u5e76\u524a\u5f31\u4e86\u5bf9\u771f\u6b63\u7a7a\u95f4\u6280\u80fd\u7684\u5f52\u56e0\u3002", "method": "\u63d0\u51fa\u4e86\u7a7a\u95f4\u667a\u80fd\u7f51\u683c\uff08SIG\uff09\u4f5c\u4e3a\u6587\u672c\u7684\u8865\u5145\u901a\u9053\uff0c\u8fd9\u662f\u4e00\u79cd\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u7f51\u683c\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u5f0f\u7f16\u7801\u7269\u4f53\u5e03\u5c40\u3001\u7269\u4f53\u95f4\u5173\u7cfb\u4ee5\u53ca\u7269\u7406\u57fa\u7840\u5148\u9a8c\uff0c\u4e3a\u57fa\u4e8eSIG\u7684\u8bc4\u4f30\u6307\u6807\u63d0\u4f9b\u4e86\u5fe0\u5b9e\u3001\u7ec4\u5408\u5f0f\u7684\u573a\u666f\u7ed3\u6784\u8868\u793a\u3002", "result": "\u5728\u5c11\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\uff0cSIG\u76f8\u6bd4\u7eafVQA\u8868\u793a\u5728\u6240\u6709VSI\u6307\u6807\u4e0a\u5747\u4ea7\u751f\u66f4\u4e00\u81f4\u3001\u66f4\u7a33\u5b9a\u4e14\u66f4\u5168\u9762\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8868\u660e\u5176\u4f5c\u4e3a\u6570\u636e\u6807\u6ce8\u548c\u8bad\u7ec3\u6a21\u5f0f\u7528\u4e8e\u5b66\u4e60\u89c6\u89c9\u7a7a\u95f4\u667a\u80fd\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u5305\u542b1.4K\u9a7e\u9a76\u5e27\u7684SIGBench\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "SIG\u4e3a\u5b66\u4e60\u89c6\u89c9\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6570\u636e\u6807\u6ce8\u548c\u8bad\u7ec3\u6a21\u5f0f\uff0c\u5176\u7ed3\u6784\u5316\u8868\u793a\u80fd\u591f\u6709\u6548\u5206\u79bb\u6a21\u578b\u7684\u7a7a\u95f4\u80fd\u529b\u4e0e\u8bed\u8a00\u5148\u9a8c\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b49\u573a\u666f\u4e2d\u7684\u673a\u5668VSI\u4efb\u52a1\u548c\u4eba\u7c7b\u6ce8\u610f\u529b\u9a71\u52a8\u7684VSI\u4efb\u52a1\u63d0\u4f9b\u4e86\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2510.21171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21171", "abs": "https://arxiv.org/abs/2510.21171", "authors": ["Qihang Zhou", "Binbin Gao", "Guansong Pang", "Xin Wang", "Jiming Chen", "Shibo He"], "title": "TokenCLIP: Token-wise Prompt Learning for Zero-shot Anomaly Detection", "comment": null, "summary": "Adapting CLIP for anomaly detection on unseen objects has shown strong\npotential in a zero-shot manner. However, existing methods typically rely on a\nsingle textual space to align with visual semantics across diverse objects and\ndomains. The indiscriminate alignment hinders the model from accurately\ncapturing varied anomaly semantics. We propose TokenCLIP, a token-wise\nadaptation framework that enables dynamic alignment between visual and\nlearnable textual spaces for fine-grained anomaly learning. Rather than mapping\nall visual tokens to a single, token-agnostic textual space, TokenCLIP aligns\neach token with a customized textual subspace that represents its visual\ncharacteristics. Explicitly assigning a unique learnable textual space to each\ntoken is computationally intractable and prone to insufficient optimization. We\ninstead expand the token-agnostic textual space into a set of orthogonal\nsubspaces, and then dynamically assign each token to a subspace combination\nguided by semantic affinity, which jointly supports customized and efficient\ntoken-wise adaptation. To this end, we formulate dynamic alignment as an\noptimal transport problem, where all visual tokens in an image are transported\nto textual subspaces based on semantic similarity. The transport constraints of\nOT ensure sufficient optimization across subspaces and encourage them to focus\non different semantics. Solving the problem yields a transport plan that\nadaptively assigns each token to semantically relevant subspaces. A top-k\nmasking is then applied to sparsify the plan and specialize subspaces for\ndistinct visual regions. Extensive experiments demonstrate the superiority of\nTokenCLIP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTokenCLIP\uff0c\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u4ee4\u724c\u7ea7\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5bf9\u9f50\u89c6\u89c9\u4ee4\u724c\u4e0e\u53ef\u5b66\u4e60\u6587\u672c\u5b50\u7a7a\u95f4\u6765\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5f02\u5e38\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u4e00\u6587\u672c\u7a7a\u95f4\u4e2d\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u591a\u6837\u5316\u5f02\u5e38\u8bed\u4e49\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u6587\u672c\u7a7a\u95f4\u6765\u5bf9\u9f50\u4e0d\u540c\u7269\u4f53\u548c\u9886\u57df\u7684\u89c6\u89c9\u8bed\u4e49\uff0c\u8fd9\u79cd\u65e0\u5dee\u522b\u7684\u5bf9\u9f50\u65b9\u5f0f\u963b\u788d\u4e86\u6a21\u578b\u51c6\u786e\u6355\u6349\u591a\u6837\u5316\u7684\u5f02\u5e38\u8bed\u4e49\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u5bf9\u9f50\u673a\u5236\u6765\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "method": "TokenCLIP\u5c06\u4ee4\u724c\u65e0\u5173\u7684\u6587\u672c\u7a7a\u95f4\u6269\u5c55\u4e3a\u4e00\u7ec4\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u52a8\u6001\u5206\u914d\u6bcf\u4e2a\u89c6\u89c9\u4ee4\u724c\u5230\u8bed\u4e49\u76f8\u5173\u7684\u5b50\u7a7a\u95f4\u7ec4\u5408\uff0c\u5e76\u5e94\u7528top-k\u63a9\u7801\u6765\u7a00\u758f\u5316\u4f20\u8f93\u8ba1\u5212\uff0c\u4f7f\u4e0d\u540c\u5b50\u7a7a\u95f4\u4e13\u6ce8\u4e8e\u4e0d\u540c\u7684\u89c6\u89c9\u533a\u57df\u8bed\u4e49\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86TokenCLIP\u7684\u4f18\u8d8a\u6027\uff0c\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u548c\u5b9a\u4f4d\u5404\u79cd\u5f02\u5e38\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u4ee4\u724c\u7ea7\u7684\u52a8\u6001\u6587\u672c-\u89c6\u89c9\u5bf9\u9f50\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u7684\u7ec6\u7c92\u5ea6\u6027\u80fd\uff0c\u6700\u4f18\u4f20\u8f93\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u9002\u5e94\u673a\u5236\uff0c\u4e3a\u672a\u6765\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.21501", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21501", "abs": "https://arxiv.org/abs/2510.21501", "authors": ["Guanghao Zheng", "Bowen Shi", "Mingxing Xu", "Ruoyu Sun", "Peisen Zhao", "Zhibo Zhang", "Wenrui Dai", "Junni Zou", "Hongkai Xiong", "Xiaopeng Zhang", "Qi Tian"], "title": "GranViT: A Fine-Grained Vision Model With Autoregressive Perception For MLLMs", "comment": "21 pages, 6 figures", "summary": "Vision encoders are indispensable for allowing impressive performance of\nMulti-modal Large Language Models (MLLMs) in vision language tasks such as\nvisual question answering and reasoning. However, existing vision encoders\nfocus on global image representations but overlook fine-grained regional\nanalysis. They are limited in fine grained perception due to the scarcity of\nfine grained annotated data and the lack of a fine grained pre-training\nparadigm. In this paper, we propose GranViT, a novel Vision Transformer that\nintegrates fine-grained feature extraction with semantic alignment to Large\nLanguage Models (LLMs) via region level autoregressive training. We first\nconstruct Gran-29M, a dataset comprising 2million natural and OCR images paired\nwith over 180 million high-quality region-level annotations, to enable large\nscale fine grained pretraining. Consequently, we develop a\npretraining-adaptation framework along with a self distillation mechanism to\ntrain fine-grained GranViT on Gran-29M. We sufficiently exploit the\nfine-grained annotations from Gran-29M to resort to bounding-box-to-caption\nregression to enhance localized visual representation of the vision encoder in\nthe pretraining and caption-to-bounding-box regression to improve vision\nfeature utilization and localization for LLM in the adaptation. We further\nincorporate a self distillation mechanism that imposes explicit localization\nconstraints on the vision encoder to strengthen its regional reasoning\ncapability. Extensive experiments show that GranViT surpasses existing vision\nencoders and attains strong transferability to varying LLMs. Remarkably, it\nachieves state-of-the-art results on fine-grained recognition, multimodal VQA,\nand OCR understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGranViT\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9Transformer\uff0c\u901a\u8fc7\u533a\u57df\u7ea7\u81ea\u56de\u5f52\u8bad\u7ec3\u5c06\u7ec6\u7c92\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bed\u4e49\u5bf9\u9f50\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u7f16\u7801\u5668\u5728\u7ec6\u7c92\u5ea6\u611f\u77e5\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u7f16\u7801\u5668\u4e3b\u8981\u5173\u6ce8\u5168\u5c40\u56fe\u50cf\u8868\u793a\u800c\u5ffd\u89c6\u7ec6\u7c92\u5ea6\u533a\u57df\u5206\u6790\uff0c\u7531\u4e8e\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u6807\u6ce8\u6570\u636e\u548c\u4e13\u95e8\u7684\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b200\u4e07\u5f20\u81ea\u7136\u548cOCR\u56fe\u50cf\u53ca1.8\u4ebf\u9ad8\u8d28\u91cf\u533a\u57df\u7ea7\u6807\u6ce8\u7684Gran-29M\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u9884\u8bad\u7ec3-\u9002\u5e94\u6846\u67b6\u7ed3\u5408\u81ea\u84b8\u998f\u673a\u5236\uff0c\u901a\u8fc7\u8fb9\u754c\u6846\u5230\u6807\u9898\u56de\u5f52\u589e\u5f3a\u89c6\u89c9\u7f16\u7801\u5668\u7684\u5c40\u90e8\u8868\u793a\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u6807\u9898\u5230\u8fb9\u754c\u6846\u56de\u5f52\u63d0\u5347LLM\u7684\u89c6\u89c9\u7279\u5f81\u5229\u7528\u548c\u5b9a\u4f4d\u80fd\u529b\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660eGranViT\u8d85\u8d8a\u4e86\u73b0\u6709\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5728\u4e0d\u540cLLM\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u3001\u591a\u6a21\u6001VQA\u548cOCR\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u5927\u89c4\u6a21\u7ec6\u7c92\u5ea6\u9884\u8bad\u7ec3\u548c\u533a\u57df\u7ea7\u81ea\u56de\u5f52\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u89c6\u89c9\u7f16\u7801\u5668\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u7406\u89e3\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.21583", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21583", "abs": "https://arxiv.org/abs/2510.21583", "authors": ["Yifu Luo", "Penghui Du", "Bo Li", "Sinan Du", "Tiantian Zhang", "Yongzhe Chang", "Kai Wu", "Kun Gai", "Xueqian Wang"], "title": "Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation", "comment": "11 pages, preprint", "summary": "Group Relative Policy Optimization (GRPO) has shown strong potential for\nflow-matching-based text-to-image (T2I) generation, but it faces two key\nlimitations: inaccurate advantage attribution, and the neglect of temporal\ndynamics of generation. In this work, we argue that shifting the optimization\nparadigm from the step level to the chunk level can effectively alleviate these\nissues. Building on this idea, we propose Chunk-GRPO, the first chunk-level\nGRPO-based approach for T2I generation. The insight is to group consecutive\nsteps into coherent 'chunk's that capture the intrinsic temporal dynamics of\nflow matching, and to optimize policies at the chunk level. In addition, we\nintroduce an optional weighted sampling strategy to further enhance\nperformance. Extensive experiments show that ChunkGRPO achieves superior\nresults in both preference alignment and image quality, highlighting the\npromise of chunk-level optimization for GRPO-based methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faChunk-GRPO\uff0c\u9996\u4e2a\u57fa\u4e8e\u5757\u7ea7\u4f18\u5316\u7684GRPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4f18\u5316\u8303\u5f0f\u4ece\u6b65\u7ea7\u8f6c\u79fb\u5230\u5757\u7ea7\u6765\u89e3\u51b3\u4f18\u52bf\u5f52\u56e0\u4e0d\u51c6\u786e\u548c\u5ffd\u7565\u751f\u6210\u65f6\u5e8f\u52a8\u6001\u7684\u95ee\u9898\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u504f\u597d\u5bf9\u9f50\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709Group Relative Policy Optimization (GRPO)\u65b9\u6cd5\u5728\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u4f18\u52bf\u5f52\u56e0\u4e0d\u51c6\u786e\uff0c\u4ee5\u53ca\u5ffd\u7565\u751f\u6210\u7684\u65f6\u5e8f\u52a8\u6001\u7279\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u6027\u80fd\u63d0\u5347\u548c\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u63d0\u51faChunk-GRPO\u65b9\u6cd5\uff0c\u5c06\u8fde\u7eed\u6b65\u9aa4\u5206\u7ec4\u4e3a\u8fde\u8d2f\u7684'\u5757'\u4ee5\u6355\u6349\u6d41\u5339\u914d\u7684\u5185\u5728\u65f6\u5e8f\u52a8\u6001\uff0c\u5728\u5757\u7ea7\u522b\u8fdb\u884c\u7b56\u7565\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u53ef\u9009\u7684\u52a0\u6743\u91c7\u6837\u7b56\u7565\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u8868\u73b0\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cChunk-GRPO\u5728\u504f\u597d\u5bf9\u9f50\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u4f18\u8d8a\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u5757\u7ea7\u4f18\u5316\u5bf9\u4e8eGRPO\u57fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u4f18\u5316\u8303\u5f0f\u4ece\u6b65\u7ea7\u8f6c\u79fb\u5230\u5757\u7ea7\u80fd\u591f\u6709\u6548\u89e3\u51b3GRPO\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5757\u7ea7\u4f18\u5316\u4e3aGRPO\u57fa\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.21307", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21307", "abs": "https://arxiv.org/abs/2510.21307", "authors": ["Bingchen Miao", "Rong Wei", "Zhiqi Ge", "Xiaoquan sun", "Shiqi Gao", "Jingzhe Zhu", "Renhan Wang", "Siliang Tang", "Jun Xiao", "Rui Tang", "Juncheng Li"], "title": "Towards Physically Executable 3D Gaussian for Embodied Navigation", "comment": "Download link of InteriorGS:\n  https://huggingface.co/datasets/spatialverse/InteriorGS", "summary": "3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic\nreal-time rendering capabilities, is regarded as an effective tool for\nnarrowing the sim-to-real gap. However, it lacks fine-grained semantics and\nphysical executability for Visual-Language Navigation (VLN). To address this,\nwe propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments\nfor 3D Navigation), a new paradigm that upgrades 3DGS into an executable,\nsemantically and physically aligned environment. It comprises two components:\n(1) Object-Centric Semantic Grounding, which adds object-level fine-grained\nannotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds\ncollision objects into 3DGS and constructs rich physical interfaces. We release\nInteriorGS, containing 1K object-annotated 3DGS indoor scene data, and\nintroduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data.\nExperiments show that 3DGS scene data is more difficult to converge, while\nexhibiting strong generalizability, improving baseline performance by 31% on\nthe VLN-CE Unseen task. The data and code will be available soon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSAGE-3D\uff0c\u4e00\u79cd\u5c063D\u9ad8\u65af\u6cfc\u6e85\u5347\u7ea7\u4e3a\u53ef\u6267\u884c\u3001\u8bed\u4e49\u548c\u7269\u7406\u5bf9\u9f50\u73af\u5883\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u8bed\u4e49\u6807\u6ce8\u548c\u7269\u7406\u611f\u77e5\u6267\u884c\u8fde\u63a5\uff0c\u89e3\u51b3\u4e863DGS\u5728\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bed\u4e49\u548c\u7269\u7406\u53ef\u6267\u884c\u6027\u7684\u95ee\u9898\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u867d\u7136\u5177\u6709\u7167\u7247\u7ea7\u5b9e\u65f6\u6e32\u67d3\u80fd\u529b\uff0c\u4f46\u5728\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u548c\u7269\u7406\u53ef\u6267\u884c\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u65b9\u9762\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86SAGE-3D\u6846\u67b6\uff0c\u5305\u542b\u5bf9\u8c61\u4e2d\u5fc3\u8bed\u4e49\u6807\u6ce8\u4e3a3DGS\u6dfb\u52a0\u5bf9\u8c61\u7ea7\u7ec6\u7c92\u5ea6\u6ce8\u91ca\uff0c\u4ee5\u53ca\u7269\u7406\u611f\u77e5\u6267\u884c\u8fde\u63a5\u57283DGS\u4e2d\u5d4c\u5165\u78b0\u649e\u5bf9\u8c61\u5e76\u6784\u5efa\u4e30\u5bcc\u7684\u7269\u7406\u63a5\u53e3\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u5305\u542b1K\u5bf9\u8c61\u6807\u6ce83DGS\u5ba4\u5185\u573a\u666f\u6570\u636e\u7684InteriorGS\u6570\u636e\u96c6\u548c\u9996\u4e2a\u57fa\u4e8e3DGS\u7684VLN\u57fa\u51c6SAGE-Bench\u3002", "result": "\u5b9e\u9a8c\u8868\u660e3DGS\u573a\u666f\u6570\u636e\u6536\u655b\u96be\u5ea6\u66f4\u5927\u4f46\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5728VLN-CE Unseen\u4efb\u52a1\u4e0a\u5c06\u57fa\u7ebf\u6027\u80fd\u63d0\u5347\u4e8631%\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u5305\u542b2M VLN\u6570\u636e\u7684SAGE-Bench\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e863DGS\u5728\u8bed\u4e49\u548c\u7269\u7406\u5bf9\u9f50\u65b9\u9762\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u4eff\u771f\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u6784\u5efa\u66f4\u903c\u771f\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u6362\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.21311", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21311", "abs": "https://arxiv.org/abs/2510.21311", "authors": ["Lu Zhang", "Jiazuo Yu", "Haomiao Xiong", "Ping Hu", "Yunzhi Zhuge", "Huchuan Lu", "You He"], "title": "FineRS: Fine-grained Reasoning and Segmentation of Small Objects with Reinforcement Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities\nacross a wide range of vision-language tasks. However, due to the restricted\ninput resolutions, MLLMs face significant challenges in precisely understanding\nand localizing visual details in high-resolution images -- particularly when\ndealing with extra-small objects embedded in cluttered contexts. To address\nthis issue, we propose \\textsc{FineRS}, a two-stage MLLM-based reinforcement\nlearning framework for jointly reasoning and segmenting extremely small objects\nwithin high-resolution scenes. \\textsc{FineRS} adopts a coarse-to-fine pipeline\ncomprising Global Semantic Exploration (GSE) and Localized Perceptual\nRefinement (LPR). Specifically, GSE performs instruction-guided reasoning to\ngenerate a textural response and a coarse target region, while LPR refines this\nregion to produce an accurate bounding box and segmentation mask. To couple the\ntwo stages, we introduce a locate-informed retrospective reward, where LPR's\noutputs are used to optimize GSE for more robust coarse region exploration. %\nAdditionally, we present \\textsc{FineRS}-4k, a new dataset for evaluating MLLMs\non attribute-level reasoning and pixel-level segmentation on subtle,\nsmall-scale targets in complex high-resolution scenes. Experimental results on\n\\textsc{FineRS}-4k and public datasets demonstrate that our method consistently\noutperforms state-of-the-art MLLM-based approaches on both instruction-guided\nsegmentation and visual reasoning tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFineRS\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u8bed\u4e49\u63a2\u7d22\u548c\u5c40\u90e8\u611f\u77e5\u7ec6\u5316\u7684\u7c97\u5230\u7cbe\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u6781\u5c0f\u578b\u7269\u4f53\u7684\u7cbe\u786e\u7406\u89e3\u548c\u5b9a\u4f4d\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u6307\u4ee4\u5f15\u5bfc\u5206\u5272\u548c\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7531\u4e8e\u8f93\u5165\u5206\u8fa8\u7387\u53d7\u9650\uff0c\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u96be\u4ee5\u7cbe\u786e\u7406\u89e3\u548c\u5b9a\u4f4d\u89c6\u89c9\u7ec6\u8282\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5d4c\u5165\u590d\u6742\u80cc\u666f\u4e2d\u7684\u6781\u5c0f\u578b\u7269\u4f53\u65f6\u9762\u4e34\u663e\u8457\u6311\u6218\u3002", "method": "FineRS\u91c7\u7528\u4e24\u9636\u6bb5\u7c97\u5230\u7cbe\u6d41\u7a0b\uff0c\u5305\u62ec\u5168\u5c40\u8bed\u4e49\u63a2\u7d22\uff08GSE\uff09\u548c\u5c40\u90e8\u611f\u77e5\u7ec6\u5316\uff08LPR\uff09\u3002GSE\u901a\u8fc7\u6307\u4ee4\u5f15\u5bfc\u63a8\u7406\u751f\u6210\u6587\u672c\u54cd\u5e94\u548c\u7c97\u7565\u76ee\u6807\u533a\u57df\uff0cLPR\u5219\u7ec6\u5316\u8be5\u533a\u57df\u4ee5\u751f\u6210\u7cbe\u786e\u8fb9\u754c\u6846\u548c\u5206\u5272\u63a9\u7801\u3002\u901a\u8fc7\u5f15\u5165\u5b9a\u4f4d\u77e5\u60c5\u7684\u56de\u987e\u6027\u5956\u52b1\u673a\u5236\uff0c\u5c06LPR\u8f93\u51fa\u7528\u4e8e\u4f18\u5316GSE\u4ee5\u83b7\u5f97\u66f4\u9c81\u68d2\u7684\u7c97\u7565\u533a\u57df\u63a2\u7d22\u3002", "result": "\u5728FineRS-4k\u6570\u636e\u96c6\u548c\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6307\u4ee4\u5f15\u5bfc\u5206\u5272\u548c\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u4e00\u81f4\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8eMLLM\u7684\u65b9\u6cd5\u3002\u65b0\u63d0\u51fa\u7684FineRS-4k\u6570\u636e\u96c6\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30MLLM\u5728\u590d\u6742\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u4e2d\u5bf9\u7ec6\u5fae\u3001\u5c0f\u5c3a\u5ea6\u76ee\u6807\u7684\u5c5e\u6027\u7ea7\u63a8\u7406\u548c\u50cf\u7d20\u7ea7\u5206\u5272\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u8026\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u5206\u5272\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u6781\u5c0f\u578b\u7269\u4f53\u7684\u7cbe\u786e\u5b9a\u4f4d\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u590d\u6742\u89c6\u89c9\u573a\u666f\u7406\u89e3\u65b9\u9762\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.21323", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21323", "abs": "https://arxiv.org/abs/2510.21323", "authors": ["Shufan Shen", "Junshu Sun", "Qingming Huang", "Shuhui Wang"], "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set", "comment": "Accepted by NeurIPS 2025", "summary": "The alignment of vision-language representations endows current\nVision-Language Models (VLMs) with strong multi-modal reasoning capabilities.\nHowever, the interpretability of the alignment component remains uninvestigated\ndue to the difficulty in mapping the semantics of multi-modal representations\ninto a unified concept set. To address this problem, we propose VL-SAE, a\nsparse autoencoder that encodes vision-language representations into its hidden\nactivations. Each neuron in its hidden layer correlates to a concept\nrepresented by semantically similar images and texts, thereby interpreting\nthese representations with a unified concept set. To establish the\nneuron-concept correlation, we encourage semantically similar representations\nto exhibit consistent neuron activations during self-supervised training.\nFirst, to measure the semantic similarity of multi-modal representations, we\nperform their alignment in an explicit form based on cosine similarity. Second,\nwe construct the VL-SAE with a distance-based encoder and two modality-specific\ndecoders to ensure the activation consistency of semantically similar\nrepresentations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)\ndemonstrate the superior capability of VL-SAE in interpreting and enhancing the\nvision-language alignment. For interpretation, the alignment between vision and\nlanguage representations can be understood by comparing their semantics with\nconcepts. For enhancement, the alignment can be strengthened by aligning\nvision-language representations at the concept level, contributing to\nperformance improvements in downstream tasks, including zero-shot image\nclassification and hallucination elimination. Codes are available at\nhttps://github.com/ssfgunner/VL-SAE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVL-SAE\uff0c\u4e00\u79cd\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u7f16\u7801\u4e3a\u9690\u85cf\u5c42\u6fc0\u6d3b\u6765\u89e3\u6784\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u6bcf\u4e2a\u795e\u7ecf\u5143\u5bf9\u5e94\u7531\u8bed\u4e49\u76f8\u4f3c\u7684\u56fe\u50cf\u548c\u6587\u672c\u8868\u793a\u7684\u6982\u5ff5\uff0c\u4ece\u800c\u5efa\u7acb\u7edf\u4e00\u7684\u89e3\u91ca\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5bf9\u9f50\u7ec4\u4ef6\u7684\u53ef\u89e3\u91ca\u6027\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u4e3b\u8981\u56f0\u96be\u5728\u4e8e\u96be\u4ee5\u5c06\u591a\u6a21\u6001\u8868\u793a\u7684\u8bed\u4e49\u6620\u5c04\u5230\u7edf\u4e00\u7684\u6982\u5ff5\u96c6\u5408\u4e2d\u3002", "method": "\u63d0\u51faVL-SAE\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u91c7\u7528\u57fa\u4e8e\u8ddd\u79bb\u7684\u7f16\u7801\u5668\u548c\u4e24\u4e2a\u6a21\u6001\u7279\u5b9a\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u663e\u5f0f\u5bf9\u9f50\u591a\u6a21\u6001\u8868\u793a\uff0c\u5e76\u9f13\u52b1\u8bed\u4e49\u76f8\u4f3c\u7684\u8868\u793a\u5728\u81ea\u76d1\u7763\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVL-SAE\u5728\u89e3\u91ca\u548c\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u65b9\u9762\u5177\u6709\u5353\u8d8a\u80fd\u529b\uff0c\u80fd\u591f\u901a\u8fc7\u6982\u5ff5\u6bd4\u8f83\u7406\u89e3\u89c6\u89c9\u4e0e\u8bed\u8a00\u8868\u793a\u7684\u5bf9\u9f50\u5173\u7cfb\uff0c\u5e76\u5728\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u548c\u5e7b\u89c9\u6d88\u9664\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u8868\u793a\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u5ff5\u7ea7\u522b\u7684\u5bf9\u9f50\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fd8\u6539\u5584\u4e86\u5b9e\u9645\u5e94\u7528\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001AI\u7cfb\u7edf\u7684\u900f\u660e\u5316\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u601d\u8def\u3002"}}
{"id": "2510.21406", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21406", "abs": "https://arxiv.org/abs/2510.21406", "authors": ["Yue Feng", "Jinwei Hu", "Qijia Lu", "Jiawei Niu", "Li Tan", "Shuo Yuan", "Ziyi Yan", "Yizhen Jia", "Qingzhi He", "Shiping Ge", "Ethan Q. Chen", "Wentong Li", "Limin Wang", "Jie Qin"], "title": "MUVR: A Multi-Modal Untrimmed Video Retrieval Benchmark with Multi-Level Visual Correspondence", "comment": "Accepted to NeurIPS 2025 D&B Track", "summary": "We propose the Multi-modal Untrimmed Video Retrieval task, along with a new\nbenchmark (MUVR) to advance video retrieval for long-video platforms. MUVR aims\nto retrieve untrimmed videos containing relevant segments using multi-modal\nqueries. It has the following features: 1) Practical retrieval paradigm: MUVR\nsupports video-centric multi-modal queries, expressing fine-grained retrieval\nneeds through long text descriptions, video tag prompts, and mask prompts. It\nadopts a one-to-many retrieval paradigm and focuses on untrimmed videos,\ntailored for long-video platform applications. 2) Multi-level visual\ncorrespondence: To cover common video categories (e.g., news, travel, dance)\nand precisely define retrieval matching criteria, we construct multi-level\nvisual correspondence based on core video content (e.g., news events, travel\nlocations, dance moves) which users are interested in and want to retrieve. It\ncovers six levels: copy, event, scene, instance, action, and others. 3)\nComprehensive evaluation criteria: We develop 3 versions of MUVR (i.e., Base,\nFilter, QA). MUVR-Base/Filter evaluates retrieval models, while MUVR-QA\nassesses MLLMs in a question-answering format. We also propose a Reranking\nScore to evaluate the reranking ability of MLLMs. MUVR consists of 53K\nuntrimmed videos from the video platform Bilibili, with 1,050 multi-modal\nqueries and 84K matches. Extensive evaluations of 3 state-of-the-art video\nretrieval models, 6 image-based VLMs, and 10 MLLMs are conducted. MUVR reveals\nthe limitations of retrieval methods in processing untrimmed videos and\nmulti-modal queries, as well as MLLMs in multi-video understanding and\nreranking. Our code and benchmark is available at\nhttps://github.com/debby-0527/MUVR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u672a\u4fee\u526a\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u548cMUVR\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u957f\u89c6\u9891\u5e73\u53f0\u4e2d\u591a\u6a21\u6001\u67e5\u8be2\u7684\u672a\u4fee\u526a\u89c6\u9891\u68c0\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b53K\u89c6\u9891\u548c1050\u4e2a\u591a\u6a21\u6001\u67e5\u8be2\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u6027\u80fd\u5c40\u9650\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u68c0\u7d22\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u4fee\u526a\u540e\u7684\u77ed\u89c6\u9891\uff0c\u96be\u4ee5\u5904\u7406\u957f\u89c6\u9891\u5e73\u53f0\u4e2d\u7684\u672a\u4fee\u526a\u89c6\u9891\u548c\u591a\u6a21\u6001\u67e5\u8be2\u9700\u6c42\uff0c\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u5bf9\u591a\u6a21\u6001\u67e5\u8be2\u652f\u6301\u548c\u672a\u4fee\u526a\u89c6\u9891\u68c0\u7d22\u7684\u4e13\u95e8\u8bc4\u4f30\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faMUVR\u57fa\u51c6\uff0c\u652f\u6301\u89c6\u9891\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u67e5\u8be2\uff08\u957f\u6587\u672c\u63cf\u8ff0\u3001\u89c6\u9891\u6807\u7b7e\u63d0\u793a\u3001\u63a9\u7801\u63d0\u793a\uff09\uff0c\u91c7\u7528\u4e00\u5bf9\u591a\u68c0\u7d22\u8303\u5f0f\uff0c\u6784\u5efa\u57fa\u4e8e\u6838\u5fc3\u89c6\u9891\u5185\u5bb9\u7684\u591a\u5c42\u6b21\u89c6\u89c9\u5bf9\u5e94\u5173\u7cfb\uff08\u590d\u5236\u3001\u4e8b\u4ef6\u3001\u573a\u666f\u3001\u5b9e\u4f8b\u3001\u52a8\u4f5c\u7b49\u516d\u4e2a\u7ea7\u522b\uff09\uff0c\u5f00\u53d1\u4e09\u4e2a\u7248\u672c\uff08Base\u3001Filter\u3001QA\uff09\u5206\u522b\u8bc4\u4f30\u68c0\u7d22\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u6a21\u578b\u3002", "result": "\u5728\u5305\u542b53K\u672a\u4fee\u526a\u89c6\u9891\u548c1050\u4e2a\u591a\u6a21\u6001\u67e5\u8be2\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e863\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u9891\u68c0\u7d22\u6a21\u578b\u30016\u4e2a\u57fa\u4e8e\u56fe\u50cf\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c10\u4e2a\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u672a\u4fee\u526a\u89c6\u9891\u548c\u591a\u6a21\u6001\u67e5\u8be2\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u591a\u89c6\u9891\u7406\u89e3\u548c\u91cd\u6392\u5e8f\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "conclusion": "MUVR\u57fa\u51c6\u7cfb\u7edf\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u9891\u68c0\u7d22\u6280\u672f\u5728\u5904\u7406\u672a\u4fee\u526a\u89c6\u9891\u548c\u591a\u6a21\u6001\u67e5\u8be2\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u6807\u51c6\u548c\u53d1\u5c55\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u7406\u89e3\u548c\u957f\u89c6\u9891\u5904\u7406\u80fd\u529b\u5728\u89c6\u9891\u68c0\u7d22\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.21412", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21412", "abs": "https://arxiv.org/abs/2510.21412", "authors": ["Whie Jung", "Semin Kim", "Junee Kim", "Seunghoon Hong"], "title": "Bridging the gap to real-world language-grounded visual concept learning", "comment": null, "summary": "Human intelligence effortlessly interprets visual scenes along a rich\nspectrum of semantic dimensions. However, existing approaches to\nlanguage-grounded visual concept learning are limited to a few predefined\nprimitive axes, such as color and shape, and are typically explored in\nsynthetic datasets. In this work, we propose a scalable framework that\nadaptively identifies image-related concept axes and grounds visual concepts\nalong these axes in real-world scenes. Leveraging a pretrained vision-language\nmodel and our universal prompting strategy, our framework identifies a diverse\nimage-related axes without any prior knowledge. Our universal concept encoder\nadaptively binds visual features to the discovered axes without introducing\nadditional model parameters for each concept. To ground visual concepts along\nthe discovered axes, we optimize a compositional anchoring objective, which\nensures that each axis can be independently manipulated without affecting\nothers. We demonstrate the effectiveness of our framework on subsets of\nImageNet, CelebA-HQ, and AFHQ, showcasing superior editing capabilities across\ndiverse real-world concepts that are too varied to be manually predefined. Our\nmethod also exhibits strong compositional generalization, outperforming\nexisting visual concept learning and text-based editing methods. The code is\navailable at https://github.com/whieya/Language-grounded-VCL.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8bed\u8a00\u5f15\u5bfc\u89c6\u89c9\u6982\u5ff5\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u8bc6\u522b\u56fe\u50cf\u76f8\u5173\u6982\u5ff5\u8f74\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6cbf\u8fd9\u4e9b\u8f74\u8fdb\u884c\u89c6\u89c9\u6982\u5ff5\u5b9a\u4f4d\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u6982\u5ff5\u7c7b\u522b\u5373\u53ef\u5b9e\u73b0\u591a\u6837\u5316\u7684\u89c6\u89c9\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u5f15\u5bfc\u7684\u89c6\u89c9\u6982\u5ff5\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u4e8e\u5c11\u6570\u9884\u5b9a\u4e49\u7684\u57fa\u672c\u7ef4\u5ea6\uff08\u5982\u989c\u8272\u548c\u5f62\u72b6\uff09\uff0c\u4e14\u4e3b\u8981\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u63a2\u7d22\uff0c\u65e0\u6cd5\u9002\u5e94\u771f\u5b9e\u573a\u666f\u4e2d\u4e30\u5bcc\u591a\u6837\u7684\u8bed\u4e49\u6982\u5ff5\u5b66\u4e60\u9700\u6c42\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u901a\u7528\u63d0\u793a\u7b56\u7565\u81ea\u9002\u5e94\u8bc6\u522b\u591a\u6837\u5316\u7684\u56fe\u50cf\u76f8\u5173\u6982\u5ff5\u8f74\uff0c\u901a\u8fc7\u901a\u7528\u6982\u5ff5\u7f16\u7801\u5668\u5c06\u89c6\u89c9\u7279\u5f81\u7ed1\u5b9a\u5230\u53d1\u73b0\u7684\u6982\u5ff5\u8f74\u4e0a\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u6982\u5ff5\u5f15\u5165\u989d\u5916\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u4f18\u5316\u7ec4\u5408\u951a\u5b9a\u76ee\u6807\u786e\u4fdd\u5404\u6982\u5ff5\u8f74\u53ef\u72ec\u7acb\u64cd\u4f5c\u3002", "result": "\u5728ImageNet\u3001CelebA-HQ\u548cAFHQ\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u7f16\u8f91\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u8fc7\u4e8e\u591a\u6837\u5316\u800c\u65e0\u6cd5\u624b\u52a8\u9884\u5b9a\u4e49\u7684\u771f\u5b9e\u4e16\u754c\u6982\u5ff5\uff0c\u5e76\u5728\u7ec4\u5408\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u89c6\u89c9\u6982\u5ff5\u5b66\u4e60\u548c\u57fa\u4e8e\u6587\u672c\u7684\u7f16\u8f91\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834\u4e86\u9884\u5b9a\u4e49\u6982\u5ff5\u8f74\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u771f\u5b9e\u573a\u666f\u4e2d\u591a\u6837\u5316\u89c6\u89c9\u6982\u5ff5\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u4e0e\u7f16\u8f91\uff0c\u4e3a\u8bed\u8a00\u5f15\u5bfc\u7684\u89c6\u89c9\u6982\u5ff5\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.21449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21449", "abs": "https://arxiv.org/abs/2510.21449", "authors": ["Shengtian Yang", "Yue Feng", "Yingshi Liu", "Jingrou Zhang", "Jie Qin"], "title": "MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection", "comment": "Accepted to NeurIPS 2025. The first two authors hold equal\n  contributions", "summary": "Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors\nwithin videos. Recently, offline VAD has garnered substantial research\nattention, which has been invigorated by the progress in large language models\n(LLMs) and vision-language models (VLMs), offering the potential for a more\nnuanced understanding of anomalies. However, online VAD has seldom received\nattention due to real-time constraints and computational intensity. In this\npaper, we introduce a novel Memory-based online scoring queue scheme for\nTraining-free VAD (MoniTor), to address the inherent complexities in online\nVAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the\ncapabilities of pre-trained large-scale models. To capture temporal\ndependencies more effectively, we incorporate a novel prediction mechanism\ninspired by Long Short-Term Memory (LSTM) networks. This ensures the model can\neffectively model past states and leverage previous predictions to identify\nanomalous behaviors. Thereby, it better understands the current frame.\nMoreover, we design a scoring queue and an anomaly prior to dynamically store\nrecent scores and cover all anomalies in the monitoring scenario, providing\nguidance for LLMs to distinguish between normal and abnormal behaviors over\ntime. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and\nXD-Violence) containing various surveillance and real-world scenarios. The\nresults demonstrate that MoniTor outperforms state-of-the-art methods and is\ncompetitive with weakly supervised methods without training. Code is available\nat https://github.com/YsTvT/MoniTor.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMoniTor\uff0c\u4e00\u79cd\u57fa\u4e8e\u5185\u5b58\u7684\u5728\u7ebf\u8bc4\u5206\u961f\u5217\u65b9\u6848\uff0c\u7528\u4e8e\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548cLSTM\u542f\u53d1\u7684\u9884\u6d4b\u673a\u5236\uff0c\u5728\u5b9e\u65f6\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u79bb\u7ebf\u573a\u666f\uff0c\u800c\u5728\u7ebf\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7531\u4e8e\u5b9e\u65f6\u6027\u7ea6\u675f\u548c\u8ba1\u7b97\u5f3a\u5ea6\u5f88\u5c11\u53d7\u5230\u5173\u6ce8\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5728\u7ebf\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u56fa\u6709\u590d\u6742\u6027\u6311\u6218\u3002", "method": "MoniTor\u91c7\u7528\u6d41\u5f0f\u8f93\u5165\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408LSTM\u7f51\u7edc\u542f\u53d1\u7684\u9884\u6d4b\u673a\u5236\u6765\u6709\u6548\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8bc4\u5206\u961f\u5217\u548c\u5f02\u5e38\u5148\u9a8c\u6765\u52a8\u6001\u5b58\u50a8\u6700\u8fd1\u5206\u6570\uff0c\u4e3aLLMs\u533a\u5206\u6b63\u5e38\u548c\u5f02\u5e38\u884c\u4e3a\u63d0\u4f9b\u65f6\u95f4\u7ef4\u5ea6\u7684\u6307\u5bfc\u3002", "result": "\u5728UCF-Crime\u548cXD-Violence\u4e24\u4e2a\u5927\u578b\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cMoniTor\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u4e0e\u5f31\u76d1\u7763\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u5728\u5728\u7ebf\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u673a\u5236\u548c\u65f6\u95f4\u5efa\u6a21\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.21464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21464", "abs": "https://arxiv.org/abs/2510.21464", "authors": ["Yiming Tang", "Wenjia Zhong", "Rushi Shah", "Dianbo Liu"], "title": "CXR-LanIC: Language-Grounded Interpretable Classifier for Chest X-Ray Diagnosis", "comment": null, "summary": "Deep learning models have achieved remarkable accuracy in chest X-ray\ndiagnosis, yet their widespread clinical adoption remains limited by the\nblack-box nature of their predictions. Clinicians require transparent,\nverifiable explanations to trust automated diagnoses and identify potential\nfailure modes. We introduce CXR-LanIC (Language-Grounded Interpretable\nClassifier for Chest X-rays), a novel framework that addresses this\ninterpretability challenge through task-aligned pattern discovery. Our approach\ntrains transcoder-based sparse autoencoders on a BiomedCLIP diagnostic\nclassifier to decompose medical image representations into interpretable visual\npatterns. By training an ensemble of 100 transcoders on multimodal embeddings\nfrom the MIMIC-CXR dataset, we discover approximately 5,000 monosemantic\npatterns spanning cardiac, pulmonary, pleural, structural, device, and artifact\ncategories. Each pattern exhibits consistent activation behavior across images\nsharing specific radiological features, enabling transparent attribution where\npredictions decompose into 20-50 interpretable patterns with verifiable\nactivation galleries. CXR-LanIC achieves competitive diagnostic accuracy on\nfive key findings while providing the foundation for natural language\nexplanations through planned large multimodal model annotation. Our key\ninnovation lies in extracting interpretable features from a classifier trained\non specific diagnostic objectives rather than general-purpose embeddings,\nensuring discovered patterns are directly relevant to clinical decision-making,\ndemonstrating that medical AI systems can be both accurate and interpretable,\nsupporting safer clinical deployment through transparent, clinically grounded\nexplanations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86CXR-LanIC\u6846\u67b6\uff0c\u901a\u8fc7\u5728BiomedCLIP\u8bca\u65ad\u5206\u7c7b\u5668\u4e0a\u8bad\u7ec3\u57fa\u4e8e\u8f6c\u7801\u5668\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u5c06\u533b\u5b66\u56fe\u50cf\u8868\u5f81\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u6a21\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u8bca\u65ad\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u900f\u660e\u53ef\u9a8c\u8bc1\u7684\u89e3\u91ca\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u80f8\u90e8X\u5149\u8bca\u65ad\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u51c6\u786e\u6027\uff0c\u4f46\u5176\u9ed1\u76d2\u9884\u6d4b\u7279\u6027\u9650\u5236\u4e86\u4e34\u5e8a\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e34\u5e8a\u533b\u751f\u9700\u8981\u900f\u660e\u53ef\u9a8c\u8bc1\u7684\u89e3\u91ca\u6765\u4fe1\u4efb\u81ea\u52a8\u5316\u8bca\u65ad\u5e76\u8bc6\u522b\u6f5c\u5728\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u8be5\u65b9\u6cd5\u8bad\u7ec3\u57fa\u4e8e\u8f6c\u7801\u5668\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u5728MIMIC-CXR\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3100\u4e2a\u8f6c\u7801\u5668\u96c6\u6210\uff0c\u4ece\u591a\u6a21\u6001\u5d4c\u5165\u4e2d\u5206\u89e3\u51fa\u7ea65,000\u4e2a\u5355\u4e49\u6027\u6a21\u5f0f\uff0c\u6db5\u76d6\u5fc3\u810f\u3001\u80ba\u90e8\u3001\u80f8\u819c\u3001\u7ed3\u6784\u3001\u8bbe\u5907\u548c\u4f2a\u5f71\u7b49\u7c7b\u522b\u3002", "result": "CXR-LanIC\u5728\u4e94\u4e2a\u5173\u952e\u53d1\u73b0\u4e0a\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u9884\u6d4b\u53ef\u5206\u89e3\u4e3a20-50\u4e2a\u53ef\u89e3\u91ca\u6a21\u5f0f\uff0c\u6bcf\u4e2a\u6a21\u5f0f\u5728\u5171\u4eab\u7279\u5b9a\u653e\u5c04\u5b66\u7279\u5f81\u7684\u56fe\u50cf\u95f4\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6fc0\u6d3b\u884c\u4e3a\uff0c\u5e76\u53ef\u901a\u8fc7\u9a8c\u8bc1\u6fc0\u6d3b\u56fe\u5e93\u8fdb\u884c\u900f\u660e\u5f52\u56e0\u3002", "conclusion": "\u8be5\u7814\u7a76\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u4ece\u9488\u5bf9\u7279\u5b9a\u8bca\u65ad\u76ee\u6807\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7279\u5f81\uff0c\u800c\u975e\u901a\u7528\u5d4c\u5165\uff0c\u786e\u4fdd\u53d1\u73b0\u7684\u6a21\u5f0f\u4e0e\u4e34\u5e8a\u51b3\u7b56\u76f4\u63a5\u76f8\u5173\uff0c\u8bc1\u660e\u533b\u5b66AI\u7cfb\u7edf\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u900f\u660e\u3001\u4e34\u5e8a\u57fa\u7840\u7684\u89e3\u91ca\u652f\u6301\u66f4\u5b89\u5168\u7684\u4e34\u5e8a\u90e8\u7f72\u3002"}}
{"id": "2510.21512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21512", "abs": "https://arxiv.org/abs/2510.21512", "authors": ["Kaibo Wang", "Jianda Mao", "Tong Wu", "Yang Xiang"], "title": "Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point Iterations", "comment": "Accepted at NeurIPS 2025 (Spotlight)", "summary": "Classifier-Free Guidance (CFG) is an essential component of text-to-image\ndiffusion models, and understanding and advancing its operational mechanisms\nremains a central focus of research. Existing approaches stem from divergent\ntheoretical interpretations, thereby limiting the design space and obscuring\nkey design choices. To address this, we propose a unified perspective that\nreframes conditional guidance as fixed point iterations, seeking to identify a\ngolden path where latents produce consistent outputs under both conditional and\nunconditional generation. We demonstrate that CFG and its variants constitute a\nspecial case of single-step short-interval iteration, which is theoretically\nproven to exhibit inefficiency. To this end, we introduce Foresight Guidance\n(FSG), which prioritizes solving longer-interval subproblems in early diffusion\nstages with increased iterations. Extensive experiments across diverse datasets\nand model architectures validate the superiority of FSG over state-of-the-art\nmethods in both image quality and computational efficiency. Our work offers\nnovel perspectives for conditional guidance and unlocks the potential of\nadaptive design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u524d\u77bb\u6027\u5f15\u5bfc\uff08FSG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6761\u4ef6\u5f15\u5bfc\u91cd\u65b0\u6784\u5efa\u4e3a\u4e0d\u52a8\u70b9\u8fed\u4ee3\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5206\u7c7b\u5668\u65e0\u5173\u5f15\u5bfc\uff08CFG\uff09\u65b9\u6cd5\u7684\u6548\u7387\u9650\u5236\u3002FSG\u5728\u65e9\u671f\u6269\u6563\u9636\u6bb5\u4f18\u5148\u89e3\u51b3\u957f\u95f4\u9694\u5b50\u95ee\u9898\u5e76\u589e\u52a0\u8fed\u4ee3\u6b21\u6570\uff0c\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5206\u7c7b\u5668\u65e0\u5173\u5f15\u5bfc\uff08CFG\uff09\u65b9\u6cd5\u57fa\u4e8e\u4e0d\u540c\u7684\u7406\u8bba\u89e3\u91ca\uff0c\u9650\u5236\u4e86\u8bbe\u8ba1\u7a7a\u95f4\u5e76\u6a21\u7cca\u4e86\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u3002\u8fd9\u4e9b\u65b9\u6cd5\u88ab\u8bc1\u660e\u6784\u6210\u5355\u6b65\u77ed\u95f4\u9694\u8fed\u4ee3\u7684\u7279\u4f8b\uff0c\u7406\u8bba\u4e0a\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u6539\u8fdb\u6761\u4ef6\u5f15\u5bfc\u673a\u5236\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u89c6\u89d2\u5c06\u6761\u4ef6\u5f15\u5bfc\u91cd\u65b0\u6784\u5efa\u4e3a\u4e0d\u52a8\u70b9\u8fed\u4ee3\u95ee\u9898\uff0c\u65e8\u5728\u627e\u5230\u6f5c\u5728\u53d8\u91cf\u5728\u6761\u4ef6\u751f\u6210\u548c\u65e0\u6761\u4ef6\u751f\u6210\u4e0b\u4ea7\u751f\u4e00\u81f4\u8f93\u51fa\u7684\u9ec4\u91d1\u8def\u5f84\u3002\u5f15\u5165\u524d\u77bb\u6027\u5f15\u5bfc\uff08FSG\uff09\u65b9\u6cd5\uff0c\u5728\u65e9\u671f\u6269\u6563\u9636\u6bb5\u4f18\u5148\u89e3\u51b3\u957f\u95f4\u9694\u5b50\u95ee\u9898\u5e76\u589e\u52a0\u8fed\u4ee3\u6b21\u6570\uff0c\u7a81\u7834\u4e86\u4f20\u7edfCFG\u7684\u5355\u6b65\u77ed\u95f4\u9694\u8fed\u4ee3\u9650\u5236\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FSG\u7684\u4f18\u8d8a\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\u3002FSG\u4e0d\u4ec5\u6539\u8fdb\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u8fd8\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u6846\u67b6\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6761\u4ef6\u5f15\u5bfc\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u7406\u8bba\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u81ea\u9002\u5e94\u8bbe\u8ba1\u7684\u6f5c\u529b\u3002\u7edf\u4e00\u6846\u67b6\u4e0d\u4ec5\u89e3\u91ca\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u8fd8\u4e3a\u672a\u6765\u6761\u4ef6\u751f\u6210\u6a21\u578b\u7684\u4f18\u5316\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u63a8\u52a8\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.21581", "categories": ["cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21581", "abs": "https://arxiv.org/abs/2510.21581", "authors": ["Ciara Rowles", "Varun Jampani", "Simon Donn\u00e9", "Shimon Vainer", "Julian Parker", "Zach Evans"], "title": "Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video", "comment": "Project Page: https://stability-ai.github.io/foleycontrol.github.io/", "summary": "Foley Control is a lightweight approach to video-guided Foley that keeps\npretrained single-modality models frozen and learns only a small\ncross-attention bridge between them. We connect V-JEPA2 video embeddings to a\nfrozen Stable Audio Open DiT text-to-audio (T2A) model by inserting compact\nvideo cross-attention after the model's existing text cross-attention, so\nprompts set global semantics while video refines timing and local dynamics. The\nfrozen backbones retain strong marginals (video; audio given text) and the\nbridge learns the audio-video dependency needed for synchronization -- without\nretraining the audio prior. To cut memory and stabilize training, we pool video\ntokens before conditioning. On curated video-audio benchmarks, Foley Control\ndelivers competitive temporal and semantic alignment with far fewer trainable\nparameters than recent multi-modal systems, while preserving prompt-driven\ncontrollability and production-friendly modularity (swap/upgrade encoders or\nthe T2A backbone without end-to-end retraining). Although we focus on\nVideo-to-Foley, the same bridge design can potentially extend to other audio\nmodalities (e.g., speech).", "AI": {"tldr": "Foley Control\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89c6\u9891\u5f15\u5bfcFoley\u65b9\u6cd5\uff0c\u901a\u8fc7\u51bb\u7ed3\u9884\u8bad\u7ec3\u7684\u5355\u6a21\u6001\u6a21\u578b\u5e76\u4ec5\u5b66\u4e60\u5b83\u4eec\u4e4b\u95f4\u7684\u5c0f\u578b\u4ea4\u53c9\u6ce8\u610f\u529b\u6865\u63a5\uff0c\u5c06V-JEPA2\u89c6\u9891\u5d4c\u5165\u8fde\u63a5\u5230\u51bb\u7ed3\u7684Stable Audio Open DiT\u6587\u672c\u5230\u97f3\u9891\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001\u97f3\u9891\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u7aef\u5230\u7aef\u91cd\u65b0\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u7f3a\u4e4f\u6a21\u5757\u5316\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u89c6\u9891\u4e0e\u97f3\u9891\u7684\u6709\u6548\u540c\u6b65\uff0c\u540c\u65f6\u4fdd\u7559\u63d0\u793a\u9a71\u52a8\u7684\u53ef\u63a7\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u5728\u73b0\u6709\u6587\u672c\u4ea4\u53c9\u6ce8\u610f\u529b\u4e4b\u540e\u63d2\u5165\u7d27\u51d1\u7684\u89c6\u9891\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u4f7f\u7528\u63d0\u793a\u8bbe\u7f6e\u5168\u5c40\u8bed\u4e49\u800c\u89c6\u9891\u7ec6\u5316\u65f6\u5e8f\u548c\u5c40\u90e8\u52a8\u6001\uff0c\u901a\u8fc7\u6c60\u5316\u89c6\u9891token\u6765\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u5e76\u7a33\u5b9a\u8bad\u7ec3\uff0c\u4fdd\u6301\u97f3\u9891\u5148\u9a8c\u4e0d\u53d8\u4ec5\u5b66\u4e60\u97f3\u9891-\u89c6\u9891\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u7cbe\u9009\u7684\u89c6\u9891-\u97f3\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFoley Control\u5728\u65f6\u95f4\u548c\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u8bad\u7ec3\u53c2\u6570\u8fdc\u5c11\u4e8e\u6700\u8fd1\u7684\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63d0\u793a\u9a71\u52a8\u7684\u53ef\u63a7\u6027\u548c\u751f\u4ea7\u53cb\u597d\u7684\u6a21\u5757\u5316\u7279\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6865\u63a5\u8bbe\u8ba1\u53ef\u4ee5\u6709\u6548\u8fde\u63a5\u51bb\u7ed3\u7684\u5355\u6a21\u6001\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u6a21\u6001\u751f\u6210\uff0c\u8fd9\u79cd\u6865\u63a5\u8bbe\u8ba1\u5177\u6709\u6269\u5c55\u5230\u5176\u4ed6\u97f3\u9891\u6a21\u6001\u7684\u6f5c\u529b\uff0c\u4e3a\u6a21\u5757\u5316\u591a\u6a21\u6001\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.21605", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21605", "abs": "https://arxiv.org/abs/2510.21605", "authors": ["Orest Kupyn", "Hirokatsu Kataoka", "Christian Rupprecht"], "title": "S3OD: Towards Generalizable Salient Object Detection with Synthetic Data", "comment": null, "summary": "Salient object detection exemplifies data-bounded tasks where expensive\npixel-precise annotations force separate model training for related subtasks\nlike DIS and HR-SOD. We present a method that dramatically improves\ngeneralization through large-scale synthetic data generation and\nambiguity-aware architecture. We introduce S3OD, a dataset of over 139,000\nhigh-resolution images created through our multi-modal diffusion pipeline that\nextracts labels from diffusion and DINO-v3 features. The iterative generation\nframework prioritizes challenging categories based on model performance. We\npropose a streamlined multi-mask decoder that naturally handles the inherent\nambiguity in salient object detection by predicting multiple valid\ninterpretations. Models trained solely on synthetic data achieve 20-50% error\nreduction in cross-dataset generalization, while fine-tuned versions reach\nstate-of-the-art performance across DIS and HR-SOD benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u6a21\u7cca\u611f\u77e5\u67b6\u6784\u663e\u8457\u63d0\u5347\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86\u5305\u542b139,000\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684S3OD\u6570\u636e\u96c6\uff0c\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u4e2d\u5b9e\u73b0\u4e8620-50%\u7684\u9519\u8bef\u7387\u964d\u4f4e\u3002", "motivation": "\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u6570\u636e\u53d7\u9650\u7684\u95ee\u9898\uff0c\u6602\u8d35\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u8feb\u4f7f\u76f8\u5173\u5b50\u4efb\u52a1\u5982DIS\u548cHR-SOD\u9700\u8981\u5206\u522b\u8bad\u7ec3\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u6269\u6563\u7ba1\u9053\u4ece\u6269\u6563\u548cDINO-v3\u7279\u5f81\u4e2d\u63d0\u53d6\u6807\u7b7e\uff0c\u6784\u5efa\u4e86\u5305\u542b139,000\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684S3OD\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6a21\u578b\u6027\u80fd\u4f18\u5148\u5904\u7406\u6311\u6218\u7c7b\u522b\u7684\u8fed\u4ee3\u751f\u6210\u6846\u67b6\uff0c\u4ee5\u53ca\u80fd\u591f\u81ea\u7136\u5904\u7406\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u56fa\u6709\u6a21\u7cca\u6027\u7684\u7b80\u5316\u591a\u63a9\u7801\u89e3\u7801\u5668\u3002", "result": "\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u4e2d\u5b9e\u73b0\u4e8620-50%\u7684\u9519\u8bef\u7387\u964d\u4f4e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u7248\u672c\u5728DIS\u548cHR-SOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u751f\u6210\u4e0e\u6a21\u7cca\u611f\u77e5\u67b6\u6784\u7684\u7ed3\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6570\u636e\u53d7\u9650\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u5728\u63a8\u52a8\u6a21\u578b\u6027\u80fd\u8fb9\u754c\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.21606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21606", "abs": "https://arxiv.org/abs/2510.21606", "authors": ["Jiaxiang Liu", "Yuan Wang", "Jiawei Du", "Joey Tianyi Zhou", "Mingkun Xu", "Zuozhu Liu"], "title": "Modest-Align: Data-Efficient Alignment for Vision-Language Models", "comment": null, "summary": "Cross-modal alignment aims to map heterogeneous modalities into a shared\nlatent space, as exemplified by models like CLIP, which benefit from\nlarge-scale image-text pretraining for strong recognition capabilities.\nHowever, when operating in resource-constrained settings with limited or\nlow-quality data, these models often suffer from overconfidence and degraded\nperformance due to the prevalence of ambiguous or weakly correlated image-text\npairs. Current contrastive learning approaches, which rely on single positive\npairs, further exacerbate this issue by reinforcing overconfidence on uncertain\nsamples. To address these challenges, we propose Modest-Align, a lightweight\nalignment framework designed for robustness and efficiency. Our approach\nleverages two complementary strategies -- Random Perturbation, which introduces\ncontrolled noise to simulate uncertainty, and Embedding Smoothing, which\ncalibrates similarity distributions in the embedding space. These mechanisms\ncollectively reduce overconfidence and improve performance on noisy or weakly\naligned samples. Extensive experiments across multiple benchmark datasets\ndemonstrate that Modest-Align outperforms state-of-the-art methods in retrieval\ntasks, achieving competitive results with over 100x less training data and 600x\nless GPU time than CLIP. Our method offers a practical and scalable solution\nfor cross-modal alignment in real-world, low-resource scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Modest-Align\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u6270\u52a8\u548c\u5d4c\u5165\u5e73\u6ed1\u4e24\u79cd\u4e92\u8865\u7b56\u7565\u6765\u7f13\u89e3\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u8fc7\u81ea\u4fe1\u95ee\u9898\uff0c\u5728\u4ec5\u4f7f\u75281%\u8bad\u7ec3\u6570\u636e\u548c0.17%GPU\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u4e0eCLIP\u76f8\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\uff0c\u5927\u89c4\u6a21\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u6a21\u578b\u5982CLIP\u9762\u4e34\u8fc7\u81ea\u4fe1\u548c\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u8fd9\u4e3b\u8981\u6e90\u4e8e\u6570\u636e\u8d28\u91cf\u4f4e\u3001\u56fe\u50cf-\u6587\u672c\u5bf9\u5173\u8054\u6027\u5f31\u4ee5\u53ca\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5bf9\u4e0d\u786e\u5b9a\u6837\u672c\u7684\u8fc7\u5ea6\u5f3a\u5316\u3002", "method": "Modest-Align\u6846\u67b6\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff1a\u968f\u673a\u6270\u52a8\u901a\u8fc7\u5f15\u5165\u53d7\u63a7\u566a\u58f0\u6a21\u62df\u4e0d\u786e\u5b9a\u6027\uff0c\u5d4c\u5165\u5e73\u6ed1\u901a\u8fc7\u6821\u51c6\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u76f8\u4f3c\u5ea6\u5206\u5e03\u6765\u51cf\u5c11\u8fc7\u81ea\u4fe1\uff0c\u5171\u540c\u63d0\u5347\u5bf9\u566a\u58f0\u548c\u5f31\u5bf9\u9f50\u6837\u672c\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cModest-Align\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528CLIP 1%\u7684\u8bad\u7ec3\u6570\u636e\u548c0.17%\u7684GPU\u65f6\u95f4\u5c31\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u73b0\u5b9e\u4e16\u754c\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6821\u51c6\u673a\u5236\u53ef\u4ee5\u5728\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.21704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21704", "abs": "https://arxiv.org/abs/2510.21704", "authors": ["Christy Li", "Josep Lopez Camu\u00f1as", "Jake Thomas Touchet", "Jacob Andreas", "Agata Lapedriza", "Antonio Torralba", "Tamar Rott Shaham"], "title": "Automated Detection of Visual Attribute Reliance with a Self-Reflective Agent", "comment": "32 pages, 10 figures, Neurips 2025", "summary": "When a vision model performs image recognition, which visual attributes drive\nits predictions? Detecting unintended reliance on specific visual features is\ncritical for ensuring model robustness, preventing overfitting, and avoiding\nspurious correlations. We introduce an automated framework for detecting such\ndependencies in trained vision models. At the core of our method is a\nself-reflective agent that systematically generates and tests hypotheses about\nvisual attributes that a model may rely on. This process is iterative: the\nagent refines its hypotheses based on experimental outcomes and uses a\nself-evaluation protocol to assess whether its findings accurately explain\nmodel behavior. When inconsistencies arise, the agent self-reflects over its\nfindings and triggers a new cycle of experimentation. We evaluate our approach\non a novel benchmark of 130 models designed to exhibit diverse visual attribute\ndependencies across 18 categories. Our results show that the agent's\nperformance consistently improves with self-reflection, with a significant\nperformance increase over non-reflective baselines. We further demonstrate that\nthe agent identifies real-world visual attribute dependencies in\nstate-of-the-art models, including CLIP's vision encoder and the YOLOv8 object\ndetector.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u53cd\u601d\u4ee3\u7406\u7cfb\u7edf\u6027\u5730\u751f\u6210\u548c\u6d4b\u8bd5\u89c6\u89c9\u5c5e\u6027\u4f9d\u8d56\u5047\u8bbe\uff0c\u7528\u4e8e\u68c0\u6d4b\u8bad\u7ec3\u597d\u7684\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u610f\u5916\u4f9d\u8d56\u5173\u7cfb\u3002\u8be5\u65b9\u6cd5\u5728\u5305\u542b130\u4e2a\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u975e\u53cd\u601d\u57fa\u7ebf\uff0c\u5e76\u80fd\u8bc6\u522bCLIP\u89c6\u89c9\u7f16\u7801\u5668\u548cYOLOv8\u7b49\u5148\u8fdb\u6a21\u578b\u4e2d\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u89c9\u5c5e\u6027\u4f9d\u8d56\u3002", "motivation": "\u89c6\u89c9\u6a21\u578b\u5728\u8fdb\u884c\u56fe\u50cf\u8bc6\u522b\u65f6\u53ef\u80fd\u610f\u5916\u4f9d\u8d56\u7279\u5b9a\u7684\u89c6\u89c9\u5c5e\u6027\uff0c\u8fd9\u79cd\u4f9d\u8d56\u6027\u4f1a\u5a01\u80c1\u6a21\u578b\u9c81\u68d2\u6027\u3001\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u865a\u5047\u76f8\u5173\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u673a\u5236\u6765\u8bc6\u522b\u8fd9\u4e9b\u6f5c\u5728\u4f9d\u8d56\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u53d1\u73b0\u548c\u9a8c\u8bc1\u6a21\u578b\u4f9d\u8d56\u6a21\u5f0f\u7684\u6846\u67b6\u3002", "method": "\u6838\u5fc3\u65b9\u6cd5\u662f\u4e00\u4e2a\u81ea\u53cd\u601d\u4ee3\u7406\uff0c\u5b83\u7cfb\u7edf\u6027\u5730\u751f\u6210\u5173\u4e8e\u6a21\u578b\u53ef\u80fd\u4f9d\u8d56\u7684\u89c6\u89c9\u5c5e\u6027\u7684\u5047\u8bbe\u5e76\u8fdb\u884c\u6d4b\u8bd5\u3002\u8be5\u8fc7\u7a0b\u662f\u8fed\u4ee3\u5f0f\u7684\uff1a\u4ee3\u7406\u6839\u636e\u5b9e\u9a8c\u7ed3\u679c\u7cbe\u70bc\u5047\u8bbe\uff0c\u5e76\u4f7f\u7528\u81ea\u8bc4\u4f30\u534f\u8bae\u6765\u9a8c\u8bc1\u5176\u53d1\u73b0\u662f\u5426\u51c6\u786e\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u3002\u5f53\u51fa\u73b0\u4e0d\u4e00\u81f4\u65f6\uff0c\u4ee3\u7406\u4f1a\u81ea\u6211\u53cd\u601d\u5e76\u89e6\u53d1\u65b0\u7684\u5b9e\u9a8c\u5faa\u73af\u3002", "result": "\u5728\u5305\u542b130\u4e2a\u8bbe\u8ba1\u5177\u670918\u4e2a\u7c7b\u522b\u4e0d\u540c\u89c6\u89c9\u5c5e\u6027\u4f9d\u8d56\u7684\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u81ea\u53cd\u601d\u4ee3\u7406\u7684\u6027\u80fd\u968f\u7740\u53cd\u601d\u8fc7\u7a0b\u6301\u7eed\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u975e\u53cd\u601d\u57fa\u7ebf\u3002\u8be5\u65b9\u6cd5\u6210\u529f\u8bc6\u522b\u4e86CLIP\u89c6\u89c9\u7f16\u7801\u5668\u548cYOLOv8\u76ee\u6807\u68c0\u6d4b\u5668\u7b49\u6700\u5148\u8fdb\u6a21\u578b\u4e2d\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u89c9\u5c5e\u6027\u4f9d\u8d56\u3002", "conclusion": "\u81ea\u53cd\u601d\u673a\u5236\u5bf9\u4e8e\u7cfb\u7edf\u5316\u68c0\u6d4b\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u5c5e\u6027\u4f9d\u8d56\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u5de5\u5177\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\u81ea\u53cd\u601d\u80fd\u591f\u663e\u8457\u63d0\u5347\u4f9d\u8d56\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u6a21\u578b\u5ba1\u8ba1\u548c\u9c81\u68d2\u6027\u8bc4\u4f30\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
