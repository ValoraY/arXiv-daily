<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 33]
- [cs.CL](#cs.CL) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Rethinking Chain-of-Thought Reasoning for Videos](https://arxiv.org/abs/2512.09616)
*Yiwu Zhong, Zi-Yuan Hu, Yin Li, Liwei Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒä¸æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡å‹ç¼©è§†è§‰ä»¤ç‰Œå’Œç”Ÿæˆç®€æ´æ¨ç†è½¨è¿¹ï¼Œåœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿé“¾å¼æ€ç»´æ¨ç†åœ¨è§†é¢‘ç†è§£ä¸­çš„å¿…è¦æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºé“¾å¼æ€ç»´æ¨ç†çš„è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€šå¸¸ä¾èµ–å†—é•¿çš„æ¨ç†é“¾å’Œå¤§é‡è¾“å…¥è§†è§‰ä»¤ç‰Œï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚æœ¬ç ”ç©¶é€šè¿‡åŸºå‡†åˆ†æå‘ç°ï¼Œç®€æ´æ¨ç†ç»“åˆå°‘é‡è§†è§‰ä»¤ç‰Œå¯èƒ½è¶³ä»¥å®ç°æœ‰æ•ˆçš„è§†é¢‘æ¨ç†ï¼Œæ—¨åœ¨éªŒè¯è¿™ä¸€å‡è®¾å¹¶æå‡æ¨¡å‹æ•ˆç‡ã€‚

**Method:** ç ”ç©¶è®¾è®¡å¹¶éªŒè¯äº†ä¸€ä¸ªé«˜æ•ˆçš„åè®­ç»ƒä¸æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨å‹ç¼©çš„è§†è§‰ä»¤ç‰Œä¸Šæ“ä½œï¼Œå¹¶åœ¨å›ç­”é—®é¢˜å‰ç”Ÿæˆç®€æ´çš„æ¨ç†è½¨è¿¹ã€‚è¯¥æ–¹æ³•æ— éœ€äººå·¥é“¾å¼æ€ç»´æ ‡æ³¨æˆ–ç›‘ç£å¾®è°ƒï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„é«˜æ•ˆæ¨ç†ã€‚

**Result:** æ‰€æå‡ºçš„æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—æå‡çš„æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ã€‚æ¨¡å‹åœ¨å‡å°‘è§†è§‰ä»¤ç‰Œæ•°é‡å’Œç¼©çŸ­æ¨ç†é“¾çš„æƒ…å†µä¸‹ï¼Œä»èƒ½æœ‰æ•ˆå®Œæˆè§†é¢‘æ¨ç†ä»»åŠ¡ï¼ŒéªŒè¯äº†ç®€æ´æ¨ç†çš„å¯è¡Œæ€§ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç±»ä¼¼äººç±»çš„é•¿é“¾å¼æ€ç»´æ¨ç†å¯¹äºé€šç”¨è§†é¢‘ç†è§£å¯èƒ½å¹¶éå¿…è¦ï¼Œç®€æ´æ¨ç†æ—¢èƒ½ä¿æŒæœ‰æ•ˆæ€§åˆèƒ½æé«˜æ•ˆç‡ã€‚è¿™ä¸€å‘ç°ä¸ºè§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è½»é‡åŒ–è®¾è®¡æä¾›äº†æ–°æ€è·¯ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿå¤æ‚æ¨ç†æ¨¡å¼åœ¨è§†é¢‘é¢†åŸŸçš„å¿…è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.


### [2] [MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI](https://arxiv.org/abs/2512.09867)
*Fengli Wu, Vaidehi Patil, Jaehong Yoon, Yue Zhang, Mohit Bansal*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MedForgetï¼Œä¸€ä¸ªå±‚æ¬¡æ„ŸçŸ¥çš„å¤šæ¨¡æ€é—å¿˜æµ‹è¯•å¹³å°ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°åŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„é€‰æ‹©æ€§é—å¿˜æ•ˆæœï¼Œå¹¶æ­ç¤ºäº†ç°æœ‰é—å¿˜æ–¹æ³•åœ¨å¤æ‚åŒ»ç–—å±‚æ¬¡ç»“æ„ä¸­çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—AIç³»ç»Ÿä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶è®­ç»ƒæ¶‰åŠæ•æ„Ÿæ‚£è€…æ•°æ®ï¼Œé¢ä¸´HIPAAå’ŒGDPRç­‰æ³•è§„ä¸‹çš„éšç§å’Œåˆè§„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯"è¢«é—å¿˜æƒ"è¦æ±‚ã€‚ç°æœ‰é—å¿˜æ–¹æ³•åœ¨å¤æ‚åŒ»ç–—ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œéœ€è¦ç³»ç»Ÿè¯„ä¼°æ¡†æ¶æ¥ç ”ç©¶å±‚æ¬¡åŒ–åŒ»ç–—æ•°æ®ä¸­çš„é€‰æ‹©æ€§é—å¿˜é—®é¢˜ã€‚

**Method:** ç ”ç©¶æå‡ºäº†MedForgetæµ‹è¯•å¹³å°ï¼Œå°†åŒ»é™¢æ•°æ®å»ºæ¨¡ä¸ºåµŒå¥—å±‚æ¬¡ç»“æ„ï¼ˆæœºæ„â†’æ‚£è€…â†’ç ”ç©¶â†’éƒ¨åˆ†ï¼‰ï¼ŒåŒ…å«3840ä¸ªå¤šæ¨¡æ€å®ä¾‹å’Œæ˜ç¡®çš„ä¿ç•™/é—å¿˜åˆ’åˆ†ã€‚å¹³å°å¼•å…¥é‡å»ºæ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡é€æ­¥æ·»åŠ å±‚æ¬¡ä¸Šä¸‹æ–‡æç¤ºæ¥æµ‹è¯•é—å¿˜æ˜¯å¦çœŸæ­£åˆ é™¤äº†å±‚æ¬¡åŒ–è·¯å¾„ï¼Œå¹¶åœ¨ä¸‰ä¸ªä»»åŠ¡ä¸Šè¯„ä¼°äº†å››ç§æœ€å…ˆè¿›çš„é—å¿˜æ–¹æ³•ã€‚

**Result:** å®éªŒè¡¨æ˜ç°æœ‰é—å¿˜æ–¹æ³•éš¾ä»¥åœ¨ä¿æŒè¯Šæ–­æ€§èƒ½çš„åŒæ—¶å®ç°å®Œå…¨ã€å±‚æ¬¡æ„ŸçŸ¥çš„é—å¿˜ã€‚ç²—ç²’åº¦é—å¿˜çš„æ¨¡å‹å¯¹é‡å»ºæ”»å‡»è¡¨ç°å‡ºå¼ºæŠµæŠ—åŠ›ï¼Œè€Œç»†ç²’åº¦é—å¿˜çš„æ¨¡å‹åˆ™å®¹æ˜“å—åˆ°æ”»å‡»ã€‚æµ‹è¯•å¹³å°åœ¨å…«ä¸ªç»„ç»‡å±‚æ¬¡ä¸Šæä¾›äº†ç»†ç²’åº¦è¯„ä¼°ï¼Œæ­ç¤ºäº†ä¸åŒé—å¿˜ç²’åº¦ä¸‹çš„å®‰å…¨æƒè¡¡ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†åŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­å±‚æ¬¡æ„ŸçŸ¥é—å¿˜çš„å¤æ‚æ€§ï¼Œä¸ºæ„å»ºåˆè§„åŒ»ç–—AIç³»ç»Ÿæä¾›äº†å®ç”¨æµ‹è¯•æ¡†æ¶ã€‚ç»“æœè¡¨æ˜éœ€è¦åœ¨é—å¿˜å®Œæ•´æ€§å’Œæ¨¡å‹å®ç”¨æ€§ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå¹¶ä¸ºæœªæ¥å¼€å‘æ›´æœ‰æ•ˆçš„åŒ»ç–—æ•°æ®é—å¿˜æ–¹æ³•å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the "right to be forgotten". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.


### [3] [What Happens When: Learning Temporal Orders of Events in Videos](https://arxiv.org/abs/2512.08979)
*Daechul Ahn, Yura Choi, Hyeonbeom Choi, Seongwon Cho, San Kim, Jonghyun Choi*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶åºç†è§£èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œæå‡ºäº†VECTORåŸºå‡†æ¥è¯„ä¼°æ¨¡å‹å¯¹äº‹ä»¶æ—¶åºé¡ºåºçš„è¯†åˆ«èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†MECOTæ–¹æ³•é€šè¿‡äº‹ä»¶çº§æŒ‡ä»¤å¾®è°ƒå’Œæ€ç»´é“¾æç¤ºæ¥å¢å¼ºæ¨¡å‹çš„æ—¶åºæ„ŸçŸ¥èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å‡†ç¡®æ•æ‰å¤šä¸ªäº‹ä»¶æ—¶åºé¡ºåºçš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç ”ç©¶å‘ç°å³ä½¿è§†é¢‘å¸§è¢«æ‰“ä¹±ï¼Œæ¨¡å‹åœ¨ç°æœ‰åŸºå‡†ä¸Šä»è¡¨ç°è‰¯å¥½ï¼Œè¿™è¡¨æ˜æ¨¡å‹å¯èƒ½ä¾èµ–å…¸å‹åœºæ™¯çš„å…ˆéªŒçŸ¥è¯†è€Œéå‡†ç¡®çš„æ—¶åºå¤„ç†æ¥å›ç­”é—®é¢˜ï¼Œå› æ­¤éœ€è¦ä¸“é—¨è¯„ä¼°å’Œæå‡æ¨¡å‹çš„æ—¶åºç†è§£èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶æå‡ºäº†VECTORåŸºå‡†æ¥æ˜ç¡®è¯„ä¼°æ¨¡å‹è¯†åˆ«äº‹ä»¶æ—¶åºé¡ºåºçš„èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†MECOTæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šåœ¨è¯¦ç»†çš„äº‹ä»¶çº§è§†é¢‘æè¿°ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä»¥åŠåœ¨æ¨ç†æ—¶ä½¿ç”¨æ€ç»´é“¾æç¤ºæ¥å¢å¼ºæ—¶åºæ„ŸçŸ¥ã€‚MECOTé€šè¿‡å¤šäº‹ä»¶æŒ‡ä»¤å¾®è°ƒå’Œæ€ç»´é“¾æœºåˆ¶ç›¸ç»“åˆæ¥æå‡æ¨¡å‹çš„æ—¶åºç†è§£èƒ½åŠ›ã€‚

**Result:** åœ¨VECTORåŸºå‡†ä¸Šï¼Œå¤šç§è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ç»å¸¸æ— æ³•æ­£ç¡®ç†è§£äº‹ä»¶é¡ºåºã€‚MECOTæ–¹æ³•åœ¨VECTORåŸºå‡†ä¸Šè¶…è¶Šäº†å…ˆå‰æŠ€æœ¯ï¼ŒåŒæ—¶åœ¨ç°æœ‰è§†é¢‘åŸºå‡†ä¸Šçš„æ€§èƒ½ä¹Ÿæœ‰æ‰€æå‡ï¼Œè¯æ˜äº†æ—¶åºç†è§£å¢å¼ºçš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ä¾›åç»­ç ”ç©¶ä½¿ç”¨ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶åºç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œéœ€è¦ä¸“é—¨çš„è¯„ä¼°åŸºå‡†å’Œæ–¹æ³•æ¥æå‡è¿™ä¸€èƒ½åŠ›ã€‚MECOTæ–¹æ³•é€šè¿‡ç»“åˆäº‹ä»¶çº§æŒ‡ä»¤å¾®è°ƒå’Œæ€ç»´é“¾æç¤ºï¼Œæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹çš„æ—¶åºæ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºè§†é¢‘ç†è§£æ¨¡å‹çš„æ—¶åºæ¨ç†æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶å¼ºè°ƒäº†æ—¶åºç†è§£åœ¨è§†é¢‘å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†åŸºå‡†å’Œæ–¹æ³•è®ºåŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.


### [4] [Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding](https://arxiv.org/abs/2512.08981)
*Tahar Chettaoui, Naser Damer, Fadi Boutros*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç»Ÿä¸€æ–‡æœ¬-å›¾åƒåµŒå…¥ï¼ˆUTIEï¼‰çš„æ–°ç­–ç•¥ï¼Œé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›å’Œè·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ï¼Œåœ¨é¢éƒ¨åµŒå…¥ä¸­å¼•å…¥äººå£ç»Ÿè®¡æ¨¡ç³Šæ€§ï¼Œä»è€Œå‡å°‘äººè„¸è¯†åˆ«ç³»ç»Ÿä¸­çš„åè§ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜éªŒè¯å‡†ç¡®æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** äººè„¸è¯†åˆ«ç³»ç»Ÿç»å¸¸å­˜åœ¨äººå£ç»Ÿè®¡åè§ï¼Œè¿™ä¸»è¦æºäºé¢éƒ¨åµŒå…¥ä¸­äººå£ç»Ÿè®¡ç‰¹å®šä¿¡æ¯ä¸èº«ä»½ç›¸å…³ç‰¹å¾çš„çº ç¼ ï¼Œè¿™ç§çº ç¼ ä¼šå¯¼è‡´äººå£ç»Ÿè®¡å±æ€§åœ¨åµŒå…¥ç©ºé—´ä¸­æ©ç›–èº«ä»½çº¿ç´¢ï¼Œä»è€Œåœ¨ä¸åŒäººå£ç»Ÿè®¡ç¾¤ä½“ä¹‹é—´äº§ç”ŸéªŒè¯æ€§èƒ½çš„å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿç‰©è¯†åˆ«æŠ€æœ¯å‘æŒ¥é‡è¦ä½œç”¨çš„å¤šæ–‡åŒ–åŸå¸‚æ™ºèƒ½åŸºç¡€è®¾æ–½ä¸­ï¼Œè¿™ä¸€é—®é¢˜å°¤ä¸ºå…³é”®ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ç»Ÿä¸€æ–‡æœ¬-å›¾åƒåµŒå…¥ï¼ˆUTIEï¼‰ç­–ç•¥ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›å’Œè·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ç‰¹æ€§ï¼Œé€šè¿‡å°†æ¯ä¸ªç¾¤ä½“çš„äººè„¸åµŒå…¥ä¸ä»å…¶ä»–äººå£ç»Ÿè®¡ç¾¤ä½“æå–çš„æ–‡æœ¬è¡ç”Ÿäººå£ç»Ÿè®¡ç‰¹å¾è¿›è¡Œä¸°å¯Œï¼Œä»è€Œåœ¨é¢éƒ¨åµŒå…¥ä¸­å¼•å…¥äººå£ç»Ÿè®¡æ¨¡ç³Šæ€§ï¼Œé¼“åŠ±åµŒå…¥ç©ºé—´æ›´å¼ºè°ƒèº«ä»½ç›¸å…³ç‰¹å¾ï¼Œä¿ƒè¿›è·¨ç¾¤ä½“çš„å…¬å¹³éªŒè¯æ€§èƒ½ã€‚

**Result:** åœ¨RFWå’ŒBFWè¿™ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„è¯„ä¼°äººè„¸è¯†åˆ«åè§çš„åŸºå‡†æµ‹è¯•ä¸Šï¼Œä½¿ç”¨CLIPã€OpenCLIPå’ŒSigLIPä¸‰ç§è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå®éªŒï¼Œç»“æœè¡¨æ˜UTIEèƒ½å¤Ÿä¸€è‡´æ€§åœ°é™ä½åè§æŒ‡æ ‡ï¼ŒåŒæ—¶åœ¨è®¸å¤šæƒ…å†µä¸‹ä¿æŒç”šè‡³æé«˜äº†äººè„¸éªŒè¯çš„å‡†ç¡®æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ï¼Œå¯ä»¥åœ¨é¢éƒ¨åµŒå…¥ä¸­æœ‰æ•ˆå¼•å…¥äººå£ç»Ÿè®¡æ¨¡ç³Šæ€§ï¼Œä»è€Œå‡å°‘äººè„¸è¯†åˆ«ç³»ç»Ÿçš„åè§ï¼Œè¿™ç§æ–¹æ³•ä¸ºå¼€å‘æ›´å…¬å¹³çš„ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ–‡åŒ–åŸå¸‚ç¯å¢ƒä¸­ï¼ŒåŒæ—¶ä¿æŒäº†ç³»ç»Ÿçš„å®ç”¨æ€§ã€‚

---

#### ğŸ“„ Abstract
Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.


### [5] [Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy](https://arxiv.org/abs/2512.08986)
*Anca Mihai, Adrian Groza*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç³–å°¿ç—…è§†ç½‘è†œç—…å˜è¯Šæ–­çš„AIè®­ç»ƒæ•°æ®è´¨é‡æ§åˆ¶æ¡†æ¶ï¼Œé€šè¿‡å¯è§£é‡Šç‰¹å¾åˆ†ç±»å™¨ç­›é€‰å›¾åƒã€æ·±åº¦å­¦ä¹ è¾…åŠ©æ ‡æ³¨ä»¥åŠæ ‡æ³¨è€…ä¸€è‡´æ€§è®¡ç®—ï¼Œç¡®ä¿ä»…ä½¿ç”¨é«˜è´¨é‡æ•°æ®ç”¨äºæ¨¡å‹è¯„ä¼°å’Œè®­ç»ƒã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç³–å°¿ç—…è§†ç½‘è†œç—…å˜çš„æ—©æœŸè¯Šæ–­å¯¹é¢„é˜²è§†åŠ›ä¸§å¤±è‡³å…³é‡è¦ï¼Œä½†AIæ¨¡å‹è®­ç»ƒéœ€è¦é«˜è´¨é‡æ ‡æ³¨æ•°æ®ã€‚ç”±äºè§†ç½‘è†œç»“æ„å¤æ‚ï¼Œå›¾åƒé‡‡é›†é”™è¯¯å’Œäººå·¥æ ‡æ³¨è€…è§£é‡Šå·®å¼‚å¯¼è‡´æ•°æ®è´¨é‡ä¸ä¸€è‡´ï¼Œè¿™å½±å“äº†AIæ¨¡å‹çš„å¯é æ€§å’Œæ€§èƒ½ã€‚

**Method:** ç ”ç©¶æå‡ºä¸€ä¸ªä¸‰é˜¶æ®µè´¨é‡æ§åˆ¶æ¡†æ¶ï¼šé¦–å…ˆä½¿ç”¨å¯è§£é‡Šç‰¹å¾åˆ†ç±»å™¨ç­›é€‰ä¸å……åˆ†å›¾åƒï¼Œç‰¹å¾é€šè¿‡å›¾åƒå¤„ç†å’Œå¯¹æ¯”å­¦ä¹ æå–ï¼›ç„¶åå¯¹å›¾åƒè¿›è¡Œå¢å¼ºå¹¶é‡‡ç”¨æ·±åº¦å­¦ä¹ è¾…åŠ©æ ‡æ³¨ï¼›æœ€åé€šè¿‡æ¨å¯¼å…¬å¼è®¡ç®—æ ‡æ³¨è€…é—´ä¸€è‡´æ€§æ¥ç¡®å®šæ ‡æ³¨çš„å¯ç”¨æ€§ã€‚

**Result:** è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œè¿‡æ»¤ä½è´¨é‡å›¾åƒï¼Œç¡®ä¿åªæœ‰ç¬¦åˆé«˜æ ‡å‡†çš„æ•°æ®ç”¨äºAIè®­ç»ƒå’Œè¯„ä¼°ã€‚é€šè¿‡æ·±åº¦å­¦ä¹ è¾…åŠ©æ ‡æ³¨æé«˜äº†æ ‡æ³¨æ•ˆç‡ï¼Œè€Œæ ‡æ³¨è€…ä¸€è‡´æ€§è®¡ç®—ä¸ºæ•°æ®è´¨é‡æä¾›äº†é‡åŒ–è¯„ä¼°æŒ‡æ ‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†æ•°æ®è´¨é‡æ§åˆ¶åœ¨åŒ»ç–—AIåº”ç”¨ä¸­çš„é‡è¦æ€§ï¼Œæå‡ºçš„æ¡†æ¶ä¸ºæ„å»ºå¯é ç³–å°¿ç—…è§†ç½‘è†œç—…å˜è¯Šæ–­ç³»ç»Ÿæä¾›äº†ç³»ç»ŸåŒ–è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å¯æ¨å¹¿åˆ°å…¶ä»–åŒ»å­¦å½±åƒåˆ†æä»»åŠ¡ï¼Œæœ‰åŠ©äºæé«˜AIè¾…åŠ©è¯Šæ–­çš„å‡†ç¡®æ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚

---

#### ğŸ“„ Abstract
Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.


### [6] [Towards Lossless Ultimate Vision Token Compression for VLMs](https://arxiv.org/abs/2512.09010)
*Dehua Zheng, Mouxiao Huang, Borui Jiang, Hailin Hu, Xinghao Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†LUVCæ¡†æ¶ï¼Œé€šè¿‡è§†è§‰ç¼–ç å™¨çš„è¿­ä»£åˆå¹¶å’ŒLLMä¸­çš„é¢‘è°±å‰ªæå•å…ƒï¼Œå®ç°è§†è§‰ä»¤ç‰Œçš„æ— æŸå‹ç¼©ï¼Œåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶æ˜¾è‘—åŠ é€Ÿè§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒå’Œè§†é¢‘æ—¶é¢ä¸´è®¡ç®—æ•ˆç‡å’Œå»¶è¿ŸæŒ‘æˆ˜ï¼Œä¸»è¦æºäºè§†è§‰ä»¤ç‰Œè¡¨ç¤ºä¸­çš„å¤§é‡å†—ä½™ã€‚ç°æœ‰åŸºäºæ³¨æ„åŠ›/ç›¸ä¼¼æ€§çš„å‹ç¼©ç®—æ³•å­˜åœ¨ä½ç½®åå·®æˆ–ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¯¼è‡´ç²¾åº¦æ˜¾è‘—ä¸‹é™ï¼Œä¸”æ— æ³•æ³›åŒ–åˆ°è·¨æ¨¡æ€äº¤äº’è¾ƒå¼±çš„æµ…å±‚LLMã€‚

**Method:** æå‡ºLUVCæ¡†æ¶ï¼Œé€šè¿‡ç©ºé—´è½´æ­£äº¤çš„æœ‰æ•ˆè¿­ä»£åˆå¹¶æ–¹æ¡ˆå°†ä»¤ç‰Œå‹ç¼©æ‰©å±•åˆ°è§†è§‰ç¼–ç å™¨ï¼ŒåŠ é€Ÿæ•´ä¸ªVLMè®¡ç®—ã€‚åœ¨LLMä¸­é›†æˆåŸºäºæ— æ³¨æ„åŠ›/ç›¸ä¼¼æ€§çš„ä½é€šæ»¤æ³¢å™¨çš„é¢‘è°±å‰ªæå•å…ƒï¼Œé€æ­¥å‰ªæå†—ä½™è§†è§‰ä»¤ç‰Œï¼Œå®Œå…¨å…¼å®¹ç°ä»£FlashAttentionã€‚ç³»ç»Ÿå‹ç¼©è§†è§‰ä»¤ç‰Œç›´è‡³LLMæœ€ç»ˆå±‚å®Œå…¨æ¶ˆé™¤ï¼Œä½¿é«˜ç»´è§†è§‰ç‰¹å¾é€æ­¥èå…¥å¤šæ¨¡æ€æŸ¥è¯¢ã€‚

**Result:** å®éªŒè¡¨æ˜LUVCåœ¨è¯­è¨€æ¨¡å‹ä¸­å®ç°2å€æ¨ç†åŠ é€Ÿï¼ŒåŒæ—¶ç²¾åº¦ä¸‹é™å¯å¿½ç•¥ä¸è®¡ã€‚æ— éœ€è®­ç»ƒçš„ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿç«‹å³éƒ¨ç½²åˆ°å¤šä¸ªVLMä¸­ï¼ŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

**Conclusion:** LUVCæ¡†æ¶é€šè¿‡æ­£äº¤ç©ºé—´å‹ç¼©å’Œé¢‘è°±å‰ªæçš„åˆ›æ–°ç»„åˆï¼Œè§£å†³äº†è§†è§‰ä»¤ç‰Œå†—ä½™é—®é¢˜ï¼Œåœ¨ä¿æŒæ¨¡å‹ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒå³å¯éƒ¨ç½²çš„ç‰¹æ€§ä¸ºå®é™…åº”ç”¨æä¾›äº†ä¾¿åˆ©ï¼Œä¸ºé«˜æ•ˆè§†è§‰è¯­è¨€æ¨¡å‹è®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.


### [7] [A Survey of Body and Face Motion: Datasets, Performance Evaluation Metrics and Generative Techniques](https://arxiv.org/abs/2512.09005)
*Lownish Rai Sookha, Nikhil Pakhale, Mudasir Ganaie, Abhinav Dhall*

#### ğŸ§© TL;DR
æœ¬æ–‡å¯¹é¢å‘å¯¹è¯äº¤äº’çš„å…¨èº«ä¸é¢éƒ¨è¿åŠ¨ç”Ÿæˆé¢†åŸŸè¿›è¡Œäº†é¦–æ¬¡å…¨é¢ç»¼è¿°ï¼Œç³»ç»Ÿæ¢³ç†äº†è¯¥é¢†åŸŸçš„æ ¸å¿ƒæ¦‚å¿µã€ç”Ÿæˆæ–¹æ³•ã€æ•°æ®é›†ä¸è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥æå‡è™šæ‹ŸåŒ–èº«çœŸå®æ„Ÿä¸è¡¨ç°åŠ›çš„ç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡ç”Ÿæˆå»ºæ¨¡ä¸å¤šæ¨¡æ€å­¦ä¹ çš„æœ€æ–°è¿›å±•ä½¿å¾—ä»è¯­éŸ³ã€å¯¹è¯ä¸Šä¸‹æ–‡å’Œè§†è§‰çº¿ç´¢ç”Ÿæˆäººä½“è¿åŠ¨æˆä¸ºå¯èƒ½ï¼Œä½†ç”Ÿæˆå…·æœ‰è¡¨ç°åŠ›ä¸”åè°ƒä¸€è‡´çš„é¢éƒ¨å’Œèº«ä½“åŠ¨æ€ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æºäºè¨€è¯­/éè¨€è¯­çº¿ç´¢ä¸ä¸ªä½“äººæ ¼ç‰¹è´¨ä¹‹é—´å¤æ‚çš„ç›¸äº’ä½œç”¨ï¼Œä¸”ç›®å‰ç¼ºä¹åŒæ—¶æ¶µç›–èº«ä½“ä¸é¢éƒ¨è¿åŠ¨ç”Ÿæˆçš„ç³»ç»Ÿæ€§ç»¼è¿°ã€‚

**Method:** æœ¬ç»¼è¿°ç³»ç»Ÿæ€§åœ°å›é¡¾äº†èº«ä½“ä¸é¢éƒ¨è¿åŠ¨ç”Ÿæˆçš„æ•´ä¸ªæŠ€æœ¯æ ˆï¼Œæ¶µç›–æ ¸å¿ƒæ¦‚å¿µå®šä¹‰ã€è¿åŠ¨è¡¨ç¤ºæŠ€æœ¯ï¼ˆå¦‚å‚æ•°åŒ–æ¨¡å‹ã€ç½‘æ ¼ã€ç‚¹äº‘ç­‰ï¼‰ã€å„ç±»ç”Ÿæˆæ–¹æ³•ï¼ˆåŒ…æ‹¬åŸºäºæ·±åº¦å­¦ä¹ çš„ç”Ÿæˆæ¨¡å‹ã€å¤šæ¨¡æ€èåˆæŠ€æœ¯ï¼‰ï¼Œå¹¶è¯¦ç»†æ¢³ç†äº†ç›¸å…³æ•°æ®é›†ä¸è¯„ä¼°æŒ‡æ ‡ä½“ç³»ã€‚

**Result:** ä½œä¸ºè¯¥é¢†åŸŸçš„é¦–æ¬¡å…¨é¢ç»¼è¿°ï¼Œæœ¬æ–‡æ•´åˆäº†èº«ä½“ä¸é¢éƒ¨è¿åŠ¨ç”Ÿæˆçš„å…³é”®æŠ€æœ¯è·¯çº¿ä¸ç ”ç©¶è¿›å±•ï¼Œæä¾›äº†è¯¦å°½çš„èµ„æºåˆ—è¡¨ï¼ˆåŒ…æ‹¬å…¬å¼€æ•°æ®é›†ã€ä»£ç åº“ä¸è¯„ä¼°åŸºå‡†ï¼‰ï¼Œå¹¶å»ºç«‹äº†ç»Ÿä¸€çš„åˆ†ææ¡†æ¶ä»¥æ¯”è¾ƒä¸åŒæ–¹æ³•çš„ä¼˜åŠ£ã€‚

**Conclusion:** è¯¥ç»¼è¿°æ˜ç¡®äº†æœªæ¥æå‡è™šæ‹ŸåŒ–èº«åœ¨äºŒå…ƒäº¤äº’åœºæ™¯ä¸­çœŸå®æ€§ã€åè°ƒæ€§ä¸è¡¨ç°åŠ›çš„å…³é”®æ–¹å‘ï¼Œå¼ºè°ƒäº†è·¨æ¨¡æ€ä¸€è‡´æ€§ä¸ä¸ªæ€§åŒ–å»ºæ¨¡çš„é‡è¦æ€§ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ç³»ç»Ÿæ€§çš„é¢†åŸŸæ¦‚è§ˆä¸æŠ€æœ¯è·¯çº¿å›¾ï¼Œç›¸å…³èµ„æºå·²é€šè¿‡é¡¹ç›®ç½‘ç«™å…¬å¼€ã€‚

---

#### ğŸ“„ Abstract
Body and face motion play an integral role in communication. They convey crucial information on the participants. Advances in generative modeling and multi-modal learning have enabled motion generation from signals such as speech, conversational context and visual cues. However, generating expressive and coherent face and body dynamics remains challenging due to the complex interplay of verbal / non-verbal cues and individual personality traits. This survey reviews body and face motion generation, covering core concepts, representations techniques, generative approaches, datasets and evaluation metrics. We highlight future directions to enhance the realism, coherence and expressiveness of avatars in dyadic settings. To the best of our knowledge, this work is the first comprehensive review to cover both body and face motion. Detailed resources are listed on https://lownish23csz0010.github.io/mogen/.


### [8] [Prompt-Based Continual Compositional Zero-Shot Learning](https://arxiv.org/abs/2512.09172)
*Sauda Maryam, Sara Nadeem, Faisal Qureshi, Mohsen Ali*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªåŸºäºæç¤ºçš„æŒç»­ç»„åˆé›¶æ ·æœ¬å­¦ä¹ æ¡†æ¶PromptCCZSLï¼Œé€šè¿‡å¤šæ•™å¸ˆè’¸é¦å’Œä¼šè¯æ„ŸçŸ¥ç»„åˆæç¤ºæ¥è§£å†³æŒç»­é€‚åº”æ–°å±æ€§å’Œå¯¹è±¡ç»„åˆåŒæ—¶é˜²æ­¢é—å¿˜çš„é—®é¢˜ï¼Œåœ¨UT-Zapposå’ŒC-GQAåŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬æ–‡æ—¨åœ¨è§£å†³ç»„åˆé›¶æ ·æœ¬å­¦ä¹ ä¸­çš„æŒç»­é€‚åº”é—®é¢˜ï¼Œå³è§†è§‰è¯­è¨€æ¨¡å‹éœ€è¦ä¸æ–­é€‚åº”æ–°å±æ€§ã€å¯¹è±¡åŠå…¶ç»„åˆï¼ŒåŒæ—¶é˜²æ­¢å…ˆå‰çŸ¥è¯†çš„é—å¿˜ã€‚ä¸ä¼ ç»Ÿçš„æŒç»­å­¦ä¹ ä¸åŒï¼ŒCCZSLæ›´ä¸ºå¤æ‚ï¼Œå› ä¸ºå±æ€§å’Œå¯¹è±¡å¯èƒ½åœ¨å¤šä¸ªä¼šè¯ä¸­é‡å¤å‡ºç°ï¼Œè€Œç»„åˆä¿æŒå”¯ä¸€æ€§ï¼Œè¿™éœ€è¦æ–°çš„æ–¹æ³•æ¥å¹³è¡¡çŸ¥è¯†ä¿ç•™å’Œç»„åˆæ³›åŒ–èƒ½åŠ›ã€‚

**Method:** è¯¥æ–¹æ³•åŸºäºå†»ç»“çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸»å¹²ï¼Œæå‡ºäº†é¦–ä¸ªåŸºäºæç¤ºçš„æŒç»­ç»„åˆé›¶æ ·æœ¬å­¦ä¹ æ¡†æ¶PromptCCZSLã€‚è¯¥æ¡†æ¶é€šè¿‡åŸºäºæœ€è¿‘æ€§çš„å¤šæ•™å¸ˆè’¸é¦æ¥ä¿ç•™å…ˆå‰çŸ¥è¯†ï¼Œä½¿ç”¨ä¼šè¯æ„ŸçŸ¥çš„ç»„åˆæç¤ºæ¥èåˆå¤šæ¨¡æ€ç‰¹å¾ä»¥å¤„ç†æ–°ç»„åˆï¼ŒåŒæ—¶é€šè¿‡ä¼šè¯æ— å…³çš„èåˆå­¦ä¹ å±æ€§å’Œå¯¹è±¡æç¤ºä»¥ä¿æŒå…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä½™å¼¦é”šå®šæŸå¤±æ¥ç¨³å®šå…ˆå‰çŸ¥è¯†çš„ä¿ç•™ï¼Œé€šè¿‡æ­£äº¤æŠ•å½±æŸå¤±ç¡®ä¿æ–°å±æ€§å’Œå¯¹è±¡åµŒå…¥ä¸å…ˆå‰åµŒå…¥ä¿æŒåŒºåˆ†ï¼Œä»¥åŠé€šè¿‡ä¼šè¯å†…å¤šæ ·æ€§æŸå¤±ä¿ƒè¿›å½“å‰ä¼šè¯åµŒå…¥çš„å¤šæ ·æ€§ä»¥è·å¾—æ›´ä¸°å¯Œã€æ›´å…·åŒºåˆ†æ€§çš„è¡¨ç¤ºã€‚

**Result:** åœ¨UT-Zapposå’ŒC-GQAåŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPromptCCZSLåœ¨æŒç»­ç»„åˆé›¶æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¤§å¹…ä¼˜äºå…ˆå‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹å’Œéè§†è§‰è¯­è¨€æ¨¡å‹çš„åŸºçº¿æ–¹æ³•ã€‚è¯¥æ–¹æ³•è¿˜å¼•å…¥äº†ä¸€ä¸ªç»¼åˆè¯„ä¼°åè®®ï¼Œèƒ½å¤Ÿè”åˆè¡¡é‡ç¾éš¾æ€§é—å¿˜å’Œç»„åˆæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå°é—­ä¸–ç•Œè®¾ç½®ä¸‹çš„CCZSLå»ºç«‹äº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºæŒç»­ç»„åˆé›¶æ ·æœ¬å­¦ä¹ é¢†åŸŸæä¾›äº†é¦–ä¸ªåŸºäºæç¤ºçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡åˆ›æ–°çš„å¤šæ•™å¸ˆè’¸é¦ç­–ç•¥å’ŒæŸå¤±å‡½æ•°è®¾è®¡ï¼Œæœ‰æ•ˆå¹³è¡¡äº†çŸ¥è¯†ä¿ç•™å’Œæ–°ç»„åˆé€‚åº”ä¹‹é—´çš„æƒè¡¡ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ä»…æå‡äº†æ€§èƒ½ï¼Œè¿˜å»ºç«‹äº†æ›´å…¨é¢çš„è¯„ä¼°æ ‡å‡†ï¼Œä¸ºæœªæ¥åœ¨æ›´å¤æ‚å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­çš„æŒç»­ç»„åˆå­¦ä¹ ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.


### [9] [GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model](https://arxiv.org/abs/2512.09251)
*Lalit Maurya, Saurabh Kaushik, Beth Tellman*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†GLACIAæ¡†æ¶ï¼Œé¦–æ¬¡å°†å¤§è¯­è¨€æ¨¡å‹ä¸åˆ†å‰²èƒ½åŠ›ç›¸ç»“åˆï¼Œç”¨äºå†°å·æ¹–ç›‘æµ‹ï¼Œä¸ä»…ç”Ÿæˆå‡†ç¡®çš„åˆ†å‰²æ©ç ï¼Œè¿˜æä¾›ç©ºé—´æ¨ç†è¾“å‡ºï¼Œä»¥æ”¯æŒæ›´ç›´è§‚çš„ç¾å®³é¢„é˜²å’Œå†³ç­–åˆ¶å®šã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºå·ç§¯ç¥ç»ç½‘ç»œå’Œè§†è§‰Transformerçš„å†°å·æ¹–åˆ†å‰²æ–¹æ³•å±€é™äºåƒç´ çº§é¢„æµ‹ï¼Œç¼ºä¹é«˜å±‚å…¨å±€åœºæ™¯è¯­ä¹‰å’Œäººç±»å¯è§£é‡Šçš„æ¨ç†èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ç¾å®³é¢„é˜²å’Œæ”¿ç­–åˆ¶å®šä¸­çš„åº”ç”¨æ•ˆæœã€‚

**Method:** æœ¬æ–‡æå‡ºäº†GLACIAæ¡†æ¶ï¼Œé¦–æ¬¡å°†å¤§è¯­è¨€æ¨¡å‹ä¸åˆ†å‰²èƒ½åŠ›ç›¸ç»“åˆï¼ŒåŒæ—¶æ„å»ºäº†Glacial Lake Position Reasoningæ•°æ®é›†ç®¡é“ï¼Œæä¾›å¤šæ ·åŒ–çš„ç©ºé—´åŸºç¡€é—®ç­”å¯¹ï¼Œä»¥è§£å†³é¥æ„Ÿæ•°æ®ä¸­å®ä¾‹æ„ŸçŸ¥ä½ç½®æ¨ç†æ•°æ®çš„ç¼ºä¹é—®é¢˜ã€‚

**Result:** GLACIAåœ¨mIoUæŒ‡æ ‡ä¸Šè¾¾åˆ°87.30ï¼Œæ˜¾è‘—è¶…è¶Šäº†åŸºäºCNNçš„æ–¹æ³•ï¼ˆ78.55-79.01ï¼‰ã€ViTæ–¹æ³•ï¼ˆ69.27-81.75ï¼‰ã€åœ°ç†åŸºç¡€æ¨¡å‹ï¼ˆ76.37-87.10ï¼‰ä»¥åŠåŸºäºæ¨ç†çš„åˆ†å‰²æ–¹æ³•ï¼ˆ60.12-75.66ï¼‰ï¼Œåœ¨æ‰€æœ‰æ¯”è¾ƒæ–¹æ³•ä¸­è¡¨ç°æœ€ä¼˜ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’æ”¯æŒæ›´é«˜æ•ˆå’Œå¯è§£é‡Šçš„å†³ç­–åˆ¶å®šï¼Œä¸ºå¿«é€Ÿå˜åŒ–çš„å†°å·ç¯å¢ƒä¸­çš„ç›´è§‚ç¾å®³å‡†å¤‡å’ŒçŸ¥æƒ…æ”¿ç­–åˆ¶å®šæä¾›äº†æ–°é€”å¾„ï¼Œå…¶ä»£ç å·²åœ¨GitHubä¸Šå¼€æºã€‚

---

#### ğŸ“„ Abstract
Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\textbf{G}lacial \textbf{LA}ke segmentation with \textbf{C}ontextual \textbf{I}nstance \textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA


### [10] [ConceptPose: Training-Free Zero-Shot Object Pose Estimation using Concept Vectors](https://arxiv.org/abs/2512.09056)
*Liming Kuang, Yordanka Velikova, Mahdi Saleh, Jan-Nico Zaech, Danda Pani Paudel, Benjamin Busam*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ConceptPoseï¼Œä¸€ç§æ— éœ€è®­ç»ƒä¸”æ¨¡å‹æ— å…³çš„ç‰©ä½“å§¿æ€ä¼°è®¡æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹åˆ›å»ºå¼€æ”¾è¯æ±‡çš„3Dæ¦‚å¿µå›¾ï¼Œåœ¨é›¶æ ·æœ¬ç›¸å¯¹å§¿æ€ä¼°è®¡åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿçš„ç‰©ä½“å§¿æ€ä¼°è®¡æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ•°æ®é›†ç‰¹å®šçš„è®­ç»ƒï¼Œè€Œå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹å±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¼¥åˆè¿™ä¸¤ä¸ªé¢†åŸŸï¼Œå¼€å‘ä¸€ç§æ— éœ€è®­ç»ƒä¸”æ¨¡å‹æ— å…³çš„å§¿æ€ä¼°è®¡æ–¹æ³•ã€‚

**Method:** ConceptPoseæ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹åˆ›å»ºå¼€æ”¾è¯æ±‡çš„3Dæ¦‚å¿µå›¾ï¼Œå…¶ä¸­æ¯ä¸ªç‚¹éƒ½é€šè¿‡æ˜¾è‘—æ€§å›¾æå–çš„æ¦‚å¿µå‘é‡è¿›è¡Œæ ‡è®°ï¼Œé€šè¿‡å»ºç«‹è·¨æ¦‚å¿µå›¾çš„é²æ£’3D-3Då¯¹åº”å…³ç³»ï¼Œå®ç°ç²¾ç¡®çš„6è‡ªç”±åº¦ç›¸å¯¹å§¿æ€ä¼°è®¡ã€‚

**Result:** åœ¨æ²¡æœ‰ä»»ä½•ç‰©ä½“æˆ–æ•°æ®é›†ç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨å¸¸è§çš„é›¶æ ·æœ¬ç›¸å¯¹å§¿æ€ä¼°è®¡åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œåœ¨ADD(-S)åˆ†æ•°ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•è¶…è¿‡62%ï¼ŒåŒ…æ‹¬é‚£äº›ä½¿ç”¨å¤§é‡æ•°æ®é›†ç‰¹å®šè®­ç»ƒçš„æ–¹æ³•ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›å¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºç‰©ä½“å§¿æ€ä¼°è®¡ä»»åŠ¡ï¼Œä¸ºæ— éœ€è®­ç»ƒçš„å§¿æ€ä¼°è®¡æä¾›äº†æ–°èŒƒå¼ï¼Œå±•ç¤ºäº†è·¨æ¨¡æ€è¡¨ç¤ºåœ¨å‡ ä½•è§†è§‰ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæœºå™¨äººæ„ŸçŸ¥å’Œå¢å¼ºç°å®ç­‰åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Object pose estimation is a fundamental task in computer vision and robotics, yet most methods require extensive, dataset-specific training. Concurrently, large-scale vision language models show remarkable zero-shot capabilities. In this work, we bridge these two worlds by introducing ConceptPose, a framework for object pose estimation that is both training-free and model-free. ConceptPose leverages a vision-language-model (VLM) to create open-vocabulary 3D concept maps, where each point is tagged with a concept vector derived from saliency maps. By establishing robust 3D-3D correspondences across concept maps, our approach allows precise estimation of 6DoF relative pose. Without any object or dataset-specific training, our approach achieves state-of-the-art results on common zero shot relative pose estimation benchmarks, significantly outperforming existing methods by over 62% in ADD(-S) score, including those that utilize extensive dataset-specific training.


### [11] [Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model](https://arxiv.org/abs/2512.09441)
*Jiantao Tan, Peixian Ma, Tong Yu, Wentao Zhang, Ruixuan Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç±»å¢é‡å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šé€‚é…å™¨ã€è·¨ä»»åŠ¡è¡¨ç¤ºæ ¡å‡†ç­–ç•¥å’Œä¸ç¡®å®šæ€§å¼•å¯¼æ¨ç†æœºåˆ¶ï¼Œæœ‰æ•ˆç¼“è§£äº†è·¨ä»»åŠ¡ç±»åˆ«æ··æ·†é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒåˆ†ç±»çš„æŒç»­å­¦ä¹ æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç±»å¢é‡å­¦ä¹ æ–¹æ³•åœ¨åŒºåˆ†ä¸åŒå­¦ä¹ ä»»åŠ¡ä¸­çš„ç±»åˆ«æ—¶ä»å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´è·¨ä»»åŠ¡ç±»åˆ«æ··æ·†é—®é¢˜ï¼Œè¿™é™åˆ¶äº†æŒç»­å­¦ä¹ ç³»ç»Ÿåœ¨åŒæ—¶å­¦ä¹ æ–°ç±»åˆ«çŸ¥è¯†å’Œä¿æŒæ—§ç±»åˆ«çŸ¥è¯†æ–¹é¢çš„æ€§èƒ½è¡¨ç°ã€‚

**Method:** è¯¥æ¡†æ¶é‡‡ç”¨é¢„è®­ç»ƒä¸”å†»ç»“çš„å›¾åƒç¼–ç å™¨ï¼Œé€šè¿‡æ·»åŠ ä»»åŠ¡ç‰¹å®šé€‚é…å™¨æ¥å­¦ä¹ æ–°çŸ¥è¯†ï¼Œå¹¶æå‡ºåŸºäºè½»é‡çº§æŠ•å½±å™¨æ··åˆçš„è·¨ä»»åŠ¡è¡¨ç¤ºæ ¡å‡†ç­–ç•¥ï¼Œä»¥åœ¨ç»Ÿä¸€ç‰¹å¾ç©ºé—´ä¸­æ›´å¥½åœ°åŒºåˆ†æ‰€æœ‰å·²å­¦ä¹ ç±»åˆ«ï¼ŒåŒæ—¶å¼€å‘äº†åŸºäºé¢„æµ‹ä¸ç¡®å®šæ€§çš„æ¨ç†ç­–ç•¥æ¥æ›´å‡†ç¡®åœ°é€‰æ‹©æœ€åˆé€‚çš„å›¾åƒç‰¹å¾è¿›è¡Œç±»åˆ«é¢„æµ‹ã€‚

**Result:** åœ¨å¤šç§æ•°æ®é›†å’Œå„ç§è®¾ç½®ä¸‹çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ç°æœ‰æ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œæœ‰æ•ˆç¼“è§£äº†è·¨ä»»åŠ¡ç±»åˆ«æ··æ·†é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†ç±»å¢é‡å­¦ä¹ åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡è·¨ä»»åŠ¡è¡¨ç¤ºæ ¡å‡†å’Œä¸ç¡®å®šæ€§å¼•å¯¼æ¨ç†æœºåˆ¶å¯ä»¥æœ‰æ•ˆè§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æŒç»­å­¦ä¹ ä¸­çš„ç±»åˆ«æ··æ·†é—®é¢˜ï¼Œä¸ºç±»å¢é‡å­¦ä¹ æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¹¶è¯æ˜äº†ä»»åŠ¡ç‰¹å®šé€‚é…å™¨ä¸ç‰¹å¾ç©ºé—´æ ¡å‡†ç­–ç•¥ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥æŒç»­å­¦ä¹ ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚

---

#### ğŸ“„ Abstract
Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.


### [12] [AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models](https://arxiv.org/abs/2512.09081)
*Arman Zarei, Jiacheng Pan, Matthew Gwilliam, Soheil Feizi, Zhenheng Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†AgentCompæ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›è‡ªä¸»æ„å»ºç»„åˆæ€§æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨æ™ºèƒ½ä½“åå¥½ä¼˜åŒ–æ–¹æ³•å¾®è°ƒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç»„åˆæ€§ç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œåœ¨T2I-CompBenchç­‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹è™½ç„¶åœ¨è§†è§‰è´¨é‡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç»„åˆæ€§æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥å‡†ç¡®æ•æ‰å¯¹è±¡å…³ç³»ã€å±æ€§ç»‘å®šå’Œæç¤ºä¸­çš„ç»†ç²’åº¦ç»†èŠ‚ã€‚æ ¸å¿ƒé™åˆ¶åœ¨äºæ¨¡å‹æœªç»è¿‡æ˜ç¡®è®­ç»ƒæ¥åŒºåˆ†ç»„åˆæ€§ç›¸ä¼¼çš„æç¤ºå’Œå›¾åƒï¼Œå¯¼è‡´è¾“å‡ºç»“æœåœ¨ç»†ç²’åº¦ç»†èŠ‚ä¸Šåç¦»é¢„æœŸæè¿°ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†AgentCompæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é…å¤‡å›¾åƒç”Ÿæˆã€ç¼–è¾‘å’Œè§†è§‰é—®ç­”å·¥å…·çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œè‡ªä¸»æ„å»ºç»„åˆæ€§æ•°æ®é›†ã€‚åŸºäºè¿™äº›æ•°æ®é›†ï¼Œé‡‡ç”¨æ™ºèƒ½ä½“åå¥½ä¼˜åŒ–æ–¹æ³•å¯¹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†ç»„åˆæ€§ç›¸ä¼¼çš„æ ·æœ¬ï¼Œä»è€Œå¢å¼ºæ•´ä½“ç»„åˆæ€§ç”Ÿæˆèƒ½åŠ›ã€‚

**Result:** AgentCompåœ¨T2I-CompBenchç­‰ç»„åˆæ€§åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼ŒåŒæ—¶æ²¡æœ‰æŸå®³å›¾åƒè´¨é‡â€”â€”è¿™æ˜¯å…ˆå‰æ–¹æ³•å¸¸è§çš„ç¼ºç‚¹ã€‚è¯¥æ–¹æ³•ç”šè‡³èƒ½å¤Ÿæ³›åŒ–åˆ°æœªæ˜ç¡®è®­ç»ƒçš„å…¶ä»–èƒ½åŠ›ï¼Œå¦‚æ–‡æœ¬æ¸²æŸ“ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªä¸»æ•°æ®æ„å»ºèƒ½åŠ›å’Œæ™ºèƒ½ä½“åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„ç»„åˆæ€§èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ä»…è§£å†³äº†ç»„åˆæ€§ç”Ÿæˆçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè¿˜é¿å…äº†å›¾åƒè´¨é‡ä¸‹é™çš„é—®é¢˜ï¼Œä¸ºæœªæ¥ç”Ÿæˆæ¨¡å‹çš„ç»„åˆæ€§æ”¹è¿›æä¾›äº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Text-to-image generative models have achieved remarkable visual quality but still struggle with compositionality$-$accurately capturing object relationships, attribute bindings, and fine-grained details in prompts. A key limitation is that models are not explicitly trained to differentiate between compositionally similar prompts and images, resulting in outputs that are close to the intended description yet deviate in fine-grained details. To address this, we propose AgentComp, a framework that explicitly trains models to better differentiate such compositional variations and enhance their reasoning ability. AgentComp leverages the reasoning and tool-use capabilities of large language models equipped with image generation, editing, and VQA tools to autonomously construct compositional datasets. Using these datasets, we apply an agentic preference optimization method to fine-tune text-to-image models, enabling them to better distinguish between compositionally similar samples and resulting in overall stronger compositional generation ability. AgentComp achieves state-of-the-art results on compositionality benchmarks such as T2I-CompBench, without compromising image quality$-$a common drawback in prior approaches$-$and even generalizes to other capabilities not explicitly trained for, such as text rendering.


### [13] [Explaining the Unseen: Multimodal Vision-Language Reasoning for Situational Awareness in Underground Mining Disasters](https://arxiv.org/abs/2512.09092)
*Mizanur Rahman Jewel, Mohamed Elmahallawy, Sanjay Madria, Samuel Frimpong*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMDSEï¼ˆå¤šæ¨¡æ€ç¾å®³æƒ…å¢ƒè§£é‡Šå™¨ï¼‰ï¼Œä¸€ç§æ–°é¢–çš„è§†è§‰-è¯­è¨€æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆåœ°ä¸‹ç¾å®³åœºæ™¯çš„è¯¦ç»†æ–‡æœ¬æè¿°ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›ã€åˆ†å‰²æ„ŸçŸ¥åŒè·¯å¾„è§†è§‰ç¼–ç å’Œèµ„æºé«˜æ•ˆTransformerè¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†åœ¨è§†è§‰é€€åŒ–ç¯å¢ƒä¸‹çš„æƒ…å¢ƒæ„ŸçŸ¥èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åœ°ä¸‹é‡‡çŸ¿ç¾å®³äº§ç”Ÿçš„é»‘æš—ã€ç°å°˜å’Œåå¡Œä¼šä¸¥é‡é®æŒ¡è§†çº¿ï¼Œä½¿äººç±»å’Œä¼ ç»Ÿç³»ç»Ÿéš¾ä»¥è·å¾—å‡†ç¡®çš„æƒ…å¢ƒæ„ŸçŸ¥ï¼Œç°æœ‰æ–¹æ³•åœ¨è§†è§‰ä¸¥é‡é€€åŒ–ç¯å¢ƒä¸‹æ— æ³•ç”Ÿæˆå‡†ç¡®è¯¦ç»†çš„åœºæ™¯æè¿°ï¼Œè¿™é˜»ç¢äº†ç´§æ€¥å“åº”å†³ç­–ã€‚

**Method:** MDSEæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºåœ¨ä¸¥é‡è§†è§‰é€€åŒ–ä¸‹å®ç°é²æ£’çš„è§†è§‰-æ–‡æœ¬ç‰¹å¾å¯¹é½ï¼›åˆ†å‰²æ„ŸçŸ¥åŒè·¯å¾„è§†è§‰ç¼–ç ï¼Œèåˆå…¨å±€å’ŒåŒºåŸŸç‰¹å®šçš„åµŒå…¥è¡¨ç¤ºï¼›èµ„æºé«˜æ•ˆTransformerè¯­è¨€æ¨¡å‹ï¼Œä»¥æœ€å°è®¡ç®—æˆæœ¬ç”Ÿæˆè¡¨è¾¾æ€§å¼ºçš„æè¿°ã€‚åŒæ—¶æ„å»ºäº†é¦–ä¸ªçœŸå®åœ°ä¸‹ç¾å®³åœºæ™¯å›¾åƒ-æè¿°æ•°æ®é›†UMDã€‚

**Result:** åœ¨UMDæ•°æ®é›†å’Œç›¸å…³åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMDSEæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„å›¾åƒæè¿°æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å‡†ç¡®ã€ä¸Šä¸‹æ–‡æ›´ç›¸å…³çš„æè¿°ï¼Œåœ¨è§†è§‰é®æŒ¡ç¯å¢ƒä¸­æ•æ‰å…³é”®ç»†èŠ‚ï¼Œä¸ºåœ°ä¸‹ç´§æ€¥å“åº”æä¾›äº†æ›´å¥½çš„æƒ…å¢ƒæ„ŸçŸ¥æ”¯æŒã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€èåˆåœ¨æ¶åŠ£è§†è§‰æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºçš„æ¡†æ¶ä¸ºç¾å®³å“åº”ç³»ç»Ÿæä¾›äº†å®ç”¨çš„è‡ªåŠ¨æƒ…å¢ƒè§£é‡Šå·¥å…·ï¼Œæœªæ¥å¯æ‰©å±•è‡³å…¶ä»–è§†è§‰å—é™çš„åº”æ€¥åœºæ™¯ï¼Œå¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸæ•°æ®é›†å’Œè¯„ä¼°æ ‡å‡†çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Underground mining disasters produce pervasive darkness, dust, and collapses that obscure vision and make situational awareness difficult for humans and conventional systems. To address this, we propose MDSE, Multimodal Disaster Situation Explainer, a novel vision-language framework that automatically generates detailed textual explanations of post-disaster underground scenes. MDSE has three-fold innovations: (i) Context-Aware Cross-Attention for robust alignment of visual and textual features even under severe degradation; (ii) Segmentation-aware dual pathway visual encoding that fuses global and region-specific embeddings; and (iii) Resource-Efficient Transformer-Based Language Model for expressive caption generation with minimal compute cost. To support this task, we present the Underground Mine Disaster (UMD) dataset--the first image-caption corpus of real underground disaster scenes--enabling rigorous training and evaluation. Extensive experiments on UMD and related benchmarks show that MDSE substantially outperforms state-of-the-art captioning models, producing more accurate and contextually relevant descriptions that capture crucial details in obscured environments, improving situational awareness for underground emergency response. The code is at https://github.com/mizanJewel/Multimodal-Disaster-Situation-Explainer.


### [14] [Food Image Generation on Multi-Noun Categories](https://arxiv.org/abs/2512.09095)
*Xinyue Pan, Yuhao Chen, Jiangpeng He, Fengqing Zhu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºFoCULRæ–¹æ³•ï¼Œé€šè¿‡èå…¥é£Ÿå“é¢†åŸŸçŸ¥è¯†å’Œåœ¨ç”Ÿæˆè¿‡ç¨‹æ—©æœŸå¼•å…¥æ ¸å¿ƒæ¦‚å¿µï¼Œè§£å†³äº†å¤šåè¯é£Ÿå“ç±»åˆ«å›¾åƒç”Ÿæˆä¸­è¯­ä¹‰è¯¯è§£å’Œç©ºé—´å¸ƒå±€é”™è¯¯çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šåè¯é£Ÿå“ç±»åˆ«ï¼ˆå¦‚"é¸¡è›‹é¢"ï¼‰åœ¨ç”Ÿæˆå›¾åƒæ—¶é¢ä¸´è¯­ä¹‰è¯¯è§£æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¨¡å‹å°†å¤åˆåç§°é”™è¯¯è§£æä¸ºå¤šä¸ªç‹¬ç«‹å®ä½“è€Œéå•ä¸€æ¦‚å¿µã€‚è¿™ç§é—®é¢˜åœ¨UEC-256ç­‰çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸­æ™®éå­˜åœ¨ï¼Œæºäºæ–‡æœ¬ç¼–ç å™¨ç¼ºä¹å¤šåè¯ç±»åˆ«ç›¸å…³çŸ¥è¯†ä»¥åŠå¯¹å¤šåè¯å…³ç³»çš„é”™è¯¯ç†è§£ï¼Œä»è€Œäº§ç”Ÿä¸æ­£ç¡®çš„ç©ºé—´å¸ƒå±€ã€‚

**Method:** æå‡ºçš„FoCULRæ–¹æ³•åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯ï¼šèå…¥é£Ÿå“é¢†åŸŸçŸ¥è¯†ä»¥å¢å¼ºæ¨¡å‹å¯¹å¤šåè¯ç±»åˆ«çš„ç†è§£ï¼Œä»¥åŠåœ¨ç”Ÿæˆè¿‡ç¨‹æ—©æœŸé˜¶æ®µå¼•å…¥æ ¸å¿ƒæ¦‚å¿µæ¥å¼•å¯¼æ­£ç¡®çš„è¯­ä¹‰è¡¨ç¤ºã€‚è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³æ–‡æœ¬ç¼–ç å™¨åœ¨å¤šåè¯å…³ç³»ç†è§£ä¸Šçš„ä¸è¶³ï¼Œé€šè¿‡é¢†åŸŸç‰¹å®šçŸ¥è¯†æ³¨å…¥æ”¹å–„è¯­ä¹‰è§£æã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒFoCULRæ–¹æ³•åœ¨é£Ÿå“é¢†åŸŸçš„å›¾åƒç”Ÿæˆæ€§èƒ½å¾—åˆ°æ˜¾è‘—æå‡ã€‚é€šè¿‡æ•´åˆé£Ÿå“é¢†åŸŸçŸ¥è¯†å’Œæ—©æœŸæ¦‚å¿µå¼•å…¥æŠ€æœ¯ï¼Œæœ‰æ•ˆå‡å°‘äº†å¤šåè¯ç±»åˆ«ç”Ÿæˆä¸­çš„è¯­ä¹‰é”™è¯¯ï¼Œæ”¹å–„äº†ç”Ÿæˆå›¾åƒçš„ç©ºé—´å¸ƒå±€å‡†ç¡®æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šåè¯é£Ÿå“ç±»åˆ«ç”Ÿæˆä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºè¯­ä¹‰å…³ç³»å’Œç©ºé—´å¸ƒå±€ç†è§£ï¼Œæå‡ºçš„FoCULRæ¡†æ¶ä¸ºé¢†åŸŸç‰¹å®šç”Ÿæˆä»»åŠ¡æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚æœªæ¥å·¥ä½œå¯æ‰©å±•è‡³å…¶ä»–å…·æœ‰å¤åˆæ¦‚å¿µçš„é¢†åŸŸï¼Œå¹¶è¿›ä¸€æ­¥æ¢ç´¢çŸ¥è¯†æ³¨å…¥ä¸ç”Ÿæˆè¿‡ç¨‹çš„æ·±åº¦èåˆæœºåˆ¶ã€‚

---

#### ğŸ“„ Abstract
Generating realistic food images for categories with multiple nouns is surprisingly challenging. For instance, the prompt "egg noodle" may result in images that incorrectly contain both eggs and noodles as separate entities. Multi-noun food categories are common in real-world datasets and account for a large portion of entries in benchmarks such as UEC-256. These compound names often cause generative models to misinterpret the semantics, producing unintended ingredients or objects. This is due to insufficient multi-noun category related knowledge in the text encoder and misinterpretation of multi-noun relationships, leading to incorrect spatial layouts. To overcome these challenges, we propose FoCULR (Food Category Understanding and Layout Refinement) which incorporates food domain knowledge and introduces core concepts early in the generation process. Experimental results demonstrate that the integration of these techniques improves image generation performance in the food domain.


### [15] [View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs](https://arxiv.org/abs/2512.09215)
*Yuanyuan Liu, Haiyang Mei, Dongyang Zhan, Jiayue Zhao, Dongsheng Zhou, Bo Dong, Xin Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„VLM Ã— SIèŒƒå¼ï¼Œé€šè¿‡å°†3Dç©ºé—´ä¿¡æ¯å¤–éƒ¨åŒ–ä¸ºåœºæ™¯å›¾ï¼Œä½¿è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿä½œä¸ºä¸»åŠ¨æ™ºèƒ½ä½“è¿›è¡Œå¢é‡æ£€ç´¢å’Œæ¨ç†ï¼Œå®ç°äº†é›¶æ ·æœ¬3Dè§†è§‰å®šä½çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰é›¶æ ·æœ¬3Dè§†è§‰å®šä½æ–¹æ³•é‡‡ç”¨VLM + SIèŒƒå¼ï¼Œå°†3Dç©ºé—´ä¿¡æ¯è½¬æ¢ä¸ºå¤åˆè¾“å…¥ï¼ˆå¦‚æŒ‡å®šè§†è§’æ¸²æŸ“æˆ–å¸¦æ ‡è®°çš„è§†é¢‘åºåˆ—ï¼‰ï¼Œå¯¼è‡´è§†è§‰è¡¨ç¤ºçº ç¼ ï¼Œè¿«ä½¿VLMå¤„ç†æ•´ä¸ªæ‚ä¹±çº¿ç´¢ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨ç©ºé—´è¯­ä¹‰å…³ç³»ã€‚

**Method:** æœ¬æ–‡æå‡ºVLM Ã— SIæ–°èŒƒå¼ï¼Œé€šè¿‡View-on-Graphæ–¹æ³•å°†åœºæ™¯ç»„ç»‡ä¸ºå¤šæ¨¡æ€ã€å¤šå±‚åœºæ™¯å›¾ï¼Œä½¿VLMèƒ½å¤Ÿä½œä¸ºä¸»åŠ¨æ™ºèƒ½ä½“åœ¨éå†åœºæ™¯æ—¶é€‰æ‹©æ€§è®¿é—®å¿…è¦çº¿ç´¢ï¼Œå®ç°å¢é‡æ£€ç´¢å’Œæ¨ç†ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVoGæ–¹æ³•åœ¨é›¶æ ·æœ¬3Dè§†è§‰å®šä½ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†ç»“æ„åŒ–åœºæ™¯æ¢ç´¢ä½œä¸ºæ¨è¿›é›¶æ ·æœ¬3DVGçš„æœ‰å‰æ™¯ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å°†3Dä¸Šä¸‹æ–‡ç»“æ„åŒ–ä¸ºç©ºé—´å’Œè¯­ä¹‰è¿è´¯çš„åœºæ™¯å›¾è€Œéçº ç¼ è§†è§‰è¾“å…¥çš„ä¼˜åŠ¿ï¼Œä¸ä»…é™ä½äº†VLMçš„æ¨ç†éš¾åº¦ï¼Œè¿˜é€šè¿‡ä¸»åŠ¨æ¢ç´¢å’Œæ¨ç†è‡ªç„¶äº§ç”Ÿå¯è§£é‡Šçš„é€æ­¥è¿½è¸ªï¼Œä¸º3Dè§†è§‰å®šä½æä¾›äº†é€æ˜ã€å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
3D visual grounding (3DVG) identifies objects in 3D scenes from language descriptions. Existing zero-shot approaches leverage 2D vision-language models (VLMs) by converting 3D spatial information (SI) into forms amenable to VLM processing, typically as composite inputs such as specified view renderings or video sequences with overlaid object markers. However, this VLM + SI paradigm yields entangled visual representations that compel the VLM to process entire cluttered cues, making it hard to exploit spatial semantic relationships effectively. In this work, we propose a new VLM x SI paradigm that externalizes the 3D SI into a form enabling the VLM to incrementally retrieve only what it needs during reasoning. We instantiate this paradigm with a novel View-on-Graph (VoG) method, which organizes the scene into a multi-modal, multi-layer scene graph and allows the VLM to operate as an active agent that selectively accesses necessary cues as it traverses the scene. This design offers two intrinsic advantages: (i) by structuring 3D context into a spatially and semantically coherent scene graph rather than confounding the VLM with densely entangled visual inputs, it lowers the VLM's reasoning difficulty; and (ii) by actively exploring and reasoning over the scene graph, it naturally produces transparent, step-by-step traces for interpretable 3DVG. Extensive experiments show that VoG achieves state-of-the-art zero-shot performance, establishing structured scene exploration as a promising strategy for advancing zero-shot 3DVG.


### [16] [LongT2IBench: A Benchmark for Evaluating Long Text-to-Image Generation with Graph-structured Annotations](https://arxiv.org/abs/2512.09271)
*Zhichao Yang, Tianjiao Gu, Jianjie Wang, Feiyu Lin, Xiangfei Sheng, Pengfei Chen, Leida Li*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†LongT2IBenchåŸºå‡†æ•°æ®é›†å’ŒLongT2IExpertè¯„ä¼°å™¨ï¼Œç”¨äºè§£å†³é•¿æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆåœºæ™¯ä¸‹çš„ç»†ç²’åº¦å¯¹é½è¯„ä¼°é—®é¢˜ï¼Œé€šè¿‡å›¾ç»“æ„æ ‡æ³¨å’Œå±‚æ¬¡åŒ–å¯¹é½æ€ç»´é“¾æ–¹æ³•å®ç°äº†å¯è§£é‡Šçš„é‡åŒ–è¯„ä¼°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ–‡æœ¬åˆ°å›¾åƒå¯¹é½è¯„ä¼°ä¸»è¦é›†ä¸­äºçŸ­æç¤ºåœºæ™¯ï¼Œç¼ºä¹é’ˆå¯¹é•¿æç¤ºçš„è‡ªåŠ¨åŒ–å’Œå¯è§£é‡Šè¯„ä¼°æ¨¡å‹ï¼Œç°æœ‰åŸºå‡†ä»…æä¾›MOSæˆ–Likerté‡è¡¨æ ‡æ³¨ï¼Œæ— æ³•æ”¯æŒé•¿æ–‡æœ¬åœºæ™¯ä¸‹çš„ç»†ç²’åº¦å¯¹é½åˆ†æï¼Œè¿™é™åˆ¶äº†é•¿æ–‡æœ¬T2Iè¯„ä¼°å™¨çš„å‘å±•ã€‚

**Method:** ç ”ç©¶é¦–å…ˆè®¾è®¡äº†åŒ…å«14Ké•¿æ–‡æœ¬-å›¾åƒå¯¹çš„LongT2IBenchåŸºå‡†ï¼Œé‡‡ç”¨Generate-Refine-Qualifyæ ‡æ³¨åè®®å°†é•¿æç¤ºè½¬æ¢ä¸ºåŒ…å«å®ä½“ã€å±æ€§å’Œå…³ç³»çš„å›¾ç»“æ„è¡¨ç¤ºï¼Œç„¶åæå‡ºLongT2IExpertè¯„ä¼°å™¨ï¼Œé€šè¿‡æŒ‡ä»¤å¾®è°ƒå’Œå±‚æ¬¡åŒ–å¯¹é½æ€ç»´é“¾æ–¹æ³•ä½¿å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæä¾›é‡åŒ–åˆ†æ•°å’Œç»“æ„åŒ–è§£é‡Šã€‚

**Result:** å®éªŒè¡¨æ˜LongT2IExpertåœ¨é•¿æ–‡æœ¬åˆ°å›¾åƒå¯¹é½è¯„ä¼°å’Œè§£é‡Šæ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§èƒ½ï¼ŒåŸºå‡†æ•°æ®é›†æä¾›äº†ç»†ç²’åº¦çš„å›¾ç»“æ„æ ‡æ³¨ï¼Œè¯„ä¼°å™¨èƒ½å¤ŸåŒæ—¶è¾“å‡ºé‡åŒ–å¯¹é½åˆ†æ•°å’Œç»“æ„åŒ–è§£é‡Šï¼Œæ˜¾è‘—æå‡äº†é•¿æç¤ºåœºæ™¯ä¸‹çš„è¯„ä¼°æ•ˆæœã€‚

**Conclusion:** è¯¥ç ”ç©¶å¡«è¡¥äº†é•¿æ–‡æœ¬åˆ°å›¾åƒå¯¹é½è¯„ä¼°çš„ç©ºç™½ï¼Œæå‡ºçš„å›¾ç»“æ„æ ‡æ³¨æ–¹æ³•å’Œå±‚æ¬¡åŒ–å¯¹é½æ€ç»´é“¾æ¡†æ¶ä¸ºå¯è§£é‡Šè¯„ä¼°æä¾›äº†æ–°èŒƒå¼ï¼ŒåŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°å™¨ä¸ºæœªæ¥é•¿æ–‡æœ¬T2Iç”Ÿæˆçš„è´¨é‡æ§åˆ¶å’Œç ”ç©¶å‘å±•å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
The increasing popularity of long Text-to-Image (T2I) generation has created an urgent need for automatic and interpretable models that can evaluate the image-text alignment in long prompt scenarios. However, the existing T2I alignment benchmarks predominantly focus on short prompt scenarios and only provide MOS or Likert scale annotations. This inherent limitation hinders the development of long T2I evaluators, particularly in terms of the interpretability of alignment. In this study, we contribute LongT2IBench, which comprises 14K long text-image pairs accompanied by graph-structured human annotations. Given the detail-intensive nature of long prompts, we first design a Generate-Refine-Qualify annotation protocol to convert them into textual graph structures that encompass entities, attributes, and relations. Through this transformation, fine-grained alignment annotations are achieved based on these granular elements. Finally, the graph-structed annotations are converted into alignment scores and interpretations to facilitate the design of T2I evaluation models. Based on LongT2IBench, we further propose LongT2IExpert, a LongT2I evaluator that enables multi-modal large language models (MLLMs) to provide both quantitative scores and structured interpretations through an instruction-tuning process with Hierarchical Alignment Chain-of-Thought (CoT). Extensive experiments and comparisons demonstrate the superiority of the proposed LongT2IExpert in alignment evaluation and interpretation. Data and code have been released in https://welldky.github.io/LongT2IBench-Homepage/.


### [17] [Dynamic Facial Expressions Analysis Based Parkinson's Disease Auxiliary Diagnosis](https://arxiv.org/abs/2512.09276)
*Xiaochen Huang, Xiaochen Bi, Cuihua Lv, Xin Wang, Haoyan Zhang, Wenjing Jiang, Xin Ma, Yibin Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€é¢éƒ¨è¡¨æƒ…åˆ†æçš„å¸•é‡‘æ£®ç—…è¾…åŠ©è¯Šæ–­æ–¹æ³•ï¼Œé€šè¿‡åˆ†æé¢éƒ¨è¡¨æƒ…å‡å°‘å’Œé¢éƒ¨åƒµç¡¬è¿™ä¸¤ä¸ªç‰¹å¾æ€§ç—‡çŠ¶ï¼Œå®ç°äº†93.1%çš„è¯Šæ–­å‡†ç¡®ç‡ï¼Œä¸ºå¸•é‡‘æ£®ç—…æä¾›äº†ä¸€ç§æ›´ä¾¿æ·çš„éä¾µå…¥å¼è¯Šæ–­æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¸•é‡‘æ£®ç—…ä½œä¸ºä¸€ç§å¸¸è§çš„ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œä¸¥é‡å½±å“æ‚£è€…çš„æ—¥å¸¸ç”Ÿæ´»å’Œç¤¾ä¼šäº¤å¾€ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿè¯Šæ–­æ–¹æ³•çš„å±€é™æ€§ï¼Œé€šè¿‡é’ˆå¯¹PDçš„ç‰¹å¾æ€§ä¸´åºŠç—‡çŠ¶â€”â€”é¢éƒ¨è¡¨æƒ…å‡å°‘ï¼ˆhypomimiaï¼‰ï¼Œå¼€å‘ä¸€ç§æ›´é«˜æ•ˆã€æ›´æ˜“è·å–çš„è¾…åŠ©è¯Šæ–­æ–¹æ³•ï¼Œæ”¹å–„æ½œåœ¨æ‚£è€…çš„è¯Šæ–­ä½“éªŒã€‚

**Method:** æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§å¤šæ¨¡æ€é¢éƒ¨è¡¨æƒ…åˆ†æç½‘ç»œï¼Œä¸“é—¨æå–æ‚£è€…æ‰§è¡Œå„ç§é¢éƒ¨è¡¨æƒ…æ—¶çš„è¡¨æƒ…å¼ºåº¦ç‰¹å¾ã€‚è¯¥ç½‘ç»œåŸºäºCLIPæ¶æ„ï¼Œæ•´åˆäº†è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼ŒåŒæ—¶ä¿ç•™äº†é¢éƒ¨è¡¨æƒ…çš„æ—¶é—´åŠ¨æ€ç‰¹æ€§ã€‚éšåï¼Œæå–çš„è¡¨æƒ…å¼ºåº¦ç‰¹å¾ç»è¿‡å¤„ç†åè¾“å…¥åˆ°åŸºäºLSTMçš„åˆ†ç±»ç½‘ç»œä¸­ï¼Œç”¨äºå¸•é‡‘æ£®ç—…çš„æœ€ç»ˆè¯Šæ–­ã€‚

**Result:** è¯¥æ–¹æ³•åœ¨å¸•é‡‘æ£®ç—…è¯Šæ–­ä»»åŠ¡ä¸­è¾¾åˆ°äº†93.1%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–ä½“å¤–PDè¯Šæ–­æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡åˆ†æé¢éƒ¨è¡¨æƒ…å‡å°‘å’Œé¢éƒ¨åƒµç¡¬è¿™ä¸¤ä¸ªä¸´åºŠè¡¨ç°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†å¸•é‡‘æ£®ç—…æ‚£è€…ä¸å¥åº·ä¸ªä½“ã€‚

**Conclusion:** æœ¬ç ”ç©¶æå‡ºçš„åŠ¨æ€é¢éƒ¨è¡¨æƒ…åˆ†ææŠ€æœ¯ä¸ºå¸•é‡‘æ£®ç—…æä¾›äº†ä¸€ç§ä¾¿æ·ã€éä¾µå…¥å¼çš„è¾…åŠ©è¯Šæ–­æ–¹æ¡ˆï¼Œæ”¹å–„äº†æ‚£è€…çš„è¯Šæ–­ä½“éªŒã€‚è¯¥æ–¹æ³•ä¸ä»…å…·æœ‰è¾ƒé«˜çš„è¯Šæ–­å‡†ç¡®æ€§ï¼Œè¿˜ä¸ºç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œæœªæ¥å¯æ‰©å±•åˆ°å…¶ä»–å…·æœ‰é¢éƒ¨è¡¨æƒ…å¼‚å¸¸çš„ç¥ç»ç³»ç»Ÿç–¾ç—…è¯Šæ–­ä¸­ã€‚

---

#### ğŸ“„ Abstract
Parkinson's disease (PD), a prevalent neurodegenerative disorder, significantly affects patients' daily functioning and social interactions. To facilitate a more efficient and accessible diagnostic approach for PD, we propose a dynamic facial expression analysis-based PD auxiliary diagnosis method. This method targets hypomimia, a characteristic clinical symptom of PD, by analyzing two manifestations: reduced facial expressivity and facial rigidity, thereby facilitating the diagnosis process. We develop a multimodal facial expression analysis network to extract expression intensity features during patients' performance of various facial expressions. This network leverages the CLIP architecture to integrate visual and textual features while preserving the temporal dynamics of facial expressions. Subsequently, the expression intensity features are processed and input into an LSTM-based classification network for PD diagnosis. Our method achieves an accuracy of 93.1%, outperforming other in-vitro PD diagnostic approaches. This technique offers a more convenient detection method for potential PD patients, improving their diagnostic experience.


### [18] [Transformer-Driven Multimodal Fusion for Explainable Suspiciousness Estimation in Visual Surveillance](https://arxiv.org/abs/2512.09311)
*Kuldeep Singh Yadav, Lalan Kumar*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†USE50kå’Œè½»é‡çº§è§†è§‰æ¡†æ¶DeepUSEvisionï¼Œç”¨äºå®æ—¶å¯ç–‘æ€§åˆ†æï¼Œé€šè¿‡å¤šæ¨¡æ€èåˆå®ç°å¯è§£é‡Šçš„å¨èƒæ£€æµ‹ï¼Œä¸ºæ™ºèƒ½ç›‘æ§å’Œå®‰å…¨å…³é”®åº”ç”¨å»ºç«‹äº†å¯æ‰©å±•çš„åŸºç¡€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¯ç–‘æ€§ä¼°è®¡å¯¹äºå¤æ‚ç¯å¢ƒä¸­çš„ä¸»åŠ¨å¨èƒæ£€æµ‹å’Œå…¬å…±å®‰å…¨è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨æ•°æ®é›†è§„æ¨¡ã€è®¡ç®—æ•ˆç‡å’Œå¯è§£é‡Šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦èƒ½å¤Ÿåœ¨å¤šæ ·åŒ–éå—æ§ç¯å¢ƒä¸­å®æ—¶åˆ†æå¤šç§å¯ç–‘çº¿ç´¢çš„ç»¼åˆè§£å†³æ–¹æ¡ˆã€‚

**Method:** æå‡ºçš„DeepUSEvisionæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŸºäºå¢å¼ºå‹YOLOv12æ¶æ„çš„å¯ç–‘ç‰©ä½“æ£€æµ‹å™¨ï¼Œç”¨äºé¢éƒ¨è¡¨æƒ…å’Œèº«ä½“è¯­è¨€è¯†åˆ«çš„åŒæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆDCNN-Iå’ŒDCNN-IIï¼‰ï¼Œä»¥åŠåŸºäºTransformerçš„åˆ¤åˆ«å™¨ç½‘ç»œï¼Œè¯¥ç½‘ç»œè‡ªé€‚åº”èåˆå¤šæ¨¡æ€è¾“å‡ºä»¥ç”Ÿæˆå¯è§£é‡Šçš„å¯ç–‘æ€§åˆ†æ•°ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ¡†æ¶åœ¨å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼ŒUSE50kæ•°æ®é›†åŒ…å«65,500å¼ æ¥è‡ªæœºåœºã€ç«è½¦ç«™ã€é¤å…ã€å…¬å›­ç­‰å¤šæ ·åŒ–éå—æ§ç¯å¢ƒçš„å›¾åƒï¼Œè¦†ç›–äº†æ­¦å™¨ã€ç«ç¾ã€äººç¾¤å¯†åº¦ã€å¼‚å¸¸é¢éƒ¨è¡¨æƒ…å’Œå¼‚å¸¸èº«ä½“å§¿åŠ¿ç­‰å¤šç§çº¿ç´¢ã€‚

**Conclusion:** USE50kæ•°æ®é›†å’ŒDeepUSEvisionæ¡†æ¶å…±åŒä¸ºæ™ºèƒ½ç›‘æ§å’Œå®æ—¶é£é™©è¯„ä¼°å»ºç«‹äº†å¼ºå¤§ä¸”å¯æ‰©å±•çš„åŸºç¡€ï¼Œé€šè¿‡å¤šæ¨¡æ€èåˆå’Œå¯è§£é‡Šæ€§è®¾è®¡ï¼Œè¯¥ç ”ç©¶ä¸ºå®‰å…¨å…³é”®åº”ç”¨ä¸­çš„ä¸»åŠ¨å¨èƒæ£€æµ‹æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å®æ—¶å¯ç–‘æ€§åˆ†æé¢†åŸŸçš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Suspiciousness estimation is critical for proactive threat detection and ensuring public safety in complex environments. This work introduces a large-scale annotated dataset, USE50k, along with a computationally efficient vision-based framework for real-time suspiciousness analysis. The USE50k dataset contains 65,500 images captured from diverse and uncontrolled environments, such as airports, railway stations, restaurants, parks, and other public areas, covering a broad spectrum of cues including weapons, fire, crowd density, abnormal facial expressions, and unusual body postures. Building on this dataset, we present DeepUSEvision, a lightweight and modular system integrating three key components, i.e., a Suspicious Object Detector based on an enhanced YOLOv12 architecture, dual Deep Convolutional Neural Networks (DCNN-I and DCNN-II) for facial expression and body-language recognition using image and landmark features, and a transformer-based Discriminator Network that adaptively fuses multimodal outputs to yield an interpretable suspiciousness score. Extensive experiments confirm the superior accuracy, robustness, and interpretability of the proposed framework compared to state-of-the-art approaches. Collectively, the USE50k dataset and the DeepUSEvision framework establish a strong and scalable foundation for intelligent surveillance and real-time risk assessment in safety-critical applications.


### [19] [TextGuider: Training-Free Guidance for Text Rendering via Attention Alignment](https://arxiv.org/abs/2512.09350)
*Kanghyun Baek, Sangyub Lee, Jin Young Choi, Jaewoo Song, Daemin Park, Jooyoung Choi, Chaehun Shin, Bohyung Han, Sungroh Yoon*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºTextGuiderï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹é½æ–‡æœ¬å†…å®¹æ ‡è®°ä¸å›¾åƒä¸­çš„æ–‡æœ¬åŒºåŸŸï¼Œè§£å†³æ‰©æ•£æ¨¡å‹ä¸­æ–‡æœ¬é—æ¼é—®é¢˜ï¼Œåœ¨æ–‡æœ¬æ¸²æŸ“ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡æ‰©æ•£å¼æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹è¿‘æœŸå–å¾—è¿›å±•ï¼Œä½†å‡†ç¡®æ–‡æœ¬æ¸²æŸ“ä»ç„¶å›°éš¾ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ–‡æœ¬å‡†ç¡®æ€§è€Œå¿½è§†äº†æ–‡æœ¬é—æ¼é—®é¢˜ï¼Œå³æœŸæœ›æ–‡æœ¬éƒ¨åˆ†æˆ–å®Œå…¨ç¼ºå¤±ï¼Œè¿™ä¸€å…³é”®é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚

**Method:** è¯¥æ–¹æ³•åˆ†æMM-DiTæ¨¡å‹ä¸­æ–‡æœ¬ç›¸å…³æ ‡è®°çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œåœ¨å»å™ªè¿‡ç¨‹çš„æ—©æœŸé˜¶æ®µåº”ç”¨æ½œåœ¨å¼•å¯¼ï¼ŒåŸºäºä½œè€…å¼•å…¥çš„ä¸¤ç§æŸå¤±å‡½æ•°æ¥å¯¹é½æ–‡æœ¬å†…å®¹æ ‡è®°ä¸å›¾åƒä¸­çš„æ–‡æœ¬åŒºåŸŸï¼Œä»è€Œå®ç°æ— éœ€è®­ç»ƒçš„æ–‡æœ¬æ¸²æŸ“ä¼˜åŒ–ã€‚

**Result:** TextGuideråœ¨æµ‹è¯•æ—¶æ–‡æœ¬æ¸²æŸ“ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å¬å›ç‡æ–¹é¢å–å¾—æ˜¾è‘—æå‡ï¼ŒåŒæ—¶åœ¨OCRå‡†ç¡®ç‡å’ŒCLIPåˆ†æ•°æ–¹é¢è¡¨ç°å‡ºå¼ºåŠ²ç»“æœï¼Œæœ‰æ•ˆè§£å†³äº†æ–‡æœ¬é—æ¼é—®é¢˜ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡åˆ†ææ³¨æ„åŠ›æ¨¡å¼å¹¶åº”ç”¨æ—©æœŸæ½œåœ¨å¼•å¯¼å¯ä»¥æœ‰æ•ˆè§£å†³æ‰©æ•£æ¨¡å‹ä¸­çš„æ–‡æœ¬é—æ¼é—®é¢˜ï¼Œä¸ºæ— éœ€è®­ç»ƒçš„æ–‡æœ¬æ¸²æŸ“ä¼˜åŒ–æä¾›äº†æ–°æ€è·¯ï¼Œå¯¹æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å®ç”¨æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
Despite recent advances, diffusion-based text-to-image models still struggle with accurate text rendering. Several studies have proposed fine-tuning or training-free refinement methods for accurate text rendering. However, the critical issue of text omission, where the desired text is partially or entirely missing, remains largely overlooked. In this work, we propose TextGuider, a novel training-free method that encourages accurate and complete text appearance by aligning textual content tokens and text regions in the image. Specifically, we analyze attention patterns in MM-DiT models, particularly for text-related tokens intended to be rendered in the image. Leveraging this observation, we apply latent guidance during the early stage of denoising steps based on two loss functions that we introduce. Our method achieves state-of-the-art performance in test-time text rendering, with significant gains in recall and strong results in OCR accuracy and CLIP score.


### [20] [Video-QTR: Query-Driven Temporal Reasoning Framework for Lightweight Video Understanding](https://arxiv.org/abs/2512.09354)
*Xinkui Zhao, Zuxin Wang, Yifan Zhang, Guanjie Cheng, Yueshen Xu, Shuiguang Deng, Chang Liu, Naibo Wang, Jianwei Yin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Video-QTRï¼ˆæŸ¥è¯¢é©±åŠ¨æ—¶åºæ¨ç†ï¼‰ï¼Œä¸€ç§è½»é‡çº§æ¡†æ¶ï¼Œé€šè¿‡å°†è§†é¢‘ç†è§£é‡æ–°å®šä¹‰ä¸ºæŸ¥è¯¢å¼•å¯¼çš„æ¨ç†è¿‡ç¨‹ï¼ŒåŠ¨æ€åˆ†é…æ„ŸçŸ¥èµ„æºï¼Œæ˜¾è‘—å‡å°‘äº†é•¿è§†é¢‘ç†è§£çš„è®¡ç®—è´Ÿæ‹…ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½çš„åŒæ—¶å°†è¾“å…¥å¸§æ¶ˆè€—é™ä½é«˜è¾¾73%ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åº”ç”¨äºé•¿è§†é¢‘ç†è§£æ—¶é¢ä¸´è®¡ç®—å¯†é›†çš„æŒ‘æˆ˜ï¼Œå¯†é›†å¸§ç¼–ç ä¼šäº§ç”Ÿè¿‡å¤šè§†è§‰æ ‡è®°ï¼Œå¯¼è‡´é«˜å†…å­˜æ¶ˆè€—ã€å†—ä½™è®¡ç®—å’Œæœ‰é™çš„å¯æ‰©å±•æ€§ï¼Œä¼ ç»Ÿ"å…ˆå¤„ç†å†æ¨ç†"èŒƒå¼åœ¨åˆ†æè§†è§‰æµæ—¶æ•ˆç‡ä½ä¸‹ã€‚

**Method:** Video-QTRæ¡†æ¶å°†è§†é¢‘ç†è§£é‡æ–°å®šä¹‰ä¸ºæŸ¥è¯¢å¼•å¯¼çš„æ¨ç†è¿‡ç¨‹ï¼Œé‡‡ç”¨åŠ¨æ€æ„ŸçŸ¥èµ„æºåˆ†é…æœºåˆ¶ï¼ŒåŸºäºæŸ¥è¯¢çš„è¯­ä¹‰æ„å›¾åˆ›å»ºæ¨ç†ä¸æ„ŸçŸ¥ä¹‹é—´çš„è‡ªé€‚åº”åé¦ˆå¾ªç¯ï¼Œé¿å…ç¼–ç æ¯ä¸€å¸§ï¼Œå®ç°è½»é‡çº§è§†é¢‘ç†è§£ã€‚

**Result:** åœ¨MSVD-QAã€Activity Net-QAã€Movie Chatå’ŒVideo MMEç­‰äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVideo-QTRå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å°†è¾“å…¥å¸§æ¶ˆè€—å‡å°‘äº†é«˜è¾¾73%ï¼Œè¯æ˜äº†å…¶é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚

**Conclusion:** æŸ¥è¯¢é©±åŠ¨æ—¶åºæ¨ç†ä¸ºè§†é¢‘ç†è§£æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡åŠ¨æ€èµ„æºåˆ†é…å’Œè‡ªé€‚åº”åé¦ˆæœºåˆ¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—è´Ÿæ‹…ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„é•¿è§†é¢‘åˆ†æå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
The rapid development of multimodal large-language models (MLLMs) has significantly expanded the scope of visual language reasoning, enabling unified systems to interpret and describe complex visual content. However, applying these models to long-video understanding remains computationally intensive. Dense frame encoding generates excessive visual tokens, leading to high memory consumption, redundant computation, and limited scalability in real-world applications. This inefficiency highlights a key limitation of the traditional process-then-reason paradigm, which analyzes visual streams exhaustively before semantic reasoning. To address this challenge, we introduce Video-QTR (Query-Driven Temporal Reasoning), a lightweight framework that redefines video comprehension as a query-guided reasoning process. Instead of encoding every frame, Video-QTR dynamically allocates perceptual resources based on the semantic intent of the query, creating an adaptive feedback loop between reasoning and perception. Extensive experiments across five benchmarks: MSVD-QA, Activity Net-QA, Movie Chat, and Video MME demonstrate that Video-QTR achieves state-of-the-art performance while reducing input frame consumption by up to 73%. These results confirm that query-driven temporal reasoning provides an efficient and scalable solution for video understanding.


### [21] [Detection and Localization of Subdural Hematoma Using Deep Learning on Computed Tomography](https://arxiv.org/abs/2512.09393)
*Vasiliki Stoumpou, Rohan Kumar, Bernard Burman, Diego Ojeda, Tapan Mehta, Dimitris Bertsimas*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºç¡¬è†œä¸‹è¡€è‚¿çš„å¿«é€Ÿæ£€æµ‹ä¸å®šä½ï¼Œé€šè¿‡æ•´åˆä¸´åºŠå˜é‡ã€3Då·ç§¯ç¥ç»ç½‘ç»œå’Œå¢å¼ºå‹2Dåˆ†å‰²æ¨¡å‹ï¼Œå®ç°äº†é«˜ç²¾åº¦è¯Šæ–­å¹¶ç”Ÿæˆè§£å‰–å­¦æ„ä¹‰çš„å®šä½å›¾ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç¡¬è†œä¸‹è¡€è‚¿æ˜¯å¸¸è§çš„ç¥ç»å¤–ç§‘æ€¥ç—‡ï¼Œç°æœ‰è‡ªåŠ¨åŒ–å·¥å…·ä¸»è¦å…³æ³¨æ£€æµ‹è€Œç¼ºä¹å¯è§£é‡Šæ€§å’Œç©ºé—´å®šä½èƒ½åŠ›ï¼Œéœ€è¦å¼€å‘é€æ˜ã€é«˜æ€§èƒ½çš„ç³»ç»Ÿæ¥æ•´åˆå¤šæ¨¡æ€ä¸´åºŠå’Œå½±åƒä¿¡æ¯ä»¥æ”¯æŒå®æ—¶å†³ç­–ã€‚

**Method:** å¼€å‘äº†å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ•´åˆç»“æ„åŒ–ä¸´åºŠå˜é‡ã€åŸºäºCTä½“ç§¯è®­ç»ƒçš„3Då·ç§¯ç¥ç»ç½‘ç»œä»¥åŠç”¨äºSDHæ£€æµ‹å’Œå®šä½çš„transformerå¢å¼º2Dåˆ†å‰²æ¨¡å‹ï¼Œé‡‡ç”¨è´ªå©ªé›†æˆç­–ç•¥ç»“åˆäº’è¡¥é¢„æµ‹å™¨ï¼Œä½¿ç”¨25,315ä¾‹å¤´CTç ”ç©¶æ•°æ®è¿›è¡Œè®­ç»ƒã€‚

**Result:** ä¸´åºŠå˜é‡å•ç‹¬ä½¿ç”¨æ—¶åˆ¤åˆ«èƒ½åŠ›æœ‰é™ï¼ˆAUC 0.75ï¼‰ï¼ŒåŸºäºCTä½“ç§¯å’Œåˆ†å‰²è¡ç”Ÿå›¾çš„å·ç§¯æ¨¡å‹æ˜¾è‘—æé«˜å‡†ç¡®æ€§ï¼ˆAUCåˆ†åˆ«ä¸º0.922å’Œ0.926ï¼‰ï¼Œå¤šæ¨¡æ€é›†æˆæ¡†æ¶å®ç°æœ€ä½³æ•´ä½“æ€§èƒ½ï¼ˆAUC 0.9407ï¼‰ï¼Œå¹¶ç”Ÿæˆä¸å·²çŸ¥SDHæ¨¡å¼ä¸€è‡´çš„è§£å‰–å­¦æ„ä¹‰å®šä½å›¾ã€‚

**Conclusion:** è¯¥å¤šæ¨¡æ€å¯è§£é‡Šæ¡†æ¶æä¾›äº†å¿«é€Ÿå‡†ç¡®çš„SDHæ£€æµ‹å’Œå®šä½ï¼Œå®ç°äº†é«˜æ£€æµ‹æ€§èƒ½å¹¶æä¾›é€æ˜ã€è§£å‰–å­¦åŸºç¡€çš„è¾“å‡ºï¼Œæ•´åˆåˆ°æ”¾å°„å­¦å·¥ä½œæµç¨‹ä¸­å¯ç®€åŒ–åˆ†è¯Šã€å‡å°‘å¹²é¢„æ—¶é—´å¹¶æé«˜SDHç®¡ç†çš„ä¸€è‡´æ€§ã€‚

---

#### ğŸ“„ Abstract
Background. Subdural hematoma (SDH) is a common neurosurgical emergency, with increasing incidence in aging populations. Rapid and accurate identification is essential to guide timely intervention, yet existing automated tools focus primarily on detection and provide limited interpretability or spatial localization. There remains a need for transparent, high-performing systems that integrate multimodal clinical and imaging information to support real-time decision-making.
  Methods. We developed a multimodal deep-learning framework that integrates structured clinical variables, a 3D convolutional neural network trained on CT volumes, and a transformer-enhanced 2D segmentation model for SDH detection and localization. Using 25,315 head CT studies from Hartford HealthCare (2015--2024), of which 3,774 (14.9\%) contained clinician-confirmed SDH, tabular models were trained on demographics, comorbidities, medications, and laboratory results. Imaging models were trained to detect SDH and generate voxel-level probability maps. A greedy ensemble strategy combined complementary predictors.
  Findings. Clinical variables alone provided modest discriminatory power (AUC 0.75). Convolutional models trained on CT volumes and segmentation-derived maps achieved substantially higher accuracy (AUCs 0.922 and 0.926). The multimodal ensemble integrating all components achieved the best overall performance (AUC 0.9407; 95\% CI, 0.930--0.951) and produced anatomically meaningful localization maps consistent with known SDH patterns.
  Interpretation. This multimodal, interpretable framework provides rapid and accurate SDH detection and localization, achieving high detection performance and offering transparent, anatomically grounded outputs. Integration into radiology workflows could streamline triage, reduce time to intervention, and improve consistency in SDH management.


### [22] [Defect-aware Hybrid Prompt Optimization via Progressive Tuning for Zero-Shot Multi-type Anomaly Detection and Segmentation](https://arxiv.org/abs/2512.09446)
*Nadeem Nazer, Hongkuan Zhou, Lavdim Halilaj, Ylli Sadikaj, Steffen Staab*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†DAPOï¼Œä¸€ç§åŸºäºæ¸è¿›å¼è°ƒä¼˜çš„ç¼ºé™·æ„ŸçŸ¥æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºé›¶æ ·æœ¬å¤šç±»å‹å’ŒäºŒå…ƒå¼‚å¸¸æ£€æµ‹ä¸åˆ†å‰²ï¼Œé€šè¿‡å°†å¼‚å¸¸ç›¸å…³å›¾åƒç‰¹å¾ä¸å¯¹åº”æ–‡æœ¬è¯­ä¹‰å¯¹é½ï¼Œåœ¨åˆ†å¸ƒåç§»ä¸‹æ˜¾è‘—æå‡äº†å¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPåœ¨å¼‚å¸¸æ£€æµ‹ä¸­ä¸»è¦åˆ©ç”¨é«˜å±‚è¯­ä¹‰ä¿¡æ¯ï¼Œä½†å¾€å¾€å¿½ç•¥ç»†ç²’åº¦å¼‚å¸¸ç±»å‹ç»†èŠ‚ï¼Œå¦‚"å­”æ´"ã€"åˆ‡å‰²"ã€"åˆ’ç—•"ç­‰ï¼Œè¿™äº›ç»†èŠ‚èƒ½æä¾›æ›´å…·ä½“çš„å¼‚å¸¸æ€§è´¨æ´å¯Ÿã€‚æ‰‹åŠ¨ä¸ºæ¯ç§ç¼ºé™·ç±»å‹è®¾è®¡æç¤ºæ—¢è€—æ—¶åˆæ˜“å—äººä¸ºåè§å½±å“ï¼Œå› æ­¤éœ€è¦ä¸€ç§è‡ªåŠ¨åŒ–çš„ç¼ºé™·æ„ŸçŸ¥æç¤ºä¼˜åŒ–æ–¹æ³•ã€‚

**Method:** DAPOæ–¹æ³•åŸºäºæ¸è¿›å¼è°ƒä¼˜ï¼Œé€šè¿‡åŒæ—¶å­¦ä¹ å›ºå®šæ–‡æœ¬é”šç‚¹å’Œå¯å­¦ä¹ æ ‡è®°åµŒå…¥çš„æ··åˆç¼ºé™·æ„ŸçŸ¥æç¤ºï¼Œå°†å¼‚å¸¸ç›¸å…³å›¾åƒç‰¹å¾ä¸å¯¹åº”æ–‡æœ¬è¯­ä¹‰å¯¹é½ã€‚è¯¥æ–¹æ³•ä¸“é—¨è®¾è®¡ç”¨äºé›¶æ ·æœ¬å¤šç±»å‹å’ŒäºŒå…ƒå¼‚å¸¸æ£€æµ‹ä¸åˆ†å‰²ä»»åŠ¡ï¼Œåœ¨åˆ†å¸ƒåç§»æ¡ä»¶ä¸‹ä¼˜åŒ–æç¤ºè¡¨ç¤ºã€‚

**Result:** åœ¨MPDDã€VisAã€MVTec-ADã€MADå’ŒReal-IADç­‰å…¬å…±åŸºå‡†æµ‹è¯•åŠå†…éƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒDAPOåœ¨åˆ†å¸ƒåç§»ä¸‹çš„å›¾åƒçº§AUROCå’Œå¹³å‡ç²¾åº¦æŒ‡æ ‡å¹³å‡æå‡3.7%ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å®šä½æ–°å‹å¼‚å¸¸ç±»å‹çš„æ€§èƒ½å¹³å‡æå‡6.5%ã€‚

**Conclusion:** DAPOé€šè¿‡ç»†ç²’åº¦å¼‚å¸¸ç±»å‹è¯†åˆ«ä¸°å¯Œäº†"å¼‚å¸¸"çš„è¡¨å¾è¯­ä¹‰ï¼Œç¼©å°äº†ç²—ç²’åº¦å¼‚å¸¸ä¿¡å·ä¸ç»†ç²’åº¦ç¼ºé™·ç±»åˆ«ä¹‹é—´çš„å·®è·ã€‚è¯¥æ–¹æ³•ä½¿åˆ¶é€ å•†èƒ½å¤Ÿç†è§£å¼‚å¸¸çš„æ ¹æœ¬åŸå› å¹¶å¿«é€Ÿå®æ–½æ›´æœ‰é’ˆå¯¹æ€§çš„çº æ­£æªæ–½ï¼Œä¸ºå·¥ä¸šå¼‚å¸¸æ£€æµ‹æä¾›äº†æ›´ç²¾ç¡®å’Œå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Recent vision language models (VLMs) like CLIP have demonstrated impressive anomaly detection performance under significant distribution shift by utilizing high-level semantic information through text prompts. However, these models often neglect fine-grained details, such as which kind of anomalies, like "hole", "cut", "scratch" that could provide more specific insight into the nature of anomalies. We argue that recognizing fine-grained anomaly types 1) enriches the representation of "abnormal" with structured semantics, narrowing the gap between coarse anomaly signals and fine-grained defect categories; 2) enables manufacturers to understand the root causes of the anomaly and implement more targeted and appropriate corrective measures quickly. While incorporating such detailed semantic information is crucial, designing handcrafted prompts for each defect type is both time-consuming and susceptible to human bias. For this reason, we introduce DAPO, a novel approach for Defect-aware Prompt Optimization based on progressive tuning for the zero-shot multi-type and binary anomaly detection and segmentation under distribution shifts. Our approach aligns anomaly-relevant image features with their corresponding text semantics by learning hybrid defect-aware prompts with both fixed textual anchors and learnable token embeddings. We conducted experiments on public benchmarks (MPDD, VisA, MVTec-AD, MAD, and Real-IAD) and an internal dataset. The results suggest that compared to the baseline models, DAPO achieves a 3.7% average improvement in AUROC and average precision metrics at the image level under distribution shift, and a 6.5% average improvement in localizing novel anomaly types under zero-shot settings.


### [23] [Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment](https://arxiv.org/abs/2512.09555)
*Yuan Li, Zitang Sun, Yen-ju Chen, Shin'ya Nishida*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é’ˆå¯¹åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç›²å›¾åƒè´¨é‡è¯„ä¼°ä¸­å­˜åœ¨çš„é¢„æµ‹çŸ›ç›¾å’Œä¸ç¨³å®šæ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè°ƒä¼˜æ–¹æ³•ï¼Œæ˜ç¡®åˆ†ç¦»è§†è§‰æ„ŸçŸ¥ä¸è´¨é‡æ¨ç†ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†é¢„æµ‹ä¸ç¨³å®šæ€§å¹¶æå‡äº†å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç›²å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆBIQAï¼‰ä¸­å­˜åœ¨çš„å…³é”®é—®é¢˜ï¼šæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æè¿°ä¸æœ€ç»ˆè´¨é‡é¢„æµ‹ä¹‹é—´å­˜åœ¨çŸ›ç›¾ï¼Œä¸”æ¨ç†è¿‡ç¨‹ä¸­é¢„æµ‹åˆ†æ•°ä¸ç¨³å®šï¼Œè¿™äº›è¡Œä¸ºä¸äººç±»æ¨ç†æ–¹å¼ä¸ä¸€è‡´ã€‚ç ”ç©¶è¯•å›¾åˆ†æå¯¼è‡´çŸ›ç›¾è¯„ä¼°å’Œä¸ç¨³å®šæ€§çš„å› ç´ ï¼Œä»¥ä¿ƒè¿›æ›´ç¬¦åˆäººç±»æ¨ç†çš„è´¨é‡è¯„ä¼°æ–¹æ³•ã€‚

**Method:** ç ”ç©¶é¦–å…ˆåˆ†æäº†æœ€ç»ˆè´¨é‡é¢„æµ‹ä¸ç”Ÿæˆè§†è§‰ç‰¹å¾ä¹‹é—´çš„å…³ç³»ï¼Œå‘ç°é¢„æµ‹å¹¶æœªå®Œå…¨åŸºäºç‰¹å¾ä¸”é€»è¾‘è¿æ¥è–„å¼±ã€‚é€šè¿‡è§£ç VLMä¸­é—´å±‚å‘ç°æ¨¡å‹è¿‡åº¦ä¾èµ–æœ‰é™å€™é€‰è¯å…ƒå¯¼è‡´é¢„æµ‹ä¸ç¨³å®šã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè°ƒä¼˜æ–¹æ³•ï¼šç¬¬ä¸€é˜¶æ®µæ¨¡å‹å­¦ä¹ è§†è§‰ç‰¹å¾ï¼Œç¬¬äºŒé˜¶æ®µä»…åŸºäºè¿™äº›ç‰¹å¾è¿›è¡Œè´¨é‡æ¨æ–­ï¼Œä»è€Œæ˜ç¡®åˆ†ç¦»è§†è§‰æ„ŸçŸ¥ä¸è´¨é‡æ¨ç†è¿‡ç¨‹ã€‚

**Result:** åœ¨SPAQå’ŒKONIQæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å°†é¢„æµ‹ä¸ç¨³å®šæ€§ä»22.00%é™ä½è‡³12.39%ã€‚åœ¨LIVEã€CSIQã€SPAQå’ŒKONIQæ•°æ®é›†ä¸Šç›¸æ¯”åŸºçº¿å¹³å‡è·å¾—0.3124/0.3507çš„SRCC/PLCCæå‡ã€‚è¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºè¯¥æ–¹æ³•åŒæ—¶æ”¹å–„äº†æ¨ç†è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡æ˜ç¡®åˆ†ç¦»è§†è§‰æ„ŸçŸ¥ä¸è´¨é‡æ¨ç†é˜¶æ®µï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³VLMåœ¨BIQAä»»åŠ¡ä¸­çš„çŸ›ç›¾è¯„ä¼°å’Œä¸ç¨³å®šæ€§é—®é¢˜ã€‚è¿™ç§æ–¹æ³•ä¿ƒè¿›äº†æ›´ç¬¦åˆäººç±»æ¨ç†çš„è´¨é‡è¯„ä¼°è¿‡ç¨‹ï¼Œä¸ºæ”¹è¿›åŸºäºVLMçš„è´¨é‡è¯„ä¼°æ¨¡å‹æä¾›äº†é‡è¦è§è§£å’Œå®ç”¨æ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.


### [24] [Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment](https://arxiv.org/abs/2512.09573)
*Yuan Li, Zitang Sun, Yen-Ju Chen, Shin'ya Nishida*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒè´¨é‡è¯„ä¼°ä¸­å­˜åœ¨çš„ä½å±‚å¤±çœŸæ„ŸçŸ¥ç¼ºé™·ï¼Œå¹¶æå‡ºé€šè¿‡è§†è§‰ç¼–ç å™¨ç»„ä»¶çº§å¾®è°ƒæ¥å¢å¼ºè§†è§‰-è¯­è¨€å¯¹é½ï¼Œä»è€Œæ˜¾è‘—æå‡å¤±çœŸè¯†åˆ«èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒè´¨é‡è¯„ä¼°ä¸­èƒ½å¤Ÿç”Ÿæˆæè¿°æ€§è§£é‡Šï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•å¯é åœ°æ£€æµ‹åŸºæœ¬çš„ä½å±‚å¤±çœŸï¼ˆå¦‚æ¨¡ç³Šã€å™ªå£°å’Œå‹ç¼©ï¼‰ï¼Œå¹¶åœ¨é‡å¤æ¨ç†ä¸­äº§ç”Ÿä¸ä¸€è‡´çš„è¯„ä¼°ç»“æœï¼Œè¿™å¼•å‘äº†å…³äºè¿™äº›æ¨¡å‹æ˜¯å¦çœŸæ­£æ„ŸçŸ¥åˆ°å…³é”®è§†è§‰ç‰¹å¾çš„è´¨ç–‘ã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†ä½å±‚å¤±çœŸæ„ŸçŸ¥ä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹å¯¹ç‰¹å®šå¤±çœŸç±»å‹çš„åˆ†ç±»èƒ½åŠ›ï¼Œé€šè¿‡ç»„ä»¶çº§åˆ†ææ¢ç©¶æ¨¡å‹ç»“æ„è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶è®¡ç®—è§†è§‰ç‰¹å¾ä¸å¯¹åº”è¯­ä¹‰æ ‡è®°ä¹‹é—´çš„è¯­ä¹‰è·ç¦»ï¼Œç‰¹åˆ«å…³æ³¨è§†è§‰ç¼–ç å™¨çš„ç»„ä»¶çº§å¾®è°ƒä»¥å¢å¼ºè§†è§‰-è¯­è¨€å¯¹é½ã€‚

**Result:** å®éªŒè¡¨æ˜ï¼Œè™½ç„¶å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç»“æ„ä¸Šèƒ½å¤Ÿè¡¨ç¤ºä½å±‚å¤±çœŸï¼Œä½†å®ƒä»¬å®¹æ˜“è¿‡æ‹Ÿåˆè®­ç»ƒæ¨¡æ¿ï¼Œå¯¼è‡´è´¨é‡è¯„åˆ†åå·®ï¼Œè€Œé€šè¿‡æ”¹è¿›è§†è§‰ç¼–ç å™¨çš„å¯¹é½ï¼Œå¤±çœŸè¯†åˆ«å‡†ç¡®ç‡ä»14.92%æ˜¾è‘—æå‡è‡³84.43%ï¼Œè¯æ˜äº†è§†è§‰ç¼–ç å™¨çº¦æŸçš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨è§†è§‰ç¼–ç å™¨ä¸­åŠ å…¥ä¸“é—¨çº¦æŸå¯ä»¥å¢å¼ºæ–‡æœ¬å¯è§£é‡Šçš„è§†è§‰è¡¨ç¤ºï¼Œä½¿åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æµç¨‹åœ¨è§†è§‰ä¸­å¿ƒä»»åŠ¡ä¸­äº§ç”Ÿæ›´ä¸€è‡´å’Œå¯è§£é‡Šçš„æ¨ç†ï¼Œä¸ºæ”¹è¿›è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ä½å±‚è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†é‡è¦æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.


### [25] [Content-Adaptive Image Retouching Guided by Attribute-Based Text Representation](https://arxiv.org/abs/2512.09580)
*Hancheng Zhu, Xinyu Liu, Rui Yao, Kunyang Sun, Leida Li, Abdulmotaleb El Saddik*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå±æ€§æ–‡æœ¬è¡¨ç¤ºçš„å†…å®¹è‡ªé€‚åº”å›¾åƒæ¶¦è‰²æ–¹æ³•ï¼ˆCA-ATPï¼‰ï¼Œé€šè¿‡å†…å®¹è‡ªé€‚åº”æ›²çº¿æ˜ å°„æ¨¡å—æ•æ‰å›¾åƒå†…éƒ¨é¢œè‰²å¤šæ ·æ€§ï¼Œå¹¶ç»“åˆå¤šå±æ€§æ–‡æœ¬è¡¨ç¤ºå®ç°ç”¨æˆ·å‹å¥½çš„é£æ ¼æŒ‡å¯¼ï¼Œåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å›¾åƒæ¶¦è‰²æ–¹æ³•ä¸»è¦ä¾èµ–å…¨å›¾ç»Ÿä¸€çš„åƒç´ çº§é¢œè‰²æ˜ å°„ï¼Œå¿½ç•¥äº†å›¾åƒå†…å®¹å¼•èµ·çš„å›ºæœ‰é¢œè‰²å˜åŒ–ï¼Œè¿™é™åˆ¶äº†æ–¹æ³•åœ¨é€‚åº”å¤šæ ·åŒ–é¢œè‰²åˆ†å¸ƒå’Œç”¨æˆ·å®šä¹‰é£æ ¼åå¥½æ–¹é¢çš„èƒ½åŠ›ï¼Œæ— æ³•å®ç°è‡ªé€‚åº”çš„å›¾åƒæ¶¦è‰²ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†CA-ATPæ–¹æ³•ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šå†…å®¹è‡ªé€‚åº”æ›²çº¿æ˜ å°„æ¨¡å—åˆ©ç”¨ä¸€ç³»åˆ—åŸºç¡€æ›²çº¿å»ºç«‹å¤šç§é¢œè‰²æ˜ å°„å…³ç³»å¹¶å­¦ä¹ ç›¸åº”çš„æƒé‡å›¾ï¼Œå®ç°åŸºäºç©ºé—´ä¸Šä¸‹æ–‡çš„å†…å®¹æ„ŸçŸ¥é¢œè‰²è°ƒæ•´ï¼›å±æ€§æ–‡æœ¬é¢„æµ‹æ¨¡å—ä»å¤šä¸ªå›¾åƒå±æ€§ç”Ÿæˆæ–‡æœ¬è¡¨ç¤ºï¼Œé€šè¿‡å¤šæ¨¡æ€æ¨¡å‹ä¸è§†è§‰ç‰¹å¾èåˆï¼Œæä¾›ç”¨æˆ·å‹å¥½çš„æ¶¦è‰²æŒ‡å¯¼ã€‚

**Result:** åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒæ¶¦è‰²ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å›¾åƒå†…å®¹çš„é¢œè‰²å¤šæ ·æ€§ï¼Œä½¿ç›¸ä¼¼é¢œè‰²å€¼æ ¹æ®å…¶ç©ºé—´ä¸Šä¸‹æ–‡è·å¾—ä¸åŒçš„å˜æ¢ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å†…å®¹è‡ªé€‚åº”é¢œè‰²æ˜ å°„ä¸å±æ€§æ–‡æœ¬è¡¨ç¤ºç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºå›¾åƒæ¶¦è‰²æä¾›äº†æ—¢èƒ½é€‚åº”å¤šæ ·åŒ–é¢œè‰²åˆ†å¸ƒåˆèƒ½æ»¡è¶³ç”¨æˆ·é£æ ¼åå¥½çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†è‡ªé€‚åº”å›¾åƒå¢å¼ºæŠ€æœ¯çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Image retouching has received significant attention due to its ability to achieve high-quality visual content. Existing approaches mainly rely on uniform pixel-wise color mapping across entire images, neglecting the inherent color variations induced by image content. This limitation hinders existing approaches from achieving adaptive retouching that accommodates both diverse color distributions and user-defined style preferences. To address these challenges, we propose a novel Content-Adaptive image retouching method guided by Attribute-based Text Representation (CA-ATP). Specifically, we propose a content-adaptive curve mapping module, which leverages a series of basis curves to establish multiple color mapping relationships and learns the corresponding weight maps, enabling content-aware color adjustments. The proposed module can capture color diversity within the image content, allowing similar color values to receive distinct transformations based on their spatial context. In addition, we propose an attribute text prediction module that generates text representations from multiple image attributes, which explicitly represent user-defined style preferences. These attribute-based text representations are subsequently integrated with visual features via a multimodal model, providing user-friendly guidance for image retouching. Extensive experiments on several public datasets demonstrate that our method achieves state-of-the-art performance.


### [26] [IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting](https://arxiv.org/abs/2512.09663)
*Tao Zhang, Yuyang Hong, Yang Xia, Kun Ding, Zeyu Zhang, Ying Wang, Shiming Xiang, Chunhong Pan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†IF-Benchï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨çº¢å¤–å›¾åƒç†è§£èƒ½åŠ›çš„é«˜è´¨é‡åŸºå‡†ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç”Ÿæˆå¼è§†è§‰æç¤ºæ–¹æ³•ï¼ˆGenViPï¼‰ï¼Œé€šè¿‡å°†çº¢å¤–å›¾åƒè½¬æ¢ä¸ºè¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„RGBå›¾åƒæ¥ç¼“è§£é¢†åŸŸåˆ†å¸ƒåç§»é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶åœ¨çº¢å¤–å›¾åƒç†è§£æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œå½“å‰ç¼ºä¹ä¸“é—¨ç”¨äºè¯„ä¼°çº¢å¤–å›¾åƒç†è§£çš„é«˜è´¨é‡åŸºå‡†ï¼Œè¿™é™åˆ¶äº†è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•å’Œåº”ç”¨å‘å±•ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†IF-BenchåŸºå‡†ï¼ŒåŒ…å«æ¥è‡ª23ä¸ªçº¢å¤–æ•°æ®é›†çš„499å¼ å›¾åƒå’Œ680ä¸ªç²¾å¿ƒè®¾è®¡çš„è§†è§‰é—®ç­”å¯¹ï¼Œè¦†ç›–10ä¸ªå›¾åƒç†è§£ç»´åº¦ï¼›åŒæ—¶æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç”Ÿæˆå¼è§†è§‰æç¤ºæ–¹æ³•ï¼ˆGenViPï¼‰ï¼Œåˆ©ç”¨å…ˆè¿›çš„å›¾åƒç¼–è¾‘æ¨¡å‹å°†çº¢å¤–å›¾åƒè½¬æ¢ä¸ºè¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„RGBå›¾åƒï¼Œä»¥ç¼“è§£é¢†åŸŸåˆ†å¸ƒåç§»é—®é¢˜ã€‚

**Result:** ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†è¶…è¿‡40ä¸ªå¼€æºå’Œé—­æºçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å¾ªç¯è¯„ä¼°ã€åŒè¯­è¯„ä¼°å’Œæ··åˆåˆ¤æ–­ç­–ç•¥ç¡®ä¿ç»“æœå¯é æ€§ï¼›å®éªŒè¡¨æ˜GenViPæ–¹æ³•èƒ½åœ¨å¤šç§MLLMsä¸Šå¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶æ­ç¤ºäº†æ¨¡å‹è§„æ¨¡ã€æ¶æ„å’Œæ¨ç†èŒƒå¼å¯¹çº¢å¤–å›¾åƒç†è§£çš„å½±å“è§„å¾‹ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¡«è¡¥äº†çº¢å¤–å›¾åƒç†è§£è¯„ä¼°åŸºå‡†çš„ç©ºç™½ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨çº¢å¤–é¢†åŸŸçš„åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒï¼›æå‡ºçš„GenViPæ–¹æ³•ä¸ºè§£å†³è·¨æ¨¡æ€é¢†åŸŸé€‚åº”é—®é¢˜æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼ŒåŸºå‡†æµ‹è¯•å’Œä»£ç çš„å¼€æºå°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå‘å±•ã€‚

---

#### ğŸ“„ Abstract
Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.


### [27] [An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence](https://arxiv.org/abs/2512.09670)
*Gil Weissman, Amir Ivry, Israel Cohen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å®Œå…¨è‡ªåŠ¨åŒ–çš„Tip-and-Cueæ¡†æ¶ï¼Œç”¨äºå«æ˜Ÿæˆåƒä»»åŠ¡åˆ†é…ä¸è°ƒåº¦ï¼Œé€šè¿‡å¤–éƒ¨æ•°æ®æºç”Ÿæˆæç¤ºå¹¶ä¼˜åŒ–å¤šå«æ˜Ÿè§‚æµ‹è®¡åˆ’ï¼Œç»“åˆäººå·¥æ™ºèƒ½æ¨¡å‹å¤„ç†å›¾åƒå¹¶ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Šï¼Œåœ¨æµ·ä¸Šèˆ¹èˆ¶è·Ÿè¸ªåœºæ™¯ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€å«æ˜Ÿæ˜Ÿåº§çš„æ‰©å±•ã€ä»»åŠ¡å»¶è¿Ÿçš„é™ä½ä»¥åŠä¼ æ„Ÿå™¨èƒ½åŠ›çš„å¤šæ ·åŒ–ï¼Œè‡ªåŠ¨åŒ–åœ°çƒè§‚æµ‹çš„æœºä¼šä¸æ–­å¢åŠ ï¼Œä½†ç°æœ‰ç³»ç»Ÿç¼ºä¹èƒ½å¤Ÿè‡ªåŠ¨æ•´åˆå¤–éƒ¨æ•°æ®æºã€ä¼˜åŒ–å¤šå«æ˜Ÿè°ƒåº¦å¹¶ç”Ÿæˆå¯æ“ä½œæ´å¯Ÿçš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨å®Œå…¨è‡ªåŠ¨åŒ–çš„Tip-and-Cueæ¡†æ¶ï¼Œå…¶ä¸­æç¤ºæ¥è‡ªå¤–éƒ¨æ•°æ®æºæˆ–å…ˆå‰å«æ˜Ÿå›¾åƒåˆ†æï¼Œç”¨äºè¯†åˆ«æ—¶ç©ºç›®æ ‡å¹¶ç¡®å®šä¼˜å…ˆçº§ï¼›å¯¹åº”çš„çº¿ç´¢æ˜¯å“åº”ç”Ÿæˆçš„æˆåƒä»»åŠ¡ï¼ŒåŒ…å«ä¼ æ„Ÿå™¨çº¦æŸã€æ—¶é—´è¦æ±‚å’Œæ•ˆç”¨å‡½æ•°ã€‚ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆå€™é€‰ä»»åŠ¡ï¼Œä½¿ç”¨è¿ç»­æ•ˆç”¨å‡½æ•°ä¼˜åŒ–å¤šå«æ˜Ÿè°ƒåº¦ï¼Œå¹¶é€šè¿‡åŸºäºäººå·¥æ™ºèƒ½çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬ç›®æ ‡æ£€æµ‹å™¨å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰å¤„ç†æ‰€å¾—å›¾åƒï¼Œç”Ÿæˆç»“æ„åŒ–è§†è§‰æŠ¥å‘Šä»¥æ”¯æŒå¯è§£é‡Šæ€§å’Œæ–°æ´å¯Ÿçš„è¯†åˆ«ã€‚

**Result:** è¯¥æ¡†æ¶åœ¨æµ·ä¸Šèˆ¹èˆ¶è·Ÿè¸ªåœºæ™¯ä¸­å±•ç¤ºäº†æœ‰æ•ˆæ€§ï¼Œåˆ©ç”¨è‡ªåŠ¨è¯†åˆ«ç³»ç»Ÿæ•°æ®è¿›è¡Œè½¨è¿¹é¢„æµ‹ã€ç›®æ ‡è§‚æµ‹å’Œå¯æ“ä½œè¾“å‡ºç”Ÿæˆã€‚æµ·ä¸Šèˆ¹èˆ¶è·Ÿè¸ªä½œä¸ºå¹¿æ³›ç ”ç©¶çš„åº”ç”¨é¢†åŸŸï¼Œå¸¸è¢«ç”¨äºè¯„ä¼°å«æ˜Ÿä»»åŠ¡åˆ†é…ã€é¢„æµ‹å’Œåˆ†æçš„æ–°æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥ç³»ç»Ÿçš„å®é™…åº”ç”¨ä»·å€¼ã€‚

**Conclusion:** è¯¥ç ”ç©¶æå‡ºçš„è‡ªåŠ¨åŒ–Tip-and-Cueæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå¤šæºæ•°æ®ã€ä¼˜åŒ–å«æ˜Ÿè§‚æµ‹è°ƒåº¦å¹¶ç”Ÿæˆç»“æ„åŒ–åˆ†ææŠ¥å‘Šï¼Œç³»ç»Ÿå¯æ‰©å±•è‡³æ™ºæ…§åŸå¸‚ç›‘æµ‹å’Œç¾å®³å“åº”ç­‰æ›´å¹¿æ³›çš„åº”ç”¨é¢†åŸŸï¼Œå…¶ä¸­åŠæ—¶çš„ä»»åŠ¡åˆ†é…å’Œè‡ªåŠ¨åŒ–åˆ†æè‡³å…³é‡è¦ï¼Œä¸ºåœ°çƒè§‚æµ‹ç³»ç»Ÿçš„æ™ºèƒ½åŒ–å‘å±•æä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.


### [28] [Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2512.09801)
*Tien-Dat Chung, Ba-Thinh Lam, Thanh-Huy Nguyen, Thien Nguyen, Nguyen Lan Vi Vu, Hoang-Loc Cao, Phat Kim Huynh, Min Xu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŠç›‘ç£å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ€ç‰¹å¼‚æ€§å¢å¼ºæ¨¡å—å’Œå¯å­¦ä¹ çš„äº’è¡¥ä¿¡æ¯èåˆæ¨¡å—ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€MRIåºåˆ—é—´è¯­ä¹‰å·®å¼‚å’Œå¯¹é½é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹çš„åˆ†å‰²æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŠç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨æ¨¡æ€é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œä¸»è¦ç”±äºMRIåºåˆ—é—´å­˜åœ¨è¯­ä¹‰å·®å¼‚å’Œé”™ä½é—®é¢˜ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹çš„æ€§èƒ½æå‡ã€‚

**Method:** æå‡ºäº†ä¸€ç§åŠç›‘ç£å¤šæ¨¡æ€æ¡†æ¶ï¼ŒåŒ…å«æ¨¡æ€ç‰¹å¼‚æ€§å¢å¼ºæ¨¡å—ï¼ˆMEMï¼‰é€šè¿‡é€šé“æ³¨æ„åŠ›æœºåˆ¶å¼ºåŒ–æ¯ä¸ªæ¨¡æ€çš„ç‹¬ç‰¹è¯­ä¹‰çº¿ç´¢ï¼Œä»¥åŠå¯å­¦ä¹ çš„äº’è¡¥ä¿¡æ¯èåˆæ¨¡å—ï¼ˆCIFï¼‰è‡ªé€‚åº”åœ°åœ¨æ¨¡æ€é—´äº¤æ¢äº’è¡¥çŸ¥è¯†ï¼Œæ•´ä½“é‡‡ç”¨ç›‘ç£åˆ†å‰²æŸå¤±å’Œè·¨æ¨¡æ€ä¸€è‡´æ€§æ­£åˆ™åŒ–çš„æ··åˆç›®æ ‡å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚

**Result:** åœ¨BraTS 2019ï¼ˆHGGå­é›†ï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨1%ã€5%å’Œ10%æ ‡æ³¨æ•°æ®è®¾ç½®ä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŠç›‘ç£å’Œå¤šæ¨¡æ€åŸºçº¿æ–¹æ³•ï¼Œåœ¨Diceå’ŒSensitivityæŒ‡æ ‡ä¸Šå‡å–å¾—æ˜¾è‘—æå‡ï¼Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†MEMå’ŒCIFæ¨¡å—åœ¨å¼¥åˆè·¨æ¨¡æ€å·®å¼‚å’Œæå‡åˆ†å‰²é²æ£’æ€§æ–¹é¢çš„äº’è¡¥æ•ˆåº”ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ˜¾å¼å¢å¼ºæ¨¡æ€ç‰¹å¼‚æ€§è¡¨ç¤ºå¹¶è‡ªé€‚åº”èåˆè·¨æ¨¡æ€ä¿¡æ¯å¯¹äºåŠç›‘ç£å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè§£å†³æ¨¡æ€é—´è¯­ä¹‰å·®å¼‚å’Œé”™ä½é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œä¸ºæœ‰é™æ ‡æ³¨æ•°æ®ä¸‹çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†æå¼€è¾Ÿäº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1\%, 5\%, and 10\% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.


### [29] [DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.09814)
*Zhizhong Wang, Tianyi Chu, Zeyi Huang, Nanyang Wang, Kehan Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºDynaIPï¼ˆåŠ¨æ€å›¾åƒæç¤ºé€‚é…å™¨ï¼‰ï¼Œä¸€ç§ç”¨äºä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å…ˆè¿›æ’ä»¶ï¼Œé€šè¿‡åŠ¨æ€è§£è€¦ç­–ç•¥å’Œåˆ†å±‚ä¸“å®¶æ··åˆç‰¹å¾èåˆæ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†æ¦‚å¿µä¿çœŸåº¦ã€æç¤ºè·Ÿéšå¹³è¡¡ä»¥åŠå¤šä¸»ä½“å¯æ‰©å±•æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•é¢ä¸´ä¸‰ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šæ¦‚å¿µä¿æŒä¸æç¤ºè·Ÿéšä¹‹é—´çš„å¹³è¡¡éš¾ä»¥æŠŠæ¡ï¼Œå‚è€ƒå›¾åƒç»†ç²’åº¦ç»†èŠ‚ä¿ç•™å›°éš¾ï¼Œä»¥åŠå¤šä¸»ä½“ä¸ªæ€§åŒ–æ‰©å±•èƒ½åŠ›å—é™ã€‚ç°æœ‰æ–¹æ³•åœ¨é›¶-shotè®¾ç½®ä¸‹éš¾ä»¥åŒæ—¶è§£å†³è¿™äº›é—®é¢˜ï¼Œéœ€è¦æ–°çš„é€‚é…å™¨è®¾è®¡æ¥æå‡æ€§èƒ½ã€‚

**Method:** åŸºäºå‘ç°çš„å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨å­˜åœ¨è§£è€¦å­¦ä¹ è¡Œä¸ºçš„è§‚å¯Ÿï¼Œæå‡ºåŠ¨æ€è§£è€¦ç­–ç•¥ï¼Œåœ¨æ¨ç†æ—¶ç§»é™¤æ¦‚å¿µæ— å…³ä¿¡æ¯çš„å¹²æ‰°ã€‚åŒæ—¶è®¾è®¡åˆ†å±‚ä¸“å®¶æ··åˆç‰¹å¾èåˆæ¨¡å—ï¼Œå……åˆ†åˆ©ç”¨CLIPç¼–ç å™¨çš„åˆ†å±‚ç‰¹å¾ï¼Œå®ç°å¯¹è§†è§‰ç²’åº¦çš„çµæ´»æ§åˆ¶å¹¶æå‡ç»†ç²’åº¦æ¦‚å¿µä¿çœŸåº¦ã€‚

**Result:** åœ¨å•ä¸»ä½“å’Œå¤šä¸»ä½“ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDynaIPåœ¨æ¦‚å¿µä¿çœŸåº¦ã€æç¤ºè·Ÿéšå¹³è¡¡ä»¥åŠå¤šä¸»ä½“ç»„åˆå¯æ‰©å±•æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†è¯¥é¢†åŸŸçš„æ˜¾è‘—è¿›æ­¥ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨çš„å†…åœ¨è§£è€¦å­¦ä¹ ç‰¹æ€§ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„åŠ¨æ€è§£è€¦å’Œåˆ†å±‚ç‰¹å¾èåˆæœºåˆ¶ï¼Œä¸ºä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œåœ¨ä¿æŒæ¦‚å¿µç»†èŠ‚å’Œéµå¾ªæ–‡æœ¬æç¤ºä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ï¼ŒåŒæ—¶å¢å¼ºäº†å¤šä¸»ä½“ç»„åˆçš„çµæ´»æ€§ã€‚

---

#### ğŸ“„ Abstract
Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.


### [30] [UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving](https://arxiv.org/abs/2512.09864)
*Hao Lu, Ziyang Liu, Guangfeng Jiang, Yuanfei Luo, Sheng Chen, Yangang Zhang, Ying-Cong Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºUniUGPæ¡†æ¶ï¼Œé€šè¿‡ç»Ÿä¸€ç†è§£-ç”Ÿæˆ-è§„åˆ’æ¶æ„å°†åœºæ™¯æ¨ç†ã€æœªæ¥è§†é¢‘ç”Ÿæˆå’Œè½¨è¿¹è§„åˆ’ç›¸ç»“åˆï¼Œä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨é•¿å°¾åœºæ™¯ä¸­çš„çŸ¥è¯†å±€é™å’Œè§†è§‰åŠ¨æ€å»ºæ¨¡ä¸è¶³é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨é•¿å°¾åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦å—é™äºæœ‰é™çš„ä¸–ç•ŒçŸ¥è¯†å’Œè–„å¼±çš„è§†è§‰åŠ¨æ€å»ºæ¨¡èƒ½åŠ›ã€‚ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ–¹æ³•æ— æ³•åˆ©ç”¨æœªæ ‡è®°è§†é¢‘è¿›è¡Œè§†è§‰å› æœå­¦ä¹ ï¼Œè€ŒåŸºäºä¸–ç•Œæ¨¡å‹çš„æ–¹æ³•åˆç¼ºä¹å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**Method:** æœ¬æ–‡æ„å»ºäº†å¤šä¸ªä¸“é—¨æ•°æ®é›†ä¸ºå¤æ‚åœºæ™¯æä¾›æ¨ç†å’Œè§„åˆ’æ ‡æ³¨ï¼Œå¹¶æå‡ºåä¸ºUniUGPçš„ç»Ÿä¸€ç†è§£-ç”Ÿæˆ-è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡æ··åˆä¸“å®¶æ¶æ„ååŒåœºæ™¯æ¨ç†ã€æœªæ¥è§†é¢‘ç”Ÿæˆå’Œè½¨è¿¹è§„åˆ’ã€‚è¯¥æ¡†æ¶é›†æˆé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œåˆ©ç”¨è§†è§‰åŠ¨æ€å’Œè¯­ä¹‰æ¨ç†æå‡è§„åˆ’æ€§èƒ½ï¼Œé‡‡ç”¨å››é˜¶æ®µè®­ç»ƒç­–ç•¥é€æ­¥æ„å»ºè¿™äº›èƒ½åŠ›ã€‚

**Result:** å®éªŒè¡¨æ˜UniUGPåœ¨æ„ŸçŸ¥ã€æ¨ç†å’Œå†³ç­–æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿å°¾åœºæ™¯ä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„æ€ç»´é“¾æ¨ç†ã€ç‰©ç†ä¸€è‡´çš„è½¨è¿¹å’Œè¿è´¯çš„æœªæ¥è§†é¢‘ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡ç»Ÿä¸€æ¡†æ¶æ•´åˆè§†è§‰åŠ¨æ€å»ºæ¨¡å’Œè¯­ä¹‰æ¨ç†å¯¹æå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚åœºæ™¯ä¸­æ€§èƒ½çš„é‡è¦æ€§ã€‚UniUGPçš„æˆåŠŸè¡¨æ˜ç»“åˆä¸“é—¨æ•°æ®é›†å’Œå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆè§£å†³é•¿å°¾åœºæ™¯æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å‘å±•æä¾›äº†æ–°çš„æ¶æ„æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.


### [31] [Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs](https://arxiv.org/abs/2512.09874)
*Pius Horn, Janis Keuper*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„PDFæ•°å­¦å…¬å¼è§£æåŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œé€šè¿‡åˆæˆPDFæ–‡æ¡£å’Œåˆ›æ–°çš„LLM-as-a-judgeè¯­ä¹‰è¯„ä¼°æ–¹æ³•ï¼Œç³»ç»Ÿè¯„ä¼°äº†20å¤šç§PDFè§£æå™¨çš„æ€§èƒ½ï¼Œä¸ºä¸‹æ¸¸åº”ç”¨æä¾›äº†å…³é”®é€‰æ‹©ä¾æ®ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰PDFè§£æåŸºå‡†æµ‹è¯•è¦ä¹ˆå®Œå…¨æ’é™¤æ•°å­¦å…¬å¼ï¼Œè¦ä¹ˆç¼ºä¹è¯­ä¹‰æ„ŸçŸ¥çš„è¯„ä¼°æŒ‡æ ‡ï¼Œè¿™é™åˆ¶äº†ä»å­¦æœ¯æ–‡çŒ®ä¸­è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹å’Œæ„å»ºç§‘å­¦çŸ¥è¯†åº“çš„èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½å¤Ÿç³»ç»Ÿæ§åˆ¶å¸ƒå±€ã€å…¬å¼å’Œå†…å®¹ç‰¹å¾çš„ç²¾ç¡®è¯„ä¼°æ¡†æ¶ã€‚

**Method:** è¯¥æ–¹æ³•åŸºäºåˆæˆç”Ÿæˆçš„PDFæ–‡æ¡£æ„å»ºåŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œä½¿ç”¨ç²¾ç¡®çš„LaTeXä½œä¸ºçœŸå®æ ‡ç­¾ï¼Œå¹¶åˆ›æ–°æ€§åœ°é‡‡ç”¨LLM-as-a-judgeè¿›è¡Œè¯­ä¹‰å…¬å¼è¯„ä¼°ï¼Œç»“åˆé²æ£’çš„ä¸¤é˜¶æ®µåŒ¹é…ç®¡é“å¤„ç†è§£æå™¨è¾“å‡ºä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œé€šè¿‡äººå·¥éªŒè¯ç¡®ä¿è¯„ä¼°æ–¹æ³•çš„å¯é æ€§ã€‚

**Result:** åœ¨250ä¸ªå…¬å¼å¯¹ï¼ˆæ¥è‡ª30ä½è¯„ä¼°è€…çš„750ä¸ªè¯„åˆ†ï¼‰ä¸Šçš„äººå·¥éªŒè¯è¡¨æ˜ï¼ŒåŸºäºLLMçš„è¯„ä¼°ä¸äººç±»åˆ¤æ–­å…·æœ‰æ˜¾è‘—æ›´é«˜çš„ç›¸å…³æ€§ï¼ˆPearson r=0.78ï¼‰ï¼Œè¿œä¼˜äºCDMï¼ˆr=0.34ï¼‰å’Œæ–‡æœ¬ç›¸ä¼¼åº¦æ–¹æ³•ï¼ˆrâ‰ˆ0ï¼‰ã€‚å¯¹20å¤šç§å½“ä»£PDFè§£æå™¨ï¼ˆåŒ…æ‹¬ä¸“ç”¨OCRæ¨¡å‹ã€è§†è§‰è¯­è¨€æ¨¡å‹å’ŒåŸºäºè§„åˆ™çš„æ–¹æ³•ï¼‰åœ¨100ä¸ªåˆæˆæ–‡æ¡£å’Œ2000å¤šä¸ªå…¬å¼ä¸Šçš„è¯„ä¼°æ­ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºä»ä¸šè€…é€‰æ‹©ä¸‹æ¸¸åº”ç”¨çš„PDFè§£æå™¨æä¾›äº†å…³é”®è§è§£ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªé²æ£’ã€å¯æ‰©å±•çš„æ–¹æ³•è®ºï¼Œèƒ½å¤Ÿå¯é‡å¤åœ°è¯„ä¼°PDFå…¬å¼æå–è´¨é‡ï¼ŒåŒæ—¶å¼€æºä»£ç å’ŒåŸºå‡†æ•°æ®ä¿ƒè¿›äº†è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚

---

#### ğŸ“„ Abstract
Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench


### [32] [VisualActBench: Can VLMs See and Act like a Human?](https://arxiv.org/abs/2512.09907)
*Daoan Zhang, Pai Liu, Xiaofei Zhou, Yuan Ge, Guangchen Lan, Jing Bi, Christopher Brinton, Ehsan Hoque, Jiebo Luo*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†è§†è§‰åŠ¨ä½œæ¨ç†æ–°ä»»åŠ¡å’ŒVisualActBenchåŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ— æ–‡æœ¬æç¤ºä¸‹åŸºäºè§†è§‰è¾“å…¥è¿›è¡Œä¸»åŠ¨æ¨ç†å’Œè¡ŒåŠ¨çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹ä¸äººç±»çº§æ¨ç†ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ„ŸçŸ¥å’Œæè¿°è§†è§‰ç¯å¢ƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶åœ¨æ— æ˜¾å¼æ–‡æœ¬æç¤ºä¸‹ä»…åŸºäºè§†è§‰è¾“å…¥è¿›è¡Œä¸»åŠ¨æ¨ç†å’Œè¡ŒåŠ¨çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†è§†è§‰åŠ¨ä½œæ¨ç†æ–°ä»»åŠ¡ï¼Œå¹¶æ„å»ºäº†VisualActBenchå¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«1,074ä¸ªè§†é¢‘å’Œ3,733ä¸ªäººå·¥æ ‡æ³¨çš„åŠ¨ä½œï¼Œæ¶µç›–å››ä¸ªçœŸå®ä¸–ç•Œåœºæ™¯ï¼Œæ¯ä¸ªåŠ¨ä½œæ ‡æ³¨äº†åŠ¨ä½œä¼˜å…ˆçº§ç­‰çº§å’Œä¸»åŠ¨-ååº”ç±»å‹ï¼Œç”¨äºè¯„ä¼°29ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹çš„äººç±»å¯¹é½æ¨ç†å’Œä»·å€¼æ•æ„Ÿæ€§ã€‚

**Result:** è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å‰æ²¿æ¨¡å‹å¦‚GPT4oè¡¨ç°å‡ºç›¸å¯¹è¾ƒå¼ºçš„æ€§èƒ½ï¼Œä½†ä¸äººç±»çº§æ¨ç†ç›¸æ¯”ä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆä¸»åŠ¨ã€é«˜ä¼˜å…ˆçº§åŠ¨ä½œæ–¹é¢ï¼Œå½“å‰æ¨¡å‹åœ¨ç†è§£å¤æ‚ä¸Šä¸‹æ–‡ã€é¢„æµ‹ç»“æœå’Œä¸äººç±»å†³ç­–æ¡†æ¶å¯¹é½æ–¹é¢å­˜åœ¨æ˜æ˜¾å±€é™æ€§ã€‚

**Conclusion:** VisualActBenchä¸ºè¯„ä¼°å’Œæ”¹è¿›ä¸»åŠ¨ã€è§†è§‰ä¸­å¿ƒAIæ™ºèƒ½ä½“çš„ç°å®ä¸–ç•Œå‡†å¤‡åº¦å»ºç«‹äº†å…¨é¢åŸºç¡€ï¼Œç ”ç©¶ç»“æœå¼ºè°ƒäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸»åŠ¨æ¨ç†å’Œäººç±»ä»·å€¼å¯¹é½æ–¹é¢çš„ä¸è¶³ï¼Œä¸ºæœªæ¥æ¨¡å‹å¼€å‘æä¾›äº†é‡è¦æ–¹å‘å’Œè¯„ä¼°æ ‡å‡†ã€‚

---

#### ğŸ“„ Abstract
Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.


### [33] [ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning](https://arxiv.org/abs/2512.09924)
*Xinyu Liu, Hangjie Yuan, Yujie Wei, Jiazheng Xing, Yujin Han, Jiahao Pan, Yanbiao Ma, Chi-Min Chan, Kang Zhao, Shiwei Zhang, Wenhan Luo, Yike Guo*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Reason-Informed Video Editing (RVE)ä»»åŠ¡ï¼Œå¹¶å¼€å‘äº†ReViSEæ¡†æ¶ï¼Œé€šè¿‡è‡ªæˆ‘åæ€æ¨ç†æœºåˆ¶å°†è§†é¢‘ç”Ÿæˆä¸è¯„ä¼°ç»Ÿä¸€èµ·æ¥ï¼Œæ˜¾è‘—æå‡äº†åŸºäºæ¨ç†çš„è§†é¢‘ç¼–è¾‘æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†é¢‘ç»Ÿä¸€æ¨¡å‹è™½ç„¶åœ¨ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŸºäºæ¨ç†çš„è§†è§‰ç¼–è¾‘æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹ä¸“é—¨ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨ç†æ„ŸçŸ¥è§†é¢‘ç¼–è¾‘çš„æ•°æ®é›†ï¼Œä»¥åŠæ¨¡å‹æ¨ç†èƒ½åŠ›ä¸ç¼–è¾‘èƒ½åŠ›ä¹‹é—´å­˜åœ¨è„±èŠ‚ï¼Œå¯¼è‡´ä¸°å¯Œçš„ç†è§£æ— æ³•æœ‰æ•ˆæŒ‡å¯¼ç¼–è¾‘è¿‡ç¨‹ã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†Reason-Informed Video Editing (RVE)ä»»åŠ¡ï¼Œæ„å»ºäº†RVE-BenchåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«æ¨ç†æ„ŸçŸ¥è§†é¢‘ç¼–è¾‘å’Œä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆä¸¤ä¸ªäº’è¡¥å­é›†ï¼Œå¹¶æå‡ºäº†ReViSEæ¡†æ¶ï¼Œé‡‡ç”¨è‡ªæˆ‘åæ€æ¨ç†æœºåˆ¶ï¼Œå°†ç”Ÿæˆä¸è¯„ä¼°ç»Ÿä¸€åœ¨å•ä¸€æ¶æ„ä¸­ï¼Œåˆ©ç”¨å†…éƒ¨è§†è§‰è¯­è¨€æ¨¡å‹æä¾›å†…åœ¨åé¦ˆæ¥ä¼˜åŒ–ç”Ÿæˆå™¨çš„æ¨ç†è¡Œä¸ºã€‚

**Result:** åœ¨RVE-Benchä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒReViSEæ˜¾è‘—æå‡äº†ç¼–è¾‘å‡†ç¡®æ€§å’Œè§†è§‰ä¿çœŸåº¦ï¼Œåœ¨æ¨ç†æ„ŸçŸ¥è§†é¢‘ç¼–è¾‘å­é›†ä¸Šç›¸æ¯”æœ€å…ˆè¿›æ–¹æ³•å®ç°äº†32%çš„æ•´ä½“åˆ†æ•°æå‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡æ•´åˆæ¨ç†ä¸è§†è§‰è½¬æ¢ï¼Œå»ºç«‹äº†è¿æ¥ç†è§£ä¸ç¼–è¾‘çš„æœ‰æ•ˆæ¡†æ¶ï¼Œä¸ºè§£å†³è§†é¢‘ç¼–è¾‘ä¸­çš„æ¨ç†æŒ‘æˆ˜æä¾›äº†ç³»ç»ŸåŒ–è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºæœªæ¥è§†é¢‘ç¼–è¾‘æ¨¡å‹çš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ï¼Œå¼ºè°ƒäº†æ¨ç†èƒ½åŠ›åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [34] [Neurosymbolic Information Extraction from Transactional Documents](https://arxiv.org/abs/2512.09666)
*Arthur Hemmer, MickaÃ«l Coustaty, Nicola Bartolo, Jean-Marc Ogier*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ–‡æ¡£ä¿¡æ¯æå–çš„ç¥ç»ç¬¦å·æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆç¬¦å·éªŒè¯æ–¹æ³•å®ç°äº†æ›´æœ‰æ•ˆçš„é›¶æ ·æœ¬è¾“å‡ºå’ŒçŸ¥è¯†è’¸é¦ï¼Œåœ¨äº‹åŠ¡æ€§æ–‡æ¡£å¤„ç†ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³äº‹åŠ¡æ€§æ–‡æ¡£ä¿¡æ¯æå–ä¸­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åœ¨æ²¡æœ‰å¤§é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹å®ç°å‡†ç¡®æå–ï¼Œä»¥åŠå¦‚ä½•ç¡®ä¿æå–ç»“æœç¬¦åˆé¢†åŸŸç‰¹å®šçš„ç®—æœ¯çº¦æŸå’Œç»“æ„è¦æ±‚ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨ç¥ç»ç¬¦å·æ¡†æ¶ï¼Œç»“åˆè¯­è¨€æ¨¡å‹ç”Ÿæˆå€™é€‰æå–ç»“æœï¼Œç„¶åé€šè¿‡å¥æ³•çº§ã€ä»»åŠ¡çº§å’Œé¢†åŸŸçº§çš„å¤šå±‚æ¬¡éªŒè¯è¿›è¡Œè¿‡æ»¤ï¼Œç¡®ä¿ç¬¦åˆé¢†åŸŸç‰¹å®šçš„ç®—æœ¯çº¦æŸï¼ŒåŒæ—¶æå‡ºäº†ç”¨äºçŸ¥è¯†è’¸é¦çš„é«˜è´¨é‡æ ‡ç­¾ç”Ÿæˆæ–¹æ³•ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº‹åŠ¡æ€§æ–‡æ¡£å¤„ç†ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒF1åˆ†æ•°å’Œå‡†ç¡®ç‡å‡æœ‰æ˜æ˜¾æ”¹å–„ï¼ŒéªŒè¯äº†ç¥ç»ç¬¦å·éªŒè¯æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç¥ç»ç¬¦å·æ–¹æ³•åœ¨æ–‡æ¡£ä¿¡æ¯æå–ä¸­çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ç¬¦å·éªŒè¯å¢å¼ºè¯­è¨€æ¨¡å‹è¾“å‡ºçš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œä¸ºä½èµ„æºåœºæ™¯ä¸‹çš„æ–‡æ¡£å¤„ç†æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†çŸ¥è¯†è’¸é¦åœ¨æå‡æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $F_1$-scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.


### [35] [ChronusOmni: Improving Time Awareness of Omni Large Language Models](https://arxiv.org/abs/2512.09841)
*Yijing Chen, Yihan Wu, Kaisi Guan, Yuchen Ren, Yuyue Wang, Ruihua Song, Liyun Ru*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºChronusOmniï¼Œä¸€ç§å¢å¼ºæ—¶é—´æ„ŸçŸ¥çš„å…¨èƒ½å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºè§£å†³è§†å¬æ¨¡æ€ä¸­æ˜¾å¼å’Œéšå¼æ—¶é—´å®šä½é—®é¢˜ï¼Œé€šè¿‡ç»Ÿä¸€çš„æ—¶é—´å»ºæ¨¡å’Œå¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹è§†è§‰è¯­è¨€åœºæ™¯ï¼Œä¸“æ³¨äºæ˜¾å¼æ—¶é—´å®šä½é—®é¢˜ï¼Œä½†å¯¹éŸ³é¢‘æ¨¡æ€åˆ©ç”¨ä¸è¶³ï¼Œä¸”å¿½è§†äº†è·¨æ¨¡æ€çš„éšå¼æ—¶é—´å®šä½é—®é¢˜ï¼Œä¾‹å¦‚åœ¨è§’è‰²è¯´è¯æ—¶è¯†åˆ«è§†è§‰å†…å®¹æˆ–åœ¨è§†è§‰äº‹ä»¶å‘ç”Ÿæ—¶ç¡®å®šè¯­éŸ³å†…å®¹ï¼Œè€Œè¿™äº›è·¨æ¨¡æ€æ—¶é—´å…³ç³»åœ¨ç°å®åœºæ™¯ä¸­æ™®éå­˜åœ¨ã€‚

**Method:** é¦–å…ˆï¼Œåœ¨æ¯ä¸ªæ—¶é—´å•å…ƒä¸­å°†åŸºäºæ–‡æœ¬çš„æ—¶é—´æˆ³æ ‡è®°ä¸è§†è§‰å’ŒéŸ³é¢‘è¡¨ç¤ºäº¤é”™æ’åˆ—ï¼Œå®ç°è·¨æ¨¡æ€çš„ç»Ÿä¸€æ—¶é—´å»ºæ¨¡ï¼›å…¶æ¬¡ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ç»“åˆä¸“é—¨è®¾è®¡çš„å¥–åŠ±å‡½æ•°æ¥å¼ºåˆ¶æ­£ç¡®çš„æ—¶é—´æ’åºå¹¶å¢å¼ºç»†ç²’åº¦æ—¶é—´æ¨ç†ï¼›æ­¤å¤–ï¼Œæ„å»ºäº†ChronusAVæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¶é—´ç²¾ç¡®ã€æ¨¡æ€å®Œæ•´ä¸”è·¨æ¨¡æ€å¯¹é½çš„æ•°æ®é›†ï¼Œç”¨äºæ”¯æŒè§†å¬æ—¶é—´å®šä½ä»»åŠ¡çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒChronusOmniåœ¨ChronusAVæ•°æ®é›†ä¸Šå®ç°äº†è¶…è¿‡30%çš„æ€§èƒ½æå‡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶åœ¨å¤§å¤šæ•°å…¶ä»–æ—¶é—´å®šä½åŸºå‡†æµ‹è¯•æŒ‡æ ‡ä¸Šå–å¾—äº†é¡¶çº§ç»“æœï¼ŒåŒæ—¶ä¿æŒäº†é€šç”¨çš„è§†é¢‘å’ŒéŸ³é¢‘ç†è§£èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†è·¨æ¨¡æ€æ—¶é—´æ„ŸçŸ¥çš„é‡è¦æ€§ï¼Œæå‡ºçš„ç»Ÿä¸€æ—¶é—´å»ºæ¨¡æ¡†æ¶å’Œå¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶æœ‰æ•ˆè§£å†³äº†æ˜¾å¼å’Œéšå¼æ—¶é—´å®šä½é—®é¢˜ï¼Œä¸ºå…¨èƒ½å¤§è¯­è¨€æ¨¡å‹çš„æ—¶é—´ç†è§£èƒ½åŠ›æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æ„å»ºçš„é«˜è´¨é‡æ•°æ®é›†ä¸ºæœªæ¥ç›¸å…³ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚

---

#### ğŸ“„ Abstract
Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.
