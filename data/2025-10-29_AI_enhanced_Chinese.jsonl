{"id": "2510.23691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23691", "abs": "https://arxiv.org/abs/2510.23691", "authors": ["Zihao Wang", "Xujing Li", "Yining Ye", "Junjie Fang", "Haoming Wang", "Longxiang Liu", "Shihao Liang", "Junting Lu", "Zhiyong Wu", "Jiazhan Feng", "Wanjun Zhong", "Zili Li", "Yu Wang", "Yu Miao", "Bo Zhou", "Yuanfan Li", "Hao Wang", "Zhongkai Zhao", "Faming Wu", "Zhengxuan Jiang", "Weihao Tan", "Heyuan Yao", "Shi Yan", "Xiangyang Li", "Yitao Liang", "Yujia Qin", "Guang Shi"], "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents", "comment": null, "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.", "AI": {"tldr": "Game-TARS\u662f\u4e00\u4e2a\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u7edf\u4e00\u53ef\u6269\u5c55\u7684\u952e\u76d8\u9f20\u6807\u52a8\u4f5c\u7a7a\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u5927\u89c4\u6a21\u8de8\u57df\u9884\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5728\u591a\u4e2a\u6e38\u620f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u548c\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u6e38\u620f\u667a\u80fd\u4f53\u4f9d\u8d56\u7279\u5b9aAPI\u6216GUI\u63a5\u53e3\u7684\u9650\u5236\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u8de8\u57df\u6301\u7eed\u9884\u8bad\u7ec3\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u5bf9\u9f50\u7684\u952e\u76d8\u9f20\u6807\u8f93\u5165\u7684\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4ee5\u652f\u6301\u5728\u64cd\u4f5c\u7cfb\u7edf\u3001\u7f51\u9875\u548c\u6a21\u62df\u6e38\u620f\u7b49\u5f02\u6784\u9886\u57df\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002", "method": "Game-TARS\u91c7\u7528\u7edf\u4e00\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u7a7a\u95f4\u951a\u5b9a\u4e8e\u4eba\u7c7b\u5bf9\u9f50\u7684\u952e\u76d8\u9f20\u6807\u8f93\u5165\uff0c\u901a\u8fc7\u8d85\u8fc7500B token\u7684\u591a\u6837\u5316\u8f68\u8ff9\u548c\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u5173\u952e\u6280\u672f\u5305\u62ec\u8870\u51cf\u6301\u7eed\u635f\u5931\u4ee5\u51cf\u5c11\u56e0\u679c\u6df7\u6dc6\uff0c\u4ee5\u53ca\u9ad8\u6548\u7684\u7a00\u758f\u601d\u7ef4\u7b56\u7565\u6765\u5e73\u8861\u63a8\u7406\u6df1\u5ea6\u548c\u63a8\u7406\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cGame-TARS\u5728\u5f00\u653e\u4e16\u754cMinecraft\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u7387\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u6a21\u578b\u63d0\u9ad8\u7ea62\u500d\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u7f51\u98753D\u6e38\u620f\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u65b0\u624b\u6c34\u5e73\uff0c\u5728FPS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86GPT-5\u3001Gemini-2.5-Pro\u548cClaude-4-Sonnet\u3002\u8bad\u7ec3\u65f6\u95f4\u548c\u6d4b\u8bd5\u65f6\u95f4\u7684\u6269\u5c55\u7ed3\u679c\u8bc1\u5b9e\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\u5728\u8de8\u6e38\u620f\u548c\u591a\u6a21\u6001\u6570\u636e\u6269\u5c55\u65f6\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7b80\u5355\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u8868\u793a\u4e0e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u76f8\u7ed3\u5408\uff0c\u4e3a\u5b9e\u73b0\u5177\u6709\u5e7f\u6cdb\u8ba1\u7b97\u673a\u4f7f\u7528\u80fd\u529b\u7684\u901a\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\u3002\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\u8303\u5f0f\u652f\u6301\u5728\u5f02\u6784\u9886\u57df\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u4e3a\u5f00\u53d1\u66f4\u901a\u7528\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23807", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23807", "abs": "https://arxiv.org/abs/2510.23807", "authors": ["Hamid R. Tizhoosh"], "title": "Why Foundation Models in Pathology Are Failing", "comment": null, "summary": "In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u57fa\u7840\u6a21\u578b\u5b58\u5728\u7684\u6839\u672c\u6027\u7f3a\u9677\uff0c\u6307\u51fa\u8fd9\u4e9b\u6a21\u578b\u4e0e\u7ec4\u7ec7\u5f62\u6001\u5b66\u672c\u8d28\u5b58\u5728\u6982\u5ff5\u6027\u4e0d\u5339\u914d\uff0c\u5e76\u8bc6\u522b\u4e86\u4e03\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u5931\u8d25\u539f\u56e0\uff0c\u547c\u5401\u5bf9\u8be5\u8303\u5f0f\u8fdb\u884c\u6839\u672c\u6027\u91cd\u65b0\u601d\u8003\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728\u975e\u533b\u5b66\u9886\u57df\u53d6\u5f97\u4e86\u9769\u547d\u6027\u7a81\u7834\uff0c\u4f46\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u5feb\u901f\u5e94\u7528\u5e76\u672a\u5b9e\u73b0\u9884\u671f\u7684\u764c\u75c7\u8bca\u65ad\u3001\u9884\u540e\u9884\u6d4b\u548c\u591a\u6a21\u6001\u68c0\u7d22\u7a81\u7834\uff0c\u53cd\u800c\u66b4\u9732\u51fa\u8bca\u65ad\u51c6\u786e\u6027\u4f4e\u3001\u9c81\u68d2\u6027\u5dee\u3001\u51e0\u4f55\u4e0d\u7a33\u5b9a\u6027\u3001\u8ba1\u7b97\u9700\u6c42\u5927\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u6839\u672c\u6027\u5f31\u70b9\uff0c\u9700\u8981\u6df1\u5165\u63a2\u7a76\u8fd9\u4e9b\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "\u672c\u6587\u91c7\u7528\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6279\u5224\u6027\u5206\u6790\u8bc6\u522b\u4e86\u57fa\u7840\u6a21\u578b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u5931\u8d25\u7684\u4e03\u4e2a\u6838\u5fc3\u539f\u56e0\uff1a\u751f\u7269\u590d\u6742\u6027\u3001\u65e0\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u8fc7\u5ea6\u6cdb\u5316\u3001\u8fc7\u5ea6\u590d\u6742\u7684\u67b6\u6784\u3001\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u521b\u65b0\u3001\u6570\u636e\u4e0d\u8db3\u4ee5\u53ca\u4e0e\u7ec4\u7ec7\u5757\u5927\u5c0f\u76f8\u5173\u7684\u57fa\u672c\u8bbe\u8ba1\u7f3a\u9677\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5b58\u5728\u8bca\u65ad\u51c6\u786e\u7387\u4f4e\u3001\u9c81\u68d2\u6027\u5dee\u3001\u51e0\u4f55\u4e0d\u7a33\u5b9a\u3001\u8ba1\u7b97\u9700\u6c42\u7e41\u91cd\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u8fd9\u4e9b\u7f3a\u9677\u6e90\u4e8e\u6a21\u578b\u5047\u8bbe\u4e0e\u4eba\u7c7b\u7ec4\u7ec7\u5185\u5728\u590d\u6742\u6027\u4e4b\u95f4\u7684\u6839\u672c\u6027\u4e0d\u5339\u914d\uff0c\u800c\u975e\u7b80\u5355\u7684\u6280\u672f\u4f18\u5316\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u8868\u660e\u5f53\u524d\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u6982\u5ff5\u5c42\u9762\u4e0e\u7ec4\u7ec7\u5f62\u6001\u5b66\u672c\u8d28\u5b58\u5728\u6839\u672c\u6027\u9519\u914d\uff0c\u9700\u8981\u5f7b\u5e95\u91cd\u65b0\u601d\u8003\u57fa\u7840\u6a21\u578b\u8303\u5f0f\u672c\u8eab\uff0c\u800c\u975e\u4ec5\u4ec5\u8fdb\u884c\u6e10\u8fdb\u5f0f\u6539\u8fdb\uff0c\u8fd9\u4e3a\u672a\u6765\u5f00\u53d1\u771f\u6b63\u9002\u7528\u4e8e\u8ba1\u7b97\u75c5\u7406\u5b66\u7684\u65b0\u578b\u5efa\u6a21\u65b9\u6cd5\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.23925", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23925", "abs": "https://arxiv.org/abs/2510.23925", "authors": ["Guohao Sun", "Hang Hua", "Jian Wang", "Jiebo Luo", "Sohail Dianat", "Majid Rabbani", "Raghuveer Rao", "Zhiqiang Tao"], "title": "Latent Chain-of-Thought for Visual Reasoning", "comment": "NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u644a\u9500\u53d8\u5206\u63a8\u7406\u7684\u53ef\u6269\u5c55\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5c06\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u540e\u9a8c\u63a8\u65ad\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6837\u6027\u5bfb\u6c42\u5f3a\u5316\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u63a8\u7406\u7f29\u653e\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u8bad\u7ec3\u7b97\u6cd5\u5982SFT\u3001PPO\u548cGRPO\u5728\u672a\u89c1\u63a8\u7406\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u4e25\u91cd\u4f9d\u8d56\u6709\u504f\u89c1\u7684\u5956\u52b1\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u644a\u9500\u53d8\u5206\u63a8\u7406\u6846\u67b6\u5c06\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u540e\u9a8c\u63a8\u65ad\u95ee\u9898\uff0c\u5f15\u5165\u57fa\u4e8e\u591a\u6837\u6027\u5bfb\u6c42\u5f3a\u5316\u5b66\u4e60\u7684\u7a00\u758f\u5956\u52b1\u51fd\u6570\uff0c\u5728token\u7ea7\u522b\u63d0\u4f9b\u5b66\u4e60\u4fe1\u53f7\u4ee5\u9f13\u52b1\u591a\u6837\u5316\u3001\u9ad8\u4f3c\u7136\u5ea6\u7684\u6f5c\u5728\u601d\u7ef4\u94fe\uff0c\u5e76\u4f7f\u7528\u8d1d\u53f6\u65af\u63a8\u7406\u7f29\u653e\u7b56\u7565\u901a\u8fc7\u8fb9\u9645\u4f3c\u7136\u66ff\u4ee3\u6602\u8d35\u7684Best-of-N\u548cBeam\u641c\u7d22\u6765\u9ad8\u6548\u6392\u5e8f\u6700\u4f18\u63a8\u7406\u8def\u5f84\u548c\u7b54\u6848\u3002", "result": "\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6709\u6548\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u63a8\u7406\u5efa\u6a21\u4e3a\u540e\u9a8c\u63a8\u65ad\u95ee\u9898\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u7684\u53d8\u5206\u63a8\u7406\u6846\u67b6\u548c\u591a\u6837\u6027\u5956\u52b1\u673a\u5236\u4e3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u540c\u65f6\u8d1d\u53f6\u65af\u7f29\u653e\u7b56\u7565\u4e3a\u9ad8\u6548\u63a8\u7406\u8def\u5f84\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u4ef7\u503c\u548c\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.24115", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24115", "abs": "https://arxiv.org/abs/2510.24115", "authors": ["Sandeep Vissapragada", "Vikrant Sahu", "Gagan Raj Gupta", "Vandita Singh"], "title": "HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology", "comment": null, "summary": "For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86HistoLens\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u9762\u5411\u75c5\u7406\u5b66\u5bb6\u7684\u900f\u660eAI\u52a9\u624b\uff0c\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u89c6\u89c9\u89e3\u91ca\u529f\u80fd\u589e\u5f3a\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u4efb\u4e0e\u5408\u4f5c\u3002\u7cfb\u7edf\u901a\u8fc7\u667a\u80fd\u7ffb\u8bd1\u7528\u6237\u67e5\u8be2\u3001\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\u5e76\u63d0\u4f9b\u70ed\u529b\u56fe\u53ef\u89c6\u5316\uff0c\u4f7fAI\u63a8\u7406\u8fc7\u7a0b\u5b8c\u5168\u900f\u660e\u3002", "motivation": "\u5f53\u524d\u533b\u7597AI\u7cfb\u7edf\u666e\u904d\u5b58\u5728\u9ed1\u7bb1\u95ee\u9898\uff0c\u533b\u751f\u96be\u4ee5\u7406\u89e3AI\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u53ef\u4fe1\u5ea6\u548c\u5e94\u7528\u4ef7\u503c\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u900f\u660e\u7684AI\u52a9\u624b\uff0c\u4f7f\u75c5\u7406\u5b66\u5bb6\u80fd\u591f\u50cf\u54a8\u8be2\u540c\u4e8b\u4e00\u6837\u4e0eAI\u7cfb\u7edf\u8fdb\u884c\u81ea\u7136\u4ea4\u4e92\uff0c\u4ece\u800c\u5efa\u7acb\u771f\u6b63\u7684\u4fe1\u4efb\u5173\u7cfb\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5c06\u75c5\u7406\u5b66\u5bb6\u7684\u82f1\u6587\u95ee\u9898\u8f6c\u5316\u4e3a\u7cbe\u786e\u7684AI\u67e5\u8be2\uff0c\u751f\u6210\u7ed3\u6784\u5316\u8bca\u65ad\u62a5\u544a\u3002\u5173\u952e\u521b\u65b0\u5728\u4e8e\u63d0\u4f9b\u5373\u65f6\u89c6\u89c9\u89e3\u91ca\u529f\u80fd\uff0c\u901a\u8fc7\u70ed\u529b\u56fe\u7cbe\u786e\u5b9a\u4f4d\u5206\u6790\u6240\u4f9d\u636e\u7684\u7ec6\u80de\u548c\u7ec4\u7ec7\u533a\u57df\u3002\u540c\u65f6\u8bad\u7ec3AI\u6a21\u578b\u4e13\u6ce8\u4e8e\u60a3\u8005\u7ec4\u7ec7\u7279\u5f81\uff0c\u81ea\u52a8\u5ffd\u7565\u80cc\u666f\u566a\u58f0\u5e72\u6270\u3002", "result": "HistoLens\u5b9e\u73b0\u4e86\u75c5\u7406\u5b66\u5bb6\u4e0eAI\u7cfb\u7edf\u4e4b\u95f4\u7684\u900f\u660e\u534f\u4f5c\u5de5\u4f5c\u6d41\u7a0b\uff0c\u533b\u751f\u80fd\u591f\u4fdd\u6301\u4e13\u5bb6\u4e3b\u5bfc\u5730\u4f4d\uff0c\u540c\u65f6\u5229\u7528\u53ef\u4fe1\u7684AI\u52a9\u624b\u9a8c\u8bc1\u8bca\u65ad\u89c1\u89e3\u3002\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6548\u7387\u548c\u4fe1\u5fc3\uff0c\u4f7f\u533b\u751f\u80fd\u591f\u505a\u51fa\u66f4\u5feb\u3001\u66f4\u53ef\u9760\u7684\u8bca\u65ad\u51b3\u7b56\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u900f\u660eAI\u7cfb\u7edf\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u5173\u952e\u4ef7\u503c\uff0c\u901a\u8fc7\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u548c\u89c6\u89c9\u8bc1\u636e\uff0c\u6210\u529f\u5efa\u7acb\u4e86\u533b\u751f\u5bf9AI\u7684\u4fe1\u4efb\u3002\u8fd9\u79cd\u534f\u4f5c\u6a21\u5f0f\u4e3a\u533b\u7597AI\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u5f3a\u8c03\u4e86\u4fdd\u6301\u4eba\u7c7b\u4e13\u5bb6\u4e3b\u5bfc\u5730\u4f4d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.24023", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24023", "abs": "https://arxiv.org/abs/2510.24023", "authors": ["Saujas Vaduguru", "Yilun Hua", "Yoav Artzi", "Daniel Fried"], "title": "Success and Cost Elicit Convention Formation for Efficient Communication", "comment": null, "summary": "Humans leverage shared conversational context to become increasingly\nsuccessful and efficient at communicating over time. One manifestation of this\nis the formation of ad hoc linguistic conventions, which allow people to\ncoordinate on short, less costly utterances that are understood using shared\nconversational context. We present a method to train large multimodal models to\nform conventions, enabling efficient communication. Our approach uses simulated\nreference games between models, and requires no additional human-produced data.\nIn repeated reference games involving photographs and tangram images, our\nmethod enables models to communicate efficiently with people: reducing the\nmessage length by up to 41% while increasing success by 15% over the course of\nthe interaction. Human listeners respond faster when interacting with our model\nthat forms conventions. We also show that training based on success or cost\nalone is insufficient - both are necessary to elicit convention formation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5f62\u6210\u8bed\u8a00\u7ea6\u5b9a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u53c2\u8003\u6e38\u620f\u4f7f\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u9ad8\u6548\u901a\u4fe1\u3002\u8be5\u65b9\u6cd5\u5728\u6d89\u53ca\u7167\u7247\u548c\u4e03\u5de7\u677f\u56fe\u50cf\u7684\u91cd\u590d\u53c2\u8003\u6e38\u620f\u4e2d\uff0c\u5c06\u6d88\u606f\u957f\u5ea6\u51cf\u5c11\u8fbe41%\u540c\u65f6\u63d0\u9ad8\u6210\u529f\u738715%\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u5171\u4eab\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u9010\u6e10\u5f62\u6210\u9ad8\u6548\u901a\u4fe1\u80fd\u529b\uff0c\u7279\u522b\u662f\u5f62\u6210\u4e34\u65f6\u8bed\u8a00\u7ea6\u5b9a\u6765\u534f\u8c03\u7b80\u77ed\u3001\u4f4e\u6210\u672c\u7684\u8868\u8fbe\u3002\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7f3a\u4e4f\u8fd9\u79cd\u901a\u8fc7\u4e0a\u4e0b\u6587\u5f62\u6210\u7ea6\u5b9a\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u901a\u4fe1\u6548\u7387\u3002", "method": "\u91c7\u7528\u6a21\u62df\u53c2\u8003\u6e38\u620f\u7684\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6a21\u578b\u4e4b\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u989d\u5916\u4eba\u5de5\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u91cd\u590d\u4ea4\u4e92\u4e2d\u7684\u6210\u529f\u7387\u548c\u901a\u4fe1\u6210\u672c\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff0c\u4fc3\u4f7f\u6a21\u578b\u81ea\u53d1\u5f62\u6210\u9ad8\u6548\u901a\u4fe1\u7ea6\u5b9a\u3002", "result": "\u5728\u7167\u7247\u548c\u4e03\u5de7\u677f\u56fe\u50cf\u7684\u53c2\u8003\u6e38\u620f\u4e2d\uff0c\u6a21\u578b\u901a\u4fe1\u6d88\u606f\u957f\u5ea6\u51cf\u5c11\u9ad8\u8fbe41%\uff0c\u540c\u65f6\u4ea4\u4e92\u6210\u529f\u7387\u63d0\u9ad815%\u3002\u4eba\u7c7b\u542c\u4f17\u4e0e\u5f62\u6210\u7ea6\u5b9a\u7684\u6a21\u578b\u4ea4\u4e92\u65f6\u54cd\u5e94\u901f\u5ea6\u66f4\u5feb\uff0c\u8bc1\u660e\u901a\u4fe1\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u4ec5\u57fa\u4e8e\u6210\u529f\u7387\u6216\u901a\u4fe1\u6210\u672c\u7684\u5355\u4e00\u8bad\u7ec3\u76ee\u6807\u4e0d\u8db3\u4ee5\u6fc0\u53d1\u7ea6\u5b9a\u5f62\u6210\uff0c\u5fc5\u987b\u540c\u65f6\u4f18\u5316\u4e24\u8005\u624d\u80fd\u5b9e\u73b0\u9ad8\u6548\u901a\u4fe1\u3002\u8be5\u65b9\u6cd5\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u9ad8\u6548\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u6a21\u62df\u4ea4\u4e92\u5b66\u4e60\u901a\u4fe1\u7ea6\u5b9a\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.23775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23775", "abs": "https://arxiv.org/abs/2510.23775", "authors": ["Aryan Mathur", "Asaduddin Ahmed", "Pushti Amit Vasoya", "Simeon Kandan Sonar", "Yasir Z", "Madesh Kuppusamy"], "title": "Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices", "comment": null, "summary": "The increasing realism of AI-generated imagery poses challenges for verifying\nvisual authenticity. We present an explainable image authenticity detection\nsystem that combines a lightweight convolutional classifier\n(\"Faster-Than-Lies\") with a Vision-Language Model (Qwen2-VL-7B) to classify,\nlocalize, and explain artifacts in 32x32 images. Our model achieves 96.5%\naccuracy on the extended CiFAKE dataset augmented with adversarial\nperturbations and maintains an inference time of 175ms on 8-core CPUs, enabling\ndeployment on local or edge devices. Using autoencoder-based reconstruction\nerror maps, we generate artifact localization heatmaps, which enhance\ninterpretability for both humans and the VLM. We further categorize 70 visual\nartifact types into eight semantic groups and demonstrate explainable text\ngeneration for each detected anomaly. This work highlights the feasibility of\ncombining visual and linguistic reasoning for interpretable authenticity\ndetection in low-resolution imagery and outlines potential cross-domain\napplications in forensics, industrial inspection, and social media moderation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u771f\u5b9e\u6027\u68c0\u6d4b\u7cfb\u7edf\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5377\u79ef\u5206\u7c7b\u5668\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u572832\u00d732\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u5b9e\u73b096.5%\u7684\u51c6\u786e\u7387\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\uff0c\u540c\u65f6\u63d0\u4f9b\u4f2a\u5f71\u5b9a\u4f4d\u548c\u6587\u672c\u89e3\u91ca\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u56fe\u50cf\u7684\u771f\u5b9e\u611f\u4e0d\u65ad\u63d0\u5347\uff0c\u9a8c\u8bc1\u89c6\u89c9\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u4f4e\u5206\u8fa8\u7387\u6761\u4ef6\u4e0b\u51c6\u786e\u68c0\u6d4b\u5e76\u89e3\u91ca\u56fe\u50cf\u4f2a\u9020\u75d5\u8ff9\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u5377\u79ef\u5206\u7c7b\u5668Faster-Than-Lies\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578bQwen2-VL-7B\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7f16\u7801\u5668\u91cd\u6784\u8bef\u5dee\u56fe\u751f\u6210\u4f2a\u5f71\u5b9a\u4f4d\u70ed\u529b\u56fe\uff0c\u5e76\u5c0670\u79cd\u89c6\u89c9\u4f2a\u5f71\u7c7b\u578b\u5f52\u7c7b\u4e3a\u516b\u4e2a\u8bed\u4e49\u7ec4\u4ee5\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5728\u5305\u542b\u5bf9\u6297\u6027\u6270\u52a8\u7684\u6269\u5c55CiFAKE\u6570\u636e\u96c6\u4e0a\u8fbe\u523096.5%\u7684\u51c6\u786e\u7387\uff0c\u57288\u6838CPU\u4e0a\u63a8\u7406\u65f6\u95f4\u4e3a175\u6beb\u79d2\uff0c\u80fd\u591f\u90e8\u7f72\u5728\u672c\u5730\u6216\u8fb9\u7f18\u8bbe\u5907\u4e0a\uff0c\u540c\u65f6\u751f\u6210\u4f2a\u5f71\u5b9a\u4f4d\u70ed\u529b\u56fe\u548c\u6587\u672c\u89e3\u91ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u89c6\u89c9\u4e0e\u8bed\u8a00\u63a8\u7406\u76f8\u7ed3\u5408\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u53ef\u89e3\u91ca\u771f\u5b9e\u6027\u68c0\u6d4b\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u53d6\u8bc1\u3001\u5de5\u4e1a\u68c0\u6d4b\u548c\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5ba1\u6838\u7b49\u8de8\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24161", "categories": ["cs.AI", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24161", "abs": "https://arxiv.org/abs/2510.24161", "authors": ["Wentao Tan", "Bowen Wang", "Heng Zhi", "Chenyu Liu", "Zhe Li", "Jian Liu", "Zengrong Lin", "Yukun Dai", "Yipeng Chen", "Wenjie Yang", "Enci Xie", "Hao Xue", "Baixu Ji", "Chen Xu", "Zhibin Wang", "Tianshi Wang", "Lei Zhu", "Heng Tao Shen"], "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8fb9\u754c\u5927\u6a21\u578bBLM\u2081\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u5b9e\u73b0\u4e86\u8de8\u7a7a\u95f4\u8fc1\u79fb\u3001\u8de8\u4efb\u52a1\u5b66\u4e60\u548c\u8de8\u5177\u8eab\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u6570\u5b57\u548c\u7269\u7406\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u5bb6\u65cf\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u548c\u4e0d\u540c\u5177\u8eab\u7cfb\u7edf\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\uff0c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4ec5\u80fd\u4ea7\u751f\u4f4e\u7ea7\u52a8\u4f5c\u800c\u7f3a\u4e4f\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\uff0c\u5927\u591a\u6570\u5177\u8eab\u5927\u8bed\u8a00\u6a21\u578b\u5c40\u9650\u4e8e\u6570\u5b57\u7a7a\u95f4\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u7269\u7406\u4e16\u754c\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u6570\u5b57\u548c\u7269\u7406\u7a7a\u95f4\u65e0\u7f1d\u64cd\u4f5c\u5e76\u8de8\u5177\u8eab\u7cfb\u7edf\u548c\u4efb\u52a1\u6cdb\u5316\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "BLM\u2081\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u7cbe\u9009\u6570\u5b57\u8bed\u6599\u5e93\u5411\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6ce8\u5165\u5177\u8eab\u77e5\u8bc6\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u610f\u56fe\u6865\u63a5\u63a5\u53e3\u8bad\u7ec3\u7b56\u7565\u6a21\u5757\uff0c\u4ece\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u9ad8\u7ea7\u8bed\u4e49\u6765\u6307\u5bfc\u63a7\u5236\u800c\u4e0d\u5fae\u8c03\u4e3b\u5e72\u7f51\u7edc\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u81ea\u6536\u96c6\u7684\u8de8\u5177\u8eab\u6f14\u793a\u5957\u4ef6\uff0c\u6db5\u76d6\u56db\u79cd\u673a\u5668\u4eba\u5177\u8eab\u548c\u516d\u4e2a\u6e10\u8fdb\u6311\u6218\u6027\u4efb\u52a1\u3002", "result": "\u5728\u6570\u5b57\u548c\u7269\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5355\u4e2aBLM\u2081\u5b9e\u4f8b\u5728\u6570\u5b57\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7ea66%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u7269\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7ea63%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u5177\u8eab\u5927\u8bed\u8a00\u6a21\u578b\u3001\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u548c\u901a\u7528\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u56db\u4e2a\u6a21\u578b\u5bb6\u65cf\u3002", "conclusion": "BLM\u2081\u8bc1\u660e\u4e86\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u53ef\u4ee5\u5b9e\u73b0\u8de8\u7a7a\u95f4\u3001\u8de8\u4efb\u52a1\u548c\u8de8\u5177\u8eab\u7684\u7edf\u4e00\u5efa\u6a21\uff0c\u4e3a\u6784\u5efa\u5728\u6570\u5b57\u548c\u7269\u7406\u4e16\u754c\u95f4\u65e0\u7f1d\u64cd\u4f5c\u7684\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6307\u4ee4\u8ddf\u968f\u548c\u63a8\u7406\u80fd\u529b\uff0c\u652f\u6301\u9c81\u68d2\u7684\u8de8\u5177\u8eab\u63a7\u5236\u3002"}}
{"id": "2510.24178", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24178", "abs": "https://arxiv.org/abs/2510.24178", "authors": ["Aaron Scott", "Maike Z\u00fcfle", "Jan Niehues"], "title": "MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations", "comment": null, "summary": "Sarcasm is a complex form of figurative language in which the intended\nmeaning contradicts the literal one. Its prevalence in social media and popular\nculture poses persistent challenges for natural language understanding,\nsentiment analysis, and content moderation. With the emergence of multimodal\nlarge language models, sarcasm detection extends beyond text and requires\nintegrating cues from audio and vision. We present MuSaG, the first German\nmultimodal sarcasm detection dataset, consisting of 33 minutes of manually\nselected and human-annotated statements from German television shows. Each\ninstance provides aligned text, audio, and video modalities, annotated\nseparately by humans, enabling evaluation in unimodal and multimodal settings.\nWe benchmark nine open-source and commercial models, spanning text, audio,\nvision, and multimodal architectures, and compare their performance to human\nannotations. Our results show that while humans rely heavily on audio in\nconversational settings, models perform best on text. This highlights a gap in\ncurrent multimodal models and motivates the use of MuSaG for developing models\nbetter suited to realistic scenarios. We release MuSaG publicly to support\nfuture research on multimodal sarcasm detection and human-model alignment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MuSaG\uff0c\u9996\u4e2a\u5fb7\u8bed\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea\u5fb7\u56fd\u7535\u89c6\u8282\u76ee\u768433\u5206\u949f\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\u5728\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u89c9\u548c\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u8bbd\u523a\u4f5c\u4e3a\u4e00\u79cd\u590d\u6742\u7684\u6bd4\u55bb\u8bed\u8a00\u5f62\u5f0f\uff0c\u5728\u793e\u4ea4\u5a92\u4f53\u548c\u6d41\u884c\u6587\u5316\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5bf9\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u60c5\u611f\u5206\u6790\u548c\u5185\u5bb9\u5ba1\u6838\u6784\u6210\u4e86\u6301\u7eed\u6311\u6218\u3002\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u8bbd\u523a\u68c0\u6d4b\u9700\u8981\u8d85\u8d8a\u6587\u672c\u8303\u56f4\uff0c\u6574\u5408\u6765\u81ea\u97f3\u9891\u548c\u89c6\u89c9\u7684\u7ebf\u7d22\uff0c\u800c\u5fb7\u8bed\u9886\u57df\u7f3a\u4e4f\u76f8\u5e94\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86MuSaG\u6570\u636e\u96c6\uff0c\u5305\u542b33\u5206\u949f\u6765\u81ea\u5fb7\u56fd\u7535\u89c6\u8282\u76ee\u7684\u4eba\u5de5\u7b5b\u9009\u548c\u6807\u6ce8\u8bed\u53e5\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u63d0\u4f9b\u5bf9\u9f50\u7684\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u6a21\u6001\uff0c\u5206\u522b\u7531\u4eba\u5de5\u6807\u6ce8\u3002\u7814\u7a76\u5bf9\u4e5d\u79cd\u5f00\u6e90\u548c\u5546\u4e1a\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u89c9\u548c\u591a\u6a21\u6001\u67b6\u6784\uff0c\u5e76\u5c06\u5b83\u4eec\u7684\u6027\u80fd\u4e0e\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4eba\u7c7b\u5728\u5bf9\u8bdd\u73af\u5883\u4e2d\u4e3b\u8981\u4f9d\u8d56\u97f3\u9891\u7ebf\u7d22\u8fdb\u884c\u8bbd\u523a\u68c0\u6d4b\uff0c\u800c\u6a21\u578b\u5728\u6587\u672c\u6a21\u6001\u4e0a\u8868\u73b0\u6700\u4f73\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u5229\u7528\u97f3\u9891\u7ebf\u7d22\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9002\u5408\u73b0\u5b9e\u573a\u666f\u7684\u591a\u6a21\u6001\u6a21\u578b\u7684\u5fc5\u8981\u6027\uff0cMuSaG\u6570\u636e\u96c6\u7684\u53d1\u5e03\u5c06\u652f\u6301\u672a\u6765\u5728\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u548c\u4eba\u673a\u5bf9\u9f50\u65b9\u9762\u7684\u7814\u7a76\uff0c\u4e3a\u6539\u8fdb\u591a\u6a21\u6001\u7406\u89e3\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u548c\u8d44\u6e90\u3002"}}
{"id": "2510.23785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23785", "abs": "https://arxiv.org/abs/2510.23785", "authors": ["Md Tanvir Hossain", "Akif Islam", "Mohd Ruhul Ameen"], "title": "CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting", "comment": "6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International\n  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE\n  2025)", "summary": "Humans can effortlessly count diverse objects by perceiving visual repetition\nand structural relationships rather than relying on class identity. However,\nmost existing counting models fail to replicate this ability; they often\nmiscount when objects exhibit complex shapes, internal symmetry, or overlapping\ncomponents. In this work, we introduce CountFormer, a transformer-based\nframework that learns to recognize repetition and structural coherence for\nclass-agnostic object counting. Built upon the CounTR architecture, our model\nreplaces its visual encoder with the self-supervised foundation model DINOv2,\nwhich produces richer and spatially consistent feature representations. We\nfurther incorporate positional embedding fusion to preserve geometric\nrelationships before decoding these features into density maps through a\nlightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model\nachieves performance comparable to current state-of-the-art methods while\ndemonstrating superior accuracy on structurally intricate or densely packed\nscenes. Our findings indicate that integrating foundation models such as DINOv2\nenables counting systems to approach human-like structural perception,\nadvancing toward a truly general and exemplar-free counting paradigm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCountFormer\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7c7b\u65e0\u5173\u7269\u4f53\u8ba1\u6570\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578bDINOv2\u6765\u8bc6\u522b\u89c6\u89c9\u91cd\u590d\u548c\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5728FSC-147\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e0e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u5728\u7ed3\u6784\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u8ba1\u6570\u6a21\u578b\u96be\u4ee5\u590d\u5236\u4eba\u7c7b\u901a\u8fc7\u611f\u77e5\u89c6\u89c9\u91cd\u590d\u548c\u7ed3\u6784\u5173\u7cfb\u800c\u975e\u7c7b\u522b\u8eab\u4efd\u6765\u8ba1\u6570\u7684\u80fd\u529b\uff0c\u5728\u7269\u4f53\u5177\u6709\u590d\u6742\u5f62\u72b6\u3001\u5185\u90e8\u5bf9\u79f0\u6027\u6216\u91cd\u53e0\u7ec4\u4ef6\u65f6\u7ecf\u5e38\u8ba1\u6570\u9519\u8bef\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8bc6\u522b\u91cd\u590d\u6027\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u7c7b\u65e0\u5173\u8ba1\u6570\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eCounTR\u67b6\u6784\u6784\u5efaCountFormer\uff0c\u4f7f\u7528\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578bDINOv2\u66ff\u6362\u89c6\u89c9\u7f16\u7801\u5668\u4ee5\u751f\u6210\u66f4\u4e30\u5bcc\u4e14\u7a7a\u95f4\u4e00\u81f4\u7684\u7279\u5f81\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4f4d\u7f6e\u5d4c\u5165\u878d\u5408\u6765\u4fdd\u6301\u51e0\u4f55\u5173\u7cfb\uff0c\u6700\u540e\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5377\u79ef\u89e3\u7801\u5668\u5c06\u7279\u5f81\u89e3\u7801\u4e3a\u5bc6\u5ea6\u56fe\u3002", "result": "\u5728FSC-147\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6a21\u578b\u6027\u80fd\u4e0e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u5728\u7ed3\u6784\u590d\u6742\u6216\u5bc6\u96c6\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u590d\u6742\u7ed3\u6784\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6574\u5408DINOv2\u7b49\u57fa\u7840\u6a21\u578b\u80fd\u4f7f\u8ba1\u6570\u7cfb\u7edf\u63a5\u8fd1\u4eba\u7c7b\u7684\u7ed3\u6784\u611f\u77e5\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u771f\u6b63\u901a\u7528\u4e14\u65e0\u9700\u793a\u4f8b\u7684\u8ba1\u6570\u8303\u5f0f\u7684\u53d1\u5c55\uff0c\u4e3a\u7c7b\u65e0\u5173\u7269\u4f53\u8ba1\u6570\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.24168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24168", "abs": "https://arxiv.org/abs/2510.24168", "authors": ["Weihua Cheng", "Ersheng Ni", "Wenlong Wang", "Yifei Sun", "Junming Liu", "Wangyu Shen", "Yirong Chen", "Botian Shi", "Ding Wang"], "title": "MGA: Memory-Driven GUI Agent for Observation-Centric Interaction", "comment": "Submitted to WWW2025", "summary": "The rapid progress of Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) has enabled agentic systems capable of perceiving and acting\nacross diverse environments. A challenging yet impactful frontier is the\ndevelopment of GUI agents, which must navigate complex desktop and web\ninterfaces while maintaining robustness and generalization. Existing paradigms\ntypically model tasks as long-chain executions, concatenating historical\ntrajectories into the context. While approaches such as Mirage and GTA1 refine\nplanning or introduce multi-branch action selection, they remain constrained by\ntwo persistent issues: Dependence on historical trajectories, which amplifies\nerror propagation. And Local exploration bias, where \"decision-first,\nobservation-later\" mechanisms overlook critical interface cues. We introduce\nthe Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the\nprinciple of observe first, then decide. MGA models each step as an\nindependent, context-rich environment state represented by a triad: current\nscreenshot, task-agnostic spatial information, and a dynamically updated\nstructured memory. Experiments on OSworld benchmarks, real desktop applications\n(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves\nsubstantial gains in robustness, generalization, and efficiency compared to\nstate-of-the-art baselines. The code is publicly available at:\n{https://anonymous.4open.science/r/MGA-3571}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8bb0\u5fc6\u9a71\u52a8\u7684GUI\u667a\u80fd\u4f53\uff08MGA\uff09\uff0c\u901a\u8fc7'\u5148\u89c2\u5bdf\u540e\u51b3\u7b56'\u7684\u8303\u5f0f\u89e3\u51b3\u4e86\u73b0\u6709GUI\u667a\u80fd\u4f53\u5bf9\u5386\u53f2\u8f68\u8ff9\u7684\u4f9d\u8d56\u548c\u5c40\u90e8\u63a2\u7d22\u504f\u5dee\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709GUI\u667a\u80fd\u4f53\u901a\u5e38\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u957f\u94fe\u6267\u884c\uff0c\u5c06\u5386\u53f2\u8f68\u8ff9\u4e32\u8054\u5230\u4e0a\u4e0b\u6587\u4e2d\uff0c\u5b58\u5728\u4e24\u4e2a\u6301\u7eed\u6027\u95ee\u9898\uff1a\u5bf9\u5386\u53f2\u8f68\u8ff9\u7684\u4f9d\u8d56\u4f1a\u653e\u5927\u9519\u8bef\u4f20\u64ad\uff0c\u4ee5\u53ca'\u51b3\u7b56\u4f18\u5148\u3001\u89c2\u5bdf\u6ede\u540e'\u673a\u5236\u5bfc\u81f4\u7684\u5c40\u90e8\u63a2\u7d22\u504f\u5dee\u4f1a\u5ffd\u7565\u5173\u952e\u754c\u9762\u7ebf\u7d22\u3002", "method": "MGA\u5c06GUI\u4ea4\u4e92\u91cd\u65b0\u6784\u5efa\u4e3a'\u5148\u89c2\u5bdf\u540e\u51b3\u7b56'\u7684\u539f\u5219\uff0c\u5c06\u6bcf\u4e2a\u6b65\u9aa4\u5efa\u6a21\u4e3a\u72ec\u7acb\u7684\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u73af\u5883\u72b6\u6001\uff0c\u7531\u4e09\u5143\u7ec4\u8868\u793a\uff1a\u5f53\u524d\u5c4f\u5e55\u622a\u56fe\u3001\u4efb\u52a1\u65e0\u5173\u7684\u7a7a\u95f4\u4fe1\u606f\u4ee5\u53ca\u52a8\u6001\u66f4\u65b0\u7684\u7ed3\u6784\u5316\u8bb0\u5fc6\u3002", "result": "\u5728OSworld\u57fa\u51c6\u6d4b\u8bd5\u3001\u771f\u5b9e\u684c\u9762\u5e94\u7528\u7a0b\u5e8f\uff08Chrome\u3001VSCode\u3001VLC\uff09\u548c\u8de8\u4efb\u52a1\u8fc1\u79fb\u5b9e\u9a8c\u4e2d\u7684\u7ed3\u679c\u8868\u660e\uff0cMGA\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MGA\u901a\u8fc7\u91cd\u6784GUI\u4ea4\u4e92\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86'\u5148\u89c2\u5bdf\u540e\u51b3\u7b56'\u65b9\u6cd5\u5728\u89e3\u51b3\u73b0\u6709GUI\u667a\u80fd\u4f53\u6838\u5fc3\u9650\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u7a33\u5065\u548c\u901a\u7528\u7684\u754c\u9762\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.24247", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24247", "abs": "https://arxiv.org/abs/2510.24247", "authors": ["Ahmad Ghannam", "Naif Alharthi", "Faris Alasmary", "Kholood Al Tabash", "Shouq Sadah", "Lahouari Ghouti"], "title": "Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using Text and Speech Representations", "comment": null, "summary": "In this work, we tackle the Diacritic Restoration (DR) task for Arabic\ndialectal sentences using a multimodal approach that combines both textual and\nspeech information. We propose a model that represents the text modality using\nan encoder extracted from our own pre-trained model named CATT. The speech\ncomponent is handled by the encoder module of the OpenAI Whisper base model.\nOur solution is designed following two integration strategies. The former\nconsists of fusing the speech tokens with the input at an early stage, where\nthe 1500 frames of the audio segment are averaged over 10 consecutive frames,\nresulting in 150 speech tokens. To ensure embedding compatibility, these\naveraged tokens are processed through a linear projection layer prior to\nmerging them with the text tokens. Contextual encoding is guaranteed by the\nCATT encoder module. The latter strategy relies on cross-attention, where text\nand speech embeddings are fused. The cross-attention output is then fed to the\nCATT classification head for token-level diacritic prediction. To further\nimprove model robustness, we randomly deactivate the speech input during\ntraining, allowing the model to perform well with or without speech. Our\nexperiments show that the proposed approach achieves a word error rate (WER) of\n0.25 and a character error rate (CER) of 0.9 on the development set. On the\ntest set, our model achieved WER and CER scores of 0.55 and 0.13, respectively.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u6587\u672c\u548c\u8bed\u97f3\u4fe1\u606f\u7684\u8de8\u6a21\u6001\u65b9\u6cd5\u6765\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u7684\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u4efb\u52a1\uff0c\u901a\u8fc7\u4e24\u79cd\u96c6\u6210\u7b56\u7565\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u5728\u5f00\u53d1\u96c6\u4e0a\u8fbe\u5230\u4e860.25\u7684\u8bcd\u9519\u8bef\u7387\u548c0.9\u7684\u5b57\u7b26\u9519\u8bef\u7387\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u8fbe\u52300.55\u548c0.13\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u53e5\u5b50\u4e2d\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u4efb\u52a1\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\uff0c\u800c\u5ffd\u7565\u4e86\u8bed\u97f3\u6a21\u6001\u63d0\u4f9b\u7684\u4e30\u5bcc\u97f5\u5f8b\u548c\u53d1\u97f3\u7279\u5f81\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u65b9\u8a00\u73af\u5883\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u7684\u6a21\u578b\u91c7\u7528\u8de8\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u6587\u672c\u6a21\u6001\u4f7f\u7528\u81ea\u7814\u9884\u8bad\u7ec3\u6a21\u578bCATT\u7684\u7f16\u7801\u5668\uff0c\u8bed\u97f3\u6a21\u6001\u4f7f\u7528OpenAI Whisper\u57fa\u7840\u6a21\u578b\u7684\u7f16\u7801\u5668\u6a21\u5757\u3002\u8bbe\u8ba1\u4e86\u4e24\u79cd\u96c6\u6210\u7b56\u7565\uff1a\u65e9\u671f\u878d\u5408\u7b56\u7565\u5c061500\u5e27\u97f3\u9891\u6bb5\u5e73\u5747\u4e3a150\u4e2a\u8bed\u97f3\u6807\u8bb0\uff0c\u901a\u8fc7\u7ebf\u6027\u6295\u5f71\u5c42\u5904\u7406\u540e\u4e0e\u6587\u672c\u6807\u8bb0\u5408\u5e76\uff1b\u4ea4\u53c9\u6ce8\u610f\u529b\u7b56\u7565\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u6587\u672c\u548c\u8bed\u97f3\u5d4c\u5165\uff0c\u8f93\u51fa\u9001\u5165CATT\u5206\u7c7b\u5934\u8fdb\u884c\u6807\u8bb0\u7ea7\u9884\u6d4b\u3002\u8bad\u7ec3\u65f6\u968f\u673a\u7981\u7528\u8bed\u97f3\u8f93\u5165\u4ee5\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5f00\u53d1\u96c6\u4e0a\u53d6\u5f97\u4e860.25\u7684\u8bcd\u9519\u8bef\u7387\u548c0.9\u7684\u5b57\u7b26\u9519\u8bef\u7387\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bcd\u9519\u8bef\u7387\u548c\u5b57\u7b26\u9519\u8bef\u7387\u5206\u522b\u8fbe\u52300.55\u548c0.13\uff0c\u8bc1\u660e\u4e86\u8de8\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u878d\u5408\u6587\u672c\u548c\u8bed\u97f3\u4fe1\u606f\u80fd\u591f\u663e\u8457\u63d0\u5347\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u7684\u6027\u80fd\uff0c\u8de8\u6a21\u6001\u65b9\u6cd5\u4e3a\u65b9\u8a00\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002\u968f\u673a\u7981\u7528\u8bed\u97f3\u8f93\u5165\u7684\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u4ec5\u6709\u6587\u672c\u8f93\u5165\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23894", "abs": "https://arxiv.org/abs/2510.23894", "authors": ["Jinxin Zhou", "Jiachen Jiang", "Zhihui Zhu"], "title": "Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation", "comment": "23 pages, 10 figures, 14 tables", "summary": "Extending CLIP models to semantic segmentation remains challenging due to the\nmisalignment between their image-level pre-training objectives and the\npixel-level visual understanding required for dense prediction. While prior\nefforts have achieved encouraging results by reorganizing the final layer and\nfeatures, they often inherit the global alignment bias of preceding layers,\nleading to suboptimal segmentation performance. In this work, we propose\nLHT-CLIP, a novel training-free framework that systematically exploits the\nvisual discriminability of CLIP across layer, head, and token levels. Through\ncomprehensive analysis, we reveal three key insights: (i) the final layers\nprimarily strengthen image-text alignment with sacrifice of visual\ndiscriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),\npartly due to the emergence of anomalous tokens; (ii) a subset of attention\nheads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual\ndiscriminability across datasets; (iii) abnormal tokens display sparse and\nconsistent activation pattern compared to normal tokens. Based on these\nfindings, we propose three complementary techniques: semantic-spatial\nreweighting, selective head enhancement, and abnormal token replacement to\neffectively restore visual discriminability and improve segmentation\nperformance without any additional training, auxiliary pre-trained networks, or\nextensive hyperparameter tuning. Extensive experiments on 8 common semantic\nsegmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art\nperformance across diverse scenarios, highlighting its effectiveness and\npracticality for real-world deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LHT-CLIP\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5229\u7528CLIP\u5728\u5c42\u3001\u5934\u548c\u4ee4\u724c\u7ea7\u522b\u7684\u89c6\u89c9\u533a\u5206\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86CLIP\u6a21\u578b\u6269\u5c55\u5230\u8bed\u4e49\u5206\u5272\u65f6\u7684\u5bf9\u9f50\u504f\u5dee\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5c06CLIP\u6a21\u578b\u6269\u5c55\u5230\u8bed\u4e49\u5206\u5272\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5176\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u76ee\u6807\u4e0e\u5bc6\u96c6\u9884\u6d4b\u6240\u9700\u7684\u50cf\u7d20\u7ea7\u89c6\u89c9\u7406\u89e3\u4e4b\u95f4\u5b58\u5728\u4e0d\u5bf9\u9f50\u3002\u5148\u524d\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u91cd\u7ec4\u6700\u7ec8\u5c42\u548c\u7279\u5f81\u53d6\u5f97\u4e86\u9f13\u821e\u4eba\u5fc3\u7684\u7ed3\u679c\uff0c\u4f46\u5f80\u5f80\u7ee7\u627f\u4e86\u524d\u5c42\u7684\u5168\u5c40\u5bf9\u9f50\u504f\u5dee\uff0c\u5bfc\u81f4\u5206\u5272\u6027\u80fd\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u4e92\u8865\u6280\u672f\uff1a\u8bed\u4e49\u7a7a\u95f4\u91cd\u52a0\u6743\u3001\u9009\u62e9\u6027\u5934\u589e\u5f3a\u548c\u5f02\u5e38\u4ee4\u724c\u66ff\u6362\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u57fa\u4e8e\u5bf9CLIP\u89c6\u89c9\u533a\u5206\u80fd\u529b\u7684\u5168\u9762\u5206\u6790\uff0c\u5305\u62ec\u53d1\u73b0\u6700\u7ec8\u5c42\u4e3b\u8981\u5f3a\u5316\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u4f46\u727a\u7272\u89c6\u89c9\u533a\u5206\u80fd\u529b\u3001\u90e8\u5206\u6ce8\u610f\u529b\u5934\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u5f3a\u89c6\u89c9\u533a\u5206\u80fd\u529b\uff0c\u4ee5\u53ca\u5f02\u5e38\u4ee4\u724c\u76f8\u6bd4\u6b63\u5e38\u4ee4\u724c\u663e\u793a\u51fa\u7a00\u758f\u4e14\u4e00\u81f4\u7684\u6fc0\u6d3b\u6a21\u5f0f\u3002", "result": "\u57288\u4e2a\u5e38\u89c1\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLHT-CLIP\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u7684\u5b9e\u7528\u6027\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u989d\u5916\u8bad\u7ec3\u3001\u8f85\u52a9\u9884\u8bad\u7ec3\u7f51\u7edc\u6216\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86CLIP\u6a21\u578b\u4e2d\u89c6\u89c9\u533a\u5206\u80fd\u529b\u7684\u5173\u952e\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3aCLIP\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u6a21\u578b\u5185\u90e8\u673a\u5236\u6765\u63d0\u5347\u6027\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24342", "abs": "https://arxiv.org/abs/2510.24342", "authors": ["Silin Chen", "Yuzhong Chen", "Zifan Wang", "Junhao Wang", "Zifeng Jia", "Keith M Kendrick", "Tuo Zhang", "Lin Zhao", "Dezhong Yao", "Tianming Liu", "Xi Jiang"], "title": "A Unified Geometric Space Bridging AI Models and the Human Brain", "comment": null, "summary": "For decades, neuroscientists and computer scientists have pursued a shared\nambition: to understand intelligence and build it. Modern artificial neural\nnetworks now rival humans in language, perception, and reasoning, yet it is\nstill largely unknown whether these artificial systems organize information as\nthe brain does. Existing brain-AI alignment studies have shown the striking\ncorrespondence between the two systems, but such comparisons remain bound to\nspecific inputs and tasks, offering no common ground for comparing how AI\nmodels with different kinds of modalities-vision, language, or multimodal-are\nintrinsically organized. Here we introduce a groundbreaking concept of\nBrain-like Space: a unified geometric space in which every AI model can be\nprecisely situated and compared by mapping its intrinsic spatial attention\ntopological organization onto canonical human functional brain networks,\nregardless of input modality, task, or sensory domain. Our extensive analysis\nof 151 Transformer-based models spanning state-of-the-art large vision models,\nlarge language models, and large multimodal models uncovers a continuous\narc-shaped geometry within this space, reflecting a gradual increase of\nbrain-likeness; different models exhibit distinct distribution patterns within\nthis geometry associated with different degrees of brain-likeness, shaped not\nmerely by their modality but by whether the pretraining paradigm emphasizes\nglobal semantic abstraction and whether the positional encoding scheme\nfacilitates deep fusion across different modalities. Moreover, the degree of\nbrain-likeness for a model and its downstream task performance are not\n\"identical twins\". The Brain-like Space provides the first unified framework\nfor situating, quantifying, and comparing intelligence across domains,\nrevealing the deep organizational principles that bridge machines and the\nbrain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8111\u76f8\u4f3c\u7a7a\u95f4\u8fd9\u4e00\u5f00\u521b\u6027\u6982\u5ff5\uff0c\u901a\u8fc7\u5c06AI\u6a21\u578b\u7684\u5185\u5728\u7a7a\u95f4\u6ce8\u610f\u529b\u62d3\u6251\u6620\u5c04\u5230\u4eba\u7c7b\u529f\u80fd\u6027\u8111\u7f51\u7edc\u4e0a\uff0c\u6784\u5efa\u4e86\u7edf\u4e00\u7684\u51e0\u4f55\u7a7a\u95f4\u6765\u91cf\u5316\u6bd4\u8f83\u4e0d\u540c\u6a21\u6001AI\u6a21\u578b\u7684\u8111\u76f8\u4f3c\u5ea6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7ec4\u7ec7\u4e0e\u5927\u8111\u529f\u80fd\u7f51\u7edc\u4e4b\u95f4\u7684\u6df1\u5c42\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u8111-AI\u5bf9\u9f50\u7814\u7a76\u867d\u7136\u663e\u793a\u4e86\u4e24\u8005\u4e4b\u95f4\u7684\u663e\u8457\u5bf9\u5e94\u5173\u7cfb\uff0c\u4f46\u8fd9\u4e9b\u6bd4\u8f83\u5c40\u9650\u4e8e\u7279\u5b9a\u8f93\u5165\u548c\u4efb\u52a1\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u591f\u8de8\u6a21\u6001\u3001\u8de8\u4efb\u52a1\u7edf\u4e00\u6bd4\u8f83\u4e0d\u540cAI\u6a21\u578b\u5185\u5728\u7ec4\u7ec7\u7ed3\u6784\u7684\u5171\u540c\u6846\u67b6\uff0c\u65e0\u6cd5\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u3001\u8bed\u8a00\u6216\u591a\u6a21\u6001\u6a21\u578b\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u4e0e\u5927\u8111\u7684\u7ec4\u7ec7\u65b9\u5f0f\u76f8\u4f3c\u3002", "method": "\u63d0\u51fa\u4e86\u8111\u76f8\u4f3c\u7a7a\u95f4\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u901a\u8fc7\u5c06Transformer\u6a21\u578b\u7684\u5185\u5728\u7a7a\u95f4\u6ce8\u610f\u529b\u62d3\u6251\u7ec4\u7ec7\u6620\u5c04\u5230\u6807\u51c6\u4eba\u7c7b\u529f\u80fd\u6027\u8111\u7f51\u7edc\u4e0a\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u51e0\u4f55\u7a7a\u95f4\uff1b\u8be5\u65b9\u6cd5\u5206\u6790\u4e86151\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u6db5\u76d6\u6700\u5148\u8fdb\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u3001\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u9884\u8bad\u7ec3\u8303\u5f0f\u548c\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u5bf9\u8111\u76f8\u4f3c\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u5728\u8111\u76f8\u4f3c\u7a7a\u95f4\u4e2d\u53d1\u73b0\u4e86\u4e00\u4e2a\u8fde\u7eed\u7684\u5f27\u5f62\u51e0\u4f55\u7ed3\u6784\uff0c\u53cd\u6620\u4e86\u8111\u76f8\u4f3c\u5ea6\u7684\u9010\u6e10\u589e\u52a0\uff1b\u4e0d\u540c\u6a21\u578b\u5728\u8be5\u51e0\u4f55\u4e2d\u5c55\u73b0\u51fa\u4e0e\u4e0d\u540c\u8111\u76f8\u4f3c\u5ea6\u76f8\u5173\u7684\u5206\u5e03\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u4e0d\u4ec5\u53d7\u6a21\u6001\u5f71\u54cd\uff0c\u66f4\u5173\u952e\u5730\u53d6\u51b3\u4e8e\u9884\u8bad\u7ec3\u8303\u5f0f\u662f\u5426\u5f3a\u8c03\u5168\u5c40\u8bed\u4e49\u62bd\u8c61\u4ee5\u53ca\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u662f\u5426\u4fc3\u8fdb\u8de8\u6a21\u6001\u6df1\u5ea6\u878d\u5408\uff1b\u6a21\u578b\u8111\u76f8\u4f3c\u5ea6\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5e76\u975e\u5b8c\u5168\u4e00\u81f4\u3002", "conclusion": "\u8111\u76f8\u4f3c\u7a7a\u95f4\u4e3a\u8de8\u9886\u57df\u667a\u80fd\u7684\u5b9a\u4f4d\u3001\u91cf\u5316\u548c\u6bd4\u8f83\u63d0\u4f9b\u4e86\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u8fde\u63a5\u673a\u5668\u4e0e\u5927\u8111\u7684\u6df1\u5c42\u7ec4\u7ec7\u539f\u5219\uff1b\u7814\u7a76\u8868\u660e\u6a21\u578b\u7684\u5185\u5728\u7ec4\u7ec7\u65b9\u5f0f\u53cd\u6620\u4e86\u5176\u4e0e\u5927\u8111\u529f\u80fd\u7f51\u7edc\u7684\u5bf9\u5e94\u7a0b\u5ea6\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4efb\u52a1\u6027\u80fd\uff0c\u8fd9\u4e3a\u7406\u89e3\u667a\u80fd\u7684\u672c\u8d28\u548c\u6784\u5efa\u66f4\u7c7b\u8111\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.24328", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24328", "abs": "https://arxiv.org/abs/2510.24328", "authors": ["Hunzalah Hassan Bhatti", "Firoj Alam"], "title": "Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants", "comment": "Cultural Knowledge, Everyday Knowledge, Open-Ended Question,\n  Chain-of-Thought, Large Language Models, Native, Multilingual, Language\n  Diversity", "summary": "Large Language Models (LLMs) are increasingly used to answer everyday\nquestions, yet their performance on culturally grounded and dialectal content\nremains uneven across languages. We propose a comprehensive method that (i)\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\nEnglish and several Arabic dialects, (ii) converts them into open-ended\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\nrationales to fine-tune models for step-by-step reasoning. Using this method,\nwe extend an existing dataset in which QAs are parallelly aligned across\nmultiple language varieties, making it, to our knowledge, the first of its\nkind. We conduct extensive experiments with both open and closed models. Our\nfindings show that (i) models underperform on Arabic dialects, revealing\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\ndeveloped dataset will be publicly released to support further research on\nculturally and linguistically inclusive evaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u65b9\u6cd5\u6765\u8bc4\u4f30LLMs\u5728\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u548c\u6587\u5316\u5185\u5bb9\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5c06\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u591a\u9009\u9898\u8f6c\u6362\u4e3a\u82f1\u8bed\u548c\u591a\u79cd\u963f\u62c9\u4f2f\u65b9\u8a00\u7684\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u5e76\u5229\u7528\u601d\u7ef4\u94fe\u5fae\u8c03\u6a21\u578b\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0c\u63ed\u793a\u4e86LLMs\u5728\u65b9\u8a00\u77e5\u8bc6\u65b9\u9762\u7684\u6301\u7eed\u5dee\u8ddd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e5\u5e38\u95ee\u7b54\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5728\u6587\u5316\u57fa\u7840\u548c\u65b9\u8a00\u5185\u5bb9\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u8bed\u8a00\u95f4\u7684\u4e0d\u5747\u8861\uff0c\u7279\u522b\u662f\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u7684\u77e5\u8bc6\u8986\u76d6\u4e0d\u8db3\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u7efc\u5408\u65b9\u6cd5\u5305\u62ec\u5c06\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u591a\u9009\u9898\u7ffb\u8bd1\u4e3a\u82f1\u8bed\u548c\u591a\u79cd\u963f\u62c9\u4f2f\u65b9\u8a00\uff0c\u8f6c\u6362\u4e3a\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u5bf9\u591a\u79cdLLMs\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u751f\u6210\u601d\u7ef4\u94fe\u63a8\u7406\u6765\u5fae\u8c03\u6a21\u578b\u8fdb\u884c\u9010\u6b65\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u65b9\u8a00\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u63ed\u793a\u6587\u5316\u57fa\u7840\u548c\u65b9\u8a00\u7279\u5b9a\u77e5\u8bc6\u7684\u6301\u7eed\u5dee\u8ddd\uff1b\u963f\u62c9\u4f2f\u4e2d\u5fc3\u6a21\u578b\u5728\u591a\u9009\u9898\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u5728\u5f00\u653e\u5f0f\u95ee\u9898\u4e0a\u56f0\u96be\uff1b\u601d\u7ef4\u94fe\u5fae\u8c03\u63d0\u9ad8\u4e86\u5224\u65ad\u6b63\u786e\u6027\u4f46n-gram\u6307\u6807\u7ed3\u679c\u4e0d\u4e00\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u65b9\u8a00\u548c\u6587\u5316\u5185\u5bb9\u7406\u89e3\u4e0a\u7684\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u5f00\u53d1\u7684\u6570\u636e\u96c6\u5c06\u516c\u5f00\u652f\u6301\u6587\u5316\u8bed\u8a00\u5305\u5bb9\u6027\u8bc4\u4f30\u7814\u7a76\uff0c\u601d\u7ef4\u94fe\u65b9\u6cd5\u5bf9\u63a8\u7406\u80fd\u529b\u63d0\u5347\u6709\u6548\u4f46\u9700\u6539\u8fdb\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2510.23907", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23907", "abs": "https://arxiv.org/abs/2510.23907", "authors": ["Eddison Pham", "Prisha Priyadarshini", "Adrian Maliackel", "Kanishk Bandi", "Cristian Meo", "Kevin Zhu"], "title": "DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning", "comment": "16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026", "summary": "Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DynaStride\u7ba1\u9053\uff0c\u7528\u4e8e\u5728\u65e0\u9700\u624b\u52a8\u573a\u666f\u5206\u5272\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u8fde\u8d2f\u7684\u6559\u5b66\u89c6\u9891\u573a\u666f\u7ea7\u5b57\u5e55\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u5e27\u91c7\u6837\u3001\u591a\u6a21\u6001\u7a97\u53e3\u548c\u52a8\u6001\u6b65\u957f\u9009\u62e9\u7b97\u6cd5\uff0c\u6709\u6548\u5e73\u8861\u65f6\u95f4\u4e0a\u4e0b\u6587\u4e0e\u5197\u4f59\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u6559\u5b66\u89c6\u9891\u4e2d\u7684\u573a\u666f\u7ea7\u5b57\u5e55\u9700\u8981\u540c\u65f6\u7406\u89e3\u89c6\u89c9\u7ebf\u7d22\u548c\u65f6\u95f4\u7ed3\u6784\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u6355\u6349\u8fd9\u79cd\u7ed3\u6784\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u5b57\u5e55\u7f3a\u4e4f\u8fde\u8d2f\u6027\u548c\u8d28\u91cf\uff0c\u4ece\u800c\u5f71\u54cd\u89c6\u9891\u7684\u6559\u80b2\u610f\u56fe\u3002\u5f53\u524d\u7f3a\u4e4f\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u573a\u666f\u7ea7\u5b57\u5e55\u4e14\u65e0\u9700\u624b\u52a8\u573a\u666f\u5206\u5272\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "DynaStride\u91c7\u7528\u81ea\u9002\u5e94\u5e27\u91c7\u6837\u548c\u591a\u6a21\u6001\u7a97\u53e3\u6280\u672f\u6765\u6355\u6349\u573a\u666f\u5185\u7684\u5173\u952e\u8f6c\u6362\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u8fc7\u7a0b\u751f\u6210\u591a\u4e2a\u52a8\u4f5c-\u5bf9\u8c61\u5bf9\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u6b65\u957f\u7a97\u53e3\u9009\u62e9\u7b97\u6cd5\u6765\u4f18\u5316\u548c\u878d\u5408\u8fd9\u4e9b\u5bf9\u3002\u8be5\u65b9\u6cd5\u5728YouCookII\u6570\u636e\u96c6\u4e0a\u5229\u7528\u573a\u666f\u6807\u6ce8\uff0c\u6700\u7ec8\u5c06\u89c6\u89c9\u8bed\u4e49\u548c\u65f6\u95f4\u63a8\u7406\u6574\u5408\u5230\u5355\u4e2a\u6559\u5b66\u5b57\u5e55\u4e2d\u3002", "result": "\u4e0eVLLaMA3\u548cGPT-4o\u7b49\u5f3a\u57fa\u7ebf\u76f8\u6bd4\uff0cDynaStride\u5728N-gram\u6307\u6807\uff08BLEU\u3001METEOR\uff09\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5ea6\u91cf\uff08BERTScore\u3001CLIPScore\uff09\u4e0a\u5747\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\u3002\u5b9a\u6027\u5206\u6790\u8fdb\u4e00\u6b65\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u5b57\u5e55\u5728\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u4fe1\u606f\u4e30\u5bcc\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "DynaStride\u5c55\u793a\u4e86\u901a\u8fc7\u81ea\u9002\u5e94\u65f6\u95f4\u5efa\u6a21\u548c\u591a\u6a21\u6001\u63a8\u7406\u6765\u6539\u8fdbAI\u9a71\u52a8\u7684\u6559\u5b66\u5185\u5bb9\u751f\u6210\u7684\u53ef\u884c\u65b9\u5411\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5b57\u5e55\u8fde\u8d2f\u6027\u7684\u540c\u65f6\u6709\u6548\u5e73\u8861\u4e86\u65f6\u95f4\u4e0a\u4e0b\u6587\u4e0e\u5197\u4f59\uff0c\u4e3a\u6559\u5b66\u89c6\u9891\u7684\u81ea\u52a8\u5b57\u5e55\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.24411", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24411", "abs": "https://arxiv.org/abs/2510.24411", "authors": ["Qiushi Sun", "Mukai Li", "Zhoumianze Liu", "Zhihui Xie", "Fangzhi Xu", "Zhangyue Yin", "Kanzhi Cheng", "Zehao Li", "Zichen Ding", "Qi Liu", "Zhiyong Wu", "Zhuosheng Zhang", "Ben Kao", "Lingpeng Kong"], "title": "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows", "comment": "work in progress", "summary": "Computer-using agents powered by Vision-Language Models (VLMs) have\ndemonstrated human-like capabilities in operating digital environments like\nmobile platforms. While these agents hold great promise for advancing digital\nautomation, their potential for unsafe operations, such as system compromise\nand privacy leakage, is raising significant concerns. Detecting these safety\nconcerns across the vast and complex operational space of mobile environments\npresents a formidable challenge that remains critically underexplored. To\nestablish a foundation for mobile agent safety research, we introduce\nMobileRisk-Live, a dynamic sandbox environment accompanied by a safety\ndetection benchmark comprising realistic trajectories with fine-grained\nannotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety\ndetection framework that synergistically combines a Formal Verifier for\ndetecting explicit system-level violations with a VLM-based Contextual Judge\nfor assessing contextual risks and agent actions. Experiments show that\nOS-Sentinel achieves 10%-30% improvements over existing approaches across\nmultiple metrics. Further analysis provides critical insights that foster the\ndevelopment of safer and more reliable autonomous mobile agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MobileRisk-Live\u52a8\u6001\u6c99\u76d2\u73af\u5883\u548cOS-Sentinel\u6df7\u5408\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u79fb\u52a8\u73af\u5883\u4e2d\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4ee3\u7406\u7684\u5b89\u5168\u64cd\u4f5c\u95ee\u9898\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5668\u548c\u4e0a\u4e0b\u6587\u98ce\u9669\u8bc4\u4f30\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534710%-30%\u7684\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5728\u79fb\u52a8\u5e73\u53f0\u7b49\u6570\u5b57\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u7c7b\u4eba\u80fd\u529b\uff0c\u4f46\u5176\u6f5c\u5728\u7684\u4e0d\u5b89\u5168\u64cd\u4f5c\uff08\u5982\u7cfb\u7edf\u7834\u574f\u548c\u9690\u79c1\u6cc4\u9732\uff09\u5f15\u53d1\u4e86\u4e25\u91cd\u62c5\u5fe7\u3002\u5728\u79fb\u52a8\u73af\u5883\u5e7f\u9614\u590d\u6742\u7684\u64cd\u4f5c\u7a7a\u95f4\u4e2d\u68c0\u6d4b\u8fd9\u4e9b\u5b89\u5168\u95ee\u9898\u662f\u4e00\u4e2a\u5de8\u5927\u6311\u6218\uff0c\u76ee\u524d\u4ecd\u5904\u4e8e\u4e25\u91cd\u672a\u63a2\u7d22\u72b6\u6001\u3002", "method": "\u63d0\u51fa\u4e86OS-Sentinel\u6df7\u5408\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u534f\u540c\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5668\u7528\u4e8e\u68c0\u6d4b\u663e\u5f0f\u7cfb\u7edf\u7ea7\u8fdd\u89c4\uff0c\u4ee5\u53ca\u57fa\u4e8eVLM\u7684\u4e0a\u4e0b\u6587\u5224\u65ad\u5668\u7528\u4e8e\u8bc4\u4f30\u4e0a\u4e0b\u6587\u98ce\u9669\u548c\u4ee3\u7406\u884c\u4e3a\u3002\u540c\u65f6\u6784\u5efa\u4e86MobileRisk-Live\u52a8\u6001\u6c99\u76d2\u73af\u5883\u53ca\u5176\u5b89\u5168\u68c0\u6d4b\u57fa\u51c6\uff0c\u5305\u542b\u5177\u6709\u7ec6\u7c92\u5ea6\u6807\u6ce8\u7684\u73b0\u5b9e\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOS-Sentinel\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e8610%-30%\u7684\u6027\u80fd\u63d0\u5347\u3002\u8be5\u6846\u67b6\u5728\u68c0\u6d4b\u79fb\u52a8\u4ee3\u7406\u5b89\u5168\u98ce\u9669\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u5b89\u5168\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8fdb\u4e00\u6b65\u5206\u6790\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u4fc3\u8fdb\u4e86\u66f4\u5b89\u5168\u53ef\u9760\u7684\u81ea\u4e3b\u79fb\u52a8\u4ee3\u7406\u7684\u53d1\u5c55\u3002\u8be5\u7814\u7a76\u4e3a\u79fb\u52a8\u4ee3\u7406\u5b89\u5168\u7814\u7a76\u5efa\u7acb\u4e86\u57fa\u7840\uff0c\u63d0\u51fa\u7684\u6df7\u5408\u68c0\u6d4b\u65b9\u6cd5\u4e3a\u89e3\u51b3\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.24446", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24446", "abs": "https://arxiv.org/abs/2510.24446", "authors": ["Viktoriia Zinkovich", "Anton Antonov", "Andrei Spiridonov", "Denis Shepelev", "Andrey Moskalenko", "Daria Pugacheva", "Elena Tutubalina", "Andrey Kuznetsov", "Vlad Shakhuro"], "title": "SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nin vision-language tasks such as reasoning segmentation, where models generate\nsegmentation masks based on textual queries. While prior work has primarily\nfocused on perturbing image inputs, semantically equivalent textual\nparaphrases-crucial in real-world applications where users express the same\nintent in varied ways-remain underexplored. To address this gap, we introduce a\nnovel adversarial paraphrasing task: generating grammatically correct\nparaphrases that preserve the original query meaning while degrading\nsegmentation performance. To evaluate the quality of adversarial paraphrases,\nwe develop a comprehensive automatic evaluation protocol validated with human\nstudies. Furthermore, we introduce SPARTA-a black-box, sentence-level\noptimization method that operates in the low-dimensional semantic latent space\nof a text autoencoder, guided by reinforcement learning. SPARTA achieves\nsignificantly higher success rates, outperforming prior methods by up to 2x on\nboth the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive\nbaselines to assess the robustness of advanced reasoning segmentation models.\nWe reveal that they remain vulnerable to adversarial paraphrasing-even under\nstrict semantic and grammatical constraints. All code and data will be released\npublicly upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6297\u6027\u6539\u5199\u4efb\u52a1\uff0c\u901a\u8fc7\u751f\u6210\u8bed\u4e49\u7b49\u4ef7\u4f46\u80fd\u964d\u4f4e\u5206\u5272\u6027\u80fd\u7684\u6587\u672c\u67e5\u8be2\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5f00\u53d1\u4e86SPARTA\u65b9\u6cd5\u5728\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9ed1\u76d2\u4f18\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u8f93\u5165\u7684\u6270\u52a8\uff0c\u800c\u8bed\u4e49\u7b49\u4ef7\u7684\u6587\u672c\u6539\u5199\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u7528\u6237\u53ef\u80fd\u4ee5\u4e0d\u540c\u65b9\u5f0f\u8868\u8fbe\u76f8\u540c\u610f\u56fe\uff0c\u8fd9\u4e00\u9886\u57df\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86SPARTA\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u9ed1\u76d2\u3001\u53e5\u5b50\u7ea7\u522b\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u6587\u672c\u81ea\u7f16\u7801\u5668\u7684\u4f4e\u7ef4\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6307\u5bfc\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u5168\u9762\u7684\u81ea\u52a8\u8bc4\u4f30\u534f\u8bae\u6765\u9a8c\u8bc1\u5bf9\u6297\u6027\u6539\u5199\u7684\u8d28\u91cf\u3002", "result": "SPARTA\u5728ReasonSeg\u548cLLMSeg-40k\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa\u6700\u591a2\u500d\uff0c\u63ed\u793a\u4e86\u5148\u8fdb\u63a8\u7406\u5206\u5272\u6a21\u578b\u5373\u4f7f\u5728\u4e25\u683c\u7684\u8bed\u4e49\u548c\u8bed\u6cd5\u7ea6\u675f\u4e0b\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u6539\u5199\u7684\u653b\u51fb\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u5206\u5272\u4efb\u52a1\u4e2d\u5bf9\u8bed\u4e49\u7b49\u4ef7\u7684\u5bf9\u6297\u6027\u6539\u5199\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff0c\u8fd9\u4e3a\u6a21\u578b\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u8003\u8651\u591a\u6837\u5316\u7528\u6237\u8868\u8fbe\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.23930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23930", "abs": "https://arxiv.org/abs/2510.23930", "authors": ["Xirui Jin", "Renbiao Jin", "Boying Li", "Danping Zou", "Wenxian Yu"], "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors", "comment": "Accepted by NeurIPS 2025. Project page: https://planargs.github.io", "summary": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PlanarGS\uff0c\u4e00\u79cd\u9488\u5bf9\u5ba4\u5185\u573a\u666f\u91cd\u5efa\u76843D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8bed\u8a00\u63d0\u793a\u5e73\u9762\u5148\u9a8c\u548c\u51e0\u4f55\u76d1\u7763\u6765\u89e3\u51b3\u5927\u8303\u56f4\u4f4e\u7eb9\u7406\u533a\u57df\u4e2d3DGS\u7684\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf3D\u9ad8\u65af\u6cfc\u6e85\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5927\u8303\u56f4\u4f4e\u7eb9\u7406\u533a\u57df\uff0c\u4ec5\u4f9d\u8d56\u5149\u5ea6\u635f\u5931\u4f1a\u5bfc\u81f4\u51e0\u4f55\u6a21\u7cca\uff0c\u65e0\u6cd5\u6062\u590d\u9ad8\u4fdd\u771f\u5ea6\u76843D\u8868\u9762\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5ba4\u5185\u73af\u5883\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "\u63d0\u51faPlanarGS\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u8bed\u8a00\u63d0\u793a\u5e73\u9762\u5148\u9a8c\u7ba1\u7ebf\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b\u5e76\u901a\u8fc7\u8de8\u89c6\u56fe\u878d\u5408\u548c\u51e0\u4f55\u5148\u9a8c\u68c0\u67e5\u6765\u4f18\u5316\u533a\u57df\u63d0\u8bae\uff1b\u57283D\u9ad8\u65af\u4f18\u5316\u4e2d\u5f15\u5165\u5e73\u9762\u5148\u9a8c\u76d1\u7763\u9879\u548c\u51e0\u4f55\u5148\u9a8c\u76d1\u7763\u9879\uff0c\u5206\u522b\u5f3a\u5236\u5e73\u9762\u4e00\u81f4\u6027\u548c\u5f15\u5bfc\u9ad8\u65af\u5206\u5e03\u671d\u5411\u6df1\u5ea6\u4e0e\u6cd5\u7ebf\u7ebf\u7d22\u3002", "result": "\u5728\u6807\u51c6\u5ba4\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPlanarGS\u80fd\u591f\u91cd\u5efa\u51c6\u786e\u4e14\u8be6\u7ec6\u76843D\u8868\u9762\uff0c\u5728\u5404\u9879\u6307\u6807\u4e0a\u5747\u4ee5\u8f83\u5927\u4f18\u52bf\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u8bed\u8a00\u5f15\u5bfc\u7684\u5e73\u9762\u5148\u9a8c\u548c\u51e0\u4f55\u76d1\u7763\u80fd\u591f\u6709\u6548\u89e3\u51b33DGS\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u5ba4\u5185\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24551", "abs": "https://arxiv.org/abs/2510.24551", "authors": ["Gang Chen", "Changshuo Liu", "Gene Anne Ooi", "Marcus Tan", "Zhongle Xie", "Jianwei Yin", "James Wei Luen Yip", "Wenqiao Zhang", "Jiaqi Zhu", "Beng Chin Ooi"], "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5c06\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u91cd\u65b0\u5b9a\u4f4d\u4e3a\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u9886\u57df\u7684\u57fa\u7840\u652f\u6491\uff0c\u901a\u8fc7\u8bed\u4e49\u5411\u91cf\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u67e5\u8be2\u7b49\u9ad8\u6548\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u652f\u6301\u4e0a\u6e38\u6a21\u578b\u7ec4\u4ef6\u548c\u4e0b\u6e38\u4e34\u5e8a\u5e94\u7528\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u9886\u57df\u7684\u90e8\u7f72\u9700\u8981\u6df1\u5165\u7406\u89e3\u533b\u7597\u4efb\u52a1\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u533b\u7597\u6570\u636e\u751f\u547d\u5468\u671f\u7684\u7cfb\u7edf\u6027\u6574\u5408\uff0c\u65e0\u6cd5\u6301\u7eed\u652f\u6301\u591a\u6837\u5316\u533b\u7597\u6570\u636e\u548c\u77e5\u8bc6\u7684\u96c6\u6210\u3001\u8868\u793a\u4e0e\u68c0\u7d22\u3002", "method": "\u63d0\u51fa\u4ee5\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u4e3a\u57fa\u77f3\u7684\u751f\u6210\u5f0f\u533b\u7597\u7cfb\u7edf\u8bbe\u8ba1\u8303\u5f0f\uff0c\u901a\u8fc7\u8bed\u4e49\u5411\u91cf\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u67e5\u8be2\u7b49\u9ad8\u6548\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u652f\u6301\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u7684\u6574\u5408\u4e0e\u68c0\u7d22\uff0c\u4e3a\u4e0a\u6d41\u6a21\u578b\u9884\u8bad\u7ec3\u548c\u9886\u57df\u5fae\u8c03\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u540c\u65f6\u4f5c\u4e3a\u77e5\u8bc6\u68c0\u7d22\u540e\u7aef\u652f\u6491\u4efb\u52a1\u7279\u5b9a\u63a8\u7406\u3002", "result": "\u8be5\u751f\u6001\u7cfb\u7edf\u80fd\u591f\u6301\u7eed\u652f\u6301\u591a\u6837\u5316\u533b\u7597\u6570\u636e\u548c\u77e5\u8bc6\u7684\u96c6\u6210\u4e0e\u68c0\u7d22\uff0c\u4e3a\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u7528\u4e8e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u4ee3\u7406\u5c42\u652f\u6301\u4efb\u52a1\u7279\u5b9a\u7684\u63a8\u7406\u5e94\u7528\u3002", "conclusion": "\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u533b\u7597\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u8bbe\u8ba1\u8303\u5f0f\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6709\u6548\u7684\u533b\u7597\u670d\u52a1\u4ea4\u4ed8\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4f4d\u6570\u636e\u751f\u547d\u5468\u671f\u548c\u6784\u5efa\u53ef\u6301\u7eed\u7684\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\uff0c\u4e3a\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u9886\u57df\u7684\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24478", "abs": "https://arxiv.org/abs/2510.24478", "authors": ["Frederik Broy", "Maike Z\u00fcfle", "Jan Niehues"], "title": "Talk2Ref: A Dataset for Reference Prediction from Scientific Talks", "comment": null, "summary": "Scientific talks are a growing medium for disseminating research, and\nautomatically identifying relevant literature that grounds or enriches a talk\nwould be highly valuable for researchers and students alike. We introduce\nReference Prediction from Talks (RPT), a new task that maps long, and\nunstructured scientific presentations to relevant papers. To support research\non RPT, we present Talk2Ref, the first large-scale dataset of its kind,\ncontaining 6,279 talks and 43,429 cited papers (26 per talk on average), where\nrelevance is approximated by the papers cited in the talk's corresponding\nsource publication. We establish strong baselines by evaluating\nstate-of-the-art text embedding models in zero-shot retrieval scenarios, and\npropose a dual-encoder architecture trained on Talk2Ref. We further explore\nstrategies for handling long transcripts, as well as training for domain\nadaptation. Our results show that fine-tuning on Talk2Ref significantly\nimproves citation prediction performance, demonstrating both the challenges of\nthe task and the effectiveness of our dataset for learning semantic\nrepresentations from spoken scientific content. The dataset and trained models\nare released under an open license to foster future research on integrating\nspoken scientific communication into citation recommendation systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6f14\u8bb2\u5f15\u7528\u9884\u6d4b\u4efb\u52a1\uff0c\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21Talk2Ref\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u79d1\u5b66\u6f14\u8bb2\u4e2d\u81ea\u52a8\u8bc6\u522b\u76f8\u5173\u6587\u732e\u7684\u6027\u80fd\u3002", "motivation": "\u79d1\u5b66\u6f14\u8bb2\u6b63\u6210\u4e3a\u4f20\u64ad\u7814\u7a76\u7684\u91cd\u8981\u5a92\u4ecb\uff0c\u4f46\u81ea\u52a8\u8bc6\u522b\u80fd\u591f\u652f\u6491\u6216\u4e30\u5bcc\u6f14\u8bb2\u5185\u5bb9\u7684\u76f8\u5173\u6587\u732e\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7814\u7a76\u4eba\u5458\u548c\u5b66\u751f\u9700\u8981\u80fd\u591f\u4ece\u975e\u7ed3\u6784\u5316\u7684\u957f\u7bc7\u79d1\u5b66\u6f14\u8bb2\u4e2d\u51c6\u786e\u6620\u5c04\u5230\u76f8\u5173\u8bba\u6587\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u53cc\u7f16\u7801\u5668\u67b6\u6784\u7684\u6a21\u578b\uff0c\u5728Talk2Ref\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u63a2\u7d22\u4e86\u5904\u7406\u957f\u6587\u672c\u8f6c\u5f55\u672c\u7684\u7b56\u7565\u4ee5\u53ca\u9886\u57df\u81ea\u9002\u5e94\u8bad\u7ec3\u65b9\u6cd5\uff0c\u540c\u65f6\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5728\u96f6\u6837\u672c\u68c0\u7d22\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728Talk2Ref\u6570\u636e\u96c6\u4e0a\u7684\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u5f15\u7528\u9884\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u4efb\u52a1\u7684\u6280\u672f\u6311\u6218\u6027\u4ee5\u53ca\u4ece\u53e3\u8bed\u79d1\u5b66\u5185\u5bb9\u4e2d\u5b66\u4e60\u8bed\u4e49\u8868\u793a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5c06\u53e3\u8bed\u79d1\u5b66\u4ea4\u6d41\u6574\u5408\u5230\u5f15\u7528\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u53d1\u5e03\u7684\u5f00\u653e\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6a21\u578b\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u79d1\u5b66\u6f14\u8bb2\u5185\u5bb9\u81ea\u52a8\u5206\u6790\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.23968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23968", "abs": "https://arxiv.org/abs/2510.23968", "authors": ["Andriy Myronenko", "Dong Yang", "Baris Turkbey", "Mariam Aboian", "Sena Azamat", "Esra Akcicek", "Hongxu Yin", "Pavlo Molchanov", "Marc Edgar", "Yufan He", "Pengfei Guo", "Yucheng Tang", "Daguang Xu"], "title": "Reasoning Visual Language Model for Chest X-Ray Analysis", "comment": "NV-Reason-CXR-3B", "summary": "Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u601d\u7ef4\u94fe\u63a8\u7406\u5f15\u5165\u80f8\u90e8X\u5149\u89e3\u8bfb\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u4fdd\u771f\u89c6\u89c9\u7f16\u7801\u4e0e\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u591a\u6807\u7b7e\u5206\u7c7b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u53ef\u5ba1\u8ba1\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u867d\u7136\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5927\u591a\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u65e0\u6cd5\u63d0\u4f9b\u4e34\u5e8a\u533b\u751f\u4f9d\u8d56\u7684\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u53ef\u4fe1\u5ea6\u548c\u5b89\u5168\u6027\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u9ad8\u4fdd\u771f\u89c6\u89c9\u7f16\u7801\u5668\u4e0e\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u8fdb\u884c\u63a8\u7406\u98ce\u683c\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8eX\u5149\u5f02\u5e38\u5217\u8868\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u8f93\u51fa\u80fd\u591f\u53cd\u6620\u653e\u5c04\u79d1\u533b\u751f\u7cfb\u7edf\u6027\u601d\u7ef4\u8fc7\u7a0b\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u9274\u522b\u8bca\u65ad\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728\u5206\u5e03\u5916\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u6027\u80fd\uff1b\u5728\u4e13\u5bb6\u653e\u5c04\u79d1\u533b\u751f\u7684\u9605\u8bfb\u7814\u7a76\u4e2d\uff0c\u5b8c\u6574\u63a8\u7406\u8f68\u8ff9\u63d0\u9ad8\u4e86\u8bca\u65ad\u4fe1\u5fc3\u3001\u652f\u6301\u9519\u8bef\u5ba1\u8ba1\uff0c\u5e76\u51cf\u5c11\u4e86\u6700\u7ec8\u62a5\u544a\u5b8c\u6210\u65f6\u95f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\u63a8\u7406\u8d28\u91cf\u4e0e\u9884\u6d4b\u8d28\u91cf\u540c\u7b49\u91cd\u8981\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u8d56\u3001\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u900f\u660e\u51b3\u7b56\u8fc7\u7a0b\u7684\u4e34\u5e8a\u5e94\u7528\u4e2d\u3002"}}
{"id": "2510.24650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24650", "abs": "https://arxiv.org/abs/2510.24650", "authors": ["Nitin Rai", "Daeun", "Choi", "Nathan S. Boyd", "Arnold W. Schumann"], "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning", "comment": "26 pages, 8 figures, and 2 tables", "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5206\u6790\u4e86\u57fa\u7840\u6a21\u578b\u5728\u4f5c\u7269\u5b9a\u70b9\u75c5\u5bb3\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u8fdb\u5c55\uff0c\u91cd\u70b9\u8003\u5bdf\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u53ca\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\uff0c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u5b9e\u65f6\u53cd\u9988\u5c06\u63a8\u52a8\u4e0b\u4e00\u4ee3\u667a\u80fd\u519c\u4e1a\u7684\u53d1\u5c55\u3002", "motivation": "\u4f20\u7edf\u4f5c\u7269\u75c5\u5bb3\u7ba1\u7406\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u548c\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u6570\u636e\u3001\u7406\u89e3\u75c7\u72b6-\u7ba1\u7406\u5173\u7cfb\u4ee5\u53ca\u652f\u6301\u4ea4\u4e92\u5f0f\u95ee\u7b54\u6765\u9769\u65b0\u5b9a\u70b9\u75c5\u5bb3\u7ba1\u7406\uff0c\u89e3\u51b3\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5728\u519c\u4e1a\u5e94\u7528\u4e2d\u5b58\u5728\u7684\u5c40\u9650\u6027\u3002", "method": "\u7814\u7a76\u7cfb\u7edf\u7b5b\u9009\u4e86\u7ea640\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u91cd\u70b9\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u519c\u4e1a\u9886\u57df\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u4ee5\u53ca\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u96c6\u6210\u65b9\u5f0f\uff0c\u7279\u522b\u5173\u6ce8\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5982\u4f55\u5b9e\u73b0\u75c7\u72b6\u8bc6\u522b\u4e0e\u7ba1\u7406\u51b3\u7b56\u7684\u534f\u540c\u4f18\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u57fa\u7840\u6a21\u578b\u5e94\u7528\u5448\u73b0\u5feb\u901f\u589e\u957f\u8d8b\u52bf\uff0c2023-24\u5e74\u6587\u732e\u6570\u91cf\u6fc0\u589e\uff1b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u901f\u5ea6\u8fdc\u8d85\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u8868\u91cf\u589e\u957f5-10\u500d\uff1b\u5f3a\u5316\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u5728\u667a\u80fd\u55b7\u6d12\u9886\u57df\u4ecd\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5\uff1b\u6570\u5b57\u5b6a\u751f\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u53ef\u6709\u6548\u6a21\u62df\u9776\u5411\u55b7\u6d12\u8fc7\u7a0b\uff1b\u4eba\u673a\u534f\u4f5c\u7279\u522b\u662f\u4eba\u5728\u73af\u65b9\u6cd5\u5e94\u7528\u6709\u9650\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u6b63\u6210\u4e3a\u519c\u4e1a\u667a\u80fd\u5316\u7684\u5173\u952e\u9a71\u52a8\u529b\uff0c\u591a\u6a21\u6001\u6a21\u578b\u7ed3\u5408\u5b9e\u65f6\u53cd\u9988\u5c06\u5851\u9020\u4e0b\u4e00\u4ee3\u5b9a\u70b9\u75c5\u5bb3\u7ba1\u7406\u7cfb\u7edf\uff1b\u89e3\u51b3\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8f6c\u6362\u5dee\u8ddd\u662f\u5b9e\u73b0\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u6311\u6218\uff1b\u52a0\u5f3a\u4eba\u673a\u534f\u4f5c\u7279\u522b\u662f\u4eba\u7c7b\u4e13\u5bb6\u5bf9\u4e0d\u786e\u5b9a\u75c5\u4f8b\u7684\u9a8c\u8bc1\u673a\u5236\u662f\u672a\u6765\u91cd\u8981\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2510.24619", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24619", "abs": "https://arxiv.org/abs/2510.24619", "authors": ["Snegha A", "Sayambhu Sen", "Piyush Singh Pasi", "Abhishek Singhania", "Preethi Jyothi"], "title": "Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation", "comment": "12 Pages", "summary": "With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e09\u79cd\u524d\u7f00\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5728Llama 3.1 8B\u548cMistral v0.3 7B\u6a21\u578b\u4e0a\uff0c\u524d\u7f00\u65b9\u6cd5\u6bd4LoRA\u57fa\u7ebf\u5728Belebele\u57fa\u51c6\u4e0a\u63d0\u5347\u4e86\u9ad8\u8fbe6%\uff0c\u4ec5\u4f7f\u75281.23M\u5b66\u4e60\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u4e00\u81f4\u7684\u6027\u80fd\u6539\u8fdb\u3002", "motivation": "\u5c3d\u7ba1Llama\u548cMistral\u7b49\u4ec5\u89e3\u7801\u5668\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5c06\u5176\u9002\u5e94\u5230\u8de8\u8bed\u8a00\u65b0\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\uff1b\u867d\u7136\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u5982LoRA\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u57fa\u4e8e\u524d\u7f00\u7684\u6280\u672f\u5982\u8f6f\u63d0\u793a\u8c03\u4f18\u3001\u524d\u7f00\u8c03\u4f18\u548cLlama Adapter\u5728\u4ec5\u89e3\u7801\u5668\u6a21\u578b\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u4e2d\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u4e09\u79cd\u524d\u7f00\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u5305\u62ec\u8f6f\u63d0\u793a\u8c03\u4f18\u3001\u524d\u7f00\u8c03\u4f18\u548cLlama Adapter\uff0c\u7528\u4e8e\u4ece\u82f1\u8bed\u523035+\u79cd\u9ad8\u8d44\u6e90\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u8fc1\u79fb\uff1b\u5206\u6790\u8fd8\u63a2\u8ba8\u4e86\u8de8\u8bed\u8a00\u5bb6\u65cf\u548c\u6587\u5b57\u7cfb\u7edf\u7684\u8fc1\u79fb\u6548\u679c\uff0c\u4ee5\u53ca\u6a21\u578b\u89c4\u6a21\u4ece1B\u523024B\u7f29\u653e\u7684\u5f71\u54cd\u3002", "result": "\u5728Llama 3.1 8B\u6a21\u578b\u4e0a\uff0c\u524d\u7f00\u65b9\u6cd5\u5728Belebele\u57fa\u51c6\u4e0a\u6bd4LoRA\u57fa\u7ebf\u9ad8\u51fa\u9ad8\u8fbe6%\uff1bMistral v0.3 7B\u6a21\u578b\u4e5f\u89c2\u5bdf\u5230\u7c7b\u4f3c\u7684\u6539\u8fdb\uff1b\u5c3d\u7ba1\u524d\u7f00\u8c03\u4f18\u4ec5\u4f7f\u75281.23M\u5b66\u4e60\u53c2\u6570\uff0c\u4f46\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u524d\u7f00\u6280\u672f\u4f5c\u4e3aLoRA\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00\u8bbe\u7f6e\u4e2d\uff1b\u524d\u7f00\u65b9\u6cd5\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u597d\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u6027\u80fd\uff0c\u4e3a\u591a\u8bed\u8a00NLP\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.23981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23981", "abs": "https://arxiv.org/abs/2510.23981", "authors": ["Jiaqi Yan", "Ruilong Ren", "Jingren Liu", "Shuning Xu", "Ling Wang", "Yiheng Wang", "Yun Wang", "Long Zhang", "Xiangyu Chen", "Changzhi Sun", "Jixiang Luo", "Dell Zhang", "Hao Sun", "Chi Zhang", "Xuelong Li"], "title": "TeleEgo: Benchmarking Egocentric AI Assistants in the Wild", "comment": null, "summary": "Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TeleEgo\u2014\u2014\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5177\u8eabAI\u52a9\u624b\u7684\u957f\u65f6\u7a0b\u3001\u6d41\u5f0f\u3001\u5168\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u8d85\u8fc714\u5c0f\u65f6\u7684\u540c\u6b65\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u6570\u636e\uff0c\u5b9a\u4e49\u4e8612\u4e2a\u8bca\u65ad\u6027\u5b50\u4efb\u52a1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u5b9e\u65f6\u6027\u3001\u591a\u6a21\u6001\u5904\u7406\u548c\u957f\u671f\u8bb0\u5fc6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u5b64\u7acb\u8bc4\u4f30\u5177\u8eabAI\u52a9\u624b\u7684\u5404\u9879\u80fd\u529b\uff0c\u7f3a\u4e4f\u771f\u5b9e\u7684\u6d41\u5f0f\u573a\u666f\u652f\u6301\uff0c\u6216\u4ec5\u652f\u6301\u77ed\u671f\u4efb\u52a1\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u73b0\u5b9e\u73af\u5883\u4e2d\u6240\u9700\u7684\u5b9e\u65f6\u5904\u7406\u3001\u591a\u6a21\u6001\u8f93\u5165\u548c\u957f\u671f\u8bb0\u5fc6\u4fdd\u6301\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u5de5\u4f5c\u5b66\u4e60\u3001\u751f\u6d3b\u65b9\u5f0f\u3001\u793e\u4ea4\u6d3b\u52a8\u548c\u5916\u51fa\u6587\u5316\u56db\u4e2a\u9886\u57df\u7684\u540c\u6b65\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u91c7\u7528\u7edf\u4e00\u5168\u5c40\u65f6\u95f4\u7ebf\u5bf9\u9f50\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u89c6\u89c9\u53d9\u8ff0\u548c\u8bed\u97f3\u8f6c\u5f55\uff0c\u5b9a\u4e49\u4e86\u8bb0\u5fc6\u3001\u7406\u89e3\u548c\u8de8\u8bb0\u5fc6\u63a8\u7406\u4e09\u5927\u6838\u5fc3\u80fd\u529b\u768412\u4e2a\u8bca\u65ad\u6027\u5b50\u4efb\u52a1\uff0c\u5305\u542b3,291\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684\u95ee\u7b54\u9879\u76ee\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u65f6\u51c6\u786e\u7387\u548c\u8bb0\u5fc6\u6301\u4e45\u65f6\u95f4\u4e24\u4e2a\u5173\u952e\u8bc4\u4f30\u6307\u6807\u3002", "result": "TeleEgo\u57fa\u51c6\u6d4b\u8bd5\u5305\u542b\u6bcf\u4f4d\u53c2\u4e0e\u8005\u8d85\u8fc714\u5c0f\u65f6\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u652f\u6301\u591a\u79cd\u95ee\u7b54\u683c\u5f0f\u7684\u4e25\u683c\u6d41\u5f0f\u8bc4\u4f30\uff0c\u901a\u8fc7\u63d0\u51fa\u7684\u53cc\u6307\u6807\u7cfb\u7edf\u80fd\u591f\u8054\u5408\u8bc4\u4f30\u6a21\u578b\u7684\u6b63\u786e\u6027\u3001\u65f6\u95f4\u54cd\u5e94\u6027\u548c\u957f\u671f\u8bb0\u5fc6\u4fdd\u6301\u80fd\u529b\u3002", "conclusion": "TeleEgo\u4e3a\u5f00\u53d1\u5b9e\u7528\u7684AI\u52a9\u624b\u63d0\u4f9b\u4e86\u73b0\u5b9e\u4e14\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u957f\u65f6\u7a0b\u6d41\u5f0f\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u7684\u5efa\u7acb\uff0c\u63a8\u52a8\u4e86\u5177\u8eabAI\u52a9\u624b\u5728\u771f\u5b9e\u65e5\u5e38\u73af\u5883\u4e2d\u7684\u80fd\u529b\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u957f\u671f\u8bb0\u5fc6\u4fdd\u6301\u548c\u5b9e\u65f6\u54cd\u5e94\u65b9\u9762\u7684\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2510.24628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24628", "abs": "https://arxiv.org/abs/2510.24628", "authors": ["Anh Ngo", "Nicolas Rollet", "Catherine Pelachaud", "Chloe Clavel"], "title": "\"Mm, Wat?\" Detecting Other-initiated Repair Requests in Dialogue", "comment": "9 pages", "summary": "Maintaining mutual understanding is a key component in human-human\nconversation to avoid conversation breakdowns, in which repair, particularly\nOther-Initiated Repair (OIR, when one speaker signals trouble and prompts the\nother to resolve), plays a vital role. However, Conversational Agents (CAs)\nstill fail to recognize user repair initiation, leading to breakdowns or\ndisengagement. This work proposes a multimodal model to automatically detect\nrepair initiation in Dutch dialogues by integrating linguistic and prosodic\nfeatures grounded in Conversation Analysis. The results show that prosodic cues\ncomplement linguistic features and significantly improve the results of\npretrained text and audio embeddings, offering insights into how different\nfeatures interact. Future directions include incorporating visual cues,\nexploring multilingual and cross-context corpora to assess the robustness and\ngeneralizability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u57fa\u4e8e\u4f1a\u8bdd\u5206\u6790\u7684\u8bed\u97f3\u5b66\u548c\u8bed\u8a00\u5b66\u7279\u5f81\uff0c\u81ea\u52a8\u68c0\u6d4b\u8377\u5170\u8bed\u5bf9\u8bdd\u4e2d\u7684\u4fee\u590d\u53d1\u8d77\u3002\u7ed3\u679c\u8868\u660e\u8bed\u97f3\u7ebf\u7d22\u8865\u5145\u4e86\u8bed\u8a00\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6587\u672c\u548c\u97f3\u9891\u5d4c\u5165\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u4eba\u7c7b\u5bf9\u8bdd\u4e2d\u7ef4\u6301\u76f8\u4e92\u7406\u89e3\u662f\u907f\u514d\u5bf9\u8bdd\u4e2d\u65ad\u7684\u5173\u952e\uff0c\u5176\u4e2d\u4fee\u590d\u7279\u522b\u662f\u4ed6\u4eba\u53d1\u8d77\u4fee\u590d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002\u7136\u800c\u4f1a\u8bdd\u4ee3\u7406\u4ecd\u7136\u65e0\u6cd5\u8bc6\u522b\u7528\u6237\u7684\u4fee\u590d\u53d1\u8d77\uff0c\u5bfc\u81f4\u5bf9\u8bdd\u4e2d\u65ad\u6216\u7528\u6237\u8131\u79bb\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u57fa\u4e8e\u4f1a\u8bdd\u5206\u6790\u7684\u8bed\u8a00\u5b66\u548c\u8bed\u97f3\u5b66\u7279\u5f81\uff0c\u81ea\u52a8\u68c0\u6d4b\u8377\u5170\u8bed\u5bf9\u8bdd\u4e2d\u7684\u4fee\u590d\u53d1\u8d77\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u7684\u6587\u672c\u548c\u97f3\u9891\u5d4c\u5165\uff0c\u5e76\u5229\u7528\u8bed\u97f3\u7ebf\u7d22\u8865\u5145\u8bed\u8a00\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8bed\u97f3\u7ebf\u7d22\u663e\u8457\u8865\u5145\u4e86\u8bed\u8a00\u7279\u5f81\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6587\u672c\u548c\u97f3\u9891\u5d4c\u5165\u7684\u6027\u80fd\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u4e0d\u540c\u7279\u5f81\u4e4b\u95f4\u7684\u4ea4\u4e92\u673a\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u4fee\u590d\u53d1\u8d77\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u672a\u6765\u65b9\u5411\u5305\u62ec\u6574\u5408\u89c6\u89c9\u7ebf\u7d22\u3001\u63a2\u7d22\u591a\u8bed\u8a00\u548c\u8de8\u4e0a\u4e0b\u6587\u8bed\u6599\u5e93\u4ee5\u8bc4\u4f30\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24010", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24010", "abs": "https://arxiv.org/abs/2510.24010", "authors": ["Mirali Purohit", "Bimal Gajera", "Vatsal Malaviya", "Irish Mehta", "Kunal Kasodekar", "Jacob Adler", "Steven Lu", "Umaa Rebbapragada", "Hannah Kerner"], "title": "Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks", "comment": "Accepted at NeurIPS 2025", "summary": "Foundation models have enabled rapid progress across many specialized domains\nby leveraging large-scale pre-training on unlabeled data, demonstrating strong\ngeneralization to a variety of downstream tasks. While such models have gained\nsignificant attention in fields like Earth Observation, their application to\nMars science remains limited. A key enabler of progress in other domains has\nbeen the availability of standardized benchmarks that support systematic\nevaluation. In contrast, Mars science lacks such benchmarks and standardized\nevaluation frameworks, which have limited progress toward developing foundation\nmodels for Martian tasks. To address this gap, we introduce Mars-Bench, the\nfirst benchmark designed to systematically evaluate models across a broad range\nof Mars-related tasks using both orbital and surface imagery. Mars-Bench\ncomprises 20 datasets spanning classification, segmentation, and object\ndetection, focused on key geologic features such as craters, cones, boulders,\nand frost. We provide standardized, ready-to-use datasets and baseline\nevaluations using models pre-trained on natural images, Earth satellite data,\nand state-of-the-art vision-language models. Results from all analyses suggest\nthat Mars-specific foundation models may offer advantages over general-domain\ncounterparts, motivating further exploration of domain-adapted pre-training.\nMars-Bench aims to establish a standardized foundation for developing and\ncomparing machine learning models for Mars science. Our data, models, and code\nare available at: https://mars-bench.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Mars-Bench\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u706b\u661f\u79d1\u5b66\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b20\u4e2a\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5206\u7c7b\u3001\u5206\u5272\u548c\u68c0\u6d4b\u4efb\u52a1\uff0c\u65e8\u5728\u5efa\u7acb\u706b\u661f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u7684\u6807\u51c6\u5316\u57fa\u7840\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728\u5730\u7403\u89c2\u6d4b\u7b49\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u706b\u661f\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\uff0c\u4e3b\u8981\u969c\u788d\u662f\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u8fd9\u9650\u5236\u4e86\u706b\u661f\u4e13\u7528\u57fa\u7840\u6a21\u578b\u7684\u5f00\u53d1\u8fdb\u7a0b\u3002", "method": "Mars-Bench\u57fa\u51c6\u5305\u542b20\u4e2a\u6807\u51c6\u5316\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5206\u7c7b\u3001\u5206\u5272\u548c\u7269\u4f53\u68c0\u6d4b\u4efb\u52a1\uff0c\u4e13\u6ce8\u4e8e\u5173\u952e\u5730\u8d28\u7279\u5f81\u5982\u9668\u77f3\u5751\u3001\u9525\u4f53\u3001\u5de8\u77f3\u548c\u971c\u51bb\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f7f\u7528\u81ea\u7136\u56fe\u50cf\u3001\u5730\u7403\u536b\u661f\u6570\u636e\u548c\u6700\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u57fa\u7ebf\u8bc4\u4f30\u3002", "result": "\u6240\u6709\u5206\u6790\u7ed3\u679c\u8868\u660e\uff0c\u706b\u661f\u4e13\u7528\u57fa\u7840\u6a21\u578b\u76f8\u6bd4\u901a\u7528\u9886\u57df\u5bf9\u5e94\u6a21\u578b\u53ef\u80fd\u5177\u6709\u4f18\u52bf\uff0c\u8fd9\u6fc0\u52b1\u4e86\u5bf9\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\uff0c\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u706b\u661f\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5f00\u53d1\u548c\u6bd4\u8f83\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u57fa\u7840\u3002", "conclusion": "\u706b\u661f\u4e13\u7528\u57fa\u7840\u6a21\u578b\u5728\u706b\u661f\u79d1\u5b66\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u7684\u6f5c\u529b\uff0cMars-Bench\u7684\u5efa\u7acb\u4e3a\u672a\u6765\u706b\u661f\u673a\u5668\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24652", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24652", "abs": "https://arxiv.org/abs/2510.24652", "authors": ["Jiawei Zhou", "Lei Chen"], "title": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning", "comment": null, "summary": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faR3\u6846\u67b6\uff0c\u4e00\u79cd\u901a\u8fc7\u8bd5\u9519\u53cd\u9988\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u68c0\u7d22\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u52a8\u6001\u4f18\u5316\u68c0\u7d22\u5668\u5728RAG\u73af\u5883\u4e2d\u7684\u76f8\u5173\u6027\u5224\u65ad\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4fe1\u606f\u68c0\u7d22\u7684\u89d2\u8272\u4ece\u4e3a\u4eba\u7c7b\u7528\u6237\u68c0\u7d22\u4fe1\u606f\u8f6c\u53d8\u4e3a\u4e3aAI\u7cfb\u7edf\u68c0\u7d22\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u5176\u4e2d\u76f8\u5173\u6027\u96be\u4ee5\u9884\u5148\u5b9a\u4e49\u6216\u6807\u6ce8\uff0c\u8fd9\u6784\u6210\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "R3\u6846\u67b6\u91c7\u7528\u57fa\u4e8e\u8bd5\u9519\u53cd\u9988\u7684\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u68c0\u7d22\u5668\u80fd\u591f\u5728RAG\u73af\u5883\u4e2d\u52a8\u6001\u63a2\u7d22\u548c\u4f18\u5316\u76f8\u5173\u6027\uff0c\u901a\u8fc7\u68c0\u7d22\u7ed3\u679c\u4e0e\u73af\u5883\u4ea4\u4e92\u4ea7\u751f\u5bf9\u6bd4\u4fe1\u53f7\u6765\u81ea\u52a8\u6307\u5bfc\u68c0\u7d22\u5668\u7684\u81ea\u6211\u6539\u8fdb\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u6ce8\u6216\u5408\u6210\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cR3\u5c06RAG\u6027\u80fd\u76f8\u6bd4\u539f\u59cb\u68c0\u7d22\u5668\u63d0\u53475.2%\uff0c\u8d85\u8d8a\u6700\u5148\u8fdb\u68c0\u7d22\u56684.9%\uff0c\u540c\u65f6\u8fbe\u5230\u4e0e\u57fa\u4e8e\u540e\u8bad\u7ec3\u6216\u6307\u4ee4\u8c03\u4f18LLM\u7684LLM\u589e\u5f3a\u68c0\u7d22\u548cRAG\u7cfb\u7edf\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u8bad\u7ec3\u4ec5\u97004\u4e2aGPU\u5e76\u5728\u5355\u65e5\u5185\u5b8c\u6210\u3002", "conclusion": "R3\u8bc1\u660e\u4e86\u5728RAG\u73af\u5883\u4e2d\u901a\u8fc7\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u68c0\u7d22\u5668\u81ea\u4f18\u5316\u7684\u53ef\u884c\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u68c0\u7d22\u8d28\u91cf\uff0c\u4e3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u4f18\u5316\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.24034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24034", "abs": "https://arxiv.org/abs/2510.24034", "authors": ["Yufan Liu", "Wanqian Zhang", "Huashan Chen", "Lin Wang", "Xiaojun Jia", "Zheng Lin", "Weiping Wang"], "title": "AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts", "comment": "Accepted by ICCV 2025", "summary": "Despite rapid advancements in text-to-image (T2I) models, their safety\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\nunsafe images. Current red-teaming methods for proactively assessing such\nvulnerabilities usually require white-box access to T2I models, and rely on\ninefficient per-prompt optimization, as well as inevitably generate\nsemantically meaningless prompts easily blocked by filters. In this paper, we\npropose APT (AutoPrompT), a black-box framework that leverages large language\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\nbenign prompts. We first introduce an alternating optimization-finetuning\npipeline between adversarial suffix optimization and fine-tuning the LLM\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\nstrategy in optimization phase, enabling the bypass of both perplexity-based\nfilter and blacklist word filter: (1) we constrain the LLM generating\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\nbanned-token penalties to suppress the explicit generation of banned-tokens in\nblacklist. Extensive experiments demonstrate the excellent red-teaming\nperformance of our human-readable, filter-resistant adversarial prompts, as\nwell as superior zero-shot transferability which enables instant adaptation to\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\n(e.g., Leonardo.Ai.).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86APT\uff08AutoPrompT\uff09\uff0c\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u5bf9\u6297\u6027\u540e\u7f00\u7684\u9ed1\u76d2\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u5fae\u8c03\u7ba1\u9053\u548c\u53cc\u91cd\u89c4\u907f\u7b56\u7565\uff0c\u80fd\u591f\u7ed5\u8fc7\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684\u8fc7\u6ee4\u5668\u548c\u9ed1\u540d\u5355\u8bcd\u8fc7\u6ee4\u5668\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u7ea2\u961f\u6d4b\u8bd5\u6027\u80fd\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u63d0\u793a\u7684\u653b\u51fb\uff0c\u800c\u73b0\u6709\u7684\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u767d\u76d2\u8bbf\u95ee\u6743\u9650\uff0c\u4f9d\u8d56\u4f4e\u6548\u7684\u9010\u63d0\u793a\u4f18\u5316\uff0c\u5e76\u4e14\u4e0d\u53ef\u907f\u514d\u5730\u751f\u6210\u8bed\u4e49\u65e0\u610f\u4e49\u7684\u63d0\u793a\uff0c\u5bb9\u6613\u88ab\u8fc7\u6ee4\u5668\u963b\u6b62\u3002", "method": "\u63d0\u51fa\u4ea4\u66ff\u4f18\u5316\u5fae\u8c03\u7ba1\u9053\uff0c\u5728\u5bf9\u6297\u6027\u540e\u7f00\u4f18\u5316\u548c\u5229\u7528\u4f18\u5316\u540e\u7f00\u5fae\u8c03LLM\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\uff1b\u96c6\u6210\u53cc\u91cd\u89c4\u907f\u7b56\u7565\uff0c\u901a\u8fc7\u8f85\u52a9LLM\u56f0\u60d1\u5ea6\u8bc4\u5206\u7ea6\u675f\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u63d0\u793a\uff0c\u5e76\u5f15\u5165\u7981\u6b62\u8bcd\u60e9\u7f5a\u6765\u6291\u5236\u9ed1\u540d\u5355\u4e2d\u663e\u5f0f\u7981\u6b62\u8bcd\u7684\u751f\u6210\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u4eba\u7c7b\u53ef\u8bfb\u3001\u6297\u8fc7\u6ee4\u7684\u5bf9\u6297\u6027\u63d0\u793a\u5177\u6709\u51fa\u8272\u7684\u7ea2\u961f\u6d4b\u8bd5\u6027\u80fd\uff0c\u4ee5\u53ca\u5353\u8d8a\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\uff0c\u80fd\u591f\u5373\u65f6\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u63d0\u793a\uff0c\u5e76\u5728\u5546\u4e1aAPI\u4e2d\u66b4\u9732\u5173\u952e\u6f0f\u6d1e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5b89\u5168\u673a\u5236\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u7684\u9ed1\u76d2\u6846\u67b6\u4e3a\u8bc4\u4f30\u548c\u589e\u5f3a\u6a21\u578b\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u4eba\u7c7b\u53ef\u8bfb\u5bf9\u6297\u6027\u63d0\u793a\u5728\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\u673a\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24038", "abs": "https://arxiv.org/abs/2510.24038", "authors": ["Xingyu Zhu", "Beier Zhu", "Shuo Wang", "Kesen Zhao", "Hanwang Zhang"], "title": "Enhancing CLIP Robustness via Cross-Modality Alignment", "comment": "NeurIPS 2025 Spotlight", "summary": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCOLA\uff0c\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u6062\u590d\u5bf9\u6297\u6270\u52a8\u4e0b\u7684\u5168\u5c40\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u548c\u5c40\u90e8\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u4e14\u4e0e\u73b0\u6709\u5fae\u8c03\u6a21\u578b\u517c\u5bb9\uff0c\u572814\u4e2a\u96f6\u6837\u672c\u5206\u7c7b\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5982CLIP\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u5bf9\u6297\u6270\u52a8\u9ad8\u5ea6\u8106\u5f31\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5bf9\u6297\u5fae\u8c03\u6216\u63d0\u793a\u4f18\u5316\uff0c\u5ffd\u89c6\u4e86CLIP\u7f16\u7801\u7279\u5f81\u4e2d\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u5dee\u8ddd\uff0c\u8fd9\u79cd\u4e0d\u5bf9\u9f50\u5728\u5bf9\u6297\u6270\u52a8\u4e0b\u88ab\u663e\u8457\u653e\u5927\uff0c\u5bfc\u81f4\u5206\u7c7b\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002", "method": "COLA\u6846\u67b6\u91c7\u7528\u6700\u4f18\u4f20\u8f93\u65b9\u6cd5\uff0c\u9996\u5148\u5c06\u5bf9\u6297\u56fe\u50cf\u5d4c\u5165\u6295\u5f71\u5230\u7c7b\u522b\u6587\u672c\u7279\u5f81\u5f20\u6210\u7684\u5b50\u7a7a\u95f4\uff0c\u8fc7\u6ee4\u975e\u8bed\u4e49\u5931\u771f\u540c\u65f6\u4fdd\u7559\u5224\u522b\u4fe1\u606f\uff1b\u7136\u540e\u5c06\u56fe\u50cf\u548c\u6587\u672c\u5efa\u6a21\u4e3a\u591a\u4e2a\u589e\u5f3a\u89c6\u56fe\u4e0a\u7684\u79bb\u6563\u5206\u5e03\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u7ec6\u5316\u5bf9\u9f50\uff0c\u5b50\u7a7a\u95f4\u6295\u5f71\u65e0\u7f1d\u96c6\u6210\u5230\u6210\u672c\u8ba1\u7b97\u4e2d\u3002", "result": "\u572814\u4e2a\u96f6\u6837\u672c\u5206\u7c7b\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cCOLA\u5728PGD\u5bf9\u6297\u653b\u51fb\u4e0b\u5728ImageNet\u53ca\u5176\u53d8\u4f53\u4e0a\u5e73\u5747\u63d0\u53476.7%\uff0c\u540c\u65f6\u5728\u5e72\u51c0\u6837\u672c\u4e0a\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002\u8be5\u65b9\u6cd5\u8bad\u7ec3\u514d\u8d39\u4e14\u4e0e\u73b0\u6709\u5fae\u8c03\u6a21\u578b\u517c\u5bb9\u3002", "conclusion": "COLA\u901a\u8fc7\u663e\u5f0f\u5904\u7406\u5bf9\u6297\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u5bf9\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9c81\u68d2\u6027\u7684\u91cd\u8981\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u589e\u5f3a\u6a21\u578b\u5bf9\u6297\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2510.24134", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24134", "abs": "https://arxiv.org/abs/2510.24134", "authors": ["Yang Du", "Zhuoran Lin", "Kaiqiang Song", "Biao Wang", "Zhicheng Zheng", "Tiezheng Ge", "Bo Zheng", "Qin Jin"], "title": "VC4VG: Optimizing Video Captions for Text-to-Video Generation", "comment": "Accepted by EMNLP 2025", "summary": "Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VC4VG\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4f18\u5316\u89c6\u9891\u5b57\u5e55\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u89c6\u9891\u91cd\u5efa\u6240\u9700\u7684\u5173\u952e\u5143\u7d20\u5e76\u6784\u5efa\u591a\u7ef4\u8bc4\u4f30\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86T2V\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u9886\u57df\u867d\u7136\u8ba4\u8bc6\u5230\u9ad8\u8d28\u91cf\u89c6\u9891-\u6587\u672c\u5bf9\u7684\u91cd\u8981\u6027\uff0c\u4f46\u4e13\u95e8\u9488\u5bf9T2V\u8bad\u7ec3\u4f18\u5316\u7684\u89c6\u9891\u5b57\u5e55\u7b56\u7565\u7814\u7a76\u4ecd\u663e\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u5b57\u5e55\u8bbe\u8ba1\u65b9\u6cd5\u548c\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u63d0\u51faVC4VG\u6846\u67b6\uff0c\u4eceT2V\u89c6\u89d2\u5206\u6790\u5b57\u5e55\u5185\u5bb9\uff0c\u5c06\u89c6\u9891\u91cd\u5efa\u6240\u9700\u7684\u5173\u952e\u5143\u7d20\u5206\u89e3\u4e3a\u591a\u4e2a\u7ef4\u5ea6\uff0c\u5efa\u7acb\u539f\u5219\u6027\u7684\u5b57\u5e55\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e76\u6784\u5efaVC4VG-Bench\u57fa\u51c6\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u3001\u591a\u7ef4\u5ea6\u548c\u5fc5\u8981\u6027\u5206\u7ea7\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5e7f\u6cdb\u7684T2V\u5fae\u8c03\u5b9e\u9a8c\u8868\u660e\uff0c\u5b57\u5e55\u8d28\u91cf\u7684\u6539\u8fdb\u4e0e\u89c6\u9891\u751f\u6210\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3aT2V\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b57\u5e55\u4f18\u5316\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u786e\u7acb\u4e86\u89c6\u9891\u5b57\u5e55\u4f18\u5316\u5bf9T2V\u751f\u6210\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u65b9\u6cd5\u8bba\uff0c\u63a8\u52a8\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u6570\u636e\u8d28\u91cf\u4f18\u5316\u7814\u7a76\u3002"}}
{"id": "2510.24078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24078", "abs": "https://arxiv.org/abs/2510.24078", "authors": ["William Yang", "Xindi Wu", "Zhiwei Deng", "Esin Tureci", "Olga Russakovsky"], "title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification", "comment": null, "summary": "Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBOB\u5fae\u8c03\u7b56\u7565\uff0c\u901a\u8fc7\u63d0\u53d6\u7c7b\u65e0\u5173\u5c5e\u6027\u5e76\u663e\u5f0f\u6761\u4ef6\u5316\u6765\u7f13\u89e3T2I\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u4f4e\u6837\u672c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u4e3a\u5206\u7c7b\u4efb\u52a1\u751f\u6210\u6709\u6548\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5fae\u8c03T2I\u6a21\u578b\u867d\u7136\u80fd\u63d0\u9ad8\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u6837\u672c\u591a\u6837\u6027\u964d\u4f4e\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fd9\u4e00\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u63d0\u51faBOB\u5fae\u8c03\u7b56\u7565\uff0c\u9996\u5148\u4ece\u5c11\u91cf\u771f\u5b9e\u6837\u672c\u4e2d\u63d0\u53d6\u7c7b\u65e0\u5173\u5c5e\u6027\uff08\u5982\u573a\u666f\u80cc\u666f\u548c\u7269\u4f53\u59ff\u6001\uff09\uff0c\u7136\u540e\u5728T2I\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u5730\u6761\u4ef6\u5316\u8fd9\u4e9b\u5c5e\u6027\uff0c\u5e76\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5bf9\u5b83\u4eec\u8fdb\u884c\u8fb9\u7f18\u5316\u5904\u7406\u3002\u8fd9\u79cd\u8bbe\u8ba1\u7f13\u89e3\u4e86\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4fdd\u6301\u4e86T2I\u6a21\u578b\u7684\u751f\u6210\u5148\u9a8c\uff0c\u51cf\u5c11\u4e86\u4f30\u8ba1\u8bef\u5dee\uff0c\u5e76\u6700\u5c0f\u5316\u4e86\u610f\u5916\u7684\u7c7b\u95f4\u5173\u8054\u3002", "result": "\u5728\u591a\u4e2aT2I\u6a21\u578b\u3001\u9aa8\u5e72\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f7f\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\u7684\u4f4e\u6837\u672c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002BOB\u5728Aircraft\u6570\u636e\u96c6\u4e0a\u6bd4DataDream\u63d0\u5347\u4e867.4%\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u4e09\u4e2a\u4e0a\uff0c\u4f7f\u75285\u4e2a\u771f\u5b9e\u56fe\u50cf\u52a0\u4e0aBOB\u589e\u5f3a\u7684\u5fae\u8c03\u6027\u80fd\u4f18\u4e8e\u4f7f\u752810\u4e2a\u771f\u5b9e\u56fe\u50cf\u7684\u5fae\u8c03\u3002\u572824\u4e2a\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u768418\u4e2a\u4e2d\u4f18\u4e8e\u5148\u524d\u6280\u672f\uff0c\u5176\u4e2d14\u4e2a\u8bbe\u7f6e\u7684\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc72%\u3002", "conclusion": "BOB\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u6761\u4ef6\u5316\u548c\u8fb9\u7f18\u5316\u7c7b\u65e0\u5173\u5c5e\u6027\uff0c\u6709\u6548\u7f13\u89e3\u4e86T2I\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u591a\u6837\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u4f7f\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u4f4e\u6837\u672c\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.24133", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24133", "abs": "https://arxiv.org/abs/2510.24133", "authors": ["Minsuk Ji", "Sanghyeok Lee", "Namhyuk Ahn"], "title": "Compositional Image Synthesis with Inference-Time Scaling", "comment": "projcet page: https://github.com/gcl-inha/ReFocus", "summary": "Despite their impressive realism, modern text-to-image models still struggle\nwith compositionality, often failing to render accurate object counts,\nattributes, and spatial relations. To address this challenge, we present a\ntraining-free framework that combines an object-centric approach with\nself-refinement to improve layout faithfulness while preserving aesthetic\nquality. Specifically, we leverage large language models (LLMs) to synthesize\nexplicit layouts from input prompts, and we inject these layouts into the image\ngeneration process, where a object-centric vision-language model (VLM) judge\nreranks multiple candidates to select the most prompt-aligned outcome\niteratively. By unifying explicit layout-grounding with self-refine-based\ninference-time scaling, our framework achieves stronger scene alignment with\nprompts compared to recent text-to-image models. The code are available at\nhttps://github.com/gcl-inha/ReFocus.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7ec4\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u76ee\u6807\u4e2d\u5fc3\u5316\u65b9\u6cd5\u548c\u81ea\u4f18\u5316\u673a\u5236\u6765\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e03\u5c40\u5fe0\u5b9e\u5ea6\u3002\u8be5\u6846\u67b6\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5408\u6210\u663e\u5f0f\u5e03\u5c40\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8fed\u4ee3\u91cd\u6392\u5e8f\uff0c\u663e\u8457\u6539\u5584\u4e86\u573a\u666f\u4e0e\u63d0\u793a\u7684\u5bf9\u9f50\u8d28\u91cf\u3002", "motivation": "\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7ec4\u5408\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u7ecf\u5e38\u65e0\u6cd5\u51c6\u786e\u6e32\u67d3\u7269\u4f53\u6570\u91cf\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u3002\u8fd9\u79cd\u5e03\u5c40\u5fe0\u5b9e\u5ea6\u7684\u7f3a\u5931\u9650\u5236\u4e86\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u751f\u6210\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u751f\u6210\u56fe\u50cf\u4e0e\u6587\u672c\u63d0\u793a\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u8f93\u5165\u63d0\u793a\u4e2d\u5408\u6210\u663e\u5f0f\u5e03\u5c40\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u5e03\u5c40\u6ce8\u5165\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u3002\u5728\u751f\u6210\u9636\u6bb5\uff0c\u91c7\u7528\u76ee\u6807\u4e2d\u5fc3\u5316\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u591a\u4e2a\u5019\u9009\u56fe\u50cf\u8fdb\u884c\u8fed\u4ee3\u91cd\u6392\u5e8f\uff0c\u9009\u62e9\u4e0e\u63d0\u793a\u6700\u5bf9\u9f50\u7684\u7ed3\u679c\uff0c\u5b9e\u73b0\u4e86\u663e\u5f0f\u5e03\u5c40\u5f15\u5bfc\u4e0e\u57fa\u4e8e\u81ea\u4f18\u5316\u7684\u63a8\u7406\u65f6\u6269\u5c55\u7684\u7edf\u4e00\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u6700\u8fd1\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u573a\u666f\u4e0e\u63d0\u793a\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u4e86\u66f4\u5f3a\u7684\u6027\u80fd\u3002\u901a\u8fc7\u663e\u5f0f\u5e03\u5c40\u5f15\u5bfc\u548c\u8fed\u4ee3\u4f18\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u5728\u7269\u4f53\u6570\u91cf\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u7f8e\u5b66\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u663e\u5f0f\u5e03\u5c40\u5f15\u5bfc\u4e0e\u81ea\u4f18\u5316\u673a\u5236\u5728\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5fe0\u5b9e\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u8fd9\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u4e3a\u6539\u5584\u6a21\u578b\u7ec4\u5408\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u573a\u666f\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u63a8\u52a8\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2510.24285", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24285", "abs": "https://arxiv.org/abs/2510.24285", "authors": ["Juntian Zhang", "Song Jin", "Chuanqi Cheng", "Yuhan Liu", "Yankai Lin", "Xun Zhang", "Yufei Zhang", "Fei Jiang", "Guojun Yin", "Wei Lin", "Rui Yan"], "title": "ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model", "comment": null, "summary": "The limited capacity for fine-grained visual perception presents a critical\nbottleneck for Vision-Language Models (VLMs) in real-world applications.\nAddressing this is challenging due to the scarcity of high-quality data and the\nlimitations of existing methods: supervised fine-tuning (SFT) often compromises\ngeneral capabilities, while reinforcement fine-tuning (RFT) prioritizes textual\nreasoning over visual perception. To bridge this gap, we propose a novel\ntwo-stage task that structures visual perception learning as a coarse-to-fine\nprogressive process. Based on this task formulation, we develop ViPER, a\nself-bootstrapping framework specifically designed to enable iterative\nevolution through self-critiquing and self-prediction. By synergistically\nintegrating image-level and instance-level reconstruction with a two-stage\nreinforcement learning strategy, ViPER establishes a closed-loop training\nparadigm, where internally synthesized data directly fuel the enhancement of\nperceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the\nQwen-Viper series. With an average gain of 1.7% on seven comprehensive\nbenchmarks spanning various tasks and up to 6.0% on fine-grained perception,\nQwen-Viper consistently demonstrates superior performance across different\nvision-language scenarios while maintaining generalizability. Beyond enabling\nself-improvement in perceptual capabilities, ViPER provides concrete evidence\nfor the reciprocal relationship between generation and understanding, a\nbreakthrough to developing more autonomous and capable VLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ViPER\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u6e10\u8fdb\u5f0f\u89c6\u89c9\u611f\u77e5\u5b66\u4e60\u548c\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u7684\u6709\u9650\u80fd\u529b\u6784\u6210\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u74f6\u9888\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\uff1a\u76d1\u7763\u5fae\u8c03\u901a\u5e38\u4f1a\u635f\u5bb3\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\uff0c\u800c\u5f3a\u5316\u5fae\u8c03\u5219\u4f18\u5148\u8003\u8651\u6587\u672c\u63a8\u7406\u800c\u975e\u89c6\u89c9\u611f\u77e5\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u5bfb\u6c42\u4e00\u79cd\u80fd\u591f\u5e73\u8861\u611f\u77e5\u80fd\u529b\u63d0\u5347\u4e0e\u901a\u7528\u6027\u4fdd\u6301\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ViPER\u81ea\u4e3e\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u611f\u77e5\u5b66\u4e60\u6784\u5efa\u4e3a\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u6e10\u8fdb\u8fc7\u7a0b\uff0c\u7ed3\u5408\u56fe\u50cf\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u91cd\u5efa\u4efb\u52a1\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5efa\u7acb\u95ed\u73af\u8bad\u7ec3\u8303\u5f0f\uff0c\u5229\u7528\u5185\u90e8\u5408\u6210\u6570\u636e\u76f4\u63a5\u9a71\u52a8\u611f\u77e5\u80fd\u529b\u7684\u8fed\u4ee3\u8fdb\u5316\u3002", "result": "\u5728Qwen2.5-VL\u7cfb\u5217\u6a21\u578b\u4e0a\u5e94\u7528ViPER\u6846\u67b6\u4ea7\u751f\u4e86Qwen-Viper\u7cfb\u5217\uff0c\u5728\u4e03\u4e2a\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53471.7%\uff0c\u5728\u7ec6\u7c92\u5ea6\u611f\u77e5\u4efb\u52a1\u4e0a\u6700\u9ad8\u63d0\u53476.0%\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u89c6\u89c9\u8bed\u8a00\u573a\u666f\u4e2d\u4fdd\u6301\u4e00\u81f4\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ViPER\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u611f\u77e5\u80fd\u529b\u7684\u81ea\u6211\u63d0\u5347\uff0c\u8fd8\u4e3a\u751f\u6210\u4e0e\u7406\u89e3\u4e4b\u95f4\u7684\u4e92\u60e0\u5173\u7cfb\u63d0\u4f9b\u4e86\u5177\u4f53\u8bc1\u636e\uff0c\u8fd9\u4e00\u7a81\u7834\u4e3a\u5f00\u53d1\u66f4\u81ea\u4e3b\u3001\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u95ed\u73af\u8bad\u7ec3\u8303\u5f0f\u5728\u6a21\u578b\u80fd\u529b\u8fdb\u5316\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.24152", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24152", "abs": "https://arxiv.org/abs/2510.24152", "authors": ["Aodi Wu", "Xubo Luo"], "title": "Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning", "comment": "RoboSense Challenge with IROS 2025", "summary": "This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u7684\u7cfb\u7edf\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u63d0\u793a\u8def\u7531\u3001\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u3001\u89c6\u89c9\u7ec4\u88c5\u6a21\u5757\u548c\u4f18\u5316\u63a8\u7406\u53c2\u6570\uff0c\u5728RoboSense\u6311\u6218\u8d5b\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u591a\u4efb\u52a1\u4ea4\u53c9\u573a\u666f\u4e0b\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u95ee\u9898\u4e4b\u95f4\u7684\u5e72\u6270\u3002", "method": "\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u6df7\u5408\u63d0\u793a\u8def\u7531\u5668\u5bf9\u95ee\u9898\u8fdb\u884c\u5206\u7c7b\u5e76\u5206\u53d1\u81f3\u4efb\u52a1\u7279\u5b9a\u7684\u4e13\u5bb6\u63d0\u793a\uff1b\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u5d4c\u5165\u4e86\u663e\u5f0f\u5750\u6807\u7cfb\u3001\u7a7a\u95f4\u63a8\u7406\u89c4\u5219\u3001\u89d2\u8272\u626e\u6f14\u548c\u94fe\u5f0f/\u6811\u72b6\u63a8\u7406\uff1b\u89c6\u89c9\u7ec4\u88c5\u6a21\u5757\u6839\u636e\u95ee\u9898\u9700\u6c42\u7ec4\u5408\u591a\u89c6\u89d2\u56fe\u50cf\u4e0e\u5bf9\u8c61\u88c1\u526a\uff1b\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u4f18\u5316\u6a21\u578b\u63a8\u7406\u53c2\u6570\u914d\u7f6e\u3002", "result": "\u5728Qwen2.5-VL-72B\u6a21\u578b\u4e0a\u5b9e\u73b0\uff0c\u8be5\u65b9\u6cd5\u5728Phase-1\uff08\u5e72\u51c0\u6570\u636e\uff09\u4e0a\u8fbe\u523070.87%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5728Phase-2\uff08\u635f\u574f\u6570\u636e\uff09\u4e0a\u8fbe\u523072.85%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u63d0\u793a\u548c\u7a7a\u95f4\u57fa\u7840\u5bf9\u5b89\u5168\u5173\u952e\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7cfb\u7edf\u5316\u7684\u63d0\u793a\u5de5\u7a0b\u548c\u7a7a\u95f4\u57fa\u7840\u80fd\u591f\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u8def\u5f84\uff0c\u540c\u65f6\u5f00\u6e90\u4ee3\u7801\u548c\u63d0\u793a\u6a21\u677f\u4fc3\u8fdb\u4e86\u76f8\u5173\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2510.24514", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24514", "abs": "https://arxiv.org/abs/2510.24514", "authors": ["Huanyu Zhang", "Wenshan Wu", "Chengzu Li", "Ning Shang", "Yan Xia", "Yangyu Huang", "Yifan Zhang", "Li Dong", "Zhang Zhang", "Liang Wang", "Tieniu Tan", "Furu Wei"], "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Latent Sketchpad\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u914d\u5907\u5185\u90e8\u89c6\u89c9\u8349\u7a3f\u672c\uff0c\u5c06\u89c6\u89c9\u751f\u6210\u76f4\u63a5\u96c6\u6210\u5230\u81ea\u56de\u5f52\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u89c6\u89c9\u89c4\u5212\u548c\u60f3\u8c61\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u89c6\u89c9\u89c4\u5212\u548c\u60f3\u8c61\u7684\u590d\u6742\u573a\u666f\u4e2d\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf\u6a21\u578b\u7684\u5185\u90e8\u89c6\u89c9\u8868\u793a\u4ec5\u9650\u4e8e\u611f\u77e5\u7406\u89e3\uff0c\u7f3a\u4e4f\u751f\u6210\u6027\u89c6\u89c9\u601d\u7ef4\u7684\u80fd\u529b\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e0a\u4e0b\u6587\u611f\u77e5\u89c6\u89c9\u5934\u81ea\u56de\u5f52\u5730\u751f\u6210\u89c6\u89c9\u8868\u793a\uff0c\u4ee5\u53ca\u9884\u8bad\u7ec3\u7684\u8349\u56fe\u89e3\u7801\u5668\u5c06\u8fd9\u4e9b\u8868\u793a\u6e32\u67d3\u4e3a\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\uff0c\u5c06\u89c6\u89c9\u751f\u6210\u76f4\u63a5\u96c6\u6210\u5230\u6a21\u578b\u7684\u539f\u751f\u81ea\u56de\u5f52\u63a8\u7406\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728MazePlanning\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLatent Sketchpad\u5728\u5404\u79cdMLLMs\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u9aa8\u5e72\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u5728\u5305\u62ecGemma3\u548cQwen2.5-VL\u5728\u5185\u7684\u524d\u6cbfMLLMs\u4e0a\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6a21\u578b\u7684\u6587\u672c\u63a8\u7406\u6269\u5c55\u5230\u89c6\u89c9\u601d\u7ef4\uff0c\u8be5\u6846\u67b6\u4e3a\u4eba\u673a\u4ea4\u4e92\u548c\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u673a\u9047\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u50cf\u4eba\u7c7b\u4f7f\u7528\u8349\u56fe\u8fdb\u884c\u89c6\u89c9\u601d\u8003\u4e00\u6837\u53d1\u5c55\u60f3\u6cd5\u3002"}}
{"id": "2510.24214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24214", "abs": "https://arxiv.org/abs/2510.24214", "authors": ["Jinhong Deng", "Wen Li", "Joey Tianyi Zhou", "Yang He"], "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs", "comment": "NeurIPS 2025", "summary": "Multimodal Large Language Models (MLLMs) typically process a large number of\nvisual tokens, leading to considerable computational overhead, even though many\nof these tokens are redundant. Existing visual token pruning methods primarily\nfocus on selecting the most salient tokens based on attention scores, resulting\nin the semantic incompleteness of the selected tokens. In this paper, we\npropose a novel visual token pruning strategy, called\n\\textbf{S}aliency-\\textbf{C}overage \\textbf{O}riented token \\textbf{P}runing\nfor \\textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and\ncoverage of the selected visual tokens to better preserve semantic\ncompleteness. Specifically, we introduce a set-coverage for a given set of\nselected tokens, computed based on the token relationships. We then define a\ntoken-coverage gain for each unselected token, quantifying how much additional\ncoverage would be obtained by including it. By integrating the saliency score\ninto the token-coverage gain, we propose our SCOPE score and iteratively select\nthe token with the highest SCOPE score. We conduct extensive experiments on\nmultiple vision-language understanding benchmarks using the LLaVA-1.5 and\nLLaVA-Next models. Experimental results demonstrate that our method\nconsistently outperforms prior approaches. Our code is available at\n\\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSCOPE\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u7b56\u7565\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u663e\u8457\u6027\u548c\u8986\u76d6\u5ea6\u6765\u4f18\u5316\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u5904\u7406\u5927\u91cf\u89c6\u89c9\u4ee4\u724c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\uff0c\u73b0\u6709\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u6ce8\u610f\u529b\u5206\u6570\u9009\u62e9\u6700\u663e\u8457\u4ee4\u724c\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6240\u9009\u4ee4\u724c\u8bed\u4e49\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSCOPE\u7b56\u7565\uff0c\u5f15\u5165\u57fa\u4e8e\u4ee4\u724c\u5173\u7cfb\u7684\u96c6\u5408\u8986\u76d6\u5ea6\u6982\u5ff5\uff0c\u5b9a\u4e49\u6bcf\u4e2a\u672a\u9009\u4ee4\u724c\u7684\u8986\u76d6\u589e\u76ca\uff0c\u5c06\u663e\u8457\u6027\u5206\u6570\u6574\u5408\u5230\u4ee4\u724c\u8986\u76d6\u589e\u76ca\u4e2d\u5f62\u6210SCOPE\u5206\u6570\uff0c\u5e76\u8fed\u4ee3\u9009\u62e9\u5177\u6709\u6700\u9ad8SCOPE\u5206\u6570\u7684\u4ee4\u724c\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528LLaVA-1.5\u548cLLaVA-Next\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "SCOPE\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u4f18\u5316\u663e\u8457\u6027\u548c\u8986\u76d6\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u4e2d\u7684\u8bed\u4e49\u4e0d\u5b8c\u6574\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2510.24278", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24278", "abs": "https://arxiv.org/abs/2510.24278", "authors": ["Pietro Bongini", "Valentina Molinari", "Andrea Costanzo", "Benedetta Tondi", "Mauro Barni"], "title": "Training-free Source Attribution of AI-generated Images via Resynthesis", "comment": "14 pages, 4 figures, 1 table, accepted at \"The 17th IEEE\n  INTERNATIONAL WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS2025)\",\n  Perth, Australia", "summary": "Synthetic image source attribution is a challenging task, especially in data\nscarcity conditions requiring few-shot or zero-shot classification\ncapabilities. We present a new training-free one-shot attribution method based\non image resynthesis. A prompt describing the image under analysis is\ngenerated, then it is used to resynthesize the image with all the candidate\nsources. The image is attributed to the model which produced the resynthesis\nclosest to the original image in a proper feature space. We also introduce a\nnew dataset for synthetic image attribution consisting of face images from\ncommercial and open-source text-to-image generators. The dataset provides a\nchallenging attribution framework, useful for developing new attribution models\nand testing their capabilities on different generative architectures. The\ndataset structure allows to test approaches based on resynthesis and to compare\nthem to few-shot methods. Results from state-of-the-art few-shot approaches and\nother baselines show that the proposed resynthesis method outperforms existing\ntechniques when only a few samples are available for training or fine-tuning.\nThe experiments also demonstrate that the new dataset is a challenging one and\nrepresents a valuable benchmark for developing and evaluating future few-shot\nand zero-shot methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u91cd\u5408\u6210\u7684\u514d\u8bad\u7ec3\u5355\u6837\u672c\u5f52\u56e0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u63cf\u8ff0\u56fe\u50cf\u63d0\u793a\u8bcd\u5e76\u5728\u5019\u9009\u751f\u6210\u5668\u4e0a\u91cd\u5408\u6210\u56fe\u50cf\uff0c\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u6bd4\u8f83\u4e0e\u539f\u56fe\u7684\u76f8\u4f3c\u5ea6\u8fdb\u884c\u5f52\u56e0\u3002\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5c11\u6837\u672c\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5408\u6210\u56fe\u50cf\u5f52\u56e0\u6570\u636e\u96c6\u3002", "motivation": "\u5408\u6210\u56fe\u50cf\u6765\u6e90\u5f52\u56e0\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5c11\u6837\u672c\u6216\u96f6\u6837\u672c\u5206\u7c7b\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u6837\u672c\u6709\u9650\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u6781\u5c11\u8bad\u7ec3\u6570\u636e\u4e0b\u6709\u6548\u5de5\u4f5c\u7684\u5f52\u56e0\u6280\u672f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u50cf\u91cd\u5408\u6210\u7684\u514d\u8bad\u7ec3\u5355\u6837\u672c\u5f52\u56e0\u65b9\u6cd5\uff1a\u9996\u5148\u751f\u6210\u5f85\u5206\u6790\u56fe\u50cf\u7684\u63cf\u8ff0\u63d0\u793a\u8bcd\uff0c\u7136\u540e\u4f7f\u7528\u6240\u6709\u5019\u9009\u751f\u6210\u5668\u91cd\u5408\u6210\u8be5\u56fe\u50cf\uff0c\u5728\u9002\u5f53\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u6bd4\u8f83\u91cd\u5408\u6210\u56fe\u50cf\u4e0e\u539f\u56fe\u7684\u76f8\u4f3c\u5ea6\uff0c\u5c06\u56fe\u50cf\u5f52\u56e0\u4e8e\u4ea7\u751f\u6700\u63a5\u8fd1\u91cd\u5408\u6210\u7684\u6a21\u578b\u3002\u540c\u65f6\u6784\u5efa\u4e86\u5305\u542b\u5546\u4e1a\u548c\u5f00\u6e90\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5668\u7684\u4eba\u8138\u56fe\u50cf\u5f52\u56e0\u6570\u636e\u96c6\u3002", "result": "\u5728\u63d0\u51fa\u7684\u65b0\u6570\u636e\u96c6\u4e0a\uff0c\u91cd\u5408\u6210\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5c11\u6837\u672c\u65b9\u6cd5\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u6216\u5fae\u8c03\u6837\u672c\u6781\u5c11\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6570\u636e\u96c6\u5177\u6709\u6311\u6218\u6027\uff0c\u4e3a\u5f00\u53d1\u548c\u8bc4\u4f30\u672a\u6765\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u3002", "conclusion": "\u57fa\u4e8e\u91cd\u5408\u6210\u7684\u514d\u8bad\u7ec3\u5f52\u56e0\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u5408\u6210\u56fe\u50cf\u6765\u6e90\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e3a\u5f52\u56e0\u65b9\u6cd5\u5f00\u53d1\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u5f52\u56e0\u6280\u672f\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2510.24260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24260", "abs": "https://arxiv.org/abs/2510.24260", "authors": ["Zhaotong Yang", "Yi Chen", "Yanying Li", "Shengfeng He", "Yangyang Xu", "Junyu Dong", "Jian Yang", "Yong Du"], "title": "DeshadowMamba: Deshadowing as 1D Sequential Similarity", "comment": null, "summary": "Recent deep models for image shadow removal often rely on attention-based\narchitectures to capture long-range dependencies. However, their fixed\nattention patterns tend to mix illumination cues from irrelevant regions,\nleading to distorted structures and inconsistent colors. In this work, we\nrevisit shadow removal from a sequence modeling perspective and explore the use\nof Mamba, a selective state space model that propagates global context through\ndirectional state transitions. These transitions yield an efficient global\nreceptive field while preserving positional continuity. Despite its potential,\ndirectly applying Mamba to image data is suboptimal, since it lacks awareness\nof shadow-non-shadow semantics and remains susceptible to color interference\nfrom nearby regions. To address these limitations, we propose CrossGate, a\ndirectional modulation mechanism that injects shadow-aware similarity into\nMamba's input gate, allowing selective integration of relevant context along\ntransition axes. To further ensure appearance fidelity, we introduce ColorShift\nregularization, a contrastive learning objective driven by global color\nstatistics. By synthesizing structured informative negatives, it guides the\nmodel to suppress color contamination and achieve robust color restoration.\nTogether, these components adapt sequence modeling to the structural integrity\nand chromatic consistency required for shadow removal. Extensive experiments on\npublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-art\nvisual quality and strong quantitative performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faDeshadowMamba\uff0c\u4e00\u79cd\u57fa\u4e8e\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u56fe\u50cf\u9634\u5f71\u53bb\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7CrossGate\u65b9\u5411\u8c03\u5236\u673a\u5236\u548cColorShift\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u5728\u9634\u5f71\u53bb\u9664\u4e2d\u6df7\u5408\u65e0\u5173\u533a\u57df\u5149\u7167\u7ebf\u7d22\u5bfc\u81f4\u7684\u7ed3\u6784\u626d\u66f2\u548c\u989c\u8272\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u56fe\u50cf\u9634\u5f71\u53bb\u9664\u6a21\u578b\u5b58\u5728\u56fa\u5b9a\u6ce8\u610f\u529b\u6a21\u5f0f\u5bb9\u6613\u6df7\u5408\u65e0\u5173\u533a\u57df\u7684\u5149\u7167\u7ebf\u7d22\uff0c\u5bfc\u81f4\u7ed3\u6784\u626d\u66f2\u548c\u989c\u8272\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u65b9\u6cd5\u6765\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u8272\u5f69\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528Mamba\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u5e8f\u5217\u5efa\u6a21\uff0c\u63d0\u51faCrossGate\u65b9\u5411\u8c03\u5236\u673a\u5236\u5c06\u9634\u5f71\u611f\u77e5\u76f8\u4f3c\u6027\u6ce8\u5165\u8f93\u5165\u95e8\u5b9e\u73b0\u76f8\u5173\u4e0a\u4e0b\u6587\u7684\u9009\u62e9\u6027\u6574\u5408\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5168\u5c40\u989c\u8272\u7edf\u8ba1\u7684ColorShift\u5bf9\u6bd4\u5b66\u4e60\u6b63\u5219\u5316\u6765\u6291\u5236\u989c\u8272\u6c61\u67d3\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDeshadowMamba\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u5b9a\u91cf\u6027\u80fd\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u989c\u8272\u6062\u590d\u548c\u7ed3\u6784\u4fdd\u6301\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5e8f\u5217\u5efa\u6a21\u5728\u9634\u5f71\u53bb\u9664\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u65b9\u5411\u72b6\u6001\u8f6c\u6362\u548c\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u8272\u5f69\u4e00\u81f4\u6027\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u5efa\u6a21\u601d\u8def\u3002"}}
{"id": "2510.24321", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24321", "abs": "https://arxiv.org/abs/2510.24321", "authors": ["Ivica Dimitrovski", "Vlatko Spasev", "Ivan Kitanovski"], "title": "Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning", "comment": null, "summary": "Remote sensing applications increasingly rely on deep learning for scene\nclassification. However, their performance is often constrained by the scarcity\nof labeled data and the high cost of annotation across diverse geographic and\nsensor domains. While recent vision-language models like CLIP have shown\npromise by learning transferable representations at scale by aligning visual\nand textual modalities, their direct application to remote sensing remains\nsuboptimal due to significant domain gaps and the need for task-specific\nsemantic adaptation. To address this critical challenge, we systematically\nexplore prompt learning as a lightweight and efficient adaptation strategy for\nfew-shot remote sensing image scene classification. We evaluate several\nrepresentative methods, including Context Optimization, Conditional Context\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\nConstraints. These approaches reflect complementary design philosophies: from\nstatic context optimization to conditional prompts for enhanced generalization,\nmulti-modal prompts for joint vision-language adaptation, and semantically\nregularized prompts for stable learning without forgetting. We benchmark these\nprompt-learning methods against two standard baselines: zero-shot CLIP with\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\nThrough extensive experiments on multiple benchmark remote sensing datasets,\nincluding cross-dataset generalization tests, we demonstrate that prompt\nlearning consistently outperforms both baselines in few-shot scenarios.\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\ncross-domain performance. Our findings underscore prompt learning as a scalable\nand efficient solution for bridging the domain gap in satellite and aerial\nimagery, providing a strong foundation for future research in this field.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u63a2\u7d22\u4e86\u63d0\u793a\u5b66\u4e60\u4f5c\u4e3a\u9065\u611f\u56fe\u50cf\u573a\u666f\u5206\u7c7b\u7684\u9ad8\u6548\u9002\u5e94\u7b56\u7565\uff0c\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5176\u4e2d\u5e26\u81ea\u7ea6\u675f\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u8de8\u57df\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u6700\u4e3a\u9c81\u68d2\u3002", "motivation": "\u9065\u611f\u5e94\u7528\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u6027\u80fd\u53d7\u5230\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u8de8\u57df\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u9650\u5236\uff0c\u800c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5982CLIP\u76f4\u63a5\u5e94\u7528\u4e8e\u9065\u611f\u9886\u57df\u5b58\u5728\u663e\u8457\u7684\u9886\u57df\u5dee\u8ddd\u548c\u4efb\u52a1\u8bed\u4e49\u9002\u5e94\u9700\u6c42\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u4ee3\u8868\u6027\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u4f18\u5316\u3001\u6761\u4ef6\u4e0a\u4e0b\u6587\u4f18\u5316\u3001\u591a\u6a21\u6001\u63d0\u793a\u5b66\u4e60\u4ee5\u53ca\u5e26\u81ea\u7ea6\u675f\u7684\u63d0\u793a\u5b66\u4e60\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u6db5\u76d6\u4e86\u4ece\u9759\u6001\u4e0a\u4e0b\u6587\u4f18\u5316\u5230\u6761\u4ef6\u63d0\u793a\u589e\u5f3a\u6cdb\u5316\u3001\u591a\u6a21\u6001\u8054\u5408\u9002\u5e94\u4ee5\u53ca\u8bed\u4e49\u6b63\u5219\u5316\u7a33\u5b9a\u5b66\u4e60\u7684\u8bbe\u8ba1\u7406\u5ff5\u3002", "result": "\u5728\u591a\u4e2a\u9065\u611f\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u6301\u7eed\u4f18\u4e8e\u96f6\u6837\u672cCLIP\u548c\u57fa\u4e8e\u51bb\u7ed3\u7279\u5f81\u7684\u7ebf\u6027\u63a2\u6d4b\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u6d4b\u8bd5\u4e2d\uff0c\u5e26\u81ea\u7ea6\u675f\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9c81\u68d2\u7684\u8de8\u57df\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u63d0\u793a\u5b66\u4e60\u4f5c\u4e3a\u8fde\u63a5\u536b\u661f\u548c\u822a\u7a7a\u5f71\u50cf\u9886\u57df\u5dee\u8ddd\u7684\u53ef\u6269\u5c55\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u8be5\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u9002\u5e94\u7b56\u7565\u5728\u9065\u611f\u573a\u666f\u5206\u7c7b\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.24385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24385", "abs": "https://arxiv.org/abs/2510.24385", "authors": ["Herman Bergstr\u00f6m", "Zhongqi Yue", "Fredrik D. Johansson"], "title": "When are radiology reports useful for training medical image classifiers?", "comment": null, "summary": "Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u63a2\u8ba8\u4e86\u653e\u5c04\u5b66\u62a5\u544a\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u8bad\u7ec3\u4e2d\u7684\u4f7f\u7528\u65f6\u673a\u548c\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u6807\u7b7e\u4e0e\u6587\u672c\u5f3a\u76f8\u5173\u65f6\u9884\u8bad\u7ec3\u6709\u76ca\uff0c\u800c\u5728\u5f31\u76f8\u5173\u65f6\u53ef\u80fd\u6709\u5bb3\uff0c\u540c\u65f6\u5fae\u8c03\u9636\u6bb5\u4f7f\u7528\u62a5\u544a\u80fd\u5e26\u6765\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u653e\u5c04\u5b66\u62a5\u544a\u4f5c\u4e3a\u4e13\u5bb6\u6ce8\u91ca\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u653e\u5c04\u79d1\u533b\u751f\u624b\u52a8\u64b0\u5199\u62a5\u544a\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6709\u6548\u5229\u7528\u653e\u5c04\u5b66\u62a5\u544a\u6765\u6539\u8fdb\u4ec5\u57fa\u4e8e\u56fe\u50cf\u7684\u5206\u7c7b\u6027\u80fd\u3002\u73b0\u6709\u7814\u7a76\u5c40\u9650\u4e8e\u4f7f\u7528\u9884\u8bad\u7ec3\u56fe\u50cf\u8868\u793a\u8fdb\u884c\u5fae\u8c03\uff0c\u5ffd\u7565\u4e86\u6807\u7b7e\u4e0e\u6587\u672c\u5f31\u5173\u8054\u7684\u4efb\u52a1\u573a\u666f\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7cfb\u7edf\u6027\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u8003\u5bdf\u653e\u5c04\u5b66\u62a5\u544a\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e24\u4e2a\u9636\u6bb5\u7684\u4f7f\u7528\u7b56\u7565\uff0c\u8986\u76d6\u8bca\u65ad\u6027\u548c\u9884\u540e\u6027\u4efb\u52a1\uff08\u598212\u4e2a\u6708\u518d\u5165\u9662\u9884\u6d4b\uff09\uff0c\u5e76\u5728\u4e0d\u540c\u8bad\u7ec3\u96c6\u89c4\u6a21\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002\u91cd\u70b9\u6bd4\u8f83\u4e86\u663e\u5f0f\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a\u5f53\u6807\u7b7e\u4e0e\u6587\u672c\u5185\u5bb9\u5f3a\u76f8\u5173\u65f6\uff0c\u9884\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u62a5\u544a\u80fd\u63d0\u5347\u4e0b\u6e38\u5206\u7c7b\u6027\u80fd\uff1b\u4f46\u5728\u6807\u7b7e\u4e0e\u6587\u672c\u5f31\u5173\u8054\u7684\u573a\u666f\u4e2d\uff0c\u663e\u5f0f\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u9884\u8bad\u7ec3\u53cd\u800c\u4f1a\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u5fae\u8c03\u9636\u6bb5\u4f7f\u7528\u62a5\u544a\u80fd\u5e26\u6765\u663e\u8457\u6539\u8fdb\uff0c\u5728\u67d0\u4e9b\u8bbe\u7f6e\u4e0b\u5176\u5f71\u54cd\u751a\u81f3\u8d85\u8fc7\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u8bad\u7ec3\u4e2d\u5982\u4f55\u6709\u6548\u5229\u7528\u7279\u6743\u6587\u672c\u6570\u636e\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u6839\u636e\u4efb\u52a1\u7279\u6027\u9009\u62e9\u9002\u5f53\u8bad\u7ec3\u7b56\u7565\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u5728\u6807\u7b7e-\u6587\u672c\u5173\u8054\u5ea6\u5f71\u54cd\u65b9\u9762\u7684\u8ba4\u77e5\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.24709", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.24709", "abs": "https://arxiv.org/abs/2510.24709", "authors": ["Yihao Li", "Saeed Salehi", "Lyle Ungar", "Konrad P. Kording"], "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?", "comment": "Accepted as a Spotlight at NeurIPS 2025", "summary": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u89c6\u89c9Transformer\uff08ViT\uff09\u80fd\u591f\u81ea\u7136\u6d8c\u73b0\u51fa\u7269\u4f53\u7ed1\u5b9a\u80fd\u529b\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u63a2\u9488\u89e3\u7801IsSameObject\u5c5e\u6027\uff0c\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u6311\u6218\u4e86ViT\u7f3a\u4e4f\u7269\u4f53\u7ed1\u5b9a\u7684\u4f20\u7edf\u89c2\u70b9\u3002", "motivation": "\u5c3d\u7ba1\u5148\u524d\u5de5\u4f5c\u5e38\u901a\u8fc7\u663e\u5f0f\u65bd\u52a0\u7269\u4f53\u4e2d\u5fc3\u6ce8\u610f\u529b\u6765\u63a2\u7d22\u7269\u4f53\u7ed1\u5b9a\u7684\u76ca\u5904\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u79cd\u80fd\u529b\u662f\u5426\u5728\u9884\u8bad\u7ec3\u7684\u89c6\u89c9Transformer\u4e2d\u81ea\u7136\u6d8c\u73b0\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76ViT\u662f\u5426\u80fd\u591f\u81ea\u53d1\u5730\u5b66\u4e60\u8bc6\u522b\u54ea\u4e9b\u56fe\u50cf\u5757\u5c5e\u4e8e\u540c\u4e00\u7269\u4f53\uff0c\u4ee5\u53ca\u8fd9\u79cd\u80fd\u529b\u5982\u4f55\u53d7\u5230\u4e0d\u540c\u9884\u8bad\u7ec3\u76ee\u6807\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u76f8\u4f3c\u6027\u63a2\u9488\u4eceViT\u5404\u5c42\u7684\u56fe\u50cf\u5757\u5d4c\u5165\u4e2d\u89e3\u7801IsSameObject\u5c5e\u6027\uff0c\u5206\u6790\u81ea\u76d1\u7763\u6a21\u578b\uff08DINO\u3001MAE\u3001CLIP\uff09\u4e0eImageNet\u76d1\u7763\u6a21\u578b\u5728\u7269\u4f53\u7ed1\u5b9a\u80fd\u529b\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1IsSameObject\u4fe1\u53f7\u5bf9\u6ce8\u610f\u529b\u673a\u5236\u7684\u5f15\u5bfc\u4f5c\u7528\u3002", "result": "\u76f8\u4f3c\u6027\u63a2\u9488\u5728\u89e3\u7801IsSameObject\u5c5e\u6027\u65f6\u8fbe\u5230\u8d85\u8fc790%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u8be5\u80fd\u529b\u5728\u81ea\u76d1\u7763ViT\u4e2d\u53ef\u9760\u6d8c\u73b0\uff0c\u800c\u5728ImageNet\u76d1\u7763\u6a21\u578b\u4e2d\u663e\u8457\u8f83\u5f31\u3002IsSameObject\u88ab\u7f16\u7801\u5728\u7269\u4f53\u7279\u5f81\u4e4b\u4e0a\u7684\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\uff0c\u5e76\u4e3b\u52a8\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6d88\u878d\u8be5\u4fe1\u53f7\u4f1a\u964d\u4f4e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u7269\u4f53\u7ed1\u5b9a\u80fd\u529b\u5e76\u975eViT\u67b6\u6784\u7684\u7b80\u5355\u4ea7\u7269\uff0c\u800c\u662f\u901a\u8fc7\u7279\u5b9a\u9884\u8bad\u7ec3\u76ee\u6807\u4e60\u5f97\u7684\u80fd\u529b\u3002\u8fd9\u79cd\u6d8c\u73b0\u7684\u7b26\u53f7\u77e5\u8bc6\u6311\u6218\u4e86\u8fde\u63a5\u4e3b\u4e49\u7cfb\u7edf\u7f3a\u4e4f\u7ed3\u6784\u5316\u8868\u5f81\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u63ed\u793a\u4e86ViT\u80fd\u591f\u81ea\u7136\u5b66\u4e60\"\u54ea\u4e9b\u90e8\u5206\u5c5e\u4e8e\u4e00\u8d77\"\u7684\u62bd\u8c61\u6982\u5ff5\u3002"}}
{"id": "2510.24563", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24563", "abs": "https://arxiv.org/abs/2510.24563", "authors": ["Hongrui Jia", "Jitong Liao", "Xi Zhang", "Haiyang Xu", "Tianbao Xie", "Chaoya Jiang", "Ming Yan", "Si Liu", "Wei Ye", "Fei Huang"], "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents", "comment": null, "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OSWorld-MCP\uff0c\u8fd9\u662f\u9996\u4e2a\u5168\u9762\u4e14\u516c\u5e73\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8bc4\u4f30\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u5de5\u5177\u8c03\u7528\u3001GUI\u64cd\u4f5c\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u586b\u8865\u4e86\u591a\u6a21\u6001\u4ee3\u7406\u5de5\u5177\u8c03\u7528\u80fd\u529b\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u4ee3\u7406\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8GUI\u4ea4\u4e92\u6280\u80fd\uff0c\u800c\u7531\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u5b9e\u73b0\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\u88ab\u4e25\u91cd\u5ffd\u89c6\uff0c\u5bfc\u81f4\u96c6\u6210\u5de5\u5177\u8c03\u7528\u7684\u4ee3\u7406\u4e0e\u4ec5\u8bc4\u4f30GUI\u4ea4\u4e92\u7684\u4ee3\u7406\u4e4b\u95f4\u5b58\u5728\u4e0d\u516c\u5e73\u6bd4\u8f83\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u6d41\u6c34\u7ebf\u6765\u521b\u5efa\u5de5\u5177\uff0c\u5e76\u7ed3\u5408\u73b0\u6709\u5de5\u5177\u7684\u7cbe\u9009\u96c6\u5408\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u4eba\u5de5\u9a8c\u8bc1\u4ea7\u751f\u4e86158\u4e2a\u9ad8\u8d28\u91cf\u5de5\u5177\uff0c\u6db5\u76d67\u4e2a\u5e38\u89c1\u5e94\u7528\u7a0b\u5e8f\uff0c\u6bcf\u4e2a\u5de5\u5177\u90fd\u9a8c\u8bc1\u4e86\u529f\u80fd\u6b63\u786e\u6027\u3001\u5b9e\u7528\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "result": "\u5728OSWorld-MCP\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cMCP\u5de5\u5177\u666e\u904d\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff08\u4f8b\u5982OpenAI o3\u572815\u6b65\u65f6\u4ece8.3%\u63d0\u5347\u81f320.4%\uff0cClaude 4 Sonnet\u572850\u6b65\u65f6\u4ece40.1%\u63d0\u5347\u81f343.3%\uff09\uff0c\u4f46\u5373\u4f7f\u662f\u6700\u5f3a\u6a21\u578b\u7684\u5de5\u5177\u8c03\u7528\u7387\u4e5f\u76f8\u5bf9\u8f83\u4f4e\uff0c\u4ec5\u4e3a36.3%\u3002", "conclusion": "\u901a\u8fc7\u660e\u786e\u6d4b\u91cfMCP\u5de5\u5177\u4f7f\u7528\u6280\u80fd\uff0cOSWorld-MCP\u52a0\u6df1\u4e86\u5bf9\u591a\u6a21\u6001\u4ee3\u7406\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u8bc4\u4f30\u590d\u6742\u5de5\u5177\u8f85\u52a9\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5de5\u5177\u8c03\u7528\u80fd\u529b\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2510.24640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24640", "abs": "https://arxiv.org/abs/2510.24640", "authors": ["Xin Zhang", "Yuqi Song", "Fei Zuo"], "title": "A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries", "comment": null, "summary": "The rapid advancement of generative AI has enabled the creation of highly\nrealistic forged facial images, posing significant threats to AI security,\ndigital media integrity, and public trust. Face forgery techniques, ranging\nfrom face swapping and attribute editing to powerful diffusion-based image\nsynthesis, are increasingly being used for malicious purposes such as\nmisinformation, identity fraud, and defamation. This growing challenge\nunderscores the urgent need for robust and generalizable face forgery detection\nmethods as a critical component of AI security infrastructure. In this work, we\npropose a novel dual-branch convolutional neural network for face forgery\ndetection that leverages complementary cues from both spatial and frequency\ndomains. The RGB branch captures semantic information, while the frequency\nbranch focuses on high-frequency artifacts that are difficult for generative\nmodels to suppress. A channel attention module is introduced to adaptively fuse\nthese heterogeneous features, highlighting the most informative channels for\nforgery discrimination. To guide the network's learning process, we design a\nunified loss function, FSC Loss, that combines focal loss, supervised\ncontrastive loss, and a frequency center margin loss to enhance class\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\nincludes forged images generated from four representative methods:\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\nstrong performance across all categories and outperforms average human\naccuracy. These results demonstrate the model's effectiveness and its potential\ncontribution to safeguarding AI ecosystems against visual forgery attacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5206\u652f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\uff0c\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u57df\u548c\u9891\u57df\u7684\u4e92\u8865\u7ebf\u7d22\uff0c\u5728DiFF\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u5e76\u8d85\u8d8a\u4eba\u7c7b\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4e3aAI\u5b89\u5168\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u6709\u6548\u7684\u89c6\u89c9\u4f2a\u9020\u9632\u5fa1\u65b9\u6848\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u4f2a\u9020\u4eba\u8138\u56fe\u50cf\u53d8\u5f97\u9ad8\u5ea6\u903c\u771f\uff0c\u5bf9AI\u5b89\u5168\u3001\u6570\u5b57\u5a92\u4f53\u5b8c\u6574\u6027\u548c\u516c\u4f17\u4fe1\u4efb\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002\u5f53\u524d\u4eba\u8138\u4f2a\u9020\u6280\u672f\u5305\u62ec\u4eba\u8138\u4ea4\u6362\u3001\u5c5e\u6027\u7f16\u8f91\u548c\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u5408\u6210\u7b49\u65b9\u6cd5\uff0c\u6b63\u88ab\u6076\u610f\u7528\u4e8e\u865a\u5047\u4fe1\u606f\u3001\u8eab\u4efd\u6b3a\u8bc8\u548c\u8bfd\u8c24\u7b49\u76ee\u7684\uff0c\u4e9f\u9700\u5f00\u53d1\u9c81\u68d2\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u4f5c\u4e3aAI\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5206\u652f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5176\u4e2dRGB\u5206\u652f\u6355\u83b7\u8bed\u4e49\u4fe1\u606f\uff0c\u9891\u7387\u5206\u652f\u4e13\u6ce8\u4e8e\u751f\u6210\u6a21\u578b\u96be\u4ee5\u6291\u5236\u7684\u9ad8\u9891\u4f2a\u5f71\u3002\u5f15\u5165\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u81ea\u9002\u5e94\u878d\u5408\u8fd9\u4e9b\u5f02\u6784\u7279\u5f81\uff0c\u7a81\u51fa\u6700\u5177\u4fe1\u606f\u91cf\u7684\u4f2a\u9020\u5224\u522b\u901a\u9053\u3002\u8bbe\u8ba1\u7edf\u4e00\u7684FSC\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u7126\u70b9\u635f\u5931\u3001\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u548c\u9891\u7387\u4e2d\u5fc3\u8fb9\u754c\u635f\u5931\uff0c\u4ee5\u589e\u5f3a\u7c7b\u522b\u53ef\u5206\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5305\u542b\u6587\u672c\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u5230\u56fe\u50cf\u3001\u4eba\u8138\u4ea4\u6362\u548c\u4eba\u8138\u7f16\u8f91\u56db\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\u751f\u6210\u7684\u4f2a\u9020\u56fe\u50cf\u7684DiFF\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u7c7b\u522b\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u52b2\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4eba\u7c7b\u5e73\u5747\u51c6\u786e\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u5bf9\u89c6\u89c9\u4f2a\u9020\u653b\u51fb\u7684\u9632\u5fa1\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u7a7a\u95f4\u548c\u9891\u57df\u7ebf\u7d22\u7684\u53cc\u5206\u652f\u67b6\u6784\u5728\u68c0\u6d4b\u751f\u6210\u5f0fAI\u4f2a\u9020\u5185\u5bb9\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u7684AI\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491\u3002\u6a21\u578b\u5728\u591a\u79cd\u4f2a\u9020\u6280\u672f\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u8868\u660e\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u89c6\u89c9\u4f2a\u9020\u5a01\u80c1\u3002"}}
{"id": "2510.24667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24667", "abs": "https://arxiv.org/abs/2510.24667", "authors": ["Mia Kan", "Yilin Liu", "Niloy Mitra"], "title": "SAGE: Structure-Aware Generative Video Transitions between Diverse Clips", "comment": "Website: https://kan32501.github.io/sage.github.io/", "summary": "Video transitions aim to synthesize intermediate frames between two clips,\nbut naive approaches such as linear blending introduce artifacts that limit\nprofessional use or break temporal coherence. Traditional techniques\n(cross-fades, morphing, frame interpolation) and recent generative inbetweening\nmethods can produce high-quality plausible intermediates, but they struggle\nwith bridging diverse clips involving large temporal gaps or significant\nsemantic differences, leaving a gap for content-aware and visually coherent\ntransitions. We address this challenge by drawing on artistic workflows,\ndistilling strategies such as aligning silhouettes and interpolating salient\nfeatures to preserve structure and perceptual continuity. Building on this, we\npropose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot\napproach that combines structural guidance, provided via line maps and motion\nflow, with generative synthesis, enabling smooth, semantically consistent\ntransitions without fine-tuning. Extensive experiments and comparison with\ncurrent alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate\nthat SAGE outperforms both classical and generative baselines on quantitative\nmetrics and user studies for producing transitions between diverse clips. Code\nto be released on acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SAGE\uff08Structure-Aware Generative vidEo transitions\uff09\uff0c\u4e00\u79cd\u96f6\u6837\u672c\u7684\u89c6\u9891\u8fc7\u6e21\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5f15\u5bfc\u548c\u751f\u6210\u5408\u6210\uff0c\u5728\u591a\u6837\u89c6\u9891\u7247\u6bb5\u4e4b\u95f4\u5b9e\u73b0\u5e73\u6ed1\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u8fc7\u6e21\u3002\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u7ecf\u5178\u548c\u751f\u6210\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8fc7\u6e21\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u5927\u65f6\u95f4\u95f4\u9694\u6216\u663e\u8457\u8bed\u4e49\u5dee\u5f02\u7684\u591a\u6837\u5316\u89c6\u9891\u7247\u6bb5\u65f6\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u6280\u672f\u5982\u4ea4\u53c9\u6de1\u5165\u6de1\u51fa\u3001\u53d8\u5f62\u548c\u5e27\u63d2\u503c\u4ee5\u53ca\u6700\u8fd1\u7684\u751f\u6210\u4e2d\u95f4\u5e27\u65b9\u6cd5\u96be\u4ee5\u5728\u4fdd\u6301\u5185\u5bb9\u611f\u77e5\u548c\u89c6\u89c9\u8fde\u8d2f\u6027\u7684\u540c\u65f6\u6865\u63a5\u8fd9\u4e9b\u5dee\u5f02\u3002", "method": "SAGE\u91c7\u7528\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u7ed3\u5408\u7ed3\u6784\u5f15\u5bfc\uff08\u901a\u8fc7\u7ebf\u6846\u56fe\u548c\u8fd0\u52a8\u6d41\u63d0\u4f9b\uff09\u4e0e\u751f\u6210\u5408\u6210\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u5e73\u6ed1\u8fc7\u6e21\u3002\u8be5\u65b9\u6cd5\u501f\u9274\u827a\u672f\u5de5\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u5bf9\u9f50\u8f6e\u5ed3\u548c\u63d2\u503c\u663e\u8457\u7279\u5f81\u6765\u4fdd\u6301\u7ed3\u6784\u548c\u611f\u77e5\u8fde\u7eed\u6027\u3002", "result": "\u4e0e\u73b0\u6709\u66ff\u4ee3\u65b9\u6cd5\uff08FILM\u3001TVG\u3001DiffMorpher\u3001VACE\u3001GI\uff09\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u6bd4\u8f83\u8868\u660e\uff0cSAGE\u5728\u5b9a\u91cf\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\u5747\u4f18\u4e8e\u7ecf\u5178\u548c\u751f\u6210\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3a\u591a\u6837\u5316\u89c6\u9891\u7247\u6bb5\u751f\u6210\u66f4\u4f18\u8d28\u7684\u8fc7\u6e21\u6548\u679c\u3002", "conclusion": "SAGE\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u7684\u751f\u6210\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6837\u5316\u89c6\u9891\u7247\u6bb5\u95f4\u7684\u8fc7\u6e21\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u7ed3\u6784\u5f15\u5bfc\u4e0e\u751f\u6210\u5408\u6210\u7684\u6709\u6548\u6027\uff0c\u4e3a\u96f6\u6837\u672c\u89c6\u9891\u8fc7\u6e21\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e13\u4e1a\u89c6\u9891\u5236\u4f5c\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
