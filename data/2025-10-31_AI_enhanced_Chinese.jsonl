{"id": "2510.26411", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26411", "abs": "https://arxiv.org/abs/2510.26411", "authors": ["Riccardo Renzulli", "Colas Lepoutre", "Enrico Cassano", "Marco Grangetto"], "title": "MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders", "comment": null, "summary": "Artificial intelligence in healthcare requires models that are accurate and\ninterpretable. We advance mechanistic interpretability in medical vision by\napplying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,\na vision-language model trained on chest radiographs and reports. To quantify\ninterpretability, we propose an evaluation framework that combines correlation\nmetrics, entropy analyzes, and automated neuron naming via the MedGEMMA\nfoundation model. Experiments on the CheXpert dataset show that MedSAE neurons\nachieve higher monosemanticity and interpretability than raw MedCLIP features.\nOur findings bridge high-performing medical AI and transparency, offering a\nscalable step toward clinically reliable representations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u533b\u5b66\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08MedSAEs\uff09\u5e94\u7528\u4e8eMedCLIP\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u7ed3\u5408\u76f8\u5173\u6027\u6307\u6807\u3001\u71b5\u5206\u6790\u548c\u81ea\u52a8\u795e\u7ecf\u5143\u547d\u540d\u7684\u65b0\u8bc4\u4f30\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u89c6\u89c9\u6a21\u578b\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff0c\u5728CheXpert\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u539f\u59cbMedCLIP\u7279\u5f81\u66f4\u9ad8\u7684\u5355\u4e49\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u533b\u7597\u4eba\u5de5\u667a\u80fd\u9700\u8981\u65e2\u51c6\u786e\u53c8\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u5f53\u524d\u7814\u7a76\u65e8\u5728\u63a8\u8fdb\u533b\u5b66\u89c6\u89c9\u9886\u57df\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff0c\u89e3\u51b3\u9ad8\u6027\u80fd\u533b\u5b66AI\u4e0e\u900f\u660e\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u4e34\u5e8a\u53ef\u9760\u8868\u793a\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u5c06\u533b\u5b66\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08MedSAEs\uff09\u5e94\u7528\u4e8eMedCLIP\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408\u76f8\u5173\u6027\u6307\u6807\u3001\u71b5\u5206\u6790\u548c\u901a\u8fc7MedGEMMA\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u795e\u7ecf\u5143\u547d\u540d\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u91cf\u5316\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728CheXpert\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMedSAE\u795e\u7ecf\u5143\u76f8\u6bd4\u539f\u59cbMedCLIP\u7279\u5f81\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5355\u4e49\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u6a21\u578b\u900f\u660e\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u67b6\u8d77\u4e86\u9ad8\u6027\u80fd\u533b\u5b66AI\u4e0e\u900f\u660e\u5ea6\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4e3a\u4e34\u5e8a\u53ef\u9760\u8868\u793a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u63a8\u52a8\u4e86\u533b\u5b66\u89c6\u89c9\u6a21\u578b\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u53d1\u5c55\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.26721", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.26721", "abs": "https://arxiv.org/abs/2510.26721", "authors": ["Xinhan Zheng", "Huyu Wu", "Xueting Wang", "Haiyun Jiang"], "title": "Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis", "comment": null, "summary": "Multimodal large language models (MLLMs) exhibit a pronounced preference for\ntextual inputs when processing vision-language data, limiting their ability to\nreason effectively from visual evidence. Unlike prior studies that attribute\nthis text bias to external factors such as data imbalance or instruction\ntuning, we propose that the bias originates from the model's internal\narchitecture. Specifically, we hypothesize that visual key vectors (Visual\nKeys) are out-of-distribution (OOD) relative to the text key space learned\nduring language-only pretraining. Consequently, these visual keys receive\nsystematically lower similarity scores during attention computation, leading to\ntheir under-utilization in the context representation. To validate this\nhypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their\ndistributional structures using qualitative (t-SNE) and quantitative\n(Jensen-Shannon divergence) methods. The results provide direct evidence that\nvisual and textual keys occupy markedly distinct subspaces within the attention\nspace. The inter-modal divergence is statistically significant, exceeding\nintra-modal variation by several orders of magnitude. These findings reveal\nthat text bias arises from an intrinsic misalignment within the attention key\nspace rather than solely from external data factors.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6587\u672c\u504f\u597d\u6e90\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u5185\u90e8\u7684\u5173\u952e\u5411\u91cf\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u800c\u975e\u5916\u90e8\u6570\u636e\u56e0\u7d20\u3002\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u5173\u952e\u5411\u91cf\u76f8\u5bf9\u4e8e\u6587\u672c\u5173\u952e\u7a7a\u95f4\u5448\u73b0\u5206\u5e03\u5916\u7279\u6027\uff0c\u5bfc\u81f4\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u89c6\u89c9\u4fe1\u606f\u88ab\u7cfb\u7edf\u6027\u4f4e\u4f30\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u65f6\u8868\u73b0\u51fa\u660e\u663e\u7684\u6587\u672c\u504f\u597d\uff0c\u9650\u5236\u4e86\u5176\u57fa\u4e8e\u89c6\u89c9\u8bc1\u636e\u8fdb\u884c\u6709\u6548\u63a8\u7406\u7684\u80fd\u529b\u3002\u4e0e\u5148\u524d\u5c06\u8fd9\u79cd\u6587\u672c\u504f\u89c1\u5f52\u56e0\u4e8e\u6570\u636e\u4e0d\u5e73\u8861\u6216\u6307\u4ee4\u8c03\u4f18\u7b49\u5916\u90e8\u56e0\u7d20\u7684\u7814\u7a76\u4e0d\u540c\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u8be5\u504f\u89c1\u6e90\u4e8e\u6a21\u578b\u5185\u90e8\u67b6\u6784\uff0c\u7279\u522b\u662f\u89c6\u89c9\u5173\u952e\u5411\u91cf\u5728\u8bed\u8a00\u9884\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u7684\u6587\u672c\u5173\u952e\u7a7a\u95f4\u4e2d\u5448\u73b0\u5206\u5e03\u5916\u7279\u6027\u3002", "method": "\u7814\u7a76\u4eceLLaVA\u548cQwen2.5-VL\u6a21\u578b\u4e2d\u63d0\u53d6\u5173\u952e\u5411\u91cf\uff0c\u5e76\u4f7f\u7528\u5b9a\u6027\uff08t-SNE\uff09\u548c\u5b9a\u91cf\uff08Jensen-Shannon\u6563\u5ea6\uff09\u65b9\u6cd5\u5206\u6790\u5176\u5206\u5e03\u7ed3\u6784\u3002\u901a\u8fc7\u6bd4\u8f83\u89c6\u89c9\u548c\u6587\u672c\u5173\u952e\u5411\u91cf\u5728\u6ce8\u610f\u529b\u7a7a\u95f4\u4e2d\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u9a8c\u8bc1\u89c6\u89c9\u5173\u952e\u5411\u91cf\u76f8\u5bf9\u4e8e\u6587\u672c\u5173\u952e\u7a7a\u95f4\u7684\u5206\u5e03\u5916\u5047\u8bbe\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u76f4\u63a5\u8bc1\u636e\u8868\u660e\u89c6\u89c9\u548c\u6587\u672c\u5173\u952e\u5411\u91cf\u5728\u6ce8\u610f\u529b\u7a7a\u95f4\u4e2d\u5360\u636e\u660e\u663e\u4e0d\u540c\u7684\u5b50\u7a7a\u95f4\u3002\u6a21\u6001\u95f4\u5dee\u5f02\u5728\u7edf\u8ba1\u4e0a\u663e\u8457\uff0c\u8d85\u8fc7\u6a21\u6001\u5185\u53d8\u5f02\u7684\u6570\u4e2a\u6570\u91cf\u7ea7\u3002\u89c6\u89c9\u5173\u952e\u5411\u91cf\u5728\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u83b7\u5f97\u7684\u76f8\u4f3c\u6027\u5f97\u5206\u7cfb\u7edf\u6027\u8f83\u4f4e\uff0c\u5bfc\u81f4\u5176\u5728\u4e0a\u4e0b\u6587\u8868\u793a\u4e2d\u7684\u5229\u7528\u4e0d\u8db3\u3002", "conclusion": "\u6587\u672c\u504f\u89c1\u6e90\u4e8e\u6ce8\u610f\u529b\u5173\u952e\u7a7a\u95f4\u5185\u90e8\u7684\u5185\u5728\u4e0d\u5bf9\u9f50\uff0c\u800c\u975e\u4ec5\u6765\u81ea\u5916\u90e8\u6570\u636e\u56e0\u7d20\u3002\u8fd9\u4e00\u53d1\u73b0\u5bf9\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u8868\u660e\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u6ce8\u610f\u529b\u673a\u5236\u5728\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5f00\u53d1\u66f4\u6709\u6548\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u65b9\u6cd5\u4ee5\u6539\u5584\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.26052", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26052", "abs": "https://arxiv.org/abs/2510.26052", "authors": ["Hoyeon Chang", "Seungjin Kim", "Yoonseok Choi"], "title": "Dynamic VLM-Guided Negative Prompting for Diffusion Models", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: The First Workshop on Generative and Protective AI for\n  Content Creation", "summary": "We propose a novel approach for dynamic negative prompting in diffusion\nmodels that leverages Vision-Language Models (VLMs) to adaptively generate\nnegative prompts during the denoising process. Unlike traditional Negative\nPrompting methods that use fixed negative prompts, our method generates\nintermediate image predictions at specific denoising steps and queries a VLM to\nproduce contextually appropriate negative prompts. We evaluate our approach on\nvarious benchmark datasets and demonstrate the trade-offs between negative\nguidance strength and text-image alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u52a8\u6001\u8d1f\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u751f\u6210\u4e2d\u95f4\u56fe\u50cf\u9884\u6d4b\u5e76\u67e5\u8be2VLM\u6765\u4ea7\u751f\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8d1f\u63d0\u793a\uff0c\u76f8\u6bd4\u4f20\u7edf\u56fa\u5b9a\u8d1f\u63d0\u793a\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u7684\u56fe\u50cf\u751f\u6210\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u8d1f\u63d0\u793a\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684\u8d1f\u63d0\u793a\u6587\u672c\uff0c\u7f3a\u4e4f\u5bf9\u751f\u6210\u8fc7\u7a0b\u4e2d\u4e0a\u4e0b\u6587\u53d8\u5316\u7684\u9002\u5e94\u6027\uff0c\u65e0\u6cd5\u6839\u636e\u4e2d\u95f4\u751f\u6210\u72b6\u6001\u52a8\u6001\u8c03\u6574\u8d1f\u5f15\u5bfc\u7b56\u7565\uff0c\u9650\u5236\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u6587\u672c\u5bf9\u9f50\u7684\u4f18\u5316\u6f5c\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u5728\u53bb\u566a\u8fc7\u7a0b\u7684\u7279\u5b9a\u6b65\u9aa4\u751f\u6210\u4e2d\u95f4\u56fe\u50cf\u9884\u6d4b\uff0c\u7136\u540e\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5206\u6790\u8fd9\u4e9b\u4e2d\u95f4\u7ed3\u679c\u5e76\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8d1f\u63d0\u793a\uff0c\u5b9e\u73b0\u4e86\u8d1f\u63d0\u793a\u7684\u52a8\u6001\u81ea\u9002\u5e94\u751f\u6210\uff0c\u800c\u975e\u4f9d\u8d56\u9884\u8bbe\u7684\u56fa\u5b9a\u63d0\u793a\u6587\u672c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8d1f\u5f15\u5bfc\u5f3a\u5ea6\u4e0e\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u5ea6\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u76f8\u6bd4\u4f20\u7edf\u56fa\u5b9a\u8d1f\u63d0\u793a\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u52a8\u6001\u8d1f\u63d0\u793a\u673a\u5236\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u751f\u6210\u63a7\u5236\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u8d1f\u63d0\u793a\u751f\u6210\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u81ea\u9002\u5e94\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.25799", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25799", "abs": "https://arxiv.org/abs/2510.25799", "authors": ["Adam S. Jovine", "Tinghan Ye", "Francis Bahk", "Jingjing Wang", "David B. Shmoys", "Peter I. Frazier"], "title": "LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection", "comment": null, "summary": "Human experts often struggle to select the best option from a large set of\nitems with multiple competing objectives, a process bottlenecked by the\ndifficulty of formalizing complex, implicit preferences. To address this, we\nintroduce LISTEN, a framework that leverages a Large Language Model (LLM) as a\nzero-shot preference oracle, guided only by an expert's high-level priorities\nin natural language. To operate within LLM constraints like context windows and\ninference costs, we propose two iterative algorithms: LISTEN-U, which uses the\nLLM to refine a parametric utility function, and LISTEN-T, a non-parametric\nmethod that performs tournament-style selections over small batches of\nsolutions. Evaluated on diverse tasks including flight booking, shopping, and\nexam scheduling, our results show LISTEN-U excels when preferences are\nparametrically aligned (a property we measure with a novel concordance metric),\nwhile LISTEN-T offers more robust performance. This work explores a promising\ndirection for steering complex multi-objective decisions directly with natural\nlanguage, reducing the cognitive burden of traditional preference elicitation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LISTEN\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u96f6\u6837\u672c\u504f\u597d\u9884\u6d4b\u5668\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u6765\u4f18\u5316\u591a\u76ee\u6807\u51b3\u7b56\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u7edf\u504f\u597d\u83b7\u53d6\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u4eba\u7c7b\u4e13\u5bb6\u5728\u9762\u5bf9\u591a\u76ee\u6807\u7ade\u4e89\u7684\u5927\u89c4\u6a21\u9009\u9879\u9009\u62e9\u65f6\u5e38\u5e38\u96be\u4ee5\u5f62\u5f0f\u5316\u590d\u6742\u7684\u9690\u542b\u504f\u597d\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u53d7\u5230\u504f\u597d\u83b7\u53d6\u56f0\u96be\u7684\u74f6\u9888\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u5229\u7528\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u6765\u7b80\u5316\u590d\u6742\u51b3\u7b56\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86LISTEN\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u8fed\u4ee3\u7b97\u6cd5\uff1aLISTEN-U\u4f7f\u7528LLM\u4f18\u5316\u53c2\u6570\u5316\u6548\u7528\u51fd\u6570\uff0cLISTEN-T\u91c7\u7528\u975e\u53c2\u6570\u65b9\u6cd5\u5728\u5c0f\u6279\u91cf\u89e3\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9526\u6807\u8d5b\u5f0f\u9009\u62e9\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u65e8\u5728\u514b\u670dLLM\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u63a8\u7406\u6210\u672c\u9650\u5236\u3002", "result": "\u5728\u822a\u73ed\u9884\u8ba2\u3001\u8d2d\u7269\u548c\u8003\u8bd5\u5b89\u6392\u7b49\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5f53\u504f\u597d\u4e0e\u53c2\u6570\u5316\u5bf9\u9f50\u65f6LISTEN-U\u8868\u73b0\u4f18\u5f02\uff0c\u800cLISTEN-T\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u4e00\u81f4\u6027\u5ea6\u91cf\u6765\u91cf\u5316\u504f\u597d\u5bf9\u9f50\u7a0b\u5ea6\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a2\u7d22\u4e86\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u76f4\u63a5\u6307\u5bfc\u590d\u6742\u591a\u76ee\u6807\u51b3\u7b56\u7684\u53ef\u884c\u65b9\u5411\uff0c\u4e3a\u51cf\u5c11\u4f20\u7edf\u504f\u597d\u83b7\u53d6\u7684\u8ba4\u77e5\u8d1f\u62c5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3aLLM\u5728\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.26105", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.26105", "abs": "https://arxiv.org/abs/2510.26105", "authors": ["Xiaosen Wang", "Zhijin Ge", "Shaokang Wang"], "title": "Security Risk of Misalignment between Text and Image in Multi-modal Model", "comment": null, "summary": "Despite the notable advancements and versatility of multi-modal diffusion\nmodels, such as text-to-image models, their susceptibility to adversarial\ninputs remains underexplored. Contrary to expectations, our investigations\nreveal that the alignment between textual and Image modalities in existing\ndiffusion models is inadequate. This misalignment presents significant risks,\nespecially in the generation of inappropriate or Not-Safe-For-Work (NSFW)\ncontent. To this end, we propose a novel attack called Prompt-Restricted\nMulti-modal Attack (PReMA) to manipulate the generated content by modifying the\ninput image in conjunction with any specified prompt, without altering the\nprompt itself. PReMA is the first attack that manipulates model outputs by\nsolely creating adversarial images, distinguishing itself from prior methods\nthat primarily generate adversarial prompts to produce NSFW content.\nConsequently, PReMA poses a novel threat to the integrity of multi-modal\ndiffusion models, particularly in image-editing applications that operate with\nfixed prompts. Comprehensive evaluations conducted on image inpainting and\nstyle transfer tasks across various models confirm the potent efficacy of\nPReMA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PReMA\u653b\u51fb\u65b9\u6cd5\uff0c\u9996\u6b21\u901a\u8fc7\u4ec5\u521b\u5efa\u5bf9\u6297\u6027\u56fe\u50cf\u6765\u64cd\u7eb5\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u4e2d\u6587\u672c\u4e0e\u56fe\u50cf\u6a21\u6001\u5bf9\u9f50\u4e0d\u8db3\u7684\u5b89\u5168\u98ce\u9669\u3002\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u4fee\u590d\u548c\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6548\u529b\uff0c\u5bf9\u56fa\u5b9a\u63d0\u793a\u8bcd\u7684\u5e94\u7528\u573a\u666f\u6784\u6210\u65b0\u578b\u5a01\u80c1\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5bf9\u5bf9\u6297\u6027\u8f93\u5165\u7684\u8106\u5f31\u6027\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u6269\u6563\u6a21\u578b\u4e2d\u6587\u672c\u4e0e\u56fe\u50cf\u6a21\u6001\u7684\u5bf9\u9f50\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u79cd\u9519\u4f4d\u5728\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\u65f6\u5e26\u6765\u663e\u8457\u98ce\u9669\uff0c\u7279\u522b\u662f\u5728NSFW\u5185\u5bb9\u751f\u6210\u65b9\u9762\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u63d0\u51fa\u4e86Prompt-Restricted Multi-modal Attack (PReMA)\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539\u8f93\u5165\u56fe\u50cf\u6765\u64cd\u7eb5\u751f\u6210\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u63d0\u793a\u8bcd\u4e0d\u53d8\u3002\u8fd9\u662f\u9996\u4e2a\u4ec5\u901a\u8fc7\u521b\u5efa\u5bf9\u6297\u6027\u56fe\u50cf\u6765\u64cd\u7eb5\u6a21\u578b\u8f93\u51fa\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u533a\u522b\u4e8e\u5148\u524d\u4e3b\u8981\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\u8bcd\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u56fe\u50cf\u4fee\u590d\u548c\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e0a\u5bf9\u591a\u79cd\u6a21\u578b\u8fdb\u884c\u7684\u5168\u9762\u8bc4\u4f30\u8bc1\u5b9e\u4e86PReMA\u7684\u5f3a\u5927\u6548\u529b\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u64cd\u7eb5\u6a21\u578b\u8f93\u51fa\uff0c\u7279\u522b\u662f\u5728\u56fa\u5b9a\u63d0\u793a\u8bcd\u7684\u56fe\u50cf\u7f16\u8f91\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u5a01\u80c1\u3002", "conclusion": "PReMA\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\u5728\u6a21\u6001\u5bf9\u9f50\u65b9\u9762\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5bf9\u56fe\u50cf\u7f16\u8f91\u5e94\u7528\u7684\u5b8c\u6574\u6027\u6784\u6210\u65b0\u578b\u5a01\u80c1\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u9700\u8981\u52a0\u5f3a\u591a\u6a21\u6001\u6a21\u578b\u5b89\u5168\u6027\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u56fa\u5b9a\u63d0\u793a\u8bcd\u64cd\u4f5c\u573a\u666f\u4e0b\u7684\u9632\u62a4\u63aa\u65bd\u3002"}}
{"id": "2510.26271", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26271", "abs": "https://arxiv.org/abs/2510.26271", "authors": ["Sukrit Sriratanawilai", "Jhayahgrit Thongwat", "Romrawin Chumpu", "Patomporn Payoungkhamdee", "Sarana Nutanong", "Peerat Limkonchotiwat"], "title": "Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual", "comment": "Work in progress", "summary": "Vision-language models (VLMs) exhibit uneven performance across languages, a\nproblem that is often exacerbated when the model size is reduced. While\nKnowledge distillation (KD) demonstrates promising results in transferring\nknowledge from larger to smaller VLMs, applying KD in multilingualism is an\nunderexplored area. This paper presents a controlled empirical study of KD\nbehavior across five distillation approaches, isolating their effects on\ncross-lingual representation consistency and downstream performance stability\nunder model compression. We study five distillation formulations across CLIP\nand SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual\nQA. We find that some configurations preserve or even improve multilingual\nretrieval robustness despite halving model size, but others fail to maintain\ncross-task stability, exposing design-sensitive trade-offs that aggregate\naccuracy alone does not reveal.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u5206\u6790\u4e86\u77e5\u8bc6\u84b8\u998f\u5728\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u4e2d\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u4e0d\u540c\u84b8\u998f\u914d\u7f6e\u5728\u8de8\u8bed\u8a00\u8868\u793a\u4e00\u81f4\u6027\u548c\u4e0b\u6e38\u6027\u80fd\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u4ec5\u9760\u805a\u5408\u51c6\u786e\u7387\u65e0\u6cd5\u53cd\u6620\u7684\u8bbe\u8ba1\u654f\u611f\u6743\u8861\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u6027\u80fd\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u5c3a\u5bf8\u51cf\u5c0f\u65f6\u66f4\u4e3a\u4e25\u91cd\uff0c\u800c\u77e5\u8bc6\u84b8\u998f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5e94\u7528\u4ecd\u662f\u4e00\u4e2a\u63a2\u7d22\u4e0d\u8db3\u7684\u9886\u57df\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u84b8\u998f\u65b9\u6cd5\u5bf9\u8de8\u8bed\u8a00\u8868\u793a\u4e00\u81f4\u6027\u548c\u6a21\u578b\u538b\u7f29\u4e0b\u6027\u80fd\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e94\u79cd\u4e0d\u540c\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u5728CLIP\u548cSigLIP2\u6a21\u578b\u4e0a\u8fdb\u884c\u63a7\u5236\u6027\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u5728\u57df\u5185\u68c0\u7d22\u548c\u57df\u5916\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u8de8\u8bed\u8a00\u8868\u793a\u4e00\u81f4\u6027\u548c\u4e0b\u6e38\u6027\u80fd\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u67d0\u4e9b\u84b8\u998f\u914d\u7f6e\u80fd\u591f\u5728\u6a21\u578b\u5c3a\u5bf8\u51cf\u534a\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u591a\u8bed\u8a00\u68c0\u7d22\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5176\u4ed6\u914d\u7f6e\u65e0\u6cd5\u7ef4\u6301\u8de8\u4efb\u52a1\u7a33\u5b9a\u6027\uff0c\u63ed\u793a\u4e86\u4ec5\u9760\u805a\u5408\u51c6\u786e\u7387\u65e0\u6cd5\u6355\u6349\u7684\u8bbe\u8ba1\u654f\u611f\u6743\u8861\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u9009\u62e9\u5bf9\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u6548\u679c\u7684\u5173\u952e\u5f71\u54cd\uff0c\u6307\u51fa\u9700\u8981\u7efc\u5408\u8003\u8651\u8de8\u8bed\u8a00\u8868\u793a\u4e00\u81f4\u6027\u548c\u4efb\u52a1\u7a33\u5b9a\u6027\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u603b\u4f53\u51c6\u786e\u7387\uff0c\u4e3a\u591a\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u91cd\u8981\u8bbe\u8ba1\u6307\u5bfc\u3002"}}
{"id": "2510.25897", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25897", "abs": "https://arxiv.org/abs/2510.25897", "authors": ["Nicolas Dufour", "Lucas Degeorge", "Arijit Ghosh", "Vicky Kalogeiton", "David Picard"], "title": "MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency", "comment": "Project page: https://nicolas-dufour.github.io/miro", "summary": "Current text-to-image generative models are trained on large uncurated\ndatasets to enable diverse generation capabilities. However, this does not\nalign well with user preferences. Recently, reward models have been\nspecifically designed to perform post-hoc selection of generated images and\nalign them to a reward, typically user preference. This discarding of\ninformative data together with the optimizing for a single reward tend to harm\ndiversity, semantic fidelity and efficiency. Instead of this post-processing,\nwe propose to condition the model on multiple reward models during training to\nlet the model learn user preferences directly. We show that this not only\ndramatically improves the visual quality of the generated images but it also\nsignificantly speeds up the training. Our proposed method, called MIRO,\nachieves state-of-the-art performances on the GenEval compositional benchmark\nand user-preference scores (PickAScore, ImageReward, HPSv2).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MIRO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u5bf9\u591a\u4e2a\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u4f7f\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u5e76\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u5728GenEval\u7ec4\u5408\u57fa\u51c6\u548c\u7528\u6237\u504f\u597d\u8bc4\u5206\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u5927\u578b\u672a\u7b5b\u9009\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4ee5\u5b9e\u73b0\u591a\u6837\u5316\u751f\u6210\u80fd\u529b\uff0c\u4f46\u8fd9\u4e0e\u7528\u6237\u504f\u597d\u5e76\u4e0d\u4e00\u81f4\u3002\u73b0\u6709\u7684\u5956\u52b1\u6a21\u578b\u65b9\u6cd5\u901a\u8fc7\u540e\u5904\u7406\u9009\u62e9\u751f\u6210\u56fe\u50cf\u6765\u5bf9\u9f50\u5956\u52b1\uff0c\u4f46\u4f1a\u4e22\u5f03\u4fe1\u606f\u6570\u636e\u5e76\u4f18\u5316\u5355\u4e00\u5956\u52b1\uff0c\u4ece\u800c\u635f\u5bb3\u591a\u6837\u6027\u3001\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u7684MIRO\u65b9\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u5bf9\u591a\u4e2a\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u800c\u4e0d\u662f\u91c7\u7528\u540e\u5904\u7406\u65b9\u5f0f\u3002\u8fd9\u79cd\u65b9\u6cd5\u907f\u514d\u4e86\u4fe1\u606f\u6570\u636e\u7684\u4e22\u5f03\uff0c\u5e76\u652f\u6301\u591a\u5956\u52b1\u4f18\u5316\u3002", "result": "MIRO\u65b9\u6cd5\u4e0d\u4ec5\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u8fd8\u5927\u5e45\u52a0\u5feb\u4e86\u8bad\u7ec3\u901f\u5ea6\u3002\u5728GenEval\u7ec4\u5408\u57fa\u51c6\u548c\u7528\u6237\u504f\u597d\u8bc4\u5206\uff08PickAScore\u3001ImageReward\u3001HPSv2\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u96c6\u6210\u591a\u5956\u52b1\u6761\u4ef6\u5316\u6bd4\u540e\u5904\u7406\u9009\u62e9\u66f4\u6709\u6548\uff0c\u80fd\u591f\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u6548\u7387\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u76f4\u63a5\u5b66\u4e60\u7528\u6237\u504f\u597d\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.26113", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26113", "abs": "https://arxiv.org/abs/2510.26113", "authors": ["Minjoon Jung", "Junbin Xiao", "Junghyun Kim", "Byoung-Tak Zhang", "Angela Yao"], "title": "EgoExo-Con: Exploring View-Invariant Video Temporal Understanding", "comment": "project page:\n  \\url{https://minjoong507.github.io/projects/EgoExo-Con/}", "summary": "Can Video-LLMs achieve consistent temporal understanding when videos capture\nthe same event from different viewpoints? To study this, we introduce\nEgoExo-Con (Consistency), a benchmark of comprehensively synchronized\negocentric and exocentric video pairs with human-refined queries in natural\nlanguage. EgoExo-Con emphasizes two temporal understanding tasks: Temporal\nVerification and Temporal Grounding. It evaluates not only correctness but\nconsistency across viewpoints. Our analysis reveals two critical limitations of\nexisting Video-LLMs: (1) models often fail to maintain consistency, with\nresults far worse than their single-view performances. (2) When naively\nfinetuned with synchronized videos of both viewpoints, the models show improved\nconsistency but often underperform those trained on a single view. For\nimprovements, we propose View-GRPO, a novel reinforcement learning framework\nthat effectively strengthens view-specific temporal reasoning while encouraging\nconsistent comprehension across viewpoints. Our method demonstrates its\nsuperiority over naive SFT and GRPO, especially for improving cross-view\nconsistency. All resources will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EgoExo-Con\u57fa\u51c6\u6765\u8bc4\u4f30\u89c6\u9891-\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u89c6\u89d2\u89c6\u9891\u4e2d\u7684\u65f6\u5e8f\u7406\u89e3\u4e00\u81f4\u6027\uff0c\u5e76\u5f00\u53d1\u4e86View-GRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u63d0\u5347\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u800c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6539\u5584\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891-\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ece\u4e0d\u540c\u89c6\u89d2\u6355\u6349\u540c\u4e00\u4e8b\u4ef6\u7684\u89c6\u9891\u4e2d\u662f\u5426\u80fd\u591f\u5b9e\u73b0\u4e00\u81f4\u7684\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6a21\u578b\u5728\u591a\u89c6\u89d2\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u4e00\u81f4\u6027\u7f3a\u9677\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u89c6\u9891\u4ece\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u548c\u5916\u90e8\u89c6\u89d2\u540c\u6b65\u8bb0\u5f55\u540c\u4e00\u4e8b\u4ef6\u65f6\uff0c\u6a21\u578b\u9700\u8981\u4fdd\u6301\u8de8\u89c6\u89d2\u7684\u65f6\u5e8f\u63a8\u7406\u4e00\u81f4\u6027\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86EgoExo-Con\u57fa\u51c6\uff0c\u5305\u542b\u5168\u9762\u540c\u6b65\u7684\u81ea\u6211\u4e2d\u5fc3\u4e0e\u5916\u90e8\u4e2d\u5fc3\u89c6\u9891\u5bf9\u53ca\u4eba\u5de5\u7cbe\u70bc\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u91cd\u70b9\u8bc4\u4f30\u65f6\u5e8f\u9a8c\u8bc1\u548c\u65f6\u5e8f\u5b9a\u4f4d\u4e24\u4e2a\u4efb\u52a1\u3002\u63d0\u51fa\u4e86View-GRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6709\u6548\u52a0\u5f3a\u4e86\u89c6\u89d2\u7279\u5b9a\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fc3\u8fdb\u4e86\u8de8\u89c6\u89d2\u7684\u4e00\u81f4\u6027\u7406\u89e3\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u73b0\u6709\u89c6\u9891-\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e24\u4e2a\u5173\u952e\u5c40\u9650\u6027\uff1a\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u5176\u8868\u73b0\u8fdc\u4f4e\u4e8e\u5355\u89c6\u89d2\u6027\u80fd\uff1b\u5f53\u4f7f\u7528\u53cc\u89c6\u89d2\u540c\u6b65\u89c6\u9891\u8fdb\u884c\u7b80\u5355\u5fae\u8c03\u65f6\uff0c\u6a21\u578b\u867d\u7136\u4e00\u81f4\u6027\u6709\u6240\u6539\u5584\uff0c\u4f46\u901a\u5e38\u8868\u73b0\u4e0d\u5982\u5355\u89c6\u89d2\u8bad\u7ec3\u7684\u6a21\u578b\u3002View-GRPO\u65b9\u6cd5\u5728\u6539\u5584\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u6734\u7d20SFT\u548cGRPO\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u8de8\u89c6\u89d2\u65f6\u5e8f\u7406\u89e3\u4e00\u81f4\u6027\u662f\u89c6\u9891-\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u8981\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002View-GRPO\u6846\u67b6\u4e3a\u89e3\u51b3\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5f3a\u8c03\u4e86\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u8003\u8651\u89c6\u89d2\u5dee\u5f02\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6a21\u578b\u7684\u4e00\u81f4\u6027\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.26345", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26345", "abs": "https://arxiv.org/abs/2510.26345", "authors": ["Mykhailo Poliakov", "Nadiya Shvai"], "title": "MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data", "comment": null, "summary": "Health-related misinformation is very prevalent and potentially harmful. It\nis difficult to identify, especially when claims distort or misinterpret\nscientific findings. We investigate the impact of synthetic data generation and\nlightweight fine-tuning techniques on the ability of large language models\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\nframework. In this work, we propose MisSynth, a pipeline that applies\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\nwhich are then used to fine-tune an LLM model. Our results show substantial\naccuracy gains with fine-tuned models compared to vanilla baselines. For\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\ndemonstrate that introducing synthetic fallacy data to augment limited\nannotated resources can significantly enhance zero-shot LLM classification\nperformance on real-world scientific misinformation tasks, even with limited\ncomputational resources. The code and synthetic dataset are available on\nhttps://github.com/mxpoliakov/MisSynth.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMisSynth\u7ba1\u9053\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5408\u6210\u8c2c\u8bef\u6837\u672c\u5e76\u5fae\u8c03LLM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u79d1\u5b66\u5065\u5eb7\u76f8\u5173\u9519\u8bef\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u4e86\u8d85\u8fc735%\u7684F1\u5206\u6570\u7edd\u5bf9\u63d0\u5347\u3002", "motivation": "\u5065\u5eb7\u76f8\u5173\u9519\u8bef\u4fe1\u606f\u666e\u904d\u5b58\u5728\u4e14\u5177\u6709\u6f5c\u5728\u5371\u5bb3\u6027\uff0c\u7279\u522b\u662f\u5f53\u8fd9\u4e9b\u58f0\u660e\u626d\u66f2\u6216\u66f2\u89e3\u79d1\u5b66\u53d1\u73b0\u65f6\u96be\u4ee5\u8bc6\u522b\u3002\u73b0\u6709\u6807\u6ce8\u8d44\u6e90\u6709\u9650\uff0c\u9700\u8981\u63a2\u7d22\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u60c5\u51b5\u4e0b\u63d0\u5347LLM\u8bc6\u522b\u8c2c\u8bef\u8bba\u8bc1\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMisSynth\u7ba1\u9053\uff0c\u5e94\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u751f\u6210\u5408\u6210\u8c2c\u8bef\u6837\u672c\uff0c\u7136\u540e\u4f7f\u7528\u8fd9\u4e9b\u6837\u672c\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8eMISSCI\u6570\u636e\u96c6\u548c\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\u6709\u9650\u7684\u6807\u6ce8\u8d44\u6e90\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u76f8\u6bd4\u539f\u59cb\u57fa\u7ebf\u6a21\u578b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5176\u4e2dLLaMA 3.1 8B\u5fae\u8c03\u6a21\u578b\u5728MISSCI\u6d4b\u8bd5\u96c6\u4e0a\u76f8\u6bd4\u539f\u59cb\u57fa\u7ebf\u5b9e\u73b0\u4e86\u8d85\u8fc735%\u7684F1\u5206\u6570\u7edd\u5bf9\u63d0\u5347\u3002\u5b9e\u9a8c\u8bc1\u660e\u5408\u6210\u8c2c\u8bef\u6570\u636e\u80fd\u591f\u663e\u8457\u589e\u5f3a\u96f6\u6837\u672cLLM\u5728\u771f\u5b9e\u4e16\u754c\u79d1\u5b66\u9519\u8bef\u4fe1\u606f\u4efb\u52a1\u4e0a\u7684\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f15\u5165\u5408\u6210\u8c2c\u8bef\u6570\u636e\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u6709\u9650\u6807\u6ce8\u8d44\u6e90\u4e0b\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u663e\u8457\u63d0\u5347LLM\u8bc6\u522b\u79d1\u5b66\u9519\u8bef\u4fe1\u606f\u7684\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u4e3a\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u5408\u6210\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.25921", "categories": ["cs.CV", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.25921", "abs": "https://arxiv.org/abs/2510.25921", "authors": ["Nikola L. Kolev", "Tommaso Rodani", "Neil J. Curson", "Taylor J. Z. Stock", "Alberto Cazzaniga"], "title": "Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy", "comment": null, "summary": "Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and\natom manipulation, but its utility is often limited by tip degradation and slow\nserial data acquisition. Fabrication adds another layer of complexity since the\ntip is often subjected to large voltages, which may alter the shape of its\napex, requiring it to be conditioned. Here, we propose a machine learning (ML)\napproach for image repair and super-resolution to alleviate both challenges.\nUsing a dataset of only 36 pristine experimental images of Si(001):H, we\ndemonstrate that a physics-informed synthetic data generation pipeline can be\nused to train several state-of-the-art flow-matching and diffusion models.\nQuantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy\n(CMMD) score and structural similarity demonstrates that our models are able to\neffectively restore images and offer a two- to fourfold reduction in image\nacquisition time by accurately reconstructing images from sparsely sampled\ndata. Our framework has the potential to significantly increase STM\nexperimental throughput by offering a route to reducing the frequency of\ntip-conditioning procedures and to enhancing frame rates in existing high-speed\nSTM systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u626b\u63cf\u96a7\u9053\u663e\u5fae\u955c\u56fe\u50cf\u4fee\u590d\u4e0e\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\u8bad\u7ec3\u5148\u8fdb\u7684\u6d41\u5339\u914d\u548c\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u4fee\u590d\u56fe\u50cf\u8d28\u91cf\u5e76\u5b9e\u73b02-4\u500d\u7684\u56fe\u50cf\u91c7\u96c6\u65f6\u95f4\u51cf\u5c11\u3002", "motivation": "\u626b\u63cf\u96a7\u9053\u663e\u5fae\u955c\u5728\u539f\u5b50\u5206\u8fa8\u7387\u6210\u50cf\u548c\u539f\u5b50\u64cd\u7eb5\u65b9\u9762\u5177\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u5176\u5b9e\u9645\u6548\u7528\u5e38\u53d7\u9650\u4e8e\u9488\u5c16\u9000\u5316\u548c\u7f13\u6162\u7684\u4e32\u884c\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\uff0c\u540c\u65f6\u9488\u5c16\u5236\u5907\u8fc7\u7a0b\u4e2d\u65bd\u52a0\u7684\u9ad8\u7535\u538b\u4f1a\u6539\u53d8\u9488\u5c16\u5c16\u7aef\u5f62\u72b6\uff0c\u9700\u8981\u9891\u7e41\u8fdb\u884c\u9488\u5c16\u8c03\u8282\u5904\u7406\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u4ec5\u4f7f\u752836\u5f20\u539f\u59cb\u5b9e\u9a8c\u56fe\u50cf\u4f5c\u4e3a\u57fa\u7840\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e86\u591a\u79cd\u5148\u8fdb\u7684\u6d41\u5339\u914d\u548c\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7CLIP\u6700\u5927\u5747\u503c\u5dee\u5f02\u5f97\u5206\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u7b49\u5b9a\u91cf\u6307\u6807\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u4fee\u590d\u56fe\u50cf\u8d28\u91cf\uff0c\u901a\u8fc7\u4ece\u7a00\u758f\u91c7\u6837\u6570\u636e\u4e2d\u51c6\u786e\u91cd\u5efa\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e862-4\u500d\u7684\u56fe\u50cf\u91c7\u96c6\u65f6\u95f4\u51cf\u5c11\uff0c\u663e\u8457\u63d0\u5347\u4e86\u626b\u63cf\u96a7\u9053\u663e\u5fae\u955c\u7684\u5b9e\u9a8c\u901a\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u51cf\u5c11\u9488\u5c16\u8c03\u8282\u8fc7\u7a0b\u7684\u9891\u7387\u548c\u589e\u5f3a\u73b0\u6709\u9ad8\u901fSTM\u7cfb\u7edf\u7684\u5e27\u7387\uff0c\u6709\u671b\u663e\u8457\u63d0\u9ad8\u626b\u63cf\u96a7\u9053\u663e\u5fae\u955c\u7684\u5b9e\u9a8c\u901a\u91cf\uff0c\u4e3a\u539f\u5b50\u5c3a\u5ea6\u6210\u50cf\u548c\u64cd\u7eb5\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26125", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26125", "abs": "https://arxiv.org/abs/2510.26125", "authors": ["Runsheng Xu", "Hubert Lin", "Wonseok Jeon", "Hao Feng", "Yuliang Zou", "Liting Sun", "John Gorman", "Kate Tolstaya", "Sarah Tang", "Brandyn White", "Ben Sapp", "Mingxing Tan", "Jyh-Jing Hwang", "Drago Anguelov"], "title": "WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios", "comment": null, "summary": "Vision-based end-to-end (E2E) driving has garnered significant interest in\nthe research community due to its scalability and synergy with multimodal large\nlanguage models (MLLMs). However, current E2E driving benchmarks primarily\nfeature nominal scenarios, failing to adequately test the true potential of\nthese systems. Furthermore, existing open-loop evaluation metrics often fall\nshort in capturing the multi-modal nature of driving or effectively evaluating\nperformance in long-tail scenarios. To address these gaps, we introduce the\nWaymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021\ndriving segments (approximately 12 hours), specifically curated for challenging\nlong-tail scenarios that that are rare in daily life with an occurring\nfrequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the\nhigh-level routing information, ego states, and 360-degree camera views from 8\nsurrounding cameras. To evaluate the E2E driving performance on these long-tail\nsituations, we propose a novel open-loop evaluation metric: Rater Feedback\nScore (RFS). Unlike conventional metrics that measure the distance between\npredicted way points and the logs, RFS measures how closely the predicted\ntrajectory matches rater-annotated trajectory preference labels. We have\nreleased rater preference labels for all WOD-E2E validation set segments, while\nthe held out test set labels have been used for the 2025 WOD-E2E Challenge.\nThrough our work, we aim to foster state of the art research into\ngeneralizable, robust, and safe end-to-end autonomous driving agents capable of\nhandling complex real-world situations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86WOD-E2E\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7f55\u89c1\u7684\u957f\u5c3e\u573a\u666f\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u4eba\u7c7b\u8bc4\u5206\u8005\u504f\u597d\u7684\u65b0\u578b\u5f00\u73af\u8bc4\u4f30\u6307\u6807RFS\uff0c\u65e8\u5728\u63a8\u52a8\u7aef\u5230\u7aef\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u9a7e\u9a76\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5e38\u89c4\u573a\u666f\uff0c\u65e0\u6cd5\u5145\u5206\u6d4b\u8bd5\u7cfb\u7edf\u5728\u7f55\u89c1\u957f\u5c3e\u573a\u666f\u4e2d\u7684\u771f\u5b9e\u6f5c\u529b\uff0c\u4e14\u73b0\u6709\u5f00\u73af\u8bc4\u4f30\u6307\u6807\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\u9a7e\u9a76\u7684\u591a\u6a21\u6001\u7279\u6027\u6216\u5728\u957f\u5c3e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b4,021\u4e2a\u9a7e\u9a76\u7247\u6bb5\uff08\u7ea612\u5c0f\u65f6\uff09\u7684WOD-E2E\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u9488\u5bf9\u53d1\u751f\u9891\u7387\u4f4e\u4e8e0.03%\u7684\u6311\u6218\u6027\u957f\u5c3e\u573a\u666f\uff0c\u6bcf\u4e2a\u7247\u6bb5\u5305\u542b\u9ad8\u7ea7\u8def\u7531\u4fe1\u606f\u3001\u81ea\u8f66\u72b6\u6001\u548c8\u4e2a\u73af\u89c6\u6444\u50cf\u5934\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bc4\u5206\u8005\u8f68\u8ff9\u504f\u597d\u6807\u6ce8\u7684\u65b0\u578b\u8bc4\u4f30\u6307\u6807RFS\u3002", "result": "WOD-E2E\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53d1\u5e03\u9a8c\u8bc1\u96c6\u7684\u8bc4\u5206\u8005\u504f\u597d\u6807\u7b7e\uff0c\u6d4b\u8bd5\u96c6\u6807\u7b7e\u7528\u4e8e2025\u5e74WOD-E2E\u6311\u6218\u8d5b\uff0c\u8be5\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e3a\u7aef\u5230\u7aef\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u957f\u5c3e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e13\u95e8\u7684\u957f\u5c3e\u573a\u666f\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4e3a\u5f00\u53d1\u901a\u7528\u6027\u5f3a\u3001\u9c81\u68d2\u6027\u9ad8\u4e14\u5b89\u5168\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u5c06\u63a8\u52a8\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2510.26521", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26521", "abs": "https://arxiv.org/abs/2510.26521", "authors": ["Yair Elboher", "Yuval Pinter"], "title": "Hebrew Diacritics Restoration using Visual Representation", "comment": null, "summary": "Diacritics restoration in Hebrew is a fundamental task for ensuring accurate\nword pronunciation and disambiguating textual meaning. Despite the language's\nhigh degree of ambiguity when unvocalized, recent machine learning approaches\nhave significantly advanced performance on this task.\n  In this work, we present DIVRIT, a novel system for Hebrew diacritization\nthat frames the task as a zero-shot classification problem. Our approach\noperates at the word level, selecting the most appropriate diacritization\npattern for each undiacritized word from a dynamically generated candidate set,\nconditioned on the surrounding textual context. A key innovation of DIVRIT is\nits use of a Hebrew Visual Language Model, which processes undiacritized text\nas an image, allowing diacritic information to be embedded directly within the\ninput's vector representation.\n  Through a comprehensive evaluation across various configurations, we\ndemonstrate that the system effectively performs diacritization without relying\non complex, explicit linguistic analysis. Notably, in an ``oracle'' setting\nwhere the correct diacritized form is guaranteed to be among the provided\ncandidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic\narchitectural enhancements and optimized training methodologies yield\nsignificant improvements in the system's overall generalization capabilities.\nThese findings highlight the promising potential of visual representations for\naccurate and automated Hebrew diacritization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDIVRIT\u7cfb\u7edf\uff0c\u5c06\u5e0c\u4f2f\u6765\u8bed\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u4efb\u52a1\u6784\u5efa\u4e3a\u96f6\u6837\u672c\u5206\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u672a\u53d8\u97f3\u6587\u672c\u4f5c\u4e3a\u56fe\u50cf\u5904\u7406\uff0c\u5728\u5019\u9009\u96c6\u5305\u542b\u6b63\u786e\u5f62\u5f0f\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u53d8\u97f3\u6062\u590d\u3002", "motivation": "\u5e0c\u4f2f\u6765\u8bed\u53d8\u97f3\u7b26\u53f7\u6062\u590d\u662f\u786e\u4fdd\u51c6\u786e\u53d1\u97f3\u548c\u6d88\u9664\u6587\u672c\u6b67\u4e49\u7684\u5173\u952e\u4efb\u52a1\uff0c\u5c3d\u7ba1\u672a\u53d8\u97f3\u6587\u672c\u5b58\u5728\u9ad8\u5ea6\u6b67\u4e49\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u4ecd\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u8fd9\u4e00\u590d\u6742\u7684\u8bed\u8a00\u5904\u7406\u95ee\u9898\u3002", "method": "DIVRIT\u7cfb\u7edf\u5c06\u53d8\u97f3\u6062\u590d\u6784\u5efa\u4e3a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u6761\u4ef6\u7684\u96f6\u6837\u672c\u5206\u7c7b\u95ee\u9898\uff0c\u4f7f\u7528\u5e0c\u4f2f\u6765\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u672a\u53d8\u97f3\u6587\u672c\u4f5c\u4e3a\u56fe\u50cf\u5904\u7406\uff0c\u4ece\u52a8\u6001\u751f\u6210\u7684\u5019\u9009\u96c6\u4e2d\u4e3a\u6bcf\u4e2a\u5355\u8bcd\u9009\u62e9\u6700\u5408\u9002\u7684\u53d8\u97f3\u6a21\u5f0f\u3002", "result": "\u5728\u5168\u9762\u8bc4\u4f30\u4e2d\uff0c\u7cfb\u7edf\u5728\u4e0d\u4f9d\u8d56\u590d\u6742\u663e\u5f0f\u8bed\u8a00\u5206\u6790\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u6267\u884c\u53d8\u97f3\u6062\u590d\uff0c\u5728\u6b63\u786e\u53d8\u97f3\u5f62\u5f0f\u4fdd\u8bc1\u5b58\u5728\u4e8e\u5019\u9009\u96c6\u7684oracle\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u9ad8\u51c6\u786e\u7387\uff0c\u67b6\u6784\u4f18\u5316\u548c\u8bad\u7ec3\u65b9\u6cd5\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u89c6\u89c9\u8868\u793a\u5728\u5e0c\u4f2f\u6765\u8bed\u81ea\u52a8\u53d8\u97f3\u6062\u590d\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u590d\u6742\u7684\u8bed\u8a00\u5206\u6790\u4f9d\u8d56\uff0c\u4e3a\u57fa\u4e8e\u89c6\u89c9\u7684\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u548c\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2510.25970", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25970", "abs": "https://arxiv.org/abs/2510.25970", "authors": ["Sung-Hoon Yoon", "Minghan Li", "Gaspard Beaudouin", "Congcong Wen", "Muhammad Rafay Azhar", "Mengyu Wang"], "title": "SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing", "comment": "Camera-ready version for NeurIPS 2025, 10 pages (main paper)", "summary": "Rectified flow models have become a de facto standard in image generation due\nto their stable sampling trajectories and high-fidelity outputs. Despite their\nstrong generative capabilities, they face critical limitations in image editing\ntasks: inaccurate inversion processes for mapping real images back into the\nlatent space, and gradient entanglement issues during editing often result in\noutputs that do not faithfully reflect the target prompt. Recent efforts have\nattempted to directly map source and target distributions via ODE-based\napproaches without inversion; however,these methods still yield suboptimal\nediting quality. In this work, we propose a flow decomposition-and-aggregation\nframework built upon an inversion-free formulation to address these\nlimitations. Specifically, we semantically decompose the target prompt into\nmultiple sub-prompts, compute an independent flow for each, and aggregate them\nto form a unified editing trajectory. While we empirically observe that\ndecomposing the original flow enhances diversity in the target space,\ngenerating semantically aligned outputs still requires consistent guidance\ntoward the full target prompt. To this end, we design a projection and\nsoft-aggregation mechanism for flow, inspired by gradient conflict resolution\nin multi-task learning. This approach adaptively weights the sub-target\nvelocity fields, suppressing semantic redundancy while emphasizing distinct\ndirections, thereby preserving both diversity and consistency in the final\nedited output. Experimental results demonstrate that our method outperforms\nexisting zero-shot editing approaches in terms of semantic fidelity and\nattribute disentanglement. The code is available at\nhttps://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5206\u89e3\u4e0e\u805a\u5408\u7684\u514d\u53cd\u6f14\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u89e3\u76ee\u6807\u63d0\u793a\u5e76\u81ea\u9002\u5e94\u805a\u5408\u5b50\u6d41\u6765\u89e3\u51b3\u6574\u6d41\u6d41\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u53cd\u6f14\u4e0d\u51c6\u786e\u548c\u68af\u5ea6\u7ea0\u7f20\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u5c5e\u6027\u89e3\u8026\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u7f16\u8f91\u65b9\u6cd5\u3002", "motivation": "\u6574\u6d41\u6d41\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u5b58\u5728\u5173\u952e\u9650\u5236\uff1a\u5c06\u771f\u5b9e\u56fe\u50cf\u6620\u5c04\u56de\u6f5c\u5728\u7a7a\u95f4\u7684\u53cd\u6f14\u8fc7\u7a0b\u4e0d\u51c6\u786e\uff0c\u4ee5\u53ca\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u68af\u5ea6\u7ea0\u7f20\u95ee\u9898\u5bfc\u81f4\u8f93\u51fa\u65e0\u6cd5\u5fe0\u5b9e\u53cd\u6620\u76ee\u6807\u63d0\u793a\u3002\u73b0\u6709\u57fa\u4e8eODE\u7684\u65b9\u6cd5\u867d\u7136\u5c1d\u8bd5\u7ed5\u8fc7\u53cd\u6f14\u76f4\u63a5\u6620\u5c04\u6e90\u548c\u76ee\u6807\u5206\u5e03\uff0c\u4f46\u4ecd\u4ea7\u751f\u6b21\u4f18\u7684\u7f16\u8f91\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u514d\u53cd\u6f14\u516c\u5f0f\u7684\u6d41\u5206\u89e3\u4e0e\u805a\u5408\u6846\u67b6\uff0c\u5c06\u76ee\u6807\u63d0\u793a\u8bed\u4e49\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u63d0\u793a\uff0c\u4e3a\u6bcf\u4e2a\u5b50\u63d0\u793a\u8ba1\u7b97\u72ec\u7acb\u6d41\uff0c\u5e76\u901a\u8fc7\u6295\u5f71\u548c\u8f6f\u805a\u5408\u673a\u5236\u81ea\u9002\u5e94\u52a0\u6743\u5b50\u76ee\u6807\u901f\u5ea6\u573a\u3002\u8be5\u673a\u5236\u53d7\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u68af\u5ea6\u51b2\u7a81\u89e3\u51b3\u7684\u542f\u53d1\uff0c\u6291\u5236\u8bed\u4e49\u5197\u4f59\u540c\u65f6\u5f3a\u8c03\u4e0d\u540c\u65b9\u5411\uff0c\u4fdd\u6301\u7f16\u8f91\u8f93\u51fa\u7684\u591a\u6837\u6027\u548c\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u5c5e\u6027\u89e3\u8026\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u96f6\u6837\u672c\u7f16\u8f91\u65b9\u6cd5\u3002\u6d41\u5206\u89e3\u589e\u5f3a\u4e86\u76ee\u6807\u7a7a\u95f4\u7684\u591a\u6837\u6027\uff0c\u800c\u8f6f\u805a\u5408\u673a\u5236\u786e\u4fdd\u751f\u6210\u8bed\u4e49\u5bf9\u9f50\u7684\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5b8c\u6574\u76ee\u6807\u63d0\u793a\u7684\u4e00\u81f4\u5f15\u5bfc\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u8bed\u4e49\u5206\u89e3\u548c\u81ea\u9002\u5e94\u6d41\u805a\u5408\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u6574\u6d41\u6d41\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6846\u67b6\u8bbe\u8ba1\u7075\u611f\u6765\u81ea\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u8de8\u9886\u57df\u65b9\u6cd5\u5728\u751f\u6210\u6a21\u578b\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7f16\u8f91\u65b9\u6cd5\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.26151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26151", "abs": "https://arxiv.org/abs/2510.26151", "authors": ["Shunjie-Fabian Zheng", "Hyeonjun Lee", "Thijs Kooi", "Ali Diba"], "title": "MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction", "comment": "Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025", "summary": "Large annotated datasets are essential for training robust Computer-Aided\nDiagnosis (CAD) models for breast cancer detection or risk prediction. However,\nacquiring such datasets with fine-detailed annotation is both costly and\ntime-consuming. Vision-Language Models (VLMs), such as CLIP, which are\npre-trained on large image-text pairs, offer a promising solution by enhancing\nrobustness and data efficiency in medical imaging tasks. This paper introduces\na novel Multi-View Mammography and Language Model for breast cancer\nclassification and risk prediction, trained on a dataset of paired mammogram\nimages and synthetic radiology reports. Our MV-MLM leverages multi-view\nsupervision to learn rich representations from extensive radiology data by\nemploying cross-modal self-supervision across image-text pairs. This includes\nmultiple views and the corresponding pseudo-radiology reports. We propose a\nnovel joint visual-textual learning strategy to enhance generalization and\naccuracy performance over different data types and tasks to distinguish breast\ntissues or cancer characteristics(calcification, mass) and utilize these\npatterns to understand mammography images and predict cancer risk. We evaluated\nour method on both private and publicly available datasets, demonstrating that\nthe proposed model achieves state-of-the-art performance in three\nclassification tasks: (1) malignancy classification, (2) subtype\nclassification, and (3) image-based cancer risk prediction. Furthermore, the\nmodel exhibits strong data efficiency, outperforming existing fully supervised\nor VLM baselines while trained on synthetic text reports and without the need\nfor actual radiology reports.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u89c6\u89d2\u4e73\u817aX\u7ebf\u6444\u5f71\u4e0e\u8bed\u8a00\u6a21\u578b\uff08MV-MLM\uff09\uff0c\u901a\u8fc7\u5229\u7528\u914d\u5bf9\u4e73\u817aX\u7ebf\u56fe\u50cf\u548c\u5408\u6210\u653e\u5c04\u5b66\u62a5\u544a\u8fdb\u884c\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5728\u4e73\u817a\u764c\u5206\u7c7b\u548c\u98ce\u9669\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u4f9d\u8d56\u5927\u91cf\u7cbe\u7ec6\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u83b7\u53d6\u6b64\u7c7b\u6570\u636e\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\uff0c\u800c\u57fa\u4e8e\u5927\u89c4\u6a21\u56fe\u50cf-\u6587\u672c\u5bf9\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3a\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u6548\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u591a\u89c6\u89d2\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u5728\u56fe\u50cf-\u6587\u672c\u5bf9\u4e0a\u8fdb\u884c\u8054\u5408\u89c6\u89c9-\u6587\u672c\u5b66\u4e60\uff0c\u5229\u7528\u591a\u4e2a\u89c6\u89d2\u548c\u76f8\u5e94\u7684\u4f2a\u653e\u5c04\u5b66\u62a5\u544a\u6765\u5b66\u4e60\u4e30\u5bcc\u7684\u8868\u793a\uff0c\u4ece\u800c\u533a\u5206\u4e73\u817a\u7ec4\u7ec7\u6216\u764c\u75c7\u7279\u5f81\uff08\u9499\u5316\u3001\u80bf\u5757\uff09\u5e76\u5229\u7528\u8fd9\u4e9b\u6a21\u5f0f\u9884\u6d4b\u764c\u75c7\u98ce\u9669\u3002", "result": "\u5728\u79c1\u6709\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4e09\u4e2a\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u6076\u6027\u5206\u7c7b\u3001\u4e9a\u578b\u5206\u7c7b\u548c\u57fa\u4e8e\u56fe\u50cf\u7684\u764c\u75c7\u98ce\u9669\u9884\u6d4b\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6570\u636e\u6548\u7387\uff0c\u5728\u4ec5\u4f7f\u7528\u5408\u6210\u6587\u672c\u62a5\u544a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u73b0\u6709\u5168\u76d1\u7763\u6216\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u89c6\u89d2\u8de8\u6a21\u6001\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u51cf\u5c11\u5bf9\u6602\u8d35\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5408\u6210\u6587\u672c\u6570\u636e\u5728\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.26615", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26615", "abs": "https://arxiv.org/abs/2510.26615", "authors": ["Yiqiao Jin", "Rachneet Kaur", "Zhen Zeng", "Sumitra Ganesh", "Srijan Kumar"], "title": "SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding", "comment": "https://slideagent.github.io/", "summary": "Multi-page visual documents such as manuals, brochures, presentations, and\nposters convey key information through layout, colors, icons, and cross-slide\nreferences. While large language models (LLMs) offer opportunities in document\nunderstanding, current systems struggle with complex, multi-page visual\ndocuments, particularly in fine-grained reasoning over elements and pages. We\nintroduce SlideAgent, a versatile agentic framework for understanding\nmulti-modal, multi-page, and multi-layout documents, especially slide decks.\nSlideAgent employs specialized agents and decomposes reasoning into three\nspecialized levels-global, page, and element-to construct a structured,\nquery-agnostic representation that captures both overarching themes and\ndetailed visual or textual cues. During inference, SlideAgent selectively\nactivates specialized agents for multi-level reasoning and integrates their\noutputs into coherent, context-aware answers. Extensive experiments show that\nSlideAgent achieves significant improvement over both proprietary (+7.9\noverall) and open-source models (+9.8 overall).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SlideAgent\uff0c\u4e00\u4e2a\u7528\u4e8e\u7406\u89e3\u591a\u6a21\u6001\u3001\u591a\u9875\u9762\u3001\u591a\u5e03\u5c40\u6587\u6863\u7684\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u4e13\u4e1a\u5316\u4ee3\u7406\u5b9e\u73b0\u4ece\u5168\u5c40\u5230\u5143\u7d20\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\uff0c\u5728\u590d\u6742\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u591a\u9875\u9762\u89c6\u89c9\u6587\u6863\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u8de8\u9875\u9762\u5143\u7d20\u7ec6\u7c92\u5ea6\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u4e3a\u6587\u6863\u7406\u89e3\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u5c1a\u672a\u6709\u6548\u89e3\u51b3\u591a\u9875\u9762\u89c6\u89c9\u6587\u6863\u7684\u590d\u6742\u7406\u89e3\u6311\u6218\u3002", "method": "SlideAgent\u91c7\u7528\u5206\u5c42\u4e13\u4e1a\u5316\u4ee3\u7406\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u5168\u5c40\u3001\u9875\u9762\u548c\u5143\u7d20\u4e09\u4e2a\u4e13\u95e8\u5c42\u7ea7\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u7684\u67e5\u8be2\u65e0\u5173\u8868\u793a\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9009\u62e9\u6027\u6fc0\u6d3b\u4e0d\u540c\u5c42\u7ea7\u4ee3\u7406\u5e76\u6574\u5408\u5176\u8f93\u51fa\u4ee5\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7b54\u6848\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSlideAgent\u76f8\u6bd4\u4e13\u6709\u6a21\u578b\u5b9e\u73b0\u4e86+7.9\u7684\u6574\u4f53\u6027\u80fd\u63d0\u5347\uff0c\u76f8\u6bd4\u5f00\u6e90\u6a21\u578b\u5b9e\u73b0\u4e86+9.8\u7684\u6574\u4f53\u6027\u80fd\u63d0\u5347\uff0c\u5728\u590d\u6742\u591a\u9875\u9762\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5206\u5c42\u4e13\u4e1a\u5316\u4ee3\u7406\u6846\u67b6\u5728\u590d\u6742\u591a\u9875\u9762\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u6587\u6863\u667a\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u4ee3\u7406\u67b6\u6784\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u8de8\u9875\u9762\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.26006", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26006", "abs": "https://arxiv.org/abs/2510.26006", "authors": ["Rishika Bhagwatkar", "Syrielle Montariol", "Angelika Romanou", "Beatriz Borges", "Irina Rish", "Antoine Bosselut"], "title": "CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments", "comment": null, "summary": "Humans can naturally identify, reason about, and explain anomalies in their\nenvironment. In computer vision, this long-standing challenge remains limited\nto industrial defects or unrealistic, synthetically generated anomalies,\nfailing to capture the richness and unpredictability of real-world anomalies.\nIn this work, we introduce CAVE, the first benchmark of real-world visual\nanomalies. CAVE supports three open-ended tasks: anomaly description,\nexplanation, and justification; with fine-grained annotations for visual\ngrounding and categorizing anomalies based on their visual manifestations,\ntheir complexity, severity, and commonness. These annotations draw inspiration\nfrom cognitive science research on how humans identify and resolve anomalies,\nproviding a comprehensive framework for evaluating Vision-Language Models\n(VLMs) in detecting and understanding anomalies. We show that state-of-the-art\nVLMs struggle with visual anomaly perception and commonsense reasoning, even\nwith advanced prompting strategies. By offering a realistic and cognitively\ngrounded benchmark, CAVE serves as a valuable resource for advancing research\nin anomaly detection and commonsense reasoning in VLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CAVE\uff0c\u9996\u4e2a\u771f\u5b9e\u4e16\u754c\u89c6\u89c9\u5f02\u5e38\u57fa\u51c6\uff0c\u652f\u6301\u5f02\u5e38\u63cf\u8ff0\u3001\u89e3\u91ca\u548c\u8bba\u8bc1\u4e09\u4e2a\u5f00\u653e\u4efb\u52a1\uff0c\u4e3a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u548c\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u65b9\u9762\u63d0\u4f9b\u4e86\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\u7684\u7efc\u5408\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u4e3b\u8981\u5c40\u9650\u4e8e\u5de5\u4e1a\u7f3a\u9677\u6216\u5408\u6210\u751f\u6210\u7684\u5f02\u5e38\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u7684\u4e30\u5bcc\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u800c\u4eba\u7c7b\u5374\u80fd\u81ea\u7136\u5730\u8bc6\u522b\u3001\u63a8\u7406\u548c\u89e3\u91ca\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u73b0\u8c61\u3002", "method": "CAVE\u57fa\u51c6\u5f15\u5165\u4e86\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7814\u7a76\u542f\u53d1\u7684\u7ec6\u7c92\u5ea6\u6807\u6ce8\u6846\u67b6\uff0c\u5305\u62ec\u89c6\u89c9\u5b9a\u4f4d\u548c\u57fa\u4e8e\u89c6\u89c9\u8868\u73b0\u3001\u590d\u6742\u6027\u3001\u4e25\u91cd\u6027\u548c\u5e38\u89c1\u6027\u7684\u5f02\u5e38\u5206\u7c7b\uff0c\u652f\u6301\u5f02\u5e38\u63cf\u8ff0\u3001\u89e3\u91ca\u548c\u8bba\u8bc1\u4e09\u4e2a\u5f00\u653e\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u91c7\u7528\u5148\u8fdb\u7684\u63d0\u793a\u7b56\u7565\uff0c\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5f02\u5e38\u611f\u77e5\u548c\u5e38\u8bc6\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u56f0\u96be\uff0c\u7a81\u663e\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7406\u89e3\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "CAVE\u4f5c\u4e3a\u73b0\u5b9e\u4e14\u8ba4\u77e5\u57fa\u7840\u624e\u5b9e\u7684\u57fa\u51c6\uff0c\u4e3a\u63a8\u8fdb\u5f02\u5e38\u68c0\u6d4b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e38\u8bc6\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u7406\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\u548c\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2510.26339", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26339", "abs": "https://arxiv.org/abs/2510.26339", "authors": ["Mingyu Sung", "Seungjae Ham", "Kangwoo Kim", "Yeokyoung Yoon", "Sangseok Yun", "Il-Min Kim", "Jae-Mo Kang"], "title": "GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?", "comment": "11 pages, 6 figures. Includes supplementary material. Under review as\n  a conference paper at ICLR 2026", "summary": "Image super-resolution(SR) is fundamental to many vision system-from\nsurveillance and autonomy to document analysis and retail analytics-because\nrecovering high-frequency details, especially scene-text, enables reliable\ndownstream perception. Scene-text, i.e., text embedded in natural images such\nas signs, product labels, and storefronts, often carries the most actionable\ninformation; when characters are blurred or hallucinated, optical character\nrecognition(OCR) and subsequent decisions fail even if the rest of the image\nappears sharp. Yet previous SR research has often been tuned to distortion\n(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that\nare largely insensitive to character-level errors. Furthermore, studies that do\naddress text SR often focus on simplified benchmarks with isolated characters,\noverlooking the challenges of text within complex natural scenes. As a result,\nscene-text is effectively treated as generic texture. For SR to be effective in\npractical deployments, it is therefore essential to explicitly optimize for\nboth text legibility and perceptual quality. We present GLYPH-SR, a\nvision-language-guided diffusion framework that aims to achieve both objectives\njointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by\nOCR data, and a ping-pong scheduler that alternates between text- and\nscene-centric guidance. To enable targeted text restoration, we train these\ncomponents on a synthetic corpus while keeping the main SR branch frozen.\nAcross SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by\nup to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)\nwhile maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed\nto satisfy both objectives simultaneously-high readability and high visual\nrealism-delivering SR that looks right and reds right.", "AI": {"tldr": "GLYPH-SR\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u8bed\u8a00\u5f15\u5bfc\u7684\u6269\u6563\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u573a\u666f\u6587\u672c\u8d85\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u53ef\u8bfb\u6027\u548c\u611f\u77e5\u8d28\u91cf\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u89c6\u89c9\u771f\u5b9e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347OCR\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u5931\u771f\u6307\u6807\uff08PSNR/SSIM\uff09\u6216\u611f\u77e5\u8d28\u91cf\u6307\u6807\u8fdb\u884c\u4f18\u5316\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u5bf9\u5b57\u7b26\u7ea7\u9519\u8bef\u4e0d\u654f\u611f\uff0c\u5bfc\u81f4\u573a\u666f\u6587\u672c\uff08\u5982\u6807\u5fd7\u3001\u4ea7\u54c1\u6807\u7b7e\u4e2d\u7684\u6587\u5b57\uff09\u5728\u8d85\u5206\u8fa8\u7387\u540e\u4ecd\u96be\u4ee5\u88abOCR\u7cfb\u7edf\u51c6\u786e\u8bc6\u522b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "GLYPH-SR\u91c7\u7528\u57fa\u4e8eOCR\u6570\u636e\u7684\u6587\u672c\u8d85\u5206\u8fa8\u7387\u878d\u5408\u63a7\u5236\u7f51\u7edc\uff08TS-ControlNet\uff09\u548c\u4e52\u4e53\u8c03\u5ea6\u5668\uff0c\u5728\u6587\u672c\u5bfc\u5411\u548c\u573a\u666f\u5bfc\u5411\u4e4b\u95f4\u4ea4\u66ff\u5f15\u5bfc\uff0c\u901a\u8fc7\u5728\u5408\u6210\u8bed\u6599\u4e0a\u8bad\u7ec3\u8fd9\u4e9b\u7ec4\u4ef6\u540c\u65f6\u4fdd\u6301\u4e3b\u8d85\u5206\u8fa8\u7387\u5206\u652f\u51bb\u7ed3\uff0c\u5b9e\u73b0\u9488\u5bf9\u6027\u6587\u672c\u6062\u590d\u3002", "result": "\u5728SVT\u3001SCUT-CTW1500\u548cCUTE80\u6570\u636e\u96c6\u4e0a\u7684x4\u548cx8\u8d85\u5206\u8fa8\u7387\u5b9e\u9a8c\u4e2d\uff0cGLYPH-SR\u76f8\u6bd4\u6269\u6563/GAN\u57fa\u7ebf\u5c06OCR F1\u5206\u6570\u63d0\u5347\u4e86\u6700\u9ad815.18\u4e2a\u767e\u5206\u70b9\uff08SVT x8, OpenOCR\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684MANIQA\u3001CLIP-IQA\u548cMUSIQ\u611f\u77e5\u8d28\u91cf\u5206\u6570\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u8d85\u5206\u8fa8\u7387\u7cfb\u7edf\u9700\u8981\u540c\u65f6\u4f18\u5316\u6587\u672c\u53ef\u8bfb\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u6027\uff0cGLYPH-SR\u6846\u67b6\u8bc1\u660e\u4e86\u901a\u8fc7\u9488\u5bf9\u6027\u8bbe\u8ba1\u53ef\u4ee5\u5b9e\u73b0\u65e2\u770b\u8d77\u6765\u6b63\u786e\u53c8\u8bfb\u8d77\u6765\u6b63\u786e\u7684\u8d85\u5206\u8fa8\u7387\u6548\u679c\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26027", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26027", "abs": "https://arxiv.org/abs/2510.26027", "authors": ["Ali Rasekh", "Erfan Bagheri Soula", "Omid Daliran", "Simon Gottschalk", "Mohsen Fayyaz"], "title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders", "comment": "Accepted to NeurIPS 2025", "summary": "Despite significant advances in Multimodal Large Language Models (MLLMs),\nunderstanding complex temporal dynamics in videos remains a major challenge.\nOur experiments show that current Video Large Language Model (Video-LLM)\narchitectures have critical limitations in temporal understanding, struggling\nwith tasks that require detailed comprehension of action sequences and temporal\nprogression. In this work, we propose a Video-LLM architecture that introduces\nstacked temporal attention modules directly within the vision encoder. This\ndesign incorporates a temporal attention in vision encoder, enabling the model\nto better capture the progression of actions and the relationships between\nframes before passing visual tokens to the LLM. Our results show that this\napproach significantly improves temporal reasoning and outperforms existing\nmodels in video question answering tasks, specifically in action recognition.\nWe improve on benchmarks including VITATECS, MVBench, and Video-MME by up to\n+5.5%. By enhancing the vision encoder with temporal structure, we address a\ncritical gap in video understanding for Video-LLMs. Project page and code are\navailable at: https://alirasekh.github.io/STAVEQ2/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u5f15\u5165\u5806\u53e0\u65f6\u5e8f\u6ce8\u610f\u529b\u6a21\u5757\u7684Video-LLM\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad8+5.5%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u5e8f\u52a8\u6001\u7406\u89e3\u65b9\u9762\u7684\u5173\u952e\u9650\u5236\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u65f6\u5e8f\u52a8\u6001\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002\u5b9e\u9a8c\u8868\u660e\u73b0\u6709Video-LLM\u67b6\u6784\u5728\u9700\u8981\u8be6\u7ec6\u7406\u89e3\u52a8\u4f5c\u5e8f\u5217\u548c\u65f6\u95f4\u8fdb\u5c55\u7684\u4efb\u52a1\u4e2d\u5b58\u5728\u5173\u952e\u9650\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5e27\u95f4\u5173\u7cfb\u548c\u52a8\u4f5c\u6f14\u8fdb\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684Video-LLM\u67b6\u6784\uff0c\u5728\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u76f4\u63a5\u5f15\u5165\u5806\u53e0\u65f6\u5e8f\u6ce8\u610f\u529b\u6a21\u5757\u3002\u8be5\u8bbe\u8ba1\u901a\u8fc7\u5728\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u96c6\u6210\u65f6\u5e8f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u5c06\u89c6\u89c9\u4ee4\u724c\u4f20\u9012\u7ed9LLM\u4e4b\u524d\u66f4\u597d\u5730\u6355\u6349\u52a8\u4f5c\u8fdb\u5c55\u548c\u5e27\u95f4\u5173\u7cfb\uff0c\u4ece\u800c\u589e\u5f3a\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5728VITATECS\u3001MVBench\u548cVideo-MME\u7b49\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8+5.5%\u7684\u6539\u8fdb\u3002\u7279\u522b\u662f\u5728\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u89c6\u89c9\u7f16\u7801\u5668\u7684\u65f6\u5e8f\u7ed3\u6784\uff0c\u672c\u7814\u7a76\u89e3\u51b3\u4e86Video-LLM\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5173\u952e\u7a7a\u767d\u3002\u8be5\u5de5\u4f5c\u8868\u660e\u76f4\u63a5\u5728\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u96c6\u6210\u65f6\u5e8f\u6ce8\u610f\u529b\u662f\u63d0\u5347\u89c6\u9891\u65f6\u5e8f\u63a8\u7406\u7684\u6709\u6548\u9014\u5f84\uff0c\u4e3a\u672a\u6765\u89c6\u9891\u7406\u89e3\u6a21\u578b\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2510.26241", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26241", "abs": "https://arxiv.org/abs/2510.26241", "authors": ["Shiho Matta", "Lis Kanashiro Pereira", "Peitao Han", "Fei Cheng", "Shigeru Kitazawa"], "title": "Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models", "comment": "10 pages", "summary": "Modern vision-language models (VLMs) excel at many multimodal tasks, yet\ntheir grasp of temporal information in video remains weak and, crucially,\nunder-evaluated. We probe this gap with a deceptively simple but revealing\nchallenge: judging the arrow of time (AoT)-whether a short clip is played\nforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated\nbenchmark that tests whether VLMs can infer temporal direction in natural\nvideos using the same stimuli and behavioral baselines established for humans.\nOur comprehensive evaluation of open-weight and proprietary, reasoning and\nnon-reasoning VLMs reveals that most models perform near chance, and even the\nbest lag far behind human accuracy on physically irreversible processes (e.g.,\nfree fall, diffusion/explosion) and causal manual actions (division/addition)\nthat humans recognize almost instantly. These results highlight a fundamental\ngap in current multimodal systems: while they capture rich visual-semantic\ncorrelations, they lack the inductive biases required for temporal continuity\nand causal understanding. We release the code and data for AoT-PsyPhyBENCH to\nencourage further progress in the physical and temporal reasoning capabilities\nof VLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u7684\u6839\u672c\u7f3a\u9677\uff0c\u901a\u8fc7\u5f15\u5165AoT-PsyPhyBENCH\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\u5927\u591a\u6570\u6a21\u578b\u5728\u5224\u65ad\u89c6\u9891\u65f6\u95f4\u65b9\u5411\u4efb\u52a1\u4e0a\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0c\u8fdc\u843d\u540e\u4e8e\u4eba\u7c7b\u8868\u73b0\u3002", "motivation": "\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u89c6\u9891\u4e2d\u65f6\u95f4\u4fe1\u606f\u7684\u7406\u89e3\u80fd\u529b\u4ecd\u7136\u8584\u5f31\u4e14\u7f3a\u4e4f\u5145\u5206\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u63a2\u7d22\u6a21\u578b\u5bf9\u65f6\u95f4\u65b9\u5411\u5224\u65ad\u7684\u57fa\u672c\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86AoT-PsyPhyBENCH\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ecf\u8fc7\u5fc3\u7406\u7269\u7406\u5b66\u9a8c\u8bc1\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u4e0e\u4eba\u7c7b\u884c\u4e3a\u57fa\u51c6\u76f8\u540c\u7684\u523a\u6fc0\u6750\u6599\u6765\u6d4b\u8bd5VLMs\u5bf9\u81ea\u7136\u89c6\u9891\u4e2d\u65f6\u95f4\u65b9\u5411\u7684\u63a8\u65ad\u80fd\u529b\uff0c\u5168\u9762\u8bc4\u4f30\u4e86\u5f00\u6e90\u548c\u4e13\u6709\u3001\u63a8\u7406\u548c\u975e\u63a8\u7406\u7c7b\u578b\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5927\u591a\u6570\u6a21\u578b\u5728\u65f6\u95f4\u65b9\u5411\u5224\u65ad\u4efb\u52a1\u4e0a\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0c\u5373\u4f7f\u5728\u7269\u7406\u4e0d\u53ef\u9006\u8fc7\u7a0b\uff08\u5982\u81ea\u7531\u843d\u4f53\u3001\u6269\u6563/\u7206\u70b8\uff09\u548c\u56e0\u679c\u624b\u52a8\u52a8\u4f5c\uff08\u9664\u6cd5/\u52a0\u6cd5\uff09\u7b49\u4eba\u7c7b\u51e0\u4e4e\u80fd\u77ac\u95f4\u8bc6\u522b\u7684\u4efb\u52a1\u4e0a\uff0c\u6700\u4f73\u6a21\u578b\u7684\u8868\u73b0\u4e5f\u8fdc\u8fdc\u843d\u540e\u4e8e\u4eba\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u7cfb\u7edf\u5b58\u5728\u6839\u672c\u6027\u5dee\u8ddd\uff1a\u867d\u7136\u5b83\u4eec\u80fd\u591f\u6355\u6349\u4e30\u5bcc\u7684\u89c6\u89c9\u8bed\u4e49\u5173\u8054\uff0c\u4f46\u7f3a\u4e4f\u65f6\u95f4\u8fde\u7eed\u6027\u548c\u56e0\u679c\u7406\u89e3\u6240\u9700\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u8fd9\u4e3a\u5f00\u53d1\u5177\u6709\u7269\u7406\u548c\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u7684\u4e0b\u4e00\u4ee3VLMs\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.26412", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26412", "abs": "https://arxiv.org/abs/2510.26412", "authors": ["Xiangqing Zheng", "Chengyue Wu", "Kehai Chen", "Min Zhang"], "title": "LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation", "comment": null, "summary": "Recently text-to-video generation has made impressive progress in producing\nshort, high-quality clips, but evaluating long-form outputs remains a major\nchallenge especially when processing complex prompts. Existing benchmarks\nmostly rely on simplified prompts and focus on low-level metrics, overlooking\nfine-grained alignment with prompts and abstract dimensions such as narrative\ncoherence and thematic expression. To address these gaps, we propose\nLoCoT2V-Bench, a benchmark specifically designed for long video generation\n(LVG) under complex input conditions. Based on various real-world videos,\nLoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating\nelements like scene transitions and event dynamics. Moreover, it constructs a\nmulti-dimensional evaluation framework that includes our newly proposed metrics\nsuch as event-level alignment, fine-grained temporal consistency, content\nclarity, and the Human Expectation Realization Degree (HERD) that focuses on\nmore abstract attributes like narrative flow, emotional response, and character\ndevelopment. Using this framework, we conduct a comprehensive evaluation of\nnine representative LVG models, finding that while current methods perform well\non basic visual and temporal aspects, they struggle with inter-event\nconsistency, fine-grained alignment, and high-level thematic adherence, etc.\nOverall, LoCoT2V-Bench provides a comprehensive and reliable platform for\nevaluating long-form complex text-to-video generation and highlights critical\ndirections for future method improvement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LoCoT2V-Bench\uff0c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u8f93\u5165\u6761\u4ef6\u4e0b\u957f\u89c6\u9891\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\u548c\u65b0\u9896\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5f53\u524d\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u4e3b\u9898\u8868\u8fbe\u7b49\u62bd\u8c61\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u867d\u7136\u5728\u751f\u6210\u77ed\u9ad8\u8d28\u91cf\u89c6\u9891\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u957f\u89c6\u9891\u751f\u6210\u8bc4\u4f30\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u4f9d\u8d56\u7b80\u5316\u63d0\u793a\u5e76\u5173\u6ce8\u4f4e\u5c42\u6b21\u6307\u6807\uff0c\u5ffd\u89c6\u4e86\u4e0e\u63d0\u793a\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u4ee5\u53ca\u53d9\u4e8b\u8fde\u8d2f\u6027\u3001\u4e3b\u9898\u8868\u8fbe\u7b49\u62bd\u8c61\u7ef4\u5ea6\u3002", "method": "\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u6784\u5efa\u4e86\u5305\u542b\u573a\u666f\u8f6c\u6362\u548c\u4e8b\u4ef6\u52a8\u6001\u7b49\u5143\u7d20\u7684\u73b0\u5b9e\u590d\u6742\u63d0\u793a\u96c6\uff0c\u5e76\u5efa\u7acb\u4e86\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u65b0\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u5982\u4e8b\u4ef6\u7ea7\u5bf9\u9f50\u3001\u7ec6\u7c92\u5ea6\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u5185\u5bb9\u6e05\u6670\u5ea6\u4ee5\u53ca\u5173\u6ce8\u53d9\u4e8b\u6d41\u7a0b\u3001\u60c5\u611f\u54cd\u5e94\u548c\u89d2\u8272\u53d1\u5c55\u7b49\u62bd\u8c61\u5c5e\u6027\u7684\u4eba\u7c7b\u671f\u671b\u5b9e\u73b0\u5ea6\u6307\u6807\u3002", "result": "\u5bf9\u4e5d\u4e2a\u4ee3\u8868\u6027\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u57fa\u672c\u89c6\u89c9\u548c\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e8b\u4ef6\u95f4\u4e00\u81f4\u6027\u3001\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u9ad8\u5c42\u6b21\u4e3b\u9898\u9075\u5faa\u7b49\u65b9\u9762\u5b58\u5728\u663e\u8457\u56f0\u96be\u3002", "conclusion": "LoCoT2V-Bench\u4e3a\u957f\u5f62\u5f0f\u590d\u6742\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5168\u9762\u53ef\u9760\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u9ad8\u7ea7\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u65b9\u6cd5\u6539\u8fdb\u6307\u660e\u4e86\u5173\u952e\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u63d0\u5347\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u4e3b\u9898\u8868\u8fbe\u80fd\u529b\u65b9\u9762\u3002"}}
{"id": "2510.26474", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26474", "abs": "https://arxiv.org/abs/2510.26474", "authors": ["Xin Guo", "Zhiheng Xi", "Yiwen Ding", "Yitao Zhai", "Xiaowei Shi", "Xunliang Cai", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing", "comment": "Preprint", "summary": "Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u6539\u8fdb\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u9a6c\u592a\u6548\u5e94\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5206\u5e03\u91cd\u5851\u548c\u8f68\u8ff9\u91cd\u91c7\u6837\u4e24\u79cd\u89c6\u89d2\u7684\u56db\u79cd\u7b56\u7565\uff0c\u6709\u6548\u5e73\u8861\u4e86\u7b80\u5355\u4e0e\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u6539\u8fdb\u8fc7\u7a0b\u4e2d\u5b58\u5728\u9a6c\u592a\u6548\u5e94\u95ee\u9898\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u4e3a\u7b80\u5355\u67e5\u8be2\u751f\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\uff0c\u800c\u96be\u4ee5\u5904\u7406\u590d\u6742\u67e5\u8be2\uff0c\u5bfc\u81f4\u4f18\u5316\u5931\u8861\u5e76\u963b\u788d\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u63d0\u5347\uff0c\u6700\u7ec8\u5f62\u6210\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u4ece\u5206\u5e03\u91cd\u5851\u548c\u8f68\u8ff9\u91cd\u91c7\u6837\u4e24\u4e2a\u89d2\u5ea6\u7684\u56db\u79cd\u9ad8\u6548\u7b56\u7565\uff0c\u5728\u63a2\u7d22\u5b66\u4e60\u7684\u81ea\u6539\u8fdb\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u5934\u5c3e\u6570\u636e\u7684\u91cd\u65b0\u5e73\u8861\uff0c\u5305\u62ec\u8c03\u6574\u6570\u636e\u5206\u5e03\u6743\u91cd\u548c\u4f18\u5316\u63a8\u7406\u8f68\u8ff9\u91c7\u6837\u673a\u5236\u3002", "result": "\u5728Qwen2-VL-7B-Instruct\u548cInternVL2.5-4B\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u6301\u7eed\u63d0\u5347\u6a21\u578b\u80fd\u529b\uff0c\u5e73\u5747\u6bd4\u539f\u59cb\u81ea\u6539\u8fdb\u65b9\u6cd5\u9ad8\u51fa3.86\u4e2a\u70b9\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u81ea\u6539\u8fdb\u8fc7\u7a0b\u4e2d\u7684\u4f18\u5316\u5931\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u5e73\u8861\u7b56\u7565\u6709\u6548\u7f13\u89e3\u4e86\u9a6c\u592a\u6548\u5e94\uff0c\u4e3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u6cd5\u8bba\uff0c\u63a8\u52a8\u4e86\u590d\u6742\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.26802", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26802", "abs": "https://arxiv.org/abs/2510.26802", "authors": ["Ziyu Guo", "Xinyan Chen", "Renrui Zhang", "Ruichuan An", "Yu Qi", "Dongzhi Jiang", "Xiangtai Li", "Manyuan Zhang", "Hongsheng Li", "Pheng-Ann Heng"], "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark", "comment": "Project Page: https://video-cof.github.io", "summary": "Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u9886\u5148\u7684\u89c6\u9891\u751f\u6210\u6a21\u578bVeo-3\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u53d1\u73b0\u5f53\u524d\u89c6\u9891\u6a21\u578b\u5728\u77ed\u65f6\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u52a8\u6001\u63a8\u7406\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u957f\u65f6\u56e0\u679c\u63a8\u7406\u548c\u4e25\u683c\u51e0\u4f55\u7ea6\u675f\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\uff0c\u5c1a\u4e0d\u80fd\u4f5c\u4e3a\u72ec\u7acb\u7684\u96f6\u6837\u672c\u63a8\u7406\u5668\u3002", "motivation": "\u5c3d\u7ba1\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u80fd\u591f\u4ea7\u751f\u9ad8\u4fdd\u771f\u3001\u65f6\u95f4\u8fde\u8d2f\u7684\u89c6\u9891\uff0c\u8868\u660e\u5176\u53ef\u80fd\u7f16\u7801\u4e86\u4e30\u5bcc\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u4f46\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\uff1a\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u80fd\u591f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u63a8\u7406\u573a\u666f\u4e2d\u4f5c\u4e3a\u96f6\u6837\u672c\u63a8\u7406\u5668\u4f7f\u7528\uff1f\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5168\u9762\u63a2\u8ba8\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86MME-CoF\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5bf9\u9886\u5148\u7684Veo-3\u6a21\u578b\u572812\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u5305\u62ec\u7a7a\u95f4\u3001\u51e0\u4f55\u3001\u7269\u7406\u3001\u65f6\u95f4\u548c\u5177\u8eab\u903b\u8f91\u7b49\u65b9\u9762\uff0c\u7cfb\u7edf\u6027\u5730\u523b\u753b\u4e86\u5176\u4f18\u52bf\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u89c6\u9891\u6a21\u578b\u5728\u77ed\u65f6\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u548c\u5c40\u90e8\u4e00\u81f4\u52a8\u6001\u63a8\u7406\u65b9\u9762\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u6a21\u5f0f\uff0c\u4f46\u5728\u957f\u65f6\u56e0\u679c\u63a8\u7406\u3001\u4e25\u683c\u51e0\u4f55\u7ea6\u675f\u548c\u62bd\u8c61\u903b\u8f91\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u5c40\u9650\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f53\u524d\u89c6\u9891\u6a21\u578b\u5c1a\u4e0d\u80fd\u4f5c\u4e3a\u53ef\u9760\u7684\u72ec\u7acb\u96f6\u6837\u672c\u63a8\u7406\u5668\uff0c\u4f46\u4f5c\u4e3a\u4e13\u7528\u63a8\u7406\u6a21\u578b\u7684\u8865\u5145\u89c6\u89c9\u5f15\u64ce\u5c55\u73b0\u51fa\u4ee4\u4eba\u9f13\u821e\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u89c6\u9891\u63a8\u7406\u6a21\u578b\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.26114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26114", "abs": "https://arxiv.org/abs/2510.26114", "authors": ["Caoshuo Li", "Zengmao Ding", "Xiaobin Hu", "Bang Li", "Donghao Luo", "Xu Peng", "Taisong Jin", "Yongge Liu", "Shengwei Han", "Jing Yang", "Xiaoping He", "Feng Gao", "AndyPian Wu", "SevenShu", "Chaoyang Wang", "Chengjie Wang"], "title": "OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research", "comment": null, "summary": "As one of the earliest writing systems, Oracle Bone Script (OBS) preserves\nthe cultural and intellectual heritage of ancient civilizations. However,\ncurrent OBS research faces two major challenges: (1) the interpretation of OBS\ninvolves a complex workflow comprising multiple serial and parallel sub-tasks,\nand (2) the efficiency of OBS information organization and retrieval remains a\ncritical bottleneck, as scholars often spend substantial effort searching for,\ncompiling, and managing relevant resources. To address these challenges, we\npresent OracleAgent, the first agent system designed for the structured\nmanagement and retrieval of OBS-related information. OracleAgent seamlessly\nintegrates multiple OBS analysis tools, empowered by large language models\n(LLMs), and can flexibly orchestrate these components. Additionally, we\nconstruct a comprehensive domain-specific multimodal knowledge base for OBS,\nwhich is built through a rigorous multi-year process of data collection,\ncleaning, and expert annotation. The knowledge base comprises over 1.4M\nsingle-character rubbing images and 80K interpretation texts. OracleAgent\nleverages this resource through its multimodal tools to assist experts in\nretrieval tasks of character, document, interpretation text, and rubbing image.\nExtensive experiments demonstrate that OracleAgent achieves superior\nperformance across a range of multimodal reasoning and generation tasks,\nsurpassing leading mainstream multimodal large language models (MLLMs) (e.g.,\nGPT-4o). Furthermore, our case study illustrates that OracleAgent can\neffectively assist domain experts, significantly reducing the time cost of OBS\nresearch. These results highlight OracleAgent as a significant step toward the\npractical deployment of OBS-assisted research and automated interpretation\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OracleAgent\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u7532\u9aa8\u6587\u7ed3\u6784\u5316\u7ba1\u7406\u548c\u68c0\u7d22\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u6a21\u6001\u77e5\u8bc6\u5e93\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7532\u9aa8\u6587\u7814\u7a76\u7684\u6548\u7387\u548c\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "motivation": "\u7532\u9aa8\u6587\u4f5c\u4e3a\u6700\u65e9\u7684\u6587\u5b57\u7cfb\u7edf\u4e4b\u4e00\uff0c\u5176\u7814\u7a76\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u7532\u9aa8\u6587\u89e3\u8bfb\u6d89\u53ca\u591a\u4e2a\u4e32\u884c\u548c\u5e76\u884c\u5b50\u4efb\u52a1\u7684\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ee5\u53ca\u7532\u9aa8\u6587\u4fe1\u606f\u7ec4\u7ec7\u548c\u68c0\u7d22\u6548\u7387\u4f4e\u4e0b\uff0c\u5b66\u8005\u9700\u8981\u82b1\u8d39\u5927\u91cf\u7cbe\u529b\u641c\u7d22\u3001\u6574\u7406\u548c\u7ba1\u7406\u76f8\u5173\u8d44\u6e90\u3002", "method": "OracleAgent\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u7532\u9aa8\u6587\u5206\u6790\u5de5\u5177\uff0c\u5e76\u7075\u6d3b\u7f16\u6392\u8fd9\u4e9b\u7ec4\u4ef6\u6765\u6784\u5efa\u667a\u80fd\u4f53\u7cfb\u7edf\uff1b\u540c\u65f6\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u9886\u57df\u7279\u5b9a\u591a\u6a21\u6001\u77e5\u8bc6\u5e93\uff0c\u5305\u542b\u8d85\u8fc7140\u4e07\u5f20\u5355\u5b57\u62d3\u7247\u56fe\u50cf\u548c8\u4e07\u6761\u89e3\u8bfb\u6587\u672c\uff0c\u901a\u8fc7\u591a\u5e74\u4e25\u683c\u7684\u6570\u636e\u6536\u96c6\u3001\u6e05\u6d17\u548c\u4e13\u5bb6\u6807\u6ce8\u8fc7\u7a0b\u5b8c\u6210\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cOracleAgent\u5728\u4e00\u7cfb\u5217\u591a\u6a21\u6001\u63a8\u7406\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4e3b\u6d41\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\uff1b\u6848\u4f8b\u7814\u7a76\u663e\u793a\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u534f\u52a9\u9886\u57df\u4e13\u5bb6\uff0c\u663e\u8457\u964d\u4f4e\u7532\u9aa8\u6587\u7814\u7a76\u7684\u65f6\u95f4\u6210\u672c\u3002", "conclusion": "OracleAgent\u4ee3\u8868\u4e86\u7532\u9aa8\u6587\u8f85\u52a9\u7814\u7a76\u548c\u81ea\u52a8\u5316\u89e3\u8bfb\u7cfb\u7edf\u5b9e\u9645\u90e8\u7f72\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u4e3a\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.26160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26160", "abs": "https://arxiv.org/abs/2510.26160", "authors": ["Jiaqi Wang", "Xiao Yang", "Kai Sun", "Parth Suresh", "Sanat Sharma", "Adam Czyzewski", "Derek Andersen", "Surya Appini", "Arkav Banerjee", "Sajal Choudhary", "Shervin Ghasemlou", "Ziqiang Guan", "Akil Iyer", "Haidar Khan", "Lingkun Kong", "Roy Luo", "Tiffany Ma", "Zhen Qiao", "David Tran", "Wenfang Xu", "Skyler Yeatman", "Chen Zhou", "Gunveer Gujral", "Yinglong Xia", "Shane Moon", "Nicolas Scheffer", "Nirav Shah", "Eun Chang", "Yue Liu", "Florian Metze", "Tammy Stark", "Zhaleh Feizollahi", "Andrea Jessee", "Mangesh Pujari", "Ahmed Aly", "Babak Damavandi", "Rakesh Wanga", "Anuj Kumar", "Rohit Patel", "Wen-tau Yih", "Xin Luna Dong"], "title": "CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark", "comment": null, "summary": "Wearable devices such as smart glasses are transforming the way people\ninteract with their surroundings, enabling users to seek information regarding\nentities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)\nplays a key role in supporting such questions, yet there is still no\ncomprehensive benchmark for this task, especially regarding wearables\nscenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG\nbenchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse\nset of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn\nconversations across 13 domains, including 6.2K egocentric images designed to\nmimic captures from wearable devices. We carefully constructed the questions to\nreflect real-world scenarios and challenges, including five types of\nimage-quality issues, six question types, varying entity popularity, differing\ninformation dynamism, and different conversation turns. We design three tasks:\nsingle-source augmentation, multi-source augmentation, and multi-turn\nconversations -- each paired with an associated retrieval corpus and APIs for\nboth image-KG retrieval and webpage retrieval. Our evaluation shows that\nstraightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM\nsingle- and multi-turn QA, respectively, whereas state-of-the-art industry\nsolutions have similar quality (32%/45%), underscoring ample room for\nimprovement. The benchmark has hosted KDD Cup 2025, attracting about 1K\nparticipants and 5K submissions, with winning solutions improving baseline\nperformance by 28%, highlighting its early impact on advancing the field.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CRAG-MM\u2014\u2014\u4e00\u4e2a\u9762\u5411\u591a\u6a21\u6001\u591a\u8f6e\u5bf9\u8bdd\u7684\u5168\u9762\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u57fa\u51c6\uff0c\u5305\u542b6.5K\u4e2a\u56fe\u50cf-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\u548c2K\u4e2a\u591a\u8f6e\u5bf9\u8bdd\uff0c\u4e13\u95e8\u9488\u5bf9\u53ef\u7a7f\u6234\u8bbe\u5907\u573a\u666f\u8bbe\u8ba1\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7f3a\u4e4f\u7efc\u5408\u57fa\u51c6\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u53ef\u7a7f\u6234\u8bbe\u5907\u573a\u666f\u7684\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08MM-RAG\uff09\u4efb\u52a1\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u7279\u522b\u662f\u80fd\u591f\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u6311\u6218\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u8fd9\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b13\u4e2a\u9886\u57df\u30016.5K\u4e2a\u56fe\u50cf-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\u548c2K\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b6.2K\u4e2a\u6a21\u62df\u53ef\u7a7f\u6234\u8bbe\u5907\u62cd\u6444\u7684\u81ea\u6211\u4e2d\u5fc3\u56fe\u50cf\uff1b\u8bbe\u8ba1\u4e86\u4e94\u79cd\u56fe\u50cf\u8d28\u91cf\u95ee\u9898\u3001\u516d\u79cd\u95ee\u9898\u7c7b\u578b\u3001\u4e0d\u540c\u5b9e\u4f53\u6d41\u884c\u5ea6\u3001\u4fe1\u606f\u52a8\u6001\u6027\u548c\u5bf9\u8bdd\u8f6e\u6b21\uff1b\u63d0\u4f9b\u4e86\u4e09\u79cd\u4efb\u52a1\u8bbe\u7f6e\u548c\u76f8\u5e94\u7684\u68c0\u7d22\u8bed\u6599\u5e93\u53caAPI\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u76f4\u63a5RAG\u65b9\u6cd5\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u95ee\u7b54\u4e0a\u7684\u771f\u5b9e\u6027\u5206\u522b\u4ec5\u4e3a32%\u548c43%\uff0c\u800c\u4e1a\u754c\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\u76f8\u4f3c\uff0832%/45%\uff09\uff1b\u8be5\u57fa\u51c6\u5df2\u6210\u529f\u4e3e\u529eKDD Cup 2025\u7ade\u8d5b\uff0c\u83b7\u80dc\u65b9\u6848\u5c06\u57fa\u7ebf\u6027\u80fd\u63d0\u5347\u4e8628%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dMM-RAG\u65b9\u6cd5\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u573a\u666f\u4e0b\u7684\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u65b9\u5411\u6307\u5f15\uff1b\u57fa\u51c6\u7684\u6210\u529f\u5e94\u7528\u8868\u660e\u5176\u5728\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.26441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26441", "abs": "https://arxiv.org/abs/2510.26441", "authors": ["Shihab Aaqil Ahamed", "Udaya S. K. P. Miriya Thanthrige", "Ranga Rodrigo", "Muhammad Haris Khan"], "title": "A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models", "comment": "23 pages, 14 figures", "summary": "Test-time prompt tuning (TPT) has emerged as a promising technique for\nadapting large vision-language models (VLMs) to unseen tasks without relying on\nlabeled data. However, the lack of dispersion between textual features can hurt\ncalibration performance, which raises concerns about VLMs' reliability,\ntrustworthiness, and safety. Current TPT approaches primarily focus on\nimproving prompt calibration by either maximizing average textual feature\ndispersion or enforcing orthogonality constraints to encourage angular\nseparation. However, these methods may not always have optimal angular\nseparation between class-wise textual features, which implies overlooking the\ncritical role of angular diversity. To address this, we propose A-TPT, a novel\nTPT framework that introduces angular diversity to encourage uniformity in the\ndistribution of normalized textual features induced by corresponding learnable\nprompts. This uniformity is achieved by maximizing the minimum pairwise angular\ndistance between features on the unit hypersphere. We show that our approach\nconsistently surpasses state-of-the-art TPT methods in reducing the aggregate\naverage calibration error while maintaining comparable accuracy through\nextensive experiments with various backbones on different datasets. Notably,\nour approach exhibits superior zero-shot calibration performance on natural\ndistribution shifts and generalizes well to medical datasets. We provide\nextensive analyses, including theoretical aspects, to establish the grounding\nof A-TPT. These results highlight the potency of promoting angular diversity to\nachieve well-dispersed textual features, significantly improving VLM\ncalibration during test-time adaptation. Our code will be made publicly\navailable.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faA-TPT\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u89d2\u5ea6\u591a\u6837\u6027\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u4e2d\u7684\u6821\u51c6\u6027\u80fd\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6700\u5927\u5316\u5f52\u4e00\u5316\u6587\u672c\u7279\u5f81\u5728\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u7684\u6700\u5c0f\u6210\u5bf9\u89d2\u5ea6\u8ddd\u79bb\u6765\u5b9e\u73b0\u7279\u5f81\u5747\u5300\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6700\u5927\u5316\u5e73\u5747\u6587\u672c\u7279\u5f81\u79bb\u6563\u5ea6\u6216\u65bd\u52a0\u6b63\u4ea4\u7ea6\u675f\u6765\u4fc3\u8fdb\u89d2\u5ea6\u5206\u79bb\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u5b9e\u73b0\u7c7b\u522b\u95f4\u6587\u672c\u7279\u5f81\u7684\u6700\u4f18\u89d2\u5ea6\u5206\u79bb\uff0c\u5ffd\u89c6\u4e86\u89d2\u5ea6\u591a\u6837\u6027\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5bfc\u81f4\u6587\u672c\u7279\u5f81\u7f3a\u4e4f\u5206\u6563\u6027\u4ece\u800c\u635f\u5bb3\u6821\u51c6\u6027\u80fd\u3002", "method": "\u63d0\u51faA-TPT\u6846\u67b6\uff0c\u901a\u8fc7\u9f13\u52b1\u7531\u53ef\u5b66\u4e60\u63d0\u793a\u8bf1\u5bfc\u7684\u5f52\u4e00\u5316\u6587\u672c\u7279\u5f81\u5728\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u5747\u5300\u5206\u5e03\u6765\u5b9e\u73b0\u89d2\u5ea6\u591a\u6837\u6027\uff0c\u5177\u4f53\u901a\u8fc7\u6700\u5927\u5316\u7279\u5f81\u95f4\u6700\u5c0f\u6210\u5bf9\u89d2\u5ea6\u8ddd\u79bb\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "result": "\u5728\u591a\u4e2a\u9aa8\u5e72\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cA-TPT\u5728\u964d\u4f4e\u805a\u5408\u5e73\u5747\u6821\u51c6\u8bef\u5dee\u65b9\u9762\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u7684TPT\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u51c6\u786e\u7387\uff0c\u5728\u81ea\u7136\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u96f6\u6837\u672c\u6821\u51c6\u6027\u80fd\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u533b\u5b66\u6570\u636e\u96c6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u4fc3\u8fdb\u89d2\u5ea6\u591a\u6837\u6027\u662f\u5b9e\u73b0\u826f\u597d\u5206\u6563\u6587\u672c\u7279\u5f81\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u663e\u8457\u6539\u5584\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u9002\u5e94\u8fc7\u7a0b\u4e2d\u7684\u6821\u51c6\u6027\u80fd\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3001\u53ef\u4fe1\u5ea6\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.26464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26464", "abs": "https://arxiv.org/abs/2510.26464", "authors": ["Yuanting Fan", "Jun Liu", "Xiaochen Chen", "Bin-Bin Gao", "Jian Li", "Yong Liu", "Jinlong Peng", "Chengjie Wang"], "title": "Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection", "comment": "12 pages, 7 figures", "summary": "Few-shot anomaly detection (FSAD) methods identify anomalous regions with few\nknown normal samples. Most existing methods rely on the generalization ability\nof pre-trained vision-language models (VLMs) to recognize potentially anomalous\nregions through feature similarity between text descriptions and images.\nHowever, due to the lack of detailed textual descriptions, these methods can\nonly pre-define image-level descriptions to match each visual patch token to\nidentify potential anomalous regions, which leads to the semantic misalignment\nbetween image descriptions and patch-level visual anomalies, achieving\nsub-optimal localization performance. To address the above issues, we propose\nthe Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and\nfine-grained textual descriptions for existing anomaly detection datasets with\nautomatic construction pipeline. Based on the MFSC, we propose a novel\nframework named FineGrainedAD to improve anomaly localization performance,\nwhich consists of two components: Multi-Level Learnable Prompt (MLLP) and\nMulti-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics\ninto multi-level learnable prompts through automatic replacement and\nconcatenation mechanism, while MLSA designs region aggregation strategy and\nmulti-level alignment training to facilitate learnable prompts better align\nwith corresponding visual regions. Experiments demonstrate that the proposed\nFineGrainedAD achieves superior overall performance in few-shot settings on\nMVTec-AD and VisA datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FineGrainedAD\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u63cf\u8ff0\u548c\u8bed\u4e49\u5bf9\u9f50\u673a\u5236\u89e3\u51b3\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8bed\u4e49\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5728MVTec-AD\u548cVisA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5f02\u5e38\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u8be6\u7ec6\u6587\u672c\u63cf\u8ff0\uff0c\u53ea\u80fd\u9884\u5b9a\u4e49\u56fe\u50cf\u7ea7\u63cf\u8ff0\u6765\u5339\u914d\u89c6\u89c9\u8865\u4e01\u6807\u8bb0\uff0c\u5bfc\u81f4\u56fe\u50cf\u63cf\u8ff0\u4e0e\u8865\u4e01\u7ea7\u89c6\u89c9\u5f02\u5e38\u4e4b\u95f4\u7684\u8bed\u4e49\u4e0d\u5bf9\u9f50\uff0c\u4ece\u800c\u83b7\u5f97\u6b21\u4f18\u7684\u5b9a\u4f4d\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u7ea7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u63cf\u8ff0\uff08MFSC\uff09\u4e3a\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\u63d0\u4f9b\u591a\u7ea7\u7ec6\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5305\u542b\u591a\u7ea7\u53ef\u5b66\u4e60\u63d0\u793a\uff08MLLP\uff09\u548c\u591a\u7ea7\u8bed\u4e49\u5bf9\u9f50\uff08MLSA\uff09\u7684FineGrainedAD\u6846\u67b6\uff0c\u5176\u4e2dMLLP\u901a\u8fc7\u81ea\u52a8\u66ff\u6362\u548c\u8fde\u63a5\u673a\u5236\u5c06\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5f15\u5165\u591a\u7ea7\u53ef\u5b66\u4e60\u63d0\u793a\uff0cMLSA\u8bbe\u8ba1\u533a\u57df\u805a\u5408\u7b56\u7565\u548c\u591a\u7ea7\u5bf9\u9f50\u8bad\u7ec3\u6765\u4fc3\u8fdb\u53ef\u5b66\u4e60\u63d0\u793a\u4e0e\u76f8\u5e94\u89c6\u89c9\u533a\u57df\u7684\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684FineGrainedAD\u5728MVTec-AD\u548cVisA\u6570\u636e\u96c6\u7684\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6574\u4f53\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u591a\u7ea7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u63cf\u8ff0\u548c\u8bed\u4e49\u5bf9\u9f50\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8bed\u4e49\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u548c\u6027\u80fd\u63d0\u5347\u65b9\u6848\u3002"}}
{"id": "2510.26466", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26466", "abs": "https://arxiv.org/abs/2510.26466", "authors": ["Pei Peng", "MingKun Xie", "Hang Hao", "Tong Jin", "ShengJun Huang"], "title": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition", "comment": null, "summary": "Object-context shortcuts remain a persistent challenge in vision-language\nmodels, undermining zero-shot reliability when test-time scenes differ from\nfamiliar training co-occurrences. We recast this issue as a causal inference\nproblem and ask: Would the prediction remain if the object appeared in a\ndifferent environment? To answer this at inference time, we estimate object and\nbackground expectations within CLIP's representation space, and synthesize\ncounterfactual embeddings by recombining object features with diverse\nalternative contexts sampled from external datasets, batch neighbors, or\ntext-derived descriptions. By estimating the Total Direct Effect and simulating\nintervention, we further subtract background-only activation, preserving\nbeneficial object-context interactions while mitigating hallucinated scores.\nWithout retraining or prompt design, our method substantially improves both\nworst-group and average accuracy on context-sensitive benchmarks, establishing\na new zero-shot state of the art. Beyond performance, our framework provides a\nlightweight representation-level counterfactual approach, offering a practical\ncausal avenue for debiased and reliable multimodal reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684\u8f7b\u91cf\u7ea7\u8868\u793a\u7ea7\u53cd\u4e8b\u5b9e\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f30\u8ba1\u5bf9\u8c61\u548c\u80cc\u666f\u671f\u671b\u5e76\u5408\u6210\u53cd\u4e8b\u5b9e\u5d4c\u5165\uff0c\u6709\u6548\u7f13\u89e3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5bf9\u8c61-\u4e0a\u4e0b\u6587\u6377\u5f84\u95ee\u9898\uff0c\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u63d0\u793a\u8bbe\u8ba1\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u53ef\u9760\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u5bf9\u8c61-\u4e0a\u4e0b\u6587\u6377\u5f84\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e86\u96f6\u6837\u672c\u53ef\u9760\u6027\uff0c\u5f53\u6d4b\u8bd5\u573a\u666f\u4e0e\u8bad\u7ec3\u5171\u73b0\u6a21\u5f0f\u4e0d\u540c\u65f6\uff0c\u6a21\u578b\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u9884\u6d4b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u5728CLIP\u8868\u793a\u7a7a\u95f4\u4e2d\u4f30\u8ba1\u5bf9\u8c61\u548c\u80cc\u666f\u671f\u671b\uff0c\u901a\u8fc7\u4ece\u5916\u90e8\u6570\u636e\u96c6\u3001\u6279\u6b21\u90bb\u5c45\u6216\u6587\u672c\u63cf\u8ff0\u4e2d\u91c7\u6837\u591a\u6837\u66ff\u4ee3\u4e0a\u4e0b\u6587\uff0c\u5408\u6210\u53cd\u4e8b\u5b9e\u5d4c\u5165\uff0c\u5e76\u5229\u7528\u603b\u76f4\u63a5\u6548\u5e94\u4f30\u8ba1\u548c\u5e72\u9884\u6a21\u62df\u6765\u51cf\u53bb\u4ec5\u80cc\u666f\u6fc0\u6d3b\uff0c\u4fdd\u7559\u6709\u76ca\u7684\u5bf9\u8c61-\u4e0a\u4e0b\u6587\u4ea4\u4e92\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u654f\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6700\u5dee\u7ec4\u548c\u5e73\u5747\u51c6\u786e\u7387\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u63d0\u793a\u8bbe\u8ba1\u5373\u5b9e\u73b0\u4e86\u65b0\u7684\u96f6\u6837\u672c\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7f13\u89e3\u5e7b\u89c9\u5206\u6570\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u8868\u793a\u7ea7\u53cd\u4e8b\u5b9e\u6846\u67b6\uff0c\u4e3a\u53bb\u504f\u548c\u53ef\u9760\u7684\u591a\u6a21\u6001\u63a8\u7406\u5f00\u8f9f\u4e86\u5b9e\u7528\u7684\u56e0\u679c\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u56e0\u679c\u63a8\u7406\u5728\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.26569", "categories": ["cs.CV", "cs.IR", "cs.MM", "68T05", "I.4.0; H.3.1; I.2.10; K.4.4"], "pdf": "https://arxiv.org/pdf/2510.26569", "abs": "https://arxiv.org/abs/2510.26569", "authors": ["Wen Xie", "Yanjun Zhu", "Gijs Overgoor", "Yakov Bart", "Agata Lapedriza Garcia", "Sarah Ostadabbas"], "title": "AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping", "comment": "Accepted at 32nd International Conference on MultiMedia Modeling", "summary": "Advertisers commonly need multiple versions of the same advertisement (ad) at\nvarying durations for a single campaign. The traditional approach involves\nmanually selecting and re-editing shots from longer video ads to create shorter\nversions, which is labor-intensive and time-consuming. In this paper, we\nintroduce a framework for automated video ad clipping using video summarization\ntechniques. We are the first to frame video clipping as a shot selection\nproblem, tailored specifically for advertising. Unlike existing general video\nsummarization methods that primarily focus on visual content, our approach\nemphasizes the critical role of audio in advertising. To achieve this, we\ndevelop a two-stream audio-visual fusion model that predicts the importance of\nvideo frames, where importance is defined as the likelihood of a frame being\nselected in the firm-produced short ad. To address the lack of ad-specific\ndatasets, we present AdSum204, a novel dataset comprising 102 pairs of\n30-second and 15-second ads from real advertising campaigns. Extensive\nexperiments demonstrate that our model outperforms state-of-the-art methods\nacross various metrics, including Average Precision, Area Under Curve,\nSpearman, and Kendall.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u6458\u8981\u6280\u672f\u7684\u81ea\u52a8\u5316\u5e7f\u544a\u526a\u8f91\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u89c6\u9891\u526a\u8f91\u5b9a\u4e49\u4e3a\u9488\u5bf9\u5e7f\u544a\u573a\u666f\u7684\u955c\u5934\u9009\u62e9\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u53cc\u6d41\u97f3\u89c6\u9891\u878d\u5408\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u5e7f\u544a\u526a\u8f91\u6027\u80fd\u3002", "motivation": "\u5e7f\u544a\u5546\u901a\u5e38\u9700\u8981\u4e3a\u540c\u4e00\u5e7f\u544a\u6d3b\u52a8\u5236\u4f5c\u4e0d\u540c\u65f6\u957f\u7684\u591a\u4e2a\u7248\u672c\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u4ece\u957f\u89c6\u9891\u5e7f\u544a\u4e2d\u9009\u62e9\u548c\u91cd\u65b0\u7f16\u8f91\u955c\u5934\u6765\u521b\u5efa\u77ed\u7248\u672c\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u65e2\u8017\u65f6\u53c8\u8d39\u529b\uff0c\u73b0\u6709\u901a\u7528\u89c6\u9891\u6458\u8981\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u5185\u5bb9\u800c\u5ffd\u7565\u4e86\u97f3\u9891\u5728\u5e7f\u544a\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6d41\u97f3\u89c6\u9891\u878d\u5408\u6a21\u578b\u6765\u9884\u6d4b\u89c6\u9891\u5e27\u7684\u91cd\u8981\u6027\uff0c\u5176\u4e2d\u91cd\u8981\u6027\u5b9a\u4e49\u4e3a\u5e27\u88ab\u9009\u5165\u4f01\u4e1a\u5236\u4f5c\u7684\u77ed\u5e7f\u544a\u4e2d\u7684\u53ef\u80fd\u6027\uff0c\u8be5\u65b9\u6cd5\u5c06\u89c6\u9891\u526a\u8f91\u6784\u5efa\u4e3a\u955c\u5934\u9009\u62e9\u95ee\u9898\uff0c\u5e76\u4e13\u95e8\u9488\u5bf9\u5e7f\u544a\u573a\u666f\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "result": "\u5728AdSum204\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u5e73\u5747\u7cbe\u5ea6\u3001\u66f2\u7ebf\u4e0b\u9762\u79ef\u3001\u65af\u76ae\u5c14\u66fc\u76f8\u5173\u7cfb\u6570\u548c\u80af\u5fb7\u5c14\u7cfb\u6570\u7b49\u591a\u4e2a\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u97f3\u9891\u4fe1\u606f\u5bf9\u5e7f\u544a\u526a\u8f91\u4efb\u52a1\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u97f3\u9891\u5728\u5e7f\u544a\u526a\u8f91\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63d0\u51fa\u7684\u53cc\u6d41\u97f3\u89c6\u9891\u878d\u5408\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u5e7f\u544a\u5236\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u53d1\u5e03\u7684AdSum204\u6570\u636e\u96c6\u586b\u8865\u4e86\u5e7f\u544a\u7279\u5b9a\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2510.26580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26580", "abs": "https://arxiv.org/abs/2510.26580", "authors": ["Manjunath Prasad Holenarasipura Rajiv", "B. M. Vidyavathi"], "title": "Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios", "comment": "Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025", "summary": "In real-world environments, AI systems often face unfamiliar scenarios\nwithout labeled data, creating a major challenge for conventional scene\nunderstanding models. The inability to generalize across unseen contexts limits\nthe deployment of vision-based applications in dynamic, unstructured settings.\nThis work introduces a Dynamic Context-Aware Scene Reasoning framework that\nleverages Vision-Language Alignment to address zero-shot real-world scenarios.\nThe goal is to enable intelligent systems to infer and adapt to new\nenvironments without prior task-specific training. The proposed approach\nintegrates pre-trained vision transformers and large language models to align\nvisual semantics with natural language descriptions, enhancing contextual\ncomprehension. A dynamic reasoning module refines predictions by combining\nglobal scene cues and object-level interactions guided by linguistic priors.\nExtensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and\nOpen Images demonstrate up to 18% improvement in scene understanding accuracy\nover baseline models in complex and unseen environments. Results also show\nrobust performance in ambiguous or cluttered scenes due to the synergistic\nfusion of vision and language. This framework offers a scalable and\ninterpretable approach for context-aware reasoning, advancing zero-shot\ngeneralization in dynamic real-world settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4e0a\u4e0b\u6587\u611f\u77e5\u573a\u666f\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u6280\u672f\u89e3\u51b3\u96f6\u6837\u672c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7406\u89e3\u95ee\u9898\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u771f\u5b9e\u73af\u5883\u4e2dAI\u7cfb\u7edf\u7ecf\u5e38\u9762\u4e34\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7684\u964c\u751f\u573a\u666f\uff0c\u4f20\u7edf\u573a\u666f\u7406\u89e3\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u4e0a\u4e0b\u6587\u73af\u5883\uff0c\u8fd9\u9650\u5236\u4e86\u89c6\u89c9\u5e94\u7528\u5728\u52a8\u6001\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u8be5\u65b9\u6cd5\u6574\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u53d8\u6362\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u89c6\u89c9\u8bed\u4e49\u4e0e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5bf9\u9f50\u4ee5\u589e\u5f3a\u4e0a\u4e0b\u6587\u7406\u89e3\uff1b\u52a8\u6001\u63a8\u7406\u6a21\u5757\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u573a\u666f\u7ebf\u7d22\u548c\u5bf9\u8c61\u7ea7\u4ea4\u4e92\uff0c\u5728\u8bed\u8a00\u5148\u9a8c\u6307\u5bfc\u4e0b\u4f18\u5316\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u5728COCO\u3001Visual Genome\u548cOpen Images\u7b49\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u672a\u89c1\u73af\u5883\u4e2d\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e8618%\u7684\u573a\u666f\u7406\u89e3\u51c6\u786e\u7387\uff1b\u5728\u6a21\u7cca\u6216\u6742\u4e71\u573a\u666f\u4e2d\u4e5f\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\uff0c\u5f97\u76ca\u4e8e\u89c6\u89c9\u4e0e\u8bed\u8a00\u7684\u534f\u540c\u878d\u5408\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u52a8\u6001\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u7684\u53d1\u5c55\uff0c\u4e3a\u667a\u80fd\u7cfb\u7edf\u5728\u65e0\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6761\u4ef6\u4e0b\u9002\u5e94\u65b0\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26582", "abs": "https://arxiv.org/abs/2510.26582", "authors": ["Xinjin Li", "Yulie Lu", "Jinghan Cao", "Yu Ma", "Zhenglin Li", "Yeyang Zhou"], "title": "CATCH: A Modular Cross-domain Adaptive Template with Hook", "comment": null, "summary": "Recent advances in Visual Question Answering (VQA) have demonstrated\nimpressive performance in natural image domains, with models like LLaVA\nleveraging large language models (LLMs) for open-ended reasoning. However,\ntheir generalization degrades significantly when transferred to out-of-domain\nscenarios such as remote sensing, medical imaging, or math diagrams, due to\nlarge distributional shifts and the lack of effective domain adaptation\nmechanisms. Existing approaches typically rely on per-domain fine-tuning or\nbespoke pipelines, which are costly, inflexible, and not scalable across\ndiverse tasks. In this paper, we propose CATCH, a plug-and-play framework for\ncross-domain adaptation that improves the generalization of VQA models while\nrequiring minimal changes to their core architecture. Our key idea is to\ndecouple visual and linguistic adaptation by introducing two lightweight\nmodules: a domain classifier to identify the input image type, and a dual\nadapter mechanism comprising a Prompt Adapter for language modulation and a\nVisual Adapter for vision feature adjustment. Both modules are dynamically\ninjected via a unified hook interface, requiring no retraining of the backbone\nmodel. Experimental results across four domain-specific VQA benchmarks\ndemonstrate that our framework achieves consistent performance gains without\nretraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on\nMedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH\nprovides a scalable and extensible approach to multi-domain VQA, enabling\npractical deployment across diverse application domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCATCH\u6846\u67b6\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u8de8\u9886\u57df\u89c6\u89c9\u95ee\u7b54\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u548c\u8bed\u8a00\u9002\u5e94\u6a21\u5757\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u4e3b\u5e72\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u591a\u9886\u57dfVQA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\u5728\u81ea\u7136\u56fe\u50cf\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9065\u611f\u3001\u533b\u5b66\u5f71\u50cf\u3001\u6570\u5b66\u56fe\u8868\u7b49\u8de8\u9886\u57df\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u7531\u4e8e\u5206\u5e03\u504f\u79fb\u548c\u7f3a\u4e4f\u6709\u6548\u7684\u9886\u57df\u9002\u5e94\u673a\u5236\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u6216\u5b9a\u5236\u6d41\u7a0b\uff0c\u6210\u672c\u9ad8\u3001\u7075\u6d3b\u6027\u5dee\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u591a\u6837\u5316\u4efb\u52a1\u3002", "method": "CATCH\u6846\u67b6\u5f15\u5165\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u5757\uff1a\u9886\u57df\u5206\u7c7b\u5668\u7528\u4e8e\u8bc6\u522b\u8f93\u5165\u56fe\u50cf\u7c7b\u578b\uff0c\u4ee5\u53ca\u5305\u542b\u63d0\u793a\u9002\u914d\u5668\u548c\u89c6\u89c9\u9002\u914d\u5668\u7684\u53cc\u9002\u914d\u673a\u5236\u3002\u4e24\u4e2a\u6a21\u5757\u901a\u8fc7\u7edf\u4e00\u94a9\u5b50\u63a5\u53e3\u52a8\u6001\u6ce8\u5165\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u4e3b\u5e72\u6a21\u578b\uff0c\u5b9e\u73b0\u89c6\u89c9\u7279\u5f81\u8c03\u6574\u548c\u8bed\u8a00\u8c03\u5236\u7684\u89e3\u8026\u9002\u5e94\u3002", "result": "\u5728\u56db\u4e2a\u9886\u57df\u7279\u5b9aVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCATCH\u6846\u67b6\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u4e3b\u5e72\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff1aMathVQA\u4e0aBLEU\u63d0\u53472.3\u5206\uff0cMedVQA-RAD\u4e0aVQA\u5f97\u5206\u63d0\u53472.6\u5206\uff0cChartQA\u4e0aROUGE\u63d0\u53473.1\u5206\u3002", "conclusion": "CATCH\u4e3a\u591a\u9886\u57dfVQA\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u673a\u5236\u5b9e\u73b0\u8de8\u9886\u57df\u6cdb\u5316\uff0c\u652f\u6301\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e94\u7528\u4e8e\u591a\u6837\u5316\u5e94\u7528\u9886\u57df\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9886\u57df\u9002\u5e94\u7684\u6210\u672c\u548c\u590d\u6742\u6027\u3002"}}
{"id": "2510.26583", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26583", "abs": "https://arxiv.org/abs/2510.26583", "authors": ["Yufeng Cui", "Honghao Chen", "Haoge Deng", "Xu Huang", "Xinghang Li", "Jirong Liu", "Yang Liu", "Zhuoyan Luo", "Jinsheng Wang", "Wenxuan Wang", "Yueze Wang", "Chengyuan Wang", "Fan Zhang", "Yingli Zhao", "Ting Pan", "Xianduo Li", "Zecheng Hao", "Wenxuan Ma", "Zhuo Chen", "Yulong Ao", "Tiejun Huang", "Zhongyuan Wang", "Xinlong Wang"], "title": "Emu3.5: Native Multimodal Models are World Learners", "comment": "project page: https://emu.world", "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Emu3.5\uff0c\u4e00\u4e2a\u901a\u8fc7\u7aef\u5230\u7aef\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u539f\u751f\u9884\u6d4b\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u4e0b\u4e00\u4e2a\u72b6\u6001\uff0c\u5e76\u5f15\u5165\u79bb\u6563\u6269\u6563\u9002\u914d\u6280\u672f\u5b9e\u73b020\u500d\u63a8\u7406\u52a0\u901f\uff0c\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0eGemini 2.5 Flash Image\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u4ea4\u7ec7\u751f\u6210\u3001\u957f\u65f6\u5e8f\u4e00\u81f4\u6027\u4fdd\u6301\u4ee5\u53ca\u63a8\u7406\u6548\u7387\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0cEmu3.5\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u539f\u751f\u5904\u7406\u89c6\u89c9\u8bed\u8a00\u4ea4\u7ec7\u8f93\u5165\u8f93\u51fa\u3001\u5177\u5907\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u4e14\u63a8\u7406\u9ad8\u6548\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u3002", "method": "Emu3.5\u91c7\u7528\u7edf\u4e00\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u76ee\u6807\u5728\u8d85\u8fc710\u4e07\u4ebftoken\u7684\u89c6\u89c9\u8bed\u8a00\u4ea4\u7ec7\u6570\u636e\u4e0a\u8fdb\u884c\u7aef\u5230\u7aef\u9884\u8bad\u7ec3\uff0c\u968f\u540e\u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u591a\u6a21\u6001\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u79bb\u6563\u6269\u6563\u9002\u914d\u6280\u672f\u5c06\u9010token\u89e3\u7801\u8f6c\u6362\u4e3a\u53cc\u5411\u5e76\u884c\u9884\u6d4b\u4ee5\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "result": "Emu3.5\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0eGemini 2.5 Flash Image\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5728\u4ea4\u7ec7\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u63a8\u7406\u52a0\u901f\u7ea620\u500d\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u957f\u65f6\u5e8f\u89c6\u89c9\u8bed\u8a00\u751f\u6210\u3001\u4efb\u610f\u5230\u56fe\u50cf\u751f\u6210\u3001\u590d\u6742\u6587\u672c\u56fe\u50cf\u751f\u6210\u4ee5\u53ca\u65f6\u7a7a\u4e00\u81f4\u6027\u4e16\u754c\u63a2\u7d22\u7b49\u5f3a\u5927\u80fd\u529b\u3002", "conclusion": "Emu3.5\u8bc1\u660e\u4e86\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u6784\u5efa\u5177\u5907\u901a\u7528\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5176\u5f00\u6e90\u53d1\u5e03\u5c06\u63a8\u52a8\u793e\u533a\u5728\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u548c\u5f00\u653e\u4e16\u754c\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.26641", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26641", "abs": "https://arxiv.org/abs/2510.26641", "authors": ["Sayed Pedram Haeri Boroujeni", "Niloufar Mehrabi", "Hazim Alzorgan", "Ahmad Sarlak", "Mahlagha Fazeli", "Abolfazl Razi"], "title": "All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles", "comment": null, "summary": "Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7269\u4f53\u68c0\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u524d\u77bb\u6027\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u751f\u6210\u5f0fAI\u7b49\u65b0\u5174\u8303\u5f0f\uff0c\u901a\u8fc7\u7cfb\u7edf\u68b3\u7406\u4f20\u611f\u5668\u878d\u5408\u3001\u6570\u636e\u96c6\u5206\u7c7b\u548c\u5148\u8fdb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u5f53\u524d\u80fd\u529b\u3001\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u673a\u9047\u7ed8\u5236\u4e86\u6e05\u6670\u8def\u7ebf\u56fe\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u5728\u590d\u6742\u591a\u6a21\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u7269\u4f53\u68c0\u6d4b\uff0c\u4f46\u5f53\u524d\u77e5\u8bc6\u5728\u8de8\u6a21\u6001\u611f\u77e5\u3001\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u534f\u540c\u667a\u80fd\u65b9\u9762\u4ecd\u7136\u788e\u7247\u5316\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u907f\u514d\u91cd\u65b0\u5ba1\u89c6\u8fc7\u65f6\u6280\u672f\u3002", "method": "\u7814\u7a76\u7cfb\u7edf\u56de\u987e\u4e86\u81ea\u52a8\u9a7e\u9a76\u4f20\u611f\u5668\uff08\u76f8\u673a\u3001\u8d85\u58f0\u6ce2\u3001\u6fc0\u5149\u96f7\u8fbe\u548c\u96f7\u8fbe\uff09\u53ca\u5176\u878d\u5408\u7b56\u7565\uff0c\u5f15\u5165\u4e86\u8d85\u8d8a\u7b80\u5355\u6536\u96c6\u7684\u7ed3\u6784\u5316\u6570\u636e\u96c6\u5206\u7c7b\uff0c\u5305\u62ec\u81ea\u8f66\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u534f\u540c\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u4e86\u4ece2D/3D\u68c0\u6d4b\u6d41\u7a0b\u5230\u6df7\u5408\u4f20\u611f\u5668\u878d\u5408\u7684\u524d\u6cbf\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u7531\u89c6\u89c9\u53d8\u6362\u5668\u3001\u5927\u5c0f\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u53d8\u6362\u5668\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u7efc\u5408\u591a\u89c6\u89d2\u5206\u6790\uff0c\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u5f53\u524d\u80fd\u529b\u3001\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u673a\u9047\u7684\u6e05\u6670\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u4e86\u65b0\u5174\u8303\u5f0f\u4e0e\u4f20\u7edf\u4f20\u611f\u5668\u878d\u5408\u7684\u6574\u5408\u6f5c\u529b\uff0c\u5e76\u5bf9\u4e0d\u540c\u6570\u636e\u7ed3\u6784\u548c\u7279\u5f81\u8fdb\u884c\u4e86\u4ea4\u53c9\u5206\u6790\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u901a\u8fc7\u6574\u5408\u65b0\u5174AI\u8303\u5f0f\u4e0e\u4f20\u7edf\u611f\u77e5\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7269\u4f53\u68c0\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u53d1\u5c55\u84dd\u56fe\uff0c\u6307\u51fa\u4e86\u5411\u591a\u6a21\u6001\u534f\u540c\u667a\u80fd\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7cfb\u7edf\u6f14\u8fdb\u7684\u5173\u952e\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.26769", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26769", "abs": "https://arxiv.org/abs/2510.26769", "authors": ["Anushka Sivakumar", "Andrew Zhang", "Zaber Hakim", "Chris Thomas"], "title": "SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models", "comment": null, "summary": "This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSteerVLM\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5f15\u5bfc\u6a21\u5757\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u8bed\u8a00\u6a21\u6001\u4e0e\u56fe\u50cf\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u6fc0\u6d3b\u8fde\u63a5\uff0c\u5b9e\u73b0\u65e0\u9700\u4fee\u6539\u6a21\u578b\u6743\u91cd\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\u65f6\u63a7\u5236\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u5b66\u4e60\u539f\u59cbVLM\u53c2\u65700.14%\u7684\u53c2\u6570\u91cf\uff0c\u5728\u5f15\u5bfc\u548c\u5e7b\u89c9\u7f13\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u5e72\u9884\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8f93\u51fa\u8bed\u4e49\u63a7\u5236\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u63a8\u7406\u65f6\u5bf9\u590d\u6742\u8f93\u51fa\u8bed\u4e49\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u975e\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u4e14\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9884\u63d0\u53d6\u9759\u6001\u5411\u91cf\u6216\u624b\u52a8\u8c03\u6574\u5e72\u9884\u70b9\uff0c\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u6027\u3002", "method": "SteerVLM\u901a\u8fc7\u5b66\u4e60\u7f16\u7801\u76ee\u6807\u884c\u4e3a\u548c\u76f8\u53cd\u884c\u4e3a\u7684\u914d\u5bf9\u63d0\u793a\u7684\u6f5c\u5728\u5d4c\u5165\uff0c\u52a8\u6001\u8c03\u6574\u8fde\u63a5\u8bed\u8a00\u6a21\u6001\u4e0e\u56fe\u50cf\u4e0a\u4e0b\u6587\u7684\u6fc0\u6d3b\u8fde\u63a5\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u7ef4\u5ea6\u7ea7\u6fc0\u6d3b\u8c03\u5236\u548c\u8de8\u5c42\u81ea\u9002\u5e94\u5f15\u5bfc\uff0c\u65e0\u9700\u9884\u63d0\u53d6\u9759\u6001\u5411\u91cf\u6216\u624b\u52a8\u8c03\u6574\u5e72\u9884\u70b9\u3002\u540c\u65f6\u5f15\u5165\u4e86VNIA\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30VLM\u5f15\u5bfc\u6280\u672f\u3002", "result": "SteerVLM\u5728VLM\u5f15\u5bfc\u548c\u5e7b\u89c9\u7f13\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u5e72\u9884\u6280\u672f\uff0c\u4ec5\u9700\u5b66\u4e60\u539f\u59cbVLM\u53c2\u65700.14%\u7684\u53c2\u6570\u91cf\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a8\u7406\u65f6\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u975e\u76ee\u6807\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6fc0\u6d3b\u5de5\u7a0b\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u63a7\u5236\u63d0\u4f9b\u4e86\u7a33\u5065\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u5f15\u5bfc\u6a21\u5757\u5728\u5b9e\u73b0\u590d\u6742\u8f93\u51fa\u8bed\u4e49\u63a7\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\u3002SteerVLM\u7684\u65b9\u6cd5\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7cbe\u786e\u884c\u4e3a\u8c03\u63a7\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.26781", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26781", "abs": "https://arxiv.org/abs/2510.26781", "authors": ["Aniruddh Bansal", "Davit Soselia", "Dang Nguyen", "Tianyi Zhou"], "title": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment", "comment": null, "summary": "Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs' capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ChartAlign\u57fa\u51c6\u6d4b\u8bd5\uff08ChartAB\uff09\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u56fe\u8868\u5143\u7d20\u5b9a\u4f4d\u3001\u5c5e\u6027\u8bc6\u522b\u548c\u591a\u56fe\u8868\u6bd4\u8f83\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u51c6\u786e\u7684\u7ec6\u8282\u611f\u77e5\u80fd\u529b\uff0c\u96be\u4ee5\u63d0\u53d6\u56fe\u8868\u7684\u7ec6\u7c92\u5ea6\u7ed3\u6784\uff0c\u8fd9\u79cd\u56fe\u8868\u5b9a\u4f4d\u80fd\u529b\u7684\u9650\u5236\u8fdb\u4e00\u6b65\u963b\u788d\u4e86\u6a21\u578b\u8fdb\u884c\u591a\u56fe\u8868\u6bd4\u8f83\u548c\u63a8\u7406\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86ChartAlign\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528JSON\u6a21\u677f\u6765\u4fc3\u8fdb\u9488\u5bf9\u6bcf\u4e2a\u5b9a\u4f4d\u4efb\u52a1\u7684\u8bc4\u4f30\u6307\u6807\u8ba1\u7b97\uff0c\u5e76\u5f15\u5165\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u63a8\u7406\u5de5\u4f5c\u6d41\u7a0b\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u4e2a\u56fe\u8868\u95f4\u5bf9\u9f50\u548c\u6bd4\u8f83\u5143\u7d20/\u5c5e\u6027\u7684\u80fd\u529b\u3002", "result": "\u5bf9\u591a\u4e2a\u6700\u65b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u5206\u6790\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u611f\u77e5\u504f\u5dee\u3001\u5f31\u70b9\u3001\u9c81\u68d2\u6027\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u8fd9\u4e9b\u53d1\u73b0\u7a81\u51fa\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u7ec6\u7c92\u5ea6\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6307\u51fa\u4e86\u5f53\u524d\u6a21\u578b\u9700\u8981\u52a0\u5f3a\u7684\u5177\u4f53\u6280\u80fd\uff0c\u4e3a\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u9886\u57df\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u5728\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2510.26794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26794", "abs": "https://arxiv.org/abs/2510.26794", "authors": ["Jing Lin", "Ruisi Wang", "Junzhe Lu", "Ziqi Huang", "Guorui Song", "Ailing Zeng", "Xian Liu", "Chen Wei", "Wanqi Yin", "Qingping Sun", "Zhongang Cai", "Lei Yang", "Ziwei Liu"], "title": "The Quest for Generalizable Motion Generation: Data, Model, and Evaluation", "comment": null, "summary": "Despite recent advances in 3D human motion generation (MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotably video generation (ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, a flow-matching-based diffusion transformer that unifies priors from\nMoCap data and ViGen models through gated multimodal conditioning. To enhance\nefficiency, we further develop ViMoGen-light, a distilled variant that\neliminates video generation dependencies while preserving strong\ngeneralization. Finally, we present MBench, a hierarchical benchmark designed\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u89c6\u9891\u751f\u6210\u54113D\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u7cfb\u7edf\u5316\u8fc1\u79fb\u77e5\u8bc6\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6ViMoGen-228K\u3001\u5f00\u53d1\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6269\u6563\u53d8\u6362\u5668\u6a21\u578b\u4ee5\u53ca\u5efa\u7acb\u5206\u5c42\u8bc4\u4f30\u57fa\u51c6MBench\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u751f\u6210\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u67093D\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u6a21\u578b\u5728\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u74f6\u9888\uff0c\u800c\u76f8\u90bb\u7684\u89c6\u9891\u751f\u6210\u9886\u57df\u5728\u5efa\u6a21\u4eba\u7c7b\u884c\u4e3a\u65b9\u9762\u5df2\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u8fd9\u4e3a\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u8fc1\u79fb\u7684\u6d1e\u5bdf\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6269\u6563\u53d8\u6362\u5668ViMoGen\uff0c\u901a\u8fc7\u95e8\u63a7\u591a\u6a21\u6001\u6761\u4ef6\u673a\u5236\u7edf\u4e00\u4e86MoCap\u6570\u636e\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff1b\u540c\u65f6\u5f00\u53d1\u4e86ViMoGen-light\u84b8\u998f\u53d8\u4f53\uff0c\u5728\u6d88\u9664\u89c6\u9891\u751f\u6210\u4f9d\u8d56\u7684\u540c\u65f6\u4fdd\u6301\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u81ea\u52a8\u8bc4\u4f30\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6784\u5efa\u7684ViMoGen-228K\u6570\u636e\u96c6\u5305\u542b228,000\u4e2a\u9ad8\u8d28\u91cf\u8fd0\u52a8\u6837\u672c\uff0c\u5927\u5e45\u6269\u5c55\u4e86\u8bed\u4e49\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u4ece\u89c6\u9891\u751f\u6210\u5411\u8fd0\u52a8\u751f\u6210\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\u7684\u6709\u6548\u6027\uff0c\u4e3a\u63d0\u5347\u8fd0\u52a8\u751f\u6210\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u8de8\u6a21\u6001\u751f\u6210\u4efb\u52a1\u7684\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2510.26795", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26795", "abs": "https://arxiv.org/abs/2510.26795", "authors": ["Philipp Lindenberger", "Paul-Edouard Sarlin", "Jan Hosang", "Matteo Balice", "Marc Pollefeys", "Simon Lynen", "Eduard Trulls"], "title": "Scaling Image Geo-Localization to Continent Level", "comment": "NeurIPS 2025", "summary": "Determining the precise geographic location of an image at a global scale\nremains an unsolved challenge. Standard image retrieval techniques are\ninefficient due to the sheer volume of images (>100M) and fail when coverage is\ninsufficient. Scalable solutions, however, involve a trade-off: global\nclassification typically yields coarse results (10+ kilometers), while\ncross-view retrieval between ground and aerial imagery suffers from a domain\ngap and has been primarily studied on smaller regions. This paper introduces a\nhybrid approach that achieves fine-grained geo-localization across a large\ngeographic expanse the size of a continent. We leverage a proxy classification\ntask during training to learn rich feature representations that implicitly\nencode precise location information. We combine these learned prototypes with\nembeddings of aerial imagery to increase robustness to the sparsity of\nground-level data. This enables direct, fine-grained retrieval over areas\nspanning multiple countries. Our extensive evaluation demonstrates that our\napproach can localize within 200m more than 68\\% of queries of a dataset\ncovering a large part of Europe. The code is publicly available at\nhttps://scaling-geoloc.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7406\u5206\u7c7b\u4efb\u52a1\u5b66\u4e60\u4e30\u5bcc\u7279\u5f81\u8868\u793a\uff0c\u7ed3\u5408\u5730\u9762\u548c\u822a\u7a7a\u56fe\u50cf\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u5728\u5927\u9646\u5c3a\u5ea6\u4e0a\u7684\u7ec6\u7c92\u5ea6\u5730\u7406\u5b9a\u4f4d\uff0c\u80fd\u591f\u5728\u6b27\u6d32\u5730\u533a68%\u7684\u67e5\u8be2\u4e2d\u5b9a\u4f4d\u5230200\u7c73\u8303\u56f4\u5185\u3002", "motivation": "\u5168\u7403\u5c3a\u5ea6\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u9762\u4e34\u6807\u51c6\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u548c\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u7c97\u7c92\u5ea6\u5206\u7c7b\u4e0e\u8de8\u89c6\u56fe\u68c0\u7d22\u9886\u57df\u5dee\u8ddd\u7684\u6743\u8861\uff0c\u9700\u8981\u5728\u5927\u5730\u7406\u8303\u56f4\u5185\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u5229\u7528\u4ee3\u7406\u5206\u7c7b\u4efb\u52a1\u5b66\u4e60\u9690\u542b\u7f16\u7801\u7cbe\u786e\u4f4d\u7f6e\u4fe1\u606f\u7684\u4e30\u5bcc\u7279\u5f81\u8868\u793a\uff0c\u5e76\u5c06\u5b66\u4e60\u5230\u7684\u539f\u578b\u4e0e\u822a\u7a7a\u56fe\u50cf\u5d4c\u5165\u76f8\u7ed3\u5408\uff0c\u589e\u5f3a\u5bf9\u5730\u9762\u6570\u636e\u7a00\u758f\u6027\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u8986\u76d6\u6b27\u6d32\u5927\u90e8\u5206\u5730\u533a\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u572868%\u4ee5\u4e0a\u7684\u67e5\u8be2\u4e2d\u5b9e\u73b0200\u7c73\u8303\u56f4\u5185\u7684\u7cbe\u786e\u5b9a\u4f4d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u901a\u8fc7\u7ed3\u5408\u5b66\u4e60\u7279\u5f81\u548c\u8de8\u89c6\u56fe\u5d4c\u5165\uff0c\u53ef\u4ee5\u5728\u5927\u9646\u5c3a\u5ea6\u4e0a\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5730\u7406\u5b9a\u4f4d\uff0c\u4e3a\u5927\u89c4\u6a21\u5730\u7406\u5b9a\u4f4d\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26800", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26800", "abs": "https://arxiv.org/abs/2510.26800", "authors": ["Yukun Huang", "Jiwen Yu", "Yanning Zhou", "Jianan Wang", "Xintao Wang", "Pengfei Wan", "Xihui Liu"], "title": "OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes", "comment": "Project page: https://yukun-huang.github.io/OmniX/", "summary": "There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OmniX\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u5229\u75282D\u751f\u6210\u5148\u9a8c\u5b9e\u73b0\u5168\u666f\u611f\u77e5\uff0c\u80fd\u591f\u751f\u6210\u9002\u7528\u4e8e\u7269\u7406\u6e32\u67d3\u3001\u91cd\u5149\u7167\u548c\u4eff\u771f\u7684\u56fe\u5f62\u5c31\u7eea3D\u573a\u666f\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf2D\u63d0\u5347\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5916\u89c2\u751f\u6210\u7684\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5168\u666f\u56fe\u76842D\u63d0\u5347\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5916\u89c2\u751f\u6210\uff0c\u800c\u5ffd\u7565\u4e86\u5185\u5728\u5c5e\u6027\u7684\u611f\u77e5\uff0c\u65e0\u6cd5\u751f\u6210\u9002\u7528\u4e8e\u7269\u7406\u6e32\u67d3\u3001\u91cd\u5149\u7167\u548c\u4eff\u771f\u7684\u56fe\u5f62\u5c31\u7eea3D\u573a\u666f\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u5c06\u5168\u666f\u56fe\u6280\u672f\u63a8\u8fdb\u5230\u80fd\u591f\u751f\u6210\u5177\u5907\u5b8c\u6574\u7269\u7406\u5c5e\u6027\u76843D\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86OmniX\u7edf\u4e00\u6846\u67b6\uff0c\u57fa\u4e8e\u8f7b\u91cf\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u9002\u914d\u5668\u7ed3\u6784\uff0c\u91cd\u65b0\u5229\u75282D\u751f\u6210\u5148\u9a8c\u8fdb\u884c\u51e0\u4f55\u3001\u7eb9\u7406\u548cPBR\u6750\u8d28\u7684\u5168\u666f\u611f\u77e5\u3002\u540c\u65f6\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u5408\u6210\u5168\u666f\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea\u591a\u6837\u5316\u5ba4\u5185\u5916\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u5168\u666f\u56fe\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6a21\u578b\u5728\u5168\u666f\u89c6\u89c9\u611f\u77e5\u548c\u56fe\u5f62\u5c31\u7eea3D\u573a\u666f\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6c89\u6d78\u5f0f\u548c\u7269\u7406\u771f\u5b9e\u7684\u865a\u62df\u4e16\u754c\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u91cd\u65b0\u5229\u75282D\u751f\u6210\u5148\u9a8c\u8fdb\u884c\u5168\u666f\u611f\u77e5\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u751f\u6210\u5177\u5907\u5b8c\u6574\u7269\u7406\u5c5e\u6027\u76843D\u573a\u666f\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u6c89\u6d78\u5f0f\u865a\u62df\u73af\u5883\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
