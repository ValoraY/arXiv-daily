<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 20]
- [cs.CL](#cs.CL) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs](https://arxiv.org/abs/2511.04727)
*Ali Faraz, Akash, Shaharukh Khan, Raja Kolla, Akshat Patidar, Suranjan Goswami, Abhinav Ravi, Chandra Khatri, Shubham Agarwal*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†IndicVisionBenchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“æ³¨äºå°åº¦æ¬¡å¤§é™†çš„å¤§è§„æ¨¡å¤šè¯­è¨€è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–è‹±è¯­å’Œ10ç§å°åº¦è¯­è¨€ï¼ŒåŒ…å«3ç§å¤šæ¨¡æ€ä»»åŠ¡å’Œ13ä¸ªæ–‡åŒ–ä¸»é¢˜ï¼Œæ­ç¤ºäº†å½“å‰VLMåœ¨æ–‡åŒ–å¤šæ ·æ€§ç¯å¢ƒä¸­çš„æ˜¾è‘—æ€§èƒ½å·®è·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å¤§å¤šæ•°è¯„ä¼°åŸºå‡†ä»ç„¶ä»¥è¥¿æ–¹ä¸ºä¸­å¿ƒï¼Œç¼ºä¹å¯¹æ–‡åŒ–å¤šæ ·æ€§å’Œå¤šè¯­è¨€ç¯å¢ƒä¸‹çš„æ€§èƒ½è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨å°åº¦æ¬¡å¤§é™†è¿™æ ·å…·æœ‰ä¸°å¯Œè¯­è¨€å’Œæ–‡åŒ–å¤šæ ·æ€§çš„åœ°åŒºå­˜åœ¨æ˜æ˜¾çš„ç ”ç©¶ç©ºç™½ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†IndicVisionBenchåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–è‹±è¯­å’Œ10ç§å°åº¦è¯­è¨€ï¼ŒåŒ…å«å…‰å­¦å­—ç¬¦è¯†åˆ«ã€å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘å’Œè§†è§‰é—®ç­”3ç§å¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ¶‰åŠ6ç§é—®é¢˜ç±»å‹ï¼ŒåŒ…å«çº¦5Kå›¾åƒå’Œ37K+é—®ç­”å¯¹ï¼Œè¦†ç›–13ä¸ªæ–‡åŒ–ä¸»é¢˜ï¼Œå¹¶å‘å¸ƒäº†10ç§å°åº¦è¯­è¨€çš„å¹¶è¡Œæ ‡æ³¨è¯­æ–™åº“ã€‚

**Result:** è¯„ä¼°äº†8ä¸ªä»ä¸“æœ‰é—­æºç³»ç»Ÿåˆ°å¼€æºæƒé‡çš„ä¸­å¤§è§„æ¨¡æ¨¡å‹ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–‡åŒ–å¤šæ ·æ€§ç¯å¢ƒä¸­å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œçªæ˜¾äº†ç°æœ‰æ¨¡å‹åœ¨è·¨æ–‡åŒ–å’Œå¤šè¯­è¨€åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚

**Conclusion:** IndicVisionBenché€šè¿‡å…³æ³¨æ–‡åŒ–å¤šæ ·æ€§å’Œå¤šè¯­è¨€æ€§ï¼Œå»ºç«‹äº†ä¸€ä¸ªå¯å¤ç°çš„è¯„ä¼°æ¡†æ¶ï¼Œä¸ºæ›´åŒ…å®¹çš„å¤šæ¨¡æ€ç ”ç©¶é“ºå¹³äº†é“è·¯ï¼Œå¼ºè°ƒäº†å¼€å‘èƒ½å¤Ÿæ›´å¥½é€‚åº”å…¨çƒæ–‡åŒ–å¤šæ ·æ€§çš„è§†è§‰è¯­è¨€æ¨¡å‹çš„å¿…è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLMs) have demonstrated impressive generalization
across multimodal tasks, yet most evaluation benchmarks remain Western-centric,
leaving open questions about their performance in culturally diverse and
multilingual settings. To address this gap, we introduce IndicVisionBench, the
first large-scale benchmark centered on the Indian subcontinent. Covering
English and 10 Indian languages, our benchmark spans 3 multimodal tasks,
including Optical Character Recognition (OCR), Multimodal Machine Translation
(MMT), and Visual Question Answering (VQA), covering 6 kinds of question types.
Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across
13 culturally grounded topics. In addition, we release a paired parallel corpus
of annotations across 10 Indic languages, creating a unique resource for
analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum
of 8 models, from proprietary closed-source systems to open-weights medium and
large-scale models. Our experiments reveal substantial performance gaps,
underscoring the limitations of current VLMs in culturally diverse contexts. By
centering cultural diversity and multilinguality, IndicVisionBench establishes
a reproducible evaluation framework that paves the way for more inclusive
multimodal research.


### [2] [CPO: Condition Preference Optimization for Controllable Image Generation](https://arxiv.org/abs/2511.04753)
*Zonglin Lyu, Ming Li, Xinxin Liu, Chen Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºæ¡ä»¶åå¥½ä¼˜åŒ–ï¼ˆCPOï¼‰ï¼Œé€šè¿‡ç›´æ¥åœ¨æ§åˆ¶ä¿¡å·è€Œéç”Ÿæˆå›¾åƒä¸Šè¿›è¡Œåå¥½å­¦ä¹ ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ä¼˜åŒ–å¯æ§æ€§æ—¶é¢ä¸´çš„ä¸ç¡®å®šæ€§å’Œæ··æ‚å› ç´ é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å¤šç§æ§åˆ¶ç±»å‹çš„å¯æ§æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•å¦‚ControlNet++ä»…ä¼˜åŒ–ä½å™ªå£°æ—¶é—´æ­¥ï¼Œå¿½ç•¥äº†é«˜å™ªå£°æ—¶é—´æ­¥çš„è´¡çŒ®å¹¶å¼•å…¥è¿‘ä¼¼è¯¯å·®ï¼Œè€Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç”±äºç”Ÿæˆæ¨¡å‹çš„ä¸ç¡®å®šæ€§ï¼Œéš¾ä»¥ç¡®ä¿èƒœè´Ÿå›¾åƒå¯¹ä»…åœ¨å¯æ§æ€§ä¸Šå­˜åœ¨å·®å¼‚è€Œä¿æŒå…¶ä»–å› ç´ ä¸å˜ã€‚

**Method:** æå‡ºæ¡ä»¶åå¥½ä¼˜åŒ–ï¼ˆCPOï¼‰ï¼Œé€šè¿‡æ„å»ºèƒœè´Ÿæ§åˆ¶ä¿¡å·ï¼ˆc^wå’Œc^lï¼‰å¹¶è®­ç»ƒæ¨¡å‹åå¥½c^wï¼Œç›´æ¥åœ¨æ§åˆ¶æ¡ä»¶è€Œéç”Ÿæˆå›¾åƒä¸Šè¿›è¡Œåå¥½å­¦ä¹ ï¼Œæ¶ˆé™¤äº†æ··æ‚å› ç´ å¹¶è·å¾—äº†ä½æ–¹å·®è®­ç»ƒç›®æ ‡ã€‚

**Result:** CPOåœ¨å¤šä¸ªæ§åˆ¶ç±»å‹ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„ControlNet++ï¼šåˆ†å‰²ä»»åŠ¡é”™è¯¯ç‡é™ä½è¶…è¿‡10%ï¼Œäººä½“å§¿æ€ä»»åŠ¡é™ä½70-80%ï¼Œè¾¹ç¼˜å’Œæ·±åº¦å›¾ä»»åŠ¡ä¸€è‡´é™ä½2-5%ã€‚

**Conclusion:** CPOä¸ä»…åœ¨ç†è®ºä¸Šå±•ç°å‡ºæ¯”DPOæ›´ä½çš„å¯¹æ¯”æŸå¤±æ–¹å·®ï¼Œå®è¯ç»“æœä¹Ÿæ›´ä¼˜ï¼ŒåŒæ—¶å‡å°‘äº†æ•°æ®é›†æ„å»ºçš„è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ï¼Œä¸ºå¯æ§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæä¾›äº†æ›´æœ‰æ•ˆçš„ä¼˜åŒ–æ–¹æ³•ã€‚

---

#### ğŸ“„ Abstract
To enhance controllability in text-to-image generation, ControlNet introduces
image-based control signals, while ControlNet++ improves pixel-level cycle
consistency between generated images and the input control signal. To avoid the
prohibitive cost of back-propagating through the sampling process, ControlNet++
optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step
approximation, which not only ignores the contribution of high-noise timesteps
but also introduces additional approximation errors. A straightforward
alternative for optimizing controllability across all timesteps is Direct
Preference Optimization (DPO), a fine-tuning method that increases model
preference for more controllable images ($I^{w}$) over less controllable ones
($I^{l}$). However, due to uncertainty in generative models, it is difficult to
ensure that win--lose image pairs differ only in controllability while keeping
other factors, such as image quality, fixed. To address this, we propose
performing preference learning over control conditions rather than generated
images. Specifically, we construct winning and losing control signals,
$\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer
$\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference
Optimization} (CPO), eliminates confounding factors and yields a low-variance
training objective. Our approach theoretically exhibits lower contrastive loss
variance than DPO and empirically achieves superior results. Moreover, CPO
requires less computation and storage for dataset curation. Extensive
experiments show that CPO significantly improves controllability over the
state-of-the-art ControlNet++ across multiple control types: over $10\%$ error
rate reduction in segmentation, $70$--$80\%$ in human pose, and consistent
$2$--$5\%$ reductions in edge and depth maps.


### [3] [An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention](https://arxiv.org/abs/2511.04811)
*Shuo Zhao, Yu Zhou, Jianxu Chen*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ•°æ®ä¸­å¿ƒçš„AIå·¥ä½œæµï¼Œé€šè¿‡ç»“åˆä¸»åŠ¨å­¦ä¹ å’Œä¼ªæ ‡ç­¾æŠ€æœ¯ï¼Œå°†ä¼ ç»Ÿç¥ç»ç½‘ç»œä¸å¤§å‹åŸºç¡€æ¨¡å‹çš„ä¼˜åŠ¿ç›¸ç»“åˆï¼Œæ˜¾è‘—å‡å°‘äº†ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„äººå·¥æ ‡æ³¨éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´ä¸¤ä¸ªä¸»è¦ç“¶é¢ˆï¼šä¼ ç»Ÿæ–¹æ³•å¦‚nnU-Netéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®è¿›è¡Œäº¤å‰éªŒè¯ï¼Œè€Œå¤§å‹åŸºç¡€æ¨¡å‹è™½å…·å¤‡é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ä½†åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¡¨ç°ä¸ä½³ã€‚å½“åªæœ‰åŸå§‹å›¾åƒè€Œæ— æ ‡æ³¨æ•°æ®å¯ç”¨æ—¶ï¼Œè¿™äº›é™åˆ¶å°¤ä¸ºçªå‡ºï¼Œé˜»ç¢äº†å…ˆè¿›AIæŠ€æœ¯åœ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨æ•°æ®ä¸­å¿ƒçš„AIå·¥ä½œæµç¨‹ï¼Œé¦–å…ˆåˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œè¿™äº›ä¼ªæ ‡ç­¾ç”¨äºnnU-Netçš„è‡ªé…ç½®è¿‡ç¨‹ã€‚éšåé€šè¿‡ä¸»åŠ¨å­¦ä¹ é€‰æ‹©ä»£è¡¨æ€§æ ¸å¿ƒé›†è¿›è¡Œæœ€å°åŒ–äººå·¥æ ‡æ³¨ï¼Œå®ç°nnU-Netæ¨¡å‹çš„æœ‰æ•ˆå¾®è°ƒã€‚è¯¥æµç¨‹ç»“åˆäº†ä¼ ç»Ÿç¥ç»ç½‘ç»œå’Œå¤§å‹åŸºç¡€æ¨¡å‹çš„å„è‡ªä¼˜åŠ¿ï¼ŒåŒæ—¶æœ€å¤§ç¨‹åº¦å‡å°‘äººå·¥å¹²é¢„ã€‚

**Result:** è¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ï¼ŒåŒæ—¶åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä¿æŒäº†ç«äº‰æ€§çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡ä¼ªæ ‡ç­¾å’Œä¸»åŠ¨å­¦ä¹ çš„ç»“åˆï¼Œèƒ½å¤Ÿåœ¨ä»…éœ€å°‘é‡äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹è¾¾åˆ°æ¥è¿‘å…¨ç›‘ç£æ–¹æ³•çš„æ€§èƒ½æ°´å¹³ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºç”Ÿç‰©åŒ»å­¦ç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¯è®¿é—®çš„è§£å†³æ–¹æ¡ˆï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿåœ¨åˆ†å‰²ä»»åŠ¡ä¸­åº”ç”¨æœ€å…ˆè¿›çš„AIæŠ€æœ¯ã€‚å·¥ä½œæµçš„è®¾è®¡å¹³è¡¡äº†è‡ªåŠ¨åŒ–ä¸æ€§èƒ½ï¼Œå±•ç¤ºäº†æ•°æ®é©±åŠ¨æ–¹æ³•åœ¨å‡å°‘æ ‡æ³¨è´Ÿæ‹…æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºèµ„æºå—é™çš„ç ”ç©¶ç¯å¢ƒæä¾›äº†å®ç”¨å·¥å…·ã€‚

---

#### ğŸ“„ Abstract
Biomedical image segmentation is critical for precise structure delineation
and downstream analysis. Traditional methods often struggle with noisy data,
while deep learning models such as U-Net have set new benchmarks in
segmentation performance. nnU-Net further automates model configuration, making
it adaptable across datasets without extensive tuning. However, it requires a
substantial amount of annotated data for cross-validation, posing a challenge
when only raw images but no labels are available. Large foundation models offer
zero-shot generalizability, but may underperform on specific datasets with
unique characteristics, limiting their direct use for analysis. This work
addresses these bottlenecks by proposing a data-centric AI workflow that
leverages active learning and pseudo-labeling to combine the strengths of
traditional neural networks and large foundation models while minimizing human
intervention. The pipeline starts by generating pseudo-labels from a foundation
model, which are then used for nnU-Net's self-configuration. Subsequently, a
representative core-set is selected for minimal manual annotation, enabling
effective fine-tuning of the nnU-Net model. This approach significantly reduces
the need for manual annotations while maintaining competitive performance,
providing an accessible solution for biomedical researchers to apply
state-of-the-art AI techniques in their segmentation tasks. The code is
available at https://github.com/MMV-Lab/AL_BioMed_img_seg.


### [4] [A benchmark multimodal oro-dental dataset for large vision-language models](https://arxiv.org/abs/2511.04948)
*Haoxin Lv, Ijazul Haq, Jin Du, Jiaxin Ma, Binnian Zhu, Xiaobing Dang, Chaoan Liang, Ruxu Du, Yingjie Zhang, Muhammad Saqib*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€å£è…”å¥åº·æ•°æ®é›†ï¼ŒåŒ…å«8775æ¬¡ç‰™ç§‘æ£€æŸ¥æ•°æ®ï¼Œå¹¶åŸºäºè¯¥æ•°æ®é›†å¾®è°ƒäº†Qwen-VLå¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨å£è…”å¼‚å¸¸åˆ†ç±»å’Œè¯Šæ–­æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å£è…”å¥åº·é¢†åŸŸäººå·¥æ™ºèƒ½çš„å‘å±•å—é™äºç¼ºä¹èƒ½å¤Ÿåæ˜ ä¸´åºŠå®è·µå¤æ‚æ€§çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œç°æœ‰æ•°æ®èµ„æºä¸è¶³ä»¥æ”¯æ’‘å…ˆè¿›AIæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚

**Method:** æ”¶é›†äº†è·¨è¶Šå…«å¹´ï¼ˆ2018-2025ï¼‰çš„8775æ¬¡ç‰™ç§‘æ£€æŸ¥æ•°æ®ï¼ŒåŒ…æ‹¬50000å¼ å£å†…å›¾åƒã€8056å¼ Xå…‰ç‰‡å’Œè¯¦ç»†æ–‡æœ¬è®°å½•ï¼›ä½¿ç”¨è¯¥æ•°æ®é›†å¯¹Qwen-VL 3Bå’Œ7Bæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨å…­ç§å£è…”å¼‚å¸¸åˆ†ç±»å’Œä»å¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆå®Œæ•´è¯Šæ–­æŠ¥å‘Šä¸¤ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ã€‚

**Result:** å¾®è°ƒåçš„æ¨¡å‹åœ¨å£è…”å¼‚å¸¸åˆ†ç±»å’Œè¯Šæ–­æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šç›¸æ¯”åŸºç¡€æ¨¡å‹å’ŒGPT-4oå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§å¹¶å±•ç¤ºäº†å…¶åœ¨æ¨è¿›AIé©±åŠ¨å£è…”å¥åº·è§£å†³æ–¹æ¡ˆæ–¹é¢çš„ä»·å€¼ã€‚

**Conclusion:** è¯¥æ•°æ®é›†ä¸ºAIç‰™ç§‘ç ”ç©¶æä¾›äº†é‡è¦èµ„æºï¼Œè¯æ˜äº†å¤§è§„æ¨¡å¤šæ¨¡æ€ä¸´åºŠæ•°æ®åœ¨æå‡å£è…”å¥åº·AIæ¨¡å‹æ€§èƒ½æ–¹é¢çš„å…³é”®ä½œç”¨ï¼Œä¸ºæœªæ¥AIç‰™ç§‘ç ”ç©¶å¥ å®šäº†åŸºç¡€å¹¶æŒ‡æ˜äº†å‘å±•æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
The advancement of artificial intelligence in oral healthcare relies on the
availability of large-scale multimodal datasets that capture the complexity of
clinical practice. In this paper, we present a comprehensive multimodal
dataset, comprising 8775 dental checkups from 4800 patients collected over
eight years (2018-2025), with patients ranging from 10 to 90 years of age. The
dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual
records, including diagnoses, treatment plans, and follow-up notes. The data
were collected under standard ethical guidelines and annotated for
benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large
vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks:
classification of six oro-dental anomalies and generation of complete
diagnostic reports from multimodal inputs. We compared the fine-tuned models
with their base counterparts and GPT-4o. The fine-tuned models achieved
substantial gains over these baselines, validating the dataset and underscoring
its effectiveness in advancing AI-driven oro-dental healthcare solutions. The
dataset is publicly available, providing an essential resource for future
research in AI dentistry.


### [5] [Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement](https://arxiv.org/abs/2511.04963)
*Xiongri Shen, Jiaqi Wang, Yi Zhong, Zhenxi Song, Leilei Zhao, Yichen Wei, Lingyan Liang, Shuqiang Wang, Baiying Lei, Demao Deng, Zhiguo Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºPDSæ–¹æ³•ï¼Œé€šè¿‡æ¨¡å¼æ„ŸçŸ¥åŒæ¨¡æ€3Dæ‰©æ•£æ¡†æ¶å’Œé›†æˆå¾®ç»“æ„ä¼˜åŒ–çš„ç»„ç»‡ç»†åŒ–ç½‘ç»œï¼Œè§£å†³äº†fMRI-dMRIè·¨æ¨¡æ€åˆæˆä¸­çš„ä¿¡å·å·®å¼‚å’Œç¥ç»è§£å‰–æ¨¡å¼æ•´åˆä¸è¶³é—®é¢˜ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„åˆæˆæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰å’Œæ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰åœ¨ç¥ç»é€€è¡Œæ€§ç–¾ç—…ç ”ç©¶ä¸­è‡³å…³é‡è¦ï¼Œä½†æ¨¡æ€ç¼ºå¤±ä¸¥é‡é™åˆ¶äº†å…¶ä¸´åºŠåº”ç”¨ã€‚ç°æœ‰åŸºäºGANå’Œæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•åœ¨fMRI-dMRIåˆæˆä¸­å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šä¸€æ˜¯fMRIä¸dMRIåœ¨æ—¶é—´/æ¢¯åº¦è½´ä¸Šå­˜åœ¨æ˜¾è‘—çš„BOLDä¸æ‰©æ•£åŠ æƒä¿¡å·å·®å¼‚ï¼ŒäºŒæ˜¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æœªèƒ½å……åˆ†æ•´åˆç–¾ç—…ç›¸å…³çš„ç¥ç»è§£å‰–æ¨¡å¼ã€‚

**Method:** æå‡ºçš„PDSæ–¹æ³•åŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šä¸€æ˜¯æ¨¡å¼æ„ŸçŸ¥åŒæ¨¡æ€3Dæ‰©æ•£æ¡†æ¶ï¼Œç”¨äºè·¨æ¨¡æ€å­¦ä¹ ï¼›äºŒæ˜¯é›†æˆé«˜æ•ˆå¾®ç»“æ„ä¼˜åŒ–çš„ç»„ç»‡ç»†åŒ–ç½‘ç»œï¼Œä»¥ä¿æŒç»“æ„ä¿çœŸåº¦å’Œç»†èŠ‚å®Œæ•´æ€§ã€‚è¯¥æ–¹æ³•ä¸“é—¨é’ˆå¯¹fMRIå’ŒdMRIä¹‹é—´çš„ä¿¡å·å·®å¼‚è¿›è¡Œä¼˜åŒ–è®¾è®¡ã€‚

**Result:** åœ¨OASIS-3ã€ADNIå’Œå†…éƒ¨æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼šfMRIåˆæˆçš„PSNR/SSIMè¾¾åˆ°29.83 dB/90.84%ï¼ˆæ¯”åŸºçº¿æå‡1.54 dB/4.12%ï¼‰ï¼ŒdMRIåˆæˆè¾¾åˆ°30.00 dB/77.55%ï¼ˆæå‡1.02 dB/2.2%ï¼‰ã€‚ä¸´åºŠéªŒè¯ä¸­ï¼Œåˆæˆæ•°æ®åœ¨æ··åˆçœŸå®-åˆæˆå®éªŒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è¯Šæ–­æ€§èƒ½ï¼ŒNC vs. MCI vs. ADåˆ†ç±»å‡†ç¡®ç‡è¾¾åˆ°67.92%/66.02%/64.15%ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†PDSæ–¹æ³•åœ¨è§£å†³fMRI-dMRIè·¨æ¨¡æ€åˆæˆæŒ‘æˆ˜æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ä»…æå‡äº†åˆæˆè´¨é‡æŒ‡æ ‡ï¼Œæ›´é‡è¦çš„æ˜¯åˆæˆæ•°æ®åœ¨ä¸´åºŠè¯Šæ–­ä»»åŠ¡ä¸­è¡¨ç°å‡ºå®ç”¨ä»·å€¼ï¼Œä¸ºç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„å½±åƒåˆ†ææä¾›äº†å¯é çš„æ¨¡æ€è¡¥å…¨è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨å‰æ™¯ã€‚

---

#### ğŸ“„ Abstract
Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and
diffusion MRI (dMRI), is essential for studying neurodegenerative diseases.
However, missing modalities pose a major barrier to their clinical use.
Although GAN- and diffusion model-based approaches have shown some promise in
modality completion, they remain limited in fMRI-dMRI synthesis due to (1)
significant BOLD vs. diffusion-weighted signal differences between fMRI and
dMRI in time/gradient axis, and (2) inadequate integration of disease-related
neuroanatomical patterns during generation. To address these challenges, we
propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D
diffusion framework for cross-modality learning, and (2) a tissue refinement
network integrated with a efficient microstructure refinement to maintain
structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house
datasets, our method achieves state-of-the-art results, with PSNR/SSIM scores
of 29.83 dB/90.84\% for fMRI synthesis (+1.54 dB/+4.12\% over baselines) and
30.00 dB/77.55\% for dMRI synthesis (+1.02 dB/+2.2\%). In clinical validation,
the synthesized data show strong diagnostic performance, achieving
67.92\%/66.02\%/64.15\% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic
experiments. Code is available in \href{https://github.com/SXR3015/PDS}{PDS
GitHub Repository}


### [6] [GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder](https://arxiv.org/abs/2511.04977)
*Heng Er Metilda Chee, Jiayin Wang, Zhiqiang Guo, Weizhi Ma, Min Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†è´´çº¸è¯­ä¹‰ç›¸ä¼¼æ€§ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†é¦–ä¸ªåŸºå‡†æ•°æ®é›†Triple-Sï¼ŒåŒæ—¶å¼€å‘äº†é€šç”¨è´´çº¸ç¼–ç å™¨GSEï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ é²æ£’çš„è´´çº¸åµŒå…¥è¡¨ç¤ºï¼Œåœ¨æœªè§è´´çº¸ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è´´çº¸å·²æˆä¸ºæµè¡Œçš„è§†è§‰äº¤æµå½¢å¼ï¼Œä½†ç”±äºå…¶å†…å®¹é«˜åº¦å¤šæ ·åŒ–å’Œç¬¦å·åŒ–ï¼Œç†è§£å…¶è¯­ä¹‰å…³ç³»ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰é¢„è®­ç»ƒè§†è§‰å’Œå¤šæ¨¡æ€æ¨¡å‹éš¾ä»¥æ•æ‰è´´çº¸çš„ç»†å¾®è¯­ä¹‰å·®å¼‚ï¼Œéœ€è¦ä¸“é—¨çš„è§£å†³æ–¹æ¡ˆã€‚

**Method:** ä½œè€…æ­£å¼å®šä¹‰äº†è´´çº¸è¯­ä¹‰ç›¸ä¼¼æ€§ä»»åŠ¡ï¼Œæ„å»ºäº†åŒ…å«905ä¸ªäººå·¥æ ‡æ³¨æ­£è´Ÿè´´çº¸å¯¹çš„Triple-SåŸºå‡†æ•°æ®é›†ã€‚æå‡ºäº†é€šç”¨è´´çº¸ç¼–ç å™¨GSEï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä¸”é€šç”¨çš„æ¨¡å‹ï¼Œåˆ©ç”¨Triple-Så’Œé¢å¤–æ•°æ®é›†å­¦ä¹ é²æ£’çš„è´´çº¸åµŒå…¥è¡¨ç¤ºã€‚

**Result:** GSEåœ¨æœªè§è´´çº¸ä¸Šå®ç°äº†ä¼˜è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡å¦‚æƒ…æ„Ÿåˆ†ç±»å’Œè´´çº¸åˆ°è´´çº¸æ£€ç´¢ä¸­è¡¨ç°å‡ºå¼ºåŠ²ç»“æœã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰é¢„è®­ç»ƒæ¨¡å‹åœ¨æ•æ‰è´´çº¸è¯­ä¹‰æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€ŒGSEæœ‰æ•ˆè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚

**Conclusion:** é€šè¿‡å‘å¸ƒTriple-SåŸºå‡†å’ŒGSEæ¨¡å‹ï¼Œæœ¬ç ”ç©¶ä¸ºæ ‡å‡†åŒ–çš„è´´çº¸ç†è§£è¯„ä¼°æä¾›äº†å·¥å…·å’Œé²æ£’åµŒå…¥è¡¨ç¤ºï¼Œä¸ºæœªæ¥è´´çº¸ç†è§£ã€æ£€ç´¢å’Œå¤šæ¨¡æ€å†…å®¹ç”Ÿæˆç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚è¿™äº›èµ„æºå·²å…¬å¼€å‘å¸ƒï¼Œå°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Stickers have become a popular form of visual communication, yet
understanding their semantic relationships remains challenging due to their
highly diverse and symbolic content. In this work, we formally {define the
Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark
for this task, consisting of 905 human-annotated positive and negative sticker
pairs. Through extensive evaluation, we show that existing pretrained vision
and multimodal models struggle to capture nuanced sticker semantics. To address
this, we propose the {General Sticker Encoder (GSE)}, a lightweight and
versatile model that learns robust sticker embeddings using both Triple-S and
additional datasets. GSE achieves superior performance on unseen stickers, and
demonstrates strong results on downstream tasks such as emotion classification
and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we
provide standardized evaluation tools and robust embeddings, enabling future
research in sticker understanding, retrieval, and multimodal content
generation. The Triple-S benchmark and GSE have been publicly released and are
available here.


### [7] [Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings](https://arxiv.org/abs/2511.05017)
*Aakriti Agrawal, Gouthaman KV, Rohith Aralikatti, Gauri Jagatap, Jiaxin Yuan, Vijay Kamarshi, Andrea Fanelli, Furong Huang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•æ¥è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡ä½¿ç”¨å¹³å‡æ± åŒ–è§†è§‰ç‰¹å¾æ¥ç²¾ç‚¼æ–‡æœ¬åµŒå…¥ï¼Œæ˜¾è‘—æ”¹å–„äº†è§†è§‰å®šä½èƒ½åŠ›å¹¶å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰LVLMæ¶æ„å­˜åœ¨å›ºæœ‰çš„è¯­è¨€æ¨¡æ€åå‘é—®é¢˜ï¼Œè¿™ä¸»è¦æºäºå°†è§†è§‰åµŒå…¥ç®€å•é™„åŠ åˆ°è¾“å…¥æ–‡æœ¬åºåˆ—çš„å¸¸è§åšæ³•ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦ä¾èµ–è¯­è¨€ä¿¡æ¯è€Œå¿½è§†è§†è§‰å†…å®¹ã€‚

**Method:** æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡é›†æˆå¹³å‡æ± åŒ–çš„è§†è§‰ç‰¹å¾æ¥ç²¾ç‚¼æ–‡æœ¬åµŒå…¥ï¼Œè¯¥æ–¹æ³•æä¾›äº†ä¸€ç§ç›´æ¥ã€é²æ£’ä¸”é«˜æ•ˆçš„è§†è§‰ä¿¡æ¯èåˆç­–ç•¥ã€‚

**Result:** è¯¥æ–¹æ³•åœ¨å·²å»ºç«‹çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æ”¹å–„äº†è§†è§‰å®šä½èƒ½åŠ›ï¼Œå¹¶å¤§å¹…å‡å°‘äº†å¹»è§‰ç°è±¡ï¼Œè¯æ˜äº†é€šè¿‡è§†è§‰ä¿¡æ¯ç²¾ç‚¼æ–‡æœ¬åµŒå…¥å¯ä»¥æœ‰æ•ˆç¼“è§£æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ã€‚

**Conclusion:** æœ¬ç ”ç©¶ä¸»è¦å…³æ³¨æ­ç¤ºæ¨¡æ€ä¸å¹³è¡¡åŠå…¶å¯¹å¹»è§‰çš„å½±å“ï¼Œå¹¶è¯æ˜é€šè¿‡è§†è§‰ä¿¡æ¯ç²¾ç‚¼æ–‡æœ¬åµŒå…¥å¯ä»¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼ŒåŒæ—¶æŒ‡å‡ºæ›´å¤æ‚çš„èåˆæ–¹æ³•å¯èƒ½è¿›ä¸€æ­¥æ”¹å–„è§†è§‰å®šä½å’Œè·¨æ¨¡æ€å¯¹é½ï¼Œä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
In this work, we identify an inherent bias in prevailing LVLM architectures
toward the language modality, largely resulting from the common practice of
simply appending visual embeddings to the input text sequence. To address this,
we propose a simple yet effective method that refines textual embeddings by
integrating average-pooled visual features. Our approach demonstrably improves
visual grounding and significantly reduces hallucinations on established
benchmarks. While average pooling offers a straightforward, robust, and
efficient means of incorporating visual information, we believe that more
sophisticated fusion methods could further enhance visual grounding and
cross-modal alignment. Given that the primary focus of this work is to
highlight the modality imbalance and its impact on hallucinations -- and to
show that refining textual embeddings with visual information mitigates this
issue -- we leave exploration of advanced fusion strategies for future work.


### [8] [Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation](https://arxiv.org/abs/2511.05034)
*Jing Jin, Xu Liu, Te Gao, Zhihong Shi, Yixiong Liang, Ruiqing Zheng, Hulin Kuang, Min Zeng, Shichao Kan*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŠ¨æ€æ®‹å·®ç¼–ç ä¸åˆ‡ç‰‡çº§å¯¹æ¯”å­¦ä¹ ï¼ˆDRE-SLCLï¼‰æ–¹æ³•ï¼Œç”¨äºç«¯åˆ°ç«¯çš„å…¨åˆ‡ç‰‡å›¾åƒè¡¨ç¤ºå­¦ä¹ ï¼Œé€šè¿‡å†…å­˜åº“å­˜å‚¨ç“¦ç‰‡ç‰¹å¾å¹¶ç»“åˆæ®‹å·®ç¼–ç æŠ€æœ¯ï¼Œæœ‰æ•ˆè§£å†³äº†GPUå†…å­˜é™åˆ¶ä¸‹æ— æ³•åŒæ—¶å¤„ç†æ‰€æœ‰ç“¦ç‰‡çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å…¨åˆ‡ç‰‡å›¾åƒè¡¨ç¤ºåœ¨ç™Œç—‡åˆ†å‹ã€è¯†åˆ«å’Œçªå˜é¢„æµ‹ä¸­è‡³å…³é‡è¦ï¼Œä½†ç”±äºæ ‡å‡†åƒå…†åƒç´ åˆ‡ç‰‡åŒ…å«æ•°ä¸‡ä¸ªå›¾åƒç“¦ç‰‡ï¼Œåœ¨å½“å‰GPUé™åˆ¶ä¸‹æ— æ³•åœ¨å•ä¸ªå°æ‰¹é‡ä¸­è®¡ç®—æ‰€æœ‰ç“¦ç‰‡çš„æ¢¯åº¦ï¼Œè¿™ç»™ç«¯åˆ°ç«¯WSIè¡¨ç¤ºæ¨¡å‹çš„è®­ç»ƒå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚

**Method:** è¯¥æ–¹æ³•ä½¿ç”¨å†…å­˜åº“å­˜å‚¨æ•°æ®é›†ä¸­æ‰€æœ‰WSIçš„ç“¦ç‰‡ç‰¹å¾ï¼Œè®­ç»ƒæ—¶æ¯ä¸ªå°æ‰¹é‡åŒ…å«å¤šä¸ªWSIï¼Œå¯¹æ¯ä¸ªWSIéšæœºé‡‡æ ·éƒ¨åˆ†ç“¦ç‰‡å¹¶é€šè¿‡ç“¦ç‰‡ç¼–ç å™¨è®¡ç®—ç‰¹å¾ï¼ŒåŒæ—¶ä»å†…å­˜åº“ä¸­é€‰æ‹©åŒä¸€WSIçš„é¢å¤–ç“¦ç‰‡ç‰¹å¾ï¼Œé‡‡ç”¨æ®‹å·®ç¼–ç æŠ€æœ¯ç»“åˆé‡‡æ ·ç‰¹å¾å’Œæ£€ç´¢ç‰¹å¾ç”Ÿæˆå•ä¸ªWSIçš„è¡¨ç¤ºï¼Œæœ€ååŸºäºå°æ‰¹é‡å†…WSIçš„è¡¨ç¤ºå’Œç»„ç»‡ç—…ç†å­¦æŠ¥å‘Šè®¡ç®—åˆ‡ç‰‡çº§å¯¹æ¯”æŸå¤±ã€‚

**Result:** åœ¨ç™Œç—‡åˆ†å‹ã€ç™Œç—‡è¯†åˆ«å’Œçªå˜é¢„æµ‹ä»»åŠ¡ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†æ‰€æå‡ºçš„DRE-SLCLæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸå…‹æœGPUå†…å­˜é™åˆ¶å¹¶å®ç°é«˜è´¨é‡çš„WSIè¡¨ç¤ºå­¦ä¹ ã€‚

**Conclusion:** DRE-SLCLæ–¹æ³•ä¸ºç«¯åˆ°ç«¯WSIè¡¨ç¤ºå­¦ä¹ æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡åŠ¨æ€æ®‹å·®ç¼–ç å’Œå¯¹æ¯”å­¦ä¹ çš„ç»“åˆï¼Œä¸ä»…è§£å†³äº†è®¡ç®—èµ„æºé™åˆ¶é—®é¢˜ï¼Œè¿˜ä¸ºç™Œç—‡ç›¸å…³çš„å¤šç§ç—…ç†å­¦ä»»åŠ¡æä¾›äº†å¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Whole Slide Image (WSI) representation is critical for cancer subtyping,
cancer recognition and mutation prediction.Training an end-to-end WSI
representation model poses significant challenges, as a standard gigapixel
slide can contain tens of thousands of image tiles, making it difficult to
compute gradients of all tiles in a single mini-batch due to current GPU
limitations. To address this challenge, we propose a method of dynamic residual
encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI
representation. Our approach utilizes a memory bank to store the features of
tiles across all WSIs in the dataset. During training, a mini-batch usually
contains multiple WSIs. For each WSI in the batch, a subset of tiles is
randomly sampled and their features are computed using a tile encoder. Then,
additional tile features from the same WSI are selected from the memory bank.
The representation of each individual WSI is generated using a residual
encoding technique that incorporates both the sampled features and those
retrieved from the memory bank. Finally, the slide-level contrastive loss is
computed based on the representations and histopathology reports ofthe WSIs
within the mini-batch. Experiments conducted over cancer subtyping, cancer
recognition, and mutation prediction tasks proved the effectiveness of the
proposed DRE-SLCL method.


### [9] [Medical Referring Image Segmentation via Next-Token Mask Prediction](https://arxiv.org/abs/2511.05044)
*Xinyu Chen, Yiran Wang, Gaoyang Pang, Jiafu Hao, Chentao Yue, Luping Zhou, Yonghui Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºNTP-MRISegæ¡†æ¶ï¼Œå°†åŒ»å­¦å‚è€ƒå›¾åƒåˆ†å‰²é‡æ–°è¡¨è¿°ä¸ºå¤šæ¨¡æ€åºåˆ—ä¸Šçš„è‡ªå›å½’ä¸‹ä¸€ä»¤ç‰Œé¢„æµ‹ä»»åŠ¡ï¼Œé€šè¿‡ç»Ÿä¸€æ¶æ„ç®€åŒ–äº†æ¨¡å‹è®¾è®¡ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŒ»å­¦å‚è€ƒå›¾åƒåˆ†å‰²æ–¹æ³•é€šå¸¸æ¶‰åŠå¤æ‚çš„å¤šæ¨¡æ€èåˆæˆ–å¤šé˜¶æ®µè§£ç å™¨è®¾è®¡ï¼Œå¯¼è‡´æ¨¡å‹æ¶æ„å¤æ‚ä¸”æ•ˆç‡ä½ä¸‹ï¼Œéœ€è¦æ¢ç´¢æ›´ç®€æ´ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆã€‚

**Method:** æå‡ºNTP-MRISegæ¡†æ¶ï¼Œå°†MRISé‡æ–°è¡¨è¿°ä¸ºå›¾åƒã€æ–‡æœ¬å’Œæ©ç è¡¨ç¤ºçš„ç»Ÿä¸€å¤šæ¨¡æ€åºåˆ—ä¸Šçš„è‡ªå›å½’ä¸‹ä¸€ä»¤ç‰Œé¢„æµ‹ä»»åŠ¡ï¼Œå¹¶å¼•å…¥ä¸‰ç§å…³é”®ç­–ç•¥ï¼šä¸‹ä¸€kä»¤ç‰Œé¢„æµ‹æ–¹æ¡ˆå‡å°‘ç´¯ç§¯è¯¯å·®ã€ä»¤ç‰Œçº§å¯¹æ¯”å­¦ä¹ å¢å¼ºè¾¹ç•Œæ•æ„Ÿæ€§ã€åŸºäºå†…å­˜çš„ç¡¬é”™è¯¯ä»¤ç‰Œä¼˜åŒ–ç­–ç•¥ä¸“æ³¨äºå›°éš¾ä»¤ç‰Œã€‚

**Result:** åœ¨QaTa-COV19å’ŒMosMedData+æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒNTP-MRISegå®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºåŒ»å­¦å‚è€ƒå›¾åƒåˆ†å‰²æä¾›äº†ä¸€ç§ç®€åŒ–çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†è‡ªå›å½’ä»¤ç‰Œé¢„æµ‹æ¡†æ¶åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æ½œåŠ›ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€åŒ»å­¦ä»»åŠ¡æä¾›äº†æ–°çš„è®¾è®¡æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Medical Referring Image Segmentation (MRIS) involves segmenting target
regions in medical images based on natural language descriptions. While
achieving promising results, recent approaches usually involve complex design
of multimodal fusion or multi-stage decoders. In this work, we propose
NTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive
next-token prediction task over a unified multimodal sequence of tokenized
image, text, and mask representations. This formulation streamlines model
design by eliminating the need for modality-specific fusion and external
segmentation models, supports a unified architecture for end-to-end training.
It also enables the use of pretrained tokenizers from emerging large-scale
multimodal models, enhancing generalization and adaptability. More importantly,
to address challenges under this formulation-such as exposure bias, long-tail
token distributions, and fine-grained lesion edges-we propose three novel
strategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative
prediction errors, (2) Token-level Contrastive Learning (TCL) to enhance
boundary sensitivity and mitigate long-tail distribution effects, and (3) a
memory-based Hard Error Token (HET) optimization strategy that emphasizes
difficult tokens during training. Extensive experiments on the QaTa-COV19 and
MosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art
performance, offering a streamlined and effective alternative to traditional
MRIS pipelines.


### [10] [Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach](https://arxiv.org/abs/2511.05057)
*Yuanxiang Huangfu, Chaochao Wang, Weilei Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Role-SynthCLIPæ¡†æ¶ï¼Œé€šè¿‡å¤šè§†è§’è§’è‰²æ‰®æ¼”æç¤ºå¼•å¯¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯­ä¹‰å¤šæ ·åŒ–çš„å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œæ˜¾è‘—æå‡äº†å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨ä»…ä½¿ç”¨100ä¸‡å¯¹æ•°æ®çš„æƒ…å†µä¸‹è¶…è¶Šäº†ç°æœ‰åŸºäº500ä¸‡å¯¹åˆæˆæ•°æ®çš„æœ€ä½³åŸºçº¿æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ä¸»è¦å…³æ³¨å¢åŠ æ•°æ®é‡ï¼Œä½†è¿™ç§å¼ºè°ƒå¾€å¾€å¯¼è‡´è¯­ä¹‰å¤šæ ·æ€§æœ‰é™ä»¥åŠç”Ÿæˆå†—ä½™æˆ–æµ…æ˜¾çš„æè¿°æ–‡æœ¬ï¼Œé™åˆ¶äº†å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒæ¨¡å‹çš„æ•ˆæœæå‡ã€‚

**Method:** æå‡ºäº†Role-SynthCLIPæ•°æ®åˆæˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤šè§†è§’è§’è‰²æ‰®æ¼”æç¤ºï¼ˆå¦‚ç»„åˆåˆ†æå¸ˆã€å›¾åƒä¸Šä¸‹æ–‡è§£é‡Šå™¨ç­‰ï¼‰å¼•å¯¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»ä¸åŒè§†è§’ç”Ÿæˆè¯­ä¹‰å¤šæ ·åŒ–çš„æè¿°æ–‡æœ¬ï¼Œå¢å¼ºåˆæˆå¯¹çš„è¯­ä¹‰å¤šæ ·æ€§å’Œç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½ï¼ŒåŒæ—¶ä¿æŒå›¾åƒ-æ–‡æœ¬å¯¹æ€»æ•°ä¸å˜ã€‚

**Result:** åœ¨MS COCOéªŒè¯é›†ä¸Šï¼Œä»…ä½¿ç”¨100ä¸‡å¯¹Role-SynthCLIPæ•°æ®è®­ç»ƒçš„CLIP-B/16æ¨¡å‹å®ç°äº†64.1%çš„Recall@1ï¼Œæ¯”ç°æœ‰åŸºäº500ä¸‡å¯¹åˆæˆæ•°æ®çš„æœ€ä½³åŸºçº¿æ–¹æ³•é«˜å‡º2.8ä¸ªç™¾åˆ†ç‚¹ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡å¤šè§†è§’è§’è‰²æ‰®æ¼”æç¤ºå¢å¼ºåˆæˆæ•°æ®çš„è¯­ä¹‰å¤šæ ·æ€§å¯ä»¥æ˜¾è‘—æå‡å¯¹æ¯”å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸ºé«˜è´¨é‡åˆæˆæ•°æ®ç”Ÿæˆæä¾›äº†æ–°çš„æ–¹å‘ï¼ŒåŒæ—¶è¯æ˜äº†åœ¨ä¿æŒæ•°æ®é‡ä¸å˜çš„æƒ…å†µä¸‹ä¼˜åŒ–æ•°æ®è´¨é‡çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
The effectiveness of Contrastive Language-Image Pre-training (CLIP) models
critically depends on the semantic diversity and quality of their training
data. However, while existing synthetic data generation methods primarily focus
on increasing data volume, such emphasis often leads to limited semantic
diversity and redundant or shallow captions. To address this limitation, we
propose Role-SynthCLIP, a novel data synthesis framework that leverages
multi-perspective role-playing prompts (e.g., a compositional analyst, an
interpreter of image context) to guide Multimodal Large Language Models (MLLMs)
in generating semantically diverse captions from distinct viewpoints. This
mechanism enhances the semantic diversity and fine-grained image-text alignment
of synthetic pairs, thereby improving caption expressiveness and accuracy while
keeping the total number of image-text pairs unchanged. Experimental results
demonstrate the effectiveness and efficiency of our method. A CLIP-B/16 model
trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on
the MS COCO validation set, surpassing the best existing synthetic data
baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained
models are released at https://github.com/huangfu170/Role-SynthCLIP.


### [11] [DeepEyesV2: Toward Agentic Multimodal Model](https://arxiv.org/abs/2511.05271)
*Jack Hong, Chenxiao Zhao, ChengLin Zhu, Weiheng Lu, Guohai Xu, Xing Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†DeepEyesV2ï¼Œä¸€ç§ä»£ç†å¼å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ˆå†·å¯åŠ¨é˜¶æ®µå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼‰å®ç°äº†ç¨³å¥çš„å·¥å…·è°ƒç”¨è¡Œä¸ºï¼Œå¹¶åœ¨RealX-Benchç­‰åŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†åœ¨çœŸå®ä¸–ç•Œç†è§£ã€æ•°å­¦æ¨ç†å’Œæœç´¢å¯†é›†å‹ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä»£ç†å¼å¤šæ¨¡æ€æ¨¡å‹ä¸ä»…éœ€è¦ç†è§£æ–‡æœ¬å’Œå›¾åƒï¼Œè¿˜éœ€è¦ä¸»åŠ¨è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚ä»£ç æ‰§è¡Œç¯å¢ƒå’Œç½‘ç»œæœç´¢ï¼‰å¹¶å°†è¿™äº›æ“ä½œæ•´åˆåˆ°æ¨ç†è¿‡ç¨‹ä¸­ã€‚ç°æœ‰æ–¹æ³•ä¸­ï¼Œä»…ä½¿ç”¨ç›´æ¥å¼ºåŒ–å­¦ä¹ æ— æ³•è¯±å¯¼å‡ºç¨³å¥çš„å·¥å…·ä½¿ç”¨è¡Œä¸ºï¼Œè¿™ä¿ƒä½¿ç ”ç©¶å¦‚ä½•æ„å»ºæœ‰æ•ˆçš„ä»£ç†å¼å¤šæ¨¡æ€æ¨¡å‹ã€‚

**Method:** é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼šå†·å¯åŠ¨é˜¶æ®µå»ºç«‹å·¥å…·ä½¿ç”¨æ¨¡å¼ï¼Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›ä¸€æ­¥ä¼˜åŒ–å·¥å…·è°ƒç”¨ã€‚æ„å»ºäº†å¤šæ ·åŒ–ã€é€‚åº¦æŒ‘æˆ˜æ€§çš„è®­ç»ƒæ•°æ®é›†ï¼Œç‰¹åˆ«åŒ…å«å·¥å…·ä½¿ç”¨æœ‰ç›Šçš„åœºæ™¯ã€‚å¼•å…¥äº†RealX-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°éœ€è¦æ•´åˆæ„ŸçŸ¥ã€æœç´¢å’Œæ¨ç†èƒ½åŠ›çš„çœŸå®ä¸–ç•Œå¤šæ¨¡æ€æ¨ç†ã€‚

**Result:** DeepEyesV2åœ¨RealX-Benchå’Œå…¶ä»–ä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨çœŸå®ä¸–ç•Œç†è§£ã€æ•°å­¦æ¨ç†å’Œæœç´¢å¯†é›†å‹ä»»åŠ¡ä¸Šå‡æ˜¾ç¤ºæœ‰æ•ˆæ€§ã€‚æ¨¡å‹å±•ç°å‡ºä»»åŠ¡è‡ªé€‚åº”çš„å·¥å…·è°ƒç”¨è¡Œä¸ºï¼Œå€¾å‘äºåœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸­ä½¿ç”¨å›¾åƒæ“ä½œï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸­ä½¿ç”¨æ•°å€¼è®¡ç®—ã€‚å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥å®ç°äº†å¤æ‚çš„å·¥å…·ç»„åˆå’ŒåŸºäºä¸Šä¸‹æ–‡çš„é€‰æ‹©æ€§å·¥å…·è°ƒç”¨ã€‚

**Conclusion:** ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹æ˜¯æ„å»ºä»£ç†å¼å¤šæ¨¡æ€æ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿä¿ƒè¿›å¤æ‚å·¥å…·ç»„åˆå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å·¥å…·é€‰æ‹©ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘ä»£ç†å¼å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œå±•ç¤ºäº†å·¥å…·è°ƒç”¨è¡Œä¸ºå¯ä»¥é€šè¿‡é€‚å½“çš„è®­ç»ƒç­–ç•¥å¾—åˆ°æœ‰æ•ˆè¯±å¯¼å’Œä¼˜åŒ–ã€‚

---

#### ğŸ“„ Abstract
Agentic multimodal models should not only comprehend text and images, but
also actively invoke external tools, such as code execution environments and
web search, and integrate these operations into reasoning. In this work, we
introduce DeepEyesV2 and explore how to build an agentic multimodal model from
the perspectives of data construction, training methods, and model evaluation.
We observe that direct reinforcement learning alone fails to induce robust
tool-use behavior. This phenomenon motivates a two-stage training pipeline: a
cold-start stage to establish tool-use patterns, and reinforcement learning
stage to further refine tool invocation. We curate a diverse, moderately
challenging training dataset, specifically including examples where tool use is
beneficial. We further introduce RealX-Bench, a comprehensive benchmark
designed to evaluate real-world multimodal reasoning, which inherently requires
the integration of multiple capabilities, including perception, search, and
reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative
benchmarks, demonstrating its effectiveness across real-world understanding,
mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2
exhibits task-adaptive tool invocation, tending to use image operations for
perception tasks and numerical computations for reasoning tasks. Reinforcement
learning further enables complex tool combinations and allows model to
selectively invoke tools based on context. We hope our study can provide
guidance for community in developing agentic multimodal models.


### [12] [A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification](https://arxiv.org/abs/2511.05092)
*Ruolin Li, Min Liu, Yuan Bian, Zhaoyang Li, Yuzhen Li, Xueping Wang, Yaonan Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒé˜¶æ®µæç¤ºé©±åŠ¨çš„éšç§ä¿æŠ¤èŒƒå¼(DPPP)ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–è™šæ‹Ÿæ•°æ®å¹¶åˆ©ç”¨æç¤ºé©±åŠ¨è§£è€¦æœºåˆ¶å­¦ä¹ é¢†åŸŸä¸å˜ç‰¹å¾ï¼Œåœ¨è¡Œäººé‡è¯†åˆ«ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ³›åŒ–æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºæ¸¸æˆå¼•æ“ç”Ÿæˆçš„è™šæ‹Ÿæ•°æ®é›†é¢ä¸´æ„å»ºå¤æ‚å’Œé¢†åŸŸæ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ï¼Œéš¾ä»¥åœ¨çœŸå®åœºæ™¯ä¸­æœ‰æ•ˆåº”ç”¨ï¼Œè¿™é™åˆ¶äº†åœ¨æ•°æ®éšç§ä¿æŠ¤èƒŒæ™¯ä¸‹ä½¿ç”¨è™šæ‹Ÿæ•°æ®è®­ç»ƒè¡Œäººé‡è¯†åˆ«æ¨¡å‹çš„å‘å±•ã€‚

**Method:** æå‡ºåŒé˜¶æ®µæ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨åŒ…å«è¡Œäººå¤–è§‚ã€å…‰ç…§å’Œè§†è§’ç­‰å¤šç»´å±æ€§çš„ä¸°å¯Œæç¤ºé©±åŠ¨æ‰©æ•£æ¨¡å‹ç«¯åˆ°ç«¯åˆæˆå¤šæ ·åŒ–æ•°æ®ï¼Œæ„å»ºåŒ…å«6,641ä¸ªèº«ä»½çš„130,519å¼ å›¾åƒçš„å¤§è§„æ¨¡è™šæ‹Ÿæ•°æ®é›†GenePersonï¼›ç¬¬äºŒé˜¶æ®µæå‡ºæç¤ºé©±åŠ¨è§£è€¦æœºåˆ¶ï¼Œå€ŸåŠ©å¯¹æ¯”å­¦ä¹ ä½¿ç”¨ä¸¤ä¸ªæ–‡æœ¬åè½¬ç½‘ç»œå°†å›¾åƒæ˜ å°„ä¸ºä»£è¡¨é£æ ¼å’Œå†…å®¹çš„ä¼ªè¯ï¼Œæ„å»ºé£æ ¼è§£è€¦çš„å†…å®¹æç¤ºæ¥æŒ‡å¯¼æ¨¡å‹åœ¨å›¾åƒå±‚é¢å­¦ä¹ é¢†åŸŸä¸å˜çš„å†…å®¹ç‰¹å¾ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨GenePersonæ•°æ®é›†ä¸Šä½¿ç”¨PDMè®­ç»ƒçš„æ¨¡å‹å®ç°äº†æœ€å…ˆè¿›çš„æ³›åŒ–æ€§èƒ½ï¼Œè¶…è¶Šäº†åœ¨æµè¡ŒçœŸå®å’Œè™šæ‹Ÿè¡Œäººé‡è¯†åˆ«æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºé©±åŠ¨èŒƒå¼å¯ä»¥æœ‰æ•ˆç”Ÿæˆé«˜è´¨é‡è™šæ‹Ÿæ•°æ®å¹¶å­¦ä¹ é¢†åŸŸä¸å˜ç‰¹å¾ï¼Œä¸ºéšç§ä¿æŠ¤ä¸‹è¡Œäººé‡è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒæä¾›äº†å¯è¡Œè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè®­ç»ƒæ•°æ®æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
With growing concerns over data privacy, researchers have started using
virtual data as an alternative to sensitive real-world images for training
person re-identification (Re-ID) models. However, existing virtual datasets
produced by game engines still face challenges such as complex construction and
poor domain generalization, making them difficult to apply in real scenarios.
To address these challenges, we propose a Dual-stage Prompt-driven
Privacy-preserving Paradigm (DPPP). In the first stage, we generate rich
prompts incorporating multi-dimensional attributes such as pedestrian
appearance, illumination, and viewpoint that drive the diffusion model to
synthesize diverse data end-to-end, building a large-scale virtual dataset
named GenePerson with 130,519 images of 6,641 identities. In the second stage,
we propose a Prompt-driven Disentanglement Mechanism (PDM) to learn
domain-invariant generalization features. With the aid of contrastive learning,
we employ two textual inversion networks to map images into pseudo-words
representing style and content, respectively, thereby constructing
style-disentangled content prompts to guide the model in learning
domain-invariant content features at the image level. Experiments demonstrate
that models trained on GenePerson with PDM achieve state-of-the-art
generalization performance, surpassing those on popular real and virtual Re-ID
datasets.


### [13] [LiveStar: Live Streaming Assistant for Real-World Online Video Understanding](https://arxiv.org/abs/2511.05299)
*Zhenyu Yang, Kairui Zhang, Yuhang Hu, Bing Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Weiming Dong, Changsheng Xu*

#### ğŸ§© TL;DR
LiveStaræ˜¯ä¸€ä¸ªå¼€åˆ›æ€§çš„ç›´æ’­æµåŠ©æ‰‹ï¼Œé€šè¿‡è‡ªé€‚åº”æµå¼è§£ç å®ç°æŒç»­ä¸»åŠ¨å“åº”ï¼Œè§£å†³äº†ç°æœ‰åœ¨çº¿è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨å®æ—¶å“åº”æ€§å’Œå™äº‹è¿è´¯æ€§æ–¹é¢çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åœ¨çº¿è§†é¢‘å¤§è¯­è¨€æ¨¡å‹é€šå¸¸éš¾ä»¥åŒæ—¶å¤„ç†è¿ç»­å¸§è¾“å…¥å¹¶ç¡®å®šæœ€ä½³å“åº”æ—¶æœºï¼Œå¾€å¾€åœ¨å®æ—¶å“åº”æ€§å’Œå™äº‹è¿è´¯æ€§ä¹‹é—´åšå‡ºå¦¥åï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç›´æ’­åœºæ™¯ä¸­çš„å®é™…åº”ç”¨æ•ˆæœã€‚

**Method:** LiveStarå¼•å…¥äº†ä¸‰ä¸ªå…³é”®æŠ€æœ¯ï¼šæ”¯æŒå¯å˜é•¿åº¦è§†é¢‘æµå¢é‡è§†é¢‘-è¯­è¨€å¯¹é½çš„è®­ç»ƒç­–ç•¥ã€é€šè¿‡å•æ¬¡å‰å‘ä¼ æ’­éªŒè¯ç¡®å®šä¸»åŠ¨å“åº”æ—¶æœºçš„å“åº”-é™é»˜è§£ç æ¡†æ¶ï¼Œä»¥åŠç»“åˆå³°å€¼-æœ«ç«¯å†…å­˜å‹ç¼©å’Œæµå¼é”®å€¼ç¼“å­˜çš„è®°å¿†æ„ŸçŸ¥åŠ é€Ÿæœºåˆ¶ã€‚

**Result:** åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLiveStarå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è¯­ä¹‰æ­£ç¡®æ€§ä¸Šå¹³å‡æå‡19.5%ï¼Œæ—¶åºå·®å¼‚å‡å°‘18.1%ï¼ŒåŒæ—¶åœ¨æ‰€æœ‰äº”ä¸ªOmniStarä»»åŠ¡ä¸­FPSæå‡12.0%ï¼Œæ¨ç†é€Ÿåº¦åŠ å¿«1.53å€ã€‚

**Conclusion:** LiveStaré€šè¿‡åˆ›æ–°çš„æµå¼è§£ç æ¡†æ¶æ˜¾è‘—æå‡äº†åœ¨çº¿è§†é¢‘ç†è§£çš„å®æ—¶æ€§å’Œå‡†ç¡®æ€§ï¼Œä¸ºç›´æ’­åœºæ™¯ä¸‹çš„æ™ºèƒ½åŠ©æ‰‹åº”ç”¨æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æ„å»ºçš„OmniStaræ•°æ®é›†ä¸ºåœ¨çº¿è§†é¢‘ç†è§£é¢†åŸŸçš„è®­ç»ƒå’Œè¯„ä¼°å»ºç«‹äº†å…¨é¢åŸºå‡†ã€‚

---

#### ğŸ“„ Abstract
Despite significant progress in Video Large Language Models (Video-LLMs) for
offline video understanding, existing online Video-LLMs typically struggle to
simultaneously process continuous frame-by-frame inputs and determine optimal
response timing, often compromising real-time responsiveness and narrative
coherence. To address these limitations, we introduce LiveStar, a pioneering
live streaming assistant that achieves always-on proactive responses through
adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a
training strategy enabling incremental video-language alignment for
variable-length video streams, preserving temporal consistency across
dynamically evolving frame sequences; (2) a response-silence decoding framework
that determines optimal proactive response timing via a single forward pass
verification; (3) memory-aware acceleration via peak-end memory compression for
online inference on 10+ minute videos, combined with streaming key-value cache
to achieve 1.53x faster inference. We also construct an OmniStar dataset, a
comprehensive dataset for training and benchmarking that encompasses 15 diverse
real-world scenarios and 5 evaluation tasks for online video understanding.
Extensive experiments across three benchmarks demonstrate LiveStar's
state-of-the-art performance, achieving an average 19.5% improvement in
semantic correctness with 18.1% reduced timing difference compared to existing
online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.
Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.


### [14] [Early Alzheimer's Disease Detection from Retinal OCT Images: A UK Biobank Study](https://arxiv.org/abs/2511.05106)
*Yasemin Turkan, F. Boray Tek, M. Serdar NazlÄ±, Ã–ykÃ¼ Eren*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é¦–æ¬¡å°†æ·±åº¦å­¦ä¹ åº”ç”¨äºåŸå§‹OCT Bæ‰«æå›¾åƒè¿›è¡Œé˜¿å°”èŒ¨æµ·é»˜ç—…æ—©æœŸé¢„æµ‹ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹å’Œç‰¹å®šå¢å¼ºæŠ€æœ¯ï¼Œåœ¨UK Biobanké˜Ÿåˆ—ä¸­å®ç°äº†0.62çš„AUCï¼Œä¸ºåŸºäºè§†ç½‘è†œæˆåƒçš„ç¥ç»é€€è¡Œæ€§ç–¾ç—…æ£€æµ‹æä¾›äº†åŸºå‡†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å…ˆå‰ç ”ç©¶ä¸»è¦å…³æ³¨åˆ†å‰²åçš„è§†ç½‘è†œå±‚åšåº¦æµ‹é‡ï¼Œè€Œæœ¬ç ”ç©¶æ¢ç´¢ç›´æ¥å¯¹åŸå§‹OCT Bæ‰«æå›¾åƒè¿›è¡Œåˆ†ç±»ä»¥å®ç°é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ—©æœŸæ£€æµ‹ï¼Œå¡«è¡¥äº†æ·±åº¦å­¦ä¹ åœ¨åŸå§‹OCTå›¾åƒä¸Šåº”ç”¨äºADé¢„æµ‹çš„ç ”ç©¶ç©ºç™½ã€‚

**Method:** ç ”ç©¶å¾®è°ƒäº†å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºäºImageNetçš„ç½‘ç»œå’ŒOCTä¸“ç”¨RETFound transformerï¼Œé‡‡ç”¨å¹´é¾„ã€æ€§åˆ«å’Œæˆåƒå®ä¾‹åŒ¹é…çš„å—è¯•è€…çº§åˆ«äº¤å‰éªŒè¯æ•°æ®é›†ï¼Œåº”ç”¨æ ‡å‡†å’ŒOCTç‰¹å®šå¢å¼ºæŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨å¹´åŠ æƒæŸå¤±å‡½æ•°ä¼˜å…ˆå¤„ç†æˆåƒåå››å¹´å†…è¯Šæ–­çš„ç—…ä¾‹ã€‚

**Result:** ResNet-34æ¨¡å‹åœ¨4å¹´é˜Ÿåˆ—ä¸­äº§ç”Ÿæœ€ç¨³å®šç»“æœï¼ŒAUCè¾¾åˆ°0.62ï¼Œè™½ç„¶ä½äºä¸´åºŠåº”ç”¨é˜ˆå€¼ï¼Œä½†å¯è§£é‡Šæ€§åˆ†æç¡®è®¤äº†ADç»„ä¸å¯¹ç…§ç»„åœ¨ä¸­å¤®é»„æ–‘äºšåŒºå­˜åœ¨å±€éƒ¨ç»“æ„å·®å¼‚ã€‚

**Conclusion:** ç ”ç©¶ä¸ºåŸºäºOCTçš„ADé¢„æµ‹æä¾›äº†åŸºå‡†ï¼Œçªæ˜¾äº†åœ¨ADè¯Šæ–­å‰æ•°å¹´æ£€æµ‹ç»†å¾®è§†ç½‘è†œç”Ÿç‰©æ ‡å¿—ç‰©çš„æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºéœ€è¦æ›´å¤§æ•°æ®é›†å’Œå¤šæ¨¡æ€æ–¹æ³•æ¥è§£å†³è¿™ä¸€æ—©æœŸæ£€æµ‹éš¾é¢˜ã€‚

---

#### ğŸ“„ Abstract
Alterations in retinal layer thickness, measurable using Optical Coherence
Tomography (OCT), have been associated with neurodegenerative diseases such as
Alzheimer's disease (AD). While previous studies have mainly focused on
segmented layer thickness measurements, this study explored the direct
classification of OCT B-scan images for the early detection of AD. To our
knowledge, this is the first application of deep learning to raw OCT B-scans
for AD prediction in the literature. Unlike conventional medical image
classification tasks, early detection is more challenging than diagnosis
because imaging precedes clinical diagnosis by several years. We fine-tuned and
evaluated multiple pretrained models, including ImageNet-based networks and the
OCT-specific RETFound transformer, using subject-level cross-validation
datasets matched for age, sex, and imaging instances from the UK Biobank
cohort. To reduce overfitting in this small, high-dimensional dataset, both
standard and OCT-specific augmentation techniques were applied, along with a
year-weighted loss function that prioritized cases diagnosed within four years
of imaging. ResNet-34 produced the most stable results, achieving an AUC of
0.62 in the 4-year cohort. Although below the threshold for clinical
application, our explainability analyses confirmed localized structural
differences in the central macular subfield between the AD and control groups.
These findings provide a baseline for OCT-based AD prediction, highlight the
challenges of detecting subtle retinal biomarkers years before AD diagnosis,
and point to the need for larger datasets and multimodal approaches.


### [15] [Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments](https://arxiv.org/abs/2511.05404)
*Laura Alejandra Encinar Gonzalez, John Folkesson, Rudolph Triebel, Riccardo Giubilato*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMPRFï¼Œä¸€ç§åŸºäºTransformeråŸºç¡€æ¨¡å‹çš„å¤šæ¨¡æ€é—­ç¯æ£€æµ‹ç®¡é“ï¼Œé€šè¿‡ç»“åˆè§†è§‰æ£€ç´¢å’Œæ˜¾å¼6-DoFå§¿æ€ä¼°è®¡ï¼Œåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­å®ç°äº†é²æ£’çš„é—­ç¯æ£€æµ‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åœ¨GNSSæ‹’æ­¢ç¯å¢ƒï¼ˆå¦‚è¡Œæ˜Ÿæ¢æµ‹ï¼‰ä¸­ï¼Œè§†è§‰ä½ç½®è¯†åˆ«å› æ··å å’Œå¼±çº¹ç†è€Œå¤±æ•ˆï¼Œè€ŒLiDARæ–¹æ³•åˆ™å—é™äºç¨€ç–æ€§å’Œæ¨¡ç³Šæ€§ï¼Œç°æœ‰æ–¹æ³•å¤§å¤šå±€é™äºæ£€ç´¢è€Œç¼ºä¹ç²¾ç¡®çš„å§¿æ€ä¼°è®¡èƒ½åŠ›ã€‚

**Method:** MPRFé‡‡ç”¨ä¸¤é˜¶æ®µè§†è§‰æ£€ç´¢ç­–ç•¥ï¼Œç»“åˆDINOv2ç‰¹å¾ä¸SALADèšåˆè¿›è¡Œå€™é€‰ç­›é€‰ï¼Œå¹¶ä½¿ç”¨SONATA-based LiDARæè¿°ç¬¦è¿›è¡Œå‡ ä½•éªŒè¯ï¼Œå®ç°ä»æ£€ç´¢åˆ°æ˜¾å¼6-DoFå§¿æ€ä¼°è®¡çš„å®Œæ•´æµç¨‹ã€‚

**Result:** åœ¨S3LIæ•°æ®é›†å’ŒS3LI Vulcanoæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMPRFåœ¨ç²¾åº¦ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ£€ç´¢æ–¹æ³•ï¼Œå¹¶åœ¨ä½çº¹ç†åŒºåŸŸæ˜¾è‘—æå‡äº†å§¿æ€ä¼°è®¡çš„é²æ£’æ€§ã€‚

**Conclusion:** MPRFé€šè¿‡æä¾›é€‚ç”¨äºSLAMåç«¯çš„å¯è§£é‡Šå¯¹åº”å…³ç³»ï¼Œåœ¨ç²¾åº¦ã€æ•ˆç‡å’Œå¯é æ€§ä¹‹é—´å®ç°äº†è‰¯å¥½å¹³è¡¡ï¼Œå±•ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨ç»Ÿä¸€ä½ç½®è¯†åˆ«å’Œå§¿æ€ä¼°è®¡æ–¹é¢çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Robust loop closure detection is a critical component of Simultaneous
Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as
in the context of planetary exploration. In these settings, visual place
recognition often fails due to aliasing and weak textures, while LiDAR-based
methods suffer from sparsity and ambiguity. This paper presents MPRF, a
multimodal pipeline that leverages transformer-based foundation models for both
vision and LiDAR modalities to achieve robust loop closure in severely
unstructured environments. Unlike prior work limited to retrieval, MPRF
integrates a two-stage visual retrieval strategy with explicit 6-DoF pose
estimation, combining DINOv2 features with SALAD aggregation for efficient
candidate screening and SONATA-based LiDAR descriptors for geometric
verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show
that MPRF outperforms state-of-the-art retrieval methods in precision while
enhancing pose estimation robustness in low-texture regions. By providing
interpretable correspondences suitable for SLAM back-ends, MPRF achieves a
favorable trade-off between accuracy, efficiency, and reliability,
demonstrating the potential of foundation models to unify place recognition and
pose estimation. Code and models will be released at github.com/DLR-RM/MPRF.


### [16] [Cross-domain EEG-based Emotion Recognition with Contrastive Learning](https://arxiv.org/abs/2511.05293)
*Rui Yan, Yibo Li, Han Ding, Fei Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºEmotionCLIPï¼Œå°†åŸºäºè„‘ç”µå›¾çš„æƒ…ç»ªè¯†åˆ«é‡æ–°å®šä¹‰ä¸ºCLIPæ¡†æ¶å†…çš„è„‘ç”µå›¾-æ–‡æœ¬åŒ¹é…ä»»åŠ¡ï¼Œé€šè¿‡å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ å®ç°äº†è·¨è¢«è¯•å’Œè·¨æ—¶é—´çš„ç¨³å¥æƒ…ç»ªè¯†åˆ«ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŸºäºè„‘ç”µå›¾çš„æƒ…ç»ªè¯†åˆ«åœ¨ç‰¹å¾åˆ©ç”¨å’Œè·¨åŸŸæ³›åŒ–æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯å¹¶å®ç°ç¨³å¥çš„è·¨åŸŸæ€§èƒ½ã€‚

**Method:** æå‡ºEmotionCLIPæ¡†æ¶ï¼Œå°†æƒ…ç»ªè¯†åˆ«é‡æ–°å®šä¹‰ä¸ºè„‘ç”µå›¾-æ–‡æœ¬åŒ¹é…ä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†SST-LegoViTéª¨å¹²ç½‘ç»œï¼Œé€šè¿‡å¤šå°ºåº¦å·ç§¯å’ŒTransformeræ¨¡å—æ•è·ç©ºé—´ã€é¢‘è°±å’Œæ—¶é—´ç‰¹å¾ã€‚

**Result:** åœ¨SEEDå’ŒSEED-IVæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œè·¨è¢«è¯•å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°88.69%å’Œ73.50%ï¼Œè·¨æ—¶é—´å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°88.46%å’Œ77.54%ï¼Œå‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ åœ¨è„‘ç”µå›¾æƒ…ç»ªè¯†åˆ«ä¸­å…·æœ‰æ˜¾è‘—æœ‰æ•ˆæ€§ï¼Œä¸ºç¨³å¥çš„è·¨åŸŸæƒ…æ„Ÿè®¡ç®—æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œè¯æ˜äº†è„‘ç”µå›¾-æ–‡æœ¬å¯¹é½ç­–ç•¥çš„ä¼˜è¶Šæ€§ã€‚

---

#### ğŸ“„ Abstract
Electroencephalogram (EEG)-based emotion recognition is vital for affective
computing but faces challenges in feature utilization and cross-domain
generalization. This work introduces EmotionCLIP, which reformulates
recognition as an EEG-text matching task within the CLIP framework. A tailored
backbone, SST-LegoViT, captures spatial, spectral, and temporal features using
multi-scale convolution and Transformer modules. Experiments on SEED and
SEED-IV datasets show superior cross-subject accuracies of 88.69% and 73.50%,
and cross-time accuracies of 88.46% and 77.54%, outperforming existing models.
Results demonstrate the effectiveness of multimodal contrastive learning for
robust EEG emotion recognition.


### [17] [TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning](https://arxiv.org/abs/2511.05489)
*Junwen Pan, Qizhe Zhang, Rui Zhang, Ming Lu, Xin Wan, Yuan Zhang, Chang Liu, Qi She*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºTimeSearch-Rï¼Œå°†æ—¶åºæœç´¢é‡æ–°å®šä¹‰ä¸ºäº¤é”™å¼æ–‡æœ¬-è§†é¢‘æ€è€ƒè¿‡ç¨‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å°†è§†é¢‘ç‰‡æ®µæœç´¢æ— ç¼é›†æˆåˆ°æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¹¶åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ—¶åºæœç´¢æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„æœç´¢è¿‡ç¨‹ï¼Œç¼ºä¹ç«¯åˆ°ç«¯ä¼˜åŒ–æ¥å­¦ä¹ æœ€ä¼˜æœç´¢ç­–ç•¥ï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆæ¢ç´¢è§†é¢‘å†…å®¹å¹¶ä¿æŒé€»è¾‘æ¨ç†çš„ä¸€è‡´æ€§ã€‚

**Method:** æå‡ºTimeSearch-Ræ¡†æ¶ï¼Œå°†æ—¶åºæœç´¢é‡æ–°å®šä¹‰ä¸ºäº¤é”™å¼æ–‡æœ¬-è§†é¢‘æ€è€ƒè¿‡ç¨‹ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œå¹¶å¼•å…¥å¸¦æœ‰å®Œæ•´æ€§è‡ªéªŒè¯çš„GRPO-CSVæ–¹æ³•ï¼Œåˆ©ç”¨åŒä¸€ç­–ç•¥æ¨¡å‹éªŒè¯æœç´¢å¸§çš„å……åˆ†æ€§ä»¥æé«˜è§†é¢‘æ¨ç†çš„å®Œæ•´æ€§ã€‚

**Result:** åœ¨Haystack-LVBenchã€Haystack-Ego4Dç­‰æ—¶åºæœç´¢åŸºå‡†ä»¥åŠVideoMMEã€MLVUç­‰é•¿è§†é¢‘ç†è§£åŸºå‡†ä¸Šå‡å–å¾—æ˜¾è‘—æå‡ï¼Œåœ¨LongVideoBenchä¸Šç›¸æ¯”åŸºç¡€æ¨¡å‹Qwen2.5-VLæå‡4.1%ï¼Œç›¸æ¯”å…ˆè¿›è§†é¢‘æ¨ç†æ¨¡å‹Video-R1æå‡2.0%ï¼Œåˆ›ä¸‹æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å°†æ—¶åºæœç´¢é‡æ–°å®šä¹‰ä¸ºäº¤é”™å¼æ¨ç†è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ï¼Œå¼ºåŒ–å­¦ä¹ ä¸å®Œæ•´æ€§è‡ªéªŒè¯çš„ç»“åˆèƒ½å¤Ÿæ˜¾è‘—æå‡é•¿è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œä¸ºå¤æ‚è§†é¢‘åˆ†æä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Temporal search aims to identify a minimal set of relevant frames from tens
of thousands based on a given query, serving as a foundation for accurate
long-form video understanding. Existing works attempt to progressively narrow
the search space. However, these approaches typically rely on a hand-crafted
search process, lacking end-to-end optimization for learning optimal search
strategies. In this paper, we propose TimeSearch-R, which reformulates temporal
search as interleaved text-video thinking, seamlessly integrating searching
video clips into the reasoning process through reinforcement learning (RL).
However, applying RL training methods, such as Group Relative Policy
Optimization (GRPO), to video reasoning can result in unsupervised intermediate
search decisions. This leads to insufficient exploration of the video content
and inconsistent logical reasoning. To address these issues, we introduce GRPO
with Completeness Self-Verification (GRPO-CSV), which gathers searched video
frames from the interleaved reasoning process and utilizes the same policy
model to verify the adequacy of searched frames, thereby improving the
completeness of video reasoning. Additionally, we construct datasets
specifically designed for the SFT cold-start and RL training of GRPO-CSV,
filtering out samples with weak temporal dependencies to enhance task
difficulty and improve temporal search capabilities. Extensive experiments
demonstrate that TimeSearch-R achieves significant improvements on temporal
search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as
long-form video understanding benchmarks like VideoMME and MLVU. Notably,
TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%
improvement over the base model Qwen2.5-VL and 2.0% over the advanced video
reasoning model Video-R1. Our code is available at
https://github.com/Time-Search/TimeSearch-R.


### [18] [PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization](https://arxiv.org/abs/2511.05393)
*Zehui Feng, Tian Qiu, Tong Wu, Junxuan Li, Huayuan Xu, Ting Han*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºPreResQ-R1ï¼Œä¸€ç§åå¥½-å“åº”è§£è€¦çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ç»Ÿä¸€ç»å¯¹åˆ†æ•°å›å½’å’Œç›¸å¯¹æ’åºä¸€è‡´æ€§æ¥æå‡è§†è§‰è´¨é‡è¯„ä¼°æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨ä»…ä½¿ç”¨6Kå›¾åƒå’Œ28Kè§†é¢‘è¿›è¡Œå¼ºåŒ–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šä¸ªIQAå’ŒVQAåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰è´¨é‡è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒæˆ–ä»…æ’åºç›®æ ‡ï¼Œå¯¼è‡´æ¨ç†æµ…å±‚ã€åˆ†æ•°æ ¡å‡†å·®ä»¥åŠè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚è¿™äº›å±€é™æ€§é˜»ç¢äº†æ¨¡å‹è¿›è¡Œç»†ç²’åº¦ã€ç¨³å®šä¸”å¯è§£é‡Šçš„æ„ŸçŸ¥è´¨é‡æ¨ç†ã€‚

**Method:** PreResQ-R1é‡‡ç”¨åå¥½-å“åº”è§£è€¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¼•å…¥åŒåˆ†æ”¯å¥–åŠ±å…¬å¼åˆ†åˆ«å»ºæ¨¡æ ·æœ¬å†…å“åº”ä¸€è‡´æ€§å’Œæ ·æœ¬é—´åå¥½å¯¹é½ï¼Œå¹¶é€šè¿‡ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–è¿›è¡Œä¼˜åŒ–ã€‚å¯¹äºè§†é¢‘è´¨é‡è¯„ä¼°ï¼Œè®¾è®¡äº†å…¨å±€-æ—¶é—´å’Œå±€éƒ¨-ç©ºé—´æ•°æ®æµç­–ç•¥æ¥æ‰©å±•é™æ€å›¾åƒå¤„ç†èƒ½åŠ›ã€‚

**Result:** åœ¨ä»…ä½¿ç”¨6Kå›¾åƒå’Œ28Kè§†é¢‘è¿›è¡Œå¼ºåŒ–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒPreResQ-R1åœ¨10ä¸ªIQAå’Œ5ä¸ªVQAåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—æœ€å…ˆè¿›ç»“æœï¼Œåœ¨IQAä»»åŠ¡ä¸­SRCCå’ŒPLCCæŒ‡æ ‡åˆ†åˆ«è¶…è¶Šç°æœ‰æ–¹æ³•5.30%å’Œ2.15%ã€‚æ¨¡å‹è¿˜äº§ç”Ÿäº†ä¸äººç±»å¯¹é½çš„æ¨ç†è½¨è¿¹ï¼Œæ­ç¤ºäº†è´¨é‡åˆ¤æ–­èƒŒåçš„æ„ŸçŸ¥çº¿ç´¢ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨ç»Ÿä¸€ç»å¯¹å’Œç›¸å¯¹è´¨é‡è¯„ä¼°ç›®æ ‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿå®ç°ç»†ç²’åº¦ã€ç¨³å®šä¸”å¯è§£é‡Šçš„é“¾å¼æ¨ç†ã€‚è¶…è¶Šå®šé‡æ€§èƒ½æå‡ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ¨ç†è½¨è¿¹ä¸ºç†è§£äººç±»æ„ŸçŸ¥è´¨é‡åˆ¤æ–­æä¾›äº†æ–°çš„æ´å¯Ÿï¼Œæ¨åŠ¨äº†å¯è§£é‡Šè´¨é‡è¯„ä¼°çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Visual Quality Assessment (QA) seeks to predict human perceptual judgments of
visual fidelity. While recent multimodal large language models (MLLMs) show
promise in reasoning about image and video quality, existing approaches mainly
rely on supervised fine-tuning or rank-only objectives, resulting in shallow
reasoning, poor score calibration, and limited cross-domain generalization. We
propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning
framework that unifies absolute score regression and relative ranking
consistency within a single reasoning-driven optimization scheme. Unlike prior
QA methods, PreResQ-R1 introduces a dual-branch reward formulation that
separately models intra-sample response coherence and inter-sample preference
alignment, optimized via Group Relative Policy Optimization (GRPO). This design
encourages fine-grained, stable, and interpretable chain-of-thought reasoning
about perceptual quality. To extend beyond static imagery, we further design a
global-temporal and local-spatial data flow strategy for Video Quality
Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and
28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5
VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30%
and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it
produces human-aligned reasoning traces that reveal the perceptual cues
underlying quality judgments. Code and model are available.


### [19] [Semantic-Guided Natural Language and Visual Fusion for Cross-Modal Interaction Based on Tiny Object Detection](https://arxiv.org/abs/2511.05474)
*Xian-Hong Huang, Hui-Kai Su, Chi-Chia Sun, Jun-Wei Hsieh*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè¯­ä¹‰å¼•å¯¼è‡ªç„¶è¯­è¨€å¤„ç†ä¸å…ˆè¿›è§†è§‰è¯†åˆ«éª¨å¹²ç½‘ç»œçš„è·¨æ¨¡æ€å¾®å°ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡BERTè¯­è¨€æ¨¡å‹ä¸CNN-based PRB-FPN-Netçš„é›†æˆï¼Œæ˜¾è‘—æå‡äº†å°ç›®æ ‡å’Œå¤æ‚ç›®æ ‡çš„æ£€æµ‹ç²¾åº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ–¹æ³•åœ¨å¤„ç†å¾®å°å’Œå¤æ‚ç›®æ ‡æ—¶çš„ç²¾åº¦ä¸è¶³é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹å¦‚ä½•æœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ä»¥æå‡æ£€æµ‹æ€§èƒ½çš„ç ”ç©¶ç©ºç™½ã€‚

**Method:** è¯¥æ–¹æ³•æ•´åˆäº†BERTè¯­è¨€æ¨¡å‹ä¸åŸºäºCNNçš„å¹¶è¡Œæ®‹å·®åŒèåˆç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆPRB-FPN-Netï¼‰ï¼Œé‡‡ç”¨ELANã€MSPå’ŒCSPç­‰åˆ›æ–°éª¨å¹²æ¶æ„ä¼˜åŒ–ç‰¹å¾æå–ä¸èåˆï¼Œé€šè¿‡è¯å½¢è¿˜åŸå’Œå¾®è°ƒæŠ€æœ¯å°†æ–‡æœ¬è¾“å…¥çš„è¯­ä¹‰çº¿ç´¢ä¸è§†è§‰ç‰¹å¾å¯¹é½ã€‚

**Result:** åœ¨COCOå’ŒObjects365æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨COCO2017éªŒè¯é›†ä¸Šè¾¾åˆ°52.6%çš„å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰ï¼Œæ˜¾è‘—ä¼˜äºYOLO-Worldï¼ŒåŒæ—¶ä»…æ¶ˆè€—Transformer-basedæ¨¡å‹ï¼ˆå¦‚GLIPï¼‰ä¸€åŠçš„å‚æ•°ï¼Œä¸åŒéª¨å¹²ç½‘ç»œï¼ˆELANã€MSPã€CSPï¼‰çš„æµ‹è¯•è¿›ä¸€æ­¥è¯æ˜äº†å…¶å¤„ç†å¤šå°ºåº¦ç›®æ ‡çš„é«˜æ•ˆæ€§ã€‚

**Conclusion:** æœ¬ç ”ç©¶å¼ºè°ƒäº†å°†è‡ªç„¶è¯­è¨€ç†è§£ä¸å…ˆè¿›éª¨å¹²æ¶æ„é›†æˆçš„æ½œåŠ›ï¼Œä¸ºç‰©ä½“æ£€æµ‹çš„å‡†ç¡®æ€§ã€æ•ˆç‡å’Œå®é™…åº”ç”¨é€‚åº”æ€§è®¾ç«‹äº†æ–°çš„åŸºå‡†ï¼Œå±•ç¤ºäº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­å®ç°å¯æ‰©å±•æ€§å’Œé²æ£’æ€§çš„æœ‰æ•ˆé€”å¾„ã€‚

---

#### ğŸ“„ Abstract
This paper introduces a cutting-edge approach to cross-modal interaction for
tiny object detection by combining semantic-guided natural language processing
with advanced visual recognition backbones. The proposed method integrates the
BERT language model with the CNN-based Parallel Residual Bi-Fusion Feature
Pyramid Network (PRB-FPN-Net), incorporating innovative backbone architectures
such as ELAN, MSP, and CSP to optimize feature extraction and fusion. By
employing lemmatization and fine-tuning techniques, the system aligns semantic
cues from textual inputs with visual features, enhancing detection precision
for small and complex objects. Experimental validation using the COCO and
Objects365 datasets demonstrates that the model achieves superior performance.
On the COCO2017 validation set, it attains a 52.6% average precision (AP),
outperforming YOLO-World significantly while maintaining half the parameter
consumption of Transformer-based models like GLIP. Several test on different of
backbones such ELAN, MSP, and CSP further enable efficient handling of
multi-scale objects, ensuring scalability and robustness in
resource-constrained environments. This study underscores the potential of
integrating natural language understanding with advanced backbone
architectures, setting new benchmarks in object detection accuracy, efficiency,
and adaptability to real-world challenges.


### [20] [Visual Spatial Tuning](https://arxiv.org/abs/2511.05491)
*Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang, Yi Lin, Hengshuang Zhao*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†è§†è§‰ç©ºé—´è°ƒä¼˜ï¼ˆVSTï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºå¤§è§„æ¨¡ç©ºé—´æ„ŸçŸ¥æ•°æ®é›†VST-På’Œç©ºé—´æ¨ç†æ•°æ®é›†VST-Rï¼Œé‡‡ç”¨ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„æ¸è¿›å¼è®­ç»ƒæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´èƒ½åŠ›ï¼ŒåŒæ—¶ä¸æŸå®³å…¶é€šç”¨èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç ”ç©¶é€šå¸¸é€šè¿‡æ·»åŠ é¢å¤–ä¸“å®¶ç¼–ç å™¨æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†è¿™ä¼šå¸¦æ¥é¢å¤–å¼€é”€å¹¶æŸå®³æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ç§é€šç”¨æ¶æ„ä¸‹çš„ç©ºé—´èƒ½åŠ›å¢å¼ºæ–¹æ³•ï¼Œä»ç©ºé—´æ„ŸçŸ¥åˆ°æ¨ç†å…¨é¢åŸ¹å…»è§†è§‰è¯­è¨€æ¨¡å‹çš„äººç±»åŒ–ç©ºé—´æ™ºèƒ½ã€‚

**Method:** æå‡ºäº†è§†è§‰ç©ºé—´è°ƒä¼˜ï¼ˆVSTï¼‰ç»¼åˆæ¡†æ¶ï¼ŒåŒ…æ‹¬æ„å»ºåŒ…å«410ä¸‡æ ·æœ¬çš„VST-Pæ•°æ®é›†è¦†ç›–19ç§ç©ºé—´æŠ€èƒ½ï¼Œä»¥åŠåŒ…å«13.5ä¸‡æ ·æœ¬çš„VST-Ræ¨ç†æ•°æ®é›†ã€‚é‡‡ç”¨æ¸è¿›å¼è®­ç»ƒæµç¨‹ï¼šé¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒå»ºç«‹åŸºç¡€ç©ºé—´çŸ¥è¯†ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æå‡ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

**Result:** VSTæ¡†æ¶åœ¨å¤šä¸ªç©ºé—´åŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€å…ˆè¿›ç»“æœï¼Œåœ¨MMSI-Benchä¸Šè¾¾åˆ°34.8%ï¼Œåœ¨VSIBenchä¸Šè¾¾åˆ°61.2%ã€‚è¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡ç©ºé—´èƒ½åŠ›çš„åŒæ—¶ï¼Œä¸ä¼šå¯¹æ¨¡å‹çš„é€šç”¨èƒ½åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹å¯ä»¥é€šè¿‡æ‰€æå‡ºçš„ç©ºé—´è°ƒä¼˜èŒƒå¼æ˜¾è‘—å¢å¼ºï¼Œä¸ºå¼€å‘æ›´å…·ç‰©ç†åŸºç¡€çš„äººå·¥æ™ºèƒ½é“ºå¹³äº†é“è·¯ã€‚è¯¥æ–¹æ³•è¯æ˜äº†åœ¨ä¸æŸå®³é€šç”¨èƒ½åŠ›çš„å‰æä¸‹ï¼Œç³»ç»Ÿæ€§åœ°åŸ¹å…»æ¨¡å‹ç©ºé—´æ™ºèƒ½çš„å¯è¡Œæ€§ã€‚

---

#### ğŸ“„ Abstract
Capturing spatial relationships from visual inputs is a cornerstone of
human-like general intelligence. Several previous studies have tried to enhance
the spatial awareness of Vision-Language Models (VLMs) by adding extra expert
encoders, which brings extra overhead and usually harms general capabilities.
To enhance the spatial ability in general architectures, we introduce Visual
Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with
human-like visuospatial abilities, from spatial perception to reasoning. We
first attempt to enhance spatial perception in VLMs by constructing a
large-scale dataset termed VST-P, which comprises 4.1 million samples spanning
19 skills across single views, multiple images, and videos. Then, we present
VST-R, a curated dataset with 135K samples that instruct models to reason in
space. In particular, we adopt a progressive training pipeline: supervised
fine-tuning to build foundational spatial knowledge, followed by reinforcement
learning to further improve spatial reasoning abilities. Without the
side-effect to general capabilities, the proposed VST consistently achieves
state-of-the-art results on several spatial benchmarks, including $34.8\%$ on
MMSI-Bench and $61.2\%$ on VSIBench. It turns out that the
Vision-Language-Action models can be significantly enhanced with the proposed
spatial tuning paradigm, paving the way for more physically grounded AI.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [21] [Evaluating LLMs' Reasoning Over Ordered Procedural Steps](https://arxiv.org/abs/2511.04688)
*Adrita Anika, Md Messal Monem Miah*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¨‹åºåºåˆ—æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œé€šè¿‡é‡æ„æ‰“ä¹±çš„é£Ÿè°±æ­¥éª¤åºåˆ—æ¥æµ‹è¯•æ¨¡å‹å¯¹ç¨‹åºé¡ºåºçš„ç†è§£èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°æ¨¡å‹æ€§èƒ½éšåºåˆ—é•¿åº¦å¢åŠ è€Œä¸‹é™ï¼Œä¸”è¾“å…¥æ­¥éª¤çš„æ›´å¤§ä½ç§»ä¼šå¯¼è‡´è¿›ä¸€æ­¥æ€§èƒ½é€€åŒ–ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç¨‹åºåºåˆ—æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®èƒ½åŠ›ï¼Œå…¶ä¸­æ­¥éª¤é¡ºåºç›´æ¥å½±å“ç»“æœã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å½“å‰LLMsåœ¨å¤„ç†ç¨‹åºåºåˆ—æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç†è§£æ­¥éª¤é—´é¡ºåºä¾èµ–å…³ç³»çš„ä»»åŠ¡ä¸­ï¼Œé€šè¿‡é£Ÿè°±è¿™ä¸€é¡ºåºè‡³å…³é‡è¦çš„é¢†åŸŸæ¥è¯„ä¼°æ¨¡å‹çš„ç¨‹åºæ¨ç†èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶ä½¿ç”¨ç²¾å¿ƒç­–åˆ’çš„é£Ÿè°±æ•°æ®é›†ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹è¯„ä¼°å¤šä¸ªLLMsã€‚æå‡ºäº†ä¸€ä¸ªç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œé‡‡ç”¨æ¥è‡ªæ’åºå’Œåºåˆ—å¯¹é½çš„æˆç†ŸæŒ‡æ ‡ï¼ŒåŒ…æ‹¬Kendall's Tauã€å½’ä¸€åŒ–æœ€é•¿å…¬å…±å­åºåˆ—å’Œå½’ä¸€åŒ–ç¼–è¾‘è·ç¦»ï¼Œè¿™äº›æŒ‡æ ‡æ•æ‰äº†æ’åºè´¨é‡çš„ä¸åŒæ–¹é¢ã€‚

**Result:** å®éªŒåˆ†æè¡¨æ˜æ¨¡å‹æ€§èƒ½éšåºåˆ—é•¿åº¦å¢åŠ è€Œä¸‹é™ï¼Œåæ˜ äº†æ›´é•¿ç¨‹åºå¸¦æ¥çš„é¢å¤–å¤æ‚æ€§ã€‚åŒæ—¶å‘ç°è¾“å…¥ä¸­æ›´å¤§çš„æ­¥éª¤ä½ç§»ï¼ˆå¯¹åº”æ›´ä¸¥é‡çš„æ‰“ä¹±ï¼‰ä¼šå¯¼è‡´è¿›ä¸€æ­¥æ€§èƒ½é€€åŒ–ï¼Œè¿™äº›æŒ‡æ ‡å…±åŒæ­ç¤ºäº†æ¨¡å‹åœ¨ç¨‹åºåºåˆ—é‡æ„ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚

**Conclusion:** ç ”ç©¶ç»“æœçªæ˜¾äº†å½“å‰LLMsåœ¨ç¨‹åºæ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ›´é•¿å’Œæ›´æ— åºè¾“å…¥æ—¶è¡¨ç°ä¸ä½³ã€‚è¿™ä¸ºæ”¹è¿›æ¨¡å‹å¯¹ç¨‹åºé¡ºåºçš„ç†è§£èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶æŒ‡å‡ºäº†åœ¨å¤æ‚ç¨‹åºæ¨ç†ä»»åŠ¡ä¸­éœ€è¦è¿›ä¸€æ­¥å‘å±•çš„æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Reasoning over procedural sequences, where the order of steps directly
impacts outcomes, is a critical capability for large language models (LLMs). In
this work, we study the task of reconstructing globally ordered sequences from
shuffled procedural steps, using a curated dataset of food recipes, a domain
where correct sequencing is essential for task success. We evaluate several
LLMs under zero-shot and few-shot settings and present a comprehensive
evaluation framework that adapts established metrics from ranking and sequence
alignment. These include Kendall's Tau, Normalized Longest Common Subsequence
(NLCS), and Normalized Edit Distance (NED), which capture complementary aspects
of ordering quality. Our analysis shows that model performance declines with
increasing sequence length, reflecting the added complexity of longer
procedures. We also find that greater step displacement in the input,
corresponding to more severe shuffling, leads to further degradation. These
findings highlight the limitations of current LLMs in procedural reasoning,
especially with longer and more disordered inputs.


### [22] [Surprisal reveals diversity gaps in image captioning and different scorers change the story](https://arxiv.org/abs/2511.04754)
*Nikolai Ilinykh, Simon Dobnik*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶å¼•å…¥åŸºäºæƒŠå¥‡åº¦æ–¹å·®çš„å¤šæ ·æ€§åº¦é‡æ–¹æ³•ï¼Œç”¨äºé‡åŒ–å›¾åƒæè¿°ä»»åŠ¡çš„è¯æ±‡å¤šæ ·æ€§ï¼Œå¹¶å‘ç°ä¾èµ–å•ä¸€è¯„åˆ†æ¨¡å‹ä¼šå®Œå…¨é¢ å€’å…³äºäººç±»ä¸æ¨¡å‹å¤šæ ·æ€§çš„ç»“è®ºã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å›¾åƒæè¿°ä»»åŠ¡ç¼ºä¹æœ‰æ•ˆçš„è¯æ±‡å¤šæ ·æ€§é‡åŒ–æ–¹æ³•ï¼Œç°æœ‰è¯„ä¼°æŒ‡æ ‡éš¾ä»¥å‡†ç¡®è¡¡é‡æè¿°æ–‡æœ¬çš„è¯­è¨€å¤šæ ·æ€§ï¼Œéœ€è¦å¼€å‘æ›´å¯é çš„å¤šæ ·æ€§è¯„ä¼°æ¡†æ¶ã€‚

**Method:** æå‡ºåŸºäºæƒŠå¥‡åº¦æ–¹å·®çš„å¤šæ ·æ€§åº¦é‡æ–¹æ³•ï¼Œä½¿ç”¨n-gramè¯­è¨€æ¨¡å‹å’Œé€šç”¨è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ†å™¨ï¼Œåœ¨MSCOCOæµ‹è¯•é›†ä¸Šæ¯”è¾ƒäº”ç§æœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€å¤§æ¨¡å‹ä¸äººç±»æè¿°ï¼Œé‡‡ç”¨è´ªå©ªè§£ç å’Œæ ¸é‡‡æ ·ä¸¤ç§è§£ç ç­–ç•¥ã€‚

**Result:** ä½¿ç”¨æè¿°è®­ç»ƒçš„è¯­è¨€æ¨¡å‹è¯„åˆ†æ—¶ï¼Œäººç±»æè¿°çš„æƒŠå¥‡åº¦æ–¹å·®çº¦ä¸ºæ¨¡å‹çš„ä¸¤å€ï¼Œä½†ä½¿ç”¨é€šç”¨è¯­è¨€æ¨¡å‹é‡æ–°è¯„åˆ†åï¼Œè¿™ä¸€æ¨¡å¼å®Œå…¨åè½¬ï¼Œè¡¨æ˜è¯„åˆ†å™¨é€‰æ‹©å¯¹å¤šæ ·æ€§è¯„ä¼°ç»“è®ºå…·æœ‰å†³å®šæ€§å½±å“ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜å›¾åƒæè¿°å¤šæ ·æ€§è¯„ä¼°å¿…é¡»è€ƒè™‘å¤šä¸ªè¯„åˆ†å™¨çš„ç»“æœï¼Œå•ä¸€è¯„åˆ†å™¨å¯èƒ½å¯¼è‡´å®Œå…¨é”™è¯¯çš„ç»“è®ºï¼Œä¸ºæ„å»ºæ›´ç¨³å¥çš„å¤šæ ·æ€§è¯„ä¼°æ¡†æ¶æä¾›äº†é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
We quantify linguistic diversity in image captioning with surprisal variance
- the spread of token-level negative log-probabilities within a caption set. On
the MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs,
decoded with greedy and nucleus sampling, to human captions. Measured with a
caption-trained n-gram LM, humans display roughly twice the surprisal variance
of models, but rescoring the same captions with a general-language model
reverses the pattern. Our analysis introduces the surprisal-based diversity
metric for image captioning. We show that relying on a single scorer can
completely invert conclusions, thus, robust diversity evaluation must report
surprisal under several scorers.


### [23] [SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents](https://arxiv.org/abs/2511.04910)
*Jaehoon Lee, Sohyun Kim, Wanggeun Park, Geon Lee, Seungkyung Kim, Minyoung Lee*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†SDS KoPub VDRï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹éŸ©æ–‡å…¬å…±æ–‡æ¡£æ£€ç´¢çš„å¤§è§„æ¨¡å…¬å¼€åŸºå‡†ï¼ŒåŒ…å«361ä¸ªçœŸå®ä¸–ç•Œæ–‡æ¡£å’Œ600ä¸ªæŸ¥è¯¢-é¡µé¢-ç­”æ¡ˆä¸‰å…ƒç»„ï¼Œé€šè¿‡åŒä»»åŠ¡è¯„ä¼°æ­ç¤ºäº†å¤šæ¨¡æ€æ£€ç´¢ä¸­çš„æ˜¾è‘—æ€§èƒ½å·®è·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰æ–‡æ¡£æ£€ç´¢åŸºå‡†ä¸»è¦å¿½è§†éè‹±è¯­è¯­è¨€å’Œå®˜æ–¹å‡ºç‰ˆç‰©çš„ç»“æ„å¤æ‚æ€§ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹é’ˆå¯¹éŸ©æ–‡å…¬å…±æ–‡æ¡£çš„å¯é è¯„ä¼°èµ„æºï¼Œè¿™é™åˆ¶äº†å¤šæ¨¡æ€AIåœ¨å¤æ‚çœŸå®ä¸–ç•Œæ–‡æ¡£æ™ºèƒ½ä¸­çš„å‘å±•ã€‚

**Method:** åŸºäº361ä¸ªçœŸå®ä¸–ç•ŒéŸ©æ–‡å…¬å…±æ–‡æ¡£æ„å»ºå¤§è§„æ¨¡è¯­æ–™åº“ï¼Œé‡‡ç”¨å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆ600ä¸ªæŸ¥è¯¢-é¡µé¢-ç­”æ¡ˆä¸‰å…ƒç»„ï¼Œå¹¶é€šè¿‡ä¸¥æ ¼çš„äººå·¥éªŒè¯ç¡®ä¿äº‹å®å‡†ç¡®æ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼Œè¯„ä¼°æ¶µç›–æ–‡æœ¬æ£€ç´¢å’Œå¤šæ¨¡æ€æ£€ç´¢ä¸¤ç§äº’è¡¥ä»»åŠ¡ã€‚

**Result:** åŒä»»åŠ¡è¯„ä¼°æ˜¾ç¤ºå³ä½¿åœ¨æœ€å…ˆè¿›æ¨¡å‹ä¸­ä¹Ÿå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦è·¨æ¨¡æ€æ¨ç†çš„å¤šæ¨¡æ€åœºæ™¯ä¸­ï¼Œçªæ˜¾äº†å½“å‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚è§†è§‰å…ƒç´ å’Œè·¨æ¨¡æ€ç†è§£æ–¹é¢çš„å±€é™æ€§ã€‚

**Conclusion:** SDS KoPub VDRä¸ä»…ä¸ºæ–‡æœ¬å’Œå¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡æä¾›äº†ä¸¥æ ¼ç»†ç²’åº¦çš„è¯„ä¼°æ¡†æ¶ï¼Œè¿˜ä¸ºæ¨è¿›å¤æ‚çœŸå®ä¸–ç•Œæ–‡æ¡£æ™ºèƒ½ä¸­çš„å¤šæ¨¡æ€AIå‘å±•æŒ‡æ˜äº†æ¸…æ™°çš„æŠ€æœ¯è·¯çº¿å›¾ã€‚

---

#### ğŸ“„ Abstract
Existing benchmarks for visual document retrieval (VDR) largely overlook
non-English languages and the structural complexity of official publications.
To address this critical gap, we introduce SDS KoPub VDR, the first
large-scale, publicly available benchmark for retrieving and understanding
Korean public documents. The benchmark is built upon a corpus of 361 real-world
documents (40,781 pages), including 256 files under the KOGL Type 1 license and
105 from official legal portals, capturing complex visual elements like tables,
charts, and multi-column layouts. To establish a challenging and reliable
evaluation set, we constructed 600 query-page-answer triples. These were
initially generated using multimodal models (e.g., GPT-4o) and subsequently
underwent a rigorous human verification and refinement process to ensure
factual accuracy and contextual relevance. The queries span six major public
domains and are systematically categorized by the reasoning modality required:
text-based, visual-based (e.g., chart interpretation), and cross-modal. We
evaluate SDS KoPub VDR on two complementary tasks that reflect distinct
retrieval paradigms: (1) text-only retrieval, which measures a model's ability
to locate relevant document pages based solely on textual signals, and (2)
multimodal retrieval, which assesses retrieval performance when visual features
(e.g., tables, charts, and layouts) are jointly leveraged alongside text. This
dual-task evaluation reveals substantial performance gaps, particularly in
multimodal scenarios requiring cross-modal reasoning, even for state-of-the-art
models. As a foundational resource, SDS KoPub VDR not only enables rigorous and
fine-grained evaluation across textual and multimodal retrieval tasks but also
provides a clear roadmap for advancing multimodal AI in complex, real-world
document intelligence.


### [24] [A multimodal multiplex of the mental lexicon for multilingual individuals](https://arxiv.org/abs/2511.05361)
*Maria Huynh, Wilder C. Rodrigues*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡æ„å»ºå¤šè¯­è¨€å¿ƒç†è¯å…¸çš„å¤šå±‚ç½‘ç»œæ¨¡å‹ï¼Œæ¢ç´¢è§†è§‰è¾“å…¥å¯¹å¤šè¯­è¨€ä¹ å¾—çš„å½±å“ï¼Œç‰¹åˆ«å…³æ³¨é—äº§è¯­è¨€åœ¨è¯­è¨€ä¹ å¾—è¿‡ç¨‹ä¸­çš„ä½œç”¨åŠå…¶ä¸è§†è§‰æ¨¡æ€çš„äº¤äº’æ•ˆåº”ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿä¸ŠåŒè¯­è¢«è§†ä¸ºè®¤çŸ¥è´Ÿæ‹…ï¼Œä½†è¿‘ä¸‰åå¹´ç ”ç©¶è¡¨æ˜å¤šè¯­è¨€è€…åœ¨è¯­è¨€å’Œè®¤çŸ¥ä»»åŠ¡ä¸­è¡¨ç°æ›´ä¼˜ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¤šè¯­è¨€å¿ƒç†è¯å…¸çš„ç»“æ„æœºåˆ¶ï¼Œç‰¹åˆ«å…³æ³¨è§†è§‰è¾“å…¥å¦‚ä½•å½±å“ç¿»è¯‘ä»»åŠ¡ä¸­çš„è¯­è¨€ç†Ÿç»ƒåº¦å’Œå‡†ç¡®æ€§ã€‚

**Method:** åŸºäºStellaç­‰äººçš„å¿ƒç†è¯å…¸å¤šè·¯å¤ç”¨æ¨¡å‹å’ŒDijkstraçš„åŒè¯­äº¤äº’æ¿€æ´»æ¡†æ¶ï¼Œé‡‡ç”¨Kivelaæå‡ºçš„å¤šå±‚ç½‘ç»œåŸç†ï¼Œåœ¨æ¨¡å‹ä¸­å¼•å…¥è§†è§‰æ¨¡æ€å±‚ï¼Œå°†è§†è§‰è¾“å…¥ä¸å¤šè¯­è¨€å±‚çš„è¯æ±‡è¡¨å¾ç›¸è¿æ¥ã€‚

**Result:** å®éªŒè®¾è®¡æ¯”è¾ƒäº†æ–‡æœ¬æ¡ä»¶å’Œè§†è§‰è¾“å…¥æ¡ä»¶ä¸‹çš„ç¿»è¯‘ä»»åŠ¡è¡¨ç°ï¼Œæ—¨åœ¨é‡åŒ–è§†è§‰æ¨¡æ€å¯¹å¤šè¯­è¨€ä¹ å¾—çš„å½±å“ï¼Œç‰¹åˆ«å…³æ³¨é—äº§è¯­è¨€åœ¨è¯­è¨€ä¹ å¾—è¿‡ç¨‹ä¸­çš„ä¿ƒè¿›ä½œç”¨ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†å¤šè¯­è¨€å¿ƒç†è¯å…¸çš„å¤šå±‚ç½‘ç»œç»“æ„ç‰¹æ€§ï¼Œè§†è§‰æ¨¡æ€çš„å¼•å…¥ä¸ºç†è§£å¤šè¯­è¨€ä¹ å¾—çš„è®¤çŸ¥æœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œå¯¹è¯­è¨€æ•™è‚²å’Œè®¤çŸ¥ç¥ç»ç§‘å­¦å…·æœ‰é‡è¦å¯ç¤ºæ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
Historically, bilingualism was often perceived as an additional cognitive
load that could hinder linguistic and intellectual development. However, over
the last three decades, this view has changed considerably. Numerous studies
have aimed to model and understand the architecture of the bilingual word
recognition system Dijkstra and van Heuven (2002), investigating how parallel
activation operates in the brain and how one language influences another Kroll
et al. (2015). Increasingly, evidence suggests that multilinguals, individuals
who speak three or more languages, can perform better than monolinguals in
various linguistic and cognitive tasks, such as learning an additional language
Abu-Rabia and Sanitsky (2010). This research proposal focuses on the study of
the mental lexicon and how it may be structured in individuals who speak
multiple languages. Building on the work of Stella et al. (2018), who
investigated explosive learning in humans using a multiplex model of the mental
lexicon, and the Bilingual Interactive Activation (BIA+) framework proposed by
Dijkstra and van Heuven (2002), the present study applies the same multilayer
network principles introduced by Kivela et al. (2014). Our experimental design
extends previous research by incorporating multimodality into the multiplex
model, introducing an additional layer that connects visual inputs to their
corresponding lexical representations across the multilingual layers of the
mental lexicon. In this research, we aim to explore how a heritage language
influences the acquisition of another language. Specifically, we ask: Does the
presence of visual input in a translation task influence participants'
proficiency and accuracy compared to text-only conditions?
