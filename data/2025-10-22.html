<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-22.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 40]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 6]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 6]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-coido-efficient-data-selection-for-visual-instruction-tuning-via-coupled-importance-diversity-optimization">[1] <a href="https://arxiv.org/abs/2510.17847">CoIDO: Efficient Data Selection for Visual Instruction Tuning via Coupled Importance-Diversity Optimization</a></h3>
<p><em>Yichen Yan, Ming Zhong, Qi Zhu, Xiaoling Gu, Jinpeng Chen, Huan Li</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CoIDOæ¡†æ¶ï¼Œä¸€ç§æ–°é¢–çš„åŒç›®æ ‡æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œé€šè¿‡è”åˆä¼˜åŒ–æ•°æ®é‡è¦æ€§å’Œå¤šæ ·æ€§æ¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æŒ‡ä»¤è°ƒä¼˜ä¸­çš„è®¡ç®—ç“¶é¢ˆé—®é¢˜ã€‚è¯¥æ–¹æ³•ä»…éœ€ä½¿ç”¨20%çš„éšæœºæ ·æœ¬è®­ç»ƒè½»é‡çº§è¯„åˆ†å™¨ï¼Œå°±èƒ½åœ¨å®Œæ•´æ•°æ®é›†ä¸Šé€‰æ‹©å‡ºæ€§èƒ½æ¥è¿‘å…¨æ•°æ®è®­ç»ƒçš„20%å­é›†ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¾èµ–å¤§è§„æ¨¡æŒ‡ä»¤è°ƒä¼˜æ¥å¯¹é½è§†è§‰å’Œè¯­è¨€èƒ½åŠ›ï¼Œä½†å…¨æ•°æ®é›†è®­ç»ƒçš„è®¡ç®—æˆæœ¬è¿‡é«˜æˆä¸ºä¸»è¦ç“¶é¢ˆã€‚ç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®ç¼ºé™·ï¼šå¤„ç†æ•´ä¸ªæ•°æ®é›†çš„è®¡ç®—å¼€é”€è¿‡é«˜ï¼Œä»¥åŠå°†é‡è¦æ€§å’Œå¤šæ ·æ€§åˆ†å¼€å¤„ç†å¯¼è‡´æ•°æ®é€‰æ‹©æ•ˆæœä¸ä½³ã€‚</p>
<p><strong>Method:</strong> CoIDOæ¡†æ¶é‡‡ç”¨åŒç›®æ ‡ä¼˜åŒ–æ–¹æ³•ï¼Œè”åˆä¼˜åŒ–æ•°æ®é‡è¦æ€§å’Œå¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è½»é‡çº§æ’ä»¶è¯„åˆ†å™¨ï¼Œä»…éœ€åœ¨å°è§„æ¨¡éšæœºæ ·æœ¬ä¸Šè®­ç»ƒå³å¯å­¦ä¹ å€™é€‰é›†çš„åˆ†å¸ƒç‰¹æ€§ï¼Œå¤§å¹…é™ä½è®¡ç®—éœ€æ±‚ã€‚é€šè¿‡åŒæ–¹å·®ä¸ç¡®å®šæ€§å…¬å¼ï¼ŒCoIDOåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆå¹³è¡¡é‡è¦æ€§å’Œå¤šæ ·æ€§ç›®æ ‡ã€‚</p>
<p><strong>Result:</strong> å®éªŒä¸­ä½¿ç”¨ä»…20%çš„éšæœºæ ·æœ¬è®­ç»ƒCoIDOè¯„åˆ†å™¨ï¼Œç„¶ååœ¨å®Œæ•´æ•°æ®é›†ä¸Šé€‰æ‹©20%å­é›†è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜ã€‚åœ¨LLaVA-1.5-7Bæ¨¡å‹ä¸Šçš„åä¸ªä¸‹æ¸¸ä»»åŠ¡æµ‹è¯•ä¸­ï¼Œæ‰€é€‰å­é›†å¹³å‡è¾¾åˆ°äº†å…¨æ•°æ®å¾®è°ƒæ€§èƒ½çš„98.2%ã€‚</p>
<p><strong>Conclusion:</strong> CoIDOæ¡†æ¶è¯æ˜é€šè¿‡è”åˆä¼˜åŒ–é‡è¦æ€§å’Œå¤šæ ·æ€§ï¼Œå¯ä»¥åœ¨å¤§å¹…å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ºå¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæä¾›äº†é«˜æ•ˆçš„æ•°æ®é€‰æ‹©è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰æ˜¾è‘—çš„å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) rely heavily on instruction tuning
to align vision and language capabilities, yet the computational cost of
training on large-scale datasets remains a major bottleneck. Existing data
selection methods aim to mitigate this by selecting important and diverse
subsets, but they often suffer from two critical drawbacks: high computational
overhead from processing the entire dataset and suboptimal data selection due
to separate treatment of importance and diversity.
  We introduce CoIDO, a novel dual-objective framework that jointly optimizes
data importance and diversity to overcome these challenges. Unlike existing
approaches that require costly evaluations across the whole dataset, CoIDO
employs a lightweight plug-in scorer. This scorer is trained on just a small
random sample of data to learn the distribution of the candidate set,
drastically reducing computational demands. By leveraging a homoscedastic
uncertainty-based formulation, CoIDO effectively balances importance and
diversity during training, enabling efficient and scalable data selection.
  In our experiments, we trained the CoIDO scorer using only 20 percent of
randomly sampled data. Once trained, CoIDO was applied to the entire dataset to
select a 20 percent subset for instruction tuning. On the widely used
LLaVA-1.5-7B model across ten downstream tasks, this selected subset achieved
an impressive 98.2 percent of the performance of full-data fine-tuning, on
average.</p>
<h3 id="2-muse-model-based-uncertainty-aware-similarity-estimation-for-zero-shot-2d-object-detection-and-segmentation">[2] <a href="https://arxiv.org/abs/2510.17866">MUSE: Model-based Uncertainty-aware Similarity Estimation for zero-shot 2D Object Detection and Segmentation</a></h3>
<p><em>Sungmin Cho, Sungbum Park, Insoo Oh</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>MUSEæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒã€åŸºäºæ¨¡å‹çš„é›¶æ ·æœ¬2Dç›®æ ‡æ£€æµ‹ä¸åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡å¤šè§†è§’æ¨¡æ¿æ¸²æŸ“å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥ç›¸ä¼¼åº¦ä¼°è®¡ï¼Œåœ¨BOP Challenge 2025ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³é›¶æ ·æœ¬2Dç›®æ ‡æ£€æµ‹ä¸åˆ†å‰²ä¸­æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§è¿‡çš„3Dç‰©ä½“ä¸Šå®ç°å‡†ç¡®çš„2Dæ£€æµ‹ä¸åˆ†å‰²ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚</p>
<p><strong>Method:</strong> MUSEæ¡†æ¶åˆ©ç”¨3Dæœªè§ç‰©ä½“çš„2Då¤šè§†è§’æ¨¡æ¿æ¸²æŸ“å’Œè¾“å…¥æŸ¥è¯¢å›¾åƒçš„2Dç›®æ ‡æè®®ï¼Œåœ¨åµŒå…¥é˜¶æ®µæ•´åˆç±»åˆ«å’Œè¡¥ä¸åµŒå…¥ï¼Œä½¿ç”¨å¹¿ä¹‰å‡å€¼æ± åŒ–å½’ä¸€åŒ–è¡¥ä¸åµŒå…¥ä»¥é«˜æ•ˆæ•è·å…¨å±€å’Œå±€éƒ¨è¡¨ç¤ºï¼Œåœ¨åŒ¹é…é˜¶æ®µé‡‡ç”¨ç»“åˆç»å¯¹å’Œç›¸å¯¹ç›¸ä¼¼åº¦çš„è”åˆç›¸ä¼¼åº¦åº¦é‡ï¼Œå¹¶é€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥ç›®æ ‡å…ˆéªŒä¼˜åŒ–ç›¸ä¼¼åº¦å¾—åˆ†ã€‚</p>
<p><strong>Result:</strong> åœ¨BOP Challenge 2025ä¸­ï¼ŒMUSEæ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒæˆ–å¾®è°ƒï¼Œåœ¨Classic Coreã€H3å’ŒIndustrialä¸‰ä¸ªèµ›é“å‡æ’åç¬¬ä¸€ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> MUSEæä¾›äº†ä¸€ä¸ªå¼ºå¤§ä¸”å¯æ³›åŒ–çš„é›¶æ ·æœ¬2Dç›®æ ‡æ£€æµ‹ä¸åˆ†å‰²æ¡†æ¶ï¼Œè¯æ˜äº†åŸºäºæ¨¡å‹çš„æ–¹æ³•åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„ç‰©ä½“è¯†åˆ«ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>In this work, we introduce MUSE (Model-based Uncertainty-aware Similarity
Estimation), a training-free framework designed for model-based zero-shot 2D
object detection and segmentation. MUSE leverages 2D multi-view templates
rendered from 3D unseen objects and 2D object proposals extracted from input
query images. In the embedding stage, it integrates class and patch embeddings,
where the patch embeddings are normalized using generalized mean pooling (GeM)
to capture both global and local representations efficiently. During the
matching stage, MUSE employs a joint similarity metric that combines absolute
and relative similarity scores, enhancing the robustness of matching under
challenging scenarios. Finally, the similarity score is refined through an
uncertainty-aware object prior that adjusts for proposal reliability. Without
any additional training or fine-tuning, MUSE achieves state-of-the-art
performance on the BOP Challenge 2025, ranking first across the Classic Core,
H3, and Industrial tracks. These results demonstrate that MUSE offers a
powerful and generalizable framework for zero-shot 2D object detection and
segmentation.</p>
<h3 id="3-3d-weakly-supervised-semantic-segmentation-via-class-aware-and-geometry-guided-pseudo-label-refinement">[3] <a href="https://arxiv.org/abs/2510.17875">3D Weakly Supervised Semantic Segmentation via Class-Aware and Geometry-Guided Pseudo-Label Refinement</a></h3>
<p><em>Xiaoxu Xu, Xuexun Liu, Jinlong Li, Yitian Yuan, Qiudan Zhang, Lin Ma, Nicu Sebe, Xu Wang</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é›†æˆ3Då‡ ä½•å…ˆéªŒçš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡ç±»åˆ«æ„ŸçŸ¥æ ‡ç­¾ç²¾ç‚¼å’Œå‡ ä½•æ„ŸçŸ¥æ ‡ç­¾ç²¾ç‚¼æœºåˆ¶ç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾ï¼Œåœ¨ScanNetå’ŒS3DISåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰3Då¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²æ–¹æ³•é¢ä¸´ä¼ªæ ‡ç­¾è´¨é‡ä½å’Œ3Då‡ ä½•å…ˆéªŒåˆ©ç”¨ä¸è¶³çš„æŠ€æœ¯ç“¶é¢ˆï¼Œè¿™é™åˆ¶äº†é«˜æ€§èƒ½æ¨¡å‹çš„å‘å±•ã€‚</p>
<p><strong>Method:</strong> æå‡ºç±»åˆ«æ„ŸçŸ¥æ ‡ç­¾ç²¾ç‚¼æ¨¡å—ç”Ÿæˆå¹³è¡¡å‡†ç¡®çš„ä¼ªæ ‡ç­¾ï¼Œå¼€å‘å‡ ä½•æ„ŸçŸ¥æ ‡ç­¾ç²¾ç‚¼ç»„ä»¶é›†æˆéšå¼3Då‡ ä½•çº¦æŸè¿‡æ»¤ä½ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾ï¼Œå¹¶é‡‡ç”¨æ ‡ç­¾æ›´æ–°ç­–ç•¥ç»“åˆè‡ªè®­ç»ƒæ‰©å±•æ ‡ç­¾è¦†ç›–èŒƒå›´ã€‚</p>
<p><strong>Result:</strong> åœ¨ScanNetå’ŒS3DISåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ— ç›‘ç£è®¾ç½®ä¸‹å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡é²æ£’è®¾è®¡ä¿æŒç«äº‰æ€§ç²¾åº¦ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•è¯æ˜äº†é›†æˆ3Då‡ ä½•å…ˆéªŒå’Œè¿­ä»£æ ‡ç­¾ç²¾ç‚¼ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæå‡å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼Œä¸ºå¼€å‘é«˜æ€§èƒ½3Då¼±ç›‘ç£æ¨¡å‹æä¾›äº†å¯è¡Œè·¯å¾„ï¼ŒåŒæ—¶å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>3D weakly supervised semantic segmentation (3D WSSS) aims to achieve semantic
segmentation by leveraging sparse or low-cost annotated data, significantly
reducing reliance on dense point-wise annotations. Previous works mainly employ
class activation maps or pre-trained vision-language models to address this
challenge. However, the low quality of pseudo-labels and the insufficient
exploitation of 3D geometric priors jointly create significant technical
bottlenecks in developing high-performance 3D WSSS models. In this paper, we
propose a simple yet effective 3D weakly supervised semantic segmentation
method that integrates 3D geometric priors into a class-aware guidance
mechanism to generate high-fidelity pseudo labels. Concretely, our designed
methodology first employs Class-Aware Label Refinement module to generate more
balanced and accurate pseudo labels for semantic categrories. This initial
refinement stage focuses on enhancing label quality through category-specific
optimization. Subsequently, the Geometry-Aware Label Refinement component is
developed, which strategically integrates implicit 3D geometric constraints to
effectively filter out low-confidence pseudo labels that fail to comply with
geometric plausibility. Moreover, to address the challenge of extensive
unlabeled regions, we propose a Label Update strategy that integrates
Self-Training to propagate labels into these areas. This iterative process
continuously enhances pseudo-label quality while expanding label coverage,
ultimately fostering the development of high-performance 3D WSSS models.
Comprehensive experimental validation reveals that our proposed methodology
achieves state-of-the-art performance on both ScanNet and S3DIS benchmarks
while demonstrating remarkable generalization capability in unsupervised
settings, maintaining competitive accuracy through its robust design.</p>
<h3 id="4-manzaiset-a-multimodal-dataset-of-viewer-responses-to-japanese-manzai-comedy">[4] <a href="https://arxiv.org/abs/2510.18014">ManzaiSet: A Multimodal Dataset of Viewer Responses to Japanese Manzai Comedy</a></h3>
<p><em>Kazuki Kawamura, Kengo Nakai, Jun Rekimoto</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡æ—¥æœ¬æ¼«æ‰å–œå‰§è§‚ä¼—ååº”å¤šæ¨¡æ€æ•°æ®é›†ManzaiSetï¼Œé€šè¿‡åˆ†æ241åå‚ä¸è€…çš„é¢éƒ¨è§†é¢‘å’ŒéŸ³é¢‘æ•°æ®ï¼Œæ­ç¤ºäº†ä¸‰ç§ä¸åŒçš„è§‚ä¼—ç±»å‹ï¼Œå¹¶å‘ç°äº†ç§¯æçš„è§‚çœ‹é¡ºåºæ•ˆåº”ï¼Œä¸ºæ–‡åŒ–æ•æ„Ÿçš„æƒ…æ„ŸAIå¼€å‘æä¾›äº†é‡è¦èµ„æºã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æƒ…æ„Ÿè®¡ç®—é¢†åŸŸå­˜åœ¨ä¸¥é‡çš„è¥¿æ–¹ä¸­å¿ƒä¸»ä¹‰åè§ï¼Œç¼ºä¹é’ˆå¯¹éè¥¿æ–¹æ–‡åŒ–èƒŒæ™¯çš„å¨±ä¹å†…å®¹è§‚ä¼—ååº”ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯æ—¥æœ¬æ¼«æ‰å–œå‰§è¿™ç§å…·æœ‰ç‹¬ç‰¹æ–‡åŒ–ç‰¹å¾çš„è¡¨æ¼”å½¢å¼å°šæœªå¾—åˆ°ç³»ç»Ÿæ€§çš„å¤šæ¨¡æ€æ•°æ®åˆ†æã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ”¶é›†äº†241åå‚ä¸è€…è§‚çœ‹10ä¸ªä¸“ä¸šæ¼«æ‰è¡¨æ¼”çš„é¢éƒ¨è§†é¢‘å’ŒéŸ³é¢‘æ•°æ®ï¼Œé‡‡ç”¨kå‡å€¼èšç±»åˆ†æè¯†åˆ«è§‚ä¼—ç±»å‹ï¼Œä½¿ç”¨ä¸ªä½“æ°´å¹³åˆ†æè¯„ä¼°è§‚çœ‹é¡ºåºæ•ˆåº”ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åŒ–å¹½é»˜åˆ†ç±»å’Œè§‚ä¼—æ°´å¹³å“åº”å»ºæ¨¡è¿›è¡Œè·¨ç±»å‹æ¯”è¾ƒã€‚</p>
<p><strong>Result:</strong> èšç±»åˆ†æè¯†åˆ«å‡ºä¸‰ç§è§‚ä¼—ç±»å‹ï¼šé«˜ç¨³å®šæ¬£èµè€…ï¼ˆ72.8%ï¼‰ã€ä½å¯å˜ä¸‹é™è€…ï¼ˆ13.2%ï¼‰å’Œå¯å˜æ”¹å–„è€…ï¼ˆ14.0%ï¼‰ï¼Œä¸ªä½“åˆ†ææ˜¾ç¤ºæ˜¾è‘—çš„ç§¯æè§‚çœ‹é¡ºåºæ•ˆåº”ï¼ˆå¹³å‡æ–œç‡=0.488ï¼Œp&lt;0.001ï¼‰ï¼Œä½†ç»FDRæ ¡æ­£åæœªå‘ç°ç±»å‹é—´å·®å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ•°æ®é›†ä¸ºå¼€å‘æ–‡åŒ–æ•æ„Ÿçš„æƒ…æ„ŸAIç³»ç»Ÿå’Œä¸ªæ€§åŒ–å¨±ä¹ç³»ç»Ÿæä¾›äº†é‡è¦åŸºç¡€ï¼Œè¯æ˜äº†åœ¨éè¥¿æ–¹æ–‡åŒ–èƒŒæ™¯ä¸‹è§‚ä¼—ååº”çš„å¼‚è´¨æ€§ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„ç–²åŠ³å‡è¯´ï¼Œå¹¶ä¸ºè·¨æ–‡åŒ–å¨±ä¹ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>We present ManzaiSet, the first large scale multimodal dataset of viewer
responses to Japanese manzai comedy, capturing facial videos and audio from 241
participants watching up to 10 professional performances in randomized order
(94.6 percent watched &gt;= 8; analyses focus on n=228). This addresses the
Western centric bias in affective computing. Three key findings emerge: (1) k
means clustering identified three distinct viewer types: High and Stable
Appreciators (72.8 percent, n=166), Low and Variable Decliners (13.2 percent,
n=30), and Variable Improvers (14.0 percent, n=32), with heterogeneity of
variance (Brown Forsythe p &lt; 0.001); (2) individual level analysis revealed a
positive viewing order effect (mean slope = 0.488, t(227) = 5.42, p &lt; 0.001,
permutation p &lt; 0.001), contradicting fatigue hypotheses; (3) automated humor
classification (77 instances, 131 labels) plus viewer level response modeling
found no type wise differences after FDR correction. The dataset enables
culturally aware emotion AI development and personalized entertainment systems
tailored to non Western contexts.</p>
<h3 id="5-savant-semantic-analysis-with-vision-augmented-anomaly-detection">[5] <a href="https://arxiv.org/abs/2510.18034">SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection</a></h3>
<p><em>Roberto Brusnicki, David Pop, Yuan Gao, Mattia Piccinini, Johannes Betz</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>SAVANTæ˜¯ä¸€ä¸ªç»“æ„åŒ–æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚åœºæ™¯åˆ†æå’ŒåŒé˜¶æ®µæµæ°´çº¿å®ç°è‡ªåŠ¨é©¾é©¶å¼‚å¸¸åœºæ™¯æ£€æµ‹ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¯­ä¹‰å¼‚å¸¸æ£€æµ‹ä¸­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶ä½¿å¼€æºå°æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šä¸“æœ‰æ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨é¢å¯¹åˆ†å¸ƒå¤–ç½•è§è¯­ä¹‰å¼‚å¸¸åœºæ™¯æ—¶å­˜åœ¨ä¸¥é‡è„†å¼±æ€§ï¼Œç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹çš„æç¤ºæ–¹æ³•æ€§èƒ½ä¸å¯é ä¸”ä¾èµ–æ˜‚è´µçš„ä¸“æœ‰æ¨¡å‹ï¼Œé™åˆ¶äº†å®é™…éƒ¨ç½²åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> æå‡ºç»“æ„åŒ–æ¨ç†æ¡†æ¶SAVANTï¼Œé‡‡ç”¨åˆ†å±‚åœºæ™¯åˆ†æå’ŒåŒé˜¶æ®µæµæ°´çº¿ï¼šé¦–å…ˆæå–ç»“æ„åŒ–åœºæ™¯æè¿°ï¼Œç„¶åè¿›è¡Œå¤šæ¨¡æ€è¯„ä¼°ï¼Œæ¶µç›–è¡—é“ã€åŸºç¡€è®¾æ–½ã€å¯ç§»åŠ¨å¯¹è±¡å’Œç¯å¢ƒå››ä¸ªè¯­ä¹‰å±‚ï¼Œå°†VLMæ¨ç†ä»ä¸´æ—¶æç¤ºè½¬å˜ä¸ºç³»ç»Ÿåˆ†æã€‚</p>
<p><strong>Result:</strong> åœ¨çœŸå®ä¸–ç•Œé©¾é©¶åœºæ™¯ä¸­è¾¾åˆ°89.6%å¬å›ç‡å’Œ88.0%å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºéç»“æ„åŒ–åŸºçº¿ï¼›æ›´é‡è¦çš„æ˜¯ï¼Œç»è¿‡å¾®è°ƒçš„70äº¿å‚æ•°å¼€æºæ¨¡å‹Qwen2.5VLå®ç°äº†90.8%å¬å›ç‡å’Œ93.8%å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†æ‰€æœ‰è¯„ä¼°æ¨¡å‹ï¼ŒåŒæ—¶èƒ½å¤Ÿä»¥æ¥è¿‘é›¶æˆæœ¬æœ¬åœ°éƒ¨ç½²ã€‚</p>
<p><strong>Conclusion:</strong> SAVANTé€šè¿‡è‡ªåŠ¨æ ‡æ³¨9640å¤šå¼ çœŸå®ä¸–ç•Œå›¾åƒè§£å†³äº†å¼‚å¸¸æ£€æµ‹ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†å¯é ã€å¯è®¿é—®çš„è¯­ä¹‰ç›‘æ§å®ç”¨è·¯å¾„ï¼Œè¯æ˜äº†ç»“æ„åŒ–æ¡†æ¶èƒ½å¤Ÿä½¿å°å‹å¼€æºæ¨¡å‹è¶…è¶Šä¸“æœ‰æ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Autonomous driving systems remain critically vulnerable to the long-tail of
rare, out-of-distribution scenarios with semantic anomalies. While Vision
Language Models (VLMs) offer promising reasoning capabilities, naive prompting
approaches yield unreliable performance and depend on expensive proprietary
models, limiting practical deployment. We introduce SAVANT (Semantic Analysis
with Vision-Augmented Anomaly deTection), a structured reasoning framework that
achieves high accuracy and recall in detecting anomalous driving scenarios from
input images through layered scene analysis and a two-phase pipeline:
structured scene description extraction followed by multi-modal evaluation. Our
approach transforms VLM reasoning from ad-hoc prompting to systematic analysis
across four semantic layers: Street, Infrastructure, Movable Objects, and
Environment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world
driving scenarios, significantly outperforming unstructured baselines. More
importantly, we demonstrate that our structured framework enables a fine-tuned
7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8%
accuracy - surpassing all models evaluated while enabling local deployment at
near-zero cost. By automatically labeling over 9,640 real-world images with
high accuracy, SAVANT addresses the critical data scarcity problem in anomaly
detection and provides a practical path toward reliable, accessible semantic
monitoring for autonomous systems.</p>
<h3 id="6-housetour-a-virtual-real-estate-aigent">[6] <a href="https://arxiv.org/abs/2510.18054">HouseTour: A Virtual Real Estate A(I)gent</a></h3>
<p><em>Ata Ã‡elen, Marc Pollefeys, Daniel Barath, Iro Armeni</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†HouseTouræ–¹æ³•ï¼Œé€šè¿‡æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆå¹³æ»‘çš„3Dç›¸æœºè½¨è¿¹ï¼Œå¹¶ç»“åˆ3Dé«˜æ–¯æ³¼æº…æ¸²æŸ“å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†ä»å›¾åƒé›†åˆè‡ªåŠ¨ç”Ÿæˆç©ºé—´æ„ŸçŸ¥çš„è™šæ‹Ÿå¯¼è§ˆè§†é¢‘å’Œè‡ªç„¶è¯­è¨€æè¿°ã€‚è¯¥æ–¹æ³•åœ¨çœŸå®æˆ¿äº§æ•°æ®é›†ä¸ŠéªŒè¯äº†3Dç›¸æœºè½¨è¿¹æ•´åˆå¯¹æ–‡æœ¬ç”Ÿæˆæ€§èƒ½çš„æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å‡ ä½•æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†3Dç©ºé—´ä¸­çš„ç›¸æœºè½¨è¿¹ç”Ÿæˆå’Œæè¿°ä»»åŠ¡ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ä»ç°æœ‰3Dç©ºé—´å›¾åƒé›†åˆè‡ªåŠ¨ç”Ÿæˆä¸“ä¸šè´¨é‡è™šæ‹Ÿå¯¼è§ˆè§†é¢‘çš„æŒ‘æˆ˜ï¼Œæ¶ˆé™¤å¯¹ä¸“ä¸šè®¾å¤‡å’Œä¸“ä¸šçŸ¥è¯†çš„éœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŸºäºæ‰©æ•£è¿‡ç¨‹çš„å¹³æ»‘ç›¸æœºè½¨è¿¹ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨å·²çŸ¥ç›¸æœºä½å§¿ä½œä¸ºçº¦æŸæ¡ä»¶ï¼Œå¹¶å°†3Då‡ ä½•ä¿¡æ¯æ•´åˆåˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å®ç°ç©ºé—´æ„ŸçŸ¥æè¿°ã€‚é‡‡ç”¨3Dé«˜æ–¯æ³¼æº…æŠ€æœ¯æ¸²æŸ“è½¨è¿¹ä¸Šçš„æ–°è§†è§’ï¼Œå¹¶æ„å»ºäº†åŒ…å«1200å¤šä¸ªæˆ¿å±‹å¯¼è§ˆè§†é¢‘çš„HouseTouræ•°æ®é›†ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œå°†3Dç›¸æœºè½¨è¿¹æ•´åˆåˆ°æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­ç›¸æ¯”ç‹¬ç«‹å¤„ç†å„é¡¹ä»»åŠ¡çš„æ–¹æ³•æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚ç ”ç©¶å¼•å…¥äº†æ–°çš„è”åˆè¯„ä¼°æŒ‡æ ‡ï¼ŒéªŒè¯äº†ç«¯åˆ°ç«¯ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œåœ¨çœŸå®æˆ¿äº§åº”ç”¨åœºæ™¯ä¸­å®ç°äº†ä¸“ä¸šè´¨é‡çš„è§†é¢‘ç”Ÿæˆã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å®ç°äº†æ— éœ€ä¸“ä¸šè®¾å¤‡æˆ–çŸ¥è¯†çš„è‡ªåŠ¨åŒ–ä¸“ä¸šè´¨é‡è§†é¢‘ç”Ÿæˆï¼Œä¸ºæˆ¿åœ°äº§å’Œæ—…æ¸¸åº”ç”¨æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚3Då‡ ä½•ä¿¡æ¯çš„æ•´åˆæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€3Dåœºæ™¯ç†è§£å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>We introduce HouseTour, a method for spatially-aware 3D camera trajectory and
natural language summary generation from a collection of images depicting an
existing 3D space. Unlike existing vision-language models (VLMs), which
struggle with geometric reasoning, our approach generates smooth video
trajectories via a diffusion process constrained by known camera poses and
integrates this information into the VLM for 3D-grounded descriptions. We
synthesize the final video using 3D Gaussian splatting to render novel views
along the trajectory. To support this task, we present the HouseTour dataset,
which includes over 1,200 house-tour videos with camera poses, 3D
reconstructions, and real estate descriptions. Experiments demonstrate that
incorporating 3D camera trajectories into the text generation process improves
performance over methods handling each task independently. We evaluate both
individual and end-to-end performance, introducing a new joint metric. Our work
enables automated, professional-quality video creation for real estate and
touristic applications without requiring specialized expertise or equipment.</p>
<h3 id="7-chimera-compositional-image-generation-using-part-based-concepting">[7] <a href="https://arxiv.org/abs/2510.18083">Chimera: Compositional Image Generation using Part-based Concepting</a></h3>
<p><em>Shivam Singh, Yiming Chen, Agneet Chatterjee, Amit Raj, James Hays, Yezhou Yang, Chitra Baral</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Chimeraï¼Œä¸€ç§ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ä»å¤šä¸ªæºå›¾åƒä¸­ç»„åˆç‰¹å®šéƒ¨åˆ†ç”Ÿæˆæ–°å¯¹è±¡ï¼Œæ— éœ€ç”¨æˆ·æŒ‡å®šçš„æ©ç æˆ–æ³¨é‡Šã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºè¯­ä¹‰åŸå­æ•°æ®é›†å’Œè®­ç»ƒå…·æœ‰éƒ¨åˆ†æ¡ä»¶å¼•å¯¼çš„æ‰©æ•£å…ˆéªŒæ¨¡å‹ï¼Œåœ¨éƒ¨åˆ†å¯¹é½å’Œç»„åˆå‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆæ¨¡å‹è™½ç„¶æ“…é•¿ä»æ–‡æœ¬æˆ–å•å¼ å›¾åƒåˆæˆå›¾åƒï¼Œä½†ç¼ºä¹å¯¹ä»å¤šä¸ªæºå›¾åƒç‰¹å®šéƒ¨åˆ†ç»„åˆå¯¹è±¡çš„æ˜¾å¼æ§åˆ¶èƒ½åŠ›ï¼Œä¸”é€šå¸¸éœ€è¦ç”¨æˆ·æä¾›æ©ç æˆ–æ³¨é‡Šã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€é™åˆ¶ï¼Œå¼€å‘èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç²¾ç¡®ç»„åˆä¸åŒæºå›¾åƒéƒ¨åˆ†çš„ç”Ÿæˆæ¨¡å‹ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é¦–å…ˆåŸºäº464ä¸ªç‹¬ç‰¹ï¼ˆéƒ¨åˆ†ï¼Œä¸»ä½“ï¼‰å¯¹æ„å»ºè¯­ä¹‰åŸå­æ•°æ®é›†ï¼Œç”Ÿæˆ37kæç¤ºå¹¶ä½¿ç”¨é«˜ä¿çœŸæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åˆæˆç›¸åº”å›¾åƒã€‚è®­ç»ƒè‡ªå®šä¹‰æ‰©æ•£å…ˆéªŒæ¨¡å‹ï¼Œé‡‡ç”¨éƒ¨åˆ†æ¡ä»¶å¼•å¯¼æŠ€æœ¯ï¼Œé€šè¿‡å¼•å¯¼å›¾åƒæ¡ä»¶ç‰¹å¾æ¥åŒæ—¶å¼ºåˆ¶è¯­ä¹‰ä¸€è‡´æ€§å’Œç©ºé—´å¸ƒå±€ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡äººç±»è¯„ä¼°å’Œæå‡ºçš„å®¢è§‚æŒ‡æ ‡PartEvaléªŒè¯ï¼ŒChimeraåœ¨éƒ¨åˆ†å¯¹é½å’Œç»„åˆå‡†ç¡®æ€§æ–¹é¢æ¯”åŸºçº¿æ–¹æ³•é«˜å‡º14%ï¼Œåœ¨è§†è§‰è´¨é‡æ–¹é¢é«˜å‡º21%ã€‚æ–°æå‡ºçš„PartEvalæŒ‡æ ‡èƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°ç”Ÿæˆç®¡é“çš„ä¿çœŸåº¦å’Œç»„åˆå‡†ç¡®æ€§ã€‚</p>
<p><strong>Conclusion:</strong> Chimeraå±•ç¤ºäº†æ— éœ€ç”¨æˆ·æŒ‡å®šæ©ç å³å¯å®ç°ç²¾ç¡®éƒ¨åˆ†ç»„åˆçš„å¯è¡Œæ€§ï¼Œä¸ºä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆæä¾›äº†æ–°çš„ç»„åˆæ§åˆ¶èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨éƒ¨åˆ†å¯¹é½å’Œè§†è§‰è´¨é‡æ–¹é¢çš„æ˜¾è‘—æå‡è¡¨æ˜éƒ¨åˆ†æ¡ä»¶å¼•å¯¼ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥ç»„åˆç”Ÿæˆç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Personalized image generative models are highly proficient at synthesizing
images from text or a single image, yet they lack explicit control for
composing objects from specific parts of multiple source images without user
specified masks or annotations. To address this, we introduce Chimera, a
personalized image generation model that generates novel objects by combining
specified parts from different source images according to textual instructions.
To train our model, we first construct a dataset from a taxonomy built on 464
unique (part, subject) pairs, which we term semantic atoms. From this, we
generate 37k prompts and synthesize the corresponding images with a
high-fidelity text-to-image model. We train a custom diffusion prior model with
part-conditional guidance, which steers the image-conditioning features to
enforce both semantic identity and spatial layout. We also introduce an
objective metric PartEval to assess the fidelity and compositional accuracy of
generation pipelines. Human evaluations and our proposed metric show that
Chimera outperforms other baselines by 14% in part alignment and compositional
accuracy and 21% in visual quality.</p>
<h3 id="8-online-in-context-distillation-for-low-resource-vision-language-models">[8] <a href="https://arxiv.org/abs/2510.18117">Online In-Context Distillation for Low-Resource Vision Language Models</a></h3>
<p><em>Zhiqi Kang, Rahaf Aljundi, Vaggelis Dorovatas, Karteek Alahari</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨çº¿ä¸Šä¸‹æ–‡è’¸é¦æ–¹æ³•ï¼Œä½¿å°å‹è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶é€šè¿‡ç¨€ç–æ¼”ç¤ºä¸æ›´å¼ºçš„æ•™å¸ˆæ¨¡å‹åä½œï¼Œæ˜¾è‘—æå‡ä½èµ„æºç¯å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥æ–¹æ³•åœ¨æœ‰é™è®¡ç®—é¢„ç®—ä¸‹è¶…è¶Šäº†å¾®è°ƒæ–¹æ³•ï¼Œå°†å°å‹æ¨¡å‹çš„æ€§èƒ½æå‡é«˜è¾¾33%ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºã€é¢„ç®—å—é™ç¯å¢ƒä¸­éƒ¨ç½²ä¸åˆ‡å®é™…ï¼Œè€Œå°å‹æ¨¡å‹è™½ç„¶é«˜æ•ˆä½†éœ€è¦æ˜‚è´µçš„å¾®è°ƒæ‰èƒ½ç¼©å°ä¸å¤§å‹æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚ç ”ç©¶æ—¨åœ¨è§£å†³å¦‚ä½•åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹æœ‰æ•ˆæå‡å°å‹è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ ¸å¿ƒé—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºåœ¨çº¿ä¸Šä¸‹æ–‡è’¸é¦æ¡†æ¶ï¼ŒåŒ…å«è·¨æ¨¡æ€æ¼”ç¤ºé€‰æ‹©ç­–ç•¥ã€æ•™å¸ˆæµ‹è¯•æ—¶ç¼©æ”¾ä»¥å‡å°‘å™ªå£°ã€ä»¥åŠå­¦ç”Ÿä¸ç¡®å®šæ€§æ¡ä»¶åŒ–æ¥åŠ¨æ€å¡«å……æ¼”ç¤ºæ± å¹¶æœ€å°åŒ–æ•™å¸ˆæŸ¥è¯¢ã€‚è¯¥æ–¹æ³•åŸºäºå¯¹è§†è§‰è¯­è¨€ä¸Šä¸‹æ–‡å­¦ä¹ å¯è¡Œæ€§çš„æ·±å…¥åˆ†æï¼Œè¯†åˆ«äº†é€‚åˆçš„æ¨¡å‹è§„æ¨¡å’Œé€‰æ‹©æ ‡å‡†ã€‚</p>
<p><strong>Result:</strong> ICDæ–¹æ³•æ˜¾è‘—æå‡äº†å°å‹æ¨¡å‹çš„æ€§èƒ½ï¼Œæœ€é«˜æå‡è¾¾33%ï¼Œä»…éœ€ç¨€ç¼ºçš„æ•™å¸ˆæ ‡æ³¨ï¼ˆä½è‡³4%ï¼‰ã€‚åœ¨å—é™è®¡ç®—é¢„ç®—ä¸‹ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ è¡¨ç°ä¼˜äºå¾®è°ƒæ–¹æ³•ï¼Œå¹¶èƒ½å¤Ÿä¸æ•™å¸ˆçš„é›¶æ ·æœ¬æ€§èƒ½ç›¸ç«äº‰ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ä¸Šä¸‹æ–‡è’¸é¦æ˜¯ä½èµ„æºç¯å¢ƒä¸‹æå‡å°å‹è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆé€”å¾„ï¼Œä¸ºèµ„æºå—é™éƒ¨ç½²æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†åœ¨æœ‰é™è®¡ç®—é¢„ç®—ä¸‹ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ ç›¸æ¯”ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸ºæœªæ¥é«˜æ•ˆæ¨¡å‹åä½œç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>As the field continues its push for ever more resources, this work turns the
spotlight on a critical question: how can vision-language models (VLMs) be
adapted to thrive in low-resource, budget-constrained settings? While large
VLMs offer strong performance, they are impractical to deploy in such settings.
Small VLMs, on the other hand, are efficient but typically require costly
fine-tuning to close the performance gap with larger models in the deployment
domain. Inspired by the in-context learning framework, we propose an online
In-Context Distillation (ICD) method, in which a small VLM collaborates with a
stronger teacher model at inference time, distilling its knowledge via sparse
demonstrations to efficiently bridge the gap between them. Our method is built
on an in-depth analysis that identifies the scale and the choice of models for
which vision-language ICL is currently feasible, and demonstrates the advantage
of ICL over fine-tuning under constrained compute budgets. We enhance our
method with a novel cross-modal demonstration selection strategy, teacher
test-time scaling to reduce noise, and student uncertainty conditioning to
dynamically populate a demonstration pool and minimize teacher queries. Our ICD
method significantly boosts the performance of small models (up to 33%) using
scarce teacher annotations (as low as 4%), and competes with the teacher's
zero-shot performance.</p>
<h3 id="9-adapting-stereo-vision-from-objects-to-3d-lunar-surface-reconstruction-with-the-stereolunar-dataset">[9] <a href="https://arxiv.org/abs/2510.18172">Adapting Stereo Vision From Objects To 3D Lunar Surface Reconstruction with the StereoLunar Dataset</a></h3>
<p><em>Clementine Grethen, Simone Gasparini, Geraldine Morin, Jeremy Lebreton, Lucas Marti, Manuel Sanchez-Gestido</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†LunarStereoï¼Œé¦–ä¸ªæœˆçƒç«‹ä½“å›¾åƒæ•°æ®é›†ï¼Œå¹¶é€šè¿‡å¾®è°ƒMASt3Ræ¨¡å‹å®ç°äº†åœ¨æœˆçƒç¯å¢ƒä¸‹çš„é²æ£’3Dé‡å»ºï¼Œæ˜¾è‘—æå‡äº†åœ¨æœˆçƒæ¶åŠ£æ¡ä»¶ä¸‹çš„é‡å»ºæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç«‹ä½“è§†è§‰é‡å»ºæ–¹æ³•åœ¨æœˆçƒè¡¨é¢é‡å»ºä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœˆçƒè¡¨é¢ç¼ºä¹çº¹ç†ç‰¹å¾ã€å¤æ‚çš„å…‰ç…§å˜åŒ–ä»¥åŠéå…¸å‹çš„è½¨é“è½¨è¿¹ã€‚å½“å‰æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸»è¦åŸºäºäººç±»å°ºåº¦æ•°æ®é›†è®­ç»ƒï¼Œå¾ˆå°‘åœ¨è¡Œæ˜Ÿå›¾åƒä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæ— æ³•ç›´æ¥è¿ç§»åˆ°æœˆçƒç¯å¢ƒã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶å¼€å‘äº†LunarStereoæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºå…‰çº¿è¿½è¸ªæŠ€æœ¯æ¨¡æ‹Ÿçš„æœˆçƒç«‹ä½“å›¾åƒå¯¹æ•°æ®é›†ï¼Œåˆ©ç”¨é«˜åˆ†è¾¨ç‡åœ°å½¢å’Œåå°„ç‡æ¨¡å‹ç”Ÿæˆã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬é€šè¿‡å¾®è°ƒMASt3Ræ¨¡å‹æ¥é€‚åº”æœˆçƒé¢†åŸŸï¼Œä¸º3Dé‡å»ºä»»åŠ¡æä¾›ç‰©ç†åŸºç¡€ç›‘ç£ã€‚</p>
<p><strong>Result:</strong> åœ¨åˆæˆå’ŒçœŸå®æœˆçƒæ•°æ®ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯„ä¼°äº†3Dè¡¨é¢é‡å»ºå’Œç›¸å¯¹å§¿æ€ä¼°è®¡æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”é›¶æ ·æœ¬åŸºçº¿å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¸ºåœ°å¤–ç¯å¢ƒä¸­çš„è·¨å°ºåº¦æ³›åŒ–å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºæœˆçƒ3Dé‡å»ºæä¾›äº†é¦–ä¸ªé«˜è´¨é‡æ•°æ®é›†å’Œæœ‰æ•ˆçš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œè¯æ˜äº†åœ¨æ¶åŠ£è¡Œæ˜Ÿç¯å¢ƒä¸‹æ·±åº¦å­¦ä¹ çš„é€‚åº”æ€§ï¼Œä¸ºæœªæ¥ç©ºé—´æ¢ç´¢ä»»åŠ¡ä¸­çš„è‡ªä¸»å¯¼èˆªå’Œåœ°å½¢åˆ†æå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Accurate 3D reconstruction of lunar surfaces is essential for space
exploration. However, existing stereo vision reconstruction methods struggle in
this context due to the Moon's lack of texture, difficult lighting variations,
and atypical orbital trajectories. State-of-the-art deep learning models,
trained on human-scale datasets, have rarely been tested on planetary imagery
and cannot be transferred directly to lunar conditions. To address this issue,
we introduce LunarStereo, the first open dataset of photorealistic stereo image
pairs of the Moon, simulated using ray tracing based on high-resolution
topography and reflectance models. It covers diverse altitudes, lighting
conditions, and viewing angles around the lunar South Pole, offering physically
grounded supervision for 3D reconstruction tasks. Based on this dataset, we
adapt the MASt3R model to the lunar domain through fine-tuning on LunarStereo.
We validate our approach through extensive qualitative and quantitative
experiments on both synthetic and real lunar data, evaluating 3D surface
reconstruction and relative pose estimation. Extensive experiments on synthetic
and real lunar data validate the approach, demonstrating significant
improvements over zero-shot baselines and paving the way for robust cross-scale
generalization in extraterrestrial environments.</p>
<h3 id="10-raddiagseg-m-a-vision-language-model-for-joint-diagnosis-and-multi-target-segmentation-in-radiology">[10] <a href="https://arxiv.org/abs/2510.18188">RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology</a></h3>
<p><em>Chengrun Li, Corentin Royer, Haozhe Luo, Bastian Wittmann, Xia Li, Ibrahim Hamamci, Sezgin Er, Anjany Sekuboyina, Bjoern Menze</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†RadDiagSeg-Dæ•°æ®é›†å’ŒRadDiagSeg-Mæ¨¡å‹ï¼Œè§£å†³äº†åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹éš¾ä»¥åŒæ—¶ç”Ÿæˆè¯Šæ–­æ–‡æœ¬å’Œåƒç´ çº§åˆ†å‰²æ©ç çš„é—®é¢˜ï¼Œå®ç°äº†å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­å’Œçµæ´»åˆ†å‰²çš„è”åˆä»»åŠ¡ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§å¤šæ•°åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹æ— æ³•åŒæ—¶ç”Ÿæˆè¯Šæ–­æ–‡æœ¬å’Œåƒç´ çº§åˆ†å‰²æ©ç ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†ä¸´åºŠåº”ç”¨çš„å®ç”¨æ€§ï¼Œå› ä¸ºæ— æ³•åŒæ—¶æä¾›ä¸¤ç§æ¨¡æ€çš„è¾…åŠ©ç³»ç»Ÿå¯¹åŒ»å­¦ä»ä¸šè€…ä»·å€¼æœ‰é™ã€‚</p>
<p><strong>Method:</strong> é¦–å…ˆæ„å»ºäº†RadDiagSeg-Dæ•°æ®é›†ï¼Œå°†å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­å’Œå¤šç›®æ ‡åˆ†å‰²æ•´åˆä¸ºç»Ÿä¸€çš„åˆ†å±‚ä»»åŠ¡ï¼›éšååŸºäºè¯¥æ•°æ®é›†å¼€å‘äº†RadDiagSeg-Mè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿè”åˆæ‰§è¡Œå¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­å’Œçµæ´»åˆ†å‰²ã€‚</p>
<p><strong>Result:</strong> RadDiagSeg-Måœ¨å¤šç›®æ ‡æ–‡æœ¬å’Œæ©ç ç”Ÿæˆä»»åŠ¡çš„æ‰€æœ‰ç»„ä»¶ä¸Šå‡è¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ï¼Œä¸ºç›¸å…³ä»»åŠ¡å»ºç«‹äº†ç¨³å¥ä¸”å…·æœ‰ç«äº‰åŠ›çš„åŸºå‡†ï¼Œæä¾›äº†å…·æœ‰é«˜åº¦ä¿¡æ¯é‡å’Œä¸´åºŠå®ç”¨æ€§çš„è¾“å‡ºã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æœ‰æ•ˆè§£å†³äº†è¾…åŠ©è¯Šæ–­ä¸­ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„éœ€æ±‚ï¼Œé€šè¿‡è”åˆç”Ÿæˆæ–‡æœ¬å’Œåˆ†å‰²æ©ç çš„æ–¹å¼æ˜¾è‘—æå‡äº†åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„å®ç”¨ä»·å€¼ï¼Œä¸ºå¤šæ¨¡æ€åŒ»å­¦AIç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Most current medical vision language models struggle to jointly generate
diagnostic text and pixel-level segmentation masks in response to complex
visual questions. This represents a major limitation towards clinical
application, as assistive systems that fail to provide both modalities
simultaneously offer limited value to medical practitioners. To alleviate this
limitation, we first introduce RadDiagSeg-D, a dataset combining abnormality
detection, diagnosis, and multi-target segmentation into a unified and
hierarchical task. RadDiagSeg-D covers multiple imaging modalities and is
precisely designed to support the development of models that produce
descriptive text and corresponding segmentation masks in tandem. Subsequently,
we leverage the dataset to propose a novel vision-language model, RadDiagSeg-M,
capable of joint abnormality detection, diagnosis, and flexible segmentation.
RadDiagSeg-M provides highly informative and clinically useful outputs,
effectively addressing the need to enrich contextual information for assistive
diagnosis. Finally, we benchmark RadDiagSeg-M and showcase its strong
performance across all components involved in the task of multi-target
text-and-mask generation, establishing a robust and competitive baseline.</p>
<h3 id="11-visual-space-optimization-for-zero-shot-learning">[11] <a href="https://arxiv.org/abs/1907.00330">Visual Space Optimization for Zero-shot Learning</a></h3>
<p><em>Xinsheng Wang, Shanmin Pang, Jihua Zhu, Zhongyu Li, Zhiqiang Tian, Yaochen Li</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>è¯¥è®ºæ–‡æå‡ºä¸¤ç§ä¼˜åŒ–è§†è§‰ç©ºé—´çš„æ–¹æ³•æ¥æ”¹è¿›é›¶æ ·æœ¬å­¦ä¹ ï¼ŒåŒ…æ‹¬è§†è§‰åŸå‹æ–¹æ³•å’Œä¸­é—´åµŒå…¥ç©ºé—´ä¼˜åŒ–ï¼Œé€šè¿‡åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è§†è§‰ç©ºé—´ä¼˜åŒ–å¯¹é›¶æ ·æœ¬å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œå…¶ä¸­åŸå‹æ–¹æ³•è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•é€šå¸¸å°†æ·±åº¦è§†è§‰ç‰¹å¾æ„æˆçš„è§†è§‰ç©ºé—´ä½œä¸ºåµŒå…¥ç©ºé—´ï¼Œä½†è§†è§‰ç©ºé—´ä¸­å®ä¾‹çš„ç¦»æ•£åˆ†å¸ƒä½¿å¾—æ•°æ®ç»“æ„ä¸å¤Ÿæ˜æ˜¾ï¼Œè¿™é™åˆ¶äº†è¯­ä¹‰å‘é‡åœ¨è§†è§‰ç©ºé—´ä¸­çš„æœ‰æ•ˆåµŒå…¥ï¼Œå› æ­¤éœ€è¦ä¼˜åŒ–è§†è§‰ç©ºé—´ä»¥æå‡é›¶æ ·æœ¬å­¦ä¹ æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸¤ç§è§†è§‰ç©ºé—´ä¼˜åŒ–ç­–ç•¥ï¼šä¸€æ˜¯è§†è§‰åŸå‹æ–¹æ³•ï¼Œä¸ºæ¯ä¸ªè§†è§‰ç±»åˆ«å­¦ä¹ ä¸€ä¸ªè§†è§‰åŸå‹ï¼Œç”¨åŸå‹ç‰¹å¾æ›¿ä»£ç¦»æ•£çš„è§†è§‰ç‰¹å¾åºåˆ—ï¼›äºŒæ˜¯ä¸­é—´åµŒå…¥ç©ºé—´ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡å¤šå±‚æ„ŸçŸ¥æœºæ¡†æ¶ç®—æ³•å­¦ä¹ å…±åŒçš„ä¸­é—´åµŒå…¥ç©ºé—´ï¼ŒåŒæ—¶ä½¿è§†è§‰æ•°æ®ç»“æ„æ›´åŠ æ˜¾è‘—ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¼˜åŒ–è§†è§‰ç©ºé—´å¯¹é›¶æ ·æœ¬å­¦ä¹ æœ‰ç›Šï¼Œæå‡ºçš„åŸºäºåŸå‹çš„æ–¹æ³•å®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ä¼˜åŒ–è§†è§‰ç©ºé—´æ˜¯æå‡é›¶æ ·æœ¬å­¦ä¹ æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œè§†è§‰åŸå‹æ–¹æ³•é€šè¿‡ç±»çº§åˆ«è¡¨ç¤ºç®€åŒ–äº†åµŒå…¥è¿‡ç¨‹ï¼Œä¸­é—´åµŒå…¥ç©ºé—´æ–¹æ³•åˆ™é€šè¿‡ç»“æ„ä¼˜åŒ–å¢å¼ºäº†æ•°æ®å¯åˆ†æ€§ï¼Œä¸ºæœªæ¥é›¶æ ·æœ¬å­¦ä¹ ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Zero-shot learning, which aims to recognize new categories that are not
included in the training set, has gained popularity owing to its potential
ability in the real-word applications. Zero-shot learning models rely on
learning an embedding space, where both semantic descriptions of classes and
visual features of instances can be embedded for nearest neighbor search.
Recently, most of the existing works consider the visual space formulated by
deep visual features as an ideal choice of the embedding space. However, the
discrete distribution of instances in the visual space makes the data structure
unremarkable. We argue that optimizing the visual space is crucial as it allows
semantic vectors to be embedded into the visual space more effectively. In this
work, we propose two strategies to accomplish this purpose. One is the visual
prototype based method, which learns a visual prototype for each visual class,
so that, in the visual space, a class can be represented by a prototype feature
instead of a series of discrete visual features. The other is to optimize the
visual feature structure in an intermediate embedding space, and in this method
we successfully devise a multilayer perceptron framework based algorithm that
is able to learn the common intermediate embedding space and meanwhile to make
the visual data structure more distinctive. Through extensive experimental
evaluation on four benchmark datasets, we demonstrate that optimizing visual
space is beneficial for zero-shot learning. Besides, the proposed prototype
based method achieves the new state-of-the-art performance.</p>
<h3 id="12-vlsu-mapping-the-limits-of-joint-multimodal-understanding-for-ai-safety">[12] <a href="https://arxiv.org/abs/2510.18214">VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</a></h3>
<p><em>Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†è§†è§‰è¯­è¨€å®‰å…¨ç†è§£ï¼ˆVLSUï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç»†ç²’åº¦ä¸¥é‡æ€§åˆ†ç±»å’Œç»„åˆåˆ†æç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹å®‰å…¨æ€§ã€‚ç ”ç©¶å‘ç°ç°æœ‰æ¨¡å‹åœ¨è”åˆå›¾åƒ-æ–‡æœ¬æ¨ç†æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§å¤±è´¥ï¼Œå³ä½¿å•ç‹¬æ¨¡æ€åˆ†ç±»æ­£ç¡®ï¼Œç»„åˆå®‰å…¨åˆ†ç±»é”™è¯¯ç‡ä»è¾¾34%ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„å®‰å…¨è¯„ä¼°é€šå¸¸å°†è§†è§‰å’Œè¯­è¨€è¾“å…¥åˆ†å¼€å¤„ç†ï¼Œå¿½ç•¥äº†è”åˆè§£é‡Šä¸­è‰¯æ€§å†…å®¹ç»„åˆå¯èƒ½äº§ç”Ÿæœ‰å®³å½±å“çš„é£é™©ã€‚ç°æœ‰æ–¹æ³•ä¹Ÿæœªèƒ½æ¸…æ™°åŒºåˆ†æ˜æ˜¾ä¸å®‰å…¨å†…å®¹ä¸è¾¹ç•Œæ¡ˆä¾‹ï¼Œå¯¼è‡´å¯¹çœŸæ­£æœ‰å®³å†…å®¹çš„è¿‡åº¦é˜»æ­¢æˆ–æ‹’ç»ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> æˆ‘ä»¬æå‡ºäº†è§†è§‰è¯­è¨€å®‰å…¨ç†è§£ï¼ˆVLSUï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç»†ç²’åº¦ä¸¥é‡æ€§åˆ†ç±»å’Œè·¨17ä¸ªä¸åŒå®‰å…¨æ¨¡å¼çš„ç»„åˆåˆ†ææ¥ç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å®‰å…¨æ€§ã€‚é‡‡ç”¨åŒ…å«çœŸå®ä¸–ç•Œå›¾åƒå’Œäººå·¥æ ‡æ³¨çš„å¤šé˜¶æ®µæµç¨‹ï¼Œæ„å»ºäº†åŒ…å«8,187ä¸ªæ ·æœ¬çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–15ä¸ªå±å®³ç±»åˆ«ã€‚</p>
<p><strong>Result:</strong> å¯¹11ä¸ªæœ€å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°æ­ç¤ºäº†ç³»ç»Ÿæ€§è”åˆç†è§£å¤±è´¥ï¼šæ¨¡å‹åœ¨æ¸…æ™°å•æ¨¡æ€å®‰å…¨ä¿¡å·ä¸Šè¾¾åˆ°90%ä»¥ä¸Šå‡†ç¡®ç‡ï¼Œä½†åœ¨éœ€è¦è”åˆå›¾åƒ-æ–‡æœ¬æ¨ç†ç¡®å®šå®‰å…¨æ ‡ç­¾æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™è‡³20-55%ã€‚æœ€å…³é”®çš„æ˜¯ï¼Œ34%çš„è”åˆå›¾åƒ-æ–‡æœ¬å®‰å…¨åˆ†ç±»é”™è¯¯å‘ç”Ÿåœ¨ä¸ªä½“æ¨¡æ€åˆ†ç±»æ­£ç¡®çš„æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹éš¾ä»¥å¹³è¡¡æ‹’ç»ä¸å®‰å…¨å†…å®¹ä¸å“åº”å€¼å¾—å‚ä¸çš„è¾¹ç•Œæ¡ˆä¾‹ï¼Œä¾‹å¦‚æŒ‡ä»¤æ¡†æ¶å¯å°†Gemini-1.5åœ¨è¾¹ç•Œå†…å®¹ä¸Šçš„è¿‡åº¦é˜»æ­¢ç‡ä»62.4%é™è‡³10.4%ï¼Œä½†ä»£ä»·æ˜¯ä¸å®‰å…¨å†…å®¹çš„æ‹’ç»ç‡ä»90.8%é™è‡³53.9%ã€‚</p>
<p><strong>Conclusion:</strong> æˆ‘ä»¬çš„æ¡†æ¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è”åˆå›¾åƒ-æ–‡æœ¬ç†è§£æ–¹é¢çš„å¼±ç‚¹å’Œå¯¹é½å·®è·ï¼Œä¸ºç ”ç©¶ç¨³å¥è§†è§‰è¯­è¨€å®‰å…¨çš„ä¸‹ä¸€ä¸ªé‡Œç¨‹ç¢‘æä¾›äº†å…³é”®æµ‹è¯•å¹³å°ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¼€å‘èƒ½å¤Ÿè¿›è¡Œç»„åˆæ¨ç†çš„å¤šæ¨¡æ€å®‰å…¨è¯„ä¼°æ–¹æ³•çš„å¿…è¦æ€§ï¼Œä»¥è§£å†³ç°æœ‰æ–¹æ³•åœ¨åŒºåˆ†è¾¹ç•Œæ¡ˆä¾‹å’Œé˜²æ­¢è¿‡åº¦é˜»æ­¢æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Safety evaluation of multimodal foundation models often treats vision and
language inputs separately, missing risks from joint interpretation where
benign content becomes harmful in combination. Existing approaches also fail to
distinguish clearly unsafe content from borderline cases, leading to
problematic over-blocking or under-refusal of genuinely harmful content. We
present Vision Language Safety Understanding (VLSU), a comprehensive framework
to systematically evaluate multimodal safety through fine-grained severity
classification and combinatorial analysis across 17 distinct safety patterns.
Using a multi-stage pipeline with real-world images and human annotation, we
construct a large-scale benchmark of 8,187 samples spanning 15 harm categories.
Our evaluation of eleven state-of-the-art models reveals systematic joint
understanding failures: while models achieve 90%-plus accuracy on clear
unimodal safety signals, performance degrades substantially to 20-55% when
joint image-text reasoning is required to determine the safety label. Most
critically, 34% of errors in joint image-text safety classification occur
despite correct classification of the individual modalities, further
demonstrating absent compositional reasoning capabilities. Additionally, we
find that models struggle to balance refusing unsafe content while still
responding to borderline cases that deserve engagement. For example, we find
that instruction framing can reduce the over-blocking rate on borderline
content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of
under-refusing on unsafe content with refusal rate dropping from 90.8% to
53.9%. Overall, our framework exposes weaknesses in joint image-text
understanding and alignment gaps in current models, and provides a critical
test bed to enable the next milestones in research on robust vision-language
safety.</p>
<h3 id="13-blendclip-bridging-synthetic-and-real-domains-for-zero-shot-3d-object-classification-with-multimodal-pretraining">[13] <a href="https://arxiv.org/abs/2510.18244">BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining</a></h3>
<p><em>Ajinkya Khoche, GergÅ‘ LÃ¡szlÃ³ Nagy, Maciej Wozniak, Thomas Gustafsson, Patric Jensfelt</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>BlendCLIPæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡è¯¾ç¨‹å¼æ•°æ®æ··åˆç­–ç•¥æœ‰æ•ˆå¼¥åˆåˆæˆæ•°æ®ä¸çœŸå®LiDARæ‰«æä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œåœ¨é›¶æ ·æœ¬3Dç‰©ä½“åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä»…éœ€å°‘é‡çœŸå®ä¸–ç•Œæ ·æœ¬å³å¯æ˜¾è‘—æå‡æ¨¡å‹åœ¨æˆ·å¤–åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰é›¶æ ·æœ¬3Dç‰©ä½“åˆ†ç±»é¢ä¸´åˆæˆæ•°æ®ä¸çœŸå®ç¨€ç–å™ªå£°LiDARæ‰«æä¹‹é—´çš„æ˜¾è‘—é¢†åŸŸå·®è·é—®é¢˜ï¼Œä»…ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„æ–¹æ³•æ— æ³•æ³›åŒ–åˆ°æˆ·å¤–åœºæ™¯ï¼Œè€Œä»…ä½¿ç”¨çœŸå®æ•°æ®çš„æ–¹æ³•åˆç¼ºä¹è¯­ä¹‰å¤šæ ·æ€§æ¥è¯†åˆ«ç½•è§æˆ–æœªè§ç‰©ä½“ã€‚</p>
<p><strong>Method:</strong> æå‡ºBlendCLIPå¤šæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶ï¼Œé¦–å…ˆæ„å»ºä»çœŸå®é©¾é©¶æ•°æ®ä¸­æŒ–æ˜çš„å¤§è§„æ¨¡ç‰©ä½“çº§ä¸‰å…ƒç»„æ•°æ®é›†ï¼ˆç‚¹äº‘ã€å›¾åƒã€æ–‡æœ¬æè¿°ï¼‰ï¼Œæ ¸å¿ƒè´¡çŒ®æ˜¯è¯¾ç¨‹å¼æ•°æ®æ··åˆç­–ç•¥ï¼Œå…ˆåœ¨è¯­ä¹‰ä¸°å¯Œçš„åˆæˆCADæ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå†é€æ­¥é€‚åº”çœŸå®ä¸–ç•Œæ‰«æçš„ç‰¹å®šç‰¹å¾ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰é«˜æ ‡ç­¾æ•ˆç‡ï¼Œä»…éœ€åœ¨æ¯æ‰¹æ¬¡ä¸­å¼•å…¥1.5%çš„çœŸå®ä¸–ç•Œæ ·æœ¬å³å¯åœ¨nuScenesåŸºå‡†ä¸Šæå‡é›¶æ ·æœ¬å‡†ç¡®ç‡27%ï¼Œæœ€ç»ˆæ¨¡å‹åœ¨nuSceneså’ŒTruckScenesç­‰æˆ·å¤–æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œæ¯”å…ˆå‰æœ€ä½³æ–¹æ³•æå‡19.3%ï¼ŒåŒæ—¶åœ¨å¤šæ ·åŒ–åˆæˆåŸºå‡†ä¸Šä¿æŒå¼ºæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜æœ‰æ•ˆçš„é¢†åŸŸé€‚åº”è€Œéå¤§è§„æ¨¡çœŸå®ä¸–ç•Œæ ‡æ³¨æ˜¯å®ç°ç¨³å¥å¼€æ”¾è¯æ±‡3Dæ„ŸçŸ¥çš„å…³é”®ï¼Œè¯¥æ–¹æ³•ä¸ºå®é™…åº”ç”¨ä¸­çš„é›¶æ ·æœ¬3Dåˆ†ç±»æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œè¯æ˜äº†åˆæˆä¸çœŸå®æ•°æ®ç­–ç•¥æ€§ç»„åˆçš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Zero-shot 3D object classification is crucial for real-world applications
like autonomous driving, however it is often hindered by a significant domain
gap between the synthetic data used for training and the sparse, noisy LiDAR
scans encountered in the real-world. Current methods trained solely on
synthetic data fail to generalize to outdoor scenes, while those trained only
on real data lack the semantic diversity to recognize rare or unseen objects.
  We introduce BlendCLIP, a multimodal pretraining framework that bridges this
synthetic-to-real gap by strategically combining the strengths of both domains.
We first propose a pipeline to generate a large-scale dataset of object-level
triplets -- consisting of a point cloud, image, and text description -- mined
directly from real-world driving data and human annotated 3D boxes. Our core
contribution is a curriculum-based data mixing strategy that first grounds the
model in the semantically rich synthetic CAD data before progressively adapting
it to the specific characteristics of real-world scans.
  Our experiments show that our approach is highly label-efficient: introducing
as few as 1.5\% real-world samples per batch into training boosts zero-shot
accuracy on the nuScenes benchmark by 27\%. Consequently, our final model
achieves state-of-the-art performance on challenging outdoor datasets like
nuScenes and TruckScenes, improving over the best prior method by 19.3\% on
nuScenes, while maintaining strong generalization on diverse synthetic
benchmarks. Our findings demonstrate that effective domain adaptation, not
full-scale real-world annotation, is the key to unlocking robust
open-vocabulary 3D perception. Our code and dataset will be released upon
acceptance on https://github.com/kesu1/BlendCLIP.</p>
<h3 id="14-the-impact-of-image-resolution-on-biomedical-multimodal-large-language-models">[14] <a href="https://arxiv.org/abs/2510.18304">The Impact of Image Resolution on Biomedical Multimodal Large Language Models</a></h3>
<p><em>Liangyu Chen, James Burgess, Jeffrey J Nirschl, Orr Zohar, Serena Yeung-Levy</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å›¾åƒåˆ†è¾¨ç‡å¯¹ç”Ÿç‰©åŒ»å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°åŸç”Ÿåˆ†è¾¨ç‡è®­ç»ƒå’Œæ¨ç†æ˜¾è‘—æå‡æ€§èƒ½ï¼Œå¹¶æå‡ºæ··åˆåˆ†è¾¨ç‡è®­ç»ƒç­–ç•¥æ¥å¹³è¡¡è®¡ç®—çº¦æŸä¸æ€§èƒ½éœ€æ±‚ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§å¤šæ•°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸»è¦é’ˆå¯¹é€šç”¨æ•°æ®é›†ä¸­çš„ä½åˆ†è¾¨ç‡å›¾åƒè®¾è®¡ï¼Œåœ¨åº”ç”¨äºéœ€è¦é«˜åˆ†è¾¨ç‡åˆ†æçš„ç”Ÿç‰©åŒ»å­¦å›¾åƒæ—¶å­˜åœ¨å…³é”®ä¿¡æ¯ä¸¢å¤±çš„é£é™©ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶å’Œä¸´åºŠåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é€šè¿‡ç³»ç»Ÿå®éªŒè¯„ä¼°ä¸åŒåˆ†è¾¨ç‡è®¾ç½®å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼ŒåŒ…æ‹¬åŸç”Ÿåˆ†è¾¨ç‡è®­ç»ƒä¸æ¨ç†ã€è®­ç»ƒä¸æ¨ç†åˆ†è¾¨ç‡ä¸åŒ¹é…æƒ…å†µä¸‹çš„æ€§èƒ½åˆ†æï¼Œä»¥åŠæ··åˆåˆ†è¾¨ç‡è®­ç»ƒç­–ç•¥çš„å¼€å‘ä¸éªŒè¯ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼šåŸç”Ÿåˆ†è¾¨ç‡è®­ç»ƒå’Œæ¨ç†åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡æ€§èƒ½ï¼›è®­ç»ƒä¸æ¨ç†åˆ†è¾¨ç‡ä¸åŒ¹é…ä¼šä¸¥é‡é™ä½æ¨¡å‹è¡¨ç°ï¼›æ··åˆåˆ†è¾¨ç‡è®­ç»ƒèƒ½æœ‰æ•ˆç¼“è§£è¿™ç§ä¸åŒ¹é…é—®é¢˜ï¼Œåœ¨è®¡ç®—çº¦æŸä¸æ€§èƒ½éœ€æ±‚ä¹‹é—´å®ç°è‰¯å¥½å¹³è¡¡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å»ºè®®åœ¨ç”Ÿç‰©åŒ»å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¼€å‘ä¸­ä¼˜å…ˆè€ƒè™‘åŸç”Ÿåˆ†è¾¨ç‡æ¨ç†å’Œæ··åˆåˆ†è¾¨ç‡æ•°æ®é›†ï¼Œè¿™å¯¹äºä¼˜åŒ–æ¨¡å‹åœ¨ç§‘å­¦ç ”ç©¶å’Œä¸´åºŠåº”ç”¨ä¸­çš„è¡¨ç°å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ï¼Œä¸ºå®ç°å˜é©æ€§å½±å“æä¾›äº†å…³é”®ç­–ç•¥ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Imaging technologies are fundamental to biomedical research and modern
medicine, requiring analysis of high-resolution images across various
modalities. While multimodal large language models (MLLMs) show promise for
biomedical image analysis, most are designed for low-resolution images from
general-purpose datasets, risking critical information loss. We investigate how
image resolution affects MLLM performance in biomedical applications and
demonstrate that: (1) native-resolution training and inference significantly
improve performance across multiple tasks, (2) misalignment between training
and inference resolutions severely degrades performance, and (3)
mixed-resolution training effectively mitigates misalignment and balances
computational constraints with performance requirements. Based on these
findings, we recommend prioritizing native-resolution inference and
mixed-resolution datasets to optimize biomedical MLLMs for transformative
impact in scientific research and clinical applications.</p>
<h3 id="15-uwbench-a-comprehensive-vision-language-benchmark-for-underwater-understanding">[15] <a href="https://arxiv.org/abs/2510.18262">UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding</a></h3>
<p><em>Da Zhang, Chenggang Rong, Bingyu Li, Feiyu Wang, Zhiyuan Zhao, Junyu Gao, Xuelong Li</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UWBenchï¼Œä¸€ä¸ªä¸“é—¨ä¸ºæ°´ä¸‹è§†è§‰è¯­è¨€ç†è§£è®¾è®¡çš„ç»¼åˆåŸºå‡†ï¼ŒåŒ…å«15,003å¼ é«˜åˆ†è¾¨ç‡æ°´ä¸‹å›¾åƒå’Œä¸°å¯Œçš„æ ‡æ³¨æ•°æ®ï¼Œç”¨äºè¯„ä¼°å¤§è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ°´ä¸‹ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶åœºæ™¯ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨æ°´ä¸‹ç¯å¢ƒä¸­çš„åº”ç”¨ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚æ°´ä¸‹å›¾åƒå­˜åœ¨ä¸¥é‡çš„å…‰è¡°å‡ã€é¢œè‰²å¤±çœŸå’Œæ‚¬æµ®é¢—ç²’æ•£å°„ç­‰ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒæ—¶éœ€è¦æµ·æ´‹ç”Ÿæ€ç³»ç»Ÿå’Œç”Ÿç‰©åˆ†ç±»å­¦çš„ä¸“ä¸šçŸ¥è¯†ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†UWBenchåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«15,003å¼ æ¥è‡ªä¸åŒæ°´ç”Ÿç¯å¢ƒçš„é«˜åˆ†è¾¨ç‡æ°´ä¸‹å›¾åƒï¼Œæ¯å¼ å›¾åƒéƒ½é…æœ‰ç»è¿‡äººå·¥éªŒè¯çš„æ ‡æ³¨ï¼ŒåŒ…æ‹¬15,281ä¸ªå¯¹è±¡æŒ‡ä»£è¡¨è¾¾å¼å’Œ124,983ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–ä»ç‰©ä½“è¯†åˆ«åˆ°ç”Ÿæ€å…³ç³»ç†è§£çš„å„ç§æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åŸºäºUWBenchå»ºç«‹äº†ä¸‰ä¸ªç»¼åˆåŸºå‡†ï¼šè¯¦ç»†å›¾åƒæè¿°ç”Ÿæˆã€æµ·æ´‹ç”Ÿç‰©ç²¾ç¡®å®šä½çš„è§†è§‰æ¥åœ°ä»¥åŠæ°´ä¸‹ç¯å¢ƒå¤šæ¨¡æ€æ¨ç†çš„è§†è§‰é—®ç­”ã€‚åœ¨æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ°´ä¸‹ç†è§£ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå­˜åœ¨æ˜¾è‘—çš„æ”¹è¿›ç©ºé—´ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥åŸºå‡†ä¸ºæ¨è¿›æ°´ä¸‹ç¯å¢ƒä¸­çš„è§†è§‰è¯­è¨€ç ”ç©¶æä¾›äº†é‡è¦èµ„æºï¼Œæ”¯æŒæµ·æ´‹ç§‘å­¦ã€ç”Ÿæ€ç›‘æµ‹å’Œè‡ªä¸»æ°´ä¸‹æ¢ç´¢ç­‰åº”ç”¨ã€‚ç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æ°´ä¸‹ç¯å¢ƒç†è§£æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ”¹è¿›æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Large vision-language models (VLMs) have achieved remarkable success in
natural scene understanding, yet their application to underwater environments
remains largely unexplored. Underwater imagery presents unique challenges
including severe light attenuation, color distortion, and suspended particle
scattering, while requiring specialized knowledge of marine ecosystems and
organism taxonomy. To bridge this gap, we introduce UWBench, a comprehensive
benchmark specifically designed for underwater vision-language understanding.
UWBench comprises 15,003 high-resolution underwater images captured across
diverse aquatic environments, encompassing oceans, coral reefs, and deep-sea
habitats. Each image is enriched with human-verified annotations including
15,281 object referring expressions that precisely describe marine organisms
and underwater structures, and 124,983 question-answer pairs covering diverse
reasoning capabilities from object recognition to ecological relationship
understanding. The dataset captures rich variations in visibility, lighting
conditions, and water turbidity, providing a realistic testbed for model
evaluation. Based on UWBench, we establish three comprehensive benchmarks:
detailed image captioning for generating ecologically informed scene
descriptions, visual grounding for precise localization of marine organisms,
and visual question answering for multimodal reasoning about underwater
environments. Extensive experiments on state-of-the-art VLMs demonstrate that
underwater understanding remains challenging, with substantial room for
improvement. Our benchmark provides essential resources for advancing
vision-language research in underwater contexts and supporting applications in
marine science, ecological monitoring, and autonomous underwater exploration.
Our code and benchmark will be available.</p>
<h3 id="16-zero-shot-vehicle-model-recognition-via-text-based-retrieval-augmented-generation">[16] <a href="https://arxiv.org/abs/2510.18502">Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation</a></h3>
<p><em>Wei-Chia Chang, Yan-Ann Chen</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ä¸æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯çš„é›¶æ ·æœ¬è½¦è¾†å“ç‰Œå‹å·è¯†åˆ«æ–¹æ³•ï¼Œé€šè¿‡æ–‡æœ¬æ¨ç†å®ç°æ— éœ€å¤§è§„æ¨¡é‡è®­ç»ƒçš„è½¦è¾†è¯†åˆ«ï¼Œç›¸æ¯”CLIPåŸºçº¿æå‡äº†è¿‘20%çš„è¯†åˆ«å‡†ç¡®ç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è½¦è¾†å“ç‰Œå‹å·è¯†åˆ«æ–¹æ³•éš¾ä»¥é€‚åº”æ–°å‘å¸ƒè½¦å‹ï¼Œè€ŒCLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹çš„å›ºå®šé¢„è®­ç»ƒæƒé‡åœ¨ç¼ºä¹å›¾åƒç‰¹å®šå¾®è°ƒæ—¶æ€§èƒ½å—é™ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿæ”¯æŒé›¶æ ·æœ¬è¯†åˆ«ä¸”æ— éœ€å¤§è§„æ¨¡é‡è®­ç»ƒçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§é›†æˆè§†è§‰è¯­è¨€æ¨¡å‹ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æµç¨‹ï¼šé¦–å…ˆä½¿ç”¨VLMå°†è½¦è¾†å›¾åƒè½¬æ¢ä¸ºæè¿°æ€§å±æ€§ï¼Œç„¶åä¸æ–‡æœ¬ç‰¹å¾æ•°æ®åº“è¿›è¡ŒåŒ¹é…æ£€ç´¢ï¼Œæœ€åç»“åˆæ£€ç´¢ç»“æœå’Œæè¿°æ„å»ºæç¤ºï¼Œç”±è¯­è¨€æ¨¡å‹æ¨ç†å‡ºè½¦è¾†å“ç‰Œå‹å·ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•ç›¸æ¯”CLIPåŸºçº¿æå‡äº†è¿‘20%çš„è¯†åˆ«å‡†ç¡®ç‡ï¼Œè¯æ˜äº†RAGå¢å¼ºçš„è¯­è¨€æ¨¡å‹æ¨ç†åœ¨è½¦è¾†è¯†åˆ«ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•é¿å…äº†å¤§è§„æ¨¡é‡è®­ç»ƒéœ€æ±‚ï¼Œé€šè¿‡æ·»åŠ æ–°è½¦å‹çš„æ–‡æœ¬æè¿°å³å¯å®ç°å¿«é€Ÿæ›´æ–°ï¼Œå±•ç¤ºäº†RAGå¢å¼ºçš„LMæ¨ç†åœ¨æ™ºæ…§åŸå¸‚åº”ç”¨ä¸­å®ç°å¯æ‰©å±•VMMRçš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Vehicle make and model recognition (VMMR) is an important task in intelligent
transportation systems, but existing approaches struggle to adapt to newly
released models. Contrastive Language-Image Pretraining (CLIP) provides strong
visual-text alignment, yet its fixed pretrained weights limit performance
without costly image-specific finetuning. We propose a pipeline that integrates
vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to
support zero-shot recognition through text-based reasoning. A VLM converts
vehicle images into descriptive attributes, which are compared against a
database of textual features. Relevant entries are retrieved and combined with
the description to form a prompt, and a language model (LM) infers the make and
model. This design avoids large-scale retraining and enables rapid updates by
adding textual descriptions of new vehicles. Experiments show that the proposed
method improves recognition by nearly 20% over the CLIP baseline, demonstrating
the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city
applications.</p>
<h3 id="17-streamingtom-streaming-token-compression-for-efficient-video-understanding">[17] <a href="https://arxiv.org/abs/2510.18269">StreamingTOM: Streaming Token Compression for Efficient Video Understanding</a></h3>
<p><em>Xueyi Chen, Keda Tao, Kele Shao, Huan Wang</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>StreamingTOMæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€å³æ’å³ç”¨çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡å› æœæ—¶é—´ç¼©å‡å’Œåœ¨çº¿é‡åŒ–å†…å­˜æŠ€æœ¯ï¼ŒåŒæ—¶è§£å†³äº†æµå¼è§†é¢‘è¯­è¨€æ¨¡å‹ä¸­é¢„LLMå’ŒåLLMçš„æ•ˆç‡ç“¶é¢ˆï¼Œå®ç°äº†å¯é¢„æµ‹å»¶è¿Ÿçš„é«˜æ•ˆæµå¼è§†é¢‘ç†è§£ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æµå¼è§†é¢‘è§†è§‰è¯­è¨€æ¨¡å‹é¢ä¸´ä¸¤ä¸ªåŸºæœ¬çº¦æŸï¼šå› æœæ€§é™åˆ¶äº†å¯¹æœªæ¥å¸§çš„è®¿é—®ï¼Œè€Œç´¯ç§¯æ€§å¯¼è‡´ä»¤ç‰Œæ— é™å¢é•¿é€ æˆæ•ˆç‡ç“¶é¢ˆã€‚ç°æœ‰æ–¹æ³•ä»…è°ƒèŠ‚åLLMçš„kvç¼“å­˜ï¼Œè€Œå¿½ç•¥äº†æˆæœ¬é«˜æ˜‚çš„é¢„LLMé¢„å¡«å……é˜¶æ®µï¼Œè¿™æˆä¸ºæµå¼è§†é¢‘å¤„ç†çš„ä¸»è¦æ•ˆç‡éšœç¢ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼šå› æœæ—¶é—´ç¼©å‡é€šè¿‡ç›¸é‚»å¸§å˜åŒ–å’Œä»¤ç‰Œæ˜¾è‘—æ€§é€‰æ‹©ä»¤ç‰Œï¼Œä¸ºæ¯å¸§æ–½åŠ å›ºå®šé¢„ç®—ï¼Œä»…å¤„ç†ç´§å‡‘çš„è§†è§‰ä»¤ç‰Œå­é›†è€Œéå…¨éƒ¨ä»¤ç‰Œï¼›åœ¨çº¿é‡åŒ–å†…å­˜å°†ä»¤ç‰Œä»¥4ä½æ ¼å¼å­˜å‚¨ï¼ŒæŒ‰éœ€æ£€ç´¢ç›¸å…³ç»„å¹¶åé‡åŒ–ï¼Œä¿æŒæ´»åŠ¨kvç¼“å­˜æœ‰ç•Œä¸”ä¸æµé•¿åº¦æ— å…³ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è¯¥æ–¹æ³•å®ç°äº†15.7å€çš„kvç¼“å­˜å‹ç¼©ã€1.2å€çš„å³°å€¼å†…å­˜é™ä½å’Œ2å€çš„é¦–ä»¤ç‰Œæ—¶é—´åŠ é€Ÿã€‚åœ¨æ— éœ€è®­ç»ƒçš„æ–¹æ³•ä¸­ä¿æŒæœ€å…ˆè¿›ç²¾åº¦ï¼Œç¦»çº¿åŸºå‡†æµ‹è¯•å¹³å‡è¾¾63.8%ï¼ŒRVSåŸºå‡†æµ‹è¯•è¾¾55.8%/3.7ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯æ˜äº†åŒé˜¶æ®µæ–¹æ³•åœ¨å®ç°æœ‰ç•Œå¢é•¿çš„é«˜æ•ˆæµå¼è§†é¢‘ç†è§£æ–¹é¢çš„å®é™…ä¼˜åŠ¿ï¼Œé€šè¿‡åŒæ—¶å¤„ç†é¢„LLMå’ŒåLLMç“¶é¢ˆï¼Œä¸ºå®æ—¶è§†é¢‘åˆ†ææä¾›äº†å¯é¢„æµ‹å»¶è¿Ÿçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†æµå¼è§†é¢‘å¤„ç†æŠ€æœ¯çš„å®ç”¨åŒ–å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Unlike offline processing, streaming video vision-language models face two
fundamental constraints: causality and accumulation. Causality prevents access
to future frames that offline methods exploit, while accumulation causes tokens
to grow unbounded, creating efficiency bottlenecks. However, existing
approaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill
unchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage
framework that addresses both pre-LLM and post-LLM bottlenecks with predictable
latency. Causal Temporal Reduction imposes a fixed per-frame budget and selects
tokens based on adjacent-frame changes and token saliency, drastically reducing
per-frame prefill cost by processing only a compact subset of visual tokens per
frame instead of all visual tokens. Online Quantized Memory stores tokens in
4-bit format, retrieves relevant groups on demand, and dequantizes them,
keeping the active kv-cache bounded regardless of stream length. Experiments
demonstrate our method achieves $15.7\times$ kv-cache compression, $1.2\times$
lower peak memory and $2\times$ faster TTFT compared to prior SOTA.
StreamingTOM maintains state-of-the-art accuracy among training-free methods
with an average of $63.8\%$ on offline benchmarks and $55.8\%/3.7$ on RVS.
These results highlight the practical benefits of our two-stage approach for
efficient streaming video understanding with bounded growth.</p>
<h3 id="18-see-the-text-from-tokenization-to-visual-reading">[18] <a href="https://arxiv.org/abs/2510.18840">See the Text: From Tokenization to Visual Reading</a></h3>
<p><em>Ling Xing, Alex Jinpeng Wang, Rui Yan, Hongyu Qu, Zechao Li, Jinhui Tang</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSeeTokæ–¹æ³•ï¼Œå°†æ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒå¹¶åˆ©ç”¨é¢„è®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè§†è§‰ç†è§£ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»ŸåŸºäºå­è¯åˆ†è¯çš„èŒƒå¼ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ï¼Œå¹¶æå‡äº†è·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›å’ŒæŠ—å™ªé²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°ä»£å¤§è¯­è¨€æ¨¡å‹ä¾èµ–å­è¯åˆ†è¯æ–¹æ³•ï¼Œå°†æ–‡æœ¬åˆ†å‰²ä¸ºå›ºå®šè¯æ±‡è¡¨ä¸­çš„ç‰‡æ®µï¼Œè¿™ç§æ–¹æ³•åœ¨é«˜èµ„æºè¯­è¨€ä¸­æœ‰æ•ˆï¼Œä½†åœ¨ä½èµ„æºè¯­è¨€ä¸­ä¼šå¯¼è‡´è¿‡åº¦åˆ†å‰²ï¼Œäº§ç”Ÿå†—é•¿ä¸”è¯­è¨€å­¦æ— æ„ä¹‰çš„åºåˆ—ï¼Œå¹¶å¢åŠ è®¡ç®—å¼€é”€ã€‚äººç±»é˜…è¯»é€šè¿‡è¯†åˆ«å•è¯ä½œä¸ºè§†è§‰å¯¹è±¡æ¥å¤„ç†æ–‡æœ¬ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æ‹¼å†™é”™è¯¯ã€å˜å½¢å­—ä½“å’Œä¸åŒæ–‡å­—ç³»ç»Ÿï¼Œè¿™å¯å‘äº†å‘è§†è§‰ä¸­å¿ƒåŒ–æ›¿ä»£æ–¹æ¡ˆçš„è½¬å˜ã€‚</p>
<p><strong>Method:</strong> SeeTokæ–¹æ³•å°†æ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒï¼ˆè§†è§‰æ–‡æœ¬ï¼‰ï¼Œåˆ©ç”¨é¢„è®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¥è§£é‡Šè¿™äº›å›¾åƒï¼Œå¤ç”¨ä»å¤§è§„æ¨¡å¤šæ¨¡æ€è®­ç»ƒä¸­å­¦åˆ°çš„å¼ºå¤§OCRå’Œæ–‡æœ¬-è§†è§‰å¯¹é½èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è§†è§‰ä¸­å¿ƒåŒ–èŒƒå¼ï¼Œç›´æ¥å¤„ç†æ–‡æœ¬çš„è§†è§‰è¡¨ç¤ºï¼Œè€Œéä¾èµ–ä¼ ç»Ÿçš„ç¬¦å·åŒ–åˆ†è¯è¿‡ç¨‹ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªä¸åŒè¯­è¨€ä»»åŠ¡ä¸­ï¼ŒSeeTokåŒ¹é…æˆ–è¶…è¶Šäº†å­è¯åˆ†è¯å™¨çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€tokenæ•°é‡å‡å°‘äº†4.43å€ï¼ŒFLOPsé™ä½äº†70.5%ã€‚è¯¥æ–¹æ³•è¿˜å±•ç°å‡ºåœ¨è·¨è¯­è¨€æ³›åŒ–ã€å¯¹æŠ—å°åˆ·å™ªå£°çš„é²æ£’æ€§ä»¥åŠè¯­è¨€å­¦å±‚æ¬¡ç»“æ„æ–¹é¢çš„é¢å¤–ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> SeeTokæ ‡å¿—ç€ä»ç¬¦å·åŒ–åˆ†è¯å‘ç±»äººè§†è§‰é˜…è¯»çš„èŒƒå¼è½¬å˜ï¼Œä¸ºæ„å»ºæ›´è‡ªç„¶å’Œè®¤çŸ¥å¯å‘çš„è¯­è¨€æ¨¡å‹è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†è§†è§‰ä¸­å¿ƒåŒ–æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ã€æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæœªæ¥è¯­è¨€æ¨¡å‹è®¾è®¡æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>People see text. Humans read by recognizing words as visual objects,
including their shapes, layouts, and patterns, before connecting them to
meaning, which enables us to handle typos, distorted fonts, and various scripts
effectively. Modern large language models (LLMs), however, rely on subword
tokenization, fragmenting text into pieces from a fixed vocabulary. While
effective for high-resource languages, this approach over-segments low-resource
languages, yielding long, linguistically meaningless sequences and inflating
computation. In this work, we challenge this entrenched paradigm and move
toward a vision-centric alternative. Our method, SeeTok, renders text as images
(visual-text) and leverages pretrained multimodal LLMs to interpret them,
reusing strong OCR and text-vision alignment abilities learned from large-scale
multimodal training. Across three different language tasks, SeeTok matches or
surpasses subword tokenizers while requiring 4.43 times fewer tokens and
reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization,
robustness to typographic noise, and linguistic hierarchy. SeeTok signals a
shift from symbolic tokenization to human-like visual reading, and takes a step
toward more natural and cognitively inspired language models.</p>
<h3 id="19-proactive-reasoning-with-retrieval-framework-for-medical-multimodal-large-language-models">[19] <a href="https://arxiv.org/abs/2510.18303">Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models</a></h3>
<p><em>Lehan Wang, Yi Qin, Honglong Yang, Xiaomeng Li</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†é¦–ä¸ªå¤šæ¨¡æ€åŒ»å­¦æ¨ç†-æ£€ç´¢æ¡†æ¶Med-RwRï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ç­–ç•¥æ¿€åŠ±å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸»åŠ¨æ£€ç´¢å¤–éƒ¨åŒ»å­¦çŸ¥è¯†ï¼Œæ˜¾è‘—æå‡äº†åŒ»å­¦è¯Šæ–­çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŒ»å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶ä»…ä¾èµ–å†…éƒ¨çŸ¥è¯†ï¼Œå½“é‡åˆ°è¶…å‡ºè®­ç»ƒèŒƒå›´çš„ç—…ä¾‹æ—¶å®¹æ˜“äº§ç”Ÿå¹»è§‰æ¨ç†å’Œäº‹å®é”™è¯¯ï¼Œè€Œç°æœ‰çš„ä»£ç†æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ä»…é™äºå•æ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œå¿½ç•¥äº†æ¨ç†å’Œæ£€ç´¢è¿‡ç¨‹ä¸­çš„å…³é”®è§†è§‰ä¿¡æ¯ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†Med-RwRå¤šæ¨¡æ€åŒ»å­¦æ¨ç†-æ£€ç´¢æ¡†æ¶ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ç­–ç•¥é…åˆå®šåˆ¶åŒ–å¥–åŠ±æœºåˆ¶ï¼Œæ¿€åŠ±æ¨¡å‹åˆ©ç”¨è§†è§‰è¯Šæ–­å‘ç°å’Œæ–‡æœ¬ä¸´åºŠä¿¡æ¯è¿›è¡Œæœ‰æ•ˆæ£€ç´¢ï¼Œå¹¶è¿›ä¸€æ­¥æå‡ºäº†åŸºäºç½®ä¿¡åº¦çš„å›¾åƒé‡æ£€ç´¢æ–¹æ³•ç”¨äºæµ‹è¯•æ—¶çš„æ‰©å±•ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªå…¬å…±åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMed-RwRç›¸æ¯”åŸºçº¿æ¨¡å‹å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨æå‡ºçš„è¶…å£°å¿ƒåŠ¨å›¾åŸºå‡†æµ‹è¯•ECBenchä¸Šè·å¾—äº†8.8%çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å¤–éƒ¨çŸ¥è¯†é›†æˆå¯¹å¢å¼ºæ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> Med-RwRå±•ç¤ºäº†å°†å¤–éƒ¨çŸ¥è¯†é›†æˆåˆ°å¤šæ¨¡æ€åŒ»å­¦æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºåŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ›´å¯é å’Œé€æ˜çš„è¯Šæ–­æ”¯æŒã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Incentivizing the reasoning ability of Multimodal Large Language Models
(MLLMs) is essential for medical applications to transparently analyze medical
scans and provide reliable diagnosis. However, existing medical MLLMs rely
solely on internal knowledge during reasoning, leading to hallucinated
reasoning and factual inaccuracies when encountering cases beyond their
training scope. Although recent Agentic Retrieval-Augmented Generation (RAG)
methods elicit the medical model's proactive retrieval ability during
reasoning, they are confined to unimodal LLMs, neglecting the crucial visual
information during reasoning and retrieval. Consequently, we propose the first
Multimodal Medical Reasoning-with-Retrieval framework, Med-RwR, which actively
retrieves external knowledge by querying observed symptoms or domain-specific
medical concepts during reasoning. Specifically, we design a two-stage
reinforcement learning strategy with tailored rewards that stimulate the model
to leverage both visual diagnostic findings and textual clinical information
for effective retrieval. Building on this foundation, we further propose a
Confidence-Driven Image Re-retrieval (CDIR) method for test-time scaling when
low prediction confidence is detected. Evaluation on various public medical
benchmarks demonstrates Med-RwR's significant improvements over baseline
models, proving the effectiveness of enhancing reasoning capabilities with
external knowledge integration. Furthermore, Med-RwR demonstrates remarkable
generalizability to unfamiliar domains, evidenced by 8.8% performance gain on
our proposed EchoCardiography Benchmark (ECBench), despite the scarcity of
echocardiography data in the training corpus. Our data, model, and codes will
be made publicly available at https://github.com/xmed-lab/Med-RwR.</p>
<h3 id="20-grasp-any-region-towards-precise-contextual-pixel-understanding-for-multimodal-llms">[20] <a href="https://arxiv.org/abs/2510.18876">Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs</a></h3>
<p><em>Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, Zhaoxiang Zhang</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†GARæ¨¡å‹ï¼Œé€šè¿‡RoIå¯¹é½ç‰¹å¾é‡æ”¾æŠ€æœ¯è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ç»†ç²’åº¦ç†è§£ä¸­çš„å±€é™æ€§ï¼Œå®ç°äº†åŒºåŸŸçº§è§†è§‰ç†è§£å¹¶æ”¯æŒå¤šæç¤ºäº¤äº’å»ºæ¨¡ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è™½ç„¶åœ¨æ•´ä½“ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†å¤æ‚å¯†é›†åœºæ™¯æ—¶éš¾ä»¥è¿›è¡Œç»†ç²’åº¦åˆ†æå’Œå¯¹è±¡é—´å…³ç³»å»ºæ¨¡ï¼Œç°æœ‰åŒºåŸŸçº§æ–¹æ³•é€šå¸¸å­¤ç«‹ç†è§£ç»™å®šåŒºåŸŸè€Œå¿½ç•¥äº†å…³é”®çš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†GARæ¨¡å‹ï¼Œé‡‡ç”¨æœ‰æ•ˆçš„RoIå¯¹é½ç‰¹å¾é‡æ”¾æŠ€æœ¯ï¼Œæ”¯æŒåˆ©ç”¨å¿…è¦å…¨å±€ä¸Šä¸‹æ–‡è¿›è¡Œç²¾ç¡®æ„ŸçŸ¥ï¼Œå¹¶èƒ½å¤Ÿå»ºæ¨¡å¤šä¸ªæç¤ºä¹‹é—´çš„äº¤äº’å…³ç³»ï¼Œä»è€Œå®ç°å…³äºä»»æ„åŒºåŸŸçš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> GAR-1Båœ¨DLC-Benchä¸Šè¶…è¶ŠDAM-3Bè¾¾4.5åˆ†ï¼Œåœ¨GAR-Bench-VQAä¸Šç”šè‡³è¶…è¿‡InternVL3-78Bï¼Œé›¶æ ·æœ¬GAR-8Båœ¨VideoRefer-BenchQä¸Šä¼˜äºé¢†åŸŸå†…VideoRefer-7Bï¼Œå±•ç¤ºäº†å¼ºå¤§çš„è·¨è§†é¢‘è¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> GARæ¨¡å‹å®ç°äº†ä»è¢«åŠ¨æè¿°åˆ°ä¸»åŠ¨å¯¹è¯çš„èŒƒå¼è½¬å˜ï¼Œé€šè¿‡æ„å»ºGAR-Benchè¯„ä¼°æ¡†æ¶ä¸ä»…å‡†ç¡®è¡¡é‡å•åŒºåŸŸç†è§£èƒ½åŠ›ï¼Œæ›´é‡è¦çš„æ˜¯èƒ½å¤Ÿè¯„ä¼°è·¨å¤šåŒºåŸŸçš„äº¤äº’å’Œå¤æ‚æ¨ç†èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>While Multimodal Large Language Models (MLLMs) excel at holistic
understanding, they struggle in capturing the dense world with complex scenes,
requiring fine-grained analysis of intricate details and object
inter-relationships. Region-level MLLMs have been a promising step. However,
previous attempts are generally optimized to understand given regions in
isolation, neglecting crucial global contexts. To address this, we introduce
Grasp Any Region (GAR) for comprehen- sive region-level visual understanding.
Empowered by an effective RoI-aligned feature replay technique, GAR supports
(1) precise perception by leveraging necessary global contexts, and (2)
modeling interactions between multiple prompts. Together, it then naturally
achieves (3) advanced compositional reasoning to answer specific free-form
questions about any region, shifting the paradigm from passive description to
active dialogue. Moreover, we construct GAR-Bench, which not only provides a
more accurate evaluation of single-region comprehension, but also, more
importantly, measures interactions and complex reasoning across multiple
regions. Extensive experiments have demonstrated that GAR-1B not only maintains
the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5
on DLC-Bench, but also excels at modeling relationships between multiple
prompts with advanced comprehension capabilities, even surpassing InternVL3-78B
on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms
in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong
capabilities can be easily transferred to videos.</p>
<h3 id="21-beyond-single-models-mitigating-multimodal-hallucinations-via-adaptive-token-ensemble-decoding">[21] <a href="https://arxiv.org/abs/2510.18321">Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding</a></h3>
<p><em>Jinlin Li, Yuran Wang, Yifei Yuan, Xiao Zhou, Yingying Zhang, Xixian Yong, Yefeng Zheng, Xian Wu</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºè‡ªé€‚åº”ä»¤ç‰Œé›†æˆè§£ç ï¼ˆATEDï¼‰ï¼Œä¸€ç§æ— éœ€è®­ç»ƒã€åŸºäºä»¤ç‰Œçº§é›†æˆçš„æ–¹æ³•ï¼Œé€šè¿‡èšåˆå¤šä¸ªå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„é¢„æµ‹æ¥ç¼“è§£å¯¹è±¡å¹»è§‰é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨æ ‡å‡†å¹»è§‰æ£€æµ‹åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨ä¸å½±å“æµç•…æ€§å’Œç›¸å…³æ€§çš„å‰æä¸‹æœ‰æ•ˆå‡å°‘å¹»è§‰ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®¹æ˜“äº§ç”Ÿå¯¹è±¡å¹»è§‰â€”â€”æè¿°ä¸å­˜åœ¨æˆ–é”™è¯¯è¯†åˆ«çš„å¯¹è±¡ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡è¾…åŠ©è®­ç»ƒç›®æ ‡æˆ–å¤–éƒ¨æ¨¡å—éƒ¨åˆ†ç¼“è§£äº†è¿™ä¸€é—®é¢˜ï¼Œä½†åœ¨å¯æ‰©å±•æ€§ã€é€‚åº”æ€§å’Œæ¨¡å‹ç‹¬ç«‹æ€§æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºè‡ªé€‚åº”ä»¤ç‰Œé›†æˆè§£ç ï¼ˆATEDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„ä»¤ç‰Œçº§é›†æˆæ¡†æ¶ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­èšåˆå¤šä¸ªLVLMçš„é¢„æµ‹æ¥ç¼“è§£å¹»è§‰ã€‚ATEDåŠ¨æ€è®¡ç®—æ¯ä¸ªæ¨¡å‹åŸºäºä¸ç¡®å®šæ€§çš„æƒé‡ï¼Œåæ˜ å…¶åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­çš„å¯é æ€§ï¼Œå¹¶é›†æˆå¤šæ ·åŒ–çš„è§£ç è·¯å¾„ä»¥æ”¹å–„ä¸Šä¸‹æ–‡åŸºç¡€å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†å¹»è§‰æ£€æµ‹åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒATEDæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨ä¸å½±å“æµç•…æ€§å’Œç›¸å…³æ€§çš„å‰æä¸‹æœ‰æ•ˆå‡å°‘å¹»è§‰ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æå‡æ¨¡å‹é²æ£’æ€§çš„èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœå¼ºè°ƒäº†è‡ªé€‚åº”é›†æˆçš„ä¼˜åŠ¿ï¼Œä¸ºåœ¨é«˜é£é™©åº”ç”¨ä¸­æé«˜LVLMé²æ£’æ€§æŒ‡æ˜äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§æ¨¡å‹æ— å…³çš„è§£å†³æ–¹æ¡ˆï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æœ‰æ•ˆç¼“è§£å¯¹è±¡å¹»è§‰é—®é¢˜ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Large Vision-Language Models (LVLMs) have recently achieved impressive
results in multimodal tasks such as image captioning and visual question
answering. However, they remain prone to object hallucination -- generating
descriptions of nonexistent or misidentified objects. Prior work has partially
mitigated this via auxiliary training objectives or external modules, but
challenges remain in terms of scalability, adaptability, and model
independence. To address these limitations, we propose Adaptive Token Ensemble
Decoding (ATED), a training-free, token-level ensemble framework that mitigates
hallucination by aggregating predictions from multiple LVLMs during inference.
ATED dynamically computes uncertainty-based weights for each model, reflecting
their reliability at each decoding step. It also integrates diverse decoding
paths to improve contextual grounding and semantic consistency. Experiments on
standard hallucination detection benchmarks demonstrate that ATED significantly
outperforms state-of-the-art methods, reducing hallucination without
compromising fluency or relevance. Our findings highlight the benefits of
adaptive ensembling and point to a promising direction for improving LVLM
robustness in high-stakes applications. The code is available at
https://github.com/jinlin2021/ATED.</p>
<h3 id="22-enhancing-few-shot-classification-of-benchmark-and-disaster-imagery-with-attbhfa-net">[22] <a href="https://arxiv.org/abs/2510.18326">Enhancing Few-Shot Classification of Benchmark and Disaster Imagery with ATTBHFA-Net</a></h3>
<p><em>Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, Vu Duong</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ATTBHFA-Netç½‘ç»œï¼Œé€šè¿‡ç»“åˆBhattacharyyaç³»æ•°å’ŒHellingerè·ç¦»æ¥æ¯”è¾ƒå’Œèšåˆç‰¹å¾æ¦‚ç‡åˆ†å¸ƒï¼Œä»¥è§£å†³ç¾éš¾å›¾åƒåˆ†ç±»ä¸­æ•°æ®ç¨€ç¼ºå’Œç±»å†…å·®å¼‚å¤§çš„é—®é¢˜ï¼Œåœ¨å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç¾éš¾è§†è§‰è¯†åˆ«é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œå¤šæ ·æ€§æŒ‘æˆ˜ï¼Œç°æœ‰å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ä¸»è¦ä¾èµ–é€šç”¨åŸºå‡†æ•°æ®é›†è€Œç¼ºä¹é¥æ„Ÿç¾éš¾å›¾åƒï¼Œä¸”ç¾éš¾å›¾åƒå…·æœ‰é«˜ç±»å†…å˜å¼‚å’Œç±»é—´ç›¸ä¼¼æ€§ï¼Œé™åˆ¶äº†ä¼ ç»ŸåŸºäºåº¦é‡çš„å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•çš„å®é™…æ•ˆæœã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åŸºäºæ³¨æ„åŠ›çš„Bhattacharyya-Hellingerç‰¹å¾èšåˆç½‘ç»œï¼ˆATTBHFA-Netï¼‰ï¼Œçº¿æ€§ç»“åˆBhattacharyyaç³»æ•°å’ŒHellingerè·ç¦»æ¥æ¯”è¾ƒå’Œèšåˆç‰¹å¾æ¦‚ç‡åˆ†å¸ƒä»¥å½¢æˆé²æ£’åŸå‹ï¼Œå…¶ä¸­Bhattacharyyaç³»æ•°ä½œä¸ºå¯¹æ¯”è¾¹ç•Œå¢å¼ºç±»é—´å¯åˆ†æ€§ï¼ŒHellingerè·ç¦»æ­£åˆ™åŒ–åŒç±»å¯¹é½ï¼Œå¹¶æå‡ºäº†åŸºäºBhattacharyya-Hellingerè·ç¦»çš„å¯¹æ¯”æŸå¤±ä½œä¸ºä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±çš„åˆ†å¸ƒå¯¹åº”ç‰©ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªå°‘æ ·æœ¬å­¦ä¹ åŸºå‡†å’Œä¸¤ä¸ªç¾éš¾å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒATTBHFA-Netç›¸æ¯”ç°æœ‰æ–¹æ³•å…·æœ‰ä¼˜è¶Šçš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†åœ¨æ¦‚ç‡åˆ†å¸ƒå±‚é¢è¿›è¡Œç‰¹å¾æ¯”è¾ƒå’Œèšåˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤„ç†æ•°æ®ç¨€ç¼ºå’Œé«˜å˜å¼‚æ€§çš„è§†è§‰è¯†åˆ«ä»»åŠ¡æä¾›äº†æ–°æ€è·¯ï¼Œå…¶åˆ†å¸ƒå¯¹æ¯”å­¦ä¹ æ¡†æ¶å¯æ‰©å±•åˆ°å…¶ä»–éœ€è¦é²æ£’ç‰¹å¾è¡¨ç¤ºçš„é¢†åŸŸã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>The increasing frequency of natural and human-induced disasters necessitates
advanced visual recognition techniques capable of analyzing critical
photographic data. With progress in artificial intelligence and resilient
computational systems, rapid and accurate disaster classification has become
crucial for efficient rescue operations. However, visual recognition in
disaster contexts faces significant challenges due to limited and diverse data
from the difficulties in collecting and curating comprehensive, high-quality
disaster imagery. Few-Shot Learning (FSL) provides a promising approach to data
scarcity, yet current FSL research mainly relies on generic benchmark datasets
lacking remote-sensing disaster imagery, limiting its practical effectiveness.
Moreover, disaster images exhibit high intra-class variation and inter-class
similarity, hindering the performance of conventional metric-based FSL methods.
To address these issues, this paper introduces the Attention-based
Bhattacharyya-Hellinger Feature Aggregation Network (ATTBHFA-Net), which
linearly combines the Bhattacharyya coefficient and Hellinger distances to
compare and aggregate feature probability distributions for robust prototype
formation. The Bhattacharyya coefficient serves as a contrastive margin that
enhances inter-class separability, while the Hellinger distance regularizes
same-class alignment. This framework parallels contrastive learning but
operates over probability distributions rather than embedded feature points.
Furthermore, a Bhattacharyya-Hellinger distance-based contrastive loss is
proposed as a distributional counterpart to cosine similarity loss, used
jointly with categorical cross-entropy to significantly improve FSL
performance. Experiments on four FSL benchmarks and two disaster image datasets
demonstrate the superior effectiveness and generalization of ATTBHFA-Net
compared to existing approaches.</p>
<h3 id="23-gptface-generative-pre-training-of-facial-linguistic-transformer-by-span-masking-and-weakly-correlated-text-image-data">[23] <a href="https://arxiv.org/abs/2510.18345">GPTFace: Generative Pre-training of Facial-Linguistic Transformer by Span Masking and Weakly Correlated Text-image Data</a></h3>
<p><em>Yudong Li, Hao Li, Xianxu Hou, Linlin Shen</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è§„æ¨¡ç½‘ç»œæ•°æ®çš„ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨äºé¢éƒ¨çŸ¥è¯†å­¦ä¹ ï¼Œé€šè¿‡è‡ªç›‘ç£ä»»åŠ¡è®­ç»ƒå®ç°å¯æ§çš„å›¾åƒ/æ–‡æœ¬ç”Ÿæˆï¼Œå¹¶åœ¨å¤šç§é¢éƒ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¾¾åˆ°ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰é¢éƒ¨çŸ¥è¯†å­¦ä¹ çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ç ”ç©¶ç›¸å¯¹æœ‰é™ï¼Œä¸»è¦ä¾èµ–äººå·¥æ ‡æ³¨çš„é¢éƒ¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¿™ç§æ ‡æ³¨æ–¹å¼æˆæœ¬é«˜æ˜‚ä¸”è®­ç»ƒå‡ºçš„æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¹‹å¤–çš„å¯æ‰©å±•æ€§æœ‰é™ã€‚</p>
<p><strong>Method:</strong> åˆ©ç”¨ä»äº’è”ç½‘çˆ¬å–çš„åŒ…å«äººè„¸çš„æ–‡æœ¬å’Œå›¾åƒæ•°æ®ï¼Œé€šè¿‡è‡ªç›‘ç£ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒï¼ŒåŒ…æ‹¬æ©ç å›¾åƒ/è¯­è¨€å»ºæ¨¡å’Œå›¾åƒ-æ–‡æœ¬åŒ¹é…ï¼Œå¹¶åœ¨ç”Ÿæˆé˜¶æ®µåˆ©ç”¨å›¾åƒ-æ–‡æœ¬åŒ¹é…æŸå¤±å°†ç”Ÿæˆåˆ†å¸ƒæ‹‰å‘æ§åˆ¶ä¿¡å·ä»¥å®ç°å¯æ§çš„å›¾åƒ/æ–‡æœ¬ç”Ÿæˆã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§é¢éƒ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¾¾åˆ°ä¸æœ€å…ˆè¿›é¢„è®­ç»ƒæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å±æ€§åˆ†ç±»å’Œè¡¨æƒ…è¯†åˆ«ï¼ŒåŒæ—¶é€‚ç”¨äºå¹¿æ³›çš„é¢éƒ¨ç¼–è¾‘ä»»åŠ¡ï¼Œå¦‚é¢éƒ¨å±æ€§ç¼–è¾‘ã€è¡¨æƒ…æ“æ§ã€æ©ç ç§»é™¤å’Œç…§ç‰‡ä¿®å¤ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨å¤§è§„æ¨¡ç½‘ç»œæ•°æ®è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼Œä¸ºé¢éƒ¨çŸ¥è¯†å­¦ä¹ æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨é¢éƒ¨ç¼–è¾‘ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Compared to the prosperity of pre-training models in natural image
understanding, the research on large-scale pre-training models for facial
knowledge learning is still limited. Current approaches mainly rely on manually
assembled and annotated face datasets for training, but labeling such datasets
is labor-intensive and the trained models have limited scalability beyond the
training data. To address these limitations, we present a generative
pre-training model for facial knowledge learning that leverages large-scale
web-built data for training. We use texts and images containing human faces
crawled from the internet and conduct pre-training on self-supervised tasks,
including masked image/language modeling (MILM) and image-text matching (ITM).
During the generation stage, we further utilize the image-text matching loss to
pull the generation distribution towards the control signal for controllable
image/text generation. Experimental results demonstrate that our model achieves
comparable performance to state-of-the-art pre-training models for various
facial downstream tasks, such as attribution classification and expression
recognition. Furthermore, our approach is also applicable to a wide range of
face editing tasks, including face attribute editing, expression manipulation,
mask removal, and photo inpainting.</p>
<h3 id="24-av-master-dual-path-comprehensive-perception-makes-better-audio-visual-question-answering">[24] <a href="https://arxiv.org/abs/2510.18346">AV-Master: Dual-Path Comprehensive Perception Makes Better Audio-Visual Question Answering</a></h3>
<p><em>Jiayu Zhang, Qilang Ye, Shuo Ye, Xun Lin, Zihan Song, Zitong Yu</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºAV-Masteræ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å»ºæ¨¡æ—¶é—´å’Œæ¨¡æ€ç»´åº¦æ¥å¢å¼ºä»å¤æ‚éŸ³è§†é¢‘åœºæ™¯ä¸­æå–å…³é”®ä¿¡æ¯çš„èƒ½åŠ›ï¼Œåœ¨å››ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„éŸ³é¢‘è§†è§‰é—®ç­”æ–¹æ³•åœ¨æ—¶é—´é‡‡æ ·å’Œæ¨¡æ€åå¥½æ„ŸçŸ¥æ–¹é¢ç¼ºä¹è¶³å¤Ÿçš„çµæ´»æ€§å’ŒåŠ¨æ€é€‚åº”æ€§ï¼Œéš¾ä»¥æ ¹æ®é—®é¢˜èšç„¦å…³é”®ä¿¡æ¯ï¼Œé™åˆ¶äº†åœ¨å¤æ‚åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŠ¨æ€è‡ªé€‚åº”ç„¦ç‚¹é‡‡æ ·æœºåˆ¶é€æ­¥å…³æ³¨ä¸é—®é¢˜æœ€ç›¸å…³çš„éŸ³è§†é¢‘ç‰‡æ®µï¼Œå¹¶é‡‡ç”¨åå¥½æ„ŸçŸ¥ç­–ç•¥ç‹¬ç«‹å»ºæ¨¡å„æ¨¡æ€è´¡çŒ®ï¼ŒåŒæ—¶å¼•å…¥åŒè·¯å¾„å¯¹æ¯”æŸå¤±æ¥å¢å¼ºæ—¶é—´å’Œæ¨¡æ€ç»´åº¦çš„ä¸€è‡´æ€§å’Œäº’è¡¥æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAV-Masteræ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŠ¨æ€å»ºæ¨¡æ—¶é—´å’Œæ¨¡æ€ç»´åº¦å¯¹äºå¤„ç†å¤æ‚éŸ³è§†é¢‘åœºæ™¯çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†æä¾›äº†æ–°çš„æ–¹å‘ï¼Œè¡¨æ˜é€‰æ‹©æ€§æ¿€æ´»å…³é”®ç‰¹å¾å’Œè·¨æ¨¡æ€åä½œè¡¨ç¤ºå­¦ä¹ çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Audio-Visual Question Answering (AVQA) requires models to effectively utilize
both visual and auditory modalities to answer complex and diverse questions
about audio-visual scenes. However, existing methods lack sufficient
flexibility and dynamic adaptability in temporal sampling and modality
preference awareness, making it difficult to focus on key information based on
the question. This limits their reasoning capability in complex scenarios. To
address these challenges, we propose a novel framework named AV-Master. It
enhances the model's ability to extract key information from complex
audio-visual scenes with substantial redundant content by dynamically modeling
both temporal and modality dimensions. In the temporal dimension, we introduce
a dynamic adaptive focus sampling mechanism that progressively focuses on
audio-visual segments most relevant to the question, effectively mitigating
redundancy and segment fragmentation in traditional sampling methods. In the
modality dimension, we propose a preference-aware strategy that models each
modality's contribution independently, enabling selective activation of
critical features. Furthermore, we introduce a dual-path contrastive loss to
reinforce consistency and complementarity across temporal and modality
dimensions, guiding the model to learn question-specific cross-modal
collaborative representations. Experiments on four large-scale benchmarks show
that AV-Master significantly outperforms existing methods, especially in
complex reasoning tasks.</p>
<h3 id="25-imagegem-in-the-wild-generative-image-interaction-dataset-for-generative-model-personalization">[25] <a href="https://arxiv.org/abs/2510.18433">ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization</a></h3>
<p><em>Yuanhe Guo, Linxi Xie, Zhuoran Chen, Kangrui Yu, Ryan Po, Guandao Yang, Gordon Wetztein, Hongyi Wen</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ImageGemæ•°æ®é›†ï¼Œç”¨äºç ”ç©¶ç†è§£ç»†ç²’åº¦ä¸ªä½“åå¥½çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†è¯¥æ•°æ®é›†åœ¨åå¥½å¯¹é½æ¨¡å‹è®­ç»ƒã€ä¸ªæ€§åŒ–å›¾åƒæ£€ç´¢å’Œç”Ÿæˆæ¨¡å‹æ¨èæ–¹é¢çš„åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰é˜»ç¢ç†è§£ä¸ªä½“åå¥½çš„ç”Ÿæˆæ¨¡å‹å‘å±•çš„å…³é”®æŒ‘æˆ˜æ˜¯ç¼ºä¹çœŸå®ä¸–ç•Œå’Œç»†ç²’åº¦çš„ç”¨æˆ·åå¥½æ ‡æ³¨æ•°æ®ï¼Œç°æœ‰æ•°æ®é›†æ— æ³•æ”¯æŒå¯¹ä¸ªæ€§åŒ–ç”Ÿæˆæ¨¡å‹çš„æ·±å…¥ç ”ç©¶ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†åŒ…å«57Kç”¨æˆ·çœŸå®äº¤äº’æ•°æ®çš„ImageGemæ•°æ®é›†ï¼ŒåŒ…å«242Kå®šåˆ¶åŒ–LoRAæ¨¡å‹ã€3Mæ–‡æœ¬æç¤ºå’Œ5Mç”Ÿæˆå›¾åƒï¼›æå‡ºäº†åœ¨æ½œåœ¨æƒé‡ç©ºé—´ä¸­ç¼–è¾‘å®šåˆ¶åŒ–æ‰©æ•£æ¨¡å‹ä»¥å¯¹é½ä¸ªä½“ç”¨æˆ·åå¥½çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚</p>
<p><strong>Result:</strong> åˆ©ç”¨æ•°æ®é›†ä¸­çš„ç”¨æˆ·åå¥½æ ‡æ³¨æˆåŠŸè®­ç»ƒäº†æ›´å¥½çš„åå¥½å¯¹é½æ¨¡å‹ï¼›åœ¨ä¸ªæ€§åŒ–å›¾åƒæ£€ç´¢å’Œç”Ÿæˆæ¨¡å‹æ¨èä»»åŠ¡ä¸­è¯„ä¼°äº†æ£€ç´¢æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼›æå‡ºçš„ç¼–è¾‘æ¡†æ¶æœ‰æ•ˆå®ç°äº†ç”Ÿæˆæ¨¡å‹çš„ä¸ªæ€§åŒ–ã€‚</p>
<p><strong>Conclusion:</strong> ImageGemæ•°æ®é›†é¦–æ¬¡å®ç°äº†ç”Ÿæˆæ¨¡å‹ä¸ªæ€§åŒ–çš„æ–°èŒƒå¼ï¼Œä¸ºç ”ç©¶ç†è§£ä¸ªä½“åå¥½çš„ç”Ÿæˆæ¨¡å‹æä¾›äº†å…³é”®æ•°æ®åŸºç¡€ï¼Œæ¨åŠ¨äº†ä¸ªæ€§åŒ–ç”ŸæˆAIçš„å‘å±•æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>We introduce ImageGem, a dataset for studying generative models that
understand fine-grained individual preferences. We posit that a key challenge
hindering the development of such a generative model is the lack of in-the-wild
and fine-grained user preference annotations. Our dataset features real-world
interaction data from 57K users, who collectively have built 242K customized
LoRAs, written 3M text prompts, and created 5M generated images. With user
preference annotations from our dataset, we were able to train better
preference alignment models. In addition, leveraging individual user
preference, we investigated the performance of retrieval models and a
vision-language model on personalized image retrieval and generative model
recommendation. Finally, we propose an end-to-end framework for editing
customized diffusion models in a latent weight space to align with individual
user preferences. Our results demonstrate that the ImageGem dataset enables,
for the first time, a new paradigm for generative model personalization.</p>
<h3 id="26-ranking-based-preference-optimization-for-diffusion-models-from-implicit-user-feedback">[26] <a href="https://arxiv.org/abs/2510.18353">Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</a></h3>
<p><em>Yi-Lun Wu, Bo-Kai Ruan, Chiang Tseng, Hong-Han Shuai</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Diffusion-DROï¼Œä¸€ç§åŸºäºé€†å¼ºåŒ–å­¦ä¹ çš„åå¥½å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å°†åå¥½å­¦ä¹ è½¬åŒ–ä¸ºæ’åºé—®é¢˜å¹¶æ•´åˆç¦»çº¿å’Œåœ¨çº¿æ•°æ®ï¼Œè§£å†³äº†ç°æœ‰DPOæ–¹æ³•åœ¨å›¾åƒæ¦‚ç‡ä¼°è®¡å’Œæ•°æ®é›†å¤šæ ·æ€§æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•è™½ç„¶åœ¨é¿å…REINFORCEç®—æ³•æ–¹é¢æé«˜äº†è®­ç»ƒç¨³å®šæ€§ï¼Œä½†ä»é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç”±äºsigmoidå‡½æ•°çš„éçº¿æ€§ç‰¹æ€§å¯¼è‡´å›¾åƒæ¦‚ç‡ä¼°è®¡ä¸å‡†ç¡®ï¼Œä»¥åŠç¦»çº¿æ•°æ®é›†å¤šæ ·æ€§æœ‰é™çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> Diffusion-DROåŸºäºé€†å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå°†åå¥½å­¦ä¹ é‡æ–°è¡¨è¿°ä¸ºæ’åºé—®é¢˜ï¼Œæ¶ˆé™¤äº†å¯¹å¥–åŠ±æ¨¡å‹çš„ä¾èµ–ï¼Œå¹¶å°†è®­ç»ƒç›®æ ‡ç®€åŒ–ä¸ºå»å™ªå…¬å¼ã€‚è¯¥æ–¹æ³•ç‹¬ç‰¹åœ°æ•´åˆäº†ç¦»çº¿ä¸“å®¶æ¼”ç¤ºå’Œåœ¨çº¿ç­–ç•¥ç”Ÿæˆçš„è´Ÿæ ·æœ¬ï¼Œæœ‰æ•ˆæ•è·äººç±»åå¥½åŒæ—¶å…‹æœç¦»çº¿æ•°æ®çš„å±€é™æ€§ã€‚</p>
<p><strong>Result:</strong> ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDiffusion-DROåœ¨ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§å’Œæœªè§è¿‡çš„æç¤ºè¯ä¸Šå‡å®ç°äº†æ”¹è¿›çš„ç”Ÿæˆè´¨é‡ï¼Œåœ¨å®šé‡æŒ‡æ ‡å’Œç”¨æˆ·ç ”ç©¶ä¸­å‡ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å°†åå¥½å­¦ä¹ é‡æ–°è¡¨è¿°ä¸ºæ’åºé—®é¢˜å¹¶é€šè¿‡ç¦»çº¿å’Œåœ¨çº¿æ•°æ®æ•´åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ‰©æ•£æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½æä¾›äº†æ›´ç¨³å®šå’Œå‡†ç¡®çš„è®­ç»ƒæ¡†æ¶ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Direct preference optimization (DPO) methods have shown strong potential in
aligning text-to-image diffusion models with human preferences by training on
paired comparisons. These methods improve training stability by avoiding the
REINFORCE algorithm but still struggle with challenges such as accurately
estimating image probabilities due to the non-linear nature of the sigmoid
function and the limited diversity of offline datasets. In this paper, we
introduce Diffusion Denoising Ranking Optimization (Diffusion-DRO), a new
preference learning framework grounded in inverse reinforcement learning.
Diffusion-DRO removes the dependency on a reward model by casting preference
learning as a ranking problem, thereby simplifying the training objective into
a denoising formulation and overcoming the non-linear estimation issues found
in prior methods. Moreover, Diffusion-DRO uniquely integrates offline expert
demonstrations with online policy-generated negative samples, enabling it to
effectively capture human preferences while addressing the limitations of
offline data. Comprehensive experiments show that Diffusion-DRO delivers
improved generation quality across a range of challenging and unseen prompts,
outperforming state-of-the-art baselines in both both quantitative metrics and
user studies. Our source code and pre-trained models are available at
https://github.com/basiclab/DiffusionDRO.</p>
<h3 id="27-cross-modal-scene-semantic-alignment-for-image-complexity-assessment">[27] <a href="https://arxiv.org/abs/2510.18377">Cross-Modal Scene Semantic Alignment for Image Complexity Assessment</a></h3>
<p><em>Yuqing Luo, Yixiao Li, Jiang Liu, Jun Fu, Hadi Amirpour, Guanghui Yue, Baoquan Zhao, Padraig Corcoran, Hantao Liu, Wei Zhou</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å›¾åƒå¤æ‚åº¦è¯„ä¼°æ–¹æ³•CM-SSAï¼Œé€šè¿‡è·¨æ¨¡æ€åœºæ™¯è¯­ä¹‰å¯¹é½æ¥å¢å¼ºå¤æ‚åº¦é¢„æµ‹æ€§èƒ½ï¼Œä½¿é¢„æµ‹ç»“æœæ›´ç¬¦åˆäººç±»ä¸»è§‚æ„ŸçŸ¥ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å›¾åƒå¤æ‚åº¦è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–æ‰‹å·¥ç‰¹å¾æˆ–æµ…å±‚å·ç§¯ç¥ç»ç½‘ç»œç‰¹å¾ï¼Œè¿™äº›å•æ¨¡æ€è§†è§‰ç‰¹å¾ä¸è¶³ä»¥å……åˆ†æ•æ‰ä¸å›¾åƒå¤æ‚åº¦å¯†åˆ‡ç›¸å…³çš„æ„ŸçŸ¥è¡¨å¾ã€‚è·¨æ¨¡æ€åœºæ™¯è¯­ä¹‰ä¿¡æ¯åœ¨æ¶‰åŠæ„ŸçŸ¥ç†è§£çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å·²è¢«è¯æ˜å…·æœ‰é‡è¦ä½œç”¨ï¼Œä½†åœ¨å›¾åƒå¤æ‚åº¦è¯„ä¼°é¢†åŸŸçš„æ¢ç´¢å°šæœªå¾—åˆ°è§£å†³ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„CM-SSAæ–¹æ³•åŒ…å«å¤æ‚åº¦å›å½’åˆ†æ”¯å’Œåœºæ™¯è¯­ä¹‰å¯¹é½åˆ†æ”¯ã€‚å¤æ‚åº¦å›å½’åˆ†æ”¯åœ¨åœºæ™¯è¯­ä¹‰å¯¹é½åˆ†æ”¯çš„æŒ‡å¯¼ä¸‹ä¼°è®¡å›¾åƒå¤æ‚åº¦æ°´å¹³ï¼Œè€Œåœºæ™¯è¯­ä¹‰å¯¹é½åˆ†æ”¯é€šè¿‡æˆå¯¹å­¦ä¹ å°†å›¾åƒä¸ä¼ è¾¾ä¸°å¯Œåœºæ™¯è¯­ä¹‰ä¿¡æ¯çš„å¯¹åº”æ–‡æœ¬æç¤ºè¿›è¡Œå¯¹é½ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªå›¾åƒå¤æ‚åº¦è¯„ä¼°æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„CM-SSAæ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†è·¨æ¨¡æ€åœºæ™¯è¯­ä¹‰ä¿¡æ¯å¯¹å›¾åƒå¤æ‚åº¦è¯„ä¼°çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†è·¨æ¨¡æ€åœºæ™¯è¯­ä¹‰å¯¹é½èƒ½å¤Ÿæœ‰æ•ˆæå‡å›¾åƒå¤æ‚åº¦è¯„ä¼°æ€§èƒ½ï¼Œä½¿é¢„æµ‹ç»“æœæ›´ç¬¦åˆäººç±»ä¸»è§‚æ„ŸçŸ¥ï¼Œä¸ºå›¾åƒå¤æ‚åº¦è¯„ä¼°é¢†åŸŸæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘å’ŒæŠ€æœ¯æ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Image complexity assessment (ICA) is a challenging task in perceptual
evaluation due to the subjective nature of human perception and the inherent
semantic diversity in real-world images. Existing ICA methods predominantly
rely on hand-crafted or shallow convolutional neural network-based features of
a single visual modality, which are insufficient to fully capture the perceived
representations closely related to image complexity. Recently, cross-modal
scene semantic information has been shown to play a crucial role in various
computer vision tasks, particularly those involving perceptual understanding.
However, the exploration of cross-modal scene semantic information in the
context of ICA remains unaddressed. Therefore, in this paper, we propose a
novel ICA method called Cross-Modal Scene Semantic Alignment (CM-SSA), which
leverages scene semantic alignment from a cross-modal perspective to enhance
ICA performance, enabling complexity predictions to be more consistent with
subjective human perception. Specifically, the proposed CM-SSA consists of a
complexity regression branch and a scene semantic alignment branch. The
complexity regression branch estimates image complexity levels under the
guidance of the scene semantic alignment branch, while the scene semantic
alignment branch is used to align images with corresponding text prompts that
convey rich scene semantic information by pair-wise learning. Extensive
experiments on several ICA datasets demonstrate that the proposed CM-SSA
significantly outperforms state-of-the-art approaches. Codes are available at
https://github.com/XQ2K/First-Cross-Model-ICA.</p>
<h3 id="28-think-with-3d-geometric-imagination-grounded-spatial-reasoning-from-limited-views">[28] <a href="https://arxiv.org/abs/2510.18632">Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</a></h3>
<p><em>Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, Ruqi Huang</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†3DThinkeræ¡†æ¶ï¼Œé¦–æ¬¡å®ç°äº†åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ— éœ€3Då…ˆéªŒè¾“å…¥å³å¯è¿›è¡Œ3Då¿ƒç†å»ºæ¨¡ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•æœ‰æ•ˆåˆ©ç”¨å›¾åƒä¸­çš„å‡ ä½•ä¿¡æ¯è¿›è¡Œ3Dç©ºé—´å…³ç³»æ¨ç†ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ä»æœ‰é™è§†è§’ç†è§£3Dç©ºé—´å…³ç³»æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–çº¯æ–‡æœ¬æˆ–2Dè§†è§‰çº¿ç´¢ï¼Œå…¶æœ‰é™çš„è¡¨ç¤ºèƒ½åŠ›é˜»ç¢äº†éœ€è¦3Dç©ºé—´æƒ³è±¡åŠ›çš„ç‰¹å®šä»»åŠ¡æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼šé¦–å…ˆé€šè¿‡ç›‘ç£è®­ç»ƒå°†VLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆçš„3Dæ½œåœ¨è¡¨ç¤ºä¸3DåŸºç¡€æ¨¡å‹å¯¹é½ï¼Œç„¶åä»…åŸºäºç»“æœä¿¡å·ä¼˜åŒ–æ•´ä¸ªæ¨ç†è½¨è¿¹ï¼Œä»è€Œç»†åŒ–åº•å±‚çš„3Då¿ƒç†å»ºæ¨¡è¿‡ç¨‹ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œ3DThinkerå§‹ç»ˆä¼˜äºå¼ºåŸºçº¿æ–¹æ³•ï¼Œä¸ºå°†3Dè¡¨ç¤ºç»Ÿä¸€åˆ°å¤šæ¨¡æ€æ¨ç†ä¸­æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†æ— éœ€æ˜¾å¼3Dæ ‡æ³¨æ•°æ®å³å¯å®ç°3Då¿ƒç†å»ºæ¨¡çš„å¯è¡Œæ€§ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†ä¸­æ•´åˆ3Dç©ºé—´ç†è§£å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºå’Œåº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>Though recent advances in vision-language models (VLMs) have achieved
remarkable progress across a wide range of multimodal tasks, understanding 3D
spatial relationships from limited views remains a significant challenge.
Previous reasoning methods typically rely on pure text (e.g., topological
cognitive maps) or on 2D visual cues. However, their limited representational
capacity hinders performance in specific tasks that require 3D spatial
imagination. To address this limitation, we propose 3DThinker, a framework that
can effectively exploits the rich geometric information embedded within images
while reasoning, like humans do. Our framework is the first to enable 3D
mentaling during reasoning without any 3D prior input, and it does not rely on
explicitly labeled 3D data for training. Specifically, our training consists of
two stages. First, we perform supervised training to align the 3D latent
generated by VLM while reasoning with that of a 3D foundation model (e.g.,
VGGT). Then, we optimize the entire reasoning trajectory solely based on
outcome signals, thereby refining the underlying 3D mentaling. Extensive
experiments across multiple benchmarks show that 3DThinker consistently
outperforms strong baselines and offers a new perspective toward unifying 3D
representations into multimodal reasoning. Our code will be available at
https://github.com/zhangquanchen/3DThinker.</p>
<h3 id="29-seg-sparsely-supervised-semantic-segmentation-of-microscopy-data">[29] <a href="https://arxiv.org/abs/2510.18637">Îµ-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data</a></h3>
<p><em>Sheida Rahnamai Kordasiabi, Damian Dalle Nogare, Florian Jug</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºÎµ-Segæ–¹æ³•ï¼Œä¸€ç§åŸºäºåˆ†å±‚å˜åˆ†è‡ªç¼–ç å™¨çš„ç¨€ç–æ ‡æ³¨è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨0.05%å›¾åƒæ•°æ®è¿›è¡Œæ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤æ‚ç”Ÿç‰©ç”µå­æ˜¾å¾®é•œå›¾åƒä¸Šå®ç°å…·æœ‰ç«äº‰åŠ›çš„åˆ†å‰²æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç”µå­æ˜¾å¾®é•œç”Ÿç‰©å›¾åƒè¯­ä¹‰åˆ†å‰²åœ¨ç”Ÿå‘½ç§‘å­¦ä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›æ•°æ®æ•è·äº†ç”Ÿç‰©ç»“æ„çš„å¤æ‚ç»†èŠ‚ï¼Œæœ‰æ—¶ç”šè‡³å¯¹äººç±»è§‚å¯Ÿè€…æ¥è¯´ä¹Ÿéš¾ä»¥å¤„ç†ï¼Œç‰¹åˆ«æ˜¯å½“è®­ç»ƒæ ‡ç­¾æå…¶ç¨€ç–æ—¶ã€‚</p>
<p><strong>Method:</strong> Îµ-Segé‡‡ç”¨åˆ†å±‚å˜åˆ†è‡ªç¼–ç å™¨æ¶æ„ï¼Œç»“åˆä¸­å¿ƒåŒºåŸŸæ©ç ã€ç¨€ç–æ ‡ç­¾å¯¹æ¯”å­¦ä¹ ã€é«˜æ–¯æ··åˆæ¨¡å‹å…ˆéªŒå’Œæ— èšç±»æ ‡ç­¾é¢„æµ‹ï¼Œé€šè¿‡ä¸­å¿ƒåŒºåŸŸæ©ç å’Œä¿®å¤æŸå¤±é¼“åŠ±æ¨¡å‹å­¦ä¹ åŒºåˆ†æ‰€éœ€ç±»åˆ«çš„é²æ£’åµŒå…¥è¡¨ç¤ºã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸¤ä¸ªå¯†é›†ç”µå­æ˜¾å¾®é•œç”Ÿç‰©ç»„ç»‡æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒÎµ-Segèƒ½å¤Ÿåœ¨è§å…‰æ˜¾å¾®é•œæ•°æ®å’Œå¤æ‚ç”Ÿç‰©å›¾åƒæ•°æ®ä¸Šå®ç°å…·æœ‰ç«äº‰åŠ›çš„ç¨€ç–ç›‘ç£åˆ†å‰²ç»“æœï¼Œå³ä½¿åœ¨è®­ç»ƒæ ‡ç­¾æ•°é‡æå…¶æœ‰é™çš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•è¯æ˜äº†åœ¨æç¨€ç–æ ‡æ³¨æ¡ä»¶ä¸‹å®ç°å¤æ‚ç”Ÿç‰©å›¾åƒè¯­ä¹‰åˆ†å‰²çš„å¯è¡Œæ€§ï¼Œä¸ºç”Ÿå‘½ç§‘å­¦ä¸­çš„å¤§è§„æ¨¡å›¾åƒåˆ†ææä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„åº”ç”¨åœºæ™¯ä¸­ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Semantic segmentation of electron microscopy (EM) images of biological
samples remains a challenge in the life sciences. EM data captures details of
biological structures, sometimes with such complexity that even human observers
can find it overwhelming. We introduce {\epsilon}-Seg, a method based on
hierarchical variational autoencoders (HVAEs), employing center-region masking,
sparse label contrastive learning (CL), a Gaussian mixture model (GMM) prior,
and clustering-free label prediction. Center-region masking and the inpainting
loss encourage the model to learn robust and representative embeddings to
distinguish the desired classes, even if training labels are sparse (0.05% of
the total image data or less). For optimal performance, we employ CL and a GMM
prior to shape the latent space of the HVAE such that encoded input patches
tend to cluster wrt. the semantic classes we wish to distinguish. Finally,
instead of clustering latent embeddings for semantic segmentation, we propose a
MLP semantic segmentation head to directly predict class labels from latent
embeddings. We show empirical results of {\epsilon}-Seg and baseline methods on
2 dense EM datasets of biological tissues and demonstrate the applicability of
our method also on fluorescence microscopy data. Our results show that
{\epsilon}-Seg is capable of achieving competitive sparsely-supervised
segmentation results on complex biological image data, even if only limited
amounts of training labels are available.</p>
<h3 id="30-dp2o-sr-direct-perceptual-preference-optimization-for-real-world-image-super-resolution">[30] <a href="https://arxiv.org/abs/2510.18851">DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution</a></h3>
<p><em>Rongyuan Wu, Lingchen Sun, Zhengqiang Zhang, Shihao Wang, Tianhe Wu, Qiaosi Yi, Shuai Li, Lei Zhang</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDPÂ²O-SRæ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥æ„ŸçŸ¥åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨T2Iæ‰©æ•£æ¨¡å‹çš„éšæœºæ€§ç”Ÿæˆå¤šæ ·æ€§è¾“å‡ºï¼Œç»“åˆå…¨å‚è€ƒå’Œæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æ„å»ºæ··åˆå¥–åŠ±ä¿¡å·ï¼Œåœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹æå‡çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡çš„æ„ŸçŸ¥è´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•è™½ç„¶èƒ½å¤Ÿåˆæˆä¸°å¯Œç»†èŠ‚ï¼Œä½†ç”±äºT2Iæ¨¡å‹çš„éšæœºæ€§ï¼Œä¸åŒå™ªå£°è¾“å…¥ä¼šå¯¼è‡´æ„ŸçŸ¥è´¨é‡å·®å¼‚è¾ƒå¤§çš„è¾“å‡ºï¼Œè¿™ç§éšæœºæ€§æ—¢æ˜¯é™åˆ¶å› ç´ ä¹Ÿæä¾›äº†æ›´å®½çš„æ„ŸçŸ¥è´¨é‡èŒƒå›´ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿåˆ©ç”¨è¿™ç§å¤šæ ·æ€§æå‡æ€§èƒ½çš„æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æå‡ºç›´æ¥æ„ŸçŸ¥åå¥½ä¼˜åŒ–æ¡†æ¶DPÂ²O-SRï¼Œç»“åˆåœ¨å¤§å‹äººç±»åå¥½æ•°æ®é›†ä¸Šè®­ç»ƒçš„å…¨å‚è€ƒå’Œæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æ¨¡å‹æ„å»ºæ··åˆå¥–åŠ±ä¿¡å·ï¼Œç¡®ä¿ç»“æ„ä¿çœŸåº¦å’Œè‡ªç„¶å¤–è§‚ï¼›è¶…è¶Šæ ‡å‡†æœ€ä½³-æœ€å·®é€‰æ‹©ï¼Œä»åŒä¸€æ¨¡å‹è¾“å‡ºæ„å»ºå¤šä¸ªåå¥½å¯¹ï¼›æå‡ºåˆ†å±‚åå¥½ä¼˜åŒ–ï¼ŒåŸºäºç»„å†…å¥–åŠ±å·®è·å’Œç»„é—´å¤šæ ·æ€§è‡ªé€‚åº”åŠ æƒè®­ç»ƒå¯¹ã€‚</p>
<p><strong>Result:</strong> åœ¨æ‰©æ•£å’Œæµå¼T2Iéª¨å¹²ç½‘ç»œä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDPÂ²O-SRæ˜¾è‘—æå‡äº†æ„ŸçŸ¥è´¨é‡ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼›åˆ†æå‘ç°æœ€ä¼˜é€‰æ‹©æ¯”ä¾‹å–å†³äºæ¨¡å‹å®¹é‡ï¼Œå°æ¨¡å‹å—ç›Šäºæ›´å¹¿è¦†ç›–ï¼Œå¤§æ¨¡å‹å¯¹ç›‘ç£ä¸­æ›´å¼ºå¯¹æ¯”å“åº”æ›´å¥½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†æ¨¡å‹å®¹é‡ä¸åå¥½é€‰æ‹©ç­–ç•¥çš„å…³ç³»ï¼Œæå‡ºçš„åˆ†å±‚ä¼˜åŒ–æ–¹æ³•å®ç°äº†æ›´é«˜æ•ˆç¨³å®šçš„å­¦ä¹ ï¼Œä¸ºæ— éœ€äººå·¥æ ‡æ³¨çš„ç”Ÿæˆæ¨¡å‹å¯¹é½æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œåœ¨çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world
image super-resolution (Real-ISR) methods can synthesize rich and realistic
details. However, due to the inherent stochasticity of T2I models, different
noise inputs often lead to outputs with varying perceptual quality. Although
this randomness is sometimes seen as a limitation, it also introduces a wider
perceptual quality range, which can be exploited to improve Real-ISR
performance. To this end, we introduce Direct Perceptual Preference
Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative
models with perceptual preferences without requiring costly human annotations.
We construct a hybrid reward signal by combining full-reference and
no-reference image quality assessment (IQA) models trained on large-scale human
preference datasets. This reward encourages both structural fidelity and
natural appearance. To better utilize perceptual diversity, we move beyond the
standard best-vs-worst selection and construct multiple preference pairs from
outputs of the same model. Our analysis reveals that the optimal selection
ratio depends on model capacity: smaller models benefit from broader coverage,
while larger models respond better to stronger contrast in supervision.
Furthermore, we propose hierarchical preference optimization, which adaptively
weights training pairs based on intra-group reward gaps and inter-group
diversity, enabling more efficient and stable learning. Extensive experiments
across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR
significantly improves perceptual quality and generalizes well to real-world
benchmarks.</p>
<h3 id="31-covmatch-cross-covariance-guided-multimodal-dataset-distillation-with-trainable-text-encoder">[31] <a href="https://arxiv.org/abs/2510.18583">CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder</a></h3>
<p><em>Yongmin Lee, Hye Won Chung</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CovMatchï¼Œä¸€ç§å¯æ‰©å±•çš„å¤šæ¨¡æ€æ•°æ®é›†è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡è·¨åæ–¹å·®å¯¹é½å’Œç‰¹å¾åˆ†å¸ƒæ­£åˆ™åŒ–å®ç°å›¾åƒç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨çš„è”åˆä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ ä¸­çš„è·¨æ¨¡æ€å¯¹é½æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€æ•°æ®é›†è’¸é¦åœ¨æ‰©å±•åˆ°å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ—¶é¢ä¸´å…³é”®æŒ‘æˆ˜ï¼šå­¦ä¹ è·¨æ¨¡æ€å¯¹é½å’Œç®¡ç†å¤§å‹ç¼–ç å™¨çš„é«˜è®¡ç®—æˆæœ¬ã€‚å…ˆå‰æ–¹æ³•é€šè¿‡å†»ç»“æ–‡æœ¬ç¼–ç å™¨ä»…æ›´æ–°å›¾åƒç¼–ç å™¨å’Œæ–‡æœ¬æŠ•å½±å±‚æ¥è§£å†³å¯æ‰©å±•æ€§é—®é¢˜ï¼Œä½†è¿™ä¸¥é‡é™åˆ¶äº†è¯­ä¹‰å¯¹é½å¹¶æˆä¸ºæ€§èƒ½æ‰©å±•çš„ç“¶é¢ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†CovMatchæ¡†æ¶ï¼Œé€šè¿‡å¯¹é½çœŸå®å’Œåˆæˆç‰¹å¾çš„è·¨åæ–¹å·®æ¥ä¼˜åŒ–è·¨æ¨¡æ€å¯¹é½ï¼ŒåŒæ—¶å¯¹æ¯ä¸ªæ¨¡æ€å†…çš„ç‰¹å¾åˆ†å¸ƒè¿›è¡Œæ­£åˆ™åŒ–ã€‚ä¸å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒCovMatchæ”¯æŒä¸¤ä¸ªç¼–ç å™¨çš„è”åˆä¼˜åŒ–ï¼Œä»è€Œå®ç°æ›´å¼ºçš„è·¨æ¨¡æ€å¯¹é½ã€‚</p>
<p><strong>Result:</strong> åœ¨Flickr30Kå’ŒCOCOæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCovMatchä¼˜äºæœ€å…ˆè¿›çš„å¤šæ¨¡æ€è’¸é¦æ–¹æ³•ï¼Œä»…ä½¿ç”¨500ä¸ªåˆæˆå¯¹å°±åœ¨æ£€ç´¢å‡†ç¡®ç‡ä¸Šå®ç°äº†é«˜è¾¾6.8%çš„ç»å¯¹å¢ç›Šã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡è·¨åæ–¹å·®å¯¹é½å’Œç‰¹å¾åˆ†å¸ƒæ­£åˆ™åŒ–å®ç°ç¼–ç å™¨è”åˆä¼˜åŒ–æ˜¯å¤šæ¨¡æ€æ•°æ®é›†è’¸é¦çš„å…³é”®çªç ´ï¼Œä¸ºé«˜æ•ˆè®­ç»ƒå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨æœ‰é™è®¡ç®—èµ„æºä¸‹å®ç°é«˜æ€§èƒ½å¤šæ¨¡æ€å­¦ä¹ çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Multimodal dataset distillation aims to synthesize a small set of image-text
pairs that enables efficient training of large-scale vision-language models.
While dataset distillation has shown promise in unimodal tasks, extending it to
multimodal contrastive learning presents key challenges: learning cross-modal
alignment and managing the high computational cost of large encoders. Prior
approaches address scalability by freezing the text encoder and update only the
image encoder and text projection layer. However, we find this severely limits
semantic alignment and becomes a bottleneck for performance scaling. We propose
CovMatch, a scalable dataset distillation framework that aligns the
cross-covariance of real and synthetic features while regularizing feature
distributions within each modality. Unlike prior approaches, CovMatch enables
joint optimization of both encoders, leading to stronger cross-modal alignment
and improved performance. Evaluated on Flickr30K and COCO, CovMatch outperforms
state-of-the-art multimodal distillation methods and achieves up to 6.8%
absolute gains in retrieval accuracy using only 500 synthetic pairs.</p>
<h3 id="32-beyond-the-pipeline-analyzing-key-factors-in-end-to-end-deep-learning-for-historical-writer-identification">[32] <a href="https://arxiv.org/abs/2510.18671">Beyond the Pipeline: Analyzing Key Factors in End-to-End Deep Learning for Historical Writer Identification</a></h3>
<p><em>Hanif Rasyidi, Moshiur Farazi</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å†å²ç¬”è¿¹è¯†åˆ«ä»»åŠ¡ä¸­çš„æ€§èƒ½å½±å“å› ç´ ï¼Œå‘ç°å¤§å¤šæ•°é…ç½®åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œä½†è¯†åˆ«å‡ºä¸€ç§ç®€åŒ–è®¾è®¡èƒ½è¾¾åˆ°ä¸é¡¶å°–ç³»ç»Ÿç›¸å½“çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å†å²ç¬”è¿¹è¯†åˆ«é¢ä¸´ç¬”è¿¹é£æ ¼å¤šæ ·æ€§ã€æ–‡æ¡£é€€åŒ–ä»¥åŠæ¯ä¸ªä½œè€…çš„æ ‡è®°æ ·æœ¬æœ‰é™ç­‰æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–æ‰‹å·¥ç‰¹å¾æå–åœ¨å°è§„æ¨¡ç²¾é€‰æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œè€Œç«¯åˆ°ç«¯æ–¹æ³•åœ¨å®é™…æ–‡æ¡£çº§åœºæ™¯ç‰¹åˆ«æ˜¯é›¶æ ·æœ¬è®¾ç½®ä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å½±å“ç«¯åˆ°ç«¯æ–¹æ³•æ€§èƒ½çš„å…³é”®å› ç´ ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ¢ç´¢äº†é¢„å¤„ç†æ–¹æ³•ã€éª¨å¹²ç½‘ç»œæ¶æ„å’Œåå¤„ç†ç­–ç•¥çš„ä¸åŒç»„åˆï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†å‰²ã€è¡¥ä¸é‡‡æ ·å’Œç‰¹å¾èšåˆæŠ€æœ¯ï¼Œç³»ç»Ÿè¯„ä¼°äº†è¿™äº›ç»„ä»¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜å¤§å¤šæ•°é…ç½®ç”±äºä½å±‚è§†è§‰ç‰¹å¾æ•è·èƒ½åŠ›å¼±ã€è¡¥ä¸è¡¨ç¤ºä¸ä¸€è‡´ä»¥åŠå¯¹å†…å®¹å™ªå£°é«˜åº¦æ•æ„Ÿè€Œè¡¨ç°ä¸ä½³ï¼Œä½†å‘ç°ä¸€ç§ç«¯åˆ°ç«¯è®¾ç½®å°½ç®¡è®¾è®¡æ›´ç®€å•ï¼Œå´èƒ½è¾¾åˆ°ä¸æ€§èƒ½æœ€ä½³ç³»ç»Ÿç›¸å½“çš„ç»“æœã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†æ„å»ºé²æ£’ç«¯åˆ°ç«¯ç³»ç»Ÿçš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºå†å²æ–‡æ¡£ç¬”è¿¹è¯†åˆ«æä¾›äº†æ”¹è¿›æ€§èƒ½çš„è®¾è®¡é€‰æ‹©è§è§£ï¼Œå¼ºè°ƒäº†ä½å±‚ç‰¹å¾è¡¨ç¤ºå’Œå™ªå£°é²æ£’æ€§åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>This paper investigates various factors that influence the performance of
end-to-end deep learning approaches for historical writer identification (HWI),
a task that remains challenging due to the diversity of handwriting styles,
document degradation, and the limited number of labelled samples per writer.
These conditions often make accurate recognition difficult, even for human
experts. Traditional HWI methods typically rely on handcrafted image processing
and clustering techniques, which tend to perform well on small and carefully
curated datasets. In contrast, end-to-end pipelines aim to automate the process
by learning features directly from document images. However, our experiments
show that many of these models struggle to generalise in more realistic,
document-level settings, especially under zero-shot scenarios where writers in
the test set are not present in the training data. We explore different
combinations of pre-processing methods, backbone architectures, and
post-processing strategies, including text segmentation, patch sampling, and
feature aggregation. The results suggest that most configurations perform
poorly due to weak capture of low-level visual features, inconsistent patch
representations, and high sensitivity to content noise. Still, we identify one
end-to-end setup that achieves results comparable to the top-performing system,
despite using a simpler design. These findings point to key challenges in
building robust end-to-end systems and offer insight into design choices that
improve performance in historical document writer identification.</p>
<h3 id="33-unigenbench-a-unified-semantic-evaluation-benchmark-for-text-to-image-generation">[33] <a href="https://arxiv.org/abs/2510.18701">UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image Generation</a></h3>
<p><em>Yibin Wang, Zhimin Li, Yuhang Zang, Jiazi Bu, Yujie Zhou, Yi Xin, Junjun He, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UniGenBench++ï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¯­ä¹‰è¯„ä¼°åŸºå‡†ï¼Œé€šè¿‡åˆ†å±‚ç»“æ„è®¾è®¡è¦†ç›–å¤šæ ·åŒ–çœŸå®åœºæ™¯å’Œå¤šè¯­è¨€æ”¯æŒï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ„å»ºå¯é çš„è¯„ä¼°æµç¨‹ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¯„ä¼°åŸºå‡†å­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™æ€§ï¼šç¼ºä¹å¤šæ ·åŒ–æç¤ºåœºæ™¯å’Œå¤šè¯­è¨€æ”¯æŒï¼Œä»…æä¾›ç²—ç²’åº¦è¯„ä¼°è€Œç¼ºä¹ç»†ç²’åº¦å­ç»´åº¦åˆ†æï¼Œä»¥åŠè¯„ä¼°ç»´åº¦è¦†ç›–èŒƒå›´æœ‰é™ï¼Œæ— æ³•æ»¡è¶³å®é™…åº”ç”¨éœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†åŒ…å«600ä¸ªæç¤ºçš„åˆ†å±‚åŸºå‡†ï¼Œæ¶µç›–5ä¸ªä¸»è¦ä¸»é¢˜å’Œ20ä¸ªå­ä¸»é¢˜çš„å¤šæ ·åŒ–åœºæ™¯ï¼Œç³»ç»Ÿè¯„ä¼°10ä¸ªä¸»è¦ç»´åº¦å’Œ27ä¸ªå­è¯„ä¼°æ ‡å‡†ï¼›åˆ©ç”¨Gemini-2.5-Proå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä¸–ç•ŒçŸ¥è¯†å’Œç»†ç²’åº¦å›¾åƒç†è§£èƒ½åŠ›å¼€å‘å¯é è¯„ä¼°æµç¨‹ï¼Œå¹¶æä¾›ä¸­è‹±æ–‡é•¿çŸ­ç‰ˆæœ¬æç¤ºä»¥æµ‹è¯•æ¨¡å‹é²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡å¯¹å¼€æºå’Œé—­æºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œç³»ç»Ÿæ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å„ç§è¯„ä¼°ç»´åº¦ä¸Šçš„ä¼˜åŠ¿ä¸ä¸è¶³ï¼Œä¸ºæ¨¡å‹æ€§èƒ½æä¾›äº†ç»†ç²’åº¦çš„é‡åŒ–åˆ†æã€‚</p>
<p><strong>Conclusion:</strong> UniGenBench++å¡«è¡¥äº†ç°æœ‰è¯„ä¼°åŸºå‡†çš„ç©ºç™½ï¼Œæä¾›äº†æ›´å…¨é¢ã€ç»†ç²’åº¦çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¯­ä¹‰ä¸€è‡´æ€§è¯„ä¼°æ¡†æ¶ï¼ŒåŒæ—¶è®­ç»ƒäº†ç¦»çº¿è¯„ä¼°æ¨¡å‹ä»¥ä¿ƒè¿›ç¤¾åŒºä½¿ç”¨ï¼Œä¸ºæ¨¡å‹å¼€å‘å’Œä¼˜åŒ–æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Recent progress in text-to-image (T2I) generation underscores the importance
of reliable benchmarks in evaluating how accurately generated images reflect
the semantics of their textual prompt. However, (1) existing benchmarks lack
the diversity of prompt scenarios and multilingual support, both essential for
real-world applicability; (2) they offer only coarse evaluations across primary
dimensions, covering a narrow range of sub-dimensions, and fall short in
fine-grained sub-dimension assessment. To address these limitations, we
introduce UniGenBench++, a unified semantic assessment benchmark for T2I
generation. Specifically, it comprises 600 prompts organized hierarchically to
ensure both coverage and efficiency: (1) spans across diverse real-world
scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively
probes T2I models' semantic consistency over 10 primary and 27 sub evaluation
criteria, with each prompt assessing multiple testpoints. To rigorously assess
model robustness to variations in language and prompt length, we provide both
English and Chinese versions of each prompt in short and long forms. Leveraging
the general world knowledge and fine-grained image understanding capabilities
of a closed-source Multi-modal Large Language Model (MLLM), i.e.,
Gemini-2.5-Pro, an effective pipeline is developed for reliable benchmark
construction and streamlined model assessment. Moreover, to further facilitate
community use, we train a robust evaluation model that enables offline
assessment of T2I model outputs. Through comprehensive benchmarking of both
open- and closed-sourced T2I models, we systematically reveal their strengths
and weaknesses across various aspects.</p>
<h3 id="34-exploring-a-unified-vision-centric-contrastive-alternatives-on-multi-modal-web-documents">[34] <a href="https://arxiv.org/abs/2510.18703">Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents</a></h3>
<p><em>Yiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shou</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºVC2Lï¼ˆVision-Centric Contrastive Learningï¼‰ï¼Œä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å°†æ–‡æœ¬ã€å›¾åƒåŠå…¶ç»„åˆå…¨éƒ¨æ¸²æŸ“ä¸ºå›¾åƒï¼Œåœ¨åƒç´ ç©ºé—´ä¸­è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œæ— éœ€OCRæˆ–æ¨¡æ€èåˆç­–ç•¥ï¼Œåœ¨å¤æ‚ç½‘é¡µæ–‡æ¡£ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†ç«äº‰æ€§æˆ–ä¼˜äºCLIPæ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„å¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨å¤„ç†å¤æ‚çœŸå®ç½‘é¡µæ–‡æ¡£æ—¶å­˜åœ¨å±€é™ï¼Œç‰¹åˆ«æ˜¯å½“æ–‡æœ¬å’Œå›¾åƒäº¤é”™æ’åˆ—ã€æ¾æ•£å¯¹é½æˆ–ä»¥è§†è§‰å½¢å¼åµŒå…¥æ—¶ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†è¿™äº›å¤šæ¨¡æ€äº¤äº’åœºæ™¯ã€‚</p>
<p><strong>Method:</strong> VC2Lé‡‡ç”¨å•ä¸€è§†è§‰å˜æ¢å™¨åœ¨åƒç´ ç©ºé—´ç»Ÿä¸€å»ºæ¨¡æ–‡æœ¬ã€å›¾åƒåŠå…¶ç»„åˆï¼Œé€šè¿‡å°†å„ç±»è¾“å…¥æ¸²æŸ“ä¸ºå›¾åƒæ¶ˆé™¤å¯¹OCRã€æ–‡æœ¬åˆ†è¯æˆ–æ¨¡æ€èåˆç­–ç•¥çš„ä¾èµ–ï¼›è¯¥æ–¹æ³•é‡‡ç”¨ç‰‡æ®µçº§å¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼Œå¯¹é½è¿ç»­çš„å¤šæ¨¡æ€ç‰‡æ®µï¼Œåˆ©ç”¨æ–‡æ¡£å†…åœ¨è¿è´¯æ€§è€Œæ— éœ€æ˜¾å¼é…å¯¹çš„å›¾æ–‡æ•°æ®ã€‚</p>
<p><strong>Result:</strong> åœ¨æå‡ºçš„AnyCIRã€SeqCIRå’ŒCSRä¸‰ä¸ªæ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVC2Låœ¨è·¨æ¨¡æ€æ£€ç´¢ã€ç»†ç²’åº¦åºåˆ—ç†è§£å’Œæœªè§æ•°æ®æ³›åŒ–æ–¹é¢å‡è¡¨ç°å‡ºè‰²ï¼›åœ¨M-BEIRå’ŒMTEBç­‰å·²å»ºç«‹æ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†ç«äº‰æ€§æˆ–ä¼˜äºCLIPé£æ ¼æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€ç½‘é¡µæ•°æ®ä½œä¸ºå¯¹æ¯”å­¦ä¹ è®­ç»ƒèµ„æºçš„å·¨å¤§æ½œåŠ›ï¼Œå±•ç¤ºäº†ç»Ÿä¸€è§†è§‰ä¸­å¿ƒæ–¹æ³•åœ¨å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä¸­çš„å¯æ‰©å±•æ€§ï¼Œä¸ºå¤„ç†å¤æ‚çœŸå®ä¸–ç•Œå¤šæ¨¡æ€åœºæ™¯æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>Contrastive vision-language models such as CLIP have demonstrated strong
performance across a wide range of multimodal tasks by learning from aligned
image-text pairs. However, their ability to handle complex, real-world web
documents remains limited, particularly in scenarios where text and images are
interleaved, loosely aligned, or embedded in visual form. To address these
challenges, we propose Vision-Centric Contrastive Learning (VC2L), a unified
framework that models text, images, and their combinations using a single
vision transformer. VC2L operates entirely in pixel space by rendering all
inputs, whether textual, visual, or combined, as images, thus eliminating the
need for OCR, text tokenization, or modality fusion strategy. To capture
complex cross-modal relationships in multimodal web documents, VC2L employs a
snippet-level contrastive learning objective that aligns consecutive multimodal
segments, leveraging the inherent coherence of documents without requiring
explicitly paired image-text data. To assess the effectiveness of this
approach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR,
designed to evaluate cross-modal retrieval, fine-grained sequential
understanding, and generalization to unseen data, respectively. Empirical
results show that VC2L achieves competitive or superior performance compared to
CLIP-style models on both the proposed benchmarks and established datasets such
as M-BEIR and MTEB. These findings underscore the potential of multimodal web
data as a valuable training resource for contrastive learning and illustrate
the scalability of a unified, vision-centric approach for multimodal
representation learning. Code and models are available at:
https://github.com/showlab/VC2L.</p>
<h3 id="35-plana3r-zero-shot-metric-planar-3d-reconstruction-via-feed-forward-planar-splatting">[35] <a href="https://arxiv.org/abs/2510.18714">PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-Forward Planar Splatting</a></h3>
<p><em>Changkun Liu, Bin Tan, Zeran Ke, Shangzhan Zhang, Jiachen Liu, Ming Qian, Nan Xue, Yujun Shen, Tristan Braud</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†PLANA3Rï¼Œä¸€ç§æ— éœ€ç›¸æœºä½å§¿çš„åº¦é‡ä¸‰ç»´å®¤å†…åœºæ™¯é‡å»ºæ¡†æ¶ï¼Œé€šè¿‡å¹³é¢åŸºå…ƒè¡¨ç¤ºå’Œå¹³é¢æ¸²æŸ“æŠ€æœ¯ï¼Œåœ¨æ²¡æœ‰æ˜¾å¼å¹³é¢ç›‘ç£çš„æƒ…å†µä¸‹å­¦ä¹ å¹³é¢ä¸‰ç»´ç»“æ„ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å‰é¦ˆæ–¹æ³•éœ€è¦ä¸‰ç»´å¹³é¢æ ‡æ³¨è¿›è¡Œè®­ç»ƒï¼Œé™åˆ¶äº†åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å¯æ‰©å±•æ€§ï¼Œä¸”å®¤å†…åœºæ™¯å›ºæœ‰çš„å‡ ä½•è§„å¾‹æ€§æœªè¢«å……åˆ†åˆ©ç”¨è¿›è¡Œç´§å‡‘è¡¨ç¤ºã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨Vision Transformersæå–ç¨€ç–å¹³é¢åŸºå…ƒå¹¶ä¼°è®¡ç›¸å¯¹ç›¸æœºä½å§¿ï¼Œé€šè¿‡å¹³é¢æ¸²æŸ“æŠ€æœ¯ç›‘ç£å‡ ä½•å­¦ä¹ ï¼Œå…¶ä¸­æ¢¯åº¦é€šè¿‡é«˜åˆ†è¾¨ç‡æ¸²æŸ“çš„æ·±åº¦å’Œæ³•çº¿å›¾ä¼ æ’­ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªå®¤å†…åœºæ™¯æ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†åœ¨è·¨åŸŸå®¤å†…ç¯å¢ƒä¸­çš„å¼ºæ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä¸‰ç»´è¡¨é¢é‡å»ºã€æ·±åº¦ä¼°è®¡å’Œç›¸å¯¹ä½å§¿ä¼°è®¡ç­‰ä»»åŠ¡ã€‚</p>
<p><strong>Conclusion:</strong> åŸºäºå¹³é¢ä¸‰ç»´è¡¨ç¤ºçš„æ–¹æ³•ä¸ä»…å®ç°äº†å‡†ç¡®çš„åº¦é‡é‡å»ºï¼Œè¿˜è‡ªç„¶å…·å¤‡äº†ç²¾ç¡®å¹³é¢åˆ†å‰²çš„èƒ½åŠ›ï¼Œä¸ºæ— ç›‘ç£ä¸‰ç»´é‡å»ºæä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>This paper addresses metric 3D reconstruction of indoor scenes by exploiting
their inherent geometric regularities with compact representations. Using
planar 3D primitives - a well-suited representation for man-made environments -
we introduce PLANA3R, a pose-free framework for metric Planar 3D Reconstruction
from unposed two-view images. Our approach employs Vision Transformers to
extract a set of sparse planar primitives, estimate relative camera poses, and
supervise geometry learning via planar splatting, where gradients are
propagated through high-resolution rendered depth and normal maps of
primitives. Unlike prior feedforward methods that require 3D plane annotations
during training, PLANA3R learns planar 3D structures without explicit plane
supervision, enabling scalable training on large-scale stereo datasets using
only depth and normal annotations. We validate PLANA3R on multiple indoor-scene
datasets with metric supervision and demonstrate strong generalization to
out-of-domain indoor environments across diverse tasks under metric evaluation
protocols, including 3D surface reconstruction, depth estimation, and relative
pose estimation. Furthermore, by formulating with planar 3D representation, our
method emerges with the ability for accurate plane segmentation. The project
page is available at https://lck666666.github.io/plana3r</p>
<h3 id="36-if-vidcap-can-video-caption-models-follow-instructions">[36] <a href="https://arxiv.org/abs/2510.18726">IF-VidCap: Can Video Caption Models Follow Instructions?</a></h3>
<p><em>Shihao Li, Yuanxing Zhang, Jiangtao Wu, Zhide Lei, Yiwen He, Runzhe Wen, Chenxi Liao, Chengkang Jiang, An Ping, Shuo Gao, Suhan Wang, Zhaozhou Bian, Zijun Zhou, Jingyi Xie, Jiayi Zhou, Jing Wang, Yifan Yao, Weihao Xie, Yingshui Tan, Yanghai Wang, Qianqian Xie, Zhaoxiang Zhang, Jiaheng Liu</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†IF-VidCapåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¯æ§è§†é¢‘å­—å¹•ç”Ÿæˆçš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨æè¿°å…¨é¢æ€§è€Œå¿½è§†æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„ç©ºç™½ã€‚é€šè¿‡å¯¹20å¤šä¸ªä¸»æµæ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œå°½ç®¡ä¸“æœ‰æ¨¡å‹ä»å ä¸»å¯¼åœ°ä½ï¼Œä½†é¡¶çº§å¼€æºè§£å†³æ–¹æ¡ˆå·²æ¥è¿‘åŒç­‰æ°´å¹³ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘å­—å¹•ç”Ÿæˆä¸­è™½ç„¶è¡¨ç°å‡ºè‰²ï¼Œä½†å®é™…åº”ç”¨éœ€è¦èƒ½å¤Ÿéµå¾ªç‰¹å®šç”¨æˆ·æŒ‡ä»¤çš„å­—å¹•ï¼Œè€Œéç”Ÿæˆè¯¦å°½æ— çº¦æŸçš„æè¿°ã€‚ç°æœ‰åŸºå‡†ä¸»è¦è¯„ä¼°æè¿°å…¨é¢æ€§ï¼Œè€Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œè¿™ä¸€ç ”ç©¶ç©ºç™½éœ€è¦å¡«è¡¥ã€‚</p>
<p><strong>Method:</strong> ä½œè€…å¼•å…¥äº†IF-VidCapåŸºå‡†ï¼ŒåŒ…å«1,400ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œé‡‡ç”¨ç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ä»æ ¼å¼æ­£ç¡®æ€§å’Œå†…å®¹æ­£ç¡®æ€§ä¸¤ä¸ªç»´åº¦è¯„ä¼°å­—å¹•è´¨é‡ã€‚è¯¥åŸºå‡†åŒºåˆ«äºç°æœ‰çš„è§†é¢‘å­—å¹•æˆ–é€šç”¨æŒ‡ä»¤è·ŸéšåŸºå‡†ï¼Œä¸“é—¨é’ˆå¯¹å¯æ§è§†é¢‘å­—å¹•ç”Ÿæˆä»»åŠ¡è®¾è®¡ã€‚</p>
<p><strong>Result:</strong> å¯¹è¶…è¿‡20ä¸ªä¸»æµæ¨¡å‹çš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œä¸“æœ‰æ¨¡å‹ä»å ä¸»å¯¼åœ°ä½ï¼Œä½†æ€§èƒ½å·®è·æ­£åœ¨ç¼©å°ï¼Œé¡¶çº§å¼€æºè§£å†³æ–¹æ¡ˆå·²å®ç°æ¥è¿‘åŒç­‰æ°´å¹³ã€‚æ­¤å¤–ï¼Œä¸“é—¨ç”¨äºå¯†é›†å­—å¹•ç”Ÿæˆçš„æ¨¡å‹åœ¨å¤æ‚æŒ‡ä»¤ä¸Šè¡¨ç°ä¸å¦‚é€šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜æœªæ¥å·¥ä½œåº”åŒæ—¶æ¨è¿›æè¿°ä¸°å¯Œæ€§å’ŒæŒ‡ä»¤è·Ÿéšä¿çœŸåº¦çš„å‘å±•ã€‚å¯†é›†å­—å¹•ä¸“ç”¨æ¨¡å‹çš„ä¸è¶³è¡¨æ˜ï¼Œé€šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œè¿™ä¸ºæ¨¡å‹è®¾è®¡æä¾›äº†é‡è¦å¯ç¤ºã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>Although Multimodal Large Language Models (MLLMs) have demonstrated
proficiency in video captioning, practical applications require captions that
follow specific user instructions rather than generating exhaustive,
unconstrained descriptions. Current benchmarks, however, primarily assess
descriptive comprehensiveness while largely overlooking instruction-following
capabilities. To address this gap, we introduce IF-VidCap, a new benchmark for
evaluating controllable video captioning, which contains 1,400 high-quality
samples. Distinct from existing video captioning or general
instruction-following benchmarks, IF-VidCap incorporates a systematic framework
that assesses captions on two dimensions: format correctness and content
correctness. Our comprehensive evaluation of over 20 prominent models reveals a
nuanced landscape: despite the continued dominance of proprietary models, the
performance gap is closing, with top-tier open-source solutions now achieving
near-parity. Furthermore, we find that models specialized for dense captioning
underperform general-purpose MLLMs on complex instructions, indicating that
future work should simultaneously advance both descriptive richness and
instruction-following fidelity.</p>
<h3 id="37-seal-semantic-aware-hierarchical-learning-for-generalized-category-discovery">[37] <a href="https://arxiv.org/abs/2510.18740">SEAL: Semantic-Aware Hierarchical Learning for Generalized Category Discovery</a></h3>
<p><em>Zhenqi He, Yuanpei Liu, Kai Han</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SEALæ¡†æ¶ï¼Œé€šè¿‡è‡ªç„¶å±‚æ¬¡ç»“æ„æŒ‡å¯¼çš„è¯­ä¹‰æ„ŸçŸ¥åˆ†å±‚å­¦ä¹ æ¥è§£å†³å¹¿ä¹‰ç±»åˆ«å‘ç°é—®é¢˜ï¼Œåœ¨å¤šä¸ªç»†ç²’åº¦åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¹¿ä¹‰ç±»åˆ«å‘ç°æ–¹æ³•é€šå¸¸ä¾èµ–äºå•å±‚è¯­ä¹‰æˆ–æ‰‹åŠ¨è®¾è®¡çš„æŠ½è±¡å±‚æ¬¡ç»“æ„ï¼Œè¿™é™åˆ¶äº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†å·²çŸ¥å’ŒæœªçŸ¥ç±»åˆ«çš„å›¾åƒåˆ†ç±»é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†SEALæ¡†æ¶ï¼ŒåŒ…å«åˆ†å±‚è¯­ä¹‰å¼•å¯¼çš„è½¯å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨å±‚æ¬¡ç›¸ä¼¼æ€§ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„è½¯è´Ÿæ ·æœ¬ï¼Œä»¥åŠè·¨ç²’åº¦ä¸€è‡´æ€§æ¨¡å—æ¥å¯¹é½ä¸åŒç²’åº¦çº§åˆ«çš„é¢„æµ‹ç»“æœã€‚</p>
<p><strong>Result:</strong> SEALåœ¨SSBåŸºå‡†ã€Oxford-Petå’ŒHerbarium19ç­‰ç»†ç²’åº¦æ•°æ®é›†ä¸ŠæŒç»­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç²—ç²’åº¦æ•°æ®é›†ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜åˆ©ç”¨è‡ªç„¶å±‚æ¬¡ç»“æ„å¯ä»¥æœ‰æ•ˆæå‡å¹¿ä¹‰ç±»åˆ«å‘ç°çš„æ€§èƒ½ï¼Œåˆ†å±‚è¯­ä¹‰å¼•å¯¼å’Œè·¨ç²’åº¦ä¸€è‡´æ€§æ˜¯è§£å†³æ­¤ç±»é—®é¢˜çš„å…³é”®æœºåˆ¶ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œè§†è§‰è¯†åˆ«æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>This paper investigates the problem of Generalized Category Discovery (GCD).
Given a partially labelled dataset, GCD aims to categorize all unlabelled
images, regardless of whether they belong to known or unknown classes. Existing
approaches typically depend on either single-level semantics or manually
designed abstract hierarchies, which limit their generalizability and
scalability. To address these limitations, we introduce a SEmantic-aware
hierArchical Learning framework (SEAL), guided by naturally occurring and
easily accessible hierarchical structures. Within SEAL, we propose a
Hierarchical Semantic-Guided Soft Contrastive Learning approach that exploits
hierarchical similarity to generate informative soft negatives, addressing the
limitations of conventional contrastive losses that treat all negatives
equally. Furthermore, a Cross-Granularity Consistency (CGC) module is designed
to align the predictions from different levels of granularity. SEAL
consistently achieves state-of-the-art performance on fine-grained benchmarks,
including the SSB benchmark, Oxford-Pet, and the Herbarium19 dataset, and
further demonstrates generalization on coarse-grained datasets. Project page:
https://visual-ai.github.io/seal/</p>
<h3 id="38-proclip-progressive-vision-language-alignment-via-llm-based-embedder">[38] <a href="https://arxiv.org/abs/2510.18795">ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</a></h3>
<p><em>Xiaoxing Hu, Kaicheng Yang, Ziyong Feng, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>ProCLIPæå‡ºäº†ä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„æ¸è¿›å¼è§†è§‰-è¯­è¨€å¯¹é½æ¡†æ¶ï¼Œé€šè¿‡å°†CLIPå›¾åƒç¼–ç å™¨ä¸åŸºäºLLMçš„åµŒå…¥å™¨è¿›è¡Œæœ‰æ•ˆå¯¹é½ï¼Œè§£å†³äº†CLIPæ–‡æœ¬ç¼–ç å™¨åœ¨å¤„ç†é•¿æ–‡æœ¬å’Œå¤šè¯­è¨€è¾“å…¥æ–¹é¢çš„å±€é™æ€§ï¼ŒåŒæ—¶é¿å…äº†ç ´åCLIPé¢„è®­ç»ƒçŸ¥è¯†çš„è§†è§‰-è¯­è¨€å¯¹é½ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŸå§‹CLIPæ–‡æœ¬ç¼–ç å™¨å­˜åœ¨77ä¸ªtokençš„æœ€å¤§è¾“å…¥é•¿åº¦é™åˆ¶ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†é•¿æ–‡æœ¬å’Œè¿›è¡Œç»†ç²’åº¦è¯­ä¹‰ç†è§£ï¼ŒåŒæ—¶ç¼ºä¹å¤šè¯­è¨€è¾“å…¥æ”¯æŒï¼Œè¿™äº›é™åˆ¶æ˜¾è‘—å½±å“äº†å…¶æ›´å¹¿æ³›çš„åº”ç”¨èŒƒå›´ã€‚ç°æœ‰ç ”ç©¶å°è¯•ç”¨åŸºäºLLMçš„åµŒå…¥å™¨æ›¿æ¢CLIPæ–‡æœ¬ç¼–ç å™¨ï¼Œä½†ç”±äºLLMè¡¨ç¤ºç©ºé—´ä¸CLIPè§†è§‰-è¯­è¨€ç©ºé—´ç‹¬ç«‹é¢„è®­ç»ƒç¼ºä¹å¯¹é½å…ˆéªŒï¼Œç›´æ¥ä½¿ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œå¯¹é½ä¼šç ´åCLIPå›¾åƒç¼–ç å™¨çš„å†…åœ¨è§†è§‰-è¯­è¨€å¯¹é½ï¼Œå¯¼è‡´é¢„è®­ç»ƒçŸ¥è¯†åˆ©ç”¨ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> ProCLIPé‡‡ç”¨åŸºäºè¯¾ç¨‹å­¦ä¹ çš„æ¸è¿›å¼è§†è§‰-è¯­è¨€å¯¹é½æ¡†æ¶ï¼Œé¦–å…ˆé€šè¿‡çŸ¥è¯†è’¸é¦å°†CLIPæ–‡æœ¬ç¼–ç å™¨çš„çŸ¥è¯†è¿ç§»åˆ°åŸºäºLLMçš„åµŒå…¥å™¨ä¸­ï¼Œåˆ©ç”¨CLIPä¸°å¯Œçš„é¢„è®­ç»ƒçŸ¥è¯†å¹¶å»ºç«‹LLMåµŒå…¥å™¨ä¸CLIPå›¾åƒç¼–ç å™¨çš„åˆå§‹å¯¹é½ã€‚éšåé€šè¿‡å›¾åƒ-æ–‡æœ¬å¯¹æ¯”è°ƒä¼˜è¿›ä¸€æ­¥å¯¹é½CLIPå›¾åƒç¼–ç å™¨ä¸åŸºäºLLMçš„åµŒå…¥å™¨ï¼Œé‡‡ç”¨è‡ªè’¸é¦æ­£åˆ™åŒ–é¿å…è¿‡æ‹Ÿåˆã€‚åœ¨è¡¨ç¤ºç»§æ‰¿å’Œå¯¹æ¯”è°ƒä¼˜é˜¶æ®µä½¿ç”¨å®ä¾‹è¯­ä¹‰å¯¹é½æŸå¤±å’ŒåµŒå…¥ç»“æ„å¯¹é½æŸå¤±ä»¥å®ç°æ›´æœ‰æ•ˆçš„å¯¹é½ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº†CLIPæ–‡æœ¬ç¼–ç å™¨çš„é•¿åº¦é™åˆ¶å’Œå¤šè¯­è¨€æ”¯æŒé—®é¢˜ï¼ŒåŒæ—¶ä¿æŒäº†CLIPå›¾åƒç¼–ç å™¨çš„é¢„è®­ç»ƒçŸ¥è¯†å®Œæ•´æ€§ã€‚é€šè¿‡æ¸è¿›å¼å¯¹é½ç­–ç•¥ï¼Œå®ç°äº†åŸºäºLLMçš„åµŒå…¥å™¨ä¸CLIPè§†è§‰ç¼–ç å™¨çš„é«˜æ•ˆé›†æˆï¼Œæå‡äº†æ¨¡å‹åœ¨é•¿æ–‡æœ¬å¤„ç†å’Œå¤šè¯­è¨€ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ProCLIPè¯æ˜äº†æ¸è¿›å¼å¯¹é½ç­–ç•¥åœ¨æ•´åˆä¸åŒé¢„è®­ç»ƒæ¨¡å‹æ—¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›æä¾›äº†æ–°æ€è·¯ã€‚è¯¥æ–¹æ³•ä¸ä»…è§£å†³äº†CLIPçš„å›ºæœ‰å±€é™æ€§ï¼Œè¿˜ä¸ºæœªæ¥è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ‰©å±•å’Œæ”¹è¿›æä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚è¯­ä¹‰å’Œå¤šè¯­è¨€åœºæ™¯æ–¹é¢å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>The original CLIP text encoder is limited by a maximum input length of 77
tokens, which hampers its ability to effectively process long texts and perform
fine-grained semantic understanding. In addition, the CLIP text encoder lacks
support for multilingual inputs. All these limitations significantly restrict
its applicability across a broader range of tasks. Recent studies have
attempted to replace the CLIP text encoder with an LLM-based embedder to
enhance its ability in processing long texts, multilingual understanding, and
fine-grained semantic comprehension. However, because the representation spaces
of LLMs and the vision-language space of CLIP are pretrained independently
without alignment priors, direct alignment using contrastive learning can
disrupt the intrinsic vision-language alignment in the CLIP image encoder,
leading to an underutilization of the knowledge acquired during pre-training.
To address this challenge, we propose ProCLIP, a curriculum learning-based
progressive vision-language alignment framework to effectively align the CLIP
image encoder with an LLM-based embedder. Specifically, ProCLIP first distills
knowledge from CLIP's text encoder into the LLM-based embedder to leverage
CLIP's rich pretrained knowledge while establishing initial alignment between
the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns
the CLIP image encoder with the LLM-based embedder through image-text
contrastive tuning, employing self-distillation regularization to avoid
overfitting. To achieve a more effective alignment, instance semantic alignment
loss and embedding structure alignment loss are employed during representation
inheritance and contrastive tuning. The Code is available at
https://github.com/VisionXLab/ProCLIP</p>
<h3 id="39-feddeap-adaptive-dual-prompt-tuning-for-multi-domain-federated-learning">[39] <a href="https://arxiv.org/abs/2510.18837">FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning</a></h3>
<p><em>Yubin Zheng, Pak-Hei Yeung, Jing Xia, Tianjie Ju, Peng Tang, Weidong Qiu, Jagath C. Rajapakse</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FedDEAPæ¡†æ¶ï¼Œé€šè¿‡è§£è€¦è¯­ä¹‰å’Œé¢†åŸŸç‰¹å®šç‰¹å¾ã€è®¾è®¡åŒæç¤ºæœºåˆ¶ä»¥åŠå¯¹é½æ–‡æœ¬è§†è§‰è¡¨ç¤ºï¼Œæœ‰æ•ˆæå‡äº†CLIPåœ¨å¤šé¢†åŸŸè”é‚¦å­¦ä¹ åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è”é‚¦å­¦ä¹ åœ¨å¤šå®¢æˆ·ç«¯åä½œè®­ç»ƒä¸­é¢ä¸´é¢†åŸŸåç§»å’Œæ ‡ç­¾å¼‚æ„çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´èšåˆçš„å…¨å±€æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ï¼Œè€Œç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPè¿›è¡Œè·¨é¢†åŸŸè”é‚¦å¾®è°ƒæ—¶å­˜åœ¨é¢†åŸŸç‰¹å®šä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºè‡ªé€‚åº”è”é‚¦æç¤ºè°ƒä¼˜æ¡†æ¶FedDEAPï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šé€šè¿‡è¯­ä¹‰å’Œé¢†åŸŸå˜æ¢ç½‘ç»œè§£è€¦å›¾åƒä¸­çš„è¯­ä¹‰å’Œé¢†åŸŸç‰¹å®šç‰¹å¾ï¼›è®¾è®¡å…¨å±€è¯­ä¹‰æç¤ºå’Œå±€éƒ¨é¢†åŸŸæç¤ºçš„åŒæç¤ºæœºåˆ¶ä»¥å¹³è¡¡å…±äº«ä¸ä¸ªæ€§åŒ–ä¿¡æ¯ï¼›åœ¨ä¸¤ç§å­¦ä¹ å˜æ¢ä¸‹å¯¹é½æ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºä»¥ä¿æŒè¯­ä¹‰å’Œé¢†åŸŸä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> ç†è®ºåˆ†æå’Œåœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†CLIPåœ¨å¤šé¢†åŸŸè”é‚¦å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­çš„æ³›åŒ–æ€§èƒ½ï¼ŒéªŒè¯äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè”é‚¦å­¦ä¹ ä¸­è·¨é¢†åŸŸè§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç‰¹å¾è§£è€¦å’ŒåŒæç¤ºè®¾è®¡å®ç°äº†è¯­ä¹‰ä¸é¢†åŸŸçŸ¥è¯†çš„å¹³è¡¡ä¿ç•™ï¼Œä¸ºå¤šé¢†åŸŸè”é‚¦å­¦ä¹ çš„å‘å±•æä¾›äº†é‡è¦å¯ç¤ºã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Federated learning (FL) enables multiple clients to collaboratively train
machine learning models without exposing local data, balancing performance and
privacy. However, domain shift and label heterogeneity across clients often
hinder the generalization of the aggregated global model. Recently, large-scale
vision-language models like CLIP have shown strong zero-shot classification
capabilities, raising the question of how to effectively fine-tune CLIP across
domains in a federated setting. In this work, we propose an adaptive federated
prompt tuning framework, FedDEAP, to enhance CLIP's generalization in
multi-domain scenarios. Our method includes the following three key components:
(1) To mitigate the loss of domain-specific information caused by
label-supervised tuning, we disentangle semantic and domain-specific features
in images by using semantic and domain transformation networks with unbiased
mappings; (2) To preserve domain-specific knowledge during global prompt
aggregation, we introduce a dual-prompt design with a global semantic prompt
and a local domain prompt to balance shared and personalized information; (3)
To maximize the inclusion of semantic and domain information from images in the
generated text features, we align textual and visual representations under the
two learned transformations to preserve semantic and domain consistency.
Theoretical analysis and extensive experiments on four datasets demonstrate the
effectiveness of our method in enhancing the generalization of CLIP for
federated image recognition across multiple domains.</p>
<h3 id="40-dsi-bench-a-benchmark-for-dynamic-spatial-intelligence">[40] <a href="https://arxiv.org/abs/2510.18873">DSI-Bench: A Benchmark for Dynamic Spatial Intelligence</a></h3>
<p><em>Ziang Zhang, Zehan Wang, Guanghao Zhang, Weilong Dai, Yan Xia, Ziang Yan, Minjie Hong, Zhou Zhao</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†åŠ¨æ€ç©ºé—´æ™ºèƒ½æ¦‚å¿µå¹¶å¼€å‘äº†DSI-BenchåŸºå‡†ï¼ŒåŒ…å«è¿‘1000ä¸ªåŠ¨æ€è§†é¢‘å’Œ1700å¤šä¸ªæ‰‹åŠ¨æ ‡æ³¨é—®é¢˜ï¼Œç³»ç»Ÿè¯„ä¼°äº†14ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹å’Œä¸“å®¶æ¨¡å‹åœ¨åŠ¨æ€3Dåœºæ™¯ç†è§£ä¸­çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹å’Œè§†è§‰ä¸“å®¶æ¨¡å‹åœ¨2Dä»»åŠ¡å’Œé™æ€åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹åŠ¨æ€3Dåœºæ™¯çš„å®Œæ•´ç†è§£èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨è§‚å¯Ÿè€…å’Œç‰©ä½“åŒæ—¶ç§»åŠ¨çš„åŠ¨æ€ç©ºé—´å…³ç³»æ¨ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åŠ¨æ€ç©ºé—´æ™ºèƒ½æ¦‚å¿µï¼Œæ„å»ºäº†DSI-BenchåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«è¿‘1000ä¸ªåŠ¨æ€è§†é¢‘å’Œè¶…è¿‡1700ä¸ªæ‰‹åŠ¨æ ‡æ³¨é—®é¢˜ï¼Œè¦†ç›–ä¹ç§è§£è€¦çš„è§‚å¯Ÿè€…å’Œç‰©ä½“è¿åŠ¨æ¨¡å¼ï¼Œé‡‡ç”¨ç©ºé—´å’Œæ—¶é—´å¯¹ç§°è®¾è®¡ä»¥å‡å°‘åå·®ã€‚</p>
<p><strong>Result:</strong> å¯¹14ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹å’Œä¸“å®¶æ¨¡å‹çš„è¯„ä¼°æ­ç¤ºäº†å…³é”®å±€é™æ€§ï¼šæ¨¡å‹ç»å¸¸æ··æ·†è§‚å¯Ÿè€…å’Œç‰©ä½“è¿åŠ¨ï¼Œè¡¨ç°å‡ºè¯­ä¹‰åå·®ï¼Œå¹¶ä¸”åœ¨åŠ¨æ€åœºæ™¯ä¸­æ— æ³•å‡†ç¡®æ¨æ–­ç›¸å¯¹ç©ºé—´å…³ç³»ã€‚</p>
<p><strong>Conclusion:</strong> DSI-Benchä¸ºåŠ¨æ€ç©ºé—´æ™ºèƒ½çš„å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„å‘ç°å’Œè§è§£ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨åŠ¨æ€3Dåœºæ™¯ç†è§£ä¸­çš„æ ¹æœ¬ç¼ºé™·ï¼Œä¸ºé€šç”¨æ¨¡å‹å’Œä¸“å®¶æ¨¡å‹çš„æœªæ¥å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>Reasoning about dynamic spatial relationships is essential, as both observers
and objects often move simultaneously. Although vision-language models (VLMs)
and visual expertise models excel in 2D tasks and static scenarios, their
ability to fully understand dynamic 3D scenarios remains limited. We introduce
Dynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly
1,000 dynamic videos and over 1,700 manually annotated questions covering nine
decoupled motion patterns of observers and objects. Spatially and temporally
symmetric designs reduce biases and enable systematic evaluation of models'
reasoning about self-motion and object motion. Our evaluation of 14 VLMs and
expert models reveals key limitations: models often conflate observer and
object motion, exhibit semantic biases, and fail to accurately infer relative
relationships in dynamic scenarios. Our DSI-Bench provides valuable findings
and insights about the future development of general and expertise models with
dynamic spatial intelligence.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="41-efficient-toxicity-detection-in-gaming-chats-a-comparative-study-of-embeddings-fine-tuned-transformers-and-llms">[41] <a href="https://arxiv.org/abs/2510.17924">Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs</a></h3>
<p><em>Yehor Tereshchenko, Mika HÃ¤mÃ¤lÃ¤inen</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡å¯¹åœ¨çº¿æ¸¸æˆèŠå¤©ä¸­çš„è‡ªåŠ¨æ¯’æ€§æ£€æµ‹æ–¹æ³•è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒåˆ†æï¼Œæå‡ºäº†ä¸€ç§æ··åˆå®¡æ ¸ç³»ç»Ÿæ¶æ„ï¼Œå®éªŒç»“æœè¡¨æ˜å¾®è°ƒçš„DistilBERTåœ¨å‡†ç¡®æ€§å’Œæˆæœ¬ä¹‹é—´å®ç°äº†æœ€ä½³æƒè¡¡ã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³åœ¨çº¿æ¸¸æˆç¯å¢ƒä¸­å†…å®¹å®¡æ ¸çš„æŒ‘æˆ˜ï¼Œé€šè¿‡ç³»ç»Ÿè¯„ä¼°ä¸åŒNLPæ–¹æ³•åœ¨æ¯’æ€§æ£€æµ‹ä¸­çš„æ€§èƒ½å·®å¼‚ï¼Œä¸ºåŠ¨æ€åœ¨çº¿ç¯å¢ƒæä¾›æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–å®¡æ ¸è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶è¯„ä¼°äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ä¸åµŒå…¥ã€å¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºã€å¾®è°ƒTransformeræ¨¡å‹ä»¥åŠæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œå¹¶æå‡ºäº†ç»“åˆè‡ªåŠ¨åŒ–æ£€æµ‹å’ŒæŒç»­å­¦ä¹ æœºåˆ¶çš„æ··åˆå®¡æ ¸ç³»ç»Ÿæ¶æ„ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºä¸åŒæ–¹æ³•åœ¨åˆ†ç±»å‡†ç¡®æ€§ã€å¤„ç†é€Ÿåº¦å’Œè®¡ç®—æˆæœ¬æ–¹é¢å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®å¼‚ï¼Œå…¶ä¸­å¾®è°ƒçš„DistilBERTæ¨¡å‹åœ¨å‡†ç¡®æ€§ä¸æˆæœ¬æƒè¡¡æ–¹é¢è¡¨ç°æœ€ä¼˜ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœä¸ºåœ¨åŠ¨æ€åœ¨çº¿æ¸¸æˆç¯å¢ƒä¸­éƒ¨ç½²æˆæœ¬æ•ˆç›Šé«˜ã€æ•ˆç‡é«˜çš„å†…å®¹å®¡æ ¸ç³»ç»Ÿæä¾›äº†å®è¯ä¾æ®ï¼Œå±•ç¤ºäº†æ··åˆç³»ç»Ÿæ¶æ„åœ¨ä¼˜åŒ–äººå·¥å®¡æ ¸å·¥ä½œè´Ÿè½½æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>This paper presents a comprehensive comparative analysis of Natural Language
Processing (NLP) methods for automated toxicity detection in online gaming
chats. Traditional machine learning models with embeddings, large language
models (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer
models, and retrieval-augmented generation (RAG) approaches are evaluated. The
evaluation framework assesses three critical dimensions: classification
accuracy, processing speed, and computational costs. A hybrid moderation system
architecture is proposed that optimizes human moderator workload through
automated detection and incorporates continuous learning mechanisms. The
experimental results demonstrate significant performance variations across
methods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs.
The findings provide empirical evidence for deploying cost-effective, efficient
content moderation systems in dynamic online gaming environments.</p>
<h3 id="42-from-local-to-global-revisiting-structured-pruning-paradigms-for-large-language-models">[42] <a href="https://arxiv.org/abs/2510.18030">From Local to Global: Revisiting Structured Pruning Paradigms for Large Language Models</a></h3>
<p><em>Ziyan Wang, Enmao Diao, Qi Le, Pu Wang, Minwoo Lee, Shu-ping Yeh, Evgeny Stupachenko, Hao Feng, Li Yang</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†GISPï¼ˆå…¨å±€è¿­ä»£ç»“æ„åŒ–å‰ªæï¼‰æ–¹æ³•ï¼Œä¸€ç§åè®­ç»ƒå…¨å±€ç»“æ„åŒ–å‰ªææŠ€æœ¯ï¼Œé€šè¿‡åŸºäºæŸå¤±çš„é‡è¦æ€§æƒé‡å’Œè¿­ä»£å‰ªæç­–ç•¥ï¼Œåœ¨ä¿æŒè¯­è¨€å»ºæ¨¡æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡ä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡ï¼Œç‰¹åˆ«åœ¨40-50%ç¨€ç–åº¦ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ä¸»æµçš„å±€éƒ¨ç»“æ„åŒ–å‰ªææ–¹æ³•å­˜åœ¨ä»»åŠ¡æ— å…³æ€§é—®é¢˜ï¼Œå®ƒä»¬é€šè¿‡ä¼˜åŒ–å±‚é—´é‡æ„è€Œéä»»åŠ¡ç›®æ ‡æ¥ä¿æŒå›°æƒ‘åº¦æˆ–é€šç”¨é›¶æ ·æœ¬è¡Œä¸ºï¼Œä½†æ— æ³•å……åˆ†åˆ©ç”¨ä»»åŠ¡ç‰¹å®šçš„æ ¡å‡†ä¿¡å·ï¼Œå¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æå‡æœ‰é™ã€‚</p>
<p><strong>Method:</strong> GISPé‡‡ç”¨å…¨å±€ç»“æ„åŒ–å‰ªææ–¹æ³•ï¼Œä½¿ç”¨åŸºäºä¸€é˜¶æŸå¤±çš„é‡è¦æ€§æƒé‡åœ¨ç»“æ„çº§åˆ«èšåˆæ³¨æ„åŠ›å¤´å’ŒMLPé€šé“ï¼Œå¹¶é€šè¿‡å—çº§å½’ä¸€åŒ–å’Œè¿­ä»£å‰ªæç­–ç•¥æ¥ç¨³å®šé«˜ç¨€ç–åº¦ä¸‹çš„ç²¾åº¦ï¼Œé¿å…å›°æƒ‘åº¦å´©æºƒä¸”æ— éœ€ä¸­é—´å¾®è°ƒã€‚</p>
<p><strong>Result:</strong> åœ¨Llama2-7B/13Bã€Llama3-8Bå’ŒMistral-0.3-7Bä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGISPæŒç»­é™ä½WikiText-2å›°æƒ‘åº¦å¹¶æå‡ä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡ï¼Œå°¤å…¶åœ¨40-50%ç¨€ç–åº¦ä¸‹è¡¨ç°çªå‡ºï¼›åœ¨DeepSeek-R1-Distill-Llama-3-8Bä¸Šï¼Œä»»åŠ¡å¯¹é½æ ¡å‡†æ˜¾è‘—æé«˜äº†GSM8Kçš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜å…¨å±€è¿­ä»£ç»“æ„åŒ–å‰ªæèƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡æ¨¡å‹å‹ç¼©ä¸ä»»åŠ¡æ€§èƒ½ï¼Œæ”¯æŒ"ä¸€æ¬¡å‰ªæã€å¤šæ¬¡éƒ¨ç½²"çš„å·¥ä½œæµç¨‹ï¼Œå¹¶ä¸ºä»»åŠ¡ç‰¹å®šä¼˜åŒ–æä¾›äº†è‡ªç„¶æ”¯æŒï¼Œä¸ºé«˜æ•ˆéƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>Structured pruning is a practical approach to deploying large language models
(LLMs) efficiently, as it yields compact, hardware-friendly architectures.
However, the dominant local paradigm is task-agnostic: by optimizing layer-wise
reconstruction rather than task objectives, it tends to preserve perplexity or
generic zero-shot behavior but fails to capitalize on modest task-specific
calibration signals, often yielding limited downstream gains. We revisit global
structured pruning and present GISP-Global Iterative Structured Pruning-a
post-training method that removes attention heads and MLP channels using
first-order, loss-based important weights aggregated at the structure level
with block-wise normalization. An iterative schedule, rather than one-shot
pruning, stabilizes accuracy at higher sparsity and mitigates perplexity
collapse without requiring intermediate fine-tuning; the pruning trajectory
also forms nested subnetworks that support a "prune-once, deploy-many"
workflow. Furthermore, because importance is defined by a model-level loss,
GISP naturally supports task-specific objectives; we instantiate perplexity for
language modeling and a margin-based objective for decision-style tasks.
Extensive experiments show that across Llama2-7B/13B, Llama3-8B, and
Mistral-0.3-7B, GISP consistently lowers WikiText-2 perplexity and improves
downstream accuracy, with especially strong gains at 40-50% sparsity; on
DeepSeek-R1-Distill-Llama-3-8B with GSM8K, task-aligned calibration
substantially boosts exact-match accuracy.</p>
<h3 id="43-text-or-pixels-it-takes-half-on-the-token-efficiency-of-visual-text-inputs-in-multimodal-llms">[43] <a href="https://arxiv.org/abs/2510.18279">Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs</a></h3>
<p><em>Yanhong Li, Zixuan Lan, Jiawei Zhou</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬è¾“å…¥å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡å°†é•¿æ–‡æœ¬æ¸²æŸ“ä¸ºå›¾åƒè¾“å…¥åˆ°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œæ˜¾è‘—å‡å°‘äº†è§£ç å™¨ä»¤ç‰Œä½¿ç”¨é‡ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½åœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶å®ç°æ¥è¿‘50%çš„ä»¤ç‰ŒèŠ‚çœã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€å¤§è¯­è¨€æ¨¡å‹åŠå…¶å¤šæ¨¡æ€å˜ä½“èƒ½å¤Ÿå¤„ç†è§†è§‰è¾“å…¥ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢æ˜¯å¦å¯ä»¥é€šè¿‡å°†æ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºå›¾åƒå½¢å¼æ¥å‡å°‘ä»¤ç‰Œä½¿ç”¨é‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ï¼Œè¿™ä¸ºè§£å†³é•¿æ–‡æœ¬å¤„ç†ä¸­çš„ä»¤ç‰Œæ•ˆç‡é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†é•¿æ–‡æœ¬è¾“å…¥æ¸²æŸ“ä¸ºå•ä¸ªå›¾åƒï¼Œç„¶åç›´æ¥æä¾›ç»™å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¤„ç†ï¼Œåˆ©ç”¨è§†è§‰æ–‡æœ¬è¡¨ç¤ºä½œä¸ºè¾“å…¥å‹ç¼©çš„æœ‰æ•ˆå½¢å¼ï¼Œç‰¹åˆ«é’ˆå¯¹è§£ç å™¨æ¶æ„çš„LLMsè¿›è¡Œäº†ä¼˜åŒ–è®¾è®¡ã€‚</p>
<p><strong>Result:</strong> åœ¨RULERé•¿ä¸Šä¸‹æ–‡æ£€ç´¢å’ŒCNN/DailyMailæ–‡æ¡£æ‘˜è¦ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ–‡æœ¬è½¬å›¾åƒæ–¹æ³•å®ç°äº†æ˜¾è‘—çš„ä»¤ç‰ŒèŠ‚çœï¼Œé€šå¸¸æ¥è¿‘50%çš„å‡å°‘ï¼ŒåŒæ—¶ä»»åŠ¡æ€§èƒ½æ²¡æœ‰å‡ºç°æ˜æ˜¾ä¸‹é™ï¼ŒéªŒè¯äº†è¯¥å‹ç¼©æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è§†è§‰æ–‡æœ¬è¡¨ç¤ºè¢«è¯æ˜æ˜¯è§£ç å™¨LLMsçš„ä¸€ç§å®ç”¨ä¸”æœ‰æ•ˆçš„è¾“å…¥å‹ç¼©å½¢å¼ï¼Œä¸ºå¤„ç†é•¿æ–‡æœ¬å†…å®¹æä¾›äº†æ–°çš„ä¼˜åŒ–é€”å¾„ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œè¿™ä¸€å‘ç°å¯¹æå‡å¤§è¯­è¨€æ¨¡å‹çš„è¾“å…¥æ•ˆç‡å…·æœ‰é‡è¦å¯ç¤ºæ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>Large language models (LLMs) and their multimodal variants can now process
visual inputs, including images of text. This raises an intriguing question:
can we compress textual inputs by feeding them as images to reduce token usage
while preserving performance? In this paper, we show that visual text
representations are a practical and surprisingly effective form of input
compression for decoder LLMs. We exploit the idea of rendering long text inputs
as a single image and provide it directly to the model. This leads to
dramatically reduced number of decoder tokens required, offering a new form of
input compression. Through experiments on two distinct benchmarks RULER
(long-context retrieval) and CNN/DailyMail (document summarization) we
demonstrate that this text-as-image method yields substantial token savings
(often nearly half) without degrading task performance.</p>
<h3 id="44-ecg-llm-training-and-evaluation-of-domain-specific-large-language-models-for-electrocardiography">[44] <a href="https://arxiv.org/abs/2510.18339">ECG-LLM-- training and evaluation of domain-specific large language models for electrocardiography</a></h3>
<p><em>Lara Ahrens, Wilhelm Haverkamp, Nils Strodthoff</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é€šè¿‡å¾®è°ƒå¼€æ”¾æƒé‡å¤§è¯­è¨€æ¨¡å‹å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œåœ¨å¿ƒè„ç—…å­¦é¢†åŸŸå®ç°äº†ä¸ä¸“æœ‰æ¨¡å‹ç›¸ç«äº‰çš„ä¸´åºŠæ€§èƒ½ï¼Œè¯æ˜äº†éšç§ä¿æŠ¤ã€æœ¬åœ°éƒ¨ç½²çš„åŒ»ç–—AIè§£å†³æ–¹æ¡ˆçš„å¯è¡Œæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰é¢†åŸŸé€‚åº”çš„å¼€æ”¾æƒé‡LLMåœ¨åŒ»ç–—åº”ç”¨ä¸­çš„æœ€ä¼˜é€‚åº”ç­–ç•¥ã€è¯„ä¼°æ–¹æ³•å’Œç›¸å¯¹äºé€šç”¨æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ä»ç¼ºä¹ç³»ç»Ÿç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å¿ƒç”µå›¾å­¦è¿™ä¸€é‡è¦å¿ƒè¡€ç®¡åŒ»å­¦é¢†åŸŸéœ€è¦æ·±å…¥æ¢ç´¢ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨å¯¹å¼€æ”¾æƒé‡æ¨¡å‹è¿›è¡Œé¢†åŸŸç‰¹å®šæ–‡çŒ®å¾®è°ƒçš„æ–¹æ³•ï¼Œå¹¶æ„å»ºäº†å¤šå±‚è¯„ä¼°æ¡†æ¶ï¼Œæ¯”è¾ƒäº†å¾®è°ƒæ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•å’ŒClaude Sonnet 3.7é€šç”¨æ¨¡å‹åœ¨å¿ƒè„ç—…å­¦ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Result:</strong> å¾®è°ƒçš„Llama 3.1 70Bæ¨¡å‹åœ¨å¤šé¡¹é€‰æ‹©é¢˜è¯„ä¼°å’Œè‡ªåŠ¨æ–‡æœ¬æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä¼˜ï¼Œåœ¨LLMä½œä¸ºè¯„åˆ¤è€…çš„è¯„ä¼°ä¸­ä»…æ¬¡äºClaude 3.7ï¼›äººç±»ä¸“å®¶è¯„ä¼°æ›´å€¾å‘äºClaude 3.7å’ŒRAGæ–¹æ³•å¤„ç†å¤æ‚æŸ¥è¯¢ï¼›å¾®è°ƒæ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°æ¨¡å¼ä¸­å‡æ˜¾è‘—ä¼˜äºå…¶åŸºç¡€ç‰ˆæœ¬ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†ä¸åŒè¯„ä¼°æ–¹æ³•é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å¼‚è´¨æ€§ï¼Œå¼ºè°ƒäº†è¯„ä¼°å¤æ‚æ€§ï¼Œä½†é€šè¿‡å¾®è°ƒå’ŒRAGçš„é¢†åŸŸç‰¹å®šé€‚åº”èƒ½å¤Ÿå®ç°ä¸ä¸“æœ‰æ¨¡å‹ç›¸ç«äº‰çš„æ€§èƒ½ï¼Œæ”¯æŒäº†éšç§ä¿æŠ¤ã€æœ¬åœ°å¯éƒ¨ç½²ä¸´åºŠè§£å†³æ–¹æ¡ˆçš„å¯è¡Œæ€§ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>Domain-adapted open-weight large language models (LLMs) offer promising
healthcare applications, from queryable knowledge bases to multimodal
assistants, with the crucial advantage of local deployment for privacy
preservation. However, optimal adaptation strategies, evaluation methodologies,
and performance relative to general-purpose LLMs remain poorly characterized.
We investigated these questions in electrocardiography, an important area of
cardiovascular medicine, by finetuning open-weight models on domain-specific
literature and implementing a multi-layered evaluation framework comparing
finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7
as a representative general-purpose model. Finetuned Llama 3.1 70B achieved
superior performance on multiple-choice evaluations and automatic text metrics,
ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert
evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned
models significantly outperformed their base counterparts across nearly all
evaluation modes. Our findings reveal substantial performance heterogeneity
across evaluation methodologies, underscoring assessment complexity.
Nevertheless, domain-specific adaptation through finetuning and RAG achieves
competitive performance with proprietary models, supporting the viability of
privacy-preserving, locally deployable clinical solutions.</p>
<h3 id="45-grounding-or-guessing-visual-signals-for-detecting-hallucinations-in-sign-language-translation">[45] <a href="https://arxiv.org/abs/2510.18439">Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation</a></h3>
<p><em>Yasser Hamidullah, Koel Dutta Chowdury, Yusser Al-Ghussin, Shakib Yazdani, Cennet Oguz, Josef van Genabith, Cristina EspaÃ±a-Bonet</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä»¤ç‰Œçº§å¯é æ€§çš„è§†è§‰è¯­è¨€æ¨¡å‹å¹»è§‰æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡é‡åŒ–è§£ç å™¨å¯¹è§†è§‰ä¿¡æ¯çš„ä¾èµ–ç¨‹åº¦æ¥é¢„æµ‹æ‰‹è¯­ç¿»è¯‘ä¸­çš„å¹»è§‰ç°è±¡ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œè§£é‡Šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ‰‹è¯­ç¿»è¯‘ä¸­è§†è§‰è¯­è¨€æ¨¡å‹å­˜åœ¨ä¸¥é‡çš„å¹»è§‰é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— ä¸­é—´è¯­ç´ ç›‘ç£çš„æ¨¡å‹ä¸­ï¼Œæ¨¡å‹å€¾å‘äºä¾èµ–è¯­è¨€å…ˆéªŒè€Œéè§†è§‰è¾“å…¥ç”Ÿæˆæ–‡æœ¬ï¼Œè¿™ä¸¥é‡å½±å“äº†ç¿»è¯‘çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºä»¤ç‰Œçº§å¯é æ€§åº¦é‡æ–¹æ³•ï¼Œç»“åˆåŸºäºç‰¹å¾çš„æ•æ„Ÿæ€§å’Œåäº‹å®ä¿¡å·ï¼Œå‰è€…é€šè¿‡æ©ç è§†é¢‘æ—¶å†…éƒ¨ç‰¹å¾çš„å˜åŒ–æ¥æµ‹é‡ï¼Œåè€…æ•æ‰å¹²å‡€ä¸ä¿®æ”¹è§†é¢‘è¾“å…¥ä¹‹é—´çš„æ¦‚ç‡å·®å¼‚ï¼Œæœ€ç»ˆèšåˆä¸ºå¥å­çº§å¯é æ€§è¯„åˆ†ã€‚</p>
<p><strong>Result:</strong> åœ¨PHOENIX-2014Tå’ŒCSL-Dailyä¸¤ä¸ªæ‰‹è¯­ç¿»è¯‘åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯é æ€§è¯„åˆ†èƒ½æœ‰æ•ˆé¢„æµ‹å¹»è§‰ç‡ï¼Œè·¨æ•°æ®é›†å’Œæ¶æ„å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§ï¼Œåœ¨è§†è§‰è´¨é‡ä¸‹é™æ—¶å¯é æ€§é™ä½ï¼Œä¸”ä¸æ–‡æœ¬ä¿¡å·ç»“åˆèƒ½è¿›ä¸€æ­¥æå‡å¹»è§‰é£é™©ä¼°è®¡ç²¾åº¦ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ç¡®ç«‹äº†å¯é æ€§ä½œä¸ºè¯Šæ–­æ‰‹è¯­ç¿»è¯‘å¹»è§‰çš„å®ç”¨å·¥å…·ï¼Œæ­ç¤ºäº†æ— ä¸­é—´è¯­ç´ æ¨¡å‹æ›´å®¹æ˜“äº§ç”Ÿå¹»è§‰çš„åŸå› ï¼Œä¸ºå¤šæ¨¡æ€ç”Ÿæˆä¸­æ›´é²æ£’çš„å¹»è§‰æ£€æµ‹å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>Hallucination, where models generate fluent text unsupported by visual
evidence, remains a major flaw in vision-language models and is particularly
critical in sign language translation (SLT). In SLT, meaning depends on precise
grounding in video, and gloss-free models are especially vulnerable because
they map continuous signer movements directly into natural language without
intermediate gloss supervision that serves as alignment. We argue that
hallucinations arise when models rely on language priors rather than visual
input. To capture this, we propose a token-level reliability measure that
quantifies how much the decoder uses visual information. Our method combines
feature-based sensitivity, which measures internal changes when video is
masked, with counterfactual signals, which capture probability differences
between clean and altered video inputs. These signals are aggregated into a
sentence-level reliability score, providing a compact and interpretable measure
of visual grounding. We evaluate the proposed measure on two SLT benchmarks
(PHOENIX-2014T and CSL-Daily) with both gloss-based and gloss-free models. Our
results show that reliability predicts hallucination rates, generalizes across
datasets and architectures, and decreases under visual degradations. Beyond
these quantitative trends, we also find that reliability distinguishes grounded
tokens from guessed ones, allowing risk estimation without references; when
combined with text-based signals (confidence, perplexity, or entropy), it
further improves hallucination risk estimation. Qualitative analysis highlights
why gloss-free models are more susceptible to hallucinations. Taken together,
our findings establish reliability as a practical and reusable tool for
diagnosing hallucinations in SLT, and lay the groundwork for more robust
hallucination detection in multimodal generation.</p>
<h3 id="46-every-step-evolves-scaling-reinforcement-learning-for-trillion-scale-thinking-model">[46] <a href="https://arxiv.org/abs/2510.18855">Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model</a></h3>
<p><em>Ling Team, Anqi Shen, Baihui Li, Bin Hu, Bin Jing, Cai Chen, Chao Huang, Chao Zhang, Chaokun Yang, Cheng Lin, Chengyao Wen, Congqi Li, Deng Zhao, Dingbo Yuan, Donghai You, Fagui Mao, Fanzhuang Meng, Feng Xu, Guojie Li, Guowei Wang, Hao Dai, Haonan Zheng, Hong Liu, Jia Guo, Jiaming Liu, Jian Liu, Jianhao Fu, Jiannan Shi, Jianwen Wang, Jianxin Lai, Jin Yang, Jun Mei, Jun Zhou, Junbo Zhao, Junping Zhao, Kuan Xu, Le Su, Lei Chen, Li Tang, Liang Jiang, Liangcheng Fu, Lianhao Xu, Linfeng Shi, Lisha Liao, Longfei Zheng, Meng Li, Mingchun Chen, Qi Zuo, Qiang Cheng, Qianggang Cao, Qitao Shi, Quanrui Guo, Senlin Zhu, Shaofei Wang, Shaomian Zheng, Shuaicheng Li, Shuwei Gu, Siba Chen, Tao Wu, Tao Zhang, Tianyu Zhang, Tianyu Zhou, Tiwei Bie, Tongkai Yang, Wang Hong, Wang Ren, Weihua Chen, Wenbo Yu, Wengang Zheng, Xiangchun Wang, Xiaodong Yan, Xiaopei Wan, Xin Zhao, Xinyu Kong, Xinyu Tang, Xudong Han, Xudong Wang, Xuemin Yang, Xueyu Hu, Yalin Zhang, Yan Sun, Yicheng Shan, Yilong Wang, Yingying Xu, Yongkang Liu, Yongzhen Guo, Yuanyuan Wang, Yuchen Yan, Yuefan Wang, Yuhong Guo, Zehuan Li, Zhankai Xu, Zhe Li, Zhenduo Zhang, Zhengke Gui, Zhenxuan Pan, Zhenyu Huang, Zhenzhong Lan, Zhiqiang Ding, Zhiqiang Zhang, Zhixun Li, Zhizhen Liu, Zihao Wang, Zujie Wen</em></p>
<h4 id="tldr_45">ğŸ§© TL;DR</h4>
<p>Ring-1Tæ˜¯é¦–ä¸ªå¼€æºçš„ä¸‡äº¿å‚æ•°æ€è€ƒæ¨¡å‹ï¼Œé€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°è§£å†³äº†ä¸‡äº¿çº§å‚æ•°æ¨¡å‹è®­ç»ƒä¸­çš„ç¨³å®šæ€§ã€æ•ˆç‡å’Œç³»ç»Ÿç“¶é¢ˆé—®é¢˜ï¼Œåœ¨å¤šé¡¹æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†çªç ´æ€§æˆæœã€‚</p>
<hr />
<h4 id="detailed-summary_45">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä¸‡äº¿çº§å‚æ•°æ¨¡å‹è®­ç»ƒé¢ä¸´çš„å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®­ç»ƒ-æ¨ç†ä¸å¯¹é½ã€æ¨æ¼”å¤„ç†æ•ˆç‡ä½ä¸‹ä»¥åŠå¼ºåŒ–å­¦ä¹ ç³»ç»Ÿç“¶é¢ˆç­‰é—®é¢˜ï¼Œè¿™äº›é™åˆ¶äº†å¤§è§„æ¨¡æ¨ç†æ¨¡å‹çš„å‘å±•å’Œåº”ç”¨ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸‰é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼šIcePopé€šè¿‡ä»¤ç‰Œçº§å·®å¼‚æ©ç å’Œè£å‰ªç¨³å®šå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼›C3PO++åœ¨ä»¤ç‰Œé¢„ç®—ä¸‹åŠ¨æ€åˆ†åŒºé•¿æ¨æ¼”ä»¥æé«˜æ—¶é—´æ•ˆç‡ï¼›ASystemæ˜¯ä¸“ä¸ºå…‹æœä¸‡äº¿å‚æ•°æ¨¡å‹è®­ç»ƒç³»ç»Ÿç“¶é¢ˆè€Œè®¾è®¡çš„é«˜æ€§èƒ½å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚</p>
<p><strong>Result:</strong> Ring-1Tåœ¨å…³é”®åŸºå‡†æµ‹è¯•ä¸­å–å¾—çªç ´æ€§æˆæœï¼šAIME-2025å¾—åˆ†ä¸º93.4ï¼ŒHMMT-2025å¾—åˆ†ä¸º86.72ï¼ŒCodeForceså¾—åˆ†ä¸º2088ï¼ŒARC-AGI-v1å¾—åˆ†ä¸º55.94ï¼Œå¹¶åœ¨IMO-2025ä¸Šè¾¾åˆ°é“¶ç‰Œæ°´å¹³ï¼Œå±•ç°äº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡å‘ç ”ç©¶ç¤¾åŒºå‘å¸ƒå®Œæ•´çš„1Tå‚æ•°MoEæ¨¡å‹ï¼Œè¯¥ç ”ç©¶åœ¨æ°‘ä¸»åŒ–å¤§è§„æ¨¡æ¨ç†æ™ºèƒ½æ–¹é¢æ ‘ç«‹äº†é‡è¦é‡Œç¨‹ç¢‘ï¼Œä¸ºå¼€æºæ¨¡å‹æ€§èƒ½å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œæ¨åŠ¨äº†å‰æ²¿æ¨ç†èƒ½åŠ›çš„æ™®åŠåº”ç”¨ã€‚</p>
<hr />
<h4 id="abstract_45">ğŸ“„ Abstract</h4>
<p>We present Ring-1T, the first open-source, state-of-the-art thinking model
with a trillion-scale parameter. It features 1 trillion total parameters and
activates approximately 50 billion per token. Training such models at a
trillion-parameter scale introduces unprecedented challenges, including
train-inference misalignment, inefficiencies in rollout processing, and
bottlenecks in the RL system. To address these, we pioneer three interconnected
innovations: (1) IcePop stabilizes RL training via token-level discrepancy
masking and clipping, resolving instability from training-inference mismatches;
(2) C3PO++ improves resource utilization for long rollouts under a token budget
by dynamically partitioning them, thereby obtaining high time efficiency; and
(3) ASystem, a high-performance RL framework designed to overcome the systemic
bottlenecks that impede trillion-parameter model training. Ring-1T delivers
breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on
HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a
silver medal-level result on the IMO-2025, underscoring its exceptional
reasoning capabilities. By releasing the complete 1T parameter MoE model to the
community, we provide the research community with direct access to cutting-edge
reasoning capabilities. This contribution marks a significant milestone in
democratizing large-scale reasoning intelligence and establishes a new baseline
for open-source model performance.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="47-activation-manifold-projection-liberating-task-specific-behaviors-from-llm-architectures">[47] <a href="https://arxiv.org/abs/2510.17902">Activation Manifold Projection: Liberating Task-Specific Behaviors from LLM Architectures</a></h3>
<p><em>Al Kari</em></p>
<h4 id="tldr_46">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Cartridge Activation Space Transfer (CAST)æ¡†æ¶ï¼Œé€šè¿‡åœ¨ä¸åŒLLMæ¶æ„çš„æ¿€æ´»æµå½¢ä¹‹é—´å­¦ä¹ éçº¿æ€§æ˜ å°„ï¼Œå®ç°äº†LoRAé€‚é…å™¨çš„é›¶æ ·æœ¬è·¨æ¶æ„è¿ç§»ï¼Œè§£å†³äº†å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒè¡Œä¸ºå—é™äºæºæ¨¡å‹æ¶æ„çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_46">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§è¯­è¨€æ¨¡å‹æ¶æ„çš„æ¿€å¢å¸¦æ¥äº†ä¸€ä¸ªæ ¹æœ¬æ€§æŒ‘æˆ˜ï¼šé€šè¿‡LoRAç­‰å¾®è°ƒæ–¹æ³•å­¦ä¹ åˆ°çš„æœ‰ä»·å€¼çš„ä»»åŠ¡ç‰¹å®šè¡Œä¸ºè¢«é”å®šåœ¨æºæ¨¡å‹æ¶æ„ä¸­ï¼Œç§°ä¸ºæ¶æ„é”å®šã€‚ç°æœ‰çš„è¿ç§»æ–¹æ³•è¯•å›¾é€šè¿‡å¯¹é½æ¨¡å‹çš„é™æ€æƒé‡ç©ºé—´æ¥å¼¥åˆè¿™ä¸€å·®è·ï¼Œä½†è¿™ç§æ–¹æ³•è„†å¼±ä¸”é—´æ¥ï¼Œä¾èµ–äºå‚æ•°å‡ ä½•ä¹‹é—´çš„å¾®å¼±ç›¸å…³æ€§ã€‚</p>
<p><strong>Method:</strong> CASTæ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„èŒƒå¼ï¼Œé€šè¿‡åœ¨ä¸åŒLLMæ¶æ„çš„æ¿€æ´»æµå½¢ä¹‹é—´å­¦ä¹ ç›´æ¥çš„éçº¿æ€§æ˜ å°„æ¥è§£æ”¾LoRAç¼–ç çš„è¡Œä¸ºã€‚è¯¥æ–¹æ³•å°†é¢„è®­ç»ƒçš„LoRAè§†ä¸ºå†»ç»“çš„"è¡Œä¸ºå†…æ ¸"ï¼Œå­¦ä¹ ä¸€ç»„è½»é‡çº§çš„åŒå‘æŠ•å½±å¤´ï¼Œå°†ç›®æ ‡æ¨¡å‹çš„æ¿€æ´»æµè½¬æ¢ä¸ºæºæ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œåº”ç”¨å†»ç»“å†…æ ¸ï¼Œç„¶åå°†ç»“æœæŠ•å½±å›æ¥ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼ŒCASTå®ç°äº†æ ‡å‡†LoRAé€‚é…å™¨çš„çœŸæ­£"é›¶æ ·æœ¬"è¿ç§»ï¼Œåœ¨Llama-2å’ŒMistralç­‰å¼‚æ„æ¨¡å‹å®¶æ—ä¹‹é—´çš„è¿ç§»ä¸­ï¼ŒCASTè¿ç§»çš„é€‚é…å™¨æ€§èƒ½è¾¾åˆ°äº†åœ¨ç›®æ ‡æ¨¡å‹ä¸Šå®Œå…¨é‡æ–°è®­ç»ƒçš„LoRAçš„85-95%ã€‚è¯¥æ–¹æ³•åœ¨å®šé‡ä¸Šä¼˜äºå½“å‰çš„æƒé‡ç©ºé—´è¿ç§»æŠ€æœ¯ï¼Œå»ºç«‹äº†æ¨¡å‹äº’æ“ä½œæ€§çš„æ–°æœ€å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> CASTæ¡†æ¶é€šè¿‡æ¿€æ´»ç©ºé—´æ˜ å°„è€Œéæƒé‡ç©ºé—´å¯¹é½ï¼Œæœ‰æ•ˆè§£è€¦äº†å­¦ä¹ æŠ€èƒ½ä¸æºæ¶æ„çš„ç»‘å®šï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºè¿ç§»æä¾›äº†æ›´ç›´æ¥å’Œé²æ£’çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†æ¨¡å‹äº’æ“ä½œæ€§çš„å‘å±•ï¼Œå¹¶ä¸ºæœªæ¥å¼‚æ„æ¨¡å‹é—´çš„çŸ¥è¯†å…±äº«å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_46">ğŸ“„ Abstract</h4>
<p>The proliferation of Large Language Model (LLM) architectures presents a
fundamental challenge: valuable, task-specific behaviors learned through
fine-tuning methods like Low-Rank Adaptation (LoRA) are effectively trapped
within their source model's architecture, herein referred to architectural
lock-in. Existing transfer methods attempt to bridge this gap by aligning the
static weight spaces of models, a brittle and indirect approach that relies on
tenuous correlations between parameter geometries. This paper introduces a
fundamentally different and more direct paradigm: the Cartridge Activation
Space Transfer (CAST), a novel framework that liberates LoRA-encoded behaviors
by learning a direct, nonlinear mapping between the activation manifolds, the
geometric structures formed by the model's internal neuron activations, of two
distinct LLM architectures. CAST treats a pre-trained LoRA as a frozen
"behavioral kernel." It learns a set of lightweight, bidirectional projection
heads that translate the target model's activation stream into the source
model's latent space, apply the frozen kernel, and project the result back.
This process, trained on a general text corpus without any task-specific data,
effectively decouples the learned skill from the source architecture. We
demonstrate that CAST enables true "zero-shot" translation of any standard LoRA
adapter. Our experiments, including transfers between heterogeneous model
families like Llama-2 and Mistral, show that CAST-translated adapters achieve
85-95\% of the performance of a LoRA fully retrained on the target model,
quantitatively outperforming current weight-space transfer techniques and
establishing a new state-of-the-art in model interoperability.</p>
<h3 id="48-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning">[48] <a href="https://arxiv.org/abs/2510.18318">Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning</a></h3>
<p><em>Aaron Bell, Amit Aides, Amr Helmy, Arbaaz Muslim, Aviad Barzilai, Aviv Slobodkin, Bolous Jaber, David Schottlander, George Leifman, Joydeep Paul, Mimi Sun, Nadav Sherman, Natalie Williams, Per Bjornsson, Roy Lee, Ruth Alcantara, Thomas Turnbull, Tomer Shekel, Vered Silverman, Yotam Gigi, Adam Boulanger, Alex Ottenwess, Ali Ahmadalipour, Anna Carter, Charles Elliott, David Andre, Elad Aharoni, Gia Jung, Hassler Thurston, Jacob Bien, Jamie McPike, Juliet Rothenberg, Kartik Hegde, Kel Markert, Kim Philipp Jablonski, Luc Houriez, Monica Bharel, Phing VanLee, Reuven Sayag, Sebastian Pilarski, Shelley Cazares, Shlomi Pasternak, Siduo Jiang, Stone Jiang, Thomas Colthurst, Yang Chen, Yehonathan Refael, Yochai Blau, Yuval Carny, Yael Maguire, Avinatan Hassidim, James Manyika, Tim Thelin, Genady Beryozkin, Gautam Prasad, Luke Barrington, Yossi Matias, Niv Efron, Shravya Shetty</em></p>
<h4 id="tldr_47">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Earth AIï¼Œä¸€ä¸ªç»“åˆåœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹å’Œæ™ºèƒ½æ¨ç†å¼•æ“çš„AIç³»ç»Ÿï¼Œé€šè¿‡å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å’ŒGeminié©±åŠ¨çš„æ™ºèƒ½ä»£ç†ï¼Œæ˜¾è‘—æå‡äº†ä»å¤æ‚åœ°ç†ç©ºé—´æ•°æ®ä¸­æå–å¯æ“ä½œæ´å¯Ÿçš„èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_47">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ°ç†ç©ºé—´æ•°æ®è™½ç„¶è•´å«å·¨å¤§æ½œåŠ›ï¼Œä½†å…¶æµ·é‡æ€§ã€å¤šæ ·æ€§ä»¥åŠä¸åŒçš„åˆ†è¾¨ç‡ã€æ—¶é—´å°ºåº¦å’Œç¨€ç–æ€§ç»™å…¨é¢åˆ†æå’Œè§£è¯»å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥æœ‰æ•ˆå¤„ç†è¿™äº›å¤æ‚æ•°æ®å¹¶ä»ä¸­æå–æ·±åˆ»æ´å¯Ÿã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æ„å»ºäº†ä¸‰ä¸ªå…³é”®é¢†åŸŸï¼ˆè¡Œæ˜Ÿå°ºåº¦å½±åƒã€äººå£å’Œç¯å¢ƒï¼‰çš„åŸºç¡€æ¨¡å‹ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªGeminié©±åŠ¨çš„æ™ºèƒ½æ¨ç†å¼•æ“ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿè”åˆæ¨ç†å¤šä¸ªåŸºç¡€æ¨¡å‹ä»¥åŠå¤§å‹åœ°ç†ç©ºé—´æ•°æ®æºå’Œå·¥å…·æ¥å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤æŸ¥è¯¢ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†åŸºç¡€æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›å’Œæ–°é¢–ç‰¹æ€§ï¼ŒéªŒè¯äº†å½“è¿™äº›æ¨¡å‹è”åˆä½¿ç”¨æ—¶èƒ½å¤Ÿä¸ºåœ°ç†ç©ºé—´æ¨ç†æä¾›äº’è¡¥ä»·å€¼ï¼Œå…¶ååŒä½œç”¨è§£é”äº†æ›´ä¼˜è¶Šçš„é¢„æµ‹èƒ½åŠ›ï¼Œåœ¨çœŸå®ä¸–ç•Œå±æœºåœºæ™¯çš„æ–°åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»£ç†èƒ½å¤Ÿæä¾›å…³é”®åŠæ—¶çš„æ´å¯Ÿã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„ååŒæ•´åˆå’Œæ™ºèƒ½ä»£ç†æ¨ç†ï¼Œæœ‰æ•ˆå¼¥åˆäº†åŸå§‹åœ°ç†ç©ºé—´æ•°æ®ä¸å¯æ“ä½œç†è§£ä¹‹é—´çš„å·®è·ï¼Œä¸ºåœ°ç†ç©ºé—´AIæä¾›äº†æ–°çš„èŒƒå¼ï¼Œå±•ç¤ºäº†åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­æä¾›åŠæ—¶æ´å¯Ÿçš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_47">ğŸ“„ Abstract</h4>
<p>Geospatial data offers immense potential for understanding our planet.
However, the sheer volume and diversity of this data along with its varied
resolutions, timescales, and sparsity pose significant challenges for thorough
analysis and interpretation. This paper introduces Earth AI, a family of
geospatial AI models and agentic reasoning that enables significant advances in
our ability to unlock novel and profound insights into our planet. This
approach is built upon foundation models across three key domains--Planet-scale
Imagery, Population, and Environment--and an intelligent Gemini-powered
reasoning engine. We present rigorous benchmarks showcasing the power and novel
capabilities of our foundation models and validate that when used together,
they provide complementary value for geospatial inference and their synergies
unlock superior predictive capabilities. To handle complex, multi-step queries,
we developed a Gemini-powered agent that jointly reasons over our multiple
foundation models along with large geospatial data sources and tools. On a new
benchmark of real-world crisis scenarios, our agent demonstrates the ability to
deliver critical and timely insights, effectively bridging the gap between raw
geospatial data and actionable understanding.</p>
<h3 id="49-med-vragent-a-framework-for-medical-visual-reasoning-enhanced-agents">[49] <a href="https://arxiv.org/abs/2510.18424">Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents</a></h3>
<p><em>Guangfu Guo, Xiaoqian Lu, Yue Feng</em></p>
<h4 id="tldr_48">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºMed-VRAgentçš„åŒ»ç–—è§†è§‰æ¨ç†æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè§†è§‰å¼•å¯¼ã€è‡ªæˆ‘å¥–åŠ±æœºåˆ¶å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåŒ»ç–—VQAåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_48">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨å¹»è§‰ã€æ¨¡ç³Šæè¿°ã€é€»è¾‘ä¸ä¸€è‡´å’Œå®šä½èƒ½åŠ›å·®ç­‰å…³é”®é—®é¢˜ï¼Œè¿™äº›å±€é™æ€§é™åˆ¶äº†å…¶åœ¨åŒ»ç–—é¢†åŸŸçš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åŸºäºè§†è§‰å¼•å¯¼å’Œè‡ªæˆ‘å¥–åŠ±èŒƒå¼ï¼Œç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢æ„å»ºåŒ»ç–—è§†è§‰æ¨ç†æ™ºèƒ½ä½“æ¡†æ¶ï¼Œå¹¶é€šè¿‡æ”¶é›†çš„è½¨è¿¹æ•°æ®ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç›®æ ‡å¯¹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŒ»ç–—è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æ‰€æå‡ºæ¡†æ¶åœ¨æå‡åŒ»ç–—è§†è§‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆè§†è§‰å¼•å¯¼ã€æ ‘æœç´¢å’Œå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½ä½“æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè§£å†³åŒ»ç–—è§†è§‰æ¨ç†ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºåŒ»ç–—AIåº”ç”¨æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¹¶å±•ç¤ºäº†é€šè¿‡è½¨è¿¹åé¦ˆè¿›è¡Œæ¨¡å‹ä¼˜åŒ–çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_48">ğŸ“„ Abstract</h4>
<p>Visual Language Models (VLMs) achieve promising results in medical reasoning
but struggle with hallucinations, vague descriptions, inconsistent logic and
poor localization. To address this, we propose a agent framework named Medical
Visual Reasoning Agent (\textbf{Med-VRAgent}). The approach is based on Visual
Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By
combining the Visual Guidance with tree search, Med-VRAgent improves the
medical visual reasoning capabilities of VLMs. We use the trajectories
collected by Med-VRAgent as feedback to further improve the performance by
fine-tuning the VLMs with the proximal policy optimization (PPO) objective.
Experiments on multiple medical VQA benchmarks demonstrate that our method
outperforms existing approaches.</p>
<h3 id="50-starbench-a-turn-based-rpg-benchmark-for-agentic-multimodal-decision-making-and-information-seeking">[50] <a href="https://arxiv.org/abs/2510.18483">StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking</a></h3>
<p><em>Haoran Zhang, Chenhao Zhu, Sicong Guo, Hanzhe Guo, Haiming Li, Donglin Yu</em></p>
<h4 id="tldr_49">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†StarBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®æ¸¸æˆå®¢æˆ·ç«¯ä¸­çš„å¤šæ¨¡æ€å†³ç­–å’Œä¸»åŠ¨ä¿¡æ¯å¯»æ±‚èƒ½åŠ›ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æ„ŸçŸ¥åˆ°æ§åˆ¶ä¿çœŸåº¦æ–¹é¢å­˜åœ¨çš„æ˜¾è‘—å·®è·ã€‚</p>
<hr />
<h4 id="detailed-summary_49">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®æ¸¸æˆå®¢æˆ·ç«¯ä¸­å®ç°äººç±»ç©å®¶çº§åˆ«çš„æ¸¸æˆèƒ½åŠ›ä»é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å°†åŸå§‹å±å¹•æˆªå›¾æ˜ å°„åˆ°æ—¶é—´ä¸€è‡´çš„ä½çº§åŠ¨ä½œï¼Œä»¥åŠåœ¨é‡åˆ°å›°éš¾æ—¶å†³å®šä½•æ—¶å¯»æ±‚æŒ‡å¯¼ã€‚ç°æœ‰ç ”ç©¶åœ¨ç®€åŒ–æ§åˆ¶æˆ–å·¥å…·æ”¯æ¶ä¸‹å–å¾—äº†é¼“èˆäººå¿ƒçš„ç»“æœï¼Œä½†äººç±»åŒ–æ¸¸æˆèƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†StarBenchåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºã€Šå´©åï¼šæ˜Ÿç©¹é“é“ã€‹çš„å›åˆåˆ¶RPGåŸºå‡†ï¼ŒåŒ…å«å…«ä¸ªæˆ˜æ–—ä»»åŠ¡å’Œä¸¤ç§æ§åˆ¶æ¨¡å¼ï¼šç›´æ¥æ§åˆ¶æ¨¡å¼ä»…æä¾›å±å¹•æˆªå›¾å¹¶è¦æ±‚è¾“å‡ºä½çº§æ“ä½œåŸè¯­ï¼Œå·¥å…·è¾…åŠ©æ§åˆ¶æ¨¡å¼å…è®¸é€šè¿‡æ£€æµ‹å™¨å’ŒOCRè¾“å‡ºå°†é«˜çº§æ„å›¾æ˜ å°„åˆ°æ“ä½œåŸè¯­ã€‚åŸºå‡†è¿˜åŒ…æ‹¬ask-or-actè¯Šæ–­ï¼Œç”¨äºæµ‹é‡ä»£ç†é€‰æ‹©è¯·æ±‚æŒ‡å¯¼çš„æ—¶æœºå’Œæ•ˆæœã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ç›´æ¥æ§åˆ¶æ¨¡å¼ä¸‹ï¼Œå½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ„ŸçŸ¥åˆ°æ§åˆ¶ä¿çœŸåº¦æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚åŒæ—¶ç ”ç©¶è¡¨æ˜ï¼Œæ˜æ™ºçš„ä¿¡æ¯å¯»æ±‚è¡Œä¸ºä¸æ”¹è¿›çš„æˆåŠŸç‡ç›¸å…³ï¼Œä¸ºä»£ç†ä¸»åŠ¨ä¿¡æ¯å¯»æ±‚å’Œå¤šæ¨¡æ€å†³ç­–æä¾›äº†å¯å¤ç°çš„è¡¡é‡æ ‡å‡†ã€‚</p>
<p><strong>Conclusion:</strong> StarBenchåŸºå‡†ä¸ºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®å®¢æˆ·ç«¯æ¸¸æˆä¸­çš„ä»£ç†ä¸»åŠ¨ä¿¡æ¯å¯»æ±‚å’Œå¤šæ¨¡æ€å†³ç­–èƒ½åŠ›æä¾›äº†æ ‡å‡†åŒ–æ¡†æ¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨ä½çº§åŠ¨ä½œæ§åˆ¶æ–¹é¢ä»æœ‰æ”¹è¿›ç©ºé—´ï¼Œè€Œé€‚å½“çš„ä¿¡æ¯å¯»æ±‚ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæå‡ä»»åŠ¡æˆåŠŸç‡ï¼Œä¸ºæœªæ¥æ™ºèƒ½ä½“ç ”ç©¶æŒ‡æ˜äº†é‡è¦æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_49">ğŸ“„ Abstract</h4>
<p>Human players do more than press buttons: they ground what they see on screen
into precise keyboard-mouse actions and, when stuck, they seek information
before trying again. We ask whether current vision-language models (VLMs) can
do the same. Despite encouraging results under simplified control or tool
scaffolds, human-like play in a real client - mapping raw screenshots to
temporally coherent low-level actions while deciding when to ask for guidance -
remains an open challenge. We introduce StarBench, a turn-based RPG benchmark
derived from Honkai: Star Rail that targets these two human-like competencies:
multimodal decision-making from pixels to actions and agentic information
seeking. StarBench standardizes evaluation across eight combat tasks and two
regimes with shared tasks and metrics: (i) direct control, where agents receive
only screenshots and must emit low-level primitives (click and keypress) with
no semantic hints; and (ii) tool-assisted control, where higher-level intents
can be mapped to primitives by detectors and OCR outputs provide optional
textualized observations to ease UI grounding. To mirror human practice,
StarBench also includes an ask-or-act diagnostic that measures whether and when
agents choose to request brief guidance before proceeding, and how that choice
affects subsequent performance. We report reference baselines for contemporary
VLMs and a human reference. Results expose sizable gaps in
perception-to-control fidelity in the direct regime, while showing that
judicious information seeking correlates with improved success, establishing
StarBench as a reproducible yardstick for agentic information seeking and
multimodal decision-making in real-client play.</p>
<h3 id="51-var-visual-attention-reasoning-via-structured-search-and-backtracking">[51] <a href="https://arxiv.org/abs/2510.18619">VAR: Visual Attention Reasoning via Structured Search and Backtracking</a></h3>
<p><em>Wei Cai, Jian Zhao, Yuchen Yuan, Tianle Zhang, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li</em></p>
<h4 id="tldr_50">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†è§†è§‰æ³¨æ„åŠ›æ¨ç†ï¼ˆVARï¼‰æ¡†æ¶ï¼Œé€šè¿‡å°†åŸºç¡€æ¨ç†é‡æ„ä¸ºç»“æ„åŒ–æœç´¢è¿‡ç¨‹ï¼Œè§£å†³äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜å’Œè„†å¼±çº¿æ€§æ¨ç†é™åˆ¶ï¼Œåœ¨å¹»è§‰å’Œå®‰å…¨åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚</p>
<hr />
<h4 id="detailed-summary_50">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å­˜åœ¨é«˜å¹»è§‰å€¾å‘å’Œä¾èµ–è„†å¼±çº¿æ€§æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ï¼Œå¯¼è‡´åœ¨å¤æ‚ä»»åŠ¡ä¸­å¤±è´¥ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿè¿›è¡Œå¯è¿½æº¯æ¨ç†å’Œè‡ªæˆ‘çº æ­£çš„æ–°æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> VARæ¡†æ¶å°†åŸºç¡€æ¨ç†åˆ†è§£ä¸ºå¯è¿½æº¯è¯æ®å®šä½å’ŒåŸºäºæœç´¢çš„æ€ç»´é“¾ç”Ÿæˆä¸¤ä¸ªå…³é”®é˜¶æ®µï¼ŒåŒ…å«å›æº¯æœºåˆ¶è¿›è¡Œè‡ªæˆ‘çº æ­£ï¼Œå¹¶é€šè¿‡å…·æœ‰è¯­ä¹‰å’Œå‡ ä½•è‡ªéªŒè¯ç»„ä»¶çš„å¤šé¢å¥–åŠ±å‡½æ•°å¼•å¯¼æœç´¢è¿‡ç¨‹ã€‚</p>
<p><strong>Result:</strong> 7Bå‚æ•°çš„VARæ¨¡å‹åœ¨å…¨é¢çš„å¹»è§‰å’Œå®‰å…¨åŸºå‡†æµ‹è¯•å¥—ä»¶ä¸­åˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸é¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿå±•ç°å‡ºç«äº‰æ€§æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“æ„åŒ–æœç´¢ç­–ç•¥åœ¨è§£å†³å¤šæ¨¡æ€æ¨¡å‹å¹»è§‰é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¼€å‘æ›´å¯é çš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæä¾›äº†ç†è®ºåŸºç¡€å’Œå®è·µæ¡†æ¶ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºå’Œåº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_50">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs), despite their advances, are
hindered by their high hallucination tendency and heavy reliance on brittle,
linear reasoning processes, leading to failures in complex tasks. To address
these limitations, we introduce Visual Attention Reasoning (VAR), a novel
framework that recasts grounded reasoning as a structured search over a
reasoning trajectory space. VAR decomposes the reasoning process into two key
stages: traceable evidence grounding and search-based chain-of-thought (CoT)
generation, which incorporates a backtracking mechanism for self-correction.
The search is guided by a multi-faceted reward function with semantic and
geometric self-verification components, which penalize outputs that are not
faithfully grounded in the visual input. We provide a theoretical analysis for
our search strategy, validating its capability to find the correct solution
with high probability. Experimental results show that our 7B model, VAR-7B,
sets a new state-of-the-art on a comprehensive suite of hallucination and
safety benchmarks, significantly outperforming existing open-source models and
demonstrating competitive performance against leading proprietary systems.</p>
<h3 id="52-seg-the-hab-language-guided-geospatial-algae-bloom-reasoning-and-segmentation">[52] <a href="https://arxiv.org/abs/2510.18751">Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation</a></h3>
<p><em>Patterson Hsieh, Jerry Yeh, Mao-Chi He, Wen-Han Hsieh, Elvis Hsieh</em></p>
<h4 id="tldr_51">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ALGOSç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆé¥æ„Ÿå›¾åƒåˆ†å‰²ä¸ä¸¥é‡æ€§è¯„ä¼°çš„è—»åç›‘æµ‹æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆGeoSAMè¾…åŠ©çš„äººå·¥è¯„ä¼°å’Œè§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒï¼Œå®ç°äº†å¯¹æœ‰å®³è—»åçš„è‡ªåŠ¨åŒ–ç›‘æµ‹å’Œä¸¥é‡ç¨‹åº¦é‡åŒ–ã€‚</p>
<hr />
<h4 id="detailed-summary_51">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ°”å€™å˜åŒ–åŠ å‰§äº†æœ‰å®³è—»åç‰¹åˆ«æ˜¯è“è—»çš„çˆ†å‘ï¼Œä¼ ç»Ÿäººå·¥æ°´è´¨ç›‘æµ‹æ–¹æ³•åŠ³åŠ¨å¯†é›†ä¸”æ—¶ç©ºè¦†ç›–æœ‰é™ï¼Œç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¥æ„Ÿå›¾åƒæ¨ç†å’Œè—»åä¸¥é‡æ€§é‡åŒ–æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> ALGOSç³»ç»Ÿæ•´åˆäº†GeoSAMè¾…åŠ©çš„äººå·¥è¯„ä¼°ç”¨äºé«˜è´¨é‡åˆ†å‰²æ©ç æ ‡æ³¨ï¼Œå¹¶åœ¨NASAçš„è“è—»èšåˆäººå·¥æ ‡ç­¾æ•°æ®é›†ä¸Šå¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œä¸¥é‡æ€§é¢„æµ‹ï¼Œå®ç°äº†åˆ†å‰²ä¸æ¨ç†çš„ååŒå¤„ç†ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ALGOSåœ¨åˆ†å‰²å’Œä¸¥é‡æ€§ç­‰çº§ä¼°è®¡æ–¹é¢å‡è¡¨ç°å‡ºç¨³å¥æ€§èƒ½ï¼Œä¸ºè‡ªåŠ¨åŒ–è“è—»ç›‘æµ‹ç³»ç»Ÿæä¾›äº†å¯é çš„æŠ€æœ¯åŸºç¡€ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå®ç”¨åŒ–è‡ªåŠ¨åŒ–è“è—»ç›‘æµ‹ç³»ç»Ÿå¼€è¾Ÿäº†é“è·¯ï¼Œå±•ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç¯å¢ƒé¥æ„Ÿç›‘æµ‹ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿå®ç°å¤§è§„æ¨¡ã€é«˜æ•ˆçš„è—»åç›‘æµ‹å’Œé£é™©è¯„ä¼°ã€‚</p>
<hr />
<h4 id="abstract_51">ğŸ“„ Abstract</h4>
<p>Climate change is intensifying the occurrence of harmful algal bloom (HAB),
particularly cyanobacteria, which threaten aquatic ecosystems and human health
through oxygen depletion, toxin release, and disruption of marine biodiversity.
Traditional monitoring approaches, such as manual water sampling, remain
labor-intensive and limited in spatial and temporal coverage. Recent advances
in vision-language models (VLMs) for remote sensing have shown potential for
scalable AI-driven solutions, yet challenges remain in reasoning over imagery
and quantifying bloom severity. In this work, we introduce ALGae Observation
and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB
monitoring that combines remote sensing image understanding with severity
estimation. Our approach integrates GeoSAM-assisted human evaluation for
high-quality segmentation mask curation and fine-tunes vision language model on
severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)
from NASA. Experiments demonstrate that ALGOS achieves robust performance on
both segmentation and severity-level estimation, paving the way toward
practical and automated cyanobacterial monitoring systems.</p>
  </article>
</body>
</html>
