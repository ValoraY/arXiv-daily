<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-01-05.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 29]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 1]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 3]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-from-sight-to-insight-improving-visual-reasoning-capabilities-of-multimodal-models-via-reinforcement-learning">[1] <a href="https://arxiv.org/abs/2601.00215">From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning</a></h3>
<p><em>Omar Sharif, Eftekhar Hossain, Patrick Ng</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±é©±åŠ¨æ–¹æ³•ï¼Œç”¨äºå¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡è®¾è®¡é’ˆå¯¹ä¸åŒæ¨ç†æ–¹é¢çš„å¥–åŠ±å‡½æ•°å¹¶é‡‡ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨è§†è§‰è°œé¢˜ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ¨ç†é“¾ç¼ºä¹è§†è§‰ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆï¼Œé™åˆ¶äº†å…¶åœ¨éœ€è¦ç²¾ç¡®è§†è§‰æ„ŸçŸ¥çš„ä»»åŠ¡ï¼ˆå¦‚è§†è§‰è°œé¢˜ï¼‰ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜è§†è§‰æ„ŸçŸ¥æ˜¯æ­¤ç±»ä»»åŠ¡çš„å…³é”®ç“¶é¢ˆï¼Œå°†å›¾åƒè½¬æ¢ä¸ºæ–‡æœ¬æè¿°å¯æ˜¾è‘—æå‡æ€§èƒ½ï¼Œä½†ç°æœ‰æ–¹æ³•éœ€è¦æ˜‚è´µçš„ç›‘ç£æ•°æ®ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨å¥–åŠ±é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè®¾è®¡äº†å…­ä¸ªé’ˆå¯¹ä¸åŒæ¨ç†æ–¹é¢çš„å¥–åŠ±å‡½æ•°ï¼Œæ¶µç›–å›¾åƒç†è§£ã€æ€ç»´æ­¥éª¤å’Œç­”æ¡ˆå‡†ç¡®æ€§ç­‰ç»´åº¦ã€‚é€šè¿‡ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œæ˜¾å¼æ¿€åŠ±æ¨¡å‹ç”Ÿæˆæ›´é•¿ã€ç»“æ„åŒ–çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶é˜²æ­¢è§†è§‰ä¿¡æ¯è¢«ç»•è¿‡ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºæ–‡æœ¬æè¿°å¯ä½¿Claude 3.5å’ŒClaude 3.7çš„æ€§èƒ½åˆ†åˆ«æå‡26.7%å’Œ23.6%ã€‚åœ¨Qwen-2.5-VL-7Bæ¨¡å‹ä¸Šï¼Œæ‰€æå‡ºçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”åŸºç¡€æ¨¡å‹å®ç°äº†5.56%çš„æ€§èƒ½æå‡ï¼Œä¸”åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–è®¾ç½®ä¸‹å‡è¡¨ç°å‡ºä¸€è‡´çš„å¢ç›Šã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯å®äº†å¼ºåŒ–å­¦ä¹ æ˜¯è§£é”å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é•¿è§†è§‰æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæœºåˆ¶ï¼Œæ— éœ€æ˜‚è´µçš„ç›‘ç£æ•°æ®ã€‚æ‰€æå‡ºçš„å¥–åŠ±å‡½æ•°è®¾è®¡å’ŒGRPOä¼˜åŒ–æ–¹æ³•ä¸ºå¢å¼ºæ¨¡å‹è§†è§‰æ¨ç†èƒ½åŠ›æä¾›äº†ç³»ç»Ÿæ¡†æ¶ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.</p>
<h3 id="2-teleworld-towards-dynamic-multimodal-synthesis-with-a-4d-world-model">[2] <a href="https://arxiv.org/abs/2601.00051">TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model</a></h3>
<p><em>Yabo Chen, Yuanzhi Liang, Jiepeng Wang, Tingxi Chen, Junfei Cheng, Zixiao Gu, Yuyang Huang, Zicheng Jiang, Wei Li, Tian Li, Weichen Li, Zuoxin Li, Guangce Liu, Jialun Liu, Junqi Liu, Haoyuan Wang, Qizhen Weng, Xuan'er Wu, Xunzhi Xiang, Xiaoyan Yang, Xin Zhang, Shiwen Zhang, Junyu Zhou, Chengcheng Zhou, Haibin Huang, Chi Zhang, Xuelong Li</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>TeleWorldæå‡ºäº†ä¸€ç§å®æ—¶å¤šæ¨¡æ€4Dä¸–ç•Œå»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆ-é‡å»º-å¼•å¯¼èŒƒå¼ç»Ÿä¸€è§†é¢‘ç”Ÿæˆã€åŠ¨æ€åœºæ™¯é‡å»ºå’Œé•¿æœŸä¸–ç•Œè®°å¿†ï¼Œå®ç°äº†ç©ºé—´ã€æ—¶é—´å’Œç‰©ç†ä¸€è‡´æ€§çš„é—­ç¯ç³»ç»Ÿï¼Œæ¨åŠ¨äº†å®ç”¨äº¤äº’å¼ä¸–ç•Œæ¨¡å‹çš„å‘å±•ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹è™½ç„¶è§†è§‰è´¨é‡ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†åœ¨å®æ—¶äº¤äº’ã€é•¿æ—¶ç¨‹ä¸€è‡´æ€§å’ŒåŠ¨æ€åœºæ™¯çš„æŒä¹…è®°å¿†æ–¹é¢å­˜åœ¨å±€é™ï¼Œé˜»ç¢äº†å…¶å‘å±•ä¸ºå®ç”¨çš„ä¸–ç•Œæ¨¡å‹ã€‚ç ”ç©¶æ—¨åœ¨è§£å†³è¿™äº›é™åˆ¶ï¼Œæ¨åŠ¨ä¸–ç•Œæ¨¡å‹å‘å®ç”¨ã€äº¤äº’å¼å’Œè®¡ç®—å¯è®¿é—®çš„ç³»ç»Ÿæ¼”è¿›ã€‚</p>
<p><strong>Method:</strong> TeleWorldé‡‡ç”¨ç”Ÿæˆ-é‡å»º-å¼•å¯¼èŒƒå¼ï¼Œå°†ç”Ÿæˆçš„è§†é¢‘æµè¿ç»­é‡å»ºä¸ºåŠ¨æ€4Dæ—¶ç©ºè¡¨ç¤ºï¼Œè¿›è€ŒæŒ‡å¯¼åç»­ç”Ÿæˆä»¥ä¿æŒä¸€è‡´æ€§ã€‚æ¡†æ¶é‡‡ç”¨è‡ªå›å½’æ‰©æ•£è§†é¢‘æ¨¡å‹ï¼Œç»“åˆå®è§‚-å¾®è§‚è§„åˆ’ï¼ˆMMPLï¼‰çš„åˆ†å±‚è§„åˆ’æ–¹æ³•å‡å°‘è¯¯å·®ç´¯ç§¯ï¼Œå¹¶é€šè¿‡é«˜æ•ˆçš„åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼ˆDMDï¼‰å®ç°å®æ—¶åˆæˆï¼Œåœ¨ç»Ÿä¸€4Dæ¡†æ¶ä¸­æ•´åˆäº†åŠ¨æ€å¯¹è±¡å»ºæ¨¡å’Œé™æ€åœºæ™¯è¡¨ç¤ºã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTeleWorldåœ¨é™æ€å’ŒåŠ¨æ€ä¸–ç•Œç†è§£ã€é•¿æœŸä¸€è‡´æ€§å’Œå®æ—¶ç”Ÿæˆæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¡†æ¶åœ¨ä¿æŒç©ºé—´ã€æ—¶é—´å’Œç‰©ç†ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œå®ç°äº†ä½å»¶è¿Ÿçš„é•¿æ—¶ç¨‹ç”Ÿæˆï¼Œä¸ºäº¤äº’å¼ã€å…·å¤‡è®°å¿†åŠŸèƒ½çš„ä¸–ç•Œæ¨¡å‹æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Conclusion:</strong> TeleWorldä»£è¡¨äº†å‘å®ç”¨äº¤äº’å¼ä¸–ç•Œæ¨¡å‹è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ï¼Œé€šè¿‡ç»Ÿä¸€è§†é¢‘ç”Ÿæˆã€åŠ¨æ€åœºæ™¯é‡å»ºå’Œé•¿æœŸè®°å¿†çš„é—­ç¯ç³»ç»Ÿï¼Œä¸ºå¤šæ¨¡æ€ç”Ÿæˆå’Œå…·èº«æ™ºèƒ½åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç”Ÿæˆ-é‡å»º-å¼•å¯¼èŒƒå¼å®ç°æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¹¶ä¸ºè®¡ç®—å—é™ç¯å¢ƒä¸‹çš„å®æ—¶ä¸–ç•Œå»ºæ¨¡æä¾›äº†å¯è¡Œè·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.</p>
<h3 id="3-its-never-too-late-noise-optimization-for-collapse-recovery-in-trained-diffusion-models">[3] <a href="https://arxiv.org/abs/2601.00090">It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models</a></h3>
<p><em>Anne Harrington, A. Sophia Koepke, Shyamgopal Karthik, Trevor Darrell, Alexei A. Efros</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å™ªå£°ä¼˜åŒ–æ¥ç¼“è§£æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­æ¨¡å¼å´©æºƒé—®é¢˜çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒåŸºç¡€æ¨¡å‹ä¿çœŸåº¦çš„åŒæ—¶æ˜¾è‘—æé«˜äº†ç”Ÿæˆå¤šæ ·æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“ä»£æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç»™å®šç›¸åŒæ–‡æœ¬æç¤ºæ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„æ¨¡å¼å´©æºƒç°è±¡ï¼Œå³ç”Ÿæˆå›¾åƒç¼ºä¹å¤šæ ·æ€§ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å¼•å¯¼æœºåˆ¶æˆ–ç”Ÿæˆå¤§é‡å€™é€‰å›¾åƒåè¿›è¡Œç²¾ç‚¼æ¥è§£å†³æ­¤é—®é¢˜ï¼Œä½†æœ¬æ–‡æ¢ç´¢äº†é€šè¿‡å™ªå£°ä¼˜åŒ–çš„ä¸åŒæ–¹å‘æ¥æå‡ç”Ÿæˆå¤šæ ·æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„å™ªå£°ä¼˜åŒ–ç›®æ ‡æ¥ç¼“è§£æ¨¡å¼å´©æºƒé—®é¢˜ï¼ŒåŒæ—¶åˆ†æäº†å™ªå£°çš„é¢‘ç‡ç‰¹æ€§ï¼Œå¹¶å±•ç¤ºäº†å…·æœ‰ä¸åŒé¢‘ç‡åˆ†å¸ƒçš„æ›¿ä»£å™ªå£°åˆå§‹åŒ–æ–¹æ³•å¯ä»¥åŒæ—¶æ”¹å–„ä¼˜åŒ–è¿‡ç¨‹å’Œæœç´¢æ•ˆæœã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œå™ªå£°ä¼˜åŒ–æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢å‡å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£æ¨¡å¼å´©æºƒç°è±¡ï¼ŒåŒæ—¶ä¿æŒåŸºç¡€æ¨¡å‹çš„ä¿çœŸåº¦ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜å™ªå£°ä¼˜åŒ–æ˜¯è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­æ¨¡å¼å´©æºƒé—®é¢˜çš„æœ‰æ•ˆé€”å¾„ï¼Œé€šè¿‡åˆ†æå™ªå£°é¢‘ç‡ç‰¹æ€§å¹¶é‡‡ç”¨é€‚å½“çš„åˆå§‹åŒ–ç­–ç•¥ï¼Œå¯ä»¥åœ¨ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡çš„å‰æä¸‹æ˜¾è‘—æå‡å¤šæ ·æ€§ï¼Œä¸ºç”Ÿæˆæ¨¡å‹çš„æ”¹è¿›æä¾›äº†æ–°çš„æŠ€æœ¯æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.</p>
<h3 id="4-spatial4d-bench-a-versatile-4d-spatial-intelligence-benchmark">[4] <a href="https://arxiv.org/abs/2601.00092">Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark</a></h3>
<p><em>Pan Wang, Yang Liu, Guile Wu, Eduardo R. Corral-Soto, Chengjie Huang, Binbin Xu, Dongfeng Bai, Xu Yan, Yuan Ren, Xingxin Chen, Yizhe Wu, Tao Huang, Wenjun Wan, Xin Wu, Pei Zhou, Xuyang Dai, Kangbo Lv, Hongbo Zhang, Yosef Fried, Aixue Ye, Bailan Feng, Zhenyu Chen, Zhen Li, Yingcong Chen, Yiyi Liao, Bingbing Liu</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Spatial4D-Benchï¼Œä¸€ä¸ªç”¨äºå…¨é¢è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹4Dç©ºé—´æ™ºèƒ½çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«çº¦40,000ä¸ªé—®ç­”å¯¹å’Œ18ä¸ªä»»åŠ¡ï¼Œæ­ç¤ºäº†å½“å‰MLLMsåœ¨4Dç©ºé—´æ¨ç†æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> äººç±»å¤©ç”Ÿå…·å¤‡4Dç©ºé—´æ™ºèƒ½ï¼Œèƒ½å¤Ÿæ„ŸçŸ¥å’Œå¤„ç†ç‰©ä½“éšæ—¶é—´çš„å˜åŒ–ï¼Œä½†ç°æœ‰ç©ºé—´æ™ºèƒ½åŸºå‡†æµ‹è¯•å¾€å¾€è§„æ¨¡è¾ƒå°æˆ–å¤šæ ·æ€§æœ‰é™ï¼Œæ— æ³•å…¨é¢è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿè¾¾åˆ°äººç±»æ°´å¹³çš„4Dç©ºé—´æ™ºèƒ½ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†Spatial4D-Benchè¿™ä¸€å¤§è§„æ¨¡å¤šä»»åŠ¡è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«çº¦40,000ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–18ä¸ªæ˜ç¡®å®šä¹‰çš„ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡è¢«ç³»ç»Ÿåœ°ç»„ç»‡ä¸ºå…­ä¸ªè®¤çŸ¥ç±»åˆ«ï¼šç‰©ä½“ç†è§£ã€åœºæ™¯ç†è§£ã€ç©ºé—´å…³ç³»ç†è§£ã€æ—¶ç©ºå…³ç³»ç†è§£ã€ç©ºé—´æ¨ç†å’Œæ—¶ç©ºæ¨ç†ã€‚</p>
<p><strong>Result:</strong> åœ¨Spatial4D-Benchä¸Šå¯¹å„ç§å¼€æºå’Œä¸“æœ‰MLLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•åï¼Œå‘ç°å®ƒä»¬åœ¨å¤šç§4Dç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼ŒåŒ…æ‹¬è·¯å¾„è§„åˆ’ã€åŠ¨ä½œè¯†åˆ«å’Œç‰©ç†åˆç†æ€§æ¨ç†ç­‰ä»»åŠ¡ï¼Œè¡¨æ˜å½“å‰æ¨¡å‹è¿œæœªè¾¾åˆ°äººç±»æ°´å¹³çš„4Dç©ºé—´æ™ºèƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºç¤¾åŒºæä¾›äº†å…³äºMLLMsç©ºé—´è®¤çŸ¥èƒ½åŠ›çš„é‡è¦è§è§£ï¼ŒSpatial4D-BenchåŸºå‡†æµ‹è¯•å°†ä¿ƒè¿›å¼€å‘æ›´å…·èƒ½åŠ›çš„MLLMsï¼Œæœç€äººç±»æ°´å¹³çš„4Dç©ºé—´æ™ºèƒ½å‘å±•ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†ç»“æ„åŒ–çš„è¯„ä¼°æ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.</p>
<h3 id="5-a-spatially-masked-adaptive-gated-network-for-multimodal-post-flood-water-extent-mapping-using-sar-and-incomplete-multispectral-data">[5] <a href="https://arxiv.org/abs/2601.00123">A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data</a></h3>
<p><em>Hyunho Lee, Wenwen Li</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSMAGNetçš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨SARæ•°æ®ä½œä¸ºæ´ªæ°´åæ°´åŸŸèŒƒå›´æ˜ å°„çš„ä¸»è¦è¾“å…¥ï¼Œå¹¶é€šè¿‡ç‰¹å¾èåˆé›†æˆäº’è¡¥çš„MSIæ•°æ®ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹ç¼ºå¤±æ•°æ®çš„é²æ£’æ€§å’Œå®é™…æ´ªæ°´ç®¡ç†åœºæ™¯çš„é€‚ç”¨æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨æ´ªæ°´å“åº”é˜¶æ®µï¼ŒåŠæ—¶å‡†ç¡®çš„æ°´åŸŸèŒƒå›´æ˜ å°„å¯¹ç¾å®³ç®¡ç†è‡³å…³é‡è¦ã€‚è™½ç„¶SARæ•°æ®æ˜¯ä¸»è¦æ•°æ®æºï¼Œä½†å°†SARä¸MSIæ•°æ®é€šè¿‡å¤šæ¨¡æ€æ–¹æ³•ç»“åˆèƒ½æå‡æ˜ å°„ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ´ªæ°´å³°å€¼æœŸé—´æˆ–ä¹‹ååŠæ—¶è§‚æµ‹æœ‰é™çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œå¦‚ä½•åœ¨SARåŸºç¡€çš„æ´ªæ°´åæ°´åŸŸèŒƒå›´æ˜ å°„è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°é›†æˆéƒ¨åˆ†å¯ç”¨çš„MSIæ•°æ®ä»æ˜¯ä¸€ä¸ªæœªå……åˆ†æ¢ç´¢çš„ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ç©ºé—´æ©è”½è‡ªé€‚åº”é—¨æ§ç½‘ç»œï¼ˆSMAGNetï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä»¥SARæ•°æ®ä½œä¸ºæ´ªæ°´åæ°´åŸŸèŒƒå›´æ˜ å°„çš„ä¸»è¦è¾“å…¥ï¼Œå¹¶é€šè¿‡ç‰¹å¾èåˆæœºåˆ¶é›†æˆäº’è¡¥çš„MSIæ•°æ®ã€‚è¯¥æ¨¡å‹é‡‡ç”¨è‡ªé€‚åº”é›†æˆç­–ç•¥ï¼Œèƒ½å¤Ÿå¤„ç†ä¸åŒå¯ç”¨ç¨‹åº¦çš„MSIæ•°æ®ï¼Œå³ä½¿åœ¨MSIæ•°æ®å®Œå…¨ç¼ºå¤±çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒæ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> åœ¨C2S-MS Floodsæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSMAGNetåœ¨ä¸åŒMSIæ•°æ®å¯ç”¨æ€§æ°´å¹³ä¸‹ï¼Œå…¶é¢„æµ‹æ€§èƒ½å§‹ç»ˆä¼˜äºå…¶ä»–å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç‰¹åˆ«å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿å½“MSIæ•°æ®å®Œå…¨ç¼ºå¤±æ—¶ï¼ŒSMAGNetçš„æ€§èƒ½åœ¨ç»Ÿè®¡ä¸Šä¸ä»…ä½¿ç”¨SARæ•°æ®è®­ç»ƒçš„U-Netæ¨¡å‹ç›¸å½“ï¼Œè¿™è¡¨æ˜æ¨¡å‹å¯¹ç¼ºå¤±æ•°æ®å…·æœ‰é²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> SMAGNeté€šè¿‡è‡ªé€‚åº”é›†æˆéƒ¨åˆ†å¯ç”¨çš„MSIæ•°æ®ï¼Œä¸ä»…æå‡äº†æ´ªæ°´åæ°´åŸŸèŒƒå›´æ˜ å°„çš„å‡†ç¡®æ€§ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹å¯¹ç¼ºå¤±æ•°æ®çš„é²æ£’æ€§ã€‚è¿™é¡¹ç ”ç©¶ä¸ºå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ åœ¨ç°å®ä¸–ç•Œæ´ªæ°´ç®¡ç†åœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®è·å–ä¸å®Œå…¨æˆ–ä¸ç¨³å®šçš„å®é™…æ¡ä»¶ä¸‹å…·æœ‰é‡è¦å®è·µä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios.</p>
<h3 id="6-fcmbench-a-comprehensive-financial-credit-multimodal-benchmark-for-real-world-applications">[6] <a href="https://arxiv.org/abs/2601.00150">FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications</a></h3>
<p><em>Yehui Yang, Dalu Yang, Wenshuo Zhou, Fangxin Shang, Yifan Liu, Jie Ren, Haojun Fei, Qing Yang, Tao Chen</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FCMBench-V1.0â€”â€”ä¸€ä¸ªé’ˆå¯¹é‡‘èä¿¡è´·é¢†åŸŸçš„å¤§è§„æ¨¡å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–18ç§æ ¸å¿ƒè¯ä¹¦ç±»å‹ï¼ŒåŒ…å«4,043å¼ éšç§åˆè§„å›¾åƒå’Œ8,446ä¸ªé—®ç­”æ ·æœ¬ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é‡‘èä¿¡è´·åº”ç”¨ä¸­çš„å®é™…æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€å¤šæ¨¡æ€AIåœ¨ä¿¡è´·é£é™©è¯„ä¼°å’Œæ–‡æ¡£å®¡æŸ¥ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå½“å‰ç¼ºä¹ä¸€ä¸ªèƒ½å¤Ÿåæ˜ é‡‘èä¿¡è´·åº”ç”¨ç‰¹å®šæ–‡æ¡£å’Œå·¥ä½œæµç¨‹ã€åŒ…å«ä¿¡è´·ç‰¹å®šç†è§£å’ŒçœŸå®ä¸–ç•Œé²æ£’æ€§ã€åŒæ—¶ä¿æŒéšç§åˆè§„æ€§è€Œä¸ç‰ºç‰²å®ç”¨æ€§çš„é¢†åŸŸä¸“ç”¨åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†FCMBench-V1.0è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«æ„ŸçŸ¥ã€æ¨ç†å’Œé²æ£’æ€§ä¸‰ä¸ªç»´åº¦ï¼Œå…¶ä¸­æ„ŸçŸ¥ç»´åº¦åŒ…æ‹¬3ä¸ªåŸºç¡€æ„ŸçŸ¥ä»»åŠ¡ï¼Œæ¨ç†ç»´åº¦åŒ…æ‹¬4ä¸ªéœ€è¦è§†è§‰è¯æ®å†³ç­–ç†è§£çš„ä¿¡è´·ç‰¹å®šä»»åŠ¡ï¼Œé²æ£’æ€§ç»´åº¦åŒ…æ‹¬10ç§çœŸå®ä¸–ç•Œé‡‡é›†ä¼ªå½±ç±»å‹ã€‚æ‰€æœ‰æ ·æœ¬é€šè¿‡å°é—­çš„åˆæˆ-é‡‡é›†æµæ°´çº¿æ„å»ºï¼šæ‰‹åŠ¨åˆæˆå¸¦æœ‰è™šæ‹Ÿå†…å®¹çš„æ–‡æ¡£æ¨¡æ¿ï¼Œå¹¶åœ¨å†…éƒ¨é‡‡é›†åœºæ™¯æ„ŸçŸ¥å›¾åƒï¼Œé¿å…äº†ç½‘ç»œæ¥æºæˆ–å…¬å¼€å‘å¸ƒå›¾åƒï¼Œä»è€Œå‡è½»é¢„è®­ç»ƒæ•°æ®æ³„éœ²é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨14ä¸ªé¡¶çº§AIå…¬å¸å’Œç ”ç©¶æœºæ„çš„23ä¸ªæœ€å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œç»“æœæ˜¾ç¤ºGemini 3 Proä½œä¸ºå•†ä¸šæ¨¡å‹è·å¾—æœ€ä½³F1åˆ†æ•°ï¼ˆ64.61%ï¼‰ï¼ŒQwen3-VL-235Bä½œä¸ºå¼€æºåŸºçº¿è·å¾—æœ€ä½³åˆ†æ•°ï¼ˆ57.27%ï¼‰ï¼Œè€Œç ”ç©¶å›¢é˜Ÿæå‡ºçš„é‡‘èä¿¡è´·ä¸“ç”¨æ¨¡å‹Qfin-VL-Instructè·å¾—æœ€é«˜æ€»ä½“åˆ†æ•°ï¼ˆ64.92%ï¼‰ã€‚é²æ£’æ€§è¯„ä¼°è¡¨æ˜å³ä½¿è¡¨ç°æœ€ä½³çš„æ¨¡å‹åœ¨é‡‡é›†ä¼ªå½±ä¸‹ä¹Ÿä¼šå‡ºç°æ˜æ˜¾çš„æ€§èƒ½ä¸‹é™ã€‚</p>
<p><strong>Conclusion:</strong> FCMBenchèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å·®å¼‚å’Œé²æ£’æ€§ï¼Œä¸ºé‡‘èä¿¡è´·é¢†åŸŸçš„å¤šæ¨¡æ€AIè¯„ä¼°æä¾›äº†æ ‡å‡†åŒ–åŸºå‡†ã€‚è¯¥åŸºå‡†é€šè¿‡éšç§åˆè§„çš„åˆæˆ-é‡‡é›†æ–¹æ³•è§£å†³äº†æ•°æ®æ³„éœ²é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒäº†å®é™…åº”ç”¨çš„å®ç”¨æ€§ï¼Œä¸ºé¢†åŸŸä¸“ç”¨å¤šæ¨¡æ€æ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦è¯„ä¼°å·¥å…·ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.</p>
<h3 id="7-faithscan-model-driven-single-pass-hallucination-detection-for-faithful-visual-question-answering">[7] <a href="https://arxiv.org/abs/2601.00269">FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering</a></h3>
<p><em>Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºFaithSCANï¼Œä¸€ç§è½»é‡çº§ç½‘ç»œï¼Œé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸°å¯Œå†…éƒ¨ä¿¡å·æ¥æ£€æµ‹VQAä¸­çš„å¹»è§‰é—®é¢˜ï¼ŒåŒæ—¶æ‰©å±•äº†LLM-as-a-JudgeèŒƒå¼ä»¥è‡ªåŠ¨ç”Ÿæˆç›‘ç£ä¿¡å·ï¼Œæ— éœ€æ˜‚è´µçš„äººå·¥æ ‡æ³¨ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> VQAä¸­çš„å¿ å®æ€§å¹»è§‰é—®é¢˜ä¸¥é‡å‰Šå¼±äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„å¯é æ€§ï¼Œç°æœ‰æ£€æµ‹æ–¹æ³•å­˜åœ¨å›ºæœ‰å±€é™æ€§ï¼šå¤–éƒ¨éªŒè¯æ–¹æ³•è®¡ç®—å¼€é”€å¤§ä¸”ä¾èµ–å¤–éƒ¨èµ„æºè´¨é‡ï¼Œä¸ç¡®å®šæ€§é©±åŠ¨æ–¹æ³•ä»…æ•æ‰æ¨¡å‹ä¸ç¡®å®šæ€§çš„æœ‰é™æ–¹é¢ä¸”æœªèƒ½å……åˆ†æ¢ç´¢ä¸å¤šæ ·åŒ–å¤±è´¥æ¨¡å¼ç›¸å…³çš„ä¸°å¯Œå†…éƒ¨ä¿¡å·ã€‚</p>
<p><strong>Method:</strong> æå‡ºFaithSCANè½»é‡çº§ç½‘ç»œï¼Œé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸°å¯Œå†…éƒ¨ä¿¡å·æ£€æµ‹å¹»è§‰ï¼ŒåŒ…æ‹¬ä»¤ç‰Œçº§è§£ç ä¸ç¡®å®šæ€§ã€ä¸­é—´è§†è§‰è¡¨ç¤ºå’Œè·¨æ¨¡æ€å¯¹é½ç‰¹å¾ï¼Œè¿™äº›ä¿¡å·é€šè¿‡åˆ†æ”¯è¯æ®ç¼–ç å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥æ³¨æ„åŠ›è¿›è¡Œèåˆï¼ŒåŒæ—¶æ‰©å±•LLM-as-a-JudgeèŒƒå¼åˆ°VQAå¹»è§‰æ£€æµ‹ï¼Œæå‡ºä½æˆæœ¬ç­–ç•¥è‡ªåŠ¨ç”Ÿæˆæ¨¡å‹ä¾èµ–çš„ç›‘ç£ä¿¡å·ï¼Œå®ç°æ— éœ€æ˜‚è´µäººå·¥æ ‡æ³¨çš„ç›‘ç£è®­ç»ƒã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªVQAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFaithSCANåœ¨æ•ˆæœå’Œæ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ·±å…¥åˆ†ææ˜¾ç¤ºå¹»è§‰æºäºè§†è§‰æ„ŸçŸ¥ã€è·¨æ¨¡æ€æ¨ç†å’Œè¯­è¨€è§£ç ä¸­ç³»ç»Ÿæ€§çš„å†…éƒ¨çŠ¶æ€å˜åŒ–ï¼Œä¸åŒå†…éƒ¨ä¿¡å·æä¾›äº’è¡¥çš„è¯Šæ–­çº¿ç´¢ï¼Œä¸”å¹»è§‰æ¨¡å¼åœ¨ä¸åŒVLMæ¶æ„é—´å­˜åœ¨å·®å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†å¹»è§‰æºäºè§†è§‰è¯­è¨€æ¨¡å‹å†…éƒ¨çŠ¶æ€çš„ç³»ç»Ÿæ€§å˜åŒ–ï¼Œä¸åŒå†…éƒ¨ä¿¡å·æä¾›äº’è¡¥çš„è¯Šæ–­ä»·å€¼ï¼Œå¹»è§‰æ¨¡å¼å…·æœ‰æ¶æ„ä¾èµ–æ€§ï¼Œè¿™ä¸ºç†è§£å¤šæ¨¡æ€å¹»è§‰çš„åº•å±‚æœºåˆ¶æä¾›äº†æ–°è§è§£ï¼ŒåŒæ—¶æå‡ºçš„è‡ªåŠ¨ç›‘ç£ä¿¡å·ç”Ÿæˆæ–¹æ³•ä¸ºä½æˆæœ¬å¹»è§‰æ£€æµ‹è®­ç»ƒæä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.</p>
<h3 id="8-focal-regionface-generating-fine-grained-multi-attribute-descriptions-for-arbitrarily-selected-face-focal-regions">[8] <a href="https://arxiv.org/abs/2601.00156">Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions</a></h3>
<p><em>Kaiwen Zheng, Junchen Fu, Songpei Xu, Yaoqing He, Joemon M. Jose, Han Hu, Xuri Ge</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FaceFocalDescè¿™ä¸€æ–°é—®é¢˜ï¼Œæ—¨åœ¨ä¸ºä»»æ„é€‰å®šçš„äººè„¸åŒºåŸŸç”Ÿæˆå’Œè¯†åˆ«åŒ…å«é¢éƒ¨åŠ¨ä½œå•å…ƒã€æƒ…æ„ŸçŠ¶æ€å’Œå¹´é¾„ä¼°è®¡çš„å¤šå±æ€§è‡ªç„¶è¯­è¨€æè¿°ï¼Œå¹¶åŸºäºQwen2.5-VLæ„å»ºäº†Focal-RegionFaceæ¨¡å‹ï¼Œé€šè¿‡æ¸è¿›å¼å¾®è°ƒå®ç°ç»†ç²’åº¦åŒºåŸŸèšç„¦åˆ†æã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬æ–‡æ—¨åœ¨è§£å†³é¢éƒ¨åˆ†æä¸­ä¸€ä¸ªå°šæœªå……åˆ†æ¢ç´¢çš„é—®é¢˜ï¼šä¸ºä»»æ„é€‰å®šçš„äººè„¸åŒºåŸŸç”Ÿæˆå’Œè¯†åˆ«åŒ…å«é¢éƒ¨åŠ¨ä½œå•å…ƒã€æƒ…æ„ŸçŠ¶æ€å’Œå¹´é¾„ä¼°è®¡çš„å¤šå±æ€§è‡ªç„¶è¯­è¨€æè¿°ã€‚ç ”ç©¶è€…è®¤ä¸ºç³»ç»Ÿå¯¹ä¸ªä½“é¢éƒ¨åŒºåŸŸçš„èšç„¦èƒ½åŠ›èƒ½å¸¦æ¥æ›´å¥½çš„ç†è§£å’Œæ§åˆ¶ï¼Œä¸ºæ­¤éœ€è¦æ„å»ºæ–°çš„æ•°æ®é›†å¹¶å¼€å‘ç›¸åº”çš„åˆ†ææ¨¡å‹ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªæ–°çš„å¤šå±æ€§æè¿°æ•°æ®é›†ï¼Œä¸ºä»»æ„é€‰å®šçš„äººè„¸åŒºåŸŸæä¾›ä¸°å¯Œçš„åŒºåŸŸçº§æ ‡æ³¨å’Œè‡ªç„¶è¯­è¨€æè¿°ã€‚åŸºäºQwen2.5-VLè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæå‡ºäº†Focal-RegionFaceæ¨¡å‹ï¼Œé€šè¿‡å¤šä¸ªæ¸è¿›å¼å¾®è°ƒé˜¶æ®µé€æ­¥ç»†åŒ–å¯¹å±€éƒ¨é¢éƒ¨ç‰¹å¾çš„å…³æ³¨ï¼Œå®ç°å¯è§£é‡Šçš„å¹´é¾„ä¼°è®¡ã€é¢éƒ¨åŠ¨ä½œå•å…ƒå’Œæƒ…æ„Ÿæ£€æµ‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒFocal-RegionFaceåœ¨æ–°åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œæ— è®ºæ˜¯åœ¨ä¼ ç»Ÿå’Œå¹¿æ³›ä½¿ç”¨çš„æŒ‡æ ‡ä¸Šï¼Œè¿˜æ˜¯åœ¨æ–°æå‡ºçš„æŒ‡æ ‡ä¸Šã€‚è¿™å……åˆ†éªŒè¯äº†è¯¥æ¨¡å‹åœ¨ç»†ç²’åº¦å¤šå±æ€§äººè„¸åŒºåŸŸèšç„¦åˆ†æåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œå¤šåŠŸèƒ½æ€§ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶å±•ç¤ºäº†åŒºåŸŸèšç„¦æ–¹æ³•åœ¨é¢éƒ¨åˆ†æä¸­çš„é‡è¦æ€§ï¼Œæå‡ºçš„Focal-RegionFaceæ¨¡å‹ä¸ºç»†ç²’åº¦å¤šå±æ€§é¢éƒ¨çŠ¶æ€åˆ†ææä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚è¯¥å·¥ä½œä¸ºé¢éƒ¨ç†è§£çš„å¯è§£é‡Šæ€§å’Œæ§åˆ¶æ€§å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œæœªæ¥å¯æ‰©å±•åˆ°æ›´å¹¿æ³›çš„é¢éƒ¨åˆ†æä»»åŠ¡å’Œå®é™…åº”ç”¨åœºæ™¯ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.</p>
<h3 id="9-harmoniad-harmonizing-local-structures-and-global-semantics-for-anomaly-detection">[9] <a href="https://arxiv.org/abs/2601.00327">HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection</a></h3>
<p><em>Naiqi Zhang, Chuancheng Shi, Jingtong Dou, Wenhua Wu, Fei Shen, Jianhua Cao</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºHarmoniADï¼Œä¸€ç§é¢‘ç‡å¼•å¯¼çš„åŒåˆ†æ”¯å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡è§£è€¦é«˜é¢‘å’Œä½é¢‘è·¯å¾„æ¥å¹³è¡¡ç»“æ„ç»†èŠ‚ä¸è¯­ä¹‰ä¸€è‡´æ€§ï¼Œåœ¨å¤šä¸ªå·¥ä¸šç¼ºé™·æ£€æµ‹åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å·¥ä¸šäº§å“è´¨é‡æ£€æµ‹ä¸­çš„å¼‚å¸¸æ£€æµ‹è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´ç»“æ„-è¯­ä¹‰æƒè¡¡é—®é¢˜ï¼šç»“æ„å¯¼å‘æ¨¡å‹å¯¹å™ªå£°æ•æ„Ÿï¼Œè€Œè¯­ä¹‰å¯¼å‘æ¨¡å‹å¾€å¾€å¿½ç•¥ç²¾ç»†ç»†èŠ‚ï¼Œå¯¼è‡´å¾®å°ç¼ºé™·æ£€æµ‹å›°éš¾ã€‚</p>
<p><strong>Method:</strong> HarmoniADé‡‡ç”¨é¢‘ç‡å¼•å¯¼çš„åŒåˆ†æ”¯æ¡†æ¶ï¼Œé¦–å…ˆé€šè¿‡CLIPå›¾åƒç¼–ç å™¨æå–ç‰¹å¾ï¼Œç„¶åè½¬æ¢åˆ°é¢‘åŸŸå¹¶è§£è€¦ä¸ºé«˜é¢‘å’Œä½é¢‘è·¯å¾„ã€‚é«˜é¢‘åˆ†æ”¯é…å¤‡ç»†ç²’åº¦ç»“æ„æ³¨æ„åŠ›æ¨¡å—å¢å¼ºçº¹ç†å’Œè¾¹ç¼˜æ£€æµ‹ï¼Œä½é¢‘åˆ†æ”¯ä½¿ç”¨å…¨å±€ç»“æ„ä¸Šä¸‹æ–‡æ¨¡å—æ•è·é•¿ç¨‹ä¾èµ–å¹¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ï¼Œä¸¤è€…äº’è¡¥å»ºæ¨¡ç»“æ„ä¸è¯­ä¹‰ã€‚</p>
<p><strong>Result:</strong> åœ¨MVTec-ADã€VisAå’ŒBTADä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHarmoniADå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å…·å¤‡é«˜æ•æ„Ÿæ€§å’Œé²æ£’æ€§ï¼Œæœ‰æ•ˆå¹³è¡¡äº†ç²¾ç»†ç»†èŠ‚æ£€æµ‹ä¸å…¨å±€è¯­ä¹‰ä¿æŒã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é¢‘ç‡åŸŸè§£è€¦ç­–ç•¥åœ¨å¼‚å¸¸æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡åŒåˆ†æ”¯äº’è¡¥å»ºæ¨¡æˆåŠŸè§£å†³äº†ç»“æ„-è¯­ä¹‰æƒè¡¡é—®é¢˜ï¼Œä¸ºå·¥ä¸šç¼ºé™·æ£€æµ‹æä¾›äº†å…¼é¡¾æ•æ„Ÿæ€§ä¸é²æ£’æ€§çš„æ–°æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å¤šç±»è”åˆè®­ç»ƒç­–ç•¥çš„å®ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD, a frequency-guided dual-branch framework. Features are first extracted by the CLIP image encoder, then transformed into the frequency domain, and finally decoupled into high- and low-frequency paths for complementary modeling of structure and semantics. The high-frequency branch is equipped with a fine-grained structural attention module (FSAM) to enhance textures and edges for detecting small anomalies, while the low-frequency branch uses a global structural context module (GSCM) to capture long-range dependencies and preserve semantic consistency. Together, these branches balance fine detail and global semantics. HarmoniAD further adopts a multi-class joint training strategy, and experiments on MVTec-AD, VisA, and BTAD show state-of-the-art performance with both sensitivity and robustness.</p>
<h3 id="10-intrastyler-exemplar-based-style-synthesis-for-cross-modality-domain-adaptation">[10] <a href="https://arxiv.org/abs/2601.00212">IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation</a></h3>
<p><em>Han Liu, Yubo Fan, Hao Li, Dewei Hu, Daniel Moyer, Zhoubing Xu, Benoit M. Dawant, Ipek Oguz</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºIntraStylerï¼Œä¸€ç§åŸºäºç¤ºä¾‹çš„é£æ ¼åˆæˆæ–¹æ³•ï¼Œç”¨äºæ— ç›‘ç£åŸŸé€‚åº”ä¸­çš„å¯æ§é£æ ¼å¤šæ ·åŒ–ï¼Œæ— éœ€å…ˆéªŒçŸ¥è¯†å³å¯æ•è·å¤šæ ·çš„åŸŸå†…é£æ ¼ï¼Œä»è€Œå¢å¼ºä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ— ç›‘ç£åŸŸé€‚åº”æ–¹æ³•ä¸»è¦å…³æ³¨æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„åŸŸåç§»ï¼Œè€ŒåŸŸå†…å˜å¼‚æ€§ç ”ç©¶ä¸è¶³ã€‚ä¼ ç»Ÿæ–¹æ³•éœ€è¦é¢„å…ˆæŒ‡å®šåŸŸå†…å˜åŒ–è¿›è¡Œé£æ ¼åˆæˆï¼Œè¿™åœ¨å®è·µä¸­å¾€å¾€ä¸åˆ‡å®é™…ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ— éœ€å…ˆéªŒçŸ¥è¯†å³å¯æ•è·å¤šæ ·åŸŸå†…é£æ ¼çš„æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æå‡ºIntraStyleræ–¹æ³•ï¼Œé‡‡ç”¨åŸºäºç¤ºä¾‹çš„é£æ ¼åˆæˆç­–ç•¥ï¼Œä½¿ç”¨ç¤ºä¾‹å›¾åƒå¼•å¯¼é£æ ¼åˆæˆä»¥ç¡®ä¿è¾“å‡ºé£æ ¼ä¸ç¤ºä¾‹é£æ ¼åŒ¹é…ã€‚å¼•å…¥é£æ ¼ç¼–ç å™¨ï¼ŒåŸºäºå¯¹æ¯”å­¦ä¹ ä»¥åˆ¤åˆ«æ–¹å¼å­¦ä¹ é£æ ¼ç‰¹å¾ï¼Œä»è€Œæå–çº¯é£æ ¼ç‰¹å¾ï¼Œå®ç°å¯æ§çš„é£æ ¼åˆæˆã€‚</p>
<p><strong>Result:</strong> åœ¨è·¨æ¨¡æ€åŸŸé€‚åº”æœ€å¤§å…¬å¼€æ•°æ®é›†CrossMoDA 2023ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¯æ§é£æ ¼åˆæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠå¤šæ ·åŒ–åˆæˆæ•°æ®å¯¹ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡çš„ç›Šå¤„ï¼Œå±•ç¤ºäº†å…¶å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†åŸŸå†…é£æ ¼å¤šæ ·åŒ–åœ¨æ— ç›‘ç£åŸŸé€‚åº”ä¸­çš„é‡è¦æ€§ï¼Œæå‡ºçš„IntraStyleræ–¹æ³•æ— éœ€å…ˆéªŒçŸ¥è¯†å³å¯å®ç°å¯æ§é£æ ¼åˆæˆï¼Œä¸ºè·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆ†æç­‰å®é™…åº”ç”¨æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†åŸŸé€‚åº”æ–¹æ³•å‘æ›´ç»†ç²’åº¦é£æ ¼æ§åˆ¶çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.</p>
<h3 id="11-motionphysics-learnable-motion-distillation-for-text-guided-simulation">[11] <a href="https://arxiv.org/abs/2601.00504">MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation</a></h3>
<p><em>Miaowei Wang, Jakub ZadroÅ¼ny, Oisin Mac Aodha, Amir Vaxman</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MotionPhysicsï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„å¯å¾®åˆ†æ¡†æ¶ï¼Œèƒ½å¤Ÿä»è‡ªç„¶è¯­è¨€æç¤ºä¸­æ¨æ–­å‡º3Dåœºæ™¯çš„åˆç†ç‰©ç†å‚æ•°ï¼Œæ— éœ€çœŸå®è½¨è¿¹æˆ–æ ‡æ³¨è§†é¢‘çš„æŒ‡å¯¼ï¼Œå®ç°äº†åŸºäºæ–‡æœ¬çš„é€¼çœŸåŠ¨æ€æ¨¡æ‹Ÿã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å‡†ç¡®æ¨¡æ‹Ÿç°æœ‰3Dç‰©ä½“å’Œå¤šç§ææ–™é€šå¸¸éœ€è¦ä¸“å®¶çŸ¥è¯†å’Œè€—æ—¶çš„ç‰©ç†å‚æ•°è°ƒæ•´æ‰èƒ½å®ç°æœŸæœ›çš„åŠ¨æ€è¡Œä¸ºï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºçœŸå®è½¨è¿¹æˆ–æ ‡æ³¨è§†é¢‘çš„æŒ‡å¯¼ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§å’Œæ˜“ç”¨æ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¼°è®¡ææ–™å‚æ•°å€¼ï¼Œå¹¶çº¦æŸåœ¨åˆç†èŒƒå›´å†…ï¼›è¿›ä¸€æ­¥æå‡ºå¯å­¦ä¹ çš„è¿åŠ¨è’¸é¦æŸå¤±ï¼Œä»é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­æå–é²æ£’çš„è¿åŠ¨å…ˆéªŒï¼ŒåŒæ—¶æœ€å°åŒ–å¤–è§‚å’Œå‡ ä½•å½’çº³åç½®ä»¥æŒ‡å¯¼æ¨¡æ‹Ÿè¿‡ç¨‹ã€‚</p>
<p><strong>Result:</strong> MotionPhysicsåœ¨è¶…è¿‡ä¸‰åä¸ªåœºæ™¯ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬çœŸå®ä¸–ç•Œã€äººå·¥è®¾è®¡å’ŒAIç”Ÿæˆçš„3Dç‰©ä½“ï¼Œæ¶µç›–å¼¹æ€§å›ºä½“ã€é‡‘å±ã€æ³¡æ²«ã€æ²™å­ä»¥åŠç‰›é¡¿å’Œéç‰›é¡¿æµä½“ç­‰å¤šç§ææ–™ï¼Œåœ¨è§†è§‰é€¼çœŸåº¦å’Œç‰©ç†åˆç†æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†åŸºäºè‡ªç„¶è¯­è¨€çš„ç‰©ç†å‚æ•°æ¨æ–­æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œä¸º3DåŠ¨æ€æ¨¡æ‹Ÿæä¾›äº†æ›´æ˜“ç”¨å’Œè‡ªåŠ¨åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†ç‰©ç†æ¨¡æ‹Ÿä¸ç”Ÿæˆæ¨¡å‹çš„ç»“åˆï¼Œä¸ºåˆ›æ„è®¾è®¡å’Œè™šæ‹Ÿå†…å®¹åˆ›ä½œå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.</p>
<h3 id="12-a-comprehensive-dataset-for-human-vs-ai-generated-image-detection">[12] <a href="https://arxiv.org/abs/2601.00553">A Comprehensive Dataset for Human vs. AI Generated Image Detection</a></h3>
<p><em>Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Vasu Sharma, Vinija Jain, Aman Chadha, Aishwarya Naresh Reganti, Amitava Das</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶å‘å¸ƒäº†MS COCOAIæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«96000ä¸ªçœŸå®ä¸åˆæˆå›¾åƒå¯¹çš„æ–°å‹æ•°æ®é›†ï¼Œç”¨äºAIç”Ÿæˆå›¾åƒæ£€æµ‹ï¼Œå¹¶æå‡ºäº†ä¸¤ä¸ªä»»åŠ¡ï¼šå›¾åƒçœŸå®æ€§åˆ†ç±»å’Œç”Ÿæˆæ¨¡å‹æº¯æºã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€Stable Diffusionã€DALL-Eå’ŒMidJourneyç­‰å¤šæ¨¡æ€ç”ŸæˆAIç³»ç»Ÿçš„æ™®åŠï¼Œåˆæˆå›¾åƒè¶Šæ¥è¶Šéš¾ä»¥ä¸çœŸå®ç…§ç‰‡åŒºåˆ†ï¼Œå¯¼è‡´è™šå‡ä¿¡æ¯å’Œè¯¯å¯¼æ€§å†…å®¹ä¼ æ’­ï¼Œå› æ­¤å¼€å‘æœ‰æ•ˆçš„æ£€æµ‹æ–¹æ³•æˆä¸ºç´§è¿«éœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶åŸºäºMS COCOæ•°æ®é›†æ„å»ºäº†åŒ…å«96000ä¸ªæ•°æ®ç‚¹çš„MS COCOAIæ•°æ®é›†ï¼Œä½¿ç”¨äº”ç§ç”Ÿæˆå™¨ï¼ˆStable Diffusion 3ã€Stable Diffusion 2.1ã€SDXLã€DALL-E 3å’ŒMidJourney v6ï¼‰ç”Ÿæˆåˆæˆå›¾åƒï¼Œå¹¶è®¾è®¡äº†ä¸¤ä¸ªæ£€æµ‹ä»»åŠ¡ï¼šå›¾åƒçœŸå®æ€§åˆ†ç±»å’Œç”Ÿæˆæ¨¡å‹æº¯æºã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªäº”ç§ä¸åŒç”Ÿæˆæ¨¡å‹çš„åˆæˆå›¾åƒï¼Œä¸ºå¼€å‘æ›´é²æ£’çš„æ£€æµ‹ç®—æ³•æä¾›äº†æ ‡å‡†åŒ–è¯„ä¼°å¹³å°ï¼Œæ•°æ®é›†å·²åœ¨Hugging Faceå¹³å°å…¬å¼€ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡æ„å»ºå…¨é¢çš„æ•°æ®é›†å’Œå®šä¹‰æ˜ç¡®çš„æ£€æµ‹ä»»åŠ¡ï¼Œä¸ºAIç”Ÿæˆå›¾åƒæ£€æµ‹é¢†åŸŸæä¾›äº†é‡è¦çš„åŸºå‡†èµ„æºï¼Œæœ‰åŠ©äºå¼€å‘æ›´æœ‰æ•ˆçš„æ£€æµ‹æ–¹æ³•ä»¥åº”å¯¹åˆæˆåª’ä½“å¸¦æ¥çš„å®‰å…¨æŒ‘æˆ˜ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.</p>
<h3 id="13-application-research-of-a-deep-learning-model-integrating-cyclegan-and-yolo-in-pcb-infrared-defect-detection">[13] <a href="https://arxiv.org/abs/2601.00237">Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection</a></h3>
<p><em>Chao Yang, Haoyuan Zheng, Yue Ma</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é›†æˆCycleGANå’ŒYOLOv8çš„è·¨æ¨¡æ€æ•°æ®å¢å¼ºæ¡†æ¶ï¼Œé€šè¿‡å°†ä¸°å¯Œçš„å¯è§å…‰PCBå›¾åƒè½¬æ¢ä¸ºçº¢å¤–åŸŸæ¥ç¼“è§£çº¢å¤–æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†PCBç¼ºé™·æ£€æµ‹åœ¨ä½æ•°æ®æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å°åˆ·ç”µè·¯æ¿ç¼ºé™·æ£€æµ‹ä¸­çº¢å¤–æ•°æ®ç¨€ç¼ºçš„å…³é”®ç“¶é¢ˆé—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–é…å¯¹ç›‘ç£æ•°æ®ï¼Œè€ŒçœŸå®çº¢å¤–æ ·æœ¬è·å–æˆæœ¬é«˜æ˜‚ä¸”æ•°é‡æœ‰é™ï¼Œè¿™ä¸¥é‡åˆ¶çº¦äº†åŸºäºæ·±åº¦å­¦ä¹ çš„ç¼ºé™·æ£€æµ‹æ¨¡å‹çš„è®­ç»ƒæ•ˆæœå’Œå®é™…åº”ç”¨éƒ¨ç½²ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨è·¨æ¨¡æ€æ•°æ®å¢å¼ºæ¡†æ¶ï¼Œé¦–å…ˆåˆ©ç”¨CycleGANè¿›è¡Œæ— é…å¯¹å›¾åƒåˆ°å›¾åƒè½¬æ¢ï¼Œå°†ä¸°å¯Œçš„å¯è§å…‰PCBå›¾åƒæ˜ å°„åˆ°çº¢å¤–åŸŸï¼Œç”Ÿæˆä¿ç•™ç¼ºé™·ç»“æ„è¯­ä¹‰å¹¶å‡†ç¡®æ¨¡æ‹Ÿçƒ­åˆ†å¸ƒæ¨¡å¼çš„é«˜ä¿çœŸä¼ªçº¢å¤–æ ·æœ¬ï¼›éšåæ„å»ºå¼‚æ„è®­ç»ƒç­–ç•¥ï¼Œèåˆç”Ÿæˆçš„ä¼ªçº¢å¤–æ•°æ®ä¸æœ‰é™çœŸå®çº¢å¤–æ ·æœ¬ï¼Œå…±åŒè®­ç»ƒè½»é‡çº§YOLOv8æ£€æµ‹å™¨ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä½æ•°æ®æ¡ä»¶ä¸‹æœ‰æ•ˆå¢å¼ºäº†ç‰¹å¾å­¦ä¹ èƒ½åŠ›ï¼Œå¢å¼ºåçš„æ£€æµ‹å™¨æ€§èƒ½æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨æœ‰é™çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶ä¸”æ¥è¿‘å®Œå…¨ç›‘ç£è®­ç»ƒçš„åŸºå‡†æ€§èƒ½ï¼Œè¯æ˜äº†ä¼ªçº¢å¤–åˆæˆä½œä¸ºå·¥ä¸šæ£€æµ‹ç¨³å¥å¢å¼ºç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶è¯æ˜äº†è·¨æ¨¡æ€æ•°æ®å¢å¼ºåœ¨ç¼“è§£å·¥ä¸šæ£€æµ‹æ•°æ®ç¨€ç¼ºé—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡æ— ç›‘ç£å›¾åƒè½¬æ¢ç”Ÿæˆé«˜è´¨é‡ä¼ªçº¢å¤–æ•°æ®èƒ½å¤Ÿæ˜¾è‘—æå‡ç¼ºé™·æ£€æµ‹æ€§èƒ½ï¼Œä¸ºå®é™…å·¥ä¸šåº”ç”¨ä¸­æ•°æ®è·å–å›°éš¾çš„åœºæ™¯æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†ç”Ÿæˆæ¨¡å‹ä¸ä¼ ç»Ÿæ£€æµ‹æ¡†æ¶ç»“åˆçš„å®é™…ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.</p>
<h3 id="14-detecting-performance-degradation-under-data-shift-in-pathology-vision-language-model">[14] <a href="https://arxiv.org/abs/2601.00716">Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model</a></h3>
<p><em>Hao Guan, Li Zhou</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆè¾“å…¥æ•°æ®åç§»æ£€æµ‹å’Œè¾“å‡ºç½®ä¿¡åº¦æŒ‡æ ‡çš„äº’è¡¥æ¡†æ¶ï¼Œç”¨äºç›‘æµ‹ç—…ç†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ•°æ®åˆ†å¸ƒå˜åŒ–ä¸‹çš„æ€§èƒ½é€€åŒ–ï¼Œå¹¶é€šè¿‡DomainSATå·¥å…·ç®±å’Œç½®ä¿¡åº¦æŒ‡æ ‡å®ç°äº†æ›´å¯é çš„æ¨¡å‹å¯é æ€§ç›‘æ§ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†æä¸­è¡¨ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼Œä½†éƒ¨ç½²åå½“è¾“å…¥æ•°æ®åˆ†å¸ƒåç¦»å¼€å‘æ—¶è§‚å¯Ÿåˆ°çš„åˆ†å¸ƒæ—¶ï¼Œå…¶æ€§èƒ½å¯èƒ½é€€åŒ–ï¼Œè€Œæ£€æµ‹è¿™ç§æ€§èƒ½é€€åŒ–å¯¹äºä¸´åºŠå¯é æ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„å¤§å‹é¢„è®­ç»ƒVLMä¸­å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶åŒæ—¶è€ƒå¯Ÿè¾“å…¥çº§æ•°æ®åç§»å’Œè¾“å‡ºçº§é¢„æµ‹è¡Œä¸ºï¼Œå¼€å‘äº†DomainSATè½»é‡çº§å·¥å…·ç®±é›†æˆä»£è¡¨æ€§åç§»æ£€æµ‹ç®—æ³•å¹¶æä¾›å›¾å½¢ç•Œé¢ï¼ŒåŒæ—¶å¼•å…¥åŸºäºç½®ä¿¡åº¦çš„æ— æ ‡ç­¾æ€§èƒ½é€€åŒ–æŒ‡æ ‡ç›´æ¥æ•æ‰æ¨¡å‹é¢„æµ‹ç½®ä¿¡åº¦çš„å˜åŒ–ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è¾“å…¥æ•°æ®åç§»æ£€æµ‹èƒ½æœ‰æ•ˆè¯†åˆ«åˆ†å¸ƒå˜åŒ–å¹¶æä¾›æ—©æœŸè¯Šæ–­ä¿¡å·ï¼Œä½†å¹¶ä¸æ€»æ˜¯å¯¹åº”å®é™…æ€§èƒ½é€€åŒ–ï¼›è€ŒåŸºäºè¾“å‡ºçš„ç½®ä¿¡åº¦æŒ‡æ ‡ä¸æ€§èƒ½é€€åŒ–å¯†åˆ‡ç›¸å…³ï¼Œå¯ä½œä¸ºè¾“å…¥åç§»æ£€æµ‹çš„æœ‰æ•ˆè¡¥å……ï¼›åœ¨å¤§å‹ç—…ç†æ•°æ®é›†ä¸Šçš„è‚¿ç˜¤åˆ†ç±»å®éªŒè¯æ˜ï¼Œç»“åˆä¸¤ç§æ–¹æ³•èƒ½æ›´å¯é åœ°æ£€æµ‹å’Œè§£é‡Šæ•°æ®åç§»ä¸‹çš„VLMæ€§èƒ½é€€åŒ–ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºæ•°å­—ç—…ç†å­¦ä¸­åŸºç¡€æ¨¡å‹çš„å¯é æ€§ç›‘æ§æä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”äº’è¡¥çš„æ¡†æ¶ï¼Œè¾“å…¥æ•°æ®åç§»æ£€æµ‹å’Œè¾“å‡ºç½®ä¿¡åº¦æŒ‡æ ‡çš„ç»“åˆä½¿ç”¨èƒ½å¤Ÿæ›´å…¨é¢åœ°ç›‘æµ‹æ¨¡å‹åœ¨åˆ†å¸ƒå˜åŒ–ä¸‹çš„æ€§èƒ½å˜åŒ–ï¼Œä¸ºä¸´åºŠéƒ¨ç½²çš„VLMæä¾›äº†é‡è¦çš„å¯é æ€§ä¿éšœæœºåˆ¶ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.</p>
<h3 id="15-totalfm-an-organ-separated-framework-for-3d-ct-vision-foundation-models">[15] <a href="https://arxiv.org/abs/2601.00260">TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models</a></h3>
<p><em>Kohei Yamamoto, Tomohiro Kikuchi</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†TotalFMï¼Œä¸€ç§åŸºäºå™¨å®˜åˆ†ç¦»æ¦‚å¿µçš„æ”¾å°„å­¦åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆè‡ªç›‘ç£é¢„è®­ç»ƒå’Œå¯¹æ¯”å­¦ä¹ ï¼Œåœ¨3D-CTå½±åƒä¸è¯­è¨€è¡¨è¾¾ä¹‹é—´å»ºç«‹é«˜æ•ˆå¯¹åº”ï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬ç—…å˜åˆ†ç±»æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ”¾å°„å­¦åŸºç¡€æ¨¡å‹åœ¨åº”ç”¨äº3D-CTå®¹ç§¯æ•°æ®æ—¶é¢ä¸´è®¡ç®—æˆæœ¬çº¦æŸçš„é‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡ä¸è¡¨ç¤ºèƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™ï¼Œéœ€è¦ä¸€ç§æ›´å®ç”¨çš„è®¾è®¡æ¡†æ¶æ¥å®ç°3D-CTåŸºç¡€æ¨¡å‹çš„å®é™…éƒ¨ç½²ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŸºäºå™¨å®˜åˆ†ç¦»æ¦‚å¿µçš„TotalFMæ¨¡å‹ï¼Œåˆ©ç”¨14ä¸‡åºåˆ—çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œé€šè¿‡åˆ†å‰²æŠ€æœ¯å’ŒåŸºäºLLMçš„æ”¾å°„å­¦æŠ¥å‘Šå¤„ç†è‡ªåŠ¨åŒ–åˆ›å»ºå™¨å®˜ä½“ç§¯ä¸å‘ç°è¯­å¥å¯¹ï¼Œç»“åˆVideoMAEçš„è‡ªç›‘ç£é¢„è®­ç»ƒå’Œä½“ç§¯-æ–‡æœ¬å¯¹çš„å¯¹æ¯”å­¦ä¹ æ¥å¹³è¡¡è®¡ç®—æ•ˆç‡ä¸è¡¨ç¤ºèƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨é›¶æ ·æœ¬å™¨å®˜ç—…å˜åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹åœ¨83%çš„å™¨å®˜ä¸Šæ¯”CT-CLIPè·å¾—æ›´é«˜F1åˆ†æ•°ï¼Œåœ¨64%çš„å™¨å®˜ä¸Šä¼˜äºMerlinï¼›åœ¨é›¶æ ·æœ¬å‘ç°ç—…å˜åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œåœ¨83%çš„å‘ç°ç±»åˆ«ä¸ŠAUROCé«˜äºMerlinï¼›åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­æ€§èƒ½ä¸ç°æœ‰VLMç›¸å½“ã€‚</p>
<p><strong>Conclusion:</strong> å™¨å®˜åˆ†ç¦»å­¦ä¹ æ¡†æ¶ä¸º3D-CTåŸºç¡€æ¨¡å‹çš„å®é™…å®æ–½æä¾›äº†ç°å®æœ‰æ•ˆçš„è®¾è®¡æŒ‡å—ï¼Œè¯æ˜äº†è¯¥æ¨¡å‹åœ¨ä¸´åºŠè¯„ä¼°ç¯å¢ƒä¸­å…·æœ‰é«˜æ³›åŒ–æ€§èƒ½ï¼Œèƒ½å¤Ÿå¹³è¡¡è®¡ç®—æ•ˆç‡ä¸è¡¨ç¤ºèƒ½åŠ›ï¼Œä¸ºæ”¾å°„å­¦åŸºç¡€æ¨¡å‹çš„ä¸´åºŠåº”ç”¨æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.</p>
<h3 id="16-s1-mmalign-a-large-scale-multi-disciplinary-dataset-for-scientific-figure-text-understanding">[16] <a href="https://arxiv.org/abs/2601.00264">S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding</a></h3>
<p><em>He Wang, Longteng Guo, Pengkang Huo, Xuanxu Lin, Yichen Yuan, Jie Jiang, Jing Liu</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†S1-MMAlignï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡1550ä¸‡é«˜è´¨é‡å›¾åƒ-æ–‡æœ¬å¯¹çš„å¤§è§„æ¨¡å¤šå­¦ç§‘å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç§‘å­¦å‘ç°ä¸­å¤æ‚ç§‘å­¦å›¾åƒä¸ç¨€ç–æ–‡æœ¬æè¿°ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿé—®é¢˜ã€‚è¯¥ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªåŸºäºQwen-VLçš„è¯­ä¹‰å¢å¼ºæµç¨‹ï¼Œæ˜¾è‘—æå‡äº†ç§‘å­¦å›¾åƒ-æ–‡æœ¬å¯¹é½çš„è´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å­¦ä¹ è™½ç„¶åœ¨é€šç”¨é¢†åŸŸä»»åŠ¡ä¸­å–å¾—äº†é©å‘½æ€§è¿›å±•ï¼Œä½†åœ¨ç§‘å­¦å‘ç°ä¸­çš„åº”ç”¨å—åˆ°å¤æ‚ç§‘å­¦å›¾åƒä¸ç¨€ç–æ–‡æœ¬æè¿°ä¹‹é—´æ·±åˆ»è¯­ä¹‰é¸¿æ²Ÿçš„é˜»ç¢ã€‚ç°æœ‰ç§‘å­¦æ•°æ®é›†æ™®éå­˜åœ¨å›¾åƒä¸æ–‡æœ¬å¼±å¯¹é½çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†è·¨æ¨¡æ€ç†è§£å’Œç§‘å­¦æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚è¯¥ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä¸ºAI for Scienceæ—¶ä»£æä¾›é«˜è´¨é‡çš„å¤šæ¨¡æ€åŸºç¡€èµ„æºã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†S1-MMAlignæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª250ä¸‡ç¯‡å¼€æ”¾è·å–ç§‘å­¦è®ºæ–‡çš„è¶…è¿‡1550ä¸‡é«˜è´¨é‡å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œæ¶µç›–ç‰©ç†ã€ç”Ÿç‰©ã€å·¥ç¨‹ç­‰å¤šä¸ªå­¦ç§‘é¢†åŸŸã€‚ä¸ºè§£å†³åŸå§‹ç§‘å­¦æ ‡é¢˜ä¸­æ™®éå­˜åœ¨çš„å¼±å¯¹é½é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ä¸ªAIå°±ç»ªçš„è¯­ä¹‰å¢å¼ºæµç¨‹ï¼Œåˆ©ç”¨Qwen-VLå¤šæ¨¡æ€å¤§æ¨¡å‹ç³»åˆ—ï¼Œé€šè¿‡ç»¼åˆè®ºæ–‡æ‘˜è¦å’Œå¼•ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥é‡æ–°æè¿°å›¾åƒå†…å®¹ã€‚</p>
<p><strong>Result:</strong> æŠ€æœ¯éªŒè¯è¡¨æ˜è¯­ä¹‰å¢å¼ºæ˜¾è‘—æå‡äº†æ•°æ®è´¨é‡ï¼šåŸºäºSciBERTçš„ä¼ªå›°æƒ‘åº¦æŒ‡æ ‡æ˜¾ç¤ºè¯­ä¹‰æ¨¡ç³Šæ€§é™ä½ï¼Œè€ŒCLIPåˆ†æ•°è¡¨æ˜å›¾åƒ-æ–‡æœ¬å¯¹é½æ”¹å–„äº†18.21%ã€‚æ•°æ®é›†è¦†ç›–äº†å®éªŒè£…ç½®ã€çƒ­åŠ›å›¾ã€æ˜¾å¾®å›¾åƒç­‰å¤šç§è§†è§‰æ¨¡æ€ï¼Œä¸ºç§‘å­¦æ¨ç†æä¾›äº†ä¸°å¯Œçš„å¤šæ¨¡æ€è¡¨ç¤ºåŸºç¡€ã€‚è¯¥æ•°æ®é›†å·²åœ¨Hugging Faceå¹³å°å…¬å¼€æä¾›ï¼Œä¾¿äºç¤¾åŒºä½¿ç”¨å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Conclusion:</strong> S1-MMAlignä¸ºæ¨è¿›ç§‘å­¦æ¨ç†å’Œè·¨æ¨¡æ€ç†è§£æä¾›äº†åŸºç¡€æ€§èµ„æºï¼Œç‰¹åˆ«æ˜¯åœ¨AI for Scienceæ—¶ä»£å…·æœ‰é‡è¦æ„ä¹‰ã€‚è¯¥ç ”ç©¶ä¸ä»…è§£å†³äº†ç§‘å­¦å¤šæ¨¡æ€æ•°æ®ä¸­çš„å¯¹é½è´¨é‡é—®é¢˜ï¼Œè¿˜ä¸ºæœªæ¥ç§‘å­¦å‘ç°ä»»åŠ¡ä¸­çš„å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚è¯­ä¹‰å¢å¼ºæµç¨‹çš„æ–¹æ³•è®ºä¸ºå¤„ç†å…¶ä»–é¢†åŸŸä¸­çš„å¼±å¯¹é½å¤šæ¨¡æ€æ•°æ®æä¾›äº†å¯å€Ÿé‰´çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.</p>
<h3 id="17-acterase-a-training-free-paradigm-for-precise-concept-erasure-via-activation-patching">[17] <a href="https://arxiv.org/abs/2601.00267">ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching</a></h3>
<p><em>Yi Sun, Xinhao Zhong, Hongyan Li, Yimin Zhou, Junhao Li, Bin Chen, Xuan Wang</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¿€æ´»æ“¦é™¤æ–¹æ³•ï¼ˆActEraseï¼‰ï¼Œé€šè¿‡è¯†åˆ«æ¿€æ´»å·®å¼‚åŒºåŸŸå¹¶åŠ¨æ€æ›¿æ¢è¾“å…¥æ¿€æ´»ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å®ç°äº†é«˜æ•ˆçš„æ¦‚å¿µæ“¦é™¤ï¼Œåœ¨ä¿æŒç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ“¦é™¤æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„æ¦‚å¿µæ“¦é™¤æ–¹æ³•å¤§å¤šä¾èµ–äºæ•°æ®å¯†é›†ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚çš„å¾®è°ƒè¿‡ç¨‹ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨æ˜¾è‘—é™åˆ¶ã€‚æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å®‰å…¨ã€ç‰ˆæƒå’Œä¼¦ç†æ–¹é¢å­˜åœ¨é£é™©ï¼Œéœ€è¦æ›´é«˜æ•ˆçš„æ–¹æ³•æ¥ç§»é™¤æ•æ„Ÿæ¦‚å¿µï¼ŒåŒæ—¶é¿å…å¯¹æ¨¡å‹æ•´ä½“ç”Ÿæˆèƒ½åŠ›çš„æŸå®³ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åŸºäºæ¨¡å‹æ¿€æ´»ä¸»è¦ç”±é€šç”¨æ¦‚å¿µç»„æˆã€ä»…æå°éƒ¨åˆ†è¡¨ç¤ºç›®æ ‡æ¦‚å¿µçš„è§‚å¯Ÿï¼Œé€šè¿‡æç¤ºå¯¹åˆ†æè¯†åˆ«æ¿€æ´»å·®å¼‚åŒºåŸŸï¼Œæå–ç›®æ ‡æ¿€æ´»å¹¶åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­åŠ¨æ€æ›¿æ¢è¾“å…¥æ¿€æ´»ã€‚è¿™ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•å®ç°äº†å³æ’å³ç”¨çš„æ¦‚å¿µæ“ä½œèŒƒå¼ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªå…³é”®æ“¦é™¤ä»»åŠ¡ï¼ˆè£¸éœ²å†…å®¹ã€è‰ºæœ¯é£æ ¼å’Œå¯¹è±¡ç§»é™¤ï¼‰ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ“¦é™¤æ€§èƒ½ï¼ŒåŒæ—¶æœ‰æ•ˆä¿æŒäº†æ¨¡å‹çš„æ•´ä½“ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ–¹æ³•è¿˜è¡¨ç°å‡ºå¯¹å¯¹æŠ—æ”»å‡»çš„å¼ºå¤§é²æ£’æ€§ï¼ŒéªŒè¯äº†å…¶å®é™…åº”ç”¨çš„å¯é æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºæ‰©æ•£æ¨¡å‹ä¸­çš„æ¦‚å¿µæ“ä½œå»ºç«‹äº†ä¸€ä¸ªæ–°çš„è½»é‡çº§å³æ’å³ç”¨èŒƒå¼ï¼Œæ— éœ€è®­ç»ƒå³å¯å®ç°é«˜æ•ˆæ¦‚å¿µæ“¦é™¤ã€‚è¯¥æ–¹æ³•åœ¨å®‰å…¨ã€ç‰ˆæƒå’Œä¼¦ç†é£é™©ç¼“è§£æ–¹é¢å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ï¼ŒåŒæ—¶ä¸ºæ¨¡å‹å¯è§£é‡Šæ€§å’Œå¯æ§ç”Ÿæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.</p>
<h3 id="18-omnivat-single-domain-generalization-for-multimodal-visual-tactile-learning">[18] <a href="https://arxiv.org/abs/2601.00352">OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning</a></h3>
<p><em>Liuxiang Qiu, Hui Da, Yuzhen Niu, Tiesong Zhao, Yang Cao, Zheng-Jun Zha</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†OmniVaTæ¡†æ¶ï¼Œé¦–æ¬¡æˆåŠŸè§£å†³äº†è§†è§‰-è§¦è§‰å­¦ä¹ ä¸­çš„å•åŸŸæ³›åŒ–é—®é¢˜ï¼Œé€šè¿‡å¤šæ¨¡æ€åˆ†æ•°å‚…é‡Œå¶é€‚é…å™¨å’Œç¦»æ•£æ ‘ç”Ÿæˆæ¨¡å—æœ‰æ•ˆç¼“è§£æ¨¡æ€å·®å¼‚å’ŒåŸŸåç§»æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰-è§¦è§‰å­¦ä¹ é¢ä¸´è§†è§‰ä¸è§¦è§‰å›¾åƒä¹‹é—´çš„æ¨¡æ€å·®å¼‚ï¼Œä»¥åŠç”±éæ ‡å‡†åŒ–è§¦è§‰ä¼ æ„Ÿå™¨å’Œä¸ä¸€è‡´æ•°æ®æ”¶é›†ç¨‹åºå¼•èµ·çš„åŸŸå·®è·é—®é¢˜ï¼Œè¿™äº›æŒ‘æˆ˜è¢«å½¢å¼åŒ–ä¸ºå•åŸŸæ³›åŒ–å¤šæ¨¡æ€è§†è§‰-è§¦è§‰å­¦ä¹ ä»»åŠ¡ã€‚</p>
<p><strong>Method:</strong> OmniVaTæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¤šæ¨¡æ€åˆ†æ•°å‚…é‡Œå¶é€‚é…å™¨å°†è§†è§‰å’Œè§¦è§‰åµŒå…¥æ˜ å°„åˆ°ç»Ÿä¸€çš„åµŒå…¥-é¢‘ç‡ç©ºé—´ä»¥ç¼“è§£æ¨¡æ€å·®è·ï¼›ç¦»æ•£æ ‘ç”Ÿæˆæ¨¡å—é€šè¿‡åˆ†å±‚æ ‘ç»“æ„è·å¾—å¤šæ ·ä¸”å¯é çš„å¤šæ¨¡æ€åˆ†æ•°è¡¨ç¤ºï¼Œå¢å¼ºå¯¹æœªè§åŸŸä¸­æ³¢åŠ¨åŸŸåç§»çš„é€‚åº”æ€§ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜OmniVaTåœ¨SDG-VTLä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„è·¨åŸŸæ³›åŒ–æ€§èƒ½ï¼Œæ— éœ€å¤šåŸŸè®­ç»ƒæ•°æ®æˆ–ç²¾ç»†çš„è·¨æ¨¡æ€èåˆç­–ç•¥å³å¯æœ‰æ•ˆå¤„ç†æ¨¡æ€å·®å¼‚å’ŒåŸŸåç§»é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é¦–æ¬¡æˆåŠŸè§£å†³äº†è§†è§‰-è§¦è§‰å­¦ä¹ ä¸­çš„å•åŸŸæ³›åŒ–æŒ‘æˆ˜ï¼Œæå‡ºçš„ç»Ÿä¸€åµŒå…¥-é¢‘ç‡ç©ºé—´æ˜ å°„å’Œåˆ†å±‚æ ‘ç»“æ„è¡¨ç¤ºæ–¹æ³•ä¸ºå¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­çš„é²æ£’æ³›åŒ–æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.</p>
<h3 id="19-efficient-prediction-of-dense-visual-embeddings-via-distillation-and-rgb-d-transformers">[19] <a href="https://arxiv.org/abs/2601.00359">Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers</a></h3>
<p><em>SÃ¶hnke Benedikt Fischedick, Daniel Seichter, Benedict Stephan, Robin Schmidt, Horst-Michael Gross</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDVEFormerï¼Œä¸€ç§åŸºäºRGB-D Transformerçš„é«˜æ•ˆæ–¹æ³•ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦é¢„æµ‹å¯†é›†æ–‡æœ¬å¯¹é½çš„è§†è§‰åµŒå…¥ï¼Œä¸ºç§»åŠ¨æœºå™¨äººæä¾›çµæ´»çš„è¯­ä¹‰ç†è§£å’Œè‡ªç„¶è¯­è¨€æŸ¥è¯¢èƒ½åŠ›ï¼ŒåŒæ—¶æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨å®¶åº­ç¯å¢ƒä¸­ï¼Œæœºå™¨äººéœ€è¦å…¨é¢ç†è§£å‘¨å›´ç¯å¢ƒæ‰èƒ½ä¸æœªç»è®­ç»ƒçš„äººç±»è¿›è¡Œæœ‰æ•ˆç›´è§‚çš„äº¤äº’ï¼Œä¼ ç»Ÿè¯­ä¹‰åˆ†å‰²æ–¹æ³•ä½¿ç”¨å›ºå®šé¢„å®šä¹‰ç±»åˆ«ï¼Œç¼ºä¹çµæ´»æ€§ï¼Œæ— æ³•æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œé«˜çº§åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºTransformerçš„RGB-Dæ¶æ„DVEFormerï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦ä»Alpha-CLIPæ•™å¸ˆæ¨¡å‹ä¸­å­¦ä¹ ç»†ç²’åº¦åƒç´ çº§åµŒå…¥ï¼Œé¢„æµ‹å¯†é›†æ–‡æœ¬å¯¹é½çš„è§†è§‰åµŒå…¥ï¼Œè€Œéç›´æ¥è¿›è¡Œä¼ ç»Ÿçš„å›ºå®šç±»åˆ«è¯­ä¹‰åˆ†å‰²ã€‚</p>
<p><strong>Result:</strong> åœ¨å¸¸è§å®¤å†…æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶æ»¡è¶³å®æ—¶è¦æ±‚ï¼Œå®Œæ•´æ¨¡å‹åœ¨NVIDIA Jetson AGX Orinä¸Šè¾¾åˆ°26.3 FPSï¼Œè¾ƒå°å˜ä½“è¾¾åˆ°77.0 FPSï¼Œå®šæ€§ç»“æœå±•ç¤ºäº†åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•å¯ä½œä¸ºä¼ ç»Ÿåˆ†å‰²æ–¹æ³•çš„ç›´æ¥æ›¿ä»£æ–¹æ¡ˆï¼ŒåŒæ—¶æ”¯æŒçµæ´»çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œæ— ç¼é›†æˆåˆ°ç§»åŠ¨æœºå™¨äºº3Då»ºå›¾æµç¨‹ä¸­ï¼Œä¸ºæœºå™¨äººç¯å¢ƒç†è§£æä¾›äº†æ›´é€šç”¨å’Œå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.</p>
<h3 id="20-bharnet-reliability-aware-body-hand-modality-expertized-networks-for-fine-grained-skeleton-action-recognition">[20] <a href="https://arxiv.org/abs/2601.00369">BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition</a></h3>
<p><em>Seungyeon Cho, Tae-kyun Kim</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¦‚ç‡åŒæµæ¡†æ¶ï¼Œç”¨äºéª¨æ¶åŠ¨ä½œè¯†åˆ«ï¼Œé€šè¿‡ç»Ÿä¸€å¯é æ€§å»ºæ¨¡å’Œå¤šæ¨¡æ€é›†æˆï¼Œåœ¨ä¸ç¡®å®šæ¡ä»¶ä¸‹å®ç°ä¸“å®¶åŒ–å­¦ä¹ ï¼Œç‰¹åˆ«å…³æ³¨ç»†ç²’åº¦æ‰‹éƒ¨åŠ¨ä½œè¯†åˆ«ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºéª¨æ¶çš„åŠ¨ä½œè¯†åˆ«æ–¹æ³•å¤§å¤šä»¥èº«ä½“ä¸ºä¸­å¿ƒï¼Œå…³æ³¨å¤§è§„æ¨¡åŠ¨ä½œè€Œå¿½ç•¥äº†å¯¹æ‰‹éƒ¨ç»†å¾®å…³èŠ‚çš„å…³é”®ä¿¡æ¯ï¼Œè¿™é™åˆ¶äº†ç»†ç²’åº¦åŠ¨ä½œè¯†åˆ«çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°å’Œå¼‚æ„æ¡ä»¶ä¸‹ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ— éœ€æ ¡å‡†çš„é¢„å¤„ç†ç®¡é“ç›´æ¥ä»åŸç”Ÿåæ ‡å­¦ä¹ ï¼›æ¦‚ç‡Noisy-ORèåˆå®ç°æ— éœ€æ˜¾å¼ç½®ä¿¡åº¦ç›‘ç£çš„å¯é æ€§æ„ŸçŸ¥åŒæµå­¦ä¹ ï¼›ä»¥åŠä»éª¨æ¶æ¨¡æ€ï¼ˆå…³èŠ‚ã€éª¨éª¼ã€å…³èŠ‚è¿åŠ¨ã€éª¨éª¼è¿åŠ¨ï¼‰åˆ°RGBè¡¨ç¤ºçš„å†…åˆ°è·¨æ¨¡æ€é›†æˆï¼Œåœ¨ç»Ÿä¸€æ¡†æ¶ä¸­æ¡¥æ¥ç»“æ„å’Œè§†è§‰è¿åŠ¨çº¿ç´¢ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼ˆNTU RGB+D~60/120ã€PKU-MMDã€N-UCLAï¼‰å’Œæ–°å®šä¹‰çš„æ‰‹éƒ¨ä¸­å¿ƒåŸºå‡†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºåœ¨å™ªå£°å’Œå¼‚æ„æ¡ä»¶ä¸‹å‡è·å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡å’Œé²æ£’æ€§æ”¹è¿›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†æ¦‚ç‡åŒæµæ¡†æ¶åœ¨ç»Ÿä¸€å¯é æ€§å»ºæ¨¡å’Œå¤šæ¨¡æ€é›†æˆæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«å¼ºè°ƒäº†æ‰‹éƒ¨ç»†å¾®å…³èŠ‚å¯¹ç»†ç²’åº¦åŠ¨ä½œè¯†åˆ«çš„é‡è¦æ€§ï¼Œä¸ºåœ¨ä¸ç¡®å®šæ¡ä»¶ä¸‹å®ç°ä¸“å®¶åŒ–å­¦ä¹ æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.</p>
<h3 id="21-cppo-contrastive-perception-for-vision-language-policy-optimization">[21] <a href="https://arxiv.org/abs/2601.00501">CPPO: Contrastive Perception for Vision Language Policy Optimization</a></h3>
<p><em>Ahmad Rezaei, Mohsen Gholami, Saeed Ranjbar Alvar, Kevin Cannons, Mohammad Asiful Hossain, Zhou Weimin, Shunbo Zhou, Yong Zhang, Mohammad Akbari</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CPPOï¼ˆå¯¹æ¯”æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼‰æ–¹æ³•ï¼Œç”¨äºå¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ£€æµ‹æ„ŸçŸ¥æ ‡è®°çš„ç†µå˜åŒ–å¹¶å¼•å…¥å¯¹æ¯”æ„ŸçŸ¥æŸå¤±ï¼Œè§£å†³äº†å¤šæ¨¡æ€æ¨ç†ä¸­æ„ŸçŸ¥ä¸æ¨ç†éš¾ä»¥åˆ†ç¦»çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¼ºåŒ–å­¦ä¹ åœ¨è¯­è¨€æ¨¡å‹æ¨ç†æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å°†å…¶æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†éœ€è¦åŒæ—¶æ”¹è¿›æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚å…ˆå‰å·¥ä½œä¸»è¦ä¾èµ–æ˜¾å¼æ„ŸçŸ¥å¥–åŠ±ï¼Œä½†åˆ†ç¦»æ„ŸçŸ¥æ ‡è®°ä¸æ¨ç†æ ‡è®°å­˜åœ¨å›°éš¾ï¼Œéœ€è¦é¢å¤–çš„LLMã€çœŸå®æ•°æ®ã€å¼ºåˆ¶åˆ†ç¦»ç­–ç•¥æ¨¡å‹æˆ–å¯¹æ‰€æœ‰è¾“å‡ºæ ‡è®°ä¸åŠ åŒºåˆ†åœ°åº”ç”¨å¥–åŠ±ã€‚</p>
<p><strong>Method:</strong> CPPOé€šè¿‡æ‰°åŠ¨è¾“å…¥å›¾åƒä¸‹æ¨¡å‹è¾“å‡ºçš„ç†µå˜åŒ–æ¥æ£€æµ‹æ„ŸçŸ¥æ ‡è®°ï¼Œå¹¶åœ¨RLç›®æ ‡å‡½æ•°ä¸­æ‰©å±•äº†å¯¹æ¯”æ„ŸçŸ¥æŸå¤±ï¼Œè¯¥æŸå¤±åœ¨ä¿¡æ¯ä¿ç•™æ‰°åŠ¨ä¸‹å¼ºåˆ¶ä¸€è‡´æ€§ï¼Œåœ¨ä¿¡æ¯ç§»é™¤æ‰°åŠ¨ä¸‹å¼ºåˆ¶æ•æ„Ÿæ€§ï¼Œä»è€Œé¿å…ä½¿ç”¨é¢å¤–æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜CPPOè¶…è¶Šäº†å…ˆå‰çš„æ„ŸçŸ¥å¥–åŠ±æ–¹æ³•ï¼ŒåŒæ—¶é¿å…äº†ä½¿ç”¨é¢å¤–æ¨¡å‹ï¼Œä½¿è®­ç»ƒæ›´åŠ é«˜æ•ˆå’Œå¯æ‰©å±•ï¼Œåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> CPPOæä¾›äº†ä¸€ç§æ— éœ€é¢å¤–æ¨¡å‹æˆ–å¼ºåˆ¶åˆ†ç¦»çš„æœ‰æ•ˆæ–¹æ³•æ¥æ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œé€šè¿‡å¯¹æ¯”æ„ŸçŸ¥æŸå¤±å®ç°äº†æ„ŸçŸ¥æ ‡è®°çš„è‡ªåŠ¨æ£€æµ‹å’Œä¼˜åŒ–ï¼Œä¸ºå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æä¾›äº†æ›´é«˜æ•ˆå’Œå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.</p>
<h3 id="22-freetext-training-free-text-rendering-in-diffusion-transformers-via-attention-localization-and-spectral-glyph-injection">[22] <a href="https://arxiv.org/abs/2601.00535">FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection</a></h3>
<p><em>Ruiqiang Zhang, Hengyi Wang, Chang Liu, Guanjie Wang, Zehua Ma, Weiming Zhang</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FreeTextï¼Œä¸€ç§æ— éœ€è®­ç»ƒã€å³æ’å³ç”¨çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£Transformeræ¨¡å‹çš„å†…åœ¨æœºåˆ¶æ¥æ”¹è¿›æ–‡æœ¬æ¸²æŸ“ã€‚è¯¥æ¡†æ¶å°†é—®é¢˜åˆ†è§£ä¸º"åœ¨å“ªé‡Œå†™"å’Œ"å†™ä»€ä¹ˆ"ä¸¤ä¸ªå­é—®é¢˜ï¼Œåˆ†åˆ«é€šè¿‡ç©ºé—´å®šä½å’Œå­—å½¢æ³¨å…¥æ¥è§£å†³ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å¼€æ”¾åŸŸåˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç²¾ç¡®æ–‡æœ¬æ¸²æŸ“æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤šè¡Œå¸ƒå±€ã€å¯†é›†æ’ç‰ˆå’Œé•¿å°¾è„šæœ¬å¦‚ä¸­æ–‡ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸éœ€è¦æ˜‚è´µçš„é‡æ–°è®­ç»ƒæˆ–ä¸¥æ ¼çš„å¤–éƒ¨å¸ƒå±€çº¦æŸï¼Œè¿™ä¼šé™ä½ç¾å­¦è´¨é‡å¹¶é™åˆ¶çµæ´»æ€§ã€‚</p>
<p><strong>Method:</strong> FreeTextæ¡†æ¶å°†é—®é¢˜åˆ†è§£ä¸º"åœ¨å“ªé‡Œå†™"å’Œ"å†™ä»€ä¹ˆ"ä¸¤ä¸ªå­é—®é¢˜ã€‚å¯¹äºç©ºé—´å®šä½ï¼Œé€šè¿‡è¯»å–æ¥è‡ªå†…ç”Ÿå›¾åƒåˆ°æ–‡æœ¬æ³¨æ„åŠ›çš„token-wiseç©ºé—´å½’å› æ¥å®šä½ä¹¦å†™åŒºåŸŸï¼Œä½¿ç”¨sink-like tokenä½œä¸ºç¨³å®šçš„ç©ºé—´é”šç‚¹ï¼Œå¹¶é€šè¿‡æ‹“æ‰‘æ„ŸçŸ¥ç»†åŒ–äº§ç”Ÿé«˜ç½®ä¿¡åº¦æ©ç ã€‚å¯¹äºå†…å®¹ç”Ÿæˆï¼Œå¼•å…¥äº†é¢‘è°±è°ƒåˆ¶å­—å½¢æ³¨å…¥ï¼Œé€šè¿‡é¢‘åŸŸå¸¦é€šè°ƒåˆ¶æ³¨å…¥å™ªå£°å¯¹é½çš„å­—å½¢å…ˆéªŒï¼Œä»¥å¢å¼ºå­—å½¢ç»“æ„å¹¶æŠ‘åˆ¶è¯­ä¹‰æ³„æ¼ã€‚</p>
<p><strong>Result:</strong> åœ¨Qwen-Imageã€FLUX.1-devå’ŒSD3å˜ä½“ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨longText-Benchmarkã€CVTGå’ŒCLT-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ–‡æœ¬å¯è¯»æ€§è·å¾—äº†ä¸€è‡´çš„æå‡ï¼ŒåŒæ—¶å¾ˆå¤§ç¨‹åº¦ä¸Šä¿æŒäº†è¯­ä¹‰å¯¹é½å’Œç¾å­¦è´¨é‡ï¼Œä»…å¸¦æ¥é€‚åº¦çš„æ¨ç†å¼€é”€ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡åˆ©ç”¨æ‰©æ•£Transformeræ¨¡å‹çš„å†…åœ¨æœºåˆ¶ï¼Œå¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æ”¹è¿›æ–‡æœ¬æ¸²æŸ“è´¨é‡ã€‚FreeTextæä¾›äº†ä¸€ç§çµæ´»ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ç²¾ç¡®æ–‡æœ¬æ¸²æŸ“é—®é¢˜å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚å¸ƒå±€å’Œéæ‹‰ä¸è„šæœ¬æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.</p>
<h3 id="23-boosting-segment-anything-model-to-generalize-visually-non-salient-scenarios">[23] <a href="https://arxiv.org/abs/2601.00537">Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios</a></h3>
<p><em>Guangqian Guo, Pengfei Chen, Yong Guo, Huafeng Chen, Boqiang Zhang, Shan Gao</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VNS-SAMï¼Œä¸€ç§å¢å¼ºSAMåœ¨è§†è§‰éæ˜¾è‘—åœºæ™¯ä¸‹åˆ†å‰²æ€§èƒ½çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥Mask-Edge Token Interactiveè§£ç å™¨å’ŒNon-Salient Feature Miningæ¨¡å—ï¼Œåœ¨ä¿æŒåŸå§‹é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡å¯¹ä½å¯¹æ¯”åº¦åœºæ™¯çš„åˆ†å‰²ç²¾åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> Segment Anything Model (SAM)åœ¨è§†è§‰éæ˜¾è‘—åœºæ™¯ä¸‹æ€§èƒ½å—é™ï¼Œè¿™äº›åœºæ™¯ä¸­å‰æ™¯ä¸èƒŒæ™¯å¯¹æ¯”åº¦ä½ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ•æ‰å‡†ç¡®è½®å»“å¹¶äº§ç”Ÿç†æƒ³åˆ†å‰²ç»“æœï¼Œå› æ­¤éœ€è¦å¢å¼ºSAMå¯¹æ­¤ç±»åœºæ™¯çš„æ„ŸçŸ¥èƒ½åŠ›åŒæ—¶ä¿æŒå…¶é›¶æ ·æœ¬æ³›åŒ–æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºVNS-SAMæ–¹æ³•ï¼Œé€šè¿‡Mask-Edge Token Interactiveè§£ç å™¨å’ŒNon-Salient Feature Miningæ¨¡å—æœ‰æ•ˆåˆ©ç”¨SAMçš„ä½å±‚ç‰¹å¾ï¼Œä½¿è§£ç å™¨èƒ½å¤Ÿæ·±å…¥ç†è§£éæ˜¾è‘—ç‰¹å¾ï¼Œä»…éœ€å°‘é‡å‚æ•°å¢åŠ å’Œè®¡ç®—å¼€é”€ï¼›åŒæ—¶æ„å»ºäº†åŒ…å«è¶…è¿‡35Kå›¾åƒçš„VNS-SEGç»Ÿä¸€æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§VNSåœºæ™¯ä»¥å¢å¼ºæ¨¡å‹é²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> VNS-SAMåœ¨å¤šç§VNSåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹æ•ˆæœæ˜¾è‘—ï¼›é¢å¤–å‚æ•°å¯åœ¨4å°æ—¶å†…ä¼˜åŒ–å®Œæˆï¼Œè¯æ˜äº†æ–¹æ³•çš„å¯è¡Œæ€§å’Œå®ç”¨æ€§ï¼›VNS-SEGæ•°æ®é›†ä¸ºæ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œæ³›åŒ–èƒ½åŠ›æä¾›äº†å…¨é¢åŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> VNS-SAMæˆåŠŸè§£å†³äº†SAMåœ¨è§†è§‰éæ˜¾è‘—åœºæ™¯ä¸‹çš„æ€§èƒ½ç“¶é¢ˆï¼Œé€šè¿‡é«˜æ•ˆçš„ç‰¹å¾æŒ–æ˜æœºåˆ¶åœ¨ä¿æŒåŸå§‹æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡åˆ†å‰²ç²¾åº¦ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼›å…¬å¼€çš„ä»£ç å’Œæ•°æ®é›†å°†ä¿ƒè¿›ç›¸å…³é¢†åŸŸç ”ç©¶å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.</p>
<h3 id="24-aegis-exploring-the-limit-of-world-knowledge-capabilities-for-unified-mulitmodal-models">[24] <a href="https://arxiv.org/abs/2601.00561">AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models</a></h3>
<p><em>Jintao Lin, Bowen Dong, Weikang Shi, Chenyang Lei, Suiyun Zhang, Rui Liu, Xihui Liu</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†AEGISåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„ä¸–ç•ŒçŸ¥è¯†åº”ç”¨èƒ½åŠ›ï¼Œå¹¶å¼•å…¥ç¡®å®šæ€§æ¸…å•è¯„ä¼°åè®®ä»¥å¢å¼ºè¯„ä¼°å¯é æ€§ã€‚å®éªŒæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ˜¾è‘—çŸ¥è¯†ç¼ºé™·ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ï¼Œä»…æä¾›å­¤ç«‹çš„å•ä»»åŠ¡è¯„ä¼°ä¸”è¯Šæ–­èƒ½åŠ›ä¸è¶³ï¼Œæ— æ³•å…¨é¢è¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡é—´åº”ç”¨ä¸–ç•ŒçŸ¥è¯†çš„èƒ½åŠ›ã€‚è¿™é˜»ç¢äº†å¯¹æ¨¡å‹è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»å’Œå¤æ‚æ¨ç†èƒ½åŠ›çš„å‡†ç¡®è¯„ä¼°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†AEGISåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1,050ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæ¶µç›–21ä¸ªä¸»é¢˜å’Œ6ç§æ¨ç†ç±»å‹ï¼Œè¦†ç›–è§†è§‰ç†è§£ã€ç”Ÿæˆã€ç¼–è¾‘å’Œäº¤é”™ç”Ÿæˆç­‰å¤šä»»åŠ¡ã€‚åŒæ—¶æå‡ºäº†ç¡®å®šæ€§æ¸…å•è¯„ä¼°åè®®ï¼Œç”¨åŸå­åŒ–çš„"æ˜¯/å¦"åˆ¤æ–­æ›¿ä»£æ¨¡ç³Šçš„æç¤ºè¯„åˆ†ï¼Œä»¥æé«˜è¯„ä¼°å¯é æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜å¤§å¤šæ•°ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹å­˜åœ¨ä¸¥é‡çš„ä¸–ç•ŒçŸ¥è¯†ç¼ºé™·ï¼Œä¸”éšç€æ¨ç†å¤æ‚åº¦å¢åŠ æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ç ”ç©¶å‘ç°ç®€å•çš„æ’ä»¶å¼æ¨ç†æ¨¡å—å¯ä»¥éƒ¨åˆ†ç¼“è§£è¿™äº›è„†å¼±æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å¼ºè°ƒäº†åŸºäºä¸–ç•ŒçŸ¥è¯†çš„æ¨ç†æ˜¯ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹å‘å±•çš„å…³é”®å‰æ²¿é¢†åŸŸã€‚AEGISåŸºå‡†å’Œç¡®å®šæ€§è¯„ä¼°åè®®ä¸ºæ¨¡å‹èƒ½åŠ›è¯„ä¼°æä¾›äº†æ›´å¯é çš„æ¡†æ¶ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.</p>
<h3 id="25-granalign-granularity-aware-alignment-framework-for-zero-shot-video-moment-retrieval">[25] <a href="https://arxiv.org/abs/2601.00584">GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval</a></h3>
<p><em>Mingyu Jeon, Sunjae Yoon, Jonghee Kim, Junyeoung Kim</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç²’åº¦æ„ŸçŸ¥å¯¹é½æ¡†æ¶ï¼ˆGranAlignï¼‰ï¼Œé€šè¿‡ç²’åº¦æ„ŸçŸ¥çš„æŸ¥è¯¢é‡å†™å’ŒæŸ¥è¯¢æ„ŸçŸ¥çš„æ ‡é¢˜ç”ŸæˆæŠ€æœ¯ï¼Œè§£å†³äº†é›¶æ ·æœ¬è§†é¢‘æ—¶åˆ»æ£€ç´¢ä¸­çš„è¯­ä¹‰ç²’åº¦ä¸åŒ¹é…é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é›¶æ ·æœ¬è§†é¢‘æ—¶åˆ»æ£€ç´¢ï¼ˆZVMRï¼‰çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºæ–‡æœ¬æŸ¥è¯¢ä¸è§†é¢‘å†…å®¹ä¹‹é—´çš„è¯­ä¹‰ç²’åº¦ä¸åŒ¹é…é—®é¢˜ã€‚å…ˆå‰ç ”ç©¶è™½ç„¶åˆ©ç”¨é«˜è´¨é‡é¢„è®­ç»ƒçŸ¥è¯†åœ¨è”åˆç©ºé—´ä¸­è¡¨ç¤ºè§†é¢‘å’Œè¯­è¨€ï¼Œä½†æœªèƒ½å¹³è¡¡ä¸åŒæ¨¡æ€æä¾›çš„é¢„è®­ç»ƒçŸ¥è¯†åœ¨ç»™å®šåœºæ™¯ä¸­çš„è¯­ä¹‰ç²’åº¦ï¼Œå¯¼è‡´å°½ç®¡å„æ¨¡æ€è¡¨ç¤ºè´¨é‡å¾ˆé«˜ï¼Œç²’åº¦ä¸åŒ¹é…ä»é€ æˆæ£€ç´¢ä¸å‡†ç¡®ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†æ— éœ€è®­ç»ƒçš„ç²’åº¦æ„ŸçŸ¥å¯¹é½æ¡†æ¶ï¼ˆGranAlignï¼‰ï¼ŒåŒ…å«ä¸¤ç§äº’è¡¥æŠ€æœ¯ï¼šåŸºäºç²’åº¦çš„æŸ¥è¯¢é‡å†™ç”¨äºç”Ÿæˆä¸åŒè¯­ä¹‰ç²’åº¦çš„æŸ¥è¯¢å˜ä½“ï¼Œä»¥åŠæŸ¥è¯¢æ„ŸçŸ¥çš„æ ‡é¢˜ç”Ÿæˆç”¨äºå°†æŸ¥è¯¢æ„å›¾åµŒå…¥è§†é¢‘å†…å®¹ã€‚é€šè¿‡å°†å¤šçº§æŸ¥è¯¢ä¸æŸ¥è¯¢æ— å…³å’ŒæŸ¥è¯¢æ„ŸçŸ¥çš„æ ‡é¢˜é…å¯¹ï¼Œæœ‰æ•ˆè§£å†³äº†è¯­ä¹‰ä¸åŒ¹é…é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªä¸»è¦åŸºå‡†æµ‹è¯•ï¼ˆQVHighlightsã€Charades-STAã€ActivityNet-Captionsï¼‰ä¸Šå‡å®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå…¶ä¸­åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„QVHighlightsæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„3.23% mAP@avgæå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç²’åº¦æ„ŸçŸ¥çš„å¯¹é½ç­–ç•¥å¯ä»¥æœ‰æ•ˆè§£å†³é›¶æ ·æœ¬è§†é¢‘æ—¶åˆ»æ£€ç´¢ä¸­çš„è¯­ä¹‰ç²’åº¦ä¸åŒ¹é…é—®é¢˜ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æ˜¾è‘—æå‡æ€§èƒ½ã€‚è¿™ä¸€æ¡†æ¶ä¸ºè·¨æ¨¡æ€å¯¹é½æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¼ºè°ƒäº†å¹³è¡¡ä¸åŒæ¨¡æ€è¯­ä¹‰ç²’åº¦çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥è§†é¢‘è¯­è¨€ç†è§£ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.</p>
<h3 id="26-modality-dominance-aware-optimization-for-embodied-rgb-infrared-perception">[26] <a href="https://arxiv.org/abs/2601.00598">Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception</a></h3>
<p><em>Xianhui Liu, Siqi Jiang, Yi Xie, Yuqing Lin, Siao Liu</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡æ€ä¸»å¯¼æ„ŸçŸ¥çš„è·¨æ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼ˆMDACLï¼‰ï¼Œé€šè¿‡é‡åŒ–RGB-çº¢å¤–æ¨¡æ€é—´çš„ä¼˜åŒ–åå·®å¹¶å¼•å…¥å±‚æ¬¡åŒ–å¼•å¯¼ä¸å¯¹æŠ—å‡è¡¡æ­£åˆ™åŒ–ï¼Œæ˜¾è‘—æå‡äº†RGB-IRå¤šæ¨¡æ€æ£€æµ‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> RGB-çº¢å¤–å¤šæ¨¡æ€æ„ŸçŸ¥åœ¨å¤æ‚ç‰©ç†ç¯å¢ƒä¸­çš„åµŒå…¥å¼å¤šåª’ä½“ç³»ç»Ÿä¸­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰è·¨æ¨¡æ€èåˆæ–¹æ³•å¿½è§†äº†ç”±æ¨¡æ€ç‰¹æ€§ä¸å¯¹ç§°å¼•èµ·çš„ä¼˜åŒ–åŠ¨æ€é—®é¢˜ã€‚å®è·µä¸­ï¼Œä¿¡æ¯å¯†åº¦å’Œç‰¹å¾è´¨é‡çš„å·®å¼‚å¯¼è‡´æŒç»­çš„ä¼˜åŒ–åå·®ï¼Œä½¿è®­ç»ƒè¿‡åº¦å¼ºè°ƒä¸»å¯¼æ¨¡æ€å¹¶é˜»ç¢æœ‰æ•ˆèåˆã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é¦–å…ˆæå‡ºæ¨¡æ€ä¸»å¯¼æŒ‡æ•°ï¼ˆMDIï¼‰ï¼Œé€šè¿‡è”åˆå»ºæ¨¡ç‰¹å¾ç†µå’Œæ¢¯åº¦è´¡çŒ®æ¥é‡åŒ–æ¨¡æ€ä¸»å¯¼ç¨‹åº¦ã€‚åŸºäºMDIï¼Œå¼€å‘äº†æ¨¡æ€ä¸»å¯¼æ„ŸçŸ¥çš„è·¨æ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼ˆMDACLï¼‰ï¼Œè¯¥æ¡†æ¶åŒ…å«å±‚æ¬¡åŒ–è·¨æ¨¡æ€å¼•å¯¼ï¼ˆHCGï¼‰ä»¥å¢å¼ºç‰¹å¾å¯¹é½ï¼Œä»¥åŠå¯¹æŠ—å‡è¡¡æ­£åˆ™åŒ–ï¼ˆAERï¼‰ä»¥å¹³è¡¡èåˆè¿‡ç¨‹ä¸­çš„ä¼˜åŒ–åŠ¨æ€ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªRGB-IRåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMDACLèƒ½æœ‰æ•ˆç¼“è§£ä¼˜åŒ–åå·®å¹¶å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ£€æµ‹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰è·¨æ¨¡æ€èåˆæ–¹æ³•ï¼ŒéªŒè¯äº†æ‰€æé‡åŒ–æŒ‡æ ‡å’Œå¹³è¡¡æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€å­¦ä¹ ä¸­ç”±ä¸å¯¹ç§°æ¨¡æ€ç‰¹æ€§å¼•èµ·çš„ä¼˜åŒ–åå·®é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§é‡åŒ–åˆ†æå’Œå¹³è¡¡ä¼˜åŒ–çš„ç³»ç»Ÿè§£å†³æ–¹æ¡ˆã€‚MDACLæ¡†æ¶ä¸ä»…æå‡äº†RGB-IRæ£€æµ‹æ€§èƒ½ï¼Œä¹Ÿä¸ºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„ä¼˜åŒ–ä¸å¹³è¡¡é—®é¢˜æä¾›äº†é€šç”¨æ–¹æ³•è®ºï¼Œæ¨åŠ¨äº†è·¨æ¨¡æ€èåˆæŠ€æœ¯çš„ç†è®ºå‘å±•ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.</p>
<h3 id="27-hyperpriv-epn-hypergraph-learning-with-privileged-knowledge-for-ependymoma-prognosis">[27] <a href="https://arxiv.org/abs/2601.00626">HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis</a></h3>
<p><em>Shuren Gabriel Yu, Sikang Ren, Yongji Tian</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºHyperPriv-EPNï¼Œä¸€ç§åŸºäºè¶…å›¾çš„ç‰¹æƒä¿¡æ¯å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åŒæµè’¸é¦ä½¿æœ¯å‰æ¨¡å‹èƒ½å¤Ÿä»è§†è§‰ç‰¹å¾ä¸­å¹»è§‰å‡ºè¯­ä¹‰ç¤¾åŒºç»“æ„ï¼Œå®ç°äº†æ— éœ€æ¨ç†æ—¶æ–‡æœ¬è¾“å…¥çš„æœ¯å‰å®¤ç®¡è†œç˜¤é¢„åé¢„æµ‹ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å®¤ç®¡è†œç˜¤çš„æœ¯å‰é¢„åå¯¹æ²»ç–—è§„åˆ’è‡³å…³é‡è¦ï¼Œä½†ç”±äºMRIç¼ºä¹æœ¯åæ‰‹æœ¯æŠ¥å‘Šä¸­çš„è¯­ä¹‰æ´å¯Ÿè€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰å¤šæ¨¡æ€æ–¹æ³•åœ¨æ¨ç†æ—¶æ— æ³•åˆ©ç”¨è¿™äº›ç‰¹æƒæ–‡æœ¬æ•°æ®ï¼Œéœ€è¦å¼¥åˆè¿™ä¸€å·®è·ã€‚</p>
<p><strong>Method:</strong> æå‡ºHyperPriv-EPNæ¡†æ¶ï¼Œé‡‡ç”¨Severed Graphç­–ç•¥ï¼Œä½¿ç”¨å…±äº«ç¼–ç å™¨å¤„ç†æ•™å¸ˆå›¾ï¼ˆåŒ…å«æœ¯åç‰¹æƒä¿¡æ¯ï¼‰å’Œå­¦ç”Ÿå›¾ï¼ˆä»…é™æœ¯å‰æ•°æ®ï¼‰ã€‚é€šè¿‡åŒæµè’¸é¦ï¼Œå­¦ç”Ÿæ¨¡å‹å­¦ä¹ ä»…ä»è§†è§‰ç‰¹å¾ä¸­å¹»è§‰å‡ºè¯­ä¹‰ç¤¾åŒºç»“æ„ã€‚</p>
<p><strong>Result:</strong> åœ¨åŒ…å«311åæ‚£è€…çš„å¤šä¸­å¿ƒé˜Ÿåˆ—éªŒè¯ä¸­ï¼ŒHyperPriv-EPNå®ç°äº†æœ€å…ˆè¿›çš„è¯Šæ–­å‡†ç¡®æ€§å’Œç”Ÿå­˜åˆ†å±‚æ€§èƒ½ï¼Œæœ‰æ•ˆå°†ä¸“å®¶çŸ¥è¯†è½¬ç§»åˆ°æœ¯å‰è®¾ç½®ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è§£é”äº†å†å²æœ¯åæ•°æ®çš„ä»·å€¼ï¼Œæ— éœ€æ¨ç†æ—¶æ–‡æœ¬è¾“å…¥å³å¯æŒ‡å¯¼æ–°æ‚£è€…çš„è¯Šæ–­ï¼Œä¸ºåŒ»å­¦å½±åƒåˆ†æä¸­ç‰¹æƒä¿¡æ¯çš„åˆ©ç”¨æä¾›äº†åˆ›æ–°æ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Preoperative prognosis of Ependymoma is critical for treatment planning but challenging due to the lack of semantic insights in MRI compared to post-operative surgical reports. Existing multimodal methods fail to leverage this privileged text data when it is unavailable during inference. To bridge this gap, we propose HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information (LUPI) framework. We introduce a Severed Graph Strategy, utilizing a shared encoder to process both a Teacher graph (enriched with privileged post-surgery information) and a Student graph (restricted to pre-operation data). Through dual-stream distillation, the Student learns to hallucinate semantic community structures from visual features alone. Validated on a multi-center cohort of 311 patients, HyperPriv-EPN achieves state-of-the-art diagnostic accuracy and survival stratification. This effectively transfers expert knowledge to the preoperative setting, unlocking the value of historical post-operative data to guide the diagnosis of new patients without requiring text at inference.</p>
<h3 id="28-crops-a-training-free-hallucination-mitigation-framework-for-vision-language-models">[28] <a href="https://arxiv.org/abs/2601.00659">CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models</a></h3>
<p><em>Neeraj Anand, Samyak Jha, Udbhav Bamba, Rahul Rahaman</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CRoPSï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„å¹»è§‰ç¼“è§£æ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©æ€§ç§»é™¤å…³é”®æ–‡æœ¬æ ‡è®°æ„å»ºå¹»è§‰æ¨¡å‹ï¼Œå¹¶ç»“åˆå¹¿ä¹‰å¯¹æ¯”è§£ç æ¥æ•æ‰å¤šæ ·åŒ–çš„å¹»è§‰æ¥æºï¼Œæ˜¾è‘—æå‡äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å–å¾—äº†å¿«é€ŸæˆåŠŸï¼Œä½†å…¶å€¾å‘äºç”Ÿæˆå¹»è§‰å†…å®¹çš„é—®é¢˜ä¸¥é‡å½±å“äº†å®é™…åº”ç”¨çš„å¯é æ€§ã€‚ç°æœ‰æ— éœ€è®­ç»ƒçš„æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šä¸€æ˜¯ä¾èµ–äºå¯¹å¹»è§‰æ¥æºçš„ç‹­éš˜å‡è®¾ï¼ŒäºŒæ˜¯å…¶æœ‰æ•ˆæ€§åœ¨ç”Ÿæˆè¿‡ç¨‹åæœŸï¼ˆå¹»è§‰æœ€å¯èƒ½å‘ç”Ÿæ—¶ï¼‰ä¼šä¸‹é™ã€‚å½“å‰åŸºäºç§»é™¤è§†è§‰æ ‡è®°æ„å»ºå¹»è§‰æ¨¡å‹çš„ç­–ç•¥ä¹Ÿä¸å¤Ÿå……åˆ†ï¼Œå› ä¸ºè§†è§‰ä¿¡æ¯ä»ä¼šä¼ æ’­åˆ°ç”Ÿæˆæ–‡æœ¬ä¸­ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¹»è§‰æ¨¡å‹æ„å»ºæ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§ç§»é™¤å…³é”®æ–‡æœ¬æ ‡è®°æ¥æ•æ‰å¹»è§‰æ•ˆåº”ã€‚è¿›ä¸€æ­¥å¼•å…¥äº†å¹¿ä¹‰å¯¹æ¯”è§£ç ï¼Œè¯¥æŠ€æœ¯æ•´åˆå¤šä¸ªå¹»è§‰æ¨¡å‹ä»¥è¡¨ç¤ºå¤šæ ·åŒ–çš„å¹»è§‰æ¥æºã€‚è¿™äº›æ€æƒ³å…±åŒæ„æˆäº†CRoPSæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å¹»è§‰ç¼“è§£æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯”åŸå§‹æ¨¡å‹ä¸å¤šä¸ªä¸“é—¨è®¾è®¡çš„å¹»è§‰æ¨¡å‹æ¥å‡å°‘å¹»è§‰ç”Ÿæˆã€‚</p>
<p><strong>Result:</strong> CRoPSåœ¨CHAIRè¯„åˆ†ä¸Šå®ç°äº†20%çš„æ”¹è¿›ï¼Œå¹¶åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å®¶æ—ä¸­å–å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ— éœ€è®­ç»ƒæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡é€‰æ‹©æ€§ç§»é™¤æ–‡æœ¬æ ‡è®°è€Œéä»…å…³æ³¨è§†è§‰æ ‡è®°ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°æ•æ‰å’Œç¼“è§£å¹»è§‰æ•ˆåº”ã€‚å¹¿ä¹‰å¯¹æ¯”è§£ç çš„å¤šæ¨¡å‹æ•´åˆç­–ç•¥ä¸ºè§£å†³å¹»è§‰æ¥æºçš„å¤šæ ·æ€§é—®é¢˜æä¾›äº†æ–°æ€è·¯ã€‚CRoPSæ¡†æ¶ä¸ºæ— éœ€è®­ç»ƒçš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹å¯é æ€§æå‡æä¾›äº†å®ç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.</p>
<h3 id="29-grading-handwritten-engineering-exams-with-multimodal-large-language-models">[29] <a href="https://arxiv.org/abs/2601.00730">Grading Handwritten Engineering Exams with Multimodal Large Language Models</a></h3>
<p><em>Janez PerÅ¡, Jon MuhoviÄ, Andrej KoÅ¡ir, BoÅ¡tjan Murovec</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç«¯åˆ°ç«¯å·¥ä½œæµï¼Œç”¨äºè‡ªåŠ¨è¯„åˆ†æ‰‹å†™STEMè€ƒè¯•ï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡å¤šé˜¶æ®µè®¾è®¡å’Œå‚è€ƒè§£å†³æ–¹æ¡ˆæ¡ä»¶åŒ–å®ç°äº†é«˜å¯é æ€§ï¼Œåœ¨çœŸå®è¯¾ç¨‹æµ‹éªŒä¸­è¾¾åˆ°äº†ä¸æ•™å¸ˆè¯„åˆ†çº¦8åˆ†çš„å¹³å‡ç»å¯¹å·®å¼‚ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ‰‹å†™STEMè€ƒè¯•èƒ½å¤Ÿæ•æ‰å¼€æ”¾å¼çš„æ¨ç†è¿‡ç¨‹å’Œå›¾è¡¨ï¼Œä½†äººå·¥è¯„åˆ†é€Ÿåº¦æ…¢ä¸”éš¾ä»¥æ‰©å±•ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿä¿æŒæ ‡å‡†è€ƒè¯•æµç¨‹ï¼ˆA4çº¸å¼ ã€æ— çº¦æŸæ‰‹å†™ï¼‰çš„è‡ªåŠ¨åŒ–è¯„åˆ†æ–¹æ¡ˆæ¥è§£å†³è¿™ä¸€å¯æ‰©å±•æ€§é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨ç«¯åˆ°ç«¯å·¥ä½œæµï¼ŒåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¤„ç†æ‰«ææ‰‹å†™è¯•å·ï¼Œé€šè¿‡æ•™å¸ˆæä¾›çš„æ‰‹å†™å‚è€ƒè§£å†³æ–¹æ¡ˆå’Œç®€çŸ­è¯„åˆ†è§„åˆ™è¿›è¡Œæ¡ä»¶åŒ–ï¼Œé‡‡ç”¨å¤šé˜¶æ®µè®¾è®¡åŒ…æ‹¬æ ¼å¼/å­˜åœ¨æ€§æ£€æŸ¥é˜²æ­¢ç©ºç™½ç­”æ¡ˆè¯„åˆ†ã€ç‹¬ç«‹è¯„åˆ†å™¨é›†æˆã€ç›‘ç£å™¨èšåˆä»¥åŠç¡®å®šæ€§éªŒè¯çš„åˆšæ€§æ¨¡æ¿ç”Ÿæˆå¯å®¡è®¡æŠ¥å‘Šã€‚</p>
<p><strong>Result:</strong> åœ¨çœŸå®è¯¾ç¨‹æµ‹éªŒè¯„ä¼°ä¸­ï¼Œä½¿ç”¨æœ€å…ˆè¿›åç«¯æ¨¡å‹ï¼ˆGPT-5.2å’ŒGemini-3 Proï¼‰çš„å®Œæ•´æµç¨‹è¾¾åˆ°äº†ä¸æ•™å¸ˆè¯„åˆ†çº¦8åˆ†çš„å¹³å‡ç»å¯¹å·®å¼‚ï¼Œåå·®è¾ƒä½ï¼Œä¼°è®¡æ‰‹åŠ¨å®¡æŸ¥è§¦å‘ç‡çº¦ä¸º17%ï¼Œæ¶ˆèå®éªŒæ˜¾ç¤ºç®€åŒ–æç¤ºå’Œç§»é™¤å‚è€ƒè§£å†³æ–¹æ¡ˆä¼šæ˜¾è‘—é™ä½å‡†ç¡®æ€§å¹¶å¼•å…¥ç³»ç»Ÿæ€§è¿‡è¯„åˆ†ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ç»“æ„åŒ–æç¤ºå’Œå‚è€ƒè§£å†³æ–¹æ¡ˆæ¡ä»¶åŒ–å¯¹äºè‡ªåŠ¨è¯„åˆ†ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œå¤šé˜¶æ®µè®¾è®¡ç¡®ä¿äº†å¯é æ€§ï¼Œè¯¥å·¥ä½œæµä¸ºæ‰‹å†™STEMè€ƒè¯•è¯„åˆ†æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒäº†æ ‡å‡†è€ƒè¯•æµç¨‹çš„å®Œæ•´æ€§ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="30-vision-language-reasoning-for-geolocalization-a-reinforcement-learning-approach">[30] <a href="https://arxiv.org/abs/2601.00388">Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach</a></h3>
<p><em>Biao Wu, Meng Fang, Ling Chen, Ke Xu, Tao Cheng, Jun Wang</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Geo-Rï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ£€ç´¢æ— å…³å›¾åƒåœ°ç†å®šä½æ¡†æ¶ï¼Œé€šè¿‡ä»çœŸå®åæ ‡ä¸­æå–ç»“æ„åŒ–æ¨ç†è·¯å¾„å¹¶åˆ©ç”¨åŸºäºHaversineè·ç¦»çš„åæ ‡å¯¹é½å¥–åŠ±è¿›è¡Œä¼˜åŒ–ï¼Œå®ç°äº†å¯è§£é‡Šä¸”å¯æ‰©å±•çš„åœ°ç†å®šä½ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾åƒåœ°ç†å®šä½ä¸­é€šå¸¸ä¾èµ–åˆæˆæ¨ç†æ ‡æ³¨æˆ–å¤–éƒ¨å›¾åƒæ£€ç´¢ï¼Œè¿™é™åˆ¶äº†æ–¹æ³•çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿç›´æ¥ä»çœŸå®åæ ‡ä¸­å­¦ä¹ ç»“æ„åŒ–æ¨ç†ä¸”æ— éœ€æ£€ç´¢çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æå‡ºGeo-Ræ¡†æ¶ï¼ŒåŒ…å«åŸºäºè§„åˆ™çš„å±‚æ¬¡æ¨ç†èŒƒå¼"åŒºåŸŸé“¾"ï¼Œå°†GPSåæ ‡æ˜ å°„åˆ°åœ°ç†å®ä½“å±‚æ¬¡ç»“æ„ä»¥ç”Ÿæˆç²¾ç¡®ç›‘ç£ï¼›é‡‡ç”¨è½»é‡çº§å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ŒåŸºäºHaversineè·ç¦»è®¾è®¡åæ ‡å¯¹é½å¥–åŠ±ï¼Œé€šè¿‡ç©ºé—´æœ‰æ„ä¹‰çš„åé¦ˆä¼˜åŒ–æ¨¡å‹é¢„æµ‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒGeo-Råœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„å®šä½ç²¾åº¦ã€æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œæ›´é€æ˜çš„æ¨ç†è¿‡ç¨‹ï¼ŒéªŒè¯äº†è¯¥æ£€ç´¢æ— å…³èŒƒå¼åœ¨å¯æ‰©å±•å’Œå¯è§£é‡Šå›¾åƒåœ°ç†å®šä½ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å»ºç«‹äº†ç»“æ„åŒ–åœ°ç†æ¨ç†ä¸ç›´æ¥ç©ºé—´ç›‘ç£ä¹‹é—´çš„æ¡¥æ¢ï¼Œä¸ºå›¾åƒåœ°ç†å®šä½æä¾›äº†æ–°çš„æ£€ç´¢æ— å…³èŒƒå¼ï¼Œé€šè¿‡å…¬å¼€æ¨¡å‹å’Œä»£ç ä¿ƒè¿›äº†è¯¥é¢†åŸŸçš„ç ”ç©¶å¯å¤ç°æ€§å’Œè¿›ä¸€æ­¥å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="31-explicit-abstention-knobs-for-predictable-reliability-in-video-question-answering">[31] <a href="https://arxiv.org/abs/2601.00138">Explicit Abstention Knobs for Predictable Reliability in Video Question Answering</a></h3>
<p><em>Jorge Ortiz</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶è¯„ä¼°äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­åŸºäºç½®ä¿¡åº¦çš„é€‰æ‹©æ€§é¢„æµ‹æœºåˆ¶ï¼Œå‘ç°åœ¨åˆ†å¸ƒå†…è®¾ç½®ä¸‹ç½®ä¿¡åº¦é˜ˆå€¼èƒ½æä¾›æœºåˆ¶æ€§é”™è¯¯ç‡æ§åˆ¶ï¼Œä½†åœ¨åˆ†å¸ƒåç§»æ—¶è¿™ç§æ§åˆ¶ä¼šå¤±æ•ˆã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é«˜é£é™©éƒ¨ç½²ä¸­éœ€è¦é€‰æ‹©æ€§é¢„æµ‹æœºåˆ¶ï¼Œå³ç³»ç»Ÿåœ¨ä¸ç¡®å®šæ—¶èƒ½å¤Ÿå¼ƒæƒä»¥é¿å…ä»£ä»·é«˜æ˜‚çš„é”™è¯¯ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶åŸºäºç½®ä¿¡åº¦çš„å¼ƒæƒæœºåˆ¶æ˜¯å¦èƒ½åœ¨è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­æä¾›å¯é çš„é”™è¯¯ç‡æ§åˆ¶ï¼Œä»¥åŠè¿™ç§æ§åˆ¶åœ¨åˆ†å¸ƒåç§»ä¸‹æ˜¯å¦ä¿æŒç¨³å¥ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨NExT-QAæ•°æ®é›†å’ŒGemini 2.0 Flashæ¨¡å‹è¿›è¡Œå®éªŒï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°æ‰«æç½®ä¿¡åº¦é˜ˆå€¼epsilonæ¥è¯„ä¼°é£é™©-è¦†ç›–æƒè¡¡æ›²çº¿ã€‚è¯¥æ–¹æ³•è€ƒå¯Ÿäº†ç½®ä¿¡åº¦é˜ˆå€¼æœºåˆ¶åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒåç§»æ¡ä»¶ä¸‹çš„è¡¨ç°å·®å¼‚ã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°ä¸¤ä¸ªå…³é”®ç»“æœï¼šé¦–å…ˆï¼Œåœ¨åˆ†å¸ƒå†…è®¾ç½®ä¸‹ï¼Œç½®ä¿¡åº¦é˜ˆå€¼èƒ½æä¾›æœºåˆ¶æ€§æ§åˆ¶ï¼Œé€šè¿‡è°ƒæ•´é˜ˆå€¼å¯ä»¥äº§ç”Ÿå¹³æ»‘çš„é£é™©-è¦†ç›–æƒè¡¡æ›²çº¿ï¼Œæœ‰æ•ˆé™ä½é”™è¯¯ç‡ã€‚å…¶æ¬¡ï¼Œè¿™ç§æ§åˆ¶åœ¨åˆ†å¸ƒåç§»æ¡ä»¶ä¸‹ä¼šå¤±æ•ˆï¼Œè¡¨æ˜ç°æœ‰ç½®ä¿¡åº¦æ ¡å‡†æ–¹æ³•å¯¹åˆ†å¸ƒå˜åŒ–çš„é²æ£’æ€§ä¸è¶³ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜è™½ç„¶ç½®ä¿¡åº¦é˜ˆå€¼åœ¨åˆ†å¸ƒå†…èƒ½æœ‰æ•ˆæ§åˆ¶é”™è¯¯ç‡ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´åˆ†å¸ƒåç§»æŒ‘æˆ˜ã€‚è¿™å¼ºè°ƒäº†å¼€å‘å¯¹åˆ†å¸ƒå˜åŒ–æ›´é²æ£’çš„ç½®ä¿¡åº¦æ ¡å‡†æ–¹æ³•çš„é‡è¦æ€§ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„å¯é éƒ¨ç½²æä¾›äº†å…³é”®è§è§£ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f</p>
<h3 id="32-da-dpo-cost-efficient-difficulty-aware-preference-optimization-for-reducing-mllm-hallucinations">[32] <a href="https://arxiv.org/abs/2601.00623">DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations</a></h3>
<p><em>Longtian Qiu, Shan Ning, Chuyu Zhang, Jiaxuan Sun, Xuming He</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºéš¾åº¦æ„ŸçŸ¥ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDA-DPOï¼‰ï¼Œä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­åå¥½æ•°æ®éš¾åº¦ä¸å¹³è¡¡é—®é¢˜çš„æˆæœ¬æ•ˆç›Šæ¡†æ¶ï¼Œé€šè¿‡é‡æ–°åŠ æƒåå¥½å¯¹æ¥ç¼“è§£è¿‡æ‹Ÿåˆï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æŠ‘åˆ¶å¹»è§‰ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤šæ¨¡æ€ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•å¸¸å› åå¥½æ•°æ®éš¾åº¦ä¸å¹³è¡¡è€Œå‡ºç°è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å€¾å‘äºè¿‡åº¦å…³æ³¨æ˜“äºåŒºåˆ†çš„åå¥½å¯¹ï¼Œè¿™é˜»ç¢äº†ç»†ç²’åº¦å¹»è§‰æŠ‘åˆ¶å¹¶å¯¼è‡´æ•´ä½“æ€§èƒ½ä¸‹é™ã€‚</p>
<p><strong>Method:</strong> DA-DPOæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šéš¾åº¦ä¼°è®¡åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ç»“åˆç”Ÿæˆå¼å’Œå¯¹æ¯”å¼ç›®æ ‡ï¼Œé€šè¿‡åˆ†å¸ƒæ„ŸçŸ¥æŠ•ç¥¨ç­–ç•¥äº§ç”Ÿé²æ£’çš„éš¾åº¦åˆ†æ•°è€Œæ— éœ€é¢å¤–è®­ç»ƒï¼›éš¾åº¦æ„ŸçŸ¥è®­ç»ƒæ ¹æ®ä¼°è®¡éš¾åº¦é‡æ–°åŠ æƒåå¥½å¯¹ï¼Œé™ä½ç®€å•æ ·æœ¬æƒé‡åŒæ—¶å¼ºè°ƒå›°éš¾æ ·æœ¬ä»¥ç¼“è§£è¿‡æ‹Ÿåˆã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜DA-DPOæŒç»­æ”¹è¿›å¤šæ¨¡æ€åå¥½ä¼˜åŒ–ï¼Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºæ›´å¼ºçš„å¹»è§‰é²æ£’æ€§å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œæ— éœ€æ–°æ•°æ®æˆ–é¢å¤–å¾®è°ƒé˜¶æ®µã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡éš¾åº¦æ„ŸçŸ¥æœºåˆ¶å¹³è¡¡å­¦ä¹ è¿‡ç¨‹ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€åå¥½ä¼˜åŒ–ä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œä¸ºæ›´ç²¾ç»†çš„å¹»è§‰æŠ‘åˆ¶æä¾›äº†å®ç”¨æ¡†æ¶ï¼ŒåŒæ—¶ä¿æŒäº†æˆæœ¬æ•ˆç›Šå’Œè®¡ç®—æ•ˆç‡ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.</p>
<h3 id="33-a-vision-and-knowledge-enhanced-large-language-model-for-generalizable-pedestrian-crossing-behavior-inference">[33] <a href="https://arxiv.org/abs/2601.00694">A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference</a></h3>
<p><em>Qingwen Pu, Kun Xie, Hong Yang, Guocong Zhai</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†PedX-LLMï¼Œä¸€ä¸ªè§†è§‰ä¸çŸ¥è¯†å¢å¼ºçš„æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆè§†è§‰ç‰¹å¾ã€æ–‡æœ¬æ•°æ®å’Œäº¤é€šé¢†åŸŸçŸ¥è¯†ï¼Œå°†è¡Œäººè¿‡è¡—è¡Œä¸ºæ¨æ–­ä»ç«™ç‚¹ç‰¹å®šæ¨¡å¼è¯†åˆ«è½¬å˜ä¸ºå¯æ³›åŒ–çš„è¡Œä¸ºæ¨ç†ï¼Œæ˜¾è‘—æå‡äº†è·¨åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è¡Œäººè¿‡è¡—è¡Œä¸ºæ¨æ–­èŒƒå¼ï¼ˆä»ç»Ÿè®¡æ¨¡å‹åˆ°ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼‰æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œåœ¨æ–°åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼›è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†ä»æ•°å€¼æ¨¡å¼æ‹Ÿåˆåˆ°è¯­ä¹‰ä¸Šä¸‹æ–‡è¡Œä¸ºæ¨ç†çš„è½¬å˜ï¼Œä½†ç°æœ‰LLMåº”ç”¨ç¼ºä¹é¢†åŸŸç‰¹å®šé€‚åº”æ€§å’Œè§†è§‰ä¸Šä¸‹æ–‡ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿæ•´åˆè§†è§‰ä¿¡æ¯å’Œé¢†åŸŸçŸ¥è¯†çš„é€šç”¨æ¨ç†æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†Pedestrian Crossing LLM (PedX-LLM)æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆLLaVAæå–çš„è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬æ•°æ®åŠäº¤é€šé¢†åŸŸçŸ¥è¯†ï¼Œé‡‡ç”¨Low-Rank Adaptation (LoRA)å¯¹LLaMA-2-7BåŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå®ç°ä»è§†è§‰å¢å¼ºç‰¹å¾åˆ°è¡Œäººè¿‡è¡—å†³ç­–çš„æ¨ç†è¿‡ç¨‹ã€‚</p>
<p><strong>Result:</strong> PedX-LLMè¾¾åˆ°82.0%çš„å¹³è¡¡å‡†ç¡®ç‡ï¼Œä¼˜äºæœ€ä½³ç»Ÿè®¡å’Œç›‘ç£å­¦ä¹ æ–¹æ³•ï¼›è§†è§‰å¢å¼ºæ¨¡å—è´¡çŒ®2.9%æ€§èƒ½æå‡ï¼Œé¢†åŸŸçŸ¥è¯†æ•´åˆå¸¦æ¥é¢å¤–4.1%æ”¹è¿›ï¼›åœ¨äº”ä¸ªæœªè§æµ‹è¯•ç«™ç‚¹ä¸Šï¼Œé›¶æ ·æœ¬é…ç½®è·å¾—66.9%å¹³è¡¡å‡†ç¡®ç‡ï¼Œä¼˜äºåŸºçº¿æ•°æ®é©±åŠ¨æ–¹æ³•è‡³å°‘18ä¸ªç™¾åˆ†ç‚¹ï¼›é€šè¿‡å°‘æ ·æœ¬å­¦ä¹ ï¼ˆä»…äº”ä¸ªéªŒè¯ç¤ºä¾‹ï¼‰å¯å°†å¹³è¡¡å‡†ç¡®ç‡è¿›ä¸€æ­¥æå‡è‡³72.2%ã€‚</p>
<p><strong>Conclusion:</strong> PedX-LLMå±•ç¤ºäº†å¼ºå¤§çš„è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ï¼Œè¯å®è§†è§‰ä¸çŸ¥è¯†å¢å¼ºçš„æ¨ç†ä½¿æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»å†³ç­–é€»è¾‘ï¼Œå…‹æœçº¯æ•°æ®é©±åŠ¨æ–¹æ³•çš„å±€é™æ€§ï¼›è¯¥æ–¹æ³•ä¸ºè¡Œäººè¡Œä¸ºå»ºæ¨¡æä¾›äº†ä»æ¨¡å¼è¯†åˆ«åˆ°è¯­ä¹‰æ¨ç†çš„èŒƒå¼è½¬å˜ï¼Œä¸ºæ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­çš„äººç±»è¡Œä¸ºç†è§£æä¾›äº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.</p>
  </article>
</body>
</html>
