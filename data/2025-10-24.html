<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-24.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 36]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 5]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 4]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-transformed-multi-view-3d-shape-features-with-contrastive-learning">[1] <a href="https://arxiv.org/abs/2510.19955">Transformed Multi-view 3D Shape Features with Contrastive Learning</a></h3>
<p><em>MÃ¡rcus VinÃ­cius Lobo Costa, Sherlon Almeida da Silva, BÃ¡rbara Caroline Benato, Leo Sampaio Ferraz Ribeiro, Moacir Antonelli Ponti</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é€šè¿‡å°†è§†è§‰Transformeræ¶æ„ä¸ç°ä»£å¯¹æ¯”å­¦ä¹ ç›®æ ‡ç›¸ç»“åˆï¼Œè§£å†³äº†3Då½¢çŠ¶ç‰¹å¾è¡¨ç¤ºå­¦ä¹ ä¸­çš„æŒ‘æˆ˜ï¼Œåœ¨ModelNet10ä¸Šè¾¾åˆ°90.6%çš„å‡†ç¡®ç‡ï¼Œç»Ÿä¸€äº†å¯¹æ¯”å­¦ä¹ å’Œ3Då½¢çŠ¶ç†è§£æµç¨‹ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è®¡ç®—æœºè§†è§‰æ–¹æ³•åœ¨ä»2Då›¾åƒè¯†åˆ«3Dç‰©ä½“æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œé€šå¸¸éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ä¸”ä¾èµ–å·ç§¯ç¥ç»ç½‘ç»œï¼Œè¿™å¯èƒ½å¿½ç•¥å…³é”®çš„å½¢çŠ¶å…³ç³»ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶ï¼Œæ¢ç´¢æ›´æœ‰æ•ˆçš„3Dè¡¨ç¤ºå­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨è§†è§‰Transformeræ¶æ„ä¸ç°ä»£å¯¹æ¯”å­¦ä¹ ç›®æ ‡ç›¸ç»“åˆçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æœ‰ç›‘ç£å’Œæ— ç›‘ç£å¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼Œåˆ©ç”¨ViTç†è§£æ•´ä½“å½¢çŠ¶çš„èƒ½åŠ›å’Œå¯¹æ¯”å­¦ä¹ ä¼˜åŒ–å±€éƒ¨åˆ¤åˆ«ç‰¹å¾çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Result:</strong> åœ¨ModelNet10æ•°æ®é›†ä¸Šï¼Œæœ‰ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•è¾¾åˆ°äº†çº¦90.6%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†ViTä¸å¯¹æ¯”å­¦ä¹ ç»“åˆåœ¨å¤šè§†è§’3Dåˆ†æä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ViTé€šè¿‡æ•æ‰å…¨å±€å½¢çŠ¶è¯­ä¹‰ä¸å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–å±€éƒ¨ç‰¹å¾çš„ç»“åˆï¼ŒæˆåŠŸå…‹æœäº†ä¼ ç»ŸCNNåœ¨æ•è·å½¢çŠ¶å…³ç³»æ–¹é¢çš„é™åˆ¶ï¼Œä¸º3Dè¡¨ç¤ºå­¦ä¹ æä¾›äº†æœ‰æ•ˆçš„å®è¯åŸºç¡€ï¼Œå‡å°‘äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>This paper addresses the challenges in representation learning of 3D shape
features by investigating state-of-the-art backbones paired with both
contrastive supervised and self-supervised learning objectives. Computer vision
methods struggle with recognizing 3D objects from 2D images, often requiring
extensive labeled data and relying on Convolutional Neural Networks (CNNs) that
may overlook crucial shape relationships. Our work demonstrates that Vision
Transformers (ViTs) based architectures, when paired with modern contrastive
objectives, achieve promising results in multi-view 3D analysis on our
downstream tasks, unifying contrastive and 3D shape understanding pipelines.
For example, supervised contrastive losses reached about 90.6% accuracy on
ModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' ability
to understand overall shapes and contrastive learning's effectiveness,
overcomes the need for extensive labeled data and the limitations of CNNs in
capturing crucial shape relationships. The success stems from capturing global
shape semantics via ViTs and refining local discriminative features through
contrastive optimization. Importantly, our approach is empirical, as it is
grounded on extensive experimental evaluation to validate the effectiveness of
combining ViTs with contrastive objectives for 3D representation learning.</p>
<h3 id="2-futrtrack-a-camera-lidar-fusion-transformer-for-3d-multiple-object-tracking">[2] <a href="https://arxiv.org/abs/2510.19981">FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking</a></h3>
<p><em>Martha Teiko Teye, Ori Maoz, Matthias Rottmann</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>FutrTrackæå‡ºäº†ä¸€ç§æ¨¡å—åŒ–çš„ç›¸æœº-LiDARå¤šç›®æ ‡è·Ÿè¸ªæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥åŸºäºTransformerçš„å¹³æ»‘å™¨å’Œèåˆé©±åŠ¨çš„è·Ÿè¸ªå™¨ï¼Œåœ¨nuSceneså’ŒKITTIåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†74.7 aMOTAçš„å¼ºæ€§èƒ½ï¼Œæ˜¾è‘—å‡å°‘äº†èº«ä»½åˆ‡æ¢ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰3Då¤šç›®æ ‡è·Ÿè¸ªæ–¹æ³•åœ¨é®æŒ¡å’Œè§†è§’å˜åŒ–ä¸‹èº«ä»½é‡è¯†åˆ«é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ï¼Œä»¥åŠå•ä¼ æ„Ÿå™¨æ–¹æ³•åœ¨ç‰¹å¾è¡¨ç¤ºä¸Šçš„å±€é™æ€§ï¼Œæ¢ç´¢å¦‚ä½•æœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¼ æ„Ÿå™¨ç‰¹å¾æ¥æå‡è·Ÿè¸ªæ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> FutrTracké‡‡ç”¨åŸºäºæŸ¥è¯¢çš„è·Ÿè¸ªæ¡†æ¶ï¼Œæ„å»ºäº†å¤šæ¨¡æ€ä¸¤é˜¶æ®µTransformerç²¾ç‚¼å’Œè·Ÿè¸ªæµæ°´çº¿ï¼ŒåŒ…æ‹¬åŸºäºç§»åŠ¨çª—å£çš„æ—¶é—´å¹³æ»‘å™¨æ¥ä¼˜åŒ–è½¨è¿¹å’Œå‡å°‘æŠ–åŠ¨ï¼Œä»¥åŠèåˆè·Ÿè¸ªå™¨é›†æˆè¾¹ç•Œæ¡†ä¸å¤šæ¨¡æ€BEVèåˆç‰¹å¾ï¼Œæ— éœ€æ˜¾å¼è¿åŠ¨æ¨¡å‹å³å¯è·¨å¸§åˆ†é…å’Œä¼ æ’­èº«ä»½ã€‚</p>
<p><strong>Result:</strong> åœ¨nuSceneså’ŒKITTIæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒFutrTrackåœ¨nuScenesæµ‹è¯•é›†ä¸Šè¾¾åˆ°74.7 aMOTAï¼Œåœ¨3D MOTåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºæ€§èƒ½ï¼Œæ˜¾è‘—å‡å°‘äº†èº«ä»½åˆ‡æ¢ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å¤šæ¨¡æ€ä¼ æ„Ÿå™¨ç‰¹å¾ç›¸æ¯”å•ä¼ æ„Ÿå™¨æ–¹æ³•çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜åŸºäºæŸ¥è¯¢çš„Transformerè·Ÿè¸ªæ–¹æ³•èƒ½å¤Ÿä»å¤šæ¨¡æ€ä¼ æ„Ÿå™¨ç‰¹å¾ä¸­æ˜¾è‘—è·ç›Šï¼Œæä¾›äº†ä¸€ä¸ªé«˜æ•ˆæ¡†æ¶æ¥æ”¹è¿›åŸºäºTransformerçš„è·Ÿè¸ªå™¨ï¼Œä½¿å…¶å³ä½¿åœ¨æœ‰é™æ•°æ®å’Œæ— éœ€é¢„è®­ç»ƒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¸å…¶ä»–åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ç«äº‰ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework
that builds on existing 3D detectors by introducing a transformer-based
smoother and a fusion-driven tracker. Inspired by query-based tracking
frameworks, FutrTrack employs a multimodal two-stage transformer refinement and
tracking pipeline. Our fusion tracker integrates bounding boxes with multimodal
bird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without
the need for an explicit motion model. The tracker assigns and propagates
identities across frames, leveraging both geometric and semantic cues for
robust re-identification under occlusion and viewpoint changes. Prior to
tracking, we refine sequences of bounding boxes with a temporal smoother over a
moving window to refine trajectories, reduce jitter, and improve spatial
consistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that
query-based transformer tracking methods benefit significantly from multimodal
sensor features compared with previous single-sensor approaches. With an aMOTA
of 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D
MOT benchmarks, reducing identity switches while maintaining competitive
accuracy. Our approach provides an efficient framework for improving
transformer-based trackers to compete with other neural-network-based methods
even with limited data and without pretraining.</p>
<h3 id="3-stablesketcher-enhancing-diffusion-model-for-pixel-based-sketch-generation-via-visual-question-answering-feedback">[3] <a href="https://arxiv.org/abs/2510.20093">StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback</a></h3>
<p><em>Jiho Park, Sieun Choi, Jaeyoon Seo, Jihie Kim</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†StableSketcheræ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–å˜åˆ†è‡ªç¼–ç å™¨çš„æ½œåœ¨è§£ç å’Œé›†æˆåŸºäºè§†è§‰é—®ç­”çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œæ˜¾è‘—æå‡äº†æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ‰‹ç»˜è‰å›¾çš„è´¨é‡å’Œæ–‡æœ¬å¯¹é½èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”ŸæˆåŸºäºåƒç´ çš„æ‰‹ç»˜è‰å›¾ï¼ˆæŠ½è±¡è¡¨è¾¾çš„ä»£è¡¨æ€§ç¤ºä¾‹ï¼‰æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å……åˆ†æ•æ‰è‰å›¾çš„é£æ ¼ç‰¹å¾å¹¶ç¡®ä¿æ–‡æœ¬-å›¾åƒè¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¦–å…ˆå¯¹å˜åˆ†è‡ªç¼–ç å™¨è¿›è¡Œå¾®è°ƒä»¥ä¼˜åŒ–æ½œåœ¨è§£ç ï¼Œä½¿å…¶æ›´å¥½åœ°æ•æ‰è‰å›¾ç‰¹å¾ï¼›å…¶æ¬¡é›†æˆåŸºäºè§†è§‰é—®ç­”çš„æ–°å‹å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œä¸“é—¨ç”¨äºæå‡æ–‡æœ¬-å›¾åƒå¯¹é½å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒStableSketcherç”Ÿæˆçš„è‰å›¾åœ¨é£æ ¼ä¿çœŸåº¦æ–¹é¢æ˜¾è‘—æå‡ï¼Œä¸æç¤ºè¯çš„å¯¹é½æ•ˆæœä¼˜äºStable DiffusionåŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶æ„å»ºäº†é¦–ä¸ªåŒ…å«å®ä¾‹çº§è‰å›¾ä¸æ ‡é¢˜åŠé—®ç­”å¯¹çš„æ•°æ®é›†SketchDUOã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ä»…æå‡ºäº†æœ‰æ•ˆçš„è‰å›¾ç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œè¿˜é€šè¿‡æ„å»ºé«˜è´¨é‡æ•°æ®é›†è§£å†³äº†ç°æœ‰æ•°æ®é›†ä¾èµ–å›¾åƒ-æ ‡ç­¾å¯¹çš„å±€é™æ€§ï¼Œä¸ºæŠ½è±¡è‰ºæœ¯è¡¨è¾¾çš„ç”Ÿæˆæ¨¡å‹ç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€èµ„æºå’Œæ–¹å‘æŒ‡å¼•ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Although recent advancements in diffusion models have significantly enriched
the quality of generated images, challenges remain in synthesizing pixel-based
human-drawn sketches, a representative example of abstract expression. To
combat these challenges, we propose StableSketcher, a novel framework that
empowers diffusion models to generate hand-drawn sketches with high prompt
fidelity. Within this framework, we fine-tune the variational autoencoder to
optimize latent decoding, enabling it to better capture the characteristics of
sketches. In parallel, we integrate a new reward function for reinforcement
learning based on visual question answering, which improves text-image
alignment and semantic consistency. Extensive experiments demonstrate that
StableSketcher generates sketches with improved stylistic fidelity, achieving
better alignment with prompts compared to the Stable Diffusion baseline.
Additionally, we introduce SketchDUO, to the best of our knowledge, the first
dataset comprising instance-level sketches paired with captions and
question-answer pairs, thereby addressing the limitations of existing datasets
that rely on image-label pairs. Our code and dataset will be made publicly
available upon acceptance.</p>
<h3 id="4-exposing-blindspots-cultural-bias-evaluation-in-generative-image-models">[4] <a href="https://arxiv.org/abs/2510.20042">Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models</a></h3>
<p><em>Huichan Seo, Sieun Choi, Minki Hong, Yi Zhou, Junseo Kim, Lukman Ismaila, Naome Etori, Mehul Agarwal, Zhixuan Liu, Jihie Kim, Jean Oh</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–æ¡†æ¶æ¥è¯„ä¼°ç”Ÿæˆå¼å›¾åƒæ¨¡å‹ä¸­çš„æ–‡åŒ–åè§ï¼Œé€šè¿‡è·¨å›½å®¶ã€è·¨æ—¶ä»£å’Œè·¨ç±»åˆ«çš„ç»Ÿä¸€è¯„ä¼°æ­ç¤ºäº†T2Iç”Ÿæˆå’ŒI2Iç¼–è¾‘ä¸­çš„æ–‡åŒ–å¤±çœŸé—®é¢˜ï¼Œå¹¶å‘å¸ƒäº†å¯å¤ç°çš„æ–‡åŒ–ä¸­å¿ƒåŸºå‡†ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿçš„æ–‡åŒ–åè§ï¼Œè€Œå›¾åƒåˆ°å›¾åƒç¼–è¾‘å™¨çš„æ–‡åŒ–åå·®é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œé€šè¿‡æ ‡å‡†åŒ–åè®®å¯¹T2Iç”Ÿæˆå’ŒI2Iç¼–è¾‘è¿›è¡Œå¯æ¯”æ€§è¯Šæ–­ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨ç»Ÿä¸€è¯„ä¼°æ¡†æ¶è¦†ç›–å…­ä¸ªå›½å®¶ï¼Œæ„å»ºåŒ…å«8ä¸ªç±»åˆ«å’Œ36ä¸ªå­ç±»åˆ«çš„è¯„ä¼°ä½“ç³»ï¼Œç»“åˆæ—¶ä»£æ„ŸçŸ¥æç¤ºè¯ï¼Œä½¿ç”¨å›ºå®šè®¾ç½®çš„å¼€æ”¾æ¨¡å‹è¿›è¡Œè·¨å›½å®¶ã€è·¨æ—¶ä»£å’Œè·¨ç±»åˆ«è¯„ä¼°ï¼Œæ•´åˆæ ‡å‡†è‡ªåŠ¨æŒ‡æ ‡ã€æ–‡åŒ–æ„ŸçŸ¥æ£€ç´¢å¢å¼ºVQAå’Œæœ¬åœ°è¯„å®¡ä¸“å®¶çš„ä¸“ä¸šäººå·¥åˆ¤æ–­ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘ç°ï¼šåœ¨å›½å®¶æ— å…³æç¤ºä¸‹æ¨¡å‹é»˜è®¤ç”Ÿæˆåå‘å…¨çƒåŒ—æ–¹å’Œç°ä»£é£æ ¼çš„æç»˜ï¼ŒæŠ¹å¹³äº†è·¨å›½å·®å¼‚ï¼›è¿­ä»£å¼I2Iç¼–è¾‘ä¼šä¾µèš€æ–‡åŒ–ä¿çœŸåº¦ï¼Œå³ä½¿ä¼ ç»ŸæŒ‡æ ‡ä¿æŒç¨³å®šæˆ–æ”¹å–„ï¼›I2Iæ¨¡å‹ä»…åº”ç”¨è¡¨é¢çº¿ç´¢è€Œéæ—¶ä»£ä¸€è‡´çš„æƒ…å¢ƒæ„ŸçŸ¥å˜åŒ–ï¼Œå¯¹å…¨çƒå—æ–¹ç›®æ ‡å¸¸ä¿ç•™æºèº«ä»½ç‰¹å¾ã€‚</p>
<p><strong>Conclusion:</strong> å½“å‰ç³»ç»Ÿä¸­çš„æ–‡åŒ–æ•æ„Ÿç¼–è¾‘ä»ç„¶ä¸å¯é ï¼Œç ”ç©¶é€šè¿‡å‘å¸ƒæ ‡å‡†åŒ–æ•°æ®ã€æç¤ºè¯å’Œäººå·¥è¯„ä¼°åè®®ï¼Œä¸ºè¯Šæ–­å’Œè¿½è¸ªç”Ÿæˆå¼å›¾åƒæ¨¡å‹ä¸­çš„æ–‡åŒ–åè§æä¾›äº†å¯å¤ç°çš„æ–‡åŒ–ä¸­å¿ƒåŸºå‡†ï¼Œå¼ºè°ƒäº†æ”¹è¿›æ–‡åŒ–è¡¨ç¤ºå‡†ç¡®æ€§çš„å¿…è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Generative image models produce striking visuals yet often misrepresent
culture. Prior work has examined cultural bias mainly in text-to-image (T2I)
systems, leaving image-to-image (I2I) editors underexplored. We bridge this gap
with a unified evaluation across six countries, an 8-category/36-subcategory
schema, and era-aware prompts, auditing both T2I generation and I2I editing
under a standardized protocol that yields comparable diagnostics. Using open
models with fixed settings, we derive cross-country, cross-era, and
cross-category evaluations. Our framework combines standard automatic metrics,
a culture-aware retrieval-augmented VQA, and expert human judgments collected
from native reviewers. To enable reproducibility, we release the complete image
corpus, prompts, and configurations. Our study reveals three findings: (1)
under country-agnostic prompts, models default to Global-North, modern-leaning
depictions that flatten cross-country distinctions; (2) iterative I2I editing
erodes cultural fidelity even when conventional metrics remain flat or improve;
and (3) I2I models apply superficial cues (palette shifts, generic props)
rather than era-consistent, context-aware changes, often retaining source
identity for Global-South targets. These results highlight that
culture-sensitive edits remain unreliable in current systems. By releasing
standardized data, prompts, and human evaluation protocols, we provide a
reproducible, culture-centered benchmark for diagnosing and tracking cultural
bias in generative image models.</p>
<h3 id="5-why-lvlms-are-more-prone-to-hallucinations-in-longer-responses-the-role-of-context">[5] <a href="https://arxiv.org/abs/2510.20229">Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context</a></h3>
<p><em>Ge Zheng, Jiaye Qian, Jiajin Tang, Sibei Yang</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„'è¯±å¯¼-æ£€æµ‹-æŠ‘åˆ¶'æ¡†æ¶ï¼Œé€šè¿‡ä¸»åŠ¨è¯±å¯¼å¹»è§‰æ¥æ£€æµ‹é«˜é£é™©æƒ…å†µå¹¶åœ¨è§£ç è¿‡ç¨‹ä¸­æŠ‘åˆ¶å¯¹è±¡çº§å¹»è§‰ï¼Œæ˜¾è‘—æ”¹å–„äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­çš„å¹»è§‰é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé•¿æ–‡æœ¬å“åº”æ—¶å®¹æ˜“å‡ºç°å¹»è§‰é—®é¢˜ï¼Œä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºè¿™ä»…ç”±é•¿åº¦å¯¼è‡´çš„é”™è¯¯ç´¯ç§¯å¼•èµ·ï¼Œä½†æœ¬æ–‡ç ”ç©¶å‘ç°å¹»è§‰é£é™©å®é™…ä¸Šæºäºé•¿æ–‡æœ¬å¯¹ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œå®Œæ•´æ€§çš„æ›´å¼ºä¾èµ–ã€‚</p>
<p><strong>Method:</strong> æå‡º'è¯±å¯¼-æ£€æµ‹-æŠ‘åˆ¶'ä¸‰å±‚æ¡†æ¶ï¼šé¦–å…ˆé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä¸Šä¸‹æ–‡ä¸»åŠ¨è¯±å¯¼å¹»è§‰ï¼Œç„¶ååˆ©ç”¨è¯±å¯¼å®ä¾‹è¿›è¡Œæ—©æœŸé«˜é£é™©æ£€æµ‹ï¼Œæœ€ååœ¨å®é™…è§£ç è¿‡ç¨‹ä¸­æŠ‘åˆ¶æ½œåœ¨çš„å¯¹è±¡çº§å¹»è§‰ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¸€è‡´çš„æ˜¾è‘—æ”¹è¿›ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ£€æµ‹èƒ½åŠ›å’Œå¹»è§‰ç¼“è§£æ•ˆæœï¼ŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ä¸ä»…æä¾›äº†æ€§èƒ½æå‡ï¼Œæ›´é‡è¦çš„æ˜¯é‡æ–°éªŒè¯äº†ä¸Šä¸‹æ–‡ä¾èµ–æ˜¯é•¿æ–‡æœ¬å¹»è§‰çš„æ ¸å¿ƒæœºåˆ¶ï¼Œä¸ºæ·±å…¥æ¢ç´¢LVLMså¹»è§‰é—®é¢˜æä¾›äº†æ–°çš„æ´è§å’Œåˆæ­¥æ¢ç´¢æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Large Vision-Language Models (LVLMs) have made significant progress in recent
years but are also prone to hallucination issues. They exhibit more
hallucinations in longer, free-form responses, often attributed to accumulated
uncertainties. In this paper, we ask: Does increased hallucination result
solely from length-induced errors, or is there a deeper underlying mechanism?
After a series of preliminary experiments and findings, we suggest that the
risk of hallucinations is not caused by length itself but by the increased
reliance on context for coherence and completeness in longer responses.
Building on these insights, we propose a novel "induce-detect-suppress"
framework that actively induces hallucinations through deliberately designed
contexts, leverages induced instances for early detection of high-risk cases,
and ultimately suppresses potential object-level hallucinations during actual
decoding. Our approach achieves consistent, significant improvements across all
benchmarks, demonstrating its efficacy. The strong detection and improved
hallucination mitigation not only validate our framework but, more importantly,
re-validate our hypothesis on context. Rather than solely pursuing performance
gains, this study aims to provide new insights and serves as a first step
toward a deeper exploration of hallucinations in LVLMs' longer responses.</p>
<h3 id="6-biocap-exploiting-synthetic-captions-beyond-labels-in-biological-foundation-models">[6] <a href="https://arxiv.org/abs/2510.20095">BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models</a></h3>
<p><em>Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G. Campolongo, Matthew J. Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, Jianyang Gu</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºBIOCAPæ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆæè¿°æ€§æ ‡æ³¨ï¼Œå°†ç”Ÿç‰©å›¾åƒä¸æ–‡æœ¬æè¿°å¯¹é½ï¼Œåœ¨ç‰©ç§åˆ†ç±»å’Œå›¾æ–‡æ£€ç´¢ä»»åŠ¡ä¸­å–å¾—ä¼˜å¼‚æ€§èƒ½ï¼Œè¯æ˜äº†æè¿°æ€§æ ‡æ³¨åœ¨ç”Ÿç‰©å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸­çš„ä»·å€¼ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç”Ÿç‰©å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ç¼ºä¹å¤§è§„æ¨¡ã€å®ä¾‹ç‰¹å®šçš„æè¿°æ€§æ ‡æ³¨ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè¿™é™åˆ¶äº†è‡ªç„¶è¯­è¨€ç›‘ç£åœ¨ç”Ÿç‰©é¢†åŸŸçš„åº”ç”¨ï¼Œè€Œå›¾åƒå’Œæè¿°æ€§æ ‡æ³¨å¯ä»¥è§†ä¸ºç‰©ç§æ½œåœ¨å½¢æ€ç©ºé—´çš„äº’è¡¥æ ·æœ¬ï¼Œæ•è·ä¸åŒçš„ç”Ÿç‰©ç‰¹å¾ã€‚</p>
<p><strong>Method:</strong> ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆæè¿°æ€§æ ‡æ³¨ï¼Œç»“åˆç»´åŸºç™¾ç§‘çš„è§†è§‰ä¿¡æ¯å’Œé’ˆå¯¹ç‰¹å®šåˆ†ç±»ç¾¤å®šåˆ¶çš„æ ¼å¼ç¤ºä¾‹ï¼Œè¿™äº›é¢†åŸŸç‰¹å®šä¸Šä¸‹æ–‡æœ‰åŠ©äºå‡å°‘å¹»è§‰å¹¶äº§ç”Ÿå‡†ç¡®çš„å®ä¾‹æè¿°æ€§æ ‡æ³¨ï¼ŒåŸºäºè¿™äº›æ ‡æ³¨è®­ç»ƒBIOCAPæ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> BIOCAPæ¨¡å‹èƒ½å¤Ÿæ•è·ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œåœ¨ç‰©ç§åˆ†ç±»å’Œæ–‡æœ¬-å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒéªŒè¯äº†æè¿°æ€§æ ‡æ³¨åœ¨ç”Ÿç‰©å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> æè¿°æ€§æ ‡æ³¨è¶…è¶Šäº†ä¼ ç»Ÿæ ‡ç­¾çš„ä»·å€¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¡¥æ¥ç”Ÿç‰©å›¾åƒä¸å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡å¼ºè°ƒæ½œåœ¨è¯Šæ–­ç‰¹å¾å¹¶æŠ‘åˆ¶è™šå‡ç›¸å…³æ€§ï¼Œä¸ºç”Ÿç‰©å¤šæ¨¡æ€å­¦ä¹ æä¾›äº†æ–°çš„ç›‘ç£èŒƒå¼ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>This work investigates descriptive captions as an additional source of
supervision for biological multimodal foundation models. Images and captions
can be viewed as complementary samples from the latent morphospace of a
species, each capturing certain biological traits. Incorporating captions
during training encourages alignment with this shared latent structure,
emphasizing potentially diagnostic characters while suppressing spurious
correlations. The main challenge, however, lies in obtaining faithful,
instance-specific captions at scale. This requirement has limited the
utilization of natural language supervision in organismal biology compared with
many other scientific domains. We complement this gap by generating synthetic
captions with multimodal large language models (MLLMs), guided by
Wikipedia-derived visual information and taxon-tailored format examples. These
domain-specific contexts help reduce hallucination and yield accurate,
instance-based descriptive captions. Using these captions, we train BIOCAP
(i.e., BIOCLIP with Captions), a biological foundation model that captures rich
semantics and achieves strong performance in species classification and
text-image retrieval. These results demonstrate the value of descriptive
captions beyond labels in bridging biological images with multimodal foundation
models.</p>
<h3 id="7-breakdance-video-classification-in-the-age-of-generative-ai">[7] <a href="https://arxiv.org/abs/2510.20287">Breakdance Video classification in the age of Generative AI</a></h3>
<p><em>Sauptik Dhar, Naveen Ramakrishnan, Michelle Munson</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶è¯„ä¼°äº†ç°ä»£è§†é¢‘åŸºç¡€æ¨¡å‹åœ¨è¡—èˆè¿åŠ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°è§†é¢‘ç¼–ç å™¨æ¨¡å‹åœ¨é¢„æµ‹ä»»åŠ¡ä¸­æŒç»­ä¼˜äºæœ€å…ˆè¿›çš„è§†é¢‘è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸ºè¡—èˆè§†é¢‘åˆ†ç±»æä¾›äº†æ¨¡å‹é€‰æ‹©å’Œå¾®è°ƒç­–ç•¥çš„æ·±å…¥åˆ†æã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸»è¦åº”ç”¨äºä¸»æµä½“è‚²é¡¹ç›®å¦‚è¶³çƒã€æ¿çƒã€ç¯®çƒç­‰ï¼Œä¸“æ³¨äºç”Ÿæˆå¼ä»»åŠ¡å¦‚è§†è§‰é—®ç­”å’Œé«˜å…‰ç”Ÿæˆï¼Œè€Œé’ˆå¯¹è¡—èˆç­‰å°ä¼—ä½†æµè¡Œçš„èˆè¹ˆä½“è‚²åº”ç”¨ç ”ç©¶ç›¸å¯¹ç¼ºä¹ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½å¹¶åˆ†æè§†é¢‘åŸºç¡€æ¨¡å‹åœ¨è¡—èˆé¢†åŸŸçš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨äº†ç°ä»£è§†é¢‘åŸºç¡€æ¨¡å‹ï¼ŒåŒ…æ‹¬ç¼–ç å™¨å’Œè§£ç å™¨ä¸¤ç§æ¶æ„ï¼Œå¯¹è¡—èˆè§†é¢‘åˆ†ç±»ä»»åŠ¡è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œé‡ç‚¹åˆ†æäº†ç¼–ç å™¨æ¨¡å‹çš„é€‰æ‹©ç­–ç•¥ä»¥åŠå¾®è°ƒè§£ç å™¨æ¨¡å‹åœ¨è¡—èˆè§†é¢‘åˆ†ç±»ä¸­çš„å·¥ä½œæœºåˆ¶ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè§†é¢‘ç¼–ç å™¨æ¨¡å‹åœ¨é¢„æµ‹ä»»åŠ¡ä¸­æŒç»­ä¼˜äºæœ€å…ˆè¿›çš„è§†é¢‘è¯­è¨€æ¨¡å‹ï¼Œç ”ç©¶æä¾›äº†ç¼–ç å™¨æ¨¡å‹é€‰æ‹©çš„æŒ‡å¯¼åŸåˆ™ï¼Œå¹¶å¯¹å¾®è°ƒè§£ç å™¨æ¨¡å‹åœ¨è¡—èˆè§†é¢‘åˆ†ç±»ä¸­çš„è¡¨ç°è¿›è¡Œäº†å…¨é¢åˆ†æã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå°ä¼—ä½“è‚²é¢†åŸŸçš„è§†é¢‘åˆ†ææä¾›äº†é‡è¦å‚è€ƒï¼Œå¼ºè°ƒç¼–ç å™¨æ¨¡å‹åœ¨é¢„æµ‹ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿åœ°ä½ï¼Œå¹¶ä¸ºè¡—èˆç­‰ä¸“ä¸šé¢†åŸŸçš„è§†é¢‘åˆ†ç±»ä»»åŠ¡æä¾›äº†å®ç”¨çš„æ¨¡å‹é€‰æ‹©å’Œå¾®è°ƒç­–ç•¥æŒ‡å¯¼ï¼Œæ¨åŠ¨äº†è§†é¢‘åŸºç¡€æ¨¡å‹åœ¨ä¸“ä¸šä½“è‚²åˆ†æä¸­çš„åº”ç”¨ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Large Vision Language models have seen huge application in several sports
use-cases recently. Most of these works have been targeted towards a limited
subset of popular sports like soccer, cricket, basketball etc; focusing on
generative tasks like visual question answering, highlight generation. This
work analyzes the applicability of the modern video foundation models (both
encoder and decoder) for a very niche but hugely popular dance sports -
breakdance. Our results show that Video Encoder models continue to outperform
state-of-the-art Video Language Models for prediction tasks. We provide
insights on how to choose the encoder model and provide a thorough analysis
into the workings of a finetuned decoder model for breakdance video
classification.</p>
<h3 id="8-revisiting-logit-distributions-for-reliable-out-of-distribution-detection">[8] <a href="https://arxiv.org/abs/2510.20134">Revisiting Logit Distributions for Reliable Out-of-Distribution Detection</a></h3>
<p><em>Jiachen Liang, Ruibing Hou, Minyang Hu, Hong Chang, Shiguang Shan, Xilin Chen</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºLogitGapï¼Œä¸€ç§æ–°é¢–çš„åˆ†å¸ƒå¤–æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡æ˜¾å¼åˆ©ç”¨æœ€å¤§logitä¸å…¶ä½™logitsä¹‹é—´çš„å…³ç³»æ¥å¢å¼ºåˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–æ ·æœ¬çš„å¯åˆ†ç¦»æ€§ï¼Œåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„åå¤„ç†æ–¹æ³•åœ¨åˆ†å¸ƒå¤–æ£€æµ‹ä¸­å¾€å¾€æœªå……åˆ†åˆ©ç”¨æ¨¡å‹logitsç©ºé—´ä¸­ä¸°å¯Œçš„åµŒå…¥ä¿¡æ¯ï¼Œè¿™é™åˆ¶äº†æ£€æµ‹æ€§èƒ½çš„è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>Method:</strong> LogitGapæ–¹æ³•é€šè¿‡åˆ†ææœ€å¤§logitä¸å…¶ä½™logitsçš„å…³ç³»æ¥å¢å¼ºå¯åˆ†ç¦»æ€§ï¼Œå¹¶å¼•å…¥æ— éœ€è®­ç»ƒçš„ç­–ç•¥è‡ªåŠ¨è¯†åˆ«logitsç©ºé—´ä¸­æœ€å…·ä¿¡æ¯é‡çš„å­é›†è¿›è¡Œè¯„åˆ†ã€‚</p>
<p><strong>Result:</strong> åœ¨è§†è§‰è¯­è¨€å’Œçº¯è§†è§‰æ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLogitGapåœ¨å¤šç§åˆ†å¸ƒå¤–æ£€æµ‹åœºæ™¯å’ŒåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†logitsç©ºé—´ä¸­æœ€å¤§logitä¸å…¶ä½™logitså…³ç³»çš„æœ‰æ•ˆåˆ©ç”¨èƒ½å¤Ÿæ˜¾è‘—æå‡åˆ†å¸ƒå¤–æ£€æµ‹æ€§èƒ½ï¼Œä¸ºåå¤„ç†æ–¹æ³•çš„æ”¹è¿›æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Out-of-distribution (OOD) detection is critical for ensuring the reliability
of deep learning models in open-world applications. While post-hoc methods are
favored for their efficiency and ease of deployment, existing approaches often
underexploit the rich information embedded in the model's logits space. In this
paper, we propose LogitGap, a novel post-hoc OOD detection method that
explicitly exploits the relationship between the maximum logit and the
remaining logits to enhance the separability between in-distribution (ID) and
OOD samples. To further improve its effectiveness, we refine LogitGap by
focusing on a more compact and informative subset of the logit space.
Specifically, we introduce a training-free strategy that automatically
identifies the most informative logits for scoring. We provide both theoretical
analysis and empirical evidence to validate the effectiveness of our approach.
Extensive experiments on both vision-language and vision-only models
demonstrate that LogitGap consistently achieves state-of-the-art performance
across diverse OOD detection scenarios and benchmarks. Code is available at
https://github.com/GIT-LJc/LogitGap.</p>
<h3 id="9-a-parameter-efficient-mixture-of-experts-framework-for-cross-modal-geo-localization">[9] <a href="https://arxiv.org/abs/2510.20291">A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization</a></h3>
<p><em>LinFeng Li, Jian Zhao, Zepeng Yang, Yuhang Song, Bojun Lin, Tianle Zhang, Yuchen Yuan, Chi Zhang, Xuelong Li</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºè·¨æ¨¡æ€æ— äººæœºå¯¼èˆªçš„è·èƒœè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡é¢†åŸŸå¯¹é½é¢„å¤„ç†æµç¨‹å’Œæ··åˆä¸“å®¶æ¡†æ¶è§£å†³äº†å¤šå¹³å°å¼‚æ„æ€§å’Œé¢†åŸŸå·®è·é—®é¢˜ï¼Œåœ¨RoboSense 2025 Track 4ä¸­å–å¾—äº†é¢†å…ˆæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è·¨æ¨¡æ€åœ°ç†å®šä½ä¸­çš„ä¸¤ä¸ªä¸»è¦éšœç¢ï¼šä¸¥é‡çš„å¹³å°é—´å¼‚æ„æ€§ï¼ˆå«æ˜Ÿ/æ— äººæœº/åœ°é¢å¹³å°ï¼‰ä»¥åŠé€šç”¨è®­ç»ƒæè¿°ä¸å¹³å°ç‰¹å®šæµ‹è¯•æŸ¥è¯¢ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œè¿™äº›å› ç´ é™åˆ¶äº†å¤šå¹³å°å›¾åƒæ£€ç´¢ç³»ç»Ÿçš„æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> æ–¹æ³•åŒ…æ‹¬é¢†åŸŸå¯¹é½é¢„å¤„ç†æµç¨‹ï¼ˆå¹³å°åˆ’åˆ†ã€å«æ˜Ÿå¢å¼ºã€æ–¹å‘è¯ç§»é™¤ï¼‰å’ŒåŸºäºLLMçš„æ ‡é¢˜ç²¾ç‚¼ç®¡é“ï¼Œä½¿ç”¨BGE-M3å’ŒEVA-CLIPåˆ†åˆ«å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼Œé€šè¿‡æ¸è¿›å¼ä¸¤é˜¶æ®µç¡¬è´Ÿæ ·æœ¬æŒ–æ˜ç­–ç•¥è®­ç»ƒä¸‰ä¸ªå¹³å°ä¸“å®¶ï¼Œå¹¶åœ¨æ¨ç†æ—¶èåˆå…¶å¾—åˆ†ã€‚</p>
<p><strong>Result:</strong> è¯¥ç³»ç»Ÿåœ¨å®˜æ–¹æ’è¡Œæ¦œä¸Šä½å±…é¦–ä½ï¼Œè¯æ˜äº†åœ¨å¼‚æ„è§†è§’ä¸‹å…·æœ‰é²æ£’çš„è·¨æ¨¡æ€åœ°ç†å®šä½èƒ½åŠ›ï¼ŒæˆåŠŸè§£å†³äº†å¤šå¹³å°å›¾åƒæ£€ç´¢ä¸­çš„é¢†åŸŸé€‚åº”é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜é¢†åŸŸå¯¹é½é¢„å¤„ç†å’Œæ··åˆä¸“å®¶æ¡†æ¶èƒ½æœ‰æ•ˆç¼“è§£è·¨å¹³å°å¼‚æ„æ€§å’Œé¢†åŸŸå·®è·é—®é¢˜ï¼Œä¸ºå¤šæ¨¡æ€åœ°ç†å®šä½ç³»ç»Ÿæä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†åœ¨çœŸå®åœºæ™¯ä¸­çš„é²æ£’æ€§èƒ½ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone
Navigation. The task retrieves the most relevant geo-referenced image from a
large multi-platform corpus (satellite/drone/ground) given a natural-language
query. Two obstacles are severe inter-platform heterogeneity and a domain gap
between generic training descriptions and platform-specific test queries. We
mitigate these with a domain-aligned preprocessing pipeline and a
Mixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite
augmentation, and removal of orientation words; (ii) an LLM-based caption
refinement pipeline to align textual semantics with the distinct visual
characteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we
train three platform experts using a progressive two-stage, hard-negative
mining strategy to enhance discriminative power, and fuse their scores at
inference. The system tops the official leaderboard, demonstrating robust
cross-modal geo-localization under heterogeneous viewpoints.</p>
<h3 id="10-tomcat-test-time-comprehensive-knowledge-accumulation-for-compositional-zero-shot-learning">[10] <a href="https://arxiv.org/abs/2510.20162">TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning</a></h3>
<p><em>Xudong Yan, Songhe Feng</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»„åˆé›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä»æ— ç›‘ç£æ•°æ®ä¸­ç§¯ç´¯æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€çš„å…¨é¢çŸ¥è¯†æ¥æ›´æ–°å¤šæ¨¡æ€åŸå‹ï¼Œè§£å†³äº†æµ‹è¯•æ—¶æ ‡ç­¾ç©ºé—´åˆ†å¸ƒåç§»å¸¦æ¥çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨é—­ä¸–ç•Œå’Œå¼€ä¸–ç•Œè®¾ç½®ä¸‹åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç»„åˆé›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•åœ¨æµ‹è¯•æ—¶é¢ä¸´æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œè¿™æºäºä»æœªè§è¿‡çš„å±æ€§-å¯¹è±¡ç»„åˆé‡æ–°ç»„åˆå¯¼è‡´çš„æ ‡ç­¾ç©ºé—´åˆ†å¸ƒåç§»ã€‚ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†è¿™ç§åˆ†å¸ƒå˜åŒ–ï¼Œé™åˆ¶äº†æ¨¡å‹å¯¹æ–°é¢–ç»„åˆçš„è¯†åˆ«èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŸºäºå¤šæ¨¡æ€åŸå‹æ›´æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”æ›´æ–°æƒé‡æ§åˆ¶åŸå‹è°ƒæ•´ç¨‹åº¦ï¼Œå¹¶å¼•å…¥åŠ¨æ€ä¼˜å…ˆçº§é˜Ÿåˆ—å­˜å‚¨é«˜ç½®ä¿¡åº¦å›¾åƒä»¥è·å–å†å²è§†è§‰çŸ¥è¯†ã€‚é‡‡ç”¨å¤šæ¨¡æ€ååŒè¡¨ç¤ºå­¦ä¹ å¯¹é½æ–‡æœ¬å’Œè§†è§‰åŸå‹ï¼Œç¡®ä¿å¤šæ¨¡æ€çŸ¥è¯†çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é—­ä¸–ç•Œå’Œå¼€ä¸–ç•Œè®¾ç½®ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†å„ç»„ä»¶å¯¹æ€§èƒ½æå‡çš„æœ‰æ•ˆè´¡çŒ®ï¼Œè¯æ˜äº†æ–¹æ³•çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨æ— ç›‘ç£æ•°æ®ç§¯ç´¯å¤šæ¨¡æ€çŸ¥è¯†å¯ä»¥æœ‰æ•ˆç¼“è§£ç»„åˆé›¶æ ·æœ¬å­¦ä¹ ä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚è‡ªé€‚åº”åŸå‹æ›´æ–°æœºåˆ¶å’Œå¤šæ¨¡æ€ååŒå­¦ä¹ ä¸ºå¤„ç†åŠ¨æ€æµ‹è¯•ç¯å¢ƒæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæœªæ¥ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Compositional Zero-Shot Learning (CZSL) aims to recognize novel
attribute-object compositions based on the knowledge learned from seen ones.
Existing methods suffer from performance degradation caused by the distribution
shift of label space at test time, which stems from the inclusion of unseen
compositions recombined from attributes and objects. To overcome the challenge,
we propose a novel approach that accumulates comprehensive knowledge in both
textual and visual modalities from unsupervised data to update multimodal
prototypes at test time. Building on this, we further design an adaptive update
weight to control the degree of prototype adjustment, enabling the model to
flexibly adapt to distribution shift during testing. Moreover, a dynamic
priority queue is introduced that stores high-confidence images to acquire
visual knowledge from historical images for inference. Considering the semantic
consistency of multimodal knowledge, we align textual and visual prototypes by
multimodal collaborative representation learning. Extensive experiments
indicate that our approach achieves state-of-the-art performance on four
benchmark datasets under both closed-world and open-world settings. Code will
be available at https://github.com/xud-yan/TOMCAT .</p>
<h3 id="11-span-continuous-modeling-of-suspicion-progression-for-temporal-intention-localization">[11] <a href="https://arxiv.org/abs/2510.20189">SPAN: Continuous Modeling of Suspicion Progression for Temporal Intention Localization</a></h3>
<p><em>Xinyi Hu, Yuran Wang, Yue Li, Wenxuan Liu, Zheng Wang</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†å¯ç–‘è¿›å±•åˆ†æç½‘ç»œï¼ˆSPANï¼‰ï¼Œå°†æ—¶åºæ„å›¾å®šä½ä»ç¦»æ•£åˆ†ç±»è½¬å˜ä¸ºè¿ç»­å›å½’ï¼Œèƒ½å¤Ÿæ•æ‰æ³¢åŠ¨æ¼”åŒ–çš„å¯ç–‘æ„å›¾ã€‚è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨HAIæ•°æ®é›†ä¸Šé™ä½MSE 19.8%ï¼Œæå‡å¹³å‡mAP 1.78%ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç¦»æ•£åˆ†ç±»æ–¹æ³•æ— æ³•æ•æ‰å¯ç–‘æ„å›¾çš„è¿ç»­ç‰¹æ€§ï¼Œé™åˆ¶äº†æ—©æœŸå¹²é¢„å’Œå¯è§£é‡Šæ€§ã€‚æ—¶åºæ„å›¾å®šä½éœ€è¦è¯†åˆ«ä¸åŒçº§åˆ«çš„å¯ç–‘æ„å›¾ä»¥æå‡è§†é¢‘ç›‘æ§å®‰å…¨æ€§ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å¤„ç†æ„å›¾çš„æ³¢åŠ¨å’Œæ¼”åŒ–è¿‡ç¨‹ã€‚</p>
<p><strong>Method:</strong> æå‡ºå¯ç–‘è¿›å±•åˆ†æç½‘ç»œï¼ˆSPANï¼‰ï¼ŒåŸºäºæ—¶åºç‚¹è¿‡ç¨‹ç†è®ºå»ºæ¨¡å¯ç–‘æ„å›¾çš„é•¿æœŸä¾èµ–æ€§å’Œç´¯ç§¯æ•ˆåº”ã€‚å¼•å…¥å¯ç–‘ç³»æ•°è°ƒåˆ¶æœºåˆ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯è°ƒæ•´å¯ç–‘ç³»æ•°ä»¥åæ˜ ä¸åŒå¯ç–‘åŠ¨ä½œçš„å½±å“å·®å¼‚ã€‚é‡‡ç”¨æ¦‚å¿µé”šå®šæ˜ å°„æ–¹æ³•å°†å¯ç–‘åŠ¨ä½œä¸é¢„å®šä¹‰æ„å›¾æ¦‚å¿µå…³è”ã€‚</p>
<p><strong>Result:</strong> åœ¨HAIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSPANæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒMSEé™ä½19.8%ï¼Œå¹³å‡mAPæå‡1.78%ã€‚åœ¨ä½é¢‘æ¡ˆä¾‹ä¸­mAPå¢ç›Šè¾¾2.74%ï¼Œè¯æ˜å…¶èƒ½æœ‰æ•ˆæ•æ‰ç»†å¾®è¡Œä¸ºå˜åŒ–ã€‚è¿ç»­å¯ç–‘å»ºæ¨¡æ–¹æ³•ç›¸æ¯”ç¦»æ•£åˆ†ç±»ç³»ç»Ÿèƒ½å®ç°æ›´æ—©æ£€æµ‹å’Œä¸»åŠ¨å¹²é¢„ã€‚</p>
<p><strong>Conclusion:</strong> è¿ç»­å¯ç–‘å»ºæ¨¡æ–¹æ³•æå¤§æå‡äº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æ—©æ£€æµ‹å¯ç–‘è¡Œä¸ºå¹¶å®ç°ä¸»åŠ¨å¹²é¢„ï¼Œä¸ºå®‰å…¨ç›‘æ§åº”ç”¨æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æ¦‚å¿µé”šå®šæ˜ å°„æ–¹æ³•åŒæ—¶æä¾›äº†å¯¹åŠ¨ä½œåŠå…¶æ½œåœ¨æ„å›¾çš„æ·±å…¥ç†è§£ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Temporal Intention Localization (TIL) is crucial for video surveillance,
focusing on identifying varying levels of suspicious intentions to improve
security monitoring. However, existing discrete classification methods fail to
capture the continuous nature of suspicious intentions, limiting early
intervention and explainability. In this paper, we propose the Suspicion
Progression Analysis Network (SPAN), which shifts from discrete classification
to continuous regression, enabling the capture of fluctuating and evolving
suspicious intentions. We reveal that suspicion exhibits long-term dependencies
and cumulative effects, similar to Temporal Point Process (TPP) theory. Based
on these insights, we define a suspicion score formula that models continuous
changes while accounting for temporal characteristics. We also introduce
Suspicion Coefficient Modulation, which adjusts suspicion coefficients using
multimodal information to reflect the varying impacts of suspicious actions.
Additionally, the Concept-Anchored Mapping method is proposed to link
suspicious actions to predefined intention concepts, offering insights into
both the actions and their potential underlying intentions. Extensive
experiments on the HAI dataset show that SPAN significantly outperforms
existing methods, reducing MSE by 19.8% and improving average mAP by 1.78%.
Notably, SPAN achieves a 2.74% mAP gain in low-frequency cases, demonstrating
its superior ability to capture subtle behavioral changes. Compared to discrete
classification systems, our continuous suspicion modeling approach enables
earlier detection and proactive intervention, greatly enhancing system
explainability and practical utility in security applications.</p>
<h3 id="12-calibrating-multimodal-consensus-for-emotion-recognition">[12] <a href="https://arxiv.org/abs/2510.20256">Calibrating Multimodal Consensus for Emotion Recognition</a></h3>
<p><em>Guowei Zhong, Junjie Li, Huaiyu Zhu, Ruohong Huan, Yun Pan</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ ¡å‡†å¤šæ¨¡æ€å…±è¯†ï¼ˆCMCï¼‰çš„æ¨¡å‹ï¼Œé€šè¿‡ä¼ªæ ‡ç­¾ç”Ÿæˆæ¨¡å—å®ç°è‡ªç›‘ç£å•æ¨¡æ€é¢„è®­ç»ƒï¼Œå¹¶é‡‡ç”¨å‚æ•°æ— å…³èåˆæ¨¡å—å’Œå¤šæ¨¡æ€å…±è¯†è·¯ç”±å™¨æ¥è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„è¯­ä¹‰ä¸ä¸€è‡´æ€§å’Œæ–‡æœ¬æ¨¡æ€ä¸»å¯¼é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•æ™®éå¿½è§†æ¨¡æ€é—´çš„è¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜ï¼Œä¾‹å¦‚æ–‡æœ¬ä¸è§†è§‰è¾“å…¥ä¹‹é—´å¯èƒ½å­˜åœ¨å†²çªçš„æƒ…æ„Ÿçº¿ç´¢ï¼ŒåŒæ—¶ç°æœ‰æ–¹æ³•ç”±äºæ–‡æœ¬æ¨¡æ€çš„å¼ºå¤§è¡¨ç¤ºèƒ½åŠ›è€Œå¾€å¾€è¢«å…¶ä¸»å¯¼ï¼Œè¿™ä¼šæŸå®³è¯†åˆ«å‡†ç¡®æ€§ã€‚</p>
<p><strong>Method:</strong> CMCæ¨¡å‹åŒ…å«ä¼ªæ ‡ç­¾ç”Ÿæˆæ¨¡å—ï¼ˆPLGMï¼‰ç”¨äºç”Ÿæˆä¼ªå•æ¨¡æ€æ ‡ç­¾ä»¥å®ç°è‡ªç›‘ç£å•æ¨¡æ€é¢„è®­ç»ƒï¼Œå‚æ•°æ— å…³èåˆæ¨¡å—ï¼ˆPFMï¼‰ç”¨äºå¤šæ¨¡æ€å¾®è°ƒï¼Œä»¥åŠå¤šæ¨¡æ€å…±è¯†è·¯ç”±å™¨ï¼ˆMCRï¼‰æ¥å¼•å¯¼èåˆè¿‡ç¨‹è¾¾æˆæ›´å¯é çš„å…±è¯†ï¼Œä»è€Œç¼“è§£æ–‡æœ¬ä¸»å¯¼é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜CMCåœ¨å››ä¸ªæ•°æ®é›†ï¼ˆCH-SIMSã€CH-SIMS v2ã€CMU-MOSIå’ŒCMU-MOSEIï¼‰ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ï¼Œåœ¨CH-SIMSå’ŒCH-SIMS v2æ•°æ®é›†ä¸Šå¯¹è¯­ä¹‰ä¸ä¸€è‡´åœºæ™¯è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è‡ªç›‘ç£å•æ¨¡æ€é¢„è®­ç»ƒå’Œå…±è¯†å¼•å¯¼çš„å¤šæ¨¡æ€èåˆæœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆè§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„è¯­ä¹‰ä¸ä¸€è‡´å’Œæ¨¡æ€ä¸»å¯¼é—®é¢˜ï¼Œä¸ºå¤šæ¨¡æ€å­¦ä¹ æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>In recent years, Multimodal Emotion Recognition (MER) has made substantial
progress. Nevertheless, most existing approaches neglect the semantic
inconsistencies that may arise across modalities, such as conflicting emotional
cues between text and visual inputs. Besides, current methods are often
dominated by the text modality due to its strong representational capacity,
which can compromise recognition accuracy. To address these challenges, we
propose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a
Pseudo Label Generation Module (PLGM) to produce pseudo unimodal labels,
enabling unimodal pretraining in a self-supervised fashion. It then employs a
Parameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for
multimodal finetuning, thereby mitigating text dominance and guiding the fusion
process toward a more reliable consensus. Experimental results demonstrate that
CMC achieves performance on par with or superior to state-of-the-art methods
across four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and
exhibits notable advantages in scenarios with semantic inconsistencies on
CH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible
at https://github.com/gw-zhong/CMC.</p>
<h3 id="13-metis-home-hybrid-optimized-mixture-of-experts-for-multimodal-reasoning">[13] <a href="https://arxiv.org/abs/2510.20519">Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning</a></h3>
<p><em>Xiaohan Lan, Fanfan Liu, Haibo Qiu, Siqi Yang, Delian Ruan, Peng Shi, Lin Ma</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMetis-HOMEæ¡†æ¶ï¼Œé€šè¿‡æ··åˆä¼˜åŒ–çš„ä¸“å®¶æ··åˆæ¶æ„å®ç°"æ··åˆæ€ç»´"èŒƒå¼ï¼Œå°†å¯†é›†æ¨¡å‹åˆ†è§£ä¸ºæ€è€ƒåˆ†æ”¯å’Œéæ€è€ƒåˆ†æ”¯ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€å¤§æ¨ç†æ¨¡å‹ä¸­æ¨ç†æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§æ¨ç†æ¨¡å‹å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™ï¼šå¯¹ç®€å•æŸ¥è¯¢ä¹Ÿé‡‡ç”¨è®¡ç®—æ˜‚è´µçš„æ¨ç†è¿‡ç¨‹å¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œä»¥åŠä¸“æ³¨äºä¸“é—¨åŒ–æ¨ç†å¾€å¾€æŸå®³å…¶æ›´å¹¿æ³›çš„é€šç”¨ç†è§£èƒ½åŠ›ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ç§æ¨ç†ä¸æ³›åŒ–ä¹‹é—´çš„æƒè¡¡å›°å¢ƒã€‚</p>
<p><strong>Method:</strong> æå‡ºMetis-HOMEæ··åˆä¼˜åŒ–ä¸“å®¶æ··åˆæ¡†æ¶ï¼Œå°†åŸå§‹å¯†é›†æ¨¡å‹ç»“æ„åŒ–åˆ†ä¸ºä¸¤ä¸ªä¸“å®¶åˆ†æ”¯ï¼šä¸“ä¸ºå¤æ‚å¤šæ­¥æ¨ç†è®¾è®¡çš„æ€è€ƒåˆ†æ”¯å’Œé’ˆå¯¹é€šç”¨VQAåŠOCRç­‰ä»»åŠ¡ä¼˜åŒ–çš„å¿«é€Ÿç›´æ¥æ¨ç†éæ€è€ƒåˆ†æ”¯ï¼Œé€šè¿‡è½»é‡çº§å¯è®­ç»ƒè·¯ç”±å™¨åŠ¨æ€åˆ†é…æŸ¥è¯¢åˆ°æœ€åˆé€‚çš„ä¸“å®¶ï¼ŒåŸºäºQwen2.5-VL-7Bå®ä¾‹åŒ–ä¸ºMoEæ¶æ„ã€‚</p>
<p><strong>Result:</strong> ç»¼åˆè¯„ä¼°è¡¨æ˜è¯¥æ–¹æ³•ä¸ä»…æ˜¾è‘—å¢å¼ºäº†å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œè¿˜æ”¹å–„äº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ï¼Œé€†è½¬äº†å…¶ä»–æ¨ç†ä¸“é—¨åŒ–æ¨¡å‹ä¸­è§‚å¯Ÿåˆ°çš„æ€§èƒ½é€€åŒ–è¶‹åŠ¿ï¼Œåœ¨ä¿æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶æå‡äº†æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶ä¸ºæ„å»ºå¼ºå¤§ä¸”é€šç”¨çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å»ºç«‹äº†æ–°èŒƒå¼ï¼Œæœ‰æ•ˆè§£å†³äº†æ™®éå­˜åœ¨çš„æ¨ç†ä¸æ³›åŒ–å›°å¢ƒï¼Œè¯æ˜äº†æ··åˆæ€ç»´æ¶æ„åœ¨å¹³è¡¡ä¸“é—¨åŒ–æ¨ç†ä¸é€šç”¨èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æ¨¡å‹è®¾è®¡æä¾›äº†é‡è¦å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Inspired by recent advancements in LLM reasoning, the field of multimodal
reasoning has seen remarkable progress, achieving significant performance gains
on intricate tasks such as mathematical problem-solving. Despite this progress,
current multimodal large reasoning models exhibit two key limitations. They
tend to employ computationally expensive reasoning even for simple queries,
leading to inefficiency. Furthermore, this focus on specialized reasoning often
impairs their broader, more general understanding capabilities. In this paper,
we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed
to address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by
structuring the original dense model into two distinct expert branches: a
thinking branch tailored for complex, multi-step reasoning, and a non-thinking
branch optimized for rapid, direct inference on tasks like general VQA and OCR.
A lightweight, trainable router dynamically allocates queries to the most
suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into
an MoE architecture. Comprehensive evaluations reveal that our approach not
only substantially enhances complex reasoning abilities but also improves the
model's general capabilities, reversing the degradation trend observed in other
reasoning-specialized models. Our work establishes a new paradigm for building
powerful and versatile MLLMs, effectively resolving the prevalent
reasoning-vs-generalization dilemma.</p>
<h3 id="14-flowcycle-pursuing-cycle-consistent-flows-for-text-based-editing">[14] <a href="https://arxiv.org/abs/2510.20212">FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing</a></h3>
<p><em>Yanghao Wang, Zhen Wang, Long Chen</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºFlowCycleï¼Œä¸€ç§åŸºäºæµçš„å…åæ¼”å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œé€šè¿‡ç›®æ ‡æ„ŸçŸ¥çš„ä¸­é—´çŠ¶æ€æ„å»ºå’Œå¾ªç¯ä¸€è‡´æ€§ä¼˜åŒ–ï¼Œè§£å†³äº†ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨ç›®æ ‡æ— å…³çš„ä¸­é—´çŠ¶æ€æ„å»ºå¯¼è‡´çš„ç¼–è¾‘é™åˆ¶å’Œä¸ä¸€è‡´é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°å›¾åƒç¼–è¾‘æ–¹æ³•é‡‡ç”¨ç›®æ ‡æ— å…³çš„ä¸­é—´çŠ¶æ€æ„å»ºæ–¹å¼ï¼Œä¸»è¦å…³æ³¨æºå›¾åƒé‡å»ºè€Œå¿½ç•¥äº†ä¸ç‰¹å®šç¼–è¾‘ç›®æ ‡ä¹‹é—´çš„è¯­ä¹‰å·®è·ï¼Œè¿™å¯¼è‡´å½“æœŸæœ›ä¿®æ”¹ä¸æºå›¾åƒæ˜¾è‘—åç¦»æ—¶å‡ºç°æœ‰é™çš„ç¼–è¾‘èƒ½åŠ›æˆ–ä¸ä¸€è‡´é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> FlowCycleæ¡†æ¶é€šè¿‡å‚æ•°åŒ–å¯å­¦ä¹ å™ªå£°æ¥æ„å»ºç›®æ ‡æ„ŸçŸ¥çš„ä¸­é—´çŠ¶æ€ï¼Œé‡‡ç”¨å¾ªç¯ä¸€è‡´æ€§è¿‡ç¨‹è¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡ä»æºåˆ°ç›®æ ‡çš„è¿­ä»£ç¼–è¾‘å’Œä»ç›®æ ‡å›æºçš„åŒå‘ä¸€è‡´æ€§çº¦æŸï¼Œå­¦ä¹ ç”Ÿæˆç›®æ ‡æ„ŸçŸ¥çš„ä¸­é—´çŠ¶æ€ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›çš„æ¶ˆèå®éªŒè¡¨æ˜ï¼ŒFlowCycleåœ¨ç¼–è¾‘è´¨é‡å’Œä¸€è‡´æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå®ç°äº†å¿ å®çš„ä¿®æ”¹åŒæ—¶ä¿æŒæºå›¾åƒçš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç›®æ ‡æ„ŸçŸ¥ä¸­é—´çŠ¶æ€æ„å»ºçš„é‡è¦æ€§ï¼Œä¸ºæ–‡æœ¬åˆ°å›¾åƒç¼–è¾‘æä¾›äº†æ–°çš„ä¼˜åŒ–èŒƒå¼ï¼Œé€šè¿‡å¾ªç¯ä¸€è‡´æ€§å­¦ä¹ å®ç°äº†ç¼–è¾‘ç›¸å…³å†…å®¹çš„æ™ºèƒ½é€‰æ‹©ä¿ç•™å’Œä¿®æ”¹ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Recent advances in pre-trained text-to-image flow models have enabled
remarkable progress in text-based image editing. Mainstream approaches always
adopt a corruption-then-restoration paradigm, where the source image is first
corrupted into an ``intermediate state'' and then restored to the target image
under the prompt guidance. However, current methods construct this intermediate
state in a target-agnostic manner, i.e., they primarily focus on realizing
source image reconstruction while neglecting the semantic gaps towards the
specific editing target. This design inherently results in limited editability
or inconsistency when the desired modifications substantially deviate from the
source. In this paper, we argue that the intermediate state should be
target-aware, i.e., selectively corrupting editing-relevant contents while
preserving editing-irrelevant ones. To this end, we propose FlowCycle, a novel
inversion-free and flow-based editing framework that parameterizes corruption
with learnable noises and optimizes them through a cycle-consistent process. By
iteratively editing the source to the target and recovering back to the source
with dual consistency constraints, FlowCycle learns to produce a target-aware
intermediate state, enabling faithful modifications while preserving source
consistency. Extensive ablations have demonstrated that FlowCycle achieves
superior editing quality and consistency over state-of-the-art methods.</p>
<h3 id="15-small-drafts-big-verdict-information-intensive-visual-reasoning-via-speculation">[15] <a href="https://arxiv.org/abs/2510.20812">Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</a></h3>
<p><em>Yuhan Liu, Lianhui Qin, Shengjie Wang</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSpeculative Verdict (SV)æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªè½»é‡çº§è‰ç¨¿ä¸“å®¶å’Œå¤§å‹è£å†³æ¨¡å‹ï¼Œè§£å†³äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¿¡æ¯å¯†é›†å‹å›¾åƒä¸Šçš„æ¨ç†æŒ‘æˆ˜ï¼Œå®ç°äº†é”™è¯¯æ ¡æ­£å’Œè®¡ç®—æ•ˆç‡çš„å¹³è¡¡ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¿¡æ¯å¯†é›†å‹å›¾åƒä¸Šè¡¨ç°ä¸ä½³ï¼Œè¿™äº›å›¾åƒå¯†é›†äº¤ç»‡æ–‡æœ¬æ ‡æ³¨ä¸ç»†ç²’åº¦å›¾å½¢å…ƒç´ ï¼Œä¸»è¦æŒ‘æˆ˜åœ¨äºç²¾ç¡®å®šä½å¯†é›†å¸ƒå±€ä¸­çš„å…³é”®çº¿ç´¢ä»¥åŠæ•´åˆåˆ†æ•£è¯æ®çš„å¤šè·³æ¨ç†ã€‚</p>
<p><strong>Method:</strong> SVæ¡†æ¶é‡‡ç”¨è®­ç»ƒå…è´¹çš„æ¨æµ‹è§£ç æ–¹æ³•ï¼Œåœ¨è‰ç¨¿é˜¶æ®µä½¿ç”¨å°å‹VLMä½œä¸ºè‰ç¨¿ä¸“å®¶ç”Ÿæˆå¤šæ ·åŒ–çš„å®šä½å€™é€‰æ¨ç†è·¯å¾„ï¼Œåœ¨è£å†³é˜¶æ®µç”±å¼ºå¤§VLMåˆæˆè¿™äº›è·¯å¾„äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆï¼Œå¹¶å¼•å…¥å…±è¯†ä¸“å®¶é€‰æ‹©æœºåˆ¶ä»…è½¬å‘é«˜ä¸€è‡´æ€§æ¨ç†è·¯å¾„ä»¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¿¡æ¯å¯†é›†å’Œé«˜åˆ†è¾¨ç‡è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŒ…æ‹¬InfographicVQAã€ChartMuseumã€ChartQAProå’ŒHR-Bench 4Kï¼ŒSVå®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œé€šè¿‡åˆæˆå¤šä¸ªéƒ¨åˆ†å‡†ç¡®æ¨ç†è·¯å¾„ä¸­çš„æ­£ç¡®è§è§£ï¼Œç›¸æ¯”å¤§å‹ä¸“æœ‰æ¨¡å‹æˆ–è®­ç»ƒæµç¨‹å®ç°äº†é”™è¯¯æ ¡æ­£å’Œæˆæœ¬æ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡åˆæˆå¤šä¸ªéƒ¨åˆ†å‡†ç¡®æ¨ç†è·¯å¾„å¯ä»¥æœ‰æ•ˆåœ°è¿›è¡Œé”™è¯¯æ ¡æ­£ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œä¸ºå¤„ç†ä¿¡æ¯å¯†é›†å‹è§†è§‰å†…å®¹æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†è½»é‡çº§æ¨¡å‹ä¸å¤§å‹æ¨¡å‹ååŒå·¥ä½œçš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Large Vision-Language Models (VLMs) have achieved remarkable progress in
multimodal understanding, yet they struggle when reasoning over
information-intensive images that densely interleave textual annotations with
fine-grained graphical elements. The main challenges lie in precisely
localizing critical cues in dense layouts and multi-hop reasoning to integrate
dispersed evidence. We propose Speculative Verdict (SV), a training-free
framework inspired by speculative decoding that combines multiple lightweight
draft experts with a large verdict model. In the draft stage, small VLMs act as
draft experts to generate reasoning paths that provide diverse localization
candidates; in the verdict stage, a strong VLM synthesizes these paths to
produce the final answer, minimizing computational cost while recovering
correct answers. To further improve efficiency and accuracy, SV introduces a
consensus expert selection mechanism that forwards only high-agreement
reasoning paths to the verdict. Empirically, SV achieves consistent gains on
challenging information-intensive and high-resolution visual question answering
benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.
By synthesizing correct insights from multiple partially accurate reasoning
paths, SV achieves both error correction and cost-efficiency compared to large
proprietary models or training pipelines. Code is available at
https://github.com/Tinaliu0123/speculative-verdict</p>
<h3 id="16-fake-in-facext-towards-fine-grained-explainable-deepfake-analysis">[16] <a href="https://arxiv.org/abs/2510.20531">Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis</a></h3>
<p><em>Lixiong Qin, Yang Zhang, Mei Wang, Jiani Hu, Weihong Deng, Weiran Xu</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Fake-in-Facextæ¡†æ¶ï¼Œé€šè¿‡å®šä¹‰ç»†ç²’åº¦é¢éƒ¨æ¦‚å¿µæ ‘å’Œæ„å»ºå¤šä»»åŠ¡å­¦ä¹ æ¶æ„ï¼Œè§£å†³äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¯è§£é‡Šæ·±åº¦ä¼ªé€ åˆ†æä¸­ç¼ºä¹ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›çš„é—®é¢˜ï¼Œåœ¨ä¼ªé€ è§£é‡Šä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¯è§£é‡Šæ·±åº¦ä¼ªé€ åˆ†æä¸­å­˜åœ¨ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå…·ä½“è¡¨ç°ä¸ºæ•°æ®æ ‡æ³¨ä¸­å¯¹ä¼ªé€ ç—•è¿¹çš„æè¿°ä¸å¯é ä¸”ç²’åº¦ç²—ç³™ï¼Œæ¨¡å‹æ— æ³•è¾“å‡ºæ–‡æœ¬ä¼ªé€ è§£é‡Šä¸è§†è§‰ä¼ªé€ è¯æ®ä¹‹é—´çš„å…³è”ï¼Œä¹Ÿä¸æ”¯æŒå¯¹ä»»æ„é¢éƒ¨åŒºåŸŸçš„æŸ¥è¯¢è¾“å…¥ï¼Œå¯¼è‡´å…¶å“åº”ç¼ºä¹é¢éƒ¨è§†è§‰ä¸Šä¸‹æ–‡çš„å……åˆ†æ”¯æ’‘ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†Fake-in-Facextæ¡†æ¶ï¼Œé¦–å…ˆå®šä¹‰äº†é¢éƒ¨å›¾åƒæ¦‚å¿µæ ‘å°†é¢éƒ¨å›¾åƒåˆ’åˆ†ä¸ºç»†ç²’åº¦åŒºåŸŸæ¦‚å¿µï¼Œæ„å»ºäº†æ›´å¯é çš„æ•°æ®æ ‡æ³¨æµç¨‹FiFa-Annotatorï¼›åœ¨æ­¤åŸºç¡€ä¸Šå¼•å…¥äº†æ–°çš„ä¼ªé€ å®šä½è§£é‡Šä»»åŠ¡ï¼Œç”Ÿæˆä¸åˆ†å‰²æ©ç äº¤ç»‡çš„æ–‡æœ¬ä¼ªé€ è§£é‡Šï¼›å¼€å‘äº†ç»Ÿä¸€çš„å¤šä»»åŠ¡å­¦ä¹ æ¶æ„FiFa-MLLMï¼ŒåŒæ—¶æ”¯æŒä¸°å¯Œçš„å¤šæ¨¡æ€è¾“å…¥è¾“å‡ºã€‚</p>
<p><strong>Result:</strong> é€šè¿‡å¤šä¸ªè¾…åŠ©ç›‘ç£ä»»åŠ¡ï¼ŒFiFa-MLLMåœ¨ä¼ªé€ å®šä½è§£é‡Šä»»åŠ¡ä¸Šè¶…è¶Šäº†å¼ºåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨ç°æœ‰å¯è§£é‡Šæ·±åº¦ä¼ªé€ åˆ†ææ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†ç»†ç²’åº¦é¢éƒ¨æ¦‚å¿µåˆ’åˆ†å’Œç»Ÿä¸€å¤šä»»åŠ¡æ¶æ„åœ¨æå‡æ·±åº¦ä¼ªé€ åˆ†æå¯è§£é‡Šæ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºæ›´å¯é çš„å¯è§£é‡ŠAIç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œç›¸å…³ä»£ç å’Œæ•°æ®å°†å¼€æºä»¥ä¿ƒè¿›ç¤¾åŒºå‘å±•ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>The advancement of Multimodal Large Language Models (MLLMs) has bridged the
gap between vision and language tasks, enabling the implementation of
Explainable DeepFake Analysis (XDFA). However, current methods suffer from a
lack of fine-grained awareness: the description of artifacts in data annotation
is unreliable and coarse-grained, and the models fail to support the output of
connections between textual forgery explanations and the visual evidence of
artifacts, as well as the input of queries for arbitrary facial regions. As a
result, their responses are not sufficiently grounded in Face Visual Context
(Facext). To address this limitation, we propose the Fake-in-Facext (FiFa)
framework, with contributions focusing on data annotation and model
construction. We first define a Facial Image Concept Tree (FICT) to divide
facial images into fine-grained regional concepts, thereby obtaining a more
reliable data annotation pipeline, FiFa-Annotator, for forgery explanation.
Based on this dedicated data annotation, we introduce a novel
Artifact-Grounding Explanation (AGE) task, which generates textual forgery
explanations interleaved with segmentation masks of manipulated artifacts. We
propose a unified multi-task learning architecture, FiFa-MLLM, to
simultaneously support abundant multimodal inputs and outputs for fine-grained
Explainable DeepFake Analysis. With multiple auxiliary supervision tasks,
FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA
performance on existing XDFA datasets. The code and data will be made
open-source at https://github.com/lxq1000/Fake-in-Facext.</p>
<h3 id="17-towards-objective-obstetric-ultrasound-assessment-contrastive-representation-learning-for-fetal-movement-detection">[17] <a href="https://arxiv.org/abs/2510.20214">Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection</a></h3>
<p><em>Talha Ilyas, Duong Nhu, Allison Thomas, Arie Levin, Lim Wei Yap, Shu Gong, David Vera Anaya, Yiwen Jiang, Deval Mehta, Ritesh Warty, Vinayak Smith, Maya Reddy, Euan Wallace, Wenlong Cheng, Zongyuan Ge, Faezeh Marzbanrad</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†CURLï¼ˆå¯¹æ¯”è¶…å£°è§†é¢‘è¡¨ç¤ºå­¦ä¹ ï¼‰æ¡†æ¶ï¼Œé€šè¿‡è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ä»èƒå„¿è¶…å£°è§†é¢‘ä¸­æ£€æµ‹èƒå„¿è¿åŠ¨ï¼Œåœ¨92åå—è¯•è€…çš„æ•°æ®é›†ä¸Šå®ç°äº†78.01%çš„æ•æ„Ÿåº¦å’Œ81.60%çš„AUROCï¼Œä¸ºäº§å‰ç›‘æµ‹æä¾›äº†å¯é çš„å®¢è§‚åˆ†ææ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿèƒå„¿è¿åŠ¨æ£€æµ‹æ–¹æ³•å¦‚æ¯ä½“æ„ŸçŸ¥å’Œèƒå¿ƒç›‘æŠ¤å­˜åœ¨ä¸»è§‚æ€§å¼ºå’Œå‡†ç¡®æ€§æœ‰é™çš„é—®é¢˜ï¼Œå¼‚å¸¸è¿åŠ¨æ¨¡å¼å¯èƒ½æŒ‡ç¤ºèƒç›˜åŠŸèƒ½éšœç¢æˆ–èƒå„¿çª˜è¿«ç­‰å¹¶å‘ç—‡ï¼Œéœ€è¦å¼€å‘æ›´å¯é çš„å®¢è§‚æ£€æµ‹æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†CURLè‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨åŒé‡å¯¹æ¯”æŸå¤±ç»“åˆç©ºé—´å’Œæ—¶é—´å¯¹æ¯”å­¦ä¹ æ¥å­¦ä¹ é²æ£’çš„è¿åŠ¨è¡¨ç¤ºï¼Œå¹¶å¼•å…¥ä»»åŠ¡ç‰¹å®šé‡‡æ ·ç­–ç•¥æœ‰æ•ˆåˆ†ç¦»è¿åŠ¨å’Œéè¿åŠ¨ç‰‡æ®µï¼Œé€šè¿‡æ¦‚ç‡å¾®è°ƒæ–¹æ³•å®ç°ä»»æ„é•¿åº¦è¶…å£°è®°å½•ä¸Šçš„çµæ´»æ¨ç†ã€‚</p>
<p><strong>Result:</strong> åœ¨åŒ…å«92åå—è¯•è€…ã€æ¯ä¾‹30åˆ†é’Ÿè¶…å£°ä¼šè¯çš„å†…éƒ¨æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒCURLå®ç°äº†78.01%çš„æ•æ„Ÿåº¦å’Œ81.60%çš„AUROCï¼Œè¯æ˜äº†å…¶åœ¨èƒå„¿è¿åŠ¨åˆ†æä¸­çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯æ˜äº†è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ åœ¨èƒå„¿è¿åŠ¨åˆ†æä¸­çš„æ½œåŠ›ï¼Œä¸ºæ”¹è¿›äº§å‰ç›‘æµ‹å’Œä¸´åºŠå†³ç­–æä¾›äº†æ–°é€”å¾„ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæä¾›å®¢è§‚å¯é çš„èƒå„¿è¿åŠ¨è¯„ä¼°ï¼Œæœ‰æœ›åœ¨ä¸´åºŠå®è·µä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Accurate fetal movement (FM) detection is essential for assessing prenatal
health, as abnormal movement patterns can indicate underlying complications
such as placental dysfunction or fetal distress. Traditional methods, including
maternal perception and cardiotocography (CTG), suffer from subjectivity and
limited accuracy. To address these challenges, we propose Contrastive
Ultrasound Video Representation Learning (CURL), a novel self-supervised
learning framework for FM detection from extended fetal ultrasound video
recordings. Our approach leverages a dual-contrastive loss, incorporating both
spatial and temporal contrastive learning, to learn robust motion
representations. Additionally, we introduce a task-specific sampling strategy,
ensuring the effective separation of movement and non-movement segments during
self-supervised training, while enabling flexible inference on arbitrarily long
ultrasound recordings through a probabilistic fine-tuning approach. Evaluated
on an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions,
CURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating its
potential for reliable and objective FM analysis. These results highlight the
potential of self-supervised contrastive learning for fetal movement analysis,
paving the way for improved prenatal monitoring and clinical decision-making.</p>
<h3 id="18-open-o3-video-grounded-video-reasoning-with-explicit-spatio-temporal-evidence">[18] <a href="https://arxiv.org/abs/2510.20579">Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</a></h3>
<p><em>Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Open-o3 Videoæ¡†æ¶ï¼Œå°†æ˜¾å¼æ—¶ç©ºè¯æ®æ•´åˆåˆ°è§†é¢‘æ¨ç†ä¸­ï¼Œé€šè¿‡ç²¾å¿ƒæ„å»ºçš„æ•°æ®é›†å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œåœ¨V-STARåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›å¯éªŒè¯çš„æ¨ç†è½¨è¿¹ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘æ¨ç†æ¨¡å‹ä»…ç”Ÿæˆæ–‡æœ¬æ¨ç†è½¨è¿¹ï¼Œæ— æ³•æŒ‡ç¤ºå…³é”®è¯æ®å‡ºç°çš„æ—¶é—´å’Œä½ç½®ï¼Œè€Œå°†è¯æ®ä¸­å¿ƒæ¨ç†èƒ½åŠ›æ‰©å±•åˆ°è§†é¢‘é¢ä¸´è”åˆæ—¶é—´è·Ÿè¸ªå’Œç©ºé—´å®šä½çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ•°æ®é›†ç¼ºä¹ç»Ÿä¸€çš„æ—¶ç©ºç›‘ç£å’Œæ¨ç†è½¨è¿¹ã€‚</p>
<p><strong>Method:</strong> æå‡ºéæ™ºèƒ½ä½“æ¡†æ¶Open-o3 Videoï¼Œæ•´åˆæ˜¾å¼æ—¶ç©ºè¯æ®åˆ°è§†é¢‘æ¨ç†ä¸­ï¼›æ„å»ºä¸¤ä¸ªé«˜è´¨é‡æ•°æ®é›†STGR-CoT-30kç”¨äºSFTå’ŒSTGR-RL-36kç”¨äºRLï¼ŒåŒ…å«ç²¾å¿ƒæ„å»ºçš„æ—¶ç©ºæ ‡æ³¨ï¼›é‡‡ç”¨å†·å¯åŠ¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè®¾è®¡å¤šä¸ªä¸“é—¨å¥–åŠ±å‡½æ•°è”åˆä¿ƒè¿›ç­”æ¡ˆå‡†ç¡®æ€§ã€æ—¶é—´å¯¹é½å’Œç©ºé—´ç²¾åº¦ã€‚</p>
<p><strong>Result:</strong> åœ¨V-STARåŸºå‡†ä¸Šå®ç°æœ€å…ˆè¿›æ€§èƒ½ï¼Œç›¸æ¯”Qwen2.5-VLåŸºçº¿å°†mAMæå‡14.4%ï¼ŒmLGMæå‡24.2%ï¼›åœ¨VideoMMEã€WorldSenseã€VideoMMMUå’ŒTVGBenchç­‰å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†ä¸Šè§‚å¯Ÿåˆ°ä¸€è‡´æ”¹è¿›ï¼›æ¨ç†è½¨è¿¹ä¸ºæµ‹è¯•æ—¶ç¼©æ”¾æä¾›æœ‰ä»·å€¼ä¿¡å·ï¼Œå®ç°ç½®ä¿¡åº¦æ„ŸçŸ¥éªŒè¯å¹¶æé«˜ç­”æ¡ˆå¯é æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å°†æ˜¾å¼æ—¶ç©ºè¯æ®æ•´åˆåˆ°è§†é¢‘æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ä»…æå‡äº†æ€§èƒ½è¿˜æä¾›äº†å¯éªŒè¯çš„æ¨ç†è¿‡ç¨‹ï¼›æå‡ºçš„æ•°æ®é›†æ„å»ºæ–¹æ³•å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¸ºè§£å†³è§†é¢‘æ—¶ç©ºæ¨ç†æŒ‘æˆ˜æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼›æ¨ç†è½¨è¿¹çš„ç½®ä¿¡åº¦æ„ŸçŸ¥èƒ½åŠ›ä¸ºå®é™…åº”ç”¨ä¸­çš„å¯é æ€§éªŒè¯å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Most video reasoning models only generate textual reasoning traces without
indicating when and where key evidence appears. Recent models such as OpenAI-o3
have sparked wide interest in evidence-centered reasoning for images, yet
extending this ability to videos is more challenging, as it requires joint
temporal tracking and spatial localization across dynamic scenes. We introduce
Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal
evidence into video reasoning, and carefully collect training data and design
training strategies to address the aforementioned challenges. The model
highlights key timestamps, objects, and bounding boxes alongside its answers,
allowing reasoning to be grounded in concrete visual observations. To enable
this functionality, we first curate and build two high-quality datasets,
STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed
temporal and spatial annotations, since most existing datasets offer either
temporal spans for videos or spatial boxes on images, lacking unified
spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start
reinforcement learning strategy with multiple specially designed rewards that
jointly encourage answer accuracy, temporal alignment, and spatial precision.
On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,
raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent
improvements are also observed on a broad range of video understanding
benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond
accuracy, the reasoning traces produced by Open-o3 Video also provide valuable
signals for test-time scaling, enabling confidence-aware verification and
improving answer reliability.</p>
<h3 id="19-unsupervised-domain-adaptation-via-similarity-based-prototypes-for-cross-modality-segmentation">[19] <a href="https://arxiv.org/abs/2510.20596">Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation</a></h3>
<p><em>Ziyu Ye, Chen Ju, Chaofan Ma, Xiaoyun Zhang</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç›¸ä¼¼æ€§åŸå‹çš„è·¨æ¨¡æ€åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡å­¦ä¹ åµŒå…¥ç©ºé—´ä¸­çš„ç±»åˆ«åŸå‹å¹¶å¼•å…¥ç›¸ä¼¼æ€§çº¦æŸï¼Œæœ‰æ•ˆè§£å†³äº†é¢†åŸŸè‡ªé€‚åº”ä¸­çš„ç±»åˆ«ç¼ºå¤±é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†è·¨æ¨¡æ€åˆ†å‰²æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å½“åº”ç”¨äºæœªè§æ•°æ®æ—¶ä¼šå‡ºç°æ€§èƒ½æ€¥å‰§ä¸‹é™çš„é—®é¢˜ã€‚ç”±äºæ¨¡å‹å¯¹é¢†åŸŸåç§»æ•æ„Ÿï¼Œæ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”æ—¨åœ¨å‡å°‘é¢†åŸŸå·®è·å¹¶é¿å…å¯¹æ–°é¢†åŸŸè¿›è¡Œæ˜‚è´µçš„æ ‡æ³¨å·¥ä½œã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç›¸ä¼¼æ€§åŸå‹çš„è·¨æ¨¡æ€åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡å­¦ä¹ åµŒå…¥ç©ºé—´ä¸­çš„ç±»åˆ«åŸå‹å¹¶å¼•å…¥ç›¸ä¼¼æ€§çº¦æŸï¼Œä½¿è¿™äº›åŸå‹èƒ½å¤Ÿä»£è¡¨æ¯ä¸ªè¯­ä¹‰ç±»åˆ«åŒæ—¶ä¸ä¸åŒç±»åˆ«ä¿æŒåˆ†ç¦»ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å­—å…¸å­˜å‚¨ä»ä¸åŒå›¾åƒä¸­æå–çš„åŸå‹ï¼Œé˜²æ­¢ç±»åˆ«ç¼ºå¤±é—®é¢˜å¹¶å®ç°åŸå‹çš„å¯¹æ¯”å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨æ¨¡æ€åˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†æ¯”å…¶ä»–æœ€å…ˆè¿›æ–¹æ³•æ›´å¥½çš„ç»“æœï¼ŒéªŒè¯äº†æ‰€ææ¡†æ¶åœ¨æå‡åˆ†å‰²æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºç›¸ä¼¼æ€§åŸå‹çš„æ¡†æ¶åœ¨è·¨æ¨¡æ€åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡åŸå‹å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ æœºåˆ¶æˆåŠŸè§£å†³äº†é¢†åŸŸè‡ªé€‚åº”ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºæ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Deep learning models have achieved great success on various vision
challenges, but a well-trained model would face drastic performance degradation
when applied to unseen data. Since the model is sensitive to domain shift,
unsupervised domain adaptation attempts to reduce the domain gap and avoid
costly annotation of unseen domains. This paper proposes a novel framework for
cross-modality segmentation via similarity-based prototypes. In specific, we
learn class-wise prototypes within an embedding space, then introduce a
similarity constraint to make these prototypes representative for each semantic
class while separable from different classes. Moreover, we use dictionaries to
store prototypes extracted from different images, which prevents the
class-missing problem and enables the contrastive learning of prototypes, and
further improves performance. Extensive experiments show that our method
achieves better results than other state-of-the-art methods.</p>
<h3 id="20-empower-words-dualground-for-structured-phrase-and-sentence-level-temporal-grounding">[20] <a href="https://arxiv.org/abs/2510.20244">Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding</a></h3>
<p><em>Minseok Kang, Minhyeok Lee, Minjung Kim, Donghyeong Kim, Sangyoun Lee</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDualGroundï¼Œä¸€ç§åŒåˆ†æ”¯æ¶æ„çš„è§†é¢‘æ—¶åºå®šä½æ–¹æ³•ï¼Œé€šè¿‡å°†[EOS]æ ‡è®°ä¸è¯æ ‡è®°åˆ†åˆ«è·¯ç”±åˆ°å¥å­çº§å’ŒçŸ­è¯­çº§è·¯å¾„ï¼Œå®ç°å…¨å±€ä¸å±€éƒ¨è¯­ä¹‰çš„æ˜¾å¼åˆ†ç¦»ï¼Œä»è€Œè§£å†³ç°æœ‰æ–¹æ³•å¿½è§†æ–‡æœ¬æ ‡è®°è¯­ä¹‰è§’è‰²å·®å¼‚çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘æ—¶åºå®šä½æ–¹æ³•é€šå¸¸å°†æ‰€æœ‰æ–‡æœ¬æ ‡è®°åœ¨è·¨æ¨¡æ€æ³¨æ„åŠ›ä¸­ç»Ÿä¸€å¤„ç†ï¼Œå¿½è§†äº†å®ƒä»¬ä¸åŒçš„è¯­ä¹‰è§’è‰²ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦ä¾èµ–[EOS]é©±åŠ¨çš„å…¨å±€è¯­ä¹‰è€Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨è¯çº§ä¿¡å·ï¼Œé™åˆ¶äº†ç»†ç²’åº¦æ—¶åºå¯¹é½çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŒåˆ†æ”¯æ¶æ„DualGroundï¼Œé€šè¿‡å°†[EOS]æ ‡è®°è·¯ç”±åˆ°å¥å­çº§è·¯å¾„å¹¶å°†è¯æ ‡è®°èšç±»ä¸ºçŸ­è¯­çº§å•å…ƒï¼Œå®ç°å…¨å±€ä¸å±€éƒ¨è¯­ä¹‰çš„æ˜¾å¼åˆ†ç¦»ï¼›å¼•å…¥æ ‡è®°è§’è‰²æ„ŸçŸ¥çš„è·¨æ¨¡æ€äº¤äº’ç­–ç•¥ï¼Œä»¥ç»“æ„è§£è€¦çš„æ–¹å¼å¯¹é½è§†é¢‘ç‰¹å¾ä¸å¥å­çº§å’ŒçŸ­è¯­çº§è¯­ä¹‰ï¼›é‡‡ç”¨è”åˆå»ºæ¨¡æ¡†æ¶åŒæ—¶æå‡å…¨å±€å¥å­çº§å¯¹é½å’Œç»†ç²’åº¦æ—¶åºå®šä½èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> DualGroundåœ¨QVHighlightså’ŒCharades-STAåŸºå‡†æµ‹è¯•çš„Moment Retrievalå’ŒHighlight Detectionä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†è¯­ä¹‰è§£è€¦å»ºæ¨¡åœ¨è§†é¢‘-è¯­è¨€å¯¹é½ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡æ˜¾å¼åˆ†ç¦»å…¨å±€å’Œå±€éƒ¨è¯­ä¹‰å¯ä»¥å®ç°æ›´ç²¾ç»†çš„è§†é¢‘æ—¶åºå®šä½ï¼Œè§£è€¦çš„è¯­ä¹‰å»ºæ¨¡èƒ½å¤Ÿæœ‰æ•ˆæå‡è§†é¢‘-è¯­è¨€å¯¹é½çš„è¡¨è¾¾èƒ½åŠ›å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºç»†ç²’åº¦è·¨æ¨¡æ€ç†è§£æä¾›äº†æ–°çš„è®¾è®¡æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Video Temporal Grounding (VTG) aims to localize temporal segments in long,
untrimmed videos that align with a given natural language query. This task
typically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection
(HD). While recent advances have been progressed by powerful pretrained
vision-language models such as CLIP and InternVideo2, existing approaches
commonly treat all text tokens uniformly during crossmodal attention,
disregarding their distinct semantic roles. To validate the limitations of this
approach, we conduct controlled experiments demonstrating that VTG models
overly rely on [EOS]-driven global semantics while failing to effectively
utilize word-level signals, which limits their ability to achieve fine-grained
temporal alignment. Motivated by this limitation, we propose DualGround, a
dual-branch architecture that explicitly separates global and local semantics
by routing the [EOS] token through a sentence-level path and clustering word
tokens into phrase-level units for localized grounding. Our method introduces
(1) tokenrole- aware cross modal interaction strategies that align video
features with sentence-level and phrase-level semantics in a structurally
disentangled manner, and (2) a joint modeling framework that not only improves
global sentence-level alignment but also enhances finegrained temporal
grounding by leveraging structured phrase-aware context. This design allows the
model to capture both coarse and localized semantics, enabling more expressive
and context-aware video grounding. DualGround achieves state-of-the-art
performance on both Moment Retrieval and Highlight Detection tasks across
QVHighlights and Charades- STA benchmarks, demonstrating the effectiveness of
disentangled semantic modeling in video-language alignment.</p>
<h3 id="21-gmfvad-using-grained-multi-modal-feature-to-improve-video-anomaly-detection">[21] <a href="https://arxiv.org/abs/2510.20268">GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection</a></h3>
<p><em>Guangyu Dai, Dong Chen, Siliang Tang, Yueting Zhuang</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGMFVADæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯çš„å¤šæ ·æ€§æ¥ç»†åŒ–æå–ç‰¹å¾ï¼Œå‡å°‘è§†è§‰ç‰¹å¾ä¸­çš„å†—ä½™ä¿¡æ¯ï¼Œä»è€Œæå‡è§†é¢‘å¼‚å¸¸æ£€æµ‹æ€§èƒ½ï¼Œåœ¨å››ä¸ªä¸»è¦æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•è™½ç„¶å°è¯•å¼•å…¥æ–‡æœ¬ç­‰å¤šæ¨¡æ€ä¿¡æ¯ï¼Œä½†ä»…ä»¥ç²—ç•¥æ–¹å¼å°†æ–‡æœ¬ç‰¹å¾æ•´åˆåˆ°è§†é¢‘ç‰‡æ®µä¸­ï¼Œå¿½ç•¥äº†è§†é¢‘ç‰‡æ®µä¸­å¯èƒ½å­˜åœ¨çš„å¤§é‡å†—ä½™ä¿¡æ¯ï¼Œè¿™é™åˆ¶äº†æ£€æµ‹æ€§èƒ½çš„è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>Method:</strong> æå‡ºGrained Multi-modal Feature for Video Anomaly Detection (GMFVAD)ï¼ŒåŸºäºè§†é¢‘ç‰‡æ®µç”Ÿæˆæ›´ç»†ç²’åº¦çš„å¤šæ¨¡æ€ç‰¹å¾ï¼Œé€šè¿‡æ€»ç»“ä¸»è¦å†…å®¹å¹¶å¼•å…¥åŸºäºåŸå§‹è§†é¢‘å­—å¹•çš„æ–‡æœ¬ç‰¹å¾æ¥è¿›ä¸€æ­¥å¢å¼ºçªå‡ºéƒ¨åˆ†çš„è§†è§‰ç‰¹å¾ã€‚</p>
<p><strong>Result:</strong> GMFVADåœ¨å››ä¸ªä¸»è¦æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¶ˆèå®éªŒéªŒè¯äº†æ€§èƒ½æå‡ç¡®å®æºäºå†—ä½™ä¿¡æ¯çš„å‡å°‘ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡å¤šæ¨¡æ€ä¿¡æ¯çš„ç»†ç²’åº¦èåˆèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘è§†è§‰ç‰¹å¾å†—ä½™ï¼Œä¸ºè§†é¢‘å¼‚å¸¸æ£€æµ‹æä¾›äº†æ–°çš„ç‰¹å¾å¢å¼ºæ€è·¯ï¼Œå¼ºè°ƒäº†å†—ä½™ä¿¡æ¯æ¶ˆé™¤åœ¨æå‡æ£€æµ‹æ€§èƒ½ä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Video anomaly detection (VAD) is a challenging task that detects anomalous
frames in continuous surveillance videos. Most previous work utilizes the
spatio-temporal correlation of visual features to distinguish whether there are
abnormalities in video snippets. Recently, some works attempt to introduce
multi-modal information, like text feature, to enhance the results of video
anomaly detection. However, these works merely incorporate text features into
video snippets in a coarse manner, overlooking the significant amount of
redundant information that may exist within the video snippets. Therefore, we
propose to leverage the diversity among multi-modal information to further
refine the extracted features, reducing the redundancy in visual features, and
we propose Grained Multi-modal Feature for Video Anomaly Detection (GMFVAD).
Specifically, we generate more grained multi-modal feature based on the video
snippet, which summarizes the main content, and text features based on the
captions of original video will be introduced to further enhance the visual
features of highlighted portions. Experiments show that the proposed GMFVAD
achieves state-of-the-art performance on four mainly datasets. Ablation
experiments also validate that the improvement of GMFVAD is due to the
reduction of redundant information.</p>
<h3 id="22-dmc3-dual-modal-counterfactual-contrastive-construction-for-egocentric-video-question-answering">[22] <a href="https://arxiv.org/abs/2510.20285">DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering</a></h3>
<p><em>Jiayi Zou, Chaofan Chen, Bing-Kun Bao, Changsheng Xu</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†åŒæ¨¡æ€åäº‹å®å¯¹æ¯”æ„å»ºï¼ˆDMCÂ³ï¼‰æ¡†æ¶ï¼Œé€šè¿‡åäº‹å®æ ·æœ¬æ„å»ºå’Œå¯¹æ¯”ä¼˜åŒ–æ¥è§£å†³ç¬¬ä¸€äººç§°è§†é¢‘é—®ç­”ä¸­çš„å¤šäº‹ä»¶ç†è§£å’Œæ‰‹ç‰©äº¤äº’è¯†åˆ«æŒ‘æˆ˜ï¼Œåœ¨EgoTaskQAå’ŒQAEGO4DåŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–¹æ³•åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘é—®ç­”ä¸­è™½ç„¶é€šè¿‡é¢„è®­ç»ƒå’Œå¾®è°ƒèŒƒå¼å–å¾—äº†è¿›å±•ï¼Œä½†å¿½ç•¥äº†ç¬¬ä¸€äººç§°è§†è§’å¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç†è§£å¤šä¸ªäº‹ä»¶å’Œè¯†åˆ«æ‰‹ç‰©äº¤äº’ï¼Œè¿™äº›é™åˆ¶å½±å“äº†æ¨¡å‹å¯¹è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘çš„æ·±åº¦ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„DMCÂ³æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘é—®ç­”åŸºçº¿æ¨¡å‹ã€åäº‹å®æ ·æœ¬æ„å»ºæ¨¡å—å’Œåäº‹å®æ ·æœ¬å‚ä¸çš„å¯¹æ¯”ä¼˜åŒ–æ¨¡å—ï¼Œå…¶ä¸­åäº‹å®æ ·æœ¬æ„å»ºé€šè¿‡äº‹ä»¶æè¿°æ”¹å†™å’Œæ ¸å¿ƒäº¤äº’æŒ–æ˜åˆ†åˆ«ç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰æ¨¡æ€çš„æ­£è´Ÿæ ·æœ¬ï¼Œç„¶åä¸åŸå§‹æ ·æœ¬ä¸€èµ·è¾“å…¥åŸºçº¿æ¨¡å‹ï¼Œæœ€åé€šè¿‡å¯¹æ¯”æŸå¤±æœ€å°åŒ–åŸå§‹æ ·æœ¬ä¸æ­£æ ·æœ¬ç‰¹å¾è·ç¦»ï¼ŒåŒæ—¶æœ€å¤§åŒ–ä¸è´Ÿæ ·æœ¬è·ç¦»ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨EgoTaskQAçš„normalå’Œindirectåˆ†å‰²ä¸Šåˆ†åˆ«è¾¾åˆ°52.51%å’Œ46.04%çš„å‡†ç¡®ç‡ï¼Œåœ¨QAEGO4Dä¸Šè¾¾åˆ°13.2%çš„å‡†ç¡®ç‡ï¼Œå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åäº‹å®å¯¹æ¯”å­¦ä¹ åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘é—®ç­”ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡å¤šäº‹ä»¶ç†è§£å’Œæ‰‹ç‰©äº¤äº’è¯†åˆ«ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘çš„ç†è§£èƒ½åŠ›ï¼Œä¸ºè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’Œä¼˜åŒ–ç­–ç•¥ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Egocentric Video Question Answering (Egocentric VideoQA) plays an important
role in egocentric video understanding, which refers to answering questions
based on first-person videos. Although existing methods have made progress
through the paradigm of pre-training and fine-tuning, they ignore the unique
challenges posed by the first-person perspective, such as understanding
multiple events and recognizing hand-object interactions. To deal with these
challenges, we propose a Dual-Modal Counterfactual Contrastive Construction
(DMC$^3$) framework, which contains an egocentric videoqa baseline, a
counterfactual sample construction module and a counterfactual sample-involved
contrastive optimization. Specifically, We first develop a counterfactual
sample construction module to generate positive and negative samples for
textual and visual modalities through event description paraphrasing and core
interaction mining, respectively. Then, We feed these samples together with the
original samples into the baseline. Finally, in the counterfactual
sample-involved contrastive optimization module, we apply contrastive loss to
minimize the distance between the original sample features and the positive
sample features, while maximizing the distance from the negative samples.
Experiments show that our method achieve 52.51\% and 46.04\% on the
\textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2\% on
QAEGO4D, both reaching the state-of-the-art performance.</p>
<h3 id="23-hyperet-efficient-training-in-hyperbolic-space-for-multi-modal-large-language-models">[23] <a href="https://arxiv.org/abs/2510.20322">HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models</a></h3>
<p><em>Zelin Peng, Zhengqin Xu, Qingyang Liu, Xiaokang Yang, Wei Shen</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºHyperETï¼Œä¸€ç§åŸºäºåŒæ›²ç©ºé—´çš„é«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´åŒæ›²åŠå¾„å®ç°è§†è§‰ä¸æ–‡æœ¬è¡¨å¾åœ¨ä»»æ„ç²’åº¦çº§åˆ«çš„å¯¹é½ï¼Œä»…éœ€å¢åŠ ä¸åˆ°1%çš„å‚æ•°å³å¯æ˜¾è‘—æå‡ç°æœ‰MLLMæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹éœ€è¦æé«˜çš„è®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒï¼Œå…¶æ ¹æœ¬åŸå› åœ¨äºå¹¿æ³›ä½¿ç”¨çš„è§†è§‰ç¼–ç å™¨ï¼ˆå¦‚CLIPå’ŒSAMï¼‰ç¼ºä¹ä¸è¯­è¨€åœ¨å¤šä¸ªç²’åº¦çº§åˆ«çš„å¯¹é½èƒ½åŠ›ï¼Œå¯¼è‡´è·¨æ¨¡æ€å¯¹é½æ•ˆç‡ä½ä¸‹ã€‚</p>
<p><strong>Method:</strong> HyperETåˆ©ç”¨åŒæ›²ç©ºé—´å¤©ç„¶å»ºæ¨¡å±‚æ¬¡ç»“æ„çš„ç‰¹ç‚¹ï¼Œé€šè¿‡å¯å­¦ä¹ çŸ©é˜µä¸MÃ¶biusä¹˜æ³•æ“ä½œå®ç°åŠ¨æ€åŒæ›²åŠå¾„è°ƒæ•´ï¼Œé‡‡ç”¨å¯¹è§’ç¼©æ”¾çŸ©é˜µã€å—å¯¹è§’çŸ©é˜µå’Œå¸¦çŠ¶çŸ©é˜µä¸‰ç§é«˜æ•ˆå‚æ•°åŒ–ç­–ç•¥ï¼Œä¼˜åŒ–è§†è§‰è¡¨å¾ä¸æ–‡æœ¬è¡¨å¾åœ¨ä»»æ„ç²’åº¦çº§åˆ«çš„å¯¹é½ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªMLLMåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒHyperETèƒ½å¤ŸæŒç»­æ˜¾è‘—æå‡ç°æœ‰é¢„è®­ç»ƒå’Œå¾®è°ƒMLLMçš„æ€§èƒ½ï¼Œä¸”ä»…éœ€å¢åŠ ä¸åˆ°1%çš„é¢å¤–å‚æ•°å³å¯å®ç°è¿™ä¸€æ”¹è¿›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŒæ›²ç©ºé—´åœ¨è§£å†³è§†è§‰-æ–‡æœ¬ç²’åº¦å¯¹é½é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä¸ºé«˜æ•ˆå¤šæ¨¡æ€å­¦ä¹ æä¾›äº†æ–°èŒƒå¼ï¼Œè¡¨æ˜é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å‡ ä½•ç©ºé—´å»ºæ¨¡å¯ä»¥å¤§å¹…é™ä½MLLMè®­ç»ƒçš„è®¡ç®—éœ€æ±‚ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Multi-modal large language models (MLLMs) have emerged as a transformative
approach for aligning visual and textual understanding. They typically require
extremely high computational resources (e.g., thousands of GPUs) for training
to achieve cross-modal alignment at multi-granularity levels. We argue that a
key source of this inefficiency lies in the vision encoders they widely equip
with, e.g., CLIP and SAM, which lack the alignment with language at
multi-granularity levels. To address this issue, in this paper, we leverage
hyperbolic space, which inherently models hierarchical levels and thus provides
a principled framework for bridging the granularity gap between visual and
textual modalities at an arbitrary granularity level. Concretely, we propose an
efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize
visual representations to align with their textual counterparts at an arbitrary
granularity level through dynamic hyperbolic radius adjustment in hyperbolic
space. HyperET employs learnable matrices with M\"{o}bius multiplication
operations, implemented via three effective configurations: diagonal scaling
matrices, block-diagonal matrices, and banded matrices, providing a flexible
yet efficient parametrization strategy. Comprehensive experiments across
multiple MLLM benchmarks demonstrate that HyperET consistently improves both
existing pre-training and fine-tuning MLLMs clearly with less than 1\%
additional parameters.</p>
<h3 id="24-mitigating-cross-modal-representation-bias-for-multicultural-image-to-recipe-retrieval">[24] <a href="https://arxiv.org/abs/2510.20393">Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval</a></h3>
<p><em>Qing Wang, Chong-Wah Ngo, Yu Cao, Ee-Peng Lim</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å› æœè¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹å›¾åƒä¸­å¯èƒ½è¢«å¿½ç•¥çš„çƒ¹é¥ªå…ƒç´ å¹¶å°†å…¶æ˜¾å¼æ³¨å…¥è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¥ç¼“è§£å›¾åƒ-é£Ÿè°±æ£€ç´¢ä¸­çš„è¡¨ç¤ºåå·®é—®é¢˜ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ­ç¤ºç»†å¾®çš„é£Ÿæå’Œçƒ¹é¥ªåŠ¨ä½œï¼Œåœ¨å•è¯­å’Œå¤šè¯­è¨€å¤šæ–‡åŒ–æ•°æ®é›†ä¸Šå‡å–å¾—äº†ä¼˜å¼‚çš„æ£€ç´¢æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å›¾åƒ-é£Ÿè°±æ£€ç´¢æ–¹æ³•éšå«å‡è®¾é£Ÿç‰©å›¾åƒèƒ½å¤Ÿå®Œå…¨æ•æ‰é£Ÿè°±ä¸­æ–‡æœ¬è®°å½•çš„æ‰€æœ‰ç»†èŠ‚ï¼Œä½†å®é™…ä¸Šé£Ÿç‰©å›¾åƒä»…åæ˜ çƒ¹é¥ªå®Œæˆåçš„è§†è§‰ç»“æœè€Œéåº•å±‚çƒ¹é¥ªè¿‡ç¨‹ã€‚è¿™å¯¼è‡´è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ å€¾å‘äºå¿½ç•¥é‚£äº›è§†è§‰ä¸Šä¸æ˜æ˜¾ä½†å¯¹é£Ÿè°±æ£€ç´¢è‡³å…³é‡è¦çš„ç»†å¾®ã€é£Ÿè°±ç‰¹å®šçš„ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®æ··åˆäº†æ¥è‡ªä¸åŒèœç³»çš„å›¾åƒå’Œé£Ÿè°±æ—¶ï¼Œè¡¨ç¤ºå­¦ä¹ çš„åå·®é—®é¢˜ä¼šæ›´åŠ ä¸¥é‡ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å› æœæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é¢„æµ‹å›¾åƒä¸­å¯èƒ½è¢«å¿½ç•¥çš„çƒ¹é¥ªå…ƒç´ ï¼Œå¹¶æ˜¾å¼åœ°å°†è¿™äº›å…ƒç´ æ³¨å…¥è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä»¥ç¼“è§£åå·®ã€‚è¯¥æ–¹æ³•ç‰¹åˆ«å…³æ³¨æ­ç¤ºç»†å¾®çš„é£Ÿæä½¿ç”¨å’Œçƒ¹é¥ªæ–¹æ³•å·®å¼‚ï¼Œé€šè¿‡å› æœæ¨ç†æœºåˆ¶æ¥å¢å¼ºè¡¨ç¤ºå­¦ä¹ å¯¹éè§†è§‰æ˜æ˜¾ä½†å…³é”®é£Ÿè°±ç»†èŠ‚çš„æ•æ‰èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†çš„å•è¯­Recipe1Mæ•°æ®é›†å’Œæ–°æ„å»ºçš„å¤šè¯­è¨€å¤šæ–‡åŒ–èœç³»æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å› æœè¡¨ç¤ºå­¦ä¹ æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºç»†å¾®çš„é£Ÿæå’Œçƒ¹é¥ªåŠ¨ä½œã€‚è¯¥æ–¹æ³•åœ¨å•è¯­å’Œå¤šè¯­è¨€å¤šæ–‡åŒ–æ•°æ®é›†ä¸Šå‡å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ£€ç´¢æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜å› æœè¡¨ç¤ºå­¦ä¹ èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£å›¾åƒ-é£Ÿè°±æ£€ç´¢ä¸­çš„è¡¨ç¤ºåå·®é—®é¢˜ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡å’Œæ³¨å…¥è¢«å›¾åƒå¿½ç•¥çš„çƒ¹é¥ªå…ƒç´ æ¥æå‡æ£€ç´¢ç²¾åº¦ã€‚è¿™ä¸€æ–¹æ³•ä¸ºå¤„ç†å¤šæ–‡åŒ–å¤šè¯­è¨€é£Ÿè°±æ£€ç´¢æä¾›äº†æ–°çš„æ€è·¯ï¼Œå±•ç¤ºäº†å› æœæ¨ç†åœ¨è·¨æ¨¡æ€å­¦ä¹ ä¸­çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥åœ¨æ›´å¤æ‚çƒ¹é¥ªåœºæ™¯ä¸­çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Existing approaches for image-to-recipe retrieval have the implicit
assumption that a food image can fully capture the details textually documented
in its recipe. However, a food image only reflects the visual outcome of a
cooked dish and not the underlying cooking process. Consequently, learning
cross-modal representations to bridge the modality gap between images and
recipes tends to ignore subtle, recipe-specific details that are not visually
apparent but are crucial for recipe retrieval. Specifically, the
representations are biased to capture the dominant visual elements, resulting
in difficulty in ranking similar recipes with subtle differences in use of
ingredients and cooking methods. The bias in representation learning is
expected to be more severe when the training data is mixed of images and
recipes sourced from different cuisines. This paper proposes a novel causal
approach that predicts the culinary elements potentially overlooked in images,
while explicitly injecting these elements into cross-modal representation
learning to mitigate biases. Experiments are conducted on the standard
monolingual Recipe1M dataset and a newly curated multilingual multicultural
cuisine dataset. The results indicate that the proposed causal representation
learning is capable of uncovering subtle ingredients and cooking actions and
achieves impressive retrieval performance on both monolingual and multilingual
multicultural datasets.</p>
<h3 id="25-conan-progressive-learning-to-reason-like-a-detective-over-multi-scale-visual-evidence">[25] <a href="https://arxiv.org/abs/2510.20470">Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence</a></h3>
<p><em>Kun Ouyang, Yuanxin Liu, Linli Yao, Yishuo Cai, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºConanæ¡†æ¶ï¼Œé€šè¿‡è¯æ®æ¥åœ°çš„å¤šæ­¥è§†é¢‘æ¨ç†æ–¹æ³•ï¼Œç»“åˆä¸Šä¸‹æ–‡å’Œè¯æ®å¸§è¯†åˆ«ã€è·¨å¸§çº¿ç´¢æ¨ç†ä»¥åŠè‡ªé€‚åº”å†³ç­–æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘æ¨ç†ä»»åŠ¡é¢ä¸´å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šå¸§æ¨ç†æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•è™½ç„¶å¢å¼ºæ¨ç†èƒ½åŠ›ä½†å¸¸äº§ç”Ÿæ— æ ¹æ®çš„æ–‡æœ¬é“¾ç»“è®ºï¼Œè€Œå¸§æ£€ç´¢æ–¹æ³•è™½ç„¶å¼•å…¥è§†è§‰æ¥åœ°ä½†ä»å­˜åœ¨è¯æ®å®šä½ä¸å‡†ç¡®çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> Conanæ¡†æ¶é‡‡ç”¨è¯†åˆ«-æ¨ç†-è¡ŒåŠ¨çš„ä¸‰é˜¶æ®µå¼ºåŒ–å­¦ä¹ è§†é¢‘æ¨ç†è®­ç»ƒæ–¹æ³•ï¼Œæ„å»ºäº†åŒ…å«91Kè‡ªåŠ¨ç”Ÿæˆæ¨ç†è½¨è¿¹çš„å¤§è§„æ¨¡æ•°æ®é›†Conan-91Kï¼Œå¹¶è®¾è®¡äº†å¤šé˜¶æ®µæ¸è¿›å¼å†·å¯åŠ¨ç­–ç•¥æ¥è”åˆå¢å¼ºå¤šæ­¥è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨å…­ä¸ªå¤šæ­¥æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒConanç›¸æ¯”åŸºçº¿Qwen2.5-VL-7B-Instructå¹³å‡å‡†ç¡®ç‡æå‡è¶…è¿‡10%ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶åœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> Conanæ¡†æ¶éªŒè¯äº†è¯æ®æ¥åœ°å¤šæ­¥è§†é¢‘æ¨ç†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†åœ¨å¤æ‚è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„å¼ºå¯æ‰©å±•æ€§å’Œé²æ£’æ€§ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’Œæ•°æ®é›†èµ„æºã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Video reasoning, which requires multi-step deduction across frames, remains a
major challenge for multimodal large language models (MLLMs). While
reinforcement learning (RL)-based methods enhance reasoning capabilities, they
often rely on text-only chains that yield ungrounded or hallucinated
conclusions. Conversely, frame-retrieval approaches introduce visual grounding
but still struggle with inaccurate evidence localization. To address these
challenges, we present Conan, a framework for evidence-grounded multi-step
video reasoning. Conan identifies contextual and evidence frames, reasons over
cross-frame clues, and adaptively decides when to conclude or explore further.
To achieve this, we (1) construct Conan-91K, a large-scale dataset of
automatically generated reasoning traces that includes frame identification,
evidence reasoning, and action decision, and (2) design a multi-stage
progressive cold-start strategy combined with an
Identification-Reasoning-Action (AIR) RLVR training framework to jointly
enhance multi-step visual reasoning. Extensive experiments on six multi-step
reasoning benchmarks demonstrate that Conan surpasses the baseline
Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving
state-of-the-art performance. Furthermore, Conan generalizes effectively to
long-video understanding tasks, validating its strong scalability and
robustness.</p>
<h3 id="26-echodistill-bidirectional-concept-distillation-for-one-step-diffusion-personalization">[26] <a href="https://arxiv.org/abs/2510.20512">EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion Personalization</a></h3>
<p><em>Yixiong Yang, Tao Wu, Senmao Li, Shiqi Yang, Yaxing Wang, Joost van de Weijer, Kai Wang</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºEchoDistillåŒå‘æ¦‚å¿µè’¸é¦æ¡†æ¶ï¼Œå®ç°å•æ­¥æ‰©æ•£ä¸ªæ€§åŒ–ï¼ˆ1-SDPï¼‰ï¼Œé€šè¿‡å¸ˆç”Ÿæ¨¡å‹ååŒè®­ç»ƒæœºåˆ¶ï¼Œåœ¨ä¿æŒå¿«é€Ÿç”Ÿæˆçš„åŒæ—¶æœ‰æ•ˆæ•æ‰æ–°æ¦‚å¿µåˆ†å¸ƒã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å•æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è™½ç„¶å®ç°äº†åŠ é€Ÿç”Ÿæˆï¼Œä½†åœ¨ä¸ªæ€§åŒ–æ–°æ¦‚å¿µæ–¹é¢å­˜åœ¨å±€é™ï¼Œå› ä¸ºå•æ­¥æ¨¡å‹éš¾ä»¥æœ‰æ•ˆæ•æ‰æ–°æ¦‚å¿µçš„åˆ†å¸ƒç‰¹å¾ï¼Œè¿™é™åˆ¶äº†å®é™…åº”ç”¨ä¸­çš„æ¦‚å¿µå®šåˆ¶èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŒå‘æ¦‚å¿µè’¸é¦æ¡†æ¶EchoDistillï¼Œé‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒæ–¹å¼åŒæ—¶ä¼˜åŒ–å¤šæ­¥æ•™å¸ˆæ¨¡å‹å’Œå•æ­¥å­¦ç”Ÿæ¨¡å‹ï¼Œé€šè¿‡æ¦‚å¿µä»æ•™å¸ˆåˆ°å­¦ç”Ÿçš„è’¸é¦ä»¥åŠä»å­¦ç”Ÿåˆ°æ•™å¸ˆçš„å›ä¼ å®ç°åŒå‘çŸ¥è¯†ä¼ é€’ï¼Œå¹¶å…±äº«æ–‡æœ¬ç¼–ç å™¨ä¿è¯è¯­ä¹‰ä¸€è‡´æ€§ï¼Œç»“åˆå¯¹æŠ—æŸå¤±å’Œå¯¹é½æŸå¤±ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è¯¥åä½œæ¡†æ¶åœ¨1-SDPè®¾ç½®ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œä¸ä»…æå‡äº†å­¦ç”Ÿæ¨¡å‹å¯¹æ–°æ¦‚å¿µçš„ä¸ªæ€§åŒ–èƒ½åŠ›ï¼Œè¿˜æ”¹å–„äº†æ•™å¸ˆæ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å»ºç«‹äº†å¿«é€Ÿæœ‰æ•ˆä¸ªæ€§åŒ–T2Iæ‰©æ•£æ¨¡å‹çš„æ–°èŒƒå¼ï¼ŒåŒå‘æ¦‚å¿µè’¸é¦æœºåˆ¶è¯æ˜äº†å¸ˆç”Ÿæ¨¡å‹ååŒè®­ç»ƒåœ¨å¹³è¡¡ç”Ÿæˆé€Ÿåº¦ä¸æ¦‚å¿µæ•æ‰èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®æ—¶ä¸ªæ€§åŒ–åº”ç”¨æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Recent advances in accelerating text-to-image (T2I) diffusion models have
enabled the synthesis of high-fidelity images even in a single step. However,
personalizing these models to incorporate novel concepts remains a challenge
due to the limited capacity of one-step models to capture new concept
distributions effectively. We propose a bidirectional concept distillation
framework, EchoDistill, to enable one-step diffusion personalization (1-SDP).
Our approach involves an end-to-end training process where a multi-step
diffusion model (teacher) and a one-step diffusion model (student) are trained
simultaneously. The concept is first distilled from the teacher model to the
student, and then echoed back from the student to the teacher. During the
EchoDistill, we share the text encoder between the two models to ensure
consistent semantic understanding. Following this, the student model is
optimized with adversarial losses to align with the real image distribution and
with alignment losses to maintain consistency with the teacher's output.
Furthermore, we introduce the bidirectional echoing refinement strategy,
wherein the student model leverages its faster generation capability to
feedback to the teacher model. This bidirectional concept distillation
mechanism not only enhances the student ability to personalize novel concepts
but also improves the generative quality of the teacher model. Our experiments
demonstrate that this collaborative framework significantly outperforms
existing personalization methods over the 1-SDP setup, establishing a novel
paradigm for rapid and effective personalization in T2I diffusion models.</p>
<h3 id="27-embodiedbrain-expanding-performance-boundaries-of-task-planning-for-embodied-intelligence">[27] <a href="https://arxiv.org/abs/2510.20578">EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence</a></h3>
<p><em>Ding Zou, Feifan Wang, Mengyu Ge, Siyuan Fan, Zongbing Zhang, Wei Chen, Lingfeng Wang, Zhongyou Hu, Wenrui Yan, Zhengwei Gao, Hao Wang, Weizhao Jin, Yu Zhang, Hainan Zhao, Mingliang Zhang, Xianxian Xi, Yaru Zhang, Wenyuan Li, Zhengguang Gao, Yurui Zhu</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†EmbodiedBrainï¼Œä¸€ç§æ–°å‹çš„å…·èº«æ™ºèƒ½è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡åˆ›æ–°çš„è®­ç»ƒæ–¹æ³•å’Œè¯„ä¼°ä½“ç³»è§£å†³äº†å½“å‰LLMå’ŒMLLMåœ¨å…·èº«ä»»åŠ¡ä¸­çš„å…³é”®é™åˆ¶ï¼Œåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç”¨äºå…·èº«ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å­˜åœ¨ä¸‰ä¸ªå…³é”®é™åˆ¶ï¼šæ¨¡å‹è®¾è®¡ä¸æ™ºèƒ½ä½“éœ€æ±‚ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€å®æ—¶å»¶è¿Ÿä¸æ€§èƒ½ä¹‹é—´ä¸å¯é¿å…çš„æƒè¡¡ã€ä»¥åŠä½¿ç”¨ä¸çœŸå®çš„ç¦»çº¿è¯„ä¼°æŒ‡æ ‡ï¼Œè¿™äº›é™åˆ¶é˜»ç¢äº†é€šç”¨äººå·¥æ™ºèƒ½åœ¨å…·èº«æ™ºèƒ½é¢†åŸŸçš„å®ç°ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†EmbodiedBrainæ¡†æ¶ï¼Œé‡‡ç”¨7Bå’Œ32Bä¸¤ç§å‚æ•°è§„æ¨¡ï¼Œè®¾è®¡äº†æ™ºèƒ½ä½“å¯¹é½çš„æ•°æ®ç»“æ„ï¼Œå¹¶é‡‡ç”¨å¤§è§„æ¨¡ç›‘ç£å¾®è°ƒä¸Step-Augmented Group Relative Policy Optimizationç›¸ç»“åˆçš„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å°†å‰åºæ­¥éª¤ä½œä¸ºå¼•å¯¼å‰ä½“æ¥æå‡é•¿æ—¶ç¨‹ä»»åŠ¡æˆåŠŸç‡ï¼ŒåŒæ—¶å¼•å…¥äº†åŒ…å«ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹çš„ç»¼åˆå¥–åŠ±ç³»ç»Ÿä»¥æå‡è®­ç»ƒæ•ˆç‡ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒEmbodiedBrainåœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œåœ¨é€šç”¨åŸºå‡†ã€è§„åˆ’åŸºå‡†å’Œç«¯åˆ°ç«¯ä»¿çœŸåŸºå‡†çš„ä¸‰éƒ¨åˆ†è¯„ä¼°ä½“ç³»ä¸­å‡è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨æå‡ºçš„æ–°å‹æŒ‘æˆ˜æ€§ä»¿çœŸç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºä¸‹ä¸€ä»£é€šç”¨å…·èº«æ™ºèƒ½ä½“çš„å‘å±•é“ºå¹³äº†é“è·¯ï¼Œé€šè¿‡å¼€æºæ‰€æœ‰æ•°æ®ã€æ¨¡å‹æƒé‡å’Œè¯„ä¼°æ–¹æ³•ï¼Œå»ºç«‹äº†å…·èº«åŸºç¡€æ¨¡å‹çš„æ–°æ ‡å‡†ï¼Œå¼ºè°ƒäº†ç»¼åˆè®­ç»ƒæ–¹æ³•å’ŒçœŸå®è¯„ä¼°ç¯å¢ƒå¯¹äºå®ç°ç¨³å¥ç©ºé—´æ„ŸçŸ¥å’Œè‡ªé€‚åº”ä»»åŠ¡æ‰§è¡Œçš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>The realization of Artificial General Intelligence (AGI) necessitates
Embodied AI agents capable of robust spatial perception, effective task
planning, and adaptive execution in physical environments. However, current
large language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks
suffer from key limitations, including a significant gap between model design
and agent requirements, an unavoidable trade-off between real-time latency and
performance, and the use of unauthentic, offline evaluation metrics. To address
these challenges, we propose EmbodiedBrain, a novel vision-language foundation
model available in both 7B and 32B parameter sizes. Our framework features an
agent-aligned data structure and employs a powerful training methodology that
integrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group
Relative Policy Optimization (Step-GRPO), which boosts long-horizon task
success by integrating preceding steps as Guided Precursors. Furthermore, we
incorporate a comprehensive reward system, including a Generative Reward Model
(GRM) accelerated at the infrastructure level, to improve training efficiency.
For enable thorough validation, we establish a three-part evaluation system
encompassing General, Planning, and End-to-End Simulation Benchmarks,
highlighted by the proposal and open-sourcing of a novel, challenging
simulation environment. Experimental results demonstrate that EmbodiedBrain
achieves superior performance across all metrics, establishing a new
state-of-the-art for embodied foundation models. Towards paving the way for the
next generation of generalist embodied agents, we open-source all of our data,
model weight, and evaluating methods, which are available at
https://zterobot.github.io/EmbodiedBrain.github.io.</p>
<h3 id="28-gencolorbench-a-color-evaluation-benchmark-for-text-to-image-generation-models">[28] <a href="https://arxiv.org/abs/2510.20586">GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models</a></h3>
<p><em>Muhammad Atif Butt, Alexandra Gomez-Villa, Tao Wu, Javier Vazquez-Corral, Joost Van De Weijer, Kai Wang</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†GenColorBenchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒé¢œè‰²ç”Ÿæˆçš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŸºäºISCC-NBSå’ŒCSS3/X11é¢œè‰²ç³»ç»Ÿæ„å»ºï¼ŒåŒ…å«44Kä¸ªé¢œè‰²ç„¦ç‚¹æç¤ºï¼Œæ­ç¤ºäº†ä¸»æµæ¨¡å‹åœ¨ç²¾ç»†é¢œè‰²æ§åˆ¶æ–¹é¢çš„æ€§èƒ½å·®å¼‚å’Œå¤±è´¥æ¨¡å¼ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç²¾ç»†é¢œè‰²å¯æ§æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæ— æ³•å‡†ç¡®åŒ¹é…æ–‡æœ¬æç¤ºä¸­æŒ‡å®šçš„é¢œè‰²ï¼Œè€Œç°æœ‰åŸºå‡†æµ‹è¯•è¦ä¹ˆå¿½ç•¥é¢œè‰²è¯„ä¼°ï¼Œè¦ä¹ˆä¾èµ–ç²—ç³™çš„è¯„ä¼°æ–¹æ³•ï¼Œç¼ºä¹å¯¹RGBæ•°å€¼è§£é‡Šå’Œäººç±»æœŸæœ›å¯¹é½ç­‰å…³é”®èƒ½åŠ›çš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†GenColorBenchåŸºå‡†æµ‹è¯•ï¼ŒåŸºäºISCC-NBSå’ŒCSS3/X11é¢œè‰²ç³»ç»Ÿæ„å»ºï¼ŒåŒ…å«44Kä¸ªé¢œè‰²ç„¦ç‚¹æç¤ºï¼Œè¦†ç›–400å¤šç§é¢œè‰²ï¼Œé¦–æ¬¡å¼•å…¥æ•°å€¼é¢œè‰²è¯„ä¼°ï¼Œé€šè¿‡æ„ŸçŸ¥å’Œè‡ªåŠ¨åŒ–è¯„ä¼°æ–¹æ³•å…¨é¢åˆ†ææ¨¡å‹çš„é¢œè‰²ç”Ÿæˆèƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å¯¹ä¸»æµæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºæ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œæ­ç¤ºäº†æ¨¡å‹å¯¹ä¸åŒé¢œè‰²çº¦å®šçš„ç†è§£ç¨‹åº¦ï¼Œè¯†åˆ«äº†å…·ä½“çš„å¤±è´¥æ¨¡å¼ï¼Œä¸ºç²¾ç¡®é¢œè‰²ç”Ÿæˆæä¾›äº†è¯¦ç»†çš„æ€§èƒ½åŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> GenColorBenchåŸºå‡†æµ‹è¯•å°†æŒ‡å¯¼æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç²¾ç¡®é¢œè‰²ç”Ÿæˆæ–¹é¢çš„æ”¹è¿›ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°ä½“ç³»çš„ç©ºç™½ï¼Œä¸ºé¢œè‰²å¯æ§æ€§ç ”ç©¶æä¾›äº†ç³»ç»Ÿæ€§çš„è¯„ä¼°æ¡†æ¶å’Œæ–¹å‘æŒ‡å¼•ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>Recent years have seen impressive advances in text-to-image generation, with
image generative or unified models producing high-quality images from text. Yet
these models still struggle with fine-grained color controllability, often
failing to accurately match colors specified in text prompts. While existing
benchmarks evaluate compositional reasoning and prompt adherence, none
systematically assess color precision. Color is fundamental to human visual
perception and communication, critical for applications from art to design
workflows requiring brand consistency. However, current benchmarks either
neglect color or rely on coarse assessments, missing key capabilities such as
interpreting RGB values or aligning with human expectations. To this end, we
propose GenColorBench, the first comprehensive benchmark for text-to-image
color generation, grounded in color systems like ISCC-NBS and CSS3/X11,
including numerical colors which are absent elsewhere. With 44K color-focused
prompts covering 400+ colors, it reveals models' true capabilities via
perceptual and automated assessments. Evaluations of popular text-to-image
models using GenColorBench show performance variations, highlighting which
color conventions models understand best and identifying failure modes. Our
GenColorBench assessments will guide improvements in precise color generation.
The benchmark will be made public upon acceptance.</p>
<h3 id="29-sevices-unifying-semantic-visual-evidence-consensus-for-long-video-understanding">[29] <a href="https://arxiv.org/abs/2510.20622">SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding</a></h3>
<p><em>Yuan Sheng, Yanbin Hao, Chenxu Li, Shuo Wang, Xiangnan He</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SeViCESæ¡†æ¶ï¼Œä¸€ç§æ— éœ€è®­ç»ƒä¸”æ¨¡å‹æ— å…³çš„é•¿è§†é¢‘ç†è§£æ–¹æ³•ï¼Œé€šè¿‡è¯­ä¹‰-è§†è§‰å…±è¯†è¯æ®é€‰æ‹©æœºåˆ¶ï¼Œåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é•¿è§†é¢‘ç†è§£é¢ä¸´è®¡ç®—å¤æ‚åº¦é«˜å’Œæ¨ç†ä¸ä¸€è‡´çš„æŒ‘æˆ˜ï¼Œç°æœ‰å¸§é€‰æ‹©æ–¹æ³•é€šå¸¸å¿½ç•¥æ—¶é—´ä¾èµ–æ€§æˆ–ä¾èµ–å•æ¨¡æ€è¯æ®ï¼Œæ— æ³•æä¾›å®Œæ•´ä¸”ä¸æŸ¥è¯¢ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</p>
<p><strong>Method:</strong> SeViCESæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šè¯­ä¹‰-è§†è§‰å…±è¯†å¸§é€‰æ‹©æ¨¡å—é€šè¿‡æ—¶é—´æ„ŸçŸ¥çš„è¯­ä¹‰åˆ†æ”¯å’Œèšç±»å¼•å¯¼çš„è§†è§‰åˆ†æ”¯è¿›è¡Œå¸§é€‰æ‹©ï¼Œç­”æ¡ˆå…±è¯†ç²¾ç‚¼æ¨¡å—é€šè¿‡è¯æ®èåˆå’Œç­”æ¡ˆç©ºé—´çº¦æŸæ¥è§£å†³è¯­ä¹‰ä¸è§†è§‰é¢„æµ‹ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSeViCESåœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…±è¯†é©±åŠ¨è¯æ®é€‰æ‹©å¯¹è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„é‡è¦æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†è¯­ä¹‰ä¸è§†è§‰è¯æ®ä¹‹é—´è¾¾æˆå…±è¯†å¯¹äºé•¿è§†é¢‘ç†è§£çš„å…³é”®ä½œç”¨ï¼Œæå‡ºçš„è®­ç»ƒæ— å…³æ¡†æ¶ä¸ºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹æä¾›äº†æœ‰æ•ˆçš„è¯æ®é€‰æ‹©æœºåˆ¶ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Long video understanding remains challenging due to its complex, diverse, and
temporally scattered content. Although video large language models (Video-LLMs)
can process videos lasting tens of minutes, applying them to truly long
sequences is computationally prohibitive and often leads to unfocused or
inconsistent reasoning. A promising solution is to select only the most
informative frames, yet existing approaches typically ignore temporal
dependencies or rely on unimodal evidence, limiting their ability to provide
complete and query-relevant context. We propose a Semantic-Visual Consensus
Evidence Selection (SeViCES) framework for effective and reliable long video
understanding. SeViCES is training-free and model-agnostic, and introduces two
key components. The Semantic-Visual Consensus Frame Selection (SVCFS) module
selects frames through (1) a temporal-aware semantic branch that leverages LLM
reasoning over captions, and (2) a cluster-guided visual branch that aligns
embeddings with semantic scores via mutual information. The Answer Consensus
Refinement (ACR) module further resolves inconsistencies between semantic- and
visual-based predictions by fusing evidence and constraining the answer space.
Extensive experiments on long video understanding benchmarks show that SeViCES
consistently outperforms state-of-the-art methods in both accuracy and
robustness, demonstrating the importance of consensus-driven evidence selection
for Video-LLMs.</p>
<h3 id="30-better-tokens-for-better-3d-advancing-vision-language-modeling-in-3d-medical-imaging">[30] <a href="https://arxiv.org/abs/2510.20639">Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging</a></h3>
<p><em>Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit, Hadrien Reynaud, Dong Yang, Pengfei Guo, Marc Edgar, Daguang Xu, Bernhard Kainz, Bjoern Menze</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>BTB3Dæå‡ºäº†ä¸€ç§å› æœå·ç§¯ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œé€šè¿‡é¢‘ç‡æ„ŸçŸ¥çš„ä½“ç´ æ ‡è®°åŒ–å’Œä¸‰é˜¶æ®µè®­ç»ƒè¯¾ç¨‹ï¼Œè§£å†³äº†3DåŒ»å­¦å½±åƒä¸­é«˜åˆ†è¾¨ç‡é•¿åºåˆ—å¤„ç†éš¾é¢˜ï¼Œåœ¨æŠ¥å‘Šç”Ÿæˆå’Œæ–‡æœ¬åˆ°CTåˆæˆä»»åŠ¡ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰3DåŒ»å­¦å½±åƒçš„è§†è§‰è¯­è¨€å»ºæ¨¡æ–¹æ³•åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡é•¿åºåˆ—ä½“ç§¯æ—¶é¢ä¸´æŒ‘æˆ˜ï¼šå¯¹æ¯”é¢„è®­ç»ƒäº§ç”Ÿçš„è§†è§‰ç¼–ç å™¨ä¸ä¸´åºŠè¯­è¨€å­˜åœ¨é”™ä½ï¼Œåˆ‡ç‰‡çº§æ ‡è®°åŒ–ä¼šæ¨¡ç³Šç²¾ç»†è§£å‰–ç»“æ„ï¼Œä»è€Œé™ä½ä¸‹æ¸¸ä»»åŠ¡çš„è¯Šæ–­æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> BTB3Dé‡‡ç”¨å› æœå·ç§¯ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œç»Ÿä¸€2Då’Œ3Dè®­ç»ƒä¸æ¨ç†ï¼Œç”Ÿæˆç´§å‡‘çš„é¢‘ç‡æ„ŸçŸ¥ä½“ç´ æ ‡è®°ã€‚é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒè¯¾ç¨‹å®ç°å±€éƒ¨é‡å»ºã€é‡å çª—å£å¹³é“ºå’Œé•¿ä¸Šä¸‹æ–‡è§£ç å™¨ç²¾ç‚¼ï¼Œæ¨¡å‹ä»çŸ­åˆ‡ç‰‡æ‘˜å½•ä¸­å­¦ä¹ ä½†èƒ½æ³›åŒ–åˆ°è¶…è¿‡300åˆ‡ç‰‡çš„æ‰«æè€Œä¸å¢åŠ å†…å­˜å¼€é”€ã€‚</p>
<p><strong>Result:</strong> BTB3Dåœ¨æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šæ¯”CT2Repã€CT-CHATå’ŒMerlinæé«˜äº†BLEUåˆ†æ•°ï¼Œä¸´åºŠF1åˆ†æ•°å¢åŠ äº†40%ï¼›åœ¨æ–‡æœ¬åˆ°CTåˆæˆä»»åŠ¡ä¸Šæ¯”GenerateCTå’ŒMedSynå°†FIDé™ä½äº†75%ï¼ŒFVDå‡åŠï¼Œç”Ÿæˆäº†è§£å‰–å­¦ä¸€è‡´çš„512<em>512</em>241ä½“ç§¯å›¾åƒã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯å®ç²¾ç¡®çš„ä¸‰ç»´æ ‡è®°åŒ–è€Œéä»…ä¾èµ–æ›´å¤§çš„è¯­è¨€éª¨å¹²ç½‘ç»œï¼Œå¯¹äº3DåŒ»å­¦å½±åƒä¸­å¯æ‰©å±•çš„è§†è§‰è¯­è¨€å»ºæ¨¡è‡³å…³é‡è¦ã€‚è¯¥æ–¹æ³•ä¸ºé«˜åˆ†è¾¨ç‡é•¿åºåˆ—åŒ»å­¦å½±åƒå¤„ç†æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†åŒ»å­¦å½±åƒåˆ†æçš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Recent progress in vision-language modeling for 3D medical imaging has been
fueled by large-scale computed tomography (CT) corpora with paired free-text
reports, stronger architectures, and powerful pretrained models. This has
enabled applications such as automated report generation and text-conditioned
3D image synthesis. Yet, current approaches struggle with high-resolution,
long-sequence volumes: contrastive pretraining often yields vision encoders
that are misaligned with clinical language, and slice-wise tokenization blurs
fine anatomy, reducing diagnostic performance on downstream tasks. We introduce
BTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder
that unifies 2D and 3D training and inference while producing compact,
frequency-aware volumetric tokens. A three-stage training curriculum enables
(i) local reconstruction, (ii) overlapping-window tiling, and (iii)
long-context decoder refinement, during which the model learns from short slice
excerpts yet generalizes to scans exceeding 300 slices without additional
memory overhead. BTB3D sets a new state-of-the-art on two key tasks: it
improves BLEU scores and increases clinical F1 by 40% over CT2Rep, CT-CHAT, and
Merlin for report generation; and it reduces FID by 75% and halves FVD compared
to GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically
consistent 512<em>512</em>241 volumes. These results confirm that precise
three-dimensional tokenization, rather than larger language backbones alone, is
essential for scalable vision-language modeling in 3D medical imaging. The
codebase is available at: https://github.com/ibrahimethemhamamci/BTB3D</p>
<h3 id="31-ultrahr-100k-enhancing-uhr-image-synthesis-with-a-large-scale-high-quality-dataset">[31] <a href="https://arxiv.org/abs/2510.20661">UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset</a></h3>
<p><em>Chen Zhao, En Ci, Yunzhe Xu, Tiehan Fan, Shanyan Guan, Yanhao Ge, Jian Yang, Ying Tai</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UltraHR-100Kè¶…é«˜æ¸…æ–‡æœ¬åˆ°å›¾åƒæ•°æ®é›†å’Œé¢‘ç‡æ„ŸçŸ¥åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ç»†èŠ‚å¯¼å‘æ—¶é—´æ­¥é‡‡æ ·å’Œè½¯åŠ æƒé¢‘ç‡æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†è¶…é«˜æ¸…å›¾åƒç”Ÿæˆçš„ç»†èŠ‚è´¨é‡å’Œæ•´ä½“ä¿çœŸåº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è¶…é«˜æ¸…æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡è¶…é«˜æ¸…æ•°æ®é›†ï¼Œä»¥åŠç¼ºä¹é’ˆå¯¹è¶…é«˜æ¸…åœºæ™¯ä¸‹ç»†ç²’åº¦ç»†èŠ‚åˆæˆçš„å®šåˆ¶åŒ–è®­ç»ƒç­–ç•¥ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨è¶…é«˜åˆ†è¾¨ç‡ä¸‹ç”Ÿæˆç²¾ç»†ç»†èŠ‚çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æ–¹æ³•åŒ…æ‹¬æ„å»ºUltraHR-100Kæ•°æ®é›†ï¼ˆåŒ…å«10ä¸‡å¼ è¶…è¿‡3Kåˆ†è¾¨ç‡çš„ç²¾é€‰å›¾åƒï¼‰å’Œæå‡ºé¢‘ç‡æ„ŸçŸ¥åè®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨ç»†èŠ‚å¯¼å‘æ—¶é—´æ­¥é‡‡æ ·èšç„¦äºç»†èŠ‚å…³é”®çš„å»å™ªæ­¥éª¤ï¼Œä»¥åŠè½¯åŠ æƒé¢‘ç‡æ­£åˆ™åŒ–åˆ©ç”¨ç¦»æ•£å‚…é‡Œå¶å˜æ¢è½¯çº¦æŸé¢‘ç‡åˆ†é‡ä»¥ä¿ƒè¿›é«˜é¢‘ç»†èŠ‚ä¿ç•™ã€‚</p>
<p><strong>Result:</strong> åœ¨æå‡ºçš„UltraHR-eval4KåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†è¶…é«˜æ¸…å›¾åƒç”Ÿæˆçš„ç»†ç²’åº¦ç»†èŠ‚è´¨é‡å’Œæ•´ä½“ä¿çœŸåº¦ï¼ŒéªŒè¯äº†æ‰€ææ•°æ®é›†å’Œè®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè¶…é«˜æ¸…æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæä¾›äº†é‡è¦çš„æ•°æ®é›†èµ„æºå’Œè®­ç»ƒæ–¹æ³•ï¼Œå¼ºè°ƒäº†é’ˆå¯¹é«˜é¢‘ç»†èŠ‚çš„ä¸“é—¨ä¼˜åŒ–ç­–ç•¥åœ¨æå‡å›¾åƒè´¨é‡ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºæœªæ¥è¶…é«˜æ¸…ç”Ÿæˆæ¨¡å‹çš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable
progress. However, two key challenges remain : 1) the absence of a large-scale
high-quality UHR T2I dataset, and (2) the neglect of tailored training
strategies for fine-grained detail synthesis in UHR scenarios. To tackle the
first challenge, we introduce \textbf{UltraHR-100K}, a high-quality dataset of
100K UHR images with rich captions, offering diverse content and strong visual
fidelity. Each image exceeds 3K resolution and is rigorously curated based on
detail richness, content complexity, and aesthetic quality. To tackle the
second challenge, we propose a frequency-aware post-training method that
enhances fine-detail generation in T2I diffusion models. Specifically, we
design (i) \textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning
on detail-critical denoising steps, and (ii) \textit{Soft-Weighting Frequency
Regularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to
softly constrain frequency components, encouraging high-frequency detail
preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks
demonstrate that our approach significantly improves the fine-grained detail
quality and overall fidelity of UHR image generation. The code is available at
\href{https://github.com/NJU-PCALab/UltraHR-100k}{here}.</p>
<h3 id="32-diagnosing-visual-reasoning-challenges-insights-and-a-path-forward">[32] <a href="https://arxiv.org/abs/2510.20696">Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward</a></h3>
<p><em>Jing Bi, Guangyu Sun, Ali Vosoughi, Chen Chen, Chenliang Xu</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä»£ç†çš„æ¶æ„ï¼Œå°†LLMæ¨ç†ä¸è½»é‡çº§è§†è§‰æ¨¡å—ç›¸ç»“åˆï¼Œä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰å¹»è§‰å’Œè¿‡åº¦ä¾èµ–æ–‡æœ¬å…ˆéªŒé—®é¢˜ã€‚è¯¥ç³»ç»Ÿåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒåŒ¹é…æˆ–è¶…è¶Šæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨ç†æ—¶ï¼Œè™½ç„¶åˆ©ç”¨æ€ç»´é“¾æç¤ºå¤„ç†å¤æ‚è§†è§‰ä»»åŠ¡ï¼Œä½†ä»å­˜åœ¨è§†è§‰å¹»è§‰å’Œè¿‡åº¦ä¾èµ–æ–‡æœ¬å…ˆéªŒçš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡ç³»ç»Ÿè¯Šæ–­æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ­ç¤ºå…³é”®å¤±è´¥æ¨¡å¼å¹¶è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºä»£ç†çš„æ¶æ„ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸è½»é‡çº§è§†è§‰æ¨¡å—ç›¸ç»“åˆï¼Œæ”¯æŒå¯¹æ¨ç†é“¾è¿›è¡Œç»†ç²’åº¦åˆ†æå’Œè¿­ä»£ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶å¯¹ç°æœ‰æ¨¡å‹è¿›è¡Œç³»ç»Ÿæ€§è¯Šæ–­ï¼Œå¹¶å¼€å‘ä¸“é—¨çš„è§†è§‰å†…å®¹åˆ†æå·¥å…·ã€‚</p>
<p><strong>Result:</strong> æ‰€æå‡ºçš„ç³»ç»Ÿåœ¨MMMUåŸºå‡†ä¸Šå®ç°äº†+10.3åˆ†çš„æ˜¾è‘—æå‡ï¼Œåœ¨MathVistaåŸºå‡†ä¸Šå®ç°äº†+6.0åˆ†çš„æå‡ï¼Œè¶…è¶Šäº†7Bå‚æ•°åŸºçº¿æ¨¡å‹ï¼Œå¹¶åŒ¹é…æˆ–è¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶å›¢é˜Ÿå°†å‘å¸ƒæ¡†æ¶å’Œè¯„ä¼°å¥—ä»¶ä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœªæ¥çš„è§†è§‰æ¨ç†æ¨¡å‹åº”ä¸“æ³¨äºæ•´åˆæ›´å¹¿æ³›çš„ä¸“é—¨åŒ–å·¥å…·æ¥åˆ†æè§†è§‰å†…å®¹ã€‚åŸºäºä»£ç†çš„æ¶æ„ç»“åˆè½»é‡çº§è§†è§‰æ¨¡å—çš„æ–¹æ³•ä¸ºè§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„å…³é”®æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) that integrate visual and textual
reasoning leverage chain-of-thought (CoT) prompting to tackle complex visual
tasks, yet continue to exhibit visual hallucinations and an over-reliance on
textual priors. We present a systematic diagnosis of state-of-the-art
vision-language models using a three-stage evaluation framework, uncovering key
failure modes. To address these, we propose an agent-based architecture that
combines LLM reasoning with lightweight visual modules, enabling fine-grained
analysis and iterative refinement of reasoning chains. Our results highlight
future visual reasoning models should focus on integrating a broader set of
specialized tools for analyzing visual content. Our system achieves significant
gains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or
surpassing much larger models. We will release our framework and evaluation
suite to facilitate future research.</p>
<h3 id="33-mixing-importance-with-diversity-joint-optimization-for-kv-cache-compression-in-large-vision-language-models">[33] <a href="https://arxiv.org/abs/2510.20707">Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models</a></h3>
<p><em>Xuyang Liu, Xiyan Gui, Yuchao Zhang, Linfeng Zhang</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MixKVæ–¹æ³•ï¼Œé€šè¿‡æ··åˆé‡è¦æ€§å’Œå¤šæ ·æ€§æ¥ä¼˜åŒ–å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„KVç¼“å­˜å‹ç¼©ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä»…å…³æ³¨é‡è¦æ€§è€Œå¿½ç•¥æ¨¡æ€ç‰¹å®šè¯­ä¹‰å†—ä½™æ¨¡å¼çš„é—®é¢˜ï¼Œåœ¨æç«¯å‹ç¼©æ¡ä»¶ä¸‹æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç†è§£ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ‰©å±•å¤šæ¨¡æ€åºåˆ—æ—¶é¢ä¸´KVç¼“å­˜è†¨èƒ€å¯¼è‡´çš„å†…å­˜ç“¶é¢ˆé—®é¢˜ï¼Œè€Œç°æœ‰çš„KVç¼“å­˜å‹ç¼©æ–¹æ³•ä¸»è¦å…³æ³¨ä¿ç•™é«˜é‡è¦æ€§KVå¯¹ä»¥æœ€å°åŒ–å­˜å‚¨ï¼Œå´å¿½ç•¥äº†å¤šæ¨¡æ€KVç¼“å­˜ä¸­å‡ºç°çš„æ¨¡æ€ç‰¹å®šè¯­ä¹‰å†—ä½™æ¨¡å¼ï¼Œä»…ä¾èµ–é‡è¦æ€§åªèƒ½è¦†ç›–KVç¼“å­˜ä¿¡æ¯åˆ†å¸ƒçš„å­é›†ï¼Œå¯èƒ½å¯¼è‡´è¯­ä¹‰è¦†ç›–æŸå¤±ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†MixKVæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ†æLVLMsä¸­KVç¼“å­˜åœ¨ä¸åŒæ³¨æ„åŠ›å¤´é—´è¡¨ç°å‡ºçš„å†—ä½™åº¦å˜åŒ–ï¼Œè‡ªé€‚åº”åœ°é€‚åº”å¤´çº§è¯­ä¹‰å†—ä½™ï¼Œåœ¨å‹ç¼©KVå¯¹æ—¶é€‰æ‹©æ€§å¹³è¡¡å¤šæ ·æ€§å’Œé‡è¦æ€§ï¼Œä»è€Œä¼˜åŒ–KVç¼“å­˜å‹ç¼©æ•ˆæœã€‚</p>
<p><strong>Result:</strong> åœ¨æç«¯å‹ç¼©æ¡ä»¶ä¸‹ï¼ˆé¢„ç®—=64ï¼‰ï¼ŒMixKVåœ¨äº”ä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†ä¸Šå¹³å‡æå‡åŸºçº¿æ–¹æ³•5.1%ï¼Œåœ¨GUI groundingä»»åŠ¡ä¸Šå¯¹SnapKVå’ŒAdaKVåˆ†åˆ«å®ç°äº†8.0%å’Œ9.0%çš„æ˜¾è‘—å¢ç›Šï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“çš„æ¨ç†æ•ˆç‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ— ç¼æ‰©å±•åˆ°LLMså¹¶è·å¾—å¯æ¯”æ€§èƒ½æå‡ã€‚</p>
<p><strong>Conclusion:</strong> MixKVé€šè¿‡æ··åˆé‡è¦æ€§å’Œå¤šæ ·æ€§ç­–ç•¥æœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€KVç¼“å­˜å‹ç¼©ä¸­çš„è¯­ä¹‰å†—ä½™é—®é¢˜ï¼Œè¯æ˜äº†è€ƒè™‘å¤´çº§è¯­ä¹‰å†—ä½™æ¨¡å¼å¯¹äºä¼˜åŒ–å‹ç¼©æ€§èƒ½çš„é‡è¦æ€§ï¼Œä¸ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„éƒ¨ç½²å¯æ‰©å±•æ€§æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†å‘çº¯è¯­è¨€æ¨¡å‹çš„è‰¯å¥½æ‰©å±•æ€§ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Recent large vision-language models (LVLMs) demonstrate remarkable
capabilities in processing extended multi-modal sequences, yet the resulting
key-value (KV) cache expansion creates a critical memory bottleneck that
fundamentally limits deployment scalability. While existing KV cache
compression methods focus on retaining high-importance KV pairs to minimize
storage, they often overlook the modality-specific semantic redundancy patterns
that emerge distinctively in multi-modal KV caches. In this work, we first
analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying
levels of redundancy across attention heads. We show that relying solely on
importance can only cover a subset of the full KV cache information
distribution, leading to potential loss of semantic coverage. To address this,
we propose \texttt{MixKV}, a novel method that mixes importance with diversity
for optimized KV cache compression in LVLMs. \texttt{MixKV} adapts to head-wise
semantic redundancy, selectively balancing diversity and importance when
compressing KV pairs. Extensive experiments demonstrate that \texttt{MixKV}
consistently enhances existing methods across multiple LVLMs. Under extreme
compression (budget=64), \texttt{MixKV} improves baseline methods by an average
of \textbf{5.1\%} across five multi-modal understanding benchmarks and achieves
remarkable gains of \textbf{8.0\%} and \textbf{9.0\%} for SnapKV and AdaKV on
GUI grounding tasks, all while maintaining comparable inference efficiency.
Furthermore, \texttt{MixKV} extends seamlessly to LLMs with comparable
performance gains. Our code is available at
\href{https://github.com/xuyang-liu16/MixKV}{\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.</p>
<h3 id="34-argenseg-image-segmentation-with-autoregressive-image-generation-model">[34] <a href="https://arxiv.org/abs/2510.20803">ARGenSeg: Image Segmentation with Autoregressive Image Generation Model</a></h3>
<p><em>Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªå›å½’ç”Ÿæˆçš„å›¾åƒåˆ†å‰²èŒƒå¼ARGenSegï¼Œé€šè¿‡å›¾åƒç”Ÿæˆæ–¹å¼å®ç°å¤šæ¨¡æ€ç†è§£å’Œåƒç´ çº§æ„ŸçŸ¥çš„ç»Ÿä¸€æ¡†æ¶ï¼Œåœ¨å¤šä¸ªåˆ†å‰²æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•å¹¶æ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å°†å›¾åƒåˆ†å‰²é›†æˆåˆ°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨è¾¹ç•Œç‚¹è¡¨ç¤ºæˆ–ä¸“ç”¨åˆ†å‰²å¤´ï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºç¦»æ•£è¡¨ç¤ºæˆ–è¾“å…¥ä»»åŠ¡ç‰¹å®šè§£ç å™¨çš„è¯­ä¹‰æç¤ºï¼Œé™åˆ¶äº†MLLMæ•æ‰ç»†ç²’åº¦è§†è§‰ç»†èŠ‚çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŸºäºå›¾åƒç”Ÿæˆçš„åˆ†å‰²æ¡†æ¶ï¼Œåˆ©ç”¨MLLMè¾“å‡ºè§†è§‰æ ‡è®°å¹¶é€šè¿‡é€šç”¨VQ-VAEå°†å…¶è§£ç ä¸ºå›¾åƒï¼Œä½¿åˆ†å‰²å®Œå…¨ä¾èµ–äºMLLMçš„åƒç´ çº§ç†è§£ï¼›é‡‡ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹ç­–ç•¥å¹¶è¡Œç”Ÿæˆæ‰€éœ€è§†è§‰æ ‡è®°ä»¥å‡å°‘æ¨ç†å»¶è¿Ÿã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåˆ†å‰²æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¶…è¶Šäº†å…ˆå‰çš„state-of-the-artæ–¹æ³•ï¼Œæ¨ç†é€Ÿåº¦æ˜¾è‘—æå‡ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºå›¾åƒç”Ÿæˆçš„åˆ†å‰²èŒƒå¼èƒ½å¤Ÿæœ‰æ•ˆç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œåƒç´ çº§æ„ŸçŸ¥ï¼Œä¸ºMLLMåœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ï¼ŒåŒæ—¶é€šè¿‡å¹¶è¡ŒåŒ–ç­–ç•¥è§£å†³äº†ç”Ÿæˆå¼æ–¹æ³•çš„æ•ˆç‡é—®é¢˜ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>We propose a novel AutoRegressive Generation-based paradigm for image
Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level
perception within a unified framework. Prior works integrating image
segmentation into multimodal large language models (MLLMs) typically employ
either boundary points representation or dedicated segmentation heads. These
methods rely on discrete representations or semantic prompts fed into
task-specific decoders, which limits the ability of the MLLM to capture
fine-grained visual details. To address these challenges, we introduce a
segmentation framework for MLLM based on image generation, which naturally
produces dense masks for target objects. We leverage MLLM to output visual
tokens and detokenize them into images using an universal VQ-VAE, making the
segmentation fully dependent on the pixel-level understanding of the MLLM. To
reduce inference latency, we employ a next-scale-prediction strategy to
generate required visual tokens in parallel. Extensive experiments demonstrate
that our method surpasses prior state-of-the-art approaches on multiple
segmentation datasets with a remarkable boost in inference speed, while
maintaining strong understanding capabilities.</p>
<h3 id="35-layercomposer-interactive-personalized-t2i-via-spatially-aware-layered-canvas">[35] <a href="https://arxiv.org/abs/2510.20820">LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas</a></h3>
<p><em>Guocheng Gordon Qian, Ruihang Zhang, Tsai-Shien Chen, Yusuf Dalva, Anujraaj Argo Goyal, Willi Menapace, Ivan Skorokhodov, Meng Dong, Arpit Sahni, Daniil Ostashev, Ju Hu, Sergey Tulyakov, Kuan-Chieh Jackson Wang</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†LayerComposerï¼Œä¸€ä¸ªç”¨äºä¸ªæ€§åŒ–å¤šä¸»ä½“æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„äº¤äº’å¼æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚ç”»å¸ƒè¡¨ç¤ºå’Œé”å®šæœºåˆ¶å®ç°äº†å¯¹ç©ºé—´ç»„åˆçš„ç²¾ç¡®æ§åˆ¶ï¼Œå¹¶åœ¨å¤šä¸»ä½“ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„ä¸ªæ€§åŒ–ç”Ÿæˆæ¨¡å‹è™½ç„¶è§†è§‰ä¿çœŸåº¦é«˜ï¼Œä½†ç¼ºä¹å¯¹ç©ºé—´ç»„åˆçš„äº¤äº’æ§åˆ¶ï¼Œå¹¶ä¸”åœ¨å¤„ç†å¤šä¸ªä¸»ä½“æ—¶æ‰©å±•æ€§å·®ï¼Œè¿™é™åˆ¶äº†å®é™…åº”ç”¨ä¸­çš„çµæ´»æ€§å’Œç”¨æˆ·æ§åˆ¶èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•å¼•å…¥äº†åˆ†å±‚ç”»å¸ƒè¡¨ç¤ºï¼Œå°†æ¯ä¸ªä¸»ä½“ç½®äºç‹¬ç«‹å±‚ä¸­å®ç°æ— é®æŒ¡ç»„åˆï¼Œå¹¶æå‡ºäº†é”å®šæœºåˆ¶æ¥ä¿æŒé€‰å®šå±‚çš„é«˜ä¿çœŸåº¦ï¼ŒåŒæ—¶å…è®¸å…¶ä»–å±‚çµæ´»é€‚åº”ä¸Šä¸‹æ–‡ï¼Œè¯¥æ–¹æ³•æ— éœ€æ¶æ„ä¿®æ”¹ï¼Œä¾èµ–ä½ç½®åµŒå…¥å’Œäº’è¡¥æ•°æ®é‡‡æ ·ç­–ç•¥ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒLayerComposeråœ¨å¤šä¸»ä½“ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆä¸­ç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨ç©ºé—´æ§åˆ¶å’Œèº«ä»½ä¿æŒæ–¹é¢å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œæä¾›äº†æ›´å¥½çš„äº¤äº’æ§åˆ¶èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åˆ†å±‚è¡¨ç¤ºå’Œé”å®šæœºåˆ¶åœ¨ä¸ªæ€§åŒ–ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šä¸»ä½“å›¾åƒåˆæˆæä¾›äº†æ–°çš„äº¤äº’èŒƒå¼ï¼Œæœªæ¥å¯æ‰©å±•åˆ°æ›´å¤æ‚çš„åœºæ™¯ç»„åˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Despite their impressive visual fidelity, existing personalized generative
models lack interactive control over spatial composition and scale poorly to
multiple subjects. To address these limitations, we present LayerComposer, an
interactive framework for personalized, multi-subject text-to-image generation.
Our approach introduces two main contributions: (1) a layered canvas, a novel
representation in which each subject is placed on a distinct layer, enabling
occlusion-free composition; and (2) a locking mechanism that preserves selected
layers with high fidelity while allowing the remaining layers to adapt flexibly
to the surrounding context. Similar to professional image-editing software, the
proposed layered canvas allows users to place, resize, or lock input subjects
through intuitive layer manipulation. Our versatile locking mechanism requires
no architectural changes, relying instead on inherent positional embeddings
combined with a new complementary data sampling strategy. Extensive experiments
demonstrate that LayerComposer achieves superior spatial control and identity
preservation compared to the state-of-the-art methods in multi-subject
personalized image generation.</p>
<h3 id="36-holocine-holistic-generation-of-cinematic-multi-shot-long-video-narratives">[36] <a href="https://arxiv.org/abs/2510.20822">HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives</a></h3>
<p><em>Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, Yujun Shen, Huamin Qu</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>HoloCineæå‡ºäº†ä¸€ç§æ•´ä½“ç”Ÿæˆè¿è´¯å¤šé•œå¤´å™äº‹è§†é¢‘çš„æ¨¡å‹ï¼Œé€šè¿‡çª—å£äº¤å‰æ³¨æ„åŠ›å’Œç¨€ç–é•œå¤´é—´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè§£å†³äº†ç°æœ‰æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹åœ¨å™äº‹ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„ç”µå½±åˆ¶ä½œèƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹æ“…é•¿ç”Ÿæˆå­¤ç«‹ç‰‡æ®µï¼Œä½†åœ¨åˆ›å»ºè¿è´¯çš„å¤šé•œå¤´å™äº‹æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œè¿™ç§"å™äº‹é¸¿æ²Ÿ"é™åˆ¶äº†çœŸæ­£çš„æ•…äº‹è®²è¿°èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> HoloCineé‡‡ç”¨çª—å£äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å°†æ–‡æœ¬æç¤ºå®šä½åˆ°ç‰¹å®šé•œå¤´ï¼ŒåŒæ—¶ä½¿ç”¨ç¨€ç–é•œå¤´é—´è‡ªæ³¨æ„åŠ›æ¨¡å¼ï¼ˆé•œå¤´å†…å¯†é›†ä½†é•œå¤´é—´ç¨€ç–ï¼‰ï¼Œç¡®ä¿åˆ†é’Ÿçº§ç”Ÿæˆæ•ˆç‡çš„åŒæ—¶ä¿æŒå…¨å±€ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> HoloCineåœ¨å™äº‹è¿è´¯æ€§æ–¹é¢è®¾ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡å‡†ï¼Œå¹¶å±•ç°å‡ºæ˜¾è‘—çš„æ–°å…´èƒ½åŠ›ï¼šå¯¹è§’è‰²å’Œåœºæ™¯çš„æŒä¹…è®°å¿†ï¼Œä»¥åŠå¯¹ç”µå½±æŠ€æœ¯çš„ç›´è§‚ç†è§£ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹å·¥ä½œæ ‡å¿—ç€ä»ç‰‡æ®µåˆæˆåˆ°è‡ªåŠ¨åŒ–ç”µå½±åˆ¶ä½œçš„å…³é”®è½¬å˜ï¼Œä½¿ç«¯åˆ°ç«¯çš„ç”µå½±åˆ›ä½œæˆä¸ºå¯å®ç°çš„æœªæ¥ï¼Œä¸ºè¿è´¯å™äº‹è§†é¢‘ç”Ÿæˆå¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>State-of-the-art text-to-video models excel at generating isolated clips but
fall short of creating the coherent, multi-shot narratives, which are the
essence of storytelling. We bridge this "narrative gap" with HoloCine, a model
that generates entire scenes holistically to ensure global consistency from the
first shot to the last. Our architecture achieves precise directorial control
through a Window Cross-Attention mechanism that localizes text prompts to
specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within
shots but sparse between them) ensures the efficiency required for minute-scale
generation. Beyond setting a new state-of-the-art in narrative coherence,
HoloCine develops remarkable emergent abilities: a persistent memory for
characters and scenes, and an intuitive grasp of cinematic techniques. Our work
marks a pivotal shift from clip synthesis towards automated filmmaking, making
end-to-end cinematic creation a tangible future. Our code is available at:
https://holo-cine.github.io/.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="37-from-denoising-to-refining-a-corrective-framework-for-vision-language-diffusion-model">[37] <a href="https://arxiv.org/abs/2510.19871">From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model</a></h3>
<p><em>Yatai Ji, Teng Wang, Yuying Ge, Zhiheng Liu, Sidi Yang, Ying Shan, Ping Luo</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºReDiffæ¡†æ¶ï¼Œå°†ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ä»è¢«åŠ¨å»å™ªé‡æ„ä¸ºä¸»åŠ¨ç²¾ç‚¼ï¼Œé€šè¿‡æ•™å¯¼æ¨¡å‹è¯†åˆ«å’Œä¿®æ­£è‡ªèº«é”™è¯¯æ¥è§£å†³å¹¶è¡Œè§£ç ä¸­çš„é”™è¯¯çº§è”é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå†…å®¹çš„è¿è´¯æ€§å’Œäº‹å®å‡†ç¡®æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­é¢ä¸´ä¸¥é‡çš„è®­ç»ƒ-æ¨ç†å·®å¼‚é—®é¢˜ï¼Œå¹¶è¡Œè§£ç è¿‡ç¨‹ä¸­çš„åˆå§‹ä»¤ç‰Œé”™è¯¯ä¼šæ±¡æŸ“ç”Ÿæˆä¸Šä¸‹æ–‡ï¼Œå¼•å‘é”™è¯¯çº§è”æ•ˆåº”ï¼Œå¯¼è‡´è¯­æ³•é”™è¯¯å’Œè¯­ä¹‰å¹»è§‰ï¼Œè¿™ä¸¥é‡é˜»ç¢äº†å…¶å®é™…åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> ReDiffæ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼šé¦–å…ˆé€šè¿‡è®­ç»ƒæ¨¡å‹ä¿®æ­£åˆæˆé”™è¯¯æ¥å»ºç«‹åŸºç¡€ä¿®è®¢èƒ½åŠ›ï¼Œç„¶åå®ç°æ–°é¢–çš„åœ¨çº¿è‡ªæ ¡æ­£å¾ªç¯ï¼Œæ¨¡å‹é€šè¿‡ä»ä¸“å®¶ä¿®æ­£ä¸­å­¦ä¹ æ¥æ˜¾å¼è®­ç»ƒä¿®æ­£è‡ªèº«æœ‰ç¼ºé™·çš„è‰ç¨¿ï¼Œè¿™ç§é”™è¯¯é©±åŠ¨å­¦ä¹ èµ‹äºˆæ¨¡å‹é‡æ–°å®¡è§†å’Œç²¾ç‚¼å·²ç”Ÿæˆè¾“å‡ºçš„å…³é”®èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ReDiffæ˜¾è‘—æå‡äº†ç”Ÿæˆå†…å®¹çš„è¿è´¯æ€§å’Œäº‹å®å‡†ç¡®æ€§ï¼Œå®ç°äº†è¿œä¼˜äºä¼ ç»Ÿå»å™ªæ–¹æ³•çš„ç¨³å®šé«˜æ•ˆå¹¶è¡Œç”Ÿæˆï¼Œæœ‰æ•ˆæ‰“ç ´äº†é”™è¯¯çº§è”æ•ˆåº”ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å°†ç”Ÿæˆè¿‡ç¨‹ä»è¢«åŠ¨å»å™ªè½¬å‘ä¸»åŠ¨ç²¾ç‚¼çš„æœ‰æ•ˆæ€§ï¼Œé”™è¯¯é©±åŠ¨å­¦ä¹ æ–¹æ³•ä¸ºè§£å†³æ‰©æ•£æ¨¡å‹ä¸­çš„é”™è¯¯ä¼ æ’­é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œä¸ºç¨³å®šé«˜æ•ˆçš„å¹¶è¡Œç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>Discrete diffusion models have emerged as a promising direction for
vision-language tasks, offering bidirectional context modeling and theoretical
parallelization. However, their practical application is severely hindered by a
train-inference discrepancy, which leads to catastrophic error cascades:
initial token errors during parallel decoding pollute the generation context,
triggering a chain reaction of compounding errors and leading to syntactic
errors and semantic hallucinations. To address this fundamental challenge, we
reframe the generation process from passive denoising to active refining. We
introduce ReDiff, a refining-enhanced diffusion framework that teaches the
model to identify and correct its own errors. Our approach features a two-stage
training process: first, we instill a foundational revision capability by
training the model to revise synthetic errors; second, we implement a novel
online self-correction loop where the model is explicitly trained to revise its
own flawed drafts by learning from an expert's corrections. This mistake-driven
learning endows the model with the crucial ability to revisit and refine its
already generated output, effectively breaking the error cascade. Extensive
experiments demonstrate that ReDiff significantly improves the coherence and
factual accuracy of generated content, enabling stable and efficient parallel
generation far superior to traditional denoising methods. Our codes and models
are available at https://rediff-hku.github.io/.</p>
<h3 id="38-can-they-dixit-yes-they-can-dixit-as-a-playground-for-multimodal-language-model-capabilities">[38] <a href="https://arxiv.org/abs/2510.19892">Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities</a></h3>
<p><em>Nishant Balepur, Dang Nguyen, Dayeon Ki</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºåŸºäºæ¸¸æˆçš„è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡Dixitå¹»æƒ³å¡ç‰Œæ¸¸æˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶æµ‹è¯•å¤šç§èƒ½åŠ›ï¼Œæä¾›å®¢è§‚ä¸”å…·å¸å¼•åŠ›çš„è¯„ä¼°æ¡†æ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ä¸»è¦ä¾èµ–é™æ€åŸºå‡†æµ‹è¯•æˆ–ä¸»è§‚çš„äººå·¥æ¯”è¾ƒï¼Œè¿™äº›æ–¹æ³•æ— æ³•å…¨é¢è¯„ä¼°æ¨¡å‹èƒ½åŠ›ã€æˆæœ¬é«˜æ˜‚ä¸”å®¹æ˜“è¢«æ¨¡å‹åˆ©ç”¨è¡¨é¢ç‰¹å¾ï¼ˆå¦‚å†—é•¿æ€§ï¼‰æ¥è™šå¢èƒœç‡ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŸºäºæ¸¸æˆçš„è¯„ä¼°æ¡†æ¶ï¼Œå…·ä½“å®ç°ä¸ºDixitå¹»æƒ³å¡ç‰Œæ¸¸æˆï¼Œè¦æ±‚ç©å®¶ä¸ºå¡ç‰Œç”Ÿæˆèƒ½å¤Ÿæ¬ºéª—éƒ¨åˆ†è€Œéå…¨éƒ¨ç©å®¶çš„æè¿°ï¼Œä»è€ŒåŒæ—¶æµ‹è¯•æ¨¡å‹çš„å¤šç§æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> äº”ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨Dixitæ¸¸æˆä¸­çš„èƒœç‡æ’åä¸ä¸»æµåŸºå‡†æµ‹è¯•ç»“æœå®Œå…¨ä¸€è‡´ï¼ŒåŒæ—¶äººæœºå¯¹æˆ˜æ­ç¤ºäº†æ¨¡å‹ç­–ç•¥ä¸äººç±»ç­–ç•¥çš„å·®å¼‚ä»¥åŠæ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ”¹è¿›ç©ºé—´ã€‚</p>
<p><strong>Conclusion:</strong> æ¸¸æˆåŒ–è¯„ä¼°ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›äº†æ›´å…¨é¢ã€å®¢è§‚ä¸”å…·å¸å¼•åŠ›çš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿæ­ç¤ºæ¨¡å‹åœ¨çœŸå®äº¤äº’ç¯å¢ƒä¸­çš„èƒ½åŠ›å±€é™ï¼Œä¸ºæœªæ¥æ¨¡å‹æ”¹è¿›æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>Multi-modal large language models (MLMs) are often assessed on static,
individual benchmarks -- which cannot jointly assess MLM capabilities in a
single task -- or rely on human or model pairwise comparisons -- which is
highly subjective, expensive, and allows models to exploit superficial
shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these
issues, we propose game-based evaluations to holistically assess MLM
capabilities. Games require multiple abilities for players to win, are
inherently competitive, and are governed by fix, objective rules, and makes
evaluation more engaging, providing a robust framework to address the
aforementioned challenges. We manifest this evaluation specifically through
Dixit, a fantasy card game where players must generate captions for a card that
trick some, but not all players, into selecting the played card. Our
quantitative experiments with five MLMs show Dixit win-rate rankings are
perfectly correlated with those on popular MLM benchmarks, while games between
human and MLM players in Dixit reveal several differences between agent
strategies and areas of improvement for MLM reasoning.</p>
<h3 id="39-are-stereotypes-leading-llms-zero-shot-stance-detection">[39] <a href="https://arxiv.org/abs/2510.20154">Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?</a></h3>
<p><em>Anthony Dubreuil, Antoine Gourru, Christine Largeron, Amine Trabelsi</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬ç«‹åœºæ£€æµ‹ä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—çš„ç¤¾ä¼šåè§ï¼Œå‘ç°æ¨¡å‹ä¼šé”™è¯¯åœ°å°†ç‰¹å®šè§‚ç‚¹ä¸ç‰¹å®šç¤¾ä¼šç¾¤ä½“çš„è¯­è¨€ç‰¹å¾ç›¸å…³è”ï¼Œå¦‚å°†æ”¯æŒå¤§éº»åˆæ³•åŒ–çš„ç«‹åœºä¸ä½æ–‡æœ¬å¤æ‚åº¦åŠéè£”ç¾å›½äººæ–¹è¨€è”ç³»èµ·æ¥ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è¯­è¨€æ¨¡å‹ä»é¢„è®­ç»ƒæ•°æ®ä¸­ç»§æ‰¿äº†åˆ»æ¿å°è±¡ï¼Œå¯¼è‡´åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å¯¹æŸäº›ç¤¾ä¼šç¾¤ä½“äº§ç”Ÿåè§è¡Œä¸ºï¼Œç„¶è€Œç«‹åœºæ£€æµ‹æ–¹æ³•ä¸­çš„æ­¤ç±»åè§è¯„ä¼°ä¸€ç›´è¢«ç ”ç©¶ç¤¾åŒºæ‰€å¿½è§†ã€‚ç«‹åœºæ£€æµ‹ä½œä¸ºæœ€æ•æ„Ÿçš„NLPä»»åŠ¡ä¹‹ä¸€ï¼Œå¸¸æ¶‰åŠæ”¿æ²»å€¾å‘åˆ¤æ–­ï¼Œå› æ­¤è¯„ä¼°LLMsåœ¨æ­¤ä»»åŠ¡ä¸­çš„åè§å°¤ä¸ºé‡è¦ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶åœ¨ç°æœ‰ç«‹åœºæ£€æµ‹æ•°æ®é›†ä¸Šè‡ªåŠ¨æ ‡æ³¨äº†ä¸¤ä¸ªå±æ€§ï¼šç‰¹å®šç¾¤ä½“çš„æ–¹è¨€æˆ–è¯­è¨€å˜ä½“ï¼Œä»¥åŠæ–‡æœ¬å¤æ‚åº¦/å¯è¯»æ€§ï¼Œä»¥æ¢ç©¶è¿™äº›å±æ€§æ˜¯å¦å½±å“æ¨¡å‹çš„ç«‹åœºæ£€æµ‹å†³ç­–ã€‚ç ”ç©¶é‡‡ç”¨é›¶æ ·æœ¬è®¾ç½®è¯„ä¼°LLMsåœ¨ç«‹åœºæ£€æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨ç«‹åœºæ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„åˆ»æ¿å°è±¡ï¼Œä¾‹å¦‚é”™è¯¯åœ°å°†æ”¯æŒå¤§éº»çš„è§‚ç‚¹ä¸ä½æ–‡æœ¬å¤æ‚åº¦ç›¸å…³è”ï¼Œå¹¶å°†éè£”ç¾å›½äººæ–¹è¨€ä¸åå¯¹å”çº³å¾·Â·ç‰¹æœ—æ™®çš„ç«‹åœºé”™è¯¯åœ°è”ç³»èµ·æ¥ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨æ•æ„ŸNLPä»»åŠ¡ä¸­ç³»ç»Ÿè¯„ä¼°å’Œç¼“è§£LLMsåè§çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯ç«‹åœºæ£€æµ‹è¿™ç±»æ¶‰åŠæ”¿æ²»åˆ¤æ–­çš„ä»»åŠ¡ã€‚ç ”ç©¶ç»“æœä¸ºå¼€å‘æ›´å…¬å¹³çš„ç«‹åœºæ£€æµ‹æ¨¡å‹æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶å‘¼åç¤¾åŒºå…³æ³¨LLMsåœ¨ç°å®åº”ç”¨ä¸­çš„åè§é—®é¢˜ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Large Language Models inherit stereotypes from their pretraining data,
leading to biased behavior toward certain social groups in many Natural
Language Processing tasks, such as hateful speech detection or sentiment
analysis. Surprisingly, the evaluation of this kind of bias in stance detection
methods has been largely overlooked by the community. Stance Detection involves
labeling a statement as being against, in favor, or neutral towards a specific
target and is among the most sensitive NLP tasks, as it often relates to
political leanings. In this paper, we focus on the bias of Large Language
Models when performing stance detection in a zero-shot setting. We
automatically annotate posts in pre-existing stance detection datasets with two
attributes: dialect or vernacular of a specific group and text
complexity/readability, to investigate whether these attributes influence the
model's stance detection decisions. Our results show that LLMs exhibit
significant stereotypes in stance detection tasks, such as incorrectly
associating pro-marijuana views with low text complexity and African American
dialect with opposition to Donald Trump.</p>
<h3 id="40-vlsp-2025-mlqa-tsr-challenge-vietnamese-multimodal-legal-question-answering-on-traffic-sign-regulation">[40] <a href="https://arxiv.org/abs/2510.20381">VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation</a></h3>
<p><em>Son T. Luu, Trung Vo, Hiep Nguyen, Khanh Quoc Tran, Kiet Van Nguyen, Vu Tran, Ngan Luu-Thuy Nguyen, Le-Minh Nguyen</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ä»‹ç»äº†VLSP 2025 MLQA-TSRå¤šæ¨¡æ€äº¤é€šæ ‡å¿—æ³•è§„é—®ç­”å…±äº«ä»»åŠ¡ï¼ŒåŒ…å«å¤šæ¨¡æ€æ³•å¾‹æ£€ç´¢å’Œå¤šæ¨¡æ€é—®ç­”ä¸¤ä¸ªå­ä»»åŠ¡ï¼Œæ—¨åœ¨æ¨è¿›è¶Šå—å¤šæ¨¡æ€æ³•å¾‹æ–‡æœ¬å¤„ç†ç ”ç©¶å¹¶å»ºç«‹åŸºå‡†æ•°æ®é›†ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è¶Šå—å¤šæ¨¡æ€æ³•å¾‹æ–‡æœ¬å¤„ç†é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯äº¤é€šæ ‡å¿—æ³•è§„æ–¹é¢çš„æ™ºèƒ½ç³»ç»Ÿå¼€å‘éœ€æ±‚ï¼Œé€šè¿‡å»ºç«‹åŸºå‡†æ•°æ®é›†æ¥ä¿ƒè¿›å¤šæ¨¡æ€æ³•å¾‹é¢†åŸŸæ™ºèƒ½ç³»ç»Ÿçš„æ„å»ºä¸è¯„ä¼°ã€‚</p>
<p><strong>Method:</strong> è¯¥ä»»åŠ¡é‡‡ç”¨å¤šæ¨¡æ€æ³•å¾‹æ£€ç´¢å’Œå¤šæ¨¡æ€é—®ç­”ä¸¤ä¸ªå­ä»»åŠ¡çš„æ¡†æ¶è®¾è®¡ï¼Œç»“åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯å¤„ç†äº¤é€šæ ‡å¿—æ³•è§„ç›¸å…³çš„é—®é¢˜ï¼Œä¸ºå‚ä¸è€…æä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°å¹³å°å’Œæ–¹æ³•è®ºæŒ‡å¯¼ã€‚</p>
<p><strong>Result:</strong> åœ¨VLSP 2025 MLQA-TSRä»»åŠ¡ä¸­ï¼Œå¤šæ¨¡æ€æ³•å¾‹æ£€ç´¢çš„æœ€ä½³F2åˆ†æ•°è¾¾åˆ°64.55%ï¼Œå¤šæ¨¡æ€é—®ç­”çš„å‡†ç¡®ç‡è¾¾åˆ°86.30%ï¼Œä¸ºç›¸å…³ç ”ç©¶è®¾å®šäº†æ€§èƒ½åŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè¶Šå—å¤šæ¨¡æ€æ³•å¾‹æ–‡æœ¬å¤„ç†å»ºç«‹äº†é‡è¦çš„åŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°æ ‡å‡†ï¼Œç‰¹åˆ«åœ¨äº¤é€šæ ‡å¿—æ³•è§„é¢†åŸŸæ¨åŠ¨äº†æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æ³•å¾‹AIåº”ç”¨æä¾›äº†åŸºç¡€æ”¯æ’‘ã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question
answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025
MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal
question answering. The goal is to advance research on Vietnamese multimodal
legal text processing and to provide a benchmark dataset for building and
evaluating intelligent systems in multimodal legal domains, with a focus on
traffic sign regulation in Vietnam. The best-reported results on VLSP 2025
MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an
accuracy of 86.30% for multimodal question answering.</p>
<h3 id="41-automated-extraction-of-fluoropyrimidine-treatment-and-treatment-related-toxicities-from-clinical-notes-using-natural-language-processing">[41] <a href="https://arxiv.org/abs/2510.20727">Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing</a></h3>
<p><em>Xizhi Wu, Madeline S. Kreider, Philip E. Empey, Chenyu Li, Yanshan Wang</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶å¼€å‘å¹¶è¯„ä¼°äº†å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•ï¼Œç”¨äºä»ä¸´åºŠç¬”è®°ä¸­æå–æ°Ÿå˜§å•¶ç±»è¯ç‰©æ²»ç–—å’Œæ¯’æ€§ä¿¡æ¯ã€‚åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é”™è¯¯åˆ†ææç¤ºæ–¹æ³•åœ¨æå–ç²¾åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢è¡¨ç°æœ€ä¼˜ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ°Ÿå˜§å•¶ç±»è¯ç‰©å¹¿æ³›ç”¨äºç»“ç›´è‚ ç™Œå’Œä¹³è…ºç™Œæ²»ç–—ï¼Œä½†å¸¸ä¼´éšæ‰‹è¶³ç»¼åˆå¾å’Œå¿ƒè„æ¯’æ€§ç­‰ä¸è‰¯ååº”ã€‚ç”±äºæ¯’æ€§è®°å½•é€šå¸¸åµŒå…¥åœ¨ä¸´åºŠç¬”è®°ä¸­ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘æœ‰æ•ˆçš„è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•æ¥è‡ªåŠ¨æå–æ²»ç–—å’Œæ¯’æ€§ä¿¡æ¯ï¼Œä»¥æ”¯æŒè‚¿ç˜¤å­¦ç ”ç©¶å’Œè¯ç‰©è­¦æˆ’å·¥ä½œã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†åŒ…å«236ä»½ä¸´åºŠç¬”è®°çš„é‡‘æ ‡å‡†æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†åŸºäºè§„åˆ™ã€æœºå™¨å­¦ä¹ ï¼ˆéšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºã€é€»è¾‘å›å½’ï¼‰ã€æ·±åº¦å­¦ä¹ ï¼ˆBERTã€ClinicalBERTï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆé›¶æ ·æœ¬æç¤ºå’Œé”™è¯¯åˆ†ææç¤ºï¼‰çš„å¤šç±»è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•ã€‚æ‰€æœ‰æ¨¡å‹é‡‡ç”¨80:20çš„è®­ç»ƒ-æµ‹è¯•åˆ†å‰²ç­–ç•¥è¿›è¡Œè¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> é”™è¯¯åˆ†ææç¤ºæ–¹æ³•åœ¨æ²»ç–—å’Œæ¯’æ€§æå–æ–¹é¢è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ï¼ˆF1=1.000ï¼‰ï¼Œé›¶æ ·æœ¬æç¤ºåœ¨æ²»ç–—æå–ä¸ŠåŒæ ·è¾¾åˆ°F1=1.000ï¼Œæ¯’æ€§æå–ä¸ºF1=0.876ã€‚é€»è¾‘å›å½’å’Œæ”¯æŒå‘é‡æœºåœ¨æ¯’æ€§æå–ä¸­æ’åç¬¬äºŒï¼ˆF1=0.937ï¼‰ï¼Œè€Œæ·±åº¦å­¦ä¹ æ–¹æ³•è¡¨ç°è¾ƒå·®ï¼ŒBERTå’ŒClinicalBERTçš„F1åˆ†æ•°åˆ†åˆ«ä¸º0.873/0.839å’Œ0.873/0.886ã€‚åŸºäºè§„åˆ™çš„æ–¹æ³•ä½œä¸ºåŸºçº¿ï¼ŒF1åˆ†æ•°ä¸º0.857å’Œ0.858ã€‚</p>
<p><strong>Conclusion:</strong> åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•åœ¨æ‰€æœ‰è¯„ä¼°æ–¹æ³•ä¸­è¡¨ç°æœ€ä¼˜ï¼Œå…¶æ¬¡ä¸ºä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ã€‚æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•å—é™äºå°è§„æ¨¡è®­ç»ƒæ•°æ®ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç½•è§ç±»åˆ«æ—¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯èƒ½å¤Ÿæœ‰æ•ˆä»ä¸´åºŠç¬”è®°ä¸­æå–æ°Ÿå˜§å•¶æ²»ç–—å’Œæ¯’æ€§ä¿¡æ¯ï¼Œå…·æœ‰æ”¯æŒè‚¿ç˜¤å­¦ç ”ç©¶å’Œè¯ç‰©è­¦æˆ’çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>Objective: Fluoropyrimidines are widely prescribed for colorectal and breast
cancers, but are associated with toxicities such as hand-foot syndrome and
cardiotoxicity. Since toxicity documentation is often embedded in clinical
notes, we aimed to develop and evaluate natural language processing (NLP)
methods to extract treatment and toxicity information.
  Materials and Methods: We constructed a gold-standard dataset of 236 clinical
notes from 204,165 adult oncology patients. Domain experts annotated categories
related to treatment regimens and toxicities. We developed rule-based, machine
learning-based (Random Forest, Support Vector Machine [SVM], Logistic
Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language
models (LLM)-based NLP approaches (zero-shot and error-analysis prompting).
Models used an 80:20 train-test split.
  Results: Sufficient data existed to train and evaluate 5 annotated
categories. Error-analysis prompting achieved optimal precision, recall, and F1
scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot
prompting reached F1=1.000 for treatment and F1=0.876 for toxicities
extraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning
underperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and
ClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods
served as our baseline with F1 scores of 0.857 in treatment and 0.858 in
toxicities.
  Discussion: LMM-based approaches outperformed all others, followed by machine
learning methods. Machine and deep learning approaches were limited by small
training data and showed limited generalizability, particularly for rare
categories.
  Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine
treatment and toxicity information from clinical notes, and has strong
potential to support oncology research and pharmacovigilance.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="42-relate-a-schema-agnostic-perceiver-encoder-for-multimodal-relational-graphs">[42] <a href="https://arxiv.org/abs/2510.19954">RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs</a></h3>
<p><em>Joseph Meyer, Divyansha Lachi, Reza Mohammadi, Roshan Reddy Upendra, Eva L. Dyer, Mark Li, Tom Palczewski</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†RELATEï¼Œä¸€ç§æ¨¡å¼æ— å…³çš„å›¾ç¥ç»ç½‘ç»œç‰¹å¾ç¼–ç å™¨ï¼Œé€šè¿‡å…±äº«æ¨¡æ€ç‰¹å®šç¼–ç å™¨å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶å®ç°å¼‚æ„æ—¶åºå›¾çš„ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—å‡å°‘å‚æ•°æ•°é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å›¾ç¥ç»ç½‘ç»œåœ¨å¤„ç†å…³ç³»å‹å¤šè¡¨æ•°æ®æ—¶ä¾èµ–æ¨¡å¼ç‰¹å®šçš„ç‰¹å¾ç¼–ç å™¨ï¼Œéœ€è¦ä¸ºæ¯ç§èŠ‚ç‚¹ç±»å‹å’Œç‰¹å¾åˆ—è®¾è®¡ç‹¬ç«‹æ¨¡å—ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œå‚æ•°å…±äº«èƒ½åŠ›ï¼Œé˜»ç¢äº†é€šç”¨å›¾ç¥ç»ç½‘ç»œçš„å‘å±•ã€‚</p>
<p><strong>Method:</strong> RELATEé‡‡ç”¨å…±äº«çš„æ¨¡æ€ç‰¹å®šç¼–ç å™¨å¤„ç†åˆ†ç±»ã€æ•°å€¼ã€æ–‡æœ¬å’Œæ—¶é—´å±æ€§ï¼Œç„¶åé€šè¿‡Perceiveré£æ ¼çš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—å°†ç‰¹å¾èšåˆä¸ºå›ºå®šå¤§å°çš„ç½®æ¢ä¸å˜èŠ‚ç‚¹è¡¨ç¤ºï¼Œå¯ä¸ä»»ä½•é€šç”¨å›¾ç¥ç»ç½‘ç»œé…åˆä½¿ç”¨ã€‚</p>
<p><strong>Result:</strong> åœ¨RelBenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRELATEä¸ReLGNNå’ŒHGTç»“åˆä½¿ç”¨æ—¶ï¼Œæ€§èƒ½è¾¾åˆ°æ¨¡å¼ç‰¹å®šç¼–ç å™¨çš„97%ä»¥ä¸Šï¼ŒåŒæ—¶å°†å‚æ•°æ•°é‡å‡å°‘é«˜è¾¾5å€ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> RELATEçš„è®¾è®¡æ”¯æŒä¸åŒæ¨¡å¼çš„æ•°æ®å¤„ç†ï¼Œå¹¶ä¸ºå…³ç³»å›¾æ•°æ®çš„å¤šæ•°æ®é›†é¢„è®­ç»ƒæä¾›äº†å¯èƒ½ï¼Œä¸ºå®ç°å…³ç³»å›¾æ•°æ®çš„åŸºç¡€æ¨¡å‹é“ºå¹³äº†é“è·¯ï¼Œæ¨åŠ¨äº†é€šç”¨å›¾ç¥ç»ç½‘ç»œçš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>Relational multi-table data is common in domains such as e-commerce,
healthcare, and scientific research, and can be naturally represented as
heterogeneous temporal graphs with multi-modal node attributes. Existing graph
neural networks (GNNs) rely on schema-specific feature encoders, requiring
separate modules for each node type and feature column, which hinders
scalability and parameter sharing. We introduce RELATE (Relational Encoder for
Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature
encoder that can be used with any general purpose GNN. RELATE employs shared
modality-specific encoders for categorical, numerical, textual, and temporal
attributes, followed by a Perceiver-style cross-attention module that
aggregates features into a fixed-size, permutation-invariant node
representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,
where it achieves performance within 3% of schema-specific encoders while
reducing parameter counts by up to 5x. This design supports varying schemas and
enables multi-dataset pretraining for general-purpose GNNs, paving the way
toward foundation models for relational graph data.</p>
<h3 id="43-multi-step-reasoning-for-embodied-question-answering-via-tool-augmentation">[43] <a href="https://arxiv.org/abs/2510.20310">Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation</a></h3>
<p><em>Mingliang Zhai, Hansheng Liang, Xiaomeng Fan, Zhi Gao, Chuanhao Li, Che Sun, Xu Bin, Yuwei Wu, Yunde Jia</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ToolEQAï¼Œä¸€ç§å°†å¤–éƒ¨å·¥å…·ä¸å¤šæ­¥æ¨ç†ç›¸ç»“åˆçš„å…·èº«é—®ç­”æ™ºèƒ½ä½“ï¼Œé€šè¿‡å·¥å…·ä½¿ç”¨è·å–é¢å¤–æœ‰æ•ˆä¿¡æ¯æ¥æ”¹å–„æ¢ç´¢æ–¹å‘ï¼Œä»è€Œåœ¨æ›´çŸ­æ¢ç´¢è·ç¦»å†…ç”Ÿæˆæ›´å‡†ç¡®å›ç­”ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒæˆåŠŸç‡æ¯”ç°æœ‰æ–¹æ³•æå‡9.2-20.2%ã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å…·èº«é—®ç­”æ–¹æ³•ç›´æ¥åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ¢ç´¢ç¯å¢ƒå¹¶å›ç­”é—®é¢˜ï¼Œç¼ºä¹æ˜¾å¼æ€è€ƒå’Œè§„åˆ’ï¼Œè¿™é™åˆ¶äº†æ¨ç†èƒ½åŠ›å¹¶å¯¼è‡´è¿‡åº¦æˆ–ä½æ•ˆæ¢ç´¢ä»¥åŠæ— æ•ˆå“åº”ã€‚</p>
<p><strong>Method:</strong> ToolEQAé€šè¿‡é›†æˆå¤–éƒ¨å·¥å…·ä¸å¤šæ­¥æ¨ç†ï¼Œä½¿å¤–éƒ¨å·¥å…·èƒ½å¤Ÿä¸ºä»»åŠ¡å®Œæˆæä¾›æ›´æœ‰ç”¨ä¿¡æ¯ï¼Œå¸®åŠ©æ¨¡å‹åœ¨ä¸‹ä¸€æ­¥æ¨ç†ä¸­æ¨å¯¼æ›´å¥½çš„æ¢ç´¢æ–¹å‘ï¼Œä»è€Œè·å¾—é¢å¤–æœ‰æ•ˆä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§æ–°é¢–çš„EQAæ•°æ®ç”Ÿæˆæµç¨‹ï¼Œè‡ªåŠ¨æ„å»ºå…·æœ‰æ¨ç†è½¨è¿¹å’Œç›¸åº”ç­”æ¡ˆçš„å¤§è§„æ¨¡EQAä»»åŠ¡ï¼Œå¹¶åŸºäºæ­¤æ”¶é›†äº†åŒ…å«çº¦18Kä»»åŠ¡çš„EQA-RTæ•°æ®é›†ã€‚</p>
<p><strong>Result:</strong> åœ¨EQA-RT-Seenå’ŒEQA-RT-Unseenä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒToolEQAç›¸æ¯”æœ€å…ˆè¿›åŸºçº¿æ–¹æ³•æˆåŠŸç‡æå‡9.2-20.2%ï¼ŒåŒæ—¶æ¯”é›¶æ ·æœ¬ToolEQAæˆåŠŸç‡é«˜å‡º10%ã€‚æ­¤å¤–ï¼ŒToolEQAåœ¨HM-EQAã€OpenEQAå’ŒEXPRESS-Benchæ•°æ®é›†ä¸Šä¹Ÿå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ToolEQAé€šè¿‡å·¥å…·ä½¿ç”¨å’Œå¤šæ­¥æ¨ç†æ˜¾è‘—æå‡äº†å…·èº«é—®ç­”çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œè¯æ˜äº†å¤–éƒ¨å·¥å…·é›†æˆåœ¨å¢å¼ºæ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•ä¸ºå…·èº«æ™ºèƒ½ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ï¼Œå±•ç¤ºäº†ç»“æ„åŒ–æ¨ç†è½¨è¿¹åœ¨å¤æ‚ç¯å¢ƒäº¤äº’ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.</p>
<h3 id="44-llm-empowered-knowledge-graph-construction-a-survey">[44] <a href="https://arxiv.org/abs/2510.20345">LLM-empowered knowledge graph construction: A survey</a></h3>
<p><em>Haonan Bian</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬è°ƒæŸ¥ç³»ç»Ÿç»¼è¿°äº†å¤§å‹è¯­è¨€æ¨¡å‹èµ‹èƒ½çŸ¥è¯†å›¾è°±æ„å»ºçš„æœ€æ–°è¿›å±•ï¼Œåˆ†æäº†LLMå¦‚ä½•é‡å¡‘ä¼ ç»Ÿçš„æœ¬ä½“å·¥ç¨‹ã€çŸ¥è¯†æŠ½å–å’ŒçŸ¥è¯†èåˆä¸‰å±‚æµæ°´çº¿ï¼Œä¸ºç¬¦å·åŒ–çŸ¥è¯†å·¥ç¨‹ä¸ç¥ç»è¯­ä¹‰ç†è§£çš„èåˆæä¾›äº†å…¨é¢æ¡†æ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼ŒçŸ¥è¯†å›¾è°±æ„å»ºæ­£ç»å†ä»åŸºäºè§„åˆ™å’Œç»Ÿè®¡çš„æµæ°´çº¿å‘è¯­è¨€é©±åŠ¨å’Œç”Ÿæˆæ¡†æ¶çš„èŒƒå¼è½¬å˜ï¼Œéœ€è¦ç³»ç»Ÿæ¢³ç†LLMå¦‚ä½•é‡å¡‘ä¼ ç»ŸçŸ¥è¯†å›¾è°±æ„å»ºçš„ä¸‰å±‚æµæ°´çº¿ï¼Œå¹¶æ¾„æ¸…LLMä¸çŸ¥è¯†å›¾è°±ä¹‹é—´ä¸æ–­æ¼”å˜çš„ç›¸äº’ä½œç”¨ã€‚</p>
<p><strong>Method:</strong> è°ƒæŸ¥ä»ä¸¤ä¸ªäº’è¡¥è§†è§’å›é¡¾æ–°å…´çš„LLMé©±åŠ¨æ–¹æ³•ï¼šå¼ºè°ƒç»“æ„ã€è§„èŒƒåŒ–å’Œä¸€è‡´æ€§çš„åŸºäºæ¨¡å¼çš„èŒƒå¼ï¼Œä»¥åŠå¼ºè°ƒçµæ´»æ€§ã€é€‚åº”æ€§å’Œå¼€æ”¾å‘ç°çš„å…æ¨¡å¼èŒƒå¼ï¼Œå¹¶åœ¨æ¯ä¸ªé˜¶æ®µç»¼åˆä»£è¡¨æ€§æ¡†æ¶å¹¶åˆ†æå…¶æŠ€æœ¯æœºåˆ¶ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡ç³»ç»Ÿåˆ†æLLMèµ‹èƒ½çŸ¥è¯†å›¾è°±æ„å»ºçš„è¿›å±•ï¼Œè°ƒæŸ¥è¯†åˆ«äº†å„é˜¶æ®µä»£è¡¨æ€§æ¡†æ¶çš„æŠ€æœ¯æœºåˆ¶å’Œå±€é™æ€§ï¼Œä¸ºç†è§£LLMå¦‚ä½•æ”¹å˜ä¼ ç»ŸçŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•è®ºæä¾›äº†å…¨é¢è§†è§’ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥è°ƒæŸ¥ä¸ºå¼€å‘è‡ªé€‚åº”ã€å¯è§£é‡Šå’Œæ™ºèƒ½çŸ¥è¯†ç³»ç»ŸæŒ‡æ˜äº†å…³é”®è¶‹åŠ¿å’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬åŸºäºçŸ¥è¯†å›¾è°±çš„LLMæ¨ç†ã€é¢å‘æ™ºèƒ½ä½“ç³»ç»Ÿçš„åŠ¨æ€çŸ¥è¯†è®°å¿†ä»¥åŠå¤šæ¨¡æ€çŸ¥è¯†å›¾è°±æ„å»ºï¼Œæœ‰æ•ˆæ¡¥æ¥äº†ç¬¦å·åŒ–çŸ¥è¯†å·¥ç¨‹ä¸ç¥ç»è¯­ä¹‰ç†è§£ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.</p>
<h3 id="45-towards-reliable-evaluation-of-large-language-models-for-multilingual-and-multimodal-e-commerce-applications">[45] <a href="https://arxiv.org/abs/2510.20632">Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications</a></h3>
<p><em>Shuyi Xie, Ziqin Liew, Hailing Zhang, Haibo Zhang, Ling Hu, Zhiqiang Zhou, Shuman Liu, Anxiang Zeng</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†EcomEvalï¼Œä¸€ä¸ªå…¨é¢çš„å¤šè¯­è¨€å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”µå­å•†åŠ¡é¢†åŸŸçš„æ€§èƒ½ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨ä»»åŠ¡å¤šæ ·æ€§ã€æ¨¡æ€è¦†ç›–å’Œè¯­è¨€èŒƒå›´æ–¹é¢çš„ä¸è¶³ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç”µå­å•†åŠ¡è¯„ä¼°åŸºå‡†å¦‚EcomInstructã€ChineseEcomQAç­‰å­˜åœ¨ä»»åŠ¡å¤šæ ·æ€§ä¸è¶³ï¼ˆç¼ºå°‘äº§å“æŒ‡å¯¼å’Œå”®åé—®é¢˜ï¼‰ã€æ¨¡æ€è¦†ç›–æœ‰é™ï¼ˆç¼ºä¹å¤šæ¨¡æ€æ•°æ®ï¼‰ã€æ•°æ®åˆæˆæˆ–äººå·¥æ•´ç†ã€ä»¥åŠè¯­è¨€è¦†ç›–ç‹­çª„ï¼ˆä»…é™è‹±è¯­å’Œä¸­æ–‡ï¼‰ç­‰é—®é¢˜ï¼Œå¯¼è‡´ç¼ºä¹å¯é å·¥å…·æ¥è¯„ä¼°æ¨¡å‹åœ¨å¤æ‚çœŸå®è´­ç‰©åœºæ™¯ä¸­çš„è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†æ¶µç›–6ä¸ªç±»åˆ«å’Œ37ä¸ªä»»åŠ¡ï¼ˆåŒ…æ‹¬8ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼‰çš„ç»¼åˆåŸºå‡†ï¼Œä¸»è¦æ¥æºäºçœŸå®å®¢æˆ·æŸ¥è¯¢å’Œäº¤æ˜“æ—¥å¿—ï¼Œé‡‡ç”¨åŠè‡ªåŠ¨æµç¨‹ç”±å¤§æ¨¡å‹ç”Ÿæˆå€™é€‰å›ç­”å¹¶ç”±50å¤šåç”µå­å•†åŠ¡å’Œå¤šè¯­è¨€ä¸“å®¶å®¡æ ¸ä¿®æ”¹ï¼Œä¸ºæ¯ä¸ªé—®é¢˜å’Œä»»åŠ¡ç±»åˆ«å®šä¹‰éš¾åº¦çº§åˆ«ï¼Œå¹¶è¦†ç›–åŒ…æ‹¬5ç§ä¸œå—äºšä½èµ„æºè¯­è¨€åœ¨å†…çš„7ç§è¯­è¨€ã€‚</p>
<p><strong>Result:</strong> EcomEvalåŸºå‡†åæ˜ äº†çœŸå®å•†ä¸šäº¤äº’çš„å™ªå£°å’Œå¼‚æ„ç‰¹æ€§ï¼Œé€šè¿‡åœ¨ä¸åŒè§„æ¨¡å’Œèƒ½åŠ›æ¨¡å‹ä¸Šçš„è¯„ä¼°åˆ†æ•°å¹³å‡æ¥å®šä¹‰éš¾åº¦çº§åˆ«ï¼Œå®ç°äº†é¢å‘æŒ‘æˆ˜çš„ç»†ç²’åº¦è¯„ä¼°ï¼Œä¸ºç”µå­å•†åŠ¡é¢†åŸŸçš„æ¨¡å‹è¯„ä¼°æä¾›äº†å¯é çš„å¤šè¯­è¨€å¤šæ¨¡æ€æµ‹è¯•å¹³å°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºç”µå­å•†åŠ¡é¢†åŸŸçš„LLMè¯„ä¼°æä¾›äº†é¦–ä¸ªå…¨é¢çš„å¤šè¯­è¨€å¤šæ¨¡æ€åŸºå‡†ï¼Œæ­ç¤ºäº†ç°æœ‰åŸºå‡†çš„å±€é™æ€§ï¼Œå¹¶ä¸ºè¯„ä¼°æ¨¡å‹åœ¨çœŸå®å¤æ‚å•†ä¸šåœºæ™¯ä¸­çš„èƒ½åŠ›å»ºç«‹äº†æ–°çš„æ ‡å‡†ï¼Œç‰¹åˆ«å¯¹ä½èµ„æºè¯­è¨€çš„æ”¯æŒå…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.</p>
  </article>
</body>
</html>
