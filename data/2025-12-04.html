<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-04.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 52]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 5]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 4]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-object-counting-with-gpt-4o-and-gpt-5-a-comparative-study">[1] <a href="https://arxiv.org/abs/2512.03233">Object Counting with GPT-4o and GPT-5: A Comparative Study</a></h3>
<p><em>Richard FÃ¼zessÃ©ry, Kaziwa Saleh, SÃ¡ndor SzÃ©nÃ¡si, ZoltÃ¡n VÃ¡mossy</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æ¢ç´¢åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆGPT-4oå’ŒGPT-5ï¼‰è¿›è¡Œé›¶æ ·æœ¬ç‰©ä½“è®¡æ•°ï¼Œä»…é€šè¿‡æ–‡æœ¬æç¤ºå®ç°æ— éœ€ç›‘ç£çš„è®¡æ•°ä»»åŠ¡ï¼Œåœ¨FSC-147æ•°æ®é›†ä¸Šå–å¾—äº†ä¸ç°æœ‰é›¶æ ·æœ¬æ–¹æ³•ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é›¶æ ·æœ¬ç‰©ä½“è®¡æ•°æ—¨åœ¨ä¼°è®¡è§†è§‰æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»æœªè§è¿‡çš„æ–°ç±»åˆ«ç‰©ä½“å®ä¾‹æ•°é‡ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®æˆ–è§†è§‰ç¤ºä¾‹å¼•å¯¼ï¼Œè€Œå¤§è¯­è¨€æ¨¡å‹å…·æœ‰å¼ºå¤§çš„æ¨ç†å’Œæ•°æ®ç†è§£èƒ½åŠ›ï¼Œæœ¬ç ”ç©¶æ¢ç´¢åˆ©ç”¨å¤šæ¨¡æ€LLMså®ç°å®Œå…¨æ— éœ€ç›‘ç£çš„é›¶æ ·æœ¬è®¡æ•°ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶æå‡ºåˆ©ç”¨ä¸¤ç§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆGPT-4oå’ŒGPT-5ï¼‰çš„è§†è§‰èƒ½åŠ›ï¼Œä»…é€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œé›¶æ ·æœ¬ç‰©ä½“è®¡æ•°ï¼Œæ— éœ€ä»»ä½•ç›‘ç£è®­ç»ƒæˆ–è§†è§‰ç¤ºä¾‹ï¼Œå®ç°äº†å®Œå…¨åŸºäºæ–‡æœ¬å¼•å¯¼çš„è®¡æ•°æ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> åœ¨FSC-147å’ŒCARPKæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸¤ç§æ¨¡å‹åœ¨FSC-147æ•°æ®é›†ä¸Šå–å¾—äº†ä¸æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€LLMsåœ¨é›¶æ ·æœ¬è®¡æ•°ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ‰§è¡Œé›¶æ ·æœ¬ç‰©ä½“è®¡æ•°ä»»åŠ¡ï¼Œä»…é€šè¿‡æ–‡æœ¬æç¤ºå³å¯å®ç°ä¸ä¸“é—¨è®¾è®¡æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œä¸ºæ— éœ€ç›‘ç£çš„è§†è§‰è®¡æ•°ä»»åŠ¡å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå±•ç¤ºäº†LLMsåœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Zero-shot object counting attempts to estimate the number of object instances belonging to novel categories that the vision model performing the counting has never encountered during training. Existing methods typically require large amount of annotated data and often require visual exemplars to guide the counting process. However, large language models (LLMs) are powerful tools with remarkable reasoning and data understanding abilities, which suggest the possibility of utilizing them for counting tasks without any supervision. In this work we aim to leverage the visual capabilities of two multi-modal LLMs, GPT-4o and GPT-5, to perform object counting in a zero-shot manner using only textual prompts. We evaluate both models on the FSC-147 and CARPK datasets and provide a comparative analysis. Our findings show that the models achieve performance comparable to the state-of-the-art zero-shot approaches on FSC-147, in some cases, even surpass them.</p>
<h3 id="2-llm-guided-material-inference-for-3d-point-clouds">[2] <a href="https://arxiv.org/abs/2512.03237">LLM-Guided Material Inference for 3D Point Clouds</a></h3>
<p><em>Nafiseh Izadyar, Teseo Schneider</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œç”¨äºä»å¸¦æœ‰ç²—åˆ†å‰²çš„ä¸‰ç»´ç‚¹äº‘ç›´æ¥æ¨æ–­ææ–™ç»„æˆï¼Œé€šè¿‡å°†ç‰©ä½“è¯†åˆ«ä¸ææ–™æ¨ç†è§£è€¦ï¼Œå®ç°äº†é›¶æ ·æœ¬çš„ææ–™ç†è§£ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ä¸‰ç»´å½¢çŠ¶æ•°æ®é›†å’Œæ¨¡å‹ä¸»è¦å…³æ³¨å‡ ä½•ç‰¹å¾ï¼Œå¿½è§†äº†å†³å®šç‰©ä½“å¤–è§‚çš„å…³é”®ææ–™å±æ€§ï¼Œå¯¼è‡´ç¼ºä¹ä»ä¸‰ç»´æ•°æ®ä¸­æ¨æ–­ææ–™ç»„æˆçš„å¯é æ–¹æ³•ï¼Œè¿™é™åˆ¶äº†ä¸‰ç»´å½¢çŠ¶çš„å®Œæ•´ç‰©ç†ç†è§£ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µå¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ï¼šç¬¬ä¸€é˜¶æ®µLLMé¢„æµ‹ç‰©ä½“çš„è¯­ä¹‰ç±»åˆ«ï¼Œç¬¬äºŒé˜¶æ®µLLMæ ¹æ®æ¨æ–­çš„è¯­ä¹‰ä¸ºæ¯ä¸ªå‡ ä½•åˆ†å‰²åˆ†é…åˆç†çš„ææ–™ï¼Œä¸¤ä¸ªé˜¶æ®µå‡ä»¥é›¶æ ·æœ¬æ–¹å¼è¿è¡Œï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚</p>
<p><strong>Result:</strong> åœ¨Fusion/ABSå’ŒShapeNetæ•°æ®é›†çš„1000ä¸ªå½¢çŠ¶ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†è¾ƒé«˜çš„è¯­ä¹‰å’Œææ–™åˆç†æ€§ï¼Œé€šè¿‡DeepEvalå®ç°çš„LLM-as-a-Judgeè¯„ä¼°æ¡†æ¶éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†é›¶æ ·æœ¬ææ–™æ¨ç†çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜è¯­è¨€æ¨¡å‹å¯ä»¥ä½œä¸ºé€šç”¨å…ˆéªŒçŸ¥è¯†ï¼Œæœ‰æ•ˆè¿æ¥ä¸‰ç»´æ•°æ®ä¸­çš„å‡ ä½•æ¨ç†ä¸ææ–™ç†è§£ï¼Œä¸ºé›¶æ ·æœ¬ææ–™æ¨æ–­æä¾›äº†æ–°èŒƒå¼ï¼Œå¹¶ä¸ºä¸‰ç»´å½¢çŠ¶çš„å®Œæ•´ç‰©ç†å±æ€§å»ºæ¨¡å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Most existing 3D shape datasets and models focus solely on geometry, overlooking the material properties that determine how objects appear. We introduce a two-stage large language model (LLM) based method for inferring material composition directly from 3D point clouds with coarse segmentations. Our key insight is to decouple reasoning about what an object is from what it is made of. In the first stage, an LLM predicts the object's semantic; in the second stage, it assigns plausible materials to each geometric segment, conditioned on the inferred semantics. Both stages operate in a zero-shot manner, without task-specific training. Because existing datasets lack reliable material annotations, we evaluate our method using an LLM-as-a-Judge implemented in DeepEval. Across 1,000 shapes from Fusion/ABS and ShapeNet, our method achieves high semantic and material plausibility. These results demonstrate that language models can serve as general-purpose priors for bridging geometric reasoning and material understanding in 3D data.</p>
<h3 id="3-text-printed-image-bridging-the-image-text-modality-gap-for-text-centric-training-of-large-vision-language-models">[3] <a href="https://arxiv.org/abs/2512.03463">Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models</a></h3>
<p><em>Shojiro Yamabe, Futa Waseda, Daiki Shiono, Tsubasa Takahashi</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºæ–‡æœ¬æ‰“å°å›¾åƒï¼ˆTPIï¼‰æ–¹æ³•ï¼Œé€šè¿‡å°†æ–‡æœ¬æè¿°ç›´æ¥æ¸²æŸ“åˆ°ç©ºç™½ç”»å¸ƒä¸Šç”Ÿæˆåˆæˆå›¾åƒï¼Œä»¥è§£å†³å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­å›¾åƒæ•°æ®ç¨€ç¼ºå’Œæ”¶é›†æˆæœ¬é«˜çš„é—®é¢˜ï¼Œå®ç°äº†æ›´æœ‰æ•ˆçš„æ–‡æœ¬ä¸­å¿ƒè®­ç»ƒã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨VQAä»»åŠ¡ä¸­éœ€è¦å¤§é‡å›¾åƒ-æ–‡æœ¬å¯¹è¿›è¡Œä»»åŠ¡ç‰¹å®šå¾®è°ƒï¼Œä½†å›¾åƒæ•°æ®æ”¶é›†å—éšç§é™åˆ¶å’Œé¢†åŸŸç¨€ç¼ºæ€§åˆ¶çº¦ä¸”æˆæœ¬é«˜æ˜‚ï¼Œè€Œæ–‡æœ¬æ•°æ®å¹¿æ³›å¯ç”¨ä¸”æ˜“äºç¼–è¾‘æ‰©å±•ï¼Œç„¶è€Œä»…ä½¿ç”¨åŸå§‹æ–‡æœ¬è®­ç»ƒä¼šå› æ¨¡æ€é¸¿æ²Ÿå¯¼è‡´æ€§èƒ½æå‡æœ‰é™ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºæ–‡æœ¬æ‰“å°å›¾åƒæ–¹æ³•ï¼Œé€šè¿‡å°†ç»™å®šæ–‡æœ¬æè¿°ç›´æ¥æ¸²æŸ“åˆ°çº¯ç™½è‰²ç”»å¸ƒä¸Šç”Ÿæˆåˆæˆå›¾åƒï¼Œè¿™ç§ç®€å•æ¸²æŸ“å°†æ–‡æœ¬æŠ•å½±åˆ°å›¾åƒæ¨¡æ€ï¼Œå¯ä½æˆæœ¬é›†æˆåˆ°ç°æœ‰LVLMè®­ç»ƒæµç¨‹ä¸­ï¼ŒåŒæ—¶ä¿ç•™äº†æ–‡æœ¬è¯­ä¹‰ï¼Œé¿å…äº†æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¸¸å‡ºç°çš„è¯­ä¹‰å¤±çœŸé—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªæ¨¡å‹å’Œä¸ƒä¸ªåŸºå‡†æµ‹è¯•çš„ç³»ç»Ÿå®éªŒä¸­ï¼ŒTPIæ–¹æ³•åœ¨æ–‡æœ¬ä¸­å¿ƒè®­ç»ƒæ–¹é¢æ¯”æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒæ›´æœ‰æ•ˆï¼Œè¿›ä¸€æ­¥æ¢ç´¢äº†TPIä½œä¸ºä½æˆæœ¬æ•°æ®å¢å¼ºç­–ç•¥çš„å®é™…æ•ˆç”¨ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šç§VQAä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜æ–‡æœ¬ä¸­å¿ƒè®­ç»ƒå…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼ŒTPIæ–¹æ³•ä¸ºå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹çš„å…¨è‡ªåŠ¨æ•°æ®ç”Ÿæˆå¼€è¾Ÿäº†æ–°è·¯å¾„ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ã€ä½æˆæœ¬çš„è®­ç»ƒèŒƒå¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¹¿æ³›å¯ç”¨çš„æ–‡æœ¬èµ„æºæ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.</p>
<h3 id="4-spatialreasoner-active-perception-for-large-scale-3d-scene-understanding">[4] <a href="https://arxiv.org/abs/2512.03284">SpatialReasoner: Active Perception for Large-Scale 3D Scene Understanding</a></h3>
<p><em>Hongpei Zheng, Shijie Li, Yanran Li, Hujun Yin</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†HÂ²U3Dæˆ¿å±‹å°ºåº¦3Dè§†è§‰é—®ç­”æ•°æ®é›†å’ŒSpatialReasonerä¸»åŠ¨æ„ŸçŸ¥æ¡†æ¶ï¼Œé€šè¿‡ç²—åˆ°ç»†å±‚æ¬¡åŒ–è¡¨ç¤ºå’Œè‡ªé€‚åº”æ¢ç´¢å¥–åŠ±æœºåˆ¶ï¼Œåœ¨å¤§è§„æ¨¡3Dç¯å¢ƒç©ºé—´æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„å›¾åƒæ•°é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤§è§„æ¨¡3Dç¯å¢ƒä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›å­˜åœ¨å±€é™ï¼Œä¸»è¦å±€é™äºæˆ¿é—´å°ºåº¦åœºæ™¯ï¼Œç¼ºä¹é’ˆå¯¹æˆ¿å±‹å°ºåº¦å¤šæ¥¼å±‚å¤æ‚ç¯å¢ƒçš„ç³»ç»Ÿè¯„ä¼°åŸºå‡†å’Œæœ‰æ•ˆæ¢ç´¢æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†HÂ²U3Dæ•°æ®é›†ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ ‡æ³¨æµç¨‹æ„å»ºä»ç²—åˆ°ç»†çš„å±‚æ¬¡åŒ–è§†è§‰è¡¨ç¤ºå¹¶ç”Ÿæˆå¤šæ ·åŒ–é—®ç­”å¯¹ï¼›åŒæ—¶å¼€å‘äº†SpatialReasonerä¸»åŠ¨æ„ŸçŸ¥æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºæ–‡æœ¬æŸ¥è¯¢è‡ªä¸»è°ƒç”¨ç©ºé—´å·¥å…·æ¢ç´¢3Dåœºæ™¯ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šç›‘ç£å¼å†·å¯åŠ¨åæ¥å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”æ¢ç´¢å¥–åŠ±æœºåˆ¶ä»¥ä¿ƒè¿›é«˜æ•ˆæ¢ç´¢åŒæ—¶å‡å°‘å†—ä½™æ“ä½œã€‚</p>
<p><strong>Result:</strong> SpatialReasoneråœ¨HÂ²U3Dæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†GPT-4oå’ŒGemini-2.5-Proç­‰å¼ºåŸºçº¿æ¨¡å‹ï¼›è¯¥æ–¹æ³•å¹³å‡ä»…éœ€3-4å¼ å›¾åƒå³å¯è¾¾åˆ°ä¼˜å¼‚æ•ˆæœï¼Œè€ŒåŸºçº¿æ–¹æ³•éœ€è¦16å¼ ä»¥ä¸Šå›¾åƒï¼Œè¯æ˜äº†ç²—åˆ°ç»†ä¸»åŠ¨æ¢ç´¢èŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡æ„å»ºæˆ¿å±‹å°ºåº¦3Dè§†è§‰é—®ç­”æ•°æ®é›†å’Œä¸»åŠ¨æ„ŸçŸ¥æ¡†æ¶ï¼Œä¸ºå¤§è§„æ¨¡3Dç¯å¢ƒç©ºé—´æ¨ç†æä¾›äº†ç³»ç»Ÿè§£å†³æ–¹æ¡ˆï¼›è‡ªé€‚åº”æ¢ç´¢å¥–åŠ±æœºåˆ¶å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ˜¾è‘—æå‡äº†æ¢ç´¢æ•ˆç‡ï¼Œä¸ºæœªæ¥3Dåœºæ™¯ç†è§£ç ”ç©¶æä¾›äº†æ–°çš„åŸºå‡†å’Œæ–¹æ³•è®ºæŒ‡å¯¼ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Spatial reasoning in large-scale 3D environments remains challenging for current vision-language models, which are typically constrained to room-scale scenarios. We introduce H$^2$U3D (Holistic House Understanding in 3D), a 3D visual question answering dataset designed for house-scale scene understanding. H$^2$U3D features multi-floor environments spanning up to three floors and 10-20 rooms, covering more than 300 m$^2$. Through an automated annotation pipeline, it constructs hierarchical coarse-to-fine visual representations and generates diverse question-answer pairs with chain-of-thought annotations. We further propose SpatialReasoner, an active perception framework that autonomously invokes spatial tools to explore 3D scenes based on textual queries. SpatialReasoner is trained through a two-stage strategy: a supervised cold start followed by reinforcement learning with an adaptive exploration reward that promotes efficient exploration while discouraging redundant operations. Extensive experiments demonstrate that SpatialReasoner achieves state-of-the-art performance on H$^2$U3D, outperforming strong baselines including GPT-4o and Gemini-2.5-Pro. Notably, our method attains superior results while using only 3-4 images in total on average, compared to baselines requiring 16+ images, highlighting the effectiveness of our coarse-to-fine active exploration paradigm.</p>
<h3 id="5-cartomapqa-a-fundamental-benchmark-dataset-evaluating-vision-language-models-on-cartographic-map-understanding">[5] <a href="https://arxiv.org/abs/2512.03558">CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding</a></h3>
<p><em>Huy Quang Ung, Guillaume Habault, Yasutaka Nishimura, Hao Niu, Roberto Legaspi, Tomoki Oya, Ryoichi Kojima, Masato Taya, Chihiro Ono, Atsunori Minamikawa, Yan Liu</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CartoMapQAåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§£è¯»åˆ¶å›¾åœ°å›¾æ–¹é¢çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨åœ°å›¾è¯­ä¹‰ç†è§£å’Œåœ°ç†ç©ºé—´æ¨ç†æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰-æ–‡æœ¬èåˆæ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶åœ¨åˆ¶å›¾åœ°å›¾ç†è§£æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰æ¨¡å‹ç¼ºä¹ä¸“é—¨é’ˆå¯¹åœ°å›¾è§£è¯»ä»»åŠ¡çš„è¯„ä¼°åŸºå‡†ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨å¯¼èˆªã€åœ°ç†æœç´¢å’ŒåŸå¸‚è§„åˆ’ç­‰å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†CartoMapQAåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡2000ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ç”±åˆ¶å›¾åœ°å›¾ã€é—®é¢˜ï¼ˆå¼€æ”¾å¼æˆ–å¤šé€‰é¢˜ï¼‰å’ŒçœŸå®ç­”æ¡ˆç»„æˆã€‚è¯¥åŸºå‡†æ¶µç›–äº†ä»ä½å±‚åˆ°é«˜å±‚çš„å¤šç§åœ°å›¾è§£è¯»æŠ€èƒ½ï¼ŒåŒ…æ‹¬ç¬¦å·è¯†åˆ«ã€åµŒå…¥å¼ä¿¡æ¯æå–ã€æ¯”ä¾‹å°ºè§£è¯»å’ŒåŸºäºè·¯å¾„çš„æ¨ç†ã€‚</p>
<p><strong>Result:</strong> å¯¹å¼€æºå’Œä¸“æœ‰è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨åœ°å›¾ç‰¹å®šè¯­ä¹‰ç†è§£æ–¹é¢å­˜åœ¨æŒç»­æŒ‘æˆ˜ï¼Œåœ°ç†ç©ºé—´æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œå¹¶ä¸”å®¹æ˜“å—åˆ°å…‰å­¦å­—ç¬¦è¯†åˆ«ç›¸å…³é”™è¯¯çš„å½±å“ã€‚è¿™äº›å¼±ç‚¹åœ¨ç¬¦å·è¯†åˆ«ã€æ¯”ä¾‹å°ºè§£è¯»å’Œå¤æ‚è·¯å¾„æ¨ç†ä»»åŠ¡ä¸­å°¤ä¸ºæ˜æ˜¾ã€‚</p>
<p><strong>Conclusion:</strong> CartoMapQAåŸºå‡†é€šè¿‡ç³»ç»Ÿæ€§åœ°è¯†åˆ«è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åœ°å›¾ç†è§£æ–¹é¢çš„å¼±ç‚¹ï¼Œä¸ºæœªæ¥æ¨¡å‹æ¶æ„æ”¹è¿›æä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼å·¥å…·ã€‚è¯¥ç ”ç©¶æ”¯æŒå¼€å‘æ›´é€‚ç”¨äºä¾èµ–å¯é åœ°å›¾ç†è§£çš„å®é™…åº”ç”¨çš„æ¨¡å‹ï¼Œå¹¶å…¬å¼€äº†æºä»£ç å’Œæ•°æ®é›†ä»¥ä¿ƒè¿›ç ”ç©¶ç¤¾åŒºçš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git</p>
<h3 id="6-step-by-step-layered-design-generation">[6] <a href="https://arxiv.org/abs/2512.03335">Step-by-step Layered Design Generation</a></h3>
<p><em>Faizan Farooq Khan, K J Joseph, Koustava Goswami, Mohamed Elhoseiny, Balaji Vasan Srinivasan</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é€æ­¥åˆ†å±‚è®¾è®¡ç”Ÿæˆé—®é¢˜è®¾å®šï¼Œå¹¶å¼€å‘äº†SLEDGEæ¨¡å‹æ¥æ¨¡æ‹Ÿè®¾è®¡å¸ˆé€æ­¥ä¿®æ”¹è®¾è®¡çš„è¿‡ç¨‹ï¼Œé€šè¿‡å°†æ¯æ¬¡æ›´æ–°å»ºæ¨¡ä¸ºåŸå­åŒ–çš„åˆ†å±‚å˜åŒ–æ¥å®ç°æŒ‡ä»¤é©±åŠ¨çš„è®¾è®¡ç”Ÿæˆã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–¹æ³•ä¸»è¦å°†è®¾è®¡åˆæˆè§†ä¸ºå•æ­¥ç”Ÿæˆé—®é¢˜ï¼Œä¸¥é‡ä½ä¼°äº†åˆ›é€ æ€§è¿‡ç¨‹çš„å›ºæœ‰å¤æ‚æ€§ï¼Œæ— æ³•æ•æ‰è®¾è®¡å¸ˆé€æ­¥ç»†åŒ–å’Œå¢å¼ºå·¥ä½œçš„æœ¬è´¨ç‰¹å¾ï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½å¤Ÿæ¨¡æ‹Ÿé€æ­¥è®¾è®¡è¿‡ç¨‹çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†é€æ­¥åˆ†å±‚è®¾è®¡ç”Ÿæˆçš„æ–°é—®é¢˜è®¾å®šï¼Œå¹¶å¼€å‘äº†SLEDGEæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å°†æ¯æ¬¡è®¾è®¡æ›´æ–°å»ºæ¨¡ä¸ºåŸºäºå…ˆå‰çŠ¶æ€çš„åŸå­åŒ–åˆ†å±‚å˜åŒ–ï¼ŒåŒæ—¶ç¡®ä¿ä¸è®¾è®¡æŒ‡ä»¤çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶æ„å»ºäº†åŒ…å«æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•çš„å®Œæ•´è¯„ä¼°å¥—ä»¶ï¼Œé€šè¿‡è¯¦å°½çš„å®éªŒåˆ†æè¡¨æ˜ï¼Œä¸é’ˆå¯¹æ–°è®¾å®šå®šåˆ¶çš„å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨é€æ­¥è®¾è®¡ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†é€æ­¥è®¾è®¡ç”Ÿæˆçš„é‡è¦æ€§ï¼Œä¸ºå®é™…åº”ç”¨åœºæ™¯æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œæœ‰æœ›å¸å¼•æ›´å¤šç ”ç©¶å…³æ³¨è¿™ä¸€è¢«ä½ä¼°ä½†å…·æœ‰é‡è¦å®è·µä»·å€¼çš„ç ”ç©¶é¢†åŸŸï¼Œæ¨åŠ¨è®¾è®¡ç”Ÿæˆæ–¹æ³•å‘æ›´ç¬¦åˆäººç±»åˆ›ä½œè¿‡ç¨‹çš„æ–¹å‘å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.</p>
<h3 id="7-thinking-with-programming-vision-towards-a-unified-view-for-thinking-with-images">[7] <a href="https://arxiv.org/abs/2512.03746">Thinking with Programming Vision: Towards a Unified View for Thinking with Images</a></h3>
<p><em>Zirun Guo, Minjie Hong, Feng Zhang, Kai Jia, Tao Jin</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCodeVisionï¼Œä¸€ç§åŸºäºä»£ç å³å·¥å…·çš„å¯æ‰©å±•æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆä»£ç ä½œä¸ºé€šç”¨æ¥å£æ¥è°ƒç”¨ä»»æ„å›¾åƒæ“ä½œï¼Œä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å·¥å…·æ¨ç†ä¸­çš„è„†å¼±æ€§å’Œå¯æ‰©å±•æ€§é™åˆ¶é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å·¥å…·æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå³ä½¿æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨é¢å¯¹ç®€å•æ–¹å‘å˜åŒ–æˆ–è‡ªç„¶æŸåçš„å›¾åƒæ—¶ä¹Ÿè¡¨ç°å‡ºæƒŠäººçš„è„†å¼±æ€§ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ŒåŒæ—¶ç°æœ‰æ–¹æ³•ä¾èµ–æœ‰é™çš„å·¥å…·é›†ï¼Œç¼ºä¹ç°å®å¿…è¦æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºCodeVisionæ¡†æ¶ï¼Œå°†ä»£ç ä½œä¸ºé€šç”¨æ¥å£æ¥è°ƒç”¨ä»»æ„å›¾åƒæ“ä½œï¼Œè¶…è¶Šå›ºå®šçš„å·¥å…·æ³¨å†Œè¡¨ï¼›é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé¦–å…ˆåœ¨é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œè¯¥æ•°æ®é›†ä¸“é—¨é’ˆå¯¹å¤æ‚å¤šè½®å·¥å…·ç»„åˆå’Œé”™è¯¯æ¢å¤è€Œæ„å»ºï¼Œéšåä½¿ç”¨å…·æœ‰æ–°é¢–å¯†é›†è¿‡ç¨‹å¥–åŠ±å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ æ¥é¼“åŠ±æˆ˜ç•¥æ€§å’Œé«˜æ•ˆçš„å·¥å…·ä½¿ç”¨ã€‚</p>
<p><strong>Result:</strong> åœ¨Qwen2.5-VLå’ŒQwen3-VLç³»åˆ—ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ä¿ƒè¿›äº†æ–°å…´èƒ½åŠ›çš„å‘å±•ï¼ŒåŒ…æ‹¬çµæ´»çš„å·¥å…·ç»„åˆã€é«˜æ•ˆçš„é“¾å¼æ‰§è¡Œä»¥åŠä»è¿è¡Œæ—¶åé¦ˆä¸­è¿›è¡Œç¨³å¥çš„é”™è¯¯æ¢å¤ï¼›åŒæ—¶æ„å»ºäº†æ–°çš„SFTå’ŒRLæ•°æ®é›†ä»¥åŠæŒ‘æˆ˜æ€§åŸºå‡†å¥—ä»¶ï¼Œç”¨äºä¸¥æ ¼è¯„ä¼°æ–¹å‘å˜åŒ–å’Œå¤šå·¥å…·æ¨ç†çš„é²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å·¥å…·æ¨ç†ä¸­çš„å…³é”®è„†å¼±æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼›CodeVisionæ¡†æ¶é€šè¿‡ä»£ç å³å·¥å…·çš„æ–¹æ³•å®ç°äº†æ›´çµæ´»å’Œé²æ£’çš„å¤šæ¨¡æ€æ¨ç†ï¼Œä¸ºæœªæ¥å·¥å…·å¢å¼ºå‹å¤šæ¨¡æ€ç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç°å®ä¸–ç•Œå¤æ‚è§†è§‰ä»»åŠ¡æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.</p>
<h3 id="8-firesentry-a-multi-modal-spatio-temporal-benchmark-dataset-for-fine-grained-wildfire-spread-forecasting">[8] <a href="https://arxiv.org/abs/2512.03369">FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting</a></h3>
<p><em>Nan Zhou, Huandong Wang, Jiahao Li, Han Li, Yali Song, Qiuhua Wang, Yong Li, Xinlei Chen</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†FireSentryæ•°æ®é›†å’ŒFiReDiffèŒƒå¼ï¼Œå‰è€…æ˜¯é¦–ä¸ªçœçº§è§„æ¨¡ã€äºšç±³çº§ç©ºé—´å’Œäºšç§’çº§æ—¶é—´åˆ†è¾¨ç‡çš„é‡ç«å¤šæ¨¡æ€æ•°æ®é›†ï¼Œåè€…æ˜¯ä¸€ç§æ–°é¢–çš„åŒæ¨¡æ€é¢„æµ‹æ–¹æ³•ï¼Œé€šè¿‡é¦–å…ˆç”Ÿæˆçº¢å¤–è§†é¢‘åºåˆ—å†ç²¾ç¡®åˆ†å‰²ç«åœºæ©ç ï¼Œæ˜¾è‘—æå‡äº†ç»†ç²’åº¦é‡ç«è”“å»¶é¢„æµ‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é‡ç«è”“å»¶é¢„æµ‹ç ”ç©¶ä¸»è¦å…³æ³¨ç²—æ—¶ç©ºå°ºåº¦å¹¶ä¾èµ–ä½åˆ†è¾¨ç‡å«æ˜Ÿæ•°æ®ï¼Œä»…èƒ½æ•æ‰å®è§‚ç«æƒ…çŠ¶æ€ï¼Œä¸¥é‡é™åˆ¶äº†é«˜ç²¾åº¦å±€éƒ¨ç«åŠ¿åŠ¨æ€å»ºæ¨¡èƒ½åŠ›ï¼ŒäºŸéœ€ç»†ç²’åº¦é¢„æµ‹æ–¹æ³•ä»¥æå‡åº”æ€¥å“åº”æ•ˆèƒ½å’Œå†³ç­–ç²¾åº¦ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é¦–å…ˆæ„å»ºäº†FireSentryæ•°æ®é›†ï¼Œé€šè¿‡åŒæ­¥æ— äººæœºå¹³å°é‡‡é›†äºšç±³çº§ç©ºé—´åˆ†è¾¨ç‡å’Œäºšç§’çº§æ—¶é—´åˆ†è¾¨ç‡çš„å¯è§å…‰ä¸çº¢å¤–è§†é¢‘æµã€ç°åœºç¯å¢ƒæµ‹é‡æ•°æ®åŠäººå·¥éªŒè¯çš„ç«åœºæ©ç ï¼›åœ¨æ­¤åŸºç¡€ä¸Šå»ºç«‹äº†æ¶µç›–ç‰©ç†æ¨¡å‹ã€æ•°æ®é©±åŠ¨æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼›æå‡ºäº†FiReDiffåŒæ¨¡æ€èŒƒå¼ï¼Œè¯¥èŒƒå¼å…ˆåœ¨çº¢å¤–æ¨¡æ€ä¸­é¢„æµ‹æœªæ¥è§†é¢‘åºåˆ—ï¼Œå†åŸºäºç”Ÿæˆçš„åŠ¨æ€ä¿¡æ¯åœ¨æ©ç æ¨¡æ€ä¸­ç²¾ç¡®åˆ†å‰²ç«åœºæ©ç ã€‚</p>
<p><strong>Result:</strong> FiReDiffåœ¨ç”Ÿæˆæ¨¡å‹åº”ç”¨ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œè§†é¢‘è´¨é‡æ–¹é¢PSNRæå‡39.2%ã€SSIMæå‡36.1%ã€LPIPSæå‡50.0%ã€FVDæå‡29.4%ï¼Œæ©ç ç²¾åº¦æ–¹é¢AUPRCæå‡3.3%ã€F1åˆ†æ•°æå‡59.1%ã€IoUæå‡42.9%ã€MSEæå‡62.5%ï¼Œå…¨é¢è¶…è¶Šäº†ç°æœ‰çš„ä»…æ©ç æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> FireSentryåŸºå‡†æ•°æ®é›†å’ŒFiReDiffèŒƒå¼å…±åŒæ¨è¿›äº†ç»†ç²’åº¦é‡ç«é¢„æµ‹å’ŒåŠ¨æ€ç¾å®³æ¨¡æ‹Ÿé¢†åŸŸçš„å‘å±•ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€æ•°æ®èåˆå’Œæ—¶åºåŠ¨æ€å»ºæ¨¡åœ¨ç²¾å‡†ç«åŠ¿é¢„æµ‹ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºæœªæ¥é«˜ç²¾åº¦åº”æ€¥å“åº”ç³»ç»Ÿæä¾›äº†é‡è¦æŠ€æœ¯åŸºç¡€å’Œæ•°æ®æ”¯æŒã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.</p>
<h3 id="9-adaptvision-efficient-vision-language-models-via-adaptive-visual-acquisition">[9] <a href="https://arxiv.org/abs/2512.03794">AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition</a></h3>
<p><em>Zichuan Lin, Yicheng Liu, Yang Yang, Lvfang Tao, Deheng Ye</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºAdaptVisionï¼Œä¸€ç§åŸºäºäººç±»ä¸»åŠ¨è§†è§‰æœºåˆ¶çš„é«˜æ•ˆè§†è§‰è¯­è¨€æ¨¡å‹èŒƒå¼ï¼Œé€šè¿‡ä»ç²—åˆ°ç»†çš„è‡ªé€‚åº”è§†è§‰æ ‡è®°è·å–ï¼Œåœ¨å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶ä¿æŒè§†è§‰é—®ç­”æ€§èƒ½ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’Œè§£è€¦å›åˆç­–ç•¥ä¼˜åŒ–ï¼Œå®ç°äº†æ ¹æ®ä»»åŠ¡éœ€æ±‚åŠ¨æ€è°ƒæ•´è§†è§‰ä¿¡æ¯è·å–çš„èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ä¾èµ–å¤§é‡è§†è§‰æ ‡è®°å¯¼è‡´æ˜¾è‘—è®¡ç®—å¼€é”€ï¼Œè€Œç°æœ‰é«˜æ•ˆæ–¹æ³•é‡‡ç”¨å›ºå®šæ¯”ä¾‹å‹ç¼©ç¼ºä¹é€‚åº”æ€§ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³VLMsèƒ½å¦è‡ªä¸»ç¡®å®šæ¯ä¸ªæ ·æœ¬æ‰€éœ€æœ€å°è§†è§‰æ ‡è®°æ•°çš„é—®é¢˜ï¼Œå—äººç±»ä¸»åŠ¨è§†è§‰æœºåˆ¶å¯å‘ï¼Œæ¢ç´¢è‡ªé€‚åº”è§†è§‰æ ‡è®°è·å–çš„æ–°èŒƒå¼ã€‚</p>
<p><strong>Method:</strong> æå‡ºAdaptVisionèŒƒå¼ï¼Œé‡‡ç”¨ä»ç²—åˆ°ç»†çš„è‡ªé€‚åº”è§†è§‰æ ‡è®°è·å–æ–¹æ³•ï¼šé¦–å…ˆå¤„ç†ä½åˆ†è¾¨ç‡å›¾åƒçš„å‹ç¼©è§†è§‰æ ‡è®°ï¼Œå¿…è¦æ—¶è°ƒç”¨è¾¹ç•Œæ¡†å·¥å…·è£å‰ªå…³é”®åŒºåŸŸè·å–é¢å¤–è§†è§‰ä¿¡æ¯ã€‚é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è®­ç»ƒï¼Œæ ¸å¿ƒæ˜¯è§£è€¦å›åˆç­–ç•¥ä¼˜åŒ–ï¼Œå°†å­¦ä¹ ç›®æ ‡åˆ†è§£ä¸ºå·¥å…·å­¦ä¹ å’Œå‡†ç¡®æ€§æ”¹è¿›ä¸¤ä¸ªç»„ä»¶ï¼Œå¹¶è¿›ä¸€æ­¥è§£è€¦ä¼˜åŠ¿ä¼°è®¡ä¸ºæ¯ä¸ªç›®æ ‡è®¡ç®—ç‹¬ç«‹ä¼˜åŠ¿ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªVQAåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒAdaptVisionåœ¨æ¶ˆè€—æ˜¾è‘—å°‘äºç°æœ‰é«˜æ•ˆVLMæ–¹æ³•çš„è§†è§‰æ ‡è®°çš„åŒæ—¶ï¼Œå®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚ä¸æ ‡å‡†GRPOç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„ä¼˜åŒ–æ•ˆæœæ›´ä¸ºæœ‰æ•ˆï¼Œåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†VLMsèƒ½å¤Ÿè‡ªä¸»ç¡®å®šæ‰€éœ€è§†è§‰æ ‡è®°æ•°é‡çš„å¯è¡Œæ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªé€‚åº”è§†è§‰ä¿¡æ¯è·å–èŒƒå¼ã€‚è§£è€¦å›åˆç­–ç•¥ä¼˜åŒ–ä¸ºå¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æä¾›äº†æœ‰æ•ˆæ¡†æ¶ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆã€æ›´æ™ºèƒ½çš„è§†è§‰è¯­è¨€ç³»ç»Ÿå¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.</p>
<h3 id="10-shelfgaussian-shelf-supervised-open-vocabulary-gaussian-based-3d-scene-understanding">[10] <a href="https://arxiv.org/abs/2512.03370">ShelfGaussian: Shelf-Supervised Open-Vocabulary Gaussian-based 3D Scene Understanding</a></h3>
<p><em>Lingjun Zhao, Yandong Luo, James Hay, Lu Gan</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ShelfGaussianï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¼€æ”¾è¯æ±‡å¤šæ¨¡æ€é«˜æ–¯åˆ†å¸ƒçš„3Dåœºæ™¯ç†è§£æ¡†æ¶ï¼Œåˆ©ç”¨ç°æˆçš„è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œç›‘ç£ï¼Œå®ç°äº†åœ¨é›¶æ ·æœ¬è¯­ä¹‰å æ®é¢„æµ‹ä»»åŠ¡ä¸Šçš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºé«˜æ–¯åˆ†å¸ƒçš„æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šä¸€æ˜¯å°†ç‰©ä½“å»ºæ¨¡ä¸ºå°é—­é›†è¯­ä¹‰é«˜æ–¯åˆ†å¸ƒï¼Œä¾èµ–äºæ ‡æ³¨çš„3Dæ ‡ç­¾ä¸”å¿½ç•¥äº†æ¸²æŸ“èƒ½åŠ›ï¼›äºŒæ˜¯é€šè¿‡çº¯2Dè‡ªç›‘ç£å­¦ä¹ å¼€æ”¾é›†é«˜æ–¯è¡¨ç¤ºï¼Œå¯¼è‡´å‡ ä½•è´¨é‡ä¸‹é™ä¸”ä»…é™äºç›¸æœºè®¾ç½®ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å……åˆ†æŒ–æ˜é«˜æ–¯åˆ†å¸ƒçš„æ½œåŠ›ï¼Œè§£å†³è¿™äº›é™åˆ¶ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†å¤šæ¨¡æ€é«˜æ–¯å˜æ¢å™¨ï¼Œä½¿é«˜æ–¯åˆ†å¸ƒèƒ½å¤Ÿä»å¤šç§ä¼ æ„Ÿå™¨æ¨¡æ€ä¸­æŸ¥è¯¢ç‰¹å¾ï¼›è®¾è®¡äº†è´§æ¶ç›‘ç£å­¦ä¹ èŒƒå¼ï¼Œåœ¨2Då›¾åƒå’Œ3Dåœºæ™¯å±‚é¢è”åˆä¼˜åŒ–é«˜æ–¯åˆ†å¸ƒä¸è§†è§‰åŸºç¡€æ¨¡å‹ç‰¹å¾ï¼›æ„å»ºäº†å¼€æ”¾è¯æ±‡å¤šæ¨¡æ€é«˜æ–¯æ¡†æ¶ï¼Œåˆ©ç”¨ç°æˆè§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œç›‘ç£ã€‚</p>
<p><strong>Result:</strong> åœ¨Occ3D-nuScenesåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬è¯­ä¹‰å æ®é¢„æµ‹æ€§èƒ½ï¼›åœ¨æ— äººåœ°é¢è½¦è¾†ä¸Šè¿›è¡Œäº†çœŸå®ä¸–ç•Œè¯„ä¼°ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ ·åŒ–åŸå¸‚åœºæ™¯ä¸­çš„é‡å¤–æ€§èƒ½ï¼›å±•ç¤ºäº†æ¡†æ¶åœ¨å„ç§æ„ŸçŸ¥å’Œè§„åˆ’ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆå¤šæ¨¡æ€ä¼ æ„Ÿå™¨ä¿¡æ¯å’Œè§†è§‰åŸºç¡€æ¨¡å‹ç›‘ç£èƒ½å¤Ÿæ˜¾è‘—æå‡é«˜æ–¯åˆ†å¸ƒæ–¹æ³•çš„åœºæ™¯ç†è§£èƒ½åŠ›ï¼Œä¸ºå¼€æ”¾è¯æ±‡3Dåœºæ™¯ç†è§£æä¾›äº†æœ‰æ•ˆæ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†åœ¨å®é™…æœºå™¨äººåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>We introduce ShelfGaussian, an open-vocabulary multi-modal Gaussian-based 3D scene understanding framework supervised by off-the-shelf vision foundation models (VFMs). Gaussian-based methods have demonstrated superior performance and computational efficiency across a wide range of scene understanding tasks. However, existing methods either model objects as closed-set semantic Gaussians supervised by annotated 3D labels, neglecting their rendering ability, or learn open-set Gaussian representations via purely 2D self-supervision, leading to degraded geometry and limited to camera-only settings. To fully exploit the potential of Gaussians, we propose a Multi-Modal Gaussian Transformer that enables Gaussians to query features from diverse sensor modalities, and a Shelf-Supervised Learning Paradigm that efficiently optimizes Gaussians with VFM features jointly at 2D image and 3D scene levels. We evaluate ShelfGaussian on various perception and planning tasks. Experiments on Occ3D-nuScenes demonstrate its state-of-the-art zero-shot semantic occupancy prediction performance. ShelfGaussian is further evaluated on an unmanned ground vehicle (UGV) to assess its in the-wild performance across diverse urban scenarios. Project website: https://lunarlab-gatech.github.io/ShelfGaussian/.</p>
<h3 id="11-multi-aspect-knowledge-enhanced-medical-vision-language-pretraining-with-multi-agent-data-generation">[11] <a href="https://arxiv.org/abs/2512.03445">Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation</a></h3>
<p><em>Xieji Li, Siyuan Yan, Yingsheng Liu, H. Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“æ•°æ®ç”Ÿæˆç³»ç»Ÿå’ŒåŸºäºæœ¬ä½“çš„å¤šç»´åº¦çŸ¥è¯†å¢å¼ºé¢„è®­ç»ƒï¼Œè§£å†³äº†åŒ»å­¦å›¾åƒåˆ†æä¸­æ•°æ®å™ªå£°å’Œé•¿æ–‡æœ¬å¤æ‚æ€§çš„æŒ‘æˆ˜ï¼Œåœ¨çš®è‚¤ç—…å­¦é¢†åŸŸå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç½‘ç»œæ”¶é›†æ•°æ®å›ºæœ‰çš„å™ªå£°é—®é¢˜ï¼Œä»¥åŠéç»“æ„åŒ–é•¿åŒ»å­¦æ–‡æœ¬çš„å¤æ‚æ€§ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†ä»å¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬å¯¹ä¸­å­¦ä¹ é«˜è´¨é‡è¡¨ç¤ºçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç²¾ç¡®åŒ»å­¦çŸ¥è¯†çš„é¢†åŸŸå¦‚çš®è‚¤ç—…å­¦ä¸­ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé›†æˆå¤šæ™ºèƒ½ä½“æ•°æ®ç”Ÿæˆç³»ç»Ÿå’ŒåŸºäºæœ¬ä½“çš„å¤šç»´åº¦çŸ¥è¯†å¢å¼ºé¢„è®­ç»ƒæ¡†æ¶ã€‚MAGENç³»ç»Ÿé€šè¿‡åŸºç¡€æ¨¡å‹è¾…åŠ©çš„æ ‡æ³¨å’ŒåŸºäºæ£€ç´¢çš„éªŒè¯æµç¨‹åˆæˆçŸ¥è¯†ä¸°å¯Œçš„æè¿°ä»¥å¢å¼ºæ•°æ®è´¨é‡ï¼›O-MAKEæ–¹æ³•å°†é•¿éç»“æ„åŒ–æ–‡æœ¬åˆ†è§£ä¸ºä¸åŒçš„çŸ¥è¯†ç»´åº¦ï¼Œå®ç°å…¨å±€å’Œå±€éƒ¨å±‚é¢çš„ç»†ç²’åº¦å¯¹é½ï¼Œå¹¶é€šè¿‡æœ¬ä½“å¼•å¯¼æœºåˆ¶æ˜¾å¼å»ºæ¨¡åŒ»å­¦æ¦‚å¿µå…³ç³»ã€‚</p>
<p><strong>Result:</strong> åœ¨çš®è‚¤ç—…å­¦é¢†åŸŸçš„ç»¼åˆå®éªŒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å…«ä¸ªæ•°æ®é›†ä¸Šçš„ç–¾ç—…åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜å‘å¸ƒäº†åŒ…å«è¶…è¿‡40ä¸‡çš®è‚¤å›¾åƒ-æ–‡æœ¬å¯¹çš„å¢å¼ºæ•°æ®é›†Derm1M-AgentAugï¼ŒéªŒè¯äº†æ¯ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡æ™ºèƒ½æ•°æ®å¢å¼ºå’Œæœ¬ä½“çŸ¥è¯†æ•´åˆå¯ä»¥æœ‰æ•ˆè§£å†³åŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚æ¡†æ¶çš„æˆåŠŸåº”ç”¨ä¸ºåŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸæä¾›äº†æ–°çš„èŒƒå¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®è´¨é‡æœ‰é™å’Œæ–‡æœ¬å¤æ‚æ€§é«˜çš„åœºæ™¯ä¸‹ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations. However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts. To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining. First, MAGEN enhances data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline. Second, O-MAKE addresses the difficulty of learning from long, unstructured texts by decomposing them into distinct knowledge aspects. This facilitates fine-grained alignment at both global and patch levels, while explicitly modeling medical concept relationships through ontology-guided mechanisms. We validate our framework in the field of dermatology, where comprehensive experiments demonstrate the effectiveness of each component. Our approach achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight datasets. Our code and the augmented dataset Derm1M-AgentAug, comprising over 400k skin-image-text pairs, will be released at https://github.com/SiyuanYan1/Derm1M.</p>
<h3 id="12-mos-mitigating-optical-sar-modality-gap-for-cross-modal-ship-re-identification">[12] <a href="https://arxiv.org/abs/2512.03404">MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification</a></h3>
<p><em>Yujian Zhao, Hankun Liu, Guanglin Niu</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMOSæ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ€ä¸€è‡´è¡¨ç¤ºå­¦ä¹ å’Œè·¨æ¨¡æ€æ•°æ®ç”Ÿæˆä¸ç‰¹å¾èåˆï¼Œæœ‰æ•ˆç¼“è§£å…‰å­¦ä¸SARå›¾åƒä¹‹é—´çš„æ¨¡æ€å·®å¼‚ï¼Œæ˜¾è‘—æå‡è·¨æ¨¡æ€èˆ¹èˆ¶é‡è¯†åˆ«æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å…‰å­¦ä¸åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å›¾åƒä¹‹é—´çš„è·¨æ¨¡æ€èˆ¹èˆ¶é‡è¯†åˆ«æ˜¯æµ·äº‹æƒ…æŠ¥ä¸ç›‘è§†ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œä½†ä¸¤ç§æ¨¡æ€é—´çš„æ˜¾è‘—å·®å¼‚æ„æˆäº†é²æ£’è¯†åˆ«çš„é‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰ç ”ç©¶å¯¹æ­¤æ¢ç´¢ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> MOSæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæ¨¡æ€ä¸€è‡´è¡¨ç¤ºå­¦ä¹ é€šè¿‡SARå›¾åƒå»å™ªå¤„ç†å’Œç±»çº§æ¨¡æ€å¯¹é½æŸå¤±æ¥å¯¹é½è·¨æ¨¡æ€çš„ç±»å†…ç‰¹å¾åˆ†å¸ƒï¼›è·¨æ¨¡æ€æ•°æ®ç”Ÿæˆä¸ç‰¹å¾èåˆåˆ©ç”¨å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹åˆæˆè·¨æ¨¡æ€æ ·æœ¬ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå°†åˆæˆç‰¹å¾ä¸åŸå§‹ç‰¹å¾èåˆä»¥å¢å¼ºå¯¹é½æ€§å’Œåˆ¤åˆ«æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨HOSS ReIDæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMOSåœ¨æ‰€æœ‰è¯„ä¼°åè®®ä¸‹å‡æ˜¾è‘—è¶…è¶Šç°æœ‰æœ€ä¼˜æ–¹æ³•ï¼Œåœ¨ALL to ALLã€Optical to SARå’ŒSAR to Opticalè®¾ç½®ä¸‹åˆ†åˆ«å®ç°äº†R1å‡†ç¡®ç‡+3.0%ã€+6.2%å’Œ+16.4%çš„æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡æ¨¡æ€å¯¹é½å’Œè·¨æ¨¡æ€æ•°æ®ç”Ÿæˆå¯ä»¥æœ‰æ•ˆç¼“è§£å…‰å­¦-SARæ¨¡æ€å·®å¼‚ï¼Œä¸ºè·¨æ¨¡æ€èˆ¹èˆ¶é‡è¯†åˆ«æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨è·¨æ¨¡æ€ç‰¹å¾å¢å¼ºä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) imagery has recently emerged as a critical yet underexplored task in maritime intelligence and surveillance. However, the substantial modality gap between optical and SAR images poses a major challenge for robust identification. To address this issue, we propose MOS, a novel framework designed to mitigate the optical-SAR modality gap and achieve modality-consistent feature learning for optical-SAR cross-modal ship ReID. MOS consists of two core components: (1) Modality-Consistent Representation Learning (MCRL) applies denoise SAR image procession and a class-wise modality alignment loss to align intra-identity feature distributions across modalities. (2) Cross-modal Data Generation and Feature fusion (CDGF) leverages a brownian bridge diffusion model to synthesize cross-modal samples, which are subsequently fused with original features during inference to enhance alignment and discriminability. Extensive experiments on the HOSS ReID dataset demonstrate that MOS significantly surpasses state-of-the-art methods across all evaluation protocols, achieving notable improvements of +3.0%, +6.2%, and +16.4% in R1 accuracy under the ALL to ALL, Optical to SAR, and SAR to Optical settings, respectively. The code and trained models will be released upon publication.</p>
<h3 id="13-think-before-you-drive-world-model-inspired-multimodal-grounding-for-autonomous-vehicles">[13] <a href="https://arxiv.org/abs/2512.03454">Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</a></h3>
<p><em>Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ThinkDeeperæ¡†æ¶ï¼Œé€šè¿‡ç©ºé—´æ„ŸçŸ¥ä¸–ç•Œæ¨¡å‹å¯¹è‡ªåŠ¨é©¾é©¶ä¸­çš„è§†è§‰å®šä½ä»»åŠ¡è¿›è¡Œå‰ç»æ€§æ¨ç†ï¼Œæœ‰æ•ˆè§£å†³äº†æ¨¡ç³ŠæŒ‡ä»¤å’Œä¸Šä¸‹æ–‡ä¾èµ–çš„æŒ‘æˆ˜ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è‡ªåŠ¨é©¾é©¶è§†è§‰å®šä½æ–¹æ³•åœ¨å¤„ç†æ¨¡ç³Šã€ä¸Šä¸‹æ–‡ä¾èµ–çš„æŒ‡ä»¤æ—¶å­˜åœ¨å›°éš¾ï¼Œä¸»è¦å› ä¸ºå®ƒä»¬ç¼ºä¹å¯¹ä¸‰ç»´ç©ºé—´å…³ç³»å’Œåœºæ™¯æ¼”å˜çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¯¹è‡ªç„¶è¯­è¨€å‘½ä»¤çš„å‡†ç¡®ç†è§£å’Œç›®æ ‡å®šä½ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ThinkDeeperæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯ç©ºé—´æ„ŸçŸ¥ä¸–ç•Œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†å½“å‰åœºæ™¯è’¸é¦ä¸ºæŒ‡ä»¤æ„ŸçŸ¥çš„æ½œåœ¨çŠ¶æ€ï¼Œå¹¶æ¨æ¼”å‡ºä¸€ç³»åˆ—æœªæ¥æ½œåœ¨çŠ¶æ€ä»¥æä¾›å‰ç»æ€§çº¿ç´¢ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨è¶…å›¾å¼•å¯¼çš„è§£ç å™¨å±‚æ¬¡åŒ–èåˆè¿™äº›çŠ¶æ€ä¸å¤šæ¨¡æ€è¾“å…¥ï¼Œæ•è·é«˜é˜¶ç©ºé—´ä¾èµ–å…³ç³»ä»¥å®ç°é²æ£’å®šä½ã€‚åŒæ—¶ï¼Œè¿˜æ„å»ºäº†DrivePilotæ•°æ®é›†ï¼Œé‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œæ€ç»´é“¾æç¤ºçš„å¤§è¯­è¨€æ¨¡å‹ç®¡é“ç”Ÿæˆè¯­ä¹‰æ ‡æ³¨ã€‚</p>
<p><strong>Result:</strong> ThinkDeeperåœ¨Talk2Caræ’è¡Œæ¦œä¸Šæ’åç¬¬ä¸€ï¼Œå¹¶åœ¨DrivePilotã€MoCADå’ŒRefCOCO/+/gåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚è¯¥æ¡†æ¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§å’Œæ•ˆç‡ï¼Œå³ä½¿åœ¨ä»…ä½¿ç”¨50%æ•°æ®è®­ç»ƒæ—¶ä»èƒ½ä¿æŒä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä¸–ç•Œæ¨¡å‹è¿›è¡Œå‰ç»æ€§ç©ºé—´æ¨ç†èƒ½æ˜¾è‘—æå‡è‡ªåŠ¨é©¾é©¶ä¸­è§†è§‰å®šä½ä»»åŠ¡çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ¨¡ç³ŠæŒ‡ä»¤å’Œå¤æ‚åœºæ™¯æ—¶ã€‚è¯¥æ–¹æ³•ä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„è‡ªç„¶è¯­è¨€äº¤äº’æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†ç©ºé—´æ„ŸçŸ¥æ¨ç†åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.</p>
<h3 id="14-vidic-video-difference-captioning">[14] <a href="https://arxiv.org/abs/2512.03405">ViDiC: Video Difference Captioning</a></h3>
<p><em>Jiangtao Wu, Shihao Li, Zhaozhou Bian, Yuanxing Zhang, Jialu Chen, Runzhe Wen, An Ping, Yiwen He, Jiakai Wang, Jiaheng Liu</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†è§†é¢‘å·®å¼‚æè¿°ï¼ˆViDiCï¼‰ä»»åŠ¡åŠç›¸åº”çš„ViDiC-1Kæ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘å¯¹ä¹‹é—´è¿›è¡Œç»†ç²’åº¦ç›¸ä¼¼æ€§å’Œå·®å¼‚æè¿°çš„èƒ½åŠ›ï¼Œå¡«è¡¥äº†ç°æœ‰è§†è§‰è¯­è¨€ç³»ç»Ÿåœ¨åŠ¨æ€åœºæ™¯æ¯”è¾ƒæ„ŸçŸ¥æ–¹é¢çš„ç ”ç©¶ç©ºç™½ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€ç³»ç»Ÿåœ¨ç†è§£åŠ¨æ€åœºæ™¯ä¹‹é—´çš„è§†è§‰å·®å¼‚æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯å¯¹ç»„åˆæ€§ã€ç©ºé—´æ€§å’Œæ—¶é—´æ€§å˜åŒ–çš„æ¯”è¾ƒæ„ŸçŸ¥èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è™½ç„¶å›¾åƒå·®å¼‚æè¿°ï¼ˆIDCï¼‰ç ”ç©¶å·²ä½¿æ¨¡å‹èƒ½å¤Ÿæè¿°é™æ€å›¾åƒä¹‹é—´çš„è¯­ä¹‰å˜åŒ–ï¼Œä½†è¿™äº›æ–¹æ³•æ— æ³•æ•æ‰è¿åŠ¨è¿ç»­æ€§ã€äº‹ä»¶æ¼”å˜æˆ–æ—¶é—´ä¸Šçš„ç¼–è¾‘ä¸€è‡´æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†è§†é¢‘å·®å¼‚æè¿°ï¼ˆViDiCï¼‰ä»»åŠ¡å¹¶æ„å»ºäº†ViDiC-1Kæ•°æ®é›†ï¼ŒåŒ…å«1,000ä¸ªç²¾å¿ƒç­–åˆ’çš„è§†é¢‘å¯¹ï¼Œæ ‡æ³¨äº†è¶…è¿‡4,000ä¸ªæ¯”è¾ƒæ£€æŸ¥é¡¹ï¼Œæ¶µç›–ä¸»ä½“ã€é£æ ¼ã€èƒŒæ™¯ã€æ‘„å½±ã€è¿åŠ¨ã€ä½ç½®å’Œæ’­æ”¾æŠ€æœ¯ç­‰ä¸ƒä¸ªç±»åˆ«ã€‚ä¸ºç¡®ä¿å¯é è¯„ä¼°ï¼Œæå‡ºäº†åŸºäºLLM-as-a-Judgeåè®®çš„åŒæ£€æŸ¥è¡¨æ¡†æ¶ï¼Œåˆ†åˆ«æµ‹é‡ç›¸ä¼¼æ€§å’Œå·®å¼‚çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨19ä¸ªä»£è¡¨æ€§å¤šæ¨¡æ€æ¨¡å‹ä¸Šçš„å®éªŒæ­ç¤ºäº†å®ƒä»¬åœ¨æ¯”è¾ƒæè¿°å’Œå·®å¼‚æ„ŸçŸ¥èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚ViDiC-1Kæ•°æ®é›†ä½œä¸ºä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œä¸ºè¯„ä¼°è§†é¢‘ç†è§£èƒ½åŠ›æä¾›äº†ç³»ç»Ÿæ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†å½“å‰æ¨¡å‹åœ¨åŠ¨æ€åœºæ™¯æ¯”è¾ƒåˆ†ææ–¹é¢çš„å±€é™æ€§ã€‚</p>
<p><strong>Conclusion:</strong> ViDiC-1Kä¸ºæ¨è¿›å¤šæ¨¡æ€æ™ºèƒ½ä¸­çš„è§†é¢‘ç†è§£ã€ç¼–è¾‘æ„ŸçŸ¥å’Œæ¯”è¾ƒæ¨ç†å¥ å®šäº†åšå®åŸºç¡€ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åŠ¨æ€åœºæ™¯æ¯”è¾ƒåˆ†æçš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥æ¨¡å‹å¼€å‘æä¾›äº†æ˜ç¡®çš„è¯„ä¼°æ ‡å‡†ï¼Œæœ‰æœ›æ¨åŠ¨è§†é¢‘å·®å¼‚ç†è§£é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.</p>
<h3 id="15-generalization-evaluation-of-deep-stereo-matching-methods-for-uav-based-forestry-applications">[15] <a href="https://arxiv.org/abs/2512.03427">Generalization Evaluation of Deep Stereo Matching Methods for UAV-Based Forestry Applications</a></h3>
<p><em>Yida Lin, Bing Xue, Mengjie Zhang, Sam Schofield, Richard Green</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é¦–æ¬¡å¯¹å…«ç§æœ€å…ˆè¿›çš„ç«‹ä½“æ·±åº¦ä¼°è®¡æ–¹æ³•åœ¨æ—ä¸šåœºæ™¯ä¸­è¿›è¡Œç³»ç»Ÿæ€§é›¶æ ·æœ¬è¯„ä¼°ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°ä¸»è¦å…³æ³¨åŸå¸‚å’Œå®¤å†…ç¯å¢ƒçš„ç©ºç™½ï¼Œå¹¶è¯†åˆ«å‡ºDEFOMä½œä¸ºæ¤è¢«å¯†é›†ç¯å¢ƒæ·±åº¦ä¼°è®¡çš„æœ€ä½³åŸºå‡†æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªä¸»æ— äººæœºæ—ä¸šä½œä¸šéœ€è¦å…·æœ‰å¼ºå¤§è·¨åŸŸæ³›åŒ–èƒ½åŠ›çš„é²æ£’æ·±åº¦ä¼°è®¡æ–¹æ³•ï¼Œä½†ç°æœ‰è¯„ä¼°ä¸»è¦é›†ä¸­äºåŸå¸‚å’Œå®¤å†…åœºæ™¯ï¼Œç¼ºä¹å¯¹æ¤è¢«å¯†é›†ä¸“ä¸šç¯å¢ƒçš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œè¿™æ„æˆäº†ä¸€ä¸ªå…³é”®çš„ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¯¹å…«ç§æœ€å…ˆè¿›çš„ç«‹ä½“æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿæ€§é›¶æ ·æœ¬è¯„ä¼°ï¼Œæ¶µç›–è¿­ä»£ç»†åŒ–ã€åŸºç¡€æ¨¡å‹å’Œé›¶æ ·æœ¬é€‚åº”èŒƒå¼ï¼ŒåŒ…æ‹¬RAFT-Stereoã€IGEVã€IGEV++ã€BridgeDepthã€StereoAnywhereã€DEFOMä»¥åŠåŸºçº¿æ–¹æ³•ACVNetã€PSMNetå’ŒTCstereoã€‚æ‰€æœ‰æ–¹æ³•ä»…åœ¨Scene Flowæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¹¶åœ¨å››ä¸ªæ ‡å‡†åŸºå‡†ï¼ˆETH3Dã€KITTI 2012/2015ã€Middleburyï¼‰å’Œä¸€ä¸ªåŒ…å«5,313å¯¹å›¾åƒçš„æ–°å‹Canterburyæ—ä¸šæ•°æ®é›†ä¸Šè¿›è¡Œæ— å¾®è°ƒè¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºåœºæ™¯ä¾èµ–çš„æ€§èƒ½æ¨¡å¼ï¼šåŸºç¡€æ¨¡å‹åœ¨ç»“æ„åŒ–åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼ˆBridgeDepthåœ¨ETH3Dä¸Šä¸º0.23 pxï¼Œåœ¨KITTIä¸Šä¸º0.83-1.07 pxï¼›DEFOMåœ¨å„åŸºå‡†ä¸Šä¸º0.35-4.65 pxï¼‰ï¼Œè€Œè¿­ä»£æ–¹æ³•ä¿æŒè·¨åŸŸé²æ£’æ€§ï¼ˆIGEV++ä¸º0.36-6.77 pxï¼›IGEVä¸º0.33-21.91 pxï¼‰ã€‚å…³é”®å‘ç°æ˜¯RAFT-Stereoåœ¨ETH3Dä¸Šå‡ºç°ç¾éš¾æ€§å¤±è´¥ï¼ˆ26.23 px EPEï¼Œ98%é”™è¯¯ç‡ï¼‰ï¼Œä½†åœ¨KITTIä¸Šè¡¨ç°æ­£å¸¸ï¼ˆ0.90-1.11 pxï¼‰ã€‚åœ¨Canterburyæ—ä¸šæ•°æ®é›†ä¸Šï¼ŒDEFOMè¢«è¯†åˆ«ä¸ºæ¤è¢«æ·±åº¦ä¼°è®¡çš„æœ€ä½³é»„é‡‘æ ‡å‡†åŸºçº¿ï¼Œå±•ç°å‡ºä¼˜äºIGEV++çš„æ·±åº¦å¹³æ»‘æ€§ã€é®æŒ¡å¤„ç†å’Œè·¨åŸŸä¸€è‡´æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†ç«‹ä½“æ·±åº¦ä¼°è®¡æ–¹æ³•çš„åœºæ™¯ä¾èµ–æ€§ï¼Œå¼ºè°ƒäº†åœ¨ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚æ—ä¸šï¼‰è¿›è¡Œé’ˆå¯¹æ€§è¯„ä¼°çš„é‡è¦æ€§ã€‚DEFOMè¢«ç¡®ç«‹ä¸ºæ¤è¢«å¯†é›†ç¯å¢ƒæ·±åº¦ä¼°è®¡çš„æ¨èåŸºå‡†æ–¹æ³•ï¼Œä¸ºè‡ªä¸»æ— äººæœºæ—ä¸šä½œä¸šæä¾›äº†å®ç”¨çš„æŠ€æœ¯æŒ‡å¯¼ã€‚ç ”ç©¶è¿˜æŒ‡å‡ºäº†RAFT-Stereoç­‰æ–¹æ³•çš„ç‰¹å®šå¤±è´¥æ¨¡å¼ï¼Œä¸ºæœªæ¥æ–¹æ³•çš„é²æ£’æ€§æ”¹è¿›æä¾›äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Autonomous UAV forestry operations require robust depth estimation methods with strong cross-domain generalization. However, existing evaluations focus on urban and indoor scenarios, leaving a critical gap for specialized vegetation-dense environments. We present the first systematic zero-shot evaluation of eight state-of-the-art stereo methods--RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM (plus baseline methods ACVNet, PSMNet, TCstereo)--spanning iterative refinement, foundation model, and zero-shot adaptation paradigms. All methods are trained exclusively on Scene Flow and evaluated without fine-tuning on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury forestry dataset captured with ZED Mini camera (1920x1080). Performance reveals scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D, 0.83-1.07 px on KITTI; DEFOM: 0.35-4.65 px across benchmarks), while iterative methods maintain cross-domain robustness (IGEV++: 0.36-6.77 px; IGEV: 0.33-21.91 px). Critical finding: RAFT-Stereo exhibits catastrophic ETH3D failure (26.23 px EPE, 98 percent error rate) due to negative disparity predictions, while performing normally on KITTI (0.90-1.11 px). Qualitative evaluation on Canterbury forestry dataset identifies DEFOM as the optimal gold-standard baseline for vegetation depth estimation, exhibiting superior depth smoothness, occlusion handling, and cross-domain consistency compared to IGEV++, despite IGEV++'s finer detail preservation.</p>
<h3 id="16-rethinking-prompt-design-for-inference-time-scaling-in-text-to-visual-generation">[16] <a href="https://arxiv.org/abs/2512.03534">Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation</a></h3>
<p><em>Subin Kim, Sangwoo Mo, Mamshad Nayeem Rizve, Yiran Xu, Difan Liu, Jinwoo Shin, Tobias Hinz</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºPRISæ¡†æ¶ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°ä¿®è®¢æç¤ºè¯æ¥æ”¹è¿›æ–‡æœ¬åˆ°è§†è§‰ç”Ÿæˆçš„å¯¹é½é—®é¢˜ã€‚è¯¥æ–¹æ³•å¼•å…¥å…ƒç´ çº§äº‹å®æ ¡æ­£éªŒè¯å™¨ï¼Œåœ¨å¤šä¸ªè§†è§‰ç”ŸæˆåŸºå‡†ä¸Šå®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ–‡æœ¬åˆ°è§†è§‰ç”Ÿæˆä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯ç”¨æˆ·æ„å›¾ä¸ç”Ÿæˆè§†è§‰å†…å®¹ä¹‹é—´çš„ç²¾ç¡®å¯¹é½ï¼Œä¼ ç»Ÿæ–¹æ³•ä¸»è¦é€šè¿‡æ‰©å±•è§†è§‰ç”Ÿæˆè¿‡ç¨‹ï¼ˆå¦‚å¢åŠ é‡‡æ ·æ­¥æ•°æˆ–ç§å­æ•°ï¼‰æ¥è§£å†³ï¼Œä½†è¿™ç§æ–¹æ³•å¾ˆå¿«è¾¾åˆ°è´¨é‡ç“¶é¢ˆï¼Œå› ä¸ºæç¤ºè¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒå›ºå®šï¼Œæ— æ³•æ ¹æ®ç”Ÿæˆç»“æœè¿›è¡Œé€‚åº”æ€§è°ƒæ•´ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºPRISæ¡†æ¶ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°ä¿®è®¢æç¤ºè¯ä»¥å“åº”æ‰©å±•çš„è§†è§‰ç”Ÿæˆã€‚æ ¸å¿ƒæ€æƒ³æ˜¯å®¡æŸ¥ç”Ÿæˆçš„è§†è§‰å†…å®¹ï¼Œè¯†åˆ«è·¨è§†è§‰çš„é‡å¤å¤±è´¥æ¨¡å¼ï¼Œå¹¶ç›¸åº”åœ°é‡æ–°è®¾è®¡æç¤ºè¯ï¼Œç„¶åä½¿ç”¨ä¿®è®¢åçš„æç¤ºè¯é‡æ–°ç”Ÿæˆè§†è§‰å†…å®¹ã€‚ä¸ºæä¾›ç²¾ç¡®çš„å¯¹é½åé¦ˆï¼Œå¼•å…¥äº†å…ƒç´ çº§äº‹å®æ ¡æ­£éªŒè¯å™¨ï¼Œåœ¨ç»†ç²’åº¦çº§åˆ«è¯„ä¼°æç¤ºå±æ€§ä¸ç”Ÿæˆè§†è§‰ä¹‹é—´çš„å¯¹é½ï¼Œç›¸æ¯”æ•´ä½“è¯„ä¼°æ–¹æ³•å®ç°æ›´å‡†ç¡®å’Œå¯è§£é‡Šçš„è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬åœ¨VBench 2.0ä¸Šå®ç°äº†15%çš„æ€§èƒ½å¢ç›Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè”åˆæ‰©å±•æç¤ºè¯å’Œè§†è§‰å†…å®¹æ˜¯å……åˆ†åˆ©ç”¨æ¨ç†æ—¶æ‰©å±•å®šå¾‹çš„å…³é”®ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­è”åˆæ‰©å±•æç¤ºè¯å’Œè§†è§‰å†…å®¹å¯¹äºå®ç°æ–‡æœ¬åˆ°è§†è§‰ç”Ÿæˆçš„ç²¾ç¡®å¯¹é½è‡³å…³é‡è¦ã€‚PRISæ¡†æ¶é€šè¿‡è‡ªé€‚åº”æç¤ºä¿®è®¢å’Œç»†ç²’åº¦éªŒè¯æœºåˆ¶ï¼Œä¸ºè§£å†³ç”Ÿæˆæ¨¡å‹ä¸­çš„å¯¹é½é—®é¢˜æä¾›äº†æ–°çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†æç¤ºè¯åŠ¨æ€è°ƒæ•´åœ¨æå‡ç”Ÿæˆè´¨é‡ä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.</p>
<h3 id="17-label-efficient-hyperspectral-image-classification-via-spectral-film-modulation-of-low-level-pretrained-diffusion-features">[17] <a href="https://arxiv.org/abs/2512.03430">Label-Efficient Hyperspectral Image Classification via Spectral FiLM Modulation of Low-Level Pretrained Diffusion Features</a></h3>
<p><em>Yuzhen Hu, Biplab Banerjee, Saurabh Prasad</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ ‡ç­¾é«˜æ•ˆçš„æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç©ºé—´ç‰¹å¾è¿›è¡Œé«˜å…‰è°±å›¾åƒåˆ†ç±»ï¼Œé€šè¿‡è½»é‡çº§FiLMèåˆæ¨¡å—æ•´åˆå…‰è°±ä¸ç©ºé—´ä¿¡æ¯ï¼Œåœ¨ç¨€ç–æ ‡æ³¨ä¸‹å®ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é«˜å…‰è°±æˆåƒè™½ç„¶èƒ½å®ç°è¯¦ç»†çš„åœ°ç‰©åˆ†ç±»ï¼Œä½†é¢ä¸´ç©ºé—´åˆ†è¾¨ç‡ä½å’Œæ ‡æ³¨ç¨€ç–çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨æœ‰é™æ ‡æ³¨ä¸‹éš¾ä»¥æœ‰æ•ˆæ•´åˆå…‰è°±ä¸ç©ºé—´ä¿¡æ¯ï¼Œéœ€è¦å¼€å‘æ ‡ç­¾é«˜æ•ˆçš„å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åˆ©ç”¨åœ¨è‡ªç„¶å›¾åƒä¸Šé¢„è®­ç»ƒçš„å†»ç»“æ‰©æ•£æ¨¡å‹æå–ç©ºé—´ç‰¹å¾ï¼Œä»æ—©æœŸå»å™ªæ—¶é—´æ­¥çš„é«˜åˆ†è¾¨ç‡è§£ç å™¨å±‚è·å–ä½å±‚è¡¨ç¤ºï¼Œå¹¶å¼•å…¥è½»é‡çº§FiLMèåˆæ¨¡å—ï¼Œé€šè¿‡å…‰è°±çº¿ç´¢è‡ªé€‚åº”è°ƒåˆ¶å†»ç»“çš„ç©ºé—´ç‰¹å¾ï¼Œå®ç°ç¨€ç–ç›‘ç£ä¸‹çš„é²æ£’å¤šæ¨¡æ€å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸¤ä¸ªè¿‘æœŸé«˜å…‰è°±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨æä¾›çš„ç¨€ç–è®­ç»ƒæ ‡ç­¾å°±è¶…è¶Šäº†æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ‰©æ•£æ¨¡å‹ç‰¹å¾å’Œå…‰è°±æ„ŸçŸ¥èåˆçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†æ¡†æ¶åœ¨æ ‡ç­¾ç¨€ç¼ºæƒ…å†µä¸‹çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ”¯æŒé¢†åŸŸæ— å…³çš„æ ‡ç­¾é«˜æ•ˆè¡¨ç¤ºå­¦ä¹ ï¼Œä¸ºé¥æ„ŸåŠæ›´å¹¿æ³›çš„ç§‘å­¦æˆåƒä»»åŠ¡æä¾›äº†æ–°æ€è·¯ï¼Œè¯æ˜äº†å†»ç»“æ‰©æ•£ç‰¹å¾åœ¨å¤šæ¨¡æ€èåˆä¸­çš„æœ‰æ•ˆæ€§å’Œå¯è¿ç§»æ€§ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Hyperspectral imaging (HSI) enables detailed land cover classification, yet low spatial resolution and sparse annotations pose significant challenges. We present a label-efficient framework that leverages spatial features from a frozen diffusion model pretrained on natural images. Our approach extracts low-level representations from high-resolution decoder layers at early denoising timesteps, which transfer effectively to the low-texture structure of HSI. To integrate spectral and spatial information, we introduce a lightweight FiLM-based fusion module that adaptively modulates frozen spatial features using spectral cues, enabling robust multimodal learning under sparse supervision. Experiments on two recent hyperspectral datasets demonstrate that our method outperforms state-of-the-art approaches using only the provided sparse training labels. Ablation studies further highlight the benefits of diffusion-derived features and spectral-aware fusion. Overall, our results indicate that pretrained diffusion models can support domain-agnostic, label-efficient representation learning for remote sensing and broader scientific imaging tasks.</p>
<h3 id="18-cookanything-a-framework-for-flexible-and-consistent-multi-step-recipe-image-generation">[18] <a href="https://arxiv.org/abs/2512.03540">CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation</a></h3>
<p><em>Ruoxuan Zhang, Bin Wen, Hongxia Xie, Yi Yao, Songhan Zuo, Jian-Yu Jiang-Lin, Hong-Han Shuai, Wen-Huang Cheng</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CookAnythingï¼Œä¸€ä¸ªçµæ´»çš„æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œç”¨äºä»ä»»æ„é•¿åº¦çš„æ–‡æœ¬çƒ¹é¥ªæŒ‡ä»¤ç”Ÿæˆè¿è´¯ã€è¯­ä¹‰åˆ†æ˜çš„å›¾åƒåºåˆ—ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç»“æ„åŒ–å¤šæ­¥éª¤åœºæ™¯å’Œå¯å˜æŒ‡ä»¤é•¿åº¦æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–å¤šæ­¥éª¤åœºæ™¯ï¼ˆå¦‚èœè°±å›¾è§£ï¼‰æ—¶å­˜åœ¨å›°éš¾ï¼Œä¸”å½“å‰èœè°±å›¾è§£æ–¹æ³•æ— æ³•é€‚åº”èœè°±é•¿åº¦çš„è‡ªç„¶å˜åŒ–ï¼Œæ— è®ºå®é™…æŒ‡ä»¤ç»“æ„å¦‚ä½•éƒ½ç”Ÿæˆå›ºå®šæ•°é‡çš„å›¾åƒï¼Œè¿™é™åˆ¶äº†åœ¨ç¨‹åºæ€§å†…å®¹åˆ›ä½œä¸­çš„åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¡†æ¶å¼•å…¥äº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ­¥éª¤çº§åŒºåŸŸæ§åˆ¶ï¼ˆSRCï¼‰åœ¨å•ä¸ªå»å™ªè¿‡ç¨‹ä¸­å°†æ–‡æœ¬æ­¥éª¤ä¸å¯¹åº”å›¾åƒåŒºåŸŸå¯¹é½ï¼›çµæ´»çš„RoPEä½œä¸ºæ­¥éª¤æ„ŸçŸ¥çš„ä½ç½®ç¼–ç æœºåˆ¶ï¼Œå¢å¼ºæ—¶é—´è¿è´¯æ€§å’Œç©ºé—´å¤šæ ·æ€§ï¼›è·¨æ­¥éª¤ä¸€è‡´æ€§æ§åˆ¶ï¼ˆCSCCï¼‰ä¿æŒæ­¥éª¤é—´ç»†ç²’åº¦æˆåˆ†ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨èœè°±å›¾è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCookAnythingåœ¨åŸºäºè®­ç»ƒå’Œæ— è®­ç»ƒè®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿæ”¯æŒå¤æ‚å¤šæ­¥éª¤æŒ‡ä»¤çš„å¯æ‰©å±•é«˜è´¨é‡è§†è§‰åˆæˆï¼ŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æå‡ºçš„æ¡†æ¶åœ¨ç¨‹åºæ€§å†…å®¹åˆ›ä½œé¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•™å­¦åª’ä½“å’Œç»“æ„åŒ–è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢ï¼Œä¸ºå¤„ç†å¯å˜é•¿åº¦å¤šæ­¥éª¤æŒ‡ä»¤çš„è§†è§‰åˆæˆæä¾›äº†ç³»ç»Ÿè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.</p>
<h3 id="19-v-iti-mitigating-hallucinations-in-multimodal-large-language-models-via-visual-inference-time-intervention">[19] <a href="https://arxiv.org/abs/2512.03542">V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention</a></h3>
<p><em>Nan Sun, Zhenyu Zhang, Xixun Lin, Kun Wang, Yanmin Shang, Naibin Gu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang, Yanan Cao</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºV-ITIï¼Œä¸€ç§è½»é‡çº§è§†è§‰æ¨ç†æ—¶å¹²é¢„æ¡†æ¶ï¼Œé€šè¿‡æ£€æµ‹å¤´çº§æ¿€æ´»æ¨¡å¼è¯†åˆ«è§†è§‰å¿½è§†ï¼Œä»…åœ¨å¿…è¦æ—¶è¿›è¡Œå¹²é¢„ï¼Œæœ‰æ•ˆç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒé€šç”¨ä»»åŠ¡æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¼—å¤šè§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨å¹»è§‰é—®é¢˜ï¼Œç”Ÿæˆä¸è¾“å…¥è§†è§‰å†…å®¹ä¸ä¸€è‡´çš„è¾“å‡ºï¼Œè¿™åœ¨ç²¾åº¦æ•æ„Ÿé¢†åŸŸä¸¥é‡æŸå®³å¯é æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å¹²é¢„æ³¨æ„åŠ›åˆ†æ•°æˆ–è¾“å‡ºlogitsæ¥ç¼“è§£å¹»è§‰ï¼Œä½†ä¸»è¦å…³æ³¨"å¦‚ä½•å¹²é¢„"è€Œå¿½ç•¥äº†"ä½•æ—¶å¹²é¢„"è¿™ä¸€å‰æï¼Œå¯¼è‡´"è¿‡åº¦å¹²é¢„"é—®é¢˜ï¼Œè¿›è€Œå¼•å…¥æ–°çš„å¹»è§‰å’Œä¸å¿…è¦çš„è®¡ç®—å¼€é”€ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é¦–å…ˆç ”ç©¶äº†è§†è§‰å¿½è§†æœºåˆ¶ï¼Œå‘ç°å¯ä»¥é€šè¿‡MLLMsä¸­çš„å¤´çº§æ¿€æ´»æ¨¡å¼å‡†ç¡®æ£€æµ‹è§†è§‰å¿½è§†ã€‚åŸºäºæ­¤æå‡ºäº†V-ITIæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè§†è§‰å¿½è§†æ£€æµ‹å™¨é€šè¿‡å¤´çº§åˆ¤åˆ«æ€§æ¢é’ˆè¯†åˆ«è§†è§‰å¿½è§†ï¼›è§†è§‰å›å¿†å¹²é¢„å™¨ä»…åœ¨æ£€æµ‹åˆ°è§†è§‰å¿½è§†æ—¶ï¼Œä½¿ç”¨é¢„å­˜å‚¨çš„è§†è§‰æ¿€æ´»ä¿¡æ¯æ¥è°ƒåˆ¶æ¿€æ´»ï¼Œå®ç°ç²¾å‡†å¹²é¢„ã€‚</p>
<p><strong>Result:</strong> åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•å’Œä¸åŒMLLMå®¶æ—ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒV-ITIèƒ½å¤ŸæŒç»­ç¼“è§£è§†è§‰ç›¸å…³å¹»è§‰ï¼ŒåŒæ—¶ä¿æŒé€šç”¨ä»»åŠ¡æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†è§†è§‰å¿½è§†å¯ä»¥é€šè¿‡å¤´çº§æ¿€æ´»æ¨¡å¼å‡†ç¡®æ£€æµ‹ï¼Œå¹¶æå‡ºäº†è½»é‡çº§çš„æ¨ç†æ—¶å¹²é¢„æ¡†æ¶V-ITIï¼Œé€šè¿‡"ä½•æ—¶å¹²é¢„"çš„ç²¾ç¡®æ§åˆ¶è§£å†³äº†è¿‡åº¦å¹²é¢„é—®é¢˜ï¼Œä¸ºç¼“è§£MLLMså¹»è§‰æä¾›äº†æ–°æ€è·¯ï¼Œåœ¨ä¿æŒæ¨¡å‹é€šç”¨èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡äº†è§†è§‰å¯é æ€§ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on "how to intervene" but overlooking the prerequisite "when to intervene", which leads to the "over-intervention" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.</p>
<h3 id="20-lm-cartseg-automated-segmentation-of-lateral-and-medial-cartilage-and-subchondral-bone-for-radiomics-analysis">[20] <a href="https://arxiv.org/abs/2512.03449">LM-CartSeg: Automated Segmentation of Lateral and Medial Cartilage and Subchondral Bone for Radiomics Analysis</a></h3>
<p><em>Tongxu Zhang</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºLM-CartSegï¼Œä¸€ç§å…¨è‡ªåŠ¨çš„è†å…³èŠ‚MRIåˆ†å‰²ä¸å½±åƒç»„å­¦åˆ†ææµç¨‹ï¼Œé€šè¿‡å‡ ä½•åå¤„ç†è§„åˆ™å’Œé›¶æ ·æœ¬é¢„æµ‹èåˆï¼Œå®ç°äº†ç¨³å¥çš„è½¯éª¨/éª¨åˆ†å‰²ä¸å†…å¤–ä¾§åˆ†å®¤ï¼Œä¸ºå¤šä¸­å¿ƒéª¨å…³èŠ‚ç‚å½±åƒç»„å­¦ç ”ç©¶æä¾›äº†å®ç”¨åŸºç¡€ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è†å…³èŠ‚MRIå½±åƒç»„å­¦ç ”ç©¶éœ€è¦ç¨³å¥ä¸”å…·æœ‰è§£å‰–å­¦æ„ä¹‰çš„æ„Ÿå…´è¶£åŒºåŸŸæ¥åŒæ—¶æ•è·è½¯éª¨å’Œè½¯éª¨ä¸‹éª¨ï¼Œç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–æ‰‹åŠ¨æ ‡æ³¨ä¸”ç¼ºä¹è´¨é‡æ§åˆ¶æŠ¥å‘Šï¼Œè¿™é™åˆ¶äº†å¤šä¸­å¿ƒç ”ç©¶çš„å¯é æ€§å’Œå¯é‡å¤æ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åŸºäºä¸¤ä¸ª3D nnU-Netæ¨¡å‹åˆ†åˆ«åœ¨SKM-TEAå’ŒOAIZIB-CMæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæµ‹è¯•æ—¶èåˆé›¶æ ·æœ¬é¢„æµ‹å¹¶é€šè¿‡å‡ ä½•è§„åˆ™è¿›è¡Œåå¤„ç†ï¼ŒåŒ…æ‹¬è¿é€šåˆ†é‡æ¸…æ´—ã€åœ¨ç‰©ç†ç©ºé—´æ„å»º10æ¯«ç±³è½¯éª¨ä¸‹éª¨å¸¦ï¼Œä»¥åŠåŸºäºPCAå’Œk-meansçš„æ•°æ®é©±åŠ¨èƒ«éª¨å†…å¤–ä¾§åˆ†å‰²ã€‚</p>
<p><strong>Result:</strong> åå¤„ç†æ˜¾è‘—æ”¹å–„äº†åˆ†å‰²æ€§èƒ½ï¼Œåœ¨OAIZIB-CMæµ‹è¯•é›†ä¸Šå®è§‚ASSDä»2.63æ¯«ç±³é™è‡³0.36æ¯«ç±³ï¼ŒHD95ä»25.2æ¯«ç±³é™è‡³3.35æ¯«ç±³ï¼ŒDSCè¾¾åˆ°0.91ï¼›åœ¨SKI-10æ•°æ®é›†ä¸Šé›¶æ ·æœ¬DSCä¸º0.80ã€‚å‡ ä½•å†…å¤–ä¾§åˆ†å‰²è§„åˆ™åœ¨ä¸åŒæ•°æ®é›†é—´äº§ç”Ÿç¨³å®šåˆ†å®¤ï¼Œè€Œç›´æ¥ä½¿ç”¨å†…å¤–ä¾§nnU-Netæ¨¡å‹åˆ™å‡ºç°åŸŸä¾èµ–çš„ä¾§å‘äº¤æ¢é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> LM-CartSegèƒ½å¤Ÿç”Ÿæˆè‡ªåŠ¨åŒ–çš„è´¨é‡æ§åˆ¶æ„Ÿå…´è¶£åŒºåŸŸå’Œå½±åƒç»„å­¦ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾æºå¸¦äº†è¶…è¶Šç®€å•å½¢æ€æµ‹é‡çš„åˆ¤åˆ«ä¿¡æ¯ï¼Œä¸ºå¤šä¸­å¿ƒè†å…³èŠ‚éª¨å…³èŠ‚ç‚å½±åƒç»„å­¦ç ”ç©¶æä¾›äº†å®ç”¨åŸºç¡€ï¼ŒåŒæ—¶å‘ç°ä»…6-12%çš„ç‰¹å¾ä¸ä½“ç§¯æˆ–åšåº¦å¼ºç›¸å…³ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Background and Objective: Radiomics of knee MRI requires robust, anatomically meaningful regions of interest (ROIs) that jointly capture cartilage and subchondral bone. Most existing work relies on manual ROIs and rarely reports quality control (QC). We present LM-CartSeg, a fully automatic pipeline for cartilage/bone segmentation, geometric lateral/medial (L/M) compartmentalisation and radiomics analysis. Methods: Two 3D nnU-Net models were trained on SKM-TEA (138 knees) and OAIZIB-CM (404 knees). At test time, zero-shot predictions were fused and refined by simple geometric rules: connected-component cleaning, construction of 10 mm subchondral bone bands in physical space, and a data-driven tibial L/M split based on PCA and k-means. Segmentation was evaluated on an OAIZIB-CM test set (103 knees) and on SKI-10 (100 knees). QC used volume and thickness signatures. From 10 ROIs we extracted 4 650 non-shape radiomic features to study inter-compartment similarity, dependence on ROI size, and OA vs. non-OA classification on OAIZIB-CM Results: Post-processing improved macro ASSD on OAIZIB-CM from 2.63 to 0.36 mm and HD95 from 25.2 to 3.35 mm, with DSC 0.91; zero-shot DSC on SKI-10 was 0.80. The geometric L/M rule produced stable compartments across datasets, whereas a direct L/M nnU-Net showed domain-dependent side swaps. Only 6 to 12 percent of features per ROI were strongly correlated with volume or thickness. Radiomics-based models models restricted to size-linked features. Conclusions: LM-CartSeg yields automatic, QCd ROIs and radiomic features that carry discriminative information beyond simple morphometry, providing a practical foundation for multi-centre knee OA radiomics studies.</p>
<h3 id="21-dynamic-content-moderation-in-livestreams-combining-supervised-classification-with-mllm-boosted-similarity-matching">[21] <a href="https://arxiv.org/abs/2512.03553">Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching</a></h3>
<p><em>Wei Chee Yew, Hailun Xu, Sanjay Saha, Xiaotian Fan, Hiok Hian Ong, David Yuchen Wang, Kanchan Sarkar, Zhenheng Yang, Danhui Guan</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¤§è§„æ¨¡ç”¨æˆ·ç”Ÿæˆè§†é¢‘å¹³å°çš„æ··åˆå†…å®¹å®¡æ ¸æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç›‘ç£åˆ†ç±»å’ŒåŸºäºå‚è€ƒçš„ç›¸ä¼¼æ€§åŒ¹é…ï¼Œä»¥åº”å¯¹ç›´æ’­ç¯å¢ƒä¸­å¤šæ¨¡æ€ã€åŠæ—¶ä¸”éœ€é€‚åº”ä¸æ–­æ¼”åŒ–çš„è¿è§„å†…å®¹æ£€æµ‹æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§è§„æ¨¡ç”¨æˆ·ç”Ÿæˆè§†é¢‘å¹³å°çš„å†…å®¹å®¡æ ¸é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨ç›´æ’­ç¯å¢ƒä¸­éœ€è¦å®ç°åŠæ—¶ã€å¤šæ¨¡æ€ä¸”å¯¹ä¸æ–­æ¼”åŒ–çš„è¿è§„å†…å®¹å…·æœ‰é²æ£’æ€§çš„æ£€æµ‹ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†æ–°å‹æˆ–å¾®å¦™çš„è¿è§„æ¡ˆä¾‹ã€‚</p>
<p><strong>Method:</strong> è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆå®¡æ ¸æ¡†æ¶ï¼Œç»“åˆç›‘ç£åˆ†ç±»ç®¡é“å¤„ç†å·²çŸ¥è¿è§„å†…å®¹ï¼Œä»¥åŠåŸºäºå‚è€ƒçš„ç›¸ä¼¼æ€§åŒ¹é…ç®¡é“æ£€æµ‹æ–°å‹æˆ–å¾®å¦™æ¡ˆä¾‹ï¼Œå¤šæ¨¡æ€è¾“å…¥é€šè¿‡ä¸¤ä¸ªç®¡é“å¤„ç†ï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å°†çŸ¥è¯†è’¸é¦åˆ°æ¯ä¸ªç®¡é“ä¸­ä»¥æå‡å‡†ç¡®æ€§åŒæ—¶ä¿æŒæ¨ç†è½»é‡çº§ã€‚</p>
<p><strong>Result:</strong> åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œåˆ†ç±»ç®¡é“åœ¨80%ç²¾ç¡®ç‡ä¸‹è¾¾åˆ°67%å¬å›ç‡ï¼Œç›¸ä¼¼æ€§ç®¡é“åœ¨80%ç²¾ç¡®ç‡ä¸‹è¾¾åˆ°76%å¬å›ç‡ï¼Œå¤§è§„æ¨¡A/Bæµ‹è¯•æ˜¾ç¤ºç”¨æˆ·è§‚çœ‹ä¸è‰¯ç›´æ’­çš„æ¯”ä¾‹å‡å°‘äº†6-8%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†ä¸€ç§å¯æ‰©å±•ä¸”é€‚åº”æ€§å¼ºçš„å¤šæ¨¡æ€å†…å®¹æ²»ç†æ–¹æ³•ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†æ˜¾æ€§è¿è§„å’Œæ–°å…´å¯¹æŠ—è¡Œä¸ºï¼Œä¸ºå¤§è§„æ¨¡è§†é¢‘å¹³å°æä¾›äº†å…¼é¡¾æ£€æµ‹å‡†ç¡®æ€§å’Œç³»ç»Ÿæ•ˆç‡çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.</p>
<h3 id="22-geovideo-introducing-geometric-regularization-into-video-generation-model">[22] <a href="https://arxiv.org/abs/2512.03453">GeoVideo: Introducing Geometric Regularization into Video Generation Model</a></h3>
<p><em>Yunpeng Bai, Shaoheng Fang, Chaohui Yu, Fan Wang, Qixing Huang</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æ·±åº¦é¢„æµ‹å¢å¼ºè§†é¢‘ç”Ÿæˆå‡ ä½•ä¸€è‡´æ€§çš„æ–¹æ³•ï¼Œå°†å¤šè§†è§’å‡ ä½•æŸå¤±å¼•å…¥æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè§†é¢‘çš„æ—¶ç©ºè¿è´¯æ€§å’Œç»“æ„åˆç†æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•ä¸»è¦åœ¨2Dåƒç´ ç©ºé—´æ“ä½œï¼Œç¼ºä¹æ˜ç¡®çš„3Dç»“æ„å»ºæ¨¡æœºåˆ¶ï¼Œå¯¼è‡´æ—¶é—´å‡ ä½•ä¸ä¸€è‡´ã€è¿åŠ¨ä¸è‡ªç„¶å’Œç»“æ„ä¼ªå½±ç­‰é—®é¢˜ï¼Œéœ€è¦å°†å‡ ä½•æ­£åˆ™åŒ–å¼•å…¥è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä»¥æå‡æ—¶ç©ºä¸€è‡´æ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é€šè¿‡ä¸ºæ½œåœ¨æ‰©æ•£æ¨¡å‹æ·»åŠ é€å¸§æ·±åº¦é¢„æµ‹æ¥å®ç°å‡ ä½•æ­£åˆ™åŒ–ï¼Œé‡‡ç”¨æ·±åº¦ä½œä¸ºå‡ ä½•è¡¨ç¤ºå› å…¶ä¸å›¾åƒç¼–ç å™¨çš„å…¼å®¹æ€§ï¼Œå¹¶æå‡ºå¤šè§†è§’å‡ ä½•æŸå¤±åœ¨å…±äº«3Dåæ ‡ç³»ä¸­å¯¹é½è·¨å¸§æ·±åº¦å›¾ä»¥å¢å¼ºç»“æ„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ç°æœ‰åŸºçº¿èƒ½äº§ç”Ÿæ˜¾è‘—æ›´ç¨³å®šå’Œå‡ ä½•ä¸€è‡´çš„ç»“æœï¼Œåœ¨æ—¶ç©ºè¿è´¯æ€§ã€å½¢çŠ¶ä¸€è‡´æ€§å’Œç‰©ç†åˆç†æ€§æ–¹é¢å‡æœ‰æ˜æ˜¾æ”¹å–„ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•æˆåŠŸå¼¥åˆäº†å¤–è§‚ç”Ÿæˆä¸3Dç»“æ„å»ºæ¨¡ä¹‹é—´çš„å·®è·ï¼Œä¸ºè§†é¢‘ç”Ÿæˆæä¾›äº†æœ‰æ•ˆçš„å‡ ä½•æ­£åˆ™åŒ–æ¡†æ¶ï¼Œå±•ç¤ºäº†æ·±åº¦è¡¨ç¤ºåœ¨æå‡ç”Ÿæˆè§†é¢‘è´¨é‡æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥ç»“åˆæ›´ä¸°å¯Œ3Då…ˆéªŒçš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.</p>
<h3 id="23-tog-bench-task-oriented-spatio-temporal-grounding-in-egocentric-videos">[23] <a href="https://arxiv.org/abs/2512.03666">ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos</a></h3>
<p><em>Qi'ao Xu, Tianwen Qian, Yuqian Fu, Kailing Li, Yang Jiao, Jiacheng Zhang, Xiaoling Wang, Liang He</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ToG-Benchï¼Œé¦–ä¸ªé¢å‘ä»»åŠ¡çš„æ—¶ç©ºè§†é¢‘å®šä½åŸºå‡†ï¼Œä¸“æ³¨äºå…·èº«æ™ºèƒ½ä¸­çš„ä»»åŠ¡å¯¼å‘å¯¹è±¡å®šä½ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å±€é™äºæè¿°æ€§æŒ‡ä»¤è€Œç¼ºä¹ä»»åŠ¡æ¨ç†èƒ½åŠ›çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ—¶ç©ºè§†é¢‘å®šä½ç ”ç©¶ä¸»è¦å±€é™äºå¯¹è±¡ä¸­å¿ƒå’Œæè¿°æ€§æŒ‡ä»¤ï¼Œå¿½è§†äº†ä»»åŠ¡å¯¼å‘æ¨ç†å¯¹äºå…·èº«æ™ºèƒ½ä½“å®ç°ç›®æ ‡å¯¼å‘äº¤äº’çš„å…³é”®ä½œç”¨ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨çœŸå®å…·èº«åœºæ™¯ä¸­çš„åº”ç”¨èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†åŸºäºScanNetè§†é¢‘çš„ToG-BenchåŸºå‡†ï¼ŒåŒ…å«100ä¸ªæ ‡æ³¨ç‰‡æ®µå’Œ2,704ä¸ªä»»åŠ¡å¯¼å‘å®šä½æŒ‡ä»¤ï¼Œé‡‡ç”¨ç»“åˆåŸºç¡€æ¨¡å‹æ ‡æ³¨å’Œäººå·¥ç»†åŒ–çš„åŠè‡ªåŠ¨åŒ–æµç¨‹ï¼Œå¹¶è®¾è®¡äº†é’ˆå¯¹å¤šå¯¹è±¡å’Œæ˜¾éšå¼å¯¹è±¡å®šä½çš„ä»»åŠ¡çº§è¯„ä¼°æŒ‡æ ‡ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¯„ä¼°äº†ä¸ƒä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºä»»åŠ¡å¯¼å‘æ—¶ç©ºè§†é¢‘å®šä½å­˜åœ¨å›ºæœ‰æŒ‘æˆ˜ï¼Œåœ¨æ˜¾éšå¼å’Œå¤šå¯¹è±¡å®šä½æ–¹é¢å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œæ­ç¤ºäº†æ„ŸçŸ¥ä¸äº¤äº’åœ¨å…·èº«åœºæ™¯ä¸­çš„èåˆéš¾åº¦ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†ä»»åŠ¡å¯¼å‘æ¨ç†åœ¨å…·èº«æ™ºèƒ½ä¸­çš„é‡è¦æ€§ï¼Œæå‡ºçš„åŸºå‡†å’Œè¯„ä¼°æ–¹æ³•ä¸ºè¿æ¥è§†è§‰æ„ŸçŸ¥ä¸ç‰©ç†äº¤äº’æä¾›äº†æ–°æ–¹å‘ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ç†è§£ä¸Šä¸‹æ–‡å’Œä»»åŠ¡æ„å›¾æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..</p>
<h3 id="24-pulse-a-unified-multi-task-architecture-for-cardiac-segmentation-diagnosis-and-few-shot-cross-modality-clinical-adaptation">[24] <a href="https://arxiv.org/abs/2512.03848">PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation</a></h3>
<p><em>Hania Ghouse, Maryam Alsharqi, Farhad R. Nezami, Muzammil Behzad</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†PULSEï¼Œä¸€ç§å¤šä»»åŠ¡è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œæ—¨åœ¨ç»Ÿä¸€å¿ƒè„å›¾åƒåˆ†æä¸­çš„è§£å‰–åˆ†å‰²ã€ç–¾ç—…åˆ†ç±»å’Œä¸´åºŠæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡è‡ªç›‘ç£è¡¨ç¤ºå’Œå¤åˆç›‘ç£ç­–ç•¥å®ç°è·¨æ¨¡æ€å’Œè·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¿ƒè„å›¾åƒåˆ†æç›®å‰å­˜åœ¨ä»»åŠ¡ç¢ç‰‡åŒ–é—®é¢˜ï¼Œè§£å‰–åˆ†å‰²ã€ç–¾ç—…åˆ†ç±»å’Œä¸´åºŠæŠ¥å‘Šç”Ÿæˆé€šå¸¸ç”±åœ¨ä¸åŒæ•°æ®æœºåˆ¶ä¸‹è®­ç»ƒçš„ç‹¬ç«‹ç½‘ç»œå¤„ç†ï¼Œç¼ºä¹èƒ½å¤Ÿå°†è¿™äº›ç›®æ ‡ç»Ÿä¸€åœ¨å•ä¸€æ¶æ„ä¸­å¹¶ä¿æŒè·¨æˆåƒæ¨¡æ€å’Œæ•°æ®é›†æ³›åŒ–èƒ½åŠ›çš„ç°æœ‰æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> PULSEæ¡†æ¶åŸºäºè‡ªç›‘ç£è¡¨ç¤ºæ„å»ºï¼Œé‡‡ç”¨å¤åˆç›‘ç£ç­–ç•¥å¹³è¡¡åŒºåŸŸé‡å å­¦ä¹ ã€åƒç´ çº§åˆ†ç±»ä¿çœŸåº¦å’Œè¾¹ç•Œæ„ŸçŸ¥IoUç»†åŒ–ï¼Œé€šè¿‡å¤šå°ºåº¦ä»¤ç‰Œé‡å»ºè§£ç å™¨å®ç°è§£å‰–åˆ†å‰²ï¼Œå…±äº«çš„å…¨å±€è¡¨ç¤ºæ”¯æŒç–¾ç—…åˆ†ç±»å’Œä¸´åºŠæ–‡æœ¬è¾“å‡ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å•ä¸€æ¶æ„å†…ä»åƒç´ åˆ°ç»“æ„å†åˆ°ä¸´åºŠæ¨ç†è¿›è¡Œè¿‡æ¸¡ã€‚</p>
<p><strong>Result:</strong> ä¸å…ˆå‰ä»»åŠ¡ç‰¹å®šç®¡é“ä¸åŒï¼ŒPULSEèƒ½å¤Ÿå­¦ä¹ ä»»åŠ¡ä¸å˜çš„å¿ƒè„å…ˆéªŒçŸ¥è¯†ï¼Œåœ¨è·¨æ•°æ®é›†ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”èƒ½å¤Ÿä»¥æœ€å°ç›‘ç£é€‚åº”æ–°çš„æˆåƒæ¨¡æ€ï¼Œä¸ºå¯æ‰©å±•çš„åŸºç¡€é£æ ¼å¿ƒè„åˆ†ææ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ¨åŠ¨äº†å¿ƒè„å›¾åƒåˆ†æé¢†åŸŸå‘ç»Ÿä¸€ã€å¯æ‰©å±•çš„åŸºç¡€æ¡†æ¶å‘å±•ï¼Œé€šè¿‡å•ä¸€æ¶æ„æ•´åˆå¤šä¸ªåˆ†æä»»åŠ¡å¹¶å®ç°è·¨æ¨¡æ€æ³›åŒ–ï¼Œä¸ºä¸´åºŠå®è·µæä¾›äº†æ›´é«˜æ•ˆã€ä¸€è‡´çš„åˆ†æå·¥å…·ï¼Œå‡å°‘äº†ä»»åŠ¡ç‰¹å®šç®¡é“å¸¦æ¥çš„å¤æ‚æ€§å’Œæ•°æ®éœ€æ±‚ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Cardiac image analysis remains fragmented across tasks: anatomical segmentation, disease classification, and grounded clinical report generation are typically handled by separate networks trained under different data regimes. No existing framework unifies these objectives within a single architecture while retaining generalization across imaging modalities and datasets. We introduce PULSE, a multi-task vision-language framework built on self-supervised representations and optimized through a composite supervision strategy that balances region overlap learning, pixel wise classification fidelity, and boundary aware IoU refinement. A multi-scale token reconstruction decoder enables anatomical segmentation, while shared global representations support disease classification and clinically grounded text output allowing the model to transition from pixels to structures and finally clinical reasoning within one architecture. Unlike prior task-specific pipelines, PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision. This moves the field closer to a scalable, foundation style cardiac analysis framework.</p>
<h3 id="25-procedural-mistake-detection-via-action-effect-modeling">[25] <a href="https://arxiv.org/abs/2512.03474">Procedural Mistake Detection via Action Effect Modeling</a></h3>
<p><em>Wenliang Guo, Yujiang Pu, Yu Kong</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºåŠ¨ä½œæ•ˆæœå»ºæ¨¡ï¼ˆAEMï¼‰æ¡†æ¶ï¼Œé€šè¿‡è”åˆå»ºæ¨¡åŠ¨ä½œæ‰§è¡ŒåŠå…¶ç»“æœæ¥æ”¹è¿›ç¨‹åºæ€§ä»»åŠ¡ä¸­çš„é”™è¯¯æ£€æµ‹ã€‚è¯¥æ¡†æ¶åœ¨å•ç±»åˆ†ç±»è®¾ç½®ä¸‹åœ¨EgoPERå’ŒCaptainCook4DåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç¨‹åºæ€§ä»»åŠ¡é”™è¯¯æ£€æµ‹æ–¹æ³•ä¸»è¦åˆ†æåŠ¨ä½œæ‰§è¡Œæ–¹å¼ï¼Œè€Œå¿½è§†äº†åŠ¨ä½œäº§ç”Ÿçš„ç»“æœï¼ˆå³åŠ¨ä½œæ•ˆæœï¼‰ã€‚è®¸å¤šé”™è¯¯å¹¶éä½“ç°åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œè€Œæ˜¯ä½“ç°åœ¨ç»“æœçŠ¶æ€ä¸­ï¼Œå¦‚æ„å¤–çš„ç‰©ä½“çŠ¶æ€æˆ–ä¸æ­£ç¡®çš„ç©ºé—´æ’åˆ—ï¼Œè¿™ä¸€ç ”ç©¶ç©ºç™½éœ€è¦è¢«å¡«è¡¥ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºåŠ¨ä½œæ•ˆæœå»ºæ¨¡ï¼ˆAEMï¼‰ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡æ¦‚ç‡å…¬å¼è”åˆæ•æ‰åŠ¨ä½œæ‰§è¡ŒåŠå…¶ç»“æœã€‚è¯¥æ–¹æ³•é¦–å…ˆåŸºäºè¯­ä¹‰ç›¸å…³æ€§å’Œè§†è§‰è´¨é‡é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ•ˆæœå¸§æ¥è¯†åˆ«åŠ¨ä½œç»“æœï¼Œç„¶åä»è§†è§‰å®šä½å’Œç¬¦å·åœºæ™¯å›¾ä¸­æå–äº’è¡¥çº¿ç´¢ï¼Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å¯¹é½ä»¥å½¢æˆé²æ£’çš„æ•ˆæœæ„ŸçŸ¥è¡¨ç¤ºã€‚ä¸ºæ£€æµ‹é”™è¯¯ï¼Œè¿›ä¸€æ­¥è®¾è®¡äº†åŸºäºæç¤ºçš„æ£€æµ‹å™¨ï¼Œç»“åˆä»»åŠ¡ç‰¹å®šæç¤ºå¹¶å°†æ¯ä¸ªåŠ¨ä½œç‰‡æ®µä¸å…¶é¢„æœŸæ‰§è¡Œè¯­ä¹‰å¯¹é½ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å•ç±»åˆ†ç±»ï¼ˆOCCï¼‰è®¾ç½®ä¸‹ï¼Œåœ¨EgoPERå’ŒCaptainCook4DåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè”åˆå»ºæ¨¡æ‰§è¡Œå’Œç»“æœèƒ½äº§ç”Ÿæ›´å¯é çš„é”™è¯¯æ£€æµ‹ï¼Œçªæ˜¾äº†æ•ˆæœæ„ŸçŸ¥è¡¨ç¤ºåœ¨æå‡æ£€æµ‹å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å»ºæ¨¡åŠ¨ä½œæ‰§è¡Œå’Œç»“æœèƒ½å®ç°æ›´å¯é çš„é”™è¯¯æ£€æµ‹ï¼Œæ•ˆæœæ„ŸçŸ¥è¡¨ç¤ºå…·æœ‰æ½œåŠ›åº”ç”¨äºæ›´å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚è¯¥æ¡†æ¶ä¸ºè§£å†³ç¨‹åºæ€§ä»»åŠ¡ä¸­åŸºäºç»“æœçš„é”™è¯¯æ£€æµ‹é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå¼ºè°ƒäº†åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­åŒæ—¶è€ƒè™‘æ‰§è¡Œè¿‡ç¨‹å’Œç»“æœçŠ¶æ€çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Mistake detection in procedural tasks is essential for building intelligent systems that support learning and task execution. Existing approaches primarily analyze how an action is performed, while overlooking what it produces, i.e., the \textbf{action effect}. Yet many errors manifest not in the execution itself but in the resulting outcome, such as an unintended object state or incorrect spatial arrangement. To address this gap, we propose Action Effect Modeling (AEM), a unified framework that jointly captures action execution and its outcomes through a probabilistic formulation. AEM first identifies the outcome of an action by selecting the most informative effect frame based on semantic relevance and visual quality. It then extracts complementary cues from visual grounding and symbolic scene graphs, aligning them in a shared latent space to form robust effect-aware representations. To detect mistakes, we further design a prompt-based detector that incorporates task-specific prompts and aligns each action segment with its intended execution semantics. Our approach achieves state-of-the-art performance on the EgoPER and CaptainCook4D benchmarks under the challenging one-class classification (OCC) setting. These results demonstrate that modeling both execution and outcome yields more reliable mistake detection, and highlight the potential of effect-aware representations to benefit a broader range of downstream applications.</p>
<h3 id="26-diq-h-evaluating-hallucination-persistence-in-vlms-under-temporal-visual-degradation">[26] <a href="https://arxiv.org/abs/2512.03992">DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation</a></h3>
<p><em>Zexin Lin, Hawen Wan, Yebin Zhong, Xiaoqiang</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DIQ-HåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯é¦–ä¸ªè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€è§†è§‰é€€åŒ–æ—¶åºåºåˆ—ä¸­é²æ£’æ€§çš„åŸºå‡†ï¼Œé€šè¿‡ç‰©ç†æ¨¡æ‹Ÿçš„è§†è§‰é€€åŒ–æ­ç¤ºç°æœ‰æ¨¡å‹åœ¨ç°å®éƒ¨ç½²ä¸­çš„ä¸¥é‡å¯é æ€§ç¼ºé™·ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†ä¸»è¦å…³æ³¨é™æ€é«˜è´¨é‡å›¾åƒï¼Œå¿½ç•¥äº†æ—¶é—´åºåˆ—ä¸­çš„è§†è§‰é€€åŒ–ä¸é”™è¯¯ä¼ æ’­é—®é¢˜ï¼Œè¿™äº›æ˜¯å®‰å…¨å…³é”®åº”ç”¨å¦‚è‡ªåŠ¨é©¾é©¶ä¸­çš„å…³é”®å¤±æ•ˆæ¨¡å¼ï¼Œå…¶ä¸­ç¬æ€è§†è§‰æŸåä¼šå¼•å‘è·¨å¸§æŒç»­å­˜åœ¨çš„å¹»è§‰ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†DIQ-HåŸºå‡†æµ‹è¯•ï¼Œåº”ç”¨åŸºäºç‰©ç†çš„è§†è§‰é€€åŒ–åŒ…æ‹¬è¿åŠ¨æ¨¡ç³Šã€ä¼ æ„Ÿå™¨å™ªå£°å’Œå‹ç¼©ä¼ªå½±ï¼Œå¹¶é€šè¿‡å¤šè½®é—®ç­”ä»»åŠ¡è¯„ä¼°å¹»è§‰æŒç»­æ€§ã€é”™è¯¯æ¢å¤å’Œæ—¶é—´ä¸€è‡´æ€§ï¼›åŒæ—¶æå‡ºäº†ä¸ç¡®å®šæ€§å¼•å¯¼è¿­ä»£ç²¾ç‚¼æ–¹æ³•ï¼Œåˆ©ç”¨è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œä¸ç¡®å®šæ€§è¿‡æ»¤ç”Ÿæˆå¯é çš„ä¼ªçœŸå€¼æ ‡æ³¨ã€‚</p>
<p><strong>Result:</strong> åœ¨16ä¸ªæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒæ­ç¤ºäº†æ˜¾è‘—çš„é²æ£’æ€§å·®è·ï¼šå³ä½¿æ˜¯GPT-4oç­‰å…ˆè¿›æ¨¡å‹ä¹Ÿä»…è¾¾åˆ°78.5%çš„æ¢å¤ç‡ï¼Œè€Œå¼€æºæ¨¡å‹çš„æ—¶é—´ä¸€è‡´æ€§è¡¨ç°ä½äº60%ï¼›æå‡ºçš„ä¸ç¡®å®šæ€§å¼•å¯¼è¿­ä»£ç²¾ç‚¼æ–¹æ³•å®ç°äº†15.3%çš„å‡†ç¡®ç‡æå‡ã€‚</p>
<p><strong>Conclusion:</strong> DIQ-HåŸºå‡†ä¸ºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­çš„å¯é æ€§æä¾›äº†å…¨é¢å¹³å°ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨åŠ¨æ€è§†è§‰é€€åŒ–æ¡ä»¶ä¸‹çš„ä¸¥é‡é²æ£’æ€§ç¼ºé™·ï¼Œå¼ºè°ƒäº†æ—¶é—´åºåˆ—è¯„ä¼°åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.</p>
<h3 id="27-fairness-aware-fine-tuning-of-vision-language-models-for-medical-glaucoma-diagnosis">[27] <a href="https://arxiv.org/abs/2512.03477">Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis</a></h3>
<p><em>Zijian Gu, Yuxi Liu, Zhenhao Zhang, Song Wang</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¬å¹³æ„ŸçŸ¥çš„ä½ç§©é€‚åº”æ–¹æ³•ï¼Œé€šè¿‡å¯å¾®åˆ†çš„MaxAccGapæŸå¤±å‡½æ•°ä¼˜åŒ–åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸åŒäººå£ç¾¤ä½“é—´çš„è¯Šæ–­å‡†ç¡®æ€§å·®å¼‚ï¼Œåœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†69%çš„å…¬å¹³æ€§å·®è·ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—å½±åƒä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸“å®¶çº§æ€§èƒ½ï¼Œä½†åœ¨ä¸åŒäººå£ç¾¤ä½“é—´å­˜åœ¨æ˜¾è‘—çš„è¯Šæ–­å‡†ç¡®æ€§å·®å¼‚ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„å…¬å¹³åº”ç”¨ï¼Œéœ€è¦å¼€å‘æ—¢èƒ½ä¿æŒå‚æ•°æ•ˆç‡åˆèƒ½ä¼˜åŒ–å…¬å¹³æ€§çš„é€‚åº”æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†å…¬å¹³æ„ŸçŸ¥çš„ä½ç§©é€‚åº”æ¡†æ¶ï¼Œæ ¸å¿ƒè´¡çŒ®æ˜¯å¯å¾®åˆ†çš„MaxAccGapæŸå¤±å‡½æ•°ï¼Œèƒ½å¤Ÿç«¯åˆ°ç«¯ä¼˜åŒ–ä¸åŒäººå£ç¾¤ä½“é—´çš„å‡†ç¡®æ€§å‡è¡¡ï¼›å…·ä½“åŒ…æ‹¬ä¸‰ç§æ–¹æ³•ï¼šFR-LoRAå°†MaxAccGapæ­£åˆ™åŒ–æ•´åˆåˆ°è®­ç»ƒç›®æ ‡ä¸­ï¼ŒGR-LoRAåº”ç”¨é€†é¢‘ç‡åŠ æƒå¹³è¡¡æ¢¯åº¦è´¡çŒ®ï¼ŒHybrid-LoRAç»“åˆä¸¤ç§æœºåˆ¶ï¼Œæ•´ä¸ªæ–¹æ³•ä»…éœ€0.24%çš„å¯è®­ç»ƒå‚æ•°ã€‚</p>
<p><strong>Result:</strong> åœ¨10,000å¼ é’å…‰çœ¼çœ¼åº•å›¾åƒä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒGR-LoRAå°†è¯Šæ–­å‡†ç¡®æ€§å·®å¼‚å‡å°‘äº†69%ï¼ŒåŒæ—¶ä¿æŒ53.15%çš„æ•´ä½“å‡†ç¡®ç‡ï¼›æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œå¼ºæ­£åˆ™åŒ–å¼ºåº¦åœ¨æœ€å°åŒ–å‡†ç¡®æ€§æŠ˜è¡·çš„æƒ…å†µä¸‹å®ç°äº†æœ€ä¼˜å…¬å¹³æ€§ï¼Œé’ˆå¯¹ç‰¹å®šç§æ—çš„ä¼˜åŒ–å®ç°äº†60%çš„å·®å¼‚å‡å°‘ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•é€šè¿‡å‚æ•°é«˜æ•ˆçš„å…¬å¹³æ€§ä¼˜åŒ–ï¼Œä¸ºèµ„æºå—é™çš„åŒ»ç–—ç¯å¢ƒä¸­éƒ¨ç½²å…¬å¹³çš„åŒ»å­¦AIæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼›ç ”ç©¶æ­ç¤ºäº†å¼ºæ­£åˆ™åŒ–åœ¨å¹³è¡¡å‡†ç¡®æ€§ä¸å…¬å¹³æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠäººå£ç¾¤ä½“ç‰¹å®šä¼˜åŒ–ç­–ç•¥çš„æ½œåŠ›ï¼Œæ¨åŠ¨äº†åŒ»ç–—AIå‘æ›´å…¬å¹³ã€å¯éƒ¨ç½²çš„æ–¹å‘å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.</p>
<h3 id="28-highly-efficient-test-time-scaling-for-t2i-diffusion-models-with-text-embedding-perturbation">[28] <a href="https://arxiv.org/abs/2512.03996">Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation</a></h3>
<p><em>Hang Xu, Linjiang Huang, Feng Zhao</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–‡æœ¬åµŒå…¥æ‰°åŠ¨æ–¹æ³•ï¼Œç”¨äºå¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„æµ‹è¯•æ—¶ç¼©æ”¾æ€§èƒ½ï¼Œé€šè¿‡ç»“åˆç©ºé—´å™ªå£°å’Œæ–‡æœ¬åµŒå…¥æ‰°åŠ¨åœ¨é¢‘åŸŸä¸Šçš„äº’è¡¥ç‰¹æ€§ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå¤šæ ·æ€§å’Œè´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ä¸»è¦å…³æ³¨æœç´¢ç­–ç•¥å’Œå¥–åŠ±æ¨¡å‹ï¼Œä½†å¿½ç•¥äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å™ªå£°éšæœºæ€§å¯¹æ–¹æ³•æ€§èƒ½çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯ç©ºé—´å™ªå£°åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¯èƒ½åœ¨é«˜é¢‘ç»†èŠ‚å¤„ç†ä¸Šå­˜åœ¨å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åŒ…å«ä¸¤ä¸ªå…³é”®è®¾è®¡ï¼šä¸€æ˜¯å¼•å…¥åŸºäºæ­¥éª¤çš„æ–‡æœ¬åµŒå…¥æ‰°åŠ¨ï¼Œç»“åˆé¢‘ç‡å¼•å¯¼çš„å™ªå£°è°ƒåº¦ä¸ç©ºé—´å™ªå£°æ‰°åŠ¨ï¼›äºŒæ˜¯æ ¹æ®é¢‘åŸŸè´¡çŒ®å’Œæ‰°åŠ¨å®¹å¿åº¦è‡ªé€‚åº”è°ƒæ•´æ‰°åŠ¨å¼ºåº¦ï¼Œåˆ©ç”¨æ–‡æœ¬åµŒå…¥æ‰°åŠ¨ä¸SDEæ³¨å…¥å™ªå£°åœ¨é¢‘åŸŸä¸Šçš„äº’è¡¥ç‰¹æ€§ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ä¸­ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå±•ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œä¸”å‡ ä¹ä¸å¢åŠ é¢å¤–è®¡ç®—å¼€é”€ï¼Œé€šè¿‡é¢‘åŸŸåˆ†æéªŒè¯äº†ç©ºé—´å™ªå£°åå¥½ä½é¢‘æˆåˆ†è€Œæ–‡æœ¬åµŒå…¥æ‰°åŠ¨å¢å¼ºé«˜é¢‘ç»†èŠ‚çš„äº’è¡¥è¡Œä¸ºã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜æ–‡æœ¬åµŒå…¥æ‰°åŠ¨æ˜¯ä¸€ç§æœ‰æ•ˆçš„éšæœºæ€§å½¢å¼ï¼Œèƒ½å¤Ÿä¸ç©ºé—´å™ªå£°è€¦åˆä»¥å¢å¼ºç”Ÿæˆå¤šæ ·æ€§å’Œè´¨é‡ï¼Œä¸ºæ‰©æ•£æ¨¡å‹ä¸­çš„æµ‹è¯•æ—¶ä¼˜åŒ–æä¾›äº†æ–°çš„éšæœºæ€§åˆ©ç”¨è§†è§’ï¼Œå¹¶æ­ç¤ºäº†ä¸åŒç»´åº¦å¯¹æ‰°åŠ¨å…·æœ‰ä¸åŒå®¹å¿åº¦çš„ç‰¹æ€§ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.</p>
<h3 id="29-towards-object-centric-understanding-for-instructional-videos">[29] <a href="https://arxiv.org/abs/2512.03479">Towards Object-centric Understanding for Instructional Videos</a></h3>
<p><em>Wenliang Guo, Yu Kong</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘å¯¹è±¡çš„è§†é¢‘ç†è§£èŒƒå¼ï¼Œå°†åŠ¨ä½œè§†ä¸ºé©±åŠ¨çŠ¶æ€è½¬æ¢çš„æœºåˆ¶ï¼Œå¹¶å¼•å…¥äº†Object-IVQAåŸºå‡†æµ‹è¯•å’Œä»£ç†æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†ç¨‹åºæ€§æ´»åŠ¨ç†è§£ä¸­çš„å¯¹è±¡çº§æ¨ç†èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ä»¥åŠ¨ä½œä¸ºä¸­å¿ƒçš„æ–¹æ³•éš¾ä»¥å¤„ç†ç°å®ç¨‹åºä¸­çš„çµæ´»æ€§ï¼Œå…¶ä¸­æ­¥éª¤é¡ºåºä¼šæ ¹æ®å¯¹è±¡çŠ¶æ€è€Œå˜åŒ–ã€‚æœ¬æ–‡æ—¨åœ¨å°†ç ”ç©¶ç„¦ç‚¹è½¬å‘ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„èŒƒå¼ï¼Œå°†åŠ¨ä½œè§†ä¸ºé©±åŠ¨çŠ¶æ€è½¬æ¢çš„æœºåˆ¶ï¼Œä»¥è§£å†³ç¨‹åºæ€§æ´»åŠ¨ç†è§£ä¸­çš„è¿™ä¸€å…³é”®é™åˆ¶ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡å¼•å…¥äº†Object-IVQAåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«107ä¸ªé•¿æ ¼å¼æ•™å­¦è§†é¢‘å’Œ514ä¸ªå¸¦æœ‰æ—¶é—´é”šå®šè¯æ®çš„å¼€æ”¾å¼é—®ç­”å¯¹ï¼Œè¯„ä¼°å¯¹è±¡ä¸­å¿ƒæ¨ç†çš„å››ä¸ªç»´åº¦ã€‚åŒæ—¶æå‡ºäº†ä¸€ä¸ªä»£ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åè°ƒå¯¹è±¡ä¸­å¿ƒè§„åˆ’ã€æ„ŸçŸ¥ã€åˆ†æå’Œç”Ÿæˆå·¥å…·ï¼Œæ”¯æŒæ˜¾å¼è¯æ®æ£€ç´¢å’Œè·¨ä¸è¿ç»­ç‰‡æ®µçš„å¤šæ¬¡æ¨ç†ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¯¹è±¡çº§è¯†åˆ«å’Œæ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œè€Œæœ¬æ–‡æå‡ºçš„æ¡†æ¶å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚Object-IVQAåŸºå‡†æµ‹è¯•è¯„ä¼°äº†çŠ¶æ€æ¼”åŒ–ã€å‰ææ¡ä»¶éªŒè¯ã€åäº‹å®æ¨ç†å’Œé”™è¯¯è¯†åˆ«å››ä¸ªæ¨ç†ç»´åº¦ï¼Œä¸ºå¯¹è±¡ä¸­å¿ƒç†è§£æä¾›äº†å…¨é¢çš„è¯„ä¼°æ ‡å‡†ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†å¯¹è±¡ä¸­å¿ƒèŒƒå¼åœ¨ç¨‹åºæ€§æ´»åŠ¨ç†è§£ä¸­çš„é‡è¦æ€§ï¼Œæå‡ºçš„æ¡†æ¶é€šè¿‡æ˜¾å¼è¯æ®æ£€ç´¢å’Œå¤šè·³æ¨ç†æœºåˆ¶æœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚Object-IVQAåŸºå‡†ä¸ºæœªæ¥è¾…åŠ©AIç³»ç»Ÿçš„å‘å±•æä¾›äº†å…³é”®çš„è¯„ä¼°å·¥å…·ï¼Œæ¨åŠ¨äº†å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡æ¨ç†èƒ½åŠ›çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Understanding procedural activities is crucial for developing future assistive AI that can reason about complex real-world tasks. Existing action-centric methods struggle with the flexibility of real procedures, where step order varies depending on object states. In this work, we propose to shift the focus to an object-centric paradigm by regarding actions as mechanisms that drive state transitions. To advance this direction, we introduce Object-IVQA, a long-form instructional video benchmark with 107 videos and 514 open-ended question-answer pairs annotated with temporally grounded evidence. The benchmark evaluates four dimensions of object-centric reasoning, including state evolution, precondition verification, counterfactual reasoning and mistake recognition. We further propose an agent framework that orchestrates object-centric planning, perception, analysis and generation tools, enabling explicit evidence retrieval and multi-hop reasoning across disjoint segments. Experiments show that existing large vision-language models struggle in object-level recognition and reasoning, whereas our framework achieves substantially improvement.</p>
<h3 id="30-divide-then-ground-adapting-frame-selection-to-query-types-for-long-form-video-understanding">[30] <a href="https://arxiv.org/abs/2512.04000">Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding</a></h3>
<p><em>Jialuo Li, Bin Li, Jiahao Li, Yan Lu</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDIGæ¡†æ¶ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„è§†é¢‘å¸§é€‰æ‹©æ–¹æ³•ï¼Œæ ¹æ®æŸ¥è¯¢ç±»å‹è‡ªé€‚åº”åœ°é‡‡ç”¨å‡åŒ€é‡‡æ ·æˆ–æŸ¥è¯¢æ„ŸçŸ¥é€‰æ‹©ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†é•¿è§†é¢‘ç†è§£ä¸­å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é•¿è§†é¢‘ç†è§£ä¸­å¤§å‹å¤šæ¨¡æ€æ¨¡å‹é¢ä¸´ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶å’Œå¯†é›†è§†é¢‘æ ‡è®°å¤„ç†çš„è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œç°æœ‰æŸ¥è¯¢æ„ŸçŸ¥å¸§é€‰æ‹©æ–¹æ³•è™½ç„¶æœ‰æ•ˆä½†è®¡ç®—å¼€é”€å¤§ï¼Œæœ¬æ–‡æ—¨åœ¨éªŒè¯å¤æ‚æœç´¢æœºåˆ¶æ˜¯å¦åœ¨æ‰€æœ‰æŸ¥è¯¢åœºæ™¯ä¸‹éƒ½å¿…è¦ï¼Œå¹¶æ¢ç´¢æ›´é«˜æ•ˆçš„å¸§é€‰æ‹©ç­–ç•¥ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é¦–å…ˆå»ºç«‹å¹¶éªŒè¯äº†æŸ¥è¯¢ç±»å‹å­¦ï¼ŒåŒºåˆ†å…¨å±€æŸ¥è¯¢å’Œå±€éƒ¨åŒ–æŸ¥è¯¢ï¼ŒåŸºäºæ­¤æå‡ºDIGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é’ˆå¯¹å…¨å±€æŸ¥è¯¢é‡‡ç”¨é«˜æ•ˆçš„å‡åŒ€é‡‡æ ·ç­–ç•¥ï¼Œé’ˆå¯¹å±€éƒ¨åŒ–æŸ¥è¯¢åˆ™æ¿€æ´»ä¸“é—¨çš„æµæ°´çº¿æå–æŸ¥è¯¢ç›¸å…³å¸§ï¼Œæ•´ä¸ªæ¡†æ¶æ— éœ€è®­ç»ƒå³å¯å®ç°è‡ªé€‚åº”å¸§é€‰æ‹©ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDIGæ¡†æ¶åœ¨æ€§èƒ½ä¸ŠæŒç»­è¶…è¶Šç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå³ä½¿åœ¨è¾“å…¥å¸§æ•°æ‰©å±•åˆ°256å¸§çš„æƒ…å†µä¸‹ï¼Œä»èƒ½ç¨³å¥åœ°æå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†æŸ¥è¯¢ç±»å‹å¯¹å¸§é€‰æ‹©ç­–ç•¥çš„é‡è¦å½±å“ï¼Œè¡¨æ˜å¤æ‚æœç´¢æœºåˆ¶å¹¶éåœ¨æ‰€æœ‰åœºæ™¯ä¸‹éƒ½å¿…è¦ï¼ŒDIGæ¡†æ¶æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„è‡ªé€‚åº”è§£å†³æ–¹æ¡ˆï¼Œä¸ºé•¿è§†é¢‘ç†è§£ä¸­çš„è®¡ç®—æ•ˆç‡ä¼˜åŒ–æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.</p>
<h3 id="31-eea-exploration-exploitation-agent-for-long-video-understanding">[31] <a href="https://arxiv.org/abs/2512.03500">EEA: Exploration-Exploitation Agent for Long Video Understanding</a></h3>
<p><em>Te Yang, Xiangyu Zhu, Bo Wang, Quan Chen, Peng Jiang, Zhen Lei</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºEEAæ¡†æ¶ï¼Œä¸€ç§é€šè¿‡è¯­ä¹‰å¼•å¯¼çš„å±‚æ¬¡æ ‘æœç´¢è¿‡ç¨‹å®ç°æ¢ç´¢-åˆ©ç”¨å¹³è¡¡çš„æ–°å‹è§†é¢‘æ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºé•¿è§†é¢‘ç†è§£ä»»åŠ¡ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªä¸»å‘ç°å¹¶åŠ¨æ€æ›´æ–°ä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰æŸ¥è¯¢ï¼ŒåŒæ—¶ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹çš„å†…åœ¨å¥–åŠ±ä¸è¯­ä¹‰å…ˆéªŒï¼Œå®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„é•¿è§†é¢‘åˆ†æã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰é•¿è§†é¢‘ç†è§£æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šå¯†é›†é¢„å¤„ç†å¯¼è‡´ä¸¥é‡è®¡ç®—å¼€é”€ï¼Œä»¥åŠæ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡ä¸å½“å¯¼è‡´ä¿¡æ¯è¦†ç›–ä¸å®Œæ•´å’Œæ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆè®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œè¦ä¹ˆæ— æ³•æœ‰æ•ˆå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œé™åˆ¶äº†é•¿è§†é¢‘åˆ†æçš„å®ç”¨æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> EEAæ¡†æ¶é€šè¿‡è¯­ä¹‰å¼•å¯¼çš„å±‚æ¬¡æ ‘æœç´¢è¿‡ç¨‹å®ç°æ¢ç´¢-åˆ©ç”¨å¹³è¡¡ï¼Œè‡ªä¸»å‘ç°å¹¶åŠ¨æ€æ›´æ–°ä»»åŠ¡ç›¸å…³è¯­ä¹‰æŸ¥è¯¢ï¼Œæ”¶é›†ä¸è¿™äº›æŸ¥è¯¢ç´§å¯†åŒ¹é…çš„è§†é¢‘å¸§ä½œä¸ºè¯­ä¹‰é”šç‚¹ã€‚åœ¨æ ‘æœç´¢è¿‡ç¨‹ä¸­ï¼ŒEEAä¼˜å…ˆæ¢ç´¢è¯­ä¹‰ç›¸å…³å¸§ï¼ŒåŒæ—¶ç¡®ä¿æœªçŸ¥æ®µçš„å……åˆ†è¦†ç›–ï¼Œå¹¶é€šè¿‡æ˜¾å¼å»ºæ¨¡ä¸ç¡®å®šæ€§å°†è§†è§‰è¯­è¨€æ¨¡å‹çš„å†…åœ¨å¥–åŠ±ä¸è¯­ä¹‰å…ˆéªŒè‡ªé€‚åº”ç»“åˆï¼Œå®ç°å¯¹è§†é¢‘æ®µçš„ç¨³å®šç²¾ç¡®è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªé•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚EEAæ¡†æ¶åœ¨ä¿æŒé«˜æ•ˆè®¡ç®—çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†é•¿è§†é¢‘ç†è§£ä»»åŠ¡çš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†å…¶åœ¨å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡è¯­ä¹‰å¼•å¯¼çš„å±‚æ¬¡æ ‘æœç´¢è¿‡ç¨‹èƒ½å¤Ÿæœ‰æ•ˆè§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„æ¢ç´¢-åˆ©ç”¨å¹³è¡¡é—®é¢˜ï¼Œä¸ºé«˜æ•ˆçš„é•¿è§†é¢‘åˆ†ææä¾›äº†æ–°æ€è·¯ã€‚EEAæ¡†æ¶çš„è‡ªé€‚åº”è¯­ä¹‰æŸ¥è¯¢æœºåˆ¶å’Œä¸ç¡®å®šæ€§å»ºæ¨¡æ–¹æ³•ä¸ºæœªæ¥è§†é¢‘æ™ºèƒ½ä½“è®¾è®¡æä¾›äº†é‡è¦å‚è€ƒï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡è§†è§‰æ•°æ®æ—¶å®ç°è®¡ç®—æ•ˆç‡ä¸ä¿¡æ¯è¦†ç›–çš„å¹³è¡¡ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.</p>
<h3 id="32-psa-pyramid-sparse-attention-for-efficient-video-understanding-and-generation">[32] <a href="https://arxiv.org/abs/2512.04025">PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</a></h3>
<p><em>Xiaolong Li, Youping Gu, Xi Lin, Weijie Wang, Bohan Zhuang</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºé‡‘å­—å¡”ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡å¤šçº§æ± åŒ–é”®å€¼è¡¨ç¤ºæ›¿ä»£ä¼ ç»ŸäºŒå€¼æ©ç ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘é«˜ç¨€ç–åº¦ä¸‹çš„ä¿¡æ¯æŸå¤±ï¼Œé€‚ç”¨äºè§†é¢‘ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ³¨æ„åŠ›æœºåˆ¶ä½œä¸ºåŸºç¡€æ¨¡å‹çš„æ ¸å¿ƒï¼Œå…¶äºŒæ¬¡å¤æ‚åº¦æ˜¯æ‰©å±•çš„å…³é”®ç“¶é¢ˆã€‚ç°æœ‰é«˜æ•ˆæ³¨æ„åŠ›æ–¹æ³•é€šå¸¸é‡‡ç”¨ç¨€ç–åŒ–èŒƒå¼ï¼Œä½†å½“å‰æ–¹æ³•ä½¿ç”¨äºŒå€¼æ©ç ä¿ç•™æˆ–ä¸¢å¼ƒæ•´ä¸ªé”®å€¼å—ï¼Œåœ¨é«˜ç¨€ç–åº¦ä¸‹ä¼šå¯¼è‡´æ˜¾è‘—çš„ä¿¡æ¯æŸå¤±ï¼Œè¿™ä¸€ç¼ºé™·äºŸå¾…è§£å†³ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºé‡‘å­—å¡”ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œé‡‡ç”¨å¤šçº§æ± åŒ–é”®å€¼è¡¨ç¤ºæ›¿ä»£ä¼ ç»ŸäºŒå€¼æ©ç ã€‚æ¯ä¸ªæŸ¥è¯¢å—åŠ¨æ€åˆ†é…è¾ƒä½çš„æ± åŒ–çº§åˆ«ç»™å…³é”®é”®å€¼å—ï¼Œè¾ƒé«˜çš„æ± åŒ–çº§åˆ«ç»™æ¬¡è¦é”®å€¼å—ï¼Œåœ¨å®Œå…¨ä¿ç•™å’Œå®Œå…¨å‰ªæä¹‹é—´åˆ›å»ºä¿¡æ¯æ€§æ’å€¼ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è§£è€¦çš„å—-ç“¦ç‰‡è®¾è®¡ï¼Œç¡®ä¿ç¡¬ä»¶å‹å¥½çš„é«˜æ•ˆæ‰§è¡Œã€‚</p>
<p><strong>Result:</strong> åœ¨è§†é¢‘ç†è§£å’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPSAåœ¨ä¿æŒä¸Šä¸‹æ–‡ä¿¡æ¯å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå§‹ç»ˆä¼˜äºæˆ–è¾¾åˆ°ä¸ç°æœ‰ç¨€ç–æ³¨æ„åŠ›åŸºçº¿ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å±•ç°å‡ºæ›´ä¼˜çš„æ•ˆç‡-è´¨é‡æƒè¡¡ã€‚è¯¥æ–¹æ³•åœ¨ä½è®¡ç®—é¢„ç®—ä¸‹æœ‰æ•ˆç¼“è§£ä¿¡æ¯æŸå¤±ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> é‡‘å­—å¡”ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡å¤šçº§æ± åŒ–è¡¨ç¤ºæä¾›äº†ç¨€ç–æ³¨æ„åŠ›è®¾è®¡çš„æ–°èŒƒå¼ï¼Œç±»æ¯”äºå®šç‚¹é‡åŒ–å’Œè®¡ç®—æœºè§†è§‰ä¸­çš„ç»å…¸ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘é«˜ç¨€ç–åº¦ä¸‹çš„ä¿¡æ¯æŸå¤±ï¼Œä¸ºé«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶çš„å‘å±•æä¾›äº†é‡è¦æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA</p>
<h3 id="33-exploiting-domain-properties-in-language-driven-domain-generalization-for-semantic-segmentation">[33] <a href="https://arxiv.org/abs/2512.03508">Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation</a></h3>
<p><em>Seogkyu Jeon, Kibeom Hong, Hyeran Byun</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDPMFormerï¼Œä¸€ç§ç”¨äºé¢†åŸŸæ³›åŒ–è¯­ä¹‰åˆ†å‰²çš„æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡é¢†åŸŸæ„ŸçŸ¥æç¤ºå­¦ä¹ å’Œä¸€è‡´æ€§å­¦ä¹ è§£å†³è§†è§‰-æ–‡æœ¬è¯­ä¹‰é”™é…é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„é¢†åŸŸæ³›åŒ–è¯­ä¹‰åˆ†å‰²æ–¹æ³•å¿½è§†äº†è§†è§‰ä¸æ–‡æœ¬ä¸Šä¸‹æ–‡ä¹‹é—´çš„è¯­ä¹‰é”™é…é—®é¢˜ï¼Œè¿™ç§é”™é…æºäºåœ¨å•ä¸€æºåŸŸä¸Šå­¦ä¹ çš„å›ºå®šä¸Šä¸‹æ–‡æç¤ºçš„åˆšæ€§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨è·¨åŸŸæ³›åŒ–æ—¶æ€§èƒ½å—é™ã€‚</p>
<p><strong>Method:</strong> æå‡ºé¢†åŸŸæ„ŸçŸ¥æç¤ºé©±åŠ¨çš„æ©ç å˜æ¢å™¨æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¢†åŸŸæ„ŸçŸ¥æç¤ºå­¦ä¹ ä»¥ä¿ƒè¿›è§†è§‰ä¸æ–‡æœ¬çº¿ç´¢çš„è¯­ä¹‰å¯¹é½ï¼›ç»“åˆçº¹ç†æ‰°åŠ¨çš„é¢†åŸŸæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ä»¥å¤šæ ·åŒ–å¯è§‚æµ‹é¢†åŸŸï¼›ä»¥åŠé¢†åŸŸé²æ£’ä¸€è‡´æ€§å­¦ä¹ ä»¥æœ€å°åŒ–åŸå§‹å›¾åƒä¸å¢å¼ºå›¾åƒé¢„æµ‹é—´çš„å·®å¼‚ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è¯¥æ¡†æ¶åœ¨å¤šä¸ªé¢†åŸŸæ³›åŒ–è¯­ä¹‰åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œé€šè¿‡ç³»ç»Ÿåˆ†æéªŒè¯äº†å„ç»„ä»¶å¯¹æå‡è·¨åŸŸæ³›åŒ–èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†è§£å†³è§†è§‰-æ–‡æœ¬è¯­ä¹‰é”™é…å¯¹é¢†åŸŸæ³›åŒ–çš„é‡è¦æ€§ï¼Œæå‡ºçš„å¤šç»„ä»¶æ¡†æ¶ä¸ºæ„å»ºå¯¹ç¯å¢ƒå˜åŒ–å…·æœ‰é²æ£’æ€§çš„è¯­ä¹‰åˆ†å‰²ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œå¹¶ä¸ºæœªæ¥è·¨åŸŸè§†è§‰ç†è§£ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.</p>
<h3 id="34-opentrack3d-towards-accurate-and-generalizable-open-vocabulary-3d-instance-segmentation">[34] <a href="https://arxiv.org/abs/2512.03532">OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation</a></h3>
<p><em>Zhishan Zhou, Siyuan Wei, Zengran Wang, Chunjie Wang, Xiaosheng Yan, Xiao Liu</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†OpenTrack3Dï¼Œä¸€ä¸ªç”¨äºå¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²çš„é€šç”¨ä¸”å‡†ç¡®çš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨çº¿è§†è§‰-ç©ºé—´è·Ÿè¸ªå™¨æ„å»ºè·¨è§†è§’ä¸€è‡´çš„å¯¹è±¡æè®®ï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¢å¼ºç»„åˆæ¨ç†èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®é™åˆ¶ï¼šä¸€æ˜¯æè®®ç”Ÿæˆä¾èµ–äºæ•°æ®é›†ç‰¹å®šçš„æè®®ç½‘ç»œæˆ–åŸºäºç½‘æ ¼çš„è¶…ç‚¹ï¼Œä½¿å…¶åœ¨æ— ç½‘æ ¼åœºæ™¯ä¸­ä¸é€‚ç”¨ä¸”é™åˆ¶äº†å‘æ–°åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ï¼›äºŒæ˜¯åŸºäºCLIPçš„åˆ†ç±»å™¨æ–‡æœ¬æ¨ç†èƒ½åŠ›è¾ƒå¼±ï¼Œéš¾ä»¥å¤„ç†ç»„åˆæ€§å’ŒåŠŸèƒ½æ€§çš„ç”¨æˆ·æŸ¥è¯¢ã€‚</p>
<p><strong>Method:</strong> OpenTrack3Dé‡‡ç”¨åœ¨çº¿è§†è§‰-ç©ºé—´è·Ÿè¸ªå™¨æ„å»ºè·¨è§†è§’ä¸€è‡´çš„å¯¹è±¡æè®®ï¼Œé¦–å…ˆåˆ©ç”¨2Då¼€æ”¾è¯æ±‡åˆ†å‰²å™¨ç”Ÿæˆæ©ç å¹¶é€šè¿‡æ·±åº¦ä¿¡æ¯æå‡åˆ°3Dç‚¹äº‘ï¼Œç„¶åä½¿ç”¨DINOç‰¹å¾å›¾æå–æ©ç å¼•å¯¼çš„å®ä¾‹ç‰¹å¾ï¼Œè·Ÿè¸ªå™¨èåˆè§†è§‰å’Œç©ºé—´çº¿ç´¢ä»¥ä¿æŒå®ä¾‹ä¸€è‡´æ€§ã€‚è¯¥æ ¸å¿ƒæµç¨‹å®Œå…¨æ— ç½‘æ ¼ï¼Œä½†æä¾›äº†å¯é€‰çš„è¶…ç‚¹ç»†åŒ–æ¨¡å—ä»¥åœ¨åœºæ™¯ç½‘æ ¼å¯ç”¨æ—¶è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼ŒåŒæ—¶ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ›¿ä»£CLIPä»¥å¢å¼ºå¤æ‚æŸ¥è¯¢çš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨ScanNet200ã€Replicaã€ScanNet++å’ŒSceneFun3Dç­‰å¤šä¸ªå¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒOpenTrack3Då®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶åœ¨æ— ç½‘æ ¼åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯¹å¤æ‚ç”¨æˆ·æŸ¥è¯¢çš„å‡†ç¡®ç†è§£ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åœ¨çº¿è§†è§‰-ç©ºé—´è·Ÿè¸ªå™¨åœ¨æ„å»ºè·¨è§†è§’ä¸€è‡´å¯¹è±¡æè®®æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¢å¼ºç»„åˆæ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä¸ºæœºå™¨äººå­¦å’ŒAR/VRåº”ç”¨ä¸­çš„å¼€æ”¾è¯æ±‡3Då®ä¾‹åˆ†å‰²æä¾›äº†é€šç”¨ä¸”å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒäº†åœ¨æ— ç½‘æ ¼ç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>Generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments is crucial for robotics and AR/VR, yet remains a significant challenge. We attribute this to two key limitations of existing methods: (1) proposal generation relies on dataset-specific proposal networks or mesh-based superpoints, rendering them inapplicable in mesh-free scenarios and limiting generalization to novel scenes; and (2) the weak textual reasoning of CLIP-based classifiers, which struggle to recognize compositional and functional user queries. To address these issues, we introduce OpenTrack3D, a generalizable and accurate framework. Unlike methods that rely on pre-generated proposals, OpenTrack3D employs a novel visual-spatial tracker to construct cross-view consistent object proposals online. Given an RGB-D stream, our pipeline first leverages a 2D open-vocabulary segmenter to generate masks, which are lifted to 3D point clouds using depth. Mask-guided instance features are then extracted using DINO feature maps, and our tracker fuses visual and spatial cues to maintain instance consistency. The core pipeline is entirely mesh-free, yet we also provide an optional superpoints refinement module to further enhance performance when scene mesh is available. Finally, we replace CLIP with a multi-modal large language model (MLLM), significantly enhancing compositional reasoning for complex user queries. Extensive experiments on diverse benchmarks, including ScanNet200, Replica, ScanNet++, and SceneFun3D, demonstrate state-of-the-art performance and strong generalization capabilities.</p>
<h3 id="35-unicomp-rethinking-video-compression-through-informational-uniqueness">[35] <a href="https://arxiv.org/abs/2512.03575">UniComp: Rethinking Video Compression Through Informational Uniqueness</a></h3>
<p><em>Chao Yuan, Shimin Chen, Minliang Lin, Limeng Qiao, Guanglu Wan, Lin Ma</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¿¡æ¯ç‹¬ç‰¹æ€§çš„è§†é¢‘å‹ç¼©æ¡†æ¶UniCompï¼Œé€šè¿‡æœ€å°åŒ–ä¿ç•™ä»¤ç‰Œä¸å®Œæ•´ä»¤ç‰Œä¹‹é—´çš„æ¡ä»¶ç†µæ¥ä¼˜åŒ–å‹ç¼©è¿‡ç¨‹ï¼Œåœ¨æœ‰é™è®¡ç®—é¢„ç®—ä¸‹æ˜¾è‘—æå‡äº†è§†è§‰ä»¤ç‰Œçš„ä¿ç•™æ•ˆæœã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ³¨æ„åŠ›æœºåˆ¶å‹ç¼©æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œæœ¬æ–‡æ—¨åœ¨ä»ä¿¡æ¯è®ºè§†è§’å‡ºå‘ï¼Œåœ¨å—é™è®¡ç®—é¢„ç®—ä¸‹æœ€å¤§åŒ–è§†é¢‘è¡¨ç¤ºçš„ä¿¡æ¯ä¿çœŸåº¦ï¼Œè§£å†³è§†è§‰å‹ç¼©ä¸­å¦‚ä½•æœ‰æ•ˆä¿ç•™å…³é”®ä¿¡æ¯çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºä¿¡æ¯ç‹¬ç‰¹æ€§æ¦‚å¿µæ¥è¡¡é‡ä»¤ç‰Œé—´çš„å†…åœ¨å†—ä½™åº¦ï¼Œå¹¶è®¾è®¡ä¸‰ä¸ªæ¸è¿›å¼æ¨¡å—ï¼šå¸§ç»„èåˆå®ç°è¯­ä¹‰å¸§åˆ†ç»„ï¼Œä»¤ç‰Œåˆ†é…è¿›è¡Œè‡ªé€‚åº”èµ„æºåˆ†é…ï¼Œç©ºé—´åŠ¨æ€å‹ç¼©æ‰§è¡Œç»†ç²’åº¦ç©ºé—´å‹ç¼©ï¼Œå…±åŒæ„æˆUniCompæ¡†æ¶ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniCompåœ¨æœ‰é™è®¡ç®—é¢„ç®—ä¸‹æŒç»­ä¼˜äºç°æœ‰å‹ç¼©æ–¹æ³•ï¼Œåœ¨ä¿ç•™å…³é”®è§†è§‰ä»¤ç‰Œæ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†ä¿¡æ¯ç‹¬ç‰¹æ€§åœ¨ä»¤ç‰Œå‹ç¼©æ•ˆæœä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯å®äº†ä¿¡æ¯ç‹¬ç‰¹æ€§åœ¨è§†è§‰å‹ç¼©ä¸­çš„æ ¸å¿ƒä»·å€¼ï¼Œä¸ºåŸºäºä¿¡æ¯è®ºçš„å‹ç¼©æ–¹æ³•æä¾›äº†æ–°æ€è·¯ï¼Œå±•ç¤ºäº†æ¸è¿›å¼è¯­ä¹‰å‹ç¼©æ¡†æ¶åœ¨èµ„æºå—é™åœºæ™¯ä¸‹çš„ä¼˜è¶Šæ€§ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Distinct from attention-based compression methods, this paper presents an information uniqueness driven video compression framework, termed UniComp, which aims to maximize the information fidelity of video representations under constrained computational budgets. Starting from the information-theoretic perspective, we formulate the vision compression as an optimization problem that minimizes conditional entropy (reconstruction error) between retained and full tokens. To achieve this, we introduce the notion of information uniqueness to measure intrinsic redundancy among tokens to link with reconstruction error. Based on uniqueness, we design three modules-Frame Group Fusion, Token Allocation, and Spatial Dynamic Compression-that progressively perform semantic frame grouping, adaptive resource allocation, and fine-grained spatial compression. Extensive experiments demonstrate that UniComp consistently outperforms existing compression methods in preserving essential visual tokens under limited computational budgets, highlighting the pivotal role of information uniqueness in token compression efficacy.</p>
<h3 id="36-cross-stain-contrastive-learning-for-paired-immunohistochemistry-and-histopathology-slide-representation-learning">[36] <a href="https://arxiv.org/abs/2512.03577">Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning</a></h3>
<p><em>Yizhi Zhang, Lei Fan, Zhulin Tao, Donglin Di, Yang Song, Sidong Liu, Cong Cong</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†è·¨æŸ“è‰²å¯¹æ¯”å­¦ä¹ ï¼ˆCSCLï¼‰æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨æ–°æ„å»ºçš„äº”æŸ“è‰²å¯¹é½æ•°æ®é›†ï¼Œå¢å¼ºH&amp;Eå…¨åˆ‡ç‰‡å›¾åƒçš„é€šç”¨è¡¨ç¤ºèƒ½åŠ›ï¼Œä½¿å…¶èƒ½æœ‰æ•ˆæ•´åˆå¤šæŸ“è‰²ç”Ÿç‰©æ ‡å¿—ç‰©ä¿¡æ¯ï¼Œä»è€Œæå‡è®¡ç®—ç—…ç†å­¦ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è®¡ç®—ç—…ç†å­¦ä¸­ï¼Œé€šç”¨ä¸”å¯è¿ç§»çš„å…¨åˆ‡ç‰‡å›¾åƒè¡¨ç¤ºè‡³å…³é‡è¦ï¼Œå°†å…ç–«ç»„åŒ–ç­‰å¤šæŸ“è‰²ä¿¡æ¯ä¸H&amp;Eç»“åˆå¯ä¸°å¯Œç‰¹å¾è¡¨è¾¾ï¼Œä½†ç°æœ‰æ–¹æ³•å—é™äºå¯¹é½è‰¯å¥½çš„å¤šæŸ“è‰²æ•°æ®é›†ç¨€ç¼ºï¼ŒæŸ“è‰²é—´é”™ä½å¯¼è‡´ç»„ç»‡å¯¹åº”å…³ç³»ä¸ä¸€è‡´ï¼Œé˜»ç¢äº†ç¨³å®šçš„è¡¥ä¸çº§ç‰¹å¾æå–å’Œåˆ‡ç‰‡çº§åµŒå…¥è´¨é‡ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é¦–å…ˆæ„å»ºäº†åˆ‡ç‰‡çº§å¯¹é½çš„äº”æŸ“è‰²æ•°æ®é›†ï¼ˆH&amp;Eã€HER2ã€KI67ã€ERã€PGRï¼‰ï¼Œå¹¶æå‡ºäº†ä¸¤é˜¶æ®µé¢„è®­ç»ƒæ¡†æ¶CSCLï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨è½»é‡é€‚é…å™¨é€šè¿‡è¡¥ä¸çº§å¯¹æ¯”å¯¹é½å¢å¼ºH&amp;Eç‰¹å¾ä¸å¯¹åº”IHCä¸Šä¸‹æ–‡çº¿ç´¢çš„å…¼å®¹æ€§ï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨å¤šç¤ºä¾‹å­¦ä¹ è¿›è¡Œåˆ‡ç‰‡çº§è¡¨ç¤ºå­¦ä¹ ï¼ŒåŒ…å«è·¨æŸ“è‰²æ³¨æ„åŠ›èåˆæ¨¡å—æ•´åˆæŸ“è‰²ç‰¹å¼‚æ€§è¡¥ä¸ç‰¹å¾ï¼Œä»¥åŠè·¨æŸ“è‰²å…¨å±€å¯¹é½æ¨¡å—å¼ºåˆ¶ä¸åŒæŸ“è‰²é—´åˆ‡ç‰‡çº§åµŒå…¥çš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨ç™Œç—‡äºšå‹åˆ†ç±»ã€IHCç”Ÿç‰©æ ‡å¿—ç‰©çŠ¶æ€åˆ†ç±»å’Œç”Ÿå­˜é¢„æµ‹ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ€§èƒ½çš„æŒç»­æå‡ï¼Œç”Ÿæˆäº†é«˜è´¨é‡ã€å¯è¿ç§»çš„H&amp;Eåˆ‡ç‰‡çº§è¡¨ç¤ºï¼ŒéªŒè¯äº†è·¨æŸ“è‰²å¯¹æ¯”å­¦ä¹ æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡æ„å»ºå¯¹é½çš„å¤šæŸ“è‰²æ•°æ®é›†å’Œåˆ›æ–°çš„è·¨æŸ“è‰²å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼ŒæˆåŠŸè§£å†³äº†è®¡ç®—ç—…ç†å­¦ä¸­å¤šæŸ“è‰²ä¿¡æ¯æ•´åˆçš„æŒ‘æˆ˜ï¼Œä¸ºç”Ÿæˆé€šç”¨ä¸”ç”Ÿç‰©å­¦æ„ä¹‰ä¸°å¯Œçš„H&amp;Eè¡¨ç¤ºæä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€ç—…ç†å›¾åƒåˆ†æçš„å‘å±•ï¼Œç›¸å…³ä»£ç å’Œæ•°æ®å·²å¼€æºã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&amp;E enriches H&amp;E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&amp;E, HER2, KI67, ER, PGR) to enable paired H&amp;E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&amp;E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&amp;E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.</p>
<h3 id="37-dynamic-optical-test-for-bot-identification-dot-bi-a-simple-check-to-identify-bots-in-surveys-and-online-processes">[37] <a href="https://arxiv.org/abs/2512.03580">Dynamic Optical Test for Bot Identification (DOT-BI): A simple check to identify bots in surveys and online processes</a></h3>
<p><em>Malte Bleeker, Mauro Gotsch</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DOT-BIï¼ˆåŠ¨æ€å…‰å­¦æµ‹è¯•æœºå™¨äººè¯†åˆ«ï¼‰ï¼Œä¸€ç§åˆ©ç”¨äººç±»è¿åŠ¨æ„ŸçŸ¥èƒ½åŠ›åŒºåˆ†äººç±»ä¸è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„å¿«é€Ÿæ–¹æ³•ï¼Œé€šè¿‡åœ¨åŠ¨æ€èƒŒæ™¯çº¹ç†ä¸­éšè—æ•°å­—ï¼Œä½¿å…¶ä»…å¯¹äººç±»å¯è§è€Œå¯¹ç®—æ³•ä¸å¯è§ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åœ¨çº¿è°ƒæŸ¥å’Œæµç¨‹ä¸­ç¼ºä¹æœ‰æ•ˆåŒºåˆ†äººç±»å—è®¿è€…ä¸è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„æ–¹æ³•ï¼Œç°æœ‰éªŒè¯æœºåˆ¶å®¹æ˜“è¢«å…ˆè¿›AIæ¨¡å‹ç»•è¿‡ï¼Œéœ€è¦å¼€å‘åŸºäºäººç±»ç‹¬ç‰¹æ„ŸçŸ¥èƒ½åŠ›çš„éªŒè¯æŠ€æœ¯ã€‚</p>
<p><strong>Method:</strong> DOT-BIé‡‡ç”¨åŠ¨æ€å…‰å­¦æµ‹è¯•æ–¹æ³•ï¼Œå°†æ•°å­—ä»¥ä¸èƒŒæ™¯ç›¸åŒçš„éšæœºé»‘ç™½åƒç´ çº¹ç†æ˜¾ç¤ºï¼Œä»…é€šè¿‡æ•°å­—ä¸èƒŒæ™¯ä¹‹é—´çš„è¿åŠ¨å’Œå°ºåº¦å·®å¼‚ä½¿æ•°å­—å¯¹äººç±»å¯è§ï¼Œè€Œé€å¸§ç®—æ³•å¤„ç†æ— æ³•æå–æœ‰æ„ä¹‰ä¿¡å·ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°æ˜¾ç¤ºæœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹ï¼ˆGPT-5-Thinkingå’ŒGemini 2.5 Proï¼‰æ— æ³•æ­£ç¡®æå–æ•°å€¼ï¼›åœ¨çº¿è°ƒæŸ¥ä¸­99.5%å‚ä¸è€…æˆåŠŸå®Œæˆä»»åŠ¡ï¼Œå¹³å‡å®Œæˆæ—¶é—´10.7ç§’ï¼›å®éªŒå®¤ç ”ç©¶æœªå‘ç°ç›¸å¯¹äºå¯¹ç…§ç»„çš„æ˜“ç”¨æ€§æˆ–å®Œæˆæ—¶é—´è´Ÿé¢å½±å“ã€‚</p>
<p><strong>Conclusion:</strong> DOT-BIé€šè¿‡åˆ©ç”¨äººç±»è¿åŠ¨æ„ŸçŸ¥çš„ç‹¬ç‰¹èƒ½åŠ›æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æœºå™¨äººè¯†åˆ«è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ³•å¯¹ç”¨æˆ·å‹å¥½ä¸”èƒ½æŠµæŠ—å…ˆè¿›AIæ”»å‡»ï¼Œä¸ºåœ¨çº¿éªŒè¯ç³»ç»Ÿæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘å’Œå®è·µå·¥å…·ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>We propose the Dynamic Optical Test for Bot Identification (DOT-BI): a quick and easy method that uses human perception of motion to differentiate between human respondents and automated systems in surveys and online processes. In DOT-BI, a 'hidden' number is displayed with the same random black-and-white pixel texture as its background. Only the difference in motion and scale between the number and the background makes the number perceptible to humans across frames, while frame-by-frame algorithmic processing yields no meaningful signal. We conducted two preliminary assessments. Firstly, state-of-the-art, video-capable, multimodal models (GPT-5-Thinking and Gemini 2.5 Pro) fail to extract the correct value, even when given explicit instructions about the mechanism. Secondly, in an online survey (n=182), 99.5% (181/182) of participants solved the task, with an average end-to-end completion time of 10.7 seconds; a supervised lab study (n=39) found no negative effects on perceived ease-of-use or completion time relative to a control. We release code to generate tests and 100+ pre-rendered variants to facilitate adoption in surveys and online processes.</p>
<h3 id="38-beyond-boundary-frames-audio-visual-semantic-guidance-for-context-aware-video-interpolation">[38] <a href="https://arxiv.org/abs/2512.03590">Beyond Boundary Frames: Audio-Visual Semantic Guidance for Context-Aware Video Interpolation</a></h3>
<p><em>Yuchen Deng, Xiuyang Wu, Hai-Tao Zheng, Jie Wang, Feidiao Yang, Yuxing Han</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†BBFï¼ˆBeyond Boundary Framesï¼‰ï¼Œä¸€ä¸ªä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†é¢‘å¸§æ’å€¼æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡éŸ³é¢‘/è§†è§‰è¯­ä¹‰è¿›è¡Œå¼•å¯¼ï¼Œåœ¨é€šç”¨æ’å€¼å’ŒéŸ³é¢‘è§†è§‰åŒæ­¥æ’å€¼ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†ä¸“é—¨çš„å…ˆè¿›æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘å¸§æ’å€¼æ–¹æ³•åœ¨å¤„ç†å¿«é€Ÿã€å¤æ‚ä¸”é«˜åº¦éçº¿æ€§çš„è¿åŠ¨æ¨¡å¼æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£åŸºæ–¹æ³•è™½ç„¶æ”¹è¿›äº†ä¼ ç»Ÿå…‰æµæ–¹æ³•ï¼Œä½†ä»éš¾ä»¥è¦†ç›–å¤šæ ·åŒ–åº”ç”¨åœºæ™¯ï¼Œä¸”åœ¨éŸ³é¢‘è§†è§‰åŒæ­¥æ’å€¼ç­‰ç»†ç²’åº¦è¿åŠ¨ä»»åŠ¡ä¸­ç»å¸¸æ— æ³•ç”Ÿæˆæ¸…æ™°ã€æ—¶é—´ä¸€è‡´çš„å¸§ã€‚</p>
<p><strong>Method:</strong> BBFæ¡†æ¶é‡‡ç”¨å¢å¼ºçš„è¾“å…¥è®¾è®¡ï¼Œèƒ½å¤Ÿçµæ´»å¤„ç†æ–‡æœ¬ã€éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘ç­‰å¤šç§æ¡ä»¶æ¨¡æ€ï¼›æå‡ºè§£è€¦çš„å¤šæ¨¡æ€èåˆæœºåˆ¶ï¼Œå°†ä¸åŒæ¡ä»¶ä¿¡å·é¡ºåºæ³¨å…¥DiTéª¨å¹²ç½‘ç»œï¼›é‡‡ç”¨æ¸è¿›å¤šé˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨èµ·å§‹-ç»“æŸå¸§å·®å¼‚åµŒå…¥åŠ¨æ€è°ƒæ•´æ•°æ®é‡‡æ ·å’ŒæŸå¤±æƒé‡ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒBBFåœ¨é€šç”¨æ’å€¼å’ŒéŸ³é¢‘è§†è§‰åŒæ­¥æ’å€¼ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†ä¸“é—¨çš„å…ˆè¿›æ–¹æ³•ï¼Œåœ¨åè°ƒå¤šé€šé“æ¡ä»¶ä¸‹å»ºç«‹äº†ç»Ÿä¸€çš„è§†é¢‘å¸§æ’å€¼æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´æ¸…æ™°ã€æ—¶é—´ä¸€è‡´çš„å¸§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œå¤šæ¨¡æ€å¼•å¯¼åœ¨è§†é¢‘å¸§æ’å€¼ä¸­çš„é‡è¦æ€§ï¼ŒBBFæ¡†æ¶é€šè¿‡çµæ´»çš„è¾“å…¥è®¾è®¡ã€è§£è€¦èåˆæœºåˆ¶å’Œæ¸è¿›è®­ç»ƒç­–ç•¥ï¼ŒæˆåŠŸè§£å†³äº†å¤šæ ·åŒ–åº”ç”¨åœºæ™¯ä¸‹çš„æ’å€¼æŒ‘æˆ˜ï¼Œä¸ºå¤šæ¡ä»¶è§†é¢‘ç”Ÿæˆæä¾›äº†ç»Ÿä¸€è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>Handling fast, complex, and highly non-linear motion patterns has long posed challenges for video frame interpolation. Although recent diffusion-based approaches improve upon traditional optical-flow-based methods, they still struggle to cover diverse application scenarios and often fail to produce sharp, temporally consistent frames in fine-grained motion tasks such as audio-visual synchronized interpolation. To address these limitations, we introduce BBF (Beyond Boundary Frames), a context-aware video frame interpolation framework, which could be guided by audio/visual semantics. First, we enhance the input design of the interpolation model so that it can flexibly handle multiple conditional modalities, including text, audio, images, and video. Second, we propose a decoupled multimodal fusion mechanism that sequentially injects different conditional signals into a DiT backbone. Finally, to maintain the generation abilities of the foundation model, we adopt a progressive multi-stage training paradigm, where the start-end frame difference embedding is used to dynamically adjust both the data sampling and the loss weighting. Extensive experimental results demonstrate that BBF outperforms specialized state-of-the-art methods on both generic interpolation and audio-visual synchronized interpolation tasks, establishing a unified framework for video frame interpolation under coordinated multi-channel conditioning.</p>
<h3 id="39-colon-x-advancing-intelligent-colonoscopy-from-multimodal-understanding-to-clinical-reasoning">[39] <a href="https://arxiv.org/abs/2512.03667">Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning</a></h3>
<p><em>Ge-Peng Ji, Jingyi Liu, Deng-Ping Fan, Nick Barnes</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†Colon-Xå¼€æ”¾è®¡åˆ’ï¼Œæ„å»ºäº†æœ€å…¨é¢çš„ç»“è‚ é•œå¤šæ¨¡æ€æ•°æ®é›†ColonVQAï¼Œå¹¶å¼€å‘äº†é¦–ä¸ªR1é£æ ¼æ¨¡å‹ColonR1ï¼Œé€šè¿‡ä»»åŠ¡è‡ªé€‚åº”å¥–åŠ±å’Œæ¢¯åº¦ç¨³å®šä¼˜åŒ–æŠ€æœ¯ï¼Œåœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹å®ç°äº†ä»å¤šæ¨¡æ€ç†è§£åˆ°ä¸´åºŠæ¨ç†çš„è½¬å˜ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ç»“è‚ é•œå¤šæ¨¡æ€æ™ºèƒ½ä»ç†è§£åˆ°ä¸´åºŠæ¨ç†çš„å…³é”®è¿‡æ¸¡é—®é¢˜ï¼Œç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠè¾“å‡ºæ–¹é¢ç¼ºä¹é²æ£’æ€§å’Œå¯ä¿¡åº¦ï¼Œéœ€è¦å¼€å‘ä¸“é—¨é’ˆå¯¹ç»“è‚ é•œçš„æ¨ç†ä¸­å¿ƒæ™ºèƒ½ç³»ç»Ÿã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é¦–å…ˆæ„å»ºäº†åŒ…å«110ä¸‡+è§†è§‰é—®ç­”æ¡ç›®çš„ColonVQAå¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ¶µç›–76ä¸ªä¸´åºŠå‘ç°å’Œ18ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼›éšåé€šè¿‡å¤šä¸“å®¶è¾©è®ºæµç¨‹æ ‡æ³¨äº†ColonReasonä¸´åºŠæ¨ç†æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†ColonR1æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ä»»åŠ¡è‡ªé€‚åº”å¥–åŠ±å’Œæ¢¯åº¦ç¨³å®šä¼˜åŒ–æŠ€æœ¯ï¼Œæ˜¯é¦–ä¸ªR1é£æ ¼çš„ç»“è‚ é•œæ¨ç†æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> ç³»ç»Ÿè¯„ä¼°äº†22ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’ŒæŠ—å¹²æ‰°æ€§ï¼Œå‘ç°ç°æœ‰æ¨¡å‹çš„ä¸´åºŠè¾“å‡ºè¿œæœªè¾¾åˆ°é²æ£’å¯ä¿¡ï¼›åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹ï¼ŒColonR1æ¨¡å‹å®ç°äº†56.61%çš„æ•´ä½“å‡†ç¡®ç‡ï¼Œæ¯”ç›‘ç£å¾®è°ƒæ–¹æ³•æå‡äº†25.22%ï¼Œä¸ºå¤šæ¨¡æ€ç»“è‚ é•œåˆ†æè®¾ç«‹äº†æ–°çš„æ¨ç†åŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æˆåŠŸå®ç°äº†ä»å¤šæ¨¡æ€ç†è§£åˆ°ä¸´åºŠæ¨ç†çš„è½¬å˜ï¼Œä¸ºç»“è‚ é•œæ™ºèƒ½åˆ†ææä¾›äº†æ•°æ®åŸºç¡€å’Œæ¨¡å‹åŸºå‡†ï¼ŒColonR1åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹çš„ä¼˜å¼‚è¡¨ç°å±•ç¤ºäº†ä»»åŠ¡è‡ªé€‚åº”å¥–åŠ±å’Œæ¢¯åº¦ç¨³å®šä¼˜åŒ–æŠ€æœ¯çš„æœ‰æ•ˆæ€§ï¼Œæ‰€æœ‰æ•°æ®å’Œæ¨¡å‹èµ„æºå·²å…¬å¼€ä»¥ä¿ƒè¿›ç¤¾åŒºå‘å±•ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.</p>
<h3 id="40-gaussianblender-instant-stylization-of-3d-gaussians-with-disentangled-latent-spaces">[40] <a href="https://arxiv.org/abs/2512.03683">GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces</a></h3>
<p><em>Melis Ocal, Xiaoyan Xing, Yue Li, Ngo Anh Vien, Sezer Karaoglu, Theo Gevers</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†GaussianBlenderï¼Œä¸€ç§ç”¨äºæ–‡æœ¬é©±åŠ¨3Dé£æ ¼åŒ–çš„å‰é¦ˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°å³æ—¶æ¨ç†ç¼–è¾‘ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•éœ€è¦é€èµ„äº§ä¼˜åŒ–å’Œå­˜åœ¨å¤šè§†è§’ä¸ä¸€è‡´æ€§çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–‡æœ¬åˆ°3Dé£æ ¼åŒ–æ–¹æ³•é€šå¸¸ä»2Då›¾åƒç¼–è¾‘å™¨è’¸é¦è€Œæ¥ï¼Œéœ€è¦è€—æ—¶çš„é€èµ„äº§ä¼˜åŒ–ï¼Œå¹¶ä¸”ç”±äºå½“å‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å±€é™æ€§è€Œè¡¨ç°å‡ºå¤šè§†è§’ä¸ä¸€è‡´æ€§ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨å¤§è§„æ¨¡ç”Ÿäº§ä¸­ä¸åˆ‡å®é™…ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•ä»ç©ºé—´åˆ†ç»„çš„3Dé«˜æ–¯ä¸­å­¦ä¹ ç»“æ„åŒ–ã€è§£è€¦çš„æ½œåœ¨ç©ºé—´ï¼Œå®ç°å‡ ä½•å’Œå¤–è§‚çš„å—æ§ä¿¡æ¯å…±äº«ï¼Œç„¶åä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨è¿™äº›å­¦ä¹ åˆ°çš„è¡¨ç¤ºä¸Šåº”ç”¨æ–‡æœ¬æ¡ä»¶ç¼–è¾‘ã€‚</p>
<p><strong>Result:</strong> ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒGaussianBlenderä¸ä»…èƒ½å¤Ÿå®ç°å³æ—¶ã€é«˜ä¿çœŸã€å‡ ä½•ä¿æŒã€å¤šè§†è§’ä¸€è‡´çš„é£æ ¼åŒ–ï¼Œè€Œä¸”è¶…è¶Šäº†éœ€è¦é€å®ä¾‹æµ‹è¯•æ—¶ä¼˜åŒ–çš„æ–¹æ³•ï¼Œè§£é”äº†å®ç”¨çš„å¤§è§„æ¨¡æ°‘ä¸»åŒ–3Dé£æ ¼åŒ–ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§åˆ›æ–°çš„å‰é¦ˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°å³æ—¶3Dé£æ ¼åŒ–ç¼–è¾‘ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤§è§„æ¨¡ç”Ÿäº§ä¸­çš„å±€é™æ€§ï¼Œä¸ºæ¸¸æˆå¼€å‘ã€è™šæ‹Ÿç°å®å’Œæ•°å­—è‰ºæœ¯é¢†åŸŸçš„3Dèµ„äº§åˆ›ä½œæä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.</p>
<h3 id="41-active-visual-perception-opportunities-and-challenges">[41] <a href="https://arxiv.org/abs/2512.03687">Active Visual Perception: Opportunities and Challenges</a></h3>
<p><em>Yian Li, Xiaoyu Guo, Hao Zhang, Shuiwang Li, Xiaowei Dai</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡å¯¹ä¸»åŠ¨è§†è§‰æ„ŸçŸ¥è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œæ¢è®¨äº†å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­é€šè¿‡æ„ŸçŸ¥ä¸è¡ŒåŠ¨äº¤äº’è·å–ä¿¡æ¯çš„èƒ½åŠ›ï¼Œç³»ç»Ÿåˆ†æäº†è¯¥é¢†åŸŸçš„æœºé‡ã€æŒ‘æˆ˜åŠæœªæ¥å‘å±•æ–¹å‘ã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¸»åŠ¨è§†è§‰æ„ŸçŸ¥ç³»ç»Ÿèƒ½å¤Ÿé€šè¿‡åŠ¨æ€æ„ŸçŸ¥å’Œè¡ŒåŠ¨ä¸ç¯å¢ƒäº¤äº’ï¼Œä½†é¢ä¸´å®æ—¶å¤„ç†å¤æ‚è§†è§‰æ•°æ®ã€åŠ¨æ€ç¯å¢ƒå†³ç­–åˆ¶å®šå’Œå¤šæ¨¡æ€æ„ŸçŸ¥èåˆç­‰æŒ‘æˆ˜ï¼Œæœ¬æ–‡æ—¨åœ¨ç³»ç»Ÿæ¢³ç†è¯¥é¢†åŸŸçš„æœºé‡ä¸éšœç¢ï¼Œä¸ºæ›´å¹¿æ³›çš„åº”ç”¨æä¾›ç†è®ºåŸºç¡€ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é‡‡ç”¨ç»¼è¿°ç ”ç©¶æ–¹æ³•ï¼Œç³»ç»Ÿåˆ†æä¸»åŠ¨è§†è§‰æ„ŸçŸ¥çš„æ ¸å¿ƒæ¦‚å¿µã€æŠ€æœ¯æ¡†æ¶å’Œåº”ç”¨åœºæ™¯ï¼Œé‡ç‚¹æ¢è®¨äº†æ³¨æ„åŠ›å¼•å¯¼ã€ä¼ æ„Ÿå™¨ç§»åŠ¨å’Œç‰©ä½“äº¤äº’ç­‰å…³é”®æŠ€æœ¯ï¼Œä»¥åŠå®ƒä»¬åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å®ç°æœºåˆ¶ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶æä¾›äº†ä¸»åŠ¨è§†è§‰æ„ŸçŸ¥åœ¨æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€äººæœºäº¤äº’å’Œç›‘æ§ç³»ç»Ÿç­‰é¢†åŸŸçš„åº”ç”¨å…¨æ™¯ï¼Œè¯†åˆ«äº†å®æ—¶æ•°æ®å¤„ç†ã€åŠ¨æ€å†³ç­–å’Œå¤šæ¨¡æ€èåˆç­‰å…³é”®æŠ€æœ¯æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰ç ”ç©¶çš„å±€é™æ€§å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚</p>
<p><strong>Conclusion:</strong> ä¸»åŠ¨è§†è§‰æ„ŸçŸ¥ä»£è¡¨äº†ä»è¢«åŠ¨è§‚å¯Ÿåˆ°ä¸»åŠ¨äº¤äº’çš„èŒƒå¼è½¬å˜ï¼Œè™½ç„¶é¢ä¸´æŠ€æœ¯æŒ‘æˆ˜ï¼Œä½†åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å®æ—¶ç®—æ³•ã€å†³ç­–æ¡†æ¶å’Œè·¨æ¨¡æ€é›†æˆæ–¹æ³•ä»¥å®ç°æ›´å¹¿æ³›çš„å®é™…åº”ç”¨ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>Active visual perception refers to the ability of a system to dynamically engage with its environment through sensing and action, allowing it to modify its behavior in response to specific goals or uncertainties. Unlike passive systems that rely solely on visual data, active visual perception systems can direct attention, move sensors, or interact with objects to acquire more informative data. This approach is particularly powerful in complex environments where static sensing methods may not provide sufficient information. Active visual perception plays a critical role in numerous applications, including robotics, autonomous vehicles, human-computer interaction, and surveillance systems. However, despite its significant promise, there are several challenges that need to be addressed, including real-time processing of complex visual data, decision-making in dynamic environments, and integrating multimodal sensory inputs. This paper explores both the opportunities and challenges inherent in active visual perception, providing a comprehensive overview of its potential, current research, and the obstacles that must be overcome for broader adoption.</p>
<h3 id="42-posa-vla-enhancing-action-generation-via-pose-conditioned-anchor-attention">[42] <a href="https://arxiv.org/abs/2512.03724">PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention</a></h3>
<p><em>Ziwen Li, Xin Wang, Hanlue Zhang, Runnan Chen, Runqi Lin, Xiao He, Han Huang, Yandong Guo, Fakhri Karray, Tongliang Liu, Mingming Gong</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„PosA-VLAæ¡†æ¶ï¼Œé€šè¿‡å§¿æ€æ¡ä»¶ç›‘ç£é”šå®šè§†è§‰æ³¨æ„åŠ›ï¼Œè§£å†³äº†ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­äº§ç”Ÿå†—ä½™åŠ¨ä½œçš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†åŠ¨ä½œç”Ÿæˆçš„ç²¾ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å…·èº«ä»»åŠ¡ä¸­ä»éš¾ä»¥äº§ç”Ÿä¸€è‡´ä¸”ç²¾ç¡®çš„ç›®æ ‡å¯¼å‘åŠ¨ä½œï¼Œç»å¸¸åœ¨è½¨è¿¹ä¸­ç”Ÿæˆå†—ä½™æˆ–ä¸ç¨³å®šçš„è¿åŠ¨ï¼Œé™åˆ¶äº†å…¶åœ¨æ—¶é—´æ•æ„Ÿåœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä½œè€…å°†è¿™äº›å†—ä½™åŠ¨ä½œå½’å› äºç°æœ‰VLAæ¨¡å‹çš„ç©ºé—´å‡åŒ€æ„ŸçŸ¥åœºï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­å®¹æ˜“è¢«ç›®æ ‡æ— å…³ç‰©ä½“åˆ†æ•£æ³¨æ„åŠ›ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†é«˜æ•ˆçš„PosA-VLAæ¡†æ¶ï¼Œé€šè¿‡å§¿æ€æ¡ä»¶ç›‘ç£é”šå®šè§†è§‰æ³¨æ„åŠ›ï¼ŒæŒç»­å¼•å¯¼æ¨¡å‹æ„ŸçŸ¥æœå‘ä»»åŠ¡ç›¸å…³åŒºåŸŸã€‚è¯¥å§¿æ€æ¡ä»¶é”šå®šæ³¨æ„åŠ›æœºåˆ¶ä½¿æ¨¡å‹èƒ½æ›´å¥½åœ°å¯¹é½æŒ‡ä»¤è¯­ä¹‰ä¸å¯æ“ä½œçš„è§†è§‰çº¿ç´¢ï¼Œä»è€Œæå‡åŠ¨ä½œç”Ÿæˆç²¾åº¦å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è½»é‡çº§æ¶æ„ï¼Œæ— éœ€è¾…åŠ©æ„ŸçŸ¥æ¨¡å—ï¼Œç¡®ä¿äº†é«˜æ•ˆæ¨ç†ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–çš„æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­èƒ½å¤Ÿä»¥ç²¾ç¡®ä¸”æ—¶é—´é«˜æ•ˆçš„æ–¹å¼æ‰§è¡Œå…·èº«ä»»åŠ¡ï¼Œå¹¶åœ¨å„ç§æŒ‘æˆ˜æ€§ç¯å¢ƒä¸­å±•ç°å‡ºé²æ£’çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼ŒPosA-VLAåœ¨åŠ¨ä½œç”Ÿæˆç²¾åº¦å’Œæ•ˆç‡æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡å§¿æ€æ¡ä»¶ç›‘ç£é”šå®šè§†è§‰æ³¨æ„åŠ›ï¼Œæœ‰æ•ˆè§£å†³äº†VLAæ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å†—ä½™åŠ¨ä½œé—®é¢˜ï¼Œä¸ºå…·èº«æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ›´ç²¾ç¡®é«˜æ•ˆçš„åŠ¨ä½œç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–æ„ŸçŸ¥æ¨¡å—çš„è½»é‡çº§è®¾è®¡ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæ—¶é—´æ•æ„Ÿåœºæ™¯ä¸‹çš„æœºå™¨äººæ“ä½œä»»åŠ¡æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.</p>
<h3 id="43-dual-level-modality-debiasing-learning-for-unsupervised-visible-infrared-person-re-identification">[43] <a href="https://arxiv.org/abs/2512.03745">Dual-level Modality Debiasing Learning for Unsupervised Visible-Infrared Person Re-Identification</a></h3>
<p><em>Jiaze Li, Yan Lu, Bin Liu, Guojun Yin, Mang Ye</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒçº§æ¨¡æ€å»åå­¦ä¹ ï¼ˆDMDLï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ¨¡å‹çº§å’Œä¼˜åŒ–çº§çš„åŒé‡å¹²é¢„æ¥è§£å†³æ— ç›‘ç£å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«ä¸­çš„æ¨¡æ€åå·®é—®é¢˜ï¼Œå®ç°äº†æ›´å¹¿ä¹‰çš„æ¨¡æ€ä¸å˜ç‰¹å¾å­¦ä¹ ã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„ä¸¤é˜¶æ®µæ— ç›‘ç£å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«æ–¹æ³•åœ¨å•æ¨¡æ€å­¦ä¹ é˜¶æ®µä¼šå¼•å…¥æ¨¡æ€ç‰¹å®šçº¿ç´¢ï¼Œè¿™äº›åå·®ä¼šä¼ æ’­åˆ°è·¨æ¨¡æ€å­¦ä¹ é˜¶æ®µï¼ŒæŸå®³èº«ä»½åˆ¤åˆ«èƒ½åŠ›å’Œæ¨¡å‹æ³›åŒ–æ€§èƒ½ï¼Œå› æ­¤éœ€è¦è§£å†³æ¨¡æ€åå·®é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åŒçº§æ¨¡æ€å»åå­¦ä¹ æ¡†æ¶ï¼ŒåŒ…å«æ¨¡å‹çº§çš„å› æœå¯å‘è°ƒæ•´å¹²é¢„æ¨¡å—ï¼Œç”¨å› æœå»ºæ¨¡æ›¿ä»£åŸºäºä¼¼ç„¶çš„å»ºæ¨¡ä»¥é˜²æ­¢æ¨¡æ€è¯±å¯¼çš„è™šå‡æ¨¡å¼ï¼›ä»¥åŠä¼˜åŒ–çº§çš„åä½œæ— åè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡æ¨¡æ€ç‰¹å®šå¢å¼ºã€æ ‡ç­¾ç»†åŒ–å’Œç‰¹å¾å¯¹é½æ¥ä¸­æ–­æ¨¡æ€åå·®åœ¨æ•°æ®ã€æ ‡ç­¾å’Œç‰¹å¾é—´çš„ä¼ æ’­ã€‚</p>
<p><strong>Result:</strong> åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDMDLæ¡†æ¶èƒ½å¤Ÿå®ç°æ¨¡æ€ä¸å˜çš„ç‰¹å¾å­¦ä¹ ï¼Œè·å¾—æ›´å…·æ³›åŒ–èƒ½åŠ›çš„æ¨¡å‹ï¼Œåœ¨æ— ç›‘ç£å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡æ¨¡å‹çº§å’Œä¼˜åŒ–çº§çš„åŒé‡å»åæœºåˆ¶æœ‰æ•ˆè§£å†³äº†æ¨¡æ€åå·®é—®é¢˜ï¼Œä¸ºè·¨æ¨¡æ€å­¦ä¹ æä¾›äº†æ–°çš„å»åæ¡†æ¶ï¼Œè¡¨æ˜åŒæ—¶å¤„ç†æ¨¡å‹ç»“æ„å’Œè®­ç»ƒè¿‡ç¨‹çš„åå·®ä¼ æ’­æ˜¯å®ç°å¹¿ä¹‰æ¨¡æ€ä¸å˜è¡¨ç¤ºçš„å…³é”®ã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>Two-stage learning pipeline has achieved promising results in unsupervised visible-infrared person re-identification (USL-VI-ReID). It first performs single-modality learning and then operates cross-modality learning to tackle the modality discrepancy. Although promising, this pipeline inevitably introduces modality bias: modality-specific cues learned in the single-modality training naturally propagate into the following cross-modality learning, impairing identity discrimination and generalization. To address this issue, we propose a Dual-level Modality Debiasing Learning (DMDL) framework that implements debiasing at both the model and optimization levels. At the model level, we propose a Causality-inspired Adjustment Intervention (CAI) module that replaces likelihood-based modeling with causal modeling, preventing modality-induced spurious patterns from being introduced, leading to a low-biased model. At the optimization level, a Collaborative Bias-free Training (CBT) strategy is introduced to interrupt the propagation of modality bias across data, labels, and features by integrating modality-specific augmentation, label refinement, and feature alignment. Extensive experiments on benchmark datasets demonstrate that DMDL could enable modality-invariant feature learning and a more generalized model.</p>
<h3 id="44-fully-unsupervised-self-debiasing-of-text-to-image-diffusion-models">[44] <a href="https://arxiv.org/abs/2512.03749">Fully Unsupervised Self-debiasing of Text-to-Image Diffusion Models</a></h3>
<p><em>Korada Sri Vardhana, Shrikrishna Lolla, Soma Biswas</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSelfDebiasï¼Œä¸€ç§å®Œå…¨æ— ç›‘ç£çš„æµ‹è¯•æ—¶å»åæ–¹æ³•ï¼Œé€‚ç”¨äºä»»ä½•ä½¿ç”¨UNetä½œä¸ºå™ªå£°é¢„æµ‹å™¨çš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«è¯­ä¹‰æ¨¡å¼å¹¶å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ä»¥å‡å°‘åè§ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè§†è§‰è´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å¤§å‹äº’è”ç½‘æ•°æ®é›†ï¼ˆå¦‚LAION-5Bï¼‰ä¸Šè®­ç»ƒæ—¶ï¼Œä¼šå­¦ä¹ å¹¶å†ç°æ•°æ®ä¸­å­˜åœ¨çš„ä¼—å¤šåè§ï¼Œå¯¼è‡´ç”Ÿæˆåˆ»æ¿å°è±¡åŒ–çš„è¾“å‡ºï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦äººå·¥æ ‡æ³¨æ•°æ®é›†æˆ–é’ˆå¯¹æ¯ä¸ªç”Ÿæˆæ¦‚å¿µè®­ç»ƒå¤–éƒ¨åˆ†ç±»å™¨ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Method:</strong> SelfDebiasæ˜¯ä¸€ç§å®Œå…¨æ— ç›‘ç£çš„æµ‹è¯•æ—¶å»åæ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«å›¾åƒç¼–ç å™¨åµŒå…¥ç©ºé—´ä¸­çš„è¯­ä¹‰èšç±»ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨è¿™äº›èšç±»å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œæœ€å°åŒ–è¾“å‡ºåˆ†å¸ƒä¸å‡åŒ€åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦ï¼Œè¯¥æ–¹æ³•ä¸ä¾èµ–äººå·¥æ ‡æ³¨æ•°æ®é›†æˆ–å¤–éƒ¨åˆ†ç±»å™¨ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«è¯­ä¹‰æ¨¡å¼ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSelfDebiasåœ¨å¤šç§æç¤ºå’Œæ‰©æ•£æ¨¡å‹æ¶æ„ï¼ˆåŒ…æ‹¬æ¡ä»¶æ¨¡å‹å’Œæ— æ¡ä»¶æ¨¡å‹ï¼‰ä¸Šå…·æœ‰è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œä¸ä»…èƒ½æœ‰æ•ˆå‡å°‘å…³é”®äººå£ç»Ÿè®¡ç»´åº¦ä¸Šçš„åè§åŒæ—¶ä¿æŒç”Ÿæˆå›¾åƒçš„è§†è§‰ä¿çœŸåº¦ï¼Œè¿˜èƒ½å¤„ç†è¯†åˆ«åè§æ›´å…·æŒ‘æˆ˜æ€§çš„æŠ½è±¡æ¦‚å¿µã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§æ— éœ€ç›‘ç£çš„å»åæ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨é€‚åº”ä¸åŒæ¦‚å¿µå’Œæ¨¡å‹æ¶æ„ï¼Œä¸ºæ‰©æ•£æ¨¡å‹çš„å…¬å¹³æ€§ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œè¡¨æ˜æ— ç›‘ç£æ–¹æ³•åœ¨è¯†åˆ«å’Œå‡è½»å¤æ‚åè§æ–¹é¢å…·æœ‰æ½œåŠ›ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ä¸å—å½±å“ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>Text-to-image (T2I) diffusion models have achieved widespread success due to their ability to generate high-resolution, photorealistic images. These models are trained on large-scale datasets, like LAION-5B, often scraped from the internet. However, since this data contains numerous biases, the models inherently learn and reproduce them, resulting in stereotypical outputs. We introduce SelfDebias, a fully unsupervised test-time debiasing method applicable to any diffusion model that uses a UNet as its noise predictor. SelfDebias identifies semantic clusters in an image encoder's embedding space and uses these clusters to guide the diffusion process during inference, minimizing the KL divergence between the output distribution and the uniform distribution. Unlike supervised approaches, SelfDebias does not require human-annotated datasets or external classifiers trained for each generated concept. Instead, it is designed to automatically identify semantic modes. Extensive experiments show that SelfDebias generalizes across prompts and diffusion model architectures, including both conditional and unconditional models. It not only effectively debiases images along key demographic dimensions while maintaining the visual fidelity of the generated images, but also more abstract concepts for which identifying biases is also challenging.</p>
<h3 id="45-heatmap-pooling-network-for-action-recognition-from-rgb-videos">[45] <a href="https://arxiv.org/abs/2512.03837">Heatmap Pooling Network for Action Recognition from RGB Videos</a></h3>
<p><em>Mengyuan Liu, Jinfu Liu, Yongkang Jiang, Bin He</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„çƒ­å›¾æ± åŒ–ç½‘ç»œï¼ˆHP-Netï¼‰ç”¨äºè§†é¢‘åŠ¨ä½œè¯†åˆ«ï¼Œé€šè¿‡åé¦ˆæ± åŒ–æ¨¡å—æå–ä¿¡æ¯ä¸°å¯Œã€é²æ£’ä¸”ç®€æ´çš„äººä½“æ± åŒ–ç‰¹å¾ï¼Œå¹¶è®¾è®¡å¤šæ¨¡æ€èåˆæ¨¡å—å®ç°æ›´ç¨³å¥çš„åŠ¨ä½œè¯†åˆ«ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰RGBè§†é¢‘åŠ¨ä½œè¯†åˆ«æ–¹æ³•åœ¨æå–æ·±åº¦ç‰¹å¾æ—¶é¢ä¸´ä¿¡æ¯å†—ä½™ã€æ˜“å—å™ªå£°å¹²æ‰°å’Œé«˜å­˜å‚¨æˆæœ¬ç­‰æŒ‘æˆ˜ï¼Œéœ€è¦å……åˆ†åˆ©ç”¨è§†é¢‘ä¸­çš„æœ‰ç”¨ä¿¡æ¯å¹¶æé«˜ç‰¹å¾æå–çš„æ•ˆç‡å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºçƒ­å›¾æ± åŒ–ç½‘ç»œï¼ˆHP-Netï¼‰ï¼ŒåŒ…å«åé¦ˆæ± åŒ–æ¨¡å—æå–ä¿¡æ¯ä¸°å¯Œä¸”é²æ£’çš„äººä½“æ± åŒ–ç‰¹å¾ï¼Œå¹¶è®¾è®¡ç©ºé—´-è¿åŠ¨ååŒå­¦ä¹ æ¨¡å—å’Œæ–‡æœ¬ç»†åŒ–è°ƒåˆ¶æ¨¡å—æ¥æ•´åˆæå–çš„æ± åŒ–ç‰¹å¾ä¸å…¶ä»–å¤šæ¨¡æ€æ•°æ®ã€‚</p>
<p><strong>Result:</strong> åœ¨NTU RGB+D 60ã€NTU RGB+D 120ã€Toyota-Smarthomeå’ŒUAV-Humanç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†HP-Netçš„æœ‰æ•ˆæ€§ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„äººç±»åŠ¨ä½œè¯†åˆ«æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> HP-Neté€šè¿‡åˆ›æ–°çš„æ± åŒ–ç‰¹å¾æå–å’Œå¤šæ¨¡æ€èåˆæœºåˆ¶ï¼Œä¸ºè§†é¢‘åŠ¨ä½œè¯†åˆ«æä¾›äº†æ›´é«˜æ•ˆã€é²æ£’çš„è§£å†³æ–¹æ¡ˆï¼Œæ‰€æå–çš„æ± åŒ–ç‰¹å¾ç›¸æ¯”ä¼ ç»Ÿå§¿æ€æ•°æ®å’Œçƒ­å›¾ç‰¹å¾å…·æœ‰æ˜æ˜¾æ€§èƒ½ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.</p>
<h3 id="46-coda-from-text-to-image-diffusion-models-to-training-free-dataset-distillation">[46] <a href="https://arxiv.org/abs/2512.03844">CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation</a></h3>
<p><em>Letian Zhou, Songhua Liu, Xinchao Wang</em></p>
<h4 id="tldr_45">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Core Distribution Alignment (CoDA)æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨ç°æˆçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å®ç°æ•°æ®é›†è’¸é¦ï¼Œæ— éœ€åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼ŒåŒæ—¶è§£å†³äº†é€šç”¨ç”Ÿæˆå…ˆéªŒä¸ç›®æ ‡è¯­ä¹‰ä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_45">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ•°æ®é›†è’¸é¦æ–¹æ³•é¢ä¸´ä¸¤ä¸ªåŸºæœ¬é™åˆ¶ï¼šå¤§å¤šæ•°åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•éœ€è¦åœ¨å®Œæ•´ç›®æ ‡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™è¿èƒŒäº†æ•°æ®é›†è’¸é¦çš„åˆè¡·ä¸”è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼›è€Œä¾èµ–é€šç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ–¹æ³•åˆ™å­˜åœ¨æ˜¾è‘—çš„åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ï¼Œå› ä¸ºç½‘ç»œè§„æ¨¡çš„å…ˆéªŒæ— æ³•å‡†ç¡®æ•æ‰ç›®æ ‡ç‰¹å®šçš„è¯­ä¹‰ï¼Œå¯¼è‡´æ€§èƒ½æ¬¡ä¼˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„Core Distribution Alignment (CoDA)æ¡†æ¶é¦–å…ˆé€šè¿‡é²æ£’çš„åŸºäºå¯†åº¦çš„å‘ç°æœºåˆ¶è¯†åˆ«ç›®æ ‡æ•°æ®é›†çš„"å†…åœ¨æ ¸å¿ƒåˆ†å¸ƒ"ï¼Œç„¶åå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ä½¿ç”Ÿæˆçš„æ ·æœ¬ä¸è¯¥æ ¸å¿ƒåˆ†å¸ƒå¯¹é½ï¼Œä»è€Œæœ‰æ•ˆå¼¥åˆé€šç”¨ç”Ÿæˆå…ˆéªŒä¸ç›®æ ‡è¯­ä¹‰ä¹‹é—´çš„å·®è·ï¼Œç”Ÿæˆå…·æœ‰é«˜åº¦ä»£è¡¨æ€§çš„è’¸é¦æ•°æ®é›†ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼ŒCoDAåœ¨ä¸ä¾èµ–ç›®æ ‡æ•°æ®é›†ç‰¹å®šè®­ç»ƒç”Ÿæˆæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œåœ¨åŒ…æ‹¬ImageNet-1KåŠå…¶å­é›†åœ¨å†…çš„æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½è¾¾åˆ°ç”šè‡³è¶…è¶Šäº†å…ˆå‰ä¾èµ–æ­¤ç±»æ¨¡å‹çš„æ–¹æ³•ï¼Œåœ¨ImageNet-1Kçš„æ¯ç±»50å›¾åƒè®¾ç½®ä¸‹è¾¾åˆ°äº†60.4%çš„æœ€æ–°å‡†ç¡®ç‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨ç°æˆæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å®ç°é«˜æ•ˆæ•°æ®é›†è’¸é¦çš„å¯è¡Œæ€§ï¼Œé€šè¿‡æ ¸å¿ƒåˆ†å¸ƒå¯¹é½æœºåˆ¶æœ‰æ•ˆè§£å†³äº†é€šç”¨ç”Ÿæˆå…ˆéªŒä¸ç›®æ ‡è¯­ä¹‰çš„åŒ¹é…é—®é¢˜ï¼Œä¸ºæ•°æ®é›†è’¸é¦é¢†åŸŸæä¾›äº†æ›´å®ç”¨ä¸”æˆæœ¬æ•ˆç›Šæ›´é«˜çš„è§£å†³æ–¹æ¡ˆï¼Œé¿å…äº†æ˜‚è´µçš„ç‰¹å®šæ•°æ®é›†æ¨¡å‹è®­ç»ƒéœ€æ±‚ã€‚</p>
<hr />
<h4 id="abstract_45">ğŸ“„ Abstract</h4>
<p>Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the "intrinsic core distribution" of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4% at the 50-images-per-class (IPC) setup on ImageNet-1K. Our code is available on the project webpage: https://github.com/zzzlt422/CoDA</p>
<h3 id="47-zero-shot-video-translation-and-editing-with-frame-spatial-temporal-correspondence">[47] <a href="https://arxiv.org/abs/2512.03905">Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence</a></h3>
<p><em>Shuai Yang, Junxin Lin, Yifan Zhou, Ziwei Liu, Chen Change Loy</em></p>
<h4 id="tldr_46">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FRESCOæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå¸§å†…ä¸å¸§é—´å¯¹åº”å…³ç³»æ¥å¢å¼ºæ—¶ç©ºçº¦æŸï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬è§†é¢‘ç¼–è¾‘çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œåœ¨è§†é¢‘åˆ°è§†é¢‘è½¬æ¢å’Œæ–‡æœ¬å¼•å¯¼è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜å¼‚è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_46">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰é›¶æ ·æœ¬è§†é¢‘ç¼–è¾‘æ–¹æ³•ä¸»è¦å…³æ³¨åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­æ•´åˆå¸§é—´å¯¹åº”å…³ç³»ï¼Œä½†å…¶è½¯çº¦æŸåœ¨è¯†åˆ«æœ‰æ•ˆç‰¹å¾æ–¹é¢ä¸è¶³ï¼Œå®¹æ˜“å¯¼è‡´æ—¶é—´ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œéœ€è¦æ›´é²æ£’çš„æ—¶ç©ºçº¦æŸæ¥ç¡®ä¿è§†é¢‘ç¼–è¾‘çš„è§†è§‰è¿è´¯æ€§ã€‚</p>
<p><strong>Method:</strong> FRESCOæ¡†æ¶æ•´åˆäº†å¸§å†…å¯¹åº”å…³ç³»ä¸å¸§é—´å¯¹åº”å…³ç³»ï¼Œå½¢æˆäº†æ›´é²æ£’çš„æ—¶ç©ºçº¦æŸæœºåˆ¶ï¼Œè¯¥æ–¹æ³•ä¸ä»…æä¾›æ³¨æ„åŠ›å¼•å¯¼ï¼Œè¿˜é€šè¿‡æ˜¾å¼ä¼˜åŒ–ç‰¹å¾æ¥ç¡®ä¿è¯­ä¹‰ç›¸ä¼¼å†…å®¹åœ¨å¸§é—´çš„ä¸€è‡´æ€§è½¬æ¢ï¼Œä»è€Œæå‡è§†é¢‘ç¼–è¾‘çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨è§†é¢‘åˆ°è§†é¢‘è½¬æ¢å’Œæ–‡æœ¬å¼•å¯¼è§†é¢‘ç¼–è¾‘ä¸¤ä¸ªé›¶æ ·æœ¬ä»»åŠ¡ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒFRESCOèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€è¿è´¯çš„è§†é¢‘å†…å®¹ï¼Œç›¸æ¯”ç°æœ‰é›¶æ ·æœ¬æ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨æå‡è§†è§‰è¿è´¯æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡æ•´åˆå¸§å†…ä¸å¸§é—´å¯¹åº”å…³ç³»æ„å»ºäº†æ›´é²æ£’çš„æ—¶ç©ºçº¦æŸæœºåˆ¶ï¼Œä¸ºæå‡é›¶æ ·æœ¬è§†é¢‘ç¼–è¾‘çš„è§†è§‰ä¸€è‡´æ€§æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä»£è¡¨äº†å½“å‰é›¶æ ·æœ¬æ–¹æ³•çš„é‡è¦è¿›å±•ï¼Œå¹¶ä¸ºæœªæ¥è§†é¢‘ç¼–è¾‘æŠ€æœ¯å‘å±•æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_46">ğŸ“„ Abstract</h4>
<p>The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.</p>
<h3 id="48-unimo-unifying-2d-video-and-3d-human-motion-with-an-autoregressive-framework">[48] <a href="https://arxiv.org/abs/2512.03918">UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework</a></h3>
<p><em>Youxin Pang, Yong Zhang, Ruizhi Shao, Xiang Deng, Feng Gao, Xu Xiaoming, Xiaoming Wei, Yebin Liu</em></p>
<h4 id="tldr_47">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UniMoï¼Œä¸€ç§åˆ›æ–°çš„è‡ªå›å½’æ¨¡å‹ï¼Œé¦–æ¬¡åœ¨ç»Ÿä¸€æ¡†æ¶å†…å®ç°äº†2Däººä½“è§†é¢‘å’Œ3Däººä½“è¿åŠ¨çš„è”åˆå»ºæ¨¡ï¼Œèƒ½å¤ŸåŒæ—¶ç”Ÿæˆå’Œç†è§£è¿™ä¸¤ç§æ¨¡æ€ï¼Œä¸ºäººç±»ä¸­å¿ƒä¿¡æ¯çš„å¤šæ¨¡æ€èåˆå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="detailed-summary_47">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä»¥å¦ä¸€ç§æ¨¡æ€ä¸ºæ¡ä»¶ç”Ÿæˆå•ä¸€æ¨¡æ€ï¼Œæˆ–å°†å…¶ä¸­ä¸€ç§æ¨¡æ€ä¸æ–‡æœ¬ã€éŸ³é¢‘ç­‰å…¶ä»–æ¨¡æ€é›†æˆï¼Œè€Œå°†2Dè§†é¢‘å’Œ3Dè¿åŠ¨ç»Ÿä¸€è¿›è¡ŒåŒæ—¶ä¼˜åŒ–å’Œç”Ÿæˆçš„ç ”ç©¶ä»ç„¶å¾ˆå°‘æ¢ç´¢ï¼Œè¿™é¢ä¸´ç€ç”±äºå®ƒä»¬åœ¨ç»“æ„å’Œåˆ†å¸ƒä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚æ‰€å¸¦æ¥çš„é‡å¤§æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•å°†è§†é¢‘å’Œ3Dè¿åŠ¨å»ºæ¨¡ä¸ºç»Ÿä¸€çš„ä»¤ç‰Œåºåˆ—ï¼Œåˆ©ç”¨å•ç‹¬çš„åµŒå…¥å±‚æ¥ç¼“è§£åˆ†å¸ƒå·®å¼‚ï¼Œå¹¶è®¾è®¡äº†é›†æˆä¸¤ç§ä¸åŒä»»åŠ¡çš„åºåˆ—å»ºæ¨¡ç­–ç•¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†æœ‰æ•ˆå¯¹é½è§†è§‰ä»¤ç‰Œå¹¶ä¿ç•™3Dç©ºé—´ä¿¡æ¯ï¼Œè®¾è®¡äº†ä¸€ç§å…·æœ‰æ—¶é—´æ‰©å±•ç­–ç•¥çš„æ–°å‹3Dè¿åŠ¨åˆ†è¯å™¨ï¼Œä½¿ç”¨å•ä¸ªVQ-VAEç”Ÿæˆé‡åŒ–è¿åŠ¨ä»¤ç‰Œï¼Œè¯¥åˆ†è¯å™¨åŒ…å«å¤šä¸ªä¸“å®¶è§£ç å™¨ï¼Œåˆ†åˆ«å¤„ç†èº«ä½“å½¢çŠ¶ã€å¹³ç§»ã€å…¨å±€æ–¹å‘å’Œèº«ä½“å§¿æ€ï¼Œä»¥å®ç°å¯é çš„3Dè¿åŠ¨é‡å»ºã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶ç”Ÿæˆç›¸åº”çš„è§†é¢‘å’Œè¿åŠ¨ï¼ŒåŒæ—¶æ‰§è¡Œç²¾ç¡®çš„è¿åŠ¨æ•æ‰ï¼Œè¯æ˜äº†ç»Ÿä¸€å»ºæ¨¡çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå±•ç¤ºäº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹å·¥ä½œæŒ–æ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹èåˆä¸åŒæ•°æ®ç±»å‹çš„èƒ½åŠ›ï¼Œä¸ºå°†äººç±»ä¸­å¿ƒä¿¡æ¯é›†æˆåˆ°ç°æœ‰æ¨¡å‹ä¸­é“ºå¹³äº†é“è·¯ï¼Œå¹¶å¯èƒ½å®ç°äººç±»ã€ç‰©ä½“å’Œåœºæ™¯çš„å¤šæ¨¡æ€å¯æ§è”åˆå»ºæ¨¡ï¼Œä»£è¡¨äº†è·¨æ¨¡æ€äººç±»è¡¨ç¤ºå­¦ä¹ çš„é‡è¦è¿›å±•ã€‚</p>
<hr />
<h4 id="abstract_47">ğŸ“„ Abstract</h4>
<p>We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.</p>
<h3 id="49-tempr1-improving-temporal-understanding-of-mllms-via-temporal-aware-multi-task-reinforcement-learning">[49] <a href="https://arxiv.org/abs/2512.03963">TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning</a></h3>
<p><em>Tao Wu, Li Yang, Gen Zhan, Yiting Liao, Junlin Li, Deliang Fu, Li Zhang, Limin Wang</em></p>
<h4 id="tldr_48">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†TempR1ï¼Œä¸€ç§é¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ—¶é—´æ„ŸçŸ¥å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»ŸåŒ–çš„å¤šä»»åŠ¡ä¼˜åŒ–æ˜¾è‘—å¢å¼ºæ¨¡å‹å¯¹è§†é¢‘æ—¶åºç»“æ„çš„ç†è§£èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_48">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç”¨äºå¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ—¶åºç†è§£çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸å±€é™äºç‰¹å®šä»»åŠ¡ç±»å‹å’Œæ•°æ®ï¼Œé™åˆ¶äº†å…¶åœ¨å¤šæ ·åŒ–æ—¶åºç†è§£åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦å¼€å‘ä¸€ç§èƒ½å¤Ÿç³»ç»ŸåŒ–æå‡æ¨¡å‹æ—¶åºç†è§£èƒ½åŠ›çš„é€šç”¨æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†TempR1æ¡†æ¶ï¼ŒåŸºäºGroup Relative Policy Optimizationç®—æ³•æ„å»ºï¼Œé€šè¿‡æ„å»ºåŒ…å«å¤šæ ·åŒ–æ—¶åºç»“æ„å’Œè¯­ä¹‰çš„å¤šä»»åŠ¡è¯­æ–™åº“ï¼Œå¹¶å°†æ—¶åºä»»åŠ¡åˆ’åˆ†ä¸ºä¸‰ç§é¢„æµ‹åŒºé—´ä¸çœŸå®å®ä¾‹å¯¹åº”ç±»å‹ï¼Œä¸ºæ¯ç§ç±»å‹è®¾è®¡å®šåˆ¶åŒ–çš„å®šä½å¥–åŠ±å‡½æ•°ï¼Œå®ç°ç¨³å®šæœ‰æ•ˆçš„è·¨ä»»åŠ¡ä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒTempR1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå…¶è”åˆä¼˜åŒ–äº’è¡¥ä»»åŠ¡äº§ç”Ÿäº†æ˜¾è‘—çš„ååŒæ•ˆåº”ï¼Œæ—¢å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¹Ÿæå‡äº†å•ä»»åŠ¡æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ—¶åºæ¨ç†å»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”åŸåˆ™æ€§çš„èŒƒå¼ï¼Œè¯æ˜äº†å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ åœ¨ç³»ç»ŸåŒ–å¢å¼ºæ—¶åºç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºé•¿è§†é¢‘åˆ†æä»»åŠ¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_48">ğŸ“„ Abstract</h4>
<p>Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.</p>
<h3 id="50-ultra-lightweight-neural-video-representation-compression">[50] <a href="https://arxiv.org/abs/2512.04019">Ultra-lightweight Neural Video Representation Compression</a></h3>
<p><em>Ho Man Kwan, Tianhao Peng, Ge Gao, Fan Zhang, Mike Nilsson, Andrew Gower, David Bull</em></p>
<h4 id="tldr_49">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºNVRC-Liteï¼Œä¸€ç§è½»é‡åŒ–çš„ç¥ç»è§†é¢‘è¡¨ç¤ºå‹ç¼©æ¡†æ¶ï¼Œé€šè¿‡é›†æˆå¤šå°ºåº¦ç‰¹å¾ç½‘æ ¼å’Œå…«å‰æ ‘ä¸Šä¸‹æ–‡æ¨¡å‹ï¼Œåœ¨ä¿æŒä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶æ˜¾è‘—æå‡äº†å‹ç¼©æ€§èƒ½å¹¶åŠ é€Ÿäº†ç†µç¼–ç è¿‡ç¨‹ã€‚</p>
<hr />
<h4 id="detailed-summary_49">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºéšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„è§†é¢‘å‹ç¼©æ–¹æ³•è™½ç„¶æ€§èƒ½ä¼˜å¼‚ï¼Œä½†é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¸€æ˜¯è½»é‡åŒ–INRåœ¨ä½å¤æ‚åº¦ä¸‹çš„æ€§èƒ½ä»æœ‰æå‡ç©ºé—´ï¼ŒäºŒæ˜¯ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨è‡ªå›å½’æ¨¡å‹è¿›è¡Œç†µç¼–ç ï¼Œè™½ç„¶æœ‰æ•ˆä½†ç¼–ç é€Ÿåº¦ç¼“æ…¢ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> NVRC-Liteé‡‡ç”¨ä¸¤ç§å…³é”®æŠ€æœ¯æ”¹è¿›ï¼šé¦–å…ˆï¼Œå°†å¤šå°ºåº¦ç‰¹å¾ç½‘æ ¼é›†æˆåˆ°è½»é‡åŒ–ç¥ç»è¡¨ç¤ºä¸­ï¼Œé€šè¿‡ä½¿ç”¨æ›´é«˜åˆ†è¾¨ç‡çš„ç½‘æ ¼æ˜¾è‘—æå‡ä½å¤æ‚åº¦ä¸‹INRçš„æ€§èƒ½ï¼›å…¶æ¬¡ï¼Œæå‡ºåŸºäºå…«å‰æ ‘çš„ä¸Šä¸‹æ–‡æ¨¡å‹ç”¨äºé«˜ç»´ç‰¹å¾ç½‘æ ¼çš„ç†µç¼–ç ï¼Œæ›¿ä»£ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹ï¼Œä»è€ŒåŠ é€Ÿæ•´ä¸ªç†µç¼–ç æ¨¡å—ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒNVRC-Liteåœ¨PSNRå’ŒMS-SSIMæŒ‡æ ‡ä¸Šåˆ†åˆ«å®ç°äº†æœ€é«˜21.03%å’Œ23.06%çš„BD-rateèŠ‚çœï¼Œä¼˜äºå½“å‰æœ€ä½³è½»é‡åŒ–INRè§†é¢‘ç¼–è§£ç å™¨C3ï¼ŒåŒæ—¶å®ç°äº†8.4å€çš„ç¼–ç åŠ é€Ÿå’Œ2.5å€çš„è§£ç åŠ é€Ÿï¼Œè®¡ç®—å¤æ‚åº¦ä¿æŒåœ¨10kMACs/åƒç´ ä»¥ä¸‹ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡å¤šå°ºåº¦ç‰¹å¾ç½‘æ ¼å’Œé«˜æ•ˆç†µç¼–ç æ¨¡å‹çš„ç»“åˆï¼Œå¯ä»¥åœ¨ä¿æŒä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶æ˜¾è‘—æå‡è½»é‡åŒ–INRè§†é¢‘å‹ç¼©çš„æ€§èƒ½ï¼Œä¸ºå®é™…éƒ¨ç½²æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†å…«å‰æ ‘ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨åŠ é€Ÿç†µç¼–ç æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="abstract_49">ğŸ“„ Abstract</h4>
<p>Recent works have demonstrated the viability of utilizing over-fitted implicit neural representations (INRs) as alternatives to autoencoder-based models for neural video compression. Among these INR-based video codecs, Neural Video Representation Compression (NVRC) was the first to adopt a fully end-to-end compression framework that compresses INRs, achieving state-of-the-art performance. Moreover, some recently proposed lightweight INRs have shown comparable performance to their baseline codecs with computational complexity lower than 10kMACs/pixel. In this work, we extend NVRC toward lightweight representations, and propose NVRC-Lite, which incorporates two key changes. Firstly, we integrated multi-scale feature grids into our lightweight neural representation, and the use of higher resolution grids significantly improves the performance of INRs at low complexity. Secondly, we address the issue that existing INRs typically leverage autoregressive models for entropy coding: these are effective but impractical due to their slow coding speed. In this work, we propose an octree-based context model for entropy coding high-dimensional feature grids, which accelerates the entropy coding module of the model. Our experimental results demonstrate that NVRC-Lite outperforms C3, one of the best lightweight INR-based video codecs, with up to 21.03% and 23.06% BD-rate savings when measured in PSNR and MS-SSIM, respectively, while achieving 8.4x encoding and 2.5x decoding speedup. The implementation of NVRC-Lite will be made available.</p>
<h3 id="51-postercopilot-toward-layout-reasoning-and-controllable-editing-for-professional-graphic-design">[51] <a href="https://arxiv.org/abs/2512.04082">PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design</a></h3>
<p><em>Jiazhe Wei, Ken Li, Tianyu Lao, Haofan Wang, Liang Wang, Caifeng Shan, Chenyang Si</em></p>
<h4 id="tldr_50">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†PosterCopilotæ¡†æ¶ï¼Œé€šè¿‡æ¸è¿›å¼ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥å¢å¼ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å‡ ä½•ç†è§£å’Œç¾å­¦æ¨ç†èƒ½åŠ›ï¼Œå¹¶ç»“åˆç”Ÿæˆæ¨¡å‹å®ç°ä¸“ä¸šçº§å›¾å½¢è®¾è®¡çš„å¯æ§è¿­ä»£ç¼–è¾‘ã€‚</p>
<hr />
<h4 id="detailed-summary_50">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å›¾å½¢è®¾è®¡è‡ªåŠ¨åŒ–æ–¹æ³•å­˜åœ¨å‡ ä½•å¸ƒå±€ä¸å‡†ç¡®å’Œç¼ºä¹ä¸“ä¸šå·¥ä½œæµç¨‹æ‰€éœ€çš„è¿­ä»£ã€åˆ†å±‚ç¼–è¾‘èƒ½åŠ›çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸“ä¸šè®¾è®¡åœºæ™¯ä¸­çš„åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> PosterCopilotæ¡†æ¶é‡‡ç”¨æ¸è¿›å¼ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šæ‰°åŠ¨ç›‘ç£å¾®è°ƒã€è§†è§‰-ç°å®å¯¹é½çš„å¼ºåŒ–å­¦ä¹ ä»¥åŠç¾å­¦åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼Œä»¥å¢å¼ºLMMçš„å‡ ä½•ç†è§£å’Œç¾å­¦æ¨ç†èƒ½åŠ›ï¼Œå¹¶ç»“åˆç”Ÿæˆæ¨¡å‹å®ç°åˆ†å±‚å¯æ§çš„è¿­ä»£ç¼–è¾‘å·¥ä½œæµç¨‹ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜PosterCopilotèƒ½å¤Ÿç”Ÿæˆå‡ ä½•å‡†ç¡®ä¸”ç¾å­¦ä¼˜è¶Šçš„å¸ƒå±€ï¼Œåœ¨ä¸“ä¸šè¿­ä»£è®¾è®¡ä¸­æä¾›å‰æ‰€æœªæœ‰çš„å¯æ§æ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡å¢å¼ºLMMçš„å‡ ä½•ç†è§£å’Œç¾å­¦æ¨ç†èƒ½åŠ›ï¼Œç»“åˆå¯æ§ç¼–è¾‘å·¥ä½œæµç¨‹ï¼Œä¸ºä¸“ä¸šå›¾å½¢è®¾è®¡è‡ªåŠ¨åŒ–æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå®ç°äº†å‡ ä½•å‡†ç¡®æ€§å’Œè®¾è®¡å¯æ§æ€§çš„æ˜¾è‘—æå‡ã€‚</p>
<hr />
<h4 id="abstract_50">ğŸ“„ Abstract</h4>
<p>Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.</p>
<h3 id="52-unique-lives-shared-world-learning-from-single-life-videos">[52] <a href="https://arxiv.org/abs/2512.04085">Unique Lives, Shared World: Learning from Single-Life Videos</a></h3>
<p><em>Tengda Han, Sayna Ebrahimi, Dilara Gokay, Li Yang Ku, Maks Ovsjanikov, Iva Babukova, Daniel Zoran, Viorica Patraucean, Joao Carreira, Andrew Zisserman, Dima Damen</em></p>
<h4 id="tldr_51">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡º'å•ä¸€ç”Ÿæ¶¯'å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡ä»…ä½¿ç”¨å•ä¸€ä¸ªä½“é‡‡é›†çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘è®­ç»ƒç‹¬ç«‹çš„è§†è§‰æ¨¡å‹ï¼Œåˆ©ç”¨è‡ªç„¶æ•è·çš„å¤šè§†è§’ä¿¡æ¯è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œè¯æ˜äº†è¯¥èŒƒå¼èƒ½å¤Ÿå­¦ä¹ åˆ°é«˜åº¦å¯¹é½ä¸”å¯æ³›åŒ–çš„å‡ ä½•è¡¨ç¤ºã€‚</p>
<hr />
<h4 id="detailed-summary_51">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢'å•ä¸€ç”Ÿæ¶¯'å­¦ä¹ èŒƒå¼ï¼Œå³ä»…ä½¿ç”¨å•ä¸€ä¸ªä½“é‡‡é›†çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘è®­ç»ƒè§†è§‰æ¨¡å‹ï¼Œç ”ç©¶è¿™ç§å—é™æ•°æ®æºæ˜¯å¦èƒ½å¤Ÿå­¦ä¹ åˆ°æœ‰æ•ˆçš„è§†è§‰è¡¨ç¤ºï¼Œä»¥åŠä¸åŒä¸ªä½“æ¨¡å‹ä¹‹é—´æ˜¯å¦èƒ½å¤Ÿå‘å±•å‡ºå¯¹é½çš„å‡ ä½•ç†è§£ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨å•ä¸€ä¸ªä½“è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­è‡ªç„¶æ•è·çš„å¤šè§†è§’ä¿¡æ¯è®­ç»ƒè§†è§‰ç¼–ç å™¨ï¼Œå¹¶å¼•å…¥åŸºäºäº¤å‰æ³¨æ„åŠ›çš„æ–°åº¦é‡æ ‡å‡†æ¥é‡åŒ–ä¸åŒæ¨¡å‹å†…éƒ¨è¡¨ç¤ºçš„åŠŸèƒ½å¯¹é½ç¨‹åº¦ï¼ŒåŒæ—¶åœ¨å®¤å†…å¤–ä¸åŒåœºæ™¯ä¸‹å¯¹å¤šä¸ªç‹¬ç«‹ä¸ªä½“æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ä¸‰ä¸ªå…³é”®å‘ç°ï¼šä¸åŒä¸ªä½“ç‹¬ç«‹è®­ç»ƒçš„æ¨¡å‹å‘å±•å‡ºé«˜åº¦å¯¹é½çš„å‡ ä½•ç†è§£ï¼›å•ä¸€ç”Ÿæ¶¯æ¨¡å‹å­¦ä¹ åˆ°çš„å‡ ä½•è¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆè¿ç§»åˆ°æ·±åº¦ä¼°è®¡ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼›ä»…ä½¿ç”¨åŒä¸€äººä¸€å‘¨å†…30å°æ—¶æ•°æ®è®­ç»ƒçš„æ€§èƒ½ä¸ä½¿ç”¨30å°æ—¶å¤šæ ·åŒ–ç½‘ç»œæ•°æ®è®­ç»ƒç›¸å½“ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ç¡®ç«‹äº†å•ä¸€ç”Ÿæ¶¯è¡¨ç¤ºå­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å…±äº«çš„ä¸–ç•Œç»“æ„ä¸ä»…å¯¼è‡´ä¸ªä½“æ¨¡å‹ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œè¿˜ä¸ºè§†è§‰è¡¨ç¤ºå­¦ä¹ æä¾›äº†å¼ºå¤§ä¿¡å·ï¼Œä¸ºä¸ªæ€§åŒ–è§†è§‰æ¨¡å‹å’Œå—é™æ•°æ®åœºæ™¯ä¸‹çš„è¡¨ç¤ºå­¦ä¹ å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_51">ğŸ“„ Abstract</h4>
<p>We introduce the "single-life" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual. We leverage the multiple viewpoints naturally captured within a single life to learn a visual encoder in a self-supervised manner. Our experiments demonstrate three key findings. First, models trained independently on different lives develop a highly aligned geometric understanding. We demonstrate this by training visual encoders on distinct datasets each capturing a different life, both indoors and outdoors, as well as introducing a novel cross-attention-based metric to quantify the functional alignment of the internal representations developed by different models. Second, we show that single-life models learn generalizable geometric representations that effectively transfer to downstream tasks, such as depth estimation, in unseen environments. Third, we demonstrate that training on up to 30 hours from one week of the same person's life leads to comparable performance to training on 30 hours of diverse web data, highlighting the strength of single-life representation learning. Overall, our results establish that the shared structure of the world, both leads to consistency in models trained on individual lives, and provides a powerful signal for visual representation learning.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="53-a-preliminary-study-on-the-promises-and-challenges-of-native-top-k-sparse-attention">[53] <a href="https://arxiv.org/abs/2512.03494">A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention</a></h3>
<p><em>Di Xiu, Hongyin Tang, Bolin Rong, Lizhi Yan, Jingang Wang, Yifan Lu, Xunliang Cai</em></p>
<h4 id="tldr_52">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æŠ¥å‘Šå¯¹Top-kæ³¨æ„åŠ›æœºåˆ¶åœ¨è§£ç å’Œè®­ç»ƒé˜¶æ®µçš„æœ‰æ•ˆæ€§ä¸ç†è®ºæœºåˆ¶è¿›è¡Œäº†åˆæ­¥æ¢ç´¢ï¼ŒéªŒè¯äº†ç²¾ç¡®Top-kè§£ç åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œå¹¶æå‡ºäº†è®­ç»ƒ-æ¨ç†ä¸€è‡´çš„Top-kæ³¨æ„åŠ›ç­–ç•¥ä»¥è¿›ä¸€æ­¥é‡Šæ”¾æ¨¡å‹æ½œåŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_52">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­æ—¥ç›Šæ™®åŠï¼Œä½†å…¶æ¨ç†è®¡ç®—æˆæœ¬å·²æˆä¸ºé˜»ç¢æ™ºèƒ½ä½“å’Œå¤šæ¨¡æ€åº”ç”¨å‘å±•çš„å…³é”®ç“¶é¢ˆï¼Œå› æ­¤éœ€è¦ç ”ç©¶æ›´é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶æ¥é™ä½è®¡ç®—å¼€é”€ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨ç²¾ç¡®Top-kè§£ç æœºåˆ¶ï¼Œåœ¨è§£ç é˜¶æ®µä»…ä¿ç•™ä¸æŸ¥è¯¢ç›¸ä¼¼åº¦æœ€é«˜çš„å…³é”®é”®ä½œä¸ºä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶è¿›ä¸€æ­¥æ¢ç´¢äº†åŸç”ŸTop-kæ³¨æ„åŠ›è®­ç»ƒç­–ç•¥ï¼Œç¡®ä¿è®­ç»ƒä¸æ¨ç†é˜¶æ®µæ“ä½œçš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ç ”ç©¶äº†è¿‘ä¼¼Top-kç®—æ³•ç²¾åº¦å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ç²¾ç¡®Top-kè§£ç åœ¨HELMETå’ŒLongBench v2ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šå…¨æ³¨æ„åŠ›æ€§èƒ½ï¼Œè®­ç»ƒ-æ¨ç†ä¸€è‡´çš„Top-kæ³¨æ„åŠ›ç­–ç•¥æ˜¾è‘—æå‡æ¨¡å‹è¡¨ç°ï¼Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸è¿‘ä¼¼ç®—æ³•ä¿çœŸåº¦å‘ˆæ­£ç›¸å…³ï¼Œä¸”Top-kæ³¨æ„åŠ›SFTæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„ç†µå‡ç°è±¡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ä»ç†µçš„è§’åº¦æä¾›äº†ç†è®ºè§£é‡Šï¼ŒéªŒè¯äº†ä½ç†µçŠ¶æ€æ›´é€‚åº”Top-kè§£ç çš„å‡è®¾ï¼Œä¸ºé™ä½LLMæ¨ç†è®¡ç®—æˆæœ¬æä¾›äº†æœ‰æ•ˆæ–¹æ³•ï¼ŒåŒæ—¶æ­ç¤ºäº†è®­ç»ƒä¸æ¨ç†æœºåˆ¶ä¸€è‡´æ€§å¯¹æ¨¡å‹æ€§èƒ½ä¼˜åŒ–çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_52">ğŸ“„ Abstract</h4>
<p>Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.</p>
<h3 id="54-dz-tdpo-non-destructive-temporal-alignment-for-mutable-state-tracking-in-long-context-dialogue">[54] <a href="https://arxiv.org/abs/2512.03704">DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue</a></h3>
<p><em>Yijun Liao</em></p>
<h4 id="tldr_53">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDZ-TDPOæ¡†æ¶ï¼Œé€šè¿‡éç ´åæ€§å¯¹é½æ–¹æ³•è§£å†³é•¿ä¸Šä¸‹æ–‡å¯¹è¯ç³»ç»Ÿä¸­çš„çŠ¶æ€æƒ¯æ€§é—®é¢˜ï¼Œç»“åˆå†²çªæ„ŸçŸ¥åŠ¨æ€KLçº¦æŸå’Œå¯å­¦ä¹ æ—¶åºæ³¨æ„åŠ›åç½®ï¼Œåœ¨ä¿æŒæ¨¡å‹é€šç”¨èƒ½åŠ›çš„åŒæ—¶å®ç°å“è¶Šçš„å¯¹è¯æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_53">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é•¿ä¸Šä¸‹æ–‡å¯¹è¯ç³»ç»Ÿå­˜åœ¨çŠ¶æ€æƒ¯æ€§é—®é¢˜ï¼Œå³é™æ€çº¦æŸé˜»ç¢æ¨¡å‹è§£å†³ç”¨æˆ·æ„å›¾æ¼”åŒ–ä¸å†å²ä¸Šä¸‹æ–‡ä¹‹é—´çš„å†²çªï¼Œè¿™é™åˆ¶äº†å¯¹è¯ç³»ç»Ÿåœ¨åŠ¨æ€äº¤äº’ä¸­çš„é€‚åº”æ€§å’Œå“åº”å‡†ç¡®æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºDZ-TDPOéç ´åæ€§å¯¹é½æ¡†æ¶ï¼Œç»“åˆå†²çªæ„ŸçŸ¥åŠ¨æ€KLçº¦æŸå’Œå¯å­¦ä¹ æ—¶åºæ³¨æ„åŠ›åç½®ï¼Œé€šè¿‡ç²¾ç¡®çš„æ³¨æ„åŠ›è°ƒèŠ‚è€Œéç ´åæ€§æƒé‡æ›´æ–°æ¥ç¼“è§£çŠ¶æ€æƒ¯æ€§é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨Multi-Session Chatæ•°æ®é›†ä¸Šï¼ŒDZ-TDPOå®ç°äº†86.2%çš„èƒœç‡ï¼ˆPhi-3.5æ¨¡å‹ï¼‰ï¼Œè€ŒQwen2.5-7Bæ¨¡å‹è¾¾åˆ°99.4%çš„è¿‘ä¹å®Œç¾å¯¹é½ä¸”å›°æƒ‘åº¦å¼€é”€å¯å¿½ç•¥ï¼ŒåŒæ—¶ä¿æŒMMLUç­‰é€šç”¨èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†"å®¹é‡-ç¨³å®šæ€§æƒè¡¡"ç°è±¡ï¼šå°æ¨¡å‹éœ€ä»˜å‡º"å¯¹é½ç¨"æ¥å…‹æœå†å²æƒ¯æ€§ï¼Œè€Œå¤§æ¨¡å‹å¯é€šè¿‡ç²¾ç¡®æ³¨æ„åŠ›è°ƒèŠ‚å®ç°å®Œç¾å¯¹é½ï¼Œè¿™ä¸ºé•¿ä¸Šä¸‹æ–‡å¯¹è¯ç³»ç»Ÿè®¾è®¡æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚</p>
<hr />
<h4 id="abstract_53">ğŸ“„ Abstract</h4>
<p>Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a "Capacity-Stability Trade-off": while smaller models incur an "alignment tax" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO</p>
<h3 id="55-improving-alignment-between-human-and-machine-codes-an-empirical-assessment-of-prompt-engineering-for-construct-identification-in-psychology">[55] <a href="https://arxiv.org/abs/2512.03818">Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology</a></h3>
<p><em>Kylie L. Anglin, Stephanie Milan, Brittney Hernandez, Claudia Ventura</em></p>
<h4 id="tldr_54">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç³»ç»ŸåŒ–æ¡†æ¶ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹åœ¨å¿ƒç†å­¦æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°ç»“åˆäººå·¥æŒ‡å¯¼ä¸è‡ªåŠ¨ç”Ÿæˆçš„æç¤ºå·¥ç¨‹æ–¹æ³•èƒ½æ˜¾è‘—æå‡æ¨¡å‹ä¸ä¸“å®¶åˆ¤æ–­çš„ä¸€è‡´æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_54">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å…¶è¾“å‡ºé«˜åº¦ä¾èµ–æç¤ºçš„æªè¾ï¼Œç°æœ‰ç ”ç©¶å¾ˆå°‘å…³æ³¨å¿ƒç†å­¦ç­‰ä¸“ä¸šé¢†åŸŸï¼Œè¿™äº›é¢†åŸŸçš„æ„å¿µå…·æœ‰ç²¾ç¡®çš„ç†è®ºå®šä¹‰ä¸”å¯èƒ½æœªåœ¨é¢„è®­ç»ƒæ•°æ®ä¸­å……åˆ†ä½“ç°ï¼Œå¯¼è‡´æ¨¡å‹ä¸ä¸“å®¶åˆ¤æ–­å­˜åœ¨åå·®ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå®è¯æ¡†æ¶ï¼Œç³»ç»Ÿè¯„ä¼°äº†äº”ç§æç¤ºç­–ç•¥ï¼šåŸºäºä»£ç æœ¬çš„ç»éªŒæç¤ºé€‰æ‹©ã€è‡ªåŠ¨æç¤ºå·¥ç¨‹ã€è§’è‰²æç¤ºã€æ€ç»´é“¾æ¨ç†å’Œè§£é‡Šæ€§æç¤ºï¼Œå¹¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åˆ†ç±»è®¾ç½®ä¸‹è¿›è¡Œå®éªŒéªŒè¯ï¼Œé‡ç‚¹å…³æ³¨æ„å¿µå®šä¹‰ã€ä»»åŠ¡æ¡†æ¶å’Œç¤ºä¾‹é€‰æ‹©ç­‰å…³é”®ç‰¹å¾ã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°è§’è‰²ã€æ€ç»´é“¾å’Œè§£é‡Šæ€§æç¤ºæ— æ³•å®Œå…¨å¼¥è¡¥ä¸è‰¯æªè¾å¸¦æ¥çš„æ€§èƒ½æŸå¤±ï¼Œæœ€å…·å½±å“åŠ›çš„æç¤ºç‰¹å¾æ˜¯æ„å¿µå®šä¹‰ã€ä»»åŠ¡æ¡†æ¶å’Œç¤ºä¾‹é€‰æ‹©ï¼Œåœ¨ä¸‰ä¸ªæ„å¿µå’Œä¸¤ä¸ªæ¨¡å‹ä¸Šï¼Œç»“åˆä»£ç æœ¬æŒ‡å¯¼çš„ç»éªŒæç¤ºé€‰æ‹©ä¸è‡ªåŠ¨æç¤ºå·¥ç¨‹çš„å°‘æ ·æœ¬æç¤ºäº§ç”Ÿäº†ä¸ä¸“å®¶åˆ¤æ–­æœ€ä¸€è‡´çš„ç»“æœã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å»ºè®®ç ”ç©¶äººå‘˜åº”å°½å¯èƒ½ç”Ÿæˆå’Œè¯„ä¼°å¤šç§æç¤ºå˜ä½“ï¼ŒåŒ…æ‹¬äººå·¥è®¾è®¡å’Œè‡ªåŠ¨ç”Ÿæˆçš„ï¼Œå¹¶åŸºäºè®­ç»ƒæ•°æ®é›†ä¸­çš„å®è¯æ€§èƒ½é€‰æ‹©æç¤ºå’Œç¤ºä¾‹ï¼Œåœ¨ä¿ç•™é›†ä¸ŠéªŒè¯æœ€ç»ˆæ–¹æ³•ï¼Œè¿™ä¸ºéœ€è¦ä¸ä¸“å®¶åˆ¤æ–­ä¿æŒä¸€è‡´çš„åœºæ™¯æä¾›äº†å®ç”¨ã€ç³»ç»Ÿä¸”ç†è®ºé©±åŠ¨çš„LLMæç¤ºä¼˜åŒ–æ–¹æ³•ã€‚</p>
<hr />
<h4 id="abstract_54">ğŸ“„ Abstract</h4>
<p>Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.</p>
<h3 id="56-training-and-evaluation-of-guideline-based-medical-reasoning-in-llms">[56] <a href="https://arxiv.org/abs/2512.03838">Training and Evaluation of Guideline-Based Medical Reasoning in LLMs</a></h3>
<p><em>Michael Staniek, Artem Sokolov, Stefan Riezler</em></p>
<h4 id="tldr_55">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºä¸€ç§é€šè¿‡å¾®è°ƒLLMséµå¾ªåŒ»å­¦å…±è¯†æŒ‡å—è¿›è¡Œé€æ­¥æ¨ç†çš„æ–¹æ³•ï¼Œä»¥è§£å†³åŒ»ç–—AIä¸­è§£é‡Šå¯ä¿¡åº¦ä¸è¶³çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ç‰¹å®šåŒ»ç–—é¢†åŸŸï¼ˆå¦‚è„“æ¯’ç—‡-3å®šä¹‰ï¼‰ä¸Šå¾®è°ƒçš„å°æ¨¡å‹åœ¨æ¨ç†æ­£ç¡®æ€§å’Œé¢„æµ‹å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºä½¿ç”¨æ˜¾å¼å®šä¹‰æç¤ºçš„å¤§å‹LLMsã€‚</p>
<hr />
<h4 id="detailed-summary_55">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŒ»ç–—æ—©æœŸé¢„æµ‹çš„æœºå™¨å­¦ä¹ ç ”ç©¶è™½ç„¶å–å¾—äº†çªç ´æ€§æ€§èƒ½ï¼Œä½†è¿‡åº¦å…³æ³¨é¢„æµ‹å‡†ç¡®æ€§å¯¼è‡´å¿½è§†äº†è·å¾—åŒ»ç–—ä»ä¸šè€…ä¿¡ä»»æ‰€éœ€çš„å¯ä¿¡è§£é‡Šã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•´åˆåŒ»å­¦ä¸­æ™®éå­˜åœ¨çš„å…±è¯†æŒ‡å—ï¼Œè¿™äº›æŒ‡å—æä¾›äº†æ ‡å‡†åŒ–çš„æ¨ç†æ­¥éª¤å’Œä¾‹å¤–æƒ…å†µå¤„ç†ï¼Œå¯¹äºç¡®ä¿æ¨¡å‹æ¨ç†çš„å¿ å®æ€§å’Œå¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºå°†åŒ»å­¦å…±è¯†æŒ‡å—å®ä¾‹åŒ–ä¸ºç”µå­å¥åº·è®°å½•ä¸­çš„è¯­è¨€åŒ–æ¨ç†è§„åˆ™ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºå¾®è°ƒæ•°æ®æ¥è®­ç»ƒLLMså­¦ä¹ å…±è¯†è§„åˆ™åŠå…¶ä¾‹å¤–æƒ…å†µã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤æ­¥è¯„ä¼°æ¡†æ¶ï¼šæ¨å¯¼æ­£ç¡®æ€§è¯„ä¼°æ¨¡å‹ä»å‰ææ­£ç¡®æ¨å¯¼ç»“è®ºçš„èƒ½åŠ›ï¼Œä»·å€¼æ­£ç¡®æ€§è¯„ä¼°é¢„æµ‹å€¼ä¸å®é™…æµ‹é‡å€¼çš„ä¸€è‡´æ€§ã€‚é’ˆå¯¹æ—¶é—´åºåˆ—é¢„æµ‹æŒ‘æˆ˜ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºå¤šæ¨¡æ€æ–¹æ³•ï¼Œå°†æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹çš„è¾“å‡ºè¡¨ç¤ºä¸LLMé›†æˆã€‚</p>
<p><strong>Result:</strong> åœ¨è„“æ¯’ç—‡-3å…±è¯†å®šä¹‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡ç‰¹å®šåŒ»ç–—é¢†åŸŸè§„åˆ™å®ä¾‹å¾®è°ƒçš„å°å‹æ¨¡å‹åœ¨æœªè§æ‚£è€…æ•°æ®ä¸Šå®ç°äº†è¿‘ä¹å®Œç¾çš„æ¨å¯¼æ­£ç¡®æ€§ï¼Œæ˜¾è‘—ä¼˜äºä½¿ç”¨æ˜¾å¼å®šä¹‰è¿›è¡Œå•æ ·æœ¬å­¦ä¹ çš„å¤§å‹LLMsä»¥åŠè®­ç»ƒæ—¶åŒ…å«å…±è¯†å®šä¹‰çš„åŒ»å­¦æ–‡æœ¬æ¨¡å‹ã€‚å¤šæ¨¡æ€é›†æˆæ–¹æ³•è¿›ä¸€æ­¥æ”¹å–„äº†ç¨€ç–ã€ä¸è§„åˆ™é‡‡æ ·ä¸´åºŠå˜é‡çš„æ—¶é—´åºåˆ—é¢„æµ‹æ€§èƒ½ï¼Œè§£å†³äº†å‘æœªæ¥æ³›åŒ–çš„æ­£äº¤é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œæ—©æœŸé¢„æµ‹çš„ä¸»è¦ç“¶é¢ˆå¹¶éåˆ†å¸ƒå¤–æ³›åŒ–ï¼Œè€Œæ˜¯å‘æœªæ¥æ—¶é—´æ³›åŒ–çš„æ­£äº¤é—®é¢˜ã€‚é€šè¿‡è¯­è¨€åŒ–è§„åˆ™å®ä¾‹è¿›è¡Œå¾®è°ƒèƒ½å¤Ÿæœ‰æ•ˆæ•™æˆLLMséµå¾ªåŒ»å­¦å…±è¯†æŒ‡å—ï¼Œå®ç°å¿ å®ä¸”å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ã€‚å¤šæ¨¡æ€é›†æˆæ–¹æ³•ä¸ºè§£å†³ä¸´åºŠå˜é‡é¢„æµ‹çš„æ—¶é—´åºåˆ—æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œä¸ºæ„å»ºå¯ä¿¡åŒ»ç–—AIç³»ç»Ÿå¥ å®šäº†æ–¹æ³•è®ºåŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_55">ğŸ“„ Abstract</h4>
<p>Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.</p>
<h3 id="57-jina-vlm-small-multilingual-vision-language-model">[57] <a href="https://arxiv.org/abs/2512.04032">Jina-VLM: Small Multilingual Vision Language Model</a></h3>
<p><em>Andreas Koukounas, Georgios Mastrapas, Florian HÃ¶nicke, Sedigheh Eslami, Guillaume Roncari, Scott Martens, Han Xiao</em></p>
<h4 id="tldr_56">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Jina-VLMï¼Œä¸€ä¸ª24äº¿å‚æ•°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨å¼€æ”¾2Bè§„æ¨¡VLMä¸­å®ç°äº†æœ€å…ˆè¿›çš„å¤šè¯­è¨€è§†è§‰é—®ç­”æ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡æ³¨æ„åŠ›æ± åŒ–è¿æ¥å™¨å°†SigLIP2è§†è§‰ç¼–ç å™¨ä¸Qwen3è¯­è¨€ä¸»å¹²è€¦åˆï¼Œæ”¯æŒä»»æ„åˆ†è¾¨ç‡å›¾åƒçš„ä»¤ç‰Œé«˜æ•ˆå¤„ç†ã€‚</p>
<hr />
<h4 id="detailed-summary_56">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰2Bè§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸è¶³é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒç«äº‰åŠ›çš„çº¯æ–‡æœ¬æ€§èƒ½çš„åŒæ—¶ï¼Œéœ€è¦æå‡å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚å½“å‰å¼€æ”¾2Bè§„æ¨¡VLMåœ¨å¤šè¯­è¨€è¯„ä¼°ä¸­è¡¨ç°æœ‰é™ï¼Œéœ€è¦æ›´é«˜æ•ˆçš„æ¶æ„æ¥å¤„ç†ä»»æ„åˆ†è¾¨ç‡å›¾åƒã€‚</p>
<p><strong>Method:</strong> Jina-VLMé‡‡ç”¨SigLIP2è§†è§‰ç¼–ç å™¨ä¸Qwen3è¯­è¨€ä¸»å¹²ç›¸ç»“åˆçš„æ¶æ„ï¼Œé€šè¿‡æ³¨æ„åŠ›æ± åŒ–è¿æ¥å™¨å®ç°è§†è§‰ä¸è¯­è¨€æ¨¡æ€çš„èåˆã€‚è¯¥è¿æ¥å™¨è®¾è®¡æ”¯æŒä»»æ„åˆ†è¾¨ç‡å›¾åƒçš„ä»¤ç‰Œé«˜æ•ˆå¤„ç†ï¼Œå‡å°‘äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹å‚æ•°è§„æ¨¡ä¸º24äº¿ã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†VQAåŸºå‡†æµ‹è¯•å’Œå¤šè¯­è¨€è¯„ä¼°ä¸­ï¼ŒJina-VLMè¶…è¶Šäº†åŒç±»å¯æ¯”æ¨¡å‹ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥æ¨¡å‹åœ¨ä¿æŒç«äº‰åŠ›çš„çº¯æ–‡æœ¬æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†å¤šè¯­è¨€è§†è§‰é—®ç­”èƒ½åŠ›ï¼Œåœ¨2Bè§„æ¨¡å¼€æ”¾VLMä¸­ç¡®ç«‹äº†æ–°çš„æ€§èƒ½æ ‡æ†ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ³¨æ„åŠ›æ± åŒ–è¿æ¥å™¨å°†å…ˆè¿›çš„è§†è§‰ç¼–ç å™¨ä¸è¯­è¨€ä¸»å¹²ç›¸ç»“åˆï¼Œå¯ä»¥åœ¨ä¸­ç­‰å‚æ•°è§„æ¨¡ä¸‹å®ç°å“è¶Šçš„å¤šè¯­è¨€è§†è§‰ç†è§£èƒ½åŠ›ã€‚è¿™ä¸€æ¶æ„ä¸ºå¼€å‘é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°æ€è·¯ï¼Œå¹³è¡¡äº†è®¡ç®—æ•ˆç‡ä¸æ€§èƒ½è¡¨ç°ï¼Œä¸ºå®é™…åº”ç”¨éƒ¨ç½²åˆ›é€ äº†æ¡ä»¶ã€‚</p>
<hr />
<h4 id="abstract_56">ğŸ“„ Abstract</h4>
<p>We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="58-evaluating-generalization-capabilities-of-llm-based-agents-in-mixed-motive-scenarios-using-concordia">[58] <a href="https://arxiv.org/abs/2512.03318">Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia</a></h3>
<p><em>Chandler Smith, Marwa Abdulhai, Manfred Diaz, Marko Tesic, Rakshit S. Trivedi, Alexander Sasha Vezhnevets, Lewis Hammond, Jesse Clifton, Minsuk Chang, Edgar A. DuÃ©Ã±ez-GuzmÃ¡n, John P. Agapiou, Jayd Matyas, Danny Karmon, Akash Kundu, Aliaksei Korshuk, Ananya Ananya, Arrasy Rahman, Avinaash Anand Kulandaivel, Bain McHale, Beining Zhang, Buyantuev Alexander, Carlos Saith Rodriguez Rojas, Caroline Wang, Chetan Talele, Chenao Liu, Chichen Lin, Diana Riazi, Di Yang Shi, Emanuel Tewolde, Elizaveta Tennant, Fangwei Zhong, Fuyang Cui, Gang Zhao, Gema ParreÃ±o Piqueras, Hyeonggeun Yun, Ilya Makarov, Jiaxun Cui, Jebish Purbey, Jim Dilkes, Jord Nguyen, Lingyun Xiao, Luis Felipe Giraldo, Manuela Chacon-Chamorro, Manuel Sebastian Rios Beltran, Marta Emili GarcÃ­a Segura, Mengmeng Wang, Mogtaba Alim, Nicanor Quijano, Nico Schiavone, Olivia Macmillan-Scott, Oswaldo PeÃ±a, Peter Stone, Ram Mohan Rao Kadiyala, Rolando Fernandez, Ruben Manrique, Sunjia Lu, Sheila A. McIlraith, Shamika Dhuri, Shuqing Shi, Siddhant Gupta, Sneheel Sarangi, Sriram Ganapathi Subramanian, Taehun Cha, Toryn Q. Klassen, Wenming Tu, Weijian Fan, Wu Ruiyang, Xue Feng, Yali Du, Yang Liu, Yiding Wang, Yipeng Kang, Yoonchang Sung, Yuxuan Chen, Zhaowei Zhang, Zhihan Wang, Zhiqiang Wu, Ziang Chen, Zilong Zheng, Zixia Jia, Ziyan Wang, Dylan Hadfield-Menell, Natasha Jaques, Tim Baarslag, Jose Hernandez-Orallo, Joel Z. Leibo</em></p>
<h4 id="tldr_57">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯„ä¼°LLMæ™ºèƒ½ä½“åœ¨é›¶æ ·æœ¬æ··åˆåŠ¨æœºç¯å¢ƒä¸­åˆä½œèƒ½åŠ›çš„æ–¹æ³•ï¼Œä½¿ç”¨Concordiaè‡ªç„¶è¯­è¨€å¤šæ™ºèƒ½ä½“ä»¿çœŸç¯å¢ƒï¼Œæ­ç¤ºäº†å½“å‰æ™ºèƒ½ä½“åœ¨éœ€è¦è¯´æœå’Œè§„èŒƒæ‰§è¡Œçš„åœºæ™¯ä¸­å­˜åœ¨æ˜¾è‘—æ³›åŒ–å·®è·ã€‚</p>
<hr />
<h4 id="detailed-summary_57">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“åœ¨ç¤¾äº¤äº’åŠ¨ä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œå¹¶è¶Šæ¥è¶Šå¤šåœ°éƒ¨ç½²åœ¨ä¸äººç±»å’Œäººå·¥æ™ºèƒ½ä½“äº¤äº’çš„åœºæ™¯ä¸­ï¼Œä½†ç°æœ‰è¯„ä¼°æ–¹æ³•æ— æ³•è¡¡é‡è¿™äº›èƒ½åŠ›åœ¨æ–°é¢–ç¤¾äº¤æƒ…å¢ƒä¸­çš„æ³›åŒ–è¡¨ç°ï¼Œè¿™æ„æˆäº†LLMæ™ºèƒ½ä½“å‘å±•çš„å…³é”®å‰æ²¿æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†åŸºäºConcordiaè‡ªç„¶è¯­è¨€å¤šæ™ºèƒ½ä½“ä»¿çœŸç¯å¢ƒçš„è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡æµ‹è¯•æ™ºèƒ½ä½“åœ¨ä¸åŒåˆä½œä¼™ä¼´å’Œæƒ…å¢ƒä¸­è¯†åˆ«å¹¶åˆ©ç”¨äº’åˆ©æœºä¼šçš„èƒ½åŠ›ï¼Œæ¥è¡¡é‡å…¶ä¸€èˆ¬åˆä½œæ™ºèƒ½ï¼Œè¯¥æ–¹æ³•ç‰¹åˆ«å…³æ³¨é›¶æ ·æœ¬æ··åˆåŠ¨æœºç¯å¢ƒä¸­çš„åˆä½œè¡¨ç°è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> åŸºäºNeurIPS 2024 Concordiaç«èµ›çš„å®è¯ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ™ºèƒ½ä½“èƒ½åŠ›ä¸ç¨³å¥æ³›åŒ–æ‰€éœ€æ°´å¹³å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦è¯´æœå’Œè§„èŒƒæ‰§è¡Œçš„åœºæ™¯ä¸­ï¼Œæ™ºèƒ½ä½“åœ¨ä»è°ˆåˆ¤åˆ°é›†ä½“è¡ŒåŠ¨é—®é¢˜çš„å¤šæ ·åŒ–æƒ…å¢ƒä¸­å®ç°äº’åˆ©çš„èƒ½åŠ›æœ‰é™ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†LLMæ™ºèƒ½ä½“åœ¨å¤æ‚ç¤¾äº¤äº’åŠ¨ä¸­çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå¼ºè°ƒäº†å¼€å‘æ›´é²æ£’åˆä½œæ™ºèƒ½çš„å¿…è¦æ€§ï¼Œä¸ºæœªæ¥æ™ºèƒ½ä½“è¯„ä¼°æ¡†æ¶è®¾è®¡æä¾›äº†é‡è¦åŸºå‡†ï¼Œå¹¶æŒ‡å‡ºäº†åœ¨è¯´æœå’Œè§„èŒƒæ‰§è¡Œç­‰å…³é”®ç¤¾äº¤èƒ½åŠ›æ–¹é¢çš„æ”¹è¿›æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_57">ğŸ“„ Abstract</h4>
<p>Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.</p>
<h3 id="59-multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents">[59] <a href="https://arxiv.org/abs/2512.03438">Multimodal Reinforcement Learning with Agentic Verifier for AI Agents</a></h3>
<p><em>Reuben Tan, Baolin Peng, Zhengyuan Yang, Hao Cheng, Oier Mees, Theodore Zhao, Andrea Tupini, Isar Meijier, Qianhui Wu, Yuncong Yang, Lars Liden, Yu Gu, Sheng Zhang, Xiaodong Liu, Lijuan Wang, Marc Pollefeys, Yong Jae Lee, Jianfeng Gao</em></p>
<h4 id="tldr_58">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Argosï¼ˆAgentic Reward for Grounded &amp; Objective Scoringï¼‰ï¼Œä¸€ç§ç”¨äºè®­ç»ƒå¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„å¥–åŠ±æ™ºèƒ½ä½“ï¼Œé€šè¿‡åŠ¨æ€é€‰æ‹©æ•™å¸ˆæ¨¡å‹å’Œè§„åˆ™è¯„åˆ†å‡½æ•°æ¥æä¾›ç»†ç²’åº¦å¥–åŠ±ä¿¡å·ï¼Œä»è€Œè§£å†³å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ä¸­ç¨€ç–å¥–åŠ±å’Œå¥–åŠ±é»‘å®¢é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_58">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„æ™ºèƒ½ä½“æ¨ç†æ¨¡å‹é€šå¸¸ä»…ä¾èµ–åŸºäºæœ€ç»ˆç­”æ¡ˆçš„ç¨€ç–å¥–åŠ±è¿›è¡Œä¼˜åŒ–ï¼Œè¿™ç§å¥–åŠ±æœºåˆ¶æ— æ³•æä¾›ç»†ç²’åº¦çš„å­¦ä¹ æŒ‡å¯¼ã€‚ä¸åŒæ ·æœ¬éœ€è¦ä¸åŒçš„è¯„åˆ†å‡½æ•°ï¼Œè€Œæ•™å¸ˆæ¨¡å‹æä¾›çš„å¥–åŠ±ä¿¡å·å¯èƒ½å­˜åœ¨å™ªå£°ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½æå‡ã€‚</p>
<p><strong>Method:</strong> Argosæ–¹æ³•ä¸ºæ¯ä¸ªæ ·æœ¬ä»æ•™å¸ˆæ¨¡å‹æ´¾ç”Ÿçš„è¯„åˆ†å‡½æ•°å’ŒåŸºäºè§„åˆ™çš„è¯„åˆ†å‡½æ•°æ± ä¸­åŠ¨æ€é€‰æ‹©ï¼ŒåŒæ—¶è¯„ä¼°ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šæœ€ç»ˆå“åº”å‡†ç¡®æ€§ã€å¼•ç”¨å®ä½“å’ŒåŠ¨ä½œçš„æ—¶ç©ºå®šä½è´¨é‡ï¼Œä»¥åŠæ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨ç›‘ç£å¾®è°ƒæ•°æ®ç­›é€‰å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒé˜¶æ®µå‡åº”ç”¨æ™ºèƒ½ä½“éªŒè¯æœºåˆ¶ã€‚</p>
<p><strong>Result:</strong> ä½¿ç”¨Argosè®­ç»ƒçš„æ¨¡å‹åœ¨ç©ºé—´æ¨ç†ã€è§†è§‰å¹»è§‰ä»¥åŠæœºå™¨äººå’Œå…·èº«AIåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚å®éªŒè¡¨æ˜ï¼Œä»…ä¾èµ–é«˜åº¦ç­›é€‰æ¨ç†æ•°æ®çš„ç›‘ç£å¾®è°ƒåè®­ç»ƒæ˜¯ä¸å¤Ÿçš„ï¼Œæ²¡æœ‰åœ¨çº¿éªŒè¯çš„æ™ºèƒ½ä½“åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä¼šå´©æºƒä¸ºæœªæ¥åœ°æ°”çš„è§£å†³æ–¹æ¡ˆã€‚Argosè¿˜èƒ½æœ‰æ•ˆå‡å°‘å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±é»‘å®¢ç°è±¡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»†ç²’åº¦ã€å¤šç»´åº¦å¥–åŠ±æœºåˆ¶å¯¹å¤šæ¨¡æ€æ¨ç†æ™ºèƒ½ä½“è®­ç»ƒçš„é‡è¦æ€§ï¼ŒArgosé€šè¿‡å¸•ç´¯æ‰˜æœ€ä¼˜æ€§æ¦‚å¿µæä¾›äº†ç†è®ºä¾æ®ã€‚ç ”ç©¶å¼ºè°ƒäº†åœ¨çº¿éªŒè¯åœ¨é˜²æ­¢æ™ºèƒ½ä½“å´©æºƒä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±è®¾è®¡æä¾›äº†æ–°èŒƒå¼ã€‚</p>
<hr />
<h4 id="abstract_58">ğŸ“„ Abstract</h4>
<p>Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded &amp; Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.</p>
<h3 id="60-memverse-multimodal-memory-for-lifelong-learning-agents">[60] <a href="https://arxiv.org/abs/2512.03627">MemVerse: Multimodal Memory for Lifelong Learning Agents</a></h3>
<p><em>Junming Liu, Yifei Sun, Weihua Cheng, Haodong Lei, Yirong Chen, Licheng Wen, Xuemeng Yang, Daocheng Fu, Pinlong Cai, Nianchen Deng, Yi Yu, Shuyue Hu, Botian Shi, Ding Wang</em></p>
<h4 id="tldr_59">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MemVerseï¼Œä¸€ç§æ¨¡å‹æ— å…³çš„å³æ’å³ç”¨è®°å¿†æ¡†æ¶ï¼Œé€šè¿‡æ¡¥æ¥å¿«é€Ÿå‚æ•°åŒ–å›å¿†ä¸åˆ†å±‚æ£€ç´¢å¼è®°å¿†ï¼Œå®ç°äº†å¯æ‰©å±•çš„è‡ªé€‚åº”å¤šæ¨¡æ€æ™ºèƒ½ï¼Œæ˜¾è‘—æå‡äº†æ™ºèƒ½ä½“çš„è®°å¿†ä¸æ¨ç†èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_59">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤§è§„æ¨¡è¯­è¨€å’Œè§†è§‰æ¨¡å‹å‘å±•è¿…é€Ÿï¼Œä½†AIæ™ºèƒ½ä½“ä»é¢ä¸´æ ¹æœ¬æ€§é™åˆ¶ï¼šæ— æ³•æœ‰æ•ˆè®°å¿†ã€‚ç¼ºä¹å¯é è®°å¿†å¯¼è‡´æ™ºèƒ½ä½“ç¾éš¾æ€§é—å¿˜è¿‡å»ç»éªŒï¼Œéš¾ä»¥è¿›è¡Œé•¿æ—¶ç¨‹æ¨ç†ï¼Œåœ¨å¤šæ¨¡æ€æˆ–äº¤äº’ç¯å¢ƒä¸­æ— æ³•ä¿æŒè¿è´¯æ“ä½œã€‚</p>
<p><strong>Method:</strong> MemVerseé‡‡ç”¨æ¨¡å‹æ— å…³çš„å³æ’å³ç”¨æ¶æ„ï¼Œå°†å¿«é€Ÿå‚æ•°åŒ–å›å¿†ä¸åˆ†å±‚æ£€ç´¢å¼è®°å¿†ç›¸ç»“åˆã€‚è¯¥æ¡†æ¶ç»´æŠ¤çŸ­æœŸè®°å¿†å¤„ç†è¿‘æœŸä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶å°†åŸå§‹å¤šæ¨¡æ€ç»éªŒè½¬åŒ–ä¸ºç»“æ„åŒ–é•¿æœŸè®°å¿†ï¼Œç»„ç»‡ä¸ºåˆ†å±‚çŸ¥è¯†å›¾è°±ã€‚é€šè¿‡å‘¨æœŸæ€§è’¸é¦æœºåˆ¶å°†é•¿æœŸè®°å¿†ä¸­çš„å…³é”®çŸ¥è¯†å‹ç¼©åˆ°å‚æ•°åŒ–æ¨¡å‹ä¸­ï¼Œå®ç°å¿«é€Ÿå¯å¾®å›å¿†å¹¶ä¿æŒå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMemVerseæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€æ¨ç†å’ŒæŒç»­å­¦ä¹ æ•ˆç‡ã€‚è¯¥æ¡†æ¶ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æ‰©å±•äº¤äº’ä¸­è®°å¿†ã€é€‚åº”å’Œè¿›è¡Œè¿è´¯æ¨ç†ï¼Œåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> MemVerseä¸ºè§£å†³æ™ºèƒ½ä½“è®°å¿†é—®é¢˜æä¾›äº†æœ‰æ•ˆæ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚è®°å¿†ç»“æ„å’Œå‘¨æœŸæ€§è’¸é¦æœºåˆ¶å®ç°äº†å¯æ‰©å±•çš„è‡ªé€‚åº”è®°å¿†ç³»ç»Ÿã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºå…·æœ‰é•¿æœŸè®°å¿†èƒ½åŠ›çš„AIæ™ºèƒ½ä½“å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå¯¹å¤šæ¨¡æ€äº¤äº’å’ŒæŒç»­å­¦ä¹ å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_59">ğŸ“„ Abstract</h4>
<p>Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember. Without reliable memory, agents catastrophically forget past experiences, struggle with long-horizon reasoning, and fail to operate coherently in multimodal or interactive environments. We introduce MemVerse, a model-agnostic, plug-and-play memory framework that bridges fast parametric recall with hierarchical retrieval-based memory, enabling scalable and adaptive multimodal intelligence. MemVerse maintains short-term memory for recent context while transforming raw multimodal experiences into structured long-term memories organized as hierarchical knowledge graphs. This design supports continual consolidation, adaptive forgetting, and bounded memory growth. To handle real-time demands, MemVerse introduces a periodic distillation mechanism that compresses essential knowledge from long-term memory into the parametric model, allowing fast, differentiable recall while preserving interpretability. Extensive experiments demonstrate that MemVerse significantly improves multimodal reasoning and continual learning efficiency, empowering agents to remember, adapt, and reason coherently across extended interactions.</p>
<h3 id="61-omni-autothink-adaptive-multimodal-reasoning-via-reinforcement-learning">[61] <a href="https://arxiv.org/abs/2512.03783">Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning</a></h3>
<p><em>Dongchao Yang, Songxiang Liu, Disong Wang, Yuanyuan Wang, Guanglu Wan, Helen Meng</em></p>
<h4 id="tldr_60">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºOmni-AutoThinkï¼Œä¸€ç§è‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦æ¥è§£å†³ç°æœ‰Omniæ¨¡å‹åœ¨ç®€å•é—®é¢˜ä¸Šè¿‡åº¦æ¨ç†æˆ–å¤æ‚é—®é¢˜ä¸Šæ¨ç†ä¸è¶³çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€è‡ªé€‚åº”æ¨ç†æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_60">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰Omniæ¨¡å‹åœ¨å¤šæ¨¡æ€æ„ŸçŸ¥å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†æ¨ç†è¡Œä¸ºä»ç„¶åƒµåŒ–ï¼Œè¦ä¹ˆåœ¨ç®€å•é—®é¢˜ä¸Šè¿‡åº¦æ¨ç†ï¼Œè¦ä¹ˆåœ¨éœ€è¦æ¨ç†æ—¶æ¨ç†ä¸è¶³ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„å®é™…åº”ç”¨æ•ˆæœå’Œæ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> æå‡ºä¸¤é˜¶æ®µè‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µé‡‡ç”¨è‡ªé€‚åº”ç›‘ç£å¾®è°ƒï¼Œä½¿ç”¨å¤§è§„æ¨¡æ¨ç†å¢å¼ºæ•°æ®èµ‹äºˆæ¨¡å‹åŸºç¡€æ¨ç†èƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨è‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ ï¼ŒåŸºäºä»»åŠ¡å¤æ‚åº¦å’Œå¥–åŠ±åé¦ˆä¼˜åŒ–æ¨ç†è¡Œä¸ºï¼Œå¹¶æ„å»ºäº†æ¶µç›–æ–‡æœ¬ã€æ–‡æœ¬-éŸ³é¢‘ã€æ–‡æœ¬-è§†è§‰ã€æ–‡æœ¬-éŸ³é¢‘-è§†è§‰æ¨¡æ€çš„å…¨é¢è‡ªé€‚åº”æ¨ç†åŸºå‡†ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åœ¨è‡ªé€‚åº”æ¨ç†æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…ˆå‰åŸºçº¿æ–¹æ³•ï¼Œæ„å»ºçš„åŸºå‡†æä¾›äº†è®­ç»ƒå’Œè¯„ä¼°åˆ†å‰²ï¼Œæ”¯æŒå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†è‡ªé€‚åº”æ¨ç†æ¡†æ¶åœ¨æå‡Omniæ¨¡å‹æ¨ç†æ•ˆç‡å’Œæ•ˆæœæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ›´çµæ´»ã€é«˜æ•ˆçš„æ¨ç†æœºåˆ¶ï¼Œæ‰€æœ‰åŸºå‡†æ•°æ®å’Œä»£ç å°†å…¬å¼€é‡Šæ”¾ä»¥ä¿ƒè¿›åç»­ç ”ç©¶ã€‚</p>
<hr />
<h4 id="abstract_60">ğŸ“„ Abstract</h4>
<p>Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.</p>
  </article>
</body>
</html>
