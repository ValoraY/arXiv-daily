<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 32]
- [cs.CL](#cs.CL) [Total: 11]
- [cs.AI](#cs.AI) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Bias Detection and Rotation-Robustness Mitigation in Vision-Language Models and Generative Image Models](https://arxiv.org/abs/2601.08860)
*Tarannum Mithila*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶åˆ†æäº†è§†è§‰è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆå¼å›¾åƒæ¨¡å‹åœ¨è¾“å…¥å˜æ¢ä¸‹çš„é²æ£’æ€§å’Œå…¬å¹³æ€§é—®é¢˜ï¼Œç‰¹åˆ«å…³æ³¨å›¾åƒæ—‹è½¬å¼•èµ·çš„åå·®ä¼ æ’­ï¼Œå¹¶æå‡ºäº†ç»“åˆæ•°æ®å¢å¼ºã€è¡¨ç¤ºå¯¹é½å’Œæ¨¡å‹æ­£åˆ™åŒ–çš„æ—‹è½¬é²æ£’ç¼“è§£ç­–ç•¥ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆå¼å›¾åƒæ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½ï¼Œä½†å…¶åœ¨è¾“å…¥å˜æ¢ä¸‹çš„é²æ£’æ€§å’Œå…¬å¹³æ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è°ƒæŸ¥æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€å’Œç”Ÿæˆæ¨¡å‹ä¸­çš„åå·®ä¼ æ’­å’Œé²æ£’æ€§é€€åŒ–é—®é¢˜ï¼Œç‰¹åˆ«å…³æ³¨å›¾åƒæ—‹è½¬å’Œåˆ†å¸ƒåç§»å¯¹æ¨¡å‹é¢„æµ‹ã€ç½®ä¿¡åº¦æ ¡å‡†å’Œäººå£ç»Ÿè®¡åå·®æ¨¡å¼çš„å½±å“ã€‚

**Method:** ä¸ºè§£å†³æ—‹è½¬å¼•èµ·çš„é²æ£’æ€§å’Œå…¬å¹³æ€§é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†æ—‹è½¬é²æ£’ç¼“è§£ç­–ç•¥ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ•°æ®å¢å¼ºã€è¡¨ç¤ºå¯¹é½å’Œæ¨¡å‹çº§æ­£åˆ™åŒ–æŠ€æœ¯ã€‚è¿™äº›æ–¹æ³•æ—¨åœ¨é€šè¿‡å¤šå±‚æ¬¡çš„å¹²é¢„æ¥å¢å¼ºæ¨¡å‹å¯¹æ—‹è½¬å˜æ¢çš„é€‚åº”æ€§ï¼ŒåŒæ—¶å‡å°‘åå·®æ”¾å¤§æ•ˆåº”ã€‚

**Result:** åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ï¼ŒåŒæ—¶å‡å°‘äº†åå·®æ”¾å¤§ï¼Œä¸”æ²¡æœ‰ç‰ºç‰²æ•´ä½“æ€§èƒ½ã€‚ç ”ç©¶é‡åŒ–äº†æ—‹è½¬è¯±å¯¼æ‰°åŠ¨å¯¹æ¨¡å‹é¢„æµ‹å’Œç½®ä¿¡åº¦æ ¡å‡†çš„å…·ä½“å½±å“ï¼Œå¹¶å±•ç¤ºäº†ç¼“è§£ç­–ç•¥åœ¨æ”¹å–„æ¨¡å‹å…¬å¹³æ€§å’Œå¯é æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€ç³»ç»Ÿçš„å…³é”®å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹è¾“å…¥å˜æ¢çš„æ•æ„Ÿæ€§å¯¼è‡´çš„é²æ£’æ€§å’Œå…¬å¹³æ€§é—®é¢˜ã€‚ç ”ç©¶æä¾›çš„å®ç”¨ç¼“è§£æŠ€æœ¯ä¸ºæ„å»ºæ›´å¯é å’Œå…¬å¹³çš„AIæ¨¡å‹æä¾›äº†æ–¹æ³•è®ºæ”¯æŒï¼Œå¼ºè°ƒäº†åœ¨æ¨¡å‹å¼€å‘ä¸­è€ƒè™‘å˜æ¢é²æ£’æ€§å’Œåå·®ç¼“è§£çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Vision-Language Models (VLMs) and generative image models have achieved remarkable performance across multimodal tasks, yet their robustness and fairness under input transformations remain insufficiently explored. This work investigates bias propagation and robustness degradation in state-of-the-art vision-language and generative models, with a particular focus on image rotation and distributional shifts. We analyze how rotation-induced perturbations affect model predictions, confidence calibration, and demographic bias patterns. To address these issues, we propose rotation-robust mitigation strategies that combine data augmentation, representation alignment, and model-level regularization. Experimental results across multiple datasets demonstrate that the proposed methods significantly improve robustness while reducing bias amplification without sacrificing overall performance. This study highlights critical limitations of current multimodal systems and provides practical mitigation techniques for building more reliable and fair AI models.


### [2] [Residual Cross-Modal Fusion Networks for Audio-Visual Navigation](https://arxiv.org/abs/2601.08868)
*Yi Wang, Yinfeng Yu, Bin Ren*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æ®‹å·®èåˆç½‘ç»œï¼ˆCRFNï¼‰ï¼Œç”¨äºè§£å†³éŸ³é¢‘-è§†è§‰å…·èº«å¯¼èˆªä¸­çš„å¤šæ¨¡æ€èåˆæŒ‘æˆ˜ï¼Œé€šè¿‡åŒå‘æ®‹å·®äº¤äº’å®ç°äº’è¡¥å»ºæ¨¡å’Œç»†ç²’åº¦å¯¹é½ï¼Œæ˜¾è‘—æå‡äº†è·¨åŸŸæ³›åŒ–æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éŸ³é¢‘-è§†è§‰å…·èº«å¯¼èˆªä»»åŠ¡çš„å…³é”®æŒ‘æˆ˜åœ¨äºå¼‚æ„ç‰¹å¾åœ¨å¤šæ¨¡æ€èåˆè¿‡ç¨‹ä¸­çš„æœ‰æ•ˆäº¤äº’å»ºæ¨¡ï¼Œä»¥é¿å…å•æ¨¡æ€ä¸»å¯¼æˆ–ä¿¡æ¯é€€åŒ–é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨åŸŸåœºæ™¯ä¸‹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–ç®€å•çš„æ‹¼æ¥æˆ–æ³¨æ„åŠ›é—¨æ§æœºåˆ¶ï¼Œéš¾ä»¥å®ç°æ¨¡æ€é—´çš„äº’è¡¥å»ºæ¨¡å’Œç»†ç²’åº¦å¯¹é½ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†è·¨æ¨¡æ€æ®‹å·®èåˆç½‘ç»œï¼ˆCRFNï¼‰ï¼Œé€šè¿‡åœ¨éŸ³é¢‘å’Œè§†è§‰æµä¹‹é—´å¼•å…¥åŒå‘æ®‹å·®äº¤äº’æ¥å®ç°äº’è¡¥å»ºæ¨¡å’Œç»†ç²’åº¦å¯¹é½ï¼ŒåŒæ—¶ä¿æŒå„è‡ªè¡¨ç¤ºçš„ç‹¬ç«‹æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ®‹å·®è¿æ¥æ˜¾å¼å»ºæ¨¡è·¨æ¨¡æ€äº¤äº’ï¼Œå¹¶èå…¥ç¨³å®šåŒ–æŠ€æœ¯ä»¥æ”¹å–„æ”¶æ•›æ€§å’Œé²æ£’æ€§ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçš„æ‹¼æ¥æˆ–æ³¨æ„åŠ›é—¨æ§æ–¹æ³•ã€‚

**Result:** åœ¨Replicaå’ŒMatterport3Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCRFNæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„å¤šæ¨¡æ€èåˆåŸºçº¿æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºæ›´å¼ºçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®éªŒè¿˜å‘ç°æ™ºèƒ½ä½“åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå·®å¼‚åŒ–çš„æ¨¡æ€ä¾èµ–æ€§ï¼Œè¿™ä¸€ç°è±¡ä¸ºç†è§£å…·èº«æ™ºèƒ½ä½“çš„è·¨æ¨¡æ€åä½œæœºåˆ¶æä¾›äº†æ–°è§†è§’ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ä»…æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è·¨æ¨¡æ€èåˆæ¡†æ¶ï¼Œè¿˜æ­ç¤ºäº†å…·èº«æ™ºèƒ½ä½“åœ¨ä¸åŒç¯å¢ƒä¸­çš„æ¨¡æ€ä¾èµ–å·®å¼‚ï¼Œä¸ºç†è§£å¤šæ¨¡æ€åä½œæœºåˆ¶æä¾›äº†é‡è¦è§è§£ã€‚CRFNçš„æˆåŠŸè¡¨æ˜æ˜¾å¼å»ºæ¨¡è·¨æ¨¡æ€æ®‹å·®äº¤äº’æ˜¯å®ç°ç¨³å¥å¤šæ¨¡æ€èåˆçš„æœ‰æ•ˆé€”å¾„ï¼Œä¸ºæœªæ¥å…·èº«å¯¼èˆªç³»ç»Ÿçš„è®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Audio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the interaction between heterogeneous features during multimodal fusion, so as to avoid single-modality dominance or information degradation, particularly in cross-domain scenarios. To address this, we propose a Cross-Modal Residual Fusion Network, which introduces bidirectional residual interactions between audio and visual streams to achieve complementary modeling and fine-grained alignment, while maintaining the independence of their representations. Unlike conventional methods that rely on simple concatenation or attention gating, CRFN explicitly models cross-modal interactions via residual connections and incorporates stabilization techniques to improve convergence and robustness. Experiments on the Replica and Matterport3D datasets demonstrate that CRFN significantly outperforms state-of-the-art fusion baselines and achieves stronger cross-domain generalization. Notably, our experiments also reveal that agents exhibit differentiated modality dependence across different datasets. The discovery of this phenomenon provides a new perspective for understanding the cross-modal collaboration mechanism of embodied agents.


### [3] [Thermo-LIO: A Novel Multi-Sensor Integrated System for Structural Health Monitoring](https://arxiv.org/abs/2601.08977)
*Chao Yang, Haoyuan Zheng, Yue Ma*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºThermo-LIOï¼Œä¸€ç§ç»“åˆçƒ­æˆåƒä¸é«˜åˆ†è¾¨ç‡LiDARçš„å¤šä¼ æ„Ÿå™¨ç³»ç»Ÿï¼Œé€šè¿‡å¤šæ¨¡æ€æ•°æ®èåˆä¸LiDARæƒ¯æ€§é‡Œç¨‹è®¡é›†æˆï¼Œæ˜¾è‘—æå‡äº†å¤§å‹åœŸæœ¨åŸºç¡€è®¾æ–½ç»“æ„å¥åº·ç›‘æµ‹çš„ç¼ºé™·æ£€æµ‹ç²¾åº¦ä¸è¦†ç›–èŒƒå›´ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»ŸäºŒç»´çƒ­æˆåƒæŠ€æœ¯åœ¨å»ºç­‘é¢†åŸŸç¼ºé™·æ£€æµ‹ä¸­è™½å…·éä¾µå…¥æ€§ä¼˜åŠ¿ï¼Œä½†éš¾ä»¥æœ‰æ•ˆè¯„ä¼°å¤æ‚å‡ ä½•ç»“æ„ã€ä¸å¯è¾¾åŒºåŸŸåŠæ¬¡è¡¨é¢ç¼ºé™·ï¼Œé™åˆ¶äº†å…¶åœ¨ç»“æ„å¥åº·ç›‘æµ‹ä¸­çš„å…¨é¢åº”ç”¨ã€‚

**Method:** ç ”ç©¶å¼€å‘äº†çƒ­æˆåƒä¸LiDARçš„å¤šæ¨¡æ€èåˆæ–¹æ³•ï¼Œå®ç°ç²¾ç¡®æ ¡å‡†ä¸åŒæ­¥æ•°æ®æµä»¥æ„å»ºå»ºç­‘ç‰©æ¸©åº¦åˆ†å¸ƒç²¾ç¡®è¡¨å¾ï¼›è¿›ä¸€æ­¥å°†è¯¥èåˆæ–¹æ³•ä¸LiDARæƒ¯æ€§é‡Œç¨‹è®¡é›†æˆï¼Œå®ç°å¤§è§„æ¨¡ç»“æ„çš„å…¨è¦†ç›–ç›‘æµ‹ä¸æ¸©åº¦å˜åŒ–è¿½è¸ªã€‚

**Result:** åœ¨æ¡¥æ¢å’Œå…å ‚å»ºç­‘çš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒThermo-LIOç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•èƒ½æ›´å‡†ç¡®åœ°æ£€æµ‹è¯¦ç»†çƒ­å¼‚å¸¸ä¸ç»“æ„ç¼ºé™·ï¼Œç³»ç»Ÿæå‡äº†è¯Šæ–­ç²¾åº¦ï¼Œæ”¯æŒå®æ—¶å¤„ç†å¹¶æ‰©å±•äº†æ£€æµ‹è¦†ç›–èŒƒå›´ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡æ€ä¼ æ„Ÿå™¨é›†æˆåœ¨æ¨è¿›å¤§å‹åœŸæœ¨åŸºç¡€è®¾æ–½ç»“æ„å¥åº·ç›‘æµ‹æ–¹æ³•ä¸­çš„å…³é”®ä½œç”¨ï¼Œç³»ç»Ÿé€šè¿‡èåˆçƒ­æˆåƒä¸LiDARæŠ€æœ¯å®ç°äº†æ›´å…¨é¢ã€ç²¾ç¡®çš„ç¼ºé™·æ£€æµ‹èƒ½åŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–ç›‘æµ‹ç³»ç»Ÿå‘å±•æä¾›äº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Traditional two-dimensional thermography, despite being non-invasive and useful for defect detection in the construction field, is limited in effectively assessing complex geometries, inaccessible areas, and subsurface defects. This paper introduces Thermo-LIO, a novel multi-sensor system that can enhance Structural Health Monitoring (SHM) by fusing thermal imaging with high-resolution LiDAR. To achieve this, the study first develops a multimodal fusion method combining thermal imaging and LiDAR, enabling precise calibration and synchronization of multimodal data streams to create accurate representations of temperature distributions in buildings. Second, it integrates this fusion approach with LiDAR-Inertial Odometry (LIO), enabling full coverage of large-scale structures and allowing for detailed monitoring of temperature variations and defect detection across inspection cycles. Experimental validations, including case studies on a bridge and a hall building, demonstrate that Thermo-LIO can detect detailed thermal anomalies and structural defects more accurately than traditional methods. The system enhances diagnostic precision, enables real-time processing, and expands inspection coverage, highlighting the crucial role of multimodal sensor integration in advancing SHM methodologies for large-scale civil infrastructure.


### [4] [Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning](https://arxiv.org/abs/2601.09111)
*Yang Li, Aming Wu, Zihao Zhang, Yahong Han*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºslow4fast-VLNæ¡†æ¶ï¼Œé€šè¿‡å»ºç«‹åŠ¨æ€äº¤äº’çš„å¿«æ…¢æ¨ç†æœºåˆ¶æ¥è§£å†³è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„æ³›åŒ–åœºæ™¯é€‚åº”é—®é¢˜ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨é¢å¯¹æœªè§ç¯å¢ƒå’ŒæŒ‡ä»¤æ—¶åŠ¨æ€ç”Ÿæˆæ³›åŒ–ç­–ç•¥ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿè§†è§‰è¯­è¨€å¯¼èˆªæ–¹æ³•é€šå¸¸éµå¾ªé—­é›†å‡è®¾ï¼Œå³è®­ç»ƒå’Œæµ‹è¯•æ•°æ®å…±äº«ç›¸åŒçš„è¾“å…¥å›¾åƒå’ŒæŒ‡ä»¤é£æ ¼ï¼Œç„¶è€Œç°å®ä¸–ç•Œæ˜¯å¼€æ”¾çš„ä¸”å……æ»¡å„ç§æœªè§ç¯å¢ƒï¼Œè¿™å¯¹é—­é›†æ–¹æ³•æ„æˆäº†å·¨å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡èšç„¦äºæ³›åŒ–åœºæ™¯é€‚åº”ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥å¤šæ ·åŒ–ç¯å¢ƒå’Œä¸ä¸€è‡´æŒ‡ä»¤æ¥å­¦ä¹ æ³›åŒ–çš„å¯¼èˆªèƒ½åŠ›ï¼Œä¸»è¦æŒ‘æˆ˜åœ¨äºå¦‚ä½•ä½¿æ™ºèƒ½ä½“åœ¨é¢å¯¹æœªè§ç¯å¢ƒå’ŒæŒ‡ä»¤æ—¶åŠ¨æ€äº§ç”Ÿæ³›åŒ–ç­–ç•¥ã€‚

**Method:** æœ¬æ–‡æå‡ºslow4fast-VLNæ¡†æ¶ï¼Œå»ºç«‹åŠ¨æ€äº¤äº’çš„å¿«æ…¢æ¨ç†æœºåˆ¶ã€‚å¿«é€Ÿæ¨ç†æ¨¡å—æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç­–ç•¥ç½‘ç»œï¼Œé€šè¿‡å®æ—¶è¾“å…¥è¾“å‡ºåŠ¨ä½œï¼Œå¹¶åœ¨å†å²å­˜å‚¨åº“ä¸­ç§¯ç´¯æ‰§è¡Œè®°å½•ä»¥æ„å»ºè®°å¿†ã€‚æ…¢é€Ÿæ¨ç†æ¨¡å—åˆ†æå¿«é€Ÿæ¨ç†æ¨¡å—ç”Ÿæˆçš„è®°å¿†ï¼Œé€šè¿‡æ·±åº¦åæ€æå–å¢å¼ºå†³ç­–æ³›åŒ–èƒ½åŠ›çš„ç»éªŒï¼Œè¿™äº›ç»éªŒè¢«ç»“æ„åŒ–å­˜å‚¨å¹¶ç”¨äºæŒç»­ä¼˜åŒ–å¿«é€Ÿæ¨ç†æ¨¡å—ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•å°†å¿«æ…¢æ¨ç†è§†ä¸ºç‹¬ç«‹æœºåˆ¶ä¸åŒï¼Œè¯¥æ¡†æ¶å®ç°äº†å¿«æ…¢äº¤äº’ï¼Œåˆ©ç”¨æ…¢é€Ÿæ¨ç†çš„ç»éªŒä½¿ç³»ç»Ÿèƒ½å¤ŸæŒç»­é€‚åº”å¹¶åœ¨é¢å¯¹æœªè§åœºæ™¯æ—¶é«˜æ•ˆæ‰§è¡Œå¯¼èˆªä»»åŠ¡ã€‚

**Result:** è™½ç„¶æ‘˜è¦ä¸­æœªæä¾›å…·ä½“çš„æ€§èƒ½æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•ç»“æœï¼Œä½†è¯¥æ–¹æ³•åœ¨ç†è®ºä¸Šè§£å†³äº†è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„æ³›åŒ–åœºæ™¯é€‚åº”é—®é¢˜ã€‚é€šè¿‡å¿«æ…¢æ¨ç†çš„åŠ¨æ€äº¤äº’æœºåˆ¶ï¼Œç³»ç»Ÿèƒ½å¤Ÿä»æ‰§è¡Œè®°å½•ä¸­æå–æ³›åŒ–ç»éªŒå¹¶æŒç»­ä¼˜åŒ–å¯¼èˆªç­–ç•¥ï¼Œä»è€Œåœ¨é¢å¯¹æœªè§ç¯å¢ƒå’Œä¸ä¸€è‡´æŒ‡ä»¤æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é€‚åº”èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ä¸ªå—äººç±»è®¤çŸ¥ç³»ç»Ÿå¯å‘çš„åŠ¨æ€äº¤äº’å¿«æ…¢æ¨ç†æ¡†æ¶ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œè§†è§‰è¯­è¨€å¯¼èˆªæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å°†å¿«é€Ÿæ‰§è¡Œä¸æ…¢é€Ÿåæ€ç›¸ç»“åˆï¼Œç³»ç»Ÿèƒ½å¤ŸæŒç»­å­¦ä¹ å’Œé€‚åº”æœªè§åœºæ™¯ï¼Œè¿™ä¸ºæ„å»ºæ›´å…·æ³›åŒ–èƒ½åŠ›çš„è‡ªä¸»å¯¼èˆªæ™ºèƒ½ä½“æä¾›äº†é‡è¦æ€è·¯ï¼Œå¹¶ä¸ºæœªæ¥åœ¨æ›´å¤æ‚å¼€æ”¾ç¯å¢ƒä¸­çš„å¯¼èˆªç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Vision-Language Navigation aims to enable agents to navigate to a target location based on language instructions. Traditional VLN often follows a close-set assumption, i.e., training and test data share the same style of the input images and instructions. However, the real world is open and filled with various unseen environments, posing enormous difficulties for close-set methods. To this end, we focus on the General Scene Adaptation (GSA-VLN) task, aiming to learn generalized navigation ability by introducing diverse environments and inconsistent intructions.Towards this task, when facing unseen environments and instructions, the challenge mainly lies in how to enable the agent to dynamically produce generalized strategies during the navigation process. Recent research indicates that by means of fast and slow cognition systems, human beings could generate stable policies, which strengthen their adaptation for open world. Inspired by this idea, we propose the slow4fast-VLN, establishing a dynamic interactive fast-slow reasoning framework. The fast-reasoning module, an end-to-end strategy network, outputs actions via real-time input. It accumulates execution records in a history repository to build memory. The slow-reasoning module analyze the memories generated by the fast-reasoning module. Through deep reflection, it extracts experiences that enhance the generalization ability of decision-making. These experiences are structurally stored and used to continuously optimize the fast-reasoning module. Unlike traditional methods that treat fast-slow reasoning as independent mechanisms, our framework enables fast-slow interaction. By leveraging the experiences from slow reasoning. This interaction allows the system to continuously adapt and efficiently execute navigation tasks when facing unseen scenarios.


### [5] [LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models](https://arxiv.org/abs/2601.09116)
*Haoyan Gong, Hongbin Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºQwen3-VLçš„ç«¯åˆ°ç«¯ç»“æ„æ„ŸçŸ¥å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å­—ç¬¦æ„ŸçŸ¥å¤šæ¨¡æ€æ¨ç†æ¨¡å—å’Œå­—ç¬¦æ§½æŸ¥è¯¢ï¼Œè§£å†³äº†è½¦ç‰Œè¯†åˆ«ä¸­å›¾åƒæ¢å¤ä¸å­—ç¬¦è¯†åˆ«ç›®æ ‡ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†ä¸¥é‡é€€åŒ–åœºæ™¯ä¸‹çš„è¯†åˆ«æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°å®ä¸–ç•Œè½¦ç‰Œè¯†åˆ«é¢ä¸´è¿åŠ¨æ¨¡ç³Šã€ä½åˆ†è¾¨ç‡å’Œå¤æ‚å…‰ç…§ç­‰ä¸¥é‡é€€åŒ–æŒ‘æˆ˜ï¼Œä¼ ç»Ÿçš„"æ¢å¤-å†è¯†åˆ«"ä¸¤é˜¶æ®µèŒƒå¼å­˜åœ¨æ ¹æœ¬ç¼ºé™·ï¼šå›¾åƒæ¢å¤æ¨¡å‹çš„åƒç´ çº§ä¼˜åŒ–ç›®æ ‡ä¸å­—ç¬¦è¯†åˆ«çš„è¯­ä¹‰ç›®æ ‡ä¸ä¸€è‡´ï¼Œå¯¼è‡´ä¼ªå½±å¹²æ‰°å’Œè¯¯å·®ç´¯ç§¯ã€‚åŒæ—¶ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹è™½ç„¶å…·å¤‡å¼ºå¤§é€šç”¨èƒ½åŠ›ï¼Œä½†ç¼ºä¹å¯¹è½¦ç‰Œå­—ç¬¦åºåˆ—å›ºå®šé•¿åº¦å’Œç‰¹å®šé¡ºåºç­‰æ˜¾å¼ç»“æ„å»ºæ¨¡ã€‚

**Method:** æœ¬æ–‡æå‡ºåŸºäºQwen3-VLçš„ç«¯åˆ°ç«¯ç»“æ„æ„ŸçŸ¥å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ï¼Œæ ¸å¿ƒåˆ›æ–°æ˜¯å­—ç¬¦æ„ŸçŸ¥å¤šæ¨¡æ€æ¨ç†æ¨¡å—ï¼Œè¯¥æ¨¡å—å¼•å…¥ä¸€ç»„å¯å­¦ä¹ çš„å­—ç¬¦æ§½æŸ¥è¯¢ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä»è§†è§‰ç‰¹å¾ä¸­ä¸»åŠ¨æ£€ç´¢å¯¹åº”å­—ç¬¦ä½ç½®çš„ç»†ç²’åº¦è¯æ®ã€‚éšåé€šè¿‡æ®‹å·®è°ƒåˆ¶å°†è¿™äº›å­—ç¬¦æ„ŸçŸ¥è¡¨ç¤ºæ³¨å…¥è§†è§‰æ ‡è®°ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤ŸåŸºäºæ˜¾å¼ç»“æ„å…ˆéªŒè¿›è¡Œè‡ªå›å½’ç”Ÿæˆã€‚ç»“åˆLoRAå‚æ•°é«˜æ•ˆå¾®è°ƒç­–ç•¥ï¼Œæ¨¡å‹åœ¨ä¿ç•™å¤§æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶å®ç°é¢†åŸŸé€‚åº”ã€‚

**Result:** åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œä¸¥é‡é€€åŒ–æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ¢å¤-è¯†åˆ«ç»„åˆæ–¹æ³•å’Œé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å°†ç»“æ„åŒ–æ¨ç†èå…¥å¤§æ¨¡å‹å¯¹äºä½è´¨é‡æ–‡æœ¬è¯†åˆ«ä»»åŠ¡çš„ä¼˜åŠ¿ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¼•å…¥æ˜¾å¼ç»“æ„å»ºæ¨¡å’Œå­—ç¬¦æ„ŸçŸ¥æ¨ç†æœºåˆ¶ï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³ä¼ ç»Ÿä¸¤é˜¶æ®µæ–¹æ³•çš„è¯­ä¹‰ç›®æ ‡ä¸ä¸€è‡´é—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸ºä½è´¨é‡æ–‡æœ¬è¯†åˆ«ä»»åŠ¡æä¾›äº†æ–°èŒƒå¼ï¼Œå±•ç¤ºäº†å°†å¤§è¯­è¨€æ¨¡å‹çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ä¸é¢†åŸŸç‰¹å®šç»“æ„å…ˆéªŒç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºç±»ä¼¼çš„ç»“æ„åŒ–è§†è§‰è¯†åˆ«ä»»åŠ¡æä¾›äº†é‡è¦å‚è€ƒã€‚

---

#### ğŸ“„ Abstract
Real-world License Plate Recognition (LPR) faces significant challenges from severe degradations such as motion blur, low resolution, and complex illumination. The prevailing "restoration-then-recognition" two-stage paradigm suffers from a fundamental flaw: the pixel-level optimization objectives of image restoration models are misaligned with the semantic goals of character recognition, leading to artifact interference and error accumulation. While Vision-Language Models (VLMs) have demonstrated powerful general capabilities, they lack explicit structural modeling for license plate character sequences (e.g., fixed length, specific order). To address this, we propose an end-to-end structure-aware multimodal reasoning framework based on Qwen3-VL. The core innovation lies in the Character-Aware Multimodal Reasoning Module (CMRM), which introduces a set of learnable Character Slot Queries. Through a cross-attention mechanism, these queries actively retrieve fine-grained evidence corresponding to character positions from visual features. Subsequently, we inject these character-aware representations back into the visual tokens via residual modulation, enabling the language model to perform autoregressive generation based on explicit structural priors. Furthermore, combined with the LoRA parameter-efficient fine-tuning strategy, the model achieves domain adaptation while retaining the generalization capabilities of the large model. Extensive experiments on both synthetic and real-world severely degraded datasets demonstrate that our method significantly outperforms existing restoration-recognition combinations and general VLMs, validating the superiority of incorporating structured reasoning into large models for low-quality text recognition tasks.


### [6] [LPCAN: Lightweight Pyramid Cross-Attention Network for Rail Surface Defect Detection Using RGB-D Data](https://arxiv.org/abs/2601.09118)
*Jackie Alex, Guoqiang Huan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§é‡‘å­—å¡”äº¤å‰æ³¨æ„åŠ›ç½‘ç»œï¼ˆLPCANetï¼‰ï¼Œåˆ©ç”¨RGB-Dæ•°æ®å®ç°é«˜æ•ˆå‡†ç¡®çš„è½¨é“ç¼ºé™·æ£€æµ‹ï¼Œåœ¨ä¿æŒä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºè§†è§‰çš„è½¨é“ç¼ºé™·æ£€æµ‹æ–¹æ³•å­˜åœ¨è®¡ç®—å¤æ‚åº¦é«˜ã€å‚æ•°é‡å¤§å’Œç²¾åº¦ä¸è¶³ç­‰å±€é™æ€§ï¼Œéœ€è¦ä¸€ç§æ›´é«˜æ•ˆå‡†ç¡®çš„è§£å†³æ–¹æ¡ˆæ¥æ»¡è¶³å·¥ä¸šç¼ºé™·æ£€æµ‹çš„å®é™…éœ€æ±‚ã€‚

**Method:** æå‡ºçš„LPCANetæ¶æ„é‡‡ç”¨MobileNetv2ä½œä¸ºRGBç‰¹å¾æå–çš„ä¸»å¹²ç½‘ç»œï¼Œç»“åˆè½»é‡çº§é‡‘å­—å¡”æ¨¡å—ï¼ˆLPMï¼‰å¤„ç†æ·±åº¦æ•°æ®ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼ˆCAMï¼‰å®ç°å¤šæ¨¡æ€èåˆï¼Œå¹¶åˆ©ç”¨ç©ºé—´ç‰¹å¾æå–å™¨ï¼ˆSFEï¼‰å¢å¼ºç»“æ„åˆ†æèƒ½åŠ›ã€‚

**Result:** åœ¨ä¸‰ä¸ªæ— ç›‘ç£RGB-Dè½¨é“æ•°æ®é›†ï¼ˆNEU-RSDDS-AUGã€RSDD-TYPE1ã€RSDD-TYPE2ï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒLPCANetä»…éœ€990ä¸‡å‚æ•°å’Œ2.50 G FLOPsï¼Œæ¨ç†é€Ÿåº¦è¾¾162.60 fpsï¼Œåœ¨SÎ±ã€IOUå’ŒMAEæŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æå‡1.48%ã€0.86%å’Œ1.77%ã€‚æ¶ˆèå®éªŒè¯å®äº†CAMå’ŒSFEçš„å…³é”®ä½œç”¨ï¼Œåœ¨éè½¨é“æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶æœ‰æ•ˆæ¡¥æ¥äº†ä¼ ç»Ÿæ–¹æ³•ä¸æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œä¸ºå·¥ä¸šç¼ºé™·æ£€æµ‹æä¾›äº†å…·æœ‰é‡è¦å®ç”¨ä»·å€¼çš„è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å·¥ä½œå°†é›†ä¸­äºè¿›ä¸€æ­¥æ¨¡å‹å‹ç¼©ä»¥å®ç°å®æ—¶éƒ¨ç½²ã€‚

---

#### ğŸ“„ Abstract
This paper addresses the limitations of current vision-based rail defect detection methods, including high computational complexity, excessive parameter counts, and suboptimal accuracy. We propose a Lightweight Pyramid Cross-Attention Network (LPCANet) that leverages RGB-D data for efficient and accurate defect identification. The architecture integrates MobileNetv2 as a backbone for RGB feature extraction with a lightweight pyramid module (LPM) for depth processing, coupled with a cross-attention mechanism (CAM) for multimodal fusion and a spatial feature extractor (SFE) for enhanced structural analysis. Comprehensive evaluations on three unsupervised RGB-D rail datasets (NEU-RSDDS-AUG, RSDD-TYPE1, RSDD-TYPE2) demonstrate that LPCANet achieves state-of-the-art performance with only 9.90 million parameters, 2.50 G FLOPs, and 162.60 fps inference speed. Compared to 18 existing methods, LPCANet shows significant improvements, including +1.48\% in $S_Î±$, +0.86\% in IOU, and +1.77\% in MAE over the best-performing baseline. Ablation studies confirm the critical roles of CAM and SFE, while experiments on non-rail datasets (DAGM2007, MT, Kolektor-SDD2) validate its generalization capability. The proposed framework effectively bridges traditional and deep learning approaches, offering substantial practical value for industrial defect inspection. Future work will focus on further model compression for real-time deployment.


### [7] [SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL](https://arxiv.org/abs/2601.09136)
*Lijun Liu, Linwei Chen, Zhishou Zhang, Meng Tian, Hengfu Cui, Ruiyang Li, Zhaocheng Liu, Qiang Ju, Qianxi Li, Hong-Yu Zhou*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSkinFlowæ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–è§†è§‰ä¿¡æ¯ä¼ è¾“æ•ˆç‡è€Œéå‚æ•°æ‰©å±•æ¥è§£å†³é€šç”¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çš®è‚¤ç—…å­¦ä¸­çš„"å¼¥æ•£æ³¨æ„åŠ›"é—®é¢˜ï¼Œåœ¨7Bå‚æ•°è§„æ¨¡ä¸‹å®ç°äº†è¶…è¶Šå¤§è§„æ¨¡é€šç”¨æ¨¡å‹çš„è¯Šæ–­æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é€šç”¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çš®è‚¤ç—…å­¦è¯Šæ–­ä¸­å­˜åœ¨"å¼¥æ•£æ³¨æ„åŠ›"é—®é¢˜ï¼Œéš¾ä»¥ä»èƒŒæ™¯å™ªå£°ä¸­åŒºåˆ†ç»†å¾®ç—…ç†ç—…å˜ï¼Œç°æœ‰ç ”ç©¶è¿‡åº¦ä¾èµ–å‚æ•°æ‰©å±•è€Œå¿½è§†äº†ä¿¡æ¯ä¼ è¾“æ•ˆç‡çš„ä¼˜åŒ–ã€‚

**Method:** æå‡ºSkinFlowæ¡†æ¶ï¼Œé‡‡ç”¨è™šæ‹Ÿå®½åº¦åŠ¨æ€è§†è§‰ç¼–ç å™¨åœ¨ä¸å¢åŠ ç‰©ç†å‚æ•°çš„æƒ…å†µä¸‹å±•å¼€å¤æ‚ç—…ç†æµå½¢ï¼Œç»“åˆä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µå¯¹é½æ˜¾æ€§åŒ»å­¦æè¿°ï¼Œç¬¬äºŒé˜¶æ®µé‡å»ºéšæ€§è¯Šæ–­çº¹ç†ï¼Œå¹¶åœ¨å—é™è¯­ä¹‰ç©ºé—´å†…è¿›è¡Œä¼˜åŒ–ã€‚

**Result:** åœ¨Fitzpatrick17kåŸºå‡†æµ‹è¯•ä¸­ï¼Œ7Bæ¨¡å‹å®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ŒTop-1å‡†ç¡®ç‡æå‡12.06%ï¼ŒTop-6å‡†ç¡®ç‡æå‡28.57%ï¼Œæ˜¾è‘—è¶…è¶ŠQwen3VL-235Bå’ŒGPT-5.2ç­‰å¤§è§„æ¨¡é€šç”¨æ¨¡å‹ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ä¼˜åŒ–å‡ ä½•å®¹é‡å’Œä¿¡æ¯æµæ¯”åŸå§‹å‚æ•°æ‰©å±•èƒ½äº§ç”Ÿæ›´ä¼˜è¶Šçš„è¯Šæ–­æ¨ç†èƒ½åŠ›ï¼Œä¸ºåŒ»å­¦AIæ¨¡å‹è®¾è®¡æä¾›äº†æ–°çš„èŒƒå¼ï¼Œå¼ºè°ƒè¯Šæ–­å®‰å…¨æ€§å’Œå±‚æ¬¡ç›¸å…³æ€§çš„ä¸´åºŠè¯„ä¼°åè®®å…·æœ‰é‡è¦å®è·µä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to "diffuse attention" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to "unfold" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.


### [8] [SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection](https://arxiv.org/abs/2601.09147)
*Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºååŒè¯­ä¹‰-è§†è§‰æç¤ºï¼ˆSSVPï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼Œé€šè¿‡èåˆå¤šæ ·åŒ–çš„è§†è§‰ç¼–ç æ¥æå‡æ¨¡å‹çš„ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªå·¥ä¸šåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰çš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ–¹æ³•å—é™äºå•ä¸€è§†è§‰éª¨å¹²ç½‘ç»œï¼Œéš¾ä»¥åŒæ—¶å…¼é¡¾å…¨å±€è¯­ä¹‰æ³›åŒ–èƒ½åŠ›å’Œç»†ç²’åº¦ç»“æ„åˆ¤åˆ«èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨å·¥ä¸šæ£€æµ‹ä¸­çš„å®é™…åº”ç”¨æ•ˆæœã€‚

**Method:** è¯¥æ–¹æ³•æå‡ºäº†ååŒè¯­ä¹‰-è§†è§‰æç¤ºï¼ˆSSVPï¼‰æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå±‚æ¬¡åŒ–è¯­ä¹‰-è§†è§‰ååŒæœºåˆ¶ï¼ˆHSVSï¼‰å°†DINOv3çš„å¤šå°ºåº¦ç»“æ„å…ˆéªŒæ·±åº¦é›†æˆåˆ°CLIPè¯­ä¹‰ç©ºé—´ä¸­ï¼›è§†è§‰æ¡ä»¶æç¤ºç”Ÿæˆå™¨ï¼ˆVCPGï¼‰åˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›å¼•å¯¼åŠ¨æ€æç¤ºç”Ÿæˆï¼Œä½¿è¯­è¨€æŸ¥è¯¢èƒ½ç²¾ç¡®å®šä½ç‰¹å®šå¼‚å¸¸æ¨¡å¼ï¼›è§†è§‰-æ–‡æœ¬å¼‚å¸¸æ˜ å°„å™¨ï¼ˆVTAMï¼‰å»ºç«‹äº†åŒé—¨æ ¡å‡†èŒƒå¼ï¼Œè§£å†³å…¨å±€è¯„åˆ†ä¸å±€éƒ¨è¯æ®ä¹‹é—´çš„ä¸ä¸€è‡´é—®é¢˜ã€‚

**Result:** åœ¨ä¸ƒä¸ªå·¥ä¸šåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°éªŒè¯äº†è¯¥æ–¹æ³•çš„é²æ£’æ€§ï¼ŒSSVPåœ¨MVTec-ADæ•°æ®é›†ä¸Šå®ç°äº†93.0%çš„å›¾åƒçº§AUROCå’Œ92.2%çš„åƒç´ çº§AUROCï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„é›¶æ ·æœ¬æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡ååŒèåˆå¤šæ ·åŒ–è§†è§‰ç¼–ç å¯ä»¥æœ‰æ•ˆæå‡é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºå·¥ä¸šè§†è§‰æ£€æµ‹æä¾›äº†ä¸€ç§æ— éœ€ç›‘ç£çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºè·¨æ¨¡æ€å¼‚å¸¸æ£€æµ‹ç ”ç©¶æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model's fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3's multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0\% Image-AUROC and 92.2\% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.


### [9] [Architecture inside the mirage: evaluating generative image models on architectural style, elements, and typologies](https://arxiv.org/abs/2601.09169)
*Jamie Magrill, Leah Gornstein, Sandra Seekins, Barry Magrill*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†äº”ç§ä¸»æµç”Ÿæˆå¼AIæ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿåœ¨å»ºç­‘å›¾åƒç”Ÿæˆä¸­çš„å‡†ç¡®æ€§ï¼Œå‘ç°è¿™äº›ç³»ç»Ÿåœ¨ç”Ÿæˆå†å²è§„åˆ™çº¦æŸçš„å»ºç­‘å›¾åƒæ—¶å‡†ç¡®æ€§æœ‰é™ï¼Œä¸”æ€§èƒ½å› å¹³å°å’Œæç¤ºè¯ç±»å‹è€Œå¼‚ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿåœ¨å»ºç­‘å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å…¶åœ¨å†å²è§„åˆ™çº¦æŸçš„å»ºç­‘é¢†åŸŸä¸­ç”Ÿæˆå‡†ç¡®å›¾åƒçš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†è¡¨å¾ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚

**Method:** ç ”ç©¶è¯„ä¼°äº†äº”ç§å¹¿æ³›ä½¿ç”¨çš„GenAIå›¾åƒå¹³å°ï¼ˆAdobe Fireflyã€DALL-E 3ã€Google Imagen 3ã€Microsoft Image Generatorå’ŒMidjourneyï¼‰ï¼Œä½¿ç”¨30ä¸ªæ¶µç›–ä¸åŒé£æ ¼ã€ç±»å‹å’Œç¼–ç å…ƒç´ çš„å»ºç­‘æç¤ºè¯ï¼Œæ¯ä¸ªæç¤ºè¯-ç”Ÿæˆå™¨ç»„åˆäº§ç”Ÿå››å¼ å›¾åƒï¼Œå…±600å¼ å›¾åƒï¼Œç”±ä¸¤ä½å»ºç­‘å†å²å­¦å®¶æ ¹æ®é¢„å®šä¹‰æ ‡å‡†ç‹¬ç«‹è¯„åˆ†å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡å…±è¯†è§£å†³åˆ†æ­§ã€‚

**Result:** å¸¸è§æç¤ºè¯çš„å›¾åƒè¾“å‡ºå‡†ç¡®æ€§æ¯”ç½•è§æç¤ºè¯é«˜2.7å€ï¼Œå„å¹³å°æ€»ä½“å‡†ç¡®æ€§æœ‰é™ï¼ˆæœ€é«˜52%ï¼Œæœ€ä½32%ï¼Œå¹³å‡42%ï¼‰ï¼Œå…¨æ­£ç¡®ç»“æœåœ¨å„å¹³å°é—´ç›¸ä¼¼ï¼Œä½†å…¨é”™è¯¯ç»“æœå·®å¼‚æ˜¾è‘—ï¼ŒImagen 3å¤±è´¥æœ€å°‘ï¼ŒMicrosoft Image Generatorå¤±è´¥æœ€å¤šï¼Œå®šæ€§åˆ†æè¯†åˆ«å‡ºè¿‡åº¦è£…é¥°ã€ä¸­ä¸–çºªé£æ ¼ä¸åæœŸå¤å…´æ··æ·†ä»¥åŠæè¿°æ€§æç¤ºè¯è¯¯è§£ç­‰é‡å¤æ¨¡å¼ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜éœ€è¦ä¸ºGenAIåˆæˆå†…å®¹æä¾›å¯è§æ ‡ç­¾ã€å»ºç«‹æœªæ¥è®­ç»ƒæ•°æ®é›†çš„æ¥æºæ ‡å‡†ï¼Œå¹¶åœ¨æ•™è‚²åº”ç”¨ä¸­è°¨æ…ä½¿ç”¨GenAIå»ºç­‘å›¾åƒï¼Œè¿™äº›å‘ç°å¯¹AIåœ¨ä¸“ä¸šé¢†åŸŸåº”ç”¨çš„å¯é æ€§å’Œä¼¦ç†è€ƒé‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
Generative artificial intelligence (GenAI) text-to-image systems are increasingly used to generate architectural imagery, yet their capacity to reproduce accurate images in a historically rule-bound field remains poorly characterized. We evaluated five widely used GenAI image platforms (Adobe Firefly, DALL-E 3, Google Imagen 3, Microsoft Image Generator, and Midjourney) using 30 architectural prompts spanning styles, typologies, and codified elements. Each prompt-generator pair produced four images (n = 600 images total). Two architectural historians independently scored each image for accuracy against predefined criteria, resolving disagreements by consensus. Set-level performance was summarized as zero to four accurate images per four-image set. Image output from Common prompts was 2.7-fold more accurate than from Rare prompts (p < 0.05). Across platforms, overall accuracy was limited (highest accuracy score 52 percent; lowest 32 percent; mean 42 percent). All-correct (4 out of 4) outcomes were similar across platforms. By contrast, all-incorrect (0 out of 4) outcomes varied substantially, with Imagen 3 exhibiting the fewest failures and Microsoft Image Generator exhibiting the highest number of failures. Qualitative review of the image dataset identified recurring patterns including over-embellishment, confusion between medieval styles and their later revivals, and misrepresentation of descriptive prompts (for example, egg-and-dart, banded column, pendentive). These findings support the need for visible labeling of GenAI synthetic content, provenance standards for future training datasets, and cautious educational use of GenAI architectural imagery.


### [10] [From Performance to Practice: Knowledge-Distilled Segmentator for On-Premises Clinical Workflows](https://arxiv.org/abs/2601.09191)
*Qizhen Lan, Aaron Choi, Jun Ma, Bo Wang, Zhaogming Zhao, Xiaoqian Jiang, Yu-Chun Hsu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘éƒ¨ç½²çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œåˆ©ç”¨çŸ¥è¯†è’¸é¦å°†é«˜æ€§èƒ½åˆ†å‰²æ¨¡å‹è½¬æ¢ä¸ºå¯æ‰©å±•çš„ç´§å‡‘å­¦ç”Ÿæ¨¡å‹å®¶æ—ï¼Œåœ¨ä¿æŒæ¶æ„å…¼å®¹æ€§çš„åŒæ—¶å®ç°ç³»ç»Ÿæ€§çš„å®¹é‡ç¼©å‡ï¼Œä¸ºä¸´åºŠå·¥ä½œæµæä¾›å®ç”¨çš„éƒ¨ç½²è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åœ¨å¸¸è§„ä¸´åºŠå·¥ä½œæµä¸­éƒ¨ç½²åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹å¸¸å—é™äºæœ¬åœ°åŸºç¡€è®¾æ–½ï¼Œå…¶ä¸­è®¡ç®—èµ„æºå›ºå®šä¸”åŸºäºäº‘çš„æ¨ç†å¯èƒ½å—æ²»ç†å’Œå®‰å…¨ç­–ç•¥é™åˆ¶ã€‚è™½ç„¶é«˜å®¹é‡æ¨¡å‹å®ç°äº†å¼ºå¤§çš„åˆ†å‰²ç²¾åº¦ï¼Œä½†å…¶è®¡ç®—éœ€æ±‚é˜»ç¢äº†åœ¨åŒ»é™¢ç¯å¢ƒä¸­çš„å®é™…éƒ¨ç½²å’Œé•¿æœŸå¯ç»´æŠ¤æ€§ï¼Œéœ€è¦ä¸€ç§èƒ½åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½è®¡ç®—å¤æ‚åº¦çš„è§£å†³æ–¹æ¡ˆã€‚

**Method:** è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§éƒ¨ç½²å¯¼å‘çš„æ¡†æ¶ï¼Œåˆ©ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯å°†é«˜æ€§èƒ½åˆ†å‰²æ¨¡å‹è½¬æ¢ä¸ºå¯æ‰©å±•çš„ç´§å‡‘å­¦ç”Ÿæ¨¡å‹å®¶æ—ï¼Œæ— éœ€ä¿®æ”¹æ¨ç†ç®¡é“ã€‚è¯¥æ–¹æ³•ä¿æŒäº†ä¸ç°æœ‰ä¸´åºŠç³»ç»Ÿçš„æ¶æ„å…¼å®¹æ€§ï¼ŒåŒæ—¶å®ç°äº†ç³»ç»Ÿæ€§çš„å®¹é‡ç¼©å‡ï¼Œæ¡†æ¶åœ¨å¤šç«™ç‚¹è„‘MRIæ•°æ®é›†ï¼ˆåŒ…å«1,104ä¸ª3Dä½“ç§¯ï¼‰ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¹¶åœ¨è…¹éƒ¨CTä¸Šè¿›ä¸€æ­¥æ£€éªŒè·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚

**Result:** åœ¨æ¿€è¿›å‚æ•°å‡å°‘94%çš„æƒ…å†µä¸‹ï¼Œè’¸é¦åçš„å­¦ç”Ÿæ¨¡å‹ä¿ç•™äº†æ•™å¸ˆæ¨¡å‹åˆ†å‰²ç²¾åº¦çš„98.7%ï¼ŒåŒæ—¶å®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼ŒåŒ…æ‹¬CPUæ¨ç†å»¶è¿Ÿå‡å°‘é«˜è¾¾67%ä¸”æ— éœ€é¢å¤–éƒ¨ç½²å¼€é”€ã€‚è¯¥æ¡†æ¶åœ¨101ä¸ªç²¾é€‰ç—…ä¾‹çš„ç‹¬ç«‹æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨è…¹éƒ¨CTæ•°æ®ä¸Šå±•ç¤ºäº†è‰¯å¥½çš„è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶åœ¨çœŸå®åŒ»ç–—ç³»ç»Ÿä¸­çš„å®ç”¨æ€§ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜çŸ¥è¯†è’¸é¦ä¸ºå°†ç ”ç©¶çº§åˆ†å‰²æ¨¡å‹è½¬æ¢ä¸ºå¯ç»´æŠ¤ã€éƒ¨ç½²å°±ç»ªçš„ç»„ä»¶æä¾›äº†å®ç”¨å¯é çš„é€”å¾„ï¼Œç‰¹åˆ«é€‚ç”¨äºæœ¬åœ°ä¸´åºŠå·¥ä½œæµã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—éœ€æ±‚ï¼Œä¸ºåŒ»ç–—ç³»ç»Ÿåœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„æ¨¡å‹éƒ¨ç½²æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Deploying medical image segmentation models in routine clinical workflows is often constrained by on-premises infrastructure, where computational resources are fixed and cloud-based inference may be restricted by governance and security policies. While high-capacity models achieve strong segmentation accuracy, their computational demands hinder practical deployment and long-term maintainability in hospital environments. We present a deployment-oriented framework that leverages knowledge distillation to translate a high-performing segmentation model into a scalable family of compact student models, without modifying the inference pipeline. The proposed approach preserves architectural compatibility with existing clinical systems while enabling systematic capacity reduction. The framework is evaluated on a multi-site brain MRI dataset comprising 1,104 3D volumes, with independent testing on 101 curated cases, and is further examined on abdominal CT to assess cross-modality generalizability. Under aggressive parameter reduction (94%), the distilled student model preserves nearly all of the teacher's segmentation accuracy (98.7%), while achieving substantial efficiency gains, including up to a 67% reduction in CPU inference latency without additional deployment overhead. These results demonstrate that knowledge distillation provides a practical and reliable pathway for converting research-grade segmentation models into maintainable, deployment-ready components for on-premises clinical workflows in real-world health systems.


### [11] [Pairing-free Group-level Knowledge Distillation for Robust Gastrointestinal Lesion Classification in White-Light Endoscopy](https://arxiv.org/abs/2601.09209)
*Qiang Hu, Qimei Wang, Yingjie Guo, Qiang Li, Zhiwei Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPaGKDçš„æ— é…å¯¹ç»„çº§çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œç”¨äºè§£å†³å†…çª¥é•œæˆåƒä¸­ä»çª„å¸¦æˆåƒï¼ˆNBIï¼‰å‘ç™½å…‰æˆåƒï¼ˆWLIï¼‰è¿›è¡Œè·¨æ¨¡æ€çŸ¥è¯†è¿ç§»çš„éš¾é¢˜ï¼Œè¯¥æ¡†æ¶æ— éœ€ä¾èµ–é…å¯¹çš„NBI-WLIå›¾åƒå¯¹ï¼Œæ˜¾è‘—æå‡äº†WLI-onlyæ¨¡å‹çš„è¯Šæ–­æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å†…çª¥é•œç™Œç—‡ç­›æŸ¥ä¸­ï¼Œçª„å¸¦æˆåƒï¼ˆNBIï¼‰ç›¸æ¯”æ ‡å‡†ç™½å…‰æˆåƒï¼ˆWLIï¼‰èƒ½æä¾›æ›´ä¼˜çš„è¯Šæ–­ç»†èŠ‚ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–åŒä¸€ç—…ç¶çš„é…å¯¹NBI-WLIå›¾åƒï¼Œè¿™ç§æ•°æ®è·å–æˆæœ¬é«˜æ˜‚ä¸”ä¸åˆ‡å®é™…ï¼Œå¯¼è‡´å¤§é‡ä¸´åºŠæ•°æ®æ— æ³•è¢«æœ‰æ•ˆåˆ©ç”¨ï¼Œé™åˆ¶äº†è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»çš„å®é™…åº”ç”¨ã€‚

**Method:** æœ¬æ–‡æå‡ºPaGKDï¼ˆPairing-free Group-level Knowledge Distillationï¼‰æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªäº’è¡¥æ¨¡å—ï¼šç»„çº§åŸå‹è’¸é¦ï¼ˆGKD-Proï¼‰é€šè¿‡å…±äº«çš„ç—…ç¶æ„ŸçŸ¥æŸ¥è¯¢æå–æ¨¡æ€ä¸å˜çš„è¯­ä¹‰åŸå‹æ¥è’¸é¦ç´§å‡‘çš„ç»„è¡¨ç¤ºï¼›ç»„çº§å¯†é›†è’¸é¦ï¼ˆGKD-Denï¼‰é€šè¿‡æ¿€æ´»å¯¼å‡ºçš„å…³ç³»å›¾å¼•å¯¼ç»„æ„ŸçŸ¥æ³¨æ„åŠ›ï¼Œå®ç°å¯†é›†çš„è·¨æ¨¡æ€å¯¹é½ã€‚è¿™ä¸¤ä¸ªæ¨¡å—å…±åŒä½œç”¨ï¼Œåœ¨ä¸è¦æ±‚å›¾åƒçº§å¯¹åº”å…³ç³»çš„æƒ…å†µä¸‹ï¼Œå¼ºåˆ¶æ‰§è¡Œå…¨å±€è¯­ä¹‰ä¸€è‡´æ€§å’Œå±€éƒ¨ç»“æ„è¿è´¯æ€§ã€‚

**Result:** åœ¨å››ä¸ªä¸´åºŠæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPaGKDæŒç»­ä¸”æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œåˆ†åˆ«å®ç°äº†3.3%ã€1.1%ã€2.8%å’Œ3.2%çš„ç›¸å¯¹AUCæå‡ï¼Œä¸ºæ— é…å¯¹æ•°æ®çš„è·¨æ¨¡æ€å­¦ä¹ å»ºç«‹äº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚

**Conclusion:** PaGKDé€šè¿‡ç»„çº§çŸ¥è¯†è’¸é¦æ‰“ç ´äº†ä¼ ç»Ÿè·¨æ¨¡æ€å­¦ä¹ å¯¹é…å¯¹æ•°æ®çš„ä¾èµ–ï¼Œä¸ºåˆ©ç”¨å¤§é‡æœªé…å¯¹ä¸´åºŠæ•°æ®æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œå¼€è¾Ÿäº†æ— é…å¯¹è·¨æ¨¡æ€å­¦ä¹ çš„æ–°æ–¹å‘ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼å’Œç ”ç©¶æ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
White-Light Imaging (WLI) is the standard for endoscopic cancer screening, but Narrow-Band Imaging (NBI) offers superior diagnostic details. A key challenge is transferring knowledge from NBI to enhance WLI-only models, yet existing methods are critically hampered by their reliance on paired NBI-WLI images of the same lesion, a costly and often impractical requirement that leaves vast amounts of clinical data untapped. In this paper, we break this paradigm by introducing PaGKD, a novel Pairing-free Group-level Knowledge Distillation framework that that enables effective cross-modal learning using unpaired WLI and NBI data. Instead of forcing alignment between individual, often semantically mismatched image instances, PaGKD operates at the group level to distill more complete and compatible knowledge across modalities. Central to PaGKD are two complementary modules: (1) Group-level Prototype Distillation (GKD-Pro) distills compact group representations by extracting modality-invariant semantic prototypes via shared lesion-aware queries; (2) Group-level Dense Distillation (GKD-Den) performs dense cross-modal alignment by guiding group-aware attention with activation-derived relation maps. Together, these modules enforce global semantic consistency and local structural coherence without requiring image-level correspondence. Extensive experiments on four clinical datasets demonstrate that PaGKD consistently and significantly outperforms state-of-the-art methods, achieving relative AUC improvements of 3.3%, 1.1%, 2.8%, and 3.2%, respectively, establishing a new direction for cross-modal learning from unpaired data.


### [12] [SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion](https://arxiv.org/abs/2601.09213)
*Jialu Li, Taiyan Zhou*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSpikeVAEDiffï¼Œä¸€ç§ç»“åˆæ·±åº¦å˜åˆ†è‡ªç¼–ç å™¨å’Œæ‰©æ•£æ¨¡å‹çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºä»ç¥ç»å°–å³°ä¿¡å·é‡å»ºé«˜åˆ†è¾¨ç‡è§†è§‰åœºæ™¯ã€‚è¯¥æ–¹æ³•åœ¨Allenè§†è§‰ç¼–ç æ•°æ®é›†ä¸ŠéªŒè¯äº†ç‰¹å®šè„‘åŒºå¯¹é‡å»ºè´¨é‡çš„å…³é”®ä½œç”¨ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä»ç¥ç»æ´»åŠ¨é‡å»ºè‡ªç„¶è§†è§‰åœºæ™¯çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨ç¥ç»å°–å³°ä¿¡å·è€ŒéfMRIæ•°æ®ï¼Œä»¥è·å–æ›´ä¼˜è¶Šçš„æ—¶ç©ºåˆ†è¾¨ç‡ã€‚å½“å‰æ–¹æ³•åœ¨ä»é«˜ç»´ç¥ç»æ•°æ®ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€è¯­ä¹‰æœ‰æ„ä¹‰çš„å›¾åƒé‡å»ºæ–¹é¢å­˜åœ¨å±€é™ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„è§£ç æ¡†æ¶ã€‚

**Method:** æœ¬æ–‡æå‡ºSpikeVAEDiffä¸¤é˜¶æ®µæ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨æ·±åº¦å˜åˆ†è‡ªç¼–ç å™¨å°†ç¥ç»å°–å³°ä¿¡å·æ˜ å°„åˆ°æ½œåœ¨è¡¨ç¤ºï¼Œç”Ÿæˆä½åˆ†è¾¨ç‡åˆæ­¥é‡å»ºï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡å›å½’æ¨¡å‹å°†å°–å³°ä¿¡å·æ˜ å°„åˆ°CLIPè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼Œåˆ©ç”¨Versatile Diffusionæ¨¡å‹é€šè¿‡å›¾åƒåˆ°å›¾åƒç”Ÿæˆè¿›è¡Œç²¾ç»†åŒ–é‡å»ºã€‚

**Result:** åœ¨Allenè§†è§‰ç¼–ç -ç¥ç»åƒç´ æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒVISIè„‘åŒºè¡¨ç°å‡ºæœ€æ˜¾è‘—çš„æ¿€æ´»å¹¶åœ¨é‡å»ºè´¨é‡ä¸­èµ·å…³é”®ä½œç”¨ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ç‰¹å®šè„‘åŒºæ•°æ®æ˜¾è‘—æå‡é‡å»ºæ€§èƒ½ï¼Œä¸fMRIæ–¹æ³•ç›¸æ¯”ï¼Œå°–å³°æ•°æ®æä¾›äº†æ›´ä¼˜è¶Šçš„æ—¶ç©ºåˆ†è¾¨ç‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆæ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç¥ç»è§£ç æä¾›äº†æ–°èŒƒå¼ã€‚å°–å³°æ•°æ®ç›¸å¯¹äºfMRIçš„ä¼˜è¶Šåˆ†è¾¨ç‡ä¼˜åŠ¿å¾—åˆ°éªŒè¯ï¼Œç‰¹å®šè„‘åŒºå¯¹è§†è§‰é‡å»ºçš„å…³é”®ä½œç”¨ä¸ºç¥ç»ç¼–ç æœºåˆ¶ç ”ç©¶æä¾›äº†é‡è¦è§è§£ã€‚

---

#### ğŸ“„ Abstract
Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.
  We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.


### [13] [Disentangle Object and Non-object Infrared Features via Language Guidance](https://arxiv.org/abs/2601.09228)
*Fan Liu, Ting Wu, Chuanyi Zhang, Liang Yao, Xing Ma, Yuhui Zheng*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰-è¯­è¨€è¡¨ç¤ºå­¦ä¹ èŒƒå¼ç”¨äºçº¢å¤–ç›®æ ‡æ£€æµ‹ï¼Œé€šè¿‡å¼•å…¥æ–‡æœ¬ç›‘ç£æ¥å¼•å¯¼å¯¹è±¡ä¸éå¯¹è±¡ç‰¹å¾çš„è§£è€¦ï¼Œä»è€Œæ˜¾è‘—æå‡æ£€æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** çº¢å¤–ç›®æ ‡æ£€æµ‹åœ¨é»‘æš—ã€é›ªå¤©ã€é›¨å¤©ç­‰å¤æ‚ç¯å¢ƒä¸­å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ï¼Œä½†ç”±äºçº¢å¤–å›¾åƒå¯¹æ¯”åº¦ä½ã€è¾¹ç¼˜ä¿¡æ¯å¼±ï¼Œéš¾ä»¥æå–å…·æœ‰åˆ¤åˆ«æ€§çš„ç›®æ ‡ç‰¹å¾ï¼Œå¯¼è‡´æ£€æµ‹é²æ£’æ€§ä¸è¶³ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ç§è§†è§‰-è¯­è¨€è¡¨ç¤ºå­¦ä¹ èŒƒå¼ï¼ŒåŒ…å«è¯­ä¹‰ç‰¹å¾å¯¹é½æ¨¡å—å°†ç›®æ ‡ç‰¹å¾ä¸å¯¹åº”æ–‡æœ¬ç‰¹å¾å¯¹é½ï¼Œä»¥åŠå¯¹è±¡ç‰¹å¾è§£è€¦æ¨¡å—é€šè¿‡æœ€å°åŒ–ç›¸å…³æ€§æ¥åˆ†ç¦»æ–‡æœ¬å¯¹é½çš„ç›®æ ‡ç‰¹å¾ä¸éç›®æ ‡ç‰¹å¾ï¼Œæœ€ç»ˆå°†è§£è€¦åçš„ç›®æ ‡ç‰¹å¾è¾“å…¥æ£€æµ‹å¤´ã€‚

**Result:** åœ¨MÂ³FDå’ŒFLIRä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†ä¼˜è¶Šæ€§èƒ½ï¼Œåˆ†åˆ«è¾¾åˆ°83.7%å’Œ86.1%çš„mAPï¼Œæ˜¾è‘—æå‡äº†çº¢å¤–ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å¼•å…¥æ–‡æœ¬ç›‘ç£è¿›è¡Œç‰¹å¾è§£è€¦çš„æœ‰æ•ˆæ€§ï¼Œä¸ºçº¢å¤–ç›®æ ‡æ£€æµ‹æä¾›äº†æ–°çš„è§†è§‰-è¯­è¨€å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡å¢å¼ºç‰¹å¾åˆ¤åˆ«æ€§å’Œå‡å°‘å™ªå£°ç‰¹å¾ï¼Œæ˜¾è‘—æ”¹å–„äº†å¤æ‚ç¯å¢ƒä¸‹çš„æ£€æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“„ Abstract
Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.


### [14] [PhyRPR: Training-Free Physics-Constrained Video Generation](https://arxiv.org/abs/2601.09255)
*Yibo Zhao, Hengjia Li, Xiaofei He, Boxi Wu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PhyRPRï¼Œä¸€ç§å…è®­ç»ƒçš„ä¸‰é˜¶æ®µæµæ°´çº¿ï¼Œé€šè¿‡å°†ç‰©ç†ç†è§£ä¸è§†è§‰åˆæˆè§£è€¦æ¥è§£å†³æ‰©æ•£è§†é¢‘ç”Ÿæˆæ¨¡å‹éš¾ä»¥æ»¡è¶³ç‰©ç†çº¦æŸçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ç‰©ç†æ¨ç†ã€è¿åŠ¨è§„åˆ’å’Œå¤–è§‚ç»†åŒ–çš„åˆ†ç¦»è®¾è®¡ï¼Œå®ç°äº†ç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹ç‰©ç†çš„æ˜¾å¼æ§åˆ¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºæ‰©æ•£çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è™½ç„¶èƒ½åˆæˆè§†è§‰ä¸Šåˆç†çš„è§†é¢‘ï¼Œä½†å¾€å¾€éš¾ä»¥æ»¡è¶³ç‰©ç†çº¦æŸã€‚ä¸»è¦åŸå› æ˜¯å¤§å¤šæ•°æ–¹æ³•é‡‡ç”¨å•é˜¶æ®µè®¾è®¡ï¼Œå°†é«˜å±‚ç‰©ç†ç†è§£ä¸ä½å±‚è§†è§‰åˆæˆçº ç¼ åœ¨ä¸€èµ·ï¼Œä½¿å¾—éœ€è¦æ˜¾å¼ç‰©ç†æ¨ç†çš„å†…å®¹ç”Ÿæˆå˜å¾—å›°éš¾ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†å…è®­ç»ƒçš„ä¸‰é˜¶æ®µæµæ°´çº¿PhyRPRï¼ŒåŒ…å«ç‰©ç†æ¨ç†ã€ç‰©ç†è§„åˆ’å’Œç‰©ç†ç»†åŒ–ä¸‰ä¸ªé˜¶æ®µã€‚PhyReasonä½¿ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œç‰©ç†çŠ¶æ€æ¨ç†å’Œå›¾åƒç”Ÿæˆå™¨åˆæˆå…³é”®å¸§ï¼›PhyPlanç¡®å®šæ€§åœ°åˆæˆå¯æ§çš„ç²—ç•¥è¿åŠ¨æ”¯æ¶ï¼›PhyRefineé€šè¿‡æ½œåœ¨èåˆç­–ç•¥å°†æ”¯æ¶æ³¨å…¥æ‰©æ•£é‡‡æ ·ä¸­ï¼Œåœ¨ä¿ç•™è§„åˆ’åŠ¨æ€çš„åŒæ—¶ç»†åŒ–å¤–è§‚ã€‚

**Result:** åœ¨ç‰©ç†çº¦æŸä¸‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æŒç»­æé«˜äº†ç‰©ç†åˆç†æ€§å’Œè¿åŠ¨å¯æ§æ€§ã€‚ä¸‰é˜¶æ®µè®¾è®¡ä½¿å¾—åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤Ÿå®ç°æ˜¾å¼çš„ç‰©ç†æ§åˆ¶ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•åœ¨æ»¡è¶³ç‰©ç†çº¦æŸæ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡è§£è€¦ç‰©ç†ç†è§£ä¸è§†è§‰åˆæˆçš„ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œä¸ºè§†é¢‘ç”Ÿæˆä¸­çš„ç‰©ç†çº¦æŸé—®é¢˜æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚åˆ†é˜¶æ®µè®¾è®¡ä¸ä»…æé«˜äº†ç‰©ç†åˆç†æ€§ï¼Œè¿˜å¢å¼ºäº†è¿åŠ¨å¯æ§æ€§ï¼Œä¸ºéœ€è¦æ˜¾å¼ç‰©ç†æ¨ç†çš„ç”Ÿæˆä»»åŠ¡å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,\textit{PhyRPR}:\textit{Phy\uline{R}eason}--\textit{Phy\uline{P}lan}--\textit{Phy\uline{R}efine}, which decouples physical understanding from visual synthesis. Specifically, \textit{PhyReason} uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; \textit{PhyPlan} deterministically synthesizes a controllable coarse motion scaffold; and \textit{PhyRefine} injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.


### [15] [Multi-Modal LLM based Image Captioning in ICT: Bridging the Gap Between General and Industry Domain](https://arxiv.org/abs/2601.09298)
*Lianying Chao, Haoran Cai, Xubin Li, Kai Zhang, Sijie Wu, Rui Xu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µæ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œç”¨äºè®­ç»ƒICTé¢†åŸŸçš„ä¸“ç”¨å›¾åƒæè¿°æ¨¡å‹ï¼ˆDICModelï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨ä»…ä½¿ç”¨7Bå‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½è¶…è¶Šäº†32Bå‚æ•°çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†é¢†åŸŸå›¾åƒç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åœ¨ICTè¡Œä¸šä¸­ï¼Œè®­ç»ƒé¢†åŸŸä¸“ç”¨å¤§è¯­è¨€æ¨¡å‹æˆ–æ„å»ºæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿéœ€è¦å¤§é‡é«˜ä»·å€¼é¢†åŸŸçŸ¥è¯†ï¼Œä½†è¿™äº›çŸ¥è¯†ä¸ä»…éšè—åœ¨æ–‡æœ¬æ¨¡æ€ä¸­ï¼Œä¹Ÿå­˜åœ¨äºå›¾åƒæ¨¡æ€ä¸­ã€‚ä¼ ç»Ÿæ–¹æ³•åªèƒ½è§£æé¢†åŸŸæ–‡æ¡£ä¸­çš„æ–‡æœ¬è€Œç¼ºä¹å›¾åƒæè¿°èƒ½åŠ›ï¼Œè€Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è™½ç„¶èƒ½ç†è§£å›¾åƒï¼Œå´ç¼ºä¹è¶³å¤Ÿçš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µæ¸è¿›å¼è®­ç»ƒç­–ç•¥æ¥è®­ç»ƒICTé¢†åŸŸçš„ä¸“ç”¨å›¾åƒæè¿°æ¨¡å‹ã€‚é¦–å…ˆä½¿ç”¨Mermaidå·¥å…·å’Œå¤§è¯­è¨€æ¨¡å‹åˆæˆçº¦7Kå›¾åƒ-æ–‡æœ¬å¯¹è¿›è¡Œç¬¬ä¸€é˜¶æ®µç›‘ç£å¾®è°ƒï¼Œç„¶åç”±ICTé¢†åŸŸä¸“å®¶æ‰‹åŠ¨æ ‡æ³¨çº¦2Kå›¾åƒ-æ–‡æœ¬å¯¹è¿›è¡Œç¬¬äºŒé˜¶æ®µç›‘ç£å¾®è°ƒï¼Œæœ€åä¸“å®¶ä¸å¤§è¯­è¨€æ¨¡å‹è”åˆåˆæˆçº¦1.5Kè§†è§‰é—®ç­”æ•°æ®è¿›è¡ŒåŸºäºæŒ‡ä»¤çš„ç›‘ç£å¾®è°ƒã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨7Bå‚æ•°çš„DICModelæ€§èƒ½ä¼˜äºå…¶ä»–32Bå‚æ•°çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚ä¸7Bå’Œ32Bå‚æ•°çš„SOTAæ¨¡å‹ç›¸æ¯”ï¼ŒDICModelå°†BLEUæŒ‡æ ‡åˆ†åˆ«æé«˜äº†çº¦56.8%å’Œ20.8%ã€‚åœ¨ICTé¢†åŸŸä¸“å®¶æ„å»ºçš„å®¢è§‚é—®é¢˜ä¸Šï¼ŒDICModelåœ¨å‡†ç¡®ç‡ä¸Šæ¯”Qwen2.5-VL 32Bé«˜å‡º1%ã€‚

**Conclusion:** è¯¥ç ”ç©¶èƒ½å¤Ÿé«˜æ•ˆå‡†ç¡®åœ°ä»å›¾åƒä¸­æå–é€»è¾‘æ–‡æœ¬ï¼Œæœ‰æœ›ä¿ƒè¿›ICTé¢†åŸŸå¤šæ¨¡æ€æ¨¡å‹çš„å‘å±•ã€‚æå‡ºçš„å¤šé˜¶æ®µæ¸è¿›å¼è®­ç»ƒç­–ç•¥å’Œæ ‡å‡†è¯„ä¼°ç³»ç»Ÿä¸ºé¢†åŸŸä¸“ç”¨å›¾åƒç†è§£æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œè¯æ˜äº†åœ¨æœ‰é™å‚æ•°è§„æ¨¡ä¸‹é€šè¿‡é’ˆå¯¹æ€§è®­ç»ƒå¯ä»¥è·å¾—è¶…è¶Šæ›´å¤§é€šç”¨æ¨¡å‹çš„é¢†åŸŸæ€§èƒ½ã€‚

---

#### ğŸ“„ Abstract
In the information and communications technology (ICT) industry, training a domain-specific large language model (LLM) or constructing a retrieval-augmented generation system requires a substantial amount of high-value domain knowledge. However, the knowledge is not only hidden in the textual modality but also in the image modality. Traditional methods can parse text from domain documents but dont have image captioning ability. Multi-modal LLM (MLLM) can understand images, but they do not have sufficient domain knowledge. To address the above issues, this paper proposes a multi-stage progressive training strategy to train a Domain-specific Image Captioning Model (DICModel) in ICT, and constructs a standard evaluation system to validate the performance of DICModel. Specifically, this work first synthesizes about 7K image-text pairs by combining the Mermaid tool and LLMs, which are used for the first-stage supervised-fine-tuning (SFT) of DICModel. Then, ICT-domain experts manually annotate about 2K image-text pairs for the second-stage SFT of DICModel. Finally, experts and LLMs jointly synthesize about 1.5K visual question answering data for the instruction-based SFT. Experimental results indicate that our DICModel with only 7B parameters performs better than other state-of-the-art models with 32B parameters. Compared to the SOTA models with 7B and 32B parameters, our DICModel increases the BLEU metric by approximately 56.8% and 20.8%, respectively. On the objective questions constructed by ICT domain experts, our DICModel outperforms Qwen2.5-VL 32B by 1% in terms of accuracy rate. In summary, this work can efficiently and accurately extract the logical text from images, which is expected to promote the development of multimodal models in the ICT domain.


### [16] [See More, Store Less: Memory-Efficient Resolution for Video Moment Retrieval](https://arxiv.org/abs/2601.09350)
*Mingyu Jeon, Sungjin Han, Jinkwon Hwang, Minchol Kwon, Jonghee Kim, Junyeong Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSMOREæ¡†æ¶ï¼Œé€šè¿‡æŸ¥è¯¢å¼•å¯¼çš„è¯­ä¹‰ç¼–ç ã€é‡è¦æ€§è°ƒåˆ¶å’Œè‡ªé€‚åº”å¸§å‹ç¼©ï¼Œåœ¨ä¿æŒé«˜ä¿¡æ¯åˆ†è¾¨ç‡çš„åŒæ—¶æ˜¾è‘—æå‡è§†é¢‘æ—¶åˆ»æ£€ç´¢çš„å†…å­˜æ•ˆç‡ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒè¯†åˆ«å’Œæ¨ç†æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†è§†é¢‘ç›¸å…³ä»»åŠ¡ä»é¢ä¸´å†…å­˜é™åˆ¶çš„æŒ‘æˆ˜ï¼Œç°æœ‰è§†é¢‘æ—¶åˆ»æ£€ç´¢æ–¹æ³•ä¾èµ–ç¨€ç–å¸§é‡‡æ ·å¯èƒ½å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿è§†é¢‘å¤„ç†ä¸­ã€‚

**Method:** SMOREæ¡†æ¶é‡‡ç”¨ä¸‰ä¸ªå…³é”®æŠ€æœ¯ï¼šä½¿ç”¨æŸ¥è¯¢å¼•å¯¼çš„æ ‡é¢˜ç¼–ç ä¸ç”¨æˆ·æ„å›¾å¯¹é½çš„è¯­ä¹‰ï¼›åº”ç”¨æŸ¥è¯¢æ„ŸçŸ¥çš„é‡è¦æ€§è°ƒåˆ¶çªå‡ºç›¸å…³ç‰‡æ®µï¼›è‡ªé€‚åº”å‹ç¼©å¸§ä»¥ä¿ç•™å…³é”®å†…å®¹åŒæ—¶å‡å°‘å†—ä½™ï¼Œå®ç°é«˜æ•ˆè§†é¢‘ç†è§£è€Œä¸è¶…å‡ºå†…å­˜é¢„ç®—ã€‚

**Result:** å®éªŒéªŒè¯è¡¨æ˜ï¼ŒSMOREåœ¨QVHighlightsã€Charades-STAå’ŒActivityNet-CaptionsåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨ä¿æŒé«˜ä¿¡æ¯åˆ†è¾¨ç‡çš„åŒæ—¶æ˜¾è‘—æå‡å†…å­˜æ•ˆç‡çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡è¯­ä¹‰å¯¹é½å’Œè‡ªé€‚åº”å‹ç¼©ç­–ç•¥å¯ä»¥æœ‰æ•ˆè§£å†³è§†é¢‘å¤„ç†ä¸­çš„å†…å­˜ç“¶é¢ˆé—®é¢˜ï¼Œä¸ºé«˜æ•ˆè§†é¢‘ç†è§£æä¾›äº†æ–°æ€è·¯ï¼Œæœªæ¥å¯æ‰©å±•è‡³æ›´å¤æ‚çš„å¤šæ¨¡æ€è§†é¢‘åˆ†æä»»åŠ¡ã€‚

---

#### ğŸ“„ Abstract
Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks.


### [17] [Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma Histology Classification](https://arxiv.org/abs/2601.09416)
*Yaxi Chen, Zi Ye, Shaheer U. Saeed, Oliver Yu, Simin Ni, Jie Huang, Yipeng Hu*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ”¾å°„ç»„å­¦ç‰¹å¾å’Œåˆ†å±‚æŸå¤±å‡½æ•°çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºéª¨è‚‰ç˜¤ç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­è‚¿ç˜¤åŒºåŸŸï¼ˆå­˜æ´»ä¸éå­˜æ´»ï¼‰çš„è‡ªåŠ¨åˆ†ç±»ï¼Œæ˜¾è‘—æå‡äº†åœ¨æ‚£è€…çº§åˆ«ç‹¬ç«‹é‡‡æ ·æµ‹è¯•æ•°æ®ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éª¨è‚‰ç˜¤æ–°è¾…åŠ©åŒ–ç–—åå­˜æ´»ä¸éå­˜æ´»è‚¿ç˜¤åŒºåŸŸçš„å‡†ç¡®ç»„ç»‡ç—…ç†å­¦è¯„ä¼°å¯¹é¢„åå’Œæ²»ç–—è§„åˆ’è‡³å…³é‡è¦ï¼Œä½†äººå·¥è¯„ä¼°å­˜åœ¨åŠ³åŠ¨å¯†é›†ã€ä¸»è§‚æ€§å¼ºå’Œè§‚å¯Ÿè€…é—´å˜å¼‚å¤§çš„é—®é¢˜ã€‚å…ˆå‰ç ”ç©¶åœ¨ç“¦ç‰‡çº§åˆ«æ³›åŒ–èƒ½åŠ›è‰¯å¥½ï¼Œä½†åœ¨æ‚£è€…çº§åˆ«ç‹¬ç«‹é‡‡æ ·æµ‹è¯•æ•°æ®ä¸Šæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚

**Method:** æœ¬ç ”ç©¶æå‡ºäº†ä¸¤ç§å…³é”®æ–¹æ³•æ”¹è¿›ï¼šé¦–å…ˆå¼•å…¥ä»å›¾åƒè¡ç”Ÿçš„æ”¾å°„ç»„å­¦ç‰¹å¾ä½œä¸ºå¤šæ¨¡æ€è¾“å…¥ï¼Œå¢å¼ºæ¨¡å‹æ€§èƒ½å¹¶æå‡å¯è§£é‡Šæ€§ï¼›å…¶æ¬¡é‡‡ç”¨åˆ†å±‚åˆ†ç±»ç­–ç•¥ï¼Œå°†å•ä¸€çš„ä¸‰åˆ†ç±»ä»»åŠ¡åˆ†è§£ä¸ºè‚¿ç˜¤ä¸éè‚¿ç˜¤ã€å­˜æ´»ä¸éå­˜æ´»ä¸¤ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶è®¾è®¡å¯è®­ç»ƒæƒé‡çš„åˆ†å±‚æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å„ç±»åˆ«æ€§èƒ½ã€‚

**Result:** åœ¨TCIAéª¨è‚‰ç˜¤è‚¿ç˜¤è¯„ä¼°æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ”¾å°„ç»„å­¦ç‰¹å¾çš„å¼•å…¥å’Œåˆ†å±‚æŸå¤±å‡½æ•°çš„åº”ç”¨å‡èƒ½æ˜¾è‘—æå‡åˆ†ç±»æ€§èƒ½ï¼Œä¸¤è€…ç»“åˆå®ç°äº†è¯¥å…¬å¼€æ•°æ®é›†ä¸Šè¯¥åº”ç”¨çš„æœ€ä½³æ€§èƒ½ï¼Œæœ‰æ•ˆè§£å†³äº†æ‚£è€…çº§åˆ«æµ‹è¯•æ•°æ®ä¸Šçš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€è¾“å…¥å’Œåˆ†å±‚å­¦ä¹ ç­–ç•¥åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç»„ç»‡ç—…ç†å­¦è‡ªåŠ¨è¯„ä¼°æä¾›äº†æ–°çš„æŠ€æœ¯æ¡†æ¶ã€‚æ”¾å°„ç»„å­¦ç‰¹å¾çš„å¼•å…¥ä¸ä»…æå‡äº†æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œè€Œåˆ†å±‚æŸå¤±å‡½æ•°åˆ™é€šè¿‡ä»»åŠ¡åˆ†è§£ä¼˜åŒ–äº†åˆ†ç±»ç²¾åº¦ï¼Œä¸ºç±»ä¼¼åŒ»å­¦å›¾åƒåˆ†æé—®é¢˜æä¾›äº†å¯å€Ÿé‰´çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Osteosarcoma (OS) is an aggressive primary bone malignancy. Accurate histopathological assessment of viable versus non-viable tumor regions after neoadjuvant chemotherapy is critical for prognosis and treatment planning, yet manual evaluation remains labor-intensive, subjective, and prone to inter-observer variability. Recent advances in digital pathology have enabled automated necrosis quantification. Evaluating on test data, independently sampled on patient-level, revealed that the deep learning model performance dropped significantly from the tile-level generalization ability reported in previous studies. First, this work proposes the use of radiomic features as additional input in model training. We show that, despite that they are derived from the images, such a multimodal input effectively improved the classification performance, in addition to its added benefits in interpretability. Second, this work proposes to optimize two binary classification tasks with hierarchical classes (i.e. tumor-vs-non-tumor and viable-vs-non-viable), as opposed to the alternative ``flat'' three-class classification task (i.e. non-tumor, non-viable tumor, viable tumor), thereby enabling a hierarchical loss. We show that such a hierarchical loss, with trainable weightings between the two tasks, the per-class performance can be improved significantly. Using the TCIA OS Tumor Assessment dataset, we experimentally demonstrate the benefits from each of the proposed new approaches and their combination, setting a what we consider new state-of-the-art performance on this open dataset for this application. Code and trained models: https://github.com/YaxiiC/RadiomicsOS.git.


### [18] [Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs](https://arxiv.org/abs/2601.09430)
*Rui Zhu, Xin Shen, Shuchen Wu, Chenxi Miao, Xin Yu, Yang Li, Weikang Li, Deguo Xia, Jizhou Huang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Video-MSRï¼Œé¦–ä¸ªä¸“é—¨è¯„ä¼°åŠ¨æ€è§†é¢‘åœºæ™¯ä¸­å¤šè·³ç©ºé—´æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œå¹¶æ„å»ºäº†MSR-9KæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œé€šè¿‡å¾®è°ƒQwen-VLæ¨¡å‹å®ç°äº†+7.82%çš„æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†åŸºå‡†ä¸»è¦å…³æ³¨å•æ­¥æ„ŸçŸ¥åˆ°åˆ¤æ–­ä»»åŠ¡ï¼Œè€Œéœ€è¦å¤æ‚è§†è§‰-ç©ºé—´é€»è¾‘é“¾çš„åœºæ™¯ç ”ç©¶ä¸¥é‡ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åŠ¨æ€è§†é¢‘ä¸­çš„å¤šè·³ç©ºé—´æ¨ç†èƒ½åŠ›è¯„ä¼°å­˜åœ¨æ˜¾è‘—ç©ºç™½ã€‚

**Method:** ç ”ç©¶æå‡ºäº†Video-MSRåŸºå‡†ï¼ŒåŒ…å«çº¦æŸå®šä½ã€é“¾å¼å‚è€ƒæ£€ç´¢ã€è·¯å¾„è§„åˆ’å’Œåäº‹å®ç‰©ç†æ¨ç†å››ä¸ªä»»åŠ¡ï¼ŒåŒ…å«3,052ä¸ªé«˜è´¨é‡è§†é¢‘å®ä¾‹å’Œ4,993ä¸ªé—®ç­”å¯¹ï¼Œé‡‡ç”¨ç»“åˆå…ˆè¿›æ¨¡å‹ç”Ÿæˆä¸ä¸¥æ ¼äººå·¥éªŒè¯çš„å¯æ‰©å±•è§†è§‰åŸºç¡€æµç¨‹æ„å»ºï¼Œå¹¶è¿›ä¸€æ­¥æ„å»ºäº†MSR-9Kä¸“ç”¨æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ç”¨äºæ¨¡å‹å¾®è°ƒã€‚

**Result:** å¯¹20ä¸ªæœ€å…ˆè¿›MLLMçš„è¯„ä¼°æ­ç¤ºäº†æ˜¾è‘—å±€é™æ€§ï¼šæ¨¡å‹åœ¨è¡¨å±‚æ„ŸçŸ¥æ–¹é¢è¡¨ç°ç†Ÿç»ƒï¼Œä½†åœ¨MSRä»»åŠ¡ä¸­æ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼Œç»å¸¸åœ¨å¤šæ­¥æ¨ç†ä¸­å‡ºç°ç©ºé—´è¿·å¤±å’Œå¹»è§‰é—®é¢˜ï¼›é€šè¿‡MSR-9Kæ•°æ®é›†å¾®è°ƒQwen-VLæ¨¡å‹ï¼Œåœ¨Video-MSRåŸºå‡†ä¸Šå®ç°äº†+7.82%çš„ç»å¯¹æ€§èƒ½æå‡ã€‚

**Conclusion:** å¤šè·³ç©ºé—´æŒ‡ä»¤æ•°æ®çš„æœ‰æ•ˆæ€§å¾—åˆ°éªŒè¯ï¼ŒVideo-MSRåŸºå‡†ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€ï¼Œæ­ç¤ºäº†å½“å‰MLLMåœ¨å¤æ‚ç©ºé—´æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œå¹¶å±•ç¤ºäº†é€šè¿‡ä¸“é—¨æŒ‡ä»¤è°ƒä¼˜å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„å¤šè·³ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.


### [19] [Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?](https://arxiv.org/abs/2601.09433)
*David Reid, Ognjen Arandjelovic*

#### ğŸ§© TL;DR
æœ¬æ–‡é¦–æ¬¡å°†Vision Transformeræ¶æ„åº”ç”¨äºå¤é’±å¸è¯­ä¹‰å…ƒç´ è¯†åˆ«ä»»åŠ¡ï¼Œé€šè¿‡å¤šæ¨¡æ€æ•°æ®è‡ªåŠ¨å­¦ä¹ ï¼Œå‘ç°ViTæ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šè¶…è¶Šäº†æ–°è®­ç»ƒçš„CNNæ¨¡å‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤é’±å¸è‡ªåŠ¨åˆ†ææœ‰åŠ©äºç ”ç©¶äººå‘˜ä»å¤§é‡é’±å¸æ”¶è—ä¸­æå–æ›´å¤šå†å²è§è§£ï¼Œå¹¶å¸®åŠ©æ”¶è—è€…ç†è§£å…¶äº¤æ˜“å¯¹è±¡ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œè¯†åˆ«é’±å¸ä¸Šçš„è¯­ä¹‰å…ƒç´ ï¼Œä½†å°šæœªæ¢ç´¢æœ€è¿‘æå‡ºçš„Vision Transformeræ¶æ„åœ¨è¯¥é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚

**Method:** æœ¬ç ”ç©¶é¦–æ¬¡å°†Vision Transformeræ·±åº¦å­¦ä¹ æ¶æ„åº”ç”¨äºå¤é’±å¸è¯­ä¹‰å…ƒç´ è¯†åˆ«ä»»åŠ¡ï¼Œé‡‡ç”¨å®Œå…¨è‡ªåŠ¨åŒ–çš„å¤šæ¨¡æ€æ•°æ®å­¦ä¹ æ–¹æ³•ï¼ŒåŒæ—¶å¤„ç†å›¾åƒå’Œéç»“æ„åŒ–æ–‡æœ¬æ•°æ®ã€‚ç ”ç©¶è¿˜è®­ç»ƒäº†CNNæ¨¡å‹ä½œä¸ºå¯¹æ¯”åŸºå‡†ï¼Œå¹¶è¯¦ç»†è®¨è®ºäº†ViTå’ŒCNNæ¨¡å‹çš„è®­ç»ƒä¸å®ç°è¿‡ç¨‹ã€‚

**Result:** å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒVision Transformeræ¨¡å‹åœ¨å¤é’±å¸è¯­ä¹‰å…ƒç´ è¯†åˆ«ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡è¶…è¶Šäº†æ–°è®­ç»ƒçš„CNNæ¨¡å‹ã€‚ç ”ç©¶æä¾›äº†ä¸¤ç§æ¶æ„çš„æ€§èƒ½å¯¹æ¯”åˆ†æï¼ŒéªŒè¯äº†ViTåœ¨è¯¥ç‰¹å®šè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šè¡¨ç°ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜Vision Transformeræ¶æ„åœ¨å¤é’±å¸åˆ†æé¢†åŸŸå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæ–‡åŒ–é—äº§æ•°å­—åŒ–å’Œè‡ªåŠ¨åˆ†ææä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ç»“åˆå›¾åƒä¸æ–‡æœ¬ä¿¡æ¯ï¼Œä¸ºå¤æ‚å†å²æ–‡ç‰©çš„æ™ºèƒ½è¯†åˆ«å¼€è¾Ÿäº†åˆ›æ–°æ–¹å‘ï¼Œæœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢Transformeræ¶æ„åœ¨æ–‡ç‰©åˆ†æä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
Automated analysis of ancient coins has the potential to help researchers extract more historical insights from large collections of coins and to help collectors understand what they are buying or selling. Recent research in this area has shown promise in focusing on identification of semantic elements as they are commonly depicted on ancient coins, by using convolutional neural networks (CNNs). This paper is the first to apply the recently proposed Vision Transformer (ViT) deep learning architecture to the task of identification of semantic elements on coins, using fully automatic learning from multi-modal data (images and unstructured text). This article summarises previous research in the area, discusses the training and implementation of ViT and CNN models for ancient coins analysis and provides an evaluation of their performance. The ViT models were found to outperform the newly trained CNN models in accuracy.


### [20] [PrivLEX: Detecting legal concepts in images through Vision-Language Models](https://arxiv.org/abs/2601.09449)
*Darya Baranouskaya, Andrea Cavallaro*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºPrivLEXï¼Œä¸€ç§åŸºäºæ³•å¾‹å®šä¹‰çš„ä¸ªäººæ•°æ®æ¦‚å¿µè¿›è¡Œå†³ç­–çš„æ–°å‹å›¾åƒéšç§åˆ†ç±»å™¨ï¼Œè¿™æ˜¯é¦–ä¸ªåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¯†åˆ«èƒ½åŠ›å¹¶ä¸æ³•å¾‹æ¦‚å¿µå¯¹é½çš„å¯è§£é‡Šéšç§åˆ†ç±»å™¨ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å›¾åƒéšç§åˆ†ç±»å™¨ç¼ºä¹ä¸æ³•å¾‹å®šä¹‰çš„ä¸ªäººæ•°æ®æ¦‚å¿µçš„å¯¹é½ï¼Œå¯¼è‡´å†³ç­–è¿‡ç¨‹ä¸é€æ˜ä¸”éš¾ä»¥è§£é‡Šï¼Œæ— æ³•æ»¡è¶³éšç§ä¿æŠ¤çš„å®é™…æ³•å¾‹éœ€æ±‚ã€‚

**Method:** PrivLEXé‡‡ç”¨é›¶æ ·æœ¬è§†è§‰è¯­è¨€æ¨¡å‹æ¦‚å¿µæ£€æµ‹æŠ€æœ¯ï¼Œé€šè¿‡æ— æ ‡ç­¾æ¦‚å¿µç“¶é¢ˆæ¨¡å‹å®ç°å¯è§£é‡Šåˆ†ç±»ï¼Œæ— éœ€è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ˜¾å¼æ¦‚å¿µæ ‡æ³¨ï¼Œå°†VLMè¯†åˆ«èƒ½åŠ›ä¸æ³•å¾‹æ¦‚å¿µæ¡†æ¶ç›¸ç»“åˆã€‚

**Result:** å®éªŒè¯æ˜PrivLEXèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å›¾åƒä¸­çš„ä¸ªäººæ•°æ®æ¦‚å¿µï¼Œå¹¶åˆ†æäº†äººç±»æ ‡æ³¨è€…å¯¹å›¾åƒéšç§æ•°æ®é›†ä¸­æ­¤ç±»æ¦‚å¿µæ•æ„Ÿåº¦çš„æ„ŸçŸ¥å·®å¼‚ï¼ŒéªŒè¯äº†æ¨¡å‹çš„æ³•å¾‹æ¦‚å¿µå¯¹é½èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºéšç§ä¿æŠ¤é¢†åŸŸæä¾›äº†é¦–ä¸ªæ³•å¾‹å¯¹é½çš„å¯è§£é‡Šåˆ†ç±»æ¡†æ¶ï¼Œé€šè¿‡VLMçš„é›¶æ ·æœ¬èƒ½åŠ›å®ç°äº†æ— éœ€æ ‡æ³¨çš„æ¦‚å¿µæ£€æµ‹ï¼Œä¸ºéšç§æ•æ„Ÿåº”ç”¨çš„é€æ˜å†³ç­–å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
We present PrivLEX, a novel image privacy classifier that grounds its decisions in legally defined personal data concepts. PrivLEX is the first interpretable privacy classifier aligned with legal concepts that leverages the recognition capabilities of Vision-Language Models (VLMs). PrivLEX relies on zero-shot VLM concept detection to provide interpretable classification through a label-free Concept Bottleneck Model, without requiring explicit concept labels during training. We demonstrate PrivLEX's ability to identify personal data concepts that are present in images. We further analyse the sensitivity of such concepts as perceived by human annotators of image privacy datasets.


### [21] [Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity](https://arxiv.org/abs/2601.09497)
*Ritabrata Chakraborty, Hrishit Mitra, Shivakumara Palaiahnakote, Umapada Pal*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶é€šè¿‡è®¾å®šç‰¹å¼‚æ€§è§†è§’ç³»ç»Ÿåˆ†æäº†è·¨æ•°æ®é›†ç›®æ ‡æ£€æµ‹é—®é¢˜ï¼Œæ­ç¤ºäº†åœ¨ç›¸åŒè®¾å®šç±»å‹å†…è¿ç§»ç›¸å¯¹ç¨³å®šè€Œè·¨ç±»å‹è¿ç§»æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„è§„å¾‹ï¼Œå¹¶æä¾›äº†è¯„ä¼°åˆ†å¸ƒåç§»ä¸‹æ£€æµ‹å™¨çš„å®ç”¨æŒ‡å¯¼ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç›®æ ‡æ£€æµ‹å™¨åœ¨åˆ†å¸ƒå†…è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸Šæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡è®¾å®šç‰¹å¼‚æ€§çš„è§†è§’æ¥ç³»ç»Ÿåˆ†æè·¨æ•°æ®é›†ç›®æ ‡æ£€æµ‹é—®é¢˜ï¼Œæ¢ç©¶æ£€æµ‹å™¨åœ¨ä¸åŒç±»å‹æ•°æ®é›†é—´è¿ç§»æ—¶çš„æ€§èƒ½å˜åŒ–è§„å¾‹ï¼Œç‰¹åˆ«æ˜¯åŒºåˆ†é¢†åŸŸåç§»å’Œæ ‡ç­¾ä¸åŒ¹é…çš„å½±å“ã€‚

**Method:** ç ”ç©¶å°†åŸºå‡†æ•°æ®é›†åˆ†ä¸ºè®¾å®šæ— å…³æ•°æ®é›†ï¼ˆåŒ…å«å¤šæ ·åŒ–æ—¥å¸¸åœºæ™¯ï¼‰å’Œè®¾å®šç‰¹å®šæ•°æ®é›†ï¼ˆå±€é™äºç‹­çª„ç¯å¢ƒï¼‰ï¼Œå¹¶è¯„ä¼°æ ‡å‡†æ£€æµ‹å™¨å®¶æ—åœ¨æ‰€æœ‰è®­ç»ƒ-æµ‹è¯•å¯¹ä¸Šçš„è¡¨ç°ã€‚ä¸ºäº†è§£è€¦é¢†åŸŸåç§»å’Œæ ‡ç­¾ä¸åŒ¹é…ï¼Œç ”ç©¶æ¯”è¾ƒäº†å°é—­æ ‡ç­¾è¿ç§»ä¸å¼€æ”¾æ ‡ç­¾åè®®ï¼Œåè€…ä½¿ç”¨CLIPç›¸ä¼¼æ€§å°†é¢„æµ‹ç±»åˆ«æ˜ å°„åˆ°æœ€è¿‘çš„ç›®æ ‡æ ‡ç­¾ã€‚

**Result:** å®éªŒæ­ç¤ºäº†è·¨æ•°æ®é›†ç›®æ ‡æ£€æµ‹çš„æ¸…æ™°ç»“æ„ï¼šç›¸åŒè®¾å®šç±»å‹å†…çš„è¿ç§»ç›¸å¯¹ç¨³å®šï¼Œè€Œè·¨ç±»å‹è¿ç§»æ€§èƒ½æ˜¾è‘—ä¸‹é™ä¸”é€šå¸¸ä¸å¯¹ç§°ã€‚æœ€ä¸¥é‡çš„æ€§èƒ½å´©æºƒå‘ç”Ÿåœ¨ä»ç‰¹å®šæºæ•°æ®é›†è¿ç§»åˆ°æ— å…³ç›®æ ‡æ•°æ®é›†æ—¶ï¼Œå³ä½¿åœ¨å¼€æ”¾æ ‡ç­¾å¯¹é½åä»ç„¶å­˜åœ¨ï¼Œè¡¨æ˜é¢†åŸŸåç§»åœ¨æœ€å›°éš¾åŒºåŸŸå ä¸»å¯¼åœ°ä½ã€‚å¼€æ”¾æ ‡ç­¾è¯„ä¼°äº§ç”Ÿäº†ä¸€è‡´ä½†æœ‰ç•Œçš„æ€§èƒ½æå‡ï¼Œè®¸å¤šæ ¡æ­£æ¡ˆä¾‹å¯¹åº”äº†å›¾åƒè¯æ®æ”¯æŒçš„è¯­ä¹‰è¿‘ä¼¼é”™è¯¯ã€‚

**Conclusion:** è¯¥ç ”ç©¶æä¾›äº†åŸºäºè®¾å®šç‰¹å¼‚æ€§çš„è·¨æ•°æ®é›†ç›®æ ‡æ£€æµ‹åŸåˆ™æ€§è¡¨å¾ï¼Œå¹¶ä¸ºè¯„ä¼°åˆ†å¸ƒåç§»ä¸‹çš„æ£€æµ‹å™¨æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚ç ”ç©¶å‘ç°é¢†åŸŸåç§»åœ¨æœ€å›°éš¾çš„è¿ç§»åœºæ™¯ä¸­èµ·ä¸»å¯¼ä½œç”¨ï¼Œè€Œå¼€æ”¾æ ‡ç­¾åè®®èƒ½å¤Ÿéƒ¨åˆ†ç¼“è§£æ ‡ç­¾ä¸åŒ¹é…é—®é¢˜ï¼Œä½†æ€§èƒ½æå‡æœ‰é™ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºç†è§£ç›®æ ‡æ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›å’Œåˆ¶å®šæ›´æœ‰æ•ˆçš„è·¨æ•°æ®é›†è¯„ä¼°ç­–ç•¥ã€‚

---

#### ğŸ“„ Abstract
Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This reveals a clear structure in CD-OD: transfer within the same setting type is relatively stable, while transfer across setting types drops substantially and is often asymmetric. The most severe breakdowns occur when transferring from specific sources to agnostic targets, and persist after open-label alignment, indicating that domain shift dominates in the hardest regimes. To disentangle domain shift from label mismatch, we compare closed-label transfer with an open-label protocol that maps predicted classes to the nearest target label using CLIP similarity. Open-label evaluation yields consistent but bounded gains, and many corrected cases correspond to semantic near-misses supported by the image evidence. Overall, we provide a principled characterization of CD-OD under setting specificity and practical guidance for evaluating detectors under distribution shift. Code will be released at \href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}.


### [22] [CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems](https://arxiv.org/abs/2601.09613)
*Yonglin Tian, Qiyao Zhang, Wei Xu, Yutong Wang, Yihao Wu, Xinyi Li, Xingyuan Dai, Hui Zhang, Zhiyong Cui, Baoqing Guo, Zujun Yu, Yisheng Lv*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CogRailåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é“è·¯å…¥ä¾µæ„ŸçŸ¥ä¸­çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªè”åˆå¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆä½ç½®æ„ŸçŸ¥ã€è¿åŠ¨é¢„æµ‹å’Œå¨èƒåˆ†æä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å®‰å…¨å…³é”®é¢†åŸŸçš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰é“è·¯å…¥ä¾µæ„ŸçŸ¥ç³»ç»Ÿä¸»è¦å…³æ³¨å›ºå®šè§†è§‰èŒƒå›´å†…çš„ç‰©ä½“åˆ†ç±»ï¼Œå¹¶åº”ç”¨åŸºäºè§„åˆ™çš„å¯å‘å¼æ–¹æ³•åˆ¤æ–­å…¥ä¾µçŠ¶æ€ï¼Œå¾€å¾€å¿½ç•¥äº†å…·æœ‰æ½œåœ¨å…¥ä¾µé£é™©çš„ç›®æ ‡ã€‚å‡†ç¡®é¢„æµ‹è¿™äº›é£é™©éœ€è¦ç†è§£æ„Ÿå…´è¶£å¯¹è±¡çš„ç©ºé—´ä¸Šä¸‹æ–‡å’Œæ—¶åºåŠ¨æ€ï¼Œè¿™å¯¹ä¼ ç»Ÿè§†è§‰æ¨¡å‹æ„æˆäº†æŒ‘æˆ˜ã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†CogRailåŸºå‡†æµ‹è¯•ï¼Œæ•´åˆäº†ç²¾é€‰çš„å¼€æºæ•°æ®é›†å’Œè®¤çŸ¥é©±åŠ¨çš„é—®ç­”æ ‡æ³¨ä»¥æ”¯æŒæ—¶ç©ºæ¨ç†å’Œé¢„æµ‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç³»ç»Ÿè¯„ä¼°äº†æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè”åˆå¾®è°ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†ä½ç½®æ„ŸçŸ¥ã€è¿åŠ¨é¢„æµ‹å’Œå¨èƒåˆ†æä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼Œä¿ƒè¿›é€šç”¨åŸºç¡€æ¨¡å‹å‘è®¤çŸ¥å…¥ä¾µæ„ŸçŸ¥ä¸“ç”¨æ¨¡å‹çš„é€‚åº”ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼Œå½“å‰å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†è®¤çŸ¥å…¥ä¾µæ„ŸçŸ¥ä»»åŠ¡æ‰€éœ€çš„å¤æ‚æ—¶ç©ºæ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œçªæ˜¾äº†ç°æœ‰åŸºç¡€æ¨¡å‹åœ¨è¿™ä¸€å®‰å…¨å…³é”®é¢†åŸŸçš„å±€é™æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæå‡ºçš„è”åˆå¾®è°ƒæ¡†æ¶é€šè¿‡é’ˆå¯¹æ€§åœ°é€‚åº”é¢†åŸŸç‰¹å®šçš„æ¨ç†éœ€æ±‚ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œæ˜¾ç¤ºäº†ç»“æ„åŒ–å¤šä»»åŠ¡å­¦ä¹ åœ¨æé«˜å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†è®¤çŸ¥é©±åŠ¨æ–¹æ³•åœ¨å®‰å…¨å…³é”®æ„ŸçŸ¥ç³»ç»Ÿä¸­çš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†é€šè¿‡ç»“æ„åŒ–å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶å°†é€šç”¨åŸºç¡€æ¨¡å‹é€‚åº”åˆ°ç‰¹å®šé¢†åŸŸä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœä¸ºå¼€å‘æ›´å¯é ã€å¯è§£é‡Šçš„é“è·¯å…¥ä¾µæ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†æ–°æ–¹å‘ï¼Œå¹¶æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤æ‚æ—¶ç©ºæ¨ç†ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚

---

#### ğŸ“„ Abstract
Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.


### [23] [Video Joint-Embedding Predictive Architectures for Facial Expression Recognition](https://arxiv.org/abs/2601.09524)
*Lennart Eing, Cristina Luna-JimÃ©nez, Silvan Mertes, Elisabeth AndrÃ©*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼ˆV-JEPAï¼‰çš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«æ–°æ–¹æ³•ï¼Œé€šè¿‡åµŒå…¥é¢„æµ‹è€Œéåƒç´ é‡å»ºçš„é¢„è®­ç»ƒæ–¹å¼ï¼Œåœ¨RAVDESSå’ŒCREMA-Dæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»ŸåŸºäºåƒç´ é‡å»ºçš„è§†é¢‘ç†è§£é¢„è®­ç»ƒæ–¹æ³•å¯èƒ½æ•è·ä¸ä»»åŠ¡æ— å…³çš„èƒŒæ™¯ä¿¡æ¯ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢ç´¢çº¯åµŒå…¥é¢„æµ‹çš„é¢„è®­ç»ƒæ–¹æ³•åœ¨é¢éƒ¨è¡¨æƒ…è¯†åˆ«ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä»¥æå‡æ¨¡å‹å¯¹ç›¸å…³ç‰¹å¾çš„æå–èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨è§†é¢‘è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼ˆV-JEPAï¼‰ï¼Œé€šè¿‡é¢„æµ‹æ©ç åŒºåŸŸçš„åµŒå…¥è¡¨ç¤ºè€Œéåƒç´ çº§é‡å»ºæ¥å­¦ä¹ è§†é¢‘è¡¨ç¤ºï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„V-JEPAè§†é¢‘ç¼–ç å™¨æå–ç‰¹å¾ï¼Œå¹¶åœ¨RAVDESSå’ŒCREMA-Dæ•°æ®é›†ä¸Šè®­ç»ƒæµ…å±‚åˆ†ç±»å™¨è¿›è¡Œé¢éƒ¨è¡¨æƒ…è¯†åˆ«ã€‚

**Result:** åœ¨RAVDESSæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨CREMA-Dæ•°æ®é›†ä¸Šè¶…è¶Šäº†æ‰€æœ‰å…¶ä»–åŸºäºè§†è§‰çš„æ–¹æ³•ï¼ˆåŠ æƒå‡†ç¡®ç‡æå‡+1.48%ï¼‰ï¼Œè·¨æ•°æ®é›†è¯„ä¼°æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†åµŒå…¥é¢„æµ‹é¢„è®­ç»ƒæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜çº¯åµŒå…¥é¢„æµ‹çš„é¢„è®­ç»ƒæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆé¿å…æ•è·æ— å…³èƒŒæ™¯ä¿¡æ¯ï¼Œåœ¨é¢éƒ¨è¡¨æƒ…è¯†åˆ«ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºè§†é¢‘ç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„é¢„è®­ç»ƒèŒƒå¼ï¼Œå…·æœ‰æ¨åŠ¨FERé¢†åŸŸå‘å±•çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
This paper introduces a novel application of Video Joint-Embedding Predictive Architectures (V-JEPAs) for Facial Expression Recognition (FER). Departing from conventional pre-training methods for video understanding that rely on pixel-level reconstructions, V-JEPAs learn by predicting embeddings of masked regions from the embeddings of unmasked regions. This enables the trained encoder to not capture irrelevant information about a given video like the color of a region of pixels in the background. Using a pre-trained V-JEPA video encoder, we train shallow classifiers using the RAVDESS and CREMA-D datasets, achieving state-of-the-art performance on RAVDESS and outperforming all other vision-based methods on CREMA-D (+1.48 WAR). Furthermore, cross-dataset evaluations reveal strong generalization capabilities, demonstrating the potential of purely embedding-based pre-training approaches to advance FER. We release our code at https://github.com/lennarteingunia/vjepa-for-fer.


### [24] [Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning](https://arxiv.org/abs/2601.09708)
*Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºFast-ThinkActæ¡†æ¶ï¼Œé€šè¿‡å¯è¡¨è¾¾çš„æ½œåœ¨æ¨ç†å®ç°ç´§å‡‘è€Œé«˜æ•ˆçš„è§„åˆ’ï¼Œæ˜¾è‘—é™ä½æ¨ç†å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„é•¿æ—¶ç¨‹è§„åˆ’èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†è§‰-è¯­è¨€-åŠ¨ä½œä»»åŠ¡éœ€è¦åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯¹å¤æ‚è§†è§‰åœºæ™¯è¿›è¡Œæ¨ç†å¹¶æ‰§è¡Œé€‚åº”æ€§åŠ¨ä½œï¼Œç°æœ‰æ˜¾å¼æ€ç»´é“¾æ–¹æ³•è™½ç„¶èƒ½æå‡æ³›åŒ–èƒ½åŠ›ï¼Œä½†å­˜åœ¨æ¨ç†è½¨è¿¹è¿‡é•¿å¯¼è‡´çš„é«˜å»¶è¿Ÿé—®é¢˜ï¼Œéœ€è¦æ›´é«˜æ•ˆçš„æ¨ç†æ¡†æ¶ã€‚

**Method:** Fast-ThinkActé‡‡ç”¨å¯è¡¨è¾¾çš„æ½œåœ¨æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡ä»æ•™å¸ˆæ¨¡å‹è’¸é¦å­¦ä¹ æ½œåœ¨æ€ç»´é“¾ï¼Œåˆ©ç”¨åå¥½å¼•å¯¼ç›®æ ‡å¯¹é½æ“ä½œè½¨è¿¹ï¼ŒåŒæ—¶è¿ç§»è¯­è¨€å’Œè§†è§‰è§„åˆ’èƒ½åŠ›ï¼Œå®ç°æ¨ç†å¢å¼ºçš„ç­–ç•¥å­¦ä¹ ï¼Œå°†ç´§å‡‘æ¨ç†ä¸åŠ¨ä½œæ‰§è¡Œæœ‰æ•ˆè¿æ¥ã€‚

**Result:** åœ¨å¤šæ ·åŒ–çš„å…·èº«æ“ä½œå’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFast-ThinkActç›¸æ¯”æœ€å…ˆè¿›çš„æ¨ç†VLAæ–¹æ³•å®ç°äº†é«˜è¾¾89.3%çš„æ¨ç†å»¶è¿Ÿé™ä½ï¼ŒåŒæ—¶ä¿æŒäº†æœ‰æ•ˆçš„é•¿æ—¶ç¨‹è§„åˆ’ã€å°‘æ ·æœ¬é€‚åº”å’Œå¤±è´¥æ¢å¤èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡æ½œåœ¨æ¨ç†è’¸é¦å’Œåå¥½å¯¹é½ï¼Œå¯ä»¥åœ¨å¤§å¹…é™ä½æ¨ç†å»¶è¿Ÿçš„åŒæ—¶ä¿æŒå¼ºå¤§çš„è§„åˆ’æ€§èƒ½ï¼Œä¸ºé«˜æ•ˆå…·èº«æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ï¼Œå¹³è¡¡äº†æ¨ç†è´¨é‡ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚

---

#### ğŸ“„ Abstract
Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.


### [25] [OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2601.09575)
*Sheng-Yu Huang, Jaesung Choe, Yu-Chiang Frank Wang, Cheng Sun*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†OpenVoxelï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œç”¨äºç¨€ç–ä½“ç´ çš„åˆ†ç»„å’Œæè¿°ï¼Œä»¥å®ç°å¼€æ”¾è¯æ±‡çš„3Dåœºæ™¯ç†è§£ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›´æ¥åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ„å»ºä¿¡æ¯ä¸°å¯Œçš„åœºæ™¯åœ°å›¾ï¼Œåœ¨å¤æ‚å‚è€ƒè¡¨è¾¾åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰3Dåœºæ™¯ç†è§£æ–¹æ³•é€šå¸¸éœ€è¦è®­ç»ƒè¿‡ç¨‹æˆ–ä¾èµ–CLIP/BERTæ–‡æœ¬ç¼–ç å™¨çš„åµŒå…¥è¡¨ç¤ºï¼Œè¿™é™åˆ¶äº†æ–¹æ³•çš„çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§æ— éœ€è®­ç»ƒçš„ç®—æ³•ï¼Œèƒ½å¤Ÿç›´æ¥å¯¹ç¨€ç–ä½“ç´ è¿›è¡Œåˆ†ç»„å’Œæè¿°ï¼Œå®ç°æ›´çµæ´»ã€é«˜æ•ˆçš„å¼€æ”¾è¯æ±‡3Dåœºæ™¯ç†è§£ã€‚

**Method:** OpenVoxelé‡‡ç”¨æ— éœ€è®­ç»ƒçš„ç®—æ³•ï¼ŒåŸºäºå¤šè§†å›¾å›¾åƒè·å¾—çš„ç¨€ç–ä½“ç´ æ …æ ¼åŒ–æ¨¡å‹ï¼Œå¯¹ç¨€ç–ä½“ç´ è¿›è¡Œæœ‰æ„ä¹‰çš„åˆ†ç»„ä»¥æè¿°åœºæ™¯ä¸­çš„ä¸åŒç‰©ä½“ã€‚è¯¥æ–¹æ³•ç›´æ¥åˆ©ç”¨å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ–‡æœ¬åˆ°æ–‡æœ¬çš„æœç´¢æ–¹å¼ä¸ºæ¯ä¸ªåˆ†ç»„ç”Ÿæˆæè¿°æ€§æ ‡é¢˜ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¼•å…¥CLIP/BERTæ–‡æœ¬ç¼–ç å™¨åµŒå…¥çš„æ­¥éª¤ã€‚

**Result:** é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼ŒOpenVoxelåœ¨å¤æ‚å‚è€ƒè¡¨è¾¾åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜äºè¿‘æœŸç ”ç©¶æ–¹æ³•çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸæ„å»ºä¿¡æ¯ä¸°å¯Œçš„åœºæ™¯åœ°å›¾ï¼Œæ”¯æŒå¼€æ”¾è¯æ±‡åˆ†å‰²å’Œå‚è€ƒè¡¨è¾¾åˆ†å‰²ç­‰è¿›ä¸€æ­¥çš„3Dåœºæ™¯ç†è§£ä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒäº†æ— éœ€è®­ç»ƒçš„ä¼˜åŠ¿ã€‚

**Conclusion:** OpenVoxelå±•ç¤ºäº†æ— éœ€è®­ç»ƒæ–¹æ³•åœ¨3Dåœºæ™¯ç†è§£ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ç›´æ¥åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ°æ–‡æœ¬æœç´¢çš„åˆ›æ–°ç­–ç•¥ã€‚è¯¥æ–¹æ³•ä¸ºå¼€æ”¾è¯æ±‡3Dåœºæ™¯ç†è§£æä¾›äº†æ›´çµæ´»ã€é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æœ›æ¨åŠ¨è¯¥é¢†åŸŸå‘æ›´å°‘ä¾èµ–é¢„è®­ç»ƒåµŒå…¥çš„æ–¹å‘å‘å±•ã€‚

---

#### ğŸ“„ Abstract
We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.


### [26] [GRCF: Two-Stage Groupwise Ranking and Calibration Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2601.09606)
*Manning Gao, Leheng Zhang, Shiqin Han, Haifeng Hu, Yuncheng Jiang, Sijie Mai*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µåˆ†ç»„æ’åºä¸æ ¡å‡†æ¡†æ¶ï¼ˆGRCFï¼‰ï¼Œé€šè¿‡å¼•å…¥ä¼˜åŠ¿åŠ æƒåŠ¨æ€è¾¹ç•Œæ’åºæŸå¤±å’ŒMAEé©±åŠ¨ç›®æ ‡ï¼Œè§£å†³äº†å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­ä¼ ç»Ÿæˆå¯¹æ’åºæ–¹æ³•å¯¹å›°éš¾æ ·æœ¬å…³æ³¨ä¸è¶³å’Œè¾¹ç•Œè®¾ç½®é™æ€çš„é—®é¢˜ï¼Œåœ¨å›å½’å’Œåˆ†ç±»ä»»åŠ¡ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æç ”ç©¶å¤§å¤šå…³æ³¨ç‚¹å¼å›å½’æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯¹æ ‡ç­¾å™ªå£°æ•æ„Ÿä¸”å¿½ç•¥äº†æ ·æœ¬é—´çš„ç›¸å¯¹é¡ºåºï¼Œå¯¼è‡´é¢„æµ‹ä¸ç¨³å®šå’Œç›¸å…³æ€§å¯¹é½å·®ã€‚è™½ç„¶æˆå¯¹æ’åºå­¦ä¹ æ¡†æ¶é€šè¿‡æ¯”è¾ƒå­¦ä¹ ç›¸å¯¹é¡ºåºæ¥å¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œä½†å®ƒä»¬å¼•å…¥äº†ä¸¤ä¸ªæ–°é—®é¢˜ï¼šä¸€æ˜¯å¯¹æ‰€æœ‰æ¯”è¾ƒèµ‹äºˆç»Ÿä¸€é‡è¦æ€§ï¼Œæœªèƒ½è‡ªé€‚åº”åœ°å…³æ³¨éš¾ä»¥æ’åºçš„æ ·æœ¬ï¼›äºŒæ˜¯é‡‡ç”¨é™æ€æ’åºè¾¹ç•Œï¼Œæ— æ³•åæ˜ æƒ…æ„Ÿç»„é—´å˜åŒ–çš„è¯­ä¹‰è·ç¦»ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸¤é˜¶æ®µåˆ†ç»„æ’åºä¸æ ¡å‡†æ¡†æ¶ï¼ˆGRCFï¼‰ï¼Œè¯¥æ¡†æ¶å€Ÿé‰´äº†åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„æ€æƒ³ã€‚ç¬¬ä¸€é˜¶æ®µå¼•å…¥äº†GRPOå¯å‘çš„ä¼˜åŠ¿åŠ æƒåŠ¨æ€è¾¹ç•Œæ’åºæŸå¤±ï¼Œä»¥æ„å»ºç»†ç²’åº¦çš„åºæ•°ç»“æ„ï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨MAEé©±åŠ¨çš„ç›®æ ‡æ¥å¯¹é½é¢„æµ‹å¹…åº¦ã€‚ä¸ºäº†éªŒè¯å…¶æ³›åŒ–èƒ½åŠ›ï¼Œä½œè€…å°†GRCFæ‰©å±•åˆ°åˆ†ç±»ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€å¹½é»˜æ£€æµ‹å’Œè®½åˆºæ£€æµ‹ã€‚

**Result:** GRCFåœ¨æ ¸å¿ƒå›å½’åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒç›¸å¯¹åºæ•°ç»“æ„çš„åŒæ—¶ï¼Œç¡®ä¿äº†ç»å¯¹åˆ†æ•°æ ¡å‡†ï¼Œå¹¶èƒ½è‡ªé€‚åº”åœ°å…³æ³¨å›°éš¾æ ·æœ¬ï¼Œä»è€Œåœ¨å¤šä¸ªå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜è¶Šçš„å®éªŒç»“æœã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†è‡ªé€‚åº”å…³æ³¨å›°éš¾æ ·æœ¬å’ŒåŠ¨æ€è¾¹ç•Œè®¾ç½®å¯¹äºå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­åºæ•°å­¦ä¹ çš„é‡è¦æ€§ã€‚GRCFæ¡†æ¶ä¸ä»…è§£å†³äº†ä¼ ç»Ÿæˆå¯¹æ’åºæ–¹æ³•çš„å±€é™æ€§ï¼Œè¿˜å±•ç¤ºäº†ä»å›å½’ä»»åŠ¡åˆ°åˆ†ç±»ä»»åŠ¡çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€åºæ•°å­¦ä¹ æä¾›äº†æ–°çš„æ–¹æ³•è®ºè§†è§’å’Œå®ç”¨æ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
Most Multimodal Sentiment Analysis research has focused on point-wise regression. While straightforward, this approach is sensitive to label noise and neglects whether one sample is more positive than another, resulting in unstable predictions and poor correlation alignment. Pairwise ordinal learning frameworks emerged to address this gap, capturing relative order by learning from comparisons. Yet, they introduce two new trade-offs: First, they assign uniform importance to all comparisons, failing to adaptively focus on hard-to-rank samples. Second, they employ static ranking margins, which fail to reflect the varying semantic distances between sentiment groups. To address this, we propose a Two-Stage Group-wise Ranking and Calibration Framework (GRCF) that adapts the philosophy of Group Relative Policy Optimization (GRPO). Our framework resolves these trade-offs by simultaneously preserving relative ordinal structure, ensuring absolute score calibration, and adaptively focusing on difficult samples. Specifically, Stage 1 introduces a GRPO-inspired Advantage-Weighted Dynamic Margin Ranking Loss to build a fine-grained ordinal structure. Stage 2 then employs an MAE-driven objective to align prediction magnitudes. To validate its generalizability, we extend GRCF to classification tasks, including multimodal humor detection and sarcasm detection. GRCF achieves state-of-the-art performance on core regression benchmarks, while also showing strong generalizability in classification tasks.


### [27] [Identifying Models Behind Text-to-Image Leaderboards](https://arxiv.org/abs/2601.09647)
*Ali Naseh, Yuefeng Peng, Anshuman Suri, Harsh Chaudhari, Alina Oprea, Amir Houmansadr*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æ­ç¤ºäº†åŸºäºæŠ•ç¥¨çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ’è¡Œæ¦œå­˜åœ¨ä¸¥é‡çš„å®‰å…¨æ¼æ´ï¼Œé€šè¿‡åˆ†æå‘ç°ä¸åŒæ¨¡å‹ç”Ÿæˆçš„å›¾åƒåœ¨åµŒå…¥ç©ºé—´ä¸­å½¢æˆç‹¬ç‰¹çš„èšç±»æ¨¡å¼ï¼Œä½¿å¾—åŒ¿ååŒ–å¯ä»¥è¢«è½»æ˜“ç ´è§£ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹è´¨é‡è¯„ä¼°ä¸»è¦ä¾èµ–åŸºäºæŠ•ç¥¨çš„æ’è¡Œæ¦œï¼Œè¿™äº›æ’è¡Œæ¦œå‡è®¾æ¨¡å‹è¾“å‡ºç»è¿‡åŒ¿ååŒ–å¤„ç†ä»¥ä¿è¯å…¬å¹³æ€§ã€‚ç„¶è€Œï¼Œè¿™ç§åŒ¿ååŒ–æœºåˆ¶çš„å®‰å…¨æ€§å°šæœªå¾—åˆ°å……åˆ†éªŒè¯ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ­ç¤ºæ­¤ç±»æ’è¡Œæ¦œä¸­å­˜åœ¨çš„æ½œåœ¨å®‰å…¨æ¼æ´ã€‚

**Method:** ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè´¨å¿ƒçš„å»åŒ¿ååŒ–æ–¹æ³•ï¼Œé€šè¿‡åˆ†æ22ä¸ªä¸åŒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨280ä¸ªæç¤ºè¯ä¸‹ç”Ÿæˆçš„15ä¸‡å¼ å›¾åƒï¼Œå‘ç°æ¯ä¸ªæ¨¡å‹çš„ç”Ÿæˆç»“æœåœ¨å›¾åƒåµŒå…¥ç©ºé—´ä¸­å½¢æˆç‹¬ç‰¹çš„èšç±»æ¨¡å¼ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦æ§åˆ¶æç¤ºè¯æˆ–è®¿é—®è®­ç»ƒæ•°æ®ï¼Œä»…åˆ©ç”¨å›¾åƒåµŒå…¥ç‰¹å¾å³å¯å®ç°å‡†ç¡®æ¨¡å‹è¯†åˆ«ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºè´¨å¿ƒçš„å»åŒ¿ååŒ–æ–¹æ³•èƒ½å¤Ÿä»¥é«˜å‡†ç¡®ç‡è¯†åˆ«ä¸åŒæ¨¡å‹çš„ç”Ÿæˆå›¾åƒï¼Œæ­ç¤ºäº†ç³»ç»Ÿæ€§çš„æ¨¡å‹ç‰¹å®šç‰¹å¾ç­¾åã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†æç¤ºè¯çº§åˆ«çš„å¯åŒºåˆ†æ€§åº¦é‡ï¼Œå¹¶è¿›è¡Œäº†å¤§è§„æ¨¡åˆ†æï¼Œå‘ç°æŸäº›ç‰¹å®šæç¤ºè¯èƒ½å¤Ÿå¯¼è‡´æ¥è¿‘å®Œç¾çš„æ¨¡å‹å¯åŒºåˆ†æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶æš´éœ²äº†æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ’è¡Œæ¦œä¸­å­˜åœ¨çš„æ ¹æœ¬æ€§å®‰å…¨ç¼ºé™·ï¼Œè¡¨æ˜å½“å‰çš„åŒ¿ååŒ–æœºåˆ¶ä¸è¶³ä»¥ä¿æŠ¤æ¨¡å‹èº«ä»½ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†éœ€è¦å¼€å‘æ›´å¼ºçš„åŒ¿ååŒ–é˜²å¾¡æªæ–½ï¼Œä»¥ç¡®ä¿æ¨¡å‹è¯„ä¼°çš„å…¬å¹³æ€§å’Œå®‰å…¨æ€§ï¼Œå¯¹AIç”Ÿæˆå†…å®¹è®¤è¯å’Œæ¨¡å‹çŸ¥è¯†äº§æƒä¿æŠ¤å…·æœ‰é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space, enabling accurate deanonymization without prompt control or training data. Using 22 models and 280 prompts (150K images), our centroid-based method achieves high accuracy and reveals systematic model-specific signatures. We further introduce a prompt-level distinguishability metric and conduct large-scale analyses showing how certain prompts can lead to near-perfect distinguishability. Our findings expose fundamental security flaws in T2I leaderboards and motivate stronger anonymization defenses.


### [28] [Image2Garment: Simulation-ready Garment Generation from a Single Image](https://arxiv.org/abs/2601.09658)
*Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu, Yang Zheng, Hugo Bertiche, Menglei Chai, Thabo Beeler, Gordon Wetzstein*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å‰é¦ˆæ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒç›´æ¥ä¼°è®¡ç‰©ç†å‡†ç¡®çš„ã€å¯ç”¨äºä»¿çœŸçš„æœè£…ï¼Œé€šè¿‡ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œææ–™å±æ€§æ¨æ–­å’Œè½»é‡çº§ç‰©ç†å‚æ•°é¢„æµ‹å™¨ï¼Œæ— éœ€å¤šè§†å›¾æ•è·æˆ–è¿­ä»£ä¼˜åŒ–å³å¯ç”Ÿæˆä»¿çœŸå°±ç»ªçš„æœè£…ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä»å•å¼ å›¾åƒä¼°è®¡ç‰©ç†å‡†ç¡®çš„ä»¿çœŸå°±ç»ªæœè£…é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šç¼ºä¹å›¾åƒåˆ°ç‰©ç†çš„æ•°æ®é›†ä»¥åŠé—®é¢˜çš„ç—…æ€æ€§ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆéœ€è¦å¤šè§†å›¾æ•è·å’Œæ˜‚è´µçš„å¯å¾®åˆ†ä»¿çœŸï¼Œè¦ä¹ˆåªèƒ½é¢„æµ‹æœè£…å‡ ä½•å½¢çŠ¶è€Œç¼ºä¹ä»¿çœŸæ‰€éœ€çš„ææ–™ç‰©ç†å±æ€§ï¼Œè¿™é™åˆ¶äº†å®é™…åº”ç”¨çš„å¯è¡Œæ€§ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨å‰é¦ˆæ¡†æ¶ï¼Œé¦–å…ˆå¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ä»çœŸå®å›¾åƒæ¨æ–­ææ–™æˆåˆ†å’Œç»‡ç‰©å±æ€§ï¼Œç„¶åè®­ç»ƒè½»é‡çº§é¢„æµ‹å™¨å°†è¿™äº›å±æ€§æ˜ å°„åˆ°ç›¸åº”çš„ç‰©ç†ç»‡ç‰©å‚æ•°ã€‚æ¡†æ¶å¼•å…¥äº†ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼ˆFTAGå’ŒT2Pï¼‰ï¼Œå¹¶é¿å…äº†è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œç›´æ¥ç”Ÿæˆä»¿çœŸå°±ç»ªçš„æœè£…è¡¨ç¤ºã€‚

**Result:** å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ææ–™æˆåˆ†ä¼°è®¡å’Œç»‡ç‰©å±æ€§é¢„æµ‹æ–¹é¢å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å°†è¿™äº›é¢„æµ‹ç»“æœè¾“å…¥ç‰©ç†å‚æ•°ä¼°è®¡å™¨ï¼Œä¸æœ€å…ˆè¿›çš„å›¾åƒåˆ°æœè£…æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´é«˜ä¿çœŸåº¦çš„ä»¿çœŸç»“æœï¼ŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’Œç‰©ç†å‚æ•°æ˜ å°„ï¼Œå¯ä»¥ä»å•å¼ å›¾åƒæœ‰æ•ˆä¼°è®¡ä»¿çœŸå°±ç»ªçš„æœè£…ï¼Œæ— éœ€å¤šè§†å›¾æ•è·æˆ–æ˜‚è´µçš„ä¼˜åŒ–è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•ä¸ºè®¡ç®—æœºå›¾å½¢å­¦å’Œè™šæ‹Ÿè¯•ç©¿åº”ç”¨æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†è·¨æ¨¡æ€å­¦ä¹ åœ¨ç‰©ç†å±æ€§ä¼°è®¡ä¸­çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.


### [29] [LiteEmbed: Adapting CLIP to Rare Classes](https://arxiv.org/abs/2601.09661)
*Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi*

#### ğŸ§© TL;DR
LiteEmbed æå‡ºäº†ä¸€ç§è½»é‡çº§æ¡†æ¶ï¼Œç”¨äº CLIP çš„å°‘æ ·æœ¬ä¸ªæ€§åŒ–ï¼Œé€šè¿‡å­ç©ºé—´å¼•å¯¼çš„æ–‡æœ¬åµŒå…¥ä¼˜åŒ–ï¼Œä½¿æ–°ç±»åˆ«èƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒç¼–ç å™¨çš„æƒ…å†µä¸‹æ·»åŠ åˆ°æ¨¡å‹ä¸­ï¼Œæ˜¾è‘—æå‡äº†ç½•è§ç±»åˆ«å’Œæœªè§ç±»åˆ«çš„è¯†åˆ«æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹å¦‚ CLIP åœ¨é›¶æ ·æœ¬è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†é¢„è®­ç»ƒæœŸé—´ç½•è§å‡ºç°çš„ç±»åˆ«æ—¶å­˜åœ¨å›°éš¾ï¼ŒåŒ…æ‹¬æ–°å‡ºç°çš„å®ä½“å’Œæ–‡åŒ–ç‰¹å®šç±»åˆ«ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„é€‚åº”æ€§å’Œè¦†ç›–èŒƒå›´ã€‚

**Method:** LiteEmbed é‡‡ç”¨å­ç©ºé—´å¼•å¯¼çš„æ–‡æœ¬åµŒå…¥ä¼˜åŒ–æ–¹æ³•ï¼ŒåŸºäº PCA åˆ†è§£å°†è¯­ä¹‰ç©ºé—´è§£è€¦ä¸ºç²—ç²’åº¦è¯­ä¹‰æ–¹å‘å’Œç»†ç²’åº¦å˜åŒ–æ–¹å‘ï¼Œé€šè¿‡ç²—ç²’åº¦å¯¹é½å’Œç»†ç²’åº¦åˆ†ç¦»ä¸¤ä¸ªäº’è¡¥ç›®æ ‡ï¼Œåœ¨ä¿æŒå…¨å±€è¯­ä¹‰ä¸€è‡´æ€§çš„åŒæ—¶å¢å¼ºè§†è§‰ç›¸ä¼¼ç±»åˆ«ä¹‹é—´çš„åŒºåˆ†æ€§ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLiteEmbed åœ¨åˆ†ç±»ã€æ£€ç´¢ã€åˆ†å‰²å’Œæ£€æµ‹ç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œä¸º CLIP åœ¨ä»£è¡¨æ€§ä¸è¶³ã€ç½•è§æˆ–æœªè§ç±»åˆ«çš„é€‚åº”æ–¹é¢å»ºç«‹äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå®ç°äº†å³æ’å³ç”¨çš„åµŒå…¥æ›¿æ¢ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹çš„å°‘æ ·æœ¬ä¸ªæ€§åŒ–æä¾›äº†è½»é‡çº§ä¸”é«˜æ•ˆçš„æ¡†æ¶ï¼Œé€šè¿‡è§£è€¦è¯­ä¹‰ç©ºé—´çš„ä¼˜åŒ–ç­–ç•¥ï¼Œåœ¨ä¿æŒæ¨¡å‹åŸæœ‰èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æ‰©å±•äº†å…¶å¯¹ç½•è§å’Œæ–°ç±»åˆ«çš„è¯†åˆ«èƒ½åŠ›ï¼Œå…·æœ‰å¹¿æ³›çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.


### [30] [Self-Supervised Animal Identification for Long Videos](https://arxiv.org/abs/2601.09663)
*Xuyang Fang, Sion Hannuna, Edwin Simpson, Neill Campbell*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è‡ªç›‘ç£åŠ¨ç‰©ä¸ªä½“è¯†åˆ«æ–¹æ³•ï¼Œå°†è¯†åˆ«ä»»åŠ¡é‡æ„ä¸ºå…¨å±€èšç±»é—®é¢˜è€Œéåºåˆ—è·Ÿè¸ªï¼Œä»…éœ€è¾¹ç•Œæ¡†æ£€æµ‹å’Œä¸ªä½“æ€»æ•°ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šå®ç°äº†è¶…è¿‡97%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—èµ„æºå’Œæ ‡æ³¨éœ€æ±‚ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»ŸåŠ¨ç‰©ä¸ªä½“è¯†åˆ«æ–¹æ³•éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨ï¼Œè€Œç°æœ‰è‡ªç›‘ç£æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ä¸”ä¸é€‚ç”¨äºé•¿è§†é¢‘åºåˆ—ï¼Œå­˜åœ¨å†…å­˜é™åˆ¶å’Œæ—¶é—´è¯¯å·®ä¼ æ’­é—®é¢˜ï¼Œé™åˆ¶äº†åœ¨èµ„æºå—é™ç ”ç©¶ç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚

**Method:** è¯¥æ–¹æ³•å°†åŠ¨ç‰©è¯†åˆ«é‡æ„ä¸ºå…¨å±€èšç±»ä»»åŠ¡ï¼Œå‡è®¾è§†é¢‘ä¸­ä¸ªä½“æ•°é‡å·²çŸ¥ä¸”å›ºå®šï¼Œä»…éœ€è¾¹ç•Œæ¡†æ£€æµ‹å’Œæ€»æ•°ä¿¡æ¯ï¼›é€šè¿‡é‡‡æ ·å¸§å¯¹ã€ä½¿ç”¨å†»ç»“é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œã€ç»“åˆåŒˆç‰™åˆ©ç®—æ³•çš„è‡ªå¼•å¯¼æœºåˆ¶è¿›è¡Œæ‰¹å†…ä¼ªæ ‡ç­¾åˆ†é…ï¼Œå¹¶é‡‡ç”¨æ¥è‡ªè§†è§‰è¯­è¨€æ¨¡å‹çš„äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°å­¦ä¹ åˆ¤åˆ«æ€§ç‰¹å¾ã€‚

**Result:** åœ¨3D-POPé¸½å­å’Œ8å¤´å°ç‰›å–‚é£Ÿè§†é¢‘ç­‰çœŸå®æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†è¶…è¿‡97%çš„å‡†ç¡®ç‡ï¼Œæ¯æ‰¹æ¬¡GPUå†…å­˜æ¶ˆè€—å°äº1GBï¼Œæ¯”æ ‡å‡†å¯¹æ¯”æ–¹æ³•ä½ä¸€ä¸ªæ•°é‡çº§ï¼Œæ€§èƒ½åŒ¹é…æˆ–è¶…è¶Šäº†ä½¿ç”¨è¶…è¿‡1000ä¸ªæ ‡æ³¨å¸§è®­ç»ƒçš„ç›‘ç£åŸºçº¿æ–¹æ³•ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ¶ˆé™¤äº†åŠ¨ç‰©è¯†åˆ«ä¸­çš„æ‰‹åŠ¨æ ‡æ³¨ç“¶é¢ˆï¼Œä½¿æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šçš„é«˜ç²¾åº¦åŠ¨ç‰©è¯†åˆ«æˆä¸ºå¯èƒ½ï¼Œåœ¨èµ„æºå—é™çš„ç ”ç©¶ç¯å¢ƒä¸­å…·æœ‰å¹¿æ³›é€‚ç”¨æ€§ï¼Œä¸ºè¡Œä¸ºç”Ÿæ€å­¦ã€é‡ç”ŸåŠ¨ç‰©ç›‘æµ‹å’Œç•œç‰§ç®¡ç†æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Identifying individual animals in long-duration videos is essential for behavioral ecology, wildlife monitoring, and livestock management. Traditional methods require extensive manual annotation, while existing self-supervised approaches are computationally demanding and ill-suited for long sequences due to memory constraints and temporal error propagation. We introduce a highly efficient, self-supervised method that reframes animal identification as a global clustering task rather than a sequential tracking problem. Our approach assumes a known, fixed number of individuals within a single video -- a common scenario in practice -- and requires only bounding box detections and the total count. By sampling pairs of frames, using a frozen pre-trained backbone, and employing a self-bootstrapping mechanism with the Hungarian algorithm for in-batch pseudo-label assignment, our method learns discriminative features without identity labels. We adapt a Binary Cross Entropy loss from vision-language models, enabling state-of-the-art accuracy ($>$97\%) while consuming less than 1 GB of GPU memory per batch -- an order of magnitude less than standard contrastive methods. Evaluated on challenging real-world datasets (3D-POP pigeons and 8-calves feeding videos), our framework matches or surpasses supervised baselines trained on over 1,000 labeled frames, effectively removing the manual annotation bottleneck. This work enables practical, high-accuracy animal identification on consumer-grade hardware, with broad applicability in resource-constrained research settings. All code written for this paper are \href{https://huggingface.co/datasets/tonyFang04/8-calves}{here}.


### [31] [STEP3-VL-10B Technical Report](https://arxiv.org/abs/2601.09668)
*Ailin Huang, Chengyuan Yao, Chunrui Han, Fanqi Wan, Hangyu Guo, Haoran Lv, Hongyu Zhou, Jia Wang, Jian Zhou, Jianjian Sun, Jingcheng Hu, Kangheng Lin, Liang Zhao, Mitt Huang, Song Yuan, Wenwen Qu, Xiangfeng Wang, Yanlin Lai, Yingxiu Zhao, Yinmin Zhang, Yukang Shi, Yuyang Chen, Zejia Weng, Ziyang Meng, Ang Li, Aobo Kong, Bo Dong, Changyi Wan, David Wang, Di Qi, Dingming Li, En Yu, Guopeng Li, Haiquan Yin, Han Zhou, Hanshan Zhang, Haolong Yan, Hebin Zhou, Hongbo Peng, Jiaran Zhang, Jiashu Lv, Jiayi Fu, Jie Cheng, Jie Zhou, Jisheng Yin, Jingjing Xie, Jingwei Wu, Jun Zhang, Junfeng Liu, Kaijun Tan, Kaiwen Yan, Liangyu Chen, Lina Chen, Mingliang Li, Qian Zhao, Quan Sun, Shaoliang Pang, Shengjie Fan, Shijie Shang, Siyuan Zhang, Tianhao You, Wei Ji, Wuxun Xie, Xiaobo Yang, Xiaojie Hou, Xiaoran Jiao, Xiaoxiao Ren, Xiangwen Kong, Xin Huang, Xin Wu, Xing Chen, Xinran Wang, Xuelin Zhang, Yana Wei, Yang Li, Yanming Xu, Yeqing Shen, Yuang Peng, Yue Peng, Yu Zhou, Yusheng Li, Yuxiang Yang, Yuyang Zhang, Zhe Xie, Zhewei Huang, Zhenyi Lu, Zhimin Fan, Zihui Cheng, Daxin Jiang, Qi Han, Xiangyu Zhang, Yibo Zhu, Zheng Ge*

#### ğŸ§© TL;DR
STEP3-VL-10Bæ˜¯ä¸€ä¸ªè½»é‡çº§å¼€æºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡åˆ›æ–°çš„è®­ç»ƒç­–ç•¥å’Œå¹¶è¡Œåè°ƒæ¨ç†æœºåˆ¶ï¼Œåœ¨ä»…10Bå‚æ•°è§„æ¨¡ä¸‹å®ç°äº†ä¸10-20å€å¤§å‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œé‡æ–°å®šä¹‰äº†ç´§å‡‘æ•ˆç‡ä¸å‰æ²¿å¤šæ¨¡æ€æ™ºèƒ½ä¹‹é—´çš„æƒè¡¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å½“å‰å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸­ç´§å‡‘æ•ˆç‡ä¸å‰æ²¿æ€§èƒ½ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œç°æœ‰æ¨¡å‹é€šå¸¸éœ€è¦æå¤§å‚æ•°é‡æ‰èƒ½è¾¾åˆ°é¡¶çº§æ€§èƒ½ï¼Œè€Œè½»é‡çº§æ¨¡å‹åœ¨å¤æ‚è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ï¼Œéœ€è¦é‡æ–°å®šä¹‰è¿™ä¸€æƒè¡¡å…³ç³»ã€‚

**Method:** æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªæˆ˜ç•¥è½¬å˜ï¼šé¦–å…ˆé‡‡ç”¨ç»Ÿä¸€å®Œå…¨è§£å†»çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œåœ¨1.2Tå¤šæ¨¡æ€tokenä¸Šæ•´åˆè¯­è¨€å¯¹é½çš„æ„ŸçŸ¥ç¼–ç å™¨å’ŒQwen3-8Bè§£ç å™¨ä»¥å»ºç«‹å†…åœ¨è§†è§‰è¯­è¨€ååŒï¼›å…¶æ¬¡å®æ–½åŒ…å«1000å¤šæ¬¡è¿­ä»£çš„å¼ºåŒ–å­¦ä¹ è§„æ¨¡åŒ–åè®­ç»ƒæµç¨‹ï¼Œå¹¶å¼•å…¥å¹¶è¡Œåè°ƒæ¨ç†æœºåˆ¶æ¥æ‰©å±•æµ‹è¯•æ—¶è®¡ç®—èµ„æºåˆ†é…ã€‚

**Result:** STEP3-VL-10Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—å“è¶Šæ€§èƒ½ï¼šMMBenchè¾¾åˆ°92.2%ï¼ŒMMMUè¾¾åˆ°80.11%ï¼Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸­AIME2025è¾¾åˆ°94.43%ï¼ŒMathVisionè¾¾åˆ°75.95%ï¼Œå…¶10Bå‚æ•°è§„æ¨¡ä¸‹æ€§èƒ½å¯åŒ¹æ•Œæˆ–è¶…è¶Š10-20å€å¤§å‹æ¨¡å‹åŠé¡¶çº§ä¸“æœ‰æ——èˆ°æ¨¡å‹ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡åˆ›æ–°çš„è®­ç»ƒç­–ç•¥å’Œæ¨ç†æœºåˆ¶ï¼Œç´§å‡‘æ¨¡å‹èƒ½å¤Ÿå®ç°å‰æ²¿å¤šæ¨¡æ€æ™ºèƒ½ï¼Œä¸ºç¤¾åŒºæä¾›äº†å¼ºå¤§ã€é«˜æ•ˆä¸”å¯å¤ç°çš„åŸºçº¿ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿä¸Šè®¤ä¸ºéœ€è¦æå¤§å‚æ•°é‡æ‰èƒ½è·å¾—é¡¶çº§æ€§èƒ½çš„å‡è®¾ï¼Œæ¨åŠ¨äº†é«˜æ•ˆå¤šæ¨¡æ€AIçš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\times$-20$\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.


### [32] [Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering](https://arxiv.org/abs/2601.09697)
*Jieying Chen, Jeffrey Hu, Joan Lasenby, Ayush Tewari*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSRENDERæ–¹æ³•ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆç¨€ç–å…³é”®å¸§ï¼Œç„¶ååˆ©ç”¨3Dé‡å»ºå’Œæ¸²æŸ“åˆæˆå®Œæ•´è§†é¢‘ï¼Œå®ç°äº†æ¯”ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹å¿«40å€ä»¥ä¸Šçš„é«˜æ•ˆè§†é¢‘ç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒé«˜è§†è§‰ä¿çœŸåº¦å’Œæ—¶é—´ç¨³å®šæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†é¢‘ç”Ÿæˆæ–¹æ³•è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œç”Ÿæˆå‡ ç§’é’Ÿè§†é¢‘éœ€è¦æ•°åˆ†é’ŸGPUæ—¶é—´ï¼Œè¿™ä¸¥é‡é˜»ç¢äº†åœ¨éœ€è¦å®æ—¶äº¤äº’çš„åº”ç”¨ï¼ˆå¦‚å…·èº«AIå’ŒVR/ARï¼‰ä¸­çš„éƒ¨ç½²ã€‚ç°æœ‰æ–¹æ³•æ— æ³•åœ¨ä¿æŒé«˜è´¨é‡çš„åŒæ—¶å®ç°é«˜æ•ˆç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åœ¨é™æ€åœºæ™¯çš„ç›¸æœºæ¡ä»¶è§†é¢‘ç”Ÿæˆæ–¹é¢å­˜åœ¨æ˜¾è‘—è®¡ç®—ç“¶é¢ˆã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨åˆ†å±‚ç­–ç•¥ï¼šé¦–å…ˆä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆç¨€ç–çš„å…³é”®å¸§é›†åˆï¼Œç„¶åå°†è¿™äº›å…³é”®å¸§æå‡åˆ°3Dè¡¨ç¤ºä¸­ï¼Œé€šè¿‡3Dé‡å»ºå’Œæ¸²æŸ“æŠ€æœ¯åˆæˆä¸­é—´è§†å›¾ã€‚ç³»ç»Ÿå¼•å…¥äº†ä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç»™å®šç›¸æœºè½¨è¿¹é¢„æµ‹æœ€ä¼˜å…³é”®å¸§æ•°é‡ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ†é…è®¡ç®—èµ„æºã€‚æœ€ç»ˆæå‡ºçš„SRENDERæ–¹æ³•æ ¹æ®ç›¸æœºè¿åŠ¨å¤æ‚åº¦åŠ¨æ€è°ƒæ•´å…³é”®å¸§å¯†åº¦ï¼Œç®€å•è½¨è¿¹ä½¿ç”¨æç¨€ç–å…³é”®å¸§ï¼Œå¤æ‚è¿åŠ¨ä½¿ç”¨è¾ƒå¯†é›†å…³é”®å¸§ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒSRENDERåœ¨ç”Ÿæˆ20ç§’è§†é¢‘æ—¶æ¯”åŸºäºæ‰©æ•£çš„åŸºçº¿æ–¹æ³•å¿«40å€ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†é«˜è§†è§‰ä¿çœŸåº¦å’Œæ—¶é—´ç¨³å®šæ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ç”Ÿæˆæˆæœ¬åˆ†æ‘Šåˆ°æ•°ç™¾å¸§ä¸­å¹¶å¼ºåˆ¶æ‰§è¡Œå‡ ä½•ä¸€è‡´æ€§ï¼Œå®ç°äº†è®¡ç®—æ•ˆç‡çš„æ˜¾è‘—æå‡ã€‚è‡ªé€‚åº”å…³é”®å¸§åˆ†é…æœºåˆ¶ç¡®ä¿äº†åœ¨ä¸åŒç›¸æœºè½¨è¿¹å¤æ‚åº¦ä¸‹çš„æœ€ä¼˜æ€§èƒ½å¹³è¡¡ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡ç»“åˆç”Ÿæˆæ¨¡å‹ä¸3Dé‡å»ºæŠ€æœ¯ï¼Œå¯ä»¥å®ç°é«˜æ•ˆä¸”å¯æ§çš„è§†é¢‘åˆæˆï¼Œä¸ºå®æ—¶äº¤äº’åº”ç”¨ä¸­çš„è§†é¢‘ç”Ÿæˆæä¾›äº†å®ç”¨è·¯å¾„ã€‚æ–¹æ³•çš„æ ¸å¿ƒæ´å¯Ÿåœ¨äºå°†æ˜‚è´µçš„ç”Ÿæˆè¿‡ç¨‹é™åˆ¶åœ¨ç¨€ç–å…³é”®å¸§ä¸Šï¼Œç„¶ååˆ©ç”¨å‡ ä½•ä¸€è‡´æ€§è¿›è¡Œé«˜æ•ˆæ’å€¼ï¼Œè¿™ç§åˆ†å±‚ç­–ç•¥ä¸ºæœªæ¥è§†é¢‘ç”Ÿæˆç³»ç»Ÿçš„è®¾è®¡æä¾›äº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [33] [TranslateGemma Technical Report](https://arxiv.org/abs/2601.09012)
*Mara Finkelstein, Isaac Caswell, Tobias Domhan, Jan-Thorsten Peter, Juraj Juraska, Parker Riley, Daniel Deutsch, Cole Dilanni, Colin Cherry, Eleftheria Briakou, Elizabeth Nielsen, Jiaming Luo, Kat Black, Ryan Mullins, Sweta Agrawal, Wenda Xu, Erin Kats, Stephane Jaskiewicz, Markus Freitag, David Vilar*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†TranslateGemmaï¼Œä¸€ä¸ªåŸºäºGemma 3åŸºç¡€æ¨¡å‹çš„å¼€æºæœºå™¨ç¿»è¯‘å¥—ä»¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•æ˜¾è‘—æå‡äº†Gemma 3çš„å¤šè¯­è¨€ç¿»è¯‘èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨å¢å¼ºGemma 3åŸºç¡€æ¨¡å‹å›ºæœ‰çš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œä½¿å…¶ä¸“é—¨é€‚ç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡ï¼Œé€šè¿‡å¼€å‘å¼€æºç¿»è¯‘æ¨¡å‹ä¸ºç ”ç©¶ç¤¾åŒºæä¾›å¼ºå¤§ä¸”å¯é€‚åº”çš„å·¥å…·ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•ï¼šé¦–å…ˆä½¿ç”¨å¤§è§„æ¨¡åˆæˆå¹¶è¡Œæ•°æ®å’Œäººå·¥ç¿»è¯‘å¹¶è¡Œæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œåˆ©ç”¨MetricX-QEå’ŒAutoMQMç­‰å¥–åŠ±æ¨¡å‹é›†æˆæ¥ä¼˜åŒ–ç¿»è¯‘è´¨é‡ã€‚

**Result:** åœ¨WMT25æµ‹è¯•é›†çš„10ä¸ªè¯­è¨€å¯¹ä¸Šè¿›è¡Œäººå·¥è¯„ä¼°ï¼Œåœ¨WMT24++åŸºå‡†çš„55ä¸ªè¯­è¨€å¯¹ä¸Šè¿›è¡Œè‡ªåŠ¨è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºTranslateGemmaåœ¨æ‰€æœ‰æ¨¡å‹å°ºå¯¸ä¸Šéƒ½æ¯”åŸºçº¿Gemma 3æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œè¾ƒå°æ¨¡å‹å¸¸èƒ½è¾¾åˆ°è¾ƒå¤§åŸºçº¿æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ï¼ŒåŒæ—¶åœ¨Vistraå›¾åƒç¿»è¯‘åŸºå‡†ä¸Šå±•ç°å‡ºå¢å¼ºçš„å¤šæ¨¡æ€èƒ½åŠ›ã€‚

**Conclusion:** TranslateGemmaé€šè¿‡ä¸“é—¨çš„ä¸¤é˜¶æ®µå¾®è°ƒæœ‰æ•ˆæå‡äº†åŸºç¡€æ¨¡å‹çš„ç¿»è¯‘æ€§èƒ½ï¼Œè¯æ˜äº†è¾ƒå°æ¨¡å‹åœ¨ä¿æŒé«˜æ•ˆæ€§çš„åŒæ—¶èƒ½è¾¾åˆ°è¾ƒå¤§æ¨¡å‹çš„ç¿»è¯‘è´¨é‡ï¼Œä¸ºæœºå™¨ç¿»è¯‘ç ”ç©¶ç¤¾åŒºæä¾›äº†é«˜è´¨é‡çš„å¼€æºå·¥å…·ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„å¤šæ¨¡æ€èƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.


### [34] [Mi:dm 2.0 Korea-centric Bilingual Language Models](https://arxiv.org/abs/2601.09066)
*Donghoon Shin, Sejung Lee, Soonmin Bae, Hwijung Ryu, Changwon Ok, Hoyoun Jung, Hyesung Ji, Jeehyun Lim, Jehoon Lee, Ji-Eun Han, Jisoo Baik, Mihyeon Kim, Riwoo Chung, Seongmin Lee, Wonjae Park, Yoonseok Heo, Youngkyung Seo, Seyoun Won, Boeun Kim, Cheolhun Heo, Eunkyeong Lee, Honghee Lee, Hyeongju Ju, Hyeontae Seo, Jeongyong Shim, Jisoo Lee, Junseok Koh, Junwoo Kim, Minho Lee, Minji Kang, Minju Kim, Sangha Nam, Seongheum Park, Taehyeong Kim, Euijai Ahn, Hong Seok Jeung, Jisu Shin, Jiyeon Kim, Seonyeong Song, Seung Hyun Kong, Sukjin Hong, Taeyang Yun, Yu-Seon Kim, A-Hyun Lee, Chae-Jeong Lee, Hye-Won Yu, Ji-Hyun Ahn, Song-Yeon Kim, Sun-Woo Jung, Eunju Kim, Eunji Ha, Jinwoo Baek, Yun-ji Lee, Wanjin Park, Jeong Yeop Kim, Eun Mi Kim, Hyoung Jun Park, Jung Won Yoon, Min Sung Noh, Myung Gyo Oh, Wongyoung Lee, Yun Jin Park, Young S. Kwon, Hyun Keun Kim, Jieun Lee, YeoJoo Park*

#### ğŸ§© TL;DR
Mi:dm 2.0 æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæ¨è¿›éŸ©å›½ä¸­å¿ƒAIè®¾è®¡çš„åŒè¯­å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ•´åˆéŸ©å›½ç¤¾ä¼šçš„ä»·å€¼è§‚ã€æ¨ç†æ¨¡å¼å’Œå¸¸è¯†çŸ¥è¯†ï¼Œåœ¨éŸ©å›½ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æä¾›äº†åŸºç¡€ç‰ˆå’Œè¿·ä½ ç‰ˆä¸¤ç§é…ç½®ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éŸ©å›½ç›¸å…³å†…å®¹æ—¶å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦æºäºéŸ©å›½æ•°æ®ä¸è¶³æˆ–è´¨é‡ä½ä¸‹ä»¥åŠç¼ºä¹æ–‡åŒ–å¯¹é½ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥ç†è§£éŸ©å›½æ–‡åŒ–èƒŒæ™¯ã€æƒ…æ„Ÿç»†å¾®å·®åˆ«å’Œç°å®åœºæ™¯ï¼Œæ— æ³•ç”Ÿæˆå¯é ä¸”æ–‡åŒ–é€‚å®œçš„å“åº”ã€‚

**Method:** è¯¥æ¨¡å‹é‡‡ç”¨å…¨é¢çš„æ•°æ®å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬ä¸“æœ‰æ•°æ®æ¸…æ´—ã€é«˜è´¨é‡åˆæˆæ•°æ®ç”Ÿæˆã€ç»“åˆè¯¾ç¨‹å­¦ä¹ çš„ç­–ç•¥æ€§æ•°æ®æ··åˆï¼Œä»¥åŠå®šåˆ¶çš„éŸ©å›½ä¼˜åŒ–åˆ†è¯å™¨ä»¥æé«˜æ•ˆç‡å’Œè¦†ç›–èŒƒå›´ï¼›æ¨¡å‹æä¾›ä¸¤ç§é…ç½®ï¼šé‡‡ç”¨æ·±åº¦æ‰©å±•ç­–ç•¥çš„Mi:dm 2.0 Baseï¼ˆ115äº¿å‚æ•°ï¼‰é€‚ç”¨äºé€šç”¨åœºæ™¯ï¼Œä»¥åŠé’ˆå¯¹èµ„æºå—é™ç¯å¢ƒå’Œä¸“é—¨ä»»åŠ¡ä¼˜åŒ–çš„Mi:dm 2.0 Miniï¼ˆ23äº¿å‚æ•°ï¼‰ã€‚

**Result:** Mi:dm 2.0 åœ¨éŸ©å›½ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨KMMLUåŸºå‡†ä¸Šå–å¾—äº†é¡¶çº§çš„é›¶æ ·æœ¬ç»“æœï¼Œå¹¶åœ¨è¯­è¨€ã€äººæ–‡å’Œç¤¾ä¼šç§‘å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„å†…éƒ¨è¯„ä¼°ç»“æœã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡æä¾›å¯è®¿é—®ä¸”é«˜æ€§èƒ½çš„éŸ©å›½ä¸­å¿ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨åŠ é€ŸéŸ©å›½å„è¡Œä¸šã€å…¬å…±æœåŠ¡å’Œæ•™è‚²é¢†åŸŸçš„AIé‡‡ç”¨ï¼ŒåŠ å¼ºéŸ©å›½AIå¼€å‘è€…ç¤¾åŒºï¼Œå¹¶ä¸ºæ›´å¹¿æ³›çš„K-intelligenceæ„¿æ™¯å¥ å®šåŸºç¡€ï¼›æ¨¡å‹ä»¥MITè®¸å¯è¯å‘å¸ƒæ”¯æŒå¹¿æ³›çš„ç ”ç©¶å’Œå•†ä¸šä½¿ç”¨ã€‚

---

#### ğŸ“„ Abstract
We introduce Mi:dm 2.0, a bilingual large language model (LLM) specifically engineered to advance Korea-centric AI. This model goes beyond Korean text processing by integrating the values, reasoning patterns, and commonsense knowledge inherent to Korean society, enabling nuanced understanding of cultural contexts, emotional subtleties, and real-world scenarios to generate reliable and culturally appropriate responses. To address limitations of existing LLMs, often caused by insufficient or low-quality Korean data and lack of cultural alignment, Mi:dm 2.0 emphasizes robust data quality through a comprehensive pipeline that includes proprietary data cleansing, high-quality synthetic data generation, strategic data mixing with curriculum learning, and a custom Korean-optimized tokenizer to improve efficiency and coverage. To realize this vision, we offer two complementary configurations: Mi:dm 2.0 Base (11.5B parameters), built with a depth-up scaling strategy for general-purpose use, and Mi:dm 2.0 Mini (2.3B parameters), optimized for resource-constrained environments and specialized tasks. Mi:dm 2.0 achieves state-of-the-art performance on Korean-specific benchmarks, with top-tier zero-shot results on KMMLU and strong internal evaluation results across language, humanities, and social science tasks. The Mi:dm 2.0 lineup is released under the MIT license to support extensive research and commercial use. By offering accessible and high-performance Korea-centric LLMs, KT aims to accelerate AI adoption across Korean industries, public services, and education, strengthen the Korean AI developer community, and lay the groundwork for the broader vision of K-intelligence. Our models are available at https://huggingface.co/K-intelligence. For technical inquiries, please contact midm-llm@kt.com.


### [35] [Contrastive Bi-Encoder Models for Multi-Label Skill Extraction: Enhancing ESCO Ontology Matching with BERT and Attention Mechanisms](https://arxiv.org/abs/2601.09119)
*Yongming Sun*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬æŠ€èƒ½æå–æ¡†æ¶ï¼Œé€šè¿‡LLMä»ESCOå®šä¹‰åˆæˆè®­ç»ƒæ•°æ®ï¼Œå¹¶å¼•å…¥å±‚æ¬¡çº¦æŸçš„å¤šæŠ€èƒ½ç”Ÿæˆï¼Œè®­ç»ƒå¯¹æ¯”åŒç¼–ç å™¨å®ç°æ— æ ‡æ³¨æ•°æ®ä¸‹çš„æŠ€èƒ½åˆ†ç±»ï¼Œæ˜¾è‘—æå‡äº†ä¸­æ–‡æ‹›è˜å¹¿å‘Šä¸­çš„æŠ€èƒ½æå–æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç»†ç²’åº¦åŠ³åŠ¨åŠ›å¸‚åœºåˆ†æéœ€è¦å°†éç»“æ„åŒ–æ‹›è˜å¹¿å‘Šæ˜ å°„åˆ°æ ‡å‡†åŒ–æŠ€èƒ½åˆ†ç±»ä½“ç³»å¦‚ESCOï¼Œè¿™æœ¬è´¨ä¸Šæ˜¯æç«¯å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜ã€‚ç„¶è€Œç›‘ç£è§£å†³æ–¹æ¡ˆå—åˆ°å¤§è§„æ¨¡ã€åˆ†ç±»å¯¹é½æ ‡æ³¨ç¨€ç¼ºä¸”æˆæœ¬é«˜æ˜‚çš„é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨éè‹±è¯­ç¯å¢ƒä¸­ï¼Œæ‹›è˜å¹¿å‘Šè¯­è¨€ä¸æ­£å¼æŠ€èƒ½å®šä¹‰å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚

**Method:** è¯¥æ¡†æ¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»ESCOå®šä¹‰åˆæˆè®­ç»ƒå®ä¾‹ï¼Œå¹¶å¼•å…¥åŸºäºESCOäºŒçº§ç±»åˆ«çš„å±‚æ¬¡çº¦æŸå¤šæŠ€èƒ½ç”Ÿæˆä»¥æå‡å¤šæ ‡ç­¾ä¸Šä¸‹æ–‡ä¸­çš„è¯­ä¹‰è¿è´¯æ€§ã€‚åœ¨åˆæˆè¯­æ–™ä¸Šè®­ç»ƒå¯¹æ¯”åŒç¼–ç å™¨ï¼Œå°†æ‹›è˜å¹¿å‘Šå¥å­ä¸ESCOæŠ€èƒ½æè¿°å¯¹é½åˆ°å…±äº«åµŒå…¥ç©ºé—´ï¼›ç¼–ç å™¨åœ¨BERTéª¨å¹²åŸºç¡€ä¸Šå¢åŠ BiLSTMå’Œæ³¨æ„åŠ›æ± åŒ–ä»¥æ›´å¥½å»ºæ¨¡é•¿è€Œä¿¡æ¯å¯†é›†çš„éœ€æ±‚é™ˆè¿°ã€‚ä¸Šæ¸¸åŸºäºRoBERTaçš„äºŒå…ƒè¿‡æ»¤å™¨ç§»é™¤éæŠ€èƒ½å¥å­ä»¥æé«˜ç«¯åˆ°ç«¯ç²¾åº¦ã€‚

**Result:** å®éªŒè¡¨æ˜å±‚æ¬¡æ¡ä»¶ç”Ÿæˆç›¸æ¯”æ— çº¦æŸé…å¯¹åœ¨æµç•…æ€§å’Œå¯åŒºåˆ†æ€§ä¸Šå‡æœ‰æ”¹å–„ï¼Œæ‰€å¾—å¤šæ ‡ç­¾æ¨¡å‹èƒ½æœ‰æ•ˆè¿ç§»åˆ°çœŸå®ä¸–ç•Œä¸­æ–‡æ‹›è˜å¹¿å‘Šï¼Œå®ç°å¼ºå¤§çš„é›¶æ ·æœ¬æ£€ç´¢æ€§èƒ½ï¼ˆF1@5 = 0.72ï¼‰ï¼Œä¼˜äºTF-IDFå’Œæ ‡å‡†BERTåŸºçº¿ã€‚

**Conclusion:** è¯¥ç ”ç©¶æå‡ºçš„æµæ°´çº¿ä¸ºåŠ³åŠ¨ç»æµå­¦å’ŒåŠ³åŠ¨åŠ›åˆ†æä¸­çš„è‡ªåŠ¨åŒ–æŠ€èƒ½ç¼–ç æä¾›äº†å¯æ‰©å±•ã€æ•°æ®é«˜æ•ˆçš„é€”å¾„ï¼Œé€šè¿‡é›¶æ ·æœ¬æ–¹æ³•å…‹æœäº†æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«é€‚ç”¨äºéè‹±è¯­ç¯å¢ƒä¸‹çš„æŠ€èƒ½æå–ä»»åŠ¡ã€‚

---

#### ğŸ“„ Abstract
Fine-grained labor market analysis increasingly relies on mapping unstructured job advertisements to standardized skill taxonomies such as ESCO. This mapping is naturally formulated as an Extreme Multi-Label Classification (XMLC) problem, but supervised solutions are constrained by the scarcity and cost of large-scale, taxonomy-aligned annotations--especially in non-English settings where job-ad language diverges substantially from formal skill definitions. We propose a zero-shot skill extraction framework that eliminates the need for manually labeled job-ad training data. The framework uses a Large Language Model (LLM) to synthesize training instances from ESCO definitions, and introduces hierarchically constrained multi-skill generation based on ESCO Level-2 categories to improve semantic coherence in multi-label contexts. On top of the synthetic corpus, we train a contrastive bi-encoder that aligns job-ad sentences with ESCO skill descriptions in a shared embedding space; the encoder augments a BERT backbone with BiLSTM and attention pooling to better model long, information-dense requirement statements. An upstream RoBERTa-based binary filter removes non-skill sentences to improve end-to-end precision. Experiments show that (i) hierarchy-conditioned generation improves both fluency and discriminability relative to unconstrained pairing, and (ii) the resulting multi-label model transfers effectively to real-world Chinese job advertisements, achieving strong zero-shot retrieval performance (F1@5 = 0.72) and outperforming TF--IDF and standard BERT baselines. Overall, the proposed pipeline provides a scalable, data-efficient pathway for automated skill coding in labor economics and workforce analytics.


### [36] [OrthoGeoLoRA: Geometric Parameter-Efficient Fine-Tuning for Structured Social Science Concept Retrieval on theWeb](https://arxiv.org/abs/2601.09185)
*Zeqiang Wang, Xinyue Wu, Chenxi Li, Zixi Chen, Nishanth Sastry, Jon Johnson, Suparna De*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºOrthoGeoLoRAï¼Œä¸€ç§åŸºäºStiefelæµå½¢çº¦æŸçš„å‡ ä½•æ„ŸçŸ¥å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡å¼ºåˆ¶ä½ç§©å› å­æ­£äº¤åŒ–æ¥å…‹æœæ ‡å‡†LoRAçš„å‡ ä½•ç¼ºé™·ï¼Œåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹ä¸ºç¤¾ä¼šç§‘å­¦ä¿¡æ¯ç³»ç»Ÿçš„æ¨¡å‹é€‚é…æä¾›äº†æ›´é«˜æ•ˆè·¯å¾„ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬ç¼–ç å™¨åœ¨ç¤¾ä¼šç§‘å­¦ä¿¡æ¯ç³»ç»Ÿä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å®Œå…¨å¾®è°ƒçš„è®¡ç®—å’Œèƒ½è€—æˆæœ¬é«˜æ˜‚ï¼Œå¯¹Web4Goodç”Ÿæ€ç³»ç»Ÿä¸­çš„å°å‹æœºæ„å’Œéè¥åˆ©ç»„ç»‡æ„æˆéšœç¢ã€‚æ ‡å‡†LoRAæ–¹æ³•å­˜åœ¨å‡ ä½•ç¼ºé™·ï¼ŒåŒ…æ‹¬è§„èŒƒè‡ªç”±åº¦ã€å°ºåº¦æ¨¡ç³Šæ€§å’Œç§©å´©æºƒå€¾å‘ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Method:** æœ¬æ–‡æå‡ºOrthoGeoLoRAæ–¹æ³•ï¼Œé€šè¿‡å¼ºåˆ¶ä½ç§©å› å­æ­£äº¤åŒ–æ¥çº¦æŸå‚æ•°æ›´æ–°å½¢å¼ä¸ºÎ”W = BÎ£AâŠ¤ï¼Œç±»ä¼¼äºSVDåˆ†è§£ã€‚è¯¥æ–¹æ³•å°†ä½ç§©å› å­çº¦æŸåœ¨Stiefelæµå½¢ä¸Šï¼Œå¹¶é€šè¿‡å‡ ä½•é‡å‚æ•°åŒ–å®ç°è¿™ä¸€çº¦æŸï¼ŒåŒæ—¶ä¿æŒä¸Adamç­‰æ ‡å‡†ä¼˜åŒ–å™¨åŠç°æœ‰å¾®è°ƒæµç¨‹çš„å…¼å®¹æ€§ã€‚ç ”ç©¶è¿˜å»ºç«‹äº†åŸºäºæ¬§æ´²è¯­è¨€ç¤¾ä¼šç§‘å­¦å™è¯è¡¨ï¼ˆELSSTï¼‰çš„å±‚æ¬¡æ¦‚å¿µæ£€ç´¢åŸºå‡†ï¼Œç”¨äºè¯„ä¼°ç¤¾ä¼šç§‘å­¦æ•°å­—èµ„æºç»„ç»‡ä¸­çš„æ¨¡å‹æ€§èƒ½ã€‚

**Result:** åœ¨å¤šè¯­è¨€å¥å­ç¼–ç å™¨ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨ç›¸åŒä½ç§©é¢„ç®—ä¸‹ï¼ŒOrthoGeoLoRAåœ¨æ’åºæŒ‡æ ‡ä¸Šä¼˜äºæ ‡å‡†LoRAå’Œå¤šç§å¼ºå‚æ•°é«˜æ•ˆå¾®è°ƒå˜ä½“ã€‚è¯¥æ–¹æ³•åœ¨è®¡ç®—å’Œå‚æ•°æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸­çš„åŸºç¡€æ¨¡å‹é€‚é…æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

**Conclusion:** OrthoGeoLoRAé€šè¿‡å‡ ä½•çº¦æŸè§£å†³äº†æ ‡å‡†LoRAçš„å›ºæœ‰ç¼ºé™·ï¼Œä¸ºç¤¾ä¼šç§‘å­¦ä¿¡æ¯ç³»ç»Ÿä¸­çš„æ¨¡å‹å¾®è°ƒæä¾›äº†æ›´é«˜æ•ˆã€æ›´ç¨³å®šçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•ç‰¹åˆ«é€‚åˆèµ„æºå—é™çš„ç ”ç©¶æœºæ„å’Œéè¥åˆ©ç»„ç»‡ï¼Œæœ‰åŠ©äºæ¨åŠ¨Web4Goodç”Ÿæ€ç³»ç»Ÿä¸­äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å…¬å¹³è·å–å’Œåº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
Large language models and text encoders increasingly power web-based information systems in the social sciences, including digital libraries, data catalogues, and search interfaces used by researchers, policymakers, and civil society. Full fine-tuning is often computationally and energy intensive, which can be prohibitive for smaller institutions and non-profit organizations in the Web4Good ecosystem. Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation (LoRA), reduces this cost by updating only a small number of parameters. We show that the standard LoRA update $Î”W = BA^\top$ has geometric drawbacks: gauge freedom, scale ambiguity, and a tendency toward rank collapse. We introduce OrthoGeoLoRA, which enforces an SVD-like form $Î”W = BÎ£A^\top$ by constraining the low-rank factors to be orthogonal (Stiefel manifold). A geometric reparameterization implements this constraint while remaining compatible with standard optimizers such as Adam and existing fine-tuning pipelines. We also propose a benchmark for hierarchical concept retrieval over the European Language Social Science Thesaurus (ELSST), widely used to organize social science resources in digital repositories. Experiments with a multilingual sentence encoder show that OrthoGeoLoRA outperforms standard LoRA and several strong PEFT variants on ranking metrics under the same low-rank budget, offering a more compute- and parameter-efficient path to adapt foundation models in resource-constrained settings.


### [37] [TeachPro: Multi-Label Qualitative Teaching Evaluation via Cross-View Graph Synergy and Semantic Anchored Evidence Encoding](https://arxiv.org/abs/2601.09246)
*Xiangqian Wang, Yifan Jia, Yang Xiang, Yumin Zhang, Yanbin Wang, Ke Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†TeachProï¼Œä¸€ä¸ªå¤šæ ‡ç­¾å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä»å¼€æ”¾å¼å­¦ç”Ÿè¯„æ•™ä¸­ç³»ç»Ÿè¯„ä¼°äº”ä¸ªå…³é”®æ•™å­¦ç»´åº¦ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å°†åé¦ˆç®€åŒ–ä¸ºäºŒå…ƒæƒ…æ„Ÿè€Œå¿½è§†å…·ä½“æ•™å­¦é—®é¢˜çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ ‡å‡†åŒ–çš„å­¦ç”Ÿè¯„æ•™é€šå¸¸å­˜åœ¨å¯é æ€§ä½ã€å“åº”é€‰é¡¹å—é™å’Œå“åº”å¤±çœŸç­‰é—®é¢˜ã€‚ç°æœ‰çš„æœºå™¨å­¦ä¹ æ–¹æ³•æŒ–æ˜å¼€æ”¾å¼è¯„è®ºæ—¶é€šå¸¸å°†åé¦ˆç®€åŒ–ä¸ºäºŒå…ƒæƒ…æ„Ÿåˆ†æï¼Œè¿™å¿½è§†äº†å†…å®¹æ¸…æ™°åº¦ã€åé¦ˆåŠæ—¶æ€§å’Œæ•™å¸ˆæ€åº¦ç­‰å…·ä½“æ•™å­¦é—®é¢˜ï¼Œæ— æ³•ä¸ºæ•™å­¦æ”¹è¿›æä¾›æœ‰æ•ˆæŒ‡å¯¼ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†TeachProå¤šæ ‡ç­¾å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…å«ç»´åº¦é”šå®šè¯æ®ç¼–ç å™¨å’Œè·¨è§†å›¾å›¾ååŒç½‘ç»œã€‚ç»´åº¦é”šå®šè¯æ®ç¼–ç å™¨æ•´åˆäº†é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ã€è¡¨ç¤ºäº”ä¸ªæ•™å­¦ç»´åº¦çš„å¯å­¦ä¹ è¯­ä¹‰é”šç‚¹æ¨¡å—ï¼Œä»¥åŠç»“æ„åŒ–è¯­ä¹‰ç©ºé—´ä¸­å¯¹é½è¯æ®ä¸æ•™å­¦ç»´åº¦çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚è·¨è§†å›¾å›¾ååŒç½‘ç»œåŒ…å«ä»è§£ææ ‘æå–æ˜¾å¼è¯­æ³•ä¾èµ–çš„å¥æ³•åˆ†æ”¯å’ŒåŸºäºBERTç›¸ä¼¼å›¾å»ºæ¨¡æ½œåœ¨æ¦‚å¿µå…³ç³»çš„è¯­ä¹‰åˆ†æ”¯ï¼Œé€šè¿‡åŒä»¿å°„èåˆæ¨¡å—å¯¹é½å¥æ³•ä¸è¯­ä¹‰å•å…ƒï¼Œå¹¶ä½¿ç”¨å·®åˆ†æ­£åˆ™åŒ–å™¨è§£è€¦åµŒå…¥ä»¥è·å¾—äº’è¡¥è¡¨ç¤ºã€‚

**Result:** å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒTeachProåœ¨å¤šæ ·åŒ–çš„è¯„ä¼°è®¾ç½®ä¸­æä¾›äº†ä¼˜è¶Šçš„è¯Šæ–­ç²’åº¦å’Œé²æ£’æ€§ã€‚ä½œè€…è¿˜è´¡çŒ®äº†ä¸€ä¸ªåŒ…å«ä¸“å®¶å®šæ€§æ ‡æ³¨å’Œå¤šæ ‡ç­¾è¯„åˆ†çš„æ–°å‹åŸºå‡†æ•°æ®é›†ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºæ•™å­¦è¯„ä¼°æä¾›äº†æ›´ç²¾ç»†çš„åˆ†æå·¥å…·ï¼Œèƒ½å¤Ÿä»å¼€æ”¾å¼å­¦ç”Ÿåé¦ˆä¸­æå–å¤šç»´åº¦çš„æ•™å­¦æ´å¯Ÿï¼Œè¶…è¶Šäº†ä¼ ç»ŸäºŒå…ƒæƒ…æ„Ÿåˆ†æçš„å±€é™æ€§ã€‚TeachProæ¡†æ¶å±•ç¤ºäº†å°†ç»“æ„åŒ–è¯­ä¹‰ç©ºé—´ä¸å¤šè§†å›¾æ–‡æœ¬è¡¨ç¤ºç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ•™è‚²æ•°æ®æŒ–æ˜å’Œæ•™å­¦æ”¹è¿›æä¾›äº†æ–°çš„æŠ€æœ¯é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Standardized Student Evaluation of Teaching often suffer from low reliability, restricted response options, and response distortion. Existing machine learning methods that mine open-ended comments usually reduce feedback to binary sentiment, which overlooks concrete concerns such as content clarity, feedback timeliness, and instructor demeanor, and provides limited guidance for instructional improvement.We propose TeachPro, a multi-label learning framework that systematically assesses five key teaching dimensions: professional expertise, instructional behavior, pedagogical efficacy, classroom experience, and other performance metrics. We first propose a Dimension-Anchored Evidence Encoder, which integrates three core components: (i) a pre-trained text encoder that transforms qualitative feedback annotations into contextualized embeddings; (ii) a prompt module that represents five teaching dimensions as learnable semantic anchors; and (iii) a cross-attention mechanism that aligns evidence with pedagogical dimensions within a structured semantic space. We then propose a Cross-View Graph Synergy Network to represent student comments. This network comprises two components: (i) a Syntactic Branch that extracts explicit grammatical dependencies from parse trees, and (ii) a Semantic Branch that models latent conceptual relations derived from BERT-based similarity graphs. BiAffine fusion module aligns syntactic and semantic units, while a differential regularizer disentangles embeddings to encourage complementary representations. Finally, a cross-attention mechanism bridges the dimension-anchored evidence with the multi-view comment representations. We also contribute a novel benchmark dataset featuring expert qualitative annotations and multi-label scores. Extensive experiments demonstrate that TeachPro offers superior diagnostic granularity and robustness across diverse evaluation settings.


### [38] [MCGA: A Multi-task Classical Chinese Literary Genre Audio Corpus](https://arxiv.org/abs/2601.09270)
*Yexing Du, Kaiyuan Liu, Bihe Zhang, Youcheng Pan, Bo Yang, Liangyu Huo, Xiyuan Zhang, Jian Xie, Daojing He, Yang Xiang, Ming Liu, Bin Qin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MCGAå¤šä»»åŠ¡å¤å…¸æ–‡å­¦éŸ³é¢‘è¯­æ–™åº“ï¼Œå¡«è¡¥äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡å¤å…¸ç ”ç©¶éŸ³é¢‘æ¨¡æ€çš„ç©ºç™½ï¼Œå¹¶é€šè¿‡è¯„ä¼°åç§MLLMæ¨¡å‹æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è¯¥é¢†åŸŸçš„æ˜¾è‘—æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå…¶åœ¨ä¸­æ–‡å¤å…¸ç ”ç©¶é¢†åŸŸçš„æ½œåŠ›å—åˆ°å…³æ³¨ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºæ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ï¼ŒéŸ³é¢‘è¯­æ–™åº“åœ¨è¯¥é¢†åŸŸä»å¤„äºæ¢ç´¢ä¸è¶³çš„çŠ¶æ€ï¼Œéœ€è¦å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚

**Method:** ç ”ç©¶æå‡ºäº†å¤šä»»åŠ¡å¤å…¸æ–‡å­¦éŸ³é¢‘è¯­æ–™åº“ï¼Œæ¶µç›–å…­ç§ä»»åŠ¡ï¼šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ã€è¯­éŸ³æƒ…æ„Ÿæè¿°ã€å£è¯­é—®ç­”ã€è¯­éŸ³ç†è§£å’Œè¯­éŸ³æ¨ç†ï¼Œå¹¶å¼•å…¥äº†è¯­éŸ³æƒ…æ„Ÿæè¿°çš„è¯„ä¼°æŒ‡æ ‡ä»¥åŠè¡¡é‡MLLMè¯­éŸ³ä¸æ–‡æœ¬èƒ½åŠ›ä¸€è‡´æ€§çš„åº¦é‡æ–¹æ³•ã€‚

**Result:** é€šè¿‡å¯¹åç§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°å®éªŒï¼Œç»“æœè¡¨æ˜å½“å‰æ¨¡å‹åœ¨å¤„ç†MCGAæµ‹è¯•é›†æ—¶ä»é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šç»´éŸ³é¢‘èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼ŒéªŒè¯äº†è¯¥è¯­æ–™åº“å¯¹æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡å¤å…¸ç ”ç©¶éŸ³é¢‘å¤„ç†æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºçš„MCGAè¯­æ–™åº“å’Œè¯„ä¼°æ¡†æ¶ä¸ºå¼€å‘å…·æœ‰æ›´å¼ºå¤§å¤šç»´éŸ³é¢‘èƒ½åŠ›çš„æ¨¡å‹æä¾›äº†é‡è¦åŸºå‡†ï¼Œå¹¶å…¬å¼€äº†è¯­æ–™åº“å’Œä»£ç ä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
With the rapid advancement of Multimodal Large Language Models (MLLMs), their potential has garnered significant attention in Chinese Classical Studies (CCS). While existing research has primarily focused on text and visual modalities, the audio corpus within this domain remains largely underexplored. To bridge this gap, we propose the Multi-task Classical Chinese Literary Genre Audio Corpus (MCGA). It encompasses a diverse range of literary genres across six tasks: Automatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Speech Emotion Captioning (SEC), Spoken Question Answering (SQA), Speech Understanding (SU), and Speech Reasoning (SR). Through the evaluation of ten MLLMs, our experimental results demonstrate that current models still face substantial challenges when processed on the MCGA test set. Furthermore, we introduce an evaluation metric for SEC and a metric to measure the consistency between the speech and text capabilities of MLLMs. We release MCGA and our code to the public to facilitate the development of MLLMs with more robust multidimensional audio capabilities in CCS. MCGA Corpus: https://github.com/yxduir/MCGA


### [39] [Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework](https://arxiv.org/abs/2601.09342)
*Ewelina Gajewska, Katarzyna Budzynska, JarosÅ‚aw A Chudziak*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºéšå«ä»‡æ¨è¨€è®ºæ£€æµ‹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¡†æ¶ï¼Œé‡‡ç”¨ç”±ä¸­å¤®ä»²è£ä»£ç†å’ŒåŠ¨æ€æ„å»ºçš„ç¤¾åŒºä»£ç†ç»„æˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡æ•´åˆç¤¾ä¼šæ–‡åŒ–èƒŒæ™¯çŸ¥è¯†ï¼Œåœ¨åˆ†ç±»å‡†ç¡®æ€§å’Œå…¬å¹³æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æç¤ºæ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰éšå«ä»‡æ¨è¨€è®ºæ£€æµ‹æ–¹æ³•ç¼ºä¹å¯¹ç¤¾ä¼šæ–‡åŒ–èƒŒæ™¯çš„å……åˆ†è€ƒè™‘ï¼Œéš¾ä»¥å®ç°èº«ä»½æ„ŸçŸ¥çš„é€‚åº¦è°ƒèŠ‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é’ˆå¯¹ç‰¹å®šäººå£ç¾¤ä½“çš„å¾®å¦™åè§è¡¨è¾¾æ—¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ›´å…¬å¹³å’Œå‡†ç¡®çš„åˆ†ç±»æ¡†æ¶ã€‚

**Method:** è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ŒåŒ…æ‹¬ä¸­å¤®ä»²è£ä»£ç†å’Œä»£è¡¨ç‰¹å®šäººå£ç¾¤ä½“çš„åŠ¨æ€ç¤¾åŒºä»£ç†ï¼Œé€šè¿‡æ•´åˆæ¥è‡ªå…¬å¼€çŸ¥è¯†æºçš„ç¤¾ä¼šæ–‡åŒ–èƒŒæ™¯ä¿¡æ¯ï¼Œå®ç°äº†èº«ä»½æ„ŸçŸ¥çš„é€‚åº¦è°ƒèŠ‚ï¼Œå¹¶é‡‡ç”¨å¹³è¡¡å‡†ç¡®ç‡ä½œä¸ºåˆ†ç±»å…¬å¹³æ€§çš„æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ã€‚

**Result:** åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ToxiGenæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•è¶…è¶Šäº†é›¶æ ·æœ¬æç¤ºã€å°‘æ ·æœ¬æç¤ºå’Œæ€ç»´é“¾æç¤ºç­‰æœ€å…ˆè¿›çš„æç¤ºæ–¹æ³•ä»¥åŠå…¶ä»–æ›¿ä»£æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ‰€æœ‰ç›®æ ‡ç¾¤ä½“çš„åˆ†ç±»å‡†ç¡®æ€§å’Œå…¬å¹³æ€§ï¼Œå¹³è¡¡å‡†ç¡®ç‡æŒ‡æ ‡éªŒè¯äº†å…¶ä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç¤¾åŒºé©±åŠ¨çš„åå•†æ¡†æ¶åœ¨éšå«ä»‡æ¨è¨€è®ºæ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡æ•´åˆç¤¾ä¼šæ–‡åŒ–èƒŒæ™¯å’Œé‡‡ç”¨å…¬å¹³æ€§è¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºå®ç°æ›´å‡†ç¡®å’Œå…¬æ­£çš„å†…å®¹å®¡æ ¸æä¾›äº†æ–°æ–¹å‘ï¼Œå¼ºè°ƒäº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•æ„Ÿå†…å®¹è¯†åˆ«ä¸­çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods (zero-shot prompting, few-shot prompting, chain-of-thought prompting) and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness that accounts for the trade-off between true positive and true negative rates. We demonstrate that our community-driven consultative framework significantly improves both classification accuracy and fairness across all target groups.


### [40] [Relation Extraction Capabilities of LLMs on Clinical Text: A Bilingual Evaluation for English and Turkish](https://arxiv.org/abs/2601.09367)
*Aidana Aidynkyzy, OÄŸuz Dikenelli, Oylum AlatlÄ±, Åebnem Bora*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªè‹±è¯­-åœŸè€³å…¶è¯­å¹³è¡Œä¸´åºŠå…³ç³»æŠ½å–æ•°æ®é›†ï¼Œå¹¶ç³»ç»Ÿè¯„ä¼°äº†å¤šç§æç¤ºç­–ç•¥ï¼Œå…¶ä¸­åŸºäºå¯¹æ¯”å­¦ä¹ çš„å…³ç³»æ„ŸçŸ¥æ£€ç´¢æ–¹æ³•åœ¨ä¸´åºŠä¿¡æ¯æŠ½å–ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ¨¡å‹ï¼Œæ­ç¤ºäº†é«˜è´¨é‡ç¤ºä¾‹æ£€ç´¢å¯¹äºè·¨è¯­è¨€ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†çš„é‡è¦æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éè‹±è¯­è¯­è¨€ä¸´åºŠä¿¡æ¯æŠ½å–æ ‡æ³¨æ•°æ®ç¨€ç¼ºé˜»ç¢äº†ä¸»è¦åŸºäºè‹±è¯­å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹æ–¹æ³•çš„è¯„ä¼°ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡æ„å»ºé¦–ä¸ªè‹±è¯­-åœŸè€³å…¶è¯­å¹³è¡Œä¸´åºŠå…³ç³»æŠ½å–æ•°æ®é›†ï¼Œç³»ç»Ÿè¯„ä¼°LLMåœ¨è·¨è¯­è¨€ä¸´åºŠå…³ç³»æŠ½å–ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶æ¢ç´¢æœ‰æ•ˆçš„æç¤ºç­–ç•¥ä»¥å¼¥è¡¥èµ„æºå·®è·ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†é¦–ä¸ªä»2010 i2b2/VAå…³ç³»åˆ†ç±»è¯­æ–™åº“è¡ç”Ÿå¹¶ç²¾å¿ƒç­–åˆ’çš„è‹±è¯­-åœŸè€³å…¶è¯­å¹³è¡Œä¸´åºŠå…³ç³»æŠ½å–æ•°æ®é›†ï¼Œç³»ç»Ÿè¯„ä¼°äº†å¤šç§æç¤ºç­–ç•¥åŒ…æ‹¬å¤šç§ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ€ç»´é“¾æ–¹æ³•ï¼Œå¹¶ä¸PUREç­‰å¾®è°ƒåŸºçº¿æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œç‰¹åˆ«æå‡ºäº†åŸºäºå¯¹æ¯”å­¦ä¹ çš„å…³ç³»æ„ŸçŸ¥æ£€ç´¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸“é—¨è®¾è®¡ç”¨äºæ•æ‰å¥å­çº§å’Œå…³ç³»çº§è¯­ä¹‰ã€‚

**Result:** åŸºäºæç¤ºçš„LLMæ–¹æ³•åœ¨æ‰€æœ‰è¯„ä¼°ä¸­å‡ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ¨¡å‹ï¼Œè‹±è¯­è¯„ä¼°ç»“æœåœ¨æ‰€æœ‰LLMå’Œæç¤ºæŠ€æœ¯ä¸Šå‡ä¼˜äºåœŸè€³å…¶è¯­å¯¹åº”ç»“æœï¼Œåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ä¸­ï¼Œå…³ç³»æ„ŸçŸ¥æ£€ç´¢è¾¾åˆ°æœ€é«˜æ€§èƒ½ï¼ŒGemini 1.5 Flashåœ¨è‹±è¯­å’ŒåœŸè€³å…¶è¯­ä¸­åˆ†åˆ«è·å¾—0.906å’Œ0.888çš„å¾®å¹³å‡F1åˆ†æ•°ï¼Œå½“RARä¸DeepSeek-V3æ¨¡å‹çš„ç»“æ„åŒ–æ¨ç†æç¤ºç»“åˆæ—¶ï¼Œè‹±è¯­æ€§èƒ½è¿›ä¸€æ­¥æå‡è‡³0.918 F1ã€‚

**Conclusion:** é«˜è´¨é‡æ¼”ç¤ºæ£€ç´¢å¯¹äºä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†è‡³å…³é‡è¦ï¼Œå…ˆè¿›çš„æ£€ç´¢å’Œæç¤ºæŠ€æœ¯å…·æœ‰å¼¥è¡¥èµ„æºå·®è·çš„æ½œåŠ›ï¼Œå…³ç³»æ„ŸçŸ¥æ£€ç´¢æ–¹æ³•é€šè¿‡æ•æ‰å¥å­çº§å’Œå…³ç³»çº§è¯­ä¹‰æ˜¾è‘—æå‡äº†è·¨è¯­è¨€ä¸´åºŠå…³ç³»æŠ½å–æ€§èƒ½ï¼Œä¸ºä½èµ„æºè¯­è¨€çš„ä¸´åºŠä¿¡æ¯å¤„ç†æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
The scarcity of annotated datasets for clinical information extraction in non-English languages hinders the evaluation of large language model (LLM)-based methods developed primarily in English. In this study, we present the first comprehensive bilingual evaluation of LLMs for the clinical Relation Extraction (RE) task in both English and Turkish. To facilitate this evaluation, we introduce the first English-Turkish parallel clinical RE dataset, derived and carefully curated from the 2010 i2b2/VA relation classification corpus. We systematically assess a diverse set of prompting strategies, including multiple in-context learning (ICL) and Chain-of-Thought (CoT) approaches, and compare their performance to fine-tuned baselines such as PURE. Furthermore, we propose Relation-Aware Retrieval (RAR), a novel in-context example selection method based on contrastive learning, that is specifically designed to capture both sentence-level and relation-level semantics. Our results show that prompting-based LLM approaches consistently outperform traditional fine-tuned models. Moreover, evaluations for English performed better than their Turkish counterparts across all evaluated LLMs and prompting techniques. Among ICL methods, RAR achieves the highest performance, with Gemini 1.5 Flash reaching a micro-F1 score of 0.906 in English and 0.888 in Turkish. Performance further improves to 0.918 F1 in English when RAR is combined with a structured reasoning prompt using the DeepSeek-V3 model. These findings highlight the importance of high-quality demonstration retrieval and underscore the potential of advanced retrieval and prompting techniques to bridge resource gaps in clinical natural language processing.


### [41] [Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats](https://arxiv.org/abs/2601.09555)
*Manyi Zhang, Ji-Fu Li, Zhongao Sun, Haoli Bai, Hui-Ling Zhen, Zhenhua Dong, Xianzhi Yu*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢ç´¢äº†åè®­ç»ƒé‡åŒ–åœ¨å¾®ç¼©æµ®ç‚¹æ ¼å¼ä¸‹çš„é€‚ç”¨æ€§ï¼Œå‘ç°MXFP8èƒ½å®ç°è¿‘ä¹æ— æŸçš„æ€§èƒ½ï¼Œè€ŒMXFP4ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸ºç°æœ‰PTQæ–¹æ³•é€‚é…MXFPé‡åŒ–æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¾®ç¼©æµ®ç‚¹æ ¼å¼å·²æˆä¸ºå¤§è¯­è¨€æ¨¡å‹æœ‰å‰æ™¯çš„ä½ç²¾åº¦æ ¼å¼ï¼Œä½†ç°æœ‰åè®­ç»ƒé‡åŒ–ç®—æ³•ä¸»è¦å…³æ³¨æ•´æ•°é‡åŒ–ï¼Œå…¶åœ¨MXFPæ ¼å¼ä¸‹çš„é€‚ç”¨æ€§å’Œè¡Œä¸ºå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚

**Method:** æœ¬ç ”ç©¶é‡‡ç”¨ç³»ç»Ÿæ€§å®éªŒæ–¹æ³•ï¼Œæ¶µç›–è¶…è¿‡7ç§åè®­ç»ƒé‡åŒ–ç®—æ³•ã€15ä¸ªè¯„ä¼°åŸºå‡†å’Œ3ä¸ªå¤§è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œç‰¹åˆ«å…³æ³¨æ ¼å¼å…¼å®¹æ€§åˆ†æï¼Œå¹¶é’ˆå¯¹MXFP4æå‡ºäº†ç®€å•çš„é¢„ç¼©æ”¾ä¼˜åŒ–ç­–ç•¥æ¥ç¼“è§£ç¼©æ”¾å› å­è¯¯å·®ã€‚

**Result:** å®éªŒå‘ç°MXFP8èƒ½æŒç»­å®ç°è¿‘ä¹æ— æŸçš„æ€§èƒ½è¡¨ç°ï¼Œè€ŒMXFP4åˆ™å¼•å…¥æ˜¾è‘—ç²¾åº¦ä¸‹é™ï¼›PTQåœ¨MXFPä¸‹çš„æœ‰æ•ˆæ€§å¼ºçƒˆä¾èµ–äºæ ¼å¼å…¼å®¹æ€§ï¼ŒæŸäº›ç®—æ³•èŒƒå¼å§‹ç»ˆæ›´æœ‰æ•ˆï¼›é‡åŒ–æ•æ„Ÿæ€§ä¸»è¦ç”±è¯­è¨€æ¨¡å‹è€Œéè§†è§‰ç¼–ç å™¨ä¸»å¯¼ï¼›MXFP4çš„ç¼©æ”¾å› å­æ˜¯å…³é”®è¯¯å·®æºï¼Œé¢„ç¼©æ”¾ä¼˜åŒ–èƒ½æ˜¾è‘—ç¼“è§£å…¶å½±å“ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºç°æœ‰PTQæ–¹æ³•é€‚é…MXFPé‡åŒ–æä¾›äº†å®ç”¨æŒ‡å¯¼ï¼Œæ­ç¤ºäº†æ ¼å¼å…¼å®¹æ€§çš„é‡è¦æ€§ï¼Œå¹¶è¡¨æ˜é‡åŒ–æ•æ„Ÿæ€§ä¸»è¦ç”±è¯­è¨€æ¨¡å‹æ¶æ„å†³å®šï¼Œä¸ºæœªæ¥ä½ç²¾åº¦å¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²æä¾›äº†é‡è¦è§è§£ã€‚

---

#### ğŸ“„ Abstract
Microscaling Floating-Point (MXFP) has emerged as a promising low-precision format for large language models (LLMs). Despite various post-training quantization (PTQ) algorithms being proposed, they mostly focus on integer quantization, while their applicability and behavior under MXFP formats remain largely unexplored. To address this gap, this work conducts a systematic investigation of PTQ under MXFP formats, encompassing over 7 PTQ algorithms, 15 evaluation benchmarks, and 3 LLM families. The key findings include: 1) MXFP8 consistently achieves near-lossless performance, while MXFP4 introduces substantial accuracy degradation and remains challenging; 2) PTQ effectiveness under MXFP depends strongly on format compatibility, with some algorithmic paradigms being consistently more effective than others; 3) PTQ performance exhibits highly consistent trends across model families and modalities, in particular, quantization sensitivity is dominated by the language model rather than the vision encoder in multimodal LLMs; 4) The scaling factor of quantization is a critical error source in MXFP4, and a simple pre-scale optimization strategy can significantly mitigate its impact. Together, these results provide practical guidance on adapting existing PTQ methods to MXFP quantization.


### [42] [LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation](https://arxiv.org/abs/2601.09631)
*Stergios Chatzikyriakidis*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹ä¸ç¡®å®šæ€§éŸ³éŸµç®—æ³•çš„æ··åˆç³»ç»Ÿï¼Œç”¨äºè§£å†³LLMåœ¨å¸Œè…Šè¯­ç­‰ä½èµ„æºè¯­è¨€ä¸­éŸµå¾‹æ£€æµ‹ä¸ç”Ÿæˆæ–¹é¢çš„ä¸è¶³ï¼Œé€šè¿‡éŸ³éŸµéªŒè¯å¾ªç¯å°†è¯—æ­Œç”Ÿæˆæœ‰æ•ˆæ€§ä»ä¸è¶³4%æå‡è‡³73.1%ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§è¯­è¨€æ¨¡å‹è™½ç„¶åœ¨NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éŸµå¾‹ç›¸å…³ç°è±¡ï¼ˆå¦‚æŠ¼éŸµæ£€æµ‹ä¸ç”Ÿæˆï¼‰ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œè¿™ä¸€é—®é¢˜åœ¨å¸Œè…Šè¯­ç­‰ä½èµ„æºè¯­è¨€ä¸­å°¤ä¸ºçªå‡ºï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿå‡†ç¡®å¤„ç†éŸ³éŸµå­¦ä»»åŠ¡çš„ä¸“é—¨ç³»ç»Ÿã€‚

**Method:** ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆç³»ç»Ÿï¼Œå°†LLMä¸ç¡®å®šæ€§éŸ³éŸµç®—æ³•ç›¸ç»“åˆï¼Œå®ç°äº†å…¨é¢çš„å¸Œè…Šè¯­æŠ¼éŸµç±»å‹åˆ†ç±»ï¼ˆåŒ…æ‹¬çº¯éŸµã€å¯ŒéŸµã€ä¸å®Œå…¨éŸµã€é©¬èµ›å…‹éŸµå’Œç›¸åŒå‰å…ƒéŸ³æ¨¡å¼ï¼‰ï¼Œå¹¶é‡‡ç”¨å¸¦æœ‰éŸ³éŸµéªŒè¯çš„æ™ºèƒ½ç”Ÿæˆæµç¨‹ï¼Œè¯„ä¼°äº†å¤šç§æç¤ºç­–ç•¥ï¼ˆé›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€æ€ç»´é“¾å’ŒRAGå¢å¼ºï¼‰åœ¨ä¸åŒLLMä¸Šçš„è¡¨ç°ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºå­˜åœ¨æ˜¾è‘—çš„"æ¨ç†é¸¿æ²Ÿ"ï¼šç±»äººæ¨ç†æ¨¡å‹ï¼ˆClaude 3.7ï¼‰åœ¨æŠ¼éŸµè¯†åˆ«ä¸­è¾¾åˆ°40%å‡†ç¡®ç‡ï¼Œè€Œæ¨ç†å¯†é›†å‹æ¨¡å‹ï¼ˆClaude 4.5ï¼‰åœ¨ä½¿ç”¨æ€ç»´é“¾æç¤ºæ—¶è¾¾åˆ°54%çš„å…ˆè¿›æ°´å¹³ï¼›çº¯LLMç”Ÿæˆå®Œå…¨å¤±è´¥ï¼ˆæœ‰æ•ˆè¯—æ­Œä¸è¶³4%ï¼‰ï¼Œè€Œæ··åˆéªŒè¯ç³»ç»Ÿå°†æ€§èƒ½æ¢å¤è‡³73.1%ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜çº¯LLMåœ¨éŸ³éŸµå­¦ä»»åŠ¡ä¸Šå­˜åœ¨æ ¹æœ¬æ€§å±€é™ï¼Œè€Œæ··åˆæ–¹æ³•é€šè¿‡ç»“åˆç®—æ³•éªŒè¯èƒ½æ˜¾è‘—æå‡æ€§èƒ½ï¼›ç ”ç©¶å‘å¸ƒçš„ç³»ç»ŸåŠåŒ…å«4ä¸‡å¤šä¸ªæŠ¼éŸµçš„æ¸…æ´è¯­æ–™åº“ä¸ºä½èµ„æºè¯­è¨€çš„éŸµå¾‹ç ”ç©¶æä¾›äº†é‡è¦èµ„æºï¼Œå¼ºè°ƒäº†é¢†åŸŸç‰¹å®šçŸ¥è¯†ä¸LLMç»“åˆçš„å¿…è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Large Language Models (LLMs), despite their remarkable capabilities across NLP tasks, struggle with phonologically-grounded phenomena like rhyme detection and generation. This is even more evident in lower-resource languages such as Modern Greek. In this paper, we present a hybrid system that combines LLMs with deterministic phonological algorithms to achieve accurate rhyme identification/analysis and generation. Our approach implements a comprehensive taxonomy of Greek rhyme types, including Pure, Rich, Imperfect, Mosaic, and Identical Pre-rhyme Vowel (IDV) patterns, and employs an agentic generation pipeline with phonological verification. We evaluate multiple prompting strategies (zero-shot, few-shot, Chain-of-Thought, and RAG-augmented) across several LLMs including Claude 3.7 and 4.5, GPT-4o, Gemini 2.0 and open-weight models like Llama 3.1 8B and 70B and Mistral Large. Results reveal a significant "Reasoning Gap": while native-like models (Claude 3.7) perform intuitively (40\% accuracy in identification), reasoning-heavy models (Claude 4.5) achieve state-of-the-art performance (54\%) only when prompted with Chain-of-Thought. Most critically, pure LLM generation fails catastrophically (under 4\% valid poems), while our hybrid verification loop restores performance to 73.1\%. We release our system and a crucial, rigorously cleaned corpus of 40,000+ rhymes, derived from the Anemoskala and Interwar Poetry corpora, to support future research.


### [43] [Empathy Applicability Modeling for General Health Queries](https://arxiv.org/abs/2601.09696)
*Shan Randhawa, Agha Ali Raza, Kentaro Toyama, Julie Hui, Mustafa Naseem*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†å…±æƒ…é€‚ç”¨æ€§æ¡†æ¶ï¼ˆEAFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç†è®ºé©±åŠ¨çš„æ–¹æ³•ï¼Œç”¨äºåœ¨ç”ŸæˆåŒ»ç”Ÿå›å¤ä¹‹å‰è¯†åˆ«æ‚£è€…æŸ¥è¯¢ä¸­çš„å…±æƒ…éœ€æ±‚ï¼Œå¹¶å»ºç«‹äº†ç”¨äºé¢„æµ‹æ€§å…±æƒ…å»ºæ¨¡çš„åŸºå‡†æ•°æ®é›†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è¯­è¨€æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°èå…¥ä¸´åºŠå·¥ä½œæµç¨‹ï¼Œä½†é€šå¸¸ç¼ºä¹ä¸´åºŠå…±æƒ…è¿™ä¸€åŒ»æ‚£æ²Ÿé€šçš„å…³é”®è¦ç´ ã€‚ç°æœ‰NLPæ¡†æ¶ä¸»è¦å…³æ³¨å¯¹åŒ»ç”Ÿå›å¤ä¸­çš„å…±æƒ…è¿›è¡Œååº”æ€§æ ‡æ³¨ï¼Œè€Œåœ¨é¢„æµ‹æ€§å»ºæ¨¡å…±æƒ…éœ€æ±‚æ–¹é¢æ”¯æŒæœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸€èˆ¬å¥åº·æŸ¥è¯¢ä¸­ã€‚

**Method:** æœ¬æ–‡å¼•å…¥äº†å…±æƒ…é€‚ç”¨æ€§æ¡†æ¶ï¼ˆEAFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸´åºŠã€ä¸Šä¸‹æ–‡å’Œè¯­è¨€çº¿ç´¢çš„ç†è®ºé©±åŠ¨æ–¹æ³•ï¼Œç”¨äºæ ¹æ®æƒ…æ„Ÿååº”å’Œè§£é‡Šçš„é€‚ç”¨æ€§å¯¹æ‚£è€…æŸ¥è¯¢è¿›è¡Œåˆ†ç±»ã€‚ç ”ç©¶è€…å‘å¸ƒäº†çœŸå®æ‚£è€…æŸ¥è¯¢çš„åŸºå‡†æ•°æ®é›†ï¼Œç”±äººç±»å’ŒGPT-4oè¿›è¡ŒåŒé‡æ ‡æ³¨ï¼Œå¹¶åœ¨äººç±»å…±è¯†å­é›†ä¸Šè®­ç»ƒåˆ†ç±»å™¨æ¥é¢„æµ‹å…±æƒ…é€‚ç”¨æ€§ã€‚

**Result:** åœ¨äººç±»å…±è¯†å­é›†ä¸­è§‚å¯Ÿåˆ°æ˜¾è‘—çš„äººç±»-GPTå¯¹é½ã€‚åŸºäºäººç±»æ ‡æ³¨å’ŒGPT-onlyæ ‡æ³¨è®­ç»ƒçš„åˆ†ç±»å™¨åœ¨é¢„æµ‹å…±æƒ…é€‚ç”¨æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å¯å‘å¼æ–¹æ³•å’Œé›¶æ ·æœ¬LLMåŸºçº¿ã€‚é”™è¯¯åˆ†ææ­ç¤ºäº†éšå¼å›°æ‰°ã€ä¸´åºŠä¸¥é‡æ€§æ¨¡ç³Šæ€§å’Œä¸Šä¸‹æ–‡å›°éš¾ç­‰æŒç»­æŒ‘æˆ˜ã€‚

**Conclusion:** EAFä¸ºåœ¨å›å¤ç”Ÿæˆå‰è¯†åˆ«å…±æƒ…éœ€æ±‚æä¾›äº†æ¡†æ¶ï¼Œå»ºç«‹äº†é¢„æµ‹æ€§å…±æƒ…å»ºæ¨¡çš„åŸºå‡†ï¼Œå¹¶æ”¯æŒå¼‚æ­¥åŒ»ç–—ä¸­çš„å…±æƒ…æ²Ÿé€šã€‚ç ”ç©¶å¼ºè°ƒäº†å¤šæ ‡æ³¨è€…å»ºæ¨¡ã€ä¸´åºŠåŒ»ç”Ÿå‚ä¸æ ¡å‡†å’Œæ–‡åŒ–å¤šæ ·æ€§æ ‡æ³¨çš„å¿…è¦æ€§ï¼Œä¸ºæœªæ¥ä¸´åºŠNLPç³»ç»Ÿè®¾è®¡æä¾›äº†é‡è¦æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
LLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors' responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We release a benchmark of real patient queries, dual-annotated by Humans and GPT-4o. In the subset with human consensus, we also observe substantial human-GPT alignment. To validate EAF, we train classifiers on human-labeled and GPT-only annotations to predict empathy applicability, achieving strong performance and outperforming the heuristic and zero-shot LLM baselines. Error analysis highlights persistent challenges: implicit distress, clinical-severity ambiguity, and contextual hardship, underscoring the need for multi-annotator modeling, clinician-in-the-loop calibration, and culturally diverse annotation. EAF provides a framework for identifying empathy needs before response generation, establishes a benchmark for anticipatory empathy modeling, and enables supporting empathetic communication in asynchronous healthcare.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [44] [AviationLMM: A Large Multimodal Foundation Model for Civil Aviation](https://arxiv.org/abs/2601.09105)
*Wenbin Li, Jingling Wu, Xiaoyong Lin. Jing Chen, Cong Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†AviationLMMæ„¿æ™¯ï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘æ°‘èˆªé¢†åŸŸçš„å¤§å‹å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨ç»Ÿä¸€æ°‘èˆªå¼‚æ„æ•°æ®æµï¼Œå®ç°ç†è§£ã€æ¨ç†ã€ç”Ÿæˆå’Œæ™ºèƒ½ä½“åº”ç”¨ï¼Œä»¥è§£å†³ç°æœ‰AIè§£å†³æ–¹æ¡ˆåœ¨æ°‘èˆªä¸­å­¤ç«‹ã€å•æ¨¡æ€çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ°‘èˆªç°æœ‰AIè§£å†³æ–¹æ¡ˆå­˜åœ¨å­¤ç«‹å’Œå•æ¨¡æ€çš„å±€é™æ€§ï¼Œæ— æ³•æœ‰æ•ˆæ•´åˆè¯­éŸ³é€šä¿¡ã€é›·è¾¾è½¨è¿¹ã€ä¼ æ„Ÿå™¨æµå’Œæ–‡æœ¬æŠ¥å‘Šç­‰å¼‚æ„æ•°æ®ï¼Œè¿™é™åˆ¶äº†æ€åŠ¿æ„ŸçŸ¥ã€é€‚åº”æ€§å’Œå®æ—¶å†³ç­–æ”¯æŒèƒ½åŠ›ï¼Œé˜»ç¢äº†æ°‘èˆªå®‰å…¨ã€æ•ˆç‡å’Œå®¢æˆ·æ»¡æ„åº¦çš„æå‡ã€‚

**Method:** è®ºæ–‡æå‡ºäº†AviationLMMçš„æ¨¡å‹æ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿå¤„ç†ç©º-åœ°è¯­éŸ³ã€ç›‘è§†æ•°æ®ã€æœºè½½é¥æµ‹ã€è§†é¢‘å’Œç»“æ„åŒ–æ–‡æœ¬ç­‰å¤šæ¨¡æ€è¾“å…¥ï¼Œæ‰§è¡Œè·¨æ¨¡æ€å¯¹é½å’Œèåˆï¼Œå¹¶ç”Ÿæˆä»æ€åŠ¿æ‘˜è¦ã€é£é™©é¢„è­¦åˆ°é¢„æµ‹æ€§è¯Šæ–­å’Œå¤šæ¨¡æ€äº‹ä»¶é‡å»ºçš„çµæ´»è¾“å‡ºã€‚

**Result:** è®ºæ–‡æœªæŠ¥å‘Šå…·ä½“çš„æ€§èƒ½æŒ‡æ ‡æˆ–å®éªŒç»“æœï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ä¸ªç ”ç©¶æ„¿æ™¯å’Œæ¡†æ¶ï¼Œå¹¶è¯†åˆ«äº†å®ç°è¯¥æ„¿æ™¯éœ€è¦è§£å†³çš„å…³é”®ç ”ç©¶æœºä¼šï¼ŒåŒ…æ‹¬æ•°æ®è·å–ã€å¯¹é½ä¸èåˆã€é¢„è®­ç»ƒã€æ¨ç†ã€å¯ä¿¡æ€§ã€éšç§ã€æ¨¡æ€ç¼ºå¤±é²æ£’æ€§å’Œåˆæˆåœºæ™¯ç”Ÿæˆç­‰æ–¹é¢ã€‚

**Conclusion:** é€šè¿‡é˜è¿°AviationLMMçš„è®¾è®¡å’ŒæŒ‘æˆ˜ï¼Œæœ¬æ–‡æ—¨åœ¨æ¨åŠ¨æ°‘èˆªåŸºç¡€æ¨¡å‹çš„è¿›å±•ï¼Œå¹¶å‚¬åŒ–åè°ƒçš„ç ”ç©¶åŠªåŠ›ï¼Œä»¥æ„å»ºä¸€ä¸ªé›†æˆã€å¯ä¿¡ä¸”ä¿æŠ¤éšç§çš„æ°‘èˆªAIç”Ÿæ€ç³»ç»Ÿï¼Œä¸ºæœªæ¥æ°‘èˆªAIå‘å±•æä¾›æ˜ç¡®çš„ç ”ç©¶æ–¹å‘å’Œæ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.


### [45] [The AI Hippocampus: How Far are We From Human Memory?](https://arxiv.org/abs/2601.09113)
*Zixia Jia, Jiaqi Li, Yipeng Kang, Yuxuan Wang, Tong Wu, Quansen Wang, Xiaobo Wang, Shuyi Zhang, Junzhe Shen, Qing Li, Siyuan Qi, Yitao Liang, Di He, Zilong Zheng, Song-Chun Zhu*

#### ğŸ§© TL;DR
æœ¬æ–‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è®°å¿†æœºåˆ¶è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œæå‡ºäº†ä¸€ä¸ªæ¶µç›–éšå¼ã€æ˜¾å¼å’Œæ™ºèƒ½ä½“è®°å¿†èŒƒå¼çš„ç»“æ„åŒ–åˆ†ç±»ä½“ç³»ï¼Œç³»ç»Ÿæ¢³ç†äº†è¯¥é¢†åŸŸçš„å…³é”®æ¶æ„è¿›å±•ã€åŸºå‡†ä»»åŠ¡å’Œå¼€æ”¾æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€å¤§è¯­è¨€æ¨¡å‹ä»é™æ€é¢„æµ‹å™¨å‘å…·å¤‡æŒç»­å­¦ä¹ å’Œä¸ªæ€§åŒ–æ¨ç†èƒ½åŠ›çš„äº¤äº’ç³»ç»Ÿæ¼”è¿›ï¼Œè®°å¿†æœºåˆ¶å·²æˆä¸ºå…¶æ¶æ„å’ŒåŠŸèƒ½å‘å±•çš„æ ¸å¿ƒä¸»é¢˜ï¼Œä½†ç›®å‰ç¼ºä¹å¯¹è®°å¿†åœ¨LLMså’ŒMLLMsä¸­ä½œç”¨çš„ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œéœ€è¦å»ºç«‹ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶æ¥ç»„ç»‡ç›¸å…³æ–‡çŒ®å¹¶æŒ‡å¯¼æœªæ¥ç ”ç©¶ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„è®°å¿†åˆ†ç±»ä½“ç³»ï¼ŒåŒ…å«ä¸‰ä¸ªä¸»è¦èŒƒå¼ï¼šéšå¼è®°å¿†æŒ‡é¢„è®­ç»ƒtransformerå†…éƒ¨å‚æ•°ä¸­åµŒå…¥çš„çŸ¥è¯†ï¼Œæ¶‰åŠè®°å¿†åŒ–ã€å…³è”æ£€ç´¢å’Œä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼›æ˜¾å¼è®°å¿†æ¶‰åŠå¤–éƒ¨å­˜å‚¨å’Œæ£€ç´¢ç»„ä»¶ï¼ŒåŒ…æ‹¬æ–‡æœ¬è¯­æ–™åº“ã€ç¨ å¯†å‘é‡å’Œå›¾ç»“æ„ç­‰åŠ¨æ€å¯æŸ¥è¯¢çŸ¥è¯†è¡¨ç¤ºï¼›æ™ºèƒ½ä½“è®°å¿†åˆ™å…³æ³¨è‡ªä¸»æ™ºèƒ½ä½“ä¸­çš„æŒä¹…æ€§ã€æ—¶é—´æ‰©å±•è®°å¿†ç»“æ„ï¼Œæ”¯æŒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„é•¿æœŸè§„åˆ’ã€è‡ªä¸€è‡´æ€§å’Œåä½œè¡Œä¸ºã€‚

**Result:** è¯¥ç»¼è¿°ç³»ç»Ÿæ¢³ç†äº†è®°å¿†æœºåˆ¶åœ¨LLMså’ŒMLLMsä¸­çš„å…³é”®æ¶æ„è¿›å±•ã€åŸºå‡†ä»»åŠ¡å’Œè¯„ä¼°æ–¹æ³•ï¼Œç‰¹åˆ«å…³æ³¨äº†å¤šæ¨¡æ€åœºæ™¯ä¸‹è·¨è§†è§‰ã€è¯­è¨€ã€éŸ³é¢‘å’ŒåŠ¨ä½œæ¨¡æ€çš„è¿è´¯æ€§éœ€æ±‚ï¼Œå¹¶è¯†åˆ«äº†è®°å¿†å®¹é‡ã€å¯¹é½ã€äº‹å®ä¸€è‡´æ€§å’Œè·¨ç³»ç»Ÿäº’æ“ä½œæ€§ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚

**Conclusion:** è®°å¿†æœºåˆ¶å¯¹äºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€é€‚åº”æ€§å’Œä¸Šä¸‹æ–‡ä¿çœŸåº¦å…·æœ‰åŸºç¡€æ€§ä½œç”¨ï¼Œè¯¥ç»¼è¿°å»ºç«‹çš„åˆ†ç±»æ¡†æ¶ä¸ºç†è§£ä¸åŒè®°å¿†èŒƒå¼æä¾›äº†ç³»ç»Ÿè§†è§’ï¼ŒåŒæ—¶æŒ‡å‡ºçš„å¼€æ”¾æŒ‘æˆ˜ä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€äº¤äº’å’Œè‡ªä¸»æ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è®°å¿†é›†æˆæ–¹é¢ã€‚

---

#### ğŸ“„ Abstract
Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.


### [46] [RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering](https://arxiv.org/abs/2601.09269)
*Wencheng Ye, Liang Peng, Xiaoyang Yuan, Yi Bin, Pengpeng Zeng, Hengyu Jin, Heng Tao Shen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†RISERï¼ˆåŸºäºè·¯ç”±å™¨çš„å¯å¼•å¯¼æ¨ç†å¢å¼ºæ¡†æ¶ï¼‰ï¼Œä¸€ç§å³æ’å³ç”¨çš„æ¿€æ´»ç©ºé—´å¹²é¢„æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€ç»„åˆå¯å¤ç”¨æ¨ç†å‘é‡æ¥è‡ªé€‚åº”å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œå®ç°äº†æ— éœ€å‚æ•°æ›´æ–°çš„é«˜æ•ˆæ¨ç†å¢å¼ºã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰é¢†åŸŸç‰¹å®šæ¨ç†æ–¹æ³•é€šå¸¸ä¾èµ–éœ€è¦å‚æ•°æ›´æ–°çš„è®­ç»ƒå¯†é›†å‹æ–¹æ³•ï¼Œè€Œç°æœ‰çš„æ¿€æ´»å¼•å¯¼æ–¹æ³•é‡‡ç”¨é™æ€ã€æ‰‹åŠ¨å¹²é¢„ï¼Œæ— æ³•é€‚åº”å¤æ‚æ¨ç†çš„åŠ¨æ€ç‰¹æ€§ï¼Œè¿™é™åˆ¶äº†å‚æ•°é«˜æ•ˆæ¨ç†å¢å¼ºçš„é€‚åº”æ€§ã€‚

**Method:** RISERæ¡†æ¶æ„å»ºäº†å¯å¤ç”¨æ¨ç†å‘é‡åº“ï¼Œå¹¶é‡‡ç”¨è½»é‡çº§è·¯ç”±å™¨åŠ¨æ€ç»„åˆè¿™äº›å‘é‡ä»¥é€‚åº”æ¯ä¸ªè¾“å…¥ï¼›è·¯ç”±å™¨é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨ä»»åŠ¡çº§å¥–åŠ±ä¸‹è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æ¶Œç°å’Œç»„åˆæ–¹å¼æ¿€æ´»æ½œåœ¨çš„è®¤çŸ¥åŸè¯­ï¼Œå®ç°è‡ªé€‚åº”æ¿€æ´»ç©ºé—´å¹²é¢„ã€‚

**Result:** åœ¨ä¸ƒä¸ªå¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRISERç›¸æ¯”åŸºç¡€æ¨¡å‹å®ç°äº†3.4-6.5%çš„å¹³å‡é›¶æ ·æœ¬å‡†ç¡®ç‡æå‡ï¼ŒåŒæ—¶è¶…è¶Šäº†æ€ç»´é“¾æ¨ç†æ–¹æ³•ï¼Œå…·æœ‰2-3å€æ›´é«˜çš„æ ‡è®°æ•ˆç‡å¹¶ä¿æŒç¨³å¥çš„å‡†ç¡®ç‡å¢ç›Šï¼›åˆ†æè¡¨æ˜RISERèƒ½å¤Ÿè‡ªä¸»ç»„åˆå¤šä¸ªå‘é‡å½¢æˆå¯è§£é‡Šçš„ç²¾ç¡®æ§åˆ¶ç­–ç•¥ã€‚

**Conclusion:** RISERå±•ç¤ºäº†é€šè¿‡åŠ¨æ€ç»„åˆæ¨ç†å‘é‡å®ç°è‡ªé€‚åº”æ¿€æ´»å¼•å¯¼çš„å¯è¡Œæ€§ï¼Œä¸ºæ›´å¯æ§å’Œé«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†æä¾›äº†æ–°æ–¹å‘ï¼Œè¡¨æ˜æ¶Œç°å¼ç»„åˆå¹²é¢„èƒ½å¤Ÿäº§ç”Ÿå¯è§£é‡Šçš„æ§åˆ¶ç­–ç•¥ï¼Œæ¨åŠ¨äº†å‚æ•°é«˜æ•ˆæ¨ç†å¢å¼ºæ–¹æ³•çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.


### [47] [M$^3$Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning](https://arxiv.org/abs/2601.09278)
*Xiaohan Yu, Chao Feng, Lang Mei, Chong Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MÂ³Searcherï¼Œä¸€ç§æ¨¡å—åŒ–çš„å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢æ™ºèƒ½ä½“ï¼Œé€šè¿‡è§£è€¦ä¿¡æ¯è·å–ä¸ç­”æ¡ˆæ¨å¯¼è¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨æ£€ç´¢å¯¼å‘çš„å¤šç›®æ ‡å¥–åŠ±ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç¯å¢ƒä¸­çš„è‡ªä¸»ä¿¡æ¯æ£€ç´¢èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºDeepResearché£æ ¼çš„æ™ºèƒ½ä½“åœ¨è‡ªä¸»ä¿¡æ¯è·å–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»…é™äºæ–‡æœ¬æ¨¡æ€ã€‚å°†è‡ªä¸»ä¿¡æ¯æ£€ç´¢æ‰©å±•åˆ°å¤šæ¨¡æ€ç¯å¢ƒé¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šå¤§è§„æ¨¡è®­ç»ƒå¤šæ¨¡æ€å·¥å…·ä½¿ç”¨æ¨¡å‹æ—¶å‡ºç°çš„ä¸“ä¸šåŒ–ä¸æ³›åŒ–æƒè¡¡é—®é¢˜ï¼Œä»¥åŠæ•æ‰å¤æ‚å¤šæ­¥å¤šæ¨¡æ€æœç´¢è½¨è¿¹çš„è®­ç»ƒæ•°æ®ä¸¥é‡ç¨€ç¼ºã€‚

**Method:** MÂ³Searcheré‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œæ˜ç¡®å°†ä¿¡æ¯è·å–ä¸ç­”æ¡ˆæ¨å¯¼è¿‡ç¨‹è§£è€¦ã€‚è¯¥æ–¹æ³•é€šè¿‡æ£€ç´¢å¯¼å‘çš„å¤šç›®æ ‡å¥–åŠ±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼Œè”åˆé¼“åŠ±äº‹å®å‡†ç¡®æ€§ã€æ¨ç†åˆç†æ€§å’Œæ£€ç´¢ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜å¼€å‘äº†MMSearchVQAå¤šæ¨¡æ€å¤šè·³æ•°æ®é›†ï¼Œä»¥æ”¯æŒæ£€ç´¢ä¸­å¿ƒçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒMÂ³Searcheråœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œåœ¨å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„è¿ç§»é€‚åº”èƒ½åŠ›å’Œæœ‰æ•ˆçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒéªŒè¯äº†æ¨¡å—åŒ–æ¶æ„å’Œå¤šç›®æ ‡å¥–åŠ±ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡è§£è€¦ä¿¡æ¯è·å–ä¸ç­”æ¡ˆæ¨å¯¼çš„æ¨¡å—åŒ–è®¾è®¡ï¼ŒæˆåŠŸè§£å†³äº†å¤šæ¨¡æ€è‡ªä¸»ä¿¡æ¯æ£€ç´¢ä¸­çš„ä¸“ä¸šåŒ–-æ³›åŒ–æƒè¡¡é—®é¢˜ã€‚æå‡ºçš„å¤šç›®æ ‡å¥–åŠ±ä¼˜åŒ–æ¡†æ¶å’Œä¸“é—¨æ•°æ®é›†ä¸ºå¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„è®­ç»ƒæä¾›äº†æ–°èŒƒå¼ï¼Œä¸ºæœªæ¥æ›´å¤æ‚çš„å¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesize from real-world web environments. However, existing approaches remain fundamentally limited to text modality. Extending autonomous information-seeking agents to multimodal settings introduces critical challenges: the specialization-generalization trade-off that emerges when training models for multimodal tool-use at scale, and the severe scarcity of training data capturing complex, multi-step multimodal search trajectories. To address these challenges, we propose M$^3$Searcher, a modular multimodal information-seeking agent that explicitly decouples information acquisition from answer derivation. M$^3$Searcher is optimized with a retrieval-oriented multi-objective reward that jointly encourages factual accuracy, reasoning soundness, and retrieval fidelity. In addition, we develop MMSearchVQA, a multimodal multi-hop dataset to support retrieval centric RL training. Experimental results demonstrate that M$^3$Searcher outperforms existing approaches, exhibits strong transfer adaptability and effective reasoning in complex multimodal tasks.


### [48] [Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning](https://arxiv.org/abs/2601.09536)
*Dongjie Cheng, Yongqi Li, Zhixin Ma, Hongru Cai, Yupeng Hu, Wenjie Wang, Liqiang Nie, Wenjie Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ç»Ÿä¸€çš„ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨ç†èŒƒå¼ï¼Œé€šè¿‡ç”Ÿæˆä¸­é—´å›¾åƒæ¥ç»Ÿä¸€å¤šæ ·çš„å¤šæ¨¡æ€æ¨ç†æŠ€èƒ½ï¼Œå¹¶å®ä¾‹åŒ–ä¸ºOmni-R1æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µSFT+RLè®­ç»ƒæ–¹æ³•ï¼Œå®ç°äº†è·¨å¤šç§å¤šæ¨¡æ€ä»»åŠ¡çš„ç»Ÿä¸€æ¨ç†èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è™½ç„¶å–å¾—äº†è¿›å±•ï¼Œä½†é€šå¸¸é‡‡ç”¨å•ä¸€ä»»åŠ¡ç‰¹å®šçš„æ¨ç†æ¨¡å¼ï¼Œé™åˆ¶äº†åœ¨ä¸åŒå¤šæ¨¡æ€ä»»åŠ¡é—´çš„æ³›åŒ–èƒ½åŠ›ã€‚è®¸å¤šå¤šæ¨¡æ€ä»»åŠ¡éœ€è¦å¤šæ ·åŒ–çš„æ¨ç†æŠ€èƒ½ï¼Œå¦‚èšç„¦ç‰¹å®šåŒºåŸŸæˆ–æ ‡è®°å›¾åƒä¸­çš„å¯¹è±¡ï¼Œè€Œç°æœ‰æ–¹æ³•æ— æ³•ç»Ÿä¸€å¤„ç†è¿™äº›å¤šæ ·åŒ–çš„æ¨ç†éœ€æ±‚ã€‚

**Method:** æå‡ºäº†ç»Ÿä¸€çš„ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨ç†èŒƒå¼ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆä¸­é—´å›¾åƒæ¥ç»Ÿä¸€å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ¨ç†æŠ€èƒ½ã€‚å…·ä½“å®ä¾‹åŒ–ä¸ºOmni-R1æ¡†æ¶ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µç›‘ç£å¾®è°ƒåŠ å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…å«æ„ŸçŸ¥å¯¹é½æŸå¤±å’Œæ„ŸçŸ¥å¥–åŠ±æœºåˆ¶ä»¥å®ç°åŠŸèƒ½æ€§å›¾åƒç”Ÿæˆã€‚åŒæ—¶æå‡ºäº†Omni-R1-Zeroï¼Œé€šè¿‡ä»çº¯æ–‡æœ¬æ¨ç†æ•°æ®ä¸­å¼•å¯¼é€æ­¥å¯è§†åŒ–ï¼Œæ¶ˆé™¤äº†å¯¹å¤šæ¨¡æ€æ ‡æ³¨çš„éœ€æ±‚ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒOmni-R1èƒ½å¤Ÿåœ¨å¹¿æ³›çš„å¤šæ¨¡æ€ä»»åŠ¡ä¸Šå®ç°ç»Ÿä¸€çš„ç”Ÿæˆå¼æ¨ç†ã€‚Omni-R1-Zeroåœ¨å¹³å‡æ€§èƒ½ä¸Šèƒ½å¤ŸåŒ¹é…ç”šè‡³è¶…è¶ŠOmni-R1ï¼Œè¿™æ˜¾ç¤ºäº†ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨ç†çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡å°‘å¯¹æ ‡æ³¨æ•°æ®ä¾èµ–æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨ç†èŒƒå¼çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ä¸­é—´å›¾åƒç”Ÿæˆç»Ÿä¸€äº†å¤šæ ·åŒ–çš„æ¨ç†æŠ€èƒ½ã€‚Omni-R1-Zeroçš„æˆåŠŸè¡¨æ˜ï¼Œä»çº¯æ–‡æœ¬æ•°æ®å¼•å¯¼å¤šæ¨¡æ€æ¨ç†æ˜¯å¯è¡Œçš„æ–¹å‘ï¼Œä¸ºå‡å°‘å¯¹æ˜‚è´µå¤šæ¨¡æ€æ ‡æ³¨çš„ä¾èµ–æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€æ¨ç†å‘æ›´é€šç”¨ã€æ›´é«˜æ•ˆçš„æ–¹å‘å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.
