<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 23]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.AI](#cs.AI) [Total: 9]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs](https://arxiv.org/abs/2512.19918)
*Houston H. Zhang, Tao Zhang, Baoze Lin, Yuanqi Xue, Yincheng Zhu, Huan Liu, Li Gu, Linfeng Ye, Ziqiang Wang, Xinxin Zuo, Yang Wang, Yuanhao Yu, Zhixiang Chi*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†Widget-to-Codeï¼ˆWidget2Codeï¼‰ä»»åŠ¡ï¼Œé’ˆå¯¹ç´§å‡‘ã€æ— ä¸Šä¸‹æ–‡çš„å°éƒ¨ä»¶ç•Œé¢ç”Ÿæˆå¯æ‰§è¡Œä»£ç ï¼Œå¹¶å¼€å‘äº†WidgetFactoryåŸºç¡€è®¾æ–½ï¼ŒåŒ…å«é¢†åŸŸç‰¹å®šè¯­è¨€å’Œç¼–è¯‘å™¨ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰ä¿çœŸåº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰UI2Codeç ”ç©¶ä¸»è¦å…³æ³¨ç½‘é¡µå’Œç§»åŠ¨ç•Œé¢ï¼Œè€Œå°éƒ¨ä»¶ä½œä¸ºç´§å‡‘ã€æ— ä¸Šä¸‹æ–‡çš„å¾®ç•Œé¢ï¼Œå…·æœ‰å¯†é›†å¸ƒå±€å’Œå›¾æ ‡åŒ–ç‰¹å¾ï¼Œä¸”ç¼ºä¹å¯è®¿é—®çš„æ ‡è®°æ•°æ®ï¼Œè¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚

**Method:** ç ”ç©¶æå‡ºäº†è”åˆæ¨è¿›æ„ŸçŸ¥ç†è§£å’Œç»“æ„åŒ–ä»£ç ç”Ÿæˆçš„åŸºçº¿æ–¹æ³•ï¼Œåœ¨æ„ŸçŸ¥å±‚é¢éµå¾ªå°éƒ¨ä»¶è®¾è®¡åŸåˆ™å°†åŸå­ç»„ä»¶ç»„è£…ä¸ºå®Œæ•´å¸ƒå±€ï¼Œé…å¤‡å›¾æ ‡æ£€ç´¢å’Œå¯é‡ç”¨å¯è§†åŒ–æ¨¡å—ï¼›åœ¨ç³»ç»Ÿå±‚é¢è®¾è®¡äº†ç«¯åˆ°ç«¯åŸºç¡€è®¾æ–½WidgetFactoryï¼ŒåŒ…å«æ¡†æ¶æ— å…³çš„å°éƒ¨ä»¶é¢†åŸŸç‰¹å®šè¯­è¨€WidgetDSLå’Œå¯ç¼–è¯‘ä¸ºå¤šç§å‰ç«¯å®ç°çš„ç¼–è¯‘å™¨ï¼Œè‡ªé€‚åº”æ¸²æŸ“æ¨¡å—è¿›ä¸€æ­¥ä¼˜åŒ–ç©ºé—´ç»´åº¦ä»¥æ»¡è¶³ç´§å‡‘æ€§çº¦æŸã€‚

**Result:** åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œå°½ç®¡é€šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¼˜äºä¸“é—¨çš„UI2Codeæ–¹æ³•ï¼Œä½†ä»äº§ç”Ÿä¸å¯é ä¸”è§†è§‰ä¸ä¸€è‡´çš„ä»£ç ï¼›æå‡ºçš„åŸºçº¿æ–¹æ³•é€šè¿‡WidgetFactoryåŸºç¡€è®¾æ–½æ˜¾è‘—æå‡äº†è§†è§‰ä¿çœŸåº¦ï¼Œä¸ºWidget2Codeç ”ç©¶å»ºç«‹äº†ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶å’ŒåŸºç¡€è®¾æ–½ã€‚

**Conclusion:** è¯¥ç ”ç©¶å½¢å¼åŒ–äº†Widget2Codeä»»åŠ¡å¹¶å»ºç«‹äº†é¦–ä¸ªä»…å›¾åƒçš„åŸºå‡†æ•°æ®é›†ï¼Œæå‡ºçš„æ–¹æ³•é€šè¿‡è”åˆæ„ŸçŸ¥ç†è§£å’Œç»“æ„åŒ–ä»£ç ç”Ÿæˆè§£å†³äº†å°éƒ¨ä»¶ç‰¹æœ‰çš„æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å¼ºå¤§çš„åŸºçº¿æ–¹æ³•å’Œç»Ÿä¸€çš„åŸºç¡€è®¾æ–½ï¼Œæ¨åŠ¨äº†ç´§å‡‘ç•Œé¢ä»£ç ç”Ÿæˆé¢†åŸŸçš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.


### [2] [Vehicle-centric Perception via Multimodal Structured Pre-training](https://arxiv.org/abs/2512.19934)
*Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºVehicleMAE-V2ï¼Œä¸€ç§é¢å‘è½¦è¾†æ„ŸçŸ¥çš„é¢„è®­ç»ƒå¤§æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥è½¦è¾†ç›¸å…³çš„ç»“æ„åŒ–å…ˆéªŒçŸ¥è¯†æ¥æŒ‡å¯¼æ©ç ä»¤ç‰Œé‡å»ºè¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†è½¦è¾†ä¸­å¿ƒæ„ŸçŸ¥ä»»åŠ¡çš„æ³›åŒ–è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•åœ¨é¢„è®­ç»ƒé˜¶æ®µç¼ºä¹å¯¹è½¦è¾†ç›¸å…³çŸ¥è¯†çš„æœ‰æ•ˆå­¦ä¹ ï¼Œå¯¼è‡´åœ¨å»ºæ¨¡é€šç”¨è½¦è¾†æ„ŸçŸ¥è¡¨ç¤ºæ—¶èƒ½åŠ›ä¸è¶³ï¼Œæ— æ³•å……åˆ†æ•æ‰è½¦è¾†ç‰¹æœ‰çš„ç»“æ„ç‰¹å¾å’Œè¯­ä¹‰ä¿¡æ¯ã€‚

**Method:** è¯¥æ–¹æ³•è®¾è®¡äº†ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šå¯¹ç§°å¼•å¯¼æ©ç æ¨¡å—åˆ©ç”¨è½¦è¾†å¯¹ç§°æ€§çº¦æŸé€‰æ‹©é«˜è´¨é‡æ©ç å›¾åƒå—å¹¶å‡å°‘ä¿¡æ¯å†—ä½™ï¼›è½®å»“å¼•å¯¼è¡¨ç¤ºæ¨¡å—é€šè¿‡æœ€å°åŒ–è½®å»“ç‰¹å¾ä¸é‡å»ºç‰¹å¾çš„æ¦‚ç‡åˆ†å¸ƒå·®å¼‚æ¥ä¿ç•™æ•´ä½“è½¦è¾†ç»“æ„ä¿¡æ¯ï¼›è¯­ä¹‰å¼•å¯¼è¡¨ç¤ºæ¨¡å—é€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œè·¨æ¨¡æ€è’¸é¦å¯¹é½å›¾åƒ-æ–‡æœ¬ç‰¹å¾ä»¥è§£å†³æ©ç é‡å»ºä¸­çš„è¯­ä¹‰æ··æ·†é—®é¢˜ã€‚

**Result:** å®éªŒæ„å»ºäº†åŒ…å«çº¦400ä¸‡è½¦è¾†å›¾åƒå’Œ12,693æ¡æ–‡æœ¬æè¿°çš„å¤§è§„æ¨¡æ•°æ®é›†Autobot4Mï¼Œå¹¶åœ¨äº”ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ï¼Œè¯æ˜äº†VehicleMAE-V2çš„ä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜åˆ©ç”¨è½¦è¾†ç‰¹æœ‰çš„ç»“æ„åŒ–å…ˆéªŒçŸ¥è¯†ï¼ˆå¯¹ç§°æ€§ã€è½®å»“ã€è¯­ä¹‰ï¼‰å¯ä»¥æœ‰æ•ˆæŒ‡å¯¼æ©ç è‡ªç¼–ç å™¨çš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œä¸ºè½¦è¾†ä¸­å¿ƒæ„ŸçŸ¥ä»»åŠ¡å­¦ä¹ æ›´å…·æ³›åŒ–èƒ½åŠ›çš„è¡¨ç¤ºï¼Œä¸ºæ™ºèƒ½äº¤é€šã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸçš„è§†è§‰æ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†æ–°çš„é¢„è®­ç»ƒèŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.


### [3] [SE360: Semantic Edit in 360$^\circ$ Panoramas via Hierarchical Data Construction](https://arxiv.org/abs/2512.19943)
*Haoyi Zhong, Fang-Lue Zhang, Andrew Chalmers, Taehyun Rhee*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†SE360ï¼Œä¸€ä¸ªç”¨äº360Â°å…¨æ™¯å›¾åƒå¤šæ¡ä»¶å¼•å¯¼å¯¹è±¡ç¼–è¾‘çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡è‡ªä¸»æ•°æ®ç”Ÿæˆç®¡é“å’Œä¸¤é˜¶æ®µæ•°æ®ç²¾ç‚¼ç­–ç•¥ï¼Œå®ç°äº†åœ¨æ–‡æœ¬ã€æ©ç æˆ–å‚è€ƒå›¾åƒå¼•å¯¼ä¸‹çš„çµæ´»å¯¹è±¡ç¼–è¾‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ–¹æ³•æ‰©å±•åˆ°360Â°å…¨æ™¯å›¾åƒæ—¶é¢ä¸´é¢å¤–æŒ‘æˆ˜ï¼Œåœ¨ç­‰è·æŸ±çŠ¶æŠ•å½±å’Œé€è§†è§†å›¾ä¸­å¸¸äº§ç”Ÿä¸åˆç†ç»“æœï¼Œéœ€è¦è§£å†³å…¨æ™¯å›¾åƒç¼–è¾‘ä¸­çš„è¯­ä¹‰ä¸€è‡´æ€§å’Œå‡ ä½•ä¸€è‡´æ€§é—®é¢˜ã€‚

**Method:** æ–¹æ³•æ ¸å¿ƒåŒ…æ‹¬æ–°é¢–çš„ä»ç²—åˆ°ç»†çš„è‡ªä¸»æ•°æ®ç”Ÿæˆç®¡é“ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å’Œè‡ªé€‚åº”æŠ•å½±è°ƒæ•´è¿›è¡Œåˆ†å±‚åˆ†æï¼Œç¡®ä¿å¯¹è±¡åŠå…¶ç‰©ç†ç¯å¢ƒçš„æ•´ä½“åˆ†å‰²ï¼›åŒæ—¶å¼•å…¥ç»æµé«˜æ•ˆçš„ä¸¤é˜¶æ®µæ•°æ®ç²¾ç‚¼ç­–ç•¥ï¼Œæå‡æ•°æ®çœŸå®æ„Ÿå¹¶å‡è½»æ¨¡å‹å¯¹æ“¦é™¤ä¼ªå½±çš„è¿‡æ‹Ÿåˆï¼›åŸºäºæ„å»ºçš„æ•°æ®é›†è®­ç»ƒåŸºäºTransformerçš„æ‰©æ•£æ¨¡å‹ã€‚

**Result:** å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œè¯­ä¹‰å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„æ•°æ®å¯¹æ—¢å…·æœ‰è¯­ä¹‰æ„ä¹‰åˆä¿æŒå‡ ä½•ä¸€è‡´æ€§ï¼Œå³ä½¿æ¥è‡ªæœªæ ‡è®°çš„å…¨æ™¯å›¾åƒä¹Ÿèƒ½ä¿è¯ç¼–è¾‘ç»“æœçš„åˆç†æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†è‡ªä¸»æ•°æ®ç”Ÿæˆç®¡é“åœ¨è§£å†³å…¨æ™¯å›¾åƒç¼–è¾‘æŒ‘æˆ˜ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºçš„å¤šæ¡ä»¶å¼•å¯¼ç¼–è¾‘æ¡†æ¶ä¸º360Â°å…¨æ™¯å›¾åƒçš„çµæ´»ç¼–è¾‘æä¾›äº†æ–°é€”å¾„ï¼ŒåŒæ—¶æ•°æ®ç²¾ç‚¼ç­–ç•¥ä¸ºå‡å°‘æ¨¡å‹è¿‡æ‹Ÿåˆæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
While instruction-based image editing is emerging, extending it to 360$^\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.


### [4] [PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification](https://arxiv.org/abs/2512.20011)
*Blessing Agyei Kyem, Joshua Kofi Asamoah, Anthony Dontoh, Andrews Danyo, Eugene Denteh, Armstrong Aboah*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†é¦–ä¸ªå…¨çƒä»£è¡¨æ€§çš„è·¯é¢ç¼ºé™·æ£€æµ‹åŸºå‡†æ•°æ®é›†ï¼Œé€šè¿‡æ•´åˆå¤šä¸ªå…¬å¼€æ•°æ®æºå¹¶æ ‡å‡†åŒ–æ ‡æ³¨æ ¼å¼ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†åœ¨æ ‡æ³¨é£æ ¼ã€ç¼ºé™·å®šä¹‰å’Œæ ¼å¼ä¸Šçš„ä¸ä¸€è‡´é—®é¢˜ï¼Œä¸ºæ¨¡å‹è®­ç»ƒå’Œå…¬å¹³æ¯”è¾ƒæä¾›äº†ç»Ÿä¸€èµ„æºã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è‡ªåŠ¨åŒ–è·¯é¢ç¼ºé™·æ£€æµ‹åœ¨å¤šæ ·åŒ–çœŸå®åœºæ™¯ä¸­æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹æ ‡å‡†åŒ–æ•°æ®é›†ã€‚ç°æœ‰æ•°æ®é›†åœ¨æ ‡æ³¨é£æ ¼ã€ç¼ºé™·ç±»å‹å®šä¹‰å’Œæ ¼å¼ä¸Šå­˜åœ¨å·®å¼‚ï¼Œé™åˆ¶äº†å®ƒä»¬æ•´åˆç”¨äºç»Ÿä¸€è®­ç»ƒï¼Œé˜»ç¢äº†æ¨¡å‹çš„å…¬å¹³æ¯”è¾ƒå’Œæ€§èƒ½è¯„ä¼°ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªç»¼åˆæ€§åŸºå‡†æ•°æ®é›†ï¼Œæ•´åˆäº†å¤šä¸ªå…¬å¼€æ•°æ®æºï¼ŒåŒ…å«æ¥è‡ªä¸ƒä¸ªå›½å®¶çš„52747å¼ å›¾åƒå’Œ135277ä¸ªè¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œè¦†ç›–13ç§ä¸åŒçš„ç¼ºé™·ç±»å‹ã€‚æ•°æ®é›†æ ‡å‡†åŒ–äº†ç±»åˆ«å®šä¹‰å’Œæ ‡æ³¨æ ¼å¼ï¼Œå¹¶é‡‡ç”¨æœ€å…ˆè¿›çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼ˆåŒ…æ‹¬YOLOv8-YOLOv12ã€Faster R-CNNå’ŒDETRï¼‰è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°å…¶åœ¨å¤šæ ·åŒ–åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

**Result:** åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ•°æ®é›†æ”¯æŒå¤šç§å…ˆè¿›ç›®æ ‡æ£€æµ‹æ¨¡å‹å–å¾—ç«äº‰æ€§æ€§èƒ½ã€‚æ•°æ®é›†æ•è·äº†å›¾åƒè´¨é‡ã€åˆ†è¾¨ç‡ã€æ‹æ‘„è§’åº¦å’Œå¤©æ°”æ¡ä»¶ç­‰æ–¹é¢çš„å¹¿æ³›çœŸå®ä¸–ç•Œå˜åŒ–ï¼Œä¸ºé›¶æ ·æœ¬è¿ç§»åˆ°æ–°ç¯å¢ƒæä¾›äº†æœ‰æ•ˆè¯„ä¼°å¹³å°ï¼Œå±•ç¤ºäº†æ¨¡å‹åœ¨å¤šæ ·åŒ–åœºæ™¯ä¸‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶æä¾›äº†é¦–ä¸ªå…¨çƒä»£è¡¨æ€§çš„è·¯é¢ç¼ºé™·æ£€æµ‹åŸºå‡†æ•°æ®é›†ï¼Œé€šè¿‡æ ‡å‡†åŒ–æ ‡æ³¨è§£å†³äº†æ•°æ®é›†ä¸ä¸€è‡´é—®é¢˜ï¼Œå®ç°äº†æ¨¡å‹çš„å…¬å¹³æ¯”è¾ƒã€‚æ•°æ®é›†æ”¯æŒé›¶æ ·æœ¬è¿ç§»è¯„ä¼°ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†ç»Ÿä¸€çš„è®­ç»ƒå’Œæµ‹è¯•å¹³å°ï¼Œæ¨åŠ¨äº†è‡ªåŠ¨åŒ–è·¯é¢æ£€æµ‹æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
Automated pavement defect detection often struggles to generalize across diverse real-world conditions due to the lack of standardized datasets. Existing datasets differ in annotation styles, distress type definitions, and formats, limiting their integration for unified training. To address this gap, we introduce a comprehensive benchmark dataset that consolidates multiple publicly available sources into a standardized collection of 52747 images from seven countries, with 135277 bounding box annotations covering 13 distinct distress types. The dataset captures broad real-world variation in image quality, resolution, viewing angles, and weather conditions, offering a unique resource for consistent training and evaluation. Its effectiveness was demonstrated through benchmarking with state-of-the-art object detection models including YOLOv8-YOLOv12, Faster R-CNN, and DETR, which achieved competitive performance across diverse scenarios. By standardizing class definitions and annotation formats, this dataset provides the first globally representative benchmark for pavement defect detection and enables fair comparison of models, including zero-shot transfer to new environments.


### [5] [A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments](https://arxiv.org/abs/2512.20025)
*Anthony Dontoh, Stephanie Ivey, Armstrong Aboah*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨åˆ†å¿ƒé©¾é©¶æ£€æµ‹ä¸­ç»“åˆé©¾é©¶å‘˜è§†è§’å’Œé“è·¯è§†è§’çš„åŒè§†å›¾æ–¹æ³•ï¼Œå‘ç°æ€§èƒ½æå‡é«˜åº¦ä¾èµ–äºæ¨¡å‹æ¶æ„è®¾è®¡ï¼Œå…¶ä¸­å•è·¯å¾„SlowOnlyæ¨¡å‹è·å¾—9.8%çš„å‡†ç¡®ç‡æå‡ï¼Œè€ŒåŒè·¯å¾„SlowFastæ¨¡å‹å› è¡¨å¾å†²çªå¯¼è‡´7.2%çš„æ€§èƒ½ä¸‹é™ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºè®¡ç®—æœºè§†è§‰çš„åˆ†å¿ƒé©¾é©¶æ£€æµ‹æ¨¡å‹ä¸»è¦ä¾èµ–é©¾é©¶å‘˜è§†è§’ï¼Œå¿½ç•¥äº†å½±å“é©¾é©¶è¡Œä¸ºçš„å…³é”®ç¯å¢ƒä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢ç»“åˆé“è·¯è§†è§’ä¸é©¾é©¶å‘˜è§†è§’çš„åŒè§†å›¾è¾“å…¥æ˜¯å¦èƒ½æå‡è‡ªç„¶é©¾é©¶æ¡ä»¶ä¸‹çš„åˆ†å¿ƒæ£€æµ‹å‡†ç¡®æ€§ã€‚

**Method:** ç ”ç©¶ä½¿ç”¨çœŸå®é©¾é©¶ç¯å¢ƒä¸­åŒæ­¥çš„åŒæ‘„åƒå¤´è®°å½•æ•°æ®ï¼Œå¯¹ä¸‰ç§é¢†å…ˆçš„æ—¶ç©ºåŠ¨ä½œè¯†åˆ«æ¶æ„è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼šSlowFast-R50ã€X3D-Må’ŒSlowOnly-R50ï¼Œæ¯ç§æ¨¡å‹åœ¨ä¸¤ç§è¾“å…¥é…ç½®ä¸‹è¿›è¡Œè¯„ä¼°ï¼šä»…é©¾é©¶å‘˜è§†è§’å’Œå †å çš„åŒè§†å›¾è¾“å…¥ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸Šä¸‹æ–‡è¾“å…¥å¯¹æ€§èƒ½æå‡çš„å½±å“é«˜åº¦ä¾èµ–äºåº•å±‚æ¶æ„ï¼Œå•è·¯å¾„SlowOnlyæ¨¡å‹åœ¨åŒè§†å›¾è¾“å…¥ä¸‹è·å¾—9.8%çš„å‡†ç¡®ç‡æå‡ï¼Œè€ŒåŒè·¯å¾„SlowFastæ¨¡å‹å› è¡¨å¾å†²çªå¯¼è‡´7.2%çš„å‡†ç¡®ç‡ä¸‹é™ï¼ŒX3D-Mæ¨¡å‹çš„è¡¨ç°åˆ™ä»‹äºä¸¤è€…ä¹‹é—´ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç®€å•åœ°æ·»åŠ è§†è§‰ä¸Šä¸‹æ–‡å¹¶ä¸è¶³ä»¥æå‡åˆ†å¿ƒé©¾é©¶æ£€æµ‹æ€§èƒ½ï¼Œåè€Œå¯èƒ½å¯¼è‡´å¹²æ‰°ï¼Œé™¤éæ¶æ„ä¸“é—¨è®¾è®¡ç”¨äºæ”¯æŒå¤šè§†å›¾é›†æˆï¼Œè¿™å¼ºè°ƒäº†èåˆæ„ŸçŸ¥è®¾è®¡å¯¹æœªæ¥å¤šæ¨¡æ€é©¾é©¶å‘˜ç›‘æ§ç³»ç»Ÿçš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå•è§†å›¾ä¸åŒè§†å›¾æ£€æµ‹æ¨¡å‹çš„ç³»ç»Ÿæ¯”è¾ƒæä¾›äº†é¦–æ‰¹å®è¯è¯æ®ã€‚

---

#### ğŸ“„ Abstract
Despite increasing interest in computer vision-based distracted driving detection, most existing models rely exclusively on driver-facing views and overlook crucial environmental context that influences driving behavior. This study investigates whether incorporating road-facing views alongside driver-facing footage improves distraction detection accuracy in naturalistic driving conditions. Using synchronized dual-camera recordings from real-world driving, we benchmark three leading spatiotemporal action recognition architectures: SlowFast-R50, X3D-M, and SlowOnly-R50. Each model is evaluated under two input configurations: driver-only and stacked dual-view. Results show that while contextual inputs can improve detection in certain models, performance gains depend strongly on the underlying architecture. The single-pathway SlowOnly model achieved a 9.8 percent improvement with dual-view inputs, while the dual-pathway SlowFast model experienced a 7.2 percent drop in accuracy due to representational conflicts. These findings suggest that simply adding visual context is not sufficient and may lead to interference unless the architecture is specifically designed to support multi-view integration. This study presents one of the first systematic comparisons of single- and dual-view distraction detection models using naturalistic driving data and underscores the importance of fusion-aware design for future multimodal driver monitoring systems.


### [6] [Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark](https://arxiv.org/abs/2512.20174)
*Hao Guo, Xugong Qin, Jun Jie Ou Yang, Peng Zhang, Gangyan Zeng, Yubo Li, Hailun Lin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºè‡ªç„¶è¯­è¨€çš„æ–‡æ¡£å›¾åƒæ£€ç´¢ï¼ˆNL-DIRï¼‰åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å¼•å…¥è¯­ä¹‰ä¸°å¯Œçš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ¥è§£å†³ç°æœ‰æ–‡æ¡£å›¾åƒæ£€ç´¢æ–¹æ³•åœ¨ç»†ç²’åº¦è¯­ä¹‰æ£€ç´¢æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶æä¾›äº†åŒ…å«41KçœŸå®æ–‡æ¡£å›¾åƒå’Œé«˜è´¨é‡æŸ¥è¯¢çš„æ•°æ®é›†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–‡æ¡£å›¾åƒæ£€ç´¢ï¼ˆDIRï¼‰æ–¹æ³•ä¸»è¦åŸºäºå›¾åƒæŸ¥è¯¢ï¼Œåªèƒ½æ£€ç´¢ç›¸åŒç²—ç²’åº¦è¯­ä¹‰ç±»åˆ«ï¼ˆå¦‚æŠ¥çº¸æˆ–æ”¶æ®ï¼‰çš„æ–‡æ¡£ï¼Œä½†åœ¨ç°å®åœºæ™¯ä¸­é€šå¸¸æä¾›å…·æœ‰ç»†ç²’åº¦è¯­ä¹‰çš„æ–‡æœ¬æŸ¥è¯¢ï¼Œè¿™äº›æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ£€ç´¢ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œéœ€è¦å»ºç«‹èƒ½å¤Ÿå¤„ç†è‡ªç„¶è¯­è¨€æè¿°çš„ç»†ç²’åº¦è¯­ä¹‰æŸ¥è¯¢çš„æ–‡æ¡£å›¾åƒæ£€ç´¢åŸºå‡†ã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†è‡ªç„¶è¯­è¨€æ–‡æ¡£å›¾åƒæ£€ç´¢ï¼ˆNL-DIRï¼‰åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«41KçœŸå®æ–‡æ¡£å›¾åƒï¼Œæ¯ä¸ªå›¾åƒé…æœ‰äº”ä¸ªé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¹¶ç»äººå·¥éªŒè¯çš„é«˜è´¨é‡ç»†ç²’åº¦è¯­ä¹‰æŸ¥è¯¢ã€‚è¯„ä¼°äº†ç°æœ‰ä¸»æµå¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹å’Œæ— éœ€OCRçš„è§†è§‰æ–‡æ¡£ç†è§£æ¨¡å‹çš„é›¶æ ·æœ¬å’Œå¾®è°ƒæ€§èƒ½ï¼Œå¹¶è¿›ä¸€æ­¥ç ”ç©¶äº†ä¸¤é˜¶æ®µæ£€ç´¢æ–¹æ³•ä»¥æé«˜æ€§èƒ½åŒæ—¶ä¿æŒæ—¶é—´å’Œç©ºé—´æ•ˆç‡ã€‚

**Result:** NL-DIRæ•°æ®é›†åŒ…å«41KçœŸå®æ–‡æ¡£å›¾åƒï¼Œæ¯ä¸ªå›¾åƒé…æœ‰äº”ä¸ªé«˜è´¨é‡ç»†ç²’åº¦è¯­ä¹‰æŸ¥è¯¢ã€‚ç ”ç©¶å¯¹ç°æœ‰ä¸»æµæ¨¡å‹è¿›è¡Œäº†é›¶æ ·æœ¬å’Œå¾®è°ƒè¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†ä¸¤é˜¶æ®µæ£€ç´¢æ–¹æ³•åœ¨æ€§èƒ½æå‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶å®ç°äº†æ—¶é—´å’Œç©ºé—´æ•ˆç‡çš„å¹³è¡¡ã€‚

**Conclusion:** æå‡ºçš„NL-DIRåŸºå‡†æµ‹è¯•ä¸ºè§†è§‰æ–‡æ¡£ç†è§£ç¤¾åŒºå¸¦æ¥äº†æ–°çš„ç ”ç©¶æœºä¼šï¼Œé€šè¿‡å¼•å…¥è¯­ä¹‰ä¸°å¯Œçš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢è§£å†³äº†ç°æœ‰æ–‡æ¡£å›¾åƒæ£€ç´¢åœ¨ç»†ç²’åº¦è¯­ä¹‰åŒ¹é…æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥åŸºå‡†å°†ä¿ƒè¿›æ–‡æ¡£å›¾åƒæ£€ç´¢é¢†åŸŸçš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®åœºæ™¯ä¸­å¤„ç†å¤æ‚è¯­ä¹‰æŸ¥è¯¢çš„èƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.


### [7] [MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis](https://arxiv.org/abs/2512.20026)
*Ziwei Qin, Xuhui Song, Deqing Huang, Na Qin, Jun Li*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†å¤šæ¿€æ´»å¹³é¢äº¤äº’å›¾ç¥ç»ç½‘ç»œï¼ˆMAPI-GNNï¼‰ï¼Œé€šè¿‡ä»è¯­ä¹‰è§£ç¼ çš„ç‰¹å¾å­ç©ºé—´å­¦ä¹ å¤šé¢å›¾é…ç½®æ–‡ä»¶ï¼Œå…‹æœäº†ä¼ ç»Ÿå•é™æ€å›¾åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„å±€é™æ€§ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€åŒ»ç–—è¯Šæ–­çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºå›¾ç¥ç»ç½‘ç»œçš„å¤šæ¨¡æ€åŒ»ç–—è¯Šæ–­æ–¹æ³•ä¸»è¦ä¾èµ–äºä»éåŒºåˆ†æ€§ç‰¹å¾æ„å»ºçš„å•ä¸€é™æ€å›¾ï¼Œè¿™ç§èŒƒå¼é™åˆ¶äº†æ¨¡å‹å¯¹æ‚£è€…ç‰¹å¼‚æ€§ç—…ç†å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›ï¼Œå¯¼è‡´è¯Šæ–­æ•ˆæœå—é™ã€‚

**Method:** MAPI-GNNæ¡†æ¶é¦–å…ˆé€šè¿‡å¤šç»´åˆ¤åˆ«å™¨æ­ç¤ºæ½œåœ¨çš„å›¾æ„ŸçŸ¥æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼æŒ‡å¯¼åŠ¨æ€æ„å»ºä¸€ç³»åˆ—æ¿€æ´»å›¾ï¼Œæœ€ç»ˆé€šè¿‡å…³ç³»èåˆå¼•æ“èšåˆå’Œæƒ…å¢ƒåŒ–è¿™äº›å¤šé¢å›¾é…ç½®æ–‡ä»¶ä»¥å®ç°ç¨³å¥è¯Šæ–­ã€‚

**Result:** åœ¨ä¸¤ä¸ªå¤šæ ·åŒ–ä»»åŠ¡ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œæ¶µç›–è¶…è¿‡1300ä¸ªæ‚£è€…æ ·æœ¬ï¼Œè¡¨æ˜MAPI-GNNåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶å¤šé¢å›¾å»ºæ¨¡æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡è¯­ä¹‰è§£ç¼ ç‰¹å¾å­ç©ºé—´æ„å»ºå¤šé¢å›¾é…ç½®æ–‡ä»¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰æ‚£è€…ç‰¹å¼‚æ€§ç—…ç†å…³ç³»ï¼Œä¸ºå›¾ç¥ç»ç½‘ç»œåœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„åŠ¨æ€å›¾æ„å»ºèŒƒå¼ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠå®è·µæ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.


### [8] [$\text{H}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning](https://arxiv.org/abs/2512.20029)
*Lin Li, Jiahui Li, Jiaming Lei, Jun Xiao, Feifei Shao, Long Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºH2emæ¡†æ¶ï¼Œé€šè¿‡åŒæ›²å‡ ä½•åµŒå…¥è§£å†³ç»„åˆé›¶æ ·æœ¬å­¦ä¹ ä¸­çš„å±‚æ¬¡ç»“æ„å»ºæ¨¡é—®é¢˜ï¼Œåˆ©ç”¨åŒæ›²ç©ºé—´çš„æŒ‡æ•°ä½“ç§¯å¢é•¿ç‰¹æ€§åŒ¹é…ç»„åˆè¯­ä¹‰çš„æŒ‡æ•°ç»“æ„ï¼Œåœ¨å°é—­ä¸–ç•Œå’Œå¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­å‡è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç»„åˆé›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•é€šå¸¸å¿½ç•¥ä¸°å¯Œçš„å±‚æ¬¡ç»“æ„ï¼Œå¦‚åŸºå…ƒçš„è¯­ä¹‰å±‚æ¬¡å’ŒåŸºå…ƒä¸ç»„åˆä¹‹é—´çš„æ¦‚å¿µå±‚æ¬¡ã€‚ç°æœ‰æ–¹æ³•åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­é€šè¿‡æŸå¤±æ­£åˆ™åŒ–å»ºæ¨¡è¿™äº›å±‚æ¬¡ï¼Œä½†æ— æ³•æ‰©å±•åˆ°ç°å®ä¸–ç•ŒCZSLæ‰€éœ€çš„å¤§è§„æ¨¡åˆ†ç±»ä½“ç³»ï¼Œå› ä¸ºæ¬§å‡ é‡Œå¾—ç©ºé—´çš„å¤šé¡¹å¼ä½“ç§¯å¢é•¿æ— æ³•åŒ¹é…ç»„åˆè¯­ä¹‰çš„æŒ‡æ•°ç»“æ„ï¼Œä»è€ŒæŸå®³æ³›åŒ–èƒ½åŠ›ã€‚

**Method:** æå‡ºH2emæ¡†æ¶ï¼Œåˆ©ç”¨åŒæ›²å‡ ä½•çš„è‡ªç„¶ç‰¹æ€§åµŒå…¥æ ‘çŠ¶ç»“æ„ã€‚è®¾è®¡åŒé‡å±‚æ¬¡è•´å«æŸå¤±ï¼Œä½¿ç”¨åŒæ›²è•´å«é”¥å¼ºåˆ¶æ‰§è¡Œé¢„å®šä¹‰çš„å±‚æ¬¡ç»“æ„ï¼›è®¾è®¡åˆ¤åˆ«å¯¹é½æŸå¤±ä¸å›°éš¾è´Ÿæ ·æœ¬æŒ–æ˜ï¼Œåœ¨è¯­ä¹‰ç›¸ä¼¼çš„ç»„åˆä¹‹é—´å»ºç«‹è¾ƒå¤§çš„æµ‹åœ°è·ç¦»ï¼›å¼€å‘åŒæ›²è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨åŒæ›²å‡ ä½•å†…å®ç°å®ä¾‹æ„ŸçŸ¥çš„è·¨æ¨¡æ€èåˆã€‚

**Result:** åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›æ¶ˆèå®éªŒè¡¨æ˜ï¼ŒH2emåœ¨å°é—­ä¸–ç•Œå’Œå¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­å‡å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº†å±‚æ¬¡å´©æºƒå’Œç»†ç²’åº¦åŒºåˆ†ä¸è¶³çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†ç»„åˆé›¶æ ·æœ¬å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜åŒæ›²å‡ ä½•ä¸ºç»„åˆé›¶æ ·æœ¬å­¦ä¹ çš„å±‚æ¬¡ç»“æ„å»ºæ¨¡æä¾›äº†è‡ªç„¶ä¸”æœ‰æ•ˆçš„æ•°å­¦æ¡†æ¶ï¼Œå…¶æŒ‡æ•°ä½“ç§¯å¢é•¿ç‰¹æ€§èƒ½å¤ŸåŒ¹é…ç»„åˆè¯­ä¹‰çš„æŒ‡æ•°ç»“æ„ã€‚è¯¥æ–¹æ³•ä¸ºå¤§è§„æ¨¡åˆ†ç±»ä½“ç³»ä¸‹çš„ç»„åˆå­¦ä¹ å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œè¯æ˜äº†åŒæ›²ç©ºé—´åœ¨å¤æ‚è¯­ä¹‰å…³ç³»å»ºæ¨¡ä¸­çš„ä¼˜åŠ¿ã€‚

---

#### ğŸ“„ Abstract
Compositional zero-shot learning (CZSL) aims to recognize unseen state-object compositions by generalizing from a training set of their primitives (state and object). Current methods often overlook the rich hierarchical structures, such as the semantic hierarchy of primitives (e.g., apple fruit) and the conceptual hierarchy between primitives and compositions (e.g, sliced apple apple). A few recent efforts have shown effectiveness in modeling these hierarchies through loss regularization within Euclidean space. In this paper, we argue that they fail to scale to the large-scale taxonomies required for real-world CZSL: the space's polynomial volume growth in flat geometry cannot match the exponential structure, impairing generalization capacity. To this end, we propose H2em, a new framework that learns Hierarchical Hyperbolic EMbeddings for CZSL. H2em leverages the unique properties of hyperbolic geometry, a space naturally suited for embedding tree-like structures with low distortion. However, a naive hyperbolic mapping may suffer from hierarchical collapse and poor fine-grained discrimination. We further design two learning objectives to structure this space: a Dual-Hierarchical Entailment Loss that uses hyperbolic entailment cones to enforce the predefined hierarchies, and a Discriminative Alignment Loss with hard negative mining to establish a large geodesic distance between semantically similar compositions. Furthermore, we devise Hyperbolic Cross-Modal Attention to realize instance-aware cross-modal infusion within hyperbolic geometry. Extensive ablations on three benchmarks demonstrate that H2em establishes a new state-of-the-art in both closed-world and open-world scenarios. Our codes will be released.


### [9] [Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva](https://arxiv.org/abs/2512.20042)
*Nguyen Lam Phu Quy, Pham Phu Hoa, Tran Chi Nguyen, Dao Sy Duy Minh, Nguyen Hoang Minh Ngoc, Huynh Trung Kiet*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€ç®¡é“ï¼Œé€šè¿‡æ•´åˆå¤–éƒ¨æ–‡æœ¬çŸ¥è¯†æ¥å¢å¼ºå›¾åƒæè¿°ï¼Œç”ŸæˆåŒ…å«äº‹ä»¶èƒŒæ™¯ã€æ—¶é—´çº¿ç´¢å’Œå‘½åå®ä½“ç­‰ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„æè¿°ï¼Œæ˜¾è‘—æå‡äº†ä¼ ç»Ÿå›¾åƒæè¿°æ–¹æ³•çš„æ·±åº¦å’Œä¿¡æ¯é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°å®ä¸–ç•Œä¸­çš„å›¾åƒæè¿°é€šå¸¸ç¼ºä¹ä¸Šä¸‹æ–‡æ·±åº¦ï¼Œå¿½ç•¥äº†äº‹ä»¶èƒŒæ™¯ã€æ—¶é—´çº¿ç´¢ã€ç»“æœå’Œä¸å¯è§†çš„å‘½åå®ä½“ç­‰å…³é”®ç»†èŠ‚ï¼Œè¿™é™åˆ¶äº†å›¾åƒç†è§£åœ¨æ–°é—»ã€æ•™è‚²å’Œæ•°å­—æ¡£æ¡ˆç­‰é¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œè¿™äº›é¢†åŸŸéœ€è¦æ›´ä¸°å¯Œã€ä¿¡æ¯é‡æ›´å¤§çš„æè¿°ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨å¤šæ¨¡æ€ç®¡é“ï¼Œä½¿ç”¨BEIT-3ï¼ˆFlickr30k-384å’ŒCOCO-384ï¼‰å’ŒSigLIP So-384æ£€ç´¢è¯­ä¹‰ç›¸ä¼¼å›¾åƒï¼Œé€šè¿‡ORBå’ŒSIFTè¿›è¡Œå‡ ä½•å¯¹é½é‡æ’åºï¼Œå¹¶é€šè¿‡è¯­ä¹‰æœç´¢ä»ç›¸å…³æ–‡ç« ä¸­æå–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç„¶åä½¿ç”¨QLoRAå¾®è°ƒçš„Qwen3æ¨¡å‹å°†ä¸Šä¸‹æ–‡ä¸Instruct BLIPï¼ˆVicuna-7Bï¼‰ç”Ÿæˆçš„åŸºç¡€æè¿°æ•´åˆï¼Œç”Ÿæˆäº‹ä»¶ä¸°å¯Œã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æè¿°ã€‚

**Result:** åœ¨OpenEvents v1æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„æè¿°ä¿¡æ¯é‡æ˜¾è‘—æ›´å¤§ï¼Œæ˜¾ç¤ºå‡ºåœ¨éœ€è¦æ›´æ·±å±‚æ¬¡è§†è§‰-æ–‡æœ¬ç†è§£çš„å®é™…åº”ç”¨ä¸­å…·æœ‰å¼ºå¤§æ½œåŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ•´åˆå¤–éƒ¨æ–‡æœ¬çŸ¥è¯†å¯¹äºå¢å¼ºå›¾åƒæè¿°ä¸Šä¸‹æ–‡æ·±åº¦çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ–°é—»ã€æ•™è‚²å’Œæ•°å­—æ¡£æ¡ˆç­‰é¢†åŸŸçš„å®é™…åº”ç”¨æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å¤šæ¨¡æ€çŸ¥è¯†æ•´åˆåœ¨æå‡è§†è§‰ç†è§£èƒ½åŠ›æ–¹é¢çš„ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding


### [10] [Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts](https://arxiv.org/abs/2512.20088)
*Jinyoung Choi, Youngchae Kwon, Injung Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç‰©å“åŒºåŸŸçš„æ—¶å°šé£æ ¼åˆ†ç±»ç½‘ç»œï¼ˆIRSNï¼‰ï¼Œé€šè¿‡åˆ†æç‰©å“ç‰¹å®šç‰¹å¾åŠå…¶ç»„åˆæ¥æ”¹è¿›æ—¶å°šé£æ ¼åˆ†ç±»ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç‰©å“åŒºåŸŸæ± åŒ–ã€é—¨æ§ç‰¹å¾èåˆå’ŒåŒéª¨å¹²æ¶æ„ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†åˆ†ç±»å‡†ç¡®ç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ—¶å°šé£æ ¼åˆ†ç±»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šåŒä¸€é£æ ¼å†…å­˜åœ¨è¾ƒå¤§çš„è§†è§‰å·®å¼‚ï¼Œä»¥åŠä¸åŒé£æ ¼ä¹‹é—´å¯èƒ½å­˜åœ¨è§†è§‰ç›¸ä¼¼æ€§ã€‚é£æ ¼ä¸ä»…ç”±æ•´ä½“å¤–è§‚è¡¨è¾¾ï¼Œè¿˜å–å†³äºå•ä¸ªç‰©å“çš„å±æ€§åŠå…¶ç»„åˆæ–¹å¼ï¼Œå› æ­¤éœ€è¦åŒæ—¶è€ƒè™‘å…¨å±€ç‰¹å¾å’Œç‰©å“çº§ç‰¹å¾ã€‚

**Method:** IRSNé‡‡ç”¨ç‰©å“åŒºåŸŸæ± åŒ–ï¼ˆIRPï¼‰æå–æ¯ä¸ªç‰©å“åŒºåŸŸçš„ç‰¹å¾ï¼Œåˆ†åˆ«è¿›è¡Œåˆ†æï¼Œç„¶åé€šè¿‡é—¨æ§ç‰¹å¾èåˆï¼ˆGFFï¼‰è¿›è¡Œç»„åˆã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨åŒéª¨å¹²æ¶æ„ï¼Œç»“åˆäº†é¢†åŸŸç‰¹å®šç‰¹å¾æå–å™¨å’Œåœ¨å¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„é€šç”¨ç‰¹å¾æå–å™¨ï¼Œä»¥å¢å¼ºç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚

**Result:** åœ¨FashionStyle14å’ŒShowniqV3æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒIRSNåº”ç”¨äºåŒ…æ‹¬EfficientNetã€ConvNeXtå’ŒSwin Transformeråœ¨å†…çš„å…­ç§éª¨å¹²ç½‘ç»œæ—¶ï¼Œå¹³å‡åˆ†åˆ«æå‡äº†6.9%å’Œ7.6%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œæœ€å¤§æå‡åˆ†åˆ«è¾¾åˆ°14.5%å’Œ15.1%ã€‚å¯è§†åŒ–åˆ†æè¿›ä¸€æ­¥è¯å®IRSNæ¨¡å‹èƒ½æ›´å¥½åœ°æ•æ‰ç›¸ä¼¼é£æ ¼ç±»åˆ«ä¹‹é—´çš„å·®å¼‚ã€‚

**Conclusion:** IRSNé€šè¿‡åŒæ—¶å»ºæ¨¡å…¨å±€ç‰¹å¾ã€ç‰©å“çº§ç‰¹å¾åŠå…¶ç»„åˆå…³ç³»ï¼Œæœ‰æ•ˆè§£å†³äº†æ—¶å°šé£æ ¼åˆ†ç±»ä¸­çš„è§†è§‰å˜å¼‚å’Œç›¸ä¼¼æ€§æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ä¸ºç»†ç²’åº¦æ—¶å°šåˆ†ææä¾›äº†æ–°çš„æŠ€æœ¯æ¡†æ¶ï¼Œå…¶åŒéª¨å¹²æ¶æ„å’Œé—¨æ§èåˆæœºåˆ¶å¯æ¨å¹¿åˆ°å…¶ä»–éœ€è¦å¤šå±‚æ¬¡ç‰¹å¾å»ºæ¨¡çš„è§†è§‰ä»»åŠ¡ä¸­ã€‚

---

#### ğŸ“„ Abstract
Fashion style classification is a challenging task because of the large visual variation within the same style and the existence of visually similar styles.
  Styles are expressed not only by the global appearance, but also by the attributes of individual items and their combinations.
  In this study, we propose an item region-based fashion style classification network (IRSN) to effectively classify fashion styles by analyzing item-specific features and their combinations in addition to global features.
  IRSN extracts features of each item region using item region pooling (IRP), analyzes them separately, and combines them using gated feature fusion (GFF).
  In addition, we improve the feature extractor by applying a dual-backbone architecture that combines a domain-specific feature extractor and a general feature extractor pre-trained with a large-scale image-text dataset.
  In experiments, applying IRSN to six widely-used backbones, including EfficientNet, ConvNeXt, and Swin Transformer, improved style classification accuracy by an average of 6.9% and a maximum of 14.5% on the FashionStyle14 dataset and by an average of 7.6% and a maximum of 15.1% on the ShowniqV3 dataset. Visualization analysis also supports that the IRSN models are better than the baseline models at capturing differences between similar style classes.


### [11] [DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation](https://arxiv.org/abs/2512.20117)
*Jingqi Tian, Yiheng Du, Haoji Zhang, Yuji Wang, Isaac Ning Lee, Xulong Bai, Tianrui Zhu, Jingxuan Niu, Yansong Tang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºDDAVSæ¡†æ¶ï¼Œé€šè¿‡è§£è€¦éŸ³é¢‘è¯­ä¹‰ä¸å»¶è¿ŸåŒå‘å¯¹é½æœºåˆ¶ï¼Œè§£å†³äº†éŸ³é¢‘-è§†è§‰åˆ†å‰²ä¸­çš„å¤šæºçº ç¼ å’Œè§†å¬é”™ä½é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éŸ³é¢‘-è§†è§‰åˆ†å‰²ä»»åŠ¡æ—¨åœ¨é€šè¿‡ç»“åˆå¬è§‰å’Œè§†è§‰ä¿¡æ¯åœ¨åƒç´ çº§åˆ«å®šä½å‘å£°ç‰©ä½“ï¼Œä½†ç°æœ‰æ–¹æ³•å¸¸å—å¤šæºçº ç¼ å’Œè§†å¬é”™ä½é—®é¢˜çš„å›°æ‰°ï¼Œå¯¼è‡´æ¨¡å‹åå‘äºæ›´å“äº®æˆ–æ›´å¤§çš„ç‰©ä½“ï¼Œè€Œå¿½ç•¥è¾ƒå¼±ã€è¾ƒå°æˆ–å…±ç°çš„å£°æºã€‚

**Method:** DDAVSæ¡†æ¶é‡‡ç”¨è§£è€¦éŸ³é¢‘è¯­ä¹‰å’Œå»¶è¿ŸåŒå‘å¯¹é½æœºåˆ¶ï¼Œé€šè¿‡å¯å­¦ä¹ æŸ¥è¯¢ä»éŸ³é¢‘åŸå‹è®°å¿†åº“ä¸­æå–éŸ³é¢‘è¯­ä¹‰å¹¶å°†å…¶é”šå®šåœ¨ç»“æ„åŒ–è¯­ä¹‰ç©ºé—´ä¸­ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ å¢å¼ºåˆ¤åˆ«æ€§å’Œé²æ£’æ€§ï¼ŒåŒæ—¶å¼•å…¥å…·æœ‰å»¶è¿Ÿæ¨¡æ€äº¤äº’çš„åŒé‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ¥æ”¹å–„å¤šæ¨¡æ€å¯¹é½çš„é²æ£’æ€§ã€‚

**Result:** åœ¨AVS-Objectså’ŒVPOåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDDAVSåœ¨å•æºã€å¤šæºå’Œå¤šå®ä¾‹åœºæ™¯ä¸­å‡ä¸€è‡´ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•ŒéŸ³é¢‘-è§†è§‰åˆ†å‰²æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è§£è€¦éŸ³é¢‘è¯­ä¹‰å’Œå»¶è¿ŸåŒå‘å¯¹é½æœºåˆ¶å¯ä»¥æœ‰æ•ˆè§£å†³éŸ³é¢‘-è§†è§‰åˆ†å‰²ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸ºå¤„ç†å¤æ‚å¤šæºåœºæ™¯æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå±•ç¤ºäº†åœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹å®ç°é²æ£’å¤šæ¨¡æ€åˆ†å‰²çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Audio-Visual Segmentation (AVS) aims to localize sound-producing objects at the pixel level by jointly leveraging auditory and visual information. However, existing methods often suffer from multi-source entanglement and audio-visual misalignment, which lead to biases toward louder or larger objects while overlooking weaker, smaller, or co-occurring sources. To address these challenges, we propose DDAVS, a Disentangled Audio Semantics and Delayed Bidirectional Alignment framework. To mitigate multi-source entanglement, DDAVS employs learnable queries to extract audio semantics and anchor them within a structured semantic space derived from an audio prototype memory bank. This is further optimized through contrastive learning to enhance discriminability and robustness. To alleviate audio-visual misalignment, DDAVS introduces dual cross-attention with delayed modality interaction, improving the robustness of multimodal alignment. Extensive experiments on the AVS-Objects and VPO benchmarks demonstrate that DDAVS consistently outperforms existing approaches, exhibiting strong performance across single-source, multi-source, and multi-instance scenarios. These results validate the effectiveness and generalization ability of our framework under challenging real-world audio-visual segmentation conditions. Project page: https://trilarflagz.github.io/DDAVS-page/


### [12] [LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation](https://arxiv.org/abs/2512.20217)
*Xiangxuan Ren, Zhongdao Wang, Pin Tang, Guoqing Wang, Jilai Zheng, Chao Ma*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºLiteFusionï¼Œä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€3Dç›®æ ‡æ£€æµ‹å™¨ï¼Œé€šè¿‡å°†LiDARæ•°æ®ä½œä¸ºå‡ ä½•ä¿¡æ¯çš„è¡¥å……æºæ¥å¢å¼ºåŸºäºæ‘„åƒå¤´çš„æ£€æµ‹ï¼Œå®Œå…¨æ¶ˆé™¤äº†å¯¹3Dä¸»å¹²ç½‘ç»œçš„ä¾èµ–ï¼Œä»è€Œæé«˜äº†éƒ¨ç½²å‹å¥½æ€§å’Œé²æ£’æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€3Dç›®æ ‡æ£€æµ‹å™¨ä¸¥é‡ä¾èµ–LiDARä¼ æ„Ÿå™¨ï¼Œåœ¨LiDARç¼ºå¤±æ—¶æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œä¸”ç”±äºä¾èµ–ä¸»è¦é’ˆå¯¹NVIDIA GPUä¼˜åŒ–çš„3Dç¨€ç–å·ç§¯ç®—å­ï¼Œéš¾ä»¥éƒ¨ç½²åˆ°NPUå’ŒFPGAç­‰å¤šæ ·åŒ–ç¡¬ä»¶å¹³å°ä¸Šï¼Œè¿™å½±å“äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å®é™…åœºæ™¯ä¸­çš„é²æ£’æ€§å’Œå®‰å…¨æ€§ã€‚

**Method:** LiteFusioné‡æ–°æ€è€ƒäº†LiDARåœ¨æ‘„åƒå¤´-LiDARèåˆèŒƒå¼ä¸­çš„ä½œç”¨ï¼Œå°†LiDARæ•°æ®ä½œä¸ºå‡ ä½•ä¿¡æ¯çš„è¡¥å……æºæ¥å¢å¼ºåŸºäºæ‘„åƒå¤´çš„æ£€æµ‹ï¼Œè€Œéå°†å…¶è§†ä¸ºå…·æœ‰ç‹¬ç«‹ç‰¹å¾æå–ä¸»å¹²ç½‘ç»œçš„ç‹¬ç«‹æ¨¡æ€ã€‚è¯¥æ–¹æ³•åœ¨å››å…ƒæ•°ç©ºé—´ä¸­é›†æˆLiDARç‚¹åˆ°å›¾åƒç‰¹å¾ä¸­ï¼Œå…¶ä¸­æ­£äº¤çº¦æŸåœ¨ç½‘ç»œè®­ç»ƒæœŸé—´å¾—åˆ°è‰¯å¥½ä¿æŒï¼Œæœ‰åŠ©äºå»ºæ¨¡è·¨æ¨¡æ€çš„é¢†åŸŸç‰¹å®šå…³ç³»ï¼Œäº§ç”Ÿç´§å‡‘çš„è·¨æ¨¡æ€åµŒå…¥ã€‚

**Result:** åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLiteFusionå°†åŸºäºè§†è§‰çš„åŸºçº¿æ£€æµ‹å™¨çš„mAPæé«˜äº†+20.4%ï¼ŒNDSæé«˜äº†+19.7%ï¼Œè€Œå‚æ•°ä»…å¢åŠ 1.1%ï¼Œä¸”æœªä½¿ç”¨ä¸“ç”¨çš„LiDARç¼–ç å™¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿åœ¨LiDARè¾“å…¥ç¼ºå¤±çš„æƒ…å†µä¸‹ï¼ŒLiteFusionä»èƒ½ä¿æŒå¼ºåŠ²ç»“æœï¼Œçªæ˜¾äº†å…¶åœ¨ä¸åŒèåˆèŒƒå¼å’Œéƒ¨ç½²åœºæ™¯ä¸­çš„è‰¯å¥½é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡é‡æ–°æ€è€ƒLiDARåœ¨å¤šæ¨¡æ€èåˆä¸­çš„ä½œç”¨ï¼Œæå‡ºäº†ä¸€ç§éƒ¨ç½²å‹å¥½ä¸”é²æ£’çš„3Dç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œå®Œå…¨æ¶ˆé™¤äº†å¯¹3Dä¸»å¹²ç½‘ç»œçš„ä¾èµ–ï¼Œä¸ºå®é™…è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„éƒ¨ç½²æä¾›äº†æ›´çµæ´»çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶åœ¨ä¸åŒç¡¬ä»¶å¹³å°ä¸Šå…·æœ‰æ›´å¥½çš„é€‚åº”æ€§ã€‚

---

#### ğŸ“„ Abstract
3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.


### [13] [LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation](https://arxiv.org/abs/2512.20257)
*Daniele Cardullo, Simone Teglia, Irene Amerini*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†LADLE-MMï¼Œä¸€ç§åœ¨æœ‰é™æ ‡æ³¨å’Œè®­ç»ƒèµ„æºä¸‹å·¥ä½œçš„å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹å™¨ï¼Œé€šè¿‡æ¨¡å‹é›†æˆåˆå§‹åŒ–æ–¹æ³•ï¼Œåœ¨å‡å°‘60.3%å¯è®­ç»ƒå‚æ•°çš„åŒæ—¶ï¼Œåœ¨DGM4å’ŒVERITEåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç«äº‰çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€å¤šåª’ä½“å†…å®¹ç”Ÿæˆå’Œæ“çºµå·¥å…·çš„æ™®åŠï¼Œè·¨å¤šæ¨¡æ€çš„é€¼çœŸåˆæˆç¯¡æ”¹å·²æˆä¸ºå¹¿æ³›å¨èƒï¼Œå¸¸è¢«ç”¨äºæ‰­æ›²é‡è¦äº‹ä»¶å™äº‹å’Œä¼ æ’­è™šå‡ä¿¡æ¯ã€‚ç°æœ‰çš„å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹æ–¹æ³•é€šå¸¸ä¾èµ–è®¡ç®—å¯†é›†å‹æ¶æ„æˆ–éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„åº”ç”¨ã€‚

**Method:** LADLE-MMé‡‡ç”¨æ¨¡å‹é›†æˆåˆå§‹åŒ–æ–¹æ³•ï¼ŒåŒ…å«ä¸¤ä¸ªå•æ¨¡æ€åˆ†æ”¯å’Œä¸€ä¸ªå¤šæ¨¡æ€åˆ†æ”¯ã€‚å¤šæ¨¡æ€åˆ†æ”¯é€šè¿‡ä»BLIPæ¨¡å‹ä¸­æå–çš„å›ºå®šå¤šæ¨¡æ€åµŒå…¥æ¥å¢å¼ºå›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºï¼Œè¿™äº›åµŒå…¥ä½œä¸ºå‚è€ƒç©ºé—´ã€‚è¯¥æ–¹æ³•åœ¨æœ‰é™æ ‡æ³¨è®¾ç½®ä¸‹å·¥ä½œï¼Œæ˜¾è‘—å‡å°‘äº†æ¨¡å‹å¤æ‚åº¦ã€‚

**Result:** åœ¨DGM4åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLADLE-MMåœ¨äºŒè¿›åˆ¶å’Œå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸Šå‡å–å¾—ç«äº‰æ€§æ€§èƒ½ï¼Œå°½ç®¡å¯è®­ç»ƒå‚æ•°æ¯”å…ˆå‰æœ€å…ˆè¿›æ¨¡å‹å‡å°‘60.3%ã€‚åœ¨æ²¡æœ‰åŸºç¡€æ ‡æ³¨çš„æƒ…å†µä¸‹è®­ç»ƒæ—¶ï¼Œå®ƒè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚åœ¨VERITEæ•°æ®é›†ä¸Šï¼ŒLADLE-MMè¶…è¶Šäº†ä½¿ç”¨æ›´å¤æ‚å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¶æ„çš„å½“å‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨å¼€æ”¾é›†è®¾ç½®ä¸­çš„æœ‰æ•ˆæ³›åŒ–èƒ½åŠ›å’Œå¯¹å•æ¨¡æ€åè§çš„å¼ºé²æ£’æ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¨¡å‹é›†æˆåˆå§‹åŒ–å’Œå¤šæ¨¡æ€è¡¨ç¤ºå¢å¼ºï¼Œå¯ä»¥åœ¨æ˜¾è‘—å‡å°‘å‚æ•°æ•°é‡çš„æƒ…å†µä¸‹å®ç°å¼ºå¤§çš„è™šå‡ä¿¡æ¯æ£€æµ‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„å¤šæ¨¡æ€å†…å®¹éªŒè¯æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨å¼€æ”¾é›†åœºæ™¯ä¸­çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œå¯¹å®é™…éƒ¨ç½²å…·æœ‰é‡è¦æ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.


### [14] [TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation](https://arxiv.org/abs/2512.20296)
*Ji-Hoon Kim, Junseok Ahn, Doyeop Kwak, Joon Son Chung, Shinji Watanabe*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†TAVIDï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬å’Œå‚è€ƒå›¾åƒä¸­åŒæ­¥ç”Ÿæˆäº¤äº’å¼é¢éƒ¨è§†é¢‘å’Œå¯¹è¯è¯­éŸ³ï¼Œè§£å†³äº†ç°æœ‰ç ”ç©¶ä¸­è§†å¬æ¨¡æ€åˆ†ç¦»çš„é—®é¢˜ï¼Œå®ç°äº†æ›´è‡ªç„¶çš„äººç±»å¯¹è¯æ¨¡æ‹Ÿã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç ”ç©¶é€šå¸¸å­¤ç«‹åœ°æ¢ç´¢è¯´è¯å¤´ç”Ÿæˆæˆ–å¯¹è¯è¯­éŸ³ç”Ÿæˆï¼Œå¿½ç•¥äº†äººç±»å¯¹è¯ä¸­ç´§å¯†è€¦åˆçš„è§†å¬äº¤äº’ç‰¹æ€§ï¼Œè¿™é™åˆ¶äº†æ„å»ºç±»äººå¯¹è¯ç³»ç»Ÿçš„èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤ŸåŒæ­¥ç”Ÿæˆäº¤äº’å¼é¢éƒ¨å’Œå¯¹è¯è¯­éŸ³çš„ç»Ÿä¸€æ¡†æ¶ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†TAVIDæ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªè·¨æ¨¡æ€æ˜ å°„å™¨ï¼ˆè¿åŠ¨æ˜ å°„å™¨å’Œè¯´è¯è€…æ˜ å°„å™¨ï¼‰æ•´åˆé¢éƒ¨å’Œè¯­éŸ³ç”Ÿæˆæµç¨‹ï¼Œå®ç°éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€ä¹‹é—´çš„åŒå‘ä¿¡æ¯äº¤æ¢ï¼Œä»è€ŒåŒæ­¥ç”Ÿæˆäº¤äº’å¼é¢éƒ¨å’Œå¯¹è¯è¯­éŸ³ã€‚

**Result:** å®éªŒåœ¨å››ä¸ªç»´åº¦ä¸Šè¯„ä¼°ç³»ç»Ÿæ€§èƒ½ï¼šè¯´è¯é¢éƒ¨çœŸå®æ€§ã€å€¾å¬å¤´éƒ¨å“åº”æ€§ã€äºŒå…ƒäº¤äº’æµç•…æ€§å’Œè¯­éŸ³è´¨é‡ï¼Œå¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ‰€æœ‰æ–¹é¢å‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼Œå®ç°äº†é«˜è´¨é‡çš„è§†å¬åŒæ­¥ç”Ÿæˆã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†ç»Ÿä¸€è§†å¬ç”Ÿæˆæ¡†æ¶åœ¨æ„å»ºç±»äººå¯¹è¯ç³»ç»Ÿä¸­çš„é‡è¦æ€§ï¼Œé€šè¿‡è·¨æ¨¡æ€ä¿¡æ¯äº¤æ¢å®ç°äº†æ›´è‡ªç„¶çš„äº¤äº’ä½“éªŒï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿçš„å‘å±•æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’Œè¯„ä¼°æ ‡å‡†ã€‚

---

#### ğŸ“„ Abstract
The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.


### [15] [CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation](https://arxiv.org/abs/2512.20362)
*V. Kovalev, A. Kuvshinov, A. Buzovkin, D. Pokidov, D. Timonin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CRAFTæ¡†æ¶ï¼Œä¸€ç§æ— éœ€è®­ç»ƒã€æ¨¡å‹æ— å…³çš„æ¨ç†æ—¶é—´ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–æ¨ç†å’Œçº¦æŸé©±åŠ¨åé¦ˆæ¥æå‡å¤šæ¨¡æ€å›¾åƒç”Ÿæˆçš„å¯é æ€§å’Œå¯æ§æ€§ã€‚è¯¥æ¡†æ¶å°†æç¤ºåˆ†è§£ä¸ºä¾èµ–ç»“æ„åŒ–çš„è§†è§‰é—®é¢˜ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹éªŒè¯ç”Ÿæˆå›¾åƒï¼Œå¹¶é€šè¿‡LLMä»£ç†è¿›è¡Œé’ˆå¯¹æ€§æç¤ºç¼–è¾‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ¨ç†æ—¶é—´ä¼˜åŒ–æ–¹æ³•é€šå¸¸ä¾èµ–éšå¼ã€æ•´ä½“çš„æ‰¹è¯„æˆ–æ— çº¦æŸçš„æç¤ºé‡å†™ï¼Œå¯¼è‡´å…¶è¡Œä¸ºéš¾ä»¥è§£é‡Šã€æ§åˆ¶æˆ–å¯é åœæ­¢ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å·²ä»åŸºäºéªŒè¯ã€é’ˆå¯¹æ€§ä¿®æ­£å’Œæ—©æœŸåœæ­¢çš„æ˜¾å¼ç»“æ„åŒ–æ€ç»´å½¢å¼ä¸­å—ç›Šï¼Œè€Œå¤šæ¨¡æ€å›¾åƒç”Ÿæˆé¢†åŸŸç¼ºä¹ç±»ä¼¼çš„ç³»ç»ŸåŒ–æ¨ç†æ¡†æ¶ã€‚

**Method:** CRAFTæ¡†æ¶å°†æç¤ºåˆ†è§£ä¸ºä¾èµ–ç»“æ„åŒ–çš„è§†è§‰é—®é¢˜ï¼Œä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹éªŒè¯ç”Ÿæˆå›¾åƒï¼Œå¹¶é€šè¿‡LLMä»£ç†åœ¨çº¦æŸå¤±è´¥å¤„åº”ç”¨é’ˆå¯¹æ€§æç¤ºç¼–è¾‘ã€‚è¯¥è¿‡ç¨‹åœ¨æ»¡è¶³æ‰€æœ‰çº¦æŸæ¡ä»¶æ—¶é‡‡ç”¨æ˜¾å¼åœæ­¢å‡†åˆ™è¿›è¡Œè¿­ä»£ï¼Œå½¢æˆä¸€ä¸ªå¯è§£é‡Šä¸”å¯æ§çš„æ¨ç†æ—¶é—´ä¼˜åŒ–å¾ªç¯ï¼Œæ— éœ€é¢å¤–è®­ç»ƒä¸”ä¸æ¨¡å‹æ— å…³ã€‚

**Result:** åœ¨å¤šä¸ªæ¨¡å‹ç³»åˆ—å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCRAFTæŒç»­æå‡äº†ç»„åˆå‡†ç¡®æ€§ã€æ–‡æœ¬æ¸²æŸ“å’ŒåŸºäºåå¥½çš„è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨è½»é‡çº§ç”Ÿæˆå™¨ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™äº›æ”¹è¿›ä»…å¸¦æ¥å¯å¿½ç•¥çš„æ¨ç†æ—¶é—´å¼€é”€ï¼Œä½¿è¾ƒå°æˆ–è¾ƒä¾¿å®œçš„æ¨¡å‹èƒ½å¤Ÿæ¥è¿‘æ›´æ˜‚è´µç³»ç»Ÿçš„è´¨é‡æ°´å¹³ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ˜¾å¼ç»“æ„åŒ–ã€çº¦æŸé©±åŠ¨çš„æ¨ç†æ—¶é—´ä¼˜åŒ–æ˜¯æå‡å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹å¯é æ€§çš„å…³é”®è¦ç´ ã€‚CRAFTæ¡†æ¶ä¸ºå›¾åƒç”Ÿæˆæä¾›äº†å¯è§£é‡Šã€å¯æ§çš„æ¨ç†å¾ªç¯ï¼Œä½¿èµ„æºå—é™çš„æ¨¡å‹èƒ½å¤Ÿé€šè¿‡æ™ºèƒ½æ¨ç†è¾¾åˆ°æ›´é«˜è´¨é‡çš„è¾“å‡ºï¼Œä¸ºæœªæ¥ç”Ÿæˆå¼AIç³»ç»Ÿçš„å¯é æ€§å’Œæ•ˆç‡ä¼˜åŒ–æä¾›äº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.
  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.
  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.


### [16] [Chain-of-Anomaly Thoughts with Large Vision-Language Models](https://arxiv.org/abs/2512.20417)
*Pedro Domingos, JoÃ£o Pereira, Vasco Lopes, JoÃ£o Neves, David Semedo*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Chain-of-Anomaly-Thoughts (CoAT)æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å½’çº³æ€§çŠ¯ç½ªåç½®çš„å¤šæ™ºèƒ½ä½“æ¨ç†æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç›‘æ§ä¸­çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–è§†é¢‘ç›‘æ§ä¸­å­˜åœ¨å›ºæœ‰çš„æ­£å¸¸æ€§åç½®ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆæ£€æµ‹çŠ¯ç½ªè¡Œä¸ºï¼Œè€Œç°æœ‰çš„æ€ç»´é“¾æ¨ç†ç­–ç•¥ç”±äºç¼ºä¹å½’çº³æ€§å¼‚å¸¸åç½®ï¼Œè¿›ä¸€æ­¥å°†æ¨¡å‹å¯¼å‘æ­£å¸¸è§£é‡Šï¼Œè¿™é™åˆ¶äº†å¼‚å¸¸æ£€æµ‹çš„å®é™…åº”ç”¨æ•ˆæœã€‚

**Method:** æœ¬æ–‡æå‡ºäº†Chain-of-Anomaly-Thoughts (CoAT)å¤šæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥å½’çº³æ€§çŠ¯ç½ªåç½®ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªä¸“æ³¨äºå¼‚å¸¸æ£€æµ‹çš„æœ€ç»ˆåˆ†ç±»å±‚ï¼Œä»è€Œç³»ç»Ÿæ€§åœ°å¼•å¯¼æ¨¡å‹å…³æ³¨å¼‚å¸¸æ¨¡å¼ã€‚

**Result:** è¯¥æ–¹æ³•åœ¨ä½åˆ†è¾¨ç‡è§†é¢‘ç‰‡æ®µä¸Šå°†å¼‚å¸¸æ£€æµ‹çš„F1åˆ†æ•°æå‡äº†11.8ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨é«˜åˆ†è¾¨ç‡è§†é¢‘ä¸­å°†å¼‚å¸¸åˆ†ç±»æ€§èƒ½æå‡äº†3.78ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§ç›‘æ§åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡åœ¨å¤šæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ä¸­å¼•å…¥å½’çº³æ€§å¼‚å¸¸åç½®ï¼Œå¯ä»¥æœ‰æ•ˆå…‹æœå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ­£å¸¸æ€§åç½®é—®é¢˜ï¼Œä¸ºè§†é¢‘ç›‘æ§ä¸­çš„å¼‚å¸¸æ£€æµ‹æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.


### [17] [Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding](https://arxiv.org/abs/2512.20451)
*Anh Dao, Manh Tran, Yufei Zhang, Xiaoming Liu, Zijun Cui*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºå°†ç‰©ç†æ¨æ–­çš„å…³èŠ‚é©±åŠ¨åŠ›æ•´åˆåˆ°äººä½“è¿åŠ¨ç†è§£æµç¨‹ä¸­ï¼Œé€šè¿‡åœ¨æ­¥æ€è¯†åˆ«ã€åŠ¨ä½œè¯†åˆ«å’Œè§†é¢‘æè¿°ä¸‰å¤§ä»»åŠ¡ä¸Šçš„ç³»ç»Ÿè¯„ä¼°ï¼Œè¯æ˜äº†åŠ›ä¿¡å·åœ¨åŠ¨æ€ã€é®æŒ¡æˆ–å¤–è§‚å˜åŒ–æ¡ä»¶ä¸‹èƒ½æ˜¾è‘—å¢å¼ºç°æœ‰è§†è§‰å’Œè¿åŠ¨å­¦ç‰¹å¾çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºè§†è§‰çš„äººä½“è¿åŠ¨ç†è§£æ–¹æ³•ä¸»è¦å…³æ³¨è¯†åˆ«ã€è·Ÿè¸ªå’Œæè¿°ä»»åŠ¡ï¼Œä½†æ™®éå¿½ç•¥äº†å…³èŠ‚é©±åŠ¨åŠ›ç­‰ç‰©ç†çº¿ç´¢ï¼Œè€Œè¿™äº›çº¿ç´¢åœ¨ç”Ÿç‰©åŠ›å­¦ä¸­å…·æœ‰åŸºç¡€æ€§ä½œç”¨ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢ç‰©ç†æ¨æ–­çš„åŠ›ä¿¡å·æ˜¯å¦ä»¥åŠä½•æ—¶èƒ½å¤Ÿå¢å¼ºè¿åŠ¨ç†è§£èƒ½åŠ›ï¼Œå¡«è¡¥å½“å‰æ–¹æ³•åœ¨ç‰©ç†çº¿ç´¢æ•´åˆæ–¹é¢çš„ç ”ç©¶ç©ºç™½ã€‚

**Method:** ç ”ç©¶å°†ç‰©ç†æ¨æ–­çš„åŠ›ä¿¡å·æ•´åˆåˆ°ç°æœ‰è¿åŠ¨ç†è§£æµç¨‹ä¸­ï¼Œç³»ç»Ÿè¯„ä¼°å…¶åœ¨ä¸‰å¤§ä»»åŠ¡ä¸Šçš„å½±å“ï¼šæ­¥æ€è¯†åˆ«ã€åŠ¨ä½œè¯†åˆ«å’Œç»†ç²’åº¦è§†é¢‘æè¿°ã€‚åœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œé€šè¿‡å°†åŠ›ä¿¡å·ä¸åŸºçº¿æ¨¡å‹ç»“åˆï¼Œæ„å»ºäº†åŒ…å«ç‰©ç†çº¿ç´¢çš„å¢å¼ºå‹è¿åŠ¨ç†è§£ç®¡é“ï¼Œé‡ç‚¹å…³æ³¨åŠ›ä¿¡å·åœ¨åŠ¨æ€ã€é®æŒ¡å’Œå¤–è§‚å˜åŒ–æ¡ä»¶ä¸‹çš„è¡¥å……ä½œç”¨ã€‚

**Result:** åœ¨CASIA-Bæ­¥æ€è¯†åˆ«åŸºå‡†ä¸Šï¼ŒRank-1å‡†ç¡®ç‡ä»89.52%æå‡è‡³90.39%ï¼ˆ+0.87%ï¼‰ï¼Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹æå‡æ›´æ˜¾è‘—ï¼šç©¿å¤–å¥—æ—¶æå‡+2.7%ï¼Œä¾§è§†å›¾æ—¶æå‡+3.0%ã€‚Gait3DåŸºå‡†ä¸Šæ€§èƒ½ä»46.0%æå‡è‡³47.3%ï¼ˆ+1.3%ï¼‰ã€‚åŠ¨ä½œè¯†åˆ«æ–¹é¢ï¼ŒCTR-GCNåœ¨Penn Actionä¸Šæå‡+2.00%ï¼Œé«˜å¼ºåº¦åŠ¨ä½œå¦‚å‡»æ‰“/æ‹æ‰“ç±»æå‡+6.96%ã€‚è§†é¢‘æè¿°ä»»åŠ¡ä¸­ï¼ŒQwen2.5-VLçš„ROUGE-Lå¾—åˆ†ä»0.310æå‡è‡³0.339ï¼ˆ+0.029ï¼‰ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ç‰©ç†æ¨æ–­çš„åŠ›ä¿¡å·èƒ½å¤Ÿæ˜¾è‘—è¡¥å……è§†è§‰å’Œè¿åŠ¨å­¦ç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€ã€é®æŒ¡æˆ–å¤–è§‚å˜åŒ–çš„æ¡ä»¶ä¸‹ã€‚è¿™ä¸€å‘ç°ä¸ºè¿åŠ¨ç†è§£é¢†åŸŸæä¾›äº†æ–°çš„ç‰©ç†çº¿ç´¢æ•´åˆèŒƒå¼ï¼Œè¡¨æ˜ç”Ÿç‰©åŠ›å­¦ä¿¡æ¯èƒ½å¤Ÿå¢å¼ºç°æœ‰è®¡ç®—æœºè§†è§‰æ–¹æ³•çš„é²æ£’æ€§å’Œè¯­ä¹‰ä¸°å¯Œæ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€è¿åŠ¨ç†è§£ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions.


### [18] [UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images](https://arxiv.org/abs/2512.20479)
*Yiming Zhao, Yuanpeng Gao, Yuxuan Luo, Jiwei Duan, Shisong Lin, Longfei Xiong, Zhouhui Lian*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†UTDesignï¼Œä¸€ä¸ªç”¨äºè®¾è®¡å›¾åƒä¸­é«˜ç²¾åº¦é£æ ¼åŒ–æ–‡æœ¬ç¼–è¾‘å’Œæ¡ä»¶æ–‡æœ¬ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ”¯æŒè‹±æ–‡å’Œä¸­æ–‡è„šæœ¬ï¼Œé€šè¿‡é›†æˆæ‰©æ•£æ¨¡å‹å’Œå¤šæ¨¡æ€æ¡ä»¶ç¼–ç å™¨å®ç°äº†é£æ ¼ä¸€è‡´ä¸”å‡†ç¡®çš„æ–‡æœ¬åˆæˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå°è§„æ¨¡æ’ç‰ˆå’Œéæ‹‰ä¸æ–‡å­—ï¼ˆå¦‚ä¸­æ–‡ï¼‰ä»ç„¶æœ‰é™ï¼Œè¿™é™åˆ¶äº†AIè¾…åŠ©å›¾å½¢è®¾è®¡çš„å®é™…åº”ç”¨æ•ˆæœã€‚

**Method:** UTDesignæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¦–å…ˆï¼Œæå‡ºäº†ä¸€ç§åŸºäºDiTçš„æ–‡æœ¬é£æ ¼è¿ç§»æ¨¡å‹ï¼Œåœ¨åˆæˆæ•°æ®é›†ä¸Šä»å¤´è®­ç»ƒï¼Œèƒ½å¤Ÿç”Ÿæˆä¿ç•™å‚è€ƒå­—å½¢é£æ ¼çš„é€æ˜RGBAæ–‡æœ¬å‰æ™¯ï¼›å…¶æ¬¡ï¼Œé€šè¿‡åœ¨å¤šæ¨¡æ€æ¡ä»¶ç¼–ç å™¨ä¸Šè®­ç»ƒï¼Œæ‰©å±•ä¸ºæ¡ä»¶æ–‡æœ¬ç”Ÿæˆæ¡†æ¶ï¼Œæ”¯æŒåŸºäºèƒŒæ™¯å›¾åƒã€æç¤ºè¯å’Œå¸ƒå±€è§„æ ¼çš„å‡†ç¡®æ–‡æœ¬åˆæˆï¼›æœ€åï¼Œå°†æ–¹æ³•é›†æˆåˆ°å…¨è‡ªåŠ¨æ–‡æœ¬åˆ°è®¾è®¡ç®¡é“ä¸­ï¼Œç»“åˆé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’ŒåŸºäºMLLMçš„å¸ƒå±€è§„åˆ’å™¨ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUTDesignåœ¨å¼€æºæ–¹æ³•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨é£æ ¼ä¸€è‡´æ€§å’Œæ–‡æœ¬å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸ä¸“æœ‰å•†ä¸šæ–¹æ³•ç›¸æ¯”ä¹Ÿå±•ç°å‡ºç‹¬ç‰¹ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¯æŒä¸­è‹±æ–‡è„šæœ¬çš„é«˜ç²¾åº¦æ–‡æœ¬ç”Ÿæˆæ–¹é¢ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºAIè¾…åŠ©å›¾å½¢è®¾è®¡æä¾›äº†ç»Ÿä¸€çš„æ–‡æœ¬å¤„ç†æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†éæ‹‰ä¸æ–‡å­—çš„æ¸²æŸ“è´¨é‡ï¼Œé€šè¿‡æ¡ä»¶ç”Ÿæˆå’Œè‡ªåŠ¨åŒ–ç®¡é“çš„ç»“åˆï¼Œä¸ºå®é™…è®¾è®¡åº”ç”¨æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºå¤šè¯­è¨€æ–‡æœ¬ç”Ÿæˆç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.


### [19] [Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition](https://arxiv.org/abs/2512.20501)
*Gorjan Radevski*

#### ğŸ§© TL;DR
è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç³»åˆ—å¤šæ¨¡æ€å¯¹é½ã€ç¿»è¯‘ã€èåˆå’Œè¿ç§»æ–¹æ³•ï¼Œé€šè¿‡äº”ä¸ªç« èŠ‚åˆ†åˆ«è§£å†³äº†ç©ºé—´è¯­è¨€ç†è§£ã€åŒ»å­¦æ–‡æœ¬å¯¼èˆªã€çŸ¥è¯†å›¾è°±é“¾æ¥ã€åŠ¨ä½œè¯†åˆ«èåˆä»¥åŠå¤šæ¨¡æ€çŸ¥è¯†è’¸é¦ç­‰å…³é”®æŒ‘æˆ˜ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—ç³»ç»Ÿå¤„ç†å¤æ‚å¤šæ¨¡æ€è¾“å…¥çš„èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æœºå™¨å­¦ä¹ ä¸­çš„å¤šä¸ªå…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å°†æ–‡æœ¬ç©ºé—´å…³ç³»è½¬æ¢ä¸ºè§†è§‰æ’åˆ—ã€åŒ»å­¦æ–‡æœ¬ä¸è§£å‰–å›¾è°±çš„ç²¾ç¡®æ˜ å°„ã€ç»“æ„åŒ–æ–‡æœ¬åˆ°çŸ¥è¯†å›¾è°±äº‹å®çš„é“¾æ¥ã€è§†é¢‘åŠ¨ä½œè¯†åˆ«çš„å¤šæ¨¡æ€èåˆï¼Œä»¥åŠå¦‚ä½•é€šè¿‡çŸ¥è¯†è¿ç§»ä½¿å•æ¨¡æ€æ¨¡å‹è·å¾—å¤šæ¨¡æ€èƒ½åŠ›ï¼Œä»è€Œå¢å¼ºè®¡ç®—ç³»ç»Ÿå¯¹å¤æ‚å¤šæ¨¡æ€è¾“å…¥çš„ç†è§£å’Œå¤„ç†èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶æå‡ºäº†äº”ç§æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•ï¼šç¬¬ä¸‰ç« å¼€å‘äº†Spatial-Reasoning Bertæ¨¡å‹ï¼Œå°†åŸºäºæ–‡æœ¬çš„ç©ºé—´å…³ç³»ç¿»è¯‘ä¸ºäºŒç»´æ’åˆ—ï¼›ç¬¬å››ç« å¼•å…¥åˆ©ç”¨åŒ»å­¦æœ¯è¯­ç©ºé—´å…±ç°çš„æŸå¤±å‡½æ•°ï¼Œå®ç°åŒ»å­¦æ–‡æœ¬åˆ°è§£å‰–å›¾è°±ä¸‰ç»´ä½ç½®çš„æ˜ å°„ï¼›ç¬¬äº”ç« å»ºç«‹äº†å°†ç»“æ„åŒ–æ–‡æœ¬é“¾æ¥åˆ°çŸ¥è¯†å›¾è°±å®ä½“å’Œè°“è¯çš„åŸºå‡†ï¼›ç¬¬å…­ç« æå‡ºäº†èåˆè§†é¢‘å¸§å’Œç‰©ä½“æ£€æµ‹è¡¨ç¤ºçš„å¤šæ¨¡æ€èåˆæ–¹æ³•ï¼›ç¬¬ä¸ƒç« æ¢ç´¢äº†å¤šæ¨¡æ€çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œä½¿RGB-onlyæ¨¡å‹èƒ½å¤Ÿæ¨¡ä»¿å¤šæ¨¡æ€èåˆæ¨¡å‹çš„æ€§èƒ½ã€‚

**Result:** ç ”ç©¶å®ç°äº†å¤šé¡¹é‡è¦æˆæœï¼šç©ºé—´è¯­è¨€åˆ°è§†è§‰æ’åˆ—çš„æœ‰æ•ˆè§£ç ï¼Œä¸ºè‡ªåŠ¨åŒ–åœºæ™¯ç”Ÿæˆå¥ å®šäº†åŸºç¡€ï¼›åŒ»å­¦æ–‡æœ¬å¯¼èˆªæ€§æ˜¾è‘—å¢å¼ºï¼Œåˆ›å»ºäº†å¯è§£é‡Šçš„æ˜ å°„å…³ç³»ï¼›å»ºç«‹äº†çŸ¥è¯†å›¾è°±é“¾æ¥çš„åŸºå‡†ï¼Œè§£å†³äº†æ–‡æœ¬æå–ä¸­çš„æ­§ä¹‰é—®é¢˜ï¼›åŠ¨ä½œè¯†åˆ«çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§å¾—åˆ°æå‡ï¼›é€šè¿‡çŸ¥è¯†è’¸é¦ï¼ŒRGB-onlyæ¨¡å‹åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚

**Conclusion:** è¯¥ç ”ç©¶åœ¨å¤šæ¨¡æ€æœºå™¨å­¦ä¹ é¢†åŸŸåšå‡ºäº†ç³»ç»Ÿæ€§è´¡çŒ®ï¼Œæ¨è¿›äº†ç©ºé—´è¯­è¨€ç†è§£ã€åŒ»å­¦æ–‡æœ¬è§£é‡Šã€çŸ¥è¯†å›¾è°±ä¸°å¯ŒåŒ–å’ŒåŠ¨ä½œè¯†åˆ«çš„æ–¹æ³•è®ºå‘å±•ã€‚è¿™äº›æ–¹æ³•å¢å¼ºäº†è®¡ç®—ç³»ç»Ÿè·¨å¤šæ ·åŒ–åº”ç”¨å¤„ç†å¤æ‚å¤šæ¨¡æ€è¾“å…¥çš„èƒ½åŠ›ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€çŸ¥è¯†è¿ç§»æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œå¹³è¡¡äº†æ€§èƒ½ä¸è®¡ç®—æ•ˆç‡çš„éœ€æ±‚ã€‚

---

#### ğŸ“„ Abstract
This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.
  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.
  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.
  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.
  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.
  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.
  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.


### [20] [Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios](https://arxiv.org/abs/2512.20556)
*Mingwei Tang, Jiahao Nie, Guang Yang, Ziqing Cui, Jie Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šç²’åº¦æ–‡æœ¬å¼•å¯¼çš„å›¾åƒèåˆæ–¹æ³•ï¼ˆMTIFï¼‰ï¼Œé€šè¿‡å¼•å…¥ç»†ç²’åº¦ã€ç»“æ„æ€§å’Œè¯­ä¹‰æ€§çš„å¤šå±‚æ¬¡æ–‡æœ¬æè¿°ï¼Œç»“åˆåˆ†å±‚è·¨æ¨¡æ€è°ƒåˆ¶æ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†å¤šæ›å…‰å’Œå¤šç„¦ç‚¹å›¾åƒèåˆçš„è´¨é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å›¾åƒèåˆæ–¹æ³•é€šå¸¸ä½¿ç”¨ç²—ç²’åº¦æ–‡æœ¬æè¿°ä½œä¸ºè¾…åŠ©æŒ‡å¯¼ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹å¯¹å›¾åƒç»†ç²’åº¦ç»†èŠ‚çš„ç†è§£èƒ½åŠ›ï¼Œå¹¶ç»™è·¨æ¨¡æ€å¯¹é½å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå¯¼è‡´èåˆè´¨é‡å—é™ã€‚

**Method:** MTIFæ–¹æ³•åŒ…å«ä¸‰ä¸ªå…³é”®è®¾è®¡ï¼šå¼•å…¥å¤šç²’åº¦æ–‡æœ¬æè¿°åˆ†åˆ«æ•æ‰ç»†èŠ‚ã€ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼›é‡‡ç”¨åˆ†å±‚è·¨æ¨¡æ€è°ƒåˆ¶æ¨¡å—å®ç°æ–‡æœ¬å¼•å¯¼çš„å›¾åƒèåˆï¼›åœ¨æ¯ä¸ªç²’åº¦çº§åˆ«æ·»åŠ ç›‘ç£ä¿¡å·ä»¥ä¿ƒè¿›è§†è§‰-æ–‡æœ¬ç‰¹å¾å¯¹é½ï¼›ä»¥åŠä½¿ç”¨æ˜¾è‘—æ€§é©±åŠ¨çš„æ•°æ®å¢å¼ºæ¨¡å—æ¥ä¸°å¯Œè®­ç»ƒæ•°æ®çš„è¯­ä¹‰å†…å®¹ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMTIFåœ¨å¤šæ›å…‰å’Œå¤šç„¦ç‚¹å›¾åƒèåˆä»»åŠ¡ä¸Šå‡æŒç»­ä¼˜äºå…ˆå‰æ–¹æ³•ï¼ŒéªŒè¯äº†å¤šç²’åº¦æ–‡æœ¬æè¿°å’Œåˆ†å±‚è·¨æ¨¡æ€è°ƒåˆ¶åœ¨æå‡å›¾åƒèåˆè´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å¤šç²’åº¦æ–‡æœ¬æŒ‡å¯¼åœ¨å›¾åƒèåˆä¸­çš„é‡è¦æ€§ï¼Œé€šè¿‡ç»†ç²’åº¦ã€ç»“æ„æ€§å’Œè¯­ä¹‰æ€§æè¿°çš„å±‚æ¬¡åŒ–æ•´åˆï¼Œèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°å®ç°è·¨æ¨¡æ€å¯¹é½ï¼Œä¸ºæ–‡æœ¬å¼•å¯¼çš„è§†è§‰ä»»åŠ¡æä¾›äº†æ–°çš„èåˆèŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.


### [21] [Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models](https://arxiv.org/abs/2512.20557)
*Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†DSR Suiteï¼Œä¸€ä¸ªé’ˆå¯¹åŠ¨æ€ç©ºé—´æ¨ç†ï¼ˆDSRï¼‰çš„æ•°æ®é›†ã€åŸºå‡†å’Œæ¨¡å‹å¢å¼ºå¥—ä»¶ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“ä»é‡å¤–è§†é¢‘ç”Ÿæˆå‡ ä½•æ„ŸçŸ¥çš„é—®ç­”å¯¹ï¼Œå¹¶å¼•å…¥è½»é‡çº§å‡ ä½•é€‰æ‹©æ¨¡å—å°†å‡ ä½•å…ˆéªŒé›†æˆåˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨åŠ¨æ€ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é€šç”¨ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŠ¨æ€ç©ºé—´æ¨ç†ï¼ˆDSRï¼‰æ–¹é¢ä»ç„¶è–„å¼±ï¼Œå³æ¨ç†3Dç©ºé—´ä¸­ç‰©ä½“å‡ ä½•å’Œå…³ç³»éšæ—¶é—´æ¼”å˜çš„èƒ½åŠ›ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹å¯æ‰©å±•çš„4Dæ„ŸçŸ¥è®­ç»ƒèµ„æºã€‚ç°æœ‰å·¥ä½œéš¾ä»¥æ»¡è¶³å¯¹é‡å¤–è§†é¢‘æºã€ç‰©ä½“å’Œåœºæ™¯çº§3Dè¦æ±‚ã€è§†ç‚¹å˜æ¢ã€å¤šç‰©ä½“äº¤äº’ä»¥åŠç»†ç²’åº¦ç¨‹åºæ€§ç­”æ¡ˆçš„éœ€æ±‚ã€‚

**Method:** ç ”ç©¶æå‡ºäº†DSR Suiteï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¦–å…ˆï¼Œå¼€å‘äº†è‡ªåŠ¨åŒ–ç®¡é“ä»é‡å¤–è§†é¢‘ç”ŸæˆåŠ¨æ€ç©ºé—´æ¨ç†çš„å¤šé€‰é—®ç­”å¯¹ï¼Œåˆ©ç”¨ç°ä»£è§†è§‰åŸºç¡€æ¨¡å‹æå–ä¸°å¯Œçš„å‡ ä½•å’Œè¿åŠ¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç›¸æœºå§¿æ€ã€å±€éƒ¨ç‚¹äº‘ã€ç‰©ä½“æ©ç ã€æ–¹å‘å’Œ3Dè½¨è¿¹ï¼›å…¶æ¬¡ï¼Œæ„å»ºäº†ç”¨äºè®­ç»ƒçš„DSR-Trainæ•°æ®é›†å’Œç»è¿‡äººå·¥ç²¾ç‚¼çš„è¯„ä¼°åŸºå‡†DSR-Benchï¼›æœ€åï¼Œæå‡ºäº†è½»é‡çº§å‡ ä½•é€‰æ‹©æ¨¡å—ï¼Œå°†å‡ ä½•å…ˆéªŒæ— ç¼é›†æˆåˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œè¯¥æ¨¡å—å‹ç¼©é—®é¢˜è¯­ä¹‰å¹¶ä»é¢„è®­ç»ƒçš„4Dé‡å»ºå…ˆéªŒä¸­æå–é—®é¢˜ç›¸å…³çŸ¥è¯†åˆ°ç´§å‡‘çš„å‡ ä½•æ ‡è®°é›†åˆä¸­ã€‚

**Result:** å®éªŒè¡¨æ˜ï¼Œå°†DSR-Trainå’Œå‡ ä½•é€‰æ‹©æ¨¡å—é›†æˆåˆ°Qwen2.5-VL-7Bæ¨¡å‹ä¸­ï¼Œæ˜¾è‘—å¢å¼ºäº†å…¶åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨é€šç”¨è§†é¢‘ç†è§£åŸºå‡†ä¸Šä¿æŒäº†å‡†ç¡®æ€§ã€‚ä¸å…ˆå‰å·¥ä½œç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨é‡å¤–è§†é¢‘æºã€ç‰©ä½“å’Œåœºæ™¯çº§3Dè¦æ±‚ã€è§†ç‚¹å˜æ¢ã€å¤šç‰©ä½“äº¤äº’ä»¥åŠç»†ç²’åº¦ç¨‹åºæ€§ç­”æ¡ˆç­‰æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡DSR Suiteåœ¨æ•°æ®é›†ã€åŸºå‡†å’Œæ¨¡å‹ä¸‰ä¸ªå±‚é¢ç³»ç»Ÿæ€§åœ°è§£å†³äº†åŠ¨æ€ç©ºé—´æ¨ç†çš„æŒ‘æˆ˜ï¼Œè¯æ˜äº†å°†å‡ ä½•å…ˆéªŒæœ‰æ•ˆé›†æˆåˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¯è¡Œæ€§ã€‚å‡ ä½•é€‰æ‹©æ¨¡å—çš„è®¾è®¡é¿å…äº†ç”¨æ— å…³çŸ¥è¯†æ·¹æ²¡æ¨¡å‹ï¼Œå®ç°äº†é’ˆå¯¹æ€§çŸ¥è¯†æå–ï¼Œä¸ºæœªæ¥4Dæ„ŸçŸ¥è§†è§‰è¯­è¨€æ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒæ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.


### [22] [FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models](https://arxiv.org/abs/2512.20561)
*Kaitong Cai, Jusheng Zhang, Jing Yang, Yijia Fan, Pengtao Xie, Jian Wang, Keze Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºFlashVLMï¼Œä¸€ç§æ–‡æœ¬å¼•å¯¼çš„è§†è§‰ä»¤ç‰Œé€‰æ‹©æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€é€‚åº”æŸ¥è¯¢çš„è§†è§‰è¾“å…¥å‹ç¼©ï¼Œåœ¨æ˜¾è‘—å‡å°‘è§†è§‰ä»¤ç‰Œçš„åŒæ—¶å®ç°è¶…è¶Šæ— æŸå‹ç¼©çš„æ€§èƒ½ï¼Œä¸ºå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†é«˜æ•ˆä¸”é²æ£’çš„æ¨ç†è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹é€šå¸¸éœ€è¦å¤„ç†æ•°ç™¾è‡³æ•°åƒä¸ªè§†è§‰ä»¤ç‰Œï¼Œå¯¼è‡´äºŒæ¬¡æ³¨æ„åŠ›æˆæœ¬å’Œå¤§é‡å†—ä½™ã€‚ç°æœ‰ä»¤ç‰Œç¼©å‡æ–¹æ³•å¾€å¾€å¿½ç•¥æ–‡æœ¬æŸ¥è¯¢æˆ–ä¾èµ–ä¸ç¨³å®šçš„æ·±åº¦æ³¨æ„åŠ›å›¾ï¼Œåœ¨æ¿€è¿›å‰ªæä¸‹ä¼šå¯¼è‡´è¯­ä¹‰å¯¹é½é€€åŒ–ã€‚

**Method:** FlashVLMé‡‡ç”¨æ–‡æœ¬å¼•å¯¼çš„è§†è§‰ä»¤ç‰Œé€‰æ‹©æ¡†æ¶ï¼Œé€šè¿‡è®¡ç®—æŠ•å½±å›¾åƒä»¤ç‰Œä¸å½’ä¸€åŒ–æ–‡æœ¬åµŒå…¥åœ¨è¯­è¨€æ¨¡å‹ç©ºé—´ä¸­çš„æ˜¾å¼è·¨æ¨¡æ€ç›¸ä¼¼åº¦ï¼Œå°†å¤–åœ¨ç›¸å…³æ€§ä¸å†…åœ¨è§†è§‰æ˜¾è‘—æ€§é€šè¿‡å¯¹æ•°åŸŸåŠ æƒå’Œæ¸©åº¦æ§åˆ¶é”åŒ–èåˆï¼Œå¹¶é‡‡ç”¨å¤šæ ·æ€§ä¿æŒåˆ†åŒºä¿ç•™æœ€å°ä½†å…·ä»£è¡¨æ€§çš„èƒŒæ™¯ä»¤ç‰Œä»¥ç»´æŒå…¨å±€ä¸Šä¸‹æ–‡ã€‚

**Result:** åœ¨ç›¸åŒä»¤ç‰Œé¢„ç®—å’Œè¯„ä¼°åè®®ä¸‹ï¼ŒFlashVLMåœ¨LLaVA 1.5ä¸Šå®ç°äº†è¶…è¶Šæ— æŸå‹ç¼©çš„æ€§èƒ½ï¼Œåœ¨å‰ªæé«˜è¾¾77.8%è§†è§‰ä»¤ç‰Œæ—¶ç•¥å¾®è¶…è¿‡æœªå‰ªæåŸºçº¿ï¼Œå³ä½¿åœ¨94.4%å‹ç¼©ç‡ä¸‹ä»ä¿æŒ92.8%å‡†ç¡®ç‡ã€‚åœ¨14ä¸ªå›¾åƒå’Œè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ•ˆç‡-æ€§èƒ½æƒè¡¡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æ˜¾å¼è·¨æ¨¡æ€ç›¸ä¼¼åº¦è®¡ç®—å’Œå¤šæ ·æ€§ä¿æŒæœºåˆ¶ï¼Œå¯ä»¥åœ¨å¤§å¹…å‡å°‘è§†è§‰ä»¤ç‰Œçš„åŒæ—¶ç»´æŒç”šè‡³æå‡æ¨¡å‹æ€§èƒ½ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆæ¨ç†æä¾›äº†é²æ£’ä¸”å¯æ³›åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å®é™…éƒ¨ç½²ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.
  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.
  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.


### [23] [SpatialTree: How Spatial Abilities Branch Out in MLLMs](https://arxiv.org/abs/2512.20617)
*Yuxi Xiao, Longfei Li, Shen Yan, Xinhang Liu, Sida Peng, Yunchao Wei, Xiaowei Zhou, Bingyi Kang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†SpatialTreeï¼Œä¸€ä¸ªå—è®¤çŸ¥ç§‘å­¦å¯å‘çš„å››å±‚æ¬¡ç©ºé—´èƒ½åŠ›åˆ†ç±»æ¡†æ¶ï¼Œå¹¶æ„å»ºäº†é¦–ä¸ªå±‚æ¬¡åŒ–åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ç©ºé—´èƒ½åŠ›åœ¨MLLMsä¸­å‘ˆç°æ¸…æ™°çš„å±‚æ¬¡ç»“æ„ï¼Œæ­ç¤ºäº†è·¨å±‚æ¬¡è¿ç§»çš„åŠ¨æ€æ¨¡å¼ï¼Œå¹¶æå‡ºè‡ªåŠ¨æ€è€ƒç­–ç•¥æ¥ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ åœ¨æ‰€æœ‰å±‚æ¬¡çš„è¡¨ç°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ç©ºé—´èƒ½åŠ›å±‚æ¬¡ç»“æ„ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚å½“å‰ç ”ç©¶å¤§å¤šå…³æ³¨ç‹­çª„çš„ä»»åŠ¡é›†ï¼Œç¼ºä¹å¯¹ç©ºé—´èƒ½åŠ›ä»ä½çº§æ„ŸçŸ¥åˆ°é«˜çº§æ¨ç†ä¸äº¤äº’çš„æ¸è¿›å‘å±•å±‚æ¬¡çš„ç³»ç»Ÿæ€§è®¤çŸ¥ç§‘å­¦ç†è§£ï¼Œè¿™é˜»ç¢äº†å¯¹MLLMsç©ºé—´èƒ½åŠ›çš„å…¨é¢è¯„ä¼°ä¸ç³»ç»Ÿæ€§æå‡ã€‚

**Method:** ç ”ç©¶æå‡ºäº†SpatialTreeæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå—è®¤çŸ¥ç§‘å­¦å¯å‘çš„å››å±‚æ¬¡ç©ºé—´èƒ½åŠ›åˆ†ç±»ä½“ç³»ï¼šä½çº§æ„ŸçŸ¥ï¼ˆL1ï¼‰ã€å¿ƒç†æ˜ å°„ï¼ˆL2ï¼‰ã€æ¨¡æ‹Ÿï¼ˆL3ï¼‰å’Œæ™ºèƒ½ä½“èƒ½åŠ›ï¼ˆL4ï¼‰ã€‚åŸºäºæ­¤åˆ†ç±»æ³•ï¼Œæ„å»ºäº†é¦–ä¸ªä»¥èƒ½åŠ›ä¸ºä¸­å¿ƒçš„å±‚æ¬¡åŒ–åŸºå‡†æµ‹è¯•ï¼Œå…¨é¢è¯„ä¼°ä¸»æµMLLMsåœ¨27ä¸ªå­èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ¢ç´¢äº†æœ‰ç›‘ç£å¾®è°ƒä¸‹çš„è¿ç§»åŠ¨æ€ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•çš„è‡ªåŠ¨æ€è€ƒç­–ç•¥æ¥æŠ‘åˆ¶ä¸å¿…è¦çš„æ·±æ€ç†Ÿè™‘ï¼Œä½¿å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿä¸€è‡´åœ°æå‡æ‰€æœ‰å±‚æ¬¡çš„è¡¨ç°ã€‚

**Result:** è¯„ä¼°ç»“æœæ˜¾ç¤ºäº†ä¸€ä¸ªæ¸…æ™°çš„ç»“æ„ï¼šL1æŠ€èƒ½åŸºæœ¬æ­£äº¤ï¼Œè€Œæ›´é«˜çº§åˆ«çš„æŠ€èƒ½åˆ™å¼ºç›¸å…³ï¼Œè¡¨æ˜éšç€å±‚æ¬¡æå‡ï¼Œèƒ½åŠ›é—´çš„ç›¸äº’ä¾èµ–æ€§å¢å¼ºã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„æœ‰ç›‘ç£å¾®è°ƒï¼Œå‘ç°äº†ä»¤äººæƒŠè®¶çš„è¿ç§»åŠ¨æ€ï¼šL1å†…éƒ¨å­˜åœ¨è´Ÿè¿ç§»ï¼Œä½†ä»ä½å±‚åˆ°é«˜å±‚èƒ½åŠ›å­˜åœ¨å¼ºè·¨å±‚è¿ç§»å¹¶è¡¨ç°å‡ºæ˜¾è‘—çš„ååŒæ•ˆåº”ã€‚å®éªŒè¿˜å‘ç°ï¼Œé¼“åŠ±å¹¿æ³›"æ€è€ƒ"çš„æœ´ç´ å¼ºåŒ–å­¦ä¹ ä¸å¯é ï¼šå®ƒæœ‰åŠ©äºå¤æ‚æ¨ç†ä½†æŸå®³ç›´è§‰æ„ŸçŸ¥ï¼Œè€Œæå‡ºçš„è‡ªåŠ¨æ€è€ƒç­–ç•¥èƒ½å¤Ÿä½¿å¼ºåŒ–å­¦ä¹ åœ¨æ‰€æœ‰å±‚æ¬¡ä¸Šä¸€è‡´æå‡æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡æ„å»ºSpatialTreeï¼Œä¸ºç†è§£å’Œç³»ç»Ÿæ€§æ‰©å±•MLLMsä¸­çš„ç©ºé—´èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ¦‚å¿µéªŒè¯æ¡†æ¶ã€‚ç ”ç©¶æ­ç¤ºäº†ç©ºé—´èƒ½åŠ›åœ¨MLLMsä¸­çš„å±‚æ¬¡ç»“æ„ç‰¹æ€§ï¼Œå‘ç°äº†è·¨å±‚æ¬¡è¿ç§»çš„åŠ¨æ€æ¨¡å¼ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„ä¼˜åŒ–ç­–ç•¥ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥å¼€å‘æ›´å…¨é¢ã€å±‚æ¬¡åŒ–çš„å¤šæ¨¡æ€èƒ½åŠ›è¯„ä¼°ä½“ç³»ä»¥åŠè®¾è®¡é’ˆå¯¹æ€§çš„æ¨¡å‹è®­ç»ƒæ–¹æ³•æä¾›äº†é‡è¦è§è§£ã€‚

---

#### ğŸ“„ Abstract
Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive "thinking" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data](https://arxiv.org/abs/2512.19864)
*Shashi Kant Gupta, Arijeet Pramanik, Jerrin John Thomas, Regina Schwind, Lauren Wiener, Avi Raju, Jeremy Kornbluth, Yanshan Wang, Zhaohui Su, Hrituraj Singh*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºä»éç»“æ„åŒ–ç”µå­å¥åº·è®°å½•ä¸­æå–ç»“æ„åŒ–è‚¿ç˜¤å­¦æ•°æ®ï¼Œåœ¨åŒ…å«40ä¸‡ä»½ä¸´åºŠæ–‡æ¡£çš„å¤§è§„æ¨¡çœŸå®æ•°æ®é›†ä¸Šå®ç°äº†å¹³å‡F1åˆ†æ•°0.93çš„ä¼˜å¼‚æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½äº†äººå·¥æ ‡æ³¨æˆæœ¬ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç”µå­å¥åº·è®°å½•ä¸­çš„éç»“æ„åŒ–ä¸´åºŠç¬”è®°åŒ…å«ä¸°å¯Œçš„è‚¿ç˜¤æ²»ç–—ä¿¡æ¯ï¼Œä½†ç”±äºæœ¯è¯­ä¸“ä¸šæ€§å¼ºã€æ–‡æ¡£æ ¼å¼ä¸ä¸€è‡´ä»¥åŠä¿¡æ¯çŸ›ç›¾ç­‰é—®é¢˜ï¼Œç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•é€šå¸¸å±€é™äºç‰¹å®šåœºæ™¯æˆ–å˜é‡ï¼Œæ— æ³•å®ç°è·¨æ–‡æ¡£çš„æ‚£è€…çº§ç»¼åˆæ•°æ®æå–ï¼Œè€Œäººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ã€‚

**Method:** ç ”ç©¶æå‡ºäº†ä¸€ç§æ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°†å¤æ‚çš„è‚¿ç˜¤å­¦æ•°æ®æå–ä»»åŠ¡åˆ†è§£ä¸ºæ¨¡å—åŒ–ã€è‡ªé€‚åº”çš„å­ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ¨ç†æ™ºèƒ½ä½“ï¼Œé…å¤‡ä¸Šä¸‹æ–‡æ•æ„Ÿæ£€ç´¢å’Œè¿­ä»£åˆæˆèƒ½åŠ›ï¼Œä»çœŸå®ä¸–ç•Œçš„è‚¿ç˜¤å­¦ç¬”è®°ä¸­å…¨é¢æå–ç»“æ„åŒ–ä¸´åºŠå˜é‡ã€‚

**Result:** åœ¨åŒ…å«40ä¸‡ä»½éç»“æ„åŒ–ä¸´åºŠç¬”è®°å’Œæ‰«æPDFæŠ¥å‘Šã€æ¶µç›–2250åç™Œç—‡æ‚£è€…çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å¹³å‡F1åˆ†æ•°è¾¾åˆ°0.93ï¼Œ103ä¸ªè‚¿ç˜¤ç‰¹å¼‚æ€§ä¸´åºŠå˜é‡ä¸­æœ‰100ä¸ªè¶…è¿‡0.85ï¼Œå…³é”®å˜é‡å¦‚ç”Ÿç‰©æ ‡å¿—ç‰©å’Œè¯ç‰©ä¿¡æ¯è¶…è¿‡0.95ï¼Œé›†æˆåˆ°æ•°æ®æ•´ç†å·¥ä½œæµåè·å¾—äº†0.94çš„ç›´æ¥äººå·¥æ‰¹å‡†ç‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶é¦–æ¬¡å®ç°äº†åŸºäºLLMæ™ºèƒ½ä½“çš„ç«¯åˆ°ç«¯ç»“æ„åŒ–è‚¿ç˜¤å­¦æ•°æ®æå–ç³»ç»Ÿï¼Œè¯æ˜äº†æ™ºèƒ½ä½“æ¡†æ¶åœ¨å¤„ç†å¤æ‚åŒ»ç–—ä¿¡æ¯æå–ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤§è§„æ¨¡ä¸´åºŠæ•°æ®è‡ªåŠ¨åŒ–å¤„ç†æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œæ˜¾è‘—é™ä½äº†äººå·¥æˆæœ¬å¹¶æé«˜äº†æ•°æ®æå–çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚

---

#### ğŸ“„ Abstract
Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale


### [25] [How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse](https://arxiv.org/abs/2512.19903)
*Kirk Vanacore, Rene F. Kizilcec*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ— éœ€å®šåˆ¶åŒ–çš„æƒ…å†µä¸‹å¯¹çœŸå®æ•™è‚²åœºæ™¯çš„è§£é‡Šèƒ½åŠ›ï¼Œå‘ç°åŸºç¡€æ¨¡å‹åœ¨åˆ†ç±»æ•™å­¦è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºæœ‰æ„ä¹‰ä½†æœ‰é™çš„èƒ½åŠ›ï¼Œæç¤ºè®¾è®¡èƒ½æå‡æ€§èƒ½ä½†æ— æ³•æ¶ˆé™¤æ ¹æœ¬çš„å¯é æ€§é™åˆ¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²æŠ€æœ¯ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ¨¡å‹é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ä¼˜åŒ–ï¼Œè€Œå¯¹æ¨¡å‹åœ¨æ— éœ€æ˜¾è‘—å®šåˆ¶åŒ–çš„æƒ…å†µä¸‹è§£é‡ŠçœŸå®æ•™è‚²åœºæ™¯çš„èƒ½åŠ›äº†è§£ä¸è¶³ã€‚åœ¨LLMç³»ç»Ÿè¢«å­¦ä¹ è€…å’Œæ•™è‚²è€…å¹¿æ³›é‡‡ç”¨çš„èƒŒæ™¯ä¸‹ï¼Œç†è§£å…¶å¼€ç®±å³ç”¨çš„èƒ½åŠ›å¯¹äºè®¾å®šæœŸæœ›å’Œå»ºç«‹åŸºå‡†è‡³å…³é‡è¦ã€‚

**Method:** ç ”ç©¶æ¯”è¾ƒäº†å…­ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹åœ¨çœŸå®è¯¾å ‚è½¬å½•æœ¬ä¸­åˆ†ç±»æ•™å­¦è¡Œä¸ºçš„åŸºç¡€æ€§èƒ½ã€‚è¯„ä¼°äº†å…¸å‹çš„æç¤ºæ–¹æ³•ï¼šé›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºï¼Œé€šè¿‡ä¸“å®¶ç¼–ç æ³¨é‡Šä½œä¸ºåŸºå‡†æ¥æµ‹é‡æ¨¡å‹çš„åˆ†ç±»å‡†ç¡®æ€§ã€‚

**Result:** ç ”ç©¶å‘ç°é›¶æ ·æœ¬æ€§èƒ½ä¸­ç­‰ï¼Œä½†æä¾›å…¨é¢ç¤ºä¾‹çš„å°‘æ ·æœ¬æç¤ºæ˜¾è‘—æå‡äº†æœ€å…ˆè¿›æ¨¡å‹çš„æ€§èƒ½ï¼Œæœ€å¼ºé…ç½®è¾¾åˆ°Cohen's Kappa = 0.58ã€‚ç„¶è€Œæ”¹è¿›å¹¶ä¸å‡åŒ€æˆ–å®Œå…¨ï¼šæ€§èƒ½å› æ•™å­¦è¡Œä¸ºç±»å‹è€Œå¼‚ï¼Œæ›´é«˜çš„å¬å›ç‡é€šå¸¸ä»¥å¢åŠ è¯¯æŠ¥ä¸ºä»£ä»·ã€‚

**Conclusion:** åŸºç¡€æ¨¡å‹åœ¨è§£é‡Šæ•™å­¦è¯è¯­æ–¹é¢è¡¨ç°å‡ºæœ‰æ„ä¹‰ä½†æœ‰é™çš„èƒ½åŠ›ï¼Œæç¤ºè®¾è®¡æœ‰åŠ©äºå±•ç°æ¨¡å‹æ½œåŠ›ä½†æ— æ³•æ¶ˆé™¤æ ¹æœ¬çš„å¯é æ€§çº¦æŸã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨çœŸå®æ•™è‚²åº”ç”¨ä¸­éœ€è¦è°¨æ…è¯„ä¼°LLMæ€§èƒ½ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶éœ€è¦è§£å†³æ¨¡å‹åœ¨ç‰¹å®šæ•™å­¦è¡Œä¸ºåˆ†ç±»ä¸Šçš„ä¸ä¸€è‡´æ€§ã€‚

---

#### ğŸ“„ Abstract
Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.


### [26] [PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation](https://arxiv.org/abs/2512.19933)
*Zhixiang Lu, Xueyuan Deng, Yiran Liu, Yulong Li, Qiang Yan, Imran Razzak, Jionglong Su*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PRISMæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆéšæœºå¾®åˆ†æ–¹ç¨‹å’Œä¸ªæ€§æ¡ä»¶éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„æ··åˆæ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿåœ¨çº¿æåŒ–ä¸­çš„å¿ƒç†å¼‚è´¨æ€§ï¼Œæ˜¾è‘—æå‡äº†äººæ ¼ä¸€è‡´æ€§å¹¶æˆåŠŸå¤ç°äº†ç†æ€§æŠ‘åˆ¶å’Œæƒ…æ„Ÿå…±é¸£ç­‰æ¶Œç°ç°è±¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»ŸåŸºäºä»£ç†çš„æ„è§åŠ¨æ€æ¨¡å‹å› é‡‡ç”¨ç®€åŒ–çš„åŒè´¨æ€§å‡è®¾è€Œæ— æ³•æ•æ‰é©±åŠ¨åœ¨çº¿æåŒ–çš„å¿ƒç†å¼‚è´¨æ€§ï¼Œè¿™é™åˆ¶äº†å¯¹ä¸ªä½“è®¤çŸ¥åå·®ä¸ä¿¡æ¯ä¼ æ’­ä¹‹é—´å…³é”®ç›¸äº’ä½œç”¨çš„ç†è§£ï¼Œé˜»ç¢äº†å¯¹æ„è¯†å½¢æ€åˆ†æ­§æ”¾å¤§æœºåˆ¶çš„ç†è§£ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸ªæ€§æŠ˜å°„æ™ºèƒ½ä»¿çœŸæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆæ¡†æ¶ï¼Œå°†ç”¨äºè¿ç»­æƒ…ç»ªæ¼”åŒ–çš„éšæœºå¾®åˆ†æ–¹ç¨‹ä¸ç”¨äºç¦»æ•£å†³ç­–çš„ä¸ªæ€§æ¡ä»¶éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ç›¸ç»“åˆï¼Œè¯¥æ¨¡å‹ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»£ç†åˆ†é…åŸºäºè¿ˆå°”æ–¯-å¸ƒé‡Œæ ¼æ–¯ç±»å‹æŒ‡æ ‡çš„è®¤çŸ¥ç­–ç•¥ï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡ç¤¾äº¤åª’ä½“æ•°æ®é›†çš„æ•°æ®é©±åŠ¨å…ˆéªŒè¿›è¡Œåˆå§‹åŒ–ã€‚

**Result:** PRISMæ¨¡å‹å®ç°äº†ä¸äººç±»çœŸå®æƒ…å†µä¸€è‡´çš„äººæ ¼ä¸€è‡´æ€§ï¼Œæ˜¾è‘—ä¼˜äºæ ‡å‡†çš„åŒè´¨æ€§å’Œå¤§äº”äººæ ¼åŸºå‡†ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆå¤ç°äº†ç†æ€§æŠ‘åˆ¶å’Œæƒ…æ„Ÿå…±é¸£ç­‰æ¶Œç°ç°è±¡ï¼Œä¸ºåˆ†æå¤æ‚ç¤¾äº¤åª’ä½“ç”Ÿæ€ç³»ç»Ÿæä¾›äº†å¼ºå¤§å·¥å…·ã€‚

**Conclusion:** è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªç¨³å¥çš„æ¡†æ¶æ¥åˆ†æå¤æ‚çš„ç¤¾äº¤åª’ä½“ç”Ÿæ€ç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆè¿ç»­æƒ…ç»ªæ¼”åŒ–å’Œç¦»æ•£å†³ç­–è¿‡ç¨‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ä¸ªä½“å¿ƒç†å¼‚è´¨æ€§å¦‚ä½•é©±åŠ¨åœ¨çº¿æåŒ–ç°è±¡ï¼Œä¸ºç¤¾ä¼šç§‘å­¦è®¡ç®—å»ºæ¨¡æä¾›äº†æ–°çš„æ–¹æ³•è®ºå·¥å…·ã€‚

---

#### ğŸ“„ Abstract
Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.


### [27] [M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2512.20136)
*Hyeongcheol Park, Jiyoung Seo, Jaewon Mun, Hogun Park, Wonmin Byeon, Sung June Kim, Hyeonsoo Im, JeungSub Lee, Sangpil Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMÂ³KG-RAGï¼Œä¸€ç§å¤šè·³å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±å¢å¼ºçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡æ„å»ºä¸Šä¸‹æ–‡ä¸°å¯Œçš„å¤šæ¨¡æ€å®ä½“ä¸‰å…ƒç»„å’Œå¼•å…¥GRASPæœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘-è§†è§‰é¢†åŸŸçš„æ¨ç†æ·±åº¦å’Œç­”æ¡ˆå¿ å®åº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆåœ¨éŸ³é¢‘-è§†è§‰é¢†åŸŸé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯ç°æœ‰å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±çš„æ¨¡æ€è¦†ç›–æœ‰é™ä¸”å¤šè·³è¿æ¥ä¸è¶³ï¼ŒäºŒæ˜¯ä»…åŸºäºå…±äº«å¤šæ¨¡æ€åµŒå…¥ç©ºé—´çš„ç›¸ä¼¼æ€§æ£€ç´¢æ— æ³•æœ‰æ•ˆè¿‡æ»¤æ— å…³æˆ–å†—ä½™çŸ¥è¯†ï¼Œå¯¼è‡´æ¨ç†æ·±åº¦å’Œç­”æ¡ˆå¿ å®åº¦å—é™ã€‚

**Method:** æœ¬æ–‡æå‡ºMÂ³KG-RAGæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¦–å…ˆè®¾è®¡è½»é‡çº§å¤šæ™ºèƒ½ä½“æµæ°´çº¿æ„å»ºå¤šè·³å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼Œå…¶ä¸­åŒ…å«ä¸Šä¸‹æ–‡ä¸°å¯Œçš„å¤šæ¨¡æ€å®ä½“ä¸‰å…ƒç»„ï¼Œæ”¯æŒåŸºäºè¾“å…¥æŸ¥è¯¢çš„æ¨¡æ€æ„ŸçŸ¥æ£€ç´¢ï¼›å…¶æ¬¡å¼•å…¥GRASPæœºåˆ¶ï¼Œç¡®ä¿å®ä½“ä¸æŸ¥è¯¢çš„ç²¾ç¡®å¯¹é½ï¼Œè¯„ä¼°ç­”æ¡ˆæ”¯æŒç›¸å…³æ€§ï¼Œå¹¶å‰ªæå†—ä½™ä¸Šä¸‹æ–‡ï¼Œä»…ä¿ç•™ç”Ÿæˆå“åº”æ‰€å¿…éœ€çš„çŸ¥è¯†ã€‚

**Result:** åœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMÂ³KG-RAGç›¸æ¯”ç°æœ‰æ–¹æ³•æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†å’Œå®ä½“å¯¹é½èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éŸ³é¢‘-è§†è§‰é¢†åŸŸçš„çŸ¥è¯†æ£€ç´¢å’Œç­”æ¡ˆç”Ÿæˆè´¨é‡æ–¹é¢å–å¾—äº†æ˜æ˜¾æ”¹è¿›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å¤šè·³å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ä¸é€‰æ‹©æ€§æ£€ç´¢å‰ªææœºåˆ¶ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œæœªæ¥å¯æ‰©å±•è‡³æ›´å¹¿æ³›çš„å¤šæ¨¡æ€åº”ç”¨åœºæ™¯ï¼Œå¹¶è¿›ä¸€æ­¥ä¼˜åŒ–çŸ¥è¯†å›¾è°±æ„å»ºå’Œæ£€ç´¢æ•ˆç‡ã€‚

---

#### ğŸ“„ Abstract
Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.


### [28] [Retrieval-augmented Prompt Learning for Pre-trained Foundation Models](https://arxiv.org/abs/2512.20145)
*Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºRetroPromptæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ£€ç´¢æœºåˆ¶å’Œè§£è€¦è®°å¿†ä¸æ³›åŒ–ï¼Œè§£å†³ä¼ ç»Ÿæç¤ºå­¦ä¹ ä¸­è¿‡åº¦ä¾èµ–è®°å¿†åŒ–çš„é—®é¢˜ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹å®ç°æ›´ä¼˜æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿæç¤ºå­¦ä¹ æ–¹æ³•ä»éµå¾ªå‚æ•°åŒ–å­¦ä¹ èŒƒå¼ï¼Œåœ¨è®°å¿†åŒ–å’Œæœºæ¢°å­¦ä¹ æ–¹é¢å­˜åœ¨æ³›åŒ–ç¨³å®šæ€§é—®é¢˜ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨éå…¸å‹å®ä¾‹å¹¶é¿å…åœ¨æœ‰é™æ•°æ®ä¸‹å¯¹æµ…å±‚æ¨¡å¼çš„è¿‡æ‹Ÿåˆã€‚

**Method:** RetroPromptæ–¹æ³•é€šè¿‡è§£è€¦çŸ¥è¯†ä¸è®°å¿†åŒ–ï¼Œåœ¨è¾“å…¥ã€è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå¼•å…¥åŸºäºè®­ç»ƒæ•°æ®ç”Ÿæˆçš„å…¬å¼€çŸ¥è¯†åº“æ£€ç´¢æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸»åŠ¨ä»è¯­æ–™åº“ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥å¢å¼ºå¯ç”¨çº¿ç´¢ã€‚

**Result:** åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„å¤šæ•°æ®é›†å®éªŒä¸­ï¼ŒRetroPromptåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œé€šè¿‡è®°å¿†æ¨¡å¼åˆ†æè¯å®å…¶æœ‰æ•ˆå‡å°‘äº†å¯¹æœºæ¢°è®°å¿†çš„ä¾èµ–ã€‚

**Conclusion:** RetroPrompté€šè¿‡æ£€ç´¢å¢å¼ºæœºåˆ¶å¹³è¡¡è®°å¿†ä¸æ³›åŒ–ï¼Œä¸ºé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„æç¤ºå­¦ä¹ æä¾›äº†æ–°èŒƒå¼ï¼Œè¡¨æ˜è§£è€¦çŸ¥è¯†è®°å¿†èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.


### [29] [Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits](https://arxiv.org/abs/2512.20578)
*Amirhosein Ghasemabadi, Di Niu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºGnosisï¼Œä¸€ç§è½»é‡çº§è‡ªæ„ŸçŸ¥æœºåˆ¶ï¼Œä½¿å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è§£ç éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æ¨¡å¼çš„ä¿¡å·è¿›è¡Œå†…åœ¨è‡ªæˆ‘éªŒè¯ï¼Œä»¥é¢„æµ‹è‡ªèº«ç”Ÿæˆé”™è¯¯ï¼Œä»…å¢åŠ çº¦500ä¸‡å‚æ•°ä¸”ä¸åºåˆ—é•¿åº¦æ— å…³ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæµç•…å¤æ‚çš„è¾“å‡ºä½†ç»å¸¸æ— æ³•è¯†åˆ«è‡ªèº«é”™è¯¯å’Œå¹»è§‰ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–å¤–éƒ¨è¯„åˆ¤å™¨ã€å¤šæ ·æœ¬ä¸€è‡´æ€§æˆ–åŸºäºæ–‡æœ¬çš„è‡ªæˆ‘æ‰¹åˆ¤ï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆå¢åŠ é¢å¤–è®¡ç®—æˆæœ¬ï¼Œè¦ä¹ˆä¸çœŸå®æ­£ç¡®æ€§ç›¸å…³æ€§è¾ƒå¼±ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢LLMsæ˜¯å¦èƒ½å¤Ÿé€šè¿‡æ¨ç†è¿‡ç¨‹ä¸­æ£€æŸ¥å†…éƒ¨çŠ¶æ€æ¥é¢„æµ‹è‡ªèº«å¤±è´¥ã€‚

**Method:** å¼•å…¥Gnosisè½»é‡çº§è‡ªæ„ŸçŸ¥æœºåˆ¶ï¼Œä½¿å†»ç»“çš„LLMsèƒ½å¤Ÿè¿›è¡Œå†…åœ¨è‡ªæˆ‘éªŒè¯ï¼Œé€šè¿‡è¢«åŠ¨è§‚å¯Ÿå†…éƒ¨è½¨è¿¹ï¼Œå°†å…¶å‹ç¼©ä¸ºå›ºå®šé¢„ç®—æè¿°ç¬¦ï¼Œå¹¶ä»¥å¯å¿½ç•¥çš„æ¨ç†æˆæœ¬é¢„æµ‹æ­£ç¡®æ€§ã€‚è¯¥æ–¹æ³•ä»…å¢åŠ çº¦500ä¸‡å‚æ•°ï¼Œç‹¬ç«‹äºåºåˆ—é•¿åº¦è¿è¡Œï¼Œè§£ç éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æ¨¡å¼çš„ä¿¡å·ã€‚

**Result:** åœ¨æ•°å­¦æ¨ç†ã€å¼€æ”¾åŸŸé—®ç­”å’Œå­¦æœ¯çŸ¥è¯†åŸºå‡†æµ‹è¯•ä¸­ï¼Œé’ˆå¯¹ä»17äº¿åˆ°200äº¿å‚æ•°çš„å†»ç»“éª¨å¹²æ¨¡å‹ï¼ŒGnosisåœ¨å‡†ç¡®æ€§å’Œæ ¡å‡†æ–¹é¢æŒç»­ä¼˜äºå¼ºå¤§çš„å†…éƒ¨åŸºçº¿å’Œå¤§å‹å¤–éƒ¨è¯„åˆ¤å™¨ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé›¶æ ·æœ¬æ³›åŒ–åˆ°éƒ¨åˆ†ç”Ÿæˆï¼Œå®ç°å¤±è´¥è½¨è¿¹çš„æ—©æœŸæ£€æµ‹å’Œè®¡ç®—æ„ŸçŸ¥æ§åˆ¶ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜å¯é çš„æ­£ç¡®æ€§çº¿ç´¢å†…åœ¨äºç”Ÿæˆè¿‡ç¨‹ï¼Œå¯ä»¥åœ¨æ²¡æœ‰å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹é«˜æ•ˆæå–ã€‚Gnosiså±•ç¤ºäº†LLMså†…éƒ¨çŠ¶æ€åŒ…å«ä¸°å¯Œçš„è‡ªæˆ‘è¯„ä¼°ä¿¡æ¯ï¼Œä¸ºæ„å»ºæ›´å¯é ã€è‡ªæˆ‘ç›‘æ§çš„è¯­è¨€æ¨¡å‹ç³»ç»Ÿæä¾›äº†æ–°é€”å¾„ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚

---

#### ğŸ“„ Abstract
Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.


### [30] [Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs](https://arxiv.org/abs/2512.20595)
*Dhruv Anand, Ehsan Shareghi*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†Cube BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é­”æ–¹è§£è°œä»»åŠ¡ä¸­çš„ç©ºé—´ä¸åºåˆ—æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡äº”ä¸ªåˆ†è§£æŠ€èƒ½çš„ç³»ç»Ÿæ€§è¯„ä¼°æ­ç¤ºäº†æ¨¡å‹åœ¨å¤æ‚åºåˆ—å†³ç­–ä¸­çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç¼ºä¹ç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´ä¸åºåˆ—æ¨ç†èƒ½åŠ›çš„æ ‡å‡†åŒ–åŸºå‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šæ­¥å†³ç­–å’Œé”™è¯¯æ¢å¤çš„å¤æ‚ä»»åŠ¡ä¸­ï¼Œç°æœ‰è¯„ä¼°æ–¹æ³•éš¾ä»¥å…¨é¢è¡¡é‡æ¨¡å‹çš„å®é™…æ¨ç†èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶è®¾è®¡äº†Cube BenchåŸºå‡†æµ‹è¯•ï¼Œå°†é­”æ–¹è§£è°œä»»åŠ¡åˆ†è§£ä¸ºäº”ä¸ªæ ¸å¿ƒæŠ€èƒ½ï¼šä»å›¾åƒå’Œæ–‡æœ¬é‡å»ºé­”æ–¹é¢ã€é€‰æ‹©æœ€ä¼˜ä¸‹ä¸€æ­¥ç§»åŠ¨ã€é¢„æµ‹å€™é€‰ç§»åŠ¨ç»“æœè€Œä¸æ‰§è¡Œã€æ‰§è¡Œå¤šæ­¥è®¡åˆ’å¹¶ä»ä¸­æ–­æ¢å¤ã€æ£€æµ‹å¹¶ä¿®æ­£è‡ªèº«é”™è¯¯ã€‚é‡‡ç”¨ç»Ÿä¸€çš„åŠ æ‰°çŠ¶æ€ã€æç¤ºè¯å’Œè§£æå™¨ï¼Œä»¥åŠå•ä¸€çš„è·ç¦»è§£å†³åº¦é‡æ ‡å‡†ï¼Œåœ¨ä¸åŒåŠ æ‰°æ·±åº¦ä¸‹ç³»ç»Ÿæ¯”è¾ƒå¤šä¸ªMLLMæ¨¡å‹ã€‚

**Result:** å®éªŒè¯„ä¼°ä¸ƒä¸ªMLLMæ¨¡å‹æ˜¾ç¤ºï¼Œéšç€åŠ æ‰°æ·±åº¦å¢åŠ ï¼Œæ¨¡å‹å‡†ç¡®ç‡æ€¥å‰§ä¸‹é™ï¼›ä¸€æ—¦è½¨è¿¹åœæ»æˆ–å‘æ•£ï¼Œæ¨¡å‹å¾ˆå°‘èƒ½æ¢å¤ï¼›é«˜é¢é‡å»ºå‡†ç¡®ç‡ä¸èƒ½ä¿è¯æœ‰æ•ˆçš„åŠ¨ä½œé€‰æ‹©æˆ–å¤šæ­¥æ‰§è¡Œèƒ½åŠ›ã€‚é—­æºä¸å¼€æºæ¨¡å‹å­˜åœ¨æ˜¾è‘—å·®è·ï¼šæœ€å¼ºé—­æºæ¨¡å‹åœ¨å•æ­¥æ„ŸçŸ¥å’Œå¤šæ­¥æ§åˆ¶ä»»åŠ¡ä¸­å‡é¢†å…ˆï¼Œè€Œå¼€æºæ¨¡å‹åœ¨æœ€å›°éš¾è®¾ç½®ä¸‹æ¥è¿‘éšæœºæ°´å¹³ï¼›å³ä½¿æœ€ä½³MLLMåœ¨æ›´é«˜é­”æ–¹å¤æ‚åº¦ä¸‹æ€§èƒ½ä¹Ÿä¼šé€€åŒ–ã€‚ç®€å•çš„è‡ªæˆ‘æ ¡æ­£é€šè¿‡åæ€æ€ç»´å¸¦æ¥é€‚åº¦æå‡ï¼Œä½†ä¹Ÿå¯èƒ½å¼•å…¥è¿‡åº¦æ€è€ƒé—®é¢˜ã€‚

**Conclusion:** Cube Benchæä¾›äº†ä¸€ä¸ªç´§å‡‘ã€å¯å¤ç°çš„åºåˆ—ç©ºé—´æ¨ç†è¯„ä¼°æ¡†æ¶ï¼Œæ­ç¤ºäº†MLLMåœ¨å¤æ‚å¤šæ­¥å†³ç­–ä»»åŠ¡ä¸­çš„æ ¹æœ¬å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯é”™è¯¯æ¢å¤èƒ½åŠ›å’Œé•¿æœŸè§„åˆ’èƒ½åŠ›çš„ä¸è¶³ã€‚ç ”ç©¶ç»“æœè¡¨æ˜å½“å‰MLLMåœ¨ç©ºé—´åºåˆ—æ¨ç†æ–¹é¢ä»æœ‰æ˜¾è‘—æå‡ç©ºé—´ï¼Œä¸”é—­æºæ¨¡å‹åœ¨æ­¤ç±»ä»»åŠ¡ä¸Šæ˜æ˜¾ä¼˜äºå¼€æºæ¨¡å‹ï¼Œä¸ºæœªæ¥æ¨¡å‹æ”¹è¿›æä¾›äº†æ˜ç¡®çš„è¯„ä¼°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [31] [Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification](https://arxiv.org/abs/2512.19957)
*Luciano Araujo Dourado Filho, Almir Moreira da Silva Neto, Rodrigo Pereira David, Rodrigo Tripodi Calumby*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºPlantClef 2025ç»†ç²’åº¦å¤šæ ‡ç­¾ç‰©ç§è¯†åˆ«æŒ‘æˆ˜çš„æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨è®­ç»ƒæ•°æ®é›†ä¸­çš„ç±»åˆ«åŸå‹ä½œä¸ºä»£ç†æŒ‡å¯¼ï¼Œè®­ç»ƒåˆ†å‰²è§†è§‰Transformeråœ¨æµ‹è¯•é›†å›¾åƒä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”ï¼Œæœ€ç»ˆåœ¨ç«èµ›ä¸­è·å¾—ç¬¬äº”åã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³PlantClef 2025æŒ‘æˆ˜ä¸­çš„ç»†ç²’åº¦å¤šæ ‡ç­¾ç‰©ç§è¯†åˆ«é—®é¢˜ï¼Œè¯¥ä»»åŠ¡éœ€è¦åœ¨åŒ…å«å¤šç§ç‰©ç§çš„é«˜åˆ†è¾¨ç‡æ¤è¢«å›¾åƒä¸­è¿›è¡Œç²¾ç¡®è¯†åˆ«ï¼Œæ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå¦‚ä½•ä»å¤šç±»åˆ«ä¸ªä½“ç‰©ç§è¯†åˆ«é€‚åº”åˆ°é«˜åˆ†è¾¨ç‡æ¤è¢«åœ°å—çš„å¤šæ ‡ç­¾åˆ†ç±»ã€‚

**Method:** æ–¹æ³•é‡‡ç”¨è®­ç»ƒæ•°æ®é›†ä¸­æå–çš„ç±»åˆ«åŸå‹ä½œä¸ºä»£ç†æŒ‡å¯¼ï¼Œé€šè¿‡K-Meansèšç±»ï¼ˆKç­‰äºæ•°æ®é›†ç±»åˆ«æ•°ï¼‰ä»è®­ç»ƒå›¾åƒç‰¹å¾ä¸­åˆ›å»ºç±»åˆ«è¡¨ç¤ºï¼Œæ„å»ºäº†ä¸€ä¸ªå®šåˆ¶åŒ–çš„çª„è§†è§‰Transformerï¼Œç”¨å†»ç»“çš„DinoV2æ›¿æ¢äº†åŸå§‹è¡¥ä¸åµŒå…¥å±‚ï¼Œè¯¥æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸ªä½“ç‰©ç§åˆ†ç±»çš„é¢„è®­ç»ƒï¼Œç„¶åè®­ç»ƒè¯¥æ¨¡å‹ä»æµ‹è¯•æ•°æ®é›†å›¾åƒä¸­é‡å»ºè®­ç»ƒæ•°æ®é›†çš„ç±»åˆ«åŸå‹ï¼Œåˆ©ç”¨è·å¾—çš„æ³¨æ„åŠ›åˆ†æ•°æ¥è¯†åˆ«å’Œå®šä½æ„Ÿå…´è¶£åŒºåŸŸä»¥æŒ‡å¯¼åˆ†ç±»è¿‡ç¨‹ã€‚

**Result:** è¯¥æ–¹æ³•åœ¨PlantCLEF 2025æŒ‘æˆ˜èµ›çš„ç§æœ‰æ’è¡Œæ¦œä¸Šè·å¾—ç¬¬äº”åï¼ŒF1åˆ†æ•°ä¸º0.33331ï¼Œä¸æœ€ä½³æäº¤ç»“æœä»…ç›¸å·®0.03åˆ†ï¼Œè¡¨æ˜åœ¨åŸºå‡†ä»»åŠ¡ä¸­å…·æœ‰ç«äº‰åŠ›ï¼Œä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚

**Conclusion:** ç ”ç©¶è¯æ˜äº†ä½¿ç”¨ç±»åˆ«åŸå‹ä½œä¸ºä»£ç†æŒ‡å¯¼è¿›è¡Œé¢†åŸŸè‡ªé€‚åº”çš„æœ‰æ•ˆæ€§ï¼ŒæˆåŠŸå°†å¤šç±»åˆ«ä¸ªä½“ç‰©ç§è¯†åˆ«é€‚åº”åˆ°é«˜åˆ†è¾¨ç‡æ¤è¢«åœ°å—çš„å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œè¯¥æ–¹æ³•åœ¨ç»†ç²’åº¦æ¤ç‰©è¯†åˆ«ä»»åŠ¡ä¸­å±•ç°å‡ºç«äº‰æ€§èƒ½ï¼Œä¸ºç±»ä¼¼çš„å¤šæ ‡ç­¾è§†è§‰è¯†åˆ«é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆæ€è·¯ã€‚

---

#### ğŸ“„ Abstract
This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.


### [32] [Reason2Decide: Rationale-Driven Multi-Task Learning](https://arxiv.org/abs/2512.20074)
*H M Quamran Hasan, Housam Khalifa Bashier, Jiayi Dai, Mi-Young Kim, Randy Goebel*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†Reason2Decideæ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè§£å†³ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿä¸­é¢„æµ‹ä¸è§£é‡Šå¯¹é½çš„é—®é¢˜ï¼Œåœ¨æ˜¾è‘—å‡å°æ¨¡å‹è§„æ¨¡çš„åŒæ—¶å®ç°äº†é«˜æ€§èƒ½çš„å¯è§£é‡Šå†³ç­–æ”¯æŒã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿé¢ä¸´å…³é”®æŒ‘æˆ˜ï¼šåœ¨å®ç°é«˜é¢„æµ‹å‡†ç¡®æ€§çš„åŒæ—¶ç”Ÿæˆä¸é¢„æµ‹å¯¹é½çš„è§£é‡Šã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨æš´éœ²åå·®é—®é¢˜ï¼Œå¯¼è‡´è§£é‡Šä¸é¢„æµ‹ä¸ä¸€è‡´ï¼Œéœ€è¦è§£å†³è‡ªè§£é‡Šä»»åŠ¡ä¸­çš„æš´éœ²åå·®å’Œä»»åŠ¡åˆ†ç¦»ç­‰å…³é”®æŒ‘æˆ˜ã€‚

**Method:** æå‡ºReason2Decideä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µè®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†ç”Ÿæˆï¼›ç¬¬äºŒé˜¶æ®µè”åˆè®­ç»ƒæ ‡ç­¾é¢„æµ‹å’Œæ¨ç†ç”Ÿæˆï¼Œåº”ç”¨è®¡åˆ’é‡‡æ ·æŠ€æœ¯é€æ­¥ä»åŸºäºé»„é‡‘æ ‡ç­¾çš„æ¡ä»¶è½¬æ¢åˆ°åŸºäºæ¨¡å‹é¢„æµ‹çš„æ¡ä»¶ï¼Œæœ‰æ•ˆè§£å†³æš´éœ²åå·®é—®é¢˜ã€‚

**Result:** åœ¨ä¸‰ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒReason2Decideåœ¨é¢„æµ‹æ€§èƒ½ï¼ˆF1ï¼‰å’Œæ¨ç†ä¿çœŸåº¦ï¼ˆBERTScoreã€BLEUã€LLM-as-a-Judgeï¼‰æ–¹é¢ä¼˜äºå…¶ä»–å¾®è°ƒåŸºçº¿å’Œéƒ¨åˆ†é›¶æ ·æœ¬LLMã€‚åœ¨åˆ†è¯Šä»»åŠ¡ä¸­ï¼Œè¯¥æ¡†æ¶å¯¹LLMç”Ÿæˆã€æŠ¤å£«æ’°å†™å’ŒæŠ¤å£«åå¤„ç†çš„æ¨ç†å‡è¡¨ç°å‡ºé²æ£’æ€§ï¼Œä¸”ä»…ä½¿ç”¨LLMç”Ÿæˆæ¨ç†è¿›è¡Œç¬¬ä¸€é˜¶æ®µè®­ç»ƒå°±èƒ½è¶…è¶Šå…¶ä»–å˜ä½“ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜LLMç”Ÿæˆçš„æ¨ç†é€‚åˆç”¨äºæ¨¡å‹é¢„è®­ç»ƒï¼Œå‡å°‘å¯¹äººç±»æ ‡æ³¨çš„ä¾èµ–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒReason2Decideä½¿ç”¨æ¯”å½“ä»£åŸºç¡€æ¨¡å‹å°40å€çš„æ¨¡å‹å®ç°äº†è¿™äº›æ€§èƒ½æå‡ï¼Œä½¿ä¸´åºŠæ¨ç†åœ¨èµ„æºå—é™çš„éƒ¨ç½²ä¸­æ›´åŠ å¯åŠï¼ŒåŒæ—¶ä»æä¾›å¯è§£é‡Šçš„å†³ç­–æ”¯æŒã€‚

---

#### ğŸ“„ Abstract
Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.


### [33] [Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection](https://arxiv.org/abs/2512.20140)
*Xingyou Yin, Ceyao Zhang, Min Hu, Kai Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ç­–ç•¥ï¼Œé€šè¿‡åœ¨æ—¶é—´åºåˆ—æ•°æ®æ ‡è®°åŒ–å‰æ³¨å…¥å™ªå£°ï¼Œæ¥æå‡å†»ç»“å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä½œä¸ºä¸€ç§æ¨ç†æ—¶å¢å¼ºæ‰‹æ®µï¼Œè¿«ä½¿æ¨¡å‹åŸºäºé²æ£’çš„æ—¶é—´æ¨¡å¼è€Œéè¡¨é¢æ•°å€¼ä¼ªå½±è¿›è¡Œå¤–æ¨ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç ”ç©¶è¯•å›¾åˆ©ç”¨å®Œå…¨å†»ç»“çš„ã€æœªç»å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œä½†å…¶æ€§èƒ½å¯¹è¾“å…¥æ•°æ®çš„æ–‡æœ¬è¡¨ç¤ºæå…¶æ•æ„Ÿï¼Œå› ä¸ºæ¨¡å‹å‚æ•°æ— æ³•é€‚åº”åˆ†å¸ƒåç§»ã€‚è¿™äº›å®Œå…¨å†»ç»“æ¨¡å‹çš„è¡¨ç°è„†å¼±æ€§æˆä¸ºå…³é”®æŒ‘æˆ˜ï¼Œéœ€è¦ä¸€ç§éä¾µå…¥æ€§å¹²é¢„æ¥å…‹æœè¿™ç§è„†å¼±æ€§ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ç­–ç•¥ï¼šåœ¨åŸå§‹æ—¶é—´åºåˆ—æ•°æ®æ ‡è®°åŒ–ä¸ºæ–‡æœ¬è¡¨ç¤ºä¹‹å‰æ³¨å…¥å™ªå£°ã€‚è¿™ç§éä¾µå…¥æ€§å¹²é¢„ä½œä¸ºä¸€ç§æ¨ç†æ—¶å¢å¼ºæ‰‹æ®µï¼Œè¿«ä½¿å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºäºé²æ£’çš„åŸºç¡€æ—¶é—´æ¨¡å¼è€Œéè¡¨é¢çš„æ•°å€¼ä¼ªå½±è¿›è¡Œå¤–æ¨ã€‚è¯¥æ–¹æ³•ä¸æ¶‰åŠä»»ä½•æ¨¡å‹å¾®è°ƒï¼Œä»…é€šè¿‡ç­–ç•¥æ€§çš„æ•°æ®é¢„å¤„ç†æ¥æå‡æ€§èƒ½ã€‚

**Result:** è¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–çš„åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†å®è¯éªŒè¯ï¼Œå¹¶å§‹ç»ˆè§‚å¯Ÿåˆ°æ€§èƒ½æå‡ã€‚ä¸ºäº†å®Œå…¨æ¶ˆé™¤å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæœŸé—´æ•°æ®æ±¡æŸ“å¯èƒ½å¸¦æ¥çš„åå·®ï¼Œç ”ç©¶å¼•å…¥äº†ä¸¤ä¸ªæ–°é¢–çš„æ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å®Œå…¨è¶…å‡ºäº†æ‰€æœ‰ä½¿ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒèŒƒå›´ï¼Œå¹¶åœ¨è¿™äº›æ•°æ®é›†ä¸Šä¸€è‡´è§‚å¯Ÿåˆ°æ”¹è¿›çš„æ€§èƒ½ã€‚

**Conclusion:** è¿™é¡¹ç ”ç©¶ä¸ºç›´æ¥åˆ©ç”¨ç°æˆçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹æä¾›äº†è¿›ä¸€æ­¥çš„è¿›å±•ã€‚å™ªå£°æ³¨å…¥ä½œä¸ºä¸€ç§æ¨ç†æ—¶å¢å¼ºæ‰‹æ®µï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å†»ç»“æ¨¡å‹å¯¹æ—¶é—´åºåˆ—æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºåˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¤„ç†æ•°å€¼åºåˆ—ä»»åŠ¡æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•è®ºå¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.


### [34] [TongSIM: A General Platform for Simulating Intelligent Machines](https://arxiv.org/abs/2512.20206)
*Zhe Sun, Kunlun Wu, Chuanjian Fu, Zeming Song, Langyong Shi, Zihe Xue, Bohan Jing, Ying Yang, Xiaomeng Gao, Aijia Li, Tianyu Guo, Huiying Li, Xueyuan Yang, Rongkai Liu, Xinyi He, Yuxi Wang, Yue Li, Mingyuan Liu, Yujie Lu, Hongzhao Xie, Shiyun Zhao, Bo Dai, Wei Wang, Tao Yuan, Song-Chun Zhu, Yujia Peng, Zhenliang Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡ä»‹ç»äº†TongSIMï¼Œä¸€ä¸ªç”¨äºè®­ç»ƒå’Œè¯„ä¼°å…·èº«æ™ºèƒ½ä½“çš„é«˜ä¿çœŸé€šç”¨å¹³å°ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ä»¿çœŸç¯å¢ƒå±€é™äºç‰¹å®šä»»åŠ¡ã€ç¼ºä¹é€šç”¨è®­ç»ƒå¹³å°çš„é—®é¢˜ï¼Œé€šè¿‡æä¾›å¤šæ ·åŒ–çš„å®¤å†…å¤–åœºæ™¯å’Œç»¼åˆè¯„ä¼°æ¡†æ¶æ¥åŠ é€Ÿé€šç”¨å…·èº«æ™ºèƒ½çš„å‘å±•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€äººå·¥æ™ºèƒ½ç‰¹åˆ«æ˜¯å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œç ”ç©¶é‡ç‚¹æ­£ä»å•æ¨¡æ€æ–‡æœ¬å¤„ç†è½¬å‘æ›´å¤æ‚çš„å¤šæ¨¡æ€å’Œå…·èº«æ™ºèƒ½é¢†åŸŸã€‚ç„¶è€Œï¼Œç°æœ‰ä»¿çœŸå¹³å°å¤§å¤šè®¾è®¡ç‹­çª„ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡å®šåˆ¶ï¼Œç¼ºä¹ä¸€ä¸ªèƒ½å¤Ÿæ”¯æŒä»ä½çº§å…·èº«å¯¼èˆªåˆ°é«˜çº§å¤åˆæ´»åŠ¨ï¼ˆå¦‚å¤šæ™ºèƒ½ä½“ç¤¾ä¼šä»¿çœŸå’Œäººæœºåä½œï¼‰çš„é€šç”¨è®­ç»ƒç¯å¢ƒã€‚

**Method:** TongSIMå¹³å°æä¾›äº†é«˜ä¿çœŸã€é€šç”¨çš„å…·èº«æ™ºèƒ½ä½“è®­ç»ƒå’Œè¯„ä¼°ç¯å¢ƒï¼ŒåŒ…å«è¶…è¿‡100ä¸ªå¤šæ ·åŒ–çš„å¤šæˆ¿é—´å®¤å†…åœºæ™¯ä»¥åŠä¸€ä¸ªå¼€æ”¾å¼ã€äº¤äº’ä¸°å¯Œçš„æˆ·å¤–åŸé•‡ä»¿çœŸã€‚å¹³å°å…·å¤‡å®šåˆ¶åœºæ™¯ã€ä»»åŠ¡è‡ªé€‚åº”ä¿çœŸåº¦ã€å¤šæ ·åŒ–æ™ºèƒ½ä½“ç±»å‹å’ŒåŠ¨æ€ç¯å¢ƒä»¿çœŸç­‰ç‰¹å¾ï¼Œæä¾›äº†å…¨é¢çš„è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ã€‚

**Result:** TongSIMå¹³å°å®ç°äº†å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œèƒ½å¤Ÿæ”¯æŒæ„ŸçŸ¥ã€è®¤çŸ¥ã€å†³ç­–ã€äººæœºåä½œä»¥åŠç©ºé—´å’Œç¤¾ä¼šæ¨ç†ç­‰å¤šç§èƒ½åŠ›è¯„ä¼°ã€‚å…¶çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ç»Ÿä¸€çš„å¹³å°ï¼ŒåŠ é€Ÿäº†è®­ç»ƒã€è¯„ä¼°å’Œé€šç”¨å…·èº«æ™ºèƒ½çš„å‘å±•è¿›ç¨‹ã€‚

**Conclusion:** TongSIMä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„é€šç”¨å¹³å°ï¼Œå¡«è¡¥äº†ç°æœ‰ä»¿çœŸç¯å¢ƒå±€é™äºç‰¹å®šä»»åŠ¡çš„ç©ºç™½ï¼Œé€šè¿‡æä¾›å¤šæ ·åŒ–çš„å®¤å†…å¤–åœºæ™¯å’Œç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œä¸ºåŠ é€Ÿé€šç”¨å…·èº«æ™ºèƒ½çš„ç ”ç©¶å’Œå‘å±•æä¾›äº†é‡è¦åŸºç¡€è®¾æ–½ã€‚è¯¥å¹³å°çš„è®¾è®¡å¼ºè°ƒçµæ´»æ€§ã€å¯æ‰©å±•æ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ï¼Œæœ‰æœ›æ¨åŠ¨å…·èº«æ™ºèƒ½ä»ç‰¹å®šä»»åŠ¡å‘é€šç”¨èƒ½åŠ›çš„è½¬å˜ã€‚

---

#### ğŸ“„ Abstract
As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.


### [35] [ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge](https://arxiv.org/abs/2512.20276)
*Yuntao Dai, Hang Gu, Teng Wang, Qianyu Cheng, Yifei Zheng, Zhiyong Qiu, Lei Gong, Wenqi Lou, Xuehai Zhou*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ActionFlowï¼Œä¸€ç§é¢å‘èµ„æºå—é™è¾¹ç¼˜å¹³å°çš„ç³»ç»Ÿçº§æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡è·¨è¯·æ±‚æµæ°´çº¿è°ƒåº¦ç­–ç•¥å’Œå†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œå°†VLAæ¨¡å‹çš„æ¨ç†å»¶è¿Ÿæ˜¾è‘—é™ä½ï¼Œå®ç°äº†å®æ—¶åŠ¨æ€æ“ä½œã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨åŠ¨æ€çœŸå®ç¯å¢ƒä¸­çš„éƒ¨ç½²å—åˆ°é«˜æ¨ç†å»¶è¿Ÿçš„ä¸¥é‡é˜»ç¢ï¼Œè™½ç„¶æµç•…çš„æœºå™¨äººäº¤äº’éœ€è¦20-30Hzçš„æ§åˆ¶é¢‘ç‡ï¼Œä½†å½“å‰VLAæ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šé€šå¸¸åªèƒ½ä»¥3-5Hzè¿è¡Œï¼Œè¿™ä¸»è¦æºäºè‡ªå›å½’è§£ç çš„å†…å­˜å—é™ç‰¹æ€§ï¼Œç°æœ‰ä¼˜åŒ–æ–¹æ³•å¾€å¾€éœ€è¦å¤§é‡é‡æ–°è®­ç»ƒæˆ–ä¼šæŸå®³æ¨¡å‹ç²¾åº¦ã€‚

**Method:** ActionFlowçš„æ ¸å¿ƒæ˜¯è·¨è¯·æ±‚æµæ°´çº¿ç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è°ƒåº¦å™¨ï¼Œå°†VLAæ¨ç†é‡æ–°å®šä¹‰ä¸ºå¾®è¯·æ±‚çš„å®æµæ°´çº¿ï¼Œè¯¥ç­–ç•¥æ™ºèƒ½åœ°å°†å†…å­˜å—é™çš„è§£ç é˜¶æ®µä¸è®¡ç®—å—é™çš„é¢„å¡«å……é˜¶æ®µåœ¨è¿ç»­æ—¶é—´æ­¥ä¸Šè¿›è¡Œæ‰¹å¤„ç†ä»¥æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡ï¼Œæ­¤å¤–è¿˜æå‡ºäº†è·¨è¯·æ±‚çŠ¶æ€æ‰“åŒ…å‰å‘ç®—å­å’Œç»Ÿä¸€KVç¯å½¢ç¼“å†²åŒºï¼Œå°†ç¢ç‰‡åŒ–çš„å†…å­˜æ“ä½œèåˆä¸ºé«˜æ•ˆçš„å¯†é›†è®¡ç®—ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒActionFlowåœ¨OpenVLA-7Bæ¨¡å‹ä¸Šå®ç°äº†2.55å€çš„FPSæå‡ï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒï¼Œè¿™ä½¿å¾—åœ¨è¾¹ç¼˜ç¡¬ä»¶ä¸Šå®ç°å®æ—¶åŠ¨æ€æ“ä½œæˆä¸ºå¯èƒ½ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æ”¹å–„äº†VLAæ¨¡å‹åœ¨èµ„æºå—é™å¹³å°ä¸Šçš„æ¨ç†æ•ˆç‡ã€‚

**Conclusion:** ActionFlowé€šè¿‡ç³»ç»Ÿçº§ä¼˜åŒ–æˆåŠŸè§£å†³äº†VLAæ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å»¶è¿Ÿç“¶é¢ˆï¼Œå…¶è·¨è¯·æ±‚è°ƒåº¦å’Œå†…å­˜ä¼˜åŒ–æŠ€æœ¯ä¸ºå®æ—¶æœºå™¨äººæ§åˆ¶æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œè¿™é¡¹å·¥ä½œä¸ºåœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå¹¶å±•ç¤ºäº†ç³»ç»Ÿçº§ä¼˜åŒ–åœ¨æå‡AIæ¨¡å‹å®é™…éƒ¨ç½²æ•ˆç‡æ–¹é¢çš„é‡è¦ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.


### [36] [A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice](https://arxiv.org/abs/2512.20344)
*Yaowei Bai, Ruiheng Zhang, Yu Lei, Xuhua Duan, Jingfeng Yao, Shuguang Ju, Chaoyang Wang, Wei Yao, Yiwan Guo, Guilin Zhang, Chao Wan, Qian Yuan, Lei Chen, Wenjuan Tang, Biqiang Zhu, Xinggang Wang, Tao Sun, Wei Zhou, Dacheng Tao, Yongchao Xu, Chuansheng Zheng, Huangxuan Zhao, Bo Du*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶å¼€å‘äº†Janus-Pro-CXRï¼ˆ1Bï¼‰èƒ¸éƒ¨Xå…‰è§£è¯»ç³»ç»Ÿï¼Œé€šè¿‡å¤šä¸­å¿ƒå‰ç»æ€§ä¸´åºŠè¯•éªŒéªŒè¯å…¶åœ¨æŠ¥å‘Šç”Ÿæˆå’Œå…³é”®æ”¾å°„å­¦å‘ç°æ£€æµ‹æ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—æå‡è¯Šæ–­å¯é æ€§å’Œå·¥ä½œæµç¨‹æ•ˆç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å…¨çƒæ”¾å°„ç§‘åŒ»ç”ŸçŸ­ç¼ºé—®é¢˜å› èƒ¸éƒ¨Xå…‰å·¥ä½œé‡å·¨å¤§è€ŒåŠ å‰§ï¼Œç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸»è¦ä¾èµ–è‡ªåŠ¨åŒ–æŒ‡æ ‡æˆ–å›é¡¾æ€§åˆ†æï¼Œç¼ºä¹ä¸¥æ ¼çš„å‰ç»æ€§ä¸´åºŠéªŒè¯ï¼Œéœ€è¦å¼€å‘ç»è¿‡ä¸´åºŠéªŒè¯çš„AIè¾…åŠ©æ”¾å°„å­¦è§£å†³æ–¹æ¡ˆã€‚

**Method:** åŸºäºDeepSeek Janus-Proæ¨¡å‹å¼€å‘äº†Janus-Pro-CXRï¼ˆ1Bï¼‰èƒ¸éƒ¨Xå…‰è§£è¯»ç³»ç»Ÿï¼Œé‡‡ç”¨è½»é‡çº§æ¶æ„å’Œé¢†åŸŸç‰¹å®šä¼˜åŒ–ï¼Œé€šè¿‡å¤šä¸­å¿ƒå‰ç»æ€§ä¸´åºŠè¯•éªŒï¼ˆNCT07117266ï¼‰è¿›è¡Œä¸¥æ ¼éªŒè¯ï¼Œå¹¶ä¸åŒ…æ‹¬ChatGPT 4oï¼ˆ200Bå‚æ•°ï¼‰åœ¨å†…çš„æœ€å…ˆè¿›æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚

**Result:** Janus-Pro-CXRåœ¨è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ï¼ŒåŒ…æ‹¬æ›´å¤§è§„æ¨¡çš„ChatGPT 4oï¼Œå¯é æ£€æµ‹å…­ç§ä¸´åºŠå…³é”®æ”¾å°„å­¦å‘ç°ï¼›å‰ç»æ€§ä¸´åºŠéƒ¨ç½²ä¸­ï¼ŒAIè¾…åŠ©æ˜¾è‘—æé«˜æŠ¥å‘Šè´¨é‡è¯„åˆ†ï¼Œå‡å°‘18.3%è§£è¯»æ—¶é—´ï¼ˆP<0.001ï¼‰ï¼Œ54.3%ç—…ä¾‹ä¸­ä¸“å®¶æ›´åå¥½AIè¾…åŠ©ç»“æœã€‚

**Conclusion:** Janus-Pro-CXRé€šè¿‡è½»é‡çº§æ¶æ„å’Œé¢†åŸŸä¼˜åŒ–æ˜¾è‘—æå‡è¯Šæ–­å¯é æ€§å’Œå·¥ä½œæµç¨‹æ•ˆç‡ï¼Œç‰¹åˆ«é€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒï¼›æ¨¡å‹æ¶æ„å’Œå®æ–½æ¡†æ¶å°†å¼€æºä»¥ä¿ƒè¿›AIè¾…åŠ©æ”¾å°„å­¦è§£å†³æ–¹æ¡ˆçš„ä¸´åºŠè½¬åŒ–ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸçš„å®é™…åº”ç”¨æä¾›äº†éªŒè¯èŒƒä¾‹ã€‚

---

#### ğŸ“„ Abstract
A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.


### [37] [Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems](https://arxiv.org/abs/2512.20387)
*YuChe Hsu, AnJui Wang, TsaiChing Ni, YuanFu Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§è§†è§‰è¯­è¨€ä»¿çœŸæ¨¡å‹ï¼ˆVLSMï¼‰ï¼Œé€šè¿‡ç»Ÿä¸€è§†è§‰å’Œæ–‡æœ¬ç†è§£ä»å¸ƒå±€è‰å›¾å’Œè‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆå¯æ‰§è¡Œçš„FlexScriptï¼Œä¸ºå·¥ä¸šä»¿çœŸç³»ç»Ÿå»ºç«‹äº†ç”Ÿæˆå¼æ•°å­—å­ªç”Ÿçš„æ–°èŒƒå¼ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å·¥ä¸šä»¿çœŸç³»ç»Ÿä¸­è·¨æ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜ï¼Œå³å¦‚ä½•å°†è§†è§‰å¸ƒå±€è‰å›¾ä¸è‡ªç„¶è¯­è¨€æè¿°ç›¸ç»“åˆï¼Œç”Ÿæˆå¯ç›´æ¥æ‰§è¡Œçš„ä»¿çœŸé€»è¾‘ä»£ç ï¼Œä»è€Œå¡«è¡¥ç”Ÿæˆå¼æ•°å­—å­ªç”Ÿé¢†åŸŸç¼ºä¹ç»Ÿä¸€è§†è§‰-è¯­è¨€-ä»£ç å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶çš„ç ”ç©¶ç©ºç™½ã€‚

**Method:** ç ”ç©¶æå‡ºäº†è§†è§‰è¯­è¨€ä»¿çœŸæ¨¡å‹ï¼ˆVLSMï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€çš„æ¶æ„æ•´åˆè§†è§‰ç¼–ç å™¨ã€è¿æ¥å™¨å’Œä»£ç é¢„è®­ç»ƒè¯­è¨€éª¨å¹²ç½‘ç»œï¼Œèƒ½å¤Ÿä»å¸ƒå±€è‰å›¾å’Œè‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆå¯æ‰§è¡Œçš„FlexScriptä»£ç ï¼›åŒæ—¶æ„å»ºäº†åŒ…å«è¶…è¿‡12ä¸‡æ¡æç¤º-è‰å›¾-ä»£ç ä¸‰å…ƒç»„çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶ä¸“é—¨è®¾è®¡äº†ç»“æ„æœ‰æ•ˆæ€§ç‡ï¼ˆSVRï¼‰ã€å‚æ•°åŒ¹é…ç‡ï¼ˆPMRï¼‰å’Œæ‰§è¡ŒæˆåŠŸç‡ï¼ˆESRï¼‰ä¸‰ä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚

**Result:** é€šè¿‡ç³»ç»Ÿæ€§çš„æ¶ˆèå®éªŒï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨è§†è§‰ç¼–ç å™¨ã€è¿æ¥å™¨å’Œä»£ç é¢„è®­ç»ƒè¯­è¨€éª¨å¹²ç½‘ç»œçš„ä¸åŒé…ç½®ä¸‹ï¼Œå®ç°äº†æ¥è¿‘å®Œç¾çš„ç»“æ„å‡†ç¡®æ€§å’Œé«˜åº¦çš„æ‰§è¡Œé²æ£’æ€§ï¼›ä¸‰ä¸ªä¸“é—¨è®¾è®¡çš„è¯„ä¼°æŒ‡æ ‡å…¨é¢éªŒè¯äº†ç”Ÿæˆä»£ç çš„ç»“æ„å®Œæ•´æ€§ã€å‚æ•°ä¿çœŸåº¦å’Œä»¿çœŸå™¨å¯æ‰§è¡Œæ€§ã€‚

**Conclusion:** è¿™é¡¹å·¥ä½œä¸ºç”Ÿæˆå¼æ•°å­—å­ªç”Ÿå¥ å®šäº†é‡è¦åŸºç¡€ï¼Œå±•ç¤ºäº†å°†è§†è§‰æ¨ç†å’Œè¯­è¨€ç†è§£æ•´åˆåˆ°å¯æ‰§è¡Œå·¥ä¸šä»¿çœŸç³»ç»Ÿä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥å·¥ä¸šè‡ªåŠ¨åŒ–ã€æ™ºèƒ½åˆ¶é€ å’Œæ•°å­—å­ªç”Ÿç³»ç»Ÿçš„æ™ºèƒ½åŒ–å‘å±•æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’Œç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.


### [38] [Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model](https://arxiv.org/abs/2512.20548)
*Zhiyi Duan, Xiangren Wang, Hongyu Yuan, Qianli Xing*

#### ğŸ§© TL;DR
æœ¬æ–‡æ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡æ•™å¸ˆå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ•°æ®é›†T-MEDï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºéå¯¹ç§°æ³¨æ„åŠ›çš„å¤šæ¨¡æ€æ•™å¸ˆæƒ…æ„Ÿåˆ†ææ¨¡å‹AAM-TSAï¼Œæ˜¾è‘—æå‡äº†æ•™å¸ˆæƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç ”ç©¶å¾€å¾€æ— æ³•å‡†ç¡®æ•æ‰æ•™å¸ˆæƒ…æ„Ÿï¼Œè¿™æ—¢å› ä¸ºæ•™å¸ˆæƒ…æ„Ÿå…·æœ‰è¡¨æ¼”æ€§ç‰¹å¾ï¼Œä¹Ÿå› ä¸ºå¿½è§†äº†æ•™å­¦ä¿¡æ¯å¯¹æƒ…æ„Ÿè¡¨è¾¾çš„å…³é”®å½±å“ï¼Œå¯¼è‡´åœ¨çœŸå®æ•™è‚²åœºæ™¯ä¸­æ•™å¸ˆæƒ…æ„Ÿåˆ†ææ•ˆæœä¸ä½³ã€‚

**Method:** ç ”ç©¶é¦–å…ˆæ„å»ºäº†åŒ…å«14,938ä¸ªå®ä¾‹çš„æ•™å¸ˆå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ•°æ®é›†T-MEDï¼Œæ¶µç›–11ä¸ªå­¦ç§‘ä»K-12åˆ°é«˜ç­‰æ•™è‚²çš„250ä¸ªçœŸå®è¯¾å ‚ï¼Œæ•´åˆäº†æ–‡æœ¬ã€éŸ³é¢‘ã€è§†é¢‘å’Œæ•™å­¦ä¿¡æ¯ç­‰å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶é‡‡ç”¨äººæœºååŒæ ‡æ³¨æµç¨‹ç¡®ä¿æ ‡æ³¨è´¨é‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†åŸºäºéå¯¹ç§°æ³¨æ„åŠ›çš„å¤šæ¨¡æ€æ•™å¸ˆæƒ…æ„Ÿåˆ†ææ¨¡å‹AAM-TSAï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†éå¯¹ç§°æ³¨æ„åŠ›æœºåˆ¶å’Œåˆ†å±‚é—¨æ§å•å…ƒï¼Œå®ç°äº†å·®å¼‚åŒ–çš„è·¨æ¨¡æ€ç‰¹å¾èåˆå’Œç²¾ç¡®çš„æƒ…æ„Ÿåˆ†ç±»ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒAAM-TSAæ¨¡å‹åœ¨T-MEDæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•åœ¨æ•™å¸ˆæƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** æœ¬ç ”ç©¶é€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†å’Œæå‡ºåˆ›æ–°æ¨¡å‹ï¼Œä¸ºæ•™å¸ˆæƒ…æ„Ÿåˆ†ææä¾›äº†ç³»ç»Ÿæ€§çš„è§£å†³æ–¹æ¡ˆï¼Œå¼ºè°ƒäº†æ•™å­¦ä¿¡æ¯åœ¨æƒ…æ„Ÿåˆ†æä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæ•™è‚²æƒ…æ„Ÿè®¡ç®—é¢†åŸŸçš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒï¼Œæœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢æ›´ç»†ç²’åº¦çš„æƒ…æ„Ÿåˆ†æå’Œä¸ªæ€§åŒ–æ•™å­¦æ”¯æŒåº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.


### [39] [LongVideoAgent: Multi-Agent Reasoning with Long Videos](https://arxiv.org/abs/2512.20618)
*Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé•¿è§†é¢‘é—®ç­”çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå…¶ä¸­ä¸»æ§LLMåè°ƒå®šä½æ™ºèƒ½ä½“å’Œè§†è§‰æ™ºèƒ½ä½“ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®ç°é«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“åä½œï¼Œæ˜¾è‘—æå‡äº†é•¿è§†é¢‘ç†è§£æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€LLMå’Œé•¿è§†é¢‘é—®ç­”ç³»ç»Ÿåœ¨å¤„ç†å°æ—¶çº§è§†é¢‘æ—¶ï¼Œé€šå¸¸é‡‡ç”¨æœ‰æŸå‹ç¼©æ‘˜è¦æˆ–ä¾èµ–æœ‰é™å·¥å…·é›†ï¼Œè¿™å‰Šå¼±äº†æ—¶é—´å®šä½èƒ½åŠ›å¹¶é—æ¼äº†ç»†ç²’åº¦è§†è§‰çº¿ç´¢ï¼Œéœ€è¦æ›´ç²¾ç¡®çš„æ—¶åºå®šä½å’Œç»†ç²’åº¦è§†è§‰ä¿¡æ¯æå–æ–¹æ³•ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå…¶ä¸­ä¸»æ§LLMåè°ƒä¸¤ä¸ªä¸“é—¨æ™ºèƒ½ä½“ï¼šå®šä½æ™ºèƒ½ä½“è´Ÿè´£å®šä½é—®é¢˜ç›¸å…³è§†é¢‘ç‰‡æ®µï¼Œè§†è§‰æ™ºèƒ½ä½“è´Ÿè´£æå–é’ˆå¯¹æ€§æ–‡æœ¬è§‚å¯Ÿã€‚ä¸»æ§æ™ºèƒ½ä½“é‡‡ç”¨æ­¥æ•°é™åˆ¶è¿›è¡Œè§„åˆ’ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒä»¥ä¿ƒè¿›ç®€æ´ã€æ­£ç¡®ä¸”é«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“åä½œã€‚

**Result:** åœ¨æå‡ºçš„LongTVQAå’ŒLongTVQA+æ•°æ®é›†ï¼ˆåŸºäºTVQA/TVQA+æ„å»ºçš„å‰§é›†çº§æ•°æ®é›†ï¼‰ä¸Šï¼Œè¯¥å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ˜¾è‘—ä¼˜äºå¼ºéæ™ºèƒ½ä½“åŸºçº¿ã€‚å®éªŒè¡¨æ˜å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥å¢å¼ºäº†è®­ç»ƒåæ™ºèƒ½ä½“çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶åœ¨é•¿è§†é¢‘ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ä¸“é—¨æ™ºèƒ½ä½“çš„åˆ†å·¥åä½œå®ç°äº†æ›´å¥½çš„æ—¶åºå®šä½å’Œè§†è§‰ç»†èŠ‚è¡¥å……ï¼Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿›ä¸€æ­¥ä¼˜åŒ–äº†å¤šæ™ºèƒ½ä½“äº¤äº’æ•ˆç‡ï¼Œä¸ºé•¿è§†é¢‘é—®ç­”ç³»ç»Ÿæä¾›äº†å¯è§£é‡Šçš„æ¨ç†è½¨è¿¹ã€‚

---

#### ğŸ“„ Abstract
Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.
