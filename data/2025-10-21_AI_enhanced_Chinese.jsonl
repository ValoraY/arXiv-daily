{"id": "2510.15963", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15963", "abs": "https://arxiv.org/abs/2510.15963", "authors": ["Jiani Huang", "Amish Sethi", "Matthew Kuo", "Mayank Keoliya", "Neelay Velingker", "JungHo Jung", "Ser-Nam Lim", "Ziyang Li", "Mayur Naik"], "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "comment": "Accepted as a Spotlight Paper at NeurIPS 2025", "summary": "Multi-modal large language models (MLLMs) are making rapid progress toward\ngeneral-purpose embodied agents. However, current training pipelines primarily\nrely on high-level vision-sound-text pairs and lack fine-grained, structured\nalignment between pixel-level visual content and textual semantics. To overcome\nthis challenge, we propose ESCA, a new framework for contextualizing embodied\nagents through structured spatial-temporal understanding. At its core is\nSGClip, a novel CLIP-based, open-domain, and promptable model for generating\nscene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic\nlearning pipeline, which harnesses model-driven self-supervision from\nvideo-caption pairs and structured reasoning, thereby eliminating the need for\nhuman-labeled scene graph annotations. We demonstrate that SGClip supports both\nprompt-based inference and task-specific fine-tuning, excelling in scene graph\ngeneration and action localization benchmarks. ESCA with SGClip consistently\nimproves both open-source and commercial MLLMs, achieving state-of-the-art\nperformance across two embodied environments. Notably, it significantly reduces\nagent perception errors and enables open-source models to surpass proprietary\nbaselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ESCA\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u65f6\u7a7a\u7406\u89e3\u6765\u60c5\u5883\u5316\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u5176\u6838\u5fc3\u662fSGClip\u6a21\u578b\u2014\u2014\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u5f00\u653e\u57df\u53ef\u63d0\u793a\u573a\u666f\u56fe\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u4e3b\u8981\u4f9d\u8d56\u9ad8\u5c42\u89c6\u89c9-\u58f0\u97f3-\u6587\u672c\u5bf9\uff0c\u7f3a\u4e4f\u50cf\u7d20\u7ea7\u89c6\u89c9\u5185\u5bb9\u4e0e\u6587\u672c\u8bed\u4e49\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u7ed3\u6784\u5316\u5bf9\u9f50\uff0c\u8fd9\u9650\u5236\u4e86\u5177\u8eab\u667a\u80fd\u4f53\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86ESCA\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662fSGClip\u6a21\u578b\u2014\u2014\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u5f00\u653e\u57df\u53ef\u63d0\u793a\u573a\u666f\u56fe\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u7ba1\u9053\u572887K+\u5f00\u653e\u57df\u89c6\u9891\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5229\u7528\u89c6\u9891-\u5b57\u5e55\u5bf9\u4e2d\u7684\u6a21\u578b\u9a71\u52a8\u81ea\u76d1\u7763\u548c\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u573a\u666f\u56fe\u6ce8\u91ca\u3002", "result": "SGClip\u5728\u573a\u666f\u56fe\u751f\u6210\u548c\u52a8\u4f5c\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cESCA\u6846\u67b6\u6301\u7eed\u63d0\u5347\u4e86\u5f00\u6e90\u548c\u5546\u4e1a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u4e24\u4e2a\u5177\u8eab\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u667a\u80fd\u4f53\u611f\u77e5\u9519\u8bef\u5e76\u4f7f\u5f00\u6e90\u6a21\u578b\u8d85\u8d8a\u4e13\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u7ed3\u6784\u5316\u7a7a\u95f4-\u65f6\u95f4\u7406\u89e3\u5bf9\u4e8e\u5177\u8eab\u667a\u80fd\u4f53\u7684\u91cd\u8981\u6027\uff0c\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.15991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15991", "abs": "https://arxiv.org/abs/2510.15991", "authors": ["Huiming Yang"], "title": "CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection", "comment": "13 pages", "summary": "The sparse cross-modality detector offers more advantages than its\ncounterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of\nadaptability for downstream tasks and computational cost savings. However,\nexisting sparse detectors overlook the quality of token representation, leaving\nit with a sub-optimal foreground quality and limited performance. In this\npaper, we identify that the geometric structure preserved and the class\ndistribution are the key to improving the performance of the sparse detector,\nand propose a Sparse Selector (SS). The core module of SS is Ray-Aware\nSupervision (RAS), which preserves rich geometric information during the\ntraining stage, and Class-Balanced Supervision, which adaptively reweights the\nsalience of class semantics, ensuring that tokens associated with small objects\nare retained during token sampling. Thereby, outperforming other sparse\nmulti-modal detectors in the representation of tokens. Additionally, we design\nRay Positional Encoding (Ray PE) to address the distribution differences\nbetween the LiDAR modality and the image. Finally, we integrate the\naforementioned module into an end-to-end sparse multi-modality detector, dubbed\nCrossRay3D. Experiments show that, on the challenging nuScenes benchmark,\nCrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,\nwhile running 1.84 faster than other leading methods. Moreover, CrossRay3D\ndemonstrates strong robustness even in scenarios where LiDAR or camera data are\npartially or entirely missing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CrossRay3D\u7a00\u758f\u591a\u6a21\u6001\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u5c04\u7ebf\u611f\u77e5\u76d1\u7763\u548c\u7c7b\u522b\u5e73\u8861\u76d1\u7763\u673a\u5236\u63d0\u5347token\u8868\u793a\u8d28\u91cf\uff0c\u5728nuScenes\u57fa\u51c6\u4e0a\u5b9e\u73b072.4 mAP\u548c74.7 NDS\u7684SOTA\u6027\u80fd\uff0c\u540c\u65f6\u8fd0\u884c\u901f\u5ea6\u63d0\u53471.84\u500d\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u68c0\u6d4b\u5668\u5ffd\u89c6\u4e86token\u8868\u793a\u8d28\u91cf\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u524d\u666f\u8d28\u91cf\u6b21\u4f18\u4e14\u6027\u80fd\u53d7\u9650\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u5bf9\u51e0\u4f55\u7ed3\u6784\u4fdd\u6301\u548c\u7c7b\u522b\u5206\u5e03\u7684\u5173\u952e\u5173\u6ce8\u3002", "method": "\u63d0\u51fa\u7a00\u758f\u9009\u62e9\u5668\uff08SS\uff09\u6838\u5fc3\u6a21\u5757\uff0c\u5305\u62ec\u5c04\u7ebf\u611f\u77e5\u76d1\u7763\uff08RAS\uff09\u5728\u8bad\u7ec3\u9636\u6bb5\u4fdd\u6301\u4e30\u5bcc\u51e0\u4f55\u4fe1\u606f\uff0c\u7c7b\u522b\u5e73\u8861\u76d1\u7763\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u7c7b\u522b\u8bed\u4e49\u663e\u8457\u6027\uff0c\u4ee5\u53ca\u5c04\u7ebf\u4f4d\u7f6e\u7f16\u7801\uff08Ray PE\uff09\u89e3\u51b3LiDAR\u4e0e\u56fe\u50cf\u6a21\u6001\u95f4\u7684\u5206\u5e03\u5dee\u5f02\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCrossRay3D\u8fbe\u523072.4 mAP\u548c74.7 NDS\u7684SOTA\u6027\u80fd\uff0c\u8fd0\u884c\u901f\u5ea6\u6bd4\u5176\u4ed6\u9886\u5148\u65b9\u6cd5\u5feb1.84\u500d\uff0c\u4e14\u5728LiDAR\u6216\u76f8\u673a\u6570\u636e\u90e8\u5206\u6216\u5b8c\u5168\u7f3a\u5931\u7684\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u51e0\u4f55\u7ed3\u6784\u4fdd\u6301\u548c\u7c7b\u522b\u5206\u5e03\u5e73\u8861\u662f\u63d0\u5347\u7a00\u758f\u68c0\u6d4b\u5668\u6027\u80fd\u7684\u5173\u952e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2510.16017", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16017", "abs": "https://arxiv.org/abs/2510.16017", "authors": ["Ibrahim Sheikh Mohamed", "Abdullah Yahya Abdullah Omaisan"], "title": "InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects", "comment": null, "summary": "Infrastructure in smart cities is increasingly monitored by networks of\nclosed circuit television (CCTV) cameras. Roads, bridges and tunnels develop\ncracks, potholes, and fluid leaks that threaten public safety and require\ntimely repair. Manual inspection is costly and hazardous, and existing\nautomatic systems typically address individual defect types or provide\nunstructured outputs that cannot directly guide maintenance crews. This paper\nproposes a comprehensive pipeline that leverages street CCTV streams for multi\ndefect detection and segmentation using the YOLO family of object detectors and\npasses the detections to a vision language model (VLM) for scene aware\nsummarization. The VLM generates a structured action plan in JSON format that\nincludes incident descriptions, recommended tools, dimensions, repair plans,\nand urgent alerts. We review literature on pothole, crack and leak detection,\nhighlight recent advances in large vision language models such as QwenVL and\nLLaVA, and describe the design of our early prototype. Experimental evaluation\non public datasets and captured CCTV clips demonstrates that the system\naccurately identifies diverse defects and produces coherent summaries. We\nconclude by discussing challenges and directions for scaling the system to city\nwide deployments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eCCTV\u76d1\u63a7\u89c6\u9891\u7684\u667a\u80fd\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7f3a\u9677\u68c0\u6d4b\u4e0e\u4fee\u590d\u89c4\u5212\u7cfb\u7edf\uff0c\u7ed3\u5408YOLO\u76ee\u6807\u68c0\u6d4b\u5668\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u591a\u79cd\u7f3a\u9677\u5e76\u751f\u6210\u7ed3\u6784\u5316\u7684\u7ef4\u62a4\u884c\u52a8\u8ba1\u5212\u3002", "motivation": "\u667a\u80fd\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u76d1\u63a7\u9762\u4e34\u624b\u52a8\u68c0\u67e5\u6210\u672c\u9ad8\u3001\u5371\u9669\u6027\u5927\uff0c\u73b0\u6709\u81ea\u52a8\u7cfb\u7edf\u901a\u5e38\u53ea\u80fd\u5904\u7406\u5355\u4e00\u7f3a\u9677\u7c7b\u578b\u6216\u8f93\u51fa\u975e\u7ed3\u6784\u5316\u7ed3\u679c\uff0c\u65e0\u6cd5\u76f4\u63a5\u6307\u5bfc\u7ef4\u62a4\u56e2\u961f\u8fdb\u884c\u4fee\u590d\u5de5\u4f5c\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5168\u9762\u68c0\u6d4b\u591a\u79cd\u7f3a\u9677\u5e76\u751f\u6210\u7ed3\u6784\u5316\u7ef4\u62a4\u8ba1\u5212\u7684\u4e00\u4f53\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528YOLO\u7cfb\u5217\u76ee\u6807\u68c0\u6d4b\u5668\u8fdb\u884c\u591a\u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u7136\u540e\u5c06\u68c0\u6d4b\u7ed3\u679c\u4f20\u9012\u7ed9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u573a\u666f\u611f\u77e5\u603b\u7ed3\uff0c\u751f\u6210\u5305\u542b\u4e8b\u4ef6\u63cf\u8ff0\u3001\u63a8\u8350\u5de5\u5177\u3001\u5c3a\u5bf8\u3001\u4fee\u590d\u8ba1\u5212\u548c\u7d27\u6025\u8b66\u62a5\u7684JSON\u683c\u5f0f\u7ed3\u6784\u5316\u884c\u52a8\u8ba1\u5212\uff0c\u6574\u5408\u4e86QwenVL\u548cLLaVA\u7b49\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5148\u8fdb\u6280\u672f\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u6355\u83b7\u7684CCTV\u89c6\u9891\u7247\u6bb5\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u591a\u79cd\u57fa\u7840\u8bbe\u65bd\u7f3a\u9677\uff0c\u5e76\u751f\u6210\u8fde\u8d2f\u7684\u7ed3\u6784\u5316\u603b\u7ed3\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u667a\u80fd\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7ef4\u62a4\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u68c0\u6d4b\u6548\u7387\u548c\u7ef4\u62a4\u54cd\u5e94\u901f\u5ea6\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u5c06\u7cfb\u7edf\u6269\u5c55\u5230\u57ce\u5e02\u8303\u56f4\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2510.16036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16036", "abs": "https://arxiv.org/abs/2510.16036", "authors": ["Zewen Li", "Zitong Yu", "Qilang Ye", "Weicheng Xie", "Wei Zhuo", "Linlin Shen"], "title": "IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection", "comment": "Accepted by IEEE Transactions on Instrumentation and Measurement\n  (TIM)", "summary": "The robust causal capability of Multimodal Large Language Models (MLLMs) hold\nthe potential of detecting defective objects in Industrial Anomaly Detection\n(IAD). However, most traditional IAD methods lack the ability to provide\nmulti-turn human-machine dialogues and detailed descriptions, such as the color\nof objects, the shape of an anomaly, or specific types of anomalies. At the\nsame time, methods based on large pre-trained models have not fully stimulated\nthe ability of large models in anomaly detection tasks. In this paper, we\nexplore the combination of rich text semantics with both image-level and\npixel-level information from images and propose IAD-GPT, a novel paradigm based\non MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate\ndetailed anomaly prompts for specific objects. These specific prompts from the\nlarge language model (LLM) are used to activate the detection and segmentation\nfunctions of the pre-trained visual-language model (i.e., CLIP). To enhance the\nvisual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein\nimage features interact with normal and abnormal text prompts to dynamically\nselect enhancement pathways, which enables language models to focus on specific\naspects of visual data, enhancing their ability to accurately interpret and\nrespond to anomalies within images. Moreover, we design a Multi-Mask Fusion\nmodule to incorporate mask as expert knowledge, which enhances the LLM's\nperception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA\ndatasets demonstrate our state-of-the-art performance on self-supervised and\nfew-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA\ndatasets. The codes are available at\n\\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIAD-GPT\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5f02\u5e38\u63d0\u793a\u751f\u6210\u5668\u548c\u6587\u672c\u5f15\u5bfc\u589e\u5f3a\u5668\u6fc0\u6d3b\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u68c0\u6d4b\u4e0e\u5206\u5272\u80fd\u529b\uff0c\u5728MVTec-AD\u548cVisA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u591a\u8f6e\u4eba\u673a\u5bf9\u8bdd\u548c\u8be6\u7ec6\u63cf\u8ff0\u80fd\u529b\uff0c\u800c\u57fa\u4e8e\u5927\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u6fc0\u53d1\u5927\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u4e0e\u56fe\u50cf\u7ea7\u3001\u50cf\u7d20\u7ea7\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5f02\u5e38\u63d0\u793a\u751f\u6210\u5668\u4e3a\u7279\u5b9a\u5bf9\u8c61\u751f\u6210\u8be6\u7ec6\u5f02\u5e38\u63d0\u793a\uff0c\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u589e\u5f3a\u5668\u4f7f\u56fe\u50cf\u7279\u5f81\u4e0e\u6b63\u5e38/\u5f02\u5e38\u6587\u672c\u63d0\u793a\u4ea4\u4e92\u4ee5\u52a8\u6001\u9009\u62e9\u589e\u5f3a\u8def\u5f84\uff0c\u5e76\u8bbe\u8ba1\u591a\u63a9\u7801\u878d\u5408\u6a21\u5757\u5c06\u63a9\u7801\u4f5c\u4e3a\u4e13\u5bb6\u77e5\u8bc6\u878d\u5165\uff0c\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u50cf\u7d20\u7ea7\u5f02\u5e38\u7684\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5728MVTec-AD\u548cVisA\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u81ea\u76d1\u7763\u548c\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5206\u5272\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u4e0e\u591a\u7ea7\u89c6\u89c9\u4fe1\u606f\u7684\u65b0\u8303\u5f0f\uff0c\u4e3a\u667a\u80fd\u5de5\u4e1a\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u8f9f\u4e86\u5927\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.16057", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16057", "abs": "https://arxiv.org/abs/2510.16057", "authors": ["Md Kamrul Siam", "Md Jobair Hossain Faruk", "Jerry Q. Cheng", "Huanying Gu"], "title": "Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus", "comment": "7 pages (Accepted to IEEE BHI 2025)", "summary": "This study presents a novel multi-model fusion framework leveraging two\nstate-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance\nthe reliability of chest X-ray interpretation on the CheXpert dataset. From the\nfull CheXpert corpus of 224,316 chest radiographs, we randomly selected 234\nradiologist-annotated studies to evaluate unimodal performance using image-only\nprompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of\n62.8% and 76.9%, respectively. A similarity-based consensus approach, using a\n95% output similarity threshold, improved accuracy to 77.6%. To assess the\nimpact of multimodal inputs, we then generated synthetic clinical notes\nfollowing the MIMIC-CXR template and evaluated a separate subset of 50 randomly\nselected cases paired with both images and synthetic text. On this multimodal\ncohort, performance improved to 84% for ChatGPT and 76% for Claude, while\nconsensus accuracy reached 91.3%. Across both experimental conditions,\nagreement-based fusion consistently outperformed individual models. These\nfindings highlight the utility of integrating complementary modalities and\nusing output-level consensus to improve the trustworthiness and clinical\nutility of AI-assisted radiological diagnosis, offering a practical path to\nreduce diagnostic errors with minimal computational overhead.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eChatGPT\u548cClaude\u7684\u591a\u6a21\u578b\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u51fa\u76f8\u4f3c\u5ea6\u5171\u8bc6\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u80f8\u90e8X\u5149\u7247\u8bca\u65ad\u7684\u51c6\u786e\u6027\uff0c\u5728CheXpert\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8691.3%\u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524dAI\u8f85\u52a9\u653e\u5c04\u5b66\u8bca\u65ad\u5b58\u5728\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5355\u4e00\u6a21\u578b\u8bca\u65ad\u51c6\u786e\u6027\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u63a2\u7d22\u591a\u6a21\u578b\u878d\u5408\u7b56\u7565\u6765\u63d0\u5347\u8bca\u65ad\u4fe1\u4efb\u5ea6\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408ChatGPT\u548cClaude\u4e24\u4e2a\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u57fa\u4e8e95%\u8f93\u51fa\u76f8\u4f3c\u5ea6\u9608\u503c\u7684\u5171\u8bc6\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u7eaf\u56fe\u50cf\u63d0\u793a\u4e0e\u56fe\u50cf\u52a0\u5408\u6210\u4e34\u5e8a\u7b14\u8bb0\u7684\u591a\u6a21\u6001\u8f93\u5165\u6548\u679c\u3002", "result": "\u5728234\u4f8b\u7eaf\u56fe\u50cf\u6d4b\u8bd5\u4e2d\uff0cChatGPT\u548cClaude\u5206\u522b\u8fbe\u523062.8%\u548c76.9%\u7684\u51c6\u786e\u7387\uff0c\u5171\u8bc6\u65b9\u6cd5\u63d0\u5347\u81f377.6%\uff1b\u572850\u4f8b\u591a\u6a21\u6001\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u81f384%\u548c76%\uff0c\u5171\u8bc6\u51c6\u786e\u7387\u8fbe\u523091.3%\uff0c\u878d\u5408\u7b56\u7565\u59cb\u7ec8\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u8f93\u5165\u548c\u8f93\u51fa\u7ea7\u5171\u8bc6\u673a\u5236\u5728\u63d0\u5347AI\u8f85\u52a9\u653e\u5c04\u8bca\u65ad\u53ef\u9760\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u51cf\u5c11\u8bca\u65ad\u9519\u8bef\u63d0\u4f9b\u4e86\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u7684\u5b9e\u7528\u8def\u5f84\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u8f6c\u5316\u4ef7\u503c\u3002"}}
{"id": "2510.15948", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15948", "abs": "https://arxiv.org/abs/2510.15948", "authors": ["MingSheng Li", "Guangze Zhao", "Sichen Liu"], "title": "VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable progress in\nmultimodal perception and generation, yet their safety alignment remains a\ncritical challenge.Existing defenses and vulnerable to multimodal jailbreaks,\nas visual inputs introduce new attack surfaces, reasoning chains lack safety\nsupervision, and alignment often degrades under modality fusion.To overcome\nthese limitation, we propose VisuoAlign, a framework for multi-modal safety\nalignment via prompt-guided tree search.VisuoAlign embeds safety constrains\ninto the reasoning process through visual-textual interactive prompts, employs\nMonte Carlo Tree Search(MCTS) to systematically construct diverse\nsafety-critical prompt trajectories, and introduces prompt-based scaling to\nensure real-time risk detection and compliant responses.Extensive experiments\ndemonstrate that VisuoAlign proactively exposes risks, enables comprehensive\ndataset generation, and significantly improves the robustness of LVLMs against\ncomplex cross-modal threats.", "AI": {"tldr": "VisuoAlign\u662f\u4e00\u4e2a\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u6811\u641c\u7d22\u5b9e\u73b0\u591a\u6a21\u6001\u5b89\u5168\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5c06\u5b89\u5168\u7ea6\u675f\u5d4c\u5165\u63a8\u7406\u8fc7\u7a0b\u3001\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6784\u5efa\u5b89\u5168\u5173\u952e\u63d0\u793a\u8f68\u8ff9\uff0c\u4ee5\u53ca\u5f15\u5165\u57fa\u4e8e\u63d0\u793a\u7684\u7f29\u653e\u6765\u663e\u8457\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u590d\u6742\u8de8\u6a21\u6001\u5a01\u80c1\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u611f\u77e5\u548c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5b89\u5168\u5bf9\u9f50\u4ecd\u9762\u4e34\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5bf9\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u5b58\u5728\u8106\u5f31\u6027\uff0c\u89c6\u89c9\u8f93\u5165\u5f15\u5165\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff0c\u63a8\u7406\u94fe\u7f3a\u4e4f\u5b89\u5168\u76d1\u7763\uff0c\u4e14\u6a21\u6001\u878d\u5408\u5e38\u5e38\u5bfc\u81f4\u5bf9\u9f50\u6027\u80fd\u4e0b\u964d\u3002", "method": "VisuoAlign\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u4ea4\u4e92\u63d0\u793a\u5c06\u5b89\u5168\u7ea6\u675f\u5d4c\u5165\u63a8\u7406\u8fc7\u7a0b\uff0c\u91c7\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7cfb\u7edf\u6027\u5730\u6784\u5efa\u591a\u6837\u5316\u7684\u5b89\u5168\u5173\u952e\u63d0\u793a\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u63d0\u793a\u7684\u7f29\u653e\u673a\u5236\u4ee5\u786e\u4fdd\u5b9e\u65f6\u98ce\u9669\u68c0\u6d4b\u548c\u5408\u89c4\u54cd\u5e94\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cVisuoAlign\u80fd\u591f\u4e3b\u52a8\u66b4\u9732\u98ce\u9669\uff0c\u5b9e\u73b0\u5168\u9762\u7684\u6570\u636e\u96c6\u751f\u6210\uff0c\u5e76\u663e\u8457\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u590d\u6742\u8de8\u6a21\u6001\u5a01\u80c1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u63d0\u793a\u5f15\u5bfc\u6811\u641c\u7d22\u6846\u67b6\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5b89\u5168\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2510.16134", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16134", "abs": "https://arxiv.org/abs/2510.16134", "authors": ["Chen Kong", "James Fort", "Aria Kang", "Jonathan Wittmer", "Simon Green", "Tianwei Shen", "Yipu Zhao", "Cheng Peng", "Gustavo Solaira", "Andrew Berkovich", "Nikhil Raina", "Vijay Baiyya", "Evgeniy Oleinik", "Eric Huang", "Fan Zhang", "Julian Straub", "Mark Schwesinger", "Luis Pesqueira", "Xiaqing Pan", "Jakob Julian Engel", "Carl Ren", "Mingfei Yan", "Richard Newcombe"], "title": "Aria Gen 2 Pilot Dataset", "comment": null, "summary": "The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset\ncaptured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely\naccess, A2PD is released incrementally with ongoing dataset enhancements. The\ninitial release features Dia'ane, our primary subject, who records her daily\nactivities alongside friends, each equipped with Aria Gen 2 glasses. It\nencompasses five primary scenarios: cleaning, cooking, eating, playing, and\noutdoor walking. In each of the scenarios, we provide comprehensive raw sensor\ndata and output data from various machine perception algorithms. These data\nillustrate the device's ability to perceive the wearer, the surrounding\nenvironment, and interactions between the wearer and the environment, while\nmaintaining robust performance across diverse users and conditions. The A2PD is\npublicly available at projectaria.com, with open-source tools and usage\nexamples provided in Project Aria Tools.", "AI": {"tldr": "Aria Gen 2 Pilot Dataset (A2PD) \u662f\u4e00\u4e2a\u4f7f\u7528\u5148\u8fdbAria Gen 2\u773c\u955c\u91c7\u96c6\u7684\u81ea\u6211\u4e2d\u5fc3\u591a\u6a21\u6001\u5f00\u653e\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u589e\u91cf\u53d1\u5e03\u65b9\u5f0f\u63d0\u4f9b\u5305\u542b\u591a\u79cd\u65e5\u5e38\u573a\u666f\u7684\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u548c\u673a\u5668\u611f\u77e5\u7b97\u6cd5\u8f93\u51fa\uff0c\u652f\u6301\u5bf9\u4f69\u6234\u8005\u3001\u73af\u5883\u548c\u4ea4\u4e92\u7684\u5168\u9762\u611f\u77e5\u7814\u7a76\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u548c\u591a\u6a21\u6001\u611f\u77e5\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u4f9b\u5305\u542b\u6e05\u6d01\u3001\u70f9\u996a\u3001\u8fdb\u98df\u3001\u6e38\u620f\u548c\u6237\u5916\u6b65\u884c\u7b49\u4e94\u79cd\u4e3b\u8981\u573a\u666f\u7684\u5168\u9762\u6570\u636e\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u7528\u6237\u591a\u6837\u6027\u548c\u73af\u5883\u590d\u6742\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u91c7\u7528Aria Gen 2\u773c\u955c\u8bbe\u5907\u91c7\u96c6\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff0c\u901a\u8fc7\u589e\u91cf\u53d1\u5e03\u7b56\u7565\u6301\u7eed\u6269\u5c55\u6570\u636e\u96c6\u89c4\u6a21\uff0c\u5305\u542b\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u548c\u591a\u79cd\u673a\u5668\u611f\u77e5\u7b97\u6cd5\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u6db5\u76d6\u4e86\u4f69\u6234\u8005\u72b6\u6001\u3001\u73af\u5883\u611f\u77e5\u4ee5\u53ca\u4eba\u673a\u4ea4\u4e92\u7b49\u591a\u4e2a\u7ef4\u5ea6\u7684\u4fe1\u606f\u3002", "result": "\u6570\u636e\u96c6\u5c55\u793a\u4e86\u8bbe\u5907\u5728\u4e0d\u540c\u7528\u6237\u548c\u6761\u4ef6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u80fd\u7684\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u611f\u77e5\u4f69\u6234\u8005\u72b6\u6001\u3001\u5468\u56f4\u73af\u5883\u4ee5\u53ca\u4f69\u6234\u8005\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u4e3a\u591a\u6a21\u6001\u611f\u77e5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u3002", "conclusion": "A2PD\u6570\u636e\u96c6\u4e3a\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u548c\u591a\u6a21\u6001\u611f\u77e5\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u57fa\u51c6\u8d44\u6e90\uff0c\u5176\u5f00\u653e\u83b7\u53d6\u7b56\u7565\u548c\u914d\u5957\u5de5\u5177\u5c06\u4fc3\u8fdb\u76f8\u5173\u9886\u57df\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u73af\u5883\u4e0b\u7684\u611f\u77e5\u7b97\u6cd5\u8bc4\u4f30\u548c\u8de8\u7528\u6237\u6cdb\u5316\u80fd\u529b\u7814\u7a76\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.16091", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16091", "abs": "https://arxiv.org/abs/2510.16091", "authors": ["Binglan Han", "Anuradha Mathrani", "Teo Susnjak"], "title": "Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification", "comment": null, "summary": "This study quantifies how prompting strategies interact with large language\nmodels (LLMs) to automate the screening stage of systematic literature reviews\n(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,\nGemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types\n(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)\nacross relevance classification and six Level-2 tasks, using accuracy,\nprecision, recall, and F1. Results show pronounced model-prompt interaction\neffects: CoT-few-shot yields the most reliable precision-recall balance;\nzero-shot maximizes recall for high-sensitivity passes; and self-reflection\nunderperforms due to over-inclusivity and instability across models. GPT-4o and\nDeepSeek provide robust overall performance, while GPT-4o-mini performs\ncompetitively at a substantially lower dollar cost. A cost-performance analysis\nfor relevance classification (per 1,000 abstracts) reveals large absolute\ndifferences among model-prompt pairings; GPT-4o-mini remains low-cost across\nprompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer\nattractive F1 at a small incremental cost. We recommend a staged workflow that\n(1) deploys low-cost models with structured prompts for first-pass screening\nand (2) escalates only borderline cases to higher-capacity models. These\nfindings highlight LLMs' uneven but promising potential to automate literature\nscreening. By systematically analyzing prompt-model interactions, we provide a\ncomparative benchmark and practical guidance for task-adaptive LLM deployment.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u91cf\u5316\u4e86\u63d0\u793a\u7b56\u7565\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7b5b\u9009\u9636\u6bb5\u7684\u4ea4\u4e92\u6548\u5e94\uff0c\u53d1\u73b0CoT\u5c11\u6837\u672c\u63d0\u793a\u63d0\u4f9b\u6700\u53ef\u9760\u7684\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6210\u672c\u6548\u76ca\u7684\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u81ea\u52a8\u5316\u7b5b\u9009\u9636\u6bb5\u4e2d\uff0c\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u4ea4\u4e92\u6548\u5e94\u5c1a\u672a\u88ab\u7cfb\u7edf\u91cf\u5316\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u63d0\u793a-\u6a21\u578b\u7ec4\u5408\u6765\u63d0\u9ad8\u7b5b\u9009\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u516d\u79cdLLM\u5728\u4e94\u79cd\u63d0\u793a\u7c7b\u578b\u4e0b\u7684\u8868\u73b0\uff0c\u5305\u62ec\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u601d\u7ef4\u94fe\u3001\u601d\u7ef4\u94fe\u5c11\u6837\u672c\u548c\u81ea\u53cd\u601d\u63d0\u793a\uff0c\u4f7f\u7528\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\uff0c\u5bf9\u76f8\u5173\u6027\u5206\u7c7b\u548c\u516d\u4e2a\u4e8c\u7ea7\u4efb\u52a1\u8fdb\u884c\u5168\u9762\u5206\u6790\u3002", "result": "\u7ed3\u679c\u663e\u793a\u663e\u8457\u7684\u6a21\u578b-\u63d0\u793a\u4ea4\u4e92\u6548\u5e94\uff1aCoT\u5c11\u6837\u672c\u63d0\u793a\u63d0\u4f9b\u6700\u53ef\u9760\u7684\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u5e73\u8861\uff1b\u96f6\u6837\u672c\u63d0\u793a\u5728\u9ad8\u654f\u611f\u5ea6\u7b5b\u9009\u65f6\u6700\u5927\u5316\u53ec\u56de\u7387\uff1b\u81ea\u53cd\u601d\u63d0\u793a\u56e0\u8fc7\u5ea6\u5305\u5bb9\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\u8868\u73b0\u4e0d\u4f73\u3002GPT-4o\u548cDeepSeek\u63d0\u4f9b\u7a33\u5065\u7684\u6574\u4f53\u6027\u80fd\uff0c\u800cGPT-4o-mini\u5728\u663e\u8457\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u7814\u7a76\u63a8\u8350\u91c7\u7528\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff1a\u9996\u5148\u4f7f\u7528\u4f4e\u6210\u672c\u6a21\u578b\u914d\u5408\u7ed3\u6784\u5316\u63d0\u793a\u8fdb\u884c\u521d\u7b5b\uff0c\u4ec5\u5c06\u8fb9\u754c\u6848\u4f8b\u5347\u7ea7\u5230\u9ad8\u5bb9\u91cf\u6a21\u578b\u5904\u7406\u3002\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86LLM\u5728\u81ea\u52a8\u5316\u6587\u732e\u7b5b\u9009\u65b9\u9762\u4e0d\u5747\u8861\u4f46\u5177\u6709\u524d\u666f\u7684\u6f5c\u529b\uff0c\u4e3a\u4efb\u52a1\u81ea\u9002\u5e94LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6bd4\u8f83\u57fa\u51c6\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2510.16095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16095", "abs": "https://arxiv.org/abs/2510.16095", "authors": ["Dou Liu", "Ying Long", "Sophia Zuoqiu", "Di Liu", "Kang Li", "Yiting Lin", "Hanyi Liu", "Rong Yin", "Tian Tang"], "title": "Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study", "comment": null, "summary": "Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for\nexplainable medical Artificial Intelligence (AI) while constrained by data\nscarcity. Although Large Language Models (LLMs) can synthesize medical data,\ntheir clinical reliability remains unverified. This study evaluates the\nreliability of LLM-generated CoTs and investigates prompting strategies to\nenhance their quality. In a blinded comparative study, senior clinicians in\nAssisted Reproductive Technology (ART) evaluated CoTs generated via three\ndistinct strategies: Zero-shot, Random Few-shot (using shallow examples), and\nSelective Few-shot (using diverse, high-quality examples). These expert ratings\nwere compared against evaluations from a state-of-the-art AI model (GPT-4o).\nThe Selective Few-shot strategy significantly outperformed other strategies\nacross all human evaluation metrics (p < .001). Critically, the Random Few-shot\nstrategy offered no significant improvement over the Zero-shot baseline,\ndemonstrating that low-quality examples are as ineffective as no examples. The\nsuccess of the Selective strategy is attributed to two principles:\n\"Gold-Standard Depth\" (reasoning quality) and \"Representative Diversity\"\n(generalization). Notably, the AI evaluator failed to discern these critical\nperformance differences. The clinical reliability of synthetic CoTs is dictated\nby strategic prompt curation, not the mere presence of examples. We propose a\n\"Dual Principles\" framework as a foundational methodology to generate\ntrustworthy data at scale. This work offers a validated solution to the data\nbottleneck and confirms the indispensable role of human expertise in evaluating\nhigh-stakes clinical AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u9a8c\u8bc1\u4e86\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u7b56\u7565\uff08\u9009\u62e9\u6027\u5c11\u6837\u672c\u5b66\u4e60\uff09\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e34\u5e8a\u601d\u7ef4\u94fe\uff0c\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u548c\u968f\u673a\u5c11\u6837\u672c\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\"\u9ec4\u91d1\u6807\u51c6\u6df1\u5ea6\"\u548c\"\u4ee3\u8868\u6027\u591a\u6837\u6027\"\u7684\u53cc\u539f\u5219\u6846\u67b6\u6765\u89e3\u51b3\u533b\u7597AI\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u533b\u7597AI\u4e2d\u9ad8\u8d28\u91cf\u4e34\u5e8a\u601d\u7ef4\u94fe\u7684\u521b\u5efa\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5408\u6210\u533b\u7597\u6570\u636e\uff0c\u4f46\u5176\u4e34\u5e8a\u53ef\u9760\u6027\u5c1a\u672a\u5f97\u5230\u9a8c\u8bc1\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30LLM\u751f\u6210\u7684\u601d\u7ef4\u94fe\u53ef\u9760\u6027\u5e76\u63a2\u7d22\u63d0\u5347\u5176\u8d28\u91cf\u7684\u63d0\u793a\u7b56\u7565\u3002", "method": "\u7814\u7a76\u91c7\u7528\u76f2\u6cd5\u6bd4\u8f83\u7814\u7a76\u8bbe\u8ba1\uff0c\u7531\u8f85\u52a9\u751f\u6b96\u6280\u672f\u9886\u57df\u7684\u9ad8\u7ea7\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u4e09\u79cd\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u751f\u6210\u7684\u601d\u7ef4\u94fe\uff1a\u96f6\u6837\u672c\u5b66\u4e60\u3001\u968f\u673a\u5c11\u6837\u672c\u5b66\u4e60\uff08\u4f7f\u7528\u6d45\u5c42\u793a\u4f8b\uff09\u548c\u9009\u62e9\u6027\u5c11\u6837\u672c\u5b66\u4e60\uff08\u4f7f\u7528\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u793a\u4f8b\uff09\uff0c\u5e76\u5c06\u4e13\u5bb6\u8bc4\u5206\u4e0e\u6700\u5148\u8fdb\u7684AI\u6a21\u578b\uff08GPT-4o\uff09\u8bc4\u4f30\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u9009\u62e9\u6027\u5c11\u6837\u672c\u7b56\u7565\u5728\u6240\u6709\u4eba\u7c7b\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7b56\u7565\uff08p < .001\uff09\uff0c\u800c\u968f\u673a\u5c11\u6837\u672c\u7b56\u7565\u76f8\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u6ca1\u6709\u663e\u8457\u6539\u8fdb\uff0c\u8868\u660e\u4f4e\u8d28\u91cf\u793a\u4f8b\u4e0e\u65e0\u793a\u4f8b\u540c\u6837\u65e0\u6548\uff1bAI\u8bc4\u4f30\u5668\u672a\u80fd\u8bc6\u522b\u8fd9\u4e9b\u5173\u952e\u6027\u80fd\u5dee\u5f02\uff0c\u9009\u62e9\u6027\u7b56\u7565\u7684\u6210\u529f\u5f52\u56e0\u4e8e\"\u9ec4\u91d1\u6807\u51c6\u6df1\u5ea6\"\u548c\"\u4ee3\u8868\u6027\u591a\u6837\u6027\"\u4e24\u4e2a\u539f\u5219\u3002", "conclusion": "\u5408\u6210\u601d\u7ef4\u94fe\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u7b56\u7565\u6027\u63d0\u793a\u8bbe\u8ba1\u800c\u975e\u5355\u7eaf\u793a\u4f8b\u5b58\u5728\uff0c\u63d0\u51fa\u7684\"\u53cc\u539f\u5219\"\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u751f\u6210\u53ef\u4fe1\u6570\u636e\u63d0\u4f9b\u4e86\u57fa\u7840\u65b9\u6cd5\u5b66\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u89e3\u51b3\u6570\u636e\u74f6\u9888\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u65b9\u6848\uff0c\u5e76\u786e\u8ba4\u4e86\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u5728\u8bc4\u4f30\u9ad8\u98ce\u9669\u4e34\u5e8aAI\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u4f5c\u7528\u3002"}}
{"id": "2510.16209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16209", "abs": "https://arxiv.org/abs/2510.16209", "authors": ["Nyle Siddiqui", "Rohit Gupta", "Sirnam Swetha", "Mubarak Shah"], "title": "StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales", "comment": null, "summary": "State space models (SSMs) have emerged as a competitive alternative to\ntransformers in various tasks. Their linear complexity and hidden-state\nrecurrence make them particularly attractive for modeling long sequences,\nwhereas attention becomes quadratically expensive. However, current training\nmethods for video understanding are tailored towards transformers and fail to\nfully leverage the unique attributes of SSMs. For example, video models are\noften trained at a fixed resolution and video length to balance the quadratic\nscaling of attention cost against performance. Consequently, these models\nsuffer from degraded performance when evaluated on videos with spatial and\ntemporal resolutions unseen during training; a property we call spatio-temporal\ninflexibility. In the context of action recognition, this severely limits a\nmodel's ability to retain performance across both short- and long-form videos.\nTherefore, we propose a flexible training method that leverages and improves\nthe inherent adaptability of SSMs. Our method samples videos at varying\ntemporal and spatial resolutions during training and dynamically interpolates\nmodel weights to accommodate any spatio-temporal scale. This instills our SSM,\nwhich we call StretchySnake, with spatio-temporal flexibility and enables it to\nseamlessly handle videos ranging from short, fine-grained clips to long,\ncomplex activities. We introduce and compare five different variants of\nflexible training, and identify the most effective strategy for video SSMs. On\nshort-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,\nStretchySnake outperforms transformer and SSM baselines alike by up to 28%,\nwith strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,\nour method provides a simple drop-in training recipe that makes video SSMs more\nrobust, resolution-agnostic, and efficient across diverse action recognition\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u65f6\u7a7a\u81ea\u9002\u5e94\u8bad\u7ec3\u65b9\u6cd5StretchySnake\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u8bad\u7ec3\u89c6\u9891\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u9891\u6a21\u578b\u5728\u672a\u89c1\u65f6\u7a7a\u5206\u8fa8\u7387\u4e0b\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u52a8\u4f5c\u8bc6\u522b\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8eTransformer\u548cSSM\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u7406\u89e3\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9Transformer\u8bbe\u8ba1\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSMs)\u7684\u72ec\u7279\u5c5e\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u9762\u5bf9\u8bad\u7ec3\u65f6\u672a\u89c1\u8fc7\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u65f6\u51fa\u73b0\u6027\u80fd\u9000\u5316\uff0c\u8fd9\u79cd\u65f6\u7a7a\u4e0d\u7075\u6d3b\u6027\u4e25\u91cd\u9650\u5236\u4e86\u6a21\u578b\u5728\u957f\u77ed\u89c6\u9891\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7075\u6d3b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u91c7\u6837\u4e0d\u540c\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u89c6\u9891\uff0c\u5e76\u52a8\u6001\u63d2\u503c\u6a21\u578b\u6743\u91cd\u4ee5\u9002\u5e94\u4efb\u610f\u65f6\u7a7a\u5c3a\u5ea6\uff0c\u5f00\u53d1\u4e86\u4e94\u79cd\u7075\u6d3b\u8bad\u7ec3\u53d8\u4f53\u5e76\u786e\u5b9a\u4e86\u6700\u9002\u5408\u89c6\u9891SSM\u7684\u7b56\u7565\uff0c\u6784\u5efa\u4e86\u540d\u4e3aStretchySnake\u7684\u65f6\u7a7a\u81ea\u9002\u5e94SSM\u6a21\u578b\u3002", "result": "\u5728\u77ed\u52a8\u4f5c\u57fa\u51c6(UCF-101\u3001HMDB-51)\u548c\u957f\u52a8\u4f5c\u57fa\u51c6(COIN\u3001Breakfast)\u4e0a\uff0cStretchySnake\u6bd4Transformer\u548cSSM\u57fa\u7ebf\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe28%\uff0c\u5728\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u6570\u636e\u96c6(SSV2\u3001Diving-48)\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u5373\u63d2\u5373\u7528\u8bad\u7ec3\u65b9\u6848\uff0c\u4f7f\u89c6\u9891SSM\u5728\u5404\u79cd\u52a8\u4f5c\u8bc6\u522b\u573a\u666f\u4e2d\u66f4\u52a0\u9c81\u68d2\u3001\u5206\u8fa8\u7387\u65e0\u5173\u4e14\u9ad8\u6548\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u5145\u5206\u5229\u7528\u4e86SSM\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u548c\u9690\u85cf\u72b6\u6001\u9012\u5f52\u4f18\u52bf\u3002"}}
{"id": "2510.16198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16198", "abs": "https://arxiv.org/abs/2510.16198", "authors": ["Mohamed Gamil", "Abdelrahman Elsayed", "Abdelrahman Lila", "Ahmed Gad", "Hesham Abdelgawad", "Mohamed Aref", "Ahmed Fares"], "title": "EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture", "comment": null, "summary": "Despite recent advances in AI, multimodal culturally diverse datasets are\nstill limited, particularly for regions in the Middle East and Africa. In this\npaper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian\nculture. By designing and running a new data collection pipeline, we collected\nover 3,000 images, covering 313 concepts across landmarks, food, and folklore.\nEach entry in the dataset is manually validated for cultural authenticity and\nmultimodal coherence. EgMM-Corpus aims to provide a reliable resource for\nevaluating and training vision-language models in an Egyptian cultural context.\nWe further evaluate the zero-shot performance of Contrastive Language-Image\nPre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and\n36.4% Top-5 accuracy in classification. These results underscore the existing\ncultural bias in large-scale vision-language models and demonstrate the\nimportance of EgMM-Corpus as a benchmark for developing culturally aware\nmodels.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86EgMM-Corpus\uff0c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u57c3\u53ca\u6587\u5316\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b3,000\u591a\u5f20\u56fe\u50cf\u8986\u76d6313\u4e2a\u6587\u5316\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u8bc4\u4f30CLIP\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u63ed\u793a\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6587\u5316\u504f\u89c1\u3002", "motivation": "\u5f53\u524dAI\u9886\u57df\u7f3a\u4e4f\u9488\u5bf9\u4e2d\u4e1c\u548c\u975e\u6d32\u5730\u533a\u7684\u591a\u6a21\u6001\u6587\u5316\u591a\u6837\u6027\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u57c3\u53ca\u6587\u5316\u80cc\u666f\u4e0b\u7684\u8d44\u6e90\u4e25\u91cd\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6587\u5316\u573a\u666f\u4e2d\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u5e76\u8fd0\u884c\u4e86\u65b0\u7684\u6570\u636e\u6536\u96c6\u6d41\u7a0b\uff0c\u6536\u96c6\u4e86\u6db5\u76d6\u5730\u6807\u3001\u98df\u7269\u548c\u6c11\u95f4\u4f20\u8bf4\u7b49313\u4e2a\u6982\u5ff5\u76843,000\u591a\u5f20\u56fe\u50cf\uff0c\u6bcf\u4e2a\u6761\u76ee\u90fd\u7ecf\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u4ee5\u786e\u4fdd\u6587\u5316\u771f\u5b9e\u6027\u548c\u591a\u6a21\u6001\u4e00\u81f4\u6027\u3002", "result": "\u5728EgMM-Corpus\u4e0a\u8bc4\u4f30CLIP\u6a21\u578b\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793aTop-1\u51c6\u786e\u7387\u4e3a21.2%\uff0cTop-5\u51c6\u786e\u7387\u4e3a36.4%\uff0c\u663e\u8457\u4f4e\u4e8e\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u6587\u5316\u504f\u89c1\uff0c\u5f3a\u8c03\u4e86EgMM-Corpus\u4f5c\u4e3a\u5f00\u53d1\u6587\u5316\u611f\u77e5\u6a21\u578b\u57fa\u51c6\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u4fc3\u8fdbAI\u7cfb\u7edf\u7684\u6587\u5316\u591a\u6837\u6027\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90\u3002"}}
{"id": "2510.16342", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16342", "abs": "https://arxiv.org/abs/2510.16342", "authors": ["Tong Zhang", "Ru Zhang", "Jianyi Liu", "Zhen Yang", "Gongshen Liu"], "title": "Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts", "comment": null, "summary": "Existing concept erasure methods for text-to-image diffusion models commonly\nrely on fixed anchor strategies, which often lead to critical issues such as\nconcept re-emergence and erosion. To address this, we conduct causal tracing to\nreveal the inherent sensitivity of erasure to anchor selection and define\nSibling Exclusive Concepts as a superior class of anchors. Based on this\ninsight, we propose \\textbf{SELECT} (Sibling-Exclusive Evaluation for\nContextual Targeting), a dynamic anchor selection framework designed to\novercome the limitations of fixed anchors. Our framework introduces a novel\ntwo-stage evaluation mechanism that automatically discovers optimal anchors for\nprecise erasure while identifying critical boundary anchors to preserve related\nconcepts. Extensive evaluations demonstrate that SELECT, as a universal anchor\nsolution, not only efficiently adapts to multiple erasure frameworks but also\nconsistently outperforms existing baselines across key performance metrics,\naveraging only 4 seconds for anchor mining of a single concept.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SELECT\u6846\u67b6\uff0c\u4e00\u79cd\u52a8\u6001\u951a\u70b9\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u6982\u5ff5\u64e6\u9664\u7684\u951a\u70b9\u654f\u611f\u6027\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u56e0\u679c\u8ffd\u8e2a\u5206\u6790\u63ed\u793a\u4e86\u56fa\u5b9a\u951a\u70b9\u7b56\u7565\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f15\u5165\u5144\u5f1f\u6392\u4ed6\u6027\u6982\u5ff5\u4f5c\u4e3a\u66f4\u4f18\u7684\u951a\u70b9\u7c7b\u522b\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u56fa\u5b9a\u951a\u70b9\u7b56\u7565\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u6982\u5ff5\u91cd\u65b0\u51fa\u73b0\u548c\u4fb5\u8680\u7b49\u5173\u952e\u95ee\u9898\u3002\u901a\u8fc7\u56e0\u679c\u8ffd\u8e2a\u5206\u6790\uff0c\u7814\u7a76\u53d1\u73b0\u64e6\u9664\u6548\u679c\u5bf9\u951a\u70b9\u9009\u62e9\u5177\u6709\u5185\u5728\u654f\u611f\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u667a\u80fd\u7684\u951a\u70b9\u9009\u62e9\u673a\u5236\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86SELECT\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u52a8\u6001\u951a\u70b9\u9009\u62e9\u65b9\u6cd5\uff0c\u91c7\u7528\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u8bc4\u4f30\u673a\u5236\u3002\u8be5\u6846\u67b6\u81ea\u52a8\u53d1\u73b0\u7528\u4e8e\u7cbe\u786e\u64e6\u9664\u7684\u6700\u4f18\u951a\u70b9\uff0c\u540c\u65f6\u8bc6\u522b\u5173\u952e\u8fb9\u754c\u951a\u70b9\u4ee5\u4fdd\u7559\u76f8\u5173\u6982\u5ff5\uff0c\u5c06\u5144\u5f1f\u6392\u4ed6\u6027\u6982\u5ff5\u5b9a\u4e49\u4e3a\u4e00\u7c7b\u66f4\u4f18\u7684\u951a\u70b9\u7c7b\u522b\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cSELECT\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u951a\u70b9\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u80fd\u591f\u9ad8\u6548\u9002\u914d\u591a\u79cd\u64e6\u9664\u6846\u67b6\uff0c\u800c\u4e14\u5728\u5173\u952e\u6027\u80fd\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u5bf9\u4e8e\u5355\u4e2a\u6982\u5ff5\u7684\u951a\u70b9\u6316\u6398\uff0c\u5e73\u5747\u4ec5\u97004\u79d2\u65f6\u95f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u6982\u5ff5\u64e6\u9664\u4e2d\u951a\u70b9\u9009\u62e9\u7684\u5173\u952e\u4f5c\u7528\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u951a\u70b9\u7b56\u7565\u76f8\u5bf9\u4e8e\u56fa\u5b9a\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002SELECT\u6846\u67b6\u4e3a\u89e3\u51b3\u6982\u5ff5\u64e6\u9664\u4e2d\u7684\u654f\u611f\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2510.16258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16258", "abs": "https://arxiv.org/abs/2510.16258", "authors": ["Claire McLean", "Makenzie Meendering", "Tristan Swartz", "Orri Gabbay", "Alexandra Olsen", "Rachel Jacobs", "Nicholas Rosen", "Philippe de Bree", "Tony Garcia", "Gadsden Merrill", "Jake Sandakly", "Julia Buffalini", "Neham Jain", "Steven Krenn", "Moneish Kumar", "Dejan Markovic", "Evonne Ng", "Fabian Prada", "Andrew Saba", "Siwei Zhang", "Vasu Agrawal", "Tim Godisart", "Alexander Richard", "Michael Zollhoefer"], "title": "Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset", "comment": null, "summary": "The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of\n500 individual hours of 3D motion data from 439 participants collected in a\nmulti-camera collection stage, amounting to over 54 million frames of tracked\n3D motion. The dataset features a wide range of single-person motion data,\nincluding prompted motions, hand gestures, and locomotion; as well as\nmulti-person behavioral and conversational data like discussions, conversations\nin different emotional states, collaborative activities, and co-living\nscenarios in an apartment-like space. We provide tracked human motion including\nhand tracking and body shape, text annotations, and a separate audio track for\neach participant.", "AI": {"tldr": "Embody 3D\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u60013D\u8fd0\u52a8\u6570\u636e\u96c6\uff0c\u5305\u542b500\u5c0f\u65f6\u6765\u81ea439\u540d\u53c2\u4e0e\u8005\u76843D\u8fd0\u52a8\u6570\u636e\uff0c\u6db5\u76d6\u5355\u4eba\u591a\u4eba\u548c\u4ea4\u4e92\u884c\u4e3a\uff0c\u4e3a\u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u548c\u884c\u4e3a\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u60013D\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u5728\u591a\u4eba\u4ea4\u4e92\u548c\u884c\u4e3a\u5206\u6790\u65b9\u9762\u5b58\u5728\u6570\u636e\u7a7a\u767d\uff0c\u9650\u5236\u4e86\u76f8\u5173\u7b97\u6cd5\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u591a\u76f8\u673a\u91c7\u96c6\u7cfb\u7edf\u6536\u96c6439\u540d\u53c2\u4e0e\u8005\u76843D\u8fd0\u52a8\u6570\u636e\uff0c\u5305\u62ec\u5355\u4eba\u8fd0\u52a8\u3001\u624b\u52bf\u3001\u79fb\u52a8\u4ee5\u53ca\u591a\u4eba\u5bf9\u8bdd\u3001\u534f\u4f5c\u6d3b\u52a8\u548c\u5171\u540c\u751f\u6d3b\u573a\u666f\uff0c\u5e76\u63d0\u4f9b\u8fd0\u52a8\u8ffd\u8e2a\u3001\u8eab\u4f53\u5f62\u6001\u3001\u6587\u672c\u6807\u6ce8\u548c\u72ec\u7acb\u97f3\u9891\u8f68\u9053\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b500\u5c0f\u65f6\u6570\u636e\u3001\u8d85\u8fc75400\u4e07\u5e273D\u8fd0\u52a8\u8ffd\u8e2a\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5e7f\u6cdb\u7684\u8fd0\u52a8\u7c7b\u578b\u548c\u4ea4\u4e92\u573a\u666f\uff0c\u4e3a\u591a\u6a21\u6001\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\u3002", "conclusion": "Embody 3D\u6570\u636e\u96c6\u586b\u8865\u4e863D\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u5728\u89c4\u6a21\u548c\u591a\u6837\u6027\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u4eba\u673a\u4ea4\u4e92\u548c\u884c\u4e3a\u5206\u6790\u7b49\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8bbe\u65bd\uff0c\u5c06\u63a8\u52a8\u76f8\u5173\u7b97\u6cd5\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002"}}
{"id": "2510.16257", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16257", "abs": "https://arxiv.org/abs/2510.16257", "authors": ["Chu Fei Luo", "Samuel Dahan", "Xiaodan Zhu"], "title": "Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback", "comment": "Findings of EMNLP 2025, 5 pages", "summary": "As language models have a greater impact on society, it is important to\nensure they are aligned to a diverse range of perspectives and are able to\nreflect nuance in human values. However, the most popular training paradigms\nfor modern language models often assume there is one optimal answer for every\nquery, leading to generic responses and poor alignment. In this work, we aim to\nenhance pluralistic alignment of language models in a low-resource setting with\ntwo methods: pluralistic decoding and model steering. We empirically\ndemonstrate that model steering offers consistent improvement over zero-shot\nand few-shot baselines with only 50 annotated samples. Our proposed methods\ndecrease false positives in several high-stakes tasks such as hate speech\ndetection and misinformation detection, and improves the distributional\nalignment to human values in GlobalOpinionQA. We hope our work highlights the\nimportance of diversity and how language models can be adapted to consider\nnuanced perspectives.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u7684\u591a\u5143\u5316\u5bf9\u9f50\u65b9\u6cd5\u2014\u2014\u591a\u5143\u5316\u89e3\u7801\u548c\u6a21\u578b\u5f15\u5bfc\uff0c\u901a\u8fc7\u4ec550\u4e2a\u6807\u6ce8\u6837\u672c\u5373\u53ef\u5728\u591a\u4e2a\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u591a\u5143\u5316\u5bf9\u9f50\u80fd\u529b\uff0c\u51cf\u5c11\u8bef\u5224\u5e76\u6539\u5584\u4eba\u7c7b\u4ef7\u503c\u89c2\u5206\u5e03\u5bf9\u9f50\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u5bf9\u793e\u4f1a\u5f71\u54cd\u65e5\u76ca\u589e\u5927\uff0c\u9700\u8981\u786e\u4fdd\u5b83\u4eec\u80fd\u591f\u4e0e\u591a\u6837\u5316\u89c6\u89d2\u5bf9\u9f50\u5e76\u53cd\u6620\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u7136\u800c\u5f53\u524d\u4e3b\u6d41\u8bad\u7ec3\u8303\u5f0f\u5047\u8bbe\u6bcf\u4e2a\u67e5\u8be2\u5b58\u5728\u552f\u4e00\u6700\u4f18\u7b54\u6848\uff0c\u5bfc\u81f4\u751f\u6210\u54cd\u5e94\u8fc7\u4e8e\u6cdb\u5316\u4e14\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u7684\u591a\u5143\u5316\u5bf9\u9f50\u65b9\u6cd5\uff1a\u591a\u5143\u5316\u89e3\u7801\u901a\u8fc7\u5f15\u5165\u591a\u6837\u6027\u673a\u5236\u751f\u6210\u591a\u4e2a\u54cd\u5e94\u53d8\u4f53\uff0c\u6a21\u578b\u5f15\u5bfc\u5219\u5229\u7528\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u76f4\u63a5\u8c03\u6574\u6a21\u578b\u884c\u4e3a\u4ee5\u66f4\u597d\u5730\u6355\u6349\u4e0d\u540c\u89c2\u70b9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5f15\u5bfc\u65b9\u6cd5\u5728\u4ec5\u4f7f\u752850\u4e2a\u6807\u6ce8\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u57fa\u7ebf\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u548c\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u7b49\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u663e\u8457\u964d\u4f4e\u8bef\u62a5\u7387\uff0c\u5e76\u5728GlobalOpinionQA\u57fa\u51c6\u4e0a\u6539\u5584\u4e86\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5206\u5e03\u5bf9\u9f50\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u5143\u5316\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff0c\u8bc1\u660e\u4e86\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u6709\u6548\u9002\u5e94\u4e0d\u540c\u89c6\u89d2\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u5305\u5bb9\u6027\u548c\u7ec6\u81f4\u7406\u89e3\u80fd\u529b\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u63a8\u52a8\u4e86\u4ef7\u503c\u89c2\u654f\u611f\u573a\u666f\u4e0b\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.16555", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16555", "abs": "https://arxiv.org/abs/2510.16555", "authors": ["Qiongyan Wang", "Xingchen Zou", "Yutian Jiang", "Haomin Wen", "Jiaheng Wei", "Qingsong Wen", "Yuxuan Liang"], "title": "Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence", "comment": null, "summary": "Rapid urbanization intensifies the demand for Urban General Intelligence\n(UGI), referring to AI systems that can understand and reason about complex\nurban environments. Recent studies have built urban foundation models using\nsupervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit\npersistent geospatial bias, producing regionally skewed predictions and limited\ngeneralization. To this end, we propose Urban-R1, a reinforcement\nlearning-based post-training framework that aligns MLLMs with the objectives of\nUGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize\nreasoning across geographic groups and employs urban region profiling as a\nproxy task to provide measurable rewards from multimodal urban data. Extensive\nexperiments across diverse regions and tasks show that Urban-R1 effectively\nmitigates geo-bias and improves cross-region generalization, outperforming both\nSFT-trained and closed-source models. Our results highlight reinforcement\nlearning alignment as a promising pathway toward equitable and trustworthy\nurban intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Urban-R1\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5730\u7406\u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u548c\u57ce\u5e02\u533a\u57df\u753b\u50cf\u4efb\u52a1\u6765\u7f13\u89e3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u57ce\u5e02\u901a\u7528\u667a\u80fd\u4e2d\u7684\u5730\u7406\u504f\u89c1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u533a\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5feb\u901f\u57ce\u5e02\u5316\u52a0\u5267\u4e86\u5bf9\u57ce\u5e02\u901a\u7528\u667a\u80fd\u7684\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u57ce\u5e02\u57fa\u7840\u6a21\u578b\u5b58\u5728\u6301\u7eed\u7684\u5730\u7406\u504f\u89c1\u95ee\u9898\uff0c\u5bfc\u81f4\u533a\u57df\u9884\u6d4b\u504f\u5dee\u548c\u6709\u9650\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u590d\u6742\u57ce\u5e02\u73af\u5883\u7684\u516c\u5e73AI\u7cfb\u7edf\u3002", "method": "Urban-R1\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u7528\u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u6765\u4f18\u5316\u8de8\u5730\u7406\u7fa4\u4f53\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4ee5\u57ce\u5e02\u533a\u57df\u753b\u50cf\u4f5c\u4e3a\u4ee3\u7406\u4efb\u52a1\uff0c\u4ece\u591a\u6a21\u6001\u57ce\u5e02\u6570\u636e\u4e2d\u63d0\u4f9b\u53ef\u6d4b\u91cf\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u8de8\u591a\u4e2a\u533a\u57df\u548c\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUrban-R1\u6709\u6548\u7f13\u89e3\u4e86\u5730\u7406\u504f\u89c1\u5e76\u663e\u8457\u6539\u5584\u4e86\u8de8\u533a\u57df\u6cdb\u5316\u6027\u80fd\uff0c\u5728\u5404\u9879\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u8bad\u7ec3\u6a21\u578b\u548c\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u662f\u5b9e\u73b0\u516c\u5e73\u53ef\u4fe1\u57ce\u5e02\u667a\u80fd\u7684\u6709\u524d\u666f\u8def\u5f84\uff0c\u4e3a\u6784\u5efa\u65e0\u5730\u7406\u504f\u89c1\u7684\u57ce\u5e02AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u6846\u67b6\u548c\u9a8c\u8bc1\u3002"}}
{"id": "2510.16290", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16290", "abs": "https://arxiv.org/abs/2510.16290", "authors": ["Yue Zheng", "Xiufang Shi", "Jiming Chen", "Yuanchao Shu"], "title": "Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models", "comment": null, "summary": "Video anomaly detection (VAD) has rapidly advanced by recent development of\nVision-Language Models (VLMs). While these models offer superior zero-shot\ndetection capabilities, their immense computational cost and unstable visual\ngrounding performance hinder real-time deployment. To overcome these\nchallenges, we introduce Cerberus, a two-stage cascaded system designed for\nefficient yet accurate real-time VAD. Cerberus learns normal behavioral rules\noffline, and combines lightweight filtering with fine-grained VLM reasoning\nduring online inference. The performance gains of Cerberus come from two key\ninnovations: motion mask prompting and rule-based deviation detection. The\nformer directs the VLM's attention to regions relevant to motion, while the\nlatter identifies anomalies as deviations from learned norms rather than\nenumerating possible anomalies. Extensive evaluations on four datasets show\nthat Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a\n151.79$\\times$ speedup, and 97.2\\% accuracy comparable to the state-of-the-art\nVLM-based VAD methods, establishing it as a practical solution for real-time\nvideo analytics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Cerberus\uff0c\u4e00\u79cd\u7528\u4e8e\u5b9e\u65f6\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u4e24\u7ea7\u7ea7\u8054\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8fc7\u6ee4\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u867d\u7136\u5177\u6709\u4f18\u8d8a\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u548c\u4e0d\u7a33\u5b9a\u7684\u89c6\u89c9\u5b9a\u4f4d\u6027\u80fd\u963b\u788d\u4e86\u5b9e\u65f6\u90e8\u7f72\u5e94\u7528\u3002", "method": "Cerberus\u91c7\u7528\u4e24\u7ea7\u7ea7\u8054\u67b6\u6784\uff0c\u5305\u62ec\u79bb\u7ebf\u5b66\u4e60\u6b63\u5e38\u884c\u4e3a\u89c4\u5219\u548c\u5728\u7ebf\u63a8\u7406\u65f6\u7684\u8f7b\u91cf\u7ea7\u8fc7\u6ee4\u4e0e\u7ec6\u7c92\u5ea6VLM\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u5173\u952e\u521b\u65b0\u5305\u62ec\u8fd0\u52a8\u63a9\u7801\u63d0\u793a\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u504f\u5dee\u68c0\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cCerberus\u5728NVIDIA L40S GPU\u4e0a\u5e73\u5747\u8fbe\u523057.68 fps\uff0c\u5b9e\u73b0\u4e86151.79\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u630197.2%\u7684\u51c6\u786e\u7387\uff0c\u4e0e\u6700\u5148\u8fdb\u7684VLM-based VAD\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "Cerberus\u901a\u8fc7\u521b\u65b0\u7684\u8fd0\u52a8\u6ce8\u610f\u529b\u5f15\u5bfc\u548c\u89c4\u5219\u504f\u5dee\u68c0\u6d4b\u673a\u5236\uff0c\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u9645\u89c6\u9891\u5206\u6790\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16387", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16387", "abs": "https://arxiv.org/abs/2510.16387", "authors": ["Fu-An Chao", "Bi-Cheng Yan", "Berlin Chen"], "title": "Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment", "comment": null, "summary": "In this paper, we explore the untapped potential of Whisper, a\nwell-established automatic speech recognition (ASR) foundation model, in the\ncontext of L2 spoken language assessment (SLA). Unlike prior studies that\nextrinsically analyze transcriptions produced by Whisper, our approach goes a\nstep further to probe its latent capabilities by extracting acoustic and\nlinguistic features from hidden representations. With only a lightweight\nclassifier being trained on top of Whisper's intermediate and final outputs,\nour method achieves strong performance on the GEPT picture-description dataset,\noutperforming existing cutting-edge baselines, including a multimodal approach.\nFurthermore, by incorporating image and text-prompt information as auxiliary\nrelevance cues, we demonstrate additional performance gains. Finally, we\nconduct an in-depth analysis of Whisper's embeddings, which reveals that, even\nwithout task-specific fine-tuning, the model intrinsically encodes both ordinal\nproficiency patterns and semantic aspects of speech, highlighting its potential\nas a powerful foundation for SLA and other spoken language understanding tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86Whisper\u8bed\u97f3\u8bc6\u522b\u57fa\u7840\u6a21\u578b\u5728\u7b2c\u4e8c\u8bed\u8a00\u53e3\u8bed\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u63d0\u53d6\u5176\u9690\u85cf\u8868\u793a\u4e2d\u7684\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u4ec5\u9700\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u5373\u53ef\u5728GEPT\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u57fa\u7ebf\uff0c\u5e76\u63ed\u793a\u4e86\u8be5\u6a21\u578b\u5185\u5728\u7f16\u7801\u4e86\u53e3\u8bed\u80fd\u529b\u7b49\u7ea7\u548c\u8bed\u4e49\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4ece\u5916\u90e8\u5206\u6790Whisper\u4ea7\u751f\u7684\u8f6c\u5f55\u6587\u672c\uff0c\u672a\u80fd\u5145\u5206\u6316\u6398\u5176\u6f5c\u5728\u80fd\u529b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22Whisper\u5728\u7b2c\u4e8c\u8bed\u8a00\u53e3\u8bed\u8bc4\u4f30\u4e2d\u7684\u9690\u85cf\u8868\u5f81\u80fd\u529b\uff0c\u586b\u8865\u4e86\u76f4\u63a5\u5229\u7528\u57fa\u7840\u6a21\u578b\u5185\u90e8\u8868\u793a\u8fdb\u884c\u53e3\u8bed\u80fd\u529b\u8bc4\u4f30\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u63d0\u53d6Whisper\u9690\u85cf\u8868\u793a\u4e2d\u7684\u58f0\u5b66\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u4ec5\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u4e8e\u5176\u4e2d\u95f4\u548c\u6700\u7ec8\u8f93\u51fa\u4e4b\u4e0a\uff0c\u5e76\u5f15\u5165\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u4fe1\u606f\u4f5c\u4e3a\u8f85\u52a9\u76f8\u5173\u6027\u7ebf\u7d22\u6765\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728GEPT\u56fe\u7247\u63cf\u8ff0\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5f3a\u52b2\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5305\u62ec\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u5185\u7684\u73b0\u6709\u5148\u8fdb\u57fa\u7ebf\uff0c\u901a\u8fc7\u6574\u5408\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u4fe1\u606f\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u8868\u73b0\uff0c\u6df1\u5165\u5206\u6790\u663e\u793aWhisper\u5d4c\u5165\u5185\u5728\u7f16\u7801\u4e86\u53e3\u8bed\u80fd\u529b\u7b49\u7ea7\u6a21\u5f0f\u548c\u8bed\u97f3\u8bed\u4e49\u65b9\u9762\u3002", "conclusion": "\u5373\u4f7f\u6ca1\u6709\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\uff0cWhisper\u6a21\u578b\u672c\u8d28\u4e0a\u7f16\u7801\u4e86\u53e3\u8bed\u80fd\u529b\u7684\u5e8f\u6570\u6a21\u5f0f\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u7a81\u663e\u4e86\u5176\u4f5c\u4e3a\u53e3\u8bed\u8bc4\u4f30\u53ca\u5176\u4ed6\u53e3\u8bed\u7406\u89e3\u4efb\u52a1\u7684\u5f3a\u5927\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4e3a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u53e3\u8bed\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.16572", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16572", "abs": "https://arxiv.org/abs/2510.16572", "authors": ["Ayush Chopra", "Aman Sharma", "Feroz Ahmad", "Luca Muscariello", "Vijoy Pandey", "Ramesh Raskar"], "title": "Ripple Effect Protocol: Coordinating Agent Populations", "comment": null, "summary": "Modern AI agents can exchange messages using protocols such as A2A and ACP,\nyet these mechanisms emphasize communication over coordination. As agent\npopulations grow, this limitation produces brittle collective behavior, where\nindividually smart agents converge on poor group outcomes. We introduce the\nRipple Effect Protocol (REP), a coordination protocol in which agents share not\nonly their decisions but also lightweight sensitivities - signals expressing\nhow their choices would change if key environmental variables shifted. These\nsensitivities ripple through local networks, enabling groups to align faster\nand more stably than with agent-centric communication alone. We formalize REP's\nprotocol specification, separating required message schemas from optional\naggregation rules, and evaluate it across scenarios with varying incentives and\nnetwork topologies. Benchmarks across three domains: (i) supply chain cascades\n(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),\nand (iii) sustainable resource allocation (Fishbanks) show that REP improves\ncoordination accuracy and efficiency over A2A by 41 to 100%, while flexibly\nhandling multimodal sensitivity signals from LLMs. By making coordination a\nprotocol-level capability, REP provides scalable infrastructure for the\nemerging Internet of Agents", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Ripple Effect Protocol (REP)\uff0c\u4e00\u79cd\u534f\u8c03\u534f\u8bae\uff0c\u901a\u8fc7\u8ba9\u667a\u80fd\u4f53\u5171\u4eab\u51b3\u7b56\u548c\u8f7b\u91cf\u7ea7\u654f\u611f\u6027\u4fe1\u53f7\u6765\u6539\u5584\u7fa4\u4f53\u534f\u8c03\u6548\u7387\uff0c\u76f8\u6bd4\u4f20\u7edfA2A\u534f\u8bae\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e8641%\u81f3100%\u7684\u534f\u8c03\u51c6\u786e\u6027\u548c\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u4ee3AI\u667a\u80fd\u4f53\u4f7f\u7528A2A\u548cACP\u7b49\u534f\u8bae\u8fdb\u884c\u901a\u4fe1\uff0c\u4f46\u8fd9\u4e9b\u673a\u5236\u5f3a\u8c03\u901a\u4fe1\u800c\u975e\u534f\u8c03\uff0c\u968f\u7740\u667a\u80fd\u4f53\u7fa4\u4f53\u89c4\u6a21\u6269\u5927\uff0c\u8fd9\u79cd\u5c40\u9650\u6027\u5bfc\u81f4\u8106\u5f31\u7684\u96c6\u4f53\u884c\u4e3a\uff0c\u5373\u4f7f\u4e2a\u4f53\u667a\u80fd\u4f53\u8868\u73b0\u4f18\u79c0\u4e5f\u4f1a\u4ea7\u751f\u4e0d\u826f\u7fa4\u4f53\u7ed3\u679c\u3002", "method": "REP\u534f\u8bae\u8ba9\u667a\u80fd\u4f53\u4e0d\u4ec5\u5171\u4eab\u51b3\u7b56\uff0c\u8fd8\u5171\u4eab\u8f7b\u91cf\u7ea7\u654f\u611f\u6027\u4fe1\u53f7\u2014\u2014\u8868\u8fbe\u5173\u952e\u73af\u5883\u53d8\u91cf\u53d8\u5316\u65f6\u5176\u9009\u62e9\u5c06\u5982\u4f55\u6539\u53d8\u7684\u4fe1\u53f7\uff0c\u8fd9\u4e9b\u654f\u611f\u6027\u5728\u5c40\u90e8\u7f51\u7edc\u4e2d\u4f20\u64ad\uff0c\u4f7f\u7fa4\u4f53\u80fd\u591f\u6bd4\u5355\u72ec\u4f7f\u7528\u667a\u80fd\u4f53\u4e2d\u5fc3\u901a\u4fe1\u66f4\u5feb\u66f4\u7a33\u5b9a\u5730\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1a\u4f9b\u5e94\u94fe\u7ea7\u8054\uff08\u5564\u9152\u6e38\u620f\uff09\u3001\u7a00\u758f\u7f51\u7edc\u4e2d\u7684\u504f\u597d\u805a\u5408\uff08\u7535\u5f71\u8c03\u5ea6\uff09\u548c\u53ef\u6301\u7eed\u8d44\u6e90\u5206\u914d\uff08\u6e14\u4e1a\u94f6\u884c\uff09\uff0cREP\u76f8\u6bd4A2A\u534f\u8bae\u5c06\u534f\u8c03\u51c6\u786e\u6027\u548c\u6548\u7387\u63d0\u9ad8\u4e8641%\u81f3100%\uff0c\u5e76\u80fd\u7075\u6d3b\u5904\u7406\u6765\u81eaLLM\u7684\u591a\u6a21\u6001\u654f\u611f\u6027\u4fe1\u53f7\u3002", "conclusion": "\u901a\u8fc7\u5c06\u534f\u8c03\u4f5c\u4e3a\u534f\u8bae\u7ea7\u80fd\u529b\uff0cREP\u4e3a\u65b0\u5174\u7684\u667a\u80fd\u4f53\u4e92\u8054\u7f51\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u4f7f\u667a\u80fd\u4f53\u7fa4\u4f53\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u6548\u7a33\u5b9a\u7684\u96c6\u4f53\u51b3\u7b56\u548c\u8d44\u6e90\u5206\u914d\u3002"}}
{"id": "2510.16295", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16295", "abs": "https://arxiv.org/abs/2510.16295", "authors": ["Ryoto Miyamoto", "Xin Fan", "Fuyuko Kido", "Tsuneo Matsumoto", "Hayato Yamana"], "title": "OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models", "comment": null, "summary": "OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in\nevaluating membership inference attacks (MIA) against large vision-language\nmodels (LVLMs). While prior work has reported high attack success rates, our\nanalysis suggests that these results often arise from detecting distributional\nbias introduced during dataset construction rather than from identifying true\nmembership status. To address this issue, we introduce a controlled benchmark\nof 6{,}000 images where the distributions of member and non-member samples are\ncarefully balanced, and ground-truth membership labels are provided across\nthree distinct training stages. Experiments using OpenLVLM-MIA demonstrated\nthat the performance of state-of-the-art MIA methods converged to random chance\nunder unbiased conditions. By offering a transparent and unbiased benchmark,\nOpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and\nprovides a solid foundation for developing stronger privacy-preserving\ntechniques.", "AI": {"tldr": "OpenLVLM-MIA\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6210\u5458\u63a8\u7406\u653b\u51fb\u65f6\u5b58\u5728\u7684\u6839\u672c\u6027\u6311\u6218\u3002\u7814\u7a76\u53d1\u73b0\u5148\u524d\u62a5\u9053\u7684\u9ad8\u653b\u51fb\u6210\u529f\u7387\u4e3b\u8981\u6e90\u4e8e\u6570\u636e\u96c6\u6784\u5efa\u5f15\u5165\u7684\u5206\u5e03\u504f\u5dee\u68c0\u6d4b\uff0c\u800c\u975e\u771f\u5b9e\u7684\u6210\u5458\u72b6\u6001\u8bc6\u522b\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6210\u5458\u63a8\u7406\u653b\u51fb\u65f6\u5b58\u5728\u7684\u6839\u672c\u95ee\u9898\u3002\u5148\u524d\u5de5\u4f5c\u62a5\u544a\u7684\u9ad8\u653b\u51fb\u6210\u529f\u7387\u53ef\u80fd\u6e90\u4e8e\u6570\u636e\u96c6\u6784\u5efa\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7684\u5206\u5e03\u504f\u5dee\uff0c\u800c\u975e\u771f\u5b9e\u7684\u6210\u5458\u72b6\u6001\u8bc6\u522b\u80fd\u529b\uff0c\u8fd9\u5bfc\u81f4\u4e86\u5bf9\u653b\u51fb\u65b9\u6cd5\u771f\u5b9e\u6027\u80fd\u7684\u8bef\u89e3\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u5305\u542b6,000\u5f20\u56fe\u50cf\u7684\u53d7\u63a7\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5176\u4e2d\u6210\u5458\u548c\u975e\u6210\u5458\u6837\u672c\u7684\u5206\u5e03\u7ecf\u8fc7\u7cbe\u5fc3\u5e73\u8861\uff0c\u5e76\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u8bad\u7ec3\u9636\u6bb5\u63d0\u4f9b\u4e86\u771f\u5b9e\u6210\u5458\u6807\u7b7e\u3002\u8be5\u57fa\u51c6\u901a\u8fc7\u6d88\u9664\u5206\u5e03\u504f\u5dee\u6765\u786e\u4fdd\u8bc4\u4f30\u7684\u516c\u6b63\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u65e0\u504f\u6761\u4ef6\u4e0b\uff0c\u6700\u5148\u8fdb\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\u7684\u6027\u80fd\u6536\u655b\u4e8e\u968f\u673a\u731c\u6d4b\u6c34\u5e73\u3002\u8fd9\u8bc1\u5b9e\u4e86\u5148\u524d\u62a5\u9053\u7684\u9ad8\u653b\u51fb\u6210\u529f\u7387\u4e3b\u8981\u6e90\u4e8e\u6570\u636e\u96c6\u504f\u5dee\u800c\u975e\u771f\u5b9e\u7684\u653b\u51fb\u80fd\u529b\u3002", "conclusion": "OpenLVLM-MIA\u57fa\u51c6\u63ed\u793a\u4e86\u5f53\u524dLVLM\u6210\u5458\u63a8\u7406\u653b\u51fb\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u8bc4\u4f30\u9690\u79c1\u653b\u51fb\u65b9\u6cd5\u65f6\u6d88\u9664\u6570\u636e\u96c6\u504f\u5dee\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.16455", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16455", "abs": "https://arxiv.org/abs/2510.16455", "authors": ["Deyi Ji", "Yuekui Yang", "Haiyang Wu", "Shaoping Ma", "Tianrun Chen", "Lanyun Zhu"], "title": "RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning", "comment": "ACL 2025 (Oral, Industry Track)", "summary": "Advertisement (Ad) video violation detection is critical for ensuring\nplatform compliance, but existing methods struggle with precise temporal\ngrounding, noisy annotations, and limited generalization. We propose RAVEN, a\nnovel framework that integrates curriculum reinforcement learning with\nmultimodal large language models (MLLMs) to enhance reasoning and cognitive\ncapabilities for violation detection. RAVEN employs a progressive training\nstrategy, combining precisely and coarsely annotated data, and leverages Group\nRelative Policy Optimization (GRPO) to develop emergent reasoning abilities\nwithout explicit reasoning annotations. Multiple hierarchical sophisticated\nreward mechanism ensures precise temporal grounding and consistent category\nprediction. Experiments on industrial datasets and public benchmarks show that\nRAVEN achieves superior performances in violation category accuracy and\ntemporal interval localization. We also design a pipeline to deploy the RAVEN\non the online Ad services, and online A/B testing further validates its\npractical applicability, with significant improvements in precision and recall.\nRAVEN also demonstrates strong generalization, mitigating the catastrophic\nforgetting issue associated with supervised fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRAVEN\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5e7f\u544a\u89c6\u9891\u8fdd\u89c4\u68c0\u6d4b\u4e2d\u7684\u65f6\u5e8f\u5b9a\u4f4d\u4e0d\u7cbe\u786e\u3001\u6807\u6ce8\u566a\u58f0\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7b49\u95ee\u9898\uff0c\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u548c\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5e7f\u544a\u89c6\u9891\u8fdd\u89c4\u68c0\u6d4b\u65b9\u6cd5\u5728\u7cbe\u786e\u65f6\u5e8f\u5b9a\u4f4d\u3001\u566a\u58f0\u6807\u6ce8\u5904\u7406\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u96be\u4ee5\u6ee1\u8db3\u5e73\u53f0\u5408\u89c4\u6027\u8981\u6c42\u7684\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "RAVEN\u6846\u67b6\u6574\u5408\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u7ed3\u5408\u7cbe\u786e\u548c\u7c97\u7565\u6807\u6ce8\u6570\u636e\uff0c\u5229\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u5f00\u53d1\u6d8c\u73b0\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u591a\u5c42\u590d\u6742\u5956\u52b1\u673a\u5236\u786e\u4fdd\u7cbe\u786e\u65f6\u5e8f\u5b9a\u4f4d\u548c\u4e00\u81f4\u7c7b\u522b\u9884\u6d4b\u3002", "result": "\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u548c\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAVEN\u5728\u8fdd\u89c4\u7c7b\u522b\u51c6\u786e\u7387\u548c\u65f6\u5e8f\u533a\u95f4\u5b9a\u4f4d\u65b9\u9762\u5747\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u9002\u7528\u6027\uff0c\u5728\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RAVEN\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e7f\u544a\u8fdd\u89c4\u68c0\u6d4b\u7684\u5173\u952e\u6311\u6218\uff0c\u5176\u5728\u7ebf\u90e8\u7f72\u9a8c\u8bc1\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u4e3a\u7c7b\u4f3c\u65f6\u5e8f\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.16658", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16658", "abs": "https://arxiv.org/abs/2510.16658", "authors": ["Shihao Yang", "Xiying Huang", "Danilo Bernardo", "Jun-En Ding", "Andrew Michael", "Jingmei Yang", "Patrick Kwan", "Ashish Raj", "Feng Liu"], "title": "Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review", "comment": null, "summary": "The advent of large-scale artificial intelligence (AI) models has a\ntransformative effect on neuroscience research, which represents a paradigm\nshift from the traditional computational methods through the facilitation of\nend-to-end learning from raw brain signals and neural data. In this paper, we\nexplore the transformative effects of large-scale AI models on five major\nneuroscience domains: neuroimaging and data processing, brain-computer\ninterfaces and neural decoding, molecular neuroscience and genomic modeling,\nclinical assistance and translational frameworks, and disease-specific\napplications across neurological and psychiatric disorders. These models are\ndemonstrated to address major computational neuroscience challenges, including\nmultimodal neural data integration, spatiotemporal pattern interpretation, and\nthe derivation of translational frameworks for clinical deployment. Moreover,\nthe interaction between neuroscience and AI has become increasingly reciprocal,\nas biologically informed architectural constraints are now incorporated to\ndevelop more interpretable and computationally efficient models. This review\nhighlights both the notable promise of such technologies and key implementation\nconsiderations, with particular emphasis on rigorous evaluation frameworks,\neffective domain knowledge integration, and comprehensive ethical guidelines\nfor clinical use. Finally, a systematic listing of critical neuroscience\ndatasets used to derive and validate large-scale AI models across diverse\nresearch applications is provided.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u89c4\u6a21AI\u6a21\u578b\u5728\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u7684\u53d8\u9769\u6027\u5f71\u54cd\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u7aef\u5230\u7aef\u5b66\u4e60\u4ece\u539f\u59cb\u8111\u4fe1\u53f7\u4e2d\u89e3\u51b3\u591a\u6a21\u6001\u795e\u7ecf\u6570\u636e\u6574\u5408\u3001\u65f6\u7a7a\u6a21\u5f0f\u89e3\u91ca\u7b49\u5173\u952e\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u6311\u6218\uff0c\u5e76\u63a2\u8ba8\u4e86\u795e\u7ecf\u79d1\u5b66\u4e0eAI\u4e4b\u95f4\u7684\u53cc\u5411\u4e92\u52a8\u5173\u7cfb\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u65b9\u6cd5\u5728\u5904\u7406\u795e\u7ecf\u79d1\u5b66\u6570\u636e\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u89e3\u51b3\u591a\u6a21\u6001\u795e\u7ecf\u6570\u636e\u6574\u5408\u3001\u65f6\u7a7a\u6a21\u5f0f\u89e3\u91ca\u4ee5\u53ca\u4e34\u5e8a\u90e8\u7f72\u7684\u8f6c\u5316\u6846\u67b6\u7b49\u4e3b\u8981\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u6311\u6218\uff0c\u800c\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u8fd9\u4e9b\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u9014\u5f84\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5927\u89c4\u6a21AI\u6a21\u578b\u8fdb\u884c\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u8111\u4fe1\u53f7\u548c\u795e\u7ecf\u6570\u636e\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u6574\u5408\u751f\u7269\u5b66\u542f\u53d1\u7684\u67b6\u6784\u7ea6\u675f\u6765\u5f00\u53d1\u66f4\u53ef\u89e3\u91ca\u548c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff0c\u8986\u76d6\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u5904\u7406\u3001\u8111\u673a\u63a5\u53e3\u3001\u5206\u5b50\u795e\u7ecf\u79d1\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u3002", "result": "\u5927\u89c4\u6a21AI\u6a21\u578b\u5728\u4e94\u4e2a\u4e3b\u8981\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u5c55\u73b0\u51fa\u663e\u8457\u6548\u679c\uff1a\u795e\u7ecf\u5f71\u50cf\u4e0e\u6570\u636e\u5904\u7406\u3001\u8111\u673a\u63a5\u53e3\u4e0e\u795e\u7ecf\u89e3\u7801\u3001\u5206\u5b50\u795e\u7ecf\u79d1\u5b66\u4e0e\u57fa\u56e0\u7ec4\u5efa\u6a21\u3001\u4e34\u5e8a\u8f85\u52a9\u4e0e\u8f6c\u5316\u6846\u67b6\uff0c\u4ee5\u53ca\u8de8\u795e\u7ecf\u548c\u7cbe\u795e\u75be\u75c5\u7684\u75be\u75c5\u7279\u5f02\u6027\u5e94\u7528\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u548c\u65f6\u7a7a\u6a21\u5f0f\u89e3\u91ca\u7b49\u5173\u952e\u95ee\u9898\u3002", "conclusion": "\u5927\u89c4\u6a21AI\u6a21\u578b\u4e3a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u5e26\u6765\u4e86\u8303\u5f0f\u8f6c\u53d8\uff0c\u4f46\u6210\u529f\u5b9e\u65bd\u9700\u8981\u5f3a\u8c03\u4e25\u683c\u7684\u8bc4\u4f30\u6846\u67b6\u3001\u6709\u6548\u7684\u9886\u57df\u77e5\u8bc6\u6574\u5408\u4ee5\u53ca\u5168\u9762\u7684\u4e34\u5e8a\u4f7f\u7528\u4f26\u7406\u6307\u5357\uff0c\u540c\u65f6\u795e\u7ecf\u79d1\u5b66\u4e0eAI\u7684\u4e92\u52a8\u65e5\u76ca\u53cc\u5411\u5316\uff0c\u751f\u7269\u5b66\u7ea6\u675f\u7684\u6574\u5408\u4fc3\u8fdb\u4e86\u66f4\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.16325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16325", "abs": "https://arxiv.org/abs/2510.16325", "authors": ["Yuyao Zhang", "Yu-Wing Tai"], "title": "Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention", "comment": "22 pages", "summary": "Ultra-high-resolution text-to-image generation demands both fine-grained\ntexture synthesis and globally coherent structure, yet current diffusion models\nremain constrained to sub-$1K \\times 1K$ resolutions due to the prohibitive\nquadratic complexity of attention and the scarcity of native $4K$ training\ndata. We present \\textbf{Scale-DiT}, a new diffusion framework that introduces\nhierarchical local attention with low-resolution global guidance, enabling\nefficient, scalable, and semantically coherent image synthesis at ultra-high\nresolutions. Specifically, high-resolution latents are divided into fixed-size\nlocal windows to reduce attention complexity from quadratic to near-linear,\nwhile a low-resolution latent equipped with scaled positional anchors injects\nglobal semantics. A lightweight LoRA adaptation bridges global and local\npathways during denoising, ensuring consistency across structure and detail. To\nmaximize inference efficiency, we repermute token sequence in Hilbert curve\norder and implement a fused-kernel for skipping masked operations, resulting in\na GPU-friendly design. Extensive experiments demonstrate that Scale-DiT\nachieves more than $2\\times$ faster inference and lower memory usage compared\nto dense attention baselines, while reliably scaling to $4K \\times 4K$\nresolution without requiring additional high-resolution training data. On both\nquantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,\nScale-DiT delivers superior global coherence and sharper local detail, matching\nor outperforming state-of-the-art methods that rely on native 4K training.\nTaken together, these results highlight hierarchical local attention with\nguided low-resolution anchors as a promising and effective approach for\nadvancing ultra-high-resolution image generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Scale-DiT\uff0c\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5c40\u90e8\u6ce8\u610f\u529b\u4e0e\u4f4e\u5206\u8fa8\u7387\u5168\u5c40\u5f15\u5bfc\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u8d85\u9ad8\u6e05\u56fe\u50cf\u7684\u9ad8\u6548\u751f\u6210\uff0c\u65e0\u9700\u989d\u5916\u7684\u9ad8\u5206\u8fa8\u7387\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u6269\u5c55\u52304K\u5206\u8fa8\u7387\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u53d7\u9650\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u6027\u548c\u539f\u751f4K\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\uff0c\u65e0\u6cd5\u5b9e\u73b0\u8d85\u9ad8\u6e05\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7eb9\u7406\u5408\u6210\u548c\u5168\u5c40\u7ed3\u6784\u4e00\u81f4\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "Scale-DiT\u91c7\u7528\u5206\u5c42\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u9ad8\u5206\u8fa8\u7387\u6f5c\u53d8\u91cf\u5212\u5206\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u5c40\u90e8\u7a97\u53e3\u4ee5\u964d\u4f4e\u6ce8\u610f\u529b\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4f7f\u7528\u914d\u5907\u7f29\u653e\u4f4d\u7f6e\u951a\u70b9\u7684\u4f4e\u5206\u8fa8\u7387\u6f5c\u53d8\u91cf\u6ce8\u5165\u5168\u5c40\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7LoRA\u9002\u914d\u5668\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6865\u63a5\u5168\u5c40\u548c\u5c40\u90e8\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u8868\u660eScale-DiT\u76f8\u6bd4\u5bc6\u96c6\u6ce8\u610f\u529b\u57fa\u7ebf\u5b9e\u73b0\u4e862\u500d\u4ee5\u4e0a\u7684\u63a8\u7406\u52a0\u901f\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u80fd\u591f\u57284K\u00d74K\u5206\u8fa8\u7387\u4e0b\u751f\u6210\u5177\u6709\u4f18\u8d8a\u5168\u5c40\u4e00\u81f4\u6027\u548c\u66f4\u6e05\u6670\u5c40\u90e8\u7ec6\u8282\u7684\u56fe\u50cf\uff0c\u5728FID\u3001IS\u548cCLIP Score\u7b49\u5b9a\u91cf\u6307\u6807\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u4f9d\u8d56\u539f\u751f4K\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5206\u5c42\u5c40\u90e8\u6ce8\u610f\u529b\u4e0e\u5f15\u5bfc\u6027\u4f4e\u5206\u8fa8\u7387\u951a\u70b9\u76f8\u7ed3\u5408\u662f\u63a8\u8fdb\u8d85\u9ad8\u6e05\u56fe\u50cf\u751f\u6210\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u65e0\u9700\u989d\u5916\u9ad8\u5206\u8fa8\u7387\u8bad\u7ec3\u6570\u636e\u7684\u5927\u89c4\u6a21\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.16797", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16797", "abs": "https://arxiv.org/abs/2510.16797", "authors": ["Vera Pavlova", "Mohammed Makhlouf"], "title": "MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning", "comment": null, "summary": "We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain\nContrastive learning), a multi-stage framework for domain adaptation of\nsentence embedding models that incorporates joint domain-specific masked\nsupervision. Our approach addresses the challenges of adapting large-scale\ngeneral-domain sentence embedding models to specialized domains. By jointly\noptimizing masked language modeling (MLM) and contrastive objectives within a\nunified training pipeline, our method enables effective learning of\ndomain-relevant representations while preserving the robust semantic\ndiscrimination properties of the original model. We empirically validate our\napproach on both high-resource and low-resource domains, achieving improvements\nup to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong\ngeneral-domain baselines. Comprehensive ablation studies further demonstrate\nthe effectiveness of each component, highlighting the importance of balanced\njoint supervision and staged adaptation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MOSAIC\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u9886\u57df\u9002\u5e94\u7684\u591a\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u548c\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u5728\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u9c81\u68d2\u8bed\u4e49\u533a\u5206\u80fd\u529b\u7684\u540c\u65f6\u6709\u6548\u5b66\u4e60\u9886\u57df\u76f8\u5173\u8868\u793a\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5c06\u5927\u89c4\u6a21\u901a\u7528\u9886\u57df\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u9002\u5e94\u5230\u4e13\u4e1a\u9886\u57df\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5982\u4f55\u5728\u4fdd\u6301\u6a21\u578b\u539f\u6709\u8bed\u4e49\u533a\u5206\u80fd\u529b\u7684\u540c\u65f6\u6709\u6548\u5b66\u4e60\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002", "method": "MOSAIC\u6846\u67b6\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u548c\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u7ed3\u5408\u9009\u62e9\u6027\u9002\u5e94\u673a\u5236\uff0c\u5728\u7edf\u4e00\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u5b9e\u73b0\u9886\u57df\u76f8\u5173\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u9ad8\u4f4e\u8d44\u6e90\u9886\u57df\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728NDCG@10\u6307\u6807\u4e0a\u76f8\u6bd4\u5f3a\u901a\u7528\u9886\u57df\u57fa\u7ebf\u63d0\u5347\u4e86\u9ad8\u8fbe13.4%\uff0c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5404\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5e73\u8861\u7684\u8054\u5408\u76d1\u7763\u548c\u5206\u9636\u6bb5\u9002\u5e94\u7b56\u7565\u5bf9\u4e8e\u9886\u57df\u9002\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u7684\u9886\u57df\u4e13\u4e1a\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f3a\u8c03\u4e86\u591a\u76ee\u6807\u8054\u5408\u4f18\u5316\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.16753", "categories": ["cs.AI", "68T30", "H.3.3"], "pdf": "https://arxiv.org/pdf/2510.16753", "abs": "https://arxiv.org/abs/2510.16753", "authors": ["Wei Huang", "Peining Li", "Meiyu Liang", "Xu Hou", "Junping Du", "Yingxia Shao", "Guanhua Ye", "Wu Liu", "Kangkang Lu", "Yang Yu"], "title": "ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion", "comment": "11 pages, 4 figures", "summary": "Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by\nincorporating visual and textual modalities, enabling richer and more\nexpressive entity representations. However, existing MKGs often suffer from\nincompleteness, which hinder their effectiveness in downstream tasks.\nTherefore, multimodal knowledge graph completion (MKGC) task is receiving\nincreasing attention. While large language models (LLMs) have shown promise for\nknowledge graph completion (KGC), their application to the multimodal setting\nremains underexplored. Moreover, applying Multimodal Large Language Models\n(MLLMs) to the task of MKGC introduces significant challenges: (1) the large\nnumber of image tokens per entity leads to semantic noise and modality\nconflicts, and (2) the high computational cost of processing large token\ninputs. To address these issues, we propose Efficient Lightweight Multimodal\nLarge Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token\nCompressor (MVTC) based on multi-head attention mechanism, which adaptively\ncompresses image tokens from both textual and visual views, thereby effectively\nreducing redundancy while retaining necessary information and avoiding modality\nconflicts. Additionally, we design an attention pruning strategy to remove\nredundant attention layers from MLLMs, thereby significantly reducing the\ninference cost. We further introduce a linear projection to compensate for the\nperformance degradation caused by pruning. Extensive experiments on benchmark\nFB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art\nperformance while substantially improving computational efficiency,\nestablishing a new paradigm for multimodal knowledge graph completion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faELMM\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u89c6\u89c9\u4ee4\u724c\u538b\u7f29\u5668\u548c\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\u56fe\u50cf\u4ee4\u724c\u5197\u4f59\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u5b58\u5728\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u5e94\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8865\u5168\u65f6\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u56fe\u50cf\u4ee4\u724c\u6570\u91cf\u8fc7\u591a\u5bfc\u81f4\u8bed\u4e49\u566a\u58f0\u548c\u6a21\u6001\u51b2\u7a81\uff0c\u4ee5\u53ca\u5904\u7406\u5927\u91cf\u4ee4\u724c\u8f93\u5165\u5e26\u6765\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51faELMM\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u591a\u5934\u6ce8\u610f\u529b\u7684\u591a\u89c6\u89d2\u89c6\u89c9\u4ee4\u724c\u538b\u7f29\u5668\uff0c\u4ece\u6587\u672c\u548c\u89c6\u89c9\u89c6\u89d2\u81ea\u9002\u5e94\u538b\u7f29\u56fe\u50cf\u4ee4\u724c\u4ee5\u51cf\u5c11\u5197\u4f59\u5e76\u907f\u514d\u6a21\u6001\u51b2\u7a81\uff1b\u540c\u65f6\u8bbe\u8ba1\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\u79fb\u9664\u5197\u4f59\u6ce8\u610f\u529b\u5c42\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u6295\u5f71\u8865\u507f\u526a\u679d\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\u3002", "result": "\u5728FB15k-237-IMG\u548cWN18-IMG\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cELMM\u5728\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5efa\u7acb\u4e86\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u65b0\u8303\u5f0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u6709\u6548\u7684\u4ee4\u724c\u538b\u7f29\u548c\u6a21\u578b\u526a\u679d\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u7387\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.16333", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16333", "abs": "https://arxiv.org/abs/2510.16333", "authors": ["Junha Song", "Sangdoo Yun", "Dongyoon Han", "Jaegul Choo", "Byeongho Heo"], "title": "RL makes MLLMs see better than SFT", "comment": null, "summary": "A dominant assumption in Multimodal Language Model (MLLM) research is that\nits performance is largely inherited from the LLM backbone, given its immense\nparameter scale and remarkable capabilities. This has created a void in the\nunderstanding of the vision encoder, which determines how MLLMs perceive\nimages. The recent shift in MLLM training paradigms, from Supervised Finetuning\n(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the\nsignificant lack of analysis on how such training reshapes the vision encoder\nas well as the MLLM. To address this, we first investigate the impact of\ntraining strategies on MLLMs, where RL shows a clear advantage over SFT in\nstrongly vision-related VQA benchmarks. Motivated by this, we conduct a\ncritical yet under-explored analysis of the vision encoder of MLLMs through\ndiverse and in-depth experiments, ranging from ImageNet classification and\nsegmentation to gradient visualization. Our results demonstrate that MLLM's\npost-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on\nMLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual\nrepresentations. Specifically, the key finding of our study is that RL produces\nstronger and precisely localized visual representations compared to SFT,\nboosting the ability of the vision encoder for MLLM. We then reframe our\nfindings into a simple recipe for building strong vision encoders for MLLMs,\nPreference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,\na PIVOT-trained vision encoder outperforms even larger and more heavily-trained\ncounterparts, despite requiring less than 1% of the computational cost of\nstandard vision pretraining. This result opens an effective and efficient path\nfor advancing the vision backbones of MLLMs. Project page available at\nhttps://june-page.github.io/pivot/", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\u80fd\u591f\u663e\u8457\u589e\u5f3a\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u7f16\u7801\u5668\u7684\u8868\u5f81\u80fd\u529b\uff0c\u63d0\u51fa\u4e86PIVOT\u65b9\u6cd5\uff0c\u4ec5\u9700\u6807\u51c6\u89c6\u89c9\u9884\u8bad\u7ec31%\u7684\u8ba1\u7b97\u6210\u672c\u5373\u53ef\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u7f16\u7801\u5668\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u666e\u904d\u5047\u8bbe\u6027\u80fd\u4e3b\u8981\u7ee7\u627f\u81eaLLM\u9aa8\u5e72\u7f51\u7edc\uff0c\u5bfc\u81f4\u5bf9\u89c6\u89c9\u7f16\u7801\u5668\u7684\u7406\u89e3\u5b58\u5728\u7a7a\u767d\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u8303\u5f0f\u4ece\u76d1\u7763\u5fae\u8c03\u8f6c\u5411\u5f3a\u5316\u5b66\u4e60\u7684\u80cc\u666f\u4e0b\uff0c\u7f3a\u4e4f\u5bf9\u8bad\u7ec3\u7b56\u7565\u5982\u4f55\u91cd\u5851\u89c6\u89c9\u7f16\u7801\u5668\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u901a\u8fc7\u591a\u6837\u5316\u7684\u6df1\u5ea6\u5b9e\u9a8c\u5206\u6790\u8bad\u7ec3\u7b56\u7565\u5bf9\u89c6\u89c9\u7f16\u7801\u5668\u7684\u5f71\u54cd\uff0c\u5305\u62ecImageNet\u5206\u7c7b\u3001\u5206\u5272\u4efb\u52a1\u548c\u68af\u5ea6\u53ef\u89c6\u5316\uff0c\u5e76\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\u63d0\u51fa\u4e86Preference-Instructed Vision OpTimization (PIVOT)\u65b9\u6cd5\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u76f8\u6bd4\u76d1\u7763\u5fae\u8c03\u5728\u5f3a\u89c6\u89c9\u76f8\u5173\u7684VQA\u57fa\u51c6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u5f3a\u4e14\u7cbe\u786e\u5b9a\u4f4d\u7684\u89c6\u89c9\u8868\u5f81\uff0cPIVOT\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5373\u4f7f\u8ba1\u7b97\u6210\u672c\u4e0d\u8db3\u6807\u51c6\u9884\u8bad\u7ec3\u76841%\uff0c\u4e5f\u80fd\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u5bf9\u7b49\u6a21\u578b\u3002", "conclusion": "\u8bad\u7ec3\u7b56\u7565\u4e0d\u4ec5\u5f71\u54cd\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u8fd8\u4ece\u6839\u672c\u4e0a\u91cd\u5851\u5176\u5e95\u5c42\u89c6\u89c9\u8868\u5f81\uff0cPIVOT\u65b9\u6cd5\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u6761\u9ad8\u6548\u8def\u5f84\uff0c\u6311\u6218\u4e86\u5f53\u524d\u5bf9\u89c6\u89c9\u7f16\u7801\u5668\u4f5c\u7528\u7684\u4f4e\u4f30\u8ba4\u77e5\u3002"}}
{"id": "2510.16844", "categories": ["cs.CL", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16844", "abs": "https://arxiv.org/abs/2510.16844", "authors": ["Jiajie Jin", "Yuyao Zhang", "Yimeng Xu", "Hongjin Qian", "Yutao Zhu", "Zhicheng Dou"], "title": "FinSight: Towards Real-World Financial Deep Research", "comment": "Working in progress", "summary": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FinSight\uff0c\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u8d22\u52a1\u62a5\u544a\u7684\u65b0\u578b\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u7f16\u7a0b\u53d8\u91cf\u7a7a\u95f4\u548c\u8fed\u4ee3\u89c6\u89c9\u589e\u5f3a\u673a\u5236\uff0c\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5206\u6790\u6df1\u5ea6\u548c\u5448\u73b0\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u96be\u4ee5\u5b8c\u5168\u81ea\u52a8\u5316\u751f\u6210\u4e13\u4e1a\u8d22\u52a1\u62a5\u544a\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u65e2\u8017\u65f6\u53c8\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7075\u6d3b\u6536\u96c6\u6570\u636e\u3001\u8fdb\u884c\u5206\u6790\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u62a5\u544a\u7684\u667a\u80fd\u7cfb\u7edf\u3002", "method": "FinSight\u91c7\u7528\u5e26\u53ef\u53d8\u5185\u5b58\u7684\u4ee3\u7801\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5c06\u5916\u90e8\u6570\u636e\u3001\u8bbe\u8ba1\u5de5\u5177\u548c\u667a\u80fd\u4f53\u7edf\u4e00\u5230\u53ef\u7f16\u7a0b\u53d8\u91cf\u7a7a\u95f4\u4e2d\uff1b\u63d0\u51fa\u8fed\u4ee3\u89c6\u89c9\u589e\u5f3a\u673a\u5236\u9010\u6b65\u4f18\u5316\u539f\u59cb\u89c6\u89c9\u8f93\u51fa\u4e3a\u4e13\u4e1a\u8d22\u52a1\u56fe\u8868\uff1b\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u5199\u4f5c\u6846\u67b6\u5c06\u7b80\u6d01\u7684\u5206\u6790\u94fe\u6269\u5c55\u4e3a\u8fde\u8d2f\u3001\u5f15\u7528\u611f\u77e5\u7684\u591a\u6a21\u6001\u62a5\u544a\u3002", "result": "\u5728\u591a\u4e2a\u516c\u53f8\u548c\u884c\u4e1a\u7ea7\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFinSight\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5206\u6790\u6df1\u5ea6\u548c\u5448\u73b0\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5305\u62ec\u9886\u5148\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\uff0c\u663e\u793a\u51fa\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u62a5\u544a\u751f\u6210\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u751f\u6210\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u8d28\u91cf\u8d22\u52a1\u62a5\u544a\u7684\u53ef\u884c\u8def\u5f84\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548c\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u5b9e\u73b0\u4e86\u4e13\u4e1a\u7ea7\u8d22\u52a1\u5206\u6790\u548c\u53ef\u89c6\u5316\uff0c\u4e3a\u81ea\u52a8\u5316\u91d1\u878d\u5206\u6790\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.16756", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16756", "abs": "https://arxiv.org/abs/2510.16756", "authors": ["Siyin Wang", "Wenyi Yu", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Lu Lu", "Chao Zhang"], "title": "End-to-end Listen, Look, Speak and Act", "comment": "22 pages, 8 figures", "summary": "Human interaction is inherently multimodal and full-duplex: we listen while\nwatching, speak while acting, and fluidly adapt to turn-taking and\ninterruptions. Realizing these capabilities is essential for building models\nsimulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),\nwhich, to our knowledge, is the first full-duplex, end-to-end model that\nsimultaneously perceives and generates across vision, text, speech, and action\nwithin a single architecture, enabling interaction patterns previously out of\nreach, yielding more natural, human-like behaviors. At its core is a novel\nSA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each\nmodality to specialized experts and fuses them through a unified attention\nbackbone. This provides a generalizable solution for joint multimodal\nperception and concurrent generation, leveraging strong pre-trained components\nwhile enabling efficient modality integration and mitigating modality\ninterference. On speech-interaction and robot-manipulation benchmarks, ELLSA\nmatches modality-specific baselines, while uniquely supporting advanced\nmultimodal and full-duplex behaviors such as dialogue and action turn-taking,\ndefective instruction rejection, speaking-while-acting, context-grounded visual\nquestion answering, and action barge-ins. We contend that ELLSA represents a\nstep toward more natural and general interactive intelligence, contributing to\nthe broader pursuit of artificial general intelligence. All data, code and\nmodel checkpoints will be released upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ELLSA\uff0c\u8fd9\u662f\u9996\u4e2a\u5168\u53cc\u5de5\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u80fd\u591f\u5728\u5355\u4e00\u67b6\u6784\u4e2d\u540c\u65f6\u611f\u77e5\u548c\u751f\u6210\u89c6\u89c9\u3001\u6587\u672c\u3001\u8bed\u97f3\u548c\u52a8\u4f5c\u6a21\u6001\uff0c\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u884c\u4e3a\u3002", "motivation": "\u4eba\u7c7b\u4ea4\u4e92\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u548c\u5168\u53cc\u5de5\u7684\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u5904\u7406\u611f\u77e5\u548c\u751f\u6210\u4efb\u52a1\uff0c\u65e0\u6cd5\u5b9e\u73b0\u6d41\u7545\u7684\u5bf9\u8bdd\u8f6e\u6362\u548c\u4e2d\u65ad\u7b49\u81ea\u7136\u4ea4\u4e92\u6a21\u5f0f\u3002", "method": "\u63d0\u51faSA-MoE\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u5c06\u5404\u6a21\u6001\u8def\u7531\u5230\u4e13\u7528\u4e13\u5bb6\uff0c\u5e76\u901a\u8fc7\u7edf\u4e00\u6ce8\u610f\u529b\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u878d\u5408\uff0c\u5b9e\u73b0\u8054\u5408\u591a\u6a21\u6001\u611f\u77e5\u548c\u5e76\u53d1\u751f\u6210\u3002", "result": "\u5728\u8bed\u97f3\u4ea4\u4e92\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cELLSA\u8fbe\u5230\u6a21\u6001\u4e13\u7528\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u540c\u65f6\u652f\u6301\u9ad8\u7ea7\u591a\u6a21\u6001\u548c\u5168\u53cc\u5de5\u884c\u4e3a\uff0c\u5305\u62ec\u5bf9\u8bdd\u8f6e\u6362\u3001\u7f3a\u9677\u6307\u4ee4\u62d2\u7edd\u3001\u8fb9\u8bf4\u8fb9\u505a\u7b49\u80fd\u529b\u3002", "conclusion": "ELLSA\u4ee3\u8868\u4e86\u5411\u66f4\u81ea\u7136\u548c\u901a\u7528\u4ea4\u4e92\u667a\u80fd\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u4e3a\u4eba\u5de5\u901a\u7528\u667a\u80fd\u7684\u8ffd\u6c42\u505a\u51fa\u8d21\u732e\uff0c\u5176\u7edf\u4e00\u67b6\u6784\u4e3a\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16335", "abs": "https://arxiv.org/abs/2510.16335", "authors": ["Bo Peng", "Jie Lu", "Guangquan Zhang", "Zhen Fang"], "title": "On the Provable Importance of Gradients for Language-Assisted Image Clustering", "comment": "revised and extended version of ICCV2025", "summary": "This paper investigates the recently emerged problem of Language-assisted\nImage Clustering (LaIC), where textual semantics are leveraged to improve the\ndiscriminability of visual representations to facilitate image clustering. Due\nto the unavailability of true class names, one of core challenges of LaIC lies\nin how to filter positive nouns, i.e., those semantically close to the images\nof interest, from unlabeled wild corpus data. Existing filtering strategies are\npredominantly based on the off-the-shelf feature space learned by CLIP;\nhowever, despite being intuitive, these strategies lack a rigorous theoretical\nfoundation. To fill this gap, we propose a novel gradient-based framework,\ntermed as GradNorm, which is theoretically guaranteed and shows strong\nempirical performance. In particular, we measure the positiveness of each noun\nbased on the magnitude of gradients back-propagated from the cross-entropy\nbetween the predicted target distribution and the softmax output.\nTheoretically, we provide a rigorous error bound to quantify the separability\nof positive nouns by GradNorm and prove that GradNorm naturally subsumes\nexisting filtering strategies as extremely special cases of itself.\nEmpirically, extensive experiments show that GradNorm achieves the\nstate-of-the-art clustering performance on various benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GradNorm\u8fd9\u4e00\u57fa\u4e8e\u68af\u5ea6\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8bed\u8a00\u8f85\u52a9\u56fe\u50cf\u805a\u7c7b\u4e2d\u7684\u6b63\u540d\u8bcd\u7b5b\u9009\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u805a\u7c7b\u6027\u80fd\u3002", "motivation": "\u8bed\u8a00\u8f85\u52a9\u56fe\u50cf\u805a\u7c7b\uff08LaIC\uff09\u9762\u4e34\u7684\u6838\u5fc3\u6311\u6218\u662f\u5982\u4f55\u4ece\u672a\u6807\u6ce8\u7684\u91ce\u751f\u8bed\u6599\u6570\u636e\u4e2d\u7b5b\u9009\u51fa\u4e0e\u76ee\u6807\u56fe\u50cf\u8bed\u4e49\u76f8\u8fd1\u7684\u6b63\u540d\u8bcd\uff0c\u73b0\u6709\u57fa\u4e8eCLIP\u7279\u5f81\u7a7a\u95f4\u7684\u7b5b\u9009\u7b56\u7565\u867d\u7136\u76f4\u89c2\u4f46\u7f3a\u4e4f\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3aGradNorm\u7684\u68af\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u9884\u6d4b\u76ee\u6807\u5206\u5e03\u4e0esoftmax\u8f93\u51fa\u4e4b\u95f4\u7684\u4ea4\u53c9\u71b5\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u5927\u5c0f\u6765\u8861\u91cf\u6bcf\u4e2a\u540d\u8bcd\u7684\u6b63\u6027\u7a0b\u5ea6\uff0c\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u80fd\u591f\u5305\u542b\u73b0\u6709\u7b5b\u9009\u7b56\u7565\u4f5c\u4e3a\u5176\u7279\u4f8b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eGradNorm\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u805a\u7c7b\u6027\u80fd\uff0c\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u6b63\u540d\u8bcd\u53ef\u5206\u6027\u7684\u4e25\u683c\u8bef\u5dee\u754c\u8bc1\u660e\u3002", "conclusion": "GradNorm\u4e0d\u4ec5\u4e3a\u8bed\u8a00\u8f85\u52a9\u56fe\u50cf\u805a\u7c7b\u63d0\u4f9b\u4e86\u7406\u8bba\u4e25\u8c28\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8fd8\u8bc1\u660e\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u662f\u8be5\u6846\u67b6\u7684\u6781\u7aef\u7279\u4f8b\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.16924", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16924", "abs": "https://arxiv.org/abs/2510.16924", "authors": ["Zhihui Yang", "Yupei Wang", "Kaijie Mo", "Zhe Zhao", "Renfen Hu"], "title": "Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?", "comment": "Accepted to EMNLP 2025 (Findings). This version corrects a redundant\n  sentence in the Results section that appeared in the camera-ready version", "summary": "Despite significant progress in multimodal language models (LMs), it remains\nunclear whether visual grounding enhances their understanding of embodied\nknowledge compared to text-only models. To address this question, we propose a\nnovel embodied knowledge understanding benchmark based on the perceptual theory\nfrom psychology, encompassing visual, auditory, tactile, gustatory, olfactory\nexternal senses, and interoception. The benchmark assesses the models'\nperceptual abilities across different sensory modalities through vector\ncomparison and question-answering tasks with over 1,700 questions. By comparing\n30 state-of-the-art LMs, we surprisingly find that vision-language models\n(VLMs) do not outperform text-only models in either task. Moreover, the models\nperform significantly worse in the visual dimension compared to other sensory\ndimensions. Further analysis reveals that the vector representations are easily\ninfluenced by word form and frequency, and the models struggle to answer\nquestions involving spatial perception and reasoning. Our findings underscore\nthe need for more effective integration of embodied knowledge in LMs to enhance\ntheir understanding of the physical world.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5fc3\u7406\u5b66\u611f\u77e5\u7406\u8bba\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u7406\u89e3\u57fa\u51c6\uff0c\u901a\u8fc7\u6bd4\u8f8330\u79cd\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u53d1\u73b0\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u77e5\u8bc6\u7406\u89e3\u65b9\u9762\u5e76\u672a\u8d85\u8d8a\u7eaf\u6587\u672c\u6a21\u578b\uff0c\u4e14\u6a21\u578b\u5728\u89c6\u89c9\u7ef4\u5ea6\u7684\u8868\u73b0\u663e\u8457\u5f31\u4e8e\u5176\u4ed6\u611f\u5b98\u7ef4\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u89c6\u89c9\u57fa\u7840\u662f\u5426\u6bd4\u7eaf\u6587\u672c\u6a21\u578b\u66f4\u80fd\u589e\u5f3a\u5176\u5bf9\u5177\u8eab\u77e5\u8bc6\u7684\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u611f\u5b98\u6a21\u6001\u4e0b\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5fc3\u7406\u5b66\u611f\u77e5\u7406\u8bba\u6784\u5efa\u4e86\u5305\u542b\u89c6\u89c9\u3001\u542c\u89c9\u3001\u89e6\u89c9\u3001\u5473\u89c9\u3001\u55c5\u89c9\u5916\u90e8\u611f\u5b98\u53ca\u5185\u611f\u53d7\u7684\u5177\u8eab\u77e5\u8bc6\u7406\u89e3\u57fa\u51c6\uff0c\u901a\u8fc7\u5411\u91cf\u6bd4\u8f83\u548c\u95ee\u7b54\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u611f\u77e5\u80fd\u529b\uff0c\u6db5\u76d6\u8d85\u8fc71700\u4e2a\u95ee\u9898\u3002", "result": "\u6bd4\u8f8330\u79cd\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u540e\u53d1\u73b0\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u5747\u672a\u8d85\u8d8a\u7eaf\u6587\u672c\u6a21\u578b\uff0c\u4e14\u6a21\u578b\u5728\u89c6\u89c9\u7ef4\u5ea6\u7684\u8868\u73b0\u663e\u8457\u5dee\u4e8e\u5176\u4ed6\u611f\u5b98\u7ef4\u5ea6\uff0c\u5411\u91cf\u8868\u793a\u6613\u53d7\u8bcd\u5f62\u548c\u9891\u7387\u5f71\u54cd\uff0c\u6a21\u578b\u5728\u6d89\u53ca\u7a7a\u95f4\u611f\u77e5\u548c\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u56f0\u96be\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u77e5\u8bc6\u6574\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u5730\u5c06\u5177\u8eab\u77e5\u8bc6\u878d\u5165\u8bed\u8a00\u6a21\u578b\u4ee5\u589e\u5f3a\u5176\u5bf9\u7269\u7406\u4e16\u754c\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.16769", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16769", "abs": "https://arxiv.org/abs/2510.16769", "authors": ["Shuo Han", "Yukun Cao", "Zezhong Ding", "Zengyi Gao", "S Kevin Zhou", "Xike Xie"], "title": "See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have shown promise in graph understanding, but\nremain limited by input-token constraints, facing scalability bottlenecks and\nlacking effective mechanisms to coordinate textual and visual modalities. To\naddress these challenges, we propose GraphVista, a unified framework that\nenhances both scalability and modality coordination in graph understanding. For\nscalability, GraphVista organizes graph information hierarchically into a\nlightweight GraphRAG base, which retrieves only task-relevant textual\ndescriptions and high-resolution visual subgraphs, compressing redundant\ncontext while preserving key reasoning elements. For modality coordination,\nGraphVista introduces a planning agent that routes tasks to the most suitable\nmodality-using the text modality for simple property reasoning and the visual\nmodality for local and structurally complex reasoning grounded in explicit\ntopology. Extensive experiments demonstrate that GraphVista scales to large\ngraphs, up to $200\\times$ larger than those used in existing benchmarks, and\nconsistently outperforms existing textual, visual, and fusion-based methods,\nachieving up to $4.4\\times$ quality improvement over the state-of-the-art\nbaselines by fully exploiting the complementary strengths of both modalities.", "AI": {"tldr": "GraphVista\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u56fe\u4fe1\u606f\u7ec4\u7ec7\u548c\u6a21\u6001\u534f\u8c03\u89c4\u5212\u4ee3\u7406\uff0c\u89e3\u51b3\u4e86\u56fe\u7406\u89e3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u591a\u6a21\u6001\u534f\u8c03\u95ee\u9898\uff0c\u80fd\u591f\u5904\u7406\u6bd4\u73b0\u6709\u57fa\u51c6\u5927200\u500d\u7684\u56fe\u5e76\u5b9e\u73b04.4\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u8f93\u5165\u4ee4\u724c\u9650\u5236\u5bfc\u81f4\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u6709\u6548\u7684\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u534f\u8c03\u673a\u5236\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u56fe\u6570\u636e\u4e0a\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "GraphVista\u91c7\u7528\u5206\u5c42\u56fe\u4fe1\u606f\u7ec4\u7ec7\u65b9\u6cd5\u6784\u5efa\u8f7b\u91cf\u7ea7GraphRAG\u57fa\u7840\uff0c\u4ec5\u68c0\u7d22\u4efb\u52a1\u76f8\u5173\u7684\u6587\u672c\u63cf\u8ff0\u548c\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u5b50\u56fe\uff0c\u540c\u65f6\u5f15\u5165\u89c4\u5212\u4ee3\u7406\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u5c06\u63a8\u7406\u8def\u7531\u5230\u6700\u9002\u5408\u7684\u6a21\u6001\uff1a\u7b80\u5355\u5c5e\u6027\u63a8\u7406\u4f7f\u7528\u6587\u672c\u6a21\u6001\uff0c\u5c40\u90e8\u548c\u7ed3\u6784\u590d\u6742\u63a8\u7406\u4f7f\u7528\u57fa\u4e8e\u663e\u5f0f\u62d3\u6251\u7684\u89c6\u89c9\u6a21\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGraphVista\u80fd\u591f\u6269\u5c55\u5230\u6bd4\u73b0\u6709\u57fa\u51c6\u5927200\u500d\u7684\u5927\u578b\u56fe\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u6587\u672c\u3001\u89c6\u89c9\u548c\u878d\u5408\u65b9\u6cd5\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5b9e\u73b0\u4e86\u9ad8\u8fbe4.4\u500d\u7684\u8d28\u91cf\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u5206\u5c42\u4fe1\u606f\u538b\u7f29\u548c\u667a\u80fd\u6a21\u6001\u8def\u7531\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u56fe\u7406\u89e3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6a21\u6001\u534f\u8c03\u6311\u6218\uff0c\u5145\u5206\u5229\u7528\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u4e92\u8865\u4f18\u52bf\u4e3a\u5927\u89c4\u6a21\u56fe\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16370", "abs": "https://arxiv.org/abs/2510.16370", "authors": ["Pulin Li", "Guocheng Wu", "Li Yin", "Yuxin Zheng", "Wei Zhang", "Yanjie Zhou"], "title": "MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization", "comment": "https://github.com/wu33learn/MIRAD", "summary": "Social manufacturing leverages community collaboration and scattered\nresources to realize mass individualization in modern industry. However, this\nparadigm shift also introduces substantial challenges in quality control,\nparticularly in defect detection. The main difficulties stem from three\naspects. First, products often have highly customized configurations. Second,\nproduction typically involves fragmented, small-batch orders. Third, imaging\nenvironments vary considerably across distributed sites. To overcome the\nscarcity of real-world datasets and tailored algorithms, we introduce the Mass\nIndividualization Robust Anomaly Detection (MIRAD) dataset. As the first\nbenchmark explicitly designed for anomaly detection in social manufacturing,\nMIRAD captures three critical dimensions of this domain: (1) diverse\nindividualized products with large intra-class variation, (2) data collected\nfrom six geographically dispersed manufacturing nodes, and (3) substantial\nimaging heterogeneity, including variations in lighting, background, and motion\nconditions. We then conduct extensive evaluations of state-of-the-art (SOTA)\nanomaly detection methods on MIRAD, covering one-class, multi-class, and\nzero-shot approaches. Results show a significant performance drop across all\nmodels compared with conventional benchmarks, highlighting the unresolved\ncomplexities of defect detection in real-world individualized production. By\nbridging industrial requirements and academic research, MIRAD provides a\nrealistic foundation for developing robust quality control solutions essential\nfor Industry 5.0. The dataset is publicly available at\nhttps://github.com/wu33learn/MIRAD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u793e\u4f1a\u5236\u9020\u573a\u666f\u7684\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6MIRAD\uff0c\u8be5\u6570\u636e\u96c6\u6355\u6349\u4e86\u4e2a\u6027\u5316\u751f\u4ea7\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u63ed\u793a\u4e86\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u4e2a\u6027\u5316\u751f\u4ea7\u7f3a\u9677\u68c0\u6d4b\u4e2d\u7684\u663e\u8457\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u793e\u4f1a\u5236\u9020\u8303\u5f0f\u901a\u8fc7\u793e\u533a\u534f\u4f5c\u548c\u5206\u6563\u8d44\u6e90\u5b9e\u73b0\u5927\u89c4\u6a21\u4e2a\u6027\u5316\u751f\u4ea7\uff0c\u4f46\u8fd9\u4e00\u8f6c\u53d8\u5728\u8d28\u91cf\u63a7\u5236\u7279\u522b\u662f\u7f3a\u9677\u68c0\u6d4b\u65b9\u9762\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u4e3b\u8981\u56f0\u96be\u6e90\u4e8e\u4ea7\u54c1\u9ad8\u5ea6\u5b9a\u5236\u5316\u914d\u7f6e\u3001\u751f\u4ea7\u6d89\u53ca\u788e\u7247\u5316\u5c0f\u6279\u91cf\u8ba2\u5355\u4ee5\u53ca\u5206\u5e03\u5f0f\u7ad9\u70b9\u6210\u50cf\u73af\u5883\u5dee\u5f02\u663e\u8457\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9488\u5bf9\u6027\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u7b97\u6cd5\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86Mass Individualization Robust Anomaly Detection (MIRAD)\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u793e\u4f1a\u5236\u9020\u5f02\u5e38\u68c0\u6d4b\u8bbe\u8ba1\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6355\u6349\u4e86\u4e2a\u6027\u5316\u751f\u4ea7\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u5177\u6709\u5927\u7c7b\u5185\u53d8\u5f02\u7684\u591a\u6837\u5316\u4e2a\u6027\u5316\u4ea7\u54c1\u3001\u6765\u81ea\u516d\u4e2a\u5730\u7406\u5206\u6563\u5236\u9020\u8282\u70b9\u7684\u6570\u636e\u6536\u96c6\u4ee5\u53ca\u5305\u542b\u5149\u7167\u3001\u80cc\u666f\u548c\u8fd0\u52a8\u6761\u4ef6\u53d8\u5316\u7684\u663e\u8457\u6210\u50cf\u5f02\u8d28\u6027\u3002", "result": "\u5728MIRAD\u6570\u636e\u96c6\u4e0a\u5bf9\u5305\u62ec\u5355\u7c7b\u3001\u591a\u7c7b\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u5728\u5185\u7684\u6700\u5148\u8fdb\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u6240\u6709\u6a21\u578b\u76f8\u6bd4\u4f20\u7edf\u57fa\u51c6\u90fd\u51fa\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u7a81\u663e\u4e86\u771f\u5b9e\u4e16\u754c\u4e2a\u6027\u5316\u751f\u4ea7\u4e2d\u7f3a\u9677\u68c0\u6d4b\u5c1a\u672a\u89e3\u51b3\u7684\u590d\u6742\u6027\u6311\u6218\u3002", "conclusion": "MIRAD\u6570\u636e\u96c6\u901a\u8fc7\u8fde\u63a5\u5de5\u4e1a\u9700\u6c42\u548c\u5b66\u672f\u7814\u7a76\uff0c\u4e3a\u5f00\u53d1\u5de5\u4e1a5.0\u6240\u9700\u7684\u9c81\u68d2\u8d28\u91cf\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u73b0\u5b9e\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5e94\u5bf9\u4e2a\u6027\u5316\u751f\u4ea7\u573a\u666f\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2510.17252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17252", "abs": "https://arxiv.org/abs/2510.17252", "authors": ["Mohd Ruhul Ameen", "Akif Islam", "Abu Saleh Musa Miah", "Ayesha Siddiqua", "Jungpil Shin"], "title": "How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design", "comment": "15 pages, 7 figures, 4 tables. Submitted to the International\n  Conference on Data and Applied Analytics (IDAA 2025)", "summary": "News media often shape the public mood not only by what they report but by\nhow they frame it. The same event can appear calm in one outlet and alarming in\nanother, reflecting subtle emotional bias in reporting. Negative or emotionally\ncharged headlines tend to attract more attention and spread faster, which in\nturn encourages outlets to frame stories in ways that provoke stronger\nreactions. This research explores that tendency through large-scale emotion\nanalysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we\nanalyzed 300000 Bengali news headlines and their content to identify the\ndominant emotion and overall tone of each. The findings reveal a clear\ndominance of negative emotions, particularly anger, fear, and disappointment,\nand significant variation in how similar stories are emotionally portrayed\nacross outlets. Based on these insights, we propose design ideas for a\nhuman-centered news aggregator that visualizes emotional cues and helps readers\nrecognize hidden affective framing in daily news.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u60c5\u611f\u5206\u6790\u63ed\u793a\u4e86\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u4e2d\u8d1f\u9762\u60c5\u7eea\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u53ef\u89c6\u5316\u60c5\u611f\u7ebf\u7d22\u7684\u65b0\u95fb\u805a\u5408\u5668\u8bbe\u8ba1\u7406\u5ff5\uff0c\u5e2e\u52a9\u8bfb\u8005\u8bc6\u522b\u65b0\u95fb\u62a5\u9053\u4e2d\u7684\u60c5\u611f\u6846\u67b6\u504f\u89c1\u3002", "motivation": "\u65b0\u95fb\u5a92\u4f53\u901a\u8fc7\u60c5\u611f\u6846\u67b6\u5f71\u54cd\u516c\u4f17\u60c5\u7eea\uff0c\u8d1f\u9762\u6216\u60c5\u7eea\u5316\u6807\u9898\u5f80\u5f80\u83b7\u5f97\u66f4\u591a\u5173\u6ce8\u5e76\u4f20\u64ad\u66f4\u5feb\uff0c\u5bfc\u81f4\u5a92\u4f53\u503e\u5411\u4e8e\u4f7f\u7528\u5f15\u53d1\u5f3a\u70c8\u53cd\u5e94\u7684\u62a5\u9053\u65b9\u5f0f\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u4e2d\u7684\u8fd9\u79cd\u60c5\u611f\u504f\u89c1\u73b0\u8c61\u3002", "method": "\u4f7f\u7528Gemma-3 4B\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u7406\uff0c\u5206\u6790\u4e8630\u4e07\u6761\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6807\u9898\u53ca\u5176\u5185\u5bb9\uff0c\u8bc6\u522b\u6bcf\u6761\u65b0\u95fb\u7684\u4e3b\u5bfc\u60c5\u7eea\u548c\u6574\u4f53\u60c5\u611f\u57fa\u8c03\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8d1f\u9762\u60c5\u7eea\u660e\u663e\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u7279\u522b\u662f\u6124\u6012\u3001\u6050\u60e7\u548c\u5931\u671b\u60c5\u7eea\uff0c\u540c\u65f6\u4e0d\u540c\u5a92\u4f53\u5bf9\u76f8\u4f3c\u65b0\u95fb\u4e8b\u4ef6\u7684\u60c5\u611f\u5448\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\u63d0\u51fa\u4e86\u4ee5\u4eba\u4e3a\u672c\u7684\u65b0\u95fb\u805a\u5408\u5668\u8bbe\u8ba1\u7406\u5ff5\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u60c5\u611f\u7ebf\u7d22\u5e2e\u52a9\u8bfb\u8005\u8bc6\u522b\u65e5\u5e38\u65b0\u95fb\u4e2d\u9690\u85cf\u7684\u60c5\u611f\u6846\u67b6\uff0c\u589e\u5f3a\u5bf9\u5a92\u4f53\u62a5\u9053\u504f\u89c1\u7684\u8ba4\u77e5\u80fd\u529b\u3002"}}
{"id": "2510.16907", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16907", "abs": "https://arxiv.org/abs/2510.16907", "authors": ["Kangrui Wang", "Pingyue Zhang", "Zihan Wang", "Yaning Gao", "Linjie Li", "Qineng Wang", "Hanyang Chen", "Chi Wan", "Yiping Lu", "Zhengyuan Yang", "Lijuan Wang", "Ranjay Krishna", "Jiajun Wu", "Li Fei-Fei", "Yejin Choi", "Manling Li"], "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "comment": "Accepted to NeurIPS 2025", "summary": "A key challenge in training Vision-Language Model (VLM) agents, compared to\nLanguage Model (LLM) agents, lies in the shift from textual states to complex\nvisual observations. This transition introduces partial observability and\ndemands robust world modeling. We ask: Can VLM agents construct internal world\nmodels through explicit visual state reasoning? To address this question, we\narchitecturally enforce and reward the agent's reasoning process via\nreinforcement learning (RL), formulating it as a Partially Observable Markov\nDecision Process (POMDP). We find that decomposing the agent's reasoning into\nState Estimation (\"what is the current state?\") and Transition Modeling (\"what\ncomes next?\") is critical for success, as demonstrated through five reasoning\nstrategies. Our investigation into how agents represent internal beliefs\nreveals that the optimal representation is task-dependent: Natural Language\nexcels at capturing semantic relationships in general tasks, while Structured\nformats are indispensable for precise manipulation and control. Building on\nthese insights, we design a World Modeling Reward that provides dense,\nturn-level supervision for accurate state prediction, and introduce Bi-Level\nGeneral Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.\nThrough this form of visual state reasoning, a 3B-parameter model achieves a\nscore of 0.82 across five diverse agent benchmarks, representing a 3$\\times$\nimprovement over its untrained counterpart (0.21) and outperforming proprietary\nreasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5\n(0.62). All experiments are conducted within our VAGEN framework, a scalable\nsystem for training and analyzing multi-turn VLM agents in diverse visual\nenvironments. Code and data are publicly available at\nhttps://vagen-ai.github.io.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\u6027\u5730\u5f3a\u5236\u548c\u5956\u52b1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u8fdb\u884c\u663e\u5f0f\u89c6\u89c9\u72b6\u6001\u63a8\u7406\uff0c\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u5728\u4e94\u4e2a\u591a\u6837\u5316\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e863\u500d\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86GPT-5\u3001Gemini 2.5 Pro\u548cClaude 4.5\u7b49\u4e13\u6709\u63a8\u7406\u6a21\u578b\u3002", "motivation": "\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u662f\u4ece\u6587\u672c\u72b6\u6001\u5411\u590d\u6742\u89c6\u89c9\u89c2\u5bdf\u7684\u8f6c\u53d8\uff0c\u8fd9\u5f15\u5165\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u5e76\u9700\u8981\u5f3a\u5927\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22VLM\u4ee3\u7406\u662f\u5426\u80fd\u591f\u901a\u8fc7\u663e\u5f0f\u89c6\u89c9\u72b6\u6001\u63a8\u7406\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\u3002", "method": "\u5c06\u4ee3\u7406\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u72b6\u6001\u4f30\u8ba1\u548c\u8f6c\u79fb\u5efa\u6a21\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u8bbe\u8ba1\u4e86\u4e16\u754c\u5efa\u6a21\u5956\u52b1\u63d0\u4f9b\u5bc6\u96c6\u7684\u56de\u5408\u7ea7\u76d1\u7763\uff0c\u5e76\u5f15\u5165\u4e86\u53cc\u5c42\u901a\u7528\u4f18\u52bf\u4f30\u8ba1\u8fdb\u884c\u56de\u5408\u611f\u77e5\u7684\u4fe1\u7528\u5206\u914d\u3002", "result": "\u901a\u8fc7\u89c6\u89c9\u72b6\u6001\u63a8\u7406\uff0c\u4e00\u4e2a30\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u5728\u4e94\u4e2a\u591a\u6837\u5316\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u4e860.82\u7684\u5206\u6570\uff0c\u76f8\u6bd4\u672a\u8bad\u7ec3\u5bf9\u5e94\u6a21\u578b\uff080.21\uff09\u5b9e\u73b0\u4e863\u500d\u6539\u8fdb\uff0c\u8d85\u8d8a\u4e86GPT-5\uff080.75\uff09\u3001Gemini 2.5 Pro\uff080.67\uff09\u548cClaude 4.5\uff080.62\uff09\u7b49\u4e13\u6709\u63a8\u7406\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u4ee3\u7406\u8868\u793a\u5185\u90e8\u4fe1\u5ff5\u7684\u6700\u4f73\u65b9\u5f0f\u662f\u4efb\u52a1\u4f9d\u8d56\u7684\uff1a\u81ea\u7136\u8bed\u8a00\u5728\u901a\u7528\u4efb\u52a1\u4e2d\u64c5\u957f\u6355\u6349\u8bed\u4e49\u5173\u7cfb\uff0c\u800c\u7ed3\u6784\u5316\u683c\u5f0f\u5bf9\u4e8e\u7cbe\u786e\u64cd\u4f5c\u548c\u63a7\u5236\u4e0d\u53ef\u6216\u7f3a\uff0c\u8fd9\u4e9b\u53d1\u73b0\u4e3aVLM\u4ee3\u7406\u7684\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u91cd\u8981\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2510.16410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16410", "abs": "https://arxiv.org/abs/2510.16410", "authors": ["Changyue Shi", "Minghao Chen", "Yiping Mao", "Chuxiao Yang", "Xinyuan Hu", "Jiajun Ding", "Zhou Yu"], "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting", "comment": null, "summary": "Bridging the gap between complex human instructions and precise 3D object\ngrounding remains a significant challenge in vision and robotics. Existing 3D\nsegmentation methods often struggle to interpret ambiguous, reasoning-based\ninstructions, while 2D vision-language models that excel at such reasoning lack\nintrinsic 3D spatial understanding. In this paper, we introduce REALM, an\ninnovative MLLM-agent framework that enables open-world reasoning-based\nsegmentation without requiring extensive 3D-specific post-training. We perform\nsegmentation directly on 3D Gaussian Splatting representations, capitalizing on\ntheir ability to render photorealistic novel views that are highly suitable for\nMLLM comprehension. As directly feeding one or more rendered views to the MLLM\ncan lead to high sensitivity to viewpoint selection, we propose a novel\nGlobal-to-Local Spatial Grounding strategy. Specifically, multiple global views\nare first fed into the MLLM agent in parallel for coarse-level localization,\naggregating responses to robustly identify the target object. Then, several\nclose-up novel views of the object are synthesized to perform fine-grained\nlocal segmentation, yielding accurate and consistent 3D masks. Extensive\nexperiments show that REALM achieves remarkable performance in interpreting\nboth explicit and implicit instructions across LERF, 3D-OVS, and our newly\nintroduced REALM3D benchmarks. Furthermore, our agent framework seamlessly\nsupports a range of 3D interaction tasks, including object removal,\nreplacement, and style transfer, demonstrating its practical utility and\nversatility. Project page: https://ChangyueShi.github.io/REALM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faREALM\u6846\u67b6\uff0c\u901a\u8fc7\u5c063D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5927\u91cf3D\u7279\u5b9a\u540e\u8bad\u7ec3\u5373\u53ef\u8fdb\u884c\u5f00\u653e\u4e16\u754c\u63a8\u7406\u5206\u5272\u7684\u521b\u65b0\u65b9\u6cd5\u3002", "motivation": "\u73b0\u67093D\u5206\u5272\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u6a21\u7cca\u7684\u63a8\u7406\u5f0f\u6307\u4ee4\uff0c\u800c\u64c5\u957f\u6b64\u7c7b\u63a8\u7406\u76842D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53c8\u7f3a\u4e4f\u5185\u5728\u76843D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u8fd9\u6784\u6210\u4e86\u590d\u6742\u4eba\u7c7b\u6307\u4ee4\u4e0e\u7cbe\u786e3D\u7269\u4f53\u5b9a\u4f4d\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\u3002", "method": "REALM\u76f4\u63a5\u57283D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u4e0a\u6267\u884c\u5206\u5272\uff0c\u5229\u7528\u5176\u6e32\u67d3\u903c\u771f\u65b0\u89c6\u89d2\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u5168\u5c40\u5230\u5c40\u90e8\u7a7a\u95f4\u5b9a\u4f4d\u7b56\u7565\uff1a\u9996\u5148\u5e76\u884c\u8f93\u5165\u591a\u4e2a\u5168\u5c40\u89c6\u56fe\u8fdb\u884c\u7c97\u7c92\u5ea6\u5b9a\u4f4d\uff0c\u7136\u540e\u5408\u6210\u591a\u4e2a\u7269\u4f53\u7279\u5199\u89c6\u56fe\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5c40\u90e8\u5206\u5272\u3002", "result": "\u5728LERF\u30013D-OVS\u548c\u65b0\u5f15\u5165\u7684REALM3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cREALM\u5728\u89e3\u91ca\u663e\u5f0f\u548c\u9690\u5f0f\u6307\u4ee4\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u8be5\u6846\u67b6\u65e0\u7f1d\u652f\u6301\u7269\u4f53\u79fb\u9664\u3001\u66ff\u6362\u548c\u98ce\u683c\u8f6c\u6362\u7b49\u591a\u79cd3D\u4ea4\u4e92\u4efb\u52a1\u3002", "conclusion": "REALM\u5c55\u793a\u4e86\u5c06\u5148\u8fdb2D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u6709\u6548\u8fc1\u79fb\u52303D\u7a7a\u95f4\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5f00\u653e\u4e16\u754c3D\u573a\u666f\u7406\u89e3\u548c\u4ea4\u4e92\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17289", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17289", "abs": "https://arxiv.org/abs/2510.17289", "authors": ["Hajar Bakarou", "Mohamed Sinane El Messoussi", "Ana\u00efs Ollagnier"], "title": "Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning", "comment": null, "summary": "Antisocial behavior (ASB) on social media -- including hate speech,\nharassment, and cyberbullying -- poses growing risks to platform safety and\nsocietal well-being. Prior research has focused largely on networks such as X\nand Reddit, while \\textit{multi-party conversational settings} remain\nunderexplored due to limited data. To address this gap, we use\n\\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB\nin multi-party conversations, and evaluate three tasks: \\textit{abuse\ndetection}, \\textit{bullying behavior analysis}, and \\textit{bullying\npeer-group identification}. We benchmark six text-based and eight graph-based\n\\textit{representation-learning methods}, analyzing lexical cues, interactional\ndynamics, and their multimodal fusion. Results show that multimodal models\noutperform unimodal baselines. The late fusion model \\texttt{mBERT + WD-SGCN}\nachieves the best overall results, with top performance on abuse detection\n(0.718) and competitive scores on peer-group identification (0.286) and\nbullying analysis (0.606). Error analysis highlights its effectiveness in\nhandling nuanced ASB phenomena such as implicit aggression, role transitions,\nand context-dependent hostility.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u793e\u4ea4\u5a92\u4f53\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u53cd\u793e\u4f1a\u884c\u4e3a\u68c0\u6d4b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u5728\u6cd5\u8bed\u6570\u636e\u96c6CyberAgressionAdo-Large\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u7279\u522b\u64c5\u957f\u5904\u7406\u9690\u5f0f\u653b\u51fb\u548c\u89d2\u8272\u8f6c\u6362\u7b49\u590d\u6742\u73b0\u8c61\u3002", "motivation": "\u5f53\u524d\u793e\u4ea4\u5a92\u4f53\u53cd\u793e\u4f1a\u884c\u4e3a\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8eX\u548cReddit\u7b49\u5e73\u53f0\uff0c\u800c\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u7531\u4e8e\u6570\u636e\u7a00\u7f3a\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u516d\u79cd\u57fa\u4e8e\u6587\u672c\u548c\u516b\u79cd\u57fa\u4e8e\u56fe\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u8bcd\u6c47\u7ebf\u7d22\u548c\u4ea4\u4e92\u52a8\u6001\uff0c\u5e76\u63a2\u7d22\u4e86\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u91cd\u70b9\u5173\u6ce8\u665a\u671f\u878d\u5408\u6a21\u578bmBERT + WD-SGCN\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff0cmBERT + WD-SGCN\u5728\u6ee5\u7528\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u52300.718\u7684\u6700\u4f73\u6027\u80fd\uff0c\u5728\u540c\u4f34\u7fa4\u4f53\u8bc6\u522b\u548c\u6b3a\u51cc\u884c\u4e3a\u5206\u6790\u4efb\u52a1\u4e0a\u5206\u522b\u83b7\u5f970.286\u548c0.606\u7684\u7ade\u4e89\u6027\u5206\u6570\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u591a\u6a21\u6001\u878d\u5408\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u53cd\u793e\u4f1a\u884c\u4e3a\u73b0\u8c61\uff0c\u5305\u62ec\u9690\u5f0f\u653b\u51fb\u3001\u89d2\u8272\u8f6c\u6362\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u654c\u610f\u884c\u4e3a\uff0c\u4e3a\u793e\u4ea4\u5a92\u4f53\u5b89\u5168\u76d1\u63a7\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.17052", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17052", "abs": "https://arxiv.org/abs/2510.17052", "authors": ["Hassan Hamad", "Yingru Xu", "Liang Zhao", "Wenbo Yan", "Narendra Gyanchandani"], "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems", "comment": null, "summary": "Tool-augmented large language models (LLMs) are increasingly employed in\nreal-world applications, but tool usage errors still hinder their reliability.\nWe introduce ToolCritic, a diagnostic framework that evaluates and improves LLM\nbehavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight\ndistinct error types specific to tool-calling (e.g., premature invocation,\nargument misalignment, and misinterpretation of tool outputs) and provides\ntargeted feedback to the main LLM. The main LLM, assumed to have strong\nreasoning, task understanding and orchestration capabilities, then revises its\nresponse based on ToolCritic's feedback. We systematically define these error\ncategories and construct a synthetic dataset to train ToolCritic. Experimental\nresults on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic\nimproves tool-calling accuracy by up to 13% over baselines, including zero-shot\nprompting and self-correction techniques. This represents a promising step\ntoward more robust LLM integration with external tools in real-world dialogue\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ToolCritic\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u4e2d\u7684\u884c\u4e3a\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u68c0\u6d4b\u516b\u79cd\u7279\u5b9a\u5de5\u5177\u8c03\u7528\u9519\u8bef\u5e76\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\uff0c\u5728SGD\u6570\u636e\u96c6\u4e0a\u5c06\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u63d0\u5347\u4e8613%\uff0c\u663e\u8457\u589e\u5f3a\u4e86LLM\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5de5\u5177\u589e\u5f3a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5de5\u5177\u4f7f\u7528\u9519\u8bef\u4ecd\u7136\u963b\u788d\u5176\u53ef\u9760\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u548c\u7ea0\u6b63\u5de5\u5177\u8c03\u7528\u8fc7\u7a0b\u4e2d\u7684\u7279\u5b9a\u9519\u8bef\u7c7b\u578b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u4e2d\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u8bca\u65ad\u6846\u67b6\u6765\u63d0\u5347LLM\u4e0e\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\u7684\u9c81\u68d2\u6027\u3002", "method": "ToolCritic\u6846\u67b6\u7cfb\u7edf\u6027\u5730\u5b9a\u4e49\u4e86\u516b\u79cd\u5de5\u5177\u8c03\u7528\u9519\u8bef\u7c7b\u578b\uff0c\u5305\u62ec\u8fc7\u65e9\u8c03\u7528\u3001\u53c2\u6570\u4e0d\u5bf9\u9f50\u548c\u5de5\u5177\u8f93\u51fa\u8bef\u89e3\u7b49\uff0c\u5e76\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u68c0\u6d4b\u8fd9\u4e9b\u7279\u5b9a\u9519\u8bef\u5e76\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\uff0c\u4f7f\u5177\u6709\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u7684\u4e3bLLM\u80fd\u591f\u57fa\u4e8e\u53cd\u9988\u4fee\u6b63\u5176\u54cd\u5e94\u3002", "result": "\u5728Schema-Guided Dialogue\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cToolCritic\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ec\u96f6\u6837\u672c\u63d0\u793a\u548c\u81ea\u6821\u6b63\u6280\u672f\uff09\u5c06\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u63d0\u5347\u4e86\u9ad8\u8fbe13%\u3002\u8be5\u6846\u67b6\u6709\u6548\u51cf\u5c11\u4e86\u5de5\u5177\u8c03\u7528\u8fc7\u7a0b\u4e2d\u7684\u5404\u7c7b\u9519\u8bef\uff0c\u663e\u8457\u6539\u5584\u4e86\u591a\u8f6e\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u7684\u6027\u80fd\u3002", "conclusion": "ToolCritic\u4ee3\u8868\u4e86\u5411\u66f4\u9c81\u68d2\u7684LLM\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u73b0\u5b9e\u5bf9\u8bdd\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8bca\u65ad\u548c\u6821\u6b63\u673a\u5236\u3002\u8be5\u6846\u67b6\u7684\u7cfb\u7edf\u5316\u9519\u8bef\u5206\u7c7b\u548c\u53cd\u9988\u673a\u5236\u4e3a\u672a\u6765\u5de5\u5177\u589e\u5f3aLLM\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\u7684\u53ef\u9760\u6027\u63d0\u5347\u65b9\u9762\u3002"}}
{"id": "2510.16416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16416", "abs": "https://arxiv.org/abs/2510.16416", "authors": ["Xiaojun Guo", "Runyu Zhou", "Yifei Wang", "Qi Zhang", "Chenheng Zhang", "Stefanie Jegelka", "Xiaohan Wang", "Jiajun Chai", "Guojun Yin", "Wei Lin", "Yisen Wang"], "title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning", "comment": null, "summary": "Vision-language models (VLMs) have shown remarkable abilities by integrating\nlarge language models with visual inputs. However, they often fail to utilize\nvisual evidence adequately, either depending on linguistic priors in\nvision-centric tasks or resorting to textual shortcuts during reasoning.\nAlthough reinforcement learning (RL) can align models with desired behaviors,\nits application to VLMs has been hindered by the lack of scalable and reliable\nreward mechanisms. To overcome this challenge, we propose SSL4RL, a novel\nframework that leverages self-supervised learning (SSL) tasks as a source of\nverifiable rewards for RL-based fine-tuning. Our approach reformulates SSL\nobjectives-such as predicting image rotation or reconstructing masked\npatches-into dense, automatic reward signals, eliminating the need for human\npreference data or unreliable AI evaluators. Experiments show that SSL4RL\nsubstantially improves performance on both vision-centric and vision-language\nreasoning benchmarks. Furthermore, through systematic ablations, we identify\nkey factors-such as task difficulty, model scale, and semantic alignment with\nthe target domain-that influence the effectiveness of SSL4RL tasks, offering\nnew design principles for future work. We also demonstrate the framework's\ngenerality by applying it to graph learning, where it yields significant gains.\nSSL4RL establishes a versatile and effective paradigm for aligning multimodal\nmodels using verifiable, self-supervised objectives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSSL4RL\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u8f6c\u5316\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u4e2d\u7f3a\u4e4f\u53ef\u6269\u5c55\u53ef\u9760\u5956\u52b1\u673a\u5236\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5229\u7528\u89c6\u89c9\u8bc1\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8981\u4e48\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\uff0c\u8981\u4e48\u4f7f\u7528\u6587\u672c\u6377\u5f84\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u56e0\u7f3a\u4e4f\u53ef\u6269\u5c55\u53ef\u9760\u7684\u5956\u52b1\u673a\u5236\u800c\u53d7\u5230\u9650\u5236\u3002", "method": "\u63d0\u51faSSL4RL\u6846\u67b6\uff0c\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\uff08\u5982\u56fe\u50cf\u65cb\u8f6c\u9884\u6d4b\u3001\u63a9\u7801\u8865\u4e01\u91cd\u5efa\uff09\u91cd\u65b0\u8868\u8ff0\u4e3a\u5bc6\u96c6\u7684\u81ea\u52a8\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u9700\u4eba\u5de5\u504f\u597d\u6570\u636e\u6216\u4e0d\u53ef\u9760\u7684AI\u8bc4\u4f30\u5668\uff0c\u4e3a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5fae\u8c03\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u6765\u6e90\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSSL4RL\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u901a\u8fc7\u7cfb\u7edf\u6d88\u878d\u7814\u7a76\u8bc6\u522b\u51fa\u4efb\u52a1\u96be\u5ea6\u3001\u6a21\u578b\u89c4\u6a21\u548c\u4e0e\u76ee\u6807\u9886\u57df\u8bed\u4e49\u5bf9\u9f50\u7b49\u5173\u952e\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u5728\u56fe\u5b66\u4e60\u9886\u57df\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u901a\u7528\u6027\u3002", "conclusion": "SSL4RL\u5efa\u7acb\u4e86\u4e00\u4e2a\u901a\u7528\u6709\u6548\u7684\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u9f50\u8303\u5f0f\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u81ea\u76d1\u7763\u76ee\u6807\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u9886\u57df\u7684\u9002\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.17354", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17354", "abs": "https://arxiv.org/abs/2510.17354", "authors": ["Chenghao Zhang", "Guanting Dong", "Xinyu Yang", "Zhicheng Dou"], "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation", "comment": "This work is in progress", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNyx\uff0c\u4e00\u79cd\u9488\u5bf9\u901a\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08URAG\uff09\u7684\u7edf\u4e00\u6df7\u5408\u6a21\u6001\u68c0\u7d22\u5668\uff0c\u901a\u8fc7\u6784\u5efaNyxQA\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u548c\u63a8\u7406\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6587\u672c\u68c0\u7d22\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5f53\u67e5\u8be2\u548c\u6587\u6863\u90fd\u5305\u542b\u6df7\u5408\u6a21\u6001\uff08\u5982\u6587\u672c\u548c\u56fe\u50cf\uff09\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u89e3\u51b3\u901a\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08URAG\uff09\u4e2d\u6df7\u5408\u6a21\u6001\u4fe1\u606f\u7684\u68c0\u7d22\u548c\u63a8\u7406\u95ee\u9898\u3002", "method": "\u63d0\u51faNyx\u7edf\u4e00\u6df7\u5408\u6a21\u6001\u68c0\u7d22\u5668\uff0c\u6784\u5efa\u56db\u9636\u6bb5\u81ea\u52a8\u6d41\u6c34\u7ebf\u751f\u6210\u548c\u8fc7\u6ee4NyxQA\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u5148\u5728NyxQA\u548c\u5f00\u6e90\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5229\u7528\u4e0b\u6e38\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u9988\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u4ee5\u5bf9\u9f50\u68c0\u7d22\u8f93\u51fa\u4e0e\u751f\u6210\u504f\u597d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eNyx\u4e0d\u4ec5\u5728\u6807\u51c6\u6587\u672cRAG\u57fa\u51c6\u4e0a\u8868\u73b0\u7ade\u4e89\u529b\uff0c\u5728\u66f4\u901a\u7528\u548c\u73b0\u5b9e\u7684URAG\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6df7\u5408\u6a21\u6001\u68c0\u7d22\u5728\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u4fe1\u606f\u9700\u6c42\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u901a\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.17590", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG", "I.2.7; H.3.3; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.17590", "abs": "https://arxiv.org/abs/2510.17590", "authors": ["Mir Nafis Sharear Shopnil", "Sharad Duwal", "Abhishek Tyagi", "Adiba Mahbub Proma"], "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning", "comment": "16 pages, 3 tables, 1 figure", "summary": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MIRAGE\uff0c\u4e00\u79cd\u63a8\u7406\u65f6\u3001\u6a21\u578b\u53ef\u63d2\u62d4\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u9a8c\u8bc1\u5206\u89e3\u4e3a\u56db\u4e2a\u987a\u5e8f\u6a21\u5757\u6765\u68c0\u6d4b\u7f51\u7edc\u865a\u5047\u4fe1\u606f\uff0c\u5728MMFakeBench\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523081.65% F1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5f3a\u96f6\u6837\u672c\u57fa\u7ebf7.65\u4e2a\u70b9\u3002", "motivation": "\u7f51\u7edc\u5e73\u53f0\u4e0a\u6bcf\u5929\u901a\u8fc7\u6570\u5341\u4ebf\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5e16\u5b50\u4f20\u64ad\u865a\u5047\u4fe1\u606f\uff0c\u8d85\u51fa\u4e86\u4eba\u5de5\u4e8b\u5b9e\u6838\u67e5\u80fd\u529b\uff0c\u800c\u76d1\u7763\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u9886\u57df\u7279\u5b9a\u7684\u8bad\u7ec3\u6570\u636e\u4e14\u65e0\u6cd5\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u64cd\u7eb5\u7b56\u7565\u3002", "method": "MIRAGE\u6846\u67b6\u5c06\u591a\u6a21\u6001\u9a8c\u8bc1\u5206\u89e3\u4e3a\u56db\u4e2a\u987a\u5e8f\u6a21\u5757\uff1a\u89c6\u89c9\u771f\u5b9e\u6027\u8bc4\u4f30\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\uff0c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5206\u6790\u8bc6\u522b\u4e0a\u4e0b\u6587\u4e0d\u5f53\u91cd\u7528\u9014\uff0c\u68c0\u7d22\u589e\u5f3a\u7684\u4e8b\u5b9e\u68c0\u67e5\u901a\u8fc7\u8fed\u4ee3\u95ee\u9898\u751f\u6210\u5c06\u58f0\u660e\u57fa\u4e8e\u7f51\u7edc\u8bc1\u636e\uff0c\u6821\u51c6\u5224\u65ad\u6a21\u5757\u6574\u5408\u6240\u6709\u4fe1\u53f7\uff0c\u534f\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e0e\u5b9a\u5411\u7f51\u7edc\u68c0\u7d22\uff0c\u8f93\u51fa\u7ed3\u6784\u5316\u4e14\u5e26\u5f15\u7528\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728MMFakeBench\u9a8c\u8bc1\u96c6\uff081,000\u6837\u672c\uff09\u4e0a\uff0cMIRAGE\u4e0eGPT-4o-mini\u7ec4\u5408\u8fbe\u523081.65% F1\u5206\u6570\u548c75.1%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u6700\u5f3a\u96f6\u6837\u672c\u57fa\u7ebf\uff08GPT-4V\u4e0eMMD-Agent\u768474.0% F1\uff097.65\u4e2a\u70b9\uff0c\u540c\u65f6\u4fdd\u630134.3%\u7684\u8bef\u62a5\u7387\uff0c\u800c\u4ec5\u5224\u65ad\u57fa\u7ebf\u7684\u8bef\u62a5\u7387\u4e3a97.3%\uff1b\u6d4b\u8bd5\u96c6\u7ed3\u679c\uff085,000\u6837\u672c\uff09\u786e\u8ba4\u6cdb\u5316\u80fd\u529b\uff0c\u8fbe\u523081.44% F1\u548c75.08%\u51c6\u786e\u7387\uff1b\u6d88\u878d\u7814\u7a76\u663e\u793a\u89c6\u89c9\u9a8c\u8bc1\u8d21\u732e5.18 F1\u70b9\uff0c\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u8d21\u732e2.97 F1\u70b9\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u7f51\u7edc\u68c0\u7d22\u7684\u5206\u89e3\u4ee3\u7406\u63a8\u7406\u53ef\u4ee5\u5728\u6ca1\u6709\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5339\u914d\u76d1\u7763\u68c0\u6d4b\u5668\u6027\u80fd\uff0c\u5728\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u591a\u6a21\u6001\u573a\u666f\u4e2d\u5b9e\u73b0\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\uff0c\u4e3a\u5e94\u5bf9\u5927\u89c4\u6a21\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16442", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16442", "abs": "https://arxiv.org/abs/2510.16442", "authors": ["Haoran Sun", "Chen Cai", "Huiping Zhuang", "Kong Aik Lee", "Lap-Pui Chau", "Yi Wang"], "title": "EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning", "comment": null, "summary": "The rapid development of deepfake video technology has not only facilitated\nartistic creation but also made it easier to spread misinformation. Traditional\ndeepfake video detection (DVD) methods face issues such as a lack of\ntransparency in their principles and insufficient generalization capabilities\nto cope with evolving forgery techniques. This highlights an urgent need for\ndetectors that can identify forged content and provide verifiable reasoning\nexplanations. This paper proposes the explainable deepfake video detection\n(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model\n(MLLM) reasoning framework, which provides traceable reasoning processes\nalongside accurate detection results and trustworthy explanations. Our approach\nfirst incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)\nto extract and fuse global and local cross-frame deepfake features, providing\nrich spatio-temporal semantic information input for MLLM reasoning. Second, we\nconstruct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which\nintroduces facial feature data as hard constraints during the reasoning process\nto achieve pixel-level spatio-temporal video localization, suppress\nhallucinated outputs, and enhance the reliability of the chain of thought. In\naddition, we build an Explainable Reasoning FF++ benchmark dataset\n(ER-FF++set), leveraging structured data to annotate videos and ensure quality\ncontrol, thereby supporting dual supervision for reasoning and detection.\nExtensive experiments demonstrate that EDVD-LLaMA achieves outstanding\nperformance and robustness in terms of detection accuracy, explainability, and\nits ability to handle cross-forgery methods and cross-dataset scenarios.\nCompared to previous DVD methods, it provides a more explainable and superior\nsolution. The source code and dataset will be publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u53ef\u89e3\u91ca\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u4efb\u52a1EDVD\uff0c\u5e76\u8bbe\u8ba1EDVD-LLaMA\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u63d0\u4f9b\u51c6\u786e\u68c0\u6d4b\u7ed3\u679c\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u7684\u63a8\u7406\u8fc7\u7a0b\u548c\u53ef\u4fe1\u7684\u89e3\u91ca\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u539f\u7406\u900f\u660e\u5ea6\u4e0d\u8db3\u548c\u5bf9\u4e0d\u65ad\u53d1\u5c55\u7684\u4f2a\u9020\u6280\u672f\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8feb\u5207\u9700\u8981\u80fd\u591f\u8bc6\u522b\u4f2a\u9020\u5185\u5bb9\u5e76\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u63a8\u7406\u89e3\u91ca\u7684\u68c0\u6d4b\u5668\u3002", "method": "\u63d0\u51faEDVD-LLaMA\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u5305\u542b\u65f6\u7a7a\u7ec6\u5fae\u4fe1\u606f\u6807\u8bb0\u5316ST-SIT\u6765\u63d0\u53d6\u548c\u878d\u5408\u5168\u5c40\u5c40\u90e8\u8de8\u5e27\u6df1\u5ea6\u4f2a\u9020\u7279\u5f81\uff0c\u4ee5\u53ca\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u601d\u7ef4\u94feFg-MCoT\u673a\u5236\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5f15\u5165\u9762\u90e8\u7279\u5f81\u6570\u636e\u4f5c\u4e3a\u786c\u7ea6\u675f\u4ee5\u5b9e\u73b0\u50cf\u7d20\u7ea7\u65f6\u7a7a\u89c6\u9891\u5b9a\u4f4d\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eEDVD-LLaMA\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u5904\u7406\u8de8\u4f2a\u9020\u65b9\u6cd5\u548c\u8de8\u6570\u636e\u96c6\u573a\u666f\u65b9\u9762\u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u76f8\u6bd4\u5148\u524dDVD\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u53ef\u89e3\u91ca\u4e14\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u548c\u53ef\u4fe1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\u5b9e\u73b0\u4e86\u68c0\u6d4b\u7ed3\u679c\u7684\u53ef\u8ffd\u6eaf\u89e3\u91ca\uff0c\u589e\u5f3a\u4e86\u68c0\u6d4b\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.17388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17388", "abs": "https://arxiv.org/abs/2510.17388", "authors": ["Henry Lim", "Kwan Hui Lim"], "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives", "comment": "11 pages, 1 figure, 8 tables", "summary": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot\nreasoning, yet their ability to execute simple, self-contained instructions\nremains underexplored, despite this being foundational to complex\ninstruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro\nbenchmarks, by systematically varying the format of option labels (alphabetic,\nnumeric, Roman) while keeping their meaning identical under four paradigms,\nnamely: (1) With explicit instructions, label changes cause large performance\nshifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format\nbias. (2) Without instructions, performance drops further (up to -10.84\\%) and\nlabel sensitivity intensifies, underscoring the role of explicit guidance. (3)\nWhen option contents are removed, models fail random-choice baselines except\nwith numeric labels, suggesting weak adherence to atomic directives. (4)\nThree-shot exemplars yield no significant gains in robustness or fidelity, and\ngeneration analyses show persistent label errors, especially for non-numeric\nformats. Across model sizes, larger LLMs achieve higher accuracy but remain\ninconsistent in instruction adherence. These results expose the insufficiencies\nof current instruction-tuning paradigms and highlight the need for evaluation\nmethods and training strategies that explicitly target atomic\ninstruction-following.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e8620\u4e2a\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u5728\u539f\u5b50\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u9009\u9879\u6807\u7b7e\u683c\u5f0f\u5b58\u5728\u663e\u8457\u504f\u89c1\u4e14\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4e0d\u8db3\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6307\u4ee4\u8c03\u4f18\u8303\u5f0f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u6267\u884c\u7b80\u5355\u3001\u81ea\u5305\u542b\u6307\u4ee4\u7684\u57fa\u672c\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u800c\u8fd9\u6b63\u662f\u590d\u6742\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u57fa\u7840\u3002", "method": "\u7814\u7a76\u5728\u4fee\u6539\u540e\u7684MMLU\u548cMMLU-Pro\u57fa\u51c6\u4e0a\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u9009\u9879\u6807\u7b7e\u683c\u5f0f\uff08\u5b57\u6bcd\u3001\u6570\u5b57\u3001\u7f57\u9a6c\u6570\u5b57\uff09\uff0c\u5728\u56db\u79cd\u5b9e\u9a8c\u8303\u5f0f\u4e0b\u8bc4\u4f3020\u4e2aIT-LLM\uff1a\u663e\u5f0f\u6307\u4ee4\u3001\u65e0\u6307\u4ee4\u3001\u79fb\u9664\u9009\u9879\u5185\u5bb9\u548c\u4e09\u6837\u672c\u793a\u4f8b\u3002", "result": "\u6807\u7b7e\u683c\u5f0f\u53d8\u5316\u5bfc\u81f4\u663e\u8457\u6027\u80fd\u6ce2\u52a8\uff08\u7f57\u9a6cvs\u6570\u5b57\u4e0b\u964d30.45%\uff09\uff0c\u65e0\u6307\u4ee4\u65f6\u6027\u80fd\u8fdb\u4e00\u6b65\u4e0b\u964d10.84%\uff0c\u79fb\u9664\u9009\u9879\u5185\u5bb9\u540e\u9664\u6570\u5b57\u6807\u7b7e\u5916\u5747\u65e0\u6cd5\u8d85\u8d8a\u968f\u673a\u57fa\u7ebf\uff0c\u4e09\u6837\u672c\u793a\u4f8b\u672a\u80fd\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5927\u6a21\u578b\u51c6\u786e\u7387\u66f4\u9ad8\u4f46\u6307\u4ee4\u9075\u5faa\u4e00\u81f4\u6027\u4ecd\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u66b4\u9732\u4e86\u5f53\u524d\u6307\u4ee4\u8c03\u4f18\u8303\u5f0f\u7684\u4e0d\u8db3\uff0c\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u539f\u5b50\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u57fa\u7840\u6307\u4ee4\u7406\u89e3\u4e0e\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2510.17771", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17771", "abs": "https://arxiv.org/abs/2510.17771", "authors": ["Zhining Liu", "Ziyi Chen", "Hui Liu", "Chen Luo", "Xianfeng Tang", "Suhang Wang", "Joy Zeng", "Zhenwei Dai", "Zhan Shi", "Tianxin Wei", "Benoit Dumoulin", "Hanghang Tong"], "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs", "comment": "21 pages, 10 figures, 6 tables", "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728'\u770b\u89c1\u4f46\u4e0d\u76f8\u4fe1'\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5728\u8f93\u51fa\u9519\u8bef\u7b54\u6848\u65f6\u4ecd\u80fd\u611f\u77e5\u89c6\u89c9\u8bc1\u636e\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6ce8\u610f\u529b\u63a9\u7801\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2aVLM\u5bb6\u65cf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5373\u4f7f\u5b58\u5728\u6b63\u786e\u7684\u89c6\u89c9\u8bc1\u636e\u65f6\u4ecd\u4f1a\u5931\u8d25\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u63a2\u7a76\u8fd9\u4e9b\u5931\u8d25\u662f\u7531\u4e8e\u672a\u80fd\u611f\u77e5\u8bc1\u636e\u8fd8\u662f\u672a\u80fd\u6709\u6548\u5229\u7528\u8bc1\u636e\uff0c\u4ee5\u7406\u89e3VLM\u5185\u90e8\u611f\u77e5\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5c42\u95f4\u6ce8\u610f\u529b\u52a8\u6001\uff0c\u53d1\u73b0\u6d45\u5c42\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u800c\u6df1\u5c42\u7a00\u758f\u4f46\u53ef\u9760\u5730\u5173\u6ce8\u5c40\u90e8\u8bc1\u636e\u533a\u57df\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u63a8\u7406\u65f6\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6ce8\u610f\u529b\u63a9\u7801\u7a81\u51fa\u6df1\u5c42\u8bc1\u636e\u533a\u57df\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0VLMs\u5728\u8f93\u51fa\u9519\u8bef\u7b54\u6848\u65f6\u901a\u5e38\u4ecd\u80fd\u611f\u77e5\u89c6\u89c9\u8bc1\u636e\uff0c\u8fd9\u79cd\u73b0\u8c61\u5e7f\u6cdb\u5b58\u5728\u4e8e\u4e3b\u8981VLM\u5bb6\u65cf\u4e2d\u3002\u63d0\u51fa\u7684\u5e72\u9884\u65b9\u6cd5\u5728LLaVA\u3001Qwen\u3001Gemma\u548cInternVL\u7b49\u591a\u4e2a\u6a21\u578b\u4e0a\u4e00\u81f4\u63d0\u5347\u4e86\u51c6\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660eVLMs\u5185\u90e8\u7f16\u7801\u4e86\u53ef\u9760\u7684\u8bc1\u636e\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\uff0c\u901a\u8fc7\u4f7f\u8fd9\u4e9b\u4fe1\u53f7\u663e\u5f0f\u5316\u53ef\u4ee5\u5f25\u5408\u611f\u77e5\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u8fd9\u4e00\u53d1\u73b0\u63a8\u8fdb\u4e86\u5bf9VLM\u7684\u8bca\u65ad\u7406\u89e3\uff0c\u4e3a\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.16444", "categories": ["cs.CV", "cs.MM", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.16444", "abs": "https://arxiv.org/abs/2510.16444", "authors": ["Kunyu Peng", "Di Wen", "Jia Fu", "Jiamin Wu", "Kailun Yang", "Junwei Zheng", "Ruiping Liu", "Yufan Chen", "Yuqian Fu", "Danda Pani Paudel", "Luc Van Gool", "Rainer Stiefelhagen"], "title": "RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba", "comment": "Extended version of ECCV 2024 paper arXiv:2407.01872. The dataset and\n  code are released at https://github.com/KPeng9510/refAVA2", "summary": "Referring Atomic Video Action Recognition (RAVAR) aims to recognize\nfine-grained, atomic-level actions of a specific person of interest conditioned\non natural language descriptions. Distinct from conventional action recognition\nand detection tasks, RAVAR emphasizes precise language-guided action\nunderstanding, which is particularly critical for interactive human action\nanalysis in complex multi-person scenarios. In this work, we extend our\npreviously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million\nframes and >75.1k annotated persons in total. We benchmark this dataset using\nbaselines from multiple related domains, including atomic action localization,\nvideo question answering, and text-video retrieval, as well as our earlier\nmodel, RefAtomNet. Although RefAtomNet surpasses other baselines by\nincorporating agent attention to highlight salient features, its ability to\nalign and retrieve cross-modal information remains limited, leading to\nsuboptimal performance in localizing the target person and predicting\nfine-grained actions. To overcome the aforementioned limitations, we introduce\nRefAtomNet++, a novel framework that advances cross-modal token aggregation\nthrough a multi-hierarchical semantic-aligned cross-attention mechanism\ncombined with multi-trajectory Mamba modeling at the partial-keyword,\nscene-attribute, and holistic-sentence levels. In particular, scanning\ntrajectories are constructed by dynamically selecting the nearest visual\nspatial tokens at each timestep for both partial-keyword and scene-attribute\nlevels. Moreover, we design a multi-hierarchical semantic-aligned\ncross-attention strategy, enabling more effective aggregation of spatial and\ntemporal tokens across different semantic hierarchies. Experiments show that\nRefAtomNet++ establishes new state-of-the-art results. The dataset and code are\nreleased at https://github.com/KPeng9510/refAVA2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RefAtomNet++\u6846\u67b6\u548cRefAVA++\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u8bed\u8a00\u63cf\u8ff0\u7684\u539f\u5b50\u7ea7\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u95ee\u9898\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u5c42\u7ea7\u8bed\u4e49\u5bf9\u9f50\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u591a\u8f68\u8ff9Mamba\u5efa\u6a21\uff0c\u5728\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc6\u522b\u548c\u4eba\u7269\u5b9a\u4f4d\u65b9\u9762\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u539f\u5b50\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u4fe1\u606f\u5bf9\u9f50\u548c\u68c0\u7d22\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5bfc\u81f4\u76ee\u6807\u4eba\u7269\u5b9a\u4f4d\u548c\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u9884\u6d4b\u6027\u80fd\u4e0d\u4f73\u3002\u7279\u522b\u662f\u5728\u590d\u6742\u591a\u4eba\u573a\u666f\u4e2d\uff0c\u7cbe\u786e\u7684\u8bed\u8a00\u5f15\u5bfc\u52a8\u4f5c\u7406\u89e3\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86RefAtomNet++\u6846\u67b6\uff0c\u91c7\u7528\u591a\u5c42\u7ea7\u8bed\u4e49\u5bf9\u9f50\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u90e8\u5206\u5173\u952e\u8bcd\u3001\u573a\u666f\u5c5e\u6027\u548c\u6574\u4f53\u53e5\u5b50\u4e09\u4e2a\u5c42\u6b21\u7684\u591a\u8f68\u8ff9Mamba\u5efa\u6a21\u3002\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u8fd1\u89c6\u89c9\u7a7a\u95f4token\u6784\u5efa\u626b\u63cf\u8f68\u8ff9\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u8bed\u4e49\u5c42\u7ea7\u7684\u65f6\u7a7atoken\u6709\u6548\u805a\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eRefAtomNet++\u5728RefAVA++\u6570\u636e\u96c6\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc7290\u4e07\u5e27\u89c6\u9891\u548c7.51\u4e07\u4e2a\u6807\u6ce8\u4eba\u7269\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc6\u522b\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u5c42\u7ea7\u8bed\u4e49\u5bf9\u9f50\u548c\u52a8\u6001\u8f68\u8ff9\u5efa\u6a21\u5728\u8de8\u6a21\u6001\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u4ea4\u4e92\u5f0f\u4eba\u7c7b\u52a8\u4f5c\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\u662f\u63d0\u5347\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2510.17405", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17405", "abs": "https://arxiv.org/abs/2510.17405", "authors": ["Mardiyyah Oduwole", "Prince Mireku", "Fatimo Adebanjo", "Oluwatosin Olajide", "Mahi Aminu Aliyu", "Jekaterina Novikova"], "title": "AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages", "comment": null, "summary": "Multimodal AI research has overwhelmingly focused on high-resource languages,\nhindering the democratization of advancements in the field. To address this, we\npresent AfriCaption, a comprehensive framework for multilingual image\ncaptioning in 20 African languages and our contributions are threefold: (i) a\ncurated dataset built on Flickr8k, featuring semantically aligned captions\ngenerated via a context-aware selection and translation process; (ii) a\ndynamic, context-preserving pipeline that ensures ongoing quality through model\nensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B\nparameter vision-to-text architecture that integrates SigLIP and NLLB200 for\ncaption generation across under-represented languages. This unified framework\nensures ongoing data quality and establishes the first scalable\nimage-captioning resource for under-represented African languages, laying the\ngroundwork for truly inclusive multimodal AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AfriCaption\u6846\u67b6\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf920\u79cd\u975e\u6d32\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u56fe\u50cf\u63cf\u8ff0\u7cfb\u7edf\uff0c\u901a\u8fc7\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\u3001\u52a8\u6001\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\u548c0.5B\u53c2\u6570\u6a21\u578b\uff0c\u4e3a\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001AI\u57fa\u7840\u8bbe\u65bd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001AI\u7814\u7a76\u8fc7\u5ea6\u96c6\u4e2d\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u8fdb\u6b65\u7684\u6c11\u4e3b\u5316\uff0c\u7279\u522b\u662f\u975e\u6d32\u8bed\u8a00\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u4e25\u91cd\u7f3a\u4e4f\u4ee3\u8868\u6027\u8d44\u6e90\u548c\u6280\u672f\u652f\u6301\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8eFlickr8k\u7684\u8bed\u4e49\u5bf9\u9f50\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u9009\u62e9\u548c\u7ffb\u8bd1\u6d41\u7a0b\uff1b\u5f00\u53d1\u4e86\u52a8\u6001\u4e0a\u4e0b\u6587\u4fdd\u6301\u7ba1\u9053\uff0c\u901a\u8fc7\u6a21\u578b\u96c6\u6210\u548c\u81ea\u9002\u5e94\u66ff\u6362\u786e\u4fdd\u6301\u7eed\u6570\u636e\u8d28\u91cf\uff1b\u8bbe\u8ba1\u4e860.5B\u53c2\u6570\u7684AfriCaption\u6a21\u578b\uff0c\u6574\u5408SigLIP\u548cNLLB200\u5b9e\u73b0\u8de8\u8bed\u8a00\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u3002", "result": "\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u9996\u4e2a\u9488\u5bf9\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u53ef\u6269\u5c55\u56fe\u50cf\u63cf\u8ff0\u8d44\u6e90\uff0c\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u786e\u4fdd\u4e86\u6301\u7eed\u7684\u6570\u636e\u8d28\u91cf\uff0c\u4e3a20\u79cd\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u975e\u6d32\u8bed\u8a00\u63d0\u4f9b\u4e86\u591a\u6a21\u6001AI\u652f\u6301\u3002", "conclusion": "AfriCaption\u4e3a\u771f\u6b63\u5305\u5bb9\u6027\u591a\u6a21\u6001AI\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u8bed\u8a00\u591a\u6837\u6027\u9e3f\u6c9f\uff0c\u5c55\u793a\u4e86\u5728\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u73af\u5883\u4e2d\u6784\u5efa\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.16450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16450", "abs": "https://arxiv.org/abs/2510.16450", "authors": ["Shan Xiong", "Jiabao Chen", "Ye Wang", "Jialin Peng"], "title": "Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy", "comment": null, "summary": "Annotation-efficient segmentation of the numerous mitochondria instances from\nvarious electron microscopy (EM) images is highly valuable for biological and\nneuroscience research. Although unsupervised domain adaptation (UDA) methods\ncan help mitigate domain shifts and reduce the high costs of annotating each\ndomain, they typically have relatively low performance in practical\napplications. Thus, we investigate weakly supervised domain adaptation (WDA)\nthat utilizes additional sparse point labels on the target domain, which\nrequire minimal annotation effort and minimal expert knowledge. To take full\nuse of the incomplete and imprecise point annotations, we introduce a multitask\nlearning framework that jointly conducts segmentation and center detection with\na novel cross-teaching mechanism and class-focused cross-domain contrastive\nlearning. While leveraging unlabeled image regions is essential, we introduce\nsegmentation self-training with a novel instance-aware pseudo-label (IPL)\nselection strategy. Unlike existing methods that typically rely on pixel-wise\npseudo-label filtering, the IPL semantically selects reliable and diverse\npseudo-labels with the help of the detection task. Comprehensive validations\nand comparisons on challenging datasets demonstrate that our method outperforms\nexisting UDA and WDA methods, significantly narrowing the performance gap with\nthe supervised upper bound. Furthermore, under the UDA setting, our method also\nachieves substantial improvements over other UDA techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7ebf\u7c92\u4f53\u5206\u5272\u7684\u5f31\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7a00\u758f\u70b9\u6807\u6ce8\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7ebf\u7c92\u4f53\u5b9e\u4f8b\u5206\u5272\u7684\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6027\u80fd\u6709\u9650\uff0c\u56e0\u6b64\u7814\u7a76\u5229\u7528\u76ee\u6807\u57df\u4e0a\u7a00\u758f\u70b9\u6807\u6ce8\u7684\u5f31\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u6807\u6ce8\u5de5\u4f5c\u91cf\u5e76\u964d\u4f4e\u4e13\u5bb6\u77e5\u8bc6\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u8fdb\u884c\u5206\u5272\u548c\u4e2d\u5fc3\u70b9\u68c0\u6d4b\uff0c\u91c7\u7528\u4ea4\u53c9\u6559\u5b66\u673a\u5236\u548c\u7c7b\u805a\u7126\u8de8\u57df\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u5f15\u5165\u5177\u6709\u5b9e\u4f8b\u611f\u77e5\u4f2a\u6807\u7b7e\u9009\u62e9\u7b56\u7565\u7684\u5206\u5272\u81ea\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8bed\u4e49\u6027\u5730\u9009\u62e9\u53ef\u9760\u4e14\u591a\u6837\u5316\u7684\u4f2a\u6807\u7b7e\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u548c\u5f31\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u76d1\u7763\u4e0a\u754c\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5728\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bbe\u7f6e\u4e0b\u4e5f\u76f8\u6bd4\u5176\u4ed6\u6280\u672f\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5229\u7528\u7a00\u758f\u70b9\u6807\u6ce8\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u6807\u6ce8\u6210\u672c\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17415", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.MM", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17415", "abs": "https://arxiv.org/abs/2510.17415", "authors": ["Jiacheng Xie", "Yang Yu", "Yibo Chen", "Hanyao Zhang", "Lening Zhao", "Jiaxuan He", "Lei Jiang", "Xiaoting Tang", "Guanghui An", "Dong Xu"], "title": "BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine", "comment": null, "summary": "Traditional Chinese Medicine (TCM), with a history spanning over two\nmillennia, plays a role in global healthcare. However, applying large language\nmodels (LLMs) to TCM remains challenging due to its reliance on holistic\nreasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain\nLLMs have made progress in text-based understanding but lack multimodal\nintegration, interpretability, and clinical applicability. To address these\nlimitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,\nintegrating structured knowledge bases, diagnostic data, and expert feedback\nrefinement. BenCao was trained through natural language instruction tuning\nrather than parameter retraining, aligning with expert-level reasoning and\nethical norms specific to TCM. The system incorporates a comprehensive\nknowledge base of over 1,000 classical and modern texts, a scenario-based\ninstruction framework for diverse interactions, a chain-of-thought simulation\nmechanism for interpretable reasoning, and a feedback refinement process\ninvolving licensed TCM practitioners. BenCao connects to external APIs for\ntongue-image classification and multimodal database retrieval, enabling dynamic\naccess to diagnostic resources. In evaluations across single-choice question\nbenchmarks and multimodal classification tasks, BenCao achieved superior\naccuracy to general-domain and TCM-domain models, particularly in diagnostics,\nherb recognition, and constitution classification. The model was deployed as an\ninteractive application on the OpenAI GPTs Store, accessed by nearly 1,000\nusers globally as of October 2025. This study demonstrates the feasibility of\ndeveloping a TCM-domain LLM through natural language-based instruction tuning\nand multimodal integration, offering a practical framework for aligning\ngenerative AI with traditional medical reasoning and a scalable pathway for\nreal-world deployment.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86BenCao\uff0c\u4e00\u4e2a\u57fa\u4e8eChatGPT\u7684\u4e2d\u533b\u591a\u6a21\u6001\u52a9\u624b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8c03\u4f18\u800c\u975e\u53c2\u6570\u91cd\u8bad\u7ec3\u7684\u65b9\u5f0f\uff0c\u6574\u5408\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u3001\u8bca\u65ad\u6570\u636e\u548c\u4e13\u5bb6\u53cd\u9988\uff0c\u5728\u4e2d\u533b\u95ee\u7b54\u548c\u8bca\u65ad\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u901a\u7528\u9886\u57df\u548c\u4e2d\u533b\u9886\u57df\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u4e2d\u533b\u4f9d\u8d56\u6574\u4f53\u63a8\u7406\u3001\u9690\u542b\u903b\u8f91\u548c\u591a\u6a21\u6001\u8bca\u65ad\u7ebf\u7d22\uff0c\u73b0\u6709\u4e2d\u533b\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u591a\u6a21\u6001\u6574\u5408\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u9002\u7528\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u5e76\u7b26\u5408\u4e2d\u533b\u4e13\u4e1a\u63a8\u7406\u548c\u4f26\u7406\u89c4\u8303\u7684\u7cfb\u7edf\u3002", "method": "\u7cfb\u7edf\u6574\u5408\u4e86\u8d85\u8fc71000\u90e8\u7ecf\u5178\u548c\u73b0\u4ee3\u6587\u732e\u7684\u5168\u9762\u77e5\u8bc6\u5e93\uff0c\u91c7\u7528\u57fa\u4e8e\u573a\u666f\u7684\u6307\u4ee4\u6846\u67b6\u652f\u6301\u591a\u6837\u5316\u4ea4\u4e92\uff0c\u5305\u542b\u7528\u4e8e\u53ef\u89e3\u91ca\u63a8\u7406\u7684\u601d\u7ef4\u94fe\u6a21\u62df\u673a\u5236\uff0c\u4ee5\u53ca\u7531\u6267\u4e1a\u4e2d\u533b\u5e08\u53c2\u4e0e\u7684\u53cd\u9988\u7cbe\u70bc\u8fc7\u7a0b\uff0c\u5e76\u8fde\u63a5\u5916\u90e8API\u5b9e\u73b0\u820c\u8c61\u5206\u7c7b\u548c\u591a\u6a21\u6001\u6570\u636e\u5e93\u68c0\u7d22\u3002", "result": "\u5728\u5355\u9879\u9009\u62e9\u95ee\u7b54\u57fa\u51c6\u548c\u591a\u6a21\u6001\u5206\u7c7b\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0cBenCao\u5728\u8bca\u65ad\u3001\u8349\u836f\u8bc6\u522b\u548c\u4f53\u8d28\u5206\u7c7b\u65b9\u9762\u5b9e\u73b0\u4e86\u4f18\u4e8e\u901a\u7528\u9886\u57df\u548c\u4e2d\u533b\u9886\u57df\u6a21\u578b\u7684\u51c6\u786e\u7387\uff0c\u8be5\u7cfb\u7edf\u5df2\u5728OpenAI GPTs\u5546\u5e97\u90e8\u7f72\u4e3a\u4ea4\u4e92\u5f0f\u5e94\u7528\uff0c\u622a\u81f32025\u5e7410\u6708\u5df2\u6709\u8fd11000\u540d\u5168\u7403\u7528\u6237\u8bbf\u95ee\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8c03\u4f18\u548c\u591a\u6a21\u6001\u96c6\u6210\u5f00\u53d1\u4e2d\u533b\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u751f\u6210\u5f0fAI\u4e0e\u4f20\u7edf\u533b\u5b66\u63a8\u7406\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u5e76\u4e3a\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u5c55\u793a\u4e86\u5c06AI\u6280\u672f\u4e0e\u4f20\u7edf\u533b\u5b66\u5b9e\u8df5\u76f8\u7ed3\u5408\u7684\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2510.16457", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16457", "abs": "https://arxiv.org/abs/2510.16457", "authors": ["Peiran Xu", "Xicheng Gong", "Yadong MU"], "title": "NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation", "comment": "ICCV 2025", "summary": "In this work we concentrate on the task of goal-oriented Vision-and-Language\nNavigation (VLN). Existing methods often make decisions based on historical\ninformation, overlooking the future implications and long-term outcomes of the\nactions. In contrast, we aim to develop a foresighted agent. Specifically, we\ndraw upon Q-learning to train a Q-model using large-scale unlabeled trajectory\ndata, in order to learn the general knowledge regarding the layout and object\nrelations within indoor scenes. This model can generate a Q-feature, analogous\nto the Q-value in traditional Q-network, for each candidate action, which\ndescribes the potential future information that may be observed after taking\nthe specific action. Subsequently, a cross-modal future encoder integrates the\ntask-agnostic Q-feature with navigation instructions to produce a set of action\nscores reflecting future prospects. These scores, when combined with the\noriginal scores based on history, facilitate an A*-style searching strategy to\neffectively explore the regions that are more likely to lead to the\ndestination. Extensive experiments conducted on widely used goal-oriented VLN\ndatasets validate the effectiveness of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u76ee\u6807\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u524d\u77bb\u6027\u667a\u80fd\u4f53\uff0c\u901a\u8fc7Q\u5b66\u4e60\u4ece\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u8f68\u8ff9\u6570\u636e\u4e2d\u5b66\u4e60\u573a\u666f\u5e03\u5c40\u548c\u7269\u4f53\u5173\u7cfb\u77e5\u8bc6\uff0c\u7ed3\u5408A*\u641c\u7d22\u7b56\u7565\u6709\u6548\u63a2\u7d22\u53ef\u80fd\u901a\u5f80\u76ee\u7684\u5730\u7684\u533a\u57df\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u5386\u53f2\u4fe1\u606f\u8fdb\u884c\u51b3\u7b56\uff0c\u5ffd\u7565\u4e86\u52a8\u4f5c\u7684\u672a\u6765\u5f71\u54cd\u548c\u957f\u671f\u7ed3\u679c\uff0c\u5bfc\u81f4\u5bfc\u822a\u6548\u7387\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u5f00\u53d1\u80fd\u591f\u9884\u89c1\u672a\u6765\u72b6\u6001\u7684\u524d\u77bb\u6027\u5bfc\u822a\u667a\u80fd\u4f53\u3002", "method": "\u91c7\u7528Q\u5b66\u4e60\u6846\u67b6\u5728\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u8f68\u8ff9\u6570\u636e\u4e0a\u8bad\u7ec3Q\u6a21\u578b\uff0c\u5b66\u4e60\u5ba4\u5185\u573a\u666f\u7684\u5e03\u5c40\u548c\u7269\u4f53\u5173\u7cfb\u77e5\u8bc6\uff0c\u751f\u6210\u63cf\u8ff0\u5019\u9009\u52a8\u4f5c\u6f5c\u5728\u672a\u6765\u4fe1\u606f\u7684Q\u7279\u5f81\u3002\u901a\u8fc7\u8de8\u6a21\u6001\u672a\u6765\u7f16\u7801\u5668\u5c06\u4efb\u52a1\u65e0\u5173\u7684Q\u7279\u5f81\u4e0e\u5bfc\u822a\u6307\u4ee4\u7ed3\u5408\uff0c\u4ea7\u751f\u53cd\u6620\u672a\u6765\u524d\u666f\u7684\u52a8\u4f5c\u8bc4\u5206\uff0c\u5e76\u4e0e\u57fa\u4e8e\u5386\u53f2\u7684\u539f\u59cb\u8bc4\u5206\u7ed3\u5408\uff0c\u5b9e\u73b0A*\u98ce\u683c\u7684\u641c\u7d22\u7b56\u7565\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u76ee\u6807\u5bfc\u5411\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u5bfc\u822a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u63a2\u7d22\u53ef\u80fd\u901a\u5f80\u76ee\u7684\u5730\u7684\u533a\u57df\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u672a\u6765\u4fe1\u606f\u7eb3\u5165\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u51b3\u7b56\u8fc7\u7a0b\u7684\u91cd\u8981\u6027\uff0c\u901a\u8fc7\u7ed3\u5408\u4efb\u52a1\u65e0\u5173\u7684\u573a\u666f\u77e5\u8bc6\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u5bfc\u822a\u6307\u4ee4\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u8def\u5f84\u89c4\u5212\u548c\u76ee\u6807\u8fbe\u6210\u3002\u8fd9\u4e00\u65b9\u6cd5\u4e3a\u5f00\u53d1\u66f4\u667a\u80fd\u7684\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u5f3a\u8c03\u4e86\u957f\u671f\u89c4\u5212\u5728\u590d\u6742\u73af\u5883\u5bfc\u822a\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2510.17720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17720", "abs": "https://arxiv.org/abs/2510.17720", "authors": ["Nanda Kumar Rengarajan", "Jun Yan", "Chun Wang"], "title": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition", "comment": null, "summary": "Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5c11\u6837\u672c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u677f\u548c\u4fdd\u7559\u5b9e\u4f53\u4fe1\u606f\u7684\u7b56\u7565\u6027\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002", "motivation": "\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u6807\u7b7e\u83b7\u53d6\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u7684\u96f6\u6837\u672c\u548c\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u9886\u57df\u7279\u5b9a\u5b9e\u4f53\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u5229\u7528\u6709\u9650\u7684\u53ef\u7528\u6570\u636e\uff0c\u8fd9\u6210\u4e3a\u5f53\u524d\u7814\u7a76\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u4e00\u662f\u8bbe\u8ba1\u65b0\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u677f\uff0c\u91c7\u7528\u7b80\u5316\u7684\u8f93\u51fa\u683c\u5f0f\u7ed3\u5408\u5148\u524dIT\u65b9\u6cd5\u7684\u539f\u7406\uff0c\u5145\u5206\u5229\u7528\u6700\u65b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\uff1b\u4e8c\u662f\u5f15\u5165\u7b56\u7565\u6027\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u4fdd\u6301\u5b9e\u4f53\u4fe1\u606f\u7684\u540c\u65f6\u5bf9\u5468\u56f4\u4e0a\u4e0b\u6587\u8fdb\u884c\u6539\u5199\uff0c\u4ece\u800c\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u800c\u4e0d\u635f\u5bb3\u8bed\u4e49\u5173\u7cfb\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5176\u4e2d\u5c11\u6837\u672c\u65b9\u6cd5\u5728CrossNER\u6570\u636e\u96c6\u4e0a\u5e73\u5747F1\u5f97\u5206\u4e3a80.1\u3002\u4f7f\u7528\u6539\u5199\u65b9\u6cd5\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\u57fa\u7ebf\u7248\u672c\u5728F1\u5206\u6570\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe17\u4e2a\u70b9\u7684\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u62e5\u6709\u6709\u9650NER\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u7fa4\u4f53\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u6846\u67b6\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u5c11\u6837\u672cNER\u4efb\u52a1\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.16505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16505", "abs": "https://arxiv.org/abs/2510.16505", "authors": ["Lukas Selch", "Yufang Hou", "M. Jehanzeb Mirza", "Sivan Doveh", "James Glass", "Rogerio Feris", "Wei Lin"], "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies", "comment": null, "summary": "Large Multimodal Models (LMMs) are increasingly applied to scientific\nresearch, yet it remains unclear whether they can reliably understand and\nreason over the multimodal complexity of papers. A central challenge lies in\ndetecting and resolving inconsistencies across text, figures, tables, and\nequations, issues that are often subtle, domain-specific, and ultimately\nundermine clarity, reproducibility, and trust. Existing benchmarks overlook\nthis issue, either isolating single modalities or relying on synthetic errors\nthat fail to capture real-world complexity. We introduce PRISMM-Bench\n(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first\nbenchmark grounded in real reviewer-flagged inconsistencies in scientific\npapers. Through a multi-stage pipeline of review mining, LLM-assisted filtering\nand human verification, we curate 262 inconsistencies from 242 papers. Based on\nthis set, we design three tasks, namely inconsistency identification, remedy\nand pair matching, which assess a model's capacity to detect, correct, and\nreason over inconsistencies across different modalities. Furthermore, to\naddress the notorious problem of choice-only shortcuts in multiple-choice\nevaluation, where models exploit answer patterns without truly understanding\nthe question, we further introduce structured JSON-based answer representations\nthat minimize linguistic biases by reducing reliance on superficial stylistic\ncues. We benchmark 21 leading LMMs, including large open-weight models\n(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5\nwith high reasoning). Results reveal strikingly low performance (26.1-54.2%),\nunderscoring the challenge of multimodal scientific reasoning and motivating\nprogress towards trustworthy scientific assistants.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PRISMM-Bench\uff0c\u9996\u4e2a\u57fa\u4e8e\u771f\u5b9e\u79d1\u5b66\u8bba\u6587\u4e2d\u5ba1\u7a3f\u4eba\u6807\u8bb0\u4e0d\u4e00\u81f4\u6027\u7684\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u53ef\u9760\u6027\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u68c0\u6d4b\u548c\u89e3\u51b3\u8de8\u6a21\u6001\u4e0d\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u63ed\u793a\u4e86\u79d1\u5b66\u591a\u6a21\u6001\u63a8\u7406\u7684\u6311\u6218\u6027\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5176\u80fd\u5426\u53ef\u9760\u7406\u89e3\u548c\u63a8\u7406\u8bba\u6587\u4e2d\u7684\u591a\u6a21\u6001\u590d\u6742\u6027\u4ecd\u4e0d\u660e\u786e\u3002\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u68c0\u6d4b\u548c\u89e3\u51b3\u6587\u672c\u3001\u56fe\u8868\u3001\u8868\u683c\u548c\u65b9\u7a0b\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u8fd9\u4e9b\u95ee\u9898\u901a\u5e38\u5f88\u5fae\u5999\u4e14\u9886\u57df\u7279\u5b9a\uff0c\u4f1a\u524a\u5f31\u6e05\u6670\u5ea6\u3001\u53ef\u91cd\u590d\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u73b0\u6709\u57fa\u51c6\u8981\u4e48\u5b64\u7acb\u5355\u4e00\u6a21\u6001\uff0c\u8981\u4e48\u4f9d\u8d56\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6027\u7684\u5408\u6210\u9519\u8bef\u3002", "method": "\u901a\u8fc7\u591a\u9636\u6bb5\u6d41\u7a0b\u6784\u5efaPRISMM-Bench\u57fa\u51c6\uff0c\u5305\u62ec\u5ba1\u7a3f\u610f\u89c1\u6316\u6398\u3001LLM\u8f85\u52a9\u8fc7\u6ee4\u548c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u4ece242\u7bc7\u8bba\u6587\u4e2d\u6536\u96c6\u4e86262\u4e2a\u4e0d\u4e00\u81f4\u6027\u3002\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u4efb\u52a1\uff1a\u4e0d\u4e00\u81f4\u6027\u8bc6\u522b\u3001\u4fee\u6b63\u548c\u914d\u5bf9\u5339\u914d\uff0c\u8bc4\u4f30\u6a21\u578b\u8de8\u6a21\u6001\u68c0\u6d4b\u3001\u7ea0\u6b63\u548c\u63a8\u7406\u80fd\u529b\u3002\u5f15\u5165\u7ed3\u6784\u5316JSON\u7b54\u6848\u8868\u793a\u4ee5\u6700\u5c0f\u5316\u8bed\u8a00\u504f\u89c1\uff0c\u51cf\u5c11\u5bf9\u8868\u9762\u98ce\u683c\u7ebf\u7d22\u7684\u4f9d\u8d56\u3002", "result": "\u5bf921\u4e2a\u9886\u5148LMM\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u5927\u578b\u5f00\u653e\u6743\u91cd\u6a21\u578b\u548c\u4e13\u6709\u6a21\u578b\u3002\u7ed3\u679c\u663e\u793a\u6027\u80fd\u6781\u4f4e\uff0826.1-54.2%\uff09\uff0c\u7a81\u663e\u4e86\u591a\u6a21\u6001\u79d1\u5b66\u63a8\u7406\u7684\u6311\u6218\u6027\u3002\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u68c0\u6d4b\u548c\u89e3\u51b3\u771f\u5b9e\u79d1\u5b66\u8bba\u6587\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u65b9\u9762\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u53ef\u4fe1\u79d1\u5b66\u52a9\u624b\u7684\u5fc5\u8981\u6027\u3002PRISMM-Bench\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u7684\u591a\u6a21\u6001\u79d1\u5b66\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u66f4\u53ef\u9760\u79d1\u5b66AI\u7cfb\u7edf\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2510.16540", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16540", "abs": "https://arxiv.org/abs/2510.16540", "authors": ["Jihoon Kwon", "Kyle Min", "Jy-yong Sohn"], "title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions", "comment": "Accepted at NeurIPS 2025 (poster). This is the camera-ready version", "summary": "Despite recent advances, vision-language models trained with standard\ncontrastive objectives still struggle with compositional reasoning -- the\nability to understand structured relationships between visual and linguistic\nelements. This shortcoming is largely due to the tendency of the text encoder\nto focus on individual words rather than their relations, a limitation\nreinforced by contrastive training that primarily aligns words with visual\nobjects. In this paper, we introduce REconstruction and Alignment of text\nDescriptions (READ), a fine-tuning method designed to enhance compositional\nreasoning by adding two auxiliary objectives to the contrastive learning: (1) a\ntoken-level reconstruction objective, where a frozen pre-trained decoder\nreconstructs alternative captions based on the embedding of the original\ncaption; and (2) a sentence-level alignment objective, which explicitly aligns\nparaphrased sentences in the embedding space. We show that READ-CLIP, a model\nderived by applying the READ method to the pre-trained CLIP model, achieves the\nstate-of-the-art performance across five major compositional reasoning\nbenchmarks, outperforming the strongest conventional fine-tuning baseline by up\nto 4.1%. Furthermore, applying the READ to existing CLIP variants (including\nNegCLIP and FSC-CLIP) also improves performance on these benchmarks.\nQuantitative and qualitative analyses reveal that our proposed objectives --\nreconstruction and alignment -- offer complementary benefits: the former\nencourages the encoder to capture relationships between words within a caption,\nwhile the latter ensures consistent representations for paraphrases expressed\nwith different wording.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86READ\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u91cd\u6784\u548c\u5bf9\u9f50\u4e24\u4e2a\u8f85\u52a9\u76ee\u6807\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u3002READ-CLIP\u5728\u4e94\u4e2a\u4e3b\u8981\u7ec4\u5408\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6bd4\u4f20\u7edf\u5fae\u8c03\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe4.1%\u3002", "motivation": "\u5c3d\u7ba1\u8fd1\u671f\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4f7f\u7528\u6807\u51c6\u5bf9\u6bd4\u76ee\u6807\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u63a8\u7406\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u56f0\u96be\uff0c\u5373\u7406\u89e3\u89c6\u89c9\u548c\u8bed\u8a00\u5143\u7d20\u4e4b\u95f4\u7684\u7ed3\u6784\u5316\u5173\u7cfb\u3002\u8fd9\u4e00\u4e0d\u8db3\u4e3b\u8981\u6e90\u4e8e\u6587\u672c\u7f16\u7801\u5668\u503e\u5411\u4e8e\u5173\u6ce8\u5355\u4e2a\u5355\u8bcd\u800c\u975e\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u79cd\u5c40\u9650\u6027\u88ab\u4e3b\u8981\u5c06\u5355\u8bcd\u4e0e\u89c6\u89c9\u5bf9\u8c61\u5bf9\u9f50\u7684\u5bf9\u6bd4\u8bad\u7ec3\u6240\u5f3a\u5316\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86READ\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u5411\u5bf9\u6bd4\u5b66\u4e60\u6dfb\u52a0\u4e24\u4e2a\u8f85\u52a9\u76ee\u6807\u6765\u589e\u5f3a\u7ec4\u5408\u63a8\u7406\u7684\u5fae\u8c03\u65b9\u6cd5\uff1a(1) \u4ee4\u724c\u7ea7\u91cd\u6784\u76ee\u6807\uff0c\u5176\u4e2d\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u57fa\u4e8e\u539f\u59cb\u6807\u9898\u7684\u5d4c\u5165\u91cd\u6784\u66ff\u4ee3\u6807\u9898\uff1b(2) \u53e5\u5b50\u7ea7\u5bf9\u9f50\u76ee\u6807\uff0c\u663e\u5f0f\u5730\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u91ca\u4e49\u53e5\u5b50\u3002", "result": "READ-CLIP\u5728\u4e94\u4e2a\u4e3b\u8981\u7ec4\u5408\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6bd4\u6700\u5f3a\u7684\u4f20\u7edf\u5fae\u8c03\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe4.1%\u3002\u5c06READ\u5e94\u7528\u4e8e\u73b0\u6709CLIP\u53d8\u4f53\uff08\u5305\u62ecNegCLIP\u548cFSC-CLIP\uff09\u4e5f\u80fd\u63d0\u9ad8\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u7684\u6027\u80fd\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u91cd\u6784\u548c\u5bf9\u9f50\u76ee\u6807\u63d0\u4f9b\u4e86\u4e92\u8865\u7684\u76ca\u5904\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u91cd\u6784\u548c\u5bf9\u9f50\u76ee\u6807\u63d0\u4f9b\u4e86\u4e92\u8865\u7684\u76ca\u5904\uff1a\u524d\u8005\u9f13\u52b1\u7f16\u7801\u5668\u6355\u83b7\u6807\u9898\u5185\u5355\u8bcd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u800c\u540e\u8005\u786e\u4fdd\u4f7f\u7528\u4e0d\u540c\u63aa\u8f9e\u8868\u8fbe\u7684\u91ca\u4e49\u5177\u6709\u4e00\u81f4\u7684\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u4e3a\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5fae\u8c03\u7b56\u7565\u3002"}}
{"id": "2510.16556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16556", "abs": "https://arxiv.org/abs/2510.16556", "authors": ["Guangyu Lin", "Li Lin", "Christina P. Walker", "Daniel S. Schiff", "Shu Hu"], "title": "Fit for Purpose? Deepfake Detection in the Real World", "comment": null, "summary": "The rapid proliferation of AI-generated content, driven by advances in\ngenerative adversarial networks, diffusion models, and multimodal large\nlanguage models, has made the creation and dissemination of synthetic media\neffortless, heightening the risks of misinformation, particularly political\ndeepfakes that distort truth and undermine trust in political institutions. In\nturn, governments, research institutions, and industry have strongly promoted\ndeepfake detection initiatives as solutions. Yet, most existing models are\ntrained and validated on synthetic, laboratory-controlled datasets, limiting\ntheir generalizability to the kinds of real-world political deepfakes\ncirculating on social platforms that affect the public. In this work, we\nintroduce the first systematic benchmark based on the Political Deepfakes\nIncident Database, a curated collection of real-world political deepfakes\nshared on social media since 2018. Our study includes a systematic evaluation\nof state-of-the-art deepfake detectors across academia, government, and\nindustry. We find that the detectors from academia and government perform\nrelatively poorly. While paid detection tools achieve relatively higher\nperformance than free-access models, all evaluated detectors struggle to\ngeneralize effectively to authentic political deepfakes, and are vulnerable to\nsimple manipulations, especially in the video domain. Results urge the need for\npolitically contextualized deepfake detection frameworks to better safeguard\nthe public in real-world settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u4e8b\u4ef6\u7684\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u653f\u6cbb\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u4e14\u6613\u53d7\u7b80\u5355\u653b\u51fb\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u5728\u5b9e\u9a8c\u5ba4\u63a7\u5236\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u9a8c\u8bc1\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u793e\u4ea4\u5a92\u4f53\u4e0a\u4f20\u64ad\u7684\u771f\u5b9e\u4e16\u754c\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u8fd9\u7c7b\u5185\u5bb9\u5bf9\u516c\u4f17\u4fe1\u4efb\u548c\u6c11\u4e3b\u5236\u5ea6\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u4e8b\u4ef6\u6570\u636e\u5e93\u6784\u5efa\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u57fa\u51c6\uff0c\u8be5\u6570\u636e\u5e93\u6536\u96c6\u4e862018\u5e74\u4ee5\u6765\u793e\u4ea4\u5a92\u4f53\u4e0a\u4f20\u64ad\u7684\u771f\u5b9e\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\uff0c\u5e76\u5bf9\u5b66\u672f\u754c\u3001\u653f\u5e9c\u548c\u5de5\u4e1a\u754c\u7684\u6700\u5148\u8fdb\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5b66\u672f\u754c\u548c\u653f\u5e9c\u5f00\u53d1\u7684\u68c0\u6d4b\u5668\u8868\u73b0\u76f8\u5bf9\u8f83\u5dee\uff0c\u4ed8\u8d39\u68c0\u6d4b\u5de5\u5177\u867d\u7136\u6027\u80fd\u4f18\u4e8e\u514d\u8d39\u6a21\u578b\uff0c\u4f46\u6240\u6709\u68c0\u6d4b\u5668\u90fd\u96be\u4ee5\u6709\u6548\u6cdb\u5316\u5230\u771f\u5b9e\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\uff0c\u4e14\u5728\u89c6\u9891\u9886\u57df\u7279\u522b\u5bb9\u6613\u53d7\u5230\u7b80\u5355\u64cd\u4f5c\u7684\u653b\u51fb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5f00\u53d1\u653f\u6cbb\u60c5\u5883\u5316\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\u7684\u8feb\u5207\u9700\u6c42\uff0c\u4ee5\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u66f4\u597d\u5730\u4fdd\u62a4\u516c\u4f17\u514d\u53d7\u653f\u6cbb\u6df1\u5ea6\u4f2a\u9020\u7684\u5a01\u80c1\uff0c\u9700\u8981\u9488\u5bf9\u653f\u6cbb\u5185\u5bb9\u7684\u7279\u6b8a\u6027\u8bbe\u8ba1\u66f4\u5177\u9c81\u68d2\u6027\u7684\u68c0\u6d4b\u65b9\u6848\u3002"}}
{"id": "2510.16596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16596", "abs": "https://arxiv.org/abs/2510.16596", "authors": ["Yiyang Huang", "Liang Shi", "Yitian Zhang", "Yi Xu", "Yun Fu"], "title": "SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense", "comment": null, "summary": "Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.\nHowever, object hallucination, where models produce plausible but inaccurate\nobject descriptions, remains a significant challenge. In contrast to previous\nwork focusing on LLM components, this paper is the first to trace LVLM\nhallucinations to visual encoders and identifies three key issues: statistical\nbias, inherent bias, and vulnerability. To address these challenges, we propose\nSHIELD, a training-free framework that mitigates hallucinations through three\nstrategies: re-weighting visual tokens to reduce statistical bias, introducing\nnoise-derived tokens to counter inherent bias, and applying adversarial attacks\nwith contrastive decoding to address vulnerability. Experiments demonstrate\nthat SHIELD effectively mitigates object hallucinations across diverse\nbenchmarks and LVLM families. Moreover, SHIELD achieves strong performance on\nthe general LVLM benchmark, highlighting its broad applicability. Code will be\nreleased.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSHIELD\u6846\u67b6\uff0c\u9996\u6b21\u5c06LVLM\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u6eaf\u6e90\u81f3\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u8bad\u7ec3\u65e0\u5173\u7b56\u7565\u6709\u6548\u7f13\u89e3\u4e86\u7edf\u8ba1\u504f\u5dee\u3001\u56fa\u6709\u504f\u5dee\u548c\u8106\u5f31\u6027\u5bfc\u81f4\u7684\u5e7b\u89c9\u73b0\u8c61\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff08\u6a21\u578b\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u4e0d\u51c6\u786e\u7684\u7269\u4f53\u63cf\u8ff0\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u4e0e\u4ee5\u5f80\u5173\u6ce8LLM\u7ec4\u4ef6\u7684\u7814\u7a76\u4e0d\u540c\uff0c\u672c\u6587\u9996\u6b21\u5c06LVLM\u5e7b\u89c9\u95ee\u9898\u6eaf\u6e90\u81f3\u89c6\u89c9\u7f16\u7801\u5668\u3002", "method": "\u63d0\u51faSHIELD\u8bad\u7ec3\u65e0\u5173\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u79cd\u7b56\u7565\uff1a\u91cd\u52a0\u6743\u89c6\u89c9token\u4ee5\u51cf\u5c11\u7edf\u8ba1\u504f\u5dee\uff0c\u5f15\u5165\u566a\u58f0\u884d\u751ftoken\u4ee5\u5bf9\u6297\u56fa\u6709\u504f\u5dee\uff0c\u5e94\u7528\u5bf9\u6297\u653b\u51fb\u4e0e\u5bf9\u6bd4\u89e3\u7801\u6765\u89e3\u51b3\u8106\u5f31\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSHIELD\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548cLVLM\u5bb6\u65cf\u4e2d\u6709\u6548\u7f13\u89e3\u4e86\u7269\u4f53\u5e7b\u89c9\uff0c\u540c\u65f6\u5728\u901a\u7528LVLM\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u5728LVLM\u5e7b\u89c9\u95ee\u9898\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63d0\u51fa\u7684SHIELD\u6846\u67b6\u4e0d\u4ec5\u89e3\u51b3\u4e86\u7279\u5b9a\u5e7b\u89c9\u95ee\u9898\uff0c\u8fd8\u5c55\u793a\u4e86\u5728\u4fdd\u6301\u901a\u7528\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.16598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16598", "abs": "https://arxiv.org/abs/2510.16598", "authors": ["Jiaying Zhu", "Yurui Zhu", "Xin Lu", "Wenrui Yan", "Dong Li", "Kunlin Liu", "Xueyang Fu", "Zheng-Jun Zha"], "title": "VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs", "comment": "22 pages, 8 figures", "summary": "Multimodal Large Language Models (MLLMs) encounter significant computational\nand memory bottlenecks from the massive number of visual tokens generated by\nhigh-resolution images or multi-image inputs. Previous token compression\ntechniques are often constrained by heuristic rules that risk discarding\ncritical information. They may suffer from biases, such as attention sinks,\nthat lead to sharp performance drops under aggressive compression ratios. To\naddress these limitations, we reformulate token compression as a lightweight\nplug-and-play framework that reformulates token compression into an end-to-end\nlearnable decision process. To be specific, we propose VisionSelector, a scorer\nmodule decoupled from the MLLM backbone that incorporates a differentiable\nTop-K mechanism and a curriculum annealing strategy to bridge the\ntraining-inference gap, enabling efficient and adaptive token selection various\narbitrary compression rates. Remarkably lightweight with only 12.85M trainable\nparameters, VisionSelector demonstrates generalization across various\ncompression rates and adaptively identifying critical tokens. This leads to\nsuperior performance across all compression budgets, evidenced by preserving\n100% accuracy on MME with 30% retention budget, outperforming prior methods by\n12.14% at 10% retention budget, and doubling prefill speed. Our code is\navailable at https://github.com/JulietChoo/VisionSelector .", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVisionSelector\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u89c9\u4ee4\u724c\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u51b3\u7b56\u8fc7\u7a0b\u5b9e\u73b0\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u4ee4\u724c\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6216\u591a\u56fe\u50cf\u8f93\u5165\u65f6\u9762\u4e34\u5927\u91cf\u89c6\u89c9\u4ee4\u724c\u5e26\u6765\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\uff0c\u73b0\u6709\u4ee4\u724c\u538b\u7f29\u6280\u672f\u53d7\u9650\u4e8e\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u5b58\u5728\u4e22\u5f03\u5173\u952e\u4fe1\u606f\u548c\u6ce8\u610f\u529b\u504f\u5dee\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u6fc0\u8fdb\u538b\u7f29\u7387\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002", "method": "\u63d0\u51faVisionSelector\u6846\u67b6\uff0c\u5c06\u4ee4\u724c\u538b\u7f29\u91cd\u65b0\u8868\u8ff0\u4e3a\u7aef\u5230\u7aef\u53ef\u5b66\u4e60\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5305\u542b\u89e3\u8026\u4e8eMLLM\u9aa8\u5e72\u7f51\u7edc\u7684\u8bc4\u5206\u5668\u6a21\u5757\uff0c\u91c7\u7528\u53ef\u5fae\u5206Top-K\u673a\u5236\u548c\u8bfe\u7a0b\u9000\u706b\u7b56\u7565\u6765\u5f25\u5408\u8bad\u7ec3-\u63a8\u7406\u5dee\u8ddd\uff0c\u652f\u6301\u4efb\u610f\u538b\u7f29\u7387\u4e0b\u7684\u9ad8\u6548\u81ea\u9002\u5e94\u4ee4\u724c\u9009\u62e9\u3002", "result": "VisionSelector\u4ec5\u970012.85M\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5728MME\u57fa\u51c6\u4e0a\u4ee530%\u4fdd\u7559\u9884\u7b97\u4fdd\u6301100%\u51c6\u786e\u7387\uff0c\u572810%\u4fdd\u7559\u9884\u7b97\u4e0b\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u534712.14%\uff0c\u9884\u586b\u5145\u901f\u5ea6\u63d0\u5347\u4e24\u500d\uff0c\u5c55\u73b0\u51fa\u8de8\u4e0d\u540c\u538b\u7f29\u9884\u7b97\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u81ea\u9002\u5e94\u8bc6\u522b\u5173\u952e\u4ee4\u724c\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u53ef\u5b66\u4e60\u51b3\u7b56\u8fc7\u7a0b\u5b9e\u73b0\u4ee4\u724c\u538b\u7f29\u7684\u6709\u6548\u6027\uff0cVisionSelector\u7684\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u548c\u6cdb\u5316\u80fd\u529b\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4e3a\u81ea\u9002\u5e94\u4ee4\u724c\u9009\u62e9\u673a\u5236\u7684\u672a\u6765\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.16781", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16781", "abs": "https://arxiv.org/abs/2510.16781", "authors": ["Shihao Ji", "Zihui Song"], "title": "Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features", "comment": null, "summary": "The remarkable zero-shot reasoning capabilities of large-scale Visual\nLanguage Models (VLMs) on static images have yet to be fully translated to the\nvideo domain. Conventional video understanding models often rely on extensive,\ntask-specific training on annotated datasets, a process that is both costly and\nlimited in scalability. This paper introduces a novel, training-free framework\nfor video understanding that circumvents end-to-end training by synergistically\ncombining the rich semantic priors of pre-trained VLMs with classic machine\nlearning algorithms for pattern discovery. Our core idea is to reframe video\nunderstanding as a self-supervised spatio-temporal clustering problem within a\nhigh-dimensional semantic feature space. The proposed pipeline first transforms\na video stream into a semantic feature trajectory using the frozen visual\nencoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal\nSegmentation (KTS), a robust machine learning technique, to partition the\ncontinuous feature stream into discrete, semantically coherent event segments.\nThese segments are then subjected to unsupervised density-based clustering to\nidentify recurring macroscopic scenes and themes throughout the video. By\nselecting representative keyframes from each discovered cluster and leveraging\nthe VLM's generative capabilities for textual description, our framework\nautomatically produces a structured, multi-modal summary of the video content.\nThis approach provides an effective, interpretable, and model-agnostic pathway\nfor zero-shot, automated structural analysis of video content.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\u4e0e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u76f8\u7ed3\u5408\uff0c\u5c06\u89c6\u9891\u7406\u89e3\u91cd\u65b0\u5b9a\u4e49\u4e3a\u9ad8\u7ef4\u8bed\u4e49\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u81ea\u76d1\u7763\u65f6\u7a7a\u805a\u7c7b\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9759\u6001\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd9\u79cd\u80fd\u529b\u5c1a\u672a\u5145\u5206\u8fc1\u79fb\u5230\u89c6\u9891\u9886\u57df\uff0c\u4f20\u7edf\u89c6\u9891\u7406\u89e3\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u53ef\u6269\u5c55\u6027\u6709\u9650\u3002", "method": "\u8be5\u6846\u67b6\u9996\u5148\u4f7f\u7528\u9884\u8bad\u7ec3VLM\u7684\u51bb\u7ed3\u89c6\u89c9\u7f16\u7801\u5668\u5c06\u89c6\u9891\u6d41\u8f6c\u6362\u4e3a\u8bed\u4e49\u7279\u5f81\u8f68\u8ff9\uff0c\u7136\u540e\u91c7\u7528\u6838\u65f6\u95f4\u5206\u5272\u7b97\u6cd5\u5c06\u8fde\u7eed\u7279\u5f81\u6d41\u5212\u5206\u4e3a\u79bb\u6563\u7684\u8bed\u4e49\u8fde\u8d2f\u4e8b\u4ef6\u7247\u6bb5\uff0c\u6700\u540e\u901a\u8fc7\u65e0\u76d1\u7763\u5bc6\u5ea6\u805a\u7c7b\u8bc6\u522b\u89c6\u9891\u4e2d\u91cd\u590d\u51fa\u73b0\u7684\u5b8f\u89c2\u573a\u666f\u548c\u4e3b\u9898\u3002", "result": "\u901a\u8fc7\u4ece\u6bcf\u4e2a\u53d1\u73b0\u7684\u805a\u7c7b\u4e2d\u9009\u62e9\u4ee3\u8868\u6027\u5173\u952e\u5e27\u5e76\u5229\u7528VLM\u7684\u751f\u6210\u80fd\u529b\u8fdb\u884c\u6587\u672c\u63cf\u8ff0\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u751f\u6210\u89c6\u9891\u5185\u5bb9\u7684\u7ed3\u6784\u5316\u591a\u6a21\u6001\u6458\u8981\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684\u81ea\u52a8\u5316\u89c6\u9891\u7ed3\u6784\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u6a21\u578b\u65e0\u5173\u7684\u8def\u5f84\uff0c\u7528\u4e8e\u89c6\u9891\u5185\u5bb9\u7684\u96f6\u6837\u672c\u81ea\u52a8\u7ed3\u6784\u5206\u6790\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u5f00\u8f9f\u4e86\u65e0\u9700\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u65b0\u8303\u5f0f\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.16688", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16688", "abs": "https://arxiv.org/abs/2510.16688", "authors": ["Yejie Guo", "Yunzhong Hou", "Wufei Ma", "Meng Tang", "Ming-Hsuan Yang"], "title": "Pursuing Minimal Sufficiency in Spatial Reasoning", "comment": null, "summary": "Spatial reasoning, the ability to ground language in 3D understanding,\nremains a persistent challenge for Vision-Language Models (VLMs). We identify\ntwo fundamental bottlenecks: inadequate 3D understanding capabilities stemming\nfrom 2D-centric pre-training, and reasoning failures induced by redundant 3D\ninformation. To address these, we first construct a Minimal Sufficient Set\n(MSS) of information before answering a given question: a compact selection of\n3D perception results from \\textit{expert models}. We introduce MSSR (Minimal\nSufficient Spatial Reasoner), a dual-agent framework that implements this\nprinciple. A Perception Agent programmatically queries 3D scenes using a\nversatile perception toolbox to extract sufficient information, including a\nnovel SOG (Situated Orientation Grounding) module that robustly extracts\nlanguage-grounded directions. A Reasoning Agent then iteratively refines this\ninformation to pursue minimality, pruning redundant details and requesting\nmissing ones in a closed loop until the MSS is curated. Extensive experiments\ndemonstrate that our method, by explicitly pursuing both sufficiency and\nminimality, significantly improves accuracy and achieves state-of-the-art\nperformance across two challenging benchmarks. Furthermore, our framework\nproduces interpretable reasoning paths, offering a promising source of\nhigh-quality training data for future models. Source code is available at\nhttps://github.com/gyj155/mssr.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MSSR\uff08\u6700\u5c0f\u5145\u5206\u7a7a\u95f4\u63a8\u7406\u5668\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u6700\u5c0f\u5145\u5206\u4fe1\u606f\u96c6\u6765\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u74f6\u9888\uff1a\u7531\u4e8e2D\u4e2d\u5fc3\u9884\u8bad\u7ec3\u5bfc\u81f4\u76843D\u7406\u89e3\u80fd\u529b\u4e0d\u8db3\uff0c\u4ee5\u53ca\u5197\u4f593D\u4fe1\u606f\u5f15\u53d1\u7684\u63a8\u7406\u5931\u8d25\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u57283D\u573a\u666f\u4e2d\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "method": "MSSR\u91c7\u7528\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5176\u4e2d\u611f\u77e5\u667a\u80fd\u4f53\u901a\u8fc7\u7a0b\u5e8f\u5316\u67e5\u8be23D\u573a\u666f\u5e76\u4f7f\u7528\u591a\u529f\u80fd\u611f\u77e5\u5de5\u5177\u7bb1\u63d0\u53d6\u5145\u5206\u4fe1\u606f\uff0c\u5305\u62ec\u65b0\u9896\u7684SOG\uff08\u60c5\u5883\u5316\u65b9\u5411\u5b9a\u4f4d\uff09\u6a21\u5757\uff1b\u63a8\u7406\u667a\u80fd\u4f53\u5219\u8fed\u4ee3\u4f18\u5316\u8fd9\u4e9b\u4fe1\u606f\u4ee5\u8ffd\u6c42\u6700\u5c0f\u5316\uff0c\u5728\u95ed\u73af\u4e2d\u4fee\u526a\u5197\u4f59\u7ec6\u8282\u5e76\u8bf7\u6c42\u7f3a\u5931\u4fe1\u606f\uff0c\u76f4\u5230\u6784\u5efa\u51fa\u6700\u5c0f\u5145\u5206\u4fe1\u606f\u96c6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u8ffd\u6c42\u5145\u5206\u6027\u548c\u6700\u5c0f\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6846\u67b6\u4ea7\u751f\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8def\u5f84\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a3D\u7a7a\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6700\u5c0f\u5145\u5206\u4fe1\u606f\u96c6\u539f\u5219\u6709\u6548\u89e3\u51b3\u4e86\u4fe1\u606f\u5197\u4f59\u548c\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4ea7\u751f\u7684\u53ef\u89e3\u91ca\u63a8\u7406\u8def\u5f84\u4e3a\u672a\u6765\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u6e90\uff0c\u63a8\u52a8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u7406\u89e3\u65b9\u9762\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.16704", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16704", "abs": "https://arxiv.org/abs/2510.16704", "authors": ["Tianxin Wei", "Yifan Chen", "Xinrui He", "Wenxuan Bao", "Jingrui He"], "title": "Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization", "comment": "Accepted by KDD 2025", "summary": "Distribution shifts between training and testing samples frequently occur in\npractice and impede model generalization performance. This crucial challenge\nthereby motivates studies on domain generalization (DG), which aim to predict\nthe label on unseen target domain data by solely using data from source\ndomains. It is intuitive to conceive the class-separated representations\nlearned in contrastive learning (CL) are able to improve DG, while the reality\nis quite the opposite: users observe directly applying CL deteriorates the\nperformance. We analyze the phenomenon with the insights from CL theory and\ndiscover lack of intra-class connectivity in the DG setting causes the\ndeficiency. We thus propose a new paradigm, domain-connecting contrastive\nlearning (DCCL), to enhance the conceptual connectivity across domains and\nobtain generalizable representations for DG. On the data side, more aggressive\ndata augmentation and cross-domain positive samples are introduced to improve\nintra-class connectivity. On the model side, to better embed the unseen test\ndomains, we propose model anchoring to exploit the intra-class connectivity in\npre-trained representations and complement the anchoring with generative\ntransformation loss. Extensive experiments on five standard DG benchmarks are\nperformed. The results verify that DCCL outperforms state-of-the-art baselines\neven without domain supervision. The detailed model implementation and the code\nare provided through https://github.com/weitianxin/DCCL", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9886\u57df\u8fde\u63a5\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0fDCCL\uff0c\u901a\u8fc7\u589e\u5f3a\u8de8\u9886\u57df\u7684\u6982\u5ff5\u8fde\u901a\u6027\u6765\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u4e2d\u76f4\u63a5\u5e94\u7528\u5bf9\u6bd4\u5b66\u4e60\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5728\u4e94\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u9886\u57df\u6cdb\u5316\u4e2d\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6837\u672c\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u4e25\u91cd\u963b\u788d\u6a21\u578b\u6cdb\u5316\u6027\u80fd\uff0c\u867d\u7136\u5bf9\u6bd4\u5b66\u4e60\u7406\u8bba\u4e0a\u5e94\u80fd\u901a\u8fc7\u7c7b\u522b\u5206\u79bb\u8868\u793a\u6539\u5584\u9886\u57df\u6cdb\u5316\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u76f4\u63a5\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u53cd\u800c\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u7814\u7a76\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\u9886\u57df\u6cdb\u5316\u8bbe\u7f6e\u4e2d\u7f3a\u4e4f\u7c7b\u5185\u8fde\u901a\u6027\u5bfc\u81f4\u7684\u3002", "method": "\u63d0\u51fa\u4e86\u9886\u57df\u8fde\u63a5\u5bf9\u6bd4\u5b66\u4e60DCCL\u8303\u5f0f\uff0c\u5728\u6570\u636e\u5c42\u9762\u91c7\u7528\u66f4\u6fc0\u8fdb\u7684\u6570\u636e\u589e\u5f3a\u548c\u8de8\u9886\u57df\u6b63\u6837\u672c\u6765\u589e\u5f3a\u7c7b\u5185\u8fde\u901a\u6027\uff0c\u5728\u6a21\u578b\u5c42\u9762\u63d0\u51fa\u6a21\u578b\u951a\u5b9a\u6280\u672f\u5229\u7528\u9884\u8bad\u7ec3\u8868\u793a\u4e2d\u7684\u7c7b\u5185\u8fde\u901a\u6027\uff0c\u5e76\u8f85\u4ee5\u751f\u6210\u53d8\u6362\u635f\u5931\u6765\u66f4\u597d\u5730\u5d4c\u5165\u672a\u89c1\u6d4b\u8bd5\u9886\u57df\u3002", "result": "\u5728\u4e94\u4e2a\u6807\u51c6\u9886\u57df\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DCCL\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u9886\u57df\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u4ee3\u7801\u5b9e\u73b0\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u63d0\u4f9b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u589e\u5f3a\u8de8\u9886\u57df\u7684\u6982\u5ff5\u8fde\u901a\u6027\u5bf9\u4e8e\u9886\u57df\u6cdb\u5316\u81f3\u5173\u91cd\u8981\uff0cDCCL\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u6a21\u578b\u951a\u5b9a\u7684\u53cc\u91cd\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5bf9\u6bd4\u5b66\u4e60\u5728\u9886\u57df\u6cdb\u5316\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4e3a\u65e0\u76d1\u7763\u9886\u57df\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16926", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16926", "abs": "https://arxiv.org/abs/2510.16926", "authors": ["Chenxu Li", "Zhicai Wang", "Yuan Sheng", "Xingyu Zhu", "Yanbin Hao", "Xiang Wang"], "title": "Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input", "comment": "23 pages,19 figures", "summary": "Multimodal Large Language Models (MLLMs) increasingly support dynamic image\nresolutions. However, current evaluation paradigms primarily assess semantic\nperformance, overlooking the critical question of resolution robustness -\nwhether performance remains stable across varying input resolutions. To address\nthis gap, we introduce \\textbf{Res-Bench}, a comprehensive benchmark comprising\n14,400 samples across 12 resolution levels and six core capability dimensions.\nWe designed a novel evaluation framework that goes beyond traditional accuracy\nmetrics to capture performance stability. This framework introduces multiple\nrobustness metrics: Spearman's correlation for assessing resolution-performance\ntrends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring\nperformance volatility. Using these metrics, we conducted a large-scale\nevaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and\ntask-centric robustness examination, (2) investigation of preprocessing\nstrategies including padding and super-resolution, and (3) exploration of\nfine-tuning for stability enhancement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Res-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u8f93\u5165\u5206\u8fa8\u7387\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u91cf\u5316\u6a21\u578b\u6027\u80fd\u968f\u5206\u8fa8\u7387\u53d8\u5316\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u6027\u80fd\uff0c\u800c\u5ffd\u89c6\u4e86\u5206\u8fa8\u7387\u9c81\u68d2\u6027\u8fd9\u4e00\u5173\u952e\u95ee\u9898\uff0c\u5373\u6a21\u578b\u6027\u80fd\u5728\u4e0d\u540c\u8f93\u5165\u5206\u8fa8\u7387\u4e0b\u662f\u5426\u4fdd\u6301\u7a33\u5b9a\uff0c\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u9700\u8981\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u89e3\u51b3\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86Res-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b14,400\u4e2a\u6837\u672c\u8986\u76d612\u4e2a\u5206\u8fa8\u7387\u7ea7\u522b\u548c6\u4e2a\u6838\u5fc3\u80fd\u529b\u7ef4\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u542bSpearman\u76f8\u5173\u6027\u5206\u6790\u548c\u7edd\u5bf9/\u76f8\u5bf9\u8fde\u7eed\u8bef\u5dee\u5728\u5185\u7684\u591a\u79cd\u9c81\u68d2\u6027\u8bc4\u4f30\u6307\u6807\u6765\u8861\u91cf\u6027\u80fd\u7a33\u5b9a\u6027\u3002", "result": "\u901a\u8fc7\u5927\u89c4\u6a21\u8bc4\u4f30\u4e3b\u6d41MLLMs\uff0c\u7814\u7a76\u4ece\u6a21\u578b\u4e2d\u5fc3\u5316\u3001\u4efb\u52a1\u4e2d\u5fc3\u5316\u89d2\u5ea6\u5206\u6790\u4e86\u9c81\u68d2\u6027\uff0c\u8003\u5bdf\u4e86\u586b\u5145\u548c\u8d85\u5206\u8fa8\u7387\u7b49\u9884\u5904\u7406\u7b56\u7565\uff0c\u5e76\u63a2\u7d22\u4e86\u5fae\u8c03\u5bf9\u7a33\u5b9a\u6027\u63d0\u5347\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5206\u8fa8\u7387\u9c81\u68d2\u6027\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u4e3aMLLMs\u7684\u7a33\u5065\u6027\u53d1\u5c55\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u7684\u6027\u80fd\u6ce2\u52a8\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u4f18\u5316\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.16714", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16714", "abs": "https://arxiv.org/abs/2510.16714", "authors": ["Xiongkun Linghu", "Jiangyong Huang", "Ziyu Zhu", "Baoxiong Jia", "Siyuan Huang"], "title": "Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes", "comment": "Project page: https://scenecot.github.io/", "summary": "Existing research on 3D Large Language Models (LLMs) still struggles to\nachieve grounded question-answering, primarily due to the under-exploration of\nthe mech- anism of human-like scene-object grounded reasoning. This paper\nbridges the gap by presenting a novel framework. We first introduce a grounded\nChain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a\ncomplex reasoning task into simpler and manageable problems, and building\ncorresponding visual clues based on multimodal expert modules. To enable such a\nmethod, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning\ndataset, consisting of 185K high-quality instances. Extensive experiments\nacross various complex 3D scene reasoning benchmarks demonstrate that our new\nframework achieves strong performance with high grounding-QA coherence. To the\nbest of our knowledge, this is the first successful application of CoT\nreasoning to 3D scene understanding, enabling step-by-step human-like reasoning\nand showing potential for extension to broader 3D scene understanding\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5e94\u7528\u4e8e3D\u573a\u666f\u7406\u89e3\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u6846\u67b6SCENECOT\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6SCENECOT-185K\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u4e13\u5bb6\u6a21\u5757\u7684\u6e10\u8fdb\u5f0f\u573a\u666f-\u5bf9\u8c61\u63a5\u5730\u63a8\u7406\uff0c\u5728\u591a\u4e2a\u590d\u67423D\u573a\u666f\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a5\u5730\u95ee\u7b54\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5bf9\u4eba\u7c7b\u573a\u666f-\u5bf9\u8c61\u63a5\u5730\u63a8\u7406\u673a\u5236\u7684\u6df1\u5165\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u89e3\u51b3\u590d\u67423D\u573a\u666f\u63a8\u7406\u4e2d\u7684\u63a5\u5730\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e3D\u573a\u666f\u7684\u63a5\u5730\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5SCENECOT\uff0c\u5c06\u590d\u6742\u63a8\u7406\u4efb\u52a1\u5206\u89e3\u4e3a\u66f4\u7b80\u5355\u7684\u5b50\u95ee\u9898\uff0c\u5e76\u5229\u7528\u591a\u6a21\u6001\u4e13\u5bb6\u6a21\u5757\u6784\u5efa\u76f8\u5e94\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u63a5\u5730CoT\u63a8\u7406\u6570\u636e\u96c6SCENECOT-185K\uff0c\u5305\u542b18.5\u4e07\u4e2a\u9ad8\u8d28\u91cf\u5b9e\u4f8b\u3002", "result": "\u5728\u591a\u4e2a\u590d\u67423D\u573a\u666f\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u6846\u67b6\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u5177\u6709\u9ad8\u5ea6\u7684\u63a5\u5730\u95ee\u7b54\u4e00\u81f4\u6027\uff0c\u9a8c\u8bc1\u4e86CoT\u63a8\u7406\u57283D\u573a\u666f\u7406\u89e3\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u662f\u601d\u7ef4\u94fe\u63a8\u7406\u57283D\u573a\u666f\u7406\u89e3\u4e2d\u7684\u9996\u6b21\u6210\u529f\u5e94\u7528\uff0c\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u7684\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u793a\u51fa\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb3D\u573a\u666f\u7406\u89e3\u573a\u666f\u7684\u6f5c\u529b\uff0c\u4e3a3D\u573a\u666f\u63a8\u7406\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.17205", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17205", "abs": "https://arxiv.org/abs/2510.17205", "authors": ["Yingqi Fan", "Anhao Zhao", "Jinlan Fu", "Junlong Tong", "Hui Su", "Yijie Pan", "Wei Zhang", "Xiaoyu Shen"], "title": "$\\mathcal{V}isi\\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs", "comment": "EMNLP 2025 Main", "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance\nacross vision-language tasks, but suffer from significant computational\noverhead due to the quadratic growth of attention computations with the number\nof multimodal tokens. Though efforts have been made to prune tokens in MLLMs,\n\\textit{they lack a fundamental understanding of how MLLMs process and fuse\nmultimodal information.} Through systematic analysis, we uncover a\n\\textbf{three-stage} cross-modal interaction process: (1) Shallow layers\nrecognize task intent, with visual tokens acting as passive attention sinks;\n(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few\ncritical visual tokens; (3) Deep layers discard vision tokens, focusing solely\non linguistic refinement. Based on these findings, we propose\n\\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\\% of\nvision-related attention computations and 53.9\\% of FLOPs on LLaVA-v1.5 7B. It\nsignificantly outperforms existing token pruning methods and generalizes across\ndiverse MLLMs. Beyond pruning, our insights further provide actionable\nguidelines for training efficient MLLMs by aligning model architecture with its\nintrinsic layer-wise processing dynamics. Our code is available at:\nhttps://github.com/EIT-NLP/VisiPruner.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e09\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u526a\u679d\u6846\u67b6VisiPruner\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u89c6\u89c9\u76f8\u5173\u6ce8\u610f\u529b\u8ba1\u7b97\u548cFLOPs\uff0c\u540c\u65f6\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u5e76\u9002\u7528\u4e8e\u591a\u79cdMLLM\u67b6\u6784\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u65f6\u9762\u4e34\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u6ce8\u610f\u529b\u8ba1\u7b97\u968f\u591a\u6a21\u6001token\u6570\u91cf\u5448\u4e8c\u6b21\u589e\u957f\uff0c\u800c\u73b0\u6709\u7684token\u526a\u679d\u65b9\u6cd5\u7f3a\u4e4f\u5bf9MLLM\u5982\u4f55\u5904\u7406\u548c\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u7684\u57fa\u672c\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u53d1\u73b0MLLM\u5b58\u5728\u4e09\u9636\u6bb5\u8de8\u6a21\u6001\u4ea4\u4e92\u8fc7\u7a0b\uff1a\u6d45\u5c42\u8bc6\u522b\u4efb\u52a1\u610f\u56fe\u3001\u4e2d\u5c42\u5173\u952e\u89c6\u89c9token\u9a71\u52a8\u8de8\u6a21\u6001\u878d\u5408\u3001\u6df1\u5c42\u4ec5\u5173\u6ce8\u8bed\u8a00\u7cbe\u70bc\uff0c\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u526a\u679d\u6846\u67b6VisiPruner\uff0c\u9488\u5bf9\u6027\u5730\u5728\u4e0d\u540c\u9636\u6bb5\u8fdb\u884c\u89c6\u89c9token\u526a\u679d\u3002", "result": "VisiPruner\u5728LLaVA-v1.5 7B\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe99%\u7684\u89c6\u89c9\u76f8\u5173\u6ce8\u610f\u529b\u8ba1\u7b97\u51cf\u5c11\u548c53.9%\u7684FLOPs\u964d\u4f4e\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709token\u526a\u679d\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u79cdMLLM\u67b6\u6784\u4e0a\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u526a\u679d\u89e3\u51b3\u65b9\u6848\uff0c\u66f4\u91cd\u8981\u7684\u662f\u63ed\u793a\u4e86MLLM\u5185\u5728\u7684\u5c42\u6b21\u5904\u7406\u52a8\u6001\uff0c\u4e3a\u8bad\u7ec3\u9ad8\u6548MLLM\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u539f\u5219\uff0c\u5373\u901a\u8fc7\u4f7f\u6a21\u578b\u67b6\u6784\u4e0e\u5176\u5185\u5728\u5904\u7406\u52a8\u6001\u5bf9\u9f50\u6765\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2510.16751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16751", "abs": "https://arxiv.org/abs/2510.16751", "authors": ["Erik Riise", "Mehmet Onurcan Kaya", "Dim P. Papadopoulos"], "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling", "comment": null, "summary": "While inference-time scaling through search has revolutionized Large Language\nModels, translating these gains to image generation has proven difficult.\nRecent attempts to apply search strategies to continuous diffusion models show\nlimited benefits, with simple random sampling often performing best. We\ndemonstrate that the discrete, sequential nature of visual autoregressive\nmodels enables effective search for image generation. We show that beam search\nsubstantially improves text-to-image generation, enabling a 2B parameter\nautoregressive model to outperform a 12B parameter diffusion model across\nbenchmarks. Systematic ablations show that this advantage comes from the\ndiscrete token space, which allows early pruning and computational reuse, and\nour verifier analysis highlights trade-offs between speed and reasoning\ncapability. These findings suggest that model architecture, not just scale, is\ncritical for inference-time optimization in visual generation.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc1\u660e\u79bb\u6563\u81ea\u56de\u5f52\u89c6\u89c9\u6a21\u578b\u80fd\u591f\u6709\u6548\u5229\u7528\u675f\u641c\u7d22\u8fdb\u884c\u63a8\u7406\u65f6\u4f18\u5316\uff0c\u4f7f2B\u53c2\u6570\u7684\u81ea\u56de\u5f52\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8d85\u8d8a12B\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u67b6\u6784\u5bf9\u63a8\u7406\u65f6\u4f18\u5316\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u5c3d\u7ba1\u63a8\u7406\u65f6\u641c\u7d22\u7b56\u7565\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u9769\u547d\u6027\u6210\u529f\uff0c\u4f46\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u7684\u5e94\u7528\u6548\u679c\u6709\u9650\uff0c\u8fde\u7eed\u6269\u6563\u6a21\u578b\u4e2d\u7684\u641c\u7d22\u7b56\u7565\u6536\u76ca\u751a\u5fae\uff0c\u7b80\u5355\u968f\u673a\u91c7\u6837\u5f80\u5f80\u8868\u73b0\u6700\u4f73\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u5bf9\u63a8\u7406\u65f6\u4f18\u5316\u7684\u9002\u5e94\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u79bb\u6563\u81ea\u56de\u5f52\u89c6\u89c9\u6a21\u578b\u67b6\u6784\uff0c\u5229\u7528\u675f\u641c\u7d22\u7b56\u7565\u8fdb\u884c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u901a\u8fc7\u79bb\u6563\u6807\u8bb0\u7a7a\u95f4\u7684\u7279\u6027\u5b9e\u73b0\u65e9\u671f\u526a\u679d\u548c\u8ba1\u7b97\u91cd\u7528\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u4e86\u9a8c\u8bc1\u5668\u5728\u901f\u5ea6\u4e0e\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u675f\u641c\u7d22\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c2B\u53c2\u6570\u7684\u81ea\u56de\u5f52\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e8612B\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\uff0c\u7cfb\u7edf\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u79bb\u6563\u6807\u8bb0\u7a7a\u95f4\u662f\u5b9e\u73b0\u8fd9\u4e00\u4f18\u52bf\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u6a21\u578b\u67b6\u6784\u800c\u4e0d\u4ec5\u4ec5\u662f\u89c4\u6a21\u5bf9\u4e8e\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u63a8\u7406\u65f6\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0c\u79bb\u6563\u81ea\u56de\u5f52\u6a21\u578b\u56e0\u5176\u652f\u6301\u6709\u6548\u641c\u7d22\u7b56\u7565\u800c\u5c55\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\uff0c\u8fd9\u4e3a\u672a\u6765\u89c6\u89c9\u751f\u6210\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2510.16772", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16772", "abs": "https://arxiv.org/abs/2510.16772", "authors": ["Thuy Phuong Vu", "Dinh-Cuong Hoang", "Minhhuy Le", "Phan Xuan Tan"], "title": "Region in Context: Text-condition Image editing with Human-like semantic reasoning", "comment": null, "summary": "Recent research has made significant progress in localizing and editing image\nregions based on text. However, most approaches treat these regions in\nisolation, relying solely on local cues without accounting for how each part\ncontributes to the overall visual and semantic composition. This often results\nin inconsistent edits, unnatural transitions, or loss of coherence across the\nimage. In this work, we propose Region in Context, a novel framework for\ntext-conditioned image editing that performs multilevel semantic alignment\nbetween vision and language, inspired by the human ability to reason about\nedits in relation to the whole scene. Our method encourages each region to\nunderstand its role within the global image context, enabling precise and\nharmonized changes. At its core, the framework introduces a dual-level guidance\nmechanism: regions are represented with full-image context and aligned with\ndetailed region-level descriptions, while the entire image is simultaneously\nmatched to a comprehensive scene-level description generated by a large\nvision-language model. These descriptions serve as explicit verbal references\nof the intended content, guiding both local modifications and global structure.\nExperiments show that it produces more coherent and instruction-aligned\nresults. Code is available at:\nhttps://github.com/thuyvuphuong/Region-in-Context.git", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Region in Context\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u7ea7\u8bed\u4e49\u5bf9\u9f50\u5b9e\u73b0\u6587\u672c\u6761\u4ef6\u56fe\u50cf\u7f16\u8f91\uff0c\u4f7f\u5c40\u90e8\u533a\u57df\u5728\u5168\u5c40\u56fe\u50cf\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u534f\u8c03\u4e00\u81f4\u7684\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u533a\u57df\u7f16\u8f91\u4e2d\u7f3a\u4e4f\u6574\u4f53\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u6761\u4ef6\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u901a\u5e38\u5c06\u56fe\u50cf\u533a\u57df\u89c6\u4e3a\u5b64\u7acb\u5355\u5143\uff0c\u4ec5\u4f9d\u8d56\u5c40\u90e8\u7ebf\u7d22\u800c\u5ffd\u7565\u4e86\u5404\u90e8\u5206\u5bf9\u6574\u4f53\u89c6\u89c9\u548c\u8bed\u4e49\u6784\u6210\u7684\u8d21\u732e\uff0c\u5bfc\u81f4\u7f16\u8f91\u7ed3\u679c\u51fa\u73b0\u4e0d\u4e00\u81f4\u3001\u4e0d\u81ea\u7136\u8fc7\u6e21\u6216\u56fe\u50cf\u6574\u4f53\u8fde\u8d2f\u6027\u4e27\u5931\u7684\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u53cc\u5c42\u7ea7\u5f15\u5bfc\u673a\u5236\uff1a\u533a\u57df\u5728\u5b8c\u6574\u56fe\u50cf\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u8868\u793a\u5e76\u4e0e\u8be6\u7ec6\u533a\u57df\u7ea7\u63cf\u8ff0\u5bf9\u9f50\uff0c\u540c\u65f6\u6574\u4e2a\u56fe\u50cf\u4e0e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5168\u9762\u573a\u666f\u7ea7\u63cf\u8ff0\u8fdb\u884c\u5339\u914d\uff0c\u8fd9\u4e9b\u63cf\u8ff0\u4f5c\u4e3a\u660e\u786e\u7684\u8bed\u8a00\u53c2\u8003\u6307\u5bfc\u5c40\u90e8\u4fee\u6539\u548c\u5168\u5c40\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u52a0\u8fde\u8d2f\u4e14\u4e0e\u6307\u4ee4\u5bf9\u9f50\u7684\u7f16\u8f91\u7ed3\u679c\uff0c\u5728\u4fdd\u6301\u5c40\u90e8\u7cbe\u786e\u6027\u7684\u540c\u65f6\u786e\u4fdd\u4e86\u6574\u4f53\u56fe\u50cf\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u8003\u8651\u5168\u5c40\u4e0a\u4e0b\u6587\u7684\u91cd\u8981\u6027\uff0c\u901a\u8fc7\u591a\u5c42\u7ea7\u8bed\u4e49\u5bf9\u9f50\u5b9e\u73b0\u4e86\u5c40\u90e8\u7f16\u8f91\u4e0e\u6574\u4f53\u573a\u666f\u7684\u534f\u8c03\u7edf\u4e00\uff0c\u4e3a\u6587\u672c\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17790", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17790", "abs": "https://arxiv.org/abs/2510.17790", "authors": ["Yuhao Yang", "Zhen Yang", "Zi-Yi Dou", "Anh Nguyen", "Keen You", "Omar Attia", "Andrew Szot", "Michael Feng", "Ram Ramrakhya", "Alexander Toshev", "Chao Huang", "Yinfei Yang", "Zhe Gan"], "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action", "comment": null, "summary": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.", "AI": {"tldr": "UltraCUA\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u884c\u52a8\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5c06GUI\u539f\u8bed\u64cd\u4f5c\u4e0e\u9ad8\u7ea7\u7a0b\u5e8f\u5316\u5de5\u5177\u8c03\u7528\u65e0\u7f1d\u96c6\u6210\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u4ec5\u4f9d\u8d56\u539f\u59cb\u64cd\u4f5c\u5bfc\u81f4\u7684\u7ea7\u8054\u5931\u8d25\u548c\u6267\u884c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u4ec5\u4f9d\u8d56\u70b9\u51fb\u3001\u8f93\u5165\u3001\u6eda\u52a8\u7b49\u539f\u59cb\u64cd\u4f5c\uff0c\u9700\u8981\u7cbe\u786e\u7684\u89c6\u89c9\u5b9a\u4f4d\u548c\u5197\u957f\u7684\u6267\u884c\u94fe\uff0c\u5bfc\u81f4\u7ea7\u8054\u5931\u8d25\u548c\u6027\u80fd\u74f6\u9888\uff0c\u540c\u65f6\u8fd9\u4e9b\u4ee3\u7406\u65e0\u6cd5\u5229\u7528\u4e30\u5bcc\u7684\u7a0b\u5e8f\u5316\u63a5\u53e3\uff08\u5982API\u3001MCP\u670d\u52a1\u5668\u3001\u5de5\u5177\uff09\u7b49\u80fd\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u542b\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u4ece\u8f6f\u4ef6\u6587\u6863\u3001\u5f00\u6e90\u4ed3\u5e93\u548c\u4ee3\u7801\u751f\u6210\u4e2d\u6269\u5c55\u7a0b\u5e8f\u5316\u5de5\u5177\u7684\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff1b\u751f\u6210\u8d85\u8fc717,000\u4e2a\u53ef\u9a8c\u8bc1\u4efb\u52a1\u7684\u5408\u6210\u6570\u636e\u5f15\u64ce\uff1b\u5305\u542b\u4f4e\u7ea7GUI\u64cd\u4f5c\u548c\u9ad8\u7ea7\u7a0b\u5e8f\u5316\u5de5\u5177\u8c03\u7528\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6df7\u5408\u884c\u52a8\u8f68\u8ff9\u6536\u96c6\uff1b\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4f4e\u7ea7\u548c\u9ad8\u7ea7\u884c\u52a8\u4e4b\u95f4\u7684\u7b56\u7565\u6027\u4ea4\u66ff\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUltraCUA\u6a21\u578b\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5b9e\u73b0\u4e8622%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u6267\u884c\u6b65\u9aa4\u51cf\u5c1111%\uff1b\u5728WindowsAgentArena\u7684\u57df\u5916\u8bc4\u4f30\u4e2d\u8fbe\u523021.7%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u5728Windows\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u6df7\u5408\u884c\u52a8\u673a\u5236\u88ab\u8bc1\u660e\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u51cf\u5c11\u9519\u8bef\u4f20\u64ad\u540c\u65f6\u4fdd\u6301\u6267\u884c\u6548\u7387\uff0c\u4e3a\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u7075\u6d3b\u7684\u884c\u52a8\u80fd\u529b\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u4ec5\u4f9d\u8d56\u539f\u59cb\u64cd\u4f5c\u7684\u6027\u80fd\u9650\u5236\u3002"}}
{"id": "2510.16776", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16776", "abs": "https://arxiv.org/abs/2510.16776", "authors": ["Mingzheng Zhang", "Jinfeng Gao", "Dan Xu", "Jiangrui Yu", "Yuhan Qiao", "Lan Chen", "Jin Tang", "Xiao Wang"], "title": "EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation", "comment": null, "summary": "X-ray image-based medical report generation (MRG) is a pivotal area in\nartificial intelligence that can significantly reduce diagnostic burdens for\nclinicians and patient wait times. Existing MRG models predominantly rely on\nLarge Language Models (LLMs) to improve report generation, with limited\nexploration of pre-trained vision foundation models or advanced fine-tuning\ntechniques. Mainstream frameworks either avoid fine-tuning or utilize\nsimplistic methods like LoRA, often neglecting the potential of enhancing\ncross-attention mechanisms. Additionally, while Transformer-based models\ndominate vision-language tasks, non-Transformer architectures, such as the\nMamba network, remain underexplored for medical report generation, presenting a\npromising avenue for future research. In this paper, we propose EMRRG, a novel\nX-ray report generation framework that fine-tunes pre-trained Mamba networks\nusing parameter-efficient methods. Specifically, X-ray images are divided into\npatches, tokenized, and processed by an SSM-based vision backbone for feature\nextraction, with Partial LoRA yielding optimal performance. An LLM with a\nhybrid decoder generates the medical report, enabling end-to-end training and\nachieving strong results on benchmark datasets. Extensive experiments on three\nwidely used benchmark datasets fully validated the effectiveness of our\nproposed strategies for the X-ray MRG. The source code of this paper will be\nreleased on https://github.com/Event-AHU/Medical_Image_Analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEMRRG\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u5fae\u8c03\u9884\u8bad\u7ec3\u7684Mamba\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86X\u5c04\u7ebf\u533b\u5b66\u62a5\u544a\u751f\u6210\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u62a5\u544a\u751f\u6210\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5bf9\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u5148\u8fdb\u5fae\u8c03\u6280\u672f\u63a2\u7d22\u6709\u9650\uff0c\u4e3b\u6d41\u6846\u67b6\u8981\u4e48\u907f\u514d\u5fae\u8c03\u8981\u4e48\u4f7f\u7528\u7b80\u5355\u65b9\u6cd5\u5982LoRA\uff0c\u540c\u65f6\u5ffd\u7565\u4e86\u975eTransformer\u67b6\u6784\u5982Mamba\u7f51\u7edc\u5728\u533b\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u7684EMRRG\u6846\u67b6\u5c06X\u5c04\u7ebf\u56fe\u50cf\u5206\u5272\u4e3a\u8865\u4e01\u5e76\u6807\u8bb0\u5316\uff0c\u4f7f\u7528\u57fa\u4e8eSSM\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5176\u4e2dPartial LoRA\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u7ed3\u5408\u5177\u6709\u6df7\u5408\u89e3\u7801\u5668\u7684LLM\u751f\u6210\u533b\u5b66\u62a5\u544a\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u5145\u5206\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7b56\u7565\u5bf9X\u5c04\u7ebf\u533b\u5b66\u62a5\u544a\u751f\u6210\u7684\u6709\u6548\u6027\uff0c\u53d6\u5f97\u4e86\u5f3a\u52b2\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86Mamba\u7f51\u7edc\u5728\u533b\u5b66\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u8bc1\u660e\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.17800", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17800", "abs": "https://arxiv.org/abs/2510.17800", "authors": ["Jiale Cheng", "Yusen Liu", "Xinyu Zhang", "Yulin Fei", "Wenyi Hong", "Ruiliang Lyu", "Weihan Wang", "Zhe Su", "Xiaotao Gu", "Xiao Liu", "Yushi Bai", "Jie Tang", "Hongning Wang", "Minlie Huang"], "title": "Glyph: Scaling Context Windows via Visual-Text Compression", "comment": null, "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGlyph\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u957f\u6587\u672c\u6e32\u67d3\u4e3a\u56fe\u50cf\u5e76\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\uff0c\u5b9e\u73b03-4\u500d\u6587\u672c\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edfLLM\u5728\u767e\u4e07\u7ea7token\u4e0a\u4e0b\u6587\u6269\u5c55\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u7406\u89e3\u3001\u4ee3\u7801\u5206\u6790\u548c\u591a\u6b65\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u5bf9\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u9700\u6c42\u7684\u589e\u957f\uff0c\u5c06\u4e0a\u4e0b\u6587\u7a97\u53e3\u6269\u5c55\u5230\u767e\u4e07token\u7ea7\u522b\u5e26\u6765\u4e86\u9ad8\u6602\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u9650\u5236\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u63d0\u51faGlyph\u6846\u67b6\uff0c\u91c7\u7528\u89c6\u89c9\u4e0a\u4e0b\u6587\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c06\u957f\u6587\u672c\u6e32\u67d3\u4e3a\u56fe\u50cf\u5e76\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\uff0c\u540c\u65f6\u8bbe\u8ba1\u57fa\u4e8eLLM\u9a71\u52a8\u7684\u9057\u4f20\u641c\u7d22\u7b97\u6cd5\u6765\u4f18\u5316\u89c6\u89c9\u6e32\u67d3\u914d\u7f6e\uff0c\u4ee5\u5e73\u8861\u7cbe\u5ea6\u548c\u538b\u7f29\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b03-4\u500dtoken\u538b\u7f29\uff0c\u7cbe\u5ea6\u4e0eQwen3-8B\u7b49\u9886\u5148LLM\u76f8\u5f53\uff0c\u9884\u586b\u5145\u548c\u89e3\u7801\u901f\u5ea6\u63d0\u5347\u7ea64\u500d\uff0cSFT\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u7ea62\u500d\uff0c128K\u4e0a\u4e0b\u6587VLM\u53ef\u6269\u5c55\u5904\u7406\u767e\u4e07token\u7ea7\u6587\u672c\u4efb\u52a1\u3002", "conclusion": "\u89c6\u89c9\u4e0a\u4e0b\u6587\u6269\u5c55\u4e3a\u957f\u6587\u672c\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u4f20\u7edftoken\u6269\u5c55\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u6e32\u67d3\u7684\u6587\u672c\u6570\u636e\u8fd8\u80fd\u53d7\u76ca\u4e8e\u73b0\u5b9e\u591a\u6a21\u6001\u4efb\u52a1\u5982\u6587\u6863\u7406\u89e3\uff0c\u4e3a\u5927\u89c4\u6a21\u957f\u4e0a\u4e0b\u6587\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2510.16785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16785", "abs": "https://arxiv.org/abs/2510.16785", "authors": ["Jiazhen Liu", "Long Chen"], "title": "Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs", "comment": null, "summary": "Integrating diverse visual capabilities into a unified model is a significant\ntrend in Multimodal Large Language Models (MLLMs). Among these, the inclusion\nof segmentation poses a distinct set of challenges. To equip MLLMs with\npixel-level segmentation abilities, prevailing methods require finetuning the\nmodel to produce specific outputs compatible with a mask decoder. This process\ntypically alters the model's output space and compromises its intrinsic\ngeneralization, which undermines the goal of building a unified model. We\nintroduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel\nplug-and-play solution. LENS attaches a lightweight, trainable head to a\ncompletely frozen MLLM. By refining the spatial cues embedded in attention\nmaps, LENS extracts keypoints and describes them into point-wise features\ndirectly compatible with the mask decoder. Extensive experiments validate our\napproach: LENS achieves segmentation performance competitive with or superior\nto that of retraining-based methods. Crucially, it does so while fully\npreserving the MLLM's generalization capabilities, which are significantly\ndegraded by finetuning approaches. As such, the attachable design of LENS\nestablishes an efficient and powerful paradigm for extending MLLMs, paving the\nway for truly multi-talented, unified models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLENS\u65b9\u6cd5\uff0c\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6dfb\u52a0\u50cf\u7d20\u7ea7\u5206\u5272\u80fd\u529b\u7684\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u63d0\u53d6\u6ce8\u610f\u529b\u56fe\u4e2d\u7684\u5173\u952e\u70b9\u7279\u5f81\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6dfb\u52a0\u5206\u5272\u80fd\u529b\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5fae\u8c03\u6a21\u578b\u4ee5\u4ea7\u751f\u4e0e\u63a9\u7801\u89e3\u7801\u5668\u517c\u5bb9\u7684\u8f93\u51fa\uff0c\u8fd9\u4f1a\u6539\u53d8\u6a21\u578b\u7684\u8f93\u51fa\u7a7a\u95f4\u5e76\u635f\u5bb3\u5176\u5185\u5728\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u8fdd\u80cc\u4e86\u6784\u5efa\u7edf\u4e00\u6a21\u578b\u7684\u76ee\u6807\u3002", "method": "LENS\u65b9\u6cd5\u5728\u5b8c\u5168\u51bb\u7ed3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u9644\u52a0\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u53ef\u8bad\u7ec3\u5934\u90e8\uff0c\u901a\u8fc7\u7cbe\u70bc\u6ce8\u610f\u529b\u56fe\u4e2d\u5d4c\u5165\u7684\u7a7a\u95f4\u7ebf\u7d22\u6765\u63d0\u53d6\u5173\u952e\u70b9\uff0c\u5e76\u5c06\u5176\u63cf\u8ff0\u4e3a\u4e0e\u63a9\u7801\u89e3\u7801\u5668\u76f4\u63a5\u517c\u5bb9\u7684\u70b9\u7ea7\u7279\u5f81\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LENS\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff1a\u5728\u5206\u5272\u6027\u80fd\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u57fa\u4e8e\u91cd\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u5b8c\u5168\u4fdd\u7559\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u5fae\u8c03\u65b9\u6cd5\u4f1a\u663e\u8457\u964d\u4f4e\u8fd9\u79cd\u80fd\u529b\u3002", "conclusion": "LENS\u7684\u53ef\u9644\u52a0\u8bbe\u8ba1\u4e3a\u6269\u5c55\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u8303\u5f0f\uff0c\u4e3a\u5b9e\u73b0\u771f\u6b63\u591a\u624d\u591a\u827a\u7684\u7edf\u4e00\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u5fae\u8c03\u65b9\u6cd5\u635f\u5bb3\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2510.16791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16791", "abs": "https://arxiv.org/abs/2510.16791", "authors": ["Chengxuan Zhu", "Shuchen Weng", "Jiacong Fang", "Peixuan Zhang", "Si Li", "Chao Xu", "Boxin Shi"], "title": "Personalized Image Filter: Mastering Your Photographic Style", "comment": null, "summary": "Photographic style, as a composition of certain photographic concepts, is the\ncharm behind renowned photographers. But learning and transferring photographic\nstyle need a profound understanding of how the photo is edited from the unknown\noriginal appearance. Previous works either fail to learn meaningful\nphotographic concepts from reference images, or cannot preserve the content of\nthe content image. To tackle these issues, we proposed a Personalized Image\nFilter (PIF). Based on a pretrained text-to-image diffusion model, the\ngenerative prior enables PIF to learn the average appearance of photographic\nconcepts, as well as how to adjust them according to text prompts. PIF then\nlearns the photographic style of reference images with the textual inversion\ntechnique, by optimizing the prompts for the photographic concepts. PIF shows\noutstanding performance in extracting and transferring various kinds of\nphotographic style. Project page: https://pif.pages.dev/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u56fe\u50cf\u6ee4\u955c\uff08PIF\uff09\u65b9\u6cd5\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u53cd\u6f14\u6280\u672f\u5b66\u4e60\u53c2\u8003\u56fe\u50cf\u7684\u6444\u5f71\u98ce\u683c\uff0c\u80fd\u591f\u6709\u6548\u63d0\u53d6\u548c\u8f6c\u79fb\u591a\u79cd\u6444\u5f71\u98ce\u683c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u53c2\u8003\u56fe\u50cf\u4e2d\u5b66\u4e60\u6709\u610f\u4e49\u7684\u6444\u5f71\u6982\u5ff5\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8981\u4e48\u65e0\u6cd5\u5b66\u4e60\u5230\u6709\u610f\u4e49\u7684\u6444\u5f71\u6982\u5ff5\uff0c\u8981\u4e48\u65e0\u6cd5\u4fdd\u6301\u5185\u5bb9\u56fe\u50cf\u7684\u5185\u5bb9\u5b8c\u6574\u6027\u3002\u6444\u5f71\u98ce\u683c\u4f5c\u4e3a\u7279\u5b9a\u6444\u5f71\u6982\u5ff5\u7684\u7ec4\u5408\uff0c\u662f\u8457\u540d\u6444\u5f71\u5e08\u4f5c\u54c1\u9b45\u529b\u7684\u5173\u952e\u6240\u5728\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u751f\u6210\u5148\u9a8c\u5b66\u4e60\u6444\u5f71\u6982\u5ff5\u7684\u5e73\u5747\u5916\u89c2\u4ee5\u53ca\u5982\u4f55\u6839\u636e\u6587\u672c\u63d0\u793a\u8c03\u6574\u8fd9\u4e9b\u6982\u5ff5\u3002\u901a\u8fc7\u6587\u672c\u53cd\u6f14\u6280\u672f\u4f18\u5316\u6444\u5f71\u6982\u5ff5\u7684\u63d0\u793a\u8bcd\u6765\u5b66\u4e60\u53c2\u8003\u56fe\u50cf\u7684\u6444\u5f71\u98ce\u683c\u3002", "result": "PIF\u5728\u63d0\u53d6\u548c\u8f6c\u79fb\u5404\u79cd\u6444\u5f71\u98ce\u683c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u6444\u5f71\u6982\u5ff5\u7684\u5e73\u5747\u5916\u89c2\u5e76\u6839\u636e\u6587\u672c\u63d0\u793a\u8fdb\u884c\u76f8\u5e94\u8c03\u6574\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u548c\u6587\u672c\u53cd\u6f14\u6280\u672f\uff0c\u4e3a\u6444\u5f71\u98ce\u683c\u5b66\u4e60\u548c\u8f6c\u79fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5728\u98ce\u683c\u63d0\u53d6\u548c\u8f6c\u79fb\u4efb\u52a1\u4e0a\u7684\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.16822", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16822", "abs": "https://arxiv.org/abs/2510.16822", "authors": ["Yahia Battach", "Abdulwahab Felemban", "Faizan Farooq Khan", "Yousef A. Radwan", "Xiang Li", "Fabio Marchese", "Sara Beery", "Burton H. Jones", "Francesca Benzoni", "Mohamed Elhoseiny"], "title": "ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification", "comment": null, "summary": "Coral reefs are rapidly declining due to anthropogenic pressures such as\nclimate change, underscoring the urgent need for scalable, automated\nmonitoring. We introduce ReefNet, a large public coral reef image dataset with\npoint-label annotations mapped to the World Register of Marine Species (WoRMS).\nReefNet aggregates imagery from 76 curated CoralNet sources and an additional\nsite from Al Wajh in the Red Sea, totaling approximately 925000 genus-level\nhard coral annotations with expert-verified labels. Unlike prior datasets,\nwhich are often limited by size, geography, or coarse labels and are not\nML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global\nscale to WoRMS. We propose two evaluation settings: (i) a within-source\nbenchmark that partitions each source's images for localized evaluation, and\n(ii) a cross-source benchmark that withholds entire sources to test domain\ngeneralization. We analyze both supervised and zero-shot classification\nperformance on ReefNet and find that while supervised within-source performance\nis promising, supervised performance drops sharply across domains, and\nperformance is low across the board for zero-shot models, especially for rare\nand visually similar genera. This provides a challenging benchmark intended to\ncatalyze advances in domain generalization and fine-grained coral\nclassification. We will release our dataset, benchmarking code, and pretrained\nmodels to advance robust, domain-adaptive, global coral reef monitoring and\nconservation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86ReefNet\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u516c\u5f00\u73ca\u745a\u7901\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea6925,000\u4e2a\u7ecf\u4e13\u5bb6\u9a8c\u8bc1\u7684\u5c5e\u7ea7\u786c\u73ca\u745a\u6807\u6ce8\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u89c4\u6a21\u3001\u5730\u7406\u8986\u76d6\u548c\u7cbe\u7ec6\u6807\u6ce8\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u73ca\u745a\u7901\u81ea\u52a8\u76d1\u6d4b\u63d0\u4f9b\u5177\u6709\u6311\u6218\u6027\u7684\u9886\u57df\u6cdb\u5316\u57fa\u51c6\u3002", "motivation": "\u73ca\u745a\u7901\u56e0\u6c14\u5019\u53d8\u5316\u7b49\u4eba\u7c7b\u538b\u529b\u800c\u8fc5\u901f\u8870\u9000\uff0c\u8feb\u5207\u9700\u8981\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u76d1\u6d4b\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u5f80\u5f80\u53d7\u9650\u4e8e\u89c4\u6a21\u3001\u5730\u7406\u8986\u76d6\u8303\u56f4\u6216\u7c97\u7c92\u5ea6\u6807\u6ce8\uff0c\u4e14\u4e0d\u7b26\u5408\u673a\u5668\u5b66\u4e60\u5c31\u7eea\u8981\u6c42\uff0c\u65e0\u6cd5\u652f\u6301\u5168\u7403\u5c3a\u5ea6\u7684\u7cbe\u7ec6\u73ca\u745a\u5206\u7c7b\u7814\u7a76\u3002", "method": "ReefNet\u6574\u5408\u4e86\u6765\u81ea76\u4e2aCoralNet\u6765\u6e90\u548c\u7ea2\u6d77Al Wajh\u7ad9\u70b9\u7684\u56fe\u50cf\u6570\u636e\uff0c\u63d0\u4f9b\u6620\u5c04\u81f3\u4e16\u754c\u6d77\u6d0b\u7269\u79cd\u540d\u5f55\u7684\u7cbe\u7ec6\u5206\u7c7b\u6807\u6ce8\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u8bc4\u4f30\u8bbe\u7f6e\uff1a\u6e90\u5185\u57fa\u51c6\u5c06\u5404\u6765\u6e90\u56fe\u50cf\u5206\u533a\u8fdb\u884c\u5c40\u90e8\u8bc4\u4f30\uff0c\u8de8\u6e90\u57fa\u51c6\u5219\u5b8c\u5168\u4fdd\u7559\u67d0\u4e9b\u6765\u6e90\u4ee5\u6d4b\u8bd5\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u5206\u6790\u663e\u793a\uff0c\u76d1\u7763\u5b66\u4e60\u5728\u6e90\u5185\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8de8\u57df\u60c5\u51b5\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u96f6\u6837\u672c\u6a21\u578b\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u8868\u73b0\u5747\u8f83\u5dee\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7a00\u6709\u548c\u89c6\u89c9\u76f8\u4f3c\u5c5e\u79cd\u7684\u5206\u7c7b\u6548\u679c\u4e0d\u4f73\uff0c\u8fd9\u4e3a\u9886\u57df\u6cdb\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u73ca\u745a\u7901\u76d1\u6d4b\u4e2d\u9886\u57df\u6cdb\u5316\u7684\u91cd\u8981\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u8de8\u57df\u8bc6\u522b\u548c\u7a00\u6709\u7269\u79cd\u5206\u7c7b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u3001\u57fa\u51c6\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u65e8\u5728\u63a8\u52a8\u9c81\u68d2\u3001\u9886\u57df\u81ea\u9002\u5e94\u7684\u5168\u7403\u73ca\u745a\u7901\u76d1\u6d4b\u4e0e\u4fdd\u62a4\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.16870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16870", "abs": "https://arxiv.org/abs/2510.16870", "authors": ["Yudan Ren", "Xinlong Wang", "Kexin Wang", "Tian Xia", "Zihan Ma", "Zhaowei Li", "Xiangrong Bi", "Xiao Li", "Xiaowei He"], "title": "Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding", "comment": "14 pages, 7 figures", "summary": "While brain-inspired artificial intelligence(AI) has demonstrated promising\nresults, current understanding of the parallels between artificial neural\nnetworks (ANNs) and human brain processing remains limited: (1) unimodal ANN\nstudies fail to capture the brain's inherent multimodal processing\ncapabilities, and (2) multimodal ANN research primarily focuses on high-level\nmodel outputs, neglecting the crucial role of individual neurons. To address\nthese limitations, we propose a novel neuron-level analysis framework that\ninvestigates the multimodal information processing mechanisms in\nvision-language models (VLMs) through the lens of human brain activity. Our\napproach uniquely combines fine-grained artificial neuron (AN) analysis with\nfMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP\nand METER. Our analysis reveals four key findings: (1) ANs successfully predict\nbiological neurons (BNs) activities across multiple functional networks\n(including language, vision, attention, and default mode), demonstrating shared\nrepresentational mechanisms; (2) Both ANs and BNs demonstrate functional\nredundancy through overlapping neural representations, mirroring the brain's\nfault-tolerant and collaborative information processing mechanisms; (3) ANs\nexhibit polarity patterns that parallel the BNs, with oppositely activated BNs\nshowing mirrored activation trends across VLM layers, reflecting the complexity\nand bidirectional nature of neural information processing; (4) The\narchitectures of CLIP and METER drive distinct BNs: CLIP's independent branches\nshow modality-specific specialization, whereas METER's cross-modal design\nyields unified cross-modal activation, highlighting the architecture's\ninfluence on ANN brain-like properties. These results provide compelling\nevidence for brain-like hierarchical processing in VLMs at the neuronal level.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u5de5\u795e\u7ecf\u5143\u5206\u6790\u548c\u57fa\u4e8efMRI\u7684\u4f53\u7d20\u7f16\u7801\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u5927\u8111\u5728\u591a\u6a21\u6001\u4fe1\u606f\u5904\u7406\u673a\u5236\u4e0a\u7684\u76f8\u4f3c\u6027\uff0c\u4e3a\u7406\u89e3\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4e0e\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u7684\u5bf9\u5e94\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc1\u636e\u3002", "motivation": "\u5f53\u524d\u5bf9\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4e0e\u4eba\u7c7b\u5927\u8111\u5904\u7406\u673a\u5236\u7684\u7406\u89e3\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u5355\u6a21\u6001ANN\u7814\u7a76\u65e0\u6cd5\u6355\u6349\u5927\u8111\u56fa\u6709\u7684\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b\uff0c\u800c\u591a\u6a21\u6001ANN\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9ad8\u5c42\u6a21\u578b\u8f93\u51fa\uff0c\u5ffd\u89c6\u4e86\u5355\u4e2a\u795e\u7ecf\u5143\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u795e\u7ecf\u5143\u7ea7\u522b\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7cbe\u7ec6\u7684\u4eba\u5de5\u795e\u7ecf\u5143\u5206\u6790\u548c\u57fa\u4e8efMRI\u7684\u4f53\u7d20\u7f16\u7801\uff0c\u5bf9\u4e24\u79cd\u67b6\u6784\u4e0d\u540c\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08CLIP\u548cMETER\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4eba\u5de5\u795e\u7ecf\u5143\u80fd\u591f\u6210\u529f\u9884\u6d4b\u591a\u4e2a\u529f\u80fd\u7f51\u7edc\u4e2d\u7684\u751f\u7269\u795e\u7ecf\u5143\u6d3b\u52a8\uff0c\u4e24\u8005\u90fd\u8868\u73b0\u51fa\u529f\u80fd\u5197\u4f59\u6027\uff0c\u4eba\u5de5\u795e\u7ecf\u5143\u5c55\u73b0\u51fa\u4e0e\u751f\u7269\u795e\u7ecf\u5143\u76f8\u4f3c\u7684\u6781\u6027\u6a21\u5f0f\uff0c\u5e76\u4e14\u4e0d\u540cVLM\u67b6\u6784\u9a71\u52a8\u4e0d\u540c\u7684\u751f\u7269\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7c7b\u8111\u5c42\u6b21\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u529b\u8bc1\u636e\uff0c\u63ed\u793a\u4e86\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4e0e\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u5728\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u5171\u4eab\u8868\u5f81\u673a\u5236\uff0c\u4e3a\u7406\u89e3AI\u7cfb\u7edf\u7684\u7c7b\u8111\u7279\u6027\u53ca\u5176\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u5bf9\u5e94\u5173\u7cfb\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.16888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16888", "abs": "https://arxiv.org/abs/2510.16888", "authors": ["Zongjian Li", "Zheyuan Liu", "Qihui Zhang", "Bin Lin", "Shenghai Yuan", "Zhiyuan Yan", "Yang Ye", "Wangbo Yu", "Yuwei Niu", "Li Yuan"], "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback", "comment": null, "summary": "Instruction-based image editing has achieved remarkable progress; however,\nmodels solely trained via supervised fine-tuning often overfit to annotated\npatterns, hindering their ability to explore and generalize beyond training\ndistributions. To this end, we introduce Edit-R1, a novel post-training\nframework for instruction-based image editing based on policy optimization.\nSpecifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a\nlikelihood-free policy optimization method consistent with the flow matching\nforward process, thereby enabling the use of higher-order samplers and more\nefficient training. Another key challenge here is the absence of a universal\nreward model, resulting from the diverse nature of editing instructions and\ntasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)\nas a unified, training-free reward model, leveraging its output logits to\nprovide fine-grained feedback. Furthermore, we carefully design a low-variance\ngroup filtering mechanism to reduce MLLM scoring noise and stabilize\noptimization. UniWorld-V2, trained with this framework, achieves\n\\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,\nscoring 4.49 and 7.83, respectively. Crucially, our framework is\nmodel-agnostic, delivering substantial performance gains when applied to\ndiverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its\nwide applicability. Code and models are publicly available at\nhttps://github.com/PKU-YuanGroup/UniWorld-V2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Edit-R1\uff0c\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u4f18\u5316\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7DiffusionNFT\u65b9\u6cd5\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u6807\u6ce8\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5176\u5728\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u7684\u63a2\u7d22\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u7684\u540e\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u91c7\u7528Diffusion Negative-aware Finetuning\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0e\u6d41\u5339\u914d\u524d\u5411\u8fc7\u7a0b\u4e00\u81f4\uff0c\u652f\u6301\u9ad8\u9636\u91c7\u6837\u5668\u548c\u9ad8\u6548\u8bad\u7ec3\uff1b\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u7edf\u4e00\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u8f93\u51falogits\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u53cd\u9988\uff1b\u8bbe\u8ba1\u4e86\u4f4e\u65b9\u5dee\u7ec4\u8fc7\u6ee4\u673a\u5236\u6765\u51cf\u5c11\u8bc4\u5206\u566a\u58f0\u5e76\u7a33\u5b9a\u4f18\u5316\u3002", "result": "UniWorld-V2\u5728ImgEdit\u548cGEdit-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u83b7\u5f974.49\u548c7.83\u7684\u5206\u6570\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff1b\u8be5\u6846\u67b6\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u5728Qwen-Image-Edit\u548cFLUX-Kontext\u7b49\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u4e0a\u5747\u80fd\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Edit-R1\u6846\u67b6\u901a\u8fc7\u7b56\u7565\u4f18\u5316\u548cMLLM\u5956\u52b1\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5176\u6a21\u578b\u65e0\u5173\u7279\u6027\u5c55\u793a\u4e86\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u4e3a\u540e\u8bad\u7ec3\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.16973", "categories": ["cs.CV", "cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.16973", "abs": "https://arxiv.org/abs/2510.16973", "authors": ["Praveenbalaji Rajendran", "Mojtaba Safari", "Wenfeng He", "Mingzhe Hu", "Shansong Wang", "Jun Zhou", "Xiaofeng Yang"], "title": "Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis", "comment": null, "summary": "Recent advancements in artificial intelligence (AI), particularly foundation\nmodels (FMs), have revolutionized medical image analysis, demonstrating strong\nzero- and few-shot performance across diverse medical imaging tasks, from\nsegmentation to report generation. Unlike traditional task-specific AI models,\nFMs leverage large corpora of labeled and unlabeled multimodal datasets to\nlearn generalized representations that can be adapted to various downstream\nclinical applications with minimal fine-tuning. However, despite the rapid\nproliferation of FM research in medical imaging, the field remains fragmented,\nlacking a unified synthesis that systematically maps the evolution of\narchitectures, training paradigms, and clinical applications across modalities.\nTo address this gap, this review article provides a comprehensive and\nstructured analysis of FMs in medical image analysis. We systematically\ncategorize studies into vision-only and vision-language FMs based on their\narchitectural foundations, training strategies, and downstream clinical tasks.\nAdditionally, a quantitative meta-analysis of the studies was conducted to\ncharacterize temporal trends in dataset utilization and application domains. We\nalso critically discuss persistent challenges, including domain adaptation,\nefficient fine-tuning, computational constraints, and interpretability along\nwith emerging solutions such as federated learning, knowledge distillation, and\nadvanced prompting. Finally, we identify key future research directions aimed\nat enhancing the robustness, explainability, and clinical integration of FMs,\nthereby accelerating their translation into real-world medical practice.", "AI": {"tldr": "\u672c\u6587\u5bf9\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u7cfb\u7edf\u6027\u5730\u5206\u7c7b\u4e86\u89c6\u89c9\u4e13\u7528\u548c\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u5206\u6790\u4e86\u5176\u67b6\u6784\u6f14\u8fdb\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u4e34\u5e8a\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u52a0\u901f\u4e34\u5e8a\u8f6c\u5316\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u8be5\u9886\u57df\u4ecd\u7f3a\u4e4f\u5bf9\u8de8\u6a21\u6001\u67b6\u6784\u6f14\u8fdb\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u4e34\u5e8a\u5e94\u7528\u7684\u7cfb\u7edf\u6027\u7efc\u5408\uff0c\u9700\u8981\u7edf\u4e00\u7684\u6846\u67b6\u6765\u6574\u5408\u73b0\u6709\u7814\u7a76\u6210\u679c\u5e76\u6307\u5bfc\u672a\u6765\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u5206\u7c7b\u65b9\u6cd5\u5c06\u7814\u7a76\u5206\u4e3a\u89c6\u89c9\u4e13\u7528\u548c\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u5206\u6790\u5176\u67b6\u6784\u57fa\u7840\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u4e0b\u6e38\u4e34\u5e8a\u4efb\u52a1\uff0c\u5e76\u8fdb\u884c\u5b9a\u91cf\u5143\u5206\u6790\u4ee5\u8868\u5f81\u6570\u636e\u96c6\u5229\u7528\u548c\u5e94\u7528\u9886\u57df\u7684\u65f6\u95f4\u8d8b\u52bf\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7efc\u8ff0\u548c\u5b9a\u91cf\u5206\u6790\u63ed\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u67b6\u6784\u6f14\u8fdb\u6a21\u5f0f\u3001\u8bad\u7ec3\u7b56\u7565\u53d1\u5c55\u8d8b\u52bf\u4ee5\u53ca\u4e34\u5e8a\u5e94\u7528\u5206\u5e03\u7279\u5f81\uff0c\u8bc6\u522b\u4e86\u9886\u57df\u9002\u5e94\u3001\u9ad8\u6548\u5fae\u8c03\u548c\u8ba1\u7b97\u7ea6\u675f\u7b49\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u9886\u57df\u9002\u5e94\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u96c6\u6210\u7b49\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5e94\u805a\u7126\u4e8e\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u4ee5\u52a0\u901f\u5b9e\u9645\u533b\u7597\u5e94\u7528\u3002"}}
{"id": "2510.16983", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16983", "abs": "https://arxiv.org/abs/2510.16983", "authors": ["Yuanzhi Zhu", "Eleftherios Tsonis", "Lucas Degeorge", "Vicky Kalogeiton"], "title": "One-step Diffusion Models with Bregman Density Ratio Matching", "comment": "work in progress", "summary": "Diffusion and flow models achieve high generative quality but remain\ncomputationally expensive due to slow multi-step sampling. Distillation methods\naccelerate them by training fast student generators, yet most existing\nobjectives lack a unified theoretical foundation. In this work, we propose\nDi-Bregman, a compact framework that formulates diffusion distillation as\nBregman divergence-based density-ratio matching. This convex-analytic view\nconnects several existing objectives through a common lens. Experiments on\nCIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves\nimproved one-step FID over reverse-KL distillation and maintains high visual\nfidelity compared to the teacher model. Our results highlight Bregman\ndensity-ratio matching as a practical and theoretically-grounded route toward\nefficient one-step diffusion generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Di-Bregman\u6846\u67b6\uff0c\u901a\u8fc7Bregman\u6563\u5ea6\u5bc6\u5ea6\u6bd4\u5339\u914d\u7edf\u4e00\u4e86\u6269\u6563\u6a21\u578b\u84b8\u998f\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e00\u6b65\u751f\u6210\uff0c\u5728CIFAR-10\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u53cd\u5411KL\u84b8\u998f\u7684FID\u6307\u6807\u3002", "motivation": "\u6269\u6563\u548c\u6d41\u6a21\u578b\u867d\u7136\u751f\u6210\u8d28\u91cf\u9ad8\uff0c\u4f46\u7531\u4e8e\u591a\u6b65\u91c7\u6837\u8fc7\u7a0b\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u6602\u8d35\uff0c\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u7cfb\u7edf\u5316\u7684\u84b8\u998f\u6846\u67b6\u6765\u52a0\u901f\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86Di-Bregman\u6846\u67b6\uff0c\u5c06\u6269\u6563\u84b8\u998f\u5efa\u6a21\u4e3a\u57fa\u4e8eBregman\u6563\u5ea6\u7684\u5bc6\u5ea6\u6bd4\u5339\u914d\u95ee\u9898\uff0c\u4ece\u51f8\u5206\u6790\u89c6\u89d2\u7edf\u4e00\u4e86\u591a\u79cd\u73b0\u6709\u76ee\u6807\u51fd\u6570\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0a\u7684\u5171\u540c\u57fa\u7840\u3002", "result": "\u5728CIFAR-10\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5b9e\u9a8c\u4e2d\uff0cDi-Bregman\u5728\u4e00\u6b65\u751f\u6210FID\u6307\u6807\u4e0a\u4f18\u4e8e\u53cd\u5411KL\u84b8\u998f\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u6559\u5e08\u6a21\u578b\u76f8\u5f53\u7684\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "Bregman\u5bc6\u5ea6\u6bd4\u5339\u914d\u4e3a\u9ad8\u6548\u4e00\u6b65\u6269\u6563\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u7406\u8bba\u57fa\u7840\u575a\u5b9e\u7684\u8def\u5f84\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u84b8\u998f\u76ee\u6807\uff0c\u4e3a\u672a\u6765\u6269\u6563\u6a21\u578b\u52a0\u901f\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u3002"}}
{"id": "2510.17045", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17045", "abs": "https://arxiv.org/abs/2510.17045", "authors": ["Deepak Sridhar", "Kartikeya Bhardwaj", "Jeya Pradha Jeyaraj", "Nuno Vasconcelos", "Ankita Nayak", "Harris Teague"], "title": "Video Reasoning without Training", "comment": null, "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faV-Reason\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684\u63a7\u5236\u5668\u4f18\u5316\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u9891\u63a8\u7406\u4e2d\u7684\u5fae\u63a2\u7d22\u548c\u5fae\u5229\u7528\u884c\u4e3a\uff0c\u65e0\u9700\u5f3a\u5316\u5b66\u4e60\u6216\u76d1\u7763\u5fae\u8c03\u5373\u53ef\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u89c6\u9891\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u63a5\u8fd1RL\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u89c6\u9891\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u5197\u957f\u7684\u601d\u7ef4\u94fe\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u7684\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\uff0c\u4e14\u63a8\u7406\u8fc7\u7a0b\u7684\u63a7\u5236\u673a\u5236\u975e\u5e38\u6709\u9650\u3002", "method": "\u5229\u7528\u6a21\u578b\u8f93\u51fa\u71b5\u4f5c\u4e3a\u4fe1\u53f7\uff0c\u53d1\u73b0\u9ad8\u8d28\u91cf\u6a21\u578b\u7ecf\u5386\u5fae\u63a2\u7d22\u548c\u5fae\u5229\u7528\u5e8f\u5217\u4ee5\u4fdd\u6301\u63a8\u7406\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51faV-Reason\u65b9\u6cd5\uff1a\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684\u76ee\u6807\u51fd\u6570\u5bf9\u5c0f\u578b\u53ef\u8bad\u7ec3\u63a7\u5236\u5668\u8fdb\u884c\u5c11\u91cf\u4f18\u5316\u6b65\u9aa4\uff0c\u81ea\u9002\u5e94\u8c03\u6574LMM\u7684\u503c\u7f13\u5b58\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u63a8\u7406\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7840\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u4e0eRL\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u7387\u5dee\u8ddd\u7f29\u5c0f\u81f30.6%\u4ee5\u5185\uff0c\u540c\u65f6\u8f93\u51fatoken\u51cf\u5c1158.6%\uff0c\u63d0\u4f9b\u5de8\u5927\u7684\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u901a\u8fc7\u71b5\u4fe1\u53f7\u76f4\u63a5\u4f18\u5316\u63a8\u7406\u884c\u4e3a\u53ef\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\u6216\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u9ad8\u6548\u89c6\u9891\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6307\u5bfc\u548c\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2510.17131", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17131", "abs": "https://arxiv.org/abs/2510.17131", "authors": ["Xin Gao", "Jiyao Liu", "Guanghao Li", "Yueming Lyu", "Jianxiong Gao", "Weichen Yu", "Ningsheng Xu", "Liang Wang", "Caifeng Shan", "Ziwei Liu", "Chenyang Si"], "title": "GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection", "comment": "28 pages, 16 figures, conference", "summary": "Recent advancements have explored text-to-image diffusion models for\nsynthesizing out-of-distribution (OOD) samples, substantially enhancing the\nperformance of OOD detection. However, existing approaches typically rely on\nperturbing text-conditioned embeddings, resulting in semantic instability and\ninsufficient shift diversity, which limit generalization to realistic OOD. To\naddress these challenges, we propose GOOD, a novel and flexible framework that\ndirectly guides diffusion sampling trajectories towards OOD regions using\noff-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level\nguidance: (1) Image-level guidance based on the gradient of log partition to\nreduce input likelihood, drives samples toward low-density regions in pixel\nspace. (2) Feature-level guidance, derived from k-NN distance in the\nclassifier's latent space, promotes sampling in feature-sparse regions. Hence,\nthis dual-guidance design enables more controllable and diverse OOD sample\ngeneration. Additionally, we introduce a unified OOD score that adaptively\ncombines image and feature discrepancies, enhancing detection robustness. We\nperform thorough quantitative and qualitative analyses to evaluate the\neffectiveness of GOOD, demonstrating that training with samples generated by\nGOOD can notably enhance OOD detection performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGOOD\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u73b0\u6210\u7684\u5206\u5e03\u5185\u5206\u7c7b\u5668\u76f4\u63a5\u5f15\u5bfc\u6269\u6563\u91c7\u6837\u8f68\u8ff9\u751f\u6210\u5206\u5e03\u5916\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8bed\u4e49\u4e0d\u7a33\u5b9a\u548c\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5e03\u5916\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u5206\u5e03\u5916\u6837\u672c\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6270\u52a8\u6587\u672c\u6761\u4ef6\u5d4c\u5165\uff0c\u5bfc\u81f4\u8bed\u4e49\u4e0d\u7a33\u5b9a\u548c\u504f\u79fb\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u5206\u5e03\u5916\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "GOOD\u6846\u67b6\u91c7\u7528\u53cc\u5c42\u7ea7\u5f15\u5bfc\u673a\u5236\uff1a\u56fe\u50cf\u7ea7\u5f15\u5bfc\u57fa\u4e8e\u5bf9\u6570\u5206\u5272\u68af\u5ea6\u964d\u4f4e\u8f93\u5165\u4f3c\u7136\uff0c\u9a71\u52a8\u6837\u672c\u5411\u50cf\u7d20\u7a7a\u95f4\u4f4e\u5bc6\u5ea6\u533a\u57df\u79fb\u52a8\uff1b\u7279\u5f81\u7ea7\u5f15\u5bfc\u57fa\u4e8e\u5206\u7c7b\u5668\u6f5c\u5728\u7a7a\u95f4\u4e2dk-NN\u8ddd\u79bb\uff0c\u4fc3\u8fdb\u5728\u7279\u5f81\u7a00\u758f\u533a\u57df\u91c7\u6837\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u53ef\u63a7\u548c\u591a\u6837\u5316\u7684\u5206\u5e03\u5916\u6837\u672c\u751f\u6210\u3002", "result": "\u5168\u9762\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u4f7f\u7528GOOD\u751f\u6210\u7684\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5206\u5e03\u5916\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u7edf\u4e00\u7684\u81ea\u9002\u5e94\u7ed3\u5408\u56fe\u50cf\u548c\u7279\u5f81\u5dee\u5f02\u7684\u5206\u5e03\u5916\u8bc4\u5206\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u68c0\u6d4b\u9c81\u68d2\u6027\u3002", "conclusion": "GOOD\u6846\u67b6\u901a\u8fc7\u76f4\u63a5\u5f15\u5bfc\u6269\u6563\u91c7\u6837\u8f68\u8ff9\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u4e3a\u5206\u5e03\u5916\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u548c\u53ef\u63a7\u7684\u6837\u672c\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u53cc\u5c42\u7ea7\u5f15\u5bfc\u8bbe\u8ba1\u786e\u4fdd\u4e86\u8bed\u4e49\u7a33\u5b9a\u6027\u548c\u591a\u6837\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.16989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16989", "abs": "https://arxiv.org/abs/2510.16989", "authors": ["Luca Zanella", "Massimiliano Mancini", "Yiming Wang", "Alessio Tonioni", "Elisa Ricci"], "title": "Training-free Online Video Step Grounding", "comment": "NeurIPS 2025. Project website at https://lucazanella.github.io/baglm/", "summary": "Given a task and a set of steps composing it, Video Step Grounding (VSG) aims\nto detect which steps are performed in a video. Standard approaches for this\ntask require a labeled training set (e.g., with step-level annotations or\nnarrations), which may be costly to collect. Moreover, they process the full\nvideo offline, limiting their applications for scenarios requiring online\ndecisions. Thus, in this work, we explore how to perform VSG online and without\ntraining. We achieve this by exploiting the zero-shot capabilities of recent\nLarge Multimodal Models (LMMs). In particular, we use LMMs to predict the step\nassociated with a restricted set of frames, without access to the whole video.\nWe show that this online strategy without task-specific tuning outperforms\noffline and training-based models. Motivated by this finding, we develop\nBayesian Grounding with Large Multimodal Models (BaGLM), further injecting\nknowledge of past frames into the LMM-based predictions. BaGLM exploits\nBayesian filtering principles, modeling step transitions via (i) a dependency\nmatrix extracted through large language models and (ii) an estimation of step\nprogress. Experiments on three datasets show superior performance of BaGLM over\nstate-of-the-art training-based offline methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5728\u7ebf\u6267\u884c\u7684\u89c6\u9891\u6b65\u9aa4\u5b9a\u4f4d\u65b9\u6cd5BaGLM\uff0c\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u6ee4\u6ce2\u6574\u5408\u5386\u53f2\u5e27\u4fe1\u606f\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u9700\u8981\u8bad\u7ec3\u7684\u4f20\u7edf\u79bb\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u89c6\u9891\u6b65\u9aa4\u5b9a\u4f4d\u65b9\u6cd5\u9700\u8981\u5e26\u6807\u6ce8\u7684\u8bad\u7ec3\u6570\u636e\u548c\u79bb\u7ebf\u5904\u7406\u5b8c\u6574\u89c6\u9891\uff0c\u8fd9\u5bfc\u81f4\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4e14\u65e0\u6cd5\u5e94\u7528\u4e8e\u9700\u8981\u5728\u7ebf\u51b3\u7b56\u7684\u573a\u666f\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u65e0\u9700\u8bad\u7ec3\u548c\u5728\u7ebf\u6267\u884c\u7684\u89c6\u9891\u6b65\u9aa4\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u57fa\u4e8e\u6709\u9650\u5e27\u96c6\u9884\u6d4b\u6b65\u9aa4\uff0c\u5e76\u5f00\u53d1\u4e86BaGLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u6ee4\u6ce2\u539f\u5219\u6574\u5408\u5386\u53f2\u5e27\u4fe1\u606f\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u7684\u4f9d\u8d56\u77e9\u9635\u548c\u6b65\u9aa4\u8fdb\u5ea6\u4f30\u8ba1\u6765\u5efa\u6a21\u6b65\u9aa4\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8c03\u4f18\u7684\u5728\u7ebf\u7b56\u7565\u4f18\u4e8e\u4f20\u7edf\u7684\u79bb\u7ebf\u548c\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u663e\u793a\uff0cBaGLM\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u8bad\u7ec3\u7684\u79bb\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\u80fd\u591f\u6709\u6548\u89e3\u51b3\u89c6\u9891\u6b65\u9aa4\u5b9a\u4f4d\u95ee\u9898\uff0c\u8d1d\u53f6\u65af\u6ee4\u6ce2\u6846\u67b6\u7684\u5f15\u5165\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.17157", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17157", "abs": "https://arxiv.org/abs/2510.17157", "authors": ["Yinghui Wang", "Xinyu Zhang", "Peng Du"], "title": "GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image", "comment": null, "summary": "Generating editable, parametric CAD models from a single image holds great\npotential to lower the barriers of industrial concept design. However, current\nmulti-modal large language models (MLLMs) still struggle with accurately\ninferring 3D geometry from 2D images due to limited spatial reasoning\ncapabilities. We address this limitation by introducing GACO-CAD, a novel\ntwo-stage post-training framework. It is designed to achieve a joint objective:\nsimultaneously improving the geometric accuracy of the generated CAD models and\nencouraging the use of more concise modeling procedures. First, during\nsupervised fine-tuning, we leverage depth and surface normal maps as dense\ngeometric priors, combining them with the RGB image to form a multi-channel\ninput. In the context of single-view reconstruction, these priors provide\ncomplementary spatial cues that help the MLLM more reliably recover 3D geometry\nfrom 2D observations. Second, during reinforcement learning, we introduce a\ngroup length reward that, while preserving high geometric fidelity, promotes\nthe generation of more compact and less redundant parametric modeling\nsequences. A simple dynamic weighting strategy is adopted to stabilize\ntraining. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD\nachieves state-of-the-art performance under the same MLLM backbone,\nconsistently outperforming existing methods in terms of code validity,\ngeometric accuracy, and modeling conciseness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGACO-CAD\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u548c\u8868\u9762\u6cd5\u7ebf\u56fe\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\uff0c\u5e76\u5f15\u5165\u7ec4\u957f\u5ea6\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u53ef\u7f16\u8f91\u53c2\u6570\u5316CAD\u6a21\u578b\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u5efa\u6a21\u7b80\u6d01\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ece2D\u56fe\u50cf\u51c6\u786e\u63a8\u65ad3D\u51e0\u4f55\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u8fd9\u963b\u788d\u4e86\u5de5\u4e1a\u6982\u5ff5\u8bbe\u8ba1\u4e2d\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u53ef\u7f16\u8f91\u53c2\u6570\u5316CAD\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6846\u67b6\uff1a\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u5229\u7528\u6df1\u5ea6\u548c\u8868\u9762\u6cd5\u7ebf\u56fe\u4f5c\u4e3a\u5bc6\u96c6\u51e0\u4f55\u5148\u9a8c\uff0c\u4e0eRGB\u56fe\u50cf\u5f62\u6210\u591a\u901a\u9053\u8f93\u5165\uff1b\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u5f15\u5165\u7ec4\u957f\u5ea6\u5956\u52b1\u673a\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u4fc3\u8fdb\u751f\u6210\u66f4\u7d27\u51d1\u7684\u5efa\u6a21\u5e8f\u5217\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u52a0\u6743\u7b56\u7565\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728DeepCAD\u548cFusion360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGACO-CAD\u5728\u76f8\u540cMLLM\u9aa8\u5e72\u7f51\u7edc\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u4ee3\u7801\u6709\u6548\u6027\u3001\u51e0\u4f55\u7cbe\u5ea6\u548c\u5efa\u6a21\u7b80\u6d01\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u7b80\u6d01\u6027\u5956\u52b1\u7684\u53cc\u91cd\u4f18\u5316\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347CAD\u6a21\u578b\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u5de5\u4e1a\u8bbe\u8ba1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u5728\u51e0\u4f55\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.17007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17007", "abs": "https://arxiv.org/abs/2510.17007", "authors": ["Ignacio M. De la Jara", "Cristian Rodriguez-Opazo", "Edison Marrese-Taylor", "Felipe Bravo-Marquez"], "title": "An empirical study of the effect of video encoders on Temporal Video Grounding", "comment": null, "summary": "Temporal video grounding is a fundamental task in computer vision, aiming to\nlocalize a natural language query in a long, untrimmed video. It has a key role\nin the scientific community, in part due to the large amount of video generated\nevery day. Although we find extensive work in this task, we note that research\nremains focused on a small selection of video representations, which may lead\nto architectural overfitting in the long run. To address this issue, we propose\nan empirical study to investigate the impact of different video features on a\nclassical architecture. We extract features for three well-known benchmarks,\nCharades-STA, ActivityNet-Captions and YouCookII, using video encoders based on\nCNNs, temporal reasoning and transformers. Our results show significant\ndifferences in the performance of our model by simply changing the video\nencoder, while also revealing clear patterns and errors derived from the use of\ncertain features, ultimately indicating potential feature complementarity.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u89c6\u9891\u7279\u5f81\u5bf9\u65f6\u5e8f\u89c6\u9891\u5b9a\u4f4d\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u7ecf\u5178\u67b6\u6784\u4e2d\u4ec5\u6539\u53d8\u89c6\u9891\u7f16\u7801\u5668\u5373\u53ef\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4e0d\u540c\u7279\u5f81\u4e4b\u95f4\u7684\u6f5c\u5728\u4e92\u8865\u6027\u3002", "motivation": "\u5f53\u524d\u65f6\u5e8f\u89c6\u9891\u5b9a\u4f4d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5c11\u6570\u51e0\u79cd\u89c6\u9891\u8868\u793a\u65b9\u6cd5\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u957f\u671f\u7684\u67b6\u6784\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u89c6\u9891\u7279\u5f81\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u57fa\u4e8eCNN\u3001\u65f6\u5e8f\u63a8\u7406\u548cTransformer\u7684\u89c6\u9891\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5728Charades-STA\u3001ActivityNet-Captions\u548cYouCookII\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u91c7\u7528\u7ecf\u5178\u67b6\u6784\u8bc4\u4f30\u4e0d\u540c\u89c6\u9891\u7279\u5f81\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u6539\u53d8\u89c6\u9891\u7f16\u7801\u5668\u5c31\u80fd\u5728\u6a21\u578b\u6027\u80fd\u4e0a\u4ea7\u751f\u663e\u8457\u5dee\u5f02\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4f7f\u7528\u7279\u5b9a\u7279\u5f81\u65f6\u4ea7\u751f\u7684\u660e\u663e\u6a21\u5f0f\u548c\u9519\u8bef\uff0c\u8868\u660e\u4e0d\u540c\u7279\u5f81\u4e4b\u95f4\u5b58\u5728\u6f5c\u5728\u7684\u4e92\u8865\u6027\u3002", "conclusion": "\u4e0d\u540c\u89c6\u9891\u7279\u5f81\u5bf9\u65f6\u5e8f\u89c6\u9891\u5b9a\u4f4d\u4efb\u52a1\u5177\u6709\u663e\u8457\u5f71\u54cd\uff0c\u7279\u5f81\u9009\u62e9\u4e0d\u5e94\u5c40\u9650\u4e8e\u5c11\u6570\u51e0\u79cd\u8868\u793a\u65b9\u6cd5\uff0c\u591a\u79cd\u7279\u5f81\u7684\u7ec4\u5408\u53ef\u80fd\u5e26\u6765\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8fd9\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bbe\u8ba1\u6307\u5bfc\u3002"}}
{"id": "2510.17197", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17197", "abs": "https://arxiv.org/abs/2510.17197", "authors": ["Pu Zhang", "Yuwei Li", "Xingyuan Xian", "Guoming Tang"], "title": "ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models", "comment": null, "summary": "As the capabilities of Vision-Language Models (VLMs) advance, they can\nprocess increasingly large inputs, which, unlike in LLMs, generates significant\nvisual token redundancy and leads to prohibitive inference costs. While many\nmethods aim to reduce these costs by pruning visual tokens, existing\napproaches, whether based on attention or diversity, typically neglect the\nguidance of the text prompt and thus fail to prioritize task relevance. In this\nwork, we propose a novel, zero-shot method that reframes the problem by\nintroducing a prompt-aware perspective, explicitly modeling visual token\npruning as a balance between task relevance and information diversity. Our\nhierarchical approach first selects a core set of task-relevant visual tokens\nand then supplements them with diversity tokens to preserve broader context.\nExperiments across multiple models and benchmarks show that our method achieves\nperformance that matches or surpasses the state-of-the-art with only minimal\naccuracy loss, even when pruning up to 90\\% of the tokens. Furthermore, these\ngains are accompanied by significant reductions in GPU memory footprint and\ninference latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u7684\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u63d0\u793a\u611f\u77e5\u89c6\u89d2\u5728\u4efb\u52a1\u76f8\u5173\u6027\u548c\u4fe1\u606f\u591a\u6837\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u80fd\u591f\u5728\u526a\u9664\u9ad8\u8fbe90%\u89c6\u89c9\u4ee4\u724c\u7684\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5904\u7406\u5927\u89c4\u6a21\u8f93\u5165\u65f6\u4f1a\u4ea7\u751f\u663e\u8457\u7684\u89c6\u89c9\u4ee4\u724c\u5197\u4f59\uff0c\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u8fc7\u9ad8\u3002\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u6216\u591a\u6837\u6027\u7684\u526a\u679d\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u6587\u672c\u63d0\u793a\u7684\u6307\u5bfc\uff0c\u65e0\u6cd5\u6709\u6548\u4f18\u5148\u8003\u8651\u4efb\u52a1\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u9996\u5148\u9009\u62e9\u4efb\u52a1\u76f8\u5173\u7684\u6838\u5fc3\u89c6\u89c9\u4ee4\u724c\u96c6\u5408\uff0c\u7136\u540e\u8865\u5145\u591a\u6837\u6027\u4ee4\u724c\u4ee5\u4fdd\u7559\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u660e\u786e\u5efa\u6a21\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u5728\u4efb\u52a1\u76f8\u5173\u6027\u548c\u4fe1\u606f\u591a\u6837\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u526a\u9664\u9ad8\u8fbe90%\u4ee4\u724c\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u4ec5\u5e26\u6765\u6700\u5c0f\u7cbe\u5ea6\u635f\u5931\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86GPU\u5185\u5b58\u5360\u7528\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u63d0\u793a\u611f\u77e5\u7684\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u4e3a\u964d\u4f4e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6210\u672c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u5728\u526a\u679d\u8fc7\u7a0b\u4e2d\u5e73\u8861\u4efb\u52a1\u76f8\u5173\u6027\u548c\u4fe1\u606f\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.17023", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.17023", "abs": "https://arxiv.org/abs/2510.17023", "authors": ["Shraman Pramanick", "Effrosyni Mavroudi", "Yale Song", "Rama Chellappa", "Lorenzo Torresani", "Triantafyllos Afouras"], "title": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs", "comment": "ICCV 2025 (Highlights)", "summary": "We introduce ED-VTG, a method for fine-grained video temporal grounding\nutilizing multi-modal large language models. Our approach harnesses the\ncapabilities of multimodal LLMs to jointly process text and video, in order to\neffectively localize natural language queries in videos through a two-stage\nprocess. Rather than being directly grounded, language queries are initially\ntransformed into enriched sentences that incorporate missing details and cues\nto aid in grounding. In the second stage, these enriched queries are grounded,\nusing a lightweight decoder, which specializes at predicting accurate\nboundaries conditioned on contextualized representations of the enriched\nqueries. To mitigate noise and reduce the impact of hallucinations, our model\nis trained with a multiple-instance-learning objective that dynamically selects\nthe optimal version of the query for each training sample. We demonstrate\nstate-of-the-art results across various benchmarks in temporal video grounding\nand paragraph grounding settings. Experiments reveal that our method\nsignificantly outperforms all previously proposed LLM-based temporal grounding\napproaches and is either superior or comparable to specialized models, while\nmaintaining a clear advantage against them in zero-shot evaluation scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ED-VTG\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5904\u7406\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u5316\u4e3a\u5bcc\u542b\u7ec6\u8282\u7684\u589e\u5f3a\u53e5\u5b50\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u5b9e\u73b0\u7cbe\u786e\u8fb9\u754c\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u65b9\u6cd5\u5728\u5904\u7406\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u65f6\u9762\u4e34\u7ec6\u8282\u7f3a\u5931\u548c\u5e7b\u89c9\u566a\u58f0\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u5730\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u6765\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5904\u7406\u6d41\u7a0b\uff1a\u9996\u5148\u5c06\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u5305\u542b\u7f3a\u5931\u7ec6\u8282\u7684\u589e\u5f3a\u53e5\u5b50\uff0c\u7136\u540e\u4f7f\u7528\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u57fa\u4e8e\u589e\u5f3a\u67e5\u8be2\u7684\u4e0a\u4e0b\u6587\u8868\u793a\u9884\u6d4b\u51c6\u786e\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7\u591a\u5b9e\u4f8b\u5b66\u4e60\u76ee\u6807\u52a8\u6001\u9009\u62e9\u6700\u4f18\u67e5\u8be2\u7248\u672c\u4ee5\u51cf\u8f7b\u566a\u58f0\u5f71\u54cd\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u548c\u6bb5\u843d\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u6240\u6709\u5148\u524d\u63d0\u51fa\u7684\u57fa\u4e8eLLM\u7684\u65f6\u5e8f\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u8bc4\u4f30\u573a\u666f\u4e2d\u76f8\u5bf9\u4e8e\u4e13\u7528\u6a21\u578b\u4fdd\u6301\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u591a\u6a21\u6001LLM\u5728\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u67e5\u8be2\u589e\u5f3a\u548c\u52a8\u6001\u9009\u62e9\u673a\u5236\u6709\u6548\u7f13\u89e3\u4e86\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.17218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17218", "abs": "https://arxiv.org/abs/2510.17218", "authors": ["Zhuo Cao", "Heming Du", "Bingqing Zhang", "Xin Yu", "Xue Li", "Sen Wang"], "title": "When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions", "comment": "Accepted to NeurIPS 2025", "summary": "Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval\n(SMR). However, one query can correspond to multiple relevant moments in\nreal-world applications. This makes the existing datasets and methods\ninsufficient for video temporal grounding. By revisiting the gap between\ncurrent MR tasks and real-world applications, we introduce a high-quality\ndatasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new\nevaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists\nof 2,212 annotations covering 6,384 video segments. Building on existing\nefforts in MMR, we propose a framework called FlashMMR. Specifically, we\npropose a Multi-moment Post-verification module to refine the moment\nboundaries. We introduce constrained temporal adjustment and subsequently\nleverage a verification module to re-evaluate the candidate segments. Through\nthis sophisticated filtering pipeline, low-confidence proposals are pruned, and\nrobust multi-moment alignment is achieved. We retrain and evaluate 6 existing\nMR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.\nResults show that QV-M$^2$ serves as an effective benchmark for training and\nevaluating MMR models, while FlashMMR provides a strong baseline. Specifically,\non QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,\n2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method\nestablish a foundation for advancing research in more realistic and challenging\nvideo temporal grounding scenarios. Code is released at\nhttps://github.com/Zhuo-Cao/QV-M2.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86QV-M\u00b2\u591a\u65f6\u523b\u68c0\u7d22\u6570\u636e\u96c6\u548cFlashMMR\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u65f6\u523b\u68c0\u7d22\u7684\u95ee\u9898\uff0c\u5728QV-M\u00b2\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u5148\u524d\u6700\u4f18\u65b9\u6cd5\u5b9e\u73b0\u4e863.00%\u7684G-mAP\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65f6\u523b\u68c0\u7d22\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u65f6\u523b\u68c0\u7d22\uff0c\u4f46\u73b0\u5b9e\u5e94\u7528\u4e2d\u4e00\u4e2a\u67e5\u8be2\u53ef\u80fd\u5bf9\u5e94\u591a\u4e2a\u76f8\u5173\u65f6\u523b\uff0c\u8fd9\u4f7f\u5f97\u73b0\u6709\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u5728\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86FlashMMR\u6846\u67b6\uff0c\u5305\u542b\u591a\u65f6\u523b\u540e\u9a8c\u8bc1\u6a21\u5757\u6765\u7cbe\u70bc\u65f6\u523b\u8fb9\u754c\uff0c\u901a\u8fc7\u7ea6\u675f\u6027\u65f6\u5e8f\u8c03\u6574\u548c\u9a8c\u8bc1\u6a21\u5757\u91cd\u65b0\u8bc4\u4f30\u5019\u9009\u7247\u6bb5\uff0c\u901a\u8fc7\u7cbe\u7ec6\u7684\u8fc7\u6ee4\u7ba1\u9053\u4fee\u526a\u4f4e\u7f6e\u4fe1\u5ea6\u63d0\u8bae\u5e76\u5b9e\u73b0\u9c81\u68d2\u7684\u591a\u65f6\u523b\u5bf9\u9f50\u3002", "result": "\u5728QV-M\u00b2\u6570\u636e\u96c6\u4e0a\uff0cFlashMMR\u76f8\u6bd4\u5148\u524d\u6700\u4f18\u65b9\u6cd5\u5728G-mAP\u4e0a\u63d0\u53473.00%\uff0c\u5728mAP@3+tgt\u4e0a\u63d0\u53472.70%\uff0c\u5728mR@3\u4e0a\u63d0\u53472.56%\u3002QV-M\u00b2\u5305\u542b2,212\u4e2a\u6807\u6ce8\u548c6,384\u4e2a\u89c6\u9891\u7247\u6bb5\u3002", "conclusion": "QV-M\u00b2\u6570\u636e\u96c6\u548cFlashMMR\u65b9\u6cd5\u4e3a\u63a8\u8fdb\u66f4\u73b0\u5b9e\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u573a\u666f\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u5728MMR\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u5e73\u53f0\u3002"}}
{"id": "2510.17034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17034", "abs": "https://arxiv.org/abs/2510.17034", "authors": ["Yutong Zhong"], "title": "Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding", "comment": null, "summary": "Multimodal 3D grounding has garnered considerable interest in Vision-Language\nModels (VLMs) \\cite{yin2025spatial} for advancing spatial reasoning in complex\nenvironments. However, these models suffer from a severe \"2D semantic bias\"\nthat arises from over-reliance on 2D image features for coarse localization,\nlargely disregarding 3D geometric inputs and resulting in suboptimal fusion\nperformance. In this paper, we propose a novel training framework called\nWhat-Where Representation Re-Forming (W2R2) to tackle this issue via\ndisentangled representation learning and targeted shortcut suppression. Our\napproach fundamentally reshapes the model's internal space by designating 2D\nfeatures as semantic beacons for \"What\" identification and 3D features as\nspatial anchors for \"Where\" localization, enabling precise 3D grounding without\nmodifying inference architecture. Key components include a dual-objective loss\nfunction with an Alignment Loss that supervises fused predictions using adapted\ncross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes\noverly effective 2D-dominant pseudo-outputs via a margin-based mechanism.\nExperiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of\nW2R2, with significant gains in localization accuracy and robustness,\nparticularly in cluttered outdoor scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faW2R2\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8868\u5f81\u5b66\u4e60\u548c\u9488\u5bf9\u6027\u6377\u5f84\u6291\u5236\u6765\u89e3\u51b3\u591a\u6a21\u60013D\u5b9a\u4f4d\u4e2d\u76842D\u8bed\u4e49\u504f\u5dee\u95ee\u9898\uff0c\u5728\u4e0d\u4fee\u6539\u63a8\u7406\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u53473D\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u591a\u6a21\u60013D\u5b9a\u4f4d\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u76842D\u8bed\u4e49\u504f\u5dee\u95ee\u9898\uff0c\u5373\u8fc7\u5ea6\u4f9d\u8d562D\u56fe\u50cf\u7279\u5f81\u8fdb\u884c\u7c97\u7565\u5b9a\u4f4d\u800c\u5ffd\u89c63D\u51e0\u4f55\u8f93\u5165\uff0c\u5bfc\u81f4\u878d\u5408\u6027\u80fd\u6b20\u4f73\u3002", "method": "\u63d0\u51faWhat-Where\u8868\u5f81\u91cd\u6784\u6846\u67b6\uff0c\u5c062D\u7279\u5f81\u4f5c\u4e3a\u8bed\u4e49\u4fe1\u6807\u7528\u4e8e\u7269\u4f53\u8bc6\u522b\uff0c3D\u7279\u5f81\u4f5c\u4e3a\u7a7a\u95f4\u951a\u70b9\u7528\u4e8e\u5b9a\u4f4d\uff0c\u91c7\u7528\u53cc\u76ee\u6807\u635f\u5931\u51fd\u6570\u5305\u62ec\u76d1\u7763\u878d\u5408\u9884\u6d4b\u7684\u5bf9\u9f50\u635f\u5931\u548c\u60e9\u7f5a2D\u4e3b\u5bfc\u4f2a\u8f93\u51fa\u7684\u8fb9\u754c\u673a\u5236\u4f2a\u6807\u7b7e\u635f\u5931\u3002", "result": "\u5728ScanRefer\u548cScanQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cW2R2\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u5ba4\u5916\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u89e3\u8026\u8868\u5f81\u5b66\u4e60\u548c\u9488\u5bf9\u6027\u6377\u5f84\u6291\u5236\u53ef\u6709\u6548\u7f13\u89e32D\u8bed\u4e49\u504f\u5dee\uff0c\u4e3a\u591a\u6a21\u60013D\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17269", "abs": "https://arxiv.org/abs/2510.17269", "authors": ["Luis Wiedmann", "Orr Zohar", "Amir Mahla", "Xiaohan Wang", "Rui Li", "Thibaud Frere", "Leandro von Werra", "Aritra Roy Gosthipaty", "Andr\u00e9s Marafioti"], "title": "FineVision: Open Data Is All You Need", "comment": null, "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FineVision\uff0c\u4e00\u4e2a\u7ecf\u8fc7\u7cbe\u5fc3\u6536\u96c6\u3001\u6574\u7406\u548c\u7edf\u4e00\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5305\u542b2400\u4e07\u4e2a\u6837\u672c\uff0c\u662f\u76ee\u524d\u540c\u7c7b\u4e2d\u6700\u5927\u7684\u5f00\u653e\u8d44\u6e90\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u4eba\u5de5\u53c2\u4e0e\u6d41\u7a0b\u6574\u5408\u4e86200\u591a\u4e2a\u6765\u6e90\uff0c\u5e76\u7ecf\u8fc7\u4e25\u683c\u53bb\u91cd\u548c\u53bb\u6c61\u67d3\u5904\u7406\uff0c\u5728\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u653e\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u53d7\u5230\u788e\u7247\u5316\u3001\u4e0d\u4e00\u81f4\u548c\u53d7\u6c61\u67d3\u516c\u5171\u6570\u636e\u96c6\u7684\u963b\u788d\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u548c\u4e25\u683c\u7684\u8d28\u91cf\u63a7\u5236\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u548c\u53ef\u9760\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u534a\u81ea\u52a8\u5316\u4eba\u5de5\u53c2\u4e0e\u6d41\u7a0b\uff0c\u81ea\u52a8\u5316\u5904\u7406\u6279\u91cf\u6570\u636e\u6444\u5165\u548c\u6a21\u5f0f\u6620\u5c04\uff0c\u540c\u65f6\u7531\u5ba1\u6838\u4eba\u5458\u5ba1\u67e5\u6620\u5c04\u7ed3\u679c\u5e76\u62bd\u6837\u68c0\u67e5\u8f93\u51fa\u8d28\u91cf\uff0c\u786e\u4fdd\u6ce8\u91ca\u7684\u5fe0\u5b9e\u8f6c\u6362\u3001\u683c\u5f0f\u9002\u5f53\u6027\u3001\u591a\u6837\u6027\u548c\u5b89\u5168\u6027\uff1b\u8be5\u6d41\u7a0b\u8fd8\u5e94\u7528\u4e86\u4e25\u683c\u7684\u5185\u90e8\u548c\u8de8\u6e90\u53bb\u91cd\uff0c\u4ee5\u53ca\u9488\u5bf966\u4e2a\u516c\u5171\u57fa\u51c6\u7684\u53bb\u6c61\u67d3\u5904\u7406\u3002", "result": "\u5728FineVision\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5e7f\u6cdb\u7684\u8bc4\u4f30\u5957\u4ef6\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u73b0\u6709\u5f00\u653e\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u96c6\u89c4\u6a21\u3001\u6570\u636e\u536b\u751f\u4ee5\u53ca\u5e73\u8861\u81ea\u52a8\u5316\u4e0e\u4eba\u5de5\u76d1\u7763\u7684\u7efc\u5408\u6548\u76ca\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u91cd\u8981\u6027\uff0c\u5c55\u793a\u4e86\u534a\u81ea\u52a8\u5316\u4eba\u5de5\u53c2\u4e0e\u6d41\u7a0b\u5728\u6570\u636e\u96c6\u6784\u5efa\u4e2d\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u53d1\u5e03\u7684\u8bed\u6599\u5e93\u548c\u6574\u7406\u5de5\u5177\u5c06\u52a0\u901f\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2510.17051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17051", "abs": "https://arxiv.org/abs/2510.17051", "authors": ["Masoud Khairi Atani", "Alon Harell", "Hyomin Choi", "Runyu Yang", "Fabien Racape", "Ivan V. Bajic"], "title": "How Universal Are SAM2 Features?", "comment": "This work has been accepted for publication in IEEE Picture Coding\n  Symposium (PCS) 2025", "summary": "The trade-off between general-purpose foundation vision models and their\nspecialized counterparts is critical for efficient feature coding design and is\nnot yet fully understood. We investigate this trade-off by comparing the\nfeature versatility of the general-purpose Hiera encoder against the\nsegmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,\ntrainable neck to probe the adaptability of their frozen features, we quantify\nthe information-theoretic cost of specialization. Our results reveal that while\nSAM2's specialization is highly effective for spatially-related tasks like\ndepth estimation, it comes at a cost. The specialized SAM2 encoder\nunderperforms its generalist predecessor, Hiera, on conceptually distant tasks\nsuch as pose estimation and image captioning, demonstrating a measurable loss\nof broader semantic information. A novel cross-neck analysis on SAM2 reveals\nthat each level of adaptation creates a further representational bottleneck.\nOur analysis illuminates these trade-offs in feature universality, providing a\nquantitative foundation for designing efficient feature coding and adaptation\nstrategies for diverse downstream applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578bHiera\u4e0e\u4e13\u7528\u5206\u5272\u6a21\u578bSAM2\u7684\u7279\u5f81\u901a\u7528\u6027\uff0c\u91cf\u5316\u4e86\u6a21\u578b\u4e13\u4e1a\u5316\u5e26\u6765\u7684\u4fe1\u606f\u8bba\u4ee3\u4ef7\uff0c\u63ed\u793a\u4e86\u4e13\u7528\u6a21\u578b\u5728\u7a7a\u95f4\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u4f18\u52bf\u4e0e\u8bed\u4e49\u4fe1\u606f\u635f\u5931\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5c1a\u672a\u5145\u5206\u7406\u89e3\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u5176\u4e13\u7528\u5bf9\u5e94\u6a21\u578b\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u8fd9\u5bf9\u4e8e\u9ad8\u6548\u7279\u5f81\u7f16\u7801\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u91cf\u5316\u4e13\u4e1a\u5316\u5e26\u6765\u7684\u4fe1\u606f\u8bba\u4ee3\u4ef7\u548c\u7279\u5f81\u901a\u7528\u6027\u635f\u5931\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u9888\u90e8\u7f51\u7edc\u6765\u63a2\u6d4b\u51bb\u7ed3\u7279\u5f81\u7684\u53ef\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u65b9\u6cd5\u91cf\u5316\u4e13\u4e1a\u5316\u6210\u672c\uff0c\u5e76\u5bf9SAM2\u8fdb\u884c\u8de8\u9888\u90e8\u5206\u6790\u4ee5\u63ed\u793a\u4e0d\u540c\u5c42\u7ea7\u9002\u5e94\u7684\u8868\u5f81\u74f6\u9888\u6548\u5e94\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aSAM2\u5728\u6df1\u5ea6\u4f30\u8ba1\u7b49\u7a7a\u95f4\u76f8\u5173\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u59ff\u6001\u4f30\u8ba1\u548c\u56fe\u50cf\u63cf\u8ff0\u7b49\u6982\u5ff5\u8ddd\u79bb\u8f83\u8fdc\u7684\u4efb\u52a1\u4e0a\u663e\u8457\u843d\u540e\u4e8e\u901a\u7528\u6a21\u578bHiera\uff0c\u8868\u660e\u4e13\u7528\u7f16\u7801\u5668\u5b58\u5728\u66f4\u5e7f\u6cdb\u7684\u8bed\u4e49\u4fe1\u606f\u635f\u5931\uff0c\u4e14\u6bcf\u4e2a\u9002\u5e94\u5c42\u7ea7\u90fd\u4f1a\u4ea7\u751f\u989d\u5916\u7684\u8868\u5f81\u74f6\u9888\u3002", "conclusion": "\u7814\u7a76\u9610\u660e\u4e86\u7279\u5f81\u901a\u7528\u6027\u4e0e\u4e13\u4e1a\u5316\u4e4b\u95f4\u7684\u91cf\u5316\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u9488\u5bf9\u4e0d\u540c\u4e0b\u6e38\u5e94\u7528\u8bbe\u8ba1\u9ad8\u6548\u7279\u5f81\u7f16\u7801\u548c\u9002\u5e94\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u5728\u6a21\u578b\u8bbe\u8ba1\u4e2d\u5e73\u8861\u4e13\u4e1a\u5316\u4e0e\u901a\u7528\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.17078", "categories": ["cs.CV", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2510.17078", "abs": "https://arxiv.org/abs/2510.17078", "authors": ["Jad Berjawi", "Yoann Dupas", "Christophe C'erin"], "title": "Towards a Generalizable Fusion Architecture for Multimodal Object Detection", "comment": "8 pages, 8 figures, accepted at ICCV 2025 MIRA Workshop", "summary": "Multimodal object detection improves robustness in chal- lenging conditions\nby leveraging complementary cues from multiple sensor modalities. We introduce\nFiltered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing\narchitecture designed to enhance the fusion of RGB and infrared (IR) inputs.\nFMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress\nredun- dant spectral features with a cross-attention-based fusion module (MCAF)\nto improve intermodal feature sharing. Unlike approaches tailored to specific\ndatasets, FMCAF aims for generalizability, improving performance across\ndifferent multimodal challenges without requiring dataset- specific tuning. On\nLLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),\nFMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50\non VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a\nflexible foundation for robust multimodal fusion in future detection pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FMCAF\uff08\u6ee4\u6ce2\u591a\u6a21\u6001\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u9891\u57df\u6ee4\u6ce2\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3aRGB\u4e0e\u7ea2\u5916\u56fe\u50cf\u7684\u7279\u5f81\u878d\u5408\uff0c\u5728\u591a\u79cd\u591a\u6a21\u6001\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u65e0\u9700\u7279\u5b9a\u6570\u636e\u96c6\u8c03\u4f18\u3002", "motivation": "\u591a\u6a21\u6001\u76ee\u6807\u68c0\u6d4b\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u901a\u8fc7\u878d\u5408\u591a\u79cd\u4f20\u611f\u5668\u6a21\u6001\u7684\u4e92\u8865\u4fe1\u606f\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u96c6\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u7684\u9884\u5904\u7406\u67b6\u6784\uff0c\u80fd\u591f\u6709\u6548\u878d\u5408RGB\u548c\u7ea2\u5916\u8f93\u5165\uff0c\u63d0\u5347\u591a\u6a21\u6001\u68c0\u6d4b\u6027\u80fd\u800c\u4e0d\u9700\u8981\u6570\u636e\u96c6\u7279\u5b9a\u7684\u8c03\u4f18\u3002", "method": "FMCAF\u67b6\u6784\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u9891\u57df\u6ee4\u6ce2\u6a21\u5757\uff08Freq-Filter\uff09\u7528\u4e8e\u6291\u5236\u5197\u4f59\u5149\u8c31\u7279\u5f81\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u878d\u5408\u6a21\u5757\uff08MCAF\uff09\u7528\u4e8e\u6539\u5584\u6a21\u6001\u95f4\u7279\u5f81\u5171\u4eab\u3002\u8be5\u67b6\u6784\u4f5c\u4e3a\u9884\u5904\u7406\u6a21\u5757\u8bbe\u8ba1\uff0c\u53ef\u4e0e\u73b0\u6709\u68c0\u6d4b\u5668\u7ed3\u5408\u4f7f\u7528\uff0c\u4e13\u6ce8\u4e8e\u589e\u5f3a\u8f93\u5165\u7279\u5f81\u7684\u8d28\u91cf\u548c\u4e92\u8865\u6027\u3002", "result": "\u5728LLVIP\uff08\u4f4e\u5149\u884c\u4eba\u68c0\u6d4b\uff09\u548cVEDAI\uff08\u822a\u7a7a\u8f66\u8f86\u68c0\u6d4b\uff09\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFMCAF\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u62fc\u63a5\u878d\u5408\u65b9\u6cd5\uff0c\u5728VEDAI\u4e0a\u5b9e\u73b0\u4e86+13.9%\u7684mAP@50\u63d0\u5347\uff0c\u5728LLVIP\u4e0a\u5b9e\u73b0\u4e86+1.1%\u7684\u63d0\u5347\u3002\u8fd9\u4e9b\u7ed3\u679c\u9a8c\u8bc1\u4e86FMCAF\u5728\u4e0d\u540c\u591a\u6a21\u6001\u6311\u6218\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "FMCAF\u5c55\u793a\u4e86\u4f5c\u4e3a\u901a\u7528\u591a\u6a21\u6001\u878d\u5408\u57fa\u7840\u67b6\u6784\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u68c0\u6d4b\u4efb\u52a1\u800c\u65e0\u9700\u7279\u5b9a\u8c03\u4f18\u3002\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u591a\u6a21\u6001\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u57fa\u7840\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u4f20\u611f\u5668\u6a21\u6001\u7ec4\u5408\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u611f\u77e5\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.17114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17114", "abs": "https://arxiv.org/abs/2510.17114", "authors": ["Hodaka Kawachi", "Tomoya Nakamura", "Hiroaki Santo", "SaiKiran Kumar Tedla", "Trevor Dalton Canham", "Yasushi Yagi", "Michael S. Brown"], "title": "Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras", "comment": null, "summary": "This paper introduces a method for using LED-based environmental lighting to\nproduce visually imperceptible watermarks for consumer cameras. Our approach\noptimizes an LED light source's spectral profile to be minimally visible to the\nhuman eye while remaining highly detectable by typical consumer cameras. The\nmethod jointly considers the human visual system's sensitivity to visible\nspectra, modern consumer camera sensors' spectral sensitivity, and narrowband\nLEDs' ability to generate broadband spectra perceived as \"white light\"\n(specifically, D65 illumination). To ensure imperceptibility, we employ\nspectral modulation rather than intensity modulation. Unlike conventional\nvisible light communication, our approach enables watermark extraction at\nstandard low frame rates (30-60 fps). While the information transfer rate is\nmodest-embedding 128 bits within a 10-second video clip-this capacity is\nsufficient for essential metadata supporting privacy protection and content\nverification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528LED\u73af\u5883\u7167\u660e\u4e3a\u6d88\u8d39\u7ea7\u76f8\u673a\u751f\u6210\u89c6\u89c9\u4e0d\u53ef\u89c1\u6c34\u5370\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316LED\u5149\u6e90\u7684\u5149\u8c31\u7279\u6027\uff0c\u4f7f\u5176\u5bf9\u4eba\u773c\u51e0\u4e4e\u4e0d\u53ef\u89c1\u4f46\u5bf9\u76f8\u673a\u9ad8\u5ea6\u53ef\u68c0\u6d4b\uff0c\u5b9e\u73b0\u4e86\u5728\u6807\u51c6\u5e27\u7387\u4e0b\u63d0\u53d6\u6c34\u5370\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u6d88\u8d39\u7ea7\u76f8\u673a\u62cd\u6444\u7684\u89c6\u9891\u4e2d\u5d4c\u5165\u4e0d\u53ef\u89c1\u6c34\u5370\u7684\u6280\u672f\uff0c\u4ee5\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u548c\u5185\u5bb9\u9a8c\u8bc1\u7b49\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u53ef\u89c1\u5149\u901a\u4fe1\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9ad8\u5e27\u7387\u4e14\u53ef\u80fd\u88ab\u4eba\u773c\u5bdf\u89c9\u3002", "method": "\u8be5\u65b9\u6cd5\u8054\u5408\u8003\u8651\u4e86\u4eba\u773c\u89c6\u89c9\u7cfb\u7edf\u5bf9\u53ef\u89c1\u5149\u8c31\u7684\u654f\u611f\u6027\u3001\u73b0\u4ee3\u6d88\u8d39\u76f8\u673a\u4f20\u611f\u5668\u7684\u5149\u8c31\u7075\u654f\u5ea6\u4ee5\u53ca\u7a84\u5e26LED\u751f\u6210\u5bbd\u5e26\u5149\u8c31\u7684\u80fd\u529b\uff0c\u91c7\u7528\u5149\u8c31\u8c03\u5236\u800c\u975e\u5f3a\u5ea6\u8c03\u5236\u6765\u786e\u4fdd\u4e0d\u53ef\u611f\u77e5\u6027\uff0c\u5e76\u4f18\u5316LED\u5149\u6e90\u7684\u5149\u8c31\u8f6e\u5ed3\u4f7f\u5176\u5728D65\u7167\u660e\u4e0b\u5448\u73b0\u4e3a\"\u767d\u5149\"\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6807\u51c6\u4f4e\u5e27\u7387\uff0830-60 fps\uff09\u4e0b\u63d0\u53d6\u6c34\u5370\uff0c\u4fe1\u606f\u4f20\u8f93\u901f\u7387\u9002\u4e2d\uff0c\u572810\u79d2\u89c6\u9891\u7247\u6bb5\u4e2d\u53ef\u5d4c\u5165128\u4f4d\u6570\u636e\uff0c\u8fd9\u4e00\u5bb9\u91cf\u8db3\u4ee5\u652f\u6301\u57fa\u672c\u7684\u5143\u6570\u636e\u9700\u6c42\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5229\u7528\u73af\u5883\u7167\u660e\u5b9e\u73b0\u89c6\u89c9\u4e0d\u53ef\u89c1\u6c34\u5370\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u548c\u5185\u5bb9\u9a8c\u8bc1\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u73b0\u6709\u6d88\u8d39\u7ea7\u76f8\u673a\u7cfb\u7edf\u7684\u517c\u5bb9\u6027\u3002"}}
{"id": "2510.17501", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17501", "abs": "https://arxiv.org/abs/2510.17501", "authors": ["Yuanli Wu", "Long Zhang", "Yue Du", "Bin Li"], "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization", "comment": null, "summary": "With the rapid proliferation of video content across social media,\nsurveillance, and education platforms, efficiently summarizing long videos into\nconcise yet semantically faithful surrogates has become increasingly vital.\nExisting supervised methods achieve strong in-domain accuracy by learning from\ndense annotations but suffer from high labeling costs and limited cross-dataset\ngeneralization, while unsupervised approaches, though label-free, often fail to\ncapture high-level human semantics and fine-grained narrative cues. More\nrecently, zero-shot prompting pipelines have leveraged large language models\n(LLMs) for training-free video summarization, yet remain highly sensitive to\nhandcrafted prompt templates and dataset-specific score normalization. To\novercome these limitations, we introduce a rubric-guided, pseudo-labeled\nprompting framework that transforms a small subset of ground-truth annotations\ninto high-confidence pseudo labels, which are aggregated into structured,\ndataset-adaptive scoring rubrics guiding interpretable scene evaluation. During\ninference, first and last segments are scored based solely on their\ndescriptions, whereas intermediate ones incorporate brief contextual summaries\nof adjacent scenes to assess narrative progression and redundancy. This\ncontextual prompting enables the LLM to balance local salience and global\ncoherence without parameter tuning. On SumMe and TVSum, our method achieves F1\nscores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior\nzero-shot baselines while approaching supervised performance. The results\ndemonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based\nscoring and establishes a general, interpretable zero-shot paradigm for video\nsummarization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u8f6c\u5316\u4e3a\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u5e76\u805a\u5408\u4e3a\u7ed3\u6784\u5316\u8bc4\u5206\u6807\u51c6\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u53c2\u6570\u8c03\u4f18\u7684\u96f6\u6837\u672c\u89c6\u9891\u6458\u8981\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728SumMe\u548cTVSum\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u65e0\u76d1\u7763\u548c\u5148\u524d\u96f6\u6837\u672c\u57fa\u7ebf\uff0c\u63a5\u8fd1\u76d1\u7763\u65b9\u6cd5\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u6807\u6ce8\u5bfc\u81f4\u9ad8\u6602\u6807\u6ce8\u6210\u672c\u548c\u6709\u9650\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u9ad8\u5c42\u6b21\u8bed\u4e49\u548c\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u7ebf\u7d22\uff0c\u800c\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u5bf9\u4eba\u5de5\u8bbe\u8ba1\u63d0\u793a\u6a21\u677f\u548c\u6570\u636e\u96c6\u7279\u5b9a\u5206\u6570\u5f52\u4e00\u5316\u9ad8\u5ea6\u654f\u611f\u3002\u672c\u6587\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u5f00\u53d1\u4e00\u79cd\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684\u96f6\u6837\u672c\u89c6\u9891\u6458\u8981\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8bc4\u5206\u6807\u51c6\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u63d0\u793a\u6846\u67b6\uff0c\u5c06\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u8f6c\u5316\u4e3a\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u5e76\u805a\u5408\u4e3a\u7ed3\u6784\u5316\u3001\u6570\u636e\u96c6\u81ea\u9002\u5e94\u7684\u8bc4\u5206\u6807\u51c6\u3002\u63a8\u7406\u65f6\u9996\u5c3e\u7247\u6bb5\u4ec5\u57fa\u4e8e\u63cf\u8ff0\u8bc4\u5206\uff0c\u4e2d\u95f4\u7247\u6bb5\u5219\u7ed3\u5408\u76f8\u90bb\u573a\u666f\u7684\u4e0a\u4e0b\u6587\u6458\u8981\u6765\u8bc4\u4f30\u53d9\u4e8b\u8fdb\u5c55\u548c\u5197\u4f59\u5ea6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u63d0\u793a\u4f7fLLM\u5728\u65e0\u9700\u53c2\u6570\u8c03\u4f18\u4e0b\u5e73\u8861\u5c40\u90e8\u663e\u8457\u6027\u548c\u5168\u5c40\u8fde\u8d2f\u6027\u3002", "result": "\u5728SumMe\u548cTVSum\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523057.58\u548c63.05\u7684F1\u5206\u6570\uff0c\u8d85\u8d8a\u4e86\u65e0\u76d1\u7763\u65b9\u6cd5\u548c\u5148\u524d\u96f6\u6837\u672c\u57fa\u7ebf\uff0c\u63a5\u8fd1\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u73b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7a33\u5b9a\u57fa\u4e8eLLM\u7684\u8bc4\u5206\u8fc7\u7a0b\u3002", "conclusion": "\u8bc4\u5206\u6807\u51c6\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7a33\u5b9aLLM\u57fa\u8bc4\u5206\u8fc7\u7a0b\uff0c\u4e3a\u89c6\u9891\u6458\u8981\u5efa\u7acb\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u96f6\u6837\u672c\u8303\u5f0f\u3002\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u7ed3\u6784\u5316\u8bc4\u5206\u6807\u51c6\u548c\u4e0a\u4e0b\u6587\u63d0\u793a\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u6458\u8981\uff0c\u4e3a\u540e\u7eed\u96f6\u6837\u672c\u89c6\u9891\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.17519", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17519", "abs": "https://arxiv.org/abs/2510.17519", "authors": ["Yongshun Zhang", "Zhongyi Fan", "Yonghang Zhang", "Zhangzikang Li", "Weifeng Chen", "Zhongwei Feng", "Chaoyue Wang", "Peng Hou", "Anxiang Zeng"], "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models", "comment": "Technical Report; Project Page:\n  \\href{https://github.com/Shopee-MUG/MUG-V}", "summary": "In recent years, large-scale generative models for visual content\n(\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\n\\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f18\u5316\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u7684\u56db\u652f\u67f1\u6846\u67b6\uff0c\u6db5\u76d6\u6570\u636e\u5904\u7406\u3001\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\uff0c\u5f00\u53d1\u51fa\u7684MUG-V 10B\u6a21\u578b\u5728\u6574\u4f53\u6027\u80fd\u4e0a\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\uff0c\u5e76\u5728\u7535\u5546\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u8d85\u8d8a\u9886\u5148\u5f00\u6e90\u57fa\u7ebf\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u9762\u4e34\u8de8\u6a21\u6001\u6587\u672c-\u89c6\u9891\u5bf9\u9f50\u3001\u957f\u5e8f\u5217\u5904\u7406\u548c\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u8bad\u7ec3\u8fc7\u7a0b\u7279\u522b\u56f0\u96be\u548c\u8d44\u6e90\u5bc6\u96c6\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u4f18\u5316\u65b9\u6848\u6765\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7684\u8bad\u7ec3\u6846\u67b6\u7cfb\u7edf\u4f18\u5316\u4e86\u56db\u4e2a\u5173\u952e\u652f\u67f1\uff1a\u6570\u636e\u5904\u7406\uff08\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u548c\u89c6\u9891\u538b\u7f29\uff09\u3001\u6a21\u578b\u67b6\u6784\uff08\u53c2\u6570\u7f29\u653e\u8bbe\u8ba1\uff09\u3001\u8bad\u7ec3\u7b56\u7565\uff08\u8bfe\u7a0b\u5f0f\u9884\u8bad\u7ec3\u548c\u5bf9\u9f50\u5bfc\u5411\u7684\u540e\u8bad\u7ec3\uff09\u4ee5\u53ca\u57fa\u7840\u8bbe\u65bd\uff08\u57fa\u4e8eMegatron-Core\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u5b9e\u73b0\uff09\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u8bad\u7ec3\u6548\u7387\u63d0\u5347\u3002", "result": "MUG-V 10B\u6a21\u578b\u5728\u6574\u4f53\u89c6\u9891\u751f\u6210\u8d28\u91cf\u4e0a\u5339\u914d\u5f53\u524d\u6700\u4f18\u6a21\u578b\uff0c\u5728\u7535\u5546\u5bfc\u5411\u7684\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u8bad\u7ec3\u6846\u67b6\u5b9e\u73b0\u4e86\u663e\u8457\u6548\u7387\u589e\u76ca\u548c\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u7cfb\u7edf\u6027\u4f18\u5316\u8bad\u7ec3\u6846\u67b6\u7684\u56db\u4e2a\u652f\u67f1\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u5f00\u6e90\u53d1\u5e03\u7684\u5b8c\u6574\u6280\u672f\u6808\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u4e8eMegatron-Core\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u8bad\u7ec3\u4ee3\u7801\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8fd1\u7ebf\u6027\u591a\u8282\u70b9\u6269\u5c55\u80fd\u529b\u3002"}}
{"id": "2510.17171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17171", "abs": "https://arxiv.org/abs/2510.17171", "authors": ["Feihong Yan", "Peiru Wang", "Yao Zhu", "Kaiyu Pang", "Qingyan Wei", "Huiqi Li", "Linfeng Zhang"], "title": "Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling", "comment": "12 pages, 6 figures", "summary": "Masked Autoregressive (MAR) models promise better efficiency in visual\ngeneration than autoregressive (AR) models for the ability of parallel\ngeneration, yet their acceleration potential remains constrained by the\nmodeling complexity of spatially correlated visual tokens in a single step. To\naddress this limitation, we introduce Generation then Reconstruction (GtR), a\ntraining-free hierarchical sampling strategy that decomposes generation into\ntwo stages: structure generation establishing global semantic scaffolding,\nfollowed by detail reconstruction efficiently completing remaining tokens.\nAssuming that it is more difficult to create an image from scratch than to\ncomplement images based on a basic image framework, GtR is designed to achieve\nacceleration by computing the reconstruction stage quickly while maintaining\nthe generation quality by computing the generation stage slowly. Moreover,\nobserving that tokens on the details of an image often carry more semantic\ninformation than tokens in the salient regions, we further propose\nFrequency-Weighted Token Selection (FTS) to offer more computation budget to\ntokens on image details, which are localized based on the energy of high\nfrequency information. Extensive experiments on ImageNet class-conditional and\ntext-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining\ncomparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),\nsubstantially outperforming existing acceleration methods across various model\nscales and generation tasks. Our codes will be released in\nhttps://github.com/feihongyan1/GtR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GtR\uff08Generation then Reconstruction\uff09\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5206\u5c42\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u751f\u6210\u5206\u89e3\u4e3a\u7ed3\u6784\u751f\u6210\u548c\u7ec6\u8282\u91cd\u5efa\u4e24\u4e2a\u9636\u6bb5\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e863.72\u500d\u7684\u52a0\u901f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u52a0\u901f\u65b9\u6cd5\u3002", "motivation": "\u63a9\u7801\u81ea\u56de\u5f52\u6a21\u578b\u867d\u7136\u5177\u5907\u5e76\u884c\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5176\u52a0\u901f\u6f5c\u529b\u53d7\u5230\u5355\u6b65\u4e2d\u7a7a\u95f4\u76f8\u5173\u89c6\u89c9\u6807\u8bb0\u5efa\u6a21\u590d\u6742\u5ea6\u7684\u9650\u5236\uff0c\u9700\u8981\u89e3\u51b3\u751f\u6210\u6548\u7387\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86GtR\u5206\u5c42\u91c7\u6837\u7b56\u7565\uff0c\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u7ed3\u6784\u751f\u6210\u548c\u7ec6\u8282\u91cd\u5efa\u4e24\u4e2a\u9636\u6bb5\uff1a\u7ed3\u6784\u751f\u6210\u5efa\u7acb\u5168\u5c40\u8bed\u4e49\u6846\u67b6\uff0c\u7ec6\u8282\u91cd\u5efa\u9ad8\u6548\u5b8c\u6210\u5269\u4f59\u6807\u8bb0\uff1b\u540c\u65f6\u5f15\u5165\u9891\u7387\u52a0\u6743\u6807\u8bb0\u9009\u62e9\u673a\u5236\uff0c\u57fa\u4e8e\u9ad8\u9891\u4fe1\u606f\u80fd\u91cf\u4e3a\u56fe\u50cf\u7ec6\u8282\u533a\u57df\u7684\u6807\u8bb0\u5206\u914d\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u5728ImageNet\u7c7b\u522b\u6761\u4ef6\u751f\u6210\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGtR\u5728MAR-H\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e863.72\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u751f\u6210\u8d28\u91cf\uff08FID\uff1a1.59\uff0cIS\uff1a304.4 vs \u539f\u59cb1.59\uff0c299.1\uff09\uff0c\u5728\u5404\u79cd\u6a21\u578b\u89c4\u6a21\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u5206\u5c42\u751f\u6210\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u5e73\u8861\u751f\u6210\u6548\u7387\u4e0e\u8d28\u91cf\uff0c\u9891\u7387\u611f\u77e5\u7684\u6807\u8bb0\u9009\u62e9\u673a\u5236\u80fd\u591f\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff0c\u4e3a\u9ad8\u6548\u89c6\u89c9\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.17651", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17651", "abs": "https://arxiv.org/abs/2510.17651", "authors": ["S\u00e9bastien Thuau", "Siba Haidar", "Ayush Bajracharya", "Rachid Chelouah"], "title": "Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs", "comment": "7 pages, 1 figure, FLTA 2025", "summary": "We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings.\n  Both approaches exceed 90% accuracy. CNN3D slightly outperforms Low-Rank\nAdaptation(LoRA)-tuned VLMs in ROC AUC and log loss, while using less energy.\nVLMs remain favorable for contextual reasoning and multimodal inference. We\nquantify energy and CO$_2$ emissions across training and inference, and analyze\nsustainability trade-offs for deployment.\n  To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics.\n  These findings support a hybrid model: lightweight CNNs for routine\nclassification, with selective VLM activation for complex or descriptive\nscenarios. The resulting framework offers a reproducible baseline for\nresponsible, resource-aware AI in video surveillance, with extensions toward\nreal-time, multimodal, and lifecycle-aware systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cd\u8282\u4fed\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u7528\u4e8e\u66b4\u529b\u68c0\u6d4b\uff1a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u4e0e\u8054\u90a6\u5fae\u8c03\u7b56\u7565\uff0c\u4ee5\u53ca\u7d27\u51d13D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u4e2a\u6027\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u80fd\u6e90\u6548\u7387\u548c\u73af\u5883\u6307\u6807\u3002", "motivation": "\u5f53\u524d\u66b4\u529b\u68c0\u6d4b\u7cfb\u7edf\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u90e8\u7f72\u65f6\u9762\u4e34\u80fd\u6e90\u6d88\u8017\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u573a\u666f\u4e0b\uff0c\u9700\u8981\u63a2\u7d22\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u5177\u5907\u80fd\u6e90\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff1a\u4f7f\u7528LLaVA-7B\u8fdb\u884c\u96f6\u6837\u672c\u548c\u8054\u90a6\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\uff0c\u4ee5\u53ca\u57fa\u4e8e65.8M\u53c2\u6570\u7d27\u51d13D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u4e2a\u6027\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u51c6\u786e\u7387\u3001\u6821\u51c6\u5ea6\u548c\u80fd\u6e90\u4f7f\u7528\u60c5\u51b5\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5747\u8d85\u8fc790%\u7684\u51c6\u786e\u7387\uff0cCNN3D\u5728ROC AUC\u548clog loss\u6307\u6807\u4e0a\u7565\u4f18\u4e8e\u4f4e\u79e9\u81ea\u9002\u5e94\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u80fd\u8017\u66f4\u4f4e\uff1b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u591a\u6a21\u6001\u63a8\u65ad\u65b9\u9762\u4ecd\u5177\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u6df7\u5408\u6a21\u578b\u67b6\u6784\uff1a\u8f7b\u91cf\u7ea7CNN\u7528\u4e8e\u5e38\u89c4\u5206\u7c7b\u4efb\u52a1\uff0c\u9009\u62e9\u6027\u6fc0\u6d3b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u6216\u63cf\u8ff0\u6027\u573a\u666f\uff0c\u4e3a\u89c6\u9891\u76d1\u63a7\u9886\u57df\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u8d44\u6e90\u611f\u77e5AI\u57fa\u51c6\u6846\u67b6\uff0c\u53ef\u6269\u5c55\u81f3\u5b9e\u65f6\u591a\u6a21\u6001\u548c\u751f\u547d\u5468\u671f\u611f\u77e5\u7cfb\u7edf\u3002"}}
{"id": "2510.17684", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17684", "abs": "https://arxiv.org/abs/2510.17684", "authors": ["Xinwei Zhang", "Hu Chen", "Zhe Yuan", "Sukun Tian", "Peng Feng"], "title": "Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model", "comment": null, "summary": "Foundation models for medical image segmentation have achieved remarkable\nperformance. Adaptive fine-tuning of natural image segmentation foundation\nmodels is crucial for medical image segmentation tasks. However, some\nlimitations exist in existing fine-tuning methods: 1) insufficient\nrepresentation of high-level features and 2) the fine-tuning process disrupts\nthe structural integrity of pretrained weights. Inspired by these critical\nproblems, we propose an intelligent communication mixture-of-experts\nboosted-medical image segmentation foundation model, named IC-MoE, with twofold\nideas: 1) We construct basic experts, semantic experts, and adaptive experts.\nMoreover, we implement a pixel probability adaptive voting strategy, which\nenables expert selection and fusion through label consistency and load\nbalancing. This approach preliminarily enhances the representation capability\nof high-level features while preserving the structural integrity of pretrained\nweights. 2) We propose a semantic-guided contrastive learning method to address\nthe issue of weak supervision in contrastive learning. This method further\nenhances the representation capability of high-level features while preserving\nthe structural integrity of pretrained weights. Extensive experiments across\nthree public medical image segmentation datasets demonstrate that the IC-MoE\noutperforms other SOTA models. Consequently, the proposed IC-MoE effectively\nsupplements foundational medical image segmentation models with high-level\nfeatures and pretrained structural integrity. We also validate the superior\ngeneralizability of the IC-MoE across diverse medical image segmentation\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIC-MoE\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u548c\u8bed\u4e49\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b\u81ea\u9002\u5e94\u5fae\u8c03\u4e2d\u7684\u9ad8\u5c42\u7279\u5f81\u8868\u793a\u4e0d\u8db3\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u7ed3\u6784\u5b8c\u6574\u6027\u7834\u574f\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b\u81ea\u9002\u5e94\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u9ad8\u5c42\u7279\u5f81\u8868\u793a\u80fd\u529b\u4e0d\u8db3\uff0c\u4ee5\u53ca\u5fae\u8c03\u8fc7\u7a0b\u4f1a\u7834\u574f\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faIC-MoE\u6a21\u578b\uff0c\u6784\u5efa\u57fa\u7840\u4e13\u5bb6\u3001\u8bed\u4e49\u4e13\u5bb6\u548c\u81ea\u9002\u5e94\u4e13\u5bb6\uff0c\u91c7\u7528\u50cf\u7d20\u6982\u7387\u81ea\u9002\u5e94\u6295\u7968\u7b56\u7565\u5b9e\u73b0\u4e13\u5bb6\u9009\u62e9\u548c\u878d\u5408\uff1b\u540c\u65f6\u63d0\u51fa\u8bed\u4e49\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6807\u7b7e\u4e00\u81f4\u6027\u548c\u8d1f\u8f7d\u5e73\u8861\u589e\u5f3a\u9ad8\u5c42\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u8bad\u7ec3\u6743\u91cd\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cIC-MoE\u6a21\u578b\u8d85\u8d8a\u4e86\u5176\u4ed6\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u6837\u5316\u533b\u5b66\u56fe\u50cf\u5206\u5272\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "IC-MoE\u6a21\u578b\u6709\u6548\u8865\u5145\u4e86\u57fa\u7840\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u7684\u9ad8\u5c42\u7279\u5f81\u8868\u793a\u80fd\u529b\u548c\u9884\u8bad\u7ec3\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u5fae\u8c03\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17685", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17685", "abs": "https://arxiv.org/abs/2510.17685", "authors": ["Min Cao", "Xinyu Zhou", "Ding Jiang", "Bo Du", "Mang Ye", "Min Zhang"], "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning", "comment": "Final version published in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI). Xplore link:\n  https://ieeexplore.ieee.org/document/11199360", "summary": "Text-to-image person retrieval (TIPR) aims to identify the target person\nusing textual descriptions, facing challenge in modality heterogeneity. Prior\nworks have attempted to address it by developing cross-modal global or local\nalignment strategies. However, global methods typically overlook fine-grained\ncross-modal differences, whereas local methods require prior information to\nexplore explicit part alignments. Additionally, current methods are\nEnglish-centric, restricting their application in multilingual contexts. To\nalleviate these issues, we pioneer a multilingual TIPR task by developing a\nmultilingual TIPR benchmark, for which we leverage large language models for\ninitial translations and refine them by integrating domain-specific knowledge.\nCorrespondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation\nReasoning and Aligning framework to learn alignment across languages and\nmodalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module\nenables bidirectional prediction of masked image and text, implicitly enhancing\nthe modeling of local relations across languages and modalities, a\nmulti-dimensional global alignment module is integrated to bridge the modality\nheterogeneity. The proposed method achieves new state-of-the-art results on all\nmultilingual TIPR datasets. Data and code are presented in\nhttps://github.com/Flame-Chasers/Bi-IRRA.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u591a\u8bed\u8a00\u6587\u672c\u5230\u56fe\u50cf\u4eba\u7269\u68c0\u7d22\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86Bi-IRRA\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u9690\u5f0f\u5173\u7cfb\u63a8\u7406\u548c\u591a\u7ef4\u5168\u5c40\u5bf9\u9f50\u6765\u5b66\u4e60\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u7684\u5bf9\u9f50\uff0c\u5728\u6240\u6709\u591a\u8bed\u8a00TIPR\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u4eba\u7269\u68c0\u7d22\u9762\u4e34\u6a21\u6001\u5f02\u8d28\u6027\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5168\u5c40\u65b9\u6cd5\u5ffd\u7565\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5dee\u5f02\uff0c\u5c40\u90e8\u65b9\u6cd5\u9700\u8981\u5148\u9a8c\u4fe1\u606f\u8fdb\u884c\u663e\u5f0f\u90e8\u5206\u5bf9\u9f50\uff0c\u4e14\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86Bi-IRRA\u6846\u67b6\uff0c\u5305\u542b\u53cc\u5411\u9690\u5f0f\u5173\u7cfb\u63a8\u7406\u6a21\u5757\uff0c\u901a\u8fc7\u53cc\u5411\u9884\u6d4b\u63a9\u7801\u56fe\u50cf\u548c\u6587\u672c\u6765\u9690\u5f0f\u589e\u5f3a\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u7684\u5c40\u90e8\u5173\u7cfb\u5efa\u6a21\uff0c\u540c\u65f6\u96c6\u6210\u4e86\u591a\u7ef4\u5168\u5c40\u5bf9\u9f50\u6a21\u5757\u6765\u5f25\u5408\u6a21\u6001\u5f02\u8d28\u6027\uff0c\u5e76\u6784\u5efa\u4e86\u591a\u8bed\u8a00TIPR\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6240\u6709\u591a\u8bed\u8a00TIPR\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u6269\u5c55\u4e86TIPR\u4efb\u52a1\u5230\u591a\u8bed\u8a00\u573a\u666f\uff0c\u8fd8\u901a\u8fc7\u9690\u5f0f\u5173\u7cfb\u63a8\u7406\u548c\u5168\u5c40\u5bf9\u9f50\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8de8\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.17722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17722", "abs": "https://arxiv.org/abs/2510.17722", "authors": ["Yaning Pan", "Zekun Wang", "Qianqian Xie", "Yongqian Wen", "Yuanxing Zhang", "Guohui Zhang", "Haoxuan Hu", "Zhiyu Pan", "Yibing Huang", "Zhidong Gan", "Yonghong Lin", "An Ping", "Tianhao Peng", "Jiaheng Liu"], "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues", "comment": "Project Website: https://github.com/NJU-LINK/MT-Video-Bench", "summary": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MT-Video-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u89c6\u9891\u7406\u89e3\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u591a\u8f6e\u89c6\u9891\u5bf9\u8bdd\u65f6\u7684\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u8f6e\u95ee\u7b54\u4efb\u52a1\uff0c\u672a\u80fd\u5145\u5206\u53cd\u6620\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u8f6e\u5bf9\u8bdd\u7684\u590d\u6742\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u6709\u6548\u8bc4\u4f30\u548c\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b987\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u591a\u8f6e\u5bf9\u8bdd\u7684MT-Video-Bench\u57fa\u51c6\uff0c\u4e3b\u8981\u8bc4\u4f30\u611f\u77e5\u6027\u548c\u4ea4\u4e92\u6027\u4e24\u5927\u7ef4\u5ea6\u7684\u516d\u9879\u6838\u5fc3\u80fd\u529b\uff0c\u6db5\u76d6\u4f53\u80b2\u5206\u6790\u548c\u89c6\u9891\u6559\u5b66\u7b49\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u9886\u57df\u3002", "result": "\u5bf9\u591a\u79cd\u5148\u8fdb\u5f00\u6e90\u548c\u95ed\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5904\u7406\u591a\u8f6e\u89c6\u9891\u5bf9\u8bdd\u65f6\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\u548c\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u57fa\u51c6\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u89c6\u9891\u5bf9\u8bdd\u7406\u89e3\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u53d1\u5c55\u65b9\u5411\uff0c\u5c06\u4fc3\u8fdb\u66f4\u9c81\u68d2\u7684\u89c6\u9891\u5bf9\u8bdd\u7cfb\u7edf\u5f00\u53d1\u3002"}}
{"id": "2510.17274", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17274", "abs": "https://arxiv.org/abs/2510.17274", "authors": ["Katie Luo", "Jingwei Ji", "Tong He", "Runsheng Xu", "Yichen Xie", "Dragomir Anguelov", "Mingxing Tan"], "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models", "comment": "In proceedings of IROS 2025", "summary": "Current autonomous driving systems rely on specialized models for perceiving\nand predicting motion, which demonstrate reliable performance in standard\nconditions. However, generalizing cost-effectively to diverse real-world\nscenarios remains a significant challenge. To address this, we propose\nPlug-and-Forecast (PnF), a plug-and-play approach that augments existing motion\nforecasting models with multimodal large language models (MLLMs). PnF builds on\nthe insight that natural language provides a more effective way to describe and\nhandle complex scenarios, enabling quick adaptation to targeted behaviors. We\ndesign prompts to extract structured scene understanding from MLLMs and distill\nthis information into learnable embeddings to augment existing behavior\nprediction models. Our method leverages the zero-shot reasoning capabilities of\nMLLMs to achieve significant improvements in motion prediction performance,\nwhile requiring no fine-tuning -- making it practical to adopt. We validate our\napproach on two state-of-the-art motion forecasting models using the Waymo Open\nMotion Dataset and the nuScenes Dataset, demonstrating consistent performance\nimprovements across both benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPlug-and-Forecast\uff08PnF\uff09\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u73b0\u6709\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u590d\u6742\u573a\u666f\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4f9d\u8d56\u4e13\u7528\u6a21\u578b\u8fdb\u884c\u611f\u77e5\u548c\u8fd0\u52a8\u9884\u6d4b\uff0c\u5728\u6807\u51c6\u6761\u4ef6\u4e0b\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u96be\u4ee5\u7ecf\u6d4e\u9ad8\u6548\u5730\u6cdb\u5316\u5230\u591a\u6837\u5316\u73b0\u5b9e\u573a\u666f\uff0c\u8fd9\u6210\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9762\u4e34\u7684\u91cd\u8981\u6311\u6218\u3002", "method": "PnF\u65b9\u6cd5\u8bbe\u8ba1\u63d0\u793a\u8bcd\u4ece\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u573a\u666f\u7406\u89e3\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4fe1\u606f\u63d0\u70bc\u4e3a\u53ef\u5b66\u4e60\u7684\u5d4c\u5165\u5411\u91cf\u6765\u589e\u5f3a\u73b0\u6709\u884c\u4e3a\u9884\u6d4b\u6a21\u578b\uff0c\u5145\u5206\u5229\u7528MLLMs\u7684\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728Waymo Open Motion Dataset\u548cnuScenes Dataset\u4e0a\u5bf9\u4e24\u79cd\u6700\u5148\u8fdb\u7684\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u83b7\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u81ea\u7136\u8bed\u8a00\u5728\u63cf\u8ff0\u548c\u5904\u7406\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u96f6\u6837\u672c\u9002\u5e94\u7279\u5b9a\u884c\u4e3a\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u7684\u7279\u6027\u4f7f\u5176\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.17305", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.17305", "abs": "https://arxiv.org/abs/2510.17305", "authors": ["ZhaoYang Han", "Qihan Lin", "Hao Liang", "Bowen Chen", "Zhou Liu", "Wentao Zhang"], "title": "LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding", "comment": "Submitted to ARR Rolling Review", "summary": "We introduce \\textbf{LongInsightBench}, the first benchmark designed to\nassess models' ability to understand long videos, with a focus on human\nlanguage, viewpoints, actions, and other contextual elements, while integrating\n\\textbf{visual, audio, and text} modalities. Our benchmark excels in three key\nareas: \\textbf{a) Long-Duration, Information-Dense Videos:} We carefully select\napproximately 1,000 videos from open-source datasets FineVideo based on\nduration limit and the information density of both visual and audio modalities,\nfocusing on content like lectures, interviews, and vlogs, which contain rich\nlanguage elements. \\textbf{b) Diverse and Challenging Task Scenarios:} We have\ndesigned six challenging task scenarios, including both Intra-Event and\nInter-Event Tasks. \\textbf{c) Rigorous and Comprehensive Quality Assurance\nPipelines:} We have developed a three-step, semi-automated data quality\nassurance pipeline to ensure the difficulty and validity of the synthesized\nquestions and answer options. Based on LongInsightBench, we designed a series\nof experiments. Experimental results shows that Omni-modal models(OLMs) still\nface challenge in tasks requiring precise temporal localization (T-Loc) and\nlong-range causal inference (CE-Caus). Extended experiments reveal the\ninformation loss and processing bias in multi-modal fusion of OLMs. Our dataset\nand code is available at\nhttps://anonymous.4open.science/r/LongInsightBench-910F/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u6ce8\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5LongInsightBench\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u6a21\u6001\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4eba\u7c7b\u8bed\u8a00\u3001\u89c2\u70b9\u3001\u52a8\u4f5c\u7b49\u4e0a\u4e0b\u6587\u5143\u7d20\u4e0a\u7684\u7406\u89e3\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5168\u6a21\u6001\u6a21\u578b\u5728\u65f6\u95f4\u5b9a\u4f4d\u548c\u957f\u8ddd\u79bb\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u957f\u89c6\u9891\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u662f\u5728\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff08\u89c6\u89c9\u3001\u97f3\u9891\u3001\u6587\u672c\uff09\u65b9\u9762\u5b58\u5728\u7814\u7a76\u7a7a\u767d\uff0c\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u5bf9\u4eba\u7c7b\u8bed\u8a00\u3001\u89c2\u70b9\u3001\u52a8\u4f5c\u7b49\u4e0a\u4e0b\u6587\u5143\u7d20\u7684\u6df1\u5ea6\u7406\u89e3\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u7ea61000\u4e2a\u957f\u65f6\u957f\u3001\u4fe1\u606f\u5bc6\u96c6\u89c6\u9891\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u8bb2\u5ea7\u3001\u8bbf\u8c08\u548cvlog\u7b49\u5bcc\u542b\u8bed\u8a00\u5143\u7d20\u7684\u5185\u5bb9\uff1b\u8bbe\u8ba1\u4e86\u516d\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u573a\u666f\uff0c\u5305\u62ec\u4e8b\u4ef6\u5185\u548c\u4e8b\u4ef6\u95f4\u4efb\u52a1\uff1b\u5f00\u53d1\u4e86\u4e09\u6b65\u534a\u81ea\u52a8\u6570\u636e\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\u786e\u4fdd\u95ee\u9898\u96be\u5ea6\u548c\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5168\u6a21\u6001\u6a21\u578b\u5728\u9700\u8981\u7cbe\u786e\u65f6\u95f4\u5b9a\u4f4d\u548c\u957f\u8ddd\u79bb\u56e0\u679c\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff1b\u6269\u5c55\u5b9e\u9a8c\u63ed\u793a\u4e86\u5168\u6a21\u6001\u6a21\u578b\u5728\u591a\u6a21\u6001\u878d\u5408\u4e2d\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u5904\u7406\u504f\u5dee\u7684\u95ee\u9898\u3002", "conclusion": "LongInsightBench\u4e3a\u957f\u89c6\u9891\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u7efc\u5408\u6027\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5168\u6a21\u6001\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6a21\u578b\u7684\u53d1\u5c55\u6307\u660e\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2510.17332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17332", "abs": "https://arxiv.org/abs/2510.17332", "authors": ["Zhaoran Zhao", "Xinli Yue", "Jianhui Sun", "Yuhao Xie", "Tao Shao", "Liangchao Yao", "Fan Xia", "Yuetang Deng"], "title": "iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA", "comment": "Accepted to ICCV 2025 Workshop", "summary": "Image Quality Assessment (IQA) has progressed from scalar quality prediction\nto more interpretable, human-aligned evaluation paradigms. In this work, we\naddress the emerging challenge of detailed and explainable IQA by proposing\niDETEX-a unified multimodal large language model (MLLM) capable of\nsimultaneously performing three key tasks: quality grounding, perception, and\ndescription. To facilitate efficient and generalizable training across these\nheterogeneous subtasks, we design a suite of task-specific offline augmentation\nmodules and a data mixing strategy. These are further complemented by online\nenhancement strategies to fully exploit multi-sourced supervision. We validate\nour approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves\nstate-of-the-art performance across all subtasks. Our model ranks first in the\nICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its\neffectiveness and robustness in delivering accurate and interpretable quality\nassessments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86iDETEX\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u6267\u884c\u8d28\u91cf\u5b9a\u4f4d\u3001\u611f\u77e5\u548c\u63cf\u8ff0\u4e09\u4e2a\u5173\u952e\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u8be6\u7ec6\u53ef\u89e3\u91ca\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u6311\u6218\u3002\u8be5\u6a21\u578b\u5728ViDA-UGC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728ICCV MIPI 2025\u8be6\u7ec6\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "motivation": "\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u5df2\u4ece\u6807\u91cf\u8d28\u91cf\u9884\u6d4b\u53d1\u5c55\u5230\u66f4\u53ef\u89e3\u91ca\u3001\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u65b0\u5174\u7684\u8be6\u7ec6\u53ef\u89e3\u91ca\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6311\u6218\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u540c\u65f6\u5904\u7406\u8d28\u91cf\u5b9a\u4f4d\u3001\u611f\u77e5\u548c\u63cf\u8ff0\u4e09\u4e2a\u5f02\u6784\u5b50\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86iDETEX\u7edf\u4e00\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86\u4e00\u5957\u4efb\u52a1\u7279\u5b9a\u7684\u79bb\u7ebf\u589e\u5f3a\u6a21\u5757\u548c\u6570\u636e\u6df7\u5408\u7b56\u7565\uff0c\u8f85\u4ee5\u5728\u7ebf\u589e\u5f3a\u7b56\u7565\u4ee5\u5145\u5206\u5229\u7528\u591a\u6e90\u76d1\u7763\uff0c\u5b9e\u73b0\u4e09\u4e2a\u5f02\u6784\u5b50\u4efb\u52a1\u7684\u9ad8\u6548\u548c\u6cdb\u5316\u8bad\u7ec3\u3002", "result": "\u5728\u5927\u578bViDA-UGC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0ciDETEX\u5728\u6240\u6709\u5b50\u4efb\u52a1\u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728ICCV MIPI 2025\u8be6\u7ec6\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u4f9b\u51c6\u786e\u548c\u53ef\u89e3\u91ca\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be6\u7ec6\u53ef\u89e3\u91ca\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u67b6\u6784\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u4e2a\u5f02\u6784\u4efb\u52a1\uff0c\u4e3a\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u52a0\u5168\u9762\u548c\u4eba\u7c7b\u5bf9\u9f50\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17347", "abs": "https://arxiv.org/abs/2510.17347", "authors": ["Jingqian Wu", "Shengpeng Xu", "Yunbo Jia", "Edmund Y. Lam"], "title": "Exploring The Missing Semantics In Event Modality", "comment": null, "summary": "Event cameras offer distinct advantages such as low latency, high dynamic\nrange, and efficient motion capture. However, event-to-video reconstruction\n(E2V), a fundamental event-based vision task, remains challenging, particularly\nfor reconstructing and recovering semantic information. This is primarily due\nto the nature of the event camera, as it only captures intensity changes,\nignoring static objects and backgrounds, resulting in a lack of semantic\ninformation in captured event modality. Further, semantic information plays a\ncrucial role in video and frame reconstruction, yet is often overlooked by\nexisting E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V\nframework that explores the missing visual semantic knowledge in event modality\nand leverages it to enhance event-to-video reconstruction. Specifically,\nSemantic-E2VID introduces a cross-modal feature alignment (CFA) module to\ntransfer the robust visual semantics from a frame-based vision foundation\nmodel, the Segment Anything Model (SAM), to the event encoder, while aligning\nthe high-level features from distinct modalities. To better utilize the learned\nsemantic feature, we further propose a semantic-aware feature fusion (SFF)\nblock to integrate learned semantics in frame modality to form event\nrepresentations with rich semantics that can be decoded by the event decoder.\nFurther, to facilitate the reconstruction of semantic information, we propose a\nnovel Semantic Perceptual E2V Supervision that helps the model to reconstruct\nsemantic details by leveraging SAM-generated categorical labels. Extensive\nexperiments demonstrate that Semantic-E2VID significantly enhances frame\nquality, outperforming state-of-the-art E2V methods across multiple benchmarks.\nThe sample code is included in the supplementary material.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSemantic-E2VID\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u8bed\u4e49\u77e5\u8bc6\u4ece\u5e27\u6a21\u6001\u8fc1\u79fb\u5230\u4e8b\u4ef6\u6a21\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u8bed\u4e49\u4fe1\u606f\u6062\u590d\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u65b9\u6cd5\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u4ec5\u6355\u83b7\u5f3a\u5ea6\u53d8\u5316\u800c\u5ffd\u7565\u9759\u6001\u7269\u4f53\u548c\u80cc\u666f\uff0c\u5bfc\u81f4\u4e8b\u4ef6\u6a21\u6001\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\uff0c\u800c\u73b0\u6709\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e86\u8bed\u4e49\u4fe1\u606f\u5728\u89c6\u9891\u91cd\u5efa\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u91cd\u5efa\u89c6\u9891\u7684\u8bed\u4e49\u8d28\u91cf\u548c\u4fe1\u606f\u5b8c\u6574\u6027\u3002", "method": "\u63d0\u51faSemantic-E2VID\u6846\u67b6\uff0c\u5305\u542b\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\u5c06Segment Anything Model\u7684\u89c6\u89c9\u8bed\u4e49\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e8b\u4ef6\u7f16\u7801\u5668\uff0c\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u878d\u5408\u5757\u96c6\u6210\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u7279\u5f81\u5f62\u6210\u5bcc\u542b\u8bed\u4e49\u7684\u4e8b\u4ef6\u8868\u793a\uff0c\u4ee5\u53ca\u8bed\u4e49\u611f\u77e5E2V\u76d1\u7763\u673a\u5236\u5229\u7528SAM\u751f\u6210\u7684\u7c7b\u522b\u6807\u7b7e\u4fc3\u8fdb\u8bed\u4e49\u7ec6\u8282\u91cd\u5efa\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSemantic-E2VID\u663e\u8457\u63d0\u5347\u4e86\u5e27\u8d28\u91cf\uff0c\u5728\u4e8b\u4ef6\u5230\u89c6\u9891\u91cd\u5efa\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8bed\u4e49\u4fe1\u606f\u8fc1\u79fb\u5bf9\u91cd\u5efa\u8d28\u91cf\u7684\u91cd\u8981\u6539\u8fdb\u4f5c\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u89c6\u89c9\u8bed\u4e49\u77e5\u8bc6\u4ece\u5e27\u6a21\u6001\u8fc1\u79fb\u5230\u4e8b\u4ef6\u6a21\u6001\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u8bed\u4e49\u589e\u5f3a\u8303\u5f0f\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u4e8b\u4ef6\u89c6\u89c9\u4efb\u52a1\u4e2d\u63d0\u5347\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.17364", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17364", "abs": "https://arxiv.org/abs/2510.17364", "authors": ["Vaggelis Dorovatas", "Soroush Seifi", "Gunshi Gupta", "Rahaf Aljundi"], "title": "Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs", "comment": "NeurIPS 2025", "summary": "Video Large Language Models (Video-LLMs) excel at understanding videos\nin-context, provided they have full access to the video when answering queries.\nHowever, these models face challenges in streaming scenarios where hour-long\nvideos must be processed online, and questions need timely responses. In this\nwork, we propose a training-free approach compatible with standard Video-LLMs,\nleveraging three key concepts: 1) LLM-informed selection of visual tokens to\nidentify those that the LLM has attended to and contributed to its\nunderstanding of each short clip. Our attention-based selection allows us to\ndiscard up to ~95% of unimportant visual tokens with minimal performance loss;\n2) Recurrent processing of past selected tokens to generate temporally coherent\nunderstanding of each processed clip; 3) Caption-based question answering for\nlightweight and accurate responses. Our method achieves state-of-the-art\nperformance on streaming video benchmarks, striking a balance between\nefficiency and effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u89c6\u89c9\u4ee4\u724c\u9009\u62e9\u3001\u5faa\u73af\u5904\u7406\u548c\u57fa\u4e8e\u63cf\u8ff0\u7684\u95ee\u7b54\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d41\u5f0f\u89c6\u9891\u5904\u7406\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u6548\u7387\u3002", "motivation": "\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u4e0a\u4e0b\u6587\u89c6\u9891\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6d41\u5f0f\u5904\u7406\u573a\u666f\u4e0b\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u9700\u8981\u5b9e\u65f6\u5904\u7406\u5c0f\u65f6\u7ea7\u957f\u5ea6\u89c6\u9891\u5e76\u7ed9\u51fa\u53ca\u65f6\u54cd\u5e94\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u6548\u7387\u8981\u6c42\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a\u57fa\u4e8eLLM\u6ce8\u610f\u529b\u7684\u89c6\u89c9\u4ee4\u724c\u9009\u62e9\u673a\u5236\u8bc6\u522b\u5bf9\u7406\u89e3\u6bcf\u4e2a\u77ed\u7247\u6bb5\u91cd\u8981\u7684\u89c6\u89c9\u4ee4\u724c\uff1b\u5faa\u73af\u5904\u7406\u8fc7\u53bb\u9009\u5b9a\u7684\u4ee4\u724c\u4ee5\u751f\u6210\u65f6\u95f4\u8fde\u8d2f\u7684\u89c6\u9891\u7406\u89e3\uff1b\u57fa\u4e8e\u63cf\u8ff0\u7684\u8f7b\u91cf\u7ea7\u95ee\u7b54\u7cfb\u7edf\u5b9e\u73b0\u51c6\u786e\u54cd\u5e94\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6d41\u5f0f\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u80fd\u591f\u4e22\u5f03\u7ea695%\u7684\u4e0d\u91cd\u8981\u89c6\u89c9\u4ee4\u724c\u800c\u4ec5\u5e26\u6765\u6700\u5c0f\u6027\u80fd\u635f\u5931\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u901a\u8fc7\u667a\u80fd\u4ee4\u724c\u9009\u62e9\u548c\u5faa\u73af\u5904\u7406\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u7406\u89e3\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u5f0f\u5904\u7406\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u957f\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.17384", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17384", "abs": "https://arxiv.org/abs/2510.17384", "authors": ["Jiajin Tang", "Zhengxuan Wei", "Ge Zheng", "Sibei Yang"], "title": "Closed-Loop Transfer for Weakly-supervised Affordance Grounding", "comment": "Accepted at ICCV 2025", "summary": "Humans can perform previously unexperienced interactions with novel objects\nsimply by observing others engage with them. Weakly-supervised affordance\ngrounding mimics this process by learning to locate object regions that enable\nactions on egocentric images, using exocentric interaction images with\nimage-level annotations. However, extracting affordance knowledge solely from\nexocentric images and transferring it one-way to egocentric images limits the\napplicability of previous works in complex interaction scenarios. Instead, this\nstudy introduces LoopTrans, a novel closed-loop framework that not only\ntransfers knowledge from exocentric to egocentric but also transfers back to\nenhance exocentric knowledge extraction. Within LoopTrans, several innovative\nmechanisms are introduced, including unified cross-modal localization and\ndenoising knowledge distillation, to bridge domain gaps between object-centered\negocentric and interaction-centered exocentric images while enhancing knowledge\ntransfer. Experiments show that LoopTrans achieves consistent improvements\nacross all metrics on image and video benchmarks, even handling challenging\nscenarios where object interaction regions are fully occluded by the human\nbody.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LoopTrans\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u95ed\u73af\u6846\u67b6\uff0c\u7528\u4e8e\u5f31\u76d1\u7763\u53ef\u627f\u53d7\u6027\u5b9a\u4f4d\uff0c\u4e0d\u4ec5\u4ece\u5916\u4e2d\u5fc3\u89c6\u89d2\u5411\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4f20\u9012\u77e5\u8bc6\uff0c\u8fd8\u901a\u8fc7\u53cd\u5411\u4f20\u9012\u589e\u5f3a\u5916\u4e2d\u5fc3\u77e5\u8bc6\u63d0\u53d6\uff0c\u5728\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5168\u9762\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5f31\u76d1\u7763\u53ef\u627f\u53d7\u6027\u5b9a\u4f4d\u65b9\u6cd5\u4ec5\u4ece\u5916\u4e2d\u5fc3\u4ea4\u4e92\u56fe\u50cf\u5355\u5411\u4f20\u9012\u77e5\u8bc6\u5230\u81ea\u6211\u4e2d\u5fc3\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\uff0c\u9700\u8981\u89e3\u51b3\u9886\u57df\u5dee\u8ddd\u548c\u77e5\u8bc6\u4f20\u9012\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86LoopTrans\u95ed\u73af\u6846\u67b6\uff0c\u5305\u542b\u7edf\u4e00\u8de8\u6a21\u6001\u5b9a\u4f4d\u548c\u53bb\u566a\u77e5\u8bc6\u84b8\u998f\u673a\u5236\uff0c\u901a\u8fc7\u53cc\u5411\u77e5\u8bc6\u4f20\u9012\u6865\u63a5\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u81ea\u6211\u4e2d\u5fc3\u56fe\u50cf\u548c\u4ee5\u4ea4\u4e92\u4e3a\u4e2d\u5fc3\u7684\u5916\u4e2d\u5fc3\u56fe\u50cf\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLoopTrans\u5728\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u7684\u6240\u6709\u6307\u6807\u4e0a\u90fd\u5b9e\u73b0\u4e86\u6301\u7eed\u6539\u8fdb\uff0c\u5373\u4f7f\u5728\u4eba\u7c7b\u8eab\u4f53\u5b8c\u5168\u906e\u6321\u5bf9\u8c61\u4ea4\u4e92\u533a\u57df\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u4e5f\u80fd\u6709\u6548\u5904\u7406\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u53cc\u5411\u77e5\u8bc6\u4f20\u9012\u5728\u53ef\u627f\u53d7\u6027\u5b9a\u4f4d\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u95ed\u73af\u6846\u67b6\u80fd\u591f\u663e\u8457\u589e\u5f3a\u77e5\u8bc6\u63d0\u53d6\u548c\u4f20\u9012\u6548\u679c\uff0c\u4e3a\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.17409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17409", "abs": "https://arxiv.org/abs/2510.17409", "authors": ["Dmitrii Galimzianov", "Viacheslav Vyshegorodtsev", "Ivan Nezhivykh"], "title": "Monitoring Horses in Stalls: From Object to Event Detection", "comment": "12 pages, 4 figures, 4 tables", "summary": "Monitoring the behavior of stalled horses is essential for early detection of\nhealth and welfare issues but remains labor-intensive and time-consuming. In\nthis study, we present a prototype vision-based monitoring system that\nautomates the detection and tracking of horses and people inside stables using\nobject detection and multi-object tracking techniques. The system leverages\nYOLOv11 and BoT-SORT for detection and tracking, while event states are\ninferred based on object trajectories and spatial relations within the stall.\nTo support development, we constructed a custom dataset annotated with\nassistance from foundation models CLIP and GroundingDINO. The system\ndistinguishes between five event types and accounts for the camera's blind\nspots. Qualitative evaluation demonstrated reliable performance for\nhorse-related events, while highlighting limitations in detecting people due to\ndata scarcity. This work provides a foundation for real-time behavioral\nmonitoring in equine facilities, with implications for animal welfare and\nstable management.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u9a6c\u5339\u884c\u4e3a\u81ea\u52a8\u76d1\u6d4b\u7cfb\u7edf\u539f\u578b\uff0c\u5229\u7528\u76ee\u6807\u68c0\u6d4b\u548c\u591a\u76ee\u6807\u8ddf\u8e2a\u6280\u672f\u5b9e\u73b0\u9a6c\u53a9\u5185\u9a6c\u5339\u548c\u4eba\u5458\u7684\u81ea\u52a8\u5316\u76d1\u63a7\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7YOLOv11\u548cBoT-SORT\u7b97\u6cd5\u7ed3\u5408\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\uff0c\u4e3a\u9a6c\u5339\u798f\u5229\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u65f6\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u9a6c\u5339\u884c\u4e3a\u76d1\u6d4b\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u89c2\u5bdf\uff0c\u5b58\u5728\u52b3\u52a8\u5bc6\u96c6\u548c\u65f6\u95f4\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5b9e\u73b0\u65e9\u671f\u5065\u5eb7\u95ee\u9898\u7684\u53ca\u65f6\u68c0\u6d4b\u3002\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u9a6c\u53a9\u73af\u5883\u4e2d\u6301\u7eed\u3001\u81ea\u52a8\u5316\u7684\u884c\u4e3a\u76d1\u63a7\u9700\u6c42\uff0c\u4e9f\u9700\u5f00\u53d1\u667a\u80fd\u5316\u7684\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u91c7\u7528YOLOv11\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u548cBoT-SORT\u8fdb\u884c\u591a\u76ee\u6807\u8ddf\u8e2a\uff0c\u57fa\u4e8e\u76ee\u6807\u8f68\u8ff9\u548c\u7a7a\u95f4\u4f4d\u7f6e\u5173\u7cfb\u63a8\u65ad\u4e8b\u4ef6\u72b6\u6001\u3002\u4e3a\u652f\u6301\u5f00\u53d1\uff0c\u6784\u5efa\u4e86\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528CLIP\u548cGroundingDINO\u7b49\u57fa\u7840\u6a21\u578b\u8f85\u52a9\u6570\u636e\u6807\u6ce8\u3002\u7cfb\u7edf\u80fd\u591f\u8bc6\u522b\u4e94\u79cd\u4e8b\u4ef6\u7c7b\u578b\u5e76\u8003\u8651\u76f8\u673a\u76f2\u533a\u3002", "result": "\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u5728\u9a6c\u5339\u76f8\u5173\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u7a00\u7f3a\uff0c\u5728\u4eba\u5458\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u7cfb\u7edf\u6210\u529f\u533a\u5206\u4e86\u4e0d\u540c\u4e8b\u4ef6\u7c7b\u578b\uff0c\u5e76\u5728\u5b9e\u9645\u9a6c\u53a9\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9a6c\u5339\u8bbe\u65bd\u4e2d\u7684\u5b9e\u65f6\u884c\u4e3a\u76d1\u6d4b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5bf9\u52a8\u7269\u798f\u5229\u548c\u53a9\u820d\u7ba1\u7406\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u672a\u6765\u5de5\u4f5c\u5e94\u7740\u91cd\u89e3\u51b3\u4eba\u5458\u68c0\u6d4b\u7684\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f18\u5316\u7cfb\u7edf\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.17434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17434", "abs": "https://arxiv.org/abs/2510.17434", "authors": ["Julien Zouein", "Hossein Javidnia", "Fran\u00e7ois Piti\u00e9", "Anil Kokaram"], "title": "Leveraging AV1 motion vectors for Fast and Dense Feature Matching", "comment": "Accepted ICIR 2025, camera-ready version", "summary": "We repurpose AV1 motion vectors to produce dense sub-pixel correspondences\nand short tracks filtered by cosine consistency. On short videos, this\ncompressed-domain front end runs comparably to sequential SIFT while using far\nless CPU, and yields denser matches with competitive pairwise geometry. As a\nsmall SfM demo on a 117-frame clip, MV matches register all images and\nreconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows\nwith match density. These results show compressed-domain correspondences are a\npractical, resource-efficient front end with clear paths to scaling in full\npipelines.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5229\u7528AV1\u8fd0\u52a8\u5411\u91cf\u751f\u6210\u5bc6\u96c6\u4e9a\u50cf\u7d20\u5bf9\u5e94\u5173\u7cfb\u548c\u4f59\u5f26\u4e00\u81f4\u6027\u8fc7\u6ee4\u7684\u77ed\u8f68\u8ff9\uff0c\u4f5c\u4e3a\u538b\u7f29\u57df\u89c6\u89c9\u524d\u7aef\u3002\u8be5\u65b9\u6cd5\u5728\u77ed\u89c6\u9891\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u987a\u5e8fSIFT\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4eCPU\u4f7f\u7528\u91cf\uff0c\u4e3a\u5b8c\u6574\u89c6\u89c9\u7ba1\u9053\u63d0\u4f9b\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u524d\u7aef\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u524d\u7aef\u65b9\u6cd5\u5982SIFT\u5728\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u65b9\u9762\u6548\u7387\u8f83\u4f4e\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u89c6\u9891\u5e8f\u5217\u65f6\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u538b\u7f29\u57df\u5bf9\u5e94\u5173\u7cfb\u4f5c\u4e3a\u8d44\u6e90\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u7684\u53ef\u884c\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u91cd\u65b0\u5229\u7528AV1\u89c6\u9891\u7f16\u7801\u6807\u51c6\u4e2d\u7684\u8fd0\u52a8\u5411\u91cf\u6765\u751f\u6210\u5bc6\u96c6\u4e9a\u50cf\u7d20\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u4f59\u5f26\u4e00\u81f4\u6027\u8fc7\u6ee4\u77ed\u8f68\u8ff9\u3002\u8fd9\u79cd\u538b\u7f29\u57df\u524d\u7aef\u907f\u514d\u4e86\u4f20\u7edf\u7279\u5f81\u63d0\u53d6\u548c\u5339\u914d\u7684\u6602\u8d35\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u76f4\u63a5\u4ece\u538b\u7f29\u89c6\u9891\u6570\u636e\u4e2d\u63d0\u53d6\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u5728117\u5e27\u89c6\u9891\u7247\u6bb5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd0\u52a8\u5411\u91cf\u5339\u914d\u6210\u529f\u914d\u51c6\u6240\u6709\u56fe\u50cf\u5e76\u91cd\u5efa\u4e8646-62\u4e07\u4e2a\u4e09\u7ef4\u70b9\uff0c\u91cd\u6295\u5f71\u8bef\u5dee\u4e3a0.51-0.53\u50cf\u7d20\u3002\u8be5\u65b9\u6cd5\u5728\u77ed\u89c6\u9891\u4e0a\u8fd0\u884c\u6027\u80fd\u4e0e\u987a\u5e8fSIFT\u76f8\u5f53\uff0c\u4f46CPU\u4f7f\u7528\u91cf\u663e\u8457\u964d\u4f4e\uff0c\u4e14\u80fd\u4ea7\u751f\u66f4\u5bc6\u96c6\u7684\u5339\u914d\u70b9\u3002", "conclusion": "\u538b\u7f29\u57df\u5bf9\u5e94\u5173\u7cfb\u88ab\u8bc1\u660e\u662f\u5b9e\u7528\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u89c6\u89c9\u524d\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6e05\u6670\u7684\u6269\u5c55\u8def\u5f84\u3002\u8be5\u65b9\u6cd5\u4e3a\u5b8c\u6574\u89c6\u89c9\u7ba1\u9053\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u6216\u9700\u8981\u5904\u7406\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u7684\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.17611", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17611", "abs": "https://arxiv.org/abs/2510.17611", "authors": ["Jia Guo", "Shuai Lu", "Lei Fan", "Zelin Li", "Donglin Di", "Yang Song", "Weihang Zhang", "Wenbing Zhu", "Hong Yan", "Fang Chen", "Huiqi Li", "Hongen Liao"], "title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection", "comment": "Extended version of CVPR2025", "summary": "Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications.", "AI": {"tldr": "Dinomaly2\u63d0\u51fa\u4e86\u9996\u4e2a\u5168\u8c31\u56fe\u50cf\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u7b80\u5355\u5143\u7d20\u7684\u534f\u540c\u8bbe\u8ba1\u5728\u591a\u7c7b\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u7a81\u7834\u6027\u6027\u80fd\uff0c\u540c\u65f6\u65e0\u7f1d\u6269\u5c55\u5230\u591a\u79cd\u6570\u636e\u6a21\u6001\u548c\u4efb\u52a1\u8bbe\u7f6e\uff0c\u8bc1\u660e\u4e86\u7b80\u5355\u6027\u662f\u771f\u6b63\u901a\u7528\u6027\u7684\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u5b58\u5728\u591a\u7c7b\u6a21\u578b\u6027\u80fd\u663e\u8457\u843d\u540e\u4e8e\u4e00\u5bf9\u4e00\u4e13\u7528\u6a21\u578b\u7684\u95ee\u9898\uff0c\u4e14\u8be5\u9886\u57df\u5df2\u5206\u88c2\u4e3a\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u7684\u4e13\u95e8\u5316\u65b9\u6cd5\uff0c\u8fd9\u9020\u6210\u4e86\u90e8\u7f72\u969c\u788d\u5e76\u51f8\u663e\u4e86\u5bf9\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\u3002", "method": "Dinomaly2\u91c7\u7528\u57fa\u4e8e\u91cd\u6784\u7684\u6807\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u7b80\u5355\u5143\u7d20\u7684\u7cbe\u5fc3\u7f16\u6392\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\uff0c\u8fd9\u79cd\u65b9\u6cd5\u8bba\u4e0a\u7684\u6781\u7b80\u4e3b\u4e49\u4f7f\u5176\u80fd\u591f\u65e0\u9700\u4fee\u6539\u5373\u53ef\u81ea\u7136\u6269\u5c55\u5230\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u3002", "result": "\u572812\u4e2a\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDinomaly2\u5728\u591a\u6a21\u6001\u3001\u4efb\u52a1\u8bbe\u7f6e\u548c\u5e94\u7528\u9886\u57df\u5747\u5c55\u73b0\u51fa\u5168\u8c31\u4f18\u52bf\uff0c\u591a\u7c7b\u6a21\u578b\u5728MVTec-AD\u548cVisA\u4e0a\u5206\u522b\u8fbe\u5230\u524d\u6240\u672a\u6709\u768499.9%\u548c99.3%\u56fe\u50cf\u7ea7AUROC\uff0c\u4ec5\u4f7f\u7528\u6bcf\u7c7b8\u4e2a\u6b63\u5e38\u6837\u672c\u5373\u53ef\u8d85\u8d8a\u5148\u524d\u5168\u6837\u672c\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6781\u7b80\u8bbe\u8ba1\u3001\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u9002\u7528\u6027\u7684\u7ed3\u5408\u4f7fDinomaly2\u6210\u4e3a\u73b0\u5b9e\u4e16\u754c\u5f02\u5e38\u68c0\u6d4b\u5e94\u7528\u5168\u8c31\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u786e\u7acb\u4e86\u7b80\u5355\u6027\u4f5c\u4e3a\u771f\u6b63\u901a\u7528\u6027\u57fa\u7840\u7684\u91cd\u8981\u539f\u5219\u3002"}}
{"id": "2510.17686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17686", "abs": "https://arxiv.org/abs/2510.17686", "authors": ["Taichi Liu", "Zhenyu Wang", "Ruofeng Liu", "Guang Wang", "Desheng Zhang"], "title": "Towards 3D Objectness Learning in an Open World", "comment": "Accepted by NeurIPS 2025", "summary": "Recent advancements in 3D object detection and novel category detection have\nmade significant progress, yet research on learning generalized 3D objectness\nremains insufficient. In this paper, we delve into learning open-world 3D\nobjectness, which focuses on detecting all objects in a 3D scene, including\nnovel objects unseen during training. Traditional closed-set 3D detectors\nstruggle to generalize to open-world scenarios, while directly incorporating 3D\nopen-vocabulary models for open-world ability struggles with vocabulary\nexpansion and semantic overlap. To achieve generalized 3D object discovery, We\npropose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect\nany objects within 3D scenes without relying on hand-crafted text prompts. We\nintroduce the strong generalization and zero-shot capabilities of 2D foundation\nmodels, utilizing both 2D semantic priors and 3D geometric priors for\nclass-agnostic proposals to broaden 3D object discovery. Then, by integrating\ncomplementary information from point cloud and RGB image in the cross-modal\nmixture of experts, OP3Det dynamically routes uni-modal and multi-modal\nfeatures to learn generalized 3D objectness. Extensive experiments demonstrate\nthe extraordinary performance of OP3Det, which significantly surpasses existing\nopen-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement\ncompared to closed-world 3D detectors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOP3Det\uff0c\u4e00\u79cd\u65e0\u9700\u6587\u672c\u63d0\u793a\u7684\u5f00\u653e\u4e16\u754c3D\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u878d\u54082D\u8bed\u4e49\u5148\u9a8c\u548c3D\u51e0\u4f55\u5148\u9a8c\u5b9e\u73b0\u901a\u75283D\u7269\u4f53\u53d1\u73b0\uff0c\u5728\u5f00\u653e\u4e16\u754c3D\u68c0\u6d4b\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d3D\u7269\u4f53\u68c0\u6d4b\u548c\u65b0\u578b\u7c7b\u522b\u68c0\u6d4b\u867d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u901a\u75283D\u7269\u4f53\u6027\u5b66\u4e60\u7814\u7a76\u4e0d\u8db3\u3002\u4f20\u7edf\u95ed\u96c63D\u68c0\u6d4b\u5668\u96be\u4ee5\u6cdb\u5316\u5230\u5f00\u653e\u4e16\u754c\u573a\u666f\uff0c\u800c\u76f4\u63a5\u5f15\u51653D\u5f00\u653e\u8bcd\u6c47\u6a21\u578b\u53c8\u9762\u4e34\u8bcd\u6c47\u6269\u5c55\u548c\u8bed\u4e49\u91cd\u53e0\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u5f00\u653e\u4e16\u754c3D\u7269\u4f53\u6027\u5b66\u4e60\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51faOP3Det\u6846\u67b6\uff0c\u5229\u75282D\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u96f6\u6837\u672c\u80fd\u529b\uff0c\u7ed3\u54082D\u8bed\u4e49\u5148\u9a8c\u548c3D\u51e0\u4f55\u5148\u9a8c\u751f\u6210\u7c7b\u522b\u65e0\u5173\u7684\u7269\u4f53\u63d0\u8bae\u3002\u901a\u8fc7\u8de8\u6a21\u6001\u4e13\u5bb6\u6df7\u5408\u673a\u5236\uff0c\u52a8\u6001\u8def\u7531\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u7279\u5f81\uff0c\u4ece\u70b9\u4e91\u548cRGB\u56fe\u50cf\u4e2d\u5b66\u4e60\u901a\u75283D\u7269\u4f53\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eOP3Det\u8868\u73b0\u5353\u8d8a\uff0c\u5728AR\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u5f00\u653e\u4e16\u754c3D\u68c0\u6d4b\u5668\u8fbe16.0%\uff0c\u76f8\u6bd4\u95ed\u4e16\u754c3D\u68c0\u6d4b\u5668\u63d0\u534713.5%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5f00\u653e\u4e16\u754c3D\u7269\u4f53\u53d1\u73b0\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u878d\u54082D\u57fa\u7840\u6a21\u578b\u4e0e3D\u51e0\u4f55\u4fe1\u606f\u5728\u5f00\u653e\u4e16\u754c3D\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u901a\u75283D\u7269\u4f53\u6027\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u4e863D\u573a\u666f\u7406\u89e3\u5411\u66f4\u901a\u7528\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2510.17777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17777", "abs": "https://arxiv.org/abs/2510.17777", "authors": ["Samir Khaki", "Junxian Guo", "Jiaming Tang", "Shang Yang", "Yukang Chen", "Konstantinos N. Plataniotis", "Yao Lu", "Song Han", "Zhijian Liu"], "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference", "comment": null, "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.", "AI": {"tldr": "SparseVILA\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u9884\u586b\u5145\u9636\u6bb5\u526a\u679d\u5197\u4f59\u89c6\u89c9\u4ee4\u724c\u5e76\u5728\u89e3\u7801\u9636\u6bb5\u68c0\u7d22\u67e5\u8be2\u76f8\u5173\u4ee4\u724c\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u65e0\u5173\u3001\u67b6\u6784\u65e0\u5173\u7684\u52a0\u901f\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u53d7\u5230\u89c6\u89c9\u4ee4\u724c\u6570\u91cf\u5feb\u901f\u589e\u957f\u7684\u9650\u5236\uff0c\u8fd9\u4e9b\u89c6\u89c9\u4ee4\u724c\u4e3b\u5bfc\u4e86\u63a8\u7406\u5ef6\u8fdf\uff0c\u963b\u788d\u4e86\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7406\u89e3\u3001\u957f\u89c6\u9891\u5206\u6790\u548c\u591a\u8f6e\u5bf9\u8bdd\u7b49\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u6548\u7387\u3002", "method": "SparseVILA\u91c7\u7528\u89e3\u8026\u89c6\u89c9\u7a00\u758f\u6027\u7684\u8bbe\u8ba1\uff0c\u5728\u9884\u586b\u5145\u9636\u6bb5\u8fdb\u884c\u67e5\u8be2\u65e0\u5173\u7684\u4ee4\u724c\u526a\u679d\uff0c\u5728\u89e3\u7801\u9636\u6bb5\u8fdb\u884c\u67e5\u8be2\u611f\u77e5\u7684\u4ee4\u724c\u68c0\u7d22\uff0c\u57fa\u4e8eAWQ\u4f18\u5316\u7684\u63a8\u7406\u7ba1\u9053\u5b9e\u73b0\uff0c\u4fdd\u7559\u5927\u90e8\u5206\u89c6\u89c9\u7f13\u5b58\u4ee5\u786e\u4fdd\u591a\u8f6e\u5bf9\u8bdd\u7684\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u89c6\u9891\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e864.0\u500d\u9884\u586b\u5145\u52a0\u901f\u30012.5\u500d\u89e3\u7801\u52a0\u901f\u548c2.6\u500d\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u540c\u65f6\u5728\u6587\u6863\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u63d0\u5347\u4e86\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u534f\u540c\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u67e5\u8be2\u65e0\u5173\u526a\u679d\u548c\u67e5\u8be2\u611f\u77e5\u68c0\u7d22\uff0cSparseVILA\u4e3a\u9ad8\u6548\u591a\u6a21\u6001\u63a8\u7406\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u63d0\u4f9b\u4e86\u65e0\u9700\u8bad\u7ec3\u3001\u67b6\u6784\u65e0\u5173\u7684\u52a0\u901f\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u5927\u6a21\u578b\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002"}}
