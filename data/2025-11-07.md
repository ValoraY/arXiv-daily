<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 21]
- [cs.CL](#cs.CL) [Total: 6]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Simple 3D Pose Features Support Human and Machine Social Scene Understanding](https://arxiv.org/abs/2511.03988)
*Wenshuo Qin, Leyla Isik*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯æ˜äººç±»ç¤¾äº¤äº’åŠ¨è¯†åˆ«ä¾èµ–äº3Dè§†è§‰ç©ºé—´å§¿æ€ä¿¡æ¯ï¼Œé€šè¿‡æå–3Då…³èŠ‚ä½ç½®å’Œé¢éƒ¨æ–¹å‘ç­‰ç®€å•ç»“æ„åŒ–ç‰¹å¾ï¼Œèƒ½å¤Ÿè¶…è¶Šå½“å‰å…ˆè¿›AIè§†è§‰æ¨¡å‹çš„æ€§èƒ½ï¼Œæ­ç¤ºäº†ç¤¾äº¤åœºæ™¯ç†è§£çš„å…³é”®è®¡ç®—æœºåˆ¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** äººç±»èƒ½å¤Ÿå¿«é€Ÿä»è§†è§‰è¾“å…¥ä¸­æå–ç¤¾äº¤äº’åŠ¨ä¿¡æ¯ï¼Œä½†æ”¯æ’‘è¿™ç§èƒ½åŠ›çš„è®¡ç®—æœºåˆ¶å°šä¸æ¸…æ¥šï¼Œä¸”å½“å‰æœ€å…ˆè¿›çš„AIè§†è§‰ç³»ç»Ÿåœ¨ç¤¾äº¤äº’åŠ¨è¯†åˆ«æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹å¯¹3Dè§†è§‰ç©ºé—´å§¿æ€ä¿¡æ¯çš„åˆ©ç”¨ã€‚

**Method:** ç»“åˆæœ€å…ˆè¿›çš„å§¿æ€ä¼°è®¡å’Œæ·±åº¦ä¼°è®¡ç®—æ³•æå–è§†é¢‘ä¸­äººç‰©çš„3Då…³èŠ‚ä½ç½®ï¼Œå¹¶æ¨å¯¼å‡ºä¸€ç»„ç´§å‡‘çš„3Dç¤¾äº¤å§¿æ€ç‰¹å¾ï¼ŒåŒ…æ‹¬é¢éƒ¨çš„3Dä½ç½®å’Œæ–¹å‘ï¼Œå°†è¿™äº›ç‰¹å¾ä¸ç°æˆAIè§†è§‰æ¨¡å‹çš„åµŒå…¥è¡¨ç¤ºç›¸ç»“åˆè¿›è¡Œå¯¹æ¯”åˆ†æã€‚

**Result:** 3Då…³èŠ‚ä½ç½®åœ¨é¢„æµ‹äººç±»ç¤¾äº¤åˆ¤æ–­æ–¹é¢ä¼˜äºå¤§å¤šæ•°å½“å‰AIè§†è§‰æ¨¡å‹ï¼Œç®€åŒ–çš„3Dç¤¾äº¤å§¿æ€ç‰¹å¾ä¸å®Œæ•´å…³èŠ‚é›†å…·æœ‰ç›¸åŒçš„é¢„æµ‹èƒ½åŠ›ï¼Œä¸”å½“ä¸æ¨¡å‹åµŒå…¥ç»“åˆæ—¶æ˜¾è‘—æå‡äº†ç°æˆAIè§†è§‰æ¨¡å‹çš„æ€§èƒ½ï¼Œæ¨¡å‹å¯¹3Dç¤¾äº¤å§¿æ€ç‰¹å¾çš„è¡¨ç¤ºç¨‹åº¦ç›´æ¥é¢„æµ‹äº†å…¶åŒ¹é…äººç±»ç¤¾äº¤åˆ¤æ–­çš„èƒ½åŠ›ã€‚

**Conclusion:** ç ”ç©¶æä¾›äº†æœ‰åŠ›è¯æ®è¡¨æ˜äººç±»ç¤¾äº¤åœºæ™¯ç†è§£ä¾èµ–äº3Då§¿æ€çš„æ˜¾å¼è¡¨ç¤ºï¼Œå¯ä»¥é€šè¿‡ç®€å•ç»“æ„åŒ–çš„è§†è§‰ç©ºé—´åŸºå…ƒæ¥æ”¯æŒï¼Œè¿™ä¸ºæ”¹è¿›AIç¤¾äº¤æ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œå¼ºè°ƒäº†æ•´åˆæ˜¾å¼3Dç©ºé—´ä¿¡æ¯çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Humans can quickly and effortlessly extract a variety of information about
others' social interactions from visual input, ranging from visuospatial cues
like whether two people are facing each other to higher-level information. Yet,
the computations supporting these abilities remain poorly understood, and
social interaction recognition continues to challenge even the most advanced AI
vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose
information to make social interaction judgments, which is absent in most AI
vision models. To test this, we combined state-of-the-art pose and depth
estimation algorithms to extract 3D joint positions of people in short video
clips depicting everyday human actions and compared their ability to predict
human social interaction judgments with current AI vision models. Strikingly,
3D joint positions outperformed most current AI vision models, revealing that
key social information is available in explicit body position but not in the
learned features of most vision models, including even the layer-wise
embeddings of the pose models used to extract joint positions. To uncover the
critical pose features humans use to make social judgments, we derived a
compact set of 3D social pose features describing only the 3D position and
direction of faces in the videos. We found that these minimal descriptors
matched the predictive strength of the full set of 3D joints and significantly
improved the performance of off-the-shelf AI vision models when combined with
their embeddings. Moreover, the degree to which 3D social pose features were
represented in each off-the-shelf AI vision model predicted the model's ability
to match human social judgments. Together, our findings provide strong evidence
that human social scene understanding relies on explicit representations of 3D
pose and can be supported by simple, structured visuospatial primitives.


### [2] [CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation](https://arxiv.org/abs/2511.03992)
*Yuwen Tao, Kanglei Zhou, Xin Tan, Yuan Xie*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CaRFæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥é«˜æ–¯åœºç›¸æœºç¼–ç å’Œè®­ç»ƒé…å¯¹è§†å›¾ç›‘ç£ï¼Œè§£å†³äº†3Dé«˜æ–¯æº…å°„åˆ†å‰²ä¸­çš„å¤šè§†å›¾ä¸€è‡´æ€§é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰çš„3Dé«˜æ–¯æº…å°„åˆ†å‰²æ–¹æ³•ä¾èµ–2Dæ¸²æŸ“ä¼ªç›‘ç£å’Œè§†å›¾ç‰¹å®šç‰¹å¾å­¦ä¹ ï¼Œå¯¼è‡´è·¨è§†å›¾ä¸€è‡´æ€§ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†è‡ªç”±å½¢å¼è¯­è¨€è¡¨è¾¾ä¸3DåŒºåŸŸå®šä½ä¹‹é—´çš„å¯¹é½é—®é¢˜ã€‚

**Method:** CaRFæ¡†æ¶åŒ…å«é«˜æ–¯åœºç›¸æœºç¼–ç ï¼ˆGFCEï¼‰å°†ç›¸æœºå‡ ä½•èå…¥é«˜æ–¯æ–‡æœ¬äº¤äº’ä»¥å»ºæ¨¡è§†å›¾ä¾èµ–å˜åŒ–ï¼Œä»¥åŠè®­ç»ƒé…å¯¹è§†å›¾ç›‘ç£ï¼ˆITPVSï¼‰åœ¨è®­ç»ƒæœŸé—´å¯¹é½æ ¡å‡†è§†å›¾é—´çš„é«˜æ–¯é€»è¾‘å€¼ï¼Œç¼“è§£å•è§†å›¾è¿‡æ‹Ÿåˆå¹¶ä¼˜åŒ–è§†å›¾é—´å·®å¼‚ã€‚

**Result:** åœ¨Ref LERFã€LERF OVSå’Œ3D OVSä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒCaRFç›¸æ¯”æœ€å…ˆè¿›æ–¹æ³•åˆ†åˆ«å®ç°äº†16.8%ã€4.3%å’Œ2.0%çš„mIoUå¹³å‡æå‡ï¼Œæ˜¾è‘—æé«˜äº†3Dåœºæ™¯ç†è§£çš„å¯é æ€§å’Œè§†å›¾ä¸€è‡´æ€§ã€‚

**Conclusion:** è¯¥å·¥ä½œæ¨åŠ¨äº†æ›´å¯é å’Œè§†å›¾ä¸€è‡´çš„3Dåœºæ™¯ç†è§£ï¼Œå¯¹å…·èº«AIã€AR/VRäº¤äº’å’Œè‡ªä¸»æ„ŸçŸ¥å…·æœ‰æ½œåœ¨ç›Šå¤„ï¼Œä¸ºè·¨æ¨¡æ€3Då®šä½æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret
free-form language expressions and localize the corresponding 3D regions in
Gaussian fields. While recent advances have introduced cross-modal alignment
between language and 3D geometry, existing pipelines still struggle with
cross-view consistency due to their reliance on 2D rendered pseudo supervision
and view specific feature learning. In this work, we present Camera Aware
Referring Field (CaRF), a fully differentiable framework that operates directly
in the 3D Gaussian space and achieves multi view consistency. Specifically,
CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates
camera geometry into Gaussian text interactions to explicitly model view
dependent variations and enhance geometric reasoning. Building on this, In
Training Paired View Supervision (ITPVS) is proposed to align per Gaussian
logits across calibrated views during training, effectively mitigating single
view overfitting and exposing inter view discrepancies for optimization.
Extensive experiments on three representative benchmarks demonstrate that CaRF
achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of
the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.
Moreover, this work promotes more reliable and view consistent 3D scene
understanding, with potential benefits for embodied AI, AR/VR interaction, and
autonomous perception.


### [3] [MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging](https://arxiv.org/abs/2511.04016)
*Mahmoud Soliman, Islam Osman, Mohamed S. Shehata, Rasika Rajapakshe*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MedDChestï¼Œä¸€ç§ä¸“ä¸ºèƒ¸éƒ¨å½±åƒä¼˜åŒ–çš„åŸºç¡€è§†è§‰Transformeræ¨¡å‹ï¼Œé€šè¿‡åœ¨120ä¸‡å¼ å¤šæ¨¡æ€åŒ»å­¦å›¾åƒä¸Šè¿›è¡Œå¤§è§„æ¨¡é¢†åŸŸå†…é¢„è®­ç»ƒï¼Œå¹¶ç»“åˆæ–°é¢–çš„å¼•å¯¼éšæœºç¼©æ”¾è£å‰ªæ•°æ®å¢å¼ºç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†èƒ¸éƒ¨è¯Šæ–­ä»»åŠ¡çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŒ»å­¦å½±åƒè§†è§‰æ¨¡å‹çš„æ€§èƒ½å—åˆ°åœ¨è‡ªç„¶å›¾åƒä¸Šé¢„è®­ç»ƒçš„ä¸»å¹²ç½‘ç»œä¸åŒ»å­¦å½±åƒé¢†åŸŸå­˜åœ¨æ ¹æœ¬æ€§é¢†åŸŸå·®è·çš„é™åˆ¶ï¼Œè¿™ç§è·¨é¢†åŸŸè¿ç§»å­¦ä¹ èŒƒå¼ä¸¥é‡é˜»ç¢äº†æ¨¡å‹åœ¨åŒ»å­¦è¯Šæ–­ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

**Method:** æå‡ºäº†MedDCheståŸºç¡€ViTæ¨¡å‹ï¼Œåœ¨æ¥è‡ª10ä¸ªå…¬å¼€æ¥æºçš„120ä¸‡å¼ å¤šæ¨¡æ€èƒ¸éƒ¨å½±åƒï¼ˆåŒ…æ‹¬Xå…‰å’ŒCTï¼‰ä¸Šè¿›è¡Œä»å¤´é¢„è®­ç»ƒï¼Œå¹¶å¼€å‘äº†å¼•å¯¼éšæœºç¼©æ”¾è£å‰ªè¿™ä¸€æ–°é¢–çš„å†…å®¹æ„ŸçŸ¥æ•°æ®å¢å¼ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡åå‘é‡‡æ ·è§£å‰–å­¦ç›¸å…³åŒºåŸŸæ¥å…‹æœæ ‡å‡†è£å‰ªæŠ€æœ¯åœ¨åŒ»å­¦æ‰«æä¸­çš„ä½æ•ˆé—®é¢˜ã€‚

**Result:** ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒMedDCheståœ¨å¤šç§ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå…¬å¼€å¯ç”¨çš„ImageNeté¢„è®­ç»ƒæ¨¡å‹ï¼ŒéªŒè¯äº†å¤§è§„æ¨¡é¢†åŸŸå†…é¢„è®­ç»ƒç»“åˆé¢†åŸŸç‰¹å®šæ•°æ®å¢å¼ºç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** é€šè¿‡å»ºç«‹å¤§è§„æ¨¡é¢†åŸŸå†…é¢„è®­ç»ƒä¸é¢†åŸŸç‰¹å®šæ•°æ®å¢å¼ºç›¸ç»“åˆçš„ä¼˜åŠ¿ï¼ŒMedDChestæä¾›äº†ä¸€ä¸ªå¼ºå¤§ä¸”é²æ£’çš„ç‰¹å¾æå–å™¨ï¼Œä¸ºå¹¿æ³›çš„èƒ¸éƒ¨è¯Šæ–­ä»»åŠ¡æä¾›äº†æ˜¾è‘—æ›´å¥½çš„èµ·ç‚¹ï¼Œæ¨¡å‹æƒé‡å°†å…¬å¼€ä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶å’Œåº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
The performance of vision models in medical imaging is often hindered by the
prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain
natural images. To address this fundamental domain gap, we propose MedDChest, a
new foundational Vision Transformer (ViT) model optimized specifically for
thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated,
multimodal dataset of over 1.2 million images, encompassing different
modalities including Chest X-ray and Computed Tomography (CT) compiled from 10
public sources. A core technical contribution of our work is Guided Random
Resized Crops, a novel content-aware data augmentation strategy that biases
sampling towards anatomically relevant regions, overcoming the inefficiency of
standard cropping techniques on medical scans. We validate our model's
effectiveness by fine-tuning it on a diverse set of downstream diagnostic
tasks. Comprehensive experiments empirically demonstrate that MedDChest
significantly outperforms strong, publicly available ImageNet-pretrained
models. By establishing the superiority of large-scale, in-domain pre-training
combined with domain-specific data augmentation, MedDChest provides a powerful
and robust feature extractor that serves as a significantly better starting
point for a wide array of thoracic diagnostic tasks. The model weights will be
made publicly available to foster future research and applications.


### [4] [Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment](https://arxiv.org/abs/2511.04078)
*Zehui Feng, Chenqi Zhang, Mingru Wang, Minuo Wei, Shiwei Cheng, Cuntai Guan, Ting Han*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Bratrixï¼Œé¦–ä¸ªå®ç°å¤šæ¨¡æ€è¯­è¨€é”šå®šè§†è§‰-å¤§è„‘å¯¹é½çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œé€šè¿‡è§£è€¦è§†è§‰åˆºæ¿€ä¸ºå±‚æ¬¡åŒ–è§†è§‰å’Œè¯­è¨€è¯­ä¹‰ç»„ä»¶ï¼Œå°†è§†è§‰å’Œå¤§è„‘è¡¨å¾æŠ•å½±åˆ°å…±äº«æ½œåœ¨ç©ºé—´ï¼Œæ˜¾è‘—æå‡äº†ç¥ç»ä¿¡å·è§£ç æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•ä¸»è¦å°†ç¥ç»æ´»åŠ¨ç›´æ¥ä¸è§†è§‰åµŒå…¥å¯¹é½ï¼Œä½†çº¯è§†è§‰è¡¨ç¤ºå¾€å¾€æ— æ³•æ•æ‰æ½œåœ¨è¯­ä¹‰ç»´åº¦ï¼Œé™åˆ¶äº†å¯è§£é‡Šæ€§å’Œæ·±åº¦é²æ£’æ€§ï¼ŒåŒæ—¶ç¥ç»ä¿¡å·çš„å—è¯•è€…å˜å¼‚æ€§å’Œè§†è§‰ç‰¹å¾çº ç¼ é—®é¢˜æ„æˆäº†æ ¹æœ¬æ€§æŒ‘æˆ˜ã€‚

**Method:** Bratrixæ¡†æ¶å°†è§†è§‰åˆºæ¿€è§£è€¦ä¸ºå±‚æ¬¡åŒ–è§†è§‰å’Œè¯­è¨€è¯­ä¹‰ç»„ä»¶ï¼Œé‡‡ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¨¡å—å¤„ç†å™ªå£°ç¥ç»ä¿¡å·ï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„è¯­è¨€é”šå®šè¯­ä¹‰çŸ©é˜µå¢å¼ºè·¨æ¨¡æ€ç›¸å…³æ€§ï¼Œå¹¶é€šè¿‡å•æ¨¡æ€é¢„è®­ç»ƒå’Œå¤šæ¨¡æ€å¾®è°ƒçš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æå‡å¯¹é½ç²¾åº¦ã€‚

**Result:** åœ¨EEGã€MEGå’ŒfMRIåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒBratrixåœ¨æ£€ç´¢ã€é‡å»ºå’Œå­—å¹•ç”Ÿæˆä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨200è·¯EEGæ£€ç´¢ä»»åŠ¡ä¸­æ€§èƒ½æå‡14.3%ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†è¯­è¨€é”šå®šçš„å¤šæ¨¡æ€å¯¹é½åœ¨ç¥ç»ä¿¡å·è§£ç ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç†è§£è§†è§‰è¯­ä¹‰çš„ç¥ç»è¡¨å¾æä¾›äº†æ–°è§†è§’ï¼Œå¹¶ä¸ºå¼€å‘æ›´é²æ£’çš„è„‘æœºæ¥å£ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ï¼ŒåŒæ—¶ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¨¡å—æ¨¡æ‹Ÿäº†äººç±»æ„ŸçŸ¥çš„å¯é æ€§ç‰¹æ€§ã€‚

---

#### ğŸ“„ Abstract
Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI
remains a fundamental challenge due to subject variability and the entangled
nature of visual features. Existing approaches primarily align neural activity
directly with visual embeddings, but visual-only representations often fail to
capture latent semantic dimensions, limiting interpretability and deep
robustness. To address these limitations, we propose Bratrix, the first
end-to-end framework to achieve multimodal Language-Anchored Vision-Brain
alignment. Bratrix decouples visual stimuli into hierarchical visual and
linguistic semantic components, and projects both visual and brain
representations into a shared latent space, enabling the formation of aligned
visual-language and brain-language embeddings. To emulate human-like perceptual
reliability and handle noisy neural signals, Bratrix incorporates a novel
uncertainty perception module that applies uncertainty-aware weighting during
alignment. By leveraging learnable language-anchored semantic matrices to
enhance cross-modal correlations and employing a two-stage training strategy of
single-modality pretraining followed by multimodal fine-tuning, Bratrix-M
improves alignment precision. Extensive experiments on EEG, MEG, and fMRI
benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and
captioning performance compared to state-of-the-art methods, specifically
surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.


### [5] [SpatialLock: Precise Spatial Control in Text-to-Image Synthesis](https://arxiv.org/abs/2511.04112)
*Biao Liu, Yuanzhi Liang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSpatialLockæ¡†æ¶ï¼Œé€šè¿‡æ„ŸçŸ¥ä¿¡å·å’Œå®šä½ä¿¡æ¯çš„è”åˆæ§åˆ¶æ¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ç‰©ä½“å®šä½ä¸ç²¾ç¡®çš„é—®é¢˜ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†è¶…è¿‡0.9çš„IOUåˆ†æ•°ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç‰©ä½“å®šä½ç²¾åº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ–‡æœ¬åˆ°å›¾åƒåˆæˆè™½ç„¶åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆå›¾åƒä¸­å¯¹ç‰©ä½“å®šä½çš„ç²¾ç¡®æ§åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨ä½ç½®ä¿¡æ¯ï¼Œå¯¼è‡´å¯¹ç‰©ä½“ç©ºé—´å¸ƒå±€çš„ç†è§£ä¸è¶³ï¼Œè¿™é™åˆ¶äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œåº”ç”¨æ•ˆæœã€‚

**Method:** SpatialLockæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä½ç½®å‚ä¸æ³¨å…¥ï¼ˆPoIï¼‰å’Œä½ç½®å¼•å¯¼å­¦ä¹ ï¼ˆPoGï¼‰ã€‚PoIé€šè¿‡æ³¨æ„åŠ›å±‚ç›´æ¥æ•´åˆç©ºé—´ä¿¡æ¯ï¼Œæœ‰æ•ˆä¿ƒè¿›æ¨¡å‹å­¦ä¹ å®šä½ä¿¡æ¯ï¼›PoGé‡‡ç”¨åŸºäºæ„ŸçŸ¥çš„ç›‘ç£æ¥è¿›ä¸€æ­¥ä¼˜åŒ–ç‰©ä½“å®šä½ï¼Œè¿™ä¸¤ä¸ªç»„ä»¶å…±åŒå·¥ä½œä»¥å®ç°ç²¾ç¡®çš„ç©ºé—´æ’åˆ—æ§åˆ¶ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒSpatialLockåœ¨ç²¾ç¡®ç‰©ä½“å®šä½æ–¹é¢è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†è¶…è¿‡0.9çš„IOUåˆ†æ•°ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒçš„è§†è§‰è´¨é‡å’Œç‰©ä½“å®šä½ç²¾åº¦ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è”åˆåˆ©ç”¨æ„ŸçŸ¥ä¿¡å·å’Œå®šä½ä¿¡æ¯å¯ä»¥æœ‰æ•ˆè§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ç©ºé—´æ§åˆ¶é—®é¢˜ï¼Œä¸ºç²¾ç¡®ç‰©ä½“å®šä½æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºè‡ªåŠ¨æ•°æ®é›†ç”Ÿæˆç­‰åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚

---

#### ğŸ“„ Abstract
Text-to-Image (T2I) synthesis has made significant advancements in recent
years, driving applications such as generating datasets automatically. However,
precise control over object localization in generated images remains a
challenge. Existing methods fail to fully utilize positional information,
leading to an inadequate understanding of object spatial layouts. To address
this issue, we propose SpatialLock, a novel framework that leverages perception
signals and grounding information to jointly control the generation of spatial
locations. SpatialLock incorporates two components: Position-Engaged Injection
(PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial
information through an attention layer, encouraging the model to learn the
grounding information effectively. PoG employs perception-based supervision to
further refine object localization. Together, these components enable the model
to generate objects with precise spatial arrangements and improve the visual
quality of the generated images. Experiments show that SpatialLock sets a new
state-of-the-art for precise object positioning, achieving IOU scores above 0.9
across multiple datasets.


### [6] [Text to Sketch Generation with Multi-Styles](https://arxiv.org/abs/2511.04123)
*Tengjie Li, Shikui Tu, Lei Xu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å…è®­ç»ƒæ¡†æ¶M3Sï¼Œé€šè¿‡æ–‡æœ¬æç¤ºå’Œå‚è€ƒé£æ ¼è‰å›¾å®ç°ç²¾ç¡®çš„è‰å›¾é£æ ¼æ§åˆ¶ã€‚è¯¥æ–¹æ³•é‡‡ç”¨çº¿æ€§å¹³æ»‘å’Œé£æ ¼-å†…å®¹å¼•å¯¼æœºåˆ¶ï¼Œæœ‰æ•ˆå‡å°‘å‚è€ƒè‰å›¾çš„å†…å®¹æ³„æ¼ï¼Œå¹¶æ”¯æŒå¯æ§çš„å¤šé£æ ¼ç”Ÿæˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è‰å›¾ç”Ÿæˆæ–¹æ³•ä¸»è¦å…³æ³¨é€šç”¨åˆæˆï¼Œç¼ºä¹å¯¹è‰å›¾é£æ ¼çš„ç²¾ç¡®æ§åˆ¶æœºåˆ¶ã€‚åŸºäºé£æ ¼è¿ç§»çš„æ–¹æ³•åœ¨è‡ªæ³¨æ„åŠ›ä¸­è¦†ç›–é”®å€¼çŸ©é˜µä¼šå¯¼è‡´å†…å®¹æ³„æ¼é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å‚è€ƒè‰å›¾ä¸ç›®æ ‡è‰å›¾ç»“æ„ç›¸ä¼¼åº¦è¾ƒä½çš„æƒ…å†µä¸‹åˆæˆè´¨é‡ä¸‹é™ã€‚

**Method:** æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„å…è®­ç»ƒæ¡†æ¶ï¼Œå°†å‚è€ƒç‰¹å¾ä½œä¸ºè¾…åŠ©ä¿¡æ¯é€šè¿‡çº¿æ€§å¹³æ»‘èå…¥ï¼Œå¹¶åˆ©ç”¨é£æ ¼-å†…å®¹å¼•å¯¼æœºåˆ¶ã€‚é€šè¿‡è”åˆAdaINæ¨¡å—æ•´åˆå¤šä¸ªå‚è€ƒè‰å›¾çš„ç‰¹å¾ï¼Œæ”¯æŒå¯æ§çš„å¤šé£æ ¼ç”Ÿæˆï¼Œé¿å…ç›´æ¥è¦†ç›–è‡ªæ³¨æ„åŠ›é”®å€¼çŸ©é˜µã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜è´¨é‡çš„è‰å›¾ç”Ÿæˆï¼Œå…·æœ‰å‡†ç¡®çš„é£æ ¼å¯¹é½å’Œæ”¹å–„çš„é£æ ¼æ§åˆ¶çµæ´»æ€§ã€‚åœ¨å‚è€ƒè‰å›¾ä¸ç›®æ ‡è‰å›¾ç»“æ„ç›¸ä¼¼åº¦è¾ƒä½çš„æƒ…å†µä¸‹ï¼Œåˆæˆè´¨é‡å¾—åˆ°æ˜¾è‘—æå‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è‰å›¾é£æ ¼æ§åˆ¶æ–¹æ³•ï¼Œé€šè¿‡é¿å…å†…å®¹æ³„æ¼å’Œå¢å¼ºé£æ ¼-å†…å®¹åˆ†ç¦»ï¼Œä¸ºç²¾ç¡®çš„è‰å›¾é£æ ¼åˆæˆå¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚æ¡†æ¶çš„å…è®­ç»ƒç‰¹æ€§ä½¿å…¶å…·æœ‰è¾ƒå¥½çš„å®ç”¨æ€§å’Œæ‰©å±•æ€§ã€‚

---

#### ğŸ“„ Abstract
Recent advances in vision-language models have facilitated progress in sketch
generation. However, existing specialized methods primarily focus on generic
synthesis and lack mechanisms for precise control over sketch styles. In this
work, we propose a training-free framework based on diffusion models that
enables explicit style guidance via textual prompts and referenced style
sketches. Unlike previous style transfer methods that overwrite key and value
matrices in self-attention, we incorporate the reference features as auxiliary
information with linear smoothing and leverage a style-content guidance
mechanism. This design effectively reduces content leakage from reference
sketches and enhances synthesis quality, especially in cases with low
structural similarity between reference and target sketches. Furthermore, we
extend our framework to support controllable multi-style generation by
integrating features from multiple reference sketches, coordinated via a joint
AdaIN module. Extensive experiments demonstrate that our approach achieves
high-quality sketch generation with accurate style alignment and improved
flexibility in style control. The official implementation of M3S is available
at https://github.com/CMACH508/M3S.


### [7] [Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology](https://arxiv.org/abs/2511.04171)
*Fatemehzahra Darzi, Rodrigo Escobar Diaz Guerrero, Thomas Bocklitz*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸åŒé¢œè‰²å˜æ¢æŠ€æœ¯å¯¹æ•°å­—ç—…ç†å­¦ä¸­H&EæŸ“è‰²å›¾åƒä¸éçº¿æ€§å¤šæ¨¡æ€å›¾åƒé…å‡†æ€§èƒ½çš„å½±å“ï¼Œå‘ç°CycleGANé¢œè‰²å˜æ¢åœ¨ä¸¤ç§é…å‡†åœºæ™¯ä¸‹å‡å®ç°äº†æœ€ä½çš„é…å‡†è¯¯å·®ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å›¾åƒçš„å¯¹é½ç²¾åº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ•°å­—ç—…ç†å­¦ä¸­å¤šæ¨¡æ€å›¾åƒé…å‡†æ˜¯å®ç°ä¸åŒæŸ“è‰²æˆ–æˆåƒæ¨¡å¼ä¿¡æ¯ç›´æ¥æ¯”è¾ƒå’Œæ•´åˆçš„å…³é”®æ­¥éª¤ï¼Œä½†ä¸åŒæ¨¡æ€å›¾åƒé—´çš„é¢œè‰²å·®å¼‚ä¸¥é‡å½±å“äº†é…å‡†ç²¾åº¦ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å„ç§é¢œè‰²å˜æ¢æŠ€æœ¯å¦‚ä½•æ”¹å–„H&EæŸ“è‰²å›¾åƒä¸éçº¿æ€§å¤šæ¨¡æ€å›¾åƒä¹‹é—´çš„é…å‡†æ€§èƒ½ã€‚

**Method:** ç ”ç©¶ä½¿ç”¨20å¯¹ç»„ç»‡æ ·æœ¬æ•°æ®é›†ï¼Œå¯¹æ¯å¯¹å›¾åƒåº”ç”¨å¤šç§é¢„å¤„ç†æ­¥éª¤åŒ…æ‹¬CycleGANã€Macenkoã€Reinhardã€Vahadaneç­‰é¢œè‰²å˜æ¢æ–¹æ³•ï¼Œä»¥åŠåè½¬ã€å¯¹æ¯”åº¦è°ƒæ•´ã€å¼ºåº¦å½’ä¸€åŒ–å’Œå»å™ªå¤„ç†ï¼Œç„¶åé‡‡ç”¨VALISé…å‡†æ–¹æ³•è¿›è¡Œåˆšæ€§é…å‡†å’Œä¸¤æ­¥éåˆšæ€§é…å‡†ï¼Œåˆ†åˆ«åœ¨åŸå§‹å’Œåè½¬å¤šæ¨¡æ€å›¾åƒä¸¤ç§åœºæ™¯ä¸‹è¿›è¡Œé…å‡†è¯„ä¼°ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºåœ¨ä¸¤ç§é…å‡†åœºæ™¯ä¸‹ï¼ŒCycleGANé¢œè‰²å˜æ¢å‡å®ç°äº†æœ€ä½çš„é…å‡†è¯¯å·®ï¼Œè€Œå…¶ä»–æ–¹æ³•è¡¨ç°å‡ºè¾ƒé«˜çš„è¯¯å·®ï¼Œé€šè¿‡ç›¸å¯¹ç›®æ ‡é…å‡†è¯¯å·®ï¼ˆrTREï¼‰çš„ä¸­ä½æ•°ä¸­å€¼ï¼ˆMMrTREï¼‰å’Œå¹³å‡ä¸­å€¼ï¼ˆAMrTREï¼‰ä»¥åŠåŸºäºåä¸ªæ‰‹åŠ¨é€‰æ‹©å…³é”®ç‚¹çš„å®šåˆ¶ç‚¹è¯„ä¼°æ–¹æ³•éªŒè¯äº†é…å‡†æ€§èƒ½ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜åœ¨é…å‡†å‰åº”ç”¨é¢œè‰²å˜æ¢èƒ½å¤Ÿæ˜¾è‘—æ”¹å–„ä¸åŒæ¨¡æ€å›¾åƒé—´çš„å¯¹é½ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­—ç—…ç†å­¦åº”ç”¨ä¸­ï¼ŒCycleGANé¢œè‰²å˜æ¢ä¸ºå¤šæ¨¡æ€å›¾åƒé…å‡†æä¾›äº†æœ€æœ‰æ•ˆçš„é¢„å¤„ç†ç­–ç•¥ï¼Œæ”¯æŒæ›´å¯é çš„ç—…ç†åˆ†æã€‚

---

#### ğŸ“„ Abstract
Image registration refers to the process of spatially aligning two or more
images by mapping them into a common coordinate system, so that corresponding
anatomical or tissue structures are matched across images. In digital
pathology, registration enables direct comparison and integration of
information from different stains or imaging modalities, sup-porting
applications such as biomarker analysis and tissue reconstruction. Accurate
registration of images from different modalities is an essential step in
digital pathology. In this study, we investigated how various color
transformation techniques affect image registration between hematoxylin and
eosin (H&E) stained images and non-linear multimodal images. We used a dataset
of 20 tissue sample pairs, with each pair undergoing several preprocessing
steps, including different color transformation (CycleGAN, Macenko, Reinhard,
Vahadane), inversion, contrast adjustment, intensity normalization, and
denoising. All images were registered using the VALIS registration method,
which first applies rigid registration and then performs non-rigid registration
in two steps on both low and high-resolution images. Registration performance
was evaluated using the relative Target Registration Error (rTRE). We reported
the median of median rTRE values (MMrTRE) and the average of median rTRE values
(AMrTRE) for each method. In addition, we performed a custom point-based
evaluation using ten manually selected key points. Registration was done
separately for two scenarios, using either the original or inverted multimodal
images. In both scenarios, CycleGAN color transformation achieved the lowest
registration errors, while the other methods showed higher errors. These
findings show that applying color transformation before registration improves
alignment between images from different modalities and supports more reliable
analysis in digital pathology.


### [8] [DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification](https://arxiv.org/abs/2511.04281)
*Yujie Yang, Shuang Li, Jun Ye, Neng Dong, Fan Li, Huafeng Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºDINOv2çš„æ­¥æ€è¡¨å¾å­¦ä¹ æ¡†æ¶(DinoGRL)ï¼Œé€šè¿‡åˆ©ç”¨DINOv2çš„è§†è§‰å…ˆéªŒå­¦ä¹ ä¸å¤–è§‚ç‰¹å¾äº’è¡¥çš„æ­¥æ€ç‰¹å¾ï¼Œè§£å†³äº†è§†é¢‘å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«ä¸­è·¨æ¨¡æ€æ—¶ç©ºä¸€è‡´æ€§å»ºæ¨¡çš„æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†é¢‘å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡æ€ä¸å˜çš„å¤–è§‚ç‰¹å¾ï¼Œä½†å¿½è§†äº†æ­¥æ€ç‰¹å¾è¿™ä¸€ä¸ä»…æ¨¡æ€ä¸å˜ä¸”å¯Œå«æ—¶é—´åŠ¨æ€ä¿¡æ¯çš„é‡è¦çº¿ç´¢ï¼Œé™åˆ¶äº†è·¨æ¨¡æ€è§†é¢‘åŒ¹é…æ‰€éœ€çš„æ—¶ç©ºä¸€è‡´æ€§å»ºæ¨¡èƒ½åŠ›ã€‚

**Method:** æå‡ºäº†DinoGRLæ¡†æ¶ï¼ŒåŒ…å«è¯­ä¹‰æ„ŸçŸ¥è½®å»“ä¸æ­¥æ€å­¦ä¹ æ¨¡å‹(SASGL)å’Œæ¸è¿›å¼åŒå‘å¤šç²’åº¦å¢å¼ºæ¨¡å—(PBMGE)ï¼Œå‰è€…åˆ©ç”¨DINOv2ç”Ÿæˆè¯­ä¹‰å¢å¼ºçš„è½®å»“è¡¨å¾å¹¶ä¸é‡è¯†åˆ«ç›®æ ‡è”åˆä¼˜åŒ–ï¼Œåè€…é€šè¿‡æ­¥æ€ä¸å¤–è§‚æµçš„åŒå‘äº¤äº’åœ¨å¤šç©ºé—´ç²’åº¦ä¸Šæ¸è¿›ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºã€‚

**Result:** åœ¨HITSZ-VCMå’ŒBUPTæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨è·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ­¥æ€ç‰¹å¾ä¸å¤–è§‚ç‰¹å¾çš„äº’è¡¥æ€§å¯¹äºè·¨æ¨¡æ€è¡Œäººé‡è¯†åˆ«çš„é‡è¦æ€§ï¼Œé€šè¿‡ç»“åˆé€šç”¨è§†è§‰å…ˆéªŒå’Œæ¸è¿›å¼å¤šç²’åº¦ä¼˜åŒ–ï¼Œèƒ½å¤Ÿäº§ç”Ÿé«˜åº¦åˆ¤åˆ«æ€§çš„åºåˆ—çº§è¡¨å¾ï¼Œä¸ºè§†é¢‘è·¨æ¨¡æ€æ£€ç´¢æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Video-based Visible-Infrared person re-identification (VVI-ReID) aims to
retrieve the same pedestrian across visible and infrared modalities from video
sequences. Existing methods tend to exploit modality-invariant visual features
but largely overlook gait features, which are not only modality-invariant but
also rich in temporal dynamics, thus limiting their ability to model the
spatiotemporal consistency essential for cross-modal video matching. To address
these challenges, we propose a DINOv2-Driven Gait Representation Learning
(DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn
gait features complementary to appearance cues, facilitating robust
sequence-level representations for cross-modal retrieval. Specifically, we
introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which
generates and enhances silhouette representations with general-purpose semantic
priors from DINOv2 and jointly optimizes them with the ReID objective to
achieve semantically enriched and task-adaptive gait feature learning.
Furthermore, we develop a Progressive Bidirectional Multi-Granularity
Enhancement (PBMGE) module, which progressively refines feature representations
by enabling bidirectional interactions between gait and appearance streams
across multiple spatial granularities, fully leveraging their complementarity
to enhance global representations with rich local details and produce highly
discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets
demonstrate the superiority of our approach, significantly outperforming
existing state-of-the-art methods.


### [9] [RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation](https://arxiv.org/abs/2511.04317)
*Xiangjun Zhang, Litong Gong, Yinglin Zheng, Yansong Liu, Wentao Jiang, Mingyi Xu, Biao Wang, Tiezheng Ge, Ming Zeng*

#### ğŸ§© TL;DR
RISE-T2Væå‡ºäº†ä¸€ç§å°†æç¤ºé‡è¿°å’Œè¯­ä¹‰ç‰¹å¾æå–é›†æˆåˆ°å•ä¸€æµç¨‹ä¸­çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡é‡è¿°é€‚é…å™¨åˆ©ç”¨LLMçš„æ–‡æœ¬éšè—çŠ¶æ€ä½œä¸ºè§†é¢‘ç”Ÿæˆæ¡ä»¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹ç”¨æˆ·æ„å›¾çš„ç†è§£å’Œè§†é¢‘ç”Ÿæˆè´¨é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¾èµ–é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨è¿›è¡Œè¯­ä¹‰å¯¹é½ï¼Œä½†åœ¨å¤„ç†ç®€æ´æç¤ºæ—¶éš¾ä»¥ä¿æŒè§†é¢‘è´¨é‡ï¼Œä¸»è¦é—®é¢˜åœ¨äºæ–‡æœ¬è¯­ä¹‰ç†è§£èƒ½åŠ›æœ‰é™ä¸”æ— æ³•åœ¨çº¿é‡è¿°æç¤ºä»¥æ›´å¥½åœ°åŒ¹é…ç”¨æˆ·æ„å›¾ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œå¯ç”¨æ€§ã€‚

**Method:** æå‡ºäº†RISE-T2Væ¡†æ¶ï¼Œåˆ›æ–°æ€§åœ°å¼•å…¥é‡è¿°é€‚é…å™¨æ¨¡å—ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨LLMåœ¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹è¿‡ç¨‹ä¸­çš„æ–‡æœ¬éšè—çŠ¶æ€ä½œä¸ºè§†é¢‘ç”Ÿæˆæ¡ä»¶ï¼Œä»è€Œå°†æç¤ºé‡è¿°å’Œè¯­ä¹‰ç‰¹å¾æå–é›†æˆåˆ°å•ä¸€æµç¨‹ä¸­ï¼Œå®ç°ä»åŸºç¡€æç¤ºåˆ°æ›´å…¨é¢è¡¨ç¤ºçš„éšå¼è½¬æ¢ã€‚

**Result:** å¤§é‡å®éªŒè¯æ˜RISE-T2Væ˜¯ä¸€ä¸ªé€‚ç”¨äºä¸åŒè§†é¢‘æ‰©æ•£æ¨¡å‹æ¶æ„çš„é€šç”¨æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ç”Ÿæˆç¬¦åˆç”¨æˆ·æ„å›¾çš„é«˜è´¨é‡è§†é¢‘çš„èƒ½åŠ›ï¼ŒåŒæ—¶æ‰©å±•äº†æ¨¡å‹å®Œæˆæ›´å¹¿æ³›æ–‡æœ¬åˆ°è§†é¢‘ä»»åŠ¡çš„èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å°†æç¤ºé‡è¿°ä¸è¯­ä¹‰ç‰¹å¾æå–é›†æˆåˆ°å•ä¸€æµç¨‹çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡é‡è¿°é€‚é…å™¨å®ç°äº†å¯¹ç”¨æˆ·æ„å›¾çš„æ›´å¥½ç†è§£ï¼Œä¸ºæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆé¢†åŸŸæä¾›äº†å¯æ‰©å±•ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Most text-to-video(T2V) diffusion models depend on pre-trained text encoders
for semantic alignment, yet they often fail to maintain video quality when
provided with concise prompts rather than well-designed ones. The primary issue
lies in their limited textual semantics understanding. Moreover, these text
encoders cannot rephrase prompts online to better align with user intentions,
which limits both the scalability and usability of the models, To address these
challenges, we introduce RISE-T2V, which uniquely integrates the processes of
prompt rephrasing and semantic feature extraction into a single and seamless
step instead of two separate steps. RISE-T2V is universal and can be applied to
various pre-trained LLMs and video diffusion models(VDMs), significantly
enhancing their capabilities for T2V tasks. We propose an innovative module
called the Rephrasing Adapter, enabling diffusion models to utilize text hidden
states during the next token prediction of the LLM as a condition for video
generation. By employing a Rephrasing Adapter, the video generation model can
implicitly rephrase basic prompts into more comprehensive representations that
better match the user's intent. Furthermore, we leverage the powerful
capabilities of LLMs to enable video generation models to accomplish a broader
range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a
versatile framework applicable to different video diffusion model
architectures, significantly enhancing the ability of T2V models to generate
high-quality videos that align with user intent. Visual results are available
on the webpage at https://rise-t2v.github.io.


### [10] [Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection](https://arxiv.org/abs/2511.04347)
*Sanjay Kumar, Tim Brophy, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†ä¼ æ„Ÿå™¨é®æŒ¡å¯¹åŸºäºBEVFusionæ¶æ„çš„3Dç›®æ ‡æ£€æµ‹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°LiDARåœ¨é‡åº¦é®æŒ¡ä¸‹æ€§èƒ½æ€¥å‰§ä¸‹é™47.3%ï¼Œè€Œç›¸æœºåœ¨ä¸­ç­‰é®æŒ¡ä¸‹å³ä¸‹é™41.3%ï¼Œæ­ç¤ºäº†æ¨¡å‹å¯¹LiDARæ•°æ®çš„æ›´å¼ºä¾èµ–æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡åŸºäºé¸Ÿç°å›¾çš„å¤šæ¨¡æ€èåˆæ¶æ„åœ¨3Dç›®æ ‡æ£€æµ‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±é›¾éœ¾ã€ç‰©ç†éšœç¢ç­‰ç¯å¢ƒæ¡ä»¶å¼•èµ·çš„ä¼ æ„Ÿå™¨é®æŒ¡å¯¹æ£€æµ‹ç²¾åº¦çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œè¿™é™åˆ¶äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚ç°å®ç¯å¢ƒä¸­çš„å®‰å…¨å¯¼èˆªèƒ½åŠ›ã€‚

**Method:** é‡‡ç”¨BEVFusionæ¶æ„åœ¨nuScenesæ•°æ®é›†ä¸Šè¯„ä¼°ç›¸æœºå’Œæ¿€å…‰é›·è¾¾é®æŒ¡å¯¹3Dæ£€æµ‹æ€§èƒ½çš„å½±å“ï¼Œé€šè¿‡å°†å¤šä¼ æ„Ÿå™¨æ•°æ®æŠ•å½±åˆ°è‡ªä¸Šè€Œä¸‹çš„ç©ºé—´æ ¼å¼è¿›è¡Œèåˆï¼Œä½¿ç”¨å¹³å‡ç²¾åº¦å‡å€¼(mAP)å’ŒnuScenesæ£€æµ‹åˆ†æ•°(NDS)ä½œä¸ºæ€§èƒ½æŒ‡æ ‡ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸­ç­‰ç›¸æœºé®æŒ¡å¯¼è‡´ä»…åŸºäºç›¸æœºçš„æ£€æµ‹mAPä¸‹é™41.3%ï¼Œè€ŒLiDARä»…åœ¨é‡åº¦é®æŒ¡ä¸‹æ€§èƒ½æ€¥å‰§ä¸‹é™47.3%ï¼Œå¯¹è¿œè·ç¦»æ£€æµ‹å½±å“å°¤ä¸ºä¸¥é‡ï¼›åœ¨èåˆè®¾ç½®ä¸­ï¼Œç›¸æœºé®æŒ¡ä»…å¯¼è‡´4.1%çš„æ€§èƒ½ä¸‹é™ï¼Œè€ŒLiDARé®æŒ¡å¯¼è‡´26.8%çš„æ˜¾è‘—ä¸‹é™ï¼Œè¡¨æ˜æ¨¡å‹å¯¹LiDARæ•°æ®å…·æœ‰æ›´å¼ºçš„ä¾èµ–æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘é®æŒ¡æ„ŸçŸ¥è¯„ä¼°æ–¹æ³•å’Œæ”¹è¿›ä¼ æ„Ÿå™¨èåˆæŠ€æœ¯çš„å¿…è¦æ€§ï¼Œä»¥ç¡®ä¿åœ¨éƒ¨åˆ†ä¼ æ„Ÿå™¨å¤±æ•ˆæˆ–ç¯å¢ƒæ¡ä»¶æ¶åŒ–æ—¶ä»èƒ½ç»´æŒæ£€æµ‹ç²¾åº¦ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é²æ£’æ„ŸçŸ¥æä¾›äº†é‡è¦æŒ‡å¯¼æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Accurate 3D object detection is essential for automated vehicles to navigate
safely in complex real-world environments. Bird's Eye View (BEV)
representations, which project multi-sensor data into a top-down spatial
format, have emerged as a powerful approach for robust perception. Although
BEV-based fusion architectures have demonstrated strong performance through
multimodal integration, the effects of sensor occlusions, caused by
environmental conditions such as fog, haze, or physical obstructions, on 3D
detection accuracy remain underexplored. In this work, we investigate the
impact of occlusions on both camera and Light Detection and Ranging (LiDAR)
outputs using the BEVFusion architecture, evaluated on the nuScenes dataset.
Detection performance is measured using mean Average Precision (mAP) and the
nuScenes Detection Score (NDS). Our results show that moderate camera
occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is
based only on the camera. On the other hand, LiDAR sharply drops in performance
only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),
with a severe impact on long-range detection. In fused settings, the effect
depends on which sensor is occluded: occluding the camera leads to a minor 4.1%
drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%
drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task
of 3D object detection. Our results highlight the need for future research into
occlusion-aware evaluation methods and improved sensor fusion techniques that
can maintain detection accuracy in the presence of partial sensor failure or
degradation due to adverse environmental conditions.


### [11] [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)
*Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡º'è§†é¢‘æ€ç»´'æ–°èŒƒå¼ï¼Œåˆ©ç”¨Sora-2ç­‰è§†é¢‘ç”Ÿæˆæ¨¡å‹ç»Ÿä¸€è§†è§‰ä¸æ–‡æœ¬æ¨ç†ï¼Œé€šè¿‡VideoThinkBenchåŸºå‡†æµ‹è¯•è¯æ˜è§†é¢‘ç”Ÿæˆæ¨¡å‹å…·å¤‡å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰'æ–‡æœ¬æ€ç»´'å’Œ'å›¾åƒæ€ç»´'èŒƒå¼å­˜åœ¨å›ºæœ‰å±€é™ï¼šå›¾åƒä»…èƒ½æ•æ‰å•ä¸€æ—¶åˆ»è€Œæ— æ³•è¡¨ç¤ºåŠ¨æ€è¿‡ç¨‹æˆ–è¿ç»­å˜åŒ–ï¼Œä¸”æ–‡æœ¬ä¸è§†è§‰ä½œä¸ºåˆ†ç¦»æ¨¡æ€é˜»ç¢äº†ç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚

**Method:** æå‡º'è§†é¢‘æ€ç»´'èŒƒå¼ï¼Œåˆ©ç”¨Sora-2ç­‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç»Ÿä¸€çš„æ—¶é—´æ¡†æ¶ä¸­æ¡¥æ¥è§†è§‰ä¸æ–‡æœ¬æ¨ç†ï¼Œå¹¶å¼€å‘VideoThinkBenchåŸºå‡†ï¼ŒåŒ…å«è§†è§‰ä¸­å¿ƒä»»åŠ¡ï¼ˆå¦‚ç›®æµ‹è°œé¢˜ï¼‰å’Œæ–‡æœ¬ä¸­å¿ƒä»»åŠ¡ï¼ˆå¦‚GSM8Kã€MMMUå­é›†ï¼‰ã€‚

**Result:** Sora-2åœ¨è§†è§‰ä¸­å¿ƒä»»åŠ¡ä¸Šè¡¨ç°ä¸æœ€å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹ç›¸å½“ï¼Œåœ¨ç›®æµ‹æ¸¸æˆç­‰ä»»åŠ¡ä¸­ç”šè‡³è¶…è¶ŠVLMsï¼›åœ¨æ–‡æœ¬ä¸­å¿ƒä»»åŠ¡ä¸Šï¼ŒMATHå‡†ç¡®ç‡è¾¾92%ï¼ŒMMMUå‡†ç¡®ç‡è¾¾75.53%ï¼›è‡ªä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡å­¦ä¹ å¯è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚

**Conclusion:** è§†é¢‘ç”Ÿæˆæ¨¡å‹å…·å¤‡æˆä¸ºç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹çš„æ½œåŠ›ï¼Œ'è§†é¢‘æ€ç»´'å¯ä½œä¸ºä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†èŒƒå¼ï¼Œä¸ºäººå·¥æ™ºèƒ½æ¨ç†èƒ½åŠ›çš„å‘å±•å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
"Thinking with Text" and "Thinking with Images" paradigm significantly
improve the reasoning ability of large language models (LLMs) and Vision
Language Models (VLMs). However, these paradigms have inherent limitations. (1)
Images capture only single moments and fail to represent dynamic processes or
continuous changes, and (2) The separation of text and vision as distinct
modalities, hindering unified multimodal understanding and generation. To
overcome these limitations, we introduce "Thinking with Video", a new paradigm
that leverages video generation models, such as Sora-2, to bridge visual and
textual reasoning in a unified temporal framework. To support this exploration,
we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench
encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing
Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our
evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,
Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even
surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric
tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.
Furthermore, we systematically analyse the source of these abilities. We also
find that self-consistency and in-context learning can improve Sora-2's
performance. In summary, our findings demonstrate that the video generation
model is the potential unified multimodal understanding and generation model,
positions "thinking with video" as a unified multimodal reasoning paradigm.


### [12] [Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA](https://arxiv.org/abs/2511.04384)
*Itbaan Safwan, Muhammad Annas Shaikh, Muhammad Haaris, Ramail Khan, Muhammad Atif Tahir*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºLoRAå¾®è°ƒFlorence-2æ¨¡å‹çš„å¤šä»»åŠ¡æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦è§†è§‰é—®ç­”ã€è§£é‡Šç”Ÿæˆå’Œè§†è§‰å®šä½ä»»åŠ¡ï¼Œæ˜¾è‘—æå‡äº†åŒ»å­¦VQAåº”ç”¨çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŒ»å­¦è§†è§‰é—®ç­”ç³»ç»Ÿé€šå¸¸ç‹¬ç«‹å¤„ç†ä¸åŒä»»åŠ¡ï¼Œç¼ºä¹å¯¹è§†è§‰å®šä½ã€æ¨ç†å’Œè§£é‡Šçš„è”åˆå­¦ä¹ èƒ½åŠ›ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›ç­”å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ä¸è¶³ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶å­¦ä¹ è§†è§‰å®šä½ã€åŒ»å­¦æ¨ç†å’Œè§£é‡Šç”Ÿæˆã€‚

**Method:** é‡‡ç”¨LoRAå¾®è°ƒçš„Florence-2æ¨¡å‹æ„å»ºå¤šä»»åŠ¡æ¡†æ¶ï¼Œæ•´åˆä¸‰ä¸ªç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ï¼šKvasir-VQA-x1ç”¨äºé—®ç­”å­¦ä¹ ï¼Œåˆæˆå¢å¼ºçš„è§£é‡Šæ•°æ®é›†æä¾›ç»“æ„åŒ–åŒ»å­¦æ¨ç†ï¼Œä»¥åŠæ–‡æœ¬åˆ°åŒºåŸŸå¯¹å°†è§†è§‰ç‰¹å¾ä¸åˆ†å‰²æ©ç å…³è”ã€‚è¯¥æ¡†æ¶æ”¯æŒè§†è§‰é—®ç­”ã€è§£é‡Šç”Ÿæˆå’Œè§†è§‰å®šä½çš„è”åˆè®­ç»ƒã€‚

**Result:** å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç­”æ¡ˆå‡†ç¡®æ€§å’Œè§†è§‰å®šä½æ–¹é¢æ˜¾è‘—ä¼˜äºå•ä»»åŠ¡åŸºçº¿ï¼Œå¤šä»»åŠ¡å­¦ä¹ æœ‰æ•ˆæå‡äº†åŒ»å­¦VQAä»»åŠ¡çš„æ€§èƒ½è¡¨ç°å’Œå®šä½ç²¾åº¦ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜åŸºäºgroundedçš„å¤šä»»åŠ¡å­¦ä¹ åœ¨åŒ»å­¦VQAåº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿç”Ÿæˆæ—¢å‡†ç¡®åˆå¯è§£é‡Šçš„å“åº”ï¼Œä¸ºåŒ»å­¦AIç³»ç»Ÿæä¾›äº†æ›´å¯é çš„å†³ç­–æ”¯æŒæ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
We present a multi-task framework for the MediaEval Medico 2025 challenge,
leveraging a LoRA-tuned Florence-2 model for simultaneous visual question
answering (VQA), explanation generation, and visual grounding. The proposed
system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer
learning, (2) a synthetically enriched explanation dataset offering structured
medical reasoning, and (3) text-to-region pairs linking visual features with
segmentation masks. This multi-task setup enables the model to jointly learn
visual grounding, reasoning, and interpretation, producing responses that are
both accurate and interpretable. Extensive evaluation demonstrates that our
approach substantially improves over single-task baselines in both answer
accuracy and visual localization, highlighting the effectiveness of grounded
multi-task learning for medical VQA applications.


### [13] [V-Thinker: Interactive Thinking with Images](https://arxiv.org/abs/2511.04460)
*Runqi Qiao, Qiuna Tan, Minghan Yang, Guanting Dong, Peiqing Yang, Shiqiang Lang, Enhui Wan, Xiaowan Wang, Yida Xu, Lan Yang, Chong Sun, Chen Li, Honggang Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†V-Thinkerï¼Œä¸€ç§é€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ å®ç°äº¤äº’å¼è§†è§‰ä¸­å¿ƒæ¨ç†çš„é€šç”¨å¤šæ¨¡æ€æ¨ç†åŠ©æ‰‹ï¼ŒåŒ…å«æ•°æ®è¿›åŒ–é£è½®å’Œè§†è§‰æ¸è¿›è®­ç»ƒè¯¾ç¨‹ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œåœ¨è§†è§‰äº¤äº’æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰LMMåŸºçº¿ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ·±åº¦æ•´åˆå›¾åƒäº¤äº’ä¸é•¿ç¨‹æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™ï¼Œè™½ç„¶'å›¾åƒæ€ç»´'èŒƒå¼å®ç°äº†ä»å›¾åƒè¾…åŠ©æ¨ç†åˆ°å›¾åƒäº¤äº’æ€ç»´çš„è½¬å˜ï¼Œä½†è¿›å±•å—åˆ°æœ‰é™è§†è§‰å·¥å…·ç©ºé—´å’Œä»»åŠ¡ç‰¹å®šå·¥ä½œæµè®¾è®¡çš„åˆ¶çº¦ã€‚

**Method:** V-Thinkeré‡‡ç”¨æ•°æ®è¿›åŒ–é£è½®è‡ªåŠ¨åˆæˆã€è¿›åŒ–å’ŒéªŒè¯äº¤äº’å¼æ¨ç†æ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·æ€§ã€è´¨é‡å’Œéš¾åº¦ä¸‰ä¸ªç»´åº¦ï¼›å¹¶é€šè¿‡è§†è§‰æ¸è¿›è®­ç»ƒè¯¾ç¨‹ï¼Œé¦–å…ˆé€šè¿‡ç‚¹çº§ç›‘ç£å¯¹é½æ„ŸçŸ¥ï¼Œç„¶åé€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ•´åˆäº¤äº’æ¨ç†ã€‚

**Result:** åœ¨VTBenchä¸“å®¶éªŒè¯åŸºå‡†æµ‹è¯•ä¸­ï¼ŒV-Thinkeråœ¨é€šç”¨å’Œäº¤äº’æ¨ç†åœºæ™¯ä¸‹å‡æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„LMMåŸºçº¿æ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨è§†è§‰ä¸­å¿ƒäº¤äº’æ¨ç†ä»»åŠ¡ä¸Šå…·æœ‰ä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** V-Thinkerä¸ºæ¨è¿›å›¾åƒäº¤äº’æ¨ç†åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå±•ç¤ºäº†ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ åœ¨æ„å»ºé€šç”¨å¤šæ¨¡æ€æ¨ç†åŠ©æ‰‹æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºè§†è§‰ä¸­å¿ƒäº¤äº’æ¨ç†çš„å‘å±•å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Empowering Large Multimodal Models (LMMs) to deeply integrate image
interaction with long-horizon reasoning capabilities remains a long-standing
challenge in this field. Recent advances in vision-centric reasoning explore a
promising "Thinking with Images" paradigm for LMMs, marking a shift from
image-assisted reasoning to image-interactive thinking. While this milestone
enables models to focus on fine-grained image regions, progress remains
constrained by limited visual tool spaces and task-specific workflow designs.
To bridge this gap, we present V-Thinker, a general-purpose multimodal
reasoning assistant that enables interactive, vision-centric thinking through
end-to-end reinforcement learning. V-Thinker comprises two key components: (1)
a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies
interactive reasoning datasets across three dimensions-diversity, quality, and
difficulty; and (2) a Visual Progressive Training Curriculum that first aligns
perception via point-level supervision, then integrates interactive reasoning
through a two-stage reinforcement learning framework. Furthermore, we introduce
VTBench, an expert-verified benchmark targeting vision-centric interactive
reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently
outperforms strong LMM-based baselines in both general and interactive
reasoning scenarios, providing valuable insights for advancing
image-interactive reasoning applications.


### [14] [Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy](https://arxiv.org/abs/2511.04525)
*Dimitrios Anastasiou, Santiago Barbarisi, Lucy Culshaw, Jayna Patel, Evangelos B. Mazomenos, Imanol Luengo, Danail Stoyanov*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºSTC-Netæ¡†æ¶ï¼Œç”¨äºåŸºäºParklandåˆ†çº§æ ‡å‡†çš„è…¹è…”é•œèƒ†å›Šåˆ‡é™¤æœ¯æ‰‹æœ¯å¤æ‚åº¦è‡ªåŠ¨è¯„ä¼°ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å¼±æ—¶é—´ç›‘ç£ä¸‹ç›´æ¥ä»å®Œæ•´æ‰‹æœ¯è§†é¢‘ä¸­åŒæ—¶æ‰§è¡Œæ—¶é—´å®šä½å’Œåˆ†çº§ä»»åŠ¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è…¹è…”é•œèƒ†å›Šåˆ‡é™¤æœ¯ä¸­å‡†ç¡®è¯„ä¼°æ‰‹æœ¯å¤æ‚åº¦è‡³å…³é‡è¦ï¼Œä¸¥é‡ç‚ç—‡ä¸æ›´é•¿æ‰‹æœ¯æ—¶é—´å’Œæ›´é«˜å¹¶å‘ç—‡é£é™©ç›¸å…³ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å±€é™äºé™æ€å›¾åƒæˆ–æ‰‹åŠ¨ä¿®å‰ªçš„è§†é¢‘ç‰‡æ®µï¼Œæ— æ³•åœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹åˆ†æå®Œæ•´æ‰‹æœ¯è§†é¢‘ã€‚

**Method:** STC-Netæ¡†æ¶åŒ…å«å®šä½ã€çª—å£æè®®å’Œåˆ†çº§ä¸‰ä¸ªæ¨¡å—ï¼Œé€šè¿‡ç»“åˆç¡¬å®šä½å’Œè½¯å®šä½ç›®æ ‡çš„æ–°å‹æŸå¤±å‡½æ•°ä»¥åŠèƒŒæ™¯æ„ŸçŸ¥åˆ†çº§ç›‘ç£ï¼Œåœ¨å¼±æ—¶é—´ç›‘ç£ä¸‹ç›´æ¥ä»å®Œæ•´è§†é¢‘ä¸­è”åˆæ‰§è¡Œæ—¶é—´å®šä½å’Œåˆ†çº§ä»»åŠ¡ã€‚

**Result:** åœ¨1,859ä¸ªLCè§†é¢‘çš„ç§æœ‰æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒSTC-Netè¾¾åˆ°62.11%çš„å‡†ç¡®ç‡å’Œ61.42%çš„F1åˆ†æ•°ï¼Œç›¸æ¯”éå®šä½åŸºçº¿æ–¹æ³•åœ¨ä¸¤ä¸ªæŒ‡æ ‡ä¸Šå‡æå‡è¶…è¿‡10%ï¼Œè¯æ˜äº†å¼±ç›‘ç£åœ¨æ‰‹æœ¯å¤æ‚åº¦è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** STC-Netå±•ç¤ºäº†ä»å®Œæ•´LCè§†é¢‘ä¸­è‡ªåŠ¨è¿›è¡ŒåŸºäºPGSçš„æ‰‹æœ¯å¤æ‚åº¦è¯„ä¼°çš„å¯æ‰©å±•æœ‰æ•ˆæ–¹æ³•ï¼Œä¸ºæœ¯ååˆ†æå’Œæ‰‹æœ¯è®­ç»ƒæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†æ‰‹æœ¯è§†é¢‘åˆ†æå‘æ›´å®ç”¨åœºæ™¯çš„åº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
Purpose: Accurate assessment of surgical complexity is essential in
Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with
longer operative times and increased risk of postoperative complications. The
Parkland Grading Scale (PGS) provides a clinically validated framework for
stratifying inflammation severity; however, its automation in surgical videos
remains largely unexplored, particularly in realistic scenarios where complete
videos must be analyzed without prior manual curation. Methods: In this work,
we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity
estimation in LC via the PGS, designed to operate under weak temporal
supervision. Unlike prior methods limited to static images or manually trimmed
clips, STC-Net operates directly on full videos. It jointly performs temporal
localization and grading through a localization, window proposal, and grading
module. We introduce a novel loss formulation combining hard and soft
localization objectives and background-aware grading supervision. Results:
Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy
of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by
over 10% in both metrics and highlighting the effectiveness of weak supervision
for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable
and effective approach for automated PGS-based surgical complexity estimation
from full LC videos, making it promising for post-operative analysis and
surgical training.


### [15] [PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning](https://arxiv.org/abs/2511.04601)
*Yicheng Xiao, Yu Chen, Haoxuan Ma, Jiale Hong, Caorui Li, Lingxiang Wu, Haiyun Guo, Jinqiao Wang*

#### ğŸ§© TL;DR
PixCLIPæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡åŒæ—¶å¤„ç†è§†è§‰æç¤ºè¾“å…¥å’Œé•¿æ–‡æœ¬æè¿°æ¥è§£å†³CLIPæ¨¡å‹åœ¨ç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½æ–¹é¢çš„å±€é™æ€§ï¼Œå®ç°äº†åƒç´ çº§äº¤äº’å’Œé•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›çš„çªç ´ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰CLIPæ¨¡å‹åœ¨ç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å…¶æ–‡æœ¬ç¼–ç å™¨çš„å›ºæœ‰tokené•¿åº¦é™åˆ¶é˜»ç¢äº†å¤„ç†åµŒå…¥é•¿æ–‡æœ¬åºåˆ—ä¸­çš„æ›´ç»†ç²’åº¦æ–‡æœ¬ä¿¡æ¯ï¼Œè€Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨é•¿è€Œè¯¦ç»†çš„æ–‡æœ¬æè¿°è®­ç»ƒå¯ä»¥æœ‰æ•ˆæ”¹å–„æ¨¡å‹çš„ç»†ç²’åº¦è§†è§‰-è¯­è¨€å¯¹é½èƒ½åŠ›ã€‚

**Method:** é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªèƒ½å¤Ÿä¸ºå›¾åƒç”Ÿæˆåƒç´ çº§å®šä½ã€é•¿å½¢å¼æ–‡æœ¬æè¿°çš„è‡ªåŠ¨æ ‡æ³¨æµç¨‹ï¼Œå¹¶æ„å»ºäº†åŒ…å«è¿‘150ä¸‡ä¸ªæ ·æœ¬çš„é«˜è´¨é‡æ•°æ®é›†LongGRITï¼›å…¶æ¬¡å°†CLIPçš„åŸå§‹æ–‡æœ¬ç¼–ç å™¨æ›¿æ¢ä¸ºLLMï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªä¸‰åˆ†æ”¯åƒç´ -æ–‡æœ¬å¯¹é½å­¦ä¹ æ¡†æ¶ï¼Œä¿ƒè¿›å›¾åƒåŒºåŸŸä¸ç›¸åº”æ–‡æœ¬æè¿°åœ¨ä»»æ„ç²’åº¦ä¸Šçš„ç»†ç²’åº¦å¯¹é½ã€‚

**Result:** å®éªŒè¡¨æ˜PixCLIPåœ¨åƒç´ çº§äº¤äº’å’Œå¤„ç†é•¿æ–‡æœ¬æ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åŒæ—¶å¢å¼ºè§†è§‰å’Œæ–‡æœ¬å†…å®¹å¤„ç†ç²’åº¦çš„ååŒä¼˜åŠ¿ï¼Œä¸ºç»†ç²’åº¦è§†è§‰-è¯­è¨€å¯¹é½æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºåƒç´ çº§äº¤äº’å’Œé•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›è®¾å®šäº†æ–°çš„æŠ€æœ¯æ ‡å‡†ã€‚

---

#### ğŸ“„ Abstract
While the Contrastive Language-Image Pretraining(CLIP) model has achieved
remarkable success in a variety of downstream vison language understanding
tasks, enhancing its capability for fine-grained image-text alignment remains
an active research focus. To this end, most existing works adopt the strategy
of explicitly increasing the granularity of visual information processing,
e.g., incorporating visual prompts to guide the model focus on specific local
regions within the image. Meanwhile, researches on Multimodal Large Language
Models(MLLMs) have demonstrated that training with long and detailed textual
descriptions can effectively improve the model's fine-grained vision-language
alignment. However, the inherent token length limitation of CLIP's text encoder
fundamentally limits CLIP to process more granular textual information embedded
in long text sequences. To synergistically leverage the advantages of enhancing
both visual and textual content processing granularity, we propose PixCLIP, a
novel framework designed to concurrently accommodate visual prompt inputs and
process lengthy textual descriptions. Specifically, we first establish an
automated annotation pipeline capable of generating pixel-level localized,
long-form textual descriptions for images. Utilizing this pipeline, we
construct LongGRIT, a high-quality dataset comprising nearly 1.5 million
samples. Secondly, we replace CLIP's original text encoder with the LLM and
propose a three-branch pixel-text alignment learning framework, facilitating
fine-grained alignment between image regions and corresponding textual
descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP
showcases breakthroughs in pixel-level interaction and handling long-form
texts, achieving state-of-the-art performance.


### [16] [NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment](https://arxiv.org/abs/2511.04628)
*Kylie Cancilla, Alexander Moore, Amar Saini, Carmen Carrano*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æµå¼è§†é¢‘è´¨é‡è¯„ä¼°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ— éœ€å‚è€ƒè§†é¢‘ä¸”æ— éœ€äººå·¥æ ‡æ³¨ï¼Œé€šè¿‡æ—¶é—´æ„ŸçŸ¥å·ç§¯æ¶æ„ç›´æ¥é¢„æµ‹å…¨å‚è€ƒæŒ‡æ ‡ï¼Œåœ¨DAVISæ•°æ®é›†ä¸Šé€šè¿‡åˆæˆé€€åŒ–è®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘è´¨é‡è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†é¢‘è´¨é‡è¯„ä¼°æ–¹æ³•é¢ä¸´é‡å¤§é™åˆ¶ï¼šå…¨å‚è€ƒæŒ‡æ ‡éœ€è¦å¹²å‡€çš„å‚è€ƒè§†é¢‘ï¼Œè€Œå¤§å¤šæ•°æ— å‚è€ƒæ¨¡å‹ä¾èµ–æ˜‚è´µçš„äººå·¥æ ‡æ³¨æ•°æ®ï¼Œä¸”å¤šæ•°æ— æ„è§æ„ŸçŸ¥çš„æ— å‚è€ƒæ–¹æ³•åŸºäºå›¾åƒå¤„ç†ï¼Œå¿½ç•¥äº†è§†é¢‘ç›®æ ‡æ£€æµ‹ä¸­è‡³å…³é‡è¦çš„æ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

**Method:** è¯¥æ–¹æ³•åˆ©ç”¨DAVISæ•°æ®é›†çš„åˆæˆé€€åŒ–ï¼Œè®­ç»ƒæ—¶é—´æ„ŸçŸ¥å·ç§¯æ¶æ„ç›´æ¥ä»é€€åŒ–è§†é¢‘é¢„æµ‹å…¨å‚è€ƒæŒ‡æ ‡ï¼ˆLPIPSã€PSNRã€SSIMï¼‰ï¼Œåœ¨æ¨ç†æ—¶æ— éœ€å‚è€ƒè§†é¢‘ï¼Œé‡‡ç”¨æµå¼å¤„ç†æ–¹å¼å®ç°å¯æ‰©å±•çš„è§†é¢‘è´¨é‡è¯„ä¼°ã€‚

**Result:** å®éªŒè¡¨æ˜è¯¥æµå¼æ–¹æ³•ä¼˜äºå›¾åƒåŸºçº¿æ¨¡å‹ï¼Œèƒ½å¤Ÿæ³›åŒ–åˆ°å¤šç§é€€åŒ–ç±»å‹ï¼Œä¸å…¨å‚è€ƒæŒ‡æ ‡çš„ç›¸å…³æ€§é«˜äºå¹¿æ³›ä½¿ç”¨çš„æ„è§æ„ŸçŸ¥å›¾åƒè´¨é‡è¯„ä¼°åŸºçº¿BRISQUEï¼ŒéªŒè¯äº†æ—¶é—´å»ºæ¨¡åœ¨è§†é¢‘è´¨é‡è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ—¶é—´å»ºæ¨¡å¯¹äºå¯æ‰©å±•è§†é¢‘è´¨é‡è¯„ä¼°çš„é‡è¦æ€§ï¼Œä¸ºå®é™…è§†è§‰ç³»ç»Ÿæä¾›äº†ä¸€ç§æ— éœ€å‚è€ƒå’Œäººå·¥æ ‡æ³¨çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†æ—¶é—´æ„ŸçŸ¥æ–¹æ³•åœ¨è§†é¢‘è´¨é‡è¯„ä¼°é¢†åŸŸçš„ä¼˜è¶Šæ€§å’Œå®ç”¨æ€§ã€‚

---

#### ğŸ“„ Abstract
Video quality assessment (VQA) is vital for computer vision tasks, but
existing approaches face major limitations: full-reference (FR) metrics require
clean reference videos, and most no-reference (NR) models depend on training on
costly human opinion labels. Moreover, most opinion-unaware NR methods are
image-based, ignoring temporal context critical for video object detection. In
this work, we present a scalable, streaming-based VQA model that is both
no-reference and opinion-unaware. Our model leverages synthetic degradations of
the DAVIS dataset, training a temporal-aware convolutional architecture to
predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without
references at inference. We show that our streaming approach outperforms our
own image-based baseline by generalizing across diverse degradations,
underscoring the value of temporal modeling for scalable VQA in real-world
vision systems. Additionally, we demonstrate that our model achieves higher
correlation with full-reference metrics compared to BRISQUE, a widely-used
opinion-aware image quality assessment baseline, validating the effectiveness
of our temporal, opinion-unaware approach.


### [17] [Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts](https://arxiv.org/abs/2511.04655)
*Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, Saining Xie*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯Šæ–­å’Œå»åå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•çš„æ¡†æ¶ï¼Œé€šè¿‡æµ‹è¯•é›†å‹åŠ›æµ‹è¯•å’Œè¿­ä»£åç½®å‰ªææ–¹æ³•ï¼Œæ­ç¤ºäº†ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­æ™®éå­˜åœ¨çš„éè§†è§‰åç½®é—®é¢˜ï¼Œå¹¶åˆ›å»ºäº†å»ååçš„åŸºå‡†ç‰ˆæœ¬ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼Œæ¨¡å‹å¯ä»¥é€šè¿‡åˆ©ç”¨è¯­è¨€å…ˆéªŒã€åç½®å’Œè¡¨é¢æ¨¡å¼è€ŒéçœŸæ­£çš„è§†è§‰ç†è§£èƒ½åŠ›æ¥è·å¾—é«˜åˆ†ï¼Œè¿™å°¤å…¶å½±å“é‚£äº›æœ¬åº”ä¾èµ–è§†è§‰è¾“å…¥çš„è§†è§‰ä¸­å¿ƒåŸºå‡†æµ‹è¯•çš„æœ‰æ•ˆæ€§ã€‚

**Method:** æå‡ºåŒ…å«ä¸¤ä¸ªç»„ä»¶çš„æ¡†æ¶ï¼šæµ‹è¯•é›†å‹åŠ›æµ‹è¯•æ–¹æ³•é€šè¿‡kæŠ˜äº¤å‰éªŒè¯åœ¨çº¯æ–‡æœ¬æµ‹è¯•é›†ä¸Šå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹æ¥æ­ç¤ºæ·å¾„æ€§èƒ½å¹¶åˆ†é…åç½®åˆ†æ•°ï¼Œè¾…ä»¥åŸºäºéšæœºæ£®æ—çš„è½»é‡çº§è¯Šæ–­ï¼›è¿­ä»£åç½®å‰ªæç¨‹åºé€šè¿‡è¿‡æ»¤é«˜åç½®æ ·æœ¬æ¥å»ååŸºå‡†æµ‹è¯•ã€‚

**Result:** åœ¨VSI-Benchã€CV-Benchã€MMMUå’ŒVideoMMEå››ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‘ç°äº†æ™®éçš„éè§†è§‰åç½®ï¼Œé€šè¿‡å®Œæ•´æ¡†æ¶åˆ›å»ºçš„VSI-Bench-Debiasedæ˜¾ç¤ºå‡ºæ˜¾è‘—é™ä½çš„éè§†è§‰å¯è§£æ€§å’Œæ›´å¤§çš„è§†è§‰ç›²æ€§èƒ½å·®è·ã€‚

**Conclusion:** åŸºå‡†æµ‹è¯•è®¾è®¡åº”é‡‡ç”¨ä¸»åŠ¨è¯Šæ–­åŸåˆ™ï¼Œé€šè¿‡è‡ªæˆ‘åšå¼ˆæ–¹å¼è¯†åˆ«å’Œç¼“è§£éè§†è§‰åç½®ï¼Œå»ååçš„åŸºå‡†æµ‹è¯•èƒ½æ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹çš„çœŸå®è§†è§‰ç†è§£èƒ½åŠ›ï¼Œä¸ºæœªæ¥åŸºå‡†æµ‹è¯•è®¾è®¡æä¾›äº†æ–¹æ³•è®ºæŒ‡å¯¼ã€‚

---

#### ğŸ“„ Abstract
Robust benchmarks are crucial for evaluating Multimodal Large Language Models
(MLLMs). Yet we find that models can ace many multimodal benchmarks without
strong visual understanding, instead exploiting biases, linguistic priors, and
superficial patterns. This is especially problematic for vision-centric
benchmarks that are meant to require visual inputs. We adopt a diagnostic
principle for benchmark design: if a benchmark can be gamed, it will be.
Designers should therefore try to ``game'' their own benchmarks first, using
diagnostic and debiasing procedures to systematically identify and mitigate
non-visual biases. Effective diagnosis requires directly ``training on the test
set'' -- probing the released test set for its intrinsic, exploitable patterns.
  We operationalize this standard with two components. First, we diagnose
benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.
Our primary diagnostic tool involves fine-tuning a powerful Large Language
Model via $k$-fold cross-validation on exclusively the non-visual, textual
inputs of the test set to reveal shortcut performance and assign each sample a
bias score $s(x)$. We complement this with a lightweight Random Forest-based
diagnostic operating on hand-crafted features for fast, interpretable auditing.
Second, we debias benchmarks by filtering high-bias samples using an
``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four
benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive
non-visual biases. As a case study, we apply our full framework to create
VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider
vision-blind performance gap than the original.


### [18] [SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding](https://arxiv.org/abs/2511.04668)
*Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSIMS-Væ¡†æ¶ï¼Œåˆ©ç”¨3Dæ¨¡æ‹Ÿå™¨çš„ç‰¹æƒä¿¡æ¯ç”Ÿæˆç©ºé—´ä¸°å¯Œçš„è§†é¢‘è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡ç³»ç»Ÿæ€§æ¶ˆèå®éªŒå‘ç°ä»…éœ€ä¸‰ç§å…³é”®é—®é¢˜ç±»åˆ«å³å¯å®ç°æœ‰æ•ˆçš„çœŸå®ä¸–ç•Œç©ºé—´æ¨ç†è¿ç§»ï¼Œä»…ç”¨25Kæ¨¡æ‹Ÿæ ·æœ¬å¾®è°ƒçš„7Bæ¨¡å‹åœ¨çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Š72BåŸºçº¿ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨é«˜çº§è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨è·¨æ—¶ç©ºç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œè€Œå½“å‰åŸºäºçœŸå®è§†é¢‘æ•°æ®çš„ç©ºé—´è®­ç»ƒæ–¹æ³•é¢ä¸´ç©ºé—´æ ‡æ³¨æ•°æ®è·å–å›°éš¾ä¸”æˆæœ¬é«˜æ˜‚çš„ç“¶é¢ˆé—®é¢˜ã€‚

**Method:** æå‡ºSIMS-Vç³»ç»ŸåŒ–æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨3Dæ¨¡æ‹Ÿå™¨çš„ç‰¹æƒä¿¡æ¯åˆ›å»ºç©ºé—´ä¸°å¯Œçš„è§†é¢‘è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡ç³»ç»Ÿæ€§æ¶ˆèå®éªŒåˆ†æé—®é¢˜ç±»å‹ã€æ··åˆæ–¹å¼å’Œè§„æ¨¡å¯¹è¿ç§»æ•ˆæœçš„å½±å“ï¼Œè¯†åˆ«å‡ºåº¦é‡æµ‹é‡ã€è§†è§’ä¾èµ–æ¨ç†å’Œæ—¶é—´è·Ÿè¸ªä¸‰ç§æœ€æœ‰æ•ˆçš„å…³é”®é—®é¢˜ç±»åˆ«ã€‚

**Result:** ä»…ä½¿ç”¨25Kæ¨¡æ‹Ÿæ ·æœ¬å¾®è°ƒçš„7Bå‚æ•°è§†é¢‘LLMåœ¨çœŸå®ä¸–ç•Œç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ›´å¤§çš„72BåŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¸ä¸“æœ‰æ¨¡å‹è¾¾åˆ°ç«äº‰æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ä¿æŒé€šç”¨è§†é¢‘ç†è§£èƒ½åŠ›çš„åŸºç¡€ä¸Šï¼Œåœ¨å…·èº«åŒ–å’ŒçœŸå®ä¸–ç•Œç©ºé—´ä»»åŠ¡ä¸Šå±•ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜é€šè¿‡ç²¾å¿ƒé€‰æ‹©çš„æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®å¯ä»¥å®ç°é«˜æ•ˆçš„çœŸå®ä¸–ç•Œè¿ç§»ï¼Œä»…éœ€ä¸‰ç§æ ¸å¿ƒé—®é¢˜ç±»å‹å³å¯å¼€å‘å‡ºå¯è¿ç§»çš„ç©ºé—´æ™ºèƒ½ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›è®­ç»ƒæä¾›äº†æ•°æ®é«˜æ•ˆä¸”æˆæœ¬å¯æ§çš„æ–°èŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
Despite impressive high-level video comprehension, multimodal language models
struggle with spatial reasoning across time and space. While current spatial
training approaches rely on real-world video data, obtaining diverse footage
with precise spatial annotations remains a bottleneck. To alleviate this
bottleneck, we present SIMS-V -- a systematic data-generation framework that
leverages the privileged information of 3D simulators to create spatially-rich
video training data for multimodal language models. Using this framework, we
investigate which properties of simulated data drive effective real-world
transfer through systematic ablations of question types, mixes, and scales. We
identify a minimal set of three question categories (metric measurement,
perspective-dependent reasoning, and temporal tracking) that prove most
effective for developing transferable spatial intelligence, outperforming
comprehensive coverage despite using fewer question types. These insights
enable highly efficient training: our 7B-parameter video LLM fine-tuned on just
25K simulated examples outperforms the larger 72B baseline and achieves
competitive performance with proprietary models on rigorous real-world spatial
reasoning benchmarks. Our approach demonstrates robust generalization,
maintaining performance on general video understanding while showing
substantial improvements on embodied and real-world spatial tasks.


### [19] [Cambrian-S: Towards Spatial Supersensing in Video](https://arxiv.org/abs/2511.04670)
*Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, Saining Xie*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ç©ºé—´è¶…æ„ŸçŸ¥çš„æ¦‚å¿µï¼Œè®¤ä¸ºçœŸæ­£çš„å¤šæ¨¡æ€æ™ºèƒ½éœ€è¦ä»ä»»åŠ¡é©±åŠ¨ç³»ç»Ÿè½¬å‘èƒ½å¤Ÿè¿›è¡Œè¯­ä¹‰æ„ŸçŸ¥ã€äº‹ä»¶è®¤çŸ¥ã€3Dç©ºé—´æ¨ç†å’Œé¢„æµ‹ä¸–ç•Œå»ºæ¨¡çš„æ¡†æ¶ï¼Œå¹¶å¼€å‘äº†VSI-SUPERåŸºå‡†æ¥æ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„å‘å±•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿä¸»è¦å±€é™äºååº”å¼ä»»åŠ¡é©±åŠ¨æ–¹æ³•ï¼Œç¼ºä¹å¯¹ç©ºé—´è®¤çŸ¥çš„å…¨é¢è¦†ç›–ï¼Œæ— æ³•å®ç°çœŸæ­£çš„ä¸–ç•Œå»ºæ¨¡ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨è¯­ä¹‰æ„ŸçŸ¥ã€äº‹ä»¶æµè®¤çŸ¥ã€3Dç©ºé—´æ¨ç†å’Œé¢„æµ‹å»ºæ¨¡ç­‰æ–¹é¢çš„èƒ½åŠ›å‘å±•ã€‚

**Method:** æå‡ºäº†ç©ºé—´è¶…æ„ŸçŸ¥çš„å››é˜¶æ®µæ¡†æ¶ï¼Œå¼€å‘äº†VSI-SUPERåŸºå‡†åŒ…å«VSRå’ŒVSCä»»åŠ¡ï¼Œæ„å»ºäº†VSI-590Kæ•°æ®é›†è®­ç»ƒCambrian-Sæ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†åŸºäºé¢„æµ‹è¯¯å·®çš„è‡ªç›‘ç£ä¸‹ä¸€æ½œåœ¨å¸§é¢„æµ‹å™¨æ¥é©±åŠ¨è®°å¿†å’Œäº‹ä»¶åˆ†å‰²ã€‚

**Result:** Cambrian-Såœ¨VSI-Benchä¸Šå®ç°äº†30%çš„ç»å¯¹æ€§èƒ½æå‡è€Œä¸ç‰ºç‰²é€šç”¨èƒ½åŠ›ï¼Œä½†åœ¨VSI-SUPERä¸Šè¡¨ç°ä»æœ‰é™ï¼›åŸºäºé¢„æµ‹æ„ŸçŸ¥çš„æ–¹æ³•åœ¨VSI-SUPERä¸Šæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„ä¸“æœ‰åŸºçº¿æ¨¡å‹ã€‚

**Conclusion:** ä»…é æ•°æ®è§„æ¨¡æ‰©å±•ä¸è¶³ä»¥å®ç°ç©ºé—´è¶…æ„ŸçŸ¥ï¼Œéœ€è¦å‘å±•èƒ½å¤Ÿé¢„æµ‹ã€é€‰æ‹©å’Œç»„ç»‡ç»éªŒçš„æ¨¡å‹ï¼Œé¢„æµ‹æ„ŸçŸ¥é€šè¿‡åˆ©ç”¨é¢„æµ‹è¯¯å·®æ¥é©±åŠ¨è®°å¿†å’Œäº‹ä»¶åˆ†å‰²ï¼Œä¸ºå®ç°çœŸæ­£çš„ç©ºé—´æ™ºèƒ½æä¾›äº†å¯è¡Œè·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
We argue that progress in true multimodal intelligence calls for a shift from
reactive, task-driven systems and brute-force long context towards a broader
paradigm of supersensing. We frame spatial supersensing as four stages beyond
linguistic-only understanding: semantic perception (naming what is seen),
streaming event cognition (maintaining memory across continuous experiences),
implicit 3D spatial cognition (inferring the world behind pixels), and
predictive world modeling (creating internal models that filter and organize
information). Current benchmarks largely test only the early stages, offering
narrow coverage of spatial cognition and rarely challenging models in ways that
require true world modeling. To drive progress in spatial supersensing, we
present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial
recall) and VSC (continual visual spatial counting). These tasks require
arbitrarily long video inputs yet are resistant to brute-force context
expansion. We then test data scaling limits by curating VSI-590K and training
Cambrian-S, achieving +30% absolute improvement on VSI-Bench without
sacrificing general capabilities. Yet performance on VSI-SUPER remains limited,
indicating that scale alone is insufficient for spatial supersensing. We
propose predictive sensing as a path forward, presenting a proof-of-concept in
which a self-supervised next-latent-frame predictor leverages surprise
(prediction error) to drive memory and event segmentation. On VSI-SUPER, this
approach substantially outperforms leading proprietary baselines, showing that
spatial supersensing requires models that not only see but also anticipate,
select, and organize experience.


### [20] [InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation](https://arxiv.org/abs/2511.04675)
*Jinlai Liu, Jian Han, Bin Yan, Hui Wu, Fengda Zhu, Xing Wang, Yi Jiang, Bingyue Peng, Zehuan Yuan*

#### ğŸ§© TL;DR
InfinityStaræ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ—¶ç©ºè‡ªå›å½’æ¡†æ¶ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒå’ŒåŠ¨æ€è§†é¢‘åˆæˆï¼Œé€šè¿‡å•ä¸€æ¶æ„åŒæ—¶æ•è·ç©ºé—´å’Œæ—¶é—´ä¾èµ–æ€§ï¼Œåœ¨VBenchåŸºå‡†ä¸Šè·å¾—83.74åˆ†ï¼Œè¶…è¶Šæ‰€æœ‰è‡ªå›å½’æ¨¡å‹å¹¶å®ç°å·¥ä¸šçº§720pè§†é¢‘ç”Ÿæˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†é¢‘ç”Ÿæˆé¢†åŸŸé¢ä¸´ç©ºé—´å’Œæ—¶é—´ä¾èµ–æ€§å»ºæ¨¡çš„åˆ†ç¦»é—®é¢˜ï¼Œä»¥åŠé«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆæ•ˆç‡ä½ä¸‹çš„æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥åŒæ—¶å¤„ç†å¤šç§ç”Ÿæˆä»»åŠ¡å¹¶æå‡ç”Ÿæˆæ•ˆç‡ã€‚

**Method:** è¯¥æ–¹æ³•åŸºäºç¦»æ•£è‡ªå›å½’å»ºæ¨¡ï¼Œæ„å»ºç»Ÿä¸€çš„æ—¶ç©ºè‡ªå›å½’æ¡†æ¶ï¼Œé€šè¿‡å•ä¸€æ¶æ„è”åˆæ•è·ç©ºé—´å’Œæ—¶é—´ä¾èµ–æ€§ï¼Œæ”¯æŒæ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘ã€å›¾åƒåˆ°è§†é¢‘å’Œé•¿äº¤äº’è§†é¢‘åˆæˆç­‰å¤šç§ç”Ÿæˆä»»åŠ¡ã€‚

**Result:** åœ¨VBenchåŸºå‡†æµ‹è¯•ä¸­è·å¾—83.74åˆ†ï¼Œæ˜¾è‘—è¶…è¶Šæ‰€æœ‰è‡ªå›å½’æ¨¡å‹ï¼Œç”šè‡³è¶…è¿‡HunyuanVideoç­‰æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿä»¥æ¯”é¢†å…ˆæ‰©æ•£æ–¹æ³•å¿«çº¦10å€çš„é€Ÿåº¦ç”Ÿæˆ5ç§’720pè§†é¢‘ï¼Œæ˜¯é¦–ä¸ªèƒ½å¤Ÿç”Ÿæˆå·¥ä¸šçº§720pè§†é¢‘çš„ç¦»æ•£è‡ªå›å½’è§†é¢‘ç”Ÿæˆå™¨ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç»Ÿä¸€æ—¶ç©ºè‡ªå›å½’æ¡†æ¶åœ¨é«˜è´¨é‡è§†é¢‘ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºé«˜æ•ˆé«˜è´¨é‡è§†é¢‘ç”Ÿæˆå¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œé€šè¿‡ä»£ç å’Œæ¨¡å‹çš„å¼€æºå°†æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

---

#### ğŸ“„ Abstract
We introduce InfinityStar, a unified spacetime autoregressive framework for
high-resolution image and dynamic video synthesis. Building on the recent
success of autoregressive modeling in both vision and language, our purely
discrete approach jointly captures spatial and temporal dependencies within a
single architecture. This unified design naturally supports a variety of
generation tasks such as text-to-image, text-to-video, image-to-video, and long
interactive video synthesis via straightforward temporal autoregression.
Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench,
outperforming all autoregressive models by large margins, even surpassing some
diffusion competitors like HunyuanVideo. Without extra optimizations, our model
generates a 5s, 720p video approximately 10x faster than leading
diffusion-based methods. To our knowledge, InfinityStar is the first discrete
autoregressive video generator capable of producing industrial level 720p
videos. We release all code and models to foster further research in efficient,
high-quality video generation.


### [21] [Tracking and Understanding Object Transformations](https://arxiv.org/abs/2511.04678)
*Yihong Sun, Xinyu Yang, Jennifer J. Sun, Bharath Hariharan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Track Any Stateä»»åŠ¡ï¼Œç”¨äºè·Ÿè¸ªç‰©ä½“åœ¨çŠ¶æ€è½¬æ¢è¿‡ç¨‹ä¸­çš„å˜åŒ–ï¼Œå¹¶ä»‹ç»äº†TubeletGraphè¿™ä¸€é›¶æ ·æœ¬ç³»ç»Ÿï¼Œèƒ½å¤Ÿæ¢å¤è½¬æ¢åç¼ºå¤±çš„ç‰©ä½“å¹¶æ„å»ºçŠ¶æ€æ¼”åŒ–å›¾ï¼Œåœ¨VOST-TASåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è·Ÿè¸ªæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°å®ä¸–ç•Œä¸­çš„ç‰©ä½“ç»å¸¸ç»å†çŠ¶æ€è½¬æ¢ï¼Œå¦‚è‹¹æœè¢«åˆ‡æˆç¢ç‰‡æˆ–è´è¶ç ´èŒ§è€Œå‡ºï¼Œç°æœ‰è·Ÿè¸ªæ–¹æ³•åœ¨ç‰©ä½“å¤–è§‚å‘ç”Ÿæ˜¾è‘—å˜åŒ–æ—¶å¸¸å¸¸ä¸¢å¤±ç›®æ ‡ï¼Œæ— æ³•æœ‰æ•ˆè·Ÿè¸ªè½¬æ¢è¿‡ç¨‹ä¸­çš„ç‰©ä½“çŠ¶æ€å˜åŒ–ã€‚

**Method:** æå‡ºäº†TubeletGraphé›¶æ ·æœ¬ç³»ç»Ÿï¼Œé¦–å…ˆè¯†åˆ«å¯èƒ½è¢«å¿½ç•¥çš„è½¨è¿¹ï¼ŒåŸºäºè¯­ä¹‰å’Œé‚»è¿‘æ€§å…ˆéªŒåˆ¤æ–­æ˜¯å¦åº”æ•´åˆè¿™äº›è½¨è¿¹ï¼Œç„¶åå¯¹æ·»åŠ çš„è½¨è¿¹è¿›è¡Œæ¨ç†å¹¶ç”Ÿæˆæè¿°æ¯ä¸ªè§‚å¯Ÿåˆ°çš„è½¬æ¢çš„çŠ¶æ€å›¾ã€‚

**Result:** TubeletGraphåœ¨çŠ¶æ€è½¬æ¢åœºæ™¯ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„è·Ÿè¸ªæ€§èƒ½ï¼ŒåŒæ—¶å±•ç¤ºäº†å¯¹ç‰©ä½“è½¬æ¢çš„æ·±åº¦ç†è§£èƒ½åŠ›ï¼Œåœ¨å¤æ‚ç‰©ä½“è½¬æ¢çš„æ—¶é—´å®šä½å’Œè¯­ä¹‰æ¨ç†æ–¹é¢è¡¨ç°å‡ºæœ‰å‰æ™¯çš„èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ä»…è§£å†³äº†ç‰©ä½“çŠ¶æ€è½¬æ¢è·Ÿè¸ªçš„æŒ‘æˆ˜ï¼Œè¿˜æä¾›äº†å¯¹ç‰©ä½“çŠ¶æ€æ¼”åŒ–çš„ç»“æ„åŒ–ç†è§£ï¼Œä¸ºç†è§£ç°å®ä¸–ç•Œç‰©ä½“åŠ¨æ€å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œåœ¨æ—¶é—´æ¨ç†å’Œè¯­ä¹‰åˆ†ææ–¹é¢å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Real-world objects frequently undergo state transformations. From an apple
being cut into pieces to a butterfly emerging from its cocoon, tracking through
these changes is important for understanding real-world objects and dynamics.
However, existing methods often lose track of the target object after
transformation, due to significant changes in object appearance. To address
this limitation, we introduce the task of Track Any State: tracking objects
through transformations while detecting and describing state changes,
accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we
present TubeletGraph, a zero-shot system that recovers missing objects after
transformation and maps out how object states are evolving over time.
TubeletGraph first identifies potentially overlooked tracks, and determines
whether they should be integrated based on semantic and proximity priors. Then,
it reasons about the added tracks and generates a state graph describing each
observed transformation. TubeletGraph achieves state-of-the-art tracking
performance under transformations, while demonstrating deeper understanding of
object transformations and promising capabilities in temporal grounding and
semantic reasoning for complex object transformations. Code, additional
results, and the benchmark dataset are available at
https://tubelet-graph.github.io.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*MikoÅ‚aj Langner, Jan Eliasz, Ewa Rudnicka, Jan KocoÅ„*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»æ–¹æ³•ï¼Œé€šè¿‡å°†åˆ†ç±»ä»»åŠ¡é‡æ–°è¡¨è¿°ä¸ºä¸€ç³»åˆ—äºŒåˆ†å†³ç­–ï¼Œç»“åˆå‰ç¼€ç¼“å­˜æœºåˆ¶ï¼Œåœ¨ä¸æŸå¤±å‡†ç¡®æ€§çš„å‰æä¸‹æ˜¾è‘—æå‡äº†çŸ­æ–‡æœ¬æ¨ç†æ•ˆç‡ã€‚è¯¥æ–¹æ³•åœ¨æƒ…æ„Ÿæ–‡æœ¬åˆ†æä¸­éªŒè¯æœ‰æ•ˆï¼Œå¹¶é€šè¿‡LLMåˆ°SLMçš„è’¸é¦æŠ€æœ¯å®ç°äº†å°æ¨¡å‹æ€§èƒ½çš„æ˜¾è‘—æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­é¢ä¸´æ•ˆç‡ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ­æ–‡æœ¬æ¨ç†åœºæ™¯ä¸‹ç”Ÿæˆç»“æ„åŒ–å“åº”æ—¶è®¡ç®—å¼€é”€è¾ƒå¤§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤šæ ‡ç­¾åˆ†ç±»çš„æ•ˆç‡é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒåˆ†ç±»å‡†ç¡®æ€§ï¼Œä¸ºå®é™…åº”ç”¨æä¾›å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

**Method:** è¯¥æ–¹æ³•å°†å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡åˆ†è§£ä¸ºç‹¬ç«‹çš„äºŒåˆ†æŸ¥è¯¢åºåˆ—ï¼Œæ¯ä¸ªç›®æ ‡ç»´åº¦å•ç‹¬è¿›è¡Œæ˜¯/å¦å†³ç­–ï¼Œç»“åˆå‰ç¼€ç¼“å­˜æœºåˆ¶ä¼˜åŒ–æ¨ç†æ•ˆç‡ã€‚é‡‡ç”¨LLMåˆ°SLMçš„è’¸é¦æ¡†æ¶ï¼Œä½¿ç”¨DeepSeek-V3ä½œä¸ºå¼ºå¤§çš„æ ‡æ³¨å™¨ç”Ÿæˆå¤šæ ‡ç­¾æ³¨é‡Šï¼Œç„¶åèšåˆè¿™äº›æ³¨é‡Šæ¥å¾®è°ƒè¾ƒå°çš„æ¨¡å‹å¦‚HerBERT-Largeã€CLARIN-1Bã€PLLuM-8Bå’ŒGemma3-1Bã€‚

**Result:** å¾®è°ƒåçš„æ¨¡å‹åœ¨é›¶æ ·æœ¬åŸºçº¿åŸºç¡€ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è§è¿‡çš„ç»´åº¦ä¸Šæ•ˆæœæ›´ä¸ºæ˜æ˜¾ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒåˆ†ç±»å‡†ç¡®æ€§çš„åŒæ—¶å®ç°äº†çŸ­æ–‡æœ¬æ¨ç†çš„å®è´¨æ€§æ•ˆç‡æå‡ï¼ŒéªŒè¯äº†äºŒåˆ†æŸ¥è¯¢åˆ†è§£ä¸ç¼“å­˜æ„ŸçŸ¥æ¨ç†ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†å¤šæ ‡ç­¾åˆ†ç±»åˆ†è§£ä¸ºäºŒåˆ†æŸ¥è¯¢ï¼Œç»“åˆè’¸é¦æŠ€æœ¯å’Œç¼“å­˜ä¼˜åŒ–æ¨ç†ï¼Œä¸ºåŸºäºLLMçš„åˆ†ç±»æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”æœ‰æ•ˆçš„æ¡†æ¶ã€‚è™½ç„¶è¯¥æ–¹æ³•åœ¨æƒ…æ„ŸçŠ¶æ€åˆ†æä¸­å¾—åˆ°éªŒè¯ï¼Œä½†å…¶é€šç”¨æ€§ä½¿å…¶å¯å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸï¼Œä¸ºé«˜æ•ˆçš„å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.


### [23] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan, Ben Prystawski, Veronica Boyce, Michael C. Frank*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡è¿­ä»£æŒ‡ç§°æ¸¸æˆè¯„ä¼°äº†äººç±»ä¸è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡æ•æ„Ÿè¯­ç”¨æ¨ç†æ–¹é¢çš„èƒ½åŠ›å·®å¼‚ï¼Œå‘ç°åœ¨ç›¸å…³ä¸Šä¸‹æ–‡æ¡ä»¶ä¸‹æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ï¼Œä½†æŠ½è±¡æŒ‡ç§°çš„å°‘æ ·æœ¬æ¸¸æˆä»ç„¶æ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹çš„éš¾ç‚¹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¿­ä»£æŒ‡ç§°æ¸¸æˆä¸ºè¯„ä¼°æ™ºèƒ½ä½“åœ¨å¤šè½®è¯­è¨€ç¯å¢ƒä¸­æ‰§è¡Œä¸Šä¸‹æ–‡æ•æ„Ÿè¯­ç”¨æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæµ‹è¯•åŸºå‡†ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢äººç±»ä¸è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ­¤ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯ä¸Šä¸‹æ–‡çš„æ•°é‡ã€é¡ºåºå’Œç›¸å…³æ€§å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨è¿­ä»£æŒ‡ç§°æ¸¸æˆèŒƒå¼ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°æ”¹å˜ä¸Šä¸‹æ–‡çš„æ•°é‡ã€é¡ºåºå’Œç›¸å…³æ€§æ¥æµ‹è¯•äººç±»å‚ä¸è€…å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œæ¯”è¾ƒäº†ä¸åŒä¸Šä¸‹æ–‡æ¡ä»¶ä¸‹æ¨¡å‹ä¸äººç±»åœ¨æŒ‡ç§°é€‰æ‹©ä»»åŠ¡ä¸­çš„è¡¨ç°å·®å¼‚ã€‚

**Result:** åœ¨æ²¡æœ‰ç›¸å…³ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹è¡¨ç°è™½é«˜äºéšæœºæ°´å¹³ä½†æ˜¾è‘—å·®äºäººç±»ï¼›è€Œåœ¨ç›¸å…³ä¸Šä¸‹æ–‡æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹æ€§èƒ½éšè¯•éªŒæ¬¡æ•°æ˜¾è‘—æå‡ï¼Œä½†æŠ½è±¡æŒ‡ç§°çš„å°‘æ ·æœ¬æŒ‡ç§°æ¸¸æˆä»ç„¶æ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ä¸Šä¸‹æ–‡ç›¸å…³æ€§å¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯­ç”¨æ¨ç†èƒ½åŠ›å…·æœ‰å†³å®šæ€§å½±å“ï¼Œæ¨¡å‹åœ¨ç›¸å…³ä¸Šä¸‹æ–‡æ”¯æŒä¸‹èƒ½å¤Ÿå¿«é€Ÿå­¦ä¹ å¹¶æ”¹è¿›è¡¨ç°ï¼Œä½†æŠ½è±¡æ¦‚å¿µçš„å°‘æ ·æœ¬å­¦ä¹ ä»ç„¶æ˜¯å½“å‰æ¨¡å‹çš„ç“¶é¢ˆï¼Œè¿™ä¸ºå¼€å‘æ›´å¼ºå¤§çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¯­è¨€æ¨¡å‹æä¾›äº†é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.


### [24] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash, Maayane Attias, Pierre Chambon, Justin Xu, Steven Truong, Jean-Benoit Delbrouck, Tessa Cook, Curtis Langlotz*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡å¤§è§„æ¨¡å¤šæ¨¡æ€è®­ç»ƒæ•°æ®ä¼˜åŒ–åŸºäºTransformerçš„æ”¾å°„å­¦æŠ¥å‘Šå»è¯†åˆ«æ¨¡å‹ï¼Œåœ¨PHIæ£€æµ‹ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰å­¦æœ¯å’Œå•†ä¸šç³»ç»Ÿï¼Œä¸ºå®‰å…¨ä¸´åºŠæ–‡æœ¬å¤„ç†å»ºç«‹äº†æ–°åŸºå‡†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ”¾å°„å­¦æŠ¥å‘Šä¸­å—ä¿æŠ¤å¥åº·ä¿¡æ¯(PHI)çš„è‡ªåŠ¨å»è¯†åˆ«å­˜åœ¨è·¨æœºæ„æ³›åŒ–èƒ½åŠ›ä¸è¶³å’Œå•†ä¸šç³»ç»Ÿæ€§èƒ½æœ‰é™çš„é—®é¢˜ï¼Œéœ€è¦å¼€å‘æ›´é²æ£’ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆæ¥ç¡®ä¿ä¸´åºŠæ•°æ®éšç§ä¿æŠ¤ã€‚

**Method:** åŸºäºæœ€å…ˆè¿›çš„Transformeræ¶æ„PHIå»è¯†åˆ«æµæ°´çº¿ï¼Œåœ¨æ–¯å¦ç¦å¤§å­¦ä¸¤ä¸ªå¤§å‹æ ‡æ³¨æ”¾å°„å­¦è¯­æ–™åº“ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ¶µç›–èƒ¸éƒ¨Xå…‰ã€èƒ¸éƒ¨CTã€è…¹éƒ¨/ç›†è…”CTå’Œè„‘éƒ¨MRæŠ¥å‘Šï¼Œå¹¶å¼•å…¥æ–°çš„AGEç±»åˆ«ï¼Œé‡‡ç”¨"éšè—äºä¼—ç›®ç½ç½ä¹‹ä¸‹"æ–¹æ³•è¯„ä¼°åˆæˆPHIç”Ÿæˆçš„ç¨³å®šæ€§ã€‚

**Result:** æ¨¡å‹åœ¨Pennæ•°æ®é›†ä¸Šè¾¾åˆ°0.973çš„æ€»ä½“F1åˆ†æ•°ï¼Œåœ¨æ–¯å¦ç¦æ•°æ®é›†ä¸Šè¾¾åˆ°0.996ï¼Œè¶…è¶Šæˆ–ä¿æŒå…ˆå‰æœ€ä¼˜æ€§èƒ½ï¼›åˆæˆPHIè¯„ä¼°æ˜¾ç¤º50ä¸ªç‹¬ç«‹å»è¯†åˆ«æ•°æ®é›†çš„æ£€æµ‹ä¸€è‡´æ€§(F1: 0.959)ï¼Œåœ¨åˆæˆPennæŠ¥å‘Šä¸Šä¼˜äºæ‰€æœ‰å•†ä¸šç³»ç»Ÿ(F1: 0.960 vs. 0.632-0.754)ã€‚

**Conclusion:** å¤§è§„æ¨¡å¤šæ¨¡æ€è®­ç»ƒæ˜¾è‘—æå‡äº†è·¨æœºæ„æ³›åŒ–èƒ½åŠ›å’Œæ¨¡å‹é²æ£’æ€§ï¼ŒåˆæˆPHIç”Ÿæˆåœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ä¿æŒäº†æ•°æ®å®ç”¨æ€§ï¼ŒåŸºäºTransformerçš„å»è¯†åˆ«æ¨¡å‹ä¸ºå®‰å…¨ä¸´åºŠæ–‡æœ¬å¤„ç†ç¡®ç«‹äº†æ–°çš„æ€§èƒ½æ ‡å‡†ã€‚

---

#### ğŸ“„ Abstract
Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.


### [25] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang, Zikang chen, Yanmeng Wang, Zhigen Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSSPOç®—æ³•ï¼Œé‡‡ç”¨å¥å­çº§é‡è¦æ€§æ¯”ç‡æ¥å¹³è¡¡GRPOå’ŒGSPOçš„ä¼˜ç¼ºç‚¹ï¼Œè§£å†³äº†RLVRç®—æ³•ä¸­ç­–ç•¥æ›´æ–°ä¸ç¨³å®šå’Œé‡‡æ ·æ•°æ®åˆ©ç”¨ç‡ä½çš„é—®é¢˜ï¼Œåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰RLVRç®—æ³•å¦‚GRPOå’ŒGSPOå­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼šGRPOåœ¨ä»¤ç‰Œçº§è®¡ç®—é‡è¦æ€§æ¯”ç‡ï¼Œå®¹æ˜“å—å¼‚å¸¸å€¼å½±å“å¯¼è‡´è®­ç»ƒå´©æºƒï¼›GSPOåœ¨å“åº”çº§è®¡ç®—é‡è¦æ€§æ¯”ç‡ï¼Œè§£å†³äº†é«˜æ–¹å·®é—®é¢˜ä½†å®¹æ˜“å› æç«¯å€¼å¯¼è‡´æ•´ä¸ªå“åº”è¢«é”™è¯¯ä¸¢å¼ƒï¼Œé™ä½äº†é‡‡æ ·æ•°æ®åˆ©ç”¨ç‡ã€‚

**Method:** æå‡ºSSPOç®—æ³•ï¼Œé‡‡ç”¨å¥å­çº§é‡è¦æ€§æ¯”ç‡è®¡ç®—æ–¹å¼ï¼Œåœ¨GRPOå’ŒGSPOä¹‹é—´å–å¾—å¹³è¡¡ï¼›åŒæ—¶åº”ç”¨å¥å­ç†µåˆ°PPO-CLIPä¸­ï¼ŒåŠ¨æ€è°ƒæ•´è£å‰ªè¾¹ç•Œï¼Œé¼“åŠ±é«˜ç†µä»¤ç‰Œæ¢ç´¢å¹¶ç¼©å°ä½ç†µä»¤ç‰Œçš„è£å‰ªèŒƒå›´ã€‚

**Result:** SSPOåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šå¹³å‡å¾—åˆ†è¾¾åˆ°46.57ï¼Œæ˜¾è‘—è¶…è¶ŠGRPOï¼ˆ43.01ï¼‰å’ŒGSPOï¼ˆ44.42ï¼‰ï¼Œå¹¶åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** SSPOé€šè¿‡å¥å­çº§é‡è¦æ€§æ¯”ç‡è®¾è®¡æœ‰æ•ˆåˆ©ç”¨äº†ç”Ÿæˆæ•°æ®ï¼Œæ—¢é¿å…äº†è®­ç»ƒå´©æºƒå’Œé«˜æ–¹å·®é—®é¢˜ï¼Œåˆé˜²æ­¢äº†å› è£å‰ªæœºåˆ¶å¯¼è‡´æ•´ä¸ªå“åº”ä»¤ç‰Œè¢«ä¸¢å¼ƒçš„é—®é¢˜ï¼Œä¸ºRLVRç®—æ³•æä¾›äº†æ›´ç¨³å®šé«˜æ•ˆçš„ä¼˜åŒ–æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.


### [26] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung, Teetouch Jaknamon, Sirinya Chaiophat, Natapong Nitarach, Chanakan Wittayasakpan, Warit Sirichotedumrong, Adisai Na-Thalang, Kunat Pipatanakul*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ThaiOCRBenchï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ³°è¯­æ–‡æœ¬ä¸°å¯Œè§†è§‰ç†è§£ä»»åŠ¡ä¸Šçš„ç»¼åˆåŸºå‡†ï¼Œå¡«è¡¥äº†æ³°è¯­åœ¨å¤šæ¨¡æ€å»ºæ¨¡è¯„ä¼°ä¸­çš„ç©ºç™½ã€‚è¯¥åŸºå‡†åŒ…å«2808ä¸ªæ ·æœ¬å’Œ13ä¸ªä»»åŠ¡ç±»åˆ«ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¯„ä¼°äº†å¤šç§æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨é«˜èµ„æºè¯­è¨€ï¼Œå¯¼è‡´æ³°è¯­åœ¨éœ€è¦æ–‡æ¡£ç»“æ„ç†è§£çš„ä»»åŠ¡ä¸­ä»£è¡¨æ€§ä¸è¶³ã€‚å°½ç®¡å¤šæ¨¡æ€å»ºæ¨¡å–å¾—äº†è¿›å±•ï¼Œä½†ç¼ºä¹ä¸“é—¨é’ˆå¯¹æ³°è¯­æ–‡æœ¬ä¸°å¯Œè§†è§‰ç†è§£çš„æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶ï¼Œè¿™é™åˆ¶äº†æ³°è¯­æ–‡æ¡£ç†è§£æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

**Method:** æ„å»ºäº†ä¸€ä¸ªå¤šæ ·åŒ–ã€äººå·¥æ ‡æ³¨çš„æ•°æ®é›†ï¼ŒåŒ…å«2808ä¸ªæ ·æœ¬ï¼Œæ¶µç›–13ä¸ªä»»åŠ¡ç±»åˆ«ã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¯„ä¼°äº†å¹¿æ³›çš„å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸“æœ‰å’Œå¼€æºç³»ç»Ÿï¼Œé€šè¿‡è¯¦ç»†çš„é”™è¯¯åˆ†æè¯†åˆ«å…³é”®æŒ‘æˆ˜ã€‚

**Result:** è¯„ä¼°ç»“æœæ˜¾ç¤ºå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚Gemini 2.5 Proï¼‰è¡¨ç°ä¼˜äºå¼€æºå¯¹åº”æ¨¡å‹ã€‚ç»†ç²’åº¦æ–‡æœ¬è¯†åˆ«å’Œæ‰‹å†™å†…å®¹æå–åœ¨å¼€æºæ¨¡å‹ä¸­è¡¨ç°å‡ºæœ€ä¸¥é‡çš„æ€§èƒ½ä¸‹é™ï¼Œé€šè¿‡é”™è¯¯åˆ†æè¯†åˆ«å‡ºè¯­è¨€åè§ã€ç»“æ„ä¸åŒ¹é…å’Œå¹»è§‰å†…å®¹ç­‰å…³é”®æŒ‘æˆ˜ã€‚

**Conclusion:** ThaiOCRBenchä¸ºè¯„ä¼°ä½èµ„æºã€å¤æ‚è„šæœ¬ç¯å¢ƒä¸‹çš„è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†æ ‡å‡†åŒ–æ¡†æ¶ï¼Œå¹¶ä¸ºæ”¹è¿›æ³°è¯­æ–‡æ¡£ç†è§£æä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚è¯¥åŸºå‡†æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æ³°è¯­æ–‡æœ¬ç†è§£æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚

---

#### ğŸ“„ Abstract
We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.


### [27] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana, Saiyma Sittul Muna, Mosammat Zannatul Samarukh, Ajwad Abrar, Tareque Mohmud Chowdhury*

#### ğŸ§© TL;DR
æœ¬æ–‡ä»‹ç»äº†BanglaMedQAå’ŒBanglaMMedBenchï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡å­ŸåŠ æ‹‰è¯­ç”Ÿç‰©åŒ»å­¦å¤šé¡¹é€‰æ‹©é¢˜æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†å¤šç§æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­–ç•¥ï¼Œå…¶ä¸­Agentic RAGåœ¨openai/gpt-oss-120bä¸Šå–å¾—äº†89.54%çš„æœ€é«˜å‡†ç¡®ç‡ï¼Œæ˜¾è‘—æå‡äº†å­ŸåŠ æ‹‰è¯­åŒ»å­¦é—®ç­”ç³»ç»Ÿçš„å¯é æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä½èµ„æºè¯­è¨€ä¸­å¼€å‘å‡†ç¡®çš„ç”Ÿç‰©åŒ»å­¦é—®ç­”ç³»ç»Ÿä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†å…¬å¹³è·å–å¯é åŒ»å­¦çŸ¥è¯†çš„æœºä¼šï¼Œç‰¹åˆ«æ˜¯åœ¨å­ŸåŠ æ‹‰è¯­ç­‰èµ„æºåŒ®ä¹çš„è¯­è¨€ç¯å¢ƒä¸­ï¼Œç¼ºä¹ä¸“é—¨è®¾è®¡çš„è¯„ä¼°åŸºå‡†å’Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

**Method:** ç ”ç©¶åº”ç”¨å¹¶åŸºå‡†æµ‹è¯•äº†å¤šç§æ£€ç´¢å¢å¼ºç”Ÿæˆç­–ç•¥ï¼ŒåŒ…æ‹¬ä¼ ç»ŸRAGã€é›¶æ ·æœ¬å›é€€ã€Agentic RAGã€è¿­ä»£åé¦ˆå’ŒèšåˆRAGï¼Œç»“åˆåŸºäºæ•™ç§‘ä¹¦çš„æ£€ç´¢å’Œç½‘ç»œæ£€ç´¢ä¸ç”Ÿæˆæ¨ç†ï¼Œé€šè¿‡å…‰å­¦å­—ç¬¦è¯†åˆ«é›†æˆå­ŸåŠ æ‹‰è¯­åŒ»å­¦æ•™ç§‘ä¹¦è¯­æ–™åº“ï¼Œå¹¶å®ç°äº†åŠ¨æ€é€‰æ‹©æ£€ç´¢å’Œæ¨ç†ç­–ç•¥çš„Agentic RAGç®¡é“ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAgentic RAGåœ¨openai/gpt-oss-120bæ¨¡å‹ä¸Šå–å¾—äº†89.54%çš„æœ€é«˜å‡†ç¡®ç‡ï¼Œä¼˜äºå…¶ä»–é…ç½®ï¼Œå¹¶å±•ç¤ºäº†å“è¶Šçš„æ¨ç†è´¨é‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å­ŸåŠ æ‹‰è¯­åŒ»å­¦é—®ç­”ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¿™äº›å‘ç°çªæ˜¾äº†åŸºäºRAGçš„æ–¹æ³•åœ¨æå‡å­ŸåŠ æ‹‰è¯­åŒ»å­¦é—®ç­”å¯é æ€§å’Œå¯è®¿é—®æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºå¤šè¯­è¨€åŒ»å­¦äººå·¥æ™ºèƒ½çš„æœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œå±•ç¤ºäº†æ™ºèƒ½æ£€ç´¢ç­–ç•¥åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­çš„é‡è¦ä½œç”¨ã€‚

---

#### ğŸ“„ Abstract
Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [28] [To See or To Read: User Behavior Reasoning in Multimodal LLMs](https://arxiv.org/abs/2511.03845)
*Tianning Dong, Luyi Ma, Varun Vasudevan, Jason Cho, Sushant Kumar, Kannan Achan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†BehaviorLensæ¡†æ¶ï¼Œç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç”¨æˆ·è¡Œä¸ºæ¨ç†ä¸­çš„æ¨¡æ€æƒè¡¡ï¼Œå‘ç°å›¾åƒè¡¨ç¤ºç›¸æ¯”æ–‡æœ¬è¡¨ç¤ºå¯å°†ä¸‹ä¸€è´­ä¹°é¢„æµ‹å‡†ç¡®ç‡æå‡87.5%ï¼Œæ— éœ€é¢å¤–è®¡ç®—æˆæœ¬ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ­£åœ¨é‡å¡‘ç°ä»£æ™ºèƒ½ç³»ç»Ÿå¯¹åºåˆ—ç”¨æˆ·è¡Œä¸ºæ•°æ®çš„æ¨ç†æ–¹å¼ï¼Œä½†æ–‡æœ¬ä¸å›¾åƒè¡¨ç¤ºå“ªç§æ›´èƒ½æœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½ä»ç¼ºä¹ç³»ç»Ÿç ”ç©¶ï¼Œéœ€è¦æ¢ç´¢ä¸åŒæ¨¡æ€è¡¨ç¤ºå¯¹ç”¨æˆ·è¡Œä¸ºæ¨ç†æ•ˆæœçš„å½±å“ã€‚

**Method:** å¼€å‘äº†BehaviorLensç³»ç»ŸåŒ–åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œåœ¨å…­ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸Šè¯„ä¼°ç”¨æˆ·è¡Œä¸ºæ¨ç†çš„æ¨¡æ€æƒè¡¡ï¼Œå°†äº¤æ˜“æ•°æ®è¡¨ç¤ºä¸ºä¸‰ç§å½¢å¼ï¼šæ–‡æœ¬æ®µè½ã€æ•£ç‚¹å›¾å’Œæµç¨‹å›¾ï¼Œä½¿ç”¨çœŸå®ä¸–ç•Œè´­ä¹°åºåˆ—æ•°æ®é›†è¿›è¡Œå®éªŒéªŒè¯ã€‚

**Result:** åŸºäºçœŸå®è´­ä¹°åºåˆ—æ•°æ®é›†çš„å®éªŒè¡¨æ˜ï¼Œå½“æ•°æ®è¡¨ç¤ºä¸ºå›¾åƒæ—¶ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€è´­ä¹°é¢„æµ‹å‡†ç¡®ç‡ç›¸æ¯”ç­‰æ•ˆæ–‡æœ¬è¡¨ç¤ºæé«˜äº†87.5%ï¼Œè¿™ä¸€æ€§èƒ½æå‡æ— éœ€ä»»ä½•é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚

**Conclusion:** ç ”ç©¶è¯å®å›¾åƒè¡¨ç¤ºåœ¨ç”¨æˆ·è¡Œä¸ºæ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæ–‡æœ¬è¡¨ç¤ºï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨æä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œè¡¨æ˜è§†è§‰æ¨¡æ€åœ¨åºåˆ—æ•°æ®åˆ†æä¸­å…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œæœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢ä¸åŒè§†è§‰è¡¨ç¤ºå½¢å¼çš„æ•ˆæœå·®å¼‚ã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Language Models (MLLMs) are reshaping how modern agentic
systems reason over sequential user-behavior data. However, whether textual or
image representations of user behavior data are more effective for maximizing
MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a
systematic benchmarking framework for assessing modality trade-offs in
user-behavior reasoning across six MLLMs by representing transaction data as
(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a
real-world purchase-sequence dataset, we find that when data is represented as
images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared
with an equivalent textual representation without any additional computational
cost.


### [29] [KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04093)
*Yuanning Cui, Zequn Sun, Wei Hu, Zhangjie Fu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†LLM-KGFRåä½œæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒçŸ¥è¯†å›¾è°±åŸºç¡€æ£€ç´¢å™¨çš„ç»“æ„åŒ–æ£€ç´¢èƒ½åŠ›ï¼Œè§£å†³äº†LLMåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶å®ç°äº†å¯¹æœªè§çŸ¥è¯†å›¾è°±çš„é›¶æ ·æœ¬æ³›åŒ–ï¼Œå¹¶é€šè¿‡æ¸è¿›å¼ä¼ æ’­ç­–ç•¥ä¿æŒåœ¨å¤§è§„æ¨¡å›¾ä¸Šçš„å¯æ‰©å±•æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†çŸ¥è¯†å¯†é›†å‹é—®é¢˜æ—¶å—åˆ°æœ‰é™ä¸Šä¸‹æ–‡å’Œå‚æ•°çŸ¥è¯†çš„é™åˆ¶ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¾®è°ƒçš„LLMæˆ–å›¾ç¥ç»ç½‘ç»œæ£€ç´¢å™¨ï¼Œå­˜åœ¨æ•°æ®é›†ç‰¹å®šè°ƒä¼˜çš„å±€é™æ€§ï¼Œä»¥åŠåœ¨å¤§è§„æ¨¡æˆ–æœªè§å›¾ä¸Šå¯æ‰©å±•æ€§ä¸è¶³çš„é—®é¢˜ã€‚

**Method:** æå‡ºäº†LLM-KGFRåä½œæ¡†æ¶ï¼Œå…¶ä¸­çŸ¥è¯†å›¾è°±åŸºç¡€æ£€ç´¢å™¨ä½¿ç”¨LLMç”Ÿæˆçš„å…³ç³»æè¿°å¯¹å…³ç³»è¿›è¡Œç¼–ç ï¼Œå¹¶æ ¹æ®å®ä½“åœ¨é—®é¢˜ä¸­çš„è§’è‰²åˆå§‹åŒ–å®ä½“è¡¨ç¤ºï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–ã€‚é‡‡ç”¨éå¯¹ç§°æ¸è¿›ä¼ æ’­ç­–ç•¥è¿›è¡Œé€æ­¥æ‰©å±•ï¼Œé€‰æ‹©æ€§é™åˆ¶é«˜åº¦èŠ‚ç‚¹åŒæ—¶ä¿ç•™ä¿¡æ¯è·¯å¾„ã€‚é€šè¿‡èŠ‚ç‚¹ã€è¾¹å’Œè·¯å¾„çº§åˆ«çš„æ¥å£ï¼ŒLLMè¿­ä»£è¯·æ±‚å€™é€‰ç­”æ¡ˆã€æ”¯æŒäº‹å®å’Œæ¨ç†è·¯å¾„ï¼Œå½¢æˆå¯æ§æ¨ç†å¾ªç¯ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM-KGFRåœ¨ä¿æŒå¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„åŒæ—¶å®ç°äº†å¼ºå¤§çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥æ¡†æ¶ä¸ºçŸ¥è¯†å›¾è°±å¢å¼ºæ¨ç†æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±ä¸Šçš„å¤æ‚æ¨ç†ä»»åŠ¡ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†LLMä¸ç»“æ„åŒ–æ£€ç´¢å™¨åä½œçš„æ½œåŠ›ï¼Œé€šè¿‡é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å’Œæ¸è¿›å¼ä¼ æ’­ç­–ç•¥ï¼Œä¸ºçŸ¥è¯†å¯†é›†å‹æ¨ç†ä»»åŠ¡æä¾›äº†å¯æ‰©å±•ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚æ¡†æ¶çš„è®¾è®¡æ”¯æŒå¯æ§æ¨ç†å¾ªç¯ï¼Œä¸ºæœªæ¥çŸ¥è¯†å¢å¼ºçš„AIç³»ç»Ÿå¼€å‘æä¾›äº†é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
Large language models (LLMs) excel at reasoning but struggle with
knowledge-intensive questions due to limited context and parametric knowledge.
However, existing methods that rely on finetuned LLMs or GNN retrievers are
limited by dataset-specific tuning and scalability on large or unseen graphs.
We propose the LLM-KGFR collaborative framework, where an LLM works with a
structured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR
encodes relations using LLM-generated descriptions and initializes entities
based on their roles in the question, enabling zero-shot generalization to
unseen KGs. To handle large graphs efficiently, it employs Asymmetric
Progressive Propagation (APP)- a stepwise expansion that selectively limits
high-degree nodes while retaining informative paths. Through node-, edge-, and
path-level interfaces, the LLM iteratively requests candidate answers,
supporting facts, and reasoning paths, forming a controllable reasoning loop.
Experiments demonstrate that LLM-KGFR achieves strong performance while
maintaining scalability and generalization, providing a practical solution for
KG-augmented reasoning.


### [30] [GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents](https://arxiv.org/abs/2511.04307)
*Jian Mu, Chaoyun Zhang, Chiming Ni, Lu Wang, Bo Qiao, Kartik Mathur, Qianhui Wu, Yuhang Xie, Xiaojun Ma, Mengyu Zhou, Si Qin, Liqun Li, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡ä»‹ç»äº†GUI-360Â°ï¼Œä¸€ä¸ªå¤§è§„æ¨¡ã€å…¨é¢çš„æ•°æ®é›†å’ŒåŸºå‡†å¥—ä»¶ï¼Œæ—¨åœ¨æ¨è¿›è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAsï¼‰çš„ç ”ç©¶ã€‚è¯¥æ•°æ®é›†é€šè¿‡LLMå¢å¼ºçš„è‡ªåŠ¨åŒ–æµç¨‹æ„å»ºï¼ŒåŒ…å«è¶…è¿‡120ä¸‡æ‰§è¡ŒåŠ¨ä½œæ­¥éª¤ï¼Œè§£å†³äº†CUAé¢†åŸŸç¼ºä¹çœŸå®ä»»åŠ¡ã€å¤šæ¨¡æ€è½¨è¿¹è‡ªåŠ¨æ”¶é›†å’Œç»Ÿä¸€è¯„ä¼°åŸºå‡†çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAsï¼‰ç ”ç©¶é¢ä¸´ä¸‰ä¸ªæŒç»­å­˜åœ¨çš„å·®è·ï¼šçœŸå®ä¸–ç•ŒCUAä»»åŠ¡çš„ç¨€ç¼ºæ€§ã€ç¼ºä¹å¤šæ¨¡æ€è½¨è¿¹çš„è‡ªåŠ¨æ”¶é›†å’Œæ ‡æ³¨æµç¨‹ï¼Œä»¥åŠç¼ºå°‘ç»Ÿä¸€è¯„ä¼°GUIå®šä½ã€å±å¹•è§£æå’ŒåŠ¨ä½œé¢„æµ‹çš„åŸºå‡†ã€‚GUI-360Â°æ—¨åœ¨è§£å†³è¿™äº›å…³é”®é™åˆ¶ï¼Œä¸ºæ¡Œé¢ç¯å¢ƒä¸­çš„æ™ºèƒ½ä»£ç†æä¾›å…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚

**Method:** GUI-360Â°é‡‡ç”¨LLMå¢å¼ºçš„è‡ªåŠ¨åŒ–æµç¨‹ï¼ŒåŒ…æ‹¬æŸ¥è¯¢æ¥æºã€ç¯å¢ƒæ¨¡æ¿æ„å»ºã€ä»»åŠ¡å®ä¾‹åŒ–ã€æ‰¹é‡æ‰§è¡Œå’ŒLLMé©±åŠ¨çš„è´¨é‡è¿‡æ»¤ã€‚è¯¥æ•°æ®é›†åŒ…å«æ•°åƒä¸ªWindowsåŠå…¬åº”ç”¨ä¸­çš„è½¨è¿¹ï¼Œæä¾›å…¨åˆ†è¾¨ç‡æˆªå›¾ã€å¯è®¿é—®æ€§å…ƒæ•°æ®ã€å®ä¾‹åŒ–ç›®æ ‡ã€ä¸­é—´æ¨ç†è½¨è¿¹ä»¥åŠæˆåŠŸå’Œå¤±è´¥çš„åŠ¨ä½œè½¨è¿¹ï¼Œæ”¯æŒGUIå®šä½ã€å±å¹•è§£æå’ŒåŠ¨ä½œé¢„æµ‹ä¸‰ä¸ªå…¸å‹ä»»åŠ¡ã€‚

**Result:** åœ¨GUI-360Â°ä¸Šå¯¹æœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œåœ¨å®šä½å’ŒåŠ¨ä½œé¢„æµ‹æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„å³ç”¨ä¸è¶³ã€‚ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è™½ç„¶å¸¦æ¥äº†æ˜¾è‘—æ”¹è¿›ï¼Œä½†æœªèƒ½è¾¾åˆ°äººç±»æ°´å¹³çš„å¯é æ€§ã€‚æ•°æ®é›†åŒ…å«è¶…è¿‡120ä¸‡æ‰§è¡ŒåŠ¨ä½œæ­¥éª¤ï¼Œä¸ºCUAç ”ç©¶æä¾›äº†å¤§è§„æ¨¡çš„çœŸå®ä¸–ç•Œè¯„ä¼°æ•°æ®ã€‚

**Conclusion:** GUI-360Â°æ­ç¤ºäº†å½“å‰CUAæ¨¡å‹åœ¨çœŸå®æ¡Œé¢ç¯å¢ƒä¸­çš„å±€é™æ€§ï¼Œå¼ºè°ƒäº†éœ€è¦æ›´å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£å’ŒåŠ¨ä½œè§„åˆ’èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†å’ŒåŸºå‡†çš„å‘å¸ƒå°†ä¿ƒè¿›å¯é‡å¤ç ”ç©¶ï¼ŒåŠ é€Ÿç¨³å¥æ¡Œé¢CUAsçš„å‘å±•ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„è¯„ä¼°åŸºç¡€å’Œæ–¹å‘æŒ‡å¼•ã€‚

---

#### ğŸ“„ Abstract
We introduce GUI-360$^\circ$, a large-scale, comprehensive dataset and
benchmark suite designed to advance computer-using agents (CUAs). CUAs present
unique challenges and is constrained by three persistent gaps: a scarcity of
real-world CUA tasks, the lack of automated collection-and-annotation pipelines
for multi-modal trajectories, and the absence of a unified benchmark that
jointly evaluates GUI grounding, screen parsing, and action prediction.
  GUI-360$^\circ$ addresses these gaps with an LLM-augmented, largely automated
pipeline for query sourcing, environment-template construction, task
instantiation, batched execution, and LLM-driven quality filtering. The
released corpus contains over 1.2M executed action steps across thousands of
trajectories in popular Windows office applications, and includes
full-resolution screenshots, accessibility metadata when available,
instantiated goals, intermediate reasoning traces, and both successful and
failed action trajectories. The dataset supports three canonical tasks, GUI
grounding, screen parsing, and action prediction, and a hybrid GUI+API action
space that reflects modern agent designs. Benchmarking state-of-the-art
vision--language models on GUI-360$^\circ$ reveals substantial out-of-the-box
shortcomings in grounding and action prediction; supervised fine-tuning and
reinforcement learning yield significant gains but do not close the gap to
human-level reliability. We release GUI-360$^\circ$ and accompanying code to
facilitate reproducible research and accelerate progress on robust desktop
CUAs.
  The full dataset has been made public on
https://huggingface.co/datasets/vyokky/GUI-360.
