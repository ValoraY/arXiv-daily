<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-01-09.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 33]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 18]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 7]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-unified-text-image-generation-with-weakness-targeted-post-training">[1] <a href="https://arxiv.org/abs/2601.04339">Unified Text-Image Generation with Weakness-Targeted Post-Training</a></h3>
<p><em>Jiahui Chen, Philippe Hansen-Estruch, Xiaochuang Han, Yushi Hu, Emily Dinan, Amita Kamath, Michal Drozdzal, Reyhane Askari-Hemmat, Luke Zettlemoyer, Marjan Ghazvininejad</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡åè®­ç»ƒå®ç°å®Œå…¨ç»Ÿä¸€çš„æ–‡æœ¬-å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å•æ¬¡æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»åœ°ä»æ–‡æœ¬æ¨ç†è¿‡æ¸¡åˆ°è§†è§‰åˆæˆï¼Œå¹¶é€šè¿‡å¥–åŠ±åŠ æƒå’Œç­–ç•¥æ€§è®¾è®¡çš„åè®­ç»ƒæ•°æ®åœ¨å¤šä¸ªT2IåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆæ¶æ„é€šå¸¸ä¾èµ–æ˜¾å¼çš„æ¨¡æ€åˆ‡æ¢æœºåˆ¶ï¼Œéœ€è¦å…ˆç”Ÿæˆæ¨ç†æ–‡æœ¬å†æ‰‹åŠ¨åˆ‡æ¢åˆ°å›¾åƒç”Ÿæˆï¼Œè¿™ç§åˆ†ç¦»çš„é¡ºåºæ¨ç†è¿‡ç¨‹é™åˆ¶äº†è·¨æ¨¡æ€è€¦åˆå¹¶é˜»ç¢äº†è‡ªåŠ¨åŒ–çš„å¤šæ¨¡æ€ç”Ÿæˆï¼Œå› æ­¤éœ€è¦æ¢ç´¢å®ç°å®Œå…¨ç»Ÿä¸€çš„æ–‡æœ¬-å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶é‡‡ç”¨åè®­ç»ƒæ–¹æ³•å®ç°å®Œå…¨ç»Ÿä¸€çš„æ–‡æœ¬-å›¾åƒç”Ÿæˆï¼Œæ¢ç´¢äº†è”åˆæ–‡æœ¬-å›¾åƒç”Ÿæˆå¯¹T2Iæ€§èƒ½çš„å½±å“ä»¥åŠåè®­ç»ƒä¸­å„æ¨¡æ€çš„ç›¸å¯¹é‡è¦æ€§ï¼Œå¹¶ç ”ç©¶äº†ä¸åŒçš„åè®­ç»ƒæ•°æ®ç­–ç•¥ï¼ŒåŒ…æ‹¬ä½¿ç”¨é’ˆå¯¹ç‰¹å®šé™åˆ¶çš„æœ‰é’ˆå¯¹æ€§æ•°æ®é›†ï¼Œä»¥åŠé‡‡ç”¨ç¦»çº¿å¥–åŠ±åŠ æƒçš„åè®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨å®Œå…¨è‡ªç”Ÿæˆçš„åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œä¸å¹¿æ³›çš„å›¾åƒ-æ ‡é¢˜è¯­æ–™åº“æˆ–åŸºå‡†å¯¹é½æ•°æ®ç›¸æ¯”ï¼Œé’ˆå¯¹ç‰¹å®šé™åˆ¶çš„æœ‰é’ˆå¯¹æ€§æ•°æ®é›†èƒ½å¤Ÿå–å¾—æ›´ä¼˜ç»“æœï¼Œé€šè¿‡å¥–åŠ±åŠ æƒä¸¤ä¸ªæ¨¡æ€å’Œç­–ç•¥æ€§è®¾è®¡çš„åè®­ç»ƒæ•°æ®ï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªä¸åŒçš„T2IåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¤šæ¨¡æ€å›¾åƒç”Ÿæˆçš„æ”¹è¿›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡åè®­ç»ƒå®ç°å®Œå…¨ç»Ÿä¸€çš„æ–‡æœ¬-å›¾åƒç”Ÿæˆæ˜¯å¯è¡Œçš„ï¼Œå¥–åŠ±åŠ æƒä¸¤ä¸ªæ¨¡æ€å’Œç­–ç•¥æ€§è®¾è®¡çš„åè®­ç»ƒæ•°æ®å¯¹äºæå‡å¤šæ¨¡æ€ç”Ÿæˆæ€§èƒ½è‡³å…³é‡è¦ï¼Œè¿™ä¸ºæ„å»ºæ›´è‡ªä¸»ã€è€¦åˆæ€§æ›´å¼ºçš„å¤šæ¨¡æ€ç”Ÿæˆç³»ç»Ÿæä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.</p>
<h3 id="2-packcache-a-training-free-acceleration-method-for-unified-autoregressive-video-generation-via-compact-kv-cache">[2] <a href="https://arxiv.org/abs/2601.04359">PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache</a></h3>
<p><em>Kunyang Li, Mubarak Shah, Yuzhang Shang</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºPackCacheï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„KVç¼“å­˜ç®¡ç†æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€å‹ç¼©ç»Ÿä¸€è‡ªå›å½’è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­çš„KVç¼“å­˜ï¼Œè§£å†³äº†KVç¼“å­˜éšç”Ÿæˆåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿å¯¼è‡´çš„æ¨ç†æ•ˆç‡ç“¶é¢ˆé—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç»Ÿä¸€è‡ªå›å½’è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¾èµ–KVç¼“å­˜æœºåˆ¶å°†æ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦ä»O(TÂ²)é™ä½åˆ°O(T)ï¼Œä½†KVç¼“å­˜å¤§å°éšç”Ÿæˆä»¤ç‰Œæ•°é‡çº¿æ€§å¢é•¿ï¼Œæˆä¸ºé™åˆ¶æ¨ç†æ•ˆç‡å’Œç”Ÿæˆé•¿åº¦çš„ä¸»è¦ç“¶é¢ˆã€‚ç ”ç©¶å‘ç°KVç¼“å­˜ä»¤ç‰Œå…·æœ‰æ˜æ˜¾çš„æ—¶ç©ºç‰¹æ€§ï¼šæ–‡æœ¬å’Œæ¡ä»¶å›¾åƒä»¤ç‰Œä½œä¸ºæŒä¹…è¯­ä¹‰é”šç‚¹æŒç»­è·å¾—é«˜æ³¨æ„åŠ›ï¼Œè€Œå¯¹å…ˆå‰å¸§çš„æ³¨æ„åŠ›éšæ—¶é—´è·ç¦»è‡ªç„¶è¡°å‡ã€‚</p>
<p><strong>Method:</strong> PackCacheé€šè¿‡ä¸‰ç§åè°ƒæœºåˆ¶åŠ¨æ€å‹ç¼©KVç¼“å­˜ï¼šæ¡ä»¶é”šå®šä¿ç•™è¯­ä¹‰å‚è€ƒï¼Œè·¨å¸§è¡°å‡å»ºæ¨¡æ ¹æ®æ—¶é—´è·ç¦»åˆ†é…ç¼“å­˜é¢„ç®—ï¼Œç©ºé—´ä¿æŒä½ç½®åµŒå…¥åœ¨ç¼“å­˜ç§»é™¤æ—¶ç»´æŒè¿è´¯çš„3Dç»“æ„ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œç›´æ¥åº”ç”¨äºç°æœ‰ç»Ÿä¸€è‡ªå›å½’è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> åœ¨48å¸§é•¿åºåˆ—ä¸Šï¼ŒPackCacheå°†ç«¯åˆ°ç«¯ç”Ÿæˆé€Ÿåº¦æå‡1.7-2.2å€ã€‚å¯¹äºå—KVç¼“å­˜æ‰©å±•å½±å“æœ€å¤§çš„æœ€åå››å¸§ï¼ˆè§†é¢‘ä¸­æœ€æ˜‚è´µçš„éƒ¨åˆ†ï¼‰ï¼Œåœ¨A40å’ŒH200ä¸Šåˆ†åˆ«å®ç°2.6å€å’Œ3.7å€çš„åŠ é€Ÿã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†é•¿åºåˆ—è§†é¢‘ç”Ÿæˆçš„æ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> PackCacheé€šè¿‡åˆ©ç”¨KVç¼“å­˜ä»¤ç‰Œçš„æ—¶ç©ºç‰¹æ€§ï¼Œæœ‰æ•ˆè§£å†³äº†ç»Ÿä¸€è‡ªå›å½’è§†é¢‘ç”Ÿæˆä¸­çš„æ¨ç†æ•ˆç‡ç“¶é¢ˆã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†åœ¨ä¸å½±å“ç”Ÿæˆè´¨é‡çš„å‰æä¸‹åŠ¨æ€ç®¡ç†ç¼“å­˜çš„é‡è¦æ€§ï¼Œä¸ºé•¿åºåˆ—è§†é¢‘ç”Ÿæˆæä¾›äº†å®ç”¨çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œå…·æœ‰æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.</p>
<h3 id="3-combining-facial-videos-and-biosignals-for-stress-estimation-during-driving">[3] <a href="https://arxiv.org/abs/2601.04376">Combining facial videos and biosignals for stress estimation during driving</a></h3>
<p><em>Paraskevi Valergaki, Vassilis C. Nicodemou, Iason Oikonomidis, Antonis Argyros, Anastasios Roussos</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè§£è€¦3Dé¢éƒ¨å‡ ä½•ç‰¹å¾å’Œè·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶çš„é©¾é©¶å‹åŠ›è¯†åˆ«æ–¹æ³•ï¼Œé€šè¿‡åˆ†æEMOCAæå–çš„3Dè¡¨æƒ…å’Œå§¿æ€ç³»æ•°ï¼Œç»“åˆTransformeræ—¶åºå»ºæ¨¡æ¡†æ¶ï¼Œå®ç°äº†é«˜ç²¾åº¦çš„å‹åŠ›çŠ¶æ€æ£€æµ‹ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºé¢éƒ¨åŠ¨ä½œå•å…ƒçš„å‹åŠ›è¯†åˆ«æ–¹æ³•é¢ä¸´ä¸»è§‚æ€§å’Œè‡ªä¸»é¢éƒ¨æ§åˆ¶çš„æŒ‘æˆ˜ï¼Œè€Œè§£è€¦çš„3Dé¢éƒ¨å‡ ä½•ç‰¹å¾åœ¨å‹åŠ›è¯†åˆ«ä¸­çš„ä½œç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œç‰¹åˆ«å…³æ³¨åˆ†å¿ƒé©¾é©¶åœºæ™¯ä¸‹çš„å‹åŠ›æ£€æµ‹é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨EMOCAæ¨¡å‹æå–3Dè¡¨æƒ…å’Œå§¿æ€ç³»æ•°ï¼Œé€šè¿‡é…å¯¹å‡è®¾æ£€éªŒåˆ†æåŸºçº¿å’Œå‹åŠ›é˜¶æ®µçš„å·®å¼‚ã€‚åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºäº†åŸºäºTransformerçš„æ—¶åºå»ºæ¨¡æ¡†æ¶ï¼Œè¯„ä¼°äº†å•æ¨¡æ€ã€æ—©æœŸèåˆå’Œè·¨æ¨¡æ€æ³¨æ„åŠ›ä¸‰ç§èåˆç­–ç•¥ï¼Œç‰¹åˆ«å…³æ³¨EMOCAç‰¹å¾ä¸ç”Ÿç†ä¿¡å·ã€æ³¨è§†ä¿¡å·çš„è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆæ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°56ä¸ªEMOCAç³»æ•°ä¸­æœ‰41ä¸ªåœ¨å‹åŠ›é˜¶æ®µè¡¨ç°å‡ºæ˜¾è‘—ä¸”ä¸€è‡´çš„å“åº”æ¨¡å¼ï¼Œä¸ç”Ÿç†æ ‡è®°ç‰©ç›¸å½“ã€‚è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆæ–¹æ³•è¡¨ç°æœ€ä½³ï¼ŒEMOCAä¸ç”Ÿç†ä¿¡å·èåˆè¾¾åˆ°AUROC 92%å’Œå‡†ç¡®ç‡86.7%ï¼ŒEMOCAä¸æ³¨è§†ä¿¡å·èåˆä¹Ÿè¾¾åˆ°AUROC 91.8%çš„ç«äº‰æ€§æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯å®äº†è§£è€¦3Dé¢éƒ¨å‡ ä½•ç‰¹å¾åœ¨å‹åŠ›è¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶åœ¨æ—¶åºå»ºæ¨¡ä¸­çš„ä¼˜åŠ¿ã€‚è¯¥æ–¹æ³•ä¸ºåŸºäºè§†è§‰çš„å‹åŠ›æ£€æµ‹æä¾›äº†æ–°æ€è·¯ï¼Œç‰¹åˆ«é€‚ç”¨äºé©¾é©¶ç›‘æ§ç­‰å®é™…åº”ç”¨åœºæ™¯ï¼Œè¡¨æ˜å¤šæ¨¡æ€èåˆç­–ç•¥èƒ½æ˜¾è‘—æå‡è¯†åˆ«æ€§èƒ½ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Reliable stress recognition from facial videos is challenging due to stress's subjective nature and voluntary facial control. While most methods rely on Facial Action Units, the role of disentangled 3D facial geometry remains underexplored. We address this by analyzing stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Paired hypothesis tests between baseline and stressor phases reveal that 41 of 56 coefficients show consistent, phase-specific stress responses comparable to physiological markers. Building on this, we propose a Transformer-based temporal modeling framework and assess unimodal, early-fusion, and cross-modal attention strategies. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92\%, Accuracy 86.7\%), with EMOCA-gaze fusion also competitive (AUROC 91.8\%). This highlights the effectiveness of temporal modeling and cross-modal attention for stress recognition.</p>
<h3 id="4-3d-agenttri-modal-multi-agent-collaboration-for-scalable-3d-object-annotation">[4] <a href="https://arxiv.org/abs/2601.04404">3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation</a></h3>
<p><em>Jusheng Zhang, Yijia Fan, Zimo Wen, Jian Wang, Keze Wang</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºTri-MARFæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆ2Då¤šè§†è§’å›¾åƒã€æ–‡æœ¬æè¿°å’Œ3Dç‚¹äº‘çš„ä¸‰æ¨¡æ€è¾“å…¥ï¼Œå¹¶é‡‡ç”¨å¤šæ™ºèƒ½ä½“åä½œæ¶æ„ï¼Œæ˜¾è‘—æå‡äº†å¤§è§„æ¨¡3Då¯¹è±¡æ ‡æ³¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> 3Då¯¹è±¡æ ‡æ³¨åœ¨è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå’Œå¢å¼ºç°å®ç­‰åº”ç”¨ä¸­é¢ä¸´ç©ºé—´å¤æ‚æ€§ã€é®æŒ¡å’Œè§†è§’ä¸ä¸€è‡´ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰åŸºäºå•ä¸€æ¨¡å‹çš„æ–¹æ³•å¾€å¾€éš¾ä»¥æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ï¼Œéœ€è¦æ›´å¼ºå¤§çš„å¤šæ¨¡æ€åä½œæ¡†æ¶æ¥æå‡æ ‡æ³¨æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> Tri-MARFæ¡†æ¶æ•´åˆä¸‰æ¨¡æ€è¾“å…¥ï¼ˆ2Då¤šè§†è§’å›¾åƒã€æ–‡æœ¬æè¿°å’Œ3Dç‚¹äº‘ï¼‰ï¼Œé‡‡ç”¨å¤šæ™ºèƒ½ä½“åä½œæ¶æ„ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªä¸“é—¨åŒ–æ™ºèƒ½ä½“ï¼šè§†è§‰è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ç”¨äºç”Ÿæˆå¤šè§†è§’æè¿°ï¼Œä¿¡æ¯èšåˆæ™ºèƒ½ä½“ç”¨äºé€‰æ‹©æœ€ä¼˜æè¿°ï¼Œä»¥åŠé—¨æ§æ™ºèƒ½ä½“ç”¨äºå¯¹é½æ–‡æœ¬è¯­ä¹‰ä¸3Då‡ ä½•ä¿¡æ¯ä»¥å®ç°ç²¾ç»†åŒ–æ ‡æ³¨ã€‚</p>
<p><strong>Result:</strong> åœ¨Objaverseã€LVISã€Objaverse XLå’ŒABOæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTri-MARFæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒCLIPScoreè¾¾åˆ°88.7åˆ†ï¼ŒViLT R@5æ£€ç´¢å‡†ç¡®ç‡åˆ†åˆ«ä¸º45.2%å’Œ43.8%ï¼Œåœ¨å•ä¸ªNVIDIA A100 GPUä¸Šå®ç°é«˜è¾¾æ¯å°æ—¶12000ä¸ªå¯¹è±¡çš„å¤„ç†ååé‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€è¾“å…¥ä¸å¤šæ™ºèƒ½ä½“åä½œæ¶æ„åœ¨3Då¯¹è±¡æ ‡æ³¨ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤æ‚3Dåœºæ™¯ç†è§£æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆæ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†åœ¨å®é™…åº”ç”¨ä¸­å®ç°é«˜æ•ˆç‡å¤§è§„æ¨¡æ ‡æ³¨çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU</p>
<h3 id="5-addressing-overthinking-in-large-vision-language-models-via-gated-perception-reasoning-optimization">[5] <a href="https://arxiv.org/abs/2601.04442">Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization</a></h3>
<p><em>Xingjian Diao, Zheyuan Liu, Chunhui Zhang, Weiyi Wu, Keyi Kong, Lin Shi, Kaize Ding, Soroush Vosoughi, Jiang Gui</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGPROï¼ˆé—¨æ§æ„ŸçŸ¥-æ¨ç†ä¼˜åŒ–ï¼‰ï¼Œä¸€ç§å…ƒæ¨ç†æ§åˆ¶å™¨ï¼Œé€šè¿‡åŠ¨æ€è·¯ç”±è®¡ç®—è·¯å¾„æ¥è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œåœ¨æå‡ä»»åŠ¡å‡†ç¡®ç‡çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹é€šè¿‡æ€ç»´é“¾æœºåˆ¶å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™ç±»æ…¢æ€è€ƒæ–¹æ³•å¸¸å¯¼è‡´è¿‡åº¦æ€è€ƒé—®é¢˜ï¼Œå³æ¨¡å‹å¯¹ç®€å•æŸ¥è¯¢ç”Ÿæˆè¿‡äºå†—é•¿çš„å“åº”ï¼Œé€ æˆæµ‹è¯•æ—¶æ•ˆç‡ä½ä¸‹ç”šè‡³å‡†ç¡®ç‡ä¸‹é™ã€‚å…ˆå‰ç ”ç©¶å°è¯•é€šè¿‡è‡ªé€‚åº”æ¨ç†ç­–ç•¥ç¼“è§£æ­¤é—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•å¤§å¤šå¿½è§†äº†ä¸€ä¸ªæ ¹æœ¬ç“¶é¢ˆï¼šè§†è§‰æ„ŸçŸ¥å¤±è´¥ã€‚æˆ‘ä»¬è®¤ä¸ºç¨³å®šæ¨ç†å…³é”®ä¾èµ–äºä½å±‚æ¬¡è§†è§‰åŸºç¡€ï¼Œæ¨ç†é”™è¯¯å¾€å¾€æºäºä¸å®Œç¾çš„æ„ŸçŸ¥è€Œéä¸è¶³çš„æ·±æ€ç†Ÿè™‘ã€‚</p>
<p><strong>Method:</strong> ä¸ºè§£å†³ä¸Šè¿°é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºGPROï¼ˆé—¨æ§æ„ŸçŸ¥-æ¨ç†ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å…ƒæ¨ç†æ§åˆ¶å™¨ï¼Œåœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤åŠ¨æ€è·¯ç”±è®¡ç®—åˆ°ä¸‰ä¸ªå†³ç­–è·¯å¾„ï¼šè½»é‡çº§å¿«é€Ÿè·¯å¾„ã€ç”¨äºé‡æ–°æ£€æŸ¥è§†è§‰è¾“å…¥çš„æ…¢æ„ŸçŸ¥è·¯å¾„ï¼Œä»¥åŠç”¨äºå†…éƒ¨è‡ªæˆ‘åæ€çš„æ…¢æ¨ç†è·¯å¾„ã€‚ä¸ºå­¦ä¹ è¿™ç§åŒºåˆ†ï¼Œæˆ‘ä»¬ä»çº¦79ä¸‡ä¸ªæ ·æœ¬ä¸­æ¨å¯¼å‡ºå¤§è§„æ¨¡å¤±è´¥å½’å› ç›‘ç£ï¼Œä½¿ç”¨æ•™å¸ˆæ¨¡å‹æ¥åŒºåˆ†æ„ŸçŸ¥å¹»è§‰ä¸æ¨ç†é”™è¯¯ã€‚ç„¶åé€šè¿‡å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ§åˆ¶å™¨ï¼Œåœ¨ä¸ç¡®å®šæ€§ä¸‹ä¼˜åŒ–ä»»åŠ¡å‡†ç¡®ç‡ä¸è®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡ã€‚</p>
<p><strong>Result:</strong> åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGPROæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡å’Œæ•ˆç‡ï¼Œä¼˜äºæœ€è¿‘çš„æ…¢æ€è€ƒæ–¹æ³•ï¼ŒåŒæ—¶ç”Ÿæˆæ˜æ˜¾æ›´çŸ­çš„å“åº”ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒé«˜ä»»åŠ¡å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæœ‰æ•ˆé™ä½äº†è®¡ç®—å¼€é”€ï¼Œå®ç°äº†ç²¾åº¦ä¸æ•ˆç‡çš„å¹³è¡¡ä¼˜åŒ–ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†è§†è§‰æ„ŸçŸ¥å¤±è´¥æ˜¯å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†é”™è¯¯çš„é‡è¦æ ¹æºï¼Œè€Œéä»…ä»…æ¨ç†èƒ½åŠ›ä¸è¶³ã€‚GPROæ¡†æ¶é€šè¿‡åŠ¨æ€è·¯ç”±æœºåˆ¶å®ç°äº†æ„ŸçŸ¥ä¸æ¨ç†çš„ååŒä¼˜åŒ–ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆã€æ›´å‡†ç¡®çš„è§†è§‰è¯­è¨€ç³»ç»Ÿæä¾›äº†æ–°èŒƒå¼ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ•´åˆä½çº§è§†è§‰åŸºç¡€ä¸é«˜çº§è®¤çŸ¥è¿‡ç¨‹çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥è‡ªé€‚åº”è®¡ç®—æ¶æ„è®¾è®¡æä¾›äº†é‡è¦å¯ç¤ºã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.</p>
<h3 id="6-unidrive-wm-unified-understanding-planning-and-generation-world-model-for-autonomous-driving">[6] <a href="https://arxiv.org/abs/2601.04453">UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving</a></h3>
<p><em>Zhexiao Xiong, Xin Ye, Burhan Yaman, Sheng Cheng, Yiren Lu, Jingru Luo, Nathan Jacobs, Liu Ren</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºUniDrive-WMï¼Œä¸€ç§ç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸–ç•Œæ¨¡å‹ï¼Œå°†é©¾é©¶åœºæ™¯ç†è§£ã€è½¨è¿¹è§„åˆ’å’Œè½¨è¿¹æ¡ä»¶æœªæ¥å›¾åƒç”Ÿæˆé›†æˆåˆ°å•ä¸€æ¶æ„ä¸­ï¼Œæ˜¾è‘—æå‡äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿé€šå¸¸å°†æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’ä½œä¸ºç‹¬ç«‹æ¨¡å—å¤„ç†ï¼Œè¿™ç§åˆ†ç¦»é™åˆ¶äº†ç³»ç»Ÿæ€§èƒ½ã€‚ç°æœ‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è§„åˆ’æ–¹æ³•æœªèƒ½å®ç°è¿™äº›åŠŸèƒ½çš„ç»Ÿä¸€é›†æˆï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½å¤Ÿè”åˆæ‰§è¡Œé©¾é©¶åœºæ™¯ç†è§£ã€è½¨è¿¹è§„åˆ’å’Œæœªæ¥å›¾åƒç”Ÿæˆçš„ç»Ÿä¸€ä¸–ç•Œæ¨¡å‹ã€‚</p>
<p><strong>Method:</strong> UniDrive-WMé‡‡ç”¨ç»Ÿä¸€çš„VLMæ¶æ„ï¼Œå…¶è½¨è¿¹è§„åˆ’å™¨é¢„æµ‹æœªæ¥è½¨è¿¹ï¼Œè¯¥è½¨è¿¹éšåæ¡ä»¶åŒ–VLMå›¾åƒç”Ÿæˆå™¨ä»¥ç”Ÿæˆåˆç†çš„æœªæ¥å¸§ã€‚è¿™äº›é¢„æµ‹æä¾›é¢å¤–çš„ç›‘ç£ä¿¡å·ï¼Œå¢å¼ºåœºæ™¯ç†è§£å¹¶è¿­ä»£ä¼˜åŒ–è½¨è¿¹ç”Ÿæˆã€‚ç ”ç©¶è¿˜æ¯”è¾ƒäº†æœªæ¥å›¾åƒé¢„æµ‹çš„ç¦»æ•£å’Œè¿ç»­è¾“å‡ºè¡¨ç¤ºï¼Œåˆ†æå®ƒä»¬å¯¹ä¸‹æ¸¸é©¾é©¶æ€§èƒ½çš„å½±å“ã€‚</p>
<p><strong>Result:</strong> åœ¨Bench2DriveåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUniDrive-WMç”Ÿæˆé«˜ä¿çœŸæœªæ¥å›¾åƒï¼Œå¹¶å°†è§„åˆ’æ€§èƒ½æå‡5.9%çš„L2è½¨è¿¹è¯¯å·®å’Œ9.2%çš„ç¢°æ’ç‡ï¼Œä¼˜äºå…ˆå‰æœ€ä½³æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨è”åˆæ¨ç†ã€è§„åˆ’å’Œç”Ÿæˆå»ºæ¨¡æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ç´§å¯†é›†æˆVLMé©±åŠ¨çš„æ¨ç†ã€è§„åˆ’å’Œç”Ÿæˆä¸–ç•Œå»ºæ¨¡å¯¹è‡ªåŠ¨é©¾é©¶å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚ç»Ÿä¸€æ¶æ„é€šè¿‡è½¨è¿¹æ¡ä»¶å›¾åƒç”Ÿæˆæä¾›é¢å¤–ç›‘ç£ä¿¡å·ï¼Œè¿­ä»£ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹è®¾è®¡æä¾›äº†æ–°èŒƒå¼ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .</p>
<h3 id="7-vision-language-agents-for-interactive-forest-change-analysis">[7] <a href="https://arxiv.org/abs/2601.04497">Vision-Language Agents for Interactive Forest Change Analysis</a></h3>
<p><em>James Brock, Ce Zhang, Nantheera Anantrasirichai</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„é›†æˆæ£®æ—å˜åŒ–åˆ†ææ™ºèƒ½ä½“ï¼Œæ”¯æŒè·¨å¤šä»»åŠ¡çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œå¹¶å¼•å…¥äº†åŒ…å«å¤šç²’åº¦è¯­ä¹‰å˜åŒ–æè¿°çš„Forest-Changeæ•°æ®é›†ï¼Œæ˜¾è‘—æå‡äº†æ£®æ—å˜åŒ–ç›‘æµ‹çš„å¯è®¿é—®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ£®æ—ç›‘æµ‹é¢ä¸´åƒç´ çº§å˜åŒ–æ£€æµ‹å’Œå¤æ‚æ£®æ—åŠ¨æ€è¯­ä¹‰å˜åŒ–æè¿°çš„æŒç»­æŒ‘æˆ˜ï¼Œå°½ç®¡å¤§è¯­è¨€æ¨¡å‹æ­£è¢«ç”¨äºäº¤äº’å¼æ•°æ®æ¢ç´¢ï¼Œä½†å…¶ä¸è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¥æ„Ÿå›¾åƒå˜åŒ–è§£é‡Šé¢†åŸŸçš„é›†æˆä»æœªè¢«å……åˆ†æ¢ç´¢ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„é›†æˆæ£®æ—å˜åŒ–åˆ†ææ™ºèƒ½ä½“ï¼Œè¯¥ç³»ç»Ÿæ„å»ºåœ¨å¤šçº§å˜åŒ–è§£é‡Šè§†è§‰è¯­è¨€éª¨å¹²ç½‘ç»œä¹‹ä¸Šï¼Œé‡‡ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œä»»åŠ¡ç¼–æ’ï¼Œå¹¶å¼•å…¥äº†Forest-Changeæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«åŒæ—¶ç›¸å«æ˜Ÿå½±åƒã€åƒç´ çº§å˜åŒ–æ©ç ä»¥åŠç»“åˆäººå·¥æ ‡æ³¨å’Œè§„åˆ™æ–¹æ³•ç”Ÿæˆçš„å¤šç²’åº¦è¯­ä¹‰å˜åŒ–æè¿°ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æç³»ç»Ÿåœ¨Forest-Changeæ•°æ®é›†ä¸Šå–å¾—äº†67.10%çš„mIoUå’Œ40.17%çš„BLEU-4åˆ†æ•°ï¼Œåœ¨LEVIR-MCI-TreesåŸºå‡†ï¼ˆä¸“æ³¨äºæ ‘æœ¨çš„è”åˆå˜åŒ–æ£€æµ‹ä¸æè¿°å­é›†ï¼‰ä¸Šåˆ†åˆ«è¾¾åˆ°88.13%å’Œ34.41%ï¼ŒéªŒè¯äº†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†äº¤äº’å¼ã€å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„é¥æ„Ÿå›¾åƒå˜åŒ–è§£é‡Šç³»ç»Ÿåœ¨æå‡æ£®æ—å˜åŒ–åˆ†æå¯è®¿é—®æ€§ã€å¯è§£é‡Šæ€§å’Œæ•ˆç‡æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºé¥æ„Ÿä¸è‡ªç„¶è¯­è¨€å¤„ç†çš„äº¤å‰é¢†åŸŸæä¾›äº†æ–°çš„æŠ€æœ¯æ¡†æ¶ï¼Œæ‰€æœ‰æ•°æ®å’Œä»£ç å‡å·²å…¬å¼€ä»¥ä¿ƒè¿›åç»­ç ”ç©¶ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.</p>
<h3 id="8-all-changes-may-have-invariant-principles-improving-ever-shifting-harmful-meme-detection-via-design-concept-reproduction">[8] <a href="https://arxiv.org/abs/2601.04567">All Changes May Have Invariant Principles: Improving Ever-Shifting Harmful Meme Detection via Design Concept Reproduction</a></h3>
<p><em>Ziyou Jiang, Mingyang Li, Junjie Wang, Yuekai Huang, Jie Huang, Zhiyuan Chang, Zhaoyang Li, Qing Wang</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºRepMDæ–¹æ³•ï¼Œé€šè¿‡å†ç°æ¶æ„ç”¨æˆ·çš„è®¾è®¡æ¦‚å¿µæ¥æ£€æµ‹ä¸æ–­æ¼”å˜çš„ç½‘ç»œæœ‰å®³æ¨¡å› ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è®¾è®¡æ¦‚å¿µå›¾æŒ‡å¯¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ£€æµ‹ï¼Œåœ¨ç±»å‹æ¼”å˜å’Œæ—¶é—´æ¼”å˜çš„æ¨¡å› ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç½‘ç»œæœ‰å®³æ¨¡å› å…·æœ‰ç±»å‹æ¼”å˜å’Œæ—¶é—´æ¼”å˜çš„ç‰¹æ€§ï¼Œéš¾ä»¥åˆ†ææ£€æµ‹ï¼Œå°½ç®¡å…·ä½“æ¨¡å› ä¸æ–­å˜åŒ–ï¼Œä½†ä¸åŒæ¨¡å› å¯èƒ½å…±äº«ä¸å˜çš„è®¾è®¡åŸåˆ™ï¼Œå³æ¶æ„ç”¨æˆ·èƒŒåçš„è®¾è®¡æ¦‚å¿µï¼Œè¿™æœ‰åŠ©äºç†è§£æ¨¡å› ä¸ºä½•æœ‰å®³å¹¶å®ç°æœ‰æ•ˆæ£€æµ‹ã€‚</p>
<p><strong>Method:</strong> RepMDæ–¹æ³•é¦–å…ˆå‚è€ƒæ”»å‡»æ ‘å®šä¹‰è®¾è®¡æ¦‚å¿µå›¾æ¥æè¿°è®¾è®¡æœ‰å®³æ¨¡å› çš„æ­¥éª¤ï¼Œç„¶åé€šè¿‡è®¾è®¡æ­¥éª¤å†ç°å’Œå›¾å‰ªæä»å†å²æ¨¡å› ä¸­æ¨å¯¼DCGï¼Œæœ€åä½¿ç”¨DCGæŒ‡å¯¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæœ‰å®³æ¨¡å› æ£€æµ‹ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°ç»“æœæ˜¾ç¤ºRepMDè¾¾åˆ°81.1%çš„æœ€é«˜å‡†ç¡®ç‡ï¼Œåœ¨æ³›åŒ–åˆ°ç±»å‹æ¼”å˜å’Œæ—¶é—´æ¼”å˜çš„æ¨¡å› æ—¶å‡†ç¡®ç‡ä»…æœ‰è½»å¾®ä¸‹é™ï¼Œäººå·¥è¯„ä¼°è¡¨æ˜RepMDèƒ½å°†äººç±»å‘ç°æœ‰å®³æ¨¡å› çš„æ•ˆç‡æé«˜åˆ°æ¯æ¨¡å› 15-30ç§’ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡æ•æ‰æ¶æ„ç”¨æˆ·çš„ä¸å˜è®¾è®¡æ¦‚å¿µè€Œéå…·ä½“å†…å®¹ï¼Œä¸ºæ£€æµ‹ä¸æ–­æ¼”å˜çš„ç½‘ç»œæœ‰å®³æ¨¡å› æä¾›äº†æœ‰æ•ˆæ–¹æ³•ï¼Œè®¾è®¡æ¦‚å¿µå›¾æ¡†æ¶èƒ½å¤ŸæŒ‡å¯¼MLLMæå‡æ£€æµ‹æ€§èƒ½ï¼Œåœ¨åŠ¨æ€ç½‘ç»œç¯å¢ƒä¸­å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Harmful memes are ever-shifting in the Internet communities, which are difficult to analyze due to their type-shifting and temporal-evolving nature. Although these memes are shifting, we find that different memes may share invariant principles, i.e., the underlying design concept of malicious users, which can help us analyze why these memes are harmful. In this paper, we propose RepMD, an ever-shifting harmful meme detection method based on the design concept reproduction. We first refer to the attack tree to define the Design Concept Graph (DCG), which describes steps that people may take to design a harmful meme. Then, we derive the DCG from historical memes with design step reproduction and graph pruning. Finally, we use DCG to guide the Multimodal Large Language Model (MLLM) to detect harmful memes. The evaluation results show that RepMD achieves the highest accuracy with 81.1% and has slight accuracy decreases when generalized to type-shifting and temporal-evolving memes. Human evaluation shows that RepMD can improve the efficiency of human discovery on harmful memes, with 15$\sim$30 seconds per meme.</p>
<h3 id="9-mildedit-reasoning-based-multi-layer-design-document-editing">[9] <a href="https://arxiv.org/abs/2601.04589">MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing</a></h3>
<p><em>Zihao Lin, Wanrong Zhu, Jiuxiang Gu, Jihyung Kil, Christopher Tensmeyer, Lin Zhang, Shilong Liu, Ruiyi Zhang, Lifu Huang, Vlad I. Morariu, Tong Sun</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MiLDEAgentï¼Œä¸€ä¸ªåŸºäºæ¨ç†çš„å¤šå±‚è®¾è®¡æ–‡æ¡£ç¼–è¾‘æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆRLè®­ç»ƒçš„å¤šæ¨¡æ€æ¨ç†å™¨å’Œå›¾åƒç¼–è¾‘å™¨ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤šå±‚æ–‡æ¡£ç¼–è¾‘ä¸­çš„å±€é™æ€§ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†MiLDEBenchåŸºå‡†æ•°æ®é›†å’ŒMiLDEEvalè¯„ä¼°åè®®ï¼Œä¸ºå¤šå±‚æ–‡æ¡£ç¼–è¾‘å»ºç«‹äº†é¦–ä¸ªå¼ºåŸºçº¿ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°å®ä¸–ç•Œä¸­çš„è®¾è®¡æ–‡æ¡£ï¼ˆå¦‚æµ·æŠ¥ï¼‰æœ¬è´¨ä¸Šæ˜¯å¤šå±‚çš„ï¼ŒåŒ…å«è£…é¥°ã€æ–‡æœ¬å’Œå›¾åƒç­‰å¤šç§å…ƒç´ ã€‚ä»è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç¼–è¾‘è¿™äº›æ–‡æ¡£éœ€è¦ç»†ç²’åº¦çš„ã€å±‚æ„ŸçŸ¥çš„æ¨ç†èƒ½åŠ›æ¥è¯†åˆ«ç›¸å…³å±‚å¹¶åè°ƒä¿®æ”¹ã€‚å…ˆå‰çš„ç ”ç©¶å¤§å¤šå¿½è§†äº†å¤šå±‚è®¾è®¡æ–‡æ¡£ç¼–è¾‘é—®é¢˜ï¼Œä¸»è¦å…³æ³¨å•å±‚å›¾åƒç¼–è¾‘æˆ–å¤šå±‚ç”Ÿæˆï¼Œè¿™äº›æ–¹æ³•å‡è®¾å¹³é¢ç”»å¸ƒä¸”ç¼ºä¹ç¡®å®šä¿®æ”¹å†…å®¹å’Œä½ç½®çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡å¼•å…¥äº†å¤šå±‚æ–‡æ¡£ç¼–è¾‘ä»£ç†ï¼ˆMiLDEAgentï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ¨ç†çš„æ¡†æ¶ï¼Œç»“åˆäº†RLè®­ç»ƒçš„å¤šæ¨¡æ€æ¨ç†å™¨ç”¨äºå±‚æ„ŸçŸ¥ç†è§£å’Œå›¾åƒç¼–è¾‘å™¨ç”¨äºç›®æ ‡ä¿®æ”¹ã€‚ç ”ç©¶è¿˜å»ºç«‹äº†MiLDEBenchåŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡20Kè®¾è®¡æ–‡æ¡£å’Œå¤šæ ·åŒ–ç¼–è¾‘æŒ‡ä»¤çš„äººå·¥å‚ä¸è¯­æ–™åº“ï¼Œå¹¶é…å¥—äº†MiLDEEvalè¯„ä¼°åè®®ï¼Œæ¶µç›–æŒ‡ä»¤éµå¾ªã€å¸ƒå±€ä¸€è‡´æ€§ã€ç¾å­¦å’Œæ–‡æœ¬æ¸²æŸ“å››ä¸ªç»´åº¦ã€‚</p>
<p><strong>Result:</strong> åœ¨14ä¸ªå¼€æºæ¨¡å‹å’Œ2ä¸ªé—­æºæ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ³›åŒ–ï¼šå¼€æºæ¨¡å‹é€šå¸¸æ— æ³•å®Œæˆå¤šå±‚æ–‡æ¡£ç¼–è¾‘ä»»åŠ¡ï¼Œè€Œé—­æºæ¨¡å‹å­˜åœ¨æ ¼å¼è¿è§„é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMiLDEAgentå®ç°äº†å¼ºå¤§çš„å±‚æ„ŸçŸ¥æ¨ç†å’Œç²¾ç¡®ç¼–è¾‘ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰å¼€æºåŸºçº¿ï¼Œå¹¶è¾¾åˆ°ä¸é—­æºæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œä»è€Œä¸ºå¤šå±‚æ–‡æ¡£ç¼–è¾‘å»ºç«‹äº†é¦–ä¸ªå¼ºåŸºçº¿ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¤šå±‚è®¾è®¡æ–‡æ¡£ç¼–è¾‘é¢†åŸŸæä¾›äº†ç³»ç»ŸåŒ–çš„åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ï¼Œè¯æ˜äº†åŸºäºæ¨ç†çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚å¤šå±‚ç¼–è¾‘ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚MiLDEAgentçš„æˆåŠŸè¡¨æ˜ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¤šæ¨¡æ€æ¨ç†ä¸ç›®æ ‡å›¾åƒç¼–è¾‘å¯ä»¥æ˜¾è‘—æå‡å±‚æ„ŸçŸ¥ç¼–è¾‘èƒ½åŠ›ï¼Œä¸ºæœªæ¥æ–‡æ¡£ç¼–è¾‘ç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Real-world design documents (e.g., posters) are inherently multi-layered, combining decoration, text, and images. Editing them from natural-language instructions requires fine-grained, layer-aware reasoning to identify relevant layers and coordinate modifications. Prior work largely overlooks multi-layer design document editing, focusing instead on single-layer image editing or multi-layer generation, which assume a flat canvas and lack the reasoning needed to determine what and where to modify. To address this gap, we introduce the Multi-Layer Document Editing Agent (MiLDEAgent), a reasoning-based framework that combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. To systematically benchmark this setting, we introduce the MiLDEBench, a human-in-the-loop corpus of over 20K design documents paired with diverse editing instructions. The benchmark is complemented by a task-specific evaluation protocol, MiLDEEval, which spans four dimensions including instruction following, layout consistency, aesthetics, and text rendering. Extensive experiments on 14 open-source and 2 closed-source models reveal that existing approaches fail to generalize: open-source models often cannot complete multi-layer document editing tasks, while closed-source models suffer from format violations. In contrast, MiLDEAgent achieves strong layer-aware reasoning and precise editing, significantly outperforming all open-source baselines and attaining performance comparable to closed-source models, thereby establishing the first strong baseline for multi-layer document editing.</p>
<h3 id="10-hyperalign-hyperbolic-entailment-cones-for-adaptive-text-to-image-alignment-assessment">[10] <a href="https://arxiv.org/abs/2601.04614">HyperAlign: Hyperbolic Entailment Cones for Adaptive Text-to-Image Alignment Assessment</a></h3>
<p><em>Wenzhi Chen, Bo Hu, Leida Li, Lihuo He, Wen Lu, Xinbo Gao</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºHyperAlignï¼Œä¸€ç§åŸºäºåŒæ›²è•´å«å‡ ä½•çš„è‡ªé€‚åº”æ–‡æœ¬-å›¾åƒå¯¹é½è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡å°†CLIPç‰¹å¾æ˜ å°„åˆ°åŒæ›²ç©ºé—´å¹¶è®¾è®¡åŠ¨æ€ç›‘ç£è•´å«å»ºæ¨¡æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç”Ÿæˆä¸æ–‡æœ¬æç¤ºå¯¹é½è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå‡†ç¡®è¯„ä¼°ç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æç¤ºä¹‹é—´çš„å¯¹é½æ€§æˆä¸ºå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–æ¬§å‡ é‡Œå¾—ç©ºé—´åº¦é‡ï¼Œå¿½è§†äº†è¯­ä¹‰å¯¹é½çš„ç»“æ„åŒ–ç‰¹æ€§ï¼ŒåŒæ—¶ç¼ºä¹å¯¹ä¸åŒæ ·æœ¬çš„è‡ªé€‚åº”èƒ½åŠ›ï¼Œè¿™äº›å±€é™æ€§ä¿ƒä½¿ç ”ç©¶è€…å¼€å‘æ›´æœ‰æ•ˆçš„è¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> HyperAlignæ¡†æ¶é¦–å…ˆä½¿ç”¨CLIPæå–æ¬§å‡ é‡Œå¾—ç‰¹å¾å¹¶å°†å…¶æ˜ å°„åˆ°åŒæ›²ç©ºé—´ï¼›å…¶æ¬¡è®¾è®¡åŠ¨æ€ç›‘ç£è•´å«å»ºæ¨¡æœºåˆ¶ï¼Œå°†ç¦»æ•£çš„è•´å«é€»è¾‘è½¬åŒ–ä¸ºè¿ç»­çš„å‡ ä½•ç»“æ„ç›‘ç£ï¼›æœ€åæå‡ºè‡ªé€‚åº”è°ƒåˆ¶å›å½’å™¨ï¼Œåˆ©ç”¨åŒæ›²å‡ ä½•ç‰¹å¾ç”Ÿæˆæ ·æœ¬çº§è°ƒåˆ¶å‚æ•°ï¼Œè‡ªé€‚åº”æ ¡å‡†æ¬§å‡ é‡Œå¾—ä½™å¼¦ç›¸ä¼¼åº¦ä»¥é¢„æµ‹æœ€ç»ˆå¾—åˆ†ã€‚</p>
<p><strong>Result:</strong> HyperAlignåœ¨å•æ•°æ®åº“è¯„ä¼°å’Œè·¨æ•°æ®åº“æ³›åŒ–ä»»åŠ¡ä¸­å‡å–å¾—äº†é«˜åº¦ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ï¼Œå……åˆ†éªŒè¯äº†åŒæ›²å‡ ä½•å»ºæ¨¡åœ¨å›¾åƒ-æ–‡æœ¬å¯¹é½è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»ŸåŸºäºæ¬§å‡ é‡Œå¾—ç©ºé—´åº¦é‡çš„æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŒæ›²å‡ ä½•å»ºæ¨¡èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è¯­ä¹‰å¯¹é½çš„ç»“æ„åŒ–ç‰¹æ€§ï¼Œä¸ºæ–‡æœ¬-å›¾åƒå¯¹é½è¯„ä¼°æä¾›äº†æ–°çš„å‡ ä½•è§†è§’ã€‚è‡ªé€‚åº”è°ƒåˆ¶æœºåˆ¶è§£å†³äº†æ ·æœ¬é—´å·®å¼‚é—®é¢˜ï¼Œä¸ºç”Ÿæˆå¼AIçš„è´¨é‡è¯„ä¼°å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºå’Œåº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>With the rapid development of text-to-image generation technology, accurately assessing the alignment between generated images and text prompts has become a critical challenge. Existing methods rely on Euclidean space metrics, neglecting the structured nature of semantic alignment, while lacking adaptive capabilities for different samples. To address these limitations, we propose HyperAlign, an adaptive text-to-image alignment assessment framework based on hyperbolic entailment geometry. First, we extract Euclidean features using CLIP and map them to hyperbolic space. Second, we design a dynamic-supervision entailment modeling mechanism that transforms discrete entailment logic into continuous geometric structure supervision. Finally, we propose an adaptive modulation regressor that utilizes hyperbolic geometric features to generate sample-level modulation parameters, adaptively calibrating Euclidean cosine similarity to predict the final score. HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks, fully validating the effectiveness of hyperbolic geometric modeling for image-text alignment assessment.</p>
<h3 id="11-agri-r1-empowering-generalizable-agricultural-reasoning-in-vision-language-models-with-reinforcement-learning">[11] <a href="https://arxiv.org/abs/2601.04672">Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning</a></h3>
<p><em>Wentao Zhang, Lifei Wang, Lina Lu, MingKun Xu, Shangyang Li, Yanchao Yang, Tao Fang</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºAgri-R1ï¼Œä¸€ç§é¢å‘å†œä¸šçš„æ¨ç†å¢å¼ºå¤§æ¨¡å‹ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–é«˜è´¨é‡æ¨ç†æ•°æ®ç”Ÿæˆå’Œåˆ›æ–°çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨ä»…ä½¿ç”¨19%å¯ç”¨æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†å†œä¸šç—…å®³è¯Šæ–­çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å†œä¸šç—…å®³è¯Šæ–­å¯¹è§†è§‰è¯­è¨€æ¨¡å‹æå‡ºæŒ‘æˆ˜ï¼Œä¼ ç»Ÿå¾®è°ƒæ–¹æ³•éœ€è¦å¤§é‡æ ‡æ³¨ã€ç¼ºä¹å¯è§£é‡Šæ€§ä¸”æ³›åŒ–èƒ½åŠ›å·®ã€‚ç°æœ‰æ¨ç†æ–¹æ³•ä¾èµ–æ˜‚è´µçš„ä¸“å®¶æ ‡æ³¨ï¼Œéš¾ä»¥å¤„ç†å†œä¸šæŸ¥è¯¢çš„å¼€æ”¾æ€§å’Œå¤šæ ·æ€§ï¼Œå› æ­¤éœ€è¦å¼€å‘æ›´é«˜æ•ˆçš„å†œä¸šä¸“ç”¨æ¨ç†å¢å¼ºæ¨¡å‹ã€‚</p>
<p><strong>Method:</strong> æå‡ºAgri-R1æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰è¯­è¨€åˆæˆå’ŒåŸºäºLLMçš„è¿‡æ»¤è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡æ¨ç†æ•°æ®ï¼Œä»…ä½¿ç”¨19%å¯ç”¨æ ·æœ¬ã€‚é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶æå‡ºæ–°é¢–çš„å¥–åŠ±å‡½æ•°ï¼Œæ•´åˆé¢†åŸŸç‰¹å®šè¯å…¸å’Œæ¨¡ç³ŠåŒ¹é…æ¥è¯„ä¼°å¼€æ”¾å›ç­”çš„æ­£ç¡®æ€§å’Œè¯­è¨€çµæ´»æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨CDDMBenchè¯„ä¼°ä¸­ï¼Œ3Bå‚æ•°æ¨¡å‹æ€§èƒ½ä¸7Bè‡³13Bå‚æ•°åŸºçº¿æ¨¡å‹ç›¸å½“ï¼Œåœ¨ç—…å®³è¯†åˆ«å‡†ç¡®ç‡ä¸Šå®ç°+23.2%ç›¸å¯¹æå‡ï¼Œå†œä¸šçŸ¥è¯†é—®ç­”æå‡+33.3%ï¼Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›æ¯”æ ‡å‡†å¾®è°ƒæé«˜26.10åˆ†ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ç»“æ„åŒ–æ¨ç†æ•°æ®ä¸GRPOé©±åŠ¨çš„æ¢ç´¢ååŒä½œç”¨æ”¯æ’‘äº†è¿™äº›å¢ç›Šã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜è‡ªåŠ¨åŒ–æ¨ç†æ•°æ®ç”Ÿæˆä¸å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çš„ç»“åˆèƒ½æœ‰æ•ˆæå‡å†œä¸šAIæ¨¡å‹çš„è¯Šæ–­èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é—®é¢˜å¤æ‚åº¦å¢åŠ æ—¶æ•ˆæœæ›´æ˜¾è‘—ã€‚è¯¥æ–¹æ³•ä¸ºèµ„æºå—é™çš„å†œä¸šåº”ç”¨æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†é¢†åŸŸç‰¹å®šå¥–åŠ±å‡½æ•°åœ¨å¼€æ”¾å›ç­”è¯„ä¼°ä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\% relative gain in disease recognition accuracy, +33.3\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.</p>
<h3 id="12-forge-and-quench-enhancing-image-generation-for-higher-fidelity-in-unified-multimodal-models">[12] <a href="https://arxiv.org/abs/2601.04706">Forge-and-Quench: Enhancing Image Generation for Higher Fidelity in Unified Multimodal Models</a></h3>
<p><em>Yanbing Zeng, Jia Wang, Hanghang Ma, Junqiang Wu, Jie Zhu, Xiaoming Wei, Jie Hu</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºForge-and-Quenchç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡ç†è§£æ¨¡å‹å¢å¼ºç”Ÿæˆæ¨¡å‹çš„ä¿çœŸåº¦å’Œç»†èŠ‚ä¸°å¯Œåº¦ï¼Œåˆ©ç”¨MLLMç”Ÿæˆå¢å¼ºæ–‡æœ¬æŒ‡ä»¤å¹¶æ˜ å°„ä¸ºæ¡¥æ¥ç‰¹å¾ï¼Œä½œä¸ºè§†è§‰å¼•å¯¼ä¿¡å·æ³¨å…¥T2Iä¸»å¹²æ¨¡å‹ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€é¢†åŸŸå°†å›¾åƒç”Ÿæˆä¸ç†è§£é›†æˆåˆ°å•ä¸€æ¡†æ¶æˆä¸ºå…³é”®ç›®æ ‡ï¼Œä½†ç†è§£å¦‚ä½•æœ‰æ•ˆè¾…åŠ©ç”Ÿæˆå°šæœªå……åˆ†æ¢ç´¢ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨åˆ©ç”¨ç†è§£æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œä¸–ç•ŒçŸ¥è¯†ï¼Œè€Œæœ¬æ–‡å¼•å…¥æ–°è§†è§’ï¼šåˆ©ç”¨ç†è§£æ¥å¢å¼ºç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦å’Œç»†èŠ‚ä¸°å¯Œåº¦ã€‚</p>
<p><strong>Method:</strong> æå‡ºForge-and-Quenchç»Ÿä¸€æ¡†æ¶ï¼Œå…¶ä¸­MLLMé¦–å…ˆå¯¹æ•´ä¸ªå¯¹è¯ä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†ç”Ÿæˆå¢å¼ºæ–‡æœ¬æŒ‡ä»¤ï¼Œç„¶åé€šè¿‡æ–°é¢–çš„æ¡¥æ¥é€‚é…å™¨å°†å…¶æ˜ å°„ä¸ºè™šæ‹Ÿè§†è§‰è¡¨ç¤ºâ€”â€”æ¡¥æ¥ç‰¹å¾ã€‚è¯¥ç‰¹å¾ä½œä¸ºå…³é”®é“¾æ¥ï¼Œå°†ç†è§£æ¨¡å‹çš„æ´å¯Ÿæ³¨å…¥ç”Ÿæˆè¿‡ç¨‹ï¼Œéšåä¸å¢å¼ºæ–‡æœ¬æŒ‡ä»¤ä¸€èµ·ä½œä¸ºè§†è§‰å¼•å¯¼ä¿¡å·æ³¨å…¥T2Iä¸»å¹²æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜Forge-and-Quenchæ˜¾è‘—æé«˜äº†å¤šä¸ªæ¨¡å‹çš„å›¾åƒä¿çœŸåº¦å’Œç»†èŠ‚ï¼ŒåŒæ—¶ä¿æŒäº†æŒ‡ä»¤è·Ÿéšå‡†ç¡®æ€§å¹¶å¢å¼ºäº†ä¸–ç•ŒçŸ¥è¯†åº”ç”¨ã€‚è¯¥æ¡†æ¶å±•ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§å’Œçµæ´»æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒMLLMå’ŒT2Iæ¨¡å‹é—´é«˜æ•ˆè¿ç§»ï¼Œæ˜¾è‘—èŠ‚çœè®­ç»ƒå¼€é”€ä¸”ä¸æŸå®³MLLMå›ºæœ‰çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºç†è§£è¾…åŠ©ç”Ÿæˆæä¾›äº†æ–°èŒƒå¼ï¼Œé€šè¿‡æ¡¥æ¥ç‰¹å¾æœ‰æ•ˆè¿æ¥ç†è§£ä¸ç”Ÿæˆè¿‡ç¨‹ã€‚æ¡†æ¶è®¾è®¡å®ç°äº†ç†è§£æ¨¡å‹æ´å¯Ÿå‘ç”Ÿæˆæ¨¡å‹çš„ç›´æ¥ä¼ é€’ï¼Œä¸ºå¤šæ¨¡æ€ç»Ÿä¸€æ¡†æ¶çš„å‘å±•æä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„å¯è¿ç§»æ€§å’Œè®­ç»ƒæ•ˆç‡ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Integrating image generation and understanding into a single framework has become a pivotal goal in the multimodal domain. However, how understanding can effectively assist generation has not been fully explored. Unlike previous works that focus on leveraging reasoning abilities and world knowledge from understanding models, this paper introduces a novel perspective: leveraging understanding to enhance the fidelity and detail richness of generated images. To this end, we propose Forge-and-Quench, a new unified framework that puts this principle into practice. In the generation process of our framework, an MLLM first reasons over the entire conversational context, including text instructions, to produce an enhanced text instruction. This refined instruction is then mapped to a virtual visual representation, termed the Bridge Feature, via a novel Bridge Adapter. This feature acts as a crucial link, forging insights from the understanding model to quench and refine the generation process. It is subsequently injected into the T2I backbone as a visual guidance signal, alongside the enhanced text instruction that replaces the original input. To validate this paradigm, we conduct comprehensive studies on the design of the Bridge Feature and Bridge Adapter. Our framework demonstrates exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant savings in training overhead, all without compromising the MLLM's inherent multimodal understanding capabilities. Experiments show that Forge-and-Quench significantly improves image fidelity and detail across multiple models, while also maintaining instruction-following accuracy and enhancing world knowledge application. Models and codes are available at https://github.com/YanbingZeng/Forge-and-Quench.</p>
<h3 id="13-on-the-holistic-approach-for-detecting-human-image-forgery">[13] <a href="https://arxiv.org/abs/2601.04715">On the Holistic Approach for Detecting Human Image Forgery</a></h3>
<p><em>Xiao Guo, Jie Zhu, Anil Jain, Xiaoming Liu</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºHuForDetï¼Œä¸€ç§ç”¨äºäººç±»å›¾åƒä¼ªé€ æ£€æµ‹çš„æ•´ä½“æ¡†æ¶ï¼Œé€šè¿‡åŒåˆ†æ”¯æ¶æ„ç»“åˆé¢éƒ¨ä¼ªé€ æ£€æµ‹å’Œä¸Šä¸‹æ–‡è¯­ä¹‰ä¸€è‡´æ€§åˆ†æï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨é¢éƒ¨åŒºåŸŸä¼ªé€ ä¸å…¨èº«åˆæˆå›¾åƒæ£€æµ‹ä¹‹é—´çš„ç¢ç‰‡åŒ–é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> AIç”Ÿæˆå†…å®¹çš„å¿«é€Ÿå‘å±•åŠ å‰§äº†æ·±åº¦ä¼ªé€ å¨èƒï¼Œä»é¢éƒ¨æ“çºµåˆ°å…¨èº«é€¼çœŸäººä½“åˆæˆï¼Œä½†ç°æœ‰æ£€æµ‹æ–¹æ³•å­˜åœ¨ç¢ç‰‡åŒ–é—®é¢˜ï¼Œä¸“é—¨é’ˆå¯¹é¢éƒ¨åŒºåŸŸä¼ªé€ æˆ–å…¨èº«åˆæˆå›¾åƒï¼Œæ— æ³•æ³›åŒ–åˆ°å®Œæ•´çš„äººç±»å›¾åƒæ“çºµè°±ç³»ã€‚</p>
<p><strong>Method:</strong> HuForDeté‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼šé¢éƒ¨ä¼ªé€ æ£€æµ‹åˆ†æ”¯åœ¨RGBå’Œé¢‘åŸŸä¸­é‡‡ç”¨å¼‚æ„ä¸“å®¶ï¼ŒåŒ…æ‹¬è‡ªé€‚åº”æ‹‰æ™®æ‹‰æ–¯-é«˜æ–¯æ¨¡å—ä»¥æ•è·ä»ç»†ç²’åº¦æ··åˆè¾¹ç•Œåˆ°ç²—å°ºåº¦çº¹ç†å¼‚å¸¸çš„ä¼ªå½±ï¼›ä¸Šä¸‹æ–‡ä¼ªé€ æ£€æµ‹åˆ†æ”¯åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åˆ†æå…¨èº«è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶é…å¤‡ç½®ä¿¡åº¦ä¼°è®¡æœºåˆ¶åœ¨ç‰¹å¾èåˆä¸­åŠ¨æ€åŠ æƒå…¶è´¡çŒ®ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡æ„å»ºç»Ÿä¸€çš„äººç±»å›¾åƒä¼ªé€ æ•°æ®é›†ï¼Œå°†ç°æœ‰é¢éƒ¨ä¼ªé€ æ•°æ®ä¸æ–°çš„å…¨èº«åˆæˆäººä½“è¯­æ–™åº“ç»“åˆï¼Œå®éªŒè¡¨æ˜HuForDetåœ¨å¤šæ ·åŒ–äººç±»å›¾åƒä¼ªé€ æ£€æµ‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„é²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆç»†ç²’åº¦é¢éƒ¨ä¼ªå½±æ£€æµ‹ä¸ä¸Šä¸‹æ–‡è¯­ä¹‰åˆ†æçš„æ•´ä½“æ¡†æ¶åœ¨äººç±»å›¾åƒä¼ªé€ æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåº”å¯¹æ—¥ç›Šå¤æ‚çš„AIGCä¼ªé€ å¨èƒæä¾›äº†ç»Ÿä¸€è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¼ºè°ƒäº†å¤šæ¨¡æ€åˆ†æå’Œè‡ªé€‚åº”ç‰¹å¾èåˆçš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>The rapid advancement of AI-generated content (AIGC) has escalated the threat of deepfakes, from facial manipulations to the synthesis of entire photorealistic human bodies. However, existing detection methods remain fragmented, specializing either in facial-region forgeries or full-body synthetic images, and consequently fail to generalize across the full spectrum of human image manipulations. We introduce HuForDet, a holistic framework for human image forgery detection, which features a dual-branch architecture comprising: (1) a face forgery detection branch that employs heterogeneous experts operating in both RGB and frequency domains, including an adaptive Laplacian-of-Gaussian (LoG) module designed to capture artifacts ranging from fine-grained blending boundaries to coarse-scale texture irregularities; and (2) a contextualized forgery detection branch that leverages a Multi-Modal Large Language Model (MLLM) to analyze full-body semantic consistency, enhanced with a confidence estimation mechanism that dynamically weights its contribution during feature fusion. We curate a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans. Extensive experiments show that our HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries.</p>
<h3 id="14-aivd-adaptive-edge-cloud-collaboration-for-accurate-and-efficient-industrial-visual-detection">[14] <a href="https://arxiv.org/abs/2601.04734">AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection</a></h3>
<p><em>Yunqing Hu, Zheming Yang, Chang Zhao, Qi Guo, Meng Gao, Pengcheng Li, Wen Ji</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†AIVDæ¡†æ¶ï¼Œé€šè¿‡è½»é‡çº§è¾¹ç¼˜æ£€æµ‹å™¨ä¸äº‘ç«¯MLLMçš„ååŒå·¥ä½œï¼Œå®ç°äº†ç²¾ç¡®ç›®æ ‡å®šä½ä¸é«˜è´¨é‡è¯­ä¹‰ç”Ÿæˆçš„ç»Ÿä¸€ï¼ŒåŒæ—¶è®¾è®¡äº†å¼‚æ„èµ„æºæ„ŸçŸ¥çš„åŠ¨æ€è°ƒåº¦ç®—æ³•ä»¥ä¼˜åŒ–è¾¹ç¼˜-äº‘ç«¯éƒ¨ç½²æ•ˆç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œè§†è§‰æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç²¾ç¡®ç›®æ ‡å®šä½å’Œèµ„æºå—é™çš„è¾¹ç¼˜-äº‘ç«¯éƒ¨ç½²åœºæ™¯ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦è§£å†³è¾¹ç¼˜è£å‰ªæ¡†å™ªå£°ã€åœºæ™¯å˜åŒ–ä»¥åŠå¼‚æ„è®¾å¤‡åŠ¨æ€ç½‘ç»œæ¡ä»¶ä¸‹çš„æ€§èƒ½ä¼˜åŒ–é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†AIVDæ¡†æ¶ï¼Œé€šè¿‡è½»é‡çº§è¾¹ç¼˜æ£€æµ‹å™¨ä¸äº‘ç«¯MLLMçš„åä½œå®ç°ç»Ÿä¸€ç²¾ç¡®å®šä½å’Œé«˜è´¨é‡è¯­ä¹‰ç”Ÿæˆï¼›è®¾è®¡äº†è§†è§‰-è¯­ä¹‰ååŒå¢å¼ºçš„é«˜æ•ˆå¾®è°ƒç­–ç•¥ä»¥æå‡äº‘ç«¯MLLMå¯¹è¾¹ç¼˜è£å‰ªæ¡†å™ªå£°å’Œåœºæ™¯å˜åŒ–çš„é²æ£’æ€§ï¼›å¼€å‘äº†å¼‚æ„èµ„æºæ„ŸçŸ¥çš„åŠ¨æ€è°ƒåº¦ç®—æ³•ä»¥ç»´æŒé«˜ååé‡å’Œä½å»¶è¿Ÿã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒAIVDæ¡†æ¶åœ¨æ˜¾è‘—é™ä½èµ„æºæ¶ˆè€—çš„åŒæ—¶ï¼Œæé«˜äº†MLLMçš„åˆ†ç±»æ€§èƒ½å’Œè¯­ä¹‰ç”Ÿæˆè´¨é‡ï¼›æ‰€æå‡ºçš„è°ƒåº¦ç­–ç•¥åœ¨å¤šæ ·åŒ–åœºæ™¯ä¸­å®ç°äº†æ›´é«˜çš„ååé‡å’Œæ›´ä½çš„å»¶è¿Ÿï¼Œæœ‰æ•ˆæå‡äº†ç³»ç»Ÿæ•´ä½“æ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡è¾¹ç¼˜-äº‘ç«¯ååŒæ¶æ„å’Œé’ˆå¯¹æ€§ä¼˜åŒ–ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³MLLMåœ¨ç²¾ç¡®å®šä½å’Œè¾¹ç¼˜éƒ¨ç½²ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„å¤šæ¨¡æ€AIç³»ç»Ÿéƒ¨ç½²æä¾›äº†å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆå’Œæ€§èƒ½ä¼˜åŒ–æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) demonstrate exceptional capabilities in semantic understanding and visual reasoning, yet they still face challenges in precise object localization and resource-constrained edge-cloud deployment. To address this, this paper proposes the AIVD framework, which achieves unified precise localization and high-quality semantic generation through the collaboration between lightweight edge detectors and cloud-based MLLMs. To enhance the cloud MLLM's robustness against edge cropped-box noise and scenario variations, we design an efficient fine-tuning strategy with visual-semantic collaborative augmentation, significantly improving classification accuracy and semantic consistency. Furthermore, to maintain high throughput and low latency across heterogeneous edge devices and dynamic network conditions, we propose a heterogeneous resource-aware dynamic scheduling algorithm. Experimental results demonstrate that AIVD substantially reduces resource consumption while improving MLLM classification performance and semantic generation quality. The proposed scheduling strategy also achieves higher throughput and lower latency across diverse scenarios.</p>
<h3 id="15-gem-vg-towards-generalized-multi-image-visual-grounding-with-multimodal-large-language-models">[15] <a href="https://arxiv.org/abs/2601.04777">GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models</a></h3>
<p><em>Shurong Zheng, Yousong Zhu, Hongyin Zhao, Fan Yang, Yufei Zhan, Ming Tang, Jinqiao Wang</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGeM-VGï¼Œä¸€ç§èƒ½å¤Ÿè¿›è¡Œå¹¿ä¹‰å¤šå›¾åƒè§†è§‰å®šä½çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç³»ç»ŸåŒ–ä»»åŠ¡åˆ†ç±»ã€æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†MG-Data-240Kä»¥åŠè®¾è®¡æ··åˆå¼ºåŒ–å¾®è°ƒç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šæ ·åŒ–å¤šå›¾åƒå®šä½ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„å¤šå›¾åƒè§†è§‰å®šä½æ–¹æ³•å—é™äºå•ç›®æ ‡å®šä½å’Œæœ‰é™çš„ä»»åŠ¡ç±»å‹ï¼Œç¼ºä¹å¯¹å¹¿ä¹‰å®šä½ä»»åŠ¡çš„ç»Ÿä¸€å»ºæ¨¡ï¼Œè¿™é™åˆ¶äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šå›¾åƒåœºæ™¯ä¸‹çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é¦–å…ˆæ ¹æ®è·¨å›¾åƒçº¿ç´¢å’Œæ¨ç†éœ€æ±‚å¯¹å¤šå›¾åƒå®šä½ä»»åŠ¡è¿›è¡Œç³»ç»Ÿåˆ†ç±»ï¼Œæ„å»ºäº†åŒ…å«240Kæ ·æœ¬çš„MG-Data-240Kæ•°æ®é›†ä»¥è§£å†³ç°æœ‰æ•°æ®é›†ä¸­ç›®æ ‡æ•°é‡å’Œå›¾åƒå…³ç³»æ–¹é¢çš„é™åˆ¶ã€‚ä¸ºè§£å†³å¤šæ ·åŒ–å¤šå›¾åƒå®šä½ä»»åŠ¡çš„é²æ£’å¤„ç†æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ€ç»´é“¾æ¨ç†å’Œç›´æ¥å›ç­”çš„æ··åˆå¼ºåŒ–å¾®è°ƒç­–ç•¥ï¼Œé‡‡ç”¨åŸºäºè§„åˆ™å¥–åŠ±å¼•å¯¼çš„R1-likeç®—æ³•ï¼Œæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹çš„æ•´ä½“æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼ŒGeM-VGåœ¨å¤šå›¾åƒå®šä½ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œåœ¨MIG-Benchå’ŒMC-BenchåŸºå‡†ä¸Šåˆ†åˆ«æ¯”å…ˆå‰é¢†å…ˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æå‡äº†2.0%å’Œ9.7%ã€‚åœ¨å•å›¾åƒå®šä½ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹åœ¨ODINWåŸºå‡†ä¸Šå®ç°äº†9.1%çš„æ”¹è¿›ã€‚åŒæ—¶ï¼Œæ¨¡å‹åœ¨é€šç”¨å¤šå›¾åƒç†è§£ä»»åŠ¡ä¸­ä¿æŒäº†å¼ºå¤§çš„èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ç»Ÿä¸€å»ºæ¨¡æ¡†æ¶ã€å¤§è§„æ¨¡æ•°æ®é›†å’Œæ··åˆå¾®è°ƒç­–ç•¥ï¼ŒæˆåŠŸè§£å†³äº†å¤šå›¾åƒè§†è§‰å®šä½ä»»åŠ¡çš„æ³›åŒ–æŒ‘æˆ˜ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šå›¾åƒåœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†åœ¨ä¿æŒé€šç”¨ç†è§£èƒ½åŠ›çš„åŒæ—¶æå‡ç‰¹å®šå®šä½æ€§èƒ½çš„å¯èƒ½æ€§ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.</p>
<h3 id="16-countervid-counterfactual-video-generation-for-mitigating-action-and-temporal-hallucinations-in-video-language-models">[16] <a href="https://arxiv.org/abs/2601.04778">CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models</a></h3>
<p><em>Tobia Poppi, Burak Uzkent, Amanmeet Garg, Lucas Porto, Garin Kessler, Yezhou Yang, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara, Florian Schiffers</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„åäº‹å®è§†é¢‘ç”Ÿæˆæ¡†æ¶CounterVidï¼Œç”¨äºç¼“è§£è§†é¢‘è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åŠ¨ä½œå’Œæ—¶åºæ¨ç†ï¼Œå¹¶å¼•å…¥MixDPOæ–¹æ³•è”åˆåˆ©ç”¨æ–‡æœ¬å’Œè§†è§‰åå¥½è¿›è¡Œä¼˜åŒ–ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŠ¨ä½œå’Œæ—¶åºæ¨ç†æ–¹é¢ä»ç„¶å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œç°æœ‰ç¼“è§£ç­–ç•¥å¦‚æ–‡æœ¬è¿‡æ»¤æˆ–éšæœºè§†é¢‘æ‰°åŠ¨å¾€å¾€æœªèƒ½è§£å†³æ ¹æœ¬åŸå› ï¼šè¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒè€Œéç»†ç²’åº¦è§†è§‰åŠ¨æ€ã€‚</p>
<p><strong>Method:</strong> æå‡ºå¯æ‰©å±•çš„åäº‹å®è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆå¤šæ¨¡æ€LLMè¿›è¡ŒåŠ¨ä½œæè®®å’Œç¼–è¾‘æŒ‡å¯¼ï¼Œåˆ©ç”¨åŸºäºæ‰©æ•£çš„å›¾åƒå’Œè§†é¢‘æ¨¡å‹ç”Ÿæˆå¤§è§„æ¨¡è¯­ä¹‰ç¡¬è´Ÿæ ·æœ¬ï¼›æ„å»ºCounterVidåˆæˆæ•°æ®é›†åŒ…å«çº¦26kåå¥½å¯¹ï¼Œå¹¶å¼•å…¥MixDPOç»Ÿä¸€ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•è”åˆåˆ©ç”¨æ–‡æœ¬å’Œè§†è§‰åå¥½ã€‚</p>
<p><strong>Result:</strong> ä½¿ç”¨MixDPOå¾®è°ƒQwen2.5-VLæ¨¡å‹å¸¦æ¥ä¸€è‡´æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—¶åºæ’åºæ–¹é¢è¡¨ç°æ˜¾è‘—æå‡ï¼Œå¹¶æœ‰æ•ˆè¿ç§»åˆ°æ ‡å‡†è§†é¢‘å¹»è§‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡åäº‹å®è§†é¢‘ç”Ÿæˆå’Œæ··åˆåå¥½ä¼˜åŒ–ï¼Œä¸ºè§£å†³è§†é¢‘è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨ä½œå’Œæ—¶åºæ¨ç†æ–¹é¢ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„å¯ä¿¡åº¦æå‡æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.</p>
<h3 id="17-measurement-consistent-langevin-corrector-a-remedy-for-latent-diffusion-inverse-solvers">[17] <a href="https://arxiv.org/abs/2601.04791">Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers</a></h3>
<p><em>Lee Hyoseok, Sohwi Lim, Eunju Cha, Tae-Hyun Oh</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†æµ‹é‡ä¸€è‡´æœ—ä¹‹ä¸‡æ ¡æ­£å™¨ï¼ˆMCLCï¼‰ï¼Œä¸€ç§ç†è®ºåŸºç¡€çš„å³æ’å³ç”¨æ ¡æ­£æ¨¡å—ï¼Œç”¨äºç¨³å®šåŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬é€†é—®é¢˜æ±‚è§£å™¨ï¼Œé€šè¿‡å‡å°‘æ±‚è§£å™¨ä¸çœŸå®åå‘æ‰©æ•£åŠ¨æ€ä¹‹é—´çš„å·®å¼‚æ¥è§£å†³ç°æœ‰æ–¹æ³•çš„ä¸ç¨³å®šæ€§é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬é€†é—®é¢˜æ±‚è§£å™¨å­˜åœ¨ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œè¡¨ç°ä¸ºä¸å¸Œæœ›çš„ä¼ªå½±å’Œé€€åŒ–è´¨é‡ã€‚ç ”ç©¶å‘ç°è¿™ç§ä¸ç¨³å®šæ€§æºäºæ±‚è§£å™¨ä¸çœŸå®åå‘æ‰©æ•£åŠ¨æ€ä¹‹é—´çš„å·®å¼‚ï¼Œéœ€è¦ä¸€ç§æ— éœ€çº¿æ€§æµå½¢å‡è®¾çš„æ›´ç¨³å®šæ ¡æ­£æ–¹æ³•æ¥è§£å†³è¿™ä¸€æ ¹æœ¬é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†æµ‹é‡ä¸€è‡´æœ—ä¹‹ä¸‡æ ¡æ­£å™¨ï¼ˆMCLCï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç†è®ºåŸºç¡€çš„å³æ’å³ç”¨æ ¡æ­£æ¨¡å—ï¼Œé€šè¿‡æµ‹é‡ä¸€è‡´çš„æœ—ä¹‹ä¸‡æ›´æ–°æ¥ä¿®æ­£åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é€†é—®é¢˜æ±‚è§£å™¨ã€‚ä¸ä¾èµ–çº¿æ€§æµå½¢å‡è®¾çš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒMCLCæ— éœ€æ­¤å‡è®¾ï¼Œèƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸­å®ç°æ›´ç¨³å®šå¯é çš„è¡Œä¸ºï¼Œç›´æ¥é’ˆå¯¹æ±‚è§£å™¨ä¸çœŸå®åå‘æ‰©æ•£åŠ¨æ€ä¹‹é—´çš„å·®å¼‚è¿›è¡Œæ ¡æ­£ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¯æ˜MCLCåœ¨å¤šç§å›¾åƒæ¢å¤ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—æ•ˆæœï¼Œå¹¶ä¸”ä¸ç°æœ‰æ±‚è§£å™¨å…¼å®¹ã€‚ç ”ç©¶è¿˜åˆ†æäº†æ–‘ç‚¹ä¼ªå½±ç°è±¡ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†å…¶æ ¹æœ¬åŸå› ï¼ŒéªŒè¯äº†MCLCåœ¨æé«˜æ±‚è§£å™¨ç¨³å®šæ€§å’Œå‡å°‘ä¼ªå½±æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> MCLCä»£è¡¨äº†å‘æ›´ç¨³å¥çš„é›¶æ ·æœ¬é€†é—®é¢˜æ±‚è§£å™¨è¿ˆå‡ºçš„å…³é”®ä¸€æ­¥ï¼Œå…¶æ— éœ€çº¿æ€§æµå½¢å‡è®¾çš„æ–¹æ³•ä¸ºè§£å†³æ½œåœ¨æ‰©æ•£æ¨¡å‹é€†é—®é¢˜æ±‚è§£ä¸­çš„ä¸ç¨³å®šæ€§é—®é¢˜æä¾›äº†ç†è®ºåŸºç¡€å’Œå®è·µæ–¹æ¡ˆã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡ºäº†æœ‰æ•ˆçš„æ ¡æ­£æœºåˆ¶ï¼Œè¿˜ä¸ºç†è§£ä¼ªå½±ç°è±¡æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>With recent advances in generative models, diffusion models have emerged as powerful priors for solving inverse problems in each domain. Since Latent Diffusion Models (LDMs) provide generic priors, several studies have explored their potential as domain-agnostic zero-shot inverse solvers. Despite these efforts, existing latent diffusion inverse solvers suffer from their instability, exhibiting undesirable artifacts and degraded quality. In this work, we first identify the instability as a discrepancy between the solver's and true reverse diffusion dynamics, and show that reducing this gap stabilizes the solver. Building on this, we introduce Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies the LDM-based inverse solvers through measurement-consistent Langevin updates. Compared to prior approaches that rely on linear manifold assumptions, which often do not hold in latent space, MCLC operates without this assumption, leading to more stable and reliable behavior. We experimentally demonstrate the effectiveness of MCLC and its compatibility with existing solvers across diverse image restoration tasks. Additionally, we analyze blob artifacts and offer insights into their underlying causes. We highlight that MCLC is a key step toward more robust zero-shot inverse problem solvers.</p>
<h3 id="18-detector-augmented-samurai-for-long-duration-drone-tracking">[18] <a href="https://arxiv.org/abs/2601.04798">Detector-Augmented SAMURAI for Long-Duration Drone Tracking</a></h3>
<p><em>Tamara R. Lenhard, Andreas Weinmann, Hichem Snoussi, Tobias Koch</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°äº†SAMURAIåŸºç¡€æ¨¡å‹åœ¨æ— äººæœºè·Ÿè¸ªä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ£€æµ‹å™¨å¢å¼ºçš„æ‰©å±•æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚åŸå¸‚ç¯å¢ƒä¸­é•¿æœŸæ— äººæœºè·Ÿè¸ªçš„é²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºæ£€æµ‹å™¨çš„æ— äººæœºè·Ÿè¸ªæ–¹æ³•è™½ç„¶å¸§çº§ç²¾åº¦è¾ƒé«˜ï¼Œä½†å­˜åœ¨æ—¶é—´ä¸ä¸€è‡´æ€§å’Œé¢‘ç¹æ£€æµ‹ä¸¢å¤±çš„é—®é¢˜ï¼Œè€ŒRGBæ— äººæœºè·Ÿè¸ªç ”ç©¶ä»æœ‰é™ä¸”ä¾èµ–ä¼ ç»Ÿè¿åŠ¨æ¨¡å‹ã€‚å°½ç®¡SAMURAIç­‰åŸºç¡€æ¨¡å‹åœ¨å…¶ä»–é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„ç±»åˆ«æ— å…³è·Ÿè¸ªæ€§èƒ½ï¼Œä½†å…¶åœ¨æ— äººæœºç‰¹å®šåœºæ™¯ä¸­çš„é€‚ç”¨æ€§å°šæœªå¾—åˆ°ç ”ç©¶ï¼Œè¿™ä¸€ç ”ç©¶ç©ºç™½ä¿ƒä½¿æœ¬ç ”ç©¶è¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶é¦–å…ˆå¯¹SAMURAIåŸºç¡€æ¨¡å‹åœ¨æ— äººæœºè·Ÿè¸ªä»»åŠ¡ä¸­çš„æ½œåŠ›è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œéšåæå‡ºäº†ä¸€ç§æ£€æµ‹å™¨å¢å¼ºçš„SAMURAIæ‰©å±•æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆæ£€æµ‹å™¨çº¿ç´¢æ¥å‡è½»å¯¹è¾¹ç•Œæ¡†åˆå§‹åŒ–å’Œåºåˆ—é•¿åº¦çš„æ•æ„Ÿæ€§ï¼Œä»è€Œæå‡è·Ÿè¸ªé²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ£€æµ‹å™¨å¢å¼ºæ‰©å±•æ–¹æ³•åœ¨å¤æ‚åŸå¸‚ç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†è·Ÿè¸ªé²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æ—¶åºåˆ—å’Œæ— äººæœºé€€å‡º-é‡å…¥äº‹ä»¶ä¸­è¡¨ç°çªå‡ºã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šç›¸æ¯”SAMURAIçš„é›¶æ ·æœ¬æ€§èƒ½å‡è·å¾—ä¸€è‡´æå‡ï¼ŒæˆåŠŸç‡æœ€é«˜æå‡+0.393ï¼Œè¯¯æŠ¥ç‡æœ€é«˜é™ä½-0.475ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶è¯å®äº†åŸºç¡€æ¨¡å‹åœ¨æ— äººæœºè·Ÿè¸ªé¢†åŸŸçš„åº”ç”¨æ½œåŠ›ï¼Œæ£€æµ‹å™¨å¢å¼ºç­–ç•¥æœ‰æ•ˆè§£å†³äº†è¾¹ç•Œæ¡†åˆå§‹åŒ–å’Œåºåˆ—é•¿åº¦æ•æ„Ÿæ€§é—®é¢˜ï¼Œä¸ºå¤æ‚åŸå¸‚ç¯å¢ƒä¸‹çš„é•¿æœŸæ— äººæœºç›‘æ§æä¾›äº†æ›´å¯é çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºæœªæ¥æ— äººæœºè·Ÿè¸ªç ”ç©¶æä¾›äº†æ–°çš„æŠ€æœ¯æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.</p>
<h3 id="19-sovabench-a-vehicle-surveillance-action-retrieval-benchmark-for-multimodal-large-language-models">[19] <a href="https://arxiv.org/abs/2601.04824">SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models</a></h3>
<p><em>Oriol Rabasseda, Zenjie Li, Kamal Nasrollahi, Sergio Escalera</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SOVABenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ç›‘æ§è§†é¢‘ä¸­è½¦è¾†åŠ¨ä½œçš„è¯†åˆ«èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å…è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆå¯è§£é‡Šçš„æè¿°åµŒå…¥æ¥æå‡åŠ¨ä½œåŒºåˆ†æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºå†…å®¹çš„è§†é¢‘æ£€ç´¢åŸºå‡†ä¸»è¦å…³æ³¨åœºæ™¯çº§ç›¸ä¼¼æ€§ï¼Œç¼ºä¹å¯¹ç›‘æ§åœºæ™¯ä¸­åŠ¨ä½œåŒºåˆ†èƒ½åŠ›çš„è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯è½¦è¾†ç›¸å…³åŠ¨ä½œçš„è¯†åˆ«å­˜åœ¨ç ”ç©¶ç©ºç™½ï¼Œéœ€è¦ä¸“é—¨çš„è¯„ä¼°åè®®æ¥è¡¡é‡è·¨åŠ¨ä½œåŒºåˆ†å’Œæ—¶é—´æ–¹å‘ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†SOVABenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸¤ç§è¯„ä¼°åè®®ï¼ˆinter-pairå’Œintra-pairï¼‰ï¼Œå¹¶å¼€å‘äº†åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å…è®­ç»ƒæ¡†æ¶ï¼Œåˆ©ç”¨MLLMçš„è§†è§‰æ¨ç†å’ŒæŒ‡ä»¤è·Ÿéšèƒ½åŠ›ç”Ÿæˆå›¾åƒå’Œè§†é¢‘çš„å¯è§£é‡Šæè¿°åµŒå…¥ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œç°æœ‰æœ€å…ˆè¿›çš„è§†è§‰å’Œå¤šæ¨¡æ€æ¨¡å‹åœ¨åŠ¨ä½œåŒºåˆ†ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œæå‡ºçš„å…è®­ç»ƒæ¡†æ¶åœ¨SOVABenchåŸºå‡†ä¸Šå–å¾—äº†å¼ºåŠ²æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹ç»å¸¸å¤±è´¥çš„ç©ºé—´å’Œè®¡æ•°åŸºå‡†ä¸Šä¹Ÿè¡¨ç°è‰¯å¥½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†ç›‘æ§è§†é¢‘ä¸­åŠ¨ä½œåŒºåˆ†å¯¹ç°æœ‰æ¨¡å‹çš„æŒ‘æˆ˜æ€§ï¼Œè¯æ˜äº†MLLMç”Ÿæˆæè¿°åµŒå…¥çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç›‘æ§è§†é¢‘åˆ†ææä¾›äº†æ–°çš„è¯„ä¼°åŸºå‡†å’Œæ–¹æ³•æ¡†æ¶ï¼Œæ¨åŠ¨äº†å¯è§£é‡Šè§†é¢‘ç†è§£æŠ€æœ¯çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.
  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.</p>
<h3 id="20-divas-interactive-3d-segmentation-of-nerfs-via-depth-weighted-voxel-aggregation">[20] <a href="https://arxiv.org/abs/2601.04860">DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation</a></h3>
<p><em>Ayush Pande</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DivASï¼Œä¸€ç§æ— éœ€ä¼˜åŒ–çš„å®Œå…¨äº¤äº’å¼æ¡†æ¶ï¼Œç”¨äºåˆ†å‰²ç¥ç»è¾å°„åœºï¼Œé€šè¿‡æ·±åº¦å¼•å¯¼çš„2Dæ©ç èšåˆå®ç°å®æ—¶3Dåˆ†å‰²ï¼Œé¿å…äº†ä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•æ‰€éœ€çš„é€åœºæ™¯è®­ç»ƒã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºä¼˜åŒ–çš„NeRFåˆ†å‰²æ–¹æ³•éœ€è¦ç¼“æ…¢çš„é€åœºæ™¯è®­ç»ƒï¼Œç‰ºç‰²äº†2DåŸºç¡€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†äº¤äº’å¼åˆ†å‰²çš„æ•ˆç‡å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Method:</strong> DivASé‡‡ç”¨åŸºäºå¿«é€ŸGUIçš„å·¥ä½œæµç¨‹ï¼Œåˆ©ç”¨ç”¨æˆ·ç‚¹æç¤ºç”Ÿæˆ2D SAMæ©ç ï¼Œå¹¶é€šè¿‡NeRFæ·±åº¦å…ˆéªŒè¿›è¡Œç²¾ç‚¼ä»¥æé«˜å‡ ä½•ç²¾åº¦å’Œå‰æ™¯-èƒŒæ™¯åˆ†ç¦»ï¼›æ ¸å¿ƒè´¡çŒ®æ˜¯è‡ªå®šä¹‰CUDAå†…æ ¸ï¼Œå¯åœ¨200æ¯«ç§’å†…å°†ç²¾ç‚¼çš„å¤šè§†è§’æ©ç èšåˆåˆ°ç»Ÿä¸€çš„3Dä½“ç´ ç½‘æ ¼ä¸­ã€‚</p>
<p><strong>Result:</strong> åœ¨Mip-NeRF 360Â°å’ŒLLFFæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDivASå®ç°äº†ä¸ä¼˜åŒ–æ–¹æ³•ç›¸å½“çš„åˆ†å‰²è´¨é‡ï¼Œç«¯åˆ°ç«¯é€Ÿåº¦æé«˜2-2.5å€ï¼Œæ’é™¤ç”¨æˆ·æç¤ºæ—¶é—´åé€Ÿåº¦æå‡å¯è¾¾ä¸€ä¸ªæ•°é‡çº§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ— éœ€ä¼˜åŒ–çš„å®æ—¶NeRFåˆ†å‰²çš„å¯è¡Œæ€§ï¼Œé€šè¿‡æ·±åº¦äº¤äº’çš„ä½“ç´ èšåˆæ–¹æ³•æœ‰æ•ˆç»“åˆäº†2DåŸºç¡€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›å’Œ3Då‡ ä½•å…ˆéªŒï¼Œä¸ºäº¤äº’å¼3Dåœºæ™¯ç†è§£å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360Â° and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.</p>
<h3 id="21-prototypicality-bias-reveals-blindspots-in-multimodal-evaluation-metrics">[21] <a href="https://arxiv.org/abs/2601.04946">Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics</a></h3>
<p><em>Subhadeep Roy, Gagan Bhatia, Steffen Eger</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€è¯„ä¼°ä¸­å­˜åœ¨çš„åŸå‹æ€§åå·®é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ProtoBiasæ¥é‡åŒ–è¿™ä¸€åå·®ï¼ŒåŒæ—¶å¼€å‘äº†ProtoScoreæŒ‡æ ‡æ¥æ˜¾è‘—å‡å°‘è¯„ä¼°å¤±è´¥ç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡é€šå¸¸æ›¿ä»£äººç±»åˆ¤æ–­è¿›è¡ŒåŸºå‡†æµ‹è¯•å’Œå¤§è§„æ¨¡ç­›é€‰ï¼Œä½†è¿™äº›æŒ‡æ ‡æ˜¯å¦çœŸæ­£ä¼˜å…ˆè€ƒè™‘è¯­ä¹‰æ­£ç¡®æ€§ï¼Œè¿˜æ˜¯å€¾å‘äºä»æœ‰åæ•°æ®åˆ†å¸ƒä¸­å­¦ä¹ åˆ°çš„è§†è§‰å’Œç¤¾ä¼šåŸå‹å›¾åƒå°šä¸æ˜ç¡®ã€‚ç ”ç©¶æ—¨åœ¨è¯†åˆ«å’Œç ”ç©¶å¤šæ¨¡æ€è¯„ä¼°ä¸­çš„åŸå‹æ€§åå·®è¿™ä¸€ç³»ç»Ÿæ€§å¤±æ•ˆæ¨¡å¼ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†å—æ§å¯¹æ¯”åŸºå‡†ProtoBiasï¼Œæ¶µç›–åŠ¨ç‰©ã€ç‰©ä½“å’Œäººå£ç»Ÿè®¡å›¾åƒï¼Œå…¶ä¸­è¯­ä¹‰æ­£ç¡®ä½†éåŸå‹çš„å›¾åƒä¸è¯­ä¹‰é”™è¯¯ä½†åŸå‹çš„å¯¹æŠ—å¯¹åº”å›¾åƒé…å¯¹ã€‚è¿™ç§è®¾ç½®èƒ½å¤Ÿå®šå‘è¯„ä¼°æŒ‡æ ‡æ˜¯éµå¾ªæ–‡æœ¬è¯­ä¹‰è¿˜æ˜¯é»˜è®¤åŸå‹ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶æå‡ºäº†ProtoScoreï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰70äº¿å‚æ•°çš„é²æ£’æ€§æŒ‡æ ‡ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¹¿æ³›ä½¿ç”¨çš„æŒ‡æ ‡åŒ…æ‹¬CLIPScoreã€PickScoreå’ŒåŸºäºVQAçš„åˆ†æ•°ç»å¸¸é”™è¯¯æ’åºè¿™äº›é…å¯¹ï¼Œè€Œå³ä½¿æ˜¯LLM-as-Judgeç³»ç»Ÿåœ¨ç¤¾ä¼šåŸºç¡€æ¡ˆä¾‹ä¸­ä¹Ÿè¡¨ç°å‡ºä¸å‡åŒ€çš„é²æ£’æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»è¯„ä¼°å§‹ç»ˆæ›´å€¾å‘äºè¯­ä¹‰æ­£ç¡®æ€§ä¸”å…·æœ‰æ›´å¤§çš„å†³ç­–è¾¹ç•Œã€‚ProtoScoreæŒ‡æ ‡æ˜¾è‘—é™ä½äº†å¤±è´¥ç‡å¹¶æŠ‘åˆ¶äº†é”™è¯¯æ’åºï¼ŒåŒæ—¶è¿è¡Œé€Ÿåº¦æ¯”GPT-5çš„æ¨ç†æ—¶é—´å¿«å‡ ä¸ªæ•°é‡çº§ï¼Œæ¥è¿‘æ›´å¤§è§„æ¨¡é—­æºè¯„ä¼°å™¨çš„é²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€è¯„ä¼°ä¸­å­˜åœ¨çš„ç³»ç»Ÿæ€§åŸå‹æ€§åå·®é—®é¢˜ï¼Œè¡¨æ˜å½“å‰è‡ªåŠ¨æŒ‡æ ‡å¯èƒ½è¿‡åº¦ä¾èµ–æœ‰åæ•°æ®åˆ†å¸ƒä¸­çš„åŸå‹æ¨¡å¼ã€‚æå‡ºçš„ProtoBiasåŸºå‡†ä¸ºè¯„ä¼°æŒ‡æ ‡åå·®æä¾›äº†æ ‡å‡†åŒ–æµ‹è¯•æ¡†æ¶ï¼Œè€ŒProtoScoreæŒ‡æ ‡å±•ç¤ºäº†é€šè¿‡ä¸“é—¨è®¾è®¡å¯ä»¥æ˜¾è‘—æé«˜è¯„ä¼°é²æ£’æ€§ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€è¯„ä¼°ç³»ç»Ÿçš„å¼€å‘æä¾›äº†é‡è¦æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \textsc{\textbf{ProtoBias}} (\textit{\textbf{Proto}typical \textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \textbf{\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.</p>
<h3 id="22-scaling-vision-language-models-for-pharmaceutical-long-form-video-reasoning-on-industrial-genai-platform">[22] <a href="https://arxiv.org/abs/2601.04891">Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform</a></h3>
<p><em>Suyash Mishra, Qiang Li, Srikanth Patil, Satyanarayan Pati, Baddu Narendra</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé¢å‘å·¥ä¸šåœºæ™¯çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ï¼Œé’ˆå¯¹åˆ¶è¯é¢†åŸŸçš„é•¿è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œåœ¨ä¸¥æ ¼çš„è®¡ç®—èµ„æºçº¦æŸä¸‹è¯„ä¼°äº†40å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­çš„å±€é™æ€§ã€æ•ˆç‡æƒè¡¡å’Œå¤±è´¥æ¨¡å¼ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¤§å¤šæ•°è¯„ä¼°é›†ä¸­äºçŸ­è§†é¢‘ä¸”å‡è®¾è®¡ç®—èµ„æºä¸å—é™åˆ¶ï¼Œè€Œå·¥ä¸šåœºæ™¯å¦‚åˆ¶è¯å†…å®¹ç†è§£éœ€è¦å¤„ç†é•¿è§†é¢‘å¹¶é¢ä¸´ä¸¥æ ¼çš„GPUã€å»¶è¿Ÿå’Œæˆæœ¬çº¦æŸï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ‰©å±•ï¼Œå› æ­¤éœ€è¦ç ”ç©¶å®é™…éƒ¨ç½²æ¡ä»¶ä¸‹çš„æ¨¡å‹æ€§èƒ½æé™ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå·¥ä¸šçº§ç”Ÿæˆå¼AIæ¡†æ¶ï¼Œå¤„ç†äº†è¶…è¿‡20ä¸‡ä»½PDFã€25,326ä¸ªæ¶µç›–å…«ç§æ ¼å¼çš„è§†é¢‘ä»¥åŠ888ä¸ªå¤šè¯­è¨€éŸ³é¢‘æ–‡ä»¶ï¼Œæ„å»ºäº†åˆ¶è¯é¢†åŸŸçš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨ç†æ¶æ„ï¼Œå¹¶åœ¨Video-MMEå’ŒMMBenchä¸¤ä¸ªé¢†å…ˆåŸºå‡†ä»¥åŠåŒ…å«14ç§ç–¾ç—…é¢†åŸŸçš„ä¸“æœ‰æ•°æ®é›†ä¸Šå¯¹40å¤šä¸ªVLMsè¿›è¡Œäº†å®è¯åˆ†æã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å•†ç”¨GPUä¸Šä½¿ç”¨SDPAæ³¨æ„åŠ›æœºåˆ¶å¯è·å¾—3-8å€çš„æ•ˆç‡æå‡ï¼Œå¤šæ¨¡æ€æ€§åœ¨8/12ä¸ªä»»åŠ¡é¢†åŸŸï¼ˆç‰¹åˆ«æ˜¯é•¿åº¦ç›¸å…³ä»»åŠ¡ï¼‰ä¸­å¸¦æ¥æ”¹è¿›ï¼ŒåŒæ—¶åœ¨å¼€æºå’Œé—­æºVLMsä¸­éƒ½å‘ç°äº†æ—¶é—´å¯¹é½å’Œå…³é”®å¸§æ£€æµ‹çš„æ˜æ˜¾ç“¶é¢ˆï¼Œæ­ç¤ºäº†é•¿è§†é¢‘æ¨ç†ä¸­æ³¨æ„åŠ›æœºåˆ¶æƒè¡¡ã€æ—¶é—´æ¨ç†é™åˆ¶å’Œè§†é¢‘åˆ†å‰²æŒ‘æˆ˜ç­‰å…³é”®å‘ç°ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶æ²¡æœ‰æå‡ºæ–°çš„"A+B"æ¨¡å‹ï¼Œè€Œæ˜¯ç³»ç»Ÿåˆ»ç”»äº†å½“å‰VLMsåœ¨å®é™…éƒ¨ç½²çº¦æŸä¸‹çš„æ€§èƒ½æé™ã€æƒè¡¡å–èˆå’Œå¤±è´¥æ¨¡å¼ï¼Œä¸ºç ”ç©¶è€…å’Œå®è·µè€…è®¾è®¡å¯æ‰©å±•çš„å·¥ä¸šçº§é•¿è§†é¢‘ç†è§£ç³»ç»Ÿæä¾›äº†å¯æ“ä½œçš„æŒ‡å¯¼ï¼Œç‰¹åˆ«å¼ºè°ƒäº†å¤šæ¨¡æ€æ€§ã€æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–å’Œæ—¶é—´å¯¹é½å¤„ç†åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new "A+B" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.</p>
<h3 id="23-verse-visual-embedding-reduction-and-space-exploration-clustering-guided-insights-for-training-data-enhancement-in-visually-rich-document-understanding">[23] <a href="https://arxiv.org/abs/2601.05125">VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding</a></h3>
<p><em>Ignacio de Rodrigo, Alvaro J. Lopez-Lopez, Jaime Boal</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VERSEæ–¹æ³•ï¼Œç”¨äºåˆ†æå’Œæ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡æ¢ç´¢è§†è§‰åµŒå…¥ç©ºé—´æ¥è¯†åˆ«é—®é¢˜åŒºåŸŸå¹¶ç”Ÿæˆåˆæˆæ•°æ®ä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£ä»»åŠ¡ä¸­å­˜åœ¨æ€§èƒ½ç“¶é¢ˆï¼Œç¼ºä¹å¯¹æ¨¡å‹è§†è§‰åµŒå…¥ç©ºé—´çš„ç³»ç»Ÿåˆ†ææ–¹æ³•ï¼Œéš¾ä»¥è¯†åˆ«å¯¼è‡´é”™è¯¯çš„è§†è§‰ç‰¹å¾å¹¶é’ˆå¯¹æ€§æ”¹è¿›æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> VERSEæ–¹æ³•é€šè¿‡å¯è§†åŒ–æ½œåœ¨è¡¨ç¤ºæ¥è¯„ä¼°æ¨¡å‹å¯è¡Œæ€§ï¼Œè¯†åˆ«é—®é¢˜åŒºåŸŸå¹¶æŒ‡å¯¼åˆæˆæ•°æ®ç”Ÿæˆï¼Œä½¿ç”¨MERITæ•°æ®é›†è¿›è¡Œè®­ç»ƒå¹¶åœ¨MERIT Secretä¸Šè¿›è¡Œè¯„ä¼°ï¼Œä¼˜åŒ–äº†Donutå’ŒIdefics2ç­‰æœ¬åœ°æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜VERSEèƒ½æœ‰æ•ˆæ­ç¤ºä¸é”™è¯¯èšç±»ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œä½¿ç”¨åŒ…å«è¿™äº›ç‰¹å¾çš„æ ·æœ¬é‡æ–°è®­ç»ƒæ˜¾è‘—æå‡äº†F1æ€§èƒ½ä¸”æœªæŸå®³æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜åŒ–åçš„æœ¬åœ°æ¨¡å‹æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¶Šäº†GPT-4å’ŒPixtralç­‰SaaSè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Conclusion:</strong> VERSEä¸ºè§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†ç³»ç»ŸåŒ–çš„åˆ†æå’Œæ”¹è¿›æ¡†æ¶ï¼Œé€šè¿‡é’ˆå¯¹æ€§æ•°æ®å¢å¼ºç­–ç•¥æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼Œè¯æ˜äº†æœ¬åœ°æ¨¡å‹ç»è¿‡é€‚å½“ä¼˜åŒ–åèƒ½å¤Ÿä¸å•†ä¸šSaaSè§£å†³æ–¹æ¡ˆç«äº‰ï¼Œä¸ºæ–‡æ¡£ç†è§£ä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„æ¨¡å‹è¯Šæ–­å’Œå¢å¼ºæ–¹æ³•ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.</p>
<h3 id="24-from-understanding-to-engagement-personalized-pharmacy-video-clips-via-vision-language-models-vlms">[24] <a href="https://arxiv.org/abs/2601.05059">From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)</a></h3>
<p><em>Suyash Mishra, Qiang Li, Srikanth Patil, Anubhav Girdhar</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘åˆ¶è¯è¡Œä¸šçš„é¢†åŸŸè‡ªé€‚åº”è§†é¢‘ç‰‡æ®µç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡é›†æˆéŸ³é¢‘è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†é«˜æ•ˆã€ä¸ªæ€§åŒ–çš„å¤šæ¨¡æ€å†…å®¹å¤„ç†ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘æ‘˜è¦çš„æ•ˆç‡å’Œæ•ˆæœã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿåˆ¶è¯è¡Œä¸šåœ¨å¤„ç†å¼‚æ„å¤šæ¨¡æ€æ•°æ®ï¼ˆæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘å’Œç½‘é¡µé“¾æ¥ï¼‰æ—¶ä¾èµ–äººå·¥æ ‡æ³¨ï¼Œå­˜åœ¨ä¸ä¸€è‡´æ€§ã€è´¨é‡ä¸‹é™å’Œåˆ©ç”¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é•¿è§†é¢‘å’ŒéŸ³é¢‘æ•°æ®ï¼ˆå¦‚ä¸´åºŠè¯•éªŒè®¿è°ˆå’Œæ•™è‚²ç ”è®¨ä¼šï¼‰çš„å¤„ç†æŒ‘æˆ˜å°¤ä¸ºçªå‡ºã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æå‡ºäº†ä¸€ä¸ªé¢†åŸŸè‡ªé€‚åº”çš„è§†é¢‘åˆ°è§†é¢‘ç‰‡æ®µç”Ÿæˆæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒè´¡çŒ®ï¼šå¯å¤ç°çš„Cut &amp; Mergeç®—æ³•ï¼ˆæ”¯æŒæ·¡å…¥æ·¡å‡ºå’Œæ—¶é—´æˆ³å½’ä¸€åŒ–ä»¥ç¡®ä¿å¹³æ»‘è¿‡æ¸¡å’ŒéŸ³è§†é¢‘å¯¹é½ï¼‰ï¼›åŸºäºè§’è‰²å®šä¹‰å’Œæç¤ºæ³¨å…¥çš„ä¸ªæ€§åŒ–æœºåˆ¶ï¼ˆé’ˆå¯¹è¥é”€ã€åŸ¹è®­ã€ç›‘ç®¡ç­‰ä¸åŒéœ€æ±‚ï¼‰ï¼›ä»¥åŠå¹³è¡¡ALM/VLMå¢å¼ºå¤„ç†çš„æˆæœ¬é«˜æ•ˆç«¯åˆ°ç«¯æµæ°´çº¿ç­–ç•¥ã€‚</p>
<p><strong>Result:</strong> åœ¨Video MMEåŸºå‡†æµ‹è¯•ï¼ˆ900ä¸ªè§†é¢‘ï¼‰å’ŒåŒ…å«16,159ä¸ªåˆ¶è¯è§†é¢‘çš„ä¸“æœ‰æ•°æ®é›†ï¼ˆæ¶µç›–14ä¸ªç–¾ç—…é¢†åŸŸï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å®ç°äº†3-4å€çš„é€Ÿåº¦æå‡å’Œ4å€çš„æˆæœ¬é™ä½ï¼ŒåŒæ—¶å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç‰‡æ®µè´¨é‡ã€‚ä¸Gemini 2.5 Proç­‰æœ€å…ˆè¿›çš„VLMåŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç‰‡æ®µè¿è´¯æ€§å¾—åˆ†ï¼ˆ0.348ï¼‰å’Œä¿¡æ¯æ€§å¾—åˆ†ï¼ˆ0.721ï¼‰æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€æ˜ã€å¯å®šåˆ¶ä¸”æ”¯æŒåˆè§„æ€§çš„è§†é¢‘æ‘˜è¦æ–¹æ³•åœ¨ç”Ÿå‘½ç§‘å­¦é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ï¼Œä¸ä»…å®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡å’Œæˆæœ¬èŠ‚çº¦ï¼Œè¿˜é€šè¿‡é¢†åŸŸè‡ªé€‚åº”å’Œä¸ªæ€§åŒ–æœºåˆ¶æé«˜äº†è¾“å‡ºè´¨é‡ï¼Œä¸ºåˆ¶è¯è¡Œä¸šçš„æ•°å­—åŒ–è½¬å‹æä¾›äº†å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut &amp; Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.</p>
<h3 id="25-vision-language-introspection-mitigating-overconfident-hallucinations-in-mllms-via-interpretable-bi-causal-steering">[25] <a href="https://arxiv.org/abs/2601.05159">Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering</a></h3>
<p><em>Shuliang Liu, Songbo Yang, Dong Fang, Sihang Jia, Yuqi Tang, Lingfeng Su, Ruoshui Peng, Yibo Yan, Xin Zou, Xuming Hu</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVision-Language Introspection (VLI)çš„è®­ç»ƒå…è´¹æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿå…ƒè®¤çŸ¥è‡ªæˆ‘çº æ­£è¿‡ç¨‹æ¥å‡å°‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç‰©ä½“å¹»è§‰é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å±æ€§å†…çœè¯Šæ–­å¹»è§‰é£é™©ï¼Œå¹¶é‡‡ç”¨å¯è§£é‡Šçš„åŒå› æœå¼•å¯¼åŠ¨æ€è°ƒæ•´æ¨ç†è¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯é æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç‰©ä½“å¹»è§‰ä¸¥é‡æŸå®³äº†å…¶å¯é æ€§ï¼Œè¿™ä¸»è¦æºäºæ¨¡å‹åœ¨è®¤çŸ¥å†…çœæ–¹é¢çš„æ ¹æœ¬æ€§å¤±è´¥ï¼Œå³æ¨¡å‹ç›²ç›®ä¿¡ä»»è¯­è¨€å…ˆéªŒè€Œéå…·ä½“è§†è§‰è¯æ®ã€‚ç°æœ‰ç¼“è§£æ–¹æ³•å­˜åœ¨æ˜æ˜¾å±€é™ï¼šå¯¹æ¯”è§£ç æ–¹æ³•ä»…è¡¨é¢æ“ä½œè€Œæœªçº æ­£å†…éƒ¨è¯­ä¹‰é”™ä½ï¼Œè€Œå½“å‰æ½œåœ¨å¼•å¯¼æ–¹æ³•ä¾èµ–ç¼ºä¹å®ä¾‹ç‰¹å®šç²¾åº¦çš„é™æ€å‘é‡ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†Vision-Language Introspection (VLI)è®­ç»ƒå…è´¹æ¨ç†æ¡†æ¶ï¼Œæ¨¡æ‹Ÿå…ƒè®¤çŸ¥è‡ªæˆ‘çº æ­£è¿‡ç¨‹ã€‚VLIé¦–å…ˆæ‰§è¡Œå±æ€§å†…çœï¼Œé€šè¿‡æ¦‚ç‡å†²çªæ£€æµ‹è¯Šæ–­å¹»è§‰é£é™©å¹¶å®šä½å› æœè§†è§‰é”šç‚¹ï¼›ç„¶åé‡‡ç”¨å¯è§£é‡Šçš„åŒå› æœå¼•å¯¼ï¼Œä¸»åŠ¨è°ƒåˆ¶æ¨ç†è¿‡ç¨‹ï¼ŒåŠ¨æ€éš”ç¦»è§†è§‰è¯æ®ä¸èƒŒæ™¯å™ªå£°ï¼ŒåŒæ—¶é€šè¿‡è‡ªé€‚åº”æ ¡å‡†æ¶ˆé™¤ç›²ç›®ç½®ä¿¡ã€‚</p>
<p><strong>Result:</strong> VLIåœ¨å…ˆè¿›æ¨¡å‹ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨MMHal-Benchä¸Šå°†ç‰©ä½“å¹»è§‰ç‡é™ä½äº†12.67%ï¼Œåœ¨POPEä¸Šå°†å‡†ç¡®ç‡æé«˜äº†5.8%ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰©ä½“è¯†åˆ«ä»»åŠ¡ä¸­çš„å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œè¯æ˜äº†è®­ç»ƒå…è´¹æ¨ç†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡æ¨¡æ‹Ÿå…ƒè®¤çŸ¥è¿‡ç¨‹è§£å†³å¤šæ¨¡æ€å¹»è§‰é—®é¢˜çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè®­ç»ƒå…è´¹å¹²é¢„æä¾›äº†æ–°èŒƒå¼ã€‚VLIæ¡†æ¶é€šè¿‡å†…çœè¯Šæ–­å’ŒåŠ¨æ€å¼•å¯¼çš„ç»“åˆï¼Œå®ç°äº†å¯¹æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„ç²¾ç»†è°ƒæ§ï¼Œä¸ºæå‡å¤šæ¨¡æ€æ¨¡å‹çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§å¼€è¾Ÿäº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.</p>
<h3 id="26-unilips-unified-lidar-pseudo-labeling-with-geometry-grounded-dynamic-scene-decomposition">[26] <a href="https://arxiv.org/abs/2601.05105">UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition</a></h3>
<p><em>Filippo Ghilotti, Samuel Brucker, Nahku Saidy, Matteo Matteucci, Mario Bijelic, Felix Heide</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£å¤šæ¨¡æ€ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œåˆ©ç”¨æ¿€å…‰é›·è¾¾æ‰«æçš„æ—¶é—´å‡ ä½•ä¸€è‡´æ€§å°†æ–‡æœ¬å’Œ2Dè§†è§‰åŸºç¡€æ¨¡å‹çš„çº¿ç´¢ç›´æ¥æå‡åˆ°3Dç©ºé—´ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ï¼Œå®ç°äº†3Dè¯­ä¹‰æ ‡ç­¾ã€è¾¹ç•Œæ¡†å’Œå¯†é›†ç‚¹äº‘çš„è‡ªåŠ¨ç”Ÿæˆã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªåŠ¨é©¾é©¶åº”ç”¨ä¸­æœªæ ‡æ³¨çš„æ¿€å…‰é›·è¾¾æ—¥å¿—è™½ç„¶åŒ…å«ä¸°å¯Œçš„3Då‡ ä½•ä¿¡æ¯ï¼Œä½†ç”±äºç¼ºä¹äººå·¥æ ‡æ³¨è€Œéš¾ä»¥åˆ©ç”¨ï¼Œå½¢æˆäº†æ„ŸçŸ¥ç ”ç©¶çš„ä¸»è¦æˆæœ¬ç“¶é¢ˆã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„äººå·¥ç›‘ç£ï¼Œé™åˆ¶äº†å¤§è§„æ¨¡æ•°æ®é›†çš„åˆ©ç”¨æ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åŸºäºä»æ—¶é—´ç´¯ç§¯çš„æ¿€å…‰é›·è¾¾åœ°å›¾ä¸­å­¦ä¹ çš„å¼ºå‡ ä½•å…ˆéªŒï¼Œé€šè¿‡æ—¶é—´å‡ ä½•ä¸€è‡´æ€§è·¨æ¿€å…‰é›·è¾¾æ‰«ææå‡å’Œèåˆæ–‡æœ¬ä¸2Dè§†è§‰åŸºç¡€æ¨¡å‹çš„çº¿ç´¢ã€‚å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è¿­ä»£æ›´æ–°è§„åˆ™ï¼Œå¼ºåˆ¶å®æ–½è”åˆå‡ ä½•-è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡ä¸ä¸€è‡´æ€§æ£€æµ‹ç§»åŠ¨ç‰©ä½“ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå±•ç¤ºäº†é²æ£’çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ç”Ÿæˆ3Dè¯­ä¹‰æ ‡ç­¾ã€3Dè¾¹ç•Œæ¡†å’Œå¯†é›†æ¿€å…‰é›·è¾¾æ‰«æã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„è¯­ä¹‰åˆ†å‰²å’Œç‰©ä½“æ£€æµ‹ä¼ªæ ‡ç­¾æ–¹æ³•ã€‚å³ä½¿ä½¿ç”¨ä¸€å°éƒ¨åˆ†å‡ ä½•ä¸€è‡´çš„ç¨ å¯†åŒ–æ¿€å…‰é›·è¾¾æ•°æ®ï¼Œä¹Ÿèƒ½åœ¨80-150ç±³å’Œ150-250ç±³èŒƒå›´å†…åˆ†åˆ«å°†æ·±åº¦é¢„æµ‹MAEæå‡51.5%å’Œ22.0%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨æ—¶é—´å‡ ä½•ä¸€è‡´æ€§å®ç°æ— ç›‘ç£3Dæ„ŸçŸ¥ä¼ªæ ‡ç­¾çš„å¯è¡Œæ€§ï¼Œä¸ºé™ä½è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç ”ç©¶çš„æ ‡æ³¨æˆæœ¬æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚å‡ ä½•ä¸€è‡´çš„ç¨ å¯†åŒ–æ¿€å…‰é›·è¾¾æ•°æ®æ˜¾è‘—æå‡äº†æ·±åº¦é¢„æµ‹æ€§èƒ½ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ¨åŠ¨å¤§è§„æ¨¡æ— ç›‘ç£3Dæ„ŸçŸ¥å­¦ä¹ æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.</p>
<h3 id="27-mechanisms-of-prompt-induced-hallucination-in-vision-language-models">[27] <a href="https://arxiv.org/abs/2601.05201">Mechanisms of Prompt-Induced Hallucination in Vision-Language Models</a></h3>
<p><em>William Rudman, Michal Golovanevsky, Dana Arad, Yonatan Belinkov, Ritambhara Singh, Carsten Eickhoff, Kyle Mahowald</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶é€šè¿‡æœºåˆ¶åˆ†ææ­ç¤ºäº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æç¤ºè¯±å¯¼å¹»è§‰çš„æ•…éšœæ¨¡å¼ï¼Œå‘ç°é€šè¿‡æ¶ˆèå°‘é‡æ³¨æ„åŠ›å¤´å¯æ˜¾è‘—å‡å°‘å¹»è§‰è¡Œä¸ºï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯é™ä½è‡³å°‘40%çš„å¹»è§‰ç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è™½ç„¶èƒ½åŠ›å¼ºå¤§ï¼Œä½†ç»å¸¸äº§ç”Ÿå¹»è§‰ï¼Œå€¾å‘äºä¾èµ–æ–‡æœ¬æç¤ºè€Œå¿½ç•¥è§†è§‰è¯æ®ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶è¿™ç§æç¤ºè¯±å¯¼å¹»è§‰çš„æ•…éšœæ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨å—æ§çš„å¯¹è±¡è®¡æ•°åœºæ™¯ä¸­ï¼Œå½“æç¤ºé«˜ä¼°å›¾åƒä¸­å¯¹è±¡æ•°é‡æ—¶æ¨¡å‹çš„è¡Œä¸ºå˜åŒ–ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨å—æ§çš„å¯¹è±¡è®¡æ•°å®éªŒè®¾ç½®ï¼Œé€šè¿‡å¯¹æ¯”æ–‡æœ¬æç¤ºä¸è§†è§‰è¯æ®çš„å·®å¼‚æ¥è¯„ä¼°æ¨¡å‹è¡Œä¸ºã€‚å¯¹ä¸‰ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæœºåˆ¶åˆ†æï¼Œè¯†åˆ«å‡ºå¯¼è‡´æç¤ºè¯±å¯¼å¹»è§‰çš„ç‰¹å®šæ³¨æ„åŠ›å¤´ï¼Œå¹¶é€šè¿‡æ¶ˆèè¿™äº›æ³¨æ„åŠ›å¤´æ¥éªŒè¯å…¶ä½œç”¨ã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°ï¼Œåœ¨ä½å¯¹è±¡æ•°é‡æ—¶æ¨¡å‹èƒ½å¤Ÿçº æ­£é«˜ä¼°ï¼Œä½†éšç€å¯¹è±¡æ•°é‡å¢åŠ ï¼Œæ¨¡å‹è¶Šæ¥è¶Šå€¾å‘äºéµå¾ªæç¤ºè€Œå¿½ç•¥è§†è§‰å·®å¼‚ã€‚æ¶ˆèè¯†åˆ«å‡ºçš„å°‘é‡æ³¨æ„åŠ›å¤´å¯å°†æç¤ºè¯±å¯¼å¹»è§‰å‡å°‘è‡³å°‘40%ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚ä¸åŒæ¨¡å‹ä¸­ï¼Œè¿™äº›å¹»è§‰å¤´ä»¥æ¨¡å‹ç‰¹å®šçš„æ–¹å¼ä»‹å¯¼æç¤ºå¤åˆ¶è¡Œä¸ºã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æç¤ºè¯±å¯¼å¹»è§‰çš„å†…éƒ¨æœºåˆ¶ï¼Œå±•ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å®ç°è¿™äº›è¡Œä¸ºæ—¶çš„ç‰¹å®šå·®å¼‚ã€‚æ¶ˆèå¹»è§‰æ³¨æ„åŠ›å¤´èƒ½å¤Ÿå¢å¼ºæ¨¡å‹å¯¹è§†è§‰è¯æ®çš„æ ¡æ­£èƒ½åŠ›ï¼Œä¸ºç†è§£å’Œç¼“è§£æ¨¡å‹å¹»è§‰æä¾›äº†æœºåˆ¶å±‚é¢çš„è§è§£ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.</p>
<h3 id="28-re-align-structured-reasoning-guided-alignment-for-in-context-image-generation-and-editing">[28] <a href="https://arxiv.org/abs/2601.05124">Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing</a></h3>
<p><em>Runze He, Yiji Cheng, Tiankai Hang, Zhimin Li, Yu Xu, Zijin Yin, Shiyi Zhang, Wenxun Dai, Penghui Du, Ao Ma, Chunyu Wang, Qinglin Lu, Jizhong Han, Jiao Dai</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºRe-Alignæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–æ¨ç†å¼•å¯¼çš„å¯¹é½æœºåˆ¶å¼¥åˆå¤šæ¨¡æ€æ¨¡å‹ä¸­ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å·®è·ï¼Œæ˜¾è‘—æå‡äº†ä¸Šä¸‹æ–‡å›¾åƒç”Ÿæˆä¸ç¼–è¾‘ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†è¿™äº›ä¼˜åŠ¿å¾€å¾€æ— æ³•æœ‰æ•ˆè¿ç§»åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡å›¾åƒç”Ÿæˆä¸ç¼–è¾‘ä»»åŠ¡ä¸­ç”¨æˆ·æ„å›¾çš„ç†è§£ä¸å¿ å®æ‰§è¡Œä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</p>
<p><strong>Method:</strong> Re-Alignæ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸Šä¸‹æ–‡æ€ç»´é“¾ç»“æ„åŒ–æ¨ç†èŒƒå¼ï¼Œå®ƒå°†è¯­ä¹‰å¼•å¯¼ä¸å‚è€ƒå…³è”è§£è€¦ï¼Œæä¾›æ¸…æ™°çš„æ–‡æœ¬ç›®æ ‡å¹¶å‡å°‘å‚è€ƒå›¾åƒé—´çš„æ··æ·†ï¼›åŒæ—¶å¼•å…¥åŸºäºä»£ç†å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ¡ˆï¼Œé€šè¿‡è¡¡é‡ç»“æ„åŒ–æ¨ç†æ–‡æœ¬ä¸ç”Ÿæˆå›¾åƒä¹‹é—´çš„å¯¹é½åº¦æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒéªŒè¯è¡¨æ˜ï¼ŒRe-Alignåœ¨æ¨¡å‹è§„æ¨¡å’Œè®¡ç®—èµ„æºç›¸å½“çš„æƒ…å†µä¸‹ï¼Œåœ¨ä¸Šä¸‹æ–‡å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šå‡ä¼˜äºç«äº‰æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¼¥åˆç†è§£ä¸ç”Ÿæˆå·®è·æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†ç»“æ„åŒ–æ¨ç†å¼•å¯¼çš„å¯¹é½æœºåˆ¶åœ¨ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæå‡ä¸Šä¸‹æ–‡å›¾åƒç”Ÿæˆä¸ç¼–è¾‘ä»»åŠ¡çš„æ€§èƒ½æä¾›äº†æœ‰æ•ˆæ¡†æ¶ï¼Œå¹¶å¼ºè°ƒäº†å¼ºåŒ–å­¦ä¹ åœ¨ä¼˜åŒ–ç”Ÿæˆå¯¹é½æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.</p>
<h3 id="29-robovip-multi-view-video-generation-with-visual-identity-prompting-augments-robot-manipulation">[29] <a href="https://arxiv.org/abs/2601.05241">RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</a></h3>
<p><em>Boyang Wang, Haoran Zhang, Shujie Zhang, Jinkun Hao, Mingda Jia, Qi Lv, Yucheng Mao, Zhaoyang Lyu, Jia Zeng, Xudong Xu, Jiangmiao Pang</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è§†è§‰èº«ä»½æç¤ºæ–¹æ³•ï¼Œé€šè¿‡æä¾›ç¤ºä¾‹å›¾åƒä½œä¸ºæ¡ä»¶è¾“å…¥æ¥å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ‰€éœ€çš„åœºæ™¯è®¾ç½®ï¼Œä»è€Œå¢å¼ºæœºå™¨äººæ“ä½œæ•°æ®ï¼Œè§£å†³äº†ç°æœ‰æ–‡æœ¬æç¤ºæ–¹æ³•åœ¨å¤šè§†è§’å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢çš„ä¸è¶³ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç”±äºç¡¬ä»¶å’Œç‰©ç†è®¾ç½®çš„é™åˆ¶ï¼Œæ”¶é›†å¤§è§„æ¨¡çœŸå®ä¸–ç•Œæ“ä½œæ•°æ®åœ¨ä¸åŒç¯å¢ƒä¸­éš¾ä»¥æ‰©å±•ï¼Œè€Œç°æœ‰çš„åŸºäºæ–‡æœ¬æç¤ºçš„å›¾åƒæ‰©æ•£æ¨¡å‹æ–¹æ³•å¾€å¾€å¿½è§†äº†æœ€å…ˆè¿›ç­–ç•¥æ¨¡å‹æ‰€éœ€çš„å¤šè§†è§’å’Œæ—¶é—´ä¸€è‡´æ€§è§‚æµ‹éœ€æ±‚ï¼Œä¸”ä»…é æ–‡æœ¬æç¤ºæ— æ³•å¯é åœ°æŒ‡å®šåœºæ™¯è®¾ç½®ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡å¼•å…¥äº†è§†è§‰èº«ä»½æç¤ºæ–¹æ³•ï¼Œå°†ç¤ºä¾‹å›¾åƒä½œä¸ºæ¡ä»¶è¾“å…¥æä¾›ç»™æ‰©æ•£æ¨¡å‹ï¼Œä»¥å¼•å¯¼ç”Ÿæˆæ‰€éœ€çš„åœºæ™¯è®¾ç½®ï¼ŒåŒæ—¶æ„å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æµç¨‹ï¼Œä»å¤§å‹æœºå™¨äººæ•°æ®é›†ä¸­ç­–åˆ’è§†è§‰èº«ä»½æ± ï¼Œç”¨äºå¢å¼ºæ“ä½œæ•°æ®ã€‚</p>
<p><strong>Result:</strong> ä½¿ç”¨å¢å¼ºçš„æ“ä½œæ•°æ®è®­ç»ƒä¸‹æ¸¸è§†è§‰-è¯­è¨€-åŠ¨ä½œå’Œè§†è§‰è¿åŠ¨ç­–ç•¥æ¨¡å‹ï¼Œåœ¨ä»¿çœŸå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­å‡è·å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è§†è§‰èº«ä»½æç¤ºæ–¹æ³•ä¸ºæœºå™¨äººæ“ä½œæ•°æ®å¢å¼ºæä¾›äº†æ›´å¯é çš„åœºæ™¯æ§åˆ¶æœºåˆ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹æœºå™¨äººæ•°æ®é›†ä¸­çš„è§†è§‰èº«ä»½æ± ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰å¤šè§†è§’ä¸€è‡´æ€§å’Œæ—¶é—´è¿è´¯æ€§çš„è§‚æµ‹æ•°æ®ï¼Œä»è€Œæå‡ç­–ç•¥æ¨¡å‹çš„è®­ç»ƒæ•ˆæœå’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.</p>
<h3 id="30-a-lightweight-and-explainable-vision-language-framework-for-crop-disease-visual-question-answering">[30] <a href="https://arxiv.org/abs/2601.05143">A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering</a></h3>
<p><em>Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Siam Ansary</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œç”¨äºä»å¶ç‰‡å›¾åƒä¸­è¿›è¡Œä½œç‰©å’Œç—…å®³è¯†åˆ«ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†Swin Transformerè§†è§‰ç¼–ç å™¨å’Œåºåˆ—åˆ°åºåˆ—è¯­è¨€è§£ç å™¨ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†å‚æ•°æ•°é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä½œç‰©ç—…å®³è§†è§‰é—®ç­”éœ€è¦å‡†ç¡®çš„è§†è§‰ç†è§£å’Œå¯é çš„è¯­è¨€ç”Ÿæˆèƒ½åŠ›ï¼Œç°æœ‰å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹å‚æ•°é‡è¿‡å¤§ï¼Œç¼ºä¹é’ˆå¯¹å†œä¸šé¢†åŸŸç‰¹å®šä»»åŠ¡çš„è½»é‡çº§é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨Swin Transformerä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œç»“åˆåºåˆ—åˆ°åºåˆ—è¯­è¨€è§£ç å™¨æ„å»ºè½»é‡çº§è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ä¼˜åŒ–è§†è§‰è¡¨ç¤ºå­¦ä¹ å’Œè·¨æ¨¡æ€å¯¹é½ï¼Œå¹¶ä½¿ç”¨Grad-CAMå’Œè¯å…ƒçº§å½’å› è¿›è¡Œå¯è§£é‡Šæ€§åˆ†æã€‚</p>
<p><strong>Result:</strong> åœ¨å¤§è§„æ¨¡ä½œç‰©ç—…å®³æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä½œç‰©å’Œç—…å®³è¯†åˆ«æ–¹é¢å‡è¾¾åˆ°é«˜å‡†ç¡®ç‡ï¼Œåœ¨BLEUã€ROUGEå’ŒBERTScoreç­‰è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶å‚æ•°é‡æ˜¾è‘—å°‘äºå¤§è§„æ¨¡è§†è§‰è¯­è¨€åŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜ä»»åŠ¡ç‰¹å®šçš„è§†è§‰é¢„è®­ç»ƒå¯¹ä½œç‰©ç—…å®³è§†è§‰é—®ç­”å…·æœ‰æ˜¾è‘—æ•ˆæœï¼Œè½»é‡çº§æ¡†æ¶åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å®ç°äº†å‚æ•°æ•ˆç‡ï¼Œä¸ºå†œä¸šé¢†åŸŸçš„è§†è§‰è¯­è¨€åº”ç”¨æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.</p>
<h3 id="31-videoauto-r1-video-auto-reasoning-via-thinking-once-answering-twice">[31] <a href="https://arxiv.org/abs/2601.05175">VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice</a></h3>
<p><em>Shuming Liu, Mingchen Zhuge, Changsheng Zhao, Jun Chen, Lemeng Wu, Zechun Liu, Chenchen Zhu, Zhipeng Cai, Chong Zhou, Haozhe Liu, Ernie Chang, Saksham Suri, Hongyu Xu, Qi Qian, Wei Wen, Balakrishnan Varadarajan, Zhuang Liu, Hu Xu, Florian Bordes, Raghuraman Krishnamoorthi, Bernard Ghanem, Vikas Chandra, Yunyang Xiong</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºVideoAuto-R1æ¡†æ¶ï¼Œé€šè¿‡"ä»…åœ¨å¿…è¦æ—¶æ¨ç†"çš„ç­–ç•¥ä¼˜åŒ–è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œåœ¨ä¿æŒæœ€å…ˆè¿›å‡†ç¡®ç‡çš„åŒæ—¶æ˜¾è‘—æå‡æ•ˆç‡ï¼Œå°†å¹³å‡å“åº”é•¿åº¦å‡å°‘çº¦3.3å€ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡æ€ç»´é“¾æ¨ç†åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†é¢‘ç†è§£ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†å…¶ç›¸å¯¹äºç›´æ¥å›ç­”çš„å¿…è¦æ€§å’Œä¼˜åŠ¿å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç ”ç©¶å‘ç°ï¼Œå¯¹äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è§†é¢‘æ¨¡å‹ï¼Œç›´æ¥å›ç­”çš„æ€§èƒ½å¾€å¾€åŒ¹é…ç”šè‡³è¶…è¶Šæ€ç»´é“¾æ¨ç†ï¼Œè€Œåè€…éœ€è¦æ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚å› æ­¤éœ€è¦æ¢ç´¢æ›´é«˜æ•ˆçš„æ¨ç†ç­–ç•¥ã€‚</p>
<p><strong>Method:</strong> æå‡ºVideoAuto-R1è§†é¢‘ç†è§£æ¡†æ¶ï¼Œé‡‡ç”¨"ä»…åœ¨å¿…è¦æ—¶æ¨ç†"çš„ç­–ç•¥ã€‚è®­ç»ƒé˜¶æ®µéµå¾ª"æ€è€ƒä¸€æ¬¡ï¼Œå›ç­”ä¸¤æ¬¡"èŒƒå¼ï¼šæ¨¡å‹é¦–å…ˆç”Ÿæˆåˆå§‹ç­”æ¡ˆï¼Œç„¶åè¿›è¡Œæ¨ç†ï¼Œæœ€åè¾“å‡ºç»è¿‡å®¡æŸ¥çš„ç­”æ¡ˆï¼Œä¸¤ä¸ªç­”æ¡ˆéƒ½é€šè¿‡å¯éªŒè¯çš„å¥–åŠ±è¿›è¡Œç›‘ç£ã€‚æ¨ç†é˜¶æ®µä½¿ç”¨åˆå§‹ç­”æ¡ˆçš„ç½®ä¿¡åº¦åˆ†æ•°æ¥å†³å®šæ˜¯å¦è¿›è¡Œæ¨ç†ã€‚</p>
<p><strong>Result:</strong> åœ¨è§†é¢‘é—®ç­”å’Œå®šä½åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVideoAuto-R1å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶æ˜¾è‘—æå‡æ•ˆç‡ï¼Œå°†å¹³å‡å“åº”é•¿åº¦å‡å°‘çº¦3.3å€ï¼ˆä¾‹å¦‚ä»149ä¸ªæ ‡è®°å‡å°‘åˆ°44ä¸ªæ ‡è®°ï¼‰ã€‚ç ”ç©¶è¿˜è§‚å¯Ÿåˆ°ï¼Œåœ¨æ„ŸçŸ¥å¯¼å‘ä»»åŠ¡ä¸­æ€ç»´æ¨¡å¼æ¿€æ´»ç‡è¾ƒä½ï¼Œè€Œåœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­æ¿€æ´»ç‡è¾ƒé«˜ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œæ˜¾å¼çš„åŸºäºè¯­è¨€çš„æ¨ç†é€šå¸¸æœ‰ç›Šä½†å¹¶éæ€»æ˜¯å¿…è¦ã€‚VideoAuto-R1æ¡†æ¶é€šè¿‡è‡ªé€‚åº”æ¨ç†ç­–ç•¥åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ï¼Œä¸ºå¤šæ¨¡æ€è§†é¢‘ç†è§£ä»»åŠ¡æä¾›äº†æ›´å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ä¸åŒä»»åŠ¡ç±»å‹å¯¹æ¨ç†éœ€æ±‚çš„å·®å¼‚æ€§ï¼Œä¸ºæœªæ¥é«˜æ•ˆå¤šæ¨¡æ€æ¨¡å‹è®¾è®¡æä¾›äº†é‡è¦è§è§£ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.</p>
<h3 id="32-objectforesight-predicting-future-3d-object-trajectories-from-human-videos">[32] <a href="https://arxiv.org/abs/2601.05237">ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos</a></h3>
<p><em>Rustin Soraki, Homanga Bharadhwaj, Ali Farhadi, Roozbeh Mottaghi</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ObjectForesightï¼Œä¸€ç§ä»è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘åºåˆ—é¢„æµ‹åˆšä½“ç‰©ä½“æœªæ¥6è‡ªç”±åº¦ä½å§¿å’Œè½¨è¿¹çš„3Dç‰©ä½“ä¸­å¿ƒåŠ¨åŠ›å­¦æ¨¡å‹ï¼Œé€šè¿‡æ˜¾å¼3Dç‰©ä½“çº§è¡¨ç¤ºå®ç°å‡ ä½•åŸºç¡€å’Œæ—¶åºä¸€è‡´çš„é¢„æµ‹ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> äººç±»èƒ½å¤Ÿè½»æ¾é¢„æµ‹ç‰©ä½“é€šè¿‡äº¤äº’å¯èƒ½å‘ç”Ÿçš„è¿åŠ¨æˆ–å˜åŒ–ï¼Œä½†ç°æœ‰è®¡ç®—ç³»ç»Ÿç¼ºä¹ç›´æ¥ä»è¢«åŠ¨è§†è§‰è§‚å¯Ÿä¸­é¢„æµ‹åˆç†æœªæ¥ç‰©ä½“è¿åŠ¨çš„èƒ½åŠ›ã€‚ä¼ ç»Ÿä¸–ç•Œæ¨¡å‹æˆ–åŠ¨åŠ›å­¦æ¨¡å‹åœ¨åƒç´ æˆ–æ½œåœ¨ç©ºé—´ä¸­æ“ä½œï¼Œæ— æ³•æä¾›å‡ ä½•åŸºç¡€å’Œæ—¶åºä¸€è‡´çš„ç‰©ä½“çº§é¢„æµ‹ï¼Œè¿™é™åˆ¶äº†ç³»ç»Ÿå¯¹ç‰©ä½“åŠŸèƒ½æ€§å’Œè½¨è¿¹çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ObjectForesighté‡‡ç”¨3Dç‰©ä½“ä¸­å¿ƒåŠ¨åŠ›å­¦æ¨¡å‹æ¶æ„ï¼Œä»çŸ­è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘åºåˆ—é¢„æµ‹åˆšä½“ç‰©ä½“çš„æœªæ¥6è‡ªç”±åº¦ä½å§¿å’Œè½¨è¿¹ã€‚è¯¥æ–¹æ³•åœ¨ç‰©ä½“çº§åˆ«æ˜¾å¼è¡¨ç¤º3Dä¸–ç•Œï¼Œè€Œéä¼ ç»Ÿçš„åƒç´ æˆ–æ½œåœ¨ç©ºé—´è¡¨ç¤ºã€‚ä¸ºå¤§è§„æ¨¡è®­ç»ƒè¯¥æ¨¡å‹ï¼Œç ”ç©¶åˆ©ç”¨åˆ†å‰²ã€ç½‘æ ¼é‡å»ºå’Œ3Dä½å§¿ä¼°è®¡çš„æœ€æ–°è¿›å±•ï¼Œæ„å»ºäº†åŒ…å«200å¤šä¸‡ä¸ªçŸ­ç‰‡æ®µå’Œä¼ªåœ°é¢çœŸå€¼3Dç‰©ä½“è½¨è¿¹çš„æ•°æ®é›†ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒObjectForesightåœ¨å‡†ç¡®æ€§ã€å‡ ä½•ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å–å¾—æ˜¾è‘—æå‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–åˆ°æœªè§è¿‡çš„ç‰©ä½“å’Œåœºæ™¯ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ•æ‰ç‰©ä½“åŠŸèƒ½æ€§å’Œè½¨è¿¹ï¼Œåœ¨é¢„æµ‹æœªæ¥ç‰©ä½“è¿åŠ¨æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œä¸ºå­¦ä¹ ç‰©ç†åŸºç¡€çš„ç‰©ä½“ä¸­å¿ƒåŠ¨åŠ›å­¦æ¨¡å‹å»ºç«‹äº†å¯æ‰©å±•æ¡†æ¶ã€‚</p>
<p><strong>Conclusion:</strong> ObjectForesightä¸ºç›´æ¥ä»è§‚å¯Ÿä¸­å­¦ä¹ ç‰©ç†åŸºç¡€çš„ç‰©ä½“ä¸­å¿ƒåŠ¨åŠ›å­¦æ¨¡å‹æä¾›äº†å¯æ‰©å±•æ¡†æ¶ï¼Œå…¶æ˜¾å¼3Dç‰©ä½“çº§è¡¨ç¤ºå®ç°äº†å‡ ä½•åŸºç¡€å’Œæ—¶åºä¸€è‡´çš„é¢„æµ‹ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å¤§è§„æ¨¡ä¼ªåœ°é¢çœŸå€¼æ•°æ®åœ¨è®­ç»ƒå¤æ‚åŠ¨åŠ›å­¦æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥åœ¨æœºå™¨äººã€å¢å¼ºç°å®å’Œäº¤äº’ç³»ç»Ÿä¸­çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io</p>
<h3 id="33-plenoptic-video-generation">[33] <a href="https://arxiv.org/abs/2601.05239">Plenoptic Video Generation</a></h3>
<p><em>Xiao Fu, Shitao Tang, Min Shi, Xian Liu, Jinwei Gu, Ming-Yu Liu, Dahua Lin, Chen-Hsuan Lin</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†PlenopticDreameræ¡†æ¶ï¼Œé€šè¿‡åŒæ­¥ç”Ÿæˆå¹»è§‰æ¥ä¿æŒæ—¶ç©ºè®°å¿†ï¼Œè§£å†³äº†å¤šè§†è§’è§†é¢‘é‡æ¸²æŸ“ä¸­çš„ä¸€è‡´æ€§é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç›¸æœºæ§åˆ¶çš„ç”Ÿæˆè§†é¢‘é‡æ¸²æŸ“æ–¹æ³•ï¼ˆå¦‚ReCamMasterï¼‰åœ¨å•è§†è§’è®¾ç½®ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤šè§†è§’åœºæ™¯ä¸­éš¾ä»¥ä¿æŒä¸€è‡´æ€§ï¼Œç”Ÿæˆæ¨¡å‹çš„éšæœºæ€§ä½¿å¾—å¹»è§‰åŒºåŸŸçš„æ—¶ç©ºè¿è´¯æ€§éš¾ä»¥ä¿è¯ï¼Œè¿™æ„æˆäº†å½“å‰ç ”ç©¶çš„ä¸»è¦å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> PlenopticDreameræ¡†æ¶çš„æ ¸å¿ƒæ˜¯è®­ç»ƒä¸€ä¸ªå¤šè¾“å…¥å•è¾“å‡ºçš„è§†é¢‘æ¡ä»¶æ¨¡å‹ï¼Œé‡‡ç”¨è‡ªå›å½’æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå¹¶è¾…ä»¥ç›¸æœºå¼•å¯¼çš„è§†é¢‘æ£€ç´¢ç­–ç•¥è‡ªé€‚åº”é€‰æ‹©å…ˆå‰ç”Ÿæˆçš„æ˜¾è‘—è§†é¢‘ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚è®­ç»ƒè¿‡ç¨‹è¿˜åŒ…å«æ¸è¿›å¼ä¸Šä¸‹æ–‡ç¼©æ”¾ä»¥æ”¹å–„æ”¶æ•›æ€§ï¼Œè‡ªæ¡ä»¶æœºåˆ¶ä»¥å¢å¼ºå¯¹è¯¯å·®ç´¯ç§¯å¯¼è‡´çš„é•¿ç¨‹è§†è§‰é€€åŒ–çš„é²æ£’æ€§ï¼Œä»¥åŠé•¿è§†é¢‘æ¡ä»¶æœºåˆ¶ä»¥æ”¯æŒæ‰©å±•è§†é¢‘ç”Ÿæˆã€‚</p>
<p><strong>Result:</strong> åœ¨Basicå’ŒAgibotåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPlenopticDreamerå®ç°äº†æœ€å…ˆè¿›çš„è§†é¢‘é‡æ¸²æŸ“æ€§èƒ½ï¼Œæä¾›äº†å“è¶Šçš„è§†è§’åŒæ­¥ã€é«˜ä¿çœŸè§†è§‰æ•ˆæœã€ç²¾ç¡®çš„ç›¸æœºæ§åˆ¶ä»¥åŠå¤šæ ·åŒ–çš„è§†è§’è½¬æ¢ï¼ˆå¦‚ç¬¬ä¸‰äººç§°åˆ°ç¬¬ä¸‰äººç§°ï¼Œä»¥åŠæœºå™¨äººæ“ä½œä¸­çš„å¤´éƒ¨è§†è§’åˆ°å¤¹çˆªè§†è§’ï¼‰ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡åŒæ­¥ç”Ÿæˆå¹»è§‰æ¥ä¿æŒæ—¶ç©ºè®°å¿†çš„æ–¹æ³•æœ‰æ•ˆè§£å†³äº†å¤šè§†è§’è§†é¢‘é‡æ¸²æŸ“ä¸­çš„ä¸€è‡´æ€§é—®é¢˜ï¼Œæå‡ºçš„æ¡†æ¶åœ¨ä¿æŒè§†è§‰è´¨é‡å’Œç›¸æœºæ§åˆ¶ç²¾åº¦çš„åŒæ—¶å®ç°äº†æ›´å¥½çš„è§†è§’åŒæ­¥ï¼Œä¸ºç”Ÿæˆè§†é¢‘é‡æ¸²æŸ“é¢†åŸŸæä¾›äº†æ–°çš„æŠ€æœ¯æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šè§†è§’ä¸€è‡´æ€§çš„åº”ç”¨åœºæ™¯ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="34-attribute-aware-controlled-product-generation-with-llms-for-e-commerce">[34] <a href="https://arxiv.org/abs/2601.04200">Attribute-Aware Controlled Product Generation with LLMs for E-commerce</a></h3>
<p><em>Virginia Negri, VÃ­ctor MartÃ­nez GÃ³mez, Sergio A. Balanya, Subburam Rajaram</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆç”µå•†äº§å“æ•°æ®çš„ç³»ç»Ÿæ¡†æ¶ï¼Œé€šè¿‡å—æ§ä¿®æ”¹ç­–ç•¥ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œåœ¨MAVEæ•°æ®é›†ä¸Šå®ç°äº†ä¸çœŸå®æ•°æ®ç›¸å½“çš„æ€§èƒ½è¡¨ç°ï¼Œä¸ºä½èµ„æºåœºæ™¯ä¸‹çš„äº§å“ä¿¡æ¯æå–æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç”µå•†äº§å“ä¿¡æ¯æå–å¯¹ç”µå•†æœåŠ¡è‡³å…³é‡è¦ï¼Œä½†è·å–é«˜è´¨é‡æ ‡æ³¨æ•°æ®é›†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºåœºæ™¯ä¸‹ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®çš„æ–¹æ³•æ¥å¢å¼ºè®­ç»ƒæ•°æ®é›†ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„åˆæˆç”µå•†äº§å“æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œé‡‡ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å—æ§ä¿®æ”¹æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ç§ç­–ç•¥ï¼šå±æ€§ä¿ç•™ä¿®æ”¹ã€å—æ§è´Ÿä¾‹ç”Ÿæˆå’Œç³»ç»Ÿå±æ€§ç§»é™¤ï¼Œä½¿ç”¨æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹é…åˆå±æ€§æ„ŸçŸ¥æç¤ºï¼Œåœ¨ä¿æŒäº§å“è¿è´¯æ€§çš„åŒæ—¶å¼ºåˆ¶æ‰§è¡Œåº—é“ºçº¦æŸã€‚</p>
<p><strong>Result:</strong> äººå·¥è¯„ä¼°2000ä¸ªåˆæˆäº§å“æ˜¾ç¤ºé«˜åº¦æœ‰æ•ˆæ€§ï¼Œ99.6%è¢«è¯„ä¸ºè‡ªç„¶ï¼Œ96.5%åŒ…å«æœ‰æ•ˆå±æ€§å€¼ï¼Œè¶…è¿‡90%æ˜¾ç¤ºä¸€è‡´çš„å±æ€§ä½¿ç”¨ï¼›åœ¨å…¬å¼€MAVEæ•°æ®é›†ä¸Šï¼Œåˆæˆæ•°æ®è¾¾åˆ°60.5%å‡†ç¡®ç‡ï¼Œä¸çœŸå®è®­ç»ƒæ•°æ®ï¼ˆ60.8%ï¼‰è¡¨ç°ç›¸å½“ï¼Œæ˜¾è‘—ä¼˜äº13.4%çš„é›¶æ ·æœ¬åŸºçº¿ï¼›ç»“åˆåˆæˆå’ŒçœŸå®æ•°æ®çš„æ··åˆé…ç½®è¿›ä¸€æ­¥å°†æ€§èƒ½æå‡è‡³68.8%å‡†ç¡®ç‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ¡†æ¶ä¸ºå¢å¼ºç”µå•†æ•°æ®é›†æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚ç”¨äºä½èµ„æºåœºæ™¯ï¼Œè¯æ˜äº†åˆæˆæ•°æ®åœ¨ä¿¡æ¯æå–ä»»åŠ¡ä¸­èƒ½å¤Ÿè¾¾åˆ°ä¸çœŸå®æ•°æ®ç›¸å½“çš„æ€§èƒ½ï¼Œæ··åˆé…ç½®çš„è¿›ä¸€æ­¥æ”¹è¿›è¡¨æ˜åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®çš„ç»“åˆå…·æœ‰ååŒæ•ˆåº”ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.</p>
<h3 id="35-teletables-a-benchmark-for-large-language-models-in-telecom-table-interpretation">[35] <a href="https://arxiv.org/abs/2601.04202">TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation</a></h3>
<p><em>Anas Ezzakri, Nicola Piovesan, Mohamed Sana, Antonio De Domenico, Fadhel Ayed, Haozhe Zhang</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†TeleTablesåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨ç”µä¿¡æŠ€æœ¯è§„èŒƒä¸­å¯¹è¡¨æ ¼çš„éšå¼çŸ¥è¯†å’Œæ˜¾å¼è§£é‡Šèƒ½åŠ›ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†3GPPæ ‡å‡†ä¸­å¯†é›†è¡¨æ ¼ä¿¡æ¯æ—¶çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†é¢†åŸŸä¸“ä¸šåŒ–å¾®è°ƒçš„å¿…è¦æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨ç”µä¿¡è¡Œä¸šåº”ç”¨ä¸­è¡¨ç°ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†3GPPæŠ€æœ¯è§„èŒƒæ—¶å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚ç ”ç©¶å‘ç°è¿™äº›æ ‡å‡†ä¸­å¯†é›†åŒ…å«å¤§é‡è¡¨æ ¼å‘ˆç°å…³é”®ä¿¡æ¯ï¼Œä½†æ¨¡å‹å¯¹è¿™äº›è¡¨æ ¼çš„çŸ¥è¯†å‚¨å¤‡å’Œè§£é‡Šèƒ½åŠ›å°šæœªå¾—åˆ°ç³»ç»Ÿè¯„ä¼°ï¼Œè¿™ä¸€ç ”ç©¶ç©ºç™½é˜»ç¢äº†LLMåœ¨ç”µä¿¡å·¥ç¨‹ä»»åŠ¡ä¸­çš„å¯é åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†TeleTablesåŸºå‡†ï¼Œé€šè¿‡åˆ›æ–°çš„å¤šé˜¶æ®µæ•°æ®ç”Ÿæˆæµç¨‹æ„å»ºè¯„ä¼°æ•°æ®é›†ã€‚è¯¥æ–¹æ³•ä»3GPPæ ‡å‡†ä¸­æå–è¡¨æ ¼ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å’Œæ¨ç†å¯¼å‘çš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¹¶éªŒè¯é—®é¢˜ï¼Œæœ€ç»ˆåˆ›å»ºäº†åŒ…å«500ä¸ªäººå·¥éªŒè¯çš„é—®ç­”å¯¹æ•°æ®é›†ï¼Œæ¯ä¸ªé—®é¢˜éƒ½å…³è”å¤šç§æ ¼å¼çš„å¯¹åº”è¡¨æ ¼ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¾ƒå°æ¨¡å‹ï¼ˆå‚æ•°å°‘äº100äº¿ï¼‰åœ¨3GPPçŸ¥è¯†å›å¿†å’Œè¡¨æ ¼è§£é‡Šæ–¹é¢å‡è¡¨ç°ä¸ä½³ï¼Œè¡¨æ˜å…¶é¢„è®­ç»ƒæ•°æ®ä¸­ç”µä¿¡æ ‡å‡†æš´éœ²ä¸è¶³ä¸”ç¼ºä¹å¤„ç†å¤æ‚æŠ€æœ¯ææ–™çš„å½’çº³åç½®ã€‚è¾ƒå¤§æ¨¡å‹åœ¨è¡¨æ ¼è§£é‡Šæ–¹é¢å±•ç°å‡ºæ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œä½†æ•´ä½“è¡¨ç°ä»æ˜¾ç¤ºé¢†åŸŸä¸“ä¸šåŒ–ä¸è¶³ã€‚</p>
<p><strong>Conclusion:</strong> TeleTablesåŸºå‡†æ­ç¤ºäº†å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨ç”µä¿¡é¢†åŸŸæŠ€æœ¯è§„èŒƒå¤„ç†ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹è¡¨æ ¼ä¿¡æ¯çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ä¸è¶³ã€‚ç ”ç©¶å¼ºè°ƒäº†é¢†åŸŸä¸“ä¸šåŒ–å¾®è°ƒå¯¹äºå¯é è§£é‡Šå’Œæ¨ç†ç”µä¿¡æ ‡å‡†çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥å¼€å‘ç”µä¿¡ä¸“ç”¨æ¨¡å‹æä¾›äº†è¯„ä¼°æ¡†æ¶å’Œæ–¹å‘æŒ‡å¯¼ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.</p>
<h3 id="36-frontalk-benchmarking-front-end-development-as-conversational-code-generation-with-multi-modal-feedback">[36] <a href="https://arxiv.org/abs/2601.04203">FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback</a></h3>
<p><em>Xueqing Wu, Zihan Xue, Da Yin, Shuyan Zhou, Kai-Wei Chang, Nanyun Peng, Yeming Wen</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FronTalkåŸºå‡†æµ‹è¯•ï¼Œç”¨äºå‰ç«¯ä»£ç ç”Ÿæˆç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨å¤šæ¨¡æ€åé¦ˆçš„å¯¹è¯å¼ä»£ç ç”Ÿæˆè¿™ä¸€ç‹¬ç‰¹äº¤äº’åŠ¨æ€ï¼Œå¹¶æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨ç‰¹å¾é—å¿˜å’Œè§†è§‰åé¦ˆç†è§£æ–¹é¢çš„ç³»ç»Ÿæ€§æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å‰ç«¯å¼€å‘ä¸­ï¼Œè‰å›¾ã€çº¿æ¡†å›¾å’Œæ ‡æ³¨æˆªå›¾ç­‰è§†è§‰å·¥ä»¶å¯¹äºä¼ è¾¾è®¾è®¡æ„å›¾è‡³å…³é‡è¦ï¼Œä½†å®ƒä»¬åœ¨å¤šè½®ä»£ç ç”Ÿæˆä¸­çš„ä½œç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œç‰¹åˆ«å…³æ³¨å‰ç«¯å¼€å‘ä»»åŠ¡ä¸­å¤šæ¨¡æ€åé¦ˆçš„å¯¹è¯å¼ä»£ç ç”Ÿæˆè¿™ä¸€ç‹¬ç‰¹äº¤äº’åŠ¨æ€ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†FronTalkåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«100ä¸ªä»æ–°é—»ã€é‡‘èå’Œè‰ºæœ¯ç­‰ä¸åŒé¢†åŸŸçœŸå®ç½‘ç«™æå–çš„å¤šè½®å¯¹è¯ã€‚æ¯ä¸ªå¯¹è¯è½®æ¬¡åŒæ—¶åŒ…å«æ–‡æœ¬æŒ‡ä»¤å’Œç­‰æ•ˆçš„è§†è§‰æŒ‡ä»¤ï¼Œä»£è¡¨ç›¸åŒçš„ç”¨æˆ·æ„å›¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†åŸºäºä»£ç†çš„è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨Webä»£ç†æ¨¡æ‹Ÿç”¨æˆ·æ¢ç´¢ç½‘ç«™ï¼Œä»è€ŒåŒæ—¶è¡¡é‡åŠŸèƒ½æ­£ç¡®æ€§å’Œç”¨æˆ·ä½“éªŒã€‚é’ˆå¯¹ç‰¹å¾é—å¿˜é—®é¢˜ï¼Œæå‡ºäº†AceCoderæ–¹æ³•ï¼Œé€šè¿‡è‡ªä¸»Webä»£ç†å¯¹æ¯ä¸ªè¿‡å»æŒ‡ä»¤çš„å®ç°è¿›è¡Œæ‰¹åˆ¤æ€§åˆ†æã€‚</p>
<p><strong>Result:</strong> å¯¹20ä¸ªæ¨¡å‹çš„è¯„ä¼°æ­ç¤ºäº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ¨¡å‹ä¼šè¦†ç›–å…ˆå‰å®ç°çš„åŠŸèƒ½å¯¼è‡´ä»»åŠ¡å¤±è´¥çš„ç‰¹å¾é—å¿˜é—®é¢˜ï¼Œä»¥åŠå¼€æºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§£é‡Šè§†è§‰åé¦ˆæ–¹é¢çš„æŒç»­å›°éš¾ã€‚æå‡ºçš„AceCoderåŸºçº¿æ–¹æ³•å°†ç‰¹å¾é—å¿˜ç‡æ˜¾è‘—é™ä½è‡³æ¥è¿‘é›¶ï¼Œå¹¶å°†æ€§èƒ½æå‡é«˜è¾¾9.3%ï¼ˆä»56.0%æå‡è‡³65.3%ï¼‰ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶ä¸ºå‰ç«¯å¼€å‘å’Œå¤šè½®å¤šæ¨¡æ€ä»£ç ç”Ÿæˆçš„ä¸€èˆ¬äº¤äº’åŠ¨æ€ç ”ç©¶æä¾›äº†åšå®åŸºç¡€ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç‰¹å¾é—å¿˜å’Œè§†è§‰åé¦ˆç†è§£æ˜¯å½“å‰æ¨¡å‹é¢ä¸´çš„é‡è¦æŒ‘æˆ˜ï¼Œéœ€è¦ç³»ç»Ÿæ€§çš„è§£å†³æ–¹æ¡ˆã€‚æå‡ºçš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·å’Œæ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk</p>
<h3 id="37-rigourate-quantifying-scientific-exaggeration-with-evidence-aligned-claim-evaluation">[37] <a href="https://arxiv.org/abs/2601.04350">RIGOURATE: Quantifying Scientific Exaggeration with Evidence-Aligned Claim Evaluation</a></h3>
<p><em>Joseph James, Chenghao Xiao, Yucheng Li, Nafise Sadat Moosavi, Chenghua Lin</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†RIGOURATEï¼Œä¸€ä¸ªä¸¤é˜¶æ®µå¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºä»è®ºæ–‡æ­£æ–‡ä¸­æ£€ç´¢æ”¯æŒè¯æ®å¹¶ä¸ºæ¯ä¸ªä¸»å¼ åˆ†é…å¤¸å¤§é™ˆè¿°åˆ†æ•°ï¼Œæ—¨åœ¨æ“ä½œåŒ–è¯æ®æ¯”ä¾‹æ€§å¹¶æ”¯æŒæ›´æ¸…æ™°ã€é€æ˜çš„ç§‘å­¦äº¤æµã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç§‘å­¦ä¸¥è°¨æ€§å¾€å¾€è¢«è¾¹ç¼˜åŒ–ï¼Œä½œè€…å€¾å‘äºåšå‡ºè¶…å‡ºå…¶ç ”ç©¶ç»“æœæ”¯æŒçš„å¤¸å¤§é™ˆè¿°ï¼Œè¿™å¯¼è‡´äº†ç§‘å­¦äº¤æµä¸­ç¼ºä¹é€æ˜åº¦å’Œè¯æ®æ¯”ä¾‹æ€§çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µå¤šæ¨¡æ€æ¡†æ¶ï¼ŒåŒ…æ‹¬ä»ICLRå’ŒNeurIPSè®ºæ–‡ä¸­æ„å»ºçš„è¶…è¿‡10Kä¸ªä¸»å¼ -è¯æ®æ•°æ®é›†ï¼Œä½¿ç”¨å…«ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶é€šè¿‡åŒè¡Œè¯„å®¡è¯„è®ºæ ¡å‡†å¤¸å¤§é™ˆè¿°åˆ†æ•°ã€‚æ¡†æ¶é‡‡ç”¨å¾®è°ƒçš„é‡æ–°æ’åºå™¨è¿›è¡Œè¯æ®æ£€ç´¢ï¼Œä»¥åŠå¾®è°ƒæ¨¡å‹æ¥é¢„æµ‹å¸¦æœ‰ç†ç”±çš„å¤¸å¤§é™ˆè¿°åˆ†æ•°ã€‚</p>
<p><strong>Result:</strong> ä¸å¼ºåŸºçº¿ç›¸æ¯”ï¼ŒRIGOURATEåœ¨è¯æ®æ£€ç´¢å’Œå¤¸å¤§é™ˆè¿°æ£€æµ‹æ–¹é¢å®ç°äº†æ”¹è¿›æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡äººç±»è¯„ä¼°éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†åœ¨ç§‘å­¦è®ºæ–‡ä¸­è¯†åˆ«å¤¸å¤§ä¸»å¼ çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹å·¥ä½œæ“ä½œåŒ–äº†è¯æ®æ¯”ä¾‹æ€§çš„æ¦‚å¿µï¼Œä¸ºæ›´æ¸…æ™°ã€é€æ˜çš„ç§‘å­¦äº¤æµæä¾›äº†æ”¯æŒå·¥å…·ã€‚è¯¥æ¡†æ¶æœ‰åŠ©äºå‡å°‘ç§‘å­¦è®ºæ–‡ä¸­çš„å¤¸å¤§é™ˆè¿°ï¼Œä¿ƒè¿›åŸºäºè¯æ®çš„ç§‘å­¦è®ºè¯å’Œæ›´ä¸¥è°¨çš„å­¦æœ¯äº¤æµå®è·µã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>Scientific rigour tends to be sidelined in favour of bold statements, leading authors to overstate claims beyond what their results support. We present RIGOURATE, a two-stage multimodal framework that retrieves supporting evidence from a paper's body and assigns each claim an overstatement score. The framework consists of a dataset of over 10K claim-evidence sets from ICLR and NeurIPS papers, annotated using eight LLMs, with overstatement scores calibrated using peer-review comments and validated through human evaluation. It employes a fine-tuned reranker for evidence retrieval and a fine-tuned model to predict overstatement scores with justification. Compared to strong baselines, RIGOURATE enables improved evidence retrieval and overstatement detection. Overall, our work operationalises evidential proportionality and supports clearer, more transparent scientific communication.</p>
<h3 id="38-identifying-good-and-bad-neurons-for-task-level-controllable-llms">[38] <a href="https://arxiv.org/abs/2601.04548">Identifying Good and Bad Neurons for Task-Level Controllable LLMs</a></h3>
<p><em>Wenjie Li, Guansong Pang, Hezhe Qiao, Debin Gao, David Lo</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºNeuronLLMï¼Œä¸€ç§åŸºäºåŠŸèƒ½æ‹®æŠ—åŸç†çš„ä»»åŠ¡çº§å¤§è¯­è¨€æ¨¡å‹ç†è§£æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ ä¿ƒè¿›ä»»åŠ¡å®Œæˆçš„"å¥½ç¥ç»å…ƒ"å’ŒæŠ‘åˆ¶ä»»åŠ¡çš„"åç¥ç»å…ƒ"ï¼Œå®ç°å¯¹LLMç¥ç»å…ƒåŠŸèƒ½çš„å…¨é¢å»ºæ¨¡ï¼Œå¹¶åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ç¥ç»å…ƒå¯è§£é‡Šæ€§çš„ç ”ç©¶å­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™ï¼šèƒ½åŠ›ç‰¹å®šæ–¹æ³•éš¾ä»¥é€‚åº”éœ€è¦å¤šç§èƒ½åŠ›åè°ƒçš„ä»»åŠ¡åœºæ™¯ï¼›ä»…å…³æ³¨ä¸ä»»åŠ¡æ­£ç›¸å…³çš„æ”¯æŒæ€§ç¥ç»å…ƒï¼Œå¿½ç•¥æŠ‘åˆ¶æ€§ç¥ç»å…ƒç­‰å…¶ä»–è§’è‰²ï¼›ç”±äºLLMçš„å¶ç„¶æ­£ç¡®è¡Œä¸ºå¯¼è‡´ç¥ç»å…ƒå½’å› é”™è¯¯ï¼Œè¿™äº›é—®é¢˜é˜»ç¢äº†å¯¹LLMåŠŸèƒ½æœºåˆ¶çš„æ·±å…¥ç†è§£ã€‚</p>
<p><strong>Method:</strong> NeuronLLMé‡‡ç”¨ç”Ÿç‰©å­¦åŠŸèƒ½æ‹®æŠ—åŸç†ï¼Œå°†ä»»åŠ¡æ€§èƒ½å»ºæ¨¡ä¸ºç”±ä¿ƒè¿›ä»»åŠ¡å®Œæˆçš„"å¥½ç¥ç»å…ƒ"å’ŒæŠ‘åˆ¶ä»»åŠ¡çš„"åç¥ç»å…ƒ"å…±åŒå†³å®šï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ åŒæ—¶å»ºæ¨¡è¿™ä¸¤ç§å¯¹ç«‹è§’è‰²ï¼Œå¹¶åˆ©ç”¨å¢å¼ºé—®é¢˜é›†æ¥å‡è½»LLMä¸­çš„å¶ç„¶æ­£ç¡®è¡Œä¸ºï¼Œå®ç°å¯¹ç¥ç»å…ƒåŠŸèƒ½çš„å…¨é¢è¯†åˆ«å’Œåˆ†æã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸åŒè§„æ¨¡å’Œå®¶æ—çš„å¤§è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œç»¼åˆå®éªŒè¡¨æ˜ï¼ŒNeuronLLMåœ¨å››ä¸ªNLPä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨ç¥ç»å…ƒè¯†åˆ«æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œä¸ºç†è§£LLMçš„åŠŸèƒ½ç»„ç»‡æä¾›äº†æ–°çš„å®è¯æ”¯æŒã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†ä»»åŠ¡æ€§èƒ½ç”±å¯¹ç«‹ç¥ç»å…ƒè§’è‰²å…±åŒå†³å®šçš„é‡è¦æœºåˆ¶ï¼Œæå‡ºçš„åŠŸèƒ½æ‹®æŠ—æ¡†æ¶ä¸ºLLMç†è§£æä¾›äº†æ–°è§†è§’ï¼Œæœ‰åŠ©äºæ›´å‡†ç¡®åœ°è¯†åˆ«å’Œè°ƒæ§æ¨¡å‹å†…éƒ¨åŠŸèƒ½å•å…ƒï¼Œå¯¹æ¨¡å‹å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ç ”ç©¶å…·æœ‰é‡è¦å¯ç¤ºã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.</p>
<h3 id="39-aligning-text-code-and-vision-a-multi-objective-reinforcement-learning-framework-for-text-to-visualization">[39] <a href="https://arxiv.org/abs/2601.04582">Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization</a></h3>
<p><em>Mizanur Rahman, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Shafiq Joty, Enamul Hoque</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºRL-Text2Visï¼Œé¦–ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–‡æœ¬åˆ°å¯è§†åŒ–ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å¤šç›®æ ‡å¥–åŠ±å‡½æ•°è”åˆä¼˜åŒ–æ–‡æœ¬å‡†ç¡®æ€§ã€ä»£ç æœ‰æ•ˆæ€§å’Œå¯è§†åŒ–è´¨é‡ï¼Œæ˜¾è‘—æå‡äº†å›¾è¡¨ç”Ÿæˆè´¨é‡ä¸ä»£ç æ‰§è¡ŒæˆåŠŸç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°å¯è§†åŒ–ç³»ç»Ÿä¸­ï¼Œé—­æºå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å›¾è¡¨å¸¸ç¼ºä¹è¯­ä¹‰å¯¹é½å’Œæ¸…æ™°åº¦ï¼Œè€Œå¼€æºæ¨¡å‹åˆ™é¢‘ç¹äº§ç”Ÿä¸å¯æ‰§è¡Œæˆ–è§†è§‰è´¨é‡å·®çš„è¾“å‡ºã€‚å°½ç®¡ç›‘ç£å¾®è°ƒèƒ½æ”¹å–„ä»£ç å¯æ‰§è¡Œæ€§ï¼Œä½†æ— æ³•æå‡æ•´ä½“å¯è§†åŒ–è´¨é‡ï¼Œå› ä¸ºä¼ ç»ŸSFTæŸå¤±æ— æ³•æ•æ‰æ‰§è¡Œååé¦ˆã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºRL-Text2Viså¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒåŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œé‡‡ç”¨æ–°é¢–çš„å¤šç›®æ ‡å¥–åŠ±å‡½æ•°ï¼Œè”åˆä¼˜åŒ–æ–‡æœ¬å‡†ç¡®æ€§ã€ä»£ç æœ‰æ•ˆæ€§å’Œå¯è§†åŒ–è´¨é‡ï¼Œåˆ©ç”¨æ‰§è¡Œååé¦ˆè¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•åœ¨Qwen2.5æ¨¡å‹ï¼ˆ7Bå’Œ14Bï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸“é—¨é’ˆå¯¹æ–‡æœ¬åˆ°å¯è§†åŒ–ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> RL-Text2Visåœ¨Text2VisåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸æ¯”GPT-4oå®ç°äº†22%çš„å›¾è¡¨è´¨é‡ç›¸å¯¹æå‡ï¼Œä»£ç æ‰§è¡ŒæˆåŠŸç‡ä»é›¶æ ·æœ¬åŸºçº¿çš„78%æå‡è‡³97%ã€‚æ¨¡å‹æ˜¾è‘—ä¼˜äºå¼ºé›¶æ ·æœ¬å’Œç›‘ç£åŸºçº¿ï¼Œå¹¶åœ¨VIS-Evalå’ŒNVBenchç­‰åŸŸå¤–æ•°æ®é›†ä¸Šå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ç¡®ç«‹äº†GRPOä½œä¸ºå¯è§†åŒ–ç”Ÿæˆä¸­ç»“æ„åŒ–å¤šæ¨¡æ€æ¨ç†çš„æœ‰æ•ˆç­–ç•¥ï¼Œè¯æ˜äº†å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨æå‡æ–‡æœ¬åˆ°å¯è§†åŒ–ç³»ç»Ÿè´¨é‡æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œä¸ºå¤æ‚ç»“æ„åŒ–è¾“å‡ºä»»åŠ¡æä¾›äº†æ–°çš„è®­ç»ƒèŒƒå¼ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.</p>
<h3 id="40-when-more-words-say-less-decoupling-length-and-specificity-in-image-description-evaluation">[40] <a href="https://arxiv.org/abs/2601.04609">When More Words Say Less: Decoupling Length and Specificity in Image Description Evaluation</a></h3>
<p><em>Rhea Kapur, Robert Hawkins, Elisa Kreiss</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æŒ‡å‡ºå½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æè¿°ç‰¹å¼‚æ€§å¸¸ä¸é•¿åº¦æ··æ·†çš„é—®é¢˜ï¼Œæå‡ºç‰¹å¼‚æ€§åº”ç›¸å¯¹äºå¯¹æ¯”é›†æ¥å®šä¹‰ï¼Œå¹¶é€šè¿‡æ„å»ºæ§åˆ¶é•¿åº¦çš„æ•°æ®é›†éªŒè¯äº†äººä»¬æ›´åå¥½ç‰¹å¼‚æ€§æè¿°è€Œéå†—é•¿æè¿°ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ç³»ç»Ÿä¸­ï¼Œæè¿°çš„ç‰¹å¼‚æ€§å¸¸è¢«é”™è¯¯åœ°ä¸æè¿°é•¿åº¦æ··ä¸ºä¸€è°ˆï¼Œå¯¼è‡´æè¿°å¯èƒ½å†—é•¿ä½†ä¿¡æ¯ç©ºæ´ï¼Œæˆ–ç®€æ´ä½†ä¿¡æ¯å¯†é›†ï¼Œè¯¥ç ”ç©¶æ—¨åœ¨å°†è¿™ä¸¤ä¸ªæ¦‚å¿µè§£è€¦å¹¶æ˜ç¡®å®šä¹‰æè¿°ç‰¹å¼‚æ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å°†æè¿°ç‰¹å¼‚æ€§å®šä¹‰ä¸ºç›¸å¯¹äºå¯¹æ¯”é›†çš„æ¦‚å¿µï¼Œå³æè¿°èƒ½æ›´å¥½åœ°åŒºåˆ†ç›®æ ‡å›¾åƒä¸å…¶ä»–å¯èƒ½å›¾åƒçš„ç¨‹åº¦ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ§åˆ¶æè¿°é•¿åº¦åŒæ—¶å˜åŒ–ä¿¡æ¯å†…å®¹çš„æ•°æ®é›†ï¼Œé€šè¿‡äººç±»åå¥½å®éªŒéªŒè¯ç‰¹å¼‚æ€§è¯„ä¼°æ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜äººä»¬ç¡®å®æ›´åå¥½ç‰¹å¼‚æ€§æè¿°è€Œéå†—é•¿æè¿°ï¼Œä¸”ä»…æ§åˆ¶é•¿åº¦ä¸è¶³ä»¥è§£é‡Šç‰¹å¼‚æ€§å·®å¼‚ï¼Œé•¿åº¦é¢„ç®—çš„åˆ†é…æ–¹å¼å¯¹æè¿°è´¨é‡æœ‰æ˜¾è‘—å½±å“ï¼Œæ”¯æŒç›´æ¥ä¼˜å…ˆè€ƒè™‘ç‰¹å¼‚æ€§è€Œéå†—é•¿çš„è¯„ä¼°æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­åŒºåˆ†æè¿°ç‰¹å¼‚æ€§ä¸é•¿åº¦çš„é‡è¦æ€§ï¼Œæå‡ºäº†åŸºäºå¯¹æ¯”é›†çš„ç‰¹å¼‚æ€§å®šä¹‰æ¡†æ¶ï¼Œä¸ºå¼€å‘æ›´ç²¾ç¡®çš„å›¾åƒæè¿°è¯„ä¼°æŒ‡æ ‡æä¾›äº†ç†è®ºåŸºç¡€ï¼Œå¹¶å»ºè®®æœªæ¥å·¥ä½œåº”ç›´æ¥ä¼˜åŒ–æè¿°çš„ä¿¡æ¯å¯†åº¦è€Œéå•çº¯æ§åˆ¶é•¿åº¦ã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>Vision-language models (VLMs) are increasingly used to make visual content accessible via text-based descriptions. In current systems, however, description specificity is often conflated with their length. We argue that these two concepts must be disentangled: descriptions can be concise yet dense with information, or lengthy yet vacuous. We define specificity relative to a contrast set, where a description is more specific to the extent that it picks out the target image better than other possible images. We construct a dataset that controls for length while varying information content, and validate that people reliably prefer more specific descriptions regardless of length. We find that controlling for length alone cannot account for differences in specificity: how the length budget is allocated makes a difference. These results support evaluation approaches that directly prioritize specificity over verbosity.</p>
<h3 id="41-safe-in-the-future-dangerous-in-the-past-dissecting-temporal-and-linguistic-vulnerabilities-in-llms">[41] <a href="https://arxiv.org/abs/2512.24556">Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs</a></h3>
<p><em>Muhammad Abdullahi Said, Muhammad Sammani Sani</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿå®¡è®¡å‘ç°ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€å®‰å…¨å¯¹é½ä¸­å­˜åœ¨å¤æ‚çš„å¹²æ‰°æœºåˆ¶è€Œéç®€å•çš„æ€§èƒ½é€€åŒ–ï¼Œæ­ç¤ºäº†å®‰å…¨æ€§èƒ½å—è¯­è¨€å’Œæ—¶æ€æ¡†æ¶äº¤äº’å½±å“çš„åŠ¨æ€ç‰¹æ€§ï¼Œå¹¶æå‡ºäº†ä¸å˜å¯¹é½çš„æ–°èŒƒå¼ã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç ”ç©¶å­˜åœ¨ä¸€ä¸ªå±é™©ç›²åŒºï¼Œå³å‡è®¾å®‰å…¨å¯¹é½èƒ½å¤Ÿä»è‹±è¯­é›¶æ ·æœ¬è¿ç§»åˆ°å…¶ä»–è¯­è¨€ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹æ­£è¢«é›†æˆåˆ°å…³é”®å…¨çƒåŸºç¡€è®¾æ–½ä¸­ï¼Œè¿™ç§å‡è®¾å¯èƒ½å¯¼è‡´å…¨çƒå—æ–¹ç”¨æˆ·é¢ä¸´æœ¬åœ°åŒ–å±å®³é£é™©ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨ç³»ç»Ÿå®¡è®¡æ–¹æ³•ï¼Œä½¿ç”¨åŸºäºè¥¿éå¨èƒåœºæ™¯æ„å»ºçš„æ–°å‹å¯¹æŠ—æ•°æ®é›†HausaSafetyï¼Œå¯¹GPT-5.1ã€Gemini 3 Proå’ŒClaude 4.5 Opusä¸‰ä¸ªæœ€å…ˆè¿›æ¨¡å‹è¿›è¡Œ1,440æ¬¡è¯„ä¼°ï¼Œé‡‡ç”¨2Ã—4å› å­è®¾è®¡æ£€éªŒè¯­è¨€ä¸æ—¶æ€æ¡†æ¶çš„éçº¿æ€§äº¤äº’ä½œç”¨ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶ç»“æœæŒ‘æˆ˜äº†å¤šè¯­è¨€å®‰å…¨å·®è·çš„ç®€å•å™äº‹ï¼Œå‘ç°å¤æ‚å¹²æ‰°æœºåˆ¶å†³å®šå®‰å…¨æ€§èƒ½ï¼ŒClaude 4.5 Opusåœ¨è±ªè¨è¯­ä¸­å®‰å…¨æ€§æ˜¾è‘—é«˜äºè‹±è¯­ï¼ŒåŒæ—¶æ¨¡å‹åœ¨æ—¶æ€æ¨ç†æ–¹é¢å­˜åœ¨ç¾éš¾æ€§å¤±è´¥ï¼Œè¿‡å»æ—¶æ¡†æ¶ç»•è¿‡é˜²å¾¡è€Œæœªæ¥æ—¶åœºæ™¯è§¦å‘è¿‡åº¦ä¿å®ˆæ‹’ç»ï¼Œæœ€å®‰å…¨ä¸æœ€è„†å¼±é…ç½®é—´å­˜åœ¨9.2å€å·®å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å½“å‰æ¨¡å‹ä¾èµ–è¡¨é¢å¯å‘å¼è€Œéç¨³å¥è¯­ä¹‰ç†è§£ï¼Œå½¢æˆå®‰å…¨ç›²ç‚¹ä½¿å…¨çƒå—æ–¹ç”¨æˆ·é¢ä¸´æœ¬åœ°åŒ–å±å®³ï¼Œéœ€è¦å‘ä¸å˜å¯¹é½èŒƒå¼è½¬å˜ä»¥ç¡®ä¿è·¨è¯­è¨€å’Œæ—¶æ€å˜åŒ–çš„å®‰å…¨ç¨³å®šæ€§ï¼Œå®‰å…¨ä¸æ˜¯å›ºå®šå±æ€§è€Œæ˜¯ä¸Šä¸‹æ–‡ä¾èµ–çŠ¶æ€ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the narrative of the multilingual safety gap. Instead of a simple degradation in low-resource settings, we identified a complex interference mechanism in which safety is determined by the intersection of variables. Although the models exhibited a reverse linguistic vulnerability with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal, they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.</p>
<h3 id="42-see-explain-and-intervene-a-few-shot-multimodal-agent-framework-for-hateful-meme-moderation">[42] <a href="https://arxiv.org/abs/2601.04692">See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation</a></h3>
<p><em>Naquee Rizwan, Subhankar Swain, Paramananda Bhaskar, Gagan Aryan, Shehryaar Shah Khan, Animesh Mukherjee</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼AIçš„é€šç”¨ä»‡æ¨è¡¨æƒ…åŒ…æ£€æµ‹ã€è§£é‡Šä¸å¹²é¢„æ¡†æ¶ï¼Œé¦–æ¬¡åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹å®ç°äº†ä¸‰ä»»åŠ¡çš„ç»Ÿä¸€å¤„ç†ï¼Œå¹¶åˆ©ç”¨ä»»åŠ¡ç‰¹å®šç”Ÿæˆå¼å¤šæ¨¡æ€ä»£ç†å’Œå¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„å°‘æ ·æœ¬é€‚åº”èƒ½åŠ›æ¥åº”å¯¹ä¸åŒç±»å‹è¡¨æƒ…åŒ…ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ä»‡æ¨è¡¨æƒ…åŒ…ç ”ç©¶å­˜åœ¨ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šæ£€æµ‹ã€è§£é‡Šå’Œå¹²é¢„é€šå¸¸è¢«åˆ†å¼€ç ”ç©¶ï¼Œä¸ç¬¦åˆç°å®åœºæ™¯éœ€æ±‚ï¼›æ„å»ºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†æˆæœ¬è¿‡é«˜ï¼›ç¼ºä¹åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹é€šç”¨çš„ä»‡æ¨è¡¨æƒ…åŒ…æ²»ç†æ–¹æ¡ˆã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™äº›ç ”ç©¶ç©ºç™½ï¼Œå®ç°ä¸‰ä»»åŠ¡çš„ç»Ÿä¸€å¤„ç†å¹¶åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹ä¿æŒæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä»»åŠ¡ç‰¹å®šçš„ç”Ÿæˆå¼å¤šæ¨¡æ€ä»£ç†å’Œå¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„å°‘æ ·æœ¬é€‚åº”èƒ½åŠ›æ¥å¤„ç†ä¸åŒç±»å‹çš„è¡¨æƒ…åŒ…ã€‚è¯¥æ¡†æ¶åœ¨ç”Ÿæˆå¼AIæ¨¡å‹åŸºç¡€ä¸Šæ„å»ºå¤šç§ç­–ç•¥ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†æ£€æµ‹ã€è§£é‡Šå’Œå¹²é¢„ä¸‰ä¸ªäº’è¡¥ä»»åŠ¡ï¼Œç‰¹åˆ«é’ˆå¯¹æ•°æ®æœ‰é™çš„å®é™…åº”ç”¨åœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> è¯¥ç ”ç©¶é¦–æ¬¡å®ç°äº†åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹å¯¹ä»‡æ¨è¡¨æƒ…åŒ…çš„é€šç”¨æ²»ç†ï¼Œæ¡†æ¶å±•ç¤ºäº†åœ¨å®é™…ç”Ÿäº§åœºæ™¯ä¸­éƒ¨ç½²çš„å¼ºå¤§æ½œåŠ›ã€‚é€šè¿‡æ•´åˆæ£€æµ‹ã€è§£é‡Šå’Œå¹²é¢„åŠŸèƒ½ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ›´å…¨é¢åœ°åº”å¯¹ä»‡æ¨è¡¨æƒ…åŒ…é—®é¢˜ï¼Œç›¸æ¯”ä¼ ç»Ÿåˆ†ç¦»å¤„ç†æ–¹æ³•æ›´ç¬¦åˆç°å®éœ€æ±‚ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶ä¸ºä»‡æ¨è¡¨æƒ…åŒ…æ²»ç†æä¾›äº†é¦–ä¸ªåœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå°†æ£€æµ‹ã€è§£é‡Šå’Œå¹²é¢„ä¸‰ä¸ªä»»åŠ¡ç»Ÿä¸€å¤„ç†å…·æœ‰é‡è¦å®è·µæ„ä¹‰ã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†ç”Ÿæˆå¼AIåœ¨å¤šæ¨¡æ€å†…å®¹æ²»ç†ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºå®é™…ç”Ÿäº§ç¯å¢ƒä¸­çš„å†…å®¹å®¡æ ¸ç³»ç»Ÿæä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.</p>
<h3 id="43-qwen3-vl-embedding-and-qwen3-vl-reranker-a-unified-framework-for-state-of-the-art-multimodal-retrieval-and-ranking">[43] <a href="https://arxiv.org/abs/2601.04720">Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking</a></h3>
<p><em>Mingxin Li, Yanzhao Zhang, Dingkun Long, Keqin Chen, Sibo Song, Shuai Bai, Zhibo Yang, Pengjun Xie, An Yang, Dayiheng Liu, Jingren Zhou, Junyang Lin</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ä»‹ç»äº†Qwen3-VL-Embeddingå’ŒQwen3-VL-Rerankeræ¨¡å‹ç³»åˆ—ï¼Œå®ƒä»¬åŸºäºQwen3-VLåŸºç¡€æ¨¡å‹æ„å»ºï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒèŒƒå¼å®ç°äº†è·¨æ–‡æœ¬ã€å›¾åƒã€æ–‡æ¡£å›¾åƒå’Œè§†é¢‘çš„ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ ï¼Œåœ¨å¤šæ¨¡æ€åµŒå…¥åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æœç´¢ä¸­ä¸åŒæ¨¡æ€æ•°æ®ï¼ˆæ–‡æœ¬ã€å›¾åƒã€æ–‡æ¡£å›¾åƒã€è§†é¢‘ï¼‰çš„ç»Ÿä¸€è¡¨ç¤ºé—®é¢˜ï¼Œæ„å»ºç«¯åˆ°ç«¯çš„é«˜ç²¾åº¦å¤šæ¨¡æ€æœç´¢ç®¡é“ï¼Œä»¥å…‹æœç°æœ‰æ–¹æ³•åœ¨è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½å’Œç»†ç²’åº¦ç›¸å…³æ€§è¯„ä¼°æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> Qwen3-VL-Embeddingé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒèŒƒå¼ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡å¯¹æ¯”é¢„è®­ç»ƒå’Œé‡æ’åºæ¨¡å‹è’¸é¦ï¼Œæ”¯æŒMatryoshkaè¡¨ç¤ºå­¦ä¹ ä»¥å®ç°çµæ´»çš„åµŒå…¥ç»´åº¦ï¼Œå¤„ç†é•¿è¾¾32kä»¤ç‰Œçš„è¾“å…¥ï¼›Qwen3-VL-Rerankeré‡‡ç”¨äº¤å‰ç¼–ç å™¨æ¶æ„ï¼Œåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¿›è¡ŒæŸ¥è¯¢-æ–‡æ¡£å¯¹çš„ç»†ç²’åº¦ç›¸å…³æ€§ä¼°è®¡ï¼›ä¸¤ä¸ªç³»åˆ—å‡ç»§æ‰¿Qwen3-VLçš„å¤šè¯­è¨€èƒ½åŠ›ï¼Œæ”¯æŒ30å¤šç§è¯­è¨€ï¼Œå¹¶æä¾›2Bå’Œ8Bå‚æ•°è§„æ¨¡ä»¥é€‚åº”ä¸åŒéƒ¨ç½²éœ€æ±‚ã€‚</p>
<p><strong>Result:</strong> Qwen3-VL-Embeddingç³»åˆ—åœ¨å¤šæ¨¡æ€åµŒå…¥è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…¶ä¸­Qwen3-VL-Embedding-8Båœ¨MMEB-V2åŸºå‡†ä¸Šè·å¾—77.8çš„ç»¼åˆå¾—åˆ†ï¼Œåœ¨æ‰€æœ‰æ¨¡å‹ä¸­æ’åç¬¬ä¸€ï¼ˆæˆªè‡³2025å¹´1æœˆ8æ—¥ï¼‰ï¼›æ¨¡å‹åœ¨å›¾åƒ-æ–‡æœ¬æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œè§†é¢‘-æ–‡æœ¬åŒ¹é…ç­‰å¤šç§å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¤šé˜¶æ®µè®­ç»ƒèŒƒå¼å’Œç»Ÿä¸€è¡¨ç¤ºç©ºé—´åœ¨å¤šæ¨¡æ€åµŒå…¥å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®é™…å¤šæ¨¡æ€æœç´¢åº”ç”¨æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼›Matryoshkaè¡¨ç¤ºå­¦ä¹ å’Œçµæ´»å‚æ•°è§„æ¨¡è®¾è®¡å¢å¼ºäº†æ¨¡å‹çš„å®ç”¨æ€§å’Œéƒ¨ç½²çµæ´»æ€§ï¼Œä¸ºå¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\textbf{2B}$ and $\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.</p>
<h3 id="44-am3safety-towards-data-efficient-alignment-of-multi-modal-multi-turn-safety-for-mllms">[44] <a href="https://arxiv.org/abs/2601.04736">AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs</a></h3>
<p><em>Han Zhu, Jiale Chen, Chengkun Cai, Shengjie Sun, Haoran Li, Yujin Zhou, Chi-Min Chan, Pengcheng Wen, Lei Li, Sirui Han, Yike Guo</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†InterSafe-Vå¤šæ¨¡æ€å¯¹è¯å®‰å…¨æ•°æ®é›†å’ŒAMÂ³Safetyæ¡†æ¶ï¼Œé€šè¿‡å†·å¯åŠ¨æ‹’ç»é˜¶æ®µå’ŒåŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„å¯¹è¯çº§å¾®è°ƒï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­çš„å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨äº¤äº’åº”ç”¨ä¸­é¢ä¸´ä¸¥é‡çš„å®‰å…¨æ¼æ´ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè½®å¤šæ¨¡æ€åœºæ™¯ä¸­ï¼Œæœ‰å®³æ„å›¾å¯èƒ½é€æ­¥é‡å»ºè€Œå®‰å…¨åè®®é€æ¸å¤±æ•ˆã€‚ç°æœ‰çš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸»è¦é’ˆå¯¹å•è½®è§†è§‰é—®ç­”ä»»åŠ¡ï¼Œä¸”éœ€è¦æ˜‚è´µçš„äººå·¥åå¥½æ ‡æ³¨ï¼Œé™åˆ¶äº†å…¶åœ¨å¯¹è¯åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†InterSafe-Vå¼€æºå¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†ï¼ŒåŒ…å«11,270ä¸ªå¯¹è¯å’Œ500ä¸ªä¸“é—¨è®¾è®¡çš„æ‹’ç»è§†è§‰é—®ç­”æ ·æœ¬ï¼Œé€šè¿‡æ¨¡å‹é—´äº¤äº’æ„å»ºä»¥æ›´å‡†ç¡®åæ˜ çœŸå®åœºæ™¯ã€‚åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†AMÂ³Safetyæ¡†æ¶ï¼Œç»“åˆå†·å¯åŠ¨æ‹’ç»é˜¶æ®µå’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œä½¿ç”¨åŸºäºæ•´ä¸ªå¯¹è¯çš„è½®æ¬¡æ„ŸçŸ¥åŒç›®æ ‡å¥–åŠ±è¿›è¡Œå¾®è°ƒã€‚</p>
<p><strong>Result:</strong> åœ¨Qwen2.5-VL-7B-Instructå’ŒLLaVA-NeXT-7Bæ¨¡å‹ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œæ”»å‡»æˆåŠŸç‡é™ä½è¶…è¿‡10%ï¼Œåœ¨å¤šæ¨¡æ€å¤šè½®å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ— å®³ç»´åº¦æå‡è‡³å°‘8%ï¼Œæœ‰å¸®åŠ©ç»´åº¦æå‡è¶…è¿‡13%ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡ä¸“é—¨æ„å»ºçš„å¤šè½®å¯¹è¯å®‰å…¨æ•°æ®é›†å’Œå¯¹è¯çº§å¾®è°ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨äº¤äº’åœºæ™¯ä¸­çš„å®‰å…¨æ€§ï¼Œä¸ºå¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿçš„å®‰å…¨å¯¹é½æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶è¯æ˜äº†åœ¨ä¿æŒæ¨¡å‹é€šç”¨èƒ½åŠ›çš„å‰æä¸‹å®ç°å®‰å…¨æ€§æå‡çš„å¯è¡Œæ€§ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\% in harmless dimension and over 13\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.</p>
<h3 id="45-when-ai-settles-down-late-stage-stability-as-a-signature-of-ai-generated-text-detection">[45] <a href="https://arxiv.org/abs/2601.04833">When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection</a></h3>
<p><em>Ke Sun, Guangsheng Bao, Han Cui, Yue Zhang</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æ­ç¤ºäº†AIç”Ÿæˆæ–‡æœ¬çš„æ™šæœŸæ³¢åŠ¨è¡°å‡ç°è±¡ï¼Œå¹¶æå‡ºåŸºäºæ™šæœŸç»Ÿè®¡ç‰¹å¾çš„é›¶æ ·æœ¬æ£€æµ‹æ–¹æ³•ï¼Œåœ¨ä¸ä¾èµ–æ‰°åŠ¨é‡‡æ ·æˆ–é¢å¤–æ¨¡å‹è®¿é—®çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ£€æµ‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„é›¶æ ·æœ¬AIæ–‡æœ¬æ£€æµ‹æ–¹æ³•é€šå¸¸åœ¨æ•´ä¸ªåºåˆ—ä¸Šèšåˆè¯å…ƒçº§ç»Ÿè®¡ä¿¡æ¯ï¼Œå¿½ç•¥äº†è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ—¶é—´åŠ¨æ€ç‰¹æ€§ï¼Œè¿™é™åˆ¶äº†æ£€æµ‹æ€§èƒ½çš„æå‡ç©ºé—´ã€‚</p>
<p><strong>Method:</strong> é€šè¿‡åˆ†æè¶…è¿‡12ä¸‡ä¸ªæ–‡æœ¬æ ·æœ¬ï¼Œå‘ç°äº†æ™šæœŸæ³¢åŠ¨è¡°å‡ç°è±¡ï¼Œå¹¶åŸºäºæ­¤æå‡ºäº†ä¸¤ä¸ªç®€å•ç‰¹å¾ï¼šå¯¼æ•°ç¦»æ•£åº¦å’Œå±€éƒ¨æ³¢åŠ¨æ€§ï¼Œè¿™äº›ç‰¹å¾ä»…ä»æ™šæœŸç»Ÿè®¡ä¿¡æ¯è®¡ç®—å¾—å‡ºï¼Œæ— éœ€æ‰°åŠ¨é‡‡æ ·æˆ–é¢å¤–æ¨¡å‹è®¿é—®ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨EvoBenchå’ŒMAGEåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒAIç”Ÿæˆæ–‡æœ¬åœ¨åºåˆ—ååŠéƒ¨åˆ†è¡¨ç°å‡º24-32%çš„æ›´ä½æ³¢åŠ¨æ€§ï¼Œå¹¶ä¸”ä¸ç°æœ‰å…¨å±€æ–¹æ³•å±•ç°å‡ºå¼ºå¤§çš„äº’è¡¥æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†AIç”Ÿæˆæ–‡æœ¬çš„æ—¶é—´åŠ¨æ€ç‰¹æ€§å·®å¼‚ï¼Œè¯æ˜äº†æ™šæœŸç»Ÿè®¡ç‰¹å¾çš„æ£€æµ‹ä»·å€¼ï¼Œä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„é›¶æ ·æœ¬æ£€æµ‹æ–¹æ³•æä¾›äº†æ–°æ–¹å‘ï¼ŒåŒæ—¶å±•ç¤ºäº†ç®€å•ç‰¹å¾ä¸å¤æ‚æ–¹æ³•çš„äº’è¡¥æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>Zero-shot detection methods for AI-generated text typically aggregate token-level statistics across entire sequences, overlooking the temporal dynamics inherent to autoregressive generation. We analyze over 120k text samples and reveal Late-Stage Volatility Decay: AI-generated text exhibits rapidly stabilizing log probability fluctuations as generation progresses, while human writing maintains higher variability throughout. This divergence peaks in the second half of sequences, where AI-generated text shows 24--32\% lower volatility. Based on this finding, we propose two simple features: Derivative Dispersion and Local Volatility, which computed exclusively from late-stage statistics. Without perturbation sampling or additional model access, our method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.</p>
<h3 id="46-misspans-fine-grained-false-span-identification-in-cross-domain-fake-news">[46] <a href="https://arxiv.org/abs/2601.04857">MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News</a></h3>
<p><em>Zhiwei Liu, Paul Thompson, Jiaqi Rong, Baojie Qu, Runteng Guo, Min Peng, Qianqian Xie, Sophia Ananiadou</em></p>
<h4 id="tldr_45">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MisSpansï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºç»†ç²’åº¦è™šå‡ä¿¡æ¯æ£€æµ‹ä¸åˆ†æçš„å¤šé¢†åŸŸã€äººå·¥æ ‡æ³¨çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«ä¸‰ä¸ªäº’è¡¥ä»»åŠ¡ï¼šè™šå‡ç‰‡æ®µè¯†åˆ«ã€è™šå‡ç±»å‹åˆ†ç±»å’ŒåŸºäºç‰‡æ®µçš„è§£é‡Šç”Ÿæˆï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¥å­çº§åˆ«è™šå‡ä¿¡æ¯å®šä½å’Œè§£é‡Šæ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_45">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è™šå‡ä¿¡æ¯æ£€æµ‹åŸºå‡†å’Œæ–¹æ³•é€šå¸¸åœ¨æ•´ä¸ªå£°æ˜æˆ–æ®µè½çº§åˆ«ä½¿ç”¨ç²—ç²’åº¦çš„äºŒå…ƒæ ‡ç­¾è¿›è¡Œè¯„ä¼°ï¼Œè¿™æ©ç›–äº†çœŸå®å’Œè™šå‡ç»†èŠ‚ç»å¸¸åœ¨å•ä¸ªå¥å­ä¸­å…±å­˜çš„ç°è±¡ã€‚è¿™äº›ç®€åŒ–é™åˆ¶äº†å¯è§£é‡Šæ€§ï¼šå…¨å±€è§£é‡Šæ— æ³•è¯†åˆ«å“ªäº›å…·ä½“ç‰‡æ®µå…·æœ‰è¯¯å¯¼æ€§ï¼Œä¹Ÿæ— æ³•åŒºåˆ†ç»†èŠ‚è™šå‡çš„æ–¹å¼ï¼ˆä¾‹å¦‚æ‰­æ›²ä¸æé€ ï¼‰ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†MisSpansåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«é…å¯¹çš„çœŸå®å’Œè™šå‡æ–°é—»æ•…äº‹ï¼Œå®šä¹‰äº†ä¸‰ä¸ªäº’è¡¥ä»»åŠ¡ï¼šMisSpansIdentityç”¨äºåœ¨å¥å­å†…ç²¾ç¡®å®šä½è™šå‡ç‰‡æ®µï¼ŒMisSpansTypeç”¨äºæŒ‰è™šå‡ä¿¡æ¯ç±»å‹å¯¹è™šå‡ç‰‡æ®µè¿›è¡Œåˆ†ç±»ï¼ŒMisSpansExplanationç”¨äºåŸºäºå·²è¯†åˆ«ç‰‡æ®µæä¾›ç†æ€§è§£é‡Šã€‚ä¸“å®¶æ ‡æ³¨è€…éµå¾ªæ ‡å‡†åŒ–æŒ‡å—å’Œä¸€è‡´æ€§æ£€æŸ¥ï¼Œå®ç°äº†è¾ƒé«˜çš„æ ‡æ³¨è€…é—´ä¸€è‡´æ€§ã€‚ç ”ç©¶è¯„ä¼°äº†15ä¸ªä»£è¡¨æ€§å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬æ¨ç†å¢å¼ºå’Œéæ¨ç†å˜ä½“ï¼Œåœ¨é›¶æ ·æœ¬å’Œå•æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œç»†ç²’åº¦è™šå‡ä¿¡æ¯è¯†åˆ«å’Œåˆ†æå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦æ·±å…¥ç†è§£å¤šä¸ªäº¤äº’å› ç´ å¦‚ä½•å½±å“æ€§èƒ½ï¼ŒåŒ…æ‹¬æ¨¡å‹å¤§å°å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥åŠé¢†åŸŸç‰¹å®šçš„æ–‡æœ¬ç‰¹å¾ã€‚æ ‡æ³¨è¿‡ç¨‹å®ç°äº†è¾ƒé«˜çš„æ ‡æ³¨è€…é—´ä¸€è‡´æ€§ï¼ŒéªŒè¯äº†æ•°æ®é›†çš„å¯é æ€§ã€‚</p>
<p><strong>Conclusion:</strong> MisSpansåŸºå‡†ä¸ºç»†ç²’åº¦è™šå‡ä¿¡æ¯æ£€æµ‹å’Œåˆ†ææä¾›äº†é¦–ä¸ªç³»ç»Ÿæ¡†æ¶ï¼Œæ­ç¤ºäº†ç°æœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨ç²¾ç¡®è¯†åˆ«å’Œè§£é‡Šè™šå‡ä¿¡æ¯ç‰‡æ®µæ–¹é¢çš„å±€é™æ€§ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†è€ƒè™‘æ¨¡å‹å¤§å°ã€æ¨ç†èƒ½åŠ›å’Œé¢†åŸŸç‰¹å¾ç­‰å¤šé‡å› ç´ çš„é‡è¦æ€§ï¼Œä¸ºå¼€å‘æ›´ç²¾ç¡®ã€å¯è§£é‡Šçš„è™šå‡ä¿¡æ¯æ£€æµ‹ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_45">ğŸ“„ Abstract</h4>
<p>Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.</p>
<h3 id="47-v-fat-benchmarking-visual-fidelity-against-text-bias">[47] <a href="https://arxiv.org/abs/2601.04897">V-FAT: Benchmarking Visual Fidelity Against Text-bias</a></h3>
<p><em>Ziteng Wang, Yujie He, Guanliang Li, Siqi Yang, Jiaqi Xiong, Songxiang Liu</em></p>
<h4 id="tldr_46">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºV-FATåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯Šæ–­å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡æœ¬åè§é—®é¢˜ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨è§†è§‰è¯æ®ä¸æ–‡æœ¬ä¿¡æ¯å†²çªæ—¶è¿‡åº¦ä¾èµ–è¯­è¨€æ·å¾„è€ŒéçœŸå®è§†è§‰åŸºç¡€çš„ç°è±¡ã€‚</p>
<hr />
<h4 id="detailed-summary_46">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ ‡å‡†è§†è§‰æ¨ç†åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨è¿‡åº¦ä¾èµ–è¯­è¨€æ·å¾„è€ŒéçœŸå®è§†è§‰åŸºç¡€çš„é—®é¢˜ï¼Œå³æ–‡æœ¬åè§ç°è±¡ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶è§†è§‰æ„ŸçŸ¥ä¸è¯­è¨€å…ˆéªŒä¹‹é—´çš„æ ¹æœ¬å¼ åŠ›ï¼Œå¹¶å°†åè§æ¥æºè§£è€¦ä¸ºå†…éƒ¨è¯­æ–™åè§å’Œå¤–éƒ¨æŒ‡ä»¤åè§ä¸¤ä¸ªç»´åº¦ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥V-FATè¯Šæ–­åŸºå‡†ï¼ŒåŒ…å«4,026ä¸ªVQAå®ä¾‹ï¼Œæ¶µç›–å…­ä¸ªè¯­ä¹‰é¢†åŸŸã€‚é‡‡ç”¨ä¸‰çº§è¯„ä¼°æ¡†æ¶ç³»ç»Ÿæ€§åœ°å¢åŠ è§†è§‰è¯æ®ä¸æ–‡æœ¬ä¿¡æ¯ä¹‹é—´çš„å†²çªï¼šL1çº§å¤„ç†éå…¸å‹å›¾åƒå¼•å‘çš„å†…éƒ¨åè§ï¼ŒL2çº§å¤„ç†è¯¯å¯¼æ€§æŒ‡ä»¤å¼•å‘çš„å¤–éƒ¨åè§ï¼ŒL3çº§å¤„ç†ä¸¤è€…ååŒçš„åè§ã€‚åŒæ—¶æå‡ºè§†è§‰é²æ£’æ€§è¯„åˆ†æŒ‡æ ‡ï¼Œæ—¨åœ¨æƒ©ç½š"å¹¸è¿"çš„è¯­è¨€çŒœæµ‹å¹¶å¥–åŠ±çœŸå®çš„è§†è§‰ä¿çœŸåº¦ã€‚</p>
<p><strong>Result:</strong> å¯¹12ä¸ªå‰æ²¿MLLMçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå°½ç®¡æ¨¡å‹åœ¨ç°æœ‰åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é«˜è¯­è¨€ä¸»å¯¼æ€§æ¡ä»¶ä¸‹ä¼šå‡ºç°æ˜¾è‘—çš„è§†è§‰å´©æºƒã€‚V-FATåŸºå‡†æ­ç¤ºäº†æ¨¡å‹åœ¨é¢å¯¹è§†è§‰è¯æ®ä¸æ–‡æœ¬ä¿¡æ¯å†²çªæ—¶çš„ç³»ç»Ÿæ€§å¼±ç‚¹ï¼Œç‰¹åˆ«æ˜¯åœ¨L2å’ŒL3çº§è¯„ä¼°ä¸­è¡¨ç°æ˜æ˜¾ä¸‹é™ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å½“å‰MLLMå­˜åœ¨ä¸¥é‡çš„æ–‡æœ¬åè§é—®é¢˜ï¼Œè¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒè€Œéè§†è§‰è¯æ®ã€‚è¿™å¼ºè°ƒäº†å¼€å‘æ›´é²æ£’çš„è§†è§‰åŸºç¡€æ¨¡å‹çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†è¯„ä¼°è§†è§‰ä¿çœŸåº¦çš„æ–°æ–¹æ³•è®ºã€‚ç ”ç©¶ç»“æœä¸ºæœªæ¥å¤šæ¨¡æ€æ¨¡å‹è®¾è®¡æä¾›äº†é‡è¦å¯ç¤ºï¼Œéœ€è¦æ›´å¥½åœ°å¹³è¡¡è§†è§‰ä¸è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_46">ğŸ“„ Abstract</h4>
<p>Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize "lucky" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.</p>
<h3 id="48-a-unified-spoken-language-model-with-injected-emotional-attribution-thinking-for-human-like-interaction">[48] <a href="https://arxiv.org/abs/2601.04960">A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction</a></h3>
<p><em>Qing Wang, Zehan Li, Yaodong Song, Hongjie Chen, Jian Kang, Jie Lian, Jie Li, Yongxiang Li, Xuelong Li</em></p>
<h4 id="tldr_47">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æƒ…æ„Ÿæ™ºèƒ½å£è¯­è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ–°é¢–çš„æ³¨å…¥å¼æƒ…æ„Ÿå½’å› æ€ç»´æ•°æ®æ„å»ºç­–ç•¥ï¼Œå°†ç”¨æˆ·æƒ…æ„ŸçŠ¶æ€åŠå…¶æˆå› èå…¥æ¨¡å‹å†…éƒ¨æ¨ç†è¿‡ç¨‹ï¼Œå®ç°äº†æƒ…æ„Ÿæ„ŸçŸ¥æ¨ç†çš„å†…åŒ–è€Œéæ˜¾å¼ç›‘ç£ã€‚</p>
<hr />
<h4 id="detailed-summary_47">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å£è¯­å¯¹è¯ç³»ç»Ÿåœ¨æƒ…æ„Ÿæ™ºèƒ½æ–¹é¢å­˜åœ¨å±€é™ï¼Œé€šå¸¸å°†æƒ…æ„Ÿæ„ŸçŸ¥ä½œä¸ºå¤–éƒ¨ç›‘ç£è€Œéæ¨¡å‹å†…éƒ¨æ¨ç†èƒ½åŠ›ï¼Œç¼ºä¹å¯¹ç”¨æˆ·æƒ…æ„ŸçŠ¶æ€åŠå…¶æˆå› çš„æ·±åº¦ç†è§£ä¸æ•´åˆï¼Œè¿™é™åˆ¶äº†ç³»ç»Ÿåœ¨æƒ…æ„Ÿè½¨è¿¹å»ºæ¨¡ã€æƒ…æ„Ÿæ¨ç†å’Œå…±æƒ…å›åº”ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> æå‡ºæ³¨å…¥å¼æƒ…æ„Ÿå½’å› æ€ç»´æ•°æ®æ„å»ºç­–ç•¥ï¼Œå°†ç”¨æˆ·æƒ…æ„ŸçŠ¶æ€åŠå…¶æˆå› èå…¥æ¨¡å‹å†…éƒ¨æ¨ç†è¿‡ç¨‹ï¼›é‡‡ç”¨ä¸¤é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡è‡ªè’¸é¦è¿›è¡Œè¯­éŸ³-æ–‡æœ¬å¯¹é½å’Œæƒ…æ„Ÿå±æ€§å»ºæ¨¡ï¼Œç¬¬äºŒé˜¶æ®µè¿›è¡Œç«¯åˆ°ç«¯è·¨æ¨¡æ€è”åˆä¼˜åŒ–ä»¥ç¡®ä¿æ–‡æœ¬ä¸å£è¯­æƒ…æ„Ÿè¡¨è¾¾çš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨Human-like Spoken Dialogue Systems Challengeæƒ…æ„Ÿæ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨æƒ…æ„Ÿè½¨è¿¹å»ºæ¨¡ã€æƒ…æ„Ÿæ¨ç†å’Œå…±æƒ…å›åº”ç”Ÿæˆæ–¹é¢å‡å–å¾—é¡¶çº§æ€§èƒ½ï¼Œåœ¨åŸºäºå¤§è¯­è¨€æ¨¡å‹å’Œäººç±»è¯„ä¼°ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> IEATç­–ç•¥é€šè¿‡å°†æƒ…æ„Ÿå½’å› æ€ç»´æ³¨å…¥æ¨¡å‹å†…éƒ¨æ¨ç†ï¼Œå®ç°äº†æƒ…æ„Ÿæ„ŸçŸ¥èƒ½åŠ›çš„å†…åŒ–ï¼Œä¸ºæ„å»ºæ›´è‡ªç„¶ã€æ›´å…·æƒ…æ„Ÿæ™ºèƒ½çš„å£è¯­å¯¹è¯ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆæ¡†æ¶ï¼Œå±•ç¤ºäº†è·¨æ¨¡æ€æƒ…æ„Ÿä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_47">ğŸ“„ Abstract</h4>
<p>This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.</p>
<h3 id="49-compositional-steering-of-large-language-models-with-steering-tokens">[49] <a href="https://arxiv.org/abs/2601.05062">Compositional Steering of Large Language Models with Steering Tokens</a></h3>
<p><em>Gorjan Radevski, Kiril Gashteovski, Giwon Hong, Carolin Lawrence, Goran GlavaÅ¡</em></p>
<h4 id="tldr_48">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º"ç»„åˆå¼•å¯¼ä»¤ç‰Œ"çš„æ–°æ–¹æ³•ï¼Œç”¨äºå®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¡Œä¸ºç»„åˆæ§åˆ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªè’¸é¦å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç¼–ç ä¸ºä¸“ç”¨ä»¤ç‰Œï¼Œå¹¶è®­ç»ƒç»„åˆä»¤ç‰Œæ¥æ³›åŒ–åˆ°æœªè§è¿‡çš„è¡Œä¸ºç»„åˆï¼Œä»è€Œåœ¨è¾“å…¥ä»¤ç‰Œç©ºé—´ä¸­å®ç°æœ‰æ•ˆçš„é›¶æ ·æœ¬ç»„åˆå¼•å¯¼ã€‚</p>
<hr />
<h4 id="detailed-summary_48">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨ç°å®åº”ç”¨ä¸­éƒ¨ç½²LLMéœ€è¦æ»¡è¶³å¤šä¸ªæœŸæœ›çš„å¯æ§è¾“å‡ºï¼Œç°æœ‰å·¥ä½œä¸»è¦å…³æ³¨å•ä¸€è¡Œä¸ºçš„å¼•å¯¼ï¼Œè€Œç»„åˆå¼•å¯¼â€”â€”å³åŒæ—¶å¼•å¯¼LLMæœå‘å¤šä¸ªè¡Œä¸ºâ€”â€”ä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„é—®é¢˜ã€‚å½“å‰æ–¹æ³•å¤§å¤šåœ¨æ¿€æ´»ç©ºé—´ä¸­æ“ä½œï¼Œç¼ºä¹å¯¹å¤šè¡Œä¸ºç»„åˆçš„æœ‰æ•ˆé›¶æ ·æœ¬æ§åˆ¶èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡è‡ªè’¸é¦å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¡¨è¾¾çš„ä¸ªä½“è¡Œä¸ºåµŒå…¥åˆ°ä¸“ç”¨ä»¤ç‰Œä¸­ï¼Œä½¿è¡Œä¸ºå¼•å¯¼åœ¨è¾“å…¥ä»¤ç‰Œç©ºé—´ä¸­æ“ä½œè€Œéæ¿€æ´»ç©ºé—´ã€‚éšåè®­ç»ƒä¸“ç”¨çš„ç»„åˆä»¤ç‰Œæ¥å¤„ç†è¡Œä¸ºå¯¹ï¼Œè¯¥ä»¤ç‰Œèƒ½å¤ŸæˆåŠŸæ•æ‰ç»„åˆæ¦‚å¿µï¼Œå¹¶æ³›åŒ–åˆ°æœªè§è¿‡çš„è¡Œä¸ºç»„åˆï¼ŒåŒ…æ‹¬åŒ…å«æœªè§è¡Œä¸ºçš„ç»„åˆä»¥åŠå…·æœ‰æœªè§è¡Œä¸ºæ•°é‡çš„ç»„åˆã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸åŒLLMæ¶æ„ä¸Šï¼Œå¼•å¯¼ä»¤ç‰Œæ–¹æ³•ç›¸æ¯”ç«äº‰æ–¹æ³•ï¼ˆæŒ‡ä»¤ã€æ¿€æ´»å¼•å¯¼å’ŒLoRAåˆå¹¶ï¼‰åœ¨å¤šè¡Œä¸ºæ§åˆ¶æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚æ­¤å¤–ï¼Œå¼•å¯¼ä»¤ç‰Œä¸è‡ªç„¶è¯­è¨€æŒ‡ä»¤å…·æœ‰äº’è¡¥æ€§ï¼ŒäºŒè€…çš„ç»„åˆèƒ½å¤Ÿå¸¦æ¥è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ï¼Œç»„åˆä»¤ç‰Œèƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–åˆ°æœªè§è¿‡çš„è¡Œä¸ºç»„åˆã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åœ¨è¾“å…¥ä»¤ç‰Œç©ºé—´ä¸­å®ç°ç»„åˆå¼•å¯¼çš„æœ‰æ•ˆæ€§ï¼Œä¸ºLLMçš„å¤šè¡Œä¸ºæ§åˆ¶æä¾›äº†æ–°èŒƒå¼ã€‚ç»„åˆä»¤ç‰Œçš„æ³›åŒ–èƒ½åŠ›è¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ï¼Œå¼•å¯¼ä»¤ç‰Œä¸è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„äº’è¡¥æ€§ä¸ºæœªæ¥æ··åˆæ§åˆ¶æ–¹æ³•çš„å‘å±•æä¾›äº†æ–¹å‘ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨é›¶æ ·æœ¬ç»„åˆæ§åˆ¶æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="abstract_48">ğŸ“„ Abstract</h4>
<p>Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.</p>
<h3 id="50-sempa-improving-sentence-embeddings-of-large-language-models-through-semantic-preference-alignment">[50] <a href="https://arxiv.org/abs/2601.05075">SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment</a></h3>
<p><em>Ziyang Chen, Zhenxuan Huang, Yile Wang, Weiqin Wang, Lu Yin, Hui Huang</em></p>
<h4 id="tldr_49">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSemPAæ–¹æ³•ï¼Œé€šè¿‡è¯­ä¹‰åå¥½å¯¹é½å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„å¥å­è¡¨ç¤ºèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå…¶ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¥å­çº§ç›´æ¥åå¥½ä¼˜åŒ–åœ¨é‡Šä¹‰ç”Ÿæˆä»»åŠ¡ä¸Šé«˜æ•ˆä¼˜åŒ–LLMsï¼Œåœ¨ä¿æŒå›ºæœ‰ç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶å­¦ä¹ åŒºåˆ†è¯­ä¹‰ç­‰æ•ˆå¥å­ã€‚</p>
<hr />
<h4 id="detailed-summary_49">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿå¥å­åµŒå…¥æ–¹æ³•åœ¨éç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ä¸Šä½¿ç”¨tokençº§å¯¹æ¯”å­¦ä¹ ï¼Œè€ŒåŸºäºç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹çš„åµŒå…¥æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªé—®é¢˜ï¼šå›ºå®šæç¤ºæ¨¡æ¿ç¼ºä¹æ¨¡å‹è¿›ä¸€æ­¥ä¼˜åŒ–å¯¼è‡´æ€§èƒ½æœ‰é™ï¼Œä¿®æ”¹æ¨¡å‹æ¶æ„ä¼šæ”¹å˜å†…éƒ¨è®¡ç®—æœºåˆ¶ä»è€ŒæŸå®³ç”Ÿæˆèƒ½åŠ›ã€‚éœ€è¦ä¸€ç§æ—¢èƒ½å¢å¼ºå¥å­è¡¨ç¤ºåˆä¸ç‰ºç‰²LLMsç”Ÿæˆèƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æå‡ºSemPAæ–¹æ³•ï¼Œé€šè¿‡è¯­ä¹‰åå¥½å¯¹é½å¢å¼ºå¥å­è¡¨ç¤ºåŒæ—¶ä¿æŒLLMsç”Ÿæˆèƒ½åŠ›ã€‚é‡‡ç”¨å¥å­çº§ç›´æ¥åå¥½ä¼˜åŒ–åœ¨é‡Šä¹‰ç”Ÿæˆä»»åŠ¡ä¸Šé«˜æ•ˆä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼Œä½¿æ¨¡å‹å­¦ä¹ åŒºåˆ†è¯­ä¹‰ç­‰æ•ˆå¥å­ã€‚åœ¨ç†è®ºå±‚é¢ï¼Œåœ¨Plackett-Luceæ¨¡å‹æ¡†æ¶ä¸‹å»ºç«‹äº†DPOä¸å¯¹æ¯”å­¦ä¹ ä¹‹é—´çš„å½¢å¼åŒ–è”ç³»ã€‚</p>
<p><strong>Result:</strong> åœ¨è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ä»»åŠ¡å’Œå„ç§å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSemPAå®ç°äº†æ›´å¥½çš„è¯­ä¹‰è¡¨ç¤ºï¼ŒåŒæ—¶ä¸ç‰ºç‰²LLMså›ºæœ‰çš„ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹ç”Ÿæˆæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡äº†å¥å­åµŒå…¥çš„è´¨é‡å’Œæ•ˆæœã€‚</p>
<p><strong>Conclusion:</strong> SemPAæä¾›äº†ä¸€ç§æœ‰æ•ˆå¹³è¡¡å¥å­è¡¨ç¤ºå¢å¼ºä¸ç”Ÿæˆèƒ½åŠ›ä¿æŒçš„æ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰åå¥½å¯¹é½å®ç°äº†ä¸¤æ–¹é¢çš„ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•ä¸ºåœ¨ä¿æŒå¤§è¯­è¨€æ¨¡å‹æ ¸å¿ƒèƒ½åŠ›çš„åŒæ—¶æ”¹è¿›å…¶åµŒå…¥è¡¨ç¤ºæä¾›äº†æ–°æ€è·¯ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µæ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_49">ğŸ“„ Abstract</h4>
<p>Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.</p>
<h3 id="51-lela-an-llm-based-entity-linking-approach-with-zero-shot-domain-adaptation">[51] <a href="https://arxiv.org/abs/2601.05192">LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation</a></h3>
<p><em>Samy Haffoudhi, Fabian M. Suchanek, Nils Holzenberger</em></p>
<h4 id="tldr_50">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†LELAï¼Œä¸€ç§æ— éœ€å¾®è°ƒçš„æ¨¡å—åŒ–ç”±ç²—åˆ°ç²¾å®ä½“é“¾æ¥æ–¹æ³•ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›ï¼Œåœ¨ä¸åŒé¢†åŸŸã€çŸ¥è¯†åº“å’ŒLLMä¸Šå‡èƒ½å·¥ä½œï¼Œå¹¶åœ¨å¤šç§å®ä½“é“¾æ¥è®¾ç½®ä¸­å±•ç°å‡ºä¸å¾®è°ƒæ–¹æ³•ç›¸ç«äº‰çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_50">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å®ä½“é“¾æ¥æ˜¯å°†æ–‡æœ¬ä¸­çš„æ¨¡ç³ŠæåŠæ˜ å°„åˆ°çŸ¥è¯†åº“å®ä½“çš„åŸºç¡€ä»»åŠ¡ï¼Œå¯¹çŸ¥è¯†å›¾è°±æ„å»ºã€é—®ç­”å’Œä¿¡æ¯æå–è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šé¢†åŸŸæˆ–çŸ¥è¯†åº“è¿›è¡Œå¾®è°ƒï¼Œç¼ºä¹é€šç”¨æ€§ä¸”éƒ¨ç½²æˆæœ¬é«˜ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ— éœ€å¾®è°ƒã€èƒ½è·¨ä¸åŒé¢†åŸŸå’ŒçŸ¥è¯†åº“å·¥ä½œçš„çµæ´»å®ä½“é“¾æ¥æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> LELAé‡‡ç”¨æ¨¡å—åŒ–ç”±ç²—åˆ°ç²¾çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›è¿›è¡Œå®ä½“é“¾æ¥ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–ä»»ä½•å¾®è°ƒé˜¶æ®µï¼Œèƒ½å¤Ÿä¸ä¸åŒçš„ç›®æ ‡é¢†åŸŸã€çŸ¥è¯†åº“å’Œå¤§è¯­è¨€æ¨¡å‹ååŒå·¥ä½œï¼Œé€šè¿‡åˆ†é˜¶æ®µå¤„ç†ç­–ç•¥å®ç°é«˜æ•ˆå‡†ç¡®çš„å®ä½“æ¶ˆæ­§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒLELAåœ¨å„ç§å®ä½“é“¾æ¥è®¾ç½®ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ç»è¿‡å¾®è°ƒçš„æ–¹æ³•ç›¸æ¯”å…·æœ‰é«˜åº¦ç«äº‰åŠ›ï¼ŒåŒæ—¶æ˜¾è‘—ä¼˜äºæ‰€æœ‰æœªç»å¾®è°ƒçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸå’ŒçŸ¥è¯†åº“ä¸Šå‡å±•ç°å‡ºç¨³å®šçš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> LELAè¯æ˜äº†æ— éœ€å¾®è°ƒçš„æ¨¡å—åŒ–å®ä½“é“¾æ¥æ–¹æ³•çš„å¯è¡Œæ€§ï¼Œä¸ºè·¨é¢†åŸŸå®ä½“é“¾æ¥æä¾›äº†çµæ´»é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é™ä½äº†å®ä½“é“¾æ¥ç³»ç»Ÿçš„éƒ¨ç½²å’Œç»´æŠ¤æˆæœ¬ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹åœ¨å®ä½“é“¾æ¥ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå…·æœ‰é‡è¦çš„å®è·µæ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_50">ğŸ“„ Abstract</h4>
<p>Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="52-guitester-enabling-gui-agents-for-exploratory-defect-discovery">[52] <a href="https://arxiv.org/abs/2601.04500">GUITester: Enabling GUI Agents for Exploratory Defect Discovery</a></h3>
<p><em>Yifei Gao, Jiang Wu, Xiaoyi Chen, Yifan Yang, Zhe Cui, Tianyi Ma, Jiaming Zhang, Jitao Sang</em></p>
<h4 id="tldr_51">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGUITesterï¼Œä¸€ç§ç”¨äºè‡ªä¸»æ¢ç´¢å¼GUIæµ‹è¯•çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè§£å†³äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“åœ¨å¯¼èˆªä»»åŠ¡ä¸­æ— æ³•è‡ªä¸»å‘ç°ç¼ºé™·çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è§£è€¦å¯¼èˆªä¸éªŒè¯ï¼Œåœ¨GUITestBenchåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†48.90%çš„F1åˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_51">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ¢ç´¢å¼GUIæµ‹è¯•å¯¹è½¯ä»¶è´¨é‡è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´é«˜æ˜‚çš„äººå·¥æˆæœ¬ã€‚å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“åœ¨å¯¼èˆªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜è€Œæ— æ³•è‡ªä¸»å‘ç°ç¼ºé™·ï¼šç›®æ ‡å¯¼å‘é®è”½ï¼ˆæ™ºèƒ½ä½“ä¼˜å…ˆå®Œæˆä»»åŠ¡è€ŒéæŠ¥å‘Šå¼‚å¸¸ï¼‰å’Œæ‰§è¡Œåå·®å½’å› ï¼ˆç³»ç»Ÿç¼ºé™·è¢«è¯¯åˆ¤ä¸ºæ™ºèƒ½ä½“é”™è¯¯ï¼‰ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é¦–å…ˆå¼•å…¥äº†GUITestBenchï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè¯¥ä»»åŠ¡çš„äº¤äº’å¼åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«26ç§ç¼ºé™·ç±»å‹çš„143ä¸ªä»»åŠ¡ã€‚éšåæå‡ºäº†GUITesterå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªæ¨¡å—è§£è€¦å¯¼èˆªä¸éªŒè¯ï¼šè§„åˆ’æ‰§è¡Œæ¨¡å—é€šè¿‡åµŒå…¥æµ‹è¯•æ„å›¾ä¸»åŠ¨æ¢æµ‹ç¼ºé™·ï¼›åˆ†å±‚åæ€æ¨¡å—é€šè¿‡äº¤äº’å†å²åˆ†æè§£å†³å½’å› æ¨¡ç³Šæ€§é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> GUITesteråœ¨GUITestBenchåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†48.90%çš„F1åˆ†æ•°ï¼ˆPass@3ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›åŸºçº¿çš„33.35%ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆè§£å†³äº†ç›®æ ‡å¯¼å‘é®è”½å’Œæ‰§è¡Œåå·®å½’å› é—®é¢˜ï¼Œè¯æ˜äº†è‡ªä¸»æ¢ç´¢å¼æµ‹è¯•çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹å·¥ä½œå±•ç¤ºäº†è‡ªä¸»æ¢ç´¢å¼GUIæµ‹è¯•çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥GUIè´¨é‡ä¿è¯æä¾›äº†åšå®åŸºç¡€ã€‚é€šè¿‡è§£è€¦å¯¼èˆªä¸éªŒè¯çš„å¤šæ™ºèƒ½ä½“æ–¹æ³•ï¼Œè§£å†³äº†MLLMæ™ºèƒ½ä½“åœ¨ç¼ºé™·å‘ç°ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸ºè‡ªåŠ¨åŒ–è½¯ä»¶æµ‹è¯•å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_51">ğŸ“„ Abstract</h4>
<p>Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\footnote{Our code is now available in~\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.</p>
<h3 id="53-specific-emitter-identification-via-active-learning">[53] <a href="https://arxiv.org/abs/2601.04502">Specific Emitter Identification via Active Learning</a></h3>
<p><em>Jingyi Wang, Fanggang Wang</em></p>
<h4 id="tldr_52">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸»åŠ¨å­¦ä¹ çš„ç‰¹å®šè¾å°„æºè¯†åˆ«æ–¹æ³•ï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µåŠç›‘ç£è®­ç»ƒæ–¹æ¡ˆï¼Œåœ¨æœ‰é™æ ‡æ³¨é¢„ç®—ä¸‹æ˜¾è‘—æå‡äº†è¯†åˆ«æ€§èƒ½å¹¶é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚</p>
<hr />
<h4 id="detailed-summary_52">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç‰¹å®šè¾å°„æºè¯†åˆ«åœ¨æ— çº¿é€šä¿¡å®‰å…¨ä¸­è‡³å…³é‡è¦ï¼Œä½†å…¶æ¨¡å‹è®­ç»ƒä¸¥é‡ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®ï¼Œè¿™äº›æ•°æ®è·å–æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚ç°æœ‰æ–¹æ³•åœ¨æœ‰é™æ ‡æ³¨æ¡ä»¶ä¸‹æ€§èƒ½å—é™ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æœªæ ‡æ³¨æ•°æ®å¹¶é™ä½æ ‡æ³¨æˆæœ¬çš„æ–°æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨ä¸‰é˜¶æ®µåŠç›‘ç£è®­ç»ƒæ–¹æ¡ˆï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¸¦åŠ¨æ€å­—å…¸æ›´æ–°æœºåˆ¶çš„è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ä»æœªæ ‡æ³¨æ•°æ®ä¸­æå–é²æ£’è¡¨ç¤ºï¼›ç¬¬äºŒé˜¶æ®µåœ¨å°è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£è®­ç»ƒï¼Œè”åˆä¼˜åŒ–å¯¹æ¯”æŸå¤±å’Œäº¤å‰ç†µæŸå¤±ä»¥å¢å¼ºç‰¹å¾å¯åˆ†æ€§å’Œåˆ†ç±»è¾¹ç•Œï¼›ç¬¬ä¸‰é˜¶æ®µå¼•å…¥ä¸»åŠ¨å­¦ä¹ æ¨¡å—ï¼ŒåŸºäºä¸ç¡®å®šæ€§å’Œä»£è¡¨æ€§å‡†åˆ™ä»æœªæ ‡æ³¨æ•°æ®ä¸­é€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œåœ¨æœ‰é™æ ‡æ³¨é¢„ç®—ä¸‹è¿›ä¸€æ­¥æå‡æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨ADS-Bå’ŒWiFiæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰é™æ ‡æ³¨æ¡ä»¶ä¸‹æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ç›‘ç£å’ŒåŠç›‘ç£æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„è¯†åˆ«å‡†ç¡®ç‡å’Œæ›´ä½çš„æ ‡æ³¨æˆæœ¬ã€‚å…·ä½“æ€§èƒ½æå‡ä½“ç°åœ¨ä¸åŒæ ‡æ³¨æ¯”ä¾‹ä¸‹çš„ç¨³å®šä¼˜åŠ¿ï¼ŒéªŒè¯äº†æ‰€æä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆå’Œä¸»åŠ¨å­¦ä¹ é€‰æ‹©æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆè‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ã€åŠç›‘ç£è®­ç»ƒå’Œä¸»åŠ¨å­¦ä¹ çš„å¤šé˜¶æ®µæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè§£å†³ç‰¹å®šè¾å°„æºè¯†åˆ«ä¸­çš„æ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸ºé€šä¿¡å®‰å…¨é¢†åŸŸçš„å®é™…åº”ç”¨æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†åœ¨æœ‰é™æ ‡æ³¨é¢„ç®—ä¸‹é€šè¿‡æ™ºèƒ½æ ·æœ¬é€‰æ‹©å’Œæ•°æ®é«˜æ•ˆå­¦ä¹ ç­–ç•¥å®ç°é«˜æ€§èƒ½è¯†åˆ«çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_52">ğŸ“„ Abstract</h4>
<p>With the rapid growth of wireless communications, specific emitter identification (SEI) is significant for communication security. However, its model training relies heavily on the large-scale labeled data, which are costly and time-consuming to obtain. To address this challenge, we propose an SEI approach enhanced by active learning (AL), which follows a three-stage semi-supervised training scheme. In the first stage, self-supervised contrastive learning is employed with a dynamic dictionary update mechanism to extract robust representations from large amounts of the unlabeled data. In the second stage, supervised training on a small labeled dataset is performed, where the contrastive and cross-entropy losses are jointly optimized to improve the feature separability and strengthen the classification boundaries. In the third stage, an AL module selects the most valuable samples from the unlabeled data for annotation based on the uncertainty and representativeness criteria, further enhancing generalization under limited labeling budgets. Experimental results on the ADS-B and WiFi datasets demonstrate that the proposed SEI approach significantly outperforms the conventional supervised and semi-supervised methods under limited annotation conditions, achieving higher recognition accuracy with lower labeling cost.</p>
<h3 id="54-integrating-distribution-matching-into-semi-supervised-contrastive-learning-for-labeled-and-unlabeled-data">[54] <a href="https://arxiv.org/abs/2601.04518">Integrating Distribution Matching into Semi-Supervised Contrastive Learning for Labeled and Unlabeled Data</a></h3>
<p><em>Shogo Nakayama, Masahiro Okuda</em></p>
<h4 id="tldr_53">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆåˆ†å¸ƒåŒ¹é…çš„ä¼ªæ ‡ç­¾åŠç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¯¹é½æ ‡è®°æ•°æ®ä¸æœªæ ‡è®°æ•°æ®çš„ç‰¹å¾åˆ†å¸ƒæ¥æå‡å›¾åƒåˆ†ç±»æ€§èƒ½ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜äºåŸºå‡†æ–¹æ³•çš„å‡†ç¡®ç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_53">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ·±åº¦å­¦ä¹ çš„ç›‘ç£å›¾åƒåˆ†ç±»éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä½†å®é™…æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œè€Œå®Œå…¨æ— æ ‡æ³¨çš„åœºæ™¯è¾ƒå°‘ï¼ŒåŠç›‘ç£å­¦ä¹ åœ¨å°‘é‡æ ‡æ³¨æ•°æ®ä¸å¤§é‡æœªæ ‡æ³¨æ•°æ®å…±å­˜çš„æƒ…å†µä¸‹æ›´å…·å®é™…æ„ä¹‰ã€‚ç°æœ‰åŸºäºä¼ªæ ‡ç­¾çš„åŠç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ä»æœ‰æ”¹è¿›ç©ºé—´ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡åˆ†å¸ƒåŒ¹é…æŠ€æœ¯æå‡ä¼ªæ ‡ç­¾è´¨é‡ï¼Œä»è€Œæ”¹å–„å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åœ¨ä¼ªæ ‡ç­¾åŠç›‘ç£å­¦ä¹ æ¡†æ¶ä¸­å¼•å…¥äº†åˆ†å¸ƒåŒ¹é…æœºåˆ¶ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¯¹é½æ ‡è®°æ•°æ®ä¸æœªæ ‡è®°æ•°æ®çš„ç‰¹å¾åµŒå…¥åˆ†å¸ƒã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åœ¨ç‰¹å¾ç©ºé—´ä¸­å¼ºåˆ¶æ ‡è®°æ ·æœ¬ä¸æœªæ ‡è®°æ ·æœ¬çš„åˆ†å¸ƒä¸€è‡´æ€§ï¼Œåˆ©ç”¨åˆ†å¸ƒåŒ¹é…æŸå¤±å‡½æ•°ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œç”Ÿæˆæ›´å¯é çš„ä¼ªæ ‡ç­¾ç”¨äºæ¨¡å‹è®­ç»ƒã€‚</p>
<p><strong>Result:</strong> å®éªŒåœ¨å¤šä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸ŠéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨åˆ†ç±»å‡†ç¡®ç‡æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿä¼ªæ ‡ç­¾æ–¹æ³•å’ŒåŸºå‡†åŠç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚é€šè¿‡åˆ†å¸ƒåŒ¹é…æœºåˆ¶ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœªæ ‡è®°æ•°æ®ï¼Œæå‡æ¨¡å‹åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ï¼Œåœ¨åŠç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸­å¼•å…¥åˆ†å¸ƒåŒ¹é…æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆæå‡ä¼ªæ ‡ç­¾è´¨é‡ï¼Œä»è€Œæ”¹å–„å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ³•ä¸ºåŠç›‘ç£å­¦ä¹ æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„å®é™…åº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œæœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢æ›´å¤æ‚çš„åˆ†å¸ƒå¯¹é½ç­–ç•¥ã€‚</p>
<hr />
<h4 id="abstract_53">ğŸ“„ Abstract</h4>
<p>The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.</p>
<h3 id="55-backdooragent-a-unified-framework-for-backdoor-attacks-on-llm-based-agents">[55] <a href="https://arxiv.org/abs/2601.04566">BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents</a></h3>
<p><em>Yunhao Feng, Yige Li, Yutao Wu, Yingshui Tan, Yanming Guo, Yifan Ding, Kun Zhai, Xingjun Ma, Yugang Jiang</em></p>
<h4 id="tldr_54">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†BackdoorAgentæ¡†æ¶ï¼Œä¸ºLLMæ™ºèƒ½ä½“ä¸­çš„åé—¨å¨èƒæä¾›äº†ç»Ÿä¸€çš„ã€é¢å‘æ™ºèƒ½ä½“çš„è§†è§’ï¼Œé€šè¿‡æ¨¡å—åŒ–ã€é˜¶æ®µæ„ŸçŸ¥çš„è®¾è®¡ç³»ç»Ÿåˆ†æäº†åé—¨è§¦å‘å™¨åœ¨æ™ºèƒ½ä½“å·¥ä½œæµä¸åŒé˜¶æ®µé—´çš„æ¿€æ´»ä¸ä¼ æ’­æœºåˆ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_54">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç ”ç©¶å¯¹LLMæ™ºèƒ½ä½“ä¸­çš„åé—¨å¨èƒåˆ†æè¾ƒä¸ºé›¶æ•£ï¼Œé€šå¸¸å­¤ç«‹åœ°åˆ†æå•ä¸ªæ”»å‡»å‘é‡ï¼Œç¼ºä¹ä»æ™ºèƒ½ä½“è§’åº¦ç†è§£åé—¨è§¦å‘å™¨åœ¨ä¸åŒé˜¶æ®µé—´çš„äº¤äº’ä¸ä¼ æ’­æœºåˆ¶ï¼Œè¿™é™åˆ¶äº†æˆ‘ä»¬å¯¹æ™ºèƒ½ä½“å·¥ä½œæµå®‰å…¨æ¼æ´çš„ç³»ç»Ÿæ€§è®¤è¯†ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†BackdoorAgentæ¡†æ¶ï¼Œå°†æ”»å‡»é¢ç»“æ„åŒ–ä¸ºæ™ºèƒ½ä½“å·¥ä½œæµçš„ä¸‰ä¸ªåŠŸèƒ½é˜¶æ®µï¼šè§„åˆ’æ”»å‡»ã€è®°å¿†æ”»å‡»å’Œå·¥å…·ä½¿ç”¨æ”»å‡»ï¼Œå¹¶é€šè¿‡æ£€æµ‹æ™ºèƒ½ä½“æ‰§è¡Œè¿‡ç¨‹æ¥ç³»ç»Ÿåˆ†æè§¦å‘å™¨åœ¨ä¸åŒé˜¶æ®µçš„æ¿€æ´»ä¸ä¼ æ’­ï¼Œæ„å»ºäº†æ¶µç›–Agent QAã€Agent Codeã€Agent Webå’ŒAgent Driveå››ä¸ªä»£è¡¨æ€§åº”ç”¨çš„æ ‡å‡†åŒ–åŸºå‡†ã€‚</p>
<p><strong>Result:</strong> å®éªŒåˆ†æè¡¨æ˜ï¼Œæ¤å…¥å•ä¸ªé˜¶æ®µçš„è§¦å‘å™¨å¯ä»¥è·¨å¤šä¸ªæ­¥éª¤æŒç»­å­˜åœ¨å¹¶é€šè¿‡ä¸­é—´çŠ¶æ€ä¼ æ’­ï¼Œåœ¨ä½¿ç”¨GPTåŸºåº§æ¨¡å‹æ—¶ï¼Œè§„åˆ’æ”»å‡»çš„è§¦å‘å™¨æŒç»­ç‡ä¸º43.58%ï¼Œè®°å¿†æ”»å‡»ä¸º77.97%ï¼Œå·¥å…·é˜¶æ®µæ”»å‡»ä¸º60.28%ï¼Œå‡¸æ˜¾äº†æ™ºèƒ½ä½“å·¥ä½œæµæœ¬èº«å¯¹åé—¨å¨èƒçš„è„†å¼±æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†LLMæ™ºèƒ½ä½“å·¥ä½œæµä¸­åé—¨å¨èƒçš„ç³»ç»Ÿæ€§é£é™©ï¼Œå¼ºè°ƒäº†è·¨é˜¶æ®µè§¦å‘å™¨ä¼ æ’­çš„é‡è¦æ€§ï¼Œæå‡ºçš„æ¡†æ¶å’ŒåŸºå‡†ä¸ºæœªæ¥æ™ºèƒ½ä½“å®‰å…¨ç ”ç©¶æä¾›äº†å¯å¤ç°çš„åŸºç¡€ï¼Œå¹¶æŒ‡å‡ºäº†éœ€è¦å¼€å‘æ›´é²æ£’çš„é˜²å¾¡æœºåˆ¶æ¥ä¿æŠ¤å¤šé˜¶æ®µæ™ºèƒ½ä½“ç³»ç»Ÿã€‚</p>
<hr />
<h4 id="abstract_54">ğŸ“„ Abstract</h4>
<p>Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \textbf{planning attacks}, \textbf{memory attacks}, and \textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \textbf{Agent QA}, \textbf{Agent Code}, \textbf{Agent Web}, and \textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\% of planning attacks, 77.97\% of memory attacks, and 60.28\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.</p>
<h3 id="56-enhancing-multimodal-retrieval-via-complementary-information-extraction-and-alignment">[56] <a href="https://arxiv.org/abs/2601.04571">Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment</a></h3>
<p><em>Delong Zeng, Yuexiang Xie, Yaliang Li, Ying Shen</em></p>
<h4 id="tldr_55">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCIEAï¼Œä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€æ£€ç´¢æ–¹æ³•ï¼Œé€šè¿‡äº’è¡¥ä¿¡æ¯æå–ä¸å¯¹é½æœºåˆ¶ï¼Œå°†æ–‡æœ¬å’Œå›¾åƒè½¬æ¢åˆ°ç»Ÿä¸€æ½œåœ¨ç©ºé—´ï¼Œå¹¶è®¾è®¡äº’è¡¥ä¿¡æ¯æå–å™¨æ¥è¯†åˆ«å’Œä¿ç•™å›¾åƒè¡¨ç¤ºä¸­çš„å·®å¼‚ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€æ£€ç´¢æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_55">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€æ£€ç´¢ç ”ç©¶ä¸»è¦å…³æ³¨æ•æ‰å¤šæ¨¡æ€æ•°æ®ä¸­ä¸é…å¯¹æ–‡æœ¬ç›¸ä¼¼çš„ä¿¡æ¯ï¼Œä½†å¾€å¾€å¿½ç•¥äº†å¤šæ¨¡æ€æ•°æ®ä¸­åŒ…å«çš„äº’è¡¥ä¿¡æ¯ï¼Œè¿™ç§ä¿¡æ¯ç¼ºå¤±é™åˆ¶äº†æ£€ç´¢ç³»ç»Ÿçš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Method:</strong> CIEAé‡‡ç”¨äº’è¡¥ä¿¡æ¯æå–ä¸å¯¹é½æ–¹æ³•ï¼Œå°†æ–‡æ¡£ä¸­çš„æ–‡æœ¬å’Œå›¾åƒè½¬æ¢åˆ°ç»Ÿä¸€æ½œåœ¨ç©ºé—´ï¼Œè®¾è®¡äº’è¡¥ä¿¡æ¯æå–å™¨æ¥è¯†åˆ«å’Œä¿ç•™å›¾åƒè¡¨ç¤ºä¸­çš„å·®å¼‚ï¼Œå¹¶ä½¿ç”¨ä¸¤ç§äº’è¡¥å¯¹æ¯”æŸå¤±è¿›è¡Œä¼˜åŒ–ä»¥ç¡®ä¿è¯­ä¹‰å®Œæ•´æ€§å¹¶æœ‰æ•ˆæ•æ‰å›¾åƒä¸­çš„äº’è¡¥ä¿¡æ¯ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¯æ˜CIEAçš„æœ‰æ•ˆæ€§ï¼Œç›¸æ¯”åˆ†æ²»æ¨¡å‹å’Œé€šç”¨å¯†é›†æ£€ç´¢æ¨¡å‹å‡å–å¾—æ˜¾è‘—æ”¹è¿›ï¼Œé€šè¿‡æ¶ˆèç ”ç©¶ã€è¿›ä¸€æ­¥è®¨è®ºå’Œæ¡ˆä¾‹ç ”ç©¶çªå‡ºäº†CIEAæ‰€å–å¾—çš„è¿›å±•ï¼Œæºä»£ç å·²åœ¨GitHubä¸Šå¼€æºã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨å¤šæ¨¡æ€æ£€ç´¢ä¸­æ•æ‰äº’è¡¥ä¿¡æ¯çš„é‡è¦æ€§ï¼ŒCIEAæ–¹æ³•ä¸ºç›¸å…³é¢†åŸŸæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¼€æºä»£ç å°†ä¿ƒè¿›ç¤¾åŒºè¿›ä¸€æ­¥ç ”ç©¶ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè¯­ä¹‰å®Œæ•´æ€§çš„åŒæ—¶æœ‰æ•ˆåˆ©ç”¨å›¾åƒä¸­çš„å·®å¼‚ä¿¡æ¯ï¼Œä¸ºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<hr />
<h4 id="abstract_55">ğŸ“„ Abstract</h4>
<p>Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.</p>
<h3 id="57-bridging-temporal-and-textual-modalities-a-multimodal-framework-for-automated-cloud-failure-root-cause-analysis">[57] <a href="https://arxiv.org/abs/2601.04709">Bridging Temporal and Textual Modalities: A Multimodal Framework for Automated Cloud Failure Root Cause Analysis</a></h3>
<p><em>Gijun Park</em></p>
<h4 id="tldr_56">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è¯Šæ–­æ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰å‹ç¼©å’Œå¯¹é½ç¼–ç æŠ€æœ¯å°†æ—¶é—´åºåˆ—è¡¨ç¤ºä¸é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åµŒå…¥ç©ºé—´ç›¸åè°ƒï¼Œè§£å†³äº†è¯­è¨€æ¨¡å‹åœ¨äº‘åŸºç¡€è®¾æ–½æ ¹å› åˆ†æä¸­å¤„ç†è¿ç»­æ•°å€¼åºåˆ—çš„æ¨¡æ€ä¸åŒ¹é…é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_56">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°ä»£äº‘åŸºç¡€è®¾æ–½æ ¹å› åˆ†æéœ€è¦ç†è§£å¼‚æ„æ•°æ®æºï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠæ ¸å¿ƒæ•…éšœç‰¹å¾çš„æ—¶é—´åºåˆ—æ€§èƒ½æŒ‡æ ‡ã€‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åŸºäºç¦»æ•£ä»¤ç‰Œçš„æ¶æ„ä¸å…·æœ‰æ—¶é—´ä¾èµ–æ€§çš„è¿ç»­æ•°å€¼åºåˆ—å­˜åœ¨æ ¹æœ¬æ€§ä¸å…¼å®¹ï¼Œå½“å‰æ–¹æ³•æœªèƒ½å……åˆ†è§£å†³è¿™ç§æ¨¡æ€ä¸åŒ¹é…é—®é¢˜ï¼Œé™åˆ¶äº†è¯­è¨€æ¨¡å‹åœ¨äº‹ä»¶ç®¡ç†å·¥ä½œæµä¸­çš„è‡ªåŠ¨åŒ–æ½œåŠ›ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€è¯Šæ–­æ¡†æ¶ï¼ŒåŒ…å«ä¸‰é¡¹æŠ€æœ¯è´¡çŒ®ï¼šä¸€æ˜¯è¯­ä¹‰å‹ç¼©æŠ€æœ¯ï¼Œå°†æ—¶é—´ç‰‡æ®µè’¸é¦ä¸ºå•ä»¤ç‰ŒæŠ½è±¡åŒæ—¶ä¿ç•™æ¨¡å¼è¯­ä¹‰ï¼›äºŒæ˜¯ä½¿ç”¨é—¨æ§äº¤å‰æ³¨æ„åŠ›çš„å¯¹é½ç¼–ç å™¨ï¼Œå°†æ—¶é—´åºåˆ—ç‰¹å¾æŠ•å½±åˆ°è¯­è¨€æ¨¡å‹æ½œåœ¨ç©ºé—´ï¼›ä¸‰æ˜¯æ£€ç´¢å¢å¼ºçš„è¯Šæ–­ç®¡é“ï¼Œå°†å¯¹é½åµŒå…¥ä¸å†å²äº‹ä»¶çŸ¥è¯†ç›¸ç»“åˆè¿›è¡Œä¸“å®¶çº§æ•…éšœå½’å› ã€‚</p>
<p><strong>Result:</strong> åœ¨å…­ä¸ªäº‘ç³»ç»ŸåŸºå‡†æµ‹è¯•ä¸­çš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶å®ç°äº†é¢†å…ˆæ€§èƒ½ï¼Œè¾¾åˆ°48.75%çš„è¯Šæ–­å‡†ç¡®ç‡ï¼Œåœ¨æ¶‰åŠå¤åˆæ•…éšœæ¨¡å¼çš„åœºæ™¯ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚ç»“æœéªŒè¯äº†åµŒå…¥ç©ºé—´å¯¹é½ä½œä¸ºä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿäº§äº‹ä»¶å“åº”ä¸Šä¸‹æ–‡ä¸­å¯¹å¤šæ¨¡æ€é¥æµ‹æ•°æ®è¿›è¡Œæ¨ç†çš„æœ‰æ•ˆç­–ç•¥ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯å®äº†åµŒå…¥ç©ºé—´å¯¹é½æ˜¯è§£å†³è¯­è¨€æ¨¡å‹ä¸è¿ç»­æ—¶é—´åºåˆ—æ•°æ®æ¨¡æ€ä¸åŒ¹é…é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä¸ºè¯­è¨€æ¨¡å‹é©±åŠ¨çš„äº‘åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–è¯Šæ–­æä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤åˆæ•…éšœæ¨¡å¼æ–¹é¢å±•ç°å‡ºä¼˜åŠ¿ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€æ¨ç†åœ¨äº‹ä»¶ç®¡ç†ä¸­çš„åº”ç”¨ã€‚</p>
<hr />
<h4 id="abstract_56">ğŸ“„ Abstract</h4>
<p>Root cause analysis in modern cloud infrastructure demands sophisticated understanding of heterogeneous data sources, particularly time-series performance metrics that involve core failure signatures. While large language models demonstrate remarkable capabilities in textual reasoning, their discrete token-based architecture creates fundamental incompatibilities with continuous numerical sequences exhibiting temporal dependencies. Current methodologies inadequately address this modality mismatch, constraining the potential of language model-driven automation in incident management workflows. This paper presents a multimodal diagnostic framework that harmonizes time-series representations with pretrained language model embedding spaces. Our approach contributes three technical advances: (1) a semantic compression technique that distills temporal segments into single-token abstractions while preserving pattern semantics, (2) an alignment encoder utilizing gated cross-attention to project time-series features into language model latent space, and (3) a retrieval-augmented diagnostic pipeline that synthesizes aligned embeddings with historical incident knowledge for expert-level failure attribution. Comprehensive evaluation across six cloud system benchmarks demonstrates that our framework achieves leading performance, reaching 48.75% diagnostic accuracy with notable improvements on scenarios involving compound failure modes. The results validate embedding-space alignment as an effective strategy for enabling language models to reason over multimodal telemetry data in production incident response contexts.</p>
<h3 id="58-aecv-bench-benchmarking-multimodal-models-on-architectural-and-engineering-drawings-understanding">[58] <a href="https://arxiv.org/abs/2601.04819">AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding</a></h3>
<p><em>Aleksei Kondratenko, Mussie Birhane, Houssame E. Hsain, Guido Maciocci</em></p>
<h4 id="tldr_57">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†AECV-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å»ºç­‘ã€å·¥ç¨‹å’Œæ–½å·¥å›¾çº¸ç†è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå‘ç°å½“å‰æ¨¡å‹åœ¨æ–‡æ¡£è¾…åŠ©ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å›¾çº¸ç¬¦å·ç†è§£å’Œè®¡æ•°ä»»åŠ¡ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚</p>
<hr />
<h4 id="detailed-summary_57">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°ä»£å¤šæ¨¡æ€å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å»ºç­‘ã€å·¥ç¨‹å’Œæ–½å·¥å›¾çº¸ç†è§£æ–¹é¢çš„èƒ½åŠ›å°šä¸æ˜ç¡®ï¼Œè¿™äº›å›¾çº¸é€šè¿‡ç¬¦å·ã€å¸ƒå±€çº¦å®šå’Œå¯†é›†æ ‡æ³¨ç¼–ç å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œéœ€è¦è¯„ä¼°æ¨¡å‹æ˜¯å¦èƒ½å¯é è§£é‡Šè¿™ç§å›¾å½¢è¯­è¨€ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†AECV-BenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸¤ä¸ªäº’è¡¥ç”¨ä¾‹ï¼šåœ¨120ä¸ªé«˜è´¨é‡å¹³é¢å›¾ä¸Šè¿›è¡Œå¯¹è±¡è®¡æ•°ä»»åŠ¡ï¼Œä»¥åŠåŸºäº192ä¸ªé—®ç­”å¯¹çš„å›¾çº¸æ–‡æ¡£é—®ç­”ä»»åŠ¡ï¼Œåè€…æ¶µç›–æ–‡æœ¬æå–ã€å®ä¾‹è®¡æ•°ã€ç©ºé—´æ¨ç†å’Œæ¯”è¾ƒæ¨ç†ï¼Œé‡‡ç”¨LLMä½œä¸ºè¯„åˆ¤è€…çš„è¯„åˆ†æµç¨‹å’Œé’ˆå¯¹è¾¹ç¼˜æƒ…å†µçš„äººå·¥è£å†³ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°æ˜¾ç¤ºç¨³å®šçš„èƒ½åŠ›æ¢¯åº¦ï¼šOCRå’Œæ–‡æœ¬ä¸­å¿ƒæ–‡æ¡£é—®ç­”è¡¨ç°æœ€å¼ºï¼Œç©ºé—´æ¨ç†ä¸­ç­‰ï¼Œè€Œç¬¦å·ä¸­å¿ƒå›¾çº¸ç†è§£ç‰¹åˆ«æ˜¯é—¨çª—å¯é è®¡æ•°ä»ç„¶æœªè§£å†³ï¼Œå¯¹è±¡è®¡æ•°ä½¿ç”¨æ¯å­—æ®µç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡å’ŒMAPEç»“æœï¼Œæ–‡æ¡£é—®ç­”ä½¿ç”¨æ•´ä½“å‡†ç¡®ç‡å’Œæ¯ç±»åˆ«ç»†åˆ†ã€‚</p>
<p><strong>Conclusion:</strong> å½“å‰ç³»ç»Ÿä½œä¸ºæ–‡æ¡£åŠ©æ‰‹è¡¨ç°è‰¯å¥½ä½†ç¼ºä¹ç¨³å¥çš„å›¾çº¸ç†è§£èƒ½åŠ›ï¼Œè¿™æ¿€åŠ±äº†é¢†åŸŸç‰¹å®šè¡¨ç¤ºå’Œå·¥å…·å¢å¼ºã€äººåœ¨ç¯çš„å·¥ä½œæµç¨‹å¼€å‘ï¼Œä»¥å®ç°é«˜æ•ˆçš„AECè‡ªåŠ¨åŒ–ï¼Œå¼ºè°ƒäº†ç¬¦å·ä¸­å¿ƒå›¾çº¸ç†è§£çš„æŒ‘æˆ˜æ€§ã€‚</p>
<hr />
<h4 id="abstract_57">ğŸ“„ Abstract</h4>
<p>AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation.</p>
  </article>
</body>
</html>
