{"id": "2510.24767", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24767", "abs": "https://arxiv.org/abs/2510.24767", "authors": ["Guorui Song", "Guocun Wang", "Zhe Huang", "Jing Lin", "Xuefei Zhe", "Jian Li", "Haoqian Wang"], "title": "Towards Fine-Grained Human Motion Video Captioning", "comment": null, "summary": "Generating accurate descriptions of human actions in videos remains a\nchallenging task for video captioning models. Existing approaches often\nstruggle to capture fine-grained motion details, resulting in vague or\nsemantically inconsistent captions. In this work, we introduce the\nMotion-Augmented Caption Model (M-ACM), a novel generative framework that\nenhances caption quality by incorporating motion-aware decoding. At its core,\nM-ACM leverages motion representations derived from human mesh recovery to\nexplicitly highlight human body dynamics, thereby reducing hallucinations and\nimproving both semantic fidelity and spatial alignment in the generated\ncaptions. To support research in this area, we present the Human Motion Insight\n(HMI) Dataset, comprising 115K video-description pairs focused on human\nmovement, along with HMI-Bench, a dedicated benchmark for evaluating\nmotion-focused video captioning. Experimental results demonstrate that M-ACM\nsignificantly outperforms previous methods in accurately describing complex\nhuman motions and subtle temporal variations, setting a new standard for\nmotion-centric video captioning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8fd0\u52a8\u589e\u5f3a\u5b57\u5e55\u6a21\u578b\uff08M-ACM\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u4eba\u4f53\u7f51\u683c\u6062\u590d\u7684\u8fd0\u52a8\u611f\u77e5\u89e3\u7801\u6765\u63d0\u5347\u89c6\u9891\u5b57\u5e55\u8d28\u91cf\uff0c\u5e76\u53d1\u5e03\u4e86\u4e13\u6ce8\u4e8e\u4eba\u4f53\u8fd0\u52a8\u7684HMI\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u590d\u6742\u4eba\u4f53\u52a8\u4f5c\u63cf\u8ff0\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5b57\u5e55\u6a21\u578b\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u5b57\u5e55\u6a21\u7cca\u6216\u8bed\u4e49\u4e0d\u4e00\u81f4\uff0c\u65e0\u6cd5\u51c6\u786e\u63cf\u8ff0\u4eba\u4f53\u52a8\u4f5c\u7684\u52a8\u6001\u7279\u5f81\u3002", "method": "M-ACM\u6846\u67b6\u5229\u7528\u4ece\u4eba\u4f53\u7f51\u683c\u6062\u590d\u4e2d\u63d0\u53d6\u7684\u8fd0\u52a8\u8868\u793a\u6765\u663e\u5f0f\u7a81\u51fa\u4eba\u4f53\u52a8\u6001\uff0c\u901a\u8fc7\u8fd0\u52a8\u611f\u77e5\u89e3\u7801\u673a\u5236\u51cf\u5c11\u5e7b\u89c9\u5e76\u6539\u5584\u751f\u6210\u5b57\u5e55\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u7a7a\u95f4\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eM-ACM\u5728\u51c6\u786e\u63cf\u8ff0\u590d\u6742\u4eba\u4f53\u52a8\u4f5c\u548c\u7ec6\u5fae\u65f6\u95f4\u53d8\u5316\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u4e3a\u8fd0\u52a8\u4e2d\u5fc3\u89c6\u9891\u5b57\u5e55\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u8fd0\u52a8\u8868\u793a\u5728\u89c6\u9891\u5b57\u5e55\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\u4e3a\u8fd0\u52a8\u611f\u77e5\u89c6\u9891\u7406\u89e3\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u63a8\u52a8\u4e86\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u63cf\u8ff0\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24777", "categories": ["cs.CV", "cs.AI", "eess.IV", "68T07", "I.2; H.5.1"], "pdf": "https://arxiv.org/pdf/2510.24777", "abs": "https://arxiv.org/abs/2510.24777", "authors": ["Yujie Nie", "Jianzhang Ni", "Yonglong Ye", "Yuan-Ting Zhang", "Yun Kwok Wing", "Xiangqing Xu", "Xin Ma", "Lizhou Fan"], "title": "Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis", "comment": "35 pages, 8 figures, and 7 tables", "summary": "Accurate diagnosis of Alzheimer's disease (AD) is essential for enabling\ntimely intervention and slowing disease progression. Multimodal diagnostic\napproaches offer considerable promise by integrating complementary information\nacross behavioral and perceptual domains. Eye-tracking and facial features, in\nparticular, are important indicators of cognitive function, reflecting\nattentional distribution and neurocognitive state. However, few studies have\nexplored their joint integration for auxiliary AD diagnosis. In this study, we\npropose a multimodal cross-enhanced fusion framework that synergistically\nleverages eye-tracking and facial features for AD detection. The framework\nincorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module\n(CEFAM), which models inter-modal interactions through cross-attention and\nglobal enhancement, and (b) a Direction-Aware Convolution Module (DACM), which\ncaptures fine-grained directional facial features via horizontal-vertical\nreceptive fields. Together, these modules enable adaptive and discriminative\nmultimodal representation learning. To support this work, we constructed a\nsynchronized multimodal dataset, including 25 patients with AD and 25 healthy\ncontrols (HC), by recording aligned facial video and eye-tracking sequences\nduring a visual memory-search paradigm, providing an ecologically valid\nresource for evaluating integration strategies. Extensive experiments on this\ndataset demonstrate that our framework outperforms traditional late fusion and\nfeature concatenation methods, achieving a classification accuracy of 95.11% in\ndistinguishing AD from HC, highlighting superior robustness and diagnostic\nperformance by explicitly modeling inter-modal dependencies and\nmodality-specific contributions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u4ea4\u53c9\u589e\u5f3a\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u5229\u7528\u773c\u52a8\u8ffd\u8e2a\u548c\u9762\u90e8\u7279\u5f81\u8fdb\u884c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\uff0c\u8be5\u6846\u67b6\u5728\u533a\u5206AD\u60a3\u8005\u4e0e\u5065\u5eb7\u5bf9\u7167\u65f6\u8fbe\u5230\u4e8695.11%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\uff0c\u867d\u7136\u591a\u6a21\u6001\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u884c\u4e3a\u548c\u611f\u77e5\u9886\u57df\u7684\u4e92\u8865\u4fe1\u606f\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u63a2\u7d22\u773c\u52a8\u8ffd\u8e2a\u548c\u9762\u90e8\u7279\u5f81\u7684\u8054\u5408\u96c6\u6210\u7528\u4e8e\u8f85\u52a9AD\u8bca\u65ad\uff0c\u8fd9\u9650\u5236\u4e86\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u4ea4\u53c9\u589e\u5f3a\u878d\u5408\u6ce8\u610f\u529b\u6a21\u5757\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u5168\u5c40\u589e\u5f3a\u5efa\u6a21\u6a21\u6001\u95f4\u4ea4\u4e92\uff0c\u65b9\u5411\u611f\u77e5\u5377\u79ef\u6a21\u5757\u901a\u8fc7\u6c34\u5e73-\u5782\u76f4\u611f\u53d7\u91ce\u6355\u83b7\u7ec6\u7c92\u5ea6\u65b9\u5411\u6027\u9762\u90e8\u7279\u5f81\uff0c\u5171\u540c\u5b9e\u73b0\u81ea\u9002\u5e94\u548c\u5224\u522b\u6027\u7684\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u5305\u542b25\u540dAD\u60a3\u8005\u548c25\u540d\u5065\u5eb7\u5bf9\u7167\u7684\u540c\u6b65\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e\u4f20\u7edf\u7684\u540e\u671f\u878d\u5408\u548c\u7279\u5f81\u62fc\u63a5\u65b9\u6cd5\uff0c\u5728\u533a\u5206AD\u4e0eHC\u65f6\u8fbe\u523095.11%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u663e\u793a\u51fa\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6a21\u6001\u95f4\u4f9d\u8d56\u5173\u7cfb\u548c\u6a21\u6001\u7279\u5b9a\u8d21\u732e\u7684\u4f18\u8d8a\u9c81\u68d2\u6027\u548c\u8bca\u65ad\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6a21\u6001\u95f4\u4ea4\u4e92\u548c\u6355\u83b7\u7ec6\u7c92\u5ea6\u7279\u5f81\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\u5728AD\u8bca\u65ad\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u51c6\u786e\u3001\u9c81\u68d2\u7684\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u6784\u5efa\u4e86\u751f\u6001\u6709\u6548\u7684\u591a\u6a21\u6001\u8d44\u6e90\u7528\u4e8e\u8bc4\u4f30\u96c6\u6210\u7b56\u7565\u3002"}}
{"id": "2510.24792", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24792", "abs": "https://arxiv.org/abs/2510.24792", "authors": ["Patrick Haller", "Fabio Barth", "Jonas Golde", "Georg Rehm", "Alan Akbik"], "title": "PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models", "comment": "8 pages, 11 tables and figures", "summary": "Vision-language models (VLMs) have demonstrated remarkable progress in\nmultimodal reasoning. However, existing benchmarks remain limited in terms of\nhigh-quality, human-verified examples. Many current datasets rely on\nsynthetically generated content by large language models (LLMs). Furthermore,\nmost datasets are limited to English, as manual quality assurance of translated\nsamples is time-consuming and costly. To fill this gap, we introduce\nPISA-Bench, a multilingual benchmark derived from English examples of the\nexpert-created PISA tests, a unified framework for the assessment of student\ncompetencies in over eighty countries. Each example consists of human-extracted\ninstructions, questions, answer options, and images, enriched with question\ntype categories, and has been translated from English into five additional\nlanguages (Spanish, German, Chinese, French, and Italian), resulting in a fully\nparallel corpus covering six languages. We evaluate state-of-the-art\nvision-language models on PISA-Bench and find that especially small models\n(<20B parameters) fail to achieve high test scores. We further find substantial\nperformance degradation on non-English splits as well as high error-rates when\nmodels are tasked with spatial and geometric reasoning. By releasing the\ndataset and evaluation framework, we provide a resource for advancing research\non multilingual multimodal reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PISA-Bench\uff0c\u4e00\u4e2a\u57fa\u4e8e\u4e13\u5bb6\u521b\u5efa\u7684PISA\u6d4b\u8bd5\u6784\u5efa\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\uff0c\u5305\u542b\u516d\u4e2a\u8bed\u8a00\u7684\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u5b58\u5728\u9ad8\u8d28\u91cf\u4eba\u5de5\u9a8c\u8bc1\u6837\u672c\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8bb8\u591a\u6570\u636e\u96c6\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u5408\u6210\u751f\u6210\u5185\u5bb9\uff0c\u4e14\u5927\u591a\u6570\u4ec5\u9650\u4e8e\u82f1\u8bed\uff0c\u591a\u8bed\u8a00\u7ffb\u8bd1\u6837\u672c\u7684\u8d28\u91cf\u4fdd\u8bc1\u8017\u65f6\u4e14\u6210\u672c\u9ad8\u6602\u3002", "method": "\u57fa\u4e8e\u82f1\u8bedPISA\u6d4b\u8bd5\u4e13\u5bb6\u521b\u5efa\u7684\u4f8b\u5b50\u6784\u5efa\u591a\u8bed\u8a00\u57fa\u51c6\uff0c\u6bcf\u4e2a\u4f8b\u5b50\u5305\u542b\u4eba\u5de5\u63d0\u53d6\u7684\u6307\u4ee4\u3001\u95ee\u9898\u3001\u7b54\u6848\u9009\u9879\u548c\u56fe\u50cf\uff0c\u5e76\u6dfb\u52a0\u95ee\u9898\u7c7b\u578b\u5206\u7c7b\uff0c\u4ece\u82f1\u8bed\u7ffb\u8bd1\u5230\u4e94\u79cd\u989d\u5916\u8bed\u8a00\uff08\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u4e2d\u6587\u3001\u6cd5\u8bed\u548c\u610f\u5927\u5229\u8bed\uff09\uff0c\u5f62\u6210\u8986\u76d6\u516d\u79cd\u8bed\u8a00\u7684\u5b8c\u5168\u5e73\u884c\u8bed\u6599\u5e93\u3002", "result": "\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d1\u73b0\uff0c\u7279\u522b\u662f\u5c0f\u578b\u6a21\u578b\uff08<200\u4ebf\u53c2\u6570\uff09\u65e0\u6cd5\u83b7\u5f97\u9ad8\u6d4b\u8bd5\u5206\u6570\uff0c\u5728\u975e\u82f1\u8bed\u5206\u5272\u4e0a\u5b58\u5728\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff0c\u5728\u7a7a\u95f4\u548c\u51e0\u4f55\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u9ad8\u9519\u8bef\u7387\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u63a8\u8fdb\u591a\u8bed\u8a00\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.24795", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24795", "abs": "https://arxiv.org/abs/2510.24795", "authors": ["Zhaoshu Yu", "Bo Wang", "Pengpeng Zeng", "Haonan Zhang", "Ji Zhang", "Lianli Gao", "Jingkuan Song", "Nicu Sebe", "Heng Tao Shen"], "title": "A Survey on Efficient Vision-Language-Action Models", "comment": "26 pages, 8 figures", "summary": "Vision-Language-Action models (VLAs) represent a significant frontier in\nembodied intelligence, aiming to bridge digital knowledge with physical-world\ninteraction. While these models have demonstrated remarkable generalist\ncapabilities, their deployment is severely hampered by the substantial\ncomputational and data requirements inherent to their underlying large-scale\nfoundation models. Motivated by the urgent need to address these challenges,\nthis survey presents the first comprehensive review of Efficient\nVision-Language-Action models (Efficient VLAs) across the entire\ndata-model-training process. Specifically, we introduce a unified taxonomy to\nsystematically organize the disparate efforts in this domain, categorizing\ncurrent techniques into three core pillars: (1) Efficient Model Design,\nfocusing on efficient architectures and model compression; (2) Efficient\nTraining, which reduces computational burdens during model learning; and (3)\nEfficient Data Collection, which addresses the bottlenecks in acquiring and\nutilizing robotic data. Through a critical review of state-of-the-art methods\nwithin this framework, this survey not only establishes a foundational\nreference for the community but also summarizes representative applications,\ndelineates key challenges, and charts a roadmap for future research. We\nmaintain a continuously updated project page to track our latest developments:\nhttps://evla-survey.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5173\u4e8e\u9ad8\u6548\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u901a\u8fc7\u5efa\u7acb\u7edf\u4e00\u5206\u7c7b\u6cd5\u5c06\u73b0\u6709\u6280\u672f\u7ec4\u7ec7\u4e3a\u4e09\u4e2a\u6838\u5fc3\u652f\u67f1\uff0c\u4e3a\u793e\u533a\u5efa\u7acb\u4e86\u57fa\u7840\u53c2\u8003\u6846\u67b6\u5e76\u89c4\u5212\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5177\u8eab\u667a\u80fd\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u90e8\u7f72\u53d7\u5230\u5e95\u5c42\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u5de8\u5927\u8ba1\u7b97\u548c\u6570\u636e\u9700\u6c42\u7684\u4e25\u91cd\u5236\u7ea6\uff0c\u8feb\u5207\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6548\u7387\u6311\u6218\u3002", "method": "\u5f15\u5165\u7edf\u4e00\u5206\u7c7b\u6cd5\u5c06\u9ad8\u6548VLA\u6280\u672f\u7cfb\u7edf\u7ec4\u7ec7\u4e3a\u4e09\u4e2a\u6838\u5fc3\u652f\u67f1\uff1a\u9ad8\u6548\u6a21\u578b\u8bbe\u8ba1\uff08\u5173\u6ce8\u9ad8\u6548\u67b6\u6784\u548c\u6a21\u578b\u538b\u7f29\uff09\u3001\u9ad8\u6548\u8bad\u7ec3\uff08\u51cf\u5c11\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u8d1f\u62c5\uff09\u4ee5\u53ca\u9ad8\u6548\u6570\u636e\u6536\u96c6\uff08\u89e3\u51b3\u673a\u5668\u4eba\u6570\u636e\u83b7\u53d6\u548c\u5229\u7528\u7684\u74f6\u9888\uff09\u3002", "result": "\u901a\u8fc7\u5728\u6b64\u6846\u67b6\u5185\u5bf9\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u6279\u5224\u6027\u56de\u987e\uff0c\u4e0d\u4ec5\u4e3a\u793e\u533a\u5efa\u7acb\u4e86\u57fa\u7840\u53c2\u8003\uff0c\u8fd8\u603b\u7ed3\u4e86\u4ee3\u8868\u6027\u5e94\u7528\uff0c\u754c\u5b9a\u4e86\u5173\u952e\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u7ed8\u5236\u4e86\u8def\u7ebf\u56fe\u3002", "conclusion": "\u8be5\u8c03\u67e5\u786e\u7acb\u4e86\u9ad8\u6548VLA\u9886\u57df\u7684\u7cfb\u7edf\u6027\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u652f\u67f1\u5206\u7c7b\u6cd5\u6574\u5408\u4e86\u5206\u6563\u7684\u7814\u7a76\u5de5\u4f5c\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u65b9\u5411\uff0c\u5e76\u7ef4\u62a4\u6301\u7eed\u66f4\u65b0\u7684\u9879\u76ee\u9875\u9762\u4ee5\u8ddf\u8e2a\u6700\u65b0\u8fdb\u5c55\u3002"}}
{"id": "2510.24760", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24760", "abs": "https://arxiv.org/abs/2510.24760", "authors": ["Mengyuan Chen", "Chengjun Dai", "Xinyang Dong", "Chengzhe Feng", "Kewei Fu", "Jianshe Li", "Zhihan Peng", "Yongqi Tong", "Junshao Zhang", "Hong Zhu"], "title": "Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments", "comment": null, "summary": "We present Dingtalk DeepResearch, a unified multi agent intelligence\nframework for real world enterprise environments, delivering deep research,\nheterogeneous table reasoning, and multimodal report generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9489\u9489DeepResearch\uff0c\u4e00\u4e2a\u9762\u5411\u4f01\u4e1a\u73af\u5883\u7684\u7edf\u4e00\u591a\u667a\u80fd\u4f53\u667a\u80fd\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u6df1\u5ea6\u7814\u7a76\u3001\u5f02\u6784\u8868\u683c\u63a8\u7406\u548c\u591a\u6a21\u6001\u62a5\u544a\u751f\u6210\u3002\u8be5\u6846\u67b6\u65e8\u5728\u89e3\u51b3\u4f01\u4e1a\u73af\u5883\u4e2d\u590d\u6742\u4fe1\u606f\u5904\u7406\u548c\u5206\u6790\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u4f01\u4e1a\u73af\u5883\u9762\u4e34\u4fe1\u606f\u5904\u7406\u590d\u6742\u3001\u591a\u6e90\u5f02\u6784\u6570\u636e\u6574\u5408\u56f0\u96be\u4ee5\u53ca\u6df1\u5ea6\u5206\u6790\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u5904\u7406\u591a\u6837\u5316\u4f01\u4e1a\u667a\u80fd\u4efb\u52a1\u7684\u6846\u67b6\u6765\u63d0\u5347\u4f01\u4e1a\u51b3\u7b56\u6548\u7387\u548c\u5206\u6790\u80fd\u529b\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u667a\u80fd\u6846\u67b6\uff0c\u6574\u5408\u4e86\u6df1\u5ea6\u7814\u7a76\u3001\u5f02\u6784\u8868\u683c\u63a8\u7406\u548c\u591a\u6a21\u6001\u62a5\u544a\u751f\u6210\u80fd\u529b\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u534f\u540c\u5de5\u4f5c\u6765\u5904\u7406\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u590d\u6742\u4fe1\u606f\u5206\u6790\u4efb\u52a1\u3002", "result": "\u6846\u67b6\u5728\u5b9e\u9645\u4f01\u4e1a\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u4fe1\u606f\u5904\u7406\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u6574\u5408\u591a\u6e90\u6570\u636e\u3001\u8fdb\u884c\u6df1\u5ea6\u5206\u6790\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u7814\u7a76\u62a5\u544a\uff0c\u63d0\u5347\u4e86\u4f01\u4e1a\u667a\u80fd\u51b3\u7b56\u7684\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728\u4f01\u4e1a\u667a\u80fd\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u7edf\u4e00\u7684\u4f01\u4e1a\u667a\u80fd\u5206\u6790\u5e73\u53f0\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u548c\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2510.25091", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25091", "abs": "https://arxiv.org/abs/2510.25091", "authors": ["Peilin Tan", "Liang Xie", "Churan Zhi", "Dian Tu", "Chuanqi Shi"], "title": "H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured Mixture of Experts", "comment": null, "summary": "Stock movement prediction remains fundamentally challenging due to complex\ntemporal dependencies, heterogeneous modalities, and dynamically evolving\ninter-stock relationships. Existing approaches often fail to unify structural,\nsemantic, and regime-adaptive modeling within a scalable framework. This work\nintroduces H3M-SSMoEs, a novel Hypergraph-based MultiModal architecture with\nLLM reasoning and Style-Structured Mixture of Experts, integrating three key\ninnovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically\ncaptures fine-grained spatiotemporal dynamics via a Local Context Hypergraph\n(LCH) and persistent inter-stock dependencies through a Global Context\nHypergraph (GCH), employing shared cross-modal hyperedges and Jensen-Shannon\nDivergence weighting mechanism for adaptive relational learning and cross-modal\nalignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large\nlanguage model with lightweight adapters to semantically fuse and align\nquantitative and textual modalities, enriching representations with\ndomain-specific financial knowledge; and (3) a Style-Structured Mixture of\nExperts (SSMoEs) that combines shared market experts and industry-specialized\nexperts, each parameterized by learnable style vectors enabling regime-aware\nspecialization under sparse activation. Extensive experiments on three major\nstock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in\nboth superior predictive accuracy and investment performance, while exhibiting\neffective risk control. Datasets, source code, and model weights are available\nat our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faH3M-SSMoEs\u6a21\u578b\uff0c\u901a\u8fc7\u8d85\u56fe\u591a\u6a21\u6001\u67b6\u6784\u7ed3\u5408LLM\u63a8\u7406\u548c\u98ce\u683c\u7ed3\u6784\u5316\u4e13\u5bb6\u6df7\u5408\uff0c\u89e3\u51b3\u4e86\u80a1\u7968\u9884\u6d4b\u4e2d\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\u3001\u5f02\u6784\u6a21\u6001\u548c\u52a8\u6001\u80a1\u7968\u5173\u7cfb\u7684\u7edf\u4e00\u5efa\u6a21\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u4e3b\u8981\u80a1\u7968\u5e02\u573a\u5b9e\u73b0\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u6295\u8d44\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u80a1\u7968\u8fd0\u52a8\u9884\u6d4b\u9762\u4e34\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\u3001\u5f02\u6784\u6a21\u6001\u548c\u52a8\u6001\u6f14\u5316\u7684\u80a1\u7968\u95f4\u5173\u7cfb\u7684\u6839\u672c\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u53ef\u6269\u5c55\u6846\u67b6\u5185\u7edf\u4e00\u7ed3\u6784\u3001\u8bed\u4e49\u548c\u673a\u5236\u81ea\u9002\u5e94\u5efa\u6a21\uff0c\u5b58\u5728\u5efa\u6a21\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faH3M-SSMoEs\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u591a\u4e0a\u4e0b\u6587\u591a\u6a21\u6001\u8d85\u56fe\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u8d85\u56fe\u5206\u5c42\u6355\u6349\u65f6\u7a7a\u52a8\u6001\u548c\u6301\u4e45\u80a1\u7968\u4f9d\u8d56\uff0c\u91c7\u7528\u5171\u4eab\u8de8\u6a21\u6001\u8d85\u8fb9\u548cJensen-Shannon\u6563\u5ea6\u52a0\u6743\u673a\u5236\uff1bLLM\u589e\u5f3a\u63a8\u7406\u6a21\u5757\u5229\u7528\u51bb\u7ed3\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8f7b\u91cf\u9002\u914d\u5668\u8bed\u4e49\u878d\u5408\u91cf\u5316\u4e0e\u6587\u672c\u6a21\u6001\uff1b\u98ce\u683c\u7ed3\u6784\u5316\u4e13\u5bb6\u6df7\u5408\u7ed3\u5408\u5171\u4eab\u5e02\u573a\u4e13\u5bb6\u548c\u884c\u4e1a\u4e13\u4e1a\u4e13\u5bb6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u98ce\u683c\u5411\u91cf\u5b9e\u73b0\u673a\u5236\u611f\u77e5\u4e13\u4e1a\u5316\u3002", "result": "\u5728\u4e09\u4e2a\u4e3b\u8981\u80a1\u7968\u5e02\u573a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cH3M-SSMoEs\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u6295\u8d44\u6027\u80fd\u4e0a\u5747\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u6709\u6548\u7684\u98ce\u9669\u63a7\u5236\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u5e02\u573a\u73af\u5883\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u7edf\u4e00\u7ed3\u6784\u3001\u8bed\u4e49\u548c\u673a\u5236\u81ea\u9002\u5e94\u5efa\u6a21\u5728\u590d\u6742\u91d1\u878d\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u65f6\u5e8f\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86LLM\u589e\u5f3a\u63a8\u7406\u548c\u98ce\u683c\u7ed3\u6784\u5316\u4e13\u5bb6\u6df7\u5408\u5728\u91d1\u878d\u9886\u57df\u5e94\u7528\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u6295\u8d44\u51b3\u7b56\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.24804", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24804", "abs": "https://arxiv.org/abs/2510.24804", "authors": ["Xiaoyang Hu"], "title": "Conflict Adaptation in Vision-Language Models", "comment": "Workshop on Interpreting Cognition in Deep Learning Models at NeurIPS\n  2025", "summary": "A signature of human cognitive control is conflict adaptation: improved\nperformance on a high-conflict trial following another high-conflict trial.\nThis phenomenon offers an account for how cognitive control, a scarce resource,\nis recruited. Using a sequential Stroop task, we find that 12 of 13\nvision-language models (VLMs) tested exhibit behavior consistent with conflict\nadaptation, with the lone exception likely reflecting a ceiling effect. To\nunderstand the representational basis of this behavior, we use sparse\nautoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B.\nPartially overlapping supernodes emerge for text and color in both early and\nlate layers, and their relative sizes mirror the automaticity asymmetry between\nreading and color naming in humans. We further isolate a conflict-modulated\nsupernode in layers 24-25 whose ablation significantly increases Stroop errors\nwhile minimally affecting congruent trials.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u987a\u5e8fStroop\u4efb\u52a1\u53d1\u73b012\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u51b2\u7a81\u9002\u5e94\u4e00\u81f4\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728InternVL 3.5 4B\u4e2d\u8bc6\u522b\u51fa\u8d1f\u8d23\u51b2\u7a81\u8c03\u5236\u7684\u5173\u952e\u795e\u7ecf\u5143\uff0c\u63ed\u793a\u4e86VLMs\u8ba4\u77e5\u63a7\u5236\u673a\u5236\u7684\u795e\u7ecf\u57fa\u7840\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u8ba4\u77e5\u63a7\u5236\u7684\u51b2\u7a81\u9002\u5e94\u73b0\u8c61\uff0c\u5373\u5728\u9ad8\u51b2\u7a81\u8bd5\u6b21\u540e\u6027\u80fd\u63d0\u5347\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u4ee5\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u52a8\u6001\u8c03\u6574\u5176\u7a00\u7f3a\u7684\u8ba4\u77e5\u8d44\u6e90\u3002", "method": "\u7814\u7a76\u91c7\u7528\u987a\u5e8fStroop\u4efb\u52a1\u8bc4\u4f3013\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u8868\u73b0\uff0c\u5e76\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728InternVL 3.5 4B\u6a21\u578b\u4e2d\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7684\u8d85\u8282\u70b9\uff0c\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u7279\u5b9a\u795e\u7ecf\u5143\u7684\u529f\u80fd\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b012\u4e2aVLMs\u8868\u73b0\u51fa\u663e\u8457\u7684\u51b2\u7a81\u9002\u5e94\u884c\u4e3a\uff0c\u4ec5\u6709\u4e00\u4e2a\u6a21\u578b\u56e0\u5929\u82b1\u677f\u6548\u5e94\u672a\u663e\u793a\u8be5\u6a21\u5f0f\uff1b\u5728InternVL 3.5 4B\u4e2d\u8bc6\u522b\u51fa\u65e9\u671f\u548c\u665a\u671f\u5c42\u4e2d\u90e8\u5206\u91cd\u53e0\u7684\u6587\u672c\u548c\u989c\u8272\u8d85\u8282\u70b9\uff0c\u5176\u76f8\u5bf9\u5927\u5c0f\u53cd\u6620\u4e86\u4eba\u7c7b\u9605\u8bfb\u4e0e\u989c\u8272\u547d\u540d\u7684\u81ea\u52a8\u6027\u4e0d\u5bf9\u79f0\uff0c\u5e76\u572824-25\u5c42\u53d1\u73b0\u51b2\u7a81\u8c03\u5236\u8d85\u8282\u70b9\uff0c\u5176\u6d88\u878d\u663e\u8457\u589e\u52a0Stroop\u9519\u8bef\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7cfb\u7edf\u6027\u5730\u8bc1\u660e\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8ba4\u77e5\u63a7\u5236\u673a\u5236\uff0c\u63ed\u793a\u4e86VLMs\u5185\u90e8\u8868\u5f81\u7ed3\u6784\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u76f8\u4f3c\u6027\uff0c\u4e3a\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba4\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u795e\u7ecf\u8ba1\u7b97\u57fa\u7840\u3002"}}
{"id": "2510.24870", "categories": ["cs.CL", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24870", "abs": "https://arxiv.org/abs/2510.24870", "authors": ["Alexander Martin", "William Walden", "Reno Kriz", "Dengjia Zhang", "Kate Sanders", "Eugene Yang", "Chihsheng Jin", "Benjamin Van Durme"], "title": "Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation", "comment": "https://github.com/alexmartin1722/mirage", "summary": "We introduce MiRAGE, an evaluation framework for retrieval-augmented\ngeneration (RAG) from multimodal sources. As audiovisual media becomes a\nprevalent source of information online, it is essential for RAG systems to\nintegrate information from these sources into generation. However, existing\nevaluations for RAG are text-centric, limiting their applicability to\nmultimodal, reasoning intensive settings because they don't verify information\nagainst sources. MiRAGE is a claim-centric approach to multimodal RAG\nevaluation, consisting of InfoF1, evaluating factuality and information\ncoverage, and CiteF1, measuring citation support and completeness. We show that\nMiRAGE, when applied by humans, strongly aligns with extrinsic quality\njudgments. We additionally introduce automatic variants of MiRAGE and three\nprominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the\nlimitations of text-centric work and laying the groundwork for automatic\nevaluation. We release open-source implementations and outline how to assess\nmultimodal RAG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MiRAGE\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165InfoF1\u548cCiteF1\u6307\u6807\u6765\u89e3\u51b3\u73b0\u6709\u6587\u672c\u4e2d\u5fc3\u8bc4\u4f30\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u63a8\u7406\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u89c6\u542c\u5a92\u4f53\u6210\u4e3a\u5728\u7ebf\u4fe1\u606f\u7684\u91cd\u8981\u6765\u6e90\uff0cRAG\u7cfb\u7edf\u9700\u8981\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u8fdb\u884c\u751f\u6210\uff0c\u4f46\u73b0\u6709\u7684RAG\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u4e2d\u5fc3\u573a\u666f\uff0c\u65e0\u6cd5\u9a8c\u8bc1\u591a\u6a21\u6001\u6765\u6e90\u7684\u4fe1\u606f\u652f\u6301\uff0c\u9650\u5236\u4e86\u5728\u591a\u6a21\u6001\u63a8\u7406\u5bc6\u96c6\u578b\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "MiRAGE\u91c7\u7528\u58f0\u660e\u4e2d\u5fc3\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u542bInfoF1\u6307\u6807\u8bc4\u4f30\u4e8b\u5b9e\u6027\u548c\u4fe1\u606f\u8986\u76d6\u7387\uff0c\u4ee5\u53caCiteF1\u6307\u6807\u8861\u91cf\u5f15\u7528\u652f\u6301\u548c\u5b8c\u6574\u6027\uff1b\u540c\u65f6\u5f15\u5165\u4e86MiRAGE\u7684\u81ea\u52a8\u53d8\u4f53\u548c\u4e09\u79cd\u4e3b\u6d41TextRAG\u6307\u6807\uff08ACLE\u3001ARGUE\u3001RAGAS\uff09\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4eba\u5de5\u5e94\u7528MiRAGE\u6846\u67b6\u65f6\u4e0e\u5916\u90e8\u8d28\u91cf\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff1b\u901a\u8fc7\u5bf9\u6bd4\u6587\u672c\u4e2d\u5fc3\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5176\u5728\u591a\u6a21\u6001\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001RAG\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5e76\u660e\u786e\u4e86\u5982\u4f55\u6709\u6548\u8bc4\u4f30\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u8d28\u91cf\u3002"}}
{"id": "2510.25101", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25101", "abs": "https://arxiv.org/abs/2510.25101", "authors": ["Zhuo Chen", "Fei Wang", "Zixuan Li", "Zhao Zhang", "Weiwei Ding", "Chuanguang Yang", "Yongjun Xu", "Xiaolong Jin", "Jiafeng Guo"], "title": "KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA", "comment": null, "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural-language\nquestions over a structured Knowledge Base (KB). Recent work improves KBQA by\nadopting an agentic reasoning paradigm, in which Large Language Models (LLMs)\niteratively decompose a question, generate its corresponding logical queries,\nand interact with the KB to derive the answer. However, these methods typically\nfine-tune LLMs on reasoning trajectories synthesized via process supervision,\nwhich offers weak incentives for exploration and thus fails to strengthen the\nagentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that\ncan autonomously perform agentic reasoning on KBs to obtain answers. To\nincentivize autonomous exploration, KnowCoder-A1 trains the LLM under\noutcome-only supervision via a multi-stage curriculum reinforcement learning\nwith an easy-to-hard curriculum. To establish foundational agentic\ncapabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of\nhigh-quality trajectories obtained through outcome-based rejection sampling.\nThen, to alleviate the reward sparsity inherent in outcome-only supervision, it\napplies multi-stage curriculum RL with reward schedules that progress from easy\nto hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful\nreasoning behaviors and consistently outperforms prior approaches across three\nmainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1\nachieves up to an 11.1% relative improvement while using only one-twelfth of\nthe training data, demonstrating strong agentic reasoning capabilities.", "AI": {"tldr": "KnowCoder-A1\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u679c\u76d1\u7763\u7684\u591a\u9636\u6bb5\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3LLM\u5728\u77e5\u8bc6\u5e93\u4e0a\u8fdb\u884c\u81ea\u4e3b\u4ee3\u7406\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86KBQA\u6027\u80fd\uff0c\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u5b9e\u73b0\u4e8611.1%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709KBQA\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u8fc7\u7a0b\u76d1\u7763\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u8fd9\u79cd\u76d1\u7763\u65b9\u5f0f\u63d0\u4f9b\u8f83\u5f31\u7684\u63a2\u7d22\u6fc0\u52b1\uff0c\u65e0\u6cd5\u6709\u6548\u589e\u5f3a\u4ee3\u7406\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6fc0\u52b1\u81ea\u4e3b\u63a2\u7d22\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u591a\u9636\u6bb5\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u9996\u5148\u901a\u8fc7\u57fa\u4e8e\u7ed3\u679c\u7684\u62d2\u7edd\u91c7\u6837\u83b7\u5f97\u9ad8\u8d28\u91cf\u8f68\u8ff9\u8fdb\u884c\u57fa\u7840\u5fae\u8c03\uff0c\u7136\u540e\u5e94\u7528\u4ece\u6613\u5230\u96be\u7684\u5956\u52b1\u8c03\u5ea6\u7b56\u7565\u6765\u7f13\u89e3\u7ed3\u679c\u76d1\u7763\u4e2d\u7684\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898\u3002", "result": "KnowCoder-A1\u5728\u4e09\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728GrailQA\u7684\u96f6\u6837\u672c\u5b50\u96c6\u4e0a\u5b9e\u73b0\u4e8611.1%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u540c\u65f6\u4ec5\u4f7f\u7528\u5341\u4e8c\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u57fa\u4e8e\u7ed3\u679c\u76d1\u7763\u7684\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u6709\u6548\u57f9\u517bLLM\u7684\u81ea\u4e3b\u4ee3\u7406\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u77e5\u8bc6\u5e93\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u5728\u6709\u9650\u76d1\u7763\u4e0b\u5b9e\u73b0\u5f3a\u5927\u63a8\u7406\u6027\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24813", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24813", "abs": "https://arxiv.org/abs/2510.24813", "authors": ["Binbin Li", "Guimiao Yang", "Zisen Qi", "Haiping Wang", "Yu Ding"], "title": "DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts", "comment": null, "summary": "Recent lightweight retrieval-augmented image caption models often utilize\nretrieved data solely as text prompts, thereby creating a semantic gap by\nleaving the original visual features unenhanced, particularly for object\ndetails or complex scenes. To address this limitation, we propose $DualCap$, a\nnovel approach that enriches the visual representation by generating a visual\nprompt from retrieved similar images. Our model employs a dual retrieval\nmechanism, using standard image-to-text retrieval for text prompts and a novel\nimage-to-image retrieval to source visually analogous scenes. Specifically,\nsalient keywords and phrases are derived from the captions of visually similar\nscenes to capture key objects and similar details. These textual features are\nthen encoded and integrated with the original image features through a\nlightweight, trainable feature fusion network. Extensive experiments\ndemonstrate that our method achieves competitive performance while requiring\nfewer trainable parameters compared to previous visual-prompting captioning\napproaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DualCap\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u68c0\u7d22\u673a\u5236\u751f\u6210\u89c6\u89c9\u63d0\u793a\u6765\u589e\u5f3a\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u7684\u89c6\u89c9\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4ec5\u5c06\u68c0\u7d22\u6570\u636e\u7528\u4f5c\u6587\u672c\u63d0\u793a\u800c\u5ffd\u7565\u539f\u59cb\u89c6\u89c9\u7279\u5f81\u589e\u5f3a\u7684\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8f7b\u91cf\u7ea7\u68c0\u7d22\u589e\u5f3a\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u901a\u5e38\u4ec5\u5c06\u68c0\u7d22\u6570\u636e\u7528\u4f5c\u6587\u672c\u63d0\u793a\uff0c\u5bfc\u81f4\u539f\u59cb\u89c6\u89c9\u7279\u5f81\u672a\u5f97\u5230\u589e\u5f3a\uff0c\u5728\u5bf9\u8c61\u7ec6\u8282\u548c\u590d\u6742\u573a\u666f\u7406\u89e3\u65b9\u9762\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u3002", "method": "\u63d0\u51faDualCap\u65b9\u6cd5\uff0c\u91c7\u7528\u53cc\u68c0\u7d22\u673a\u5236\uff1a\u6807\u51c6\u56fe\u50cf\u5230\u6587\u672c\u68c0\u7d22\u7528\u4e8e\u6587\u672c\u63d0\u793a\uff0c\u65b0\u9896\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u68c0\u7d22\u7528\u4e8e\u83b7\u53d6\u89c6\u89c9\u76f8\u4f3c\u573a\u666f\u3002\u4ece\u89c6\u89c9\u76f8\u4f3c\u573a\u666f\u7684\u6807\u9898\u4e2d\u63d0\u53d6\u5173\u952e\u8bcd\u8bed\u548c\u77ed\u8bed\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u7279\u5f81\u878d\u5408\u7f51\u7edc\u5c06\u8fd9\u4e9b\u6587\u672c\u7279\u5f81\u7f16\u7801\u5e76\u4e0e\u539f\u59cb\u56fe\u50cf\u7279\u5f81\u96c6\u6210\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u6027\u80fd\u540c\u65f6\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u89c6\u89c9\u63d0\u793a\u63cf\u8ff0\u65b9\u6cd5\u9700\u8981\u66f4\u5c11\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u53cc\u68c0\u7d22\u673a\u5236\u751f\u6210\u89c6\u89c9\u63d0\u793a\u80fd\u6709\u6548\u589e\u5f3a\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u7684\u89c6\u89c9\u8868\u793a\u80fd\u529b\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u68c0\u7d22\u589e\u5f3a\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\uff0c\u5728\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2510.25303", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25303", "abs": "https://arxiv.org/abs/2510.25303", "authors": ["Soumyadeep Jana", "Sanasam Ranbir Singh"], "title": "Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation to a Parameter-Efficient Student", "comment": null, "summary": "Multimodal sarcasm detection is challenging, especially in low-resource\nsettings where subtle image-text contradictions are hard to learn due to scarce\nannotated data, which hinders the model's performance. Parameter-efficient\nfine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce\noverfitting but struggle to reach optimal performance due to limited\nsupervision from few-shot data. We propose PEKD, a unified framework that\nenhances PEFT methods via distillation from an expert model trained on\nlarge-scale sarcasm data, which acts as the teacher. To mitigate unreliable\nsignals from the teacher, we introduce an entropy-aware gating mechanism that\ndynamically adjusts the distillation strength based on teacher confidence.\nExperiments on two public datasets demonstrate that our PEKD framework enables\nPEFT methods to outperform both prior parameter-efficient approaches and large\nmultimodal models, achieving strong results in the few-shot scenario. The\nframework is modular and adaptable to a wide range of multimodal models and\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPEKD\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5927\u89c4\u6a21\u8bbd\u523a\u6570\u636e\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\u4e2d\u63d0\u53d6\u77e5\u8bc6\u6765\u589e\u5f3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u4e2d\u76d1\u7763\u4fe1\u53f7\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5f15\u5165\u71b5\u611f\u77e5\u95e8\u63a7\u673a\u5236\u52a8\u6001\u8c03\u6574\u84b8\u998f\u5f3a\u5ea6\uff0c\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86PEFT\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u9762\u4e34\u6311\u6218\uff0c\u7531\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u5b66\u4e60\u56fe\u50cf-\u6587\u672c\u95f4\u7684\u5fae\u5999\u77db\u76fe\u3002\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u867d\u7136\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\uff0c\u4f46\u5728\u5c11\u6837\u672c\u6570\u636e\u4e0b\u56e0\u76d1\u7763\u4fe1\u53f7\u6709\u9650\u800c\u65e0\u6cd5\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "method": "\u63d0\u51faPEKD\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5927\u89c4\u6a21\u8bbd\u523a\u6570\u636e\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u6765\u589e\u5f3aPEFT\u65b9\u6cd5\u3002\u5f15\u5165\u71b5\u611f\u77e5\u95e8\u63a7\u673a\u5236\uff0c\u6839\u636e\u6559\u5e08\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u52a8\u6001\u8c03\u6574\u84b8\u998f\u5f3a\u5ea6\uff0c\u4ee5\u7f13\u89e3\u6765\u81ea\u6559\u5e08\u6a21\u578b\u7684\u4e0d\u53ef\u9760\u4fe1\u53f7\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPEKD\u6846\u67b6\u4f7fPEFT\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u53d6\u5f97\u4e86\u5f3a\u52b2\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u6a21\u5757\u5316\u7279\u6027\uff0c\u53ef\u9002\u5e94\u5e7f\u6cdb\u7684\u591a\u6a21\u6001\u6a21\u578b\u548c\u4efb\u52a1\u3002\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u589e\u5f3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5728\u4f4e\u8d44\u6e90\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.25179", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25179", "abs": "https://arxiv.org/abs/2510.25179", "authors": ["Juan Ren", "Mark Dras", "Usman Naseem"], "title": "Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models", "comment": null, "summary": "Agentic methods have emerged as a powerful and autonomous paradigm that\nenhances reasoning, collaboration, and adaptive control, enabling systems to\ncoordinate and independently solve complex tasks. We extend this paradigm to\nsafety alignment by introducing Agentic Moderation, a model-agnostic framework\nthat leverages specialised agents to defend multimodal systems against\njailbreak attacks. Unlike prior approaches that apply as a static layer over\ninputs or outputs and provide only binary classifications (safe or unsafe), our\nmethod integrates dynamic, cooperative agents, including Shield, Responder,\nEvaluator, and Reflector, to achieve context-aware and interpretable\nmoderation. Extensive experiments across five datasets and four representative\nLarge Vision-Language Models (LVLMs) demonstrate that our approach reduces the\nAttack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),\nand improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,\nand well-balanced safety performance. By harnessing the flexibility and\nreasoning capacity of agentic architectures, Agentic Moderation provides\nmodular, scalable, and fine-grained safety enforcement, highlighting the\nbroader potential of agentic systems as a foundation for automated safety\ngovernance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Agentic Moderation\u6846\u67b6\uff0c\u5229\u7528\u4e13\u4e1a\u5316\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u6765\u9632\u5fa1\u591a\u6a21\u6001\u7cfb\u7edf\u5bf9\u6297\u8d8a\u72f1\u653b\u51fb\uff0c\u901a\u8fc7\u52a8\u6001\u534f\u4f5c\u7684\u667a\u80fd\u4f53\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u7684\u5185\u5bb9\u5ba1\u6838\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u4f5c\u4e3a\u9759\u6001\u5c42\u5e94\u7528\u4e8e\u8f93\u5165\u6216\u8f93\u51fa\uff0c\u4ec5\u63d0\u4f9b\u4e8c\u5143\u5206\u7c7b\uff08\u5b89\u5168\u6216\u4e0d\u5b89\u5168\uff09\uff0c\u7f3a\u4e4f\u52a8\u6001\u6027\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u590d\u6742\u7684\u8d8a\u72f1\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u4e86Agentic Moderation\u6846\u67b6\uff0c\u5305\u542bShield\u3001Responder\u3001Evaluator\u548cReflector\u56db\u4e2a\u52a8\u6001\u534f\u4f5c\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u6a21\u578b\u65e0\u5173\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u5b89\u5168\u9632\u5fa1\uff0c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u7684\u5ba1\u6838\u673a\u5236\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u548c\u56db\u4e2a\u4ee3\u8868\u6027\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e7-19%\uff0c\u4fdd\u6301\u7a33\u5b9a\u7684\u4e0d\u8ddf\u968f\u7387\uff0c\u5e76\u5c06\u62d2\u7edd\u7387\u63d0\u9ad84-20%\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u5e73\u8861\u7684\u5b89\u5168\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u667a\u80fd\u4f53\u67b6\u6784\u7684\u7075\u6d3b\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0cAgentic Moderation\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u548c\u7ec6\u7c92\u5ea6\u7684\u5b89\u5168\u6267\u884c\uff0c\u7a81\u663e\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u4f5c\u4e3a\u81ea\u52a8\u5316\u5b89\u5168\u6cbb\u7406\u57fa\u7840\u7684\u66f4\u5e7f\u6cdb\u6f5c\u529b\u3002"}}
{"id": "2510.24816", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24816", "abs": "https://arxiv.org/abs/2510.24816", "authors": ["Cui Yakun", "Fushuo Huo", "Weijie Shi", "Juntao Dai", "Hang Du", "Zhenghao Zhu", "Sirui Han", "Yike Guo"], "title": "Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection", "comment": null, "summary": "The advent of multi-modal large language models (MLLMs) has greatly advanced\nresearch into applications for Video fake news detection (VFND) tasks.\nTraditional video-based FND benchmarks typically focus on the accuracy of the\nfinal decision, often failing to provide fine-grained assessments for the\nentire detection process, making the detection process a black box. Therefore,\nwe introduce the MVFNDB (Multi-modal Video Fake News Detection Benchmark) based\non the empirical analysis, which provides foundation for tasks definition. The\nbenchmark comprises 10 tasks and is meticulously crafted to probe MLLMs'\nperception, understanding, and reasoning capacities during detection, featuring\n9730 human-annotated video-related questions based on a carefully constructed\ntaxonomy ability of VFND. To validate the impact of combining multiple features\non the final results, we design a novel framework named MVFND-CoT, which\nincorporates both creator-added content and original shooting footage\nreasoning. Building upon the benchmark, we conduct an in-depth analysis of the\ndeeper factors influencing accuracy, including video processing strategies and\nthe alignment between video features and model capabilities. We believe this\nbenchmark will lay a solid foundation for future evaluations and advancements\nof MLLMs in the domain of video fake news detection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86MVFNDB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u7684\u611f\u77e5\u3001\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86MVFND-CoT\u6846\u67b6\u6765\u9a8c\u8bc1\u591a\u7279\u5f81\u878d\u5408\u5bf9\u68c0\u6d4b\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u5047\u65b0\u95fb\u68c0\u6d4b\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u51b3\u7b56\u7684\u51c6\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u6574\u4e2a\u68c0\u6d4b\u8fc7\u7a0b\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u4f7f\u5f97\u68c0\u6d4b\u8fc7\u7a0b\u6210\u4e3a\u9ed1\u7bb1\uff0c\u65e0\u6cd5\u6df1\u5165\u7406\u89e3\u6a21\u578b\u7684\u611f\u77e5\u3001\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u7ecf\u9a8c\u5206\u6790\u6784\u5efa\u4e86MVFNDB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b10\u4e2a\u4efb\u52a1\u548c9730\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u89c6\u9891\u76f8\u5173\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86MVFND-CoT\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u878d\u5408\u4e86\u521b\u4f5c\u8005\u6dfb\u52a0\u5185\u5bb9\u548c\u539f\u59cb\u62cd\u6444\u7d20\u6750\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u7814\u7a76\u5bf9\u5f71\u54cd\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u6df1\u5c42\u56e0\u7d20\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u5305\u62ec\u89c6\u9891\u5904\u7406\u7b56\u7565\u4ee5\u53ca\u89c6\u9891\u7279\u5f81\u4e0e\u6a21\u578b\u80fd\u529b\u4e4b\u95f4\u7684\u5bf9\u9f50\u5173\u7cfb\uff0c\u9a8c\u8bc1\u4e86\u591a\u7279\u5f81\u878d\u5408\u5bf9\u6700\u7ec8\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u672a\u6765\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u5047\u65b0\u95fb\u68c0\u6d4b\u9886\u57df\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\u7684\u7cfb\u7edf\u5316\u548c\u6df1\u5165\u5316\u53d1\u5c55\u3002"}}
{"id": "2510.25364", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25364", "abs": "https://arxiv.org/abs/2510.25364", "authors": ["Luca Capone", "Alessandro Bondielli", "Alessandro Lenci"], "title": "CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs", "comment": "Paper accepted for oral presentation at the BabyLM Challange 2025\n  (EMNLP2025)", "summary": "This work investigates whether small-scale LMs can benefit from instruction\ntuning. We compare conversational and question-answering instruction tuning\ndatasets, applied either in a merged or sequential curriculum, using\ndecoder-only models with 100M and 140M parameters. Evaluation spans both\nfine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and\npsycholinguistic correlation) settings. Results show that instruction tuning\nyields small but consistent gains in fine-tuning scenarios, with sequential\ncurricula outperforming merged data; however, improvements do not consistently\ntransfer to zero-shot tasks, suggesting a trade-off between interaction-focused\nadaptation and broad linguistic generalization. These results highlight both\nthe potential and the constraints of adapting human-inspired learning\nstrategies to low-resource LMs, and point toward hybrid, curriculum-based\napproaches for enhancing generalization under ecological training limits.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u4ece\u6307\u4ee4\u8c03\u4f18\u4e2d\u53d7\u76ca\uff0c\u53d1\u73b0\u6307\u4ee4\u8c03\u4f18\u5728\u5fae\u8c03\u573a\u666f\u4e2d\u5e26\u6765\u5c0f\u5e45\u4f46\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u6539\u8fdb\u5e76\u4e0d\u4e00\u81f4\u5730\u8fc1\u79fb\u5230\u96f6\u6837\u672c\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u4ea4\u4e92\u5bfc\u5411\u9002\u5e94\u4e0e\u5e7f\u6cdb\u8bed\u8a00\u6cdb\u5316\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u4ece\u6307\u4ee4\u8c03\u4f18\u4e2d\u53d7\u76ca\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5728\u6709\u9650\u8d44\u6e90\u6761\u4ef6\u4e0b\u5982\u4f55\u901a\u8fc7\u4eba\u7c7b\u542f\u53d1\u5f0f\u5b66\u4e60\u7b56\u7565\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u5173\u6ce8\u6307\u4ee4\u8c03\u4f18\u5bf9\u4f4e\u53c2\u6570\u91cf\u6a21\u578b\u7684\u9002\u7528\u6027\u548c\u5c40\u9650\u6027\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u5bf9\u8bdd\u5f0f\u548c\u95ee\u7b54\u5f0f\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5408\u5e76\u6216\u987a\u5e8f\u8bfe\u7a0b\u4e24\u79cd\u7b56\u7565\uff0c\u4f7f\u7528100M\u548c140M\u53c2\u6570\u7684\u4ec5\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5728\u5fae\u8c03\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6307\u4ee4\u8c03\u4f18\u5728\u5fae\u8c03\u573a\u666f\uff08SuperGLUE\uff09\u4e2d\u4ea7\u751f\u5c0f\u5e45\u4f46\u4e00\u81f4\u7684\u6027\u80fd\u589e\u76ca\uff0c\u987a\u5e8f\u8bfe\u7a0b\u7b56\u7565\u4f18\u4e8e\u5408\u5e76\u6570\u636e\u65b9\u6cd5\uff1b\u7136\u800c\u8fd9\u4e9b\u6539\u8fdb\u5728\u96f6\u6837\u672c\u4efb\u52a1\uff08BLiMP\u3001EWoK\u3001WUGs\u7b49\uff09\u4e2d\u5e76\u4e0d\u4e00\u81f4\u5730\u8fc1\u79fb\uff0c\u8868\u660e\u5b58\u5728\u7279\u5b9a\u6743\u8861\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5c06\u4eba\u7c7b\u542f\u53d1\u5f0f\u5b66\u4e60\u7b56\u7565\u5e94\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u548c\u7ea6\u675f\uff0c\u6307\u51fa\u4e86\u5728\u751f\u6001\u8bad\u7ec3\u9650\u5236\u4e0b\u901a\u8fc7\u6df7\u5408\u8bfe\u7a0b\u65b9\u6cd5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u5411\uff0c\u4e3a\u5c0f\u578b\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.25668", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.25668", "abs": "https://arxiv.org/abs/2510.25668", "authors": ["Tianyu Yang", "Terry Ruas", "Yijun Tian", "Jan Philip Wahle", "Daniel Kurzawe", "Bela Gipp"], "title": "ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents", "comment": null, "summary": "Vision-language models (VLMs) excel at interpreting text-rich images but\nstruggle with long, visually complex documents that demand analysis and\nintegration of information spread across multiple pages. Existing approaches\ntypically rely on fixed reasoning templates or rigid pipelines, which force\nVLMs into a passive role and hinder both efficiency and generalization. We\npresent Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement\nlearning framework that fine-tunes VLMs as interactive agents capable of\nactively navigating long, visually rich documents. ALDEN introduces a novel\nfetch action that directly accesses the page by index, complementing the\nclassic search action and better exploiting document structure. For dense\nprocess supervision and efficient training, we propose a rule-based cross-level\nreward that provides both turn- and token-level signals. To address the\nempirically observed training instability caused by numerous visual tokens from\nlong documents, we further propose a visual-semantic anchoring mechanism that\napplies a dual-path KL-divergence constraint to stabilize visual and textual\nrepresentations separately during training. Trained on a corpus constructed\nfrom three open-source datasets, ALDEN achieves state-of-the-art performance on\nfive long-document benchmarks. Overall, ALDEN marks a step beyond passive\ndocument reading toward agents that autonomously navigate and reason across\nlong, visually rich documents, offering a robust path to more accurate and\nefficient long-document understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ALDEN\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u8f6e\u4ea4\u4e92\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e3a\u4e3b\u52a8\u5bfc\u822a\u957f\u6587\u6863\u7684\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u957f\u6587\u6863\u7406\u89e3\u4e2d\u7684\u5c40\u9650\u6027\u3002\u8be5\u6846\u67b6\u5f15\u5165\u65b0\u9896\u7684\u9875\u9762\u7d22\u5f15\u8bbf\u95ee\u52a8\u4f5c\u548c\u89c6\u89c9\u8bed\u4e49\u951a\u5b9a\u673a\u5236\uff0c\u5728\u4e94\u4e2a\u957f\u6587\u6863\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u8de8\u591a\u9875\u5206\u6790\u548c\u4fe1\u606f\u6574\u5408\u7684\u957f\u800c\u590d\u6742\u7684\u6587\u6863\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u7684\u63a8\u7406\u6a21\u677f\u6216\u521a\u6027\u6d41\u7a0b\uff0c\u8feb\u4f7f\u6a21\u578b\u5904\u4e8e\u88ab\u52a8\u89d2\u8272\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "ALDEN\u6846\u67b6\u91c7\u7528\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5f15\u5165\u9875\u9762\u7d22\u5f15\u8bbf\u95ee\u52a8\u4f5c\u4ee5\u5229\u7528\u6587\u6863\u7ed3\u6784\uff0c\u63d0\u51fa\u57fa\u4e8e\u89c4\u5219\u7684\u8de8\u5c42\u7ea7\u5956\u52b1\u673a\u5236\u8fdb\u884c\u5bc6\u96c6\u8fc7\u7a0b\u76d1\u7763\uff0c\u5e76\u8bbe\u8ba1\u89c6\u89c9\u8bed\u4e49\u951a\u5b9a\u673a\u5236\u901a\u8fc7\u53cc\u8def\u5f84KL\u6563\u5ea6\u7ea6\u675f\u5206\u522b\u7a33\u5b9a\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\u4ee5\u89e3\u51b3\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "result": "\u5728\u57fa\u4e8e\u4e09\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u6784\u5efa\u7684\u8bed\u6599\u5e93\u4e0a\u8bad\u7ec3\u540e\uff0cALDEN\u5728\u4e94\u4e2a\u957f\u6587\u6863\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u6587\u6863\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "ALDEN\u6807\u5fd7\u7740\u4ece\u88ab\u52a8\u6587\u6863\u9605\u8bfb\u5411\u80fd\u591f\u81ea\u4e3b\u5bfc\u822a\u548c\u8de8\u957f\u6587\u6863\u63a8\u7406\u7684\u667a\u80fd\u4f53\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u4e3a\u66f4\u51c6\u786e\u9ad8\u6548\u7684\u957f\u6587\u6863\u7406\u89e3\u63d0\u4f9b\u4e86\u7a33\u5065\u8def\u5f84\uff0c\u5c55\u793a\u4e86\u4e3b\u52a8\u4ea4\u4e92\u5f0f\u6587\u6863\u7406\u89e3\u65b9\u6cd5\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.24820", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24820", "abs": "https://arxiv.org/abs/2510.24820", "authors": ["Ruiyang Zhang", "Jiahao Luo", "Xiaoru Feng", "Qiufan Pang", "Yaodong Yang", "Juntao Dai"], "title": "SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing", "comment": null, "summary": "With the rapid advancement of text-to-image (T2I) models, ensuring their\nsafety has become increasingly critical. Existing safety approaches can be\ncategorized into training-time and inference-time methods. While inference-time\nmethods are widely adopted due to their cost-effectiveness, they often suffer\nfrom limitations such as over-refusal and imbalance between safety and utility.\nTo address these challenges, we propose a multi-round safety editing framework\nthat functions as a model-agnostic, plug-and-play module, enabling efficient\nsafety alignment for any text-to-image model. Central to this framework is\nMR-SafeEdit, a multi-round image-text interleaved dataset specifically\nconstructed for safety editing in text-to-image generation. We introduce a\npost-hoc safety editing paradigm that mirrors the human cognitive process of\nidentifying and refining unsafe content. To instantiate this paradigm, we\ndevelop SafeEditor, a unified MLLM capable of multi-round safety editing on\ngenerated images. Experimental results show that SafeEditor surpasses prior\nsafety approaches by reducing over-refusal while achieving a more favorable\nsafety-utility balance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8f6e\u5b89\u5168\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efaMR-SafeEdit\u6570\u636e\u96c6\u548c\u5f00\u53d1SafeEditor\u6a21\u578b\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u6a21\u578b\u65e0\u5173\u7684\u5b89\u5168\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8fc7\u5ea6\u62d2\u7edd\u5e76\u6539\u5584\u4e86\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u65b9\u6cd5\u4e3b\u8981\u5206\u4e3a\u8bad\u7ec3\u65f6\u548c\u63a8\u7406\u65f6\u4e24\u7c7b\uff0c\u5176\u4e2d\u63a8\u7406\u65f6\u65b9\u6cd5\u56e0\u6210\u672c\u6548\u76ca\u800c\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5b58\u5728\u8fc7\u5ea6\u62d2\u7edd\u4ee5\u53ca\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u4e4b\u95f4\u5e73\u8861\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8f6e\u5b89\u5168\u7f16\u8f91\u6846\u67b6\u4f5c\u4e3a\u6a21\u578b\u65e0\u5173\u7684\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u6838\u5fc3\u662f\u4e13\u95e8\u4e3a\u5b89\u5168\u7f16\u8f91\u6784\u5efa\u7684MR-SafeEdit\u591a\u8f6e\u56fe\u6587\u4ea4\u9519\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86SafeEditor\u7edf\u4e00\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u540e\u9a8c\u5b89\u5168\u7f16\u8f91\u8303\u5f0f\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u4f18\u5316\u4e0d\u5b89\u5168\u5185\u5bb9\u7684\u8ba4\u77e5\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSafeEditor\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u5b89\u5168\u65b9\u6cd5\uff0c\u5728\u51cf\u5c11\u8fc7\u5ea6\u62d2\u7edd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u5e73\u8861\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5b89\u5168\u5bf9\u9f50\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u591a\u8f6e\u5b89\u5168\u7f16\u8f91\u6846\u67b6\u5728\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u7684\u540e\u9a8c\u7f16\u8f91\u8303\u5f0f\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u5b89\u5168\u9632\u62a4\u3002"}}
{"id": "2510.25413", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25413", "abs": "https://arxiv.org/abs/2510.25413", "authors": ["Shakib Yazdani", "Yasser Hamidullah", "Cristina Espa\u00f1a-Bonet", "Josef van Genabith"], "title": "Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media", "comment": "Accepted by RANLP 2025", "summary": "Most existing sign language translation (SLT) datasets are limited in scale,\nlack multilingual coverage, and are costly to curate due to their reliance on\nexpert annotation and controlled recording setup. Recently, Vision Language\nModels (VLMs) have demonstrated strong capabilities as evaluators and real-time\nassistants. Despite these advancements, their potential remains untapped in the\ncontext of sign language dataset acquisition. To bridge this gap, we introduce\nthe first automated annotation and filtering framework that utilizes VLMs to\nreduce reliance on manual effort while preserving data quality. Our method is\napplied to TikTok videos across eight sign languages and to the already curated\nYouTube-SL-25 dataset in German Sign Language for the purpose of additional\nevaluation. Our VLM-based pipeline includes a face visibility detection, a sign\nactivity recognition, a text extraction from video content, and a judgment step\nto validate alignment between video and text, implementing generic filtering,\nannotation and validation steps. Using the resulting corpus, TikTok-SL-8, we\nassess the performance of two off-the-shelf SLT models on our filtered dataset\nfor German and American Sign Languages, with the goal of establishing baselines\nand evaluating the robustness of recent models on automatically extracted,\nslightly noisy data. Our work enables scalable, weakly supervised pretraining\nfor SLT and facilitates data acquisition from social media.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u624b\u8bed\u7ffb\u8bd1\u6570\u636e\u96c6\u81ea\u52a8\u6807\u6ce8\u548c\u8fc7\u6ee4\u7684\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u4f9d\u8d56\uff0c\u5e76\u6784\u5efa\u4e86\u6db5\u76d6\u516b\u79cd\u624b\u8bed\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6TikTok-SL-8\uff0c\u4e3a\u624b\u8bed\u7ffb\u8bd1\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\u6570\u636e\u6e90\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u7ffb\u8bd1\u6570\u636e\u96c6\u666e\u904d\u5b58\u5728\u89c4\u6a21\u6709\u9650\u3001\u591a\u8bed\u8a00\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e14\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\u548c\u53d7\u63a7\u5f55\u5236\u73af\u5883\u5bfc\u81f4\u6210\u672c\u9ad8\u6602\uff0c\u800c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u624b\u8bed\u6570\u636e\u83b7\u53d6\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u6316\u6398\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u6807\u6ce8\u8fc7\u6ee4\u6846\u67b6\uff0c\u5305\u542b\u4eba\u8138\u53ef\u89c1\u6027\u68c0\u6d4b\u3001\u624b\u8bed\u6d3b\u52a8\u8bc6\u522b\u3001\u89c6\u9891\u6587\u672c\u63d0\u53d6\u4ee5\u53ca\u89c6\u9891\u6587\u672c\u5bf9\u9f50\u9a8c\u8bc1\u56db\u4e2a\u6b65\u9aa4\uff0c\u5e94\u7528\u4e8eTikTok\u516b\u79cd\u624b\u8bed\u89c6\u9891\u548cYouTube-SL-25\u5fb7\u8bed\u624b\u8bed\u6570\u636e\u96c6\u3002", "result": "\u6784\u5efa\u4e86TikTok-SL-8\u591a\u8bed\u8a00\u624b\u8bed\u6570\u636e\u96c6\uff0c\u5e76\u5728\u8fc7\u6ee4\u540e\u7684\u5fb7\u8bed\u548c\u7f8e\u56fd\u624b\u8bed\u6570\u636e\u4e0a\u8bc4\u4f30\u4e86\u4e24\u4e2a\u73b0\u6210\u624b\u8bed\u7ffb\u8bd1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u63d0\u53d6\u7684\u5e26\u566a\u58f0\u6570\u636e\u5efa\u7acb\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5b9e\u73b0\u4e86\u624b\u8bed\u7ffb\u8bd1\u7684\u53ef\u6269\u5c55\u5f31\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u4fc3\u8fdb\u4e86\u4ece\u793e\u4ea4\u5a92\u4f53\u83b7\u53d6\u624b\u8bed\u6570\u636e\u7684\u80fd\u529b\uff0c\u4e3a\u624b\u8bed\u7ffb\u8bd1\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u83b7\u53d6\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.24821", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24821", "abs": "https://arxiv.org/abs/2510.24821", "authors": ["Inclusion AI", ":", "Bowen Ma", "Cheng Zou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianing Li", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jianping Jiang", "Jun Peng", "Kaixiang Ji", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Longhua Tan", "Lan Wang", "Mochen Bai", "Ning Gao", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Ruobing Zheng", "Sirui Gao", "Tianqi Li", "Tinghao Liu", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaolong Wang", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yuting Xiao", "Yunxiao Sun", "Yipeng Chen", "Yifan Mao", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zhiqiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zizheng Yang", "Zhengyu He"], "title": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation", "comment": "18 pages, 5 figures", "summary": "We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a\nsparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion\ntotal parameters, of which only 6.1 billion are active per token. This\narchitecture enables highly efficient scaling (dramatically improving\ncomputational efficiency while significantly expanding model capacity) and\nempowers stronger unified multimodal intelligence across vision, speech, and\nlanguage, representing a key step toward Artificial General Intelligence (AGI).\nCompared to its predecessor, the upgraded version exhibits substantial\nimprovements across multimodal understanding and generation. We significantly\nadvance speech recognition capabilities, achieving state-of-the-art performance\nin contextual ASR and highly competitive results in dialect-aware ASR. In image\ngeneration, Ming-Flash-Omni introduces high-fidelity text rendering and\ndemonstrates marked gains in scene consistency and identity preservation during\nimage editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,\na capability that not only achieves strong standalone segmentation performance\nbut also enhances spatial control in image generation and improves editing\nconsistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in\ntext-to-image generation and generative segmentation, and sets new records on\nall 12 contextual ASR benchmarks, all within a single unified architecture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Ming-Flash-Omni\uff0c\u8fd9\u662fMing-Omni\u7684\u5347\u7ea7\u7248\u672c\uff0c\u91c7\u7528\u57fa\u4e8eLing-Flash-2.0\u7684\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u5177\u67091000\u4ebf\u603b\u53c2\u6570\u4f46\u6bcftoken\u4ec5\u6fc0\u6d3b61\u4ebf\u53c2\u6570\uff0c\u5728\u7edf\u4e00\u591a\u6a21\u6001\u667a\u80fd\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u5e76\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u751f\u6210\u5f0f\u5206\u5272\u548c\u4e0a\u4e0b\u6587\u8bed\u97f3\u8bc6\u522b\u7b49\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u5bb9\u91cf\u6269\u5c55\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u540c\u65f6\u63a8\u52a8\u7edf\u4e00\u591a\u6a21\u6001\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u6db5\u76d6\u89c6\u89c9\u3001\u8bed\u97f3\u548c\u8bed\u8a00\u7b49\u591a\u4e2a\u6a21\u6001\uff0c\u4e3a\u5b9e\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\u8fc8\u51fa\u5173\u952e\u4e00\u6b65\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8eLing-Flash-2.0\u7684\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u53d8\u4f53\u6784\u5efa\uff0c\u603b\u53c2\u6570\u91cf\u8fbe1000\u4ebf\u4f46\u6bcftoken\u4ec5\u6fc0\u6d3b61\u4ebf\u53c2\u6570\uff0c\u901a\u8fc7\u8fd9\u79cd\u67b6\u6784\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u751f\u6210\u5f0f\u5206\u5272\u7b49\u65b0\u80fd\u529b\u6765\u589e\u5f3a\u7a7a\u95f4\u63a7\u5236\u548c\u7f16\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u76f8\u6bd4\u524d\u4ee3\u6709\u663e\u8457\u63d0\u5347\uff0c\u5728\u4e0a\u4e0b\u6587ASR\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u5e76\u5728\u65b9\u8a00\u611f\u77e5ASR\u4e0a\u83b7\u5f97\u9ad8\u5ea6\u7ade\u4e89\u529b\uff0c\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u6587\u672c\u6e32\u67d3\uff0c\u5e76\u5728\u573a\u666f\u4e00\u81f4\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728\u6240\u670912\u4e2a\u4e0a\u4e0b\u6587ASR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u521b\u9020\u4e86\u65b0\u8bb0\u5f55\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u80fd\u591f\u6709\u6548\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u5bb9\u91cf\uff0c\u7edf\u4e00\u7684\u67b6\u6784\u8bbe\u8ba1\u4e3a\u591a\u6a21\u6001\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u751f\u6210\u5f0f\u5206\u5272\u7b49\u65b0\u80fd\u529b\u4e0d\u4ec5\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u8fd8\u589e\u5f3a\u4e86\u56fe\u50cf\u751f\u6210\u7684\u7a7a\u95f4\u63a7\u5236\uff0c\u4e3a\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2510.25434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25434", "abs": "https://arxiv.org/abs/2510.25434", "authors": ["Shakib Yazdani", "Yasser Hamidullah", "Cristina Espa\u00f1a-Bonet", "Eleftherios Avramidis", "Josef van Genabith"], "title": "A Critical Study of Automatic Evaluation in Sign Language Translation", "comment": "Submitted to the LREC 2026 conference", "summary": "Automatic evaluation metrics are crucial for advancing sign language\ntranslation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are\nonly text-based, and it remains unclear to what extent text-based metrics can\nreliably capture the quality of SLT outputs. To address this gap, we\ninvestigate the limitations of text-based SLT evaluation metrics by analyzing\nsix metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one\nhand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA\nzero-shot direct assessment on the other hand. Specifically, we assess the\nconsistency and robustness of these metrics under three controlled conditions:\nparaphrasing, hallucinations in model outputs, and variations in sentence\nlength. Our analysis highlights the limitations of lexical overlap metrics and\ndemonstrates that while LLM-based evaluators better capture semantic\nequivalence often missed by conventional metrics, they can also exhibit bias\ntoward LLM-paraphrased translations. Moreover, although all metrics are able to\ndetect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and\nLLM-based evaluators are comparatively lenient toward subtle cases. This\nmotivates the need for multimodal evaluation frameworks that extend beyond\ntext-based metrics to enable a more holistic assessment of SLT outputs.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u624b\u8bed\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u6587\u672c\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u4f20\u7edf\u8bcd\u6c47\u91cd\u53e0\u6307\u6807\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u5668\u867d\u80fd\u66f4\u597d\u6355\u6349\u8bed\u4e49\u5bf9\u7b49\uff0c\u4f46\u5bf9LLM\u751f\u6210\u7684\u91ca\u4e49\u5b58\u5728\u504f\u89c1\uff0c\u63ed\u793a\u4e86\u6784\u5efa\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5f53\u524d\u624b\u8bed\u7ffb\u8bd1\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56BLEU\u3001ROUGE\u7b49\u7eaf\u6587\u672c\u6307\u6807\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u80fd\u53ef\u9760\u8bc4\u4f30\u624b\u8bed\u7ffb\u8bd1\u8d28\u91cf\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u6587\u672c\u6307\u6807\u5728\u624b\u8bed\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u7684\u5c40\u9650\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u516d\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5305\u62ecBLEU\u3001chrF\u3001ROUGE\u548cBLEURT\u7b49\u4f20\u7edf\u6307\u6807\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684G-Eval\u548cGEMBA\u96f6\u6837\u672c\u76f4\u63a5\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u5728\u91ca\u4e49\u3001\u6a21\u578b\u8f93\u51fa\u5e7b\u89c9\u548c\u53e5\u5b50\u957f\u5ea6\u53d8\u5316\u4e09\u79cd\u53d7\u63a7\u6761\u4ef6\u4e0b\u8bc4\u4f30\u8fd9\u4e9b\u6307\u6807\u7684\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5206\u6790\u8868\u660e\u8bcd\u6c47\u91cd\u53e0\u6307\u6807\u5b58\u5728\u660e\u663e\u5c40\u9650\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u5668\u80fd\u66f4\u597d\u6355\u6349\u4f20\u7edf\u6307\u6807\u5e38\u5ffd\u7565\u7684\u8bed\u4e49\u5bf9\u7b49\uff0c\u4f46\u5bf9LLM\u751f\u6210\u7684\u91ca\u4e49\u5b58\u5728\u504f\u89c1\uff1b\u6240\u6709\u6307\u6807\u90fd\u80fd\u68c0\u6d4b\u5e7b\u89c9\uff0c\u4f46BLEU\u8fc7\u4e8e\u654f\u611f\uff0c\u800cBLEURT\u548cLLM\u8bc4\u4f30\u5668\u5bf9\u7ec6\u5fae\u5e7b\u89c9\u6848\u4f8b\u76f8\u5bf9\u5bbd\u677e\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7eaf\u6587\u672c\u8bc4\u4f30\u6307\u6807\u5728\u624b\u8bed\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u8d85\u8d8a\u6587\u672c\u6307\u6807\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u624b\u8bed\u7ffb\u8bd1\u8f93\u51fa\u7684\u66f4\u5168\u9762\u8bc4\u4f30\uff0c\u63a8\u52a8\u624b\u8bed\u7ffb\u8bd1\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24827", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.24827", "abs": "https://arxiv.org/abs/2510.24827", "authors": ["Haoyang Zhang", "Zhou Yang", "Ke Sun", "Yucai Pang", "Guoliang Xu"], "title": "MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal Interaction for Multimodal Emotion Recognition", "comment": "The paper will be published in the MMAsia2025 conference proceedings", "summary": "Multimodal emotion recognition is crucial for future human-computer\ninteraction. However, accurate emotion recognition still faces significant\nchallenges due to differences between different modalities and the difficulty\nof characterizing unimodal emotional information. To solve these problems, a\nhybrid network model based on multipath cross-modal interaction (MCIHN) is\nproposed. First, adversarial autoencoders (AAE) are constructed separately for\neach modality. The AAE learns discriminative emotion features and reconstructs\nthe features through a decoder to obtain more discriminative information about\nthe emotion classes. Then, the latent codes from the AAE of different\nmodalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to\nreduce the discrepancy between modalities, establish the emotional relationship\nbetween interacting modalities, and generate the interaction features between\ndifferent modalities. Multimodal fusion using the Feature Fusion module (FFM)\nfor better emotion recognition. Experiments were conducted on publicly\navailable SIMS and MOSI datasets, demonstrating that MCIHN achieves superior\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u8def\u5f84\u8de8\u6a21\u6001\u4ea4\u4e92\u7684\u6df7\u5408\u7f51\u7edc\u6a21\u578bMCIHN\uff0c\u901a\u8fc7\u5bf9\u6297\u81ea\u7f16\u7801\u5668\u548c\u8de8\u6a21\u6001\u95e8\u63a7\u673a\u5236\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u6a21\u6001\u5dee\u5f02\u548c\u60c5\u611f\u7279\u5f81\u8868\u5f81\u96be\u9898\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u9762\u4e34\u6a21\u6001\u95f4\u5dee\u5f02\u663e\u8457\u4ee5\u53ca\u5355\u6a21\u6001\u60c5\u611f\u4fe1\u606f\u8868\u5f81\u56f0\u96be\u4e24\u5927\u6311\u6218\uff0c\u8fd9\u9650\u5236\u4e86\u60c5\u611f\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faMCIHN\u6df7\u5408\u7f51\u7edc\u6a21\u578b\uff0c\u9996\u5148\u4e3a\u6bcf\u4e2a\u6a21\u6001\u6784\u5efa\u5bf9\u6297\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u5224\u522b\u6027\u60c5\u611f\u7279\u5f81\u5e76\u8fdb\u884c\u91cd\u6784\u589e\u5f3a\uff0c\u7136\u540e\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u8de8\u6a21\u6001\u95e8\u63a7\u673a\u5236CGMM\u51cf\u5c11\u6a21\u6001\u5dee\u5f02\u5e76\u5efa\u7acb\u6a21\u6001\u95f4\u60c5\u611f\u5173\u7cfb\uff0c\u6700\u540e\u4f7f\u7528\u7279\u5f81\u878d\u5408\u6a21\u5757FFM\u8fdb\u884c\u591a\u6a21\u6001\u878d\u5408\u3002", "result": "\u5728\u516c\u5f00\u53ef\u7528\u7684SIMS\u548cMOSI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMCIHN\u6a21\u578b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u5bf9\u6297\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u5224\u522b\u7279\u5f81\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u673a\u5236\u51cf\u5c11\u6a21\u6001\u5dee\u5f02\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.25628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25628", "abs": "https://arxiv.org/abs/2510.25628", "authors": ["Yusheng Liao", "Chaoyi Wu", "Junwei Liu", "Shuyang Jiang", "Pengcheng Qiu", "Haowen Wang", "Yun Yue", "Shuai Zhen", "Jian Wang", "Qianrui Fan", "Jinjie Gu", "Ya Zhang", "Yanfeng Wang", "Yu Wang", "Weidi Xie"], "title": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis", "comment": null, "summary": "Electronic Health Records (EHRs) contain rich yet complex information, and\ntheir automated analysis is critical for clinical decision-making. Despite\nrecent advances of large language models (LLMs) in clinical workflows, their\nability to analyze EHRs remains limited due to narrow task coverage and lack of\nEHR-oriented reasoning capabilities. This paper aims to bridge the gap,\nspecifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning\ninstruction dataset, comprising 300k high-quality reasoning cases and 4M\nnon-reasoning cases across 42 distinct EHR tasks. Its core innovation is a\nthinking-graph-driven framework that enables to generate high-quality reasoning\ndata at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced\nLLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage\ntraining paradigm, including domain adaptation, reasoning enhancement, and\nreinforcement learning, EHR-R1 systematically acquires domain knowledge and\ndiverse reasoning capabilities, enabling accurate and robust EHR analysis.\nLastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning\n42 tasks, to comprehensively assess reasoning and prediction across EHR\nscenarios. In experiments, we show that the resulting EHR-R1 consistently\noutperforms state-of-the-art commercial and open-source LLMs (including\nDeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and\nachieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,\nEHR-R1, and EHR-Bench have significantly advanced the development for more\nreliable and clinically relevant EHR analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86EHR-Ins\u5927\u89c4\u6a21\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u63a8\u7406\u6307\u4ee4\u6570\u636e\u96c6\u3001EHR-R1\u63a8\u7406\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u5217\u4ee5\u53caEHR-Bench\u8bc4\u4f30\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728EHR\u5206\u6790\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u548c\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5206\u6790\u4e2d\u5b58\u5728\u4efb\u52a1\u8986\u76d6\u8303\u56f4\u6709\u9650\u548c\u7f3a\u4e4f\u9762\u5411EHR\u7684\u63a8\u7406\u80fd\u529b\u7b49\u5173\u952e\u9650\u5236\uff0c\u963b\u788d\u4e86\u5176\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u6709\u6548\u5e94\u7528\u3002", "method": "\u91c7\u7528\u601d\u7ef4\u56fe\u9a71\u52a8\u6846\u67b6\u751f\u6210\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\uff0c\u901a\u8fc7\u9886\u57df\u9002\u5e94\u3001\u63a8\u7406\u589e\u5f3a\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u5f00\u53d1\u53c2\u6570\u9ad8\u8fbe720\u4ebf\u7684EHR-R1\u6a21\u578b\u7cfb\u5217\uff0c\u5e76\u6784\u5efa\u6db5\u76d642\u4e2a\u4efb\u52a1\u7684EHR-Bench\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "EHR-R1\u5728MIMIC-Bench\u4e0a\u8d85\u8d8aGPT-4o\u8d85\u8fc730\u5206\uff0c\u5728EHRSHOT\u4e0a\u5b9e\u73b010%\u7684\u96f6\u6837\u672cAUROC\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u5305\u62ecDeepSeek-V3\u548cGPT-4o\u5728\u5185\u7684\u6700\u5148\u8fdb\u5546\u4e1a\u548c\u5f00\u6e90LLM\u3002", "conclusion": "EHR-Ins\u3001EHR-R1\u548cEHR-Bench\u5171\u540c\u63a8\u52a8\u4e86\u66f4\u53ef\u9760\u548c\u4e34\u5e8a\u76f8\u5173\u7684EHR\u5206\u6790\u53d1\u5c55\uff0c\u4e3a\u533b\u7597AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u9886\u57df\u77e5\u8bc6\u83b7\u53d6\u548c\u591a\u6837\u5316\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u65b9\u6848\u3002"}}
{"id": "2510.24919", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24919", "abs": "https://arxiv.org/abs/2510.24919", "authors": ["Hossein R. Nowdeh", "Jie Ji", "Xiaolong Ma", "Fatemeh Afghah"], "title": "Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient Modulation for Harmonized Multimodal Learning", "comment": null, "summary": "In multimodal learning, dominant modalities often overshadow others, limiting\ngeneralization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),\na model-agnostic framework that applies to many modalities and supports early\nand late fusion scenarios. In every iteration, M-SAM in three steps optimizes\nlearning. \\textbf{First, it identifies the dominant modality} based on\nmodalities' contribution in the accuracy using Shapley. \\textbf{Second, it\ndecomposes the loss landscape}, or in another language, it modulates the loss\nto prioritize the robustness of the model in favor of the dominant modality,\nand \\textbf{third, M-SAM updates the weights} by backpropagation of modulated\ngradients. This ensures robust learning for the dominant modality while\nenhancing contributions from others, allowing the model to explore and exploit\ncomplementary features that strengthen overall performance. Extensive\nexperiments on four diverse datasets show that M-SAM outperforms the latest\nstate-of-the-art optimization and gradient manipulation methods and\nsignificantly balances and improves multimodal learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6a21\u6001\u611f\u77e5\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\uff08M-SAM\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u4e3b\u5bfc\u6a21\u6001\u5e76\u8c03\u5236\u635f\u5931\u51fd\u6570\u6765\u5e73\u8861\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002\u8be5\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u5316\u548c\u68af\u5ea6\u64cd\u4f5c\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\uff0c\u4e3b\u5bfc\u6a21\u6001\u5f80\u5f80\u4f1a\u538b\u5236\u5176\u4ed6\u6a21\u6001\u7684\u8d21\u732e\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5e73\u8861\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u9650\u5236\u4e86\u6a21\u578b\u4ece\u4e92\u8865\u7279\u5f81\u4e2d\u83b7\u76ca\u7684\u80fd\u529b\u3002", "method": "M-SAM\u6846\u67b6\u91c7\u7528\u4e09\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff1a\u9996\u5148\u57fa\u4e8eShapley\u503c\u8bc6\u522b\u4e3b\u5bfc\u6a21\u6001\uff0c\u7136\u540e\u901a\u8fc7\u635f\u5931\u51fd\u6570\u5206\u89e3\u8c03\u5236\u635f\u5931\u666f\u89c2\u4ee5\u589e\u5f3a\u4e3b\u5bfc\u6a21\u6001\u7684\u9c81\u68d2\u6027\uff0c\u6700\u540e\u901a\u8fc7\u8c03\u5236\u68af\u5ea6\u7684\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u6743\u91cd\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u65e9\u671f\u548c\u665a\u671f\u878d\u5408\u573a\u666f\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u6001\u7c7b\u578b\u3002", "result": "\u5728\u56db\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cM-SAM\u663e\u8457\u4f18\u4e8e\u6700\u65b0\u7684\u6700\u4f18\u5316\u548c\u68af\u5ea6\u64cd\u4f5c\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u8fc7\u7a0b\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u548c\u5229\u7528\u4e92\u8865\u7279\u5f81\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "M-SAM\u901a\u8fc7\u6a21\u6001\u611f\u77e5\u7684\u635f\u5931\u8c03\u5236\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u4e3b\u5bfc\u6a21\u6001\u538b\u5236\u95ee\u9898\u3002\u8be5\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u89c6\u89d2\uff0c\u80fd\u591f\u4fc3\u8fdb\u6a21\u578b\u66f4\u597d\u5730\u63a2\u7d22\u548c\u5229\u7528\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u4e92\u8865\u7279\u5f81\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.25682", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25682", "abs": "https://arxiv.org/abs/2510.25682", "authors": ["Jiani Zheng", "Zhiyang Teng", "Xiangtai Li", "Anran Wang", "Yu Tian", "Kunpeng Qiu", "Ye Tian", "Haochen Wang", "Zhuochen Wang"], "title": "PairUni: Pairwise Training for Unified Multimodal Language Models", "comment": null, "summary": "Unified vision-language models (UVLMs) must perform both understanding and\ngeneration within a single architecture, but these tasks rely on heterogeneous\ndata and supervision, making it difficult to balance them during reinforcement\nlearning (RL). We propose PairUni, a unified framework that reorganizes data\ninto understanding-generation (UG) pairs and aligns optimization accordingly.\nWe first use GPT-o3 to augment single-task data, generating captions for\nunderstanding samples and question-answer (QA) pairs for generation samples,\nforming aligned pairs from the same instance. Additionally, for each generation\nsample, we retrieve a semantically related understanding example to form a\nretrieved pair, linking different but related data points. These paired\nstructures expose cross-task semantic correspondences and support consistent\npolicy learning. To leverage this structure, we present Pair-GPRO, a pair-aware\nvariant based on Group Relative Policy Optimization. It assigns a similarity\nscore to each pair to modulate the advantage, strengthening learning from\nwell-aligned examples and reducing task interference. We curate a high-quality\ndataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on\nthe powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on\nvarious UVLMs, outperforming strong UVLM RL baselines. Code:\n\\href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PairUni\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6570\u636e\u91cd\u7ec4\u4e3a\u7406\u89e3-\u751f\u6210\u5bf9\uff0c\u5e76\u5f00\u53d1Pair-GPRO\u4f18\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5728\u5355\u4e00\u67b6\u6784\u4e2d\u540c\u65f6\u6267\u884c\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u4f46\u8fd9\u4e9b\u4efb\u52a1\u4f9d\u8d56\u4e8e\u5f02\u6784\u6570\u636e\u548c\u76d1\u7763\u4fe1\u53f7\uff0c\u5bfc\u81f4\u5728\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u5b9e\u73b0\u4efb\u52a1\u95f4\u7684\u5e73\u8861\u4f18\u5316\u3002", "method": "\u63d0\u51faPairUni\u6846\u67b6\uff0c\u9996\u5148\u4f7f\u7528GPT-4o\u589e\u5f3a\u5355\u4efb\u52a1\u6570\u636e\uff0c\u4e3a\u7406\u89e3\u6837\u672c\u751f\u6210\u63cf\u8ff0\u3001\u4e3a\u751f\u6210\u6837\u672c\u751f\u6210\u95ee\u7b54\u5bf9\uff0c\u5f62\u6210\u5bf9\u9f50\u7684\u5b9e\u4f8b\u5bf9\uff1b\u540c\u65f6\u901a\u8fc7\u68c0\u7d22\u8bed\u4e49\u76f8\u5173\u7684\u7406\u89e3\u793a\u4f8b\u6784\u5efa\u68c0\u7d22\u5bf9\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1Pair-GPRO\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u8bc4\u5206\u8c03\u8282\u4f18\u52bf\u51fd\u6570\uff0c\u5f3a\u5316\u5bf9\u9f50\u826f\u597d\u7684\u6837\u672c\u5b66\u4e60\u5e76\u51cf\u5c11\u4efb\u52a1\u5e72\u6270\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b16K\u4e2a\u7406\u89e3-\u751f\u6210\u5bf9\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6PairUG\uff0c\u5728\u5f3a\u5927\u7684Janus-Pro\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5e73\u8861\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u5404\u79cd\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u6570\u636e\u91cd\u7ec4\u548c\u914d\u5bf9\u611f\u77e5\u4f18\u5316\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u66f4\u5e73\u8861\u7684\u591a\u6a21\u6001\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24980", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24980", "abs": "https://arxiv.org/abs/2510.24980", "authors": ["Reza Saadati Fard", "Emmanuel Agu", "Palawat Busaranuvong", "Deepak Kumar", "Shefalika Gautam", "Bengisu Tulu", "Diane Strong", "Lorraine Loretz"], "title": "FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for Pressure Ulcer Severity Classification with Reasoning", "comment": null, "summary": "Pressure ulcers (PUs) are a serious and prevalent healthcare concern.\nAccurate classification of PU severity (Stages I-IV) is essential for proper\ntreatment but remains challenging due to subtle visual distinctions and\nsubjective interpretation, leading to variability among clinicians. Prior\nAI-based approaches using Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViTs) achieved promising accuracy but offered limited\ninterpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal\nmodel), a fine-tuned multimodal large language model (MLLM) with an agentic\nself-reflection mechanism for pressure ulcer severity classification. Inspired\nby clinician-style diagnostic reassessment, FT-ARM iteratively refines its\npredictions by reasoning over visual features and encoded clinical knowledge\nfrom text, enhancing both accuracy and consistency. On the publicly available\nPressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B,\nachieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based\nmodels by +4%. Unlike earlier CNN/ViT studies that relied solely on offline\nevaluations, FT-ARM is designed and tested for live inference, reflecting\nreal-time deployment conditions. Furthermore, it produces clinically grounded\nnatural-language explanations, improving interpretability and trust. By\nintegrating fine-tuning and reflective reasoning across multimodal inputs,\nFT-ARM advances the reliability, transparency, and clinical applicability of\nautomated wound assessment systems, addressing the critical need for consistent\nand explainable PU staging to support improved patient care.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FT-ARM\uff0c\u4e00\u79cd\u57fa\u4e8e\u5fae\u8c03\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u53cd\u5f0f\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u673a\u5236\u5728\u538b\u529b\u6027\u635f\u4f24\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8685%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u73b0\u6709CNN\u65b9\u6cd5\u63d0\u53474%\uff0c\u540c\u65f6\u63d0\u4f9b\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "motivation": "\u538b\u529b\u6027\u635f\u4f24\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u5b58\u5728\u89c6\u89c9\u7279\u5f81\u7ec6\u5fae\u5dee\u5f02\u548c\u4e3b\u89c2\u5224\u65ad\u53d8\u5f02\u6027\u7b49\u6311\u6218\uff0c\u73b0\u6709\u57fa\u4e8eCNN\u548cViT\u7684AI\u65b9\u6cd5\u867d\u7136\u51c6\u786e\u7387\u8f83\u9ad8\u4f46\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e34\u5e8a\u90e8\u7f72\u5bf9\u900f\u660e\u5ea6\u548c\u4e00\u81f4\u6027\u7684\u9700\u6c42\u3002", "method": "FT-ARM\u57fa\u4e8eLLaMA 3.2 90B\u8fdb\u884c\u5fae\u8c03\uff0c\u91c7\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u96c6\u6210\u4e86\u667a\u80fd\u4f53\u81ea\u53cd\u5f0f\u673a\u5236\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u8fc7\u7a0b\u5bf9\u89c6\u89c9\u7279\u5f81\u548c\u7f16\u7801\u7684\u4e34\u5e8a\u77e5\u8bc6\u8fdb\u884c\u7efc\u5408\u5206\u6790\uff0c\u6a21\u62df\u4e34\u5e8a\u533b\u751f\u7684\u8bca\u65ad\u518d\u8bc4\u4f30\u8fc7\u7a0b\u3002", "result": "\u5728\u516c\u5f00\u538b\u529b\u6027\u635f\u4f24\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cFT-ARM\u5728I-IV\u671f\u538b\u529b\u6027\u635f\u4f24\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u523085%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u5148\u524dCNN\u6a21\u578b\u63d0\u53474%\uff0c\u5e76\u5728\u5b9e\u65f6\u63a8\u7406\u573a\u666f\u4e0b\u9a8c\u8bc1\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u751f\u6210\u57fa\u4e8e\u4e34\u5e8a\u77e5\u8bc6\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "conclusion": "FT-ARM\u901a\u8fc7\u7ed3\u5408\u5fae\u8c03\u548c\u591a\u6a21\u6001\u81ea\u53cd\u5f0f\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u4f24\u53e3\u8bc4\u4f30\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3001\u900f\u660e\u5ea6\u548c\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u89e3\u51b3\u538b\u529b\u6027\u635f\u4f24\u5206\u671f\u7684\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027\u9700\u6c42\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2510.25701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25701", "abs": "https://arxiv.org/abs/2510.25701", "authors": ["Saeed AlMarri", "Kristof Juhasz", "Mathieu Ravaut", "Gautier Marti", "Hamdan Al Ahbabi", "Ibrahim Elfadel"], "title": "Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?", "comment": "8 pages, 6 figures, 3 tables, CIKM 2025 FinFAI workshop", "summary": "Large Language Models (LLMs) are increasingly explored as flexible\nalternatives to classical machine learning models for classification tasks\nthrough zero-shot prompting. However, their suitability for structured tabular\ndata remains underexplored, especially in high-stakes financial applications\nsuch as financial risk assessment. This study conducts a systematic comparison\nbetween zero-shot LLM-based classifiers and LightGBM, a state-of-the-art\ngradient-boosting model, on a real-world loan default prediction task. We\nevaluate their predictive performance, analyze feature attributions using SHAP,\nand assess the reliability of LLM-generated self-explanations. While LLMs are\nable to identify key financial risk indicators, their feature importance\nrankings diverge notably from LightGBM, and their self-explanations often fail\nto align with empirical SHAP attributions. These findings highlight the\nlimitations of LLMs as standalone models for structured financial risk\nprediction and raise concerns about the trustworthiness of their self-generated\nexplanations. Our results underscore the need for explainability audits,\nbaseline comparisons with interpretable models, and human-in-the-loop oversight\nwhen deploying LLMs in risk-sensitive financial environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u96f6\u6837\u672cLLM\u5206\u7c7b\u5668\u4e0eLightGBM\u5728\u91d1\u878d\u98ce\u9669\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLM\u5728\u7ed3\u6784\u5316\u91d1\u878d\u6570\u636e\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u4e14\u81ea\u89e3\u91ca\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u5728\u98ce\u9669\u654f\u611f\u91d1\u878d\u73af\u5883\u4e2d\u90e8\u7f72LLM\u65f6\u9700\u8981\u53ef\u89e3\u91ca\u6027\u5ba1\u8ba1\u548c\u4eba\u5de5\u76d1\u7763\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5206\u7c7b\u4efb\u52a1\u7684\u7075\u6d3b\u66ff\u4ee3\u65b9\u6848\u5728\u96f6\u6837\u672c\u63d0\u793a\u4e0b\u88ab\u5e7f\u6cdb\u63a2\u7d22\uff0c\u4f46\u5176\u5728\u7ed3\u6784\u5316\u8868\u683c\u6570\u636e\u7279\u522b\u662f\u9ad8\u98ce\u9669\u91d1\u878d\u5e94\u7528\u5982\u91d1\u878d\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u9002\u7528\u6027\u4ecd\u672a\u5145\u5206\u7814\u7a76\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u6b64\u7c7b\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\u548c\u53ef\u9760\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7cfb\u7edf\u6bd4\u8f83\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u4efb\u52a1\u4e2d\u5bf9\u6bd4\u96f6\u6837\u672cLLM\u5206\u7c7b\u5668\u4e0e\u6700\u5148\u8fdb\u7684\u68af\u5ea6\u63d0\u5347\u6a21\u578bLightGBM\uff0c\u4f7f\u7528SHAP\u8fdb\u884c\u7279\u5f81\u5f52\u56e0\u5206\u6790\uff0c\u5e76\u8bc4\u4f30LLM\u751f\u6210\u81ea\u89e3\u91ca\u7684\u53ef\u9760\u6027\uff0c\u5168\u9762\u8003\u5bdf\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u3001\u7279\u5f81\u91cd\u8981\u6027\u548c\u89e3\u91ca\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aLLM\u80fd\u591f\u8bc6\u522b\u5173\u952e\u91d1\u878d\u98ce\u9669\u6307\u6807\uff0c\u4f46\u5176\u7279\u5f81\u91cd\u8981\u6027\u6392\u5e8f\u4e0eLightGBM\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14LLM\u7684\u81ea\u89e3\u91ca\u5f80\u5f80\u65e0\u6cd5\u4e0e\u7ecf\u9a8cSHAP\u5f52\u56e0\u4fdd\u6301\u4e00\u81f4\uff0c\u8868\u660eLLM\u5728\u7ed3\u6784\u5316\u91d1\u878d\u98ce\u9669\u9884\u6d4b\u4e2d\u4f5c\u4e3a\u72ec\u7acb\u6a21\u578b\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u98ce\u9669\u654f\u611f\u91d1\u878d\u73af\u5883\u4e2d\u90e8\u7f72LLM\u65f6\u9700\u8981\u53ef\u89e3\u91ca\u6027\u5ba1\u8ba1\u3001\u4e0e\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u57fa\u7ebf\u6bd4\u8f83\u4ee5\u53ca\u4eba\u5de5\u76d1\u7763\u7684\u5fc5\u8981\u6027\uff0c\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u91d1\u878d\u9886\u57dfAI\u7cfb\u7edf\u7684\u53ef\u4fe1\u90e8\u7f72\u5177\u6709\u91cd\u8981\u6307\u5bfc\u610f\u4e49\uff0c\u63d0\u9192\u4e1a\u754c\u5173\u6ce8LLM\u81ea\u89e3\u91ca\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002"}}
{"id": "2510.25032", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25032", "abs": "https://arxiv.org/abs/2510.25032", "authors": ["Zahra Ebrahimi Vargoorani", "Amir Mohammad Ghoreyshi", "Ching Yee Suen"], "title": "Efficient License Plate Recognition via Pseudo-Labeled Supervision with Grounding DINO and YOLOv8", "comment": "6 pages, 8 figures. Presented at 2025 IEEE International Workshop on\n  Machine Learning for Signal Processing (MLSP), August 31 - September 3, 2025,\n  Istanbul, Turkey", "summary": "Developing a highly accurate automatic license plate recognition system\n(ALPR) is challenging due to environmental factors such as lighting, rain, and\ndust. Additional difficulties include high vehicle speeds, varying camera\nangles, and low-quality or low-resolution images. ALPR is vital in traffic\ncontrol, parking, vehicle tracking, toll collection, and law enforcement\napplications. This paper proposes a deep learning strategy using YOLOv8 for\nlicense plate detection and recognition tasks. This method seeks to enhance the\nperformance of the model using datasets from Ontario, Quebec, California, and\nNew York State. It achieved an impressive recall rate of 94% on the dataset\nfrom the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and\n91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised\nlearning framework, combining a small set of manually labeled data with\npseudo-labels generated by Grounding DINO to train our detection model.\nGrounding DINO, a powerful vision-language model, automatically annotates many\nimages with bounding boxes for license plates, thereby minimizing the reliance\non labor-intensive manual labeling. By integrating human-verified and\nmodel-generated annotations, we can scale our dataset efficiently while\nmaintaining label quality, which significantly enhances the training process\nand overall model performance. Furthermore, it reports character error rates\nfor both datasets, providing additional insight into system performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv8\u548c\u534a\u76d1\u7763\u5b66\u4e60\u7684\u81ea\u52a8\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408Grounding DINO\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u4e0e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u53ec\u56de\u7387\u548c\u5b57\u7b26\u9519\u8bef\u7387\u3002", "motivation": "\u81ea\u52a8\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf\u9762\u4e34\u73af\u5883\u56e0\u7d20\uff08\u5982\u5149\u7167\u3001\u96e8\u6c34\u3001\u7070\u5c18\uff09\u3001\u9ad8\u901f\u8f66\u8f86\u3001\u591a\u53d8\u6444\u50cf\u5934\u89d2\u5ea6\u4ee5\u53ca\u4f4e\u8d28\u91cf\u56fe\u50cf\u7b49\u6311\u6218\uff0c\u8fd9\u4e9b\u56e0\u7d20\u9650\u5236\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6280\u672f\u96be\u9898\uff0c\u63d0\u5347ALPR\u7cfb\u7edf\u5728\u590d\u6742\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u91c7\u7528\u57fa\u4e8eYOLOv8\u7684\u6df1\u5ea6\u5b66\u4e60\u7b56\u7565\u8fdb\u884c\u8f66\u724c\u68c0\u6d4b\u4e0e\u8bc6\u522b\uff0c\u5e76\u5f15\u5165\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u548cGrounding DINO\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u6765\u8bad\u7ec3\u68c0\u6d4b\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u5927\u91cf\u56fe\u50cf\u51cf\u5c11\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u7b7e\u8d28\u91cf\u3002", "result": "\u5728CENPARMI\u6570\u636e\u96c6\u4e0a\u8fbe\u523094%\u7684\u53ec\u56de\u7387\uff0c\u5728UFPR-ALPR\u6570\u636e\u96c6\u4e0a\u8fbe\u523091%\u7684\u53ec\u56de\u7387\uff0c\u540c\u65f6\u62a5\u544a\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\u7684\u5b57\u7b26\u9519\u8bef\u7387\uff0c\u4e3a\u7cfb\u7edf\u6027\u80fd\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u534a\u76d1\u7763\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u6210\u672c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6807\u6ce8\u6548\u7387\u3002"}}
{"id": "2510.25761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25761", "abs": "https://arxiv.org/abs/2510.25761", "authors": ["Chumeng Liang", "Jiaxuan You"], "title": "DiagramEval: Evaluating LLM-Generated Diagrams via Graphs", "comment": null, "summary": "Diagrams play a central role in research papers for conveying ideas, yet they\nare often notoriously complex and labor-intensive to create. Although diagrams\nare presented as images, standard image generative models struggle to produce\nclear diagrams with well-defined structure. We argue that a promising direction\nis to generate demonstration diagrams directly in textual form as SVGs, which\ncan leverage recent advances in large language models (LLMs). However, due to\nthe complexity of components and the multimodal nature of diagrams,\nsufficiently discriminative and explainable metrics for evaluating the quality\nof LLM-generated diagrams remain lacking. In this paper, we propose\nDiagramEval, a novel evaluation metric designed to assess demonstration\ndiagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams\nas graphs, treating text elements as nodes and their connections as directed\nedges, and evaluates diagram quality using two new groups of metrics: node\nalignment and path alignment. For the first time, we effectively evaluate\ndiagrams produced by state-of-the-art LLMs on recent research literature,\nquantitatively demonstrating the validity of our metrics. Furthermore, we show\nhow the enhanced explainability of our proposed metrics offers valuable\ninsights into the characteristics of LLM-generated diagrams. Code:\nhttps://github.com/ulab-uiuc/diagram-eval.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DiagramEval\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u7684\u6f14\u793a\u56fe\u8d28\u91cf\u3002\u8be5\u65b9\u6cd5\u5c06\u56fe\u8868\u793a\u4e3a\u56fe\u7ed3\u6784\uff0c\u901a\u8fc7\u8282\u70b9\u5bf9\u9f50\u548c\u8def\u5f84\u5bf9\u9f50\u4e24\u4e2a\u65b0\u6307\u6807\u7ec4\u6765\u91cf\u5316\u8bc4\u4f30\u56fe\u7684\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3LLM\u751f\u6210\u56fe\u8bc4\u4f30\u4e2d\u7f3a\u4e4f\u8db3\u591f\u533a\u5206\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6307\u6807\u7684\u95ee\u9898\u3002\u5c3d\u7ba1\u56fe\u5728\u8bba\u6587\u4e2d\u4f5c\u4e3a\u56fe\u50cf\u5448\u73b0\uff0c\u4f46\u6807\u51c6\u56fe\u50cf\u751f\u6210\u6a21\u578b\u96be\u4ee5\u751f\u6210\u5177\u6709\u660e\u786e\u7ed3\u6784\u7684\u6e05\u6670\u56fe\uff0c\u800cLLM\u76f4\u63a5\u751f\u6210SVG\u683c\u5f0f\u7684\u56fe\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "DiagramEval\u5c06\u56fe\u6982\u5ff5\u5316\u4e3a\u56fe\u7ed3\u6784\uff0c\u5c06\u6587\u672c\u5143\u7d20\u89c6\u4e3a\u8282\u70b9\uff0c\u8fde\u63a5\u5173\u7cfb\u89c6\u4e3a\u6709\u5411\u8fb9\u3002\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u6307\u6807\u7ec4\uff1a\u8282\u70b9\u5bf9\u9f50\u548c\u8def\u5f84\u5bf9\u9f50\uff0c\u901a\u8fc7\u56fe\u8868\u793a\u6765\u8bc4\u4f30\u56fe\u7684\u8d28\u91cf\u3002", "result": "\u7814\u7a76\u9996\u6b21\u6709\u6548\u8bc4\u4f30\u4e86\u6700\u5148\u8fdbLLM\u5728\u8fd1\u671f\u7814\u7a76\u6587\u732e\u4e0a\u751f\u6210\u7684\u56fe\uff0c\u5b9a\u91cf\u8bc1\u660e\u4e86\u6240\u63d0\u6307\u6807\u7684\u6709\u6548\u6027\u3002\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u4e3aLLM\u751f\u6210\u56fe\u7684\u7279\u5f81\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "DiagramEval\u4e3aLLM\u751f\u6210\u56fe\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cf\u5316\u6846\u67b6\uff0c\u5176\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3LLM\u751f\u6210\u56fe\u7684\u7279\u6027\uff0c\u4e3a\u672a\u6765\u56fe\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u6d1e\u5bdf\u3002"}}
{"id": "2510.25051", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25051", "abs": "https://arxiv.org/abs/2510.25051", "authors": ["Shunjie-Fabian Zheng", "Hyeonjun Lee", "Thijs Kooi", "Ali Diba"], "title": "Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference Models", "comment": "Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025", "summary": "Breast cancer remains the most commonly diagnosed malignancy among women in\nthe developed world. Early detection through mammography screening plays a\npivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)\nsystems have shown promise in assisting radiologists, existing approaches face\ncritical limitations in clinical deployment - particularly in handling the\nnuanced interpretation of multi-modal data and feasibility due to the\nrequirement of prior clinical history. This study introduces a novel framework\nthat synergistically combines visual features from 2D mammograms with\nstructured textual descriptors derived from easily accessible clinical metadata\nand synthesized radiological reports through innovative tokenization modules.\nOur proposed methods in this study demonstrate that strategic integration of\nconvolutional neural networks (ConvNets) with language representations achieves\nsuperior performance to vision transformer-based models while handling\nhigh-resolution images and enabling practical deployment across diverse\npopulations. By evaluating it on multi-national cohort screening mammograms,\nour multi-modal approach achieves superior performance in cancer detection and\ncalcification identification compared to unimodal baselines, with particular\nimprovements. The proposed method establishes a new paradigm for developing\nclinically viable VLM-based CAD systems that effectively leverage imaging data\nand contextual patient information through effective fusion mechanisms.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u5c062D\u4e73\u817aX\u7ebf\u6444\u5f71\u7684\u89c6\u89c9\u7279\u5f81\u4e0e\u4e34\u5e8a\u5143\u6570\u636e\u548c\u5408\u6210\u653e\u5c04\u5b66\u62a5\u544a\u7684\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e73\u817a\u764c\u68c0\u6d4b\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u764c\u75c7\u68c0\u6d4b\u548c\u9499\u5316\u8bc6\u522b\u65b9\u9762\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff0c\u4e3a\u5f00\u53d1\u4e34\u5e8a\u53ef\u884c\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u5728\u4e34\u5e8a\u90e8\u7f72\u4e2d\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u7684\u7ec6\u5fae\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u7531\u4e8e\u9700\u8981\u5148\u524d\u7684\u4e34\u5e8a\u5386\u53f2\u800c\u7f3a\u4e4f\u53ef\u884c\u6027\u3002\u4e73\u817a\u764c\u4f5c\u4e3a\u53d1\u8fbe\u56fd\u5bb6\u5973\u6027\u6700\u5e38\u89c1\u7684\u6076\u6027\u80bf\u7624\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u964d\u4f4e\u6b7b\u4ea1\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u53ef\u83b7\u53d6\u7684\u4e34\u5e8a\u4fe1\u606f\u548c\u5f71\u50cf\u6570\u636e\u7684\u534f\u540c\u6548\u5e94\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6807\u8bb0\u5316\u6a21\u5757\u5c062D\u4e73\u817aX\u7ebf\u6444\u5f71\u7684\u89c6\u89c9\u7279\u5f81\u4e0e\u6765\u81ea\u6613\u83b7\u53d6\u4e34\u5e8a\u5143\u6570\u636e\u548c\u5408\u6210\u653e\u5c04\u5b66\u62a5\u544a\u7684\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\u8fdb\u884c\u534f\u540c\u6574\u5408\u3002\u8be5\u65b9\u6cd5\u7b56\u7565\u6027\u5730\u5c06\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e0e\u8bed\u8a00\u8868\u793a\u76f8\u7ed3\u5408\uff0c\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u4e8e\u57fa\u4e8e\u89c6\u89c9\u53d8\u6362\u5668\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u652f\u6301\u5728\u4e0d\u540c\u4eba\u7fa4\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "result": "\u901a\u8fc7\u5728\u8de8\u56fd\u961f\u5217\u7b5b\u67e5\u4e73\u817aX\u7ebf\u6444\u5f71\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\uff0c\u8be5\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u764c\u75c7\u68c0\u6d4b\u548c\u9499\u5316\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7279\u5b9a\u6539\u8fdb\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u89c6\u89c9\u7279\u5f81\u4e0e\u6587\u672c\u63cf\u8ff0\u7684\u6709\u6548\u878d\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u4e34\u5e8a\u53ef\u884c\u7684\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6709\u6548\u7684\u878d\u5408\u673a\u5236\u5145\u5206\u5229\u7528\u5f71\u50cf\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u60a3\u8005\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u591a\u6a21\u6001\u6574\u5408\u5728\u533b\u7597\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u4e34\u5e8a\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.25067", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25067", "abs": "https://arxiv.org/abs/2510.25067", "authors": ["Yusen Peng", "Sachin Kumar"], "title": "DRIP: Dynamic patch Reduction via Interpretable Pooling", "comment": null, "summary": "Recently, the advances in vision-language models, including contrastive\npretraining and instruction tuning, have greatly pushed the frontier of\nmultimodal AI. However, owing to the large-scale and hence expensive\npretraining, the efficiency concern has discouraged researchers from attempting\nto pretrain a vision language model from scratch. In this work, we propose\nDynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the\ninput images and dynamically merges tokens in the deeper layers of a visual\nencoder. Our results on both ImageNet training from scratch and CLIP\ncontrastive pretraining demonstrate a significant GFLOP reduction while\nmaintaining comparable classification/zero-shot performance. To further\nvalidate our proposed method, we conduct continual pretraining on a large\nbiology dataset, extending its impact into scientific domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u52a8\u6001\u8865\u4e01\u7f29\u51cf\u53ef\u89e3\u91ca\u6c60\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5408\u5e76\u6df1\u5c42\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u7684\u4ee4\u724c\uff0c\u5728\u4fdd\u6301\u5206\u7c7b\u548c\u96f6\u6837\u672c\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u8be5\u65b9\u6cd5\u5728ImageNet\u4ece\u5934\u8bad\u7ec3\u548cCLIP\u5bf9\u6bd4\u9884\u8bad\u7ec3\u4e2d\u5747\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u751f\u7269\u5b66\u9886\u57df\u7684\u5927\u89c4\u6a21\u6301\u7eed\u9884\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u7814\u7a76\u8005\u5f80\u5f80\u907f\u514d\u4ece\u5934\u5f00\u59cb\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u6765\u63a8\u52a8\u591a\u6a21\u6001AI\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u8865\u4e01\u7f29\u51cf\u53ef\u89e3\u91ca\u6c60\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u8f93\u5165\u56fe\u50cf\u81ea\u9002\u5e94\u5730\u5728\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6df1\u5c42\u5408\u5e76\u4ee4\u724c\u3002\u8fd9\u79cd\u52a8\u6001\u5408\u5e76\u673a\u5236\u80fd\u591f\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u573a\u666f\u3002", "result": "\u5728ImageNet\u4ece\u5934\u8bad\u7ec3\u548cCLIP\u5bf9\u6bd4\u9884\u8bad\u7ec3\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u7684GFLOPs\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u7684\u5206\u7c7b\u548c\u96f6\u6837\u672c\u6027\u80fd\u3002\u5728\u5927\u578b\u751f\u7269\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u52a8\u6001\u8865\u4e01\u7f29\u51cf\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6548\u7387\u4f18\u5316\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u901a\u7528\u9886\u57df\uff0c\u8fd8\u80fd\u6269\u5c55\u5230\u79d1\u5b66\u8ba1\u7b97\u7b49\u4e13\u4e1a\u9886\u57df\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.25070", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25070", "abs": "https://arxiv.org/abs/2510.25070", "authors": ["Manjunath Prasad Holenarasipura Rajiv", "B. M. Vidyavathi"], "title": "Vision-Language Integration for Zero-Shot Scene Understanding in Real-World Environments", "comment": "Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025", "summary": "Zero-shot scene understanding in real-world settings presents major\nchallenges due to the complexity and variability of natural scenes, where\nmodels must recognize new objects, actions, and contexts without prior labeled\nexamples. This work proposes a vision-language integration framework that\nunifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models\n(e.g., GPT-based architectures) to achieve semantic alignment between visual\nand textual modalities. The goal is to enable robust zero-shot comprehension of\nscenes by leveraging natural language as a bridge to generalize over unseen\ncategories and contexts. Our approach develops a unified model that embeds\nvisual inputs and textual prompts into a shared space, followed by multimodal\nfusion and reasoning layers for contextual interpretation. Experiments on\nVisual Genome, COCO, ADE20K, and custom real-world datasets demonstrate\nsignificant gains over state-of-the-art zero-shot models in object recognition,\nactivity detection, and scene captioning. The proposed system achieves up to\n18% improvement in top-1 accuracy and notable gains in semantic coherence\nmetrics, highlighting the effectiveness of cross-modal alignment and language\ngrounding in enhancing generalization for real-world scene understanding.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9-\u8bed\u8a00\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u96f6\u6837\u672c\u7406\u89e3\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u81ea\u7136\u573a\u666f\u7684\u590d\u6742\u6027\u548c\u591a\u53d8\u6027\u8981\u6c42\u6a21\u578b\u5728\u6ca1\u6709\u5148\u9a8c\u6807\u6ce8\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u65b0\u5bf9\u8c61\u3001\u52a8\u4f5c\u548c\u4e0a\u4e0b\u6587\uff0c\u8fd9\u9700\u8981\u89e3\u51b3\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\uff0c\u5c06\u89c6\u89c9\u8f93\u5165\u548c\u6587\u672c\u63d0\u793a\u5d4c\u5165\u5230\u5171\u4eab\u7a7a\u95f4\u4e2d\uff0c\u968f\u540e\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u63a8\u7406\u5c42\u8fdb\u884c\u4e0a\u4e0b\u6587\u89e3\u91ca\uff0c\u96c6\u6210\u4e86CLIP\u3001ViT\u7b49\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u4e0eGPT\u67b6\u6784\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728Visual Genome\u3001COCO\u3001ADE20K\u548c\u81ea\u5b9a\u4e49\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7269\u4f53\u8bc6\u522b\u3001\u6d3b\u52a8\u68c0\u6d4b\u548c\u573a\u666f\u63cf\u8ff0\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe18%\u7684top-1\u51c6\u786e\u7387\u63d0\u5347\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u6307\u6807\u7684\u663e\u8457\u589e\u76ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u8bed\u8a00\u63a5\u5730\u5728\u589e\u5f3a\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7406\u89e3\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u96f6\u6837\u672c\u89c6\u89c9\u7406\u89e3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u89c6\u89c9-\u8bed\u8a00\u96c6\u6210\u6846\u67b6\u5728\u590d\u6742\u573a\u666f\u7406\u89e3\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.25094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25094", "abs": "https://arxiv.org/abs/2510.25094", "authors": ["Chanhyeong Yang", "Taehoon Song", "Jihwan Park", "Hyunwoo J. Kim"], "title": "Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection", "comment": "Accepted by NeurIPS 2025", "summary": "Zero-shot Human-Object Interaction detection aims to localize humans and\nobjects in an image and recognize their interaction, even when specific\nverb-object pairs are unseen during training. Recent works have shown promising\nresults using prompt learning with pretrained vision-language models such as\nCLIP, which align natural language prompts with visual features in a shared\nembedding space. However, existing approaches still fail to handle the visual\ncomplexity of interaction, including (1) intra-class visual diversity, where\ninstances of the same verb appear in diverse poses and contexts, and (2)\ninter-class visual entanglement, where distinct verbs yield visually similar\npatterns. To address these challenges, we propose VDRP, a framework for Visual\nDiversity and Region-aware Prompt learning. First, we introduce a visual\ndiversity-aware prompt learning strategy that injects group-wise visual\nvariance into the context embedding. We further apply Gaussian perturbation to\nencourage the prompts to capture diverse visual variations of a verb. Second,\nwe retrieve region-specific concepts from the human, object, and union regions.\nThese are used to augment the diversity-aware prompt embeddings, yielding\nregion-aware prompts that enhance verb-level discrimination. Experiments on the\nHICO-DET benchmark demonstrate that our method achieves state-of-the-art\nperformance under four zero-shot evaluation settings, effectively addressing\nboth intra-class diversity and inter-class visual entanglement. Code is\navailable at https://github.com/mlvlab/VDRP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VDRP\u6846\u67b6\uff0c\u4e00\u79cd\u9488\u5bf9\u96f6\u6837\u672c\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\u7684\u89c6\u89c9\u591a\u6837\u6027\u548c\u533a\u57df\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u591a\u6837\u6027\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u548c\u533a\u57df\u7279\u5b9a\u6982\u5ff5\u68c0\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u540c\u7c7b\u89c6\u89c9\u591a\u6837\u6027\u548c\u5f02\u7c7b\u89c6\u89c9\u7ea0\u7f20\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCLIP\u7b49\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u4ea4\u4e92\u7684\u89c6\u89c9\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u540c\u7c7b\u89c6\u89c9\u591a\u6837\u6027\uff08\u540c\u4e00\u52a8\u8bcd\u5728\u4e0d\u540c\u59ff\u6001\u548c\u4e0a\u4e0b\u6587\u4e2d\u7684\u89c6\u89c9\u8868\u73b0\u5dee\u5f02\uff09\u548c\u5f02\u7c7b\u89c6\u89c9\u7ea0\u7f20\uff08\u4e0d\u540c\u52a8\u8bcd\u4ea7\u751f\u76f8\u4f3c\u89c6\u89c9\u6a21\u5f0f\uff09\u8fd9\u4e24\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "VDRP\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u89c6\u89c9\u591a\u6837\u6027\u611f\u77e5\u63d0\u793a\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u5206\u7ec4\u89c6\u89c9\u65b9\u5dee\u6ce8\u5165\u4e0a\u4e0b\u6587\u5d4c\u5165\u5e76\u5e94\u7528\u9ad8\u65af\u6270\u52a8\u6765\u6355\u6349\u52a8\u8bcd\u7684\u591a\u6837\u5316\u89c6\u89c9\u53d8\u5316\uff1b\u533a\u57df\u7279\u5b9a\u6982\u5ff5\u68c0\u7d22\u673a\u5236\uff0c\u4ece\u4eba\u3001\u7269\u548c\u8054\u5408\u533a\u57df\u63d0\u53d6\u6982\u5ff5\u6765\u589e\u5f3a\u591a\u6837\u6027\u611f\u77e5\u63d0\u793a\u5d4c\u5165\uff0c\u751f\u6210\u80fd\u591f\u63d0\u5347\u52a8\u8bcd\u7ea7\u522b\u533a\u5206\u5ea6\u7684\u533a\u57df\u611f\u77e5\u63d0\u793a\u3002", "result": "\u5728HICO-DET\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56db\u79cd\u96f6\u6837\u672c\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u540c\u7c7b\u89c6\u89c9\u591a\u6837\u6027\u548c\u5f02\u7c7b\u89c6\u89c9\u7ea0\u7f20\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u590d\u6742\u89c6\u89c9\u4ea4\u4e92\u6a21\u5f0f\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u591a\u6837\u6027\u5efa\u6a21\u548c\u533a\u57df\u611f\u77e5\u63d0\u793a\u5b66\u4e60\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u5904\u7406\u590d\u6742\u89c6\u89c9\u4ea4\u4e92\u6a21\u5f0f\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5e76\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.25146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25146", "abs": "https://arxiv.org/abs/2510.25146", "authors": ["Xiaoyu Zhou", "Jingqi Wang", "Yuang Jia", "Yongtao Wang", "Deqing Sun", "Ming-Hsuan Yang"], "title": "EA3D: Online Open-World 3D Object Extraction from Streaming Videos", "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems(NeurIPS 2025)", "summary": "Current 3D scene understanding methods are limited by offline-collected\nmulti-view data or pre-constructed 3D geometry. In this paper, we present\nExtractAnything3D (EA3D), a unified online framework for open-world 3D object\nextraction that enables simultaneous geometric reconstruction and holistic\nscene understanding. Given a streaming video, EA3D dynamically interprets each\nframe using vision-language and 2D vision foundation encoders to extract\nobject-level knowledge. This knowledge is integrated and embedded into a\nGaussian feature map via a feed-forward online update strategy. We then\niteratively estimate visual odometry from historical frames and incrementally\nupdate online Gaussian features with new observations. A recurrent joint\noptimization module directs the model's attention to regions of interest,\nsimultaneously enhancing both geometric reconstruction and semantic\nunderstanding. Extensive experiments across diverse benchmarks and tasks,\nincluding photo-realistic rendering, semantic and instance segmentation, 3D\nbounding box and semantic occupancy estimation, and 3D mesh generation,\ndemonstrate the effectiveness of EA3D. Our method establishes a unified and\nefficient framework for joint online 3D reconstruction and holistic scene\nunderstanding, enabling a broad range of downstream tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ExtractAnything3D\uff08EA3D\uff09\uff0c\u4e00\u4e2a\u7528\u4e8e\u5f00\u653e\u4e16\u754c3D\u7269\u4f53\u63d0\u53d6\u7684\u7edf\u4e00\u5728\u7ebf\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u51e0\u4f55\u91cd\u5efa\u548c\u6574\u4f53\u573a\u666f\u7406\u89e3\uff0c\u901a\u8fc7\u52a8\u6001\u96c6\u6210\u89c6\u89c9\u8bed\u8a00\u77e5\u8bc6\u548c\u5728\u7ebf\u9ad8\u65af\u7279\u5f81\u66f4\u65b0\u6765\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d3D\u573a\u666f\u7406\u89e3\u65b9\u6cd5\u53d7\u9650\u4e8e\u79bb\u7ebf\u6536\u96c6\u7684\u591a\u89c6\u89d2\u6570\u636e\u6216\u9884\u6784\u5efa\u76843D\u51e0\u4f55\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5728\u7ebf\u52a8\u6001\u73af\u5883\u4e0b\u540c\u65f6\u8fdb\u884c\u51e0\u4f55\u91cd\u5efa\u548c\u8bed\u4e49\u7406\u89e3\u7684\u6311\u6218\uff0c\u586b\u8865\u5f00\u653e\u4e16\u754c3D\u7269\u4f53\u63d0\u53d6\u6846\u67b6\u7684\u7a7a\u767d\u3002", "method": "EA3D\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u548c2D\u89c6\u89c9\u57fa\u7840\u7f16\u7801\u5668\u52a8\u6001\u89e3\u91ca\u89c6\u9891\u6d41\u5e27\uff0c\u901a\u8fc7\u524d\u9988\u5728\u7ebf\u66f4\u65b0\u7b56\u7565\u5c06\u7269\u4f53\u7ea7\u77e5\u8bc6\u96c6\u6210\u5230\u9ad8\u65af\u7279\u5f81\u56fe\u4e2d\uff0c\u7ed3\u5408\u8fed\u4ee3\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u548c\u589e\u91cf\u7279\u5f81\u66f4\u65b0\uff0c\u5e76\u91c7\u7528\u5faa\u73af\u8054\u5408\u4f18\u5316\u6a21\u5757\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u611f\u5174\u8da3\u533a\u57df\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u548c\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEA3D\u5728\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u6e32\u67d3\u3001\u8bed\u4e49\u548c\u5b9e\u4f8b\u5206\u5272\u30013D\u8fb9\u754c\u6846\u548c\u8bed\u4e49\u5360\u636e\u4f30\u8ba1\u4ee5\u53ca3D\u7f51\u683c\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u7edf\u4e00\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u9ad8\u6548\u7684\u5728\u7ebf3D\u91cd\u5efa\u548c\u6574\u4f53\u573a\u666f\u7406\u89e3\u6846\u67b6\uff0c\u4e3a\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u57fa\u7840\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u5f00\u653e\u4e16\u754c3D\u573a\u666f\u7406\u89e3\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.25163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25163", "abs": "https://arxiv.org/abs/2510.25163", "authors": ["Wenhao Zheng", "Chenwei Sun", "Wenbo Zhang", "Jiancheng Lv", "Xianggen Liu"], "title": "Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD Generation", "comment": null, "summary": "Deep generative models, such as diffusion models, have shown promising\nprogress in image generation and audio generation via simplified continuity\nassumptions. However, the development of generative modeling techniques for\ngenerating multi-modal data, such as parametric CAD sequences, still lags\nbehind due to the challenges in addressing long-range constraints and parameter\nsensitivity. In this work, we propose a novel framework for quantitatively\nconstrained CAD generation, termed Target-Guided Bayesian Flow Network (TGBFN).\nFor the first time, TGBFN handles the multi-modality of CAD sequences (i.e.,\ndiscrete commands and continuous parameters) in a unified continuous and\ndifferentiable parameter space rather than in the discrete data space. In\naddition, TGBFN penetrates the parameter update kernel and introduces a guided\nBayesian flow to control the CAD properties. To evaluate TGBFN, we construct a\nnew dataset for quantitatively constrained CAD generation. Extensive\ncomparisons across single-condition and multi-condition constrained generation\ntasks demonstrate that TGBFN achieves state-of-the-art performance in\ngenerating high-fidelity, condition-aware CAD sequences. The code is available\nat https://github.com/scu-zwh/TGBFN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u76ee\u6807\u5f15\u5bfc\u8d1d\u53f6\u65af\u6d41\u7f51\u7edc\uff08TGBFN\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u5b9a\u91cf\u7ea6\u675fCAD\u751f\u6210\u7684\u65b0\u6846\u67b6\uff0c\u9996\u6b21\u5728\u7edf\u4e00\u7684\u8fde\u7eed\u53ef\u5fae\u5206\u53c2\u6570\u7a7a\u95f4\u4e2d\u5904\u7406CAD\u5e8f\u5217\u7684\u591a\u6a21\u6001\u7279\u6027\uff0c\u5e76\u5728\u5355\u6761\u4ef6\u548c\u591a\u6761\u4ef6\u7ea6\u675f\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u53c2\u6570\u5316CAD\u5e8f\u5217\uff09\u7684\u751f\u6210\u5efa\u6a21\u6280\u672f\u53d1\u5c55\u6ede\u540e\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u5904\u7406\u957f\u7a0b\u7ea6\u675f\u548c\u53c2\u6570\u654f\u611f\u6027\uff0c\u8fd9\u9650\u5236\u4e86CAD\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002", "method": "TGBFN\u6846\u67b6\u901a\u8fc7\u5c06CAD\u5e8f\u5217\u7684\u79bb\u6563\u547d\u4ee4\u548c\u8fde\u7eed\u53c2\u6570\u7edf\u4e00\u6620\u5c04\u5230\u8fde\u7eed\u53ef\u5fae\u5206\u53c2\u6570\u7a7a\u95f4\u6765\u5904\u7406\u591a\u6a21\u6001\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u5f15\u5bfc\u8d1d\u53f6\u65af\u6d41\u673a\u5236\u6765\u7a7f\u900f\u53c2\u6570\u66f4\u65b0\u6838\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9CAD\u5c5e\u6027\u7684\u7cbe\u786e\u63a7\u5236\u3002", "result": "\u5728\u65b0\u5efa\u7684\u5b9a\u91cf\u7ea6\u675fCAD\u751f\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTGBFN\u5728\u5355\u6761\u4ef6\u548c\u591a\u6761\u4ef6\u7ea6\u675f\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u4e14\u6761\u4ef6\u611f\u77e5\u7684CAD\u5e8f\u5217\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u7edf\u4e00\u8fde\u7eed\u7a7a\u95f4\u4e2d\u5904\u7406CAD\u591a\u6a21\u6001\u6570\u636e\u7684\u6709\u6548\u6027\uff0c\u4e3a\u53c2\u6570\u5316CAD\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5e76\u5c55\u793a\u4e86\u5f15\u5bfc\u8d1d\u53f6\u65af\u6d41\u5728\u63a7\u5236\u751f\u6210\u5c5e\u6027\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u590d\u6742\u5de5\u7a0b\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.25327", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25327", "abs": "https://arxiv.org/abs/2510.25327", "authors": ["Runxi Huang", "Mingxuan Yu", "Mingyu Tsoi", "Xiaomin Ouyang"], "title": "MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding", "comment": "Accepted by SenSys 2026", "summary": "Real-time multimodal inference on resource-constrained edge devices is\nessential for applications such as autonomous driving, human-computer\ninteraction, and mobile health. However, prior work often overlooks the tight\ncoupling between sensing dynamics and model execution, as well as the complex\ninter-modality dependencies. In this paper, we propose MMEdge, an new on-device\nmulti-modal inference framework based on pipelined sensing and encoding.\nInstead of waiting for complete sensor inputs, MMEdge decomposes the entire\ninference process into a sequence of fine-grained sensing and encoding units,\nallowing computation to proceed incrementally as data arrive. MMEdge also\nintroduces a lightweight but effective temporal aggregation module that\ncaptures rich temporal dynamics across different pipelined units to maintain\naccuracy performance. Such pipelined design also opens up opportunities for\nfine-grained cross-modal optimization and early decision-making during\ninference. To further enhance system performance under resource variability and\ninput data complexity, MMEdge incorporates an adaptive multimodal configuration\noptimizer that dynamically selects optimal sensing and model configurations for\neach modality under latency constraints, and a cross-modal speculative skipping\nmechanism that bypasses future units of slower modalities when early\npredictions reach sufficient confidence. We evaluate MMEdge using two public\nmultimodal datasets and deploy it on a real-world unmanned aerial vehicle\n(UAV)-based multimodal testbed. The results show that MMEdge significantly\nreduces end-to-end latency while maintaining high task accuracy across various\nsystem and data dynamics.", "AI": {"tldr": "MMEdge\u662f\u4e00\u4e2a\u57fa\u4e8e\u6d41\u6c34\u7ebf\u611f\u77e5\u548c\u7f16\u7801\u7684\u7aef\u4fa7\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u589e\u91cf\u8ba1\u7b97\u548c\u8de8\u6a21\u6001\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u611f\u77e5\u52a8\u6001\u4e0e\u6a21\u578b\u6267\u884c\u4e4b\u95f4\u7684\u7d27\u5bc6\u8026\u5408\u4ee5\u53ca\u590d\u6742\u7684\u6a21\u6001\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u800c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u591a\u6a21\u6001\u63a8\u7406\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u3001\u4eba\u673a\u4ea4\u4e92\u548c\u79fb\u52a8\u5065\u5eb7\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "MMEdge\u5c06\u6574\u4e2a\u63a8\u7406\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u7ec6\u7c92\u5ea6\u7684\u611f\u77e5\u548c\u7f16\u7801\u5355\u5143\uff0c\u91c7\u7528\u589e\u91cf\u8ba1\u7b97\u65b9\u5f0f\u5904\u7406\u5230\u8fbe\u6570\u636e\uff1b\u5f15\u5165\u8f7b\u91cf\u7ea7\u65f6\u95f4\u805a\u5408\u6a21\u5757\u6355\u83b7\u8de8\u6d41\u6c34\u7ebf\u5355\u5143\u7684\u4e30\u5bcc\u65f6\u95f4\u52a8\u6001\uff1b\u5305\u542b\u81ea\u9002\u5e94\u591a\u6a21\u6001\u914d\u7f6e\u4f18\u5316\u5668\u548c\u8de8\u6a21\u6001\u63a8\u6d4b\u8df3\u8fc7\u673a\u5236\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f18\u914d\u7f6e\u5e76\u5728\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u8db3\u591f\u65f6\u8df3\u8fc7\u8f83\u6162\u6a21\u6001\u7684\u672a\u6765\u5355\u5143\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u4ee5\u53ca\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u591a\u6a21\u6001\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u7ed3\u679c\u8868\u660e\uff0cMMEdge\u5728\u5404\u79cd\u7cfb\u7edf\u548c\u6570\u636e\u52a8\u6001\u4e0b\u663e\u8457\u964d\u4f4e\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u4efb\u52a1\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6d41\u6c34\u7ebf\u611f\u77e5\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u89e3\u8026\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u611f\u77e5\u4e0e\u8ba1\u7b97\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u591a\u6a21\u6001\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u6a21\u6001\u4f18\u5316\u548c\u65e9\u671f\u51b3\u7b56\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.25173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25173", "abs": "https://arxiv.org/abs/2510.25173", "authors": ["Kejing Xia", "Jidong Jia", "Ke Jin", "Yucai Bai", "Li Sun", "Dacheng Tao", "Youjian Zhang"], "title": "$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction", "comment": null, "summary": "Recently, Gaussian Splatting (GS) has shown great potential for urban scene\nreconstruction in the field of autonomous driving. However, current urban scene\nreconstruction methods often depend on multimodal sensors as inputs,\n\\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR\npoint clouds can largely mitigate ill-posedness in reconstruction, acquiring\nsuch accurate LiDAR data is still challenging in practice: i) precise\nspatiotemporal calibration between LiDAR and other sensors is required, as they\nmay not capture data simultaneously; ii) reprojection errors arise from spatial\nmisalignment when LiDAR and cameras are mounted at different locations. To\navoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a\nLiDAR-free urban scene reconstruction framework. In this work, we obtain\ngeometry priors that are as effective as LiDAR while being denser and more\naccurate. $\\textbf{First}$, we initialize a dense point cloud by\nback-projecting multi-view metric depth predictions. This point cloud is then\noptimized by a Progressive Pruning strategy to improve the global consistency.\n$\\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense\nmetric depth via a Depth Enhancer. Specifically, we leverage diffusion priors\nfrom a depth foundation model to enhance the depth maps rendered by Gaussians.\nIn turn, the enhanced depths provide stronger geometric constraints during\nGaussian training. $\\textbf{Finally}$, we improve the accuracy of ground\ngeometry by constraining the shape and normal attributes of Gaussians within\nroad regions. Extensive experiments on the Waymo dataset demonstrate that our\nmethod consistently outperforms state-of-the-art methods, producing more\naccurate geometry even when compared with those using ground-truth LiDAR data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700LiDAR\u7684\u57ce\u5e02\u573a\u666f\u91cd\u5efa\u6846\u67b6D\u00b2GS\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u6df1\u5ea6\u9884\u6d4b\u548c\u6269\u6563\u5148\u9a8c\u83b7\u5f97\u6bd4LiDAR\u66f4\u5bc6\u96c6\u51c6\u786e\u7684\u51e0\u4f55\u5148\u9a8c\uff0c\u5728Waymo\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5305\u62ec\u4f7f\u7528\u771f\u5b9eLiDAR\u6570\u636e\u7684\u65b9\u6cd5\u5728\u5185\u7684\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57ce\u5e02\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u591a\u6a21\u6001\u4f20\u611f\u5668\u8f93\u5165\uff08\u5982LiDAR\u548c\u56fe\u50cf\uff09\uff0c\u4f46\u83b7\u53d6\u7cbe\u786eLiDAR\u6570\u636e\u5b58\u5728\u6311\u6218\uff1a\u9700\u8981\u7cbe\u786e\u7684\u65f6\u7a7a\u6807\u5b9a\uff0c\u4e14LiDAR\u4e0e\u76f8\u673a\u5b89\u88c5\u4f4d\u7f6e\u4e0d\u540c\u4f1a\u4ea7\u751f\u91cd\u6295\u5f71\u8bef\u5dee\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u5f00\u53d1\u65e0\u9700LiDAR\u7684\u9ad8\u8d28\u91cf\u57ce\u5e02\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u63d0\u51faD\u00b2GS\u6846\u67b6\uff0c\u9996\u5148\u901a\u8fc7\u591a\u89c6\u89d2\u5ea6\u91cf\u6df1\u5ea6\u9884\u6d4b\u53cd\u6295\u5f71\u521d\u59cb\u5316\u5bc6\u96c6\u70b9\u4e91\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u526a\u679d\u7b56\u7565\u4f18\u5316\u5168\u5c40\u4e00\u81f4\u6027\uff1b\u5176\u6b21\u901a\u8fc7\u6df1\u5ea6\u589e\u5f3a\u5668\u8054\u5408\u4f18\u5316\u9ad8\u65af\u51e0\u4f55\u548c\u9884\u6d4b\u6df1\u5ea6\uff0c\u5229\u7528\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u7684\u6269\u6563\u5148\u9a8c\u589e\u5f3a\u9ad8\u65af\u6e32\u67d3\u7684\u6df1\u5ea6\u56fe\uff1b\u6700\u540e\u5728\u9053\u8def\u533a\u57df\u7ea6\u675f\u9ad8\u65af\u5f62\u72b6\u548c\u6cd5\u5411\u91cf\u5c5e\u6027\u4ee5\u6539\u8fdb\u5730\u9762\u51e0\u4f55\u7cbe\u5ea6\u3002", "result": "\u5728Waymo\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5373\u4f7f\u4e0e\u4f7f\u7528\u771f\u5b9eLiDAR\u6570\u636e\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4e5f\u80fd\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u51e0\u4f55\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u65e0\u9700LiDAR\u4f20\u611f\u5668\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u57ce\u5e02\u573a\u666f\u91cd\u5efa\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6df1\u5ea6\u9884\u6d4b\u548c\u6269\u6563\u5148\u9a8c\u7684\u7ec4\u5408\u53ef\u4ee5\u4ea7\u751f\u6bd4\u5b9e\u9645LiDAR\u6570\u636e\u66f4\u5bc6\u96c6\u51c6\u786e\u7684\u51e0\u4f55\u5148\u9a8c\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.25175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25175", "abs": "https://arxiv.org/abs/2510.25175", "authors": ["Yingjie Gao", "Yanan Zhang", "Zhi Cai", "Di Huang"], "title": "Test-Time Adaptive Object Detection with Foundation Model", "comment": "Accepted by NeurIPS 2025", "summary": "In recent years, test-time adaptive object detection has attracted increasing\nattention due to its unique advantages in online domain adaptation, which\naligns more closely with real-world application scenarios. However, existing\napproaches heavily rely on source-derived statistical characteristics while\nmaking the strong assumption that the source and target domains share an\nidentical category space. In this paper, we propose the first foundation\nmodel-powered test-time adaptive object detection method that eliminates the\nneed for source data entirely and overcomes traditional closed-set limitations.\nSpecifically, we design a Multi-modal Prompt-based Mean-Teacher framework for\nvision-language detector-driven test-time adaptation, which incorporates text\nand visual prompt tuning to adapt both language and vision representation\nspaces on the test data in a parameter-efficient manner. Correspondingly, we\npropose a Test-time Warm-start strategy tailored for the visual prompts to\neffectively preserve the representation capability of the vision branch.\nFurthermore, to guarantee high-quality pseudo-labels in every test batch, we\nmaintain an Instance Dynamic Memory (IDM) module that stores high-quality\npseudo-labels from previous test samples, and propose two novel\nstrategies-Memory Enhancement and Memory Hallucination-to leverage IDM's\nhigh-quality instances for enhancing original predictions and hallucinating\nimages without available pseudo-labels, respectively. Extensive experiments on\ncross-corruption and cross-dataset benchmarks demonstrate that our method\nconsistently outperforms previous state-of-the-art methods, and can adapt to\narbitrary cross-domain and cross-category target data. Code is available at\nhttps://github.com/gaoyingjay/ttaod_foundation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u63d0\u793a\u8c03\u4f18\u548c\u5b9e\u4f8b\u52a8\u6001\u8bb0\u5fc6\u6a21\u5757\uff0c\u5728\u65e0\u9700\u6e90\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u8de8\u57df\u548c\u8de8\u7c7b\u522b\u7684\u81ea\u9002\u5e94\u68c0\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u6e90\u57df\u7edf\u8ba1\u7279\u5f81\uff0c\u5e76\u5047\u8bbe\u6e90\u57df\u548c\u76ee\u6807\u57df\u5177\u6709\u76f8\u540c\u7684\u7c7b\u522b\u7a7a\u95f4\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u6d88\u9664\u5bf9\u6e90\u6570\u636e\u7684\u4f9d\u8d56\u5e76\u514b\u670d\u4f20\u7edf\u95ed\u96c6\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u81ea\u9002\u5e94\u68c0\u6d4b\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u63d0\u793a\u5747\u503c\u6559\u5e08\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u4ee5\u53c2\u6570\u9ad8\u6548\u65b9\u5f0f\u9002\u5e94\u6d4b\u8bd5\u6570\u636e\u7684\u8bed\u8a00\u548c\u89c6\u89c9\u8868\u793a\u7a7a\u95f4\uff1b\u8bbe\u8ba1\u6d4b\u8bd5\u65f6\u70ed\u542f\u52a8\u7b56\u7565\u4fdd\u62a4\u89c6\u89c9\u5206\u652f\u8868\u793a\u80fd\u529b\uff1b\u6784\u5efa\u5b9e\u4f8b\u52a8\u6001\u8bb0\u5fc6\u6a21\u5757\u5b58\u50a8\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u5e76\u63d0\u51fa\u8bb0\u5fc6\u589e\u5f3a\u548c\u8bb0\u5fc6\u5e7b\u89c9\u7b56\u7565\u63d0\u5347\u9884\u6d4b\u8d28\u91cf\u3002", "result": "\u5728\u8de8\u635f\u574f\u548c\u8de8\u6570\u636e\u96c6\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u9002\u5e94\u4efb\u610f\u8de8\u57df\u548c\u8de8\u7c7b\u522b\u7684\u76ee\u6807\u6570\u636e\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u65e0\u9700\u6e90\u6570\u636e\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u95ed\u96c6\u5047\u8bbe\u9650\u5236\uff0c\u4e3a\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u81ea\u9002\u5e94\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.25199", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25199", "abs": "https://arxiv.org/abs/2510.25199", "authors": ["Manisha More", "Kavya Bhand", "Kaustubh Mukdam", "Kavya Sharma", "Manas Kawtikwar", "Hridayansh Kaware", "Prajwal Kavhar"], "title": "AI-Powered Early Detection of Critical Diseases using Image Processing and Audio Analysis", "comment": null, "summary": "Early diagnosis of critical diseases can significantly improve patient\nsurvival and reduce treatment costs. However, existing diagnostic techniques\nare often costly, invasive, and inaccessible in low-resource regions. This\npaper presents a multimodal artificial intelligence (AI) diagnostic framework\nintegrating image analysis, thermal imaging, and audio signal processing for\nearly detection of three major health conditions: skin cancer, vascular blood\nclots, and cardiopulmonary abnormalities. A fine-tuned MobileNetV2\nconvolutional neural network was trained on the ISIC 2019 dataset for skin\nlesion classification, achieving 89.3% accuracy, 91.6% sensitivity, and 88.2%\nspecificity. A support vector machine (SVM) with handcrafted features was\nemployed for thermal clot detection, achieving 86.4% accuracy (AUC = 0.89) on\nsynthetic and clinical data. For cardiopulmonary analysis, lung and heart sound\ndatasets from PhysioNet and Pascal were processed using Mel-Frequency Cepstral\nCoefficients (MFCC) and classified via Random Forest, reaching 87.2% accuracy\nand 85.7% sensitivity. Comparative evaluation against state-of-the-art models\ndemonstrates that the proposed system achieves competitive results while\nremaining lightweight and deployable on low-cost devices. The framework\nprovides a promising step toward scalable, real-time, and accessible AI-based\npre-diagnostic healthcare solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u56fe\u50cf\u5206\u6790\u3001\u70ed\u6210\u50cf\u548c\u97f3\u9891\u4fe1\u53f7\u5904\u7406\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u76ae\u80a4\u764c\u3001\u8840\u7ba1\u8840\u6813\u548c\u5fc3\u80ba\u5f02\u5e38\u7684\u65e9\u671f\u68c0\u6d4b\u3002\u8be5\u7cfb\u7edf\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u5b9e\u65f6AI\u9884\u8bca\u65ad\u533b\u7597\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u8bca\u65ad\u6280\u672f\u901a\u5e38\u6210\u672c\u9ad8\u6602\u3001\u5177\u6709\u4fb5\u5165\u6027\u4e14\u5728\u4f4e\u8d44\u6e90\u5730\u533a\u96be\u4ee5\u83b7\u53d6\uff0c\u800c\u65e9\u671f\u8bca\u65ad\u5bf9\u4e8e\u63d0\u9ad8\u60a3\u8005\u751f\u5b58\u7387\u548c\u964d\u4f4e\u6cbb\u7597\u6210\u672c\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u533b\u7597\u53ef\u53ca\u6027\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u591a\u6a21\u6001AI\u8bca\u65ad\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001AI\u6846\u67b6\u6574\u5408\u4e09\u79cd\u8bca\u65ad\u6a21\u5f0f\uff1a\u4f7f\u7528\u5728ISIC 2019\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684MobileNetV2\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\uff1b\u91c7\u7528\u652f\u6301\u5411\u91cf\u673a\u7ed3\u5408\u624b\u5de5\u7279\u5f81\u8fdb\u884c\u70ed\u6210\u50cf\u8840\u6813\u68c0\u6d4b\uff1b\u5229\u7528Mel\u9891\u7387\u5012\u8c31\u7cfb\u6570\u7279\u5f81\u63d0\u53d6\u548c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u5904\u7406\u5fc3\u80ba\u58f0\u97f3\u6570\u636e\u3002", "result": "\u76ae\u80a4\u764c\u68c0\u6d4b\u8fbe\u523089.3%\u51c6\u786e\u7387\u300191.6%\u7075\u654f\u5ea6\u548c88.2%\u7279\u5f02\u6027\uff1b\u70ed\u6210\u50cf\u8840\u6813\u68c0\u6d4b\u5728\u5408\u6210\u548c\u4e34\u5e8a\u6570\u636e\u4e0a\u83b7\u5f9786.4%\u51c6\u786e\u7387\u548c0.89 AUC\uff1b\u5fc3\u80ba\u5f02\u5e38\u5206\u6790\u8fbe\u523087.2%\u51c6\u786e\u7387\u548c85.7%\u7075\u654f\u5ea6\u3002\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u7cfb\u7edf\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001AI\u6846\u67b6\u4e3a\u53ef\u6269\u5c55\u3001\u5b9e\u65f6\u4e14\u6613\u4e8e\u83b7\u53d6\u7684\u9884\u8bca\u65ad\u533b\u7597\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6280\u672f\u8def\u5f84\uff0c\u7279\u522b\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u8bca\u65ad\u6a21\u5f0f\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u6a21\u578b\u8f7b\u91cf\u5316\u7684\u540c\u65f6\u5b9e\u73b0\u51c6\u786e\u7684\u65e9\u671f\u75be\u75c5\u68c0\u6d4b\uff0c\u4e3a\u6539\u5584\u5168\u7403\u533b\u7597\u53ef\u53ca\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2510.25237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25237", "abs": "https://arxiv.org/abs/2510.25237", "authors": ["Yinqi Cai", "Jichang Li", "Zhaolun Li", "Weikai Chen", "Rushi Lan", "Xi Xie", "Xiaonan Luo", "Guanbin Li"], "title": "DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis", "comment": "ICCV 2025", "summary": "Recent advances in deep generative models have made it easier to manipulate\nface videos, raising significant concerns about their potential misuse for\nfraud and misinformation. Existing detectors often perform well in in-domain\nscenarios but fail to generalize across diverse manipulation techniques due to\ntheir reliance on forgery-specific artifacts. In this work, we introduce\nDeepShield, a novel deepfake detection framework that balances local\nsensitivity and global generalization to improve robustness across unseen\nforgeries. DeepShield enhances the CLIP-ViT encoder through two key components:\nLocal Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG\napplies spatiotemporal artifact modeling and patch-wise supervision to capture\nfine-grained inconsistencies often overlooked by global models. GFD introduces\ndomain feature augmentation, leveraging domain-bridging and boundary-expanding\nfeature generation to synthesize diverse forgeries, mitigating overfitting and\nenhancing cross-domain adaptability. Through the integration of novel local and\nglobal analysis for deepfake detection, DeepShield outperforms state-of-the-art\nmethods in cross-dataset and cross-manipulation evaluations, achieving superior\nrobustness against unseen deepfake attacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DeepShield\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u8865\u4e01\u5f15\u5bfc\u548c\u5168\u5c40\u4f2a\u9020\u591a\u6837\u5316\u6280\u672f\uff0c\u5728CLIP-ViT\u7f16\u7801\u5668\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u5e73\u8861\u5c40\u90e8\u654f\u611f\u6027\u548c\u5168\u5c40\u6cdb\u5316\u80fd\u529b\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u672a\u89c1\u4f2a\u9020\u6280\u672f\u4e0a\u7684\u68c0\u6d4b\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u5728\u57df\u5185\u573a\u666f\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e\u8fc7\u5ea6\u4f9d\u8d56\u7279\u5b9a\u4f2a\u9020\u4f2a\u5f71\u800c\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u5316\u7684\u64cd\u7eb5\u6280\u672f\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u672a\u77e5\u4f2a\u9020\u653b\u51fb\u65f6\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "method": "DeepShield\u6846\u67b6\u57fa\u4e8eCLIP-ViT\u7f16\u7801\u5668\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u5c40\u90e8\u8865\u4e01\u5f15\u5bfc\u901a\u8fc7\u65f6\u7a7a\u4f2a\u5f71\u5efa\u6a21\u548c\u9010\u8865\u4e01\u76d1\u7763\u6355\u83b7\u7ec6\u7c92\u5ea6\u4e0d\u4e00\u81f4\u6027\uff1b\u5168\u5c40\u4f2a\u9020\u591a\u6837\u5316\u901a\u8fc7\u9886\u57df\u7279\u5f81\u589e\u5f3a\u3001\u9886\u57df\u6865\u63a5\u548c\u8fb9\u754c\u6269\u5c55\u7279\u5f81\u751f\u6210\u5408\u6210\u591a\u6837\u5316\u4f2a\u9020\u6837\u672c\uff0c\u7f13\u89e3\u8fc7\u62df\u5408\u5e76\u63d0\u5347\u8de8\u57df\u9002\u5e94\u6027\u3002", "result": "\u5728\u8de8\u6570\u636e\u96c6\u548c\u8de8\u64cd\u7eb5\u6280\u672f\u7684\u8bc4\u4f30\u4e2d\uff0cDeepShield\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5bf9\u672a\u89c1\u6df1\u5ea6\u4f2a\u9020\u653b\u51fb\u7684\u5353\u8d8a\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u7ed3\u5408\u65b0\u9896\u7684\u5c40\u90e8\u548c\u5168\u5c40\u5206\u6790\u7b56\u7565\u80fd\u591f\u6709\u6548\u63d0\u5347\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u4f2a\u9020\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u601d\u8def\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u68c0\u6d4b\u6846\u67b6\u4e2d\u5e73\u8861\u5c40\u90e8\u654f\u611f\u6027\u548c\u5168\u5c40\u6cdb\u5316\u7684\u5173\u952e\u4ef7\u503c\u3002"}}
{"id": "2510.25238", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25238", "abs": "https://arxiv.org/abs/2510.25238", "authors": ["Qianqian Qiao", "DanDan Zheng", "Yihang Bo", "Bao Peng", "Heng Huang", "Longteng Jiang", "Huaye Wang", "Jingdong Chen", "Jun Zhou", "Xin Jin"], "title": "VADB: A Large-Scale Video Aesthetic Database with Professional and Multi-Dimensional Annotations", "comment": null, "summary": "Video aesthetic assessment, a vital area in multimedia computing, integrates\ncomputer vision with human cognition. Its progress is limited by the lack of\nstandardized datasets and robust models, as the temporal dynamics of video and\nmultimodal fusion challenges hinder direct application of image-based methods.\nThis study introduces VADB, the largest video aesthetic database with 10,490\ndiverse videos annotated by 37 professionals across multiple aesthetic\ndimensions, including overall and attribute-specific aesthetic scores, rich\nlanguage comments and objective tags. We propose VADB-Net, a dual-modal\npre-training framework with a two-stage training strategy, which outperforms\nexisting video quality assessment models in scoring tasks and supports\ndownstream video aesthetic assessment tasks. The dataset and source code are\navailable at https://github.com/BestiVictory/VADB.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86VADB\u2014\u2014\u6700\u5927\u7684\u89c6\u9891\u7f8e\u5b66\u8bc4\u4f30\u6570\u636e\u5e93\uff0c\u5305\u542b10,490\u4e2a\u591a\u6837\u5316\u89c6\u9891\uff0c\u5e76\u5f00\u53d1\u4e86VADB-Net\u53cc\u6a21\u6001\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7f8e\u5b66\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u7f8e\u5b66\u8bc4\u4f30\u4f5c\u4e3a\u591a\u5a92\u4f53\u8ba1\u7b97\u7684\u91cd\u8981\u9886\u57df\uff0c\u5176\u53d1\u5c55\u53d7\u5230\u6807\u51c6\u5316\u6570\u636e\u96c6\u7f3a\u4e4f\u548c\u9c81\u68d2\u6a21\u578b\u4e0d\u8db3\u7684\u9650\u5236\uff0c\u89c6\u9891\u7684\u65f6\u5e8f\u52a8\u6001\u7279\u6027\u548c\u591a\u6a21\u6001\u878d\u5408\u6311\u6218\u963b\u788d\u4e86\u57fa\u4e8e\u56fe\u50cf\u65b9\u6cd5\u7684\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86VADB-Net\u53cc\u6a21\u6001\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u89c6\u9891\u7f8e\u5b66\u8bc4\u4f30\u4e2d\u7684\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u95ee\u9898\u3002", "result": "VADB-Net\u5728\u8bc4\u5206\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u652f\u6301\u4e0b\u6e38\u89c6\u9891\u7f8e\u5b66\u8bc4\u4f30\u4efb\u52a1\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u6807\u51c6\u5316\u89c6\u9891\u7f8e\u5b66\u6570\u636e\u5e93\uff0c\u8fd8\u5f00\u53d1\u4e86\u6709\u6548\u7684\u53cc\u6a21\u6001\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u4e3a\u89c6\u9891\u7f8e\u5b66\u8bc4\u4f30\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8bbe\u65bd\u548c\u65b9\u6cd5\u8bba\u652f\u6301\u3002"}}
{"id": "2510.25263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25263", "abs": "https://arxiv.org/abs/2510.25263", "authors": ["Yang Miao", "Jan-Nico Zaech", "Xi Wang", "Fabien Despinoy", "Danda Pani Paudel", "Luc Van Gool"], "title": "LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation", "comment": "10 pages, 5 figures, 14 tables, Neurips 2025", "summary": "We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based\nframework for open-vocabulary object-part instance segmentation. Given an\nimage, LangHOPS can jointly detect and segment hierarchical object and part\ninstances from open-vocabulary candidate categories. Unlike prior approaches\nthat rely on heuristic or learnable visual grouping, our approach grounds\nobject-part hierarchies in language space. It integrates the MLLM into the\nobject-part parsing pipeline to leverage its rich knowledge and reasoning\ncapabilities, and link multi-granularity concepts within the hierarchies. We\nevaluate LangHOPS across multiple challenging scenarios, including in-domain\nand cross-dataset object-part instance segmentation, and zero-shot semantic\nsegmentation. LangHOPS achieves state-of-the-art results, surpassing previous\nmethods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on\nthe PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K\n(zero-shot). Ablation studies further validate the effectiveness of the\nlanguage-grounded hierarchy and MLLM driven part query refinement strategy. The\ncode will be released here.", "AI": {"tldr": "LangHOPS\u662f\u9996\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61-\u90e8\u4ef6\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u7a7a\u95f4\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\u5b9e\u73b0\u5bf9\u8c61\u548c\u90e8\u4ef6\u7684\u8054\u5408\u68c0\u6d4b\u4e0e\u5206\u5272\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u53ef\u5b66\u4e60\u7684\u89c6\u89c9\u5206\u7ec4\u7b56\u7565\uff0c\u96be\u4ee5\u6709\u6548\u5904\u7406\u5f00\u653e\u8bcd\u6c47\u7684\u5bf9\u8c61-\u90e8\u4ef6\u5c42\u6b21\u7ed3\u6784\u89e3\u6790\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u8bed\u8a00\u77e5\u8bc6\u6765\u5efa\u7acb\u591a\u7c92\u5ea6\u6982\u5ff5\u95f4\u8054\u7cfb\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5c06MLLM\u96c6\u6210\u5230\u5bf9\u8c61-\u90e8\u4ef6\u89e3\u6790\u6d41\u7a0b\u4e2d\uff0c\u5229\u7528\u5176\u4e30\u5bcc\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5728\u8bed\u8a00\u7a7a\u95f4\u4e2d\u5efa\u7acb\u5bf9\u8c61-\u90e8\u4ef6\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u91c7\u7528MLLM\u9a71\u52a8\u7684\u90e8\u4ef6\u67e5\u8be2\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728PartImageNet\u6570\u636e\u96c6\u4e0a\uff0cLangHOPS\u5728\u57df\u5185\u548c\u8de8\u6570\u636e\u96c6\u5bf9\u8c61-\u90e8\u4ef6\u5b9e\u4f8b\u5206\u5272\u4e2d\u5206\u522b\u4ee55.5%\u548c4.8%\u7684\u5e73\u5747\u7cbe\u5ea6\u4f18\u52bf\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\uff0c\u5728ADE20K\u7684\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u4e2d\u672a\u89c1\u5bf9\u8c61\u90e8\u4ef6\u4e0a\u83b7\u5f972.5% mIOU\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u57fa\u4e8e\u8bed\u8a00\u5c42\u6b21\u7ed3\u6784\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u5bf9\u8c61-\u90e8\u4ef6\u89e3\u6790\u4efb\u52a1\uff0cMLLM\u7684\u77e5\u8bc6\u63a8\u7406\u80fd\u529b\u5bf9\u591a\u7c92\u5ea6\u6982\u5ff5\u94fe\u63a5\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u5f00\u653e\u8bcd\u6c47\u7684\u5c42\u6b21\u5316\u89c6\u89c9\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.25318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25318", "abs": "https://arxiv.org/abs/2510.25318", "authors": ["Yushen Huang", "Zhiming Wang"], "title": "Prototype-Driven Adaptation for Few-Shot Object Detection", "comment": "7 pages,1 figure,2 tables,Preprint", "summary": "Few-shot object detection (FSOD) often suffers from base-class bias and\nunstable calibration when only a few novel samples are available. We propose\nPrototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN\nthat provides a prototype-based \"second opinion\" complementary to the linear\nclassifier. PDA maintains support-only prototypes in a learnable\nidentity-initialized projection space and optionally applies\nprototype-conditioned RoI alignment to reduce geometric mismatch. During\nfine-tuning, prototypes can be adapted via exponential moving average(EMA)\nupdates on labeled foreground RoIs-without introducing class-specific\nparameters-and are frozen at inference to ensure strict protocol compliance.\nPDA employs a best-of-K matching scheme to capture intra-class multi-modality\nand temperature-scaled fusion to combine metric similarities with detector\nlogits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently\nimproves novel-class performance with minimal impact on base classes and\nnegligible computational overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u539f\u578b\u9a71\u52a8\u5bf9\u9f50\uff08PDA\uff09\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63d2\u4ef6\u5f0f\u5ea6\u91cf\u5934\uff0c\u901a\u8fc7\u63d0\u4f9b\u4e0e\u7ebf\u6027\u5206\u7c7b\u5668\u4e92\u8865\u7684\u539f\u578b\u5316\u201c\u7b2c\u4e8c\u610f\u89c1\u201d\u6765\u89e3\u51b3\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u57fa\u7840\u7c7b\u504f\u5dee\u548c\u6821\u51c6\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728VOC FSOD\u548cGFSOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u65b0\u7c7b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u7840\u7c7b\u6027\u80fd\u5e76\u4ec5\u5f15\u5165\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\uff08FSOD\uff09\u5728\u4ec5\u6709\u5c11\u91cf\u65b0\u7c7b\u6837\u672c\u53ef\u7528\u65f6\uff0c\u5e38\u5e38\u9762\u4e34\u57fa\u7840\u7c7b\u504f\u5dee\u548c\u6821\u51c6\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u9650\u7684\u65b0\u7c7b\u6570\u636e\u4e0b\u96be\u4ee5\u5e73\u8861\u57fa\u7840\u7c7b\u548c\u65b0\u7c7b\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u4e14\u7ebf\u6027\u5206\u7c7b\u5668\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u5bb9\u6613\u4ea7\u751f\u504f\u5dee\u9884\u6d4b\u3002", "method": "PDA\u5728DeFRCN\u6846\u67b6\u4e2d\u5f15\u5165\u652f\u6301\u96c6\u539f\u578b\u7ef4\u62a4\u673a\u5236\uff0c\u5728\u53ef\u5b66\u4e60\u7684\u8eab\u4efd\u521d\u59cb\u5316\u6295\u5f71\u7a7a\u95f4\u4e2d\u6784\u5efa\u539f\u578b\u8868\u793a\uff0c\u5e76\u53ef\u9009\u5730\u5e94\u7528\u539f\u578b\u6761\u4ef6RoI\u5bf9\u9f50\u4ee5\u51cf\u5c11\u51e0\u4f55\u4e0d\u5339\u914d\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08EMA\uff09\u66f4\u65b0\u6807\u8bb0\u524d\u666fRoI\u6765\u9002\u5e94\u539f\u578b\uff0c\u65e0\u9700\u5f15\u5165\u7c7b\u7279\u5b9a\u53c2\u6570\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u51bb\u7ed3\u539f\u578b\u4ee5\u786e\u4fdd\u534f\u8bae\u5408\u89c4\u6027\u3002PDA\u4f7f\u7528\u6700\u4f73K\u5339\u914d\u65b9\u6848\u6355\u6349\u7c7b\u5185\u591a\u6a21\u6001\u6027\uff0c\u5e76\u901a\u8fc7\u6e29\u5ea6\u7f29\u653e\u878d\u5408\u5c06\u5ea6\u91cf\u76f8\u4f3c\u5ea6\u4e0e\u68c0\u6d4b\u5668\u903b\u8f91\u503c\u7ed3\u5408\u3002", "result": "\u5728VOC FSOD\u548cGFSOD\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPDA\u80fd\u591f\u6301\u7eed\u63d0\u5347\u65b0\u7c7b\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u5bf9\u57fa\u7840\u7c7b\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002\u8be5\u65b9\u6cd5\u4ee5\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u539f\u578b\u9a71\u52a8\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "PDA\u8bc1\u660e\u4e86\u539f\u578b\u5316\u5ea6\u91cf\u5b66\u4e60\u4f5c\u4e3a\u7ebf\u6027\u5206\u7c7b\u5668\u8865\u5145\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7a33\u5b9a\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u4fdd\u6301\u57fa\u7840\u7c7b\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u65b0\u7c7b\u68c0\u6d4b\u80fd\u529b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u5c11\u6837\u672c\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u548c\u8bbe\u8ba1\u601d\u8def\u3002"}}
{"id": "2510.25332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25332", "abs": "https://arxiv.org/abs/2510.25332", "authors": ["Yuhang Hu", "Zhenyu Yang", "Shihan Wang", "Shengsheng Qian", "Bin Wen", "Fan Yang", "Tingting Gao", "Changsheng Xu"], "title": "StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA", "comment": null, "summary": "The rapid growth of streaming video applications demands multimodal models\nwith enhanced capabilities for temporal dynamics understanding and complex\nreasoning. However, current Video Question Answering (VideoQA) datasets suffer\nfrom two critical limitations: 1) Static annotation mechanisms fail to capture\nthe evolving nature of answers in temporal video streams, and 2) The absence of\nexplicit reasoning process annotations restricts model interpretability and\nlogical deduction capabilities. To address these challenges, We introduce\nStreamingCoT, the first dataset explicitly designed for temporally evolving\nreasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our\nframework first establishes a dynamic hierarchical annotation architecture that\ngenerates per-second dense descriptions and constructs temporally-dependent\nsemantic segments through similarity fusion, paired with question-answer sets\nconstrained by temporal evolution patterns. We further propose an explicit\nreasoning chain generation paradigm that extracts spatiotemporal objects via\nkeyframe semantic alignment, derives object state transition-based reasoning\npaths using large language models, and ensures logical coherence through\nhuman-verified validation. This dataset establishes a foundation for advancing\nresearch in streaming video understanding, complex temporal reasoning, and\nmultimodal inference. Our StreamingCoT and its construction toolkit can be\naccessed at https://github.com/Fleeting-hyh/StreamingCoT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86StreamingCoT\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u548c\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u4efb\u52a1\u8bbe\u8ba1\u7684\u5177\u6709\u65f6\u95f4\u6f14\u5316\u63a8\u7406\u80fd\u529b\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u52a8\u6001\u5c42\u6b21\u6807\u6ce8\u67b6\u6784\u548c\u663e\u5f0f\u63a8\u7406\u94fe\u751f\u6210\u8303\u5f0f\u89e3\u51b3\u4e86\u73b0\u6709VideoQA\u6570\u636e\u96c6\u5728\u65f6\u95f4\u52a8\u6001\u6027\u548c\u63a8\u7406\u900f\u660e\u5ea6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u95ee\u7b54\u6570\u636e\u96c6\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\u6027\uff1a\u9759\u6001\u6807\u6ce8\u673a\u5236\u65e0\u6cd5\u6355\u6349\u65f6\u95f4\u89c6\u9891\u6d41\u4e2d\u7b54\u6848\u7684\u6f14\u5316\u7279\u6027\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\u8fc7\u7a0b\u6807\u6ce8\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u963b\u788d\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u6d41\u5f0f\u89c6\u9891\u5e94\u7528\u4e2d\u5bf9\u65f6\u95f4\u52a8\u6001\u7406\u89e3\u548c\u590d\u6742\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u52a8\u6001\u5c42\u6b21\u6807\u6ce8\u67b6\u6784\uff0c\u751f\u6210\u6bcf\u79d2\u5bc6\u96c6\u63cf\u8ff0\u5e76\u901a\u8fc7\u76f8\u4f3c\u6027\u878d\u5408\u6784\u5efa\u65f6\u95f4\u4f9d\u8d56\u7684\u8bed\u4e49\u7247\u6bb5\uff0c\u540c\u65f6\u8bbe\u8ba1\u65f6\u95f4\u6f14\u5316\u6a21\u5f0f\u7ea6\u675f\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\uff1b\u8fdb\u4e00\u6b65\u63d0\u51fa\u663e\u5f0f\u63a8\u7406\u94fe\u751f\u6210\u8303\u5f0f\uff0c\u901a\u8fc7\u5173\u952e\u5e27\u8bed\u4e49\u5bf9\u9f50\u63d0\u53d6\u65f6\u7a7a\u5bf9\u8c61\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u57fa\u4e8e\u5bf9\u8c61\u72b6\u6001\u8f6c\u6362\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "StreamingCoT\u6570\u636e\u96c6\u5efa\u7acb\u4e86\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u3001\u590d\u6742\u65f6\u95f4\u63a8\u7406\u548c\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u7684\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u9996\u4e2a\u5177\u6709\u65f6\u95f4\u6f14\u5316\u63a8\u7406\u80fd\u529b\u7684VideoQA\u6570\u636e\u96c6\uff0c\u5305\u542b\u52a8\u6001\u5c42\u6b21\u6807\u6ce8\u548c\u663e\u5f0f\u63a8\u7406\u94fe\u6807\u6ce8\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u57fa\u51c6\u548c\u5de5\u5177\u652f\u6301\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u3001\u590d\u6742\u65f6\u95f4\u63a8\u7406\u548c\u591a\u6a21\u6001\u63a8\u7406\u9886\u57df\u5efa\u7acb\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u6784\u5efa\u5de5\u5177\u5c06\u63a8\u52a8\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u5728\u65f6\u95f4\u52a8\u6001\u7406\u89e3\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u8fdb\u6b65\uff0c\u5e76\u4e3a\u53ef\u89e3\u91ca\u6027\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.25387", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25387", "abs": "https://arxiv.org/abs/2510.25387", "authors": ["Bill Psomas", "George Retsinas", "Nikos Efthymiadis", "Panagiotis Filntisis", "Yannis Avrithis", "Petros Maragos", "Ondrej Chum", "Giorgos Tolias"], "title": "Instance-Level Composed Image Retrieval", "comment": "NeurIPS 2025", "summary": "The progress of composed image retrieval (CIR), a popular research direction\nin image retrieval, where a combined visual and textual query is used, is held\nback by the absence of high-quality training and evaluation data. We introduce\na new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an\ninstance-level class definition. The goal is to retrieve images that contain\nthe same particular object as the visual query, presented under a variety of\nmodifications defined by textual queries. Its design and curation process keep\nthe dataset compact to facilitate future research, while maintaining its\nchallenge-comparable to retrieval among more than 40M random\ndistractors-through a semi-automated selection of hard negatives.\n  To overcome the challenge of obtaining clean, diverse, and suitable training\ndata, we leverage pre-trained vision-and-language models (VLMs) in a\ntraining-free approach called BASIC. The method separately estimates\nquery-image-to-image and query-text-to-image similarities, performing late\nfusion to upweight images that satisfy both queries, while down-weighting those\nthat exhibit high similarity with only one of the two. Each individual\nsimilarity is further improved by a set of components that are simple and\nintuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR\ndatasets that follow a semantic-level class definition. Project page:\nhttps://vrg.fel.cvut.cz/icir/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86i-CIR\u8bc4\u4f30\u6570\u636e\u96c6\u548cBASIC\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u9886\u57df\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002BASIC\u65b9\u6cd5\u901a\u8fc7\u5206\u522b\u4f30\u8ba1\u89c6\u89c9\u548c\u6587\u672c\u67e5\u8be2\u4e0e\u56fe\u50cf\u7684\u76f8\u4f3c\u5ea6\u5e76\u8fdb\u884c\u540e\u671f\u878d\u5408\uff0c\u5728\u591a\u4e2aCIR\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u7814\u7a76\u7684\u8fdb\u5c55\u53d7\u5230\u9ad8\u8d28\u91cf\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u7f3a\u4e4f\u7684\u9650\u5236\uff0c\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u7ea7\u522b\u7684\u7c7b\u522b\u5b9a\u4e49\uff0c\u800c\u7f3a\u4e4f\u9488\u5bf9\u7279\u5b9a\u5b9e\u4f8b\u7ea7\u522b\u5bf9\u8c61\u7684\u68c0\u7d22\u8bc4\u4f30\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86BASIC\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u5206\u522b\u8ba1\u7b97\u67e5\u8be2\u56fe\u50cf\u5230\u56fe\u50cf\u548c\u67e5\u8be2\u6587\u672c\u5230\u56fe\u50cf\u7684\u76f8\u4f3c\u5ea6\uff0c\u901a\u8fc7\u540e\u671f\u878d\u5408\u5bf9\u540c\u65f6\u6ee1\u8db3\u4e24\u4e2a\u67e5\u8be2\u7684\u56fe\u50cf\u8fdb\u884c\u52a0\u6743\uff0c\u5e76\u5f15\u5165\u7b80\u5355\u76f4\u89c2\u7684\u7ec4\u4ef6\u6765\u6539\u8fdb\u5404\u4e2a\u76f8\u4f3c\u5ea6\u4f30\u8ba1\u3002", "result": "BASIC\u65b9\u6cd5\u5728\u63d0\u51fa\u7684i-CIR\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5728\u9075\u5faa\u8bed\u4e49\u7ea7\u522b\u7c7b\u522b\u5b9a\u4e49\u7684\u73b0\u6709CIR\u6570\u636e\u96c6\u4e0a\u4e5f\u8fbe\u5230\u4e86\u6700\u4f73\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u95ee\u9898\uff0ci-CIR\u6570\u636e\u96c6\u4e3a\u5b9e\u4f8b\u7ea7\u522b\u68c0\u7d22\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.25463", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25463", "abs": "https://arxiv.org/abs/2510.25463", "authors": ["Hongjie Zhang", "Gideon Billings", "Stefan B. Williams"], "title": "SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time, Monocular Depth Estimation in Underwater Environments", "comment": null, "summary": "Underwater infrastructure requires frequent inspection and maintenance due to\nharsh marine conditions. Current reliance on human divers or remotely operated\nvehicles is limited by perceptual and operational challenges, especially around\ncomplex structures or in turbid water. Enhancing the spatial awareness of\nunderwater vehicles is key to reducing piloting risks and enabling greater\nautonomy. To address these challenges, we present SPADE: SParsity Adaptive\nDepth Estimator, a monocular depth estimation pipeline that combines\npre-trained relative depth estimator with sparse depth priors to produce dense,\nmetric scale depth maps. Our two-stage approach first scales the relative depth\nmap with the sparse depth points, then refines the final metric prediction with\nour proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves\nimproved accuracy and generalisation over state-of-the-art baselines and runs\nefficiently at over 15 FPS on embedded hardware, promising to support practical\nunderwater inspection and intervention. This work has been submitted to IEEE\nJournal of Oceanic Engineering Special Issue of AUV 2026.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSPADE\uff1a\u7a00\u758f\u81ea\u9002\u5e94\u6df1\u5ea6\u4f30\u8ba1\u5668\uff0c\u4e00\u79cd\u7ed3\u5408\u9884\u8bad\u7ec3\u76f8\u5bf9\u6df1\u5ea6\u4f30\u8ba1\u5668\u4e0e\u7a00\u758f\u6df1\u5ea6\u5148\u9a8c\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7ba1\u9053\uff0c\u80fd\u591f\u751f\u6210\u5bc6\u96c6\u7684\u5ea6\u91cf\u5c3a\u5ea6\u6df1\u5ea6\u56fe\uff0c\u663e\u8457\u63d0\u5347\u6c34\u4e0b\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\u7684\u81ea\u4e3b\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u6c34\u4e0b\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\u4f9d\u8d56\u4eba\u7c7b\u6f5c\u6c34\u5458\u6216\u9065\u63a7\u64cd\u4f5c\u8f66\u8f86\uff0c\u9762\u4e34\u590d\u6742\u7ed3\u6784\u548c\u6d51\u6d4a\u6c34\u57df\u4e2d\u7684\u611f\u77e5\u4e0e\u64cd\u4f5c\u6311\u6218\uff0c\u9700\u8981\u589e\u5f3a\u6c34\u4e0b\u8f66\u8f86\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u4ee5\u964d\u4f4e\u64cd\u63a7\u98ce\u9669\u5e76\u63d0\u9ad8\u81ea\u4e3b\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528\u7a00\u758f\u6df1\u5ea6\u70b9\u5bf9\u76f8\u5bf9\u6df1\u5ea6\u56fe\u8fdb\u884c\u5c3a\u5ea6\u7f29\u653e\uff0c\u7136\u540e\u901a\u8fc7\u63d0\u51fa\u7684\u7ea7\u8054\u5377\u79ef-\u53ef\u53d8\u5f62Transformer\u5757\u5bf9\u6700\u7ec8\u5ea6\u91cf\u9884\u6d4b\u8fdb\u884c\u7cbe\u7ec6\u5316\u5904\u7406\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u8fd0\u884c\u6548\u7387\u8d85\u8fc715 FPS\uff0c\u4e3a\u5b9e\u9645\u6c34\u4e0b\u68c0\u67e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SPADE\u6846\u67b6\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u76f8\u5bf9\u6df1\u5ea6\u4f30\u8ba1\u4e0e\u7a00\u758f\u6df1\u5ea6\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4e3a\u6c34\u4e0b\u81ea\u4e3b\u68c0\u67e5\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.25739", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25739", "abs": "https://arxiv.org/abs/2510.25739", "authors": ["Zhi-Kai Chen", "Jun-Peng Jiang", "Han-Jia Ye", "De-Chuan Zhan"], "title": "Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image Generation", "comment": null, "summary": "Autoregressive (AR) image generation models are capable of producing\nhigh-fidelity images but often suffer from slow inference due to their\ninherently sequential, token-by-token decoding process. Speculative decoding,\nwhich employs a lightweight draft model to approximate the output of a larger\nAR model, has shown promise in accelerating text generation without\ncompromising quality. However, its application to image generation remains\nlargely underexplored. The challenges stem from a significantly larger sampling\nspace, which complicates the alignment between the draft and target model\noutputs, coupled with the inadequate use of the two-dimensional spatial\nstructure inherent in images, thereby limiting the modeling of local\ndependencies. To overcome these challenges, we introduce Hawk, a new approach\nthat harnesses the spatial structure of images to guide the speculative model\ntoward more accurate and efficient predictions. Experimental results on\nmultiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR\nmodels, while preserving both image fidelity and diversity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Hawk\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u50cf\u7684\u7a7a\u95f4\u7ed3\u6784\u6765\u5f15\u5bfc\u63a8\u6d4b\u6a21\u578b\u8fdb\u884c\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u7684\u9884\u6d4b\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u6807\u51c6\u81ea\u56de\u5f52\u6a21\u578b\u5b9e\u73b0\u4e861.71\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u591f\u4ea7\u751f\u9ad8\u4fdd\u771f\u56fe\u50cf\uff0c\u4f46\u7531\u4e8e\u5176\u56fa\u6709\u7684\u987a\u5e8f\u3001\u9010\u4e2a\u4ee4\u724c\u7684\u89e3\u7801\u8fc7\u7a0b\uff0c\u901a\u5e38\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002\u63a8\u6d4b\u89e3\u7801\u5728\u6587\u672c\u751f\u6210\u4e2d\u5df2\u663e\u793a\u51fa\u52a0\u901f\u6f5c\u529b\uff0c\u4f46\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u4e3b\u8981\u6311\u6218\u5305\u62ec\u66f4\u5927\u7684\u91c7\u6837\u7a7a\u95f4\u5bfc\u81f4\u8349\u7a3f\u6a21\u578b\u4e0e\u76ee\u6807\u6a21\u578b\u8f93\u51fa\u5bf9\u9f50\u56f0\u96be\uff0c\u4ee5\u53ca\u672a\u80fd\u5145\u5206\u5229\u7528\u56fe\u50cf\u7684\u4e8c\u7ef4\u7a7a\u95f4\u7ed3\u6784\u6765\u5efa\u6a21\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Hawk\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u56fe\u50cf\u7684\u7a7a\u95f4\u7ed3\u6784\u6765\u5f15\u5bfc\u63a8\u6d4b\u6a21\u578b\u8fdb\u884c\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u7684\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u66f4\u597d\u5730\u5efa\u6a21\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5305\u62ec\u91c7\u6837\u7a7a\u95f4\u5927\u548c\u7a7a\u95f4\u7ed3\u6784\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u6587\u672c\u5230\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHawk\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6\u81ea\u56de\u5f52\u6a21\u578b\u5b9e\u73b0\u4e861.71\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u52a0\u901f\u63a8\u7406\u7684\u540c\u65f6\u6ca1\u6709\u727a\u7272\u751f\u6210\u8d28\u91cf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Hawk\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u6269\u5c55\u5230\u56fe\u50cf\u751f\u6210\u9886\u57df\uff0c\u901a\u8fc7\u5229\u7528\u7a7a\u95f4\u7ed3\u6784\u6307\u5bfc\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002\u8fd9\u9879\u7814\u7a76\u4e3a\u52a0\u901f\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u8868\u660e\u7a7a\u95f4\u611f\u77e5\u7684\u63a8\u6d4b\u89e3\u7801\u662f\u89e3\u51b3\u56fe\u50cf\u751f\u6210\u63a8\u7406\u6548\u7387\u95ee\u9898\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.25760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25760", "abs": "https://arxiv.org/abs/2510.25760", "authors": ["Xu Zheng", "Zihao Dongfang", "Lutao Jiang", "Boyuan Zheng", "Yulong Guo", "Zhenquan Zhang", "Giuliano Albanese", "Runyi Yang", "Mengjiao Ma", "Zixin Zhang", "Chenfei Liao", "Dingcheng Zhen", "Yuanhuiyi Lyu", "Yuqian Fu", "Bin Ren", "Linfeng Zhang", "Danda Pani Paudel", "Nicu Sebe", "Luc Van Gool", "Xuming Hu"], "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks", "comment": null, "summary": "Humans possess spatial reasoning abilities that enable them to understand\nspaces through multimodal observations, such as vision and sound. Large\nmultimodal reasoning models extend these abilities by learning to perceive and\nreason, showing promising performance across diverse spatial tasks. However,\nsystematic reviews and publicly available benchmarks for these models remain\nlimited. In this survey, we provide a comprehensive review of multimodal\nspatial reasoning tasks with large models, categorizing recent progress in\nmultimodal large language models (MLLMs) and introducing open benchmarks for\nevaluation. We begin by outlining general spatial reasoning, focusing on\npost-training techniques, explainability, and architecture. Beyond classical 2D\ntasks, we examine spatial relationship reasoning, scene and layout\nunderstanding, as well as visual question answering and grounding in 3D space.\nWe also review advances in embodied AI, including vision-language navigation\nand action models. Additionally, we consider emerging modalities such as audio\nand egocentric video, which contribute to novel spatial understanding through\nnew sensors. We believe this survey establishes a solid foundation and offers\ninsights into the growing field of multimodal spatial reasoning. Updated\ninformation about this survey, codes and implementation of the open benchmarks\ncan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5bf9\u591a\u6a21\u6001\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u56de\u987e\uff0c\u6db5\u76d6\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u5f15\u5165\u4e86\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u7528\u4e8e\u8bc4\u4f30\uff0c\u4e3a\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u5efa\u7acb\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u7c7b\u5177\u5907\u901a\u8fc7\u89c6\u89c9\u548c\u58f0\u97f3\u7b49\u591a\u6a21\u6001\u89c2\u5bdf\u7406\u89e3\u7a7a\u95f4\u7684\u80fd\u529b\uff0c\u4e14\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u9488\u5bf9\u8fd9\u4e9b\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\u548c\u516c\u5f00\u53ef\u7528\u57fa\u51c6\u6d4b\u8bd5\u4ecd\u7136\u6709\u9650\uff0c\u9700\u8981\u5efa\u7acb\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u672c\u7efc\u8ff0\u7cfb\u7edf\u6027\u5730\u5206\u7c7b\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u540e\u8bad\u7ec3\u6280\u672f\u3001\u53ef\u89e3\u91ca\u6027\u548c\u67b6\u6784\u8bbe\u8ba1\uff0c\u6db5\u76d6\u4e86\u4ece\u7ecf\u51782D\u4efb\u52a1\u52303D\u7a7a\u95f4\u4e2d\u7684\u89c6\u89c9\u95ee\u7b54\u4e0e\u5b9a\u4f4d\uff0c\u4ee5\u53ca\u5177\u8eabAI\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u548c\u52a8\u4f5c\u6a21\u578b\u3002", "result": "\u7814\u7a76\u5efa\u7acb\u4e86\u591a\u6a21\u6001\u7a7a\u95f4\u63a8\u7406\u7684\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e86\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\u3001\u573a\u666f\u4e0e\u5e03\u5c40\u7406\u89e3\u30013D\u7a7a\u95f4\u4e2d\u7684\u89c6\u89c9\u95ee\u7b54\u4e0e\u5b9a\u4f4d\u7b49\u4efb\u52a1\uff0c\u5e76\u8003\u8651\u4e86\u97f3\u9891\u548c\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7b49\u65b0\u5174\u6a21\u6001\u5bf9\u7a7a\u95f4\u7406\u89e3\u7684\u65b0\u8d21\u732e\u3002", "conclusion": "\u672c\u7efc\u8ff0\u4e3a\u591a\u6a21\u6001\u7a7a\u95f4\u63a8\u7406\u9886\u57df\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u5bf9\u8be5\u9886\u57df\u53d1\u5c55\u7684\u6df1\u523b\u89c1\u89e3\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u5f15\u5165\uff0c\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7814\u7a76\u7684\u6807\u51c6\u5316\u548c\u53ef\u6bd4\u6027\u3002"}}
