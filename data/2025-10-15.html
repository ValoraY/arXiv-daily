<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-15.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 35]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 5]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 6]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-data-or-language-supervision-what-makes-clip-better-than-dino">[1] <a href="https://arxiv.org/abs/2510.11835">Data or Language Supervision: What Makes CLIP Better than DINO?</a></h3>
<p><em>Yiming Liu, Yuhui Zhang, Dhruba Ghosh, Ludwig Schmidt, Serena Yeung-Levy</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é€šè¿‡å—æ§å®éªŒæ­ç¤ºäº†CLIPåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ä¼˜äºè‡ªç›‘ç£æ¨¡å‹DINOçš„åŸå› ï¼šCLIPé€šè¿‡è¯­è¨€ç›‘ç£æ•è·é«˜çº§è¯­ä¹‰ä¿¡æ¯ï¼Œè€ŒDINOæ›´å…³æ³¨ä½çº§è§†è§‰ç‰¹å¾ã€‚CLIPåœ¨æ–‡æœ¬å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè€ŒDINOåœ¨è§†è§‰ä¸­å¿ƒä»»åŠ¡ä¸­ç•¥æœ‰ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> CLIPåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ä½œä¸ºè§†è§‰ç¼–ç å™¨ä¼˜äºè‡ªç›‘ç£æ¨¡å‹å¦‚DINOï¼Œä½†å°šä¸æ¸…æ¥šè¿™ç§ä¼˜åŠ¿æ˜¯æºäºCLIPçš„è¯­è¨€ç›‘ç£è¿˜æ˜¯å…¶æ›´å¤§çš„è®­ç»ƒæ•°æ®é‡ã€‚æœ¬ç ”ç©¶æ—¨åœ¨åˆ†ç¦»è¿™ä¸¤ä¸ªå› ç´ ï¼Œæ¢ç©¶è¯­è¨€ç›‘ç£å¯¹è§†è§‰ç¼–ç å™¨è¡¨å¾èƒ½åŠ›çš„å…·ä½“å½±å“ã€‚</p>
<p><strong>Method:</strong> åœ¨å—æ§è®¾ç½®ä¸‹é¢„è®­ç»ƒCLIPå’ŒDINOæ¨¡å‹ï¼Œä½¿ç”¨ç›¸åŒçš„æ¶æ„ã€æ•°æ®é›†å’Œè®­ç»ƒé…ç½®ï¼Œç¡®ä¿ä¸¤è€…åœ¨ImageNetä¸Šè¾¾åˆ°ç›¸ä¼¼çš„å‡†ç¡®ç‡ã€‚é€šè¿‡åµŒå…¥åˆ†ææ¯”è¾ƒä¸¤ç§æ¨¡å‹æ•è·çš„ç‰¹å¾ç±»å‹ï¼Œå¹¶åœ¨20ä¸ªVQAåŸºå‡†ä¸Šè¯„ä¼°å®ƒä»¬é›†æˆåˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> åµŒå…¥åˆ†ææ˜¾ç¤ºCLIPæ•è·é«˜çº§è¯­ä¹‰ä¿¡æ¯ï¼ˆå¦‚ç‰©ä½“ç±»åˆ«ã€æ–‡æœ¬ï¼‰ï¼Œè€ŒDINOå¯¹ä½çº§ç‰¹å¾ï¼ˆå¦‚é¢œè‰²ã€é£æ ¼ï¼‰æ›´æ•æ„Ÿã€‚åœ¨VQAè¯„ä¼°ä¸­ï¼ŒCLIPåœ¨æ–‡æœ¬å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒDINOåœ¨è§†è§‰ä¸­å¿ƒä»»åŠ¡ä¸­ç•¥æœ‰ä¼˜åŠ¿ã€‚è¯­è¨€ç›‘ç£å˜ä½“ï¼ˆå¦‚sigmoidæŸå¤±ã€é¢„è®­ç»ƒè¯­è¨€ç¼–ç å™¨ï¼‰å¸¦æ¥çš„æ”¹è¿›æœ‰é™ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ä¸ºè§†è§‰ç¼–ç å™¨è®¾è®¡æä¾›äº†ç§‘å­¦è§è§£ï¼šè¯­è¨€ç›‘ç£ä¿ƒä½¿æ¨¡å‹å­¦ä¹ æ›´å…·è¯­ä¹‰æ„ä¹‰çš„è¡¨å¾ï¼Œè¿™å¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å…·æœ‰é‡è¦å½±å“ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºæŒ‡å¯¼æœªæ¥è§†è§‰ç¼–ç å™¨çš„ä¼˜åŒ–å’Œé€‰æ‹©ç­–ç•¥ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>CLIP outperforms self-supervised models like DINO as vision encoders for
vision-language models (VLMs), but it remains unclear whether this advantage
stems from CLIP's language supervision or its much larger training data. To
disentangle these factors, we pre-train CLIP and DINO under controlled settings
-- using the same architecture, dataset, and training configuration --
achieving similar ImageNet accuracy. Embedding analysis shows that CLIP
captures high-level semantics (e.g., object categories, text), while DINO is
more responsive to low-level features like colors and styles. When integrated
into VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive
tasks, while DINO slightly outperforms on vision-centric ones. Variants of
language supervision (e.g., sigmoid loss, pre-trained language encoders) yield
limited gains. Our findings provide scientific insights into vision encoder
design and its impact on VLM performance.</p>
<h3 id="2-mammodino-anatomically-aware-self-supervision-for-mammographic-images">[2] <a href="https://arxiv.org/abs/2510.11883">MammoDINO: Anatomically Aware Self-Supervision for Mammographic Images</a></h3>
<p><em>Sicheng Zhou, Lei Wu, Cao Xiao, Parminder Bhatia, Taha Kass-Hout</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MammoDINOï¼Œä¸€ç§ä¸“é—¨é’ˆå¯¹ä¹³è…ºæ‘„å½±çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åœ¨140ä¸‡å¼ ä¹³è…ºå›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç»“åˆä¹³è…ºç»„ç»‡æ„ŸçŸ¥æ•°æ®å¢å¼ºå’Œè·¨åˆ‡ç‰‡å¯¹æ¯”å­¦ä¹ ï¼Œåœ¨å¤šä¸ªä¹³è…ºç™Œç­›æŸ¥ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªç›‘ç£å­¦ä¹ åœ¨é€šç”¨è§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨åŒ»å­¦å½±åƒä¸­åº”ç”¨æœ‰é™ï¼Œä¸»è¦å—é™äºæ•°æ®ç¨€ç¼ºå’Œé¢†åŸŸç‰¹å¼‚æ€§åå·®ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¹³è…ºæ‘„å½±é¢†åŸŸéœ€è¦èƒ½å¤Ÿæ•æ‰ä¸´åºŠç›¸å…³ç‰¹å¾çš„é¢„è®­ç»ƒæ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¹³è…ºç»„ç»‡æ„ŸçŸ¥æ•°æ®å¢å¼ºé‡‡æ ·å™¨ï¼ŒåŒæ—¶æ”¯æŒå›¾åƒçº§å’Œå—çº§ç›‘ç£ï¼Œå¹¶è®¾è®¡äº†è·¨åˆ‡ç‰‡å¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼Œå°†3Dæ•°å­—ä¹³è…ºæ–­å±‚åˆæˆç»“æ„ä¿¡æ¯èå…¥2Dé¢„è®­ç»ƒè¿‡ç¨‹ã€‚</p>
<p><strong>Result:</strong> MammoDINOåœ¨å¤šä¸ªä¹³è…ºç™Œç­›æŸ¥ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºä¹³è…ºæ‘„å½±è®¡ç®—æœºè¾…åŠ©è¯Šæ–­æä¾›äº†å¯æ‰©å±•çš„æ— æ ‡æ³¨åŸºç¡€ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºä¹³è…ºæ‘„å½±æä¾›äº†å¯æ‰©å±•çš„æ— æ ‡æ³¨é¢„è®­ç»ƒåŸºç¡€ï¼Œæœ‰åŠ©äºå‡å°‘æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œè´Ÿæ‹…å¹¶æé«˜ä¹³è…ºç™Œç­›æŸ¥çš„è¯Šæ–­æ•ˆç‡ï¼Œä¸ºå¤šç”¨é€”è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å·¥å…·çš„å¼€å‘å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Self-supervised learning (SSL) has transformed vision encoder training in
general domains but remains underutilized in medical imaging due to limited
data and domain specific biases. We present MammoDINO, a novel SSL framework
for mammography, pretrained on 1.4 million mammographic images. To capture
clinically meaningful features, we introduce a breast tissue aware data
augmentation sampler for both image-level and patch-level supervision and a
cross-slice contrastive learning objective that leverages 3D digital breast
tomosynthesis (DBT) structure into 2D pretraining. MammoDINO achieves
state-of-the-art performance on multiple breast cancer screening tasks and
generalizes well across five benchmark datasets. It offers a scalable,
annotation-free foundation for multipurpose computer-aided diagnosis (CAD)
tools for mammogram, helping reduce radiologists' workload and improve
diagnostic efficiency in breast cancer screening.</p>
<h3 id="3-task-specific-dual-model-framework-for-comprehensive-traffic-safety-video-description-and-analysis">[3] <a href="https://arxiv.org/abs/2510.11907">Task-Specific Dual-Model Framework for Comprehensive Traffic Safety Video Description and Analysis</a></h3>
<p><em>Blessing Agyei Kyem, Neema Jakisa Owor, Andrews Danyo, Joshua Kofi Asamoah, Eugene Denteh, Tanner Muturi, Anthony Dontoh, Yaw Adu-Gyamfi, Armstrong Aboah</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç‹¬ç‰¹çš„åŒæ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šä¼˜åŒ–ç­–ç•¥æ€§åœ°åˆ©ç”¨VideoLLaMAå’ŒQwen2.5-VLçš„äº’è¡¥ä¼˜åŠ¿ï¼Œç”¨äºäº¤é€šè§†é¢‘å®‰å…¨åˆ†æã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†ç¦»è®­ç»ƒæœ€å°åŒ–ä»»åŠ¡å¹²æ‰°ï¼Œåœ¨WTSæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> äº¤é€šè§†é¢‘å®‰å…¨åˆ†æéœ€è¦å¤æ‚çš„è§†é¢‘ç†è§£èƒ½åŠ›æ¥æ•æ‰ç»†ç²’åº¦çš„è¡Œä¸ºæ¨¡å¼å¹¶ç”Ÿæˆå…¨é¢çš„æè¿°ä»¥é¢„é˜²äº‹æ•…ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™ç§å¤šä»»åŠ¡éœ€æ±‚æ—¶å­˜åœ¨ä»»åŠ¡å¹²æ‰°å’Œæ€§èƒ½é™åˆ¶çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨åŒæ¨¡å‹æ¡†æ¶ç­–ç•¥æ€§åœ°ç»“åˆVideoLLaMAå’ŒQwen2.5-VLçš„äº’è¡¥ä¼˜åŠ¿ï¼Œé€šè¿‡åˆ†ç¦»è®­ç»ƒç­–ç•¥åˆ†åˆ«ä¼˜åŒ–æè¿°ç”Ÿæˆå’Œè§†è§‰é—®ç­”ä»»åŠ¡ï¼ŒVideoLLaMAä¸“é—¨è´Ÿè´£æ—¶åºæ¨ç†ï¼ŒQwen2.5-VLä¸“æ³¨äºè§†è§‰ç†è§£ã€‚</p>
<p><strong>Result:</strong> åœ¨WTSæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVideoLLaMAåœ¨æ—¶åºæ¨ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ŒCIDErå¾—åˆ†è¾¾åˆ°1.1001ï¼ŒQwen2.5-VLåœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°çªå‡ºï¼ŒVQAå‡†ç¡®ç‡è¾¾åˆ°60.80%ï¼Œåœ¨2025 AI City Challenge Track 2ä¸­S2å¾—åˆ†è¾¾åˆ°45.7572ï¼Œæ’åç¬¬10ä½ã€‚</p>
<p><strong>Conclusion:</strong> åˆ†ç¦»è®­ç»ƒç­–ç•¥ç›¸æ¯”è”åˆè®­ç»ƒåœ¨VQAå‡†ç¡®ç‡ä¸Šæå‡8.6%çš„åŒæ—¶ä¿æŒäº†æè¿°ç”Ÿæˆè´¨é‡ï¼Œè¯æ˜äº†ä»»åŠ¡ç‰¹å®šä¼˜åŒ–åœ¨å¤šæ¨¡æ€è§†é¢‘ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤æ‚è§†é¢‘åˆ†æä»»åŠ¡æä¾›äº†æ–°çš„æ¡†æ¶è®¾è®¡æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Traffic safety analysis requires complex video understanding to capture
fine-grained behavioral patterns and generate comprehensive descriptions for
accident prevention. In this work, we present a unique dual-model framework
that strategically utilizes the complementary strengths of VideoLLaMA and
Qwen2.5-VL through task-specific optimization to address this issue. The core
insight behind our approach is that separating training for captioning and
visual question answering (VQA) tasks minimizes task interference and allows
each model to specialize more effectively. Experimental results demonstrate
that VideoLLaMA is particularly effective in temporal reasoning, achieving a
CIDEr score of 1.1001, while Qwen2.5-VL excels in visual understanding with a
VQA accuracy of 60.80\%. Through extensive experiments on the WTS dataset, our
method achieves an S2 score of 45.7572 in the 2025 AI City Challenge Track 2,
placing 10th on the challenge leaderboard. Ablation studies validate that our
separate training strategy outperforms joint training by 8.6\% in VQA accuracy
while maintaining captioning quality.</p>
<h3 id="4-prompt-guided-spatial-understanding-with-rgb-d-transformers-for-fine-grained-object-relation-reasoning">[4] <a href="https://arxiv.org/abs/2510.11996">Prompt-Guided Spatial Understanding with RGB-D Transformers for Fine-Grained Object Relation Reasoning</a></h3>
<p><em>Tanner Muturi, Blessing Agyei Kyem, Joshua Kofi Asamoah, Neema Jakisa Owor, Richard Dyzinela, Andrews Danyo, Yaw Adu-Gyamfi, Armstrong Aboah</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“é—¨çš„ç©ºé—´æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡å°†æ©ç ç»´åº¦åµŒå…¥è¾“å…¥æç¤ºä¸­å¢å¼ºç©ºé—´ç†è§£ï¼Œåœ¨Physical AI Spatial Intelligence Warehouseæ•°æ®é›†ä¸Šå®ç°äº†73.0606çš„æœ€ç»ˆå¾—åˆ†ï¼Œåœ¨å…¬å¼€æ’è¡Œæ¦œä¸Šæ’åç¬¬4ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§è§„æ¨¡3Dç¯å¢ƒä¸­çš„ç©ºé—´æ¨ç†é¢ä¸´åœºæ™¯æ‚ä¹±ã€é®æŒ¡å’Œç²¾ç¡®ç©ºé—´ç†è§£çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ¨¡å‹è¿‡åº¦ä¾èµ–å±€éƒ¨å¤–è§‚ç‰¹å¾ä¸”ç¼ºä¹æ˜¾å¼ç©ºé—´åŸºç¡€ï¼Œå¯¼è‡´åœ¨ä»“åº“ç­‰å·¥ä¸šç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¡†æ¶å°†è¾¹ç•Œæ¡†åæ ‡å½¢å¼çš„æ©ç ç»´åº¦ç›´æ¥åµŒå…¥è¾“å…¥æç¤ºä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¨ç†å¯¹è±¡å‡ ä½•å’Œå¸ƒå±€ï¼Œå¹¶åœ¨è·ç¦»ä¼°è®¡ã€å¯¹è±¡è®¡æ•°ã€å¤šé€‰åŸºç¡€å’Œç©ºé—´å…³ç³»æ¨ç†å››ä¸ªé—®é¢˜ç±»åˆ«ä¸Šä½¿ç”¨ä»»åŠ¡ç‰¹å®šç›‘ç£è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶åœ¨è®­ç»ƒé›†ä¸­å°†æ ‡å‡†åŒ–ç­”æ¡ˆé™„åŠ åˆ°GPTå“åº”ä¸­ä»¥æé«˜ä¸è¯„ä¼°ç³»ç»Ÿçš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> è¯¥ç»¼åˆç®¡é“åœ¨å…¬å¼€æ’è¡Œæ¦œä¸Šå–å¾—äº†73.0606çš„æœ€ç»ˆå¾—åˆ†ï¼Œæ€»ä½“æ’åç¬¬4ä½ï¼Œè¯æ˜äº†æ‰€ææ–¹æ³•åœ¨çœŸå®å·¥ä¸šç¯å¢ƒç©ºé—´æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç»“æ„åŒ–æç¤ºä¸°å¯Œå’Œé’ˆå¯¹æ€§ä¼˜åŒ–èƒ½æœ‰æ•ˆæ¨è¿›çœŸå®å·¥ä¸šç¯å¢ƒä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä¸ºå¤æ‚3Dåœºæ™¯ä¸‹çš„è§†è§‰è¯­è¨€ç³»ç»Ÿæä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Spatial reasoning in large-scale 3D environments such as warehouses remains a
significant challenge for vision-language systems due to scene clutter,
occlusions, and the need for precise spatial understanding. Existing models
often struggle with generalization in such settings, as they rely heavily on
local appearance and lack explicit spatial grounding. In this work, we
introduce a dedicated spatial reasoning framework for the Physical AI Spatial
Intelligence Warehouse dataset introduced in the Track 3 2025 AI City
Challenge. Our approach enhances spatial comprehension by embedding mask
dimensions in the form of bounding box coordinates directly into the input
prompts, enabling the model to reason over object geometry and layout. We
fine-tune the framework across four question categories namely: Distance
Estimation, Object Counting, Multi-choice Grounding, and Spatial Relation
Inference using task-specific supervision. To further improve consistency with
the evaluation system, normalized answers are appended to the GPT response
within the training set. Our comprehensive pipeline achieves a final score of
73.0606, placing 4th overall on the public leaderboard. These results
demonstrate the effectiveness of structured prompt enrichment and targeted
optimization in advancing spatial reasoning for real-world industrial
environments.</p>
<h3 id="5-vision-language-models-map-logos-to-text-via-semantic-entanglement-in-the-visual-projector">[5] <a href="https://arxiv.org/abs/2510.12287">Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector</a></h3>
<p><em>Sifan Li, Hongkai Chen, Yujun Cai, Qingwen Ye, Liyang Chen, Junsong Yuan, Yiwei Wang</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„logoå¹»è§‰é—®é¢˜ï¼Œå‘ç°æ¨¡å‹å€¾å‘äºåŸºäºç¬¦å·å…ˆéªŒè€ŒéçœŸå®å­—å½¢æ„ŸçŸ¥ç”Ÿæˆå“ç‰Œåç§°ï¼Œå¹¶é€šè¿‡æŠ•å½±å™¨å­ç©ºé—´åˆ†ææ­ç¤ºäº†è¿™ä¸€å¤±è´¥æ¨¡å¼çš„å…³é”®æœºåˆ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é¢å¯¹ä¸åŒ…å«å¯è§æ–‡å­—çš„çº¯ç¬¦å·logoæ—¶ï¼Œä»ç„¶å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå³è¾“å‡ºç¼ºä¹è§†è§‰è¯æ®æ”¯æŒçš„å“ç‰Œåç§°æˆ–æ–‡æœ¬å†…å®¹ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨ç²¾å¿ƒç­–åˆ’çš„çº¯ç¬¦å·ã€æ··åˆå‹å’Œå«æ–‡æœ¬logoæ•°æ®é›†ï¼ŒåŒ…æ‹¬å…·æœ‰æŒ‘æˆ˜æ€§çš„Hard-60å­é›†ï¼Œé€šè¿‡ä¹ç§ç»“æ„åŒ–æ‰°åŠ¨æµ‹è¯•æ¨¡å‹é²æ£’æ€§ï¼Œå¹¶å¯¹å¼€æºLLaVAæ¨¡å‹è¿›è¡ŒåµŒå…¥çº§åˆ†æä»¥è¯†åˆ«æŠ•å½±å™¨ç»´åº¦ä¸å¹»è§‰çš„å…³è”ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜å¹»è§‰ç°è±¡åœ¨å„ç§æ‰°åŠ¨ä¸‹æŒç»­å­˜åœ¨ï¼Œå…¶ä¸­é®æŒ¡æš´éœ²äº†æœ€ä¸¥é‡çš„å¼±ç‚¹ï¼ŒåµŒå…¥åˆ†ææ˜¾ç¤ºå¹»è§‰ä¸æŠ•å½±å™¨çš„å°‘é‡ç»´åº¦å¯†åˆ‡ç›¸å…³ï¼Œé’ˆå¯¹æ€§æ¶ˆèèƒ½æ˜¾è‘—å‡å°‘é”™è¯¯åŒæ—¶ä¿æŒOCRå‡†ç¡®æ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å‘ç°VLMså¯¹æ ‡å¿—æ€§åœ†å½¢logoä¸»è¦ä¾èµ–ç¬¦å·å…ˆéªŒè€ŒéçœŸå®è§†è§‰æ„ŸçŸ¥ï¼ŒæŠ•å½±å™¨å­ç©ºé—´åœ¨è¿™ä¸€å¤±è´¥æ¨¡å¼ä¸­èµ·å†³å®šæ€§ä½œç”¨ï¼Œæå‡ºäº†æŠ•å½±å™¨è§£è€¦å’ŒOCRå¼•å¯¼è§£ç ä½œä¸ºæ„å»ºæ›´å¯ä¿¡å¤šæ¨¡æ€ç³»ç»Ÿçš„æœ‰å‰æ™¯æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Vision Language Models (VLMs) have achieved impressive progress in multimodal
reasoning; yet, they remain vulnerable to hallucinations, where outputs are not
grounded in visual evidence. In this paper, we investigate a previously
overlooked setting: logo hallucination, where models generate brand names or
textual content despite logos containing no visible words. Using curated splits
of pure symbols, hybrids, and text-bearing logos, as well as the challenging
Hard-60 subset, we systematically measure hallucination across leading VLMs. We
further probe robustness through nine structured perturbations and show that
hallucinations persist even under strong distortions, with occlusion exposing
the sharpest weaknesses. Embedding-level analysis with open-weight LLaVA
demonstrates that hallucination is tied to a small subset of projector
dimensions, and targeted ablation substantially reduces errors while preserving
OCR accuracy. Together, these findings reveal that VLMs often rely on symbolic
priors rather than genuine glyph perception, particularly for iconic circular
logos, and that projector subspaces play a decisive role in this failure mode.
Our work contributes both a novel diagnostic lens and actionable mitigation
insights, highlighting projector disentanglement and OCR-guided decoding as
promising directions for building more trustworthy multimodal systems.</p>
<h3 id="6-il3d-a-large-scale-indoor-layout-dataset-for-llm-driven-3d-scene-generation">[6] <a href="https://arxiv.org/abs/2510.12095">IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation</a></h3>
<p><em>Wenxu Zhou, Kaixuan Nie, Hang Du, Dong Yin, Wei Huang, Siqiang Guo, Xiaobo Zhang, Pengbo Hu</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†IL3Dï¼Œä¸€ä¸ªä¸“ä¸ºLLMé©±åŠ¨çš„3Dåœºæ™¯ç”Ÿæˆè®¾è®¡çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«27,816ä¸ªå®¤å†…å¸ƒå±€å’Œ29,215ä¸ªé«˜ä¿çœŸ3Då¯¹è±¡èµ„äº§ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒæ˜¾è‘—æå‡äº†åœºæ™¯ç”Ÿæˆçš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å®¤å†…å¸ƒå±€è®¾è®¡ä¸­é«˜è´¨é‡å¤šæ ·åŒ–è®­ç»ƒæ•°æ®çš„è¿«åˆ‡éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„3Dåœºæ™¯ç”Ÿæˆä»»åŠ¡ï¼Œå½“å‰ç¼ºä¹å…·æœ‰å®ä¾‹çº§è‡ªç„¶è¯­è¨€æ ‡æ³¨çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†åŒ…å«27,816ä¸ªå®¤å†…å¸ƒå±€å’Œ18ç§å¸¸è§æˆ¿é—´ç±»å‹çš„å¤§è§„æ¨¡æ•°æ®é›†IL3Dï¼Œé…å¤‡29,215ä¸ªé«˜ä¿çœŸ3Då¯¹è±¡èµ„äº§ï¼Œå¹¶æä¾›äº†å®ä¾‹çº§è‡ªç„¶è¯­è¨€æ ‡æ³¨ä»¥æ”¯æŒå¤šæ¨¡æ€å­¦ä¹ ï¼ŒåŒæ—¶å»ºç«‹äº†ä¸¥æ ¼çš„åŸºå‡†è¯„ä¼°LLMé©±åŠ¨çš„åœºæ™¯ç”Ÿæˆã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨IL3Dæ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒæ˜¾è‘—æå‡äº†LLMçš„æ³›åŒ–èƒ½åŠ›ï¼Œå…¶æ€§èƒ½è¶…è¶Šäº†åœ¨å…¶ä»–æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒçš„ç»“æœï¼Œæ•°æ®é›†æ”¯æŒç‚¹äº‘ã€3Dè¾¹ç•Œæ¡†ã€å¤šè§†è§’å›¾åƒã€æ·±åº¦å›¾ã€æ³•çº¿å›¾å’Œè¯­ä¹‰æ©ç ç­‰å¤šç§æ¨¡æ€æ•°æ®å¯¼å‡ºã€‚</p>
<p><strong>Conclusion:</strong> IL3Dä½œä¸ºä¸€ä¸ªå¤šåŠŸèƒ½ä¸”ç¨³å¥çš„èµ„æºï¼Œé€šè¿‡æä¾›é«˜ä¿çœŸåœºæ™¯æ•°æ®æ¥æ”¯æŒå…·èº«æ™ºèƒ½ä½“çš„ç¯å¢ƒæ„ŸçŸ¥ä»»åŠ¡ï¼Œæ˜¾è‘—æ¨è¿›äº†3Dåœºæ™¯ç”Ÿæˆå’Œå…·èº«æ™ºèƒ½é¢†åŸŸçš„ç ”ç©¶å‘å±•ï¼Œä¸ºå„ç§è§†è§‰ä»»åŠ¡æä¾›äº†æ— ç¼é€‚é…èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>In this study, we present IL3D, a large-scale dataset meticulously designed
for large language model (LLM)-driven 3D scene generation, addressing the
pressing demand for diverse, high-quality training data in indoor layout
design. Comprising 27,816 indoor layouts across 18 prevalent room types and a
library of 29,215 high-fidelity 3D object assets, IL3D is enriched with
instance-level natural language annotations to support robust multimodal
learning for vision-language tasks. We establish rigorous benchmarks to
evaluate LLM-driven scene generation. Experimental results show that supervised
fine-tuning (SFT) of LLMs on IL3D significantly improves generalization and
surpasses the performance of SFT on other datasets. IL3D offers flexible
multimodal data export capabilities, including point clouds, 3D bounding boxes,
multiview images, depth maps, normal maps, and semantic masks, enabling
seamless adaptation to various visual tasks. As a versatile and robust
resource, IL3D significantly advances research in 3D scene generation and
embodied intelligence, by providing high-fidelity scene data to support
environment perception tasks of embodied agents.</p>
<h3 id="7-reasoning-in-the-dark-interleaved-vision-text-reasoning-in-latent-space">[7] <a href="https://arxiv.org/abs/2510.12603">Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space</a></h3>
<p><em>Chao Chen, Zhixin Ma, Yongqi Li, Yupeng Hu, Yinwei Wei, Wenjie Li, Liqiang Nie</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€æ½œåœ¨æ¨ç†æ–¹æ³•IVT-LRï¼Œé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­èåˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶å‡å°‘äº†æ ‡æ³¨éœ€æ±‚ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€æ¨ç†æ–¹æ³•ä¾èµ–æ˜¾å¼æ¨ç†æ­¥éª¤ï¼Œéœ€è¦å¤§é‡äººå·¥æ ‡æ³¨çš„è§†è§‰-æ–‡æœ¬æ•°æ®ï¼Œå¹¶ä¸”æ¨ç†å»¶è¿Ÿè¾ƒé«˜ï¼Œè¿™é™åˆ¶äº†å®é™…åº”ç”¨æ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†äº¤ç»‡è§†è§‰-æ–‡æœ¬æ½œåœ¨æ¨ç†æ–¹æ³•ï¼Œå°†æ¯ä¸ªæ¨ç†æ­¥éª¤è¡¨ç¤ºä¸ºæ½œåœ¨æ–‡æœ¬å’Œæ½œåœ¨è§†è§‰çš„ç»“åˆï¼Œå¹¶é‡‡ç”¨æ¸è¿›å¼å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥æ¥è®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ‰§è¡Œæ½œåœ¨æ¨ç†ã€‚</p>
<p><strong>Result:</strong> åœ¨M3CoTå’ŒScienceQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒIVT-LRæ–¹æ³•å¹³å‡å‡†ç¡®ç‡æå‡5.45%ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æ¯”ç°æœ‰æ–¹æ³•å¿«5å€ä»¥ä¸Šã€‚</p>
<p><strong>Conclusion:</strong> å¤šæ¨¡æ€æ½œåœ¨æ¨ç†é€šè¿‡æ½œåœ¨ç©ºé—´è¡¨ç¤ºæœ‰æ•ˆè§£å†³äº†æ˜¾å¼æ¨ç†çš„æ•ˆç‡å’Œæ ‡æ³¨æˆæœ¬é—®é¢˜ï¼Œä¸ºé«˜æ•ˆå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Multimodal reasoning aims to enhance the capabilities of MLLMs by
incorporating intermediate reasoning steps before reaching the final answer. It
has evolved from text-only reasoning to the integration of visual information,
enabling the thought process to be conveyed through both images and text.
Despite its effectiveness, current multimodal reasoning methods depend on
explicit reasoning steps that require labor-intensive vision-text annotations
and inherently introduce significant inference latency. To address these
issues, we introduce multimodal latent reasoning with the advantages of
multimodal representation, reduced annotation, and inference efficiency. To
facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),
which injects both visual and textual information in the reasoning process
within the latent space. Specifically, IVT-LR represents each reasoning step by
combining two implicit parts: latent text (the hidden states from the previous
step) and latent vision (a set of selected image embeddings). We further
introduce a progressive multi-stage training strategy to enable MLLMs to
perform the above multimodal latent reasoning steps. Experiments on M3CoT and
ScienceQA demonstrate that our IVT-LR method achieves an average performance
increase of 5.45% in accuracy, while simultaneously achieving a speed increase
of over 5 times compared to existing approaches. Code available at
https://github.com/FYYDCC/IVT-LR.</p>
<h3 id="8-imagesentinel-protecting-visual-datasets-from-unauthorized-retrieval-augmented-image-generation">[8] <a href="https://arxiv.org/abs/2510.12119">ImageSentinel: Protecting Visual Datasets from Unauthorized Retrieval-Augmented Image Generation</a></h3>
<p><em>Ziyuan Luo, Yangyi Zhao, Ka Chun Cheung, Simon See, Renjie Wan</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ImageSentinelæ¡†æ¶ï¼Œé€šè¿‡åˆæˆå“¨å…µå›¾åƒæ¥ä¿æŠ¤è§†è§‰æ•°æ®é›†åœ¨æ£€ç´¢å¢å¼ºå›¾åƒç”Ÿæˆç³»ç»Ÿä¸­çš„æœªç»æˆæƒä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸åŸå§‹æ•°æ®é›†è§†è§‰ä¸€è‡´çš„å“¨å…µå›¾åƒï¼Œå¹¶é€šè¿‡éšæœºå­—ç¬¦åºåˆ—ä½œä¸ºæ£€ç´¢å¯†é’¥è¿›è¡Œä¿æŠ¤éªŒè¯ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ£€ç´¢å¢å¼ºå›¾åƒç”Ÿæˆç³»ç»Ÿçš„å¹¿æ³›é‡‡ç”¨å¼•å‘äº†å…³äºæœªç»æˆæƒä½¿ç”¨ç§æœ‰å›¾åƒæ•°æ®é›†çš„ä¸¥é‡æ‹…å¿§ã€‚ä¼ ç»Ÿæ•°å­—æ°´å°æ–¹æ³•åœ¨RAIGç³»ç»Ÿä¸­é¢ä¸´å±€é™æ€§ï¼Œå› ä¸ºå¤æ‚çš„ç‰¹å¾æå–å’Œé‡ç»„è¿‡ç¨‹æ— æ³•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿ç•™æ°´å°ä¿¡å·ï¼Œä¿æŠ¤è§†è§‰æ•°æ®é›†å…å—æ­¤ç±»ç³»ç»Ÿä¸­çš„æœªç»æˆæƒä½¿ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„ImageSentinelæ¡†æ¶åˆæˆä¸åŸå§‹æ•°æ®é›†ä¿æŒè§†è§‰ä¸€è‡´æ€§çš„å“¨å…µå›¾åƒï¼Œè¿™äº›å“¨å…µé€šè¿‡éšæœºç”Ÿæˆçš„å­—ç¬¦åºåˆ—ä½œä¸ºæ£€ç´¢å¯†é’¥å®ç°ä¿æŠ¤éªŒè¯ã€‚ä¸ºç¡®ä¿æ— ç¼é›†æˆï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆå“¨å…µå›¾åƒï¼Œä»è€Œåœ¨ä¿æŠ¤æ•°æ®é›†çš„åŒæ—¶ç»´æŒç³»ç»Ÿçš„æ­£å¸¸åŠŸèƒ½ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒImageSentinelèƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹æœªç»æˆæƒçš„æ•°æ®é›†ä½¿ç”¨ï¼ŒåŒæ—¶åœ¨æˆæƒåº”ç”¨ä¸­ä¿æŒç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŠ¤éªŒè¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶åœ¨RAIGç³»ç»Ÿä¸­ä¿æŠ¤è§†è§‰æ•°æ®é›†çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè§£å†³RAIGç³»ç»Ÿä¸­è§†è§‰æ•°æ®é›†ä¿æŠ¤é—®é¢˜æä¾›äº†åˆ›æ–°è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†é€šè¿‡åˆæˆå“¨å…µå›¾åƒå®ç°ä¿æŠ¤éªŒè¯çš„å¯è¡Œæ€§ã€‚ImageSentinelæ¡†æ¶ä¸ºä¿æŠ¤ç§æœ‰è§†è§‰æ•°æ®å…å—æœªç»æˆæƒä½¿ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆç³»ç»Ÿçš„æ€§èƒ½ï¼Œå¯¹æœªæ¥æ•°æ®ä¿æŠ¤æŠ€æœ¯çš„å‘å±•å…·æœ‰é‡è¦å¯ç¤ºæ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>The widespread adoption of Retrieval-Augmented Image Generation (RAIG) has
raised significant concerns about the unauthorized use of private image
datasets. While these systems have shown remarkable capabilities in enhancing
generation quality through reference images, protecting visual datasets from
unauthorized use in such systems remains a challenging problem. Traditional
digital watermarking approaches face limitations in RAIG systems, as the
complex feature extraction and recombination processes fail to preserve
watermark signals during generation. To address these challenges, we propose
ImageSentinel, a novel framework for protecting visual datasets in RAIG. Our
framework synthesizes sentinel images that maintain visual consistency with the
original dataset. These sentinels enable protection verification through
randomly generated character sequences that serve as retrieval keys. To ensure
seamless integration, we leverage vision-language models to generate the
sentinel images. Experimental results demonstrate that ImageSentinel
effectively detects unauthorized dataset usage while preserving generation
quality for authorized applications. Code is available at
https://github.com/luo-ziyuan/ImageSentinel.</p>
<h3 id="9-srum-fine-grained-self-rewarding-for-unified-multimodal-models">[9] <a href="https://arxiv.org/abs/2510.12784">SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models</a></h3>
<p><em>Weiyang Jin, Yuwei Niu, Jiaqi Liao, Chengqi Duan, Aoxue Li, Shenghua Gao, Xihui Liu</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSRUMï¼Œä¸€ç§è‡ªå¥–åŠ±åè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡è®©ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„ç†è§£æ¨¡å—ä½œä¸ºå†…éƒ¨è¯„ä¼°å™¨æ¥æŒ‡å¯¼å…¶ç”Ÿæˆæ¨¡å—çš„æ”¹è¿›ï¼Œå®ç°äº†æ— éœ€é¢å¤–äººå·¥æ ‡æ³¨æ•°æ®çš„è‡ªæˆ‘æå‡ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†è§†è§‰ç”Ÿæˆæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹å­˜åœ¨ç†è§£èƒ½åŠ›ä¸ç”Ÿæˆèƒ½åŠ›ä¹‹é—´çš„æ˜¾è‘—å·®è·ï¼Œæ¨¡å‹èƒ½å¤Ÿæ­£ç¡®ç†è§£å›¾åƒå´æ— æ³•ç”Ÿæˆå¿ å®äºæ–‡æœ¬æç¤ºçš„å›¾åƒï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæ¨¡å‹èƒ½å¦åˆ©ç”¨å…¶ç†è§£æ¨¡å—æ¥å¥–åŠ±ç”Ÿæˆæ¨¡å—ä»¥å®ç°è‡ªæˆ‘æ”¹è¿›ã€‚</p>
<p><strong>Method:</strong> SRUMæ¡†æ¶åˆ›å»ºäº†ä¸€ä¸ªåé¦ˆå¾ªç¯ï¼Œå…¶ä¸­æ¨¡å‹çš„ç†è§£æ¨¡å—ä½œä¸ºå†…éƒ¨è¯„ä¼°å™¨æä¾›çº æ­£ä¿¡å·æ¥æ”¹è¿›ç”Ÿæˆæ¨¡å—ï¼Œæ— éœ€é¢å¤–äººå·¥æ ‡æ³¨æ•°æ®ã€‚è¯¥æ¡†æ¶è®¾è®¡äº†å…¨å±€-å±€éƒ¨åŒå¥–åŠ±ç³»ç»Ÿï¼Œå…¨å±€å¥–åŠ±ç¡®ä¿æ•´ä½“è§†è§‰è¯­ä¹‰å’Œå¸ƒå±€çš„æ­£ç¡®æ€§ï¼Œå±€éƒ¨å¥–åŠ±ç»†åŒ–ç»†ç²’åº¦çš„å¯¹è±¡çº§ä¿çœŸåº¦ã€‚</p>
<p><strong>Result:</strong> SRUMåœ¨T2I-CompBenchåŸºå‡†ä¸Šä»82.18æå‡è‡³88.37ï¼Œåœ¨T2I-ReasonBenchåŸºå‡†ä¸Šä»43.82æå‡è‡³46.75ï¼Œå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›å’Œè‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶å»ºç«‹äº†ä¸€ç§å¼ºå¤§çš„æ–°èŒƒå¼ï¼Œä½¿ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è‡ªå¥–åŠ±æœºåˆ¶å®ç°ç†è§£æ¨¡å—å¯¹ç”Ÿæˆæ¨¡å—çš„å¼•å¯¼å’Œå¢å¼ºï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„è‡ªæˆ‘æ”¹è¿›å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Recently, remarkable progress has been made in Unified Multimodal Models
(UMMs), which integrate vision-language generation and understanding
capabilities within a single framework. However, a significant gap exists where
a model's strong visual understanding often fails to transfer to its visual
generation. A model might correctly understand an image based on user
instructions, yet be unable to generate a faithful image from text prompts.
This phenomenon directly raises a compelling question: Can a model achieve
self-improvement by using its understanding module to reward its generation
module? To bridge this gap and achieve self-improvement, we introduce SRUM, a
self-rewarding post-training framework that can be directly applied to existing
UMMs of various designs. SRUM creates a feedback loop where the model's own
understanding module acts as an internal ``evaluator'', providing corrective
signals to improve its generation module, without requiring additional
human-labeled data. To ensure this feedback is comprehensive, we designed a
global-local dual reward system. To tackle the inherent structural complexity
of images, this system offers multi-scale guidance: a \textbf{global reward}
ensures the correctness of the overall visual semantics and layout, while a
\textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads
to powerful capabilities and shows strong generalization, boosting performance
on T2I-CompBench from 82.18 to \textbf{88.37} and on T2I-ReasonBench from 43.82
to \textbf{46.75}. Overall, our work establishes a powerful new paradigm for
enabling a UMMs' understanding module to guide and enhance its own generation
via self-rewarding.</p>
<h3 id="10-metacaptioner-towards-generalist-visual-captioning-with-open-source-suites">[10] <a href="https://arxiv.org/abs/2510.12126">MetaCaptioner: Towards Generalist Visual Captioning with Open-source Suites</a></h3>
<p><em>Zhenxin Lei, Zhangwei Gao, Changyao Tian, Erfei Cui, Guanzhou Chen, Danni Yang, Yuchen Duan, Zhaokai Wang, Wenhao Li, Weiyun Wang, Xiangyu Zhao, Jiayi Ji, Yu Qiao, Wenhai Wang, Gen Luo</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CapFlowå¤šæ™ºèƒ½ä½“åä½œå·¥ä½œæµï¼Œé¦–æ¬¡è¯æ˜åˆ©ç”¨å¼€æºæ¨¡å‹å¯åœ¨å¤šä¸ªè§†è§‰é¢†åŸŸè¾¾åˆ°ä¸GPT-4ç›¸å½“çš„æè¿°è´¨é‡ï¼ŒåŒæ—¶æˆæœ¬é™ä½89.5%ã€‚é€šè¿‡CapFlowä½œä¸ºæ•°æ®åˆæˆå™¨ï¼Œè®­ç»ƒå‡ºçš„MetaCaptioneråœ¨å¼€æºç¤¾åŒºè¾¾åˆ°é¡¶çº§å¤šæ¨¡æ€æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¼€æºè§†è§‰æè¿°æ¨¡å‹ä¸å•†ä¸šæ¨¡å‹å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œé™åˆ¶äº†æ•°æ®åˆæˆç­‰åº”ç”¨çš„å‘å±•ã€‚è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œæ–¹æ³•å¼¥åˆè¿™ä¸€å·®è·ï¼Œä¸ºå¤šæ¨¡æ€ç ”ç©¶æä¾›é«˜è´¨é‡ä¸”æˆæœ¬æ•ˆç›Šçš„è§†è§‰æè¿°è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†CapFlowå¤šæ™ºèƒ½ä½“åä½œå·¥ä½œæµï¼Œåˆ©ç”¨å¼€æºæ¨¡å‹æ„å»ºæ•°æ®åˆæˆå™¨ï¼Œé€šè¿‡å¤§è§„æ¨¡å›¾åƒå’Œè§†é¢‘æ•°æ®ç”Ÿæˆé«˜è´¨é‡è§†è§‰æè¿°ï¼Œå¹¶åŸºäºè¿™äº›æ•°æ®å¾®è°ƒå¾—åˆ°é€šç”¨è§†è§‰æè¿°å™¨MetaCaptionerã€‚</p>
<p><strong>Result:</strong> CapFlowåœ¨å¤šä¸ªè§†è§‰é¢†åŸŸå®ç°äº†ä¸GPT-4ç›¸å½“çš„æè¿°è´¨é‡ï¼ŒåŒæ—¶æˆæœ¬é™ä½89.5%ã€‚MetaCaptionerä¸ä»…å…·å¤‡ä¸å•†ä¸šæ¨¡å‹ç›¸å½“çš„æè¿°èƒ½åŠ›ï¼Œåœ¨å¼€æºå¤šæ¨¡æ€ç¤¾åŒºä¸­è¾¾åˆ°äº†é¡¶çº§æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> CapFlowå’ŒMetaCaptionerä¸ºæœªæ¥å¤šæ¨¡æ€ç ”ç©¶æä¾›äº†å¼ºå¤§ä¸”æˆæœ¬æ•ˆç›Šçš„è§†è§‰æè¿°è§£å†³æ–¹æ¡ˆï¼Œè¯æ˜äº†é€šè¿‡åˆç†çš„å·¥ä½œæµè®¾è®¡ï¼Œå¼€æºæ¨¡å‹èƒ½å¤Ÿè¾¾åˆ°å•†ä¸šæ¨¡å‹çš„æ€§èƒ½æ°´å¹³ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Generalist visual captioning goes beyond a simple appearance description
task, but requires integrating a series of visual cues into a caption and
handling various visual domains. In this task, current open-source models
present a large performance gap with commercial ones, which limits various
applications such as data synthesis. To bridge the gap, this paper proposes
CapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for
the first time that, by capitalizing on open-source models, it is possible to
achieve caption quality on par with GPT-4.1 in various domains with an 89.5%
reduction in costs. By leveraging CapFlow as the data synthesizer, we produce
high-quality visual captions from image and video domains at scale, and obtain
a generalist visual captioner via fine-tuning, namely MetaCaptioner. Through
extensive experiments, we show that MetaCaptioner not only achieves comparable
captioning capabilities with commercial models but also reaches top-tier
multimodal performance in the open-source community. We hope CapFlow and
MetaCaptioner can benefit future multimodal research by providing a strong and
cost-effective visual captioning solution.</p>
<h3 id="11-state-space-prompting-via-gathering-and-spreading-spatio-temporal-information-for-video-understanding">[11] <a href="https://arxiv.org/abs/2510.12160">State Space Prompting via Gathering and Spreading Spatio-Temporal Information for Video Understanding</a></h3>
<p><em>Jiahuan Zhou, Kai Zhu, Zhenyu Cui, Zichen Liu, Xu Zou, Gang Hua</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§çŠ¶æ€ç©ºé—´æç¤ºï¼ˆSSPï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¸§å†…å’Œå¸§é—´æç¤ºæ¥èšåˆå’Œä¼ æ’­è§†é¢‘ä¸­çš„å…³é”®æ—¶ç©ºä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘ç†è§£æ€§èƒ½ï¼Œåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå¹³å‡ä¼˜äºç°æœ‰SOTAæ–¹æ³•2.76%ï¼ŒåŒæ—¶å‡å°‘äº†å¾®è°ƒå‚æ•°å¼€é”€ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é¢„è®­ç»ƒçŠ¶æ€ç©ºé—´æ¨¡å‹ä¸­çš„é¡ºåºå‹ç¼©è§†è§‰æç¤ºä»¤ç‰Œæ— æ³•æœ‰æ•ˆæ•è·è§†é¢‘ä¸­çš„ç©ºé—´å’Œæ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé™åˆ¶äº†ç©ºé—´ä¿¡æ¯åœ¨è§†é¢‘å¸§å†…å’Œæ—¶é—´ä¿¡æ¯åœ¨å¸§é—´çš„æœ‰æ•ˆä¼ æ’­ï¼Œä»è€Œå½±å“äº†åˆ¤åˆ«æ€§ä¿¡æ¯çš„æå–æ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†çŠ¶æ€ç©ºé—´æç¤ºï¼ˆSSPï¼‰æ–¹æ³•ï¼ŒåŒ…å«å¸§å†…èšé›†ï¼ˆIFGï¼‰æ¨¡å—ç”¨äºèšåˆæ¯å¸§å†…çš„ç©ºé—´å…³é”®ä¿¡æ¯ï¼Œä»¥åŠå¸§é—´ä¼ æ’­ï¼ˆIFSï¼‰æ¨¡å—ç”¨äºåœ¨ä¸åŒå¸§é—´ä¼ æ’­åˆ¤åˆ«æ€§æ—¶ç©ºä¿¡æ¯ï¼Œé€šè¿‡è‡ªé€‚åº”å¹³è¡¡å’Œå‹ç¼©å¸§å†…å’Œå¸§é—´çš„å…³é”®æ—¶ç©ºä¿¡æ¯æ¥äº’è¡¥åœ°ä¼ æ’­è§†é¢‘ä¸­çš„åˆ¤åˆ«æ€§ä¿¡æ¯ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªè§†é¢‘åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯è¡¨æ˜ï¼ŒSSPæ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰SOTAæ–¹æ³•ï¼Œå¹³å‡æ€§èƒ½æå‡è¾¾2.76%ï¼ŒåŒæ—¶å‡å°‘äº†å¾®è°ƒå‚æ•°çš„å¼€é”€ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡å¸§å†…å’Œå¸§é—´æç¤ºçš„äº’è¡¥è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆæå‡çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä¸ºé«˜æ•ˆè§†é¢‘è¡¨ç¤ºå­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯ï¼ŒåŒæ—¶ä¿æŒäº†å‚æ•°æ•ˆç‡çš„ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Recently, pre-trained state space models have shown great potential for video
classification, which sequentially compresses visual tokens in videos with
linear complexity, thereby improving the processing efficiency of video data
while maintaining high performance. To apply powerful pre-trained models to
downstream tasks, prompt learning is proposed to achieve efficient downstream
task adaptation with only a small number of fine-tuned parameters. However, the
sequentially compressed visual prompt tokens fail to capture the spatial and
temporal contextual information in the video, thus limiting the effective
propagation of spatial information within a video frame and temporal
information between frames in the state compression model and the extraction of
discriminative information. To tackle the above issue, we proposed a State
Space Prompting (SSP) method for video understanding, which combines
intra-frame and inter-frame prompts to aggregate and propagate key
spatiotemporal information in the video. Specifically, an Intra-Frame Gathering
(IFG) module is designed to aggregate spatial key information within each
frame. Besides, an Inter-Frame Spreading (IFS) module is designed to spread
discriminative spatio-temporal information across different frames. By
adaptively balancing and compressing key spatio-temporal information within and
between frames, our SSP effectively propagates discriminative information in
videos in a complementary manner. Extensive experiments on four video benchmark
datasets verify that our SSP significantly outperforms existing SOTA methods by
2.76% on average while reducing the overhead of fine-tuning parameters.</p>
<h3 id="12-unigs-unified-geometry-aware-gaussian-splatting-for-multimodal-rendering">[12] <a href="https://arxiv.org/abs/2510.12174">UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering</a></h3>
<p><em>Yusen Xie, Zhenmin Huang, Jianhao Jiao, Dimitrios Kanoulas, Jun Ma</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UniGSï¼Œä¸€ç§åŸºäº3Dé«˜æ–¯æº…å°„çš„ç»Ÿä¸€åœ°å›¾è¡¨ç¤ºå’Œå¯å¾®åˆ†æ¡†æ¶ï¼Œç”¨äºé«˜ä¿çœŸå¤šæ¨¡æ€3Dé‡å»ºã€‚è¯¥æ¡†æ¶é€šè¿‡é‡æ–°è®¾è®¡å…‰æ …åŒ–è¿‡ç¨‹å®ç°å‡ ä½•æ„ŸçŸ¥çš„æ·±åº¦å’Œæ³•çº¿æ¸²æŸ“ï¼Œå¹¶åœ¨æ‰€æœ‰æ¨¡æ€ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„é‡å»ºç²¾åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰3Dé‡å»ºæ–¹æ³•åœ¨å¤šæ¨¡æ€æ•°æ®èåˆå’Œå‡ ä½•ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±åº¦å’Œè¡¨é¢æ³•çº¿æ¸²æŸ“çš„ç²¾åº¦ä¸ä¸€è‡´æ€§æ–¹é¢éœ€è¦æ”¹è¿›ã€‚ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨é«˜æ–¯ä¸­å¿ƒè¿›è¡Œæ·±åº¦æ¸²æŸ“æ— æ³•æœ‰æ•ˆä¼˜åŒ–æ—‹è½¬å’Œå°ºåº¦å±æ€§ï¼Œä¸”ç¼ºä¹å¯¹å‡ ä½•ä¸€è‡´æ€§çš„å……åˆ†ä¿è¯ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†CUDAåŠ é€Ÿçš„å…‰æ …åŒ–æµæ°´çº¿ï¼Œèƒ½å¤ŸåŒæ—¶æ¸²æŸ“çœŸå®æ„ŸRGBå›¾åƒã€å‡ ä½•ç²¾ç¡®æ·±åº¦å›¾ã€ä¸€è‡´è¡¨é¢æ³•çº¿å’Œè¯­ä¹‰é€»è¾‘ã€‚é‡æ–°è®¾è®¡å…‰æ …åŒ–è¿‡ç¨‹ï¼Œé€šè¿‡å¯å¾®åˆ†å°„çº¿-æ¤­çƒä½“ç›¸äº¤è€Œéé«˜æ–¯ä¸­å¿ƒè¿›è¡Œæ·±åº¦æ¸²æŸ“ï¼Œæ¨å¯¼è¡¨é¢æ³•çº¿æ¸²æŸ“çš„è§£ææ¢¯åº¦å…¬å¼ï¼Œå¹¶å¼•å…¥å¯å­¦ä¹ å±æ€§å®ç°è®­ç»ƒæœŸé—´å¯¹è´¡çŒ®æœ€å°çš„é«˜æ–¯è¿›è¡Œå¯å¾®åˆ†å‰ªæã€‚</p>
<p><strong>Result:</strong> å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼ŒUniGSåœ¨æ‰€æœ‰æ¨¡æ€ä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„é‡å»ºç²¾åº¦ï¼ŒéªŒè¯äº†å‡ ä½•æ„ŸçŸ¥èŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•åœ¨æ·±åº¦é‡å»ºã€è¡¨é¢æ³•çº¿ä¸€è‡´æ€§å’Œè¯­ä¹‰åˆ†å‰²ç­‰å¤šä¸ªä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> UniGSæ¡†æ¶é€šè¿‡å‡ ä½•æ„ŸçŸ¥çš„æ¸²æŸ“æ–¹æ³•æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€3Dé‡å»ºçš„è´¨é‡å’Œä¸€è‡´æ€§ï¼Œä¸ºé«˜ä¿çœŸåœºæ™¯é‡å»ºæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥å·¥ä½œå±•ç¤ºäº†å¯å¾®åˆ†å‡ ä½•å¤„ç†åœ¨å¤šæ¨¡æ€é‡å»ºä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å¼€æºä»£ç å’Œå¤šæ¨¡æ€æŸ¥çœ‹å™¨ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>In this paper, we propose UniGS, a unified map representation and
differentiable framework for high-fidelity multimodal 3D reconstruction based
on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated
rasterization pipeline capable of rendering photo-realistic RGB images,
geometrically accurate depth maps, consistent surface normals, and semantic
logits simultaneously. We redesign the rasterization to render depth via
differentiable ray-ellipsoid intersection rather than using Gaussian centers,
enabling effective optimization of rotation and scale attribute through
analytic depth gradients. Furthermore, we derive the analytic gradient
formulation for surface normal rendering, ensuring geometric consistency among
reconstructed 3D scenes. To improve computational and storage efficiency, we
introduce a learnable attribute that enables differentiable pruning of
Gaussians with minimal contribution during training. Quantitative and
qualitative experiments demonstrate state-of-the-art reconstruction accuracy
across all modalities, validating the efficacy of our geometry-aware paradigm.
Source code and multimodal viewer will be available on GitHub.</p>
<h3 id="13-compodistill-attention-distillation-for-compositional-reasoning-in-multimodal-llms">[13] <a href="https://arxiv.org/abs/2510.12184">CompoDistill: Attention Distillation for Compositional Reasoning in Multimodal LLMs</a></h3>
<p><em>Jiwan Kim, Kibum Kim, Sangwoo Seo, Chanyoung Park</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCompoDistillï¼Œä¸€ç§æ–°é¢–çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼å¯¹é½å¸ˆç”Ÿæ¨¡å‹çš„è§†è§‰æ³¨æ„åŠ›æ¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è§†è§‰æ„ŸçŸ¥èƒ½åŠ›è’¸é¦ä¸è¶³çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†ç»„åˆæ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çŸ¥è¯†è’¸é¦æ–¹æ³•åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­éš¾ä»¥æœ‰æ•ˆå°†æ•™å¸ˆæ¨¡å‹çš„ä¸°å¯Œè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ä¼ é€’ç»™å­¦ç”Ÿæ¨¡å‹ï¼Œè¿™ä¸€é—®é¢˜åœ¨å…ˆå‰ç ”ç©¶ä¸­è¢«å¿½è§†ï¼Œä¸»è¦åŸå› æ˜¯å¸ˆç”Ÿæ¨¡å‹ä¹‹é—´çš„è§†è§‰æ³¨æ„åŠ›é”™ä½ã€‚</p>
<p><strong>Method:</strong> æå‡ºCompoDistillçŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼å¯¹é½å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹çš„è§†è§‰æ³¨æ„åŠ›æ¥å¢å¼ºå­¦ç”Ÿçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£å†³è§†è§‰æ³¨æ„åŠ›é”™é…é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜CompoDistillåœ¨éœ€è¦è§†è§‰æ„ŸçŸ¥èƒ½åŠ›çš„ç»„åˆæ¨ç†ä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—æå‡ï¼ŒåŒæ—¶åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šä¿æŒå¼ºå¤§æ€§èƒ½ï¼Œä¸”åœ¨æ›´å…ˆè¿›éª¨å¹²ç½‘ç»œä¸Šä»ä¿æŒæœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è§†è§‰æ³¨æ„åŠ›å¯¹é½æ˜¯æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†è’¸é¦æ•ˆæœçš„å…³é”®æœºåˆ¶ï¼ŒCompoDistillæ¡†æ¶ä¸ºè§£å†³è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ä¼ é€’ä¸è¶³é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§å’Œæ¨å¹¿ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Recently, efficient Multimodal Large Language Models (MLLMs) have gained
significant attention as a solution to their high computational complexity,
making them more practical for real-world applications. In this regard, the
knowledge distillation (KD) approach has emerged as a promising alternative,
which transfers the rich visual and linguistic knowledge from a larger model
(teacher) to a smaller model (student). However, we observe that existing KD
methods struggle to effectively distill the teacher MLLM's rich visual
perception abilities to the student, a challenge that has been largely
overlooked in previous studies. Through a systematic analysis, we identify
visual attention misalignment between student and teacher as the main cause of
this issue. Based on this insight, we propose CompoDistill, a novel KD
framework that explicitly aligns the student's visual attention with that of
the teacher to enhance the student's visual perception abilities. Our extensive
experiments show that CompoDistill significantly improves performance on
compositional reasoning tasks that require visual perception abilities while
maintaining strong performance on visual question answering tasks, as done in
existing studies. Furthermore, CompoDistill demonstrates effectiveness with a
more advanced backbone, highlighting its generalizability.</p>
<h3 id="14-hierarchical-reasoning-with-vision-language-models-for-incident-reports-from-dashcam-videos">[14] <a href="https://arxiv.org/abs/2510.12190">Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos</a></h3>
<p><em>Shingo Yokoi, Kento Sasaki, Yu Yamaguchi</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºä»è¡Œè½¦è®°å½•ä»ªè§†é¢‘ç”Ÿæˆäº‹æ•…æŠ¥å‘Šçš„åˆ†å±‚æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†å¸§çº§æè¿°ã€äº‹æ•…å¸§æ£€æµ‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„ç»†ç²’åº¦æ¨ç†ï¼Œåœ¨2COOOLæŒ‘æˆ˜èµ›ä¸­æ’åç¬¬2å¹¶å–å¾—äº†æœ€ä½³CIDEr-Dåˆ†æ•°ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¨¡å‹åœ¨åˆ†å¸ƒå¤–åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼ŒCOOOLåŸºå‡†æµ‹è¯•æ—¨åœ¨è§£å†³è¿™ä¸€å·®è·ï¼Œè€Œ2COOOLæŒ‘æˆ˜èµ›è¿›ä¸€æ­¥æ‰©å±•ä¸ºç”Ÿæˆäººç±»å¯è§£é‡Šçš„äº‹æ•…æŠ¥å‘Šï¼Œä»¥æå‡å¯¹å®‰å…¨å…³é”®äº¤é€šäº‹ä»¶çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨åˆ†å±‚æ¨ç†æ¡†æ¶ï¼Œç»“åˆå¸§çº§æè¿°ã€äº‹æ•…å¸§æ£€æµ‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„ç»†ç²’åº¦æ¨ç†ï¼Œé€šè¿‡æ¨¡å‹é›†æˆå’Œç›²A/Bè¯„åˆ†é€‰æ‹©åè®®æ¥æé«˜äº‹å®å‡†ç¡®æ€§å’Œå¯è¯»æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å®˜æ–¹2COOOLå¼€æ”¾æ’è¡Œæ¦œä¸Šï¼Œè¯¥æ–¹æ³•åœ¨29ä¸ªå›¢é˜Ÿä¸­æ’åç¬¬2ï¼Œå¹¶å–å¾—äº†æœ€ä½³CIDEr-Dåˆ†æ•°ï¼Œèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®ä¸”è¿è´¯çš„äº‹æ•…å™è¿°ã€‚</p>
<p><strong>Conclusion:</strong> ç»“æœè¡¨æ˜ï¼ŒåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„åˆ†å±‚æ¨ç†æ˜¯äº‹æ•…åˆ†æå’Œå®‰å…¨å…³é”®äº¤é€šäº‹ä»¶ç†è§£çš„ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨è¯„ä¼°æä¾›äº†æœ‰æ•ˆå·¥å…·ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Recent advances in end-to-end (E2E) autonomous driving have been enabled by
training on diverse large-scale driving datasets, yet autonomous driving models
still struggle in out-of-distribution (OOD) scenarios. The COOOL benchmark
targets this gap by encouraging hazard understanding beyond closed taxonomies,
and the 2COOOL challenge extends it to generating human-interpretable incident
reports. We present a hierarchical reasoning framework for incident report
generation from dashcam videos that integrates frame-level captioning, incident
frame detection, and fine-grained reasoning within vision-language models
(VLMs). We further improve factual accuracy and readability through model
ensembling and a Blind A/B Scoring selection protocol. On the official 2COOOL
open leaderboard, our method ranks 2nd among 29 teams and achieves the best
CIDEr-D score, producing accurate and coherent incident narratives. These
results indicate that hierarchical reasoning with VLMs is a promising direction
for accident analysis and for broader understanding of safety-critical traffic
events. The implementation and code are available at
https://github.com/riron1206/kaggle-2COOOL-2nd-Place-Solution.</p>
<h3 id="15-a-text-image-fusion-method-with-data-augmentation-capabilities-for-referring-medical-image-segmentation">[15] <a href="https://arxiv.org/abs/2510.12482">A Text-Image Fusion Method with Data Augmentation Capabilities for Referring Medical Image Segmentation</a></h3>
<p><em>Shurong Chai, Rahul Kumar JAIN, Rui Xu, Shaocong Mo, Ruibo Hou, Shiyu Teng, Jiaqing Liu, Lanfen Lin, Yen-Wei Chen</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ—©æœŸèåˆæ¡†æ¶ï¼Œé€šè¿‡åœ¨æ•°æ®å¢å¼ºå‰å°†æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ç›¸ç»“åˆæ¥ä¿æŒç©ºé—´ä¸€è‡´æ€§ï¼Œå¹¶è®¾è®¡è½»é‡çº§ç”Ÿæˆå™¨å°†æ–‡æœ¬åµŒå…¥æŠ•å½±åˆ°è§†è§‰ç©ºé—´ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ–‡æœ¬å¼•å¯¼å›¾åƒåˆ†å‰²æ–¹æ³•é¢ä¸´æ•°æ®å¢å¼ºï¼ˆå¦‚æ—‹è½¬å’Œç¿»è½¬ï¼‰ç ´åå›¾åƒä¸æ–‡æœ¬ç©ºé—´å¯¹é½çš„é—®é¢˜ï¼Œè¿™å‰Šå¼±äº†æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦æˆåƒè¿™ç§æ•°æ®æœ‰é™çš„é¢†åŸŸã€‚</p>
<p><strong>Method:</strong> æå‡ºæ—©æœŸèåˆæ¡†æ¶ï¼Œåœ¨æ•°æ®å¢å¼ºé˜¶æ®µä¹‹å‰èåˆæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ä»¥ä¿æŒç©ºé—´ä¸€è‡´æ€§ï¼›è®¾è®¡è½»é‡çº§ç”Ÿæˆå™¨å°†æ–‡æœ¬åµŒå…¥æŠ•å½±åˆ°è§†è§‰ç©ºé—´ï¼Œå¼¥åˆè¯­ä¹‰é¸¿æ²Ÿï¼›é€šè¿‡ç”Ÿæˆä¼ªå›¾åƒå®ç°ç²¾ç¡®åŒºåŸŸå®šä½ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªåŒ»å­¦æˆåƒä»»åŠ¡å’Œå››ä¸ªåˆ†å‰²æ¡†æ¶ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å¯è§†åŒ–ç”Ÿæˆçš„ä¼ªå›¾åƒæ˜¾ç¤ºèƒ½å¤Ÿå‡†ç¡®å®šä½ç›®æ ‡åŒºåŸŸã€‚</p>
<p><strong>Conclusion:</strong> æ—©æœŸèåˆæ–¹æ³•æœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€åˆ†å‰²ä¸­æ•°æ®å¢å¼ºç ´åç©ºé—´å¯¹é½çš„é—®é¢˜ï¼›æ–‡æœ¬åˆ°è§†è§‰ç©ºé—´çš„æŠ•å½±ç­–ç•¥ä¸ºè·¨æ¨¡æ€å­¦ä¹ æä¾›äº†æ–°æ€è·¯ï¼›è¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Deep learning relies heavily on data augmentation to mitigate limited data,
especially in medical imaging. Recent multimodal learning integrates text and
images for segmentation, known as referring or text-guided image segmentation.
However, common augmentations like rotation and flipping disrupt spatial
alignment between image and text, weakening performance. To address this, we
propose an early fusion framework that combines text and visual features before
augmentation, preserving spatial consistency. We also design a lightweight
generator that projects text embeddings into visual space, bridging semantic
gaps. Visualization of generated pseudo-images shows accurate region
localization. Our method is evaluated on three medical imaging tasks and four
segmentation frameworks, achieving state-of-the-art results. Code is publicly
available on GitHub: https://github.com/11yxk/MedSeg_EarlyFusion.</p>
<h3 id="16-honeybee-data-recipes-for-vision-language-reasoners">[16] <a href="https://arxiv.org/abs/2510.12225">HoneyBee: Data Recipes for Vision-Language Reasoners</a></h3>
<p><em>Hritik Bansal, Devandra Singh Sachan, Kai-Wei Chang, Aditya Grover, Gargi Ghosh, Wen-tau Yih, Ramakanth Pasunuru</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿåˆ†æäº†è§†è§‰è¯­è¨€æ¨ç†è®­ç»ƒæ•°æ®é›†çš„æ„å»ºåŸåˆ™ï¼Œæå‡ºäº†å¤šç§æ•°æ®ç­›é€‰æ–¹æ³•ï¼Œå¹¶æ„å»ºäº†å¤§è§„æ¨¡é«˜è´¨é‡æ¨ç†æ•°æ®é›†HoneyBeeï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†æ„å»ºé«˜æ€§èƒ½è§†è§‰è¯­è¨€æ¨ç†è®­ç»ƒæ•°æ®é›†çš„åŸºæœ¬åŸåˆ™ä»ç„¶ç¼ºä¹æ·±å…¥ç†è§£ï¼Œç°æœ‰æ–¹æ³•åœ¨æ•°æ®æºé€‰æ‹©ã€å¹²é¢„ç­–ç•¥å’Œè§„æ¨¡æ‰©å±•ç­‰æ–¹é¢ç¼ºä¹ç³»ç»Ÿæ€§ç ”ç©¶ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†å¤šç§æ•°æ®ç­›é€‰æ–¹æ³•ï¼ŒåŒ…æ‹¬åˆ†æä¸Šä¸‹æ–‡ï¼ˆå›¾åƒå’Œé—®é¢˜å¯¹ï¼‰æ¥æºçš„å½±å“ã€å®æ–½é’ˆå¯¹æ€§æ•°æ®å¹²é¢„ï¼ˆå¦‚å›¾åƒæè¿°è¾…åŠ©ä¿¡å·å’Œçº¯æ–‡æœ¬æ¨ç†ï¼‰ã€ä»¥åŠç³»ç»Ÿæ‰©å±•å›¾åƒã€é—®é¢˜å’Œæ€ç»´é“¾è§£å†³æ–¹æ¡ˆçš„è§„æ¨¡ï¼Œå¹¶æ„å»ºäº†åŒ…å«250ä¸‡æ ·æœ¬çš„å¤§è§„æ¨¡é«˜è´¨é‡æ¨ç†æ•°æ®é›†HoneyBeeã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ä¸Šä¸‹æ–‡æ¥æºç­–ç•¥æ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ï¼Œå›¾åƒæè¿°è¾…åŠ©ä¿¡å·å’Œçº¯æ–‡æœ¬æ¨ç†å¹²é¢„å¸¦æ¥æ˜¾è‘—æå‡ï¼Œå¤šç»´åº¦æ•°æ®æ‰©å±•æŒç»­æ”¹å–„æ¨ç†èƒ½åŠ›ï¼ŒHoneyBeeè®­ç»ƒçš„3Bå‚æ•°æ¨¡å‹åœ¨MathVerseåŸºå‡†ä¸Šåˆ†åˆ«è¶…è¶Šæœ€å…ˆè¿›æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹7.8%å’Œ24.8%ï¼ŒåŒæ—¶æå‡ºçš„æµ‹è¯•æ—¶æ‰©å±•ç­–ç•¥å°†è§£ç æˆæœ¬é™ä½73%è€Œä¸æŸå¤±ç²¾åº¦ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶ä¸ºè§†è§‰è¯­è¨€æ¨ç†æ•°æ®é›†æ„å»ºæä¾›äº†æ”¹è¿›ç­–ç•¥ï¼Œè¯æ˜äº†æ•°æ®è´¨é‡ã€å¹²é¢„æ–¹æ³•å’Œè§„æ¨¡æ‰©å±•å¯¹æ¨¡å‹æ€§èƒ½çš„å…³é”®ä½œç”¨ï¼Œæå‡ºçš„æµ‹è¯•æ—¶ä¼˜åŒ–æ–¹æ³•ä¸ºå®é™…éƒ¨ç½²æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨ç†ç ”ç©¶çš„ç³»ç»ŸåŒ–å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Recent advances in vision-language models (VLMs) have made them highly
effective at reasoning tasks. However, the principles underlying the
construction of performant VL reasoning training datasets remain poorly
understood. In this work, we introduce several data curation approaches and
study their impacts on VL reasoning capabilities by carefully controlling
training and evaluation setups. We analyze the effects of context (image and
question pair) sources, implement targeted data interventions, and explore
scaling up images, questions, and chain-of-thought (CoT) solutions. Our
findings reveal that (a) context source strategies significantly affect VLM
performance, (b) interventions such as auxiliary signals from image captions
and the inclusion of text-only reasoning yield substantial gains, and (c)
scaling all data dimensions (e.g., unique questions per image and unique CoTs
per image-question pair) consistently improves reasoning capability. Motivated
by these insights, we introduce HoneyBee, a large-scale, high-quality CoT
reasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs
trained with HoneyBee outperform state-of-the-art models across model sizes.
For instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA
model and the base model by 7.8% and 24.8%, respectively, on MathVerse.
Furthermore, we propose a test-time scaling strategy that reduces decoding cost
by 73% without sacrificing accuracy. Overall, this work presents improved
strategies for VL reasoning dataset curation research.</p>
<h3 id="17-ivan-istd-rethinking-cross-domain-heteroscedastic-noise-perturbations-in-infrared-small-target-detection">[17] <a href="https://arxiv.org/abs/2510.12241">Ivan-ISTD: Rethinking Cross-domain Heteroscedastic Noise Perturbations in Infrared Small Target Detection</a></h3>
<p><em>Yuehui Li, Yahao Lu, Haoyuan Wu, Sen Zhang, Liang Lin, Yukai Shi</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒé‡å°æ³¢å¼•å¯¼çš„ä¸å˜æ€§å­¦ä¹ æ¡†æ¶Ivan-ISTDï¼Œé€šè¿‡å°æ³¢å¼•å¯¼çš„è·¨åŸŸåˆæˆå’ŒçœŸå®åŸŸå™ªå£°ä¸å˜æ€§å­¦ä¹ ï¼Œè§£å†³äº†çº¢å¤–å°ç›®æ ‡æ£€æµ‹ä¸­çš„è·¨åŸŸåç§»å’Œå¼‚æ–¹å·®å™ªå£°æ‰°åŠ¨é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³çº¢å¤–å°ç›®æ ‡æ£€æµ‹ä¸­å­˜åœ¨çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šè·¨åŸŸåç§»å’Œå¼‚æ–¹å·®å™ªå£°æ‰°åŠ¨ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†çœŸå®ä¸–ç•Œåº”ç”¨ä¸­é‡åˆ°çš„åˆ†ä½ˆåç§»å’Œå™ªå£°ç‰¹æ€§å˜åŒ–æ—¶å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— äººæœºå¤šæ¨¡æ€æ„ŸçŸ¥åœºæ™¯ä¸‹ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„Ivan-ISTDæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé‡‡ç”¨å°æ³¢å¼•å¯¼çš„è·¨åŸŸåˆæˆï¼Œé€šè¿‡å¤šé¢‘å°æ³¢æ»¤æ³¢ç²¾ç¡®åˆ†ç¦»ç›®æ ‡èƒŒæ™¯ï¼›ç¬¬äºŒé˜¶æ®µå¼•å…¥çœŸå®åŸŸå™ªå£°ä¸å˜æ€§å­¦ä¹ ï¼Œä»ç›®æ ‡åŸŸæå–çœŸå®å™ªå£°ç‰¹å¾æ„å»ºåŠ¨æ€å™ªå£°åº“ï¼Œå¹¶é€šè¿‡è‡ªç›‘ç£æŸå¤±å­¦ä¹ å™ªå£°ä¸å˜æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå®šé‡æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨åŸŸåœºæ™¯ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„é²æ£’æ€§ã€‚ç ”ç©¶è¿˜åˆ›å»ºäº†Dynamic-ISTDåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºæ¨¡æ‹ŸçœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„åˆ†å¸ƒåç§»ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å°æ³¢å¼•å¯¼çš„è·¨åŸŸå¯¹é½å’ŒçœŸå®å™ªå£°ä¸å˜æ€§å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œä¸ºçº¢å¤–å°ç›®æ ‡æ£€æµ‹æä¾›äº†æ›´é²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚æ–¹æ³•å±•ç¤ºäº†åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºè·¨åŸŸè§†è§‰ä»»åŠ¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>In the multimedia domain, Infrared Small Target Detection (ISTD) plays a
important role in drone-based multi-modality sensing. To address the dual
challenges of cross-domain shift and heteroscedastic noise perturbations in
ISTD, we propose a doubly wavelet-guided Invariance learning
framework(Ivan-ISTD). In the first stage, we generate training samples aligned
with the target domain using Wavelet-guided Cross-domain Synthesis. This
wavelet-guided alignment machine accurately separates the target background
through multi-frequency wavelet filtering. In the second stage, we introduce
Real-domain Noise Invariance Learning, which extracts real noise
characteristics from the target domain to build a dynamic noise library. The
model learns noise invariance through self-supervised loss, thereby overcoming
the limitations of distribution bias in traditional artificial noise modeling.
Finally, we create the Dynamic-ISTD Benchmark, a cross-domain dynamic
degradation dataset that simulates the distribution shifts encountered in
real-world applications. Additionally, we validate the versatility of our
method using other real-world datasets. Experimental results demonstrate that
our approach outperforms existing state-of-the-art methods in terms of many
quantitative metrics. In particular, Ivan-ISTD demonstrates excellent
robustness in cross-domain scenarios. The code for this work can be found at:
https://github.com/nanjin1/Ivan-ISTD.</p>
<h3 id="18-beyond-seeing-evaluating-multimodal-llms-on-tool-enabled-image-perception-transformation-and-reasoning">[18] <a href="https://arxiv.org/abs/2510.12712">Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning</a></h3>
<p><em>Xingang Guo, Utkarsh Tyagi, Advait Gosai, Paula Vergara, Ernesto Gabriel HernÃ¡ndez Montoya, Chen Bo Calvin Zhang, Bin Hu, Yunzhong He, Bing Liu, Rakshith Sharma Srinivasa</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†IRISåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“æ³¨äº'ä¸å›¾åƒä¸€èµ·æ€è€ƒ'èŒƒå¼çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è§†è§‰-æ–‡æœ¬ä»»åŠ¡ä¸­æ„ŸçŸ¥ã€è½¬æ¢å’Œæ¨ç†çš„èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸»è¦é‡‡ç”¨'å…³äºå›¾åƒæ€è€ƒ'çš„èŒƒå¼ï¼Œå°†å›¾åƒè§†ä¸ºé™æ€è¾“å…¥ï¼Œè€Œå¿½è§†äº†çœŸå®åœºæ™¯ä¸­ç”¨æˆ·æä¾›çš„å›¾åƒå¾€å¾€ä¸å®Œç¾ï¼Œéœ€è¦ä¸»åŠ¨è¿›è¡Œè£å‰ªã€ç¼–è¾‘æˆ–å¢å¼ºç­‰æ“ä½œæ¥æå–å…³é”®è§†è§‰çº¿ç´¢ã€‚ä»å°†è§†è§‰è§†ä¸ºè¢«åŠ¨ä¸Šä¸‹æ–‡åˆ°å¯æ“ä½œè®¤çŸ¥å·¥ä½œç©ºé—´çš„è½¬å˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†IRISåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1,204ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾å¼è§†è§‰ä»»åŠ¡ï¼Œæ¶µç›–603ä¸ªå•è½®å¯¹è¯å’Œ601ä¸ªå¤šè½®å¯¹è¯ä»»åŠ¡ï¼Œåˆ†å¸ƒåœ¨äº”ä¸ªä¸åŒé¢†åŸŸã€‚æ¯ä¸ªä»»åŠ¡éƒ½é…æœ‰è¯¦ç»†çš„è¯„åˆ†æ ‡å‡†ï¼Œæ”¯æŒå¯¹æ¨¡å‹åœ¨'ä¸å›¾åƒä¸€èµ·æ€è€ƒ'èŒƒå¼ä¸‹çš„ç³»ç»Ÿè¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°ç»“æœæ˜¾ç¤ºå½“å‰MLLMsåœ¨éœ€è¦æœ‰æ•ˆæ•´åˆè§†è§‰å’Œé€šç”¨å·¥å…·çš„å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå³ä½¿æœ€å¼ºçš„GPT-5-thinkæ¨¡å‹ä¹Ÿä»…è¾¾åˆ°18.68%çš„é€šè¿‡ç‡ã€‚ç ”ç©¶è¿˜è§‚å¯Ÿåˆ°ä¸åŒçš„å·¥å…·ä½¿ç”¨è¡Œä¸ºæ¨¡å¼ï¼ŒOpenAIæ¨¡å‹èƒ½ä»å¤šæ ·åŒ–çš„å›¾åƒæ“ä½œä¸­å—ç›Šï¼Œè€ŒGemini-2.5-proåˆ™æœªè§æ”¹å–„ã€‚</p>
<p><strong>Conclusion:</strong> IRISåŸºå‡†æµ‹è¯•ä¸ºæ¨è¿›MLLMsçš„è§†è§‰æ™ºèƒ½æä¾›äº†å…³é”®è§è§£ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨åŠ¨æ€è§†è§‰æ¨ç†å’Œå·¥å…·æ•´åˆæ–¹é¢çš„å±€é™æ€§ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ä»é™æ€è§†è§‰æ„ŸçŸ¥å‘äº¤äº’å¼è§†è§‰è®¤çŸ¥è½¬å˜çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿçš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) are increasingly applied in
real-world scenarios where user-provided images are often imperfect, requiring
active image manipulations such as cropping, editing, or enhancement to uncover
salient visual cues. Beyond static visual perception, MLLMs must also think
with images: dynamically transforming visual content and integrating it with
other tools to solve complex tasks. However, this shift from treating vision as
passive context to a manipulable cognitive workspace remains underexplored.
Most existing benchmarks still follow a think about images paradigm, where
images are regarded as static inputs. To address this gap, we introduce IRIS,
an Interactive Reasoning with Images and Systems that evaluates MLLMs' ability
to perceive, transform, and reason across complex visual-textual tasks under
the think with images paradigm. IRIS comprises 1,204 challenging, open-ended
vision tasks (603 single-turn, 601 multi-turn) spanning across five diverse
domains, each paired with detailed rubrics to enable systematic evaluation. Our
evaluation shows that current MLLMs struggle with tasks requiring effective
integration of vision and general-purpose tools. Even the strongest model
(GPT-5-think) reaches only 18.68% pass rate. We further observe divergent
tool-use behaviors, with OpenAI models benefiting from diverse image
manipulations while Gemini-2.5-pro shows no improvement. By introducing the
first benchmark centered on think with images, IRIS offers critical insights
for advancing visual intelligence in MLLMs.</p>
<h3 id="19-vectorized-video-representation-with-easy-editing-via-hierarchical-spatio-temporally-consistent-proxy-embedding">[19] <a href="https://arxiv.org/abs/2510.12256">Vectorized Video Representation with Easy Editing via Hierarchical Spatio-Temporally Consistent Proxy Embedding</a></h3>
<p><em>Ye Chen, Liming Tan, Yupeng Zhu, Yuanbin Wang, Bingbing Ni</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ—¶ç©ºä¸€è‡´ä»£ç†èŠ‚ç‚¹çš„è§†é¢‘è¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡åˆ†å±‚ä»£ç†èŠ‚ç‚¹ç¨³å®šè¡¨è¾¾è§†è§‰å¯¹è±¡çš„å¤šå°ºåº¦ç»“æ„ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿåƒç´ çº§åŒ¹é…å’Œè·Ÿè¸ªæ–¹æ³•å¯¹è·Ÿè¸ªè¯¯å·®ã€é®æŒ¡å’Œå¤§è¿åŠ¨çš„è„†å¼±æ€§é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘è¡¨ç¤ºæ–¹æ³•ä¸¥é‡ä¾èµ–ä¸ç¨³å®šä¸”è¿‡äºç»†ç²’åº¦çš„è¿åŠ¨å’Œå¤–è§‚å»ºæ¨¡å…ˆéªŒï¼Œå¦‚åƒç´ çº§åŒ¹é…å’Œè·Ÿè¸ªï¼Œå¯¼è‡´å¯¹è·Ÿè¸ªè¯¯å·®ã€é®æŒ¡å’Œå¤§è¿åŠ¨ç­‰åœºæ™¯æåº¦è„†å¼±ï¼Œå‡ ä¸ªåƒç´ çš„è·Ÿè¸ªè¯¯å·®å°±å¯èƒ½å¯¼è‡´è§†è§‰å¯¹è±¡è¡¨ç¤ºçš„å´©æºƒã€‚</p>
<p><strong>Method:</strong> æå‡ºä½¿ç”¨æ—¶ç©ºä¸€è‡´çš„ä»£ç†èŠ‚ç‚¹æ¥è¡¨ç¤ºè§†é¢‘ä¸­åŠ¨æ€å˜åŒ–çš„ç‰©ä½“/åœºæ™¯ï¼Œåˆ†å±‚ä»£ç†èŠ‚ç‚¹èƒ½å¤Ÿç¨³å®šè¡¨è¾¾è§†è§‰å¯¹è±¡çš„å¤šå°ºåº¦ç»“æ„ï¼Œä¸å—ç´¯ç§¯è·Ÿè¸ªè¯¯å·®ã€é•¿æœŸè¿åŠ¨ã€é®æŒ¡å’Œè§†è§’å˜åŒ–çš„å½±å“ï¼ŒåŒæ—¶é€šè¿‡åŠ¨æ€è¡¨ç¤ºæ›´æ–°æœºåˆ¶åˆ©ç”¨è§†é¢‘çš„æ—¶ç©ºå…ˆéªŒæ¥å‡è½»ä¸å‡†ç¡®è·Ÿè¸ªå™¨çš„å½±å“ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è¡¨ç¤ºæ–¹æ³•ä»¥æ›´å°‘çš„å‚æ•°å®ç°äº†é«˜ç²¾åº¦çš„è§†é¢‘é‡å»ºï¼Œå¹¶æ”¯æŒå¤æ‚çš„è§†é¢‘å¤„ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†é¢‘ä¿®å¤å’ŒåŸºäºå…³é”®å¸§çš„æ—¶é—´ä¸€è‡´æ€§è§†é¢‘ç¼–è¾‘ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•é€šè¿‡è§£è€¦å½¢çŠ¶å’Œçº¹ç†è¡¨ç¤ºçš„æ–¹å¼ï¼Œå®ç°äº†å¯¹è§†é¢‘ä¸­ä¸åŒè§†è§‰å¯¹è±¡çš„å¯æ§å’Œç»†ç²’åº¦å¤–è§‚ç¼–è¾‘èƒ½åŠ›ï¼Œä¸ºè§†é¢‘è¡¨ç¤ºå’Œå¤„ç†æä¾›äº†æ›´é²æ£’å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Current video representations heavily rely on unstable and over-grained
priors for motion and appearance modelling, \emph{i.e.}, pixel-level matching
and tracking. A tracking error of just a few pixels would lead to the collapse
of the visual object representation, not to mention occlusions and large motion
frequently occurring in videos. To overcome the above mentioned vulnerability,
this work proposes spatio-temporally consistent proxy nodes to represent
dynamically changing objects/scenes in the video. On the one hand, the
hierarchical proxy nodes have the ability to stably express the multi-scale
structure of visual objects, so they are not affected by accumulated tracking
error, long-term motion, occlusion, and viewpoint variation. On the other hand,
the dynamic representation update mechanism of the proxy nodes adequately
leverages spatio-temporal priors of the video to mitigate the impact of
inaccurate trackers, thereby effectively handling drastic changes in scenes and
objects. Additionally, the decoupled encoding manner of the shape and texture
representations across different visual objects in the video facilitates
controllable and fine-grained appearance editing capability. Extensive
experiments demonstrate that the proposed representation achieves high video
reconstruction accuracy with fewer parameters and supports complex video
processing tasks, including video in-painting and keyframe-based temporally
consistent video editing.</p>
<h3 id="20-vqart-bench-a-semantically-rich-vqa-benchmark-for-art-and-cultural-heritage">[20] <a href="https://arxiv.org/abs/2510.12750">VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural Heritage</a></h3>
<p><em>A. Alfarano, L. Venturoli, D. Negueruela del Castillo</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VQArt-Benchï¼Œä¸€ä¸ªç”¨äºæ–‡åŒ–é—äº§é¢†åŸŸçš„è§†è§‰é—®ç­”åŸºå‡†ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“æµæ°´çº¿ç”Ÿæˆå…·æœ‰æ·±åº¦è¯­ä¹‰ç†è§£çš„é—®é¢˜ï¼Œæ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„æ˜¾è‘—å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰é—®ç­”åŸºå‡†åœ¨è¯„ä¼°æ·±åº¦è¯­ä¹‰ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰è‰ºæœ¯åˆ†æç­‰å¤æ‚é¢†åŸŸï¼Œè¿™äº›é—®é¢˜å±€é™äºç®€å•å¥æ³•ç»“æ„å’Œè¡¨é¢å±æ€§ï¼Œæ— æ³•æ•æ‰äººç±»è§†è§‰æ¢ç©¶çš„å¤šæ ·æ€§å’Œæ·±åº¦ï¼Œå¯¼è‡´æ¨¡å‹å€¾å‘äºåˆ©ç”¨ç»Ÿè®¡æ·å¾„è€Œéè¿›è¡ŒçœŸæ­£çš„è§†è§‰æ¨ç†ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨æ–°é¢–çš„å¤šæ™ºèƒ½ä½“æµæ°´çº¿ï¼Œå…¶ä¸­ä¸“é—¨è®¾è®¡çš„æ™ºèƒ½ä½“åä½œç”Ÿæˆç»è¿‡éªŒè¯ä¸”è¯­è¨€å¤šæ ·åŒ–çš„ç»†è‡´é—®é¢˜ï¼Œæ„å»ºçš„åŸºå‡†ç»“æ„æ²¿ç€ç›¸å…³çš„è§†è§‰ç†è§£ç»´åº¦ï¼Œæ¢ç´¢æ¨¡å‹è§£é‡Šç¬¦å·æ„ä¹‰ã€å™äº‹å’Œå¤æ‚è§†è§‰å…³ç³»çš„èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å¯¹14ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹çš„æ˜¾è‘—å±€é™æ€§ï¼ŒåŒ…æ‹¬åœ¨ç®€å•è®¡æ•°ä»»åŠ¡ä¸­çš„æ„å¤–å¼±ç‚¹ï¼Œä»¥åŠä¸“æœ‰æ¨¡å‹ä¸å¼€æºæ¨¡å‹ä¹‹é—´æ˜æ˜¾çš„æ€§èƒ½å·®è·ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘èƒ½å¤Ÿå¤„ç†å¤æ‚è§†è§‰è¯­ä¹‰ç†è§£ä»»åŠ¡çš„æ›´å¼ºå¤§æ¨¡å‹çš„å¿…è¦æ€§ï¼ŒåŒæ—¶ä¸ºæ–‡åŒ–é—äº§é¢†åŸŸçš„å¤šæ¨¡æ€ç†è§£æä¾›äº†æ–°çš„è¯„ä¼°æ ‡å‡†å’Œæ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in joint visual and linguistic tasks. However, existing Visual
Question Answering (VQA) benchmarks often fail to evaluate deep semantic
understanding, particularly in complex domains like visual art analysis.
Confined to simple syntactic structures and surface-level attributes, these
questions fail to capture the diversity and depth of human visual inquiry. This
limitation incentivizes models to exploit statistical shortcuts rather than
engage in visual reasoning. To address this gap, we introduce VQArt-Bench, a
new, large-scale VQA benchmark for the cultural heritage domain. This benchmark
is constructed using a novel multi-agent pipeline where specialized agents
collaborate to generate nuanced, validated, and linguistically diverse
questions. The resulting benchmark is structured along relevant visual
understanding dimensions that probe a model's ability to interpret symbolic
meaning, narratives, and complex visual relationships. Our evaluation of 14
state-of-the-art MLLMs on this benchmark reveals significant limitations in
current models, including a surprising weakness in simple counting tasks and a
clear performance gap between proprietary and open-source models.</p>
<h3 id="21-angularfuse-a-closer-look-at-angle-based-perception-for-spatial-sensitive-multi-modality-image-fusion">[21] <a href="https://arxiv.org/abs/2510.12260">AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion</a></h3>
<p><em>Xiaopeng Liu, Yupei Lin, Sen Zhang, Xiao Wang, Yukai Shi, Liang Lin</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§’åº¦æ„ŸçŸ¥çš„å¯è§å…‰-çº¢å¤–å›¾åƒèåˆæ¡†æ¶AngularFuseï¼Œé€šè¿‡è·¨æ¨¡æ€äº’è¡¥æ©ç æ¨¡å—ã€ç²¾ç»†åŒ–å‚è€ƒå›¾åƒåˆæˆç­–ç•¥å’Œè§’åº¦æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œè§£å†³äº†ç°æœ‰æ— ç›‘ç£èåˆæ–¹æ³•åœ¨ç»†èŠ‚ä¿ç•™å’Œäº®åº¦å¹³è¡¡æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¯è§å…‰-çº¢å¤–å›¾åƒèåˆæ–¹æ³•ä¸»è¦ä¾èµ–æ‰‹å·¥è®¾è®¡çš„æŸå¤±å‡½æ•°ï¼Œå­˜åœ¨æ˜æ˜¾å±€é™æ€§ï¼šä¸€æ–¹é¢æ„å»ºçš„å‚è€ƒå›¾åƒç¼ºä¹ç»†èŠ‚ä¸”äº®åº¦ä¸å‡ï¼Œå¦ä¸€æ–¹é¢å¹¿æ³›ä½¿ç”¨çš„æ¢¯åº¦æŸå¤±ä»…å…³æ³¨æ¢¯åº¦å¹…å€¼è€Œå¿½ç•¥æ–¹å‘ä¿¡æ¯ï¼Œå¯¼è‡´èåˆç»“æœåœ¨çº¹ç†å¼ºåº¦å’Œè¾¹ç¼˜æ–¹å‘ä¿æŒæ–¹é¢è¡¨ç°ä¸ä½³ã€‚</p>
<p><strong>Method:</strong> æå‡ºAngularFuseæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè·¨æ¨¡æ€äº’è¡¥æ©ç æ¨¡å—å¼ºåˆ¶ç½‘ç»œå­¦ä¹ æ¨¡æ€é—´çš„äº’è¡¥ä¿¡æ¯ï¼›ç²¾ç»†åŒ–å‚è€ƒå›¾åƒåˆæˆç­–ç•¥ç»“åˆæ‹‰æ™®æ‹‰æ–¯è¾¹ç¼˜å¢å¼ºå’Œè‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–ç”Ÿæˆç»†èŠ‚æ›´ä¸°å¯Œã€äº®åº¦æ›´å¹³è¡¡çš„å‚è€ƒå›¾åƒï¼›è§’åº¦æ„ŸçŸ¥æŸå¤±å‡½æ•°é¦–æ¬¡åœ¨æ¢¯åº¦åŸŸåŒæ—¶çº¦æŸæ¢¯åº¦å¹…å€¼å’Œæ–¹å‘ï¼Œç¡®ä¿èåˆå›¾åƒä¿æŒçº¹ç†å¼ºåº¦å’Œæ­£ç¡®è¾¹ç¼˜æ–¹å‘ã€‚</p>
<p><strong>Result:</strong> åœ¨MSRSã€RoadSceneå’ŒM3FDä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒAngularFuseæ˜æ˜¾ä¼˜äºç°æœ‰ä¸»æµæ–¹æ³•ï¼Œè§†è§‰å¯¹æ¯”è¿›ä¸€æ­¥è¯å®è¯¥æ–¹æ³•åœ¨æŒ‘æˆ˜æ€§åœºæ™¯ä¸­èƒ½äº§ç”Ÿæ›´æ¸…æ™°ã€ç»†èŠ‚æ›´ä¸°å¯Œçš„èåˆç»“æœï¼Œå±•ç°å‡ºå“è¶Šçš„èåˆèƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜åŒæ—¶è€ƒè™‘æ¢¯åº¦å¹…å€¼å’Œæ–¹å‘çš„è§’åº¦æ„ŸçŸ¥æŸå¤±èƒ½æ˜¾è‘—æå‡å›¾åƒèåˆè´¨é‡ï¼Œè·¨æ¨¡æ€äº’è¡¥å­¦ä¹ å’Œç²¾ç»†åŒ–å‚è€ƒå›¾åƒæ„å»ºç­–ç•¥ä¸ºå¤šæ¨¡æ€å›¾åƒèåˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶å’Œå¤œé—´ç›‘æ§ç­‰å…³é”®åº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Visible-infrared image fusion is crucial in key applications such as
autonomous driving and nighttime surveillance. Its main goal is to integrate
multimodal information to produce enhanced images that are better suited for
downstream tasks. Although deep learning based fusion methods have made
significant progress, mainstream unsupervised approaches still face serious
challenges in practical applications. Existing methods mostly rely on manually
designed loss functions to guide the fusion process. However, these loss
functions have obvious limitations. On one hand, the reference images
constructed by existing methods often lack details and have uneven brightness.
On the other hand, the widely used gradient losses focus only on gradient
magnitude. To address these challenges, this paper proposes an angle-based
perception framework for spatial-sensitive image fusion (AngularFuse). At
first, we design a cross-modal complementary mask module to force the network
to learn complementary information between modalities. Then, a fine-grained
reference image synthesis strategy is introduced. By combining Laplacian edge
enhancement with adaptive histogram equalization, reference images with richer
details and more balanced brightness are generated. Last but not least, we
introduce an angle-aware loss, which for the first time constrains both
gradient magnitude and direction simultaneously in the gradient domain.
AngularFuse ensures that the fused images preserve both texture intensity and
correct edge orientation. Comprehensive experiments on the MSRS, RoadScene, and
M3FD public datasets show that AngularFuse outperforms existing mainstream
methods with clear margin. Visual comparisons further confirm that our method
produces sharper and more detailed results in challenging scenes, demonstrating
superior fusion capability.</p>
<h3 id="22-unifusion-vision-language-model-as-unified-encoder-in-image-generation">[22] <a href="https://arxiv.org/abs/2510.12789">UniFusion: Vision-Language Model as Unified Encoder in Image Generation</a></h3>
<p><em>Kevin Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, Ajinkya Kale</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>UniFusionæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡å†»ç»“çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºç»Ÿä¸€å¤šæ¨¡æ€ç¼–ç å™¨ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨åˆ†ç¦»çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å±‚æ³¨æ„åŠ›æ± åŒ–æœºåˆ¶å’ŒVLMå¯ç”¨çš„é‡å†™æ³¨å…¥æŠ€æœ¯ï¼Œå®ç°äº†è·¨æ¨¡æ€æ¨ç†å’ŒçŸ¥è¯†è¿ç§»ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰ç”Ÿæˆæ¶æ„å¤§å¤šä¾èµ–ç‹¬ç«‹çš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œè¿™ç§åˆ†ç¦»é™åˆ¶äº†æ‰©æ•£æ¨¡å‹æ‰§è¡Œè·¨æ¨¡æ€æ¨ç†å’ŒçŸ¥è¯†è¿ç§»çš„èƒ½åŠ›ã€‚å…ˆå‰å°è¯•å¼¥åˆè¿™ä¸€å·®è·çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨VLMçš„æœ€åä¸€å±‚ä¿¡æ¯ã€é‡‡ç”¨å¤šä¸ªè§†è§‰ç¼–ç å™¨ï¼Œæˆ–è”åˆè®­ç»ƒå¤§å‹ç»Ÿä¸€æ¨¡å‹è¿›è¡Œæ–‡æœ¬å’Œå›¾åƒç”Ÿæˆï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œå¤§è§„æ¨¡æ•°æ®ï¼Œé™åˆ¶äº†å…¶å¯è®¿é—®æ€§ã€‚</p>
<p><strong>Method:</strong> UniFusionçš„æ ¸å¿ƒæ˜¯å±‚æ³¨æ„åŠ›æ± åŒ–æœºåˆ¶ï¼Œä»å†»ç»“VLMçš„æ–‡æœ¬å’Œè§†è§‰æ ‡è®°ä¸­æå–é«˜çº§è¯­ä¹‰å’Œä½çº§ç»†èŠ‚æ¥æ¡ä»¶åŒ–æ‰©æ•£ç”Ÿæˆæ¨¡å‹ã€‚åŒæ—¶æå‡ºäº†VLMå¯ç”¨çš„é‡å†™æ³¨å…¥ä¸çµæ´»æ¨ç†æŠ€æœ¯ï¼Œä»…åœ¨VLMè¿›è¡Œæ¨¡å‹å†…æç¤ºé‡å†™æ—¶å¯¹æ‰©æ•£å˜æ¢å™¨è¿›è¡Œæ–‡æœ¬æ ‡è®°æ¡ä»¶åŒ–ã€‚</p>
<p><strong>Result:</strong> å±‚æ³¨æ„åŠ›æ± åŒ–æœºåˆ¶åœ¨æ–‡æœ¬å›¾åƒå¯¹é½ç”Ÿæˆå’Œè§†è§‰ä¿¡æ¯å¿ å®ä¼ è¾“æ–¹é¢ä¼˜äºå…¶ä»–æµ…å±‚èåˆæ¶æ„ã€‚åœ¨ç¼–è¾‘ä»»åŠ¡ä¸Šçš„å¾®è°ƒä¸ä»…æé«˜äº†ç”Ÿæˆçš„æ–‡æœ¬å›¾åƒå¯¹é½ï¼Œæ˜¾ç¤ºå‡ºè·¨æ¨¡æ€çŸ¥è¯†è¿ç§»ï¼Œè¿˜å±•ç°å‡ºå·¨å¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å•å›¾åƒç¼–è¾‘è®­ç»ƒåèƒ½å¤Ÿé›¶æ ·æœ¬æ³›åŒ–åˆ°å¤šå›¾åƒå‚è€ƒã€‚</p>
<p><strong>Conclusion:</strong> UniFusionçš„ç»Ÿä¸€ç¼–ç å™¨è®¾è®¡è¯æ˜äº†è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡åˆ©ç”¨å†»ç»“VLMä½œä¸ºç»Ÿä¸€å¤šæ¨¡æ€ç¼–ç å™¨ï¼Œå®ç°äº†é«˜æ•ˆçš„è·¨æ¨¡æ€æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ºæ„å»ºæ›´é«˜æ•ˆçš„ç”Ÿæˆæ¨¡å‹æä¾›äº†æ–°æ€è·¯ï¼Œå±•ç¤ºäº†ç»Ÿä¸€ç¼–ç å™¨è®¾è®¡åœ¨æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œæ¨ç†çµæ´»æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Although recent advances in visual generation have been remarkable, most
existing architectures still depend on distinct encoders for images and text.
This separation constrains diffusion models' ability to perform cross-modal
reasoning and knowledge transfer. Prior attempts to bridge this gap often use
the last layer information from VLM, employ multiple visual encoders, or train
large unified models jointly for text and image generation, which demands
substantial computational resources and large-scale data, limiting its
accessibility.We present UniFusion, a diffusion-based generative model
conditioned on a frozen large vision-language model (VLM) that serves as a
unified multimodal encoder. At the core of UniFusion is the Layerwise Attention
Pooling (LAP) mechanism that extracts both high level semantics and low level
details from text and visual tokens of a frozen VLM to condition a diffusion
generative model. We demonstrate that LAP outperforms other shallow fusion
architectures on text-image alignment for generation and faithful transfer of
visual information from VLM to the diffusion model which is key for editing. We
propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),
which conditions a diffusion transformer (DiT) only on the text tokens
generated by the VLM during in-model prompt rewriting. VERIFI combines the
alignment of the conditioning distribution with the VLM's reasoning
capabilities for increased capabilities and flexibility at inference. In
addition, finetuning on editing task not only improves text-image alignment for
generation, indicative of cross-modality knowledge transfer, but also exhibits
tremendous generalization capabilities. Our model when trained on single image
editing, zero-shot generalizes to multiple image references further motivating
the unified encoder design of UniFusion.</p>
<h3 id="23-spinebench-benchmarking-multimodal-llms-for-spinal-pathology-analysis">[23] <a href="https://arxiv.org/abs/2510.12267">SpineBench: Benchmarking Multimodal LLMs for Spinal Pathology Analysis</a></h3>
<p><em>Chenghanyu Zhang, Zekun Li, Peipei Li, Xing Cui, Shuhan Xia, Weixiang Yan, Yiqiao Zhang, Qianyu Zhuang</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SpineBenchï¼Œä¸€ä¸ªä¸“é—¨é’ˆå¯¹è„ŠæŸ±é¢†åŸŸçš„è§†è§‰é—®ç­”åŸºå‡†ï¼ŒåŒ…å«64,878ä¸ªé—®ç­”å¯¹å’Œ40,263å¼ è„ŠæŸ±å›¾åƒï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è„ŠæŸ±åŒ»å­¦ä»»åŠ¡ä¸­çš„ç»†ç²’åº¦æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºå‡†ä¸»è¦è¯„ä¼°é€šç”¨åŒ»å­¦ä»»åŠ¡ï¼Œæ— æ³•å……åˆ†æ•æ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è„ŠæŸ±ç­‰ä¾èµ–è§†è§‰è¾“å…¥çš„ç»†åˆ†é¢†åŸŸçš„æ€§èƒ½è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯è„ŠæŸ±ç–¾ç—…è¯Šæ–­å’Œç—…ç¶å®šä½ç­‰å…³é”®ä¸´åºŠä»»åŠ¡ã€‚</p>
<p><strong>Method:</strong> é€šè¿‡æ•´åˆå’Œæ ‡å‡†åŒ–å¼€æºè„ŠæŸ±ç–¾ç—…æ•°æ®é›†çš„å›¾åƒæ ‡ç­¾å¯¹ï¼Œæ„å»ºåŒ…å«11ç§è„ŠæŸ±ç–¾ç—…çš„è§†è§‰é—®ç­”åŸºå‡†ï¼Œå¹¶ä¸ºæ¯ä¸ªé—®ç­”å¯¹åŸºäºè§†è§‰ç›¸ä¼¼æ€§é‡‡æ ·å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¡¬è´Ÿæ ·æœ¬é€‰é¡¹ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å›°éš¾åœºæ™¯ã€‚</p>
<p><strong>Result:</strong> å¯¹12ä¸ªé¢†å…ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨è„ŠæŸ±ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå·®ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è„ŠæŸ±é¢†åŸŸçš„å±€é™æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è„ŠæŸ±åŒ»å­¦åº”ç”¨ä¸­çš„ä¸è¶³ï¼Œä¸ºæœªæ¥æ”¹è¿›è„ŠæŸ±åŒ»å­¦åº”ç”¨æä¾›äº†æŒ‡å¯¼æ–¹å‘ï¼ŒåŒæ—¶å…¬å¼€çš„åŸºå‡†å°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>With the increasing integration of Multimodal Large Language Models (MLLMs)
into the medical field, comprehensive evaluation of their performance in
various medical domains becomes critical. However, existing benchmarks
primarily assess general medical tasks, inadequately capturing performance in
nuanced areas like the spine, which relies heavily on visual input. To address
this, we introduce SpineBench, a comprehensive Visual Question Answering (VQA)
benchmark designed for fine-grained analysis and evaluation of MLLMs in the
spinal domain. SpineBench comprises 64,878 QA pairs from 40,263 spine images,
covering 11 spinal diseases through two critical clinical tasks: spinal disease
diagnosis and spinal lesion localization, both in multiple-choice format.
SpineBench is built by integrating and standardizing image-label pairs from
open-source spinal disease datasets, and samples challenging hard negative
options for each VQA pair based on visual similarity (similar but not the same
disease), simulating real-world challenging scenarios. We evaluate 12 leading
MLLMs on SpineBench. The results reveal that these models exhibit poor
performance in spinal tasks, highlighting limitations of current MLLM in the
spine domain and guiding future improvements in spinal medicine applications.
SpineBench is publicly available at
https://zhangchenghanyu.github.io/SpineBench.github.io/.</p>
<h3 id="24-drivevla-w0-world-models-amplify-data-scaling-law-in-autonomous-driving">[24] <a href="https://arxiv.org/abs/2510.12796">DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</a></h3>
<p><em>Yingyan Li, Shuyao Shang, Weisong Liu, Bing Zhan, Haochen Wang, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, Lu Hou, Lue Fan, Zhaoxiang Zhang</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDriveVLA-W0è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡ä¸–ç•Œå»ºæ¨¡é¢„æµ‹æœªæ¥å›¾åƒæ¥è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­çš„ç›‘ç£ç¨€ç–é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†é©¾é©¶æ™ºèƒ½çš„æ³›åŒ–èƒ½åŠ›å’Œæ•°æ®ç¼©æ”¾æ•ˆç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å­˜åœ¨ç›‘ç£ç¨€ç–é—®é¢˜ï¼Œæ¨¡å‹çš„å¤§å®¹é‡ä»…ç”±ç¨€ç–çš„ä½ç»´åŠ¨ä½œç›‘ç£ï¼Œå¯¼è‡´å…¶è¡¨ç¤ºèƒ½åŠ›æœªè¢«å……åˆ†åˆ©ç”¨ï¼Œé™åˆ¶äº†é©¾é©¶æ™ºèƒ½çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> æå‡ºDriveVLA-W0è®­ç»ƒèŒƒå¼ï¼Œé‡‡ç”¨ä¸–ç•Œå»ºæ¨¡é¢„æµ‹æœªæ¥å›¾åƒç”Ÿæˆå¯†é›†è‡ªç›‘ç£ä¿¡å·ï¼›é’ˆå¯¹ä¸¤ç§ä¸»æµVLAæ¶æ„åˆ†åˆ«å®ç°è‡ªå›å½’ä¸–ç•Œæ¨¡å‹å’Œæ‰©æ•£ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶å¼•å…¥è½»é‡çº§åŠ¨ä½œä¸“å®¶æ¨¡å—ä»¥é™ä½æ¨ç†å»¶è¿Ÿã€‚</p>
<p><strong>Result:</strong> åœ¨NAVSIM v1/v2åŸºå‡†æµ‹è¯•å’Œ680å€æ›´å¤§çš„å†…éƒ¨æ•°æ®é›†ä¸Šï¼ŒDriveVLA-W0æ˜¾è‘—è¶…è¶ŠBEVå’ŒVLAåŸºçº¿æ–¹æ³•ï¼Œå¹¶å¢å¼ºäº†æ•°æ®ç¼©æ”¾å®šå¾‹ï¼Œè¡¨æ˜éšç€è®­ç»ƒæ•°æ®é‡å¢åŠ æ€§èƒ½æå‡åŠ é€Ÿã€‚</p>
<p><strong>Conclusion:</strong> ä¸–ç•Œå»ºæ¨¡ä¸ºVLAæ¨¡å‹æä¾›äº†æœ‰æ•ˆçš„å¯†é›†ç›‘ç£ä¿¡å·ï¼Œèƒ½å¤Ÿå­¦ä¹ é©¾é©¶ç¯å¢ƒçš„åº•å±‚åŠ¨æ€ï¼Œæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½å¹¶ä¼˜åŒ–æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼Œä¸ºå®æ—¶éƒ¨ç½²æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Scaling Vision-Language-Action (VLA) models on large-scale data offers a
promising path to achieving a more generalized driving intelligence. However,
VLA models are limited by a ``supervision deficit'': the vast model capacity is
supervised by sparse, low-dimensional actions, leaving much of their
representational power underutilized. To remedy this, we propose
\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to
predict future images. This task generates a dense, self-supervised signal that
compels the model to learn the underlying dynamics of the driving environment.
We showcase the paradigm's versatility by instantiating it for two dominant VLA
archetypes: an autoregressive world model for VLAs that use discrete visual
tokens, and a diffusion world model for those operating on continuous visual
features. Building on the rich representations learned from world modeling, we
introduce a lightweight action expert to address the inference latency for
real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a
680x larger in-house dataset demonstrate that DriveVLA-W0 significantly
outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling
law, showing that performance gains accelerate as the training dataset size
increases.</p>
<h3 id="25-dual-learning-with-dynamic-knowledge-distillation-and-soft-alignment-for-partially-relevant-video-retrieval">[25] <a href="https://arxiv.org/abs/2510.12283">Dual Learning with Dynamic Knowledge Distillation and Soft Alignment for Partially Relevant Video Retrieval</a></h3>
<p><em>Jianfeng Dong, Lei Huang, Daizong Liu, Xianke Chen, Xun Yang, Changting Lin, Xun Wang, Meng Wang</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºéƒ¨åˆ†ç›¸å…³è§†é¢‘æ£€ç´¢ï¼ˆPRVRï¼‰çš„åŒé‡å­¦ä¹ æ¡†æ¶DL-DKD++ï¼Œé€šè¿‡ä»å¤§è§„æ¨¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ä¸­è’¸é¦æ³›åŒ–çŸ¥è¯†ï¼Œå¹¶å°†å…¶è½¬ç§»åˆ°è½»é‡çº§çš„ä»»åŠ¡ç‰¹å®šç½‘ç»œä¸­ï¼Œè§£å†³äº†æœªä¿®å‰ªé•¿è§†é¢‘ä¸­éƒ¨åˆ†ç›¸å…³å†…å®¹çš„æ£€ç´¢æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–‡æœ¬åˆ°è§†é¢‘æ£€ç´¢æ–¹æ³•é€šå¸¸å‡è®¾è§†é¢‘æ˜¯ç»è¿‡é¢„ä¿®å‰ªçš„çŸ­ç‰‡æ®µä¸”ä»…åŒ…å«ä¸æ–‡æœ¬ç›¸å…³çš„å†…å®¹ï¼Œç„¶è€Œå®é™…åº”ç”¨ä¸­è§†é¢‘å¾€å¾€æ˜¯æœªä¿®å‰ªçš„é•¿ç‰‡æ®µä¸”åŒ…å«å¤æ‚çš„èƒŒæ™¯å†…å®¹ï¼Œå› æ­¤éœ€è¦è§£å†³æ›´å®ç”¨ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„éƒ¨åˆ†ç›¸å…³è§†é¢‘æ£€ç´¢é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åŒé‡å­¦ä¹ æ¡†æ¶ä¸åŠ¨æ€çŸ¥è¯†è’¸é¦ï¼ˆDL-DKD++ï¼‰ï¼Œå…¶ä¸­å¤§å‹æ•™å¸ˆæ¨¡å‹ä¸ºç´§å‡‘çš„åŒåˆ†æ”¯å­¦ç”Ÿç½‘ç»œæä¾›ç›‘ç£ï¼Œå­¦ç”Ÿæ¨¡å‹åŒ…å«ç»§æ‰¿åˆ†æ”¯å’Œæ¢ç´¢åˆ†æ”¯ï¼Œåˆ†åˆ«ä»æ•™å¸ˆæ¨¡å‹å¸æ”¶å¯è¿ç§»çŸ¥è¯†å’Œä»PRVRæ•°æ®é›†å­¦ä¹ ä»»åŠ¡ç‰¹å®šä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨åŠ¨æ€è½¯ç›®æ ‡æ„å»ºæœºåˆ¶æ›¿ä»£ç¡¬ç›®æ ‡ç›‘ç£ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨TVRã€ActivityNetå’ŒCharades-STAæ•°æ®é›†ä¸Šçš„PRVRä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯äº†å…¶åœ¨å¤„ç†æœªä¿®å‰ªé•¿è§†é¢‘éƒ¨åˆ†ç›¸å…³å†…å®¹æ£€ç´¢æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡çŸ¥è¯†è’¸é¦å’ŒåŒé‡å­¦ä¹ æœºåˆ¶æœ‰æ•ˆè§£å†³äº†å®é™…è§†é¢‘æ£€ç´¢ä¸­çš„é¢†åŸŸå·®è·é—®é¢˜ï¼ŒåŠ¨æ€è½¯ç›®æ ‡æ„å»ºèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è§†é¢‘ä¸æŸ¥è¯¢ä¹‹é—´çš„ç»†ç²’åº¦éƒ¨åˆ†ç›¸å…³æ€§ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„æœªä¿®å‰ªè§†é¢‘æ£€ç´¢æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Almost all previous text-to-video retrieval works ideally assume that videos
are pre-trimmed with short durations containing solely text-related content.
However, in practice, videos are typically untrimmed in long durations with
much more complicated background content. Therefore, in this paper, we focus on
the more practical yet challenging task of Partially Relevant Video Retrieval
(PRVR), which aims to retrieve partially relevant untrimmed videos with the
given query. To tackle this task, we propose a novel framework that distills
generalization knowledge from a powerful large-scale vision-language
pre-trained model and transfers it to a lightweight, task-specific PRVR
network. Specifically, we introduce a Dual Learning framework with Dynamic
Knowledge Distillation (DL-DKD++), where a large teacher model provides
supervision to a compact dual-branch student network. The student model
comprises two branches: an inheritance branch that absorbs transferable
knowledge from the teacher, and an exploration branch that learns task-specific
information from the PRVR dataset to address domain gaps. To further enhance
learning, we incorporate a dynamic soft-target construction mechanism. By
replacing rigid hard-target supervision with adaptive soft targets that evolve
during training, our method enables the model to better capture the
fine-grained, partial relevance between videos and queries. Experiment results
demonstrate that our proposed model achieves state-of-the-art performance on
TVR, ActivityNet, and Charades-STA datasets for PRVR. The code is available at
https://github.com/HuiGuanLab/DL-DKD.</p>
<h3 id="26-towards-general-urban-monitoring-with-vision-language-models-a-review-evaluation-and-a-research-agenda">[26] <a href="https://arxiv.org/abs/2510.12400">Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda</a></h3>
<p><em>AndrÃ© Torneiro, Diogo Monteiro, Paulo Novais, Pedro Rangel Henriques, Nuno F. Rodrigues</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬ç³»ç»Ÿç»¼è¿°æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸå¸‚åŸºç¡€è®¾æ–½ç›‘æ§ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«å…³æ³¨é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œé€šè¿‡åˆ†æ32é¡¹ç ”ç©¶æ­ç¤ºäº†VLMså¦‚ä½•ä½¿æœºå™¨èƒ½å¤Ÿåƒå¸‚æ°‘ä¸€æ ·é€šè¿‡è§†è§‰è§‚å¯Ÿæ¥è¯„ä¼°åŸå¸‚ç¯å¢ƒçŠ¶å†µã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸå¸‚åŸºç¡€è®¾æ–½ç›‘æ§ä¸»è¦ä¾èµ–ç‰©è”ç½‘ä¼ æ„Ÿå™¨å’Œäººå·¥æ£€æŸ¥ï¼Œè¿™äº›æ–¹æ³•æˆæœ¬é«˜æ˜‚ã€éš¾ä»¥æ‰©å±•ï¼Œä¸”ä¸å¸‚æ°‘é€šè¿‡ç›´æ¥è§†è§‰è§‚å¯Ÿå½¢æˆçš„æ„ŸçŸ¥å­˜åœ¨åå·®ï¼Œå› æ­¤éœ€è¦æ¢ç´¢æœºå™¨æ˜¯å¦èƒ½å¤Ÿåƒå¸‚æ°‘ä¸€æ ·é€šè¿‡è§†è§‰ç†è§£æ¥è¯„ä¼°åŸå¸‚åŸºç¡€è®¾æ–½çŠ¶å†µã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨PRISMAç³»ç»Ÿç»¼è¿°æ–¹æ³•ï¼Œåˆ†æäº†2021è‡³2025å¹´é—´å‘è¡¨çš„32é¡¹åŒè¡Œè¯„å®¡ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬åº”ç”¨ï¼Œç³»ç»Ÿæ¢³ç†äº†ä¸åŒVLMæ¶æ„ã€æ¡†æ¶åŠå…¶åœ¨åŸå¸‚ç›‘æ§ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>Result:</strong> ç»¼è¿°è¯†åˆ«äº†VLMsåœ¨åŸå¸‚ç›‘æ§ä¸­çš„æœ‰æ•ˆåº”ç”¨ä»»åŠ¡ï¼Œç¡®å®šäº†è¡¨ç°ä¼˜å¼‚çš„VLMæ¶æ„å’Œæ¡†æ¶ï¼Œæ•´ç†äº†æ”¯æŒè¯¥é¢†åŸŸå‘å±•çš„æ•°æ®é›†èµ„æºï¼Œå¹¶æ±‡æ€»äº†ç°æœ‰VLMåº”ç”¨çš„è¯„ä¼°æ–¹æ³•å’ŒæŠ¥å‘Šçš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> è§†è§‰è¯­è¨€æ¨¡å‹å±•ç°å‡ºåœ¨åŸå¸‚åŸºç¡€è®¾æ–½ç›‘æ§ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹èƒ½å¤Ÿæ¨¡æ‹Ÿå¸‚æ°‘çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºä½æˆæœ¬ã€å¯æ‰©å±•çš„åŸå¸‚ç›‘æ§è§£å†³æ–¹æ¡ˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œä½†ä»éœ€åœ¨æ¨¡å‹æ³›åŒ–æ€§å’Œè¯„ä¼°æ ‡å‡†æ–¹é¢è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Urban monitoring of public infrastructure (such as waste bins, road signs,
vegetation, sidewalks, and construction sites) poses significant challenges due
to the diversity of objects, environments, and contextual conditions involved.
Current state-of-the-art approaches typically rely on a combination of IoT
sensors and manual inspections, which are costly, difficult to scale, and often
misaligned with citizens' perception formed through direct visual observation.
This raises a critical question: Can machines now "see" like citizens and infer
informed opinions about the condition of urban infrastructure? Vision-Language
Models (VLMs), which integrate visual understanding with natural language
reasoning, have recently demonstrated impressive capabilities in processing
complex visual information, turning them into a promising technology to address
this challenge. This systematic review investigates the role of VLMs in urban
monitoring, with particular emphasis on zero-shot applications. Following the
PRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021
and 2025 to address four core research questions: (1) What urban monitoring
tasks have been effectively addressed using VLMs? (2) Which VLM architectures
and frameworks are most commonly used and demonstrate superior performance? (3)
What datasets and resources support this emerging field? (4) How are VLM-based
applications evaluated, and what performance levels have been reported?</p>
<h3 id="27-videolucy-deep-memory-backtracking-for-long-video-understanding">[27] <a href="https://arxiv.org/abs/2510.12422">VideoLucy: Deep Memory Backtracking for Long Video Understanding</a></h3>
<p><em>Jialong Zuo, Yongtai Deng, Lingdong Kong, Jingkang Yang, Rui Jin, Yiwei Zhang, Nong Sang, Liang Pan, Ziwei Liu, Changxin Gao</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VideoLucyï¼Œä¸€ç§åŸºäºæ·±åº¦è®°å¿†å›æº¯çš„æ¡†æ¶ï¼Œç”¨äºè§£å†³é•¿è§†é¢‘ç†è§£ä¸­æ—¶åºä¸Šä¸‹æ–‡ç¼ºå¤±å’Œå…³é”®ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†å±‚è®°å¿†ç»“æ„å’Œè¿­ä»£å›æº¯æœºåˆ¶ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†GPT-4oç­‰ä¸“æœ‰æ¨¡å‹ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºLLMçš„æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é•¿è§†é¢‘ç†è§£ä¸­å­˜åœ¨ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯å¯¹å•å¸§è¿›è¡Œå»ºæ¨¡å’Œæ¨ç†ï¼Œéš¾ä»¥æ•æ‰è¿ç»­å¸§çš„æ—¶åºä¸Šä¸‹æ–‡ï¼›äºŒæ˜¯ä¸ºé™ä½å¯†é›†å¸§æ ‡æ³¨æˆæœ¬è€Œé‡‡ç”¨ç¨€ç–å¸§é‡‡æ ·ï¼Œå¯èƒ½å¯¼è‡´å…³é”®ä¿¡æ¯ä¸¢å¤±ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†å¯¹é•¿è§†é¢‘ä¸­å¤æ‚äº‹ä»¶çš„æœ‰æ•ˆç†è§£ã€‚</p>
<p><strong>Method:</strong> VideoLucyé‡‡ç”¨å—äººç±»ä»ç²—åˆ°ç»†å›å¿†è¿‡ç¨‹å¯å‘çš„åˆ†å±‚è®°å¿†ç»“æ„ï¼Œè¯¥ç»“æ„åœ¨ä¸åŒå±‚æ¬¡æ·±åº¦ä¸Šæ˜ç¡®å®šä¹‰äº†è®°å¿†çš„ç»†èŠ‚æ°´å¹³å’Œæ—¶é—´èŒƒå›´ã€‚é€šè¿‡åŸºäºæ™ºèƒ½ä½“çš„è¿­ä»£å›æº¯æœºåˆ¶ï¼Œç³»ç»Ÿæ€§åœ°æŒ–æ˜è§†é¢‘èŒƒå›´å†…ä¸é—®é¢˜ç›¸å…³çš„æ·±åº¦è®°å¿†ï¼Œç›´åˆ°æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯ä»¥æä¾›å¯é ç­”æ¡ˆã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVideoLucyæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚åŸºäºå¼€æºæ¨¡å‹æ„å»ºçš„VideoLucyåœ¨æ€§èƒ½ä¸Šç”šè‡³è¶…è¶Šäº†GPT-4oç­‰æœ€æ–°ä¸“æœ‰æ¨¡å‹ï¼ŒåŒæ—¶ä½œè€…è¿˜å¼•å…¥äº†EgoMemæ–°åŸºå‡†æ¥å…¨é¢è¯„ä¼°æ¨¡å‹å¯¹é•¿æ—¶é—´å¤æ‚äº‹ä»¶çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> VideoLucyæ¡†æ¶é€šè¿‡åˆ†å±‚è®°å¿†å’Œæ·±åº¦å›æº¯æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†é•¿è§†é¢‘ç†è§£ä¸­çš„æ—¶åºå»ºæ¨¡å’Œä¿¡æ¯ä¿ç•™é—®é¢˜ã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡ºäº†åˆ›æ–°çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œè¿˜å»ºç«‹äº†æ–°çš„è¯„ä¼°åŸºå‡†ï¼Œä¸ºé•¿è§†é¢‘ç†è§£é¢†åŸŸçš„å‘å±•æä¾›äº†é‡è¦æ¨åŠ¨åŠ›ï¼Œå±•ç¤ºäº†å¼€æºæ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šçš„å·¨å¤§æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Recent studies have shown that agent-based systems leveraging large language
models (LLMs) for key information retrieval and integration have emerged as a
promising approach for long video understanding. However, these systems face
two major challenges. First, they typically perform modeling and reasoning on
individual frames, struggling to capture the temporal context of consecutive
frames. Second, to reduce the cost of dense frame-level captioning, they adopt
sparse frame sampling, which risks discarding crucial information. To overcome
these limitations, we propose VideoLucy, a deep memory backtracking framework
for long video understanding. Inspired by the human recollection process from
coarse to fine, VideoLucy employs a hierarchical memory structure with
progressive granularity. This structure explicitly defines the detail level and
temporal scope of memory at different hierarchical depths. Through an
agent-based iterative backtracking mechanism, VideoLucy systematically mines
video-wide, question-relevant deep memories until sufficient information is
gathered to provide a confident answer. This design enables effective temporal
understanding of consecutive frames while preserving critical details. In
addition, we introduce EgoMem, a new benchmark for long video understanding.
EgoMem is designed to comprehensively evaluate a model's ability to understand
complex events that unfold over time and capture fine-grained details in
extremely long videos. Extensive experiments demonstrate the superiority of
VideoLucy. Built on open-source models, VideoLucy significantly outperforms
state-of-the-art methods on multiple long video understanding benchmarks,
achieving performance even surpassing the latest proprietary models such as
GPT-4o. Our code and dataset will be made publicly at
https://videolucy.github.io</p>
<h3 id="28-unlocking-zero-shot-plant-segmentation-with-plntnet-intelligence">[28] <a href="https://arxiv.org/abs/2510.12579">Unlocking Zero-Shot Plant Segmentation with Pl@ntNet Intelligence</a></h3>
<p><em>Simon RavÃ©, Jean-Christophe Lombardo, Pejman Rasti, Alexis Joly, David Rousseau</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬å†œä¸šå›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆPlantNetæ¤ç‰©åˆ†ç±»æ¨¡å‹ã€DinoV2éª¨å¹²ç½‘ç»œå’ŒSegment Anything Model (SAM)ï¼Œæ— éœ€æ”¶é›†æ–°æ•°æ®é›†å³å¯å®ç°ç²¾ç¡®çš„æ¤ç‰©åˆ†å‰²ã€‚è¯¥æ–¹æ³•åˆ©ç”¨PlantNetçš„æ¤ç‰©ä¸“ç”¨è¡¨å¾ç”Ÿæˆç²—åˆ†å‰²æ©ç ï¼Œå†ç”±SAMè¿›è¡Œç»†åŒ–ï¼Œåœ¨å¤šä¸ªå¤æ‚å†œä¸šåœºæ™¯æ•°æ®é›†ä¸Šå±•ç°å‡ºä¼˜äºåŸºç¡€DinoV2æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å†œä¸šå›¾åƒåˆ†å‰²é¢ä¸´æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œå¤æ‚ç”°é—´æ¡ä»¶å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä¼ ç»Ÿç›‘ç£æ–¹æ³•åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹æ€§èƒ½å—é™ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å†œä¸šåœºæ™¯ä¸­çš„æ ‡æ³¨ç“¶é¢ˆé—®é¢˜ï¼Œæ¢ç´¢å¦‚ä½•åˆ©ç”¨ç°æœ‰åŸºç¡€æ¨¡å‹å’Œæ¤ç‰©ä¸“ç”¨æ¨¡å‹å®ç°æœ‰æ•ˆçš„é›¶æ ·æœ¬åˆ†å‰²ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨PlantNetæ¤ç‰©åˆ†ç±»æ¨¡å‹ç»“åˆå…¶DinoV2éª¨å¹²ç½‘ç»œæå–æ¤ç‰©åŒºåŸŸç‰¹å¾ï¼Œç”Ÿæˆç²—åˆ†å‰²æ©ç ï¼Œç„¶ååˆ©ç”¨Segment Anything Model (SAM)å¯¹è¿™äº›æ©ç è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚æ•´ä¸ªæµç¨‹æ— éœ€é¢å¤–æ•°æ®æ ‡æ³¨ï¼Œå……åˆ†åˆ©ç”¨äº†PlantNetåœ¨æ¤ç‰©è¯†åˆ«æ–¹é¢çš„ä¸“ä¸šçŸ¥è¯†å’ŒSAMçš„å¼ºå¤§åˆ†å‰²èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªä¸åŒå¤æ‚åº¦çš„å…¬å¼€å†œä¸šæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨PlantNetå¾®è°ƒçš„DinoV2æ¨¡å‹ç›¸æ¯”åŸºç¡€DinoV2æ¨¡å‹åœ¨JaccardæŒ‡æ•°(IoU)ä¸Šå–å¾—äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚è¯¥æ–¹æ³•åœ¨å¯¹æ¯”åº¦å˜åŒ–å¤§ã€è®­ç»ƒæ•°æ®æœ‰é™å’Œç”°é—´æ¡ä»¶å¤æ‚çš„åœºæ™¯ä¸­å‡è¡¨ç°å‡ºç¨³å®šçš„åˆ†å‰²æ•ˆæœã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯æ˜äº†å°†åŸºç¡€æ¨¡å‹ä¸æ¤ç‰©ä¸“ç”¨æ¨¡å‹ç›¸ç»“åˆå¯ä»¥æœ‰æ•ˆç¼“è§£å†œä¸šå›¾åƒåˆ†å‰²ä¸­çš„æ ‡æ³¨ç“¶é¢ˆé—®é¢˜ã€‚è¿™ç§ç»„åˆæ–¹æ³•ä¸ºå¤šæ ·åŒ–å†œä¸šåœºæ™¯ä¸‹çš„æœ‰æ•ˆåˆ†å‰²æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œå±•ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸåº”ç”¨çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºå†œä¸šè®¡ç®—æœºè§†è§‰ç ”ç©¶æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>We present a zero-shot segmentation approach for agricultural imagery that
leverages Plantnet, a large-scale plant classification model, in conjunction
with its DinoV2 backbone and the Segment Anything Model (SAM). Rather than
collecting and annotating new datasets, our method exploits Plantnet's
specialized plant representations to identify plant regions and produce coarse
segmentation masks. These masks are then refined by SAM to yield detailed
segmentations. We evaluate on four publicly available datasets of various
complexity in terms of contrast including some where the limited size of the
training data and complex field conditions often hinder purely supervised
methods. Our results show consistent performance gains when using
Plantnet-fine-tuned DinoV2 over the base DinoV2 model, as measured by the
Jaccard Index (IoU). These findings highlight the potential of combining
foundation models with specialized plant-centric models to alleviate the
annotation bottleneck and enable effective segmentation in diverse agricultural
scenarios.</p>
<h3 id="29-zero-shot-cfc-fast-real-world-image-denoising-based-on-cross-frequency-consistency">[29] <a href="https://arxiv.org/abs/2510.12646">Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency Consistency</a></h3>
<p><em>Yanlin Jiang, Yuchen Liu, Mingren Liu</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè·¨é¢‘ä¸€è‡´æ€§çš„é›¶æ ·æœ¬å»å™ªæ–¹æ³•ZSCFCï¼Œè¯¥æ–¹æ³•ä»…éœ€å•å¼ å™ªå£°å›¾åƒå³å¯å®ç°é«˜æ•ˆè®­ç»ƒå’Œå»å™ªï¼Œä¸ä¾èµ–å™ªå£°åˆ†å¸ƒå‡è®¾ï¼Œåœ¨è®¡ç®—æ•ˆç‡å’Œå»å™ªæ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰é›¶æ ·æœ¬æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é›¶æ ·æœ¬å»å™ªæ–¹æ³•å­˜åœ¨è®­ç»ƒæ—¶é—´é•¿ã€ä¾èµ–å™ªå£°ç‹¬ç«‹æ€§å’Œé›¶å‡å€¼å‡è®¾çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®ä¸–ç•Œå¤æ‚å™ªå£°åœºæ™¯ä¸‹çš„åº”ç”¨æ•ˆæœï¼Œå› æ­¤éœ€è¦å¼€å‘ä¸ä¾èµ–å™ªå£°åˆ†å¸ƒå‡è®¾ä¸”æ›´é«˜æ•ˆçš„å»å™ªæ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> åŸºäºå›¾åƒçº¹ç†åœ¨ä¸åŒé¢‘å¸¦é—´å…·æœ‰ä½ç½®ç›¸ä¼¼æ€§å’Œå†…å®¹ä¸€è‡´æ€§è€Œå™ªå£°ä¸å…·å¤‡è¿™ä¸€ç‰¹æ€§ï¼Œæå‡ºäº†è·¨é¢‘ä¸€è‡´æ€§æŸå¤±å‡½æ•°å’Œè¶…è½»é‡ç½‘ç»œæ¶æ„ï¼Œé€šè¿‡åˆ©ç”¨é¢‘åŸŸç‰¹æ€§å®ç°å•å›¾åƒå»å™ªè®­ç»ƒã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œå›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒZSCFCåœ¨è®¡ç®—æ•ˆç‡å’Œå»å™ªæ€§èƒ½æ–¹é¢å‡ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ–¹æ³•ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å¤æ‚å™ªå£°åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•è¯æ˜äº†åˆ©ç”¨è·¨é¢‘ä¸€è‡´æ€§ç‰¹æ€§å¯ä»¥æœ‰æ•ˆè§£å†³çœŸå®ä¸–ç•Œå»å™ªé—®é¢˜ï¼Œä¸ºä¸ä¾èµ–å™ªå£°åˆ†å¸ƒå‡è®¾çš„é›¶æ ·æœ¬å»å™ªæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Zero-shot denoisers address the dataset dependency of deep-learning-based
denoisers, enabling the denoising of unseen single images. Nonetheless,
existing zero-shot methods suffer from long training times and rely on the
assumption of noise independence and a zero-mean property, limiting their
effectiveness in real-world denoising scenarios where noise characteristics are
more complicated. This paper proposes an efficient and effective method for
real-world denoising, the Zero-Shot denoiser based on Cross-Frequency
Consistency (ZSCFC), which enables training and denoising with a single noisy
image and does not rely on assumptions about noise distribution. Specifically,
image textures exhibit position similarity and content consistency across
different frequency bands, while noise does not. Based on this property, we
developed cross-frequency consistency loss and an ultralight network to realize
image denoising. Experiments on various real-world image datasets demonstrate
that our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of
computational efficiency and denoising performance.</p>
<h3 id="30-terracodec-compressing-earth-observations">[30] <a href="https://arxiv.org/abs/2510.12670">TerraCodec: Compressing Earth Observations</a></h3>
<p><em>Julen Costa-Watanabe, Isabelle Wittmann, Benedikt Blumenstiel, Konrad Schindler</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†TerraCodec (TEC)ç³»åˆ—å­¦ä¹ å‹ç¼–è§£ç å™¨ï¼Œä¸“é—¨é’ˆå¯¹åœ°çƒè§‚æµ‹æ•°æ®è®¾è®¡ï¼Œé€šè¿‡æ—¶é—´Transformeræ¨¡å‹å’Œæ½œåœ¨é‡æ‰“åŒ…æŠ€æœ¯å®ç°äº†æ¯”ä¼ ç»Ÿç¼–è§£ç å™¨å¼º3-10å€çš„å‹ç¼©æ€§èƒ½ï¼Œå¹¶å…·å¤‡é›¶æ ·æœ¬äº‘ä¿®å¤èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ°çƒè§‚æµ‹å«æ˜Ÿäº§ç”Ÿæµ·é‡çš„å¤šå…‰è°±å›¾åƒæ—¶é—´åºåˆ—æ•°æ®ï¼Œç°æœ‰å­¦ä¹ å‹å‹ç¼©æ–¹æ³•å­˜åœ¨ç¢ç‰‡åŒ–é—®é¢˜ï¼Œç¼ºä¹å…¬å¼€é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸”ä¸è‡ªç„¶å›¾åƒå‹ç¼©è¿›å±•è„±èŠ‚ï¼Œä¼ ç»Ÿå›¾åƒç¼–è§£ç å™¨å¿½ç•¥æ—¶é—´å†—ä½™ï¼Œè€Œè§†é¢‘ç¼–è§£ç å™¨ä¾èµ–çš„è¿åŠ¨å…ˆéªŒæ— æ³•æ•æ‰é™æ€åœºæ™¯çš„è¾å°„æ¼”åŒ–ç‰¹å¾ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†TerraCodecç³»åˆ—ç¼–è§£ç å™¨ï¼ŒåŒ…æ‹¬é€‚åº”å¤šå…‰è°±è¾“å…¥çš„é«˜æ•ˆå›¾åƒå˜ä½“å’Œåˆ©ç”¨æ—¶é—´ä¾èµ–æ€§çš„æ—¶é—´Transformeræ¨¡å‹(TEC-TT)ï¼Œå¹¶å¼•å…¥äº†æ½œåœ¨é‡æ‰“åŒ…æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§è®­ç»ƒçµæ´»ç‡å˜æ¢å™¨æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œå¯åœ¨ä¸åŒç‡å¤±çœŸè®¾ç½®ä¸‹è¿è¡Œã€‚</p>
<p><strong>Result:</strong> åœ¨Sentinel-2æ•°æ®ä¸Šè®­ç»ƒåï¼ŒTerraCodecåœ¨åŒç­‰å›¾åƒè´¨é‡ä¸‹å®ç°äº†æ¯”ä¼ ç»Ÿç¼–è§£ç å™¨å¼º3-10å€çš„å‹ç¼©æ€§èƒ½ï¼ŒTEC-TTæ¨¡å‹åœ¨AllClearåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é›¶æ ·æœ¬äº‘ä¿®å¤ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸“é—¨å®šåˆ¶çš„å­¦ä¹ å‹å‹ç¼©ç®—æ³•æ˜¯åœ°çƒè§‚æµ‹é¢†åŸŸçš„ä¸€ä¸ªæœ‰å‰æ™¯æ–¹å‘ï¼Œä»£ç å’Œæ¨¡å‹æƒé‡å°†åœ¨å®½æ¾è®¸å¯ä¸‹å‘å¸ƒï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Earth observation (EO) satellites produce massive streams of multispectral
image time series, posing pressing challenges for storage and transmission.
Yet, learned EO compression remains fragmented, lacking publicly available
pretrained models and misaligned with advances in compression for natural
imagery. Image codecs overlook temporal redundancy, while video codecs rely on
motion priors that fail to capture the radiometric evolution of largely static
scenes. We introduce TerraCodec (TEC), a family of learned codecs tailored to
EO. TEC includes efficient image-based variants adapted to multispectral
inputs, as well as a Temporal Transformer model (TEC-TT) that leverages
dependencies across time. To overcome the fixed-rate setting of today's neural
codecs, we present Latent Repacking, a novel method for training flexible-rate
transformer models that operate on varying rate-distortion settings. Trained on
Sentinel-2 data, TerraCodec outperforms classical codecs, achieving 3-10x
stronger compression at equivalent image quality. Beyond compression, TEC-TT
enables zero-shot cloud inpainting, surpassing state-of-the-art methods on the
AllClear benchmark. Our results establish bespoke, learned compression
algorithms as a promising direction for Earth observation. Code and model
weights will be released under a permissive license.</p>
<h3 id="31-anyup-universal-feature-upsampling">[31] <a href="https://arxiv.org/abs/2510.12764">AnyUp: Universal Feature Upsampling</a></h3>
<p><em>Thomas Wimmer, Prune Truong, Marie-Julie Rakotosaona, Michael Oechsle, Federico Tombari, Bernt Schiele, Jan Eric Lenssen</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†AnyUpï¼Œä¸€ç§æ— éœ€ç¼–ç å™¨ç‰¹å®šè®­ç»ƒå³å¯åº”ç”¨äºä»»æ„è§†è§‰ç‰¹å¾å’Œåˆ†è¾¨ç‡çš„ç‰¹å¾ä¸Šé‡‡æ ·æ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰åŸºäºå­¦ä¹ çš„ä¸Šé‡‡æ ·å™¨éœ€è¦ä¸ºæ¯ä¸ªç‰¹å¾æå–å™¨é‡æ–°è®­ç»ƒçš„é™åˆ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºå­¦ä¹ çš„ä¸Šé‡‡æ ·å™¨ï¼ˆå¦‚DINOæˆ–CLIPç‰¹å¾ï¼‰éœ€è¦ä¸ºæ¯ä¸ªç‰¹å¾æå–å™¨é‡æ–°è®­ç»ƒï¼Œæ— æ³•åœ¨æ¨ç†æ—¶æ³›åŒ–åˆ°ä¸åŒçš„ç‰¹å¾ç±»å‹ï¼Œè¿™é™åˆ¶äº†æ–¹æ³•çš„é€šç”¨æ€§å’Œåº”ç”¨èŒƒå›´ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§æ¨ç†æ—¶ç‰¹å¾æ— å…³çš„ä¸Šé‡‡æ ·æ¶æ„ï¼Œè¯¥æ¶æ„ä¸ä¾èµ–äºç‰¹å®šç¼–ç å™¨çš„è®­ç»ƒï¼Œèƒ½å¤Ÿå¤„ç†ä¸åŒç±»å‹çš„è§†è§‰ç‰¹å¾å¹¶ä¿æŒç‰¹å¾è¯­ä¹‰çš„å®Œæ•´æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜AnyUpåœ¨ç‰¹å¾ä¸Šé‡‡æ ·è´¨é‡ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œèƒ½å¤Ÿæ³›åŒ–åˆ°ä¸åŒçš„ç‰¹å¾ç±»å‹ï¼ŒåŒæ—¶ä¿æŒç‰¹å¾è¯­ä¹‰å¹¶é«˜æ•ˆåº”ç”¨äºå¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
<p><strong>Conclusion:</strong> AnyUpæ–¹æ³•ä¸ºè§†è§‰ç‰¹å¾ä¸Šé‡‡æ ·æä¾›äº†é€šç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œçªç ´äº†ç°æœ‰æ–¹æ³•å¯¹ç‰¹å®šç¼–ç å™¨çš„ä¾èµ–ï¼Œä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å„ç§åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>We introduce AnyUp, a method for feature upsampling that can be applied to
any vision feature at any resolution, without encoder-specific training.
Existing learning-based upsamplers for features like DINO or CLIP need to be
re-trained for every feature extractor and thus do not generalize to different
feature types at inference time. In this work, we propose an inference-time
feature-agnostic upsampling architecture to alleviate this limitation and
improve upsampling quality. In our experiments, AnyUp sets a new state of the
art for upsampled features, generalizes to different feature types, and
preserves feature semantics while being efficient and easy to apply to a wide
range of downstream tasks.</p>
<h3 id="32-what-if-understanding-motion-through-sparse-interactions">[32] <a href="https://arxiv.org/abs/2510.12777">What If : Understanding Motion Through Sparse Interactions</a></h3>
<p><em>Stefan Andreas Baumann, Nick Stracke, Timy Phan, BjÃ¶rn Ommer</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Flow Poke Transformer (FPT)æ¡†æ¶ï¼Œç”¨äºç›´æ¥é¢„æµ‹åŸºäºç¨€ç–äº¤äº’ï¼ˆç§°ä¸º"pokes"ï¼‰çš„å±€éƒ¨è¿åŠ¨åˆ†å¸ƒï¼Œæä¾›å¯¹å¤šæ¨¡æ€åœºæ™¯è¿åŠ¨åŠå…¶ä¸ç¡®å®šæ€§çš„å¯è§£é‡Šè¡¨ç¤ºã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿæ–¹æ³•é€šå¸¸åªèƒ½å¯¹åœºæ™¯åŠ¨æ€è¿›è¡Œå¯†é›†é‡‡æ ·ç”Ÿæˆå•ä¸€å®ç°ï¼Œæ— æ³•æœ‰æ•ˆè¡¨ç¤ºå¤šæ¨¡æ€åœºæ™¯è¿åŠ¨åŠå…¶å¯¹ç‰©ç†äº¤äº’çš„ä¾èµ–æ€§ï¼Œä»¥åŠåœºæ™¯åŠ¨æ€å›ºæœ‰çš„ä¸ç¡®å®šæ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†Flow Poke Transformer (FPT)æ¡†æ¶ï¼Œé€šè¿‡ç¨€ç–äº¤äº’ï¼ˆpokesï¼‰ç›´æ¥é¢„æµ‹å±€éƒ¨è¿åŠ¨åˆ†å¸ƒï¼Œæä¾›å¯è§£é‡Šä¸”å¯ç›´æ¥è®¿é—®çš„å¤šæ¨¡æ€åœºæ™¯è¿åŠ¨è¡¨ç¤ºã€‚</p>
<p><strong>Result:</strong> åœ¨å¯†é›†äººè„¸è¿åŠ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œé¢„è®­ç»ƒçš„é€šç”¨æ¨¡å‹è¶…è¶Šäº†ä¸“ç”¨åŸºçº¿æ–¹æ³•ï¼›åœ¨å¼ºåˆ†å¸ƒå¤–ä»»åŠ¡ä¸­ï¼Œç»è¿‡å¾®è°ƒçš„FPTåœ¨å…³èŠ‚ç‰©ä½“è¿åŠ¨ä¼°è®¡ä¸Šæ˜¾è‘—ä¼˜äºåŸŸå†…æ–¹æ³•ï¼›åœ¨åŸºäºpokesçš„è¿åŠ¨éƒ¨ä»¶åˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†ç«äº‰æ€§æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> FPTæ¡†æ¶å±•ç¤ºäº†ç›´æ¥é¢„æµ‹æ˜¾å¼è¿åŠ¨åˆ†å¸ƒåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸ºåœºæ™¯åŠ¨æ€ç†è§£æä¾›äº†æ–°çš„å¯è§£é‡Šè¡¨ç¤ºæ–¹æ³•ï¼Œå¹¶è¯æ˜äº†åœ¨åˆ†å¸ƒå¤–ä»»åŠ¡ä¸­çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Understanding the dynamics of a physical scene involves reasoning about the
diverse ways it can potentially change, especially as a result of local
interactions. We present the Flow Poke Transformer (FPT), a novel framework for
directly predicting the distribution of local motion, conditioned on sparse
interactions termed "pokes". Unlike traditional methods that typically only
enable dense sampling of a single realization of scene dynamics, FPT provides
an interpretable directly accessible representation of multi-modal scene
motion, its dependency on physical interactions and the inherent uncertainties
of scene dynamics. We also evaluate our model on several downstream tasks to
enable comparisons with prior methods and highlight the flexibility of our
approach. On dense face motion generation, our generic pre-trained model
surpasses specialized baselines. FPT can be fine-tuned in strongly
out-of-distribution tasks such as synthetic datasets to enable significant
improvements over in-domain methods in articulated object motion estimation.
Additionally, predicting explicit motion distributions directly enables our
method to achieve competitive performance on tasks like moving part
segmentation from pokes which further demonstrates the versatility of our FPT.
Code and models are publicly available at
https://compvis.github.io/flow-poke-transformer.</p>
<h3 id="33-vico-a-training-strategy-towards-semantic-aware-dynamic-high-resolution">[33] <a href="https://arxiv.org/abs/2510.12793">ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</a></h3>
<p><em>Long Cui, Weiyun Wang, Jie Shao, Zichen Wen, Gen Luo, Linfeng Zhang, Yanting Zhang, Yu Qiao, Wenhai Wang</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºè§†è§‰ä¸€è‡´æ€§å­¦ä¹ ï¼ˆViCOï¼‰ï¼Œä¸€ç§æ–°é¢–çš„è®­ç»ƒç®—æ³•ï¼Œé€šè¿‡åŸºäºå›¾åƒè¯­ä¹‰å¤æ‚åº¦åŠ¨æ€è°ƒæ•´è§†è§‰tokenæ•°é‡çš„æ–¹å¼ï¼Œæ˜¾è‘—é™ä½å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ„ŸçŸ¥ã€æ¨ç†å’ŒOCRèƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”±äºå›¾åƒè¾“å…¥å¼•å…¥çš„é¢å¤–è§†è§‰tokenè€Œå¯¼è‡´æ¨ç†æˆæœ¬æ˜¾è‘—å¢åŠ ï¼Œè¿™æˆä¸ºéƒ¨ç½²é«˜æ•ˆMLLMçš„ä¸»è¦ç“¶é¢ˆï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿæ ¹æ®å›¾åƒè¯­ä¹‰å¤æ‚åº¦è‡ªé€‚åº”è°ƒæ•´è®¡ç®—èµ„æºçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨å¤šä¸ªå…·æœ‰ä¸åŒå›¾åƒå‹ç¼©ç‡çš„MLPè¿æ¥å™¨ï¼Œæ ¹æ®å›¾åƒè¯­ä¹‰å¤æ‚åº¦å¯¹è§†è§‰tokenè¿›è¡Œä¸‹é‡‡æ ·ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å°åŒ–ä¸åŒMLPè¿æ¥å™¨æ¡ä»¶ä¸‹å“åº”çš„KLæ•£åº¦ï¼Œå¹¶åœ¨æ¨ç†æ—¶å¼•å…¥è§†è§‰åˆ†è¾¨ç‡è·¯ç”±å™¨ï¼ˆViRï¼‰è‡ªåŠ¨ä¸ºæ¯ä¸ªå›¾åƒå—é€‰æ‹©åˆé€‚çš„å‹ç¼©ç‡ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå°†è§†è§‰tokenæ•°é‡å‡å°‘é«˜è¾¾50%ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ„ŸçŸ¥ã€æ¨ç†å’ŒOCRèƒ½åŠ›ï¼Œç›¸æ¯”ç°æœ‰çš„åŸºäºå›¾åƒåˆ†è¾¨ç‡çš„åŠ¨æ€é«˜åˆ†è¾¨ç‡ç­–ç•¥ï¼Œæœ¬æ–¹æ³•èƒ½å¤Ÿæ ¹æ®è¯­ä¹‰å¤æ‚åº¦åŠ¨æ€è°ƒæ•´è§†è§‰tokenæ•°é‡ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹å·¥ä½œä¸ºå¼€å‘æ›´é«˜æ•ˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›äº†é‡è¦è´¡çŒ®ï¼Œé€šè¿‡åŸºäºè¯­ä¹‰å¤æ‚åº¦çš„è‡ªé€‚åº”è§†è§‰tokenå‹ç¼©æœºåˆ¶ï¼Œåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ï¼Œä¸ºæœªæ¥é«˜æ•ˆMLLMç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Existing Multimodal Large Language Models (MLLMs) suffer from increased
inference costs due to the additional vision tokens introduced by image inputs.
In this work, we propose Visual Consistency Learning (ViCO), a novel training
algorithm that enables the model to represent images of varying semantic
complexities using different numbers of vision tokens. The key idea behind our
method is to employ multiple MLP connectors, each with a different image
compression ratio, to downsample the vision tokens based on the semantic
complexity of the image. During training, we minimize the KL divergence between
the responses conditioned on different MLP connectors. At inference time, we
introduce an image router, termed Visual Resolution Router (ViR), that
automatically selects the appropriate compression rate for each image patch.
Compared with existing dynamic high-resolution strategies, which adjust the
number of visual tokens based on image resolutions, our method dynamically
adapts the number of visual tokens according to semantic complexity.
Experimental results demonstrate that our method can reduce the number of
vision tokens by up to 50% while maintaining the model's perception, reasoning,
and OCR capabilities. We hope this work will contribute to the development of
more efficient MLLMs. The code and models will be released to facilitate future
research.</p>
<h3 id="34-detect-anything-via-next-point-prediction">[34] <a href="https://arxiv.org/abs/2510.12798">Detect Anything via Next Point Prediction</a></h3>
<p><em>Qing Jiang, Junan Huo, Xingyu Chen, Yuda Xiong, Zhaoyang Zeng, Yihao Chen, Tianhe Ren, Junzhi Yu, Lei Zhang</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Rex-Omniï¼Œä¸€ä¸ª3Bè§„æ¨¡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡åˆ›æ–°çš„ä»»åŠ¡è¡¨è¿°ã€æ•°æ®å¼•æ“å’Œè®­ç»ƒæµç¨‹ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å®ç°äº†ä¸å›å½’æ¨¡å‹ç›¸åª²ç¾çš„ç‰©ä½“æ£€æµ‹æ€§èƒ½ï¼ŒåŒæ—¶å…·å¤‡å¤šåŠŸèƒ½çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç‰©ä½“æ£€æµ‹é¢†åŸŸé•¿æœŸç”±åŸºäºåæ ‡å›å½’çš„ä¼ ç»Ÿæ¨¡å‹ä¸»å¯¼ï¼Œè€Œç°æœ‰å°è¯•åˆ©ç”¨MLLMè§£å†³è¯¥ä»»åŠ¡çš„æ–¹æ³•é¢ä¸´å¬å›ç‡ä½ã€é‡å¤é¢„æµ‹ã€åæ ‡ä¸å¯¹é½ç­‰æŒ‘æˆ˜ï¼Œéœ€è¦å¼¥åˆè¿™ä¸€æŠ€æœ¯å·®è·ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨ä¸‰ä¸ªå…³é”®è®¾è®¡ï¼šä½¿ç”¨ç‰¹æ®Šæ ‡è®°è¡¨ç¤º0åˆ°999çš„é‡åŒ–åæ ‡æ¥é™ä½å­¦ä¹ éš¾åº¦ï¼›æ„å»ºå¤šä¸ªæ•°æ®å¼•æ“ç”Ÿæˆé«˜è´¨é‡çš„å®šä½ã€æŒ‡ä»£å’ŒæŒ‡å‘æ•°æ®ï¼›å®æ–½ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œç»“åˆ2200ä¸‡æ•°æ®çš„ç›‘ç£å¾®è°ƒå’ŒåŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒï¼Œåˆ©ç”¨å‡ ä½•æ„ŸçŸ¥å¥–åŠ±æ¥å¼¥åˆç¦»æ•£åˆ°è¿ç»­åæ ‡é¢„æµ‹çš„å·®è·ã€‚</p>
<p><strong>Result:</strong> åœ¨COCOå’ŒLVISç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRex-Omniåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¾¾åˆ°æˆ–è¶…è¿‡äº†å›å½’æ¨¡å‹ï¼ˆå¦‚DINOã€Grounding DINOï¼‰çš„æ€§èƒ½ï¼ŒåŒæ—¶å±•ç°å‡ºç‰©ä½“æŒ‡ä»£ã€æŒ‡å‘ã€è§†è§‰æç¤ºã€GUIå®šä½ã€ç©ºé—´æŒ‡ä»£ã€OCRå’Œå…³é”®ç‚¹å®šä½ç­‰å¤šåŠŸèƒ½èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> Rex-Omniä¸ºæ›´é€šç”¨å’Œè¯­è¨€æ„ŸçŸ¥çš„è§†è§‰æ„ŸçŸ¥ç³»ç»Ÿå¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œè¯æ˜äº†MLLMåœ¨ç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿç»Ÿä¸€å¤šç§è§†è§‰æ„ŸçŸ¥èƒ½åŠ›äºå•ä¸€æ¨¡å‹ä¸­ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>Object detection has long been dominated by traditional coordinate
regression-based models, such as YOLO, DETR, and Grounding DINO. Although
recent efforts have attempted to leverage MLLMs to tackle this task, they face
challenges like low recall rate, duplicate predictions, coordinate
misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a
3B-scale MLLM that achieves state-of-the-art object perception performance. On
benchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or
exceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot
setting. This is enabled by three key designs: 1) Task Formulation: we use
special tokens to represent quantized coordinates from 0 to 999, reducing the
model's learning difficulty and improving token efficiency for coordinate
prediction; 2) Data Engines: we construct multiple data engines to generate
high-quality grounding, referring, and pointing data, providing semantically
rich supervision for training; \3) Training Pipelines: we employ a two-stage
training process, combining supervised fine-tuning on 22 million data with
GRPO-based reinforcement post-training. This RL post-training leverages
geometry-aware rewards to effectively bridge the discrete-to-continuous
coordinate prediction gap, improve box accuracy, and mitigate undesirable
behaviors like duplicate predictions that stem from the teacher-guided nature
of the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent
language understanding enables versatile capabilities such as object referring,
pointing, visual prompting, GUI grounding, spatial referring, OCR and
key-pointing, all systematically evaluated on dedicated benchmarks. We believe
that Rex-Omni paves the way for more versatile and language-aware visual
perception systems.</p>
<h3 id="35-deepmmsearch-r1-empowering-multimodal-llms-in-multimodal-web-search">[35] <a href="https://arxiv.org/abs/2510.12801">DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search</a></h3>
<p><em>Kartik Narayan, Yang Xu, Tian Cao, Kavya Nerella, Vishal M. Patel, Navid Shiee, Peter Grasch, Chao Jia, Yinfei Yang, Zhe Gan</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DeepMMSearch-R1ï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿæ‰§è¡ŒæŒ‰éœ€å¤šè½®ç½‘é¡µæœç´¢å¹¶ä¸ºå›¾åƒå’Œæ–‡æœ¬æœç´¢å·¥å…·åŠ¨æ€æ„å»ºæŸ¥è¯¢çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢çš„æ•ˆç‡å’Œæ•ˆæœã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–¹æ³•å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆã€æœç´¢ä»£ç†å’Œé…å¤‡æœç´¢åŠŸèƒ½çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å­˜åœ¨æµç¨‹åƒµåŒ–ã€æœç´¢è°ƒç”¨è¿‡å¤šå’Œæœç´¢æŸ¥è¯¢æ„å»ºä¸å½“ç­‰é—®é¢˜ï¼Œå¯¼è‡´åœ¨å¤„ç†ä¿¡æ¯å¯»æ±‚å’ŒçŸ¥è¯†å¯†é›†å‹ç”¨æˆ·æŸ¥è¯¢æ—¶æ•ˆç‡ä½ä¸‹ä¸”æ•ˆæœä¸ä½³ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼šå†·å¯åŠ¨ç›‘ç£å¾®è°ƒé˜¶æ®µå’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œå¼•å…¥DeepMMSearchVQAæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹åˆ›å»ºå¹¶èåˆç½‘é¡µæœç´¢å·¥å…·çš„çœŸå®ä¿¡æ¯ï¼ŒåŒ…å«å¤šæ ·åŒ–çš„å¤šè·³æŸ¥è¯¢ï¼Œæ•™å¯¼æ¨¡å‹ä½•æ—¶æœç´¢ã€æœç´¢ä»€ä¹ˆã€ä½¿ç”¨å“ªç§æœç´¢å·¥å…·ä»¥åŠå¦‚ä½•å¯¹æ£€ç´¢ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªçŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œæ¨¡å‹èƒ½å¤ŸåŸºäºè¾“å…¥å›¾åƒçš„ç›¸å…³è£å‰ªå¯åŠ¨ç½‘é¡µæœç´¢ä½¿å›¾åƒæœç´¢æ›´æœ‰æ•ˆï¼Œå¹¶èƒ½è¿­ä»£è°ƒæ•´æ–‡æœ¬æœç´¢æŸ¥è¯¢å®ç°è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘çº æ­£ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºæ¨è¿›å¤šæ¨¡æ€ç½‘é¡µæœç´¢æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå±•ç¤ºäº†åŠ¨æ€æŸ¥è¯¢æ„å»ºå’Œè¿­ä»£æœç´¢ç­–ç•¥åœ¨æå‡å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢æ€§èƒ½æ–¹é¢çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æœç´¢ç³»ç»Ÿçš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) in real-world applications require
access to external knowledge sources and must remain responsive to the dynamic
and ever-changing real-world information in order to address
information-seeking and knowledge-intensive user queries. Existing approaches,
such as retrieval augmented generation (RAG) methods, search agents, and search
equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and
poorly constructed search queries, which result in inefficiencies and
suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,
the first multimodal LLM capable of performing on-demand, multi-turn web
searches and dynamically crafting queries for both image and text search tools.
Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops
of the input image making the image search more effective, and can iteratively
adapt text search queries based on retrieved information, thereby enabling
self-reflection and self-correction. Our approach relies on a two-stage
training pipeline: a cold start supervised finetuning phase followed by an
online reinforcement learning optimization. For training, we introduce
DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated
pipeline intermixed with real-world information from web search tools. This
dataset contains diverse, multi-hop queries that integrate textual and visual
information, teaching the model when to search, what to search for, which
search tool to use and how to reason over the retrieved information. We conduct
extensive experiments across a range of knowledge-intensive benchmarks to
demonstrate the superiority of our approach. Finally, we analyze the results
and provide insights that are valuable for advancing multimodal web-search.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="36-improving-text-to-image-generation-with-input-side-inference-time-scaling">[36] <a href="https://arxiv.org/abs/2510.12041">Improving Text-to-Image Generation with Input-Side Inference-Time Scaling</a></h3>
<p><em>Ruibo Chen, Jiacheng Pan, Heng Huang, Zhenheng Yang</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæç¤ºè¯é‡å†™çš„æ¡†æ¶ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±ç³»ç»Ÿå’Œè¿­ä»£å¼ç›´æ¥åå¥½ä¼˜åŒ–è®­ç»ƒï¼Œæ— éœ€ç›‘ç£å¾®è°ƒæ•°æ®å³å¯æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†ç®€å•æˆ–æœªå……åˆ†æŒ‡å®šçš„æç¤ºè¯æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´å›¾åƒ-æ–‡æœ¬å¯¹é½åº¦ã€ç¾å­¦è´¨é‡å’Œè§†è§‰è´¨é‡ä¸‹é™ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿè‡ªåŠ¨ä¼˜åŒ–ç”¨æˆ·è¾“å…¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æç¤ºè¯é‡å†™æ¡†æ¶ï¼ŒåŒ…å«ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±ç³»ç»Ÿå’Œè¿­ä»£å¼ç›´æ¥åå¥½ä¼˜åŒ–è®­ç»ƒæµç¨‹ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€ç›‘ç£å¾®è°ƒæ•°æ®çš„æƒ…å†µä¸‹å¢å¼ºæç¤ºè¯è´¨é‡ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æç¤ºè¯é‡å†™å™¨åœ¨å„ç§æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­ä¸€è‡´æå‡äº†å›¾åƒ-æ–‡æœ¬å¯¹é½åº¦ã€è§†è§‰è´¨é‡å’Œç¾å­¦è¡¨ç°ï¼Œä¼˜äºå¼ºåŸºçº¿æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„è·¨æ¨¡å‹è¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜æç¤ºè¯é‡å†™æ˜¯ä¸€ç§æœ‰æ•ˆã€å¯æ‰©å±•ä¸”æ¨¡å‹æ— å…³çš„ç­–ç•¥ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿæ€§èƒ½ï¼ŒåŒæ—¶å‘ç°æ€§èƒ½å¢ç›Šä¸æ‰€ç”¨å¤§è¯­è¨€æ¨¡å‹å®¹é‡å‘ˆæ­£ç›¸å…³å…³ç³»ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models often struggle with simple or underspecified
prompts, leading to suboptimal image-text alignment, aesthetics, and quality.
We propose a prompt rewriting framework that leverages large language models
(LLMs) to refine user inputs before feeding them into T2I backbones. Our
approach introduces a carefully designed reward system and an iterative direct
preference optimization (DPO) training pipeline, enabling the rewriter to
enhance prompts without requiring supervised fine-tuning data. We evaluate our
method across diverse T2I models and benchmarks. Results show that our prompt
rewriter consistently improves image-text alignment, visual quality, and
aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong
transferability by showing that a prompt rewriter trained on one T2I backbone
generalizes effectively to others without needing to be retrained. We also
systematically study scalability, evaluating how performance gains scale with
the capacity of the large LLM used as the rewriter. These findings highlight
that prompt rewriting is an effective, scalable, and practical model-agnostic
strategy for improving T2I systems. We plan to release the code and trained
prompt rewriters soon.</p>
<h3 id="37-safemt-multi-turn-safety-for-multimodal-language-models">[37] <a href="https://arxiv.org/abs/2510.12133">SafeMT: Multi-turn Safety for Multimodal Language Models</a></h3>
<p><em>Han Zhu, Juntao Dai, Jiaming Ji, Haoran Li, Chengkun Cai, Pengcheng Wen, Chi-Min Chan, Boyuan Chen, Yaodong Yang, Sirui Han, Yike Guo</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SafeMTåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­çš„å®‰å…¨æ€§ï¼Œå¹¶å‘ç°éšç€å¯¹è¯è½®æ•°å¢åŠ ï¼Œæ¨¡å‹æ”»å‡»æˆåŠŸç‡æ˜¾è‘—ä¸Šå‡ï¼ŒåŒæ—¶æå‡ºäº†ä¸€ä¸ªå¯¹è¯å®‰å…¨è°ƒèŠ‚å™¨æ¥æ£€æµ‹å¯¹è¯ä¸­çš„æ¶æ„æ„å›¾ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ï¼Œå…¶å®‰å…¨æ€§é—®é¢˜æ—¥ç›Šçªå‡ºï¼Œç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½å……åˆ†è¯„ä¼°å¤šè½®å¯¹è¯åœºæ™¯ä¸‹çš„å®‰å…¨é£é™©ï¼Œè€Œå¤šè½®å¯¹è¯åœ¨æ—¥å¸¸äº¤äº’ä¸­æ›´ä¸ºå¸¸è§ä¸”é£é™©æ›´é«˜ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†åŒ…å«10,000ä¸ªæ ·æœ¬çš„SafeMTåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–17ç§ä¸åŒåœºæ™¯å’Œå››ç§è¶Šç‹±æ–¹æ³•ï¼Œæå‡ºäº†å®‰å…¨æŒ‡æ•°æ¥è¯„ä¼°å¯¹è¯æœŸé—´çš„æ•´ä½“å®‰å…¨æ€§ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªèƒ½å¤Ÿæ£€æµ‹å¯¹è¯ä¸­éšè—æ¶æ„æ„å›¾å¹¶æä¾›ç›¸å…³å®‰å…¨ç­–ç•¥çš„å¯¹è¯å®‰å…¨è°ƒèŠ‚å™¨ã€‚</p>
<p><strong>Result:</strong> å¯¹17ä¸ªæ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œéšç€æœ‰å®³å¯¹è¯è½®æ•°å¢åŠ ï¼ŒæˆåŠŸæ”»å‡»çš„é£é™©æ˜¾è‘—ä¸Šå‡ï¼Œå®éªŒç»“æœè¡¨æ˜æ‰€æå‡ºçš„å®‰å…¨è°ƒèŠ‚å™¨åœ¨é™ä½å¤šè½®æ”»å‡»æˆåŠŸç‡æ–¹é¢æ¯”ç°æœ‰é˜²æŠ¤æ¨¡å‹æ›´æœ‰æ•ˆã€‚</p>
<p><strong>Conclusion:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æœºåˆ¶åœ¨è¯†åˆ«å¯¹è¯äº¤äº’ä¸­çš„å±é™©æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦ä¸“é—¨è®¾è®¡çš„å®‰å…¨è§£å†³æ–¹æ¡ˆæ¥åº”å¯¹å¤šè½®å¯¹è¯åœºæ™¯ä¸‹çš„å®‰å…¨æŒ‘æˆ˜ï¼Œå¯¹è¯å®‰å…¨è°ƒèŠ‚å™¨ä¸ºæå‡æ¨¡å‹å®‰å…¨æ€§æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>With the widespread use of multi-modal Large Language models (MLLMs), safety
issues have become a growing concern. Multi-turn dialogues, which are more
common in everyday interactions, pose a greater risk than single prompts;
however, existing benchmarks do not adequately consider this situation. To
encourage the community to focus on the safety issues of these models in
multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues
of varying lengths generated from harmful queries accompanied by images. This
benchmark consists of 10,000 samples in total, encompassing 17 different
scenarios and four jailbreak methods. Additionally, we propose Safety Index
(SI) to evaluate the general safety of MLLMs during conversations. We assess
the safety of 17 models using this benchmark and discover that the risk of
successful attacks on these models increases as the number of turns in harmful
dialogues rises. This observation indicates that the safety mechanisms of these
models are inadequate for recognizing the hazard in dialogue interactions. We
propose a dialogue safety moderator capable of detecting malicious intent
concealed within conversations and providing MLLMs with relevant safety
policies. Experimental results from several open-source models indicate that
this moderator is more effective in reducing multi-turn ASR compared to existed
guard models.</p>
<h3 id="38-not-in-sync-unveiling-temporal-bias-in-audio-chat-models">[38] <a href="https://arxiv.org/abs/2510.12185">Not in Sync: Unveiling Temporal Bias in Audio Chat Models</a></h3>
<p><em>Jiayu Yao, Shenghua Liu, Yiwei Wang, Rundong Cheng, Lingrui Mei, Baolong Bi, Zhen Xiong, Xueqi Cheng</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ä¸­çš„æ—¶é—´åå·®é—®é¢˜ï¼Œå‘ç°æ¨¡å‹åœ¨é¢„æµ‹äº‹ä»¶æ—¶é—´æˆ³æ—¶å­˜åœ¨ç³»ç»Ÿæ€§åå·®ï¼Œè¿™ç§åå·®éšéŸ³é¢‘é•¿åº¦å¢åŠ è€Œç´¯ç§¯ï¼Œå¹¶æå‡ºäº†æ—¶é—´åå·®æŒ‡æ•°æ¥é‡åŒ–è¿™ä¸€ç°è±¡ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘ç†è§£å’Œå¤šæ¨¡æ€æ¨ç†ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å…¶å®šä½äº‹ä»¶å‘ç”Ÿæ—¶é—´çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—¶é—´æˆ³é¢„æµ‹æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§åå·®é—®é¢˜äºŸå¾…ç ”ç©¶ã€‚</p>
<p><strong>Method:</strong> é€šè¿‡åœ¨å¸¦æ—¶é—´æˆ³çš„æ•°æ®é›†ä¸Šè¿›è¡Œå—æ§å®éªŒï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†æ—¶é—´åå·®æŒ‡æ•°æ¥é‡åŒ–é¢„æµ‹äº‹ä»¶æ—¶é—´ä¸çœŸå®æ—¶é—´ä¹‹é—´çš„ç³»ç»Ÿæ€§é”™ä½ï¼Œå¹¶è¾…ä»¥å¯è§†åŒ–æ¡†æ¶è¿›è¡Œåˆ†æã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘ç°æ—¶é—´åå·®åœ¨ä¸åŒæ•°æ®é›†å’Œæ¨¡å‹ä¸­æ™®éå­˜åœ¨ï¼Œä¸”éšéŸ³é¢‘é•¿åº¦å¢åŠ è€Œç´¯ç§¯ï¼Œåœ¨é•¿å½•éŸ³ä¸­å¯è¾¾æ•°åç§’ï¼ŒåŒæ—¶åå·®ç¨‹åº¦å› äº‹ä»¶ç±»å‹å’Œä½ç½®è€Œå¼‚ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´å®šä½æ–¹é¢çš„æ ¹æœ¬å±€é™æ€§ï¼Œå¼ºè°ƒäº†å¼€å‘æ—¶é—´é²æ£’æ¶æ„çš„å¿…è¦æ€§ï¼Œä¸ºæœªæ¥æ¨¡å‹æ”¹è¿›æä¾›äº†é‡è¦æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>Large Audio Language Models (LALMs) are increasingly applied to audio
understanding and multimodal reasoning, yet their ability to locate when events
occur remains underexplored. We present the first systematic study of temporal
bias in LALMs, revealing a key limitation in their timestamp prediction. For
example, when asked "At which second does the lecturer introduce the key
formula?", models often predict timestamps that are consistently earlier or
later than the ground truth. Through controlled experiments on timestamped
datasets, we find that temporal bias (i) is prevalent across datasets and
models, (ii) increases with audio length - even accumulating to tens of seconds
in extended recordings, and (iii) varies across event types and positions. We
quantify this effect with the Temporal Bias Index (TBI), measuring systematic
misalignment in predicted event timings, and complement it with a visualization
framework. Our findings highlight a fundamental limitation in current LALMs and
call for the development of temporally robust architectures.</p>
<h3 id="39-smec-rethinking-matryoshka-representation-learning-for-retrieval-embedding-compression">[39] <a href="https://arxiv.org/abs/2510.12474">SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression</a></h3>
<p><em>Biao Zhang, Lixin Chen, Tong Liu, Bo Zheng</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSMECçš„åºåˆ—åµŒå¥—åµŒå…¥å‹ç¼©è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡é™ä½é«˜ç»´åµŒå…¥çš„è®¡ç®—å’Œå­˜å‚¨å¤æ‚åº¦ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å®ç°æ˜¾è‘—çš„ç»´åº¦å‹ç¼©ã€‚è¯¥æ–¹æ³•åœ¨BEIRæ•°æ®é›†ä¸Šç›¸æ¯”ç°æœ‰æ–¹æ³•æå‡äº†å‹ç¼©åLLM2VecåµŒå…¥çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„é«˜ç»´åµŒå…¥è™½ç„¶èƒ½æ•æ‰ä¸°å¯Œçš„è¯­ä¹‰å’Œå¥æ³•ä¿¡æ¯ï¼Œä½†åŠ å‰§äº†è®¡ç®—å¤æ‚åº¦å’Œå­˜å‚¨éœ€æ±‚ï¼Œé˜»ç¢äº†å®é™…éƒ¨ç½²ã€‚ç°æœ‰æ–¹æ³•åœ¨ç»´åº¦å‹ç¼©è¿‡ç¨‹ä¸­é¢ä¸´æ¢¯åº¦æ–¹å·®å¤§ã€ä¿¡æ¯é€€åŒ–ä¸¥é‡ä»¥åŠé«˜ä½ç»´åµŒå…¥é—´æ— ç›‘ç£å­¦ä¹ æ•ˆæœä¸ä½³ç­‰é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åºåˆ—åµŒå¥—åµŒå…¥å‹ç¼©æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåºåˆ—åµŒå¥—è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ç”¨äºç¼“è§£è®­ç»ƒä¸­çš„æ¢¯åº¦æ–¹å·®ï¼Œè‡ªé€‚åº”ç»´åº¦é€‰æ‹©æ¨¡å—å‡å°‘ç»´åº¦å‰ªææ—¶çš„ä¿¡æ¯é€€åŒ–ï¼Œå¯é€‰æ‹©çš„è·¨æ‰¹æ¬¡è®°å¿†æ¨¡å—å¢å¼ºé«˜ä½ç»´åµŒå…¥é—´çš„æ— ç›‘ç£å­¦ä¹ æ•ˆæœã€‚</p>
<p><strong>Result:</strong> åœ¨å›¾åƒã€æ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSMECåœ¨å®ç°æ˜¾è‘—ç»´åº¦å‹ç¼©çš„åŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚åœ¨BEIRæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å°†å‹ç¼©åçš„LLM2VecåµŒå…¥æ€§èƒ½ç›¸æ¯”Matryoshka-Adaptorå’ŒSearch-Adaptoræ¨¡å‹åˆ†åˆ«æå‡äº†1.1å’Œ2.7ä¸ªç‚¹ã€‚</p>
<p><strong>Conclusion:</strong> SMECæ¡†æ¶æœ‰æ•ˆè§£å†³äº†é«˜ç»´åµŒå…¥å‹ç¼©ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºå®é™…éƒ¨ç½²æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒè¯­ä¹‰è¡¨ç¤ºè´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å’Œå­˜å‚¨å¼€é”€ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Large language models (LLMs) generate high-dimensional embeddings that
capture rich semantic and syntactic information. However, high-dimensional
embeddings exacerbate computational complexity and storage requirements,
thereby hindering practical deployment. To address these challenges, we propose
a novel training framework named Sequential Matryoshka Embedding Compression
(SMEC). This framework introduces the Sequential Matryoshka Representation
Learning(SMRL) method to mitigate gradient variance during training, the
Adaptive Dimension Selection (ADS) module to reduce information degradation
during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module
to enhance unsupervised learning between high- and low-dimensional embeddings.
Experiments on image, text, and multimodal datasets demonstrate that SMEC
achieves significant dimensionality reduction while maintaining performance.
For instance, on the BEIR dataset, our approach improves the performance of
compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points
compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.</p>
<h3 id="40-omni-captioner-data-pipeline-models-and-benchmark-for-omni-detailed-perception">[40] <a href="https://arxiv.org/abs/2510.12720">Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception</a></h3>
<p><em>Ziyang Ma, Ruiyang Xu, Zhenghao Xing, Yunfei Chu, Yuxuan Wang, Jinzheng He, Jin Xu, Pheng-Ann Heng, Kai Yu, Junyang Lin, Eng Siong Chng, Xie Chen</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢ç´¢äº†å…¨æ¨¡æ€è¯­è¨€æ¨¡å‹çš„ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œæå‡ºäº†Omni-Detectiveæ•°æ®ç”Ÿæˆç®¡é“å’ŒOmni-Clozeè¯„ä¼°åŸºå‡†ï¼Œåœ¨éŸ³é¢‘å’Œè§†å¬ç»†ç²’åº¦æè¿°ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å…¨æ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦å¤šæ¨¡æ€ä¿¡æ¯æ„ŸçŸ¥æ–¹é¢å­˜åœ¨å±€é™ï¼Œç‰¹åˆ«æ˜¯ç»†èŠ‚æè¿°ä¸å¹»è§‰ç”Ÿæˆä¹‹é—´å­˜åœ¨å›ºæœ‰çš„'å…±å¢é•¿'é—®é¢˜ï¼Œç¼ºä¹ä¸“é—¨é’ˆå¯¹ç»†ç²’åº¦æ„ŸçŸ¥çš„è¯„ä¼°åŸºå‡†ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†Omni-Detectiveä»£ç†å¼æ•°æ®ç”Ÿæˆç®¡é“ï¼Œé›†æˆå·¥å…·è°ƒç”¨è‡ªåŠ¨ç”Ÿæˆé«˜ç»†èŠ‚ä½å¹»è§‰çš„å¤šæ¨¡æ€æ•°æ®ï¼›åŸºäºæ­¤è®­ç»ƒäº†Audio-Captionerå’ŒOmni-Captionerä¸¤ä¸ªæè¿°æ¨¡å‹ï¼›è®¾è®¡äº†Omni-Clozeå¡«ç©ºå¼è¯„ä¼°åŸºå‡†ç”¨äºç¨³å®šå¯é çš„ç»†ç²’åº¦æ„ŸçŸ¥è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> Audio-Captioneråœ¨MMAUå’ŒMMARåŸºå‡†ä¸Šè¶…è¶Šæ‰€æœ‰å¼€æºæ¨¡å‹ï¼Œæ€§èƒ½åª²ç¾Gemini 2.5 Proï¼›Omni-Captioneråœ¨VDCåŸºå‡†ä¸Šè¾¾åˆ°æ–°SOTAï¼Œåœ¨video-SALMONN 2æµ‹è¯•é›†ä¸Šå®ç°ç»†èŠ‚ä¸å¹»è§‰çš„æœ€ä½³å¹³è¡¡ï¼›Omni-Clozeè¯„ä¼°æ˜¾ç¤ºå‡ºä¼˜è¶Šçš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Conclusion:</strong> Omni-Detectiveèƒ½æœ‰æ•ˆç”Ÿæˆé«˜è´¨é‡ç»†ç²’åº¦æè¿°æ•°æ®ï¼ŒOmni-Clozeä¸ºå…¨æ¨¡æ€ç»†ç²’åº¦æ„ŸçŸ¥æä¾›äº†å¯é çš„è¯„ä¼°æ¡†æ¶ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€ç†è§£ç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®ç”Ÿæˆå’Œè¯„ä¼°æ–¹æ³•è®ºåŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="41-hicotrajzero-shot-demographic-reasoning-via-hierarchical-chain-of-thought-prompting-from-trajectory">[41] <a href="https://arxiv.org/abs/2510.12067">HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory</a></h3>
<p><em>Junyi Xie, Yuankun Jiao, Jina Kim, Yao-Yi Chiang, Lingyi Zhao, Khurram Shafique</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†HiCoTrajæ¡†æ¶ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬å­¦ä¹ å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œæ— éœ€æ ‡æ³¨è®­ç»ƒæ•°æ®å³å¯ä»äººç±»ç§»åŠ¨è½¨è¿¹ä¸­æ¨æ–­äººå£ç»Ÿè®¡å±æ€§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®å’Œæ³›åŒ–æ€§å·®çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºç§»åŠ¨è½¨è¿¹çš„äººå£ç»Ÿè®¡æ¨æ–­ç ”ç©¶ä¸¥é‡ä¾èµ–å¸¦æœ‰äººå£ç»Ÿè®¡æ ‡ç­¾çš„å¤§è§„æ¨¡è½¨è¿¹æ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹å¯è§£é‡Šæ€§æœ‰é™ä¸”åœ¨ä¸åŒæ•°æ®é›†å’Œç”¨æˆ·ç¾¤ä½“é—´æ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œè¿™é™åˆ¶äº†è¯¥æ–¹æ³•åœ¨å…¬å…±å«ç”Ÿå¹²é¢„ã€åŸå¸‚è§„åˆ’ç­‰å…³é”®åº”ç”¨ä¸­çš„å®é™…éƒ¨ç½²ã€‚</p>
<p><strong>Method:</strong> HiCoTrajæ¡†æ¶é€šè¿‡å°†è½¨è¿¹è½¬æ¢ä¸ºè¯­ä¹‰ä¸°å¯Œçš„è‡ªç„¶è¯­è¨€è¡¨ç¤ºï¼ŒåŒ…æ‹¬è¯¦ç»†çš„æ´»åŠ¨ç¼–å¹´å²å’Œå¤šå°ºåº¦è®¿é—®æ‘˜è¦ï¼Œç„¶åé‡‡ç”¨æ–°é¢–çš„åˆ†å±‚æ€ç»´é“¾æ¨ç†æ–¹æ³•ï¼Œç³»ç»Ÿå¼•å¯¼LLMsé€šè¿‡ä¸‰ä¸ªè®¤çŸ¥é˜¶æ®µï¼šäº‹å®ç‰¹å¾æå–ã€è¡Œä¸ºæ¨¡å¼åˆ†æä»¥åŠç»“æ„åŒ–è¾“å‡ºçš„äººå£ç»Ÿè®¡æ¨æ–­ã€‚</p>
<p><strong>Result:</strong> åœ¨çœŸå®ä¸–ç•Œè½¨è¿¹æ•°æ®ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒHiCoTrajåœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹å¯¹å¤šä¸ªäººå£ç»Ÿè®¡å±æ€§å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®æƒ…å†µä¸‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§æ— éœ€æ ‡æ³¨è®­ç»ƒæ•°æ®çš„äººå£ç»Ÿè®¡æ¨æ–­æ–°èŒƒå¼ï¼Œé€šè¿‡é€æ˜åŒ–çš„æ¨ç†é“¾æ¡è§£å†³äº†æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œä¸ºåŸºäºç§»åŠ¨è½¨è¿¹çš„æ™ºèƒ½åº”ç”¨å¼€è¾Ÿäº†æ›´å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®éšç§ä¿æŠ¤å’Œè·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>Inferring demographic attributes such as age, sex, or income level from human
mobility patterns enables critical applications such as targeted public health
interventions, equitable urban planning, and personalized transportation
services. Existing mobility-based demographic inference studies heavily rely on
large-scale trajectory data with demographic labels, leading to limited
interpretability and poor generalizability across different datasets and user
groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical
Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs'
zero-shot learning and semantic understanding capabilities to perform
demographic inference without labeled training data. HiCoTraj transforms
trajectories into semantically rich, natural language representations by
creating detailed activity chronicles and multi-scale visiting summaries. Then
HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically
guide LLMs through three cognitive stages: factual feature extraction,
behavioral pattern analysis, and demographic inference with structured output.
This approach addresses the scarcity challenge of labeled demographic data
while providing transparent reasoning chains. Experimental evaluation on
real-world trajectory data demonstrates that HiCoTraj achieves competitive
performance across multiple demographic attributes in zero-shot scenarios.</p>
<h3 id="42-matscibench-benchmarking-the-reasoning-ability-of-large-language-models-in-materials-science">[42] <a href="https://arxiv.org/abs/2510.12171">MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science</a></h3>
<p><em>Junkai Zhang, Jingru Gan, Xiaoxuan Wang, Zian Jia, Changquan Gu, Jianpeng Chen, Yanqiao Zhu, Mingyu Derek Ma, Dawei Zhou, Ling Li, Wei Wang</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†MatSciBenchï¼Œä¸€ä¸ªåŒ…å«1340ä¸ªé—®é¢˜çš„ç»¼åˆæ€§å¤§å­¦æ°´å¹³ææ–™ç§‘å­¦åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ææ–™ç§‘å­¦é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸåŸºå‡†æµ‹è¯•çš„ç©ºç™½ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨ææ–™ç§‘å­¦é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œç°æœ‰ç ”ç©¶ç¼ºä¹ä¸“é—¨é’ˆå¯¹ææ–™ç§‘å­¦é¢†åŸŸçš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å’Œæ¨åŠ¨æ¨¡å‹èƒ½åŠ›çš„å‘å±•ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†MatSciBenchåŸºå‡†æµ‹è¯•ï¼Œé‡‡ç”¨ç»“æ„åŒ–ç»†ç²’åº¦åˆ†ç±»æ³•å°†ææ–™ç§‘å­¦é—®é¢˜åˆ’åˆ†ä¸º6ä¸ªä¸»è¦é¢†åŸŸå’Œ31ä¸ªå­é¢†åŸŸï¼ŒåŒ…å«åŸºäºæ¨ç†é•¿åº¦çš„ä¸‰çº§éš¾åº¦åˆ†ç±»ï¼Œå¹¶æä¾›è¯¦ç»†å‚è€ƒè§£å†³æ–¹æ¡ˆæ”¯æŒç²¾ç¡®é”™è¯¯åˆ†æï¼ŒåŒæ—¶é€šè¿‡è§†è§‰ä¸Šä¸‹æ–‡æ•´åˆå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æ€§èƒ½æœ€é«˜çš„Gemini-2.5-Proæ¨¡å‹åœ¨å¤§å­¦æ°´å¹³ææ–™ç§‘å­¦é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿä½äº80%ï¼Œç³»ç»Ÿåˆ†æè¡¨æ˜åŸºç¡€æ€ç»´é“¾ã€å·¥å…·å¢å¼ºå’Œè‡ªæ ¡æ­£ç­‰ä¸åŒæ¨ç†ç­–ç•¥åœ¨ä¸åŒåœºæ™¯ä¸‹è¡¨ç°å„å¼‚ï¼Œæ²¡æœ‰å•ä¸€æ–¹æ³•åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> MatSciBenchä¸ºè¯„ä¼°å’Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ææ–™ç§‘å­¦é¢†åŸŸçš„ç§‘å­¦æ¨ç†èƒ½åŠ›å»ºç«‹äº†å…¨é¢å¯é çš„åŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤æ‚ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦å‚è€ƒæ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>Large Language Models (LLMs) have demonstrated remarkable abilities in
scientific reasoning, yet their reasoning capabilities in materials science
remain underexplored. To fill this gap, we introduce MatSciBench, a
comprehensive college-level benchmark comprising 1,340 problems that span the
essential subdisciplines of materials science. MatSciBench features a
structured and fine-grained taxonomy that categorizes materials science
questions into 6 primary fields and 31 sub-fields, and includes a three-tier
difficulty classification based on the reasoning length required to solve each
question. MatSciBench provides detailed reference solutions enabling precise
error analysis and incorporates multimodal reasoning through visual contexts in
numerous questions. Evaluations of leading models reveal that even the
highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on
college-level materials science questions, highlighting the complexity of
MatSciBench. Our systematic analysis of different reasoning strategie--basic
chain-of-thought, tool augmentation, and self-correction--demonstrates that no
single method consistently excels across all scenarios. We further analyze
performance by difficulty level, examine trade-offs between efficiency and
accuracy, highlight the challenges inherent in multimodal reasoning tasks,
analyze failure modes across LLMs and reasoning methods, and evaluate the
influence of retrieval-augmented generation. MatSciBench thus establishes a
comprehensive and solid benchmark for assessing and driving improvements in the
scientific reasoning capabilities of LLMs within the materials science domain.</p>
<h3 id="43-evolution-of-metas-llama-models-and-parameter-efficient-fine-tuning-of-large-language-models-a-survey">[43] <a href="https://arxiv.org/abs/2510.12178">Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey</a></h3>
<p><em>Abdulhady Abas Abdullah, Arkaitz Zubiaga, Seyedali Mirjalili, Amir H. Gandomi, Fatemeh Daneshfar, Mohammadsadra Amini, Alan Salam Mohammed, Hadi Veisi</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ç»¼è¿°äº†Meta AIçš„LLaMAç³»åˆ—æ¨¡å‹ä»LLaMA 1åˆ°LLaMA 4çš„å¿«é€Ÿæ¼”è¿›ï¼Œä»¥åŠä¸ºè¿™äº›æ¨¡å‹å¼€å‘çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œæä¾›äº†å…³äºæ¨¡å‹æ¶æ„ã€æ€§èƒ½ç‰¹å¾å’Œé«˜æ•ˆå¾®è°ƒç­–ç•¥çš„å…¨é¢èµ„æºã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹é«˜æ•ˆé€‚åº”ç‰¹å®šä»»åŠ¡çš„éœ€æ±‚ï¼Œé€šè¿‡ç³»ç»Ÿæ¢³ç†LLaMAç³»åˆ—åŸºç¡€æ¨¡å‹çš„å‘å±•å†ç¨‹å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œä¸ºç ”ç©¶è€…å’Œå®è·µè€…æä¾›ä¸€ç«™å¼å‚è€ƒèµ„æºã€‚</p>
<p><strong>Method:</strong> è®ºæ–‡ç³»ç»Ÿåˆ†æäº†LLaMAç³»åˆ—åŸºç¡€æ¨¡å‹æ¶æ„ï¼ˆåŒ…æ‹¬å¤šæ¨¡æ€å’Œä¸“å®¶æ··åˆå˜ä½“ï¼‰ä»¥åŠäº”ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼šLoRAã€LLaMA-Adapter V1å’ŒV2ã€LLaMA-Excitorå’ŒQLoRAï¼Œé‡ç‚¹å…³æ³¨è¿™äº›æ–¹æ³•çš„æœºåˆ¶ã€å‚æ•°èŠ‚çœå’Œåº”ç”¨åœºæ™¯ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶æä¾›äº†ç»“æ„åŒ–çš„æ¨¡å‹å’Œé€‚é…å™¨æ¶æ„åˆ†æã€å‚æ•°æ•°é‡ç»Ÿè®¡ä»¥åŠåŸºå‡†æµ‹è¯•ç»“æœï¼ŒåŒ…æ‹¬å¾®è°ƒåLLaMAæ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šæ›´å¤§åŸºçº¿æ¨¡å‹çš„å®ä¾‹ï¼Œå¹¶å±•ç¤ºäº†åœ¨æ³•å¾‹å’ŒåŒ»ç–—ç­‰å®é™…åº”ç”¨åœºæ™¯ä¸­çš„æˆåŠŸæ¡ˆä¾‹ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç»¼è¿°æ­ç¤ºäº†LLaMAæ¨¡å‹å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼ï¼ŒåŒæ—¶æŒ‡å‡ºäº†æ‰©å±•åˆ°æ›´å¤§ä¸Šä¸‹æ–‡å’Œæå‡é²æ£’æ€§ç­‰æŒç»­æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦è§è§£ã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>This review surveys the rapid evolution of Meta AI's LLaMA (Large Language
Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized
parameter-efficient fine-tuning (PEFT) methods developed for these models. We
first describe the LLaMA family of foundation models (7B-65B to 288B
parameters), their architectures (including native multimodal and
Mixtureof-Experts variants), and key performance characteristics. We then
describe and discuss the concept of PEFT, which adapts large pre-trained models
by updating only a small subset of parameters, and review five PEFT methods
that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1
and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's
mechanism, parameter savings, and example application to LLaMA (e.g.,
instruction tuning, multimodal tasks). We provide structured discussion and
analysis of model and adapter architectures, parameter counts, and benchmark
results (including examples where fine-tuned LLaMA models outperform larger
baselines). Finally, we examine real-world use cases where LLaMA-based models
and PEFT have been successfully applied (e.g., legal and medical domains), and
we discuss ongoing challenges and future research directions (such as scaling
to even larger contexts and improving robustness). This survey paper provides a
one-stop resource for ML researchers and practitioners interested in LLaMA
models and efficient fine-tuning strategies.</p>
<h3 id="44-goat-a-training-framework-for-goal-oriented-agent-with-tools">[44] <a href="https://arxiv.org/abs/2510.12218">GOAT: A Training Framework for Goal-Oriented Agent with Tools</a></h3>
<p><em>Hyunji Min, Sangwon Jung, Junyoung Sung, Dosung Lee, Leekyeung Han, Paul Hongsuck Seo</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†GOATè®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹å¾®è°ƒLLMä»£ç†ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ç›®æ ‡å¯¼å‘çš„APIæ‰§è¡Œä»»åŠ¡ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰LLMä»£ç†åœ¨å¤„ç†ç›®æ ‡å¯¼å‘æŸ¥è¯¢æ—¶èƒ½åŠ›æœ‰é™ï¼Œéœ€è¦å°†é«˜çº§ç›®æ ‡åˆ†è§£ä¸ºå¤šä¸ªç›¸äº’ä¾èµ–çš„APIè°ƒç”¨å¹¶è¿›è¡Œæ­£ç¡®è§„åˆ’å’Œæ‰§è¡Œï¼Œè€Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–é›¶æ ·æœ¬è¯„ä¼°ä¸”ç¼ºä¹è®­ç»ƒæ•°æ®ï¼Œç‰¹åˆ«æ˜¯å¼€æºå°æ¨¡å‹åœ¨å¤æ‚å·¥å…·ä½¿ç”¨æ–¹é¢è¡¨ç°ä¸ä½³ã€‚</p>
<p><strong>Method:</strong> GOATæ¡†æ¶é€šè¿‡ä»ç»™å®šçš„APIæ–‡æ¡£è‡ªåŠ¨æ„å»ºç›®æ ‡å¯¼å‘APIæ‰§è¡Œä»»åŠ¡çš„åˆæˆæ•°æ®é›†ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯è®­ç»ƒLLMä»£ç†ï¼Œä½¿æ¨¡å‹å…·å¤‡å¯¹ç›¸äº’ä¾èµ–è°ƒç”¨çš„æ¨ç†èƒ½åŠ›å¹¶ç”Ÿæˆè¿è´¯å“åº”ã€‚</p>
<p><strong>Result:</strong> ç»è¿‡GOATè®­ç»ƒçš„ä»£ç†åœ¨å¤šä¸ªç°æœ‰ç›®æ ‡å¯¼å‘åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶åœ¨æ–°æå‡ºçš„GOATBenchåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> GOATä¸ºæ„å»ºå…·æœ‰å¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›çš„ç¨³å¥å¼€æºLLMä»£ç†æä¾›äº†ä¸€æ¡å®ç”¨è·¯å¾„ï¼Œé€šè¿‡è‡ªåŠ¨æ•°æ®ç”Ÿæˆå’Œè®­ç»ƒæ¡†æ¶è§£å†³äº†ç›®æ ‡å¯¼å‘ä»»åŠ¡ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>Large language models (LLMs) have recently been extended beyond traditional
text generation to serve as interactive agents capable of using external tools
based on user intent. However, current LLM agents still show limited ability to
handle goal-oriented queries, which require decomposing a high-level objective
into multiple interdependent API calls with correct planning and execution.
Current approaches mainly rely on zero-shot evaluation due to the absence of
training data. While proprietary closed-source models such as GPT-4 demonstrate
strong reasoning abilities, smaller open-source models struggle to perform
complex tool use effectively. Thus, we propose a novel training framework GOAT,
which enables fine-tuning of LLM agents in a human annotation-free setting.
GOAT automatically constructs synthetic datasets of goal-oriented API execution
tasks directly from given API documents, equipping models with the ability to
reason over interdependent calls and generate coherent responses. Through
extensive experiments, we show that GOAT-trained agents achieve
state-of-the-art performance across multiple existing goal-oriented benchmarks.
In addition, we introduce GOATBench, a new goal-oriented API execution
benchmark, and demonstrate that agents trained with GOAT also excel in this
setting. These results highlight GOAT as a practical path toward building
robust open-source LLM agents capable of complex reasoning and tool use.</p>
<h3 id="45-rag-anything-all-in-one-rag-framework">[45] <a href="https://arxiv.org/abs/2510.12323">RAG-Anything: All-in-One RAG Framework</a></h3>
<p><em>Zirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>RAG-Anythingæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å°†å¤šæ¨¡æ€å†…å®¹é‡æ–°æ¦‚å¿µåŒ–ä¸ºç›¸äº’è¿æ¥çš„çŸ¥è¯†å®ä½“ï¼Œè§£å†³äº†ç°æœ‰RAGç³»ç»Ÿä»…é™äºæ–‡æœ¬å†…å®¹è€Œæ— æ³•å¤„ç†å¤šæ¨¡æ€æ–‡æ¡£çš„æ ¹æœ¬é™åˆ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿä¸çœŸå®ä¸–ç•Œä¿¡æ¯ç¯å¢ƒå­˜åœ¨ä¸¥é‡é”™é…ï¼Œç°ä»£çŸ¥è¯†åº“æœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€çš„ï¼ŒåŒ…å«æ–‡æœ¬å†…å®¹ã€è§†è§‰å…ƒç´ ã€ç»“æ„åŒ–è¡¨æ ¼å’Œæ•°å­¦è¡¨è¾¾å¼çš„ä¸°å¯Œç»„åˆï¼Œä½†ç°æœ‰RAGæ¡†æ¶ä»…é™äºæ–‡æœ¬å†…å®¹ï¼Œåœ¨å¤„ç†å¤šæ¨¡æ€æ–‡æ¡£æ—¶äº§ç”Ÿæ ¹æœ¬æ€§å·®è·ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¡†æ¶å¼•å…¥åŒå›¾æ„å»ºæ¥æ•æ‰è·¨æ¨¡æ€å…³ç³»å’Œæ–‡æœ¬è¯­ä¹‰çš„ç»Ÿä¸€è¡¨ç¤ºï¼Œå¼€å‘äº†ç»“åˆç»“æ„çŸ¥è¯†å¯¼èˆªå’Œè¯­ä¹‰åŒ¹é…çš„è·¨æ¨¡æ€æ··åˆæ£€ç´¢æ–¹æ³•ï¼Œå°†å¤šæ¨¡æ€å†…å®¹é‡æ–°æ¦‚å¿µåŒ–ä¸ºç›¸äº’è¿æ¥çš„çŸ¥è¯†å®ä½“è€Œéå­¤ç«‹çš„æ•°æ®ç±»å‹ã€‚</p>
<p><strong>Result:</strong> RAG-Anythingåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œç›¸æ¯”æœ€å…ˆè¿›æ–¹æ³•å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¼ ç»Ÿæ–¹æ³•å¤±æ•ˆçš„é•¿æ–‡æ¡£ä¸Šæ€§èƒ½æå‡å°¤ä¸ºæ˜æ˜¾ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ¡†æ¶å»ºç«‹äº†å¤šæ¨¡æ€çŸ¥è¯†è®¿é—®çš„æ–°èŒƒå¼ï¼Œæ¶ˆé™¤äº†å½“å‰ç³»ç»Ÿçš„æ¶æ„ç¢ç‰‡åŒ–é™åˆ¶ï¼Œä¸ºå¤„ç†è·¨æ¨¡æ€çš„å¼‚æ„å†…å®¹æä¾›äº†æœ‰æ•ˆçš„æ¨ç†èƒ½åŠ›ï¼Œå…¶ä¸­ç›¸å…³è¯æ®å¯èƒ½è·¨è¶Šå¤šä¸ªæ¨¡æ€ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm
for expanding Large Language Models beyond their static training limitations.
However, a critical misalignment exists between current RAG capabilities and
real-world information environments. Modern knowledge repositories are
inherently multimodal, containing rich combinations of textual content, visual
elements, structured tables, and mathematical expressions. Yet existing RAG
frameworks are limited to textual content, creating fundamental gaps when
processing multimodal documents. We present RAG-Anything, a unified framework
that enables comprehensive knowledge retrieval across all modalities. Our
approach reconceptualizes multimodal content as interconnected knowledge
entities rather than isolated data types. The framework introduces dual-graph
construction to capture both cross-modal relationships and textual semantics
within a unified representation. We develop cross-modal hybrid retrieval that
combines structural knowledge navigation with semantic matching. This enables
effective reasoning over heterogeneous content where relevant evidence spans
multiple modalities. RAG-Anything demonstrates superior performance on
challenging multimodal benchmarks, achieving significant improvements over
state-of-the-art methods. Performance gains become particularly pronounced on
long documents where traditional approaches fail. Our framework establishes a
new paradigm for multimodal knowledge access, eliminating the architectural
fragmentation that constrains current systems. Our framework is open-sourced
at: https://github.com/HKUDS/RAG-Anything.</p>
<h3 id="46-artificial-intelligence-virtual-cells-from-measurements-to-decisions-across-modality-scale-dynamics-and-evaluation">[46] <a href="https://arxiv.org/abs/2510.12498">Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation</a></h3>
<p><em>Chengpeng Hu, Calvin Yu-Chian Chen</em></p>
<h4 id="tldr_45">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ç»†èƒçŠ¶æ€æ½œåœ¨ï¼ˆCSLï¼‰è§†è§’ï¼Œé€šè¿‡æ“ä½œç¬¦è¯­æ³•ç»„ç»‡å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶å»ºç«‹äº†è·¨æ¨¡æ€ã€å°ºåº¦ã€æƒ…å¢ƒå’Œå¹²é¢„çš„å†³ç­–å¯¹é½è¯„ä¼°è“å›¾ï¼Œä»¥è§£å†³äººå·¥æ™ºèƒ½è™šæ‹Ÿç»†èƒåœ¨è·¨å®éªŒå®¤å¯è¿ç§»æ€§å’Œè·¨å°ºåº¦è€¦åˆæ–¹é¢çš„æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_45">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰äººå·¥æ™ºèƒ½è™šæ‹Ÿç»†èƒç ”ç©¶é¢ä¸´è·¨å®éªŒå®¤å’Œå¹³å°çš„å¯è¿ç§»æ€§å—é™ã€æ•°æ®åˆ†å‰²å­˜åœ¨æ³„æ¼å’Œè¦†ç›–åå·®é£é™©ã€å‰‚é‡æ—¶é—´å’Œç»„åˆæ•ˆåº”ç¼ºä¹ç³»ç»Ÿå¤„ç†ç­‰é—®é¢˜ï¼ŒåŒæ—¶åˆ†å­ã€ç»†èƒå’Œç»„ç»‡æ°´å¹³ä¹‹é—´çš„è·¨å°ºåº¦è€¦åˆä»ç„¶å—é™ï¼Œä¸ç§‘å­¦æˆ–ä¸´åºŠè¯»æ•°çš„å¯¹é½åœ¨ä¸åŒç ”ç©¶ä¸­å­˜åœ¨å·®å¼‚ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†æ¨¡å‹æ— å…³çš„ç»†èƒçŠ¶æ€æ½œåœ¨ï¼ˆCSLï¼‰è§†è§’ï¼Œé€šè¿‡æ“ä½œç¬¦è¯­æ³•ç»„ç»‡å­¦ä¹ è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æµ‹é‡ã€è·¨å°ºåº¦è€¦åˆçš„å‡é™æŠ•å½±ä»¥åŠå‰‚é‡å’Œè°ƒåº¦çš„å¹²é¢„æ“ä½œï¼Œå¼ºè°ƒåŠŸèƒ½ç©ºé—´è¯»æ•°å¦‚é€šè·¯æ´»æ€§ã€ç©ºé—´é‚»åŸŸå’Œä¸´åºŠç›¸å…³ç»ˆç‚¹ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å»ºç«‹äº†å†³ç­–å¯¹é½çš„è¯„ä¼°è“å›¾ï¼Œæ¶µç›–æ¨¡æ€ã€å°ºåº¦ã€æƒ…å¢ƒå’Œå¹²é¢„å››ä¸ªç»´åº¦ï¼Œå¹¶æ¨èæ“ä½œç¬¦æ„ŸçŸ¥çš„æ•°æ®è®¾è®¡ã€æŠ—æ³„æ¼åˆ†åŒºä»¥åŠé€æ˜æ ¡å‡†å’ŒæŠ¥å‘Šæ–¹æ³•ï¼Œä»¥å®ç°å¯é‡å¤çš„åŒç±»æ¯”è¾ƒã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºäººå·¥æ™ºèƒ½è™šæ‹Ÿç»†èƒçš„å‘å±•æä¾›äº†ç³»ç»Ÿæ€§çš„è¯„ä¼°æ¡†æ¶å’Œæ–¹æ³•è®ºæŒ‡å¯¼ï¼Œå¼ºè°ƒé€šè¿‡æ ‡å‡†åŒ–çš„æ“ä½œç¬¦è¯­æ³•å’Œå†³ç­–å¯¹é½è¯„ä¼°æ¥æå‡æ¨¡å‹çš„å¯è¿ç§»æ€§ã€å¯è§£é‡Šæ€§å’Œä¸´åºŠç›¸å…³æ€§ï¼Œä¸ºæœªæ¥ç»†èƒçŠ¶æ€å»ºæ¨¡ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_45">ğŸ“„ Abstract</h4>
<p>Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,
decision-relevant models of cell state from multimodal, multiscale
measurements. Recent studies have introduced single-cell and spatial foundation
models, improved cross-modality alignment, scaled perturbation atlases, and
explored pathway-level readouts. Nevertheless, although held-out validation is
standard practice, evaluations remain predominantly within single datasets and
settings; evidence indicates that transport across laboratories and platforms
is often limited, that some data splits are vulnerable to leakage and coverage
bias, and that dose, time and combination effects are not yet systematically
handled. Cross-scale coupling also remains constrained, as anchors linking
molecular, cellular and tissue levels are sparse, and alignment to scientific
or clinical readouts varies across studies. We propose a model-agnostic
Cell-State Latent (CSL) perspective that organizes learning via an operator
grammar: measurement, lift/project for cross-scale coupling, and intervention
for dosing and scheduling. This view motivates a decision-aligned evaluation
blueprint across modality, scale, context and intervention, and emphasizes
function-space readouts such as pathway activity, spatial neighborhoods and
clinically relevant endpoints. We recommend operator-aware data design,
leakage-resistant partitions, and transparent calibration and reporting to
enable reproducible, like-for-like comparisons.</p>
  </article>
</body>
</html>
