<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-05.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 48]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 2]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 7]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-text-only-training-for-image-captioning-with-retrieval-augmentation-and-modality-gap-correction">[1] <a href="https://arxiv.org/abs/2512.04309">Text-Only Training for Image Captioning with Retrieval Augmentation and Modality Gap Correction</a></h3>
<p><em>Rui Fonseca, Bruno Martins, Gil Rocha</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†TOMCapï¼Œä¸€ç§æ”¹è¿›çš„ä»…æ–‡æœ¬è®­ç»ƒæ–¹æ³•ï¼Œæ— éœ€å¯¹é½çš„å›¾åƒ-æ ‡é¢˜å¯¹å³å¯æ‰§è¡Œå›¾åƒæè¿°ç”Ÿæˆã€‚è¯¥æ–¹æ³•é€šè¿‡å‡å°‘æ¨¡æ€é—´éš™çš„CLIPè¡¨ç¤ºæ¥æç¤ºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¹¶ç»“åˆæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œåœ¨è®­ç»ƒè‡ªç”±å’Œä»…æ–‡æœ¬æ–¹æ³•ä¸­å–å¾—äº†ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å›¾åƒæè¿°ç”Ÿæˆç ”ç©¶ä¸»è¦ä¾èµ–äººå·¥æ ‡æ³¨çš„å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®ï¼Œç°æœ‰æ— éœ€å¯¹é½å›¾åƒ-æ ‡é¢˜å¯¹çš„è®­ç»ƒæ–¹æ³•æ€§èƒ½ä»è½åäºå®Œå…¨ç›‘ç£æ–¹æ³•ã€‚æœ¬æ–‡æ—¨åœ¨å‡å°‘å¯¹ç²¾å¿ƒç­–åˆ’æ•°æ®çš„ä¾èµ–ï¼Œæ¢ç´¢æ— éœ€äººç±»æ ‡æ³¨å›¾åƒ-æ–‡æœ¬å¯¹çš„å›¾åƒæè¿°ç”Ÿæˆæ–¹æ³•ï¼Œä»¥è§£å†³æ•°æ®ä¾èµ–æ€§é—®é¢˜å¹¶æå‡è®­ç»ƒè‡ªç”±æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> TOMCapæ–¹æ³•åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è§£ç å™¨ï¼Œé€šè¿‡ç»è¿‡æ¨¡æ€é—´éš™å‡å°‘å¤„ç†çš„CLIPè¡¨ç¤ºè¿›è¡Œæç¤ºã€‚è¯¥æ–¹æ³•ç»“åˆæ£€ç´¢åˆ°çš„æ ‡é¢˜ç¤ºä¾‹å’Œæ½œåœ¨å‘é‡è¡¨ç¤ºæ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œå…·ä½“åŒ…æ‹¬æ£€ç´¢å¢å¼ºç»„ä»¶å’Œæ¨¡æ€é—´éš™å‡å°‘ç»„ä»¶çš„é…ç½®é€‰æ‹©ï¼Œå®ç°äº†æ— éœ€å¯¹é½å›¾åƒ-æ ‡é¢˜å¯¹çš„ä»…æ–‡æœ¬è®­ç»ƒã€‚</p>
<p><strong>Result:</strong> é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼ŒTOMCapåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å…¶ä»–è®­ç»ƒè‡ªç”±å’Œä»…æ–‡æœ¬æ–¹æ³•ã€‚ç ”ç©¶è¿˜åˆ†æäº†æ£€ç´¢å¢å¼ºå’Œæ¨¡æ€é—´éš™å‡å°‘ç»„ä»¶ä¸åŒé…ç½®é€‰æ‹©çš„å½±å“ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ— éœ€å¯¹é½å›¾åƒ-æ ‡é¢˜å¯¹æƒ…å†µä¸‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè®­ç»ƒè‡ªç”±å›¾åƒæè¿°ç”Ÿæˆæä¾›äº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç»“åˆCLIPè¡¨ç¤ºã€æ¨¡æ€é—´éš™å‡å°‘å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œå¯ä»¥åœ¨æ— éœ€å¯¹é½å›¾åƒ-æ ‡é¢˜å¯¹çš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆçš„å›¾åƒæè¿°ç”Ÿæˆã€‚TOMCapä¸ºå‡å°‘æ•°æ®ä¾èµ–æ€§çš„å›¾åƒæè¿°ç”Ÿæˆæä¾›äº†æ–°æ€è·¯ï¼Œå±•ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹ä¸æ£€ç´¢å¢å¼ºç»“åˆåœ¨è®­ç»ƒè‡ªç”±åœºæ™¯ä¸­çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥æ— ç›‘ç£å¤šæ¨¡æ€å­¦ä¹ ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Image captioning has drawn considerable attention from the natural language processing and computer vision fields. Aiming to reduce the reliance on curated data, several studies have explored image captioning without any humanly-annotated image-text pairs for training, although existing methods are still outperformed by fully supervised approaches. This paper proposes TOMCap, i.e., an improved text-only training method that performs captioning without the need for aligned image-caption pairs. The method is based on prompting a pre-trained language model decoder with information derived from a CLIP representation, after undergoing a process to reduce the modality gap. We specifically tested the combined use of retrieved examples of captions, and latent vector representations, to guide the generation process. Through extensive experiments, we show that TOMCap outperforms other training-free and text-only methods. We also analyze the impact of different choices regarding the configuration of the retrieval-augmentation and modality gap reduction components.</p>
<h3 id="2-mitigating-object-and-action-hallucinations-in-multimodal-llms-via-self-augmented-contrastive-alignment">[2] <a href="https://arxiv.org/abs/2512.04356">Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment</a></h3>
<p><em>Kai-Po Chang, Wei-Yuan Cheng, Chi-Pin Huang, Fu-En Yang, Yu-Chiang Frank Wang</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSANTAçš„è‡ªå¢å¼ºå¯¹æ¯”å¯¹é½æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æè¿°ç”Ÿæˆä¸­çš„äº‹å®æ€§ä¸å‡†ç¡®é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è§†è§‰å¯¹è±¡å’Œæ—¶é—´åŠ¨ä½œçš„å¹»è§‰ç°è±¡ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æè¿°ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶ç”Ÿæˆæè¿°ä¸­å­˜åœ¨ä¸¥é‡çš„äº‹å®æ€§ä¸å‡†ç¡®é—®é¢˜ï¼Œå¯¼è‡´å¹»è§‰ç°è±¡é¢‘å‘ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨é™æ€å›¾åƒçš„å¹»è§‰ç¼“è§£ï¼Œè€Œé’ˆå¯¹åŠ¨æ€è§†é¢‘ä¸­è§†è§‰å¯¹è±¡å’Œæ—¶é—´åŠ¨ä½œçš„è”åˆå¹»è§‰ç¼“è§£ä»æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ä¸”æœªè§£å†³çš„ä»»åŠ¡ã€‚</p>
<p><strong>Method:</strong> SANTAæ¡†æ¶é‡‡ç”¨è‡ªå¢å¼ºå¯¹æ¯”å¯¹é½æ–¹æ³•ï¼Œé€šè¿‡å¹»è§‰æ€§è‡ªå¢å¼ºæ–¹æ¡ˆè¯†åˆ«MLLMä¸­æ½œåœ¨çš„å¹»è§‰å†…å®¹ï¼Œå¹¶å°†åŸå§‹æè¿°è½¬æ¢ä¸ºå¯¹æ¯”è´Ÿæ ·æœ¬ã€‚åŒæ—¶ï¼Œå¼€å‘äº†è½¨è¿¹-çŸ­è¯­å¯¹æ¯”å¯¹é½æœºåˆ¶ï¼Œå°†åŒºåŸŸå¯¹è±¡å’Œå…³ç³»å¼•å¯¼çš„åŠ¨ä½œä¸å…¶å¯¹åº”çš„è§†è§‰å’Œæ—¶é—´çŸ­è¯­è¿›è¡ŒåŒ¹é…ï¼Œä»è€Œæ¶ˆé™¤è™šå‡ç›¸å…³æ€§å¹¶å¼ºåŒ–å¯¹è§†è§‰äº‹å®çš„å…³æ³¨ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSANTAåœ¨ç¼“è§£å¯¹è±¡å’ŒåŠ¨ä½œå¹»è§‰æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¹»è§‰æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜è¶Šæ€§èƒ½ï¼Œæœ‰æ•ˆæå‡äº†è§†é¢‘æè¿°ç”Ÿæˆçš„äº‹å®å‡†ç¡®æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè§£å†³è§†é¢‘æè¿°ç”Ÿæˆä¸­çš„å¹»è§‰é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„æ¡†æ¶ï¼Œé€šè¿‡è‡ªå¢å¼ºå¯¹æ¯”å¯¹é½æœºåˆ¶å®ç°äº†å¯¹è§†è§‰å¯¹è±¡å’Œæ—¶é—´åŠ¨ä½œçš„è”åˆä¼˜åŒ–ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§æ”¹è¿›å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºå’Œåº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.</p>
<h3 id="3-onsight-pathology-a-real-time-platform-agnostic-computational-pathology-companion-for-histopathology">[3] <a href="https://arxiv.org/abs/2512.04187">OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology</a></h3>
<p><em>Jinzhen Hu, Kevin Faust, Parsa Babaei Zadeh, Adrienn Bourkas, Shane Eaton, Andrew Young, Anzar Alvi, Dimitrios George Oreopoulos, Ameesha Paliwal, Assem Saleh Alrumeh, Evelyn Rose Kamski-Hennekam, Phedias Diamandis</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†OnSight Pathologyï¼Œä¸€ä¸ªå¹³å°æ— å…³çš„è®¡ç®—æœºè§†è§‰è½¯ä»¶ï¼Œé€šè¿‡è¿ç»­å±å¹•æ•è·åœ¨ç”¨æˆ·å®¡æŸ¥æ•°å­—åˆ‡ç‰‡å›¾åƒæ—¶æä¾›å®æ—¶AIæ¨ç†ï¼Œè§£å†³äº†æ•°å­—ç—…ç†å­¦ä¸­AIå·¥å…·éƒ¨ç½²çš„éšœç¢ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿç»„ç»‡å­¦æ£€æŸ¥ä¾èµ–ä¸»è§‚è§£é‡Šå’Œä¸“å®¶ç»éªŒï¼Œå½±å“å‡†ç¡®æ€§å’Œä¸´åºŠæŠ¤ç†ï¼Œè€Œç°æœ‰æ•°å­—ç—…ç†è§£å†³æ–¹æ¡ˆå­˜åœ¨ä¸“æœ‰æ€§éšœç¢ï¼Œé™åˆ¶äº†AIå·¥å…·åœ¨ç°å®ä¸–ç•Œä¸­çš„éƒ¨ç½²å’Œåº”ç”¨ã€‚</p>
<p><strong>Method:</strong> OnSight Pathologyé‡‡ç”¨å¹³å°æ— å…³çš„è®¡ç®—æœºè§†è§‰è½¯ä»¶æ¶æ„ï¼Œé€šè¿‡è¿ç»­è‡ªå®šä¹‰å±å¹•æ•è·æŠ€æœ¯æä¾›å®æ—¶AIæ¨ç†ï¼Œä½œä¸ºå•ä¸€å¯æ‰§è¡Œæ–‡ä»¶åœ¨æ¶ˆè´¹çº§ä¸ªäººç”µè„‘ä¸Šæœ¬åœ°è¿è¡Œï¼Œæ— éœ€å¤æ‚è½¯ä»¶é›†æˆï¼Œå¹¶åŒ…å«å¤šæ¨¡æ€èŠå¤©åŠ©æ‰‹è¿›è¡Œå›¾åƒéªŒè¯ã€‚</p>
<p><strong>Result:</strong> åœ¨è¶…è¿‡2,500å¼ å…¬å¼€å¯ç”¨çš„å…¨åˆ‡ç‰‡å›¾åƒå’Œä¸´åºŠæ•°å­—ç—…ç†æ¡ˆä¾‹ä¸­éªŒè¯äº†è½¯ä»¶çš„é²æ£’æ€§ï¼ŒæˆåŠŸåº”ç”¨äºå¸¸è§è„‘è‚¿ç˜¤åˆ†ç±»ã€æœ‰ä¸åˆ†è£‚æ£€æµ‹å’Œå…ç–«ç»„åŒ–æŸ“è‰²å®šé‡ç­‰å¸¸è§„ç»„ç»‡ç—…ç†å­¦ä»»åŠ¡ï¼Œå¹¶å±•ç¤ºäº†ä¸å®æ—¶æ˜¾å¾®é•œæ‘„åƒå¤´ï¼ˆåŒ…æ‹¬æ™ºèƒ½æ‰‹æœºï¼‰çš„å…¼å®¹æ€§ã€‚</p>
<p><strong>Conclusion:</strong> OnSight Pathologyèƒ½å¤Ÿè·¨å¹¿æ³›ç—…ç†å­¦æµç¨‹æä¾›å®æ—¶AIæ¨ç†ï¼Œæ¶ˆé™¤äº†AIå·¥å…·åœ¨ç»„ç»‡ç—…ç†å­¦ä¸­é‡‡ç”¨çš„å…³é”®éšœç¢ï¼Œä¸ºç ”ç©¶ã€ä¸´åºŠå·¥ä½œæµç¨‹ä»¥åŠè¿œç¨‹ç—…ç†å­¦å’Œæœ¯ä¸­è®¾ç½®æä¾›äº†æˆæœ¬æ•ˆç›Šé«˜ä¸”å®‰å…¨çš„éƒ¨ç½²æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>The microscopic examination of surgical tissue remains a cornerstone of disease classification but relies on subjective interpretations and access to highly specialized experts, which can compromise accuracy and clinical care. While emerging breakthroughs in artificial intelligence (AI) offer promise for automated histological analysis, the growing number of proprietary digital pathology solutions has created barriers to real-world deployment. To address these challenges, we introduce OnSight Pathology, a platform-agnostic computer vision software that uses continuous custom screen captures to provide real-time AI inferences to users as they review digital slide images. Accessible as a single, self-contained executable file (https://onsightpathology.github.io/ ), OnSight Pathology operates locally on consumer-grade personal computers without complex software integration, enabling cost-effective and secure deployment in research and clinical workflows. Here we demonstrate the utility of OnSight Pathology using over 2,500 publicly available whole slide images across different slide viewers, as well as cases from our clinical digital pathology setup. The software's robustness is highlighted across routine histopathological tasks, including the classification of common brain tumor types, mitosis detection, and the quantification of immunohistochemical stains. A built-in multi-modal chat assistant provides verifiable descriptions of images, free of rigid class labels, for added quality control. Lastly, we show compatibility with live microscope camera feeds, including from personal smartphones, offering potential for deployment in more analog, inter-operative, and telepathology settings. Together, we highlight how OnSight Pathology can deliver real-time AI inferences across a broad range of pathology pipelines, removing key barriers to the adoption of AI tools in histopathology.</p>
<h3 id="4-season-mitigating-temporal-hallucination-in-video-large-language-models-via-self-diagnostic-contrastive-decoding">[4] <a href="https://arxiv.org/abs/2512.04643">SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding</a></h3>
<p><em>Chang-Hsun Wu, Kai-Po Chang, Yu-Yang Sheng, Hung-Kai Chung, Kuei-Chun Wang, Yu-Chiang Frank Wang</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSEASONï¼ˆè‡ªè¯Šæ–­å¯¹æ¯”è§£ç ï¼‰ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è¯Šæ–­æ¯ä¸ªè¾“å‡ºtokençš„å¹»è§‰å€¾å‘å¹¶å¯¹å…¶å¯¹åº”çš„æ—¶ç©ºè´Ÿæ ·æœ¬è¿›è¡Œè‡ªé€‚åº”å¯¹æ¯”è§£ç ï¼Œæœ‰æ•ˆå¢å¼ºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶ç©ºç»´åº¦ä¸Šçš„å¿ å®æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å“åº”ç”¨æˆ·æŸ¥è¯¢æ—¶ä»éš¾ä»¥æœ‰æ•ˆæ„ŸçŸ¥å’Œåˆ©ç”¨è§†é¢‘ä¸­ä¸°å¯Œçš„æ—¶åºä¿¡æ¯ï¼Œå¯¼è‡´ç”Ÿæˆçš„äº‹ä»¶æè¿°å­˜åœ¨æ—¶åºä¸ä¸€è‡´æˆ–å› æœä¸åˆç†çš„é—®é¢˜ï¼Œå¼•å‘ä¸¥é‡çš„å¹»è§‰é—®é¢˜ã€‚å°½ç®¡å…ˆå‰ç ”ç©¶ä¸»è¦å…³æ³¨ç©ºé—´å¹»è§‰ï¼ˆå¦‚ç‰©ä½“ä¸åŒ¹é…ï¼‰ï¼Œä½†è§†é¢‘ç†è§£ä¸­çš„æ—¶åºæ¨ç†ä»ç›¸å¯¹æœªè¢«å……åˆ†æ¢ç´¢ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºè‡ªè¯Šæ–­å¯¹æ¯”è§£ç ï¼ˆSEASONï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€è¯Šæ–­æ¯ä¸ªè¾“å‡ºtokençš„å¹»è§‰å€¾å‘ï¼Œå¹¶å¯¹å…¶å¯¹åº”çš„æ—¶åºå’Œç©ºé—´è´Ÿæ ·æœ¬è¿›è¡Œè‡ªé€‚åº”å¯¹æ¯”è§£ç ï¼Œä»è€Œè‡ªé€‚åº”åœ°å¢å¼ºæ¯ä¸ªè¾“å‡ºtokenåœ¨æ—¶åºå’Œç©ºé—´ç»´åº¦ä¸Šçš„å¿ å®æ€§ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSEASONåœ¨ä¸‰ä¸ªå¹»è§‰æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæ‰€æœ‰ç°æœ‰çš„æ— éœ€è®­ç»ƒçš„å¹»è§‰ç¼“è§£æ–¹æ³•ï¼ŒåŒæ—¶åœ¨å››ä¸ªé€šç”¨è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¿›ä¸€æ­¥æå‡äº†è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨æ—¶åºå’Œç©ºé—´å¹»è§‰ç¼“è§£æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†è§†é¢‘ç†è§£ä¸­æ—¶åºæ¨ç†çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚SEASONæ–¹æ³•é€šè¿‡è‡ªé€‚åº”å¯¹æ¯”è§£ç æœºåˆ¶ï¼Œä¸ºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰ç¼“è§£æä¾›äº†æ–°æ€è·¯ï¼ŒåŒæ—¶å±•ç¤ºäº†åœ¨é€šç”¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæœªæ¥è§†é¢‘è¯­è¨€æ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.</p>
<h3 id="5-reasonx-mllm-guided-intrinsic-image-decomposition">[5] <a href="https://arxiv.org/abs/2512.04222">ReasonX: MLLM-Guided Intrinsic Image Decomposition</a></h3>
<p><em>Alara Dirik, Tuanfeng Wang, Duygu Ceylan, Stefanos Zafeiriou, Anna FrÃ¼hstÃ¼ck</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ReasonXæ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºæ„ŸçŸ¥è¯„åˆ¤å™¨æä¾›ç›¸å¯¹å†…åœ¨æ¯”è¾ƒï¼Œå¹¶å°†è¿™äº›æ¯”è¾ƒä½œä¸ºGRPOå¥–åŠ±æ¥å¾®è°ƒæœªæ ‡è®°çœŸå®ä¸–ç•Œå›¾åƒä¸Šçš„å†…åœ¨å›¾åƒåˆ†è§£æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å¤šç§åŸºç¡€æ¶æ„å’Œæ¨¡æ€çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡åŸºäºæ‰©æ•£å’ŒTransformerçš„æ¨¡å‹å—ç›Šäºåˆæˆæ•°æ®é›†çš„é…å¯¹ç›‘ç£ï¼Œä½†å®ƒä»¬åœ¨å¤šæ ·åŒ–çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†æœªæ ‡è®°çš„é‡å¤–å›¾åƒçš„å†…åœ¨åˆ†è§£ä»»åŠ¡ã€‚</p>
<p><strong>Method:</strong> ReasonXæ¡†æ¶é‡‡ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºæ„ŸçŸ¥è¯„åˆ¤å™¨ï¼Œç”Ÿæˆç›¸å¯¹å†…åœ¨æ¯”è¾ƒä½œä¸ºGRPOå¥–åŠ±ä¿¡å·ï¼Œé€šè¿‡å¥–åŠ±æ¨¡å‹è¾“å‡ºä¸è¯„åˆ¤å™¨å…³ç³»è¯„ä¼°ä¹‹é—´çš„ä¸€è‡´æ€§æ¥å¾®è°ƒæ¡ä»¶å†…åœ¨é¢„æµ‹å™¨ï¼Œè¯¥æ¡†æ¶ä¸æ¨¡å‹æ— å…³ï¼Œå¯åº”ç”¨äºä¸åŒçš„å†…åœ¨é¢„æµ‹å™¨å’Œå¤šç§æ¨¡æ€ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šç§åŸºç¡€æ¶æ„å’Œæ¨¡æ€ä¸Šï¼ŒReasonXå®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬åœ¨IIWåç…§ç‡æ•°æ®é›†ä¸ŠWHDRé™ä½9-25%ï¼Œåœ¨ETH3Dæ·±åº¦æ•°æ®é›†ä¸Šæ·±åº¦å‡†ç¡®ç‡æå‡é«˜è¾¾46%ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¼•å¯¼çš„æ¯”è¾ƒç›‘ç£åœ¨è¿æ¥ä½å±‚å’Œé«˜å±‚è§†è§‰æ¨ç†æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºåˆ©ç”¨æœªæ ‡è®°çœŸå®ä¸–ç•Œæ•°æ®æ”¹è¿›å†…åœ¨å›¾åƒåˆ†è§£æä¾›äº†æ¨¡å‹æ— å…³çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¼€è¾Ÿäº†ç»“åˆé«˜çº§è¯­ä¹‰ç†è§£å’Œä½çº§è§†è§‰ä»»åŠ¡çš„æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Intrinsic image decomposition aims to separate images into physical components such as albedo, depth, normals, and illumination. While recent diffusion- and transformer-based models benefit from paired supervision from synthetic datasets, their generalization to diverse, real-world scenarios remains challenging. We propose ReasonX, a novel framework that leverages a multimodal large language model (MLLM) as a perceptual judge providing relative intrinsic comparisons, and uses these comparisons as GRPO rewards for fine-tuning intrinsic decomposition models on unlabeled, in-the-wild images. Unlike RL methods for generative models, our framework aligns conditional intrinsic predictors by rewarding agreement between the judge's relational assessments and analytically derived relations from the model's outputs. ReasonX is model-agnostic and can be applied to different intrinsic predictors. Across multiple base architectures and modalities, ReasonX yields significant improvements, including 9-25% WHDR reduction on IIW albedo and up to 46% depth accuracy gains on ETH3D, highlighting the promise of MLLM-guided comparative supervision to bridge low- and high-level vision reasoning.</p>
<h3 id="6-6-fingers-1-kidney-natural-adversarial-medical-images-reveal-critical-weaknesses-of-vision-language-models">[6] <a href="https://arxiv.org/abs/2512.04238">6 Fingers, 1 Kidney: Natural Adversarial Medical Images Reveal Critical Weaknesses of Vision-Language Models</a></h3>
<p><em>Leon Mayer, Piotr Kalinowski, Caroline Ebersbach, Marcel Knopp, Tim RÃ¤dsch, Evangelia Christodoulou, Annika Reinke, Fiona R. Kolbinger, Lena Maier-Hein</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†AdversarialAnatomyBenchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ç½•è§è§£å‰–å˜å¼‚çš„è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å½“å‰åŒ»å­¦VLMåœ¨ç½•è§è§£å‰–è¡¨ç°ä¸Šçš„æ³›åŒ–èƒ½åŠ›ä¸¥é‡ä¸è¶³ï¼Œæ€§èƒ½ä¸‹é™é«˜è¾¾45ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†ä¸»è¦è¯„ä¼°å¸¸è§è§£å‰–è¡¨ç°ï¼Œæ— æ³•æ•æ‰ç½•è§è§£å‰–å˜å¼‚å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¿™å¯¼è‡´åŒ»å­¦AIç³»ç»Ÿåœ¨å®é™…ä¸´åºŠåº”ç”¨ä¸­å¯èƒ½å¯¹éå…¸å‹è§£å‰–æƒ…å†µè¡¨ç°ä¸ä½³ï¼Œå­˜åœ¨æœªè¢«é‡åŒ–çš„å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†AdversarialAnatomyBenchåŸºå‡†ï¼ŒåŒ…å«è·¨å¤šç§æˆåƒæ¨¡æ€å’Œè§£å‰–åŒºåŸŸçš„è‡ªç„¶å‘ç”Ÿç½•è§è§£å‰–å˜å¼‚ï¼Œè¿™äº›å˜å¼‚è¢«ç§°ä¸º"è‡ªç„¶å¯¹æŠ—è§£å‰–"ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°22ä¸ªæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Result:</strong> åœ¨ç½•è§è§£å‰–å˜å¼‚ä¸Šï¼Œæ¨¡å‹å¹³å‡å‡†ç¡®ç‡ä»å…¸å‹è§£å‰–çš„74%éª¤é™è‡³29%ï¼Œæ€§èƒ½ä¸‹é™è¾¾45ä¸ªç™¾åˆ†ç‚¹ï¼›å³ä½¿è¡¨ç°æœ€ä½³çš„GPT-5ã€Gemini 2.5 Proå’ŒLlama 4 Maverickæ¨¡å‹ä¹Ÿå‡ºç°41-51%çš„æ€§èƒ½ä¸‹é™ï¼Œæ¨¡å‹é”™è¯¯æ¨¡å¼ä¸é¢„æœŸè§£å‰–åå·®é«˜åº¦ä¸€è‡´ï¼Œä¸”æ¨¡å‹æ‰©å±•å’Œå¹²é¢„æªæ–½å‡æœªèƒ½è§£å†³è¿™äº›é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹å¯¹ç½•è§è§£å‰–è¡¨ç°çš„æ³›åŒ–èƒ½åŠ›å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼ŒAdversarialAnatomyBenchä¸ºç³»ç»Ÿæµ‹é‡å’Œå‡è½»å¤šæ¨¡æ€åŒ»å­¦AIç³»ç»Ÿä¸­çš„è§£å‰–åå·®æä¾›äº†åŸºç¡€ï¼Œå¼ºè°ƒäº†åœ¨ä¸´åºŠåº”ç”¨ä¸­è€ƒè™‘è§£å‰–å˜å¼‚çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Vision-language models are increasingly integrated into clinical workflows. However, existing benchmarks primarily assess performance on common anatomical presentations and fail to capture the challenges posed by rare variants. To address this gap, we introduce AdversarialAnatomyBench, the first benchmark comprising naturally occurring rare anatomical variants across diverse imaging modalities and anatomical regions. We call such variants that violate learned priors about "typical" human anatomy natural adversarial anatomy. Benchmarking 22 state-of-the-art VLMs with AdversarialAnatomyBench yielded three key insights. First, when queried with basic medical perception tasks, mean accuracy dropped from 74% on typical to 29% on atypical anatomy. Even the best-performing models, GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick, showed performance drops of 41-51%. Second, model errors closely mirrored expected anatomical biases. Third, neither model scaling nor interventions, including bias-aware prompting and test-time reasoning, resolved these issues. These findings highlight a critical and previously unquantified limitation in current VLM: their poor generalization to rare anatomical presentations. AdversarialAnatomyBench provides a foundation for systematically measuring and mitigating anatomical bias in multimodal medical AI systems.</p>
<h3 id="7-unilight-a-unified-representation-for-lighting">[7] <a href="https://arxiv.org/abs/2512.04267">UniLight: A Unified Representation for Lighting</a></h3>
<p><em>Zitian Zhang, Iliyan Georgiev, Michael Fischer, Yannick Hold-Geoffroy, Jean-FranÃ§ois Lalonde, Valentin Deschaintre</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UniLightï¼Œä¸€ç§è”åˆæ½œåœ¨ç©ºé—´ä½œä¸ºå…‰ç…§è¡¨ç¤ºï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ ç»Ÿä¸€äº†æ–‡æœ¬ã€å›¾åƒã€è¾ç…§åº¦å’Œç¯å¢ƒè´´å›¾ç­‰å¤šç§æ¨¡æ€ï¼Œå®ç°äº†è·¨æ¨¡æ€çš„å…‰ç…§ç‰¹å¾ç†è§£å’Œçµæ´»æ“æ§ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å…‰ç…§è¡¨ç¤ºæ–¹æ³•å¦‚ç¯å¢ƒè´´å›¾ã€è¾ç…§åº¦ã€çƒè°å‡½æ•°æˆ–æ–‡æœ¬ç­‰å­˜åœ¨æ¨¡æ€ä¸å…¼å®¹é—®é¢˜ï¼Œè¿™é™åˆ¶äº†è·¨æ¨¡æ€çš„å…‰ç…§ç‰¹å¾è¿ç§»å’Œåº”ç”¨ï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½å¤Ÿç»Ÿä¸€å¤šç§å…‰ç…§è¡¨ç¤ºæ¨¡æ€çš„é€šç”¨è¡¨ç¤ºæ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> UniLighté‡‡ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒæ¨¡æ€ç‰¹å®šçš„ç¼–ç å™¨ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€è¾ç…§åº¦å’Œç¯å¢ƒè´´å›¾ç¼–ç å™¨ï¼Œé€šè¿‡å…±äº«åµŒå…¥ç©ºé—´å¯¹é½ä¸åŒæ¨¡æ€çš„è¡¨ç¤ºï¼Œå¹¶å¼•å…¥è¾…åŠ©çš„çƒè°å‡½æ•°é¢„æµ‹ä»»åŠ¡æ¥å¢å¼ºæ–¹å‘æ€§ç†è§£ï¼Œæ„å»ºäº†æ”¯æŒå¤§è§„æ¨¡è®­ç»ƒçš„å¤šæ¨¡æ€æ•°æ®æµæ°´çº¿ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜UniLightè¡¨ç¤ºèƒ½å¤Ÿæ•è·ä¸€è‡´ä¸”å¯è¿ç§»çš„å…‰ç…§ç‰¹å¾ï¼Œåœ¨å…‰ç…§æ£€ç´¢ã€ç¯å¢ƒè´´å›¾ç”Ÿæˆå’Œæ‰©æ•£æ¨¡å‹å›¾åƒåˆæˆä¸­çš„å…‰ç…§æ§åˆ¶ä¸‰ä¸ªä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå®ç°äº†è·¨æ¨¡æ€çš„çµæ´»å…‰ç…§æ“æ§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»Ÿä¸€å…‰ç…§è¡¨ç¤ºç©ºé—´çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸ºè·¨æ¨¡æ€çš„å…‰ç…§ç†è§£å’Œæ“æ§æä¾›äº†æ–°èŒƒå¼ï¼Œåœ¨è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦åº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œæœªæ¥å¯æ‰©å±•è‡³æ›´å¤šå…‰ç…§ç›¸å…³ä»»åŠ¡ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Lighting has a strong influence on visual appearance, yet understanding and representing lighting in images remains notoriously difficult. Various lighting representations exist, such as environment maps, irradiance, spherical harmonics, or text, but they are incompatible, which limits cross-modal transfer. We thus propose UniLight, a joint latent space as lighting representation, that unifies multiple modalities within a shared embedding. Modality-specific encoders for text, images, irradiance, and environment maps are trained contrastively to align their representations, with an auxiliary spherical-harmonics prediction task reinforcing directional understanding. Our multi-modal data pipeline enables large-scale training and evaluation across three tasks: lighting-based retrieval, environment-map generation, and lighting control in diffusion-based image synthesis. Experiments show that our representation captures consistent and transferable lighting features, enabling flexible manipulation across modalities.</p>
<h3 id="8-draco-draft-as-cot-for-text-to-image-preview-and-rare-concept-generation">[8] <a href="https://arxiv.org/abs/2512.05112">DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</a></h3>
<p><em>Dongzhi Jiang, Renrui Zhang, Haodong Li, Zhuofan Zong, Ziyu Guo, Jun He, Claire Guo, Junyan Ye, Rongyao Fang, Weijia Li, Rui Liu, Hongsheng Li</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DraCoï¼ˆDraft-as-CoTï¼‰ï¼Œä¸€ç§æ–°é¢–çš„äº¤é”™æ¨ç†èŒƒå¼ï¼Œé€šè¿‡ç”Ÿæˆä½åˆ†è¾¨ç‡è‰å›¾ä½œä¸ºè§†è§‰è§„åˆ’ï¼Œå¹¶åˆ©ç”¨æ¨¡å‹å†…åœ¨ç†è§£èƒ½åŠ›è¿›è¡Œè¯­ä¹‰å¯¹é½éªŒè¯å’Œé€‰æ‹©æ€§ä¿®æ­£ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè¦ä¹ˆå°†æ¨¡å‹ä»…è§†ä¸ºç‹¬ç«‹ç”Ÿæˆå™¨ï¼Œè¦ä¹ˆä¾èµ–æŠ½è±¡æ–‡æœ¬è§„åˆ’ï¼Œæ— æ³•è§£å†³æ–‡æœ¬è§„åˆ’çš„ç²—ç²’åº¦ç‰¹æ€§ä»¥åŠç”Ÿæˆç½•è§å±æ€§ç»„åˆçš„å›°éš¾ã€‚</p>
<p><strong>Method:</strong> DraCoæ–¹æ³•é¦–å…ˆç”Ÿæˆä½åˆ†è¾¨ç‡è‰å›¾ä½œä¸ºé¢„è§ˆï¼Œæä¾›å…·ä½“ç»“æ„åŒ–è§†è§‰è§„åˆ’ï¼›ç„¶ååˆ©ç”¨æ¨¡å‹å†…åœ¨ç†è§£èƒ½åŠ›éªŒè¯è‰å›¾ä¸è¾“å…¥æç¤ºä¹‹é—´çš„æ½œåœ¨è¯­ä¹‰é”™ä½ï¼Œå¹¶é€šè¿‡é€‰æ‹©æ€§ä¿®æ­£å’Œè¶…åˆ†è¾¨ç‡è¿›è¡Œç»†åŒ–ï¼›åŒæ—¶æ„å»ºäº†DraCo-240Kè®­ç»ƒæ•°æ®é›†å¢å¼ºä¸‰ç§åŸå­èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†DraCo-CFGä¸“é—¨åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ç­–ç•¥æ”¯æŒäº¤é”™æ¨ç†ã€‚</p>
<p><strong>Result:</strong> DraCoåœ¨GenEvalåŸºå‡†ä¸Šå®ç°äº†+8%çš„æå‡ï¼Œåœ¨Imagine-Benchä¸Šè·å¾—+0.91åˆ†æ”¹è¿›ï¼Œåœ¨GenEval++ä¸Šå–å¾—+3%å¢é•¿ï¼Œæ˜¾è‘—ä¼˜äºç›´æ¥ç”Ÿæˆæ–¹æ³•åŠå…¶ä»–åŸºäºæ€ç»´é“¾çš„ç”Ÿæˆæ–¹æ³•ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†äº¤é”™æ¨ç†èŒƒå¼åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬å’Œè§†è§‰å†…å®¹çš„æ€ç»´é“¾å®ç°äº†æ›´å¥½çš„è§„åˆ’å’ŒéªŒè¯ï¼Œä¸ºè§£å†³ç½•è§å±æ€§ç»„åˆç”Ÿæˆå’Œè¯­ä¹‰å¯¹é½é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿè®¾è®¡å¼€è¾Ÿäº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.</p>
<h3 id="9-inference-time-stochastic-refinement-of-gru-normalizing-flow-for-real-time-video-motion-transfer">[9] <a href="https://arxiv.org/abs/2512.04282">Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer</a></h3>
<p><em>Tasmiah Haque, Srinjoy Das</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGRU-SNFï¼ˆé—¨æ§å¾ªç¯å•å…ƒ-éšæœºå½’ä¸€åŒ–æµï¼‰ï¼Œä¸€ç§åœ¨æ¨ç†æ—¶ç»“åˆMCMCé‡‡æ ·çš„æ–°å‹åºåˆ—é¢„æµ‹æ–¹æ³•ï¼Œé€šè¿‡å‘GRU-NFæ³¨å…¥éšæœºæ€§æ¥å¢å¼ºè§†é¢‘è¿åŠ¨ä¼ è¾“ä¸­æœªæ¥é¢„æµ‹çš„å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å®æ—¶è§†é¢‘è¿åŠ¨ä¼ è¾“åº”ç”¨å¦‚æ²‰æµ¸å¼æ¸¸æˆå’ŒåŸºäºè§†è§‰çš„å¼‚å¸¸æ£€æµ‹éœ€è¦å‡†ç¡®ä¸”å¤šæ ·åŒ–çš„æœªæ¥é¢„æµ‹ï¼Œä»¥æ”¯æŒçœŸå®åˆæˆå’Œä¸ç¡®å®šæ€§ä¸‹çš„ç¨³å¥ä¸‹æ¸¸å†³ç­–ã€‚GRU-NFè™½ç„¶èƒ½é€šè¿‡å½’ä¸€åŒ–æµæ•è·å¤šæ¨¡æ€åˆ†å¸ƒï¼Œä½†å…¶ç¡®å®šæ€§å˜æ¢ç»“æ„é™åˆ¶äº†è¡¨è¾¾èƒ½åŠ›ï¼Œéœ€è¦æ”¹è¿›åºåˆ—é¢„æµ‹çš„å¤šæ ·æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„æ¨ç†æ—¶ç²¾ç‚¼æŠ€æœ¯ï¼Œå°†GRU-NFä¸éšæœºé‡‡æ ·æ–¹æ³•ç»“åˆï¼Œå—éšæœºå½’ä¸€åŒ–æµå¯å‘ï¼Œåœ¨GRU-NFæ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›æ­¥éª¤ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¢ç´¢æ›´ä¸°å¯Œçš„è¾“å‡ºç©ºé—´å¹¶æ›´å¥½åœ°è¿‘ä¼¼çœŸå®æ•°æ®åˆ†å¸ƒï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¯¥æ–¹æ³•åœ¨åŸºäºå…³é”®ç‚¹çš„è§†é¢‘è¿åŠ¨ä¼ è¾“ç®¡é“ä¸­è¿›è¡ŒéªŒè¯ï¼Œå…¶ä¸­æ•è·æ—¶é—´ä¸€è‡´ä¸”æ„ŸçŸ¥å¤šæ ·çš„æœªæ¥è½¨è¿¹å¯¹äºçœŸå®æ ·æœ¬å’Œä½å¸¦å®½é€šä¿¡è‡³å…³é‡è¦ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼ŒGRU-SNFåœ¨ç”Ÿæˆå¤šæ ·åŒ–è¾“å‡ºæ–¹é¢ä¼˜äºGRU-NFï¼Œä¸”ä¸ç‰ºç‰²å‡†ç¡®æ€§ï¼Œå³ä½¿åœ¨æ›´é•¿çš„é¢„æµ‹æ—¶é—´èŒƒå›´å†…ä¹Ÿæ˜¯å¦‚æ­¤ã€‚é€šè¿‡æ¨ç†æ—¶æ³¨å…¥éšæœºæ€§ï¼Œè¯¥æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°æ•è·å¤šæ¨¡æ€è¡Œä¸ºï¼Œåœ¨å…³é”®ç‚¹è§†é¢‘è¿åŠ¨ä¼ è¾“ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†éšæœºåŠ¨åŠ›å­¦ä¸åŸºäºæµçš„åºåˆ—æ¨¡å‹ç›¸ç»“åˆåœ¨ç”Ÿæˆå¼æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å…·æœ‰æ½œåŠ›ã€‚æ¨ç†æ—¶éšæœºæ€§æ³¨å…¥èƒ½å¤Ÿå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼Œä¸ºéœ€è¦å¤šæ ·åŒ–ä¸”å‡†ç¡®é¢„æµ‹çš„å®æ—¶åº”ç”¨æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†éšæœºå½’ä¸€åŒ–æµæ€æƒ³åœ¨æ—¶åºå»ºæ¨¡ä¸­çš„æ‰©å±•ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.</p>
<h3 id="10-how-miscalibrated-is-your-federated-clip-and-what-to-do-about-it">[10] <a href="https://arxiv.org/abs/2512.04305">How (Mis)calibrated is Your Federated CLIP and What To Do About It?</a></h3>
<p><em>Mainak Singha, Masih Aminbeidokhti, Paolo Casari, Elisa Ricci, Subhankar Roy</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ç ”ç©¶äº†è”é‚¦å­¦ä¹ ç¯å¢ƒä¸‹CLIPæ¨¡å‹çš„æ ¡å‡†é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºFLÂ²oRAçš„LoRA-basedæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è”é‚¦è®¾ç½®ä¸­è‡ªç„¶æ”¹å–„æ¨¡å‹æ ¡å‡†ï¼Œå‡å°‘äº†å¯¹æ˜¾å¼æ ¡å‡†è¿‡ç¨‹çš„éœ€æ±‚ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPå·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†å…¶åœ¨è”é‚¦å­¦ä¹ è®¾ç½®ä¸­çš„æ ¡å‡†é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ç¦»çº¿ç¯å¢ƒä¸‹çš„CLIPæ ¡å‡†ï¼Œè€Œè”é‚¦å­¦ä¹ å¾®è°ƒå¯¹æ¨¡å‹å¯é æ€§çš„å½±å“ä»å±æœªçŸ¥é¢†åŸŸï¼Œè¿™æ„æˆäº†æœ¬ç ”ç©¶è¦è§£å†³çš„æ ¸å¿ƒç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶é¦–å…ˆåˆ†æäº†æ–‡æœ¬æç¤ºè°ƒä¼˜æ–¹æ³•åœ¨è”é‚¦å­¦ä¹ ä¸‹çš„æ ¡å‡†è¡¨ç°ï¼Œè¯„ä¼°äº†ç°æœ‰è®­ç»ƒä¸­æ ¡å‡†æŠ€æœ¯åœ¨ä¸åŒå…¨å±€èšåˆæ–¹æ³•ä¸­çš„æ•ˆæœã€‚åŸºäºè¿™äº›åˆ†æï¼Œæå‡ºäº†FLÂ²oRAæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLoRAçš„ç®€å•æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©é€‚å½“çš„å¾®è°ƒç»„ä»¶æ¥è‡ªç„¶æ”¹å–„è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­çš„æ¨¡å‹æ ¡å‡†ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œæ–‡æœ¬æç¤ºè°ƒä¼˜æ–¹æ³•åœ¨è”é‚¦å­¦ä¹ è®¾ç½®ä¸‹ä¼šæ˜¾è‘—é™ä½æ ¡å‡†æŒ‡æ ‡ï¼Œè€Œç°æœ‰è®­ç»ƒä¸­æ ¡å‡†æŠ€æœ¯åœ¨ä¸åŒèšåˆæ–¹æ³•ä¸­ä»…æä¾›æœ‰é™æ”¹è¿›ã€‚FLÂ²oRAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆäº§ç”Ÿè‰¯å¥½æ ¡å‡†çš„æ¨¡å‹ï¼Œæœ‰æ•ˆå‡å°‘äº†æ˜¾å¼æ ¡å‡†ç¨‹åºçš„éœ€æ±‚ï¼Œå…¶æœ‰æ•ˆæ€§æºäºå¯¹å¾®è°ƒç»„ä»¶çš„åˆç†é€‰æ‹©ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†è”é‚¦å­¦ä¹ ä¸­CLIPæ ¡å‡†çš„å…³é”®æŒ‘æˆ˜ä¸ä»…åœ¨äºèšåˆæˆ–æ ¡å‡†æ–¹æ³•ï¼Œæ›´åœ¨äºé€‰æ‹©å“ªäº›ç»„ä»¶è¿›è¡Œå¾®è°ƒã€‚FLÂ²oRAçš„æˆåŠŸè¡¨æ˜ï¼Œé€šè¿‡é€‚å½“çš„å‚æ•°é«˜æ•ˆå¾®è°ƒç­–ç•¥ï¼Œå¯ä»¥åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è‡ªç„¶å®ç°æ¨¡å‹å¯é æ€§æå‡ï¼Œä¸ºè”é‚¦è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é éƒ¨ç½²æä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>While vision-language models like CLIP have been extensively studied, their calibration, crucial for reliable predictions, has received limited attention. Although a few prior works have examined CLIP calibration in offline settings, the impact of fine-tuning CLIP in a federated learning (FL) setup remains unexplored. In this work, we investigate how FL affects CLIP calibration and propose strategies to improve reliability in this distributed setting. We first analyze Textual Prompt Tuning approaches and show that they degrade calibration metrics when operating under FL. We also evaluate existing in-training calibration techniques across four global aggregation methods, finding that they provide limited improvements. Our results suggest that the key challenge lies not only in how we aggregate or calibrate, but in which components we choose to fine-tune. Motivated by this insight, we propose $\text{FL}^2\text{oRA}$, a straightforward LoRA-based approach that naturally improves calibration in FL, and we analyze the factors behind its effectiveness. Experiments on multiple benchmarks demonstrate that $\text{FL}^2\text{oRA}$ consistently produces well-calibrated models, reducing the need for explicit calibration procedures. Codes are available at https://github.com/mainaksingha01/FL2oRA.</p>
<h3 id="11-fourier-attentive-representation-learning-a-fourier-guided-framework-for-few-shot-generalization-in-vision-language-models">[11] <a href="https://arxiv.org/abs/2512.04395">Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models</a></h3>
<p><em>Hieu Dinh Trung Pham, Huy Minh Nhat Nguyen, Cuong Tuan Nguyen</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºå‚…é‡Œå¶æ³¨æ„åŠ›è¡¨ç¤ºå­¦ä¹ ï¼ˆFARLï¼‰æ¡†æ¶ï¼Œé€šè¿‡å‚…é‡Œå¶åˆ†ææ˜¾å¼è§£è€¦è§†è§‰è¡¨ç¤ºä¸­çš„ç»“æ„ç‰¹å¾ä¸é£æ ¼ç‰¹å¾ï¼Œä»¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ•´ä½“è¡¨ç¤ºé€šå¸¸å°†å›¾åƒçš„é¢†åŸŸä¸å˜ç»“æ„ä¸é¢†åŸŸç‰¹å®šé£æ ¼éšå¼çº ç¼ åœ¨ä¸€èµ·ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ–¹æ³•æ¥æ˜¾å¼è§£è€¦è¿™äº›è§†è§‰çº¿ç´¢ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºå‚…é‡Œå¶æ³¨æ„åŠ›è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯åŒäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­å¯å­¦ä¹ çš„è¡¨ç¤ºæ ‡è®°åˆ†åˆ«ä»å›¾åƒçš„ç›¸ä½è°±æŸ¥è¯¢ç»“æ„ç‰¹å¾å’Œä»å¹…åº¦è°±æŸ¥è¯¢é£æ ¼ç‰¹å¾ï¼Œç”Ÿæˆè§£è€¦çš„ä¸°å¯Œæ ‡è®°åé€šè¿‡éå¯¹ç§°æ³¨å…¥ç­–ç•¥æ·±åº¦æ³¨å…¥åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ç¼–ç å™¨ä¸­ä»¥å¼•å¯¼é€‚åº”è¿‡ç¨‹ã€‚</p>
<p><strong>Result:</strong> åœ¨15ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒFARLæ¡†æ¶æ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œé€šè¿‡æ˜¾å¼è§£è€¦ç»“æ„ç‰¹å¾ä¸é£æ ¼ç‰¹å¾å®ç°äº†æ›´é²æ£’çš„è§†è§‰è¯­è¨€å¯¹é½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡å‚…é‡Œå¶åˆ†ææ˜¾å¼è§£è€¦è§†è§‰è¡¨ç¤ºä¸­çš„ç»“æ„ä¸é£æ ¼ç‰¹å¾èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œéå¯¹ç§°æ³¨å…¥ç­–ç•¥å¼ºåˆ¶æ¨¡å‹å­¦ä¹ æ›´é²æ£’çš„è§†è§‰è¯­è¨€å¯¹é½ï¼Œä¸ºæå‡å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.</p>
<h3 id="12-explainable-parkinsons-disease-gait-recognition-using-multimodal-rgb-d-fusion-and-large-language-models">[12] <a href="https://arxiv.org/abs/2512.04425">Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models</a></h3>
<p><em>Manar Alnaasan, Md Selim Sarowar, Sungho Kim</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œé€šè¿‡èåˆRGB-Dæ•°æ®æ¥è¯†åˆ«å¸•é‡‘æ£®ç—…æ­¥æ€æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸´åºŠå¯ç†è§£çš„è§£é‡Šï¼Œå®ç°äº†è§†è§‰è¯†åˆ«ä¸ä¸´åºŠç†è§£ä¹‹é—´çš„æ¡¥æ¢ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¸•é‡‘æ£®ç—…æ­¥æ€åˆ†ææ–¹æ³•å­˜åœ¨å•æ¨¡æ€è¾“å…¥é™åˆ¶ã€é²æ£’æ€§ä¸è¶³ä»¥åŠç¼ºä¹ä¸´åºŠé€æ˜åº¦ç­‰é—®é¢˜ï¼Œéš¾ä»¥åœ¨çœŸå®æ¡ä»¶ä¸‹è¿›è¡Œå‡†ç¡®ä¸”å¯è§£é‡Šçš„æ­¥æ€åˆ†æï¼Œè¿™é˜»ç¢äº†æ—©æœŸæ£€æµ‹å’Œä¸´åºŠåº”ç”¨çš„è¿›å±•ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨åŒYOLOv11ç¼–ç å™¨åˆ†åˆ«æå–RGBå’Œæ·±åº¦æ¨¡æ€ç‰¹å¾ï¼Œé€šè¿‡å¤šå°ºåº¦å±€éƒ¨-å…¨å±€æå–æ¨¡å—å’Œè·¨ç©ºé—´é¢ˆéƒ¨èåˆæœºåˆ¶å¢å¼ºæ—¶ç©ºè¡¨ç¤ºï¼Œå¹¶å¼•å…¥å†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹å°†èåˆçš„è§†è§‰åµŒå…¥å’Œç»“æ„åŒ–å…ƒæ•°æ®è½¬æ¢ä¸ºä¸´åºŠå¯ç†è§£çš„æ–‡æœ¬è§£é‡Šã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šæ¨¡æ€æ­¥æ€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥RGB-Dèåˆæ¡†æ¶ç›¸æ¯”å•è¾“å…¥åŸºçº¿å®ç°äº†æ›´é«˜çš„è¯†åˆ«å‡†ç¡®ç‡ï¼Œå¯¹ç¯å¢ƒå˜åŒ–å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ï¼Œå¹¶èƒ½æä¾›æ¸…æ™°çš„è§†è§‰-è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œæœ‰æ•ˆå¤„ç†ä½å…‰ç…§æˆ–è¡£ç‰©é®æŒ¡ç­‰æŒ‘æˆ˜æ€§åœºæ™¯ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ç»“åˆå¤šæ¨¡æ€ç‰¹å¾å­¦ä¹ å’ŒåŸºäºè¯­è¨€çš„è§£é‡Šæ€§ï¼Œä¸ºå¯é ä¸”å¯è§£é‡Šçš„å¸•é‡‘æ£®ç—…æ­¥æ€åˆ†ææä¾›äº†æ–°çš„è§†è§‰-è¯­è¨€èŒƒå¼ï¼Œå¼¥åˆäº†è§†è§‰è¯†åˆ«ä¸ä¸´åºŠç†è§£ä¹‹é—´çš„é¸¿æ²Ÿï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM</p>
<h3 id="13-phyvllm-physics-guided-video-language-model-with-motion-appearance-disentanglement">[13] <a href="https://arxiv.org/abs/2512.04532">PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement</a></h3>
<p><em>Yu-Wei Zhan, Xin Wang, Hong Chen, Tongtong Feng, Wei Feng, Ren Wang, Guangyao Li, Qing Li, Wenwu Zhu</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºPhyVLLMï¼Œä¸€ç§ç‰©ç†å¼•å¯¼çš„è§†é¢‘-è¯­è¨€æ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼åœ°å°†ç‰©ç†è¿åŠ¨å»ºæ¨¡èå…¥è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨ç‰©ç†åŠ¨æ€ç†è§£æ–¹é¢çš„å±€é™æ€§ï¼Œæ˜¾è‘—æå‡äº†ç‰©ç†æ¨ç†å’Œé€šç”¨è§†é¢‘ç†è§£èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨éœ€è¦æ·±å…¥ç†è§£ç‰©ç†åŠ¨æ€çš„åœºæ™¯ä¸­ç»å¸¸å¤±è´¥ï¼Œä¸»è¦æºäºå…¶ä¾èµ–å¤–è§‚åŒ¹é…çš„å±€é™æ€§ã€‚ç‰©ç†è¿åŠ¨å»ºæ¨¡é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šè¿åŠ¨ä¿¡å·ä¸å¤–è§‚å˜åŒ–çº ç¼ éš¾ä»¥æå–å¹²å‡€ç‰©ç†çº¿ç´¢ï¼›æœ‰æ•ˆè¿åŠ¨å»ºæ¨¡éœ€è¦è¿ç»­æ—¶é—´è¡¨ç¤ºå’Œç‰©ç†åŠ¨æ€æ•æ‰ï¼›ç‰©ç†å±æ€§æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”ä¸åˆ‡å®é™…ã€‚</p>
<p><strong>Method:</strong> PhyVLLMé‡‡ç”¨åŒåˆ†æ”¯ç¼–ç å™¨è§£è€¦è§†è§‰å¤–è§‚å’Œç‰©ä½“è¿åŠ¨ï¼Œé€šè¿‡ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹æ¨¡å—å»ºæ¨¡æ—¶é—´ä¸Šçš„ç‰©ç†åŠ¨æ€ï¼Œç”Ÿæˆå¯å¾®åˆ†çš„ç‰©ç†åŠ¨æ€è¡¨ç¤ºã€‚è¿™äº›è¿åŠ¨æ„ŸçŸ¥è¡¨ç¤ºè¢«æŠ•å½±åˆ°é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„æ ‡è®°ç©ºé—´ï¼Œå®ç°ç‰©ç†æ¨ç†è€Œä¸æŸå®³åŸå§‹å¤šæ¨¡æ€èƒ½åŠ›ã€‚ä¸ºé¿å…æ˜¾å¼ç‰©ç†æ ‡æ³¨ï¼Œé‡‡ç”¨è‡ªç›‘ç£æ–¹å¼å»ºæ¨¡ç‰©ä½“è¿åŠ¨çš„è¿ç»­æ¼”åŒ–ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒPhyVLLMåœ¨ç‰©ç†æ¨ç†å’Œé€šç”¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†æ˜¾å¼ç‰©ç†å»ºæ¨¡çš„æœ‰æ•ˆæ€§åŠå…¶å¯¹æ¨¡å‹æ•´ä½“è§†é¢‘ç†è§£èƒ½åŠ›çš„æå‡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œå°†æ˜¾å¼ç‰©ç†è¿åŠ¨å»ºæ¨¡èå…¥è§†é¢‘å¤§è¯­è¨€æ¨¡å‹æ˜¯æå‡ç‰©ç†æ¨ç†å’Œè§†é¢‘ç†è§£èƒ½åŠ›çš„å…³é”®é€”å¾„ã€‚PhyVLLMçš„æˆåŠŸè¯æ˜äº†é€šè¿‡è§£è€¦å¤–è§‚ä¸è¿åŠ¨ã€è¿ç»­æ—¶é—´åŠ¨æ€å»ºæ¨¡ä»¥åŠè‡ªç›‘ç£å­¦ä¹ ï¼Œå¯ä»¥åœ¨æ— éœ€æ˜‚è´µæ ‡æ³¨çš„æƒ…å†µä¸‹æœ‰æ•ˆæ•´åˆç‰©ç†çŸ¥è¯†ï¼Œä¸ºæ›´æ™ºèƒ½çš„è§†é¢‘ç†è§£ç³»ç»Ÿå¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.</p>
<h3 id="14-minddrive-an-all-in-one-framework-bridging-world-models-and-vision-language-model-for-end-to-end-autonomous-driving">[14] <a href="https://arxiv.org/abs/2512.04441">MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving</a></h3>
<p><em>Bin Suna, Yaoguang Caob, Yan Wanga, Rui Wanga, Jiachen Shanga, Xiejie Fenga, Jiayi Lu, Jia Shi, Shichun Yang, Xiaoyu Yane, Ziying Song</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MindDriveï¼Œä¸€ä¸ªå°†é«˜è´¨é‡è½¨è¿¹ç”Ÿæˆä¸å…¨é¢å†³ç­–æ¨ç†ç›¸åè°ƒçš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–æ¨ç†èŒƒå¼åœ¨NAVSIMåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç ”ç©¶ä¸»è¦éµå¾ªä¸¤ä¸ªæ–¹å‘ï¼šè½¨è¿¹ç”Ÿæˆå¯¼å‘æ–¹æ³•ä¸“æ³¨äºç”Ÿæˆé«˜è´¨é‡è½¨è¿¹ä½†å†³ç­–æœºåˆ¶ç®€å•ï¼Œè€Œè½¨è¿¹é€‰æ‹©å¯¼å‘æ–¹æ³•è¿›è¡Œå¤šç»´åº¦è¯„ä¼°ä½†ç¼ºä¹è¶³å¤Ÿçš„ç”Ÿæˆèƒ½åŠ›ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæ•´åˆé«˜è´¨é‡è½¨è¿¹ç”Ÿæˆä¸å…¨é¢å†³ç­–æ¨ç†çš„åè°ƒæ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> MindDriveå»ºç«‹äº†"ä¸Šä¸‹æ–‡æ¨¡æ‹Ÿ-å€™é€‰ç”Ÿæˆ-å¤šç›®æ ‡æƒè¡¡"çš„ç»“æ„åŒ–æ¨ç†èŒƒå¼ï¼ŒåŒ…æ‹¬åŸºäºä¸–ç•Œè¡ŒåŠ¨æ¨¡å‹çš„æœªæ¥æ„ŸçŸ¥è½¨è¿¹ç”Ÿæˆå™¨è¿›è¡Œè‡ªæˆ‘æ¡ä»¶"å‡è®¾"æ¨¡æ‹Ÿé¢„æµ‹æ½œåœ¨æœªæ¥åœºæ™¯ï¼Œä»¥åŠåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„VLMå¯¼å‘è¯„ä¼°å™¨åœ¨å®‰å…¨ã€èˆ’é€‚å’Œæ•ˆç‡ç»´åº¦è¿›è¡Œå¤šç›®æ ‡è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> åœ¨NAVSIM-v1å’ŒNAVSIM-v2åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMindDriveåœ¨å¤šç»´åº¦é©¾é©¶æŒ‡æ ‡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†å®‰å…¨æ€§ã€åˆè§„æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥å·¥ä½œä¸ºå¯è§£é‡Šå’Œè®¤çŸ¥å¼•å¯¼çš„è‡ªåŠ¨é©¾é©¶æä¾›äº†ä¸€æ¡æœ‰å‰æ™¯çš„è·¯å¾„ï¼Œé€šè¿‡æ•´åˆç”Ÿæˆä¸è¯„ä¼°èƒ½åŠ›å®ç°äº†æ›´å…¨é¢ã€äººç±»å¯¹é½çš„å†³ç­–åˆ¶å®šï¼Œå±•ç¤ºäº†ç»“æ„åŒ–æ¨ç†èŒƒå¼åœ¨å¤æ‚é©¾é©¶åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that integrates high-quality trajectory generation with comprehensive decision reasoning. It establishes a structured reasoning paradigm of "context simulation - candidate generation - multi-objective trade-off". In particular, the proposed Future-aware Trajectory Generator (FaTG), based on a World Action Model (WaM), performs ego-conditioned "what-if" simulations to predict potential future scenes and generate foresighted trajectory candidates. Building upon this, the VLM-oriented Evaluator (VLoE) leverages the reasoning capability of a large vision-language model to conduct multi-objective evaluations across safety, comfort, and efficiency dimensions, leading to reasoned and human-aligned decision making. Extensive experiments on the NAVSIM-v1 and NAVSIM-v2 benchmarks demonstrate that MindDrive achieves state-of-the-art performance across multi-dimensional driving metrics, significantly enhancing safety, compliance, and generalization. This work provides a promising path toward interpretable and cognitively guided autonomous driving.</p>
<h3 id="15-streameqa-towards-streaming-video-understanding-for-embodied-scenarios">[15] <a href="https://arxiv.org/abs/2512.04451">StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios</a></h3>
<p><em>Yifei Wang, Zhenkai Li, Tianwen Qian, Huanran Zheng, Zheng Wang, Yuqian Fu, Xiaoling Wang</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†StreamEQAï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘å…·èº«æ™ºèƒ½åœºæ™¯çš„æµå¼è§†é¢‘é—®ç­”åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¿ç»­è§†è§‰è¾“å…¥ä¸‹çš„æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨æµå¼è§†é¢‘ç†è§£æ–¹é¢çš„æ˜¾è‘—ä¸è¶³ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€å…·èº«æ™ºèƒ½å‘ç°å®ä¸–ç•Œéƒ¨ç½²æ¨è¿›ï¼Œæ™ºèƒ½ä½“éœ€è¦å…·å¤‡å¯¹è¿ç»­è§†è§‰æµè¿›è¡Œæ„ŸçŸ¥å’Œæ¨ç†çš„èƒ½åŠ›ï¼Œä»¥ç»´æŒç¯å¢ƒæ€åŠ¿æ„ŸçŸ¥ã€ç†è§£ä¸å‘¨å›´å®ä½“çš„äº¤äº’ï¼Œå¹¶åŸºäºè¿‡å»è§‚å¯Ÿã€å½“å‰ä¸Šä¸‹æ–‡å’Œé¢„æœŸæœªæ¥äº‹ä»¶åŠ¨æ€è§„åˆ’è¡ŒåŠ¨ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†ç¼ºä¹ä¸“é—¨é’ˆå¯¹å…·èº«åœºæ™¯ä¸­æµå¼è§†é¢‘é—®ç­”çš„è¯„ä¼°æ¡†æ¶ï¼Œè¿™é™åˆ¶äº†ç›¸å…³ç ”ç©¶è¿›å±•ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†StreamEQAåŸºå‡†ï¼Œè¯¥åŸºå‡†ä»ä¸¤ä¸ªæ­£äº¤ç»´åº¦è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼šå…·èº«ç»´åº¦å’Œæµå¼ç»´åº¦ã€‚å…·èº«ç»´åº¦å°†é—®é¢˜åˆ†ä¸ºæ„ŸçŸ¥ã€äº¤äº’å’Œè§„åˆ’ä¸‰ä¸ªå±‚æ¬¡ï¼Œé€æ­¥è¯„ä¼°æ¨¡å‹è¯†åˆ«ç»†ç²’åº¦è§†è§‰ç»†èŠ‚ã€æ¨ç†æ™ºèƒ½ä½“-å¯¹è±¡äº¤äº’ä»¥åŠæ‰§è¡Œé«˜å±‚ç›®æ ‡å¯¼å‘æ¨ç†çš„èƒ½åŠ›ã€‚æµå¼ç»´åº¦åˆ™å°†é—®é¢˜åˆ†ä¸ºåå‘æ¨ç†ã€å®æ—¶æ¨ç†å’Œå‰å‘æ¨ç†ä¸‰ç§æ¨¡å¼ï¼Œæ¯ç§æ¨¡å¼ä¾èµ–ä¸åŒçš„æ—¶é—´ä¸Šä¸‹æ–‡ã€‚è¯¥åŸºå‡†åŸºäº156ä¸ªç‹¬ç«‹é•¿è§†é¢‘æ„å»ºï¼Œé€šè¿‡ç»“åˆè‡ªåŠ¨ç”Ÿæˆå’Œäººå·¥ç²¾ç‚¼çš„æ··åˆæµç¨‹ï¼Œå®šä¹‰äº†42ä¸ªä»»åŠ¡å¹¶ç”Ÿæˆäº†çº¦21Kä¸ªå¸¦æœ‰ç²¾ç¡®æ—¶é—´æˆ³çš„é—®ç­”å¯¹ã€‚</p>
<p><strong>Result:</strong> å¯¹13ä¸ªæœ€å…ˆè¿›çš„è§†é¢‘-å¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°è¡¨æ˜ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹åœ¨ä¼ ç»ŸåŸºå‡†ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨å…·èº«åœºæ™¯çš„æµå¼è§†é¢‘ç†è§£æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å›°éš¾ã€‚è¯„ä¼°ç»“æœæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨è¿ç»­è§†é¢‘æµä¸­ç»´æŒæƒ…å¢ƒæ„ŸçŸ¥ã€ç†è§£åŠ¨æ€äº¤äº’ä»¥åŠè¿›è¡Œæ—¶é—´æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œçªæ˜¾äº†å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨æµå¼è§†é¢‘ç†è§£èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚</p>
<p><strong>Conclusion:</strong> StreamEQAåŸºå‡†çš„å¼•å…¥å¡«è¡¥äº†å…·èº«æ™ºèƒ½é¢†åŸŸæµå¼è§†é¢‘ç†è§£è¯„ä¼°çš„ç©ºç™½ï¼Œä¸ºç ”ç©¶ç¤¾åŒºæä¾›äº†ç³»ç»ŸåŒ–çš„è¯„ä¼°æ¡†æ¶ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¿ç»­è§†è§‰è¾“å…¥å¤„ç†æ–¹é¢çš„å±€é™æ€§ï¼Œæœ‰æœ›å‚¬åŒ–å…·èº«åº”ç”¨ä¸­æµå¼è§†é¢‘ç†è§£çš„ç›¸å…³ç ”ç©¶ï¼Œæ¨åŠ¨æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­å®æ—¶æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.</p>
<h3 id="16-measuring-the-unspoken-a-disentanglement-model-and-benchmark-for-psychological-analysis-in-the-wild">[16] <a href="https://arxiv.org/abs/2512.04728">Measuring the Unspoken: A Disentanglement Model and Benchmark for Psychological Analysis in the Wild</a></h3>
<p><em>Yigui Feng, Qinglin Wang, Haotian Mo, Yang Liu, Ke Liu, Gencheng Liu, Xinhai Chen, Siqi Shen, Songzhu Mei, Jie Liu</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„ç”Ÿæ€ç³»ç»Ÿæ¥è§£å†³ç”Ÿæˆå¼å¿ƒç†åˆ†æä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šé€šè¿‡MINDå±‚æ¬¡è§†è§‰ç¼–ç å™¨è§£å†³å‘éŸ³-æƒ…æ„Ÿæ­§ä¹‰é—®é¢˜ï¼Œå¹¶æ„å»ºConvoInsight-DBæ•°æ®é›†å’ŒPRISMè¯„ä¼°æ¡†æ¶ï¼Œåœ¨å¾®è¡¨æƒ…æ£€æµ‹ä¸Šå®ç°äº†+86.95%çš„æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç”Ÿæˆå¼å¿ƒç†åˆ†æé¢ä¸´ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹æ— æ³•è§£å†³å‘éŸ³-æƒ…æ„Ÿæ­§ä¹‰é—®é¢˜ï¼Œå³è¯­éŸ³çš„è§†è§‰æ¨¡å¼ä¼šæ¨¡ä»¿æƒ…æ„Ÿè¡¨è¾¾ï¼›åŒæ—¶ç¼ºä¹èƒ½å¤Ÿè¯„ä¼°è§†è§‰åŸºç¡€å’Œæ¨ç†æ·±åº¦çš„å¯éªŒè¯è¯„ä¼°æŒ‡æ ‡ï¼Œè¿™é˜»ç¢äº†è¯¥é¢†åŸŸçš„è¿›å±•ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„ç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¦–å…ˆï¼Œå¼•å…¥å¤šçº§æ´å¯Ÿç½‘ç»œç”¨äºè§£ç¼ ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å±‚æ¬¡è§†è§‰ç¼–ç å™¨ï¼Œé€šè¿‡çŠ¶æ€åˆ¤æ–­æ¨¡å—åŸºäºæ—¶é—´ç‰¹å¾æ–¹å·®ç®—æ³•æ€§åœ°æŠ‘åˆ¶æ¨¡ç³Šçš„å”‡éƒ¨ç‰¹å¾ï¼Œå®ç°æ˜¾å¼è§†è§‰è§£ç¼ ï¼›å…¶æ¬¡ï¼Œæ„å»ºäº†ConvoInsight-DBå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«ä¸“å®¶æ ‡æ³¨çš„å¾®è¡¨æƒ…å’Œæ·±åº¦å¿ƒç†æ¨ç†ï¼›ç¬¬ä¸‰ï¼Œè®¾è®¡äº†å¿ƒç†æ¨ç†æ´å¯Ÿè¯„çº§æŒ‡æ ‡ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–ç»´åº¦æ¡†æ¶ï¼Œä½¿ç”¨ä¸“å®¶æŒ‡å¯¼çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯„ä¼°å¤§å‹å¿ƒç†è§†è§‰æ¨¡å‹çš„å¤šç»´æ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> åœ¨PRISMåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMINDæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œåœ¨å¾®è¡¨æƒ…æ£€æµ‹ä¸Šå®ç°äº†+86.95%çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†å…ˆå‰çš„æœ€å…ˆè¿›æ°´å¹³ã€‚æ¶ˆèç ”ç©¶è¯å®ï¼ŒçŠ¶æ€åˆ¤æ–­è§£ç¼ æ¨¡å—æ˜¯è¿™ä¸€æ€§èƒ½é£è·ƒçš„æœ€å…³é”®ç»„ä»¶ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ç®—æ³•æ€§è§†è§‰è§£ç¼ ã€å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†å’Œè‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä¸ºç”Ÿæˆå¼å¿ƒç†åˆ†æå»ºç«‹äº†ä¸€ä¸ªå®Œæ•´çš„ç”Ÿæ€ç³»ç»Ÿã€‚çŠ¶æ€åˆ¤æ–­æ¨¡å—çš„æˆåŠŸè¡¨æ˜æ˜¾å¼å¤„ç†å‘éŸ³-æƒ…æ„Ÿæ­§ä¹‰å¯¹äºå¿ƒç†æ¨ç†è‡³å…³é‡è¦ï¼Œä¸ºæœªæ¥åœ¨çœŸå®å¯¹è¯ä¸­çš„å¿ƒç†çŠ¶æ€åˆ†ææä¾›äº†å¯é çš„æŠ€æœ¯åŸºç¡€å’Œæ–¹æ³•è®ºæ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Generative psychological analysis of in-the-wild conversations faces two fundamental challenges: (1) existing Vision-Language Models (VLMs) fail to resolve Articulatory-Affective Ambiguity, where visual patterns of speech mimic emotional expressions; and (2) progress is stifled by a lack of verifiable evaluation metrics capable of assessing visual grounding and reasoning depth. We propose a complete ecosystem to address these twin challenges. First, we introduce Multilevel Insight Network for Disentanglement(MIND), a novel hierarchical visual encoder that introduces a Status Judgment module to algorithmically suppress ambiguous lip features based on their temporal feature variance, achieving explicit visual disentanglement. Second, we construct ConvoInsight-DB, a new large-scale dataset with expert annotations for micro-expressions and deep psychological inference. Third, Third, we designed the Mental Reasoning Insight Rating Metric (PRISM), an automated dimensional framework that uses expert-guided LLM to measure the multidimensional performance of large mental vision models. On our PRISM benchmark, MIND significantly outperforms all baselines, achieving a +86.95% gain in micro-expression detection over prior SOTA. Ablation studies confirm that our Status Judgment disentanglement module is the most critical component for this performance leap. Our code has been opened.</p>
<h3 id="17-dvlm-ad-enhance-diffusion-vision-language-model-for-driving-via-controllable-reasoning">[17] <a href="https://arxiv.org/abs/2512.04459">dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning</a></h3>
<p><em>Yingzi Ma, Yulong Cao, Wenhao Ding, Shuibai Zhang, Yan Wang, Boris Ivanovic, Ming Jiang, Marco Pavone, Chaowei Xiao</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºdVLM-ADï¼Œä¸€ç§åŸºäºæ‰©æ•£çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ï¼Œé€šè¿‡ç»Ÿä¸€æ„ŸçŸ¥ã€ç»“æ„åŒ–æ¨ç†å’Œä½å±‚è§„åˆ’æ¥è§£å†³ç°æœ‰è‡ªå›å½’VLMåœ¨æ¨ç†-è¡ŒåŠ¨ä¸€è‡´æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œåœ¨é•¿å°¾åœºæ™¯ä¸­å±•ç°å‡ºæ›´å¥½çš„å¯æ§æ€§å’Œå¯é æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è‡ªåŠ¨é©¾é©¶ç ”ç©¶ä¸»è¦ä¾èµ–è‡ªå›å½’è§†è§‰è¯­è¨€æ¨¡å‹æ¥æå‡ç«¯åˆ°ç«¯é©¾é©¶ç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†è¿™äº›æ¨¡å‹å—é™äºå› æœæ³¨æ„åŠ›å’Œé¡ºåºä»¤ç‰Œç”Ÿæˆæœºåˆ¶ï¼Œéš¾ä»¥ç»´æŒé«˜å±‚æ¨ç†ä¸ä½å±‚è§„åˆ’ä¹‹é—´çš„ä¸€è‡´æ€§å’Œå¯æ§æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åˆ†å¸ƒå¤–é©¾é©¶åœºæ™¯æ—¶å­˜åœ¨æ˜æ˜¾ç¼ºé™·ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºdVLM-ADï¼Œä¸€ç§åŸºäºç¦»æ•£æ‰©æ•£çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨åŒå‘æ³¨æ„åŠ›å’Œè¿­ä»£å»å™ªæœºåˆ¶æ¥ç»Ÿä¸€æ„ŸçŸ¥ã€ç»“æ„åŒ–æ¨ç†å’Œä½å±‚è§„åˆ’ä»»åŠ¡ï¼Œç›¸æ¯”è‡ªå›å½’æ¨¡å‹å…·æœ‰æ›´å¥½çš„å¯æ§æ€§å’Œå¯é æ€§ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´ä¸€è‡´çš„æ¨ç†-è¡ŒåŠ¨å¯¹ã€‚</p>
<p><strong>Result:</strong> åœ¨nuSceneså’ŒWOD-E2EåŸºå‡†æµ‹è¯•ä¸­ï¼ŒdVLM-ADåœ¨è¡Œä¸º-è½¨è¿¹ä¸€è‡´æ€§æ–¹é¢æ¯”è‡ªå›å½’åŸºçº¿æå‡9%ï¼Œåœ¨é•¿å°¾WOD-E2Eåœºæ™¯ä¸­RFSæŒ‡æ ‡æå‡6%ï¼Œå°½ç®¡ä½¿ç”¨ç›¸å¯¹ç®€å•çš„éª¨å¹²ç½‘ç»œï¼Œå…¶è§„åˆ’æ€§èƒ½ä»ä¸ç°æœ‰é©¾é©¶VLM/VLAç³»ç»Ÿç›¸å½“ï¼Œå¹¶äº§ç”Ÿæ›´ä¸€è‡´çš„æ¨ç†-è¡ŒåŠ¨å¯¹ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜åŸºäºæ‰©æ•£çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºå¯æ‰©å±•çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æä¾›äº†ä¸€æ¡å¯æ§ä¸”å¯é çš„é€”å¾„ï¼ŒåŒå‘æ³¨æ„åŠ›æœºåˆ¶å’Œè¿­ä»£å»å™ªè¿‡ç¨‹èƒ½å¤Ÿæœ‰æ•ˆæ”¹å–„æ¨ç†ä¸è§„åˆ’çš„ä¸€è‡´æ€§ï¼Œä¸ºè§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„é•¿å°¾åˆ†å¸ƒå¤–åœºæ™¯æŒ‘æˆ˜æä¾›äº†æ–°çš„æŠ€æœ¯æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision-language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. However, most existing VLMs or vision-language agents (VLAs) are built upon autoregressive (AR) models. In this paper, we observe that existing AR-based VLMs -- limited by causal attention and sequential token generation -- often fail to maintain consistency and controllability between high-level reasoning and low-level planning. In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. Building on these observations, we introduce dVLM-AD, a diffusion-based vision-language model that unifies perception, structured reasoning, and low-level planning for end-to-end driving. Evaluated on nuScenes and WOD-E2E, dVLM-AD yields more consistent reasoning-action pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, outperforming AR-based baselines with a 9 percent improvement in behavior-trajectory consistency and a 6 percent increase in RFS on long-tail WOD-E2E scenarios. These results suggest a controllable and reliable pathway for scalable end-to-end driving.</p>
<h3 id="18-e3ad-an-emotion-aware-vision-language-action-model-for-human-centric-end-to-end-autonomous-driving">[18] <a href="https://arxiv.org/abs/2512.04733">E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving</a></h3>
<p><em>Yihong Tang, Haicheng Liao, Tong Nie, Junlin He, Ao Qu, Kehua Chen, Wei Ma, Zhenning Li, Lijun Sun, Chengzhong Xu</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºE3ADï¼Œä¸€ç§æƒ…æ„Ÿæ„ŸçŸ¥çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆè¿ç»­æƒ…æ„Ÿå»ºæ¨¡å’ŒåŒé€šè·¯ç©ºé—´æ¨ç†ï¼Œå®ç°äº†å¯¹ä¹˜å®¢æƒ…æ„ŸçŠ¶æ€çš„ç†è§£ä¸å“åº”ï¼Œä»è€Œæå‡è‡ªåŠ¨é©¾é©¶çš„äººæœºå¯¹é½ä¸èˆ’é€‚åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæ™®éé‡‡ç”¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œä½†é€šå¸¸å¿½ç•¥ä¹˜å®¢çš„æƒ…æ„ŸçŠ¶æ€ï¼Œè€Œæƒ…æ„ŸçŠ¶æ€å¯¹äºé©¾é©¶èˆ’é€‚åº¦å’Œè‡ªåŠ¨é©¾é©¶æ¥å—åº¦è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œæå‡ºå¼€æ”¾åŸŸç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¦‚å¿µï¼Œè¦æ±‚è‡ªåŠ¨é©¾é©¶è½¦è¾†èƒ½å¤Ÿç†è§£è‡ªç”±å½¢å¼çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€æ¨æ–­æƒ…æ„Ÿå¹¶è§„åˆ’ç‰©ç†å¯è¡Œçš„è½¨è¿¹ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºE3ADæƒ…æ„Ÿæ„ŸçŸ¥VLAæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªè®¤çŸ¥å¯å‘ç»„ä»¶ï¼šé‡‡ç”¨è¿ç»­Valence-Arousal-Dominanceæƒ…æ„Ÿæ¨¡å‹ä»è¯­è¨€ä¸­æ•æ‰è¯­æ°”å’Œç´§è¿«æ€§ï¼›è®¾è®¡åŒé€šè·¯ç©ºé—´æ¨ç†æ¨¡å—ï¼Œèåˆè‡ªæˆ‘ä¸­å¿ƒå’Œä»–è€…ä¸­å¿ƒè§†è§’ä»¥å®ç°ç±»äººç©ºé—´è®¤çŸ¥ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨ä¸€è‡´æ€§å¯¼å‘çš„è®­ç»ƒæ–¹æ¡ˆï¼Œç»“åˆæ¨¡æ€é¢„è®­ç»ƒå’ŒåŸºäºåå¥½çš„å¯¹é½ï¼Œç¡®ä¿æƒ…æ„Ÿæ„å›¾ä¸é©¾é©¶è¡Œä¸ºçš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒE3ADæ˜¾è‘—æå‡äº†è§†è§‰åŸºç¡€å®šä½å’Œè·¯å¾„ç‚¹è§„åˆ’æ€§èƒ½ï¼Œå¹¶åœ¨æƒ…æ„Ÿä¼°è®¡æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„VADç›¸å…³æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡æ³¨å…¥æƒ…æ„Ÿç†è§£ï¼Œå®ç°äº†æ›´ç¬¦åˆäººç±»æœŸæœ›çš„åŸºç¡€å®šä½ã€è§„åˆ’å’Œäººæœ¬åé¦ˆã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å°†æƒ…æ„Ÿç†è§£æ•´åˆåˆ°VLAé£æ ¼è‡ªåŠ¨é©¾é©¶ä¸­èƒ½å¤Ÿäº§ç”Ÿæ›´äººæ€§åŒ–çš„é©¾é©¶è¡Œä¸ºï¼Œè¿™ä¸ºæœªæ¥è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„äººæœºäº¤äº’è®¾è®¡æä¾›äº†é‡è¦å¯ç¤ºã€‚æƒ…æ„Ÿæ„ŸçŸ¥ä¸ä»…æå‡äº†æŠ€æœ¯æ€§èƒ½ï¼Œæ›´é‡è¦çš„æ˜¯å¢å¼ºäº†ç³»ç»Ÿçš„äººæœ¬å¯¹é½ç‰¹æ€§ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶çš„å¹¿æ³›æ¥å—å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.</p>
<h3 id="19-units-unified-time-series-generative-model-for-remote-sensing">[19] <a href="https://arxiv.org/abs/2512.04461">UniTS: Unified Time Series Generative Model for Remote Sensing</a></h3>
<p><em>Yuxiang Zhang, Shunlin Liang, Wenyuan Li, Han Ma, Jianglei Xu, Yichuan Ma, Jiangwei Xie, Wei Li, Mengmeng Zhang, Ran Tao, Xiang-Gen Xia</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UniTSï¼ˆç»Ÿä¸€æ—¶é—´åºåˆ—ç”Ÿæˆæ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæµåŒ¹é…ç”ŸæˆèŒƒå¼çš„é€šç”¨æ¡†æ¶ï¼Œèƒ½å¤Ÿç»Ÿä¸€å¤„ç†æ—¶é—´åºåˆ—é‡å»ºã€äº‘å»é™¤ã€è¯­ä¹‰å˜åŒ–æ£€æµ‹å’Œé¢„æµ‹ç­‰å¤šç§ä»»åŠ¡ï¼Œé€šè¿‡æ„å»ºä»å™ªå£°åˆ°ç›®æ ‡çš„ç¡®å®šæ€§æ¼”åŒ–è·¯å¾„å®ç°æ—¶ç©ºç‰¹å¾ç»Ÿä¸€å»ºæ¨¡ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å«æ˜Ÿé¥æ„Ÿæ–¹æ³•é€šå¸¸éœ€è¦ä¸ºä¸åŒä»»åŠ¡è®¾è®¡ä¸“é—¨æ¨¡å‹ï¼Œç¼ºä¹è·¨å¤šä¸ªæ—¶é—´åºåˆ—ä»»åŠ¡çš„æ—¶ç©ºç‰¹å¾ç»Ÿä¸€å»ºæ¨¡èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚åœ°çƒç¯å¢ƒåŠ¨æ€æ•æ‰ä¸­çš„é€šç”¨æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> UniTSåŸºäºæµåŒ¹é…ç”ŸæˆèŒƒå¼ï¼Œæ„å»ºä»å™ªå£°åˆ°ç›®æ ‡çš„ç¡®å®šæ€§æ¼”åŒ–è·¯å¾„ï¼Œé‡‡ç”¨å…·æœ‰æ—¶ç©ºå—çš„æ‰©æ•£å˜æ¢å™¨æ¶æ„ï¼Œè®¾è®¡äº†è‡ªé€‚åº”æ¡ä»¶æ³¨å…¥å™¨å¢å¼ºå¤šæ¨¡æ€è¾“å…¥çš„æ¡ä»¶æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶å¼•å…¥æ—¶ç©ºæ„ŸçŸ¥è°ƒåˆ¶å™¨æå‡å¤æ‚æ—¶ç©ºä¾èµ–çš„æ•è·èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜UniTSåœ¨ä½å±‚å’Œé«˜å±‚æ—¶é—´åºåˆ—ä»»åŠ¡ä¸­å‡å±•ç°å‡ºå“è¶Šçš„ç”Ÿæˆå’Œè®¤çŸ¥èƒ½åŠ›ï¼Œåœ¨ä¸¥é‡äº‘æ±¡æŸ“ã€æ¨¡æ€ç¼ºå¤±å’Œç‰©å€™å˜åŒ–é¢„æµ‹ç­‰æŒ‘æˆ˜ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶æ„å»ºçš„TS-S12å’ŒTS-S12CRæ•°æ®é›†å¡«è¡¥äº†æ—¶é—´åºåˆ—äº‘å»é™¤å’Œé¢„æµ‹ä»»åŠ¡çš„åŸºå‡†æ•°æ®é›†ç©ºç™½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºæµåŒ¹é…çš„ç»Ÿä¸€ç”Ÿæˆæ¡†æ¶åœ¨å¤šç§æ—¶é—´åºåˆ—ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œä¸ºå«æ˜Ÿé¥æ„Ÿé¢†åŸŸçš„å¤šä»»åŠ¡ç»Ÿä¸€å»ºæ¨¡æä¾›äº†æ–°èŒƒå¼ï¼ŒåŒæ—¶é«˜è´¨é‡æ•°æ®é›†çš„æ„å»ºå°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„åŸºå‡†è¯„ä¼°å’Œç®—æ³•å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>One of the primary objectives of satellite remote sensing is to capture the complex dynamics of the Earth environment, which encompasses tasks such as reconstructing continuous cloud-free time series images, detecting land cover changes, and forecasting future surface evolution. However, existing methods typically require specialized models tailored to different tasks, lacking unified modeling of spatiotemporal features across multiple time series tasks. In this paper, we propose a Unified Time Series Generative Model (UniTS), a general framework applicable to various time series tasks, including time series reconstruction, time series cloud removal, time series semantic change detection, and time series forecasting. Based on the flow matching generative paradigm, UniTS constructs a deterministic evolution path from noise to targets under the guidance of task-specific conditions, achieving unified modeling of spatiotemporal representations for multiple tasks. The UniTS architecture consists of a diffusion transformer with spatio-temporal blocks, where we design an Adaptive Condition Injector (ACor) to enhance the model's conditional perception of multimodal inputs, enabling high-quality controllable generation. Additionally, we design a Spatiotemporal-aware Modulator (STM) to improve the ability of spatio-temporal blocks to capture complex spatiotemporal dependencies. Furthermore, we construct two high-quality multimodal time series datasets, TS-S12 and TS-S12CR, filling the gap of benchmark datasets for time series cloud removal and forecasting tasks. Extensive experiments demonstrate that UniTS exhibits exceptional generative and cognitive capabilities in both low-level and high-level time series tasks. It significantly outperforms existing methods, particularly when facing challenges such as severe cloud contamination, modality absence, and forecasting phenological variations.</p>
<h3 id="20-reflection-removal-through-efficient-adaptation-of-diffusion-transformers">[20] <a href="https://arxiv.org/abs/2512.05000">Reflection Removal through Efficient Adaptation of Diffusion Transformers</a></h3>
<p><em>Daniyar Zakarin, Thiemo Wandel, Anton Obukhov, Dengxin Dai</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰çš„å•å›¾åƒåå°„å»é™¤æ¡†æ¶ï¼Œé€šè¿‡é‡æ–°åˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ‰©æ•£æ¨¡å‹å¹¶ç»“åˆç‰©ç†æ¸²æŸ“åˆæˆæ•°æ®ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„åå°„å»é™¤æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åå°„å»é™¤æ–¹æ³•é€šå¸¸ä¾èµ–ä»»åŠ¡ç‰¹å®šçš„æ¶æ„ï¼Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ï¼Œä¸”ç°æœ‰åå°„å»é™¤æ•°æ®é›†åœ¨å¤šæ ·æ€§ã€å¯æ‰©å±•æ€§å’ŒçœŸå®æ„Ÿæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¿™é™åˆ¶äº†åå°„å»é™¤æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨æ‰©æ•£å˜æ¢å™¨æ¡†æ¶ï¼Œé‡æ–°åˆ©ç”¨é¢„è®­ç»ƒçš„DiTåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡æ¡ä»¶åå°„æ±¡æŸ“è¾“å…¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆå¹²å‡€çš„é€å°„å±‚ï¼›ä¸ºè§£å†³æ•°æ®çŸ­ç¼ºé—®é¢˜ï¼Œåœ¨Blenderä¸­æ„å»ºåŸºäºç‰©ç†çš„æ¸²æŸ“ç®¡é“ï¼Œä½¿ç”¨Principled BSDFåˆæˆé€¼çœŸçš„ç»ç’ƒæè´¨å’Œåå°„æ•ˆæœï¼›é‡‡ç”¨é«˜æ•ˆçš„LoRAé€‚é…æŠ€æœ¯å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨é¢†åŸŸå†…å’Œé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¡¨æ˜é¢„è®­ç»ƒæ‰©æ•£å˜æ¢å™¨ä¸ç‰©ç†åŸºç¡€æ•°æ®åˆæˆå’Œé«˜æ•ˆé€‚é…ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæä¾›å¯æ‰©å±•ä¸”é«˜ä¿çœŸçš„åå°„å»é™¤è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„æ‰©æ•£å˜æ¢å™¨æ¨¡å‹åœ¨ç»“åˆç‰©ç†åŸºç¡€æ•°æ®åˆæˆå’Œé«˜æ•ˆé€‚é…åï¼Œèƒ½å¤Ÿä¸ºåå°„å»é™¤ä»»åŠ¡æä¾›å¯æ‰©å±•ä¸”é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™ä¸ºåˆ©ç”¨åŸºç¡€æ¨¡å‹è§£å†³å›¾åƒæ¢å¤é—®é¢˜æä¾›äº†æ–°çš„èŒƒå¼ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web</p>
<h3 id="21-not-all-birds-look-the-same-identity-preserving-generation-for-birds">[21] <a href="https://arxiv.org/abs/2512.04485">Not All Birds Look The Same: Identity-Preserving Generation For Birds</a></h3>
<p><em>Aaron Sun, Oindrila Saha, Subhransu Maji</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰èº«ä»½ä¿æŒå›¾åƒç”Ÿæˆæ¨¡å‹åœ¨éåˆšæ€§ã€ç»†ç²’åº¦ç±»åˆ«ï¼ˆå¦‚é¸Ÿç±»ï¼‰ä¸Šçš„å±€é™æ€§ï¼Œæå‡ºäº†NABirds Look-Alikes (NABLA)æ•°æ®é›†ä½œä¸ºè¯„ä¼°åŸºå‡†ï¼Œå¹¶é€šè¿‡æŒ‰ç‰©ç§ã€å¹´é¾„å’Œæ€§åˆ«åˆ†ç»„çš„è®­ç»ƒç­–ç•¥æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨é¸Ÿç±»èº«ä»½ä¿æŒç”Ÿæˆä¸Šçš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¯æ§å›¾åƒç”Ÿæˆæ¨¡å‹è™½ç„¶åœ¨äººç±»å’Œåˆšæ€§æ—¥å¸¸ç‰©ä½“ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éåˆšæ€§æˆ–ç»†ç²’åº¦ç±»åˆ«ï¼ˆå¦‚é¸Ÿç±»ï¼‰ä¸Šä»å­˜åœ¨å±€é™æ€§ï¼Œè¿™äº›é¢†åŸŸç¼ºä¹é«˜è´¨é‡ã€å¯è®¿é—®çš„æ•°æ®ï¼ˆå°¤å…¶æ˜¯è§†é¢‘æˆ–å¤šè§†è§’è§‚æµ‹æ•°æ®ï¼‰ï¼Œä½¿å¾—è¯„ä¼°å’Œæ”¹è¿›å˜å¾—å›°éš¾ï¼Œè€Œé¸Ÿç±»å› å…¶é«˜å¤šæ ·æ€§ã€ç»†ç²’åº¦è¯†åˆ«éœ€æ±‚å’Œå„ç§å§¿æ€å˜åŒ–æˆä¸ºç ”ç©¶è¿™ä¸€é—®é¢˜çš„ç†æƒ³é¢†åŸŸã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†NABirds Look-Alikes (NABLA)æ•°æ®é›†ï¼ŒåŒ…å«4,759ä¸ªä¸“å®¶æ ‡æ³¨çš„å›¾åƒå¯¹ï¼Œç»“åˆä»iNaturalistæ”¶é›†çš„1,073ä¸ªå¤šå›¾åƒè§‚æµ‹å¯¹å’Œå°‘é‡è§†é¢‘æ•°æ®ï¼Œå½¢æˆäº†ä¸€ä¸ªè¯„ä¼°é¸Ÿç±»èº«ä»½ä¿æŒç”Ÿæˆçš„åŸºå‡†ï¼Œå¹¶æå‡ºäº†æŒ‰ç‰©ç§ã€å¹´é¾„å’Œæ€§åˆ«åˆ†ç»„è®­ç»ƒçš„ç­–ç•¥ï¼Œå°†è¿™äº›å±æ€§ä½œä¸ºèº«ä»½çš„ä»£ç†æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šæ— æ³•æœ‰æ•ˆä¿æŒé¸Ÿç±»èº«ä»½ï¼Œè€Œé‡‡ç”¨æŒ‰ç‰©ç§ã€å¹´é¾„å’Œæ€§åˆ«åˆ†ç»„çš„è®­ç»ƒç­–ç•¥åï¼Œæ¨¡å‹åœ¨å·²è§å’Œæœªè§ç‰©ç§ä¸Šçš„æ€§èƒ½å‡å¾—åˆ°æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ç»†ç²’åº¦èº«ä»½ä¿æŒç”Ÿæˆä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†ç»†ç²’åº¦ã€éåˆšæ€§ç±»åˆ«åœ¨èº«ä»½ä¿æŒç”Ÿæˆä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºçš„NABLAæ•°æ®é›†ä¸ºè¯„ä¼°æ­¤ç±»ä»»åŠ¡æä¾›äº†é‡è¦åŸºå‡†ï¼Œåˆ†ç»„è®­ç»ƒç­–ç•¥å±•ç¤ºäº†åˆ©ç”¨å¯è®¿é—®å±æ€§ä½œä¸ºèº«ä»½ä»£ç†çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè¶…è¶Šå†…å®¹åˆ›ä½œå‘éœ€è¦å‡†ç¡®æ€§å’Œç²¾ç»†ç»†èŠ‚çš„åº”ç”¨é¢†åŸŸæ¨è¿›æä¾›äº†æ–¹æ³•è®ºåŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Since the advent of controllable image generation, increasingly rich modes of control have enabled greater customization and accessibility for everyday users. Zero-shot, identity-preserving models such as Insert Anything and OminiControl now support applications like virtual try-on without requiring additional fine-tuning. While these models may be fitting for humans and rigid everyday objects, they still have limitations for non-rigid or fine-grained categories. These domains often lack accessible, high-quality data -- especially videos or multi-view observations of the same subject -- making them difficult both to evaluate and to improve upon. Yet, such domains are essential for moving beyond content creation toward applications that demand accuracy and fine detail. Birds are an excellent domain for this task: they exhibit high diversity, require fine-grained cues for identification, and come in a wide variety of poses. We introduce the NABirds Look-Alikes (NABLA) dataset, consisting of 4,759 expert-curated image pairs. Together with 1,073 pairs collected from multi-image observations on iNaturalist and a small set of videos, this forms a benchmark for evaluating identity-preserving generation of birds. We show that state-of-the-art baselines fail to maintain identity on this dataset, and we demonstrate that training on images grouped by species, age, and sex -- used as a proxy for identity -- substantially improves performance on both seen and unseen species.</p>
<h3 id="22-boundary-aware-test-time-adaptation-for-zero-shot-medical-image-segmentation">[22] <a href="https://arxiv.org/abs/2512.04520">Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation</a></h3>
<p><em>Chenlin Xu, Lei Zhang, Lituan Wang, Xinyu Pu, Pengfei Ma, Guangwu Qian, Zizhou Wang, Yan Wang</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºBA-TTA-SAMæ¡†æ¶ï¼Œé€šè¿‡æµ‹è¯•æ—¶è‡ªé€‚åº”æ˜¾è‘—å¢å¼ºSAMåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œæ— éœ€æºåŸŸè®­ç»ƒæ•°æ®å³å¯å®ç°å¹³å‡12.4%çš„DICEåˆ†æ•°æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œè®¡ç®—æˆæœ¬é«˜çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä»éœ€ä¸‹æ¸¸ä»»åŠ¡ç‰¹å®šè®­ç»ƒã€‚è™½ç„¶SAMç­‰åŸºç¡€æ¨¡å‹å±•ç°å‡ºè‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šä»å—é¢†åŸŸåç§»é™åˆ¶ï¼Œå› æ­¤éœ€è¦é«˜æ•ˆçš„é›¶æ ·æœ¬å¢å¼ºæ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æå‡ºä»»åŠ¡æ— å…³çš„æµ‹è¯•æ—¶è‡ªé€‚åº”æ¡†æ¶BA-TTA-SAMï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®æœºåˆ¶ï¼šç¼–ç å™¨çº§é«˜æ–¯æç¤ºæ³¨å…¥å°†é«˜æ–¯æç¤ºç›´æ¥åµŒå…¥å›¾åƒç¼–ç å™¨ï¼Œä¸ºåˆå§‹è¡¨ç¤ºå­¦ä¹ æä¾›æ˜¾å¼æŒ‡å¯¼ï¼›è·¨å±‚è¾¹ç•Œæ„ŸçŸ¥æ³¨æ„åŠ›å¯¹é½åˆ©ç”¨ViTä¸»å¹²ä¸­çš„å±‚æ¬¡ç‰¹å¾äº¤äº’ï¼Œå°†æ·±å±‚è¯­ä¹‰å“åº”ä¸æµ…å±‚è¾¹ç•Œçº¿ç´¢å¯¹é½ã€‚</p>
<p><strong>Result:</strong> åœ¨ISICã€Kvasirã€BUSIå’ŒREFUGEå››ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸æ¯”SAMçš„é›¶æ ·æœ¬åˆ†å‰²æ€§èƒ½ï¼Œå¹³å‡DICEåˆ†æ•°æå‡12.4%ã€‚è¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æŒç»­ä¼˜äºæœ€å…ˆè¿›æ¨¡å‹ï¼Œä¸”æ— éœ€ä»»ä½•æºåŸŸè®­ç»ƒæ•°æ®ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ¡†æ¶æ˜¾è‘—å¢å¼ºäº†SAMçš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡æµ‹è¯•æ—¶è‡ªé€‚åº”æœ‰æ•ˆè§£å†³äº†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸåç§»é—®é¢˜ã€‚ç ”ç©¶è¯æ˜äº†ä»»åŠ¡æ— å…³è‡ªé€‚åº”åœ¨é›¶æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ— éœ€è®­ç»ƒæ•°æ®çš„æ¨¡å‹é€‚åº”æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM's zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.</p>
<h3 id="23-identity-clue-refinement-and-enhancement-for-visible-infrared-person-re-identification">[23] <a href="https://arxiv.org/abs/2512.04522">Identity Clue Refinement and Enhancement for Visible-Infrared Person Re-Identification</a></h3>
<p><em>Guoqing Zhang, Zhun Wang, Hairui Wang, Zhonglin Ye, Yuhui Zheng</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„èº«ä»½çº¿ç´¢ç²¾ç‚¼ä¸å¢å¼ºç½‘ç»œï¼Œé€šè¿‡æŒ–æ˜å’Œåˆ©ç”¨æ¨¡æ€ç‰¹å®šå±æ€§ä¸­çš„éšå¼åˆ¤åˆ«çŸ¥è¯†æ¥è§£å†³å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«ä¸­çš„æ¨¡æ€å·®å¼‚é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†è·¨æ¨¡æ€åŒ¹é…æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«æ–¹æ³•ä¸»è¦å…³æ³¨å­¦ä¹ æ¨¡æ€ä¸å˜ç‰¹å¾ï¼Œä½†å¾€å¾€åªå…³æ³¨è·¨æ¨¡æ€çš„å…±æœ‰åˆ¤åˆ«è¯­ä¹‰ï¼Œè€Œå¿½è§†äº†æ¨¡æ€ç‰¹å®šèº«ä»½æ„ŸçŸ¥çŸ¥è¯†åœ¨åˆ¤åˆ«ç‰¹å¾å­¦ä¹ ä¸­çš„å…³é”®ä½œç”¨ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†èº«ä»½çº¿ç´¢ç²¾ç‚¼ä¸å¢å¼ºç½‘ç»œï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¤šæ„ŸçŸ¥ç‰¹å¾ç²¾ç‚¼æ¨¡å—é€šè¿‡èšåˆå…±äº«åˆ†æ”¯çš„æµ…å±‚ç‰¹å¾æ¥æ•è·æ˜“è¢«å¿½è§†çš„æ¨¡æ€ç‰¹å®šå±æ€§ï¼›è¯­ä¹‰è’¸é¦çº§è”å¢å¼ºæ¨¡å—ä»èšåˆçš„æµ…å±‚ç‰¹å¾ä¸­è’¸é¦èº«ä»½æ„ŸçŸ¥çŸ¥è¯†å¹¶æŒ‡å¯¼æ¨¡æ€ä¸å˜ç‰¹å¾å­¦ä¹ ï¼›èº«ä»½çº¿ç´¢å¼•å¯¼æŸå¤±åˆ™ç”¨äºç¼“è§£å¢å¼ºç‰¹å¾ä¸­çš„æ¨¡æ€å·®å¼‚å¹¶ä¿ƒè¿›å¤šæ ·åŒ–è¡¨ç¤ºç©ºé—´çš„å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ICREç½‘ç»œæ˜æ˜¾ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨è·¨æ¨¡æ€åŒ¹é…ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†æŒ–æ˜æ¨¡æ€ç‰¹å®šèº«ä»½æ„ŸçŸ¥çŸ¥è¯†çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†æ¨¡æ€ç‰¹å®šèº«ä»½æ„ŸçŸ¥çŸ¥è¯†åœ¨å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«ä¸­çš„é‡è¦æ€§ï¼Œæå‡ºçš„æ¡†æ¶é€šè¿‡ç²¾ç‚¼å’Œå¢å¼ºèº«ä»½çº¿ç´¢æœ‰æ•ˆç¼“è§£äº†æ¨¡æ€å·®å¼‚ï¼Œä¸ºè·¨æ¨¡æ€ç‰¹å¾å­¦ä¹ æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•è®ºæŒ‡å¯¼ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Visible-Infrared Person Re-Identification (VI-ReID) is a challenging cross-modal matching task due to significant modality discrepancies. While current methods mainly focus on learning modality-invariant features through unified embedding spaces, they often focus solely on the common discriminative semantics across modalities while disregarding the critical role of modality-specific identity-aware knowledge in discriminative feature learning. To bridge this gap, we propose a novel Identity Clue Refinement and Enhancement (ICRE) network to mine and utilize the implicit discriminative knowledge inherent in modality-specific attributes. Initially, we design a Multi-Perception Feature Refinement (MPFR) module that aggregates shallow features from shared branches, aiming to capture modality-specific attributes that are easily overlooked. Then, we propose a Semantic Distillation Cascade Enhancement (SDCE) module, which distills identity-aware knowledge from the aggregated shallow features and guide the learning of modality-invariant features. Finally, an Identity Clues Guided (ICG) Loss is proposed to alleviate the modality discrepancies within the enhanced features and promote the learning of a diverse representation space. Extensive experiments across multiple public datasets clearly show that our proposed ICRE outperforms existing SOTA methods.</p>
<h3 id="24-x-humanoid-robotize-human-videos-to-generate-humanoid-videos-at-scale">[24] <a href="https://arxiv.org/abs/2512.04537">X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale</a></h3>
<p><em>Pei Yang, Hai Ci, Yiren Song, Mike Zheng Shou</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†X-Humanoidï¼Œä¸€ç§ç”Ÿæˆå¼è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œé€šè¿‡å°†å¼ºå¤§çš„Wan 2.2æ¨¡å‹é€‚é…ä¸ºè§†é¢‘åˆ°è§†é¢‘ç»“æ„å¹¶é’ˆå¯¹äººå½¢æœºå™¨äººè½¬æ¢ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œè§£å†³äº†ä»ç¬¬ä¸‰äººç§°è§†é¢‘ä¸­ç”Ÿæˆå¤§è§„æ¨¡äººå½¢æœºå™¨äººè®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å…·èº«äººå·¥æ™ºèƒ½çš„å‘å±•å—åˆ°å¤§è§„æ¨¡å¤šæ ·åŒ–è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„ä¸¥é‡åˆ¶çº¦ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸Š"å åŠ "æœºå™¨äººæ‰‹è‡‚æ¥å¤„ç†ï¼Œæ— æ³•å¤„ç†ç¬¬ä¸‰äººç§°è§†é¢‘ä¸­å¤æ‚çš„å…¨èº«è¿åŠ¨å’Œåœºæ™¯é®æŒ¡ï¼Œå› æ­¤ä¸é€‚åˆå°†äººç±»è§†é¢‘è½¬åŒ–ä¸ºæœºå™¨äººè®­ç»ƒæ•°æ®ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•å°†Wan 2.2æ¨¡å‹é€‚é…ä¸ºè§†é¢‘åˆ°è§†é¢‘ç»“æ„ï¼Œå¹¶é’ˆå¯¹äººå½¢æœºå™¨äººè½¬æ¢ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶è®¾è®¡äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®åˆ›å»ºæµç¨‹ï¼Œåˆ©ç”¨Unreal Engineå°†ç¤¾åŒºèµ„äº§è½¬åŒ–ä¸º17å°æ—¶ä»¥ä¸Šçš„é…å¯¹åˆæˆè§†é¢‘ï¼Œç„¶åå°†è®­ç»ƒæ¨¡å‹åº”ç”¨äº60å°æ—¶çš„Ego-Exo4Dè§†é¢‘ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶ç”Ÿæˆäº†åŒ…å«è¶…è¿‡360ä¸‡å¸§"æœºå™¨äººåŒ–"äººå½¢æœºå™¨äººè§†é¢‘å¸§çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå®šé‡åˆ†æå’Œç”¨æˆ·ç ”ç©¶è¯å®äº†è¯¥æ–¹æ³•ä¼˜äºç°æœ‰åŸºçº¿ï¼š69%çš„ç”¨æˆ·åœ¨è¿åŠ¨ä¸€è‡´æ€§æ–¹é¢ç»™äºˆæœ€é«˜è¯„åˆ†ï¼Œ62.1%çš„ç”¨æˆ·åœ¨ä½“ç°æ­£ç¡®æ€§æ–¹é¢ç»™äºˆæœ€é«˜è¯„åˆ†ã€‚</p>
<p><strong>Conclusion:</strong> X-Humanoidæ–¹æ³•æˆåŠŸè§£å†³äº†ä»äººç±»è§†é¢‘ç”Ÿæˆäººå½¢æœºå™¨äººè®­ç»ƒæ•°æ®çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºå…·èº«äººå·¥æ™ºèƒ½æä¾›äº†å¤§è§„æ¨¡å¤šæ ·åŒ–çš„è®­ç»ƒèµ„æºï¼Œå…¶å¯æ‰©å±•çš„æ•°æ®åˆ›å»ºæµç¨‹å’Œç”Ÿæˆçš„é«˜è´¨é‡æ•°æ®é›†æœ‰æœ›åŠ é€Ÿäººå½¢æœºå™¨äººè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹å’Œä¸–ç•Œæ¨¡å‹çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to "robotize" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly "overlay" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million "robotized" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.</p>
<h3 id="25-videomem-enhancing-ultra-long-video-understanding-via-adaptive-memory-management">[25] <a href="https://arxiv.org/abs/2512.04540">VideoMem: Enhancing Ultra-Long Video Understanding via Adaptive Memory Management</a></h3>
<p><em>Hongbo Jin, Qingyuan Wang, Wenhao Zhang, Yang Liu, Sijie Cheng</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºVideoMemæ¡†æ¶ï¼Œå°†è¶…é•¿è§†é¢‘ç†è§£å»ºæ¨¡ä¸ºåºåˆ—ç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡è‡ªé€‚åº”å†…å­˜ç®¡ç†å’Œæ¸è¿›å¼å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ˜¾è‘—æå‡äº†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¶…é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¶…é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å­˜åœ¨ä¸Šä¸‹æ–‡é•¿åº¦æœ‰é™å’Œé•¿æœŸè®°å¿†ä¿ç•™æ•ˆç‡ä½çš„é—®é¢˜ï¼Œè€ŒåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„å¤–éƒ¨çŸ¥è¯†åº“æ–¹æ³•åˆ™å¸¦æ¥å·¨å¤§çš„å­˜å‚¨å’Œè®¡ç®—å¼€é”€ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> VideoMemæ¡†æ¶å°†è¶…é•¿è§†é¢‘ç†è§£å»ºæ¨¡ä¸ºåºåˆ—ç”Ÿæˆä»»åŠ¡ï¼Œé‡‡ç”¨è‡ªé€‚åº”å†…å­˜ç®¡ç†åŠ¨æ€æ›´æ–°å…¨å±€å†…å­˜ç¼“å†²åŒºï¼Œä¿ç•™å…³é”®ä¿¡æ¯å¹¶ä¸¢å¼ƒå†—ä½™å†…å®¹ï¼›åŒæ—¶é›†æˆäº†æ¸è¿›å¼åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ŒåŒ…å«æ¸è¿›çŠ¶æ€ä¼ æ’­æ¨¡å—è‡ªé€‚åº”ä¿ç•™æœ‰æ•ˆçŠ¶æ€å¹¶ä¼ æ’­åˆ°ä¸‹ä¸€æ­¥ï¼Œä»¥åŠæ—¶é—´çº§è”å¥–åŠ±æ¨¡å—ç¼“è§£å¥–åŠ±ç¨€ç–æ€§é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªè¶…é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVideoMemæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ï¼Œåœ¨æ ·æœ¬åˆ©ç”¨ç‡å’Œæ”¶æ•›é€Ÿåº¦æ–¹é¢å‡æœ‰æ˜æ˜¾æå‡ï¼ŒéªŒè¯äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å°†è¶…é•¿è§†é¢‘ç†è§£å»ºæ¨¡ä¸ºåºåˆ—ç”Ÿæˆä»»åŠ¡å¹¶é€šè¿‡è‡ªé€‚åº”å†…å­˜ç®¡ç†ç»“åˆæ¸è¿›å¼å¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œä¸ºé•¿è§†é¢‘ç†è§£ä»»åŠ¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œé¿å…äº†ä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•çš„é«˜æ˜‚å¼€é”€ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Ultra long video understanding remains an open challenge, as existing vision language models (VLMs) falter on such content due to limited context length and inefficient long term memory retention. To address this, recent works have attempted to construct external knowledge bases and corresponding retrieval agumented generation (RAG) systems, yet these incur enormous storage and computational overhead. In this paper, we propose VideoMem, a novel framework that pioneers models long video understanding as a sequential generation task via adaptive memory management. Specifically, VideoMem dynamically updates a global memory buffer, which adaptively retains critical information while discarding redundant content across the video timeline. To efficiently train VLMs for such long-term tasks, VideoMem integrates the Progressive Grouped Relative Policy Optimization (PRPO) algorithm, equipped with two core modules: Progressive State Propagation (PSP) adaptively retains valid current states, propagates them to the next rollout step, and gradually narrows the model exploration space. Temporal Cascading Reward (TCR) further alleviates reward sparsity, improving sample utilization and accelerating convergence. Extensive experiments demonstrate that VideoMem significantly outperforms existing open-source models across diverse benchmarks for ultra-long video understanding tasks.</p>
<h3 id="26-counterfeit-answers-adversarial-forgery-against-ocr-free-document-visual-question-answering">[26] <a href="https://arxiv.org/abs/2512.04554">Counterfeit Answers: Adversarial Forgery against OCR-Free Document Visual Question Answering</a></h3>
<p><em>Marco Pintore, Maura Pintor, Dimosthenis Karatzas, Battista Biggio</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡é’ˆå¯¹æ–‡æ¡£è§†è§‰é—®ç­”ï¼ˆDocVQAï¼‰ç³»ç»Ÿæå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯¹æŠ—æ”»å‡»åœºæ™¯ï¼Œå¼€å‘äº†èƒ½å¤Ÿç”Ÿæˆè§†è§‰ä¸Šéš¾ä»¥å¯Ÿè§‰ä½†è¯­ä¹‰ä¸Šæœ‰é’ˆå¯¹æ€§çš„ä¼ªé€ æ–‡æ¡£çš„æ”»å‡»ç®—æ³•ï¼Œå¹¶åœ¨ä¸¤ç§æœ€å…ˆè¿›çš„ç«¯åˆ°ç«¯æ¨¡å‹ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å½“å‰æ–‡æ¡£è§†è§‰é—®ç­”æ¨¡å‹å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ”»å‡»çš„å¨èƒã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰DocVQAç³»ç»Ÿåœ¨é¢å¯¹è§†è§‰ä¸Šéš¾ä»¥å¯Ÿè§‰ä½†è¯­ä¹‰ä¸Šæœ‰é’ˆå¯¹æ€§çš„æ–‡æ¡£ä¼ªé€ æ”»å‡»æ—¶çš„è„†å¼±æ€§é—®é¢˜ï¼Œæ¢ç´¢æ”»å‡»è€…å¦‚ä½•é€šè¿‡ç‰¹å®šæ–¹å¼ç¯¡æ”¹æ–‡æ¡£å†…å®¹æ¥è¯±å¯¼æ¨¡å‹äº§ç”Ÿé”™è¯¯ç­”æ¡ˆã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶å¼€å‘äº†ä¸“é—¨çš„æ”»å‡»ç®—æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆé’ˆå¯¹ä¸åŒæ”»å‡»è€…ç›®æ ‡å®šåˆ¶çš„å¯¹æŠ—æ€§ä¼ªé€ æ–‡æ¡£ï¼ŒåŒ…æ‹¬æœ‰é’ˆå¯¹æ€§çš„é”™è¯¯ä¿¡æ¯ä¼ æ’­å’Œç³»ç»Ÿæ€§æ¨¡å‹å¤±æ•ˆåœºæ™¯ã€‚è¿™äº›ç®—æ³•æ—¨åœ¨ä»¥è§†è§‰ä¸Šéš¾ä»¥å¯Ÿè§‰çš„æ–¹å¼ä¿®æ”¹æ–‡æ¡£å†…å®¹ï¼ŒåŒæ—¶å®ç°è¯­ä¹‰å±‚é¢çš„é’ˆå¯¹æ€§æ”»å‡»ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶åœ¨ä¸¤ç§æœ€å…ˆè¿›çš„ç«¯åˆ°ç«¯æ¨¡å‹ä¸ŠéªŒè¯äº†æ”»å‡»æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼šPix2Structï¼ˆä¸€ç§é€šè¿‡åºåˆ—åˆ°åºåˆ—å»ºæ¨¡è”åˆå¤„ç†å›¾åƒå’Œæ–‡æœ¬çš„è§†è§‰è¯­è¨€å˜æ¢å™¨ï¼‰å’ŒDonutï¼ˆä¸€ç§ç›´æ¥ä»æ–‡æ¡£å›¾åƒä¸­æå–æ–‡æœ¬å¹¶å›ç­”é—®é¢˜çš„åŸºäºå˜æ¢å™¨çš„æ¨¡å‹ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ”»å‡»æ–¹æ³•èƒ½å¤ŸæˆåŠŸè¯±å¯¼è¿™äº›æ¨¡å‹äº§ç”Ÿç‰¹å®šæˆ–æ™®éé”™è¯¯çš„ç­”æ¡ˆã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶æ­ç¤ºäº†å½“å‰DocVQAç³»ç»Ÿä¸­å­˜åœ¨çš„å…³é”®å®‰å…¨æ¼æ´ï¼Œè¡¨æ˜å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿå®¹æ˜“å—åˆ°ç²¾å¿ƒè®¾è®¡çš„å¯¹æŠ—æ”»å‡»ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¼€å‘æ›´å¼ºå¤§é˜²å¾¡æœºåˆ¶çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å…³äºæ–‡æ¡£è§†è§‰é—®ç­”ç³»ç»Ÿé²æ£’æ€§çš„é‡è¦è§è§£ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Document Visual Question Answering (DocVQA) enables end-to-end reasoning grounded on information present in a document input. While recent models have shown impressive capabilities, they remain vulnerable to adversarial attacks. In this work, we introduce a novel attack scenario that aims to forge document content in a visually imperceptible yet semantically targeted manner, allowing an adversary to induce specific or generally incorrect answers from a DocVQA model. We develop specialized attack algorithms that can produce adversarially forged documents tailored to different attackers' goals, ranging from targeted misinformation to systematic model failure scenarios. We demonstrate the effectiveness of our approach against two end-to-end state-of-the-art models: Pix2Struct, a vision-language transformer that jointly processes image and text through sequence-to-sequence modeling, and Donut, a transformer-based model that directly extracts text and answers questions from document images. Our findings highlight critical vulnerabilities in current DocVQA systems and call for the development of more robust defenses.</p>
<h3 id="27-cooper-a-unified-model-for-cooperative-perception-and-reasoning-in-spatial-intelligence">[27] <a href="https://arxiv.org/abs/2512.04563">COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence</a></h3>
<p><em>Zefeng Zhang, Xiangzhao Hao, Hengzhu Tang, Zhenyu Zhang, Jiawei Sheng, Xiaodong Li, Zhenyang Li, Li Gao, Daiting Shi, Dawei Yin, Tingwen Liu</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCOOPERï¼Œä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ·±åº¦å’Œåˆ†å‰²ä½œä¸ºè¾…åŠ©æ¨¡æ€ï¼Œç»“åˆè‡ªé€‚åº”äº¤é”™æ¨ç†ï¼Œæ˜¾è‘—æå‡è§†è§‰ç©ºé—´æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒé€šç”¨æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸‰ç»´æ„ŸçŸ¥æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å­¤ç«‹åœ°å¢å¼ºæ„ŸçŸ¥ï¼ˆé€šè¿‡æ·±åº¦å’Œåˆ†å‰²ç­‰è¾…åŠ©æ¨¡æ€ï¼‰æˆ–æ¨ç†ï¼ˆé€šè¿‡ç©ºé—´VQAæ•°æ®é›†è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œç¼ºä¹ç»Ÿä¸€çš„æ¡†æ¶æ¥ååŒæå‡ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> COOPERé‡‡ç”¨ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œåˆ©ç”¨æ·±åº¦å’Œåˆ†å‰²ä½œä¸ºè¾…åŠ©æ¨¡æ€ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µå­¦ä¹ è¾…åŠ©æ¨¡æ€ç”Ÿæˆèƒ½åŠ›ï¼Œç¬¬äºŒé˜¶æ®µè·å¾—è‡ªé€‚åº”äº¤é”™æ¨ç†èƒ½åŠ›ï¼Œå®ç°æ„ŸçŸ¥ä¸æ¨ç†çš„ååŒå¢å¼ºã€‚</p>
<p><strong>Result:</strong> COPERåœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†å¹³å‡6.91%çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä¿æŒäº†é€šç”¨æ€§èƒ½ï¼›ä»…è¿›è¡Œè¾…åŠ©æ¨¡æ€ç”Ÿæˆçš„å˜ä½“åœ¨è·ç¦»å’Œå¤§å°ä¼°è®¡ä»»åŠ¡ä¸Šè·å¾—äº†7.92%çš„å¢ç›Šï¼Œè¡¨æ˜è¾…åŠ©æ¨¡æ€ç”Ÿæˆæœ‰åŠ©äºå†…åŒ–ç©ºé—´çŸ¥è¯†ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç»Ÿä¸€çš„æ¨¡å‹æ¶æ„å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå‘å±•å‡ºå†…åœ¨çš„ç©ºé—´æ„ŸçŸ¥å¢å¼ºèƒ½åŠ›ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”äº¤é”™æ¨ç†å®ç°æ›´å¼ºçš„ç©ºé—´æ™ºèƒ½ï¼Œä¸ºä¸‰ç»´æ„ŸçŸ¥æ¨ç†æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \textbf{7.92\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.</p>
<h3 id="28-tardis-time-attenuated-representation-disentanglement-for-incomplete-multi-modal-tumor-segmentation-and-classification">[28] <a href="https://arxiv.org/abs/2512.04576">TARDis: Time Attenuated Representation Disentanglement for Incomplete Multi-Modal Tumor Segmentation and Classification</a></h3>
<p><em>Zishuo Wan, Qinqin Kang, Yi Huang, Yun Bian, Dawei Ding, Ke Yan</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ—¶é—´è¡°å‡è¡¨ç¤ºè§£è€¦ï¼ˆTARDisï¼‰çš„ç‰©ç†æ„ŸçŸ¥æ¡†æ¶ï¼Œé€šè¿‡å°†ç¼ºå¤±æ¨¡æ€é‡æ–°å®šä¹‰ä¸ºè¿ç»­æ—¶é—´è¡°å‡æ›²çº¿ä¸Šçš„ç¼ºå¤±é‡‡æ ·ç‚¹ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæœŸç›¸CTæˆåƒä¸­çš„æ¨¡æ€ç¼ºå¤±é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†è‚¿ç˜¤åˆ†å‰²å’Œè¯Šæ–­çš„é²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨å¯¹æ¯”å¢å¼ºCTæˆåƒä¸­ï¼Œè‚¿ç˜¤åˆ†å‰²å’Œè¯Šæ–­ä¸¥é‡ä¾èµ–å¯¹æ¯”å‰‚çš„ç”Ÿç†åŠ¨åŠ›å­¦ç‰¹æ€§ï¼Œä½†ä¸´åºŠå®è·µä¸­å¸¸å› è¾å°„æ‹…å¿§æˆ–æ‰«æé™åˆ¶è€Œæ— æ³•è·å–å®Œæ•´çš„å¤šæœŸç›¸åºåˆ—ï¼Œå¯¼è‡´æ¨¡æ€ç¼ºå¤±é—®é¢˜ã€‚ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•é€šå¸¸å°†ç¼ºå¤±æœŸç›¸è§†ä¸ºç‹¬ç«‹çš„ç¼ºå¤±é€šé“ï¼Œå¿½ç•¥äº†è¡€æµåŠ¨åŠ›å­¦çš„å›ºæœ‰æ—¶é—´è¿ç»­æ€§ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨æ•°æ®ç¨€ç–åœºæ™¯ä¸‹çš„è¯Šæ–­æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†æ—¶é—´è¡°å‡è¡¨ç¤ºè§£è€¦ï¼ˆTARDisï¼‰æ¡†æ¶ï¼Œå°†ç¼ºå¤±æ¨¡æ€é‡æ–°å®šä¹‰ä¸ºè¿ç»­æ—¶é—´è¡°å‡æ›²çº¿ä¸Šçš„ç¼ºå¤±é‡‡æ ·ç‚¹ã€‚è¯¥æ–¹æ³•é€šè¿‡åŒè·¯å¾„æ¶æ„æ˜¾å¼è§£è€¦æ½œåœ¨ç‰¹å¾ç©ºé—´ï¼šä¸€ä¸ªåŸºäºé‡åŒ–ç¼–ç çš„è·¯å¾„ä½¿ç”¨å¯å­¦ä¹ åµŒå…¥å­—å…¸æå–ä¸€è‡´çš„è§£å‰–ç»“æ„ï¼ˆæ—¶é—´ä¸å˜é™æ€åˆ†é‡ï¼‰ï¼Œå¦ä¸€ä¸ªæ¦‚ç‡è·¯å¾„ä½¿ç”¨æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨å»ºæ¨¡ä¾èµ–äºä¼°è®¡æ‰«ææ—¶é—´çš„åŠ¨æ€å¢å¼ºç‰¹å¾ï¼ˆæ—¶é—´ä¾èµ–åŠ¨æ€åˆ†é‡ï¼‰ã€‚è¿™ç§è®¾è®¡ä½¿ç½‘ç»œèƒ½å¤Ÿé€šè¿‡å­¦ä¹ åˆ°çš„æ½œåœ¨åˆ†å¸ƒé‡‡æ ·æ¥ç”Ÿæˆç¼ºå¤±çš„è¡€æµåŠ¨åŠ›å­¦ç‰¹å¾ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤§è§„æ¨¡ç§æœ‰è…¹éƒ¨CTæ•°æ®é›†ï¼ˆ2,282ä¾‹ï¼‰å’Œä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTARDisæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„ä¸å®Œæ•´æ¨¡æ€æ¡†æ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿åœ¨æç«¯æ•°æ®ç¨€ç–åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•ä»èƒ½ä¿æŒç¨³å¥çš„è¯Šæ–­æ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨å‡å°‘è¾å°„æš´éœ²åŒæ—¶ä¿æŒè¯Šæ–­ç²¾åº¦çš„æ½œåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> TARDisæ¡†æ¶é€šè¿‡å°†ç‰©ç†å…ˆéªŒçŸ¥è¯†èå…¥æ·±åº¦å­¦ä¹ æ¶æ„ï¼ŒæˆåŠŸè§£å†³äº†å¤šæœŸç›¸CTæˆåƒä¸­çš„æ¨¡æ€ç¼ºå¤±é—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸ä»…æå‡äº†æ¨¡å‹åœ¨æ•°æ®ä¸å®Œæ•´æƒ…å†µä¸‹çš„æ€§èƒ½ï¼Œè¿˜ä¸ºå‡å°‘ä¸´åºŠæ‰«ææ¬¡æ•°å’Œè¾å°„å‰‚é‡æä¾›äº†å¯è¡Œé€”å¾„ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚æœªæ¥çš„ç ”ç©¶æ–¹å‘å¯åŒ…æ‹¬å°†è¯¥æ¡†æ¶æ‰©å±•åˆ°å…¶ä»–æ—¶é—´åºåˆ—åŒ»å­¦æˆåƒæ¨¡æ€ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>Tumor segmentation and diagnosis in contrast-enhanced Computed Tomography (CT) rely heavily on the physiological dynamics of contrast agents. However, obtaining a complete multi-phase series is often clinically unfeasible due to radiation concerns or scanning limitations, leading to the "missing modality" problem. Existing deep learning approaches typically treat missing phases as absent independent channels, ignoring the inherent temporal continuity of hemodynamics. In this work, we propose Time Attenuated Representation Disentanglement (TARDis), a novel physics-aware framework that redefines missing modalities as missing sample points on a continuous Time-Attenuation Curve. TARDis explicitly disentangles the latent feature space into a time-invariant static component (anatomy) and a time-dependent dynamic component (perfusion). We achieve this via a dual-path architecture: a quantization-based path using a learnable embedding dictionary to extract consistent anatomical structures, and a probabilistic path using a Conditional Variational Autoencoder to model dynamic enhancement conditioned on the estimated scan time. This design allows the network to hallucinate missing hemodynamic features by sampling from the learned latent distribution. Extensive experiments on a large-scale private abdominal CT dataset (2,282 cases) and two public datasets demonstrate that TARDis significantly outperforms state-of-the-art incomplete modality frameworks. Notably, our method maintains robust diagnostic performance even in extreme data-sparsity scenarios, highlighting its potential for reducing radiation exposure while maintaining diagnostic precision.</p>
<h3 id="29-sam3-i-segment-anything-with-instructions">[29] <a href="https://arxiv.org/abs/2512.04585">SAM3-I: Segment Anything with Instructions</a></h3>
<p><em>Jingjing Li, Yue Feng, Yuchen Guo, Jincai Huang, Yongri Piao, Qi Bi, Miao Zhang, Xiaoqi Zhao, Qiang Chen, Shihao Zou, Wei Ji, Huchuan Lu, Li Cheng</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SAM3-Iï¼Œä¸€ä¸ªå¢å¼ºçš„æ¡†æ¶ï¼Œå°†æ¦‚å¿µçº§ç†è§£ä¸æŒ‡ä»¤çº§æ¨ç†ç»Ÿä¸€äºSAMç³»åˆ—ä¸­ï¼Œé€šè¿‡æŒ‡ä»¤æ„ŸçŸ¥çº§è”é€‚é…æœºåˆ¶ä½¿SAM3èƒ½å¤Ÿç›´æ¥éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œåˆ†å‰²ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸæœ‰çš„æ¦‚å¿µé©±åŠ¨èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è™½ç„¶SAM3é€šè¿‡å¯æç¤ºæ¦‚å¿µåˆ†å‰²æ¨è¿›äº†å¼€æ”¾è¯æ±‡åˆ†å‰²ï¼Œä½†ç°å®åº”ç”¨éœ€è¦æ›´ä¸°å¯Œçš„è¡¨è¾¾ï¼ŒåŒ…æ‹¬å±æ€§ã€ç©ºé—´å…³ç³»ã€åŠŸèƒ½ã€åŠ¨ä½œã€çŠ¶æ€ç”šè‡³å¯¹å®ä¾‹çš„éšå¼æ¨ç†ã€‚å½“å‰SAM3ä¾èµ–å¤–éƒ¨å¤šæ¨¡æ€ä»£ç†å°†å¤æ‚æŒ‡ä»¤è½¬æ¢ä¸ºåè¯çŸ­è¯­å¹¶è¿›è¡Œè¿­ä»£æ©ç è¿‡æ»¤ï¼Œä½†è¿™äº›åè¯çŸ­è¯­çº§æ¦‚å¿µè¿‡äºç²—ç³™ï¼Œå¾€å¾€æ— æ³•ç²¾ç¡®è¡¨ç¤ºç‰¹å®šå®ä¾‹ã€‚</p>
<p><strong>Method:</strong> SAM3-Iå¼•å…¥äº†æŒ‡ä»¤æ„ŸçŸ¥çº§è”é€‚é…æœºåˆ¶ï¼Œé€æ­¥å°†è¡¨è¾¾æ€§æŒ‡ä»¤è¯­ä¹‰ä¸SAM3ç°æœ‰çš„è§†è§‰è¯­è¨€è¡¨ç¤ºå¯¹é½ï¼Œå®ç°ç›´æ¥æŒ‡ä»¤è·Ÿéšåˆ†å‰²è€Œä¸ç‰ºç‰²å…¶åŸå§‹æ¦‚å¿µé©±åŠ¨èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†è·¨è¶Šæ¦‚å¿µã€ç®€å•å’Œå¤æ‚çº§åˆ«çš„ç»“æ„åŒ–æŒ‡ä»¤åˆ†ç±»æ³•ï¼Œå¹¶å¼€å‘äº†å¯æ‰©å±•çš„æ•°æ®å¼•æ“æ¥æ„å»ºåŒ…å«å¤šæ ·åŒ–æŒ‡ä»¤-æ©ç å¯¹çš„æ•°æ®é›†ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜SAM3-Iå±•ç°å‡ºä»¤äººæ»¡æ„çš„æ€§èƒ½ï¼Œè¯æ˜SAM3å¯ä»¥æœ‰æ•ˆæ‰©å±•åˆ°éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼ŒåŒæ—¶ä¿æŒå…¶å¼ºå¤§çš„æ¦‚å¿µåŸºç¡€ã€‚è¯¥æ¡†æ¶å¼€æºå¹¶æä¾›å®ç”¨çš„å¾®è°ƒå·¥ä½œæµç¨‹ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿå°†å…¶é€‚é…åˆ°ç‰¹å®šé¢†åŸŸåº”ç”¨ä¸­ã€‚</p>
<p><strong>Conclusion:</strong> SAM3-IæˆåŠŸåœ°å°†æ¦‚å¿µçº§ç†è§£ä¸æŒ‡ä»¤çº§æ¨ç†ç»Ÿä¸€äºSAMæ¡†æ¶ä¸­ï¼Œé€šè¿‡åˆ›æ–°çš„é€‚é…æœºåˆ¶å®ç°äº†ç›´æ¥æŒ‡ä»¤è·Ÿéšåˆ†å‰²èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å°†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°æ›´å¤æ‚ã€è¡¨è¾¾æ€§æ›´å¼ºçš„æŒ‡ä»¤ç†è§£ä»»åŠ¡çš„å¯è¡Œæ€§ï¼Œä¸ºå¼€æ”¾è¯æ±‡åˆ†å‰²çš„å®é™…åº”ç”¨æä¾›äº†æ›´çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.</p>
<h3 id="30-malicious-image-analysis-via-vision-language-segmentation-fusion-detection-element-and-location-in-one-shot">[30] <a href="https://arxiv.org/abs/2512.04599">Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot</a></h3>
<p><em>Sheng Hang, Chaoxiang He, Hongsheng Hu, Hanqing Hu, Bin Benjamin Zhu, Shi-Feng Sun, Dawu Gu, Shuo Wang</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬æ¶æ„å›¾åƒå†…å®¹æ£€æµ‹ä¸å®šä½ç®¡é“ï¼Œèƒ½å¤ŸåŒæ—¶æ£€æµ‹æœ‰å®³å†…å®¹ã€è¯†åˆ«å…³é”®å…ƒç´ å¹¶è¿›è¡Œåƒç´ çº§å®šä½ï¼Œæ˜¾è‘—æå‡äº†ç»†ç²’åº¦å†…å®¹å®¡æ ¸çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å›¾åƒçº§NSFWæ ‡è®°æ— æ³•æ»¡è¶³å†…å®¹å®¡æ ¸éœ€æ±‚ï¼Œå®¡æ ¸äººå‘˜éœ€è¦çŸ¥é“å›¾åƒä¸­å“ªäº›å…·ä½“å¯¹è±¡æ„æˆéæ³•å†…å®¹åŠå…¶ç²¾ç¡®ä½ç½®ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤ŸåŒæ—¶æ£€æµ‹æœ‰å®³å†…å®¹ã€è¯†åˆ«å…³é”®å…ƒç´ å¹¶è¿›è¡Œåƒç´ çº§å®šä½çš„ç»†ç²’åº¦å®¡æ ¸å·¥å…·ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨é›¶æ ·æœ¬ç®¡é“ï¼Œé¦–å…ˆä½¿ç”¨åŸºç¡€åˆ†å‰²æ¨¡å‹ç”Ÿæˆå€™é€‰å¯¹è±¡æ©ç å¹¶ç»†åŒ–ä¸ºç‹¬ç«‹åŒºåŸŸï¼Œç„¶åé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ä½¿ç”¨å¼€æ”¾è¯æ±‡æç¤ºå¯¹æ¯ä¸ªåŒºåŸŸè¿›è¡Œæ¶æ„ç›¸å…³æ€§è¯„åˆ†ï¼Œæœ€åé€šè¿‡åŠ æƒèåˆæ­¥éª¤ç”Ÿæˆç»Ÿä¸€çš„æ¶æ„å¯¹è±¡åœ°å›¾ï¼Œå¹¶é‡‡ç”¨å¤šåˆ†å‰²å™¨é›†æˆç­–ç•¥å¢å¼ºå¯¹æŠ—æ”»å‡»é²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨åŒ…å«790å¼ å›¾åƒçš„æ–°æ ‡æ³¨æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨æ¯’å“ã€è‰²æƒ…ã€æš´åŠ›å’Œæç«¯ä¸»ä¹‰å†…å®¹ä¸Šå–å¾—äº†85.8%çš„å…ƒç´ çº§å¬å›ç‡ã€78.1%çš„ç²¾ç¡®ç‡å’Œ92.1%çš„åˆ†å‰²æˆåŠŸç‡ï¼Œæ¯”ç›´æ¥é›¶æ ·æœ¬VLMå®šä½æ–¹æ³•å¬å›ç‡æé«˜äº†27.4%ï¼Œåœ¨å¯¹æŠ—PGDæ‰°åŠ¨æ”»å‡»ä¸‹ç²¾åº¦å’Œå¬å›ç‡ä¸‹é™ä¸è¶…è¿‡10%ï¼Œè¡¨ç°å‡ºé«˜é²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æå‡ºäº†é¦–ä¸ªå®ç”¨çš„ç»†ç²’åº¦ã€å¯è§£é‡Šæ¶æ„å›¾åƒå®¡æ ¸å·¥å…·ï¼Œèƒ½å¤Ÿåœ¨ç§’çº§å¤„ç†å›¾åƒå¹¶æ— ç¼é›†æˆåˆ°ç°æœ‰VLMå·¥ä½œæµä¸­ï¼Œä¸ºå†…å®¹å®¡æ ¸æä¾›äº†åŒæ—¶å…·å¤‡é«˜å‡†ç¡®æ€§ã€å¼ºé²æ£’æ€§å’Œåƒç´ çº§å®šä½èƒ½åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Detecting illicit visual content demands more than image-level NSFW flags; moderators must also know what objects make an image illegal and where those objects occur. We introduce a zero-shot pipeline that simultaneously (i) detects if an image contains harmful content, (ii) identifies each critical element involved, and (iii) localizes those elements with pixel-accurate masks - all in one pass. The system first applies foundation segmentation model (SAM) to generate candidate object masks and refines them into larger independent regions. Each region is scored for malicious relevance by a vision-language model using open-vocabulary prompts; these scores weight a fusion step that produces a consolidated malicious object map. An ensemble across multiple segmenters hardens the pipeline against adaptive attacks that target any single segmentation method. Evaluated on a newly-annotated 790-image dataset spanning drug, sexual, violent and extremist content, our method attains 85.8% element-level recall, 78.1% precision and a 92.1% segment-success rate - exceeding direct zero-shot VLM localization by 27.4% recall at comparable precision. Against PGD adversarial perturbations crafted to break SAM and VLM, our method's precision and recall decreased by no more than 10%, demonstrating high robustness against attacks. The full pipeline processes an image in seconds, plugs seamlessly into existing VLM workflows, and constitutes the first practical tool for fine-grained, explainable malicious-image moderation.</p>
<h3 id="31-denoise-to-track-harnessing-video-diffusion-priors-for-robust-correspondence">[31] <a href="https://arxiv.org/abs/2512.04619">Denoise to Track: Harnessing Video Diffusion Priors for Robust Correspondence</a></h3>
<p><em>Tianyu Yuan, Yuanbo Yang, Lin-Zhuo Chen, Yao Yao, Zhuzhong Qian</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†HeFTï¼ˆHead-Frequency Trackerï¼‰ï¼Œä¸€ç§åŸºäºé¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹è§†è§‰å…ˆéªŒçš„é›¶æ ·æœ¬ç‚¹è·Ÿè¸ªæ¡†æ¶ï¼Œé€šè¿‡åˆ†æVDiTå†…éƒ¨è¡¨ç¤ºå‘ç°æ³¨æ„åŠ›å¤´çš„åŠŸèƒ½ç‰¹åŒ–ï¼Œå¹¶æå‡ºå¤´-é¢‘ç‡æ„ŸçŸ¥ç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼Œåœ¨TAP-VidåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬è·Ÿè¸ªæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹å¦‚ä½•ç¼–ç æ—¶ç©ºä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨å…¶è§†è§‰å…ˆéªŒè§£å†³é›¶æ ·æœ¬ç‚¹è·Ÿè¸ªé—®é¢˜ï¼Œä»¥å¼¥åˆæ— ç›‘ç£æ–¹æ³•ä¸ç›‘ç£æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼ŒåŒæ—¶é¿å…å¯¹æ ‡æ³¨è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é¦–å…ˆåˆ†æè§†é¢‘æ‰©æ•£å˜æ¢å™¨ï¼ˆVDiTï¼‰çš„å†…éƒ¨è¡¨ç¤ºï¼Œå‘ç°æ³¨æ„åŠ›å¤´ä½œä¸ºæœ€å°åŠŸèƒ½å•å…ƒå…·æœ‰åŒ¹é…ã€è¯­ä¹‰ç†è§£å’Œä½ç½®ç¼–ç ç­‰ä¸åŒç‰¹åŒ–åŠŸèƒ½ï¼Œå¹¶æå‡ºå¤´-é¢‘ç‡æ„ŸçŸ¥ç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼Œè”åˆé€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ³¨æ„åŠ›å¤´å’Œä½é¢‘åˆ†é‡ï¼Œé€šè¿‡å•æ­¥å»å™ªæå–åˆ¤åˆ«æ€§ç‰¹å¾ï¼Œé‡‡ç”¨è½¯argmaxå®šä½å’Œå‰å‘-åå‘ä¸€è‡´æ€§æ£€æŸ¥è¿›è¡Œå¯¹åº”å…³ç³»ä¼°è®¡ã€‚</p>
<p><strong>Result:</strong> åœ¨TAP-VidåŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHeFTå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬è·Ÿè¸ªæ€§èƒ½ï¼Œå…¶å‡†ç¡®ç‡æ¥è¿‘ç›‘ç£æ–¹æ³•æ°´å¹³ï¼ŒåŒæ—¶å®Œå…¨ä¸éœ€è¦æ ‡æ³¨è®­ç»ƒæ•°æ®ï¼ŒéªŒè¯äº†ä½é¢‘åˆ†é‡åœ¨å»ºç«‹å¯¹åº”å…³ç³»ä¸­çš„å…³é”®ä½œç”¨ä»¥åŠé«˜é¢‘åˆ†é‡å¼•å…¥å™ªå£°çš„è§‚å¯Ÿã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶æ­ç¤ºäº†è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­æ³¨æ„åŠ›å¤´çš„åŠŸèƒ½ç‰¹åŒ–å’Œé¢‘ç‡åˆ†é‡çš„ä¸åŒä½œç”¨ï¼Œè¯æ˜äº†è§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§åŸºç¡€æ¨¡å‹åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œä¸ºæ„å»ºç»Ÿä¸€çš„è§†è§‰åŸºç¡€æ¨¡å‹å¼€è¾Ÿäº†é“è·¯ï¼ŒåŒæ—¶å±•ç¤ºäº†é›¶æ ·æœ¬æ–¹æ³•åœ¨ç‚¹è·Ÿè¸ªä»»åŠ¡ä¸­è¾¾åˆ°ç›‘ç£æ–¹æ³•æ€§èƒ½çš„å¯èƒ½æ€§ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>In this work, we introduce HeFT (Head-Frequency Tracker), a zero-shot point tracking framework that leverages the visual priors of pretrained video diffusion models. To better understand how they encode spatiotemporal information, we analyze the internal representations of Video Diffusion Transformer (VDiT). Our analysis reveals that attention heads act as minimal functional units with distinct specializations for matching, semantic understanding, and positional encoding. Additionally, we find that the low-frequency components in VDiT features are crucial for establishing correspondences, whereas the high-frequency components tend to introduce noise. Building on these insights, we propose a head- and frequency-aware feature selection strategy that jointly selects the most informative attention head and low-frequency components to enhance tracking performance. Specifically, our method extracts discriminative features through single-step denoising, applies feature selection, and employs soft-argmax localization with forward-backward consistency checks for correspondence estimation. Extensive experiments on TAP-Vid benchmarks demonstrate that HeFT achieves state-of-the-art zero-shot tracking performance, approaching the accuracy of supervised methods while eliminating the need for annotated training data. Our work further underscores the promise of video diffusion models as powerful foundation models for a wide range of downstream tasks, paving the way toward unified visual foundation models.</p>
<h3 id="32-i2i-bench-a-comprehensive-benchmark-suite-for-image-to-image-editing-models">[32] <a href="https://arxiv.org/abs/2512.04660">I2I-Bench: A Comprehensive Benchmark Suite for Image-to-Image Editing Models</a></h3>
<p><em>Juntong Wang, Jiarui Wang, Huiyu Duan, Jiaxiang Kang, Guangtao Zhai, Xiongkuo Min</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†I2I-Benchï¼Œä¸€ä¸ªå…¨é¢çš„å›¾åƒåˆ°å›¾åƒç¼–è¾‘æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ¶µç›–10ä¸ªä»»åŠ¡ç±»åˆ«å’Œ30ä¸ªç»†ç²’åº¦è¯„ä¼°ç»´åº¦ï¼Œé‡‡ç”¨è‡ªåŠ¨åŒ–æ··åˆè¯„ä¼°æ–¹æ³•ï¼Œå¹¶é€šè¿‡ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§éªŒè¯ç¡®ä¿äº†è¯„ä¼°çš„å¯é æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•å­˜åœ¨ä»»åŠ¡èŒƒå›´æœ‰é™ã€è¯„ä¼°ç»´åº¦ä¸è¶³ã€è¿‡åº¦ä¾èµ–äººå·¥æ ‡æ³¨ç­‰é—®é¢˜ï¼Œä¸¥é‡åˆ¶çº¦äº†å…¶å¯æ‰©å±•æ€§å’Œå®é™…åº”ç”¨ä»·å€¼ï¼Œéœ€è¦å»ºç«‹ä¸€ä¸ªæ›´å…¨é¢ã€è‡ªåŠ¨åŒ–çš„è¯„ä¼°æ¡†æ¶æ¥æ¨åŠ¨å›¾åƒç¼–è¾‘æ¨¡å‹çš„å‘å±•ã€‚</p>
<p><strong>Method:</strong> è¯¥ç ”ç©¶æå‡ºäº†I2I-BenchåŸºå‡†æµ‹è¯•æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒè®¾è®¡ï¼šæ¶µç›–10ä¸ªä»»åŠ¡ç±»åˆ«çš„å¤šæ ·åŒ–ä»»åŠ¡é›†ï¼ŒåŒ…å«30ä¸ªè§£è€¦ç»†ç²’åº¦è¯„ä¼°ç»´åº¦çš„ç»¼åˆè¯„ä¼°ä½“ç³»ï¼Œä»¥åŠç»“åˆä¸“ä¸šå·¥å…·å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„è‡ªåŠ¨åŒ–æ··åˆè¯„ä¼°æ–¹æ³•ï¼Œå¹¶é€šè¿‡ä¸¥æ ¼çš„å¯¹é½éªŒè¯ç¡®ä¿è¯„ä¼°ç»“æœä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶ä½¿ç”¨I2I-Benchå¯¹å¤šä¸ªä¸»æµå›¾åƒç¼–è¾‘æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å„ç§è¯„ä¼°ç»´åº¦ä¸Šçš„æ€§èƒ½å·®è·å’Œæƒè¡¡å…³ç³»ï¼ŒéªŒè¯äº†è‡ªåŠ¨åŒ–è¯„ä¼°æ–¹æ³•ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ï¼Œä¸ºæ¨¡å‹æ¯”è¾ƒæä¾›äº†å…¨é¢å¯é çš„è¯„ä¼°æ•°æ®ã€‚</p>
<p><strong>Conclusion:</strong> I2I-Benchä¸ºå›¾åƒç¼–è¾‘é¢†åŸŸæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€è‡ªåŠ¨åŒ–çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œå…¶å¼€æºç‰¹æ€§å°†ä¿ƒè¿›æœªæ¥ç ”ç©¶çš„å‘å±•ï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å¤šä¸ªç»´åº¦ä¸Šçš„æ€§èƒ½æƒè¡¡ï¼Œä¸ºæ¨¡å‹é€‰æ‹©å’Œä¼˜åŒ–æä¾›äº†é‡è¦å‚è€ƒä¾æ®ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Image editing models are advancing rapidly, yet comprehensive evaluation remains a significant challenge. Existing image editing benchmarks generally suffer from limited task scopes, insufficient evaluation dimensions, and heavy reliance on manual annotations, which significantly constrain their scalability and practical applicability. To address this, we propose \textbf{I2I-Bench}, a comprehensive benchmark for image-to-image editing models, which features (i) diverse tasks, encompassing 10 task categories across both single-image and multi-image editing tasks, (ii) comprehensive evaluation dimensions, including 30 decoupled and fine-grained evaluation dimensions with automated hybrid evaluation methods that integrate specialized tools and large multimodal models (LMMs), and (iii) rigorous alignment validation, justifying the consistency between our benchmark evaluations and human preferences. Using I2I-Bench, we benchmark numerous mainstream image editing models, investigating the gaps and trade-offs between editing models across various dimensions. We will open-source all components of I2I-Bench to facilitate future research.</p>
<h3 id="33-reward-forcing-efficient-streaming-video-generation-with-rewarded-distribution-matching-distillation">[33] <a href="https://arxiv.org/abs/2512.04678">Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation</a></h3>
<p><em>Yunhong Lu, Yanhong Zeng, Haobo Li, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jiapeng Zhu, Hengyuan Cao, Zhipeng Zhang, Xing Zhu, Yujun Shen, Min Zhang</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºReward Forcingæ¡†æ¶ï¼Œé€šè¿‡EMA-Sinkæœºåˆ¶å’ŒRewarded Distribution Matching Distillationæ–¹æ³•ï¼Œè§£å†³äº†æµå¼è§†é¢‘ç”Ÿæˆä¸­åˆå§‹å¸§è¿‡åº¦ä¾èµ–å’Œè¿åŠ¨åŠ¨æ€æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå®ç°äº†é«˜è´¨é‡å®æ—¶è§†é¢‘ç”Ÿæˆã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æµå¼è§†é¢‘ç”Ÿæˆæ–¹æ³•é‡‡ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†åˆå§‹å¸§ä½œä¸ºsink tokenä»¥å‡å°‘è¯¯å·®ç´¯ç§¯ï¼Œä½†è¿™å¯¼è‡´è§†é¢‘å¸§è¿‡åº¦ä¾èµ–é™æ€åˆå§‹å¸§ï¼Œé€ æˆåˆå§‹å¸§å¤åˆ¶å’Œè¿åŠ¨åŠ¨æ€æ€§å‡å¼±çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºReward Forcingæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒè®¾è®¡ï¼šEMA-Sinkæœºåˆ¶é€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡èåˆè¢«ç§»å‡ºæ»‘åŠ¨çª—å£çš„tokenï¼Œåœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹æ•è·é•¿æœŸä¸Šä¸‹æ–‡å’Œè¿‘æœŸåŠ¨æ€ï¼›Rewarded Distribution Matching Distillationåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°æ ·æœ¬åŠ¨æ€æ€§ï¼Œé€šè¿‡å¥–åŠ±æœºåˆ¶åç½®è¾“å‡ºåˆ†å¸ƒï¼Œä¼˜å…ˆå­¦ä¹ é«˜åŠ¨æ€å†…å®¹ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜Reward Forcingåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨å•å—H100 GPUä¸Šä»¥23.1 FPSçš„é€Ÿåº¦å®ç°é«˜è´¨é‡æµå¼è§†é¢‘ç”Ÿæˆï¼Œæ˜¾è‘—æå‡äº†è¿åŠ¨è´¨é‡åŒæ—¶ä¿æŒäº†æ•°æ®ä¿çœŸåº¦ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡åŠ¨æ€tokenæ›´æ–°æœºåˆ¶å’ŒåŸºäºå¥–åŠ±çš„åˆ†å¸ƒåŒ¹é…è’¸é¦ï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³æµå¼è§†é¢‘ç”Ÿæˆä¸­çš„åˆå§‹å¸§ä¾èµ–é—®é¢˜ï¼Œä¸ºå®æ—¶äº¤äº’å¼åŠ¨æ€ä¸–ç•Œæ¨¡æ‹Ÿæä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨ä¿æŒé•¿æœŸä¸€è‡´æ€§çš„åŒæ—¶å¢å¼ºè¿åŠ¨åŠ¨æ€æ€§çš„å¯è¡Œæ€§ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.</p>
<h3 id="34-towards-cross-view-point-correspondence-in-vision-language-models">[34] <a href="https://arxiv.org/abs/2512.04686">Towards Cross-View Point Correspondence in Vision-Language Models</a></h3>
<p><em>Yipu Wang, Yuheng Ji, Yuyang Liu, Enshen Zhou, Ziqiang Yang, Yuxuan Tian, Ziheng Qin, Yue Liu, Huajie Tan, Cheng Chi, Zhiyuan Ma, Daniel Dajun Zeng, Xiaolong Zheng</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†è·¨è§†å›¾ç‚¹å¯¹åº”ä»»åŠ¡ï¼ˆCVPCï¼‰å’ŒCrossPoint-BenchåŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç²¾ç»†ç‚¹çº§å¯¹åº”èƒ½åŠ›ä¸Šçš„ä¸¥é‡ä¸è¶³ï¼Œå¹¶æ„å»ºäº†CrossPoint-378Kæ•°æ®é›†å’ŒCroPondæ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†è·¨è§†å›¾å¯¹åº”æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å®ç°è·¨è§†å›¾å¯¹åº”æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç¡®ç‚¹çº§å¯¹åº”èƒ½åŠ›ä¸Šï¼Œè¿™å¯¹äºå®ç°ç²¾ç¡®çš„å¯ä¾›æ€§äº¤äº’è‡³å…³é‡è¦ã€‚ç°æœ‰æ¨¡å‹åœ¨ä»ç²—ç²’åº¦åˆ¤æ–­è¿‡æ¸¡åˆ°ç»†ç²’åº¦åæ ‡é¢„æµ‹æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸äººç±»è¡¨ç°å­˜åœ¨å·¨å¤§å·®è·ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†è·¨è§†å›¾ç‚¹å¯¹åº”ä»»åŠ¡å’ŒCrossPoint-BenchåŸºå‡†ï¼Œè¯¥åŸºå‡†é‡‡ç”¨åˆ†å±‚è®¾è®¡ï¼Œæ¨¡æ‹Ÿäººç±»"æ„ŸçŸ¥-æ¨ç†-å¯¹åº”"çš„è®¤çŸ¥è¿‡ç¨‹ã€‚ä¸ºè§£å†³è¯¥é—®é¢˜ï¼Œæ„å»ºäº†åŒ…å«378Ké—®ç­”å¯¹çš„CrossPoint-378Kæ•°æ®é›†ï¼Œä¸“æ³¨äºåæ˜ çœŸå®ä¸–ç•Œæ“ä½œåœºæ™¯çš„å¯ä¾›æ€§åŒºåŸŸï¼Œå¹¶æå‡ºäº†åœ¨è¯¥æ•°æ®é›†ä¸Šè®­ç»ƒçš„CroPondæ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°æ˜¾ç¤ºæœ€å…ˆè¿›æ¨¡å‹å¦‚Gemini-2.5-Proåœ¨æ•´ä½“å‡†ç¡®ç‡ä¸Šè½åäººç±»è¶…è¿‡54.65%ã€‚CroPondæ¨¡å‹åœ¨CrossPoint-Benchä¸Šå®ç°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¶…è¶ŠGemini-2.5-Pro 39.7%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ç¼©å°äº†ä¸äººç±»è¡¨ç°çš„å·®è·ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç²¾ç»†ç©ºé—´å¯¹åº”èƒ½åŠ›ä¸Šçš„æ ¹æœ¬æ€§å±€é™ï¼Œä¸ºç©ºé—´ç†è§£å’Œå…·èº«AIæä¾›äº†é‡è¦åŸºå‡†ã€‚æå‡ºçš„æ•°æ®é›†å’Œæ¨¡å‹ä¸ºæœªæ¥è·¨è§†å›¾å¯¹åº”ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œå¼ºè°ƒäº†ä»ç²—ç²’åº¦åˆ°ç»†ç²’åº¦ç©ºé—´æ¨ç†è½¬å˜çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of "perceive", "reason", and "correspond". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.</p>
<h3 id="35-emma-efficient-multimodal-understanding-generation-and-editing-with-a-unified-architecture">[35] <a href="https://arxiv.org/abs/2512.04810">EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture</a></h3>
<p><em>Xin He, Longhui Wei, Jianbo Ouyang, Lingxi Xie, Qi Tian</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†EMMAï¼Œä¸€ç§é«˜æ•ˆç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆä¸ç¼–è¾‘æ¶æ„ï¼Œé€šè¿‡åˆ›æ–°çš„å‹ç¼©ç­–ç•¥å’Œç½‘ç»œè®¾è®¡æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ä»»åŠ¡çš„æ€§èƒ½ä¸æ•ˆç‡ï¼Œåœ¨4Bå‚æ•°è§„æ¨¡ä¸‹è¶…è¶Šäº†ç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç»Ÿä¸€å¤šæ¨¡æ€æ¶æ„åœ¨å¤„ç†ç†è§£ä¸ç”Ÿæˆä»»åŠ¡æ—¶é¢ä¸´æ•ˆç‡ä½ä¸‹å’Œæ€§èƒ½ä¸è¶³çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰tokenå‹ç¼©å’Œä»»åŠ¡ç‰¹å®šå»ºæ¨¡æ–¹é¢å­˜åœ¨å±€é™ï¼Œéœ€è¦ä¸€ç§æ—¢èƒ½é«˜æ•ˆå¤„ç†å¤šæ¨¡æ€è¾“å…¥åˆèƒ½å¹³è¡¡ä¸åŒä»»åŠ¡éœ€æ±‚çš„ç»Ÿä¸€è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> EMMAæ¶æ„åŒ…å«å››ä¸ªå…³é”®æŠ€æœ¯ï¼šé‡‡ç”¨32å€å‹ç¼©æ¯”çš„é«˜æ•ˆè‡ªç¼–ç å™¨å‡å°‘ç”Ÿæˆæ‰€éœ€tokenæ•°é‡ï¼›ä½¿ç”¨é€šé“çº§è€Œétokençº§æ‹¼æ¥è¿›ä¸€æ­¥å‡å°‘è§†è§‰tokenï¼›è®¾è®¡å…±äº«è§£è€¦ç½‘ç»œå®ç°ä»»åŠ¡é—´ç›¸äº’ä¿ƒè¿›ä¸ä»»åŠ¡ç‰¹å®šå»ºæ¨¡ï¼›åœ¨è§†è§‰ç†è§£ç¼–ç å™¨ä¸­å¼•å…¥æ··åˆä¸“å®¶æœºåˆ¶ä»¥å°‘é‡å‚æ•°æå‡æ„ŸçŸ¥èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œ4Bå‚æ•°çš„EMMAåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰ç»Ÿä¸€å¤šæ¨¡æ€æ–¹æ³•å¦‚BAGEL-7Bï¼ŒåŒæ—¶ä¸æœ€æ–°çš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¸“å®¶æ¨¡å‹å¦‚Qwen3-VLå’ŒQwen-Imageç›¸æ¯”ä¹Ÿå–å¾—äº†ç«äº‰æ€§ç»“æœï¼ŒéªŒè¯äº†å…¶æ¶æ„è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> EMMAä¸ºç»Ÿä¸€å¤šæ¨¡æ€æ¶æ„çš„æœªæ¥å‘å±•å¥ å®šäº†åšå®åŸºç¡€ï¼Œå…¶é«˜æ•ˆå‹ç¼©ç­–ç•¥å’Œçµæ´»çš„ç½‘ç»œè®¾è®¡ä¸ºè§£å†³å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æ•ˆç‡ä¸æ€§èƒ½å¹³è¡¡é—®é¢˜æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œå±•ç¤ºäº†ç»Ÿä¸€æ¶æ„åœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶å®ç°å‚æ•°æ•ˆç‡ä¼˜åŒ–çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.</p>
<h3 id="36-autoregressive-image-generation-needs-only-a-few-lines-of-cached-tokens">[36] <a href="https://arxiv.org/abs/2512.04857">Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens</a></h3>
<p><em>Ziran Qin, Youru Lv, Mingbao Lin, Zeren Zhang, Chanfan Gan, Tieyuan Chen, Weiyao Lin</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†LineARï¼Œä¸€ç§æ— éœ€è®­ç»ƒã€åŸºäºæ¸è¿›å¼é”®å€¼ç¼“å­˜å‹ç¼©çš„è‡ªå›å½’è§†è§‰ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨è§†è§‰æ³¨æ„åŠ›çš„å†…åœ¨ç‰¹æ€§ï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘å†…å­˜å ç”¨å¹¶æé«˜ååé‡ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è‡ªå›å½’è§†è§‰ç”Ÿæˆæ–¹æ³•åœ¨è§£ç è¿‡ç¨‹ä¸­éœ€è¦ç¼“å­˜æ‰€æœ‰å…ˆå‰ç”Ÿæˆçš„è§†è§‰æ ‡è®°ï¼Œå¯¼è‡´ä¸¥é‡çš„å†…å­˜ç“¶é¢ˆï¼Œè¡¨ç°ä¸ºé«˜å­˜å‚¨éœ€æ±‚å’Œä½ååé‡ï¼Œè¿™é™åˆ¶äº†è‡ªå›å½’å›¾åƒç”Ÿæˆçš„å®é™…åº”ç”¨æ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> LineARæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¸è¿›å¼é”®å€¼ç¼“å­˜å‹ç¼©æµæ°´çº¿ï¼Œå®ƒåˆ©ç”¨è§†è§‰æ³¨æ„åŠ›çš„å†…åœ¨ç‰¹æ€§ï¼Œé€šè¿‡äºŒç»´è§†å›¾åœ¨çº¿çº§åˆ«ç®¡ç†ç¼“å­˜ï¼Œä¿ç•™è§†è§‰ä¾èµ–åŒºåŸŸï¼ŒåŒæ—¶æ ¹æ®è¡Œé—´æ³¨æ„åŠ›æŒ‡å¯¼é€æ­¥æ·˜æ±°å¯¹åç»­è¡Œç”Ÿæˆæ— å®³çš„ä¿¡æ¯è¾ƒå°‘æ ‡è®°ï¼Œä»…ä½¿ç”¨å°‘é‡ç¼“å­˜è¡Œå®ç°é«˜æ•ˆç”Ÿæˆã€‚</p>
<p><strong>Result:</strong> åœ¨å…­ä¸ªè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬LlamaGen-XLå’ŒJanus-Pro-1Bä¸ŠImageNet FIDä»2.77æå‡è‡³2.68ï¼ŒCOCO FIDä»23.85æå‡è‡³22.86ï¼ŒåŒæ—¶ä»…ä¿ç•™1/6é”®å€¼ç¼“å­˜ï¼›åœ¨Lumina-mGPT-768ä¸Šä»…ä½¿ç”¨1/8ç¼“å­˜å³å¯æå‡DPGï¼›åœ¨LlamaGen-XLä¸Šå®ç°67.61%å†…å­˜å‡å°‘å’Œ7.57å€åŠ é€Ÿï¼Œåœ¨Janus-Pro-7Bä¸Šå®ç°39.66%å†…å­˜å‡å°‘å’Œ5.62å€åŠ é€Ÿã€‚</p>
<p><strong>Conclusion:</strong> LineARé€šè¿‡æœ‰æ•ˆç®¡ç†è‡ªå›å½’è§†è§‰ç”Ÿæˆçš„ç¼“å­˜æœºåˆ¶ï¼Œåœ¨ä¿æŒæˆ–æå‡ç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—ä¼˜åŒ–å†…å­˜æ•ˆç‡å’Œè®¡ç®—ååé‡ï¼Œä¸ºå¤§è§„æ¨¡è‡ªå›å½’å›¾åƒç”Ÿæˆæä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†åŸºäºè§†è§‰æ³¨æ„åŠ›ç‰¹æ€§çš„ç¼“å­˜å‹ç¼©ç­–ç•¥çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.</p>
<h3 id="37-semantics-lead-the-way-harmonizing-semantic-and-texture-modeling-with-asynchronous-latent-diffusion">[37] <a href="https://arxiv.org/abs/2512.04926">Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion</a></h3>
<p><em>Yueming Pan, Ruoyu Feng, Qi Dai, Yuqi Wang, Wenfeng Lin, Mingyu Guo, Chong Luo, Nanning Zheng</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†è¯­ä¹‰ä¼˜å…ˆæ‰©æ•£ï¼ˆSFDï¼‰ï¼Œä¸€ç§æ˜¾å¼ä¼˜å…ˆè€ƒè™‘è¯­ä¹‰å½¢æˆçš„æ½œåœ¨æ‰©æ•£èŒƒå¼ï¼Œé€šè¿‡å¼‚æ­¥å»å™ªè¯­ä¹‰å’Œçº¹ç†æ½œåœ¨è¡¨ç¤ºï¼Œåˆ©ç”¨è¯­ä¹‰å…ˆå¯¼æŒ‡å¯¼çº¹ç†ç»†åŒ–ï¼Œå®ç°äº†è‡ªç„¶çš„ä»ç²—åˆ°ç»†çš„å›¾åƒç”Ÿæˆã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰è™½ç„¶éµå¾ªä»ç²—åˆ°ç»†çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œä½†é€šå¸¸åŒæ­¥å»å™ªè¯­ä¹‰å’ŒVAEç¼–ç çš„çº¹ç†ï¼Œå¿½ç•¥äº†è¯­ä¹‰å½¢æˆç•¥å¾®æ—©äºçº¹ç†ç”Ÿæˆçš„æ—¶åºç‰¹æ€§ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è¯­ä¹‰é”šç‚¹å¯¹çº¹ç†ç»†åŒ–çš„æŒ‡å¯¼ä½œç”¨ã€‚</p>
<p><strong>Method:</strong> SFDé¦–å…ˆé€šè¿‡ä¸“ç”¨çš„è¯­ä¹‰VAEä»é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ä¸­æå–ç´§å‡‘çš„è¯­ä¹‰æ½œåœ¨è¡¨ç¤ºï¼Œä¸çº¹ç†æ½œåœ¨è¡¨ç¤ºç»„åˆæ„å»ºå¤åˆæ½œåœ¨è¡¨ç¤ºï¼›æ ¸å¿ƒåˆ›æ–°æ˜¯ä½¿ç”¨åˆ†ç¦»çš„å™ªå£°è°ƒåº¦å¼‚æ­¥å»å™ªè¯­ä¹‰å’Œçº¹ç†æ½œåœ¨è¡¨ç¤ºï¼Œè¯­ä¹‰å»å™ªæ¯”çº¹ç†æå‰ä¸€ä¸ªæ—¶é—´åç§»ï¼Œä¸ºçº¹ç†ç»†åŒ–æä¾›æ›´æ¸…æ™°çš„é«˜å±‚è¯­ä¹‰æŒ‡å¯¼ã€‚</p>
<p><strong>Result:</strong> åœ¨ImageNet 256Ã—256æ•°æ®é›†ä¸Šï¼ŒSFDåœ¨å¼•å¯¼ç”Ÿæˆæ¡ä»¶ä¸‹è¾¾åˆ°äº†FID 1.06ï¼ˆLightningDiT-XLï¼‰å’ŒFID 1.04ï¼ˆ1.0B LightningDiT-XXLï¼‰ï¼Œç›¸æ¯”åŸå§‹DiTå®ç°äº†é«˜è¾¾100å€çš„æ”¶æ•›åŠ é€Ÿï¼›SFDè¿˜èƒ½æœ‰æ•ˆæ”¹è¿›ç°æœ‰æ–¹æ³•å¦‚ReDiå’ŒVA-VAEï¼Œè¯æ˜äº†å¼‚æ­¥è¯­ä¹‰å¼•å¯¼å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ˜¾å¼å»ºæ¨¡è¯­ä¹‰ä¼˜å…ˆçš„å¼‚æ­¥æ‰©æ•£èŒƒå¼èƒ½å¤Ÿæ˜¾è‘—æå‡å›¾åƒç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡ï¼Œä¸ºæ½œåœ¨æ‰©æ•£æ¨¡å‹æä¾›äº†æ›´è‡ªç„¶çš„ä»ç²—åˆ°ç»†ç”Ÿæˆæœºåˆ¶ï¼Œè¿™ä¸€æ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºæ”¹è¿›å¤šç§ç°æœ‰æ‰©æ•£æ¨¡å‹æ¶æ„ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.</p>
<h3 id="38-towards-adaptive-fusion-of-multimodal-deep-networks-for-human-action-recognition">[38] <a href="https://arxiv.org/abs/2512.04943">Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition</a></h3>
<p><em>Novanto Yudistira</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œå’Œå¤šæ¨¡æ€è‡ªé€‚åº”èåˆç­–ç•¥çš„åˆ›æ–°äººç±»åŠ¨ä½œè¯†åˆ«æ–¹æ³•ï¼Œé€šè¿‡é—¨æ§æœºåˆ¶èåˆRGBã€å…‰æµã€éŸ³é¢‘å’Œæ·±åº¦ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†åŠ¨ä½œè¯†åˆ«çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿå•æ¨¡æ€åŠ¨ä½œè¯†åˆ«æ–¹æ³•å­˜åœ¨å›ºæœ‰å±€é™æ€§ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯çš„äº’è¡¥ä¼˜åŠ¿ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€èåˆç­–ç•¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ¢ç´¢åœ¨åŠ¨ä½œè¯†åˆ«ä»»åŠ¡ä¸­æ•´åˆRGBã€å…‰æµã€éŸ³é¢‘å’Œæ·±åº¦ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºé—¨æ§æœºåˆ¶çš„å¤šæ¨¡æ€è‡ªé€‚åº”èåˆæ¶æ„ï¼Œé€šè¿‡é€‰æ‹©æ€§æ•´åˆä¸åŒæ¨¡æ€çš„ç›¸å…³ä¿¡æ¯ï¼Œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæŠ€æœ¯å’Œé—¨æ§èåˆç­–ç•¥ï¼Œç³»ç»Ÿç ”ç©¶äº†å¤šç§é—¨æ§èåˆæ–¹æ³•ä»¥ç¡®å®šæœ€æœ‰æ•ˆçš„å¤šæ¨¡æ€åŠ¨ä½œè¯†åˆ«æ–¹æ¡ˆã€‚</p>
<p><strong>Result:</strong> åœ¨äººç±»åŠ¨ä½œè¯†åˆ«ã€æš´åŠ›åŠ¨ä½œæ£€æµ‹å’Œå¤šä¸ªè‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡çš„åŸºå‡†æ•°æ®é›†è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„å•æ¨¡æ€æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€ä¿¡æ¯èåˆåœ¨åŠ¨ä½œè¯†åˆ«é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºç›‘æ§ç³»ç»Ÿå’Œäººæœºäº¤äº’ç­‰åº”ç”¨æä¾›äº†æ›´å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸»åŠ¨è¾…åŠ©ç”Ÿæ´»ç­‰åœºæ™¯ä¸­å…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>This study introduces a pioneering methodology for human action recognition by harnessing deep neural network techniques and adaptive fusion strategies across multiple modalities, including RGB, optical flows, audio, and depth information. Employing gating mechanisms for multimodal fusion, we aim to surpass limitations inherent in traditional unimodal recognition methods while exploring novel possibilities for diverse applications. Through an exhaustive investigation of gating mechanisms and adaptive weighting-based fusion architectures, our methodology enables the selective integration of relevant information from various modalities, thereby bolstering both accuracy and robustness in action recognition tasks. We meticulously examine various gated fusion strategies to pinpoint the most effective approach for multimodal action recognition, showcasing its superiority over conventional unimodal methods. Gating mechanisms facilitate the extraction of pivotal features, resulting in a more holistic representation of actions and substantial enhancements in recognition performance. Our evaluations across human action recognition, violence action detection, and multiple self-supervised learning tasks on benchmark datasets demonstrate promising advancements in accuracy. The significance of this research lies in its potential to revolutionize action recognition systems across diverse fields. The fusion of multimodal information promises sophisticated applications in surveillance and human-computer interaction, especially in contexts related to active assisted living.</p>
<h3 id="39-faster-toward-efficient-autoregressive-vision-language-action-modeling-via-neural-action-tokenization">[39] <a href="https://arxiv.org/abs/2512.04952">FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization</a></h3>
<p><em>Yicheng Liu, Shiduo Zhang, Zibin Dong, Baijun Ye, Tianyuan Yuan, Xiaopeng Yu, Linqi Yin, Chenhao Lu, Junhao Shi, Luca Jiang-Tao Yu, Liangtao Zheng, Tao Jiang, Jingjing Gong, Xipeng Qiu, Hang Zhao</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºFASTeræ¡†æ¶ï¼Œé€šè¿‡å¯å­¦ä¹ çš„åŠ¨ä½œåˆ†è¯å™¨å’ŒåŸºäºæ­¤çš„è‡ªå›å½’ç­–ç•¥ï¼Œè§£å†³äº†è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­åŠ¨ä½œåˆ†è¯åœ¨é‡å»ºä¿çœŸåº¦ä¸æ¨ç†æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œå®ç°äº†æ›´é«˜æ•ˆä¸”æ³›åŒ–æ€§æ›´å¼ºçš„æœºå™¨äººå­¦ä¹ ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªå›å½’è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†å…¶æ ¸å¿ƒçš„åŠ¨ä½œåˆ†è¯è¿‡ç¨‹é€šå¸¸éœ€è¦åœ¨é‡å»ºä¿çœŸåº¦ä¸æ¨ç†æ•ˆç‡ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½å’Œåº”ç”¨æ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> FASTeræ¡†æ¶æ•´åˆäº†å¯å­¦ä¹ åˆ†è¯å™¨ä¸åŸºäºæ­¤çš„è‡ªå›å½’ç­–ç•¥ï¼Œå…¶ä¸­FASTerVQå°†åŠ¨ä½œå—ç¼–ç ä¸ºå•é€šé“å›¾åƒä»¥æ•è·å…¨å±€æ—¶ç©ºä¾èµ–å¹¶ä¿æŒé«˜å‹ç¼©æ¯”ï¼Œè€ŒFASTerVLAåœ¨æ­¤åŸºç¡€ä¸Šé‡‡ç”¨åˆ†å—è‡ªå›å½’è§£ç å’Œè½»é‡çº§åŠ¨ä½œä¸“å®¶æ¨¡å—ã€‚</p>
<p><strong>Result:</strong> åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒFASTerVQå±•ç°å‡ºå“è¶Šçš„é‡å»ºè´¨é‡ã€é«˜ä»¤ç‰Œåˆ©ç”¨ç‡å’Œå¼ºå¤§çš„è·¨ä»»åŠ¡ä¸è·¨å…·èº«æ³›åŒ–èƒ½åŠ›ï¼Œè€ŒFASTerVLAåœ¨æ¨ç†é€Ÿåº¦å’Œä»»åŠ¡æ€§èƒ½ä¸Šå‡è¶…è¶Šäº†å…ˆå‰æœ€å…ˆè¿›çš„VLAæ¨¡å‹ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç»Ÿä¸€æ¡†æ¶æ•´åˆå¯å­¦ä¹ åˆ†è¯å™¨ä¸è‡ªå›å½’ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä¸ºé«˜æ•ˆä¸”æ³›åŒ–æ€§å¼ºçš„æœºå™¨äººå­¦ä¹ æä¾›äº†æ–°æ–¹å‘ï¼ŒåŒæ—¶å±•ç¤ºäº†åœ¨ä¿æŒé«˜è´¨é‡åŠ¨ä½œé‡å»ºçš„åŒæ—¶å®ç°å¿«é€Ÿæ¨ç†çš„å¯è¡Œæ€§ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.</p>
<h3 id="40-rethinking-the-use-of-vision-transformers-for-ai-generated-image-detection">[40] <a href="https://arxiv.org/abs/2512.04969">Rethinking the Use of Vision Transformers for AI-Generated Image Detection</a></h3>
<p><em>NaHyeon Park, Kunhee Kim, Junsuk Choe, Hyunjung Shim</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMoLDçš„è‡ªé€‚åº”æ–¹æ³•ï¼Œé€šè¿‡é—¨æ§æœºåˆ¶åŠ¨æ€é›†æˆCLIP-ViTçš„å¤šå±‚ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†AIç”Ÿæˆå›¾åƒæ£€æµ‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰AIç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³•ä¸»è¦åˆ©ç”¨CLIP-ViTçš„æœ€åä¸€å±‚ç‰¹å¾ï¼Œä½†ç¼ºä¹å¯¹å±‚é—´ç‰¹å¾è´¡çŒ®çš„ç³»ç»Ÿåˆ†æï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ä¸åŒå±‚æ¬¡ç‰¹å¾çš„äº’è¡¥ä¿¡æ¯ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºMoLDæ–¹æ³•ï¼Œé‡‡ç”¨åŸºäºé—¨æ§çš„æœºåˆ¶åŠ¨æ€é›†æˆViTçš„å¤šå±‚ç‰¹å¾ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªé€‚åº”åœ°èåˆä¸åŒå±‚æ¬¡çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶éªŒè¯äº†å…¶åœ¨å…¶ä»–é¢„è®­ç»ƒViTæ¨¡å‹ä¸Šçš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼ŒMoLDåœ¨GANå’Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒæ£€æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå¢å¼ºäº†è·¨ä¸åŒç”Ÿæˆæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨çœŸå®åœºæ™¯ä¸­è¡¨ç°å‡ºé²æ£’æ€§ï¼Œæ—©æœŸå±‚ç‰¹å¾åœ¨æ£€æµ‹ä»»åŠ¡ä¸­å¾€å¾€ä¼˜äºæœ€åä¸€å±‚ç‰¹å¾ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†ViTä¸åŒå±‚æ¬¡ç‰¹å¾åœ¨AIç”Ÿæˆå›¾åƒæ£€æµ‹ä¸­çš„äº’è¡¥ä»·å€¼ï¼Œæå‡ºçš„è‡ªé€‚åº”é›†æˆæ–¹æ³•ä¸ºç‰¹å¾åˆ©ç”¨æä¾›äº†æ–°èŒƒå¼ï¼Œè¯¥æ–¹æ³•å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯åº”ç”¨äºå…¶ä»–é¢„è®­ç»ƒè§†è§‰Transformeræ¨¡å‹ã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.</p>
<h3 id="41-stable-single-pixel-contrastive-learning-for-semantic-and-geometric-tasks">[41] <a href="https://arxiv.org/abs/2512.04970">Stable Single-Pixel Contrastive Learning for Semantic and Geometric Tasks</a></h3>
<p><em>Leonid Pogorelyuk, Niels Bracher, Aaron Verkleeren, Lars KÃ¼hmichel, Stefan T. Radev</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨³å®šçš„å¯¹æ¯”æŸå¤±å‡½æ•°æ—ï¼Œç”¨äºå­¦ä¹ åŒæ—¶æ•è·è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯çš„åƒç´ çº§è¡¨ç¤ºï¼Œè¯¥æ–¹æ³•é€šè¿‡è¿‡å®Œå¤‡æè¿°ç¬¦å®ç°è·¨å›¾åƒçš„ç²¾ç¡®ç‚¹å¯¹åº”ï¼Œæ— éœ€åŸºäºåŠ¨é‡çš„å¸ˆç”Ÿè®­ç»ƒã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–¹æ³•åœ¨åŒæ—¶æ•è·åƒç´ çº§è¡¨ç¤ºçš„è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯æ–¹é¢å­˜åœ¨å±€é™ï¼Œä¸”é€šå¸¸éœ€è¦å¤æ‚çš„åŠ¨é‡æ›´æ–°å¸ˆç”Ÿè®­ç»ƒæ¶æ„æ¥å®ç°è·¨å›¾åƒçš„ç‚¹å¯¹åº”ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§æ›´ç®€å•æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§ç¨³å®šçš„å¯¹æ¯”æŸå¤±å‡½æ•°æ—ï¼Œå°†å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ æ˜ å°„åˆ°è¿‡å®Œå¤‡æè¿°ç¬¦ï¼Œè¯¥æè¿°ç¬¦åŒæ—¶å…·å¤‡è§†è§’ä¸å˜æ€§å’Œè¯­ä¹‰æ„ä¹‰ï¼Œæ— éœ€ä¾èµ–åŸºäºåŠ¨é‡çš„å¸ˆç”Ÿè®­ç»ƒå³å¯å®ç°ç²¾ç¡®çš„ç‚¹å¯¹åº”ã€‚</p>
<p><strong>Result:</strong> åœ¨åˆæˆ2Då’Œ3Dç¯å¢ƒä¸­çš„å®éªŒéªŒè¯äº†æ‰€æå‡ºæŸå¤±å‡½æ•°çš„ç‰¹æ€§åŠå…¶ç”Ÿæˆçš„è¿‡å®Œå¤‡è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°ç²¾ç¡®çš„è·¨å›¾åƒç‚¹å¯¹åº”ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•ä¸ºåƒç´ çº§è¡¨ç¤ºå­¦ä¹ æä¾›äº†ä¸€ç§æ›´ç®€å•æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡è¿‡å®Œå¤‡æè¿°ç¬¦åŒæ—¶ç¼–ç è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ï¼Œä¸ºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„å¯†é›†å¯¹åº”å’Œåœºæ™¯ç†è§£å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>We pilot a family of stable contrastive losses for learning pixel-level representations that jointly capture semantic and geometric information. Our approach maps each pixel of an image to an overcomplete descriptor that is both view-invariant and semantically meaningful. It enables precise point-correspondence across images without requiring momentum-based teacher-student training. Two experiments in synthetic 2D and 3D environments demonstrate the properties of our loss and the resulting overcomplete representations.</p>
<h3 id="42-aligned-but-stereotypical-the-hidden-influence-of-system-prompts-on-social-bias-in-lvlm-based-text-to-image-models">[42] <a href="https://arxiv.org/abs/2512.04981">Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models</a></h3>
<p><em>NaHyeon Park, Namin An, Kunhee Kim, Soyeon Yoon, Jiahao Huo, Hyunjung Shim</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶å‘ç°åŸºäºå¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡ç”Ÿå›¾ç³»ç»Ÿæ¯”éLVLMæ¨¡å‹äº§ç”Ÿæ›´ä¸¥é‡çš„ç¤¾ä¼šåè§ï¼Œå¹¶æå‡ºFairProè®­ç»ƒæ— å…³çš„å…ƒæç¤ºæ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿæç¤ºè‡ªå®¡æ˜¾è‘—å‡å°‘åè§åŒæ—¶ä¿æŒå›¾æ–‡å¯¹é½ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŸºäºå¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡ç”Ÿå›¾ç³»ç»Ÿå·²æˆä¸ºå›¾åƒç”Ÿæˆçš„ä¸»æµèŒƒå¼ï¼Œä½†è¿™äº›ç³»ç»Ÿæ˜¯å¦æ”¾å¤§ç¤¾ä¼šåè§ä»ç¼ºä¹å……åˆ†ç†è§£ï¼Œéœ€è¦ç³»ç»Ÿè¯„ä¼°LVLMæ¨¡å‹çš„ç¤¾ä¼šåè§ç¨‹åº¦åŠå…¶äº§ç”Ÿæœºåˆ¶ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†åŒ…å«1024ä¸ªæç¤ºçš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å››ä¸ªè¯­è¨€å¤æ‚åº¦å±‚æ¬¡ï¼Œé€šè¿‡è§£ç ä¸­é—´è¡¨ç¤ºã€ä»¤ç‰Œæ¦‚ç‡è¯Šæ–­å’ŒåµŒå…¥å…³è”åˆ†ææ­ç¤ºç³»ç»Ÿæç¤ºå¦‚ä½•ç¼–ç äººå£ç»Ÿè®¡å…ˆéªŒï¼Œå¹¶æå‡ºFairProè®­ç»ƒæ— å…³çš„å…ƒæç¤ºæ¡†æ¶ï¼Œä½¿LVLMèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶è‡ªå®¡å¹¶æ„å»ºå…¬å¹³æ„ŸçŸ¥çš„ç³»ç»Ÿæç¤ºã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜LVLMæ¨¡å‹æ¯”éLVLMæ¨¡å‹äº§ç”Ÿæ˜æ˜¾æ›´ä¸¥é‡çš„ç¤¾ä¼šåè§å›¾åƒï¼Œç³»ç»Ÿæç¤ºæ˜¯åè§è¡Œä¸ºçš„ä¸»è¦é©±åŠ¨å› ç´ ï¼ŒFairProæ¡†æ¶åœ¨SANAå’ŒQwen-Imageä¸¤ä¸ªLVLMæ¨¡å‹ä¸Šæ˜¾è‘—å‡å°‘äººå£ç»Ÿè®¡åè§ï¼ŒåŒæ—¶ä¿æŒæ–‡æœ¬-å›¾åƒå¯¹é½æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†ç³»ç»Ÿæç¤ºåœ¨åè§ä¼ æ’­ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œä¸ºæ„å»ºæ›´è´Ÿè´£ä»»çš„æ–‡ç”Ÿå›¾ç³»ç»Ÿæä¾›äº†å®ç”¨å¯éƒ¨ç½²çš„æ–¹æ³•ï¼ŒFairProæ¡†æ¶å±•ç¤ºäº†é€šè¿‡å…ƒæç¤ºå®ç°åè§ç¼“è§£çš„æœ‰æ•ˆé€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.</p>
<h3 id="43-htr-convtext-leveraging-convolution-and-textual-information-for-handwritten-text-recognition">[43] <a href="https://arxiv.org/abs/2512.05021">HTR-ConvText: Leveraging Convolution and Textual Information for Handwritten Text Recognition</a></h3>
<p><em>Pham Thach Thanh Truc, Dang Hoai Nam, Huynh Tong Dang Khoa, Vo Nguyen Le Duy</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºHTR-ConvTextæ¨¡å‹ï¼Œé€šè¿‡ç»“åˆå±€éƒ¨ç¬”ç”»ç‰¹å¾ä¸å…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–æ¥è§£å†³æ‰‹å†™æ–‡æœ¬è¯†åˆ«ä¸­çš„æŒ‘æˆ˜ï¼Œåœ¨æœ‰é™æ•°æ®å’Œå¤æ‚ä¹¦å†™é£æ ¼åœºæ™¯ä¸‹å®ç°äº†æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ‰‹å†™æ–‡æœ¬è¯†åˆ«é¢ä¸´æ•°æ®æœ‰é™ã€ä¹¦å†™é£æ ¼å·®å¼‚å¤§ä»¥åŠå¤æ‚å˜éŸ³ç¬¦å·ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡åˆæˆæ•°æ®æ‰èƒ½æ³›åŒ–ï¼Œéš¾ä»¥åœ¨æœ‰é™è®­ç»ƒæ ·æœ¬å’Œé«˜å¤šæ ·æ€§æ‰‹å†™åœºæ™¯ä¸­å–å¾—è‰¯å¥½æ•ˆæœã€‚</p>
<p><strong>Method:</strong> æå‡ºHTR-ConvTextæ¨¡å‹ï¼Œåœ¨ç‰¹å¾æå–é˜¶æ®µé›†æˆæ®‹å·®å·ç§¯ç¥ç»ç½‘ç»œä¸»å¹²ä¸å¸¦ä½ç½®ç¼–ç çš„MobileViTæ¨¡å—ï¼Œä»¥åŒæ—¶æ•æ‰ç»“æ„æ¨¡å¼å’Œç»†å¾®ä¹¦å†™ç»†èŠ‚ï¼›å¼•å…¥ConvTextç¼–ç å™¨ï¼Œç»“åˆå…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç‰¹å¾çš„æ··åˆæ¶æ„ï¼Œé€šè¿‡åˆ†å±‚ç»“æ„å‡å°‘åºåˆ—é•¿åº¦æé«˜æ•ˆç‡ï¼›æ·»åŠ è¾…åŠ©æ¨¡å—æ³¨å…¥æ–‡æœ¬ä¸Šä¸‹æ–‡ä»¥ç¼“è§£è¿æ¥æ—¶åºåˆ†ç±»çš„å¼±ç‚¹ã€‚</p>
<p><strong>Result:</strong> åœ¨IAMã€READ2016ã€LAMå’ŒHANDS-VNOnDBæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ç°æœ‰æ–¹æ³•å®ç°äº†æ€§èƒ½æå‡å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ ·æœ¬æœ‰é™å’Œæ‰‹å†™å¤šæ ·æ€§é«˜çš„åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†ç»“åˆå±€éƒ¨ç¬”ç”»çº§ç‰¹å¾ä¸å…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœ‰é™æ•°æ®ä¸‹çš„æ‰‹å†™æ–‡æœ¬è¯†åˆ«æä¾›äº†æ–°æ€è·¯ï¼Œæ··åˆæ¶æ„è®¾è®¡åœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶æå‡äº†æ¨¡å‹å¯¹å¤æ‚ä¹¦å†™é£æ ¼çš„é€‚åº”æ€§ã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>Handwritten Text Recognition remains challenging due to the limited data, high writing style variance, and scripts with complex diacritics. Existing approaches, though partially address these issues, often struggle to generalize without massive synthetic data. To address these challenges, we propose HTR-ConvText, a model designed to capture fine-grained, stroke-level local features while preserving global contextual dependencies. In the feature extraction stage, we integrate a residual Convolutional Neural Network backbone with a MobileViT with Positional Encoding block. This enables the model to both capture structural patterns and learn subtle writing details. We then introduce the ConvText encoder, a hybrid architecture combining global context and local features within a hierarchical structure that reduces sequence length for improved efficiency. Additionally, an auxiliary module injects textual context to mitigate the weakness of Connectionist Temporal Classification. Evaluations on IAM, READ2016, LAM and HANDS-VNOnDB demonstrate that our approach achieves improved performance and better generalization compared to existing methods, especially in scenarios with limited training samples and high handwriting diversity.</p>
<h3 id="44-ramen-resolution-adjustable-multimodal-encoder-for-earth-observation">[44] <a href="https://arxiv.org/abs/2512.05025">RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation</a></h3>
<p><em>Nicolas HoudrÃ©, Diego Marcos, Hugo Riffaud de Turckheim, Dino Ienco, Laurent Wendling, Camille Kurtz, Sylvain Lobry</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºRAMENï¼Œä¸€ç§åˆ†è¾¨ç‡å¯è°ƒçš„å¤šæ¨¡æ€ç¼–ç å™¨ï¼Œèƒ½å¤Ÿåœ¨å®Œå…¨ä¼ æ„Ÿå™¨æ— å…³çš„æ–¹å¼ä¸‹å­¦ä¹ åœ°çƒè§‚æµ‹æ•°æ®çš„å…±äº«è§†è§‰è¡¨ç¤ºï¼Œé€šè¿‡å°†ç©ºé—´åˆ†è¾¨ç‡å®šä¹‰ä¸ºå¯æ§è¾“å‡ºå‚æ•°ï¼Œå®ç°äº†è·¨å¼‚æ„æ¨¡æ€çš„ç»Ÿä¸€åˆ†æã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹é€šå¸¸æœŸæœ›å›ºå®šè¾“å…¥åˆ†è¾¨ç‡æˆ–åŸºäºç‰¹å®šä¼ æ„Ÿå™¨ç¼–ç å™¨ï¼Œé™åˆ¶äº†è·¨å¼‚æ„EOæ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†å¹¿æ³›çš„ç©ºé—´ã€å…‰è°±å’Œæ—¶é—´åˆ†è¾¨ç‡èŒƒå›´ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿç»Ÿä¸€å¤„ç†ä¸åŒä¼ æ„Ÿå™¨å’Œåˆ†è¾¨ç‡æ•°æ®çš„ä¼ æ„Ÿå™¨æ— å…³æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> RAMENå°†æ¨¡æ€ã€ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡ä½œä¸ºå…³é”®è¾“å…¥ç‰¹å¾ï¼Œå®šä¹‰ç©ºé—´åˆ†è¾¨ç‡ä¸ºå¯æ§è¾“å‡ºå‚æ•°ï¼Œä½¿ç”¨å•ä¸€ç»Ÿä¸€çš„Transformerç¼–ç å™¨é‡å»ºæ¥è‡ªå¤šæ ·æ¥æºçš„æ©ç å¤šæ¨¡æ€EOæ•°æ®ï¼Œé€šè¿‡åˆ†è¾¨ç‡è°ƒæ•´æœºåˆ¶å®ç°ç©ºé—´ç²¾åº¦ä¸è®¡ç®—æˆæœ¬ä¹‹é—´çš„æ˜¾å¼æƒè¡¡ã€‚</p>
<p><strong>Result:</strong> RAMENåœ¨ç¤¾åŒºæ ‡å‡†PANGAEAåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæ›´å¤§çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œæœ‰æ•ˆè¿ç§»åˆ°å·²çŸ¥å’Œæœªè§è¿‡çš„ä¼ æ„Ÿå™¨é…ç½®ï¼Œåœ¨åŒ…å«å„ç§å¤šä¼ æ„Ÿå™¨å’Œå¤šåˆ†è¾¨ç‡ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶è·¨ä¼ æ„Ÿå™¨å’Œåˆ†è¾¨ç‡çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§ä¼ æ„Ÿå™¨æ— å…³çš„åœ°çƒè§‚æµ‹æ•°æ®ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¯æ§åˆ†è¾¨ç‡è¾“å‡ºå®ç°äº†åˆ†æçµæ´»æ€§ä¸è®¡ç®—æ•ˆç‡çš„å¹³è¡¡ï¼Œä¸ºè·¨æ¨¡æ€EOåˆ†æå¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å¼€æºä¿ƒè¿›ç¤¾åŒºé‡‡ç”¨ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.</p>
<h3 id="45-semantic-guided-two-stage-gan-for-face-inpainting-with-hybrid-perceptual-encoding">[45] <a href="https://arxiv.org/abs/2512.05039">Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding</a></h3>
<p><em>Abhigyan Bhattacharya, Hiranmoy Roy</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¯­ä¹‰å¼•å¯¼åˆ†å±‚åˆæˆæ¶æ„ï¼Œç”¨äºé¢éƒ¨å›¾åƒä¿®å¤ï¼Œé€šè¿‡ç»“åˆCNNçš„å±€éƒ¨ç‰¹å¾æå–å’ŒVision Transformerçš„å…¨å±€ç‰¹å¾å»ºæ¨¡ï¼Œæœ‰æ•ˆå¤„ç†å¤§é¢ç§¯ä¸è§„åˆ™æ©ç ï¼Œå¹¶åœ¨ä¿æŒèº«ä»½ä¸€è‡´æ€§å’Œç»“æ„è¿è´¯æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é¢éƒ¨å›¾åƒä¿®å¤æ–¹æ³•åœ¨å¤„ç†å¤§é¢ç§¯ä¸è§„åˆ™æ©ç æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¸¸äº§ç”Ÿæ¨¡ç³Šçº¹ç†ã€è¯­ä¹‰ä¸ä¸€è‡´æˆ–ä¸åˆç†çš„é¢éƒ¨ç»“æ„ï¼Œè¿™ä¸»è¦æºäºç›´æ¥åƒç´ çº§åˆæˆæ–¹æ³•å’Œå¯¹é¢éƒ¨å…ˆéªŒçŸ¥è¯†çš„æœ‰é™åˆ©ç”¨ï¼Œæ— æ³•åœ¨ä¿æŒèº«ä»½ä¸€è‡´æ€§å’Œç»“æ„è¿è´¯æ€§çš„åŒæ—¶å®ç°é€¼çœŸçš„ä¿®å¤æ•ˆæœã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºè¯­ä¹‰å¼•å¯¼åˆ†å±‚åˆæˆæ¶æ„ï¼ŒåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µç»“åˆCNNçš„å±€éƒ¨ç‰¹å¾æå–å’ŒVision Transformerçš„å…¨å±€ç‰¹å¾å»ºæ¨¡ï¼Œç”Ÿæˆæ¸…æ™°è¯¦ç»†çš„è¯­ä¹‰å¸ƒå±€ï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨å¤šæ¨¡æ€çº¹ç†ç”Ÿæˆå™¨ï¼Œé€šè¿‡å¤šå°ºåº¦ä¿¡æ¯èåˆç»†åŒ–çº¹ç†ï¼Œè¯¥æ¶æ„é€šè¿‡åŠ¨æ€æ³¨æ„åŠ›æœºåˆ¶è‡ªç„¶å¤„ç†ä»»æ„æ©ç é…ç½®ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šæ©ç è¿›è¡Œè®­ç»ƒã€‚</p>
<p><strong>Result:</strong> åœ¨CelebA-HQå’ŒFFHQæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨LPIPSã€PSNRå’ŒSSIMç­‰æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤§é¢ç§¯ä¿®å¤åœºæ™¯ä¸­ï¼Œèƒ½å¤Ÿäº§ç”Ÿè§†è§‰ä¸Šå¼•äººæ³¨ç›®çš„ç»“æœï¼Œå¹¶å®ç°æ›´å¥½çš„è¯­ä¹‰ä¿æŒå’Œç»“æ„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†è¯­ä¹‰å¼•å¯¼åˆ†å±‚åˆæˆæ–¹æ³•åœ¨é¢éƒ¨å›¾åƒä¿®å¤ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡åˆ†ç¦»è¯­ä¹‰å¸ƒå±€ç”Ÿæˆå’Œçº¹ç†ç»†åŒ–è¿‡ç¨‹ï¼Œç»“åˆå±€éƒ¨ä¸å…¨å±€ç‰¹å¾å»ºæ¨¡ï¼Œä¸ºå¤„ç†å¤æ‚æ©ç é…ç½®æä¾›äº†æ–°æ€è·¯ï¼Œæœªæ¥å¯æ‰©å±•åˆ°å…¶ä»–å›¾åƒä¿®å¤ä»»åŠ¡å’Œæ›´å¹¿æ³›çš„ç”Ÿæˆå¼è§†è§‰åº”ç”¨ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>Facial Image inpainting aim is to restore the missing or corrupted regions in face images while preserving identity, structural consistency and photorealistic image quality, a task specifically created for photo restoration. Though there are recent lot of advances in deep generative models, existing methods face problems with large irregular masks, often producing blurry textures on the edges of the masked region, semantic inconsistencies, or unconvincing facial structures due to direct pixel level synthesis approach and limited exploitation of facial priors. In this paper we propose a novel architecture, which address these above challenges through semantic-guided hierarchical synthesis. Our approach starts with a method that organizes and synthesizes information based on meaning, followed by refining the texture. This process gives clear insights into the facial structure before we move on to creating detailed images. In the first stage, we blend two techniques: one that focuses on local features with CNNs and global features with Vision Transformers. This helped us create clear and detailed semantic layouts. In the second stage, we use a Multi-Modal Texture Generator to refine these layouts by pulling in information from different scales, ensuring everything looks cohesive and consistent. The architecture naturally handles arbitrary mask configurations through dynamic attention without maskspecific training. Experiment on two datasets CelebA-HQ and FFHQ shows that our model outperforms other state-of-the-art methods, showing improvements in metrics like LPIPS, PSNR, and SSIM. It produces visually striking results with better semantic preservation, in challenging large-area inpainting situations.</p>
<h3 id="46-visual-reasoning-tracer-object-level-grounded-reasoning-benchmark">[46] <a href="https://arxiv.org/abs/2512.05091">Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark</a></h3>
<p><em>Haobo Yuan, Yueyi Sun, Yanwei Li, Tao Zhang, Xueqing Deng, Henghui Ding, Lu Qi, Anran Wang, Xiangtai Li, Ming-Hsuan Yang</em></p>
<h4 id="tldr_45">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†è§†è§‰æ¨ç†è¿½è¸ªï¼ˆVRTï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸é€æ˜çš„é—®é¢˜ï¼Œå¹¶æ„å»ºäº†VRT-Benchè¯„ä¼°åŸºå‡†å’ŒVRT-80kè®­ç»ƒæ•°æ®é›†ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹ä¸­é—´æ¨ç†æ­¥éª¤çš„æ˜¾å¼è¿½è¸ªèƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_45">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰å®šä½å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹é€šå¸¸ä¸é€æ˜ï¼Œä»…è¾“å‡ºæœ€ç»ˆé¢„æµ‹è€Œç¼ºä¹æ­ç¤ºä¸­é—´æ­¥éª¤æˆ–ç»†ç²’åº¦è¯æ®ï¼ˆå¦‚åƒç´ ã€ä½ç½®ï¼‰çš„èƒ½åŠ›ï¼Œè¿™ä¸äººç±»é€šè¿‡è§†è§‰æ¨ç†é“¾è¿›è¡Œæ€è€ƒçš„è‡ªç„¶æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡å¼•å…¥äº†è§†è§‰æ¨ç†è¿½è¸ªï¼ˆVRTï¼‰ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹ä¸ä»…å®šä½ç›®æ ‡å¯¹è±¡ï¼Œè¿˜éœ€æ˜¾å¼é¢„æµ‹æ„æˆæ¨ç†è·¯å¾„çš„ä¸­é—´å¯¹è±¡ï¼›ä¸ºæ­¤æ„å»ºäº†VRT-Benchäººå·¥æ ‡æ³¨è¯„ä¼°åŸºå‡†ã€æ–°çš„æ¨ç†è½¨è¿¹è´¨é‡è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥åŠç”¨äºè®­ç»ƒçš„å¤§è§„æ¨¡æ•°æ®é›†VRT-80kã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹è™½ç„¶å¸¸èƒ½äº§ç”Ÿæ­£ç¡®æœ€ç»ˆè¾“å‡ºï¼Œä½†åœ¨ä¸­é—´æ¨ç†çš„æ˜¾å¼å®šä½æ–¹é¢è¡¨ç°ä¸ä½³ï¼›ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨VRT-80kæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨è¿½è¸ªæ¨ç†è·¯å¾„æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•å’Œæ•°æ®èµ„æºçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ˜¾å¼è§†è§‰æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºçš„VRTä»»åŠ¡æ¡†æ¶ã€è¯„ä¼°åŸºå‡†å’Œè®­ç»ƒæ•°æ®é›†ä¸ºæå‡æ¨¡å‹æ¨ç†é€æ˜åº¦æä¾›äº†ç³»ç»Ÿè§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å¯è§£é‡Šå¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„å‘å±•æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_45">ğŸ“„ Abstract</h4>
<p>Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path.</p>
<h3 id="47-neuralremaster-phase-preserving-diffusion-for-structure-aligned-generation">[47] <a href="https://arxiv.org/abs/2512.05106">NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation</a></h3>
<p><em>Yu Zeng, Charles Ochoa, Mingyuan Zhou, Vishal M. Patel, Vitor Guizilini, Rowan McAllister</em></p>
<h4 id="tldr_46">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ç›¸ä½ä¿æŒæ‰©æ•£ï¼ˆÏ†-PDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„æ‰©æ•£è¿‡ç¨‹é‡æ„æ–¹æ³•ï¼Œé€šè¿‡ä¿ç•™è¾“å…¥ç›¸ä½å¹¶éšæœºåŒ–å¹…åº¦ï¼Œå®ç°äº†æ— éœ€æ¶æ„ä¿®æ”¹æˆ–é¢å¤–å‚æ•°çš„ç»“æ„å¯¹é½ç”Ÿæˆï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦å‡ ä½•ä¸€è‡´æ€§çš„ä»»åŠ¡ã€‚</p>
<hr />
<h4 id="detailed-summary_46">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ ‡å‡†æ‰©æ•£è¿‡ç¨‹ä½¿ç”¨é«˜æ–¯å™ªå£°ç ´åæ•°æ®çš„ç›¸ä½å’Œå¹…åº¦åˆ†é‡ï¼Œè¿™è™½ç„¶é€‚ç”¨äºæ— æ¡ä»¶æˆ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œä½†ç›¸ä½ç ´åä¼šæ¶ˆé™¤ç©ºé—´ç»“æ„ï¼Œä½¿å…¶ä¸é€‚ç”¨äºéœ€è¦å‡ ä½•ä¸€è‡´æ€§çš„ä»»åŠ¡ï¼Œå¦‚é‡æ¸²æŸ“ã€ä»¿çœŸå¢å¼ºå’Œå›¾åƒåˆ°å›¾åƒè½¬æ¢ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ç›¸ä½ä¿æŒæ‰©æ•£ï¼ˆÏ†-PDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„æ‰©æ•£è¿‡ç¨‹é‡æ„æ–¹æ³•ï¼Œé€šè¿‡ä¿ç•™è¾“å…¥ç›¸ä½åŒæ—¶éšæœºåŒ–å¹…åº¦æ¥å®ç°ç»“æ„å¯¹é½ç”Ÿæˆã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†é¢‘ç‡é€‰æ‹©æ€§ç»“æ„åŒ–ï¼ˆFSSï¼‰å™ªå£°ï¼Œé€šè¿‡å•ä¸€é¢‘ç‡æˆªæ­¢å‚æ•°æä¾›å¯¹ç»“æ„åˆšåº¦çš„è¿ç»­æ§åˆ¶ã€‚è¯¥æ–¹æ³•ä¸å¢åŠ æ¨ç†æ—¶é—´æˆæœ¬ï¼Œä¸”ä¸ä»»ä½•å›¾åƒæˆ–è§†é¢‘æ‰©æ•£æ¨¡å‹å…¼å®¹ã€‚</p>
<p><strong>Result:</strong> åœ¨ç…§ç‰‡çº§çœŸå®æ„Ÿå’Œé£æ ¼åŒ–é‡æ¸²æŸ“ä»¥åŠé©¾é©¶è§„åˆ’å™¨çš„ä»¿çœŸåˆ°çœŸå®å¢å¼ºä»»åŠ¡ä¸­ï¼ŒÏ†-PDäº§ç”Ÿäº†å¯æ§ä¸”ç©ºé—´å¯¹é½çš„ç»“æœã€‚å½“åº”ç”¨äºCARLAä»¿çœŸå™¨æ—¶ï¼ŒÏ†-PDå°†CARLAåˆ°Waymoè§„åˆ’å™¨æ€§èƒ½æå‡äº†50%ã€‚è¯¥æ–¹æ³•ä¸ç°æœ‰æ¡ä»¶æ–¹æ³•äº’è¡¥ï¼Œå¹¿æ³›é€‚ç”¨äºå›¾åƒåˆ°å›¾åƒå’Œè§†é¢‘åˆ°è§†é¢‘ç”Ÿæˆã€‚</p>
<p><strong>Conclusion:</strong> ç›¸ä½ä¿æŒæ‰©æ•£æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥è§£å†³æ‰©æ•£æ¨¡å‹ä¸­å‡ ä½•ä¸€è‡´æ€§é—®é¢˜ï¼Œé€šè¿‡ç›¸ä½ä¿ç•™æœºåˆ¶å®ç°äº†ç»“æ„å¯¹é½ç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹æ— å…³æ€§å’Œé›¶æ¨ç†æˆæœ¬ã€‚è¯¥æ–¹æ³•ä¸ºéœ€è¦ç©ºé—´ç»“æ„ä¿æŒçš„åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå¹¶å±•ç¤ºäº†åœ¨ä»¿çœŸå¢å¼ºå’Œè·¨åŸŸè½¬æ¢ä»»åŠ¡ä¸­çš„å®é™…ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_46">ğŸ“„ Abstract</h4>
<p>Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion Ï†-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. Ï†-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, Ï†-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, Ï†-PD improves CARLA-to-Waymo planner performance by 50\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.</p>
<h3 id="48-arm-thinker-reinforcing-multimodal-generative-reward-models-with-agentic-tool-use-and-visual-reasoning">[48] <a href="https://arxiv.org/abs/2512.05111">ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning</a></h3>
<p><em>Shengyuan Ding, Xinyu Fang, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiangyu Zhao, Haodong Duan, Xiaoyi Dong, Jianze Liang, Bin Wang, Conghui He, Dahua Lin, Jiaqi Wang</em></p>
<h4 id="tldr_47">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºARM-Thinkerï¼Œä¸€ç§èƒ½å¤Ÿè‡ªä¸»è°ƒç”¨å¤–éƒ¨å·¥å…·è¿›è¡Œå¯éªŒè¯è¯æ®æ¨ç†çš„æ™ºèƒ½å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡å·¥å…·è°ƒç”¨æœºåˆ¶æ˜¾è‘—æå‡äº†å¥–åŠ±æ¨¡å‹åœ¨å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_47">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€ç³»ç»Ÿçš„å¥–åŠ±æ¨¡å‹å­˜åœ¨å¹»è§‰é—®é¢˜ã€è§†è§‰åŸºç¡€è–„å¼±ä»¥åŠæ— æ³•ä½¿ç”¨å·¥å…·è¿›è¡ŒéªŒè¯ç­‰å±€é™æ€§ï¼Œè¿™äº›ç¼ºé™·é™åˆ¶äº†å…¶åœ¨å¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­çš„å¯é æ€§ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿè¿›è¡Œå¯éªŒè¯æ¨ç†çš„æ™ºèƒ½å¥–åŠ±æ¨¡å‹ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºARM-Thinkeræ¨¡å‹ï¼Œé‡‡ç”¨æ™ºèƒ½å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹æ¶æ„ï¼Œèƒ½å¤Ÿè‡ªä¸»è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚å›¾åƒè£å‰ªã€æ–‡æ¡£é¡µé¢æ£€ç´¢ï¼‰æ¥åŸºäºå¯éªŒè¯è¯æ®è¿›è¡Œåˆ¤æ–­ï¼Œæ›¿ä»£ä¼ ç»Ÿçš„é™æ€éäº¤äº’å¼å¥–åŠ±è¯„åˆ†ï¼Œå¹¶é€šè¿‡å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ è”åˆä¼˜åŒ–å·¥å…·è°ƒç”¨å†³ç­–å’Œåˆ¤æ–­å‡†ç¡®æ€§ã€‚</p>
<p><strong>Result:</strong> ARM-Thinkeråœ¨å¥–åŠ±å»ºæ¨¡åŸºå‡†ä¸Šå®ç°äº†å¹³å‡16.2%çš„æ€§èƒ½æå‡ï¼Œåœ¨å·¥å…·ä½¿ç”¨ä»»åŠ¡ä¸Šæå‡äº†9.6%ï¼Œå¹¶åœ¨å¤šæ¨¡æ€æ•°å­¦å’Œé€»è¾‘æ¨ç†åŸºå‡†ä¸Šä¼˜äºåŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶å¼•å…¥äº†ARMBench-VLè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªä¸“é—¨è¯„ä¼°ç»†ç²’åº¦è§†è§‰åŸºç¡€ã€å¤šé¡µæ–‡æ¡£ç†è§£å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„åŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜æ™ºèƒ½èƒ½åŠ›æ˜¾è‘—æå‡äº†å¥–åŠ±æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œé€šè¿‡å·¥å…·è°ƒç”¨æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤ŸéªŒè¯ç»†ç²’åº¦è§†è§‰ç»†èŠ‚ã€äº¤å‰å¼•ç”¨å¤šé¡µè¯æ®å’ŒéªŒè¯æ¨ç†ä¸»å¼ ï¼Œä¸ºæ„å»ºæ›´å¯é çš„å¤šæ¨¡æ€å¯¹é½ç³»ç»Ÿæä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_47">ğŸ“„ Abstract</h4>
<p>Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="49-msme-a-multi-stage-multi-expert-framework-for-zero-shot-stance-detection">[49] <a href="https://arxiv.org/abs/2512.04492">MSME: A Multi-Stage Multi-Expert Framework for Zero-Shot Stance Detection</a></h3>
<p><em>Yuanshuo Zhang, Aohua Li, Bo Chen, Jingbo Sun, Xiaobing Zhao</em></p>
<h4 id="tldr_48">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMSMEï¼Œä¸€ç§ç”¨äºé›¶æ ·æœ¬ç«‹åœºæ£€æµ‹çš„å¤šé˜¶æ®µå¤šä¸“å®¶æ¡†æ¶ï¼Œé€šè¿‡çŸ¥è¯†å‡†å¤‡ã€ä¸“å®¶æ¨ç†å’Œå†³ç­–èšåˆä¸‰é˜¶æ®µå¤„ç†å¤æ‚ç°å®åœºæ™¯ä¸­çš„ç«‹åœºç†è§£æŒ‘æˆ˜ï¼Œåœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_48">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡åŸºäºLLMçš„æ–¹æ³•åœ¨é›¶æ ·æœ¬ç«‹åœºæ£€æµ‹ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éœ€è¦åŠ¨æ€èƒŒæ™¯çŸ¥è¯†ã€æ¶‰åŠå¤åˆå®ä½“æˆ–äº‹ä»¶çš„ç›®æ ‡å®šä¹‰éœ€è¦ä¸ç«‹åœºæ ‡ç­¾æ˜ç¡®å…³è”ï¼Œä»¥åŠè®½åˆºç­‰ä¿®è¾æ‰‹æ³•å¸¸å¸¸æ©ç›–ä½œè€…çœŸå®æ„å›¾ã€‚</p>
<p><strong>Method:</strong> MSMEæ¡†æ¶åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šçŸ¥è¯†å‡†å¤‡é˜¶æ®µæ£€ç´¢ç›¸å…³èƒŒæ™¯çŸ¥è¯†å¹¶æ¾„æ¸…ç«‹åœºæ ‡ç­¾ï¼›ä¸“å®¶æ¨ç†é˜¶æ®µåŒ…å«ä¸‰ä¸ªä¸“é—¨æ¨¡å—â€”â€”çŸ¥è¯†ä¸“å®¶ä»çŸ¥è¯†è§’åº¦æç‚¼å…³é”®äº‹å®å¹¶è¿›è¡Œæ¨ç†ï¼Œæ ‡ç­¾ä¸“å®¶ç»†åŒ–ç«‹åœºæ ‡ç­¾å¹¶ç›¸åº”æ¨ç†ï¼Œè¯­ç”¨ä¸“å®¶æ£€æµ‹è®½åˆºç­‰ä¿®è¾çº¿ç´¢ä»è¯­ç”¨è§’åº¦æ¨æ–­æ„å›¾ï¼›å†³ç­–èšåˆé˜¶æ®µé€šè¿‡å…ƒæ³•å®˜æ•´åˆæ‰€æœ‰ä¸“å®¶åˆ†æç”Ÿæˆæœ€ç»ˆç«‹åœºé¢„æµ‹ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMSMEåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…¨é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤æ‚é›¶æ ·æœ¬ç«‹åœºæ£€æµ‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> MSMEæ¡†æ¶é€šè¿‡å¤šé˜¶æ®µå¤šä¸“å®¶è®¾è®¡æœ‰æ•ˆè§£å†³äº†å¤æ‚ç°å®åœºæ™¯ä¸­çš„ç«‹åœºæ£€æµ‹æŒ‘æˆ˜ï¼Œå…¶æ¨¡å—åŒ–æ¶æ„å…è®¸ä¸“é—¨å¤„ç†çŸ¥è¯†ã€æ ‡ç­¾å’Œè¯­ç”¨ç­‰ä¸åŒç»´åº¦ï¼Œä¸ºéœ€è¦åŠ¨æ€èƒŒæ™¯ç†è§£å’Œä¿®è¾åˆ†æçš„NLPä»»åŠ¡æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒæ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_48">ğŸ“„ Abstract</h4>
<p>LLM-based approaches have recently achieved impressive results in zero-shot stance detection. However, they still struggle in complex real-world scenarios, where stance understanding requires dynamic background knowledge, target definitions involve compound entities or events that must be explicitly linked to stance labels, and rhetorical devices such as irony often obscure the author's actual intent. To address these challenges, we propose MSME, a Multi-Stage, Multi-Expert framework for zero-shot stance detection. MSME consists of three stages: (1) Knowledge Preparation, where relevant background knowledge is retrieved and stance labels are clarified; (2) Expert Reasoning, involving three specialized modules-Knowledge Expert distills salient facts and reasons from a knowledge perspective, Label Expert refines stance labels and reasons accordingly, and Pragmatic Expert detects rhetorical cues such as irony to infer intent from a pragmatic angle; (3) Decision Aggregation, where a Meta-Judge integrates all expert analyses to produce the final stance prediction. Experiments on three public datasets show that MSME achieves state-of-the-art performance across the board.</p>
<h3 id="50-factuality-and-transparency-are-all-rag-needs-self-explaining-contrastive-evidence-re-ranking">[50] <a href="https://arxiv.org/abs/2512.05012">Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking</a></h3>
<p><em>Francielle Vargas, Daniel Pedronette</em></p>
<h4 id="tldr_49">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè‡ªè§£é‡Šå¯¹æ¯”è¯æ®é‡æ’åºï¼ˆCERï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹ å¾®è°ƒåµŒå…¥å¹¶ä¸ºæ¯ä¸ªæ£€ç´¢åˆ°çš„æ®µè½ç”Ÿæˆè¯çº§å½’å› è§£é‡Šï¼Œä»è€Œå›´ç»•äº‹å®è¯æ®é‡æ„æ£€ç´¢è¿‡ç¨‹ï¼Œæ—¨åœ¨æé«˜æ£€ç´¢å‡†ç¡®æ€§å¹¶å‡å°‘RAGç³»ç»Ÿä¸­çš„å¹»è§‰é£é™©ã€‚</p>
<hr />
<h4 id="detailed-summary_49">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿæ£€ç´¢æ–¹æ³•åœ¨å®‰å…¨å…³é”®é¢†åŸŸä¸­å¯èƒ½å­˜åœ¨çš„å¹»è§‰é£é™©é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦åŸºäºè¯æ®çš„å¯é æ£€ç´¢åœºæ™¯ä¸­ï¼Œå¦‚ä¸´åºŠè¯•éªŒæŠ¥å‘Šåˆ†æï¼Œä¼ ç»Ÿæ–¹æ³•ç¼ºä¹å¯¹äº‹å®è¯æ®çš„æ˜¾å¼å¯¹é½å’Œé€æ˜è§£é‡Šæœºåˆ¶ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨è‡ªè§£é‡Šå¯¹æ¯”è¯æ®é‡æ’åºæ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¾®è°ƒåµŒå…¥è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨åŸºäºä¸»è§‚æ€§çš„æ ‡å‡†è‡ªåŠ¨é€‰æ‹©å›°éš¾è´Ÿæ ·æœ¬ï¼ŒåŒæ—¶ä¸ºæ¯ä¸ªæ£€ç´¢åˆ°çš„æ®µè½ç”Ÿæˆè¯çº§å½’å› è§£é‡Šï¼Œè¿«ä½¿æ¨¡å‹å°†äº‹å®è§£é‡Šæ‹‰è¿‘è€Œå°†ä¸»è§‚æˆ–è¯¯å¯¼æ€§è§£é‡Šæ¨å¼€ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸´åºŠè¯•éªŒæŠ¥å‘Šä¸Šçš„åˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼ŒCERæ–¹æ³•æ˜¾è‘—æé«˜äº†æ£€ç´¢å‡†ç¡®æ€§ï¼Œæœ‰æ•ˆç¼“è§£äº†RAGç³»ç»Ÿä¸­çš„æ½œåœ¨å¹»è§‰é£é™©ï¼Œå¹¶æä¾›äº†é€æ˜ã€åŸºäºè¯æ®çš„æ£€ç´¢æœºåˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®é¢†åŸŸä¸­å¢å¼ºäº†ç³»ç»Ÿçš„å¯é æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•åˆ›é€ äº†ä¸€ä¸ªä¸è¯æ®æ¨ç†æ˜¾å¼å¯¹é½çš„åµŒå…¥ç©ºé—´ï¼Œä¸ºå®‰å…¨å…³é”®é¢†åŸŸæä¾›äº†æ›´å¯é çš„æ£€ç´¢è§£å†³æ–¹æ¡ˆï¼Œå…¶é€æ˜æ€§å’ŒåŸºäºè¯æ®çš„ç‰¹æ€§ä½¿å…¶ç‰¹åˆ«é€‚ç”¨äºéœ€è¦é«˜å¯é æ€§çš„åº”ç”¨åœºæ™¯ï¼Œä¸ºæœªæ¥æ£€ç´¢ç³»ç»Ÿçš„å¯ä¿¡æ€§ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_49">ğŸ“„ Abstract</h4>
<p>This extended abstract introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method that restructures retrieval around factual evidence by fine-tuning embeddings with contrastive learning and generating token-level attribution rationales for each retrieved passage. Hard negatives are automatically selected using a subjectivity-based criterion, forcing the model to pull factual rationales closer while pushing subjective or misleading explanations apart. As a result, the method creates an embedding space explicitly aligned with evidential reasoning. We evaluated our method on clinical trial reports, and initial experimental results show that CER improves retrieval accuracy, mitigates the potential for hallucinations in RAG systems, and provides transparent, evidence-based retrieval that enhances reliability, especially in safety-critical domains.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="51-bitagent-a-task-aware-modular-framework-for-bidirectional-coupling-between-multimodal-large-language-models-and-world-models">[51] <a href="https://arxiv.org/abs/2512.04513">BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models</a></h3>
<p><em>Yu-Wei Zhan, Xin Wang, Pengzhe Mao, Tongtong Feng, Ren Wang, Wenwu Zhu</em></p>
<h4 id="tldr_50">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºBiTAgentï¼Œä¸€ç§ä»»åŠ¡æ„ŸçŸ¥çš„åŠ¨æ€è”åˆæ¡†æ¶ï¼Œé€šè¿‡åŒå‘è€¦åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸ä¸–ç•Œæ¨¡å‹æ¥è§£å†³å¼€æ”¾ä¸–ç•Œå…·èº«æ™ºèƒ½ä¸­çš„è¯­ä¹‰æ„å›¾ä¸åŠ¨æ€çŠ¶æ€è¡¨ç¤ºå¯¹é½é—®é¢˜ï¼Œå®ç°äº†è·¨ä»»åŠ¡å’Œè·¨ç¯å¢ƒçš„ç¨³å®šæ³›åŒ–ã€‚</p>
<hr />
<h4 id="detailed-summary_50">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ„å»ºé€šç”¨å…·èº«æ™ºèƒ½ä½“éœ€è¦ç»Ÿä¸€ç³»ç»Ÿæ¥è§£è¯»å¤šæ¨¡æ€ç›®æ ‡ã€å»ºæ¨¡ç¯å¢ƒåŠ¨æ€å¹¶æ‰§è¡Œå¯é åŠ¨ä½œï¼Œä½†ç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸ä¸–ç•Œæ¨¡å‹æ—¶é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šå»ºç«‹MLLMè¯­ä¹‰æ„å›¾ä¸WMæ½œåœ¨ç©ºé—´åŠ¨æ€çŠ¶æ€è¡¨ç¤ºä¹‹é—´çš„ç´§å¯†è€¦åˆï¼Œä»¥åŠå®ç°æ”¯æŒå¤šä»»åŠ¡å­¦ä¹ å’Œè·¨ç¯å¢ƒæ³›åŒ–çš„ä»»åŠ¡æ„ŸçŸ¥é€‚åº”æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºBiTAgentæ¡†æ¶ï¼Œå»ºç«‹åŒå‘è€¦åˆæœºåˆ¶ï¼šå‰å‘è·¯å¾„å°†MLLMè¡¨ç¤ºæ³¨å…¥WMæ½œåœ¨ç©ºé—´å®ç°è¯­ä¹‰å¼•å¯¼çš„æƒ³è±¡ï¼Œåå‘è·¯å¾„é€šè¿‡å¯†é›†æ–‡æœ¬æ¡ä»¶å¥–åŠ±åˆ©ç”¨WMç”Ÿæˆåé¦ˆä¼˜åŒ–MLLMè¯­ä¹‰ç©ºé—´ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªååŒç»„ä»¶ï¼šä»»åŠ¡æ„ŸçŸ¥åŠ¨æ€è”åˆå­¦ä¹ ã€ä»»åŠ¡æ„ŸçŸ¥è¡Œä¸ºå­¦ä¹ å’ŒMLLM-WMè”åˆä¼˜åŒ–ï¼Œå…±åŒåè°ƒè¯­ä¹‰æ¨ç†ä¸åŠ¨æ€é¢„æµ‹ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä»»åŠ¡å’Œè·¨ç¯å¢ƒè®¾ç½®ä¸‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒBiTAgentåœ¨ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›åŸºçº¿æ–¹æ³•ï¼Œæ ‡å¿—ç€å‘å¼€æ”¾ä¸–ç•Œå…·èº«å­¦ä¹ è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Conclusion:</strong> BiTAgenté€šè¿‡åŒå‘è€¦åˆæœºåˆ¶æˆåŠŸè§£å†³äº†MLLMä¸WMé›†æˆä¸­çš„è¯­ä¹‰-åŠ¨æ€å¯¹é½é—®é¢˜ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œå…·èº«æ™ºèƒ½æä¾›äº†å¯æ‰©å±•çš„æ¡†æ¶ï¼Œå…¶ä»»åŠ¡æ„ŸçŸ¥è®¾è®¡æ”¯æŒå¤šä»»åŠ¡å­¦ä¹ å’Œè·¨ç¯å¢ƒæ³›åŒ–ï¼Œä¸ºæ„å»ºæ›´é€šç”¨çš„å…·èº«æ™ºèƒ½ä½“æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_50">ğŸ“„ Abstract</h4>
<p>Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.</p>
<h3 id="52-slidegen-collaborative-multimodal-agents-for-scientific-slide-generation">[52] <a href="https://arxiv.org/abs/2512.04529">SlideGen: Collaborative Multimodal Agents for Scientific Slide Generation</a></h3>
<p><em>Xin Liang, Xiang Zhang, Yiwei Xu, Siqi Sun, Chenyu You</em></p>
<h4 id="tldr_51">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SlideGenï¼Œä¸€ç§ç”¨äºä»ç§‘å­¦è®ºæ–‡ç”Ÿæˆå­¦æœ¯å¹»ç¯ç‰‡çš„æ™ºèƒ½æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰è¯­è¨€ä»£ç†çš„åä½œæ¨ç†ï¼Œå®ç°äº†å…¼å…·é€»è¾‘æµç¨‹å’Œè§†è§‰å¸å¼•åŠ›çš„å¯ç¼–è¾‘PPTXå¹»ç¯ç‰‡ç”Ÿæˆï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_51">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–¹æ³•ä¸»è¦å°†å­¦æœ¯å¹»ç¯ç‰‡ç”Ÿæˆç®€åŒ–ä¸ºçº¯æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ï¼Œå¿½è§†äº†è§†è§‰ç»„ä»¶å’Œè®¾è®¡å¯†é›†å‹ç‰¹æ€§ï¼Œå¯¼è‡´æ— æ³•æ»¡è¶³ç§‘å­¦è®ºæ–‡åˆ°å¹»ç¯ç‰‡è½¬æ¢çš„å¤šæ¨¡æ€æ¨ç†éœ€æ±‚ï¼Œè¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> SlideGené‡‡ç”¨æ™ºèƒ½ã€æ¨¡å—åŒ–ä¸”è§†è§‰åœ¨ç¯çš„æ¡†æ¶ï¼Œé€šè¿‡åè°ƒå¤šä¸ªè§†è§‰è¯­è¨€ä»£ç†åä½œæ¨ç†æ–‡æ¡£ç»“æ„å’Œè¯­ä¹‰ï¼Œé›†æˆå¤§çº²è§„åˆ’ã€å†…å®¹æ˜ å°„ã€å¸ƒå±€å®‰æ’ã€ç¬”è®°åˆæˆå’Œè¿­ä»£ä¼˜åŒ–ç­‰æ¨¡å—ï¼Œç”Ÿæˆå¯ç¼–è¾‘çš„PPTXæ ¼å¼å¹»ç¯ç‰‡ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šæ ·åŒ–çš„åŸºå‡†æµ‹è¯•å’Œå¼ºåŸºçº¿å¯¹æ¯”ä¸­ï¼ŒSlideGenåœ¨è§†è§‰è´¨é‡ã€å†…å®¹å¿ å®åº¦å’Œå¯è¯»æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤ŸæŒç»­ç”Ÿæˆä¸“å®¶çº§è´¨é‡çš„å¹»ç¯ç‰‡ï¼Œç¡®ç«‹äº†è‡ªåŠ¨å¹»ç¯ç‰‡ç”Ÿæˆé¢†åŸŸçš„æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè®¾è®¡æ„ŸçŸ¥çš„å¤šæ¨¡æ€å¹»ç¯ç‰‡ç”Ÿæˆå¥ å®šäº†åŸºç¡€ï¼Œå±•ç¤ºäº†æ™ºèƒ½ä»£ç†åä½œå¦‚ä½•å¼¥åˆå¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­çš„ç†è§£ä¸å‘ˆç°ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºè‡ªåŠ¨åŒ–å­¦æœ¯æ¼”ç¤ºåˆ›å»ºæä¾›äº†æ–°çš„èŒƒå¼ã€‚</p>
<hr />
<h4 id="abstract_51">ğŸ“„ Abstract</h4>
<p>Generating academic slides from scientific papers is a challenging multimodal reasoning task that requires both long context understanding and deliberate visual planning. Existing approaches largely reduce it to text only summarization, overlooking the visual component and design intensive nature of slide creation. In this paper we introduce SlideGen, an agentic, modular, and visual in the loop framework for scientific paper to slide generation. SlideGen orchestrates a group of vision language agents that reason collaboratively over the document structure and semantics, producing editable PPTX slides with logical flow and compelling visual presentation. By integrating coordinated outlining, mapping, arrangement, note synthesis, and iterative refinement, our system consistently delivers slides of expert level quality. Across diverse benchmarks and strong baselines, SlideGen outperforms existing methods in visual quality, content faithfulness, and readability, positioning it as the new state of the art in automated slide generation. Our work establishes a foundation for design aware multimodal slide generation, demonstrating how agentic collaboration can bridge understanding and presentation in complex multimodal reasoning tasks.</p>
<h3 id="53-are-llms-truly-multilingual-exploring-zero-shot-multilingual-capability-of-llms-for-information-retrieval-an-italian-healthcare-use-case">[53] <a href="https://arxiv.org/abs/2512.04834">Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case</a></h3>
<p><em>Vignesh Kumar Kembu, Pierandrea Morandini, Marta Bianca Maria Ranzini, Antonino Nocera</em></p>
<h4 id="tldr_52">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶è¯„ä¼°äº†å¼€æºå¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹åœ¨æ„å¤§åˆ©è¯­ç”µå­å¥åº·è®°å½•å®æ—¶ä¿¡æ¯æå–ä¸­çš„èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨å…±ç—…æå–ä»»åŠ¡ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€æœ¬åœ°éƒ¨ç½²è®¾ç½®ä¸‹çš„æ€§èƒ½å±€é™æ€§å’Œæ³›åŒ–æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_52">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¸´åºŠè®°å½•ä¿¡æ¯æå–æ˜¯æ•°å­—åŒ»ç–—ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œä¼ ç»ŸNLPæŠ€æœ¯å› ä¸´åºŠè¯­è¨€çš„å¤æ‚æ€§ã€å˜å¼‚æ€§å’Œé«˜åº¦è¯­ä¹‰å†…æ¶µè€Œè¡¨ç°ä¸è¶³ï¼Œéœ€è¦æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹åœ¨ç†è§£æ„å¤§åˆ©è¯­ç”µå­å¥åº·è®°å½•å¹¶è¿›è¡Œå®æ—¶ä¿¡æ¯æå–çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨å¼€æºå¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨é›¶æ ·æœ¬å’Œæœ¬åœ°éƒ¨ç½²è®¾ç½®ä¸‹è¯„ä¼°å…¶å¯¹æ„å¤§åˆ©è¯­ç”µå­å¥åº·è®°å½•çš„ç†è§£èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨å…±ç—…æå–ä»»åŠ¡ï¼Œå¹¶ä¸åŸç”Ÿæ¨¡å¼åŒ¹é…æ–¹æ³•å’Œäººå·¥æ ‡æ³¨è¿›è¡Œå¯¹æ¯”åˆ†æã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œéƒ¨åˆ†å¤§è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€æœ¬åœ°éƒ¨ç½²è®¾ç½®ä¸‹è¡¨ç°ä¸ä½³ï¼Œä¸åŒæ¨¡å‹æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”åœ¨å¤šç§ç–¾ç—…é—´çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œä¸åŸç”Ÿæ¨¡å¼åŒ¹é…å’Œäººå·¥æ ‡æ³¨ç›¸æ¯”å­˜åœ¨æ˜æ˜¾å·®è·ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†å¼€æºå¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠä¿¡æ¯æå–ä»»åŠ¡ä¸­çš„å®é™…å±€é™æ€§ï¼Œå¼ºè°ƒäº†åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨ä¸­éœ€è¦è€ƒè™‘éƒ¨ç½²ç¯å¢ƒã€è¯­è¨€ç‰¹å¼‚æ€§å’Œç–¾ç—…æ³›åŒ–èƒ½åŠ›ç­‰å› ç´ ï¼Œä¸ºæœªæ¥åŒ»ç–—NLPç³»ç»Ÿå¼€å‘æä¾›äº†é‡è¦å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract_52">ğŸ“„ Abstract</h4>
<p>Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.</p>
<h3 id="54-neural-decoding-of-overt-speech-from-ecog-using-vision-transformers-and-contrastive-representation-learning">[54] <a href="https://arxiv.org/abs/2512.04618">Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning</a></h3>
<p><em>Mohamed Baha Ben Ticha, Xingchen Ran, Guillaume Saldanha, GaÃ«l Le Godais, PhilÃ©mon Roussel, Marc Aubert, Amina Fontanell, Thomas Costecalde, Lucas Struber, Serpil Karakas, Shaomin Zhang, Philippe Kahane, Guillaume Charvet, StÃ©phan ChabardÃ¨s, Blaise Yvert</em></p>
<h4 id="tldr_53">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç¼–ç å™¨-è§£ç å™¨æ·±åº¦ç¥ç»æ¶æ„çš„ç¦»çº¿è¯­éŸ³è§£ç ç®¡é“ï¼Œé¦–æ¬¡å°è¯•ä»å®Œå…¨æ¤å…¥å¼æ— çº¿ç¡¬è†œå¤–è®°å½•ç³»ç»Ÿè§£ç è¯­éŸ³ï¼Œä¸ºé•¿æœŸä½¿ç”¨çš„è„‘æœºæ¥å£æä¾›äº†æ–°è§†è§’ã€‚</p>
<hr />
<h4 id="detailed-summary_53">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è¯­éŸ³è„‘æœºæ¥å£é¢ä¸´çš„å…³é”®æŒ‘æˆ˜æ˜¯å®ç°æµå¼è¯­éŸ³é‡å»ºï¼Œç‰¹åˆ«æ˜¯ä»è¡¨é¢ECoGè®°å½•ç›´æ¥å›å½’åˆ°å£°å­¦è¯­éŸ³ã€‚è™½ç„¶æœ€è¿‘ä½¿ç”¨çš®å±‚å†…æ•°æ®å·²å–å¾—è¿›å±•ï¼Œä½†éœ€è¦è¿›ä¸€æ­¥å·¥ä½œä»¥è·å¾—ä¸è¡¨é¢ECoGè®°å½•ç›¸å½“çš„ç»“æœï¼Œå…¶ä¸­ç¥ç»è§£ç å™¨çš„ä¼˜åŒ–å˜å¾—è‡³å…³é‡è¦ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ç§ç¦»çº¿è¯­éŸ³è§£ç ç®¡é“ï¼ŒåŸºäºç¼–ç å™¨-è§£ç å™¨æ·±åº¦ç¥ç»æ¶æ„ï¼Œæ•´åˆäº†è§†è§‰å˜æ¢å™¨å’Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯ï¼Œä»¥å¢å¼ºä»ECoGä¿¡å·ç›´æ¥å›å½’è¯­éŸ³çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼šä¸€ä¸ªæ¥è‡ªç™«ç—«æ‚£è€…çš„ä¸´åºŠç¡¬è†œä¸‹ç”µææ•°æ®ï¼Œå¦ä¸€ä¸ªæ¥è‡ªè¿åŠ¨è„‘æœºæ¥å£è¯•éªŒå‚ä¸è€…ä½¿ç”¨çš„å®Œå…¨æ¤å…¥å¼WIMAGINEç¡¬è†œå¤–ç³»ç»Ÿã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªä¸åŒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸´åºŠç¡¬è†œä¸‹ç”µææ•°æ®å’Œå®Œå…¨æ¤å…¥å¼WIMAGINEç¡¬è†œå¤–ç³»ç»Ÿæ•°æ®ã€‚æ®ä½œè€…æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•ä»å®Œå…¨æ¤å…¥å¼æ— çº¿ç¡¬è†œå¤–è®°å½•ç³»ç»Ÿè§£ç è¯­éŸ³ï¼Œä¸ºé•¿æœŸä½¿ç”¨çš„è„‘æœºæ¥å£æä¾›äº†å¯è¡Œæ€§éªŒè¯ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†ä»å®Œå…¨æ¤å…¥å¼æ— çº¿ç¡¬è†œå¤–ç³»ç»Ÿè§£ç è¯­éŸ³çš„é¦–æ¬¡å°è¯•ï¼Œä¸ºé•¿æœŸè„‘æœºæ¥å£åº”ç”¨æä¾›äº†æœ‰å‰æ™¯çš„è§†è§’ã€‚æ•´åˆè§†è§‰å˜æ¢å™¨å’Œå¯¹æ¯”å­¦ä¹ çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„åœ¨ä¼˜åŒ–ECoGä¿¡å·åˆ°è¯­éŸ³çš„ç›´æ¥å›å½’æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä¸ºæœªæ¥æµå¼è¯­éŸ³é‡å»ºç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_53">ğŸ“„ Abstract</h4>
<p>Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.</p>
<h3 id="55-stella-guiding-large-language-models-for-time-series-forecasting-with-semantic-abstractions">[55] <a href="https://arxiv.org/abs/2512.04871">STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions</a></h3>
<p><em>Junjie Fan, Hongye Zhao, Linduo Wei, Jiayu Rao, Guijia Li, Jiaxin Yuan, Wenqi Xu, Yong Qi</em></p>
<h4 id="tldr_54">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSTELLAæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è¯­ä¹‰æŠ½è±¡æœºåˆ¶å°†æ—¶é—´åºåˆ—åˆ†è§£ä¸ºè¶‹åŠ¿ã€å­£èŠ‚æ€§å’Œæ®‹å·®åˆ†é‡ï¼Œå¹¶ç”Ÿæˆå±‚æ¬¡åŒ–è¯­ä¹‰é”šç‚¹æ¥å¢å¼ºLLMçš„æ—¶é—´åºåˆ—é¢„æµ‹èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_54">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¢å¼ºåŸå§‹åºåˆ—ä¿¡æ¯ï¼Œå¯¼è‡´LLMçš„æ¨ç†èƒ½åŠ›æœªè¢«å……åˆ†åˆ©ç”¨ã€‚ç°æœ‰æç¤ºç­–ç•¥ä¾èµ–é™æ€ç›¸å…³æ€§è€ŒéåŠ¨æ€è¡Œä¸ºçš„ç”Ÿæˆå¼è§£é‡Šï¼Œç¼ºä¹å…³é”®çš„å…¨å±€å’Œå®ä¾‹ç‰¹å®šä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</p>
<p><strong>Method:</strong> STELLAæ¡†æ¶é‡‡ç”¨åŠ¨æ€è¯­ä¹‰æŠ½è±¡æœºåˆ¶ï¼Œå°†è¾“å…¥åºåˆ—è§£è€¦ä¸ºè¶‹åŠ¿ã€å­£èŠ‚æ€§å’Œæ®‹å·®åˆ†é‡ï¼Œå¹¶å°†è¿™äº›åˆ†é‡çš„å†…åœ¨è¡Œä¸ºç‰¹å¾è½¬åŒ–ä¸ºå±‚æ¬¡åŒ–è¯­ä¹‰é”šç‚¹ï¼šç”¨äºå…¨å±€ä¸Šä¸‹æ–‡çš„è¯­æ–™çº§è¯­ä¹‰å…ˆéªŒå’Œç”¨äºå®ä¾‹çº§æ¨¡å¼çš„ç»†ç²’åº¦è¡Œä¸ºæç¤ºã€‚è¿™äº›é”šç‚¹ä½œä¸ºå‰ç¼€æç¤ºå¼•å¯¼LLMå»ºæ¨¡å†…åœ¨åŠ¨æ€ã€‚</p>
<p><strong>Result:</strong> åœ¨å…«ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSTELLAåœ¨é•¿æœŸå’ŒçŸ­æœŸé¢„æµ‹ä»»åŠ¡ä¸­å‡ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­å±•ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†åŠ¨æ€ç”Ÿæˆè¯­ä¹‰é”šç‚¹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> STELLAé€šè¿‡ç³»ç»Ÿæ€§åœ°æŒ–æ˜å’Œæ³¨å…¥ç»“æ„åŒ–è¡¥å……ä¿¡æ¯ï¼Œæœ‰æ•ˆè§£å†³äº†LLMåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­ä¿¡æ¯å¢å¼ºä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶çš„åŠ¨æ€è¯­ä¹‰æŠ½è±¡æœºåˆ¶ä¸ºLLMæä¾›äº†å…³é”®çš„å…¨å±€å’Œå®ä¾‹ç‰¹å®šä¸Šä¸‹æ–‡ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹æ€§èƒ½ï¼Œä¸ºæ—¶é—´åºåˆ—åˆ†æä¸LLMçš„èåˆæä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_54">ğŸ“„ Abstract</h4>
<p>Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.</p>
<h3 id="56-astride-a-security-threat-modeling-platform-for-agentic-ai-applications">[56] <a href="https://arxiv.org/abs/2512.04785">ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications</a></h3>
<p><em>Eranga Bandara, Amin Hass, Ross Gore, Sachin Shetty, Ravi Mukkamala, Safdar H. Bouk, Xueping Liang, Ng Wee Keong, Kasun De Zoysa, Aruna Withanage, Nilaan Loganathan</em></p>
<h4 id="tldr_55">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ASTRIDEï¼Œä¸€ä¸ªä¸“ä¸ºåŸºäºAIä»£ç†çš„ç³»ç»Ÿè®¾è®¡çš„è‡ªåŠ¨åŒ–å¨èƒå»ºæ¨¡å¹³å°ï¼Œé€šè¿‡æ‰©å±•ç»å…¸STRIDEæ¡†æ¶å¹¶å¼•å…¥æ–°çš„AIä»£ç†ç‰¹å®šæ”»å‡»ç±»åˆ«ï¼Œç»“åˆå¾®è°ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œæ¨ç†LLMï¼Œå®ç°äº†ä»æ¶æ„å›¾åˆ°å¨èƒåˆ†æçš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–ã€‚</p>
<hr />
<h4 id="detailed-summary_55">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŸºäºAIä»£ç†çš„ç³»ç»Ÿåœ¨ç°ä»£è½¯ä»¶æ¶æ„ä¸­æ—¥ç›Šé‡è¦ï¼Œä½†å¼•å…¥äº†ä¼ ç»Ÿå¨èƒå»ºæ¨¡æ¡†æ¶æ— æ³•æœ‰æ•ˆæ•æ‰çš„æ–°å‹å®‰å…¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æç¤ºæ³¨å…¥æ”»å‡»ã€ä¸Šä¸‹æ–‡æ±¡æŸ“ã€æ¨¡å‹æ“çºµå’Œä¸é€æ˜çš„ä»£ç†é—´é€šä¿¡ç­‰æ¼æ´ï¼Œéœ€è¦ä¸“é—¨é’ˆå¯¹AIä»£ç†ç³»ç»Ÿçš„å¨èƒå»ºæ¨¡è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> ASTRIDEå¹³å°æ‰©å±•äº†ç»å…¸STRIDEæ¡†æ¶ï¼Œå¼•å…¥äº†æ–°çš„å¨èƒç±»åˆ«"A"ä»£è¡¨AIä»£ç†ç‰¹å®šæ”»å‡»ï¼Œæ¶µç›–æç¤ºæ³¨å…¥ã€ä¸å®‰å…¨å·¥å…·è°ƒç”¨å’Œæ¨ç†é¢ è¦†ç­‰æ–°å…´æ¼æ´ï¼›é‡‡ç”¨å¾®è°ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹è”ç›Ÿä¸OpenAI-gpt-ossæ¨ç†LLMç›¸ç»“åˆï¼Œç›´æ¥ä»è§†è§‰ä»£ç†æ¶æ„å›¾ï¼ˆå¦‚æ•°æ®æµå›¾ï¼‰è¿›è¡Œç«¯åˆ°ç«¯åˆ†æï¼›LLMä»£ç†åè°ƒVLMè”ç›Ÿä¸æ¨ç†LLMä¹‹é—´çš„äº¤äº’ï¼Œå®ç°å¨èƒå»ºæ¨¡è‡ªåŠ¨åŒ–æµç¨‹çš„ç¼–æ’ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒASTRIDEä¸ºä¸‹ä¸€ä»£æ™ºèƒ½ç³»ç»Ÿæä¾›äº†å‡†ç¡®ã€å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„å¨èƒå»ºæ¨¡èƒ½åŠ›ï¼›è¯¥æ¡†æ¶æ˜¯é¦–ä¸ªæ—¢æ‰©å±•STRIDEä»¥æ¶µç›–AIç‰¹å®šå¨èƒï¼Œåˆé›†æˆå¾®è°ƒVLMä¸æ¨ç†LLMä»¥å®Œå…¨è‡ªåŠ¨åŒ–åŸºäºå›¾çš„å¨èƒå»ºæ¨¡çš„ç³»ç»Ÿã€‚</p>
<p><strong>Conclusion:</strong> ASTRIDEå¡«è¡¥äº†AIä»£ç†ç³»ç»Ÿå®‰å…¨è¯„ä¼°çš„å…³é”®ç©ºç™½ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–å¨èƒå»ºæ¨¡æ¡†æ¶ä¸ºæ™ºèƒ½ç³»ç»Ÿçš„å®‰å…¨è®¾è®¡æä¾›äº†ç³»ç»ŸåŒ–æ–¹æ³•ï¼›è¯¥ç ”ç©¶ä¸ºåº”å¯¹æ–°å…´AIå®‰å…¨æŒ‘æˆ˜æä¾›äº†å®ç”¨å·¥å…·ï¼Œå¹¶ä¸ºæœªæ¥AIå®‰å…¨ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œå¼ºè°ƒäº†å°†ä¼ ç»Ÿå®‰å…¨æ¡†æ¶é€‚åº”AIç‰¹å®šå¨èƒçš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_55">ğŸ“„ Abstract</h4>
<p>AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.</p>
<h3 id="57-chameleon-adaptive-adversarial-agents-for-scaling-based-visual-prompt-injection-in-multimodal-ai-systems">[57] <a href="https://arxiv.org/abs/2512.04895">Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems</a></h3>
<p><em>M Zeeshan, Saud Satti</em></p>
<h4 id="tldr_56">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºChameleonï¼Œä¸€ç§æ–°é¢–çš„è‡ªé€‚åº”å¯¹æŠ—æ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹ç”Ÿäº§çº§è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å›¾åƒç¼©æ”¾é¢„å¤„ç†æ¼æ´ï¼Œé€šè¿‡åŠ¨æ€ä¼˜åŒ–æœºåˆ¶ç”Ÿæˆèƒ½å¤Ÿå­˜æ´»äºæ ‡å‡†ä¸‹é‡‡æ ·æ“ä½œå¹¶åŠ«æŒä¸‹æ¸¸æ‰§è¡Œçš„å¯¹æŠ—æ ·æœ¬ã€‚</p>
<hr />
<h4 id="detailed-summary_56">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€AIç³»ç»Ÿï¼ˆç‰¹åˆ«æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼‰ä¸¥é‡ä¾èµ–é¢„å¤„ç†æµæ°´çº¿è¿›è¡Œé«˜æ•ˆè¾“å…¥å¤„ç†ï¼Œä½†æ ‡å‡†é¢„å¤„ç†æ“ä½œï¼ˆå¦‚å›¾åƒä¸‹é‡‡æ ·ï¼‰å­˜åœ¨è¢«å¿½è§†çš„å®‰å…¨æ¼æ´ã€‚ç°æœ‰å¯¹æŠ—ç­–ç•¥å¤šä¸ºé™æ€æ”»å‡»ï¼Œæ— æ³•é€‚åº”ç°ä»£æ™ºèƒ½ä½“å·¥ä½œæµçš„åŠ¨æ€ç‰¹æ€§ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤ŸåŠ¨æ€é€‚åº”å¹¶åˆ©ç”¨ç¼©æ”¾æ¼æ´çš„æ–°å‹å¯¹æŠ—æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> Chameleoné‡‡ç”¨åŸºäºæ™ºèƒ½ä½“çš„è¿­ä»£ä¼˜åŒ–æœºåˆ¶ï¼Œæ ¹æ®ç›®æ ‡æ¨¡å‹çš„å®æ—¶åé¦ˆåŠ¨æ€ç»†åŒ–å›¾åƒæ‰°åŠ¨ï¼Œä»è€Œç”Ÿæˆèƒ½å¤Ÿç»å—æ ‡å‡†ä¸‹é‡‡æ ·æ“ä½œçš„é«˜åº¦é²æ£’å¯¹æŠ—æ ·æœ¬ã€‚è¯¥æ¡†æ¶ä¸“é—¨è®¾è®¡ç”¨äºæš´éœ²å’Œåˆ©ç”¨ç”Ÿäº§çº§è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç¼©æ”¾æ¼æ´ï¼Œä¸ä¼ ç»Ÿé™æ€æ”»å‡»æ–¹æ³•å½¢æˆé²œæ˜å¯¹æ¯”ã€‚</p>
<p><strong>Result:</strong> åœ¨é’ˆå¯¹Gemini 2.5 Flashæ¨¡å‹çš„å®éªŒä¸­ï¼ŒChameleonåœ¨ä¸åŒç¼©æ”¾å› å­ä¸‹å®ç°äº†84.5%çš„æ”»å‡»æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºå¹³å‡ä»…32.1%çš„é™æ€åŸºçº¿æ”»å‡»ã€‚æ­¤å¤–ï¼Œè¿™äº›æ”»å‡»èƒ½æœ‰æ•ˆç ´åæ™ºèƒ½ä½“æµæ°´çº¿ï¼Œåœ¨å¤šæ­¥ä»»åŠ¡ä¸­å°†å†³ç­–å‡†ç¡®ç‡é™ä½è¶…è¿‡45%ï¼Œè¯æ˜äº†å…¶å¯¹å®é™…ç”Ÿäº§ç³»ç»Ÿçš„ä¸¥é‡å¨èƒã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹ä¸­é¢„å¤„ç†ç¼©æ”¾æ“ä½œçš„ä¸¥é‡å®‰å…¨æ¼æ´ï¼Œè¡¨æ˜ä¼ ç»Ÿé™æ€å¯¹æŠ—æ”»å‡»å·²ä¸è¶³ä»¥åº”å¯¹ç°ä»£æ™ºèƒ½ä½“ç³»ç»Ÿçš„åŠ¨æ€ç‰¹æ€§ã€‚ç ”ç©¶å»ºè®®é‡‡ç”¨å¤šå°ºåº¦ä¸€è‡´æ€§æ£€æŸ¥ä½œä¸ºå¿…è¦çš„é˜²å¾¡æœºåˆ¶ï¼Œå¹¶å¼ºè°ƒäº†åœ¨ç”Ÿäº§çº§å¤šæ¨¡æ€AIç³»ç»Ÿä¸­åŠ å¼ºé¢„å¤„ç†é˜¶æ®µå®‰å…¨æ€§çš„è¿«åˆ‡éœ€æ±‚ã€‚</p>
<hr />
<h4 id="abstract_56">ğŸ“„ Abstract</h4>
<p>Multimodal Artificial Intelligence (AI) systems, particularly Vision-Language Models (VLMs), have become integral to critical applications ranging from autonomous decision-making to automated document processing. As these systems scale, they rely heavily on preprocessing pipelines to handle diverse inputs efficiently. However, this dependency on standard preprocessing operations, specifically image downscaling, creates a significant yet often overlooked security vulnerability. While intended for computational optimization, scaling algorithms can be exploited to conceal malicious visual prompts that are invisible to human observers but become active semantic instructions once processed by the model. Current adversarial strategies remain largely static, failing to account for the dynamic nature of modern agentic workflows. To address this gap, we propose Chameleon, a novel, adaptive adversarial framework designed to expose and exploit scaling vulnerabilities in production VLMs. Unlike traditional static attacks, Chameleon employs an iterative, agent-based optimization mechanism that dynamically refines image perturbations based on the target model's real-time feedback. This allows the framework to craft highly robust adversarial examples that survive standard downscaling operations to hijack downstream execution. We evaluate Chameleon against Gemini 2.5 Flash model. Our experiments demonstrate that Chameleon achieves an Attack Success Rate (ASR) of 84.5% across varying scaling factors, significantly outperforming static baseline attacks which average only 32.1%. Furthermore, we show that these attacks effectively compromise agentic pipelines, reducing decision-making accuracy by over 45% in multi-step tasks. Finally, we discuss the implications of these vulnerabilities and propose multi-scale consistency checks as a necessary defense mechanism.</p>
  </article>
</body>
</html>
