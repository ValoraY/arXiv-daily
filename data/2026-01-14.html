<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-01-14.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 41]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 10]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 11]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-an-empirical-study-on-knowledge-transfer-under-domain-and-label-shifts-in-3d-lidar-point-clouds">[1] <a href="https://arxiv.org/abs/2601.07855">An Empirical Study on Knowledge Transfer under Domain and Label Shifts in 3D LiDAR Point Clouds</a></h3>
<p><em>Subeen Lee, Siyeong Lee, Namil Kim, Jaesik Choi</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†ROADåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LiDARç‚¹äº‘åˆ†ç±»åœ¨æŒç»­å­¦ä¹ å’Œè¿ç§»å­¦ä¹ åœºæ™¯ä¸‹çš„é²æ£’æ€§ï¼Œç‰¹åˆ«å…³æ³¨åŒæ—¶å‘ç”Ÿçš„é¢†åŸŸåç§»å’Œæ ‡ç­¾æ¼”åŒ–ï¼Œå¡«è¡¥äº†3Dæ„ŸçŸ¥ç³»ç»Ÿåœ¨ç°å®åº”ç”¨ä¸­çš„é€‚åº”æ€§ç ”ç©¶ç©ºç™½ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> 3Dæ„ŸçŸ¥ç³»ç»Ÿåœ¨è‡ªåŠ¨é©¾é©¶å’Œå…·èº«AIç­‰ç°å®åº”ç”¨ä¸­éœ€è¦é€‚åº”ä¸æ–­æ¼”åŒ–çš„ç‰©ä½“å®šä¹‰å’Œä¼ æ„Ÿå™¨é¢†åŸŸï¼Œç„¶è€Œä¸2Dè§†è§‰ç›¸æ¯”ï¼Œ3Dç‚¹äº‘æ„ŸçŸ¥ä¸­çš„æŒç»­å­¦ä¹ å’Œè¿ç§»å­¦ä¹ ç ”ç©¶ä»ç„¶ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒæ—¶é¢ä¸´é¢†åŸŸåç§»å’Œæ ‡ç­¾å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸€ç ”ç©¶ç©ºç™½éœ€è¦è¢«å¡«è¡¥ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ROADåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºLiDARç‚¹äº‘åˆ†ç±»è®¾è®¡çš„ç»¼åˆè¯„ä¼°å¥—ä»¶ï¼Œæ˜ç¡®è€ƒè™‘äº†é¢†åŸŸåç§»ä»¥åŠä¸‰ç§å…³é”®æ ‡ç­¾æ¼”åŒ–å½¢å¼ï¼šç±»åˆ«åˆ†è£‚ã€ç±»åˆ«æ‰©å±•å’Œç±»åˆ«æ’å…¥ã€‚ç ”ç©¶ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆWaymoã€NuScenesã€Argoverse2ï¼‰è¯„ä¼°äº†é›¶æ ·æœ¬è¿ç§»ã€çº¿æ€§æ¢æµ‹å’ŒæŒç»­å­¦ä¹ æ–¹æ³•ï¼Œå¹¶åˆ†æäº†éª¨å¹²ç½‘ç»œæ¶æ„ã€è®­ç»ƒç›®æ ‡å’ŒæŒç»­å­¦ä¹ æ–¹æ³•çš„å½±å“ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶ç»“æœæ­ç¤ºäº†ç°æœ‰æ–¹æ³•åœ¨ç°å®åç§»ä¸‹çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒæ—¶å¤„ç†é¢†åŸŸå’Œæ ‡ç­¾å˜åŒ–æ—¶çš„æ€§èƒ½ä¸è¶³ã€‚é€šè¿‡ç³»ç»Ÿè¯„ä¼°ï¼Œç ”ç©¶å»ºç«‹äº†æœªæ¥é²æ£’3Dæ„ŸçŸ¥ç ”ç©¶çš„å¼ºåŸºçº¿ï¼Œä¸ºä¸åŒæ¶æ„å’Œæ–¹æ³•åœ¨å¤æ‚æ¼”åŒ–åœºæ™¯ä¸‹çš„è¡¨ç°æä¾›äº†é‡åŒ–åˆ†æã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†3Dæ„ŸçŸ¥ç³»ç»Ÿåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­é€‚åº”æŒç»­å˜åŒ–çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰æ–¹æ³•åœ¨å¤„ç†åŒæ—¶å‘ç”Ÿçš„é¢†åŸŸå’Œæ ‡ç­¾åç§»æ—¶çš„ä¸è¶³ã€‚ROADåŸºå‡†æµ‹è¯•ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œæœ‰åŠ©äºæ¨åŠ¨æ›´é²æ£’çš„3Dæ„ŸçŸ¥æ¨¡å‹å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®åº”ç”¨ä¸­ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>For 3D perception systems to be practical in real-world applications -- from autonomous driving to embodied AI -- models must adapt to continuously evolving object definitions and sensor domains. Yet, research on continual and transfer learning in 3D point cloud perception remains underexplored compared to 2D vision -- particularly under simultaneous domain and label shifts. To address this gap, we propose the RObust Autonomous driving under Dataset shifts (ROAD) benchmark, a comprehensive evaluation suite for LiDAR-based object classification that explicitly accounts for domain shifts as well as three key forms of label evolution: class split, class expansion, and class insertion. Using large-scale datasets (Waymo, NuScenes, Argoverse2), we evaluate zero-shot transfer, linear probe, and CL, and analyze the impact of backbone architectures, training objectives, and CL methods. Our findings reveal limitations of existing approaches under realistic shifts and establish strong baselines for future research in robust 3D perception.</p>
<h3 id="2-cashew-stabilizing-multimodal-reasoning-via-iterative-trajectory-aggregation">[2] <a href="https://arxiv.org/abs/2601.08010">CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation</a></h3>
<p><em>Chaoyu Li, Deeparghya Dutta Barua, Fei Tao, Pooyan Fazli</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCASHEWå’ŒCASHEW-RLä¸¤ç§äº’è¡¥æ–¹æ³•ï¼Œé€šè¿‡æ¨ç†æ—¶èšåˆå¤šä¸ªå€™é€‰è½¨è¿¹å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹å¤šæ­¥æ¨ç†çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šç§å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å¤šæ­¥æ¨ç†è¿‡ç¨‹å­˜åœ¨ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œå¯¹ç›¸åŒè¾“å…¥çš„é‡å¤é‡‡æ ·ä¼šäº§ç”Ÿå‘æ•£æ¨ç†è½¨è¿¹å’Œä¸ä¸€è‡´çš„æœ€ç»ˆé¢„æµ‹ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„å¯é æ€§å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºä¸¤ç§äº’è¡¥æ–¹æ³•ï¼šCASHEWæ˜¯ä¸€ä¸ªæ¨ç†æ—¶æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£èšåˆå¤šä¸ªå€™é€‰æ¨ç†è½¨è¿¹å½¢æˆæ›´é«˜è´¨é‡æ¨ç†è·¯å¾„ï¼Œå¹¶åˆ©ç”¨æ˜¾å¼è§†è§‰éªŒè¯è¿‡æ»¤å¹»è§‰æ­¥éª¤ï¼›CASHEW-RLåˆ™é€šè¿‡Group Sequence Policy Optimizationè®­ç»ƒï¼Œä½¿ç”¨å¤åˆå¥–åŠ±å‡½æ•°é¼“åŠ±åŸºäºæœ€å°å……åˆ†è§†è§‰è¯æ®çš„æ­£ç¡®ç­”æ¡ˆï¼Œå¹¶è‡ªé€‚åº”åˆ†é…æ¨ç†è®¡ç®—èµ„æºã€‚</p>
<p><strong>Result:</strong> åœ¨13ä¸ªå›¾åƒç†è§£ã€è§†é¢‘ç†è§£å’Œè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…¶ä¸­ScienceQAä¸Šæå‡è¾¾23.6ä¸ªç™¾åˆ†ç‚¹ï¼ŒEgoSchemaä¸Šæå‡8.1ä¸ªç™¾åˆ†ç‚¹ï¼Œè¯æ˜äº†æ¨ç†ç¨³å®šæ€§å’Œå‡†ç¡®æ€§çš„æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡æ¨ç†æ—¶è½¨è¿¹èšåˆå’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå¯ä»¥æ˜¾è‘—æå‡è§†è§‰è¯­è¨€æ¨¡å‹å¤šæ­¥æ¨ç†çš„ç¨³å®šæ€§å’Œå¯é æ€§ï¼Œä¸ºæ„å»ºæ›´ç¨³å¥çš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆé€”å¾„ï¼ŒåŒæ—¶å±•ç¤ºäº†è‡ªé€‚åº”æ¨ç†è®¡ç®—åˆ†é…çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.</p>
<h3 id="3-representations-of-text-and-images-align-from-layer-one">[3] <a href="https://arxiv.org/abs/2601.08017">Representations of Text and Images Align From Layer One</a></h3>
<p><em>EvÅ¾en Wybitul, Javier Rando, Florian TramÃ¨r, Stanislav Fort</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¼˜åŒ–çš„åˆæˆæ–¹æ³•ï¼Œæ­ç¤ºäº†åŸºäºé€‚é…å™¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å›¾åƒä¸æ–‡æœ¬è¡¨ç¤ºåœ¨æ—©æœŸå±‚å°±å·²å­˜åœ¨æœ‰æ„ä¹‰çš„å¯¹é½ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿè®¤ä¸ºæ­¤ç±»å¯¹é½ä»…å‡ºç°åœ¨æ·±å±‚ç½‘ç»œçš„è§‚ç‚¹ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨æŒ‘æˆ˜ä¼ ç»Ÿè§‚ç‚¹ï¼Œå³åŸºäºé€‚é…å™¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å›¾åƒä¸æ–‡æœ¬çš„å¯¹é½ä»…å‡ºç°åœ¨ç½‘ç»œæ·±å±‚ã€‚ç°æœ‰ç ”ç©¶æ™®éè®¤ä¸ºè·¨æ¨¡æ€å¯¹é½éœ€è¦ç»è¿‡å¤šå±‚å¤„ç†æ‰èƒ½å½¢æˆï¼Œä½†è¯¥ç ”ç©¶è¯•å›¾æ¢ç´¢è¿™ç§å¯¹é½æ˜¯å¦åœ¨æ›´æ—©çš„ç½‘ç»œå±‚ä¸­å°±å·²å­˜åœ¨ï¼Œä»è€Œæ­ç¤ºæ¨¡å‹å†…éƒ¨è¡¨ç¤ºå½¢æˆçš„åŠ¨æ€è¿‡ç¨‹ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ç§å—DeepDreamå¯å‘çš„åˆæˆæ–¹æ³•ï¼šç»™å®šæ–‡æœ¬æ¦‚å¿µå¦‚"æœ¨æ˜Ÿ"ï¼Œåœ¨ç‰¹å®šå±‚æå–å…¶æ¦‚å¿µå‘é‡ï¼Œç„¶åé€šè¿‡ä¼˜åŒ–è¿‡ç¨‹åˆæˆä¸€ä¸ªå›¾åƒï¼Œä½¿å…¶è¡¨ç¤ºä¸è¯¥å‘é‡å¯¹é½ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦è¾…åŠ©æ¨¡å‹æˆ–æ•°æ®é›†ï¼Œç›´æ¥åœ¨Gemma 3æ¨¡å‹çš„ä¸ƒä¸ªå±‚ä¸Šå¯¹æ•°ç™¾ä¸ªæ¦‚å¿µè¿›è¡Œæµ‹è¯•ï¼Œé€šè¿‡å¯è§†åŒ–æ¨¡å‹çš„è¡¨ç¤ºç©ºé—´æ¥éªŒè¯å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨Gemma 3æ¨¡å‹çš„ç¬¬ä¸€å±‚ï¼Œè¶…è¿‡50%çš„åˆæˆå›¾åƒå·²ç»èƒ½å¤Ÿæç»˜ç›®æ ‡æ–‡æœ¬æ¦‚å¿µçš„å¯è¯†åˆ«è§†è§‰ç‰¹å¾ï¼Œå¦‚åŠ¨ç‰©ã€æ´»åŠ¨æˆ–å­£èŠ‚çš„æ˜¾è‘—ç‰¹å¾ã€‚è¿™è¡¨æ˜å›¾åƒä¸æ–‡æœ¬è¡¨ç¤ºçš„å¯¹é½ä»ç½‘ç»œæ—©æœŸå±‚å°±å¼€å§‹å‡ºç°ï¼Œè€Œéä»…é™äºæ·±å±‚ï¼Œä¸ºæ¦‚å¿µå±‚é¢çš„è·¨æ¨¡æ€å¯¹é½æä¾›äº†ç›´æ¥è¯æ®ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æä¾›äº†ç›´æ¥ã€å»ºè®¾æ€§çš„è¯æ®ï¼Œè¡¨æ˜åŸºäºé€‚é…å™¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å›¾åƒ-æ–‡æœ¬å¯¹é½åœ¨æ—©æœŸå±‚å°±å·²å­˜åœ¨ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿç†è§£ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¸ºæ¨¡å‹å¯è§£é‡Šæ€§æä¾›äº†æ–°é€”å¾„ï¼Œé€šè¿‡å¯è§†åŒ–è¡¨ç¤ºç©ºé—´æ¥ç†è§£æ¨¡å‹å†…éƒ¨å·¥ä½œæœºåˆ¶ï¼Œè€Œä¸”æä¾›äº†ä¸€ç§ç®€å•ã€å¿«é€Ÿä¸”æ— éœ€å¤–éƒ¨èµ„æºçš„è·¨æ¨¡æ€å¯¹é½è¯„ä¼°æ–¹æ³•ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>We show that for a variety of concepts in adapter-based vision-language models, the representations of their images and their text descriptions are meaningfully aligned from the very first layer. This contradicts the established view that such image-text alignment only appears in late layers. We show this using a new synthesis-based method inspired by DeepDream: given a textual concept such as "Jupiter", we extract its concept vector at a given layer, and then use optimisation to synthesise an image whose representation aligns with that vector. We apply our approach to hundreds of concepts across seven layers in Gemma 3, and find that the synthesised images often depict salient visual features of the targeted textual concepts: for example, already at layer 1, more than 50 % of images depict recognisable features of animals, activities, or seasons. Our method thus provides direct, constructive evidence of image-text alignment on a concept-by-concept and layer-by-layer basis. Unlike previous methods for measuring multimodal alignment, our approach is simple, fast, and does not require auxiliary models or datasets. It also offers a new path towards model interpretability, by providing a way to visualise a model's representation space by backtracing through its image processing components.</p>
<h3 id="4-training-free-zero-shot-visual-anomaly-localization-via-diffusion-inversion">[4] <a href="https://arxiv.org/abs/2601.08022">Training Free Zero-Shot Visual Anomaly Localization via Diffusion Inversion</a></h3>
<p><em>Samet Hicsonmez, Abd El Rahman Shabayek, Djamila Aouada</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„è§†è§‰é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ¡†æ¶DIVADï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒå»å™ªæ‰©æ•£éšå¼æ¨¡å‹çš„åè½¬è¿‡ç¨‹ï¼Œåœ¨æ— éœ€ç»†ç²’åº¦æç¤ºçš„æƒ…å†µä¸‹å®ç°å¼‚å¸¸æ£€æµ‹ä¸å®šä½ï¼Œåœ¨VISAæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰é›¶æ ·æœ¬å›¾åƒå¼‚å¸¸æ£€æµ‹æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šåŸºäºè¯­è¨€çš„æ–¹æ³•éœ€è¦ä¾èµ–ç»†ç²’åº¦æç¤ºæ¥å®ç°å®šä½ï¼Œè€Œçº¯è§†è§‰æ–¹æ³•é€šå¸¸ä»…é™äºå›¾åƒçº§åˆ†ç±»ï¼Œç¼ºä¹ç©ºé—´å®šä½ç²¾åº¦ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§æ— éœ€è®­ç»ƒã€ä¸ä¾èµ–è¾…åŠ©æ¨¡æ€çš„è§†è§‰é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œä»¥å…‹æœå¯¹æç¤ºçš„ä¾èµ–å¹¶æé«˜å®šä½èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒå»å™ªæ‰©æ•£éšå¼æ¨¡å‹çš„åè½¬æ¡†æ¶DIVADã€‚å…·ä½“è€Œè¨€ï¼Œç»™å®šè¾“å…¥å›¾åƒå’Œé€šç”¨æ–‡æœ¬æè¿°ï¼Œé¦–å…ˆå°†å›¾åƒåè½¬åˆ°æ½œåœ¨ç©ºé—´è·å¾—æ½œåœ¨è¡¨ç¤ºï¼Œç„¶åä»å›ºå®šçš„ä¸­é—´æ—¶é—´æ­¥å¼€å§‹å»å™ªè¿‡ç¨‹ä»¥é‡å»ºå›¾åƒã€‚ç”±äºåº•å±‚æ‰©æ•£æ¨¡å‹ä»…åœ¨æ­£å¸¸æ•°æ®ä¸Šè®­ç»ƒï¼Œè¯¥è¿‡ç¨‹ä¼šäº§ç”Ÿæ­£å¸¸å¤–è§‚çš„é‡å»ºï¼Œè¾“å…¥å›¾åƒä¸é‡å»ºå›¾åƒä¹‹é—´çš„å·®å¼‚åˆ™çªå‡ºäº†æ½œåœ¨å¼‚å¸¸åŒºåŸŸã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨VISAæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„å¼‚å¸¸å®šä½èƒ½åŠ›ï¼Œæ— éœ€ä»»ä½•è¾…åŠ©æ¨¡æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹å’Œå®šä½å¼‚å¸¸ï¼ŒåŒæ—¶é¿å…äº†ä¼ ç»Ÿæ–¹æ³•å¯¹ç»†ç²’åº¦æç¤ºçš„ä¾èµ–ï¼Œä¸ºçº¯è§†è§‰é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æä¾›äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶è¯æ˜äº†åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åè½¬è¿‡ç¨‹å¯ä»¥åœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆçš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä¸å®šä½ï¼Œä¸ºå‡å°‘å¯¹æç¤ºä¾èµ–çš„å¼‚å¸¸æ£€æµ‹ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†çº¯è§†è§‰æ–¹æ³•çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ç ”ç©¶æä¾›äº†ç®€å•è€Œæœ‰æ•ˆçš„æ¡†æ¶ï¼Œä¿ƒè¿›äº†è¯¥é¢†åŸŸä»æç¤ºä¾èµ–å‘æ›´é€šç”¨æ–¹æ³•çš„è½¬å˜ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Zero-Shot image Anomaly Detection (ZSAD) aims to detect and localise anomalies without access to any normal training samples of the target data. While recent ZSAD approaches leverage additional modalities such as language to generate fine-grained prompts for localisation, vision-only methods remain limited to image-level classification, lacking spatial precision. In this work, we introduce a simple yet effective training-free vision-only ZSAD framework that circumvents the need for fine-grained prompts by leveraging the inversion of a pretrained Denoising Diffusion Implicit Model (DDIM). Specifically, given an input image and a generic text description (e.g., "an image of an [object class]"), we invert the image to obtain latent representations and initiate the denoising process from a fixed intermediate timestep to reconstruct the image. Since the underlying diffusion model is trained solely on normal data, this process yields a normal-looking reconstruction. The discrepancy between the input image and the reconstructed one highlights potential anomalies. Our method achieves state-of-the-art performance on VISA dataset, demonstrating strong localisation capabilities without auxiliary modalities and facilitating a shift away from prompt dependence for zero-shot anomaly detection research. Code is available at https://github.com/giddyyupp/DIVAD.</p>
<h3 id="5-a-highly-efficient-diversity-based-input-selection-for-dnn-improvement-using-vlms">[5] <a href="https://arxiv.org/abs/2601.08024">A Highly Efficient Diversity-based Input Selection for DNN Improvement Using VLMs</a></h3>
<p><em>Amin Abbasishahkoo, Mahboubeh Dadkhah, Lionel Briand</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¦‚å¿µå¤šæ ·æ€§çš„é«˜æ•ˆå›¾åƒè¾“å…¥é€‰æ‹©æ–¹æ³•CBDï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è®¡ç®—å¤šæ ·æ€§åº¦é‡ï¼Œå¹¶ç»“åˆä¸ç¡®å®šæ€§åº¦é‡æ„å»ºæ··åˆé€‰æ‹©ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ·±åº¦ç¥ç»ç½‘ç»œå¾®è°ƒä¸­æ ‡æ³¨æ ·æœ¬é€‰æ‹©çš„æ•ˆç‡å’Œæ•ˆæœã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ·±åº¦ç¥ç»ç½‘ç»œå¾®è°ƒéœ€è¦æ ‡æ³¨æ–°æ”¶é›†çš„è¾“å…¥æ•°æ®ï¼Œè¿™ä¸€è¿‡ç¨‹é€šå¸¸æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚ç°æœ‰çš„å¤šæ ·æ€§é€‰æ‹©æ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†è®¡ç®—å¯†é›†ä¸”ç¼ºä¹å¯æ‰©å±•æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡è¾“å…¥é›†ä¸Šçš„å®é™…åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†æ¦‚å¿µå¤šæ ·æ€§åº¦é‡æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹é«˜æ•ˆè®¡ç®—å›¾åƒè¾“å…¥çš„å¤šæ ·æ€§ç‰¹å¾ã€‚åŸºäºCBDä¸å‡ ä½•å¤šæ ·æ€§ä¹‹é—´çš„å¼ºç›¸å…³æ€§å‘ç°ï¼Œæ„å»ºäº†CBDä¸ç®€å•ä¸ç¡®å®šæ€§åº¦é‡Marginç›¸ç»“åˆçš„æ··åˆè¾“å…¥é€‰æ‹©æ–¹æ³•ï¼Œå®ç°äº†æ•ˆç‡ä¸æ•ˆæœçš„å¹³è¡¡ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒCBDä¸å‡ ä½•å¤šæ ·æ€§åº¦é‡å‘ˆç°å¼ºç›¸å…³æ€§ï¼ŒåŒæ—¶è®¡ç®—æ—¶é—´å¤§å¹…å‡å°‘ã€‚åœ¨å¤šç§DNNæ¨¡å‹ã€è¾“å…¥é›†å’Œé€‰æ‹©é¢„ç®—ä¸‹ï¼ŒCBD-basedé€‰æ‹©æ–¹æ³•åœ¨äº”ä¸ªæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ä¸­è¡¨ç°æœ€ä¼˜ï¼Œä¸”é€‰æ‹©æ—¶é—´æ¥è¿‘ç®€å•ä¸ç¡®å®šæ€§æ–¹æ³•ï¼Œåœ¨ImageNetç­‰å¤§è§„æ¨¡æ•°æ®é›†ä¸Šä»ä¿æŒé«˜æ•ˆã€‚</p>
<p><strong>Conclusion:</strong> CBD-basedæ–¹æ³•ä¸ä»…è¯æ˜äº†å…¶ç›¸å¯¹äºæ··åˆåŸºçº¿çš„æœ‰æ•ˆæ€§å’Œè®¡ç®—ä¼˜åŠ¿ï¼Œè¿˜å±•ç¤ºäº†åœ¨é‡å¤å’Œå¤§è§„æ¨¡è¾“å…¥é€‰æ‹©åœºæ™¯ä¸­çš„å¯æ‰©å±•æ€§ã€‚è¯¥æ–¹æ³•ä¸ºé«˜æ•ˆæ ‡æ³¨æ ·æœ¬é€‰æ‹©æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå¹³è¡¡äº†è®¡ç®—æ•ˆç‡ä¸é€‰æ‹©è´¨é‡ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Maintaining or improving the performance of Deep Neural Networks (DNNs) through fine-tuning requires labeling newly collected inputs, a process that is often costly and time-consuming. To alleviate this problem, input selection approaches have been developed in recent years to identify small, yet highly informative subsets for labeling. Diversity-based selection is one of the most effective approaches for this purpose. However, they are often computationally intensive and lack scalability for large input sets, limiting their practical applicability. To address this challenge, we introduce Concept-Based Diversity (CBD), a highly efficient metric for image inputs that leverages Vision-Language Models (VLM). Our results show that CBD exhibits a strong correlation with Geometric Diversity (GD), an established diversity metric, while requiring only a fraction of its computation time. Building on this finding, we propose a hybrid input selection approach that combines CBD with Margin, a simple uncertainty metric. We conduct a comprehensive evaluation across a diverse set of DNN models, input sets, selection budgets, and five most effective state-of-the-art selection baselines. The results demonstrate that the CBD-based selection consistently outperforms all baselines at guiding input selection to improve the DNN model. Furthermore, the CBD-based selection approach remains highly efficient, requiring selection times close to those of simple uncertainty-based methods such as Margin, even on larger input sets like ImageNet. These results confirm not only the effectiveness and computational advantage of the CBD-based approach, particularly compared to hybrid baselines, but also its scalability in repetitive and extensive input selection scenarios.</p>
<h3 id="6-figex2-visual-conditioned-panel-detection-and-captioning-for-scientific-compound-figures">[6] <a href="https://arxiv.org/abs/2601.08026">FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures</a></h3>
<p><em>Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºFigEx2ï¼Œä¸€ç§è§†è§‰æ¡ä»¶åŒ–æ¡†æ¶ï¼Œç”¨äºä»ç§‘å­¦å¤åˆå›¾ä¸­å®šä½é¢æ¿å¹¶ç”Ÿæˆé¢æ¿çº§æè¿°ï¼Œé€šè¿‡å™ªå£°æ„ŸçŸ¥é—¨æ§èåˆæ¨¡å—å’Œä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥æ˜¾è‘—æå‡äº†æ£€æµ‹å’Œæè¿°ç”Ÿæˆæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç§‘å­¦å¤åˆå›¾å°†å¤šä¸ªå¸¦æ ‡ç­¾é¢æ¿ç»„åˆæˆå•ä¸€å›¾åƒï¼Œä½†å®é™…æµç¨‹ä¸­çš„å›¾æ³¨ç»å¸¸ç¼ºå¤±æˆ–ä»…æä¾›å›¾çº§æ‘˜è¦ï¼Œè¿™ä½¿å¾—é¢æ¿çº§ç†è§£å˜å¾—å›°éš¾ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿç›´æ¥ä»å¤åˆå›¾ä¸­å®šä½é¢æ¿å¹¶ç”Ÿæˆé¢æ¿çº§æè¿°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> FigEx2é‡‡ç”¨è§†è§‰æ¡ä»¶åŒ–æ¡†æ¶ï¼ŒåŒ…å«å™ªå£°æ„ŸçŸ¥é—¨æ§èåˆæ¨¡å—ä»¥è‡ªé€‚åº”è¿‡æ»¤æ ‡è®°çº§ç‰¹å¾æ¥ç¨³å®šæ£€æµ‹æŸ¥è¯¢ç©ºé—´ï¼Œå¹¶é‡‡ç”¨ç»“åˆç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼Œåˆ©ç”¨åŸºäºCLIPçš„å¯¹é½å’ŒåŸºäºBERTScoreçš„è¯­ä¹‰å¥–åŠ±æ¥å¼ºåˆ¶ä¸¥æ ¼çš„å¤šæ¨¡æ€ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºFigEx2åœ¨æ£€æµ‹æ–¹é¢è¾¾åˆ°0.726 mAP@0.5:0.95çš„ä¼˜å¼‚æ€§èƒ½ï¼Œåœ¨METEORå’ŒBERTScoreæŒ‡æ ‡ä¸Šåˆ†åˆ«æ˜¾è‘—è¶…è¶ŠQwen3-VL-8Bæ¨¡å‹0.51å’Œ0.24åˆ†ï¼Œå¹¶åœ¨æœªç»å¾®è°ƒçš„æƒ…å†µä¸‹å±•ç°å‡ºå¯¹åˆ†å¸ƒå¤–ç§‘å­¦é¢†åŸŸçš„å“è¶Šé›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡åˆ›æ–°çš„å™ªå£°æ„ŸçŸ¥èåˆæœºåˆ¶å’Œä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³äº†ç§‘å­¦å¤åˆå›¾çš„é¢æ¿çº§ç†è§£é—®é¢˜ï¼Œæ„å»ºçš„BioSci-Fig-CapåŸºå‡†å’Œè·¨å­¦ç§‘æµ‹è¯•å¥—ä»¶ä¸ºåç»­ç ”ç©¶æä¾›äº†é«˜è´¨é‡ç›‘ç£æ•°æ®ï¼Œå±•ç¤ºäº†åœ¨ç§‘å­¦è§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.</p>
<h3 id="7-rescind-countering-image-misconduct-in-biomedical-publications-with-vision-language-and-state-space-modeling">[7] <a href="https://arxiv.org/abs/2601.08040">Rescind: Countering Image Misconduct in Biomedical Publications with Vision-Language and State-Space Modeling</a></h3>
<p><em>Soumyaroop Nandi, Prem Natarajan</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†é¦–ä¸ªè§†è§‰è¯­è¨€å¼•å¯¼çš„ç”Ÿç‰©åŒ»å­¦å›¾åƒä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ‰©æ•£åˆæˆä¸è§†è§‰è¯­è¨€æç¤ºï¼Œå®ç°äº†å¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­å¤åˆ¶ã€æ‹¼æ¥å’ŒåŒºåŸŸç§»é™¤ç­‰æ“ä½œçš„é€¼çœŸä¸”è¯­ä¹‰å¯æ§çš„ä¼ªé€ ï¼Œå¹¶å»ºç«‹äº†å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†Rescindå’Œæ£€æµ‹æ¡†æ¶Integscanã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç”Ÿç‰©åŒ»å­¦å‡ºç‰ˆç‰©ä¸­çš„ç§‘å­¦å›¾åƒç¯¡æ”¹å¯¹ç ”ç©¶å®Œæ•´æ€§å’Œå¯é‡å¤æ€§æ„æˆæ—¥ç›Šä¸¥é‡çš„å¨èƒï¼Œä¸è‡ªç„¶å›¾åƒå–è¯ä¸åŒï¼Œç”Ÿç‰©åŒ»å­¦ä¼ªé€ æ£€æµ‹é¢ä¸´é¢†åŸŸç‰¹å®šä¼ªå½±ã€å¤æ‚çº¹ç†å’Œéç»“æ„åŒ–å›¾åƒå¸ƒå±€ç­‰ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåº”å¯¹ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§ç»“åˆæ‰©æ•£åˆæˆä¸è§†è§‰è¯­è¨€æç¤ºçš„ç”Ÿæˆæ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹éªŒè¯å¾ªç¯ç¡®ä¿è¯­ä¹‰ä¿çœŸåº¦ï¼›å»ºç«‹äº†å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†Rescindï¼ŒåŒ…å«ç»†ç²’åº¦æ ‡æ³¨å’Œæ¨¡æ€ç‰¹å®šåˆ’åˆ†ï¼›è®¾è®¡äº†Integscanæ£€æµ‹æ¡†æ¶ï¼Œé‡‡ç”¨æ³¨æ„åŠ›å¢å¼ºçš„è§†è§‰ç¼–ç ä¸æç¤ºæ¡ä»¶è¯­ä¹‰å¯¹é½çš„ç»“æ„åŒ–çŠ¶æ€ç©ºé—´å»ºæ¨¡æ–¹æ³•ï¼Œå®ç°ç²¾ç¡®çš„ä¼ªé€ å®šä½ã€‚</p>
<p><strong>Result:</strong> åœ¨Rescindå’Œç°æœ‰åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒIntegscanåœ¨æ£€æµ‹å’Œå®šä½ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œä¸ºè‡ªåŠ¨åŒ–ç§‘å­¦å®Œæ•´æ€§åˆ†æå»ºç«‹äº†åšå®åŸºç¡€ï¼ŒéªŒè¯äº†æ‰€ææ¡†æ¶åœ¨å¤šç§ç”Ÿç‰©åŒ»å­¦æ¨¡æ€ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºç”Ÿç‰©åŒ»å­¦å›¾åƒä¼ªé€ æ£€æµ‹æä¾›äº†é¦–ä¸ªå…¨é¢çš„è§†è§‰è¯­è¨€å¼•å¯¼æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆä¸æ£€æµ‹çš„ååŒè®¾è®¡è§£å†³äº†é¢†åŸŸç‰¹å®šæŒ‘æˆ˜ï¼Œæ‰€å»ºç«‹çš„æ•°æ®é›†å’Œæ£€æµ‹æ–¹æ³•ä¸ºç§‘å­¦å®Œæ•´æ€§åˆ†æå¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œç ”ç©¶æ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Scientific image manipulation in biomedical publications poses a growing threat to research integrity and reproducibility. Unlike natural image forensics, biomedical forgery detection is uniquely challenging due to domain-specific artifacts, complex textures, and unstructured figure layouts. We present the first vision-language guided framework for both generating and detecting biomedical image forgeries. By combining diffusion-based synthesis with vision-language prompting, our method enables realistic and semantically controlled manipulations, including duplication, splicing, and region removal, across diverse biomedical modalities. We introduce Rescind, a large-scale benchmark featuring fine-grained annotations and modality-specific splits, and propose Integscan, a structured state space modeling framework that integrates attention-enhanced visual encoding with prompt-conditioned semantic alignment for precise forgery localization. To ensure semantic fidelity, we incorporate a vision-language model based verification loop that filters generated forgeries based on consistency with intended prompts. Extensive experiments on Rescind and existing benchmarks demonstrate that Integscan achieves state of the art performance in both detection and localization, establishing a strong foundation for automated scientific integrity analysis.</p>
<h3 id="8-from-prompts-to-deployment-auto-curated-domain-specific-dataset-generation-via-diffusion-models">[8] <a href="https://arxiv.org/abs/2601.08095">From Prompts to Deployment: Auto-Curated Domain-Specific Dataset Generation via Diffusion Models</a></h3>
<p><em>Dongsik Yoon, Jongeun Kim</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è‡ªåŠ¨åŒ–æµæ°´çº¿ï¼Œç”¨äºç”Ÿæˆé¢†åŸŸç‰¹å®šçš„åˆæˆæ•°æ®é›†ï¼Œé€šè¿‡ä¸‰é˜¶æ®µæ¡†æ¶è§£å†³é¢„è®­ç»ƒæ¨¡å‹ä¸çœŸå®éƒ¨ç½²ç¯å¢ƒä¹‹é—´çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œä»è€Œå‡å°‘å¯¹å¤§è§„æ¨¡çœŸå®æ•°æ®æ”¶é›†çš„ä¾èµ–ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³é¢„è®­ç»ƒæ¨¡å‹ä¸çœŸå®ä¸–ç•Œéƒ¨ç½²ç¯å¢ƒä¹‹é—´çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹è¶³å¤Ÿé¢†åŸŸç‰¹å®šçœŸå®æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¤§é‡çœŸå®æ•°æ®æ”¶é›†ï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ï¼Œå› æ­¤éœ€è¦ä¸€ç§è‡ªåŠ¨åŒ–ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®é›†çš„æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µè‡ªåŠ¨åŒ–æµæ°´çº¿æ¡†æ¶ï¼šé¦–å…ˆé€šè¿‡å—æ§ä¿®å¤æŠ€æœ¯å°†ç›®æ ‡å¯¹è±¡åˆæˆåˆ°é¢†åŸŸç‰¹å®šèƒŒæ™¯ä¸­ï¼›ç„¶åé‡‡ç”¨å¤šæ¨¡æ€è¯„ä¼°æ–¹æ³•è¿›è¡ŒéªŒè¯ï¼ŒåŒ…æ‹¬å¯¹è±¡æ£€æµ‹ã€ç¾å­¦è¯„åˆ†å’Œè§†è§‰è¯­è¨€å¯¹é½ï¼›æœ€åä½¿ç”¨ç”¨æˆ·åå¥½åˆ†ç±»å™¨æ¥æ•æ‰ä¸»è§‚é€‰æ‹©æ ‡å‡†ï¼Œç¡®ä¿ç”Ÿæˆæ•°æ®çš„è´¨é‡å’Œé€‚ç”¨æ€§ã€‚</p>
<p><strong>Result:</strong> è¯¥æµæ°´çº¿èƒ½å¤Ÿé«˜æ•ˆæ„å»ºé«˜è´¨é‡ã€å¯éƒ¨ç½²çš„åˆæˆæ•°æ®é›†ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹å¤§è§„æ¨¡çœŸå®ä¸–ç•Œæ•°æ®æ”¶é›†çš„ä¾èµ–ï¼Œé€šè¿‡å¤šæ¨¡æ€è¯„ä¼°ç¡®ä¿äº†ç”Ÿæˆæ•°æ®çš„è´¨é‡ï¼Œç”¨æˆ·åå¥½åˆ†ç±»å™¨çš„å¼•å…¥è¿›ä¸€æ­¥æå‡äº†æ•°æ®çš„ä¸»è§‚è´¨é‡å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºé¢†åŸŸç‰¹å®šæ•°æ®é›†çš„ç”Ÿæˆæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å’Œç³»ç»ŸåŒ–éªŒè¯æµç¨‹çš„ç»“åˆï¼Œä¸ä»…è§£å†³äº†åˆ†å¸ƒåç§»é—®é¢˜ï¼Œè¿˜ä¸ºå®é™…éƒ¨ç½²ç¯å¢ƒä¸­çš„æ•°æ®éœ€æ±‚æä¾›äº†å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®è·µåº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>In this paper, we present an automated pipeline for generating domain-specific synthetic datasets with diffusion models, addressing the distribution shift between pre-trained models and real-world deployment environments. Our three-stage framework first synthesizes target objects within domain-specific backgrounds through controlled inpainting. The generated outputs are then validated via a multi-modal assessment that integrates object detection, aesthetic scoring, and vision-language alignment. Finally, a user-preference classifier is employed to capture subjective selection criteria. This pipeline enables the efficient construction of high-quality, deployable datasets while reducing reliance on extensive real-world data collection.</p>
<h3 id="9-how-do-optical-flow-and-textual-prompts-collaborate-to-assist-in-audio-visual-semantic-segmentation">[9] <a href="https://arxiv.org/abs/2601.08133">How Do Optical Flow and Textual Prompts Collaborate to Assist in Audio-Visual Semantic Segmentation?</a></h3>
<p><em>Peng Gao, Yujian Lee, Yongqi Xu, Wentao Fan</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSSPï¼ˆStepping Stone Plusï¼‰çš„æ–°å‹åä½œæ¡†æ¶ï¼Œç”¨äºéŸ³é¢‘-è§†è§‰è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼Œè¯¥æ¡†æ¶é€šè¿‡é›†æˆå…‰æµå’Œæ–‡æœ¬æç¤ºæ¥å¢å¼ºåˆ†å‰²ç²¾åº¦ï¼Œå¹¶åœ¨å¤æ‚åœºæ™¯ä¸­è¶…è¶Šäº†ç°æœ‰AVSæ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éŸ³é¢‘-è§†è§‰è¯­ä¹‰åˆ†å‰²ä»»åŠ¡éœ€è¦è¶…è¶Šç®€å•çš„å‘å£°å¯¹è±¡è¯†åˆ«ï¼Œå®ç°åœºæ™¯çš„è¯­ä¹‰ç†è§£ã€‚ç°æœ‰æ–¹æ³•å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼Œä½†é¢ä¸´è¿åŠ¨å¯¹è±¡å’Œé™æ­¢å‘å£°å¯¹è±¡ï¼ˆå¦‚é—¹é’Ÿï¼‰çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ›´ç²¾ç¡®çš„æ—¶ç©ºä¸Šä¸‹æ–‡å’Œè¯­ä¹‰æ•´åˆã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†SSPåä½œæ¡†æ¶ï¼Œé‡‡ç”¨é¢„æ©ç æŠ€æœ¯åˆ©ç”¨å…‰æµæ•æ‰è¿åŠ¨åŠ¨æ€ï¼Œä¸ºç²¾ç¡®åˆ†å‰²æä¾›æ—¶é—´ä¸Šä¸‹æ–‡ã€‚é’ˆå¯¹é™æ­¢å‘å£°å¯¹è±¡ï¼ŒSSPæ•´åˆäº†ä¸¤ç§æ–‡æœ¬æç¤ºï¼šå¯¹è±¡ç±»åˆ«è¯†åˆ«å’Œåœºæ™¯æè¿°ã€‚æ­¤å¤–ï¼Œè¿˜å®ç°äº†è§†è§‰-æ–‡æœ¬å¯¹é½æ¨¡å—ä»¥ä¿ƒè¿›è·¨æ¨¡æ€æ•´åˆï¼Œå¹¶é‡‡ç”¨åæ©ç æŠ€æœ¯è®­ç»ƒæ¨¡å‹å­¦ä¹ å…‰æµå›¾ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒSSPæ¡†æ¶åœ¨éŸ³é¢‘-è§†è§‰åˆ†å‰²ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰AVSæ–¹æ³•ï¼Œèƒ½å¤Ÿæä¾›é«˜æ•ˆä¸”ç²¾ç¡®çš„åˆ†å‰²ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆå…‰æµå’Œæ–‡æœ¬æç¤ºçš„åä½œæ¡†æ¶åœ¨éŸ³é¢‘-è§†è§‰è¯­ä¹‰åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤„ç†è¿åŠ¨å¯¹è±¡å’Œé™æ­¢å‘å£°å¯¹è±¡æä¾›äº†åˆ›æ–°è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºå¤šæ¨¡æ€åœºæ™¯ç†è§£å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Audio-visual semantic segmentation (AVSS) represents an extension of the audio-visual segmentation (AVS) task, necessitating a semantic understanding of audio-visual scenes beyond merely identifying sound-emitting objects at the visual pixel level. Contrary to a previous methodology, by decomposing the AVSS task into two discrete subtasks by initially providing a prompted segmentation mask to facilitate subsequent semantic analysis, our approach innovates on this foundational strategy. We introduce a novel collaborative framework, \textit{S}tepping \textit{S}tone \textit{P}lus (SSP), which integrates optical flow and textual prompts to assist the segmentation process. In scenarios where sound sources frequently coexist with moving objects, our pre-mask technique leverages optical flow to capture motion dynamics, providing essential temporal context for precise segmentation. To address the challenge posed by stationary sound-emitting objects, such as alarm clocks, SSP incorporates two specific textual prompts: one identifies the category of the sound-emitting object, and the other provides a broader description of the scene. Additionally, we implement a visual-textual alignment module (VTA) to facilitate cross-modal integration, delivering more coherent and contextually relevant semantic interpretations. Our training regimen involves a post-mask technique aimed at compelling the model to learn the diagram of the optical flow. Experimental results demonstrate that SSP outperforms existing AVS methods, delivering efficient and precise segmentation results.</p>
<h3 id="10-subspace-alignment-for-vision-language-model-test-time-adaptation">[10] <a href="https://arxiv.org/abs/2601.08139">Subspace Alignment for Vision-Language Model Test-time Adaptation</a></h3>
<p><em>Zhichen Zeng, Wenxuan Bao, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Xuying Ning, Yuchen Yan, Chen Luo, Monica Xiao Cheng, Jingrui He, Hanghang Tong</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSubTTAæ–¹æ³•ï¼Œé€šè¿‡å¯¹é½è§†è§‰ä¸æ–‡æœ¬æ¨¡æ€çš„è¯­ä¹‰å­ç©ºé—´æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æµ‹è¯•æ—¶é€‚åº”èƒ½åŠ›ï¼Œè§£å†³äº†åˆ†å¸ƒåç§»ä¸‹ä¼ªæ ‡ç­¾ä¸å¯é çš„é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡2.24%çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹å…·æœ‰å‡ºè‰²çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä½†åœ¨åˆ†å¸ƒåç§»ä¸‹è¡¨ç°è„†å¼±ã€‚ç°æœ‰æµ‹è¯•æ—¶é€‚åº”æ–¹æ³•ä¸¥é‡ä¾èµ–é›¶æ ·æœ¬é¢„æµ‹ä½œä¸ºä¼ªæ ‡ç­¾è¿›è¡Œè‡ªè®­ç»ƒï¼Œä½†è¿™äº›ä¼ªæ ‡ç­¾åœ¨åˆ†å¸ƒåç§»ä¸‹ä¸å¯é ï¼Œä¸»è¦å—ä¸¤ä¸ªåŸºæœ¬é™åˆ¶å½±å“ï¼šæ¨¡æ€é—´éš™å¯¼è‡´è·¨æ¨¡æ€å…³ç³»ä¸å‡†ç¡®ï¼Œä»¥åŠè§†è§‰åµŒå…¥ç¼–ç äº†ä¸°å¯Œä½†ä¸ä»»åŠ¡æ— å…³çš„å™ªå£°ï¼Œè¿™äº›å™ªå£°åœ¨åˆ†å¸ƒåç§»ä¸‹å¸¸å¸¸æ·¹æ²¡ä»»åŠ¡ç‰¹å®šçš„è¯­ä¹‰ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºSubTTAæ–¹æ³•ï¼Œé€šè¿‡å¯¹é½ä¸¤ç§æ¨¡æ€çš„è¯­ä¹‰å­ç©ºé—´æ¥å¢å¼ºé›¶æ ·æœ¬é¢„æµ‹ä»¥æ›´å¥½åœ°æŒ‡å¯¼TTAè¿‡ç¨‹ã€‚ä¸ºå¼¥åˆæ¨¡æ€é—´éš™ï¼ŒSubTTAæå–ä¸¤ç§æ¨¡æ€çš„ä¸»å­ç©ºé—´ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–å¼¦è·ç¦»å°†è§†è§‰æµå½¢å¯¹é½åˆ°æ–‡æœ¬è¯­ä¹‰é”šç‚¹ã€‚ä¸ºæ¶ˆé™¤è§†è§‰å™ªå£°ï¼ŒSubTTAå°†å¯¹é½åçš„è§†è§‰ç‰¹å¾æŠ•å½±åˆ°ä»»åŠ¡ç‰¹å®šçš„æ–‡æœ¬å­ç©ºé—´ï¼Œé€šè¿‡å°†è§†è§‰åµŒå…¥çº¦æŸåœ¨æœ‰æ•ˆè¯­ä¹‰èŒƒå›´å†…æ¥è¿‡æ»¤æ‰ä»»åŠ¡æ— å…³å™ªå£°ï¼Œç„¶ååœ¨çº¯åŒ–ç©ºé—´ä¸Šæ‰§è¡Œæ ‡å‡†TTAä»¥ç»†åŒ–å†³ç­–è¾¹ç•Œã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’ŒVLMæ¶æ„ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†SubTTAçš„æœ‰æ•ˆæ€§ï¼Œç›¸æ¯”æœ€å…ˆè¿›çš„TTAæ–¹æ³•å¹³å‡æå‡äº†2.24%çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æ”¹å–„äº†åˆ†å¸ƒåç§»ä¸‹çš„é€‚åº”èƒ½åŠ›ï¼Œé€šè¿‡å­ç©ºé—´å¯¹é½å’Œå™ªå£°è¿‡æ»¤æœºåˆ¶å¢å¼ºäº†ä¼ªæ ‡ç­¾çš„å¯é æ€§ã€‚</p>
<p><strong>Conclusion:</strong> SubTTAé€šè¿‡è¯­ä¹‰å­ç©ºé—´å¯¹é½æœ‰æ•ˆè§£å†³äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶é€‚åº”ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒé™åˆ¶ï¼Œä¸ºåˆ†å¸ƒåç§»ä¸‹çš„å¯é é€‚åº”æä¾›äº†æ–°æ€è·¯ã€‚è¯¥æ–¹æ³•ä¸ä»…æå‡äº†æ€§èƒ½ï¼Œè¿˜æ­ç¤ºäº†æ¨¡æ€å¯¹é½å’Œå™ªå£°è¿‡æ»¤åœ¨è·¨æ¨¡æ€å­¦ä¹ ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥è§†è§‰è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Vision-language models (VLMs), despite their extraordinary zero-shot capabilities, are vulnerable to distribution shifts. Test-time adaptation (TTA) emerges as a predominant strategy to adapt VLMs to unlabeled test data on the fly. However, existing TTA methods heavily rely on zero-shot predictions as pseudo-labels for self-training, which can be unreliable under distribution shifts and misguide adaptation due to two fundamental limitations. First (Modality Gap), distribution shifts induce gaps between visual and textual modalities, making cross-modal relations inaccurate. Second (Visual Nuisance), visual embeddings encode rich but task-irrelevant noise that often overwhelms task-specific semantics under distribution shifts. To address these limitations, we propose SubTTA, which aligns the semantic subspaces of both modalities to enhance zero-shot predictions to better guide the TTA process. To bridge the modality gap, SubTTA extracts the principal subspaces of both modalities and aligns the visual manifold to the textual semantic anchor by minimizing their chordal distance. To eliminate visual nuisance, SubTTA projects the aligned visual features onto the task-specific textual subspace, which filters out task-irrelevant noise by constraining visual embeddings within the valid semantic span, and standard TTA is further performed on the purified space to refine the decision boundaries. Extensive experiments on various benchmarks and VLM architectures demonstrate the effectiveness of SubTTA, yielding an average improvement of 2.24% over state-of-the-art TTA methods.</p>
<h3 id="11-where-does-vision-meet-language-understanding-and-refining-visual-fusion-in-mllms-via-contrastive-attention">[11] <a href="https://arxiv.org/abs/2601.08151">Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention</a></h3>
<p><em>Shezheng Song, Shasha Li, Jie Yu</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ€§çš„å±‚é—´æ©ç åˆ†ææ­ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­è§†è§‰-æ–‡æœ¬èåˆçš„æ¼”åŒ–æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¯¹é½æ³¨æ„åŠ›æ¡†æ¶æ¥æå‡å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å†…éƒ¨å¦‚ä½•æ•´åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ä»ç„¶ç¼ºä¹æ·±å…¥ç†è§£ï¼Œè¿™ç§é»‘ç®±æ€§è´¨é™åˆ¶äº†æ¨¡å‹çš„è¿›ä¸€æ­¥ä¼˜åŒ–å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨ç³»ç»Ÿæ€§å±‚é—´æ©ç åˆ†ææŠ€æœ¯ï¼Œå¯¹å¤šç§æ¨¡å‹æ¶æ„è¿›è¡Œé€å±‚åˆ†æä»¥è¿½è¸ªè§†è§‰-æ–‡æœ¬èåˆçš„æ¼”åŒ–è¿‡ç¨‹ï¼Œå¹¶åŸºäºåˆ†æç»“æœæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¯¹é½æ³¨æ„åŠ›æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å»ºæ¨¡æ—©æœŸèåˆå±‚ä¸æœ€ç»ˆå±‚ä¹‹é—´çš„æ³¨æ„åŠ›å˜æ¢æ¥çªå‡ºæœ‰æ„ä¹‰çš„æ³¨æ„åŠ›è½¬ç§»ã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°èåˆè¿‡ç¨‹é›†ä¸­åœ¨ç‰¹å®šå±‚è€Œéå‡åŒ€åˆ†å¸ƒï¼Œéƒ¨åˆ†æ¨¡å‹åœ¨è¾“å‡ºå‰å‡ºç°è§†è§‰ä¿¡å·é‡æ–°æ¿€æ´»çš„"å›é¡¾"ç°è±¡ï¼›æ³¨æ„åŠ›åˆ†ææ˜¾ç¤ºä¸ç›¸å…³åŒºåŸŸå­˜åœ¨æŒç»­çš„é«˜æ³¨æ„åŠ›å™ªå£°ï¼Œè€Œæ–‡æœ¬å¯¹é½åŒºåŸŸçš„æ³¨æ„åŠ›é€æ¸å¢å¼ºï¼›æå‡ºçš„å¯¹é½æ³¨æ„åŠ›æ¡†æ¶åœ¨å¤šç§MLLMå’ŒåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†MLLMä¸­è§†è§‰-æ–‡æœ¬èåˆçš„å±‚çº§æ¼”åŒ–æœºåˆ¶ï¼Œä¸ºæ¨¡å‹å¯è§£é‡Šæ€§æä¾›äº†é‡è¦è§è§£ï¼›æå‡ºçš„è®­ç»ƒæ— å…³æ³¨æ„åŠ›å¯¹é½æ–¹æ³•å±•ç¤ºäº†é€šè¿‡ç†è§£å†…éƒ¨è¡¨ç¤ºåŠ¨æ€æ¥æå‡æ¨¡å‹æ€§èƒ½çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æ¨¡å‹ä¼˜åŒ–å’Œæ¶æ„è®¾è®¡æä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage "review" phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released.</p>
<h3 id="12-representation-learning-with-semantic-aware-instance-and-sparse-token-alignments">[12] <a href="https://arxiv.org/abs/2601.08165">Representation Learning with Semantic-aware Instance and Sparse Token Alignments</a></h3>
<p><em>Phuoc-Nguyen Bui, Toan Duc Nguyen, Junghyun Bum, Duc-Tai Le, Hyunseung Choo</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šçº§å¯¹é½æ¡†æ¶SISTAï¼Œé€šè¿‡åˆ©ç”¨åŒ»å­¦å›¾åƒä¸æ”¾å°„å­¦æŠ¥å‘Šåœ¨å›¾åƒ-æŠ¥å‘Šå’Œè¡¥ä¸-è¯çº§åˆ«ä¸Šçš„è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œæ”¹è¿›äº†åŒ»å­¦å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å°†æ‰€æœ‰æœªé…å¯¹æ ·æœ¬è§†ä¸ºè´Ÿä¾‹å¯¼è‡´çš„è¯­ä¹‰ç»“æ„ç ´åé—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»ŸåŒ»å­¦å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ–¹æ³•é€šå¸¸å°†é…å¯¹å›¾åƒ-æŠ¥å‘Šæ ·æœ¬è§†ä¸ºæ­£ä¾‹ï¼Œæœªé…å¯¹æ ·æœ¬è§†ä¸ºè´Ÿä¾‹ï¼Œä½†åœ¨åŒ»å­¦æ•°æ®é›†ä¸­ï¼Œä¸åŒæ‚£è€…çš„å›¾åƒæˆ–æŠ¥å‘Šä¹‹é—´å¯èƒ½å­˜åœ¨æ˜¾è‘—ç›¸ä¼¼æ€§ï¼Œå°†æ‰€æœ‰æœªé…å¯¹æ ·æœ¬è§†ä¸ºè´Ÿä¾‹ä¼šç ´ååº•å±‚è¯­ä¹‰ç»“æ„å¹¶å½±å“å­¦ä¹ è¡¨ç¤ºçš„è´¨é‡ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†å¤šçº§å¯¹é½æ¡†æ¶SISTAï¼Œé€šè¿‡åˆ©ç”¨åŒ»å­¦å›¾åƒä¸æ”¾å°„å­¦æŠ¥å‘Šåœ¨å›¾åƒ-æŠ¥å‘Šå’Œè¡¥ä¸-è¯ä¸¤ä¸ªçº§åˆ«çš„è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œæ”¹è¿›äº†ä¼ ç»Ÿå¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œå…·ä½“åŒ…æ‹¬å¼•å…¥æŠ¥å‘Šé—´ç›¸ä¼¼æ€§ä»¥æ¶ˆé™¤å‡è´Ÿä¾‹ï¼Œå¹¶å¼€å‘äº†æœ‰æ•ˆå¯¹é½å›¾åƒè¡¥ä¸ä¸ç›¸å…³è¯æ ‡è®°çš„æ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„ä¸‰ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼ˆå›¾åƒåˆ†ç±»ã€å›¾åƒåˆ†å‰²å’Œç›®æ ‡æ£€æµ‹ï¼‰ä¸­æ˜¾è‘—æé«˜äº†è¿ç§»æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ ‡æ³¨æ•°æ®çš„ç»†ç²’åº¦ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†å…¬å¼€æä¾›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨åŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒä¸­è€ƒè™‘è¯­ä¹‰ç›¸ä¼¼æ€§çš„é‡è¦æ€§ï¼Œæå‡ºçš„å¤šçº§å¯¹é½æ¡†æ¶ä¸ºå¤„ç†åŒ»å­¦æ•°æ®ä¸­çš„å¤æ‚è¯­ä¹‰å…³ç³»æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ä»èƒ½å®ç°ç»†ç²’åº¦ä»»åŠ¡çš„æ€§èƒ½æå‡ï¼Œä¸ºåŒ»å­¦AIåº”ç”¨æä¾›äº†æ›´é²æ£’çš„è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Medical contrastive vision-language pre-training (VLP) has demonstrated significant potential in improving performance on downstream tasks. Traditional approaches typically employ contrastive learning, treating paired image-report samples as positives and unpaired ones as negatives. However, in medical datasets, there can be substantial similarities between images or reports from different patients. Rigidly treating all unpaired samples as negatives, can disrupt the underlying semantic structure and negatively impact the quality of the learned representations. In this paper, we propose a multi-level alignment framework, Representation Learning with Semantic-aware Instance and Sparse Token Alignments (SISTA) by exploiting the semantic correspondence between medical image and radiology reports at two levels, i.e., image-report and patch-word levels. Specifically, we improve the conventional contrastive learning by incorporating inter-report similarity to eliminate the false negatives and introduce a method to effectively align image patches with relevant word tokens. Experimental results demonstrate the effectiveness of the proposed framework in improving transfer performance across different datasets on three downstream tasks: image classification, image segmentation, and object detection. Notably, our framework achieves significant improvements in fine-grained tasks even with limited labeled data. Codes and pre-trained models will be made available.</p>
<h3 id="13-instruction-driven-3d-facial-expression-generation-and-transition">[13] <a href="https://arxiv.org/abs/2601.08179">Instruction-Driven 3D Facial Expression Generation and Transition</a></h3>
<p><em>Anh H. Vo, Tae-Seok Kim, Hulin Jin, Soo-Mi Choi, Yong-Guk Kim</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æŒ‡ä»¤é©±åŠ¨çš„ä¸‰ç»´é¢éƒ¨è¡¨æƒ…ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æŒ‡ä»¤åœ¨ä»»æ„ä¸¤ç§æŒ‡å®šè¡¨æƒ…ä¹‹é—´ç”Ÿæˆå¹³æ»‘çš„é¢éƒ¨è¡¨æƒ…è¿‡æ¸¡åºåˆ—ï¼Œæ˜¾è‘—æ‰©å±•äº†ä¸‰ç»´è™šæ‹ŸåŒ–èº«çš„è¡¨æƒ…è¡¨è¾¾èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿä¸‰ç»´è™šæ‹ŸåŒ–èº«é€šå¸¸ä»…æ”¯æŒå…­ç§åŸºæœ¬é¢éƒ¨è¡¨æƒ…ï¼Œç¼ºä¹æ¨¡æ‹ŸçœŸå®æƒ…æ„Ÿå˜åŒ–çš„çµæ´»æ€§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¦‚ä½•æ ¹æ®æ–‡æœ¬æŒ‡ä»¤åœ¨ä»»æ„ä¸¤ç§é¢éƒ¨è¡¨æƒ…ä¹‹é—´ç”Ÿæˆå¹³æ»‘è¿‡æ¸¡åºåˆ—çš„é—®é¢˜ï¼Œä»¥æ‰©å±•è™šæ‹ŸåŒ–èº«çš„æƒ…æ„Ÿè¡¨è¾¾èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†æŒ‡ä»¤é©±åŠ¨çš„é¢éƒ¨è¡¨æƒ…åˆ†è§£å™¨æ¨¡å—æ¥å­¦ä¹ å¤šæ¨¡æ€æ•°æ®å¹¶æ•æ‰æ–‡æœ¬æè¿°ä¸é¢éƒ¨è¡¨æƒ…ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚éšåå¼€å‘äº†æŒ‡ä»¤åˆ°é¢éƒ¨è¡¨æƒ…è¿‡æ¸¡æ–¹æ³•ï¼Œåˆ©ç”¨è¯¥åˆ†è§£å™¨å’Œé¡¶ç‚¹é‡å»ºæŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ½œåœ¨å‘é‡çš„è¯­ä¹‰ç†è§£ï¼Œä»è€Œæ ¹æ®ç»™å®šæŒ‡ä»¤ç”Ÿæˆé¢éƒ¨è¡¨æƒ…åºåˆ—ã€‚æœ€åæ„å»ºäº†é¢éƒ¨è¡¨æƒ…è¿‡æ¸¡æ¨¡å‹æ¥ç”Ÿæˆè¡¨æƒ…ä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ã€‚</p>
<p><strong>Result:</strong> åœ¨CK+å’ŒCelebV-HQæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨é¢éƒ¨è¡¨æƒ…ç”Ÿæˆä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆå‡†ç¡®çš„é¢éƒ¨è¡¨æƒ…è½¨è¿¹ï¼Œå¹¶ä¸”é€šè¿‡æ–‡æœ¬æç¤ºå¯ä»¥æå¤§åœ°æ‰©å±•é¢éƒ¨è¡¨æƒ…åŠå…¶è¿‡æ¸¡çš„å¤šæ ·æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºä¸‰ç»´è™šæ‹ŸåŒ–èº«æä¾›äº†çµæ´»çš„è¡¨æƒ…æ§åˆ¶æœºåˆ¶ï¼Œé€šè¿‡æ–‡æœ¬æŒ‡ä»¤é©±åŠ¨çš„æ–¹å¼æ˜¾è‘—å¢å¼ºäº†æƒ…æ„Ÿè¡¨è¾¾çš„å¤šæ ·æ€§å’Œè‡ªç„¶æ€§ã€‚è¯¥æ¡†æ¶åœ¨è™šæ‹Ÿç°å®ã€æ¸¸æˆè§’è‰²åŠ¨ç”»å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä¸ºåŸºäºå¤šæ¨¡æ€è¾“å…¥çš„é¢éƒ¨åŠ¨ç”»ç”Ÿæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/</p>
<h3 id="14-gi-bench-a-panoramic-benchmark-revealing-the-knowledge-experience-dissociation-of-multimodal-large-language-models-in-gastrointestinal-endoscopy-against-clinical-standards">[14] <a href="https://arxiv.org/abs/2601.08183">GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards</a></h3>
<p><em>Yan Zhu, Te Luo, Pei-Yao Fu, Zhen Zhang, Zi-Long Wang, Yi-Fan Qu, Zi-Han Geng, Jia-Qi Xu, Lu Yao, Li-Yun Ma, Wei Su, Wei-Feng Chen, Quan-Lin Li, Shuo Wang, Ping-Hong Zhou</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨èƒƒè‚ å†…çª¥é•œä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨è¯Šæ–­æ¨ç†æ–¹é¢å¯åª²ç¾åˆçº§å†…é•œåŒ»å¸ˆï¼Œä½†åœ¨ç©ºé—´å®šä½å’Œäº‹å®å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ç“¶é¢ˆï¼Œå¹¶æå‡ºäº†GI-BenchåŠ¨æ€åŸºå‡†æµ‹è¯•å¹³å°ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨èƒƒè‚ ç—…å­¦ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶åœ¨å®Œæ•´ä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„è¡¨ç°ä»¥åŠä¸äººç±»åŸºå‡†çš„å¯¹æ¯”å°šæœªå¾—åˆ°ç³»ç»ŸéªŒè¯ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œè¯„ä¼°æ¨¡å‹åœ¨èƒƒè‚ å†…çª¥é•œå…¨æ™¯å·¥ä½œæµç¨‹ä¸­çš„ä¸´åºŠæ•ˆç”¨ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†åŒ…å«20ä¸ªç»†ç²’åº¦ç—…å˜ç±»åˆ«çš„GI-BenchåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†12ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨äº”é˜¶æ®µä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬è§£å‰–å®šä½ã€ç—…å˜è¯†åˆ«ã€è¯Šæ–­ã€å‘ç°æè¿°å’Œç®¡ç†ï¼Œå¹¶ä½¿ç”¨Macro-F1ã€å¹³å‡äº¤å¹¶æ¯”å’Œå¤šç»´åº¦æå…‹ç‰¹é‡è¡¨å°†æ¨¡å‹æ€§èƒ½ä¸ä¸‰ååˆçº§å†…é•œåŒ»å¸ˆå’Œä¸‰åä½é™¢åŒ»å¸ˆè¿›è¡Œå¯¹æ¯”ã€‚</p>
<p><strong>Result:</strong> Gemini-3-Proå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è¯Šæ–­æ¨ç†æ–¹é¢ï¼Œé¡¶çº§æ¨¡å‹çš„Macro-F1åˆ†æ•°ä¸º0.641ï¼Œä¼˜äºä½é™¢åŒ»å¸ˆçš„0.492ï¼Œå¹¶ä¸åˆçº§å†…é•œåŒ»å¸ˆçš„0.727ç›¸å½“ï¼Œä½†å­˜åœ¨å…³é”®çš„"ç©ºé—´å®šä½ç“¶é¢ˆ"ï¼Œäººç±»ç—…å˜å®šä½çš„mIoUè¶…è¿‡0.506ï¼Œæ˜¾è‘—ä¼˜äºæœ€ä½³æ¨¡å‹çš„0.345ï¼ŒåŒæ—¶å‘ç°"æµç•…æ€§-å‡†ç¡®æ€§æ‚–è®º"ï¼Œæ¨¡å‹ç”ŸæˆæŠ¥å‘Šçš„è¯­è¨€å¯è¯»æ€§ä¼˜äºäººç±»ä½†äº‹å®å‡†ç¡®æ€§æ˜¾è‘—è¾ƒä½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¯Šæ–­æ¨ç†æ–¹é¢å·²è¾¾åˆ°ä¸´åºŠå¯ç”¨æ°´å¹³ï¼Œä½†åœ¨ç©ºé—´å®šä½å’Œè§†è§‰ç‰¹å¾è§£é‡Šæ–¹é¢ä»éœ€æ”¹è¿›ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨åŒ»å­¦åº”ç”¨ä¸­å­˜åœ¨çš„"è¿‡åº¦è§£é‡Š"å’Œå¹»è§‰é—®é¢˜ï¼ŒGI-BenchåŠ¨æ€æ’è¡Œæ¦œä¸ºè·Ÿè¸ªæ¨¡å‹åœ¨ä¸´åºŠå†…çª¥é•œä¸­çš„æ¼”è¿›æä¾›äº†æŒç»­è¯„ä¼°æ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) show promise in gastroenterology, yet their performance against comprehensive clinical workflows and human benchmarks remains unverified. To systematically evaluate state-of-the-art MLLMs across a panoramic gastrointestinal endoscopy workflow and determine their clinical utility compared with human endoscopists. We constructed GI-Bench, a benchmark encompassing 20 fine-grained lesion categories. Twelve MLLMs were evaluated across a five-stage clinical workflow: anatomical localization, lesion identification, diagnosis, findings description, and management. Model performance was benchmarked against three junior endoscopists and three residency trainees using Macro-F1, mean Intersection-over-Union (mIoU), and multi-dimensional Likert scale. Gemini-3-Pro achieved state-of-the-art performance. In diagnostic reasoning, top-tier models (Macro-F1 0.641) outperformed trainees (0.492) and rivaled junior endoscopists (0.727; p&gt;0.05). However, a critical "spatial grounding bottleneck" persisted; human lesion localization (mIoU &gt;0.506) significantly outperformed the best model (0.345; p&lt;0.05). Furthermore, qualitative analysis revealed a "fluency-accuracy paradox": models generated reports with superior linguistic readability compared with humans (p&lt;0.05) but exhibited significantly lower factual correctness (p&lt;0.05) due to "over-interpretation" and hallucination of visual features.GI-Bench maintains a dynamic leaderboard that tracks the evolving performance of MLLMs in clinical endoscopy. The current rankings and benchmark results are available at https://roterdl.github.io/GIBench/.</p>
<h3 id="15-route-retrieve-reflect-repair-self-improving-agentic-framework-for-visual-detection-and-linguistic-reasoning-in-medical-imaging">[15] <a href="https://arxiv.org/abs/2601.08192">Route, Retrieve, Reflect, Repair: Self-Improving Agentic Framework for Visual Detection and Linguistic Reasoning in Medical Imaging</a></h3>
<p><em>Md. Faiyaz Abdullah Sayeedi, Rashedur Rahman, Siam Tahsin Bhuiyan, Sefatul Wasi, Ashraful Islam, Saadia Binte Alam, AKM Mahbubur Rahman</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºRâ´æ¡†æ¶ï¼Œä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†æçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡è·¯ç”±ã€æ£€ç´¢ã€åæ€å’Œä¿®å¤å››ä¸ªåè°ƒæ™ºèƒ½ä½“ï¼Œå°†å¼ºä½†è„†å¼±çš„è§†è§‰è¯­è¨€æ¨¡å‹è½¬å˜ä¸ºæ›´å¯é ã€ç©ºé—´åŸºç¡€æ›´å¥½çš„ä¸´åºŠå›¾åƒè§£é‡Šå·¥å…·ï¼Œæ— éœ€åŸºäºæ¢¯åº¦çš„å¾®è°ƒå³å¯æ˜¾è‘—æå‡æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŒ»å­¦å›¾åƒåˆ†æä¸»è¦ä¾èµ–å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä½†å¤§å¤šæ•°ç³»ç»Ÿä»æ˜¯å•æ¬¡é€šè¿‡çš„é»‘ç›’ï¼Œåœ¨æ¨ç†æ§åˆ¶ã€å®‰å…¨æ€§å’Œç©ºé—´åŸºç¡€æ–¹é¢å­˜åœ¨å±€é™ï¼Œç¼ºä¹å¯¹ä¸´åºŠé”™è¯¯æ¨¡å¼çš„æœ‰æ•ˆå¤„ç†æœºåˆ¶ã€‚</p>
<p><strong>Method:</strong> Râ´æ¡†æ¶å°†åŒ»å­¦å½±åƒå·¥ä½œæµåˆ†è§£ä¸ºå››ä¸ªåè°ƒæ™ºèƒ½ä½“ï¼šè·¯ç”±å™¨æ ¹æ®å›¾åƒã€æ‚£è€…ç—…å²å’Œå…ƒæ•°æ®é…ç½®ä»»åŠ¡å’Œä¸“ä¸šåŒ–æç¤ºï¼›æ£€ç´¢å™¨ä½¿ç”¨ç¤ºä¾‹è®°å¿†å’Œpass@ké‡‡æ ·è”åˆç”Ÿæˆè‡ªç”±æ–‡æœ¬æŠ¥å‘Šå’Œè¾¹ç•Œæ¡†ï¼›åæ€å™¨é’ˆå¯¹æ¯ä¸ªè‰ç¨¿-æ¡†å¯¹æ‰¹åˆ¤å…­ç§å…³é”®ä¸´åºŠé”™è¯¯æ¨¡å¼ï¼›ä¿®å¤å™¨åœ¨é’ˆå¯¹æ€§çº¦æŸä¸‹è¿­ä»£ä¿®è®¢å™äº‹å’Œç©ºé—´è¾“å‡ºï¼ŒåŒæ—¶ä¸ºæœªæ¥ç—…ä¾‹ç­–åˆ’é«˜è´¨é‡ç¤ºä¾‹ã€‚</p>
<p><strong>Result:</strong> åœ¨èƒ¸éƒ¨Xå°„çº¿åˆ†æä¸­ï¼ŒRâ´ä½¿ç”¨å¤šä¸ªç°ä»£VLMéª¨å¹²è¿›è¡Œè¯„ä¼°ï¼Œåœ¨æŠ¥å‘Šç”Ÿæˆå’Œå¼±ç›‘ç£æ£€æµ‹ä»»åŠ¡ä¸Šï¼Œç›¸æ¯”å¼ºå¤§çš„å•VLMåŸºçº¿ï¼ŒæŒç»­æå‡LLM-as-a-Judgeè¯„åˆ†çº¦1.7-2.5åˆ†ï¼ŒmAP50æå‡2.5-3.5ä¸ªç»å¯¹ç™¾åˆ†ç‚¹ï¼Œä¸”æ— éœ€ä»»ä½•åŸºäºæ¢¯åº¦çš„å¾®è°ƒã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œæ™ºèƒ½ä½“è·¯ç”±ã€åæ€å’Œä¿®å¤æœºåˆ¶èƒ½å¤Ÿå°†å¼ºå¤§ä½†è„†å¼±çš„è§†è§‰è¯­è¨€æ¨¡å‹è½¬å˜ä¸ºæ›´å¯é ã€åŸºç¡€æ›´å¥½çš„ä¸´åºŠå›¾åƒè§£é‡Šå·¥å…·ï¼Œä¸ºåŒ»å­¦å½±åƒåˆ†ææä¾›äº†å¯æ§åˆ¶ã€å¯è§£é‡Šä¸”æ€§èƒ½æ›´ä¼˜çš„æ¡†æ¶ï¼Œå±•ç¤ºäº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨åŒ»ç–—AIä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Medical image analysis increasingly relies on large vision-language models (VLMs), yet most systems remain single-pass black boxes that offer limited control over reasoning, safety, and spatial grounding. We propose R^4, an agentic framework that decomposes medical imaging workflows into four coordinated agents: a Router that configures task- and specialization-aware prompts from the image, patient history, and metadata; a Retriever that uses exemplar memory and pass@k sampling to jointly generate free-text reports and bounding boxes; a Reflector that critiques each draft-box pair for key clinical error modes (negation, laterality, unsupported claims, contradictions, missing findings, and localization errors); and a Repairer that iteratively revises both narrative and spatial outputs under targeted constraints while curating high-quality exemplars for future cases. Instantiated on chest X-ray analysis with multiple modern VLM backbones and evaluated on report generation and weakly supervised detection, R^4 consistently boosts LLM-as-a-Judge scores by roughly +1.7-+2.5 points and mAP50 by +2.5-+3.5 absolute points over strong single-VLM baselines, without any gradient-based fine-tuning. These results show that agentic routing, reflection, and repair can turn strong but brittle VLMs into more reliable and better grounded tools for clinical image interpretation. Our code can be found at: https://github.com/faiyazabdullah/MultimodalMedAgent</p>
<h3 id="16-unified-multi-site-multi-sequence-brain-mri-harmonization-enriched-by-biomedical-semantic-style">[16] <a href="https://arxiv.org/abs/2601.08193">Unified Multi-Site Multi-Sequence Brain MRI Harmonization Enriched by Biomedical Semantic Style</a></h3>
<p><em>Mengqi Wu, Yongheng Sun, Qianqian Wang, Pew-Thian Yap, Mingxia Liu</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MMHï¼Œä¸€ä¸ªç”¨äºå¤šç«™ç‚¹å¤šåºåˆ—è„‘MRIåè°ƒçš„ç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç”Ÿç‰©åŒ»å­¦è¯­ä¹‰å…ˆéªŒè¿›è¡Œåºåˆ—æ„ŸçŸ¥çš„é£æ ¼å¯¹é½ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæ‰©æ•£æ¨¡å‹å®ç°æ— éœ€é…å¯¹æ•°æ®çš„è§£å‰–ç»“æ„ä¿ç•™åè°ƒã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šç«™ç‚¹è„‘MRIæ•°æ®èšåˆå¯å¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒï¼Œä½†ä¼šå¼•å…¥ç”±ç«™ç‚¹ç‰¹å®šå˜å¼‚ï¼ˆå¦‚æ‰«æä»ªå‚å•†ã€é‡‡é›†å‚æ•°å’Œæˆåƒåè®®å·®å¼‚ï¼‰å¯¼è‡´çš„éç”Ÿç‰©å¼‚è´¨æ€§ï¼Œä»è€ŒæŸå®³æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰å›é¡¾æ€§MRIåè°ƒæ–¹æ³•é€šå¸¸ä¾èµ–æœ‰é™çš„é…å¯¹æ—…è¡Œè€…æ•°æ®ï¼Œæˆ–æœªèƒ½æœ‰æ•ˆè§£è€¦é£æ ¼ä¸è§£å‰–ç»“æ„ï¼Œä¸”å¤§å¤šä»…å¤„ç†å•åºåˆ—åè°ƒï¼Œé™åˆ¶äº†åœ¨å¸¸è§„è·å–å¤šåºåˆ—MRIçš„çœŸå®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> MMHæ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼šç¬¬ä¸€é˜¶æ®µä¸ºåŸºäºæ‰©æ•£çš„å…¨å±€åè°ƒå™¨ï¼Œé€šè¿‡é£æ ¼æ— å…³çš„æ¢¯åº¦æ¡ä»¶å°†MRå›¾åƒæ˜ å°„åˆ°åºåˆ—ç‰¹å®šçš„ç»Ÿä¸€åŸŸï¼›ç¬¬äºŒé˜¶æ®µä¸ºç›®æ ‡ç‰¹å®šå¾®è°ƒå™¨ï¼Œå°†å…¨å±€å¯¹é½å›¾åƒé€‚é…åˆ°æœŸæœ›çš„ç›®æ ‡åŸŸã€‚é‡‡ç”¨ä¸‰å¹³é¢æ³¨æ„åŠ›BiomedCLIPç¼–ç å™¨èšåˆå¤šè§†å›¾åµŒå…¥ä»¥è¡¨å¾ä½“ç§¯é£æ ¼ä¿¡æ¯ï¼Œå®ç°æ— éœ€é…å¯¹æ•°æ®çš„å›¾åƒé£æ ¼ä¸è§£å‰–ç»“æ„çš„æ˜¾å¼è§£è€¦ã€‚</p>
<p><strong>Result:</strong> åœ¨4,163ä¸ªT1å’ŒT2åŠ æƒMRIä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMMHåœ¨å›¾åƒç‰¹å¾èšç±»ã€ä½“ç´ çº§æ¯”è¾ƒã€ç»„ç»‡åˆ†å‰²ä»¥åŠä¸‹æ¸¸å¹´é¾„å’Œç«™ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šç«™ç‚¹å¤šåºåˆ—è„‘MRIåè°ƒæ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨ç”Ÿç‰©åŒ»å­¦è¯­ä¹‰å…ˆéªŒè¿›è¡Œåºåˆ—æ„ŸçŸ¥é£æ ¼å¯¹é½çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šç«™ç‚¹å¤šåºåˆ—è„‘MRIåè°ƒæä¾›äº†ç»Ÿä¸€è§£å†³æ–¹æ¡ˆã€‚MMHæ¡†æ¶é€šè¿‡æ˜¾å¼è§£è€¦é£æ ¼ä¸è§£å‰–ç»“æ„ï¼Œæ— éœ€é…å¯¹æ•°æ®å³å¯å®ç°é«˜è´¨é‡åè°ƒï¼Œä¸ºåŒ»å­¦å½±åƒåˆ†æä¸­çš„åŸŸé€‚åº”å’Œæ³›åŒ–é—®é¢˜æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Aggregating multi-site brain MRI data can enhance deep learning model training, but also introduces non-biological heterogeneity caused by site-specific variations (e.g., differences in scanner vendors, acquisition parameters, and imaging protocols) that can undermine generalizability. Recent retrospective MRI harmonization seeks to reduce such site effects by standardizing image style (e.g., intensity, contrast, noise patterns) while preserving anatomical content. However, existing methods often rely on limited paired traveling-subject data or fail to effectively disentangle style from anatomy. Furthermore, most current approaches address only single-sequence harmonization, restricting their use in real-world settings where multi-sequence MRI is routinely acquired. To this end, we introduce MMH, a unified framework for multi-site multi-sequence brain MRI harmonization that leverages biomedical semantic priors for sequence-aware style alignment. MMH operates in two stages: (1) a diffusion-based global harmonizer that maps MR images to a sequence-specific unified domain using style-agnostic gradient conditioning, and (2) a target-specific fine-tuner that adapts globally aligned images to desired target domains. A tri-planar attention BiomedCLIP encoder aggregates multi-view embeddings to characterize volumetric style information, allowing explicit disentanglement of image styles from anatomy without requiring paired data. Evaluations on 4,163 T1- and T2-weighted MRIs demonstrate MMH's superior harmonization over state-of-the-art methods in image feature clustering, voxel-level comparison, tissue segmentation, and downstream age and site classification.</p>
<h3 id="17-knowledge-based-learning-in-text-rag-and-image-rag">[17] <a href="https://arxiv.org/abs/2601.08226">Knowledge-based learning in Text-RAG and Image-RAG</a></h3>
<p><em>Alexander Shim, Khalil Saieh, Samuel Clarke</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é€šè¿‡å¯¹æ¯”åŸºäºEVA-ViTå›¾åƒç¼–ç å™¨çš„å¤šæ¨¡æ€æ–¹æ³•ä¸LLaMAæˆ–ChatGPT LLMï¼Œæ—¨åœ¨å‡å°‘åŒ»å­¦å½±åƒåˆ†æä¸­çš„å¹»è§‰é—®é¢˜å¹¶æå‡èƒ¸éƒ¨Xå…‰ç–¾ç—…æ£€æµ‹æ€§èƒ½ï¼Œå‘ç°åŸºäºæ–‡æœ¬çš„RAGèƒ½æœ‰æ•ˆé™ä½å¹»è§‰ç‡ï¼Œè€ŒåŸºäºå›¾åƒçš„RAGé€šè¿‡KNNæ–¹æ³•æé«˜äº†é¢„æµ‹ç½®ä¿¡åº¦å’Œæ ¡å‡†æ•ˆæœã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³åŒ»å­¦å½±åƒåˆ†æä¸­å¤šæ¨¡æ€æ–¹æ³•å­˜åœ¨çš„å¹»è§‰é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨èƒ¸éƒ¨Xå…‰ç–¾ç—…æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œæ¢ç´¢å¦‚ä½•æœ‰æ•ˆç»“åˆè§†è§‰Transformerå›¾åƒç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹æ¥æå‡è¯Šæ–­å‡†ç¡®æ€§å’Œå¯é æ€§ï¼ŒåŒæ—¶åº”å¯¹æ•°æ®ä¸å¹³è¡¡å’Œå¤æ‚å¤šé˜¶æ®µç»“æ„çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨åŸºäºEVA-ViTçš„å›¾åƒç¼–ç å™¨ä¸LLaMAæˆ–ChatGPT LLMç›¸ç»“åˆçš„å¤šæ¨¡æ€æ–¹æ³•ï¼Œä½¿ç”¨NIHèƒ¸éƒ¨Xå…‰å›¾åƒæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¯¹æ¯”äº†ä¸‰ç§ä¸åŒé…ç½®ï¼šåŸºäºå›¾åƒçš„RAGï¼ˆé‡‡ç”¨KNNæ–¹æ³•ï¼‰ã€åŸºäºæ–‡æœ¬çš„RAGï¼ˆåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†ä¿¡æ¯ï¼‰ä»¥åŠåŸºçº¿æ–¹æ³•ï¼Œä»¥ç³»ç»Ÿè¯„ä¼°ä¸åŒç­–ç•¥å¯¹å¹»è§‰é—®é¢˜å’Œç–¾ç—…æ£€æµ‹æ€§èƒ½çš„å½±å“ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæ–‡æœ¬çš„RAGèƒ½æœ‰æ•ˆåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†ä¿¡æ¯æ˜¾è‘—é™ä½å¹»è§‰é—®é¢˜ï¼Œè€ŒåŸºäºå›¾åƒçš„RAGé€šè¿‡KNNæ–¹æ³•æé«˜äº†é¢„æµ‹ç½®ä¿¡åº¦å’Œæ ¡å‡†æ•ˆæœï¼›GPT LLMåœ¨æ€§èƒ½è¡¨ç°ã€å¹»è§‰ç‡å’ŒæœŸæœ›æ ¡å‡†è¯¯å·®æ–¹é¢å‡ä¼˜äºLLaMAæ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ•´ä½“è¡¨ç°å’Œå¯é æ€§ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€åŒ»å­¦å½±åƒåˆ†æä¸­æ•°æ®ä¸å¹³è¡¡å’Œç»“æ„å¤æ‚æ€§çš„æŒ‘æˆ˜ï¼ŒåŒæ—¶è¯æ˜äº†ç»“åˆå¤–éƒ¨çŸ¥è¯†çš„RAGæ–¹æ³•å’ŒGPT LLMåœ¨å‡å°‘å¹»è§‰ã€æå‡æ ¡å‡†æ•ˆæœæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºæ›´å¯é çš„åŒ»å­¦è¯Šæ–­ç³»ç»Ÿæä¾›äº†é‡è¦å‚è€ƒï¼Œå¹¶å»ºè®®éœ€è¦å¤§è§„æ¨¡å®éªŒç¯å¢ƒå’Œå¹³è¡¡çš„ç”¨ä¾‹ç¤ºä¾‹æ¥è¿›ä¸€æ­¥ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>This research analyzed and compared the multi-modal approach in the Vision Transformer(EVA-ViT) based image encoder with the LlaMA or ChatGPT LLM to reduce the hallucination problem and detect diseases in chest x-ray images. In this research, we utilized the NIH Chest X-ray image to train the model and compared it in image-based RAG, text-based RAG, and baseline. [3] [5] In a result, the text-based RAG[2] e!ectively reduces the hallucination problem by using external knowledge information, and the image-based RAG improved the prediction con"dence and calibration by using the KNN methods. [4] Moreover, the GPT LLM showed better performance, a low hallucination rate, and better Expected Calibration Error(ECE) than Llama Llama-based model. This research shows the challenge of data imbalance, a complex multi-stage structure, but suggests a large experience environment and a balanced example of use.</p>
<h3 id="18-improving-zero-shot-adl-recognition-with-large-language-models-through-event-based-context-and-confidence">[18] <a href="https://arxiv.org/abs/2601.08241">Improving Zero-shot ADL Recognition with Large Language Models through Event-based Context and Confidence</a></h3>
<p><em>Michele Fiori, Gabriele Civitarese, Marco Colussi, Claudio Bettini</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºäº‹ä»¶åˆ†å‰²å’Œç½®ä¿¡åº¦ä¼°è®¡çš„é›¶æ ·æœ¬ADLè¯†åˆ«æ–¹æ³•ï¼Œé€šè¿‡äº‹ä»¶åˆ†å‰²å–ä»£ä¼ ç»Ÿæ—¶é—´åˆ†å‰²ï¼Œå¹¶å¼•å…¥é¢„æµ‹ç½®ä¿¡åº¦ä¼°è®¡æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ™ºèƒ½å®¶å±…æ´»åŠ¨è¯†åˆ«ä¸­çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬ADLè¯†åˆ«æ–¹æ³•ä¾èµ–æ—¶é—´åˆ†å‰²ç­–ç•¥ï¼Œè¿™ä¸LLMsçš„ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ä¸åŒ¹é…ï¼Œä¸”ç¼ºä¹é¢„æµ‹ç½®ä¿¡åº¦ä¼°è®¡æœºåˆ¶ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„åº”ç”¨æ•ˆæœå’Œå¯é æ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨äº‹ä»¶åˆ†å‰²ç­–ç•¥æ›¿ä»£ä¼ ç»Ÿæ—¶é—´åˆ†å‰²ï¼Œä½¿åˆ†å‰²è¾¹ç•Œä¸æ´»åŠ¨äº‹ä»¶çš„è‡ªç„¶è¾¹ç•Œå¯¹é½ï¼ŒåŒæ—¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„é¢„æµ‹ç½®ä¿¡åº¦ä¼°è®¡æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†æ­£ç¡®ä¸é”™è¯¯é¢„æµ‹ï¼Œæå‡æ¨¡å‹å¯é æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œäº‹ä»¶åˆ†å‰²æ–¹æ³•åœ¨å¤æ‚ç°å®æ•°æ®é›†ä¸ŠæŒç»­ä¼˜äºåŸºäºæ—¶é—´çš„LLMæ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†ç›‘ç£æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œå³ä½¿ä½¿ç”¨ç›¸å¯¹è¾ƒå°çš„LLMæ¨¡å‹ï¼ˆå¦‚Gemma 3 27Bï¼‰ä¹Ÿèƒ½å–å¾—ä¼˜å¼‚æ€§èƒ½ï¼Œä¸”æå‡ºçš„ç½®ä¿¡åº¦åº¦é‡èƒ½æœ‰æ•ˆåŒºåˆ†é¢„æµ‹æ­£ç¡®æ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜äº‹ä»¶åˆ†å‰²ç­–ç•¥èƒ½æ›´å¥½åœ°åˆ©ç”¨LLMsçš„ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡é›¶æ ·æœ¬ADLè¯†åˆ«æ€§èƒ½ï¼Œç½®ä¿¡åº¦ä¼°è®¡æœºåˆ¶å¢å¼ºäº†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ï¼Œä¸ºæ™ºèƒ½å®¶å±…ä¸­çš„æ´»åŠ¨è¯†åˆ«æä¾›äº†æ›´æœ‰æ•ˆçš„é›¶æ ·æœ¬è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Unobtrusive sensor-based recognition of Activities of Daily Living (ADLs) in smart homes by processing data collected from IoT sensing devices supports applications such as healthcare, safety, and energy management. Recent zero-shot methods based on Large Language Models (LLMs) have the advantage of removing the reliance on labeled ADL sensor data. However, existing approaches rely on time-based segmentation, which is poorly aligned with the contextual reasoning capabilities of LLMs. Moreover, existing approaches lack methods for estimating prediction confidence. This paper proposes to improve zero-shot ADL recognition with event-based segmentation and a novel method for estimating prediction confidence. Our experimental evaluation shows that event-based segmentation consistently outperforms time-based LLM approaches on complex, realistic datasets and surpasses supervised data-driven methods, even with relatively small LLMs (e.g., Gemma 3 27B). The proposed confidence measure effectively distinguishes correct from incorrect predictions.</p>
<h3 id="19-kidvis-do-multimodal-large-language-models-possess-the-visual-perceptual-capabilities-of-a-6-year-old">[19] <a href="https://arxiv.org/abs/2601.08292">KidVis: Do Multimodal Large Language Models Possess the Visual Perceptual Capabilities of a 6-Year-Old?</a></h3>
<p><em>Xianfeng Wang, Kaiwei Zhang, Qi Jia, Zijian Chen, Guangtao Zhai, Xiongkuo Min</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶å¼•å…¥KidVisåŸºå‡†æµ‹è¯•ï¼ŒåŸºäºäººç±»è§†è§‰å‘å±•ç†è®ºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€è§†è§‰èƒ½åŠ›ï¼Œå‘ç°å½“å‰æœ€å…ˆè¿›çš„MLLMsåœ¨å„¿ç«¥å·²æŒæ¡çš„åŸå­è§†è§‰èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œä¸”å‚æ•°ç¼©æ”¾æ— æ³•çº¿æ€§æå‡è¿™äº›åŸºç¡€èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é«˜çº§æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å°šä¸æ¸…æ¥šå®ƒä»¬æ˜¯å¦å…·å¤‡ä¸äººç±»ç›´è§‰ç›¸å½“çš„åŸºç¡€è§†è§‰åŸè¯­èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶MLLMsæ˜¯å¦æ‹¥æœ‰ç±»ä¼¼6-7å²å„¿ç«¥å·²æŒæ¡çš„åŸºæœ¬è§†è§‰èƒ½åŠ›ï¼Œä»¥è¯„ä¼°å…¶å¹¿ä¹‰è§†è§‰æ™ºèƒ½çš„ç”Ÿç†åŸºç¡€ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥KidVisåŸºå‡†æµ‹è¯•ï¼Œè¯¥åŸºå‡†åŸºäºäººç±»è§†è§‰å‘å±•ç†è®ºï¼Œå°†è§†è§‰æ™ºèƒ½è§£æ„ä¸ºå…­ä¸ªåŸå­èƒ½åŠ›ï¼šä¸“æ³¨åŠ›ã€è¿½è¸ªã€è¾¨åˆ«ã€è®°å¿†ã€ç©ºé—´å’Œé—­åˆèƒ½åŠ›ã€‚è¿™äº›èƒ½åŠ›æ„æˆ10ä¸ªä½è¯­ä¹‰ä¾èµ–çš„è§†è§‰ä»»åŠ¡ç±»åˆ«ï¼Œç”¨äºè¯„ä¼°20ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸äººç±»ç”Ÿç†åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°ç»“æœæ˜¾ç¤ºæ˜¾è‘—æ€§èƒ½å·®è·ï¼šäººç±»å„¿ç«¥å¹³å‡å¾—åˆ†æ¥è¿‘å®Œç¾ï¼ˆ95.32ï¼‰ï¼Œè€Œæœ€å…ˆè¿›çš„GPT-5ä»…è·å¾—67.33åˆ†ã€‚ç ”ç©¶è§‚å¯Ÿåˆ°"ç¼©æ”¾å®šå¾‹æ‚–è®º"ï¼šå•çº¯å¢åŠ æ¨¡å‹å‚æ•°æ— æ³•çº¿æ€§æå‡è¿™äº›åŸºç¡€è§†è§‰èƒ½åŠ›ã€‚æ‰€æœ‰20ä¸ªMLLMsåœ¨å„¿ç«¥å·²æŒæ¡çš„åŸå­è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ä¸ç†æƒ³ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯å®å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å°½ç®¡å…·å¤‡é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œä½†ç¼ºä¹å®ç°å¹¿ä¹‰è§†è§‰æ™ºèƒ½æ‰€éœ€çš„åŸºæœ¬ç”Ÿç†æ„ŸçŸ¥åŸè¯­ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†å•çº¯é€šè¿‡å‚æ•°æ‰©å±•å°±èƒ½å®ç°å…¨é¢è§†è§‰æ™ºèƒ½çš„å‡è®¾ï¼Œè¡¨æ˜éœ€è¦æ–°çš„æ¶æ„æˆ–è®­ç»ƒèŒƒå¼æ¥å¼¥è¡¥åŸºç¡€è§†è§‰èƒ½åŠ›çš„ä¸è¶³ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>While Multimodal Large Language Models (MLLMs) have demonstrated impressive proficiency in high-level reasoning tasks, such as complex diagrammatic interpretation, it remains an open question whether they possess the fundamental visual primitives comparable to human intuition. To investigate this, we introduce KidVis, a novel benchmark grounded in the theory of human visual development. KidVis deconstructs visual intelligence into six atomic capabilities - Concentration, Tracking, Discrimination, Memory, Spatial, and Closure - already possessed by 6-7 year old children, comprising 10 categories of low-semantic-dependent visual tasks. Evaluating 20 state-of-the-art MLLMs against a human physiological baseline reveals a stark performance disparity. Results indicate that while human children achieve a near-perfect average score of 95.32, the state-of-the-art GPT-5 attains only 67.33. Crucially, we observe a "Scaling Law Paradox": simply increasing model parameters fails to yield linear improvements in these foundational visual capabilities. This study confirms that current MLLMs, despite their reasoning prowess, lack the essential physiological perceptual primitives required for generalized visual intelligence.</p>
<h3 id="20-enhancing-image-quality-assessment-ability-of-lmms-via-retrieval-augmented-generation">[20] <a href="https://arxiv.org/abs/2601.08311">Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation</a></h3>
<p><em>Kang Fu, Huiyu Duan, Zicheng Zhang, Yucheng Zhu, Jun Zhao, Xiongkuo Min, Jia Wang, Guangtao Zhai</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºIQARAGï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯æå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒè´¨é‡è¯„ä¼°ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä¸ºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•æä¾›äº†èµ„æºé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒè´¨é‡è¯„ä¼°ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä½†å®ç°æœ€å…ˆè¿›æ€§èƒ½é€šå¸¸éœ€è¦è®¡ç®—æˆæœ¬é«˜æ˜‚çš„å¾®è°ƒæ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•æ—¨åœ¨å°†è´¨é‡ç›¸å…³æ ‡è®°çš„è¾“å‡ºåˆ†å¸ƒä¸å›¾åƒè´¨é‡æ°´å¹³å¯¹é½ï¼Œå› æ­¤éœ€è¦å¼€å‘æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> IQARAGé‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼ŒåŒ…å«æ£€ç´¢ç‰¹å¾æå–ã€å›¾åƒæ£€ç´¢ä»¥åŠé›†æˆä¸è´¨é‡åˆ†æ•°ç”Ÿæˆä¸‰ä¸ªå…³é”®é˜¶æ®µï¼Œé€šè¿‡æ£€ç´¢è¯­ä¹‰ç›¸ä¼¼ä½†è´¨é‡å˜åŒ–çš„å‚è€ƒå›¾åƒåŠå…¶å¹³å‡æ„è§åˆ†æ•°ï¼Œå¹¶å°†è¿™äº›æ£€ç´¢åˆ°çš„å›¾åƒä¸è¾“å…¥å›¾åƒæ•´åˆåˆ°ç‰¹å®šæç¤ºä¸­ï¼Œä¸ºLMMæä¾›è§†è§‰æ„ŸçŸ¥é”šç‚¹ã€‚</p>
<p><strong>Result:</strong> åœ¨KADIDã€KonIQã€LIVE Challengeå’ŒSPAQç­‰å¤šä¸ªå¤šæ ·åŒ–å›¾åƒè´¨é‡è¯„ä¼°æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒIQARAGæœ‰æ•ˆæå‡äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å›¾åƒè´¨é‡è¯„ä¼°æ€§èƒ½ï¼Œä¸ºè´¨é‡è¯„ä¼°ä»»åŠ¡æä¾›äº†èµ„æºé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ— éœ€è®­ç»ƒçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶åœ¨æå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å›¾åƒè´¨é‡è¯„ä¼°èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¼ ç»Ÿè®¡ç®—å¯†é›†å‹å¾®è°ƒæ–¹æ³•æä¾›äº†å®ç”¨ä¸”é«˜æ•ˆçš„æ›¿ä»£è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Large Multimodal Models (LMMs) have recently shown remarkable promise in low-level visual perception tasks, particularly in Image Quality Assessment (IQA), demonstrating strong zero-shot capability. However, achieving state-of-the-art performance often requires computationally expensive fine-tuning methods, which aim to align the distribution of quality-related token in output with image quality levels. Inspired by recent training-free works for LMM, we introduce IQARAG, a novel, training-free framework that enhances LMMs' IQA ability. IQARAG leverages Retrieval-Augmented Generation (RAG) to retrieve some semantically similar but quality-variant reference images with corresponding Mean Opinion Scores (MOSs) for input image. These retrieved images and input image are integrated into a specific prompt. Retrieved images provide the LMM with a visual perception anchor for IQA task. IQARAG contains three key phases: Retrieval Feature Extraction, Image Retrieval, and Integration &amp; Quality Score Generation. Extensive experiments across multiple diverse IQA datasets, including KADID, KonIQ, LIVE Challenge, and SPAQ, demonstrate that the proposed IQARAG effectively boosts the IQA performance of LMMs, offering a resource-efficient alternative to fine-tuning for quality assessment.</p>
<h3 id="21-um-text-a-unified-multimodal-model-for-image-understanding">[21] <a href="https://arxiv.org/abs/2601.08321">UM-Text: A Unified Multimodal Model for Image Understanding</a></h3>
<p><em>Lichen Ma, Xiaolong Fu, Gaojing Zhou, Zipeng Guo, Ting Zhu, Yichun Liu, Yu Shi, Jason Li, Junshi Huang</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºUM-Textï¼Œä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å®ç°ä¸Šä¸‹æ–‡ç†è§£å’Œè§†è§‰æ–‡æœ¬ç¼–è¾‘ï¼Œè§£å†³äº†è§†è§‰æ–‡æœ¬ç”Ÿæˆä¸­é£æ ¼ä¸€è‡´æ€§çš„æŒ‘æˆ˜ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰æ–‡æœ¬ç¼–è¾‘æ–¹æ³•é€šå¸¸éœ€è¦å¤æ‚æ­¥éª¤æŒ‡å®šæ–‡æœ¬å†…å®¹å’Œå±æ€§ï¼ˆå¦‚å­—ä½“å¤§å°ã€é¢œè‰²ã€å¸ƒå±€ï¼‰ï¼Œè€Œæœªå……åˆ†è€ƒè™‘ä¸å‚è€ƒå›¾åƒçš„é£æ ¼ä¸€è‡´æ€§ï¼Œè¿™é™åˆ¶äº†è‡ªç„¶è¯­è¨€æŒ‡ä»¤é©±åŠ¨çš„è§†è§‰æ–‡æœ¬ç”Ÿæˆæ•ˆæœã€‚</p>
<p><strong>Method:</strong> æå‡ºUM-Textç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¼•å…¥è§†è§‰è¯­è¨€æ¨¡å‹å¤„ç†æŒ‡ä»¤å’Œå‚è€ƒå›¾åƒä»¥ç²¾å¿ƒè®¾è®¡æ–‡æœ¬å†…å®¹å’Œå¸ƒå±€ï¼Œè®¾è®¡UM-Encoderè‡ªåŠ¨é…ç½®å¤šç§æ¡ä»¶ä¿¡æ¯çš„åµŒå…¥ç»„åˆï¼Œé‡‡ç”¨åŒºåŸŸä¸€è‡´æ€§æŸå¤±åœ¨æ½œåœ¨ç©ºé—´å’ŒRGBç©ºé—´æä¾›å­—å½¢ç”Ÿæˆç›‘ç£ï¼Œå¹¶å¼€å‘ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥å¢å¼ºæ€§èƒ½ï¼ŒåŒæ—¶è´¡çŒ®åŒ…å«20ä¸‡å¼ å¤šæ ·åŒ–åœºæ™¯è§†è§‰æ–‡æœ¬å›¾åƒçš„UM-DATA-200Kæ•°æ®é›†ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®šæ€§å’Œå®šé‡ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”Ÿæˆçš„è§†è§‰æ–‡æœ¬å›¾åƒåœ¨å‡†ç¡®æ€§å’Œä¸å‚è€ƒå›¾åƒçš„å’Œè°åº¦æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¡†æ¶æœ‰æ•ˆè§£å†³äº†è§†è§‰æ–‡æœ¬ç¼–è¾‘ä¸­çš„é£æ ¼ä¸€è‡´æ€§é—®é¢˜ï¼Œæå‡ºçš„åŒºåŸŸä¸€è‡´æ€§æŸå¤±å’Œä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ä¸ºå­—å½¢ç”Ÿæˆæä¾›äº†æ›´æœ‰æ•ˆçš„ç›‘ç£ï¼Œå¤§è§„æ¨¡æ•°æ®é›†çš„è´¡çŒ®ä¹Ÿä¸ºè¯¥é¢†åŸŸç ”ç©¶æä¾›äº†é‡è¦èµ„æºï¼Œæ¨åŠ¨äº†è‡ªç„¶è¯­è¨€æŒ‡ä»¤é©±åŠ¨çš„è§†è§‰æ–‡æœ¬ç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.</p>
<h3 id="22-coma-contextual-massing-generation-with-vision-language-models">[22] <a href="https://arxiv.org/abs/2601.08464">CoMa: Contextual Massing Generation with Vision-Language Models</a></h3>
<p><em>Evgenii Maslov, Valentin Khrulkov, Anastasia Volkova, Anton Gusarov, Andrey Kuznetsov, Ivan Oseledets</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–å»ºç­‘ä½“é‡ç”Ÿæˆæ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†CoMa-20Kæ•°æ®é›†æ¥è§£å†³æ•°æ®é©±åŠ¨å»ºç­‘è®¾è®¡ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œé€šè¿‡å°†ä½“é‡ç”Ÿæˆæ„å»ºä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ¡ä»¶ä»»åŠ¡ï¼Œå±•ç¤ºäº†æ•°æ®é©±åŠ¨æ–¹æ³•åœ¨å»ºç­‘æ¦‚å¿µè®¾è®¡ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å»ºç­‘å’ŒåŸå¸‚è§„åˆ’ä¸­çš„æ¦‚å¿µè®¾è®¡é˜¶æ®µï¼Œç‰¹åˆ«æ˜¯å»ºç­‘ä½“é‡è®¾è®¡ï¼Œå…·æœ‰é«˜åº¦å¤æ‚æ€§ä¸”ä¸¥é‡ä¾èµ–è®¾è®¡å¸ˆçš„ç›´è§‰å’Œæ‰‹åŠ¨å·¥ä½œï¼Œè€Œæ•°æ®é©±åŠ¨æ–¹æ³•é¢ä¸´çš„ä¸»è¦éšœç¢æ˜¯ç¼ºä¹åˆé€‚çš„æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†è‡ªåŠ¨åŒ–è®¾è®¡æ¡†æ¶çš„å‘å±•ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºåŠŸèƒ½éœ€æ±‚å’Œåœºåœ°ä¸Šä¸‹æ–‡çš„è‡ªåŠ¨åŒ–å»ºç­‘ä½“é‡ç”Ÿæˆæ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†CoMa-20Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¯¦ç»†çš„ä½“é‡å‡ ä½•ã€ç»æµå’Œç¨‹åºæ•°æ®ä»¥åŠå¼€å‘åœºåœ°åœ¨ç°æœ‰åŸå¸‚ç¯å¢ƒä¸­çš„è§†è§‰è¡¨ç¤ºï¼Œé€šè¿‡å°†ä½“é‡ç”Ÿæˆæ„å»ºä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ¡ä»¶ä»»åŠ¡è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†å¾®è°ƒæ¨¡å‹å’Œå¤§å‹é›¶æ ·æœ¬æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒæ­ç¤ºäº†å»ºç­‘ä½“é‡ç”Ÿæˆä»»åŠ¡çš„å†…åœ¨å¤æ‚æ€§ï¼ŒåŒæ—¶è¯æ˜äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆä¸Šä¸‹æ–‡æ•æ„Ÿçš„ä½“é‡é€‰é¡¹æ–¹é¢çš„æ½œåŠ›ï¼Œæ•°æ®é›†å’Œåˆ†æä¸ºæ•°æ®é©±åŠ¨çš„å»ºç­‘è®¾è®¡å»ºç«‹äº†åŸºç¡€åŸºå‡†ï¼Œå¹¶çªå‡ºäº†è¯¥é¢†åŸŸæœªæ¥ç ”ç©¶çš„é‡è¦æœºä¼šã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºæ•°æ®é©±åŠ¨çš„å»ºç­‘è®¾è®¡å»ºç«‹äº†é‡è¦çš„åŸºå‡†å’Œèµ„æºï¼Œå±•ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è®¾è®¡ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ï¼ŒåŒæ—¶å¼ºè°ƒäº†å»ºç­‘ä½“é‡ç”Ÿæˆä»»åŠ¡çš„æŒ‘æˆ˜æ€§ï¼Œä¸ºæœªæ¥åœ¨è‡ªåŠ¨åŒ–å»ºç­‘è®¾è®¡å’ŒåŸå¸‚è§„åˆ’æ–¹é¢çš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>The conceptual design phase in architecture and urban planning, particularly building massing, is complex and heavily reliant on designer intuition and manual effort. To address this, we propose an automated framework for generating building massing based on functional requirements and site context. A primary obstacle to such data-driven methods has been the lack of suitable datasets. Consequently, we introduce the CoMa-20K dataset, a comprehensive collection that includes detailed massing geometries, associated economical and programmatic data, and visual representations of the development site within its existing urban context. We benchmark this dataset by formulating massing generation as a conditional task for Vision-Language Models (VLMs), evaluating both fine-tuned and large zero-shot models. Our experiments reveal the inherent complexity of the task while demonstrating the potential of VLMs to produce context-sensitive massing options. The dataset and analysis establish a foundational benchmark and highlight significant opportunities for future research in data-driven architectural design.</p>
<h3 id="23-tissue-classification-and-whole-slide-images-analysis-via-modeling-of-the-tumor-microenvironment-and-biological-pathways">[23] <a href="https://arxiv.org/abs/2601.08336">Tissue Classification and Whole-Slide Images Analysis via Modeling of the Tumor Microenvironment and Biological Pathways</a></h3>
<p><em>Junzhuo Liu, Xuemei Du, Daniel Reisenbuchler, Ye Chen, Markus Eckstein, Christian Matek, Friedrich Feuerhake, Dorit Merhof</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†BioMorphNetï¼Œä¸€ç§å¤šæ¨¡æ€ç½‘ç»œï¼Œé€šè¿‡è‡ªåŠ¨æ•´åˆç»„ç»‡å½¢æ€å­¦ç‰¹å¾å’Œç©ºé—´åŸºå› è¡¨è¾¾æ•°æ®æ¥æ”¯æŒç»„ç»‡åˆ†ç±»å’Œå·®å¼‚åŸºå› åˆ†æï¼Œåœ¨å¤šç§ç™Œç—‡æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†åˆ†ç±»æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨å•ä¸ªåŸºå› åºåˆ—å’Œåˆ‡ç‰‡çº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ï¼Œå¯¹ç©ºé—´è½¬å½•ç»„å­¦å’Œæ–‘å—çº§åˆ«åº”ç”¨å…³æ³¨æœ‰é™ï¼Œè¿™é™åˆ¶äº†å…¨åˆ‡ç‰‡å›¾åƒä¸åŸºå› è¡¨è¾¾è°±çš„æ•´åˆåœ¨ç²¾å‡†ä¸´åºŠè¯Šæ–­å’Œç™Œç—‡è¿›å±•ç ”ç©¶ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Method:</strong> BioMorphNetæ„å»ºå›¾æ¨¡å‹æ¥å»ºæ¨¡ç›®æ ‡æ–‘å—ä¸å…¶é‚»åŸŸçš„å…³ç³»ï¼ŒåŸºäºå½¢æ€å­¦å’Œåˆ†å­æ°´å¹³çš„ç›¸ä¼¼æ€§è°ƒæ•´å“åº”å¼ºåº¦ä»¥æ›´å¥½è¡¨å¾è‚¿ç˜¤å¾®ç¯å¢ƒï¼›ä»ç©ºé—´è½¬å½•ç»„æ•°æ®ä¸­æå–ä¸´åºŠé€šè·¯ç‰¹å¾ä½œä¸ºç»„ç»‡å½¢æ€ä¸åŸºå› è¡¨è¾¾çš„æ¡¥æ¢ï¼›è®¾è®¡å¯å­¦ä¹ é€šè·¯æ¨¡å—è‡ªåŠ¨æ¨¡æ‹Ÿç”Ÿç‰©é€šè·¯å½¢æˆè¿‡ç¨‹ï¼Œä¸ºç°æœ‰ä¸´åºŠé€šè·¯æä¾›è¡¥å……è¡¨ç¤ºã€‚</p>
<p><strong>Result:</strong> ä¸æœ€æ–°çš„å½¢æ€-åŸºå› å¤šæ¨¡æ€æ–¹æ³•ç›¸æ¯”ï¼ŒBioMorphNetåœ¨å‰åˆ—è…ºç™Œã€ç»“ç›´è‚ ç™Œå’Œä¹³è…ºç™Œæ•°æ®é›†ä¸Šçš„å¹³å‡åˆ†ç±»æŒ‡æ ‡åˆ†åˆ«æå‡äº†2.67%ã€5.48%å’Œ6.29%ï¼Œä¸ä»…å‡†ç¡®åˆ†ç±»WSIå†…çš„ç»„ç»‡ç±»åˆ«ä»¥æ”¯æŒè‚¿ç˜¤å®šä½ï¼Œè¿˜èƒ½åŸºäºé¢„æµ‹ç½®ä¿¡åº¦åˆ†æç»„ç»‡ç±»åˆ«é—´çš„å·®å¼‚åŸºå› è¡¨è¾¾ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºç»„ç»‡å½¢æ€å­¦ä¸ç©ºé—´åŸºå› è¡¨è¾¾çš„æ•´åˆæä¾›äº†åˆ›æ–°æ¡†æ¶ï¼Œä¸ä»…æå‡äº†ç™Œç—‡ç»„ç»‡åˆ†ç±»çš„å‡†ç¡®æ€§ï¼Œè¿˜æ”¯æŒå·®å¼‚åŸºå› åˆ†æå’Œæ½œåœ¨è‚¿ç˜¤ç”Ÿç‰©æ ‡å¿—ç‰©çš„å‘ç°ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦æ•°æ®åˆ†æåœ¨ç²¾å‡†åŒ»ç–—ä¸­çš„åº”ç”¨ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Automatic integration of whole slide images (WSIs) and gene expression profiles has demonstrated substantial potential in precision clinical diagnosis and cancer progression studies. However, most existing studies focus on individual gene sequences and slide level classification tasks, with limited attention to spatial transcriptomics and patch level applications. To address this limitation, we propose a multimodal network, BioMorphNet, which automatically integrates tissue morphological features and spatial gene expression to support tissue classification and differential gene analysis. For considering morphological features, BioMorphNet constructs a graph to model the relationships between target patches and their neighbors, and adjusts the response strength based on morphological and molecular level similarity, to better characterize the tumor microenvironment. In terms of multimodal interactions, BioMorphNet derives clinical pathway features from spatial transcriptomic data based on a predefined pathway database, serving as a bridge between tissue morphology and gene expression. In addition, a novel learnable pathway module is designed to automatically simulate the biological pathway formation process, providing a complementary representation to existing clinical pathways. Compared with the latest morphology gene multimodal methods, BioMorphNet's average classification metrics improve by 2.67%, 5.48%, and 6.29% for prostate cancer, colorectal cancer, and breast cancer datasets, respectively. BioMorphNet not only classifies tissue categories within WSIs accurately to support tumor localization, but also analyzes differential gene expression between tissue categories based on prediction confidence, contributing to the discovery of potential tumor biomarkers.</p>
<h3 id="24-videohedge-entropy-based-hallucination-detection-for-video-vlms-via-semantic-clustering-and-spatiotemporal-perturbations">[24] <a href="https://arxiv.org/abs/2601.08557">VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations</a></h3>
<p><em>Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, PÃ¥l Halvorsen</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VideoHEDGEæ¡†æ¶ï¼Œç”¨äºæ£€æµ‹è§†é¢‘è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œé€šè¿‡æ‰©å±•åŸºäºç†µçš„å¯é æ€§ä¼°è®¡æ–¹æ³•åˆ°æ—¶ç©ºç»“æ„åŒ–è¾“å…¥ï¼Œå¹¶å¼•å…¥è§†è§‰å¢å¼ºè¯­ä¹‰ç†µ(VASE)æŒ‡æ ‡ï¼Œåœ¨å¤šä¸ª7Bå‚æ•°è§†é¢‘VLMä¸Šå®ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„å¹»è§‰æ£€æµ‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†é¢‘è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å¹»è§‰ç°è±¡é¢‘ç¹ä¸”ç½®ä¿¡åº¦é«˜ï¼Œè€Œç°æœ‰çš„ä¸ç¡®å®šæ€§åº¦é‡æ–¹æ³•å¾€å¾€æ— æ³•ä¸æ­£ç¡®æ€§å¯¹é½ï¼Œè¿™æ„æˆäº†å½“å‰è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­çš„å…³é”®æŒ‘æˆ˜å’Œç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> VideoHEDGEæ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œé€šè¿‡ä»åŸå§‹è§†é¢‘ç‰‡æ®µåŠå…¶å…‰åº¦å’Œæ—¶ç©ºæ‰°åŠ¨å˜ä½“ä¸­ç”ŸæˆåŸºçº¿ç­”æ¡ˆå’Œå¤šä¸ªé«˜æ¸©é‡‡æ ·å“åº”ï¼Œç„¶åä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨ç†æˆ–åµŒå…¥æ–¹æ³•å°†æ–‡æœ¬è¾“å‡ºèšç±»ä¸ºè¯­ä¹‰å‡è®¾ï¼Œæœ€ç»ˆåŸºäºèšç±»çº§æ¦‚ç‡è´¨é‡è®¡ç®—ä¸‰ç§å¯é æ€§åˆ†æ•°ï¼šè¯­ä¹‰ç†µã€RadFlagå’Œè§†è§‰å¢å¼ºè¯­ä¹‰ç†µã€‚</p>
<p><strong>Result:</strong> åœ¨SoccerChatåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨ä¸‰ä¸ª7Bå‚æ•°è§†é¢‘VLMè¿›è¡Œè¯„ä¼°ï¼Œè§†è§‰å¢å¼ºè¯­ä¹‰ç†µåœ¨è¾ƒå¤§å¤±çœŸé¢„ç®—ä¸‹å§‹ç»ˆè·å¾—æœ€é«˜çš„ROC-AUCæ€§èƒ½ï¼Œè€Œè¯­ä¹‰ç†µå’ŒRadFlagå¾€å¾€æ¥è¿‘éšæœºæ°´å¹³ï¼›åµŒå…¥èšç±»åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶è¾¾åˆ°ä¸è‡ªç„¶è¯­è¨€æ¨ç†èšç±»ç›¸å½“çš„æ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜è§†è§‰å¢å¼ºè¯­ä¹‰ç†µæ˜¯æ£€æµ‹è§†é¢‘VLMå¹»è§‰çš„æœ‰æ•ˆæŒ‡æ ‡ï¼ŒåµŒå…¥èšç±»æä¾›äº†è®¡ç®—æ•ˆç‡çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé¢†åŸŸå¾®è°ƒè™½èƒ½å‡å°‘å¹»è§‰é¢‘ç‡ä½†å¯¹æ ¡å‡†æ”¹å–„æœ‰é™ï¼ŒåŒæ—¶å‘å¸ƒçš„hedge-benchåº“æ”¯æŒå¯å¤ç°å’Œå¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .</p>
<h3 id="25-semantic-misalignment-in-vision-language-models-under-perceptual-degradation">[25] <a href="https://arxiv.org/abs/2601.08355">Semantic Misalignment in Vision-Language Models under Perceptual Degradation</a></h3>
<p><em>Guo Cheng</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶ç³»ç»Ÿåˆ†æäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ„ŸçŸ¥é€€åŒ–ä¸‹çš„è¯­ä¹‰å¯¹é½é—®é¢˜ï¼Œæ­ç¤ºäº†åƒç´ çº§é²æ£’æ€§ä¸å¤šæ¨¡æ€è¯­ä¹‰å¯é æ€§ä¹‹é—´çš„è„±èŠ‚ï¼Œå¹¶æå‡ºäº†ä¸€å¥—è¯­è¨€çº§é”™ä½åº¦é‡æ ‡å‡†æ¥é‡åŒ–å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„VLMå¤±æ•ˆã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨ç°å®æ„ŸçŸ¥é€€åŒ–ä¸‹çš„é²æ£’æ€§å°šæœªå¾—åˆ°å……åˆ†ç†è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’Œå…·èº«AIç­‰å®‰å…¨å…³é”®ç³»ç»Ÿä¸­ï¼Œæ„ŸçŸ¥ä¸ç¡®å®šæ€§å¯èƒ½å¯¼è‡´ä¸¥é‡çš„è¯­ä¹‰æ¨ç†å’Œå†³ç­–å¤±è¯¯ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨Cityscapesæ•°æ®é›†ä¸Šçš„è¯­ä¹‰åˆ†å‰²ä½œä¸ºä»£è¡¨æ€§æ„ŸçŸ¥æ¨¡å—ï¼Œå¼•å…¥æ„ŸçŸ¥ç°å®æ€§é€€åŒ–ï¼Œè¿™äº›é€€åŒ–ä»…å¯¼è‡´ä¼ ç»Ÿåˆ†å‰²æŒ‡æ ‡é€‚åº¦ä¸‹é™ï¼Œä½†ä¼šå¼•å‘ä¸‹æ¸¸VLMè¡Œä¸ºä¸¥é‡å¤±æ•ˆï¼›åŒæ—¶æå‡ºäº†ä¸€å¥—è¯­è¨€çº§é”™ä½åº¦é‡æ ‡å‡†ï¼Œç”¨äºé‡åŒ–å¹»è§‰ã€å…³é”®é—æ¼å’Œå®‰å…¨è¯¯åˆ¤ç­‰ç°è±¡ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ„ŸçŸ¥é€€åŒ–åœ¨ä¼ ç»Ÿåˆ†å‰²æŒ‡æ ‡ä¸Šä»…é€ æˆé€‚åº¦ä¸‹é™ï¼Œå´å¯¼è‡´ä¸‹æ¸¸VLMå‡ºç°ä¸¥é‡å¤±æ•ˆï¼ŒåŒ…æ‹¬å¹»è§‰å¯¹è±¡æåŠã€å®‰å…¨å…³é”®å®ä½“é—æ¼ä»¥åŠä¸ä¸€è‡´çš„å®‰å…¨åˆ¤æ–­ï¼›ç ”ç©¶è¿˜æ­ç¤ºäº†åƒç´ çº§é²æ£’æ€§ä¸å¤šæ¨¡æ€è¯­ä¹‰å¯é æ€§ä¹‹é—´çš„æ˜æ˜¾è„±èŠ‚ï¼Œè¿™ä¸€ç°è±¡åœ¨å¤šä¸ªå¯¹æ¯”æ€§å’Œç”Ÿæˆæ€§VLMä¸­å¾—åˆ°éªŒè¯ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰åŸºäºVLMçš„ç³»ç»Ÿåœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„å…³é”®å±€é™æ€§ï¼Œå¼ºè°ƒäº†è¯„ä¼°æ¡†æ¶éœ€è¦æ˜ç¡®è€ƒè™‘æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„å¿…è¦æ€§ï¼Œä¸ºæœªæ¥å¼€å‘æ›´å¯é çš„è§†è§‰è¯­è¨€ç³»ç»Ÿæä¾›äº†é‡è¦è§è§£å’Œæ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.</p>
<h3 id="26-saferedir-prompt-embedding-redirection-for-robust-unlearning-in-image-generation-models">[26] <a href="https://arxiv.org/abs/2601.08623">SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models</a></h3>
<p><em>Renyang Liu, Kangjie Chen, Han Qiu, Jie Zhang, Kwok-Yan Lam, Tianwei Zhang, See-Kiong Ng</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>SafeRediræ˜¯ä¸€ä¸ªè½»é‡çº§æ¨ç†æ—¶æ¡†æ¶ï¼Œé€šè¿‡æç¤ºåµŒå…¥é‡å®šå‘å®ç°é²æ£’æ€§é—å¿˜ï¼Œæ— éœ€ä¿®æ”¹åº•å±‚å›¾åƒç”Ÿæˆæ¨¡å‹å³å¯æœ‰æ•ˆæ¶ˆé™¤æœ‰å®³æ¦‚å¿µï¼ŒåŒæ—¶ä¿æŒè‰¯æ€§ç”Ÿæˆçš„è´¨é‡å’Œè¯­ä¹‰å®Œæ•´æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å›¾åƒç”Ÿæˆæ¨¡å‹å®¹æ˜“è®°å¿†è®­ç»ƒæ•°æ®ä¸­çš„ä¸è‰¯æ¦‚å¿µï¼Œå¯¼è‡´ç”Ÿæˆä¸å®‰å…¨å†…å®¹å’Œå—ç‰ˆæƒä¿æŠ¤çš„è‰ºæœ¯é£æ ¼ï¼Œç°æœ‰é—å¿˜æ–¹æ³•å­˜åœ¨éœ€è¦æ˜‚è´µé‡æ–°è®­ç»ƒã€é™ä½è‰¯æ€§ç”Ÿæˆè´¨é‡æˆ–æ— æ³•æŠµæŠ—æç¤ºæ”¹å†™å’Œå¯¹æŠ—æ”»å‡»ç­‰å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> SafeRediræ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šç”¨äºè¯†åˆ«ä¸å®‰å…¨ç”Ÿæˆè½¨è¿¹çš„æ½œåœ¨æ„ŸçŸ¥å¤šæ¨¡æ€å®‰å…¨åˆ†ç±»å™¨ï¼Œä»¥åŠç”¨äºç²¾ç¡®è¯­ä¹‰é‡å®šå‘çš„ä»¤ç‰Œçº§å¢é‡ç”Ÿæˆå™¨ï¼Œåè€…é…å¤‡ä»¤ç‰Œæ©ç å’Œè‡ªé€‚åº”ç¼©æ”¾è¾…åŠ©é¢„æµ‹å™¨ä»¥å®šä½å’Œè°ƒèŠ‚å¹²é¢„ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒSafeRediråœ¨å¤šä¸ªä»£è¡¨æ€§é—å¿˜ä»»åŠ¡ä¸­å®ç°äº†æœ‰æ•ˆçš„é—å¿˜èƒ½åŠ›ã€é«˜è¯­ä¹‰å’Œæ„ŸçŸ¥ä¿æŒã€é²æ£’çš„å›¾åƒè´¨é‡ä»¥åŠå¢å¼ºçš„æŠ—å¯¹æŠ—æ”»å‡»èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šç§æ‰©æ•£éª¨å¹²å’Œç°æœ‰é—å¿˜æ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§æ— éœ€ä¿®æ”¹åº•å±‚æ¨¡å‹çš„è½»é‡çº§æ¨ç†æ—¶é—å¿˜è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å³æ’å³ç”¨å…¼å®¹æ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ï¼Œä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹çš„å®‰å…¨éƒ¨ç½²æä¾›äº†æ–°çš„æŠ€æœ¯é€”å¾„ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆè´¨é‡å’Œè¯­ä¹‰å®Œæ•´æ€§ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Image generation models (IGMs), while capable of producing impressive and creative content, often memorize a wide range of undesirable concepts from their training data, leading to the reproduction of unsafe content such as NSFW imagery and copyrighted artistic styles. Such behaviors pose persistent safety and compliance risks in real-world deployments and cannot be reliably mitigated by post-hoc filtering, owing to the limited robustness of such mechanisms and a lack of fine-grained semantic control. Recent unlearning methods seek to erase harmful concepts at the model level, which exhibit the limitations of requiring costly retraining, degrading the quality of benign generations, or failing to withstand prompt paraphrasing and adversarial attacks. To address these challenges, we introduce SafeRedir, a lightweight inference-time framework for robust unlearning via prompt embedding redirection. Without modifying the underlying IGMs, SafeRedir adaptively routes unsafe prompts toward safe semantic regions through token-level interventions in the embedding space. The framework comprises two core components: a latent-aware multi-modal safety classifier for identifying unsafe generation trajectories, and a token-level delta generator for precise semantic redirection, equipped with auxiliary predictors for token masking and adaptive scaling to localize and regulate the intervention. Empirical results across multiple representative unlearning tasks demonstrate that SafeRedir achieves effective unlearning capability, high semantic and perceptual preservation, robust image quality, and enhanced resistance to adversarial attacks. Furthermore, SafeRedir generalizes effectively across a variety of diffusion backbones and existing unlearned models, validating its plug-and-play compatibility and broad applicability. Code and data are available at https://github.com/ryliu68/SafeRedir.</p>
<h3 id="27-edge-optimized-multimodal-learning-for-uav-video-understanding-via-blip-2">[27] <a href="https://arxiv.org/abs/2601.08408">Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2</a></h3>
<p><em>Yizhan Feng, Hichem Snoussi, Jing Teng, Jian Liu, Yuyang Wang, Abel Cherouat, Tian Wang</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºBLIP-2çš„è½»é‡çº§å¤šæ¨¡æ€ä»»åŠ¡å¹³å°ï¼Œé€šè¿‡é›†æˆYOLO-Worldå’ŒYOLOv8-Segæ¨¡å‹ï¼Œå¹¶è®¾è®¡å†…å®¹æ„ŸçŸ¥å…³é”®å¸§é‡‡æ ·æœºåˆ¶å’Œç»Ÿä¸€æç¤ºä¼˜åŒ–æ–¹æ¡ˆï¼Œè§£å†³äº†æ— äººæœºè¾¹ç¼˜è®¾å¤‡ä¸Šå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹é«˜è®¡ç®—æˆæœ¬ä¸æœ‰é™è®¡ç®—èµ„æºä¹‹é—´çš„çŸ›ç›¾ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ— äººæœºåœ¨å¤æ‚åœºæ™¯ä¸­éœ€è¦å®æ—¶è§†è§‰ç†è§£ä¸äº¤äº’èƒ½åŠ›ï¼Œä½†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„é«˜è®¡ç®—æˆæœ¬ä¸æ— äººæœºè¾¹ç¼˜è®¾å¤‡çš„æœ‰é™è®¡ç®—èµ„æºä¹‹é—´å­˜åœ¨æ˜¾è‘—çŸ›ç›¾ï¼Œè¿™é˜»ç¢äº†å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ— äººæœºå¹³å°ä¸Šçš„å®é™…éƒ¨ç½²ä¸åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼šé¦–å…ˆå°†BLIP-2ä¸YOLO-Worldå’ŒYOLOv8-Segæ¨¡å‹æ·±åº¦èåˆï¼Œåˆ©ç”¨YOLOçš„ç²¾ç¡®æ„ŸçŸ¥ç»“æœå¢å¼ºè§†è§‰æ³¨æ„åŠ›ç†è§£ï¼›å…¶æ¬¡è®¾è®¡åŸºäºK-Meansèšç±»çš„å†…å®¹æ„ŸçŸ¥å…³é”®å¸§é‡‡æ ·æœºåˆ¶ï¼Œç»“åˆæ™ºèƒ½å¸§é€‰æ‹©å’Œæ—¶åºç‰¹å¾æ‹¼æ¥ï¼›æœ€åå®æ–½ç»Ÿä¸€çš„å¤šä»»åŠ¡é€‚åº”æç¤ºä¼˜åŒ–æ–¹æ¡ˆï¼Œå°†YOLOçš„ç»“æ„åŒ–äº‹ä»¶æ—¥å¿—ä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯æ³¨å…¥BLIP-2è¾“å…¥ï¼Œå¹¶è®¾è®¡è¾“å‡ºçº¦æŸè¿‡æ»¤æŠ€æœ¯ç»†èŠ‚ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨æ— éœ€å¯¹æ— äººæœºæ•°æ®è¿›è¡Œä»»åŠ¡ç‰¹å®šå¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸæ‰©å±•äº†BLIP-2çš„å¤šä»»åŠ¡èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è§†é¢‘çº§äº¤äº’ä»»åŠ¡ï¼Œå¹¶ç”Ÿæˆå‡†ç¡®ä¸”ä¸Šä¸‹æ–‡ç›¸å…³çš„è¾“å‡ºï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—éœ€æ±‚åŒæ—¶ä¿æŒäº†å¤šæ¨¡æ€ç†è§£æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºæ— äººæœºè¾¹ç¼˜è®¡ç®—æä¾›äº†å®ç”¨çš„è½»é‡çº§å¤šæ¨¡æ€ä»»åŠ¡å¹³å°è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ¨¡å‹é›†æˆã€æ™ºèƒ½å¸§é‡‡æ ·å’Œæç¤ºä¼˜åŒ–çš„ååŒè®¾è®¡ï¼Œå®ç°äº†åœ¨æœ‰é™èµ„æºä¸‹ä¿æŒå…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹èƒ½åŠ›ï¼Œä¸ºæ— äººæœºå®æ—¶è§†è§‰ç†è§£ä¸äº¤äº’åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.</p>
<h3 id="28-ur-bench-a-benchmark-for-multi-hop-reasoning-over-ultra-high-resolution-images">[28] <a href="https://arxiv.org/abs/2601.08748">UR-Bench: A Benchmark for Multi-Hop Reasoning over Ultra-High-Resolution Images</a></h3>
<p><em>Siqi Li, Xinyu Cai, Jianbiao Mei, Nianchen Deng, Pinlong Cai, Licheng Wen, Yufan Shen, Xuemeng Yang, Botian Shi, Yong Liu</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UR-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¶…é«˜åˆ†è¾¨ç‡å›¾åƒä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåŸºäºä»£ç†çš„æ¡†æ¶ï¼Œé€šè¿‡è°ƒç”¨å¤–éƒ¨è§†è§‰å·¥å…·æ¥é«˜æ•ˆå¤„ç†åƒå…†åƒç´ çº§å›¾åƒã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è¶…é«˜åˆ†è¾¨ç‡å›¾åƒä¸Šçš„æ€§èƒ½å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å½“å‰çš„è§†è§‰é—®ç­”åŸºå‡†é€šå¸¸ä½¿ç”¨ä¸­ç­‰åˆ†è¾¨ç‡æ•°æ®ï¼Œè§†è§‰å¤æ‚åº¦æœ‰é™ï¼Œæ— æ³•è¯„ä¼°æ¨¡å‹åœ¨æç«¯è§†è§‰ä¿¡æ¯ä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†UR-BenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«äººæ–‡åœºæ™¯å’Œè‡ªç„¶åœºæ™¯ä¸¤å¤§ç±»åˆ«ï¼Œæ¶µç›–å››ä¸ªå…·æœ‰ä¸åŒç©ºé—´ç»“æ„å’Œæ•°æ®æºçš„è¶…é«˜åˆ†è¾¨ç‡å›¾åƒå­é›†ï¼Œå›¾åƒåˆ†è¾¨ç‡ä»æ•°ç™¾å…†åƒç´ åˆ°åƒå…†åƒç´ ä¸ç­‰ã€‚åŒæ—¶å¼€å‘äº†ä¸€ä¸ªåŸºäºä»£ç†çš„æ¡†æ¶ï¼Œå…¶ä¸­è¯­è¨€æ¨¡å‹é€šè¿‡è°ƒç”¨å¤–éƒ¨è§†è§‰å·¥å…·è¿›è¡Œæ¨ç†ï¼Œå¹¶å¼•å…¥äº†è¯­ä¹‰æŠ½è±¡å’Œæ£€ç´¢å·¥å…·ä»¥å®ç°å¯¹è¶…é«˜åˆ†è¾¨ç‡å›¾åƒçš„é«˜æ•ˆå¤„ç†ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶è¯„ä¼°äº†æœ€å…ˆè¿›çš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’ŒåŸºäºä»£ç†çš„æ¡†æ¶ï¼Œç»“æœè¡¨æ˜æ‰€æå‡ºçš„æ¡†æ¶åœ¨å¤„ç†è¶…é«˜åˆ†è¾¨ç‡å›¾åƒæ¨ç†ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿåº”å¯¹ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å¤„ç†çš„æç«¯è§†è§‰å¤æ‚åº¦æŒ‘æˆ˜ã€‚</p>
<p><strong>Conclusion:</strong> UR-Benchå¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨è¶…é«˜åˆ†è¾¨ç‡å›¾åƒæ¨ç†è¯„ä¼°æ–¹é¢çš„ç©ºç™½ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è§†è§‰åœºæ™¯ä¸‹çš„èƒ½åŠ›è¯„ä¼°æä¾›äº†æ ‡å‡†åŒ–æµ‹è¯•å¹³å°ã€‚åŸºäºä»£ç†çš„æ¡†æ¶å±•ç¤ºäº†é€šè¿‡æ¨¡å—åŒ–å·¥å…·è°ƒç”¨å¤„ç†æç«¯è§†è§‰ä¿¡æ¯çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥è¶…é«˜åˆ†è¾¨ç‡è§†è§‰ç†è§£ç ”ç©¶æä¾›äº†æ–°çš„æ–¹æ³•è®ºæ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>Recent multimodal large language models (MLLMs) show strong capabilities in visual-language reasoning, yet their performance on ultra-high-resolution imagery remains largely unexplored. Existing visual question answering (VQA) benchmarks typically rely on medium-resolution data, offering limited visual complexity. To bridge this gap, we introduce Ultra-high-resolution Reasoning Benchmark (UR-Bench), a benchmark designed to evaluate the reasoning capabilities of MLLMs under extreme visual information. UR-Bench comprises two major categories, Humanistic Scenes and Natural Scenes, covering four subsets of ultra-high-resolution images with distinct spatial structures and data sources. Each subset contains images ranging from hundreds of megapixels to gigapixels, accompanied by questions organized into three levels, enabling evaluation of models' reasoning capabilities in ultra-high-resolution scenarios. We further propose an agent-based framework in which a language model performs reasoning by invoking external visual tools. In addition, we introduce Semantic Abstraction and Retrieval tools that enable more efficient processing of ultra-high-resolution images. We evaluate state-of-the-art models using both an end-to-end MLLMs and our agent-based framework, demonstrating the effectiveness of our framework.</p>
<h3 id="29-mmlgnet-cross-modal-alignment-of-remote-sensing-data-using-clip">[29] <a href="https://arxiv.org/abs/2601.08420">MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP</a></h3>
<p><em>Aditya Chaudhary, Sneha Barman, Mainak Singha, Ankit Jha, Girish Mishra, Biplab Banerjee</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€è¯­è¨€å¼•å¯¼ç½‘ç»œï¼ˆMMLGNetï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œåˆ©ç”¨CLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹å°†é«˜å…‰è°±æˆåƒå’ŒLiDARç­‰å¼‚æ„é¥æ„Ÿæ¨¡æ€ä¸è‡ªç„¶è¯­è¨€è¯­ä¹‰å¯¹é½ï¼Œé€šè¿‡è¯­è¨€ç›‘ç£æ˜¾è‘—æå‡äº†é¥æ„Ÿæ•°æ®çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€å¤šæ¨¡æ€åœ°çƒè§‚æµ‹æ•°æ®çš„æ—¥ç›Šå¢å¤šï¼Œè¿«åˆ‡éœ€è¦èƒ½å¤Ÿæœ‰æ•ˆèåˆå…‰è°±ã€ç©ºé—´å’Œå‡ ä½•ä¿¡æ¯å¹¶å®ç°è¯­ä¹‰çº§ç†è§£çš„æ–¹æ³•ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¼‚æ„é¥æ„Ÿæ¨¡æ€ä¸è¯­è¨€è¯­ä¹‰å¯¹é½æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> MMLGNeté‡‡ç”¨æ¨¡æ€ç‰¹å®šçš„ç¼–ç å™¨ï¼Œé€šè¿‡åŒå‘å¯¹æ¯”å­¦ä¹ åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å°†è§†è§‰ç‰¹å¾ä¸æ‰‹å·¥åˆ¶ä½œçš„æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œå€Ÿé‰´CLIPçš„è®­ç»ƒèŒƒå¼ï¼Œä½¿ç”¨ç®€å•çš„CNNç¼–ç å™¨å®ç°é«˜ç»´é¥æ„Ÿæ•°æ®ä¸è¯­è¨€å¼•å¯¼è§£é‡Šçš„æ¡¥æ¥ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒMMLGNetè¶…è¶Šäº†å¤šç§å·²å»ºç«‹çš„å¤šæ¨¡æ€çº¯è§†è§‰æ–¹æ³•ï¼Œå–å¾—äº†å¼ºåŠ²çš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†è¯­è¨€ç›‘ç£å¯¹é¥æ„Ÿæ•°æ®ç†è§£çš„æ˜¾è‘—ç›Šå¤„ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜è¯­è¨€ç›‘ç£èƒ½å¤Ÿæœ‰æ•ˆæå‡é¥æ„Ÿæ•°æ®çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€åœ°çƒè§‚æµ‹æ•°æ®çš„è§£é‡Šæä¾›äº†æ–°çš„èŒƒå¼ï¼Œå±•ç¤ºäº†ç®€å•CNNç¼–ç å™¨ç»“åˆè¯­è¨€å¼•å¯¼çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP's training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.</p>
<h3 id="30-s3-clip-video-super-resolution-for-person-reid">[30] <a href="https://arxiv.org/abs/2601.08807">S3-CLIP: Video Super Resolution for Person-ReID</a></h3>
<p><em>Tamas Endrei, Gyorgy Cserey</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†S3-CLIPï¼Œä¸€ç§åŸºäºè§†é¢‘è¶…åˆ†è¾¨ç‡çš„CLIP-ReIDæ¡†æ¶ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶è§†é¢‘è¶…åˆ†è¾¨ç‡æŠ€æœ¯å¦‚ä½•é€šè¿‡æå‡è½¨è¿¹è´¨é‡æ¥å¢å¼ºè·¨è§†è§’è¡Œäººé‡è¯†åˆ«æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç©ºä¸­åˆ°åœ°é¢å’Œåœ°é¢åˆ°ç©ºä¸­åœºæ™¯ä¸­ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è¡Œäººé‡è¯†åˆ«æ–¹æ³•å¤§å¤šå°†è½¨è¿¹è´¨é‡è§†ä¸ºæ¬¡è¦é—®é¢˜ï¼Œä¸»è¦å…³æ³¨åŸºç¡€æ¨¡å‹çš„æ¶æ„æ”¹è¿›ï¼Œå¿½è§†äº†è½¨è¿¹è´¨é‡åœ¨ç°å®ä¸–ç•Œå›°éš¾åœºæ™¯éƒ¨ç½²ä¸­çš„å…³é”®é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨è§†è§’æ¡ä»¶ä¸‹éœ€è¦å¤„ç†ä½è´¨é‡è½¨è¿¹çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æå‡ºäº†S3-CLIPæ¡†æ¶ï¼Œå°†æœ€æ–°çš„è¶…åˆ†è¾¨ç‡ç½‘ç»œè¿›å±•ä¸ä»»åŠ¡é©±åŠ¨çš„è¶…åˆ†è¾¨ç‡æµç¨‹ç›¸ç»“åˆï¼Œä¸“é—¨é’ˆå¯¹è§†é¢‘è¡Œäººé‡è¯†åˆ«åœºæ™¯è¿›è¡Œé€‚é…ï¼Œé€šè¿‡è§†é¢‘è¶…åˆ†è¾¨ç‡æŠ€æœ¯æå‡è½¨è¿¹è´¨é‡æ¥å¢å¼ºé‡è¯†åˆ«æ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºS3-CIPåœ¨VReID-XFDæŒ‘æˆ˜ä¸­å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œåœ¨ç©ºå¯¹åœ°åœºæ™¯è¾¾åˆ°37.52% mAPï¼Œåœ¨åœ°å¯¹ç©ºåœºæ™¯è¾¾åˆ°29.16% mAPï¼Œç‰¹åˆ«æ˜¯åœ¨åœ°å¯¹ç©ºè®¾ç½®ä¸­ï¼ŒRank-1ã€Rank-5å’ŒRank-10å‡†ç¡®ç‡åˆ†åˆ«æå‡äº†11.24%ã€13.48%å’Œ17.98%ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ¢ç´¢äº†è§†é¢‘è¶…åˆ†è¾¨ç‡ä½œä¸ºæå‡è¡Œäººé‡è¯†åˆ«è½¨è¿¹è´¨é‡çš„æ‰‹æ®µï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨è§†è§’æ¡ä»¶ä¸‹ï¼Œä¸ºå¤„ç†ç°å®ä¸–ç•Œå›°éš¾åœºæ™¯ä¸­çš„ä½è´¨é‡è½¨è¿¹é—®é¢˜æä¾›äº†æ–°çš„æŠ€æœ¯é€”å¾„ï¼Œå±•ç¤ºäº†è¶…åˆ†è¾¨ç‡æŠ€æœ¯åœ¨é‡è¯†åˆ«ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.</p>
<h3 id="31-incentivizing-cardiologist-like-reasoning-in-mllms-for-interpretable-echocardiographic-diagnosis">[31] <a href="https://arxiv.org/abs/2601.08440">Incentivizing Cardiologist-Like Reasoning in MLLMs for Interpretable Echocardiographic Diagnosis</a></h3>
<p><em>Yi Qin, Lehan Wang, Chenxu Zhao, Alex P. W. Lee, Xiaomeng Li</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¶…å£°å¿ƒåŠ¨å›¾è¯Šæ–­æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å¿ƒè„æ¨ç†æ¨¡æ¿å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„CardiacMindæ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚å¿ƒè„ç–¾ç—…çš„è¯Šæ–­æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è¶…å£°å¿ƒåŠ¨å›¾åŸºç¡€æ¨¡å‹æœªèƒ½æœ‰æ•ˆæ•æ‰å®šé‡æµ‹é‡ä¸ä¸´åºŠè¡¨ç°ä¹‹é—´çš„å…³ç³»ï¼Œè€ŒåŒ»å­¦æ¨ç†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹éœ€è¦æ˜‚è´µçš„è¯¦ç»†æ¨ç†è·¯å¾„æ„å»ºï¼Œä¸”éš¾ä»¥ç›´æ¥èå…¥è¶…å£°å¿ƒåŠ¨å›¾å…ˆéªŒçŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¤æ‚å¿ƒè„ç–¾ç—…è¯Šæ–­ä¸­çš„åº”ç”¨æ•ˆæœã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¿ƒè„æ¨ç†æ¨¡æ¿æä¾›å¤æ‚å¿ƒè„ç–¾ç—…çš„é€æ­¥è§„èŒƒåŒ–è¯Šæ–­æµç¨‹ï¼Œç®€åŒ–æ¨ç†è·¯å¾„æ„å»ºï¼›CardiacMindå¼ºåŒ–å­¦ä¹ æ¡†æ¶å¼•å…¥ä¸‰ç§æ–°å‹å¥–åŠ±æœºåˆ¶â€”â€”è¿‡ç¨‹æ•°é‡å¥–åŠ±ä¿ƒè¿›è¯¦ç»†æ¨ç†ï¼Œè¿‡ç¨‹è´¨é‡å¥–åŠ±ä¿ƒè¿›è·¨è§†å›¾å’Œæ¨¡æ€çš„è¯æ®æ•´åˆï¼Œè¶…å£°å¿ƒåŠ¨å›¾è¯­ä¹‰å¥–åŠ±ç¡®ä¿é€æ­¥æè¿°ä¸è§†è§‰å†…å®¹çš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨15ç§å¤æ‚å¿ƒè„ç–¾ç—…çš„å¤šè§†å›¾è¶…å£°å¿ƒåŠ¨å›¾è¯Šæ–­ä¸­å®ç°äº†48%çš„æ€§èƒ½æå‡ï¼Œåœ¨CardiacNet-PAHæ•°æ®é›†ä¸Šç›¸æ¯”å…ˆå‰æ–¹æ³•æé«˜äº†5%ã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œå…¶æ¨ç†è¾“å‡ºçš„ä¸´åºŠåŒ»ç”ŸåŒæ„ç‡è¾¾åˆ°93.33%ï¼Œæ˜¾ç¤ºå‡ºä¸å¿ƒè„ç—…ä¸“å®¶ç›¸ä¼¼çš„æ¨ç†é€»è¾‘ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥å¿ƒè„ç—…ä¸“å®¶æ€ç»´æ¨¡å¼ï¼Œæœ‰æ•ˆæå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¶…å£°å¿ƒåŠ¨å›¾è¯Šæ–­ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œä¸ºåŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ç»“æ„åŒ–æ¨ç†æ¡†æ¶å’Œæœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Echocardiographic diagnosis is vital for cardiac screening yet remains challenging. Existing echocardiography foundation models do not effectively capture the relationships between quantitative measurements and clinical manifestations, whereas medical reasoning multimodal large language models (MLLMs) require costly construction of detailed reasoning paths and remain ineffective at directly incorporating such echocardiographic priors into their reasoning. To address these limitations, we propose a novel approach comprising Cardiac Reasoning Template (CRT) and CardiacMind to enhance MLLM's echocardiographic reasoning by introducing cardiologist-like mindset. Specifically, CRT provides stepwise canonical diagnostic procedures for complex cardiac diseases to streamline reasoning path construction without the need for costly case-by-case verification. To incentivize reasoning MLLM under CRT, we develop CardiacMind, a new reinforcement learning scheme with three novel rewards: Procedural Quantity Reward (PQtR), Procedural Quality Reward (PQlR), and Echocardiographic Semantic Reward (ESR). PQtR promotes detailed reasoning; PQlR promotes integration of evidence across views and modalities, while ESR grounds stepwise descriptions in visual content. Our methods show a 48% improvement in multiview echocardiographic diagnosis for 15 complex cardiac diseases and a 5% improvement on CardiacNet-PAH over prior methods. The user study on our method's reasoning outputs shows 93.33% clinician agreement with cardiologist-like reasoning logic. Our code will be available.</p>
<h3 id="32-reasoning-matters-for-3d-visual-grounding">[32] <a href="https://arxiv.org/abs/2601.08811">Reasoning Matters for 3D Visual Grounding</a></h3>
<p><em>Hsiang-Wei Huang, Kuang-Ming Chen, Wenhao Chai, Cheng-Yen Yang, Jen-Hao Cheng, Jenq-Neng Hwang</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åˆæˆ3Dè§†è§‰å®šä½æ•°æ®åŠå…¶æ¨ç†è¿‡ç¨‹çš„æ•°æ®ç®¡é“ï¼Œå¹¶åŸºäºè¯¥æ•°æ®å¾®è°ƒLLMå¾—åˆ°Reason3DVG-8Bæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»…ä½¿ç”¨1.6%çš„è®­ç»ƒæ•°æ®å°±åœ¨3Dè§†è§‰å®šä½ä»»åŠ¡ä¸Šè¶…è¶Šäº†å…ˆå‰æ–¹æ³•ï¼Œè¯æ˜äº†æ¨ç†èƒ½åŠ›åœ¨3Dè§†è§‰å®šä½ä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰3Dè§†è§‰å®šä½ä»»åŠ¡é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šç°æœ‰æ¨¡å‹æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œä¸”ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨çš„3Dæ•°æ®è¿›è¡Œç›‘ç£è®­ç»ƒï¼›åŒæ—¶ï¼Œç°æœ‰åŸºäºåˆæˆæ•°æ®æ‰©å±•çš„æ–¹æ³•æ€§èƒ½æå‡æœ‰é™ä¸”ä¸æ•°æ®æ”¶é›†æˆæœ¬ä¸æˆæ¯”ä¾‹ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ›´é«˜æ•ˆçš„æ•°æ®ç”Ÿæˆæ–¹æ³•æ¥æå‡3Dè§†è§‰å®šä½æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§èƒ½å¤Ÿè‡ªåŠ¨åˆæˆ3Dè§†è§‰å®šä½æ•°æ®åŠå…¶å¯¹åº”æ¨ç†è¿‡ç¨‹çš„æ•°æ®ç®¡é“ï¼Œè¯¥ç®¡é“æ— éœ€äººå·¥æ ‡æ³¨å³å¯ç”Ÿæˆé«˜è´¨é‡çš„3Dè§†è§‰å®šä½è®­ç»ƒæ•°æ®ï¼›åŸºäºç”Ÿæˆçš„æ•°æ®ï¼Œç ”ç©¶è€…å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¼€å‘äº†Reason3DVG-8Bæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹3Dè§†è§‰å®šä½ä»»åŠ¡ä¼˜åŒ–çš„è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> Reason3DVG-8Bæ¨¡å‹åœ¨3Dè§†è§‰å®šä½ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä»…ä½¿ç”¨å…ˆå‰LLM-basedæ–¹æ³•3D-GRANDæ‰€éœ€è®­ç»ƒæ•°æ®çš„1.6%å°±å®ç°äº†æ€§èƒ½è¶…è¶Šï¼›è¿™ä¸€ç»“æœä¸ä»…è¯æ˜äº†æ‰€æå‡ºæ•°æ®ç®¡é“çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿå‡¸æ˜¾äº†æ¨ç†è¿‡ç¨‹åœ¨3Dè§†è§‰å®šä½ä»»åŠ¡ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶è¯æ˜äº†è‡ªåŠ¨åˆæˆçš„3Dè§†è§‰å®šä½æ•°æ®åŠå…¶æ¨ç†è¿‡ç¨‹èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¹…é™ä½æ•°æ®éœ€æ±‚ï¼›è¿™ä¸€å‘ç°ä¸º3Dè§†è§‰ç†è§£é¢†åŸŸæä¾›äº†æ–°çš„æ•°æ®ç”ŸæˆèŒƒå¼ï¼Œå¼ºè°ƒäº†æ¨ç†èƒ½åŠ›åœ¨è·¨æ¨¡æ€3Dä»»åŠ¡ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥å¼€å‘æ›´é«˜æ•ˆçš„3Dè§†è§‰å®šä½ç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.</p>
<h3 id="33-modality-decoupled-rgb-thermal-object-detector-via-query-fusion">[33] <a href="https://arxiv.org/abs/2601.08458">Modality-Decoupled RGB-Thermal Object Detector via Query Fusion</a></h3>
<p><em>Chao Tian, Zikun Zhou, Chao Yang, Guoqing Zhu, Fu'an Zhong, Zhenyu He</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡æ€è§£è€¦çš„RGB-Tæ£€æµ‹æ¡†æ¶MDQFï¼Œé€šè¿‡æŸ¥è¯¢èåˆæœºåˆ¶å¹³è¡¡æ¨¡æ€äº’è¡¥ä¸åˆ†ç¦»ï¼Œåœ¨æç«¯æ¡ä»¶ä¸‹æ’é™¤é€€åŒ–æ¨¡æ€å¹²æ‰°ï¼ŒåŒæ—¶æ”¯æŒä½¿ç”¨éé…å¯¹æ•°æ®è¿›è¡Œåˆ†æ”¯ä¼˜åŒ–ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> RGB-Tæ£€æµ‹è™½ç„¶èƒ½é€šè¿‡æ¨¡æ€èåˆåˆ©ç”¨è·¨æ¨¡æ€äº’è¡¥ä¿¡æ¯å®ç°é²æ£’æ£€æµ‹ï¼Œä½†åœ¨æç«¯æ¡ä»¶ä¸‹å½“ä¸€ä¸ªæ¨¡æ€è´¨é‡è¾ƒå·®æ—¶ä¼šå¹²æ‰°æ£€æµ‹æ€§èƒ½ï¼Œå› æ­¤éœ€è¦æ¨¡æ€åˆ†ç¦»æ¥å‡è½»å™ªå£°å½±å“ï¼Œç°æœ‰æ–¹æ³•åœ¨å¹³è¡¡æ¨¡æ€äº’è¡¥ä¸åˆ†ç¦»æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> æå‡ºæ¨¡æ€è§£è€¦çš„RGB-Tæ£€æµ‹æ¡†æ¶MDQFï¼Œé‡‡ç”¨ç±»ä¼¼DETRçš„æ£€æµ‹å™¨ä½œä¸ºRGBå’ŒTIRå›¾åƒçš„åˆ†æ”¯ï¼Œåœ¨æ¯ä¸ªç»†åŒ–é˜¶æ®µé€šè¿‡æŸ¥è¯¢é€‰æ‹©ä¸é€‚åº”å°†é«˜è´¨é‡æŸ¥è¯¢ä»ä¸€ä¸ªåˆ†æ”¯é¦ˆé€åˆ°å¦ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ç°æŸ¥è¯¢èåˆå¹¶æ’é™¤é€€åŒ–æ¨¡æ€å¹²æ‰°ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨RGB-Tæ£€æµ‹ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´å¥½çš„æ¨¡æ€ç‹¬ç«‹æ€§ï¼Œå¹¶ä¸”è§£è€¦æ¡†æ¶å…è®¸ä½¿ç”¨éé…å¯¹çš„RGBæˆ–TIRå›¾åƒå•ç‹¬ä¼˜åŒ–æ¯ä¸ªåˆ†æ”¯ï¼Œå‡å°‘äº†å¯¹é…å¯¹æ•°æ®çš„éœ€æ±‚ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åœ¨æç«¯æ¡ä»¶ä¸‹å¹³è¡¡æ¨¡æ€äº’è¡¥ä¸åˆ†ç¦»çš„é‡è¦æ€§ï¼Œæå‡ºçš„æŸ¥è¯¢èåˆæœºåˆ¶èƒ½æœ‰æ•ˆæ’é™¤é€€åŒ–æ¨¡æ€å¹²æ‰°ï¼Œè§£è€¦æ¡†æ¶ä¸ºä½¿ç”¨éé…å¯¹æ•°æ®ä¼˜åŒ–å¤šæ¨¡æ€ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ï¼Œå¢å¼ºäº†RGB-Tæ£€æµ‹çš„é²æ£’æ€§å’Œå®ç”¨æ€§ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>The advantage of RGB-Thermal (RGB-T) detection lies in its ability to perform modality fusion and integrate cross-modality complementary information, enabling robust detection under diverse illumination and weather conditions. However, under extreme conditions where one modality exhibits poor quality and disturbs detection, modality separation is necessary to mitigate the impact of noise. To address this problem, we propose a Modality-Decoupled RGB-T detection framework with Query Fusion (MDQF) to balance modality complementation and separation. In this framework, DETR-like detectors are employed as separate branches for the RGB and TIR images, with query fusion interspersed between the two branches in each refinement stage. Herein, query fusion is performed by feeding the high-quality queries from one branch to the other one after query selection and adaptation. This design effectively excludes the degraded modality and corrects the predictions using high-quality queries. Moreover, the decoupled framework allows us to optimize each individual branch with unpaired RGB or TIR images, eliminating the need for paired RGB-T data. Extensive experiments demonstrate that our approach delivers superior performance to existing RGB-T detectors and achieves better modality independence.</p>
<h3 id="34-motion-attribution-for-video-generation">[34] <a href="https://arxiv.org/abs/2601.08828">Motion Attribution for Video Generation</a></h3>
<p><em>Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-TaixÃ©, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Motiveæ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è¿åŠ¨å½’å› æ–¹æ³•ï¼Œé€šè¿‡æ¢¯åº¦å½’å› æŠ€æœ¯è¯†åˆ«å½±å“æ—¶é—´åŠ¨æ€çš„å…³é”®è®­ç»ƒæ•°æ®ï¼Œå¹¶åˆ©ç”¨è¿™äº›å‘ç°æŒ‡å¯¼æ•°æ®ç­›é€‰ä»¥æå‡è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡è§†é¢‘ç”Ÿæˆæ¨¡å‹å‘å±•è¿…é€Ÿï¼Œä½†æ•°æ®å¦‚ä½•å½±å“è¿åŠ¨åŠ¨æ€çš„é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†ç†è§£ã€‚ç°æœ‰ç ”ç©¶ç¼ºä¹ä¸“é—¨é’ˆå¯¹è¿åŠ¨è€Œéè§†è§‰å¤–è§‚çš„æ•°æ®å½’å› æ–¹æ³•ï¼Œè¿™é™åˆ¶äº†é€šè¿‡æ•°æ®ç­›é€‰ä¼˜åŒ–è§†é¢‘æ—¶é—´ä¸€è‡´æ€§å’Œç‰©ç†åˆç†æ€§çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> Motiveæ˜¯ä¸€ä¸ªåŸºäºæ¢¯åº¦çš„è¿åŠ¨ä¸­å¿ƒæ•°æ®å½’å› æ¡†æ¶ï¼Œé€šè¿‡è¿åŠ¨åŠ æƒæŸå¤±æ©ç å°†æ—¶é—´åŠ¨æ€ä¸é™æ€å¤–è§‚åˆ†ç¦»ï¼Œå®ç°äº†é«˜æ•ˆå¯æ‰©å±•çš„è¿åŠ¨ç‰¹å®šå½±å“è®¡ç®—ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ‰©å±•åˆ°ç°ä»£å¤§è§„æ¨¡é«˜è´¨é‡è§†é¢‘æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä¸“é—¨ç”¨äºåˆ†æå“ªäº›å¾®è°ƒç‰‡æ®µä¼šæ”¹å–„æˆ–æ¶åŒ–æ—¶é—´åŠ¨æ€ã€‚</p>
<p><strong>Result:</strong> åœ¨æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸Šï¼ŒMotiveèƒ½å¤Ÿè¯†åˆ«å¯¹è¿åŠ¨æœ‰å¼ºçƒˆå½±å“çš„è®­ç»ƒç‰‡æ®µï¼ŒæŒ‡å¯¼çš„æ•°æ®ç­›é€‰æ˜¾è‘—æå‡äº†æ—¶é—´ä¸€è‡´æ€§å’Œç‰©ç†åˆç†æ€§ã€‚ä½¿ç”¨Motiveé€‰æ‹©çš„é«˜å½±å“åŠ›æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œåœ¨VBenchåŸºå‡†ä¸ŠåŒæ—¶æ”¹å–„äº†è¿åŠ¨å¹³æ»‘åº¦å’ŒåŠ¨æ€ç¨‹åº¦ï¼Œç›¸æ¯”é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹è·å¾—äº†74.1%çš„äººç±»åå¥½èƒœç‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é¦–æ¬¡å®ç°äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­è¿åŠ¨è€Œéè§†è§‰å¤–è§‚çš„æ•°æ®å½’å› ï¼Œä¸ºç†è§£æ•°æ®å¦‚ä½•å½±å“æ—¶é—´åŠ¨æ€æä¾›äº†ç³»ç»Ÿæ¡†æ¶ã€‚Motiveä¸ä»…èƒ½å¤Ÿè¯Šæ–­ç°æœ‰æ¨¡å‹çš„è¿åŠ¨é—®é¢˜ï¼Œè¿˜èƒ½æŒ‡å¯¼é«˜æ•ˆçš„æ•°æ®ç­›é€‰ç­–ç•¥ï¼Œä¸ºæå‡è§†é¢‘ç”Ÿæˆè´¨é‡å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.</p>
<h3 id="35-zero-shot-distracted-driver-detection-via-vision-language-models-with-double-decoupling">[35] <a href="https://arxiv.org/abs/2601.08467">Zero-Shot Distracted Driver Detection via Vision Language Models with Double Decoupling</a></h3>
<p><em>Takamichi Miyata, Sumiko Miyata, Andrew Morris</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸»ä½“è§£è€¦æ¡†æ¶ï¼Œé€šè¿‡æå–é©¾é©¶å‘˜å¤–è§‚åµŒå…¥å¹¶åœ¨é›¶æ ·æœ¬åˆ†ç±»å‰æ¶ˆé™¤å…¶å¯¹å›¾åƒåµŒå…¥çš„å½±å“ï¼Œè§£å†³äº†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„é©¾é©¶å‘˜åˆ†å¿ƒæ£€æµ‹ä¸­ä¸»ä½“ç‰¹å®šå¤–è§‚å˜åŒ–å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åˆ†å¿ƒé©¾é©¶æ˜¯äº¤é€šäº‹æ•…çš„ä¸»è¦åŸå› ï¼Œéœ€è¦é²æ£’ä¸”å¯æ‰©å±•çš„æ£€æµ‹æ–¹æ³•ã€‚ç°æœ‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„åˆ†å¿ƒé©¾é©¶å‘˜æ£€æµ‹å™¨åœ¨çœŸå®åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç“¶é¢ˆåœ¨äºä¸»ä½“ç‰¹å®šå¤–è§‚å˜åŒ–ï¼ˆå¦‚æœè£…ã€å¹´é¾„ã€æ€§åˆ«ï¼‰ä¸è¡Œä¸ºçº¿ç´¢çš„çº ç¼ ï¼Œå¯¼è‡´æ¨¡å‹å†³ç­–åŸºäºé©¾é©¶å‘˜èº«ä»½è€Œéé©¾é©¶è¡Œä¸ºã€‚</p>
<p><strong>Method:</strong> æå‡ºä¸»ä½“è§£è€¦æ¡†æ¶ï¼Œæå–é©¾é©¶å‘˜å¤–è§‚åµŒå…¥å¹¶åœ¨é›¶æ ·æœ¬åˆ†ç±»å‰æ¶ˆé™¤å…¶å¯¹å›¾åƒåµŒå…¥çš„å½±å“ï¼Œä»è€Œå¼ºè°ƒä¸åˆ†å¿ƒç›¸å…³çš„è¯æ®ã€‚è¿›ä¸€æ­¥é€šè¿‡åº¦é‡æŠ•å½±åˆ°Stiefelæµå½¢å¯¹æ–‡æœ¬åµŒå…¥è¿›è¡Œæ­£äº¤åŒ–ï¼Œåœ¨ä¿æŒåŸå§‹è¯­ä¹‰æ¥è¿‘çš„åŒæ—¶æé«˜å¯åˆ†æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸æ¯”å…ˆå‰åŸºçº¿å–å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†ä¸»ä½“è§£è€¦å’Œæ–‡æœ¬åµŒå…¥æ­£äº¤åŒ–ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†åœ¨çœŸå®é“è·¯å®‰å…¨åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†ä¸»ä½“å¤–è§‚å˜åŒ–æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åˆ†å¿ƒé©¾é©¶æ£€æµ‹ä¸­çš„å…³é”®ç“¶é¢ˆï¼Œæå‡ºçš„è§£è€¦æ¡†æ¶é€šè¿‡åˆ†ç¦»å¤–è§‚ä¸è¡Œä¸ºçº¿ç´¢æ˜¾è‘—æå‡äº†æ£€æµ‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ºåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰å®é™…é“è·¯å®‰å…¨åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Distracted driving is a major cause of traffic collisions, calling for robust and scalable detection methods. Vision-language models (VLMs) enable strong zero-shot image classification, but existing VLM-based distracted driver detectors often underperform in real-world conditions. We identify subject-specific appearance variations (e.g., clothing, age, and gender) as a key bottleneck: VLMs entangle these factors with behavior cues, leading to decisions driven by who the driver is rather than what the driver is doing. To address this, we propose a subject decoupling framework that extracts a driver appearance embedding and removes its influence from the image embedding prior to zero-shot classification, thereby emphasizing distraction-relevant evidence. We further orthogonalize text embeddings via metric projection onto Stiefel manifold to improve separability while staying close to the original semantics. Experiments demonstrate consistent gains over prior baselines, indicating the promise of our approach for practical road-safety applications.</p>
<h3 id="36-cross-modal-proxy-evolving-for-ood-detection-with-vision-language-models">[36] <a href="https://arxiv.org/abs/2601.08476">Cross-modal Proxy Evolving for OOD Detection with Vision-Language Models</a></h3>
<p><em>Hao Tang, Yu Liu, Shuanglin Yan, Fei Shen, Shengfeng He, Jing Qin</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCoEvoï¼Œä¸€ç§æ— éœ€è®­ç»ƒå’Œæ ‡æ³¨çš„æµ‹è¯•æ—¶æ¡†æ¶ï¼Œé€šè¿‡åŒå‘ã€æ ·æœ¬æ¡ä»¶åŒ–çš„æ–‡æœ¬ä¸è§†è§‰ä»£ç†è‡ªé€‚åº”æœºåˆ¶ï¼Œå®ç°é›¶æ ·æœ¬åˆ†å¸ƒå¤–æ£€æµ‹ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»£ç†å¯¹é½çš„ååŒè¿›åŒ–æœºåˆ¶åŠ¨æ€æŒ–æ˜ä¸Šä¸‹æ–‡æ–‡æœ¬è´Ÿä¾‹å¹¶è¿­ä»£ä¼˜åŒ–è§†è§‰ä»£ç†ï¼Œæ˜¾è‘—æå‡äº†å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹çš„æ£€æµ‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­éƒ¨ç½²è§†è§‰è¯­è¨€æ¨¡å‹éœ€è¦å¯é çš„é›¶æ ·æœ¬åˆ†å¸ƒå¤–æ£€æµ‹èƒ½åŠ›ï¼Œä½†ç°æœ‰åŸºäºå›ºå®šæ–‡æœ¬ä»£ç†çš„è´Ÿæ ‡ç­¾æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä¸€æ˜¯å¯¹åˆ†å¸ƒå¤–è¯­ä¹‰ç©ºé—´çš„ç¨€ç–é‡‡æ ·ä¸è¶³ï¼ŒäºŒæ˜¯æ–‡æœ¬ä»£ç†é™æ€ä¸å˜è€Œè§†è§‰ç‰¹å¾æ¼‚ç§»å¯¼è‡´è·¨æ¨¡æ€é”™ä½å’Œé¢„æµ‹ä¸ç¨³å®šï¼Œè¿™é™åˆ¶äº†é›¶æ ·æœ¬OODæ£€æµ‹åœ¨åˆ†å¸ƒåç§»ä¸‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Method:</strong> CoEvoé‡‡ç”¨æ— éœ€è®­ç»ƒå’Œæ ‡æ³¨çš„æµ‹è¯•æ—¶æ¡†æ¶ï¼Œé€šè¿‡ä»£ç†å¯¹é½çš„ååŒè¿›åŒ–æœºåˆ¶ç»´æŠ¤ä¸¤ä¸ªè¿›åŒ–çš„ä»£ç†ç¼“å­˜ï¼Œå®ç°åŒå‘ã€æ ·æœ¬æ¡ä»¶åŒ–çš„è‡ªé€‚åº”ã€‚è¯¥æ–¹æ³•åŠ¨æ€æŒ–æ˜ç”±æµ‹è¯•å›¾åƒå¼•å¯¼çš„ä¸Šä¸‹æ–‡æ–‡æœ¬è´Ÿä¾‹ï¼Œè¿­ä»£ä¼˜åŒ–è§†è§‰ä»£ç†ï¼Œé€æ­¥é‡æ–°å¯¹é½è·¨æ¨¡æ€ç›¸ä¼¼æ€§å¹¶æ‰©å¤§å±€éƒ¨OODè¾¹ç•Œï¼Œæœ€ååŠ¨æ€é‡æ–°åŠ æƒåŒæ¨¡æ€ä»£ç†çš„è´¡çŒ®ä»¥è·å¾—å¯¹åˆ†å¸ƒåç§»é²æ£’çš„æ ¡å‡†OODåˆ†æ•°ã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCoEvoå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸æ¯”å¼ºè´Ÿæ ‡ç­¾åŸºçº¿ï¼Œåœ¨ImageNet-1Kä¸ŠAUROCæå‡äº†1.33%ï¼ŒFPR95é™ä½äº†45.98%ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„é›¶æ ·æœ¬OODæ£€æµ‹èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡åŒå‘ã€æ ·æœ¬æ¡ä»¶åŒ–çš„ä»£ç†è‡ªé€‚åº”æœºåˆ¶å¯ä»¥æœ‰æ•ˆè§£å†³é›¶æ ·æœ¬OODæ£€æµ‹ä¸­çš„è·¨æ¨¡æ€é”™ä½é—®é¢˜ï¼ŒåŠ¨æ€ä»£ç†ç¼“å­˜å’ŒååŒè¿›åŒ–ç­–ç•¥ä¸ºå¼€æ”¾ä¸–ç•Œè§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é éƒ¨ç½²æä¾›äº†æ–°æ€è·¯ï¼Œæœªæ¥å¯æ‰©å±•åˆ°æ›´å¤æ‚çš„å¤šæ¨¡æ€åœºæ™¯å’ŒåŠ¨æ€ç¯å¢ƒé€‚åº”ä¸­ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>Reliable zero-shot detection of out-of-distribution (OOD) inputs is critical for deploying vision-language models in open-world settings. However, the lack of labeled negatives in zero-shot OOD detection necessitates proxy signals that remain effective under distribution shift. Existing negative-label methods rely on a fixed set of textual proxies, which (i) sparsely sample the semantic space beyond in-distribution (ID) classes and (ii) remain static while only visual features drift, leading to cross-modal misalignment and unstable predictions. In this paper, we propose CoEvo, a training- and annotation-free test-time framework that performs bidirectional, sample-conditioned adaptation of both textual and visual proxies. Specifically, CoEvo introduces a proxy-aligned co-evolution mechanism to maintain two evolving proxy caches, which dynamically mines contextual textual negatives guided by test images and iteratively refines visual proxies, progressively realigning cross-modal similarities and enlarging local OOD margins. Finally, we dynamically re-weight the contributions of dual-modal proxies to obtain a calibrated OOD score that is robust to distribution shift. Extensive experiments on standard benchmarks demonstrate that CoEvo achieves state-of-the-art performance, improving AUROC by 1.33% and reducing FPR95 by 45.98% on ImageNet-1K compared to strong negative-label baselines.</p>
<h3 id="37-end-to-end-video-character-replacement-without-structural-guidance">[37] <a href="https://arxiv.org/abs/2601.08587">End-to-End Video Character Replacement without Structural Guidance</a></h3>
<p><em>Zhengbo Xu, Jie Ma, Ziheng Wang, Zhan Peng, Jun Liang, Jing Li</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MoChaæ¡†æ¶ï¼Œé€šè¿‡ä»…éœ€å•å¸§ä»»æ„æ©ç å®ç°å¯æ§è§†é¢‘è§’è‰²æ›¿æ¢ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•å¯¹é€å¸§åˆ†å‰²æ©ç å’Œæ˜¾å¼ç»“æ„æŒ‡å¯¼çš„ä¾èµ–ï¼Œæ˜¾è‘—æå‡äº†åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›å’Œè§†è§‰è´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¯æ§è§†é¢‘è§’è‰²æ›¿æ¢é¢ä¸´ç¼ºä¹é…å¯¹è§†é¢‘æ•°æ®çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–é‡å»ºèŒƒå¼ï¼Œéœ€è¦é€å¸§åˆ†å‰²æ©ç å’Œæ˜¾å¼ç»“æ„æŒ‡å¯¼ï¼ˆå¦‚éª¨æ¶ã€æ·±åº¦ï¼‰ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†åœ¨é®æŒ¡ã€è§’è‰²-ç‰©ä½“äº¤äº’ã€å¼‚å¸¸å§¿æ€æˆ–å¤æ‚å…‰ç…§ç­‰å¤æ‚åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¸¸å¯¼è‡´è§†è§‰ä¼ªå½±å’Œæ—¶é—´ä¸ä¸€è‡´æ€§ã€‚</p>
<p><strong>Method:</strong> MoChaæ¡†æ¶ä»…éœ€å•å¸§ä»»æ„æ©ç ï¼Œæ— éœ€é€å¸§åˆ†å‰²æˆ–æ˜¾å¼ç»“æ„æŒ‡å¯¼ï¼›å¼•å…¥äº†æ¡ä»¶æ„ŸçŸ¥çš„RoPEæœºåˆ¶æ¥é€‚åº”å¤šæ¨¡æ€è¾“å…¥æ¡ä»¶å¹¶å¢å¼ºé¢éƒ¨èº«ä»½ç‰¹å¾ï¼›é‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒé˜¶æ®µï¼›ä¸ºè§£å†³åˆæ ¼é…å¯¹è®­ç»ƒæ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæå‡ºäº†ç»¼åˆæ•°æ®æ„å»ºæµç¨‹ï¼ŒåŒ…æ‹¬åŸºäºUnreal Engine 5æ„å»ºçš„é«˜ä¿çœŸæ¸²æŸ“æ•°æ®é›†ã€é€šè¿‡å½“å‰è‚–åƒåŠ¨ç”»æŠ€æœ¯åˆæˆçš„è¡¨æƒ…é©±åŠ¨æ•°æ®é›†ï¼Œä»¥åŠä»ç°æœ‰è§†é¢‘-æ©ç å¯¹è¡ç”Ÿçš„å¢å¼ºæ•°æ®é›†ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸‹è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡å°‘äº†è§†è§‰ä¼ªå½±å¹¶æé«˜äº†æ—¶é—´ä¸€è‡´æ€§ï¼Œå®ç°äº†æ›´é«˜è´¨é‡çš„è§†é¢‘è§’è‰²æ›¿æ¢æ•ˆæœã€‚</p>
<p><strong>Conclusion:</strong> MoChaé€šè¿‡ç®€åŒ–è¾“å…¥è¦æ±‚å¹¶æ„å»ºç»¼åˆè®­ç»ƒæ•°æ®ï¼Œä¸ºå¯æ§è§†é¢‘è§’è‰²æ›¿æ¢æä¾›äº†æ›´å®ç”¨å’Œé²æ£’çš„è§£å†³æ–¹æ¡ˆï¼Œå…¶æ¡ä»¶æ„ŸçŸ¥æœºåˆ¶å’Œåè®­ç»ƒç­–ç•¥ä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°æ€è·¯ï¼Œä»£ç çš„å‘å¸ƒå°†ä¿ƒè¿›è¯¥æ–¹å‘çš„è¿›ä¸€æ­¥æ¢ç´¢ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha</p>
<h3 id="38-soc-semantic-orthogonal-calibration-for-test-time-prompt-tuning">[38] <a href="https://arxiv.org/abs/2601.08617">SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning</a></h3>
<p><em>Leo Fillioux, Omprakash Chakraborty, Ismail Ben Ayed, Paul-Henry CournÃ¨de, Stergios Christodoulidis, Maria Vakalopoulou, Jose Dolz</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè¯­ä¹‰æ­£äº¤æ ¡å‡†ï¼ˆSoCï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºæ”¹å–„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶æç¤ºè°ƒä¼˜ä¸­çš„ä¸ç¡®å®šæ€§æ ¡å‡†é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡Huber-basedæ­£åˆ™åŒ–å™¨å®ç°å¹³æ»‘çš„åŸå‹åˆ†ç¦»ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰é‚»è¿‘æ€§ï¼Œä»è€Œåœ¨ä¿æŒç«äº‰æ€§åˆ¤åˆ«èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡æ ¡å‡†æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—å’Œè‡ªåŠ¨é©¾é©¶ç­‰å…³é”®å†³ç­–ç³»ç»Ÿä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå…¶ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ ¡å‡†å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰VLMæµ‹è¯•æ—¶æç¤ºè°ƒä¼˜ç ”ç©¶ä¸»è¦å…³æ³¨æå‡åˆ¤åˆ«æ€§èƒ½ï¼Œè€Œæ ¡å‡†ç»´åº¦åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«å……åˆ†æ¢ç´¢ã€‚å½“å‰æœ€å…ˆè¿›æ–¹æ³•å¼ºåˆ¶æ–‡æœ¬æç¤ºåµŒå…¥çš„å®Œå…¨æ­£äº¤æ€§ä»¥å¢å¼ºå¯åˆ†ç¦»æ€§ï¼Œä½†ç†è®ºåˆ†æè¡¨æ˜è¿™ç§å®Œå…¨æ­£äº¤çº¦æŸä¼šè¿‡åº¦æ¨åŠ¨è¯­ä¹‰ç›¸å…³ç±»åˆ«åˆ†ç¦»ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦è‡ªä¿¡ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†è¯­ä¹‰æ­£äº¤æ ¡å‡†ï¼ˆSoCï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºHuberçš„æ­£åˆ™åŒ–å™¨ï¼Œæ—¨åœ¨å®ç°å¹³æ»‘çš„åŸå‹åˆ†ç¦»åŒæ—¶ä¿æŒè¯­ä¹‰é‚»è¿‘æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ç†è®ºåˆ†ææ­ç¤ºäº†å®Œå…¨æ­£äº¤çº¦æŸçš„å±€é™æ€§ï¼Œå³å…¶æ¢¯åº¦ä¼šå¼ºçƒˆæ¨åŠ¨è¯­ä¹‰ç›¸å…³ç±»åˆ«åˆ†ç¦»ï¼Œä»è€Œè®¾è®¡å‡ºèƒ½å¤Ÿå¹³è¡¡å¯åˆ†ç¦»æ€§å’Œè¯­ä¹‰ä¿æŒçš„æ­£åˆ™åŒ–ç­–ç•¥ï¼Œç›¸æ¯”å…ˆå‰åŸºäºæ­£äº¤æ€§çš„æ–¹æ³•èƒ½æ›´å¥½åœ°æ”¹å–„æ ¡å‡†æ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡å…¨é¢çš„å®è¯éªŒè¯ï¼Œç ”ç©¶è¡¨æ˜SoCæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸€è‡´åœ°æ”¹å–„äº†æ ¡å‡†æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨æå‡æ ¡å‡†è´¨é‡çš„åŒæ—¶ï¼Œä»ä¿æŒäº†ç«äº‰æ€§çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œåœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æµ‹è¯•æ—¶æç¤ºè°ƒä¼˜åœºæ™¯ä¸­å®ç°äº†æ ¡å‡†ä¸æ€§èƒ½çš„å¹³è¡¡ä¼˜åŒ–ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶æ­ç¤ºäº†å®Œå…¨æ­£äº¤çº¦æŸåœ¨è§†è§‰è¯­è¨€æ¨¡å‹æ ¡å‡†ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ›´æœ‰æ•ˆçš„è¯­ä¹‰æ„ŸçŸ¥æ ¡å‡†æ–¹æ³•ã€‚SoCæ–¹æ³•é€šè¿‡å¹³è¡¡åŸå‹åˆ†ç¦»å’Œè¯­ä¹‰ä¿æŒï¼Œä¸ºVLMä¸ç¡®å®šæ€§æ ¡å‡†æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¯¹åŒ»ç–—å’Œè‡ªåŠ¨é©¾é©¶ç­‰é«˜é£é™©åº”ç”¨ä¸­çš„å¯é å†³ç­–å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†ç†è®ºæŒ‡å¯¼å’Œå®ç”¨å·¥å…·ã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>With the increasing adoption of vision-language models (VLMs) in critical decision-making systems such as healthcare or autonomous driving, the calibration of their uncertainty estimates becomes paramount. Yet, this dimension has been largely underexplored in the VLM test-time prompt-tuning (TPT) literature, which has predominantly focused on improving their discriminative performance. Recent state-of-the-art advocates for enforcing full orthogonality over pairs of text prompt embeddings to enhance separability, and therefore calibration. Nevertheless, as we theoretically show in this work, the inherent gradients from fully orthogonal constraints will strongly push semantically related classes away, ultimately making the model overconfident. Based on our findings, we propose Semantic Orthogonal Calibration (SoC), a Huber-based regularizer that enforces smooth prototype separation while preserving semantic proximity, thereby improving calibration compared to prior orthogonality-based approaches. Across a comprehensive empirical validation, we demonstrate that SoC consistently improves calibration performance, while also maintaining competitive discriminative capabilities.</p>
<h3 id="39-ctrlfuse-mask-prompt-guided-controllable-infrared-and-visible-image-fusion">[39] <a href="https://arxiv.org/abs/2601.08619">CtrlFuse: Mask-Prompt Guided Controllable Infrared and Visible Image Fusion</a></h3>
<p><em>Yiming Sun, Yuan Ruan, Qinghua Hu, Pengfei Zhu</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCtrlFuseï¼Œä¸€ç§å¯æ§çš„çº¢å¤–ä¸å¯è§å…‰å›¾åƒèåˆæ¡†æ¶ï¼Œé€šè¿‡æ©ç æç¤ºå®ç°äº¤äº’å¼åŠ¨æ€èåˆï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å¿½è§†ä¸‹æ¸¸ä»»åŠ¡é€‚åº”æ€§å’Œæ— æ³•äº¤äº’å¤„ç†å¤šæ ·åŒ–è¯­ä¹‰ç›®æ ‡æ„ŸçŸ¥éœ€æ±‚çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çº¢å¤–ä¸å¯è§å…‰å›¾åƒèåˆæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šä¸€æ˜¯ä¸“æ³¨äºåƒç´ çº§èåˆè€Œå¿½è§†ä¸‹æ¸¸ä»»åŠ¡é€‚åº”æ€§ï¼ŒäºŒæ˜¯é€šè¿‡çº§è”æ£€æµ‹/åˆ†å‰²æ¨¡å‹éšå¼å­¦ä¹ åˆšæ€§è¯­ä¹‰ï¼Œæ— æ³•äº¤äº’å¼å¤„ç†å¤šæ ·åŒ–çš„è¯­ä¹‰ç›®æ ‡æ„ŸçŸ¥éœ€æ±‚ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§å¯æ§çš„èåˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å…·ä½“ä»»åŠ¡éœ€æ±‚åŠ¨æ€è°ƒæ•´èåˆç­–ç•¥ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„CtrlFuseæ¡†æ¶åŒ…å«å¤šæ¨¡æ€ç‰¹å¾æå–å™¨ã€å‚è€ƒæç¤ºç¼–ç å™¨ï¼ˆRPEï¼‰å’Œæç¤º-è¯­ä¹‰èåˆæ¨¡å—ï¼ˆPSFMï¼‰ã€‚RPEé€šè¿‡è¾“å…¥æ©ç å¼•å¯¼å¾®è°ƒé¢„è®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼ŒåŠ¨æ€ç¼–ç ä»»åŠ¡ç‰¹å®šçš„è¯­ä¹‰æç¤ºï¼›PSFMå°†è¿™äº›è¯­ä¹‰æ˜ç¡®æ³¨å…¥èåˆç‰¹å¾ä¸­ã€‚é€šè¿‡å¹¶è¡Œåˆ†å‰²å’Œèåˆåˆ†æ”¯çš„ååŒä¼˜åŒ–ï¼Œå®ç°ä»»åŠ¡æ€§èƒ½ä¸èåˆè´¨é‡çš„ç›¸äº’å¢å¼ºã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼ŒCtrlFuseåœ¨èåˆå¯æ§æ€§å’Œåˆ†å‰²å‡†ç¡®æ€§æ–¹é¢å‡è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚ç‰¹åˆ«å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»è¿‡é€‚é…çš„ä»»åŠ¡åˆ†æ”¯ç”šè‡³è¶…è¶Šäº†åŸå§‹åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ï¼Œè¯æ˜äº†æ‰€ææ¡†æ¶åœ¨å¢å¼ºä¸‹æ¸¸ä»»åŠ¡è¡¨ç°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡äº¤äº’å¼åŠ¨æ€èåˆæ¡†æ¶å®ç°ä»»åŠ¡æ„ŸçŸ¥å‹å›¾åƒèåˆçš„å¯è¡Œæ€§ï¼Œä¸ºæ™ºèƒ½æ— äººç³»ç»Ÿçš„ç¯å¢ƒæ„ŸçŸ¥æä¾›äº†æ›´çµæ´»å’Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æ¡†æ¶çš„å¯æ§æ€§è®¾è®¡ä¸ºå¤šæ¨¡æ€èåˆé¢†åŸŸå¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œèƒ½å¤Ÿæ ¹æ®å…·ä½“åº”ç”¨éœ€æ±‚å®šåˆ¶èåˆç­–ç•¥ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Infrared and visible image fusion generates all-weather perception-capable images by combining complementary modalities, enhancing environmental awareness for intelligent unmanned systems. Existing methods either focus on pixel-level fusion while overlooking downstream task adaptability or implicitly learn rigid semantics through cascaded detection/segmentation models, unable to interactively address diverse semantic target perception needs. We propose CtrlFuse, a controllable image fusion framework that enables interactive dynamic fusion guided by mask prompts. The model integrates a multi-modal feature extractor, a reference prompt encoder (RPE), and a prompt-semantic fusion module (PSFM). The RPE dynamically encodes task-specific semantic prompts by fine-tuning pre-trained segmentation models with input mask guidance, while the PSFM explicitly injects these semantics into fusion features. Through synergistic optimization of parallel segmentation and fusion branches, our method achieves mutual enhancement between task performance and fusion quality. Experiments demonstrate state-of-the-art results in both fusion controllability and segmentation accuracy, with the adapted task branch even outperforming the original segmentation model.</p>
<h3 id="40-near-perfect-photo-id-of-the-hula-painted-frog-with-zero-shot-deep-local-feature-matching">[40] <a href="https://arxiv.org/abs/2601.08798">Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching</a></h3>
<p><em>Maayan Yesharim, R. G. Bina Perl, Uri Roll, Sarig Gafny, Eli Geffen, Yoav Ram</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºæ¿’å±ä¸¤æ –åŠ¨ç‰©èƒ¡æ‹‰å½©è›™çš„éä¾µå…¥å¼ç…§ç‰‡é‡è¯†åˆ«æ–¹æ³•ï¼Œé€šè¿‡æ¯”è¾ƒå±€éƒ¨ç‰¹å¾åŒ¹é…ä¸å…¨å±€ç‰¹å¾åµŒå…¥æ¨¡å‹ï¼Œå¼€å‘äº†ä¸€ä¸ªç»“åˆä¸¤è€…ä¼˜åŠ¿çš„ä¸¤é˜¶æ®µå·¥ä½œæµç¨‹ï¼Œå®ç°äº†é«˜ç²¾åº¦ä¸”å¯æ‰©å±•çš„ä¸ªä½“è¯†åˆ«ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å‡†ç¡®è¯†åˆ«ä¸ªä½“å¯¹äºç›‘æµ‹ç¨€æœ‰ä¸¤æ –åŠ¨ç‰©è‡³å…³é‡è¦ï¼Œä½†ä¾µå…¥å¼æ ‡è®°æ–¹æ³•é€šå¸¸ä¸é€‚ç”¨äºæåº¦æ¿’å±ç‰©ç§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°æœ€å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰æ–¹æ³•ï¼Œä¸ºèƒ¡æ‹‰å½©è›™å¼€å‘ä¸€ç§éä¾µå…¥å¼çš„ç…§ç‰‡é‡è¯†åˆ«æŠ€æœ¯ï¼Œä»¥æ”¯æŒä¿æŠ¤ç›‘æµ‹å’Œæ•è·-é‡æ•è·åˆ†æã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ¯”è¾ƒäº†é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æ·±åº¦å±€éƒ¨ç‰¹å¾åŒ¹é…ä¸æ·±åº¦å…¨å±€ç‰¹å¾åµŒå…¥æ¨¡å‹ï¼Œä½¿ç”¨äº†2013-2020å¹´é—´é‡‡é›†çš„191åªä¸ªä½“çš„1,233å¼ è…¹é¢å›¾åƒã€‚ä¸ºäº†ç»“åˆå¯æ‰©å±•æ€§ä¸å‡†ç¡®æ€§ï¼Œå®ç°äº†ä¸€ä¸ªä¸¤é˜¶æ®µå·¥ä½œæµç¨‹ï¼šé¦–å…ˆä½¿ç”¨å¾®è°ƒåçš„å…¨å±€ç‰¹å¾æ¨¡å‹æ£€ç´¢å€™é€‰åˆ—è¡¨ï¼Œç„¶åé€šè¿‡å±€éƒ¨ç‰¹å¾åŒ¹é…è¿›è¡Œé‡æ–°æ’åºã€‚</p>
<p><strong>Result:</strong> å±€éƒ¨ç‰¹å¾ç®¡é“åœ¨å°é—­é›†è¯†åˆ«ä¸­è¾¾åˆ°äº†98%çš„top-1å‡†ç¡®ç‡ï¼Œä¼˜äºæ‰€æœ‰å…¨å±€ç‰¹å¾æ¨¡å‹ï¼›å¾®è°ƒåçš„æœ€ä½³å…¨å±€ç‰¹å¾æ¨¡å‹è¾¾åˆ°60% top-1å‡†ç¡®ç‡ï¼ˆ91% top-10ï¼‰ã€‚ä¸¤é˜¶æ®µå·¥ä½œæµç¨‹å°†ç«¯åˆ°ç«¯è¿è¡Œæ—¶é—´ä»6.5-7.8å°æ—¶å‡å°‘åˆ°çº¦38åˆ†é’Ÿï¼ŒåŒæ—¶ä¿æŒçº¦96%çš„top-1å°é—­é›†å‡†ç¡®ç‡ã€‚ç›¸åŒä¸ªä½“ä¸ä¸åŒä¸ªä½“å¯¹çš„åŒ¹é…åˆ†æ•°åˆ†ç¦»æ”¯æŒå¼€æ”¾é›†è¯†åˆ«çš„é˜ˆå€¼è®¾ç½®ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºè¯¥ç‰©ç§ï¼Œé›¶æ ·æœ¬æ·±åº¦å±€éƒ¨ç‰¹å¾åŒ¹é…ä¼˜äºå…¨å±€ç‰¹å¾åµŒå…¥ï¼Œå¯ä½œä¸ºç…§ç‰‡è¯†åˆ«çš„å¼ºå¤§é»˜è®¤æ–¹æ³•ã€‚å¼€å‘çš„ä¸¤é˜¶æ®µç®¡é“ç»“åˆäº†å¯æ‰©å±•æ€§ä¸å‡†ç¡®æ€§ï¼Œå·²éƒ¨ç½²ä¸ºæ”¯æŒå¸¸è§„é‡å¤–ä½¿ç”¨çš„Webåº”ç”¨ç¨‹åºï¼Œä¸ºéä¾µå…¥å¼ä¿æŠ¤ç›‘æµ‹æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>Accurate individual identification is essential for monitoring rare amphibians, yet invasive marking is often unsuitable for critically endangered species. We evaluate state-of-the-art computer-vision methods for photographic re-identification of the Hula painted frog (Latonia nigriventer) using 1,233 ventral images from 191 individuals collected during 2013-2020 capture-recapture surveys. We compare deep local-feature matching in a zero-shot setting with deep global-feature embedding models. The local-feature pipeline achieves 98% top-1 closed-set identification accuracy, outperforming all global-feature models; fine-tuning improves the best global-feature model to 60% top-1 (91% top-10) but remains below local matching. To combine scalability with accuracy, we implement a two-stage workflow in which a fine-tuned global-feature model retrieves a short candidate list that is re-ranked by local-feature matching, reducing end-to-end runtime from 6.5-7.8 hours to ~38 minutes while maintaining ~96% top-1 closed-set accuracy on the labeled dataset. Separation of match scores between same- and different-individual pairs supports thresholding for open-set identification, enabling practical handling of novel individuals. We deploy this pipeline as a web application for routine field use, providing rapid, standardized, non-invasive identification to support conservation monitoring and capture-recapture analyses. Overall, in this species, zero-shot deep local-feature matching outperformed global-feature embedding and provides a strong default for photo-identification.</p>
<h3 id="41-raven-erasing-invisible-watermarks-via-novel-view-synthesis">[41] <a href="https://arxiv.org/abs/2601.08832">RAVEN: Erasing Invisible Watermarks via Novel View Synthesis</a></h3>
<p><em>Fahad Shamshad, Nils Lukas, Karthik Nandakumar</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æ­ç¤ºäº†ä¸å¯è§æ°´å°çš„åŸºæœ¬è„†å¼±æ€§ï¼Œé€šè¿‡å°†æ°´å°å»é™¤é‡æ–°è¡¨è¿°ä¸ºè§†å›¾åˆæˆé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é›¶æ ·æœ¬æ‰©æ•£æ¡†æ¶ï¼Œåœ¨15ç§æ°´å°æ–¹æ³•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ°´å°æŠ‘åˆ¶æ•ˆæœï¼ŒåŒæ—¶ä¿æŒå“è¶Šçš„æ„ŸçŸ¥è´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡ä¸å¯è§æ°´å°å·²æˆä¸ºè®¤è¯AIç”Ÿæˆå›¾åƒå†…å®¹çš„å…³é”®æœºåˆ¶ï¼Œä½†è¯„ä¼°è¿™äº›æ–¹æ¡ˆå¯¹æŠ—å¤æ‚å»é™¤æ”»å‡»çš„è„†å¼±æ€§å¯¹äºè¯„ä¼°å…¶å¯é æ€§å’ŒæŒ‡å¯¼ç¨³å¥è®¾è®¡è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ­ç¤ºä¸å¯è§æ°´å°çš„åŸºæœ¬è„†å¼±æ€§ï¼Œç‰¹åˆ«æ˜¯å‘ç°æ°´å°å³ä½¿å¯¹åƒç´ ç©ºé—´å’Œé¢‘åŸŸæ”»å‡»å…·æœ‰é²æ£’æ€§ï¼Œä»ç„¶å®¹æ˜“å—åˆ°è¯­ä¹‰ä¿æŒçš„è§†ç‚¹å˜æ¢æ”»å‡»ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§é›¶æ ·æœ¬æ‰©æ•£æ¡†æ¶ï¼Œå°†æ°´å°å»é™¤é‡æ–°è¡¨è¿°ä¸ºè§†å›¾åˆæˆé—®é¢˜ã€‚è¯¥æ–¹æ³•çš„å…³é”®æ´å¯Ÿæ˜¯ç”Ÿæˆç›¸åŒè¯­ä¹‰å†…å®¹çš„æ„ŸçŸ¥ä¸€è‡´æ›¿ä»£è§†å›¾ï¼Œç±»ä¼¼äºä»åç§»è§†è§’é‡æ–°è§‚å¯Ÿåœºæ™¯ï¼Œä»è€Œè‡ªç„¶å»é™¤åµŒå…¥æ°´å°åŒæ—¶ä¿æŒè§†è§‰ä¿çœŸåº¦ã€‚æ¡†æ¶åœ¨æ½œåœ¨ç©ºé—´ä¸­åº”ç”¨å—æ§å‡ ä½•å˜æ¢ï¼Œå¹¶é€šè¿‡è§†å›¾å¼•å¯¼å¯¹åº”æ³¨æ„åŠ›å¢å¼ºä»¥åœ¨é‡å»ºè¿‡ç¨‹ä¸­ä¿æŒç»“æ„ä¸€è‡´æ€§ï¼Œæ— éœ€è®¿é—®æ£€æµ‹å™¨æˆ–æ°´å°çŸ¥è¯†å³å¯åœ¨å†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹ä¸Šæ“ä½œã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨15ç§æ°´å°æ–¹æ³•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ°´å°æŠ‘åˆ¶æ•ˆæœï¼Œè¶…è¶Šäº†14ç§åŸºçº¿æ”»å‡»æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¿æŒå“è¶Šçš„æ„ŸçŸ¥è´¨é‡ï¼ŒåŒæ—¶æœ‰æ•ˆå»é™¤æ°´å°ï¼ŒéªŒè¯äº†è¯­ä¹‰ä¿æŒè§†ç‚¹å˜æ¢å¯¹æ°´å°å»é™¤çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶æ­ç¤ºäº†ä¸å¯è§æ°´å°çš„åŸºæœ¬è„†å¼±æ€§ï¼Œå³æ°´å°å³ä½¿å¯¹ä¼ ç»Ÿåƒç´ ç©ºé—´å’Œé¢‘åŸŸæ”»å‡»å…·æœ‰é²æ£’æ€§ï¼Œä»ç„¶å®¹æ˜“å—åˆ°è¯­ä¹‰ä¿æŒçš„è§†ç‚¹å˜æ¢æ”»å‡»ã€‚è¿™ä¸€å‘ç°ä¸ºæ°´å°æ–¹æ¡ˆçš„ç¨³å¥è®¾è®¡æä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œå¼ºè°ƒäº†éœ€è¦è€ƒè™‘è¯­ä¹‰çº§æ”»å‡»å‘é‡ï¼Œå¹¶ä¸ºæœªæ¥æ°´å°æŠ€æœ¯å‘å±•æä¾›äº†å…³é”®è§è§£ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>Invisible watermarking has become a critical mechanism for authenticating AI-generated image content, with major platforms deploying watermarking schemes at scale. However, evaluating the vulnerability of these schemes against sophisticated removal attacks remains essential to assess their reliability and guide robust design. In this work, we expose a fundamental vulnerability in invisible watermarks by reformulating watermark removal as a view synthesis problem. Our key insight is that generating a perceptually consistent alternative view of the same semantic content, akin to re-observing a scene from a shifted perspective, naturally removes the embedded watermark while preserving visual fidelity. This reveals a critical gap: watermarks robust to pixel-space and frequency-domain attacks remain vulnerable to semantic-preserving viewpoint transformations. We introduce a zero-shot diffusion-based framework that applies controlled geometric transformations in latent space, augmented with view-guided correspondence attention to maintain structural consistency during reconstruction. Operating on frozen pre-trained models without detector access or watermark knowledge, our method achieves state-of-the-art watermark suppression across 15 watermarking methods--outperforming 14 baseline attacks while maintaining superior perceptual quality across multiple datasets.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="42-cross-cultural-expert-level-art-critique-evaluation-with-vision-language-models">[42] <a href="https://arxiv.org/abs/2601.07984">Cross-Cultural Expert-Level Art Critique Evaluation with Vision-Language Models</a></h3>
<p><em>Haorui Yu, Ramon Ruiz-Dolz, Xuehang Wen, Fengrui Zhang, Qiufeng Yi</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸‰å±‚è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è·¨æ–‡åŒ–è‰ºæœ¯æ‰¹è¯„ä¸­çš„æ–‡åŒ–ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡æ ¡å‡†è¯„åˆ†æœºåˆ¶æ˜¾è‘—é™ä½äº†ä¸äººç±»è¯„åˆ†çš„è¯¯å·®ï¼Œå¹¶æ­ç¤ºäº†ç°æœ‰è‡ªåŠ¨æŒ‡æ ‡åœ¨è¡¡é‡æ–‡åŒ–æ·±åº¦æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨è‰ºæœ¯ä½œå“ä¸­è§£è¯»æ–‡åŒ–æ„ä¹‰çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†éªŒè¯ï¼Œç°æœ‰è¯„ä¼°æ–¹æ³•ç¼ºä¹å¯¹è·¨æ–‡åŒ–è‰ºæœ¯æ‰¹è¯„ä¸­æ–‡åŒ–ç†è§£æ·±åº¦çš„ç³»ç»Ÿè¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸‰å±‚è¯„ä¼°æ¡†æ¶ï¼šç¬¬ä¸€å±‚ç¦»çº¿è®¡ç®—è‡ªåŠ¨è¦†ç›–ç‡å’Œé£é™©æŒ‡æ ‡ï¼›ç¬¬äºŒå±‚é‡‡ç”¨å•ä¸€ä¸»è¯„å®¡å‘˜åŸºäºäº”ä¸ªç»´åº¦è¿›è¡ŒåŸºäºé‡è§„çš„è¯„åˆ†ï¼›ç¬¬ä¸‰å±‚é€šè¿‡ä¿åºå›å½’å°†ç¬¬äºŒå±‚èšåˆåˆ†æ•°æ ¡å‡†åˆ°äººç±»è¯„åˆ†ï¼Œä»è€Œäº§ç”Ÿæ ¡å‡†åçš„æ–‡åŒ–ç†è§£åˆ†æ•°ã€‚</p>
<p><strong>Result:</strong> è¯¥æ¡†æ¶åœ¨152ä¸ªæ ·æœ¬çš„ä¿ç•™é›†ä¸Šå®ç°äº†5.2%çš„å¹³å‡ç»å¯¹è¯¯å·®é™ä½ï¼Œè¯„ä¼°äº†15ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¶µç›–å…­ç§æ–‡åŒ–ä¼ ç»Ÿçš„294ä¸ªä¸“å®¶é”šç‚¹ä¸Šçš„è¡¨ç°ï¼Œå‘ç°è‡ªåŠ¨æŒ‡æ ‡æ— æ³•å¯é ä»£ç†æ–‡åŒ–æ·±åº¦ï¼Œè¥¿æ–¹æ ·æœ¬å¾—åˆ†é«˜äºéè¥¿æ–¹æ ·æœ¬ï¼Œä¸”è·¨è¯„å®¡å‘˜å°ºåº¦ä¸åŒ¹é…ä½¿å¾—æœ´ç´ å¹³å‡è¯„åˆ†ä¸å¯é ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å¼ºè°ƒäº†éœ€è¦ä¸“é—¨çš„æ–‡åŒ–ç†è§£è¯„ä¼°æ¡†æ¶è€Œéä¾èµ–é€šç”¨è‡ªåŠ¨æŒ‡æ ‡ï¼Œå•ä¸€ä¸»è¯„å®¡å‘˜é…åˆæ˜¾å¼æ ¡å‡†çš„æ–¹æ³•æ›´å¯é ï¼Œæ¡†æ¶è¾“å‡ºçš„æ ¡å‡†åˆ†æ•°å¯ç”¨äºæ¨¡å‹é€‰æ‹©å’Œæ–‡åŒ–å·®è·è¯Šæ–­ï¼Œä¸ºè·¨æ–‡åŒ–è‰ºæœ¯AIè¯„ä¼°æä¾›äº†ç³»ç»Ÿæ–¹æ³•è®ºã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) excel at visual perception, yet their ability to interpret cultural meaning in art remains under-validated. We present a tri-tier evaluation framework for cross-cultural art-critique assessment: Tier I computes automated coverage and risk indicators offline; Tier II applies rubric-based scoring using a single primary judge across five dimensions; and Tier III calibrates the Tier II aggregate score to human ratings via isotonic regression, yielding a 5.2% reduction in MAE on a 152-sample held-out set. The framework outputs a calibrated cultural-understanding score for model selection and cultural-gap diagnosis, together with dimension-level diagnostics and risk indicators. We evaluate 15 VLMs on 294 expert anchors spanning six cultural traditions. Key findings are that (i) automated metrics are unreliable proxies for cultural depth, (ii) Western samples score higher than non-Western samples under our sampling and rubric, and (iii) cross-judge scale mismatch makes naive score averaging unreliable, motivating a single primary judge with explicit calibration. Dataset and code are available in the supplementary materials.</p>
<h3 id="43-multilingual-multimodal-pipeline-for-creating-authentic-and-structured-fact-checked-claim-dataset">[43] <a href="https://arxiv.org/abs/2601.07985">Multilingual, Multimodal Pipeline for Creating Authentic and Structured Fact-Checked Claim Dataset</a></h3>
<p><em>Z. Melce HÃ¼sÃ¼nbeyi, Virginie Mouilleron, Leonie Uhling, Daniel Foppe, Tatjana Scheffler, DjamÃ© Seddah</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ„å»ºå¤šè¯­è¨€ã€å¤šæ¨¡æ€äº‹å®æ ¸æŸ¥æ•°æ®é›†çš„æ•°æ®æ”¶é›†ä¸å¤„ç†æµç¨‹ï¼Œé€šè¿‡èšåˆClaimReviewæºã€æŠ“å–å®Œæ•´è¾Ÿè°£æ–‡ç« ã€è§„èŒƒåŒ–å¼‚æ„å£°æ˜è£å†³ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¯æ®æå–å’Œç†ç”±ç”Ÿæˆï¼Œä¸ºå¯è§£é‡Šçš„äº‹å®æ ¸æŸ¥æ¨¡å‹å¼€å‘å¥ å®šåŸºç¡€ã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨çº¿å¹³å°ä¸Šçš„è™šå‡ä¿¡æ¯å¿«é€Ÿä¼ æ’­å‡¸æ˜¾äº†å¯¹å¼ºå¤§ã€æœ€æ–°ã€å¯è§£é‡Šä¸”å¤šè¯­è¨€äº‹å®æ ¸æŸ¥èµ„æºçš„è¿«åˆ‡éœ€æ±‚ï¼Œç„¶è€Œç°æœ‰æ•°æ®é›†åœ¨èŒƒå›´ä¸Šå­˜åœ¨å±€é™ï¼Œé€šå¸¸ç¼ºä¹å¤šæ¨¡æ€è¯æ®ã€ç»“æ„åŒ–æ ‡æ³¨ä»¥åŠå£°æ˜ã€è¯æ®ä¸è£å†³ä¹‹é—´çš„è¯¦ç»†å…³è”ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®æ”¶é›†ä¸å¤„ç†æµç¨‹ï¼Œé€šè¿‡èšåˆClaimReviewæºã€æŠ“å–å®Œæ•´è¾Ÿè°£æ–‡ç« ã€è§„èŒƒåŒ–å¼‚æ„å£°æ˜è£å†³ï¼Œå¹¶åˆ©ç”¨æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¯æ®æå–å’Œç†ç”±ç”Ÿæˆï¼ŒåŒæ—¶ä¸ºæ•°æ®æ·»åŠ ç»“æ„åŒ–å…ƒæ•°æ®å’Œå¯¹é½çš„è§†è§‰å†…å®¹ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡G-Evalå’Œäººå·¥è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æµç¨‹èƒ½å¤Ÿå®ç°å¯¹ä¸åŒç»„ç»‡æˆ–åª’ä½“å¸‚åœºä¹‹é—´äº‹å®æ ¸æŸ¥å®è·µçš„ç»†ç²’åº¦æ¯”è¾ƒï¼Œä¿ƒè¿›å¼€å‘æ›´å…·å¯è§£é‡Šæ€§å’Œè¯æ®åŸºç¡€çš„äº‹å®æ ¸æŸ¥æ¨¡å‹ï¼Œå¹¶ä¸ºå¤šè¯­è¨€ã€å¤šæ¨¡æ€è™šå‡ä¿¡æ¯éªŒè¯çš„æœªæ¥ç ”ç©¶å¥ å®šåŸºç¡€ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºæ„å»ºå…¨é¢çš„å¤šè¯­è¨€ã€å¤šæ¨¡æ€äº‹å®æ ¸æŸ¥æ•°æ®é›†æä¾›äº†ç³»ç»ŸåŒ–æ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–è¯æ®æå–å’Œç†ç”±ç”Ÿæˆå¢å¼ºäº†äº‹å®æ ¸æŸ¥çš„å¯è§£é‡Šæ€§ï¼Œä¸ºå¼€å‘æ›´å¯é çš„äº‹å®æ ¸æŸ¥ç³»ç»Ÿå’Œè·¨è¯­è¨€ã€è·¨åª’ä½“å¸‚åœºçš„æ¯”è¾ƒç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>The rapid proliferation of misinformation across online platforms underscores the urgent need for robust, up-to-date, explainable, and multilingual fact-checking resources. However, existing datasets are limited in scope, often lacking multimodal evidence, structured annotations, and detailed links between claims, evidence, and verdicts. This paper introduces a comprehensive data collection and processing pipeline that constructs multimodal fact-checking datasets in French and German languages by aggregating ClaimReview feeds, scraping full debunking articles, normalizing heterogeneous claim verdicts, and enriching them with structured metadata and aligned visual content. We used state-of-the-art large language models (LLMs) and multimodal LLMs for (i) evidence extraction under predefined evidence categories and (ii) justification generation that links evidence to verdicts. Evaluation with G-Eval and human assessment demonstrates that our pipeline enables fine-grained comparison of fact-checking practices across different organizations or media markets, facilitates the development of more interpretable and evidence-grounded fact-checking models, and lays the groundwork for future research on multilingual, multimodal misinformation verification.</p>
<h3 id="44-vulca-bench-a-multicultural-vision-language-benchmark-for-evaluating-cultural-understanding">[44] <a href="https://arxiv.org/abs/2601.07986">VULCA-Bench: A Multicultural Vision-Language Benchmark for Evaluating Cultural Understanding</a></h3>
<p><em>Haorui Yu, Ramon Ruiz-Dolz, Diji Yang, Hang He, Fengrui Zhang, Qiufeng Yi</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VULCA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹æ–‡åŒ–ç†è§£èƒ½åŠ›çš„å¤šæ–‡åŒ–è‰ºæœ¯è¯„è®ºåŸºå‡†ï¼Œè¶…è¶Šäº†è¡¨é¢è§†è§‰æ„ŸçŸ¥ï¼ŒåŒ…å«7,410ä¸ªå›¾åƒ-è¯„è®ºå¯¹ï¼Œæ¶µç›–å…«ç§æ–‡åŒ–ä¼ ç»Ÿï¼Œå¹¶é‡‡ç”¨äº”å±‚æ–‡åŒ–ç†è§£æ¡†æ¶è¿›è¡Œç³»ç»Ÿè¯„ä¼°ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†ä¸»è¦è¯„ä¼°L1-L2èƒ½åŠ›ï¼ˆç‰©ä½“è¯†åˆ«ã€åœºæ™¯æè¿°å’Œäº‹å®é—®ç­”ï¼‰ï¼Œè€Œå¿½è§†äº†æ›´é«˜å±‚æ¬¡çš„æ–‡åŒ–è§£é‡Šèƒ½åŠ›ï¼Œå¯¼è‡´æ¨¡å‹æ–‡åŒ–ç†è§£è¯„ä¼°ä¸è¶³ï¼Œæ— æ³•å…¨é¢è¡¡é‡è·¨æ–‡åŒ–åœºæ™¯ä¸‹çš„æ·±åº¦è®¤çŸ¥è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†åŒ…å«7,410ä¸ªåŒ¹é…å›¾åƒ-è¯„è®ºå¯¹çš„å¤šæ–‡åŒ–è‰ºæœ¯è¯„è®ºåŸºå‡†ï¼Œæ¶µç›–å…«ç§æ–‡åŒ–ä¼ ç»Ÿå¹¶æ”¯æŒä¸­è‹±åŒè¯­ï¼›é‡‡ç”¨äº”å±‚æ–‡åŒ–ç†è§£æ¡†æ¶ï¼ˆL1-L5ï¼Œä»è§†è§‰æ„ŸçŸ¥åˆ°å“²å­¦ç¾å­¦ï¼‰ï¼Œå…·ä½“åŒ–ä¸º225ä¸ªæ–‡åŒ–ç‰¹å®šç»´åº¦ï¼Œå¹¶ç”±ä¸“å®¶æ’°å†™åŒè¯­è¯„è®ºè¿›è¡Œå®ä¾‹åŒ–ã€‚</p>
<p><strong>Result:</strong> åˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼Œé«˜å±‚æ¬¡æ¨ç†ï¼ˆL3-L5ï¼‰å§‹ç»ˆæ¯”è§†è§‰å’ŒæŠ€æœ¯åˆ†æï¼ˆL1-L2ï¼‰æ›´å…·æŒ‘æˆ˜æ€§ï¼ŒéªŒè¯äº†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–‡åŒ–æ·±åº¦ç†è§£æ–¹é¢çš„å±€é™æ€§ï¼Œæ•°æ®é›†ã€è¯„ä¼°è„šæœ¬å’Œæ ‡æ³¨å·¥å…·å·²é€šè¿‡CC BY 4.0è®¸å¯åœ¨è¡¥å……ææ–™ä¸­å…¬å¼€ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹æ–‡åŒ–ç†è§£èƒ½åŠ›çš„é‡è¦æ€§ï¼Œæå‡ºçš„äº”å±‚æ¡†æ¶ä¸ºç³»ç»Ÿè¯„ä¼°æä¾›äº†ç»“æ„åŒ–æ–¹æ³•ï¼Œå…¬å¼€çš„æ•°æ®é›†å’Œå·¥å…·å°†ä¿ƒè¿›è·¨æ–‡åŒ–AIç ”ç©¶ï¼Œæ¨åŠ¨æ¨¡å‹ä»è¡¨é¢æ„ŸçŸ¥å‘æ·±åº¦æ–‡åŒ–è§£é‡Šå‘å±•ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>We introduce VULCA-Bench, a multicultural art-critique benchmark for evaluating Vision-Language Models' (VLMs) cultural understanding beyond surface-level visual perception. Existing VLM benchmarks predominantly measure L1-L2 capabilities (object recognition, scene description, and factual question answering) while under-evaluate higher-order cultural interpretation. VULCA-Bench contains 7,410 matched image-critique pairs spanning eight cultural traditions, with Chinese-English bilingual coverage. We operationalise cultural understanding using a five-layer framework (L1-L5, from Visual Perception to Philosophical Aesthetics), instantiated as 225 culture-specific dimensions and supported by expert-written bilingual critiques. Our pilot results indicate that higher-layer reasoning (L3-L5) is consistently more challenging than visual and technical analysis (L1-L2). The dataset, evaluation scripts, and annotation tools are available under CC BY 4.0 in the supplementary materials.</p>
<h3 id="45-generation-augmented-generation-a-plug-and-play-framework-for-private-knowledge-injection-in-large-language-models">[45] <a href="https://arxiv.org/abs/2601.08209">Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models</a></h3>
<p><em>Rongji Li, Jian Xu, Xueqing Chen, Yisheng Yang, Jiayi Wang, Xingyu Chen, Chunyu Xie, Dawei Leng, Xu-Yao Zhang</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºç”Ÿæˆå¢å¼ºç”Ÿæˆï¼ˆGAGï¼‰æ–¹æ³•ï¼Œå°†ç§æœ‰ä¸“ä¸šçŸ¥è¯†è§†ä¸ºä¸“å®¶æ¨¡æ€ï¼Œé€šè¿‡ç´§å‡‘çš„è¡¨å¾çº§æ¥å£å¯¹é½åˆ°å†»ç»“çš„åŸºç¡€æ¨¡å‹ï¼Œè§£å†³äº†ç§æœ‰çŸ¥è¯†æ³¨å…¥ä¸­å¾®è°ƒè¿­ä»£æˆæœ¬é«˜å’ŒRAGåœ¨ä¸“ä¸šè¯­æ–™ä¸­è„†å¼±æ€§çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨ç”Ÿç‰©åŒ»å­¦ã€ææ–™å’Œé‡‘èç­‰é«˜é£é™©é¢†åŸŸéƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œéœ€è¦æ³¨å…¥ç§æœ‰ã€é¢†åŸŸç‰¹å®šçš„çŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†å…·æœ‰ä¸“æœ‰æ€§ã€å¿«é€Ÿæ¼”å˜æ€§ä¸”åœ¨å…¬å¼€é¢„è®­ç»ƒä¸­ä»£è¡¨æ€§ä¸è¶³ã€‚ç„¶è€Œï¼Œå½“å‰ä¸¤ç§ä¸»è¦çš„ç§æœ‰çŸ¥è¯†æ³¨å…¥èŒƒå¼å„æœ‰æ˜æ˜¾ç¼ºé™·ï¼šå¾®è°ƒè¿­ä»£æˆæœ¬é«˜æ˜‚ä¸”æŒç»­æ›´æ–°å¯èƒ½å¯¼è‡´ç¾éš¾æ€§é—å¿˜å’Œé€šç”¨èƒ½åŠ›é€€åŒ–ï¼›æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è™½ç„¶ä¿æŒåŸºç¡€æ¨¡å‹å®Œæ•´ï¼Œä½†åœ¨ä¸“ä¸šç§æœ‰è¯­æ–™ä¸­å› åˆ†å—å¯¼è‡´çš„è¯æ®ç¢ç‰‡åŒ–ã€æ£€ç´¢æ¼‚ç§»å’Œé•¿ä¸Šä¸‹æ–‡å‹åŠ›è€Œè¡¨ç°è„†å¼±ã€‚</p>
<p><strong>Method:</strong> å—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å°†å¼‚æ„æ¨¡æ€å¯¹é½åˆ°å…±äº«è¯­ä¹‰ç©ºé—´çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºç”Ÿæˆå¢å¼ºç”Ÿæˆï¼ˆGAGï¼‰æ–¹æ³•ï¼Œå°†ç§æœ‰ä¸“ä¸šçŸ¥è¯†è§†ä¸ºé¢å¤–çš„ä¸“å®¶æ¨¡æ€ï¼Œé€šè¿‡ç´§å‡‘çš„è¡¨å¾çº§æ¥å£å¯¹é½åˆ°å†»ç»“çš„åŸºç¡€æ¨¡å‹ã€‚è¯¥æ–¹æ³•é¿å…äº†æç¤ºæ—¶çš„è¯æ®åºåˆ—åŒ–ï¼ŒåŒæ—¶å®ç°äº†å³æ’å³ç”¨çš„ä¸“ä¸šåŒ–ä»¥åŠå¯é é€‰æ‹©æ€§æ¿€æ´»çš„å¯æ‰©å±•å¤šé¢†åŸŸç»„åˆã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸¤ä¸ªç§æœ‰ç§‘å­¦é—®ç­”åŸºå‡†ï¼ˆå…ç–«å­¦ä½å‰‚å’Œå‚¬åŒ–ææ–™ï¼‰ä»¥åŠæ··åˆé¢†åŸŸè¯„ä¼°ä¸­ï¼ŒGAGåœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šåˆ†åˆ«æ¯”å¼ºå¤§çš„RAGåŸºçº¿æé«˜äº†15.34%å’Œ14.86%çš„ä¸“ä¸šæ€§èƒ½ã€‚åŒæ—¶ï¼Œåœ¨å…­ä¸ªå¼€æ”¾é€šç”¨åŸºå‡†ä¸Šä¿æŒäº†æ€§èƒ½ï¼Œå¹¶å®ç°äº†æ¥è¿‘oracleçš„é€‰æ‹©æ€§æ¿€æ´»ï¼Œæ”¯æŒå¯æ‰©å±•çš„å¤šé¢†åŸŸéƒ¨ç½²ã€‚</p>
<p><strong>Conclusion:</strong> GAGæ–¹æ³•é€šè¿‡å°†ç§æœ‰çŸ¥è¯†ä½œä¸ºä¸“å®¶æ¨¡æ€æ³¨å…¥ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯é çš„ç§æœ‰çŸ¥è¯†é›†æˆæ–¹æ¡ˆï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•æ”¯æŒå³æ’å³ç”¨çš„ä¸“ä¸šåŒ–å’Œå¯æ‰©å±•çš„å¤šé¢†åŸŸç»„åˆï¼Œä¸ºé«˜é£é™©é¢†åŸŸçš„å¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¹³è¡¡äº†ä¸“ä¸šæ€§èƒ½ä¸é€šç”¨èƒ½åŠ›ä¿æŒçš„éœ€æ±‚ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining. However, the two dominant paradigms for private knowledge injection each have pronounced drawbacks: fine-tuning is expensive to iterate, and continual updates risk catastrophic forgetting and general-capability regression; retrieval-augmented generation (RAG) keeps the base model intact but is brittle in specialized private corpora due to chunk-induced evidence fragmentation, retrieval drift, and long-context pressure that yields query-dependent prompt inflation. Inspired by how multimodal LLMs align heterogeneous modalities into a shared semantic space, we propose Generation-Augmented Generation (GAG), which treats private expertise as an additional expert modality and injects it via a compact, representation-level interface aligned to the frozen base model, avoiding prompt-time evidence serialization while enabling plug-and-play specialization and scalable multi-domain composition with reliable selective activation. Across two private scientific QA benchmarks (immunology adjuvant and catalytic materials) and mixed-domain evaluations, GAG improves specialist performance over strong RAG baselines by 15.34% and 14.86% on the two benchmarks, respectively, while maintaining performance on six open general benchmarks and enabling near-oracle selective activation for scalable multi-domain deployment.</p>
<h3 id="46-agriagent-contract-driven-planning-and-capability-aware-tool-orchestration-in-real-world-agriculture">[46] <a href="https://arxiv.org/abs/2601.08308">AgriAgent: Contract-Driven Planning and Capability-Aware Tool Orchestration in Real-World Agriculture</a></h3>
<p><em>Bo Yang, Yu Zhang, Yunkui Chen, Lanfei Feng, Xiao Xu, Nueraili Aierken, Shijian Li</em></p>
<h4 id="tldr_45">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†AgriAgentï¼Œä¸€ä¸ªç”¨äºçœŸå®å†œä¸šåœºæ™¯çš„ä¸¤çº§æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚æ‰§è¡Œç­–ç•¥å¤„ç†ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸Šç›¸æ¯”ç°æœ‰ç»Ÿä¸€æ‰§è¡ŒèŒƒå¼çš„å·¥å…·ä¸­å¿ƒåŒ–æ™ºèƒ½ä½“åŸºçº¿å®ç°äº†æ›´é«˜çš„æ‰§è¡ŒæˆåŠŸç‡å’Œé²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_45">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> çœŸå®å†œä¸šåœºæ™¯ä¸­çš„æ™ºèƒ½ä½“ç³»ç»Ÿéœ€è¦å¤„ç†ä»è½»é‡çº§ä¿¡æ¯ç†è§£åˆ°å¤æ‚å¤šæ­¥æ‰§è¡Œçš„å¤šæ¨¡æ€è¾“å…¥ä»»åŠ¡ï¼Œä½†ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–ç»Ÿä¸€æ‰§è¡ŒèŒƒå¼ï¼Œéš¾ä»¥é€‚åº”å†œä¸šç¯å¢ƒä¸­å¸¸è§çš„ä»»åŠ¡å¤æ‚åº¦å·®å¼‚å¤§å’Œå·¥å…·å¯ç”¨æ€§ä¸å®Œæ•´çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> AgriAgenté‡‡ç”¨åŸºäºä»»åŠ¡å¤æ‚åº¦çš„åˆ†å±‚æ‰§è¡Œç­–ç•¥ï¼šç®€å•ä»»åŠ¡é€šè¿‡ç‰¹å®šæ¨¡æ€æ™ºèƒ½ä½“ç›´æ¥æ¨ç†å¤„ç†ï¼Œå¤æ‚ä»»åŠ¡åˆ™è§¦å‘å¥‘çº¦é©±åŠ¨çš„è§„åˆ’æœºåˆ¶ï¼Œå°†ä»»åŠ¡å½¢å¼åŒ–ä¸ºèƒ½åŠ›éœ€æ±‚ï¼Œæ‰§è¡Œèƒ½åŠ›æ„ŸçŸ¥çš„å·¥å…·ç¼–æ’å’ŒåŠ¨æ€å·¥å…·ç”Ÿæˆï¼Œå®ç°å¯éªŒè¯çš„å¤šæ­¥æ‰§è¡Œå’Œæ•…éšœæ¢å¤ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒAgriAgentåœ¨å¤æ‚ä»»åŠ¡ä¸Šç›¸æ¯”ä¾èµ–ç»Ÿä¸€æ‰§è¡ŒèŒƒå¼çš„ç°æœ‰å·¥å…·ä¸­å¿ƒåŒ–æ™ºèƒ½ä½“åŸºçº¿å®ç°äº†æ›´é«˜çš„æ‰§è¡ŒæˆåŠŸç‡å’Œé²æ£’æ€§ï¼Œæ‰€æœ‰ä»£ç å’Œæ•°æ®å°†åœ¨è®ºæ–‡è¢«æ¥å—åå‘å¸ƒä»¥ä¿ƒè¿›å¯é‡å¤ç ”ç©¶ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åˆ†å±‚æ‰§è¡Œç­–ç•¥åœ¨å¤„ç†å†œä¸šåœºæ™¯ä¸­å¤šæ ·åŒ–ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œå¥‘çº¦é©±åŠ¨çš„è§„åˆ’æœºåˆ¶èƒ½å¤Ÿé€‚åº”ä¸å®Œæ•´çš„å·¥å…·å¯ç”¨æ€§ï¼Œä¸ºçœŸå®ä¸–ç•Œå†œä¸šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†æ›´çµæ´»å’Œé²æ£’çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å†œä¸šäººå·¥æ™ºèƒ½å‘æ›´å®ç”¨çš„æ–¹å‘å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_45">ğŸ“„ Abstract</h4>
<p>Intelligent agent systems in real-world agricultural scenarios must handle diverse tasks under multimodal inputs, ranging from lightweight information understanding to complex multi-step execution. However, most existing approaches rely on a unified execution paradigm, which struggles to accommodate large variations in task complexity and incomplete tool availability commonly observed in agricultural environments. To address this challenge, we propose AgriAgent, a two-level agent framework for real-world agriculture. AgriAgent adopts a hierarchical execution strategy based on task complexity: simple tasks are handled through direct reasoning by modality-specific agents, while complex tasks trigger a contract-driven planning mechanism that formulates tasks as capability requirements and performs capability-aware tool orchestration and dynamic tool generation, enabling multi-step and verifiable execution with failure recovery. Experimental results show that AgriAgent achieves higher execution success rates and robustness on complex tasks compared to existing tool-centric agent baselines that rely on unified execution paradigms. All code, data will be released at after our work be accepted to promote reproducible research.</p>
<h3 id="47-detecting-mental-manipulation-in-speech-via-synthetic-multi-speaker-dialogue">[47] <a href="https://arxiv.org/abs/2601.08342">Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue</a></h3>
<p><em>Run Chen, Wen Liang, Ziwei Gong, Lin Ai, Julia Hirschberg</em></p>
<h4 id="tldr_46">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡é¦–æ¬¡ç ”ç©¶äº†è¯­éŸ³å¯¹è¯ä¸­çš„å¿ƒç†æ“çºµæ£€æµ‹ï¼Œæ„å»ºäº†é¦–ä¸ªå¤šè¯´è¯äººè¯­éŸ³åŸºå‡†SPEECHMENTALMANIPï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨è¯­éŸ³æ¨¡æ€ä¸Šæ£€æµ‹æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„ç°è±¡ï¼Œå¼ºè°ƒäº†å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿéœ€è¦æ¨¡æ€æ„ŸçŸ¥çš„å®‰å…¨å¯¹é½ã€‚</p>
<hr />
<h4 id="detailed-summary_46">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å…ˆå‰å…³äºå¿ƒç†æ“çºµæ£€æµ‹çš„ç ”ç©¶ä»…å…³æ³¨æ–‡æœ¬å¯¹è¯ï¼Œå¿½è§†äº†æ“çºµç­–ç•¥åœ¨è¯­éŸ³ä¸­çš„è¡¨ç°å½¢å¼ï¼Œå­˜åœ¨æ¨¡æ€è¦†ç›–ä¸å…¨çš„ç ”ç©¶ç©ºç™½ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¯­éŸ³å¯¹è¯ä¸­å¿ƒç†æ“çºµæ£€æµ‹è¿™ä¸€æ–°å…´ä»»åŠ¡ï¼Œæ¢ç´¢æ¨¡æ€å¦‚ä½•å½±å“æ£€æµ‹å‡†ç¡®æ€§å’Œæ„ŸçŸ¥ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶æ„å»ºäº†é¦–ä¸ªåˆæˆå¤šè¯´è¯äººè¯­éŸ³åŸºå‡†SPEECHMENTALMANIPï¼Œé€šè¿‡é«˜è´¨é‡ã€è¯­éŸ³ä¸€è‡´çš„æ–‡æœ¬åˆ°è¯­éŸ³æŠ€æœ¯å°†æ–‡æœ¬æ•°æ®é›†å¢å¼ºä¸ºéŸ³é¢‘æ•°æ®ã€‚é‡‡ç”¨å°‘æ ·æœ¬å¤§å‹éŸ³é¢‘-è¯­è¨€æ¨¡å‹å’Œäººå·¥æ ‡æ³¨æ–¹æ³•ï¼Œç³»ç»Ÿè¯„ä¼°äº†æ¨¡æ€å¯¹æ£€æµ‹æ€§èƒ½çš„å½±å“ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨è¯­éŸ³æ•°æ®ä¸Šè¡¨ç°å‡ºé«˜ç‰¹å¼‚æ€§ä½†å¬å›ç‡æ˜¾è‘—ä½äºæ–‡æœ¬æ•°æ®ï¼Œè¡¨æ˜æ¨¡å‹å¯¹è®­ç»ƒä¸­ç¼ºå¤±çš„å£°å­¦æˆ–éŸµå¾‹çº¿ç´¢æ•æ„Ÿã€‚äººç±»æ ‡æ³¨è€…åœ¨éŸ³é¢‘è®¾ç½®ä¸‹è¡¨ç°å‡ºç±»ä¼¼çš„ä¸ç¡®å®šæ€§ï¼Œå‡¸æ˜¾äº†æ“çºµæ€§è¯­éŸ³å›ºæœ‰çš„æ¨¡ç³Šæ€§ç‰¹å¾ã€‚</p>
<p><strong>Conclusion:</strong> è¿™äº›å‘ç°å¼ºè°ƒäº†å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿéœ€è¦æ¨¡æ€æ„ŸçŸ¥çš„è¯„ä¼°å’Œå®‰å…¨å¯¹é½æœºåˆ¶ã€‚ç ”ç©¶æ­ç¤ºäº†è¯­éŸ³æ¨¡æ€ä¸­å¿ƒç†æ“çºµæ£€æµ‹çš„ç‰¹æ®ŠæŒ‘æˆ˜ï¼Œä¸ºæœªæ¥å¼€å‘æ›´é²æ£’çš„å¤šæ¨¡æ€ç¤¾äº¤æ¨ç†ç³»ç»Ÿæä¾›äº†é‡è¦æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_46">ğŸ“„ Abstract</h4>
<p>Mental manipulation, the strategic use of language to covertly influence or exploit others, is a newly emerging task in computational social reasoning. Prior work has focused exclusively on textual conversations, overlooking how manipulative tactics manifest in speech. We present the first study of mental manipulation detection in spoken dialogues, introducing a synthetic multi-speaker benchmark SPEECHMENTALMANIP that augments a text-based dataset with high-quality, voice-consistent Text-to-Speech rendered audio. Using few-shot large audio-language models and human annotation, we evaluate how modality affects detection accuracy and perception. Our results reveal that models exhibit high specificity but markedly lower recall on speech compared to text, suggesting sensitivity to missing acoustic or prosodic cues in training. Human raters show similar uncertainty in the audio setting, underscoring the inherent ambiguity of manipulative speech. Together, these findings highlight the need for modality-aware evaluation and safety alignment in multimodal dialogue systems.</p>
<h3 id="48-graphsearch-agentic-search-augmented-reasoning-for-zero-shot-graph-learning">[48] <a href="https://arxiv.org/abs/2601.08621">GraphSearch: Agentic Search-Augmented Reasoning for Zero-Shot Graph Learning</a></h3>
<p><em>Jiajin Liu, Yuanfu Sun, Dongzhe Fan, Qiaoyu Tan</em></p>
<h4 id="tldr_47">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†GraphSearchæ¡†æ¶ï¼Œé¦–æ¬¡å°†æœç´¢å¢å¼ºæ¨ç†æ‰©å±•åˆ°å›¾å­¦ä¹ é¢†åŸŸï¼Œå®ç°äº†æ— éœ€ä»»åŠ¡ç‰¹å®šå¾®è°ƒçš„é›¶æ ·æœ¬å›¾å­¦ä¹ ã€‚è¯¥æ¡†æ¶é€šè¿‡å›¾æ„ŸçŸ¥æŸ¥è¯¢è§„åˆ’å™¨å’Œæ£€ç´¢å™¨ï¼Œåœ¨å¤šæ ·åŒ–çš„å›¾åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¸ç›‘ç£æ–¹æ³•ç›¸åª²ç¾ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_47">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æœç´¢å¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤„ç†å›¾ç»“æ„æ•°æ®æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè€Œå›¾æ•°æ®åœ¨ç”µå­å•†åŠ¡ã€ç¤¾äº¤ç½‘ç»œå’Œç§‘å­¦å¼•ç”¨ç­‰é¢†åŸŸæ™®éå­˜åœ¨ã€‚å›¾ç»“æ„ç¼–ç äº†ä¸°å¯Œçš„æ‹“æ‰‘ä¿¡å·ï¼Œå¯ä½œä¸ºæ£€ç´¢çš„å®è´µå…ˆéªŒçŸ¥è¯†ï¼Œä½†æœ‰æ•ˆåˆ©ç”¨è¿™ç§ç»“æ„é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç”Ÿæˆå›¾è¡¨è¾¾æ€§æŸ¥è¯¢çš„å›°éš¾ä»¥åŠå¹³è¡¡ç»“æ„å’Œè¯­ä¹‰ç›¸å…³æ€§çš„å¯é æ£€ç´¢é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> GraphSearchæ¡†æ¶åŒ…å«å›¾æ„ŸçŸ¥æŸ¥è¯¢è§„åˆ’å™¨ï¼Œå°†æœç´¢ç©ºé—´ï¼ˆå¦‚1è·³ã€å¤šè·³æˆ–å…¨å±€é‚»å±…ï¼‰ä¸è¯­ä¹‰æŸ¥è¯¢è§£è€¦ï¼Œä»¥åŠå›¾æ„ŸçŸ¥æ£€ç´¢å™¨ï¼ŒåŸºäºæ‹“æ‰‘æ„å»ºå€™é€‰é›†å¹¶ä½¿ç”¨æ··åˆè¯„åˆ†å‡½æ•°è¿›è¡Œæ’åºã€‚æ¡†æ¶å®ä¾‹åŒ–äº†ä¸¤ç§éå†æ¨¡å¼ï¼šGraphSearch-Ré€’å½’æ‰©å±•é‚»åŸŸï¼Œè€ŒGraphSearch-Fçµæ´»æ£€ç´¢å±€éƒ¨å’Œå…¨å±€é‚»åŸŸè€Œä¸å—è·³æ•°çº¦æŸã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGraphSearchåœ¨é›¶æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»å’Œé“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­å–å¾—äº†ä¸ç›‘ç£å›¾å­¦ä¹ æ–¹æ³•ç›¸ç«äº‰ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ï¼Œå¹¶åˆ›ä¸‹äº†æœ€å…ˆè¿›çš„ç»“æœã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªå›¾å­¦ä¹ ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> GraphSearchä½œä¸ºä¸€ä¸ªçµæ´»ä¸”å¯æ³›åŒ–çš„èŒƒå¼ï¼Œä¸ºå›¾ä¸Šçš„æ™ºèƒ½æ¨ç†æä¾›äº†æ–°é€”å¾„ï¼Œè¯æ˜äº†æœç´¢å¢å¼ºæ¨ç†åœ¨å›¾ç»“æ„æ•°æ®ä¸Šçš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºæ— éœ€ä»»åŠ¡ç‰¹å®šè®­ç»ƒçš„å¤§è§„æ¨¡å›¾å­¦ä¹ å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œå¹¶ä¸ºå›¾ä¸è¯­è¨€æ¨¡å‹çš„ç»“åˆæä¾›äº†é‡è¦è§è§£ã€‚</p>
<hr />
<h4 id="abstract_47">ğŸ“„ Abstract</h4>
<p>Recent advances in search-augmented large reasoning models (LRMs) enable the retrieval of external knowledge to reduce hallucinations in multistep reasoning. However, their ability to operate on graph-structured data, prevalent in domains such as e-commerce, social networks, and scientific citations, remains underexplored. Unlike plain text corpora, graphs encode rich topological signals that connect related entities and can serve as valuable priors for retrieval, enabling more targeted search and improved reasoning efficiency. Yet, effectively leveraging such structure poses unique challenges, including the difficulty of generating graph-expressive queries and ensuring reliable retrieval that balances structural and semantic relevance. To address this gap, we introduce GraphSearch, the first framework that extends search-augmented reasoning to graph learning, enabling zero-shot graph learning without task-specific fine-tuning. GraphSearch combines a Graph-aware Query Planner, which disentangles search space (e.g., 1-hop, multi-hop, or global neighbors) from semantic queries, with a Graph-aware Retriever, which constructs candidate sets based on topology and ranks them using a hybrid scoring function. We further instantiate two traversal modes: GraphSearch-R, which recursively expands neighborhoods hop by hop, and GraphSearch-F, which flexibly retrieves across local and global neighborhoods without hop constraints. Extensive experiments across diverse benchmarks show that GraphSearch achieves competitive or even superior performance compared to supervised graph learning methods, setting state-of-the-art results in zero-shot node classification and link prediction. These findings position GraphSearch as a flexible and generalizable paradigm for agentic reasoning over graphs.</p>
<h3 id="49-how-order-sensitive-are-llms-orderprobe-for-deterministic-structural-reconstruction">[49] <a href="https://arxiv.org/abs/2601.08626">How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction</a></h3>
<p><em>Yingjie He, Zhaolu Kang, Kehan Jiang, Qianyuan Zhang, Jiachen Qian, Chunlei Meng, Yujie Feng, Yuan Wang, Jiabao Dou, Aming Wu, Leqi Zheng, Pengxiang Zhao, Jiaxin Liu, Zeyu Zhang, Lei Wang, Guansu Wang, Qishi Zhan, Xiaomin He, Meisheng Zhang, Jianyuan Ni</em></p>
<h4 id="tldr_48">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†OrderProbeåŸºå‡†å’Œè¯Šæ–­æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨ç»“æ„é‡å»ºä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†å³ä½¿å‰æ²¿æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ä¹Ÿéš¾ä»¥æ¢å¤å›ºå®šå››å­—ç¬¦è¡¨è¾¾å¼çš„è§„èŒƒé¡ºåºï¼Œä¸”è¯­ä¹‰èƒ½åŠ›ä¸ç»“æ„è§„åˆ’ä¹‹é—´å­˜åœ¨ç³»ç»Ÿæ€§åˆ†ç¦»ã€‚</p>
<hr />
<h4 id="detailed-summary_48">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§è¯­è¨€æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ä»ä¹±åºè¾“å…¥ä¸­é‡å»ºå†…éƒ¨ç»“æ„çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å¥å­çº§æ¢å¤ä»»åŠ¡ç”±äºå­˜åœ¨å¤šç§æœ‰æ•ˆè¯åºè€Œéš¾ä»¥è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½å¤Ÿæ”¯æŒç²¾ç¡®åŒ¹é…è¯„åˆ†çš„ç¡®å®šæ€§åŸºå‡†æ¥ç³»ç»Ÿè¯„ä¼°æ¨¡å‹çš„ç»“æ„é‡å»ºèƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†OrderProbeåŸºå‡†ï¼Œä½¿ç”¨ä¸­æ–‡ã€æ—¥æ–‡å’ŒéŸ©æ–‡ä¸­çš„å›ºå®šå››å­—ç¬¦è¡¨è¾¾å¼è¿›è¡Œç»“æ„é‡å»ºè¯„ä¼°ï¼Œè¿™äº›è¡¨è¾¾å¼å…·æœ‰å”¯ä¸€çš„è§„èŒƒé¡ºåºï¼Œæ”¯æŒç²¾ç¡®åŒ¹é…è¯„åˆ†ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ä¸ªè¯Šæ–­æ¡†æ¶ï¼Œè¶…è¶Šæ¢å¤å‡†ç¡®ç‡ï¼Œè¯„ä¼°è¯­ä¹‰ä¿çœŸåº¦ã€é€»è¾‘æœ‰æ•ˆæ€§ã€ä¸€è‡´æ€§ã€é²æ£’æ€§æ•æ„Ÿåº¦å’Œä¿¡æ¯å¯†åº¦ç­‰å¤šä¸ªç»´åº¦ã€‚</p>
<p><strong>Result:</strong> åœ¨åäºŒä¸ªå¹¿æ³›ä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»“æ„é‡å»ºä»»åŠ¡å³ä½¿å¯¹å‰æ²¿ç³»ç»Ÿä¹Ÿå…·æœ‰æŒ‘æˆ˜æ€§ï¼šé›¶æ ·æœ¬æ¢å¤å‡†ç¡®ç‡ç»å¸¸ä½äº35%ã€‚ç ”ç©¶è¿˜è§‚å¯Ÿåˆ°è¯­ä¹‰å›å¿†ä¸ç»“æ„è§„åˆ’ä¹‹é—´å­˜åœ¨ä¸€è‡´æ€§çš„åˆ†ç¦»ç°è±¡ï¼Œè¡¨æ˜ç»“æ„é²æ£’æ€§å¹¶éè¯­ä¹‰èƒ½åŠ›çš„è‡ªåŠ¨å‰¯äº§å“ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨ç»“æ„æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œè¡¨æ˜è¯­ä¹‰ç†è§£ä¸ç»“æ„è§„åˆ’æ˜¯ç›¸å¯¹ç‹¬ç«‹çš„è®¤çŸ¥èƒ½åŠ›ã€‚OrderProbeåŸºå‡†ä¸ºç³»ç»Ÿè¯„ä¼°è¯­è¨€æ¨¡å‹çš„ç»“æ„é‡å»ºèƒ½åŠ›æä¾›äº†å¯é å·¥å…·ï¼Œæœªæ¥ç ”ç©¶éœ€è¦ä¸“é—¨é’ˆå¯¹ç»“æ„é²æ£’æ€§è¿›è¡Œæ¨¡å‹æ”¹è¿›ï¼Œè€Œéä»…ä»…ä¾èµ–è¯­ä¹‰èƒ½åŠ›çš„æå‡ã€‚</p>
<hr />
<h4 id="abstract_48">ğŸ“„ Abstract</h4>
<p>Large language models (LLMs) excel at semantic understanding, yet their ability to reconstruct internal structure from scrambled inputs remains underexplored. Sentence-level restoration is ill-posed for automated evaluation because multiple valid word orders often exist. We introduce OrderProbe, a deterministic benchmark for structural reconstruction using fixed four-character expressions in Chinese, Japanese, and Korean, which have a unique canonical order and thus support exact-match scoring. We further propose a diagnostic framework that evaluates models beyond recovery accuracy, including semantic fidelity, logical validity, consistency, robustness sensitivity, and information density. Experiments on twelve widely used LLMs show that structural reconstruction remains difficult even for frontier systems: zero-shot recovery frequently falls below 35%. We also observe a consistent dissociation between semantic recall and structural planning, suggesting that structural robustness is not an automatic byproduct of semantic competence.</p>
<h3 id="50-a-parallel-cross-lingual-benchmark-for-multimodal-idiomaticity-understanding">[50] <a href="https://arxiv.org/abs/2601.08645">A Parallel Cross-Lingual Benchmark for Multimodal Idiomaticity Understanding</a></h3>
<p><em>Dilara TorunoÄŸlu-Selamet, Dogukan Arslan, Rodrigo Wilkens, Wei He, Doruk EryiÄŸit, Thomas Pickard, Adriana S. Pagano, Aline Villavicencio, GÃ¼lÅŸen EryiÄŸit, Ãgnes Abuczki, Aida Cardoso, Alesia Lazarenka, Dina Almassova, Amalia Mendes, Anna Kanellopoulou, Antoni Brosa-RodrÃ­guez, Baiba Saulite, Beata Wojtowicz, Bolette Pedersen, Carlos Manuel Hidalgo-Ternero, Chaya Liebeskind, Danka JokiÄ‡, Diego Alves, Eleni Triantafyllidi, Erik Velldal, Fred Philippy, Giedre Valunaite Oleskeviciene, Ieva Rizgeliene, Inguna Skadina, Irina Lobzhanidze, Isabell Stinessen Haugen, Jauza Akbar Krito, Jelena M. MarkoviÄ‡, Johanna Monti, Josue Alejandro Sauca, Kaja Dobrovoljc, Kingsley O. Ugwuanyi, Laura Rituma, Lilja Ã˜vrelid, Maha Tufail Agro, Manzura Abjalova, Maria Chatzigrigoriou, MarÃ­a del Mar SÃ¡nchez Ramos, Marija Pendevska, Masoumeh Seyyedrezaei, Mehrnoush Shamsfard, Momina Ahsan, Muhammad Ahsan Riaz Khan, Nathalie Carmen Hau Norman, Nilay Erdem AyyÄ±ldÄ±z, Nina Hosseini-Kivanani, NoÃ©mi Ligeti-Nagy, Numaan Naeem, Olha Kanishcheva, Olha Yatsyshyna, Daniil Orel, Petra Giommarelli, Petya Osenova, Radovan Garabik, Regina E. Semou, Rozane Rebechi, Salsabila Zahirah Pranida, Samia Touileb, Sanni Nimb, Sarfraz Ahmad, Sarvinoz Nematkhonova, Shahar Golan, Shaoxiong Ji, Sopuruchi Christian Aboh, Srdjan Sucur, Stella Markantonatou, Sussi Olsen, Vahide Tajalli, Veronika Lipp, Voula Giouli, Yelda YeÅŸildal EraydÄ±n, Zahra Saaberi, Zhuohan Xie</em></p>
<h4 id="tldr_49">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†XMPIEï¼Œä¸€ä¸ªåŒ…å«34ç§è¯­è¨€ã€è¶…è¿‡ä¸€ä¸‡ä¸ªé¡¹ç›®çš„å¹³è¡Œå¤šè¯­è¨€å¤šæ¨¡æ€æ½œåœ¨ä¹ è¯­è¡¨è¾¾æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°NLPç³»ç»Ÿåœ¨è¯­è¨€å’Œæ–‡åŒ–ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œæ”¯æŒè·¨è¯­è¨€å’Œè·¨æ¨¡æ€çš„ä¹ è¯­ç†è§£ç ”ç©¶ã€‚</p>
<hr />
<h4 id="detailed-summary_49">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ½œåœ¨ä¹ è¯­è¡¨è¾¾ä¸ç‰¹å®šè¯­è¨€ç¤¾åŒºçš„æ—¥å¸¸ç»éªŒå¯†åˆ‡ç›¸å…³ï¼Œå¯¹è¯„ä¼°NLPç³»ç»Ÿçš„è¯­è¨€å’Œæ–‡åŒ–ç†è§£èƒ½åŠ›æ„æˆæŒ‘æˆ˜ï¼Œå½“å‰ç¼ºä¹èƒ½å¤Ÿæ”¯æŒè·¨è¯­è¨€å’Œè·¨æ¨¡æ€æ¯”è¾ƒåˆ†æçš„é«˜è´¨é‡æ•°æ®é›†ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†XMPIEå¹³è¡Œå¤šè¯­è¨€å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«34ç§è¯­è¨€å’Œè¶…è¿‡ä¸€ä¸‡ä¸ªé¡¹ç›®ï¼Œæ¯ä¸ªæ½œåœ¨ä¹ è¯­è¡¨è¾¾é…æœ‰äº”å¼ å›¾åƒï¼Œæ¶µç›–ä»ä¹ è¯­åˆ°å­—é¢æ„ä¹‰çš„è¿ç»­è°±ï¼ŒåŒ…æ‹¬è¯­ä¹‰ç›¸å…³å’Œéšæœºå¹²æ‰°é¡¹ï¼Œæ•°æ®ç”±è¯­è¨€ä¸“å®¶æ ¹æ®å¤šè¯­è¨€æŒ‡å¯¼åŸåˆ™åˆ›å»ºã€‚</p>
<p><strong>Result:</strong> XMPIEæ•°æ®é›†æä¾›äº†é«˜è´¨é‡çš„å¤šè¯­è¨€å¤šæ¨¡æ€åŸºå‡†ï¼Œæ”¯æŒè¯­è¨€ç‰¹å®šå®ç°å’Œåå¥½çš„æ¯”è¾ƒåˆ†æï¼Œèƒ½å¤Ÿè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ä¸­çš„ä¹ è¯­ç†è§£æ€§èƒ½ï¼Œä»¥åŠè·¨è¯­è¨€å’Œè·¨æ¨¡æ€çš„ç†è§£è¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ•°æ®é›†ä¸ºè¯„ä¼°å¤šè¯­è¨€å’Œå¤šæ¨¡æ€ä¹ è¯­ç†è§£æä¾›äº†æ ‡å‡†åŒ–åŸºå‡†ï¼Œæœ‰åŠ©äºç ”ç©¶ä¸åŒè¯­è¨€ç¤¾åŒºå…±äº«çš„æ–‡åŒ–æ–¹é¢ï¼Œå¹¶ä¸ºç†è§£ä¹ è¯­çŸ¥è¯†åœ¨è¯­è¨€å’Œæ¨¡æ€é—´çš„è¿ç§»æœºåˆ¶æä¾›äº†å®è¯åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_49">ğŸ“„ Abstract</h4>
<p>Potentially idiomatic expressions (PIEs) construe meanings inherently tied to the everyday experience of a given language community. As such, they constitute an interesting challenge for assessing the linguistic (and to some extent cultural) capabilities of NLP systems. In this paper, we present XMPIE, a parallel multilingual and multimodal dataset of potentially idiomatic expressions. The dataset, containing 34 languages and over ten thousand items, allows comparative analyses of idiomatic patterns among language-specific realisations and preferences in order to gather insights about shared cultural aspects. This parallel dataset allows to evaluate model performance for a given PIE in different languages and whether idiomatic understanding in one language can be transferred to another. Moreover, the dataset supports the study of PIEs across textual and visual modalities, to measure to what extent PIE understanding in one modality transfers or implies in understanding in another modality (text vs. image). The data was created by language experts, with both textual and visual components crafted under multilingual guidelines, and each PIE is accompanied by five images representing a spectrum from idiomatic to literal meanings, including semantically related and random distractors. The result is a high-quality benchmark for evaluating multilingual and multimodal idiomatic language understanding.</p>
<h3 id="51-from-rows-to-reasoning-a-retrieval-augmented-multimodal-framework-for-spreadsheet-understanding">[51] <a href="https://arxiv.org/abs/2601.08741">From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding</a></h3>
<p><em>Anmol Gulati, Sahil Sen, Waqar Sarguroh, Kevin Paul</em></p>
<h4 id="tldr_50">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FRTR-Benchï¼Œé¦–ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€ç”µå­è¡¨æ ¼æ¨ç†åŸºå‡†ï¼Œä»¥åŠFRTRæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»†ç²’åº¦åµŒå…¥åˆ†è§£ã€æ··åˆæ£€ç´¢å’Œè§†è§‰æ•´åˆï¼Œæ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹å¯¹å¤æ‚ä¼ä¸šç”µå­è¡¨æ ¼çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_50">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†åŒ…å«æ•°åƒè¡Œæ•°å€¼ã€å¤šä¸ªå…³è”å·¥ä½œè¡¨ä»¥åŠå›¾è¡¨ã€æ”¶æ®ç­‰åµŒå…¥è§†è§‰å†…å®¹çš„ä¼ä¸šçº§ç”µå­è¡¨æ ¼æ—¶å­˜åœ¨æ¨ç†å›°éš¾ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å•è¡¨å‹ç¼©æˆ–å…¨ä¸Šä¸‹æ–‡ç¼–ç ï¼Œè¿™é™åˆ¶äº†å¯æ‰©å±•æ€§ä¸”æ— æ³•åæ˜ ç”¨æˆ·ä¸å¤æ‚å¤šæ¨¡æ€å·¥ä½œç°¿çš„çœŸå®äº¤äº’æ–¹å¼ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†FRTR-BenchåŸºå‡†ï¼ŒåŒ…å«30ä¸ªä¼ä¸šçº§Excelå·¥ä½œç°¿ï¼Œæ¶µç›–è¿‘å››ç™¾ä¸‡ä¸ªå•å…ƒæ ¼å’Œ50å¤šä¸ªåµŒå…¥å›¾åƒã€‚ä¸ºè§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œå¼€å‘äº†FRTRæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†Excelå·¥ä½œç°¿åˆ†è§£ä¸ºç»†ç²’åº¦çš„è¡Œã€åˆ—å’Œå—åµŒå…¥ï¼Œé‡‡ç”¨åŸºäºäº’é€†æ’åºèåˆçš„æ··åˆè¯æ±‡-ç¨ å¯†æ£€ç´¢ï¼Œå¹¶æ•´åˆå¤šæ¨¡æ€åµŒå…¥ä»¥åŒæ—¶æ¨ç†æ•°å€¼å’Œè§†è§‰ä¿¡æ¯ã€‚</p>
<p><strong>Result:</strong> åœ¨å…­ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šæµ‹è¯•FRTRï¼Œåœ¨FRTR-BenchåŸºå‡†ä¸Šä½¿ç”¨Claude Sonnet 4.5è¾¾åˆ°74%çš„ç­”æ¡ˆå‡†ç¡®ç‡ï¼Œç›¸æ¯”ä¹‹å‰ä»…24%çš„æœ€å…ˆè¿›æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚åœ¨SpreadsheetLLMåŸºå‡†ä¸Šï¼ŒFRTRä½¿ç”¨GPT-5è¾¾åˆ°87%å‡†ç¡®ç‡ï¼ŒåŒæ—¶ç›¸æ¯”ä¸Šä¸‹æ–‡å‹ç¼©æ–¹æ³•å‡å°‘äº†çº¦50%çš„ä»¤ç‰Œä½¿ç”¨é‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç»†ç²’åº¦åµŒå…¥åˆ†è§£å’Œæ··åˆæ£€ç´¢ç­–ç•¥ï¼Œç»“åˆå¤šæ¨¡æ€ä¿¡æ¯æ•´åˆï¼Œå¯ä»¥æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹å¯¹å¤æ‚ä¼ä¸šç”µå­è¡¨æ ¼çš„æ¨ç†èƒ½åŠ›ã€‚FRTRæ¡†æ¶åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å¤§å¹…é™ä½äº†è®¡ç®—å¼€é”€ï¼Œä¸ºå®é™…ä¼ä¸šåº”ç”¨ä¸­çš„ç”µå­è¡¨æ ¼è‡ªåŠ¨åŒ–åˆ†ææä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_50">ğŸ“„ Abstract</h4>
<p>Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="52-forecast-aware-deep-reinforcement-learning-for-efficient-electricity-load-scheduling-in-dairy-farms">[52] <a href="https://arxiv.org/abs/2601.08052">Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms</a></h3>
<p><em>Nawazish Alia, Rachael Shawb, Karl Mason</em></p>
<h4 id="tldr_51">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºå¥¶ç‰›åœºé«˜æ•ˆè´Ÿè·è°ƒåº¦çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆçŸ­æœŸé¢„æµ‹å’Œè‡ªé€‚åº”KLæ•£åº¦æ§åˆ¶ï¼Œåœ¨åŠ¨æ€ç”µä»·å’Œå¯å†ç”Ÿèƒ½æºé—´æ­‡æ€§æ¡ä»¶ä¸‹å®ç°æˆæœ¬æœ€å°åŒ–ã€‚</p>
<hr />
<h4 id="detailed-summary_51">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¥¶ç‰›åœºä½œä¸ºé«˜èƒ½è€—è¡Œä¸šä¸¥é‡ä¾èµ–ç”µç½‘ä¾›ç”µï¼Œå¯å†ç”Ÿèƒ½æºçš„é—´æ­‡æ€§ç»™å®æ—¶ä¾›éœ€å¹³è¡¡å¸¦æ¥æŒ‘æˆ˜ã€‚ç°æœ‰å¼ºåŒ–å­¦ä¹ è°ƒåº¦æ–¹æ³•é€šå¸¸å‡è®¾å®Œå…¨çŸ¥æ™“æœªæ¥ç”µä»·æˆ–å‘ç”µé‡ï¼Œè¿™åœ¨åŠ¨æ€ç¯å¢ƒä¸­ä¸åˆ‡å®é™…ï¼Œä¸”æ ‡å‡†PPOå˜ä½“ä¾èµ–å›ºå®šè£å‰ªæˆ–KLæ•£åº¦é˜ˆå€¼ï¼Œåœ¨å¯å˜ç”µä»·ä¸‹å¸¸å¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸“æ³¨äºç”µæ± å‚¨èƒ½å’Œçƒ­æ°´è´Ÿè·è°ƒåº¦ã€‚å…¶ä¸­Forecast Aware PPOæ•´åˆäº†åŸºäºå°æ—¶å’Œæœˆä»½çš„æ®‹å·®æ ¡å‡†çŸ­æœŸéœ€æ±‚ä¸å¯å†ç”Ÿèƒ½æºé¢„æµ‹ï¼Œè€ŒPID KL PPOå˜ä½“é‡‡ç”¨æ¯”ä¾‹ç§¯åˆ†å¾®åˆ†æ§åˆ¶å™¨è‡ªé€‚åº”è°ƒèŠ‚KLæ•£åº¦ä»¥å®ç°ç¨³å®šçš„ç­–ç•¥æ›´æ–°ã€‚</p>
<p><strong>Result:</strong> åœ¨çœŸå®å¥¶ç‰›åœºæ•°æ®ä¸Šçš„è®­ç»ƒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ¯”æ ‡å‡†PPOé™ä½1%çš„ç”µè´¹æˆæœ¬ï¼Œæ¯”DQNé™ä½4.8%ï¼Œæ¯”SACé™ä½1.5%ã€‚åœ¨ç”µæ± è°ƒåº¦æ–¹é¢ï¼ŒPPOå‡å°‘äº†13.1%çš„ç”µç½‘è¾“å…¥ï¼Œè¯æ˜äº†å…¶åœ¨ç°ä»£å¥¶ç‰›åœºå¯æŒç»­èƒ½æºç®¡ç†ä¸­çš„å¯æ‰©å±•æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ•´åˆé¢„æµ‹å’Œè‡ªé€‚åº”KLæ§åˆ¶æœºåˆ¶çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè§£å†³åŠ¨æ€ç¯å¢ƒä¸‹çš„èƒ½æºè°ƒåº¦é—®é¢˜ï¼Œä¸ºå†œä¸šé«˜èƒ½è€—è¡Œä¸šçš„å¯æŒç»­èƒ½æºç®¡ç†æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒè”åˆå›½å¯æŒç»­å‘å±•ç›®æ ‡7çš„å®ç°ã€‚</p>
<hr />
<h4 id="abstract_51">ğŸ“„ Abstract</h4>
<p>Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.</p>
<h3 id="53-zerodvfs-zero-shot-llm-guided-core-and-frequency-allocation-for-embedded-platforms">[53] <a href="https://arxiv.org/abs/2601.08166">ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms</a></h3>
<p><em>Mohammad Pivezhandi, Mahdi Banisharif, Abusayeed Saifullah, Ali Jannesari</em></p>
<h4 id="tldr_52">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„åˆ†å±‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¤šæ ¸åµŒå…¥å¼ç³»ç»Ÿçš„çƒ­èƒ½å’Œèƒ½è€—æ„ŸçŸ¥è°ƒåº¦ã€‚è¯¥æ¡†æ¶ç»“åˆLLMè¯­ä¹‰ç‰¹å¾æå–å’Œç¯å¢ƒæ¨¡å‹ï¼Œå®ç°äº†é›¶æ ·æœ¬éƒ¨ç½²å’Œå¿«é€Ÿå†³ç­–ï¼Œæ˜¾è‘—æå‡äº†èƒ½æ•ˆå’Œè°ƒåº¦æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_52">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŠ¨æ€ç”µå‹é¢‘ç‡ç¼©æ”¾å’Œä»»åŠ¡åˆ†é…æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šåŸºäºåˆ©ç”¨ç‡çš„å¯å‘å¼æ–¹æ³•å¿½ç•¥äº†åœé¡¿æ—¶é—´ï¼Œè€ŒåŸºäºç¦»çº¿æ€§èƒ½åˆ†æçš„æ–¹æ³•éœ€è¦å¤§é‡ç¦»çº¿åˆ†æç”Ÿæˆè¡¨æ ¼ï¼Œæ— æ³•é€‚åº”è¿è¡Œæ—¶å˜åŒ–ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†åŠ¨æ€åµŒå…¥å¼ç³»ç»Ÿä¸­é«˜æ•ˆçš„çƒ­ç®¡ç†å’Œèƒ½è€—æ€§èƒ½å¹³è¡¡ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„åˆ†å±‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨ä¸¤ä¸ªåä½œæ™ºèƒ½ä½“åˆ†è§£æŒ‡æ•°çº§åŠ¨ä½œç©ºé—´ï¼Œå†³ç­–å»¶è¿Ÿä»…ä¸º358æ¯«ç§’ã€‚æ¡†æ¶ç»“åˆLLMè¯­ä¹‰ç‰¹å¾æå–æŠ€æœ¯ï¼Œä»OpenMPç¨‹åºä¸­æå–13ä¸ªä»£ç çº§ç‰¹å¾è€Œæ— éœ€æ‰§è¡Œï¼Œå¹¶åˆ©ç”¨å›å½’æŠ€æœ¯æ„å»ºå‡†ç¡®çš„ç¯å¢ƒæ¨¡å‹é¢„æµ‹çƒ­åŠ›å­¦å’Œæ€§èƒ½çŠ¶æ€ã€‚é‡‡ç”¨Dyna-Qå¯å‘çš„æ¡†æ¶ï¼Œå°†ç›´æ¥å¼ºåŒ–å­¦ä¹ ä¸åŸºäºæ¨¡å‹çš„è§„åˆ’ç›¸ç»“åˆï¼Œé€šè¿‡ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®å®ç°é›¶æ ·æœ¬éƒ¨ç½²ã€‚</p>
<p><strong>Result:</strong> å®éªŒåœ¨BOTSå’ŒPolybenchCåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œï¼Œè¦†ç›–NVIDIA Jetson TX2ã€Jetson Orin NXã€RubikPiå’ŒIntel Core i7å¹³å°ã€‚ç»“æœæ˜¾ç¤ºç›¸æ¯”Linux ondemandè°ƒåº¦å™¨ï¼Œèƒ½æ•ˆæå‡7.09å€ï¼Œå®Œå·¥æ—¶é—´æ”¹å–„4.0å€ã€‚é¦–æ¬¡å†³ç­–å»¶è¿Ÿä¸º3.5è‡³8.0ç§’ï¼ˆå«ä¸€æ¬¡æ€§LLMç‰¹å¾æå–ï¼‰ï¼Œåç»­å†³ç­–ä»…éœ€358æ¯«ç§’ï¼Œæ¯”åŸºäºè¡¨æ ¼çš„åˆ†ææ–¹æ³•å¿«8300å€ã€‚æ¨¡å‹æ”¶æ•›é€Ÿåº¦æ¯”æ— æ¨¡å‹æ–¹æ³•å¿«20å€ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆLLMè¯­ä¹‰ç‰¹å¾æå–å’Œç¯å¢ƒæ¨¡å‹çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨åŠ¨æ€åµŒå…¥å¼ç³»ç»Ÿä¸­çš„å®ç”¨æ€§ï¼Œå®ç°äº†é›¶æ ·æœ¬éƒ¨ç½²å’Œå¿«é€Ÿè‡ªé€‚åº”è°ƒåº¦ã€‚è¯¥æ–¹æ³•ä¸ºçƒ­èƒ½å’Œèƒ½è€—æ„ŸçŸ¥è°ƒåº¦æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦å®æ—¶é€‚åº”æ–°å·¥ä½œè´Ÿè½½çš„åŠ¨æ€åµŒå…¥å¼ç³»ç»Ÿç¯å¢ƒã€‚</p>
<hr />
<h4 id="abstract_52">ğŸ“„ Abstract</h4>
<p>Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.</p>
<h3 id="54-the-agents-first-day-benchmarking-learning-exploration-and-scheduling-in-the-workplace-scenarios">[54] <a href="https://arxiv.org/abs/2601.08173">The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios</a></h3>
<p><em>Daocheng Fu, Jianbiao Mei, Rong Wu, Xuemeng Yang, Jia Xu, Ding Wang, Pinlong Cai, Yong Liu, Licheng Wen, Botian Shi</em></p>
<h4 id="tldr_53">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†EvoEnvï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­é²æ£’æ€§çš„åŠ¨æ€è¯„ä¼°æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨é™æ€ç¯å¢ƒæ€§èƒ½ä¸Šé™è€Œå¿½è§†å®é™…éƒ¨ç½²ä¸­éšæœºæ€§æŒ‘æˆ˜çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_53">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç ”ç©¶ä¸»è¦é’ˆå¯¹é™æ€ç¯å¢ƒä¸­çš„æ€§èƒ½ä¸Šé™ï¼Œå¿½è§†äº†å®é™…éƒ¨ç½²ä¸­çš„éšæœºæ€§å’ŒåŠ¨æ€æ€§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€ä»»åŠ¡è°ƒåº¦ã€ä¸ç¡®å®šæ€§ä¸‹çš„ä¸»åŠ¨æ¢ç´¢ä»¥åŠä»ç»éªŒä¸­æŒç»­å­¦ä¹ è¿™ä¸‰ä¸ªå…³é”®æ–¹é¢å­˜åœ¨ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†EvoEnvåŠ¨æ€è¯„ä¼°ç¯å¢ƒï¼Œæ¨¡æ‹Ÿ"å—è®­è€…"æ™ºèƒ½ä½“åœ¨æ–°å‹è®¾ç½®ä¸­çš„æŒç»­æ¢ç´¢è¿‡ç¨‹ï¼Œä»ä¸‰ä¸ªç»´åº¦è¯„ä¼°æ™ºèƒ½ä½“ï¼šé¢å‘æµå¼ä»»åŠ¡çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è°ƒåº¦ã€é€šè¿‡ä¸»åŠ¨æ¢ç´¢å‡å°‘å¹»è§‰çš„è°¨æ…ä¿¡æ¯è·å–ï¼Œä»¥åŠä»åŸºäºè§„åˆ™åŠ¨æ€ç”Ÿæˆçš„ä»»åŠ¡ä¸­æç‚¼é€šç”¨ç­–ç•¥çš„æŒç»­æ¼”åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸»åŠ¨æ¢ç´¢å’ŒæŒç»­å­¦ä¹ æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œè¿™æ­ç¤ºäº†ç°æœ‰è¯„ä¼°æ–¹æ³•ä¸å®é™…ç”Ÿäº§åœºæ™¯ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°æ™ºèƒ½ä½“å¯é æ€§çš„æ¡†æ¶ï¼Œå°†è¯„ä¼°é‡ç‚¹ä»é™æ€æµ‹è¯•è½¬å‘ç°å®çš„ç”Ÿäº§å¯¼å‘åœºæ™¯ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­çš„é²æ£’æ€§è¯„ä¼°æä¾›äº†æ–°çš„æ–¹æ³•è®ºåŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_53">ğŸ“„ Abstract</h4>
<p>The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \method{}, a dynamic evaluation environment that simulates a "trainee" agent continuously exploring a novel setting. Unlike traditional benchmarks, \method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv</p>
<h3 id="55-mpci-bench-a-benchmark-for-multimodal-pairwise-contextual-integrity-evaluation-of-language-model-agents">[55] <a href="https://arxiv.org/abs/2601.08235">MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents</a></h3>
<p><em>Shouju Wang, Haopeng Zhang</em></p>
<h4 id="tldr_54">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MPCI-Benchï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°æ™ºèƒ½ä½“éšç§è¡Œä¸ºçš„å¤šæ¨¡æ€é…å¯¹ä¸Šä¸‹æ–‡å®Œæ•´æ€§åŸºå‡†ï¼Œé€šè¿‡ä¸‰å±‚è¯„ä¼°æ¡†æ¶æ­ç¤ºç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨å¹³è¡¡éšç§ä¸æ•ˆç”¨æ–¹é¢çš„ç³»ç»Ÿæ€§ç¼ºé™·ã€‚</p>
<hr />
<h4 id="detailed-summary_54">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ä»è¢«åŠ¨èŠå¤©æœºå™¨äººæ¼”å˜ä¸ºå¤„ç†ä¸ªäººæ•°æ®çš„ä¸»åŠ¨åŠ©æ‰‹ï¼Œè¯„ä¼°å…¶å¯¹ç¤¾ä¼šè§„èŒƒçš„éµå®ˆå˜å¾—æ—¥ç›Šé‡è¦ï¼Œä½†ç°æœ‰ä¸Šä¸‹æ–‡å®Œæ•´æ€§åŸºå‡†ä¸»è¦å…³æ³¨æ–‡æœ¬åœºæ™¯å’Œè´Ÿé¢æ‹’ç»æƒ…å†µï¼Œå¿½è§†äº†å¤šæ¨¡æ€éšç§é£é™©ä»¥åŠéšç§ä¸æ•ˆç”¨çš„åŸºæœ¬æƒè¡¡é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†MPCI-Benchå¤šæ¨¡æ€é…å¯¹ä¸Šä¸‹æ–‡å®Œæ•´æ€§åŸºå‡†ï¼ŒåŒ…å«ä»ç›¸åŒè§†è§‰æºè¡ç”Ÿçš„æ­£è´Ÿé…å¯¹å®ä¾‹ï¼Œå¹¶å®ä¾‹åŒ–ä¸ºä¸‰ä¸ªå±‚çº§ï¼šè§„èŒƒæ€§ç§å­åˆ¤æ–­ã€ä¸Šä¸‹æ–‡ä¸°å¯Œçš„æ•…äº‹æ¨ç†å’Œå¯æ‰§è¡Œçš„æ™ºèƒ½ä½“è¡ŒåŠ¨è½¨è¿¹ï¼Œé€šè¿‡ä¸‰åŸåˆ™è¿­ä»£ç²¾ç‚¼æµç¨‹ç¡®ä¿æ•°æ®è´¨é‡ã€‚</p>
<p><strong>Result:</strong> å¯¹æœ€å…ˆè¿›å¤šæ¨¡æ€æ¨¡å‹çš„è¯„ä¼°æ­ç¤ºäº†ç³»ç»Ÿæ€§å¹³è¡¡éšç§ä¸æ•ˆç”¨çš„å¤±è´¥ï¼Œä»¥åŠæ˜¾è‘—çš„æ¨¡æ€æ³„éœ²å·®è·ï¼Œå…¶ä¸­æ•æ„Ÿè§†è§‰ä¿¡æ¯æ¯”æ–‡æœ¬ä¿¡æ¯æ³„éœ²æ›´é¢‘ç¹ï¼Œè¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨å¤šæ¨¡æ€éšç§ä¿æŠ¤æ–¹é¢å­˜åœ¨ä¸¥é‡ç¼ºé™·ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘èƒ½å¤Ÿå¹³è¡¡éšç§ä¸æ•ˆç”¨çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„é‡è¦æ€§ï¼Œæ­ç¤ºäº†æ¨¡æ€æ³„éœ²å·®è·è¿™ä¸€æ–°é—®é¢˜ï¼ŒMPCI-Benchçš„å¼€æºå°†ä¸ºæ™ºèƒ½ä½“ä¸Šä¸‹æ–‡å®Œæ•´æ€§ç ”ç©¶æä¾›é‡è¦åŸºç¡€ï¼Œæ¨åŠ¨æ›´å…¨é¢çš„éšç§ä¿æŠ¤è¯„ä¼°æ¡†æ¶å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_54">ğŸ“„ Abstract</h4>
<p>As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.</p>
<h3 id="56-creativity-in-ai-as-emergence-from-domain-limited-generative-models">[56] <a href="https://arxiv.org/abs/2601.08388">Creativity in AI as Emergence from Domain-Limited Generative Models</a></h3>
<p><em>Corina Chutaux</em></p>
<h4 id="tldr_55">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆè§†è§’ä¸‹çš„AIåˆ›é€ åŠ›æ¡†æ¶ï¼Œå°†åˆ›é€ åŠ›è§†ä¸ºæœ‰é™é¢†åŸŸç”Ÿæˆæ¨¡å‹åœ¨å—é™ä¿¡æ¯ç¯å¢ƒä¸­æ¶Œç°çš„å±æ€§ï¼Œè€Œéäº‹åè¯„ä¼°æ ‡ç­¾ï¼Œå¹¶å»ºç«‹äº†åŒ…å«å››ä¸ªç›¸äº’ä½œç”¨ç»„ä»¶çš„æ¦‚å¿µåˆ†è§£ã€‚</p>
<hr />
<h4 id="detailed-summary_55">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰AIåˆ›é€ åŠ›ç ”ç©¶ä¸»è¦é‡‡ç”¨è¯„ä¼°æ¡†æ¶æ¥è¡¡é‡ç”Ÿæˆè¾“å‡ºçš„æ–°é¢–æ€§ã€å¤šæ ·æ€§æˆ–å®ç”¨æ€§ï¼Œå°†åˆ›é€ åŠ›è§†ä¸ºå¾…è¯„ä¼°çš„å±æ€§è€Œéå¾…å»ºæ¨¡çš„ç°è±¡ï¼Œè¿™å¿½ç•¥äº†åˆ›é€ åŠ›ä½œä¸ºæ¶Œç°ç°è±¡çš„ç»“æ„å’Œæƒ…å¢ƒæ¡ä»¶ï¼Œéœ€è¦å»ºç«‹æ›´åŸºç¡€çš„æŠ€æœ¯æ¡†æ¶æ¥ç ”ç©¶AIç³»ç»Ÿä¸­çš„åˆ›é€ åŠ›æ¶Œç°æœºåˆ¶ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ç”Ÿæˆè§†è§’ä¸‹çš„åˆ›é€ åŠ›æ¡†æ¶ï¼Œå°†åˆ›é€ åŠ›è§†ä¸ºé¢†åŸŸå—é™ç”Ÿæˆæ¨¡å‹åœ¨æœ‰é™ä¿¡æ¯ç¯å¢ƒä¸­çš„æ¶Œç°å±æ€§ï¼Œå¹¶å¼•å…¥äº†åŒ…å«å››ä¸ªç›¸äº’ä½œç”¨ç»„ä»¶çš„æ¦‚å¿µåˆ†è§£ï¼šåŸºäºæ¨¡å¼çš„ç”Ÿæˆã€è¯±å¯¼ä¸–ç•Œæ¨¡å‹ã€ä¸Šä¸‹æ–‡åŸºç¡€æ€§å’Œä»»æ„æ€§ï¼Œç‰¹åˆ«å…³æ³¨è¿™äº›ç»„ä»¶åœ¨å¤šæ¨¡æ€ç”Ÿæˆç³»ç»Ÿä¸­çš„å…·ä½“è¡¨ç°ã€‚</p>
<p><strong>Result:</strong> è¯¥ç ”ç©¶å»ºç«‹äº†å°†åˆ›é€ åŠ›è§†ä¸ºç”ŸæˆåŠ¨æ€ä¸é¢†åŸŸç‰¹å®šè¡¨ç¤ºä¹‹é—´ç›¸äº’ä½œç”¨æ¶Œç°ç°è±¡çš„æŠ€æœ¯æ¡†æ¶ï¼Œé€šè¿‡æ¦‚å¿µåˆ†è§£æ­ç¤ºäº†åˆ›é€ åŠ›åœ¨å¤šæ¨¡æ€ç”Ÿæˆç³»ç»Ÿä¸­çš„å…·ä½“è¡¨ç°æœºåˆ¶ï¼Œä¸ºç†è§£å¤§è§„æ¨¡ç”Ÿæˆç³»ç»Ÿï¼ˆç‰¹åˆ«æ˜¯å¤šæ¨¡æ€æ¶æ„ï¼‰ä¸­æ—¥ç›Šå¤æ‚çš„æ¨¡å¼é‡ç»„è¡Œä¸ºæä¾›äº†ç†è®ºåŸºç¡€ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹å·¥ä½œä¸ºå°†åˆ›é€ åŠ›ä½œä¸ºAIç³»ç»Ÿä¸­çš„æ¶Œç°ç°è±¡è€Œéäº‹åè¯„ä¼°æ ‡ç­¾è¿›è¡Œç ”ç©¶æä¾›äº†æŠ€æœ¯æ¡†æ¶ï¼Œé€šè¿‡å°†åˆ›é€ åŠ›åŸºç¡€å»ºç«‹åœ¨ç”ŸæˆåŠ¨æ€ä¸é¢†åŸŸç‰¹å®šè¡¨ç¤ºçš„ç›¸äº’ä½œç”¨ä¸Šï¼Œä¸ºç†è§£æœºå™¨åˆ›é€ åŠ›çš„æœ¬è´¨å’Œé™åˆ¶å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶AIç³»ç»Ÿä¸­çš„åˆ›é€ æ€§è¡Œä¸ºæä¾›äº†æ¦‚å¿µåŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_55">ğŸ“„ Abstract</h4>
<p>Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.</p>
<h3 id="57-an-under-explored-application-for-explainable-multimodal-misogyny-detection-in-code-mixed-hindi-english">[57] <a href="https://arxiv.org/abs/2601.08457">An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English</a></h3>
<p><em>Sargam Yadav, Abhishek Kaushik, Kevin Mc Daid</em></p>
<h4 id="tldr_56">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¯è§£é‡Šçš„Webåº”ç”¨ç¨‹åºï¼Œç”¨äºæ£€æµ‹å°åœ°è¯­-è‹±è¯­æ··åˆè¯­è¨€æ–‡æœ¬å’Œè¡¨æƒ…åŒ…ä¸­çš„åŒå¥³å†…å®¹ï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†å…ˆè¿›çš„Transformeræ¨¡å‹å’ŒSHAPã€LIMEç­‰å¯è§£é‡Šæ€§æŠ€æœ¯ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œå†…å®¹å®¡æ ¸å‘˜æä¾›é€æ˜åŒ–çš„ä»‡æ¨è¨€è®ºæ£€æµ‹å·¥å…·ã€‚</p>
<hr />
<h4 id="detailed-summary_56">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ•°å­—å¹³å°ç”¨æˆ·è§„æ¨¡ä¸æ–­æ‰©å¤§ï¼Œä½†åŒæ—¶ä¹ŸåŠ©é•¿äº†ä»‡æ¨è¨€è®ºå’ŒåŒå¥³å†…å®¹çš„ä¼ æ’­ï¼Œç°æœ‰äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€å’Œæ··åˆè¯­è¨€ç¯å¢ƒä¸‹çš„åº”ç”¨ä¸è¶³ï¼Œä¸”ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œè¿™åœ¨ä»‡æ¨è¨€è®ºæ£€æµ‹ç­‰æ•æ„Ÿé¢†åŸŸä¸­å°¤ä¸ºå…³é”®ã€‚</p>
<p><strong>Method:</strong> ç³»ç»Ÿé‡‡ç”¨åŸºäºTransformerçš„å¤šè¯­è¨€å¤šæ¨¡æ€æ¶æ„ï¼Œæ–‡æœ¬æ£€æµ‹ä½¿ç”¨XLM-RoBERTaå’ŒmBERTæ¨¡å‹å¤„ç†çº¦4,193æ¡è¯„è®ºï¼Œå¤šæ¨¡æ€è¡¨æƒ…åŒ…æ£€æµ‹ç»“åˆmBERTä¸EfficientNetã€ResNETæ¨¡å‹å¤„ç†çº¦4,218ä¸ªè¡¨æƒ…åŒ…ï¼Œå¹¶é›†æˆSHAPå’ŒLIMEæŠ€æœ¯æä¾›ç‰¹å¾é‡è¦æ€§è§£é‡Šã€‚</p>
<p><strong>Result:</strong> ç³»ç»Ÿé€šè¿‡äººç±»è¯„ä¼°è€…ä½¿ç”¨èŠå¤©æœºå™¨äººå¯ç”¨æ€§é—®å·å’Œç”¨æˆ·ä½“éªŒé—®å·è¿›è¡Œè¯„ä¼°ï¼Œç¡®å®šäº†æ•´ä½“å¯ç”¨æ€§ï¼Œä½†å…·ä½“æ€§èƒ½æŒ‡æ ‡æœªåœ¨æ‘˜è¦ä¸­è¯¦ç»†è¯´æ˜ï¼Œè¯„ä¼°é‡ç‚¹åœ¨äºç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿå®ç”¨æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºæ··åˆè¯­è¨€ç¯å¢ƒä¸‹çš„åŒå¥³å†…å®¹æ£€æµ‹æä¾›äº†é€æ˜åŒ–çš„å¤šæ¨¡æ€è§£å†³æ–¹æ¡ˆï¼Œä¿ƒè¿›äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½åœ¨æ•æ„Ÿé¢†åŸŸçš„åº”ç”¨ï¼Œæœ‰åŠ©äºæ‰“å‡»åŸºäºæ€§åˆ«çš„æ•°å­—æš´åŠ›ï¼Œç¡®ä¿å®‰å…¨çš„æ•°å­—ç©ºé—´ï¼Œå¹¶ä¸ºåç»­ç ”ç©¶æä¾›äº†å®ç”¨å·¥å…·ã€‚</p>
<hr />
<h4 id="abstract_56">ğŸ“„ Abstract</h4>
<p>Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.</p>
<h3 id="58-what-if-tsf-a-benchmark-for-reframing-forecasting-as-scenario-guided-multimodal-forecasting">[58] <a href="https://arxiv.org/abs/2601.08509">What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting</a></h3>
<p><em>Jinkwan Jang, Hyunbin Jin, Hyungjin Park, Kyubyung Chae, Taesup Kim</em></p>
<h4 id="tldr_57">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†What If TSFï¼ˆWITï¼‰å¤šæ¨¡æ€æ—¶é—´åºåˆ—é¢„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹æ˜¯å¦èƒ½å¤ŸåŸºäºä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆç‰¹åˆ«æ˜¯æœªæ¥åœºæ™¯ï¼‰è¿›è¡Œæ¡ä»¶é¢„æµ‹ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨åœºæ™¯å¼•å¯¼é¢„æµ‹è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚</p>
<hr />
<h4 id="detailed-summary_57">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•å¤§å¤šä¸ºå•æ¨¡æ€ä¸”ä¾èµ–å†å²æ¨¡å¼å¤–æ¨ï¼Œè€Œå½“å‰å¤šæ¨¡æ€é¢„æµ‹åŸºå‡†ä¸»è¦æä¾›å›é¡¾æ€§æˆ–æœªå¯¹é½çš„åŸå§‹ä¸Šä¸‹æ–‡ï¼Œæ— æ³•æ˜ç¡®è¯„ä¼°æ¨¡å‹æ˜¯å¦çœŸæ­£åˆ©ç”¨æ–‡æœ¬è¾“å…¥è¿›è¡Œé¢„æµ‹ã€‚å®é™…åº”ç”¨ä¸­ï¼Œäººç±»ä¸“å®¶ä¼šç»“åˆå†å²è¯æ®å’Œå‡è®¾åœºæ™¯ï¼Œåœ¨ä¸åŒåœºæ™¯ä¸‹åŸºäºç›¸åŒè§‚æµ‹äº§ç”Ÿä¸åŒçš„é¢„æµ‹ç»“æœã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†What If TSFï¼ˆWITï¼‰å¤šæ¨¡æ€é¢„æµ‹åŸºå‡†ï¼Œè¯¥åŸºå‡†é€šè¿‡æä¾›ä¸“å®¶ç²¾å¿ƒè®¾è®¡çš„åˆç†æˆ–åäº‹å®åœºæ™¯ï¼Œä¸ºåœºæ™¯å¼•å¯¼çš„å¤šæ¨¡æ€é¢„æµ‹æä¾›ä¸¥æ ¼çš„æµ‹è¯•å¹³å°ã€‚åŸºå‡†è®¾è®¡æ—¨åœ¨è¯„ä¼°æ¨¡å‹èƒ½å¦åŸºäºä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆç‰¹åˆ«æ˜¯æœªæ¥åœºæ™¯ï¼‰å¯¹å…¶é¢„æµ‹è¿›è¡Œæ¡ä»¶åŒ–å¤„ç†ã€‚</p>
<p><strong>Result:</strong> WITåŸºå‡†æä¾›äº†ä¸“å®¶ç²¾å¿ƒè®¾è®¡çš„åˆç†æˆ–åäº‹å®åœºæ™¯ï¼Œä¸ºåœºæ™¯å¼•å¯¼çš„å¤šæ¨¡æ€é¢„æµ‹å»ºç«‹äº†ä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ã€‚è¯¥åŸºå‡†å·²å…¬å¼€å¯ç”¨ï¼Œä¸ºç ”ç©¶ç¤¾åŒºæä¾›äº†è¯„ä¼°æ¨¡å‹æ˜¯å¦çœŸæ­£åˆ©ç”¨æ–‡æœ¬ä¸Šä¸‹æ–‡è¿›è¡Œé¢„æµ‹çš„æ ‡å‡†æµ‹è¯•å¹³å°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†åœºæ™¯å¼•å¯¼é¢„æµ‹çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†è¯„ä¼°å¤šæ¨¡æ€æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹èƒ½åŠ›çš„æ ‡å‡†åŒ–åŸºå‡†ã€‚WITåŸºå‡†å¡«è¡¥äº†ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ï¼Œä¸ºæœªæ¥ç ”ç©¶å¦‚ä½•æœ‰æ•ˆæ•´åˆæ–‡æœ¬åœºæ™¯ä¿¡æ¯è¿›è¡Œé¢„æµ‹æä¾›äº†åŸºç¡€æ¡†æ¶å’Œæ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_57">ğŸ“„ Abstract</h4>
<p>Time series forecasting is critical to real-world decision making, yet most existing approaches remain unimodal and rely on extrapolating historical patterns. While recent progress in large language models (LLMs) highlights the potential for multimodal forecasting, existing benchmarks largely provide retrospective or misaligned raw context, making it unclear whether such models meaningfully leverage textual inputs. In practice, human experts incorporate what-if scenarios with historical evidence, often producing distinct forecasts from the same observations under different scenarios. Inspired by this, we introduce What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting. The benchmark is available at https://github.com/jinkwan1115/WhatIfTSF.</p>
<h3 id="59-sketch-based-facade-renovation-with-generative-ai-a-streamlined-framework-for-bypassing-as-built-modelling-in-industrial-adaptive-reuse">[59] <a href="https://arxiv.org/abs/2601.08531">Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse</a></h3>
<p><em>Warissara Booranamaitree, Xusheng Du, Yushu Cai, Zhengyang Wang, Ye Zhang, Haoran Xie</em></p>
<h4 id="tldr_58">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆç”Ÿæˆå¼äººå·¥æ™ºèƒ½å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥å¤„ç†ç²—ç•¥ç»“æ„è‰å›¾å’Œæ–‡æœ¬æè¿°æ¥ç”Ÿæˆä¸€è‡´çš„ç«‹é¢æ”¹é€ æ–¹æ¡ˆï¼Œæœ‰æ•ˆç»•è¿‡äº†ä¼ ç»Ÿæ–¹æ³•ä¸­éœ€è¦è¯¦ç»†ç«£å·¥å»ºæ¨¡çš„ç¹çæµç¨‹ã€‚</p>
<hr />
<h4 id="detailed-summary_58">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç«‹é¢æ”¹é€ ä½œä¸ºæ¯”å®Œå…¨æ‹†é™¤æ›´å¯æŒç»­çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå½“å‰å·¥ä½œæµç¨‹é€šå¸¸éœ€è¦åœ¨è®¾è®¡å‰è¿›è¡Œè¯¦ç»†çš„ç«£å·¥å»ºæ¨¡ï¼Œè¿™ä¸€è¿‡ç¨‹è€—æ—¶è€—åŠ›ä¸”ç»å¸¸æ¶‰åŠé‡å¤ä¿®è®¢ï¼Œé˜»ç¢äº†å»ºç­‘å¸ˆå¿«é€Ÿæ¢ç´¢è®¾è®¡æ–¹æ¡ˆå’Œè¿­ä»£æ—©æœŸæ¦‚å¿µçš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰é˜¶æ®µæ–¹æ³•ï¼šé¦–å…ˆä½¿ç”¨å¾®è°ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹æ ¹æ®è¾“å…¥è‰å›¾é¢„æµ‹éœ€è¦ä¿®æ”¹åŒºåŸŸçš„è¾¹ç•Œæ¡†å’Œåº”æ·»åŠ çš„ç»„ä»¶ï¼›æ¥ç€é€šè¿‡ç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–°å…ƒç´ çš„è¯¦ç»†è‰å›¾ï¼Œå¹¶é€šè¿‡ç”Ÿæˆå¼ä¿®å¤ç®¡é“å°†å…¶ä¸åŸå§‹è½®å»“åˆå¹¶ï¼›æœ€ååˆ©ç”¨ControlNetå°†ç»“æœç»†åŒ–ä¸ºé€¼çœŸçš„å›¾åƒã€‚</p>
<p><strong>Result:</strong> åœ¨æ•°æ®é›†å’ŒçœŸå®å·¥ä¸šå»ºç­‘ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆæ—¢ä¿ç•™åŸå§‹ç»“æ„åˆæå‡ç«‹é¢ç»†èŠ‚è´¨é‡çš„æ”¹é€ æ–¹æ¡ˆï¼Œæœ‰æ•ˆéªŒè¯äº†æ–¹æ³•åœ¨ç»•è¿‡è¯¦ç»†ç«£å·¥å»ºæ¨¡éœ€æ±‚æ–¹é¢çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•ä¸ºå»ºç­‘å¸ˆæä¾›äº†å¿«é€Ÿæ¢ç´¢è®¾è®¡æ›¿ä»£æ–¹æ¡ˆã€è¿­ä»£æ—©æœŸæ¦‚å¿µå¹¶ä»¥æ›´æ¸…æ™°æ–¹å¼ä¼ è¾¾æ”¹é€ æ„å›¾çš„å·¥å…·ï¼Œä»£è¡¨äº†ç”Ÿæˆå¼AIåœ¨å»ºç­‘æ”¹é€ è®¾è®¡æµç¨‹ä¸­çš„åˆ›æ–°åº”ç”¨ï¼Œå…·æœ‰æ˜¾è‘—çš„å®è·µä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_58">ğŸ“„ Abstract</h4>
<p>Facade renovation offers a more sustainable alternative to full demolition, yet producing design proposals that preserve existing structures while expressing new intent remains challenging. Current workflows typically require detailed as-built modelling before design, which is time-consuming, labour-intensive, and often involves repeated revisions. To solve this issue, we propose a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) that directly processes rough structural sketch and textual descriptions to produce consistent renovation proposals. First, the input sketch is used by a fine-tuned VLM model to predict bounding boxes specifying where modifications are needed and which components should be added. Next, a stable diffusion model generates detailed sketches of new elements, which are merged with the original outline through a generative inpainting pipeline. Finally, ControlNet is employed to refine the result into a photorealistic image. Experiments on datasets and real industrial buildings indicate that the proposed framework can generate renovation proposals that preserve the original structure while improving facade detail quality. This approach effectively bypasses the need for detailed as-built modelling, enabling architects to rapidly explore design alternatives, iterate on early-stage concepts, and communicate renovation intentions with greater clarity.</p>
<h3 id="60-vidore-v3-a-comprehensive-evaluation-of-retrieval-augmented-generation-in-complex-real-world-scenarios">[60] <a href="https://arxiv.org/abs/2601.08620">ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios</a></h3>
<p><em>AntÃ³nio Loison, Quentin MacÃ©, Antoine Edy, Victor Xing, Tom Balough, Gabriel Moreira, Bo Liu, Manuel Faysse, CÃ©line Hudelot, Gautier Viaud</em></p>
<h4 id="tldr_59">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ViDoRe v3ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”ŸæˆåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹å¤„ç†è§†è§‰ä¸°å¯Œæ–‡æ¡£ã€å¤šç±»å‹æŸ¥è¯¢å’Œå¤šè¯­è¨€åœºæ™¯çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†å½“å‰RAGç³»ç»Ÿåœ¨è§†è§‰ç†è§£å’Œç»†ç²’åº¦å®šä½æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_59">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ£€ç´¢å¢å¼ºç”ŸæˆåŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æ–‡æœ¬æ•°æ®ã€å•æ–‡æ¡£ç†è§£æˆ–å­¤ç«‹è¯„ä¼°æ£€ç´¢ä¸ç”Ÿæˆç»„ä»¶ï¼Œæ— æ³•æ•æ‰çœŸå®åº”ç”¨ä¸­å¤„ç†è§†è§‰å…ƒç´ ï¼ˆè¡¨æ ¼ã€å›¾è¡¨ã€å›¾åƒï¼‰ã€è·¨æ–‡æ¡£ä¿¡æ¯åˆæˆå’Œå‡†ç¡®æ¥æºå®šä½çš„å¤æ‚æ€§ï¼Œè¿™é™åˆ¶äº†å¤šæ¨¡æ€RAGç³»ç»Ÿçš„æœ‰æ•ˆè¯„ä¼°ä¸å‘å±•ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ViDoRe v3åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«10ä¸ªä¸“ä¸šé¢†åŸŸæ•°æ®é›†ï¼Œçº¦26,000ä¸ªæ–‡æ¡£é¡µé¢å’Œ3,099ä¸ªäººå·¥éªŒè¯æŸ¥è¯¢ï¼Œæ”¯æŒ6ç§è¯­è¨€ï¼Œé€šè¿‡12,000å°æ—¶äººå·¥æ ‡æ³¨æä¾›æ£€ç´¢ç›¸å…³æ€§ã€è¾¹ç•Œæ¡†å®šä½å’ŒéªŒè¯å‚è€ƒç­”æ¡ˆçš„é«˜è´¨é‡æ ‡æ³¨ï¼Œç³»ç»Ÿè¯„ä¼°äº†æœ€å…ˆè¿›çš„RAGæµæ°´çº¿åœ¨ä¸åŒé…ç½®ä¸‹çš„è¡¨ç°ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè§†è§‰æ£€ç´¢å™¨ä¼˜äºæ–‡æœ¬æ£€ç´¢å™¨ï¼Œæ™šæœŸäº¤äº’æ¨¡å‹å’Œæ–‡æœ¬é‡æ’åºæ˜¾è‘—æå‡æ€§èƒ½ï¼Œæ··åˆæˆ–çº¯è§†è§‰ä¸Šä¸‹æ–‡èƒ½æ”¹å–„ç­”æ¡ˆç”Ÿæˆè´¨é‡ï¼Œä½†å½“å‰æ¨¡å‹åœ¨å¤„ç†éæ–‡æœ¬å…ƒç´ ã€å¼€æ”¾å¼æŸ¥è¯¢å’Œç»†ç²’åº¦è§†è§‰å®šä½æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾å›°éš¾ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€RAGç³»ç»Ÿåœ¨è§†è§‰ç†è§£å’Œè·¨æ–‡æ¡£æ¨ç†æ–¹é¢çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„è¯„ä¼°åŸºå‡†ï¼Œé€šè¿‡å•†ä¸šå‹å¥½è®¸å¯å‘å¸ƒçš„æ•°æ®é›†å°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰ä¸°å¯Œæ–‡æ¡£å¤„ç†å’Œç»†ç²’åº¦ä¿¡æ¯å®šä½æ–¹é¢çš„æŠ€æœ¯è¿›æ­¥ã€‚</p>
<hr />
<h4 id="abstract_59">ğŸ“„ Abstract</h4>
<p>Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.</p>
<h3 id="61-resisting-manipulative-bots-in-memecoin-copy-trading-a-multi-agent-approach-with-chain-of-thought-reasoning">[61] <a href="https://arxiv.org/abs/2601.08641">Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning</a></h3>
<p><em>Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu</em></p>
<h4 id="tldr_60">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç”¨äºæ¨¡å› å¸è·Ÿå•äº¤æ˜“ï¼Œé€šè¿‡åˆ†è§£å¤æ‚ä»»åŠ¡å¹¶åè°ƒä¸“ä¸šæ™ºèƒ½ä½“ï¼Œåœ¨è¯†åˆ«é«˜è´¨é‡æ¨¡å› å¸é¡¹ç›®å’Œå…³é”®æ„è§é¢†è¢–é’±åŒ…æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹å’Œå•ä¸€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</p>
<hr />
<h4 id="detailed-summary_60">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ¨¡å› å¸è·Ÿå•äº¤æ˜“é¢ä¸´æ“çºµæ€§æœºå™¨äººæ³›æ»¥ã€è¢«è·Ÿéšé’±åŒ…æœªæ¥è¡¨ç°ä¸ç¡®å®šä»¥åŠäº¤æ˜“æ‰§è¡Œå»¶è¿Ÿç­‰æŒ‘æˆ˜ï¼Œè€Œå•ä¸€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†èµ„äº§åˆ†é…ç­‰å¤æ‚å¤šé¢ä»»åŠ¡æ—¶èƒ½åŠ›æœ‰é™ï¼Œä¸”åœ¨åŠ å¯†è´§å¸é¢†åŸŸç¼ºä¹è¶³å¤Ÿçš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚</p>
<p><strong>Method:</strong> å—èµ„äº§ç®¡ç†å›¢é˜Ÿç»“æ„å¯å‘ï¼Œæå‡ºå¯è§£é‡Šçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡å¹¶ç”±ä¸“ä¸šæ™ºèƒ½ä½“åä½œè§£å†³ï¼›é‡‡ç”¨å°‘æ ·æœ¬æ€ç»´é“¾æç¤ºæŠ€æœ¯ï¼Œä½¿æ¯ä¸ªæ™ºèƒ½ä½“è·å–ä¸“ä¸šæ¨¡å› å¸äº¤æ˜“çŸ¥è¯†ã€è§£é‡Šå¤šæ¨¡æ€æ•°æ®å¹¶ç”Ÿæˆå¯è§£é‡Šçš„å†³ç­–ã€‚</p>
<p><strong>Result:</strong> åœ¨åŒ…å«1000ä¸ªæ¨¡å› å¸é¡¹ç›®äº¤æ˜“æ•°æ®çš„æ•°æ®é›†ä¸Šï¼Œæ‰€æå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨è¯†åˆ«é«˜è´¨é‡æ¨¡å› å¸é¡¹ç›®å’Œå…³é”®æ„è§é¢†è¢–é’±åŒ…æ–¹é¢åˆ†åˆ«è¾¾åˆ°73%å’Œ70%çš„ç²¾ç¡®ç‡ï¼Œæ‰€é€‰å…³é”®æ„è§é¢†è¢–åœ¨è¿™äº›é¡¹ç›®ä¸­ç´¯è®¡äº§ç”Ÿ50ä¸‡ç¾å…ƒçš„æ€»åˆ©æ¶¦ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½æœ‰æ•ˆè§£å†³å•ä¸€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é‡‘èä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œé€šè¿‡ä»»åŠ¡åˆ†è§£å’Œä¸“ä¸šæ™ºèƒ½ä½“åä½œæ˜¾è‘—æå‡æ¨¡å› å¸è·Ÿå•äº¤æ˜“æ€§èƒ½ï¼Œä¸ºå¯è§£é‡Šçš„é‡‘èäººå·¥æ™ºèƒ½ç³»ç»Ÿè®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_60">ğŸ“„ Abstract</h4>
<p>The launch of \$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.
  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \$500,000 across these projects.</p>
<h3 id="62-memeweaver-inter-meme-graph-reasoning-for-sexism-and-misogyny-detection">[62] <a href="https://arxiv.org/abs/2601.08684">MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection</a></h3>
<p><em>Paolo Italiani, David Gimeno-Gomez, Luca Ragazzi, Gianluca Moro, Paolo Rosso</em></p>
<h4 id="tldr_61">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MemeWeaverï¼Œä¸€ç§ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œé€šè¿‡æ–°é¢–çš„æ¨¡å› é—´å›¾æ¨ç†æœºåˆ¶æ£€æµ‹æ€§åˆ«æ­§è§†å’ŒåŒå¥³å†…å®¹ï¼Œåœ¨MAMIå’ŒEXISTåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•å¹¶å®ç°æ›´å¿«çš„è®­ç»ƒæ”¶æ•›ã€‚</p>
<hr />
<h4 id="detailed-summary_61">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¥³æ€§é­å—åœ¨çº¿éªšæ‰°çš„å¯èƒ½æ€§æ˜¯ç”·æ€§çš„ä¸¤å€ï¼Œä½†ç°æœ‰å¤šæ¨¡æ€å†…å®¹å®¡æ ¸æ–¹æ³•å¤§å¤šå¿½è§†äº†è¿™ç§ç°è±¡èƒŒåçš„ç¤¾ä¼šåŠ¨æ€ï¼Œå³æ–½å®³è€…åœ¨å¿—åŒé“åˆçš„ç¤¾åŒºä¸­å¼ºåŒ–åè§å’Œç¾¤ä½“è®¤åŒã€‚åŸºäºå›¾çš„æ–¹æ³•è™½ç„¶æœ‰æœ›æ•æ‰æ­¤ç±»äº’åŠ¨ï¼Œä½†ç°æœ‰è§£å†³æ–¹æ¡ˆä»å—é™äºå¯å‘å¼å›¾æ„å»ºã€æµ…å±‚æ¨¡æ€èåˆå’Œå®ä¾‹çº§æ¨ç†ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†MemeWeaverï¼Œä¸€ç§ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œé‡‡ç”¨æ–°é¢–çš„æ¨¡å› é—´å›¾æ¨ç†æœºåˆ¶æ¥æ£€æµ‹æ€§åˆ«æ­§è§†å’ŒåŒå¥³å†…å®¹ã€‚è¯¥æ–¹æ³•ç³»ç»Ÿè¯„ä¼°äº†å¤šç§è§†è§‰-æ–‡æœ¬èåˆç­–ç•¥ï¼Œå¹¶é€šè¿‡å›¾ç»“æ„å­¦ä¹ æ¥æ•æ‰æ¨¡å› ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„å¯å‘å¼å›¾æ„å»ºæ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> MemeWeaveråœ¨MAMIå’ŒEXISTåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶å®ç°äº†æ›´å¿«çš„è®­ç»ƒæ”¶æ•›é€Ÿåº¦ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œå­¦ä¹ åˆ°çš„å›¾ç»“æ„èƒ½å¤Ÿæ•æ‰è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æ¨¡å¼ï¼Œä¸ºåœ¨çº¿ä»‡æ¨çš„å…³ç³»æ€§è´¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å›¾æ¨ç†æœºåˆ¶åœ¨æ•æ‰åœ¨çº¿æ€§åˆ«æ­§è§†çš„ç¤¾ä¼šåŠ¨æ€æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šæ¨¡æ€å†…å®¹å®¡æ ¸æä¾›äº†æ–°çš„æ–¹å‘ã€‚å­¦ä¹ åˆ°çš„å›¾ç»“æ„ä¸ä»…æå‡äº†æ£€æµ‹æ€§èƒ½ï¼Œè¿˜æ­ç¤ºäº†æ¨¡å› ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œæœ‰åŠ©äºç†è§£åœ¨çº¿ä»‡æ¨çš„ä¼ æ’­æ¨¡å¼ã€‚</p>
<hr />
<h4 id="abstract_61">ğŸ“„ Abstract</h4>
<p>Women are twice as likely as men to face online harassment due to their gender. Despite recent advances in multimodal content moderation, most approaches still overlook the social dynamics behind this phenomenon, where perpetrators reinforce prejudices and group identity within like-minded communities. Graph-based methods offer a promising way to capture such interactions, yet existing solutions remain limited by heuristic graph construction, shallow modality fusion, and instance-level reasoning. In this work, we present MemeWeaver, an end-to-end trainable multimodal framework for detecting sexism and misogyny through a novel inter-meme graph reasoning mechanism. We systematically evaluate multiple visual--textual fusion strategies and show that our approach consistently outperforms state-of-the-art baselines on the MAMI and EXIST benchmarks, while achieving faster training convergence. Further analyses reveal that the learned graph structure captures semantically meaningful patterns, offering valuable insights into the relational nature of online hate.</p>
  </article>
</body>
</html>
