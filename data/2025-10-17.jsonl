{"id": "2510.13889", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13889", "abs": "https://arxiv.org/abs/2510.13889", "authors": ["Yue Hu", "Guohang Zhuang"], "title": "MultiFoodhat: A potential new paradigm for intelligent food quality inspection", "comment": null, "summary": "Food image classification plays a vital role in intelligent food quality\ninspection, dietary assessment, and automated monitoring. However, most\nexisting supervised models rely heavily on large labeled datasets and exhibit\nlimited generalization to unseen food categories. To overcome these challenges,\nthis study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning\nframework for zero-shot food recognition. The framework integrates\nvision-language models (VLMs) and large language models (LLMs) to enable\ncollaborative reasoning through multi-round visual-textual dialogues. An Object\nPerception Token (OPT) captures fine-grained visual attributes, while an\nInteractive Reasoning Agent (IRA) dynamically interprets contextual cues to\nrefine predictions. This multi-agent design allows flexible and human-like\nunderstanding of complex food scenes without additional training or manual\nannotations. Experiments on multiple public food datasets demonstrate that\nMultiFoodChat achieves superior recognition accuracy and interpretability\ncompared with existing unsupervised and few-shot methods, highlighting its\npotential as a new paradigm for intelligent food quality inspection and\nanalysis."}
{"id": "2510.13993", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13993", "abs": "https://arxiv.org/abs/2510.13993", "authors": ["Jia Yun Chua", "Argyrios Zolotas", "Miguel Arana-Catania"], "title": "Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models", "comment": "11 pages, 7 figures, 8 tables. To be published in Applied AI Letters", "summary": "Remote sensing has become a vital tool across sectors such as urban planning,\nenvironmental monitoring, and disaster response. While the volume of data\ngenerated has increased significantly, traditional vision models are often\nconstrained by the requirement for extensive domain-specific labelled data and\ntheir limited ability to understand the context within complex environments.\nVision Language Models offer a complementary approach by integrating visual and\ntextual data; however, their application to remote sensing remains\nunderexplored, particularly given their generalist nature. This work\ninvestigates the combination of vision models and VLMs to enhance image\nanalysis in remote sensing, with a focus on aircraft detection and scene\nunderstanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and\nGemini aims to achieve more accurate and contextually aware image\ninterpretation. Performance is evaluated on both labelled and unlabelled remote\nsensing data, as well as degraded image scenarios which are crucial for remote\nsensing. The findings show an average MAE improvement of 48.46% across models\nin the accuracy of aircraft detection and counting, especially in challenging\nconditions, in both raw and degraded scenarios. A 6.17% improvement in\nCLIPScore for comprehensive understanding of remote sensing images is obtained.\nThe proposed approach combining traditional vision models and VLMs paves the\nway for more advanced and efficient remote sensing image analysis, especially\nin few-shot learning scenarios."}
{"id": "2510.14032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14032", "abs": "https://arxiv.org/abs/2510.14032", "authors": ["Xiaoqian Shen", "Wenxuan Zhang", "Jun Chen", "Mohamed Elhoseiny"], "title": "Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding", "comment": "NeurIPS 2025 (Spotlight). Webpage at\n  https://xiaoqian-shen.github.io/Vgent", "summary": "Understanding and reasoning over long videos pose significant challenges for\nlarge video language models (LVLMs) due to the difficulty in processing\nintensive video tokens beyond context window and retaining long-term sequential\ninformation. Retrieval-Augmented Generation (RAG) has demonstrated\neffectiveness in processing long context for Large Language Models (LLMs);\nhowever, applying RAG to long video faces challenges such as disrupted temporal\ndependencies and inclusion of irrelevant information that can hinder accurate\nreasoning. To address these limitations, we propose Vgent, a novel graph-based\nretrieval-reasoning-augmented generation framework to enhance LVLMs for long\nvideo understanding. Our approach introduces two key innovations: (i) It\nrepresents videos by structured graphs with semantic relationships across video\nclips preserved to improve retrieval effectiveness. (ii) It introduces an\nintermediate reasoning step to mitigate the reasoning limitation of LVLMs,\nwhich leverages structured verification to reduce retrieval noise and\nfacilitate the explicit aggregation of relevant information across clips,\nresulting in more accurate and context-aware responses. We comprehensively\nevaluate our framework with various open-source LVLMs on three long-video\nunderstanding benchmarks. Our approach yielded an overall performance\nimprovement of $3.0\\%\\sim 5.4\\%$ over base models on MLVU, and outperformed\nstate-of-the-art video RAG methods by $8.6\\%$. Our code is publicly available\nat https://xiaoqian-shen.github.io/Vgent."}
{"id": "2510.14081", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.14081", "abs": "https://arxiv.org/abs/2510.14081", "authors": ["Emanuel Garbin", "Guy Adam", "Oded Krams", "Zohar Barzelay", "Eran Guendelman", "Michael Schwarz", "Moran Vatelmacher", "Yigal Shenkman", "Eli Peker", "Itai Druker", "Uri Patish", "Yoav Blum", "Max Bluvstein", "Junxuan Li", "Rawal Khirodkar", "Shunsuke Saito"], "title": "Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images", "comment": null, "summary": "We present a novel, zero-shot pipeline for creating hyperrealistic,\nidentity-preserving 3D avatars from a few unstructured phone images. Existing\nmethods face several challenges: single-view approaches suffer from geometric\ninconsistencies and hallucinations, degrading identity preservation, while\nmodels trained on synthetic data fail to capture high-frequency details like\nskin wrinkles and fine hair, limiting realism. Our method introduces two key\ncontributions: (1) a generative canonicalization module that processes multiple\nunstructured views into a standardized, consistent representation, and (2) a\ntransformer-based model trained on a new, large-scale dataset of high-fidelity\nGaussian splatting avatars derived from dome captures of real people. This\n\"Capture, Canonicalize, Splat\" pipeline produces static quarter-body avatars\nwith compelling realism and robust identity preservation from unstructured\nphotos."}
{"id": "2510.13979", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13979", "abs": "https://arxiv.org/abs/2510.13979", "authors": ["Supriti Sinhamahapatra", "Jan Niehues"], "title": "Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks", "comment": null, "summary": "State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily\nrely on acoustic information while disregarding additional multi-modal context.\nHowever, visual information are essential in disambiguation and adaptation.\nWhile most work focus on speaker images to handle noise conditions, this work\nalso focuses on integrating presentation slides for the use cases of scientific\npresentation.\n  In a first step, we create a benchmark for multi-modal presentation including\nan automatic analysis of transcribing domain-specific terminology. Next, we\nexplore methods for augmenting speech models with multi-modal information. We\nmitigate the lack of datasets with accompanying slides by a suitable approach\nof data augmentation. Finally, we train a model using the augmented dataset,\nresulting in a relative reduction in word error rate of approximately 34%,\nacross all words and 35%, for domain-specific terms compared to the baseline\nmodel."}
{"id": "2510.13827", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13827", "abs": "https://arxiv.org/abs/2510.13827", "authors": ["Ashish Kattamuri", "Ishita Prasad", "Meetu Malhotra", "Arpita Vats", "Rahul Raja", "Albert Lie"], "title": "Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL", "comment": "20th International Workshop on Semantic and Social Media Adaptation &\n  Personalization", "summary": "Current Text-to-SQL methods are evaluated and only focused on executable\nqueries, overlooking the semantic alignment challenge -- both in terms of the\nsemantic meaning of the query and the correctness of the execution results.\nEven execution accuracy itself shows significant drops when moving from English\nto other languages, with an average decline of 6 percentage points across\nnon-English languages. We address these challenges by presenting a new\nframework that combines Group Relative Policy Optimization (GRPO) within a\nmultilingual contrastive reward signal to enhance both task efficiency and\nsemantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method\nteaches models to obtain better correspondence between SQL generation and user\nintent by combining a reward signal based on semantic similarity. On the\nseven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO\nimproved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and\nsemantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive\nreward signal in the GRPO framework further improved the average semantic\naccuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our\nexperiments showcase that a smaller, parameter-efficient 3B LLaMA model\nfine-tuned with our contrastive reward signal outperforms a much larger\nzero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from\n81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly\nmatches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using\njust 3,000 reinforcement learning training examples. These results demonstrate\nhow we can improve the performance of Text-to-SQL systems with contrastive\nrewards for directed semantic alignment, without requiring large-scale training\ndatasets."}
{"id": "2510.14203", "categories": ["cs.CV", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.14203", "abs": "https://arxiv.org/abs/2510.14203", "authors": ["Ryo Masumura", "Shota Orihashi", "Mana Ihori", "Tomohiro Tanaka", "Naoki Makishima", "Taiga Yamane", "Naotaka Kawata", "Satoshi Suzuki", "Taichi Katayama"], "title": "Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition", "comment": "Accepted at APSIPA ASC 2025", "summary": "This paper proposes a joint modeling method of the Big Five, which has long\nbeen studied, and HEXACO, which has recently attracted attention in psychology,\nfor automatically recognizing apparent personality traits from multimodal human\nbehavior. Most previous studies have used the Big Five for multimodal apparent\npersonality-trait recognition. However, no study has focused on apparent HEXACO\nwhich can evaluate an Honesty-Humility trait related to displaced aggression\nand vengefulness, social-dominance orientation, etc. In addition, the\nrelationships between the Big Five and HEXACO when modeled by machine learning\nhave not been clarified. We expect awareness of multimodal human behavior to\nimprove by considering these relationships. The key advance of our proposed\nmethod is to optimize jointly recognizing the Big Five and HEXACO. Experiments\nusing a self-introduction video dataset demonstrate that the proposed method\ncan effectively recognize the Big Five and HEXACO."}
{"id": "2510.14035", "categories": ["cs.AI", "I.2.6; I.2.9"], "pdf": "https://arxiv.org/pdf/2510.14035", "abs": "https://arxiv.org/abs/2510.14035", "authors": ["Rajesh Mangannavar", "Prasad Tadepalli"], "title": "GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations", "comment": "10 pages content. 2 pages references", "summary": "We introduce an action-centric graph representation framework for learning to\nguide planning in Partially Observable Markov Decision Processes (POMDPs).\nUnlike existing approaches that require domain-specific neural architectures\nand struggle with scalability, GammaZero leverages a unified graph-based belief\nrepresentation that enables generalization across problem sizes within a\ndomain. Our key insight is that belief states can be systematically transformed\ninto action-centric graphs where structural patterns learned on small problems\ntransfer to larger instances. We employ a graph neural network with a decoder\narchitecture to learn value functions and policies from expert demonstrations\non computationally tractable problems, then apply these learned heuristics to\nguide Monte Carlo tree search on larger problems. Experimental results on\nstandard POMDP benchmarks demonstrate that GammaZero achieves comparable\nperformance to BetaZero when trained and tested on the same-sized problems,\nwhile uniquely enabling zero-shot generalization to problems 2-4 times larger\nthan those seen during training, maintaining solution quality with reduced\nsearch requirements."}
{"id": "2510.13856", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13856", "abs": "https://arxiv.org/abs/2510.13856", "authors": ["A H M Rezaul Karim", "Ozlem Uzuner"], "title": "Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA", "comment": null, "summary": "Medical Visual Question Answering (MedVQA) enables natural language queries\nover medical images to support clinical decision-making and patient care. The\nMEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to\ngenerate free-text responses and structured wound attributes from images and\npatient queries. We present the MasonNLP system, which employs a\ngeneral-domain, instruction-tuned large language model with a\nretrieval-augmented generation (RAG) framework that incorporates textual and\nvisual examples from in-domain data. This approach grounds outputs in\nclinically relevant exemplars, improving reasoning, schema adherence, and\nresponse quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our\nbest-performing system ranked 3rd among 19 teams and 51 submissions with an\naverage score of 41.37%, demonstrating that lightweight RAG with\ngeneral-purpose LLMs -- a minimal inference-time layer that adds a few relevant\nexemplars via simple indexing and fusion, with no extra training or complex\nre-ranking -- provides a simple and effective baseline for multimodal clinical\nNLP tasks."}
{"id": "2510.14241", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14241", "abs": "https://arxiv.org/abs/2510.14241", "authors": ["Soumyya Kanti Datta", "Tanvi Ranga", "Chengzhe Sun", "Siwei Lyu"], "title": "PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis", "comment": null, "summary": "The rise of manipulated media has made deepfakes a particularly insidious\nthreat, involving various generative manipulations such as lip-sync\nmodifications, face-swaps, and avatar-driven facial synthesis. Conventional\ndetection methods, which predominantly depend on manually designed\nphoneme-viseme alignment thresholds, fundamental frame-level consistency\nchecks, or a unimodal detection strategy, inadequately identify modern-day\ndeepfakes generated by advanced generative models such as GANs, diffusion\nmodels, and neural rendering techniques. These advanced techniques generate\nnearly perfect individual frames yet inadvertently create minor temporal\ndiscrepancies frequently overlooked by traditional detectors. We present a\nnovel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic\nAnalysis(PIA), incorporating language, dynamic face motion, and facial\nidentification cues to address these limitations. We utilize phoneme sequences,\nlip geometry data, and advanced facial identity embeddings. This integrated\nmethod significantly improves the detection of subtle deepfake alterations by\nidentifying inconsistencies across multiple complementary modalities. Code is\navailable at https://github.com/skrantidatta/PIA"}
{"id": "2510.14136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14136", "abs": "https://arxiv.org/abs/2510.14136", "authors": ["David Roqui", "Adèle Cormier", "nistor Grozavu", "Ann Bourges"], "title": "A Multimodal Approach to Heritage Preservation in the Context of Climate Change", "comment": null, "summary": "Cultural heritage sites face accelerating degradation due to climate change,\nyet tradi- tional monitoring relies on unimodal analysis (visual inspection or\nenvironmental sen- sors alone) that fails to capture the complex interplay\nbetween environmental stres- sors and material deterioration. We propose a\nlightweight multimodal architecture that fuses sensor data (temperature,\nhumidity) with visual imagery to predict degradation severity at heritage\nsites. Our approach adapts PerceiverIO with two key innovations: (1) simplified\nencoders (64D latent space) that prevent overfitting on small datasets (n=37\ntraining samples), and (2) Adaptive Barlow Twins loss that encourages modality\ncomplementarity rather than redundancy. On data from Strasbourg Cathedral, our\nmodel achieves 76.9% accu- racy, a 43% improvement over standard multimodal\narchitectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.\nAblation studies reveal that sensor-only achieves 61.5% while image-only\nreaches 46.2%, confirming successful multimodal synergy. A systematic\nhyperparameter study identifies an optimal moderate correlation target ({\\tau}\n=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy\ncompared to other {\\tau} values ({\\tau} =0.1/0.5/0.7: 53.8%, {\\tau} =0.9:\n61.5%). This work demonstrates that architectural sim- plicity combined with\ncontrastive regularization enables effective multimodal learning in data-scarce\nheritage monitoring contexts, providing a foundation for AI-driven con-\nservation decision support systems."}
{"id": "2510.13862", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.13862", "abs": "https://arxiv.org/abs/2510.13862", "authors": ["Chenyu Zhang", "Sharifa Alghowinem", "Cynthia Breazeal"], "title": "Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues", "comment": "4 pages, 3 figures. Published in the 11th International Conference on\n  Affective Computing and Intelligent Interaction (ACII 2025), Late-Breaking\n  Results Track", "summary": "While recent studies have examined the leaning impact of large language model\n(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring\nremain insufficiently understood. This work introduces the first ensemble-LLM\nframework for large-scale affect sensing in tutoring dialogues, advancing the\nconversation on responsible pathways for integrating generative AI into\neducation by attending to learners' evolving affective states. To achieve this,\nwe analyzed two semesters' worth of 16,986 conversational turns exchanged\nbetween PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across\nthree U.S. institutions. To investigate learners' emotional experiences, we\ngenerate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,\nClaude), including scalar ratings of valence, arousal, and\nlearning-helpfulness, along with free-text emotion labels. These estimates are\nfused through rank-weighted intra-model pooling and plurality consensus across\nmodels to produce robust emotion profiles. Our analysis shows that during\ninteraction with the AI tutor, students typically report mildly positive affect\nand moderate arousal. Yet learning is not uniformly smooth: confusion and\ncuriosity are frequent companions to problem solving, and frustration, while\nless common, still surfaces in ways that can derail progress. Emotional states\nare short-lived--positive moments last slightly longer than neutral or negative\nones, but they are fragile and easily disrupted. Encouragingly, negative\nemotions often resolve quickly, sometimes rebounding directly into positive\nstates. Neutral moments frequently act as turning points, more often steering\nstudents upward than downward, suggesting opportunities for tutors to intervene\nat precisely these junctures."}
{"id": "2510.14304", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14304", "abs": "https://arxiv.org/abs/2510.14304", "authors": ["Kyungryul Back", "Seongbeom Park", "Milim Kim", "Mincheol Kwon", "SangHyeok Lee", "Hyunyoung Lee", "Junhee Cho", "Seunghyun Park", "Jinkyu Kim"], "title": "Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding", "comment": "EMNLP 2025 Findings; Project: https://github.com/KR-0822/TCD", "summary": "Large Vision-Language Models (LVLMs) have recently shown promising results on\nvarious multimodal tasks, even achieving human-comparable performance in\ncertain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often\nrely heavily on a single modality or memorize training data without properly\ngrounding their outputs. To address this, we propose a training-free, tri-layer\ncontrastive decoding with watermarking, which proceeds in three steps: (1)\nselect a mature layer and an amateur layer among the decoding layers, (2)\nidentify a pivot layer using a watermark-related question to assess whether the\nlayer is visually well-grounded, and (3) apply tri-layer contrastive decoding\nto generate the final output. Experiments on public benchmarks such as POPE,\nMME and AMBER demonstrate that our method achieves state-of-the-art performance\nin reducing hallucinations in LVLMs and generates more visually grounded\nresponses."}
{"id": "2510.14176", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14176", "abs": "https://arxiv.org/abs/2510.14176", "authors": ["Roger Creus Castanyer", "Faisal Mohamed", "Pablo Samuel Castro", "Cyrus Neary", "Glen Berseth"], "title": "ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) algorithms are highly sensitive to reward\nfunction specification, which remains a central challenge limiting their broad\napplicability. We present ARM-FM: Automated Reward Machines via Foundation\nModels, a framework for automated, compositional reward design in RL that\nleverages the high-level reasoning capabilities of foundation models (FMs).\nReward machines (RMs) -- an automata-based formalism for reward specification\n-- are used as the mechanism for RL objective specification, and are\nautomatically constructed via the use of FMs. The structured formalism of RMs\nyields effective task decompositions, while the use of FMs enables objective\nspecifications in natural language. Concretely, we (i) use FMs to automatically\ngenerate RMs from natural language specifications; (ii) associate language\nembeddings with each RM automata-state to enable generalization across tasks;\nand (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse\nsuite of challenging environments, including evidence of zero-shot\ngeneralization."}
{"id": "2510.13885", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13885", "abs": "https://arxiv.org/abs/2510.13885", "authors": ["Ariel Kamen"], "title": "Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization", "comment": "10 pages, 4 figures,", "summary": "This study presents a comparative evaluation of ten state-of-the-art large\nlanguage models (LLMs) applied to unstructured text categorization using the\nInteractive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis\nemployed a uniform dataset of 8,660 human-annotated samples and identical\nzero-shot prompts to ensure methodological consistency across all models.\nEvaluation metrics included four classic measures - accuracy, precision,\nrecall, and F1-score - and three LLM-specific indicators: hallucination ratio,\ninflation ratio, and categorization cost.\n  Results show that, despite their rapid advancement, contemporary LLMs achieve\nonly moderate classic performance, with average scores of 34% accuracy, 42%\nprecision, 45% recall, and 41% F1-score. Hallucination and inflation ratios\nreveal that models frequently overproduce categories relative to human\nannotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B\noffered the most favorable cost-to-performance balance, while GPT 120B\ndemonstrated the lowest hallucination ratio. The findings suggest that scaling\nand architectural improvements alone do not ensure better categorization\naccuracy, as the task requires compressing rich unstructured text into a\nlimited taxonomy - a process that challenges current model architectures.\n  To address these limitations, a separate ensemble-based approach was\ndeveloped and tested. The ensemble method, in which multiple LLMs act as\nindependent experts, substantially improved accuracy, reduced inflation, and\ncompletely eliminated hallucinations. These results indicate that coordinated\norchestration of models - rather than sheer scale - may represent the most\neffective path toward achieving or surpassing human-expert performance in\nlarge-scale text categorization."}
{"id": "2510.14349", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14349", "abs": "https://arxiv.org/abs/2510.14349", "authors": ["Yunnan Wang", "Fan Lu", "Kecheng Zheng", "Ziyuan Huang", "Ziqiang Li", "Wenjun Zeng", "Xin Jin"], "title": "Vision-Centric Activation and Coordination for Multimodal Large Language Models", "comment": "Under Review", "summary": "Multimodal large language models (MLLMs) integrate image features from visual\nencoders with LLMs, demonstrating advanced comprehension capabilities. However,\nmainstream MLLMs are solely supervised by the next-token prediction of textual\ntokens, neglecting critical vision-centric information essential for analytical\nabilities. To track this dilemma, we introduce VaCo, which optimizes MLLM\nrepresentations through Vision-Centric activation and Coordination from\nmultiple vision foundation models (VFMs). VaCo introduces visual discriminative\nalignment to integrate task-aware perceptual features extracted from VFMs,\nthereby unifying the optimization of both textual and visual outputs in MLLMs.\nSpecifically, we incorporate the learnable Modular Task Queries (MTQs) and\nVisual Alignment Layers (VALs) into MLLMs, activating specific visual signals\nunder the supervision of diverse VFMs. To coordinate representation conflicts\nacross VFMs, the crafted Token Gateway Mask (TGM) restricts the information\nflow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo\nsignificantly improves the performance of different MLLMs on various\nbenchmarks, showcasing its superior capabilities in visual comprehension."}
{"id": "2510.14194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14194", "abs": "https://arxiv.org/abs/2510.14194", "authors": ["Göktuğ Bender", "Samer Faraj", "Anand Bhardwaj"], "title": "Implementation of AI in Precision Medicine", "comment": "Accepted to SMASH 2025", "summary": "Artificial intelligence (AI) has become increasingly central to precision\nmedicine by enabling the integration and interpretation of multimodal data, yet\nimplementation in clinical settings remains limited. This paper provides a\nscoping review of literature from 2019-2024 on the implementation of AI in\nprecision medicine, identifying key barriers and enablers across data quality,\nclinical reliability, workflow integration, and governance. Through an\necosystem-based framework, we highlight the interdependent relationships\nshaping real-world translation and propose future directions to support\ntrustworthy and sustainable implementation."}
{"id": "2510.13909", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13909", "abs": "https://arxiv.org/abs/2510.13909", "authors": ["Xingrui Zhuo", "Jiapu Wang", "Gongqing Wu", "Zhongyuan Wang", "Jichen Zhang", "Shirui Pan", "Xindong Wu"], "title": "Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning", "comment": null, "summary": "Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in\nopen-domain KGs containing unknown entities and relations, which poses a\nchallenge for KGR models in comprehending uncertain KG components. Existing\nstudies have proposed Knowledge Graph Foundation Models (KGFMs) that learn\nstructural invariances across KGs to handle this uncertainty. Recently, Large\nLanguage Models (LLMs) have demonstrated strong capabilities for open-domain\nknowledge reasoning. As a result, the latest research has focused on LLM-based\nKGFMs that integrate LLM knowledge with KG context for inductive KGR. However,\nthe intrinsic knowledge of LLMs may be overshadowed by sparse KG context,\nleading to LLM knowledge distortion, which can cause irreversible damage to\nmodel reasoning. Moreover, existing LLM-based KGR methods still struggle to\nfully constrain generative hallucinations in LLMs, severely limiting the\ncredibility of reasoning results. To address these limitations, we propose a\nKnowledge Reasoning Language Model (KRLM) that achieves unified coordination\nbetween LLM knowledge and KG context throughout the KGR process. Specifically,\nwe design a Knowledge Reasoning Language (KRL) instruction format and a KRL\ntokenizer to align LLM knowledge with KG representations. Then, we propose a\nKRL attention layer that coordinates intrinsic LLM knowledge with additional KG\ncontext through a dynamic knowledge memory mechanism. Finally, a\nstructure-aware next-entity predictor is proposed, which strictly constrains\nthe reasoning results within a trustworthy knowledge domain. Extensive\nexperimental results on 25 real-world inductive KGR datasets demonstrate the\nsignificant superiority of the proposed KRLM\\footnote{Our source codes are\navailable at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot\nreasoning and fine-tuning scenarios."}
{"id": "2510.14374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14374", "abs": "https://arxiv.org/abs/2510.14374", "authors": ["Han Qiu", "Peng Gao", "Lewei Lu", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "Spatial Preference Rewarding for MLLMs Spatial Understanding", "comment": "ICCV 2025", "summary": "Multimodal large language models~(MLLMs) have demonstrated promising spatial\nunderstanding capabilities, such as referencing and grounding object\ndescriptions. Despite their successes, MLLMs still fall short in fine-grained\nspatial perception abilities, such as generating detailed region descriptions\nor accurately localizing objects. Additionally, they often fail to respond to\nthe user's requirements for desired fine-grained spatial understanding. This\nissue might arise because existing approaches primarily focus on tuning MLLMs\nto model pre-annotated instruction data to inject spatial knowledge, without\ndirect supervision of MLLMs' actual responses. We address this issue by SPR, a\nSpatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial\ncapabilities by rewarding MLLMs' detailed responses with precise object\nlocalization over vague or inaccurate responses. With randomly selected image\nregions and region descriptions from MLLMs, SPR introduces semantic and\nlocalization scores to comprehensively evaluate the text quality and\nlocalization quality in MLLM-generated descriptions. We also refine the MLLM\ndescriptions with better localization accuracy and pair the best-scored\nrefinement with the initial descriptions of the lowest score for direct\npreference optimization, thereby enhancing fine-grained alignment with visual\ninput. Extensive experiments over standard referring and grounding benchmarks\nshow that SPR improves MLLM spatial understanding capabilities effectively with\nminimal overhead in training. Data and code will be released at\nhttps://github.com/hanqiu-hq/SPR"}
{"id": "2510.14387", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14387", "abs": "https://arxiv.org/abs/2510.14387", "authors": ["Yijie Hu", "Zihao Zhou", "Kaizhu Huang", "Xiaowei Huang", "Qiufeng Wang"], "title": "Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?", "comment": null, "summary": "Math reasoning has been one crucial ability of large language models (LLMs),\nwhere significant advancements have been achieved in recent years. However,\nmost efforts focus on LLMs by curating high-quality annotation data and\nintricate training (or inference) paradigms, while the math reasoning\nperformance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM\ntypically consists of an LLM and a vision block, we wonder: Can MLLMs directly\nabsorb math reasoning abilities from off-the-shelf math LLMs without tuning?\nRecent model-merging approaches may offer insights into this question. However,\nthey overlook the alignment between the MLLM and LLM, where we find that there\nis a large gap between their parameter spaces, resulting in lower performance.\nOur empirical evidence reveals two key factors behind this issue: the\nidentification of crucial reasoning-associated layers in the model and the\nmitigation of the gaps in parameter space. Based on the empirical insights, we\npropose IP-Merging that first identifies the reasoning-associated parameters in\nboth MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to\nmaintain the alignment, and finally merges parameters in this subspace.\nIP-Merging is a tuning-free approach since parameters are directly adjusted.\nExtensive experiments demonstrate that our IP-Merging method can enhance the\nmath reasoning ability of MLLMs directly from Math LLMs without compromising\ntheir other capabilities."}
{"id": "2510.14305", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14305", "abs": "https://arxiv.org/abs/2510.14305", "authors": ["Mahbub E Sobhani", "Md. Faiyaz Abdullah Sayeedi", "Tasnim Mohiuddin", "Md Mofijul Islam", "Swakkhar Shatabda"], "title": "MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning", "comment": null, "summary": "Mathematical reasoning remains one of the most challenging domains for large\nlanguage models (LLMs), requiring not only linguistic understanding but also\nstructured logical deduction and numerical precision. While recent LLMs\ndemonstrate strong general-purpose reasoning abilities, their mathematical\ncompetence across diverse languages remains underexplored. Existing benchmarks\nprimarily focus on English or a narrow subset of high-resource languages,\nleaving significant gaps in assessing multilingual and cross-lingual\nmathematical reasoning. To address this, we introduce MathMist, a parallel\nmultilingual benchmark for mathematical problem solving and reasoning. MathMist\nencompasses over 21K aligned question-answer pairs across seven languages,\nrepresenting a balanced coverage of high-, medium-, and low-resource linguistic\nsettings. The dataset captures linguistic variety, multiple types of problem\nsettings, and solution synthesizing capabilities. We systematically evaluate a\ndiverse suite of models, including open-source small and medium LLMs,\nproprietary systems, and multilingual-reasoning-focused models, under\nzero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our\nresults reveal persistent deficiencies in LLMs' ability to perform consistent\nand interpretable mathematical reasoning across languages, with pronounced\ndegradation in low-resource settings. All the codes and data are available at\nGitHub: https://github.com/mahbubhimel/MathMist"}
{"id": "2510.14376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14376", "abs": "https://arxiv.org/abs/2510.14376", "authors": ["Dongnam Byun", "Jungwon Park", "Jumgmin Ko", "Changin Choi", "Wonjong Rhee"], "title": "DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation", "comment": null, "summary": "Recent progress in text-to-image (T2I) generative models has led to\nsignificant improvements in generating high-quality images aligned with text\nprompts. However, these models still struggle with prompts involving multiple\nobjects, often resulting in object neglect or object mixing. Through extensive\nstudies, we identify four problematic scenarios, Similar Shapes, Similar\nTextures, Dissimilar Background Biases, and Many Objects, where inter-object\nrelationships frequently lead to such failures. Motivated by two key\nobservations about CLIP embeddings, we propose DOS (Directional Object\nSeparation), a method that modifies three types of CLIP text embeddings before\npassing them into text-to-image models. Experimental results show that DOS\nconsistently improves the success rate of multi-object image generation and\nreduces object mixing. In human evaluations, DOS significantly outperforms four\ncompeting methods, receiving 26.24%-43.04% more votes across four benchmarks.\nThese results highlight DOS as a practical and effective solution for improving\nmulti-object image generation."}
{"id": "2510.14388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14388", "abs": "https://arxiv.org/abs/2510.14388", "authors": ["Zhe Wu", "Hongjin Lu", "Junliang Xing", "Changhao Zhang", "Yin Zhu", "Yuhao Yang", "Yuheng Jing", "Kai Li", "Kun Shao", "Jianye Hao", "Jun Wang", "Yuanchun Shi"], "title": "Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control", "comment": null, "summary": "Building agents that autonomously operate mobile devices has attracted\nincreasing attention. While Vision-Language Models (VLMs) show promise, most\nexisting approaches rely on direct state-to-action mappings, which lack\nstructured reasoning and planning, and thus generalize poorly to novel tasks or\nunseen UI layouts. We introduce Hi-Agent, a trainable hierarchical\nvision-language agent for mobile control, featuring a high-level reasoning\nmodel and a low-level action model that are jointly optimized. For efficient\ntraining, we reformulate multi-step decision-making as a sequence of\nsingle-step subgoals and propose a foresight advantage function, which\nleverages execution feedback from the low-level model to guide high-level\noptimization. This design alleviates the path explosion issue encountered by\nGroup Relative Policy Optimization (GRPO) in long-horizon tasks and enables\nstable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art\n(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,\nsignificantly outperforming prior methods across three paradigms: prompt-based\n(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement\nlearning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot\ngeneralization on the ScreenSpot-v2 benchmark. On the more challenging\nAndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,\nshowing strong adaptability in high-complexity mobile control scenarios."}
{"id": "2510.14307", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14307", "abs": "https://arxiv.org/abs/2510.14307", "authors": ["Sathyanarayanan Ramamoorthy", "Vishwa Shah", "Simran Khanuja", "Zaid Sheikh", "Shan Jie", "Ann Chia", "Shearman Chua", "Graham Neubig"], "title": "MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and Linking", "comment": null, "summary": "This paper introduces MERLIN, a novel testbed system for the task of\nMultilingual Multimodal Entity Linking. The created dataset includes BBC news\narticle titles, paired with corresponding images, in five languages: Hindi,\nJapanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity\nmentions linked to 2,500 unique Wikidata entities. We also include several\nbenchmarks using multilingual and multimodal entity linking methods exploring\ndifferent language models like LLaMa-2 and Aya-23. Our findings indicate that\nincorporating visual data improves the accuracy of entity linking, especially\nfor entities where the textual context is ambiguous or insufficient, and\nparticularly for models that do not have strong multilingual abilities. For the\nwork, the dataset, methods are available here at\nhttps://github.com/rsathya4802/merlin"}
{"id": "2510.14526", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14526", "abs": "https://arxiv.org/abs/2510.14526", "authors": ["Yunze Tong", "Didi Zhu", "Zijing Hu", "Jinluan Yang", "Ziyu Zhao"], "title": "Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models", "comment": "Appendix will be appended soon", "summary": "In text-to-image generation, different initial noises induce distinct\ndenoising paths with a pretrained Stable Diffusion (SD) model. While this\npattern could output diverse images, some of them may fail to align well with\nthe prompt. Existing methods alleviate this issue either by altering the\ndenoising dynamics or by drawing multiple noises and conducting post-selection.\nIn this paper, we attribute the misalignment to a training-inference mismatch:\nduring training, prompt-conditioned noises lie in a prompt-specific subset of\nthe latent space, whereas at inference the noise is drawn from a\nprompt-agnostic Gaussian prior. To close this gap, we propose a noise projector\nthat applies text-conditioned refinement to the initial noise before denoising.\nConditioned on the prompt embedding, it maps the noise to a prompt-aware\ncounterpart that better matches the distribution observed during SD training,\nwithout modifying the SD model. Our framework consists of these steps: we first\nsample some noises and obtain token-level feedback for their corresponding\nimages from a vision-language model (VLM), then distill these signals into a\nreward model, and finally optimize the noise projector via a quasi-direct\npreference optimization. Our design has two benefits: (i) it requires no\nreference images or handcrafted priors, and (ii) it incurs small inference\ncost, replacing multi-sample selection with a single forward pass. Extensive\nexperiments further show that our prompt-aware noise projection improves\ntext-image alignment across diverse prompts."}
{"id": "2510.14621", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14621", "abs": "https://arxiv.org/abs/2510.14621", "authors": ["Yuanyi Song", "Heyuan Huang", "Qiqiang Lin", "Yin Zhao", "Xiangmou Qu", "Jun Wang", "Xingyu Lou", "Weiwen Liu", "Zhuosheng Zhang", "Jun Wang", "Yong Yu", "Weinan Zhang", "Zhaoxiang Wang"], "title": "ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks", "comment": null, "summary": "The rapid advancement of multimodal large language models has enabled agents\nto operate mobile devices by directly interacting with graphical user\ninterfaces, opening new possibilities for mobile automation. However,\nreal-world mobile tasks are often complex and allow for multiple valid\nsolutions. This contradicts current mobile agent evaluation standards: offline\nstatic benchmarks can only validate a single predefined \"golden path\", while\nonline dynamic testing is constrained by the complexity and non-reproducibility\nof real devices, making both approaches inadequate for comprehensively\nassessing agent capabilities. To bridge the gap between offline and online\nevaluation and enhance testing stability, this paper introduces a novel\ngraph-structured benchmarking framework. By modeling the finite states observed\nduring real-device interactions, it achieves static simulation of dynamic\nbehaviors. Building on this, we develop ColorBench, a benchmark focused on\ncomplex long-horizon tasks. It supports evaluation of multiple valid solutions,\nsubtask completion rate statistics, and atomic-level capability analysis.\nColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average\nlength of over 13 steps. Each task includes at least two correct paths and\nseveral typical error paths, enabling quasi-dynamic interaction. By evaluating\nColorBench across various baselines, we discover limitations of existing models\nand propose improvement directions and feasible technical pathways to enhance\nagents' performance on complex, long-horizon problems based on experimental\nresults. Code and data are available at:\nhttps://github.com/MadeAgents/ColorBench."}
{"id": "2510.14377", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14377", "abs": "https://arxiv.org/abs/2510.14377", "authors": ["Mykolas Sveistrys", "Richard Kunert"], "title": "PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora", "comment": null, "summary": "Recent advances in large language models (LLMs) and retrieval-augmented\ngeneration (RAG) have enabled progress on question answering (QA) when relevant\nevidence is in one (single-hop) or multiple (multi-hop) passages. Yet many\nrealistic questions about recurring report data - medical records, compliance\nfilings, maintenance logs - require aggregation across all documents, with no\nclear stopping point for retrieval and high sensitivity to even one missed\npassage. We term these pluri-hop questions and formalize them by three\ncriteria: recall sensitivity, exhaustiveness, and exactness. To study this\nsetting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48\npluri-hop questions built from 191 real-world wind industry reports in German\nand English. We show that PluriHopWIND is 8-40% more repetitive than other\ncommon datasets and thus has higher density of distractor documents, better\nreflecting practical challenges of recurring report corpora. We test a\ntraditional RAG pipeline as well as graph-based and multimodal variants, and\nfind that none of the tested approaches exceed 40% in statement-wise F1 score.\nMotivated by this, we propose PluriHopRAG, a RAG architecture that follows a\n\"check all documents individually, filter cheaply\" approach: it (i) decomposes\nqueries into document-level subquestions and (ii) uses a cross-encoder filter\nto discard irrelevant documents before costly LLM reasoning. We find that\nPluriHopRAG achieves relative F1 score improvements of 18-52% depending on base\nLLM. Despite its modest size, PluriHopWIND exposes the limitations of current\nQA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance\nhighlights the value of exhaustive retrieval and early filtering as a powerful\nalternative to top-k methods."}
{"id": "2510.14528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14528", "abs": "https://arxiv.org/abs/2510.14528", "authors": ["Cheng Cui", "Ting Sun", "Suyin Liang", "Tingquan Gao", "Zelun Zhang", "Jiaxuan Liu", "Xueqing Wang", "Changda Zhou", "Hongen Liu", "Manhui Lin", "Yue Zhang", "Yubo Zhang", "Handong Zheng", "Jing Zhang", "Jun Zhang", "Yi Liu", "Dianhai Yu", "Yanjun Ma"], "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model", "comment": null, "summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model\ntailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a\ncompact yet powerful vision-language model (VLM) that integrates a NaViT-style\ndynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to\nenable accurate element recognition. This innovative model efficiently supports\n109 languages and excels in recognizing complex elements (e.g., text, tables,\nformulas, and charts), while maintaining minimal resource consumption. Through\ncomprehensive evaluations on widely used public benchmarks and in-house\nbenchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document\nparsing and element-level recognition. It significantly outperforms existing\nsolutions, exhibits strong competitiveness against top-tier VLMs, and delivers\nfast inference speeds. These strengths make it highly suitable for practical\ndeployment in real-world scenarios."}
{"id": "2510.14861", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14861", "abs": "https://arxiv.org/abs/2510.14861", "authors": ["Le Cong", "Zaixi Zhang", "Xiaotong Wang", "Yin Di", "Ruofan Jin", "Michal Gerasimiuk", "Yinkai Wang", "Ravi K. Dinesh", "David Smerkous", "Alex Smerkous", "Xuekun Wu", "Shilong Liu", "Peishan Li", "Yi Zhu", "Simran Serrao", "Ning Zhao", "Imran A. Mohammad", "John B. Sunwoo", "Joseph C. Wu", "Mengdi Wang"], "title": "LabOS: The AI-XR Co-Scientist That Sees and Works With Humans", "comment": null, "summary": "Modern science advances fastest when thought meets action. LabOS represents\nthe first AI co-scientist that unites computational reasoning with physical\nexperimentation through multimodal perception, self-evolving agents, and\nEntended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model\nAI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see\nwhat scientists see, understand experimental context, and assist in real-time\nexecution. Across applications--from cancer immunotherapy target discovery to\nstem-cell engineering -- LabOS shows that AI can move beyond computational\ndesign to participation, turning the laboratory into an intelligent,\ncollaborative environment where human and machine discovery evolve together."}
{"id": "2510.14438", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14438", "abs": "https://arxiv.org/abs/2510.14438", "authors": ["Rui Wang", "Ce Zhang", "Jun-Yu Ma", "Jianshu Zhang", "Hongru Wang", "Yi Chen", "Boyang Xue", "Tianqing Fang", "Zhisong Zhang", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Kam-Fai Wong"], "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents", "comment": null, "summary": "Deep research web agents not only retrieve information from diverse sources\nsuch as web environments, files, and multimodal inputs, but more importantly,\nthey need to rigorously analyze and aggregate knowledge for insightful\nresearch. However, existing open-source deep research agents predominantly\nfocus on enhancing information-seeking capabilities of web agents to locate\nspecific information, while overlooking the essential need for information\naggregation, which would limit their ability to support in-depth research. We\npropose an Explore to Evolve paradigm to scalably construct verifiable training\ndata for web agents. Begins with proactive online exploration, an agent sources\ngrounded information by exploring the real web. Using the collected evidence,\nthe agent then self-evolves an aggregation program by selecting, composing, and\nrefining operations from 12 high-level logical types to synthesize a verifiable\nQA pair. This evolution from high-level guidance to concrete operations allowed\nus to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K\nwebsites and 11 domains. Based on an open-source agent framework, SmolAgents,\nwe collect supervised fine-tuning trajectories to develop a series of\nfoundation models, WebAggregator. WebAggregator-8B matches the performance of\nGPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text\nand closely approaches Claude-3.7-sonnet. Moreover, given the limited\navailability of benchmarks that evaluate web agents' information aggregation\nabilities, we construct a human-annotated evaluation split of WebAggregatorQA\nas a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves\n28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all\nreferences, they still struggle on WebAggregatorQA, highlighting the need to\nstrengthen the information aggregation capabilities of web agent foundations."}
{"id": "2510.14532", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14532", "abs": "https://arxiv.org/abs/2510.14532", "authors": ["Xinrui Huang", "Fan Xiao", "Dongming He", "Anqi Gao", "Dandan Li", "Xiaofan Zhang", "Shaoting Zhang", "Xudong Wang"], "title": "Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology", "comment": null, "summary": "Oral and maxillofacial radiology plays a vital role in dental healthcare, but\nradiographic image interpretation is limited by a shortage of trained\nprofessionals. While AI approaches have shown promise, existing dental AI\nsystems are restricted by their single-modality focus, task-specific design,\nand reliance on costly labeled data, hindering their generalization across\ndiverse clinical scenarios. To address these challenges, we introduce DentVFM,\nthe first family of vision foundation models (VFMs) designed for dentistry.\nDentVFM generates task-agnostic visual representations for a wide range of\ndental applications and uses self-supervised learning on DentVista, a large\ncurated dental imaging dataset with approximately 1.6 million multi-modal\nradiographic images from various medical centers. DentVFM includes 2D and 3D\nvariants based on the Vision Transformer (ViT) architecture. To address gaps in\ndental intelligence assessment and benchmarks, we introduce DentBench, a\ncomprehensive benchmark covering eight dental subspecialties, more diseases,\nimaging modalities, and a wide geographical distribution. DentVFM shows\nimpressive generalist intelligence, demonstrating robust generalization to\ndiverse dental tasks, such as disease diagnosis, treatment analysis, biomarker\nidentification, and anatomical landmark detection and segmentation.\nExperimental results indicate DentVFM significantly outperforms supervised,\nself-supervised, and weakly supervised baselines, offering superior\ngeneralization, label efficiency, and scalability. Additionally, DentVFM\nenables cross-modality diagnostics, providing more reliable results than\nexperienced dentists in situations where conventional imaging is unavailable.\nDentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and\nlabel-efficient model to improve intelligent dental healthcare and address\ncritical gaps in global oral healthcare."}
{"id": "2510.14922", "categories": ["cs.AI", "cs.CL", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.14922", "abs": "https://arxiv.org/abs/2510.14922", "authors": ["Annisaa Fitri Nurfidausi", "Eleonora Mancini", "Paolo Torroni"], "title": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG", "comment": null, "summary": "Depression is a widespread mental health disorder, yet its automatic\ndetection remains challenging. Prior work has explored unimodal and multimodal\napproaches, with multimodal systems showing promise by leveraging complementary\nsignals. However, existing studies are limited in scope, lack systematic\ncomparisons of features, and suffer from inconsistent evaluation protocols. We\naddress these gaps by systematically exploring feature representations and\nmodelling strategies across EEG, together with speech and text. We evaluate\nhandcrafted features versus pre-trained embeddings, assess the effectiveness of\ndifferent neural encoders, compare unimodal, bimodal, and trimodal\nconfigurations, and analyse fusion strategies with attention to the role of\nEEG. Consistent subject-independent splits are applied to ensure robust,\nreproducible benchmarking. Our results show that (i) the combination of EEG,\nspeech and text modalities enhances multimodal detection, (ii) pretrained\nembeddings outperform handcrafted features, and (iii) carefully designed\ntrimodal models achieve state-of-the-art performance. Our work lays the\ngroundwork for future research in multimodal depression detection."}
{"id": "2510.14616", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14616", "abs": "https://arxiv.org/abs/2510.14616", "authors": ["Shuangshuang Ying", "Yunwen Li", "Xingwei Qu", "Xin Li", "Sheng Jin", "Minghao Liu", "Zhoufutu Wen", "Xeron Du", "Tianyu Zheng", "Yichi Zhang", "Letian Ni", "Yuyang Cheng", "Qiguang Chen", "Jingzhe Ding", "Shengda Long", "Wangchunshu Zhou", "Jiazhan Feng", "Wanjun Zhong", "Libo Qin", "Ge Zhang", "Wenhao Huang", "Wanxiang Che", "Chenghua Lin"], "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures", "comment": null, "summary": "Current preference learning methods achieve high accuracy on standard\nbenchmarks but exhibit significant performance degradation when objective\nquality signals are removed. We introduce WritingPreferenceBench, a dataset of\n1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8\ncreative writing genres, where responses are matched for objective correctness,\nfactual accuracy, and length. On this benchmark, sequence-based reward\nmodels--the standard architecture for RLHF--achieve only 52.7% mean accuracy,\nwhile zero-shot language model judges perform at 53.9%. In contrast, generative\nreward models that produce explicit reasoning chains achieve 81.8% accuracy. We\nobserve high within-model variance across genres: individual models range from\n18.2% to 81.8% accuracy across different writing categories, with standard\ndeviations averaging 10.1%. This variance persists regardless of model scale,\nwith 27B parameter models showing no consistent improvement over 8B variants.\nOur results suggest that current RLHF methods primarily learn to detect\nobjective errors rather than capture subjective quality preferences (e.g.,\ncreativity, stylistic flair, and emotional resonance), and that successful\npreference modeling may require intermediate reasoning representations rather\nthan direct classification."}
{"id": "2510.14535", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14535", "abs": "https://arxiv.org/abs/2510.14535", "authors": ["Keima Abe", "Hayato Muraki", "Shuhei Tomoshige", "Kenichi Oishi", "Hitoshi Iyatomi"], "title": "Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval", "comment": "6 pages,3 figures, 3 tables. Accepted at 2025 IEEE International\n  Conference on Systems, Man, and Cybernetics (IEEE SMC 2025)", "summary": "Medical images like MR scans often show domain shifts across imaging sites\ndue to scanner and protocol differences, which degrade machine learning\nperformance in tasks such as disease classification. Domain harmonization is\nthus a critical research focus. Recent approaches encode brain images\n$\\boldsymbol{x}$ into a low-dimensional latent space $\\boldsymbol{z}$, then\ndisentangle it into $\\boldsymbol{z_u}$ (domain-invariant) and\n$\\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these\nmethods often lack interpretability$-$an essential requirement in medical\napplications$-$leaving practical issues unresolved. We propose\nPseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a\ngeneral framework for domain harmonization and interpretable representation\nlearning that preserves disease-relevant information in brain MR images.\nPL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract\n$\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, a decoder to reconstruct the image\n$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the\nencoder and domain predictor, the model learns to reconstruct the input image\n$\\boldsymbol{x}$ by summing reconstructions from $\\boldsymbol{z_u}$ and\n$\\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared\nto prior methods, PL-SE-ADA achieves equal or better performance in image\nreconstruction, disease classification, and domain recognition. It also enables\nvisualization of both domain-independent brain features and domain-specific\ncomponents, offering high interpretability across the entire framework."}
{"id": "2510.13827", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13827", "abs": "https://arxiv.org/abs/2510.13827", "authors": ["Ashish Kattamuri", "Ishita Prasad", "Meetu Malhotra", "Arpita Vats", "Rahul Raja", "Albert Lie"], "title": "Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL", "comment": "20th International Workshop on Semantic and Social Media Adaptation &\n  Personalization", "summary": "Current Text-to-SQL methods are evaluated and only focused on executable\nqueries, overlooking the semantic alignment challenge -- both in terms of the\nsemantic meaning of the query and the correctness of the execution results.\nEven execution accuracy itself shows significant drops when moving from English\nto other languages, with an average decline of 6 percentage points across\nnon-English languages. We address these challenges by presenting a new\nframework that combines Group Relative Policy Optimization (GRPO) within a\nmultilingual contrastive reward signal to enhance both task efficiency and\nsemantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method\nteaches models to obtain better correspondence between SQL generation and user\nintent by combining a reward signal based on semantic similarity. On the\nseven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO\nimproved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and\nsemantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive\nreward signal in the GRPO framework further improved the average semantic\naccuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our\nexperiments showcase that a smaller, parameter-efficient 3B LLaMA model\nfine-tuned with our contrastive reward signal outperforms a much larger\nzero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from\n81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly\nmatches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using\njust 3,000 reinforcement learning training examples. These results demonstrate\nhow we can improve the performance of Text-to-SQL systems with contrastive\nrewards for directed semantic alignment, without requiring large-scale training\ndatasets."}
{"id": "2510.14738", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14738", "abs": "https://arxiv.org/abs/2510.14738", "authors": ["Mengzhao Jia", "Zhihan Zhang", "Ignacio Cases", "Zheyuan Liu", "Meng Jiang", "Peng Qi"], "title": "AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning", "comment": null, "summary": "Multimodal large language models (MLLMs) have rapidly advanced from\nperception tasks to complex multi-step reasoning, yet reinforcement learning\nwith verifiable rewards (RLVR) often leads to spurious reasoning since only the\nfinal-answer correctness is rewarded. To address this limitation, we propose\nAutoRubric-R1V, a framework that integrates RLVR with process-level supervision\nthrough automatically collected rubric-based generative rewards. Our key\ninnovation lies in a scalable self-aggregation method that distills consistent\nreasoning checkpoints from successful trajectories, enabling problem-specific\nrubric construction without human annotation or stronger teacher models. By\njointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves\nstate-of-the-art performance on six multimodal reasoning benchmarks and\nsubstantially improves reasoning faithfulness in dedicated evaluations."}
{"id": "2510.14543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14543", "abs": "https://arxiv.org/abs/2510.14543", "authors": ["Ziqi Jiang", "Yanghao Wang", "Long Chen"], "title": "Exploring Cross-Modal Flows for Few-Shot Learning", "comment": "13 pages, 6 figures", "summary": "Aligning features from different modalities, is one of the most fundamental\nchallenges for cross-modal tasks. Although pre-trained vision-language models\ncan achieve a general alignment between image and text, they often require\nparameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT\nmethods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively\nfine-tune a subset of parameters, which can slightly adjust either visual or\ntextual features, and avoid overfitting. In this paper, we are the first to\nhighlight that all existing PEFT methods perform one-step adjustment. It is\ninsufficient for complex (or difficult) datasets, where features of different\nmodalities are highly entangled. To this end, we propose the first\nmodel-agnostic multi-step adjustment approach by learning a cross-modal\nvelocity field: Flow Matching Alignment (FMA). Specifically, to ensure the\ncorrespondence between categories during training, we first utilize a fixed\ncoupling strategy. Then, we propose a noise augmentation strategy to alleviate\nthe data scarcity issue. Finally, we design an early-stopping solver, which\nterminates the transformation process earlier, improving both efficiency and\naccuracy. Compared with one-step PEFT methods, FMA has the multi-step\nrectification ability to achieve more precise and robust alignment. Extensive\nresults have demonstrated that FMA can consistently yield significant\nperformance gains across various benchmarks and backbones, particularly on\nchallenging datasets."}
{"id": "2510.13856", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13856", "abs": "https://arxiv.org/abs/2510.13856", "authors": ["A H M Rezaul Karim", "Ozlem Uzuner"], "title": "Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA", "comment": null, "summary": "Medical Visual Question Answering (MedVQA) enables natural language queries\nover medical images to support clinical decision-making and patient care. The\nMEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to\ngenerate free-text responses and structured wound attributes from images and\npatient queries. We present the MasonNLP system, which employs a\ngeneral-domain, instruction-tuned large language model with a\nretrieval-augmented generation (RAG) framework that incorporates textual and\nvisual examples from in-domain data. This approach grounds outputs in\nclinically relevant exemplars, improving reasoning, schema adherence, and\nresponse quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our\nbest-performing system ranked 3rd among 19 teams and 51 submissions with an\naverage score of 41.37%, demonstrating that lightweight RAG with\ngeneral-purpose LLMs -- a minimal inference-time layer that adds a few relevant\nexemplars via simple indexing and fusion, with no extra training or complex\nre-ranking -- provides a simple and effective baseline for multimodal clinical\nNLP tasks."}
{"id": "2510.14824", "categories": ["cs.CL", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14824", "abs": "https://arxiv.org/abs/2510.14824", "authors": ["Ziqi Dai", "Xin Zhang", "Mingxin Li", "Yanzhao Zhang", "Dingkun Long", "Pengjun Xie", "Meishan Zhang", "Wenjie Li", "Min Zhang"], "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking", "comment": null, "summary": "In information retrieval, training reranking models mainly focuses on two\ntypes of objectives: metric learning (e.g. contrastive loss to increase the\npredicted scores on relevant query-document pairs) and classification (binary\nlabel prediction of relevance vs. irrelevance). For BERT-style encoders,\nvarious studies have shown that contrastive learning (CL) can be more effective\nthan discriminative (classification) learning. However, for large language\nmodels (LLMs), classification via supervised fine-tuning (SFT), which predicts\n''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears\nmore promising as it aligns well with the generative nature of LLMs. This\ndivergence raises a central question: which objective is intrinsically better\nsuited to LLM-based reranking, and what mechanism underlies the difference? In\nthis work, we conduct a comprehensive comparison and analysis between CL and\nSFT for reranking, taking the universal multimodal retrieval (UMR) as the\nexperimental playground. We first decompose the objectives into two components:\nweight, which controls the magnitude of those updates, and direction, which\nguides the model updates, then present a unified framework for understanding\ntheir interactions. Through probing experiments, we find that SFT provides a\nsubstantially stronger weighting scheme than CL, whereas the preferred scoring\ndirection shows no clear winner. Taken together, these results point to a\nconsistent advantage of SFT over CL for LLM reranking. To further validate our\nfindings, we conduct large-scale training with SFT and present new\nstate-of-the-art rerankers on the MRB benchmark. We also provide ablations on\nSFT settings and expect our findings to benefit future research and\napplications in this area."}
{"id": "2510.14553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14553", "abs": "https://arxiv.org/abs/2510.14553", "authors": ["Song Tang", "Peihao Gong", "Kunyu Li", "Kai Guo", "Boyu Wang", "Mao Ye", "Jianwei Zhang", "Xiatian Zhu"], "title": "Consistent text-to-image generation via scene de-contextualization", "comment": null, "summary": "Consistent text-to-image (T2I) generation seeks to produce\nidentity-preserving images of the same subject across diverse scenes, yet it\noften fails due to a phenomenon called identity (ID) shift. Previous methods\nhave tackled this issue, but typically rely on the unrealistic assumption of\nknowing all target scenes in advance. This paper reveals that a key source of\nID shift is the native correlation between subject and scene context, called\nscene contextualization, which arises naturally as T2I models fit the training\ndistribution of vast natural images. We formally prove the near-universality of\nthis scene-ID correlation and derive theoretical bounds on its strength. On\nthis basis, we propose a novel, efficient, training-free prompt embedding\nediting approach, called Scene De-Contextualization (SDeC), that imposes an\ninversion process of T2I's built-in scene contextualization. Specifically, it\nidentifies and suppresses the latent scene-ID correlation within the ID\nprompt's embedding by quantifying the SVD directional stability to adaptively\nre-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene\nuse (one scene per prompt) without requiring prior access to all target scenes.\nThis makes it a highly flexible and general solution well-suited to real-world\napplications where such prior knowledge is often unavailable or varies over\ntime. Experiments demonstrate that SDeC significantly enhances identity\npreservation while maintaining scene diversity."}
{"id": "2510.13862", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.13862", "abs": "https://arxiv.org/abs/2510.13862", "authors": ["Chenyu Zhang", "Sharifa Alghowinem", "Cynthia Breazeal"], "title": "Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues", "comment": "4 pages, 3 figures. Published in the 11th International Conference on\n  Affective Computing and Intelligent Interaction (ACII 2025), Late-Breaking\n  Results Track", "summary": "While recent studies have examined the leaning impact of large language model\n(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring\nremain insufficiently understood. This work introduces the first ensemble-LLM\nframework for large-scale affect sensing in tutoring dialogues, advancing the\nconversation on responsible pathways for integrating generative AI into\neducation by attending to learners' evolving affective states. To achieve this,\nwe analyzed two semesters' worth of 16,986 conversational turns exchanged\nbetween PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across\nthree U.S. institutions. To investigate learners' emotional experiences, we\ngenerate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,\nClaude), including scalar ratings of valence, arousal, and\nlearning-helpfulness, along with free-text emotion labels. These estimates are\nfused through rank-weighted intra-model pooling and plurality consensus across\nmodels to produce robust emotion profiles. Our analysis shows that during\ninteraction with the AI tutor, students typically report mildly positive affect\nand moderate arousal. Yet learning is not uniformly smooth: confusion and\ncuriosity are frequent companions to problem solving, and frustration, while\nless common, still surfaces in ways that can derail progress. Emotional states\nare short-lived--positive moments last slightly longer than neutral or negative\nones, but they are fragile and easily disrupted. Encouragingly, negative\nemotions often resolve quickly, sometimes rebounding directly into positive\nstates. Neutral moments frequently act as turning points, more often steering\nstudents upward than downward, suggesting opportunities for tutors to intervene\nat precisely these junctures."}
{"id": "2510.14937", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14937", "abs": "https://arxiv.org/abs/2510.14937", "authors": ["Jianfeng Zhu", "Julina Maharjan", "Xinyu Li", "Karin G. Coifman", "Ruoming Jin"], "title": "AI-Powered Early Diagnosis of Mental Health Disorders from Real-World Clinical Conversations", "comment": "7 pages 1 figure", "summary": "Mental health disorders remain among the leading cause of disability\nworldwide, yet conditions such as depression, anxiety, and Post-Traumatic\nStress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to\nsubjective assessments, limited clinical resources, and stigma and low\nawareness. In primary care settings, studies show that providers misidentify\ndepression or anxiety in over 60% of cases, highlighting the urgent need for\nscalable, accessible, and context-aware diagnostic tools that can support early\ndetection and intervention. In this study, we evaluate the effectiveness of\nmachine learning models for mental health screening using a unique dataset of\n553 real-world, semistructured interviews, each paried with ground-truth\ndiagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We\nbenchmark multiple model classes, including zero-shot prompting with GPT-4.1\nMini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank\nAdaptation (LoRA). Our models achieve over 80% accuracy across diagnostic\ncategories, with especially strongperformance on PTSD (up to 89% accuracy and\n98% recall). We also find that using shorter context, focused context segments\nimproves recall, suggesting that focused narrative cues enhance detection\nsensitivity. LoRA fine-tuning proves both efficient and effective, with\nlower-rank configurations (e.g., rank 8 and 16) maintaining competitive\nperformance across evaluation metrics. Our results demonstrate that LLM-based\nmodels can offer substantial improvements over traditional self-report\nscreening tools, providing a path toward low-barrier, AI-powerd early\ndiagnosis. This work lays the groundwork for integrating machine learning into\nreal-world clinical workflows, particularly in low-resource or high-stigma\nenvironments where access to timely mental health care is most limited."}
{"id": "2510.14583", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14583", "abs": "https://arxiv.org/abs/2510.14583", "authors": ["Matan Rusanovsky", "Shimon Malnick", "Shai Avidan"], "title": "Talking Points: Describing and Localizing Pixels", "comment": null, "summary": "Vision-language models have achieved remarkable success in cross-modal\nunderstanding. Yet, these models remain limited to object-level or region-level\ngrounding, lacking the capability for pixel-precise keypoint comprehension\nthrough natural language. We introduce a novel framework for pixel level\ngrounding. The framework consists of two complementary components: a Point\nDescriptor that generates rich, contextual descriptions of individual\nkeypoints, and a Point Localizer that regresses precise pixel coordinates from\nthese descriptions. Unlike prior work that relies on templated prompts or\nkeypoint names, our approach produces free-form, coarse-to-fine descriptions\nthat situate keypoints within their visual context. Since there is no available\ndataset to train such a system, we introduce LlamaPointInPart, a carefully\ncurated dataset of 20K+ image-keypoint-description triplets synthesized from\nmultiple vision-language models, capturing multi-scale information from\nscene-level context to visual features around the keypoint. For cross-category\ngeneralization, we optimize the Point Descriptor on AP-10K via GRPO, using the\nfrozen Point Localizer as a reward model to produce descriptions that maximize\nlocalization accuracy. To evaluate our results we establish a new evaluation\nprotocol. Instead of comparing the text description produced by our method to\nthe ground truth, we use the localizer to determine how close is the predicted\npoint generated to the ground truth point. Experiments demonstrate superior\nperformance compared to baseline models on LlamaPointInPart.The bidirectional\nnature of our framework should enable future applications in both\nkeypoint-guided image understanding and language-guided precise localization.\nOur code and dataset are publicly available at\nhttps://github.com/matanr/Talking_Points."}
{"id": "2510.13885", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13885", "abs": "https://arxiv.org/abs/2510.13885", "authors": ["Ariel Kamen"], "title": "Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization", "comment": "10 pages, 4 figures,", "summary": "This study presents a comparative evaluation of ten state-of-the-art large\nlanguage models (LLMs) applied to unstructured text categorization using the\nInteractive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis\nemployed a uniform dataset of 8,660 human-annotated samples and identical\nzero-shot prompts to ensure methodological consistency across all models.\nEvaluation metrics included four classic measures - accuracy, precision,\nrecall, and F1-score - and three LLM-specific indicators: hallucination ratio,\ninflation ratio, and categorization cost.\n  Results show that, despite their rapid advancement, contemporary LLMs achieve\nonly moderate classic performance, with average scores of 34% accuracy, 42%\nprecision, 45% recall, and 41% F1-score. Hallucination and inflation ratios\nreveal that models frequently overproduce categories relative to human\nannotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B\noffered the most favorable cost-to-performance balance, while GPT 120B\ndemonstrated the lowest hallucination ratio. The findings suggest that scaling\nand architectural improvements alone do not ensure better categorization\naccuracy, as the task requires compressing rich unstructured text into a\nlimited taxonomy - a process that challenges current model architectures.\n  To address these limitations, a separate ensemble-based approach was\ndeveloped and tested. The ensemble method, in which multiple LLMs act as\nindependent experts, substantially improved accuracy, reduced inflation, and\ncompletely eliminated hallucinations. These results indicate that coordinated\norchestration of models - rather than sheer scale - may represent the most\neffective path toward achieving or surpassing human-expert performance in\nlarge-scale text categorization."}
{"id": "2510.14949", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14949", "abs": "https://arxiv.org/abs/2510.14949", "authors": ["Yu Zhou", "Sohyun An", "Haikang Deng", "Da Yin", "Clark Peng", "Cho-Jui Hsieh", "Kai-Wei Chang", "Nanyun Peng"], "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation", "comment": null, "summary": "Contact languages like English exhibit rich regional variations in the form\nof dialects, which are often used by dialect speakers interacting with\ngenerative models. However, can multimodal generative models effectively\nproduce content given dialectal textual input? In this work, we study this\nquestion by constructing a new large-scale benchmark spanning six common\nEnglish dialects. We work with dialect speakers to collect and verify over 4200\nunique prompts and evaluate on 17 image and video generative models. Our\nautomatic and human evaluation results show that current state-of-the-art\nmultimodal generative models exhibit 32.26% to 48.17% performance degradation\nwhen a single dialect word is used in the prompt. Common mitigation methods\nsuch as fine-tuning and prompt rewriting can only improve dialect performance\nby small margins (< 7%), while potentially incurring significant performance\ndegradation in Standard American English (SAE). To this end, we design a\ngeneral encoder-based mitigation strategy for multimodal generative models. Our\nmethod teaches the model to recognize new dialect features while preserving SAE\nperformance. Experiments on models such as Stable Diffusion 1.5 show that our\nmethod is able to simultaneously raise performance on five dialects to be on\npar with SAE (+34.4%), while incurring near zero cost to SAE performance."}
{"id": "2510.14594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14594", "abs": "https://arxiv.org/abs/2510.14594", "authors": ["Hugo Markoff", "Jevgenijs Galaktionovs"], "title": "Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers", "comment": "Extended abstract. Submitted to AICC: Workshop on AI for Climate and\n  Conservation - EurIPS 2025 (non-archival)", "summary": "State-of-the-art animal classification models like SpeciesNet provide\npredictions across thousands of species but use conservative rollup strategies,\nresulting in many animals labeled at high taxonomic levels rather than species.\nWe present a hierarchical re-classification system for the Animal Detect\nplatform that combines SpeciesNet EfficientNetV2-M predictions with CLIP\nembeddings and metric learning to refine high-level taxonomic labels toward\nspecies-level identification. Our five-stage pipeline (high-confidence\nacceptance, bird override, centroid building, triplet-loss metric learning, and\nadaptive cosine-distance scoring) is evaluated on a segment of the LILA BC\nDesert Lion Conservation dataset (4,018 images, 15,031 detections). After\nrecovering 761 bird detections from \"blank\" and \"animal\" labels, we re-classify\n456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving\nspecies-level identification for 64.9 percent"}
{"id": "2510.13909", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13909", "abs": "https://arxiv.org/abs/2510.13909", "authors": ["Xingrui Zhuo", "Jiapu Wang", "Gongqing Wu", "Zhongyuan Wang", "Jichen Zhang", "Shirui Pan", "Xindong Wu"], "title": "Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning", "comment": null, "summary": "Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in\nopen-domain KGs containing unknown entities and relations, which poses a\nchallenge for KGR models in comprehending uncertain KG components. Existing\nstudies have proposed Knowledge Graph Foundation Models (KGFMs) that learn\nstructural invariances across KGs to handle this uncertainty. Recently, Large\nLanguage Models (LLMs) have demonstrated strong capabilities for open-domain\nknowledge reasoning. As a result, the latest research has focused on LLM-based\nKGFMs that integrate LLM knowledge with KG context for inductive KGR. However,\nthe intrinsic knowledge of LLMs may be overshadowed by sparse KG context,\nleading to LLM knowledge distortion, which can cause irreversible damage to\nmodel reasoning. Moreover, existing LLM-based KGR methods still struggle to\nfully constrain generative hallucinations in LLMs, severely limiting the\ncredibility of reasoning results. To address these limitations, we propose a\nKnowledge Reasoning Language Model (KRLM) that achieves unified coordination\nbetween LLM knowledge and KG context throughout the KGR process. Specifically,\nwe design a Knowledge Reasoning Language (KRL) instruction format and a KRL\ntokenizer to align LLM knowledge with KG representations. Then, we propose a\nKRL attention layer that coordinates intrinsic LLM knowledge with additional KG\ncontext through a dynamic knowledge memory mechanism. Finally, a\nstructure-aware next-entity predictor is proposed, which strictly constrains\nthe reasoning results within a trustworthy knowledge domain. Extensive\nexperimental results on 25 real-world inductive KGR datasets demonstrate the\nsignificant superiority of the proposed KRLM\\footnote{Our source codes are\navailable at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot\nreasoning and fine-tuning scenarios."}
{"id": "2510.13979", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13979", "abs": "https://arxiv.org/abs/2510.13979", "authors": ["Supriti Sinhamahapatra", "Jan Niehues"], "title": "Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks", "comment": null, "summary": "State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily\nrely on acoustic information while disregarding additional multi-modal context.\nHowever, visual information are essential in disambiguation and adaptation.\nWhile most work focus on speaker images to handle noise conditions, this work\nalso focuses on integrating presentation slides for the use cases of scientific\npresentation.\n  In a first step, we create a benchmark for multi-modal presentation including\nan automatic analysis of transcribing domain-specific terminology. Next, we\nexplore methods for augmenting speech models with multi-modal information. We\nmitigate the lack of datasets with accompanying slides by a suitable approach\nof data augmentation. Finally, we train a model using the augmented dataset,\nresulting in a relative reduction in word error rate of approximately 34%,\nacross all words and 35%, for domain-specific terms compared to the baseline\nmodel."}
{"id": "2510.14596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14596", "abs": "https://arxiv.org/abs/2510.14596", "authors": ["Hugo Markoff", "Jevgenijs Galaktionovs"], "title": "Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering", "comment": "Extended abstract. Submitted to AICC: Workshop on AI for Climate and\n  Conservation - EurIPS 2025 (non-archival)", "summary": "Camera traps generate millions of wildlife images, yet many datasets contain\nspecies that are absent from existing classifiers. This work evaluates\nzero-shot approaches for organizing unlabeled wildlife imagery using\nself-supervised vision transformers, developed and tested within the Animal\nDetect platform for camera trap analysis. We compare unsupervised clustering\nmethods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)\ncombined with dimensionality reduction techniques (PCA, UMAP), and we\ndemonstrate continuous 1D similarity ordering via t-SNE projection. On a\n5-species test set with ground truth labels used only for evaluation, DINOv2\nwith UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D\nsorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent\nfor fish across 1,500 images. Based on these findings, we deployed continuous\nsimilarity ordering in production, enabling rapid exploratory analysis and\naccelerating manual annotation workflows for biodiversity monitoring."}
{"id": "2510.13993", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13993", "abs": "https://arxiv.org/abs/2510.13993", "authors": ["Jia Yun Chua", "Argyrios Zolotas", "Miguel Arana-Catania"], "title": "Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models", "comment": "11 pages, 7 figures, 8 tables. To be published in Applied AI Letters", "summary": "Remote sensing has become a vital tool across sectors such as urban planning,\nenvironmental monitoring, and disaster response. While the volume of data\ngenerated has increased significantly, traditional vision models are often\nconstrained by the requirement for extensive domain-specific labelled data and\ntheir limited ability to understand the context within complex environments.\nVision Language Models offer a complementary approach by integrating visual and\ntextual data; however, their application to remote sensing remains\nunderexplored, particularly given their generalist nature. This work\ninvestigates the combination of vision models and VLMs to enhance image\nanalysis in remote sensing, with a focus on aircraft detection and scene\nunderstanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and\nGemini aims to achieve more accurate and contextually aware image\ninterpretation. Performance is evaluated on both labelled and unlabelled remote\nsensing data, as well as degraded image scenarios which are crucial for remote\nsensing. The findings show an average MAE improvement of 48.46% across models\nin the accuracy of aircraft detection and counting, especially in challenging\nconditions, in both raw and degraded scenarios. A 6.17% improvement in\nCLIPScore for comprehensive understanding of remote sensing images is obtained.\nThe proposed approach combining traditional vision models and VLMs paves the\nway for more advanced and efficient remote sensing image analysis, especially\nin few-shot learning scenarios."}
{"id": "2510.14203", "categories": ["cs.CV", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.14203", "abs": "https://arxiv.org/abs/2510.14203", "authors": ["Ryo Masumura", "Shota Orihashi", "Mana Ihori", "Tomohiro Tanaka", "Naoki Makishima", "Taiga Yamane", "Naotaka Kawata", "Satoshi Suzuki", "Taichi Katayama"], "title": "Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition", "comment": "Accepted at APSIPA ASC 2025", "summary": "This paper proposes a joint modeling method of the Big Five, which has long\nbeen studied, and HEXACO, which has recently attracted attention in psychology,\nfor automatically recognizing apparent personality traits from multimodal human\nbehavior. Most previous studies have used the Big Five for multimodal apparent\npersonality-trait recognition. However, no study has focused on apparent HEXACO\nwhich can evaluate an Honesty-Humility trait related to displaced aggression\nand vengefulness, social-dominance orientation, etc. In addition, the\nrelationships between the Big Five and HEXACO when modeled by machine learning\nhave not been clarified. We expect awareness of multimodal human behavior to\nimprove by considering these relationships. The key advance of our proposed\nmethod is to optimize jointly recognizing the Big Five and HEXACO. Experiments\nusing a self-introduction video dataset demonstrate that the proposed method\ncan effectively recognize the Big Five and HEXACO."}
{"id": "2510.14605", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14605", "abs": "https://arxiv.org/abs/2510.14605", "authors": ["Yuyang Hong", "Jiaqi Gu", "Qi Yang", "Lubin Fan", "Yue Wu", "Ying Wang", "Kun Ding", "Shiming Xiang", "Jieping Ye"], "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering", "comment": "Accepted by NeurIPS 2025", "summary": "Knowledge-based visual question answering (KB-VQA) requires visual language\nmodels (VLMs) to integrate visual understanding with external knowledge\nretrieval. Although retrieval-augmented generation (RAG) achieves significant\nadvances in this task by combining knowledge-base querying, it still struggles\nwith the quality of multimodal queries and the relevance of retrieved results.\nTo overcome these challenges, we propose a novel three-stage method, termed\nWiki-PRF, including Processing, Retrieval and Filtering stages. The processing\nstage dynamically invokes visual tools to extract precise multimodal\ninformation for retrieval. The retrieval stage integrates visual and text\nfeatures to achieve multimodal knowledge retrieval. The filtering stage\nperforms relevance filtering and concentration on retrieval results. To this\nend, we introduce a visual language model trained with answer accuracy and\nformat consistency as reward signals via a reinforcement learning manner. This\nenhances the model's reasoning, tool invocation for accurate queries, and\nfiltering of irrelevant content. Experiments on benchmark datasets (E-VQA and\nInfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,\nachieving state-of-the-art performance. Code is available at\nhttps://github.com/cqu-student/Wiki-PRF"}
{"id": "2510.14304", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14304", "abs": "https://arxiv.org/abs/2510.14304", "authors": ["Kyungryul Back", "Seongbeom Park", "Milim Kim", "Mincheol Kwon", "SangHyeok Lee", "Hyunyoung Lee", "Junhee Cho", "Seunghyun Park", "Jinkyu Kim"], "title": "Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding", "comment": "EMNLP 2025 Findings; Project: https://github.com/KR-0822/TCD", "summary": "Large Vision-Language Models (LVLMs) have recently shown promising results on\nvarious multimodal tasks, even achieving human-comparable performance in\ncertain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often\nrely heavily on a single modality or memorize training data without properly\ngrounding their outputs. To address this, we propose a training-free, tri-layer\ncontrastive decoding with watermarking, which proceeds in three steps: (1)\nselect a mature layer and an amateur layer among the decoding layers, (2)\nidentify a pivot layer using a watermark-related question to assess whether the\nlayer is visually well-grounded, and (3) apply tri-layer contrastive decoding\nto generate the final output. Experiments on public benchmarks such as POPE,\nMME and AMBER demonstrate that our method achieves state-of-the-art performance\nin reducing hallucinations in LVLMs and generates more visually grounded\nresponses."}
{"id": "2510.14583", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14583", "abs": "https://arxiv.org/abs/2510.14583", "authors": ["Matan Rusanovsky", "Shimon Malnick", "Shai Avidan"], "title": "Talking Points: Describing and Localizing Pixels", "comment": null, "summary": "Vision-language models have achieved remarkable success in cross-modal\nunderstanding. Yet, these models remain limited to object-level or region-level\ngrounding, lacking the capability for pixel-precise keypoint comprehension\nthrough natural language. We introduce a novel framework for pixel level\ngrounding. The framework consists of two complementary components: a Point\nDescriptor that generates rich, contextual descriptions of individual\nkeypoints, and a Point Localizer that regresses precise pixel coordinates from\nthese descriptions. Unlike prior work that relies on templated prompts or\nkeypoint names, our approach produces free-form, coarse-to-fine descriptions\nthat situate keypoints within their visual context. Since there is no available\ndataset to train such a system, we introduce LlamaPointInPart, a carefully\ncurated dataset of 20K+ image-keypoint-description triplets synthesized from\nmultiple vision-language models, capturing multi-scale information from\nscene-level context to visual features around the keypoint. For cross-category\ngeneralization, we optimize the Point Descriptor on AP-10K via GRPO, using the\nfrozen Point Localizer as a reward model to produce descriptions that maximize\nlocalization accuracy. To evaluate our results we establish a new evaluation\nprotocol. Instead of comparing the text description produced by our method to\nthe ground truth, we use the localizer to determine how close is the predicted\npoint generated to the ground truth point. Experiments demonstrate superior\nperformance compared to baseline models on LlamaPointInPart.The bidirectional\nnature of our framework should enable future applications in both\nkeypoint-guided image understanding and language-guided precise localization.\nOur code and dataset are publicly available at\nhttps://github.com/matanr/Talking_Points."}
{"id": "2510.14617", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14617", "abs": "https://arxiv.org/abs/2510.14617", "authors": ["Ning Ding", "Keisuke Fujii", "Toru Tamaki"], "title": "Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding", "comment": "9 pages, 3 figures. Accepted to ACM MMSports 2025", "summary": "Tactical understanding in badminton involves interpreting not only individual\nactions but also how tactics are dynamically executed over time. In this paper,\nwe propose \\textbf{Shot2Tactic-Caption}, a novel framework for semantic and\ntemporal multi-scale video captioning in badminton, capable of generating\nshot-level captions that describe individual actions and tactic-level captions\nthat capture how these actions unfold over time within a tactical execution. We\nalso introduce the Shot2Tactic-Caption Dataset, the first badminton captioning\ndataset containing 5,494 shot captions and 544 tactic captions.\nShot2Tactic-Caption adopts a dual-branch design, with both branches including a\nvisual encoder, a spatio-temporal Transformer encoder, and a Transformer-based\ndecoder to generate shot and tactic captions. To support tactic captioning, we\nadditionally introduce a Tactic Unit Detector that identifies valid tactic\nunits, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic\ncaptioning, we further incorporate a shot-wise prompt-guided mechanism, where\nthe predicted tactic type and state are embedded as prompts and injected into\nthe decoder via cross-attention. The shot-wise prompt-guided mechanism enables\nour system not only to describe successfully executed tactics but also to\ncapture tactical executions that are temporarily interrupted and later resumed.\nExperimental results demonstrate the effectiveness of our framework in\ngenerating both shot and tactic captions. Ablation studies show that the\nResNet50-based spatio-temporal encoder outperforms other variants, and that\nshot-wise prompt structuring leads to more coherent and accurate tactic\ncaptioning."}
{"id": "2510.14307", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14307", "abs": "https://arxiv.org/abs/2510.14307", "authors": ["Sathyanarayanan Ramamoorthy", "Vishwa Shah", "Simran Khanuja", "Zaid Sheikh", "Shan Jie", "Ann Chia", "Shearman Chua", "Graham Neubig"], "title": "MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and Linking", "comment": null, "summary": "This paper introduces MERLIN, a novel testbed system for the task of\nMultilingual Multimodal Entity Linking. The created dataset includes BBC news\narticle titles, paired with corresponding images, in five languages: Hindi,\nJapanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity\nmentions linked to 2,500 unique Wikidata entities. We also include several\nbenchmarks using multilingual and multimodal entity linking methods exploring\ndifferent language models like LLaMa-2 and Aya-23. Our findings indicate that\nincorporating visual data improves the accuracy of entity linking, especially\nfor entities where the textual context is ambiguous or insufficient, and\nparticularly for models that do not have strong multilingual abilities. For the\nwork, the dataset, methods are available here at\nhttps://github.com/rsathya4802/merlin"}
{"id": "2510.14621", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14621", "abs": "https://arxiv.org/abs/2510.14621", "authors": ["Yuanyi Song", "Heyuan Huang", "Qiqiang Lin", "Yin Zhao", "Xiangmou Qu", "Jun Wang", "Xingyu Lou", "Weiwen Liu", "Zhuosheng Zhang", "Jun Wang", "Yong Yu", "Weinan Zhang", "Zhaoxiang Wang"], "title": "ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks", "comment": null, "summary": "The rapid advancement of multimodal large language models has enabled agents\nto operate mobile devices by directly interacting with graphical user\ninterfaces, opening new possibilities for mobile automation. However,\nreal-world mobile tasks are often complex and allow for multiple valid\nsolutions. This contradicts current mobile agent evaluation standards: offline\nstatic benchmarks can only validate a single predefined \"golden path\", while\nonline dynamic testing is constrained by the complexity and non-reproducibility\nof real devices, making both approaches inadequate for comprehensively\nassessing agent capabilities. To bridge the gap between offline and online\nevaluation and enhance testing stability, this paper introduces a novel\ngraph-structured benchmarking framework. By modeling the finite states observed\nduring real-device interactions, it achieves static simulation of dynamic\nbehaviors. Building on this, we develop ColorBench, a benchmark focused on\ncomplex long-horizon tasks. It supports evaluation of multiple valid solutions,\nsubtask completion rate statistics, and atomic-level capability analysis.\nColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average\nlength of over 13 steps. Each task includes at least two correct paths and\nseveral typical error paths, enabling quasi-dynamic interaction. By evaluating\nColorBench across various baselines, we discover limitations of existing models\nand propose improvement directions and feasible technical pathways to enhance\nagents' performance on complex, long-horizon problems based on experimental\nresults. Code and data are available at:\nhttps://github.com/MadeAgents/ColorBench."}
{"id": "2510.14624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14624", "abs": "https://arxiv.org/abs/2510.14624", "authors": ["Natan Bagrov", "Eugene Khvedchenia", "Borys Tymchenko", "Shay Aharon", "Lior Kadoch", "Tomer Keren", "Ofri Masad", "Yonatan Geifman", "Ran Zilberstein", "Tuomas Rintamaki", "Matthieu Le", "Andrew Tao"], "title": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference", "comment": null, "summary": "Vision-language models (VLMs) have recently expanded from static image\nunderstanding to video reasoning, but their scalability is fundamentally\nlimited by the quadratic cost of processing dense frame sequences. Long videos\noften exceed the token budget of modern language models, leading to severe\ncontext limitations and latency issues. We introduce Efficient Video Sampling\n(EVS), a simple, plug-and-play method for reducing token redundancy in videos\nby identifying and pruning temporally static patches -- spatial regions that\nremain unchanged across consecutive frames. EVS preserves positional identity,\nrequires no architectural changes or retraining. We show that EVS substantially\nreduces token count while maintaining semantic fidelity, enabling faster\ninference and longer input sequences. Applied at inference time, EVS reduces\nlarge language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal\naccuracy loss. When combined with an uptraining phase using stochastic pruning\nrates, EVS yields models that are robust to varying compression levels and\nretain full performance under aggressive pruning. Extensive experiments\ndemonstrate that EVS consistently improves efficiency-accuracy trade-offs,\nunlocking scalable video-language understanding without sacrificing quality."}
{"id": "2510.14605", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14605", "abs": "https://arxiv.org/abs/2510.14605", "authors": ["Yuyang Hong", "Jiaqi Gu", "Qi Yang", "Lubin Fan", "Yue Wu", "Ying Wang", "Kun Ding", "Shiming Xiang", "Jieping Ye"], "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering", "comment": "Accepted by NeurIPS 2025", "summary": "Knowledge-based visual question answering (KB-VQA) requires visual language\nmodels (VLMs) to integrate visual understanding with external knowledge\nretrieval. Although retrieval-augmented generation (RAG) achieves significant\nadvances in this task by combining knowledge-base querying, it still struggles\nwith the quality of multimodal queries and the relevance of retrieved results.\nTo overcome these challenges, we propose a novel three-stage method, termed\nWiki-PRF, including Processing, Retrieval and Filtering stages. The processing\nstage dynamically invokes visual tools to extract precise multimodal\ninformation for retrieval. The retrieval stage integrates visual and text\nfeatures to achieve multimodal knowledge retrieval. The filtering stage\nperforms relevance filtering and concentration on retrieval results. To this\nend, we introduce a visual language model trained with answer accuracy and\nformat consistency as reward signals via a reinforcement learning manner. This\nenhances the model's reasoning, tool invocation for accurate queries, and\nfiltering of irrelevant content. Experiments on benchmark datasets (E-VQA and\nInfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,\nachieving state-of-the-art performance. Code is available at\nhttps://github.com/cqu-student/Wiki-PRF"}
{"id": "2510.14866", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14866", "abs": "https://arxiv.org/abs/2510.14866", "authors": ["Hatef Otroshi Shahreza", "Sébastien Marcel"], "title": "Benchmarking Multimodal Large Language Models for Face Recognition", "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance\nacross diverse vision-and-language tasks. However, their potential in face\nrecognition remains underexplored. In particular, the performance of\nopen-source MLLMs needs to be evaluated and compared with existing face\nrecognition models on standard benchmarks with similar protocol. In this work,\nwe present a systematic benchmark of state-of-the-art MLLMs for face\nrecognition on several face recognition datasets, including LFW, CALFW, CPLFW,\nCFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich\nsemantic cues useful for face-related tasks, they lag behind specialized models\nin high-precision recognition scenarios in zero-shot applications. This\nbenchmark provides a foundation for advancing MLLM-based face recognition,\noffering insights for the design of next-generation models with higher accuracy\nand generalization. The source code of our benchmark is publicly available in\nthe project page."}
{"id": "2510.14630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14630", "abs": "https://arxiv.org/abs/2510.14630", "authors": ["Ming Gui", "Johannes Schusterbauer", "Timy Phan", "Felix Krause", "Josh Susskind", "Miguel Angel Bautista", "Björn Ommer"], "title": "Adapting Self-Supervised Representations as a Latent Space for Efficient Generation", "comment": "Code: https://github.com/CompVis/RepTok", "summary": "We introduce Representation Tokenizer (RepTok), a generative modeling\nframework that represents an image using a single continuous latent token\nobtained from self-supervised vision transformers. Building on a pre-trained\nSSL encoder, we fine-tune only the semantic token embedding and pair it with a\ngenerative decoder trained jointly using a standard flow matching objective.\nThis adaptation enriches the token with low-level, reconstruction-relevant\ndetails, enabling faithful image reconstruction. To preserve the favorable\ngeometry of the original SSL space, we add a cosine-similarity loss that\nregularizes the adapted token, ensuring the latent space remains smooth and\nsuitable for generation. Our single-token formulation resolves spatial\nredundancies of 2D latent spaces and significantly reduces training costs.\nDespite its simplicity and efficiency, RepTok achieves competitive results on\nclass-conditional ImageNet generation and naturally extends to text-to-image\nsynthesis, reaching competitive zero-shot performance on MS-COCO under\nextremely limited training budgets. Our findings highlight the potential of\nfine-tuned SSL representations as compact and effective latent spaces for\nefficient generative modeling."}
{"id": "2510.14616", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14616", "abs": "https://arxiv.org/abs/2510.14616", "authors": ["Shuangshuang Ying", "Yunwen Li", "Xingwei Qu", "Xin Li", "Sheng Jin", "Minghao Liu", "Zhoufutu Wen", "Xeron Du", "Tianyu Zheng", "Yichi Zhang", "Letian Ni", "Yuyang Cheng", "Qiguang Chen", "Jingzhe Ding", "Shengda Long", "Wangchunshu Zhou", "Jiazhan Feng", "Wanjun Zhong", "Libo Qin", "Ge Zhang", "Wenhao Huang", "Wanxiang Che", "Chenghua Lin"], "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures", "comment": null, "summary": "Current preference learning methods achieve high accuracy on standard\nbenchmarks but exhibit significant performance degradation when objective\nquality signals are removed. We introduce WritingPreferenceBench, a dataset of\n1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8\ncreative writing genres, where responses are matched for objective correctness,\nfactual accuracy, and length. On this benchmark, sequence-based reward\nmodels--the standard architecture for RLHF--achieve only 52.7% mean accuracy,\nwhile zero-shot language model judges perform at 53.9%. In contrast, generative\nreward models that produce explicit reasoning chains achieve 81.8% accuracy. We\nobserve high within-model variance across genres: individual models range from\n18.2% to 81.8% accuracy across different writing categories, with standard\ndeviations averaging 10.1%. This variance persists regardless of model scale,\nwith 27B parameter models showing no consistent improvement over 8B variants.\nOur results suggest that current RLHF methods primarily learn to detect\nobjective errors rather than capture subjective quality preferences (e.g.,\ncreativity, stylistic flair, and emotional resonance), and that successful\npreference modeling may require intermediate reasoning representations rather\nthan direct classification."}
{"id": "2510.14885", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14885", "abs": "https://arxiv.org/abs/2510.14885", "authors": ["Logan Lawrence", "Oindrila Saha", "Megan Wei", "Chen Sun", "Subhransu Maji", "Grant Van Horn"], "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction", "comment": "Accepted to WACV26. 12 pages, 8 tables, 5 figures", "summary": "Despite the renewed interest in zero-shot visual classification due to the\nrise of Multimodal Large Language Models (MLLMs), the problem of evaluating\nfree-form responses of auto-regressive models remains a persistent challenge.\nMost existing works focus on language-only tasks or don't consider Multiple\nChoice Questions (MCQs) beyond 5-way options, both of which are critical\ncapabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where\nchoice counts are in the hundreds to thousands and the choices are highly\nrelated. Furthermore, in this highly multi-way MCQ setting it is not clear how\nto extend LLM choice extraction to retrieval-based problems, where computing\nprobabilities over the choice set is computationally costly. In this work we\ninvestigate nlg2choice, a simple two-stage method which first asks the MLLM an\nopen-ended question for the task with minimal constraints, then uses text-only\nconstrained decoding to predict the most likely choice. In retrieval settings,\nwe compute the probability of the constrained response taking that choice with\nan early stopping method to significantly improve throughput. Our results show\nimprovement over a suite of seven fine-grained visual datasets when evaluating\nin terms of classification and retrieval, and show that this performance holds\nover the various ways that users of LLMs can implement tasks in natural\nlanguage."}
{"id": "2510.14648", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14648", "abs": "https://arxiv.org/abs/2510.14648", "authors": ["Xinyao Liao", "Xianfang Zeng", "Ziye Song", "Zhoujie Fu", "Gang Yu", "Guosheng Lin"], "title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "comment": null, "summary": "Despite the rapid progress of instruction-based image editing, its extension\nto video remains underexplored, primarily due to the prohibitive cost and\ncomplexity of constructing large-scale paired video editing datasets. To\naddress this challenge, we introduce a low-cost pretraining strategy for\ninstruction-based video editing that leverages in-context learning from\nunpaired video clips. We show that pretraining a foundation video generation\nmodel with this strategy endows it with general editing capabilities, such as\nadding, replacing, or deleting operations, according to input editing\ninstructions. The pretrained model can then be efficiently refined with a small\namount of high-quality paired editing data. Built upon HunyuanVideoT2V, our\nframework first pretrains on approximately 1M real video clips to learn basic\nediting concepts, and subsequently fine-tunes on fewer than 150k curated\nediting pairs to extend more editing tasks and improve the editing quality.\nComparative experiments show that our method surpasses existing\ninstruction-based video editing approaches in both instruction alignment and\nvisual fidelity, achieving a 12\\% improvement in editing instruction following\nand a 15\\% improvement in editing quality."}
{"id": "2510.14648", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14648", "abs": "https://arxiv.org/abs/2510.14648", "authors": ["Xinyao Liao", "Xianfang Zeng", "Ziye Song", "Zhoujie Fu", "Gang Yu", "Guosheng Lin"], "title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "comment": null, "summary": "Despite the rapid progress of instruction-based image editing, its extension\nto video remains underexplored, primarily due to the prohibitive cost and\ncomplexity of constructing large-scale paired video editing datasets. To\naddress this challenge, we introduce a low-cost pretraining strategy for\ninstruction-based video editing that leverages in-context learning from\nunpaired video clips. We show that pretraining a foundation video generation\nmodel with this strategy endows it with general editing capabilities, such as\nadding, replacing, or deleting operations, according to input editing\ninstructions. The pretrained model can then be efficiently refined with a small\namount of high-quality paired editing data. Built upon HunyuanVideoT2V, our\nframework first pretrains on approximately 1M real video clips to learn basic\nediting concepts, and subsequently fine-tunes on fewer than 150k curated\nediting pairs to extend more editing tasks and improve the editing quality.\nComparative experiments show that our method surpasses existing\ninstruction-based video editing approaches in both instruction alignment and\nvisual fidelity, achieving a 12\\% improvement in editing instruction following\nand a 15\\% improvement in editing quality."}
{"id": "2510.14922", "categories": ["cs.AI", "cs.CL", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.14922", "abs": "https://arxiv.org/abs/2510.14922", "authors": ["Annisaa Fitri Nurfidausi", "Eleonora Mancini", "Paolo Torroni"], "title": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG", "comment": null, "summary": "Depression is a widespread mental health disorder, yet its automatic\ndetection remains challenging. Prior work has explored unimodal and multimodal\napproaches, with multimodal systems showing promise by leveraging complementary\nsignals. However, existing studies are limited in scope, lack systematic\ncomparisons of features, and suffer from inconsistent evaluation protocols. We\naddress these gaps by systematically exploring feature representations and\nmodelling strategies across EEG, together with speech and text. We evaluate\nhandcrafted features versus pre-trained embeddings, assess the effectiveness of\ndifferent neural encoders, compare unimodal, bimodal, and trimodal\nconfigurations, and analyse fusion strategies with attention to the role of\nEEG. Consistent subject-independent splits are applied to ensure robust,\nreproducible benchmarking. Our results show that (i) the combination of EEG,\nspeech and text modalities enhances multimodal detection, (ii) pretrained\nembeddings outperform handcrafted features, and (iii) carefully designed\ntrimodal models achieve state-of-the-art performance. Our work lays the\ngroundwork for future research in multimodal depression detection."}
{"id": "2510.14668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14668", "abs": "https://arxiv.org/abs/2510.14668", "authors": ["Md. Abdur Rahman", "Mohaimenul Azam Khan Raiaan", "Sami Azam", "Asif Karim", "Jemima Beissbarth", "Amanda Leach"], "title": "WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging", "comment": null, "summary": "Knowledge distillation (KD) has traditionally relied on a static\nteacher-student framework, where a large, well-trained teacher transfers\nknowledge to a single student model. However, these approaches often suffer\nfrom knowledge degradation, inefficient supervision, and reliance on either a\nvery strong teacher model or large labeled datasets, which limits their\neffectiveness in real-world, limited-data scenarios. To address these, we\npresent the first-ever Weakly-supervised Chain-based KD network (WeCKD) that\nredefines knowledge transfer through a structured sequence of interconnected\nmodels. Unlike conventional KD, it forms a progressive distillation chain,\nwhere each model not only learns from its predecessor but also refines the\nknowledge before passing it forward. This structured knowledge transfer further\nenhances feature learning, reduces data dependency, and mitigates the\nlimitations of one-step KD. Each model in the distillation chain is trained on\nonly a fraction of the dataset and demonstrates that effective learning can be\nachieved with minimal supervision. Extensive evaluations across four otoscopic\nimaging datasets demonstrate that it not only matches but in many cases\nsurpasses the performance of existing supervised methods. Experimental results\non two other datasets further underscore its generalization across diverse\nmedical imaging modalities, including microscopic and magnetic resonance\nimaging. Furthermore, our evaluations resulted in cumulative accuracy gains of\nup to +23% over a single backbone trained on the same limited data, which\nhighlights its potential for real-world adoption."}
{"id": "2510.14866", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14866", "abs": "https://arxiv.org/abs/2510.14866", "authors": ["Hatef Otroshi Shahreza", "Sébastien Marcel"], "title": "Benchmarking Multimodal Large Language Models for Face Recognition", "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance\nacross diverse vision-and-language tasks. However, their potential in face\nrecognition remains underexplored. In particular, the performance of\nopen-source MLLMs needs to be evaluated and compared with existing face\nrecognition models on standard benchmarks with similar protocol. In this work,\nwe present a systematic benchmark of state-of-the-art MLLMs for face\nrecognition on several face recognition datasets, including LFW, CALFW, CPLFW,\nCFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich\nsemantic cues useful for face-related tasks, they lag behind specialized models\nin high-precision recognition scenarios in zero-shot applications. This\nbenchmark provides a foundation for advancing MLLM-based face recognition,\noffering insights for the design of next-generation models with higher accuracy\nand generalization. The source code of our benchmark is publicly available in\nthe project page."}
{"id": "2510.14958", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14958", "abs": "https://arxiv.org/abs/2510.14958", "authors": ["Weikang Shi", "Aldrich Yu", "Rongyao Fang", "Houxing Ren", "Ke Wang", "Aojun Zhou", "Changyao Tian", "Xinyu Fu", "Yuxuan Hu", "Zimu Lu", "Linjiang Huang", "Si Liu", "Rui Liu", "Hongsheng Li"], "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning", "comment": "Project Page: https://mathcanvas.github.io/", "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/"}
{"id": "2510.14672", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14672", "abs": "https://arxiv.org/abs/2510.14672", "authors": ["Jinglei Zhang", "Yuanfan Guo", "Rolandos Alexandros Potamias", "Jiankang Deng", "Hang Xu", "Chao Ma"], "title": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning", "comment": "Accepted by ICCV 2025", "summary": "In recent years, video question answering based on multimodal large language\nmodels (MLLM) has garnered considerable attention, due to the benefits from the\nsubstantial advancements in LLMs. However, these models have a notable\ndeficiency in the domains of video temporal grounding and reasoning, posing\nchallenges to the development of effective real-world video understanding\nsystems. Inspired by how humans use video players to interact with the progress\nbar for video comprehension, we introduce VTimeCoT, a simple yet effective\ntraining-free framework, designed for high-performance video grounding and\nreasoning. The proposed framework incorporates two novel visual tools of the\nprogress bar: a plug-and-play progress bar integration tool and a\nhigh-efficiency highlighting tool. In addition, to address the limitations of\nconventional text-based chain-of-thought (CoT) approaches, we introduce a\nvisuotemporal CoT process that integrates cross-modality reasoning across both\nvideo and text. Our approach demonstrates significant performance improvements\non both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and\nreasoning-based question answering. Finally, we showcase that the proposed\nframework achieves a compositional and interpretable reasoning process. Project\npage: https://vtimecot.github.io"}
{"id": "2510.14975", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14975", "abs": "https://arxiv.org/abs/2510.14975", "authors": ["Hengyuan Xu", "Wei Cheng", "Peng Xing", "Yixiao Fang", "Shuhan Wu", "Rui Wang", "Xianfang Zeng", "Daxin Jiang", "Gang Yu", "Xingjun Ma", "Yu-Gang Jiang"], "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation", "comment": "23 Pages; Project Page: https://doby-xu.github.io/WithAnyone/; Code:\n  https://github.com/Doby-Xu/WithAnyone", "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration."}
{"id": "2510.14737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14737", "abs": "https://arxiv.org/abs/2510.14737", "authors": ["Seulki Park", "Zilin Wang", "Stella X. Yu"], "title": "Free-Grained Hierarchical Recognition", "comment": "26 pages", "summary": "Hierarchical image classification predicts labels across a semantic taxonomy,\nbut existing methods typically assume complete, fine-grained annotations, an\nassumption rarely met in practice. Real-world supervision varies in\ngranularity, influenced by image quality, annotator expertise, and task\ndemands; a distant bird may be labeled Bird, while a close-up reveals Bald\neagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet\nand structured into cognitively inspired basic, subordinate, and fine-grained\nlevels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,\nmixed-granularity labels reflecting human annotation behavior. We propose\nfree-grain learning, with heterogeneous supervision across instances. We\ndevelop methods that enhance semantic guidance via pseudo-attributes from\nvision-language models and visual guidance via semi-supervised learning. These,\nalong with strong baselines, substantially improve performance under mixed\nsupervision. Together, our benchmark and methods advance hierarchical\nclassification under real-world constraints."}
{"id": "2510.14979", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14979", "abs": "https://arxiv.org/abs/2510.14979", "authors": ["Haiwen Diao", "Mingxuan Li", "Silei Wu", "Linjun Dai", "Xiaohua Wang", "Hanming Deng", "Lewei Lu", "Dahua Lin", "Ziwei Liu"], "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale", "comment": "21 pages, 7 figures", "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO."}
{"id": "2510.14792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14792", "abs": "https://arxiv.org/abs/2510.14792", "authors": ["Hojun Choi", "Youngsun Lim", "Jaeyo Shin", "Hyunjung Shim"], "title": "CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection", "comment": "28 pages, 13 Figures, 12 Tables", "summary": "Open-vocabulary object detection (OVD) seeks to recognize and localize object\ncategories beyond those seen during training. Recent approaches typically\nleverage vision-language models (VLMs) to generate pseudo-labels using\nimage-text alignment, allowing detectors to generalize to unseen classes\nwithout explicit supervision. However, these methods depend heavily on direct\nimage-text matching, neglecting the intermediate reasoning steps essential for\ninterpreting semantically complex scenes. This results in limited robustness\nwhen confronted with crowded or occluded visual contexts. In this paper, we\nintroduce CoT-PL, a new framework that employs structured visual\nchain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL\ndecomposes object understanding into three interpretable steps: (1) region\nperception even for unseen objects, (2) category recognition via zero-shot\nreasoning, and (3) background grounding to separate semantically complex\nobjects. Crucially, the third step naturally motivates our contrastive\nbackground learning (CBL) that uses the pre-computed background cues as\nnegatives to promote feature disentanglement between objects and background. In\nthis way, CoT reasoning and CBL form an integrated pipeline tailored to robust\npseudo-labeling in crowded or occluded scenes. Notably, in these two settings,\nour novel-class pseudo-label quality achieves relative improvements of 103.4%\nand 168.4% over the best prior, respectively. Our extensive experiments\ndemonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9\nmask AP on LVIS for novel classes, setting a new state of the art."}
{"id": "2510.14836", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14836", "abs": "https://arxiv.org/abs/2510.14836", "authors": ["Yixuan Li", "Yuhui Chen", "Mingcai Zhou", "Haoran Li"], "title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models", "comment": null, "summary": "Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)\nmodels to accomplish fine-grained manipulation tasks. However, existing\napproaches often lack the ability to understand and reason over the essential\n3D structures necessary for precise control. To address this limitation, we\npropose QDepth-VLA, a general framework that augments VLA models with an\nauxiliary depth prediction task. A dedicated depth expert is designed to\npredict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,\nenabling the model to learn depth-aware representations that capture critical\ngeometric cues. Experimental results on the simulation benchmarks and\nreal-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning\nand competitive performance on manipulation tasks."}
{"id": "2510.14862", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.14862", "abs": "https://arxiv.org/abs/2510.14862", "authors": ["Mihai-Cristian Pîrvu", "Marius Leordeanu"], "title": "Multi-modal video data-pipelines for machine learning with minimal human supervision", "comment": null, "summary": "The real-world is inherently multi-modal at its core. Our tools observe and\ntake snapshots of it, in digital form, such as videos or sounds, however much\nof it is lost. Similarly for actions and information passing between humans,\nlanguages are used as a written form of communication. Traditionally, Machine\nLearning models have been unimodal (i.e. rgb -> semantic or text ->\nsentiment_class). Recent trends go towards bi-modality, where images and text\nare learned together, however, in order to truly understand the world, we need\nto integrate all these independent modalities. In this work we try to combine\nas many visual modalities as we can using little to no human supervision. In\norder to do this, we use pre-trained experts and procedural combinations\nbetween them on top of raw videos using a fully autonomous data-pipeline, which\nwe also open-source. We then make use of PHG-MAE, a model specifically designed\nto leverage multi-modal data. We show that this model which was efficiently\ndistilled into a low-parameter (<1M) can have competitive results compared to\nmodels of ~300M parameters. We deploy this model and analyze the use-case of\nreal-time semantic segmentation from handheld devices or webcams on commodity\nhardware. Finally, we deploy other off-the-shelf models using the same\nframework, such as DPT for near real-time depth estimation."}
{"id": "2510.14866", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14866", "abs": "https://arxiv.org/abs/2510.14866", "authors": ["Hatef Otroshi Shahreza", "Sébastien Marcel"], "title": "Benchmarking Multimodal Large Language Models for Face Recognition", "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance\nacross diverse vision-and-language tasks. However, their potential in face\nrecognition remains underexplored. In particular, the performance of\nopen-source MLLMs needs to be evaluated and compared with existing face\nrecognition models on standard benchmarks with similar protocol. In this work,\nwe present a systematic benchmark of state-of-the-art MLLMs for face\nrecognition on several face recognition datasets, including LFW, CALFW, CPLFW,\nCFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich\nsemantic cues useful for face-related tasks, they lag behind specialized models\nin high-precision recognition scenarios in zero-shot applications. This\nbenchmark provides a foundation for advancing MLLM-based face recognition,\noffering insights for the design of next-generation models with higher accuracy\nand generalization. The source code of our benchmark is publicly available in\nthe project page."}
{"id": "2510.14882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14882", "abs": "https://arxiv.org/abs/2510.14882", "authors": ["Keli Liu", "Zhendong Wang", "Wengang Zhou", "Shaodong Xu", "Ruixiao Dong", "Houqiang Li"], "title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention", "comment": null, "summary": "Text-to-image generation with visual autoregressive~(VAR) models has recently\nachieved impressive advances in generation fidelity and inference efficiency.\nWhile control mechanisms have been explored for diffusion models, enabling\nprecise and flexible control within VAR paradigm remains underexplored. To\nbridge this critical gap, in this paper, we introduce ScaleWeaver, a novel\nframework designed to achieve high-fidelity, controllable generation upon\nadvanced VAR models through parameter-efficient fine-tuning. The core module in\nScaleWeaver is the improved MMDiT block with the proposed Reference Attention\nmodule, which efficiently and effectively incorporates conditional information.\nDifferent from MM Attention, the proposed Reference Attention module discards\nthe unnecessary attention from image$\\rightarrow$condition, reducing\ncomputational cost while stabilizing control injection. Besides, it\nstrategically emphasizes parameter reuse, leveraging the capability of the VAR\nbackbone itself with a few introduced parameters to process control\ninformation, and equipping a zero-initialized linear projection to ensure that\ncontrol signals are incorporated effectively without disrupting the generative\ncapability of the base model. Extensive experiments show that ScaleWeaver\ndelivers high-quality generation and precise control while attaining superior\nefficiency over diffusion-based methods, making ScaleWeaver a practical and\neffective solution for controllable text-to-image generation within the visual\nautoregressive paradigm. Code and models will be released."}
{"id": "2510.14885", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14885", "abs": "https://arxiv.org/abs/2510.14885", "authors": ["Logan Lawrence", "Oindrila Saha", "Megan Wei", "Chen Sun", "Subhransu Maji", "Grant Van Horn"], "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction", "comment": "Accepted to WACV26. 12 pages, 8 tables, 5 figures", "summary": "Despite the renewed interest in zero-shot visual classification due to the\nrise of Multimodal Large Language Models (MLLMs), the problem of evaluating\nfree-form responses of auto-regressive models remains a persistent challenge.\nMost existing works focus on language-only tasks or don't consider Multiple\nChoice Questions (MCQs) beyond 5-way options, both of which are critical\ncapabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where\nchoice counts are in the hundreds to thousands and the choices are highly\nrelated. Furthermore, in this highly multi-way MCQ setting it is not clear how\nto extend LLM choice extraction to retrieval-based problems, where computing\nprobabilities over the choice set is computationally costly. In this work we\ninvestigate nlg2choice, a simple two-stage method which first asks the MLLM an\nopen-ended question for the task with minimal constraints, then uses text-only\nconstrained decoding to predict the most likely choice. In retrieval settings,\nwe compute the probability of the constrained response taking that choice with\nan early stopping method to significantly improve throughput. Our results show\nimprovement over a suite of seven fine-grained visual datasets when evaluating\nin terms of classification and retrieval, and show that this performance holds\nover the various ways that users of LLMs can implement tasks in natural\nlanguage."}
{"id": "2510.14896", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14896", "abs": "https://arxiv.org/abs/2510.14896", "authors": ["Furkan Mumcu", "Michael J. Jones", "Anoop Cherian", "Yasin Yilmaz"], "title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection", "comment": null, "summary": "Existing semi-supervised video anomaly detection (VAD) methods often struggle\nwith detecting complex anomalies involving object interactions and generally\nlack explainability. To overcome these limitations, we propose a novel VAD\nframework leveraging Multimodal Large Language Models (MLLMs). Unlike previous\nMLLM-based approaches that make direct anomaly judgments at the frame level,\nour method focuses on extracting and interpreting object activity and\ninteractions over time. By querying an MLLM with visual inputs of object pairs\nat different moments, we generate textual descriptions of the activity and\ninteractions from nominal videos. These textual descriptions serve as a\nhigh-level representation of the activity and interactions of objects in a\nvideo. They are used to detect anomalies during test time by comparing them to\ntextual descriptions found in nominal training videos. Our approach inherently\nprovides explainability and can be combined with many traditional VAD methods\nto further enhance their interpretability. Extensive experiments on benchmark\ndatasets demonstrate that our method not only detects complex interaction-based\nanomalies effectively but also achieves state-of-the-art performance on\ndatasets without interaction anomalies."}
{"id": "2510.14945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14945", "abs": "https://arxiv.org/abs/2510.14945", "authors": ["JoungBin Lee", "Jaewoo Jung", "Jisang Han", "Takuya Narihira", "Kazumi Fukuda", "Junyoung Seo", "Sunghwan Hong", "Yuki Mitsufuji", "Seungryong Kim"], "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation", "comment": "Project page : https://cvlab-kaist.github.io/3DScenePrompt/", "summary": "We present 3DScenePrompt, a framework that generates the next video chunk\nfrom arbitrary-length input while enabling precise camera control and\npreserving scene consistency. Unlike methods conditioned on a single image or a\nshort clip, we employ dual spatio-temporal conditioning that reformulates\ncontext-view referencing across the input video. Our approach conditions on\nboth temporally adjacent frames for motion continuity and spatially adjacent\ncontent for scene consistency. However, when generating beyond temporal\nboundaries, directly using spatially adjacent frames would incorrectly preserve\ndynamic elements from the past. We address this by introducing a 3D scene\nmemory that represents exclusively the static geometry extracted from the\nentire input video. To construct this memory, we leverage dynamic SLAM with our\nnewly introduced dynamic masking strategy that explicitly separates static\nscene geometry from moving elements. The static scene representation can then\nbe projected to any target viewpoint, providing geometrically consistent warped\nviews that serve as strong 3D spatial prompts while allowing dynamic regions to\nevolve naturally from temporal context. This enables our model to maintain\nlong-range spatial coherence and precise camera control without sacrificing\ncomputational efficiency or motion realism. Extensive experiments demonstrate\nthat our framework significantly outperforms existing methods in scene\nconsistency, camera controllability, and generation quality. Project page :\nhttps://cvlab-kaist.github.io/3DScenePrompt/"}
{"id": "2510.14954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14954", "abs": "https://arxiv.org/abs/2510.14954", "authors": ["Zhe Li", "Weihao Yuan", "Weichao Shen", "Siyu Zhu", "Zilong Dong", "Chang Xu"], "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression", "comment": null, "summary": "Whole-body multi-modal human motion generation poses two primary challenges:\ncreating an effective motion generation mechanism and integrating various\nmodalities, such as text, speech, and music, into a cohesive framework. Unlike\nprevious methods that usually employ discrete masked modeling or autoregressive\nmodeling, we develop a continuous masked autoregressive motion transformer,\nwhere a causal attention is performed considering the sequential nature within\nthe human motion. Within this transformer, we introduce a gated linear\nattention and an RMSNorm module, which drive the transformer to pay attention\nto the key actions and suppress the instability caused by either the abnormal\nmovements or the heterogeneous distributions within multi-modalities. To\nfurther enhance both the motion generation and the multimodal generalization,\nwe employ the DiT structure to diffuse the conditions from the transformer\ntowards the targets. To fuse different modalities, AdaLN and cross-attention\nare leveraged to inject the text, speech, and music signals. Experimental\nresults demonstrate that our framework outperforms previous methods across all\nmodalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public."}
{"id": "2510.14958", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14958", "abs": "https://arxiv.org/abs/2510.14958", "authors": ["Weikang Shi", "Aldrich Yu", "Rongyao Fang", "Houxing Ren", "Ke Wang", "Aojun Zhou", "Changyao Tian", "Xinyu Fu", "Yuxuan Hu", "Zimu Lu", "Linjiang Huang", "Si Liu", "Rui Liu", "Hongsheng Li"], "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning", "comment": "Project Page: https://mathcanvas.github.io/", "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/"}
{"id": "2510.14965", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14965", "abs": "https://arxiv.org/abs/2510.14965", "authors": ["Miao Hu", "Zhiwei Huang", "Tai Wang", "Jiangmiao Pang", "Dahua Lin", "Nanning Zheng", "Runsen Xu"], "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes", "comment": "30 pages", "summary": "Real-world robots localize objects from natural-language instructions while\nscenes around them keep changing. Yet most of the existing 3D visual grounding\n(3DVG) method still assumes a reconstructed and up-to-date point cloud, an\nassumption that forces costly re-scans and hinders deployment. We argue that\n3DVG should be formulated as an active, memory-driven problem, and we introduce\nChangingGrounding, the first benchmark that explicitly measures how well an\nagent can exploit past observations, explore only where needed, and still\ndeliver precise 3D boxes in changing scenes. To set a strong reference point,\nwe also propose Mem-ChangingGrounder, a zero-shot method for this task that\nmarries cross-modal retrieval with lightweight multi-view fusion: it identifies\nthe object type implied by the query, retrieves relevant memories to guide\nactions, then explores the target efficiently in the scene, falls back when\nprevious operations are invalid, performs multi-view scanning of the target,\nand projects the fused evidence from multi-view scans to get accurate object\nbounding boxes. We evaluate different baselines on ChangingGrounding, and our\nMem-ChangingGrounder achieves the highest localization accuracy while greatly\nreducing exploration cost. We hope this benchmark and method catalyze a shift\ntoward practical, memory-centric 3DVG research for real-world applications.\nProject page: https://hm123450.github.io/CGB/ ."}
{"id": "2510.14975", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14975", "abs": "https://arxiv.org/abs/2510.14975", "authors": ["Hengyuan Xu", "Wei Cheng", "Peng Xing", "Yixiao Fang", "Shuhan Wu", "Rui Wang", "Xianfang Zeng", "Daxin Jiang", "Gang Yu", "Xingjun Ma", "Yu-Gang Jiang"], "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation", "comment": "23 Pages; Project Page: https://doby-xu.github.io/WithAnyone/; Code:\n  https://github.com/Doby-Xu/WithAnyone", "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration."}
{"id": "2510.14978", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14978", "abs": "https://arxiv.org/abs/2510.14978", "authors": ["Nupur Kumari", "Sheng-Yu Wang", "Nanxuan Zhao", "Yotam Nitzan", "Yuheng Li", "Krishna Kumar Singh", "Richard Zhang", "Eli Shechtman", "Jun-Yan Zhu", "Xun Huang"], "title": "Learning an Image Editing Model without Image Editing Pairs", "comment": "project page: https://nupurkmr9.github.io/npedit/", "summary": "Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO."}
{"id": "2510.14979", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14979", "abs": "https://arxiv.org/abs/2510.14979", "authors": ["Haiwen Diao", "Mingxuan Li", "Silei Wu", "Linjun Dai", "Xiaohua Wang", "Hanming Deng", "Lewei Lu", "Dahua Lin", "Ziwei Liu"], "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale", "comment": "21 pages, 7 figures", "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO."}
{"id": "2509.25991", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25991", "abs": "https://arxiv.org/abs/2509.25991", "authors": ["Haiyang Li", "Yaxiong Wang", "Shengeng Tang", "Lianwei Wu", "Lechao Cheng", "Zhun Zhong"], "title": "Towards Unified Multimodal Misinformation Detection in Social Media: A Benchmark Dataset and Baseline", "comment": null, "summary": "In recent years, detecting fake multimodal content on social media has drawn\nincreasing attention. Two major forms of deception dominate: human-crafted\nmisinformation (e.g., rumors and misleading posts) and AI-generated content\nproduced by image synthesis models or vision-language models (VLMs). Although\nboth share deceptive intent, they are typically studied in isolation. NLP\nresearch focuses on human-written misinformation, while the CV community\ntargets AI-generated artifacts. As a result, existing models are often\nspecialized for only one type of fake content. In real-world scenarios,\nhowever, the type of a multimodal post is usually unknown, limiting the\neffectiveness of such specialized systems. To bridge this gap, we construct the\nOmnibus Dataset for Multimodal News Deception (OmniFake), a comprehensive\nbenchmark of 127K samples that integrates human-curated misinformation from\nexisting resources with newly synthesized AI-generated examples. Based on this\ndataset, we propose Unified Multimodal Fake Content Detection (UMFDet), a\nframework designed to handle both forms of deception. UMFDet leverages a VLM\nbackbone augmented with a Category-aware Mixture-of-Experts (MoE) Adapter to\ncapture category-specific cues, and an attribution chain-of-thought mechanism\nthat provides implicit reasoning guidance for locating salient deceptive\nsignals. Extensive experiments demonstrate that UMFDet achieves robust and\nconsistent performance across both misinformation types, outperforming\nspecialized baselines and offering a practical solution for real-world\nmultimodal deception detection."}
{"id": "2510.13856", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13856", "abs": "https://arxiv.org/abs/2510.13856", "authors": ["A H M Rezaul Karim", "Ozlem Uzuner"], "title": "Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA", "comment": null, "summary": "Medical Visual Question Answering (MedVQA) enables natural language queries\nover medical images to support clinical decision-making and patient care. The\nMEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to\ngenerate free-text responses and structured wound attributes from images and\npatient queries. We present the MasonNLP system, which employs a\ngeneral-domain, instruction-tuned large language model with a\nretrieval-augmented generation (RAG) framework that incorporates textual and\nvisual examples from in-domain data. This approach grounds outputs in\nclinically relevant exemplars, improving reasoning, schema adherence, and\nresponse quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our\nbest-performing system ranked 3rd among 19 teams and 51 submissions with an\naverage score of 41.37%, demonstrating that lightweight RAG with\ngeneral-purpose LLMs -- a minimal inference-time layer that adds a few relevant\nexemplars via simple indexing and fusion, with no extra training or complex\nre-ranking -- provides a simple and effective baseline for multimodal clinical\nNLP tasks."}
{"id": "2510.14824", "categories": ["cs.CL", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14824", "abs": "https://arxiv.org/abs/2510.14824", "authors": ["Ziqi Dai", "Xin Zhang", "Mingxin Li", "Yanzhao Zhang", "Dingkun Long", "Pengjun Xie", "Meishan Zhang", "Wenjie Li", "Min Zhang"], "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking", "comment": null, "summary": "In information retrieval, training reranking models mainly focuses on two\ntypes of objectives: metric learning (e.g. contrastive loss to increase the\npredicted scores on relevant query-document pairs) and classification (binary\nlabel prediction of relevance vs. irrelevance). For BERT-style encoders,\nvarious studies have shown that contrastive learning (CL) can be more effective\nthan discriminative (classification) learning. However, for large language\nmodels (LLMs), classification via supervised fine-tuning (SFT), which predicts\n''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears\nmore promising as it aligns well with the generative nature of LLMs. This\ndivergence raises a central question: which objective is intrinsically better\nsuited to LLM-based reranking, and what mechanism underlies the difference? In\nthis work, we conduct a comprehensive comparison and analysis between CL and\nSFT for reranking, taking the universal multimodal retrieval (UMR) as the\nexperimental playground. We first decompose the objectives into two components:\nweight, which controls the magnitude of those updates, and direction, which\nguides the model updates, then present a unified framework for understanding\ntheir interactions. Through probing experiments, we find that SFT provides a\nsubstantially stronger weighting scheme than CL, whereas the preferred scoring\ndirection shows no clear winner. Taken together, these results point to a\nconsistent advantage of SFT over CL for LLM reranking. To further validate our\nfindings, we conduct large-scale training with SFT and present new\nstate-of-the-art rerankers on the MRB benchmark. We also provide ablations on\nSFT settings and expect our findings to benefit future research and\napplications in this area."}
{"id": "2510.14949", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14949", "abs": "https://arxiv.org/abs/2510.14949", "authors": ["Yu Zhou", "Sohyun An", "Haikang Deng", "Da Yin", "Clark Peng", "Cho-Jui Hsieh", "Kai-Wei Chang", "Nanyun Peng"], "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation", "comment": null, "summary": "Contact languages like English exhibit rich regional variations in the form\nof dialects, which are often used by dialect speakers interacting with\ngenerative models. However, can multimodal generative models effectively\nproduce content given dialectal textual input? In this work, we study this\nquestion by constructing a new large-scale benchmark spanning six common\nEnglish dialects. We work with dialect speakers to collect and verify over 4200\nunique prompts and evaluate on 17 image and video generative models. Our\nautomatic and human evaluation results show that current state-of-the-art\nmultimodal generative models exhibit 32.26% to 48.17% performance degradation\nwhen a single dialect word is used in the prompt. Common mitigation methods\nsuch as fine-tuning and prompt rewriting can only improve dialect performance\nby small margins (< 7%), while potentially incurring significant performance\ndegradation in Standard American English (SAE). To this end, we design a\ngeneral encoder-based mitigation strategy for multimodal generative models. Our\nmethod teaches the model to recognize new dialect features while preserving SAE\nperformance. Experiments on models such as Stable Diffusion 1.5 show that our\nmethod is able to simultaneously raise performance on five dialects to be on\npar with SAE (+34.4%), while incurring near zero cost to SAE performance."}
