<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 41]
- [cs.CL](#cs.CL) [Total: 14]
- [cs.AI](#cs.AI) [Total: 11]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MultiFoodhat: A potential new paradigm for intelligent food quality inspection](https://arxiv.org/abs/2510.13889)
*Yue Hu, Guohang Zhuang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MultiFoodChatï¼Œä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“æ¨ç†çš„å¯¹è¯å¼é›¶æ ·æœ¬é£Ÿç‰©è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆè§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹å®ç°æ— éœ€é¢å¤–è®­ç»ƒçš„é£Ÿç‰©åˆ†ç±»ï¼Œåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå±•ç°äº†ä¼˜è¶Šçš„è¯†åˆ«å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç›‘ç£æ¨¡å‹ä¸¥é‡ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®ä¸”å¯¹æœªè§é£Ÿç‰©ç±»åˆ«æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œè¿™é™åˆ¶äº†æ™ºèƒ½é£Ÿå“è´¨é‡æ£€æµ‹å’Œé¥®é£Ÿè¯„ä¼°ç³»ç»Ÿçš„å®é™…åº”ç”¨ï¼Œå› æ­¤éœ€è¦å¼€å‘æ— éœ€é¢å¤–è®­ç»ƒå³å¯è¯†åˆ«æ–°ç±»åˆ«é£Ÿç‰©çš„é›¶æ ·æœ¬æ–¹æ³•ã€‚

**Method:** è¯¥æ¡†æ¶é‡‡ç”¨å¤šæ™ºèƒ½ä½“å¯¹è¯æ¨ç†æœºåˆ¶ï¼Œæ•´åˆè§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šè½®è§†è§‰-æ–‡æœ¬å¯¹è¯åä½œï¼Œé€šè¿‡å¯¹è±¡æ„ŸçŸ¥ä»¤ç‰Œæ•è·ç»†ç²’åº¦è§†è§‰å±æ€§ï¼Œç»“åˆäº¤äº’å¼æ¨ç†æ™ºèƒ½ä½“åŠ¨æ€è§£æä¸Šä¸‹æ–‡çº¿ç´¢ä»¥ä¼˜åŒ–é¢„æµ‹ç»“æœã€‚

**Result:** åœ¨å¤šä¸ªå…¬å¼€é£Ÿç‰©æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMultiFoodChatç›¸æ¯”ç°æœ‰æ— ç›‘ç£å’Œå°‘æ ·æœ¬æ–¹æ³•å®ç°äº†æ›´ä¼˜è¶Šçš„è¯†åˆ«å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** MultiFoodChatä¸ºæ™ºèƒ½é£Ÿå“è´¨é‡æ£€æµ‹å’Œåˆ†ææä¾›äº†æ–°èŒƒå¼ï¼Œå…¶å¤šæ™ºèƒ½ä½“å¯¹è¯æ¨ç†è®¾è®¡å®ç°äº†æ— éœ€äººå·¥æ ‡æ³¨çš„å¤æ‚é£Ÿç‰©åœºæ™¯ç†è§£ï¼Œå±•ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹ä¸å¤§è¯­è¨€æ¨¡å‹ååŒåœ¨é›¶æ ·æœ¬è¯†åˆ«ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Food image classification plays a vital role in intelligent food quality
inspection, dietary assessment, and automated monitoring. However, most
existing supervised models rely heavily on large labeled datasets and exhibit
limited generalization to unseen food categories. To overcome these challenges,
this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning
framework for zero-shot food recognition. The framework integrates
vision-language models (VLMs) and large language models (LLMs) to enable
collaborative reasoning through multi-round visual-textual dialogues. An Object
Perception Token (OPT) captures fine-grained visual attributes, while an
Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to
refine predictions. This multi-agent design allows flexible and human-like
understanding of complex food scenes without additional training or manual
annotations. Experiments on multiple public food datasets demonstrate that
MultiFoodChat achieves superior recognition accuracy and interpretability
compared with existing unsupervised and few-shot methods, highlighting its
potential as a new paradigm for intelligent food quality inspection and
analysis.


### [2] [Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models](https://arxiv.org/abs/2510.13993)
*Jia Yun Chua, Argyrios Zolotas, Miguel Arana-Catania*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆä¼ ç»Ÿè§†è§‰æ¨¡å‹ä¸è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–¹æ³•æ¥å¢å¼ºé¥æ„Ÿå›¾åƒåˆ†æï¼Œåœ¨é£æœºæ£€æµ‹å’Œåœºæ™¯ç†è§£ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹è¡¨ç°å‡ºè‰²ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿè§†è§‰æ¨¡å‹åœ¨é¥æ„Ÿåº”ç”¨ä¸­é¢ä¸´éœ€è¦å¤§é‡é¢†åŸŸç‰¹å®šæ ‡æ³¨æ•°æ®ä»¥åŠç†è§£å¤æ‚ç¯å¢ƒä¸Šä¸‹æ–‡èƒ½åŠ›çš„é™åˆ¶ï¼Œè€Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¥æ„Ÿé¢†åŸŸçš„åº”ç”¨ä»å¤„äºæ¢ç´¢ä¸è¶³çŠ¶æ€ï¼Œç‰¹åˆ«æ˜¯å…¶é€šç”¨æ€§ç‰¹ç‚¹å°šæœªå……åˆ†å‘æ˜ã€‚

**Method:** è¯¥æ–¹æ³•æ•´åˆäº†YOLOç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸LLaVAã€ChatGPTã€Geminiç­‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°æ›´å‡†ç¡®ä¸”å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›¾åƒè§£é‡Šï¼Œå¹¶åœ¨æ ‡æ³¨å’Œæœªæ ‡æ³¨é¥æ„Ÿæ•°æ®ä»¥åŠé€€åŒ–å›¾åƒåœºæ™¯ä¸­è¿›è¡Œè¯„ä¼°ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºåœ¨é£æœºæ£€æµ‹å’Œè®¡æ•°ä»»åŠ¡ä¸­ï¼Œæ‰€æœ‰æ¨¡å‹çš„å¹³å‡MAEæå‡äº†48.46%ï¼Œç‰¹åˆ«æ˜¯åœ¨æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹è¡¨ç°ä¼˜å¼‚ï¼›åŒæ—¶åœ¨é¥æ„Ÿå›¾åƒå…¨é¢ç†è§£æ–¹é¢ï¼ŒCLIPScoreæŒ‡æ ‡æå‡äº†6.17%ã€‚

**Conclusion:** æ‰€æå‡ºçš„ä¼ ç»Ÿè§†è§‰æ¨¡å‹ä¸è§†è§‰è¯­è¨€æ¨¡å‹ç»“åˆæ–¹æ³•ä¸ºæ›´å…ˆè¿›å’Œé«˜æ•ˆçš„é¥æ„Ÿå›¾åƒåˆ†æå¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå°¤å…¶åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤æ‚ç¯å¢ƒä¸‹çš„å›¾åƒç†è§£æŒ‘æˆ˜ã€‚

---

#### ğŸ“„ Abstract
Remote sensing has become a vital tool across sectors such as urban planning,
environmental monitoring, and disaster response. While the volume of data
generated has increased significantly, traditional vision models are often
constrained by the requirement for extensive domain-specific labelled data and
their limited ability to understand the context within complex environments.
Vision Language Models offer a complementary approach by integrating visual and
textual data; however, their application to remote sensing remains
underexplored, particularly given their generalist nature. This work
investigates the combination of vision models and VLMs to enhance image
analysis in remote sensing, with a focus on aircraft detection and scene
understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and
Gemini aims to achieve more accurate and contextually aware image
interpretation. Performance is evaluated on both labelled and unlabelled remote
sensing data, as well as degraded image scenarios which are crucial for remote
sensing. The findings show an average MAE improvement of 48.46% across models
in the accuracy of aircraft detection and counting, especially in challenging
conditions, in both raw and degraded scenarios. A 6.17% improvement in
CLIPScore for comprehensive understanding of remote sensing images is obtained.
The proposed approach combining traditional vision models and VLMs paves the
way for more advanced and efficient remote sensing image analysis, especially
in few-shot learning scenarios.


### [3] [Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding](https://arxiv.org/abs/2510.14032)
*Xiaoqian Shen, Wenxuan Zhang, Jun Chen, Mohamed Elhoseiny*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Vgentï¼Œä¸€ç§åŸºäºå›¾çš„æ£€ç´¢-æ¨ç†å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–å›¾è¡¨ç¤ºå’Œä¸­é—´æ¨ç†æ­¥éª¤æ¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­æ—¶åºä¾èµ–æ–­è£‚å’Œæ— å…³ä¿¡æ¯å¹²æ‰°çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é•¿è§†é¢‘ç†è§£å¯¹å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹æ„æˆé‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå¤„ç†è¶…å‡ºä¸Šä¸‹æ–‡çª—å£çš„å¯†é›†è§†é¢‘æ ‡è®°å’Œä¿ç•™é•¿æœŸåºåˆ—ä¿¡æ¯çš„å›°éš¾ã€‚ç°æœ‰æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•åœ¨é•¿è§†é¢‘åº”ç”¨ä¸­é¢ä¸´æ—¶åºä¾èµ–æ–­è£‚å’Œæ— å…³ä¿¡æ¯åŒ…å«ç­‰é—®é¢˜ï¼Œè¿™äº›å› ç´ ä¼šé˜»ç¢å‡†ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚

**Method:** Vgentæ¡†æ¶å¼•å…¥ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šä¸€æ˜¯ä½¿ç”¨ç»“æ„åŒ–å›¾è¡¨ç¤ºè§†é¢‘ï¼Œä¿æŒè§†é¢‘ç‰‡æ®µé—´çš„è¯­ä¹‰å…³ç³»ä»¥æé«˜æ£€ç´¢æ•ˆæœï¼›äºŒæ˜¯å¼•å…¥ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œé€šè¿‡ç»“æ„åŒ–éªŒè¯å‡å°‘æ£€ç´¢å™ªå£°ï¼Œä¿ƒè¿›è·¨ç‰‡æ®µç›¸å…³ä¿¡æ¯çš„æ˜¾å¼èšåˆï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å“åº”ã€‚

**Result:** åœ¨ä¸‰ä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”åŸºç¡€æ¨¡å‹åœ¨MLVUä¸Šå®ç°äº†3.0%âˆ¼5.4%çš„æ•´ä½“æ€§èƒ½æå‡ï¼Œå¹¶åœ¨è§†é¢‘æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ä¸­ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•8.6%ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ç»“æ„åŒ–å›¾è¡¨ç¤ºå’Œä¸­é—´æ¨ç†æ­¥éª¤èƒ½æœ‰æ•ˆè§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºè§†é¢‘è¯­è¨€æ¨¡å‹å¤„ç†å¤æ‚æ—¶åºä¿¡æ¯æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Understanding and reasoning over long videos pose significant challenges for
large video language models (LVLMs) due to the difficulty in processing
intensive video tokens beyond context window and retaining long-term sequential
information. Retrieval-Augmented Generation (RAG) has demonstrated
effectiveness in processing long context for Large Language Models (LLMs);
however, applying RAG to long video faces challenges such as disrupted temporal
dependencies and inclusion of irrelevant information that can hinder accurate
reasoning. To address these limitations, we propose Vgent, a novel graph-based
retrieval-reasoning-augmented generation framework to enhance LVLMs for long
video understanding. Our approach introduces two key innovations: (i) It
represents videos by structured graphs with semantic relationships across video
clips preserved to improve retrieval effectiveness. (ii) It introduces an
intermediate reasoning step to mitigate the reasoning limitation of LVLMs,
which leverages structured verification to reduce retrieval noise and
facilitate the explicit aggregation of relevant information across clips,
resulting in more accurate and context-aware responses. We comprehensively
evaluate our framework with various open-source LVLMs on three long-video
understanding benchmarks. Our approach yielded an overall performance
improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed
state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available
at https://xiaoqian-shen.github.io/Vgent.


### [4] [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](https://arxiv.org/abs/2510.14081)
*Emanuel Garbin, Guy Adam, Oded Krams, Zohar Barzelay, Eran Guendelman, Michael Schwarz, Moran Vatelmacher, Yigal Shenkman, Eli Peker, Itai Druker, Uri Patish, Yoav Blum, Max Bluvstein, Junxuan Li, Rawal Khirodkar, Shunsuke Saito*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬æµæ°´çº¿ï¼Œå¯ä»å°‘é‡éç»“æ„åŒ–æ‰‹æœºå›¾åƒåˆ›å»ºè¶…é€¼çœŸä¸”ä¿æŒèº«ä»½ä¸€è‡´æ€§çš„3Dè™šæ‹ŸåŒ–èº«ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå¼è§„èŒƒåŒ–æ¨¡å—å’ŒåŸºäºTransformerçš„æ¨¡å‹ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å‡ ä½•ä¸€è‡´æ€§å’Œé«˜é¢‘ç»†èŠ‚æ•æ‰æ–¹é¢çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼šå•è§†å›¾æ–¹æ³•å­˜åœ¨å‡ ä½•ä¸ä¸€è‡´æ€§å’Œå¹»è§‰é—®é¢˜ï¼Œå¯¼è‡´èº«ä»½ä¿æŒèƒ½åŠ›ä¸‹é™ï¼›è€ŒåŸºäºåˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹æ— æ³•æ•æ‰çš®è‚¤çš±çº¹å’Œç»†å‘ç­‰é«˜é¢‘ç»†èŠ‚ï¼Œé™åˆ¶äº†çœŸå®æ„Ÿã€‚è¿™äº›å±€é™æ€§ä¿ƒä½¿ç ”ç©¶è€…å¼€å‘èƒ½å¤Ÿä»éç»“æ„åŒ–ç…§ç‰‡ä¸­ç”Ÿæˆé«˜è´¨é‡è™šæ‹ŸåŒ–èº«çš„è§£å†³æ–¹æ¡ˆã€‚

**Method:** è¯¥æ–¹æ³•å¼•å…¥ä¸¤ä¸ªå…³é”®è´¡çŒ®ï¼šç”Ÿæˆå¼è§„èŒƒåŒ–æ¨¡å—å¤„ç†å¤šä¸ªéç»“æ„åŒ–è§†å›¾å¹¶è½¬åŒ–ä¸ºæ ‡å‡†åŒ–ã€ä¸€è‡´çš„è¡¨ç¤ºï¼›åŸºäºTransformerçš„æ¨¡å‹åœ¨ä»çœŸå®äººç‰©ç©¹é¡¶æ•æ‰æ•°æ®æ„å»ºçš„å¤§è§„æ¨¡é«˜æ–¯æº…å°„è™šæ‹ŸåŒ–èº«æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚æ•´ä¸ª"æ•æ‰-è§„èŒƒåŒ–-æº…å°„"æµæ°´çº¿å¯ä»éç»“æ„åŒ–ç…§ç‰‡ç”Ÿæˆé™æ€åŠèº«è™šæ‹ŸåŒ–èº«ã€‚

**Result:** è¯¥æµæ°´çº¿ç”Ÿæˆçš„é™æ€åŠèº«è™šæ‹ŸåŒ–èº«å±•ç°å‡ºä»¤äººä¿¡æœçš„çœŸå®æ„Ÿå’Œé²æ£’çš„èº«ä»½ä¿æŒèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒèº«ä»½ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é«˜é¢‘ç»†èŠ‚ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†è™šæ‹ŸåŒ–èº«çš„è§†è§‰è´¨é‡ã€‚

**Conclusion:** è¿™é¡¹ç ”ç©¶å±•ç¤ºäº†ä»éç»“æ„åŒ–ç…§ç‰‡ç”Ÿæˆé«˜è´¨é‡3Dè™šæ‹ŸåŒ–èº«çš„å¯è¡Œæ€§ï¼Œä¸ºæ•°å­—èº«ä»½åˆ›å»ºæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨çœŸå®æ„Ÿå’Œèº«ä»½ä¿æŒæ–¹é¢å‡å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä¸ºè™šæ‹Ÿç°å®ã€æ•°å­—äººç­‰åº”ç”¨é¢†åŸŸæä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚

---

#### ğŸ“„ Abstract
We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.


### [5] [Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition](https://arxiv.org/abs/2510.14203)
*Ryo Masumura, Shota Orihashi, Mana Ihori, Tomohiro Tanaka, Naoki Makishima, Taiga Yamane, Naotaka Kawata, Satoshi Suzuki, Taichi Katayama*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è”åˆå»ºæ¨¡æ–¹æ³•ï¼Œç”¨äºä»å¤šæ¨¡æ€äººç±»è¡Œä¸ºä¸­è‡ªåŠ¨è¯†åˆ«å¤§äº”äººæ ¼å’ŒHEXACOäººæ ¼ç‰¹è´¨ã€‚è¯¥æ–¹æ³•é€šè¿‡è”åˆä¼˜åŒ–å¤§äº”äººæ ¼å’ŒHEXACOçš„è¯†åˆ«ï¼Œæé«˜äº†å¤šæ¨¡æ€äººæ ¼ç‰¹è´¨è¯†åˆ«çš„æ•ˆæœã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç ”ç©¶ä¸»è¦ä½¿ç”¨å¤§äº”äººæ ¼è¿›è¡Œå¤šæ¨¡æ€äººæ ¼ç‰¹è´¨è¯†åˆ«ï¼Œä½†ç¼ºä¹å¯¹HEXACOäººæ ¼çš„å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯èƒ½å¤Ÿè¯„ä¼°è¯šå®-è°¦é€Šç‰¹è´¨ï¼ˆä¸æ›¿ä»£æ€§æ”»å‡»å’ŒæŠ¥å¤å¿ƒç›¸å…³ï¼‰çš„ç»´åº¦ã€‚åŒæ—¶ï¼Œæœºå™¨å­¦ä¹ å»ºæ¨¡ä¸­å¤§äº”äººæ ¼å’ŒHEXACOä¹‹é—´çš„å…³ç³»å°šæœªæ˜ç¡®ï¼Œè€ƒè™‘è¿™äº›å…³ç³»æœ‰æœ›æå‡å¤šæ¨¡æ€äººç±»è¡Œä¸ºçš„è®¤çŸ¥æ•ˆæœã€‚

**Method:** æå‡ºäº†ä¸€ç§è”åˆä¼˜åŒ–è¯†åˆ«å¤§äº”äººæ ¼å’ŒHEXACOçš„æ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€äººç±»è¡Œä¸ºæ•°æ®è¿›è¡Œå»ºæ¨¡ï¼Œæ¢ç´¢ä¸¤ç§äººæ ¼æ¨¡å‹ä¹‹é—´çš„å†…åœ¨è”ç³»ï¼Œå®ç°æ›´å…¨é¢çš„äººæ ¼ç‰¹è´¨è¯†åˆ«ã€‚

**Result:** åœ¨è‡ªæˆ‘ä»‹ç»è§†é¢‘æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¤§äº”äººæ ¼å’ŒHEXACOäººæ ¼ç‰¹è´¨ï¼ŒéªŒè¯äº†è”åˆå»ºæ¨¡æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†è”åˆå»ºæ¨¡å¤§äº”äººæ ¼å’ŒHEXACOåœ¨å¤šæ¨¡æ€äººæ ¼è¯†åˆ«ä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºæ›´å…¨é¢çš„äººæ ¼ç‰¹è´¨è¯„ä¼°æä¾›äº†æ–°æ€è·¯ï¼Œæœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢ä¸¤ç§äººæ ¼æ¨¡å‹åœ¨è¡Œä¸ºåˆ†æä¸­çš„äº’è¡¥å…³ç³»ã€‚

---

#### ğŸ“„ Abstract
This paper proposes a joint modeling method of the Big Five, which has long
been studied, and HEXACO, which has recently attracted attention in psychology,
for automatically recognizing apparent personality traits from multimodal human
behavior. Most previous studies have used the Big Five for multimodal apparent
personality-trait recognition. However, no study has focused on apparent HEXACO
which can evaluate an Honesty-Humility trait related to displaced aggression
and vengefulness, social-dominance orientation, etc. In addition, the
relationships between the Big Five and HEXACO when modeled by machine learning
have not been clarified. We expect awareness of multimodal human behavior to
improve by considering these relationships. The key advance of our proposed
method is to optimize jointly recognizing the Big Five and HEXACO. Experiments
using a self-introduction video dataset demonstrate that the proposed method
can effectively recognize the Big Five and HEXACO.


### [6] [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](https://arxiv.org/abs/2510.14241)
*Soumyya Kanti Datta, Tanvi Ranga, Chengzhe Sun, Siwei Lyu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€éŸ³é¢‘-è§†è§‰æ¡†æ¶PIAï¼Œé€šè¿‡æ•´åˆè¯­è¨€ã€åŠ¨æ€é¢éƒ¨è¿åŠ¨å’Œé¢éƒ¨è¯†åˆ«çº¿ç´¢ï¼Œæ˜¾è‘—æå‡äº†æ£€æµ‹å…ˆè¿›ç”Ÿæˆæ¨¡å‹äº§ç”Ÿçš„æ·±åº¦ä¼ªé€ å†…å®¹çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿæ£€æµ‹å™¨åœ¨è¯†åˆ«ç°ä»£æ·±åº¦ä¼ªé€ æŠ€æœ¯äº§ç”Ÿçš„æ—¶é—´ä¸ä¸€è‡´æ€§æ–¹é¢çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿæ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–æ‰‹åŠ¨è®¾è®¡çš„éŸ³ç´ -è§†ä½å¯¹é½é˜ˆå€¼ã€åŸºç¡€å¸§çº§ä¸€è‡´æ€§æ£€æŸ¥æˆ–å•æ¨¡æ€æ£€æµ‹ç­–ç•¥ï¼Œæ— æ³•æœ‰æ•ˆè¯†åˆ«ç”±GANã€æ‰©æ•£æ¨¡å‹å’Œç¥ç»æ¸²æŸ“æŠ€æœ¯ç­‰å…ˆè¿›ç”Ÿæˆæ¨¡å‹äº§ç”Ÿçš„ç°ä»£æ·±åº¦ä¼ªé€ å†…å®¹ã€‚è¿™äº›å…ˆè¿›æŠ€æœ¯èƒ½ç”Ÿæˆè¿‘ä¹å®Œç¾çš„å•å¸§å›¾åƒï¼Œä½†ä¼šæ— æ„ä¸­äº§ç”Ÿä¼ ç»Ÿæ£€æµ‹å™¨ç»å¸¸å¿½ç•¥çš„å¾®å°æ—¶é—´å·®å¼‚ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†Phoneme-Temporal and Identity-Dynamic Analysis (PIA)å¤šæ¨¡æ€éŸ³é¢‘-è§†è§‰æ¡†æ¶ï¼Œæ•´åˆäº†è¯­è¨€ã€åŠ¨æ€é¢éƒ¨è¿åŠ¨å’Œé¢éƒ¨è¯†åˆ«çº¿ç´¢ã€‚è¯¥æ–¹æ³•åˆ©ç”¨éŸ³ç´ åºåˆ—ã€å˜´å”‡å‡ ä½•æ•°æ®å’Œå…ˆè¿›çš„é¢éƒ¨èº«ä»½åµŒå…¥ï¼Œé€šè¿‡è¯†åˆ«å¤šä¸ªäº’è¡¥æ¨¡æ€ä¹‹é—´çš„ä¸ä¸€è‡´æ€§æ¥æ£€æµ‹ç»†å¾®çš„æ·±åº¦ä¼ªé€ ç¯¡æ”¹ã€‚

**Result:** è¯¥é›†æˆæ–¹æ³•æ˜¾è‘—æé«˜äº†å¯¹ç»†å¾®æ·±åº¦ä¼ªé€ ç¯¡æ”¹çš„æ£€æµ‹èƒ½åŠ›ï¼Œé€šè¿‡å¤šæ¨¡æ€ä¸€è‡´æ€§åˆ†ææœ‰æ•ˆè¯†åˆ«äº†ä¼ ç»Ÿæ£€æµ‹å™¨éš¾ä»¥å‘ç°çš„æ—¶é—´å·®å¼‚å’Œä¸ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ£€æµ‹ç°ä»£ç”Ÿæˆæ¨¡å‹äº§ç”Ÿçš„æ·±åº¦ä¼ªé€ å†…å®¹æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** PIAæ¡†æ¶é€šè¿‡å¤šæ¨¡æ€åˆ†æè§£å†³äº†ä¼ ç»Ÿæ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•çš„å±€é™æ€§ï¼Œä¸ºæ£€æµ‹å…ˆè¿›ç”ŸæˆæŠ€æœ¯äº§ç”Ÿçš„ä¼ªé€ å†…å®¹æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æ•´åˆè¯­è¨€ã€åŠ¨æ€è¿åŠ¨å’Œèº«ä»½è¯†åˆ«çº¿ç´¢åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æ£€æµ‹æŠ€æœ¯çš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
The rise of manipulated media has made deepfakes a particularly insidious
threat, involving various generative manipulations such as lip-sync
modifications, face-swaps, and avatar-driven facial synthesis. Conventional
detection methods, which predominantly depend on manually designed
phoneme-viseme alignment thresholds, fundamental frame-level consistency
checks, or a unimodal detection strategy, inadequately identify modern-day
deepfakes generated by advanced generative models such as GANs, diffusion
models, and neural rendering techniques. These advanced techniques generate
nearly perfect individual frames yet inadvertently create minor temporal
discrepancies frequently overlooked by traditional detectors. We present a
novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic
Analysis(PIA), incorporating language, dynamic face motion, and facial
identification cues to address these limitations. We utilize phoneme sequences,
lip geometry data, and advanced facial identity embeddings. This integrated
method significantly improves the detection of subtle deepfake alterations by
identifying inconsistencies across multiple complementary modalities. Code is
available at https://github.com/skrantidatta/PIA


### [7] [Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding](https://arxiv.org/abs/2510.14304)
*Kyungryul Back, Seongbeom Park, Milim Kim, Mincheol Kwon, SangHyeok Lee, Hyunyoung Lee, Junhee Cho, Seunghyun Park, Jinkyu Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ä¸‰å±‚å¯¹æ¯”è§£ç ä¸æ°´å°æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æˆç†Ÿå±‚ä¸ä¸šä½™å±‚ã€è¯†åˆ«è§†è§‰æ¥åœ°è‰¯å¥½çš„æ¢è½´å±‚ï¼Œå¹¶åº”ç”¨ä¸‰å±‚å¯¹æ¯”è§£ç æ¥å‡å°‘å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å°½ç®¡åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œå¾€å¾€è¿‡åº¦ä¾èµ–å•ä¸€æ¨¡æ€æˆ–è®°å¿†è®­ç»ƒæ•°æ®è€Œæœªèƒ½æ­£ç¡®æ¥åœ°å…¶è¾“å‡ºï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„å¯é æ€§å’Œå®ç”¨æ€§ã€‚

**Method:** æå‡ºæ— éœ€è®­ç»ƒçš„ä¸‰å±‚å¯¹æ¯”è§£ç ä¸æ°´å°æ–¹æ³•ï¼šé¦–å…ˆåœ¨è§£ç å±‚ä¸­é€‰æ‹©æˆç†Ÿå±‚ä¸ä¸šä½™å±‚ï¼Œç„¶åä½¿ç”¨æ°´å°ç›¸å…³é—®é¢˜è¯†åˆ«è§†è§‰æ¥åœ°è‰¯å¥½çš„æ¢è½´å±‚ï¼Œæœ€ååº”ç”¨ä¸‰å±‚å¯¹æ¯”è§£ç ç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚

**Result:** åœ¨POPEã€MMEå’ŒAMBERç­‰å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘LVLMså¹»è§‰æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ç”Ÿæˆäº†æ›´å…·è§†è§‰æ¥åœ°æ€§çš„å“åº”ã€‚

**Conclusion:** è¯¥æ–¹æ³•é€šè¿‡åˆ›æ–°çš„ä¸‰å±‚å¯¹æ¯”è§£ç æœºåˆ¶æœ‰æ•ˆæå‡äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è¾“å‡ºå¯é æ€§ï¼Œä¸ºå‡å°‘æ¨¡å‹å¹»è§‰æä¾›äº†æ— éœ€è®­ç»ƒçš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Large Vision-Language Models (LVLMs) have recently shown promising results on
various multimodal tasks, even achieving human-comparable performance in
certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often
rely heavily on a single modality or memorize training data without properly
grounding their outputs. To address this, we propose a training-free, tri-layer
contrastive decoding with watermarking, which proceeds in three steps: (1)
select a mature layer and an amateur layer among the decoding layers, (2)
identify a pivot layer using a watermark-related question to assess whether the
layer is visually well-grounded, and (3) apply tri-layer contrastive decoding
to generate the final output. Experiments on public benchmarks such as POPE,
MME and AMBER demonstrate that our method achieves state-of-the-art performance
in reducing hallucinations in LVLMs and generates more visually grounded
responses.


### [8] [Vision-Centric Activation and Coordination for Multimodal Large Language Models](https://arxiv.org/abs/2510.14349)
*Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, Xin Jin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºVaCoæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥è§†è§‰ä¸­å¿ƒæ¿€æ´»å’Œå¤šè§†è§‰åŸºç¡€æ¨¡å‹çš„åè°ƒæ¥ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºå­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†MLLMåœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¸»æµå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»…é€šè¿‡æ–‡æœ¬æ ‡è®°çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹è¿›è¡Œç›‘ç£ï¼Œå¿½è§†äº†å¯¹äºåˆ†æèƒ½åŠ›è‡³å…³é‡è¦çš„è§†è§‰ä¸­å¿ƒä¿¡æ¯ï¼Œå¯¼è‡´è§†è§‰ç†è§£èƒ½åŠ›å—é™ã€‚

**Method:** VaCoå¼•å…¥å¯å­¦ä¹ çš„æ¨¡å—åŒ–ä»»åŠ¡æŸ¥è¯¢å’Œè§†è§‰å¯¹é½å±‚æ¥æ¿€æ´»ç‰¹å®šè§†è§‰ä¿¡å·ï¼Œå¹¶é€šè¿‡ä»¤ç‰Œç½‘å…³æ©ç åè°ƒä¸åŒè§†è§‰åŸºç¡€æ¨¡å‹ä¹‹é—´çš„è¡¨ç¤ºå†²çªï¼Œå®ç°æ–‡æœ¬å’Œè§†è§‰è¾“å‡ºçš„ç»Ÿä¸€ä¼˜åŒ–ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜VaCoæ˜¾è‘—æå‡äº†ä¸åŒMLLMåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰ç†è§£æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è§†è§‰ä¸­å¿ƒæ¿€æ´»å’Œåè°ƒæœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºMLLMçš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æä¾›äº†æ–°çš„ä¼˜åŒ–æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Multimodal large language models (MLLMs) integrate image features from visual
encoders with LLMs, demonstrating advanced comprehension capabilities. However,
mainstream MLLMs are solely supervised by the next-token prediction of textual
tokens, neglecting critical vision-centric information essential for analytical
abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
representations through Vision-Centric activation and Coordination from
multiple vision foundation models (VFMs). VaCo introduces visual discriminative
alignment to integrate task-aware perceptual features extracted from VFMs,
thereby unifying the optimization of both textual and visual outputs in MLLMs.
Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
under the supervision of diverse VFMs. To coordinate representation conflicts
across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
significantly improves the performance of different MLLMs on various
benchmarks, showcasing its superior capabilities in visual comprehension.


### [9] [Spatial Preference Rewarding for MLLMs Spatial Understanding](https://arxiv.org/abs/2510.14374)
*Han Qiu, Peng Gao, Lewei Lu, Xiaoqin Zhang, Ling Shao, Shijian Lu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSPRï¼ˆç©ºé—´åå¥½å¥–åŠ±ï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¥–åŠ±å…·æœ‰ç²¾ç¡®å®šä½èƒ½åŠ›çš„è¯¦ç»†å“åº”æ¥å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ä¸”è®­ç»ƒå¼€é”€æå°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´ç†è§£æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨ç»†ç²’åº¦ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå¦‚ç”Ÿæˆè¯¦ç»†åŒºåŸŸæè¿°æˆ–ç²¾ç¡®å®šä½ç‰©ä½“ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å¯¹é¢„æ ‡æ³¨æŒ‡ä»¤æ•°æ®çš„å»ºæ¨¡ï¼Œç¼ºä¹å¯¹æ¨¡å‹å®é™…å“åº”çš„ç›´æ¥ç›‘ç£ï¼Œå¯¼è‡´æ— æ³•æ»¡è¶³ç”¨æˆ·å¯¹ç»†ç²’åº¦ç©ºé—´ç†è§£çš„éœ€æ±‚ã€‚

**Method:** SPRæ–¹æ³•é€šè¿‡éšæœºé€‰æ‹©å›¾åƒåŒºåŸŸå’Œæ¨¡å‹ç”Ÿæˆçš„åŒºåŸŸæè¿°ï¼Œå¼•å…¥è¯­ä¹‰å’Œå®šä½è¯„åˆ†æ¥å…¨é¢è¯„ä¼°æ–‡æœ¬è´¨é‡å’Œå®šä½è´¨é‡ã€‚é€šè¿‡ç²¾ç¡®å®šä½ç²¾åº¦ä¼˜åŒ–æ¨¡å‹æè¿°ï¼Œå¹¶å°†æœ€é«˜åˆ†çš„ä¼˜åŒ–æè¿°ä¸æœ€ä½åˆ†çš„åˆå§‹æè¿°é…å¯¹è¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–ï¼Œä»è€Œå¢å¼ºä¸è§†è§‰è¾“å…¥çš„ç»†ç²’åº¦å¯¹é½ã€‚

**Result:** åœ¨æ ‡å‡†å¼•ç”¨å’Œå®šä½åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSPRæ–¹æ³•æœ‰æ•ˆæå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œä¸”è®­ç»ƒå¼€é”€æå°ï¼Œåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚

**Conclusion:** SPRæ–¹æ³•é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–æœºåˆ¶æœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦ç©ºé—´ç†è§£æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºå¢å¼ºæ¨¡å‹ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Multimodal large language models~(MLLMs) have demonstrated promising spatial
understanding capabilities, such as referencing and grounding object
descriptions. Despite their successes, MLLMs still fall short in fine-grained
spatial perception abilities, such as generating detailed region descriptions
or accurately localizing objects. Additionally, they often fail to respond to
the user's requirements for desired fine-grained spatial understanding. This
issue might arise because existing approaches primarily focus on tuning MLLMs
to model pre-annotated instruction data to inject spatial knowledge, without
direct supervision of MLLMs' actual responses. We address this issue by SPR, a
Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial
capabilities by rewarding MLLMs' detailed responses with precise object
localization over vague or inaccurate responses. With randomly selected image
regions and region descriptions from MLLMs, SPR introduces semantic and
localization scores to comprehensively evaluate the text quality and
localization quality in MLLM-generated descriptions. We also refine the MLLM
descriptions with better localization accuracy and pair the best-scored
refinement with the initial descriptions of the lowest score for direct
preference optimization, thereby enhancing fine-grained alignment with visual
input. Extensive experiments over standard referring and grounding benchmarks
show that SPR improves MLLM spatial understanding capabilities effectively with
minimal overhead in training. Data and code will be released at
https://github.com/hanqiu-hq/SPR


### [10] [DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation](https://arxiv.org/abs/2510.14376)
*Dongnam Byun, Jungwon Park, Jumgmin Ko, Changin Choi, Wonjong Rhee*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºDOSæ–¹æ³•ï¼Œé€šè¿‡ä¿®æ”¹CLIPæ–‡æœ¬åµŒå…¥æ¥æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤šå¯¹è±¡åœºæ™¯ä¸‹çš„æ€§èƒ½ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹è±¡å¿½ç•¥å’Œæ··åˆé—®é¢˜ï¼Œåœ¨äººç±»è¯„ä¼°ä¸­ä¼˜äºå››ç§ç«äº‰æ–¹æ³•26.24%-43.04%ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†åŒ…å«å¤šä¸ªå¯¹è±¡çš„æç¤ºæ—¶å­˜åœ¨å¯¹è±¡å¿½ç•¥å’Œå¯¹è±¡æ··åˆé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›¸ä¼¼å½¢çŠ¶ã€ç›¸ä¼¼çº¹ç†ã€ä¸åŒèƒŒæ™¯åå·®å’Œå¤§é‡å¯¹è±¡å››ç§åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚

**Method:** åŸºäºå¯¹CLIPåµŒå…¥çš„ä¸¤ä¸ªå…³é”®è§‚å¯Ÿï¼Œæå‡ºäº†DOSæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å°†æ–‡æœ¬åµŒå…¥è¾“å…¥æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¹‹å‰ä¿®æ”¹ä¸‰ç§ç±»å‹çš„CLIPæ–‡æœ¬åµŒå…¥ï¼Œä»è€Œæ”¹å–„å¤šå¯¹è±¡å›¾åƒç”Ÿæˆä¸­çš„å¯¹è±¡åˆ†ç¦»æ•ˆæœã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºDOSæ–¹æ³•æŒç»­æé«˜äº†å¤šå¯¹è±¡å›¾åƒç”Ÿæˆçš„æˆåŠŸç‡å¹¶å‡å°‘äº†å¯¹è±¡æ··åˆç°è±¡ï¼Œåœ¨äººç±»è¯„ä¼°ä¸­æ˜¾è‘—ä¼˜äºå››ç§ç«äº‰æ–¹æ³•ï¼Œåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†26.24%-43.04%æ›´å¤šçš„æŠ•ç¥¨æ”¯æŒã€‚

**Conclusion:** DOSæ–¹æ³•ä¸ºè§£å†³å¤šå¯¹è±¡å›¾åƒç”Ÿæˆä¸­çš„å…³é”®æŒ‘æˆ˜æä¾›äº†å®ç”¨æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ”¹è¿›CLIPåµŒå…¥å¤„ç†æœºåˆ¶æ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡ï¼Œä¸ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†é‡è¦æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Recent progress in text-to-image (T2I) generative models has led to
significant improvements in generating high-quality images aligned with text
prompts. However, these models still struggle with prompts involving multiple
objects, often resulting in object neglect or object mixing. Through extensive
studies, we identify four problematic scenarios, Similar Shapes, Similar
Textures, Dissimilar Background Biases, and Many Objects, where inter-object
relationships frequently lead to such failures. Motivated by two key
observations about CLIP embeddings, we propose DOS (Directional Object
Separation), a method that modifies three types of CLIP text embeddings before
passing them into text-to-image models. Experimental results show that DOS
consistently improves the success rate of multi-object image generation and
reduces object mixing. In human evaluations, DOS significantly outperforms four
competing methods, receiving 26.24%-43.04% more votes across four benchmarks.
These results highlight DOS as a practical and effective solution for improving
multi-object image generation.


### [11] [Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models](https://arxiv.org/abs/2510.14526)
*Yunze Tong, Didi Zhu, Zijing Hu, Jinluan Yang, Ziyu Zhao*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å™ªå£°æŠ•å½±å™¨ï¼Œé€šè¿‡åœ¨å»å™ªå‰å¯¹åˆå§‹å™ªå£°è¿›è¡Œæ–‡æœ¬æ¡ä»¶åŒ–ç²¾ç‚¼ï¼Œå°†å™ªå£°æ˜ å°„åˆ°ä¸è®­ç»ƒåˆ†å¸ƒæ›´åŒ¹é…çš„æç¤ºæ„ŸçŸ¥å¯¹åº”ç‰©ï¼Œä»è€Œè§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„è®­ç»ƒ-æ¨ç†ä¸åŒ¹é…é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ–‡æœ¬-å›¾åƒå¯¹é½æ•ˆæœã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬æ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„è®­ç»ƒ-æ¨ç†ä¸åŒ¹é…é—®é¢˜ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæç¤ºæ¡ä»¶åŒ–å™ªå£°ä½äºæ½œåœ¨ç©ºé—´çš„æç¤ºç‰¹å®šå­é›†ä¸­ï¼Œè€Œåœ¨æ¨ç†æ—¶å™ªå£°æ˜¯ä»æç¤ºæ— å…³çš„é«˜æ–¯å…ˆéªŒä¸­é‡‡æ ·çš„ï¼Œè¿™ç§åˆ†å¸ƒå·®å¼‚å¯¼è‡´ç”Ÿæˆçš„å›¾åƒä¸æç¤ºå¯¹é½ä¸ä½³ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå™ªå£°æŠ•å½±å™¨æ¡†æ¶ï¼Œé¦–å…ˆé‡‡æ ·å™ªå£°å¹¶é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹è·å–å¯¹åº”å›¾åƒçš„tokençº§åé¦ˆï¼Œç„¶åå°†è¿™äº›ä¿¡å·è’¸é¦åˆ°å¥–åŠ±æ¨¡å‹ä¸­ï¼Œæœ€åé€šè¿‡å‡†ç›´æ¥åå¥½ä¼˜åŒ–æ¥ä¼˜åŒ–å™ªå£°æŠ•å½±å™¨ï¼Œè¯¥è®¾è®¡æ— éœ€å‚è€ƒå›¾åƒæˆ–æ‰‹å·¥å…ˆéªŒï¼Œä¸”æ¨ç†æˆæœ¬ä½ã€‚

**Result:** å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æç¤ºæ„ŸçŸ¥å™ªå£°æŠ•å½±æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡å¤šæ ·æç¤ºä¸‹çš„æ–‡æœ¬-å›¾åƒå¯¹é½æ•ˆæœï¼Œç›¸æ¯”å¤šæ ·æœ¬é€‰æ‹©æ–¹æ³•ä»…éœ€å•æ¬¡å‰å‘ä¼ æ’­ï¼Œåœ¨ä¿æŒç”Ÿæˆå¤šæ ·æ€§çš„åŒæ—¶æé«˜äº†å¯¹é½è´¨é‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†è®­ç»ƒ-æ¨ç†åˆ†å¸ƒä¸åŒ¹é…æ˜¯æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå¯¹é½é—®é¢˜çš„å…³é”®åŸå› ï¼Œæå‡ºçš„å™ªå£°æŠ•å½±æ–¹æ³•ä¸ºæ”¹å–„ç”Ÿæˆè´¨é‡æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œä¸”ä¸ä¿®æ”¹åŸå§‹æ‰©æ•£æ¨¡å‹ï¼Œå…·æœ‰è¾ƒå¥½çš„å®ç”¨æ€§å’Œæ‰©å±•æ€§ã€‚

---

#### ğŸ“„ Abstract
In text-to-image generation, different initial noises induce distinct
denoising paths with a pretrained Stable Diffusion (SD) model. While this
pattern could output diverse images, some of them may fail to align well with
the prompt. Existing methods alleviate this issue either by altering the
denoising dynamics or by drawing multiple noises and conducting post-selection.
In this paper, we attribute the misalignment to a training-inference mismatch:
during training, prompt-conditioned noises lie in a prompt-specific subset of
the latent space, whereas at inference the noise is drawn from a
prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
that applies text-conditioned refinement to the initial noise before denoising.
Conditioned on the prompt embedding, it maps the noise to a prompt-aware
counterpart that better matches the distribution observed during SD training,
without modifying the SD model. Our framework consists of these steps: we first
sample some noises and obtain token-level feedback for their corresponding
images from a vision-language model (VLM), then distill these signals into a
reward model, and finally optimize the noise projector via a quasi-direct
preference optimization. Our design has two benefits: (i) it requires no
reference images or handcrafted priors, and (ii) it incurs small inference
cost, replacing multi-sample selection with a single forward pass. Extensive
experiments further show that our prompt-aware noise projection improves
text-image alignment across diverse prompts.


### [12] [PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model](https://arxiv.org/abs/2510.14528)
*Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Handong Zheng, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, Yanjun Ma*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PaddleOCR-VLï¼Œä¸€ä¸ªä¸“ä¸ºæ–‡æ¡£è§£æè®¾è®¡çš„SOTAä¸”èµ„æºé«˜æ•ˆçš„æ¨¡å‹ï¼Œå…¶æ ¸å¿ƒæ˜¯PaddleOCR-VL-0.9Bâ€”â€”ä¸€ä¸ªç´§å‡‘è€Œå¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡é›†æˆåŠ¨æ€åˆ†è¾¨ç‡è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹å®ç°å‡†ç¡®çš„å…ƒç´ è¯†åˆ«ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ–‡æ¡£è§£æç³»ç»Ÿåœ¨å¤„ç†å¤šè¯­è¨€æ”¯æŒå’Œå¤æ‚å…ƒç´ è¯†åˆ«æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„å®é™…éƒ¨ç½²åœºæ™¯ä¸­ï¼Œéœ€è¦å¼€å‘æ—¢èƒ½ä¿æŒé«˜æ€§èƒ½åˆå…·æœ‰æœ€å°èµ„æºæ¶ˆè€—çš„è§£å†³æ–¹æ¡ˆã€‚

**Method:** è¯¥æ¨¡å‹é‡‡ç”¨åˆ›æ–°çš„æ¶æ„è®¾è®¡ï¼Œå°†NaViTé£æ ¼çš„åŠ¨æ€åˆ†è¾¨ç‡è§†è§‰ç¼–ç å™¨ä¸ERNIE-4.5-0.3Bè¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œæ”¯æŒ109ç§è¯­è¨€ï¼Œèƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«æ–‡æœ¬ã€è¡¨æ ¼ã€å…¬å¼å’Œå›¾è¡¨ç­‰å¤æ‚æ–‡æ¡£å…ƒç´ ã€‚

**Result:** åœ¨å¹¿æ³›ä½¿ç”¨çš„å…¬å…±åŸºå‡†æµ‹è¯•å’Œå†…éƒ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPaddleOCR-VLåœ¨é¡µé¢çº§æ–‡æ¡£è§£æå’Œå…ƒç´ çº§è¯†åˆ«æ–¹é¢å‡è¾¾åˆ°SOTAæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆï¼Œä¸é¡¶çº§VLMæ¨¡å‹ç›¸æ¯”å…·æœ‰å¼ºå¤§ç«äº‰åŠ›ï¼ŒåŒæ—¶æä¾›å¿«é€Ÿçš„æ¨ç†é€Ÿåº¦ã€‚

**Conclusion:** PaddleOCR-VLå±•ç¤ºäº†åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å®ç°èµ„æºæ•ˆç‡çš„å¯è¡Œæ€§ï¼Œå…¶ç´§å‡‘çš„æ¨¡å‹è®¾è®¡å’Œå¿«é€Ÿçš„æ¨ç†èƒ½åŠ›ä½¿å…¶éå¸¸é€‚åˆåœ¨å®é™…åœºæ™¯ä¸­éƒ¨ç½²ï¼Œä¸ºæ–‡æ¡£è§£æé¢†åŸŸæä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful vision-language model (VLM) that integrates a NaViT-style
dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to
enable accurate element recognition. This innovative model efficiently supports
109 languages and excels in recognizing complex elements (e.g., text, tables,
formulas, and charts), while maintaining minimal resource consumption. Through
comprehensive evaluations on widely used public benchmarks and in-house
benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document
parsing and element-level recognition. It significantly outperforms existing
solutions, exhibits strong competitiveness against top-tier VLMs, and delivers
fast inference speeds. These strengths make it highly suitable for practical
deployment in real-world scenarios.


### [13] [Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology](https://arxiv.org/abs/2510.14532)
*Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†DentVFMï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºç‰™ç§‘è®¾è®¡çš„è§†è§‰åŸºç¡€æ¨¡å‹å®¶æ—ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ åœ¨åŒ…å«çº¦160ä¸‡å¼ å¤šæ¨¡æ€æ”¾å°„å½±åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†ç‰™ç§‘AIçš„æ³›åŒ–èƒ½åŠ›ã€æ ‡ç­¾æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç‰™é¢Œé¢æ”¾å°„å­¦åœ¨å£è…”åŒ»ç–—ä¸­è‡³å…³é‡è¦ï¼Œä½†æ”¾å°„å½±åƒè§£è¯»å—é™äºä¸“ä¸šåŒ»å¸ˆçŸ­ç¼ºã€‚ç°æœ‰ç‰™ç§‘AIç³»ç»Ÿå­˜åœ¨å•æ¨¡æ€é™åˆ¶ã€ä»»åŠ¡ç‰¹å®šè®¾è®¡ä»¥åŠå¯¹æ˜‚è´µæ ‡æ³¨æ•°æ®çš„ä¾èµ–ç­‰é—®é¢˜ï¼Œé˜»ç¢äº†å…¶åœ¨å¤šæ ·åŒ–ä¸´åºŠåœºæ™¯ä¸­çš„æ³›åŒ–åº”ç”¨ã€‚

**Method:** æå‡ºäº†DentVFMè§†è§‰åŸºç¡€æ¨¡å‹å®¶æ—ï¼ŒåŸºäºVision Transformeræ¶æ„å¼€å‘äº†2Då’Œ3Då˜ä½“ï¼Œä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ åœ¨DentVistaæ•°æ®é›†ï¼ˆçº¦160ä¸‡å¼ å¤šæ¨¡æ€æ”¾å°„å½±åƒï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶æ„å»ºäº†æ¶µç›–å…«ä¸ªç‰™ç§‘äºšä¸“ä¸šçš„ç»¼åˆåŸºå‡†DentBenchã€‚

**Result:** DentVFMå±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ™ºèƒ½ï¼Œåœ¨ç–¾ç—…è¯Šæ–­ã€æ²»ç–—åˆ†æã€ç”Ÿç‰©æ ‡å¿—ç‰©è¯†åˆ«å’Œè§£å‰–æ ‡å¿—ç‚¹æ£€æµ‹åˆ†å‰²ç­‰å¤šæ ·åŒ–ç‰™ç§‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºç›‘ç£å­¦ä¹ ã€è‡ªç›‘ç£å­¦ä¹ å’Œå¼±ç›‘ç£å­¦ä¹ çš„åŸºçº¿æ–¹æ³•ï¼Œåœ¨ä¼ ç»Ÿå½±åƒä¸å¯ç”¨æ—¶ç”šè‡³èƒ½æä¾›æ¯”ç»éªŒä¸°å¯Œçš„ç‰™åŒ»æ›´å¯é çš„è·¨æ¨¡æ€è¯Šæ–­ç»“æœã€‚

**Conclusion:** DentVFMä¸ºç‰™ç§‘AIè®¾ç«‹äº†æ–°èŒƒå¼ï¼Œæä¾›äº†å¯æ‰©å±•ã€é€‚åº”æ€§å¼ºä¸”æ ‡ç­¾é«˜æ•ˆçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿæ”¹å–„æ™ºèƒ½ç‰™ç§‘åŒ»ç–—å¹¶è§£å†³å…¨çƒå£è…”åŒ»ç–—ä¸­çš„å…³é”®ç¼ºå£ï¼Œæ¨åŠ¨äº†ç‰™ç§‘AIä»ä»»åŠ¡ç‰¹å®šå‘é€šç”¨æ™ºèƒ½çš„è½¬å˜ã€‚

---

#### ğŸ“„ Abstract
Oral and maxillofacial radiology plays a vital role in dental healthcare, but
radiographic image interpretation is limited by a shortage of trained
professionals. While AI approaches have shown promise, existing dental AI
systems are restricted by their single-modality focus, task-specific design,
and reliance on costly labeled data, hindering their generalization across
diverse clinical scenarios. To address these challenges, we introduce DentVFM,
the first family of vision foundation models (VFMs) designed for dentistry.
DentVFM generates task-agnostic visual representations for a wide range of
dental applications and uses self-supervised learning on DentVista, a large
curated dental imaging dataset with approximately 1.6 million multi-modal
radiographic images from various medical centers. DentVFM includes 2D and 3D
variants based on the Vision Transformer (ViT) architecture. To address gaps in
dental intelligence assessment and benchmarks, we introduce DentBench, a
comprehensive benchmark covering eight dental subspecialties, more diseases,
imaging modalities, and a wide geographical distribution. DentVFM shows
impressive generalist intelligence, demonstrating robust generalization to
diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker
identification, and anatomical landmark detection and segmentation.
Experimental results indicate DentVFM significantly outperforms supervised,
self-supervised, and weakly supervised baselines, offering superior
generalization, label efficiency, and scalability. Additionally, DentVFM
enables cross-modality diagnostics, providing more reliable results than
experienced dentists in situations where conventional imaging is unavailable.
DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and
label-efficient model to improve intelligent dental healthcare and address
critical gaps in global oral healthcare.


### [14] [Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval](https://arxiv.org/abs/2510.14535)
*Keima Abe, Hayato Muraki, Shuhei Tomoshige, Kenichi Oishi, Hitoshi Iyatomi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºPL-SE-ADAæ¡†æ¶ï¼Œé€šè¿‡ä¼ªçº¿æ€§é£æ ¼ç¼–ç å™¨å’Œå¯¹æŠ—æ€§åŸŸé€‚åº”å®ç°è„‘éƒ¨MRå›¾åƒçš„é¢†åŸŸåè°ƒå’Œå¯è§£é‡Šè¡¨ç¤ºå­¦ä¹ ï¼Œåœ¨ä¿æŒç–¾ç—…ç›¸å…³ä¿¡æ¯çš„åŒæ—¶æä¾›é«˜å¯è§£é‡Šæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŒ»å­¦å›¾åƒå¦‚MRæ‰«æå¸¸å› æ‰«æä»ªå’Œåè®®å·®å¼‚å‡ºç°é¢†åŸŸåç§»ï¼Œè¿™ä¼šé™ä½æœºå™¨å­¦ä¹ åœ¨ç–¾ç—…åˆ†ç±»ç­‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶é€šè¿‡è§£è€¦æ½œåœ¨ç©ºé—´å–å¾—è‰¯å¥½æ•ˆæœï¼Œä½†ç¼ºä¹åŒ»ç–—åº”ç”¨å¿…éœ€çš„å¯è§£é‡Šæ€§ï¼Œå¯¼è‡´å®é™…é—®é¢˜æœªèƒ½è§£å†³ã€‚

**Method:** PL-SE-ADAæ¡†æ¶åŒ…å«ä¸¤ä¸ªç¼–ç å™¨f_Eå’Œf_SEåˆ†åˆ«æå–é¢†åŸŸä¸å˜ç‰¹å¾z_uå’Œé¢†åŸŸç‰¹å®šç‰¹å¾z_dï¼Œä»¥åŠè§£ç å™¨f_Då’Œé¢†åŸŸé¢„æµ‹å™¨g_Dã€‚é™¤äº†ç¼–ç å™¨ä¸é¢†åŸŸé¢„æµ‹å™¨ä¹‹é—´çš„å¯¹æŠ—è®­ç»ƒï¼Œæ¨¡å‹é€šè¿‡å­¦ä¹ å°†è¾“å…¥å›¾åƒxé‡æ„ä¸ºz_uå’Œz_dé‡æ„ä¹‹å’Œï¼Œç¡®ä¿åè°ƒæ€§å’Œä¿¡æ¯ä¿ç•™ã€‚

**Result:** ä¸å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼ŒPL-SE-ADAåœ¨å›¾åƒé‡æ„ã€ç–¾ç—…åˆ†ç±»å’Œé¢†åŸŸè¯†åˆ«æ–¹é¢è¾¾åˆ°åŒç­‰æˆ–æ›´ä¼˜æ€§èƒ½ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¯è§†åŒ–é¢†åŸŸæ— å…³çš„è„‘éƒ¨ç‰¹å¾å’Œé¢†åŸŸç‰¹å®šç»„ä»¶ï¼Œä¸ºæ•´ä¸ªç³»ç»Ÿæä¾›é«˜å¯è§£é‡Šæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸåè°ƒä¸­åŒæ—¶å®ç°é«˜æ€§èƒ½å’Œé«˜å¯è§£é‡Šæ€§çš„å¯è¡Œæ€§ã€‚PL-SE-ADAä¸ä»…æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè¿˜æä¾›äº†å¯¹é¢†åŸŸä¸å˜å’Œé¢†åŸŸç‰¹å®šç‰¹å¾çš„ç›´è§‚ç†è§£ï¼Œä¸ºåŒ»ç–—AIåº”ç”¨æä¾›äº†æ›´å¯é çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Medical images like MR scans often show domain shifts across imaging sites
due to scanner and protocol differences, which degrade machine learning
performance in tasks such as disease classification. Domain harmonization is
thus a critical research focus. Recent approaches encode brain images
$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then
disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and
$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these
methods often lack interpretability$-$an essential requirement in medical
applications$-$leaving practical issues unresolved. We propose
Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a
general framework for domain harmonization and interpretable representation
learning that preserves disease-relevant information in brain MR images.
PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract
$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image
$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the
encoder and domain predictor, the model learns to reconstruct the input image
$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and
$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared
to prior methods, PL-SE-ADA achieves equal or better performance in image
reconstruction, disease classification, and domain recognition. It also enables
visualization of both domain-independent brain features and domain-specific
components, offering high interpretability across the entire framework.


### [15] [Exploring Cross-Modal Flows for Few-Shot Learning](https://arxiv.org/abs/2510.14543)
*Ziqi Jiang, Yanghao Wang, Long Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªæ¨¡å‹æ— å…³çš„å¤šæ­¥è°ƒæ•´æ–¹æ³•Flow Matching Alignment (FMA)ï¼Œé€šè¿‡è·¨æ¨¡æ€é€Ÿåº¦åœºå­¦ä¹ æ¥è§£å†³å¤æ‚æ•°æ®é›†ä¸­æ¨¡æ€ç‰¹å¾é«˜åº¦çº ç¼ çš„é—®é¢˜ï¼Œç›¸æ¯”å•æ­¥PEFTæ–¹æ³•å®ç°äº†æ›´ç²¾ç¡®å’Œé²æ£’çš„å¯¹é½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ä»…æ‰§è¡Œå•æ­¥è°ƒæ•´ï¼Œå¯¹äºç‰¹å¾é«˜åº¦çº ç¼ çš„å¤æ‚æ•°æ®é›†æ¥è¯´è°ƒæ•´ä¸è¶³ï¼Œæ— æ³•å®ç°å……åˆ†çš„è·¨æ¨¡æ€å¯¹é½ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿè¿›è¡Œå¤šæ­¥æ ¡æ­£çš„è°ƒæ•´æ–¹æ³•ã€‚

**Method:** æå‡ºFlow Matching Alignmentæ–¹æ³•ï¼Œé¦–å…ˆé‡‡ç”¨å›ºå®šè€¦åˆç­–ç•¥ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­ç±»åˆ«å¯¹åº”å…³ç³»ï¼Œç„¶åä½¿ç”¨å™ªå£°å¢å¼ºç­–ç•¥ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæœ€åè®¾è®¡æ—©åœæ±‚è§£å™¨æå‰ç»ˆæ­¢å˜æ¢è¿‡ç¨‹ä»¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**Result:** åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œéª¨å¹²ç½‘ç»œä¸Šï¼ŒFMAèƒ½å¤ŸæŒç»­å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºï¼Œè¯æ˜äº†å…¶å¤šæ­¥æ ¡æ­£èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** FMAé€šè¿‡å¤šæ­¥è°ƒæ•´æœºåˆ¶å®ç°äº†æ›´ç²¾ç¡®çš„è·¨æ¨¡æ€å¯¹é½ï¼Œä¸ºå¤æ‚åœºæ™¯ä¸‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæä¾›äº†æ–°æ€è·¯ï¼Œåœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å›°éš¾æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚

---

#### ğŸ“„ Abstract
Aligning features from different modalities, is one of the most fundamental
challenges for cross-modal tasks. Although pre-trained vision-language models
can achieve a general alignment between image and text, they often require
parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT
methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively
fine-tune a subset of parameters, which can slightly adjust either visual or
textual features, and avoid overfitting. In this paper, we are the first to
highlight that all existing PEFT methods perform one-step adjustment. It is
insufficient for complex (or difficult) datasets, where features of different
modalities are highly entangled. To this end, we propose the first
model-agnostic multi-step adjustment approach by learning a cross-modal
velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the
correspondence between categories during training, we first utilize a fixed
coupling strategy. Then, we propose a noise augmentation strategy to alleviate
the data scarcity issue. Finally, we design an early-stopping solver, which
terminates the transformation process earlier, improving both efficiency and
accuracy. Compared with one-step PEFT methods, FMA has the multi-step
rectification ability to achieve more precise and robust alignment. Extensive
results have demonstrated that FMA can consistently yield significant
performance gains across various benchmarks and backbones, particularly on
challenging datasets.


### [16] [Consistent text-to-image generation via scene de-contextualization](https://arxiv.org/abs/2510.14553)
*Song Tang, Peihao Gong, Kunyu Li, Kai Guo, Boyu Wang, Mao Ye, Jianwei Zhang, Xiatian Zhu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºåœºæ™¯å»ä¸Šä¸‹æ–‡åŒ–ï¼ˆSDeCï¼‰çš„è®­ç»ƒè‡ªç”±æç¤ºåµŒå…¥ç¼–è¾‘æ–¹æ³•ï¼Œé€šè¿‡æŠ‘åˆ¶æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­å›ºæœ‰çš„åœºæ™¯-èº«ä»½ç›¸å…³æ€§ï¼Œæ˜¾è‘—æå‡äº†è·¨åœºæ™¯çš„èº«ä»½ä¸€è‡´æ€§ç”Ÿæˆæ•ˆæœã€‚è¯¥æ–¹æ³•æ— éœ€é¢„å…ˆçŸ¥é“æ‰€æœ‰ç›®æ ‡åœºæ™¯ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†é«˜åº¦çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰çš„ä¸€è‡´æ€§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†è·¨åœºæ™¯èº«ä»½ä¿æŒæ—¶ç»å¸¸å¤±è´¥ï¼Œä¸»è¦å½’å› äºèº«ä»½åç§»ç°è±¡ï¼Œä¸”ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å…ˆçŸ¥é“æ‰€æœ‰ç›®æ ‡åœºæ™¯çš„ä¸åˆ‡å®é™…å‡è®¾ã€‚æœ¬æ–‡æ­ç¤ºäº†åœºæ™¯ä¸Šä¸‹æ–‡åŒ–è¿™ä¸€å…³é”®é—®é¢˜æ ¹æºï¼Œå³ä¸»ä½“ä¸åœºæ™¯ä¸Šä¸‹æ–‡ä¹‹é—´çš„åŸç”Ÿç›¸å…³æ€§ï¼Œè¿™ç§ç›¸å…³æ€§åœ¨T2Iæ¨¡å‹æ‹Ÿåˆè‡ªç„¶å›¾åƒè®­ç»ƒåˆ†å¸ƒæ—¶è‡ªç„¶äº§ç”Ÿã€‚

**Method:** æœ¬æ–‡æå‡ºåœºæ™¯å»ä¸Šä¸‹æ–‡åŒ–ï¼ˆSDeCï¼‰æ–¹æ³•ï¼Œé€šè¿‡å®æ–½T2Iæ¨¡å‹å†…ç½®åœºæ™¯ä¸Šä¸‹æ–‡åŒ–çš„é€†è¿‡ç¨‹æ¥æŠ‘åˆ¶èº«ä»½æç¤ºåµŒå…¥ä¸­çš„æ½œåœ¨åœºæ™¯-èº«ä»½ç›¸å…³æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡åŒ–SVDæ–¹å‘ç¨³å®šæ€§æ¥è‡ªé€‚åº”é‡æ–°åŠ æƒç›¸åº”ç‰¹å¾å€¼ï¼Œæ— éœ€è®­ç»ƒå³å¯å®ç°é«˜æ•ˆçš„æç¤ºåµŒå…¥ç¼–è¾‘ã€‚å…³é”®åˆ›æ–°åœ¨äºæ”¯æŒæ¯ä¸ªåœºæ™¯å•ç‹¬ä½¿ç”¨ï¼Œæ— éœ€é¢„å…ˆè®¿é—®æ‰€æœ‰ç›®æ ‡åœºæ™¯ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒSDeCæ–¹æ³•åœ¨ä¿æŒåœºæ™¯å¤šæ ·æ€§çš„åŒæ—¶æ˜¾è‘—å¢å¼ºäº†èº«ä»½ä¿æŒèƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨è·¨åœºæ™¯èº«ä»½ä¸€è‡´æ€§ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†å…¶ç†è®ºæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå®é™…åº”ç”¨çš„å¯è¡Œæ€§ã€‚

**Conclusion:** æœ¬ç ”ç©¶ä»ç†è®ºä¸Šè¯æ˜äº†åœºæ™¯-èº«ä»½ç›¸å…³æ€§çš„æ™®éå­˜åœ¨æ€§ï¼Œå¹¶æ¨å¯¼äº†å…¶å¼ºåº¦çš„ç†è®ºç•Œé™ï¼Œä¸ºç†è§£T2Iç”Ÿæˆä¸­çš„èº«ä»½åç§»é—®é¢˜æä¾›äº†æ–°çš„ç†è®ºè§†è§’ã€‚SDeCæ–¹æ³•ä¸ºè§£å†³ç°å®åº”ç”¨ä¸­ç¼ºä¹å…ˆéªŒåœºæ™¯çŸ¥è¯†çš„æŒ‘æˆ˜æä¾›äº†é«˜åº¦çµæ´»å’Œé€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Consistent text-to-image (T2I) generation seeks to produce
identity-preserving images of the same subject across diverse scenes, yet it
often fails due to a phenomenon called identity (ID) shift. Previous methods
have tackled this issue, but typically rely on the unrealistic assumption of
knowing all target scenes in advance. This paper reveals that a key source of
ID shift is the native correlation between subject and scene context, called
scene contextualization, which arises naturally as T2I models fit the training
distribution of vast natural images. We formally prove the near-universality of
this scene-ID correlation and derive theoretical bounds on its strength. On
this basis, we propose a novel, efficient, training-free prompt embedding
editing approach, called Scene De-Contextualization (SDeC), that imposes an
inversion process of T2I's built-in scene contextualization. Specifically, it
identifies and suppresses the latent scene-ID correlation within the ID
prompt's embedding by quantifying the SVD directional stability to adaptively
re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene
use (one scene per prompt) without requiring prior access to all target scenes.
This makes it a highly flexible and general solution well-suited to real-world
applications where such prior knowledge is often unavailable or varies over
time. Experiments demonstrate that SDeC significantly enhances identity
preservation while maintaining scene diversity.


### [17] [Talking Points: Describing and Localizing Pixels](https://arxiv.org/abs/2510.14583)
*Matan Rusanovsky, Shimon Malnick, Shai Avidan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åƒç´ çº§è§†è§‰è¯­è¨€ç†è§£æ¡†æ¶ï¼Œé€šè¿‡äº’è¡¥çš„ç‚¹æè¿°å™¨å’Œç‚¹å®šä½å™¨ç»„ä»¶ï¼Œå®ç°äº†ä»è‡ªç„¶è¯­è¨€åˆ°åƒç´ çº§å…³é”®ç‚¹çš„ç²¾ç¡®ç†è§£ä¸å®šä½ï¼Œå¡«è¡¥äº†ç°æœ‰æ¨¡å‹åœ¨åƒç´ çº§å…³é”®ç‚¹ç†è§£æ–¹é¢çš„ç©ºç™½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è·¨æ¨¡æ€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä¸»è¦å±€é™äºç‰©ä½“çº§æˆ–åŒºåŸŸçº§çš„å®šä½èƒ½åŠ›ï¼Œç¼ºä¹é€šè¿‡è‡ªç„¶è¯­è¨€å®ç°åƒç´ çº§ç²¾ç¡®å…³é”®ç‚¹ç†è§£çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€å…³é”®èƒ½åŠ›ç¼ºå£ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»è‡ªç”±å½¢å¼çš„è¯­è¨€æè¿°ä¸­ç²¾ç¡®å®šä½åƒç´ çº§å…³é”®ç‚¹ã€‚

**Method:** æå‡ºçš„æ¡†æ¶åŒ…å«ä¸¤ä¸ªäº’è¡¥ç»„ä»¶ï¼šç‚¹æè¿°å™¨ç”Ÿæˆä¸°å¯Œçš„æƒ…å¢ƒåŒ–å…³é”®ç‚¹æè¿°ï¼Œç‚¹å®šä½å™¨ä»è¿™äº›æè¿°ä¸­å›å½’ç²¾ç¡®çš„åƒç´ åæ ‡ã€‚ä¸åŒäºä¾èµ–æ¨¡æ¿åŒ–æç¤ºæˆ–å…³é”®ç‚¹åç§°çš„ç°æœ‰æ–¹æ³•ï¼Œæœ¬æ–¹æ³•ç”Ÿæˆä»åœºæ™¯çº§ä¸Šä¸‹æ–‡åˆ°å…³é”®ç‚¹å‘¨å›´è§†è§‰ç‰¹å¾çš„å¤šå°ºåº¦è‡ªç”±å½¢å¼æè¿°ã€‚ä¸ºè§£å†³è®­ç»ƒæ•°æ®ç¼ºä¹é—®é¢˜ï¼Œæ„å»ºäº†åŒ…å«20K+å›¾åƒ-å…³é”®ç‚¹-æè¿°ä¸‰å…ƒç»„çš„LlamaPointInPartæ•°æ®é›†ï¼Œå¹¶é€šè¿‡GRPOåœ¨AP-10Kä¸Šä¼˜åŒ–ç‚¹æè¿°å™¨ï¼Œä½¿ç”¨å†»ç»“çš„ç‚¹å®šä½å™¨ä½œä¸ºå¥–åŠ±æ¨¡å‹æ¥æœ€å¤§åŒ–å®šä½ç²¾åº¦ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨LlamaPointInPartæ•°æ®é›†ä¸Šï¼Œæ‰€ææ¡†æ¶ç›¸æ¯”åŸºçº¿æ¨¡å‹å±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚å»ºç«‹äº†æ–°çš„è¯„ä¼°åè®®ï¼Œé€šè¿‡å®šä½å™¨æµ‹é‡é¢„æµ‹ç‚¹ä¸çœŸå®ç‚¹ä¹‹é—´çš„è·ç¦»æ¥è¯„ä¼°æè¿°è´¨é‡ï¼Œè€Œéç›´æ¥æ¯”è¾ƒæ–‡æœ¬æè¿°ã€‚è¯¥æ¡†æ¶åœ¨è·¨ç±»åˆ«æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶åƒç´ çº§å®šä½èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶çš„åŒå‘æ¡†æ¶ä¸ºå…³é”®ç‚¹å¼•å¯¼çš„å›¾åƒç†è§£å’Œè¯­è¨€å¼•å¯¼çš„ç²¾ç¡®å®šä½å¼€è¾Ÿäº†æ–°çš„åº”ç”¨æ–¹å‘ã€‚é€šè¿‡å°†è‡ªç„¶è¯­è¨€æè¿°ä¸åƒç´ çº§å®šä½ç›¸ç»“åˆï¼Œæ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨¡å‹å‘æ›´ç²¾ç»†ç²’åº¦çš„ç†è§£èƒ½åŠ›å‘å±•ã€‚å…¬å¼€çš„ä»£ç å’Œæ•°æ®é›†ä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€ï¼Œæœ‰æœ›ä¿ƒè¿›åƒç´ çº§è§†è§‰è¯­è¨€ç†è§£é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Vision-language models have achieved remarkable success in cross-modal
understanding. Yet, these models remain limited to object-level or region-level
grounding, lacking the capability for pixel-precise keypoint comprehension
through natural language. We introduce a novel framework for pixel level
grounding. The framework consists of two complementary components: a Point
Descriptor that generates rich, contextual descriptions of individual
keypoints, and a Point Localizer that regresses precise pixel coordinates from
these descriptions. Unlike prior work that relies on templated prompts or
keypoint names, our approach produces free-form, coarse-to-fine descriptions
that situate keypoints within their visual context. Since there is no available
dataset to train such a system, we introduce LlamaPointInPart, a carefully
curated dataset of 20K+ image-keypoint-description triplets synthesized from
multiple vision-language models, capturing multi-scale information from
scene-level context to visual features around the keypoint. For cross-category
generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the
frozen Point Localizer as a reward model to produce descriptions that maximize
localization accuracy. To evaluate our results we establish a new evaluation
protocol. Instead of comparing the text description produced by our method to
the ground truth, we use the localizer to determine how close is the predicted
point generated to the ground truth point. Experiments demonstrate superior
performance compared to baseline models on LlamaPointInPart.The bidirectional
nature of our framework should enable future applications in both
keypoint-guided image understanding and language-guided precise localization.
Our code and dataset are publicly available at
https://github.com/matanr/Talking_Points.


### [18] [Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers](https://arxiv.org/abs/2510.14594)
*Hugo Markoff, Jevgenijs Galaktionovs*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºåŠ¨ç‰©æ£€æµ‹å¹³å°çš„åˆ†å±‚é‡åˆ†ç±»ç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆSpeciesNet EfficientNetV2-Mé¢„æµ‹ä¸CLIPåµŒå…¥å’Œåº¦é‡å­¦ä¹ ï¼Œå°†é«˜çº§åˆ†ç±»å­¦æ ‡ç­¾ç»†åŒ–ä¸ºç‰©ç§çº§è¯†åˆ«ï¼Œæ˜¾è‘—æå‡äº†åŠ¨ç‰©ç‰©ç§è¯†åˆ«ç²¾åº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æœ€å…ˆè¿›çš„åŠ¨ç‰©åˆ†ç±»æ¨¡å‹å¦‚SpeciesNetè™½ç„¶èƒ½å¯¹æ•°åƒç§ç‰©ç§è¿›è¡Œé¢„æµ‹ï¼Œä½†é‡‡ç”¨ä¿å®ˆçš„æ±‡æ€»ç­–ç•¥ï¼Œå¯¼è‡´è®¸å¤šåŠ¨ç‰©ä»…è¢«æ ‡è®°åœ¨é«˜çº§åˆ†ç±»å­¦å±‚çº§è€Œéç‰©ç§çº§åˆ«ï¼Œè¿™é™åˆ¶äº†ç²¾ç¡®ç‰©ç§è¯†åˆ«çš„èƒ½åŠ›ã€‚

**Method:** å¼€å‘äº†ä¸€ä¸ªäº”é˜¶æ®µåˆ†å±‚é‡åˆ†ç±»æµæ°´çº¿ï¼ŒåŒ…æ‹¬é«˜ç½®ä¿¡åº¦æ¥å—ã€é¸Ÿç±»è¦†ç›–ã€è´¨å¿ƒæ„å»ºã€ä¸‰å…ƒç»„æŸå¤±åº¦é‡å­¦ä¹ å’Œè‡ªé€‚åº”ä½™å¼¦è·ç¦»è¯„åˆ†ï¼Œç»“åˆSpeciesNet EfficientNetV2-Mé¢„æµ‹ä¸CLIPåµŒå…¥å’Œåº¦é‡å­¦ä¹ æŠ€æœ¯ã€‚

**Result:** åœ¨LILA BC Desert Lion Conservationæ•°æ®é›†ï¼ˆ4,018å¼ å›¾åƒï¼Œ15,031ä¸ªæ£€æµ‹ï¼‰ä¸Šè¯„ä¼°ï¼Œä»'ç©ºç™½'å’Œ'åŠ¨ç‰©'æ ‡ç­¾ä¸­æ¢å¤äº†761ä¸ªé¸Ÿç±»æ£€æµ‹ï¼Œå¹¶ä»¥96.5%çš„å‡†ç¡®ç‡é‡æ–°åˆ†ç±»äº†456ä¸ªæ ‡è®°ä¸ºåŠ¨ç‰©ã€å“ºä¹³åŠ¨ç‰©æˆ–ç©ºç™½çš„æ£€æµ‹ï¼Œå®ç°äº†64.9%çš„ç‰©ç§çº§è¯†åˆ«ç‡ã€‚

**Conclusion:** è¯¥åˆ†å±‚é‡åˆ†ç±»ç³»ç»Ÿæœ‰æ•ˆè§£å†³äº†ç°æœ‰åŠ¨ç‰©åˆ†ç±»æ¨¡å‹åœ¨ç‰©ç§çº§è¯†åˆ«ä¸Šçš„å±€é™æ€§ï¼Œè¯æ˜äº†ç»“åˆå¤šæ¨¡æ€åµŒå…¥å’Œåº¦é‡å­¦ä¹ èƒ½å¤Ÿæ˜¾è‘—æå‡ç»†ç²’åº¦ç‰©ç§è¯†åˆ«æ€§èƒ½ï¼Œä¸ºé‡ç”ŸåŠ¨ç‰©ç›‘æµ‹å’Œä¿æŠ¤æä¾›äº†æ›´ç²¾ç¡®çš„æŠ€æœ¯æ”¯æŒã€‚

---

#### ğŸ“„ Abstract
State-of-the-art animal classification models like SpeciesNet provide
predictions across thousands of species but use conservative rollup strategies,
resulting in many animals labeled at high taxonomic levels rather than species.
We present a hierarchical re-classification system for the Animal Detect
platform that combines SpeciesNet EfficientNetV2-M predictions with CLIP
embeddings and metric learning to refine high-level taxonomic labels toward
species-level identification. Our five-stage pipeline (high-confidence
acceptance, bird override, centroid building, triplet-loss metric learning, and
adaptive cosine-distance scoring) is evaluated on a segment of the LILA BC
Desert Lion Conservation dataset (4,018 images, 15,031 detections). After
recovering 761 bird detections from "blank" and "animal" labels, we re-classify
456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving
species-level identification for 64.9 percent


### [19] [Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering](https://arxiv.org/abs/2510.14596)
*Hugo Markoff, Jevgenijs Galaktionovs*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯„ä¼°äº†è‡ªç›‘ç£è§†è§‰å˜æ¢å™¨åœ¨é‡ç”ŸåŠ¨ç‰©å›¾åƒé›¶æ ·æœ¬åˆ†ç±»ä¸­çš„è¡¨ç°ï¼Œåœ¨5ç‰©ç§æµ‹è¯•é›†ä¸ŠDINOv2ç»“åˆUMAPå’ŒGMMè¾¾åˆ°88.6%å‡†ç¡®ç‡ï¼Œå¹¶å°†è¿ç»­ç›¸ä¼¼æ€§æ’åºéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ˜¾è‘—åŠ é€Ÿäº†ç”Ÿç‰©å¤šæ ·æ€§ç›‘æµ‹çš„æ‰‹åŠ¨æ ‡æ³¨æµç¨‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç›¸æœºé™·é˜±äº§ç”Ÿæ•°ç™¾ä¸‡å¼ é‡ç”ŸåŠ¨ç‰©å›¾åƒï¼Œä½†è®¸å¤šæ•°æ®é›†åŒ…å«ç°æœ‰åˆ†ç±»å™¨æœªæ¶µç›–çš„ç‰©ç§ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿå¤„ç†æœªæ ‡è®°é‡ç”ŸåŠ¨ç‰©å›¾åƒçš„é›¶æ ·æœ¬æ–¹æ³•æ¥è§£å†³è¿™ä¸€æ•°æ®æ ‡æ³¨ç“¶é¢ˆã€‚

**Method:** ç ”ç©¶æ¯”è¾ƒäº†ä¸‰ç§æ¶æ„ï¼ˆCLIPã€DINOv2ã€MegaDescriptorï¼‰ä¸æ— ç›‘ç£èšç±»æ–¹æ³•ï¼ˆDBSCANã€GMMï¼‰çš„ç»“åˆï¼Œå¹¶é‡‡ç”¨é™ç»´æŠ€æœ¯ï¼ˆPCAã€UMAPï¼‰å’Œt-SNEæŠ•å½±å®ç°è¿ç»­ä¸€ç»´ç›¸ä¼¼æ€§æ’åºï¼Œåœ¨Animal Detectå¹³å°ä¸Šè¿›è¡Œå¼€å‘å’Œæµ‹è¯•ã€‚

**Result:** åœ¨ä»…ç”¨äºè¯„ä¼°çš„5ç‰©ç§æµ‹è¯•é›†ä¸Šï¼ŒDINOv2ä¸UMAPå’ŒGMMç»„åˆè¾¾åˆ°88.6%å‡†ç¡®ç‡ï¼ˆå®F1=0.874ï¼‰ï¼Œä¸€ç»´æ’åºåœ¨1500å¼ å›¾åƒä¸­å“ºä¹³åŠ¨ç‰©å’Œé¸Ÿç±»è¾¾åˆ°88.2%ä¸€è‡´æ€§ï¼Œé±¼ç±»è¾¾åˆ°95.2%ä¸€è‡´æ€§ã€‚

**Conclusion:** åŸºäºå®éªŒç»“æœï¼Œè¿ç»­ç›¸ä¼¼æ€§æ’åºå·²éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œèƒ½å¤Ÿå®ç°å¿«é€Ÿæ¢ç´¢æ€§åˆ†æå¹¶æ˜¾è‘—åŠ é€Ÿç”Ÿç‰©å¤šæ ·æ€§ç›‘æµ‹çš„æ‰‹åŠ¨æ ‡æ³¨å·¥ä½œæµç¨‹ï¼Œä¸ºé›¶æ ·æœ¬é‡ç”ŸåŠ¨ç‰©å›¾åƒç»„ç»‡æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Camera traps generate millions of wildlife images, yet many datasets contain
species that are absent from existing classifiers. This work evaluates
zero-shot approaches for organizing unlabeled wildlife imagery using
self-supervised vision transformers, developed and tested within the Animal
Detect platform for camera trap analysis. We compare unsupervised clustering
methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)
combined with dimensionality reduction techniques (PCA, UMAP), and we
demonstrate continuous 1D similarity ordering via t-SNE projection. On a
5-species test set with ground truth labels used only for evaluation, DINOv2
with UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D
sorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent
for fish across 1,500 images. Based on these findings, we deployed continuous
similarity ordering in production, enabling rapid exploratory analysis and
accelerating manual annotation workflows for biodiversity monitoring.


### [20] [Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering](https://arxiv.org/abs/2510.14605)
*Yuyang Hong, Jiaqi Gu, Qi Yang, Lubin Fan, Yue Wu, Ying Wang, Kun Ding, Shiming Xiang, Jieping Ye*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºWiki-PRFçš„ä¸‰é˜¶æ®µæ–¹æ³•ï¼Œé€šè¿‡å¤„ç†ã€æ£€ç´¢å’Œè¿‡æ»¤é˜¶æ®µæ”¹è¿›åŸºäºçŸ¥è¯†çš„è§†è§‰é—®ç­”ä»»åŠ¡ï¼Œç»“åˆè§†è§‰å·¥å…·è°ƒç”¨å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œåœ¨E-VQAå’ŒInfoSeekåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŸºäºçŸ¥è¯†çš„è§†è§‰é—®ç­”ä»»åŠ¡ä¸­ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•è™½ç„¶å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨å¤šæ¨¡æ€æŸ¥è¯¢è´¨é‡å’Œæ£€ç´¢ç»“æœç›¸å…³æ€§æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ•´åˆè§†è§‰ç†è§£ä¸å¤–éƒ¨çŸ¥è¯†æ£€ç´¢æ—¶çš„å±€é™æ€§ã€‚

**Method:** æå‡ºäº†ä¸‰é˜¶æ®µçš„Wiki-PRFæ–¹æ³•ï¼šå¤„ç†é˜¶æ®µåŠ¨æ€è°ƒç”¨è§†è§‰å·¥å…·æå–ç²¾ç¡®çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼›æ£€ç´¢é˜¶æ®µæ•´åˆè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾å®ç°å¤šæ¨¡æ€çŸ¥è¯†æ£€ç´¢ï¼›è¿‡æ»¤é˜¶æ®µå¯¹æ£€ç´¢ç»“æœè¿›è¡Œç›¸å…³æ€§è¿‡æ»¤å’Œé›†ä¸­ï¼Œå¹¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»¥ç­”æ¡ˆå‡†ç¡®æ€§å’Œæ ¼å¼ä¸€è‡´æ€§ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚

**Result:** åœ¨E-VQAå’ŒInfoSeekåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç­”æ¡ˆè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ï¼ˆ36.0å’Œ42.8ï¼‰ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µæ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå¯ä»¥æœ‰æ•ˆæå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸ºå¤šæ¨¡æ€çŸ¥è¯†æ£€ç´¢å’Œæ¨ç†æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Knowledge-based visual question answering (KB-VQA) requires visual language
models (VLMs) to integrate visual understanding with external knowledge
retrieval. Although retrieval-augmented generation (RAG) achieves significant
advances in this task by combining knowledge-base querying, it still struggles
with the quality of multimodal queries and the relevance of retrieved results.
To overcome these challenges, we propose a novel three-stage method, termed
Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing
stage dynamically invokes visual tools to extract precise multimodal
information for retrieval. The retrieval stage integrates visual and text
features to achieve multimodal knowledge retrieval. The filtering stage
performs relevance filtering and concentration on retrieval results. To this
end, we introduce a visual language model trained with answer accuracy and
format consistency as reward signals via a reinforcement learning manner. This
enhances the model's reasoning, tool invocation for accurate queries, and
filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and
InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,
achieving state-of-the-art performance. Code is available at
https://github.com/cqu-student/Wiki-PRF


### [21] [Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding](https://arxiv.org/abs/2510.14617)
*Ning Ding, Keisuke Fujii, Toru Tamaki*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Shot2Tactic-Captionæ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤ŸåŒæ—¶ç”Ÿæˆç¾½æ¯›çƒæ¯”èµ›ä¸­å‡»çƒçº§å’Œæˆ˜æœ¯çº§å¤šå°ºåº¦è§†é¢‘æè¿°çš„ç³»ç»Ÿï¼Œé€šè¿‡åŒåˆ†æ”¯è®¾è®¡å’ŒåŸºäºæç¤ºçš„æœºåˆ¶å®ç°äº†å¯¹æˆ˜æœ¯æ‰§è¡Œçš„è¯­ä¹‰å’Œæ—¶é—´ç†è§£ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç¾½æ¯›çƒæˆ˜æœ¯ç†è§£ä¸ä»…éœ€è¦è§£é‡Šå•ä¸ªåŠ¨ä½œï¼Œè¿˜éœ€è¦ç†è§£æˆ˜æœ¯å¦‚ä½•éšæ—¶é—´åŠ¨æ€æ‰§è¡Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æˆ˜æœ¯å±‚é¢åŠ¨æ€æ‰§è¡Œçš„æè¿°èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æˆ˜æœ¯ä¸­æ–­å’Œæ¢å¤ç­‰å¤æ‚åœºæ™¯æ—¶å­˜åœ¨å±€é™ã€‚

**Method:** é‡‡ç”¨åŒåˆ†æ”¯æ¶æ„è®¾è®¡ï¼ŒåŒ…å«è§†è§‰ç¼–ç å™¨ã€æ—¶ç©ºTransformerç¼–ç å™¨å’ŒåŸºäºTransformerçš„è§£ç å™¨ï¼›å¼•å…¥æˆ˜æœ¯å•å…ƒæ£€æµ‹å™¨è¯†åˆ«æœ‰æ•ˆæˆ˜æœ¯å•å…ƒã€ç±»å‹å’ŒçŠ¶æ€ï¼›æå‡ºåŸºäºå‡»çƒçš„æç¤ºå¼•å¯¼æœºåˆ¶ï¼Œå°†é¢„æµ‹çš„æˆ˜æœ¯ç±»å‹å’ŒçŠ¶æ€ä½œä¸ºæç¤ºé€šè¿‡äº¤å‰æ³¨æ„åŠ›æ³¨å…¥è§£ç å™¨ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶åœ¨ç”Ÿæˆå‡»çƒå’Œæˆ˜æœ¯æè¿°æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœï¼Œæ¶ˆèç ”ç©¶æ˜¾ç¤ºåŸºäºResNet50çš„æ—¶ç©ºç¼–ç å™¨ä¼˜äºå…¶ä»–å˜ä½“ï¼ŒåŸºäºå‡»çƒçš„æç¤ºç»“æ„èƒ½å¤Ÿäº§ç”Ÿæ›´è¿è´¯å’Œå‡†ç¡®çš„æˆ˜æœ¯æè¿°ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å¤šå°ºåº¦è§†é¢‘æè¿°åœ¨ä½“è‚²åˆ†æä¸­çš„ä»·å€¼ï¼Œæå‡ºçš„æç¤ºå¼•å¯¼æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚æˆ˜æœ¯åœºæ™¯ï¼Œä¸ºç†è§£åŠ¨æ€æˆ˜æœ¯æ‰§è¡Œæä¾›äº†æ–°æ€è·¯ï¼Œæœªæ¥å¯æ‰©å±•è‡³å…¶ä»–éœ€è¦æ—¶åºç†è§£çš„è§†é¢‘åˆ†æä»»åŠ¡ã€‚

---

#### ğŸ“„ Abstract
Tactical understanding in badminton involves interpreting not only individual
actions but also how tactics are dynamically executed over time. In this paper,
we propose \textbf{Shot2Tactic-Caption}, a novel framework for semantic and
temporal multi-scale video captioning in badminton, capable of generating
shot-level captions that describe individual actions and tactic-level captions
that capture how these actions unfold over time within a tactical execution. We
also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning
dataset containing 5,494 shot captions and 544 tactic captions.
Shot2Tactic-Caption adopts a dual-branch design, with both branches including a
visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based
decoder to generate shot and tactic captions. To support tactic captioning, we
additionally introduce a Tactic Unit Detector that identifies valid tactic
units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic
captioning, we further incorporate a shot-wise prompt-guided mechanism, where
the predicted tactic type and state are embedded as prompts and injected into
the decoder via cross-attention. The shot-wise prompt-guided mechanism enables
our system not only to describe successfully executed tactics but also to
capture tactical executions that are temporarily interrupted and later resumed.
Experimental results demonstrate the effectiveness of our framework in
generating both shot and tactic captions. Ablation studies show that the
ResNet50-based spatio-temporal encoder outperforms other variants, and that
shot-wise prompt structuring leads to more coherent and accurate tactic
captioning.


### [22] [Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference](https://arxiv.org/abs/2510.14624)
*Natan Bagrov, Eugene Khvedchenia, Borys Tymchenko, Shay Aharon, Lior Kadoch, Tomer Keren, Ofri Masad, Yonatan Geifman, Ran Zilberstein, Tuomas Rintamaki, Matthieu Le, Andrew Tao*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§†é¢‘é‡‡æ ·æ–¹æ³•EVSï¼Œé€šè¿‡è¯†åˆ«å¹¶å‰ªææ—¶é—´ä¸Šé™æ€çš„è§†è§‰è¡¥ä¸æ¥å‡å°‘è§†é¢‘å¤„ç†ä¸­çš„ä»¤ç‰Œå†—ä½™ï¼Œå®ç°äº†åœ¨ä¸ç‰ºç‰²è¯­ä¹‰ä¿çœŸåº¦çš„å‰æä¸‹æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œä¸ºå¯æ‰©å±•çš„è§†é¢‘è¯­è¨€ç†è§£æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è§†é¢‘æ—¶é¢ä¸´ä¸¥é‡çš„å¯æ‰©å±•æ€§é™åˆ¶ï¼Œå¯†é›†å¸§åºåˆ—çš„äºŒæ¬¡è®¡ç®—æˆæœ¬å¯¼è‡´ä»¤ç‰Œé¢„ç®—ä¸è¶³ï¼Œå¼•å‘ä¸Šä¸‹æ–‡é™åˆ¶å’Œå»¶è¿Ÿé—®é¢˜ï¼Œè¿«åˆ‡éœ€è¦å‡å°‘è§†é¢‘ä¸­çš„ä»¤ç‰Œå†—ä½™ä»¥æ”¯æŒé•¿è§†é¢‘ç†è§£ã€‚

**Method:** æå‡ºäº†é«˜æ•ˆè§†é¢‘é‡‡æ ·æ–¹æ³•EVSï¼Œè¯¥æ–¹æ³•é€šè¿‡è¯†åˆ«è¿ç»­å¸§é—´ä¿æŒä¸å˜çš„æ—¶ç©ºé™æ€è¡¥ä¸å¹¶è¿›è¡Œå‰ªæï¼Œä¿ç•™äº†ä½ç½®èº«ä»½ä¿¡æ¯ï¼Œæ— éœ€æ¶æ„ä¿®æ”¹æˆ–é‡æ–°è®­ç»ƒï¼Œæ”¯æŒæ¨ç†æ—¶ç›´æ¥åº”ç”¨ã€‚

**Result:** EVSæ˜¾è‘—å‡å°‘äº†ä»¤ç‰Œæ•°é‡åŒæ—¶ä¿æŒè¯­ä¹‰ä¿çœŸåº¦ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„é¦–ä»¤ç‰Œæ—¶é—´æœ€å¤šé™ä½4å€ä¸”ç²¾åº¦æŸå¤±æœ€å°ï¼Œç»“åˆéšæœºå‰ªæç‡çš„ä¸Šè®­ç»ƒå¯äº§ç”Ÿå¯¹ä¸åŒç¨‹åº¦å‹ç¼©å…·æœ‰é²æ£’æ€§çš„æ¨¡å‹ã€‚

**Conclusion:** EVSæ–¹æ³•æœ‰æ•ˆæ”¹å–„äº†æ•ˆç‡ä¸ç²¾åº¦çš„æƒè¡¡å…³ç³»ï¼Œä¸ºå¯æ‰©å±•çš„è§†é¢‘è¯­è¨€ç†è§£å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œè¯æ˜é€šè¿‡æ™ºèƒ½ä»¤ç‰Œå‡å°‘å¯ä»¥åœ¨ä¸ç‰ºç‰²è´¨é‡çš„å‰æä¸‹å®ç°å¤§è§„æ¨¡è§†é¢‘å¤„ç†ã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLMs) have recently expanded from static image
understanding to video reasoning, but their scalability is fundamentally
limited by the quadratic cost of processing dense frame sequences. Long videos
often exceed the token budget of modern language models, leading to severe
context limitations and latency issues. We introduce Efficient Video Sampling
(EVS), a simple, plug-and-play method for reducing token redundancy in videos
by identifying and pruning temporally static patches -- spatial regions that
remain unchanged across consecutive frames. EVS preserves positional identity,
requires no architectural changes or retraining. We show that EVS substantially
reduces token count while maintaining semantic fidelity, enabling faster
inference and longer input sequences. Applied at inference time, EVS reduces
large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal
accuracy loss. When combined with an uptraining phase using stochastic pruning
rates, EVS yields models that are robust to varying compression levels and
retain full performance under aggressive pruning. Extensive experiments
demonstrate that EVS consistently improves efficiency-accuracy trade-offs,
unlocking scalable video-language understanding without sacrificing quality.


### [23] [Benchmarking Multimodal Large Language Models for Face Recognition](https://arxiv.org/abs/2510.14866)
*Hatef Otroshi Shahreza, SÃ©bastien Marcel*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é¢éƒ¨è¯†åˆ«ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå‘ç°åœ¨é›¶æ ·æœ¬åº”ç”¨ä¸­è™½ç„¶èƒ½å¤Ÿæ•æ‰ä¸°å¯Œçš„è¯­ä¹‰çº¿ç´¢ï¼Œä½†åœ¨é«˜ç²¾åº¦è¯†åˆ«åœºæ™¯ä¸­ä»è½åäºä¸“ç”¨æ¨¡å‹ã€‚è¯¥åŸºå‡†ä¸ºæ¨è¿›åŸºäºMLLMçš„é¢éƒ¨è¯†åˆ«æä¾›äº†åŸºç¡€ï¼Œå¹¶ä¸ºä¸‹ä¸€ä»£æ›´é«˜ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›çš„æ¨¡å‹è®¾è®¡æä¾›äº†è§è§£ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ€§èƒ½ï¼Œä½†å…¶åœ¨é¢éƒ¨è¯†åˆ«é¢†åŸŸçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç‰¹åˆ«æ˜¯éœ€è¦è¯„ä¼°å¼€æºMLLMsåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œå¹¶ä¸ç°æœ‰é¢éƒ¨è¯†åˆ«æ¨¡å‹åœ¨ç›¸ä¼¼åè®®ä¸‹è¿›è¡Œæ¯”è¾ƒã€‚

**Method:** æœ¬ç ”ç©¶åœ¨å¤šä¸ªé¢éƒ¨è¯†åˆ«æ•°æ®é›†ä¸Šå¯¹æœ€å…ˆè¿›çš„MLLMsè¿›è¡Œäº†ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬LFWã€CALFWã€CPLFWã€CFPã€AgeDBå’ŒRFWã€‚é€šè¿‡æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œå¯¹æ¯”åˆ†æäº†MLLMsä¸ä¸“ç”¨é¢éƒ¨è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½å·®å¼‚ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶MLLMsèƒ½å¤Ÿæ•æ‰å¯¹é¢éƒ¨ç›¸å…³ä»»åŠ¡æœ‰ç”¨çš„ä¸°å¯Œè¯­ä¹‰çº¿ç´¢ï¼Œä½†åœ¨é›¶æ ·æœ¬åº”ç”¨çš„é«˜ç²¾åº¦è¯†åˆ«åœºæ™¯ä¸­ï¼Œå®ƒä»¬ä»è½åäºä¸“ç”¨æ¨¡å‹ã€‚åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¯„ä¼°æ­ç¤ºäº†MLLMsåœ¨å½“å‰æŠ€æœ¯æ°´å¹³ä¸‹çš„å±€é™æ€§ã€‚

**Conclusion:** è¯¥åŸºå‡†ä¸ºæ¨è¿›åŸºäºMLLMçš„é¢éƒ¨è¯†åˆ«ç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€ï¼Œæ­ç¤ºäº†å½“å‰MLLMsåœ¨é¢éƒ¨è¯†åˆ«ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¾¹ç•Œã€‚ç ”ç©¶ç»“æœä¸ºè®¾è®¡å…·æœ‰æ›´é«˜ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›çš„ä¸‹ä¸€ä»£æ¨¡å‹æä¾›äº†å…³é”®è§è§£ï¼ŒæŒ‡å‡ºäº†æ”¹è¿›æ–¹å‘å’Œå‘å±•æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Multimodal large language models (MLLMs) have achieved remarkable performance
across diverse vision-and-language tasks. However, their potential in face
recognition remains underexplored. In particular, the performance of
open-source MLLMs needs to be evaluated and compared with existing face
recognition models on standard benchmarks with similar protocol. In this work,
we present a systematic benchmark of state-of-the-art MLLMs for face
recognition on several face recognition datasets, including LFW, CALFW, CPLFW,
CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich
semantic cues useful for face-related tasks, they lag behind specialized models
in high-precision recognition scenarios in zero-shot applications. This
benchmark provides a foundation for advancing MLLM-based face recognition,
offering insights for the design of next-generation models with higher accuracy
and generalization. The source code of our benchmark is publicly available in
the project page.


### [24] [Adapting Self-Supervised Representations as a Latent Space for Efficient Generation](https://arxiv.org/abs/2510.14630)
*Ming Gui, Johannes Schusterbauer, Timy Phan, Felix Krause, Josh Susskind, Miguel Angel Bautista, BjÃ¶rn Ommer*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†è¡¨ç¤ºåˆ†è¯å™¨ï¼ˆRepTokï¼‰ï¼Œä¸€ç§åŸºäºè‡ªç›‘ç£è§†è§‰å˜æ¢å™¨çš„ç”Ÿæˆå»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡å•ä¸ªè¿ç»­æ½œåœ¨tokenè¡¨ç¤ºå›¾åƒï¼Œåœ¨ä¿æŒé«˜æ•ˆè®­ç»ƒçš„åŒæ—¶å®ç°ç«äº‰æ€§çš„ç”Ÿæˆæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿ2Dæ½œåœ¨ç©ºé—´åœ¨ç”Ÿæˆå»ºæ¨¡ä¸­çš„ç©ºé—´å†—ä½™é—®é¢˜ï¼ŒåŒæ—¶æ¢ç´¢å¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒè‡ªç›‘ç£è¡¨ç¤ºæ„å»ºç´§å‡‘ä¸”æœ‰æ•ˆçš„æ½œåœ¨ç©ºé—´ï¼Œä»¥æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬å¹¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚

**Method:** RepTokæ¡†æ¶åŸºäºé¢„è®­ç»ƒçš„SSLç¼–ç å™¨ï¼Œä»…å¾®è°ƒè¯­ä¹‰tokenåµŒå…¥ï¼Œå¹¶ä¸ä½¿ç”¨æµåŒ¹é…ç›®æ ‡è”åˆè®­ç»ƒçš„ç”Ÿæˆè§£ç å™¨é…å¯¹ã€‚é€šè¿‡æ·»åŠ ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±æ¥æ­£åˆ™åŒ–é€‚åº”åçš„tokenï¼Œä¿æŒåŸå§‹SSLç©ºé—´çš„æœ‰åˆ©å‡ ä½•ç‰¹æ€§ï¼ŒåŒæ—¶ä¸°å¯ŒtokenåŒ…å«çš„ä½çº§é‡å»ºç›¸å…³ä¿¡æ¯ã€‚

**Result:** åœ¨ç±»åˆ«æ¡ä»¶ImageNetç”Ÿæˆä¸Šå–å¾—ç«äº‰æ€§ç»“æœï¼Œåœ¨MS-COCOæ–‡æœ¬åˆ°å›¾åƒåˆæˆä»»åŠ¡ä¸­ï¼Œåœ¨ææœ‰é™è®­ç»ƒé¢„ç®—ä¸‹è¾¾åˆ°ç«äº‰æ€§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜å¾®è°ƒåçš„SSLè¡¨ç¤ºå¯ä»¥ä½œä¸ºç´§å‡‘æœ‰æ•ˆçš„æ½œåœ¨ç©ºé—´ç”¨äºé«˜æ•ˆç”Ÿæˆå»ºæ¨¡ï¼Œå•tokenå…¬å¼è§£å†³äº†2Dæ½œåœ¨ç©ºé—´çš„ç©ºé—´å†—ä½™é—®é¢˜ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„é«˜è´¨é‡ç”Ÿæˆæä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
We introduce Representation Tokenizer (RepTok), a generative modeling
framework that represents an image using a single continuous latent token
obtained from self-supervised vision transformers. Building on a pre-trained
SSL encoder, we fine-tune only the semantic token embedding and pair it with a
generative decoder trained jointly using a standard flow matching objective.
This adaptation enriches the token with low-level, reconstruction-relevant
details, enabling faithful image reconstruction. To preserve the favorable
geometry of the original SSL space, we add a cosine-similarity loss that
regularizes the adapted token, ensuring the latent space remains smooth and
suitable for generation. Our single-token formulation resolves spatial
redundancies of 2D latent spaces and significantly reduces training costs.
Despite its simplicity and efficiency, RepTok achieves competitive results on
class-conditional ImageNet generation and naturally extends to text-to-image
synthesis, reaching competitive zero-shot performance on MS-COCO under
extremely limited training budgets. Our findings highlight the potential of
fine-tuned SSL representations as compact and effective latent spaces for
efficient generative modeling.


### [25] [You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction](https://arxiv.org/abs/2510.14885)
*Logan Lawrence, Oindrila Saha, Megan Wei, Chen Sun, Subhransu Maji, Grant Van Horn*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†nlg2choiceæ–¹æ³•ï¼Œä¸€ç§ç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸­çš„ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡å¼€æ”¾é—®ç­”ä¸çº¦æŸè§£ç ç›¸ç»“åˆçš„æ–¹å¼ï¼Œæœ‰æ•ˆè§£å†³äº†é«˜ç»´å¤šé€‰åœºæ™¯ä¸‹çš„åˆ†ç±»ä¸æ£€ç´¢é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰é›¶æ ·æœ¬è§†è§‰åˆ†ç±»è¯„ä¼°é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨çº¯è¯­è¨€ä»»åŠ¡æˆ–å±€é™äº5é€‰é¡¹ä»¥å†…çš„å¤šé€‰é¢˜ï¼Œè€Œç»†ç²’åº¦è§†è§‰åˆ†ç±»ä»»åŠ¡é€šå¸¸æ¶‰åŠæ•°ç™¾è‡³æ•°åƒä¸ªé«˜åº¦ç›¸å…³çš„é€‰é¡¹ï¼›åŒæ—¶åœ¨é«˜ç»´å¤šé€‰è®¾ç½®ä¸‹ï¼Œå¦‚ä½•å°†LLMé€‰æ‹©æå–æ‰©å±•åˆ°åŸºäºæ£€ç´¢çš„é—®é¢˜ä¸­ï¼Œé¿å…å¯¹é€‰æ‹©é›†è¿›è¡Œæ¦‚ç‡è®¡ç®—çš„å·¨å¤§è®¡ç®—æˆæœ¬ã€‚

**Method:** æå‡ºäº†nlg2choiceä¸¤é˜¶æ®µæ–¹æ³•ï¼šé¦–å…ˆå‘MLLMæå‡ºæ— çº¦æŸçš„å¼€æ”¾æ€§é—®é¢˜ï¼Œç„¶åä½¿ç”¨çº¯æ–‡æœ¬çº¦æŸè§£ç æ¥é¢„æµ‹æœ€å¯èƒ½çš„é€‰æ‹©ï¼›åœ¨æ£€ç´¢è®¾ç½®ä¸­ï¼Œé‡‡ç”¨æå‰åœæ­¢æ–¹æ³•è®¡ç®—çº¦æŸå“åº”é€‰æ‹©è¯¥é€‰é¡¹çš„æ¦‚ç‡ï¼Œæ˜¾è‘—æé«˜äº†å¤„ç†ååé‡ã€‚

**Result:** åœ¨ä¸ƒä¸ªç»†ç²’åº¦è§†è§‰æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†ç±»å’Œæ£€ç´¢è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºæ”¹è¿›ï¼Œå¹¶ä¸”è¿™ç§æ€§èƒ½ä¼˜åŠ¿åœ¨ä¸åŒè‡ªç„¶è¯­è¨€ä»»åŠ¡å®ç°æ–¹å¼ä¸‹ä¿æŒç¨³å®šã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åœ¨é«˜åº¦å¤šè·¯å¤šé€‰é¢˜è®¾ç½®ä¸‹ï¼Œç»“åˆå¼€æ”¾é—®ç­”ä¸çº¦æŸè§£ç çš„ä¸¤é˜¶æ®µæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£å†³ç»†ç²’åº¦è§†è§‰åˆ†ç±»ä¸­çš„è¯„ä¼°æŒ‘æˆ˜ï¼Œä¸ºMLLMåœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„æ€§èƒ½è¯„ä¼°æä¾›äº†å®ç”¨æ¡†æ¶ï¼ŒåŒæ—¶é€šè¿‡è®¡ç®—ä¼˜åŒ–ç¡®ä¿äº†æ–¹æ³•çš„å®é™…å¯è¡Œæ€§ã€‚

---

#### ğŸ“„ Abstract
Despite the renewed interest in zero-shot visual classification due to the
rise of Multimodal Large Language Models (MLLMs), the problem of evaluating
free-form responses of auto-regressive models remains a persistent challenge.
Most existing works focus on language-only tasks or don't consider Multiple
Choice Questions (MCQs) beyond 5-way options, both of which are critical
capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where
choice counts are in the hundreds to thousands and the choices are highly
related. Furthermore, in this highly multi-way MCQ setting it is not clear how
to extend LLM choice extraction to retrieval-based problems, where computing
probabilities over the choice set is computationally costly. In this work we
investigate nlg2choice, a simple two-stage method which first asks the MLLM an
open-ended question for the task with minimal constraints, then uses text-only
constrained decoding to predict the most likely choice. In retrieval settings,
we compute the probability of the constrained response taking that choice with
an early stopping method to significantly improve throughput. Our results show
improvement over a suite of seven fine-grained visual datasets when evaluating
in terms of classification and retrieval, and show that this performance holds
over the various ways that users of LLMs can implement tasks in natural
language.


### [26] [In-Context Learning with Unpaired Clips for Instruction-based Video Editing](https://arxiv.org/abs/2510.14648)
*Xinyao Liao, Xianfang Zeng, Ziye Song, Zhoujie Fu, Gang Yu, Guosheng Lin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæŒ‡ä»¤çš„è§†é¢‘ç¼–è¾‘é¢„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ä»éé…å¯¹è§†é¢‘ç‰‡æ®µä¸­å­¦ä¹ ç¼–è¾‘æ¦‚å¿µï¼Œæ˜¾è‘—é™ä½äº†å¤§è§„æ¨¡é…å¯¹è§†é¢‘ç¼–è¾‘æ•°æ®é›†çš„æ„å»ºæˆæœ¬ï¼Œå¹¶åœ¨ç¼–è¾‘æŒ‡ä»¤éµå¾ªå’Œè§†è§‰è´¨é‡æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œä½†å…¶åœ¨è§†é¢‘é¢†åŸŸçš„æ‰©å±•ä»æœªè¢«å……åˆ†æ¢ç´¢ï¼Œä¸»è¦éšœç¢åœ¨äºæ„å»ºå¤§è§„æ¨¡é…å¯¹è§†é¢‘ç¼–è¾‘æ•°æ®é›†çš„é«˜æ˜‚æˆæœ¬å’Œå¤æ‚æ€§ï¼Œè¿™é™åˆ¶äº†æŒ‡ä»¤è§†é¢‘ç¼–è¾‘æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚

**Method:** é‡‡ç”¨ä½æˆæœ¬çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨éé…å¯¹è§†é¢‘ç‰‡æ®µè¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä½¿åŸºç¡€è§†é¢‘ç”Ÿæˆæ¨¡å‹è·å¾—é€šç”¨ç¼–è¾‘èƒ½åŠ›ï¼›é¦–å…ˆåœ¨çº¦100ä¸‡ä¸ªçœŸå®è§†é¢‘ç‰‡æ®µä¸Šè¿›è¡Œé¢„è®­ç»ƒå­¦ä¹ åŸºæœ¬ç¼–è¾‘æ¦‚å¿µï¼Œç„¶ååœ¨å°‘äº15ä¸‡ä¸ªç²¾é€‰ç¼–è¾‘å¯¹ä¸Šè¿›è¡Œå¾®è°ƒä»¥æ‰©å±•ç¼–è¾‘ä»»åŠ¡å¹¶æå‡ç¼–è¾‘è´¨é‡ï¼Œè¯¥æ¡†æ¶åŸºäºHunyuanVideoT2Væ„å»ºã€‚

**Result:** æ¯”è¾ƒå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¼–è¾‘æŒ‡ä»¤éµå¾ªå’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢å‡è¶…è¶Šäº†ç°æœ‰çš„åŸºäºæŒ‡ä»¤è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œå®ç°äº†ç¼–è¾‘æŒ‡ä»¤éµå¾ªèƒ½åŠ›æå‡12%å’Œç¼–è¾‘è´¨é‡æå‡15%çš„æ˜¾è‘—æ”¹è¿›ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ç­–ç•¥å¯ä»¥æœ‰æ•ˆèµ‹äºˆè§†é¢‘ç”Ÿæˆæ¨¡å‹é€šç”¨ç¼–è¾‘èƒ½åŠ›ï¼Œè¯æ˜äº†åœ¨æœ‰é™é«˜è´¨é‡é…å¯¹æ•°æ®ä¸‹è¿›è¡Œé«˜æ•ˆå¾®è°ƒçš„å¯è¡Œæ€§ï¼Œä¸ºæŒ‡ä»¤è§†é¢‘ç¼–è¾‘æä¾›äº†ä¸€ç§ç»æµé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨å‡å°‘æ•°æ®ä¾èµ–çš„åŒæ—¶å®ç°é«˜è´¨é‡ç¼–è¾‘çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Despite the rapid progress of instruction-based image editing, its extension
to video remains underexplored, primarily due to the prohibitive cost and
complexity of constructing large-scale paired video editing datasets. To
address this challenge, we introduce a low-cost pretraining strategy for
instruction-based video editing that leverages in-context learning from
unpaired video clips. We show that pretraining a foundation video generation
model with this strategy endows it with general editing capabilities, such as
adding, replacing, or deleting operations, according to input editing
instructions. The pretrained model can then be efficiently refined with a small
amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our
framework first pretrains on approximately 1M real video clips to learn basic
editing concepts, and subsequently fine-tunes on fewer than 150k curated
editing pairs to extend more editing tasks and improve the editing quality.
Comparative experiments show that our method surpasses existing
instruction-based video editing approaches in both instruction alignment and
visual fidelity, achieving a 12\% improvement in editing instruction following
and a 15\% improvement in editing quality.


### [27] [WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging](https://arxiv.org/abs/2510.14668)
*Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Sami Azam, Asif Karim, Jemima Beissbarth, Amanda Leach*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªå¼±ç›‘ç£é“¾å¼çŸ¥è¯†è’¸é¦ç½‘ç»œWeCKDï¼Œé€šè¿‡æ„å»ºæ¸è¿›å¼è’¸é¦é“¾é‡æ–°å®šä¹‰çŸ¥è¯†ä¼ é€’ï¼Œå…¶ä¸­æ¯ä¸ªæ¨¡å‹ä¸ä»…ä»å‰é©±å­¦ä¹ çŸ¥è¯†ï¼Œè¿˜å¯¹å…¶è¿›è¡Œç²¾ç‚¼åä¼ é€’ç»™åç»­æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†æ•°æ®ä¾èµ–å¹¶æå‡äº†ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•å­˜åœ¨çŸ¥è¯†é€€åŒ–ã€ç›‘ç£æ•ˆç‡ä½ä¸‹ä»¥åŠä¾èµ–å¼ºå¤§æ•™å¸ˆæ¨¡å‹æˆ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œæœ‰é™æ•°æ®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒç­‰æ•°æ®ç¨€ç¼ºé¢†åŸŸã€‚

**Method:** WeCKDé‡‡ç”¨ç»“æ„åŒ–åºåˆ—çš„äº’è¿æ¨¡å‹æ„å»ºæ¸è¿›å¼è’¸é¦é“¾ï¼Œæ¯ä¸ªæ¨¡å‹ä»…ä½¿ç”¨æ•°æ®é›†çš„ä¸€éƒ¨åˆ†è¿›è¡Œè®­ç»ƒï¼Œä¸ä»…ä»å‰é©±æ¨¡å‹å­¦ä¹ çŸ¥è¯†ï¼Œè¿˜å¯¹çŸ¥è¯†è¿›è¡Œç²¾ç‚¼åä¼ é€’ç»™åç»­æ¨¡å‹ï¼Œå®ç°äº†å¼±ç›‘ç£ä¸‹çš„é«˜æ•ˆçŸ¥è¯†ä¼ é€’ã€‚

**Result:** åœ¨å››ä¸ªè€³é•œæˆåƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…åŒ¹é…è€Œä¸”åœ¨è®¸å¤šæƒ…å†µä¸‹è¶…è¶Šäº†ç°æœ‰ç›‘ç£æ–¹æ³•çš„æ€§èƒ½ï¼Œåœ¨å¦å¤–ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨ä¸åŒåŒ»å­¦å½±åƒæ¨¡æ€ï¼ˆåŒ…æ‹¬æ˜¾å¾®å’Œç£å…±æŒ¯æˆåƒï¼‰ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œç›¸æ¯”åœ¨ç›¸åŒæœ‰é™æ•°æ®ä¸Šè®­ç»ƒçš„å•ä¸€éª¨å¹²ç½‘ç»œï¼Œç´¯è®¡å‡†ç¡®ç‡æå‡é«˜è¾¾+23%ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç»“æ„åŒ–çŸ¥è¯†ä¼ é€’é“¾å¯ä»¥åœ¨å¼±ç›‘ç£æ¡ä»¶ä¸‹å®ç°é«˜æ•ˆå­¦ä¹ ï¼Œæ˜¾è‘—é™ä½äº†åŒ»å­¦å½±åƒåˆ†æå¯¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä¸ºç°å®ä¸–ç•Œæ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„æ¨¡å‹éƒ¨ç½²æä¾›äº†å¯è¡Œè§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨å¤šæ ·åŒ–åŒ»å­¦å½±åƒæ¨¡æ€ä¸Šçš„è‰¯å¥½æ³›åŒ–æ€§èƒ½ã€‚

---

#### ğŸ“„ Abstract
Knowledge distillation (KD) has traditionally relied on a static
teacher-student framework, where a large, well-trained teacher transfers
knowledge to a single student model. However, these approaches often suffer
from knowledge degradation, inefficient supervision, and reliance on either a
very strong teacher model or large labeled datasets, which limits their
effectiveness in real-world, limited-data scenarios. To address these, we
present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that
redefines knowledge transfer through a structured sequence of interconnected
models. Unlike conventional KD, it forms a progressive distillation chain,
where each model not only learns from its predecessor but also refines the
knowledge before passing it forward. This structured knowledge transfer further
enhances feature learning, reduces data dependency, and mitigates the
limitations of one-step KD. Each model in the distillation chain is trained on
only a fraction of the dataset and demonstrates that effective learning can be
achieved with minimal supervision. Extensive evaluations across four otoscopic
imaging datasets demonstrate that it not only matches but in many cases
surpasses the performance of existing supervised methods. Experimental results
on two other datasets further underscore its generalization across diverse
medical imaging modalities, including microscopic and magnetic resonance
imaging. Furthermore, our evaluations resulted in cumulative accuracy gains of
up to +23% over a single backbone trained on the same limited data, which
highlights its potential for real-world adoption.


### [28] [MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2510.14958)
*Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MathCanvasæ¡†æ¶ï¼Œèµ‹äºˆç»Ÿä¸€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å†…åœ¨çš„è§†è§‰æ€ç»´é“¾èƒ½åŠ›ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•åœ¨æ•°å­¦å‡ ä½•é—®é¢˜ä¸Šå®ç°äº†86%çš„ç›¸å¯¹æ€§èƒ½æå‡ï¼Œä¸ºè§£å†³LLMåœ¨è§†è§‰ä¾èµ–æ•°å­¦é¢†åŸŸçš„æ¨ç†éš¾é¢˜æä¾›äº†å®Œæ•´å·¥å…·åŒ…ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å‡ ä½•ç­‰ä¾èµ–è§†è§‰è¾…åŠ©çš„æ•°å­¦é¢†åŸŸå­˜åœ¨å›°éš¾ï¼Œç°æœ‰è§†è§‰æ€ç»´é“¾æ–¹æ³•å—é™äºåƒµåŒ–çš„å¤–éƒ¨å·¥å…·æˆ–æ— æ³•ç”Ÿæˆé«˜ä¿çœŸã€ç­–ç•¥æ€§å®šæ—¶çš„å›¾è¡¨æ¥æ”¯æŒå¤æ‚é—®é¢˜è§£å†³ã€‚

**Method:** æå‡ºMathCanvasæ¡†æ¶ï¼ŒåŒ…å«è§†è§‰æ“ä½œå’Œç­–ç•¥æ€§è§†è§‰è¾…åŠ©æ¨ç†ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡1500ä¸‡å¯¹è¯­æ–™é¢„è®­ç»ƒæ¨¡å‹æŒæ¡å›¾è¡¨ç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ï¼Œç¬¬äºŒé˜¶æ®µåœ¨21.9ä¸‡ä¾‹äº¤é”™è§†è§‰æ–‡æœ¬æ¨ç†è·¯å¾„æ•°æ®é›†ä¸Šå¾®è°ƒï¼Œæ•™ä¼šæ¨¡å‹ä½•æ—¶ä»¥åŠå¦‚ä½•åˆ©ç”¨è§†è§‰è¾…åŠ©ã€‚

**Result:** åŸºäºè¯¥æ¡†æ¶è®­ç»ƒçš„BAGEL-Canvasæ¨¡å‹åœ¨MathCanvas-BenchåŸºå‡†ä¸Šç›¸æ¯”å¼ºå¤§å¤šæ¨¡æ€æ¨¡å‹åŸºçº¿å®ç°äº†86%çš„ç›¸å¯¹æ€§èƒ½æå‡ï¼Œå¹¶åœ¨å…¶ä»–å…¬å…±æ•°å­¦åŸºå‡†ä¸Šå±•ç°å‡ºä¼˜ç§€çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºè§£é”å¤šæ¨¡æ€æ¨¡å‹ä¸­å¤æ‚ã€ç±»äººçš„è§†è§‰è¾…åŠ©æ¨ç†æä¾›äº†å®Œæ•´çš„å·¥å…·åŒ…ï¼ŒåŒ…æ‹¬æ¡†æ¶ã€æ•°æ®é›†å’ŒåŸºå‡†ï¼Œæ¨åŠ¨äº†è§†è§‰ä¾èµ–æ•°å­¦é—®é¢˜è§£å†³èƒ½åŠ›çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
While Large Language Models (LLMs) have excelled in textual reasoning, they
struggle with mathematical domains like geometry that intrinsically rely on
visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often
limited by rigid external tools or fail to generate the high-fidelity,
strategically-timed diagrams necessary for complex problem-solving. To bridge
this gap, we introduce MathCanvas, a comprehensive framework designed to endow
unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for
mathematics. Our approach consists of two phases. First, a Visual Manipulation
stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M
caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing
trajectories (MathCanvas-Edit), to master diagram generation and editing.
Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on
MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual
reasoning paths, teaching it when and how to leverage visual aids. To
facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging
benchmark with 3K problems that require models to produce interleaved
visual-textual solutions. Our model, BAGEL-Canvas, trained under this
framework, achieves an 86% relative improvement over strong LMM baselines on
MathCanvas-Bench, demonstrating excellent generalization to other public math
benchmarks. Our work provides a complete toolkit-framework, datasets, and
benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project
Page: https://mathcanvas.github.io/


### [29] [VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning](https://arxiv.org/abs/2510.14672)
*Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†VTimeCoTæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è¿›åº¦æ¡è§†è§‰å·¥å…·å’Œè§†è§‰æ—¶åºæ€ç»´é“¾ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ—¶åºå®šä½å’Œæ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå®ç°äº†ç»„åˆå¼å¯è§£é‡Šæ¨ç†è¿‡ç¨‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†é¢‘é—®ç­”ç³»ç»Ÿåœ¨è§†é¢‘æ—¶åºå®šä½å’Œæ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œè¿™é™åˆ¶äº†å®é™…è§†é¢‘ç†è§£ç³»ç»Ÿçš„å¼€å‘æ•ˆæœï¼Œå› æ­¤éœ€è¦è§£å†³æ¨¡å‹åœ¨æ—¶åºç†è§£å’Œè·¨æ¨¡æ€æ¨ç†æ–¹é¢çš„èƒ½åŠ›ç¼ºé™·ã€‚

**Method:** æå‡ºäº†VTimeCoTè®­ç»ƒå…è´¹æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ–°é¢–çš„è¿›åº¦æ¡è§†è§‰å·¥å…·ï¼šå³æ’å³ç”¨çš„è¿›åº¦æ¡é›†æˆå·¥å…·å’Œé«˜æ•ˆé«˜äº®å·¥å…·ï¼ŒåŒæ—¶å¼•å…¥äº†è§†è§‰æ—¶åºæ€ç»´é“¾è¿‡ç¨‹ï¼Œå°†è§†é¢‘å’Œæ–‡æœ¬çš„è·¨æ¨¡æ€æ¨ç†ç›¸ç»“åˆã€‚

**Result:** åœ¨Qwen2VL-7Bå’ŒGPT4oåŸºçº¿ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘æ—¶åºå®šä½å’ŒåŸºäºæ¨ç†çš„é—®ç­”ä»»åŠ¡ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å±•ç¤ºäº†ç»„åˆå¼å’Œå¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†è¿›åº¦æ¡è§†è§‰å·¥å…·ä¸è§†è§‰æ—¶åºæ€ç»´é“¾çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè§†é¢‘ç†è§£ç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨æ—¶åºæ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
In recent years, video question answering based on multimodal large language
models (MLLM) has garnered considerable attention, due to the benefits from the
substantial advancements in LLMs. However, these models have a notable
deficiency in the domains of video temporal grounding and reasoning, posing
challenges to the development of effective real-world video understanding
systems. Inspired by how humans use video players to interact with the progress
bar for video comprehension, we introduce VTimeCoT, a simple yet effective
training-free framework, designed for high-performance video grounding and
reasoning. The proposed framework incorporates two novel visual tools of the
progress bar: a plug-and-play progress bar integration tool and a
high-efficiency highlighting tool. In addition, to address the limitations of
conventional text-based chain-of-thought (CoT) approaches, we introduce a
visuotemporal CoT process that integrates cross-modality reasoning across both
video and text. Our approach demonstrates significant performance improvements
on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and
reasoning-based question answering. Finally, we showcase that the proposed
framework achieves a compositional and interpretable reasoning process. Project
page: https://vtimecot.github.io


### [30] [WithAnyone: Towards Controllable and ID Consistent Image Generation](https://arxiv.org/abs/2510.14975)
*Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, Yu-Gang Jiang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†WithAnyoneæ¨¡å‹ï¼Œé€šè¿‡æ„å»ºå¤§è§„æ¨¡å¤šèº«ä»½æ•°æ®é›†MultiID-2Må’Œå¯¹æ¯”èº«ä»½æŸå¤±å‡½æ•°ï¼Œæœ‰æ•ˆè§£å†³äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å¤åˆ¶ç²˜è´´é—®é¢˜ï¼Œåœ¨ä¿æŒèº«ä»½ä¸€è‡´æ€§çš„åŒæ—¶å®ç°äº†å¤šæ ·åŒ–çš„å¯æ§ç”Ÿæˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰èº«ä»½ä¸€è‡´æ€§ç”Ÿæˆæ–¹æ³•å› ç¼ºä¹å¤§è§„æ¨¡é…å¯¹æ•°æ®é›†è€Œä¾èµ–é‡å»ºè®­ç»ƒï¼Œå¯¼è‡´æ¨¡å‹å‡ºç°å¤åˆ¶ç²˜è´´é—®é¢˜ï¼Œå³ç›´æ¥å¤åˆ¶å‚è€ƒé¢éƒ¨è€Œéåœ¨å§¿æ€ã€è¡¨æƒ…æˆ–å…‰ç…§å˜åŒ–ä¸­ä¿æŒèº«ä»½ä¸€è‡´æ€§ï¼Œè¿™å‰Šå¼±äº†ç”Ÿæˆçš„å¯æ§æ€§å’Œè¡¨è¾¾èƒ½åŠ›ã€‚

**Method:** æ„å»ºäº†é’ˆå¯¹å¤šäººç‰©åœºæ™¯çš„å¤§è§„æ¨¡é…å¯¹æ•°æ®é›†MultiID-2Mï¼Œä¸ºæ¯ä¸ªèº«ä»½æä¾›å¤šæ ·åŒ–å‚è€ƒï¼›æå‡ºé‡åŒ–å¤åˆ¶ç²˜è´´ä¼ªå½±å’Œèº«ä»½ä¿çœŸåº¦-å˜åŒ–æƒè¡¡çš„åŸºå‡†ï¼›å¼•å…¥åŸºäºå¯¹æ¯”èº«ä»½æŸå¤±çš„æ–°è®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨é…å¯¹æ•°æ®å¹³è¡¡ä¿çœŸåº¦ä¸å¤šæ ·æ€§ï¼Œæœ€ç»ˆå¼€å‘äº†åŸºäºæ‰©æ•£çš„WithAnyoneæ¨¡å‹ã€‚

**Result:** å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒWithAnyoneæ˜¾è‘—å‡å°‘äº†å¤åˆ¶ç²˜è´´ä¼ªå½±ï¼Œæé«˜äº†å¯¹å§¿æ€å’Œè¡¨æƒ…çš„å¯æ§æ€§ï¼Œå¹¶ä¿æŒäº†å¼ºå¤§çš„æ„ŸçŸ¥è´¨é‡ï¼›ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•åœ¨å®ç°é«˜èº«ä»½ä¿çœŸåº¦çš„åŒæ—¶æ”¯æŒè¡¨è¾¾æ€§å¯æ§ç”Ÿæˆã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†æ„å»ºå’Œå¯¹æ¯”å­¦ä¹ ç­–ç•¥å¯ä»¥æœ‰æ•ˆç¼“è§£èº«ä»½ä¸€è‡´æ€§ç”Ÿæˆä¸­çš„å¤åˆ¶ç²˜è´´é—®é¢˜ï¼Œä¸ºå¹³è¡¡èº«ä»½ä¿çœŸåº¦å’Œç”Ÿæˆå¤šæ ·æ€§æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å¯æ§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Identity-consistent generation has become an important focus in text-to-image
research, with recent models achieving notable success in producing images
aligned with a reference identity. Yet, the scarcity of large-scale paired
datasets containing multiple images of the same individual forces most
approaches to adopt reconstruction-based training. This reliance often leads to
a failure mode we term copy-paste, where the model directly replicates the
reference face rather than preserving identity across natural variations in
pose, expression, or lighting. Such over-similarity undermines controllability
and limits the expressive power of generation. To address these limitations, we
(1) construct a large-scale paired dataset MultiID-2M, tailored for
multi-person scenarios, providing diverse references for each identity; (2)
introduce a benchmark that quantifies both copy-paste artifacts and the
trade-off between identity fidelity and variation; and (3) propose a novel
training paradigm with a contrastive identity loss that leverages paired data
to balance fidelity with diversity. These contributions culminate in
WithAnyone, a diffusion-based model that effectively mitigates copy-paste while
preserving high identity similarity. Extensive qualitative and quantitative
experiments demonstrate that WithAnyone significantly reduces copy-paste
artifacts, improves controllability over pose and expression, and maintains
strong perceptual quality. User studies further validate that our method
achieves high identity fidelity while enabling expressive controllable
generation.


### [31] [Free-Grained Hierarchical Recognition](https://arxiv.org/abs/2510.14737)
*Seulki Park, Zilin Wang, Stella X. Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ImageNet-FåŸºå‡†å’Œè‡ªç”±ç²’åº¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè§£å†³ç°å®ä¸–ç•Œä¸­å›¾åƒåˆ†ç±»æ ‡æ³¨ç²’åº¦ä¸ä¸€è‡´çš„é—®é¢˜ã€‚é€šè¿‡ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’ŒåŠç›‘ç£å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†æ··åˆç²’åº¦ç›‘ç£ä¸‹çš„åˆ†å±‚åˆ†ç±»æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åˆ†å±‚å›¾åƒåˆ†ç±»æ–¹æ³•é€šå¸¸å‡è®¾å®Œæ•´çš„ç»†ç²’åº¦æ ‡æ³¨ï¼Œè€Œç°å®ä¸–ç•Œä¸­çš„ç›‘ç£æ ‡æ³¨ç²’åº¦å› å›¾åƒè´¨é‡ã€æ ‡æ³¨è€…ä¸“ä¸šçŸ¥è¯†å’Œä»»åŠ¡éœ€æ±‚è€Œå­˜åœ¨å·®å¼‚ï¼Œå¯¼è‡´æ ‡æ³¨ç²’åº¦ä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¿™ç§æ··åˆç²’åº¦æ ‡æ³¨åœ¨å®é™…åº”ç”¨ä¸­æ™®éå­˜åœ¨ï¼Œä½†ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆå¤„ç†ã€‚

**Method:** ä½œè€…æ„å»ºäº†ImageNet-Få¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŸºäºè®¤çŸ¥å¿ƒç†å­¦å°†å…¶åˆ’åˆ†ä¸ºåŸºç¡€ã€ä»å±å’Œç»†ç²’åº¦ä¸‰ä¸ªå±‚æ¬¡ã€‚åˆ©ç”¨CLIPæ¨¡å‹æ¨¡æ‹Ÿè¯­ä¹‰æ¨¡ç³Šæ€§ï¼Œç”Ÿæˆåæ˜ äººç±»æ ‡æ³¨è¡Œä¸ºçš„æ··åˆç²’åº¦æ ‡ç­¾ã€‚æå‡ºäº†è‡ªç”±ç²’åº¦å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆä¼ªå±æ€§å¢å¼ºè¯­ä¹‰æŒ‡å¯¼ï¼Œå¹¶ç»“åˆåŠç›‘ç£å­¦ä¹ æå‡è§†è§‰æŒ‡å¯¼ã€‚

**Result:** æ‰€æå‡ºçš„æ–¹æ³•åœ¨æ··åˆç›‘ç£è®¾ç½®ä¸‹æ˜¾è‘—æå‡äº†åˆ†å±‚åˆ†ç±»æ€§èƒ½ã€‚ä¸å¼ºåŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œç»“åˆä¼ªå±æ€§å’ŒåŠç›‘ç£å­¦ä¹ çš„ç­–ç•¥åœ¨ImageNet-FåŸºå‡†ä¸Šå–å¾—äº†å®è´¨æ€§æ”¹è¿›ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•å¤„ç†ç°å®ä¸–ç•Œæ ‡æ³¨çº¦æŸçš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥è®¤çŸ¥å¯å‘çš„åŸºå‡†å’Œè‡ªç”±ç²’åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ¨è¿›äº†ç°å®çº¦æŸä¸‹çš„åˆ†å±‚åˆ†ç±»ç ”ç©¶ã€‚æ··åˆç²’åº¦ç›‘ç£å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„ç»“åˆä¸ºè§£å†³æ ‡æ³¨ä¸ä¸€è‡´é—®é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„å›¾åƒåˆ†ç±»ç³»ç»Ÿè®¾è®¡æä¾›äº†é‡è¦å‚è€ƒã€‚

---

#### ğŸ“„ Abstract
Hierarchical image classification predicts labels across a semantic taxonomy,
but existing methods typically assume complete, fine-grained annotations, an
assumption rarely met in practice. Real-world supervision varies in
granularity, influenced by image quality, annotator expertise, and task
demands; a distant bird may be labeled Bird, while a close-up reveals Bald
eagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet
and structured into cognitively inspired basic, subordinate, and fine-grained
levels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,
mixed-granularity labels reflecting human annotation behavior. We propose
free-grain learning, with heterogeneous supervision across instances. We
develop methods that enhance semantic guidance via pseudo-attributes from
vision-language models and visual guidance via semi-supervised learning. These,
along with strong baselines, substantially improve performance under mixed
supervision. Together, our benchmark and methods advance hierarchical
classification under real-world constraints.


### [32] [From Pixels to Words -- Towards Native Vision-Language Primitives at Scale](https://arxiv.org/abs/2510.14979)
*Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†NEOç³»åˆ—åŸç”Ÿè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ„å»ºåŸºäºç¬¬ä¸€æ€§åŸç†çš„å¯†é›†å•ä½“æ¶æ„ï¼Œè§£å†³äº†åŸç”ŸVLMä¸æ¨¡å—åŒ–VLMä¹‹é—´çš„æ ¹æœ¬å·®å¼‚é—®é¢˜ï¼Œä»…éœ€3.9äº¿å›¾åƒæ–‡æœ¬æ ·æœ¬å³å¯å®ç°ä¸é¡¶çº§æ¨¡å—åŒ–æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸç”Ÿè§†è§‰è¯­è¨€æ¨¡å‹é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¸€æ˜¯ä¸æ¨¡å—åŒ–VLMç›¸æ¯”å­˜åœ¨æ ¹æœ¬æ€§çº¦æŸä¸”è¿™äº›éšœç¢çš„å…‹æœç¨‹åº¦å°šä¸æ˜ç¡®ï¼ŒäºŒæ˜¯å¦‚ä½•ä½¿åŸç”ŸVLMç ”ç©¶æ›´åŠ æ™®åŠå’Œæ°‘ä¸»åŒ–ä»¥åŠ é€Ÿé¢†åŸŸè¿›å±•ã€‚

**Method:** æå‡ºäº†æ„å»ºåŸç”ŸVLMçš„ä¸‰é¡¹åŸºæœ¬åŸåˆ™ï¼šåœ¨å…±äº«è¯­ä¹‰ç©ºé—´ä¸­æœ‰æ•ˆå¯¹é½åƒç´ å’Œè¯è¡¨ç¤ºã€æ— ç¼æ•´åˆå…ˆå‰åˆ†ç¦»çš„è§†è§‰å’Œè¯­è¨€æ¨¡å—ä¼˜åŠ¿ã€å›ºæœ‰ä½“ç°æ”¯æŒç»Ÿä¸€è§†è§‰è¯­è¨€ç¼–ç å¯¹é½å’Œæ¨ç†çš„è·¨æ¨¡æ€ç‰¹æ€§ï¼Œå¹¶åŸºäºæ­¤å¼€å‘äº†NEOç³»åˆ—å¯†é›†å•ä½“æ¨¡å‹ã€‚

**Result:** NEOæ¨¡å‹ä»…ä½¿ç”¨3.9äº¿å›¾åƒæ–‡æœ¬æ ·æœ¬å³å¯ä»é›¶å¼€å§‹é«˜æ•ˆå‘å±•è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œåœ¨å¤šæ ·åŒ–ç°å®åœºæ™¯ä¸­èƒ½å¤Ÿä¸é¡¶çº§æ¨¡å—åŒ–æ¨¡å‹ç«äº‰ï¼ŒåŒæ—¶ç¼“è§£äº†å¯†é›†å•ä½“æ¨¡å‹å†…éƒ¨çš„è§†è§‰è¯­è¨€å†²çªã€‚

**Conclusion:** NEOä¸ºå¯æ‰©å±•ä¸”å¼ºå¤§çš„åŸç”ŸVLMå¥ å®šäº†åŸºçŸ³ï¼Œé…åˆä¸°å¯Œçš„å¯å¤ç”¨ç»„ä»¶æ„å»ºäº†æˆæœ¬æ•ˆç›Šé«˜ä¸”å¯æ‰©å±•çš„ç”Ÿæ€ç³»ç»Ÿï¼Œæ¨åŠ¨äº†åŸç”Ÿè§†è§‰è¯­è¨€æ¨¡å‹ç ”ç©¶çš„æ°‘ä¸»åŒ–å’ŒåŠ é€Ÿå‘å±•ã€‚

---

#### ğŸ“„ Abstract
The edifice of native Vision-Language Models (VLMs) has emerged as a rising
contender to typical modular VLMs, shaped by evolving model architectures and
training paradigms. Yet, two lingering clouds cast shadows over its widespread
exploration and promotion: (-) What fundamental constraints set native VLMs
apart from modular ones, and to what extent can these barriers be overcome? (-)
How to make research in native VLMs more accessible and democratized, thereby
accelerating progress in the field. In this paper, we clarify these challenges
and outline guiding principles for constructing native VLMs. Specifically, one
native VLM primitive should: (i) effectively align pixel and word
representations within a shared semantic space; (ii) seamlessly integrate the
strengths of formerly separate vision and language modules; (iii) inherently
embody various cross-modal properties that support unified vision-language
encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of
native VLMs built from first principles, capable of rivaling top-tier modular
counterparts across diverse real-world scenarios. With only 390M image-text
examples, NEO efficiently develops visual perception from scratch while
mitigating vision-language conflicts inside a dense and monolithic model
crafted from our elaborate primitives. We position NEO as a cornerstone for
scalable and powerful native VLMs, paired with a rich set of reusable
components that foster a cost-effective and extensible ecosystem. Our code and
models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.


### [33] [CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection](https://arxiv.org/abs/2510.14792)
*Hojun Choi, Youngsun Lim, Jaeyo Shin, Hyunjung Shim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CoT-PLæ¡†æ¶ï¼Œé€šè¿‡å°†ç»“æ„åŒ–è§†è§‰æ€ç»´é“¾æ¨ç†å¼•å…¥ä¼ªæ ‡ç­¾ç”Ÿæˆè¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹åœ¨æ‹¥æŒ¤å’Œé®æŒ¡åœºæ™¯ä¸­çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•åœ¨å¼€æ”¾è¯æ±‡COCOå’ŒLVISåŸºå‡†ä¸Šå‡å®ç°äº†æ–°çš„æœ€ä¼˜æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–ç›´æ¥çš„å›¾åƒ-æ–‡æœ¬åŒ¹é…ï¼Œå¿½ç•¥äº†ç†è§£è¯­ä¹‰å¤æ‚åœºæ™¯æ‰€éœ€çš„ä¸­å±‚æ¨ç†æ­¥éª¤ï¼Œå¯¼è‡´åœ¨æ‹¥æŒ¤æˆ–é®æŒ¡çš„è§†è§‰ç¯å¢ƒä¸­é²æ£’æ€§æœ‰é™ã€‚

**Method:** CoT-PLæ¡†æ¶å°†ç›®æ ‡ç†è§£åˆ†è§£ä¸ºä¸‰ä¸ªå¯è§£é‡Šæ­¥éª¤ï¼šåŒºåŸŸæ„ŸçŸ¥ã€é›¶æ ·æœ¬ç±»åˆ«è¯†åˆ«å’ŒèƒŒæ™¯å®šä½ï¼Œå¹¶æå‡ºäº†å¯¹æ¯”èƒŒæ™¯å­¦ä¹ æœºåˆ¶ï¼Œåˆ©ç”¨é¢„è®¡ç®—çš„èƒŒæ™¯çº¿ç´¢ä½œä¸ºè´Ÿæ ·æœ¬æ¥ä¿ƒè¿›ç›®æ ‡ä¸èƒŒæ™¯ç‰¹å¾è§£è€¦ã€‚

**Result:** åœ¨æ‹¥æŒ¤å’Œé®æŒ¡åœºæ™¯ä¸­ï¼Œæ–°ç±»åˆ«ä¼ªæ ‡ç­¾è´¨é‡åˆ†åˆ«æ¯”å…ˆå‰æœ€ä½³æ–¹æ³•æå‡äº†103.4%å’Œ168.4%ï¼Œåœ¨å¼€æ”¾è¯æ±‡COCOä¸Šå®ç°äº†+7.7 AP50çš„æå‡ï¼Œåœ¨LVISä¸Šæ–°ç±»åˆ«å®ç°äº†+2.9 mask APçš„æå‡ï¼Œå‡è¾¾åˆ°æ–°çš„æœ€ä¼˜æ°´å¹³ã€‚

**Conclusion:** ç»“æ„åŒ–æ€ç»´é“¾æ¨ç†ä¸å¯¹æ¯”èƒŒæ™¯å­¦ä¹ çš„é›†æˆèƒ½å¤Ÿæœ‰æ•ˆæå‡å¼€æ”¾è¯æ±‡æ£€æµ‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„é²æ£’æ€§ï¼Œä¸ºå¤„ç†è¯­ä¹‰å¤æ‚è§†è§‰ç¯å¢ƒæä¾›äº†ä¸€ç§æ–°çš„èŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
Open-vocabulary object detection (OVD) seeks to recognize and localize object
categories beyond those seen during training. Recent approaches typically
leverage vision-language models (VLMs) to generate pseudo-labels using
image-text alignment, allowing detectors to generalize to unseen classes
without explicit supervision. However, these methods depend heavily on direct
image-text matching, neglecting the intermediate reasoning steps essential for
interpreting semantically complex scenes. This results in limited robustness
when confronted with crowded or occluded visual contexts. In this paper, we
introduce CoT-PL, a new framework that employs structured visual
chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL
decomposes object understanding into three interpretable steps: (1) region
perception even for unseen objects, (2) category recognition via zero-shot
reasoning, and (3) background grounding to separate semantically complex
objects. Crucially, the third step naturally motivates our contrastive
background learning (CBL) that uses the pre-computed background cues as
negatives to promote feature disentanglement between objects and background. In
this way, CoT reasoning and CBL form an integrated pipeline tailored to robust
pseudo-labeling in crowded or occluded scenes. Notably, in these two settings,
our novel-class pseudo-label quality achieves relative improvements of 103.4%
and 168.4% over the best prior, respectively. Our extensive experiments
demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9
mask AP on LVIS for novel classes, setting a new state of the art.


### [34] [QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models](https://arxiv.org/abs/2510.14836)
*Yixuan Li, Yuhui Chen, Mingcai Zhou, Haoran Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºQDepth-VLAæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ·±åº¦é¢„æµ‹ä»»åŠ¡å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„3Dç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œåœ¨æ“ä½œä»»åŠ¡ä¸­å®ç°äº†å¼ºå¤§çš„ç©ºé—´æ¨ç†å’Œç«äº‰æ€§æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨ç»†ç²’åº¦æ“ä½œä»»åŠ¡ä¸­ç¼ºä¹å¯¹å…³é”®3Dç»“æ„çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç²¾ç¡®æ§åˆ¶æ–¹é¢çš„è¡¨ç°ã€‚

**Method:** è®¾è®¡äº†ä¸“é—¨çš„æ·±åº¦ä¸“å®¶æ¨¡å—æ¥é¢„æµ‹ä»VQ-VAEç¼–ç å™¨è·å¾—çš„æ·±åº¦å›¾çš„é‡åŒ–æ½œåœ¨æ ‡è®°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ•æ‰å…³é”®å‡ ä½•çº¿ç´¢çš„æ·±åº¦æ„ŸçŸ¥è¡¨ç¤ºã€‚

**Result:** åœ¨ä»¿çœŸåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒQDepth-VLAåœ¨æ“ä½œä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„ç©ºé—´æ¨ç†èƒ½åŠ›å’Œç«äº‰æ€§æ€§èƒ½ã€‚

**Conclusion:** é€šè¿‡æ·±åº¦é¢„æµ‹ä»»åŠ¡å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæå‡å…¶ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œä¸ºç²¾ç»†æ“ä½œä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)
models to accomplish fine-grained manipulation tasks. However, existing
approaches often lack the ability to understand and reason over the essential
3D structures necessary for precise control. To address this limitation, we
propose QDepth-VLA, a general framework that augments VLA models with an
auxiliary depth prediction task. A dedicated depth expert is designed to
predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,
enabling the model to learn depth-aware representations that capture critical
geometric cues. Experimental results on the simulation benchmarks and
real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning
and competitive performance on manipulation tasks.


### [35] [Multi-modal video data-pipelines for machine learning with minimal human supervision](https://arxiv.org/abs/2510.14862)
*Mihai-Cristian PÃ®rvu, Marius Leordeanu*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ— éœ€äººå·¥ç›‘ç£çš„å¤šæ¨¡æ€è§†è§‰å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡é¢„è®­ç»ƒä¸“å®¶æ¨¡å‹å’Œç¨‹åºåŒ–ç»„åˆæ„å»ºå…¨è‡ªåŠ¨æ•°æ®æµæ°´çº¿ï¼Œå¹¶å°†PHG-MAEæ¨¡å‹é«˜æ•ˆè’¸é¦è‡³å°äº1Må‚æ•°ï¼Œåœ¨å®æ—¶è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­è¾¾åˆ°ä¸300Må‚æ•°æ¨¡å‹ç›¸ç«äº‰çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°å®ä¸–ç•Œæœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€çš„ï¼Œä½†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹å¤šä¸ºå•æ¨¡æ€æˆ–åŒæ¨¡æ€ï¼Œæ— æ³•å…¨é¢ç†è§£ä¸–ç•Œã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ•´åˆå°½å¯èƒ½å¤šçš„è§†è§‰æ¨¡æ€ï¼Œä½¿ç”¨æå°‘æˆ–æ— éœ€äººå·¥ç›‘ç£çš„æ–¹å¼å®ç°å¤šæ¨¡æ€å­¦ä¹ ï¼Œä»¥å¼¥è¡¥ç°æœ‰æ–¹æ³•åœ¨æ¨¡æ€æ•´åˆæ–¹é¢çš„ä¸è¶³ã€‚

**Method:** é‡‡ç”¨é¢„è®­ç»ƒä¸“å®¶æ¨¡å‹å’Œç¨‹åºåŒ–ç»„åˆæŠ€æœ¯ï¼Œæ„å»ºå…¨è‡ªåŠ¨æ•°æ®æµæ°´çº¿å¤„ç†åŸå§‹è§†é¢‘æ•°æ®ã€‚ä½¿ç”¨ä¸“é—¨è®¾è®¡çš„PHG-MAEæ¨¡å‹æ¥åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶é€šè¿‡é«˜æ•ˆè’¸é¦æŠ€æœ¯å°†æ¨¡å‹å‚æ•°å‹ç¼©è‡³å°äº1Mã€‚éƒ¨ç½²æ¡†æ¶æ”¯æŒå®æ—¶è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ç­‰ä»»åŠ¡ã€‚

**Result:** ç»è¿‡è’¸é¦çš„PHG-MAEæ¨¡å‹ï¼ˆå‚æ•°<1Mï¼‰åœ¨æ€§èƒ½ä¸Šèƒ½å¤Ÿä¸çº¦300Må‚æ•°çš„å¤§å‹æ¨¡å‹ç›¸ç«äº‰ã€‚è¯¥æ¨¡å‹æˆåŠŸéƒ¨ç½²äºæ‰‹æŒè®¾å¤‡å’Œç½‘ç»œæ‘„åƒå¤´çš„å®æ—¶è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼Œå¹¶åœ¨å•†å“ç¡¬ä»¶ä¸Šå®ç°é«˜æ•ˆè¿è¡Œã€‚ç›¸åŒæ¡†æ¶ä¸‹è¿˜éƒ¨ç½²äº†DPTæ¨¡å‹ç”¨äºè¿‘å®æ—¶æ·±åº¦ä¼°è®¡ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜é€šè¿‡æœ‰æ•ˆçš„å¤šæ¨¡æ€æ•´åˆå’Œæ¨¡å‹è’¸é¦æŠ€æœ¯ï¼Œå¯ä»¥åœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶æ˜¾è‘—å‡å°‘æ¨¡å‹å‚æ•°ã€‚å…¨è‡ªåŠ¨æ•°æ®æµæ°´çº¿å’Œå¼€æºæ¡†æ¶ä¸ºå¤šæ¨¡æ€å­¦ä¹ æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œè¯æ˜äº†åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šå®ç°é«˜æ€§èƒ½å¤šæ¨¡æ€è§†è§‰ä»»åŠ¡çš„å¯è¡Œæ€§ã€‚

---

#### ğŸ“„ Abstract
The real-world is inherently multi-modal at its core. Our tools observe and
take snapshots of it, in digital form, such as videos or sounds, however much
of it is lost. Similarly for actions and information passing between humans,
languages are used as a written form of communication. Traditionally, Machine
Learning models have been unimodal (i.e. rgb -> semantic or text ->
sentiment_class). Recent trends go towards bi-modality, where images and text
are learned together, however, in order to truly understand the world, we need
to integrate all these independent modalities. In this work we try to combine
as many visual modalities as we can using little to no human supervision. In
order to do this, we use pre-trained experts and procedural combinations
between them on top of raw videos using a fully autonomous data-pipeline, which
we also open-source. We then make use of PHG-MAE, a model specifically designed
to leverage multi-modal data. We show that this model which was efficiently
distilled into a low-parameter (<1M) can have competitive results compared to
models of ~300M parameters. We deploy this model and analyze the use-case of
real-time semantic segmentation from handheld devices or webcams on commodity
hardware. Finally, we deploy other off-the-shelf models using the same
framework, such as DPT for near real-time depth estimation.


### [36] [ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention](https://arxiv.org/abs/2510.14882)
*Keli Liu, Zhendong Wang, Wengang Zhou, Shaodong Xu, Ruixiao Dong, Houqiang Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ScaleWeaveræ¡†æ¶ï¼Œé€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒåœ¨è§†è§‰è‡ªå›å½’æ¨¡å‹ä¸Šå®ç°é«˜è´¨é‡å¯æ§ç”Ÿæˆï¼Œæ ¸å¿ƒåˆ›æ–°æ˜¯æ”¹è¿›çš„MMDiTå—å’ŒReference Attentionæ¨¡å—ï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å®ç°ç²¾ç¡®æ§åˆ¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è‡ªå›å½’æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç›¸æ¯”æ‰©æ•£æ¨¡å‹ï¼ŒVARèŒƒå¼ä¸‹çš„ç²¾ç¡®çµæ´»æ§åˆ¶æœºåˆ¶ä»æœªè¢«å……åˆ†æ¢ç´¢ï¼Œéœ€è¦å¡«è¡¥è¿™ä¸€å…³é”®ç©ºç™½ä»¥å®ç°é«˜æ•ˆå¯æ§ç”Ÿæˆã€‚

**Method:** ScaleWeaveræ¡†æ¶é‡‡ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒç­–ç•¥ï¼Œæ ¸å¿ƒæ¨¡å—æ˜¯æ”¹è¿›çš„MMDiTå—å’Œæå‡ºçš„Reference Attentionæ¨¡å—ï¼Œè¯¥æ¨¡å—æ‘’å¼ƒäº†å›¾åƒåˆ°æ¡ä»¶çš„éå¿…è¦æ³¨æ„åŠ›ï¼Œé™ä½è®¡ç®—æˆæœ¬å¹¶ç¨³å®šæ§åˆ¶æ³¨å…¥ï¼ŒåŒæ—¶é€šè¿‡é›¶åˆå§‹åŒ–çº¿æ€§æŠ•å½±ç¡®ä¿æ§åˆ¶ä¿¡å·æœ‰æ•ˆèå…¥è€Œä¸ç ´ååŸºç¡€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼ŒScaleWeaverèƒ½å¤Ÿå®ç°é«˜è´¨é‡ç”Ÿæˆå’Œç²¾ç¡®æ§åˆ¶ï¼Œåœ¨æ•ˆç‡ä¸Šä¼˜äºåŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œä¸ºè§†è§‰è‡ªå›å½’èŒƒå¼ä¸‹çš„å¯æ§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæä¾›äº†å®ç”¨æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

**Conclusion:** ScaleWeaveré€šè¿‡åˆ›æ–°çš„Reference Attentionæ¨¡å—å’Œå‚æ•°é‡ç”¨ç­–ç•¥ï¼ŒæˆåŠŸå°†ç²¾ç¡®æ§åˆ¶èƒ½åŠ›å¼•å…¥è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼Œåœ¨ä¿æŒé«˜æ•ˆæ€§çš„åŒæ—¶å®ç°äº†ä¸æ‰©æ•£æ¨¡å‹ç›¸åª²ç¾çš„æ§åˆ¶ç²¾åº¦ï¼Œä¸ºVARèŒƒå¼ä¸‹çš„å¯æ§ç”Ÿæˆå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Text-to-image generation with visual autoregressive~(VAR) models has recently
achieved impressive advances in generation fidelity and inference efficiency.
While control mechanisms have been explored for diffusion models, enabling
precise and flexible control within VAR paradigm remains underexplored. To
bridge this critical gap, in this paper, we introduce ScaleWeaver, a novel
framework designed to achieve high-fidelity, controllable generation upon
advanced VAR models through parameter-efficient fine-tuning. The core module in
ScaleWeaver is the improved MMDiT block with the proposed Reference Attention
module, which efficiently and effectively incorporates conditional information.
Different from MM Attention, the proposed Reference Attention module discards
the unnecessary attention from image$\rightarrow$condition, reducing
computational cost while stabilizing control injection. Besides, it
strategically emphasizes parameter reuse, leveraging the capability of the VAR
backbone itself with a few introduced parameters to process control
information, and equipping a zero-initialized linear projection to ensure that
control signals are incorporated effectively without disrupting the generative
capability of the base model. Extensive experiments show that ScaleWeaver
delivers high-quality generation and precise control while attaining superior
efficiency over diffusion-based methods, making ScaleWeaver a practical and
effective solution for controllable text-to-image generation within the visual
autoregressive paradigm. Code and models will be released.


### [37] [Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection](https://arxiv.org/abs/2510.14896)
*Furkan Mumcu, Michael J. Jones, Anoop Cherian, Yasin Yilmaz*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡æå–å’Œè§£é‡Šå¯¹è±¡æ´»åŠ¨ä¸äº¤äº’çš„æ–‡æœ¬æè¿°æ¥æ£€æµ‹å¤æ‚å¼‚å¸¸ï¼Œä¸ä»…æœ‰æ•ˆæ£€æµ‹äº¤äº’å‹å¼‚å¸¸ï¼Œè¿˜åœ¨éäº¤äº’å‹å¼‚å¸¸æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰çš„åŠç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•åœ¨å¤„ç†æ¶‰åŠå¯¹è±¡äº¤äº’çš„å¤æ‚å¼‚å¸¸æ—¶å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”æ™®éç¼ºä¹å¯è§£é‡Šæ€§ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨æ•ˆæœå’Œå¯ä¿¡åº¦ã€‚

**Method:** è¯¥æ–¹æ³•é€šè¿‡å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¾“å…¥ä¸åŒæ—¶åˆ»çš„å¯¹è±¡å¯¹è§†è§‰ä¿¡æ¯ï¼Œç”Ÿæˆæ­£å¸¸è§†é¢‘ä¸­å¯¹è±¡æ´»åŠ¨å’Œäº¤äº’çš„æ–‡æœ¬æè¿°ï¼Œè¿™äº›æ–‡æœ¬æè¿°ä½œä¸ºè§†é¢‘ä¸­å¯¹è±¡æ´»åŠ¨çš„é«˜å±‚è¡¨ç¤ºï¼Œåœ¨æµ‹è¯•æ—¶é€šè¿‡æ¯”è¾ƒä¸è®­ç»ƒè§†é¢‘ä¸­æ–‡æœ¬æè¿°çš„å·®å¼‚æ¥æ£€æµ‹å¼‚å¸¸ã€‚

**Result:** åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹åŸºäºäº¤äº’çš„å¤æ‚å¼‚å¸¸ï¼Œè¿˜åœ¨ä¸å«äº¤äº’å¼‚å¸¸çš„æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** è¯¥æ¡†æ¶ä¸ä»…æä¾›äº†å›ºæœ‰çš„å¯è§£é‡Šæ€§ï¼Œè¿˜èƒ½ä¸è®¸å¤šä¼ ç»Ÿè§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•ç»“åˆä»¥è¿›ä¸€æ­¥å¢å¼ºå…¶å¯è§£é‡Šæ€§ï¼Œä¸ºå¤æ‚å¼‚å¸¸æ£€æµ‹å’Œæ¨¡å‹é€æ˜åº¦æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Existing semi-supervised video anomaly detection (VAD) methods often struggle
with detecting complex anomalies involving object interactions and generally
lack explainability. To overcome these limitations, we propose a novel VAD
framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous
MLLM-based approaches that make direct anomaly judgments at the frame level,
our method focuses on extracting and interpreting object activity and
interactions over time. By querying an MLLM with visual inputs of object pairs
at different moments, we generate textual descriptions of the activity and
interactions from nominal videos. These textual descriptions serve as a
high-level representation of the activity and interactions of objects in a
video. They are used to detect anomalies during test time by comparing them to
textual descriptions found in nominal training videos. Our approach inherently
provides explainability and can be combined with many traditional VAD methods
to further enhance their interpretability. Extensive experiments on benchmark
datasets demonstrate that our method not only detects complex interaction-based
anomalies effectively but also achieves state-of-the-art performance on
datasets without interaction anomalies.


### [38] [3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation](https://arxiv.org/abs/2510.14945)
*JoungBin Lee, Jaewoo Jung, Jisang Han, Takuya Narihira, Kazumi Fukuda, Junyoung Seo, Sunghwan Hong, Yuki Mitsufuji, Seungryong Kim*

#### ğŸ§© TL;DR
3DScenePromptæ˜¯ä¸€ä¸ªè§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡åŒæ—¶ç©ºæ¡ä»¶æœºåˆ¶å’Œ3Dåœºæ™¯è®°å¿†å®ç°é•¿è§†é¢‘ç”Ÿæˆï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶ç›¸æœºè§†è§’å¹¶ä¿æŒåœºæ™¯ä¸€è‡´æ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•é€šå¸¸åŸºäºå•å¼ å›¾åƒæˆ–çŸ­ç‰‡æ®µè¿›è¡Œæ¡ä»¶ç”Ÿæˆï¼Œéš¾ä»¥åœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶ä¿æŒåœºæ™¯ä¸€è‡´æ€§å’Œç²¾ç¡®ç›¸æœºæ§åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨è¶Šæ—¶é—´è¾¹ç•Œæ—¶åŠ¨æ€å…ƒç´ ä¼šé”™è¯¯ä¿ç•™çš„é—®é¢˜ã€‚

**Method:** æå‡ºåŒæ—¶ç©ºæ¡ä»¶æœºåˆ¶ï¼Œç»“åˆæ—¶é—´ç›¸é‚»å¸§çš„è¿åŠ¨è¿ç»­æ€§å’Œç©ºé—´ç›¸é‚»å†…å®¹çš„åœºæ™¯ä¸€è‡´æ€§ï¼›å¼•å…¥3Dåœºæ™¯è®°å¿†ä¸“é—¨è¡¨ç¤ºä»è¾“å…¥è§†é¢‘ä¸­æå–çš„é™æ€å‡ ä½•ç»“æ„ï¼Œé€šè¿‡åŠ¨æ€SLAMå’Œæ–°æå‡ºçš„åŠ¨æ€æ©ç ç­–ç•¥åˆ†ç¦»é™æ€åœºæ™¯å‡ ä½•ä¸åŠ¨æ€å…ƒç´ ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åœºæ™¯ä¸€è‡´æ€§ã€ç›¸æœºå¯æ§æ€§å’Œç”Ÿæˆè´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿä¿æŒé•¿ç¨‹ç©ºé—´è¿è´¯æ€§å’Œç²¾ç¡®ç›¸æœºæ§åˆ¶ï¼ŒåŒæ—¶ä¸ç‰ºç‰²è®¡ç®—æ•ˆç‡æˆ–è¿åŠ¨çœŸå®æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡3Dåœºæ™¯è¡¨ç¤ºå’Œæ—¶ç©ºåˆ†ç¦»ç­–ç•¥å¯ä»¥æœ‰æ•ˆè§£å†³é•¿è§†é¢‘ç”Ÿæˆä¸­çš„åœºæ™¯ä¸€è‡´æ€§é—®é¢˜ï¼Œä¸ºå¯æ§è§†é¢‘ç”Ÿæˆæä¾›äº†æ–°æ€è·¯ï¼Œæœªæ¥å¯æ‰©å±•åˆ°æ›´å¤æ‚çš„åŠ¨æ€åœºæ™¯å»ºæ¨¡ã€‚

---

#### ğŸ“„ Abstract
We present 3DScenePrompt, a framework that generates the next video chunk
from arbitrary-length input while enabling precise camera control and
preserving scene consistency. Unlike methods conditioned on a single image or a
short clip, we employ dual spatio-temporal conditioning that reformulates
context-view referencing across the input video. Our approach conditions on
both temporally adjacent frames for motion continuity and spatially adjacent
content for scene consistency. However, when generating beyond temporal
boundaries, directly using spatially adjacent frames would incorrectly preserve
dynamic elements from the past. We address this by introducing a 3D scene
memory that represents exclusively the static geometry extracted from the
entire input video. To construct this memory, we leverage dynamic SLAM with our
newly introduced dynamic masking strategy that explicitly separates static
scene geometry from moving elements. The static scene representation can then
be projected to any target viewpoint, providing geometrically consistent warped
views that serve as strong 3D spatial prompts while allowing dynamic regions to
evolve naturally from temporal context. This enables our model to maintain
long-range spatial coherence and precise camera control without sacrificing
computational efficiency or motion realism. Extensive experiments demonstrate
that our framework significantly outperforms existing methods in scene
consistency, camera controllability, and generation quality. Project page :
https://cvlab-kaist.github.io/3DScenePrompt/


### [39] [OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression](https://arxiv.org/abs/2510.14954)
*Zhe Li, Weihao Yuan, Weichao Shen, Siyu Zhu, Zilong Dong, Chang Xu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§è¿ç»­æ©ç è‡ªå›å½’è¿åŠ¨å˜æ¢å™¨ï¼Œé€šè¿‡ç»“åˆé—¨æ§çº¿æ€§æ³¨æ„åŠ›å’ŒRMSNormæ¨¡å—ï¼Œè§£å†³äº†å…¨èº«å¤šæ¨¡æ€äººä½“è¿åŠ¨ç”Ÿæˆä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œå¹¶åœ¨æ–‡æœ¬ã€è¯­éŸ³å’ŒéŸ³ä¹ç­‰å¤šç§æ¨¡æ€ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å…¨èº«å¤šæ¨¡æ€äººä½“è¿åŠ¨ç”Ÿæˆé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šæ„å»ºæœ‰æ•ˆçš„è¿åŠ¨ç”Ÿæˆæœºåˆ¶ä»¥åŠå°†æ–‡æœ¬ã€è¯­éŸ³å’ŒéŸ³ä¹ç­‰å¤šç§æ¨¡æ€æ•´åˆåˆ°ç»Ÿä¸€æ¡†æ¶ä¸­ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸é‡‡ç”¨ç¦»æ•£æ©ç å»ºæ¨¡æˆ–è‡ªå›å½’å»ºæ¨¡ï¼Œæ— æ³•å……åˆ†å¤„ç†äººä½“è¿åŠ¨çš„åºåˆ—ç‰¹æ€§å’Œå¤šæ¨¡æ€åˆ†å¸ƒå¼‚è´¨æ€§é—®é¢˜ã€‚

**Method:** å¼€å‘äº†è¿ç»­æ©ç è‡ªå›å½’è¿åŠ¨å˜æ¢å™¨ï¼Œé‡‡ç”¨å› æœæ³¨æ„åŠ›æœºåˆ¶å¤„ç†äººä½“è¿åŠ¨çš„åºåˆ—ç‰¹æ€§ã€‚å¼•å…¥é—¨æ§çº¿æ€§æ³¨æ„åŠ›å’ŒRMSNormæ¨¡å—ï¼Œä½¿å˜æ¢å™¨èƒ½å¤Ÿå…³æ³¨å…³é”®åŠ¨ä½œå¹¶æŠ‘åˆ¶å¼‚å¸¸è¿åŠ¨æˆ–å¤šæ¨¡æ€åˆ†å¸ƒå¼‚è´¨æ€§å¼•èµ·çš„ä¸ç¨³å®šæ€§ã€‚åˆ©ç”¨DiTç»“æ„æ‰©æ•£å˜æ¢å™¨çš„æ¡ä»¶ä¿¡æ¯ï¼Œå¹¶é€šè¿‡AdaLNå’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶èåˆæ–‡æœ¬ã€è¯­éŸ³å’ŒéŸ³ä¹ä¿¡å·ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ‰€æœ‰æ¨¡æ€ä¸Šå‡ä¼˜äºå…ˆå‰æ–¹æ³•ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è¿åŠ¨ã€è¯­éŸ³åˆ°æ‰‹åŠ¿å’ŒéŸ³ä¹åˆ°èˆè¹ˆä»»åŠ¡ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€äººä½“è¿åŠ¨ç”Ÿæˆæä¾›äº†ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡è¿ç»­æ©ç è‡ªå›å½’å»ºæ¨¡å’Œæœ‰æ•ˆçš„å¤šæ¨¡æ€èåˆæœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡å’Œæ¨¡æ€é€‚åº”æ€§ã€‚æ¡†æ¶çš„å¯æ‰©å±•æ€§ä¸ºæœªæ¥æ›´å¤æ‚çš„å¤šæ¨¡æ€äº¤äº’ä»»åŠ¡å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Whole-body multi-modal human motion generation poses two primary challenges:
creating an effective motion generation mechanism and integrating various
modalities, such as text, speech, and music, into a cohesive framework. Unlike
previous methods that usually employ discrete masked modeling or autoregressive
modeling, we develop a continuous masked autoregressive motion transformer,
where a causal attention is performed considering the sequential nature within
the human motion. Within this transformer, we introduce a gated linear
attention and an RMSNorm module, which drive the transformer to pay attention
to the key actions and suppress the instability caused by either the abnormal
movements or the heterogeneous distributions within multi-modalities. To
further enhance both the motion generation and the multimodal generalization,
we employ the DiT structure to diffuse the conditions from the transformer
towards the targets. To fuse different modalities, AdaLN and cross-attention
are leveraged to inject the text, speech, and music signals. Experimental
results demonstrate that our framework outperforms previous methods across all
modalities, including text-to-motion, speech-to-gesture, and music-to-dance.
The code of our method will be made public.


### [40] [ChangingGrounding: 3D Visual Grounding in Changing Scenes](https://arxiv.org/abs/2510.14965)
*Miao Hu, Zhiwei Huang, Tai Wang, Jiangmiao Pang, Dahua Lin, Nanning Zheng, Runsen Xu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ChangingGroundingåŸºå‡†æµ‹è¯•å’ŒMem-ChangingGrounderæ–¹æ³•ï¼Œé¦–æ¬¡å°†3Dè§†è§‰å®šä½é‡æ–°å®šä¹‰ä¸ºä¸»åŠ¨ã€è®°å¿†é©±åŠ¨çš„é—®é¢˜ï¼Œåœ¨åŠ¨æ€åœºæ™¯ä¸­é€šè¿‡åˆ©ç”¨å†å²è§‚æµ‹å’Œé«˜æ•ˆæ¢ç´¢å®ç°ç²¾ç¡®çš„3Dè¾¹ç•Œæ¡†å®šä½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰3Dè§†è§‰å®šä½æ–¹æ³•é€šå¸¸å‡è®¾å­˜åœ¨é‡å»ºä¸”æœ€æ–°çš„ç‚¹äº‘æ•°æ®ï¼Œè¿™éœ€è¦æ˜‚è´µçš„é‡å¤æ‰«æä¸”é˜»ç¢å®é™…éƒ¨ç½²ï¼Œè€ŒçœŸå®ä¸–ç•Œä¸­æœºå™¨äººéœ€è¦åœ¨åœºæ™¯ä¸æ–­å˜åŒ–çš„æƒ…å†µä¸‹æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤å®šä½ç‰©ä½“ï¼Œå› æ­¤éœ€è¦å°†3Dè§†è§‰å®šä½é‡æ–°å®šä¹‰ä¸ºä¸»åŠ¨ã€è®°å¿†é©±åŠ¨çš„é—®é¢˜ã€‚

**Method:** æå‡ºäº†Mem-ChangingGrounderé›¶æ ·æœ¬æ–¹æ³•ï¼Œç»“åˆè·¨æ¨¡æ€æ£€ç´¢ä¸è½»é‡çº§å¤šè§†å›¾èåˆï¼šé¦–å…ˆè¯†åˆ«æŸ¥è¯¢éšå«çš„ç‰©ä½“ç±»å‹ï¼Œæ£€ç´¢ç›¸å…³è®°å¿†æ¥æŒ‡å¯¼è¡ŒåŠ¨ï¼Œç„¶ååœ¨åœºæ™¯ä¸­é«˜æ•ˆæ¢ç´¢ç›®æ ‡ï¼Œå½“å…ˆå‰æ“ä½œæ— æ•ˆæ—¶å›é€€ï¼Œå¯¹ç›®æ ‡è¿›è¡Œå¤šè§†å›¾æ‰«æï¼Œå¹¶å°†å¤šè§†å›¾æ‰«æçš„èåˆè¯æ®æŠ•å½±ä»¥è·å¾—å‡†ç¡®çš„ç‰©ä½“è¾¹ç•Œæ¡†ã€‚

**Result:** åœ¨ChangingGroundingåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†ä¸åŒåŸºçº¿æ–¹æ³•ï¼ŒMem-ChangingGrounderå®ç°äº†æœ€é«˜çš„å®šä½ç²¾åº¦ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†æ¢ç´¢æˆæœ¬ã€‚

**Conclusion:** è¿™é¡¹ç ”ç©¶æ¨åŠ¨äº†3Dè§†è§‰å®šä½å‘å®ç”¨åŒ–ã€ä»¥è®°å¿†ä¸ºä¸­å¿ƒçš„ç ”ç©¶æ–¹å‘è½¬å˜ï¼Œä¸ºçœŸå®ä¸–ç•Œåº”ç”¨æä¾›äº†æ–°çš„åŸºå‡†å’Œæ–¹æ³•æ¡†æ¶ï¼Œæœ‰æœ›å‚¬åŒ–è¯¥é¢†åŸŸç ”ç©¶èŒƒå¼çš„è½¬å˜ã€‚

---

#### ğŸ“„ Abstract
Real-world robots localize objects from natural-language instructions while
scenes around them keep changing. Yet most of the existing 3D visual grounding
(3DVG) method still assumes a reconstructed and up-to-date point cloud, an
assumption that forces costly re-scans and hinders deployment. We argue that
3DVG should be formulated as an active, memory-driven problem, and we introduce
ChangingGrounding, the first benchmark that explicitly measures how well an
agent can exploit past observations, explore only where needed, and still
deliver precise 3D boxes in changing scenes. To set a strong reference point,
we also propose Mem-ChangingGrounder, a zero-shot method for this task that
marries cross-modal retrieval with lightweight multi-view fusion: it identifies
the object type implied by the query, retrieves relevant memories to guide
actions, then explores the target efficiently in the scene, falls back when
previous operations are invalid, performs multi-view scanning of the target,
and projects the fused evidence from multi-view scans to get accurate object
bounding boxes. We evaluate different baselines on ChangingGrounding, and our
Mem-ChangingGrounder achieves the highest localization accuracy while greatly
reducing exploration cost. We hope this benchmark and method catalyze a shift
toward practical, memory-centric 3DVG research for real-world applications.
Project page: https://hm123450.github.io/CGB/ .


### [41] [Learning an Image Editing Model without Image Editing Pairs](https://arxiv.org/abs/2510.14978)
*Nupur Kumari, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, Xun Huang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€é…å¯¹æ•°æ®çš„å›¾åƒç¼–è¾‘è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡å°†æ‰©æ•£æ¨¡å‹å±•å¼€è®­ç»ƒå¹¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„åé¦ˆè¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œåœ¨æ— ç›‘ç£è®¾ç½®ä¸‹å®ç°äº†ä¸ç›‘ç£æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å›¾åƒç¼–è¾‘æ¨¡å‹ä¾èµ–å¤§è§„æ¨¡è¾“å…¥-ç›®æ ‡é…å¯¹æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä½†è¿™ç±»è‡ªç„¶é…å¯¹çš„è®­ç»ƒæ•°æ®éš¾ä»¥å¤§è§„æ¨¡è·å–ï¼Œè€Œä½¿ç”¨åˆæˆè®­ç»ƒå¯¹ä¼šä¼ æ’­é¢„è®­ç»ƒæ¨¡å‹çš„ä¼ªå½±é—®é¢˜ï¼Œå› æ­¤éœ€è¦æ¶ˆé™¤å¯¹é…å¯¹æ•°æ®çš„ä¾èµ–ã€‚

**Method:** è¯¥æ–¹æ³•é€šè¿‡å±•å¼€å¤šæ­¥æ‰©æ•£æ¨¡å‹è¿›è¡Œç›´æ¥ä¼˜åŒ–ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°ç¼–è¾‘æ˜¯å¦éµå¾ªæŒ‡ä»¤å¹¶ä¿ç•™æœªæ”¹å˜å†…å®¹ï¼Œæä¾›ç«¯åˆ°ç«¯ä¼˜åŒ–çš„ç›´æ¥æ¢¯åº¦ï¼ŒåŒæ—¶ç»“åˆåˆ†å¸ƒåŒ¹é…æŸå¤±æ¥ç¡®ä¿ç”Ÿæˆå›¾åƒä¿æŒåœ¨é¢„è®­ç»ƒæ¨¡å‹å­¦ä¹ åˆ°çš„å›¾åƒæµå½¢å†…ã€‚

**Result:** åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€ä»»ä½•é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨å°‘æ­¥è®¾ç½®ä¸‹ä¸å„ç§åŸºäºå¤§é‡ç›‘ç£é…å¯¹æ•°æ®è®­ç»ƒçš„å›¾åƒç¼–è¾‘æ‰©æ•£æ¨¡å‹æ€§èƒ½ç›¸å½“ï¼Œå¹¶ä¸”åœ¨ç›¸åŒè§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºå¥–åŠ±æ¨¡å‹æ—¶ï¼Œä¼˜äºåŸºäºå¼ºåŒ–å­¦ä¹ çš„æŠ€æœ¯å¦‚Flow-GRPOã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ— éœ€é…å¯¹æ•°æ®çš„å›¾åƒç¼–è¾‘è®­ç»ƒå¯è¡Œæ€§ï¼Œé€šè¿‡ç›´æ¥ä¼˜åŒ–å’Œè§†è§‰è¯­è¨€æ¨¡å‹åé¦ˆå¯ä»¥å…‹æœç›‘ç£æ•°æ®ç¨€ç¼ºçš„ç“¶é¢ˆï¼Œä¸ºå›¾åƒç¼–è¾‘æ¨¡å‹è®­ç»ƒæä¾›äº†æ–°çš„æ— ç›‘ç£èŒƒå¼ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Recent image editing models have achieved impressive results while following
natural language editing instructions, but they rely on supervised fine-tuning
with large datasets of input-target pairs. This is a critical bottleneck, as
such naturally occurring pairs are hard to curate at scale. Current workarounds
use synthetic training pairs that leverage the zero-shot capabilities of
existing models. However, this can propagate and magnify the artifacts of the
pretrained model into the final trained model. In this work, we present a new
training paradigm that eliminates the need for paired data entirely. Our
approach directly optimizes a few-step diffusion model by unrolling it during
training and leveraging feedback from vision-language models (VLMs). For each
input and editing instruction, the VLM evaluates if an edit follows the
instruction and preserves unchanged content, providing direct gradients for
end-to-end optimization. To ensure visual fidelity, we incorporate distribution
matching loss (DMD), which constrains generated images to remain within the
image manifold learned by pretrained models. We evaluate our method on standard
benchmarks and include an extensive ablation study. Without any paired data,
our method performs on par with various image editing diffusion models trained
on extensive supervised paired data, under the few-step setting. Given the same
VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [42] [Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL](https://arxiv.org/abs/2510.13827)
*Ashish Kattamuri, Ishita Prasad, Meetu Malhotra, Arpita Vats, Rahul Raja, Albert Lie*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»“åˆç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œå¤šè¯­è¨€å¯¹æ¯”å¥–åŠ±ä¿¡å·çš„æ¡†æ¶ï¼Œç”¨äºæå‡è·¨è¯­è¨€Text-to-SQLç³»ç»Ÿçš„ä»»åŠ¡æ•ˆç‡å’Œè¯­ä¹‰å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦å¥–åŠ±ä¿¡å·å¢å¼ºSQLç”Ÿæˆä¸ç”¨æˆ·æ„å›¾çš„å¯¹é½ï¼Œåœ¨å°‘é‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ ·æœ¬ä¸‹æ˜¾è‘—æå‡äº†æ‰§è¡Œå‡†ç¡®ç‡å’Œè¯­ä¹‰å‡†ç¡®ç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰Text-to-SQLæ–¹æ³•ä»…å…³æ³¨å¯æ‰§è¡ŒæŸ¥è¯¢çš„è¯„ä¼°ï¼Œå¿½è§†äº†è¯­ä¹‰å¯¹é½æŒ‘æˆ˜â€”â€”åŒ…æ‹¬æŸ¥è¯¢è¯­ä¹‰å«ä¹‰å’Œæ‰§è¡Œç»“æœæ­£ç¡®æ€§ã€‚ä»è‹±è¯­è¿ç§»åˆ°å…¶ä»–è¯­è¨€æ—¶ï¼Œæ‰§è¡Œå‡†ç¡®ç‡å¹³å‡ä¸‹é™6ä¸ªç™¾åˆ†ç‚¹ï¼Œè¡¨æ˜ç°æœ‰æ–¹æ³•åœ¨è·¨è¯­è¨€åœºæ™¯ä¸‹å­˜åœ¨æ˜¾è‘—æ€§èƒ½ç“¶é¢ˆã€‚

**Method:** æå‡ºæ–°æ¡†æ¶ç»“åˆç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸å¤šè¯­è¨€å¯¹æ¯”å¥–åŠ±ä¿¡å·ï¼Œé€šè¿‡åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„å¥–åŠ±ä¿¡å·æ•™å¯¼æ¨¡å‹è·å¾—æ›´å¥½çš„SQLç”Ÿæˆä¸ç”¨æˆ·æ„å›¾å¯¹åº”å…³ç³»ã€‚è¯¥æ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­é›†æˆè¯­ä¹‰å¯¹é½ç›®æ ‡ï¼Œä½¿ç”¨ä»…3000ä¸ªè®­ç»ƒæ ·æœ¬è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚

**Result:** åœ¨ä¸ƒè¯­è¨€MultiSpideræ•°æ®é›†ä¸Šï¼Œä½¿ç”¨GRPOå¾®è°ƒLLaMA-3-3Bæ¨¡å‹å°†æ‰§è¡Œå‡†ç¡®ç‡æå‡è‡³87.4%ï¼ˆæ¯”é›¶æ ·æœ¬æå‡26ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œè¯­ä¹‰å‡†ç¡®ç‡è¾¾52.29%ï¼ˆæå‡32.86ä¸ªç™¾åˆ†ç‚¹ï¼‰ã€‚åŠ å…¥å¯¹æ¯”å¥–åŠ±ä¿¡å·åï¼Œå¹³å‡è¯­ä¹‰å‡†ç¡®ç‡è¿›ä¸€æ­¥æå‡è‡³59.14%ï¼ˆæå‡6.85ä¸ªç™¾åˆ†ç‚¹ï¼Œè¶Šå—è¯­æœ€é«˜æå‡10ä¸ªç™¾åˆ†ç‚¹ï¼‰ã€‚3Bæ¨¡å‹åœ¨ä»…3000æ ·æœ¬è®­ç»ƒä¸‹è¶…è¶Š8Bé›¶æ ·æœ¬æ¨¡å‹çš„æ‰§è¡Œå‡†ç¡®ç‡ï¼ˆ88.86% vs 81.43%ï¼‰ï¼Œè¯­ä¹‰å‡†ç¡®ç‡æ¥è¿‘ï¼ˆ59.14% vs 68.57%ï¼‰ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜é€šè¿‡å¯¹æ¯”å¥–åŠ±ä¿¡å·å®ç°å®šå‘è¯­ä¹‰å¯¹é½ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡Text-to-SQLç³»ç»Ÿæ€§èƒ½ã€‚è¾ƒå°çš„3Bå‚æ•°æ¨¡å‹ç»è¿‡é«˜æ•ˆå¾®è°ƒåèƒ½å¤Ÿè¶…è¶Šæ›´å¤§çš„é›¶æ ·æœ¬æ¨¡å‹ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è·¨è¯­è¨€åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§å’Œå‚æ•°æ•ˆç‡ã€‚è¯¥æ–¹æ³•ä¸ºè§£å†³è¯­ä¹‰å¯¹é½æŒ‘æˆ˜æä¾›äº†æ–°æ€è·¯ï¼Œå¯¹å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨å…·æœ‰é‡è¦ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Current Text-to-SQL methods are evaluated and only focused on executable
queries, overlooking the semantic alignment challenge -- both in terms of the
semantic meaning of the query and the correctness of the execution results.
Even execution accuracy itself shows significant drops when moving from English
to other languages, with an average decline of 6 percentage points across
non-English languages. We address these challenges by presenting a new
framework that combines Group Relative Policy Optimization (GRPO) within a
multilingual contrastive reward signal to enhance both task efficiency and
semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method
teaches models to obtain better correspondence between SQL generation and user
intent by combining a reward signal based on semantic similarity. On the
seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO
improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and
semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive
reward signal in the GRPO framework further improved the average semantic
accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our
experiments showcase that a smaller, parameter-efficient 3B LLaMA model
fine-tuned with our contrastive reward signal outperforms a much larger
zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from
81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly
matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using
just 3,000 reinforcement learning training examples. These results demonstrate
how we can improve the performance of Text-to-SQL systems with contrastive
rewards for directed semantic alignment, without requiring large-scale training
datasets.


### [43] [Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA](https://arxiv.org/abs/2510.13856)
*A H M Rezaul Karim, Ozlem Uzuner*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MasonNLPç³»ç»Ÿï¼Œé‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ç»“åˆé€šç”¨é¢†åŸŸæŒ‡ä»¤è°ƒä¼˜å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸ºä¼¤å£æŠ¤ç†è§†è§‰é—®ç­”ä»»åŠ¡æä¾›ç®€å•æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨MEDIQA-WV 2025å…±äº«ä»»åŠ¡ä¸­æ’åç¬¬ä¸‰ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŒ»ç–—è§†è§‰é—®ç­”åœ¨ä¸´åºŠå†³ç­–å’Œæ‚£è€…æŠ¤ç†ä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œä½†ç°æœ‰ç³»ç»Ÿåœ¨ä¼¤å£æŠ¤ç†é¢†åŸŸçš„å¤šæ¨¡æ€ç†è§£å’Œç»“æ„åŒ–å±æ€§ç”Ÿæˆæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦æé«˜æ¨ç†èƒ½åŠ›ã€æ¨¡å¼éµå¾ªå’Œå“åº”è´¨é‡ã€‚

**Method:** é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆé€šç”¨é¢†åŸŸæŒ‡ä»¤è°ƒä¼˜å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç®€å•çš„ç´¢å¼•å’Œèåˆæœºåˆ¶èå…¥é¢†åŸŸå†…æ–‡æœ¬å’Œè§†è§‰ç¤ºä¾‹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæˆ–å¤æ‚é‡æ’åºï¼Œå®ç°è½»é‡çº§å¤šæ¨¡æ€æ¨ç†ã€‚

**Result:** åœ¨MEDIQA-WV 2025å…±äº«ä»»åŠ¡çš„19ä¸ªå›¢é˜Ÿ51ä¸ªæäº¤ä¸­æ’åç¬¬ä¸‰ï¼Œå¹³å‡å¾—åˆ†41.37%ï¼Œåœ¨dBLEUã€ROUGEã€BERTScoreå’ŒåŸºäºLLMçš„è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜è½»é‡çº§RAGä¸é€šç”¨LLMçš„ç»„åˆä¸ºå¤šæ¨¡æ€ä¸´åºŠNLPä»»åŠ¡æä¾›äº†ç®€å•æœ‰æ•ˆçš„åŸºçº¿æ–¹æ³•ï¼Œé€šè¿‡å°‘é‡ç›¸å…³ç¤ºä¾‹çš„æ£€ç´¢å¢å¼ºå³å¯æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼Œæ— éœ€å¤æ‚æ¶æ„æˆ–é¢å¤–è®­ç»ƒæˆæœ¬ã€‚

---

#### ğŸ“„ Abstract
Medical Visual Question Answering (MedVQA) enables natural language queries
over medical images to support clinical decision-making and patient care. The
MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to
generate free-text responses and structured wound attributes from images and
patient queries. We present the MasonNLP system, which employs a
general-domain, instruction-tuned large language model with a
retrieval-augmented generation (RAG) framework that incorporates textual and
visual examples from in-domain data. This approach grounds outputs in
clinically relevant exemplars, improving reasoning, schema adherence, and
response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our
best-performing system ranked 3rd among 19 teams and 51 submissions with an
average score of 41.37%, demonstrating that lightweight RAG with
general-purpose LLMs -- a minimal inference-time layer that adds a few relevant
exemplars via simple indexing and fusion, with no extra training or complex
re-ranking -- provides a simple and effective baseline for multimodal clinical
NLP tasks.


### [44] [Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues](https://arxiv.org/abs/2510.13862)
*Chenyu Zhang, Sharifa Alghowinem, Cynthia Breazeal*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªç”¨äºå¤§è§„æ¨¡è¾…å¯¼å¯¹è¯æƒ…æ„Ÿæ„ŸçŸ¥çš„é›†æˆLLMæ¡†æ¶ï¼Œé€šè¿‡åˆ†æAIè¾…å¯¼è¿‡ç¨‹ä¸­å­¦ä¹ è€…çš„æƒ…æ„ŸåŠ¨æ€ï¼Œä¸ºè´Ÿè´£ä»»åœ°å°†ç”Ÿæˆå¼AIæ•´åˆåˆ°æ•™è‚²ä¸­æä¾›äº†æ–°è§†è§’ã€‚ç ”ç©¶å‘ç°å­¦ç”Ÿåœ¨AIè¾…å¯¼ä¸­é€šå¸¸å‘ˆç°è½»åº¦ç§¯ææƒ…æ„Ÿï¼Œä½†å›°æƒ‘å’Œå¥½å¥‡æ˜¯è§£å†³é—®é¢˜çš„å¸¸è§ä¼´éšçŠ¶æ€ï¼Œè€Œè´Ÿé¢æƒ…ç»ªå¾€å¾€èƒ½å¿«é€Ÿç¼“è§£ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å·²æœ‰ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²ç¯å¢ƒä¸­çš„å­¦ä¹ å½±å“ï¼Œä½†LLMä»‹å¯¼è¾…å¯¼ä¸­çš„æƒ…æ„ŸåŠ¨æ€ä»æœªè¢«å……åˆ†ç†è§£ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œé€šè¿‡å…³æ³¨å­¦ä¹ è€…ä¸æ–­å˜åŒ–çš„æƒ…æ„ŸçŠ¶æ€ï¼Œæ¨è¿›ç”Ÿæˆå¼AIåœ¨æ•™è‚²ä¸­è´Ÿè´£ä»»æ•´åˆçš„è®¨è®ºã€‚

**Method:** ç ”ç©¶å¼€å‘äº†é¦–ä¸ªé›†æˆLLMæ¡†æ¶ç”¨äºå¤§è§„æ¨¡è¾…å¯¼å¯¹è¯æƒ…æ„Ÿæ„ŸçŸ¥ï¼Œåˆ†æäº†ä¸¤ä¸ªå­¦æœŸå…±16,986ä¸ªå¯¹è¯è½®æ¬¡ï¼Œæ¶‰åŠ261åæœ¬ç§‘ç”Ÿä¸PyTutor AIè¾…å¯¼ç³»ç»Ÿçš„äº¤äº’ã€‚é€šè¿‡ä¸‰ä¸ªå‰æ²¿LLMï¼ˆGeminiã€GPT-4oã€Claudeï¼‰ç”Ÿæˆé›¶æ ·æœ¬æƒ…æ„Ÿæ ‡æ³¨ï¼ŒåŒ…æ‹¬æ•ˆä»·ã€å”¤é†’åº¦å’Œå­¦ä¹ å¸®åŠ©æ€§çš„æ ‡åº¦è¯„åˆ†ä»¥åŠè‡ªç”±æ–‡æœ¬æƒ…æ„Ÿæ ‡ç­¾ï¼Œé‡‡ç”¨æ’ååŠ æƒæ¨¡å‹å†…æ± åŒ–å’Œè·¨æ¨¡å‹å¤šæ•°å…±è¯†èåˆæ–¹æ³•äº§ç”Ÿç¨³å¥çš„æƒ…æ„Ÿæ¡£æ¡ˆã€‚

**Result:** åˆ†ææ˜¾ç¤ºå­¦ç”Ÿåœ¨ä¸AIè¾…å¯¼ç³»ç»Ÿäº¤äº’æ—¶é€šå¸¸æŠ¥å‘Šè½»åº¦ç§¯ææƒ…æ„Ÿå’Œä¸­ç­‰å”¤é†’åº¦ï¼Œä½†å­¦ä¹ è¿‡ç¨‹å¹¶ä¸æ€»æ˜¯é¡ºåˆ©ï¼šå›°æƒ‘å’Œå¥½å¥‡æ˜¯è§£å†³é—®é¢˜çš„å¸¸è§ä¼´éšçŠ¶æ€ï¼Œè€Œæ²®ä¸§è™½ç„¶è¾ƒå°‘å‡ºç°ä½†ä»å¯èƒ½é˜»ç¢è¿›æ­¥ã€‚æƒ…æ„ŸçŠ¶æ€æŒç»­æ—¶é—´è¾ƒçŸ­ï¼Œç§¯ææ—¶åˆ»æ¯”ä¸­æ€§æˆ–æ¶ˆææ—¶åˆ»ç¨é•¿ä½†æ˜“å—å¹²æ‰°ï¼Œè´Ÿé¢æƒ…ç»ªé€šå¸¸èƒ½å¿«é€Ÿç¼“è§£ï¼Œæœ‰æ—¶ç›´æ¥åå¼¹è‡³ç§¯æçŠ¶æ€ã€‚ä¸­æ€§æ—¶åˆ»å¸¸ä½œä¸ºè½¬æŠ˜ç‚¹ï¼Œæ›´å€¾å‘äºå¼•å¯¼å­¦ç”Ÿå‘ä¸Šè€Œéå‘ä¸‹ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†AIè¾…å¯¼ä¸­å­¦ä¹ è€…æƒ…æ„ŸåŠ¨æ€çš„å¤æ‚æ€§ï¼Œä¸­æ€§æ—¶åˆ»ä½œä¸ºæ½œåœ¨å¹²é¢„ç‚¹çš„å‘ç°ä¸ºæ™ºèƒ½è¾…å¯¼ç³»ç»Ÿè®¾è®¡æä¾›äº†é‡è¦å¯ç¤ºã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨AIæ•™è‚²åº”ç”¨ä¸­å…³æ³¨æƒ…æ„Ÿç»´åº¦çš„é‡è¦æ€§ï¼Œä¸ºå¼€å‘æ›´å…·å“åº”æ€§å’Œæ”¯æŒæ€§çš„æ•™è‚²AIç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨æƒ…æ„Ÿè½¬æŠ˜ç‚¹è¿›è¡Œé€‚æ—¶å¹²é¢„çš„æœºä¼šã€‚

---

#### ğŸ“„ Abstract
While recent studies have examined the leaning impact of large language model
(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring
remain insufficiently understood. This work introduces the first ensemble-LLM
framework for large-scale affect sensing in tutoring dialogues, advancing the
conversation on responsible pathways for integrating generative AI into
education by attending to learners' evolving affective states. To achieve this,
we analyzed two semesters' worth of 16,986 conversational turns exchanged
between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across
three U.S. institutions. To investigate learners' emotional experiences, we
generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,
Claude), including scalar ratings of valence, arousal, and
learning-helpfulness, along with free-text emotion labels. These estimates are
fused through rank-weighted intra-model pooling and plurality consensus across
models to produce robust emotion profiles. Our analysis shows that during
interaction with the AI tutor, students typically report mildly positive affect
and moderate arousal. Yet learning is not uniformly smooth: confusion and
curiosity are frequent companions to problem solving, and frustration, while
less common, still surfaces in ways that can derail progress. Emotional states
are short-lived--positive moments last slightly longer than neutral or negative
ones, but they are fragile and easily disrupted. Encouragingly, negative
emotions often resolve quickly, sometimes rebounding directly into positive
states. Neutral moments frequently act as turning points, more often steering
students upward than downward, suggesting opportunities for tutors to intervene
at precisely these junctures.


### [45] [Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization](https://arxiv.org/abs/2510.13885)
*Ariel Kamen*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶å¯¹åç§æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹åœ¨éç»“æ„åŒ–æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œäº†ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œå‘ç°å°½ç®¡æ¨¡å‹è§„æ¨¡ä¸æ–­æ‰©å¤§ï¼Œä½†åˆ†ç±»æ€§èƒ½ä»ç„¶æœ‰é™ï¼Œè€Œé›†æˆæ–¹æ³•æ˜¾è‘—æå‡äº†å‡†ç¡®ç‡å¹¶å®Œå…¨æ¶ˆé™¤äº†å¹»è§‰é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨éç»“æ„åŒ–æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„å®é™…æ€§èƒ½å°šä¸æ˜ç¡®ï¼Œç‰¹åˆ«æ˜¯åœ¨å°†ä¸°å¯Œæ–‡æœ¬å†…å®¹å‹ç¼©åˆ°æœ‰é™åˆ†ç±»ä½“ç³»æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨çœŸå®åˆ†ç±»åœºæ™¯ä¸‹çš„è¡¨ç°å’Œå±€é™æ€§ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨ç»Ÿä¸€çš„IAB 2.2å±‚æ¬¡åˆ†ç±»ä½“ç³»å’Œ8,660ä¸ªäººå·¥æ ‡æ³¨æ ·æœ¬ï¼Œä½¿ç”¨é›¶æ ·æœ¬æç¤ºå¯¹åç§å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹æ¯”è¯„ä¼°ï¼Œå¹¶å¼€å‘äº†åŸºäºå¤šæ¨¡å‹ç‹¬ç«‹ä¸“å®¶çš„é›†æˆæ–¹æ³•æ¥æå‡åˆ†ç±»æ€§èƒ½ã€‚

**Result:** è¯„ä¼°ç»“æœæ˜¾ç¤ºå½“ä»£å¤§è¯­è¨€æ¨¡å‹åœ¨ç»å…¸æŒ‡æ ‡ä¸Šè¡¨ç°ä¸­ç­‰ï¼Œå¹³å‡å‡†ç¡®ç‡34%ã€ç²¾ç¡®ç‡42%ã€å¬å›ç‡45%ã€F1åˆ†æ•°41%ï¼ŒåŒæ—¶å­˜åœ¨è¾ƒé«˜çš„å¹»è§‰ç‡å’Œè†¨èƒ€ç‡ï¼Œè€Œé›†æˆæ–¹æ³•æ˜¾è‘—æå‡äº†å‡†ç¡®ç‡å¹¶å®Œå…¨æ¶ˆé™¤äº†å¹»è§‰é—®é¢˜ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜å•çº¯ä¾èµ–æ¨¡å‹è§„æ¨¡å’Œæ¶æ„æ”¹è¿›æ— æ³•ä¿è¯æ›´å¥½çš„åˆ†ç±»æ€§èƒ½ï¼Œè€Œæ¨¡å‹åè°ƒç¼–æ’æ¯”å•çº¯æ‰©å¤§è§„æ¨¡æ›´èƒ½æœ‰æ•ˆæå‡å¤§è§„æ¨¡æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è¡¨ç°ï¼Œç”šè‡³å¯èƒ½è¾¾åˆ°æˆ–è¶…è¶Šäººç±»ä¸“å®¶æ°´å¹³ã€‚

---

#### ğŸ“„ Abstract
This study presents a comparative evaluation of ten state-of-the-art large
language models (LLMs) applied to unstructured text categorization using the
Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis
employed a uniform dataset of 8,660 human-annotated samples and identical
zero-shot prompts to ensure methodological consistency across all models.
Evaluation metrics included four classic measures - accuracy, precision,
recall, and F1-score - and three LLM-specific indicators: hallucination ratio,
inflation ratio, and categorization cost.
  Results show that, despite their rapid advancement, contemporary LLMs achieve
only moderate classic performance, with average scores of 34% accuracy, 42%
precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios
reveal that models frequently overproduce categories relative to human
annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B
offered the most favorable cost-to-performance balance, while GPT 120B
demonstrated the lowest hallucination ratio. The findings suggest that scaling
and architectural improvements alone do not ensure better categorization
accuracy, as the task requires compressing rich unstructured text into a
limited taxonomy - a process that challenges current model architectures.
  To address these limitations, a separate ensemble-based approach was
developed and tested. The ensemble method, in which multiple LLMs act as
independent experts, substantially improved accuracy, reduced inflation, and
completely eliminated hallucinations. These results indicate that coordinated
orchestration of models - rather than sheer scale - may represent the most
effective path toward achieving or surpassing human-expert performance in
large-scale text categorization.


### [46] [Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning](https://arxiv.org/abs/2510.13909)
*Xingrui Zhuo, Jiapu Wang, Gongqing Wu, Zhongyuan Wang, Jichen Zhang, Shirui Pan, Xindong Wu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†çŸ¥è¯†æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆKRLMï¼‰ï¼Œé€šè¿‡è®¾è®¡çŸ¥è¯†æ¨ç†è¯­è¨€æŒ‡ä»¤æ ¼å¼å’ŒåŠ¨æ€çŸ¥è¯†è®°å¿†æœºåˆ¶ï¼Œå®ç°äº†å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†ä¸çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡åœ¨å½’çº³çŸ¥è¯†å›¾è°±æ¨ç†ä¸­çš„ç»Ÿä¸€åè°ƒï¼Œæœ‰æ•ˆè§£å†³äº†LLMçŸ¥è¯†æ‰­æ›²å’Œç”Ÿæˆå¹»è§‰é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å›¾è°±åŸºç¡€æ¨¡å‹åœ¨å½’çº³çŸ¥è¯†å›¾è°±æ¨ç†ä¸­å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šLLMçš„å†…åœ¨çŸ¥è¯†å¯èƒ½è¢«ç¨€ç–çš„çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡æ‰€æ©ç›–å¯¼è‡´çŸ¥è¯†æ‰­æ›²ï¼Œä»¥åŠç°æœ‰æ–¹æ³•éš¾ä»¥å……åˆ†çº¦æŸLLMçš„ç”Ÿæˆå¹»è§‰ï¼Œä¸¥é‡é™åˆ¶äº†æ¨ç†ç»“æœçš„å¯ä¿¡åº¦ã€‚

**Method:** æå‡ºäº†çŸ¥è¯†æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆKRLMï¼‰ï¼ŒåŒ…æ‹¬è®¾è®¡çŸ¥è¯†æ¨ç†è¯­è¨€æŒ‡ä»¤æ ¼å¼å’ŒKRLåˆ†è¯å™¨æ¥å¯¹é½LLMçŸ¥è¯†ä¸KGè¡¨ç¤ºï¼Œæå‡ºKRLæ³¨æ„åŠ›å±‚é€šè¿‡åŠ¨æ€çŸ¥è¯†è®°å¿†æœºåˆ¶åè°ƒå†…åœ¨LLMçŸ¥è¯†ä¸é¢å¤–KGä¸Šä¸‹æ–‡ï¼Œä»¥åŠè®¾è®¡ç»“æ„æ„ŸçŸ¥çš„ä¸‹ä¸€ä¸ªå®ä½“é¢„æµ‹å™¨æ¥ä¸¥æ ¼çº¦æŸæ¨ç†ç»“æœåœ¨å¯ä¿¡çŸ¥è¯†åŸŸå†…ã€‚

**Result:** åœ¨25ä¸ªçœŸå®ä¸–ç•Œå½’çº³çŸ¥è¯†å›¾è°±æ¨ç†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„KRLMåœ¨é›¶æ ·æœ¬æ¨ç†å’Œå¾®è°ƒåœºæ™¯ä¸‹å‡è¡¨ç°å‡ºæ˜¾è‘—ä¼˜è¶Šæ€§ï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨è§£å†³çŸ¥è¯†æ‰­æ›²å’Œçº¦æŸç”Ÿæˆå¹»è§‰æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡ç»Ÿä¸€åè°ƒLLMçŸ¥è¯†ä¸KGä¸Šä¸‹æ–‡ï¼ŒæˆåŠŸè§£å†³äº†å½’çº³çŸ¥è¯†å›¾è°±æ¨ç†ä¸­çš„çŸ¥è¯†æ‰­æ›²å’Œç”Ÿæˆå¹»è§‰é—®é¢˜ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„å¼€æ”¾åŸŸçŸ¥è¯†æ¨ç†ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨çœŸå®åœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in
open-domain KGs containing unknown entities and relations, which poses a
challenge for KGR models in comprehending uncertain KG components. Existing
studies have proposed Knowledge Graph Foundation Models (KGFMs) that learn
structural invariances across KGs to handle this uncertainty. Recently, Large
Language Models (LLMs) have demonstrated strong capabilities for open-domain
knowledge reasoning. As a result, the latest research has focused on LLM-based
KGFMs that integrate LLM knowledge with KG context for inductive KGR. However,
the intrinsic knowledge of LLMs may be overshadowed by sparse KG context,
leading to LLM knowledge distortion, which can cause irreversible damage to
model reasoning. Moreover, existing LLM-based KGR methods still struggle to
fully constrain generative hallucinations in LLMs, severely limiting the
credibility of reasoning results. To address these limitations, we propose a
Knowledge Reasoning Language Model (KRLM) that achieves unified coordination
between LLM knowledge and KG context throughout the KGR process. Specifically,
we design a Knowledge Reasoning Language (KRL) instruction format and a KRL
tokenizer to align LLM knowledge with KG representations. Then, we propose a
KRL attention layer that coordinates intrinsic LLM knowledge with additional KG
context through a dynamic knowledge memory mechanism. Finally, a
structure-aware next-entity predictor is proposed, which strictly constrains
the reasoning results within a trustworthy knowledge domain. Extensive
experimental results on 25 real-world inductive KGR datasets demonstrate the
significant superiority of the proposed KRLM\footnote{Our source codes are
available at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot
reasoning and fine-tuning scenarios.


### [47] [MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning](https://arxiv.org/abs/2510.14305)
*Mahbub E Sobhani, Md. Faiyaz Abdullah Sayeedi, Tasnim Mohiuddin, Md Mofijul Islam, Swakkhar Shatabda*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†MathMistï¼Œä¸€ä¸ªåŒ…å«è¶…è¿‡21Kå¯¹é½é—®é¢˜-ç­”æ¡ˆå¯¹çš„å¹¶è¡Œå¤šè¯­è¨€æ•°å­¦æ¨ç†åŸºå‡†ï¼Œæ¶µç›–ä¸ƒç§è¯­è¨€ï¼Œç³»ç»Ÿè¯„ä¼°äº†LLMsåœ¨è·¨è¯­è¨€æ•°å­¦æ¨ç†ä¸­çš„ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ç¼ºé™·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ•°å­¦æ¨ç†åŸºå‡†ä¸»è¦å…³æ³¨è‹±è¯­æˆ–å°‘æ•°é«˜èµ„æºè¯­è¨€ï¼Œç¼ºä¹å¯¹å¤šè¯­è¨€å’Œè·¨è¯­è¨€æ•°å­¦æ¨ç†èƒ½åŠ›çš„å…¨é¢è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­å­˜åœ¨æ˜¾è‘—çš„ç ”ç©¶ç©ºç™½ã€‚

**Method:** æ„å»ºäº†è¦†ç›–é«˜ã€ä¸­ã€ä½èµ„æºè¯­è¨€çš„å¹³è¡Œå¤šè¯­è¨€æ•°æ®é›†ï¼Œç³»ç»Ÿè¯„ä¼°äº†å¼€æºå’Œä¸“æœ‰LLMsåœ¨é›¶æ ·æœ¬ã€æ€ç»´é“¾å’Œä»£ç åˆ‡æ¢æ¨ç†èŒƒå¼ä¸‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬å¤šè¯­è¨€æ¨ç†ä¸“ç”¨æ¨¡å‹ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºLLMsåœ¨è·¨è¯­è¨€æ•°å­¦æ¨ç†ä¸­å­˜åœ¨æŒç»­ç¼ºé™·ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæ¨¡å‹éš¾ä»¥ä¿æŒæ¨ç†çš„ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰LLMsåœ¨å¤šè¯­è¨€æ•°å­¦æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œå¼ºè°ƒäº†å¼€å‘æ›´é²æ£’çš„å¤šè¯­è¨€æ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†åŸºå‡†å’Œæ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Mathematical reasoning remains one of the most challenging domains for large
language models (LLMs), requiring not only linguistic understanding but also
structured logical deduction and numerical precision. While recent LLMs
demonstrate strong general-purpose reasoning abilities, their mathematical
competence across diverse languages remains underexplored. Existing benchmarks
primarily focus on English or a narrow subset of high-resource languages,
leaving significant gaps in assessing multilingual and cross-lingual
mathematical reasoning. To address this, we introduce MathMist, a parallel
multilingual benchmark for mathematical problem solving and reasoning. MathMist
encompasses over 21K aligned question-answer pairs across seven languages,
representing a balanced coverage of high-, medium-, and low-resource linguistic
settings. The dataset captures linguistic variety, multiple types of problem
settings, and solution synthesizing capabilities. We systematically evaluate a
diverse suite of models, including open-source small and medium LLMs,
proprietary systems, and multilingual-reasoning-focused models, under
zero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our
results reveal persistent deficiencies in LLMs' ability to perform consistent
and interpretable mathematical reasoning across languages, with pronounced
degradation in low-resource settings. All the codes and data are available at
GitHub: https://github.com/mahbubhimel/MathMist


### [48] [MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and Linking](https://arxiv.org/abs/2510.14307)
*Sathyanarayanan Ramamoorthy, Vishwa Shah, Simran Khanuja, Zaid Sheikh, Shan Jie, Ann Chia, Shearman Chua, Graham Neubig*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MERLINï¼Œä¸€ä¸ªç”¨äºå¤šè¯­è¨€å¤šæ¨¡æ€å®ä½“é“¾æ¥ä»»åŠ¡çš„æ–°å‹æµ‹è¯•å¹³å°ç³»ç»Ÿï¼ŒåŒ…å«BBCæ–°é—»æ–‡ç« æ ‡é¢˜å’Œå¯¹åº”å›¾åƒçš„å¤šè¯­è¨€æ•°æ®é›†ï¼Œå¹¶è¯æ˜äº†è§†è§‰æ•°æ®èƒ½å¤Ÿæå‡å®ä½“é“¾æ¥å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ä¸Šä¸‹æ–‡æ¨¡ç³Šæˆ–ä¸è¶³çš„æƒ…å†µä¸‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šè¯­è¨€å®ä½“é“¾æ¥ä»»åŠ¡ä¸»è¦ä¾èµ–æ–‡æœ¬ä¿¡æ¯ï¼Œç¼ºä¹å¯¹å¤šæ¨¡æ€æ•°æ®çš„å……åˆ†åˆ©ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ä¸Šä¸‹æ–‡æ¨¡ç³Šæˆ–ä¸è¶³çš„æƒ…å†µä¸‹å®ä½“é“¾æ¥å‡†ç¡®æ€§å—é™ï¼Œéœ€è¦æ¢ç´¢è§†è§‰ä¿¡æ¯åœ¨å¤šè¯­è¨€å®ä½“é“¾æ¥ä¸­çš„ä»·å€¼ã€‚

**Method:** æ„å»ºäº†åŒ…å«BBCæ–°é—»æ–‡ç« æ ‡é¢˜å’Œå¯¹åº”å›¾åƒçš„å¤šè¯­è¨€æ•°æ®é›†ï¼Œæ¶µç›–å°åœ°è¯­ã€æ—¥è¯­ã€å°å°¼è¯­ã€è¶Šå—è¯­å’Œæ³°ç±³å°”è¯­äº”ç§è¯­è¨€ï¼ŒåŒ…å«7000å¤šä¸ªå‘½åå®ä½“æåŠé“¾æ¥åˆ°2500ä¸ªç‹¬ç‰¹çš„Wikidataå®ä½“ï¼Œå¹¶é‡‡ç”¨å¤šè¯­è¨€å’Œå¤šæ¨¡æ€å®ä½“é“¾æ¥æ–¹æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ¢ç´¢äº†LLaMa-2å’ŒAya-23ç­‰ä¸åŒè¯­è¨€æ¨¡å‹ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œèå…¥è§†è§‰æ•°æ®èƒ½å¤Ÿæ˜¾è‘—æå‡å®ä½“é“¾æ¥çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ä¸Šä¸‹æ–‡æ¨¡ç³Šæˆ–ä¸è¶³çš„å®ä½“è¯†åˆ«ä¸­æ•ˆæœæ›´ä¸ºæ˜æ˜¾ï¼Œå¯¹äºç¼ºä¹å¼ºå¤§å¤šè¯­è¨€èƒ½åŠ›çš„æ¨¡å‹è€Œè¨€ï¼Œè§†è§‰ä¿¡æ¯çš„è¡¥å……ä½œç”¨å°¤ä¸ºçªå‡ºã€‚

**Conclusion:** å¤šæ¨¡æ€æ–¹æ³•åœ¨å¤šè¯­è¨€å®ä½“é“¾æ¥ä»»åŠ¡ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œè§†è§‰ä¿¡æ¯èƒ½å¤Ÿæœ‰æ•ˆè¡¥å……æ–‡æœ¬ä¿¡æ¯çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šè¯­è¨€åœºæ™¯æ—¶ï¼Œæœªæ¥ç ”ç©¶åº”æ›´å……åˆ†åœ°æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ä»¥æå‡å®ä½“é“¾æ¥æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºåŒ®ä¹è¯­è¨€ç¯å¢ƒä¸­ã€‚

---

#### ğŸ“„ Abstract
This paper introduces MERLIN, a novel testbed system for the task of
Multilingual Multimodal Entity Linking. The created dataset includes BBC news
article titles, paired with corresponding images, in five languages: Hindi,
Japanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity
mentions linked to 2,500 unique Wikidata entities. We also include several
benchmarks using multilingual and multimodal entity linking methods exploring
different language models like LLaMa-2 and Aya-23. Our findings indicate that
incorporating visual data improves the accuracy of entity linking, especially
for entities where the textual context is ambiguous or insufficient, and
particularly for models that do not have strong multilingual abilities. For the
work, the dataset, methods are available here at
https://github.com/rsathya4802/merlin


### [49] [PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora](https://arxiv.org/abs/2510.14377)
*Mykolas Sveistrys, Richard Kunert*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PluriHopRAGæ¡†æ¶æ¥è§£å†³å¤šè·³é—®ç­”ä¸­çš„æ–‡æ¡£é‡å¤å’Œå¹²æ‰°é¡¹æŒ‘æˆ˜ï¼Œé€šè¿‡æ–‡æ¡£çº§å­é—®é¢˜åˆ†è§£å’Œäº¤å‰ç¼–ç å™¨è¿‡æ»¤æœºåˆ¶ï¼Œåœ¨çœŸå®é£èƒ½è¡Œä¸šæŠ¥å‘Šæ•°æ®é›†ä¸Šå®ç°äº†18-52%çš„ç›¸å¯¹F1åˆ†æ•°æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰é—®ç­”ç³»ç»Ÿåœ¨å¤„ç†éœ€è¦è·¨æ‰€æœ‰æ–‡æ¡£èšåˆä¿¡æ¯çš„pluri-hopé—®é¢˜æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™ç±»é—®é¢˜å¯¹å¬å›ç‡æ•æ„Ÿã€éœ€è¦ç©·å°½æ€§æ£€ç´¢ä¸”å¯¹é—æ¼æ–‡æ¡£é«˜åº¦æ•æ„Ÿï¼Œè€Œä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•åœ¨é‡å¤æ€§å¼ºã€å¹²æ‰°æ–‡æ¡£å¯†é›†çš„å®é™…æŠ¥å‘Šè¯­æ–™ä¸Šè¡¨ç°ä¸ä½³ã€‚

**Method:** æå‡ºäº†PluriHopRAGæ¶æ„ï¼Œé‡‡ç”¨"é€ä¸ªæ£€æŸ¥æ‰€æœ‰æ–‡æ¡£ã€å»‰ä»·è¿‡æ»¤"ç­–ç•¥ï¼šé¦–å…ˆå°†æŸ¥è¯¢åˆ†è§£ä¸ºæ–‡æ¡£çº§å­é—®é¢˜ï¼Œç„¶åä½¿ç”¨äº¤å‰ç¼–ç å™¨è¿‡æ»¤å™¨åœ¨æ˜‚è´µçš„LLMæ¨ç†å‰ä¸¢å¼ƒæ— å…³æ–‡æ¡£ï¼Œä»è€Œä¼˜åŒ–æ£€ç´¢æ•ˆç‡ã€‚

**Result:** åœ¨PluriHopWINDå¤šè¯­è¨€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¼ ç»ŸRAGã€å›¾åŸºå’Œå¤šåª’ä½“å˜ä½“æ–¹æ³•çš„è¯­å¥çº§F1åˆ†æ•°å‡æœªè¶…è¿‡40%ï¼Œè€ŒPluriHopRAGå®ç°äº†18-52%çš„ç›¸å¯¹F1åˆ†æ•°æå‡ï¼Œå…·ä½“æå‡å¹…åº¦å–å†³äºåŸºç¡€LLMæ¨¡å‹ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†å½“å‰é—®ç­”ç³»ç»Ÿåœ¨é‡å¤æ€§å¼ºã€å¹²æ‰°é¡¹ä¸°å¯Œçš„è¯­æ–™ä¸Šçš„å±€é™æ€§ï¼Œè¯æ˜äº†ç©·å°½æ€§æ£€ç´¢å’Œæ—©æœŸè¿‡æ»¤ä½œä¸ºtop-kæ–¹æ³•æ›¿ä»£æ–¹æ¡ˆçš„ä»·å€¼ï¼Œä¸ºå¤„ç†ç°å®ä¸–ç•ŒæŠ¥å‘Šæ•°æ®æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Recent advances in large language models (LLMs) and retrieval-augmented
generation (RAG) have enabled progress on question answering (QA) when relevant
evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many
realistic questions about recurring report data - medical records, compliance
filings, maintenance logs - require aggregation across all documents, with no
clear stopping point for retrieval and high sensitivity to even one missed
passage. We term these pluri-hop questions and formalize them by three
criteria: recall sensitivity, exhaustiveness, and exactness. To study this
setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48
pluri-hop questions built from 191 real-world wind industry reports in German
and English. We show that PluriHopWIND is 8-40% more repetitive than other
common datasets and thus has higher density of distractor documents, better
reflecting practical challenges of recurring report corpora. We test a
traditional RAG pipeline as well as graph-based and multimodal variants, and
find that none of the tested approaches exceed 40% in statement-wise F1 score.
Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a
"check all documents individually, filter cheaply" approach: it (i) decomposes
queries into document-level subquestions and (ii) uses a cross-encoder filter
to discard irrelevant documents before costly LLM reasoning. We find that
PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base
LLM. Despite its modest size, PluriHopWIND exposes the limitations of current
QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance
highlights the value of exhaustive retrieval and early filtering as a powerful
alternative to top-k methods.


### [50] [Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents](https://arxiv.org/abs/2510.14438)
*Rui Wang, Ce Zhang, Jun-Yu Ma, Jianshu Zhang, Hongru Wang, Yi Chen, Boyang Xue, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu, Kam-Fai Wong*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¢ç´¢è¿›åŒ–èŒƒå¼ï¼Œç”¨äºæ„å»ºå¯éªŒè¯çš„Webæ™ºèƒ½ä½“è®­ç»ƒæ•°æ®ï¼Œå¹¶å¼€å‘äº†WebAggregatorç³»åˆ—åŸºç¡€æ¨¡å‹ï¼Œåœ¨ä¿¡æ¯èšåˆèƒ½åŠ›ä¸Šè¶…è¶Šäº†GPT-4.1å’ŒClaude-3.7-sonnetã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¼€æºæ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ä¸»è¦å…³æ³¨å¢å¼ºä¿¡æ¯æ£€ç´¢èƒ½åŠ›ï¼Œä½†å¿½è§†äº†ä¿¡æ¯èšåˆè¿™ä¸€å…³é”®éœ€æ±‚ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ”¯æŒæ·±åº¦ç ”ç©¶çš„èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦è§£å†³Webæ™ºèƒ½ä½“åœ¨ä¿¡æ¯èšåˆæ–¹é¢çš„èƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚

**Method:** é‡‡ç”¨æ¢ç´¢è¿›åŒ–èŒƒå¼ï¼Œé€šè¿‡ä¸»åŠ¨åœ¨çº¿æ¢ç´¢æ”¶é›†çœŸå®ç½‘ç»œè¯æ®ï¼Œç„¶åæ™ºèƒ½ä½“è‡ªè¿›åŒ–èšåˆç¨‹åºï¼Œä»12ç§é«˜çº§é€»è¾‘ç±»å‹ä¸­é€‰æ‹©ã€ç»„åˆå’Œä¼˜åŒ–æ“ä½œæ¥åˆæˆå¯éªŒè¯çš„é—®ç­”å¯¹ï¼ŒåŸºäºSmolAgentsæ¡†æ¶æ”¶é›†ç›‘ç£å¾®è°ƒè½¨è¿¹å¼€å‘WebAggregatorç³»åˆ—åŸºç¡€æ¨¡å‹ã€‚

**Result:** æ„å»ºäº†åŒ…å«10Kæ ·æœ¬ã€è¦†ç›–50Kç½‘ç«™å’Œ11ä¸ªé¢†åŸŸçš„WebAggregatorQAæ•°æ®é›†ï¼ŒWebAggregator-8Bæ¨¡å‹æ€§èƒ½ä¸GPT-4.1ç›¸å½“ï¼Œ32Bå˜ä½“åœ¨GAIA-textä¸Šè¶…è¶ŠGPT-4.1è¶…è¿‡10%ï¼Œæ¥è¿‘Claude-3.7-sonnetï¼Œåœ¨WebAggregatorQAåŸºå‡†æµ‹è¯•ä¸­Claude-3.7-sonnetä»…å¾—28%ï¼ŒGPT-4.1å¾—25.8%ã€‚

**Conclusion:** å³ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ£€ç´¢åˆ°æ‰€æœ‰å‚è€ƒèµ„æ–™ï¼Œåœ¨WebAggregatorQAä¸Šä»ç„¶è¡¨ç°ä¸ä½³ï¼Œå‡¸æ˜¾äº†åŠ å¼ºWebæ™ºèƒ½ä½“åŸºç¡€æ¨¡å‹ä¿¡æ¯èšåˆèƒ½åŠ›çš„å¿…è¦æ€§ï¼Œä¸ºæ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†é‡è¦æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Deep research web agents not only retrieve information from diverse sources
such as web environments, files, and multimodal inputs, but more importantly,
they need to rigorously analyze and aggregate knowledge for insightful
research. However, existing open-source deep research agents predominantly
focus on enhancing information-seeking capabilities of web agents to locate
specific information, while overlooking the essential need for information
aggregation, which would limit their ability to support in-depth research. We
propose an Explore to Evolve paradigm to scalably construct verifiable training
data for web agents. Begins with proactive online exploration, an agent sources
grounded information by exploring the real web. Using the collected evidence,
the agent then self-evolves an aggregation program by selecting, composing, and
refining operations from 12 high-level logical types to synthesize a verifiable
QA pair. This evolution from high-level guidance to concrete operations allowed
us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K
websites and 11 domains. Based on an open-source agent framework, SmolAgents,
we collect supervised fine-tuning trajectories to develop a series of
foundation models, WebAggregator. WebAggregator-8B matches the performance of
GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text
and closely approaches Claude-3.7-sonnet. Moreover, given the limited
availability of benchmarks that evaluate web agents' information aggregation
abilities, we construct a human-annotated evaluation split of WebAggregatorQA
as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves
28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all
references, they still struggle on WebAggregatorQA, highlighting the need to
strengthen the information aggregation capabilities of web agent foundations.


### [51] [Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures](https://arxiv.org/abs/2510.14616)
*Shuangshuang Ying, Yunwen Li, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Xeron Du, Tianyu Zheng, Yichi Zhang, Letian Ni, Yuyang Cheng, Qiguang Chen, Jingzhe Ding, Shengda Long, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Ge Zhang, Wenhao Huang, Wanxiang Che, Chenghua Lin*

#### ğŸ§© TL;DR
æœ¬æ–‡å¼•å…¥WritingPreferenceBenchåŸºå‡†ï¼Œå‘ç°å½“å‰RLHFæ–¹æ³•ä¸»è¦å­¦ä¹ æ£€æµ‹å®¢è§‚é”™è¯¯è€Œéæ•æ‰ä¸»è§‚è´¨é‡åå¥½ï¼Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹é€šè¿‡æ˜¾å¼æ¨ç†é“¾åœ¨ä¸»è§‚åå¥½è¯„ä¼°ä¸Šæ˜¾è‘—ä¼˜äºåºåˆ—å¼å¥–åŠ±æ¨¡å‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åå¥½å­¦ä¹ æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å½“ç§»é™¤å®¢è§‚è´¨é‡ä¿¡å·æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¿™æš´éœ²äº†ç°æœ‰æ–¹æ³•åœ¨æ•æ‰ä¸»è§‚è´¨é‡åå¥½ï¼ˆå¦‚åˆ›æ„æ€§ã€é£æ ¼ç‰¹è‰²å’Œæƒ…æ„Ÿå…±é¸£ï¼‰æ–¹é¢çš„å±€é™æ€§ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†åŒ…å«1,800ä¸ªäººå·¥æ ‡æ³¨åå¥½å¯¹çš„å¤šè¯­è¨€å†™ä½œæ•°æ®é›†ï¼Œæ¯”è¾ƒäº†åºåˆ—å¼å¥–åŠ±æ¨¡å‹ã€é›¶æ ·æœ¬è¯­è¨€æ¨¡å‹è¯„ä¼°å™¨å’Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹åœ¨ä¸»è§‚è´¨é‡åå¥½è¯„ä¼°ä¸Šçš„è¡¨ç°ï¼Œå…¶ä¸­ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹é€šè¿‡äº§ç”Ÿæ˜¾å¼æ¨ç†é“¾æ¥è¿›è¡Œåå¥½åˆ¤æ–­ã€‚

**Result:** åºåˆ—å¼å¥–åŠ±æ¨¡å‹å¹³å‡å‡†ç¡®ç‡ä»…ä¸º52.7%ï¼Œé›¶æ ·æœ¬è¯­è¨€æ¨¡å‹è¯„ä¼°å™¨ä¸º53.9%ï¼Œè€Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹è¾¾åˆ°81.8%å‡†ç¡®ç‡ï¼›ä¸åŒå†™ä½œä½“è£é—´å­˜åœ¨é«˜åº¦æ¨¡å‹æ–¹å·®ï¼Œå‡†ç¡®ç‡èŒƒå›´ä»18.2%åˆ°81.8%ï¼Œæ ‡å‡†å·®å¹³å‡ä¸º10.1%ï¼Œä¸”æ¨¡å‹è§„æ¨¡æ‰©å¤§å¹¶æœªå¸¦æ¥ä¸€è‡´æ”¹è¿›ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜æˆåŠŸçš„åå¥½å»ºæ¨¡å¯èƒ½éœ€è¦ä¸­é—´æ¨ç†è¡¨ç¤ºè€Œéç›´æ¥åˆ†ç±»ï¼Œå½“å‰RLHFæ–¹æ³•ä¸»è¦å­¦ä¹ æ£€æµ‹å®¢è§‚é”™è¯¯è€Œéæ•æ‰ä¸»è§‚è´¨é‡åå¥½ï¼Œè¿™ä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„åå¥½å­¦ä¹ æ¡†æ¶æä¾›äº†é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
Current preference learning methods achieve high accuracy on standard
benchmarks but exhibit significant performance degradation when objective
quality signals are removed. We introduce WritingPreferenceBench, a dataset of
1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8
creative writing genres, where responses are matched for objective correctness,
factual accuracy, and length. On this benchmark, sequence-based reward
models--the standard architecture for RLHF--achieve only 52.7% mean accuracy,
while zero-shot language model judges perform at 53.9%. In contrast, generative
reward models that produce explicit reasoning chains achieve 81.8% accuracy. We
observe high within-model variance across genres: individual models range from
18.2% to 81.8% accuracy across different writing categories, with standard
deviations averaging 10.1%. This variance persists regardless of model scale,
with 27B parameter models showing no consistent improvement over 8B variants.
Our results suggest that current RLHF methods primarily learn to detect
objective errors rather than capture subjective quality preferences (e.g.,
creativity, stylistic flair, and emotional resonance), and that successful
preference modeling may require intermediate reasoning representations rather
than direct classification.


### [52] [AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning](https://arxiv.org/abs/2510.14738)
*Mengzhao Jia, Zhihan Zhang, Ignacio Cases, Zheyuan Liu, Meng Jiang, Peng Qi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºAutoRubric-R1Væ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨æ”¶é›†åŸºäºé‡è§„çš„ç”Ÿæˆå¥–åŠ±å°†å¼ºåŒ–å­¦ä¹ ä¸è¿‡ç¨‹çº§ç›‘ç£ç›¸ç»“åˆï¼Œè§£å†³äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­ä»…å¥–åŠ±æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§å¯¼è‡´çš„è™šå‡æ¨ç†é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å·²ä»æ„ŸçŸ¥ä»»åŠ¡å‘å±•åˆ°å¤æ‚å¤šæ­¥æ¨ç†ï¼Œä½†åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ é€šå¸¸å¯¼è‡´è™šå‡æ¨ç†ï¼Œå› ä¸ºä»…å¥–åŠ±æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§è€Œå¿½ç•¥äº†æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚

**Method:** æå‡ºAutoRubric-R1Væ¡†æ¶ï¼Œæ ¸å¿ƒåˆ›æ–°æ˜¯å¯æ‰©å±•çš„è‡ªèšåˆæ–¹æ³•ï¼Œä»æˆåŠŸè½¨è¿¹ä¸­æç‚¼ä¸€è‡´çš„æ¨ç†æ£€æŸ¥ç‚¹ï¼Œæ— éœ€äººå·¥æ ‡æ³¨æˆ–æ›´å¼ºæ•™å¸ˆæ¨¡å‹å³å¯æ„å»ºé—®é¢˜ç‰¹å®šçš„é‡è§„ï¼Œé€šè¿‡è”åˆåˆ©ç”¨åŸºäºé‡è§„çš„å¥–åŠ±å’Œç»“æœå¥–åŠ±å®ç°è¿‡ç¨‹çº§ç›‘ç£ã€‚

**Result:** AutoRubric-R1Våœ¨å…­ä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä¸“é—¨çš„è¯„ä¼°ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†çš„å¿ å®åº¦ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜å°†è¿‡ç¨‹çº§ç›‘ç£ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆå¯æœ‰æ•ˆæå‡å¤šæ¨¡æ€æ¨ç†çš„è´¨é‡å’Œå¯é æ€§ï¼Œä¸ºå¤æ‚æ¨ç†ä»»åŠ¡çš„è®­ç»ƒæä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨ç›‘ç£èµ„æºã€‚

---

#### ğŸ“„ Abstract
Multimodal large language models (MLLMs) have rapidly advanced from
perception tasks to complex multi-step reasoning, yet reinforcement learning
with verifiable rewards (RLVR) often leads to spurious reasoning since only the
final-answer correctness is rewarded. To address this limitation, we propose
AutoRubric-R1V, a framework that integrates RLVR with process-level supervision
through automatically collected rubric-based generative rewards. Our key
innovation lies in a scalable self-aggregation method that distills consistent
reasoning checkpoints from successful trajectories, enabling problem-specific
rubric construction without human annotation or stronger teacher models. By
jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves
state-of-the-art performance on six multimodal reasoning benchmarks and
substantially improves reasoning faithfulness in dedicated evaluations.


### [53] [Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking](https://arxiv.org/abs/2510.14824)
*Ziqi Dai, Xin Zhang, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, Min Zhang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ¯”è¾ƒå’Œç†è®ºåˆ†æï¼Œå‘ç°åœ¨åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ£€ç´¢é‡æ’åºä»»åŠ¡ä¸­ï¼Œç›‘ç£å¾®è°ƒ(SFT)ç›¸æ¯”å¯¹æ¯”å­¦ä¹ (CL)å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸»è¦å½’å› äºSFTæä¾›æ›´å¼ºçš„æƒé‡æ›´æ–°æœºåˆ¶ï¼Œå¹¶åœ¨MRBåŸºå‡†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ£€ç´¢é‡æ’åºæ¨¡å‹è®­ç»ƒå­˜åœ¨ä¸¤ç§ä¸»è¦ç›®æ ‡ï¼šåŸºäºåº¦é‡å­¦ä¹ çš„å¯¹æ¯”æŸå¤±å’ŒåŸºäºåˆ†ç±»çš„ç›‘ç£å¾®è°ƒï¼Œå¯¹äºBERTç¼–ç å™¨å¯¹æ¯”å­¦ä¹ æ›´æœ‰æ•ˆï¼Œè€Œå¯¹äºå¤§è¯­è¨€æ¨¡å‹ç›‘ç£å¾®è°ƒä¼¼ä¹æ›´æœ‰å‰æ™¯ï¼Œè¿™ç§åˆ†æ­§å¼•å‘äº†å¯¹å“ªç§ç›®æ ‡æ›´é€‚åˆLLMé‡æ’åºåŠå…¶å†…åœ¨æœºåˆ¶çš„æ ¸å¿ƒç ”ç©¶é—®é¢˜ã€‚

**Method:** æœ¬ç ”ç©¶åœ¨é€šç”¨å¤šæ¨¡æ€æ£€ç´¢(UMR)å®éªŒå¹³å°ä¸Šå¯¹å¯¹æ¯”å­¦ä¹ (CL)å’Œç›‘ç£å¾®è°ƒ(SFT)è¿›è¡Œç³»ç»Ÿæ¯”è¾ƒï¼Œå°†ç›®æ ‡å‡½æ•°åˆ†è§£ä¸ºæ§åˆ¶æ›´æ–°å¹…åº¦çš„æƒé‡ç»„ä»¶å’ŒæŒ‡å¯¼æ¨¡å‹æ›´æ–°æ–¹å‘çš„æ–¹å‘ç»„ä»¶ï¼Œæå‡ºç»Ÿä¸€æ¡†æ¶åˆ†æå…¶äº¤äº’ä½œç”¨ï¼Œå¹¶é€šè¿‡æ¢æµ‹å®éªŒéªŒè¯ä¸åŒç»„ä»¶çš„è´¡çŒ®ã€‚

**Result:** å®éªŒå‘ç°SFTç›¸æ¯”CLæä¾›æ˜¾è‘—æ›´å¼ºçš„æƒé‡æ›´æ–°æ–¹æ¡ˆï¼Œè€Œè¯„åˆ†æ–¹å‘åå¥½æ— æ˜æ˜¾ä¼˜åŠ£ï¼Œç»¼åˆç»“æœè¡¨æ˜SFTåœ¨LLMé‡æ’åºä¸­å…·æœ‰ä¸€è‡´ä¼˜åŠ¿ï¼Œå¤§è§„æ¨¡SFTè®­ç»ƒåœ¨MRBåŸºå‡†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›é‡æ’åºå™¨æ€§èƒ½ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†SFTåœ¨LLMé‡æ’åºä¸­çš„å†…åœ¨ä¼˜åŠ¿æœºåˆ¶ï¼Œä¸»è¦æºäºå…¶æ›´å¼ºçš„æƒé‡æ›´æ–°èƒ½åŠ›ï¼Œä¸ºæœªæ¥è¯¥é¢†åŸŸç ”ç©¶å’Œåº”ç”¨æä¾›äº†é‡è¦æŒ‡å¯¼ï¼ŒåŒæ—¶é€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†SFTè®¾ç½®çš„æœ‰æ•ˆæ€§ï¼Œæ¨åŠ¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ£€ç´¢é‡æ’åºæŠ€æœ¯å‘å±•ã€‚

---

#### ğŸ“„ Abstract
In information retrieval, training reranking models mainly focuses on two
types of objectives: metric learning (e.g. contrastive loss to increase the
predicted scores on relevant query-document pairs) and classification (binary
label prediction of relevance vs. irrelevance). For BERT-style encoders,
various studies have shown that contrastive learning (CL) can be more effective
than discriminative (classification) learning. However, for large language
models (LLMs), classification via supervised fine-tuning (SFT), which predicts
''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears
more promising as it aligns well with the generative nature of LLMs. This
divergence raises a central question: which objective is intrinsically better
suited to LLM-based reranking, and what mechanism underlies the difference? In
this work, we conduct a comprehensive comparison and analysis between CL and
SFT for reranking, taking the universal multimodal retrieval (UMR) as the
experimental playground. We first decompose the objectives into two components:
weight, which controls the magnitude of those updates, and direction, which
guides the model updates, then present a unified framework for understanding
their interactions. Through probing experiments, we find that SFT provides a
substantially stronger weighting scheme than CL, whereas the preferred scoring
direction shows no clear winner. Taken together, these results point to a
consistent advantage of SFT over CL for LLM reranking. To further validate our
findings, we conduct large-scale training with SFT and present new
state-of-the-art rerankers on the MRB benchmark. We also provide ablations on
SFT settings and expect our findings to benefit future research and
applications in this area.


### [54] [AI-Powered Early Diagnosis of Mental Health Disorders from Real-World Clinical Conversations](https://arxiv.org/abs/2510.14937)
*Jianfeng Zhu, Julina Maharjan, Xinyu Li, Karin G. Coifman, Ruoming Jin*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯„ä¼°äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å¿ƒç†å¥åº·ç­›æŸ¥ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨553ä¸ªçœŸå®ä¸–ç•ŒåŠç»“æ„åŒ–è®¿è°ˆæ•°æ®é›†ï¼Œåœ¨æŠ‘éƒã€ç„¦è™‘å’ŒPTSDè¯Šæ–­ä¸­å®ç°äº†è¶…è¿‡80%çš„å‡†ç¡®ç‡ï¼Œä¸ºä¸´åºŠç¯å¢ƒæä¾›äº†å¯æ‰©å±•çš„AIè¾…åŠ©è¯Šæ–­å·¥å…·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¿ƒç†å¥åº·éšœç¢æ˜¯å…¨çƒè‡´æ®‹çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œä½†æŠ‘éƒç—‡ã€ç„¦è™‘ç—‡å’Œåˆ›ä¼¤ååº”æ¿€éšœç¢ç­‰ç–¾ç—…å¸¸å› ä¸»è§‚è¯„ä¼°ã€ä¸´åºŠèµ„æºæœ‰é™ä»¥åŠæ±¡ååŒ–å’Œè®¤çŸ¥ä¸è¶³è€Œè¢«æ¼è¯Šæˆ–è¯¯è¯Šï¼Œåˆçº§ä¿å¥ç¯å¢ƒä¸­è¶…è¿‡60%çš„ç—…ä¾‹è¢«è¯¯åˆ¤ï¼ŒäºŸéœ€å¼€å‘å¯æ‰©å±•ã€æ˜“è·å–ä¸”å…·æœ‰æƒ…å¢ƒæ„ŸçŸ¥èƒ½åŠ›çš„è¯Šæ–­å·¥å…·æ¥æ”¯æŒæ—©æœŸæ£€æµ‹å’Œå¹²é¢„ã€‚

**Method:** ç ”ç©¶ä½¿ç”¨553ä¸ªçœŸå®ä¸–ç•ŒåŠç»“æ„åŒ–è®¿è°ˆæ•°æ®é›†ï¼Œè¯„ä¼°äº†å¤šç§æ¨¡å‹ç±»åˆ«ï¼ŒåŒ…æ‹¬GPT-4.1 Miniå’ŒMetaLLaMAçš„é›¶æ ·æœ¬æç¤ºæ–¹æ³•ï¼Œä»¥åŠä½¿ç”¨ä½ç§©é€‚åº”æŠ€æœ¯å¾®è°ƒçš„RoBERTaæ¨¡å‹ï¼Œç‰¹åˆ«æ¢ç´¢äº†è¾ƒçŸ­ä¸Šä¸‹æ–‡å’Œèšç„¦ä¸Šä¸‹æ–‡ç‰‡æ®µå¯¹æ£€æµ‹æ€§èƒ½çš„å½±å“ã€‚

**Result:** æ¨¡å‹åœ¨å„ç±»è¯Šæ–­ä¸­å®ç°äº†è¶…è¿‡80%çš„å‡†ç¡®ç‡ï¼Œå…¶ä¸­PTSDæ£€æµ‹è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œå‡†ç¡®ç‡è¾¾åˆ°89%ï¼Œå¬å›ç‡è¾¾åˆ°98%ï¼Œä½¿ç”¨è¾ƒçŸ­ä¸Šä¸‹æ–‡ç‰‡æ®µèƒ½æ˜¾è‘—æå‡å¬å›ç‡ï¼Œä½ç§©é…ç½®åœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶å®ç°äº†é«˜æ•ˆçš„å¾®è°ƒæ•ˆæœã€‚

**Conclusion:** åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœºå™¨å­¦ä¹ æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿè‡ªæŠ¥å‘Šç­›æŸ¥å·¥å…·å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºä½é—¨æ§›AIè¾…åŠ©æ—©æœŸè¯Šæ–­æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œè¿™é¡¹ç ”ç©¶ä¸ºå°†æœºå™¨å­¦ä¹ æ•´åˆåˆ°çœŸå®ä¸–ç•Œä¸´åºŠå·¥ä½œæµç¨‹å¥ å®šäº†åŸºç¡€ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºåŒ®ä¹æˆ–é«˜æ±¡ååŒ–ç¯å¢ƒä¸­å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Mental health disorders remain among the leading cause of disability
worldwide, yet conditions such as depression, anxiety, and Post-Traumatic
Stress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to
subjective assessments, limited clinical resources, and stigma and low
awareness. In primary care settings, studies show that providers misidentify
depression or anxiety in over 60% of cases, highlighting the urgent need for
scalable, accessible, and context-aware diagnostic tools that can support early
detection and intervention. In this study, we evaluate the effectiveness of
machine learning models for mental health screening using a unique dataset of
553 real-world, semistructured interviews, each paried with ground-truth
diagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We
benchmark multiple model classes, including zero-shot prompting with GPT-4.1
Mini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank
Adaptation (LoRA). Our models achieve over 80% accuracy across diagnostic
categories, with especially strongperformance on PTSD (up to 89% accuracy and
98% recall). We also find that using shorter context, focused context segments
improves recall, suggesting that focused narrative cues enhance detection
sensitivity. LoRA fine-tuning proves both efficient and effective, with
lower-rank configurations (e.g., rank 8 and 16) maintaining competitive
performance across evaluation metrics. Our results demonstrate that LLM-based
models can offer substantial improvements over traditional self-report
screening tools, providing a path toward low-barrier, AI-powerd early
diagnosis. This work lays the groundwork for integrating machine learning into
real-world clinical workflows, particularly in low-resource or high-stigma
environments where access to timely mental health care is most limited.


### [55] [DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation](https://arxiv.org/abs/2510.14949)
*Yu Zhou, Sohyun An, Haikang Deng, Da Yin, Clark Peng, Cho-Jui Hsieh, Kai-Wei Chang, Nanyun Peng*

#### ğŸ§© TL;DR
æœ¬æ–‡æ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡è‹±è¯­æ–¹è¨€å¤šæ¨¡æ€ç”ŸæˆåŸºå‡†ï¼Œå‘ç°ç°æœ‰æ¨¡å‹åœ¨æ–¹è¨€è¾“å…¥ä¸Šå­˜åœ¨32-48%æ€§èƒ½ä¸‹é™ï¼Œå¹¶æå‡ºä¸€ç§ç¼–ç å™¨ä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨ä¿æŒæ ‡å‡†è‹±è¯­æ€§èƒ½çš„åŒæ—¶å°†æ–¹è¨€ç”Ÿæˆè´¨é‡æå‡34.4%ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†è‹±è¯­æ–¹è¨€è¾“å…¥æ—¶å­˜åœ¨ä¸¥é‡æ€§èƒ½é€€åŒ–é—®é¢˜ï¼Œä½†ç¼ºä¹ç³»ç»Ÿæ€§è¯„ä¼°æ–¹è¨€ç†è§£èƒ½åŠ›çš„åŸºå‡†ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½å¹¶æ¢ç´¢æœ‰æ•ˆçš„ç¼“è§£ç­–ç•¥ã€‚

**Method:** æ„å»ºäº†æ¶µç›–å…­ç§å¸¸è§è‹±è¯­æ–¹è¨€çš„å¤§è§„æ¨¡åŸºå‡†ï¼ŒåŒ…å«4200å¤šä¸ªç»è¿‡æ–¹è¨€ä½¿ç”¨è€…éªŒè¯çš„æç¤ºï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºç¼–ç å™¨çš„é€šç”¨ä¼˜åŒ–æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«æ–°æ–¹è¨€ç‰¹å¾è€Œä¸æŸå®³æ ‡å‡†è‹±è¯­æ€§èƒ½ã€‚

**Result:** å®éªŒè¯„ä¼°17ä¸ªå›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹æ˜¾ç¤ºï¼Œå½“æç¤ºä¸­ä½¿ç”¨å•ä¸ªæ–¹è¨€è¯æ±‡æ—¶ï¼Œæ€§èƒ½ä¸‹é™è¾¾32.26%è‡³48.17%ï¼›æå‡ºçš„ç¼–ç å™¨æ–¹æ³•åœ¨Stable Diffusion 1.5ä¸ŠæˆåŠŸå°†äº”ç§æ–¹è¨€æ€§èƒ½æå‡è‡³ä¸æ ‡å‡†è‹±è¯­ç›¸å½“æ°´å¹³ï¼ˆ+34.4%ï¼‰ï¼ŒåŒæ—¶æ ‡å‡†è‹±è¯­æ€§èƒ½å‡ ä¹æ— æŸå¤±ã€‚

**Conclusion:** å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹å­˜åœ¨æ˜¾è‘—çš„æ–¹è¨€ç†è§£ç¼ºé™·ï¼Œä¼ ç»Ÿå¾®è°ƒå’Œæç¤ºé‡å†™æ–¹æ³•æ•ˆæœæœ‰é™ï¼Œè€Œæå‡ºçš„ç¼–ç å™¨ä¼˜åŒ–ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡æ–¹è¨€é€‚åº”ä¸æ ‡å‡†è‹±è¯­ä¿æŒï¼Œä¸ºæ„å»ºæ›´å…·åŒ…å®¹æ€§çš„å¤šæ¨¡æ€AIç³»ç»Ÿæä¾›äº†å¯è¡Œè·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Contact languages like English exhibit rich regional variations in the form
of dialects, which are often used by dialect speakers interacting with
generative models. However, can multimodal generative models effectively
produce content given dialectal textual input? In this work, we study this
question by constructing a new large-scale benchmark spanning six common
English dialects. We work with dialect speakers to collect and verify over 4200
unique prompts and evaluate on 17 image and video generative models. Our
automatic and human evaluation results show that current state-of-the-art
multimodal generative models exhibit 32.26% to 48.17% performance degradation
when a single dialect word is used in the prompt. Common mitigation methods
such as fine-tuning and prompt rewriting can only improve dialect performance
by small margins (< 7%), while potentially incurring significant performance
degradation in Standard American English (SAE). To this end, we design a
general encoder-based mitigation strategy for multimodal generative models. Our
method teaches the model to recognize new dialect features while preserving SAE
performance. Experiments on models such as Stable Diffusion 1.5 show that our
method is able to simultaneously raise performance on five dialects to be on
par with SAE (+34.4%), while incurring near zero cost to SAE performance.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [56] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra, Jan Niehues*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§èåˆè§†è§‰ä¿¡æ¯çš„å¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆæ¼”ç¤ºå¹»ç¯ç‰‡æ¥å¢å¼ºç§‘å­¦æ¼”è®²åœºæ™¯ä¸‹çš„è¯­éŸ³è¯†åˆ«æ€§èƒ½ï¼Œåœ¨é¢†åŸŸç‰¹å®šæœ¯è¯­è¯†åˆ«ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æœ€å…ˆè¿›çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸»è¦ä¾èµ–å£°å­¦ä¿¡æ¯è€Œå¿½ç•¥äº†å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œç„¶è€Œè§†è§‰ä¿¡æ¯åœ¨æ¶ˆæ­§å’Œé€‚åº”ä¸­è‡³å…³é‡è¦ã€‚å¤§å¤šæ•°å·¥ä½œä¸“æ³¨äºä½¿ç”¨è¯´è¯è€…å›¾åƒå¤„ç†å™ªå£°æ¡ä»¶ï¼Œæœ¬ç ”ç©¶åˆ™ä¸“æ³¨äºåœ¨ç§‘å­¦æ¼”è®²åœºæ™¯ä¸­æ•´åˆæ¼”ç¤ºå¹»ç¯ç‰‡ä»¥æå‡è¯†åˆ«å‡†ç¡®æ€§ã€‚

**Method:** ç ”ç©¶é¦–å…ˆåˆ›å»ºäº†åŒ…å«é¢†åŸŸç‰¹å®šæœ¯è¯­è‡ªåŠ¨åˆ†æçš„å¤šæ¨¡æ€æ¼”è®²åŸºå‡†ï¼Œç„¶åæ¢ç´¢äº†ç”¨å¤šæ¨¡æ€ä¿¡æ¯å¢å¼ºè¯­éŸ³æ¨¡å‹çš„æ–¹æ³•ã€‚é€šè¿‡æ•°æ®å¢å¼ºæ–¹æ³•ç¼“è§£äº†ä¼´éšå¹»ç¯ç‰‡çš„æ•°æ®é›†ç¼ºä¹é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¢å¼ºæ•°æ®é›†è®­ç»ƒäº†å¤šæ¨¡æ€èåˆæ¨¡å‹ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„å¤šæ¨¡æ€æ–¹æ³•åœ¨æ‰€æœ‰è¯æ±‡ä¸Šå®ç°äº†çº¦34%çš„è¯é”™è¯¯ç‡ç›¸å¯¹é™ä½ï¼Œåœ¨é¢†åŸŸç‰¹å®šæœ¯è¯­ä¸Šå®ç°äº†35%çš„ç›¸å¯¹æ”¹è¿›ï¼Œæ˜¾è‘—æå‡äº†ç§‘å­¦æ¼”è®²åœºæ™¯ä¸‹çš„è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åœ¨ç§‘å­¦æ¼”è®²åœºæ™¯ä¸­æ•´åˆæ¼”ç¤ºå¹»ç¯ç‰‡ç­‰è§†è§‰ä¿¡æ¯å¯¹æå‡è¯­éŸ³è¯†åˆ«æ€§èƒ½çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹é¢†åŸŸç‰¹å®šæœ¯è¯­çš„è¯†åˆ«æ”¹è¿›å°¤ä¸ºæ˜¾è‘—ï¼Œä¸ºå¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«åœ¨ä¸“ä¸šé¢†åŸŸçš„åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚

---

#### ğŸ“„ Abstract
State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.


### [57] [GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations](https://arxiv.org/abs/2510.14035)
*Rajesh Mangannavar, Prasad Tadepalli*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†GammaZeroï¼Œä¸€ç§åŸºäºåŠ¨ä½œä¸­å¿ƒå›¾è¡¨ç¤ºçš„éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹è§„åˆ’å¼•å¯¼æ¡†æ¶ï¼Œé€šè¿‡ç»Ÿä¸€çš„å›¾è¡¨ç¤ºå®ç°è·¨é—®é¢˜è§„æ¨¡çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä¿æŒè§£è´¨é‡çš„åŒæ—¶æ˜¾è‘—å‡å°‘æœç´¢éœ€æ±‚ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰POMDPè§„åˆ’æ–¹æ³•éœ€è¦é¢†åŸŸç‰¹å®šçš„ç¥ç»ç½‘ç»œæ¶æ„ä¸”éš¾ä»¥æ‰©å±•ï¼ŒGammaZeroæ—¨åœ¨è§£å†³è¿™ä¸€å¯æ‰©å±•æ€§é—®é¢˜ï¼Œé€šè¿‡å¼€å‘ç»Ÿä¸€çš„å›¾è¡¨ç¤ºæ¡†æ¶å®ç°åœ¨é¢†åŸŸå†…ä¸åŒè§„æ¨¡é—®é¢˜é—´çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Method:** GammaZeroé‡‡ç”¨åŠ¨ä½œä¸­å¿ƒå›¾è¡¨ç¤ºä¿¡å¿µçŠ¶æ€ï¼Œåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå’Œç¼–ç å™¨-è§£ç å™¨æ¶æ„ä»ä¸“å®¶æ¼”ç¤ºä¸­å­¦ä¹ ä»·å€¼å‡½æ•°å’Œç­–ç•¥ï¼Œç„¶åå°†ä¹ å¾—çš„å¯å‘å¼æ–¹æ³•åº”ç”¨äºæ›´å¤§è§„æ¨¡é—®é¢˜çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢å¼•å¯¼ã€‚

**Result:** åœ¨æ ‡å‡†POMDPåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGammaZeroåœ¨ç›¸åŒè§„æ¨¡é—®é¢˜ä¸Šä¸BetaZeroæ€§èƒ½ç›¸å½“ï¼ŒåŒæ—¶å®ç°äº†é›¶æ ·æœ¬æ³›åŒ–åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„2-4å€å¤§è§„æ¨¡é—®é¢˜ï¼Œåœ¨ä¿æŒè§£è´¨é‡çš„åŒæ—¶å‡å°‘äº†æœç´¢éœ€æ±‚ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜åŠ¨ä½œä¸­å¿ƒå›¾è¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆæ•è·POMDPä¸­çš„ç»“æ„æ¨¡å¼ï¼Œä½¿å¾—åœ¨å°è§„æ¨¡é—®é¢˜ä¸Šå­¦ä¹ çš„å¯å‘å¼æ–¹æ³•èƒ½å¤ŸæˆåŠŸè¿ç§»åˆ°æ›´å¤§è§„æ¨¡é—®é¢˜ï¼Œä¸ºå¯æ‰©å±•çš„POMDPè§„åˆ’æä¾›äº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
We introduce an action-centric graph representation framework for learning to
guide planning in Partially Observable Markov Decision Processes (POMDPs).
Unlike existing approaches that require domain-specific neural architectures
and struggle with scalability, GammaZero leverages a unified graph-based belief
representation that enables generalization across problem sizes within a
domain. Our key insight is that belief states can be systematically transformed
into action-centric graphs where structural patterns learned on small problems
transfer to larger instances. We employ a graph neural network with a decoder
architecture to learn value functions and policies from expert demonstrations
on computationally tractable problems, then apply these learned heuristics to
guide Monte Carlo tree search on larger problems. Experimental results on
standard POMDP benchmarks demonstrate that GammaZero achieves comparable
performance to BetaZero when trained and tested on the same-sized problems,
while uniquely enabling zero-shot generalization to problems 2-4 times larger
than those seen during training, maintaining solution quality with reduced
search requirements.


### [58] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui, AdÃ¨le Cormier, nistor Grozavu, Ann Bourges*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§å¤šæ¨¡æ€æ¶æ„ï¼Œèåˆä¼ æ„Ÿå™¨æ•°æ®å’Œè§†è§‰å›¾åƒæ¥é¢„æµ‹æ–‡åŒ–é—äº§åœ°çš„é€€åŒ–ä¸¥é‡ç¨‹åº¦ï¼Œé€šè¿‡ç®€åŒ–çš„ç¼–ç å™¨å’Œè‡ªé€‚åº”Barlow TwinsæŸå¤±åœ¨æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹å®ç°76.9%çš„å‡†ç¡®ç‡ï¼Œç›¸æ¯”æ ‡å‡†å¤šæ¨¡æ€æ–¹æ³•æå‡43%ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ–‡åŒ–é—äº§åœ°å› æ°”å€™å˜åŒ–é¢ä¸´åŠ é€Ÿé€€åŒ–ï¼Œä½†ä¼ ç»Ÿç›‘æµ‹æ–¹æ³•ä¾èµ–å•æ¨¡æ€åˆ†æï¼ˆä»…è§†è§‰æ£€æŸ¥æˆ–ç¯å¢ƒä¼ æ„Ÿå™¨ï¼‰ï¼Œæ— æ³•æ•æ‰ç¯å¢ƒå‹åŠ›ä¸ææ–™é€€åŒ–ä¹‹é—´çš„å¤æ‚ç›¸äº’ä½œç”¨ã€‚

**Method:** é‡‡ç”¨æ”¹è¿›çš„PerceiverIOæ¶æ„ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šç®€åŒ–çš„ç¼–ç å™¨ï¼ˆ64ç»´æ½œåœ¨ç©ºé—´ï¼‰é˜²æ­¢å°æ•°æ®é›†è¿‡æ‹Ÿåˆï¼Œä»¥åŠè‡ªé€‚åº”Barlow TwinsæŸå¤±é¼“åŠ±æ¨¡æ€äº’è¡¥æ€§è€Œéå†—ä½™ã€‚

**Result:** åœ¨æ–¯ç‰¹æ‹‰æ–¯å ¡å¤§æ•™å ‚æ•°æ®ä¸Šè¾¾åˆ°76.9%å‡†ç¡®ç‡ï¼Œæ¯”æ ‡å‡†å¤šæ¨¡æ€æ¶æ„æå‡43%ï¼Œæ¯”åŸå§‹PerceiverIOæå‡25%ã€‚æ¶ˆèç ”ç©¶æ˜¾ç¤ºä¼ æ„Ÿå™¨å•ç‹¬ä¸º61.5%ï¼Œå›¾åƒå•ç‹¬ä¸º46.2%ï¼Œè¯å®äº†å¤šæ¨¡æ€ååŒæ•ˆåº”ã€‚

**Conclusion:** æ¶æ„ç®€æ´æ€§ç»“åˆå¯¹æ¯”æ­£åˆ™åŒ–èƒ½å¤Ÿåœ¨æ•°æ®ç¨€ç¼ºçš„æ–‡åŒ–é—äº§ç›‘æµ‹åœºæ™¯ä¸­å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€å­¦ä¹ ï¼Œä¸ºAIé©±åŠ¨çš„ä¿æŠ¤å†³ç­–æ”¯æŒç³»ç»Ÿå¥ å®šåŸºç¡€ï¼ŒåŒæ—¶ç³»ç»Ÿè¶…å‚æ•°ç ”ç©¶ç¡®å®šäº†æœ€ä½³ä¸­ç­‰ç›¸å…³æ€§ç›®æ ‡ï¼ˆÏ„=0.3ï¼‰ä»¥å¹³è¡¡å¯¹é½å’Œäº’è¡¥æ€§ã€‚

---

#### ğŸ“„ Abstract
Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.


### [59] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer, Faisal Mohamed, Pablo Samuel Castro, Cyrus Neary, Glen Berseth*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºARM-FMæ¡†æ¶ï¼Œé€šè¿‡åŸºç¡€æ¨¡å‹è‡ªåŠ¨æ„å»ºå¥–åŠ±æœºæ¥è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå®ç°äº†ä»è‡ªç„¶è¯­è¨€åˆ°ç»“æ„åŒ–å¥–åŠ±è§„èŒƒçš„è‡ªåŠ¨åŒ–è½¬æ¢ï¼Œå¹¶åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­å±•ç¤ºäº†æœ‰æ•ˆæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹å¥–åŠ±å‡½æ•°è§„èŒƒé«˜åº¦æ•æ„Ÿï¼Œè¿™é™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å®ç°è‡ªåŠ¨åŒ–ã€ç»„åˆå¼çš„å¥–åŠ±è®¾è®¡ï¼Œéœ€è¦è§£å†³ä»é«˜å±‚æ¬¡ä»»åŠ¡æè¿°åˆ°å…·ä½“å¥–åŠ±è§„èŒƒçš„è½¬æ¢é—®é¢˜ã€‚

**Method:** é‡‡ç”¨å¥–åŠ±æœºä½œä¸ºå¼ºåŒ–å­¦ä¹ ç›®æ ‡è§„èŒƒçš„å½¢å¼åŒ–æœºåˆ¶ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹çš„é«˜å±‚æ¨ç†èƒ½åŠ›è‡ªåŠ¨ä»è‡ªç„¶è¯­è¨€è§„èŒƒç”Ÿæˆå¥–åŠ±æœºï¼Œå¹¶ä¸ºæ¯ä¸ªè‡ªåŠ¨æœºçŠ¶æ€å…³è”è¯­è¨€åµŒå…¥ä»¥å®ç°è·¨ä»»åŠ¡æ³›åŒ–ã€‚

**Result:** åœ¨å¤šæ ·åŒ–æŒ‘æˆ˜æ€§ç¯å¢ƒå¥—ä»¶ä¸­æä¾›äº†ARM-FMæœ‰æ•ˆæ€§çš„å®è¯è¯æ®ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›çš„å±•ç¤ºï¼Œè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸå®ç°ä»è‡ªç„¶è¯­è¨€åˆ°ç»“æ„åŒ–å¥–åŠ±è§„èŒƒçš„è½¬æ¢ã€‚

**Conclusion:** ARM-FMæ¡†æ¶é€šè¿‡ç»“åˆåŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¥–åŠ±æœºçš„ç»“æ„åŒ–å½¢å¼åŒ–ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±è®¾è®¡æä¾›äº†è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†è¯­è¨€å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ½œåŠ›ï¼Œä¸ºæ›´å¹¿æ³›çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.


### [60] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*GÃ¶ktuÄŸ Bender, Samer Faraj, Anand Bhardwaj*

#### ğŸ§© TL;DR
æœ¬æ–‡é€šè¿‡èŒƒå›´ç»¼è¿°åˆ†æäº†2019-2024å¹´é—´äººå·¥æ™ºèƒ½åœ¨ç²¾å‡†åŒ»å­¦ä¸­çš„å®æ–½æƒ…å†µï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºç”Ÿæ€ç³»ç»Ÿçš„æ¡†æ¶æ¥è¯†åˆ«å…³é”®éšœç¢å’Œä¿ƒè¿›å› ç´ ï¼Œå¹¶ä¸ºå¯ä¿¡èµ–å’Œå¯æŒç»­çš„å®æ–½æä¾›æœªæ¥æ–¹å‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡äººå·¥æ™ºèƒ½åœ¨ç²¾å‡†åŒ»å­¦ä¸­é€šè¿‡æ•´åˆå’Œè§£é‡Šå¤šæ¨¡æ€æ•°æ®å‘æŒ¥ç€è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ï¼Œä½†åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å®æ–½ä»ç„¶æœ‰é™ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€å®æ–½å·®è·ã€‚

**Method:** é‡‡ç”¨èŒƒå›´ç»¼è¿°æ–¹æ³•ï¼Œåˆ†æ2019-2024å¹´æ–‡çŒ®ï¼Œè¯†åˆ«æ•°æ®è´¨é‡ã€ä¸´åºŠå¯é æ€§ã€å·¥ä½œæµç¨‹æ•´åˆå’Œæ²»ç†ç­‰å…³é”®ç»´åº¦çš„éšœç¢å’Œä¿ƒè¿›å› ç´ ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŸºäºç”Ÿæ€ç³»ç»Ÿçš„åˆ†ææ¡†æ¶ã€‚

**Result:** ç ”ç©¶è¯†åˆ«äº†å½±å“äººå·¥æ™ºèƒ½åœ¨ç²¾å‡†åŒ»å­¦ä¸­å®é™…è½¬åŒ–çš„å…³é”®éšœç¢å’Œä¿ƒè¿›å› ç´ ï¼Œå¼ºè°ƒäº†å¡‘é€ ç°å®ä¸–ç•Œè½¬åŒ–çš„ç›¸äº’ä¾èµ–å…³ç³»ï¼Œä¸ºå®æ–½ç­–ç•¥æä¾›äº†å®è¯åŸºç¡€ã€‚

**Conclusion:** ç ”ç©¶å¼ºè°ƒäº†é‡‡ç”¨ç”Ÿæ€ç³»ç»Ÿè§†è§’ç†è§£äººå·¥æ™ºèƒ½åœ¨ç²¾å‡†åŒ»å­¦ä¸­å®æ–½å¤æ‚æ€§çš„é‡è¦æ€§ï¼Œæå‡ºäº†æ”¯æŒå¯ä¿¡èµ–å’Œå¯æŒç»­å®æ–½çš„å…·ä½“æ–¹å‘ï¼ŒåŒ…æ‹¬æ”¹è¿›æ•°æ®æ²»ç†ã€å¢å¼ºä¸´åºŠå¯é æ€§å’Œä¼˜åŒ–å·¥ä½œæµç¨‹æ•´åˆã€‚

---

#### ğŸ“„ Abstract
Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.


### [61] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu, Zihao Zhou, Kaizhu Huang, Xiaowei Huang, Qiufeng Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºIP-Mergingæ–¹æ³•ï¼Œä¸€ç§æ— éœ€è°ƒä¼˜çš„æ¨¡å‹èåˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿç›´æ¥å°†æ•°å­¦æ¨ç†èƒ½åŠ›ä»ç°æˆçš„æ•°å­¦LLMè½¬ç§»åˆ°å¤šæ¨¡æ€LLMä¸­ï¼ŒåŒæ—¶ä¿æŒå¤šæ¨¡æ€å¯¹é½ä¸é€€åŒ–ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†èƒ½åŠ›ä¸Šæ˜¾è‘—è½åäºçº¯æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ï¼Œè€Œç°æœ‰çš„æ¨¡å‹èåˆæ–¹æ³•å¿½è§†äº†MLLMä¸LLMä¹‹é—´çš„å‚æ•°ç©ºé—´å¯¹é½é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**Method:** æå‡ºIP-Mergingæ–¹æ³•ï¼Œé¦–å…ˆè¯†åˆ«MLLMå’Œæ•°å­¦LLMä¸­ä¸æ¨ç†ç›¸å…³çš„å…³é”®å‚æ•°å±‚ï¼Œç„¶åå°†è¿™äº›å‚æ•°æŠ•å½±åˆ°MLLMçš„å­ç©ºé—´ä¸­ä¿æŒå¯¹é½ï¼Œæœ€ååœ¨è¯¥å­ç©ºé—´å†…è¿›è¡Œå‚æ•°èåˆã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼ŒIP-Mergingæ–¹æ³•èƒ½å¤Ÿåœ¨ä¸æŸå®³MLLMå…¶ä»–èƒ½åŠ›çš„å‰æä¸‹ï¼Œç›´æ¥ä»æ•°å­¦LLMä¸­å¢å¼ºMLLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å‚æ•°ç©ºé—´å¯¹é½å’Œèåˆç­–ç•¥ï¼Œå¯ä»¥å®ç°è·¨æ¨¡æ€èƒ½åŠ›çš„æœ‰æ•ˆè¿ç§»ï¼Œä¸ºæå‡MLLMçš„ä¸“é—¨åŒ–èƒ½åŠ›æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.


### [62] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu, Hongjin Lu, Junliang Xing, Changhao Zhang, Yin Zhu, Yuhao Yang, Yuheng Jing, Kai Li, Kun Shao, Jianye Hao, Jun Wang, Yuanchun Shi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Hi-Agentï¼Œä¸€ç§å¯è®­ç»ƒçš„åˆ†å±‚è§†è§‰è¯­è¨€ä»£ç†ï¼Œç”¨äºç§»åŠ¨è®¾å¤‡æ§åˆ¶ï¼Œé€šè¿‡è”åˆä¼˜åŒ–é«˜å±‚æ¨ç†æ¨¡å‹å’Œä½å±‚åŠ¨ä½œæ¨¡å‹ï¼Œåœ¨Android-in-the-WildåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†87.9%çš„æœ€æ–°ä»»åŠ¡æˆåŠŸç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç§»åŠ¨è®¾å¤‡æ§åˆ¶æ–¹æ³•ä¸»è¦ä¾èµ–ç›´æ¥çš„çŠ¶æ€åˆ°åŠ¨ä½œæ˜ å°„ï¼Œç¼ºä¹ç»“æ„åŒ–æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨æ–°ä»»åŠ¡æˆ–æœªè§è¿‡çš„UIå¸ƒå±€ä¸Šæ³›åŒ–æ€§èƒ½è¾ƒå·®ã€‚

**Method:** Hi-Agenté‡‡ç”¨åˆ†å±‚æ¶æ„ï¼ŒåŒ…å«é«˜å±‚æ¨ç†æ¨¡å‹å’Œä½å±‚åŠ¨ä½œæ¨¡å‹ï¼Œé€šè¿‡å°†å¤šæ­¥å†³ç­–é‡æ„ä¸ºå•æ­¥å­ç›®æ ‡åºåˆ—ï¼Œå¹¶æå‡ºäº†å‰ç»ä¼˜åŠ¿å‡½æ•°ï¼Œåˆ©ç”¨ä½å±‚æ¨¡å‹çš„æ‰§è¡Œåé¦ˆæ¥æŒ‡å¯¼é«˜å±‚ä¼˜åŒ–ï¼Œç¼“è§£äº†é•¿æ—¶åŸŸä»»åŠ¡ä¸­çš„è·¯å¾„çˆ†ç‚¸é—®é¢˜ã€‚

**Result:** åœ¨Android-in-the-WildåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°87.9%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºåŸºäºæç¤ºçš„AppAgentï¼ˆ17.7%ï¼‰ã€ç›‘ç£å­¦ä¹ çš„Filtered BCï¼ˆ54.5%ï¼‰å’Œå¼ºåŒ–å­¦ä¹ çš„DigiRLï¼ˆ71.9%ï¼‰ï¼Œåœ¨ScreenSpot-v2åŸºå‡†æµ‹è¯•ä¸Šå±•ç°å‡ºç«äº‰åŠ›çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** åˆ†å±‚è®¾è®¡å’Œè”åˆä¼˜åŒ–ç­–ç•¥æœ‰æ•ˆè§£å†³äº†ç§»åŠ¨è®¾å¤‡æ§åˆ¶ä¸­çš„æ³›åŒ–æŒ‘æˆ˜ï¼Œå‰ç»ä¼˜åŠ¿å‡½æ•°å®ç°äº†æ— è¯„è®ºå®¶çš„ç¨³å®šè®­ç»ƒï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚ç§»åŠ¨æ§åˆ¶åœºæ™¯ä¸­å±•ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚

---

#### ğŸ“„ Abstract
Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.


### [63] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song, Heyuan Huang, Qiqiang Lin, Yin Zhao, Xiangmou Qu, Jun Wang, Xingyu Lou, Weiwen Liu, Zhuosheng Zhang, Jun Wang, Yong Yu, Weinan Zhang, Zhaoxiang Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å›¾ç»“æ„åŸºå‡†æ¡†æ¶ColorBenchï¼Œç”¨äºè¯„ä¼°ç§»åŠ¨æ™ºèƒ½ä½“åœ¨å¤æ‚é•¿è§†é‡ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œé€šè¿‡é™æ€æ¨¡æ‹ŸåŠ¨æ€è¡Œä¸ºæ¥å¼¥åˆç¦»çº¿ä¸åœ¨çº¿è¯„ä¼°ä¹‹é—´çš„å·®è·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç§»åŠ¨æ™ºèƒ½ä½“è¯„ä¼°æ ‡å‡†å­˜åœ¨å±€é™æ€§ï¼Œç¦»çº¿é™æ€åŸºå‡†åªèƒ½éªŒè¯å•ä¸€é¢„å®šä¹‰è·¯å¾„ï¼Œè€Œåœ¨çº¿åŠ¨æ€æµ‹è¯•å—é™äºçœŸå®è®¾å¤‡çš„å¤æ‚æ€§å’Œä¸å¯é‡ç°æ€§ï¼Œä¸¤è€…å‡æ— æ³•å…¨é¢è¯„ä¼°æ™ºèƒ½ä½“åœ¨å…è®¸å¤šç§æœ‰æ•ˆè§£å†³æ–¹æ¡ˆçš„å¤æ‚ç°å®ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚

**Method:** é€šè¿‡å»ºæ¨¡çœŸå®è®¾å¤‡äº¤äº’ä¸­è§‚å¯Ÿåˆ°çš„æœ‰é™çŠ¶æ€ï¼Œå®ç°äº†åŠ¨æ€è¡Œä¸ºçš„é™æ€æ¨¡æ‹Ÿï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¼€å‘äº†ColorBenchåŸºå‡†ï¼Œæ”¯æŒå¤šç§æœ‰æ•ˆè§£å†³æ–¹æ¡ˆè¯„ä¼°ã€å­ä»»åŠ¡å®Œæˆç‡ç»Ÿè®¡å’ŒåŸå­çº§èƒ½åŠ›åˆ†æã€‚

**Result:** ColorBenchåŒ…å«175ä¸ªä»»åŠ¡ï¼ˆ74ä¸ªå•åº”ç”¨ä»»åŠ¡å’Œ101ä¸ªè·¨åº”ç”¨ä»»åŠ¡ï¼‰ï¼Œå¹³å‡é•¿åº¦è¶…è¿‡13æ­¥ï¼Œæ¯ä¸ªä»»åŠ¡è‡³å°‘åŒ…å«ä¸¤æ¡æ­£ç¡®è·¯å¾„å’Œè‹¥å¹²å…¸å‹é”™è¯¯è·¯å¾„ï¼Œé€šè¿‡è¯„ä¼°å„ç§åŸºçº¿æ¨¡å‹å‘ç°äº†ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ã€‚

**Conclusion:** åŸºäºå®éªŒç»“æœæå‡ºäº†æ”¹è¿›æ–¹å‘å’Œå¯è¡Œçš„æŠ€æœ¯è·¯å¾„ï¼Œä»¥å¢å¼ºæ™ºèƒ½ä½“åœ¨å¤æ‚é•¿è§†é‡é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼Œä¸ºç§»åŠ¨æ™ºèƒ½ä½“çš„ç»¼åˆèƒ½åŠ›è¯„ä¼°æä¾›äº†æ–°çš„åŸºå‡†æ¡†æ¶å’Œæ–¹æ³•è®ºã€‚

---

#### ğŸ“„ Abstract
The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.


### [64] [LabOS: The AI-XR Co-Scientist That Sees and Works With Humans](https://arxiv.org/abs/2510.14861)
*Le Cong, Zaixi Zhang, Xiaotong Wang, Yin Di, Ruofan Jin, Michal Gerasimiuk, Yinkai Wang, Ravi K. Dinesh, David Smerkous, Alex Smerkous, Xuekun Wu, Shilong Liu, Peishan Li, Yi Zhu, Simran Serrao, Ning Zhao, Imran A. Mohammad, John B. Sunwoo, Joseph C. Wu, Mengdi Wang*

#### ğŸ§© TL;DR
LabOSæ˜¯é¦–ä¸ªå°†è®¡ç®—æ¨ç†ä¸ç‰©ç†å®éªŒç›¸ç»“åˆçš„äººå·¥æ™ºèƒ½å…±åŒç§‘å­¦å®¶ï¼Œé€šè¿‡å¤šæ¨¡æ€æ„ŸçŸ¥ã€è‡ªè¿›åŒ–ä»£ç†å’Œæ‰©å±•ç°å®ï¼ˆXRï¼‰èµ‹èƒ½çš„äººæœºåä½œï¼Œå°†å®éªŒå®¤è½¬å˜ä¸ºæ™ºèƒ½åä½œç¯å¢ƒã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½åœ¨ç§‘å­¦ç ”ç©¶ä¸­ä»…å±€é™äºè®¡ç®—è®¾è®¡è€Œæ— æ³•å‚ä¸ç‰©ç†å®éªŒçš„é—®é¢˜ï¼Œé€šè¿‡è¿æ¥äººç±»ç§‘å­¦å®¶çš„å®éªŒç¯å¢ƒä¸AIç³»ç»Ÿï¼Œå®ç°ä»è®¡ç®—è®¾è®¡åˆ°å®é™…å‚ä¸çš„è½¬å˜ã€‚

**Method:** LabOSé‡‡ç”¨å¤šæ¨¡å‹AIä»£ç†ç³»ç»Ÿã€æ™ºèƒ½çœ¼é•œå’Œæ‰©å±•ç°å®ï¼ˆXRï¼‰æŠ€æœ¯ï¼Œç»“åˆå¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½¿AIèƒ½å¤Ÿç†è§£å®éªŒç¯å¢ƒå¹¶å®æ—¶ååŠ©ç§‘å­¦å®¶æ‰§è¡Œå®éªŒæ“ä½œã€‚

**Result:** åœ¨ç™Œç—‡å…ç–«æ²»ç–—é¶ç‚¹å‘ç°å’Œå¹²ç»†èƒå·¥ç¨‹ç­‰å¤šä¸ªåº”ç”¨é¢†åŸŸä¸­ï¼ŒLabOSå±•ç¤ºäº†AIèƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿè®¡ç®—è®¾è®¡ï¼Œç›´æ¥å‚ä¸ç‰©ç†å®éªŒè¿‡ç¨‹ï¼Œå®ç°äººæœºåä½œçš„æ™ºèƒ½å®éªŒç¯å¢ƒã€‚

**Conclusion:** è¿™é¡¹ç ”ç©¶è¡¨æ˜äººå·¥æ™ºèƒ½å¯ä»¥æˆä¸ºç§‘å­¦ç ”ç©¶çš„ç§¯æå‚ä¸è€…ï¼Œé€šè¿‡äººæœºåä½œå°†å®éªŒå®¤è½¬å˜ä¸ºæ™ºèƒ½å‘ç°ç¯å¢ƒï¼Œä¸ºæœªæ¥ç§‘å­¦ç ”ç©¶èŒƒå¼å¸¦æ¥é©å‘½æ€§å˜é©ã€‚

---

#### ğŸ“„ Abstract
Modern science advances fastest when thought meets action. LabOS represents
the first AI co-scientist that unites computational reasoning with physical
experimentation through multimodal perception, self-evolving agents, and
Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model
AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see
what scientists see, understand experimental context, and assist in real-time
execution. Across applications--from cancer immunotherapy target discovery to
stem-cell engineering -- LabOS shows that AI can move beyond computational
design to participation, turning the laboratory into an intelligent,
collaborative environment where human and machine discovery evolve together.


### [65] [TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922)
*Annisaa Fitri Nurfidausi, Eleonora Mancini, Paolo Torroni*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ¢ç´¢EEGã€è¯­éŸ³å’Œæ–‡æœ¬çš„å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºä¸å»ºæ¨¡ç­–ç•¥ï¼Œå»ºç«‹äº†æŠ‘éƒç—‡æ£€æµ‹çš„ç¨³å¥åŸºå‡†æ¡†æ¶ï¼Œè¯æ˜ä¸‰æ¨¡æ€ç»„åˆä¸é¢„è®­ç»ƒåµŒå…¥èƒ½æ˜¾è‘—æå‡æ£€æµ‹æ€§èƒ½å¹¶è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æŠ‘éƒç—‡è‡ªåŠ¨æ£€æµ‹ç ”ç©¶å­˜åœ¨èŒƒå›´æœ‰é™ã€ç¼ºä¹ç‰¹å¾ç³»ç»Ÿæ€§æ¯”è¾ƒä»¥åŠè¯„ä¼°åè®®ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¤šæ¨¡æ€æ–¹æ³•è™½ç„¶æ˜¾ç¤ºå‡ºæ½œåŠ›ä½†å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚

**Method:** ç³»ç»Ÿè¯„ä¼°äº†æ‰‹å·¥ç‰¹å¾ä¸é¢„è®­ç»ƒåµŒå…¥çš„æœ‰æ•ˆæ€§ï¼Œæ¯”è¾ƒäº†ä¸åŒç¥ç»ç¼–ç å™¨çš„æ€§èƒ½ï¼Œåˆ†æäº†å•æ¨¡æ€ã€åŒæ¨¡æ€å’Œä¸‰æ¨¡æ€é…ç½®ï¼Œå¹¶ç‰¹åˆ«å…³æ³¨äº†EEGåœ¨å¤šæ¨¡æ€èåˆä¸­çš„è§’è‰²ä½œç”¨ï¼Œé‡‡ç”¨ä¸€è‡´çš„å—è¯•è€…ç‹¬ç«‹åˆ†å‰²ç¡®ä¿å¯å¤ç°æ€§ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜EEGã€è¯­éŸ³å’Œæ–‡æœ¬ä¸‰æ¨¡æ€ç»„åˆèƒ½æ˜¾è‘—å¢å¼ºå¤šæ¨¡æ€æ£€æµ‹æ€§èƒ½ï¼Œé¢„è®­ç»ƒåµŒå…¥å…¨é¢ä¼˜äºæ‰‹å·¥è®¾è®¡ç‰¹å¾ï¼Œç²¾å¿ƒè®¾è®¡çš„ä¸‰æ¨¡æ€æ¨¡å‹å®ç°äº†æœ€å…ˆè¿›çš„æ£€æµ‹æ€§èƒ½ã€‚

**Conclusion:** æœ¬ç ”ç©¶ä¸ºå¤šæ¨¡æ€æŠ‘éƒç—‡æ£€æµ‹çš„æœªæ¥ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ï¼Œè¯æ˜äº†ç³»ç»Ÿç‰¹å¾æ¢ç´¢å’Œç¨³å¥è¯„ä¼°æ¡†æ¶çš„é‡è¦æ€§ï¼Œä¸ºä¸´åºŠåº”ç”¨ä¸­æ›´å¯é çš„æŠ‘éƒç—‡ç­›æŸ¥å·¥å…·å¼€å‘æä¾›äº†æ–¹æ³•è®ºæ”¯æŒã€‚

---

#### ğŸ“„ Abstract
Depression is a widespread mental health disorder, yet its automatic
detection remains challenging. Prior work has explored unimodal and multimodal
approaches, with multimodal systems showing promise by leveraging complementary
signals. However, existing studies are limited in scope, lack systematic
comparisons of features, and suffer from inconsistent evaluation protocols. We
address these gaps by systematically exploring feature representations and
modelling strategies across EEG, together with speech and text. We evaluate
handcrafted features versus pre-trained embeddings, assess the effectiveness of
different neural encoders, compare unimodal, bimodal, and trimodal
configurations, and analyse fusion strategies with attention to the role of
EEG. Consistent subject-independent splits are applied to ensure robust,
reproducible benchmarking. Our results show that (i) the combination of EEG,
speech and text modalities enhances multimodal detection, (ii) pretrained
embeddings outperform handcrafted features, and (iii) carefully designed
trimodal models achieve state-of-the-art performance. Our work lays the
groundwork for future research in multimodal depression detection.


### [66] [Towards Unified Multimodal Misinformation Detection in Social Media: A Benchmark Dataset and Baseline](https://arxiv.org/abs/2509.25991)
*Haiyang Li, Yaxiong Wang, Shengeng Tang, Lianwei Wu, Lechao Cheng, Zhun Zhong*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ç»Ÿä¸€å¤šæ¨¡æ€è™šå‡å†…å®¹æ£€æµ‹æ¡†æ¶UMFDetï¼Œé€šè¿‡æ„å»ºåŒ…å«äººç±»åˆ¶ä½œå’ŒAIç”Ÿæˆè™šå‡å†…å®¹çš„ç»¼åˆæ•°æ®é›†OmniFakeï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä»…é’ˆå¯¹å•ä¸€ç±»å‹è™šå‡å†…å®¹çš„å±€é™æ€§ï¼Œå®ç°äº†å¯¹æœªçŸ¥ç±»å‹å¤šæ¨¡æ€è™šå‡å†…å®¹çš„é²æ£’æ£€æµ‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€è™šå‡å†…å®¹æ£€æµ‹ç ”ç©¶å­˜åœ¨é¢†åŸŸéš”ç¦»é—®é¢˜ï¼ŒNLPé¢†åŸŸä¸“æ³¨äºäººç±»åˆ¶ä½œçš„è™šå‡ä¿¡æ¯ï¼Œè€Œè®¡ç®—æœºè§†è§‰é¢†åŸŸä¸»è¦é’ˆå¯¹AIç”Ÿæˆå†…å®¹ï¼Œå¯¼è‡´ç°æœ‰æ¨¡å‹é€šå¸¸ä»…èƒ½å¤„ç†å•ä¸€ç±»å‹çš„è™šå‡å†…å®¹ã€‚åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œå¤šæ¨¡æ€å¸–å­çš„ç±»å‹é€šå¸¸æ˜¯æœªçŸ¥çš„ï¼Œè¿™ç§ä¸“ä¸šåŒ–ç³»ç»Ÿçš„æœ‰æ•ˆæ€§å—åˆ°é™åˆ¶ã€‚

**Method:** æå‡ºäº†ç»Ÿä¸€å¤šæ¨¡æ€è™šå‡å†…å®¹æ£€æµ‹æ¡†æ¶UMFDetï¼Œè¯¥æ¡†æ¶é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œå¹¶å¢å¼ºä»¥ç±»åˆ«æ„ŸçŸ¥çš„æ··åˆä¸“å®¶é€‚é…å™¨æ¥æ•æ‰ç±»åˆ«ç‰¹å®šçš„çº¿ç´¢ï¼ŒåŒæ—¶å¼•å…¥å½’å› æ€ç»´é“¾æœºåˆ¶ä¸ºå®šä½æ˜¾è‘—æ¬ºéª—ä¿¡å·æä¾›éšå¼æ¨ç†æŒ‡å¯¼ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUMFDetåœ¨ä¸¤ç§è™šå‡ä¿¡æ¯ç±»å‹ä¸Šéƒ½å®ç°äº†é²æ£’ä¸”ä¸€è‡´çš„æ€§èƒ½è¡¨ç°ï¼Œè¶…è¶Šäº†ä¸“ä¸šåŒ–çš„åŸºçº¿æ–¹æ³•ï¼Œä¸ºçœŸå®ä¸–ç•Œçš„å¤šæ¨¡æ€æ¬ºéª—æ£€æµ‹æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç»Ÿä¸€æ¡†æ¶åœ¨å¤„ç†ä¸åŒç±»å‹å¤šæ¨¡æ€è™šå‡å†…å®¹æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡æ„å»ºç»¼åˆæ•°æ®é›†å’Œå¼•å…¥ç±»åˆ«æ„ŸçŸ¥æœºåˆ¶ï¼Œä¸ºå¤šæ¨¡æ€è™šå‡å†…å®¹æ£€æµ‹æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œå¼ºè°ƒäº†åœ¨çœŸå®åœºæ™¯ä¸­å¤„ç†æœªçŸ¥ç±»å‹è™šå‡å†…å®¹çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
In recent years, detecting fake multimodal content on social media has drawn
increasing attention. Two major forms of deception dominate: human-crafted
misinformation (e.g., rumors and misleading posts) and AI-generated content
produced by image synthesis models or vision-language models (VLMs). Although
both share deceptive intent, they are typically studied in isolation. NLP
research focuses on human-written misinformation, while the CV community
targets AI-generated artifacts. As a result, existing models are often
specialized for only one type of fake content. In real-world scenarios,
however, the type of a multimodal post is usually unknown, limiting the
effectiveness of such specialized systems. To bridge this gap, we construct the
Omnibus Dataset for Multimodal News Deception (OmniFake), a comprehensive
benchmark of 127K samples that integrates human-curated misinformation from
existing resources with newly synthesized AI-generated examples. Based on this
dataset, we propose Unified Multimodal Fake Content Detection (UMFDet), a
framework designed to handle both forms of deception. UMFDet leverages a VLM
backbone augmented with a Category-aware Mixture-of-Experts (MoE) Adapter to
capture category-specific cues, and an attribution chain-of-thought mechanism
that provides implicit reasoning guidance for locating salient deceptive
signals. Extensive experiments demonstrate that UMFDet achieves robust and
consistent performance across both misinformation types, outperforming
specialized baselines and offering a practical solution for real-world
multimodal deception detection.
