<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-11-14.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 47]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 8]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 7]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-mmada-parallel-multimodal-large-diffusion-language-models-for-thinking-aware-editing-and-generation">[1] <a href="https://arxiv.org/abs/2511.09611">MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</a></h3>
<p><em>Ye Tian, Ling Yang, Jiongfan Yang, Anran Wang, Yu Tian, Jiani Zheng, Haochen Wang, Zhiyang Teng, Zhuochen Wang, Yinjie Wang, Yunhai Tong, Mengdi Wang, Xiangtai Li</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMMaDA-Parallelå¹¶è¡Œå¤šæ¨¡æ€æ‰©æ•£æ¡†æ¶ï¼Œé€šè¿‡å¹¶è¡Œå¼ºåŒ–å­¦ä¹ è§£å†³æ€ç»´æ„ŸçŸ¥ç”Ÿæˆä¸­çš„é”™è¯¯ä¼ æ’­é—®é¢˜ï¼Œåœ¨ParaBenchåŸºå‡†ä¸Šç›¸æ¯”æœ€å…ˆè¿›æ¨¡å‹Bagelå®ç°äº†6.9%çš„è¾“å‡ºå¯¹é½æ”¹è¿›ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é¡ºåºè‡ªå›å½’æ–¹æ³•åœ¨æ€ç»´æ„ŸçŸ¥ç”Ÿæˆä¸­å­˜åœ¨é”™è¯¯ä¼ æ’­é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆæ¨ç†ä¸æœ€ç»ˆå›¾åƒä¹‹é—´çš„å¯¹é½ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> æå‡ºå¹¶è¡Œå¤šæ¨¡æ€æ‰©æ•£æ¡†æ¶MMaDA-Parallelï¼Œæ”¯æŒæ–‡æœ¬å’Œå›¾åƒåœ¨æ•´ä¸ªå»å™ªè½¨è¿¹ä¸­çš„è¿ç»­åŒå‘äº¤äº’ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒè®­ç»ƒï¼Œå¹¶é‡‡ç”¨å¹¶è¡Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨è½¨è¿¹ä¸Šåº”ç”¨è¯­ä¹‰å¥–åŠ±æ¥å¢å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒéªŒè¯æ¨¡å‹æ˜¾è‘—æ”¹å–„äº†è·¨æ¨¡æ€å¯¹é½å’Œè¯­ä¹‰ä¸€è‡´æ€§ï¼Œåœ¨ParaBenchåŸºå‡†ä¸Šç›¸æ¯”æœ€å…ˆè¿›æ¨¡å‹Bagelå®ç°äº†6.9%çš„è¾“å‡ºå¯¹é½æ”¹è¿›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºæ€ç»´æ„ŸçŸ¥å›¾åƒåˆæˆå»ºç«‹äº†æ›´é²æ£’çš„èŒƒå¼ï¼Œé€šè¿‡å¹¶è¡Œäº¤äº’æœºåˆ¶æœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿè‡ªå›å½’æ–¹æ³•çš„é”™è¯¯ä¼ æ’­é—®é¢˜ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel</p>
<h3 id="2-slideredit-continuous-image-editing-with-fine-grained-instruction-control">[2] <a href="https://arxiv.org/abs/2511.09715">SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control</a></h3>
<p><em>Arman Zarei, Samyadeep Basu, Mobina Pournemat, Sayan Nag, Ryan Rossi, Soheil Feizi</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SliderEditæ¡†æ¶ï¼Œä¸ºåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ¨¡å‹æä¾›è¿ç»­ã€ç»†ç²’åº¦çš„æŒ‡ä»¤æ§åˆ¶ï¼Œé€šè¿‡è§£è€¦å¤šæŒ‡ä»¤æç¤ºä¸­çš„å„ä¸ªç¼–è¾‘æ“ä½œå¹¶å°†å…¶æš´éœ²ä¸ºå¯è°ƒèŠ‚çš„æ»‘å—ï¼Œå®ç°å¯¹å•ä¸ªç¼–è¾‘å¼ºåº¦çš„å¹³æ»‘è°ƒæ•´ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨å¤„ç†å¤šæŒ‡ä»¤æç¤ºæ—¶ï¼Œå¯¹æ¯ä¸ªæŒ‡ä»¤åº”ç”¨å›ºå®šçš„ç¼–è¾‘å¼ºåº¦ï¼Œé™åˆ¶äº†ç”¨æˆ·å¯¹å•ä¸ªç¼–è¾‘æ“ä½œå¼ºåº¦çš„ç²¾ç¡®å’Œè¿ç»­æ§åˆ¶èƒ½åŠ›ï¼Œæ— æ³•å®ç°ç»†ç²’åº¦çš„ç¼–è¾‘è°ƒèŠ‚ã€‚</p>
<p><strong>Method:</strong> SliderEditæ¡†æ¶é€šè¿‡è§£è€¦å¤šéƒ¨åˆ†ç¼–è¾‘æŒ‡ä»¤ä¸­çš„å„ä¸ªæŒ‡ä»¤ï¼Œå°†æ¯ä¸ªæŒ‡ä»¤æš´éœ²ä¸ºå…¨å±€è®­ç»ƒçš„æ»‘å—ï¼Œä½¿ç”¨å•ä¸€ä½ç§©é€‚åº”çŸ©é˜µé›†åˆæ¥æ³›åŒ–å¤„ç†å¤šæ ·åŒ–çš„ç¼–è¾‘ã€å±æ€§å’Œç»„åˆæŒ‡ä»¤ï¼Œå®ç°æ²¿å•ä¸ªç¼–è¾‘ç»´åº¦çš„è¿ç»­æ’å€¼åŒæ—¶ä¿æŒç©ºé—´å±€éƒ¨æ€§å’Œå…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> å°†SliderEditåº”ç”¨äºFLUX-Kontextå’ŒQwen-Image-Editç­‰æœ€å…ˆè¿›çš„å›¾åƒç¼–è¾‘æ¨¡å‹åï¼Œè§‚å¯Ÿåˆ°åœ¨ç¼–è¾‘å¯æ§æ€§ã€è§†è§‰ä¸€è‡´æ€§å’Œç”¨æˆ·å¯å¼•å¯¼æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æ¡†æ¶åœ¨è¿ç»­æŒ‡ä»¤æ§åˆ¶æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºäº¤äº’å¼ã€æŒ‡ä»¤é©±åŠ¨çš„å›¾åƒæ“ä½œæä¾›äº†è¿ç»­å’Œç»„åˆæ§åˆ¶çš„æ–°é€”å¾„ï¼Œé¦–æ¬¡æ¢ç´¢å¹¶æå‡ºäº†åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ¨¡å‹ä¸­è¿ç»­ç»†ç²’åº¦æŒ‡ä»¤æ§åˆ¶çš„æ¡†æ¶ï¼Œæ¨åŠ¨äº†å›¾åƒç¼–è¾‘å·¥å…·å‘æ›´ç²¾ç¡®å’Œç”¨æˆ·å‹å¥½çš„æ–¹å‘å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.</p>
<h3 id="3-storm-segment-track-and-object-re-localization-from-a-single-3d-model">[3] <a href="https://arxiv.org/abs/2511.09771">STORM: Segment, Track, and Object Re-Localization from a Single 3D Model</a></h3>
<p><em>Yu Deng, Teng Cao, Hikaru Shindo, Jiahong Xue, Quentin Delfosse, Kristian Kersting</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSTORMç³»ç»Ÿï¼Œä¸€ç§æ— éœ€äººå·¥æ ‡æ³¨çš„å®æ—¶6Då§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè§†è§‰è¯­è¨€ç†è§£ä¸è‡ªç›‘ç£ç‰¹å¾åŒ¹é…çš„ä¸‰é˜¶æ®µæµç¨‹ï¼Œåœ¨å·¥ä¸šæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç²¾åº¦å’Œå®æ—¶æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰6Då§¿æ€ä¼°è®¡æ–¹æ³•é€šå¸¸ä¾èµ–ç¬¬ä¸€å¸§ä¸­ç›®æ ‡çš„æ‰‹åŠ¨æ ‡æ³¨åˆ†å‰²æ©ç ï¼Œè¿™æ—¢è€—æ—¶åˆä¼šåœ¨é¢å¯¹é®æŒ¡æˆ–å¿«é€Ÿè¿åŠ¨æ—¶å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå› æ­¤éœ€è¦å¼€å‘æ— éœ€äººå·¥æ ‡æ³¨çš„é²æ£’å®æ—¶ç³»ç»Ÿã€‚</p>
<p><strong>Method:</strong> STORMé‡‡ç”¨æ–°é¢–çš„ä¸‰é˜¶æ®µæµç¨‹ï¼šä¸Šä¸‹æ–‡å¯¹è±¡æè¿°æŒ‡å¯¼å®šä½ï¼Œè‡ªäº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¯†åˆ«å€™é€‰åŒºåŸŸï¼Œåˆ†å‰²æ¨¡å‹ç”Ÿæˆç²¾ç¡®æ©ç ç”¨äºå§¿æ€ä¼°è®¡ï¼Œå¹¶å¼•å…¥è‡ªåŠ¨é‡æ–°æ³¨å†Œæœºåˆ¶é€šè¿‡ç‰¹å¾ç›¸ä¼¼æ€§ç›‘æ§æ£€æµ‹è·Ÿè¸ªå¤±è´¥å¹¶ä»ä¸­æ¢å¤ã€‚</p>
<p><strong>Result:</strong> STORMåœ¨å…·æœ‰å¤šç›®æ ‡é®æŒ¡ã€é«˜é€Ÿè¿åŠ¨å’Œå˜åŒ–å…‰ç…§çš„æŒ‘æˆ˜æ€§å·¥ä¸šæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç²¾åº¦ï¼ŒåŒæ—¶ä¿æŒå®æ—¶è¿è¡Œé€Ÿåº¦ä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚</p>
<p><strong>Conclusion:</strong> è¿™ç§æ— éœ€æ ‡æ³¨çš„æ–¹æ³•æ˜¾è‘—é™ä½äº†éƒ¨ç½²å¼€é”€ï¼Œä¸ºæŸ”æ€§åˆ¶é€ å’Œæ™ºèƒ½è´¨é‡æ§åˆ¶ç­‰ç°ä»£åº”ç”¨æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†è§†è§‰è¯­è¨€ç†è§£ä¸è‡ªç›‘ç£å­¦ä¹ çš„æœ‰æ•ˆç»“åˆã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Accurate 6D pose estimation and tracking are fundamental capabilities for physical AI systems such as robots. However, existing approaches typically rely on a manually annotated segmentation mask of the target in the first frame, which is labor-intensive and leads to reduced performance when faced with occlusions or rapid movement. To address these limi- tations, we propose STORM (Segment, Track, and Object Re-localization from a single 3D Model), an open-source robust real-time 6D pose estimation system that requires no manual annotation. STORM employs a novel three-stage pipeline combining vision-language understanding with self-supervised feature matching: contextual object descriptions guide localization, self-cross-attention mechanisms identify candidate regions, and a segmentation model produces precise masks for accurate pose estimation. Another key innovation is our automatic re-registration mechanism that detects tracking failures through feature similarity monitoring and recovers from severe occlusions or rapid motion. STORM achieves state-of-the-art accuracy on challenging industrial datasets featuring multi-object occlusions, high-speed motion, and varying illumination, while operating at real-time speeds without additional training. This annotation-free approach significantly reduces deployment overhead, providing a practical solution for modern applications, such as flexible manufacturing and intelligent quality control.</p>
<h3 id="4-panda-patch-and-distribution-aware-augmentation-for-long-tailed-exemplar-free-continual-learning">[4] <a href="https://arxiv.org/abs/2511.09791">PANDA - Patch And Distribution-Aware Augmentation for Long-Tailed Exemplar-Free Continual Learning</a></h3>
<p><em>Siddeshwar Raghavan, Jiangpeng He, Fengqing Zhu</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºPANDAæ¡†æ¶ï¼Œä¸€ç§é¢å‘æ— ç¤ºä¾‹æŒç»­å­¦ä¹ çš„è¡¥ä¸ä¸åˆ†å¸ƒæ„ŸçŸ¥å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡CLIPç¼–ç å™¨è¯†åˆ«ä»£è¡¨æ€§åŒºåŸŸå¹¶è¿›è¡Œç±»é—´ç§»æ¤ï¼Œç»“åˆè‡ªé€‚åº”å¹³è¡¡ç­–ç•¥è§£å†³ç°å®æ•°æ®æµä¸­çš„åŒé‡ä¸å¹³è¡¡é—®é¢˜ï¼Œæ˜¾è‘—æå‡ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹åœ¨æŒç»­å­¦ä¹ ä¸­çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ— ç¤ºä¾‹æŒç»­å­¦ä¹ é¢ä¸´ç¾éš¾æ€§é—å¿˜çš„ä¸¥é‡æŒ‘æˆ˜ï¼Œç°æœ‰åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•å¾€å¾€å¿½è§†ç°å®ä¸–ç•Œæ•°æ®åˆ†å¸ƒçš„å†…åœ¨ä¸å¹³è¡¡æ€§ã€‚ç ”ç©¶å‘ç°ç°å®æ•°æ®æµæ™®éå­˜åœ¨åŒé‡ä¸å¹³è¡¡ï¼šæ•°æ®é›†çº§åˆ†å¸ƒä¸å•ä¸ªä»»åŠ¡å†…çš„æç«¯æˆ–åå‘åæ–œç›¸ç»“åˆï¼Œå½¢æˆä»»åŠ¡å†…å’Œä»»åŠ¡é—´çš„ä¸å¹³è¡¡ï¼Œé˜»ç¢æœ‰æ•ˆå­¦ä¹ å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> PANDAæ¡†æ¶é‡‡ç”¨CLIPç¼–ç å™¨è¯†åˆ«ä½é¢‘ç‡ç±»åˆ«çš„ä»£è¡¨æ€§åŒºåŸŸï¼Œå¹¶å°†å…¶ç§»æ¤åˆ°é«˜é¢‘ç±»åˆ«æ ·æœ¬ä¸­ï¼Œå®ç°ç±»åˆ«é—´çŸ¥è¯†å¢å¼ºã€‚åŒæ—¶å¼•å…¥è‡ªé€‚åº”å¹³è¡¡ç­–ç•¥ï¼Œåˆ©ç”¨å…ˆå‰ä»»åŠ¡åˆ†å¸ƒå¹³æ»‘ä»»åŠ¡é—´ä¸å¹³è¡¡ï¼Œç¼©å°ä»»åŠ¡é—´å¹³å‡æ ·æœ¬å·®è·ï¼Œä½¿å†»ç»“é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ›´å…¬å¹³çš„å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒPANDAèƒ½å¤Ÿä¸ç°æœ‰åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„æŒç»­å­¦ä¹ æ–¹æ³•æ— ç¼é›†æˆï¼Œæ˜¾è‘—æé«˜åˆ†ç±»å‡†ç¡®ç‡å¹¶æœ‰æ•ˆå‡å°‘ç¾éš¾æ€§é—å¿˜ç°è±¡ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†å…¶å¯¹åŒé‡ä¸å¹³è¡¡é—®é¢˜çš„æœ‰æ•ˆè§£å†³èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> PANDAæ¡†æ¶æ­ç¤ºäº†å¤„ç†ç°å®ä¸–ç•Œæ•°æ®æµä¸­åŒé‡ä¸å¹³è¡¡çš„é‡è¦æ€§ï¼Œä¸ºé¢„è®­ç»ƒæ¨¡å‹åœ¨æŒç»­å­¦ä¹ ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„å¢å¼ºç­–ç•¥ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åˆ†å¸ƒæ„ŸçŸ¥å¢å¼ºåœ¨ç¼“è§£ç¾éš¾æ€§é—å¿˜ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºæœªæ¥æŒç»­å­¦ä¹ æ–¹æ³•è®¾è®¡æä¾›äº†é‡è¦å¯ç¤ºã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Exemplar-Free Continual Learning (EFCL) restricts the storage of previous task data and is highly susceptible to catastrophic forgetting. While pre-trained models (PTMs) are increasingly leveraged for EFCL, existing methods often overlook the inherent imbalance of real-world data distributions. We discovered that real-world data streams commonly exhibit dual-level imbalances, dataset-level distributions combined with extreme or reversed skews within individual tasks, creating both intra-task and inter-task disparities that hinder effective learning and generalization. To address these challenges, we propose PANDA, a Patch-and-Distribution-Aware Augmentation framework that integrates seamlessly with existing PTM-based EFCL methods. PANDA amplifies low-frequency classes by using a CLIP encoder to identify representative regions and transplanting those into frequent-class samples within each task. Furthermore, PANDA incorporates an adaptive balancing strategy that leverages prior task distributions to smooth inter-task imbalances, reducing the overall gap between average samples across tasks and enabling fairer learning with frozen PTMs. Extensive experiments and ablation studies demonstrate PANDA's capability to work with existing PTM-based CL methods, improving accuracy and reducing catastrophic forgetting.</p>
<h3 id="5-test-time-spectrum-aware-latent-steering-for-zero-shot-generalization-in-vision-language-models">[5] <a href="https://arxiv.org/abs/2511.09809">Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models</a></h3>
<p><em>Konstantinos M. Dafnis, Dimitris N. Metaxas</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æµ‹è¯•æ—¶é€‚åº”æ¡†æ¶STSï¼Œé€šè¿‡æå–æ–‡æœ¬åµŒå…¥çš„è°±å­ç©ºé—´æ¥å®šä¹‰ä¸»è¦è¯­ä¹‰æ–¹å‘ï¼Œå¹¶ä»¥é¢‘è°±æ„ŸçŸ¥çš„æ–¹å¼è°ƒæ•´æ½œåœ¨è¡¨ç¤ºï¼Œæ— éœ€åå‘ä¼ æ’­æˆ–ä¿®æ”¹å†»ç»“ç¼–ç å™¨ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æµ‹è¯•æ—¶åŸŸåç§»ä¸‹æ€§èƒ½ä¼šä¸‹é™ã€‚è™½ç„¶æµ‹è¯•æ—¶é€‚åº”ç­–ç•¥å·²ç»å‡ºç°ï¼Œä½†ç°æœ‰æ–¹æ³•å¦‚æµ‹è¯•æ—¶æç¤ºè°ƒä¼˜é€šå¸¸éœ€è¦åå‘ä¼ æ’­å¤§å‹ç¼–ç å™¨æƒé‡æˆ–æ”¹å˜æ ¸å¿ƒæ¨¡å‹ç»„ä»¶ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§å’Œå®ç°å¤æ‚ã€‚</p>
<p><strong>Method:</strong> STSæ¡†æ¶ä»æ–‡æœ¬åµŒå…¥ä¸­æå–è°±å­ç©ºé—´æ¥å®šä¹‰ä¸»è¦è¯­ä¹‰æ–¹å‘ï¼Œé€šè¿‡é€‚åº”å°‘é‡æ¯ä¸ªæ ·æœ¬çš„åç§»å‚æ•°ä»¥æœ€å°åŒ–å¢å¼ºè§†å›¾é—´çš„ç†µï¼Œä»¥é¢‘è°±æ„ŸçŸ¥çš„æ–¹å¼è°ƒæ•´æ½œåœ¨è¡¨ç¤ºã€‚è¯¥æ–¹æ³•å®Œå…¨åœ¨æ¨ç†é˜¶æ®µçš„æ½œåœ¨ç©ºé—´ä¸­æ“ä½œï¼Œæ— éœ€é€šè¿‡å†»ç»“ç¼–ç å™¨è¿›è¡Œåå‘ä¼ æ’­æˆ–ä¿®æ”¹ã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†è¯„ä¼°åè®®ä¸‹ï¼ŒSTSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¤§å¹…è¶…è¶Šæˆ–ä¸æœ€å…ˆè¿›çš„æµ‹è¯•æ—¶é€‚åº”æ–¹æ³•ç›¸åª²ç¾ï¼ŒåŒæ—¶ä»…å¼•å…¥å°‘é‡é¢å¤–å‚æ•°ï¼Œæ¨ç†é€Ÿåº¦æ¯”ä¼ ç»Ÿæµ‹è¯•æ—¶æç¤ºè°ƒä¼˜å¿«8å€ï¼Œå†…å­˜å ç”¨å‡å°‘12å€ã€‚</p>
<p><strong>Conclusion:</strong> STSè¯æ˜äº†åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè½»é‡çº§é€‚åº”çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæµ‹è¯•æ—¶åŸŸé€‚åº”æä¾›äº†é«˜æ•ˆå®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†æ— éœ€ä¿®æ”¹æ ¸å¿ƒæ¨¡å‹ç»„ä»¶å³å¯å®ç°æ˜¾è‘—æ€§èƒ½æå‡çš„æ½œåŠ›ï¼Œä¸ºå®é™…éƒ¨ç½²ä¸­çš„èµ„æºå—é™åœºæ™¯æä¾›äº†é‡è¦å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.</p>
<h3 id="6-from-street-to-orbit-training-free-cross-view-retrieval-via-location-semantics-and-llm-guidance">[6] <a href="https://arxiv.org/abs/2511.09820">From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance</a></h3>
<p><em>Jeongho Min, Dongyoung Kim, Jaehyup Lee</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„è·¨è§†è§’å›¾åƒæ£€ç´¢æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆé¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†è¡—æ™¯åˆ°å«æ˜Ÿå›¾åƒçš„é›¶æ ·æœ¬åŒ¹é…ï¼Œå¹¶åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰å­¦ä¹ æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„è·¨è§†è§’å›¾åƒæ£€ç´¢æ–¹æ³•é€šå¸¸éœ€è¦ç›‘ç£è®­ç»ƒå’Œç‰¹å®šæ•°æ®é›†ï¼Œä¸”ä¾èµ–å…¨æ™¯æˆ–æ— äººæœºå›¾åƒï¼Œè¿™é™åˆ¶äº†å®é™…éƒ¨ç½²ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›é™åˆ¶ï¼Œå¼€å‘ä¸€ç§æ— éœ€è®­ç»ƒã€ä»…ä½¿ç”¨å•ç›®è¡—æ™¯å›¾åƒå³å¯å®ç°è¡—æ™¯åˆ°å«æ˜ŸåŒ¹é…çš„é€šç”¨æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ï¼ˆå¦‚DINOv2ï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡åŸºäºç½‘ç»œçš„å›¾åƒæœç´¢å’ŒLLMä½ç½®æ¨æ–­æå–åœ°ç†çº¿ç´¢ï¼Œåˆ©ç”¨åœ°ç†ç¼–ç APIç”Ÿæˆå«æ˜ŸæŸ¥è¯¢ï¼Œå¹¶ä½¿ç”¨PCAç™½åŒ–ç‰¹å¾ç²¾åŒ–è¿›è¡ŒåŒ¹é…æ£€ç´¢ï¼Œæ•´ä¸ªè¿‡ç¨‹æ— éœ€é¢å¤–è®­ç»ƒã€‚</p>
<p><strong>Result:</strong> åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†å…ˆå‰çš„å­¦ä¹ æ–¹æ³•ï¼Œæ— éœ€åœ°é¢çœŸå€¼ç›‘ç£æˆ–å¾®è°ƒã€‚æ­¤å¤–ï¼Œè¯¥æµç¨‹èƒ½å¤Ÿè‡ªåŠ¨æ„å»ºè¯­ä¹‰å¯¹é½çš„è¡—æ™¯åˆ°å«æ˜Ÿæ•°æ®é›†ï¼Œä¸ºæ‰‹åŠ¨æ ‡æ³¨æä¾›äº†å¯æ‰©å±•ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é¢„è®­ç»ƒæ¨¡å‹ä¸LLMç»“åˆåœ¨è·¨è§†è§’æ£€ç´¢ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºé›¶æ ·æœ¬åœ°ç†å®šä½æä¾›äº†æ–°èŒƒå¼ï¼ŒåŒæ—¶å±•ç¤ºäº†è‡ªåŠ¨æ•°æ®é›†æ„å»ºçš„æ½œåŠ›ï¼Œä¸ºå¤§è§„æ¨¡åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments. However, existing approaches often require supervised training on curated datasets and rely on panoramic or UAV-based images, which limits real-world deployment. In this paper, we present a simple yet effective cross-view image retrieval framework that leverages a pretrained vision encoder and a large language model (LLM), requiring no additional training. Given a monocular street-view image, our method extracts geographic cues through web-based image search and LLM-based location inference, generates a satellite query via geocoding API, and retrieves matching tiles using a pretrained vision encoder (e.g., DINOv2) with PCA-based whitening feature refinement. Despite using no ground-truth supervision or finetuning, our proposed method outperforms prior learning-based approaches on the benchmark dataset under zero-shot settings. Moreover, our pipeline enables automatic construction of semantically aligned street-to-satellite datasets, which is offering a scalable and cost-efficient alternative to manual annotation. All source codes will be made publicly available at https://jeonghomin.github.io/street2orbit.github.io/.</p>
<h3 id="7-remember-me-bridging-the-long-range-gap-in-lvlms-with-three-step-inference-only-decay-resilience-strategies">[7] <a href="https://arxiv.org/abs/2511.09868">Remember Me: Bridging the Long-Range Gap in LVLMs with Three-Step Inference-Only Decay Resilience Strategies</a></h3>
<p><em>Peng Gao, Yujian Lee, Xiaofeng Zhang, Zailong Chen, Hui Zhang</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†æ¨ç†é˜¶æ®µçš„ä¸‰æ­¥è¡°å‡æ¢å¤ç­–ç•¥ï¼ˆT-DRSï¼‰ï¼Œé€šè¿‡è¯­ä¹‰é©±åŠ¨ã€è·ç¦»æ„ŸçŸ¥æ§åˆ¶å’Œè¿œç¨‹ä¾èµ–å†å¼ºåŒ–ä¸‰ä¸ªæ­¥éª¤ï¼Œæœ‰æ•ˆç¼“è§£å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æ—‹è½¬ä½ç½®ç¼–ç å¯¼è‡´çš„è¿œç¨‹æ³¨æ„åŠ›è¡°å‡é—®é¢˜ï¼Œåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨æ—‹è½¬ä½ç½®ç¼–ç æ—¶é¢ä¸´è¿œç¨‹ä¾èµ–å»ºæ¨¡çš„å…³é”®æŒ‘æˆ˜ï¼Œè™½ç„¶æ—‹è½¬ä½ç½®ç¼–ç èƒ½å¤Ÿç²¾ç¡®å»ºæ¨¡æ ‡è®°ä½ç½®ï¼Œä½†éšç€æ ‡è®°è·ç¦»å¢åŠ ä¼šå¯¼è‡´æ¸è¿›æ€§æ³¨æ„åŠ›è¡°å‡ï¼Œç‰¹åˆ«æ˜¯å¯¹è¿œç¨‹æ ‡è®°å¯¹çš„æ³¨æ„åŠ›é€æ¸å‡å¼±ï¼Œä¸¥é‡æŸå®³æ¨¡å‹è®°å¿†å…¨å±€ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†æ¨ç†é˜¶æ®µçš„ä¸‰æ­¥è¡°å‡æ¢å¤ç­–ç•¥ï¼ˆT-DRSï¼‰ï¼ŒåŒ…æ‹¬è¯­ä¹‰é©±åŠ¨è¡°å‡æ¢å¤ç­–ç•¥ï¼ˆSD-DRSï¼‰é€šè¿‡å†…å®¹æ„ŸçŸ¥æ®‹å·®æ”¾å¤§è¯­ä¹‰é‡è¦ä½†è·ç¦»è¾ƒè¿œçš„ä¿¡å·ï¼Œè·ç¦»æ„ŸçŸ¥æ§åˆ¶è¡°å‡æ¢å¤ç­–ç•¥ï¼ˆDC-DRSï¼‰åŸºäºä½ç½®è·ç¦»å¹³æ»‘è°ƒèŠ‚æƒé‡ä»¥å‡€åŒ–æ³¨æ„åŠ›å¹¶æŠ‘åˆ¶å™ªå£°ï¼Œä»¥åŠè¿œç¨‹ä¾èµ–å†å¼ºåŒ–è¡°å‡æ¢å¤ç­–ç•¥ï¼ˆreRD-DRSï¼‰å·©å›ºå‰©ä½™çš„ä¿¡æ¯æ€§è¿œç¨‹ä¾èµ–ä»¥ç»´æŒå…¨å±€è¿è´¯æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒT-DRSèƒ½å¤Ÿä»¥æ— éœ€è®­ç»ƒçš„æ–¹å¼æŒç»­æå‡æ¨¡å‹æ€§èƒ½ï¼Œæœ‰æ•ˆæ¢å¤äº†è¢«æŠ‘åˆ¶çš„è¿œç¨‹æ ‡è®°å¯¹è€Œä¸æŸå®³å±€éƒ¨å½’çº³åç½®ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¨ç†é˜¶æ®µç­–ç•¥å¯ä»¥æœ‰æ•ˆç¼“è§£æ—‹è½¬ä½ç½®ç¼–ç çš„æ³¨æ„åŠ›è¡°å‡é—®é¢˜ï¼Œä¸ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è¿œç¨‹ä¾èµ–å»ºæ¨¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„å±€éƒ¨æ¨ç†èƒ½åŠ›ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Large Vision-Language Models (LVLMs) have achieved impressive performance across a wide range of multimodal tasks. However, they still face critical challenges in modeling long-range dependencies under the usage of Rotary Positional Encoding (ROPE). Although it can facilitate precise modeling of token positions, it induces progressive attention decay as token distance increases, especially with progressive attention decay over distant token pairs, which severely impairs the model's ability to remember global context. To alleviate this issue, we propose inference-only Three-step Decay Resilience Strategies (T-DRS), comprising (1) Semantic-Driven DRS (SD-DRS), amplifying semantically meaningful but distant signals via content-aware residuals, (2) Distance-aware Control DRS (DC-DRS), which can purify attention by smoothly modulating weights based on positional distances, suppressing noise while preserving locality, and (3) re-Reinforce Distant DRS (reRD-DRS), consolidating the remaining informative remote dependencies to maintain global coherence. Together, the T-DRS recover suppressed long-range token pairs without harming local inductive biases. Extensive experiments on Vision Question Answering (VQA) benchmarks demonstrate that T-DRS can consistently improve performance in a training-free manner. The code can be accessed in https://github.com/labixiaoq-qq/Remember-me</p>
<h3 id="8-sam-daq-segment-anything-model-with-depth-guided-adaptive-queries-for-rgb-d-video-salient-object-detection">[8] <a href="https://arxiv.org/abs/2511.09870">SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection</a></h3>
<p><em>Jia Lin, Xiaofei Zhou, Jiyuan Liu, Runmin Cong, Guodao Zhang, Zhi Liu, Jiyong Zhang</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SAM-DAQæ–¹æ³•ï¼Œé€šè¿‡æ·±åº¦å¼•å¯¼è‡ªé€‚åº”æŸ¥è¯¢å°†SAM2é€‚é…äºRGB-Dè§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹ä»»åŠ¡ï¼Œè§£å†³äº†æ‰‹åŠ¨æç¤ºä¾èµ–ã€åºåˆ—é€‚é…å™¨å†…å­˜æ¶ˆè€—é«˜å’Œå†…å­˜æ³¨æ„åŠ›è®¡ç®—è´Ÿæ‹…å¤§ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªRGB-D VSODæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç ”ç©¶å°è¯•å°†åŸºç¡€æ¨¡å‹ç›´æ¥åº”ç”¨äºRGB-Dè§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹ä»»åŠ¡æ—¶é¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå¯¹äººå·¥æç¤ºçš„ä¾èµ–æ€§ã€åºåˆ—é€‚é…å™¨çš„é«˜å†…å­˜æ¶ˆè€—ä»¥åŠå†…å­˜æ³¨æ„åŠ›çš„è®¡ç®—è´Ÿæ‹…ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†åŸºç¡€æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸­çš„é«˜æ•ˆåº”ç”¨ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†SAM-DAQæ–¹æ³•ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šåŸºäºå¹¶è¡Œé€‚é…å™¨çš„å¤šæ¨¡æ€å›¾åƒç¼–ç å™¨ï¼ˆPAMIEï¼‰å’ŒæŸ¥è¯¢é©±åŠ¨æ—¶åºå†…å­˜ï¼ˆQTMï¼‰æ¨¡å—ã€‚PAMIEé€šè¿‡æ·±åº¦å¼•å¯¼å¹¶è¡Œé€‚é…å™¨åœ¨æ— æç¤ºæ¡ä»¶ä¸‹å¾®è°ƒå†»ç»“çš„SAMç¼–ç å™¨ï¼ŒQTMæ¨¡å—é€šè¿‡åŒæ—¶åˆ©ç”¨å¸§çº§æŸ¥è¯¢å’Œè§†é¢‘çº§æŸ¥è¯¢ï¼Œé€‰æ‹©æ€§æå–æ—¶åºä¸€è‡´æ€§ç‰¹å¾å¹¶è¿­ä»£æ›´æ–°æŸ¥è¯¢çš„æ—¶åºè¡¨ç¤ºã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªRGB-D VSODæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SAM-DAQæ–¹æ³•åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¸€è‡´ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡æ·±åº¦å¼•å¯¼è‡ªé€‚åº”æŸ¥è¯¢æœºåˆ¶æœ‰æ•ˆé€‚é…åŸºç¡€åˆ†å‰²æ¨¡å‹è‡³è§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹ä»»åŠ¡ï¼Œä¸ºå¤šæ¨¡æ€è§†é¢‘åˆ†ææä¾›äº†ç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¹¶ä¸ºè§†è§‰åŸºç¡€æ¨¡å‹åœ¨æ—¶åºä»»åŠ¡ä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Recently segment anything model (SAM) has attracted widespread concerns, and it is often treated as a vision foundation model for universal segmentation. Some researchers have attempted to directly apply the foundation model to the RGB-D video salient object detection (RGB-D VSOD) task, which often encounters three challenges, including the dependence on manual prompts, the high memory consumption of sequential adapters, and the computational burden of memory attention. To address the limitations, we propose a novel method, namely Segment Anything Model with Depth-guided Adaptive Queries (SAM-DAQ), which adapts SAM2 to pop-out salient objects from videos by seamlessly integrating depth and temporal cues within a unified framework. Firstly, we deploy a parallel adapter-based multi-modal image encoder (PAMIE), which incorporates several depth-guided parallel adapters (DPAs) in a skip-connection way. Remarkably, we fine-tune the frozen SAM encoder under prompt-free conditions, where the DPA utilizes depth cues to facilitate the fusion of multi-modal features. Secondly, we deploy a query-driven temporal memory (QTM) module, which unifies the memory bank and prompt embeddings into a learnable pipeline. Concretely, by leveraging both frame-level queries and video-level queries simultaneously, the QTM module can not only selectively extract temporal consistency features but also iteratively update the temporal representations of the queries. Extensive experiments are conducted on three RGB-D VSOD datasets, and the results show that the proposed SAM-DAQ consistently outperforms state-of-the-art methods in terms of all evaluation metrics.</p>
<h3 id="9-hcc-3d-hierarchical-compensatory-compression-for-98-3d-token-reduction-in-vision-language-models">[9] <a href="https://arxiv.org/abs/2511.09883">HCC-3D: Hierarchical Compensatory Compression for 98% 3D Token Reduction in Vision-Language Models</a></h3>
<p><em>Liheng Zhang, Jin Wang, Hui Li, Bingfeng Zhang, Weifeng Liu</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†åˆ†å±‚è¡¥å¿å‹ç¼©æ–¹æ³•HCC-3Dï¼Œé€šè¿‡å…¨å±€ç»“æ„å‹ç¼©å’Œè‡ªé€‚åº”ç»†èŠ‚æŒ–æ˜æ¨¡å—ï¼Œåœ¨ä¿æŒå…³é”®ç»†èŠ‚çš„åŒæ—¶å®ç°äº†3Dä»¤ç‰Œçš„æç«¯å‹ç¼©ï¼Œæ˜¾è‘—æå‡äº†3Dè§†è§‰è¯­è¨€æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰3Dè§†è§‰è¯­è¨€æ¨¡å‹ç›´æ¥å°†ç‚¹äº‘åµŒå…¥ä¸º3Dä»¤ç‰Œï¼Œå¯¼è‡´åœ¨å¤§è¯­è¨€æ¨¡å‹éƒ¨åˆ†å¤„ç†æ‰€æœ‰3Dä»¤ç‰Œæ—¶äº§ç”Ÿå·¨å¤§çš„è®¡ç®—å¼€é”€ï¼Œè¿™é™åˆ¶äº†å…¶å®é™…åº”ç”¨ï¼Œå› æ­¤éœ€è¦æ‰¾åˆ°åœ¨å‡å°‘è®¡ç®—è´Ÿæ‹…çš„åŒæ—¶ä¿æŒå…³é”®ä¿¡æ¯å®Œæ•´æ€§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åˆ†å±‚è¡¥å¿å‹ç¼©æ¡†æ¶HCC-3Dï¼ŒåŒ…å«å…¨å±€ç»“æ„å‹ç¼©æ¨¡å—ä½¿ç”¨å…¨å±€æŸ¥è¯¢å°†3Dä»¤ç‰Œå‹ç¼©ä¸ºå°‘é‡å…³é”®ä»¤ç‰Œä»¥ä¿æŒæ•´ä½“ç»“æ„ä¿¡æ¯ï¼Œä»¥åŠè‡ªé€‚åº”ç»†èŠ‚æŒ–æ˜æ¨¡å—é€šè¿‡äº’è¡¥è¯„åˆ†é€‰æ‹©æ€§é‡æ–°å‹ç¼©æ˜¾è‘—ä½†æœªè¢«å……åˆ†å…³æ³¨çš„ç»†èŠ‚ç‰¹å¾æ¥è¡¥å¿ä¿¡æ¯æŸå¤±ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜HCC-3Dç›¸æ¯”ä¹‹å‰çš„3D-VLMså®ç°äº†çº¦98%çš„æç«¯å‹ç¼©ç‡ï¼ŒåŒæ—¶è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸¤æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡åˆ†å±‚è¡¥å¿å‹ç¼©ç­–ç•¥å¯ä»¥åœ¨å¤§å¹…å‡å°‘è®¡ç®—å¼€é”€çš„åŒæ—¶ä¿æŒç”šè‡³æå‡æ¨¡å‹æ€§èƒ½ï¼Œä¸ºé«˜æ•ˆ3Då¤šæ¨¡æ€ç†è§£æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œè¯æ˜äº†å‹ç¼©ä¸æ€§èƒ½å¹¶éå¿…ç„¶æƒè¡¡å…³ç³»ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>3D understanding has drawn significant attention recently, leveraging Vision-Language Models (VLMs) to enable multi-modal reasoning between point cloud and text data. Current 3D-VLMs directly embed the 3D point clouds into 3D tokens, following large 2D-VLMs with powerful reasoning capabilities. However, this framework has a great computational cost limiting its application, where we identify that the bottleneck lies in processing all 3D tokens in the Large Language Model (LLM) part. This raises the question: how can we reduce the computational overhead introduced by 3D tokens while preserving the integrity of their essential information? To address this question, we introduce Hierarchical Compensatory Compression (HCC-3D) to efficiently compress 3D tokens while maintaining critical detail retention. Specifically, we first propose a global structure compression (GSC), in which we design global queries to compress all 3D tokens into a few key tokens while keeping overall structural information. Then, to compensate for the information loss in GSC, we further propose an adaptive detail mining (ADM) module that selectively recompresses salient but under-attended features through complementary scoring. Extensive experiments demonstrate that HCC-3D not only achieves extreme compression ratios (approximately 98%) compared to previous 3D-VLMs, but also achieves new state-of-the-art performance, showing the great improvements on both efficiency and performance.</p>
<h3 id="10-regional-attention-enhanced-swin-transformer-for-clinically-relevant-medical-image-captioning">[10] <a href="https://arxiv.org/abs/2511.09893">Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning</a></h3>
<p><em>Zubia Naz, Farhan Asghar, Muhammad Ishfaq Hussain, Yahya Hadadi, Muhammad Aasim Rafique, Wookjin Choi, Moongu Jeon</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºSwin-BARTç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„è‡ªåŠ¨åŒ–åŒ»å­¦å›¾åƒæè¿°ç³»ç»Ÿï¼Œé€šè¿‡è½»é‡çº§åŒºåŸŸæ³¨æ„åŠ›æ¨¡å—å¢å¼ºè¯Šæ–­å…³é”®åŒºåŸŸï¼Œåœ¨ROCOæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è¯­ä¹‰ä¿çœŸåº¦å’Œå¯è§£é‡Šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªåŠ¨åŒ–åŒ»å­¦å›¾åƒæè¿°æ—¨åœ¨å°†å¤æ‚çš„æ”¾å°„å­¦å›¾åƒè½¬åŒ–ä¸ºè¯Šæ–­æ€§å™è¿°ï¼Œä»¥æ”¯æŒæŠ¥å‘Šå·¥ä½œæµç¨‹ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨è¯­ä¹‰å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨Swin-BARTç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¼•å…¥è½»é‡çº§åŒºåŸŸæ³¨æ„åŠ›æ¨¡å—åœ¨äº¤å‰æ³¨æ„åŠ›å‰æ”¾å¤§è¯Šæ–­å…³é”®åŒºåŸŸï¼Œä½¿ç”¨æŸæœç´¢è§£ç ç­–ç•¥ï¼ˆæŸå¤§å°=4ï¼Œé•¿åº¦æƒ©ç½š=1.1ï¼Œæ— é‡å¤n-gramå¤§å°=3ï¼Œæœ€å¤§é•¿åº¦=128ï¼‰ã€‚</p>
<p><strong>Result:</strong> åœ¨ROCOæ•°æ®é›†ä¸Šå–å¾—æœ€ä¼˜æ€§èƒ½ï¼šROUGEå¾—åˆ†0.603ï¼ˆResNet-CNN 0.356ï¼ŒBLIP2-OPT 0.255ï¼‰ï¼ŒBERTScore 0.807ï¼ˆBLIP2-OPT 0.645ï¼ŒResNet-CNN 0.623ï¼‰ï¼ŒBLEUã€CIDErå’ŒMETEORæŒ‡æ ‡å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶æä¾›æ¶ˆèå®éªŒã€æ¨¡æ€åˆ†æå’Œå®šæ€§çƒ­åŠ›å›¾éªŒè¯ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥è®¾è®¡ç”Ÿæˆå‡†ç¡®ä¸”ä¸´åºŠæœ¯è¯­åŒ–çš„æè¿°ï¼Œæä¾›é€æ˜çš„åŒºåŸŸå½’å› ï¼Œæ”¯æŒåœ¨äººç±»ç›‘ç£ä¸‹çš„å®‰å…¨ç ”ç©¶åº”ç”¨ï¼Œä¸ºåŒ»å­¦å›¾åƒç†è§£æä¾›äº†å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Automated medical image captioning translates complex radiological images into diagnostic narratives that can support reporting workflows. We present a Swin-BART encoder-decoder system with a lightweight regional attention module that amplifies diagnostically salient regions before cross-attention. Trained and evaluated on ROCO, our model achieves state-of-the-art semantic fidelity while remaining compact and interpretable. We report results as mean$\pm$std over three seeds and include $95\%$ confidence intervals. Compared with baselines, our approach improves ROUGE (proposed 0.603, ResNet-CNN 0.356, BLIP2-OPT 0.255) and BERTScore (proposed 0.807, BLIP2-OPT 0.645, ResNet-CNN 0.623), with competitive BLEU, CIDEr, and METEOR. We further provide ablations (regional attention on/off and token-count sweep), per-modality analysis (CT/MRI/X-ray), paired significance tests, and qualitative heatmaps that visualize the regions driving each description. Decoding uses beam search (beam size $=4$), length penalty $=1.1$, $no_repeat_ngram_size$ $=3$, and max length $=128$. The proposed design yields accurate, clinically phrased captions and transparent regional attributions, supporting safe research use with a human in the loop.</p>
<h3 id="11-mosaicdoc-a-large-scale-bilingual-benchmark-for-visually-rich-document-understanding">[11] <a href="https://arxiv.org/abs/2511.09919">MosaicDoc: A Large-Scale Bilingual Benchmark for Visually Rich Document Understanding</a></h3>
<p><em>Ketong Chen, Yuhao Chen, Yang Xue</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DocWeaverå¤šæ™ºèƒ½ä½“æµæ°´çº¿å’ŒMosaicDocåŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŒè¯­è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£åŸºå‡†ï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆæ–¹æ³•è§£å†³äº†ç°æœ‰è¯„ä¼°åŸºå‡†åœ¨è¯­è¨€å¤šæ ·æ€§ã€å¸ƒå±€å¤æ‚æ€§å’Œä»»åŠ¡è¦†ç›–èŒƒå›´æ–¹é¢çš„ä¸è¶³ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°åŸºå‡†ä¸»è¦å­˜åœ¨ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šä»¥è‹±è¯­ä¸ºä¸­å¿ƒã€å¸ƒå±€è¿‡äºç®€åŒ–ä»¥åŠæ”¯æŒä»»åŠ¡æœ‰é™ï¼Œè¿™äº›é™åˆ¶ä½¿å¾—å®ƒä»¬æ— æ³•æœ‰æ•ˆè¯„ä¼°æ¨¡å‹åœ¨è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£è¿™ä¸€å…³é”®æŒ‘æˆ˜ä¸Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å¤„ç†å¤æ‚å¸ƒå±€å’Œå¯†é›†æ–‡æœ¬çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†DocWeaverå¤šæ™ºèƒ½ä½“æµæ°´çº¿ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆæ–°çš„è¯„ä¼°åŸºå‡†ï¼Œæœ€ç»ˆæ„å»ºäº†MosaicDocè¿™ä¸€å¤§è§„æ¨¡åŒè¯­èµ„æºï¼Œè¯¥åŸºå‡†æºè‡ªæŠ¥çº¸å’Œæ‚å¿—ï¼Œå…·æœ‰å¤šæ ·åŒ–çš„å¤æ‚å¸ƒå±€ã€æ¥è‡ª196ä¸ªå‡ºç‰ˆå•†çš„ä¸°å¯Œé£æ ¼å˜åŒ–ï¼Œä»¥åŠæ¶µç›–OCRã€VQAã€é˜…è¯»é¡ºåºå’Œå®šä½çš„å…¨é¢å¤šä»»åŠ¡æ ‡æ³¨ã€‚</p>
<p><strong>Result:</strong> MosaicDocåŒ…å«72Kå¼ å›¾åƒå’Œè¶…è¿‡600Kä¸ªé—®ç­”å¯¹ï¼Œé€šè¿‡å¯¹ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹åœ¨è¯¥åŸºå‡†ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œæ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨å¤„ç†çœŸå®ä¸–ç•Œæ–‡æ¡£å¤æ‚æ€§æ–¹é¢çš„å½“å‰å±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ¸…æ™°çš„æ–¹å‘ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ä»…æä¾›äº†ä¸€ä¸ªæƒå¨çš„è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£åŸºå‡†ï¼Œè¿˜é€šè¿‡ç³»ç»Ÿè¯„ä¼°æ­ç¤ºäº†å½“å‰æ¨¡å‹çš„ä¸è¶³ï¼Œå¼ºè°ƒäº†å¤„ç†å¤æ‚æ–‡æ¡£å¸ƒå±€å’Œå¤šè¯­è¨€èƒ½åŠ›çš„é‡è¦æ€§ï¼Œä¸ºä¸‹ä¸€ä»£æ–‡æ¡£ç†è§£æ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Despite the rapid progress of Vision-Language Models (VLMs), their capabilities are inadequately assessed by existing benchmarks, which are predominantly English-centric, feature simplistic layouts, and support limited tasks. Consequently, they fail to evaluate model performance for Visually Rich Document Understanding (VRDU), a critical challenge involving complex layouts and dense text. To address this, we introduce DocWeaver, a novel multi-agent pipeline that leverages Large Language Models to automatically generate a new benchmark. The result is MosaicDoc, a large-scale, bilingual (Chinese and English) resource designed to push the boundaries of VRDU. Sourced from newspapers and magazines, MosaicDoc features diverse and complex layouts (including multi-column and non-Manhattan), rich stylistic variety from 196 publishers, and comprehensive multi-task annotations (OCR, VQA, reading order, and localization). With 72K images and over 600K QA pairs, MosaicDoc serves as a definitive benchmark for the field. Our extensive evaluation of state-of-the-art models on this benchmark reveals their current limitations in handling real-world document complexity and charts a clear path for future research.</p>
<h3 id="12-tspe-gs-probabilistic-depth-extraction-for-semi-transparent-surface-reconstruction-via-3d-gaussian-splatting">[12] <a href="https://arxiv.org/abs/2511.09944">TSPE-GS: Probabilistic Depth Extraction for Semi-Transparent Surface Reconstruction via 3D Gaussian Splatting</a></h3>
<p><em>Zhiyuan Xu, Nan Min, Yuhang Guo, Tong Wei</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºTSPE-GSæ–¹æ³•ï¼Œé€šè¿‡å‡åŒ€é‡‡æ ·é€å°„ç‡æ¥å»ºæ¨¡åƒç´ çº§å¤šæ¨¡æ€ä¸é€æ˜åº¦å’Œæ·±åº¦åˆ†å¸ƒï¼Œè§£å†³äº†3Dé«˜æ–¯æ³¼æº…åœ¨åŠé€æ˜è¡¨é¢é‡å»ºä¸­çš„å±€é™æ€§ï¼Œæ˜¾è‘—æå‡äº†åŠé€æ˜å‡ ä½•é‡å»ºè´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> 3Dé«˜æ–¯æ³¼æº…åœ¨é€Ÿåº¦-è´¨é‡æƒè¡¡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨é‡å»ºåŠé€æ˜è¡¨é¢æ—¶å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•å¤§å¤šå‡è®¾æ¯ä¸ªåƒç´ ä»…æœ‰ä¸€ä¸ªæ·±åº¦å€¼ï¼Œå½“å¤šä¸ªè¡¨é¢å¯è§æ—¶è¿™ç§å‡è®¾å°±ä¼šå¤±æ•ˆã€‚</p>
<p><strong>Method:</strong> TSPE-GSæ–¹æ³•é‡‡ç”¨å‡åŒ€é‡‡æ ·é€å°„ç‡æ¥å»ºæ¨¡åƒç´ çº§å¤šæ¨¡æ€ä¸é€æ˜åº¦å’Œæ·±åº¦åˆ†å¸ƒï¼Œæ›¿ä»£äº†å…ˆå‰çš„å•å³°å‡è®¾ï¼Œè§£å†³äº†è·¨è¡¨é¢æ·±åº¦æ¨¡ç³Šé—®é¢˜ï¼›é€šè¿‡æ¸è¿›å¼èåˆæˆªæ–­ç¬¦å·è·ç¦»å‡½æ•°ï¼Œåœ¨ç»Ÿä¸€æ¡†æ¶å†…åˆ†åˆ«é‡å»ºå¤–éƒ¨å’Œå†…éƒ¨è¡¨é¢ã€‚</p>
<p><strong>Result:</strong> åœ¨å…¬å¼€å’Œè‡ªé‡‡é›†çš„åŠé€æ˜åŠä¸é€æ˜æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTSPE-GSæ˜¾è‘—æ”¹å–„äº†åŠé€æ˜å‡ ä½•é‡å»ºè´¨é‡ï¼ŒåŒæ—¶åœ¨ä¸é€æ˜åœºæ™¯ä¸Šä¿æŒäº†åŸæœ‰æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒå¼€é”€å³å¯æ³›åŒ–åˆ°å…¶ä»–åŸºäºé«˜æ–¯çš„é‡å»ºæµç¨‹ä¸­ï¼Œä¸ºåŠé€æ˜è¡¨é¢é‡å»ºæä¾›äº†ç»Ÿä¸€è§£å†³æ–¹æ¡ˆï¼Œæ‰©å±•äº†3Dé«˜æ–¯æ³¼æº…çš„åº”ç”¨èŒƒå›´ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>3D Gaussian Splatting offers a strong speed-quality trade-off but struggles to reconstruct semi-transparent surfaces because most methods assume a single depth per pixel, which fails when multiple surfaces are visible. We propose TSPE-GS (Transparent Surface Probabilistic Extraction for Gaussian Splatting), which uniformly samples transmittance to model a pixel-wise multi-modal distribution of opacity and depth, replacing the prior single-peak assumption and resolving cross-surface depth ambiguity. By progressively fusing truncated signed distance functions, TSPE-GS reconstructs external and internal surfaces separately within a unified framework. The method generalizes to other Gaussian-based reconstruction pipelines without extra training overhead. Extensive experiments on public and self-collected semi-transparent and opaque datasets show TSPE-GS significantly improves semi-transparent geometry reconstruction while maintaining performance on opaque scenes.</p>
<h3 id="13-beyond-cosine-similarity-magnitude-aware-clip-for-no-reference-image-quality-assessment">[13] <a href="https://arxiv.org/abs/2511.09948">Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment</a></h3>
<p><em>Zhicheng Liao, Dongxu Wu, Zhenshan Shi, Sijie Mai, Hanwei Zhu, Lingyu Zhu, Yuncheng Jiang, Baoliang Chen</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”èåˆæ¡†æ¶ï¼Œå°†CLIPå›¾åƒç‰¹å¾çš„å¹…åº¦ä¿¡æ¯ä¸ä½™å¼¦ç›¸ä¼¼åº¦ç›¸ç»“åˆç”¨äºæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ã€‚è¯¥æ–¹æ³•é€šè¿‡Box-Coxå˜æ¢å½’ä¸€åŒ–ç‰¹å¾åˆ†å¸ƒï¼Œå¹¶è®¾è®¡ç½®ä¿¡åº¦å¼•å¯¼çš„èåˆç­–ç•¥ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–¹æ³•ä¸»è¦åˆ©ç”¨CLIPæ¨¡å‹çš„ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼Œä½†å¿½è§†äº†CLIPå›¾åƒç‰¹å¾å¹…åº¦ä¸æ„ŸçŸ¥è´¨é‡ä¹‹é—´çš„å¼ºç›¸å…³æ€§ã€‚è¯­ä¹‰ç›¸ä¼¼åº¦æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨è¿™ä¸€å…³é”®ä½†æœªè¢«å……åˆ†æ¢ç´¢çš„çº¿ç´¢ï¼Œå¯¼è‡´è¯„ä¼°æ€§èƒ½å—é™ã€‚</p>
<p><strong>Method:</strong> æå‡ºè‡ªé€‚åº”èåˆæ¡†æ¶ï¼Œé¦–å…ˆæå–CLIPå›¾åƒç‰¹å¾çš„ç»å¯¹å€¼å¹¶åº”ç”¨Box-Coxå˜æ¢è¿›è¡Œç»Ÿè®¡å½’ä¸€åŒ–ä»¥å‡è½»è¯­ä¹‰æ•æ„Ÿæ€§ã€‚è®¾è®¡ç½®ä¿¡åº¦å¼•å¯¼çš„èåˆæ–¹æ¡ˆï¼Œæ ¹æ®æ¯ä¸ªçº¿ç´¢çš„ç›¸å¯¹å¼ºåº¦è‡ªé€‚åº”åŠ æƒï¼Œæœ‰æ•ˆæ•´åˆä½™å¼¦ç›¸ä¼¼åº¦å’Œå¹…åº¦æ„ŸçŸ¥è´¨é‡çº¿ç´¢ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†IQAæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸€è‡´ä¼˜äºæ ‡å‡†çš„åŸºäºCLIPçš„IQAæ–¹æ³•å’Œæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œä¸”æ— éœ€ä»»ä½•ä»»åŠ¡ç‰¹å®šçš„è®­ç»ƒã€‚å®éªŒéªŒè¯äº†å¹…åº¦çº¿ç´¢ä¸æ„ŸçŸ¥è´¨é‡ä¹‹é—´çš„å¼ºç›¸å…³æ€§åŠå…¶å¯¹æ€§èƒ½æå‡çš„é‡è¦è´¡çŒ®ã€‚</p>
<p><strong>Conclusion:</strong> CLIPå›¾åƒç‰¹å¾å¹…åº¦æ˜¯æ„ŸçŸ¥è´¨é‡è¯„ä¼°çš„é‡è¦çº¿ç´¢ï¼Œä¸ä½™å¼¦ç›¸ä¼¼åº¦å…·æœ‰äº’è¡¥æ€§ã€‚è‡ªé€‚åº”èåˆæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤šæ¨¡æ€çº¿ç´¢ï¼Œä¸ºæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå±•ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹ç‰¹å¾ç»Ÿè®¡ç‰¹æ€§çš„æ½œåœ¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as "a good photo" or "a bad photo." However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality. In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue. Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity. The resulting scalar summary serves as a semantically-normalized auxiliary cue that complements cosine-based prompt matching. To integrate both cues effectively, we further design a confidence-guided fusion scheme that adaptively weighs each term according to its relative strength. Extensive experiments on multiple benchmark IQA datasets demonstrate that our method consistently outperforms standard CLIP-based IQA and state-of-the-art baselines, without any task-specific training.</p>
<h3 id="14-robust-object-detection-with-pseudo-labels-from-vlms-using-per-object-co-teaching">[14] <a href="https://arxiv.org/abs/2511.09955">Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching</a></h3>
<p><em>Uday Bhaskar, Rishabh Bhattacharya, Avinash Patel, Sarthak Khoche, Praveen Anil Kulkarni, Naresh Manwani</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆä¼ªæ ‡ç­¾æ¥è®­ç»ƒé«˜æ•ˆå®æ—¶ç›®æ ‡æ£€æµ‹å™¨çš„æ–°é¢–æµç¨‹ï¼Œé€šè¿‡é€å¯¹è±¡ååŒæ•™å­¦ç­–ç•¥æœ‰æ•ˆç¼“è§£VLMç”Ÿæˆæ ‡ç­¾ä¸­çš„å™ªå£°é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸‹çš„ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŸºç¡€æ¨¡å‹ç‰¹åˆ«æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ç­‰éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®çš„é¢†åŸŸæä¾›äº†æœ‰å‰æ™¯çš„é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹èƒ½åŠ›ï¼Œä½†å…¶æ£€æµ‹å»¶è¿Ÿå’Œå¹»è§‰é¢„æµ‹é—®é¢˜ä½¿å…¶æ— æ³•ç›´æ¥éƒ¨ç½²åº”ç”¨ï¼Œè€Œæ‰‹åŠ¨æ ‡æ³¨æˆæœ¬åˆæå…¶æ˜‚è´µã€‚</p>
<p><strong>Method:</strong> æå‡ºåŸºäºé€å¯¹è±¡ååŒæ•™å­¦çš„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡ä¸¤ä¸ªYOLOæ¨¡å‹åä½œå­¦ä¹ ï¼Œæ ¹æ®å¯¹ç­‰æ¨¡å‹çš„é€å¯¹è±¡æŸå¤±å€¼åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿‡æ»¤ä¸å¯é çš„è¾¹ç•Œæ¡†ï¼Œè€Œä¸æ˜¯è¿‡æ»¤æ•´å¼ å›¾åƒï¼Œä»è€Œæœ‰æ•ˆç¼“è§£VLMç”Ÿæˆæ ‡ç­¾ä¸­çš„å™ªå£°é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨KITTIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿YOLOv5mæ¨¡å‹ï¼ŒmAP@0.5ä»31.12%æå‡è‡³46.61%ï¼ŒåŒæ—¶ä¿æŒå®æ—¶æ£€æµ‹å»¶è¿Ÿï¼›è¡¥å……10%çœŸå®æ ‡ç­¾åæ€§èƒ½è¿›ä¸€æ­¥æå‡è‡³57.97% mAP@0.5ï¼Œåœ¨ACDCå’ŒBDD100kæ•°æ®é›†ä¸Šä¹Ÿè§‚å¯Ÿåˆ°ç±»ä¼¼æ€§èƒ½æå‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æµç¨‹ä¸ºè‡ªåŠ¨é©¾é©¶æä¾›äº†ä¸€ç§é«˜æ•ˆã€é²æ£’ä¸”å¯æ‰©å±•çš„é«˜æ€§èƒ½ç›®æ ‡æ£€æµ‹å™¨è®­ç»ƒæ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹æ˜‚è´µäººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œè¯æ˜äº†åˆ©ç”¨VLMç”Ÿæˆä¼ªæ ‡ç­¾ç»“åˆååŒæ•™å­¦ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®é™…åº”ç”¨éƒ¨ç½²æä¾›äº†å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Foundation models, especially vision-language models (VLMs), offer compelling zero-shot object detection for applications like autonomous driving, a domain where manual labelling is prohibitively expensive. However, their detection latency and tendency to hallucinate predictions render them unsuitable for direct deployment. This work introduces a novel pipeline that addresses this challenge by leveraging VLMs to automatically generate pseudo-labels for training efficient, real-time object detectors. Our key innovation is a per-object co-teaching-based training strategy that mitigates the inherent noise in VLM-generated labels. The proposed per-object coteaching approach filters noisy bounding boxes from training instead of filtering the entire image. Specifically, two YOLO models learn collaboratively, filtering out unreliable boxes from each mini-batch based on their peers' per-object loss values. Overall, our pipeline provides an efficient, robust, and scalable approach to train high-performance object detectors for autonomous driving, significantly reducing reliance on costly human annotation. Experimental results on the KITTI dataset demonstrate that our method outperforms a baseline YOLOv5m model, achieving a significant mAP@0.5 boost ($31.12\%$ to $46.61\%$) while maintaining real-time detection latency. Furthermore, we show that supplementing our pseudo-labelled data with a small fraction of ground truth labels ($10\%$) leads to further performance gains, reaching $57.97\%$ mAP@0.5 on the KITTI dataset. We observe similar performance improvements for the ACDC and BDD100k datasets.</p>
<h3 id="15-difference-vector-equalization-for-robust-fine-tuning-of-vision-language-models">[15] <a href="https://arxiv.org/abs/2511.09973">Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models</a></h3>
<p><em>Satoshi Suzuki, Shin'ya Yamaguchi, Shoichiro Takeda, Taiga Yamane, Naoki Makishima, Naotaka Kawata, Mana Ihori, Tomohiro Tanaka, Shota Orihashi, Ryo Masumura</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†å·®å¼‚å‘é‡å‡è¡¡åŒ–æ–¹æ³•ï¼Œé€šè¿‡çº¦æŸé¢„è®­ç»ƒæ¨¡å‹ä¸å¾®è°ƒæ¨¡å‹åµŒå…¥ä¹‹é—´çš„å·®å¼‚å‘é‡æ¥ä¿æŒåµŒå…¥ç©ºé—´çš„å‡ ä½•ç»“æ„ï¼Œä»è€Œåœ¨æå‡åˆ†å¸ƒå†…æ€§èƒ½çš„åŒæ—¶ä¿æŒåˆ†å¸ƒå¤–å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºå¯¹æ¯”å­¦ä¹ çš„é²æ£’å¾®è°ƒæ–¹æ³•ä¼šæ‰­æ›²åµŒå…¥ç©ºé—´çš„å‡ ä½•ç»“æ„ï¼Œè¿™ç§å‡ ä½•ç»“æ„å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå¯¼è‡´åœ¨åˆ†å¸ƒå¤–å’Œé›¶æ ·æœ¬åœºæ™¯ä¸‹æ€§èƒ½å—é™ã€‚</p>
<p><strong>Method:</strong> æå‡ºå·®å¼‚å‘é‡å‡è¡¡åŒ–æ–¹æ³•ï¼Œé€šè¿‡å¹³å‡å‘é‡æŸå¤±å’Œæˆå¯¹å‘é‡æŸå¤±ä¸¤ç§çº¦æŸæœºåˆ¶æ¥ä¿æŒå‡ ä½•ç»“æ„ã€‚å¹³å‡å‘é‡æŸå¤±é€šè¿‡çº¦æŸå·®å¼‚å‘é‡ä¸å…¶åŠ æƒå¹³å‡å€¼ç›¸ç­‰æ¥å…¨å±€ä¿æŒå‡ ä½•ç»“æ„ï¼Œè€Œæˆå¯¹å‘é‡æŸå¤±åˆ™é€šè¿‡ç¡®ä¿ä¸€è‡´çš„å¤šæ¨¡æ€å¯¹é½æ¥å±€éƒ¨ä¿æŒå‡ ä½•ç»“æ„ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆä¿æŒåµŒå…¥ç©ºé—´çš„å‡ ä½•ç»“æ„ï¼Œåœ¨åˆ†å¸ƒå†…ã€åˆ†å¸ƒå¤–å’Œé›¶æ ·æœ¬è¯„ä¼°æŒ‡æ ‡ä¸Šå‡å–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•è¯æ˜äº†åœ¨è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­ä¿æŒåµŒå…¥ç©ºé—´å‡ ä½•ç»“æ„çš„é‡è¦æ€§ï¼Œä¸ºæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Contrastive pre-trained vision-language models, such as CLIP, demonstrate strong generalization abilities in zero-shot classification by leveraging embeddings extracted from image and text encoders. This paper aims to robustly fine-tune these vision-language models on in-distribution (ID) data without compromising their generalization abilities in out-of-distribution (OOD) and zero-shot settings. Current robust fine-tuning methods tackle this challenge by reusing contrastive learning, which was used in pre-training, for fine-tuning. However, we found that these methods distort the geometric structure of the embeddings, which plays a crucial role in the generalization of vision-language models, resulting in limited OOD and zero-shot performance. To address this, we propose Difference Vector Equalization (DiVE), which preserves the geometric structure during fine-tuning. The idea behind DiVE is to constrain difference vectors, each of which is obtained by subtracting the embeddings extracted from the pre-trained and fine-tuning models for the same data sample. By constraining the difference vectors to be equal across various data samples, we effectively preserve the geometric structure. Therefore, we introduce two losses: average vector loss (AVL) and pairwise vector loss (PVL). AVL preserves the geometric structure globally by constraining difference vectors to be equal to their weighted average. PVL preserves the geometric structure locally by ensuring a consistent multimodal alignment. Our experiments demonstrate that DiVE effectively preserves the geometric structure, achieving strong results across ID, OOD, and zero-shot metrics.</p>
<h3 id="16-anomagic-crossmodal-prompt-driven-zero-shot-anomaly-generation">[16] <a href="https://arxiv.org/abs/2511.10020">Anomagic: Crossmodal Prompt-driven Zero-shot Anomaly Generation</a></h3>
<p><em>Yuxin Jiang, Wei Luo, Hui Zhang, Qiyu Chen, Haiming Yao, Weiming Shen, Yunkang Cao</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºAnomagicï¼Œä¸€ç§é›¶æ ·æœ¬å¼‚å¸¸ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡è·¨æ¨¡æ€æç¤ºç¼–ç æ–¹æ¡ˆç»Ÿä¸€è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œæ— éœ€çœŸå®å¼‚å¸¸æ ·æœ¬å³å¯ç”Ÿæˆè¯­ä¹‰ä¸€è‡´çš„å¼‚å¸¸ã€‚è¯¥æ–¹æ³•ç»“åˆå¯¹æ¯”ç²¾ç‚¼ç­–ç•¥å¢å¼ºå¼‚å¸¸ä¸æ©ç çš„å¯¹é½ï¼Œæ˜¾è‘—æå‡ä¸‹æ¸¸å¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¼‚å¸¸ç”Ÿæˆæ–¹æ³•é€šå¸¸ä¾èµ–çœŸå®å¼‚å¸¸æ ·æœ¬ä½œä¸ºå‚è€ƒï¼Œé™åˆ¶äº†åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„åº”ç”¨èƒ½åŠ›ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³æ— éœ€çœŸå®å¼‚å¸¸æ ·æœ¬å³å¯ç”Ÿæˆè¯­ä¹‰ä¸€è‡´å¼‚å¸¸çš„é—®é¢˜ï¼Œå¡«è¡¥é›¶æ ·æœ¬å¼‚å¸¸ç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> æå‡ºè·¨æ¨¡æ€æç¤ºç¼–ç æ–¹æ¡ˆç»Ÿä¸€è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯å¼•å¯¼åŸºäºä¿®å¤çš„ç”Ÿæˆæµç¨‹ã€‚é‡‡ç”¨å¯¹æ¯”ç²¾ç‚¼ç­–ç•¥å¼ºåˆ¶åˆæˆå¼‚å¸¸ä¸å…¶æ©ç ä¹‹é—´çš„ç²¾ç¡®å¯¹é½ã€‚æ„å»ºAnomVerseæ•°æ®é›†ï¼ŒåŒ…å«12,987ä¸ªå¼‚å¸¸-æ©ç -æè¿°ä¸‰å…ƒç»„ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–è§†è§‰æç¤ºå’ŒåŸºäºæ¨¡æ¿çš„æ–‡æœ¬æç¤ºã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜Anomagicåœ¨AnomVerseä¸Šè®­ç»ƒåèƒ½ç”Ÿæˆæ¯”ç°æœ‰æ–¹æ³•æ›´çœŸå®å¤šæ ·çš„å¼‚å¸¸ï¼Œæ˜¾è‘—æå‡ä¸‹æ¸¸å¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºä»»ä½•æ­£å¸¸ç±»åˆ«å›¾åƒç”Ÿæˆç”¨æˆ·å®šä¹‰æç¤ºçš„å¼‚å¸¸ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> Anomagicå»ºç«‹äº†å¼‚å¸¸ç”Ÿæˆçš„é€šç”¨åŸºç¡€æ¨¡å‹ï¼Œä¸ºé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†è·¨æ¨¡æ€ä¿¡æ¯èåˆåœ¨å¼‚å¸¸ç”Ÿæˆä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºæœªæ¥æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>We propose Anomagic, a zero-shot anomaly generation method that produces semantically coherent anomalies without requiring any exemplar anomalies. By unifying both visual and textual cues through a crossmodal prompt encoding scheme, Anomagic leverages rich contextual information to steer an inpainting-based generation pipeline. A subsequent contrastive refinement strategy enforces precise alignment between synthesized anomalies and their masks, thereby bolstering downstream anomaly detection accuracy. To facilitate training, we introduce AnomVerse, a collection of 12,987 anomaly-mask-caption triplets assembled from 13 publicly available datasets, where captions are automatically generated by multimodal large language models using structured visual prompts and template-based textual hints. Extensive experiments demonstrate that Anomagic trained on AnomVerse can synthesize more realistic and varied anomalies than prior methods, yielding superior improvements in downstream anomaly detection. Furthermore, Anomagic can generate anomalies for any normal-category image using user-defined prompts, establishing a versatile foundation model for anomaly generation.</p>
<h3 id="17-lampq-towards-accurate-layer-wise-mixed-precision-quantization-for-vision-transformers">[17] <a href="https://arxiv.org/abs/2511.10004">LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers</a></h3>
<p><em>Minjun Kim, Jaeri Lee, Jongjin Kim, Jeongin Yun, Yongmo Kwon, U Kang</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†LampQæ–¹æ³•ï¼Œä¸€ç§é’ˆå¯¹Vision Transformersçš„å±‚çº§æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡ç±»å‹æ„ŸçŸ¥çš„Fisheræ•æ„Ÿåº¦åº¦é‡ã€æ•´æ•°çº¿æ€§è§„åˆ’ä¼˜åŒ–å’Œè¿­ä»£ä½å®½åˆ†é…ï¼Œè§£å†³äº†ç°æœ‰æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•åœ¨ç²’åº¦ã€åº¦é‡å°ºåº¦å’Œä½å®½åˆ†é…æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰Vision Transformeré‡åŒ–æ–¹æ³•ä¸»è¦é‡‡ç”¨ç»Ÿä¸€ç²¾åº¦ç­–ç•¥ï¼Œå¿½è§†äº†ä¸åŒç»„ä»¶å¯¹é‡åŒ–çš„æ•æ„Ÿåº¦å·®å¼‚ï¼Œè€ŒåŸºäºåº¦é‡çš„æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•å­˜åœ¨ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šç²’åº¦è¿‡äºç²—ç³™ã€ä¸åŒç±»å‹ç»„ä»¶é—´åº¦é‡å°ºåº¦ä¸åŒ¹é…ã€ä»¥åŠä½å®½åˆ†é…æœªè€ƒè™‘é‡åŒ–å½±å“ã€‚</p>
<p><strong>Method:</strong> LampQé‡‡ç”¨å±‚çº§é‡åŒ–å®ç°ç»†ç²’åº¦æ§åˆ¶å’Œé«˜æ•ˆåŠ é€Ÿï¼Œå¼•å…¥ç±»å‹æ„ŸçŸ¥çš„Fisheræ•æ„Ÿåº¦åº¦é‡æ–¹æ³•ï¼Œé€šè¿‡æ•´æ•°çº¿æ€§è§„åˆ’è¿›è¡Œæœ€ä¼˜ä½å®½åˆ†é…ï¼Œå¹¶é‡‡ç”¨è¿­ä»£æ›´æ–°ç­–ç•¥è¿›ä¸€æ­¥ä¼˜åŒ–ä½å®½é…ç½®ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLampQåœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œé›¶æ ·æœ¬é‡åŒ–ç­‰å¤šç§ä»»åŠ¡ä¸­é¢„è®­ç»ƒçš„Vision Transformersé‡åŒ–æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> LampQé€šè¿‡ç»†ç²’åº¦çš„å±‚çº§åˆ«æ··åˆç²¾åº¦é‡åŒ–ç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³äº†Vision Transformersé‡åŒ–ä¸­çš„æ•æ„Ÿåº¦å·®å¼‚é—®é¢˜ï¼Œä¸ºé«˜æ•ˆéƒ¨ç½²å¤§è§„æ¨¡è§†è§‰Transformeræ¨¡å‹æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>How can we accurately quantize a pre-trained Vision Transformer model? Quantization algorithms compress Vision Transformers (ViTs) into low-bit formats, reducing memory and computation demands with minimal accuracy degradation. However, existing methods rely on uniform precision, ignoring the diverse sensitivity of ViT components to quantization. Metric-based Mixed Precision Quantization (MPQ) is a promising alternative, but previous MPQ methods for ViTs suffer from three major limitations: 1) coarse granularity, 2) mismatch in metric scale across component types, and 3) quantization-unaware bit allocation. In this paper, we propose LampQ (Layer-wise Mixed Precision Quantization for Vision Transformers), an accurate metric-based MPQ method for ViTs to overcome these limitations. LampQ performs layer-wise quantization to achieve both fine-grained control and efficient acceleration, incorporating a type-aware Fisher-based metric to measure sensitivity. Then, LampQ assigns bit-widths optimally through integer linear programming and further updates them iteratively. Extensive experiments show that LampQ provides the state-of-the-art performance in quantizing ViTs pre-trained on various tasks such as image classification, object detection, and zero-shot quantization.</p>
<h3 id="18-right-looks-wrong-reasons-compositional-fidelity-in-text-to-image-generation">[18] <a href="https://arxiv.org/abs/2511.10136">Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation</a></h3>
<p><em>Mayank Vatsa, Aparna Bharati, Richa Singh</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡è°ƒæŸ¥äº†å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨é€»è¾‘ç»„åˆæ–¹é¢çš„æ ¹æœ¬ç¼ºé™·ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å¦å®šã€è®¡æ•°å’Œç©ºé—´å…³ç³»ç­‰æ ¸å¿ƒåŸè¯­ç»„åˆæ—¶å‡ºç°æ€§èƒ½å´©æºƒçš„ç°è±¡ï¼Œå¹¶æŒ‡å‡ºå®ç°çœŸæ­£ç»„åˆæ€§éœ€è¦è¡¨ç¤ºå’Œæ¨ç†æ–¹é¢çš„æ ¹æœ¬æ€§çªç ´ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰é¢†å…ˆçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ¶æ„å­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·ï¼šæ— æ³•å¤„ç†é€»è¾‘ç»„åˆã€‚æœ¬ç ”ç©¶æ—¨åœ¨è°ƒæŸ¥è¿™ç§æ•…éšœåœ¨ä¸‰ä¸ªæ ¸å¿ƒåŸè¯­ï¼ˆå¦å®šã€è®¡æ•°å’Œç©ºé—´å…³ç³»ï¼‰ä¸Šçš„è¡¨ç°ï¼Œæ­ç¤ºæ¨¡å‹åœ¨ç»„åˆè¿™äº›åŸè¯­æ—¶å‡ºç°çš„ä¸¥é‡æ€§èƒ½å´©æºƒé—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶é€šè¿‡åˆ†æè¿‘æœŸåŸºå‡†æµ‹è¯•å’Œæ–¹æ³•ï¼Œè°ƒæŸ¥äº†æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨é€»è¾‘ç»„åˆæ–¹é¢çš„å¤±è´¥åŸå› ã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨ä¸‰ä¸ªå…³é”®å› ç´ ï¼šè®­ç»ƒæ•°æ®ä¸­æ˜ç¡®å¦å®šçš„å‡ ä¹å®Œå…¨ç¼ºå¤±ã€è¿ç»­æ³¨æ„åŠ›æ¶æ„å¯¹ç¦»æ•£é€»è¾‘çš„æ ¹æœ¬ä¸é€‚ç”¨æ€§ï¼Œä»¥åŠè¯„ä¼°æŒ‡æ ‡åå‘è§†è§‰åˆç†æ€§è€Œéçº¦æŸæ»¡è¶³çš„é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åˆ†ææ˜¾ç¤ºæ¨¡å‹åœ¨å•ä¸ªåŸè¯­ä¸Šå‡†ç¡®ï¼Œä½†åœ¨ç»„åˆæ—¶å‡ºç°æ€¥å‰§æ€§èƒ½å´©æºƒï¼Œæš´éœ²äº†ä¸¥é‡çš„å¹²æ‰°æ•ˆåº”ã€‚ç ”ç©¶è¡¨æ˜å½“å‰è§£å†³æ–¹æ¡ˆå’Œç®€å•çš„è§„æ¨¡æ‰©å±•æ— æ³•å¼¥åˆè¿™ä¸€å·®è·ï¼Œæ¨¡å‹åœ¨å¤„ç†é€»è¾‘ç»„åˆæ—¶è¡¨ç°å‡ºç³»ç»Ÿæ€§çš„å¤±è´¥æ¨¡å¼ã€‚</p>
<p><strong>Conclusion:</strong> å®ç°çœŸæ­£çš„ç»„åˆæ€§éœ€è¦è¡¨ç¤ºå’Œæ¨ç†æ–¹é¢çš„æ ¹æœ¬æ€§è¿›æ­¥ï¼Œè€Œéå¯¹ç°æœ‰æ¶æ„çš„å¢é‡è°ƒæ•´ã€‚ç ”ç©¶ç»“è®ºæŒ‡å‡ºå½“å‰åŸºäºè¿ç»­æ³¨æ„åŠ›æ¶æ„çš„æ–¹æ³•åœ¨æœ¬è´¨ä¸Šä¸é€‚åˆå¤„ç†ç¦»æ•£é€»è¾‘é—®é¢˜ï¼Œè¿™ä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>The architectural blueprint of today's leading text-to-image models contains a fundamental flaw: an inability to handle logical composition. This survey investigates this breakdown across three core primitives-negation, counting, and spatial relations. Our analysis reveals a dramatic performance collapse: models that are accurate on single primitives fail precipitously when these are combined, exposing severe interference. We trace this failure to three key factors. First, training data show a near-total absence of explicit negations. Second, continuous attention architectures are fundamentally unsuitable for discrete logic. Third, evaluation metrics reward visual plausibility over constraint satisfaction. By analyzing recent benchmarks and methods, we show that current solutions and simple scaling cannot bridge this gap. Achieving genuine compositionality, we conclude, will require fundamental advances in representation and reasoning rather than incremental adjustments to existing architectures.</p>
<h3 id="19-affordbot-3d-fine-grained-embodied-reasoning-via-multimodal-large-language-models">[19] <a href="https://arxiv.org/abs/2511.10017">AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models</a></h3>
<p><em>Xinyi Wang, Xun Yang, Yanlong Xu, Yuchen Wu, Zhen Li, Na Zhao</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ç»†ç²’åº¦3Då…·èº«æ¨ç†æ–°ä»»åŠ¡ï¼Œå¹¶å¼€å‘äº†AffordBotæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸é“¾å¼æ€ç»´æ¨ç†ç›¸ç»“åˆï¼Œå®ç°äº†åŸºäºä»»åŠ¡æŒ‡ä»¤çš„3Dåœºæ™¯å¯æ“ä½œå…ƒç´ ç»“æ„åŒ–é¢„æµ‹ï¼Œåœ¨SceneFun3Dæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨å¯¹è±¡çº§åˆ«æ“ä½œæˆ–é›¶æ•£å¤„ç†ç»†ç²’åº¦å¯ä¾›æ€§æ¨ç†ï¼Œç¼ºä¹è¿è´¯çš„æŒ‡ä»¤é©±åŠ¨å¼æ¥åœ°ä¸æ¨ç†èƒ½åŠ›ï¼Œæ— æ³•æœ‰æ•ˆæ”¯æŒç‰©ç†ç¯å¢ƒä¸­çš„äººæœºåä½œï¼Œè¿™éœ€è¦åŒæ—¶ç†è§£æ“ä½œå¯¹è±¡ã€ç©ºé—´ä½ç½®åŠäº¤äº’æ–¹å¼ã€‚</p>
<p><strong>Method:</strong> æå‡ºAffordBotæ¡†æ¶ï¼Œé›†æˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸å®šåˆ¶åŒ–é“¾å¼æ€ç»´æ¨ç†èŒƒå¼ï¼Œé€šè¿‡æ¸²æŸ“åœºæ™¯ç¯è§†å›¾åƒå¹¶å°†3Då…ƒç´ å€™é€‰æŠ•å½±è‡³è¿™äº›è§†å›¾ï¼Œæ„å»ºä¸åœºæ™¯å‡ ä½•å¯¹é½çš„ä¸°å¯Œè§†è§‰è¡¨ç¤ºï¼Œæ¨ç†æµç¨‹åŒ…å«ä¸»åŠ¨æ„ŸçŸ¥é˜¶æ®µé€‰æ‹©æœ€ä¼˜è§†è§’ï¼Œç„¶åé€æ­¥æ¨ç†å®šä½å¯ä¾›æ€§å…ƒç´ å¹¶æ¨æ–­åˆç†äº¤äº’è¿åŠ¨ã€‚</p>
<p><strong>Result:</strong> åœ¨SceneFun3Dæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒAffordBotå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»…ä½¿ç”¨3Dç‚¹äº‘è¾“å…¥å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å°±å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç‰©ç†æ¥åœ°æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸ç»“æ„åŒ–æ¨ç†èŒƒå¼åœ¨ç»†ç²’åº¦3Då…·èº«æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç‰©ç†ç¯å¢ƒä¸­çš„äººæœºåä½œæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†ä»…å‡­3Dç‚¹äº‘è¾“å…¥å®ç°å¤æ‚æ¨ç†ä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.</p>
<h3 id="20-gea-generation-enhanced-alignment-for-text-to-image-person-retrieval">[20] <a href="https://arxiv.org/abs/2511.10154">GEA: Generation-Enhanced Alignment for Text-to-Image Person Retrieval</a></h3>
<p><em>Hao Zou, Runqing Zhang, Xue Zhou, Jianxiao Zou</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆè§†è§’çš„ç”Ÿæˆå¢å¼ºå¯¹é½æ–¹æ³•GEAï¼Œé€šè¿‡æ‰©æ•£ç”Ÿæˆå›¾åƒä½œä¸ºä¸­é—´è¯­ä¹‰è¡¨ç¤ºæ¥å¼¥åˆæ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„æ¨¡æ€é¸¿æ²Ÿï¼Œæ˜¾è‘—æå‡äº†æ–‡æœ¬åˆ°å›¾åƒè¡Œäººæ£€ç´¢çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ–‡æœ¬åˆ°å›¾åƒè¡Œäººæ£€ç´¢ä¸­ï¼Œæ–‡æœ¬æŸ¥è¯¢å¾€å¾€æ— æ³•å‡†ç¡®å…¨é¢åœ°åæ˜ å›¾åƒå†…å®¹ï¼Œå¯¼è‡´è·¨æ¨¡æ€å¯¹é½æ•ˆæœä¸ä½³å’Œæ•°æ®é›†è¿‡æ‹Ÿåˆé—®é¢˜ï¼ŒåŒæ—¶æ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„å›ºæœ‰æ¨¡æ€å·®å¼‚è¿›ä¸€æ­¥åŠ å‰§äº†è¿™äº›æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> GEAåŒ…å«ä¸¤ä¸ªå¹¶è¡Œæ¨¡å—ï¼šæ–‡æœ¬å¼•å¯¼çš„ä»¤ç‰Œå¢å¼ºé€šè¿‡æ‰©æ•£ç”Ÿæˆå›¾åƒä½œä¸ºä¸­é—´è¯­ä¹‰è¡¨ç¤ºæ¥ä¸°å¯Œæ–‡æœ¬è¯­ä¹‰å¹¶ä¿ƒè¿›è·¨æ¨¡æ€å¯¹é½ï¼›ç”Ÿæˆä¸­é—´èåˆç»“åˆç”Ÿæˆå›¾åƒã€åŸå§‹å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„äº¤å‰æ³¨æ„åŠ›ï¼Œé€šè¿‡ä¸‰å…ƒç»„å¯¹é½æŸå¤±ä¼˜åŒ–ç”Ÿæˆç»Ÿä¸€è¡¨ç¤ºã€‚</p>
<p><strong>Result:</strong> åœ¨CUHK-PEDESã€RSTPReidå’ŒICFG-PEDESä¸‰ä¸ªå…¬å¼€TIPRæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†GEAæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ä»ç”Ÿæˆè§†è§’å‡ºå‘ï¼Œåˆ©ç”¨ç”Ÿæˆå›¾åƒä½œä¸ºä¸­é—´è¯­ä¹‰æ¡¥æ¢èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£è·¨æ¨¡æ€æ£€ç´¢ä¸­çš„è¯­ä¹‰é¸¿æ²Ÿé—®é¢˜ï¼Œä¸ºæ–‡æœ¬-å›¾åƒå¯¹é½æä¾›äº†æ–°çš„è§£å†³æ€è·¯å’Œæ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Text-to-Image Person Retrieval (TIPR) aims to retrieve person images based on natural language descriptions. Although many TIPR methods have achieved promising results, sometimes textual queries cannot accurately and comprehensively reflect the content of the image, leading to poor cross-modal alignment and overfitting to limited datasets. Moreover, the inherent modality gap between text and image further amplifies these issues, making accurate cross-modal retrieval even more challenging. To address these limitations, we propose the Generation-Enhanced Alignment (GEA) from a generative perspective. GEA contains two parallel modules: (1) Text-Guided Token Enhancement (TGTE), which introduces diffusion-generated images as intermediate semantic representations to bridge the gap between text and visual patterns. These generated images enrich the semantic representation of text and facilitate cross-modal alignment. (2) Generative Intermediate Fusion (GIF), which combines cross-attention between generated images, original images, and text features to generate a unified representation optimized by triplet alignment loss. We conduct extensive experiments on three public TIPR datasets, CUHK-PEDES, RSTPReid, and ICFG-PEDES, to evaluate the performance of GEA. The results justify the effectiveness of our method. More implementation details and extended results are available at https://github.com/sugelamyd123/Sup-for-GEA.</p>
<h3 id="21-adaptive-residual-update-steering-for-low-overhead-hallucination-mitigation-in-large-vision-language-models">[21] <a href="https://arxiv.org/abs/2511.10292">Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models</a></h3>
<p><em>Zhengtao Zou, Ya Gao, Jiarui Guan, Bin Li, Pekka Marttinen</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºRUDDERæ¡†æ¶ï¼Œé€šè¿‡æ®‹å·®æ›´æ–°å¯¼å‘çš„è§£ç è°ƒæ§æ¥ç¼“è§£å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹è±¡å¹»è§‰é—®é¢˜ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­æå–è§†è§‰è¯æ®å‘é‡ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”é—¨æ§æœºåˆ¶è¿›è¡Œä»¤ç‰Œçº§æ ¡æ­£ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ™®éå­˜åœ¨å¯¹è±¡å¹»è§‰é—®é¢˜ï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¸è§†è§‰è¾“å…¥ä¸ä¸€è‡´ï¼Œä¸¥é‡å½±å“äº†æ¨¡å‹çš„å¯é æ€§ã€‚ç°æœ‰çš„æ¨ç†æ—¶å¹²é¢„æ–¹æ³•é¢ä¸´è®¡ç®—æ•ˆç‡ä¸æ€§èƒ½çš„æƒè¡¡å›°å¢ƒï¼Œè™½ç„¶å†…éƒ¨çŠ¶æ€å¼•å¯¼æˆ–è¾“å‡ºå¯¹æ•°è°ƒæ•´æ–¹æ³•æœ‰æ•ˆï¼Œä½†é€šå¸¸éœ€è¦é¢å¤–çš„å‰å‘ä¼ æ’­è®¡ç®—ï¼Œå¯¼è‡´æ˜¾è‘—çš„è®¡ç®—å¼€é”€ï¼Œé™åˆ¶äº†åœ¨å®æ—¶å»¶è¿Ÿæ•æ„Ÿåœºæ™¯ä¸­çš„å®é™…åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> RUDDERæ¡†æ¶åŸºäºä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šä¸Šä¸‹æ–‡æ¿€æ´»æ®‹å·®æ–¹å‘å‘é‡ï¼Œåœ¨å•æ¬¡æ ‡å‡†å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä»è‡ªæ³¨æ„åŠ›å±‚çš„æ®‹å·®æ›´æ–°ä¸­æå–æ¯ä¸ªæ ·æœ¬çš„è§†è§‰è¯æ®å‘é‡ï¼›ä»¥åŠè´å¶æ–¯å¯å‘çš„è‡ªé€‚åº”é—¨æ§æœºåˆ¶ï¼Œæ‰§è¡Œä»¤ç‰Œçº§æ³¨å…¥ï¼Œæ ¹æ®æ¨¡å‹åç¦»è§†è§‰ä¸Šä¸‹æ–‡ç¨‹åº¦è°ƒæ•´æ ¡æ­£ä¿¡å·çš„å¼ºåº¦ã€‚</p>
<p><strong>Result:</strong> åœ¨POPEå’ŒCHAIRç­‰å…³é”®å¹»è§‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒRUDDERåœ¨å®ç°ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æ€§èƒ½çš„åŒæ—¶ï¼Œä»…å¼•å…¥å¯å¿½ç•¥çš„è®¡ç®—å»¶è¿Ÿï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¸æ˜¾è‘—ç‰ºç‰²æ•ˆç‡çš„å‰æä¸‹æé«˜LVLMå¯é æ€§çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> RUDDERä¸ºè§£å†³LVLMå¯¹è±¡å¹»è§‰é—®é¢˜æä¾›äº†ä¸€ç§å®ç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡åˆ›æ–°çš„æ®‹å·®å‘é‡æå–å’Œè‡ªé€‚åº”é—¨æ§æœºåˆ¶ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—æå‡æ¨¡å‹å¯é æ€§ï¼Œä¸ºå®é™…éƒ¨ç½²ä¸­çš„å»¶è¿Ÿæ•æ„Ÿåº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments. In this work, we aim to address this trade-off with Residual-Update Directed DEcoding Regulation (RUDDER), a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a single, standard forward pass. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context. Extensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency.</p>
<h3 id="22-dgfusion-dual-guided-fusion-for-robust-multi-modal-3d-object-detection">[22] <a href="https://arxiv.org/abs/2511.10035">DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection</a></h3>
<p><em>Feiyang Jia, Caiyan Jia, Ailin Liu, Shaoqing Xu, Qiming Xia, Lin Liu, Lei Yang, Yan Gong, Ziying Song</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDGFusionï¼Œä¸€ç§åŸºäºåŒå¼•å¯¼èŒƒå¼çš„å¤šæ¨¡æ€3Dç‰©ä½“æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡éš¾åº¦æ„ŸçŸ¥å®ä¾‹é…å¯¹å’ŒåŒå¼•å¯¼æ¨¡å—æœ‰æ•ˆè§£å†³ç¡¬å®ä¾‹æ£€æµ‹é—®é¢˜ï¼Œåœ¨nuScenesæ•°æ®é›†ä¸Šå®ç°äº†æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰3Dç‰©ä½“æ£€æµ‹æ–¹æ³•åœ¨å¤„ç†è¿œè·ç¦»ã€å°å°ºå¯¸æˆ–è¢«é®æŒ¡ç‰©ä½“ç­‰ç¡¬å®ä¾‹æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™äº›æ£€æµ‹å¤±è´¥ç›´æ¥å½±å“è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚ç°æœ‰å¤šæ¨¡æ€æ–¹æ³•é€šå¸¸é‡‡ç”¨å•å¼•å¯¼èŒƒå¼ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘ä¸åŒæ¨¡æ€é—´ç¡¬å®ä¾‹ä¿¡æ¯å¯†åº¦çš„å·®å¼‚ã€‚</p>
<p><strong>Method:</strong> æå‡ºDGFusionæ¡†æ¶ï¼Œæ ¸å¿ƒåŒ…æ‹¬éš¾åº¦æ„ŸçŸ¥å®ä¾‹é…å¯¹å™¨ï¼ˆDIPMï¼‰å’ŒåŒå¼•å¯¼æ¨¡å—ã€‚DIPMåŸºäºéš¾åº¦è¿›è¡Œå®ä¾‹çº§ç‰¹å¾åŒ¹é…ç”Ÿæˆæ˜“å®ä¾‹å’Œç¡¬å®ä¾‹å¯¹ï¼ŒåŒå¼•å¯¼æ¨¡å—å……åˆ†åˆ©ç”¨ä¸¤ç§é…å¯¹ç±»å‹çš„ä¼˜åŠ¿å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€ç‰¹å¾èåˆã€‚</p>
<p><strong>Result:</strong> åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDGFusionç›¸æ¯”åŸºçº¿æ–¹æ³•åˆ†åˆ«å®ç°äº†+1.0% mAPã€+0.8% NDSå’Œ+1.3%å¹³å‡å¬å›ç‡çš„æå‡ã€‚å¤§é‡å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨è‡ªæˆ‘è·ç¦»ã€å°ºå¯¸ã€å¯è§æ€§å’Œå°è§„æ¨¡è®­ç»ƒåœºæ™¯ä¸‹å¯¹ç¡¬å®ä¾‹æ£€æµ‹å…·æœ‰ä¸€è‡´çš„é²æ£’æ€§å¢ç›Šã€‚</p>
<p><strong>Conclusion:</strong> åŒå¼•å¯¼èŒƒå¼èƒ½å¤Ÿæœ‰æ•ˆè§£å†³å•å¼•å¯¼èŒƒå¼çš„å±€é™æ€§ï¼Œéš¾åº¦æ„ŸçŸ¥çš„å®ä¾‹é…å¯¹ç­–ç•¥ä¸ºå¤šæ¨¡æ€ç‰¹å¾èåˆæä¾›äº†æ–°æ€è·¯ã€‚è¯¥æ–¹æ³•ä¸ºè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿä¸­çš„ç¡¬å®ä¾‹æ£€æµ‹é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>As a critical task in autonomous driving perception systems, 3D object detection is used to identify and track key objects, such as vehicles and pedestrians. However, detecting distant, small, or occluded objects (hard instances) remains a challenge, which directly compromises the safety of autonomous driving systems. We observe that existing multi-modal 3D object detection methods often follow a single-guided paradigm, failing to account for the differences in information density of hard instances between modalities. In this work, we propose DGFusion, based on the Dual-guided paradigm, which fully inherits the advantages of the Point-guide-Image paradigm and integrates the Image-guide-Point paradigm to address the limitations of the single paradigms. The core of DGFusion, the Difficulty-aware Instance Pair Matcher (DIPM), performs instance-level feature matching based on difficulty to generate easy and hard instance pairs, while the Dual-guided Modules exploit the advantages of both pair types to enable effective multi-modal feature fusion. Experimental results demonstrate that our DGFusion outperforms the baseline methods, with respective improvements of +1.0\% mAP, +0.8\% NDS, and +1.3\% average recall on nuScenes. Extensive experiments demonstrate consistent robustness gains for hard instance detection across ego-distance, size, visibility, and small-scale training scenarios.</p>
<h3 id="23-rethinking-visual-information-processing-in-multimodal-llms">[23] <a href="https://arxiv.org/abs/2511.10301">Rethinking Visual Information Processing in Multimodal LLMs</a></h3>
<p><em>Dongwan Kim, Viresh Ranjan, Takashi Nagata, Arnab Dhua, Amit Kumar K C</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>LLaViT æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹æ‰©å±•ä¸ºè§†è§‰å˜æ¢å™¨ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ä¿®æ”¹ä½¿ LLM åŒæ—¶ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œæ˜¾è‘—è¶…è¶Šäº† LLaVA åŸºçº¿æ–¹æ³•å¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡ LLaVA æ¶æ„åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶è®¾è®¡æœ¬è´¨ä¸Šéš¾ä»¥æœ‰æ•ˆæ•´åˆè§†è§‰ç‰¹å¾ï¼Œä¸»è¦æºäºæ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¹‹é—´çš„å›ºæœ‰å¤±é…é—®é¢˜ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„å……åˆ†åˆ©ç”¨èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> LLaViT é€šè¿‡ä¸‰ä¸ªå…³é”®ä¿®æ”¹å®ç° LLM ä½œä¸ºè§†è§‰ç¼–ç å™¨çš„åŠŸèƒ½ï¼šå­¦ä¹ è§†è§‰æ¨¡æ€çš„ç‹¬ç«‹ QKV æŠ•å½±ã€å¯ç”¨è§†è§‰ä»¤ç‰Œçš„åŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥åŠèåˆå…¨å±€å’Œå±€éƒ¨è§†è§‰è¡¨ç¤ºï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€å¤„ç†æ¡†æ¶ã€‚</p>
<p><strong>Result:</strong> åœ¨å¹¿æ³›çš„å—æ§å®éªŒä¸­ï¼ŒLLaViT åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ LLaVA æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†å‚æ•°æ•°é‡æ˜¯å…¶ä¸¤å€çš„æ¨¡å‹ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è§†è§‰è¯­è¨€å»ºæ¨¡ä¸­çš„å“è¶Šæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹ç ”ç©¶ç¡®ç«‹äº†å°† LLM ä½œä¸ºæ‰©å±•è§†è§‰å˜æ¢å™¨çš„æ›´æœ‰æ•ˆæ–¹æ³•ï¼Œä¸ºè§†è§‰è¯­è¨€å»ºæ¨¡æä¾›äº†æ–°çš„èŒƒå¼ï¼Œå±•ç¤ºäº†ç»Ÿä¸€æ¶æ„åœ¨å¤„ç†å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„å¤šæ¨¡æ€æ¨¡å‹è®¾è®¡æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling.</p>
<h3 id="24-fredft-frequency-domain-fusion-transformer-for-visible-infrared-object-detection">[24] <a href="https://arxiv.org/abs/2511.10046">FreDFT: Frequency Domain Fusion Transformer for Visible-Infrared Object Detection</a></h3>
<p><em>Wencong Wu, Xiuwei Zhang, Hanlin Yin, Shun Dai, Hongxi Zhang, Yanning Zhang</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢‘åŸŸèåˆå˜æ¢å™¨FreDFTï¼Œç”¨äºè§£å†³å¯è§å…‰-çº¢å¤–ç›®æ ‡æ£€æµ‹ä¸­çš„æ¨¡æ€ä¿¡æ¯ä¸å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡åœ¨é¢‘åŸŸä¸­æŒ–æ˜è·¨æ¨¡æ€äº’è¡¥ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚åœºæ™¯ä¸‹çš„æ£€æµ‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¯è§å…‰-çº¢å¤–ç›®æ ‡æ£€æµ‹æ–¹æ³•å­˜åœ¨æ¨¡æ€ä¿¡æ¯ä¸å¹³è¡¡é—®é¢˜ï¼Œå¯¼è‡´è·¨æ¨¡æ€èåˆä¸è¶³å’Œæ£€æµ‹æ€§èƒ½ä¸‹é™ï¼ŒåŒæ—¶å¤§å¤šæ•°æ–¹æ³•ä»…åœ¨ç©ºé—´åŸŸä½¿ç”¨å˜æ¢å™¨è€Œå¿½ç•¥äº†é¢‘åŸŸå˜æ¢å™¨åœ¨æŒ–æ˜äº’è¡¥ä¿¡æ¯æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Method:</strong> æå‡ºé¢‘åŸŸèåˆå˜æ¢å™¨FreDFTï¼ŒåŒ…å«å¤šæ¨¡æ€é¢‘åŸŸæ³¨æ„åŠ›æœºåˆ¶MFDAæŒ–æ˜æ¨¡æ€é—´äº’è¡¥ä¿¡æ¯ï¼Œé¢‘åŸŸå‰é¦ˆå±‚FDFFLé€šè¿‡æ··åˆå°ºåº¦é¢‘åŸŸç‰¹å¾èåˆç­–ç•¥å¢å¼ºå¤šæ¨¡æ€ç‰¹å¾ï¼Œè·¨æ¨¡æ€å…¨å±€å»ºæ¨¡æ¨¡å—CGMMåœ¨ç©ºé—´å’Œé€šé“ç»´åº¦è¿›è¡Œåƒç´ çº§è·¨æ¨¡æ€ç‰¹å¾äº¤äº’ï¼Œå±€éƒ¨ç‰¹å¾å¢å¼ºæ¨¡å—LFEMåˆ©ç”¨å¤šç§å·ç§¯å±‚å’Œé€šé“æ··æ´—å¼ºåŒ–å¤šæ¨¡æ€å±€éƒ¨ç‰¹å¾è¡¨ç¤ºã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„FreDFTæ–¹æ³•ç›¸æ¯”å…¶ä»–æœ€å…ˆè¿›æ–¹æ³•å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯äº†é¢‘åŸŸèåˆç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> é¢‘åŸŸå˜æ¢å™¨åœ¨å¯è§å…‰-çº¢å¤–ç›®æ ‡æ£€æµ‹ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³æ¨¡æ€ä¿¡æ¯ä¸å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡é¢‘åŸŸæ³¨æ„åŠ›æœºåˆ¶å’Œæ··åˆå°ºåº¦èåˆç­–ç•¥å¯ä»¥æ›´å¥½åœ°æŒ–æ˜è·¨æ¨¡æ€äº’è¡¥ä¿¡æ¯ï¼Œä¸ºå¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Visible-infrared object detection has gained sufficient attention due to its detection performance in low light, fog, and rain conditions. However, visible and infrared modalities captured by different sensors exist the information imbalance problem in complex scenarios, which can cause inadequate cross-modal fusion, resulting in degraded detection performance. \textcolor{red}{Furthermore, most existing methods use transformers in the spatial domain to capture complementary features, ignoring the advantages of developing frequency domain transformers to mine complementary information.} To solve these weaknesses, we propose a frequency domain fusion transformer, called FreDFT, for visible-infrared object detection. The proposed approach employs a novel multimodal frequency domain attention (MFDA) to mine complementary information between modalities and a frequency domain feed-forward layer (FDFFL) via a mixed-scale frequency feature fusion strategy is designed to better enhance multimodal features. To eliminate the imbalance of multimodal information, a cross-modal global modeling module (CGMM) is constructed to perform pixel-wise inter-modal feature interaction in a spatial and channel manner. Moreover, a local feature enhancement module (LFEM) is developed to strengthen multimodal local feature representation and promote multimodal feature fusion by using various convolution layers and applying a channel shuffle. Extensive experimental results have verified that our proposed FreDFT achieves excellent performance on multiple public datasets compared with other state-of-the-art methods. The code of our FreDFT is linked at https://github.com/WenCongWu/FreDFT.</p>
<h3 id="25-monkeyocr-v15-technical-report-unlocking-robust-document-parsing-for-complex-patterns">[25] <a href="https://arxiv.org/abs/2511.10390">MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns</a></h3>
<p><em>Jiarui Zhang, Yuliang Liu, Zijun Wu, Guosheng Pang, Zhili Ye, Yupei Zhong, Junteng Ma, Tao Wei, Haiyang Xu, Weikai Chen, Zeen Wang, Qiangjun Ji, Fanxi Zhou, Qi Zhang, Yuanrui Hu, Jiahao Liu, Zhang Li, Ziyang Zhang, Qiang Liu, Xiang Bai</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MonkeyOCR v1.5ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè§£ææµç¨‹å¢å¼ºæ–‡æ¡£å¸ƒå±€ç†è§£å’Œå†…å®¹è¯†åˆ«ï¼Œåœ¨å¤æ‚å¸ƒå±€æ–‡æ¡£è§£æä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°å®ä¸–ç•Œæ–‡æ¡£é€šå¸¸åŒ…å«å¤šçº§è¡¨æ ¼ã€åµŒå…¥å¼å›¾åƒæˆ–å…¬å¼ä»¥åŠè·¨é¡µç»“æ„ç­‰å¤æ‚å¸ƒå±€ï¼Œè¿™äº›å¤æ‚ç»“æ„å¯¹ç°æœ‰OCRç³»ç»Ÿæ„æˆäº†æ˜¾è‘—æŒ‘æˆ˜ï¼Œéœ€è¦å¼€å‘èƒ½å¤ŸåŒæ—¶å¤„ç†å¸ƒå±€ç†è§£å’Œå†…å®¹è¯†åˆ«çš„ç»Ÿä¸€è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨ä¸¤é˜¶æ®µè§£ææµç¨‹ï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è”åˆé¢„æµ‹æ–‡æ¡£å¸ƒå±€å’Œé˜…è¯»é¡ºåºï¼›ç¬¬äºŒé˜¶æ®µåœ¨æ£€æµ‹åŒºåŸŸå†…æ‰§è¡Œæ–‡æœ¬ã€å…¬å¼å’Œè¡¨æ ¼çš„å±€éƒ¨è¯†åˆ«ã€‚é’ˆå¯¹å¤æ‚è¡¨æ ¼ç»“æ„ï¼Œæå‡ºäº†åŸºäºè§†è§‰ä¸€è‡´æ€§çš„å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œä»¥åŠå›¾åƒè§£è€¦è¡¨æ ¼è§£æå’Œç±»å‹å¼•å¯¼è¡¨æ ¼åˆå¹¶ä¸¤ä¸ªä¸“é—¨æ¨¡å—ã€‚</p>
<p><strong>Result:</strong> åœ¨OmniDocBench v1.5ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒMonkeyOCR v1.5å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†PPOCR-VLå’ŒMinerU 2.5ï¼Œåœ¨è§†è§‰å¤æ‚æ–‡æ¡£åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†ç»Ÿä¸€è§†è§‰-è¯­è¨€æ¡†æ¶åœ¨å¤æ‚æ–‡æ¡£è§£æä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ç»“åˆå¸ƒå±€ç†è§£å’Œå†…å®¹è¯†åˆ«ï¼Œä»¥åŠä¸“é—¨é’ˆå¯¹å¤æ‚è¡¨æ ¼ç»“æ„çš„åˆ›æ–°æ–¹æ³•ï¼Œä¸ºæ–‡æ¡£æ™ºèƒ½åº”ç”¨æä¾›äº†æ›´å¯é çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.</p>
<h3 id="26-musc-v2-zero-shot-multimodal-industrial-anomaly-classification-and-segmentation-with-mutual-scoring-of-unlabeled-samples">[26] <a href="https://arxiv.org/abs/2511.10047">MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples</a></h3>
<p><em>Xurui Li, Feng Xue, Yu Zhou</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMuSc-V2æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨æ­£å¸¸å›¾åƒå—åœ¨2Då¤–è§‚å’Œ3Då½¢çŠ¶ä¸Šçš„ç›¸ä¼¼æ€§ç‰¹å¾ï¼Œæ„å»ºäº’è¯„åˆ†æœºåˆ¶å®ç°é›¶æ ·æœ¬å¼‚å¸¸åˆ†ç±»ä¸åˆ†å‰²ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ–¹æ³•å¿½ç•¥äº†ä¸€ä¸ªå…³é”®ç‰¹æ€§ï¼šå·¥ä¸šäº§å“ä¸­çš„æ­£å¸¸å›¾åƒå—é€šå¸¸åœ¨2Då¤–è§‚å’Œ3Då½¢çŠ¶ä¸Šéƒ½èƒ½æ‰¾åˆ°è®¸å¤šç›¸ä¼¼å—ï¼Œè€Œå¼‚å¸¸åˆ™ä¿æŒå¤šæ ·æ€§å’Œå­¤ç«‹æ€§ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ˜¾å¼åˆ©ç”¨è¿™ä¸€åˆ¤åˆ«æ€§ç‰¹å¾ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº’è¯„åˆ†æ¡†æ¶MuSc-V2ï¼ŒåŒ…å«è¿­ä»£ç‚¹åˆ†ç»„æ”¹è¿›3Dè¡¨ç¤ºã€å¤šåº¦ç›¸ä¼¼æ€§é‚»åŸŸèšåˆèåˆ2D/3Dç‰¹å¾ã€äº’è¯„åˆ†æœºåˆ¶è¿›è¡Œæ ·æœ¬é—´è¯„åˆ†ã€è·¨æ¨¡æ€å¼‚å¸¸å¢å¼ºæ¢å¤ç¼ºå¤±å¼‚å¸¸ï¼Œä»¥åŠçº¦æŸé‚»åŸŸé‡è¯„åˆ†æŠ‘åˆ¶è¯¯åˆ†ç±»ã€‚</p>
<p><strong>Result:</strong> åœ¨MVTec 3D-ADæ•°æ®é›†ä¸Šè·å¾—+23.7% APæå‡ï¼Œåœ¨Eyecandiesæ•°æ®é›†ä¸Šè·å¾—+19.3%æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†å…ˆå‰çš„é›¶æ ·æœ¬åŸºå‡†ï¼Œç”šè‡³ä¼˜äºå¤§å¤šæ•°å°‘æ ·æœ¬æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ¡†æ¶å±•ç¤ºäº†åˆ©ç”¨æ­£å¸¸æ ·æœ¬çš„ç›¸ä¼¼æ€§ç‰¹å¾åœ¨é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡å¤šæ¨¡æ€èåˆå’Œäº’è¯„åˆ†æœºåˆ¶å®ç°äº†é²æ£’æ€§èƒ½ï¼Œä¸ºå·¥ä¸šç¼ºé™·æ£€æµ‹æä¾›äº†çµæ´»ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a $\textbf{+23.7\%}$ AP gain on the MVTec 3D-AD dataset and a $\textbf{+19.3\%}$ boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at \href{https://github.com/HUST-SLOW/MuSc-V2}{https://github.com/HUST-SLOW/MuSc-V2}.</p>
<h3 id="27-a-style-is-worth-one-code-unlocking-code-to-style-image-generation-with-discrete-style-space">[27] <a href="https://arxiv.org/abs/2511.10555">A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space</a></h3>
<p><em>Huijie Liu, Shuhao Cui, Haoxiang Cao, Shuai Ma, Kai Wu, Guoliang Kang</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCoTyleæ–¹æ³•ï¼Œé¦–æ¬¡åœ¨å¼€æºç¤¾åŒºå®ç°ä»£ç åˆ°é£æ ¼çš„å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œä»…éœ€æ•°å€¼é£æ ¼ä»£ç å³å¯ç”Ÿæˆæ–°é¢–ä¸”ä¸€è‡´çš„è§†è§‰é£æ ¼å›¾åƒï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨é£æ ¼ä¸€è‡´æ€§å’Œåˆ›é€ æ€§æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç”Ÿæˆæ–¹æ³•ä¾èµ–å†—é•¿æ–‡æœ¬æç¤ºã€å‚è€ƒå›¾åƒæˆ–å‚æ•°é«˜æ•ˆå¾®è°ƒæ¥å¼•å¯¼é£æ ¼æ„ŸçŸ¥å›¾åƒç”Ÿæˆï¼Œä½†å­˜åœ¨é£æ ¼ä¸€è‡´æ€§å·®ã€åˆ›é€ æ€§æœ‰é™å’Œé£æ ¼è¡¨ç¤ºå¤æ‚ç­‰é—®é¢˜ã€‚å­¦æœ¯ç•Œå°šæœªå¯¹ä»…å‡­æ•°å€¼ä»£ç ç”Ÿæˆæ–°é¢–è§†è§‰é£æ ¼çš„å¼€æºæ–¹æ³•è¿›è¡Œæ¢ç´¢ï¼Œè€Œå·¥ä¸šç•Œå·²æœ‰æ‰€å°è¯•ã€‚</p>
<p><strong>Method:</strong> é¦–å…ˆä»å›¾åƒé›†åˆä¸­è®­ç»ƒç¦»æ•£é£æ ¼ç æœ¬æå–é£æ ¼åµŒå…¥ï¼Œè¿™äº›åµŒå…¥ä½œä¸ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ç”Ÿæˆé£æ ¼åŒ–å›¾åƒã€‚éšååœ¨ç¦»æ•£é£æ ¼åµŒå…¥ä¸Šè®­ç»ƒè‡ªå›å½’é£æ ¼ç”Ÿæˆå™¨ä»¥å»ºæ¨¡å…¶åˆ†å¸ƒï¼Œä»è€Œåˆæˆæ–°é¢–é£æ ¼åµŒå…¥ã€‚æ¨ç†æ—¶ï¼Œæ•°å€¼é£æ ¼ä»£ç é€šè¿‡é£æ ¼ç”Ÿæˆå™¨æ˜ å°„ä¸ºå”¯ä¸€é£æ ¼åµŒå…¥ï¼Œå¹¶å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯¹åº”é£æ ¼çš„å›¾åƒã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒéªŒè¯CoTyleèƒ½æœ‰æ•ˆå°†æ•°å€¼ä»£ç è½¬åŒ–ä¸ºé£æ ¼æ§åˆ¶å™¨ï¼Œè¯æ˜ä¸€ä¸ªé£æ ¼ç¡®å®å¯¹åº”ä¸€ä¸ªä»£ç ã€‚è¯¥æ–¹æ³•åœ¨é£æ ¼ä¸€è‡´æ€§å’Œå¤šæ ·æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿä»æœ€å°è¾“å…¥ä¸­è§£é”å¤§é‡å¯å¤ç°çš„é£æ ¼ç©ºé—´ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜æ•°å€¼é£æ ¼ä»£ç èƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶è§†è§‰é£æ ¼ç”Ÿæˆï¼Œä¸ºè‰ºæœ¯åˆ›ä½œæä¾›äº†å‰æ‰€æœªæœ‰çš„ç®€æ´æ€§å’Œå¤šæ ·æ€§ã€‚è¿™é¡¹å·¥ä½œå¡«è¡¥äº†å­¦æœ¯ç•Œåœ¨ä»£ç åˆ°é£æ ¼ç”Ÿæˆä»»åŠ¡ä¸Šçš„ç©ºç™½ï¼Œä¸ºæœªæ¥é£æ ¼å¯æ§çš„å›¾åƒç”Ÿæˆç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.</p>
<h3 id="28-image-aesthetic-reasoning-via-hcm-grpo-empowering-compact-model-for-superior-performance">[28] <a href="https://arxiv.org/abs/2511.10055">Image Aesthetic Reasoning via HCM-GRPO: Empowering Compact Model for Superior Performance</a></h3>
<p><em>Zhiyuan Hu, Zheng Sun, Yi Wei, Long Yu</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„å›¾åƒç­›é€‰è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†å’Œå¼€å‘HCM-GRPOæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒç¾å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œè¶…è¶Šäº†ç°æœ‰å¼€æºå’Œé—­æºæ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å›¾åƒç­›é€‰ç ”ç©¶ç¨€ç¼ºï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒç¾å­¦æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä¸»è¦ç”±äºç¼ºä¹ä¸“é—¨çš„æ•°æ®é›†å’Œæ¨¡å‹ç¾å­¦æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›æ•°æ®å’Œæ–¹æ³•è®ºä¸Šçš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†åŒ…å«128kæ ·æœ¬ã€640kå›¾åƒçš„ç»¼åˆå›¾åƒç­›é€‰æ•°æ®é›†ï¼Œè¯„ä¼°å¤–è§‚å˜å½¢ã€ç‰©ç†é˜´å½±ã€å¸ƒå±€ä½ç½®å’Œæ‰©å±•åˆç†æ€§å››ä¸ªç»´åº¦ï¼›æå‡ºäº†HCM-GRPOæ–¹æ³•ï¼Œå°†å›°éš¾æ¡ˆä¾‹æŒ–æ˜ç­–ç•¥å’ŒåŠ¨æ€æ¯”ä¾‹ç²¾åº¦å¥–åŠ±é›†æˆåˆ°ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ä¸­ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜å³ä½¿æ˜¯GPT4oå’ŒQwen-VL-Maxç­‰é¡¶å°–é—­æºMLLMåœ¨å›¾åƒç¾å­¦æ¨ç†ä¸Šè¡¨ç°æ¥è¿‘éšæœºçŒœæµ‹ï¼Œè€ŒHCM-GRPOæ–¹æ³•èƒ½å¤Ÿä»¥æ›´å°çš„æ¨¡å‹è¶…è¶Šå¤§è§„æ¨¡å¼€æºå’Œé¢†å…ˆé—­æºæ¨¡å‹çš„è¯„åˆ†ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ä¸“é—¨æ•°æ®é›†å’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯¹æå‡MLLMå›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ï¼Œä¸ºå›¾åƒè´¨é‡è¯„ä¼°æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†å°æ¨¡å‹é€šè¿‡ä¼˜åŒ–æ–¹æ³•å¯ä»¥è¶…è¶Šå¤§æ¨¡å‹çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>The performance of image generation has been significantly improved in recent years. However, the study of image screening is rare and its performance with Multimodal Large Language Models (MLLMs) is unsatisfactory due to the lack of data and the weak image aesthetic reasoning ability in MLLMs. In this work, we propose a complete solution to address these problems in terms of data and methodology. For data, we collect a comprehensive image screening dataset with over 128k samples, about 640k images. Each sample consists of an original image, four generated images. The dataset evaluates the image aesthetic reasoning ability under four aspects: appearance deformation, physical shadow, placement layout, and extension rationality. Regarding data annotation, we investigate multiple approaches, including purely manual, fully automated, and answer-driven annotations, to acquire high-quality chains of thought (CoT) data in the most cost-effective manner. Methodologically, we introduce a Hard Cases Mining (HCM) strategy with a Dynamic Proportional Accuracy (DPA) reward into the Group Relative Policy Optimization (GRPO) framework, called HCM-GRPO. This enhanced method demonstrates superior image aesthetic reasoning capabilities compared to the original GRPO. Our experimental results reveal that even state-of-the-art closed-source MLLMs, such as GPT4o and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic reasoning. In contrast, by leveraging the HCM-GRPO, we are able to surpass the scores of both large-scale open-source and leading closed-source models with a much smaller model.</p>
<h3 id="29-when-eyes-and-ears-disagree-can-mllms-discern-audio-visual-confusion">[29] <a href="https://arxiv.org/abs/2511.10059">When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?</a></h3>
<p><em>Qilang Ye, Wei Zeng, Meng Liu, Jie Zhang, Yupeng Hu, Zitong Yu, Yu Zhou</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†RL-CoMMæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ åä½œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¥è§£å†³MLLMsåœ¨éŸ³é¢‘-è§†è§‰æ··æ·†åœºæ™¯ä¸­çš„è§†è§‰ä¸»å¯¼æ¨ç†é—®é¢˜ï¼Œåœ¨éŸ³é¢‘-è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šæ¯”åŸºçº¿æ¨¡å‹æå‡10-30%çš„å‡†ç¡®ç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘-è§†è§‰æ··æ·†åœºæ™¯ä¸­å­˜åœ¨è§†è§‰ä¸»å¯¼æ¨ç†é—®é¢˜ï¼Œå³å½“è§†é¢‘ä¸­æŸä¸ªå¯¹è±¡è§†è§‰å­˜åœ¨ä½†éŸ³é¢‘ç¼ºå¤±æ—¶ï¼Œæ¨¡å‹éš¾ä»¥å‡†ç¡®åˆ¤æ–­ä¸å­˜åœ¨çš„éŸ³é¢‘ï¼Œè¿™é™åˆ¶äº†MLLMsåœ¨å¤æ‚å¤šæ¨¡æ€ç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„åä½œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹RL-CoMMï¼ŒåŒ…å«ä¸¤é˜¶æ®µæ–¹æ³•ï¼šé¦–å…ˆå¼•å…¥å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ä½œä¸ºå‚è€ƒæ¨¡å‹ç”Ÿæˆçº¯éŸ³é¢‘æ¨ç†ï¼Œè®¾è®¡é€æ­¥æ¨ç†å¥–åŠ±å‡½æ•°ä½¿MLLMsèƒ½å¤Ÿè‡ªæˆ‘æ”¹è¿›éŸ³é¢‘-è§†è§‰æ¨ç†ï¼›å…¶æ¬¡é‡‡ç”¨ç­”æ¡ˆä¸­å¿ƒç½®ä¿¡åº¦ä¼˜åŒ–æ¥å‡å°‘æ½œåœ¨å¼‚æ„æ¨ç†å·®å¼‚çš„ä¸ç¡®å®šæ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨éŸ³é¢‘-è§†è§‰é—®ç­”å’ŒéŸ³é¢‘-è§†è§‰å¹»è§‰ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒRL-CoMMåœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸‹æ¯”åŸºçº¿æ¨¡å‹å‡†ç¡®ç‡æå‡10-30%ï¼Œæœ‰æ•ˆè§£å†³äº†è§†è§‰ä¸»å¯¼æ¨ç†å¯¼è‡´çš„éŸ³é¢‘åˆ¤æ–­é”™è¯¯é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†MLLMsåœ¨éŸ³é¢‘-è§†è§‰æ¨ç†ä¸­çš„è§†è§‰ä¸»å¯¼åå·®é—®é¢˜ï¼Œæå‡ºçš„RL-CoMMæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå¤šæ¨¡å‹åä½œæœ‰æ•ˆç¼“è§£äº†è¿™ä¸€é—®é¢˜ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„å¹³è¡¡æ¨ç†æä¾›äº†æ–°æ€è·¯ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Can Multimodal Large Language Models (MLLMs) discern confused objects that are visually present but audio-absent? To study this, we introduce a new benchmark, AV-ConfuseBench, which simulates an ``Audio-Visual Confusion'' scene by modifying the corresponding sound of an object in the video, e.g., mute the sounding object and ask MLLMs Is there a/an muted-object sound''. Experimental results reveal that MLLMs, such as Qwen2.5-Omni and Gemini 2.5, struggle to discriminate non-existent audio due to visually dominated reasoning. Motivated by this observation, we introduce RL-CoMM, a Reinforcement Learning-based Collaborative Multi-MLLM that is built upon the Qwen2.5-Omni foundation. RL-CoMM includes two stages: 1) To alleviate visually dominated ambiguities, we introduce an external model, a Large Audio Language Model (LALM), as the reference model to generate audio-only reasoning. Then, we design a Step-wise Reasoning Reward function that enables MLLMs to self-improve audio-visual reasoning with the audio-only reference. 2) To ensure an accurate answer prediction, we introduce Answer-centered Confidence Optimization to reduce the uncertainty of potential heterogeneous reasoning differences. Extensive experiments on audio-visual question answering and audio-visual hallucination show that RL-CoMM improves the accuracy by 10~30\% over the baseline model with limited training data. Follow: https://github.com/rikeilong/AVConfusion.</p>
<h3 id="30-vlf-msc-vision-language-feature-based-multimodal-semantic-communication-system">[30] <a href="https://arxiv.org/abs/2511.10074">VLF-MSC: Vision-Language Feature-Based Multimodal Semantic Communication System</a></h3>
<p><em>Gwangyeon Ahn, Jiwan Seo, Joonhyuk Kang</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºè§†è§‰è¯­è¨€ç‰¹å¾çš„å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ç³»ç»ŸVLF-MSCï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ä¼ è¾“å•ä¸€ç´§å‡‘çš„è§†è§‰è¯­è¨€è¡¨ç¤ºæ¥åŒæ—¶æ”¯æŒæ¥æ”¶ç«¯çš„å›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆï¼Œæ˜¾è‘—æé«˜äº†é¢‘è°±æ•ˆç‡å¹¶å¢å¼ºäº†è¯­ä¹‰ä¿çœŸåº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è¯­ä¹‰é€šä¿¡æŠ€æœ¯é€šå¸¸ç‹¬ç«‹å¤„ç†æ¯ç§æ¨¡æ€ï¼Œå¯¼è‡´éœ€è¦ç‰¹å®šæ¨¡æ€çš„ä¼ è¾“æµæˆ–é‡ä¼ ï¼Œé™ä½äº†é¢‘è°±æ•ˆç‡ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€é€šä¿¡ä¸­èµ„æºåˆ©ç”¨ç‡ä½å’Œè¯­ä¹‰ä¿çœŸåº¦ä¸è¶³çš„é—®é¢˜ï¼Œé€šè¿‡ç»Ÿä¸€çš„è§†è§‰è¯­è¨€è¡¨ç¤ºæ¥åŒæ—¶æ”¯æŒå›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆã€‚</p>
<p><strong>Method:</strong> VLF-MSCé‡‡ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å°†æºå›¾åƒç¼–ç ä¸ºè§†è§‰è¯­è¨€è¯­ä¹‰ç‰¹å¾ï¼Œé€šè¿‡æ— çº¿ä¿¡é“ä¼ è¾“è¯¥ç‰¹å¾ã€‚åœ¨æ¥æ”¶ç«¯ï¼ŒåŸºäºè§£ç å™¨çš„è¯­è¨€æ¨¡å‹å’ŒåŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆå™¨éƒ½ä»¥æ­¤ç‰¹å¾ä¸ºæ¡ä»¶ï¼Œåˆ†åˆ«ç”Ÿæˆæè¿°æ€§æ–‡æœ¬å’Œè¯­ä¹‰å¯¹é½çš„å›¾åƒã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜VLF-MSCåœ¨ä½ä¿¡å™ªæ¯”æ¡ä»¶ä¸‹ä¼˜äºä»…æ–‡æœ¬å’Œä»…å›¾åƒçš„åŸºçº¿æ–¹æ³•ï¼Œåœ¨ä¸¤ç§æ¨¡æ€ä¸Šéƒ½å®ç°äº†æ›´é«˜çš„è¯­ä¹‰å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å¸¦å®½éœ€æ±‚ã€‚ç³»ç»Ÿå±•ç°å‡ºå¯¹ä¿¡é“å™ªå£°çš„é²æ£’æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è¯­ä¹‰ä¿çœŸåº¦ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»Ÿä¸€è§†è§‰è¯­è¨€è¡¨ç¤ºåœ¨å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„é«˜æ•ˆå¤šæ¨¡æ€é€šä¿¡æä¾›äº†æ–°èŒƒå¼ã€‚åŸºäºåŸºç¡€æ¨¡å‹çš„æ–¹æ³•ä¸ºå®ç°é²æ£’ä¸”é«˜æ•ˆçš„è¯­ä¹‰é€šä¿¡å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>We propose Vision-Language Feature-based Multimodal Semantic Communication (VLF-MSC), a unified system that transmits a single compact vision-language representation to support both image and text generation at the receiver. Unlike existing semantic communication techniques that process each modality separately, VLF-MSC employs a pre-trained vision-language model (VLM) to encode the source image into a vision-language semantic feature (VLF), which is transmitted over the wireless channel. At the receiver, a decoder-based language model and a diffusion-based image generator are both conditioned on the VLF to produce a descriptive text and a semantically aligned image. This unified representation eliminates the need for modality-specific streams or retransmissions, improving spectral efficiency and adaptability. By leveraging foundation models, the system achieves robustness to channel noise while preserving semantic fidelity. Experiments demonstrate that VLF-MSC outperforms text-only and image-only baselines, achieving higher semantic accuracy for both modalities under low SNR with significantly reduced bandwidth.</p>
<h3 id="31-gridprune-from-where-to-look-to-what-to-select-in-visual-token-pruning-for-mllms">[31] <a href="https://arxiv.org/abs/2511.10081">GridPrune: From "Where to Look" to "What to Select" in Visual Token Pruning for MLLMs</a></h3>
<p><em>Yuxiang Duan, Ao Li, Yingqin Li, Luyu Li, Pengwei Wang</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGridPruneæ–¹æ³•ï¼Œé€šè¿‡'å…¨å±€å¼•å¯¼ã€å±€éƒ¨é€‰æ‹©'çš„åŒºåŸŸé€‰æ‹©ç³»ç»Ÿè§£å†³MLLMsä¸­è§†è§‰ä»¤ç‰Œå‰ªæçš„ç©ºé—´åˆ†é…æ•ˆç‡é—®é¢˜ï¼Œåœ¨LLaVA-NeXT-7Bä¸Šä»…ä½¿ç”¨11.1%çš„ä»¤ç‰Œå³å¯ä¿æŒ96.98%çš„å®Œæ•´æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰MLLMsè§†è§‰ä»¤ç‰Œå‰ªææ–¹æ³•ä¸»è¦å…³æ³¨'é€‰æ‹©ä»€ä¹ˆ'çš„ç›´æ¥ä¼˜åŒ–ï¼Œè€Œå¿½ç•¥äº†è®¤çŸ¥ç§‘å­¦ä¸­'çœ‹å‘ä½•å¤„'çš„å…³é”®ç­–ç•¥ï¼Œå¯¼è‡´ç©ºé—´åˆ†é…æ•ˆç‡ä½ä¸‹ã€ä½ç½®åå·®ä»¥åŠä¿ç•™æ— å…³æˆ–å†—ä½™ä»¤ç‰Œç­‰é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> GridPruneé‡‡ç”¨ä¸¤é˜¶æ®µå‰ªæç­–ç•¥ï¼šé¦–å…ˆä½¿ç”¨æ–‡æœ¬æ¡ä»¶å¼•å¯¼åŠ¨æ€åˆ†é…è·¨ç©ºé—´åŒºåŸŸçš„ä»¤ç‰Œé¢„ç®—ï¼Œç„¶ååœ¨æ¯ä¸ªé¢„ç®—åŒºåŸŸå†…æ‰§è¡Œå±€éƒ¨é€‰æ‹©ï¼Œæ›¿ä»£å…¨å±€Top-Kæœºåˆ¶ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜GridPruneåœ¨å„ç§MLLMæ¶æ„ä¸Šå‡å®ç°ä¼˜è¶Šæ€§èƒ½ï¼Œåœ¨LLaVA-NeXT-7Bä¸Šä»¥ç›¸åŒå‰ªæç‡ä¼˜äºæœ€ä½³åŸºçº¿æ–¹æ³•2.34%ï¼Œä»…ä½¿ç”¨11.1%ä»¤ç‰Œå³å¯ä¿æŒ96.98%å®Œæ•´æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ¨¡æ‹Ÿäººç±»è§†è§‰æ³¨æ„æœºåˆ¶çš„ä¸¤é˜¶æ®µå‰ªæç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä¸ºMLLMsæ•ˆç‡ä¼˜åŒ–æä¾›äº†æ–°æ€è·¯ï¼Œå³é€šè¿‡ç©ºé—´é¢„ç®—åˆ†é…å’Œå±€éƒ¨é€‰æ‹©ç›¸ç»“åˆçš„æ–¹å¼å®ç°æ›´æ™ºèƒ½çš„ä»¤ç‰Œå‰ªæã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) have shown remarkable capabilities in a wide range of vision-language tasks. However, the large number of visual tokens introduces significant computational overhead. To address this issue, visual token pruning has emerged as a key technique for enhancing the efficiency of MLLMs. In cognitive science, humans tend to first determine which regions of a scene to attend to ("where to look") before deciding which specific elements within those regions to process in detail ("what to select"). This two-stage strategy enables the visual system to efficiently allocate attention at a coarse spatial level before performing fine-grained selection. However, existing pruning methods primarily focus on directly optimizing "what to select", typically using attention scores or similarity metrics. They rarely consider "where to look", which has been shown to lead to inefficient spatial allocation, positional bias, and the retention of irrelevant or redundant tokens. In this paper, we propose GridPrune, a method that replaces the global Top-K mechanism with a "guide-globally, select-locally" zonal selection system. GridPrune splits the pruning process into two steps: first, it uses text-conditional guidance to dynamically allocate a token budget across spatial zones; and then, it performs local selection within each budgeted zone. Experimental results demonstrate that GridPrune achieves superior performance across various MLLM architectures. On LLaVA-NeXT-7B, GridPrune retains 96.98% of the full performance while using 11.1% of the tokens, outperforming the best-performing baseline by 2.34% at the same pruning rate.</p>
<h3 id="32-sugar-learning-skeleton-representation-with-visual-motion-knowledge-for-action-recognition">[32] <a href="https://arxiv.org/abs/2511.10091">SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition</a></h3>
<p><em>Qilang Ye, Yu Zhou, Lian He, Jie Zhang, Xuanming Guo, Jiayu Zhang, Mingkui Tan, Weicheng Xie, Yue Sun, Tao Tan, Xiaochen Yuan, Ghada Khoriba, Zitong Yu</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSUGARæ¡†æ¶ï¼Œé€šè¿‡å°†å¤§è§„æ¨¡è§†é¢‘æ¨¡å‹ä½œä¸ºçŸ¥è¯†åº“ç”Ÿæˆè§†è§‰è¿åŠ¨å…ˆéªŒï¼Œç›‘ç£éª¨æ¶å­¦ä¹ è·å¾—ç¦»æ•£è¡¨ç¤ºï¼Œä½¿æœªç»å¾®è°ƒçš„LLMèƒ½å¤Ÿç†è§£éª¨æ¶åºåˆ—å¹¶æ‰§è¡ŒåŠ¨ä½œåˆ†ç±»ä¸æè¿°ä»»åŠ¡ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç ”ç©¶é¢ä¸´ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šLLMå¦‚ä½•ç†è§£äººä½“éª¨æ¶æ•°æ®ï¼Œä»¥åŠå¦‚ä½•åŒºåˆ†ä¸åŒåŠ¨ä½œç±»åˆ«ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†éª¨æ¶åºåˆ—æ—¶å­˜åœ¨è¡¨ç¤ºå­¦ä¹ ä¸è¶³å’ŒLLMéª¨æ¶ç†è§£èƒ½åŠ›æœ‰é™çš„é—®é¢˜ï¼Œéœ€è¦æ¢ç´¢æœ‰æ•ˆçš„éª¨æ¶è¡¨ç¤ºå­¦ä¹ èŒƒå¼ã€‚</p>
<p><strong>Method:</strong> æå‡ºSUGARæ¡†æ¶ï¼Œé¦–å…ˆåˆ©ç”¨ç°æˆçš„å¤§è§„æ¨¡è§†é¢‘æ¨¡å‹ä½œä¸ºçŸ¥è¯†åº“ç”Ÿæˆè§†è§‰å’Œè¿åŠ¨ç›¸å…³å…ˆéªŒä¿¡æ¯ï¼›ç„¶åé€šè¿‡è¯¥å…ˆéªŒçŸ¥è¯†ç›‘ç£éª¨æ¶å­¦ä¹ è·å¾—ç¦»æ•£è¡¨ç¤ºï¼›æœ€åä½¿ç”¨æœªç»å¾®è°ƒçš„é¢„è®­ç»ƒLLMç†è§£è¿™äº›è¡¨ç¤ºå¹¶ç”ŸæˆåŠ¨ä½œç›®æ ‡å’Œæè¿°ã€‚ç‰¹åˆ«è®¾è®¡äº†æ—¶åºæŸ¥è¯¢æŠ•å½±æ¨¡å—æ¥è¿ç»­å»ºæ¨¡é•¿åºåˆ—éª¨æ¶ä¿¡å·ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºäºéª¨æ¶çš„åŠ¨ä½œåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†SUGARçš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¯”åŸºäºçº¿æ€§æ–¹æ³•å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚</p>
<p><strong>Conclusion:</strong> SUGARæ¡†æ¶æˆåŠŸåœ°å°†LLMçš„éšå«çŸ¥è¯†ä¸éª¨æ¶è¡¨ç¤ºå­¦ä¹ ç›¸ç»“åˆï¼Œä¸ºéª¨æ¶åŠ¨ä½œè¯†åˆ«æä¾›äº†æ–°çš„èŒƒå¼ï¼Œè¯æ˜äº†åˆ©ç”¨è§†è§‰è¿åŠ¨å…ˆéªŒçŸ¥è¯†ç›‘ç£éª¨æ¶å­¦ä¹ èƒ½å¤Ÿæœ‰æ•ˆæå‡LLMåœ¨åŠ¨ä½œç†è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹å±•ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Large Language Models (LLMs) hold rich implicit knowledge and powerful transferability. In this paper, we explore the combination of LLMs with the human skeleton to perform action classification and description. However, when treating LLM as a recognizer, two questions arise: 1) How can LLMs understand skeleton? 2) How can LLMs distinguish among actions? To address these problems, we introduce a novel paradigm named learning Skeleton representation with visUal-motion knowledGe for Action Recognition (SUGAR). In our pipeline, we first utilize off-the-shelf large-scale video models as a knowledge base to generate visual, motion information related to actions. Then, we propose to supervise skeleton learning through this prior knowledge to yield discrete representations. Finally, we use the LLM with untouched pre-training weights to understand these representations and generate the desired action targets and descriptions. Notably, we present a Temporal Query Projection (TQP) module to continuously model the skeleton signals with long sequences. Experiments on several skeleton-based action classification benchmarks demonstrate the efficacy of our SUGAR. Moreover, experiments on zero-shot scenarios show that SUGAR is more versatile than linear-based methods.</p>
<h3 id="33-mtattack-multi-target-backdoor-attacks-against-large-vision-language-models">[33] <a href="https://arxiv.org/abs/2511.10098">MTAttack: Multi-Target Backdoor Attacks against Large Vision-Language Models</a></h3>
<p><em>Zihan Wang, Guansong Pang, Wenjun Miao, Jin Zheng, Xiao Bai</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MTAttackï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šç›®æ ‡åé—¨æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡è”åˆä¼˜åŒ–å¤šä¸ªè§¦å‘å™¨å¹¶ç¡®ä¿å…¶å¯åˆ†ç¦»æ€§ï¼Œå®ç°äº†é«˜æˆåŠŸç‡çš„æ”»å‡»ï¼Œæ­ç¤ºäº†LVLMsåœ¨å¤šç›®æ ‡åé—¨æ”»å‡»ä¸‹çš„ä¸¥é‡å®‰å…¨æ¼æ´ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åé—¨æ”»å‡»ä¸»è¦å…³æ³¨å•ç›®æ ‡æ”»å‡»ï¼Œå³é’ˆå¯¹ç‰¹å®šè§¦å‘å™¨çš„å•ä¸€æ¶æ„è¾“å‡ºï¼Œè€Œç°å®åº”ç”¨ä¸­å¤šç›®æ ‡åé—¨æ”»å‡»æ„æˆæ›´å¤§å¨èƒï¼Œä½†æ‰§è¡Œæ­¤ç±»æ”»å‡»åœ¨LVLMsä¸­å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºä¸åŒè§¦å‘å™¨ä¹‹é—´çš„ç‰¹å¾å¹²æ‰°ä¼šå¯¼è‡´å¤§é‡é”™è¯¯çš„è§¦å‘å™¨-ç›®æ ‡æ˜ å°„ã€‚</p>
<p><strong>Method:</strong> MTAttackæ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…å«ä»£ç†ç©ºé—´åˆ’åˆ†çº¦æŸå’Œè§¦å‘å™¨åŸå‹é”šå®šçº¦æŸä¸¤ä¸ªå…³é”®çº¦æŸæ¡ä»¶ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­è”åˆä¼˜åŒ–å¤šä¸ªè§¦å‘å™¨ï¼Œä½¿æ¯ä¸ªè§¦å‘å™¨ç‹¬ç«‹åœ°å°†å¹²å‡€å›¾åƒæ˜ å°„åˆ°å”¯ä¸€çš„ä»£ç†ç±»åˆ«ï¼ŒåŒæ—¶ä¿è¯å®ƒä»¬çš„å¯åˆ†ç¦»æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMTAttackåœ¨å¤šç›®æ ‡æ”»å‡»ä¸­å®ç°äº†é«˜æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ”»å‡»æ–¹æ³•ï¼Œå¹¶ä¸”è¯¥æ”»å‡»åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯¹åé—¨é˜²å¾¡ç­–ç•¥å…·æœ‰é²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¿™äº›å‘ç°å‡¸æ˜¾äº†LVLMsåœ¨å¤šç›®æ ‡åé—¨æ”»å‡»ä¸‹çš„è„†å¼±æ€§ï¼Œå¼ºè°ƒäº†ç¼“è§£æ­¤ç±»å¨èƒçš„ç´§è¿«éœ€æ±‚ï¼Œä¸ºLVLMsçš„å®‰å…¨é˜²æŠ¤ç ”ç©¶æä¾›äº†é‡è¦å¯ç¤ºï¼Œå¹¶æ¨åŠ¨äº†æ›´å¥å£®é˜²å¾¡æœºåˆ¶çš„å‘å±•æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Recent advances in Large Visual Language Models (LVLMs) have demonstrated impressive performance across various vision-language tasks by leveraging large-scale image-text pretraining and instruction tuning. However, the security vulnerabilities of LVLMs have become increasingly concerning, particularly their susceptibility to backdoor attacks. Existing backdoor attacks focus on single-target attacks, i.e., targeting a single malicious output associated with a specific trigger. In this work, we uncover multi-target backdoor attacks, where multiple independent triggers corresponding to different attack targets are added in a single pass of training, posing a greater threat to LVLMs in real-world applications. Executing such attacks in LVLMs is challenging since there can be many incorrect trigger-target mappings due to severe feature interference among different triggers. To address this challenge, we propose MTAttack, the first multi-target backdoor attack framework for enforcing accurate multiple trigger-target mappings in LVLMs. The core of MTAttack is a novel optimization method with two constraints, namely Proxy Space Partitioning constraint and Trigger Prototype Anchoring constraint. It jointly optimizes multiple triggers in the latent space, with each trigger independently mapping clean images to a unique proxy class while at the same time guaranteeing their separability. Experiments on popular benchmarks demonstrate a high success rate of MTAttack for multi-target attacks, substantially outperforming existing attack methods. Furthermore, our attack exhibits strong generalizability across datasets and robustness against backdoor defense strategies. These findings highlight the vulnerability of LVLMs to multi-target backdoor attacks and underscore the urgent need for mitigating such threats. Code is available at https://github.com/mala-lab/MTAttack.</p>
<h3 id="34-explicit-temporal-semantic-modeling-for-dense-video-captioning-via-context-aware-cross-modal-interaction">[34] <a href="https://arxiv.org/abs/2511.10134">Explicit Temporal-Semantic Modeling for Dense Video Captioning via Context-Aware Cross-Modal Interaction</a></h3>
<p><em>Mingda Jia, Weiliang Meng, Zenghuang Fu, Yiheng Li, Qi Zeng, Yifan Zhang, Ju Xin, Rongtao Xu, Jiguang Zhang, Xiaopeng Zhang</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ˜¾å¼æ—¶åºè¯­ä¹‰å»ºæ¨¡æ¡†æ¶CACMIï¼Œé€šè¿‡è·¨æ¨¡æ€å¸§èšåˆå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰¹å¾å¢å¼ºï¼Œåœ¨å¯†é›†è§†é¢‘æè¿°ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆæ•æ‰äº†è§†é¢‘äº‹ä»¶åºåˆ—çš„æ—¶åºè¿è´¯æ€§å’Œè§†è§‰ä¸Šä¸‹æ–‡çš„å…¨é¢è¯­ä¹‰ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¯†é›†è§†é¢‘æè¿°æ–¹æ³•ä¸»è¦ä¾èµ–éšå¼å»ºæ¨¡ï¼Œä½¿ç”¨å¸§çº§æˆ–ç¢ç‰‡åŒ–è§†é¢‘ç‰¹å¾ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰äº‹ä»¶åºåˆ—é—´çš„æ—¶åºè¿è´¯æ€§å’Œè§†è§‰ä¸Šä¸‹æ–‡ä¸­çš„å…¨é¢è¯­ä¹‰ã€‚è¿™ç§å±€é™æ€§é™åˆ¶äº†æ¨¡å‹å¯¹è§†é¢‘å†…å®¹çš„ç†è§£å’Œæè¿°èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„CACMIæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè·¨æ¨¡æ€å¸§èšåˆé€šè¿‡è·¨æ¨¡æ€æ£€ç´¢èšåˆç›¸å…³å¸§ï¼Œæå–æ—¶åºè¿è´¯çš„äº‹ä»¶å¯¹é½æ–‡æœ¬ç‰¹å¾ï¼›ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰¹å¾å¢å¼ºåˆ©ç”¨æŸ¥è¯¢å¼•å¯¼æ³¨æ„åŠ›å°†è§†è§‰åŠ¨æ€ä¸ä¼ªäº‹ä»¶è¯­ä¹‰è¿›è¡Œæ•´åˆã€‚</p>
<p><strong>Result:</strong> åœ¨ActivityNet Captionså’ŒYouCook2æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCACMIåœ¨å¯†é›†è§†é¢‘æè¿°ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼ŒéªŒè¯äº†æ˜¾å¼æ—¶åºè¯­ä¹‰å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ˜¾å¼å»ºæ¨¡è§†é¢‘æ—¶åºç‰¹æ€§å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡çš„é‡è¦æ€§ï¼Œä¸ºå¯†é›†è§†é¢‘æè¿°ä»»åŠ¡æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œå¼ºè°ƒäº†è·¨æ¨¡æ€äº¤äº’åœ¨ç†è§£å¤æ‚è§†é¢‘å†…å®¹ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>Dense video captioning jointly localizes and captions salient events in untrimmed videos. Recent methods primarily focus on leveraging additional prior knowledge and advanced multi-task architectures to achieve competitive performance. However, these pipelines rely on implicit modeling that uses frame-level or fragmented video features, failing to capture the temporal coherence across event sequences and comprehensive semantics within visual contexts. To address this, we propose an explicit temporal-semantic modeling framework called Context-Aware Cross-Modal Interaction (CACMI), which leverages both latent temporal characteristics within videos and linguistic semantics from text corpus. Specifically, our model consists of two core components: Cross-modal Frame Aggregation aggregates relevant frames to extract temporally coherent, event-aligned textual features through cross-modal retrieval; and Context-aware Feature Enhancement utilizes query-guided attention to integrate visual dynamics with pseudo-event semantics. Extensive experiments on the ActivityNet Captions and YouCook2 datasets demonstrate that CACMI achieves the state-of-the-art performance on dense video captioning task.</p>
<h3 id="35-heatv2x-scalable-heterogeneous-collaborative-perception-via-efficient-alignment-and-interaction">[35] <a href="https://arxiv.org/abs/2511.10211">HeatV2X: Scalable Heterogeneous Collaborative Perception via Efficient Alignment and Interaction</a></h3>
<p><em>Yueran Zhao, Zhang Zhang, Chao Sun, Tianze Wang, Chao Yue, Nuoran Li</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºHeatV2Xå¼‚æ„é€‚åº”æ¡†æ¶ï¼Œé€šè¿‡å±€éƒ¨å¼‚æ„å¾®è°ƒå’Œå…¨å±€ååŒå¾®è°ƒè§£å†³V2XååŒæ„ŸçŸ¥ä¸­çš„å¼‚æ„æ€§å’Œå¯æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œåœ¨æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬çš„åŒæ—¶å®ç°ä¼˜è¶Šçš„æ„ŸçŸ¥æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰V2XååŒæ„ŸçŸ¥æ¡†æ¶é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šå‚ä¸æ™ºèƒ½ä½“æœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€å’Œå¼‚æ„çš„ï¼Œè¿™éœ€è¦æœ‰æ•ˆçš„è·¨æ™ºèƒ½ä½“ç‰¹å¾å¯¹é½æ¥å‡è½»å¼‚æ„æ€§æŸå¤±ï¼›åŒæ—¶åä½œæ¡†æ¶å¿…é¡»èƒ½å¤Ÿæ‰©å±•ä»¥é€‚åº”æ–°æ™ºèƒ½ä½“ï¼Œè¿™ä½¿å¾—å…¨å‚æ•°è®­ç»ƒä¸åˆ‡å®é™…ï¼Œå‡¸æ˜¾äº†å¯æ‰©å±•é€‚åº”çš„é‡è¦æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºHeatV2Xå¯æ‰©å±•åä½œæ¡†æ¶ï¼Œé¦–å…ˆåŸºäºå¼‚æ„å›¾æ³¨æ„åŠ›è®­ç»ƒé«˜æ€§èƒ½æ™ºèƒ½ä½“ä½œä¸ºåä½œå­¦ä¹ åŸºç¡€ï¼Œç„¶åè®¾è®¡å±€éƒ¨å¼‚æ„å¾®è°ƒå’Œå…¨å±€åä½œå¾®è°ƒæ¥å®ç°å¼‚æ„æ™ºèƒ½ä½“é—´çš„æœ‰æ•ˆå¯¹é½å’Œäº¤äº’ã€‚å±€éƒ¨å¾®è°ƒä½¿ç”¨å¼‚æ„æ„ŸçŸ¥é€‚é…å™¨é«˜æ•ˆæå–æ¨¡æ€ç‰¹å®šå·®å¼‚ï¼Œå…¨å±€å¾®è°ƒé‡‡ç”¨å¤šè®¤çŸ¥é€‚é…å™¨å¢å¼ºè·¨æ™ºèƒ½ä½“åä½œå¹¶å……åˆ†æŒ–æ˜èåˆæ½œåŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨OPV2V-Hå’ŒDAIR-V2Xæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä»¥æ˜¾è‘—é™ä½çš„è®­ç»ƒå¼€é”€å®ç°äº†ä¼˜è¶Šçš„æ„ŸçŸ¥æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†æ¡†æ¶åœ¨æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„é€‚é…å™¨æœºåˆ¶å’Œåˆ†å±‚å¾®è°ƒç­–ç•¥ï¼Œå¯ä»¥åœ¨æœ€å°åŒ–è®­ç»ƒæˆæœ¬çš„åŒæ—¶æ˜¾è‘—æå‡å¼‚æ„V2XååŒæ„ŸçŸ¥ç³»ç»Ÿçš„æ€§èƒ½ï¼Œä¸ºå¤§è§„æ¨¡å®é™…éƒ¨ç½²æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºæœªæ¥å¼‚æ„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯æ‰©å±•åä½œå­¦ä¹ æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Vehicle-to-Everything (V2X) collaborative perception extends sensing beyond single vehicle limits through transmission. However, as more agents participate, existing frameworks face two key challenges: (1) the participating agents are inherently multi-modal and heterogeneous, and (2) the collaborative framework must be scalable to accommodate new agents. The former requires effective cross-agent feature alignment to mitigate heterogeneity loss, while the latter renders full-parameter training impractical, highlighting the importance of scalable adaptation. To address these issues, we propose Heterogeneous Adaptation (HeatV2X), a scalable collaborative framework. We first train a high-performance agent based on heterogeneous graph attention as the foundation for collaborative learning. Then, we design Local Heterogeneous Fine-Tuning and Global Collaborative Fine-Tuning to achieve effective alignment and interaction among heterogeneous agents. The former efficiently extracts modality-specific differences using Hetero-Aware Adapters, while the latter employs the Multi-Cognitive Adapter to enhance cross-agent collaboration and fully exploit the fusion potential. These designs enable substantial performance improvement of the collaborative framework with minimal training cost. We evaluate our approach on the OPV2V-H and DAIR-V2X datasets. Experimental results demonstrate that our method achieves superior perception performance with significantly reduced training overhead, outperforming existing state-of-the-art approaches. Our implementation will be released soon.</p>
<h3 id="36-next-frame-feature-prediction-for-multimodal-deepfake-detection-and-temporal-localization">[36] <a href="https://arxiv.org/abs/2511.10212">Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization</a></h3>
<p><em>Ashutosh Anshul, Shreyas Gopal, Deepu Rajan, Eng Siong Chng</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å•é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå•æ¨¡æ€å’Œè·¨æ¨¡æ€ç‰¹å¾çš„ä¸‹ä¸€å¸§é¢„æµ‹æ¥å¢å¼ºæ·±åº¦ä¼ªé€ æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¼•å…¥çª—å£çº§æ³¨æ„åŠ›æœºåˆ¶æ¥æ•è·é¢„æµ‹å¸§ä¸å®é™…å¸§ä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œå®ç°å¯¹å®Œå…¨æ“çºµè§†é¢‘çš„å‡†ç¡®åˆ†ç±»å’Œéƒ¨åˆ†ä¼ªé€ æ ·æœ¬çš„ç²¾ç¡®æ—¶é—´å®šä½ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é¢å‘æ³›åŒ–çš„å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•éœ€è¦çœŸå®æ ·æœ¬çš„é¢„è®­ç»ƒï¼Œä¸”ä¸»è¦å…³æ³¨éŸ³é¢‘-è§†è§‰ä¸ä¸€è‡´æ€§æ£€æµ‹ï¼Œå¯èƒ½å¿½ç•¥ä¿æŒéŸ³é¢‘-è§†è§‰å¯¹é½çš„æ“çºµä¸­å­˜åœ¨çš„å•æ¨¡æ€ä¼ªå½±ï¼Œå¯¼è‡´å¯¹è¿™äº›æ“çºµçš„æ£€æµ‹å¤±è´¥ã€‚</p>
<p><strong>Method:</strong> æå‡ºå•é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆå•æ¨¡æ€å’Œè·¨æ¨¡æ€ç‰¹å¾çš„ä¸‹ä¸€å¸§é¢„æµ‹æ¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¼•å…¥çª—å£çº§æ³¨æ„åŠ›æœºåˆ¶æ•è·é¢„æµ‹å¸§ä¸å®é™…å¸§ä¹‹é—´çš„å·®å¼‚ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ£€æµ‹æ¯å¸§å‘¨å›´çš„å±€éƒ¨ä¼ªå½±ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç²¾ç¡®çš„æ—¶é—´å®šä½æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å•é˜¶æ®µè®­ç»ƒæ¡†æ¶åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ä¸‹ä¸€å¸§é¢„æµ‹å’Œçª—å£çº§æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤ŸåŒæ—¶å¤„ç†å®Œå…¨æ“çºµè§†é¢‘å’Œéƒ¨åˆ†ä¼ªé€ æ ·æœ¬ï¼Œä¸ºå¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>Recent multimodal deepfake detection methods designed for generalization conjecture that single-stage supervised training struggles to generalize across unseen manipulations and datasets. However, such approaches that target generalization require pretraining over real samples. Additionally, these methods primarily focus on detecting audio-visual inconsistencies and may overlook intra-modal artifacts causing them to fail against manipulations that preserve audio-visual alignment. To address these limitations, we propose a single-stage training framework that enhances generalization by incorporating next-frame prediction for both uni-modal and cross-modal features. Additionally, we introduce a window-level attention mechanism to capture discrepancies between predicted and actual frames, enabling the model to detect local artifacts around every frame, which is crucial for accurately classifying fully manipulated videos and effectively localizing deepfake segments in partially spoofed samples. Our model, evaluated on multiple benchmark datasets, demonstrates strong generalization and precise temporal localization.</p>
<h3 id="37-tubermc-tube-conditioned-reconstruction-with-mutual-constraints-for-weakly-supervised-spatio-temporal-video-grounding">[37] <a href="https://arxiv.org/abs/2511.10241">TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding</a></h3>
<p><em>Jinxuan Li, Yi Zhang, Jian-Fang Hu, Chaolei Tan, Tianming Liang, Beihao Xia</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºTubeRMCæ¡†æ¶ï¼Œé€šè¿‡ç®¡çŠ¶æ¡ä»¶é‡æ„ä¸äº’çº¦æŸæœºåˆ¶è§£å†³å¼±ç›‘ç£æ—¶ç©ºè§†é¢‘å®šä½ä¸­çš„ç›®æ ‡è¯†åˆ«é”™è¯¯å’Œè·Ÿè¸ªä¸ä¸€è‡´é—®é¢˜ï¼Œåœ¨VidSTGå’ŒHCSTVGåŸºå‡†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¼±ç›‘ç£æ—¶ç©ºè§†é¢‘å®šä½æ–¹æ³•é€šå¸¸é‡‡ç”¨ç®€å•çš„åæœŸèåˆæ–¹å¼ï¼Œç‹¬ç«‹äºæ–‡æœ¬æè¿°ç”Ÿæˆç®¡çŠ¶åŒºåŸŸï¼Œå¯¼è‡´ç›®æ ‡è¯†åˆ«å¤±è´¥å’Œè·Ÿè¸ªä¸ä¸€è‡´çš„é—®é¢˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„æ–‡æœ¬-ç®¡çŠ¶åŒºåŸŸäº¤äº’æœºåˆ¶ã€‚</p>
<p><strong>Method:</strong> æå‡ºTubeRMCæ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒè§†è§‰å®šä½æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ¡ä»¶å€™é€‰ç®¡çŠ¶åŒºåŸŸï¼Œé€šè¿‡ç®¡çŠ¶æ¡ä»¶é‡æ„ä»æ—¶é—´ã€ç©ºé—´å’Œæ—¶ç©ºä¸‰ä¸ªè§†è§’æ•è·ä¸°å¯Œçš„ç®¡çŠ¶-æ–‡æœ¬å¯¹åº”å…³ç³»ï¼Œå¹¶å¼•å…¥ç©ºé—´å’Œæ—¶é—´å»ºè®®ä¹‹é—´çš„äº’çº¦æŸæœºåˆ¶æå‡é‡æ„è´¨é‡ã€‚</p>
<p><strong>Result:</strong> åœ¨VidSTGå’ŒHCSTVGä¸¤ä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTubeRMCä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¯è§†åŒ–ç»“æœè¡¨æ˜è¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†ç›®æ ‡è¯†åˆ«é”™è¯¯å’Œè·Ÿè¸ªä¸ä¸€è‡´é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> ç®¡çŠ¶æ¡ä»¶é‡æ„ä¸äº’çº¦æŸæœºåˆ¶èƒ½å¤Ÿæ˜¾è‘—æå‡å¼±ç›‘ç£æ—¶ç©ºè§†é¢‘å®šä½çš„æ€§èƒ½ï¼Œä¸ºå¤æ‚è§†è§‰è¯­è¨€ç†è§£å’Œæ—¶ç©ºæ¨ç†ä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å¯æ‰©å±•è‡³å…¶ä»–å¤šæ¨¡æ€å®šä½ä»»åŠ¡ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>Spatio-Temporal Video Grounding (STVG) aims to localize a spatio-temporal tube that corresponds to a given language query in an untrimmed video. This is a challenging task since it involves complex vision-language understanding and spatiotemporal reasoning. Recent works have explored weakly-supervised setting in STVG to eliminate reliance on fine-grained annotations like bounding boxes or temporal stamps. However, they typically follow a simple late-fusion manner, which generates tubes independent of the text description, often resulting in failed target identification and inconsistent target tracking. To address this limitation, we propose a Tube-conditioned Reconstruction with Mutual Constraints (\textbf{TubeRMC}) framework that generates text-conditioned candidate tubes with pre-trained visual grounding models and further refine them via tube-conditioned reconstruction with spatio-temporal constraints. Specifically, we design three reconstruction strategies from temporal, spatial, and spatio-temporal perspectives to comprehensively capture rich tube-text correspondences. Each strategy is equipped with a Tube-conditioned Reconstructor, utilizing spatio-temporal tubes as condition to reconstruct the key clues in the query. We further introduce mutual constraints between spatial and temporal proposals to enhance their quality for reconstruction. TubeRMC outperforms existing methods on two public benchmarks VidSTG and HCSTVG. Further visualization shows that TubeRMC effectively mitigates both target identification errors and inconsistent tracking.</p>
<h3 id="38-facial-r1-aligning-reasoning-and-recognition-for-facial-emotion-analysis">[38] <a href="https://arxiv.org/abs/2511.10254">Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis</a></h3>
<p><em>Jiulong Wu, Yucheng Shen, Lingyong Yan, Haixin Sun, Deguo Xia, Jizhou Huang, Min Cao</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºFacial-R1æ¡†æ¶ï¼Œé€šè¿‡ä¸‰é˜¶æ®µå¯¹é½æ–¹æ³•è§£å†³é¢éƒ¨æƒ…æ„Ÿåˆ†æä¸­çš„å¹»è§‰æ¨ç†å’Œè¯†åˆ«-æ¨ç†ä¸å¯¹é½é—®é¢˜ï¼Œåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶å¼•å…¥FEA-20Kæ•°æ®é›†ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„é¢éƒ¨æƒ…æ„Ÿåˆ†ææ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™ï¼šä¸€æ˜¯å¹»è§‰æ¨ç†é—®é¢˜ï¼Œæ¨¡å‹å› ç¼ºä¹è¶³å¤Ÿçš„æƒ…æ„Ÿç‰¹å®šçŸ¥è¯†è€Œç”Ÿæˆçœ‹ä¼¼åˆç†ä½†ä¸å‡†ç¡®çš„è§£é‡Šï¼›äºŒæ˜¯æƒ…æ„Ÿæ¨ç†ä¸è¯†åˆ«ä¹‹é—´çš„ä¸å¯¹é½ï¼Œæºäºè§‚å¯Ÿåˆ°çš„é¢éƒ¨ç‰¹å¾ä¸æœ€ç»ˆæ ‡ç­¾ä¹‹é—´çš„ç¢ç‰‡åŒ–è¿æ¥ã€‚</p>
<p><strong>Method:</strong> æå‡ºä¸‰é˜¶æ®µå¯¹é½æ¡†æ¶ï¼šé¦–å…ˆé€šè¿‡æŒ‡ä»¤å¾®è°ƒå»ºç«‹åŸºç¡€æƒ…æ„Ÿæ¨ç†èƒ½åŠ›ï¼›å…¶æ¬¡å¼•å…¥åŸºäºæƒ…æ„Ÿå’ŒåŠ¨ä½œå•å…ƒæ ‡ç­¾ä½œä¸ºå¥–åŠ±ä¿¡å·çš„å¼ºåŒ–è®­ç»ƒï¼Œæ˜¾å¼å¯¹é½ç”Ÿæˆæ¨ç†è¿‡ç¨‹ä¸é¢„æµ‹æƒ…æ„Ÿï¼›æœ€åè®¾è®¡æ•°æ®åˆæˆæµç¨‹ï¼Œè¿­ä»£åˆ©ç”¨å‰é˜¶æ®µæ‰©å±•è®­ç»ƒæ•°æ®é›†ï¼Œå®ç°æ¨¡å‹çš„å¯æ‰©å±•è‡ªæˆ‘æ”¹è¿›ã€‚</p>
<p><strong>Result:</strong> åœ¨å…«ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFacial-R1åœ¨é¢éƒ¨æƒ…æ„Ÿåˆ†æä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’çš„å¯è§£é‡Šæ€§ï¼Œå¹¶å¼•å…¥äº†åŒ…å«17,737ä¸ªè®­ç»ƒæ ·æœ¬å’Œ1,688ä¸ªæµ‹è¯•æ ·æœ¬çš„FEA-20KåŸºå‡†æ•°æ®é›†ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ä¸‰é˜¶æ®µå¯¹é½æ¡†æ¶å¯ä»¥æœ‰æ•ˆè§£å†³é¢éƒ¨æƒ…æ„Ÿåˆ†æä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œæœ€å°åŒ–ç›‘ç£éœ€æ±‚çš„åŒæ—¶å®ç°æ¨¡å‹æ€§èƒ½çš„æŒç»­è‡ªæˆ‘æ”¹è¿›ï¼Œä¸ºç»†ç²’åº¦æƒ…æ„Ÿç†è§£æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’ŒåŸºå‡†èµ„æºã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>Facial Emotion Analysis (FEA) extends traditional facial emotion recognition by incorporating explainable, fine-grained reasoning. The task integrates three subtasks: emotion recognition, facial Action Unit (AU) recognition, and AU-based emotion reasoning to model affective states jointly. While recent approaches leverage Vision-Language Models (VLMs) and achieve promising results, they face two critical limitations: (1) hallucinated reasoning, where VLMs generate plausible but inaccurate explanations due to insufficient emotion-specific knowledge; and (2) misalignment between emotion reasoning and recognition, caused by fragmented connections between observed facial features and final labels. We propose Facial-R1, a three-stage alignment framework that effectively addresses both challenges with minimal supervision. First, we employ instruction fine-tuning to establish basic emotional reasoning capability. Second, we introduce reinforcement training guided by emotion and AU labels as reward signals, which explicitly aligns the generated reasoning process with the predicted emotion. Third, we design a data synthesis pipeline that iteratively leverages the prior stages to expand the training dataset, enabling scalable self-improvement of the model. Built upon this framework, we introduce FEA-20K, a benchmark dataset comprising 17,737 training and 1,688 test samples with fine-grained emotion analysis annotations. Extensive experiments across eight standard benchmarks demonstrate that Facial-R1 achieves state-of-the-art performance in FEA, with strong generalization and robust interpretability.</p>
<h3 id="39-propa-toward-process-level-optimization-in-visual-reasoning-via-reinforcement-learning">[39] <a href="https://arxiv.org/abs/2511.10279">PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning</a></h3>
<p><em>Yanbei Jiang, Chao Lei, Yihao Ding, Krista Ehinger, Jey Han Lau</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºPROPAæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ä¸GRPOç”Ÿæˆå¯†é›†çš„è¿‡ç¨‹çº§å¥–åŠ±ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯ä¼˜åŒ–è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨å¤šæ­¥ä¾èµ–å¯¼è‡´çš„é”™è¯¯çº§è”é—®é¢˜ï¼Œè€Œç›‘ç£å¾®è°ƒéœ€è¦æ˜‚è´µçš„æ­¥éª¤çº§æ ‡æ³¨ï¼ŒåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¦‚GRPOä»…æä¾›ç¨€ç–çš„ç»“æœçº§åé¦ˆï¼Œé™åˆ¶äº†ç¨³å®šä¼˜åŒ–ã€‚</p>
<p><strong>Method:</strong> PROPAæ¡†æ¶æ•´åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ä¸GRPOç”Ÿæˆå¯†é›†çš„è¿‡ç¨‹çº§å¥–åŠ±ï¼Œé€šè¿‡äº¤é”™GRPOæ›´æ–°ä¸ç›‘ç£å¾®è°ƒè§£å†³å†·å¯åŠ¨é—®é¢˜ï¼Œå¹¶è®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹åœ¨æ¨ç†æ—¶æŒ‡å¯¼æœç´¢ï¼Œä½¿æµ‹è¯•æ—¶æœç´¢ä¸è®­ç»ƒä¿¡å·å¯¹é½ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•å’Œå››ä¸ªVLMéª¨å¹²ç½‘ç»œä¸Šï¼ŒPROPAä¸€è‡´ä¼˜äºåŸºäºç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„åŸºçº¿æ–¹æ³•ï¼Œåœ¨åŸŸå†…ä»»åŠ¡ä¸Šè·å¾—æœ€é«˜17.0%çš„æå‡ï¼Œåœ¨åŸŸå¤–ä»»åŠ¡ä¸Šè·å¾—æœ€é«˜21.0%çš„æå‡ã€‚</p>
<p><strong>Conclusion:</strong> PROPAå»ºç«‹äº†å¼ºå¤§çš„è§†è§‰æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†è¿‡ç¨‹çº§æ¨ç†ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ— éœ€äººå·¥æ ‡æ³¨çš„å¤æ‚è§†è§‰æ¨ç†æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†äº¤é”™è®­ç»ƒç­–ç•¥åœ¨å…‹æœå†·å¯åŠ¨é—®é¢˜ä¸Šçš„ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.</p>
<h3 id="40-clip4vi-reid-learning-modality-shared-representations-via-clip-semantic-bridge-for-visible-infrared-person-re-identification">[40] <a href="https://arxiv.org/abs/2511.10309">CLIP4VI-ReID: Learning Modality-shared Representations via CLIP Semantic Bridge for Visible-Infrared Person Re-identification</a></h3>
<p><em>Xiaomei Yang, Xizhan Gao, Sijie Niu, Fa Zhu, Guang Feng, Xiaofeng Qu, David Camacho</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„CLIPé©±åŠ¨çš„æ¨¡æ€å…±äº«è¡¨ç¤ºå­¦ä¹ ç½‘ç»œCLIP4VI-ReIDï¼Œé€šè¿‡æ–‡æœ¬è¯­ä¹‰ç”Ÿæˆã€çº¢å¤–ç‰¹å¾åµŒå…¥å’Œé«˜çº§è¯­ä¹‰å¯¹é½ä¸‰ä¸ªæ¨¡å—ï¼Œæœ‰æ•ˆè§£å†³äº†å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«ä¸­çš„æ¨¡æ€å·®å¼‚é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¯è§å…‰å›¾åƒä¸çº¢å¤–å›¾åƒåœ¨ç‰©ç†ç‰¹æ€§ä¸Šå­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œå¯¼è‡´ä¼ ç»Ÿæ–¹æ³•åœ¨å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«ä»»åŠ¡ä¸­é¢ä¸´ä¸¥é‡çš„æ¨¡æ€å·®å¼‚æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå­¦ä¹ è·¨æ¨¡æ€çš„å…±äº«è¡¨ç¤ºã€‚</p>
<p><strong>Method:</strong> æå‡ºä¸‰é˜¶æ®µæ¡†æ¶ï¼šæ–‡æœ¬è¯­ä¹‰ç”Ÿæˆæ¨¡å—ä¸ºå¯è§å…‰å›¾åƒç”Ÿæˆæ–‡æœ¬æè¿°å®ç°åˆæ­¥çš„å¯è§å…‰-æ–‡æœ¬æ¨¡æ€å¯¹é½ï¼›çº¢å¤–ç‰¹å¾åµŒå…¥æ¨¡å—åˆ©ç”¨ç”Ÿæˆçš„æ–‡æœ¬è¯­ä¹‰ä¿®æ­£çº¢å¤–å›¾åƒç‰¹å¾åµŒå…¥ï¼›é«˜çº§è¯­ä¹‰å¯¹é½æ¨¡å—ç²¾ç‚¼é«˜å±‚è¯­ä¹‰å¯¹é½ï¼Œç¡®ä¿æ–‡æœ¬è¯­ä¹‰ä»…åŒ…å«èº«ä»½ç›¸å…³ä¿¡æ¯ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„VI-ReIDæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒCLIP4VI-ReIDç›¸æ¯”å…¶ä»–æœ€å…ˆè¿›æ–¹æ³•å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•é€šè¿‡æ–‡æœ¬ä½œä¸ºæ¡¥æ¢å®ç°äº†é—´æ¥çš„å¯è§å…‰-çº¢å¤–æ¨¡æ€å¯¹é½ï¼Œå¢å¼ºäº†å­¦ä¹ åˆ°çš„æ¨¡æ€å…±äº«è¡¨ç¤ºçš„åˆ¤åˆ«æ€§ï¼Œä¸ºè·¨æ¨¡æ€è¡Œäººé‡è¯†åˆ«æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>This paper proposes a novel CLIP-driven modality-shared representation learning network named CLIP4VI-ReID for VI-ReID task, which consists of Text Semantic Generation (TSG), Infrared Feature Embedding (IFE), and High-level Semantic Alignment (HSA). Specifically, considering the huge gap in the physical characteristics between natural images and infrared images, the TSG is designed to generate text semantics only for visible images, thereby enabling preliminary visible-text modality alignment. Then, the IFE is proposed to rectify the feature embeddings of infrared images using the generated text semantics. This process injects id-related semantics into the shared image encoder, enhancing its adaptability to the infrared modality. Besides, with text serving as a bridge, it enables indirect visible-infrared modality alignment. Finally, the HSA is established to refine the high-level semantic alignment. This process ensures that the fine-tuned text semantics only contain id-related information, thereby achieving more accurate cross-modal alignment and enhancing the discriminability of the learned modal-shared representations. Extensive experimental results demonstrate that the proposed CLIP4VI-ReID achieves superior performance than other state-of-the-art methods on some widely used VI-ReID datasets.</p>
<h3 id="41-learning-to-tell-apart-weakly-supervised-video-anomaly-detection-via-disentangled-semantic-alignment">[41] <a href="https://arxiv.org/abs/2511.10334">Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment</a></h3>
<p><em>Wenti Yin, Huaxin Zhang, Xiang Wang, Yuqing Lu, Yicheng Zhang, Bingquan Gong, Jialong Zuo, Li Yu, Changxin Gao, Nong Sang</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è§£è€¦è¯­ä¹‰å¯¹é½ç½‘ç»œï¼ˆDSANetï¼‰ï¼Œé€šè¿‡ä»ç²—ç²’åº¦å’Œç»†ç²’åº¦å±‚é¢æ˜¾å¼åˆ†ç¦»å¼‚å¸¸å’Œæ­£å¸¸ç‰¹å¾ï¼Œè§£å†³äº†å¼±ç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹ä¸­å¿½è§†æ­£å¸¸æ¨¡å¼æŒ–æ˜å’Œç±»åˆ«æ··æ·†çš„é—®é¢˜ï¼Œåœ¨XD-Violenceå’ŒUCF-CrimeåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„å¼±ç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•å€¾å‘äºæ£€æµ‹æœ€æ˜¾è‘—å“åº”ç‰‡æ®µï¼Œè€Œå¿½è§†äº†ä»å¼‚å¸¸ä¸­åˆ†ç¦»å¤šæ ·æ­£å¸¸æ¨¡å¼çš„æŒ–æ˜ï¼Œå¹¶ä¸”ç”±äºç›¸ä¼¼å¤–è§‚å®¹æ˜“å¯¼è‡´ç±»åˆ«æ··æ·†ï¼Œå¯¼è‡´ç»†ç²’åº¦åˆ†ç±»ç»“æœä¸ç†æƒ³ã€‚</p>
<p><strong>Method:</strong> DSANetåœ¨ç²—ç²’åº¦å±‚é¢å¼•å…¥äº†è‡ªå¼•å¯¼æ­£å¸¸æ€§å»ºæ¨¡åˆ†æ”¯ï¼Œé€šè¿‡å­¦ä¹ åˆ°çš„æ­£å¸¸åŸå‹æŒ‡å¯¼é‡æ„è¾“å…¥è§†é¢‘ç‰¹å¾ï¼Œé¼“åŠ±æ¨¡å‹åˆ©ç”¨è§†é¢‘ä¸­å›ºæœ‰çš„æ­£å¸¸æ€§çº¿ç´¢ï¼›åœ¨ç»†ç²’åº¦å±‚é¢æå‡ºäº†è§£è€¦å¯¹æ¯”è¯­ä¹‰å¯¹é½æœºåˆ¶ï¼Œé¦–å…ˆä½¿ç”¨å¸§çº§å¼‚å¸¸åˆ†æ•°å°†æ¯ä¸ªè§†é¢‘æ—¶é—´åˆ†è§£ä¸ºäº‹ä»¶ä¸­å¿ƒå’ŒèƒŒæ™¯ä¸­å¿ƒç»„ä»¶ï¼Œç„¶ååº”ç”¨è§†è§‰è¯­è¨€å¯¹æ¯”å­¦ä¹ å¢å¼ºç±»åˆ«åˆ¤åˆ«æ€§è¡¨ç¤ºã€‚</p>
<p><strong>Result:</strong> åœ¨XD-Violenceå’ŒUCF-Crimeä¸¤ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDSANetè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¼±ç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡æ˜¾å¼åˆ†ç¦»å¼‚å¸¸å’Œæ­£å¸¸ç‰¹å¾çš„åŒé‡æœºåˆ¶ï¼Œæœ‰æ•ˆæå‡äº†å¼±ç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œä¸ºå¤„ç†ç±»åˆ«æ··æ·†å’ŒæŒ–æ˜æ­£å¸¸æ¨¡å¼æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œæ¨åŠ¨äº†ç»†ç²’åº¦å¼‚å¸¸æ£€æµ‹çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories. However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separated from anomalies, and are prone to category confusion due to similar appearance, leading to unsatisfactory fine-grained classification results. Therefore, we propose a novel Disentangled Semantic Alignment Network (DSANet) to explicitly separate abnormal and normal features from coarse-grained and fine-grained aspects, enhancing the distinguishability. Specifically, at the coarse-grained level, we introduce a self-guided normality modeling branch that reconstructs input video features under the guidance of learned normal prototypes, encouraging the model to exploit normality cues inherent in the video, thereby improving the temporal separation of normal patterns and anomalous events. At the fine-grained level, we present a decoupled contrastive semantic alignment mechanism, which first temporally decomposes each video into event-centric and background-centric components using frame-level anomaly scores and then applies visual-language contrastive learning to enhance class-discriminative representations. Comprehensive experiments on two standard benchmarks, namely XD-Violence and UCF-Crime, demonstrate that DSANet outperforms existing state-of-the-art methods.</p>
<h3 id="42-found-fourier-based-von-mises-distribution-for-robust-single-domain-generalization-in-object-detection">[42] <a href="https://arxiv.org/abs/2511.10352">FOUND: Fourier-based von Mises Distribution for Robust Single Domain Generalization in Object Detection</a></h3>
<p><em>Mengzhu Wang, Changyuan Deng, Shanshan Wang, Nan Yin, Long Lan, Liang Yang</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å•åŸŸæ³›åŒ–ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆvon Mises-Fisheråˆ†å¸ƒå’Œå‚…é‡Œå¶å˜æ¢åˆ°CLIPå¼•å¯¼çš„æµç¨‹ä¸­ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æœªè§ç›®æ ‡åŸŸä¸Šçš„æ³›åŒ–æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¿ç•™äº†CLIPçš„è¯­ä¹‰å¯¹é½ä¼˜åŠ¿ï¼Œè¿˜å¢å¼ºäº†ç‰¹å¾å¤šæ ·æ€§å’Œè·¨åŸŸç»“æ„ä¸€è‡´æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å•åŸŸæ³›åŒ–ç›®æ ‡æ£€æµ‹æ–¹æ³•è™½ç„¶é€šè¿‡CLIPè¯­ä¹‰å¢å¼ºå–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†å¾€å¾€å¿½è§†äº†ç‰¹å¾åˆ†å¸ƒåº•å±‚ç»“æ„å’Œé¢‘åŸŸç‰¹æ€§å¯¹æ¨¡å‹é²æ£’æ€§çš„å…³é”®å½±å“ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†å»ºæ¨¡æ–¹å‘æ€§ç‰¹å¾åˆ†å¸ƒå’Œé¢‘åŸŸæ‰°åŠ¨ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨æœªè§åŸŸä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„æ¡†æ¶é‡‡ç”¨von Mises-Fisheråˆ†å¸ƒå»ºæ¨¡ç›®æ ‡è¡¨ç¤ºçš„æ–¹å‘æ€§ç‰¹å¾ï¼Œä»¥æ›´å¥½åœ°æ•æ‰åµŒå…¥ç©ºé—´ä¸­çš„åŸŸä¸å˜è¯­ä¹‰ç»“æ„ã€‚åŒæ—¶å¼•å…¥åŸºäºå‚…é‡Œå¶å˜æ¢çš„å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡æ‰°åŠ¨æŒ¯å¹…å’Œç›¸ä½åˆ†é‡æ¥æ¨¡æ‹Ÿé¢‘åŸŸä¸­çš„åŸŸåç§»ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡ç‰¹å¾é²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šæ ·åŒ–å¤©æ°”é©¾é©¶åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•åŸŸæ³›åŒ–ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†æ‰€ææ¡†æ¶åœ¨æå‡è·¨åŸŸæ³›åŒ–æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆæ–¹å‘æ€§ç‰¹å¾å»ºæ¨¡å’Œé¢‘åŸŸå¢å¼ºå¯¹äºæå‡å•åŸŸæ³›åŒ–ç›®æ ‡æ£€æµ‹æ€§èƒ½çš„é‡è¦æ€§ï¼Œä¸ºå¼€å‘æ›´é²æ£’çš„è·¨åŸŸè§†è§‰ç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚é€šè¿‡åŒæ—¶è€ƒè™‘è¯­ä¹‰ç»“æ„å’Œé¢‘åŸŸç‰¹æ€§ï¼Œè¯¥æ–¹æ³•ä¸ºåº”å¯¹ç°å®ä¸–ç•Œä¸­çš„åŸŸåç§»æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>Single Domain Generalization (SDG) for object detection aims to train a model on a single source domain that can generalize effectively to unseen target domains. While recent methods like CLIP-based semantic augmentation have shown promise, they often overlook the underlying structure of feature distributions and frequency-domain characteristics that are critical for robustness. In this paper, we propose a novel framework that enhances SDG object detection by integrating the von Mises-Fisher (vMF) distribution and Fourier transformation into a CLIP-guided pipeline. Specifically, we model the directional features of object representations using vMF to better capture domain-invariant semantic structures in the embedding space. Additionally, we introduce a Fourier-based augmentation strategy that perturbs amplitude and phase components to simulate domain shifts in the frequency domain, further improving feature robustness. Our method not only preserves the semantic alignment benefits of CLIP but also enriches feature diversity and structural consistency across domains. Extensive experiments on the diverse weather-driving benchmark demonstrate that our approach outperforms the existing state-of-the-art method.</p>
<h3 id="43-msgnav-unleashing-the-power-of-multi-modal-3d-scene-graph-for-zero-shot-embodied-navigation">[43] <a href="https://arxiv.org/abs/2511.10376">MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation</a></h3>
<p><em>Xun Huang, Shijia Zhao, Yunxiang Wang, Xin Lu, Wanfa Zhang, Rongsheng Qu, Weixin Li, Yunhong Wang, Chenglu Wen</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€3Dåœºæ™¯å›¾ï¼ˆM3DSGï¼‰å’ŒåŸºäºæ­¤çš„MSGNavé›¶æ ·æœ¬å¯¼èˆªç³»ç»Ÿï¼Œé€šè¿‡ä¿ç•™è§†è§‰çº¿ç´¢çš„åŠ¨æ€å›¾åƒå…³ç³»è¾¹æ›¿ä»£çº¯æ–‡æœ¬å…³ç³»ï¼Œè§£å†³äº†ç°æœ‰é›¶æ ·æœ¬å¯¼èˆªæ–¹æ³•ä¸­è§†è§‰ä¿¡æ¯ä¸¢å¤±å’Œè¯æ±‡å—é™çš„é—®é¢˜ï¼Œåœ¨GOAT-Benchå’ŒHM3D-OVONæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é›¶æ ·æœ¬å¯¼èˆªæ–¹æ³•æ„å»ºæ˜¾å¼3Dåœºæ™¯å›¾æ—¶é€šå¸¸å°†ä¸°å¯Œçš„è§†è§‰è§‚å¯Ÿå‹ç¼©ä¸ºçº¯æ–‡æœ¬å…³ç³»ï¼Œå¯¼è‡´æ„å»ºæˆæœ¬é«˜ã€è§†è§‰è¯æ®ä¸å¯é€†ä¸¢å¤±ä»¥åŠè¯æ±‡å—é™ï¼Œæ— æ³•æ»¡è¶³ç°å®ä¸–ç•Œéƒ¨ç½²æ‰€éœ€çš„å¼€æ”¾è¯æ±‡æ³›åŒ–å’Œä½è®­ç»ƒå¼€é”€è¦æ±‚ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†å¤šæ¨¡æ€3Dåœºæ™¯å›¾ï¼ˆM3DSGï¼‰ï¼Œç”¨åŠ¨æ€åˆ†é…çš„å›¾åƒæ›¿æ¢æ–‡æœ¬å…³ç³»è¾¹ä»¥ä¿ç•™è§†è§‰çº¿ç´¢ï¼›æ„å»ºäº†MSGNavé›¶æ ·æœ¬å¯¼èˆªç³»ç»Ÿï¼ŒåŒ…å«å…³é”®å­å›¾é€‰æ‹©æ¨¡å—ã€è‡ªé€‚åº”è¯æ±‡æ›´æ–°æ¨¡å—ã€é—­ç¯æ¨ç†æ¨¡å—ï¼Œå¹¶é’ˆå¯¹æœ€åä¸€è‹±é‡Œé—®é¢˜æå‡ºäº†åŸºäºå¯è§æ€§çš„è§†ç‚¹å†³ç­–æ¨¡å—ã€‚</p>
<p><strong>Result:</strong> åœ¨GOAT-Benchå’ŒHM3D-OVONæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼ŒMSGNavå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬å¯¼èˆªä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> M3DSGé€šè¿‡ä¿ç•™è§†è§‰è¯æ®æœ‰æ•ˆè§£å†³äº†ç°æœ‰é›¶æ ·æœ¬å¯¼èˆªæ–¹æ³•çš„å±€é™æ€§ï¼ŒMSGNavç³»ç»Ÿä¸ºå¼€æ”¾è¯æ±‡å¯¼èˆªæä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æ˜ç¡®è§£å†³äº†æœ€åä¸€è‹±é‡Œé—®é¢˜ï¼Œä¸ºé›¶æ ·æœ¬å¯¼èˆªçš„å®é™…éƒ¨ç½²å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images. Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning. Additionally, we further identify the last-mile problem in zero-shot navigation - determining the feasible target location with a suitable final viewpoint, and propose a Visibility-based Viewpoint Decision module to explicitly resolve it. Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets. The open-source code will be publicly available.</p>
<h3 id="44-rodepil-a-video-dataset-of-laboratory-rodents-for-seizure-detection-and-benchmark-evaluation">[44] <a href="https://arxiv.org/abs/2511.10431">RodEpil: A Video Dataset of Laboratory Rodents for Seizure Detection and Benchmark Evaluation</a></h3>
<p><em>Daniele Perlo, Vladimir Despotovic, Selma Boudissa, Sang-Yoon Kim, Petr Nazarov, Yanrong Zhang, Max Wintermark, Olivier Keunen</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºè‡ªåŠ¨æ£€æµ‹å®éªŒå®¤å•®é½¿åŠ¨ç‰©æƒŠå¥äº‹ä»¶çš„ç²¾é€‰è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å«10,101ä¸ªé˜´æ€§æ ·æœ¬å’Œ2,952ä¸ªé˜³æ€§æ ·æœ¬ï¼Œå¹¶ä½¿ç”¨åŸºäºTransformerçš„è§†é¢‘åˆ†ç±»å™¨å®ç°äº†97%çš„å¹³å‡F1åˆ†æ•°ã€‚è¯¥æ•°æ®é›†å’ŒåŸºå‡†ä»£ç å·²å…¬å¼€ï¼Œæ—¨åœ¨æ”¯æŒä¸´åºŠå‰ç™«ç—«ç ”ç©¶ä¸­éä¾µå…¥æ€§è§†é¢‘ç›‘æµ‹çš„å¯é‡å¤ç ”ç©¶ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ä¸´åºŠå‰ç™«ç—«ç ”ç©¶ç¼ºä¹æ ‡å‡†åŒ–ã€å…¬å¼€å¯ç”¨çš„è§†é¢‘æ•°æ®é›†æ¥æ”¯æŒè‡ªåŠ¨æƒŠå¥äº‹ä»¶æ£€æµ‹ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œé€šè¿‡æ„å»ºä¸€ä¸ªç²¾å¿ƒæ ‡æ³¨çš„å®éªŒå®¤å•®é½¿åŠ¨ç‰©è§†é¢‘æ•°æ®é›†ï¼Œä¸ºå¼€å‘éä¾µå…¥æ€§è§†é¢‘ç›‘æµ‹æ–¹æ³•æä¾›å¯é åŸºç¡€ï¼Œä»è€Œå‡å°‘å¯¹äººå·¥è§‚å¯Ÿçš„ä¾èµ–å¹¶æé«˜æ£€æµ‹æ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨åŸºäºTransformerçš„è§†é¢‘åˆ†ç±»å™¨TimeSformeræ¶æ„è¿›è¡ŒåŸºå‡†å®éªŒï¼Œä½¿ç”¨ä¸¥æ ¼çš„å—è¯•è€…é—´äº”æŠ˜äº¤å‰éªŒè¯æ¥é˜²æ­¢æ•°æ®æ³„éœ²ã€‚æ•°æ®é›†åŒ…å«10ç§’çš„ä¿¯è§†å’Œä¾§è§†è§†é¢‘ç‰‡æ®µï¼Œåˆ†ä¸ºæ­£å¸¸æ´»åŠ¨å’ŒæƒŠå¥ä¸¤ç±»ï¼Œå…±è®¡13,053ä¸ªæ ·æœ¬æ¥è‡ª19ä¸ªå—è¯•è€…ï¼Œå¹¶è¯¦ç»†æè¿°äº†æ•°æ®æ•´ç†ã€æ ‡æ³¨åè®®å’Œé¢„å¤„ç†æµç¨‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºTimeSformeræ¶æ„èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†æƒŠå¥å’Œæ­£å¸¸æ´»åŠ¨ï¼Œå¹³å‡F1åˆ†æ•°è¾¾åˆ°97%ã€‚é‡‡ç”¨ä¸¥æ ¼çš„å—è¯•è€…é—´åˆ’åˆ†ç¡®ä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œæ¯ä¸ªå—è¯•è€…ä»…å‡ºç°åœ¨å•ä¸€æŠ˜ä¸­ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨æœªè§è¿‡çš„å—è¯•è€…ä¸Šçš„é²æ£’æ€§è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºè§†é¢‘çš„è‡ªåŠ¨æƒŠå¥æ£€æµ‹åœ¨ä¸´åºŠå‰ç™«ç—«ç ”ç©¶ä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºå¼€å‘éä¾µå…¥æ€§ç›‘æµ‹å·¥å…·æä¾›äº†é‡è¦åŸºç¡€ã€‚å…¬å¼€çš„æ•°æ®é›†å’Œä»£ç å°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„å¯é‡å¤ç ”ç©¶ï¼Œæœ‰æœ›å‡å°‘åŠ¨ç‰©å®éªŒä¸­å¯¹ä¾µå…¥æ€§ç›‘æµ‹çš„ä¾èµ–ï¼Œå¹¶æé«˜ç™«ç—«ç ”ç©¶çš„æ ‡å‡†åŒ–æ°´å¹³ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>We introduce a curated video dataset of laboratory rodents for automatic detection of convulsive events. The dataset contains short (10~s) top-down and side-view video clips of individual rodents, labeled at clip level as normal activity or seizure. It includes 10,101 negative samples and 2,952 positive samples collected from 19 subjects. We describe the data curation, annotation protocol and preprocessing pipeline, and report baseline experiments using a transformer-based video classifier (TimeSformer). Experiments employ five-fold cross-validation with strict subject-wise partitioning to prevent data leakage (no subject appears in more than one fold). Results show that the TimeSformer architecture enables discrimination between seizure and normal activity with an average F1-score of 97%. The dataset and baseline code are publicly released to support reproducible research on non-invasive, video-based monitoring in preclinical epilepsy research. RodEpil Dataset access - DOI: 10.5281/zenodo.17601357</p>
<h3 id="45-semanticvla-semantic-aligned-sparsification-and-enhancement-for-efficient-robotic-manipulation">[45] <a href="https://arxiv.org/abs/2511.10518">SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation</a></h3>
<p><em>Wei Li, Renshan Zhang, Rui Shao, Zhijian Fang, Kaiwen Zhou, Zhuotao Tian, Liqiang Nie</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSemanticVLAæ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰å¯¹é½çš„ç¨€ç–åŒ–å’Œå¢å¼ºæŠ€æœ¯è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä¸­çš„æ„ŸçŸ¥å†—ä½™å’Œè¯­ä¹‰å¯¹é½ä¸è¶³é—®é¢˜ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡æ•ˆç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œåº”ç”¨ä¸­é¢ä¸´ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šæ„ŸçŸ¥å†—ä½™å¯¼è‡´æ— å…³è§†è§‰è¾“å…¥å¤„ç†æ•ˆç‡ä½ä¸‹ï¼Œä»¥åŠæµ…å±‚çš„æŒ‡ä»¤-è§†è§‰å¯¹é½é˜»ç¢äº†åŠ¨ä½œçš„è¯­ä¹‰åŸºç¡€ï¼Œè¿™äº›å› ç´ é™åˆ¶äº†å®é™…éƒ¨ç½²çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„SemanticVLAæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè¯­ä¹‰å¼•å¯¼çš„åŒé‡è§†è§‰å‰ªæå™¨é€šè¿‡æŒ‡ä»¤é©±åŠ¨å‰ªæå™¨å’Œç©ºé—´èšåˆå‰ªæå™¨åˆ†åˆ«å¤„ç†å…¨å±€åŠ¨ä½œçº¿ç´¢å’Œå‡ ä½•ç‰¹å¾ï¼›è¯­ä¹‰äº’è¡¥çš„å±‚æ¬¡èåˆå™¨æ•´åˆSigLIPå’ŒDINOv2çš„å¯†é›†è¡¥ä¸ä¸ç¨€ç–æ ‡è®°ï¼›è¯­ä¹‰æ¡ä»¶åŠ¨ä½œè€¦åˆå™¨æ›¿ä»£ä¼ ç»Ÿçš„è§‚æµ‹åˆ°è‡ªç”±åº¦æ–¹æ³•ï¼Œå®ç°æ›´é«˜æ•ˆçš„æœºå™¨äººè¡Œä¸ºå»ºæ¨¡ã€‚</p>
<p><strong>Result:</strong> åœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSemanticVLAåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡è¾¾åˆ°æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œåœ¨LIBEROåŸºå‡†ä¸Šç›¸æ¯”OpenVLAæˆåŠŸç‡æå‡21.1%ï¼ŒåŒæ—¶è®­ç»ƒæˆæœ¬å’Œæ¨ç†å»¶è¿Ÿåˆ†åˆ«é™ä½3.0å€å’Œ2.7å€ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†è¯­ä¹‰å¯¹é½çš„ç¨€ç–åŒ–ç­–ç•¥èƒ½æœ‰æ•ˆå¹³è¡¡æœºå™¨äººæ“ä½œä»»åŠ¡çš„æ€§èƒ½ä¸æ•ˆç‡ï¼Œä¸ºå®é™…éƒ¨ç½²æä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ï¼ŒåŒæ—¶å¼€æºå®ç°ä¿ƒè¿›äº†ç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA</p>
<h3 id="46-benchmarking-diversity-in-image-generation-via-attribute-conditional-human-evaluation">[46] <a href="https://arxiv.org/abs/2511.10547">Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation</a></h3>
<p><em>Isabela Albuquerque, Ira Ktena, Olivia Wiles, Ivana KajiÄ‡, Amal Rannen-Triki, Cristina Vasconcelos, Aida Nematzadeh</em></p>
<h4 id="tldr_45">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºç³»ç»Ÿè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¤šæ ·æ€§çš„æ¡†æ¶ï¼Œé€šè¿‡è¯„ä¼°ä¸ªä½“æ¦‚å¿µåŠå…¶ç›¸å…³å˜å¼‚å› ç´ æ¥è§£å†³å½“å‰æ¨¡å‹ç”ŸæˆåŒè´¨åŒ–è¾“å‡ºçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬äººç±»è¯„ä¼°æ¨¡æ¿ã€ç²¾å¿ƒè®¾è®¡çš„æç¤ºé›†å’ŒåŸºäºäºŒé¡¹æ£€éªŒçš„æ¨¡å‹æ¯”è¾ƒæ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_45">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šæœ‰æ‰€è¿›æ­¥ï¼Œä½†å¾€å¾€ç¼ºä¹å¤šæ ·æ€§ï¼Œäº§ç”ŸåŒè´¨åŒ–çš„è¾“å‡ºç»“æœã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æ¨¡å‹å¤šæ ·æ€§çš„ç³»ç»Ÿè¯„ä¼°æ¡†æ¶ï¼Œæ— æ³•å‡†ç¡®è¡¡é‡ä¸åŒæ¦‚å¿µå’Œå˜å¼‚å› ç´ ä¸‹çš„ç”Ÿæˆå¤šæ ·æ€§è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„å¤šæ ·æ€§è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬è®¾è®¡æ–°é¢–çš„äººç±»è¯„ä¼°æ¨¡æ¿ç”¨äºç»†è‡´å¤šæ ·æ€§è¯„ä¼°ï¼Œæ„å»ºè¦†ç›–å¤šæ ·åŒ–æ¦‚å¿µåŠå…¶å˜å¼‚å› ç´ çš„æç¤ºé›†ï¼Œä»¥åŠé€šè¿‡äºŒé¡¹æ£€éªŒæ¯”è¾ƒæ¨¡å‹çš„äººç±»æ ‡æ³¨ç»“æœã€‚åŒæ—¶ä¸¥æ ¼æ¯”è¾ƒäº†å¤šç§å›¾åƒåµŒå…¥æ–¹æ³•åœ¨å¤šæ ·æ€§æµ‹é‡ä¸­çš„è¡¨ç°ã€‚</p>
<p><strong>Result:</strong> è¯¥æ¡†æ¶èƒ½å¤Ÿå¯¹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹è¿›è¡Œå¤šæ ·æ€§æ’åï¼Œè¯†åˆ«å‡ºæ¨¡å‹åœ¨ç‰¹å®šç±»åˆ«ä¸­è¡¨ç°ä¸ä½³çš„æƒ…å†µã€‚é€šè¿‡ç³»ç»Ÿè¯„ä¼°å‘ç°ä¸åŒæ¨¡å‹åœ¨å¤šæ ·æ€§æ–¹é¢çš„æ˜¾è‘—å·®å¼‚ï¼Œå¹¶éªŒè¯äº†æ‰€æå‡ºè¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶æä¾›äº†ä¸€ä¸ªç¨³å¥çš„å¤šæ ·æ€§è¯„ä¼°æ–¹æ³•è®ºå’Œé‡è¦è§è§£ï¼Œä¸ºæ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¤šæ ·æ€§å’Œåº¦é‡å¼€å‘é“ºå¹³äº†é“è·¯ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸæŒ‡å¯¼æ¨¡å‹å¼€å‘è€…è¯†åˆ«å¤šæ ·æ€§ä¸è¶³çš„é¢†åŸŸï¼Œå¹¶æ¨åŠ¨æ›´å…¨é¢çš„æ¨¡å‹è¯„ä¼°æ ‡å‡†å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_45">ğŸ“„ Abstract</h4>
<p>Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.
  Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.</p>
<h3 id="47-omnivggt-omni-modality-driven-visual-geometry-grounded">[47] <a href="https://arxiv.org/abs/2511.10560">OmniVGGT: Omni-Modality Driven Visual Geometry Grounded</a></h3>
<p><em>Haosong Peng, Hao Li, Yalun Dai, Yushi Lan, Yihang Luo, Tianyu Qi, Zhengshen Zhang, Yufeng Zhan, Junfei Zhang, Wenchao Xu, Ziwei Liu</em></p>
<h4 id="tldr_46">ğŸ§© TL;DR</h4>
<p>OmniVGGTæ˜¯ä¸€ä¸ªæ–°é¢–çš„3DåŸºç¡€æ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡GeoAdapteræœ‰æ•ˆæ•´åˆä»»æ„æ•°é‡çš„å‡ ä½•æ¨¡æ€è¾“å…¥ï¼Œé‡‡ç”¨é›¶åˆå§‹åŒ–å·ç§¯æ¸è¿›æ³¨å…¥å‡ ä½•ä¿¡æ¯ï¼Œå¹¶åœ¨å¤šè§†å›¾æ·±åº¦ä¼°è®¡ã€ç«‹ä½“åŒ¹é…å’Œç›¸æœºå§¿æ€ä¼°è®¡ä»»åŠ¡ä¸Šå–å¾—æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_46">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§å¤šæ•°3DåŸºç¡€æ¨¡å‹ä»…å‡è®¾RGBè¾“å…¥è€Œå¿½ç•¥äº†æ˜“äºè·å–çš„å‡ ä½•çº¿ç´¢ï¼ˆå¦‚ç›¸æœºå†…å‚ã€å§¿æ€å’Œæ·±åº¦å›¾ï¼‰ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹å¯¹ç©ºé—´ä¿¡æ¯çš„å……åˆ†åˆ©ç”¨å’Œæ€§èƒ½æå‡ã€‚</p>
<p><strong>Method:</strong> æå‡ºGeoAdapteræ¨¡å—ï¼Œä½¿ç”¨é›¶åˆå§‹åŒ–å·ç§¯æ¸è¿›å¼ç¼–ç æ·±åº¦å’Œç›¸æœºå†…å¤–å‚æ•°åˆ°ç©ºé—´åŸºç¡€æ¨¡å‹ä¸­ï¼›é‡‡ç”¨éšæœºå¤šæ¨¡æ€èåˆç­–ç•¥ï¼Œåœ¨è®­ç»ƒæ—¶éšæœºé‡‡æ ·æ¨¡æ€å­é›†ä»¥å¢å¼ºæ¨¡å‹é²æ£’æ€§ï¼›è¯¥è®¾è®¡ä¿æŒæ¨ç†é€Ÿåº¦ä¸VGGTç›¸å½“ä¸”ä¼˜åŒ–ç¨³å®šã€‚</p>
<p><strong>Result:</strong> åœ¨å•ç›®/å¤šè§†å›¾æ·±åº¦ä¼°è®¡ã€å¤šè§†å›¾ç«‹ä½“åŒ¹é…å’Œç›¸æœºå§¿æ€ä¼°è®¡ä»»åŠ¡ä¸Šï¼ŒOmniVGGTè¶…è¶Šäº†ç°æœ‰è¾…åŠ©è¾“å…¥æ–¹æ³•ï¼Œå³ä½¿ä»…ä½¿ç”¨RGBè¾“å…¥ä¹Ÿèƒ½è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼›é›†æˆåˆ°è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ååœ¨ä¸»æµåŸºå‡†æµ‹è¯•å’Œæœºå™¨äººä»»åŠ¡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜å‡ ä½•çº¿ç´¢çš„æœ‰æ•ˆæ•´åˆèƒ½æ˜¾è‘—æå‡3DåŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼ŒGeoAdapterçš„é›¶åˆå§‹åŒ–è®¾è®¡ç¡®ä¿äº†ç¨³å®šä¼˜åŒ–ï¼Œéšæœºå¤šæ¨¡æ€èåˆç­–ç•¥å¢å¼ºäº†æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ„å»ºæ›´å¼ºå¤§çš„å¤šæ¨¡æ€3Dæ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_46">ğŸ“„ Abstract</h4>
<p>General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="48-omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages">[48] <a href="https://arxiv.org/abs/2511.09690">Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages</a></h3>
<p><em>Omnilingual ASR team, Gil Keren, Artyom Kozhevnikov, Yen Meng, Christophe Ropers, Matthew Setzler, Skyler Wang, Ife Adebara, Michael Auli, Can Balioglu, Kevin Chan, Chierh Cheng, Joe Chuang, Caley Droof, Mark Duppenthaler, Paul-Ambroise Duquenne, Alexander Erben, Cynthia Gao, Gabriel Mejia Gonzalez, Kehan Lyu, Sagar Miglani, Vineel Pratap, Kaushik Ram Sadagopan, Safiyyah Saleem, Arina Turkatenko, Albert Ventayol-Boada, Zheng-Xin Yong, Yu-An Chung, Jean Maillard, Rashel Moritz, Alexandre Mourachko, Mary Williamson, Shireen Yates</em></p>
<h4 id="tldr_47">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Omnilingual ASRï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºå¯æ‰©å±•æ€§è®¾è®¡çš„å¤§è§„æ¨¡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œèƒ½å¤Ÿä»…ç”¨å°‘é‡æ•°æ®æ ·æœ¬æ”¯æŒæœªè¦†ç›–çš„è¯­è¨€ï¼Œå°†ASRè¦†ç›–èŒƒå›´æ‰©å±•åˆ°1600å¤šç§è¯­è¨€ï¼Œå…¶ä¸­åŒ…æ‹¬500å¤šç§æ­¤å‰ä»æœªè¢«ASRæœåŠ¡è¿‡çš„è¯­è¨€ã€‚</p>
<hr />
<h4 id="detailed-summary_47">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯åœ¨é«˜èµ„æºè¯­è¨€ä¸­å–å¾—äº†è¿›å±•ï¼Œä½†å…¨çƒ7000å¤šç§è¯­è¨€ä¸­çš„å¤§å¤šæ•°ä»æœªè¢«æ”¯æŒï¼Œå¯¼è‡´æ•°åƒç§é•¿å°¾è¯­è¨€è¢«å¿½è§†ã€‚æ‰©å±•ASRè¦†ç›–èŒƒå›´æˆæœ¬é«˜æ˜‚ä¸”å—é™äºæ¶æ„é™åˆ¶ï¼ŒåŒæ—¶åœ¨æ²¡æœ‰ç¤¾åŒºåä½œçš„æƒ…å†µä¸‹è¿˜å­˜åœ¨ä¼¦ç†é—®é¢˜ï¼Œè¿™ä½¿å¾—å¤§å¤šæ•°è¯­è¨€æ— æ³•è·å¾—ASRæœåŠ¡ã€‚</p>
<p><strong>Method:</strong> è¯¥ç³»ç»Ÿé‡‡ç”¨7Bå‚æ•°çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ¥å­¦ä¹ é²æ£’çš„è¯­éŸ³è¡¨ç¤ºï¼Œå¹¶å¼•å…¥äº†ä¸“ä¸ºé›¶æ ·æœ¬æ³›åŒ–è®¾è®¡çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œåˆ©ç”¨LLMå¯å‘çš„è§£ç å™¨ã€‚é€šè¿‡ç»“åˆå…¬å…±èµ„æºå’Œé€šè¿‡æœ‰å¿æœ¬åœ°åˆä½œä¼™ä¼´å…³ç³»æ”¶é›†çš„ç¤¾åŒºå½•éŸ³ï¼Œæ„å»ºäº†å¤§è§„æ¨¡å¤šæ ·åŒ–çš„è®­ç»ƒè¯­æ–™åº“ã€‚</p>
<p><strong>Result:</strong> è‡ªåŠ¨è¯„ä¼°æ˜¾ç¤ºï¼Œä¸å…ˆå‰ç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿåœ¨ä½èµ„æºæ¡ä»¶ä¸‹å–å¾—äº†æ˜¾è‘—æå‡ï¼Œå¹¶è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ¨¡å‹å®¶æ—ä»é€‚ç”¨äºä½åŠŸè€—è®¾å¤‡çš„300Må˜ä½“åˆ°è¿½æ±‚æœ€å¤§ç²¾åº¦çš„7Bå˜ä½“ï¼Œè¦†ç›–äº†è¶…è¿‡1600ç§è¯­è¨€ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§çš„æ­¤ç±»åŠªåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€æºæ¨¡å‹å’Œå·¥å…·å¦‚ä½•é™ä½ç ”ç©¶äººå‘˜å’Œç¤¾åŒºçš„å‡†å…¥é—¨æ§›ï¼Œé‚€è¯·æ–°çš„å‚ä¸å½¢å¼ã€‚é€šè¿‡åæ€å¡‘é€ è¿™ä¸€è®¾è®¡çš„ä¼¦ç†è€ƒé‡ï¼Œè®¨è®ºäº†å…¶ç¤¾ä¼šå½±å“ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡ç¤¾åŒºåä½œå’Œè¡¥å¿æ€§åˆä½œæ¥ä¿ƒè¿›è¯­è¨€æŠ€æœ¯çš„åŒ…å®¹æ€§å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_47">ğŸ“„ Abstract</h4>
<p>Automatic speech recognition (ASR) has advanced in high-resource languages, but most of the world's 7,000+ languages remain unsupported, leaving thousands of long-tail languages behind. Expanding ASR coverage has been costly and limited by architectures that restrict language support, making extension inaccessible to most--all while entangled with ethical concerns when pursued without community collaboration. To transcend these limitations, we introduce Omnilingual ASR, the first large-scale ASR system designed for extensibility. Omnilingual ASR enables communities to introduce unserved languages with only a handful of data samples. It scales self-supervised pre-training to 7B parameters to learn robust speech representations and introduces an encoder-decoder architecture designed for zero-shot generalization, leveraging a LLM-inspired decoder. This capability is grounded in a massive and diverse training corpus; by combining breadth of coverage with linguistic variety, the model learns representations robust enough to adapt to unseen languages. Incorporating public resources with community-sourced recordings gathered through compensated local partnerships, Omnilingual ASR expands coverage to over 1,600 languages, the largest such effort to date--including over 500 never before served by ASR. Automatic evaluations show substantial gains over prior systems, especially in low-resource conditions, and strong generalization. We release Omnilingual ASR as a family of models, from 300M variants for low-power devices to 7B for maximum accuracy. We reflect on the ethical considerations shaping this design and conclude by discussing its societal impact. In particular, we highlight how open-sourcing models and tools can lower barriers for researchers and communities, inviting new forms of participation. Open-source artifacts are available at https://github.com/facebookresearch/omnilingual-asr.</p>
<h3 id="49-termgpt-multi-level-contrastive-fine-tuning-for-terminology-adaptation-in-legal-and-financial-domain">[49] <a href="https://arxiv.org/abs/2511.09854">TermGPT: Multi-Level Contrastive Fine-Tuning for Terminology Adaptation in Legal and Financial Domain</a></h3>
<p><em>Yidan Sun, Mengying Zhu, Feiyue Chen, Yangyang Wu, Xiaolei Dan, Mengyuan Yang, Xiaolin Zheng, Shenglin Ben</em></p>
<h4 id="tldr_48">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºTermGPTï¼Œä¸€ç§é’ˆå¯¹æœ¯è¯­é€‚åº”çš„å¤šå±‚çº§å¯¹æ¯”å¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºå¥å­å›¾å’Œè®¾è®¡å¥å­çº§ä¸è¯å…ƒçº§çš„å¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆè§£å†³äº†å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸæœ¯è¯­è¡¨ç¤ºä¸­çš„å„å‘åŒæ€§é—®é¢˜å’Œæœ¯è¯­çº§è¡¨ç¤ºä¸è¶³çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_48">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åµŒå…¥ç©ºé—´å­˜åœ¨å„å‘åŒæ€§é—®é¢˜ï¼Œå¯¼è‡´åœ¨ä¸“ä¸šé¢†åŸŸï¼ˆç‰¹åˆ«æ˜¯æ³•å¾‹å’Œé‡‘èé¢†åŸŸï¼‰çš„æœ¯è¯­è¡¨ç¤ºèƒ½åŠ›ä¸è¶³ï¼Œè¿™ç§æœ¯è¯­çº§è¡¨ç¤ºçš„å¼±ç‚¹ä¸¥é‡å½±å“äº†æ³•å¾‹åˆ¤å†³é¢„æµ‹å’Œé‡‘èé£é™©åˆ†æç­‰ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œå› ä¸ºè¿™äº›ä»»åŠ¡å¯¹ç»†å¾®çš„è¯­ä¹‰åŒºåˆ†è¦æ±‚å¾ˆé«˜ã€‚</p>
<p><strong>Method:</strong> æˆ‘ä»¬é¦–å…ˆæ„å»ºå¥å­å›¾æ¥æ•æ‰è¯­ä¹‰å’Œç»“æ„å…³ç³»ï¼ŒåŸºäºä¸Šä¸‹æ–‡å’Œæ‹“æ‰‘çº¿ç´¢ç”Ÿæˆè¯­ä¹‰ä¸€è‡´ä¸”å…·æœ‰åŒºåˆ†åº¦çš„æ­£è´Ÿæ ·æœ¬ï¼›ç„¶åè®¾è®¡äº†ä¸€ä¸ªå¤šå±‚çº§å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œåœ¨å¥å­çº§å’Œè¯å…ƒçº§åŒæ—¶è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œä»¥å¢å¼ºå…¨å±€ä¸Šä¸‹æ–‡ç†è§£å’Œç»†ç²’åº¦æœ¯è¯­åŒºåˆ†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒTermGPTåœ¨é‡‘èå’Œæ³•å¾‹é¢†åŸŸçš„æœ¯è¯­åŒºåˆ†ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼›ä¸ºäº†æ”¯æŒç¨³å¥è¯„ä¼°ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†é¦–ä¸ªåŸºäºå®˜æ–¹ç›‘ç®¡æ–‡ä»¶çš„é‡‘èæœ¯è¯­æ•°æ®é›†ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¤šå±‚çº§å¯¹æ¯”å­¦ä¹ æ¡†æ¶åœ¨æå‡ä¸“ä¸šé¢†åŸŸæœ¯è¯­è¡¨ç¤ºèƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸåº”ç”¨ä¸­çš„æœ¯è¯­è¡¨ç¤ºç“¶é¢ˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¹¶ä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é¦–ä¸ªé‡‘èæœ¯è¯­æ•°æ®é›†èµ„æºã€‚</p>
<hr />
<h4 id="abstract_48">ğŸ“„ Abstract</h4>
<p>Large language models (LLMs) have demonstrated impressive performance in text generation tasks; however, their embedding spaces often suffer from the isotropy problem, resulting in poor discrimination of domain-specific terminology, particularly in legal and financial contexts. This weakness in terminology-level representation can severely hinder downstream tasks such as legal judgment prediction or financial risk analysis, where subtle semantic distinctions are critical. To address this problem, we propose TermGPT, a multi-level contrastive fine-tuning framework designed for terminology adaptation. We first construct a sentence graph to capture semantic and structural relations, and generate semantically consistent yet discriminative positive and negative samples based on contextual and topological cues. We then devise a multi-level contrastive learning approach at both the sentence and token levels, enhancing global contextual understanding and fine-grained terminology discrimination. To support robust evaluation, we construct the first financial terminology dataset derived from official regulatory documents. Experiments show that TermGPT outperforms existing baselines in term discrimination tasks within the finance and legal domains.</p>
<h3 id="50-hi-transpa-hearing-impairments-translation-personal-assistant">[50] <a href="https://arxiv.org/abs/2511.09915">HI-TransPA: Hearing Impairments Translation Personal Assistant</a></h3>
<p><em>Zhiming Ma, Shiyu Gan, Junhao Zhao, Xianming Li, Qingyun Pan, Peidong Wang, Mingjun Pan, Yuhao Mo, Jiajie Cheng, Chengxin Chen, Zhonglun Cao, Chonghan Liu, Shi Cheng</em></p>
<h4 id="tldr_49">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºHI-TransPAï¼Œä¸€ç§åŸºäºOmni-ModelèŒƒå¼çš„æŒ‡ä»¤é©±åŠ¨è§†å¬ä¸ªäººåŠ©æ‰‹ï¼Œé€šè¿‡èåˆæ¨¡ç³Šè¯­éŸ³ä¸é«˜å¸§ç‡å”‡éƒ¨åŠ¨æ€ï¼Œä¸ºå¬éšœäººå£«æä¾›ç»Ÿä¸€çš„ç¿»è¯‘å’Œå¯¹è¯è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¨¡å‹åœ¨ä¸“é—¨æ„å»ºçš„HI-Dialogueæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å­—é¢å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_49">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¸ºè§£å†³å¬éšœäººå£«æ—¥å¸¸äº¤æµä¸­çš„ç»Ÿä¸€æ€§å’Œçµæ´»æ€§éœ€æ±‚ï¼Œæœ¬ç ”ç©¶å°†Omni-ModelèŒƒå¼å¼•å…¥è¾…åŠ©æŠ€æœ¯é¢†åŸŸã€‚ç°æœ‰Omni-Modelå¯¹å¬éšœäººå£«è¯­éŸ³çš„é€‚åº”æ€§æœ‰é™ï¼Œä¸”åŸå§‹æ•°æ®å­˜åœ¨å™ªå£°å’Œå¼‚è´¨æ€§æŒ‘æˆ˜ï¼Œéœ€è¦å¼€å‘ä¸“é—¨çš„å¤šæ¨¡æ€å¤„ç†æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨ç»¼åˆé¢„å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬é¢éƒ¨å…³é”®ç‚¹æ£€æµ‹ã€å”‡éƒ¨åŒºåŸŸéš”ç¦»ä¸ç¨³å®šåŒ–ï¼Œä»¥åŠå¤šæ¨¡æ€æ ·æœ¬è´¨é‡å®šé‡è¯„ä¼°ã€‚åŸºäºè´¨é‡åˆ†æ•°çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥å…ˆè®­ç»ƒå¹²å‡€é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼Œé€æ­¥å¼•å…¥å›°éš¾æ ·æœ¬å¢å¼ºé²æ£’æ€§ã€‚ç»“åˆSigLIPç¼–ç å™¨å’Œç»Ÿä¸€3Dé‡é‡‡æ ·å™¨é«˜æ•ˆç¼–ç é«˜å¸§ç‡å”‡éƒ¨è¿åŠ¨ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸“é—¨æ„å»ºçš„HI-Dialogueæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHI-TransPAåœ¨å­—é¢å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦æ–¹é¢å‡è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚è¯¥æ¨¡å‹æˆåŠŸå®ç°äº†å•ä¸€å¤šæ¨¡æ€æ¡†æ¶å†…çš„ç¿»è¯‘å’Œå¯¹è¯åŠŸèƒ½ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶ä¸ºå°†Omni-Modelåº”ç”¨äºè¾…åŠ©é€šä¿¡æŠ€æœ¯å¥ å®šäº†åŸºç¡€ï¼Œæä¾›äº†ç«¯åˆ°ç«¯å»ºæ¨¡æ¡†æ¶å’Œå…³é”®å¤„ç†å·¥å…·ã€‚å·¥ä½œå±•ç¤ºäº†å¤šæ¨¡æ€èåˆåœ¨å¬éšœè¾…åŠ©æŠ€æœ¯ä¸­çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒå’ŒåŸºç¡€è®¾æ–½ã€‚</p>
<hr />
<h4 id="abstract_49">ğŸ“„ Abstract</h4>
<p>To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with high-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existing Omni-Models to hearing-impaired speech, we construct a comprehensive preprocessing and curation pipeline that detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses multimodal sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt a SigLIP encoder combined with a Unified 3D-Resampler to efficiently encode high-frame-rate lip motion. Experiments on our purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. This work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research.</p>
<h3 id="51-do-language-models-associate-sound-with-meaning-a-multimodal-study-of-sound-symbolism">[51] <a href="https://arxiv.org/abs/2511.10045">Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism</a></h3>
<p><em>Jinhong Jeong, Sunghyun Lee, Jaeyoung Lee, Seonah Han, Youngjae Yu</em></p>
<h4 id="tldr_50">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºLEX-ICONæ•°æ®é›†ï¼Œé€šè¿‡åˆ†æå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³è±¡å¾æ€§ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œé¦–æ¬¡å®ç°äº†å¯¹æ¨¡å‹è¯­éŸ³è±¡ä¼¼æ€§è§£é‡Šèƒ½åŠ›çš„å¤§è§„æ¨¡å®šé‡åˆ†æã€‚ç ”ç©¶å‘ç°MLLMså±•ç°å‡ºä¸è¯­è¨€å­¦ç†è®ºä¸€è‡´çš„è¯­éŸ³ç›´è§‰ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹å¯¹æ ‡å¿—æ€§éŸ³ç´ çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚</p>
<hr />
<h4 id="detailed-summary_50">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯­éŸ³è±¡å¾æ€§æ˜¯è¯­è¨€å­¦ä¸­ç ”ç©¶è¯­éŸ³å½¢å¼ä¸æ„ä¹‰ä¹‹é—´éä»»æ„æ€§å…³è”çš„æ¦‚å¿µï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¦‚ä½•è§£é‡Šäººç±»è¯­è¨€ä¸­çš„å¬è§‰ä¿¡æ¯ï¼Œå¡«è¡¥äº†AIæ¨¡å‹åœ¨è¯­éŸ³è±¡ä¼¼æ€§ç†è§£æ–¹é¢çš„ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†LEX-ICONæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªè‹±è¯­ã€æ³•è¯­ã€æ—¥è¯­å’ŒéŸ©è¯­çš„8,052ä¸ªè‡ªç„¶è¯­è¨€è¯æ±‡å’Œ2,930ä¸ªç³»ç»Ÿæ„å»ºçš„ä¼ªè¯ï¼Œæ ‡æ³¨äº†25ä¸ªè¯­ä¹‰ç»´åº¦çš„ç‰¹å¾ã€‚é€šè¿‡æµ‹é‡éŸ³ç´ çº§æ³¨æ„åŠ›åˆ†æ•°ï¼Œåˆ†ææ¨¡å‹åœ¨ä¸åŒè¾“å…¥å½¢å¼ï¼ˆæ­£å­—æ³•å’Œå›½é™…éŸ³æ ‡ï¼‰å’Œå¬è§‰æ¨¡æ€ä¸‹çš„å±‚çº§ä¿¡æ¯å¤„ç†æ¨¡å¼ã€‚</p>
<p><strong>Result:</strong> å…³é”®å‘ç°è¡¨æ˜MLLMsåœ¨å¤šä¸ªè¯­ä¹‰ç»´åº¦ä¸Šå±•ç°å‡ºä¸ç°æœ‰è¯­è¨€å­¦ç ”ç©¶ä¸€è‡´çš„è¯­éŸ³ç›´è§‰ï¼ŒåŒæ—¶æ­ç¤ºäº†æ¨¡å‹å¯¹æ ‡å¿—æ€§éŸ³ç´ çš„æ³¨æ„åŠ›èšç„¦æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼é€šè¿‡éŸ³ç´ è¯­ä¹‰æ³¨æ„åŠ›åˆ†æå¾—ä»¥é‡åŒ–ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶åœ¨äººå·¥æ™ºèƒ½ä¸è®¤çŸ¥è¯­è¨€å­¦ä¹‹é—´å»ºç«‹äº†æ¡¥æ¢ï¼Œä¸ºMLLMsçš„å¯è§£é‡Šæ€§ç ”ç©¶æä¾›äº†é¦–ä¸ªå¤§è§„æ¨¡å®šé‡åˆ†ææ¡†æ¶ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†è¯­éŸ³è±¡å¾æ€§æ—¶çš„è®¤çŸ¥æœºåˆ¶ï¼Œä¸ºç†è§£å¤šæ¨¡æ€æ¨¡å‹çš„è¯­ä¹‰å¤„ç†èƒ½åŠ›æä¾›äº†æ–°è§†è§’ã€‚</p>
<hr />
<h4 id="abstract_50">ğŸ“„ Abstract</h4>
<p>Sound symbolism is a linguistic concept that refers to non-arbitrary associations between phonetic forms and their meanings. We suggest that this can be a compelling probe into how Multimodal Large Language Models (MLLMs) interpret auditory information in human languages. We investigate MLLMs' performance on phonetic iconicity across textual (orthographic and IPA) and auditory forms of inputs with up to 25 semantic dimensions (e.g., sharp vs. round), observing models' layer-wise information processing by measuring phoneme-level attention fraction scores. To this end, we present LEX-ICON, an extensive mimetic word dataset consisting of 8,052 words from four natural languages (English, French, Japanese, and Korean) and 2,930 systematically constructed pseudo-words, annotated with semantic features applied across both text and audio modalities. Our key findings demonstrate (1) MLLMs' phonetic intuitions that align with existing linguistic research across multiple semantic dimensions and (2) phonosemantic attention patterns that highlight models' focus on iconic phonemes. These results bridge domains of artificial intelligence and cognitive linguistics, providing the first large-scale, quantitative analyses of phonetic iconicity in terms of MLLMs' interpretability.</p>
<h3 id="52-format-matters-the-robustness-of-multimodal-llms-in-reviewing-evidence-from-tables-and-charts">[52] <a href="https://arxiv.org/abs/2511.10075">Format Matters: The Robustness of Multimodal LLMs in Reviewing Evidence from Tables and Charts</a></h3>
<p><em>Xanh Ho, Yun-Ang Wu, Sunisth Kumar, Florian Boudin, Atsuhiro Takasu, Akiko Aizawa</em></p>
<h4 id="tldr_51">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶è¯„ä¼°äº†12ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦å£°æ˜éªŒè¯ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°å½“å‰æ¨¡å‹åœ¨å¤„ç†è¡¨æ ¼è¯æ®æ—¶è¡¨ç°æ›´å¥½ï¼Œè€Œåœ¨å›¾è¡¨è¯æ®ä¸Šè¡¨ç°ä¸ä½³ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„å…³é”®å·®è·ã€‚</p>
<hr />
<h4 id="detailed-summary_51">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€æäº¤çš„ç§‘å­¦è®ºæ–‡æ•°é‡ä¸æ–­å¢åŠ ï¼Œå¯¹èƒ½å¤ŸååŠ©å®¡ç¨¿äººè¯„ä¼°ç ”ç©¶å£°æ˜çš„ç³»ç»Ÿéœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚å®éªŒç»“æœæ˜¯ç§‘å­¦å·¥ä½œçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œé€šå¸¸ä»¥è¡¨æ ¼æˆ–å›¾è¡¨ç­‰ä¸åŒæ ¼å¼å‘ˆç°ã€‚ç†è§£å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè¯æ®æ ¼å¼ä¸‹éªŒè¯ç§‘å­¦å£°æ˜çš„é²æ£’æ€§æ˜¯ä¸€ä¸ªé‡è¦ä¸”å°šæœªå……åˆ†æ¢ç´¢çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶è®¾è®¡å¹¶å®æ–½äº†ä¸€ç³»åˆ—å®éªŒï¼Œè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½¿ç”¨è¡¨æ ¼å’Œå›¾è¡¨ä½œä¸ºè¯æ®éªŒè¯ç§‘å­¦å£°æ˜çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ”¹ç¼–äº†ä¸¤ä¸ªç°æœ‰çš„ç§‘å­¦è®ºæ–‡æ•°æ®é›†ï¼ŒåŠ å…¥äº†å¤šæ¨¡æ€å£°æ˜éªŒè¯ä»»åŠ¡æ‰€éœ€çš„æ³¨é‡Šå’Œç»“æ„ã€‚ä½¿ç”¨è¿™ä¸ªæ”¹ç¼–çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†12ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨å¤„ç†åŸºäºè¡¨æ ¼çš„è¯æ®æ—¶è¡¨ç°æ›´å¥½ï¼Œè€Œåœ¨åŸºäºå›¾è¡¨çš„è¯æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚äººç±»è¯„ä¼°è¡¨æ˜äººç±»åœ¨ä¸¤ç§æ ¼å¼ä¸‹éƒ½ä¿æŒå¼ºåŠ²è¡¨ç°ï¼Œä¸æ¨¡å‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚åˆ†æè¿˜å‘ç°ï¼Œè¾ƒå°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆä½äº80äº¿å‚æ•°ï¼‰åœ¨åŸºäºè¡¨æ ¼å’ŒåŸºäºå›¾è¡¨çš„ä»»åŠ¡ä¹‹é—´è¡¨ç°å‡ºå¼±ç›¸å…³æ€§ï¼Œè¡¨æ˜è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</p>
<p><strong>Conclusion:</strong> è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ–¹é¢çš„å…³é”®å·®è·ã€‚å»ºè®®æœªæ¥çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åº”æ›´åŠ é‡è§†æ”¹è¿›å›¾è¡¨ç†è§£èƒ½åŠ›ï¼Œä»¥æ›´å¥½åœ°æ”¯æŒç§‘å­¦å£°æ˜éªŒè¯ã€‚ç ”ç©¶å¼ºè°ƒäº†æå‡æ¨¡å‹åœ¨å›¾è¡¨è¯æ®å¤„ç†æ–¹é¢çš„èƒ½åŠ›å¯¹äºç§‘å­¦è¯„ä¼°åº”ç”¨çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_51">ğŸ“„ Abstract</h4>
<p>With the growing number of submitted scientific papers, there is an increasing demand for systems that can assist reviewers in evaluating research claims. Experimental results are a core component of scientific work, often presented in varying formats such as tables or charts. Understanding how robust current multimodal large language models (multimodal LLMs) are at verifying scientific claims across different evidence formats remains an important and underexplored challenge. In this paper, we design and conduct a series of experiments to assess the ability of multimodal LLMs to verify scientific claims using both tables and charts as evidence. To enable this evaluation, we adapt two existing datasets of scientific papers by incorporating annotations and structures necessary for a multimodal claim verification task. Using this adapted dataset, we evaluate 12 multimodal LLMs and find that current models perform better with table-based evidence while struggling with chart-based evidence. We further conduct human evaluations and observe that humans maintain strong performance across both formats, unlike the models. Our analysis also reveals that smaller multimodal LLMs (under 8B) show weak correlation in performance between table-based and chart-based tasks, indicating limited cross-modal generalization. These findings highlight a critical gap in current models' multimodal reasoning capabilities. We suggest that future multimodal LLMs should place greater emphasis on improving chart understanding to better support scientific claim verification.</p>
<h3 id="53-analogical-structure-minimal-contextual-cues-and-contrastive-distractors-input-design-for-sample-efficient-linguistic-rule-induction">[53] <a href="https://arxiv.org/abs/2511.10441">Analogical Structure, Minimal Contextual Cues and Contrastive Distractors: Input Design for Sample-Efficient Linguistic Rule Induction</a></h3>
<p><em>Chunyang Jiang, Paola Merlo</em></p>
<h4 id="tldr_52">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç±»æ¯”èŒƒå¼ç»„ç»‡çš„è®¡ç®—æ–¹æ³•ï¼Œä½¿è½»é‡çº§æ¨¡å‹ä»…éœ€å°‘é‡æ•°æ®å³å¯è¾¾åˆ°ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¤çŸ¥å¯å‘çš„ç±»æ¯”ç»“æ„ã€å¯¹æ¯”å­¦ä¹ å’Œæœ€å°ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œåœ¨ä»…ä½¿ç”¨100ä¸ªç»“æ„åŒ–ç¤ºä¾‹çš„æƒ…å†µä¸‹å®ç°äº†F1=0.95çš„ä¼˜å¼‚è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_52">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ä¾èµ–æµ·é‡æ•°æ®è®­ç»ƒæ‰èƒ½è·å¾—å¼ºå¤§æ€§èƒ½ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢æ˜¯å¦é€šè¿‡ç±»æ¯”èŒƒå¼ç»„ç»‡ï¼Œä½¿è½»é‡çº§æ¨¡å‹èƒ½å¤Ÿä»¥æå°‘æ•°æ®è¾¾åˆ°åŒç­‰æ€§èƒ½æ°´å¹³ï¼Œè§£å†³æ•°æ®æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> å¼€å‘äº†ä¸€ç§åŸºäºä¸‰ä¸ªè®¤çŸ¥å¯å‘åŸåˆ™çš„è®¡ç®—æ–¹æ³•ï¼šç±»æ¯”ç»“æ„ã€å¯¹æ¯”å­¦ä¹ å’Œæœ€å°ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚ä½¿ç”¨BERT+CNNæ¶æ„çš„è½»é‡çº§æ¨¡å‹ï¼ˆ50ä¸‡å‚æ•°ï¼‰ï¼Œåœ¨ç»“æ„åŒ–å®Œå½¢å¡«ç©ºä»»åŠ¡ä¸­é€šè¿‡ç±»æ¯”æ¨¡å¼å’Œå¯¹æ¯”æ›¿ä»£é¡¹è¯†åˆ«æ­£ç¡®çš„å¥å­è¡¥å…¨ã€‚</p>
<p><strong>Result:</strong> åœ¨è‹±è¯­ä½¿å½¹/èµ·å§‹äº¤æ›¿çš„ä»…100ä¸ªç»“æ„åŒ–ç¤ºä¾‹ä¸Šè®­ç»ƒï¼Œè½»é‡çº§æ¨¡å‹è¾¾åˆ°F1=0.95ï¼Œä¼˜äºé›¶æ ·æœ¬GPT-o3çš„F1=0.87ã€‚æ¶ˆèç ”ç©¶è¯å®ç±»æ¯”ç»„ç»‡å’Œå¯¹æ¯”ç»“æ„æå‡æ€§èƒ½ï¼Œåœ¨ä¸åŒæ¶æ„ä¸­å§‹ç»ˆä¼˜äºéšæœºæ‰“ä¹±çš„åŸºçº¿ã€‚è·¨ç°è±¡éªŒè¯ä½¿ç”¨æœªæŒ‡å®šå®¾è¯­äº¤æ›¿é‡ç°äº†è¿™äº›æ•ˆç‡å¢ç›Šã€‚</p>
<p><strong>Conclusion:</strong> ç±»æ¯”èŒƒå¼ç»„ç»‡èƒ½å¤Ÿå®ç°å…·æœ‰ç«äº‰åŠ›çš„è¯­è¨€è§„åˆ™å­¦ä¹ ï¼Œæ‰€éœ€æ•°æ®é‡æ¯”ä¼ ç»Ÿæ–¹æ³•å°‘å‡ ä¸ªæ•°é‡çº§ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†è®¤çŸ¥å¯å‘çš„ç»“æ„åŒ–å­¦ä¹ åœ¨æ•°æ®æ•ˆç‡æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„è‡ªç„¶è¯­è¨€å¤„ç†æä¾›äº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_52">ğŸ“„ Abstract</h4>
<p>Large language models achieve strong performance through training on vast datasets. Can analogical paradigm organization enable lightweight models to match this performance with minimal data? We develop a computational approach implementing three cognitive-inspired principles: analogical structure, contrastive learning, and minimal contextual cues. We test this approach with structured completion tasks where models identify correct sentence completions from analogical patterns with contrastive alternatives. Training lightweight models (BERT+CNN, $0.5M$ parameters) on only one hundred structured examples of English causative/inchoative alternations achieves $F1=0.95$, outperforming zero-shot \texttt{GPT-o3} ($F1=0.87$). Ablation studies confirm that analogical organization and contrastive structure improve performance, consistently surpassing randomly shuffled baselines across architectures. Cross-phenomenon validation using unspecified object alternations replicates these efficiency gains, confirming approach robustness. Our results show that analogical paradigm organization enables competitive linguistic rule learning with orders of magnitude less data than conventional approaches require.</p>
<h3 id="54-urag-unified-retrieval-and-generation-in-multimodal-llms-for-efficient-long-document-understanding">[54] <a href="https://arxiv.org/abs/2511.10552">URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding</a></h3>
<p><em>Yongxin Shi, Jiapeng Wang, Zeyu Shan, Dezhi Peng, Zening Lin, Lianwen Jin</em></p>
<h4 id="tldr_53">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºURaGæ¡†æ¶ï¼Œé€šè¿‡ç»Ÿä¸€æ£€ç´¢ä¸ç”Ÿæˆè¿‡ç¨‹ï¼Œåˆ©ç”¨MLLMså†…åœ¨çš„è¯æ®å®šä½èƒ½åŠ›å®ç°é«˜æ•ˆé•¿æ–‡æ¡£ç†è§£ã€‚è¯¥æ–¹æ³•å°†æ—©æœŸTransformerå±‚è½¬æ¢ä¸ºè½»é‡çº§è¯æ®é€‰æ‹©å™¨ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚</p>
<hr />
<h4 id="detailed-summary_53">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿æ–‡æ¡£ç†è§£æ–¹é¢é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå¤§é‡æ— å…³å†…å®¹é€ æˆçš„ä¿¡æ¯å¹²æ‰°ï¼Œä»¥åŠåŸºäºTransformeræ¶æ„çš„äºŒæ¬¡è®¡ç®—æˆæœ¬ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šç‰ºç‰²ç»†ç²’åº¦ç»†èŠ‚çš„ä»¤ç‰Œå‹ç¼©ï¼Œä»¥åŠå¢åŠ ç³»ç»Ÿå¤æ‚æ€§å¹¶é˜»ç¢ç«¯åˆ°ç«¯ä¼˜åŒ–çš„å¤–éƒ¨æ£€ç´¢å™¨å¼•å…¥ã€‚</p>
<p><strong>Method:</strong> æå‡ºURaGæ¡†æ¶ï¼Œå¼•å…¥è½»é‡çº§è·¨æ¨¡æ€æ£€ç´¢æ¨¡å—ï¼Œå°†æ—©æœŸTransformerå±‚è½¬æ¢ä¸ºé«˜æ•ˆè¯æ®é€‰æ‹©å™¨ï¼Œè¯†åˆ«å¹¶ä¿ç•™æœ€ç›¸å…³é¡µé¢åŒæ—¶ä¸¢å¼ƒæ— å…³å†…å®¹ã€‚è¿™ç§è®¾è®¡ä½¿æ·±å±‚å±‚èƒ½å¤Ÿå°†è®¡ç®—èµ„æºé›†ä¸­åœ¨ç›¸å…³ä¿¡æ¯ä¸Šï¼Œå®ç°æ£€ç´¢ä¸ç”Ÿæˆçš„ç»Ÿä¸€ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒURaGåœ¨å®ç°æœ€å…ˆè¿›æ€§èƒ½çš„åŒæ—¶ï¼Œå°†è®¡ç®—å¼€é”€é™ä½äº†44-56%ã€‚è¯¥æ¡†æ¶åœ¨é•¿æ–‡æ¡£ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜MLLMså…·æœ‰ç±»ä¼¼äººç±»çš„ä»ç²—åˆ°ç»†æ¨ç†æ¨¡å¼ï¼Œæ—©æœŸå±‚å¹¿æ³›å…³æ³¨æ–‡æ¡£ï¼Œæ·±å±‚å±‚èšç„¦ç›¸å…³è¯æ®é¡µé¢ã€‚URaGæˆåŠŸåˆ©ç”¨è¿™ç§å†…åœ¨èƒ½åŠ›å®ç°é«˜æ•ˆæ£€ç´¢ï¼Œä¸ºé•¿æ–‡æ¡£ç†è§£æä¾›äº†ç«¯åˆ°ç«¯çš„ä¼˜åŒ–è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹è‡ªèº«æ¨ç†è¿‡ç¨‹çš„å¯åˆ©ç”¨æ€§ã€‚</p>
<hr />
<h4 id="abstract_53">ğŸ“„ Abstract</h4>
<p>Recent multimodal large language models (MLLMs) still struggle with long document understanding due to two fundamental challenges: information interference from abundant irrelevant content, and the quadratic computational cost of Transformer-based architectures. Existing approaches primarily fall into two categories: token compression, which sacrifices fine-grained details; and introducing external retrievers, which increase system complexity and prevent end-to-end optimization. To address these issues, we conduct an in-depth analysis and observe that MLLMs exhibit a human-like coarse-to-fine reasoning pattern: early Transformer layers attend broadly across the document, while deeper layers focus on relevant evidence pages. Motivated by this insight, we posit that the inherent evidence localization capabilities of MLLMs can be explicitly leveraged to perform retrieval during the reasoning process, facilitating efficient long document understanding. To this end, we propose URaG, a simple-yet-effective framework that Unifies Retrieval and Generation within a single MLLM. URaG introduces a lightweight cross-modal retrieval module that converts the early Transformer layers into an efficient evidence selector, identifying and preserving the most relevant pages while discarding irrelevant content. This design enables the deeper layers to concentrate computational resources on pertinent information, improving both accuracy and efficiency. Extensive experiments demonstrate that URaG achieves state-of-the-art performance while reducing computational overhead by 44-56%. The code is available at https://github.com/shi-yx/URaG.</p>
<h3 id="55-mined-prompting-and-metadata-guided-generation-for-wound-care-visual-question-answering">[55] <a href="https://arxiv.org/abs/2511.10591">Mined Prompting and Metadata-Guided Generation for Wound Care Visual Question Answering</a></h3>
<p><em>Bavana Durgapraveen, Sornaraj Sivasankaran, Abhinand Balachandran, Sriram Rajkumar</em></p>
<h4 id="tldr_54">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é’ˆå¯¹MEDIQA-WV 2025å…±äº«ä»»åŠ¡ï¼Œæå‡ºäº†ä¸¤ç§äº’è¡¥æ–¹æ³•ç”¨äºä¼¤å£æŠ¤ç†æŸ¥è¯¢çš„æ–‡æœ¬å›å¤ç”Ÿæˆï¼šåŸºäºåµŒå…¥æ£€ç´¢çš„æŒ–æ˜æç¤ºç­–ç•¥å’ŒåŸºäºå…ƒæ•°æ®é¢„æµ‹çš„å¼•å¯¼ç”Ÿæˆæ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†å›å¤çš„ç›¸å…³æ€§å’Œä¸´åºŠç²¾ç¡®åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_54">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¼‚æ­¥è¿œç¨‹åŒ»ç–—çš„å¿«é€Ÿå‘å±•åŠ å‰§äº†åŒ»æŠ¤äººå‘˜çš„å·¥ä½œè´Ÿæ‹…ï¼Œè¿«åˆ‡éœ€è¦èƒ½å¤Ÿé«˜æ•ˆå¤„ç†æ‚£è€…æŸ¥è¯¢çš„AIè¾…åŠ©ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“åˆå›¾åƒä¿¡æ¯çš„ä¼¤å£æŠ¤ç†åœºæ™¯ä¸­ï¼Œç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆå‡†ç¡®ä¸´åºŠå›å¤æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> ç¬¬ä¸€ç§æ–¹æ³•é‡‡ç”¨æŒ–æ˜æç¤ºç­–ç•¥ï¼Œé€šè¿‡åµŒå…¥è®­ç»ƒæ•°æ®å¹¶æ£€ç´¢æœ€ç›¸ä¼¼çš„top-kç¤ºä¾‹ä½œä¸ºå°‘æ ·æœ¬æ¼”ç¤ºï¼›ç¬¬äºŒç§æ–¹æ³•åŸºäºå…ƒæ•°æ®æ¶ˆèç ”ç©¶è¯†åˆ«å‡ºå››ä¸ªå…³é”®å…ƒæ•°æ®å±æ€§ï¼Œè®­ç»ƒåˆ†ç±»å™¨é¢„æµ‹è¿™äº›å±æ€§å¹¶åŸºäºé¢„æµ‹ç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´ç”Ÿæˆæµç¨‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜æŒ–æ˜æç¤ºç­–ç•¥æ˜¾è‘—æå‡äº†å›å¤ç›¸å…³æ€§ï¼Œè€Œå…ƒæ•°æ®å¼•å¯¼çš„ç”Ÿæˆæ–¹æ³•è¿›ä¸€æ­¥ä¼˜åŒ–äº†ä¸´åºŠç²¾ç¡®åº¦ï¼Œä¸¤ç§æ–¹æ³•åœ¨ä¼¤å£æŠ¤ç†å›å¤ç”Ÿæˆä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Conclusion:</strong> è¿™äº›æ–¹æ³•ä¸ºå¼€å‘å¯é é«˜æ•ˆçš„AIé©±åŠ¨ä¼¤å£æŠ¤ç†æ”¯æŒå·¥å…·æŒ‡æ˜äº†æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå±•ç¤ºäº†ç»“åˆæ£€ç´¢å¢å¼ºå’Œå…ƒæ•°æ®å¼•å¯¼åœ¨åŒ»ç–—æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„ååŒä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_54">ğŸ“„ Abstract</h4>
<p>The rapid expansion of asynchronous remote care has intensified provider workload, creating demand for AI systems that can assist clinicians in managing patient queries more efficiently. The MEDIQA-WV 2025 shared task addresses this challenge by focusing on generating free-text responses to wound care queries paired with images. In this work, we present two complementary approaches developed for the English track. The first leverages a mined prompting strategy, where training data is embedded and the top-k most similar examples are retrieved to serve as few-shot demonstrations during generation. The second approach builds on a metadata ablation study, which identified four metadata attributes that consistently enhance response quality. We train classifiers to predict these attributes for test cases and incorporate them into the generation pipeline, dynamically adjusting outputs based on prediction confidence. Experimental results demonstrate that mined prompting improves response relevance, while metadata-guided generation further refines clinical precision. Together, these methods highlight promising directions for developing AI-driven tools that can provide reliable and efficient wound care support.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="56-slidebot-a-multi-agent-framework-for-generating-informative-reliable-multi-modal-presentations">[56] <a href="https://arxiv.org/abs/2511.09804">SlideBot: A Multi-Agent Framework for Generating Informative, Reliable, Multi-Modal Presentations</a></h3>
<p><em>Eric Xie, Danielle Waterfield, Michael Kennedy, Aidong Zhang</em></p>
<h4 id="tldr_55">ğŸ§© TL;DR</h4>
<p>SlideBotæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–å¤šä»£ç†å¹»ç¯ç‰‡ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡é›†æˆå¤§è¯­è¨€æ¨¡å‹ã€æ£€ç´¢æœºåˆ¶å’Œä»£ç ç”ŸæˆæŠ€æœ¯ï¼Œè§£å†³äº†æ•™è‚²é¢†åŸŸå¹»ç¯ç‰‡è‡ªåŠ¨ç”Ÿæˆçš„å¯é æ€§å’Œä¿¡æ¯è´¨é‡é—®é¢˜ã€‚è¯¥ç³»ç»ŸåŸºäºè®¤çŸ¥è´Ÿè·ç†è®ºå’Œå¤šåª’ä½“å­¦ä¹ ç†è®ºï¼Œåœ¨AIå’Œç”Ÿç‰©åŒ»å­¦æ•™è‚²è¯„ä¼°ä¸­å±•ç°å‡ºå“è¶Šçš„æ¦‚å¿µå‡†ç¡®æ€§å’Œæ•™å­¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="detailed-summary_55">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¹»ç¯ç‰‡ç”Ÿæˆè§£å†³æ–¹æ¡ˆåœ¨ç”Ÿæˆå¯é ä¸”ä¿¡æ¯ä¸°å¯Œçš„è¾“å‡ºæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šæ¨¡æ€å†…å®¹åˆ›å»ºå’Œç²¾ç¡®é¢†åŸŸç‰¹å®šä¿¡æ¯æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æ•™è‚²åº”ç”¨ä¸­çš„ä»·å€¼ã€‚</p>
<p><strong>Method:</strong> SlideBoté‡‡ç”¨æ¨¡å—åŒ–å¤šä»£ç†æ¡†æ¶ï¼Œé›†æˆæ£€ç´¢ã€ç»“æ„åŒ–è§„åˆ’å’Œä»£ç ç”ŸæˆæŠ€æœ¯ï¼Œé€šè¿‡ä¸“é—¨ä»£ç†åä½œæ£€ç´¢ä¿¡æ¯ã€æ€»ç»“å†…å®¹ã€ç”Ÿæˆå›¾è¡¨å’Œä½¿ç”¨LaTeXæ ¼å¼åŒ–å¹»ç¯ç‰‡ã€‚ç³»ç»ŸåŸºäºè®¤çŸ¥è´Ÿè·ç†è®ºå’Œå¤šåª’ä½“å­¦ä¹ ç†è®ºï¼Œä½¿ç”¨ç»“æ„åŒ–è§„åˆ’ç®¡ç†å†…åœ¨è´Ÿè·ï¼Œå¹¶é€šè¿‡ä¸€è‡´çš„å¯è§†åŒ–å®å‡å°‘å¤–åœ¨è´Ÿè·ä»¥å¢å¼ºåŒé€šé“å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> åœ¨AIå’Œç”Ÿç‰©åŒ»å­¦æ•™è‚²é¢†åŸŸçš„ä¸“å®¶å’Œå­¦ç”Ÿè¯„ä¼°ä¸­ï¼ŒSlideBotåœ¨æ¦‚å¿µå‡†ç¡®æ€§ã€æ¸…æ™°åº¦å’Œæ•™å­¦ä»·å€¼æ–¹é¢æŒç»­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜å…¶èƒ½å¤Ÿæœ‰æ•ˆæå‡å¹»ç¯ç‰‡å‡†å¤‡æ•ˆç‡åŒæ—¶ç¡®ä¿å†…å®¹çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚</p>
<p><strong>Conclusion:</strong> SlideBotå±•ç¤ºäº†é€šè¿‡å¤šä»£ç†åä½œå’Œè®¤çŸ¥ç†è®ºæŒ‡å¯¼çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ•™è‚²å¹»ç¯ç‰‡ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ï¼Œä¸ºé«˜ç­‰æ•™è‚²ä¸­çš„è‡ªé€‚åº”å†…å®¹åˆ›å»ºæä¾›äº†å¯è¡Œè§£å†³æ–¹æ¡ˆï¼Œå¹¶å¼ºè°ƒäº†ç»“åˆæ•™å­¦ç†è®ºå’Œå®é™…åº”ç”¨éœ€æ±‚çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_55">ğŸ“„ Abstract</h4>
<p>Large Language Models (LLMs) have shown immense potential in education, automating tasks like quiz generation and content summarization. However, generating effective presentation slides introduces unique challenges due to the complexity of multimodal content creation and the need for precise, domain-specific information. Existing LLM-based solutions often fail to produce reliable and informative outputs, limiting their educational value. To address these limitations, we introduce SlideBot - a modular, multi-agent slide generation framework that integrates LLMs with retrieval, structured planning, and code generation. SlideBot is organized around three pillars: informativeness, ensuring deep and contextually grounded content; reliability, achieved by incorporating external sources through retrieval; and practicality, which enables customization and iterative feedback through instructor collaboration. It incorporates evidence-based instructional design principles from Cognitive Load Theory (CLT) and the Cognitive Theory of Multimedia Learning (CTML), using structured planning to manage intrinsic load and consistent visual macros to reduce extraneous load and enhance dual-channel learning. Within the system, specialized agents collaboratively retrieve information, summarize content, generate figures, and format slides using LaTeX, aligning outputs with instructor preferences through interactive refinement. Evaluations from domain experts and students in AI and biomedical education show that SlideBot consistently enhances conceptual accuracy, clarity, and instructional value. These findings demonstrate SlideBot's potential to streamline slide preparation while ensuring accuracy, relevance, and adaptability in higher education.</p>
<h3 id="57-egoems-a-high-fidelity-multimodal-egocentric-dataset-for-cognitive-assistance-in-emergency-medical-services">[57] <a href="https://arxiv.org/abs/2511.09894">EgoEMS: A High-Fidelity Multimodal Egocentric Dataset for Cognitive Assistance in Emergency Medical Services</a></h3>
<p><em>Keshara Weerasinghe, Xueren Ge, Tessa Heick, Lahiru Nuwan Wijayasingha, Anthony Cortez, Abhishek Satpathy, John Stankovic, Homa Alemzadeh</em></p>
<h4 id="tldr_56">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†EgoEMSï¼Œè¿™æ˜¯é¦–ä¸ªç«¯åˆ°ç«¯ã€é«˜ä¿çœŸã€å¤šæ¨¡æ€ã€å¤šå‚ä¸è€…çš„æ€¥æ•‘åŒ»ç–—æœåŠ¡æ•°æ®é›†ï¼ŒåŒ…å«233ä¸ªæ¨¡æ‹Ÿç´§æ€¥åœºæ™¯ä¸­è¶…è¿‡20å°æ—¶çš„è‡ªæˆ‘ä¸­å¿ƒè§†è§’æ•°æ®ï¼Œæ—¨åœ¨å¼€å‘AIè®¤çŸ¥åŠ©æ‰‹ä»¥æ”¯æŒæ€¥æ•‘äººå‘˜çš„å®æ—¶å†³ç­–ã€‚</p>
<hr />
<h4 id="detailed-summary_56">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ€¥æ•‘åŒ»ç–—æœåŠ¡ä¸­çš„æ€¥æ•‘äººå‘˜åœ¨é«˜é£é™©æƒ…å†µä¸‹é¢ä¸´å·¨å¤§çš„è®¤çŸ¥è´Ÿæ‹…ï¼Œè€Œç°æœ‰çš„AIè®¤çŸ¥åŠ©æ‰‹ç¼ºä¹çœŸå®ã€å…¨é¢çš„æ•°æ®é›†æ¥æ”¯æŒå®æ—¶æ•°æ®æ”¶é›†å’Œå†³ç­–åˆ¶å®šï¼Œè¿™é™åˆ¶äº†æ™ºèƒ½EMSç³»ç»Ÿçš„å‘å±•ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿä¸EMSä¸“å®¶åˆä½œå¼€å‘äº†ä¸€ä¸ªå¼€æºã€ä½æˆæœ¬ä¸”å¯å¤åˆ¶çš„æ•°æ®æ”¶é›†ç³»ç»Ÿï¼Œé‡‡é›†äº†62åå‚ä¸è€…ï¼ˆåŒ…æ‹¬46åEMSä¸“ä¸šäººå‘˜ï¼‰åœ¨233ä¸ªæ¨¡æ‹Ÿç´§æ€¥åœºæ™¯ä¸­çš„å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶æä¾›äº†å…³é”®æ­¥éª¤æ ‡æ³¨ã€å¸¦æ—¶é—´æˆ³çš„éŸ³é¢‘è½¬å½•ä¸è¯´è¯äººåˆ†ç¦»ã€åŠ¨ä½œè´¨é‡æŒ‡æ ‡ä»¥åŠå¸¦åˆ†å‰²æ©ç çš„è¾¹ç•Œæ¡†ã€‚</p>
<p><strong>Result:</strong> EgoEMSæ•°æ®é›†åŒ…å«è¶…è¿‡20å°æ—¶çš„çœŸå®EMSæ´»åŠ¨è®°å½•ï¼Œæ¶µç›–äº†å“åº”è€…ä¸æ‚£è€…ä¹‹é—´çš„çœŸå®äº’åŠ¨åŠ¨æ€ï¼Œå¹¶å»ºç«‹äº†ä¸€å¥—ç”¨äºå®æ—¶å¤šæ¨¡æ€å…³é”®æ­¥éª¤è¯†åˆ«å’ŒåŠ¨ä½œè´¨é‡è¯„ä¼°çš„åŸºå‡†æµ‹è¯•ï¼Œä¸ºå¼€å‘EMS AIæ”¯æŒå·¥å…·æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Conclusion:</strong> EgoEMSæ•°æ®é›†å¡«è¡¥äº†æ™ºèƒ½EMSç³»ç»Ÿå¼€å‘ä¸­çš„æ•°æ®ç©ºç™½ï¼Œé€šè¿‡å¼ºè°ƒçœŸå®æ€§å’Œä¸“ä¸šæ ‡å‡†å¯¹é½ï¼Œä¸ºç ”ç©¶ç¤¾åŒºæä¾›äº†æ¨åŠ¨æ™ºèƒ½EMSç³»ç»Ÿå‘å±•çš„å…³é”®èµ„æºï¼Œæœ‰æœ›æœ€ç»ˆæ”¹å–„æ‚£è€…æ²»ç–—æ•ˆæœã€‚</p>
<hr />
<h4 id="abstract_56">ğŸ“„ Abstract</h4>
<p>Emergency Medical Services (EMS) are critical to patient survival in emergencies, but first responders often face intense cognitive demands in high-stakes situations. AI cognitive assistants, acting as virtual partners, have the potential to ease this burden by supporting real-time data collection and decision making. In pursuit of this vision, we introduce EgoEMS, the first end-to-end, high-fidelity, multimodal, multiperson dataset capturing over 20 hours of realistic, procedural EMS activities from an egocentric view in 233 simulated emergency scenarios performed by 62 participants, including 46 EMS professionals. Developed in collaboration with EMS experts and aligned with national standards, EgoEMS is captured using an open-source, low-cost, and replicable data collection system and is annotated with keysteps, timestamped audio transcripts with speaker diarization, action quality metrics, and bounding boxes with segmentation masks. Emphasizing realism, the dataset includes responder-patient interactions reflecting real-world emergency dynamics. We also present a suite of benchmarks for real-time multimodal keystep recognition and action quality estimation, essential for developing AI support tools for EMS. We hope EgoEMS inspires the research community to push the boundaries of intelligent EMS systems and ultimately contribute to improved patient outcomes.</p>
<h3 id="58-learning-to-pose-problems-reasoning-driven-and-solver-adaptive-data-synthesis-for-large-reasoning-models">[58] <a href="https://arxiv.org/abs/2511.09907">Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models</a></h3>
<p><em>Yongxian Wei, Yilin Zhao, Li Shen, Xinrui Chen, Runxi Cheng, Sinan Du, Hao Yu, Gang Liu, Jiahong Yan, Chun Yuan, Dian Li</em></p>
<h4 id="tldr_57">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§èƒ½å¤Ÿæ˜¾å¼æ¨ç†å¹¶é€‚åº”æ±‚è§£å™¨èƒ½åŠ›çš„é—®é¢˜ç”Ÿæˆå™¨ï¼Œé€šè¿‡æ„å»ºç›¸å…³é—®é¢˜å¯¹å¹¶åˆ©ç”¨æ¨ç†æ¨¡å‹ç”Ÿæˆä¸­é—´é—®é¢˜è®¾è®¡æ€ç»´é“¾ï¼ŒåŒæ—¶å°†æ±‚è§£å™¨åé¦ˆä½œä¸ºå¥–åŠ±ä¿¡å·æ¥æ ¡å‡†é—®é¢˜éš¾åº¦ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚</p>
<hr />
<h4 id="detailed-summary_57">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ•°æ®åˆæˆæ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯å¿½ç•¥æ±‚è§£å™¨èƒ½åŠ›çš„ç›²ç›®ç”Ÿæˆå¯¼è‡´ä½ä»·å€¼é—®é¢˜ï¼Œæˆ–ä¾èµ–å¤æ‚æ•°æ®ç®¡é“æ¥å¹³è¡¡é—®é¢˜éš¾åº¦ï¼›äºŒæ˜¯é—®é¢˜ç”Ÿæˆç¼ºä¹æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´äº§ç”Ÿæµ…å±‚é—®é¢˜å˜ä½“ï¼Œæ— æ³•æœ‰æ•ˆæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æ„å»ºç›¸å…³é—®é¢˜å¯¹å¹¶é€šè¿‡æ¨ç†æ¨¡å‹ç”Ÿæˆä¸­é—´é—®é¢˜è®¾è®¡æ€ç»´é“¾ï¼Œä»è€Œå¼•å¯¼é—®é¢˜ç”Ÿæˆç­–ç•¥ã€‚åŒæ—¶å°†æ±‚è§£å™¨å¯¹åˆæˆé—®é¢˜çš„åé¦ˆä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä½¿ç”Ÿæˆå™¨èƒ½å¤Ÿæ ¡å‡†éš¾åº¦å¹¶äº§ç”Ÿæ¥è¿‘æ±‚è§£å™¨èƒ½åŠ›è¾¹ç•Œçš„äº’è¡¥é—®é¢˜ï¼Œå®ç°éš¾åº¦è‡ªé€‚åº”ã€‚</p>
<p><strong>Result:</strong> åœ¨10ä¸ªæ•°å­¦å’Œé€šç”¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¹³å‡æ€§èƒ½æå‡2.5%ï¼Œå¹¶èƒ½æ³›åŒ–åˆ°è¯­è¨€å’Œè§†è§‰è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡ååŒè¿›åŒ–ï¼Œæ±‚è§£å™¨åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒåèƒ½ä¸ºç”Ÿæˆå™¨æä¾›æ”¹è¿›çš„å¥–åŠ±ä¿¡å·ï¼Œè¿›ä¸€æ­¥å¸¦æ¥0.7%çš„æ€§èƒ½å¢ç›Šã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ˜¾å¼æ¨ç†å¼•å¯¼çš„é—®é¢˜ç”Ÿæˆä¸éš¾åº¦è‡ªé€‚åº”ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤§è§„æ¨¡æ¨ç†æ¨¡å‹è®­ç»ƒæä¾›äº†å¯æ‰©å±•çš„æ•°æ®åˆæˆæ–¹æ¡ˆã€‚ååŒè¿›åŒ–æœºåˆ¶å±•ç¤ºäº†ç”Ÿæˆå™¨ä¸æ±‚è§£å™¨ç›¸äº’ä¿ƒè¿›çš„æ½œåŠ›ï¼Œä¸ºæŒç»­æ”¹è¿›æ¨¡å‹æ€§èƒ½å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_57">ğŸ“„ Abstract</h4>
<p>Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.</p>
<h3 id="59-oida-qa-a-multimodal-benchmark-for-analyzing-the-opioid-industry-documents-archive">[59] <a href="https://arxiv.org/abs/2511.09914">OIDA-QA: A Multimodal Benchmark for Analyzing the Opioid Industry Documents Archive</a></h3>
<p><em>Xuan Shen, Brian Wingenroth, Zichao Wang, Jason Kuen, Wanrong Zhu, Ruiyi Zhang, Yiwei Wang, Lichun Ma, Anqi Liu, Hongfu Liu, Tong Sun, Kevin S. Hawkins, Kate Tasker, G. Caleb Alexander, Jiuxiang Gu</em></p>
<h4 id="tldr_58">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é’ˆå¯¹é˜¿ç‰‡ç±»è¯ç‰©å±æœºç›¸å…³æ–‡æ¡£åˆ†ææŒ‘æˆ˜ï¼Œå¼€å‘äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’ŒåŸºå‡†æ•°æ®é›†ï¼Œé€šè¿‡æ•´åˆæ–‡æœ¬ã€è§†è§‰å’Œå¸ƒå±€ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†æ–‡æ¡£ä¿¡æ¯æå–å’Œé—®ç­”ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_58">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é˜¿ç‰‡ç±»è¯ç‰©å±æœºæ­ç¤ºäº†ç›‘ç®¡ç³»ç»Ÿã€åŒ»ç–—å®è·µå’Œä¼ä¸šæ²»ç†ç­‰å¤šæ–¹é¢çš„ç³»ç»Ÿæ€§ç¼ºé™·ï¼Œä½†åˆ†æè¿™äº›ç›¸äº’å…³è”ç³»ç»Ÿçš„å¤±æ•ˆéœ€è¦åˆ›æ–°çš„åˆ†ææ–¹æ³•æ¥å¤„ç†UCSF-JHUé˜¿ç‰‡ç±»è¯ç‰©è¡Œä¸šæ–‡æ¡£æ¡£æ¡ˆä¸­çš„å¤§é‡æ•°æ®å’Œæ–‡æ¡£ã€‚è¿™äº›åŒ»ç–—ç›¸å…³æ³•å¾‹å’Œä¼ä¸šæ–‡æ¡£çš„å¤æ‚æ€§ã€å¤šæ¨¡æ€ç‰¹æ€§å’Œä¸“ä¸šç‰¹å¾è¦æ±‚å¼€å‘é’ˆå¯¹ç‰¹å®šæ•°æ®ç±»å‹å’Œè¯¦ç»†æ ‡æ³¨çš„å…ˆè¿›æ–¹æ³•å’Œæ¨¡å‹ï¼Œä»¥ç¡®ä¿åˆ†æçš„ç²¾ç¡®æ€§å’Œä¸“ä¸šæ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é€šè¿‡æŒ‰æ–‡æ¡£å±æ€§ç»„ç»‡åŸå§‹æ•°æ®é›†ï¼Œæ„å»ºåŒ…å«40ä¸‡è®­ç»ƒæ–‡æ¡£å’Œ1ä¸‡æµ‹è¯•æ–‡æ¡£çš„åŸºå‡†ï¼Œä»æ¯ä¸ªæ–‡æ¡£ä¸­æå–ä¸°å¯Œçš„å¤šæ¨¡æ€ä¿¡æ¯åŒ…æ‹¬æ–‡æœ¬å†…å®¹ã€è§†è§‰å…ƒç´ å’Œå¸ƒå±€ç»“æ„ä»¥æ•è·å…¨é¢ç‰¹å¾ã€‚ä½¿ç”¨å¤šä¸ªAIæ¨¡å‹ç”ŸæˆåŒ…å«36ä¸‡è®­ç»ƒé—®ç­”å¯¹å’Œ1ä¸‡æµ‹è¯•é—®ç­”å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¼€å‘é¢†åŸŸç‰¹å®šçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶æ¢ç´¢å¤šæ¨¡æ€è¾“å…¥å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚ä¸ºæé«˜å›ç­”å‡†ç¡®æ€§ï¼Œå°†å†å²é—®ç­”å¯¹ä½œä¸ºä¸Šä¸‹æ–‡åŸºç¡€ï¼Œåœ¨ç­”æ¡ˆä¸­æ•´åˆé¡µé¢å¼•ç”¨å¹¶å¼•å…¥åŸºäºé‡è¦æ€§çš„é¡µé¢åˆ†ç±»å™¨ã€‚</p>
<p><strong>Result:</strong> åˆæ­¥ç»“æœè¡¨æ˜æˆ‘ä»¬çš„AIåŠ©æ‰‹åœ¨æ–‡æ¡£ä¿¡æ¯æå–å’Œé—®ç­”ä»»åŠ¡æ–¹é¢å–å¾—äº†æ”¹è¿›ï¼Œé€šè¿‡æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯å’Œä¸Šä¸‹æ–‡åŸºç¡€æ˜¾è‘—æå‡äº†ä¿¡æ¯æä¾›çš„ç²¾ç¡®æ€§å’Œç›¸å…³æ€§ã€‚æ„å»ºçš„å¤§è§„æ¨¡æ•°æ®é›†å’Œæ¨¡å‹å·²åœ¨Hugging Faceå¹³å°å…¬å¼€å¯ç”¨ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚åŒ»ç–—æ³•å¾‹æ–‡æ¡£åˆ†æä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡æ•´åˆæ–‡æœ¬ã€è§†è§‰å’Œå¸ƒå±€ä¿¡æ¯ä»¥åŠä¸Šä¸‹æ–‡åŸºç¡€ï¼Œæ˜¾è‘—æå‡äº†æ–‡æ¡£ç†è§£å’Œé—®ç­”æ€§èƒ½ã€‚å¼€å‘çš„åŸºå‡†æ•°æ®é›†å’Œé¢†åŸŸç‰¹å®šæ¨¡å‹ä¸ºç±»ä¼¼å¤æ‚æ–‡æ¡£åˆ†æä»»åŠ¡æä¾›äº†å¯å¤ç°çš„æ¡†æ¶å’Œæ–¹æ³•è®ºï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œç ”ç©¶æ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_58">ğŸ“„ Abstract</h4>
<p>The opioid crisis represents a significant moment in public health that reveals systemic shortcomings across regulatory systems, healthcare practices, corporate governance, and public policy. Analyzing how these interconnected systems simultaneously failed to protect public health requires innovative analytic approaches for exploring the vast amounts of data and documents disclosed in the UCSF-JHU Opioid Industry Documents Archive (OIDA). The complexity, multimodal nature, and specialized characteristics of these healthcare-related legal and corporate documents necessitate more advanced methods and models tailored to specific data types and detailed annotations, ensuring the precision and professionalism in the analysis. In this paper, we tackle this challenge by organizing the original dataset according to document attributes and constructing a benchmark with 400k training documents and 10k for testing. From each document, we extract rich multimodal information-including textual content, visual elements, and layout structures-to capture a comprehensive range of features. Using multiple AI models, we then generate a large-scale dataset comprising 360k training QA pairs and 10k testing QA pairs. Building on this foundation, we develop domain-specific multimodal Large Language Models (LLMs) and explore the impact of multimodal inputs on task performance. To further enhance response accuracy, we incorporate historical QA pairs as contextual grounding for answering current queries. Additionally, we incorporate page references within the answers and introduce an importance-based page classifier, further improving the precision and relevance of the information provided. Preliminary results indicate the improvements with our AI assistant in document information extraction and question-answering tasks. The dataset and models are publicly available at: https://huggingface.co/opioidarchive</p>
<h3 id="60-mtp-exploring-multimodal-urban-traffic-profiling-with-modality-augmentation-and-spectrum-fusion">[60] <a href="https://arxiv.org/abs/2511.10218">MTP: Exploring Multimodal Urban Traffic Profiling with Modality Augmentation and Spectrum Fusion</a></h3>
<p><em>Haolong Xiang, Peisi Wang, Xiaolong Xu, Kun Yi, Xuyun Zhang, Quanzheng Sheng, Amin Beheshti, Wei Fan</em></p>
<h4 id="tldr_59">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMTPçš„å¤šæ¨¡æ€åŸå¸‚äº¤é€šä¿¡å·åˆ†ææ¡†æ¶ï¼Œé€šè¿‡æ•°å€¼ã€è§†è§‰å’Œæ–‡æœ¬ä¸‰ä¸ªè§†è§’å­¦ä¹ å¤šæ¨¡æ€ç‰¹å¾ï¼Œåœ¨é¢‘åŸŸä¸­è¿›è¡Œäº¤é€šä¿¡å·å»ºæ¨¡ï¼Œå¹¶åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå±•ç¤ºäº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_59">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰äº¤é€šä¿¡å·å»ºæ¨¡æ–¹æ³•é€šå¸¸ä¾èµ–åŸå§‹æ•°å€¼æ¨¡æ€ï¼Œå¿½ç•¥äº†å¤šæ¨¡æ€å¼‚æ„åŸå¸‚æ•°æ®ä¸­å­˜åœ¨çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè¿™é˜»ç¢äº†å¯¹äº¤é€šä¿¡å·çš„å…¨é¢ç†è§£å¹¶é™åˆ¶äº†å¤æ‚äº¤é€šåŠ¨æ€çš„å‡†ç¡®é¢„æµ‹ã€‚</p>
<p><strong>Method:</strong> æå‡ºMTPå¤šæ¨¡æ€æ¡†æ¶ï¼Œé€šè¿‡æ•°å€¼åˆ†æ”¯ä½¿ç”¨é¢‘ç‡å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œè§†è§‰åˆ†æ”¯å°†ä¿¡å·è½¬æ¢ä¸ºé¢‘ç‡å›¾åƒå’Œå‘¨æœŸæ€§å›¾åƒï¼Œæ–‡æœ¬åˆ†æ”¯åŸºäºç‰¹å®šä¸»é¢˜ã€èƒŒæ™¯ä¿¡æ¯å’Œé¡¹ç›®æè¿°ç”Ÿæˆæè¿°æ€§æ–‡æœ¬ï¼Œå¹¶è®¾è®¡åˆ†å±‚å¯¹æ¯”å­¦ä¹ æ¥èåˆä¸‰ä¸ªæ¨¡æ€çš„é¢‘è°±ä¿¡æ¯ã€‚</p>
<p><strong>Result:</strong> åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”æœ€å…ˆè¿›æ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€æ–¹æ³•åœ¨äº¤é€šä¿¡å·åˆ†æä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€å­¦ä¹ èƒ½å¤Ÿæ›´å…¨é¢åœ°ç†è§£åŸå¸‚äº¤é€šä¿¡å·ï¼Œé¢‘åŸŸå­¦ä¹ ç­–ç•¥èƒ½å¤Ÿç²¾ç»†åœ°æå–ä¿¡æ¯ï¼Œä¸ºå¤æ‚äº¤é€šåŠ¨æ€é¢„æµ‹æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆå’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_59">ğŸ“„ Abstract</h4>
<p>With rapid urbanization in the modern era, traffic signals from various sensors have been playing a significant role in monitoring the states of cities, which provides a strong foundation in ensuring safe travel, reducing traffic congestion and optimizing urban mobility. Most existing methods for traffic signal modeling often rely on the original data modality, i.e., numerical direct readings from the sensors in cities. However, this unimodal approach overlooks the semantic information existing in multimodal heterogeneous urban data in different perspectives, which hinders a comprehensive understanding of traffic signals and limits the accurate prediction of complex traffic dynamics. To address this problem, we propose a novel \textit{M}ultimodal framework, \textit{MTP}, for urban \textit{T}raffic \textit{P}rofiling, which learns multimodal features through numeric, visual, and textual perspectives. The three branches drive for a multimodal perspective of urban traffic signal learning in the frequency domain, while the frequency learning strategies delicately refine the information for extraction. Specifically, we first conduct the visual augmentation for the traffic signals, which transforms the original modality into frequency images and periodicity images for visual learning. Also, we augment descriptive texts for the traffic signals based on the specific topic, background information and item description for textual learning. To complement the numeric information, we utilize frequency multilayer perceptrons for learning on the original modality. We design a hierarchical contrastive learning on the three branches to fuse the spectrum of three modalities. Finally, extensive experiments on six real-world datasets demonstrate superior performance compared with the state-of-the-art approaches.</p>
<h3 id="61-peptrix-a-framework-for-explainable-peptide-analysis-through-protein-language-models">[61] <a href="https://arxiv.org/abs/2511.10244">PepTriX: A Framework for Explainable Peptide Analysis through Protein Language Models</a></h3>
<p><em>Vincent Schilling, Akshat Dubey, Georges Hattab</em></p>
<h4 id="tldr_60">ğŸ§© TL;DR</h4>
<p>PepTriXæ˜¯ä¸€ä¸ªæ–°é¢–çš„è‚½åˆ†ç±»æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆä¸€ç»´åºåˆ—åµŒå…¥å’Œä¸‰ç»´ç»“æ„ç‰¹å¾ï¼Œç»“åˆå›¾æ³¨æ„åŠ›ç½‘ç»œã€å¯¹æ¯”è®­ç»ƒå’Œè·¨æ¨¡æ€å…±åŒæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†åœ¨å¤šä¸ªè‚½åˆ†ç±»ä»»åŠ¡ä¸­çš„ä¼˜å¼‚æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†å¯è§£é‡Šçš„ç»“æ„å’Œç”Ÿç‰©ç‰©ç†åŸºåºæ´å¯Ÿã€‚</p>
<hr />
<h4 id="detailed-summary_60">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿè‚½åˆ†ç±»æ–¹æ³•ä¾èµ–æ‰‹å·¥ç¼–ç çš„ä¸€ç»´åºåˆ—è¡¨ç¤ºï¼Œé™åˆ¶äº†è·¨ä»»åŠ¡å’Œæ•°æ®é›†çš„å¯æ³›åŒ–æ€§ï¼›è›‹ç™½è´¨è¯­è¨€æ¨¡å‹è™½ç„¶æ€§èƒ½å¼ºå¤§ä½†é¢ä¸´è®¡ç®—æˆæœ¬é«˜ã€æ½œåœ¨è¡¨ç¤ºå¤æ‚éš¾ä»¥è§£é‡Šçš„é—®é¢˜ï¼›ç°æœ‰æ¡†æ¶å¤šä¸ºç‰¹å®šè‚½åˆ†ç±»ä»»åŠ¡è®¾è®¡ï¼Œç¼ºä¹é€šç”¨æ€§ï¼Œéš¾ä»¥å°†æ¨¡å‹é¢„æµ‹ä¸ç”Ÿç‰©å­¦ç›¸å…³åŸºåºå’Œç»“æ„ç‰¹æ€§è”ç³»èµ·æ¥ã€‚</p>
<p><strong>Method:</strong> PepTriXæ¡†æ¶æ•´åˆä¸€ç»´åºåˆ—åµŒå…¥å’Œä¸‰ç»´ç»“æ„ç‰¹å¾ï¼Œé‡‡ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œæ¶æ„ï¼Œå¢å¼ºå¯¹æ¯”è®­ç»ƒå’Œè·¨æ¨¡æ€å…±åŒæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨é€‚åº”ä¸åŒæ•°æ®é›†ï¼Œç”Ÿæˆä»»åŠ¡ç‰¹å®šçš„è‚½å‘é‡ï¼ŒåŒæ—¶ä¿æŒç”Ÿç‰©å­¦åˆç†æ€§ã€‚</p>
<p><strong>Result:</strong> é¢†åŸŸä¸“å®¶è¯„ä¼°è¡¨æ˜ï¼ŒPepTriXåœ¨å¤šä¸ªè‚½åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæä¾›äº†å…³äºé©±åŠ¨é¢„æµ‹çš„ç»“æ„å’Œç”Ÿç‰©ç‰©ç†åŸºåºçš„å¯è§£é‡Šæ´å¯Ÿï¼Œå®ç°äº†é¢„æµ‹é²æ£’æ€§å’Œå¯è§£é‡ŠéªŒè¯çš„ç»“åˆã€‚</p>
<p><strong>Conclusion:</strong> PepTriXå¼¥åˆäº†æ€§èƒ½é©±åŠ¨çš„è‚½çº§æ¨¡å‹ä¸é¢†åŸŸçº§ç†è§£ä¹‹é—´çš„å·®è·ï¼Œä¸ºè‚½ç ”ç©¶æä¾›äº†æ—¢å…·æœ‰é¢„æµ‹é²æ£’æ€§åˆå…·å¤‡å¯è§£é‡ŠéªŒè¯èƒ½åŠ›çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†è‚½åˆ†ç±»ä»»åŠ¡ä»é»‘ç›’é¢„æµ‹å‘ç”Ÿç‰©å­¦å¯è§£é‡Šå»ºæ¨¡çš„è½¬å˜ã€‚</p>
<hr />
<h4 id="abstract_60">ğŸ“„ Abstract</h4>
<p>Peptide classification tasks, such as predicting toxicity and HIV inhibition, are fundamental to bioinformatics and drug discovery. Traditional approaches rely heavily on handcrafted encodings of one-dimensional (1D) peptide sequences, which can limit generalizability across tasks and datasets. Recently, protein language models (PLMs), such as ESM-2 and ESMFold, have demonstrated strong predictive performance. However, they face two critical challenges. First, fine-tuning is computationally costly. Second, their complex latent representations hinder interpretability for domain experts. Additionally, many frameworks have been developed for specific types of peptide classification, lacking generalization. These limitations restrict the ability to connect model predictions to biologically relevant motifs and structural properties. To address these limitations, we present PepTriX, a novel framework that integrates one dimensional (1D) sequence embeddings and three-dimensional (3D) structural features via a graph attention network enhanced with contrastive training and cross-modal co-attention. PepTriX automatically adapts to diverse datasets, producing task-specific peptide vectors while retaining biological plausibility. After evaluation by domain experts, we found that PepTriX performs remarkably well across multiple peptide classification tasks and provides interpretable insights into the structural and biophysical motifs that drive predictions. Thus, PepTriX offers both predictive robustness and interpretable validation, bridging the gap between performance-driven peptide-level models (PLMs) and domain-level understanding in peptide research.</p>
<h3 id="62-causal-halbench-uncovering-lvlms-object-hallucinations-through-causal-intervention">[62] <a href="https://arxiv.org/abs/2511.10268">Causal-HalBench: Uncovering LVLMs Object Hallucinations Through Causal Intervention</a></h3>
<p><em>Zhe Xu, Zhicai Wang, Junkang Wu, Jinda Lu, Xiang Wang</em></p>
<h4 id="tldr_61">ğŸ§© TL;DR</h4>
<p>è¯¥è®ºæ–‡é€šè¿‡å› æœåˆ†ææ­ç¤ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ç‰©ä½“å¹»è§‰é—®é¢˜çš„æ ¹æºï¼Œæå‡ºç»“æ„åŒ–å› æœæ¨¡å‹æ¥å½¢å¼åŒ–å®šä¹‰è™šå‡ç›¸å…³æ€§ï¼Œå¹¶æ„å»ºäº†Causal-HalBenchåŸºå‡†æ¥é‡åŒ–è¯„ä¼°æ¨¡å‹å¯¹è™šå‡ç›¸å…³æ€§çš„é²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_61">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ™®éå­˜åœ¨ç‰©ä½“å¹»è§‰é—®é¢˜ï¼Œé”™è¯¯åˆ¤æ–­å›¾åƒä¸­ç‰©ä½“çš„å­˜åœ¨ï¼Œè¿™ä¸»è¦æºäºè®­ç»ƒè¿‡ç¨‹ä¸­é«˜åº¦å…±ç°ç‰©ä½“ä¹‹é—´çš„è™šå‡ç›¸å…³æ€§ï¼Œè€Œç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨å¹»è§‰æ£€æµ‹ï¼Œç¼ºä¹å¯¹è™šå‡ç›¸å…³æ€§çš„å½¢å¼åŒ–å®šä¹‰å’Œå®šé‡è¯„ä¼°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å°†å› æœåˆ†æå¼•å…¥LVLMsçš„ç‰©ä½“è¯†åˆ«åœºæ™¯ï¼Œå»ºç«‹ç»“æ„åŒ–å› æœæ¨¡å‹æ¥å½¢å¼åŒ–å®šä¹‰å…±ç°åå·®å¯¼è‡´çš„è™šå‡ç›¸å…³æ€§ï¼Œå¼€å‘äº†Causal-HalBenchåŸºå‡†ï¼Œè¯¥åŸºå‡†åŒ…å«åäº‹å®æ ·æœ¬å¹¶é›†æˆäº†å…¨é¢çš„å› æœæŒ‡æ ‡ï¼ŒåŒæ—¶æå‡ºäº†åˆ©ç”¨ä¸“æœ‰LVLMså’Œæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆåäº‹å®æ ·æœ¬çš„å¯æ‰©å±•æµæ°´çº¿ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸»æµLVLMsä¸Šä½¿ç”¨Causal-HalBenchè¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¿™äº›æ¨¡å‹åœ¨ä¸åŒç¨‹åº¦ä¸Šéƒ½è¡¨ç°å‡ºå¯¹è™šå‡ç›¸å…³æ€§çš„æ•æ„Ÿæ€§ï¼ŒéªŒè¯äº†æ‰€æå‡ºåŸºå‡†çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹é²æ£’æ€§è¯„ä¼°çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†LVLMsä¸­ç‰©ä½“å¹»è§‰çš„å› æœæœºåˆ¶ï¼Œä¸ºç†è§£æ¨¡å‹åå·®æä¾›äº†ç†è®ºæ¡†æ¶ï¼Œæå‡ºçš„åŸºå‡†å’Œè¯„ä¼°æ–¹æ³•ä¸ºæœªæ¥æ¨¡å‹é²æ£’æ€§æ”¹è¿›æä¾›äº†é‡è¦å·¥å…·ï¼Œå¼ºè°ƒäº†åœ¨æ¨¡å‹è¯„ä¼°ä¸­è€ƒè™‘å› æœå…³ç³»çš„å¿…è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_61">ğŸ“„ Abstract</h4>
<p>Large Vision-Language Models (LVLMs) often suffer from object hallucination, making erroneous judgments about the presence of objects in images. We propose this primar- ily stems from spurious correlations arising when models strongly associate highly co-occurring objects during train- ing, leading to hallucinated objects influenced by visual con- text. Current benchmarks mainly focus on hallucination de- tection but lack a formal characterization and quantitative evaluation of spurious correlations in LVLMs. To address this, we introduce causal analysis into the object recognition scenario of LVLMs, establishing a Structural Causal Model (SCM). Utilizing the language of causality, we formally de- fine spurious correlations arising from co-occurrence bias. To quantify the influence induced by these spurious correla- tions, we develop Causal-HalBench, a benchmark specifically constructed with counterfactual samples and integrated with comprehensive causal metrics designed to assess model ro- bustness against spurious correlations. Concurrently, we pro- pose an extensible pipeline for the construction of these coun- terfactual samples, leveraging the capabilities of proprietary LVLMs and Text-to-Image (T2I) models for their genera- tion. Our evaluations on mainstream LVLMs using Causal- HalBench demonstrate these models exhibit susceptibility to spurious correlations, albeit to varying extents.</p>
  </article>
</body>
</html>
