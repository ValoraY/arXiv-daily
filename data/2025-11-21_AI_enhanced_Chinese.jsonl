{"id": "2511.16054", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16054", "abs": "https://arxiv.org/abs/2511.16054", "authors": ["Gwen Yidou-Weng", "Ian Li", "Anji Liu", "Oliver Broadrick", "Guy Van den Broeck", "Benjie Wang"], "title": "Learning Tractable Distributions Of Language Model Continuations", "comment": null, "summary": "Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intractable. Prior work uses tractable surrogates such as hidden Markov models (HMMs) to approximate the distribution over continuations and adjust the model's next-token logits at decoding time. However, we find that these surrogates are often weakly context aware, which reduces query quality. We propose Learning to Look Ahead (LTLA), a hybrid approach that pairs the same base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. Two efficiency pitfalls arise when adding neural context: (i) naively rescoring the prefix with every candidate next token requires a sweep over the entire vocabulary at each step, and (ii) predicting fresh surrogate parameters for each prefix, although tractable at a single step, forces recomputation of future probabilities for every new prefix and eliminates reuse. LTLA avoids both by using a single batched HMM update to account for all next-token candidates at once, and by conditioning only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed, so computations can be reused across prefixes. Empirically, LTLA attains higher conditional likelihood than an unconditional HMM, approximates continuation distributions for vision-language models where a standalone HMM cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks, with minimal inference overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5b66\u4e60\u524d\u77bb\uff08LTLA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u4e0e\u56fa\u5b9a\u53ef\u5904\u7406\u7684\u4ee3\u7406\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u53d7\u63a7\u6587\u672c\u751f\u6210\u4e2d\u672a\u6765\u4f9d\u8d56\u7ea6\u675f\u7684\u8fd1\u4f3c\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u5355\u6b21\u6279\u91cfHMM\u66f4\u65b0\u8ba1\u7b97\u6240\u6709\u5019\u9009\u6807\u8bb0\u7684\u5ef6\u7eed\u6982\u7387\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u7ea6\u675f\u6ee1\u8db3\u5ea6\u3002", "motivation": "\u73b0\u6709\u53d7\u63a7\u6587\u672c\u751f\u6210\u65b9\u6cd5\u4f7f\u7528\u53ef\u5904\u7406\u4ee3\u7406\u6a21\u578b\uff08\u5982HMM\uff09\u8fd1\u4f3c\u672a\u6765\u4f9d\u8d56\u7ea6\u675f\uff0c\u4f46\u8fd9\u4e9b\u4ee3\u7406\u6a21\u578b\u901a\u5e38\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u8f83\u5f31\uff0c\u5bfc\u81f4\u67e5\u8be2\u8d28\u91cf\u4e0b\u964d\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u6dfb\u52a0\u795e\u7ecf\u4e0a\u4e0b\u6587\u65f6\u9762\u4e34\u4e24\u4e2a\u6548\u7387\u74f6\u9888\uff1a\u9700\u8981\u5bf9\u6bcf\u4e2a\u5019\u9009\u6807\u8bb0\u91cd\u65b0\u8bc4\u5206\u524d\u7f00\uff0c\u4ee5\u53ca\u4e3a\u6bcf\u4e2a\u524d\u7f00\u9884\u6d4b\u65b0\u7684\u4ee3\u7406\u53c2\u6570\u5bfc\u81f4\u8ba1\u7b97\u65e0\u6cd5\u590d\u7528\u3002", "method": "LTLA\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u5c06\u76f8\u540c\u7684\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u4e30\u5bcc\u524d\u7f00\u7f16\u7801\uff0c\u540c\u65f6\u4f7f\u7528\u56fa\u5b9a\u7684\u53ef\u5904\u7406\u4ee3\u7406\u6a21\u578b\u8ba1\u7b97\u7cbe\u786e\u7684\u5ef6\u7eed\u6982\u7387\u3002\u901a\u8fc7\u5355\u6b21\u6279\u91cfHMM\u66f4\u65b0\u540c\u65f6\u5904\u7406\u6240\u6709\u5019\u9009\u6807\u8bb0\uff0c\u5e76\u4ec5\u5c06\u4ee3\u7406\u6a21\u578b\u7684\u6f5c\u5728\u72b6\u6001\u5148\u9a8c\u6761\u4ef6\u4e8eLM\u7684\u9690\u85cf\u8868\u793a\uff0c\u4fdd\u6301\u4ee3\u7406\u89e3\u7801\u5668\u56fa\u5b9a\u4ee5\u5b9e\u73b0\u8de8\u524d\u7f00\u7684\u8ba1\u7b97\u590d\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLTLA\u83b7\u5f97\u4e86\u6bd4\u65e0\u6761\u4ef6HMM\u66f4\u9ad8\u7684\u6761\u4ef6\u4f3c\u7136\uff0c\u80fd\u591f\u8fd1\u4f3c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5ef6\u7eed\u5206\u5e03\uff08\u8fd9\u662f\u72ec\u7acbHMM\u65e0\u6cd5\u7f16\u7801\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u573a\u666f\uff09\uff0c\u5728\u53d7\u63a7\u751f\u6210\u4efb\u52a1\u4e2d\u4ee5\u53ef\u6bd4\u8f83\u7684\u6d41\u7545\u5ea6\u63d0\u9ad8\u4e86\u7ea6\u675f\u6ee1\u8db3\u5ea6\uff0c\u4e14\u63a8\u7406\u5f00\u9500\u6700\u5c0f\u3002", "conclusion": "LTLA\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5408\u7406\u7ed3\u5408\u795e\u7ecf\u6a21\u578b\u548c\u53ef\u5904\u7406\u4ee3\u7406\u6a21\u578b\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u53d7\u63a7\u751f\u6210\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u672a\u6765\u4f9d\u8d56\u7ea6\u675f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6df7\u5408\u8303\u5f0f\uff0c\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7b49\u590d\u6742\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6269\u5c55\u6027\uff0c\u4e3a\u7ea6\u675f\u6587\u672c\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2511.16654", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16654", "abs": "https://arxiv.org/abs/2511.16654", "authors": ["Elias Lumer", "Alex Cardenas", "Matt Melich", "Myles Mason", "Sara Dieter", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "Roberto Hernandez"], "title": "Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems", "comment": null, "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u4e24\u79cd\u591a\u6a21\u6001RAG\u68c0\u7d22\u65b9\u6cd5\uff0c\u53d1\u73b0\u76f4\u63a5\u591a\u6a21\u6001\u5d4c\u5165\u68c0\u7d22\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u6458\u8981\u7684\u6587\u672c\u68c0\u7d22\uff0c\u5728\u91d1\u878d\u8d22\u62a5\u95ee\u7b54\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8613%\u7684mAP@5\u7edd\u5bf9\u63d0\u5347\u548c32%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u4fdd\u7559\u89c6\u89c9\u4e0a\u4e0b\u6587\u5bf9\u591a\u6a21\u6001\u68c0\u7d22\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u4f9d\u8d56LLM\u6458\u8981\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u6587\u672c\u8868\u793a\uff0c\u5bfc\u81f4\u89c6\u89c9\u7ec6\u8282\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e22\u5931\uff0c\u8fd9\u5bf9\u4e0b\u6e38\u68c0\u7d22\u548c\u95ee\u7b54\u4efb\u52a1\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u5305\u542b\u56fe\u8868\u3001\u56fe\u5f62\u548c\u8868\u683c\u7684\u91d1\u878d\u6587\u6863\u4e2d\u3002", "method": "\u672c\u7814\u7a76\u5bf9\u4e24\u79cd\u591a\u6a21\u6001RAG\u68c0\u7d22\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u6bd4\u8f83\u5206\u6790\uff1a\u57fa\u4e8e\u6587\u672c\u5206\u5757\u68c0\u7d22\u548c\u76f4\u63a5\u591a\u6a21\u6001\u5d4c\u5165\u68c0\u7d22\uff0c\u57286\u4e2aLLM\u6a21\u578b\u548c2\u4e2a\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f7f\u7528\u5305\u542b40\u4e2a\u95ee\u7b54\u5bf9\u7684\u65b0\u5efa\u91d1\u878d\u8d22\u62a5\u7535\u8bdd\u4f1a\u8bae\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u76f4\u63a5\u591a\u6a21\u6001\u5d4c\u5165\u68c0\u7d22\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eLLM\u6458\u8981\u7684\u65b9\u6cd5\uff0c\u5728mAP@5\u4e0a\u5b9e\u73b013%\u7684\u7edd\u5bf9\u63d0\u5347\u548c32%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u5728nDCG@5\u4e0a\u5b9e\u73b011%\u7684\u7edd\u5bf9\u63d0\u5347\u548c20%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u540c\u65f6\u901a\u8fc7LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u6210\u5bf9\u6bd4\u8f83\u663e\u793a\u8be5\u65b9\u6cd5\u4ea7\u751f\u66f4\u51c6\u786e\u548c\u4e8b\u5b9e\u4e00\u81f4\u7684\u7b54\u6848\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660eLLM\u6458\u8981\u4f1a\u5728\u9884\u5904\u7406\u9636\u6bb5\u5f15\u5165\u4fe1\u606f\u635f\u5931\uff0c\u800c\u76f4\u63a5\u591a\u6a21\u6001\u5d4c\u5165\u80fd\u591f\u4fdd\u7559\u89c6\u89c9\u4e0a\u4e0b\u6587\u7528\u4e8e\u68c0\u7d22\u548c\u63a8\u7406\uff0c\u4e3a\u591a\u6a21\u6001RAG\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u539f\u751f\u89c6\u89c9\u8868\u793a\u5728\u590d\u6742\u6587\u6863\u7406\u89e3\u4e2d\u7684\u5173\u952e\u4ef7\u503c\u3002"}}
{"id": "2511.16664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16664", "abs": "https://arxiv.org/abs/2511.16664", "authors": ["Ali Taghibakhshi", "Sharath Turuvekere Sreenivas", "Saurav Muralidharan", "Ruisi Cai", "Marcin Chochowski", "Ameya Sunil Mahabaleshwarkar", "Yoshi Suhara", "Oluwatobi Olabiyi", "Daniel Korzekwa", "Mostofa Patwary", "Mohammad Shoeybi", "Jan Kautz", "Bryan Catanzaro", "Ashwath Aithal", "Nima Tajbakhsh", "Pavlo Molchanov"], "title": "Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs", "comment": null, "summary": "Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Nemotron Elastic\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e00\u7236\u6a21\u578b\u4e2d\u5d4c\u5165\u591a\u4e2a\u5d4c\u5957\u5b50\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u591a\u5c3a\u5ea6\u63a8\u7406\u5bfc\u5411LLM\u7684\u9ad8\u6548\u6784\u5efa\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5b9e\u73b0\u4e86360\u500d\u7684\u6210\u672c\u964d\u4f4e\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3\u9488\u5bf9\u4e0d\u540c\u89c4\u6a21\u548c\u90e8\u7f72\u76ee\u6807\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\u6210\u672c\u6781\u5176\u6602\u8d35\uff0c\u9700\u8981\u4e3a\u6bcf\u4e2a\u4e0d\u540c\u5c3a\u5bf8\u5355\u72ec\u8bad\u7ec3\uff0c\u5373\u4f7f\u91c7\u7528\u526a\u679d\u548c\u77e5\u8bc6\u84b8\u998f\u7b49\u538b\u7f29\u6280\u672f\uff0c\u6bcf\u4e2a\u538b\u7f29\u6a21\u578b\u4ecd\u9700\u6d88\u8017\u6570\u5343\u4ebftoken\u7684\u8bad\u7ec3\u6210\u672c\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u6df7\u5408Mamba-Attention\u67b6\u6784\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u8def\u7531\u5668\u4e0e\u4e24\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u591a\u4e2a\u5d4c\u5957\u5b50\u6a21\u578b\u7684\u6743\u91cd\u5171\u4eab\u548c\u96f6-shot\u63d0\u53d6\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u7ec4\u611f\u77e5SSM\u5f39\u6027\u5316\u3001\u5f02\u6784MLP\u5f39\u6027\u5316\u3001\u57fa\u4e8e\u5f52\u4e00\u5316MSE\u7684\u5c42\u91cd\u8981\u6027\u8bc4\u4f30\u4ee5\u53ca\u77e5\u8bc6\u84b8\u998f\u7b49\u6280\u672f\u3002", "result": "\u5728Nemotron Nano V2 12B\u6a21\u578b\u4e0a\u7684\u5e94\u7528\u663e\u793a\uff0c\u4ec5\u4f7f\u7528110B\u8bad\u7ec3token\u5373\u53ef\u540c\u65f6\u751f\u62109B\u548c6B\u6a21\u578b\uff0c\u76f8\u6bd4\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\u5bb6\u65cf\u5b9e\u73b0\u4e86360\u500d\u4ee5\u4e0a\u7684\u6210\u672c\u964d\u4f4e\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u538b\u7f29\u6280\u672f\u5b9e\u73b0\u4e86\u7ea67\u500d\u7684\u6539\u8fdb\uff0c\u4e14\u6bcf\u4e2a\u5d4c\u5957\u6a21\u578b\u5728\u51c6\u786e\u7387\u4e0a\u5747\u8fbe\u5230\u6216\u4f18\u4e8e\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u663e\u8457\u964d\u4f4e\u4e86\u591a\u5c3a\u5ea6\u6a21\u578b\u6784\u5efa\u7684\u6210\u672c\uff0c\u8fd8\u5b9e\u73b0\u4e86\u6052\u5b9a\u90e8\u7f72\u5185\u5b58\u7684\u591a\u5408\u4e00\u63a8\u7406\u6a21\u578b\uff0c\u4e3a\u9ad8\u6548\u6784\u5efa\u548c\u90e8\u7f72\u4e0d\u540c\u9884\u7b97\u914d\u7f6e\u7684LLM\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2511.15719", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15719", "abs": "https://arxiv.org/abs/2511.15719", "authors": ["William Brach", "Lukas Galke Poech"], "title": "Chain of Summaries: Summarization Through Iterative Questioning", "comment": null, "summary": "Large Language Models (LLMs) are increasingly using external web content. However, much of this content is not easily digestible by LLMs due to LLM-unfriendly formats and limitations of context length. To address this issue, we propose a method for generating general-purpose, information-dense summaries that act as plain-text repositories of web content. Inspired by Hegel's dialectical method, our approach, denoted as Chain of Summaries (CoS), iteratively refines an initial summary (thesis) by identifying its limitations through questioning (antithesis), leading to a general-purpose summary (synthesis) that can satisfy current and anticipate future information needs. Experiments on the TriviaQA, TruthfulQA, and SQUAD datasets demonstrate that CoS outperforms zero-shot LLM baselines by up to 66% and specialized summarization methods such as BRIO and PEGASUS by up to 27%. CoS-generated summaries yield higher Q&A performance compared to the source content, while requiring substantially fewer tokens and being agnostic to the specific downstream LLM. CoS thus resembles an appealing option for website maintainers to make their content more accessible for LLMs, while retaining possibilities for human oversight.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6458\u8981\u94fe\uff08CoS\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ed1\u683c\u5c14\u8fa9\u8bc1\u6cd5\u542f\u53d1\u7684\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u751f\u6210\u4fe1\u606f\u5bc6\u96c6\u7684\u901a\u7528\u6458\u8981\uff0c\u663e\u8457\u63d0\u5347LLM\u5bf9\u7f51\u7edc\u5185\u5bb9\u7684\u5904\u7406\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u96f6\u6837\u672cLLM\u57fa\u7ebf\u548c\u4e13\u4e1a\u6458\u8981\u65b9\u6cd5\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11token\u4f7f\u7528\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e5\u76ca\u4f9d\u8d56\u5916\u90e8\u7f51\u7edc\u5185\u5bb9\uff0c\u4f46\u73b0\u6709\u5185\u5bb9\u683c\u5f0f\u5bf9LLM\u4e0d\u53cb\u597d\u4e14\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5bfc\u81f4\u4fe1\u606f\u6d88\u5316\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5c06\u7f51\u7edc\u5185\u5bb9\u8f6c\u5316\u4e3aLLM\u53ef\u9ad8\u6548\u5904\u7406\u7684\u901a\u7528\u6458\u8981\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u683c\u5f0f\u517c\u5bb9\u6027\u548c\u4fe1\u606f\u5bc6\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6458\u8981\u94fe\uff08CoS\uff09\u65b9\u6cd5\uff0c\u53d7\u9ed1\u683c\u5c14\u8fa9\u8bc1\u6cd5\u542f\u53d1\uff0c\u901a\u8fc7\u521d\u59cb\u6458\u8981\uff08\u6b63\u9898\uff09\u3001\u8d28\u7591\u8bc6\u522b\u5c40\u9650\u6027\uff08\u53cd\u9898\uff09\u3001\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u901a\u7528\u6458\u8981\uff08\u5408\u9898\uff09\u7684\u4e09\u9636\u6bb5\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6ee1\u8db3\u5f53\u524d\u5e76\u9884\u6d4b\u672a\u6765\u4fe1\u606f\u9700\u6c42\uff0c\u751f\u6210\u4fe1\u606f\u5bc6\u96c6\u7684\u7eaf\u6587\u672c\u6458\u8981\u5b58\u50a8\u5e93\u3002", "result": "\u5728TriviaQA\u3001TruthfulQA\u548cSQUAD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCoS\u6bd4\u96f6\u6837\u672cLLM\u57fa\u7ebf\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe66%\uff0c\u4f18\u4e8eBRIO\u548cPEGASUS\u7b49\u4e13\u4e1a\u6458\u8981\u65b9\u6cd5\u8fbe27%\u3002CoS\u751f\u6210\u7684\u6458\u8981\u76f8\u6bd4\u539f\u59cb\u5185\u5bb9\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\u91cf\u4e14\u4e0e\u4e0b\u6e38LLM\u65e0\u5173\u3002", "conclusion": "CoS\u4e3a\u7f51\u7ad9\u7ef4\u62a4\u8005\u63d0\u4f9b\u4e86\u4f7f\u5185\u5bb9\u66f4\u6613\u4e8eLLM\u8bbf\u95ee\u7684\u53ef\u884c\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u7559\u4eba\u5de5\u76d1\u7763\u7684\u53ef\u80fd\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u7684\u4fe1\u606f\u5bc6\u96c6\u6458\u8981\u4e0d\u4ec5\u63d0\u5347\u5f53\u524d\u4efb\u52a1\u6027\u80fd\uff0c\u8fd8\u80fd\u9002\u5e94\u672a\u6765\u4fe1\u606f\u9700\u6c42\uff0c\u4e3aLLM\u4e0e\u7f51\u7edc\u5185\u5bb9\u7684\u9ad8\u6548\u4ea4\u4e92\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.15714", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15714", "abs": "https://arxiv.org/abs/2511.15714", "authors": ["Ariel Kamen", "Yakov Kamen"], "title": "Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization", "comment": "17 pages, 7 figures", "summary": "This study introduces an ensemble framework for unstructured text categorization using large language models (LLMs). By integrating multiple models, the ensemble large language model (eLLM) framework addresses common weaknesses of individual systems, including inconsistency, hallucination, category inflation, and misclassification. The eLLM approach yields a substantial performance improvement of up to 65\\% in F1-score over the strongest single model. We formalize the ensemble process through a mathematical model of collective decision-making and establish principled aggregation criteria. Using the Interactive Advertising Bureau (IAB) hierarchical taxonomy, we evaluate ten state-of-the-art LLMs under identical zero-shot conditions on a human-annotated corpus of 8{,}660 samples. Results show that individual models plateau in performance due to the compression of semantically rich text into sparse categorical representations, while eLLM improves both robustness and accuracy. With a diverse consortium of models, eLLM achieves near human-expert-level performance, offering a scalable and reliable solution for taxonomy-based classification that may significantly reduce dependence on human expert labeling.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u6210\u6846\u67b6eLLM\uff0c\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u6587\u672c\u5206\u7c7b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u6a21\u578b\u89e3\u51b3\u4e86\u5355\u4e2a\u7cfb\u7edf\u7684\u5e38\u89c1\u5f31\u70b9\uff0c\u5728F1\u5206\u6570\u4e0a\u6bd4\u6700\u5f3a\u5355\u6a21\u578b\u63d0\u5347\u4e8665%\uff0c\u8fbe\u5230\u4e86\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u7684\u6027\u80fd\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u5b58\u5728\u4e00\u81f4\u6027\u5dee\u3001\u5e7b\u89c9\u95ee\u9898\u3001\u7c7b\u522b\u81a8\u80c0\u548c\u8bef\u5206\u7c7b\u7b49\u5e38\u89c1\u5f31\u70b9\uff0c\u8fd9\u4e9b\u9650\u5236\u5f71\u54cd\u4e86\u5206\u7c7b\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u96c6\u6210\u65b9\u6cd5\u514b\u670d\u5355\u4e2a\u6a21\u578b\u5728\u5c06\u8bed\u4e49\u4e30\u5bcc\u7684\u6587\u672c\u538b\u7f29\u4e3a\u7a00\u758f\u5206\u7c7b\u8868\u793a\u65f6\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6eLLM\uff0c\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u96c6\u4f53\u51b3\u7b56\u8fc7\u7a0b\u5e76\u5efa\u7acb\u539f\u5219\u6027\u805a\u5408\u51c6\u5219\u3002\u5728IAB\u5206\u5c42\u5206\u7c7b\u4f53\u7cfb\u4e0b\uff0c\u5bf910\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u57288,660\u4e2a\u4eba\u5de5\u6807\u6ce8\u6837\u672c\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u6761\u4ef6\u8bc4\u4f30\uff0c\u91c7\u7528\u591a\u6837\u5316\u6a21\u578b\u8054\u76df\u5b9e\u73b0\u96c6\u6210\u5206\u7c7b\u3002", "result": "eLLM\u6846\u67b6\u5728F1\u5206\u6570\u4e0a\u6bd4\u6700\u5f3a\u5355\u6a21\u578b\u63d0\u5347\u4e8665%\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u7684\u5206\u7c7b\u7cbe\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\u5355\u4e2a\u6a21\u578b\u7531\u4e8e\u8bed\u4e49\u538b\u7f29\u95ee\u9898\u800c\u6027\u80fd\u8d8b\u4e8e\u9971\u548c\uff0c\u800ceLLM\u540c\u65f6\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "eLLM\u6846\u67b6\u4e3a\u57fa\u4e8e\u5206\u7c7b\u5b66\u7684\u6587\u672c\u5206\u7c7b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u4eba\u7c7b\u4e13\u5bb6\u6807\u6ce8\u7684\u4f9d\u8d56\u3002\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u96c6\u6210\u65b9\u6cd5\u5728\u514b\u670d\u5927\u8bed\u8a00\u6a21\u578b\u56fa\u6709\u5c40\u9650\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2511.15831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15831", "abs": "https://arxiv.org/abs/2511.15831", "authors": ["Wei Zhang", "Yeying Jin", "Xin Li", "Yan Zhang", "Xiaofeng Cong", "Cong Wang", "Fengcai Qiao", "zhichao Lian"], "title": "UniFit: Towards Universal Virtual Try-on with MLLM-Guided Semantic Alignment", "comment": "accepted to AAAI-2026", "summary": "Image-based virtual try-on (VTON) aims to synthesize photorealistic images of a person wearing specified garments. Despite significant progress, building a universal VTON framework that can flexibly handle diverse and complex tasks remains a major challenge. Recent methods explore multi-task VTON frameworks guided by textual instructions, yet they still face two key limitations: (1) semantic gap between text instructions and reference images, and (2) data scarcity in complex scenarios. To address these challenges, we propose UniFit, a universal VTON framework driven by a Multimodal Large Language Model (MLLM). Specifically, we introduce an MLLM-Guided Semantic Alignment Module (MGSA), which integrates multimodal inputs using an MLLM and a set of learnable queries. By imposing a semantic alignment loss, MGSA captures cross-modal semantic relationships and provides coherent and explicit semantic guidance for the generative process, thereby reducing the semantic gap. Moreover, by devising a two-stage progressive training strategy with a self-synthesis pipeline, UniFit is able to learn complex tasks from limited data. Extensive experiments show that UniFit not only supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, but also achieves state-of-the-art performance. The source code and pretrained models are available at https://github.com/zwplus/UniFit.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUniFit\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u901a\u7528\u865a\u62df\u8bd5\u7a7f\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u6307\u4ee4\u4e0e\u53c2\u8003\u56fe\u50cf\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u4ee5\u53ca\u590d\u6742\u573a\u666f\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5728\u591a\u79cd\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6587\u672c\u6307\u4ee4\u7684\u591a\u4efb\u52a1\u865a\u62df\u8bd5\u7a7f\u6846\u67b6\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u6587\u672c\u6307\u4ee4\u4e0e\u53c2\u8003\u56fe\u50cf\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u4ee5\u53ca\u590d\u6742\u573a\u666f\u4e0b\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u901a\u7528\u865a\u62df\u8bd5\u7a7f\u7cfb\u7edf\u7684\u5f00\u53d1\u4e0e\u5e94\u7528\u3002", "method": "UniFit\u6846\u67b6\u5f15\u5165\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u67e5\u8be2\u96c6\u6574\u5408\u591a\u6a21\u6001\u8f93\u5165\uff0c\u5e76\u65bd\u52a0\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\u6765\u6355\u6349\u8de8\u6a21\u6001\u8bed\u4e49\u5173\u7cfb\uff1b\u540c\u65f6\u91c7\u7528\u4e24\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u548c\u81ea\u5408\u6210\u6d41\u7a0b\uff0c\u4ece\u6709\u9650\u6570\u636e\u4e2d\u5b66\u4e60\u590d\u6742\u4efb\u52a1\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUniFit\u4e0d\u4ec5\u652f\u6301\u591a\u670d\u88c5\u548c\u6a21\u578b\u95f4\u8bd5\u7a7f\u7b49\u5e7f\u6cdb\u7684\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\uff0c\u800c\u4e14\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6709\u6548\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u548c\u6e10\u8fdb\u5f0f\u5b66\u4e60\u7b56\u7565\uff0c\u4e3a\u6784\u5efa\u901a\u7528\u865a\u62df\u8bd5\u7a7f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2511.15848", "categories": ["cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.15848", "abs": "https://arxiv.org/abs/2511.15848", "authors": ["Fei Tian", "Xiangyu Tony Zhang", "Yuxin Zhang", "Haoyang Zhang", "Yuxin Li", "Daijiao Liu", "Yayue Deng", "Donghang Wu", "Jun Chen", "Liang Zhao", "Chengyuan Yao", "Hexin Liu", "Eng Siong Chng", "Xuerui Yang", "Xiangyu Zhang", "Daxin Jiang", "Gang Yu"], "title": "Step-Audio-R1 Technical Report", "comment": "15 pages, 5 figures. Technical Report", "summary": "Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Step-Audio-R1\uff0c\u9996\u4e2a\u6210\u529f\u89e3\u9501\u97f3\u9891\u9886\u57df\u63a8\u7406\u80fd\u529b\u7684\u97f3\u9891\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u6001\u951a\u5b9a\u63a8\u7406\u84b8\u998f\u6846\u67b6\uff0c\u4f7f\u97f3\u9891\u667a\u80fd\u80fd\u591f\u4ece\u6df1\u601d\u719f\u8651\u4e2d\u771f\u6b63\u53d7\u76ca\uff0c\u5728\u97f3\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86Gemini 2.5 Pro\u5e76\u8fbe\u5230\u4e0eGemini 3 Pro\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e00\u4e2a\u4ee4\u4eba\u56f0\u60d1\u7684\u73b0\u8c61\uff1a\u5b83\u4eec\u5728\u6700\u5c0f\u5316\u6216\u6ca1\u6709\u63a8\u7406\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u2014\u2014\u97f3\u9891\u667a\u80fd\u662f\u5426\u771f\u7684\u80fd\u4ece\u6df1\u601d\u719f\u8651\u4e2d\u53d7\u76ca\u3002\u73b0\u6709\u63a8\u7406\u6a21\u578b\u5728\u6587\u672c\u548c\u89c6\u89c9\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u97f3\u9891\u9886\u57df\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u4e86\u6a21\u6001\u951a\u5b9a\u63a8\u7406\u84b8\u998f\u6846\u67b6\uff0c\u4f7fStep-Audio-R1\u80fd\u591f\u751f\u6210\u4e0e\u97f3\u9891\u76f8\u5173\u7684\u63a8\u7406\u94fe\uff0c\u8fd9\u4e9b\u63a8\u7406\u94fe\u771f\u6b63\u57fa\u4e8e\u58f0\u5b66\u7279\u5f81\u800c\u975e\u4ea7\u751f\u8131\u8282\u7684\u5e7b\u89c9\u6027\u601d\u8003\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u63a8\u7406\u8fc7\u7a0b\u951a\u5b9a\u5728\u97f3\u9891\u6a21\u6001\u7279\u5f81\u4e0a\uff0c\u786e\u4fdd\u63a8\u7406\u94fe\u4e0e\u97f3\u9891\u5185\u5bb9\u7684\u76f8\u5173\u6027\u548c\u4e00\u81f4\u6027\u3002", "result": "Step-Audio-R1\u5728\u5168\u9762\u7684\u97f3\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u97f3\u9891\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86Gemini 2.5 Pro\uff0c\u5e76\u5728\u6db5\u76d6\u8bed\u97f3\u3001\u73af\u5883\u58f0\u97f3\u548c\u97f3\u4e50\u7684\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdb\u7684Gemini 3 Pro\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u63a8\u7406\u662f\u4e00\u79cd\u53ef\u8de8\u6a21\u6001\u8fc1\u79fb\u7684\u80fd\u529b\uff0c\u5f53\u9002\u5f53\u951a\u5b9a\u65f6\uff0c\u6269\u5c55\u7684\u6df1\u601d\u719f\u8651\u53ef\u4ee5\u4ece\u8d1f\u62c5\u8f6c\u53d8\u4e3a\u97f3\u9891\u667a\u80fd\u7684\u5f3a\u5927\u8d44\u4ea7\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6784\u5efa\u771f\u6b63\u8de8\u6240\u6709\u611f\u5b98\u6a21\u6001\u8fdb\u884c\u6df1\u5ea6\u601d\u8003\u7684\u591a\u6a21\u6001\u63a8\u7406\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u8bc1\u660e\u4e86\u97f3\u9891\u667a\u80fd\u786e\u5b9e\u80fd\u591f\u4ece\u6df1\u601d\u719f\u8651\u4e2d\u53d7\u76ca\u3002"}}
{"id": "2511.15884", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15884", "abs": "https://arxiv.org/abs/2511.15884", "authors": ["Yintao Ma", "Sajjad Pakdamansavoji", "Amir Rasouli", "Tongtong Cao"], "title": "Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes", "comment": null, "summary": "Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.\n  To this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBox6D\uff0c\u4e00\u79cd\u9488\u5bf9\u4ed3\u5e93\u573a\u666f\u4e2d\u5b58\u50a8\u7bb1\u7684\u7c7b\u522b\u7ea76D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5feb\u901f\u4e8c\u5206\u641c\u7d22\u63a8\u65ad\u7bb1\u5b50\u5c3a\u5bf8\u5e76\u4f7f\u7528\u7c7b\u522bCAD\u6a21\u677f\u4f30\u8ba1\u59ff\u6001\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u7cbe\u5ea6\u7684\u540c\u65f6\u5c06\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u7ea676%\u3002", "motivation": "\u73b0\u67096D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u9700\u8981\u7cbe\u786eCAD\u6a21\u578b\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u65e0\u6a21\u578b\u65b9\u6cd5\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u5bb9\u6613\u5931\u8d25\uff0c\u7c7b\u522b\u7ea7\u65b9\u6cd5\u8fc7\u4e8e\u901a\u7528\u4e14\u5ffd\u7565\u73af\u5883\u548c\u7269\u4f53\u5148\u9a8c\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4ed3\u5e93\u81ea\u52a8\u5316\u4e2d\u5b58\u50a8\u7bb1\u5728\u6742\u4e71\u548c\u906e\u6321\u6761\u4ef6\u4e0b\u7684\u9ad8\u6548\u51c6\u786e\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\u3002", "method": "Box6D\u4ece\u5355\u6b21RGB-D\u89c2\u6d4b\u4e2d\u901a\u8fc7\u5feb\u901f\u4e8c\u5206\u641c\u7d22\u63a8\u65ad\u7bb1\u5b50\u5c3a\u5bf8\uff0c\u4f7f\u7528\u7c7b\u522bCAD\u6a21\u677f\u800c\u975e\u5b9e\u4f8b\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\uff0c\u91c7\u7528\u57fa\u4e8e\u6df1\u5ea6\u7684\u5408\u7406\u6027\u8fc7\u6ee4\u5668\u548c\u65e9\u505c\u7b56\u7565\u6765\u62d2\u7edd\u4e0d\u5408\u7406\u5047\u8bbe\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u771f\u5b9e\u4ed3\u5e93\u573a\u666f\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u76846D\u59ff\u6001\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5c06\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea676%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "Box6D\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u5de5\u4e1a\u573a\u666f\u4e2d\u5229\u7528\u7c7b\u522b\u5148\u9a8c\u548c\u73af\u5883\u7ea6\u675f\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4ed3\u5e93\u81ea\u52a8\u5316\u4e2d\u7684\u5b9e\u65f6\u7269\u4f53\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u7c7b\u522b\u7ea7\u65b9\u6cd5\u5728\u5e73\u8861\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.16221", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16221", "abs": "https://arxiv.org/abs/2511.16221", "authors": ["Caixin Kang", "Yifei Huang", "Liangyang Ouyang", "Mingfang Zhang", "Ruicong Liu", "Yoichi Sato"], "title": "Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions", "comment": null, "summary": "Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u4ea4\u4e92\u6b3a\u9a97\u8bc4\u4f30\u4efb\u52a1\u548c\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u4e2d\u6b3a\u9a97\u884c\u4e3a\u65b9\u9762\u7684\u663e\u8457\u7f3a\u9677\uff0c\u5e76\u5f00\u53d1\u4e86\u793e\u4ea4\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u52a8\u6001\u793e\u4ea4\u8ba4\u77e5\u8bb0\u5fc6\u6a21\u5757\u6765\u63d0\u5347\u6a21\u578b\u7684\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u5148\u8fdb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u660e\u663e\u7f3a\u4e4f\u4eba\u7c7b\u667a\u80fd\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff1a\u5728\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u4e2d'\u5bdf\u8a00\u89c2\u8272'\u548c\u8bc4\u4f30\u6b3a\u9a97\u884c\u4e3a\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u4e25\u683c\u91cf\u5316\u8fd9\u4e00\u7f3a\u9677\uff0c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u7406\u89e3\u591a\u6a21\u6001\u793e\u4ea4\u7ebf\u7d22\u548c\u5efa\u6a21\u4ed6\u4eba\u77e5\u8bc6\u3001\u4fe1\u5ff5\u6216\u610f\u56fe\u7684\u95ee\u9898\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u591a\u6a21\u6001\u4ea4\u4e92\u6b3a\u9a97\u8bc4\u4f30\u65b0\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b\u540c\u6b65\u89c6\u9891\u548c\u6587\u672c\u6570\u636e\u53ca\u53ef\u9a8c\u8bc1\u771f\u5b9e\u6807\u7b7e\u7684\u65b0\u578b\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002\u4e3a\u89e3\u51b3\u8be5\u6311\u6218\uff0c\u8bbe\u8ba1\u4e86\u793e\u4ea4\u601d\u7ef4\u94fe\u63a8\u7406\u6d41\u7a0b\u548c\u52a8\u6001\u793e\u4ea4\u8ba4\u77e5\u8bb0\u5fc6\u6a21\u5757\uff0c\u8fd9\u4e9b\u7ec4\u4ef6\u5171\u540c\u6784\u6210\u4e86\u63d0\u5347\u6a21\u578b\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u7684\u65b0\u6846\u67b6\u3002", "result": "\u5bf912\u4e2a\u6700\u5148\u8fdb\u7684\u5f00\u653e\u548c\u95ed\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5373\u4f7f\u662fGPT-4o\u7b49\u5f3a\u5927\u6a21\u578b\u4e5f\u96be\u4ee5\u53ef\u9760\u5730\u533a\u5206\u771f\u5047\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8be5\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u6784\u5efa\u5177\u5907\u771f\u6b63\u7c7b\u4eba\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u8def\u5f84\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6709\u6548\u5c06\u8bed\u8a00\u4e0e\u591a\u6a21\u6001\u793e\u4ea4\u7ebf\u7d22\u8fdb\u884c\u63a5\u5730\u4ee5\u53ca\u5efa\u6a21\u4ed6\u4eba\u5fc3\u7406\u72b6\u6001\u65b9\u9762\u7684\u6839\u672c\u6027\u7f3a\u9677\uff0c\u5f3a\u8c03\u4e86\u6784\u5efa\u66f4\u5177\u6d1e\u5bdf\u529b\u548c\u53ef\u4fe1\u5ea6AI\u7cfb\u7edf\u7684\u8feb\u5207\u9700\u6c42\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u63a8\u52a8\u4e86\u5411\u771f\u6b63\u7c7b\u4eba\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.15720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15720", "abs": "https://arxiv.org/abs/2511.15720", "authors": ["Islem Sahraoui"], "title": "Automated Hazard Detection in Construction Sites Using Large Language and Vision-Language Models", "comment": "Master thesis, University of Houton", "summary": "This thesis explores a multimodal AI framework for enhancing construction safety through the combined analysis of textual and visual data. In safety-critical environments such as construction sites, accident data often exists in multiple formats, such as written reports, inspection records, and site imagery, making it challenging to synthesize hazards using traditional approaches. To address this, this thesis proposed a multimodal AI framework that combines text and image analysis to assist in identifying safety hazards on construction sites. Two case studies were consucted to evaluate the capabilities of large language models (LLMs) and vision-language models (VLMs) for automated hazard identification.The first case study introduces a hybrid pipeline that utilizes GPT 4o and GPT 4o mini to extract structured insights from a dataset of 28,000 OSHA accident reports (2000-2025). The second case study extends this investigation using Molmo 7B and Qwen2 VL 2B, lightweight, open-source VLMs. Using the public ConstructionSite10k dataset, the performance of the two models was evaluated on rule-level safety violation detection using natural language prompts. This experiment served as a cost-aware benchmark against proprietary models and allowed testing at scale with ground-truth labels. Despite their smaller size, Molmo 7B and Quen2 VL 2B showed competitive performance in certain prompt configurations, reinforcing the feasibility of low-resource multimodal systems for rule-aware safety monitoring.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001AI\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u5206\u6790\u6765\u589e\u5f3a\u5efa\u7b51\u5de5\u5730\u5b89\u5168\u76d1\u6d4b\uff0c\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u8bc6\u522b\u5b89\u5168\u98ce\u9669\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5728\u5efa\u7b51\u5de5\u5730\u7b49\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\uff0c\u4e8b\u6545\u6570\u636e\u901a\u5e38\u4ee5\u591a\u79cd\u683c\u5f0f\u5b58\u5728\uff0c\u5982\u4e66\u9762\u62a5\u544a\u3001\u68c0\u67e5\u8bb0\u5f55\u548c\u73b0\u573a\u56fe\u50cf\uff0c\u8fd9\u4f7f\u5f97\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7efc\u5408\u8bc6\u522b\u5b89\u5168\u9690\u60a3\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u7684\u667a\u80fd\u6846\u67b6\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u591a\u6a21\u6001AI\u6846\u67b6\uff0c\u7b2c\u4e00\u4e2a\u6848\u4f8b\u7814\u7a76\u4f7f\u7528GPT-4o\u548cGPT-4o mini\u4ece28,000\u4efdOSHA\u4e8b\u6545\u62a5\u544a\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u89c1\u89e3\uff0c\u7b2c\u4e8c\u4e2a\u6848\u4f8b\u7814\u7a76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578bMolmo 7B\u548cQwen2 VL 2B\uff0c\u5728ConstructionSite10k\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u7684\u89c4\u5219\u7ea7\u5b89\u5168\u8fdd\u89c4\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u8f83\u5c0f\uff0cMolmo 7B\u548cQwen2 VL 2B\u5728\u67d0\u4e9b\u63d0\u793a\u914d\u7f6e\u4e0b\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u4f4e\u8d44\u6e90\u591a\u6a21\u6001\u7cfb\u7edf\u5728\u89c4\u5219\u611f\u77e5\u5b89\u5168\u76d1\u6d4b\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u540c\u65f6\u4e3a\u4e13\u6709\u6a21\u578b\u63d0\u4f9b\u4e86\u6210\u672c\u611f\u77e5\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001AI\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u652f\u6301\u5efa\u7b51\u5b89\u5168\u76d1\u6d4b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u81ea\u52a8\u5316\u5b89\u5168\u98ce\u9669\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u6e90\u6a21\u578b\u5728\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.15943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15943", "abs": "https://arxiv.org/abs/2511.15943", "authors": ["Zihan Li", "Yiqing Wang", "Sina Farsiu", "Paul Kinahan"], "title": "Boosting Medical Visual Understanding From Multi-Granular Language Learning", "comment": "Preprint. 40 pages", "summary": "Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at \\href{https://github.com/HUANGLIZI/MGLL}{https://github.com/HUANGLIZI/MGLL}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u7c92\u5ea6\u8bed\u8a00\u5b66\u4e60\uff08MGLL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u6807\u7b7e\u76d1\u7763\u548c\u8de8\u7c92\u5ea6\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86CLIP\u5728\u533b\u5b66\u5f71\u50cf\u7b49\u591a\u6807\u7b7e\u3001\u591a\u7c92\u5ea6\u573a\u666f\u4e2d\u7684\u5bf9\u9f50\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u6807\u7b7e\u3001\u5355\u7c92\u5ea6\u7684\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\uff0c\u4f46\u5728\u533b\u5b66\u5f71\u50cf\u7b49\u590d\u6742\u9886\u57df\u4e2d\uff0c\u56fe\u50cf\u901a\u5e38\u5bf9\u5e94\u591a\u4e2a\u9ad8\u5c42\u6807\u7b7e\u548c\u4e0d\u540c\u7c92\u5ea6\u7684\u6ce8\u91ca\uff0c\u8fd9\u79cd\u5355\u7c92\u5ea6\u5bf9\u9f50\u65b9\u5f0f\u9650\u5236\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "method": "MGLL\u6846\u67b6\u91c7\u7528\u7ed3\u6784\u5316\u591a\u6807\u7b7e\u76d1\u7763\u673a\u5236\uff0c\u6574\u5408\u4e0d\u540c\u7c92\u5ea6\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u5f15\u5165\u70b9\u5f0f\u7ea6\u675f\u7684\u8f6f\u6807\u7b7e\u76d1\u7763\u6765\u589e\u5f3a\u5bf9\u9f50\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u5e73\u6ed1KL\u6563\u5ea6\u786e\u4fdd\u8de8\u7c92\u5ea6\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u6784\u5efa\u7684\u5927\u89c4\u6a21\u591a\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u540e\uff0cMGLL\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u6807\u7b7e\u548c\u8de8\u7c92\u5ea6\u5bf9\u9f50\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "MGLL\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u7c92\u5ea6\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u7b49\u9700\u8981\u5904\u7406\u590d\u6742\u6807\u7b7e\u7ed3\u6784\u7684\u9886\u57df\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6807\u7b7e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.16334", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16334", "abs": "https://arxiv.org/abs/2511.16334", "authors": ["Kaichen Zhang", "Keming Wu", "Zuhao Yang", "Kairui Hu", "Bin Wang", "Ziwei Liu", "Xingxuan Li", "Lidong Bing"], "title": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "comment": null, "summary": "Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OpenMMReasoner\uff0c\u4e00\u4e2a\u5b8c\u5168\u900f\u660e\u7684\u591a\u6a21\u6001\u63a8\u7406\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u6784\u5efa\u53ef\u590d\u73b0\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5728\u4e5d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4Qwen2.5-VL-7B-Instruct\u57fa\u7ebf\u63d0\u5347\u4e8611.6%\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u63a8\u7406\u9886\u57df\u7f3a\u4e4f\u900f\u660e\u4e14\u53ef\u590d\u73b0\u7684\u6570\u636e\u6784\u5efa\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u8fd9\u6210\u4e3a\u53ef\u6269\u5c55\u7814\u7a76\u7684\u4e3b\u8981\u969c\u788d\uff0c\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u5316\u7684\u8bad\u7ec3\u65b9\u6cd5\u6765\u63a8\u52a8\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u6784\u5efa\u4e86\u5305\u542b87.4\u4e07\u6837\u672c\u7684\u51b7\u542f\u52a8\u6570\u636e\u96c6\u5e76\u8fdb\u884c\u4e25\u683c\u7684\u9010\u6b65\u9a8c\u8bc1\uff1b\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u4f7f\u75287.4\u4e07\u6837\u672c\u8de8\u591a\u4e2a\u9886\u57df\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u7a33\u5b9a\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u4e5d\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4Qwen2.5-VL-7B-Instruct\u57fa\u7ebf\u5b9e\u73b0\u4e8611.6%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u8bad\u7ec3\u65b9\u6848\u7684\u6709\u6548\u6027\u548c\u6570\u636e\u8d28\u91cf\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6570\u636e\u8d28\u91cf\u548c\u8bad\u7ec3\u8bbe\u8ba1\u5bf9\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u7684\u51b3\u5b9a\u6027\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u5efa\u7acb\u4e86\u575a\u5b9e\u7684\u5b9e\u8bc1\u57fa\u7840\uff0c\u6240\u6709\u4ee3\u7801\u3001\u6d41\u7a0b\u548c\u6570\u636e\u5747\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2511.15722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15722", "abs": "https://arxiv.org/abs/2511.15722", "authors": ["Weichen Liu", "Qiyao Xue", "Haoming Wang", "Xiangyu Yin", "Boyuan Yang", "Wei Gao"], "title": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods", "comment": null, "summary": "Spatial reasoning, which requires ability to perceive and manipulate spatial relationships in the 3D world, is a fundamental aspect of human intelligence, yet remains a persistent challenge for Multimodal large language models (MLLMs). While existing surveys often categorize recent progress based on input modality (e.g., text, image, video, or 3D), we argue that spatial ability is not solely determined by the input format. Instead, our survey introduces a taxonomy that organizes spatial intelligence from cognitive aspect and divides tasks in terms of reasoning complexity, linking them to several cognitive functions. We map existing benchmarks across text only, vision language, and embodied settings onto this taxonomy, and review evaluation metrics and methodologies for assessing spatial reasoning ability. This cognitive perspective enables more principled cross-task comparisons and reveals critical gaps between current model capabilities and human-like reasoning. In addition, we analyze methods for improving spatial ability, spanning both training-based and reasoning-based approaches. This dual perspective analysis clarifies their respective strengths, uncovers complementary mechanisms. By surveying tasks, benchmarks, and recent advances, we aim to provide new researchers with a comprehensive understanding of the field and actionable directions for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u89c6\u89d2\u7684\u7a7a\u95f4\u667a\u80fd\u5206\u7c7b\u6cd5\uff0c\u5c06\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u6309\u63a8\u7406\u590d\u6742\u5ea6\u7ec4\u7ec7\uff0c\u5e76\u6620\u5c04\u73b0\u6709\u57fa\u51c6\u5230\u8be5\u5206\u7c7b\u6846\u67b6\u4e2d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u57fa\u4e8e\u8f93\u5165\u6a21\u6001\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u5206\u7c7b\uff0c\u4f46\u7a7a\u95f4\u80fd\u529b\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u8f93\u5165\u683c\u5f0f\u3002\u672c\u6587\u65e8\u5728\u4ece\u8ba4\u77e5\u89d2\u5ea6\u6784\u5efa\u66f4\u539f\u5219\u6027\u7684\u5206\u7c7b\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8ba4\u77e5\u89c6\u89d2\u7684\u7a7a\u95f4\u667a\u80fd\u5206\u7c7b\u6cd5\uff0c\u6309\u63a8\u7406\u590d\u6742\u5ea6\u7ec4\u7ec7\u4efb\u52a1\u5e76\u5c06\u5176\u4e0e\u591a\u4e2a\u8ba4\u77e5\u529f\u80fd\u5173\u8054\uff1b\u5c06\u6587\u672c\u3001\u89c6\u89c9\u8bed\u8a00\u548c\u5177\u8eab\u73af\u5883\u4e2d\u7684\u73b0\u6709\u57fa\u51c6\u6620\u5c04\u5230\u8be5\u5206\u7c7b\u6846\u67b6\uff1b\u5206\u6790\u8bc4\u4f30\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u6307\u6807\u548c\u65b9\u6cd5\uff1b\u540c\u65f6\u8003\u5bdf\u57fa\u4e8e\u8bad\u7ec3\u548c\u57fa\u4e8e\u63a8\u7406\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u8ba4\u77e5\u89c6\u89d2\u7684\u5206\u7c7b\u6846\u67b6\u5b9e\u73b0\u4e86\u66f4\u539f\u5219\u6027\u7684\u8de8\u4efb\u52a1\u6bd4\u8f83\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u80fd\u529b\u4e0e\u4eba\u7c7b\u63a8\u7406\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\uff1b\u5206\u6790\u8868\u660e\u57fa\u4e8e\u8bad\u7ec3\u548c\u57fa\u4e8e\u63a8\u7406\u7684\u6539\u8fdb\u65b9\u6cd5\u5177\u6709\u4e92\u8865\u673a\u5236\u548c\u5404\u81ea\u4f18\u52bf\u3002", "conclusion": "\u8ba4\u77e5\u89c6\u89d2\u7684\u5206\u7c7b\u6cd5\u4e3a\u7a7a\u95f4\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\u548c\u672a\u6765\u65b9\u5411\uff1b\u57fa\u4e8e\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u6539\u8fdb\u65b9\u6cd5\u5177\u6709\u4e92\u8865\u6027\uff0c\u4e3a\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.15967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15967", "abs": "https://arxiv.org/abs/2511.15967", "authors": ["Muyao Yuan", "Yuanhong Zhang", "Weizhan Zhang", "Lan Ma", "Yuan Gao", "Jiangyong Ying", "Yudeng Xin"], "title": "InfoCLIP: Bridging Vision-Language Pretraining and Open-Vocabulary Semantic Segmentation via Information-Theoretic Alignment Transfer", "comment": "Accepted by AAAI 2026", "summary": "Recently, the strong generalization ability of CLIP has facilitated open-vocabulary semantic segmentation, which labels pixels using arbitrary text. However, existing methods that fine-tune CLIP for segmentation on limited seen categories often lead to overfitting and degrade the pretrained vision-language alignment. To stabilize modality alignment during fine-tuning, we propose InfoCLIP, which leverages an information-theoretic perspective to transfer alignment knowledge from pretrained CLIP to the segmentation task. Specifically, this transfer is guided by two novel objectives grounded in mutual information. First, we compress the pixel-text modality alignment from pretrained CLIP to reduce noise arising from its coarse-grained local semantic representations learned under image-text supervision. Second, we maximize the mutual information between the alignment knowledge of pretrained CLIP and the fine-tuned model to transfer compact local semantic relations suited for the segmentation task. Extensive evaluations across various benchmarks validate the effectiveness of InfoCLIP in enhancing CLIP fine-tuning for open-vocabulary semantic segmentation, demonstrating its adaptability and superiority in asymmetric transfer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faInfoCLIP\uff0c\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u89c6\u89d2\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u5f15\u5bfc\u5c06\u9884\u8bad\u7ec3CLIP\u7684\u5bf9\u9f50\u77e5\u8bc6\u8fc1\u79fb\u5230\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u548c\u6a21\u6001\u5bf9\u9f50\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u9650\u53ef\u89c1\u7c7b\u522b\u4e0a\u5fae\u8c03CLIP\u8fdb\u884c\u5206\u5272\u65f6\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u5e76\u7834\u574f\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7a33\u5b9a\u6a21\u6001\u5bf9\u9f50\u7684\u5fae\u8c03\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u4e24\u4e2a\u65b0\u76ee\u6807\uff1a\u538b\u7f29\u9884\u8bad\u7ec3CLIP\u7684\u50cf\u7d20-\u6587\u672c\u6a21\u6001\u5bf9\u9f50\u4ee5\u51cf\u5c11\u566a\u58f0\uff0c\u540c\u65f6\u6700\u5927\u5316\u9884\u8bad\u7ec3CLIP\u4e0e\u5fae\u8c03\u6a21\u578b\u5bf9\u9f50\u77e5\u8bc6\u95f4\u7684\u4e92\u4fe1\u606f\u6765\u8fc1\u79fb\u9002\u5408\u5206\u5272\u4efb\u52a1\u7684\u7d27\u51d1\u5c40\u90e8\u8bed\u4e49\u5173\u7cfb\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u9a8c\u8bc1\u4e86InfoCLIP\u5728\u589e\u5f3aCLIP\u5fae\u8c03\u7528\u4e8e\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u975e\u5bf9\u79f0\u8fc1\u79fb\u4e2d\u7684\u9002\u5e94\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u4fe1\u606f\u8bba\u89c6\u89d2\u4e3aCLIP\u5fae\u8c03\u63d0\u4f9b\u4e86\u7a33\u5b9a\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0cInfoCLIP\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u5bf9\u9f50\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u4e3a\u975e\u5bf9\u79f0\u6a21\u6001\u8fc1\u79fb\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.16423", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16423", "abs": "https://arxiv.org/abs/2511.16423", "authors": ["Li Zhang", "Zhongxuan Han", "XiaoHua Feng", "Jiaming Zhang", "Yuyuan Li", "Linbo Jiang", "Jianan Lin", "Chaochao Chen"], "title": "TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models", "comment": "Accepted by AAAI 2026", "summary": "Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684one-shot\u8054\u90a6\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u6846\u67b6TOFA\uff0c\u901a\u8fc7\u5c42\u6b21\u8d1d\u53f6\u65af\u6a21\u578b\u548c\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u673a\u5236\uff0c\u5728\u5355\u8f6e\u901a\u4fe1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u6a21\u6001\u8054\u90a6\u9002\u5e94\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u6784\u6027\u548c\u901a\u4fe1\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u65b9\u6cd5\u5b58\u5728\u8fed\u4ee3\u8bad\u7ec3\u5bfc\u81f4\u7684\u901a\u4fe1\u6210\u672c\u9ad8\u3001\u6613\u53d7\u653b\u51fb\u95ee\u9898\uff0c\u800c\u5f53\u524done-shot\u65b9\u6cd5\u5728\u8054\u90a6\u8bbe\u7f6e\u4e0b\u9762\u4e34\u591a\u6a21\u6001\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u3001\u7f3a\u4e4f\u4e13\u95e8\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u7684\u7b56\u7565\u4ee5\u53ca\u9700\u8981\u989d\u5916\u8bad\u7ec3\u8d44\u6e90\u7b49\u6311\u6218\u3002", "method": "TOFA\u6846\u67b6\u5305\u542b\u89c6\u89c9\u548c\u6587\u672c\u4e24\u6761\u5e76\u884c\u5904\u7406\u8def\u5f84\uff1a\u89c6\u89c9\u8def\u5f84\u91c7\u7528\u5c42\u6b21\u8d1d\u53f6\u65af\u6a21\u578b\u5b66\u4e60\u4e2a\u6027\u5316\u7684\u7c7b\u7279\u5b9a\u539f\u578b\u5206\u5e03\uff0c\u6587\u672c\u8def\u5f84\u8bc4\u4f30\u5e76\u5168\u5c40\u5bf9\u9f50\u751f\u6210\u7684\u672c\u5730\u6587\u672c\u63d0\u793a\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5f15\u5165\u81ea\u9002\u5e94\u6743\u91cd\u6821\u51c6\u673a\u5236\u5e73\u8861\u4e2a\u6027\u5316\u548c\u9c81\u68d2\u6027\u3002", "result": "\u57289\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTOFA\u5728\u5404\u79cd\u8054\u90a6\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u5ba2\u6237\u7aef\u6216\u670d\u52a1\u5668\u7aef\u7684\u989d\u5916\u8bad\u7ec3\u8d44\u6e90\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684one-shot\u8054\u90a6\u9002\u5e94\u3002", "conclusion": "TOFA\u8bc1\u660e\u4e86\u65e0\u9700\u8bad\u7ec3\u7684one-shot\u8054\u90a6\u9002\u5e94\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u8f7b\u91cf\u7ea7\u8054\u90a6\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.15741", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15741", "abs": "https://arxiv.org/abs/2511.15741", "authors": ["Hyo-Jeong Jang"], "title": "Uncertainty-Resilient Multimodal Learning via Consistency-Guided Cross-Modal Transfer", "comment": "Master's thesis, Korea University, 2025", "summary": "Multimodal learning systems often face substantial uncertainty due to noisy data, low-quality labels, and heterogeneous modality characteristics. These issues become especially critical in human-computer interaction settings, where data quality, semantic reliability, and annotation consistency vary across users and recording conditions. This thesis tackles these challenges by exploring uncertainty-resilient multimodal learning through consistency-guided cross-modal transfer. The central idea is to use cross-modal semantic consistency as a basis for robust representation learning. By projecting heterogeneous modalities into a shared latent space, the proposed framework mitigates modality gaps and uncovers structural relations that support uncertainty estimation and stable feature learning. Building on this foundation, the thesis investigates strategies to enhance semantic robustness, improve data efficiency, and reduce the impact of noise and imperfect supervision without relying on large, high-quality annotations. Experiments on multimodal affect-recognition benchmarks demonstrate that consistency-guided cross-modal transfer significantly improves model stability, discriminative ability, and robustness to noisy or incomplete supervision. Latent space analyses further show that the framework captures reliable cross-modal structure even under challenging conditions. Overall, this thesis offers a unified perspective on resilient multimodal learning by integrating uncertainty modeling, semantic alignment, and data-efficient supervision, providing practical insights for developing reliable and adaptive brain-computer interface systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e00\u81f4\u6027\u5f15\u5bfc\u8de8\u6a21\u6001\u8fc1\u79fb\u7684\u4e0d\u786e\u5b9a\u6027\u5f39\u6027\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5f02\u6784\u6a21\u6001\u6295\u5f71\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u6765\u7f13\u89e3\u6a21\u6001\u5dee\u8ddd\u5e76\u63ed\u793a\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u76d1\u7763\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u7cfb\u7edf\u9762\u4e34\u6570\u636e\u566a\u58f0\u3001\u4f4e\u8d28\u91cf\u6807\u7b7e\u548c\u5f02\u6784\u6a21\u6001\u7279\u6027\u5e26\u6765\u7684\u663e\u8457\u4e0d\u786e\u5b9a\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4eba\u673a\u4ea4\u4e92\u73af\u5883\u4e2d\uff0c\u6570\u636e\u8d28\u91cf\u3001\u8bed\u4e49\u53ef\u9760\u6027\u548c\u6807\u6ce8\u4e00\u81f4\u6027\u968f\u7528\u6237\u548c\u8bb0\u5f55\u6761\u4ef6\u53d8\u5316\u800c\u5b58\u5728\u8f83\u5927\u5dee\u5f02\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u4e00\u81f4\u6027\u5f15\u5bfc\u7684\u8de8\u6a21\u6001\u8fc1\u79fb\u65b9\u6cd5\uff0c\u5229\u7528\u8de8\u6a21\u6001\u8bed\u4e49\u4e00\u81f4\u6027\u4f5c\u4e3a\u9c81\u68d2\u8868\u793a\u5b66\u4e60\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u5c06\u5f02\u6784\u6a21\u6001\u6295\u5f71\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u6765\u7f13\u89e3\u6a21\u6001\u5dee\u8ddd\uff0c\u5e76\u63ed\u793a\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u7a33\u5b9a\u7279\u5f81\u5b66\u4e60\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u540c\u65f6\u63a2\u7d22\u589e\u5f3a\u8bed\u4e49\u9c81\u68d2\u6027\u3001\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\u7684\u7b56\u7565\u3002", "result": "\u5728\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e00\u81f4\u6027\u5f15\u5bfc\u7684\u8de8\u6a21\u6001\u8fc1\u79fb\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7a33\u5b9a\u6027\u3001\u5224\u522b\u80fd\u529b\u548c\u5bf9\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u76d1\u7763\u7684\u9c81\u68d2\u6027\uff0c\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\u8fdb\u4e00\u6b65\u8868\u660e\u8be5\u6846\u67b6\u5373\u4f7f\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u4e5f\u80fd\u6355\u83b7\u53ef\u9760\u7684\u8de8\u6a21\u6001\u7ed3\u6784\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6574\u5408\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u6570\u636e\u9ad8\u6548\u76d1\u7763\uff0c\u4e3a\u5f39\u6027\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0c\u4e3a\u5f00\u53d1\u53ef\u9760\u548c\u81ea\u9002\u5e94\u7684\u8111\u673a\u63a5\u53e3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5728\u6784\u5efa\u7a33\u5065\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002"}}
{"id": "2511.15986", "categories": ["cs.CV", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15986", "abs": "https://arxiv.org/abs/2511.15986", "authors": ["Dawei Li", "Zijian Gu", "Peng Wang", "Chuhan Song", "Zhen Tan", "Mohan Zhang", "Tianlong Chen", "Yu Tian", "Song Wang"], "title": "Fairness in Multi-modal Medical Diagnosis with Demonstration Selection", "comment": "10 pages (including 2 pages of references), 4 figures. This work explores fairness in multi-modal medical image reasoning using in-context learning", "summary": "Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u611f\u77e5\u7684\u6f14\u793a\u9009\u62e9\u65b9\u6cd5FADS\uff0c\u901a\u8fc7\u805a\u7c7b\u91c7\u6837\u6784\u5efa\u4eba\u53e3\u7edf\u8ba1\u5b66\u5e73\u8861\u4e14\u8bed\u4e49\u76f8\u5173\u7684\u6f14\u793a\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u65e0\u9700\u5fae\u8c03\u7684\u516c\u5e73\u6027\u63d0\u5347\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u51cf\u5c11\u4e86\u6027\u522b\u3001\u79cd\u65cf\u548c\u6c11\u65cf\u76f8\u5173\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u63a8\u7406\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u4f46\u8de8\u4eba\u53e3\u7edf\u8ba1\u5b66\u7fa4\u4f53\u7684\u516c\u5e73\u6027\u4ecd\u662f\u4e3b\u8981\u5173\u5207\u3002\u73b0\u6709\u7684\u53bb\u504f\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u6216\u5fae\u8c03\uff0c\u8fd9\u5bf9\u4e8e\u57fa\u7840\u89c4\u6a21\u6a21\u578b\u6765\u8bf4\u4e0d\u5207\u5b9e\u9645\u3002\u672c\u6587\u63a2\u7d22\u5c06\u4e0a\u4e0b\u6587\u5b66\u4e60\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u8c03\u4f18\u7684\u66ff\u4ee3\u65b9\u6848\u6765\u6539\u5584\u516c\u5e73\u6027\u3002", "method": "\u7814\u7a76\u53d1\u73b0\u4f20\u7edf\u6f14\u793a\u9009\u62e9\u7b56\u7565\u56e0\u6240\u9009\u793a\u4f8b\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u4e0d\u5e73\u8861\u800c\u65e0\u6cd5\u786e\u4fdd\u516c\u5e73\u6027\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u516c\u5e73\u611f\u77e5\u6f14\u793a\u9009\u62e9\u65b9\u6cd5FADS\uff0c\u901a\u8fc7\u57fa\u4e8e\u805a\u7c7b\u7684\u91c7\u6837\u6784\u5efa\u4eba\u53e3\u7edf\u8ba1\u5b66\u5e73\u8861\u4e14\u8bed\u4e49\u76f8\u5173\u7684\u6f14\u793a\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u6a21\u578b\u5fae\u8c03\uff0c\u76f4\u63a5\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\u4e2d\u5b9e\u73b0\u516c\u5e73\u6027\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFADS\u4e00\u81f4\u51cf\u5c11\u4e86\u6027\u522b\u3001\u79cd\u65cf\u548c\u6c11\u65cf\u76f8\u5173\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7fa4\u4f53\u95f4\u7684\u516c\u5e73\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u516c\u5e73\u6027\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u516c\u5e73\u611f\u77e5\u4e0a\u4e0b\u6587\u5b66\u4e60\u4f5c\u4e3a\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u4e3a\u516c\u5e73\u533b\u5b66\u56fe\u50cf\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002\u8be5\u65b9\u6cd5\u4e3a\u5927\u578b\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u8ba1\u7b97\u5bc6\u96c6\u578b\u5fae\u8c03\u7684\u516c\u5e73\u6027\u63d0\u5347\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.16595", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16595", "abs": "https://arxiv.org/abs/2511.16595", "authors": ["Boshen Xu", "Zihan Xiao", "Jiaze Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Qin Jin"], "title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding", "comment": "Project page: https://xuboshen.github.io/TimeViper", "summary": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TimeViper\uff0c\u4e00\u79cd\u6df7\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408Mamba\u548cTransformer\u67b6\u6784\u4ee5\u53ca\u521b\u65b0\u7684TransV\u4ee4\u724c\u538b\u7f29\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8d85\u8fc710,000\u5e27\u7684\u957f\u65f6\u95f4\u89c6\u9891\u7684\u9ad8\u6548\u7406\u89e3\u3002", "motivation": "\u957f\u65f6\u95f4\u89c6\u9891\u7406\u89e3\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u9700\u8981\u9ad8\u6548\u7684\u6a21\u578b\u67b6\u6784\u548c\u6709\u6548\u7684\u957f\u65f6\u5e8f\u4e0a\u4e0b\u6587\u5904\u7406\u673a\u5236\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8d85\u957f\u89c6\u9891\u65f6\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u89c6\u89c9\u4ee4\u724c\u5197\u4f59\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5904\u7406\u5c0f\u65f6\u7ea7\u89c6\u9891\u7684\u80fd\u529b\u3002", "method": "TimeViper\u91c7\u7528\u6df7\u5408Mamba-Transformer\u9aa8\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u9ad8\u6548\u6027\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u8868\u8fbe\u80fd\u529b\u3002\u9488\u5bf9\u89c6\u89c9\u5230\u6587\u672c\u4fe1\u606f\u805a\u5408\u73b0\u8c61\u5bfc\u81f4\u7684\u89c6\u89c9\u4ee4\u724c\u5197\u4f59\uff0c\u63d0\u51fa\u4e86TransV\u4ee4\u724c\u4fe1\u606f\u4f20\u8f93\u6a21\u5757\uff0c\u5c06\u89c6\u89c9\u4ee4\u724c\u8f6c\u79fb\u5e76\u538b\u7f29\u5230\u6307\u4ee4\u4ee4\u724c\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eTimeViper\u80fd\u591f\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u7ade\u4e89\uff0c\u540c\u65f6\u663e\u8457\u6269\u5c55\u4e86\u5904\u7406\u5e27\u6570\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u8d85\u8fc710,000\u5e27\u7684\u5c0f\u65f6\u7ea7\u89c6\u9891\u3002\u5bf9Mamba\u548cTransformer\u5c42\u6ce8\u610f\u529b\u884c\u4e3a\u7684\u5206\u6790\u4e3a\u6df7\u5408\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u5f00\u53d1\u3001\u89e3\u91ca\u548c\u538b\u7f29\u6df7\u5408Mamba-Transformer\u67b6\u6784\u7684\u521d\u6b65\u63a2\u7d22\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u5230\u6587\u672c\u4fe1\u606f\u6d41\u4e2d\u7684\u4ee4\u724c\u5197\u4f59\u73b0\u8c61\uff0c\u5e76\u4e3a\u9ad8\u6548\u957f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u6df7\u5408\u67b6\u6784\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.15825", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15825", "abs": "https://arxiv.org/abs/2511.15825", "authors": ["Tuan-Anh Le", "Anh Mai Vu", "David Yang", "Akash Awasthi", "Hien Van Nguyen"], "title": "IMACT-CXR - An Interactive Multi-Agent Conversational Tutoring System for Chest X-Ray Interpretation", "comment": null, "summary": "IMACT-CXR is an interactive multi-agent conversational tutor that helps trainees interpret chest X-rays by unifying spatial annotation, gaze analysis, knowledge retrieval, and image-grounded reasoning in a single AutoGen-based workflow. The tutor simultaneously ingests learner bounding boxes, gaze samples, and free-text observations. Specialized agents evaluate localization quality, generate Socratic coaching, retrieve PubMed evidence, suggest similar cases from REFLACX, and trigger NV-Reason-CXR-3B for vision-language reasoning when mastery remains low or the learner explicitly asks. Bayesian Knowledge Tracing (BKT) maintains skill-specific mastery estimates that drive both knowledge reinforcement and case similarity retrieval. A lung-lobe segmentation module derived from a TensorFlow U-Net enables anatomically aware gaze feedback, and safety prompts prevent premature disclosure of ground-truth labels. We describe the system architecture, implementation highlights, and integration with the REFLACX dataset for real DICOM cases. IMACT-CXR demonstrates responsive tutoring flows with bounded latency, precise control over answer leakage, and extensibility toward live residency deployment. Preliminary evaluation shows improved localization and diagnostic reasoning compared to baselines.", "AI": {"tldr": "IMACT-CXR\u662f\u4e00\u4e2a\u57fa\u4e8eAutoGen\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u5f0f\u80f8\u90e8X\u5149\u89e3\u8bfb\u6559\u5b66\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u6807\u6ce8\u3001\u89c6\u7ebf\u5206\u6790\u3001\u77e5\u8bc6\u68c0\u7d22\u548c\u56fe\u50cf\u63a8\u7406\uff0c\u4e3a\u533b\u5b66\u5b66\u5458\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u6559\u5b66\u6307\u5bfc\u3002\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u54cd\u5e94\u5f0f\u6559\u5b66\u6d41\u7a0b\u3001\u7cbe\u786e\u7684\u7b54\u6848\u6cc4\u9732\u63a7\u5236\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5b9a\u4f4d\u548c\u8bca\u65ad\u63a8\u7406\u65b9\u9762\u7684\u6539\u8fdb\u6548\u679c\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u6559\u5b66\u4e2d\u7f3a\u4e4f\u7efc\u5408\u6027\u4ea4\u4e92\u5f0f\u6559\u5b66\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u5b66\u5458\u7684\u7a7a\u95f4\u6807\u6ce8\u3001\u89c6\u7ebf\u6570\u636e\u548c\u6587\u672c\u89c2\u5bdf\uff0c\u96be\u4ee5\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u5b9e\u65f6\u6559\u5b66\u53cd\u9988\u548c\u77e5\u8bc6\u5f3a\u5316\u3002\u73b0\u6709\u7cfb\u7edf\u5728\u6574\u5408\u591a\u79cd\u8f93\u5165\u6a21\u6001\u3001\u9632\u6b62\u7b54\u6848\u8fc7\u65e9\u6cc4\u9732\u4ee5\u53ca\u57fa\u4e8e\u6280\u80fd\u638c\u63e1\u7a0b\u5ea6\u7684\u81ea\u9002\u5e94\u6559\u5b66\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528\u57fa\u4e8eAutoGen\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u540c\u65f6\u5904\u7406\u5b66\u5458\u7684\u8fb9\u754c\u6846\u6807\u6ce8\u3001\u89c6\u7ebf\u91c7\u6837\u548c\u81ea\u7531\u6587\u672c\u89c2\u5bdf\u3002\u4e13\u7528\u667a\u80fd\u4f53\u8bc4\u4f30\u5b9a\u4f4d\u8d28\u91cf\u3001\u751f\u6210\u82cf\u683c\u62c9\u5e95\u5f0f\u6307\u5bfc\u3001\u68c0\u7d22PubMed\u8bc1\u636e\u3001\u4eceREFLACX\u63a8\u8350\u76f8\u4f3c\u75c5\u4f8b\uff0c\u5e76\u5728\u638c\u63e1\u5ea6\u4f4e\u6216\u5b66\u5458\u660e\u786e\u8981\u6c42\u65f6\u89e6\u53d1NV-Reason-CXR-3B\u8fdb\u884c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u3002\u7cfb\u7edf\u8fd8\u96c6\u6210\u4e86\u57fa\u4e8eTensorFlow U-Net\u7684\u80ba\u53f6\u5206\u5272\u6a21\u5757\u7528\u4e8e\u89e3\u5256\u611f\u77e5\u7684\u89c6\u7ebf\u53cd\u9988\uff0c\u4ee5\u53ca\u8d1d\u53f6\u65af\u77e5\u8bc6\u8ffd\u8e2a\u7528\u4e8e\u6280\u80fd\u638c\u63e1\u5ea6\u4f30\u8ba1\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cIMACT-CXR\u5728\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002\u7cfb\u7edf\u5b9e\u73b0\u4e86\u54cd\u5e94\u5f0f\u6559\u5b66\u6d41\u7a0b\uff0c\u5177\u6709\u6709\u754c\u5ef6\u8fdf\u7279\u6027\uff0c\u5e76\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u7b54\u6848\u6cc4\u9732\u3002\u8be5\u7cfb\u7edf\u4e0eREFLACX\u771f\u5b9eDICOM\u75c5\u4f8b\u6570\u636e\u96c6\u6210\u529f\u96c6\u6210\uff0c\u5c55\u793a\u4e86\u5411\u5b9e\u9645\u4f4f\u9662\u533b\u5e08\u90e8\u7f72\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "IMACT-CXR\u8bc1\u660e\u4e86\u591a\u667a\u80fd\u4f53\u67b6\u6784\u5728\u533b\u5b66\u5f71\u50cf\u6559\u5b66\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u8f93\u5165\u6a21\u6001\u548c\u81ea\u9002\u5e94\u6559\u5b66\u7b56\u7565\uff0c\u80fd\u591f\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u5b9e\u65f6\u6307\u5bfc\u3002\u8be5\u7cfb\u7edf\u4e3a\u533b\u5b66\u6559\u80b2\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8303\u5f0f\uff0c\u5177\u6709\u5411\u4e34\u5e8a\u57f9\u8bad\u73af\u5883\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u590d\u6742\u7684\u4ea4\u4e92\u5f0f\u6559\u5b66\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.16024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16024", "abs": "https://arxiv.org/abs/2511.16024", "authors": ["Xiao He", "Zhijun Tu", "Kun Cheng", "Mingrui Zhu", "Jie Hu", "Nannan Wang", "Xinbo Gao"], "title": "Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution", "comment": "16 pages, Accepted by AAAI 2026", "summary": "The demonstrated success of sparsely-gated Mixture-of-Experts (MoE) architectures, exemplified by models such as DeepSeek and Grok, has motivated researchers to investigate their adaptation to diverse domains. In real-world image super-resolution (Real-ISR), existing approaches mainly rely on fine-tuning pre-trained diffusion models through Low-Rank Adaptation (LoRA) module to reconstruct high-resolution (HR) images. However, these dense Real-ISR models are limited in their ability to adaptively capture the heterogeneous characteristics of complex real-world degraded samples or enable knowledge sharing between inputs under equivalent computational budgets. To address this, we investigate the integration of sparse MoE into Real-ISR and propose a Mixture-of-Ranks (MoR) architecture for single-step image super-resolution. We introduce a fine-grained expert partitioning strategy that treats each rank in LoRA as an independent expert. This design enables flexible knowledge recombination while isolating fixed-position ranks as shared experts to preserve common-sense features and minimize routing redundancy. Furthermore, we develop a degradation estimation module leveraging CLIP embeddings and predefined positive-negative text pairs to compute relative degradation scores, dynamically guiding expert activation. To better accommodate varying sample complexities, we incorporate zero-expert slots and propose a degradation-aware load-balancing loss, which dynamically adjusts the number of active experts based on degradation severity, ensuring optimal computational resource allocation. Comprehensive experiments validate our framework's effectiveness and state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5355\u6b65\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u6df7\u5408\u79e9\uff08MoR\uff09\u67b6\u6784\uff0c\u5c06\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u673a\u5236\u5f15\u5165\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u5212\u5206\u7b56\u7565\u548c\u9000\u5316\u611f\u77e5\u8def\u7531\u673a\u5236\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u9000\u5316\u6a21\u5f0f\u7684\u9002\u5e94\u6027\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLoRA\u5fae\u8c03\u6269\u6563\u6a21\u578b\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u81ea\u9002\u5e94\u5730\u6355\u6349\u590d\u6742\u771f\u5b9e\u4e16\u754c\u9000\u5316\u6837\u672c\u7684\u5f02\u8d28\u6027\u7279\u5f81\uff0c\u4e5f\u65e0\u6cd5\u5728\u540c\u7b49\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u8f93\u5165\u95f4\u7684\u77e5\u8bc6\u5171\u4eab\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u591a\u6837\u5316\u9000\u5316\u6a21\u5f0f\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u79e9\u67b6\u6784\uff0c\u5c06LoRA\u4e2d\u7684\u6bcf\u4e2a\u79e9\u89c6\u4e3a\u72ec\u7acb\u4e13\u5bb6\uff0c\u91c7\u7528\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u5212\u5206\u7b56\u7565\u5b9e\u73b0\u7075\u6d3b\u77e5\u8bc6\u91cd\u7ec4\uff1b\u8bbe\u8ba1\u57fa\u4e8eCLIP\u5d4c\u5165\u548c\u9884\u5b9a\u4e49\u6b63\u8d1f\u6587\u672c\u5bf9\u7684\u9000\u5316\u4f30\u8ba1\u6a21\u5757\u8ba1\u7b97\u76f8\u5bf9\u9000\u5316\u5206\u6570\uff0c\u52a8\u6001\u6307\u5bfc\u4e13\u5bb6\u6fc0\u6d3b\uff1b\u5f15\u5165\u96f6\u4e13\u5bb6\u69fd\u548c\u9000\u5316\u611f\u77e5\u8d1f\u8f7d\u5747\u8861\u635f\u5931\uff0c\u6839\u636e\u9000\u5316\u4e25\u91cd\u7a0b\u5ea6\u52a8\u6001\u8c03\u6574\u6d3b\u8dc3\u4e13\u5bb6\u6570\u91cf\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5904\u7406\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u9000\u5316\u6837\u672c\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u673a\u5236\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u5212\u5206\u548c\u52a8\u6001\u8def\u7531\u7b56\u7565\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u9000\u5316\u6a21\u5f0f\u7684\u6709\u6548\u5efa\u6a21\uff0c\u4e3a\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u4e0b\u7684\u9ad8\u6548\u56fe\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.16635", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16635", "abs": "https://arxiv.org/abs/2511.16635", "authors": ["Guolin Huang", "Wenting Chen", "Jiaqi Yang", "Xinheng Lyu", "Xiaoling Luo", "Sen Yang", "Xiaohan Xing", "Linlin Shen"], "title": "SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction", "comment": "20 pages", "summary": "Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.", "AI": {"tldr": "SurvAgent\u662f\u9996\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u751f\u5b58\u9884\u6d4b\u7684\u5206\u5c42\u601d\u7ef4\u94fe\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u6784\u5efa\u75c5\u4f8b\u5e93\u548c\u591a\u4e13\u5bb6\u667a\u80fd\u4f53\u63a8\u7406\uff0c\u5728TCGA\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u3001\u4e13\u6709\u5927\u8bed\u8a00\u6a21\u578b\u548c\u533b\u7597\u667a\u80fd\u4f53\uff0c\u4e3a\u7cbe\u51c6\u80bf\u7624\u5b66\u4e2d\u7684\u53ef\u89e3\u91caAI\u9a71\u52a8\u751f\u5b58\u9884\u6d4b\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u751f\u5b58\u5206\u6790\u65b9\u6cd5\u7f3a\u4e4f\u4e34\u5e8a\u91c7\u7528\u6240\u9700\u7684\u900f\u660e\u5ea6\uff0c\u800c\u5f53\u524d\u75c5\u7406\u667a\u80fd\u4f53\u5728\u751f\u5b58\u9884\u6d4b\u65b9\u9762\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u65e0\u6cd5\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u3001\u65e0\u6548\u7684\u5174\u8da3\u533a\u57df\u63a2\u7d22\u4ee5\u53ca\u672a\u80fd\u5229\u7528\u5386\u53f2\u75c5\u4f8b\u7684\u7ecf\u9a8c\u5b66\u4e60\u3002", "method": "SurvAgent\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u4f4e\u500d\u955c\u7b5b\u67e5\u3001\u8de8\u6a21\u6001\u76f8\u4f3c\u6027\u611f\u77e5\u8865\u4e01\u6316\u6398\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u8865\u4e01\u6316\u6398\u5bf9\u75c5\u7406\u56fe\u50cf\u8fdb\u884c\u5206\u5c42\u5206\u6790\uff0c\u540c\u65f6\u5bf9\u516d\u4e2a\u529f\u80fd\u57fa\u56e0\u7c7b\u522b\u8fdb\u884c\u57fa\u56e0\u5206\u5c42\u5206\u6790\uff0c\u751f\u6210\u5e26\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u7ed3\u6784\u5316\u62a5\u544a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u68c0\u7d22\u76f8\u4f3c\u75c5\u4f8b\uff0c\u5e76\u901a\u8fc7\u6e10\u8fdb\u533a\u95f4\u7cbe\u5316\u6574\u5408\u591a\u6a21\u6001\u62a5\u544a\u4e0e\u4e13\u5bb6\u9884\u6d4b\u3002", "result": "\u5728\u4e94\u4e2aTCGA\u961f\u5217\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSurvAgent\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u3001\u4e13\u6709\u5927\u8bed\u8a00\u6a21\u578b\u548c\u533b\u7597\u667a\u80fd\u4f53\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u6a21\u6001\u751f\u5b58\u9884\u6d4b\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7cbe\u51c6\u80bf\u7624\u5b66\u4e2d\u7684\u53ef\u89e3\u91caAI\u9a71\u52a8\u751f\u5b58\u9884\u6d4b\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5206\u5c42\u601d\u7ef4\u94fe\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u900f\u660e\u5ea6\u3001\u591a\u6a21\u6001\u6574\u5408\u548c\u7ecf\u9a8c\u5b66\u4e60\u65b9\u9762\u7684\u5c40\u9650\u3002"}}
{"id": "2511.16031", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16031", "abs": "https://arxiv.org/abs/2511.16031", "authors": ["Timilehin T. Ayanlade", "Anirudha Powadi", "Talukder Z. Jubery", "Baskar Ganapathysubramanian", "Soumik Sarkar"], "title": "Crossmodal learning for Crop Canopy Trait Estimation", "comment": "18 pages, 7 figures", "summary": "Recent advances in plant phenotyping have driven widespread adoption of multi sensor platforms for collecting crop canopy reflectance data. This includes the collection of heterogeneous data across multiple platforms, with Unmanned Aerial Vehicles (UAV) seeing significant usage due to their high performance in crop monitoring, forecasting, and prediction tasks. Similarly, satellite missions have been shown to be effective for agriculturally relevant tasks. In contrast to UAVs, such missions are bound to the limitation of spatial resolution, which hinders their effectiveness for modern farming systems focused on micro-plot management. In this work, we propose a cross modal learning strategy that enriches high-resolution satellite imagery with UAV level visual detail for crop canopy trait estimation. Using a dataset of approximately co registered satellite UAV image pairs collected from replicated plots of 84 hybrid maize varieties across five distinct locations in the U.S. Corn Belt, we train a model that learns fine grained spectral spatial correspondences between sensing modalities. Results show that the generated UAV-like representations from satellite inputs consistently outperform real satellite imagery on multiple downstream tasks, including yield and nitrogen prediction, demonstrating the potential of cross-modal correspondence learning to bridge the gap between satellite and UAV sensing in agricultural monitoring.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e0e\u65e0\u4eba\u673a\u7ea7\u522b\u7684\u89c6\u89c9\u7ec6\u8282\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u4f5c\u7269\u51a0\u5c42\u6027\u72b6\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u5728\u4ea7\u91cf\u548c\u6c2e\u7d20\u9884\u6d4b\u7b49\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u751f\u6210\u7684\u65e0\u4eba\u673a\u6837\u8868\u793a\u59cb\u7ec8\u4f18\u4e8e\u771f\u5b9e\u536b\u661f\u56fe\u50cf\u3002", "motivation": "\u5f53\u524d\u519c\u4e1a\u76d1\u6d4b\u4e2d\uff0c\u536b\u661f\u4efb\u52a1\u53d7\u9650\u4e8e\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u5fae\u533a\u7ba1\u7406\u519c\u4e1a\u7cfb\u7edf\u7684\u9700\u6c42\uff0c\u800c\u65e0\u4eba\u673a\u867d\u7136\u6027\u80fd\u4f18\u8d8a\u4f46\u8986\u76d6\u8303\u56f4\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u536b\u661f\u548c\u65e0\u4eba\u673a\u4f20\u611f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5b66\u4e60\u6765\u4e30\u5bcc\u536b\u661f\u56fe\u50cf\u7684\u89c6\u89c9\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u5b66\u4e60\u7b56\u7565\uff0c\u4f7f\u7528\u5728\u7f8e\u56fd\u7389\u7c73\u5e26\u4e94\u4e2a\u4e0d\u540c\u5730\u70b9\u6536\u96c6\u768484\u4e2a\u6742\u4ea4\u7389\u7c73\u54c1\u79cd\u7684\u8fd1\u4f3c\u5171\u914d\u51c6\u536b\u661f-\u65e0\u4eba\u673a\u56fe\u50cf\u5bf9\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6a21\u578b\u5b66\u4e60\u4e0d\u540c\u4f20\u611f\u6a21\u6001\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5149\u8c31\u7a7a\u95f4\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ece\u536b\u661f\u8f93\u5165\u751f\u6210\u7684\u65e0\u4eba\u673a\u6837\u8868\u793a\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u771f\u5b9e\u536b\u661f\u56fe\u50cf\uff0c\u7279\u522b\u662f\u5728\u4ea7\u91cf\u9884\u6d4b\u548c\u6c2e\u7d20\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8de8\u6a21\u6001\u5bf9\u5e94\u5b66\u4e60\u5177\u6709\u5f25\u5408\u519c\u4e1a\u76d1\u6d4b\u4e2d\u536b\u661f\u548c\u65e0\u4eba\u673a\u4f20\u611f\u5dee\u8ddd\u7684\u6f5c\u529b\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u519c\u4e1a\u9065\u611f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5bc6\u96c6\u65e0\u4eba\u673a\u90e8\u7f72\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u7cbe\u7ec6\u7684\u4f5c\u7269\u76d1\u6d4b\u6570\u636e\u3002"}}
{"id": "2511.16671", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16671", "abs": "https://arxiv.org/abs/2511.16671", "authors": ["Ziyu Guo", "Renrui Zhang", "Hongyu Li", "Manyuan Zhang", "Xinyan Chen", "Sifan Wang", "Yan Feng", "Peng Pei", "Pheng-Ann Heng"], "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation", "comment": "Project Page: https://think-while-gen.github.io Code: https://github.com/ZiyuGuo99/Thinking-while-Generating", "summary": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Thinking-while-Generating (TwiG)\u6846\u67b6\uff0c\u8fd9\u662f\u9996\u4e2a\u5728\u89c6\u89c9\u751f\u6210\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u6587\u672c\u63a8\u7406\u4e0e\u751f\u6210\u4ea4\u7ec7\u6f14\u8fdb\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4ea4\u4e92\u4ea7\u751f\u66f4\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u8bed\u4e49\u4e30\u5bcc\u7684\u89c6\u89c9\u8f93\u51fa\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u751f\u6210\u65b9\u6cd5\u4ec5\u5728\u751f\u6210\u524d\u4f5c\u4e3a\u9884\u89c4\u5212\u6216\u751f\u6210\u540e\u4f5c\u4e3a\u540e\u4f18\u5316\u5f15\u5165\u6587\u672c\u63a8\u7406\uff0c\u7f3a\u4e4f\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u5b9e\u65f6\u591a\u6a21\u6001\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u8fd9\u79cd\u5c40\u9650\u6027\u9650\u5236\u4e86\u751f\u6210\u5185\u5bb9\u4e0e\u8bed\u4e49\u7406\u89e3\u4e4b\u95f4\u7684\u52a8\u6001\u534f\u8c03\u3002", "method": "TwiG\u6846\u67b6\u901a\u8fc7\u5728\u89c6\u89c9\u5185\u5bb9\u9010\u6b65\u751f\u6210\u8fc7\u7a0b\u4e2d\u4ea4\u7ec7\u6587\u672c\u63a8\u7406\uff0c\u65e2\u6307\u5bfc\u5373\u5c06\u751f\u6210\u7684\u5c40\u90e8\u533a\u57df\u53c8\u53cd\u601d\u5df2\u5408\u6210\u7684\u5185\u5bb9\uff1b\u7814\u7a76\u4e86\u4e09\u79cd\u7b56\u7565\uff1a\u96f6\u6837\u672c\u63d0\u793a\u3001\u57fa\u4e8eTwiG-50K\u6570\u636e\u96c6\u7684\u76d1\u7763\u5fae\u8c03\u4ee5\u53ca\u5b9a\u5236\u7684TwiG-GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u89c6\u89c9\u751f\u6210\u4e0e\u6587\u672c\u63a8\u7406\u7684\u52a8\u6001\u534f\u540c\u8fdb\u5316\uff0c\u4ea7\u751f\u66f4\u52a0\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u8bed\u4e49\u4e30\u5bcc\u7684\u89c6\u89c9\u8f93\u51fa\uff0c\u4e09\u79cd\u7b56\u7565\u5206\u522b\u63d0\u4f9b\u4e86\u4ea4\u7ec7\u63a8\u7406\u52a8\u6001\u5b66\u7684\u72ec\u7279\u89c1\u89e3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4ea4\u7ec7\u6587\u672c\u63a8\u7406\u4ee5\u589e\u5f3a\u89c6\u89c9\u751f\u6210\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5c55\u793a\u4e86\u5b9e\u65f6\u591a\u6a21\u6001\u4ea4\u4e92\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u91cd\u8981\u4ef7\u503c\uff0c\u6709\u671b\u6fc0\u53d1\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2511.15997", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.15997", "abs": "https://arxiv.org/abs/2511.15997", "authors": ["Noah Bissell", "Ethan Paley", "Joshua Harrison", "Juliano Calil", "Myungin Lee"], "title": "Sensorium Arc: AI Agent System for Oceanic Data Exploration and Interactive Eco-Art", "comment": "(to appear) NeurIPS 2025 Creative AI Track", "summary": "Sensorium Arc (AI reflects on climate) is a real-time multimodal interactive AI agent system that personifies the ocean as a poetic speaker and guides users through immersive explorations of complex marine data. Built on a modular multi-agent system and retrieval-augmented large language model (LLM) framework, Sensorium enables natural spoken conversations with AI agents that embodies the ocean's perspective, generating responses that blend scientific insight with ecological poetics. Through keyword detection and semantic parsing, the system dynamically triggers data visualizations and audiovisual playback based on time, location, and thematic cues drawn from the dialogue. Developed in collaboration with the Center for the Study of the Force Majeure and inspired by the eco-aesthetic philosophy of Newton Harrison, Sensorium Arc reimagines ocean data not as an abstract dataset but as a living narrative. The project demonstrates the potential of conversational AI agents to mediate affective, intuitive access to high-dimensional environmental data and proposes a new paradigm for human-machine-ecosystem.", "AI": {"tldr": "Sensorium Arc\u662f\u4e00\u4e2a\u5b9e\u65f6\u591a\u6a21\u6001\u4ea4\u4e92\u5f0fAI\u4ee3\u7406\u7cfb\u7edf\uff0c\u5c06\u6d77\u6d0b\u4eba\u683c\u5316\u4e3a\u8bd7\u610f\u53d9\u8ff0\u8005\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u591a\u4ee3\u7406\u7cfb\u7edf\u548c\u68c0\u7d22\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u5b9e\u73b0\u4e0e\u6d77\u6d0b\u89c6\u89d2\u7684\u81ea\u7136\u8bed\u97f3\u5bf9\u8bdd\uff0c\u52a8\u6001\u89e6\u53d1\u6570\u636e\u53ef\u89c6\u5316\u548c\u89c6\u542c\u64ad\u653e\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73af\u5883\u6570\u636e\u62bd\u8c61\u5316\u7684\u95ee\u9898\uff0c\u91cd\u65b0\u6784\u60f3\u6d77\u6d0b\u6570\u636e\u4f5c\u4e3a\u6d3b\u751f\u751f\u7684\u53d9\u4e8b\u800c\u975e\u62bd\u8c61\u6570\u636e\u96c6\uff0c\u63a2\u7d22\u5bf9\u8bdd\u5f0fAI\u4ee3\u7406\u5728\u8c03\u89e3\u4eba\u7c7b\u5bf9\u9ad8\u7ef4\u73af\u5883\u6570\u636e\u7684\u60c5\u611f\u76f4\u89c9\u8bbf\u95ee\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u591a\u4ee3\u7406\u7cfb\u7edf\u548c\u68c0\u7d22\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u952e\u8bcd\u68c0\u6d4b\u548c\u8bed\u4e49\u89e3\u6790\u6280\u672f\uff0c\u57fa\u4e8e\u5bf9\u8bdd\u4e2d\u7684\u65f6\u95f4\u3001\u4f4d\u7f6e\u548c\u4e3b\u9898\u7ebf\u7d22\u52a8\u6001\u89e6\u53d1\u6570\u636e\u53ef\u89c6\u5316\u548c\u89c6\u542c\u64ad\u653e\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u4e0e\u6d77\u6d0b\u89c6\u89d2AI\u4ee3\u7406\u7684\u81ea\u7136\u8bed\u97f3\u5bf9\u8bdd\uff0c\u751f\u6210\u878d\u5408\u79d1\u5b66\u6d1e\u5bdf\u4e0e\u751f\u6001\u8bd7\u610f\u7684\u54cd\u5e94\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u4ea4\u4e92\u5728\u73af\u5883\u6570\u636e\u53d9\u4e8b\u5316\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4eba-\u673a-\u751f\u6001\u7cfb\u7edf\u4ea4\u4e92\u7684\u65b0\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u5bf9\u8bdd\u5f0fAI\u4ee3\u7406\u5728\u8c03\u89e3\u73af\u5883\u6570\u636e\u60c5\u611f\u8bbf\u95ee\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u751f\u6001\u7f8e\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u878d\u5408\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.16107", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16107", "abs": "https://arxiv.org/abs/2511.16107", "authors": ["Shao-Jun Xia", "Huixin Zhang", "Zhengzhong Tu"], "title": "T2T-VICL: Unlocking the Boundaries of Cross-Task Visual In-Context Learning via Implicit Text-Driven VLMs", "comment": null, "summary": "In large language models (LLM), in-context learning (ICL) refers to performing new tasks by conditioning on small demonstrations provided in the input context. Recent advances in visual in-context learning (VICL) demonstrate promising capabilities for solving downstream tasks by unified vision-language models (VLMs). When the visual prompt and the target images originate from different visual tasks, can VLMs still enable VICL? In the paper, we propose a fully collaborative pipeline, i.e. T2T-VICL, for VLMs to investigate the potential of cross-task VICL. Fundamentally, we design a mechanism to generate and select text prompts that best implicitly describe the differences between two distinct low-level vision tasks, and construct the first cross-task VICL dataset. Building upon this, we propose a novel inference framework that combines perceptual score-based reasoning with traditional evaluation metrics to perform cross-task VICL. Our approach achieves top-tier results across nine cross-task scenarios and second-tier performance in ten additional scenarios, unlocking the boundaries of cross-task VICL within VLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faT2T-VICL\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u751f\u6210\u548c\u611f\u77e5\u8bc4\u5206\u63a8\u7406\u673a\u5236\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u4efb\u52a1\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u5e94\u7528\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u4efb\u52a1\u8fb9\u754c\u9650\u5236\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e3b\u8981\u5c40\u9650\u4e8e\u540c\u4efb\u52a1\u573a\u666f\uff0c\u5f53\u89c6\u89c9\u63d0\u793a\u4e0e\u76ee\u6807\u56fe\u50cf\u6765\u81ea\u4e0d\u540c\u89c6\u89c9\u4efb\u52a1\u65f6\uff0c\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4ecd\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u8de8\u4efb\u52a1\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u4ecd\u662f\u4e00\u4e2a\u672a\u63a2\u7d22\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51faT2T-VICL\u5168\u534f\u4f5c\u6d41\u6c34\u7ebf\uff0c\u8bbe\u8ba1\u6587\u672c\u63d0\u793a\u751f\u6210\u4e0e\u9009\u62e9\u673a\u5236\u6765\u9690\u5f0f\u63cf\u8ff0\u4e0d\u540c\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\u95f4\u7684\u5dee\u5f02\uff0c\u6784\u5efa\u9996\u4e2a\u8de8\u4efb\u52a1VICL\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u7ed3\u5408\u611f\u77e5\u8bc4\u5206\u63a8\u7406\u4e0e\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u7684\u65b0\u578b\u63a8\u7406\u6846\u67b6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e5d\u4e2a\u8de8\u4efb\u52a1\u573a\u666f\u4e2d\u53d6\u5f97\u9876\u7ea7\u6027\u80fd\uff0c\u5728\u53e6\u5916\u5341\u4e2a\u573a\u666f\u4e2d\u83b7\u5f97\u6b21\u4f18\u6027\u80fd\uff0c\u663e\u8457\u6269\u5c55\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u4efb\u52a1\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u80fd\u529b\u8fb9\u754c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u4efb\u52a1\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u7edf\u4e00\u6a21\u578b\u5904\u7406\u591a\u6837\u5316\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u5411\u66f4\u901a\u7528\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2511.16014", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16014", "abs": "https://arxiv.org/abs/2511.16014", "authors": ["Jinhao Li", "Jianzhong Qi", "Soyeon Caren Han", "Eun-Jung Holden"], "title": "MUSEKG: A Knowledge Graph Over Museum Collections", "comment": null, "summary": "Digital transformation in the cultural heritage sector has produced vast yet fragmented collections of artefact data. Existing frameworks for museum information systems struggle to integrate heterogeneous metadata, unstructured documents, and multimodal artefacts into a coherent and queryable form. We present MuseKG, an end-to-end knowledge-graph framework that unifies structured and unstructured museum data through symbolic-neural integration. MuseKG constructs a typed property graph linking objects, people, organisations, and visual or textual labels, and supports natural language queries. Evaluations on real museum collections demonstrate robust performance across queries over attributes, relations, and related entities, surpassing large-language-model zero-shot, few-shot and SPARQL prompt baselines. The results highlight the importance of symbolic grounding for interpretable and scalable cultural heritage reasoning, and pave the way for web-scale integration of digital heritage knowledge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MuseKG\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u77e5\u8bc6\u56fe\u8c31\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7-\u795e\u7ecf\u96c6\u6210\u7edf\u4e00\u535a\u7269\u9986\u4e2d\u7684\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u3002\u8be5\u6846\u67b6\u5728\u771f\u5b9e\u535a\u7269\u9986\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548cSPARQL\u63d0\u793a\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u7b26\u53f7\u57fa\u7840\u5728\u53ef\u89e3\u91ca\u6587\u5316\u9057\u4ea7\u63a8\u7406\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u6587\u5316\u9057\u4ea7\u9886\u57df\u7684\u6570\u5b57\u5316\u8f6c\u578b\u4ea7\u751f\u4e86\u5927\u91cf\u4f46\u788e\u7247\u5316\u7684\u6587\u7269\u6570\u636e\uff0c\u73b0\u6709\u535a\u7269\u9986\u4fe1\u606f\u7cfb\u7edf\u96be\u4ee5\u5c06\u5f02\u6784\u5143\u6570\u636e\u3001\u975e\u7ed3\u6784\u5316\u6587\u6863\u548c\u591a\u6a21\u6001\u6587\u7269\u6574\u5408\u4e3a\u7edf\u4e00\u53ef\u67e5\u8be2\u7684\u5f62\u5f0f\u3002", "method": "MuseKG\u6784\u5efa\u7c7b\u578b\u5316\u5c5e\u6027\u56fe\uff0c\u8fde\u63a5\u5bf9\u8c61\u3001\u4eba\u7269\u3001\u7ec4\u7ec7\u4ee5\u53ca\u89c6\u89c9\u6216\u6587\u672c\u6807\u7b7e\uff0c\u5e76\u652f\u6301\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u901a\u8fc7\u7b26\u53f7-\u795e\u7ecf\u96c6\u6210\u65b9\u6cd5\u7edf\u4e00\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u535a\u7269\u9986\u6570\u636e\u3002", "result": "\u5728\u771f\u5b9e\u535a\u7269\u9986\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5c5e\u6027\u67e5\u8be2\u3001\u5173\u7cfb\u67e5\u8be2\u548c\u76f8\u5173\u5b9e\u4f53\u67e5\u8be2\u65b9\u9762\u8868\u73b0\u51fa\u7a33\u5065\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548cSPARQL\u63d0\u793a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u7b26\u53f7\u57fa\u7840\u5bf9\u4e8e\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u6587\u5316\u9057\u4ea7\u63a8\u7406\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6570\u5b57\u9057\u4ea7\u77e5\u8bc6\u5728\u7f51\u7edc\u89c4\u6a21\u4e0a\u7684\u6574\u5408\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5c55\u793a\u4e86\u7b26\u53f7-\u795e\u7ecf\u6df7\u5408\u65b9\u6cd5\u5728\u590d\u6742\u6587\u5316\u9057\u4ea7\u6570\u636e\u7ba1\u7406\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.16136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16136", "abs": "https://arxiv.org/abs/2511.16136", "authors": ["Jiazhen Yan", "Ziqiang Li", "Fan Wang", "Kai Zeng", "Zhangjie Fu"], "title": "How Noise Benefits AI-generated Image Detection", "comment": null, "summary": "The rapid advancement of generative models has made real and synthetic images increasingly indistinguishable. Although extensive efforts have been devoted to detecting AI-generated images, out-of-distribution generalization remains a persistent challenge. We trace this weakness to spurious shortcuts exploited during training and we also observe that small feature-space perturbations can mitigate shortcut dominance. To address this problem in a more controllable manner, we propose the Positive-Incentive Noise for CLIP (PiN-CLIP), which jointly trains a noise generator and a detection network under a variational positive-incentive principle. Specifically, we construct positive-incentive noise in the feature space via cross-attention fusion of visual and categorical semantic features. During optimization, the noise is injected into the feature space to fine-tune the visual encoder, suppressing shortcut-sensitive directions while amplifying stable forensic cues, thereby enabling the extraction of more robust and generalized artifact representations. Comparative experiments are conducted on an open-world dataset comprising synthetic images generated by 42 distinct generative models. Our method achieves new state-of-the-art performance, with notable improvements of 5.4 in average accuracy over existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPiN-CLIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u566a\u58f0\u751f\u6210\u5668\u548c\u68c0\u6d4b\u7f51\u7edc\uff0c\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u6ce8\u5165\u6b63\u6fc0\u52b1\u566a\u58f0\u6765\u6291\u5236\u6377\u5f84\u654f\u611f\u65b9\u5411\u5e76\u653e\u5927\u7a33\u5b9a\u53d6\u8bc1\u7ebf\u7d22\uff0c\u572842\u79cd\u751f\u6210\u6a21\u578b\u5408\u6210\u7684\u5f00\u653e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u771f\u5b9e\u56fe\u50cf\u4e0e\u5408\u6210\u56fe\u50cf\u8d8a\u6765\u8d8a\u96be\u4ee5\u533a\u5206\uff0c\u5c3d\u7ba1\u5df2\u6709\u5927\u91cf\u7814\u7a76\u81f4\u529b\u4e8e\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\uff0c\u4f46\u5206\u5e03\u5916\u6cdb\u5316\u4ecd\u7136\u662f\u4e00\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\u3002\u4f5c\u8005\u5c06\u6b64\u5f31\u70b9\u5f52\u56e0\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u88ab\u5229\u7528\u7684\u865a\u5047\u6377\u5f84\uff0c\u5e76\u89c2\u5bdf\u5230\u5c0f\u7684\u7279\u5f81\u7a7a\u95f4\u6270\u52a8\u53ef\u4ee5\u7f13\u89e3\u6377\u5f84\u4e3b\u5bfc\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6b63\u6fc0\u52b1\u566a\u58f0CLIP\uff08PiN-CLIP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u6b63\u6fc0\u52b1\u539f\u7406\u8054\u5408\u8bad\u7ec3\u566a\u58f0\u751f\u6210\u5668\u548c\u68c0\u6d4b\u7f51\u7edc\u3002\u5177\u4f53\u800c\u8a00\uff0c\u901a\u8fc7\u89c6\u89c9\u548c\u7c7b\u522b\u8bed\u4e49\u7279\u5f81\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u6784\u5efa\u6b63\u6fc0\u52b1\u566a\u58f0\uff0c\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5c06\u566a\u58f0\u6ce8\u5165\u7279\u5f81\u7a7a\u95f4\u4ee5\u5fae\u8c03\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u6291\u5236\u6377\u5f84\u654f\u611f\u65b9\u5411\u540c\u65f6\u653e\u5927\u7a33\u5b9a\u53d6\u8bc1\u7ebf\u7d22\u3002", "result": "\u5728\u5305\u542b42\u79cd\u4e0d\u540c\u751f\u6210\u6a21\u578b\u5408\u6210\u56fe\u50cf\u7684\u5f00\u653e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6bd4\u8f83\u5b9e\u9a8c\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u5e73\u5747\u51c6\u786e\u7387\u4e0a\u663e\u8457\u63d0\u5347\u4e865.4\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u6270\u52a8\u6709\u6548\u7f13\u89e3\u4e86AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u4e2d\u7684\u6377\u5f84\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u53d6\u4e86\u66f4\u9c81\u68d2\u548c\u6cdb\u5316\u7684\u4f2a\u5f71\u8868\u793a\uff0c\u4e3a\u5206\u5e03\u5916\u6cdb\u5316\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u6b63\u6fc0\u52b1\u566a\u58f0\u5728\u63d0\u5347\u68c0\u6d4b\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.16183", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16183", "abs": "https://arxiv.org/abs/2511.16183", "authors": ["Jeremie Ochin", "Raphael Chekroun", "Bogdan Stanciulescu", "Sotiris Manitsaris"], "title": "FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos", "comment": null, "summary": "Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FOOTPASS\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u9762\u5411\u5b8c\u6574\u8db3\u7403\u6bd4\u8d5b\u7684\u591a\u6a21\u6001\u3001\u591a\u667a\u80fd\u4f53\u6218\u672f\u80cc\u666f\u4e0b\u7684\u9010\u573a\u52a8\u4f5c\u8bc6\u522b\u57fa\u51c6\uff0c\u901a\u8fc7\u6574\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u8f93\u51fa\u548c\u8db3\u7403\u6218\u672f\u5148\u9a8c\u77e5\u8bc6\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u9010\u573a\u6570\u636e\u6d41\u81ea\u52a8\u63d0\u53d6\u3002", "motivation": "\u5f53\u524d\u8db3\u7403\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u5728\u6784\u5efa\u53ef\u9760\u7684\u9010\u573a\u6570\u636e\u65b9\u9762\u4ecd\u663e\u4e0d\u8db3\uff0c\u901a\u5e38\u53ea\u80fd\u8f85\u52a9\u800c\u975e\u5b8c\u5168\u81ea\u52a8\u5316\u6807\u6ce8\uff0c\u800c\u6218\u672f\u5efa\u6a21\u3001\u8f68\u8ff9\u9884\u6d4b\u548c\u6027\u80fd\u5206\u6790\u7b49\u7814\u7a76\u90fd\u4f9d\u8d56\u4e8e\u6bd4\u8d5b\u72b6\u6001\u548c\u9010\u573a\u6570\u636e\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u5229\u7528\u6218\u672f\u77e5\u8bc6\u4f5c\u4e3a\u5148\u9a8c\u6765\u652f\u6301\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u9884\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u3001\u591a\u667a\u80fd\u4f53\u6218\u672f\u80cc\u666f\u4e0b\u7684\u9010\u573a\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\uff0c\u6574\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u8f93\u51fa\uff08\u5982\u8ddf\u8e2a\u3001\u8bc6\u522b\uff09\u548c\u8db3\u7403\u6218\u672f\u5148\u9a8c\u77e5\u8bc6\uff0c\u5305\u62ec\u957f\u671f\u6218\u672f\u89c4\u5f8b\u6027\uff0c\u4ee5\u751f\u6210\u53ef\u9760\u7684\u9010\u573a\u6570\u636e\u6d41\u3002", "result": "\u5efa\u7acb\u4e86FOOTPASS\u6570\u636e\u96c6\u4f5c\u4e3a\u9996\u4e2a\u9762\u5411\u5b8c\u6574\u8db3\u7403\u6bd4\u8d5b\u7684\u9010\u573a\u52a8\u4f5c\u8bc6\u522b\u57fa\u51c6\uff0c\u652f\u6301\u5f00\u53d1\u4ee5\u7403\u5458\u4e3a\u4e2d\u5fc3\u7684\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u4f53\u80b2\u5206\u6790\u63d0\u4f9b\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5c06\u6218\u672f\u77e5\u8bc6\u4f5c\u4e3a\u5148\u9a8c\u6574\u5408\u5230\u8ba1\u7b97\u673a\u89c6\u89c9\u9884\u6d4b\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u66f4\u81ea\u52a8\u5316\u548c\u53ef\u9760\u7684\u8db3\u7403\u9010\u573a\u6570\u636e\u63d0\u53d6\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5bf9\u6570\u636e\u9a71\u52a8\u7684\u4f53\u80b2\u5206\u6790\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.16150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16150", "abs": "https://arxiv.org/abs/2511.16150", "authors": ["Chunxu Liu", "Jiyuan Yang", "Ruopeng Gao", "Yuhan Zhu", "Feng Zhu", "Rui Zhao", "Limin Wang"], "title": "Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval", "comment": null, "summary": "Multimodal embeddings are widely used in downstream tasks such as multimodal retrieval, enabling alignment of interleaved modalities in a shared representation space. While recent studies show that Multimodal Large Language Models (MLLMs) can serve as strong embedding extractors, existing approaches treat embedding extraction as a direct encoding step, overlooking the fact that MLLMs possess the generative capability for reasoning that could be leveraged to enhance representation quality. In this work, we explore how to explicitly incorporate reasoning into the embedding process. To this end, we propose Reasoning Guided Embeddings (RGE), which preserves the generative rationale process of MLLMs and couples it with contrastive training. Our method first enables the model to perform structured rationale generation conditioned on the instruction, and then extracts representations after reasoning has unfolded. This simple design enhances the context-conditional inference signals within the embedding, leading to improved multimodal representation quality. Experiments on the MMEB benchmark show that reasoning-guided conditioning improves multimodal retrieval performance by 4.9% over the non-reasoning baseline, confirming that explicit reasoning can effectively enhance embedding quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u63a8\u7406\u5f15\u5bfc\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u5f0f\u63a8\u7406\u80fd\u529b\u663e\u5f0f\u6574\u5408\u5230\u5d4c\u5165\u63d0\u53d6\u8fc7\u7a0b\u4e2d\uff0c\u5229\u7528\u7ed3\u6784\u5316\u539f\u7406\u751f\u6210\u548c\u5bf9\u6bd4\u8bad\u7ec3\u6765\u589e\u5f3a\u591a\u6a21\u6001\u8868\u793a\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u63d0\u53d6\u89c6\u4e3a\u76f4\u63a5\u7f16\u7801\u6b65\u9aa4\uff0c\u5ffd\u89c6\u4e86\u5176\u751f\u6210\u5f0f\u63a8\u7406\u80fd\u529b\u53ef\u7528\u4e8e\u63d0\u5347\u8868\u793a\u8d28\u91cf\u7684\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5982\u4f55\u5c06\u663e\u5f0f\u63a8\u7406\u6574\u5408\u5230\u5d4c\u5165\u8fc7\u7a0b\u4e2d\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u5f15\u5bfc\u5d4c\u5165\u65b9\u6cd5\uff0c\u9996\u5148\u8ba9\u6a21\u578b\u57fa\u4e8e\u6307\u4ee4\u6267\u884c\u7ed3\u6784\u5316\u539f\u7406\u751f\u6210\uff0c\u5728\u63a8\u7406\u5c55\u5f00\u540e\u63d0\u53d6\u8868\u793a\uff0c\u5e76\u5c06\u751f\u6210\u5f0f\u63a8\u7406\u8fc7\u7a0b\u4e0e\u5bf9\u6bd4\u8bad\u7ec3\u76f8\u7ed3\u5408\u6765\u589e\u5f3a\u5d4c\u5165\u4e2d\u7684\u4e0a\u4e0b\u6587\u6761\u4ef6\u63a8\u7406\u4fe1\u53f7\u3002", "result": "\u5728MMEB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u63a8\u7406\u5f15\u5bfc\u7684\u6761\u4ef6\u5316\u65b9\u6cd5\u76f8\u6bd4\u975e\u63a8\u7406\u57fa\u7ebf\u5c06\u591a\u6a21\u6001\u68c0\u7d22\u6027\u80fd\u63d0\u5347\u4e864.9%\uff0c\u8bc1\u5b9e\u663e\u5f0f\u63a8\u7406\u80fd\u6709\u6548\u589e\u5f3a\u5d4c\u5165\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u5f0f\u63a8\u7406\u80fd\u529b\u53ef\u88ab\u663e\u5f0f\u5229\u7528\u6765\u589e\u5f3a\u5d4c\u5165\u8868\u793a\uff0c\u4e3a\u6539\u8fdb\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5373\u901a\u8fc7\u6574\u5408\u63a8\u7406\u8fc7\u7a0b\u6765\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2511.16205", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16205", "abs": "https://arxiv.org/abs/2511.16205", "authors": ["Xu Qiang", "Shengyuan Bai", "Leqing Chen", "Zijing Liu", "Yu Li"], "title": "ChemLabs on ChemO: A Multi-Agent System for Multimodal Reasoning on IChO 2025", "comment": "13 pages, 1 figures", "summary": "Olympiad-level benchmarks in mathematics and physics are crucial testbeds for advanced AI reasoning, but chemistry, with its unique multimodal symbolic language, has remained an open challenge. We introduce ChemO, a new benchmark built from the International Chemistry Olympiad (IChO) 2025. ChemO features two key innovations for automated assessment: Assessment-Equivalent Reformulation (AER), which converts problems requiring visual outputs (e.g., drawing molecules) into computationally tractable formats, and Structured Visual Enhancement (SVE), a diagnostic mechanism to disentangle a model's visual perception capabilities from its core chemical reasoning. To tackle this benchmark, we propose ChemLabs, a hierarchical multi-agent framework that mimics human expert collaboration through specialized agents for problem decomposition, perception, reasoning, and auditing. Experiments on state-of-the-art multimodal models demonstrate that combining SVE with our multi-agent system yields dramatic performance gains. Our top configuration achieves a score of 93.6 out of 100, surpassing an estimated human gold medal threshold and establishing a new state-of-the-art in automated chemical problem-solving. ChemO Dataset: https://huggingface.co/datasets/IDEA-AI4SCI/ChemO", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ChemO\u57fa\u51c6\u6d4b\u8bd5\u548cChemLabs\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5316\u5b66\u5965\u6797\u5339\u514b\u7ade\u8d5b\u7ea7\u522b\u7684AI\u63a8\u7406\u6311\u6218\u3002\u901a\u8fc7\u8bc4\u4f30\u7b49\u6548\u91cd\u6784\u548c\u7ed3\u6784\u5316\u89c6\u89c9\u589e\u5f3a\u6280\u672f\uff0c\u5b9e\u73b0\u4e8693.6/100\u7684\u4f18\u5f02\u6210\u7ee9\uff0c\u8d85\u8d8a\u4e86\u4eba\u7c7b\u91d1\u724c\u6c34\u5e73\u3002", "motivation": "\u5f53\u524dAI\u63a8\u7406\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u6570\u5b66\u548c\u7269\u7406\u9886\u57df\uff0c\u800c\u5316\u5b66\u56e0\u5176\u72ec\u7279\u7684\u591a\u6a21\u6001\u7b26\u53f7\u8bed\u8a00\u7279\u6027\u4e00\u76f4\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002\u5316\u5b66\u5965\u6797\u5339\u514b\u7ade\u8d5b\u7ea7\u522b\u7684\u63a8\u7406\u4efb\u52a1\u9700\u8981\u5904\u7406\u590d\u6742\u7684\u89c6\u89c9\u8f93\u51fa\u548c\u7b26\u53f7\u63a8\u7406\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u3002", "method": "\u63d0\u51fa\u4e86ChemO\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u8bc4\u4f30\u7b49\u6548\u91cd\u6784\u6280\u672f\u5c06\u9700\u8981\u89c6\u89c9\u8f93\u51fa\u7684\u95ee\u9898\u8f6c\u6362\u4e3a\u8ba1\u7b97\u53ef\u884c\u7684\u683c\u5f0f\uff0c\u4ee5\u53ca\u7ed3\u6784\u5316\u89c6\u89c9\u589e\u5f3a\u673a\u5236\u6765\u5206\u79bb\u6a21\u578b\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u548c\u5316\u5b66\u63a8\u7406\u80fd\u529b\u3002\u540c\u65f6\u5f00\u53d1\u4e86ChemLabs\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u534f\u4f5c\uff0c\u5305\u542b\u95ee\u9898\u5206\u89e3\u3001\u611f\u77e5\u3001\u63a8\u7406\u548c\u5ba1\u6838\u7b49\u4e13\u95e8\u5316\u667a\u80fd\u4f53\u3002", "result": "\u5728\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u89c6\u89c9\u589e\u5f3a\u4e0e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u6700\u4f18\u914d\u7f6e\u83b7\u5f97\u4e8693.6\u5206\uff08\u6ee1\u5206100\u5206\uff09\uff0c\u8d85\u8fc7\u4e86\u9884\u4f30\u7684\u4eba\u7c7b\u91d1\u724c\u9608\u503c\uff0c\u5728\u81ea\u52a8\u5316\u5316\u5b66\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u4e13\u95e8\u7684\u57fa\u51c6\u8bbe\u8ba1\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0cAI\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u5316\u5b66\u63a8\u7406\u4efb\u52a1\u3002\u7ed3\u6784\u5316\u89c6\u89c9\u589e\u5f3a\u6280\u672f\u4e3a\u5206\u79bb\u4e0d\u540c\u8ba4\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u8bca\u65ad\u5de5\u5177\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u63a8\u7406\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2511.16156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16156", "abs": "https://arxiv.org/abs/2511.16156", "authors": ["Jian Ma", "Qirong Peng", "Xujie Zhu", "Peixing Xie", "Chen Chen", "Haonan Lu"], "title": "Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers", "comment": "https://github.com/OPPO-Mente-Lab/Qwen-Image-Pruning", "summary": "Diffusion Transformers (DiTs) have shown exceptional performance in image generation, yet their large parameter counts incur high computational costs, impeding deployment in resource-constrained settings. To address this, we propose Pluggable Pruning with Contiguous Layer Distillation (PPCL), a flexible structured pruning framework specifically designed for DiT architectures. First, we identify redundant layer intervals through a linear probing mechanism combined with the first-order differential trend analysis of similarity metrics. Subsequently, we propose a plug-and-play teacher-student alternating distillation scheme tailored to integrate depth-wise and width-wise pruning within a single training phase. This distillation framework enables flexible knowledge transfer across diverse pruning ratios, eliminating the need for per-configuration retraining. Extensive experiments on multiple Multi-Modal Diffusion Transformer architecture models demonstrate that PPCL achieves a 50\\% reduction in parameter count compared to the full model, with less than 3\\% degradation in key objective metrics. Notably, our method maintains high-quality image generation capabilities while achieving higher compression ratios, rendering it well-suited for resource-constrained environments. The open-source code, checkpoints for PPCL can be found at the following link: https://github.com/OPPO-Mente-Lab/Qwen-Image-Pruning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPPCL\u6846\u67b6\uff0c\u4e00\u79cd\u4e13\u95e8\u4e3a\u6269\u6563\u53d8\u6362\u5668\u8bbe\u8ba1\u7684\u53ef\u63d2\u62d4\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u5c42\u84b8\u998f\u5b9e\u73b050%\u53c2\u6570\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u90e8\u7f72\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53c2\u6570\u91cf\u5927\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u963b\u788d\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u5e94\u7528\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53ef\u63d2\u62d4\u526a\u679d\u4e0e\u8fde\u7eed\u5c42\u84b8\u998f\u6846\u67b6\uff0c\u9996\u5148\u901a\u8fc7\u7ebf\u6027\u63a2\u6d4b\u548c\u4e00\u9636\u5fae\u5206\u8d8b\u52bf\u5206\u6790\u8bc6\u522b\u5197\u4f59\u5c42\u533a\u95f4\uff0c\u7136\u540e\u8bbe\u8ba1\u5373\u63d2\u5373\u7528\u7684\u5e08\u751f\u4ea4\u66ff\u84b8\u998f\u65b9\u6848\uff0c\u5728\u5355\u4e00\u8bad\u7ec3\u9636\u6bb5\u5185\u96c6\u6210\u6df1\u5ea6\u548c\u5bbd\u5ea6\u526a\u679d\u3002", "result": "\u5728\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPPCL\u76f8\u6bd4\u5b8c\u6574\u6a21\u578b\u5b9e\u73b050%\u53c2\u6570\u51cf\u5c11\uff0c\u5173\u952e\u5ba2\u89c2\u6307\u6807\u9000\u5316\u5c0f\u4e8e3%\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u80fd\u529b\u7684\u540c\u65f6\u8fbe\u5230\u66f4\u9ad8\u538b\u7f29\u6bd4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6269\u6563\u53d8\u6362\u5668\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u538b\u7f29\u65b9\u6848\uff0c\u652f\u6301\u591a\u79cd\u526a\u679d\u914d\u7f6e\u800c\u65e0\u9700\u9010\u914d\u7f6e\u91cd\u65b0\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u53ef\u884c\u6027\u3002"}}
{"id": "2511.16216", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16216", "abs": "https://arxiv.org/abs/2511.16216", "authors": ["Zhen Hao Wong", "Jingwen Deng", "Hao Liang", "Runming He", "Chengyu Shen", "Wentao Zhang"], "title": "FlipVQA-Miner: Cross-Page Visual Question-Answer Mining from Textbooks", "comment": null, "summary": "The development of Large Language Models (LLMs) increasingly depends on high-quality supervised data, yet existing instruction-tuning and RL datasets remain costly to curate and often rely on synthetic samples that introduce hallucination and limited diversity. At the same time, textbooks and exercise materials contain abundant, high-quality human-authored Question-Answer(QA) content that remains underexploited due to the difficulty of transforming raw PDFs into AI-ready supervision. Although modern OCR and vision-language models can accurately parse document structure, their outputs lack the semantic alignment required for training. We propose an automated pipeline that extracts well-formed QA and visual-QA (VQA) pairs from educational documents by combining layout-aware OCR with LLM-based semantic parsing. Experiments across diverse document types show that the method produces accurate, aligned, and low-noise QA/VQA pairs. This approach enables scalable use of real-world educational content and provides a practical alternative to synthetic data generation for improving reasoning-oriented LLM training. All code and data-processing pipelines are open-sourced at https://github.com/OpenDCAI/DataFlow.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u7ed3\u5408\u5e03\u5c40\u611f\u77e5OCR\u4e0e\u57fa\u4e8eLLM\u7684\u8bed\u4e49\u89e3\u6790\uff0c\u4ece\u6559\u80b2\u6587\u6863\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\u548c\u89c6\u89c9\u95ee\u7b54\u5bf9\uff0c\u4e3aLLM\u8bad\u7ec3\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u771f\u5b9e\u4e16\u754c\u6559\u80b2\u5185\u5bb9\u66ff\u4ee3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u51c6\u786e\u5bf9\u9f50\u4e14\u4f4e\u566a\u58f0\u7684\u76d1\u7763\u6570\u636e\uff0c\u6709\u6548\u7f13\u89e3\u5408\u6210\u6570\u636e\u5e26\u6765\u7684\u5e7b\u89c9\u548c\u591a\u6837\u6027\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u4e25\u91cd\u4f9d\u8d56\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\uff0c\u4f46\u73b0\u6709\u7684\u6307\u4ee4\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u96c6\u6784\u5efa\u6210\u672c\u9ad8\u6602\u4e14\u901a\u5e38\u4f9d\u8d56\u5408\u6210\u6837\u672c\uff0c\u5bfc\u81f4\u5e7b\u89c9\u95ee\u9898\u548c\u591a\u6837\u6027\u53d7\u9650\u3002\u540c\u65f6\uff0c\u6559\u79d1\u4e66\u548c\u7ec3\u4e60\u6750\u6599\u5305\u542b\u4e30\u5bcc\u7684\u9ad8\u8d28\u91cf\u4eba\u5de5\u7f16\u5199\u95ee\u7b54\u5185\u5bb9\uff0c\u4f46\u7531\u4e8e\u5c06\u539f\u59cbPDF\u8f6c\u6362\u4e3aAI\u5c31\u7eea\u76d1\u7763\u6570\u636e\u7684\u56f0\u96be\u800c\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u5e03\u5c40\u611f\u77e5OCR\u6280\u672f\u4e0e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u89e3\u6790\uff0c\u4ece\u6559\u80b2\u6587\u6863\u4e2d\u63d0\u53d6\u683c\u5f0f\u826f\u597d\u7684\u95ee\u7b54\u5bf9\u548c\u89c6\u89c9\u95ee\u7b54\u5bf9\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u5229\u7528\u73b0\u4ee3OCR\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u51c6\u786e\u89e3\u6790\u6587\u6863\u7ed3\u6784\uff0c\u7136\u540e\u901a\u8fc7LLM\u8fdb\u884c\u8bed\u4e49\u5bf9\u9f50\u5904\u7406\uff0c\u786e\u4fdd\u8f93\u51fa\u6570\u636e\u7b26\u5408\u8bad\u7ec3\u8981\u6c42\u3002", "result": "\u5728\u591a\u79cd\u6587\u6863\u7c7b\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u51c6\u786e\u3001\u5bf9\u9f50\u4e14\u4f4e\u566a\u58f0\u7684\u95ee\u7b54\u5bf9\u548c\u89c6\u89c9\u95ee\u7b54\u5bf9\u3002\u63d0\u53d6\u7684\u6570\u636e\u8d28\u91cf\u9ad8\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u63a8\u7406\u5bfc\u5411\u7684LLM\u8bad\u7ec3\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u6559\u80b2\u5185\u5bb9\u7684\u89c4\u6a21\u5316\u4f7f\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5229\u7528\u771f\u5b9e\u6559\u80b2\u5185\u5bb9\u4f5c\u4e3a\u76d1\u7763\u6570\u636e\u6e90\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u6539\u8fdbLLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5408\u6210\u6570\u636e\u66ff\u4ee3\u65b9\u6848\u3002\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u7684\u5f00\u53d1\u4f7f\u5f97\u5927\u89c4\u6a21\u5229\u7528\u9ad8\u8d28\u91cf\u4eba\u7c7b\u7f16\u5199\u5185\u5bb9\u6210\u4e3a\u53ef\u80fd\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u3002\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u5904\u7406\u6d41\u6c34\u7ebf\u5747\u5df2\u5f00\u6e90\uff0c\u4fc3\u8fdb\u793e\u533a\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2511.16160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16160", "abs": "https://arxiv.org/abs/2511.16160", "authors": ["Yibin Huang", "Wang Xu", "Wanyue Zhang", "Helu Zhi", "Jingjing Huang", "Yangbin Xu", "Yangang Sun", "Conghui Zhu", "Tiejun Zhao"], "title": "Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning", "comment": null, "summary": "Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Video2Layout\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u89c6\u9891\u4e2d\u91cd\u5efa\u57fa\u4e8e\u5ea6\u91cf\u7684\u7a7a\u95f4\u5e03\u5c40\uff0c\u4f7f\u7528\u8fde\u7eed\u7269\u4f53\u8fb9\u754c\u5750\u6807\u6765\u91cf\u5316\u7269\u4f53\u95f4\u7684\u7269\u7406\u8ddd\u79bb\u548c\u5c3a\u5bf8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u4e8e\u7f51\u683c\u8ba4\u77e5\u5730\u56fe\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7f51\u683c\u7684\u8ba4\u77e5\u5730\u56fe\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u79bb\u6563\u5316\u7684\u6805\u683c\u8868\u793a\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u7cbe\u786e\u91cf\u5316\u7269\u4f53\u95f4\u7684\u7269\u7406\u8ddd\u79bb\u548c\u5c3a\u5bf8\uff0c\u5bfc\u81f4\u7a7a\u95f4\u5173\u7cfb\u63cf\u8ff0\u5b58\u5728\u56fa\u6709\u6a21\u7cca\u6027\u3002", "method": "Video2Layout\u6846\u67b6\u91c7\u7528\u8fde\u7eed\u7269\u4f53\u8fb9\u754c\u5750\u6807\u6765\u91cf\u5316\u7269\u4f53\u95f4\u7269\u7406\u8ddd\u79bb\u548c\u7269\u4f53\u5c3a\u5bf8\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u9636\u6bb5\uff1a\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u4eceAI2THOR\u6a21\u62df\u5668\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5b66\u4e60\u89c6\u89c9\u8f93\u5165\u5230\u7cbe\u786e\u8fb9\u754c\u5750\u6807\u7684\u6620\u5c04\uff0c\u5f3a\u5316\u5fae\u8c03\u9636\u6bb5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728QVS-Bench\u548c\u4e3b\u6d41\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cV2LO-7B\u6a21\u578b\u76f8\u6bd4\u57fa\u4e8e\u7f51\u683c\u5730\u56fe\u8bad\u7ec3\u7684\u6a21\u578b\u5e73\u5747\u63d0\u5347\u4e864.92%\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u4e86\u8ba4\u77e5\u5730\u56fe\u51c6\u786e\u6027\u4e0e\u56fe\u50cf\u6570\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "\u8fde\u7eed\u8fb9\u754c\u5750\u6807\u8868\u793a\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7a7a\u95f4\u5173\u7cfb\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u66f4\u7cbe\u786e\u7684\u7a7a\u95f4\u8ba4\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u540c\u65f6\u63d0\u51fa\u7684QVS-Bench\u4e3a\u76f8\u5173\u673a\u5236\u5206\u6790\u63d0\u4f9b\u4e86\u8bca\u65ad\u57fa\u51c6\u3002"}}
{"id": "2511.16163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16163", "abs": "https://arxiv.org/abs/2511.16163", "authors": ["Zhi Luo", "Zenghui Yuan", "Wenqi Wei", "Daizong Liu", "Pan Zhou"], "title": "An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs", "comment": null, "summary": "With the remarkable success of Vision-Language Models (VLMs) on multimodal tasks, concerns regarding their deployment efficiency have become increasingly prominent. In particular, the number of tokens consumed during the generation process has emerged as a key evaluation metric.Prior studies have shown that specific inputs can induce VLMs to generate lengthy outputs with low information density, which significantly increases energy consumption, latency, and token costs. However, existing methods simply delay the occurrence of the EOS token to implicitly prolong output, and fail to directly maximize the output token length as an explicit optimization objective, lacking stability and controllability.To address these limitations, this paper proposes a novel verbose-text induction attack (VTIA) to inject imperceptible adversarial perturbations into benign images via a two-stage framework, which identifies the most malicious prompt embeddings for optimizing and maximizing the output token of the perturbed images.Specifically, we first perform adversarial prompt search, employing reinforcement learning strategies to automatically identify adversarial prompts capable of inducing the LLM component within VLMs to produce verbose outputs. We then conduct vision-aligned perturbation optimization to craft adversarial examples on input images, maximizing the similarity between the perturbed image's visual embeddings and those of the adversarial prompt, thereby constructing malicious images that trigger verbose text generation. Comprehensive experiments on four popular VLMs demonstrate that our method achieves significant advantages in terms of effectiveness, efficiency, and generalization capability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5197\u957f\u6587\u672c\u8bf1\u5bfc\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6846\u67b6\u5411\u826f\u6027\u56fe\u50cf\u6ce8\u5165\u96be\u4ee5\u5bdf\u89c9\u7684\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u663e\u8457\u589e\u52a0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\u4ee4\u724c\u6570\u91cf\uff0c\u4ece\u800c\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u7684\u90e8\u7f72\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u7684\u663e\u8457\u6210\u529f\uff0c\u5176\u90e8\u7f72\u6548\u7387\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u7279\u522b\u662f\u751f\u6210\u8fc7\u7a0b\u4e2d\u6d88\u8017\u7684\u4ee4\u724c\u6570\u91cf\u5df2\u6210\u4e3a\u5173\u952e\u8bc4\u4f30\u6307\u6807\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u901a\u8fc7\u5ef6\u8fdfEOS\u4ee4\u724c\u7684\u51fa\u73b0\u6765\u9690\u5f0f\u5ef6\u957f\u8f93\u51fa\uff0c\u7f3a\u4e4f\u76f4\u63a5\u6700\u5927\u5316\u8f93\u51fa\u4ee4\u724c\u957f\u5ea6\u7684\u663e\u5f0f\u4f18\u5316\u76ee\u6807\uff0c\u5bfc\u81f4\u7a33\u5b9a\u6027\u548c\u53ef\u63a7\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u9996\u5148\u8fdb\u884c\u5bf9\u6297\u6027\u63d0\u793a\u641c\u7d22\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u81ea\u52a8\u8bc6\u522b\u80fd\u591f\u8bf1\u5bfcVLM\u4e2dLLM\u7ec4\u4ef6\u4ea7\u751f\u5197\u957f\u8f93\u51fa\u7684\u5bf9\u6297\u6027\u63d0\u793a\uff1b\u7136\u540e\u8fdb\u884c\u89c6\u89c9\u5bf9\u9f50\u6270\u52a8\u4f18\u5316\uff0c\u5728\u8f93\u5165\u56fe\u50cf\u4e0a\u6784\u5efa\u5bf9\u6297\u6837\u672c\uff0c\u6700\u5927\u5316\u6270\u52a8\u56fe\u50cf\u89c6\u89c9\u5d4c\u5165\u4e0e\u5bf9\u6297\u6027\u63d0\u793a\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u6784\u5efa\u89e6\u53d1\u5197\u957f\u6587\u672c\u751f\u6210\u7684\u6076\u610f\u56fe\u50cf\u3002", "result": "\u5728\u56db\u4e2a\u6d41\u884c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u663e\u8457\u589e\u52a0\u6a21\u578b\u7684\u8f93\u51fa\u4ee4\u724c\u6570\u91cf\uff0c\u9a8c\u8bc1\u4e86\u653b\u51fb\u65b9\u6cd5\u7684\u5b9e\u9645\u5a01\u80c1\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u6297\u6027\u653b\u51fb\u65f6\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u6027\u548c\u90e8\u7f72\u6548\u7387\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u5728\u6a21\u578b\u90e8\u7f72\u524d\u8fdb\u884c\u5145\u5206\u5b89\u5168\u6d4b\u8bd5\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u9632\u5fa1\u673a\u5236\u7684\u8bbe\u8ba1\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.16417", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16417", "abs": "https://arxiv.org/abs/2511.16417", "authors": ["Yan Chen", "Yu Zou", "Jialei Zeng", "Haoran You", "Xiaorui Zhou", "Aixi Zhong"], "title": "Pharos-ESG: A Framework for Multimodal Parsing, Contextual Narration, and Hierarchical Labeling of ESG Report", "comment": "Accepted to AAAI 26:main technical track Oral", "summary": "Environmental, Social, and Governance (ESG) principles are reshaping the foundations of global financial gover- nance, transforming capital allocation architectures, regu- latory frameworks, and systemic risk coordination mecha- nisms. However, as the core medium for assessing corpo- rate ESG performance, the ESG reports present significant challenges for large-scale understanding, due to chaotic read- ing order from slide-like irregular layouts and implicit hier- archies arising from lengthy, weakly structured content. To address these challenges, we propose Pharos-ESG, a uni- fied framework that transforms ESG reports into structured representations through multimodal parsing, contextual nar- ration, and hierarchical labeling. It integrates a reading-order modeling module based on layout flow, hierarchy-aware seg- mentation guided by table-of-contents anchors, and a multi- modal aggregation pipeline that contextually transforms vi- sual elements into coherent natural language. The framework further enriches its outputs with ESG, GRI, and sentiment labels, yielding annotations aligned with the analytical de- mands of financial research. Extensive experiments on anno- tated benchmarks demonstrate that Pharos-ESG consistently outperforms both dedicated document parsing systems and general-purpose multimodal models. In addition, we release Aurora-ESG, the first large-scale public dataset of ESG re- ports, spanning Mainland China, Hong Kong, and U.S. mar- kets, featuring unified structured representations of multi- modal content, enriched with fine-grained layout and seman- tic annotations to better support ESG integration in financial governance and decision-making.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPharos-ESG\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u89e3\u6790\u3001\u4e0a\u4e0b\u6587\u53d9\u8ff0\u548c\u5c42\u6b21\u5316\u6807\u6ce8\u5c06ESG\u62a5\u544a\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u8868\u793a\uff0c\u5e76\u53d1\u5e03\u9996\u4e2a\u5927\u89c4\u6a21ESG\u62a5\u544a\u6570\u636e\u96c6Aurora-ESG\uff0c\u663e\u8457\u63d0\u5347\u4e86ESG\u6587\u6863\u5206\u6790\u7684\u6027\u80fd\u3002", "motivation": "ESG\u62a5\u544a\u4f5c\u4e3a\u8bc4\u4f30\u4f01\u4e1aESG\u8868\u73b0\u7684\u6838\u5fc3\u5a92\u4ecb\uff0c\u7531\u4e8e\u7c7b\u4f3c\u5e7b\u706f\u7247\u7684\u975e\u89c4\u5219\u5e03\u5c40\u5bfc\u81f4\u7684\u6df7\u4e71\u9605\u8bfb\u987a\u5e8f\u4ee5\u53ca\u5197\u957f\u3001\u5f31\u7ed3\u6784\u5316\u5185\u5bb9\u4ea7\u751f\u7684\u9690\u542b\u5c42\u6b21\u7ed3\u6784\uff0c\u7ed9\u5927\u89c4\u6a21\u7406\u89e3\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u57fa\u4e8e\u5e03\u5c40\u6d41\u7684\u9605\u8bfb\u987a\u5e8f\u5efa\u6a21\u6a21\u5757\u3001\u7531\u76ee\u5f55\u951a\u70b9\u5f15\u5bfc\u7684\u5c42\u6b21\u611f\u77e5\u5206\u5272\u6a21\u5757\uff0c\u4ee5\u53ca\u5c06\u89c6\u89c9\u5143\u7d20\u4e0a\u4e0b\u6587\u8f6c\u5316\u4e3a\u8fde\u8d2f\u81ea\u7136\u8bed\u8a00\u7684\u591a\u6a21\u6001\u805a\u5408\u6d41\u6c34\u7ebf\uff0c\u8fdb\u4e00\u6b65\u901a\u8fc7ESG\u3001GRI\u548c\u60c5\u611f\u6807\u7b7e\u4e30\u5bcc\u8f93\u51fa\u3002", "result": "\u5728\u6807\u6ce8\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPharos-ESG\u59cb\u7ec8\u4f18\u4e8e\u4e13\u7528\u6587\u6863\u89e3\u6790\u7cfb\u7edf\u548c\u901a\u7528\u591a\u6a21\u6001\u6a21\u578b\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u9996\u4e2a\u8986\u76d6\u4e2d\u56fd\u5927\u9646\u3001\u9999\u6e2f\u548c\u7f8e\u56fd\u5e02\u573a\u7684\u5927\u89c4\u6a21\u516c\u5f00ESG\u62a5\u544a\u6570\u636e\u96c6Aurora-ESG\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91d1\u878d\u6cbb\u7406\u548c\u51b3\u7b56\u4e2d\u7684ESG\u6574\u5408\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u652f\u6301\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u7ed3\u6784\u5316\u8868\u793a\u548c\u7ec6\u7c92\u5ea6\u5e03\u5c40\u8bed\u4e49\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86ESG\u6587\u6863\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.16166", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16166", "abs": "https://arxiv.org/abs/2511.16166", "authors": ["Zeting Liu", "Zida Yang", "Zeyu Zhang", "Hao Tang"], "title": "EvoVLA: Self-Evolving Vision-Language-Action Model", "comment": null, "summary": "Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.", "AI": {"tldr": "EvoVLA\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u9636\u6bb5\u5bf9\u9f50\u5956\u52b1\u3001\u57fa\u4e8e\u59ff\u6001\u7684\u5bf9\u8c61\u63a2\u7d22\u548c\u957f\u89c6\u91ce\u8bb0\u5fc6\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u591a\u9636\u6bb5\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u9636\u6bb5\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u957f\u89c6\u91ce\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b58\u5728\u9636\u6bb5\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u667a\u80fd\u4f53\u5229\u7528\u7c97\u7565\u8bc4\u4f30\u4fe1\u53f7\u5728\u591a\u6b65\u4efb\u52a1\u4e2d\u8d70\u6377\u5f84\uff0c\u62a5\u544a\u9ad8\u8fdb\u5ea6\u4f46\u672a\u771f\u6b63\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "EvoVLA\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\u7ec4\u4ef6\uff1a\u9636\u6bb5\u5bf9\u9f50\u5956\u52b1\u4f7f\u7528\u4e09\u91cd\u5bf9\u6bd4\u5b66\u4e60\u548cGemini\u751f\u6210\u7684\u56f0\u96be\u8d1f\u6837\u672c\u6765\u9632\u6b62\u89c6\u89c9\u6377\u5f84\uff1b\u57fa\u4e8e\u59ff\u6001\u7684\u5bf9\u8c61\u63a2\u7d22\u5c06\u597d\u5947\u5fc3\u5efa\u7acb\u5728\u76f8\u5bf9\u5bf9\u8c61-\u5939\u722a\u59ff\u6001\u800c\u975e\u539f\u59cb\u50cf\u7d20\u4e0a\uff1b\u957f\u89c6\u91ce\u8bb0\u5fc6\u901a\u8fc7\u9009\u62e9\u6027\u4e0a\u4e0b\u6587\u4fdd\u7559\u548c\u95e8\u63a7\u878d\u5408\u6765\u7a33\u5b9a\u6269\u5c55\u8f68\u8ff9\u4e2d\u7684\u5185\u5728\u5851\u9020\u8fc7\u7a0b\u3002", "result": "\u5728Discoverse-L\u957f\u89c6\u91ce\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEvoVLA\u5c06\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534710.2\u4e2a\u767e\u5206\u70b9\u81f369.2%\uff0c\u6837\u672c\u6548\u7387\u63d0\u9ad81.5\u500d\uff0c\u5e76\u5c06\u9636\u6bb5\u5e7b\u89c9\u4ece38.5%\u964d\u4f4e\u81f314.8%\uff1b\u5728\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u8fbe\u523054.6%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u6bd4OpenVLA-OFT\u9ad8\u51fa11\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "EvoVLA\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u7684\u9636\u6bb5\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u6709\u6548\u8fc1\u79fb\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u957f\u89c6\u91ce\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86VLA\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.16170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16170", "abs": "https://arxiv.org/abs/2511.16170", "authors": ["Jiahao Li", "Yang Lu", "Yachao Zhang", "Yong Xie", "Fangyong Wang", "Yuan Xie", "Yanyun Qu"], "title": "Target Refocusing via Attention Redistribution for Open-Vocabulary Semantic Segmentation: An Explainability Perspective", "comment": "Accepted by AAAI 2026", "summary": "Open-vocabulary semantic segmentation (OVSS) employs pixel-level vision-language alignment to associate category-related prompts with corresponding pixels. A key challenge is enhancing the multimodal dense prediction capability, specifically this pixel-level multimodal alignment. Although existing methods achieve promising results by leveraging CLIP's vision-language alignment, they rarely investigate the performance boundaries of CLIP for dense prediction from an interpretability mechanisms perspective. In this work, we systematically investigate CLIP's internal mechanisms and identify a critical phenomenon: analogous to human distraction, CLIP diverts significant attention resources from target regions to irrelevant tokens. Our analysis reveals that these tokens arise from dimension-specific over-activation; filtering them enhances CLIP's dense prediction performance. Consequently, we propose ReFocusing CLIP (RF-CLIP), a training-free approach that emulates human distraction-refocusing behavior to redirect attention from distraction tokens back to target regions, thereby refining CLIP's multimodal alignment granularity. Our method achieves SOTA performance on eight benchmarks while maintaining high inference efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RF-CLIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u6ce8\u610f\u529b\u5206\u6563-\u91cd\u65b0\u805a\u7126\u884c\u4e3a\u6765\u63d0\u5347CLIP\u5728\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u5bc6\u96c6\u9884\u6d4b\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u867d\u7136\u5229\u7528CLIP\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u53d6\u5f97\u4e86\u4e0d\u9519\u6548\u679c\uff0c\u4f46\u5f88\u5c11\u4ece\u53ef\u89e3\u91ca\u6027\u673a\u5236\u89d2\u5ea6\u7814\u7a76CLIP\u5728\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8fb9\u754c\u3002\u7814\u7a76\u53d1\u73b0CLIP\u5b58\u5728\u7c7b\u4f3c\u4eba\u7c7b\u6ce8\u610f\u529b\u5206\u6563\u7684\u73b0\u8c61\uff0c\u5c06\u5927\u91cf\u6ce8\u610f\u529b\u8d44\u6e90\u4ece\u76ee\u6807\u533a\u57df\u8f6c\u79fb\u5230\u65e0\u5173token\u4e0a\u3002", "method": "\u63d0\u51faReFocusing CLIP\uff08RF-CLIP\uff09\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u8fc7\u6ee4\u7ef4\u5ea6\u7279\u5f02\u6027\u8fc7\u5ea6\u6fc0\u6d3b\u4ea7\u751f\u7684\u5e72\u6270token\uff0c\u91cd\u65b0\u5c06\u6ce8\u610f\u529b\u5f15\u5bfc\u56de\u76ee\u6807\u533a\u57df\uff0c\u4ece\u800c\u63d0\u5347CLIP\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u7c92\u5ea6\u3002\u8be5\u65b9\u6cd5\u6a21\u62df\u4eba\u7c7b\u6ce8\u610f\u529b\u5206\u6563-\u91cd\u65b0\u805a\u7126\u884c\u4e3a\u673a\u5236\u3002", "result": "RF-CLIP\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u63a8\u7406\u6548\u7387\u3002\u5206\u6790\u8868\u660e\u8fc7\u6ee4\u5e72\u6270token\u80fd\u663e\u8457\u63d0\u5347CLIP\u7684\u5bc6\u96c6\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86CLIP\u5185\u90e8\u673a\u5236\u4e2d\u5b58\u5728\u7684\u6ce8\u610f\u529b\u5206\u6563\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u6709\u6548\u7684\u6ce8\u610f\u529b\u91cd\u805a\u7126\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u4e5f\u4e3a\u7406\u89e3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u4ef7\u503c\u548c\u5b9e\u9645\u5e94\u7528\u610f\u4e49\u3002"}}
{"id": "2511.16548", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16548", "abs": "https://arxiv.org/abs/2511.16548", "authors": ["Guanchen Wu", "Yuzhang Xie", "Huanwei Wu", "Zhe He", "Hui Shao", "Xiao Hu", "Carl Yang"], "title": "Utilizing Large Language Models for Zero-Shot Medical Ontology Extension from Clinical Notes", "comment": "BIBM 2025 (WS#44: Biological ontologies and knowledge bases (BiOK) in the LLM era)", "summary": "Integrating novel medical concepts and relationships into existing ontologies can significantly enhance their coverage and utility for both biomedical research and clinical applications. Clinical notes, as unstructured documents rich with detailed patient observations, offer valuable context-specific insights and represent a promising yet underutilized source for ontology extension. Despite this potential, directly leveraging clinical notes for ontology extension remains largely unexplored. To address this gap, we propose CLOZE, a novel framework that uses large language models (LLMs) to automatically extract medical entities from clinical notes and integrate them into hierarchical medical ontologies. By capitalizing on the strong language understanding and extensive biomedical knowledge of pre-trained LLMs, CLOZE effectively identifies disease-related concepts and captures complex hierarchical relationships. The zero-shot framework requires no additional training or labeled data, making it a cost-efficient solution. Furthermore, CLOZE ensures patient privacy through automated removal of protected health information (PHI). Experimental results demonstrate that CLOZE provides an accurate, scalable, and privacy-preserving ontology extension framework, with strong potential to support a wide range of downstream applications in biomedical research and clinical informatics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CLOZE\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u81ea\u52a8\u63d0\u53d6\u533b\u5b66\u672f\u8bed\u5e76\u6574\u5408\u5230\u5c42\u6b21\u5316\u533b\u5b66\u672c\u4f53\u4e2d\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u672c\u4f53\u6269\u5c55\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u96f6\u6837\u672c\u65b9\u5f0f\u6709\u6548\u8bc6\u522b\u75be\u75c5\u76f8\u5173\u6982\u5ff5\u5e76\u6355\u83b7\u590d\u6742\u5c42\u6b21\u5173\u7cfb\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u7814\u7a76\u548c\u4e34\u5e8a\u4fe1\u606f\u5b66\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u672c\u4f53\u5728\u8986\u76d6\u8303\u56f4\u548c\u5b9e\u7528\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800c\u4e34\u5e8a\u7b14\u8bb0\u4f5c\u4e3a\u5bcc\u542b\u8be6\u7ec6\u60a3\u8005\u89c2\u5bdf\u4fe1\u606f\u7684\u975e\u7ed3\u6784\u5316\u6587\u6863\uff0c\u867d\u5177\u6709\u4e30\u5bcc\u7684\u60c5\u5883\u7279\u5b9a\u6d1e\u5bdf\u529b\uff0c\u4f46\u5728\u672c\u4f53\u6269\u5c55\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5f00\u53d1\u3002\u76ee\u524d\u76f4\u63a5\u5229\u7528\u4e34\u5e8a\u7b14\u8bb0\u8fdb\u884c\u672c\u4f53\u6269\u5c55\u7684\u7814\u7a76\u4ecd\u5904\u4e8e\u63a2\u7d22\u4e0d\u8db3\u7684\u72b6\u6001\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u63d0\u53d6\u533b\u5b66\u5b9e\u4f53\u5e76\u6574\u5408\u5230\u5c42\u6b21\u5316\u533b\u5b66\u672c\u4f53\u4e2d\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "CLOZE\u6846\u67b6\u57fa\u4e8e\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6784\u5efa\uff0c\u5229\u7528\u5176\u5f3a\u5927\u7684\u8bed\u8a00\u7406\u89e3\u548c\u5e7f\u6cdb\u7684\u751f\u7269\u533b\u5b66\u77e5\u8bc6\uff0c\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u81ea\u52a8\u63d0\u53d6\u533b\u5b66\u5b9e\u4f53\u5e76\u6574\u5408\u5230\u5c42\u6b21\u5316\u533b\u5b66\u672c\u4f53\u4e2d\u3002\u8be5\u6846\u67b6\u91c7\u7528\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6807\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u79fb\u9664\u53d7\u4fdd\u62a4\u5065\u5eb7\u4fe1\u606f\u6765\u786e\u4fdd\u60a3\u8005\u9690\u79c1\uff0c\u5b9e\u73b0\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u672c\u4f53\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eCLOZE\u6846\u67b6\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u75be\u75c5\u76f8\u5173\u6982\u5ff5\u5e76\u6355\u83b7\u590d\u6742\u7684\u5c42\u6b21\u5173\u7cfb\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cbe\u786e\u3001\u53ef\u6269\u5c55\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u672c\u4f53\u6269\u5c55\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5728\u751f\u7269\u533b\u5b66\u7814\u7a76\u548c\u4e34\u5e8a\u4fe1\u606f\u5b66\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u5e94\u7528\u573a\u666f\u7684\u5b9e\u73b0\u3002", "conclusion": "CLOZE\u6846\u67b6\u8bc1\u660e\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u81ea\u52a8\u6269\u5c55\u533b\u5b66\u672c\u4f53\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u96f6\u6837\u672c\u672c\u4f53\u6269\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002\u8be5\u7814\u7a76\u4e3a\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u8868\u793a\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6491\uff0c\u5177\u6709\u63a8\u52a8\u533b\u5b66\u4fe1\u606f\u5b66\u9886\u57df\u8fdb\u6b65\u7684\u6df1\u8fdc\u610f\u4e49\u3002"}}
{"id": "2511.16175", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16175", "abs": "https://arxiv.org/abs/2511.16175", "authors": ["Yi Yang", "Xueqi Li", "Yiyang Chen", "Jin Song", "Yihan Wang", "Zipeng Xiao", "Jiadi Su", "You Qiaoben", "Pengfei Liu", "Zhijie Deng"], "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $\u03c0_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMantis\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u9884\u6d4b\u4e0e\u4e3b\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u5143\u67e5\u8be2\u548c\u6269\u6563Transformer\u5934\uff0c\u5728\u4fdd\u6301\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u7684\u540c\u65f6\u6709\u6548\u5b66\u4e60\u89c6\u89c9\u8f68\u8ff9\u4e2d\u7684\u6f5c\u5728\u52a8\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u76f4\u63a5\u9884\u6d4b\u9ad8\u7ef4\u89c6\u89c9\u72b6\u6001\u4f1a\u5206\u6563\u6a21\u578b\u5bb9\u91cf\u5e76\u5e26\u6765\u9ad8\u6602\u8bad\u7ec3\u6210\u672c\uff0c\u800c\u538b\u7f29\u89c6\u89c9\u72b6\u6001\u4e3a\u7d27\u51d1\u76d1\u7763\u4fe1\u53f7\u5219\u4f1a\u4ea7\u751f\u4fe1\u606f\u74f6\u9888\uff0c\u540c\u65f6\u8fd9\u4e9b\u65b9\u6cd5\u5f80\u5f80\u56e0\u5ffd\u89c6\u8bed\u8a00\u76d1\u7763\u800c\u7f3a\u4e4f\u8db3\u591f\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "Mantis\u6846\u67b6\u91c7\u7528\u89e3\u8026\u89c6\u89c9\u9884\u6d4b\u65b9\u6cd5\uff0c\u5c06\u89c6\u89c9\u9884\u6d4b\u4ece\u4e3b\u5e72\u7f51\u7edc\u4e2d\u5206\u79bb\uff0c\u7ed3\u5408\u5143\u67e5\u8be2\u548c\u6269\u6563Transformer\u5934\uff0c\u901a\u8fc7\u6b8b\u5dee\u8fde\u63a5\u63d0\u4f9b\u5f53\u524d\u89c6\u89c9\u72b6\u6001\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u4e0b\u4e00\u72b6\u6001\u9884\u6d4b\u76ee\u6807\u4f7f\u5143\u67e5\u8be2\u81ea\u52a8\u6355\u6349\u63cf\u8ff0\u89c6\u89c9\u8f68\u8ff9\u7684\u6f5c\u5728\u52a8\u4f5c\uff0c\u4ece\u800c\u4fc3\u8fdb\u663e\u5f0f\u52a8\u4f5c\u7684\u5b66\u4e60\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684Mantis\u8fbe\u523096.7%\u7684\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u5f3a\u5927\u57fa\u7ebf\u5e76\u5c55\u73b0\u51fa\u9ad8\u6536\u655b\u901f\u5ea6\uff1b\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u663e\u793aMantis\u5728\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3001\u5bf9\u672a\u89c1\u6307\u4ee4\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5747\u4f18\u4e8e\u9886\u5148\u7684\u5f00\u6e90VLA\u6a21\u578b\u03c00.5\u3002", "conclusion": "\u89e3\u8026\u89c6\u89c9\u9884\u6d4b\u51cf\u8f7b\u4e86VLA\u4e3b\u5e72\u7f51\u7edc\u7684\u8d1f\u62c5\uff0c\u4f7f\u5176\u80fd\u591f\u901a\u8fc7\u8bed\u8a00\u76d1\u7763\u4fdd\u6301\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff1b\u8be5\u65b9\u6cd5\u5728\u4eba\u7c7b\u64cd\u4f5c\u89c6\u9891\u3001\u673a\u5668\u4eba\u6f14\u793a\u548c\u56fe\u50cf-\u6587\u672c\u5bf9\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u540e\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.16600", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16600", "abs": "https://arxiv.org/abs/2511.16600", "authors": ["Tianlong Zhang", "Hongwei Xue", "Shilin Yan", "Di Wu", "Chen Xu", "Yunyun Yang"], "title": "You Only Forward Once: An Efficient Compositional Judging Paradigm", "comment": null, "summary": "Multimodal large language models (MLLMs) show strong potential as judges. However, existing approaches face a fundamental trade-off: adapting MLLMs to output a single score misaligns with the generative nature of MLLMs and limits fine-grained requirement understanding, whereas autoregressively generating judging analyses is prohibitively slow in high-throughput settings. Observing that judgment reduces to verifying whether inputs satisfy a set of structured requirements, we propose YOFO, a template-conditioned method that judges all requirements in a single forward pass. Built on an autoregressive model, YOFO accepts a structured requirement template and, in one inference step, produces a binary yes/no decision for each requirement by reading the logits of the final token associated with that requirement. This design yields orders-of-magnitude speedups while preserving interpretability. Extensive experiments show that YOFO not only achieves state-of-the-art results on standard recommendation datasets, but also supports dependency-aware analysis-where subsequent judgments are conditioned on previous ones-and further benefits from post-hoc CoT.", "AI": {"tldr": "YOFO\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u677f\u6761\u4ef6\u5316\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u5224\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u5e76\u884c\u9a8c\u8bc1\u7ed3\u6784\u5316\u9700\u6c42\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u8bc4\u5224\u901f\u5ea6\u4e0e\u7ec6\u7c92\u5ea6\u7406\u89e3\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u7684\u52a0\u901f\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709MLLM\u8bc4\u5224\u65b9\u6cd5\u9762\u4e34\u6839\u672c\u6027\u6743\u8861\uff1a\u5c06MLLM\u9002\u914d\u4e3a\u8f93\u51fa\u5355\u4e00\u5206\u6570\u4e0e\u751f\u6210\u5f0f\u672c\u8d28\u4e0d\u5339\u914d\u4e14\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u9700\u6c42\u7406\u89e3\uff0c\u800c\u81ea\u56de\u5f52\u751f\u6210\u8bc4\u5224\u5206\u6790\u5728\u9ad8\u541e\u5410\u91cf\u573a\u666f\u4e0b\u901f\u5ea6\u8fc7\u6162\u3002\u8be5\u7814\u7a76\u89c2\u5bdf\u5230\u8bc4\u5224\u53ef\u7b80\u5316\u4e3a\u9a8c\u8bc1\u8f93\u5165\u662f\u5426\u6ee1\u8db3\u4e00\u7ec4\u7ed3\u6784\u5316\u9700\u6c42\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6548\u7387\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "YOFO\u57fa\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u6784\u5efa\uff0c\u91c7\u7528\u6a21\u677f\u6761\u4ef6\u5316\u65b9\u6cd5\uff0c\u5728\u5355\u6b21\u63a8\u7406\u6b65\u9aa4\u4e2d\u901a\u8fc7\u8bfb\u53d6\u4e0e\u6bcf\u4e2a\u9700\u6c42\u5173\u8054\u7684\u6700\u7ec8token\u7684logits\uff0c\u4e3a\u6240\u6709\u7ed3\u6784\u5316\u9700\u6c42\u5e76\u884c\u751f\u6210\u4e8c\u5143\u662f/\u5426\u51b3\u7b56\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u4f9d\u8d56\u611f\u77e5\u5206\u6790\uff0c\u5176\u4e2d\u540e\u7eed\u5224\u65ad\u57fa\u4e8e\u5148\u524d\u5224\u65ad\uff0c\u5e76\u80fd\u4ece\u540e\u9a8c\u601d\u7ef4\u94fe\u4e2d\u83b7\u76ca\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eYOFO\u5728\u6807\u51c6\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u83b7\u5f97\u4e86\u6570\u91cf\u7ea7\u7684\u52a0\u901f\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u5224\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u8fbe\u5230\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u6307\u6807\uff0c\u8fd8\u9a8c\u8bc1\u4e86\u4f9d\u8d56\u611f\u77e5\u5206\u6790\u548c\u540e\u9a8c\u601d\u7ef4\u94fe\u7684\u989d\u5916\u4f18\u52bf\u3002", "conclusion": "YOFO\u8bc1\u660e\u4e86\u5c06\u8bc4\u5224\u4efb\u52a1\u91cd\u65b0\u6784\u5efa\u4e3a\u7ed3\u6784\u5316\u9700\u6c42\u9a8c\u8bc1\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u541e\u5410\u91cfMLLM\u8bc4\u5224\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u7684\u6210\u529f\u8868\u660e\u5728\u4fdd\u6301\u751f\u6210\u6a21\u578b\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63a8\u7406\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72MLLM\u8bc4\u5224\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.16184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16184", "abs": "https://arxiv.org/abs/2511.16184", "authors": ["Nianchang Huang", "Yi Xu", "Ruida Xi", "Ruida Xi", "Qiang Zhang"], "title": "Domain-Shared Learning and Gradual Alignment for Unsupervised Domain Adaptation Visible-Infrared Person Re-Identification", "comment": null, "summary": "Recently, Visible-Infrared person Re-Identification (VI-ReID) has achieved remarkable performance on public datasets. However, due to the discrepancies between public datasets and real-world data, most existing VI-ReID algorithms struggle in real-life applications. To address this, we take the initiative to investigate Unsupervised Domain Adaptation Visible-Infrared person Re-Identification (UDA-VI-ReID), aiming to transfer the knowledge learned from the public data to real-world data without compromising accuracy and requiring the annotation of new samples. Specifically, we first analyze two basic challenges in UDA-VI-ReID, i.e., inter-domain modality discrepancies and intra-domain modality discrepancies. Then, we design a novel two-stage model, i.e., Domain-Shared Learning and Gradual Alignment (DSLGA), to handle these discrepancies. In the first pre-training stage, DSLGA introduces a Domain-Shared Learning Strategy (DSLS) to mitigate ineffective pre-training caused by inter-domain modality discrepancies via exploiting shared information between the source and target domains. While, in the second fine-tuning stage, DSLGA designs a Gradual Alignment Strategy (GAS) to handle the cross-modality alignment challenges between visible and infrared data caused by the large intra-domain modality discrepancies through a cluster-to-holistic alignment way. Finally, a new UDA-VI-ReID testing method i.e., CMDA-XD, is constructed for training and testing different UDA-VI-ReID models. A large amount of experiments demonstrate that our method significantly outperforms existing domain adaptation methods for VI-ReID and even some supervised methods under various settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6a21\u578bDSLGA\u6765\u89e3\u51b3\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u95ee\u9898\uff0c\u901a\u8fc7\u57df\u5171\u4eab\u5b66\u4e60\u7b56\u7565\u548c\u6e10\u8fdb\u5bf9\u9f50\u7b56\u7565\u6709\u6548\u5904\u7406\u8de8\u57df\u548c\u8de8\u6a21\u6001\u5dee\u5f02\uff0c\u5728\u591a\u4e2a\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\u7b97\u6cd5\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7531\u4e8e\u516c\u5f00\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u53ef\u89c1\u5149-\u7ea2\u5916\u884c\u4eba\u91cd\u8bc6\u522b\uff0c\u5c06\u516c\u5f00\u6570\u636e\u5b66\u5230\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u7cbe\u5ea6\u4e14\u65e0\u9700\u65b0\u6837\u672c\u6807\u6ce8\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6a21\u578bDSLGA\uff1a\u7b2c\u4e00\u9636\u6bb5\u9884\u8bad\u7ec3\u5f15\u5165\u57df\u5171\u4eab\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u6316\u6398\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u5171\u4eab\u4fe1\u606f\u6765\u7f13\u89e3\u8de8\u57df\u6a21\u6001\u5dee\u5f02\u5bfc\u81f4\u7684\u65e0\u6548\u9884\u8bad\u7ec3\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5fae\u8c03\u8bbe\u8ba1\u6e10\u8fdb\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7\u4ece\u805a\u7c7b\u5230\u6574\u4f53\u7684\u5bf9\u9f50\u65b9\u5f0f\u5904\u7406\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u6570\u636e\u4e4b\u95f4\u7531\u5927\u6a21\u6001\u5dee\u5f02\u5e26\u6765\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u6311\u6218\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684VI-ReID\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u751a\u81f3\u4f18\u4e8e\u67d0\u4e9b\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002\u540c\u65f6\u6784\u5efa\u4e86\u65b0\u7684UDA-VI-ReID\u6d4b\u8bd5\u65b9\u6cd5CMDA-XD\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4e0d\u540c\u7684UDA-VI-ReID\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3VI-ReID\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u57df\u9002\u5e94\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7b56\u7565\u5206\u522b\u5904\u7406\u8de8\u57df\u548c\u8de8\u6a21\u6001\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fdVI-ReID\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2511.16602", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16602", "abs": "https://arxiv.org/abs/2511.16602", "authors": ["Yi Zhang", "Che Liu", "Xiancong Ren", "Hanchu Ni", "Yingji Zhang", "Shuai Zhang", "Zeyuan Ding", "Jiayu Hu", "Haozhe Shan", "Junbo Qi", "Yan Bai", "Dengjie Li", "Jiachen Luo", "Yidong Wang", "Yong Dai", "Zenglin Xu", "Bin Shen", "Qifan Wang", "Jian Tang", "Xiaozhu Ju"], "title": "Bridging VLMs and Embodied Intelligence with Deliberate Practice Policy Optimization", "comment": null, "summary": "Developing a universal and versatile embodied intelligence system presents two primary challenges: the critical embodied data bottleneck, where real-world data is scarce and expensive, and the algorithmic inefficiency of existing methods, which are resource-prohibitive. To address these limitations, we introduce Deliberate Practice Policy Optimization (DPPO), a metacognitive ``Metaloop'' training framework that dynamically alternates between supervised fine-tuning (competence expansion) and reinforcement learning (skill refinement). This enables automatic weakness identification and targeted resource allocation, specifically designed to maximize learning efficiency from sparse, finite data. Theoretically, DPPO can be formalised as a unified preference-learning framework. Empirically, training a vision-language embodied model with DPPO, referred to as Pelican-VL 1.0, yields a 20.3% performance improvement over the base model and surpasses open-source models at the 100B-parameter scale by 10.6%. We are open-sourcing both the models and code, providing the first systematic framework that alleviates the data and resource bottleneck and enables the community to build versatile embodied agents efficiently.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Deliberate Practice Policy Optimization (DPPO)\u6846\u67b6\uff0c\u4e00\u79cd\u5143\u8ba4\u77e5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4ea4\u66ff\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u89e3\u51b3\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u6570\u636e\u74f6\u9888\u548c\u7b97\u6cd5\u6548\u7387\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728Pelican-VL 1.0\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e8620.3%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86100B\u53c2\u6570\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u901a\u7528\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u9762\u4e34\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u5173\u952e\u5177\u8eab\u6570\u636e\u74f6\u9888\uff08\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u7a00\u7f3a\u4e14\u6602\u8d35\uff09\u548c\u73b0\u6709\u65b9\u6cd5\u7684\u7b97\u6cd5\u6548\u7387\u4f4e\u4e0b\uff08\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\uff09\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86\u9ad8\u6548\u6784\u5efa\u591a\u529f\u80fd\u5177\u8eab\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u7684\u6838\u5fc3\u65b9\u6cd5\u662fDeliberate Practice Policy Optimization (DPPO)\uff0c\u8fd9\u662f\u4e00\u79cd\u5143\u8ba4\u77e5\u201cMetaloop\u201d\u8bad\u7ec3\u6846\u67b6\uff0c\u52a8\u6001\u4ea4\u66ff\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08\u80fd\u529b\u6269\u5c55\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08\u6280\u80fd\u7cbe\u70bc\uff09\u3002\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u5f31\u70b9\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u8d44\u6e90\u5206\u914d\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u4ece\u7a00\u758f\u6709\u9650\u6570\u636e\u4e2d\u6700\u5927\u5316\u5b66\u4e60\u6548\u7387\u3002\u7406\u8bba\u4e0a\uff0cDPPO\u53ef\u5f62\u5f0f\u5316\u4e3a\u7edf\u4e00\u7684\u504f\u597d\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528DPPO\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u5177\u8eab\u6a21\u578bPelican-VL 1.0\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u4e8620.3%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728100B\u53c2\u6570\u89c4\u6a21\u4e0a\u8d85\u8d8a\u4e86\u5f00\u6e90\u6a21\u578b10.6%\u3002\u8be5\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86\u6570\u636e\u548c\u8d44\u6e90\u74f6\u9888\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u6846\u67b6\u6765\u7f13\u89e3\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u6570\u636e\u4e0e\u8d44\u6e90\u74f6\u9888\uff0c\u4f7f\u793e\u533a\u80fd\u591f\u9ad8\u6548\u6784\u5efa\u591a\u529f\u80fd\u5177\u8eab\u667a\u80fd\u4f53\u3002\u7814\u7a76\u56e2\u961f\u5f00\u6e90\u4e86\u6a21\u578b\u548c\u4ee3\u7801\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.16203", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16203", "abs": "https://arxiv.org/abs/2511.16203", "authors": ["Yuping Yan", "Yuhan Xie", "Yinxin Zhang", "Lingjuan Lyu", "Yaochu Jin"], "title": "When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VLA-Fool\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u5177\u8eab\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u9ed1\u767d\u76d2\u8bbe\u7f6e\u4e0b\u7684\u591a\u6a21\u6001\u5bf9\u6297\u9c81\u68d2\u6027\u7efc\u5408\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5373\u4f7f\u8f7b\u5fae\u7684\u591a\u6a21\u6001\u6270\u52a8\u4e5f\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u884c\u4e3a\u504f\u5dee\uff0c\u8bc1\u660e\u4e86\u5177\u8eab\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5177\u8eab\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8fd9\u4e9b\u7cfb\u7edf\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u5728\u73b0\u5b9e\u591a\u6a21\u6001\u548c\u9ed1\u76d2\u6761\u4ef6\u4e0b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6270\u52a8\uff0c\u5ffd\u7565\u4e86\u4ece\u6839\u672c\u4e0a\u5f71\u54cd\u5177\u8eab\u63a8\u7406\u548c\u51b3\u7b56\u7684\u8de8\u6a21\u6001\u9519\u4f4d\u95ee\u9898\u3002", "method": "VLA-Fool\u7edf\u4e00\u4e86\u4e09\u4e2a\u5c42\u6b21\u7684\u591a\u6a21\u6001\u5bf9\u6297\u653b\u51fb\uff1a\u57fa\u4e8e\u68af\u5ea6\u548c\u63d0\u793a\u7684\u6587\u672c\u6270\u52a8\u3001\u901a\u8fc7\u8865\u4e01\u548c\u566a\u58f0\u5931\u771f\u7684\u89c6\u89c9\u6270\u52a8\uff0c\u4ee5\u53ca\u6545\u610f\u7834\u574f\u611f\u77e5\u4e0e\u6307\u4ee4\u95f4\u8bed\u4e49\u5bf9\u5e94\u7684\u8de8\u6a21\u6001\u9519\u4f4d\u653b\u51fb\u3002\u7814\u7a76\u8fd8\u9996\u6b21\u5c06VLA\u611f\u77e5\u8bed\u4e49\u7a7a\u95f4\u878d\u5165\u8bed\u8a00\u63d0\u793a\uff0c\u5f00\u53d1\u4e86\u81ea\u52a8\u6784\u5efa\u7684\u8bed\u4e49\u5f15\u5bfc\u63d0\u793a\u6846\u67b6\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528\u5fae\u8c03\u7684OpenVLA\u6a21\u578b\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u8f7b\u5fae\u7684\u591a\u6a21\u6001\u6270\u52a8\u4e5f\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u884c\u4e3a\u504f\u5dee\uff0c\u8bc1\u660e\u4e86\u5177\u8eab\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5177\u8eab\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u591a\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u4e25\u91cd\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u8003\u8651\u8de8\u6a21\u6001\u9c81\u68d2\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u66f4\u5b89\u5168\u7684\u5177\u8eabAI\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2511.16227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16227", "abs": "https://arxiv.org/abs/2511.16227", "authors": ["Boyue Xu", "Ruichao Hou", "Tongwei Ren", "Dongming Zhou", "Gangshan Wu", "Jinde Cao"], "title": "SwiTrack: Tri-State Switch for Cross-Modal Object Tracking", "comment": null, "summary": "Cross-modal object tracking (CMOT) is an emerging task that maintains target consistency while the video stream switches between different modalities, with only one modality available in each frame, mostly focusing on RGB-Near Infrared (RGB-NIR) tracking. Existing methods typically connect parallel RGB and NIR branches to a shared backbone, which limits the comprehensive extraction of distinctive modality-specific features and fails to address the issue of object drift, especially in the presence of unreliable inputs. In this paper, we propose SwiTrack, a novel state-switching framework that redefines CMOT through the deployment of three specialized streams. Specifically, RGB frames are processed by the visual encoder, while NIR frames undergo refinement via a NIR gated adapter coupled with the visual encoder to progressively calibrate shared latent space features, thereby yielding more robust cross-modal representations. For invalid modalities, a consistency trajectory prediction module leverages spatio-temporal cues to estimate target movement, ensuring robust tracking and mitigating drift. Additionally, we incorporate dynamic template reconstruction to iteratively update template features and employ a similarity alignment loss to reinforce feature consistency. Experimental results on the latest benchmarks demonstrate that our tracker achieves state-of-the-art performance, boosting precision rate and success rate gains by 7.2\\% and 4.3\\%, respectively, while maintaining real-time tracking at 65 frames per second. Code and models are available at https://github.com/xuboyue1999/SwiTrack.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSwiTrack\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u72b6\u6001\u5207\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u90e8\u7f72\u4e09\u4e2a\u4e13\u7528\u6d41\u91cd\u65b0\u5b9a\u4e49\u8de8\u6a21\u6001\u76ee\u6807\u8ddf\u8e2a\uff0c\u5728RGB-NIR\u8ddf\u8e2a\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u8ddf\u8e2a\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u901a\u5e38\u5c06\u5e76\u884cRGB\u548cNIR\u5206\u652f\u8fde\u63a5\u5230\u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\uff0c\u8fd9\u9650\u5236\u4e86\u533a\u5206\u6027\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u7684\u5168\u9762\u63d0\u53d6\uff0c\u4e14\u65e0\u6cd5\u89e3\u51b3\u76ee\u6807\u6f02\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u4e0d\u53ef\u9760\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u3002", "method": "SwiTrack\u91c7\u7528\u4e09\u6d41\u67b6\u6784\uff1aRGB\u5e27\u7531\u89c6\u89c9\u7f16\u7801\u5668\u5904\u7406\uff0cNIR\u5e27\u901a\u8fc7NIR\u95e8\u63a7\u9002\u914d\u5668\u7ed3\u5408\u89c6\u89c9\u7f16\u7801\u5668\u8fdb\u884c\u7ec6\u5316\u4ee5\u6821\u51c6\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u7279\u5f81\uff0c\u5bf9\u4e8e\u65e0\u6548\u6a21\u6001\u5219\u4f7f\u7528\u4e00\u81f4\u6027\u8f68\u8ff9\u9884\u6d4b\u6a21\u5757\u5229\u7528\u65f6\u7a7a\u7ebf\u7d22\u4f30\u8ba1\u76ee\u6807\u8fd0\u52a8\uff0c\u540c\u65f6\u7ed3\u5408\u52a8\u6001\u6a21\u677f\u91cd\u5efa\u548c\u76f8\u4f3c\u6027\u5bf9\u9f50\u635f\u5931\u6765\u589e\u5f3a\u7279\u5f81\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6700\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u8ddf\u8e2a\u5668\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7cbe\u5ea6\u7387\u548c\u6210\u529f\u7387\u5206\u522b\u63d0\u5347\u4e867.2%\u548c4.3%\uff0c\u540c\u65f6\u4fdd\u630165\u5e27/\u79d2\u7684\u5b9e\u65f6\u8ddf\u8e2a\u901f\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u4e13\u95e8\u7684\u72b6\u6001\u5207\u6362\u6846\u67b6\u548c\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u63d0\u53d6\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u8de8\u6a21\u6001\u8ddf\u8e2a\u4e2d\u7684\u76ee\u6807\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u5904\u7406\u52a8\u6001\u6a21\u6001\u5207\u6362\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ddf\u8e2a\u7cbe\u5ea6\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.16378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16378", "abs": "https://arxiv.org/abs/2511.16378", "authors": ["Pan Yang", "Cheng Deng", "Jing Yang", "Han Zhao", "Yun Liu", "Yuling Chen", "Xiaoli Ruan", "Yanping Chen"], "title": "CAMS: Towards Compositional Zero-Shot Learning via Gated Cross-Attention and Multi-Space Disentanglement", "comment": null, "summary": "Compositional zero-shot learning (CZSL) aims to learn the concepts of attributes and objects in seen compositions and to recognize their unseen compositions. Most Contrastive Language-Image Pre-training (CLIP)-based CZSL methods focus on disentangling attributes and objects by leveraging the global semantic representation obtained from the image encoder. However, this representation has limited representational capacity and do not allow for complete disentanglement of the two. To this end, we propose CAMS, which aims to extract semantic features from visual features and perform semantic disentanglement in multidimensional spaces, thereby improving generalization over unseen attribute-object compositions. Specifically, CAMS designs a Gated Cross-Attention that captures fine-grained semantic features from the high-level image encoding blocks of CLIP through a set of latent units, while adaptively suppressing background and other irrelevant information. Subsequently, it conducts Multi-Space Disentanglement to achieve disentanglement of attribute and object semantics. Experiments on three popular benchmarks (MIT-States, UT-Zappos, and C-GQA) demonstrate that CAMS achieves state-of-the-art performance in both closed-world and open-world settings. The code is available at https://github.com/ybyangjing/CAMS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCAMS\u65b9\u6cd5\uff0c\u901a\u8fc7\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u591a\u7a7a\u95f4\u89e3\u8026\u673a\u5236\uff0c\u5728CLIP\u6846\u67b6\u4e2d\u5b9e\u73b0\u5c5e\u6027\u548c\u5bf9\u8c61\u8bed\u4e49\u7684\u7ec6\u7c92\u5ea6\u89e3\u8026\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u5728\u672a\u89c1\u7ec4\u5408\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCLIP\u7684\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u56fe\u50cf\u7f16\u7801\u5668\u83b7\u5f97\u7684\u5168\u5c40\u8bed\u4e49\u8868\u793a\uff0c\u8fd9\u79cd\u8868\u793a\u80fd\u529b\u6709\u9650\u4e14\u65e0\u6cd5\u5b9e\u73b0\u5c5e\u6027\u548c\u5bf9\u8c61\u7684\u5b8c\u5168\u89e3\u8026\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u5c5e\u6027-\u5bf9\u8c61\u7ec4\u5408\u7684\u6cdb\u5316\u6027\u80fd\u3002", "method": "CAMS\u8bbe\u8ba1\u4e86\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u4e00\u7ec4\u6f5c\u5728\u5355\u5143\u4eceCLIP\u7684\u9ad8\u5c42\u56fe\u50cf\u7f16\u7801\u5757\u4e2d\u6355\u83b7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7279\u5f81\uff0c\u540c\u65f6\u81ea\u9002\u5e94\u6291\u5236\u80cc\u666f\u548c\u65e0\u5173\u4fe1\u606f\uff1b\u968f\u540e\u8fdb\u884c\u591a\u7a7a\u95f4\u89e3\u8026\uff0c\u5728\u591a\u7ef4\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u5c5e\u6027\u548c\u5bf9\u8c61\u8bed\u4e49\u7684\u5206\u79bb\u3002", "result": "\u5728\u4e09\u4e2a\u4e3b\u6d41\u57fa\u51c6\u6570\u636e\u96c6\uff08MIT-States\u3001UT-Zappos\u548cC-GQA\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCAMS\u5728\u5c01\u95ed\u4e16\u754c\u548c\u5f00\u653e\u4e16\u754c\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5728CLIP\u6846\u67b6\u4e2d\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u548c\u591a\u7a7a\u95f4\u89e3\u8026\u673a\u5236\u80fd\u591f\u6709\u6548\u63d0\u5347\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2511.16435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16435", "abs": "https://arxiv.org/abs/2511.16435", "authors": ["Jin Wang", "Bingfeng Zhang", "Jian Pang", "Mengyu Liu", "Honglong Chen", "Weifeng Liu"], "title": "Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation", "comment": null, "summary": "Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm. Existing methods mainly mine references from support images as meta guidance. However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes. In this paper, we argue that the references from support images may not be essential, the key to the support role is to provide unbiased meta guidance for both trained and untrained classes. We then introduce a Language-Driven Attribute Generalization (LDAG) architecture to utilize inherent target property language descriptions to build robust support strategy. Specifically, to obtain an unbiased support representation, we design a Multi-attribute Enhancement (MaE) module, which produces multiple detailed attribute descriptions of the target class through Large Language Models (LLMs), and then builds refined visual-text prior guidance utilizing multi-modal matching. Meanwhile, due to text-vision modal shift, attribute text struggles to promote visual feature representation, we design a Multi-modal Attribute Alignment (MaA) to achieve cross-modal interaction between attribute texts and visual feature. Experiments show that our proposed method outperforms existing approaches by a clear margin and achieves the new state-of-the art performance. The code will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u9a71\u52a8\u7684\u5c5e\u6027\u6cdb\u5316\u67b6\u6784\uff0c\u901a\u8fc7\u5229\u7528\u76ee\u6807\u7c7b\u522b\u7684\u8bed\u8a00\u63cf\u8ff0\u800c\u975e\u652f\u6301\u56fe\u50cf\u6765\u6784\u5efa\u9c81\u68d2\u7684\u652f\u6301\u7b56\u7565\uff0c\u5728\u5c11\u6837\u672c\u5206\u5272\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u5c5e\u6027\u63cf\u8ff0\u5e76\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4ee5\u63d0\u4f9b\u65e0\u504f\u7684\u5143\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u4ece\u652f\u6301\u56fe\u50cf\u4e2d\u6316\u6398\u53c2\u8003\u4fe1\u606f\u4f5c\u4e3a\u5143\u6307\u5bfc\uff0c\u4f46\u7531\u4e8e\u89c6\u89c9\u8868\u793a\u4e2d\u7684\u7c7b\u5185\u5dee\u5f02\uff0c\u4ece\u652f\u6301\u56fe\u50cf\u63d0\u53d6\u7684\u5143\u4fe1\u606f\u65e0\u6cd5\u4e3a\u672a\u8bad\u7ec3\u7c7b\u522b\u63d0\u4f9b\u51c6\u786e\u6307\u5bfc\u3002\u672c\u6587\u8ba4\u4e3a\u652f\u6301\u56fe\u50cf\u63d0\u4f9b\u7684\u53c2\u8003\u53ef\u80fd\u5e76\u975e\u5fc5\u9700\uff0c\u5173\u952e\u5728\u4e8e\u4e3a\u5df2\u8bad\u7ec3\u548c\u672a\u8bad\u7ec3\u7c7b\u522b\u63d0\u4f9b\u65e0\u504f\u7684\u5143\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u8bed\u8a00\u9a71\u52a8\u7684\u5c5e\u6027\u6cdb\u5316\u67b6\u6784\uff0c\u5305\u542b\u591a\u5c5e\u6027\u589e\u5f3a\u6a21\u5757\u548c\u591a\u6a21\u6001\u5c5e\u6027\u5bf9\u9f50\u6a21\u5757\u3002\u524d\u8005\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u76ee\u6807\u7c7b\u522b\u7684\u591a\u4e2a\u8be6\u7ec6\u5c5e\u6027\u63cf\u8ff0\uff0c\u5229\u7528\u591a\u6a21\u6001\u5339\u914d\u6784\u5efa\u7cbe\u7ec6\u7684\u89c6\u89c9-\u6587\u672c\u5148\u9a8c\u6307\u5bfc\uff1b\u540e\u8005\u901a\u8fc7\u8de8\u6a21\u6001\u4ea4\u4e92\u89e3\u51b3\u6587\u672c-\u89c6\u89c9\u6a21\u6001\u504f\u79fb\u95ee\u9898\uff0c\u4fc3\u8fdb\u5c5e\u6027\u6587\u672c\u5bf9\u89c6\u89c9\u7279\u5f81\u8868\u793a\u7684\u63d0\u5347\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5c11\u6837\u672c\u5206\u5272\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5206\u5272\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5229\u7528\u76ee\u6807\u7c7b\u522b\u7684\u8bed\u8a00\u5c5e\u6027\u63cf\u8ff0\u800c\u975e\u652f\u6301\u56fe\u50cf\u53ef\u4ee5\u63d0\u4f9b\u66f4\u9c81\u68d2\u548c\u65e0\u504f\u7684\u5143\u6307\u5bfc\uff0c\u8fd9\u4e3a\u5c11\u6837\u672c\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002\u591a\u6a21\u6001\u5c5e\u6027\u5bf9\u9f50\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c-\u89c6\u89c9\u6a21\u6001\u504f\u79fb\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u8bed\u8a00\u9a71\u52a8\u65b9\u6cd5\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.16440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16440", "abs": "https://arxiv.org/abs/2511.16440", "authors": ["Diogo J. Paulo", "Jo\u00e3o Martins", "Hugo Proen\u00e7a", "Jo\u00e3o C. Neves"], "title": "StreetView-Waste: A Multi-Task Dataset for Urban Waste Management", "comment": "Accepted at WACV 2026", "summary": "Urban waste management remains a critical challenge for the development of smart cities. Despite the growing number of litter detection datasets, the problem of monitoring overflowing waste containers, particularly from images captured by garbage trucks, has received little attention. While existing datasets are valuable, they often lack annotations for specific container tracking or are captured in static, decontextualized environments, limiting their utility for real-world logistics. To address this gap, we present StreetView-Waste, a comprehensive dataset of urban scenes featuring litter and waste containers. The dataset supports three key evaluation tasks: (1) waste container detection, (2) waste container tracking, and (3) waste overflow segmentation. Alongside the dataset, we provide baselines for each task by benchmarking state-of-the-art models in object detection, tracking, and segmentation. Additionally, we enhance baseline performance by proposing two complementary strategies: a heuristic-based method for improved waste container tracking and a model-agnostic framework that leverages geometric priors to refine litter segmentation. Our experimental results show that while fine-tuned object detectors achieve reasonable performance in detecting waste containers, baseline tracking methods struggle to accurately estimate their number; however, our proposed heuristics reduce the mean absolute counting error by 79.6%. Similarly, while segmenting amorphous litter is challenging, our geometry-aware strategy improves segmentation mAP@0.5 by 27% on lightweight models, demonstrating the value of multimodal inputs for this task. Ultimately, StreetView-Waste provides a challenging benchmark to encourage research into real-world perception systems for urban waste management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86StreetView-Waste\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u57ce\u5e02\u5783\u573e\u5bb9\u5668\u76d1\u6d4b\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u652f\u6301\u5783\u573e\u5bb9\u5668\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u6ea2\u51fa\u5206\u5272\u4e09\u4e2a\u5173\u952e\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u8ddf\u8e2a\u65b9\u6cd5\u548c\u51e0\u4f55\u5148\u9a8c\u5206\u5272\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5783\u573e\u68c0\u6d4b\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u4e00\u822c\u5783\u573e\u8bc6\u522b\uff0c\u7f3a\u4e4f\u9488\u5bf9\u5783\u573e\u5bb9\u5668\u76d1\u6d4b\u7684\u4e13\u95e8\u6807\u6ce8\uff0c\u7279\u522b\u662f\u4ece\u5783\u573e\u8f66\u89c6\u89d2\u91c7\u96c6\u7684\u56fe\u50cf\u6570\u636e\uff0c\u4e14\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u9759\u6001\u73af\u5883\u62cd\u6444\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u57ce\u5e02\u7269\u6d41\u573a\u666f\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u4e86StreetView-Waste\u6570\u636e\u96c6\u5e76\u5efa\u7acb\u4e86\u4e09\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u76ee\u6807\u68c0\u6d4b\u3001\u76ee\u6807\u8ddf\u8e2a\u548c\u8bed\u4e49\u5206\u5272\u7684\u5148\u8fdb\u6a21\u578b\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u4e24\u79cd\u8865\u5145\u7b56\u7565\uff1a\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u6539\u8fdb\u5783\u573e\u5bb9\u5668\u8ddf\u8e2a\uff0c\u4ee5\u53ca\u5229\u7528\u51e0\u4f55\u5148\u9a8c\u7684\u6a21\u578b\u65e0\u5173\u6846\u67b6\u6765\u4f18\u5316\u5783\u573e\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5fae\u8c03\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u5783\u573e\u5bb9\u5668\u68c0\u6d4b\u4e0a\u8868\u73b0\u5408\u7406\uff0c\u4f46\u57fa\u7ebf\u8ddf\u8e2a\u65b9\u6cd5\u5728\u6570\u91cf\u4f30\u8ba1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u5c06\u5e73\u5747\u7edd\u5bf9\u8ba1\u6570\u8bef\u5dee\u964d\u4f4e\u4e8679.6%\uff1b\u51e0\u4f55\u611f\u77e5\u7b56\u7565\u5728\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e0a\u5c06\u5206\u5272mAP@0.5\u63d0\u9ad8\u4e8627%\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u8f93\u5165\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u4ef7\u503c\u3002", "conclusion": "StreetView-Waste\u4e3a\u57ce\u5e02\u5783\u573e\u7ba1\u7406\u7684\u771f\u5b9e\u611f\u77e5\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u548c\u51e0\u4f55\u5148\u9a8c\u80fd\u663e\u8457\u63d0\u5347\u5783\u573e\u76d1\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u667a\u80fd\u57ce\u5e02\u5e9f\u7269\u7ba1\u7406\u7cfb\u7edf\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.16449", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16449", "abs": "https://arxiv.org/abs/2511.16449", "authors": ["Ziyan Liu", "Yeqiu Chen", "Hongyi Cai", "Tao Lin", "Shuo Yang", "Zheng Liu", "Bo Zhao"], "title": "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVLA-Pruner\uff0c\u4e00\u79cd\u4e13\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u8bbe\u8ba1\u7684\u53cc\u5c42\u7ea7\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u76f8\u5173\u6027\u548c\u52a8\u4f5c\u91cd\u8981\u6027\u6807\u51c6\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901fVLA\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\u4ec5\u57fa\u4e8e\u8bed\u4e49\u663e\u8457\u6027\u6307\u6807\u9009\u62e9\u4ee4\u724c\uff0c\u5ffd\u7565\u4e86VLA\u6a21\u578b\u517c\u5177\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\u548c\u4f4e\u5c42\u52a8\u4f5c\u6267\u884c\u7684\u53cc\u7cfb\u7edf\u7279\u6027\uff0c\u5bfc\u81f4\u504f\u5411\u4fdd\u7559\u8bed\u4e49\u7ebf\u7d22\u800c\u4e22\u5f03\u5bf9\u52a8\u4f5c\u751f\u6210\u81f3\u5173\u91cd\u8981\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4eVLA\u6027\u80fd\u3002", "method": "VLA-Pruner\u91c7\u7528\u53cc\u5c42\u7ea7\u91cd\u8981\u6027\u6807\u51c6\uff1a\u89c6\u89c9-\u8bed\u8a00\u9884\u586b\u5145\u6ce8\u610f\u529b\u7528\u4e8e\u8bed\u4e49\u7ea7\u76f8\u5173\u6027\uff0c\u901a\u8fc7\u65f6\u95f4\u5e73\u6ed1\u4f30\u8ba1\u7684\u52a8\u4f5c\u89e3\u7801\u6ce8\u610f\u529b\u7528\u4e8e\u52a8\u4f5c\u7ea7\u91cd\u8981\u6027\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u81ea\u9002\u5e94\u53cc\u5c42\u7ea7\u4ee4\u724c\u9009\u62e9\u7b56\u7565\uff0c\u5728\u7ed9\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u4fdd\u7559\u7d27\u51d1\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u89c6\u89c9\u4ee4\u724c\u96c6\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVLA-Pruner\u5728\u591a\u79cdVLA\u67b6\u6784\u548c\u591a\u6837\u5316\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86VLA\u6a21\u578b\u4ee4\u724c\u526a\u679d\u9700\u8981\u8003\u8651\u5176\u53cc\u7cfb\u7edf\u7279\u6027\uff0c\u63d0\u51fa\u7684\u53cc\u5c42\u7ea7\u65b9\u6cd5\u4e3a\u9ad8\u6548VLA\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u5177\u8eabAI\u7cfb\u7edf\u7684\u5b9e\u65f6\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.16541", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16541", "abs": "https://arxiv.org/abs/2511.16541", "authors": ["Jaime \u00c1lvarez Urue\u00f1a", "David Camacho", "Javier Huertas Tato"], "title": "Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution", "comment": "17 pages, 6 figures, 6 tables", "summary": "The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical.\n  This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators.\n  With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3\\%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70\\% and 4.27\\% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u5c11\u6837\u672c\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5408\u6210\u56fe\u50cf\u68c0\u6d4b\u4e2d\u7684\u6cdb\u5316\u6311\u6218\uff0c\u5728\u4ec5\u9700\u5c11\u91cf\u672a\u89c1\u751f\u6210\u5668\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u5408\u6210\u56fe\u50cf\u8d8a\u6765\u8d8a\u96be\u4ee5\u4e0e\u771f\u5b9e\u5185\u5bb9\u533a\u5206\uff0c\u8fd9\u5bf9\u6570\u5b57\u5a92\u4f53\u5b8c\u6574\u6027\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5b9a\u671f\u91cd\u65b0\u8bad\u7ec3\uff0c\u800c\u65b0\u578b\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5e03\u5468\u671f\u4f7f\u5f97\u8fd9\u79cd\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\u4e14\u64cd\u4f5c\u4e0a\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u6b64\u9700\u8981\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d1\u5c55\u7684\u751f\u6210AI\u73af\u5883\u800c\u65e0\u9700\u5168\u9762\u91cd\u65b0\u8bad\u7ec3\u534f\u8bae\u7684\u9c81\u68d2\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u901a\u8fc7\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u7684\u89c6\u89c9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ece\u8f93\u5165\u56fe\u50cf\u4e2d\u63d0\u53d6\u5224\u522b\u6027\u5d4c\u5165\uff0c\u8be5\u6a21\u578b\u5728\u53ef\u7528\u751f\u6210\u5668\u7684\u6218\u7565\u5206\u533a\u5b50\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u7279\u5b9a\u67b6\u6784\u88ab\u4fdd\u7559\u7528\u4e8e\u4e25\u683c\u6d88\u878d\u8de8\u751f\u6210\u5668\u6cdb\u5316\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u5b66\u4e60\u7684\u5d4c\u5165\u7a7a\u95f4\u4e0a\u4f7f\u7528k\u8fd1\u90bb\u5206\u7c7b\u5668\uff0c\u91c7\u7528\u5c11\u6837\u672c\u5b66\u4e60\u8303\u5f0f\uff0c\u7ed3\u5408\u6765\u81ea\u5148\u524d\u672a\u89c1\u6d4b\u8bd5\u751f\u6210\u5668\u7684\u6709\u9650\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u5c11\u6837\u672c\u5b66\u4e60\u673a\u5236\u4e0b\u4ec5\u9700\u6bcf\u7c7b150\u5f20\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e8691.3%\u7684\u5e73\u5747\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e865.2\u4e2a\u767e\u5206\u70b9\uff1b\u5728\u6765\u6e90\u5f52\u56e0\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5f00\u653e\u96c6\u5206\u7c7b\u80cc\u666f\u4e0bAUC\u548cOSCR\u5206\u522b\u63d0\u9ad8\u4e8614.70%\u548c4.27%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u5c11\u6837\u672c\u5b66\u4e60\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d1\u5c55\u7684\u751f\u6210AI\u73af\u5883\u800c\u65e0\u9700\u5168\u9762\u91cd\u65b0\u8bad\u7ec3\u534f\u8bae\u7684\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u53d6\u8bc1\u5f52\u56e0\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u4e3a\u89e3\u51b3\u5408\u6210\u56fe\u50cf\u68c0\u6d4b\u4e2d\u7684\u6cdb\u5316\u6311\u6218\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16454", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16454", "abs": "https://arxiv.org/abs/2511.16454", "authors": ["Doriand Petit", "Steve Bourgeois", "Vincent Gay-Bellile", "Florian Chabot", "Lo\u00efc Barthe"], "title": "LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs", "comment": "Accepted at AAAI'26", "summary": "Developing a multi-modal language model capable of understanding 3D scenes remains challenging due to the limited availability of 3D training data, in contrast to the abundance of 2D datasets used for vision-language models (VLM). As an alternative, we introduce LLaVA$^3$ (pronounced LLaVA-Cube), a novel method that improves the 3D scene understanding capabilities of VLM using only multi-view 2D images and without any fine-tuning. Inspired by Cubist painters, who represented multiple viewpoints of a 3D object within a single picture, we propose to describe the 3D scene for the VLM through omnidirectional visual representations of each object. These representations are derived from an intermediate multi-view 3D reconstruction of the scene. Extensive experiments on 3D VQA and 3D language grounding show that our approach outperforms previous 2D-based VLM solutions.", "AI": {"tldr": "LLaVA\u00b3\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b3D\u573a\u666f\u7406\u89e3\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u591a\u89c6\u89d22D\u56fe\u50cf\uff0c\u901a\u8fc7\u7acb\u4f53\u4e3b\u4e49\u542f\u53d1\u7684\u5168\u65b9\u4f4d\u89c6\u89c9\u8868\u793a\u6765\u63cf\u8ff03D\u573a\u666f\uff0c\u57283D\u89c6\u89c9\u95ee\u7b54\u548c3D\u8bed\u8a00\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e2D\u7684VLM\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u57283D\u573a\u666f\u7406\u89e3\u65b9\u9762\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f3D\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u76f8\u6bd4\u4e4b\u4e0b\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u76842D\u6570\u636e\u96c6\u5219\u975e\u5e38\u4e30\u5bcc\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u4e09\u7ef4\u7a7a\u95f4\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u53d7\u7acb\u4f53\u4e3b\u4e49\u753b\u5bb6\u542f\u53d1\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e2d\u95f4\u591a\u89c6\u89d23D\u91cd\u5efa\u751f\u6210\u6bcf\u4e2a\u7269\u4f53\u7684\u5168\u65b9\u4f4d\u89c6\u89c9\u8868\u793a\uff0c\u5229\u7528\u8fd9\u4e9b\u8868\u793a\u5411\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63cf\u8ff03D\u573a\u666f\uff0c\u6574\u4e2a\u8fc7\u7a0b\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\u64cd\u4f5c\u3002", "result": "\u57283D\u89c6\u89c9\u95ee\u7b54\u548c3D\u8bed\u8a00\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5148\u524d\u57fa\u4e8e2D\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4ec5\u4f7f\u75282D\u591a\u89c6\u89d2\u56fe\u50cf\u5373\u53ef\u6709\u6548\u63d0\u53473D\u573a\u666f\u7406\u89e3\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u514b\u670d3D\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u8f9f\u4e86\u57fa\u4e8e2D\u6570\u636e\u589e\u5f3a3D\u7406\u89e3\u80fd\u529b\u7684\u65b0\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.16566", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16566", "abs": "https://arxiv.org/abs/2511.16566", "authors": ["Misaal Khan", "Mayank Vatsa", "Kuldeep Singh", "Richa Singh"], "title": "NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening", "comment": "Accepted in AAAI 2026 Special Track on AI for Social Impact", "summary": "Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NutriScreener\uff0c\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u7684\u591a\u59ff\u6001\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408CLIP\u89c6\u89c9\u5d4c\u5165\u3001\u7c7b\u522b\u589e\u5f3a\u77e5\u8bc6\u68c0\u7d22\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u5b9e\u73b0\u4e86\u4ece\u513f\u7ae5\u56fe\u50cf\u4e2d\u8fdb\u884c\u8425\u517b\u4e0d\u826f\u68c0\u6d4b\u548c\u4eba\u4f53\u6d4b\u91cf\u9884\u6d4b\u7684\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u532e\u4e4f\u73af\u5883\u3002", "motivation": "\u513f\u7ae5\u8425\u517b\u4e0d\u826f\u4ecd\u7136\u662f\u5168\u7403\u6027\u5371\u673a\uff0c\u73b0\u6709\u7b5b\u67e5\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u963b\u788d\u4e86\u65e9\u671f\u5e72\u9884\uff0c\u4e9f\u9700\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u89e3\u51b3\u6cdb\u5316\u6027\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u7684\u81ea\u52a8\u5316\u7b5b\u67e5\u5de5\u5177\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u7684\u591a\u59ff\u6001\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u67b6\u6784\uff0c\u6574\u5408\u4e86CLIP\u89c6\u89c9\u5d4c\u5165\u3001\u7c7b\u522b\u589e\u5f3a\u77e5\u8bc6\u68c0\u7d22\u673a\u5236\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u59ff\u6001\u4fe1\u606f\u878d\u5408\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u5339\u914d\u77e5\u8bc6\u5e93\u6765\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5305\u542b2,141\u540d\u513f\u7ae5\u7684\u4e34\u5e8a\u7814\u7a76\u4e2d\uff0c\u533b\u751f\u8bc4\u5206\u663e\u793a\u51c6\u786e\u60274.3/5\u3001\u6548\u73874.6/5\uff0c\u6a21\u578b\u8fbe\u52300.79\u53ec\u56de\u7387\u548c0.82 AUC\uff0c\u4eba\u4f53\u6d4b\u91cfRMSE\u663e\u8457\u964d\u4f4e\uff0c\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\u663e\u793a\u4f7f\u7528\u4eba\u53e3\u7edf\u8ba1\u5b66\u5339\u914d\u77e5\u8bc6\u5e93\u53ef\u83b7\u5f9725%\u53ec\u56de\u7387\u63d0\u5347\u548c3.5\u5398\u7c73RMSE\u51cf\u5c11\u3002", "conclusion": "NutriScreener\u4e3a\u8d44\u6e90\u532e\u4e4f\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u65e9\u671f\u8425\u517b\u4e0d\u826f\u68c0\u6d4b\u65b9\u6848\uff0c\u5176\u68c0\u7d22\u589e\u5f3a\u67b6\u6784\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u786e\u4fdd\u4e86\u5728\u65e0\u7ea6\u675f\u513f\u79d1\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u80fd\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2511.16524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16524", "abs": "https://arxiv.org/abs/2511.16524", "authors": ["Rahul Kumar", "Vipul Baghel", "Sudhanshu Singh", "Bikash Kumar Badatya", "Shivam Yadav", "Babji Srinivasan", "Ravi Hegde"], "title": "BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization", "comment": null, "summary": "Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments. In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing. The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes. Each clip is manually segmented and labeled to ensure precise temporal boundaries and class consistency, capturing a wide range of motion styles, camera angles, and athlete physiques. This dataset is specifically curated to support research in real-time vision-based action recognition, especially in low-resource and unconstrained environments. By providing a rich benchmark with diverse punch examples, this contribution aims to accelerate progress in movement analysis, automated coaching, and performance assessment within boxing and related domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u62f3\u51fb\u51fb\u6253\u68c0\u6d4b\u548c\u5206\u7c7b\u7684\u7efc\u5408\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5305\u542b6,915\u4e2a\u9ad8\u8d28\u91cf\u51fb\u6253\u7247\u6bb5\uff0c\u6db5\u76d6\u516d\u79cd\u4e0d\u540c\u51fb\u6253\u7c7b\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u52a8\u6001\u65e0\u7ea6\u675f\u73af\u5883\u4e2d\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u683c\u6597\u8fd0\u52a8\u5206\u6790\u9762\u4e34\u7684\u4e3b\u8981\u74f6\u9888\u662f\u7f3a\u4e4f\u9c81\u68d2\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u7531\u4e8e\u52a8\u4f5c\u7684\u52a8\u6001\u975e\u7ed3\u6784\u5316\u7279\u6027\u4ee5\u53ca\u5f55\u5236\u73af\u5883\u53d8\u5316\u9020\u6210\u7684\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u548c\u65e0\u7ea6\u675f\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u89c6\u89c9\u52a8\u4f5c\u8bc6\u522b\u7814\u7a76\u9700\u8981\u4e13\u95e8\u7684\u6570\u636e\u96c6\u652f\u6301\u3002", "method": "\u7814\u7a76\u56e2\u961f\u4ece20\u4e2a\u516c\u5f00YouTube\u8bad\u7ec3\u89c6\u9891\u4e2d\u63d0\u53d6\u4e866,915\u4e2a\u9ad8\u8d28\u91cf\u51fb\u6253\u7247\u6bb5\uff0c\u6d89\u53ca18\u540d\u4e0d\u540c\u8fd0\u52a8\u5458\uff0c\u6bcf\u4e2a\u7247\u6bb5\u90fd\u7ecf\u8fc7\u624b\u52a8\u5206\u5272\u548c\u6807\u6ce8\u4ee5\u786e\u4fdd\u7cbe\u786e\u7684\u65f6\u95f4\u8fb9\u754c\u548c\u7c7b\u522b\u4e00\u81f4\u6027\uff0c\u6db5\u76d6\u4e86\u5e7f\u6cdb\u7684\u8fd0\u52a8\u98ce\u683c\u3001\u6444\u50cf\u673a\u89d2\u5ea6\u548c\u8fd0\u52a8\u5458\u4f53\u578b\u3002", "result": "\u6784\u5efa\u7684\u6570\u636e\u96c6\u5305\u542b\u516d\u4e2a\u4e0d\u540c\u51fb\u6253\u7c7b\u578b\u76846,915\u4e2a\u6807\u6ce8\u7247\u6bb5\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u57fa\u51c6\u6d4b\u8bd5\u793a\u4f8b\uff0c\u80fd\u591f\u652f\u6301\u62f3\u51fb\u52a8\u4f5c\u5206\u6790\u3001\u81ea\u52a8\u5316\u6559\u7ec3\u548c\u6027\u80fd\u8bc4\u4f30\u7b49\u7814\u7a76\u65b9\u5411\u7684\u53d1\u5c55\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86\u62f3\u51fb\u52a8\u4f5c\u8bc6\u522b\u9886\u57df\u7684\u6570\u636e\u7a7a\u767d\uff0c\u901a\u8fc7\u63d0\u4f9b\u591a\u6837\u5316\u7684\u51fb\u6253\u793a\u4f8b\u57fa\u51c6\uff0c\u6709\u671b\u52a0\u901f\u8fd0\u52a8\u5206\u6790\u3001\u81ea\u52a8\u5316\u6559\u7ec3\u548c\u6027\u80fd\u8bc4\u4f30\u5728\u62f3\u51fb\u53ca\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u548c\u65e0\u7ea6\u675f\u73af\u5883\u4e0b\u7684\u5e94\u7528\u3002"}}
{"id": "2511.16527", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16527", "abs": "https://arxiv.org/abs/2511.16527", "authors": ["Kwun Ho Ngan", "Saman Sadeghi Afgeh", "Joe Townsend", "Artur d'Avila Garcez"], "title": "Contrastive vision-language learning with paraphrasing and negation", "comment": null, "summary": "Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSemCLIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u91ca\u4e49\u548c\u5426\u5b9a\u5904\u7406\u6765\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b\uff0c\u5728\u4fdd\u6301CLIP\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5bf9\u5426\u5b9a\u6587\u672c\u7684\u9c81\u68d2\u6027\uff0c\u5728CC-Neg\u57fa\u51c6\u4e0a\u5c06\u56fe\u50cf\u68c0\u7d22\u51c6\u786e\u7387\u4ece68.1%\u63d0\u5347\u81f378.1%\u3002", "motivation": "\u5bf9\u6bd4\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5426\u5b9a\u548c\u91ca\u4e49\u6587\u672c\u65f6\u8868\u73b0\u51fa\u6df7\u5408\u6027\u80fd\uff0c\u56e0\u4e3a\u5426\u5b9a\u4f1a\u901a\u8fc7\u6700\u5c0f\u8bcd\u6c47\u53d8\u5316\u5f7b\u5e95\u6539\u53d8\u8bed\u4e49\uff0c\u800c\u91ca\u4e49\u53ef\u80fd\u4ea7\u751f\u5b8c\u5168\u4e0d\u540c\u4f46\u8bed\u4e49\u76f8\u540c\u7684\u6587\u672c\u8868\u8fbe\uff0c\u8fd9\u5bf9\u6539\u8fdb\u6a21\u578b\u8bc4\u4f30\u7ed3\u679c\u548c\u8bed\u4e49\u5bf9\u9f50\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u672c\u6587\u8bc4\u4f30\u4e86\u91ca\u4e49\u548c\u5426\u5b9a\u7684\u7ec4\u5408\uff0c\u63d0\u51fa\u4e86\u65b0\u7684CLIP\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u6765\u540c\u65f6\u8003\u8651\u91ca\u4e49\u548c\u5426\u5b9a\uff0c\u5e76\u5e94\u7528LLM\u751f\u6210\u7684\u8bad\u7ec3\u4e09\u5143\u7ec4\uff08\u5305\u542b\u539f\u59cb\u3001\u91ca\u4e49\u548c\u5426\u5b9a\u6587\u672c\u63cf\u8ff0\uff09\u6765\u8bad\u7ec3CLIP\u7c7b\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u79f0\u4e3aSemCLIP\uff0c\u80fd\u591f\u5c06\u91ca\u4e49\u63cf\u8ff0\u5411\u539f\u59cb\u56fe\u50cf\u5d4c\u5165\u62c9\u8fd1\uff0c\u540c\u65f6\u5c06\u5426\u5b9a\u63cf\u8ff0\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u63a8\u8fdc\u3002", "result": "\u5728CC-Neg\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u539f\u59cb\u5bf9\u5426\u5b9a\u7684\u56fe\u50cf\u68c0\u7d22\u51c6\u786e\u7387\u6307\u6807\uff0cSemCLIP\u5c06\u51c6\u786e\u7387\u4ece68.1%\u63d0\u5347\u81f378.1%\uff1b\u5728Sugarcrepe++\u57fa\u51c6\u4e0a\u8868\u73b0\u6df7\u5408\u4f46\u603b\u4f53\u4e0a\u4f18\u4e8e\u4ec5\u4f7f\u7528\u5426\u5b9a\u63cf\u8ff0\u8bad\u7ec3\u7684\u6a21\u578b\uff1b\u5728\u4e0b\u6e38\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5728Sugarcrepe++\u4e0a\u9884\u8bad\u7ec3\u7684SemCLIP\u5728\u6240\u6709\u6d4b\u8bd5\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8eCLIP\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660eSemCLIP\u80fd\u591f\u5b9e\u73b0\u5bf9\u8bed\u4e49\u53d8\u6362\u7684\u663e\u8457\u9c81\u68d2\u6027\uff0c\u8868\u660e\u901a\u8fc7\u7ed3\u5408\u91ca\u4e49\u548c\u5426\u5b9a\u5904\u7406\u7684\u8bad\u7ec3\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bf9\u590d\u6742\u8bed\u4e49\u53d8\u5316\u7684\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u8bed\u4e49\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2511.16674", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16674", "abs": "https://arxiv.org/abs/2511.16674", "authors": ["George Cazenavette", "Antonio Torralba", "Vincent Sitzmann"], "title": "Dataset Distillation for Pre-Trained Self-Supervised Vision Models", "comment": "Accepted at NeurIPS 2025. Project page: https://linear-gradient-matching.github.io/ Code: https://github.com/GeorgeCazenavette/linear-gradient-matching", "summary": "The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u7ebf\u6027\u68af\u5ea6\u5339\u914d\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u5408\u6210\u56fe\u50cf\u4f7f\u5f97\u7ebf\u6027\u5206\u7c7b\u5668\u4ea7\u751f\u7684\u68af\u5ea6\u4e0e\u771f\u5b9e\u6570\u636e\u5339\u914d\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u771f\u5b9e\u56fe\u50cf\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4ece\u968f\u673a\u521d\u59cb\u5316\u6a21\u578b\u8bad\u7ec3\uff0c\u4f46\u5f53\u524d\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u65b9\u6cd5\u8d8a\u6765\u8d8a\u591a\u5730\u57fa\u4e8e\u5927\u578b\u9884\u8bad\u7ec3\u81ea\u76d1\u7763\u6a21\u578b\u800c\u975e\u4ece\u5934\u8bad\u7ec3\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u4e3a\u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u84b8\u998f\u51fa\u80fd\u591f\u6700\u4f18\u8bad\u7ec3\u7ebf\u6027\u63a2\u9488\u7684\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u7ebf\u6027\u68af\u5ea6\u5339\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5408\u6210\u56fe\u50cf\u4f7f\u5f97\u5f53\u5b83\u4eec\u901a\u8fc7\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u65f6\uff0c\u5728\u7ebf\u6027\u5206\u7c7b\u5668\u4e2d\u4ea7\u751f\u7684\u68af\u5ea6\u4e0e\u771f\u5b9e\u6570\u636e\u4ea7\u751f\u7684\u68af\u5ea6\u76f8\u4f3c\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9ad8\u6548\u6570\u636e\u96c6\u84b8\u998f\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5728\u6240\u6709\u771f\u5b9e\u56fe\u50cf\u57fa\u7ebf\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u591f\u8de8\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u6cdb\u5316\uff0c\u4f8b\u5982\u4f7f\u7528DINO\u9aa8\u5e72\u7f51\u7edc\u84b8\u998f\u7684\u6570\u636e\u96c6\u53ef\u4ee5\u8bad\u7ec3\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684CLIP\u7ebf\u6027\u63a2\u9488\uff0c\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\u3002", "conclusion": "\u8be5\u84b8\u998f\u6570\u636e\u96c6\u4e3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u80fd\u591f\u9884\u6d4b\u4e24\u4e2a\u6a21\u578b\u5d4c\u5165\u7a7a\u95f4\u7684\u76f8\u4f3c\u6027\u4ee5\u53ca\u6a21\u578b\u5bf9\u5bf9\u6297\u6570\u636e\u96c6\u4e2d\u865a\u5047\u76f8\u5173\u6027\u7684\u654f\u611f\u6027\uff0c\u9a8c\u8bc1\u4e86\u67cf\u62c9\u56fe\u8868\u793a\u5047\u8bbe\u4e0b\u7684\u6a21\u578b\u884c\u4e3a\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.16555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16555", "abs": "https://arxiv.org/abs/2511.16555", "authors": ["Junpeng Jing", "Weixun Luo", "Ye Mao", "Krystian Mikolajczyk"], "title": "Lite Any Stereo: Efficient Zero-Shot Stereo Matching", "comment": null, "summary": "Recent advances in stereo matching have focused on accuracy, often at the cost of significantly increased model size. Traditionally, the community has regarded efficient models as incapable of zero-shot ability due to their limited capacity. In this paper, we introduce Lite Any Stereo, a stereo depth estimation framework that achieves strong zero-shot generalization while remaining highly efficient. To this end, we design a compact yet expressive backbone to ensure scalability, along with a carefully crafted hybrid cost aggregation module. We further propose a three-stage training strategy on million-scale data to effectively bridge the sim-to-real gap. Together, these components demonstrate that an ultra-light model can deliver strong generalization, ranking 1st across four widely used real-world benchmarks. Remarkably, our model attains accuracy comparable to or exceeding state-of-the-art non-prior-based accurate methods while requiring less than 1% computational cost, setting a new standard for efficient stereo matching.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Lite Any Stereo\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5176\u8ba1\u7b97\u6210\u672c\u4e0d\u5230\u73b0\u6709\u65b9\u6cd5\u76841%\u5374\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u5148\u8fdb\u975e\u5148\u9a8c\u65b9\u6cd5\u7684\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u7acb\u4f53\u5339\u914d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7cbe\u5ea6\u63d0\u5347\uff0c\u4f46\u5f80\u5f80\u4ee5\u663e\u8457\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u4e3a\u4ee3\u4ef7\uff0c\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u9ad8\u6548\u6a21\u578b\u7531\u4e8e\u5bb9\u91cf\u6709\u9650\u800c\u65e0\u6cd5\u5177\u5907\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6548\u7387\u4e0e\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u7d27\u51d1\u800c\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u9aa8\u5e72\u7f51\u7edc\u786e\u4fdd\u53ef\u6269\u5c55\u6027\uff0c\u6784\u5efa\u4e86\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6df7\u5408\u4ee3\u4ef7\u805a\u5408\u6a21\u5757\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u767e\u4e07\u7ea7\u6570\u636e\u4e0a\u7684\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u4ee5\u6709\u6548\u5f25\u5408\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "result": "\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u7cbe\u5ea6\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u5148\u8fdb\u7684\u975e\u5148\u9a8c\u65b9\u6cd5\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u4e0d\u5230\u8fd9\u4e9b\u65b9\u6cd5\u76841%\uff0c\u4e3a\u9ad8\u6548\u7acb\u4f53\u5339\u914d\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u8d85\u8f7b\u91cf\u7ea7\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u8ba4\u4e3a\u9ad8\u6548\u6a21\u578b\u65e0\u6cd5\u5177\u5907\u96f6\u6837\u672c\u80fd\u529b\u7684\u89c2\u5ff5\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16618", "categories": ["cs.CV", "eess.IV", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2511.16618", "abs": "https://arxiv.org/abs/2511.16618", "authors": ["Haofeng Liu", "Ziyue Wang", "Sudhanshu Mishra", "Mingqi Gao", "Guanyi Qin", "Chang Han Low", "Alex Y. W. Kong", "Yueming Jin"], "title": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking", "comment": "11 pages, 4 figures", "summary": "Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \\textbf{SAM2} for \\textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\\mathcal{J}$\\&$\\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\\mathcal{J}$\\&$\\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86SAM2S\uff0c\u4e00\u4e2a\u589e\u5f3aSAM2\u7528\u4e8e\u5916\u79d1\u624b\u672f\u4ea4\u4e92\u5f0f\u89c6\u9891\u5206\u5272\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efaSA-SV\u57fa\u51c6\u6570\u636e\u96c6\u5e76\u5f15\u5165\u591a\u6837\u5316\u8bb0\u5fc6\u673a\u5236\u3001\u65f6\u5e8f\u8bed\u4e49\u5b66\u4e60\u548c\u6297\u6a21\u7cca\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u573a\u666f\u4e2d\u7684\u957f\u671f\u8ddf\u8e2a\u6027\u80fd\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5916\u79d1\u624b\u672f\u89c6\u9891\u5206\u5272\u5bf9\u4e8e\u8ba1\u7b97\u673a\u8f85\u52a9\u624b\u672f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u4ea4\u4e92\u5f0f\u89c6\u9891\u5206\u5272\u6a21\u578b\u5982SAM2\u5728\u5916\u79d1\u624b\u672f\u573a\u666f\u4e2d\u9762\u4e34\u9886\u57df\u5dee\u8ddd\u548c\u6709\u9650\u957f\u671f\u8ddf\u8e2a\u80fd\u529b\u7684\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u624b\u672f\u573a\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86SA-SV\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86SAM2S\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1aDiveMem\u591a\u6837\u5316\u8bb0\u5fc6\u673a\u5236\u7528\u4e8e\u9c81\u68d2\u957f\u671f\u8ddf\u8e2a\uff0c\u65f6\u5e8f\u8bed\u4e49\u5b66\u4e60\u7528\u4e8e\u5668\u68b0\u7406\u89e3\uff0c\u4ee5\u53ca\u6297\u6a21\u7cca\u5b66\u4e60\u673a\u5236\u6765\u7f13\u89e3\u591a\u6e90\u6570\u636e\u96c6\u4e2d\u7684\u6807\u6ce8\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728SA-SV\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4f7fSAM2\u7684\u5e73\u5747J&F\u6307\u6807\u63d0\u5347\u4e8612.99\u70b9\uff0c\u800cSAM2S\u8fdb\u4e00\u6b65\u5c06\u6027\u80fd\u63d0\u5347\u81f380.42\u5e73\u5747J&F\uff0c\u5206\u522b\u6bd4\u539f\u59cbSAM2\u548c\u5fae\u8c03SAM2\u9ad8\u51fa17.10\u548c4.11\u70b9\uff0c\u540c\u65f6\u4fdd\u630168 FPS\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u548c\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u4e13\u95e8\u9488\u5bf9\u5916\u79d1\u624b\u672f\u573a\u666f\u8bbe\u8ba1\u7684\u4ea4\u4e92\u5f0f\u89c6\u9891\u5206\u5272\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0cSAM2S\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u8ddf\u8e2a\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u8ba1\u7b97\u673a\u8f85\u52a9\u624b\u672f\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2511.16668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16668", "abs": "https://arxiv.org/abs/2511.16668", "authors": ["Yang Luo", "Xuanlei Zhao", "Baijiong Lin", "Lingting Zhu", "Liyao Tang", "Yuqi Liu", "Ying-Cong Chen", "Shengju Qian", "Xin Wang", "Yang You"], "title": "V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models", "comment": "Project Page: https://oahzxl.github.io/VReasonBench", "summary": "Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86V-ReasonBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u89c6\u9891\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6db5\u76d6\u7ed3\u6784\u5316\u95ee\u9898\u89e3\u51b3\u3001\u7a7a\u95f4\u8ba4\u77e5\u3001\u6a21\u5f0f\u63a8\u7406\u548c\u7269\u7406\u52a8\u529b\u5b66\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u4e3a\u89c6\u9891\u63a8\u7406\u6a21\u578b\u7684\u53ef\u9760\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u89c6\u9891\u6a21\u578b\uff08\u5982Veo-3\uff09\u5c55\u73b0\u51fa\u4ee4\u4eba\u60ca\u8bb6\u7684\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\uff0c\u8feb\u5207\u9700\u8981\u7cfb\u7edf\u4e14\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5f53\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u89c6\u9891\u63a8\u7406\u80fd\u529b\u7684\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u5e8f\u5217\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u8bbe\u8ba1\u4e86\u591a\u6837\u5316\u4e14\u7b54\u6848\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\uff0c\u786e\u4fdd\u8bc4\u4f30\u7684\u53ef\u91cd\u590d\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u65e0\u6b67\u4e49\u6027\uff0c\u8bc4\u4f30\u4e86\u516d\u79cd\u6700\u5148\u8fdb\u7684\u89c6\u9891\u6a21\u578b\u5728\u56db\u4e2a\u63a8\u7406\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u89c6\u9891\u6a21\u578b\u5728\u7ed3\u6784\u5316\u3001\u7a7a\u95f4\u3001\u6a21\u5f0f\u63a8\u7406\u548c\u7269\u7406\u52a8\u529b\u5b66\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fdb\u4e00\u6b65\u6bd4\u8f83\u4e86\u89c6\u9891\u6a21\u578b\u4e0e\u5f3a\u56fe\u50cf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5206\u6790\u4e86\u5e38\u89c1\u7684\u5e7b\u89c9\u884c\u4e3a\uff0c\u5e76\u7814\u7a76\u4e86\u89c6\u9891\u65f6\u957f\u5bf9\u5e27\u94fe\u63a8\u7406\u7684\u5f71\u54cd\u3002", "conclusion": "V-ReasonBench\u4e3a\u6d4b\u91cf\u89c6\u9891\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u53ef\u91cd\u590d\u7684\u6846\u67b6\uff0c\u65e8\u5728\u652f\u6301\u5f00\u53d1\u5177\u6709\u66f4\u53ef\u9760\u3001\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u63a8\u7406\u6280\u80fd\u7684\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u9891\u6a21\u578b\u5728\u4e0d\u540c\u63a8\u7406\u7ef4\u5ea6\u4e0a\u7684\u5177\u4f53\u4f18\u52bf\u548c\u5c40\u9650\u3002"}}
{"id": "2511.16669", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16669", "abs": "https://arxiv.org/abs/2511.16669", "authors": ["Junhao Cheng", "Liang Hou", "Xin Tao", "Jing Liao"], "title": "Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO", "comment": "Project page: https://video-as-answer.github.io/", "summary": "While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVANS\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u89c6\u9891\u6269\u6563\u6a21\u578b\u5bf9\u9f50\uff0c\u7528\u4e8e\u89c6\u9891\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u4ece\u6587\u672c\u63cf\u8ff0\u5230\u52a8\u6001\u89c6\u9891\u751f\u6210\u7684\u8f6c\u53d8\uff0c\u5728\u7a0b\u5e8f\u6027\u548c\u9884\u6d4b\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5f71\u54cd\u5e7f\u6cdb\uff0c\u4f46\u89c6\u9891\u751f\u6210\u4e3b\u8981\u5c40\u9650\u4e8e\u5a31\u4e50\u9886\u57df\u3002\u89c6\u9891\u5177\u6709\u5c55\u793a\u96be\u4ee5\u901a\u8fc7\u7eaf\u8bed\u8a00\u4f20\u8fbe\u7684\u7269\u7406\u4e16\u754c\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u672c\u6587\u8bc6\u522b\u4e86\u5c06\u89c6\u9891\u4f5c\u4e3a\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u65b0\u7b54\u6848\u6a21\u6001\u7684\u672a\u5145\u5206\u5229\u7528\u673a\u4f1a\uff0c\u63d0\u51fa\u4e86\u89c6\u9891\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u4efb\u52a1\uff0c\u4ece\u201c\u8bb2\u8ff0\u201d\u8f6c\u5411\u201c\u5c55\u793a\u201d\u4ee5\u89e3\u9501\u66f4\u76f4\u89c2\u548c\u5b9a\u5236\u5316\u7684\u7a0b\u5e8f\u5b66\u4e60\u548c\u521b\u9020\u6027\u63a2\u7d22\u7b54\u6848\u3002", "method": "\u672c\u6587\u5f15\u5165VANS\u6a21\u578b\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u89c6\u9891\u6269\u6563\u6a21\u578b\u5bf9\u9f50\u7528\u4e8eVNEP\u4efb\u52a1\u3002\u6838\u5fc3\u662f\u63d0\u51fa\u7684Joint-GRPO\u65b9\u6cd5\uff0c\u5c06VLM\u548cVDM\u534f\u8c03\u4e3a\u4e00\u4e2a\u5355\u5143\u8fd0\u884c\uff0c\u901a\u8fc7\u5bf9\u5176\u5404\u81ea\u8f93\u51fa\u7684\u5171\u4eab\u5956\u52b1\u8fdb\u884c\u4f18\u5316\uff0c\u4f7fVLM\u751f\u6210\u65e2\u51c6\u786e\u53c8\u6613\u4e8e\u53ef\u89c6\u5316\u7684\u63cf\u8ff0\uff0c\u540c\u65f6\u6307\u5bfcVDM\u751f\u6210\u5fe0\u5b9e\u4e8e\u8fd9\u4e9b\u63cf\u8ff0\u548c\u8f93\u5165\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u89c6\u9891\u3002\u4e3a\u652f\u6301\u6b64\u5b66\u4e60\u8fc7\u7a0b\uff0c\u6784\u5efa\u4e86\u4e13\u7528\u7684VANS-Data-100K\u6570\u636e\u96c6\u3002", "result": "\u5728\u7a0b\u5e8f\u6027\u548c\u9884\u6d4b\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVANS\u5728\u89c6\u9891\u4e8b\u4ef6\u9884\u6d4b\u548c\u53ef\u89c6\u5316\u65b9\u9762\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u7406\u89e3\u591a\u6a21\u6001\u8f93\u5165\u3001\u8fdb\u884c\u6307\u4ee4\u6761\u4ef6\u63a8\u7406\uff0c\u5e76\u751f\u6210\u5177\u6709\u89c6\u89c9\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u89c6\u9891\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5c55\u793a\u4e86\u5c06\u89c6\u9891\u4f5c\u4e3a\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u7b54\u6848\u6a21\u6001\u7684\u6f5c\u529b\uff0c\u4ece\u6587\u672c\u63cf\u8ff0\u8f6c\u5411\u52a8\u6001\u89c6\u9891\u751f\u6210\u80fd\u591f\u63d0\u4f9b\u66f4\u76f4\u89c2\u548c\u5b9a\u5236\u5316\u7684\u56de\u7b54\u3002VANS\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u591a\u6a21\u6001\u7ec4\u4ef6\u7684\u65b9\u6cd5\u4e3a\u89e3\u51b3VNEP\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u7a0b\u5e8f\u6027\u5b66\u4e60\u548c\u521b\u9020\u6027\u63a2\u7d22\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.16670", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16670", "abs": "https://arxiv.org/abs/2511.16670", "authors": ["Chenyu Lin", "Cheng Chi", "Jinlin Wu", "Sharon Li", "Kaiyang Zhou"], "title": "Learning to Think Fast and Slow for Visual Language Models", "comment": null, "summary": "When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6839\u636e\u4efb\u52a1\u96be\u5ea6\u81ea\u52a8\u5207\u6362\u5feb\u901f\u601d\u8003\u548c\u6162\u901f\u601d\u8003\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u9762\u5411\u63a8\u7406\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u8ffd\u6c42\u5197\u957f\u7684\u63a8\u7406\u94fe\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u4eba\u7c7b\u8ba4\u77e5\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u5206\u914d\u8ba4\u77e5\u8d44\u6e90\uff0c\u8fd9\u79cd\u53cc\u7cfb\u7edf\u601d\u8003\u673a\u5236\u5c1a\u672a\u5728VLMs\u4e2d\u5f97\u5230\u6709\u6548\u5b9e\u73b0\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u57fa\u4e8e\u6a21\u578b\u8f93\u51fa\u957f\u5ea6\u6807\u6ce8\u6570\u636e\u4e3a\u5feb\u901f\u601d\u8003\u6216\u6162\u901f\u601d\u8003\u6a21\u5f0f\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528GRPO\u7ed3\u5408\u601d\u8003\u6a21\u5f0f\u6807\u7b7e\u8bad\u7ec3\u6a21\u578b\u53d1\u5c55\u53cc\u6a21\u5f0f\u601d\u8003\u80fd\u529b\u3002", "result": "DualMindVLM\u6a21\u578b\u663e\u8457\u8d85\u8d8a\u57fa\u7840\u6a21\u578b\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u63a8\u7406\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6781\u9ad8\u7684token\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u63a8\u7406\u6027\u80fd\u7684\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u7b80\u5355\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6a21\u62df\u4eba\u7c7b\u53cc\u7cfb\u7edf\u8ba4\u77e5\u673a\u5236\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u89c6\u89c9\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u601d\u8003\u6a21\u5f0f\u5207\u6362\u5728\u63d0\u5347\u6a21\u578b\u6548\u7387\u65b9\u9762\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2511.16672", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16672", "abs": "https://arxiv.org/abs/2511.16672", "authors": ["Omkat Thawakar", "Shravan Venkatraman", "Ritesh Thawkar", "Abdelrahman Shaker", "Hisham Cholakkal", "Rao Muhammad Anwer", "Salman Khan", "Fahad Khan"], "title": "EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards", "comment": "9 Pages, 6 Figures, 4 Tables", "summary": "Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\\sim$3\\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEvoLMM\uff0c\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u5956\u52b1\u6a21\u578b\u7684\u7eaf\u65e0\u76d1\u7763\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u534f\u4f5c\u4ee3\u7406\uff08\u63d0\u8bae\u8005\u548c\u6c42\u89e3\u8005\uff09\u5b9e\u73b0\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u81ea\u6211\u6539\u8fdb\uff0c\u5728\u591a\u4e2a\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u8bad\u7ec3\u6d41\u7a0b\u4ecd\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6216\u5916\u90e8\u9a8c\u8bc1\u7684\u5956\u52b1\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u81ea\u4e3b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65e0\u9700\u4efb\u4f55\u6807\u6ce8\u6570\u636e\u6216\u5956\u52b1\u84b8\u998f\u7684\u7eaf\u65e0\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEvoLMM\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u4ece\u5355\u4e00\u9aa8\u5e72\u6a21\u578b\u5b9e\u4f8b\u5316\u4e24\u4e2a\u534f\u4f5c\u4ee3\u7406\uff1a\u63d0\u8bae\u8005\u751f\u6210\u591a\u6837\u5316\u7684\u56fe\u50cf\u57fa\u7840\u95ee\u9898\uff0c\u6c42\u89e3\u8005\u901a\u8fc7\u5185\u90e8\u4e00\u81f4\u6027\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b66\u4e60\u8fc7\u7a0b\u901a\u8fc7\u6301\u7eed\u7684\u81ea\u5956\u52b1\u673a\u5236\u8fdb\u884c\uff0c\u65e0\u9700\u4f9d\u8d56\u771f\u5b9e\u6807\u7b7e\u6216\u4eba\u5de5\u5224\u65ad\u3002", "result": "\u4ee5Qwen2.5-VL\u4e3a\u57fa\u7840\u6a21\u578b\uff0cEvoLMM\u5728ChartQA\u3001MathVista\u548cMathVision\u7b49\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u7ea63%\u7684\u6301\u7eed\u6027\u80fd\u63d0\u5347\uff0c\u4ec5\u4f7f\u7528\u539f\u59cb\u8bad\u7ec3\u56fe\u50cf\u3002", "conclusion": "\u8fd9\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u7eaf\u65e0\u76d1\u7763\u65b9\u6cd5\u4e3a\u81ea\u6539\u8fdb\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u6a21\u578b\u80fd\u529b\u6301\u7eed\u8fdb\u5316\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u81ea\u8fdb\u5316\u591a\u6a21\u6001\u6a21\u578b\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
