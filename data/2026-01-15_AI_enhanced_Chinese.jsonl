{"id": "2601.08860", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08860", "abs": "https://arxiv.org/abs/2601.08860", "authors": ["Tarannum Mithila"], "title": "Bias Detection and Rotation-Robustness Mitigation in Vision-Language Models and Generative Image Models", "comment": "Preprint. This work is derived from the author's Master's research. Code and supplementary materials will be released separately", "summary": "Vision-Language Models (VLMs) and generative image models have achieved remarkable performance across multimodal tasks, yet their robustness and fairness under input transformations remain insufficiently explored. This work investigates bias propagation and robustness degradation in state-of-the-art vision-language and generative models, with a particular focus on image rotation and distributional shifts. We analyze how rotation-induced perturbations affect model predictions, confidence calibration, and demographic bias patterns. To address these issues, we propose rotation-robust mitigation strategies that combine data augmentation, representation alignment, and model-level regularization. Experimental results across multiple datasets demonstrate that the proposed methods significantly improve robustness while reducing bias amplification without sacrificing overall performance. This study highlights critical limitations of current multimodal systems and provides practical mitigation techniques for building more reliable and fair AI models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u751f\u6210\u5f0f\u56fe\u50cf\u6a21\u578b\u5728\u8f93\u5165\u53d8\u6362\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u56fe\u50cf\u65cb\u8f6c\u5f15\u8d77\u7684\u504f\u5dee\u4f20\u64ad\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u3001\u8868\u793a\u5bf9\u9f50\u548c\u6a21\u578b\u6b63\u5219\u5316\u7684\u65cb\u8f6c\u9c81\u68d2\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u751f\u6210\u5f0f\u56fe\u50cf\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\uff0c\u4f46\u5176\u5728\u8f93\u5165\u53d8\u6362\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u548c\u751f\u6210\u6a21\u578b\u4e2d\u7684\u504f\u5dee\u4f20\u64ad\u548c\u9c81\u68d2\u6027\u9000\u5316\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u56fe\u50cf\u65cb\u8f6c\u548c\u5206\u5e03\u504f\u79fb\u5bf9\u6a21\u578b\u9884\u6d4b\u3001\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\u6a21\u5f0f\u7684\u5f71\u54cd\u3002", "method": "\u4e3a\u89e3\u51b3\u65cb\u8f6c\u5f15\u8d77\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u65cb\u8f6c\u9c81\u68d2\u7f13\u89e3\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6570\u636e\u589e\u5f3a\u3001\u8868\u793a\u5bf9\u9f50\u548c\u6a21\u578b\u7ea7\u6b63\u5219\u5316\u6280\u672f\u3002\u8fd9\u4e9b\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u591a\u5c42\u6b21\u7684\u5e72\u9884\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u65cb\u8f6c\u53d8\u6362\u7684\u9002\u5e94\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u504f\u5dee\u653e\u5927\u6548\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u504f\u5dee\u653e\u5927\uff0c\u4e14\u6ca1\u6709\u727a\u7272\u6574\u4f53\u6027\u80fd\u3002\u7814\u7a76\u91cf\u5316\u4e86\u65cb\u8f6c\u8bf1\u5bfc\u6270\u52a8\u5bf9\u6a21\u578b\u9884\u6d4b\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u5177\u4f53\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u7f13\u89e3\u7b56\u7565\u5728\u6539\u5584\u6a21\u578b\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5bf9\u8f93\u5165\u53d8\u6362\u7684\u654f\u611f\u6027\u5bfc\u81f4\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u95ee\u9898\u3002\u7814\u7a76\u63d0\u4f9b\u7684\u5b9e\u7528\u7f13\u89e3\u6280\u672f\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u548c\u516c\u5e73\u7684AI\u6a21\u578b\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u652f\u6301\uff0c\u5f3a\u8c03\u4e86\u5728\u6a21\u578b\u5f00\u53d1\u4e2d\u8003\u8651\u53d8\u6362\u9c81\u68d2\u6027\u548c\u504f\u5dee\u7f13\u89e3\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.08868", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08868", "abs": "https://arxiv.org/abs/2601.08868", "authors": ["Yi Wang", "Yinfeng Yu", "Bin Ren"], "title": "Residual Cross-Modal Fusion Networks for Audio-Visual Navigation", "comment": "Main paper (10 pages). Accepted for publication by the 14th international conference on Computational Visual Media (CVM 2026)", "summary": "Audio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the interaction between heterogeneous features during multimodal fusion, so as to avoid single-modality dominance or information degradation, particularly in cross-domain scenarios. To address this, we propose a Cross-Modal Residual Fusion Network, which introduces bidirectional residual interactions between audio and visual streams to achieve complementary modeling and fine-grained alignment, while maintaining the independence of their representations. Unlike conventional methods that rely on simple concatenation or attention gating, CRFN explicitly models cross-modal interactions via residual connections and incorporates stabilization techniques to improve convergence and robustness. Experiments on the Replica and Matterport3D datasets demonstrate that CRFN significantly outperforms state-of-the-art fusion baselines and achieves stronger cross-domain generalization. Notably, our experiments also reveal that agents exhibit differentiated modality dependence across different datasets. The discovery of this phenomenon provides a new perspective for understanding the cross-modal collaboration mechanism of embodied agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u6b8b\u5dee\u878d\u5408\u7f51\u7edc\uff08CRFN\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u97f3\u9891-\u89c6\u89c9\u5177\u8eab\u5bfc\u822a\u4e2d\u7684\u591a\u6a21\u6001\u878d\u5408\u6311\u6218\uff0c\u901a\u8fc7\u53cc\u5411\u6b8b\u5dee\u4ea4\u4e92\u5b9e\u73b0\u4e92\u8865\u5efa\u6a21\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u97f3\u9891-\u89c6\u89c9\u5177\u8eab\u5bfc\u822a\u4efb\u52a1\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u5f02\u6784\u7279\u5f81\u5728\u591a\u6a21\u6001\u878d\u5408\u8fc7\u7a0b\u4e2d\u7684\u6709\u6548\u4ea4\u4e92\u5efa\u6a21\uff0c\u4ee5\u907f\u514d\u5355\u6a21\u6001\u4e3b\u5bfc\u6216\u4fe1\u606f\u9000\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8de8\u57df\u573a\u666f\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7b80\u5355\u7684\u62fc\u63a5\u6216\u6ce8\u610f\u529b\u95e8\u63a7\u673a\u5236\uff0c\u96be\u4ee5\u5b9e\u73b0\u6a21\u6001\u95f4\u7684\u4e92\u8865\u5efa\u6a21\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u8de8\u6a21\u6001\u6b8b\u5dee\u878d\u5408\u7f51\u7edc\uff08CRFN\uff09\uff0c\u901a\u8fc7\u5728\u97f3\u9891\u548c\u89c6\u89c9\u6d41\u4e4b\u95f4\u5f15\u5165\u53cc\u5411\u6b8b\u5dee\u4ea4\u4e92\u6765\u5b9e\u73b0\u4e92\u8865\u5efa\u6a21\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u5404\u81ea\u8868\u793a\u7684\u72ec\u7acb\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6b8b\u5dee\u8fde\u63a5\u663e\u5f0f\u5efa\u6a21\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u5e76\u878d\u5165\u7a33\u5b9a\u5316\u6280\u672f\u4ee5\u6539\u5584\u6536\u655b\u6027\u548c\u9c81\u68d2\u6027\uff0c\u533a\u522b\u4e8e\u4f20\u7edf\u7684\u62fc\u63a5\u6216\u6ce8\u610f\u529b\u95e8\u63a7\u65b9\u6cd5\u3002", "result": "\u5728Replica\u548cMatterport3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCRFN\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u878d\u5408\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b9e\u9a8c\u8fd8\u53d1\u73b0\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5dee\u5f02\u5316\u7684\u6a21\u6001\u4f9d\u8d56\u6027\uff0c\u8fd9\u4e00\u73b0\u8c61\u4e3a\u7406\u89e3\u5177\u8eab\u667a\u80fd\u4f53\u7684\u8de8\u6a21\u6001\u534f\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8de8\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u8fd8\u63ed\u793a\u4e86\u5177\u8eab\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u6a21\u6001\u4f9d\u8d56\u5dee\u5f02\uff0c\u4e3a\u7406\u89e3\u591a\u6a21\u6001\u534f\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002CRFN\u7684\u6210\u529f\u8868\u660e\u663e\u5f0f\u5efa\u6a21\u8de8\u6a21\u6001\u6b8b\u5dee\u4ea4\u4e92\u662f\u5b9e\u73b0\u7a33\u5065\u591a\u6a21\u6001\u878d\u5408\u7684\u6709\u6548\u9014\u5f84\uff0c\u4e3a\u672a\u6765\u5177\u8eab\u5bfc\u822a\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.08977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08977", "abs": "https://arxiv.org/abs/2601.08977", "authors": ["Chao Yang", "Haoyuan Zheng", "Yue Ma"], "title": "Thermo-LIO: A Novel Multi-Sensor Integrated System for Structural Health Monitoring", "comment": "27pages,12figures", "summary": "Traditional two-dimensional thermography, despite being non-invasive and useful for defect detection in the construction field, is limited in effectively assessing complex geometries, inaccessible areas, and subsurface defects. This paper introduces Thermo-LIO, a novel multi-sensor system that can enhance Structural Health Monitoring (SHM) by fusing thermal imaging with high-resolution LiDAR. To achieve this, the study first develops a multimodal fusion method combining thermal imaging and LiDAR, enabling precise calibration and synchronization of multimodal data streams to create accurate representations of temperature distributions in buildings. Second, it integrates this fusion approach with LiDAR-Inertial Odometry (LIO), enabling full coverage of large-scale structures and allowing for detailed monitoring of temperature variations and defect detection across inspection cycles. Experimental validations, including case studies on a bridge and a hall building, demonstrate that Thermo-LIO can detect detailed thermal anomalies and structural defects more accurately than traditional methods. The system enhances diagnostic precision, enables real-time processing, and expands inspection coverage, highlighting the crucial role of multimodal sensor integration in advancing SHM methodologies for large-scale civil infrastructure.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faThermo-LIO\uff0c\u4e00\u79cd\u7ed3\u5408\u70ed\u6210\u50cf\u4e0e\u9ad8\u5206\u8fa8\u7387LiDAR\u7684\u591a\u4f20\u611f\u5668\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u4e0eLiDAR\u60ef\u6027\u91cc\u7a0b\u8ba1\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u571f\u6728\u57fa\u7840\u8bbe\u65bd\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u7684\u7f3a\u9677\u68c0\u6d4b\u7cbe\u5ea6\u4e0e\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u4f20\u7edf\u4e8c\u7ef4\u70ed\u6210\u50cf\u6280\u672f\u5728\u5efa\u7b51\u9886\u57df\u7f3a\u9677\u68c0\u6d4b\u4e2d\u867d\u5177\u975e\u4fb5\u5165\u6027\u4f18\u52bf\uff0c\u4f46\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u3001\u4e0d\u53ef\u8fbe\u533a\u57df\u53ca\u6b21\u8868\u9762\u7f3a\u9677\uff0c\u9650\u5236\u4e86\u5176\u5728\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u4e2d\u7684\u5168\u9762\u5e94\u7528\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u70ed\u6210\u50cf\u4e0eLiDAR\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u5b9e\u73b0\u7cbe\u786e\u6821\u51c6\u4e0e\u540c\u6b65\u6570\u636e\u6d41\u4ee5\u6784\u5efa\u5efa\u7b51\u7269\u6e29\u5ea6\u5206\u5e03\u7cbe\u786e\u8868\u5f81\uff1b\u8fdb\u4e00\u6b65\u5c06\u8be5\u878d\u5408\u65b9\u6cd5\u4e0eLiDAR\u60ef\u6027\u91cc\u7a0b\u8ba1\u96c6\u6210\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u7ed3\u6784\u7684\u5168\u8986\u76d6\u76d1\u6d4b\u4e0e\u6e29\u5ea6\u53d8\u5316\u8ffd\u8e2a\u3002", "result": "\u5728\u6865\u6881\u548c\u5385\u5802\u5efa\u7b51\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cThermo-LIO\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u68c0\u6d4b\u8be6\u7ec6\u70ed\u5f02\u5e38\u4e0e\u7ed3\u6784\u7f3a\u9677\uff0c\u7cfb\u7edf\u63d0\u5347\u4e86\u8bca\u65ad\u7cbe\u5ea6\uff0c\u652f\u6301\u5b9e\u65f6\u5904\u7406\u5e76\u6269\u5c55\u4e86\u68c0\u6d4b\u8986\u76d6\u8303\u56f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u4f20\u611f\u5668\u96c6\u6210\u5728\u63a8\u8fdb\u5927\u578b\u571f\u6728\u57fa\u7840\u8bbe\u65bd\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u65b9\u6cd5\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u7cfb\u7edf\u901a\u8fc7\u878d\u5408\u70ed\u6210\u50cf\u4e0eLiDAR\u6280\u672f\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u3001\u7cbe\u786e\u7684\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u81ea\u52a8\u5316\u76d1\u6d4b\u7cfb\u7edf\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.09012", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09012", "abs": "https://arxiv.org/abs/2601.09012", "authors": ["Mara Finkelstein", "Isaac Caswell", "Tobias Domhan", "Jan-Thorsten Peter", "Juraj Juraska", "Parker Riley", "Daniel Deutsch", "Cole Dilanni", "Colin Cherry", "Eleftheria Briakou", "Elizabeth Nielsen", "Jiaming Luo", "Kat Black", "Ryan Mullins", "Sweta Agrawal", "Wenda Xu", "Erin Kats", "Stephane Jaskiewicz", "Markus Freitag", "David Vilar"], "title": "TranslateGemma Technical Report", "comment": null, "summary": "We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TranslateGemma\uff0c\u4e00\u4e2a\u57fa\u4e8eGemma 3\u57fa\u7840\u6a21\u578b\u7684\u5f00\u6e90\u673a\u5668\u7ffb\u8bd1\u5957\u4ef6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86Gemma 3\u7684\u591a\u8bed\u8a00\u7ffb\u8bd1\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u589e\u5f3aGemma 3\u57fa\u7840\u6a21\u578b\u56fa\u6709\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u4f7f\u5176\u4e13\u95e8\u9002\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\uff0c\u901a\u8fc7\u5f00\u53d1\u5f00\u6e90\u7ffb\u8bd1\u6a21\u578b\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u5f3a\u5927\u4e14\u53ef\u9002\u5e94\u7684\u5de5\u5177\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528\u5927\u89c4\u6a21\u5408\u6210\u5e76\u884c\u6570\u636e\u548c\u4eba\u5de5\u7ffb\u8bd1\u5e76\u884c\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u5229\u7528MetricX-QE\u548cAutoMQM\u7b49\u5956\u52b1\u6a21\u578b\u96c6\u6210\u6765\u4f18\u5316\u7ffb\u8bd1\u8d28\u91cf\u3002", "result": "\u5728WMT25\u6d4b\u8bd5\u96c6\u768410\u4e2a\u8bed\u8a00\u5bf9\u4e0a\u8fdb\u884c\u4eba\u5de5\u8bc4\u4f30\uff0c\u5728WMT24++\u57fa\u51c6\u768455\u4e2a\u8bed\u8a00\u5bf9\u4e0a\u8fdb\u884c\u81ea\u52a8\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aTranslateGemma\u5728\u6240\u6709\u6a21\u578b\u5c3a\u5bf8\u4e0a\u90fd\u6bd4\u57fa\u7ebfGemma 3\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347\uff0c\u8f83\u5c0f\u6a21\u578b\u5e38\u80fd\u8fbe\u5230\u8f83\u5927\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u540c\u65f6\u5728Vistra\u56fe\u50cf\u7ffb\u8bd1\u57fa\u51c6\u4e0a\u5c55\u73b0\u51fa\u589e\u5f3a\u7684\u591a\u6a21\u6001\u80fd\u529b\u3002", "conclusion": "TranslateGemma\u901a\u8fc7\u4e13\u95e8\u7684\u4e24\u9636\u6bb5\u5fae\u8c03\u6709\u6548\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u7ffb\u8bd1\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8f83\u5c0f\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u6548\u6027\u7684\u540c\u65f6\u80fd\u8fbe\u5230\u8f83\u5927\u6a21\u578b\u7684\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e3a\u673a\u5668\u7ffb\u8bd1\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u591a\u6a21\u6001\u80fd\u529b\u3002"}}
{"id": "2601.09111", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09111", "abs": "https://arxiv.org/abs/2601.09111", "authors": ["Yang Li", "Aming Wu", "Zihao Zhang", "Yahong Han"], "title": "Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning", "comment": null, "summary": "Vision-Language Navigation aims to enable agents to navigate to a target location based on language instructions. Traditional VLN often follows a close-set assumption, i.e., training and test data share the same style of the input images and instructions. However, the real world is open and filled with various unseen environments, posing enormous difficulties for close-set methods. To this end, we focus on the General Scene Adaptation (GSA-VLN) task, aiming to learn generalized navigation ability by introducing diverse environments and inconsistent intructions.Towards this task, when facing unseen environments and instructions, the challenge mainly lies in how to enable the agent to dynamically produce generalized strategies during the navigation process. Recent research indicates that by means of fast and slow cognition systems, human beings could generate stable policies, which strengthen their adaptation for open world. Inspired by this idea, we propose the slow4fast-VLN, establishing a dynamic interactive fast-slow reasoning framework. The fast-reasoning module, an end-to-end strategy network, outputs actions via real-time input. It accumulates execution records in a history repository to build memory. The slow-reasoning module analyze the memories generated by the fast-reasoning module. Through deep reflection, it extracts experiences that enhance the generalization ability of decision-making. These experiences are structurally stored and used to continuously optimize the fast-reasoning module. Unlike traditional methods that treat fast-slow reasoning as independent mechanisms, our framework enables fast-slow interaction. By leveraging the experiences from slow reasoning. This interaction allows the system to continuously adapt and efficiently execute navigation tasks when facing unseen scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faslow4fast-VLN\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u7acb\u52a8\u6001\u4ea4\u4e92\u7684\u5feb\u6162\u63a8\u7406\u673a\u5236\u6765\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7684\u6cdb\u5316\u573a\u666f\u9002\u5e94\u95ee\u9898\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u9762\u5bf9\u672a\u89c1\u73af\u5883\u548c\u6307\u4ee4\u65f6\u52a8\u6001\u751f\u6210\u6cdb\u5316\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u9075\u5faa\u95ed\u96c6\u5047\u8bbe\uff0c\u5373\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u5171\u4eab\u76f8\u540c\u7684\u8f93\u5165\u56fe\u50cf\u548c\u6307\u4ee4\u98ce\u683c\uff0c\u7136\u800c\u73b0\u5b9e\u4e16\u754c\u662f\u5f00\u653e\u7684\u4e14\u5145\u6ee1\u5404\u79cd\u672a\u89c1\u73af\u5883\uff0c\u8fd9\u5bf9\u95ed\u96c6\u65b9\u6cd5\u6784\u6210\u4e86\u5de8\u5927\u6311\u6218\u3002\u672c\u6587\u805a\u7126\u4e8e\u6cdb\u5316\u573a\u666f\u9002\u5e94\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u591a\u6837\u5316\u73af\u5883\u548c\u4e0d\u4e00\u81f4\u6307\u4ee4\u6765\u5b66\u4e60\u6cdb\u5316\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u5982\u4f55\u4f7f\u667a\u80fd\u4f53\u5728\u9762\u5bf9\u672a\u89c1\u73af\u5883\u548c\u6307\u4ee4\u65f6\u52a8\u6001\u4ea7\u751f\u6cdb\u5316\u7b56\u7565\u3002", "method": "\u672c\u6587\u63d0\u51faslow4fast-VLN\u6846\u67b6\uff0c\u5efa\u7acb\u52a8\u6001\u4ea4\u4e92\u7684\u5feb\u6162\u63a8\u7406\u673a\u5236\u3002\u5feb\u901f\u63a8\u7406\u6a21\u5757\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7b56\u7565\u7f51\u7edc\uff0c\u901a\u8fc7\u5b9e\u65f6\u8f93\u5165\u8f93\u51fa\u52a8\u4f5c\uff0c\u5e76\u5728\u5386\u53f2\u5b58\u50a8\u5e93\u4e2d\u79ef\u7d2f\u6267\u884c\u8bb0\u5f55\u4ee5\u6784\u5efa\u8bb0\u5fc6\u3002\u6162\u901f\u63a8\u7406\u6a21\u5757\u5206\u6790\u5feb\u901f\u63a8\u7406\u6a21\u5757\u751f\u6210\u7684\u8bb0\u5fc6\uff0c\u901a\u8fc7\u6df1\u5ea6\u53cd\u601d\u63d0\u53d6\u589e\u5f3a\u51b3\u7b56\u6cdb\u5316\u80fd\u529b\u7684\u7ecf\u9a8c\uff0c\u8fd9\u4e9b\u7ecf\u9a8c\u88ab\u7ed3\u6784\u5316\u5b58\u50a8\u5e76\u7528\u4e8e\u6301\u7eed\u4f18\u5316\u5feb\u901f\u63a8\u7406\u6a21\u5757\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5c06\u5feb\u6162\u63a8\u7406\u89c6\u4e3a\u72ec\u7acb\u673a\u5236\u4e0d\u540c\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5feb\u6162\u4ea4\u4e92\uff0c\u5229\u7528\u6162\u901f\u63a8\u7406\u7684\u7ecf\u9a8c\u4f7f\u7cfb\u7edf\u80fd\u591f\u6301\u7eed\u9002\u5e94\u5e76\u5728\u9762\u5bf9\u672a\u89c1\u573a\u666f\u65f6\u9ad8\u6548\u6267\u884c\u5bfc\u822a\u4efb\u52a1\u3002", "result": "\u867d\u7136\u6458\u8981\u4e2d\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u6027\u80fd\u6307\u6807\u548c\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u4f46\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7684\u6cdb\u5316\u573a\u666f\u9002\u5e94\u95ee\u9898\u3002\u901a\u8fc7\u5feb\u6162\u63a8\u7406\u7684\u52a8\u6001\u4ea4\u4e92\u673a\u5236\uff0c\u7cfb\u7edf\u80fd\u591f\u4ece\u6267\u884c\u8bb0\u5f55\u4e2d\u63d0\u53d6\u6cdb\u5316\u7ecf\u9a8c\u5e76\u6301\u7eed\u4f18\u5316\u5bfc\u822a\u7b56\u7565\uff0c\u4ece\u800c\u5728\u9762\u5bf9\u672a\u89c1\u73af\u5883\u548c\u4e0d\u4e00\u81f4\u6307\u4ee4\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u7684\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\u63d0\u51fa\u4e86\u4e00\u4e2a\u53d7\u4eba\u7c7b\u8ba4\u77e5\u7cfb\u7edf\u542f\u53d1\u7684\u52a8\u6001\u4ea4\u4e92\u5feb\u6162\u63a8\u7406\u6846\u67b6\uff0c\u4e3a\u5f00\u653e\u4e16\u754c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u5c06\u5feb\u901f\u6267\u884c\u4e0e\u6162\u901f\u53cd\u601d\u76f8\u7ed3\u5408\uff0c\u7cfb\u7edf\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u548c\u9002\u5e94\u672a\u89c1\u573a\u666f\uff0c\u8fd9\u4e3a\u6784\u5efa\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u81ea\u4e3b\u5bfc\u822a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u91cd\u8981\u601d\u8def\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u66f4\u590d\u6742\u5f00\u653e\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.09066", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09066", "abs": "https://arxiv.org/abs/2601.09066", "authors": ["Donghoon Shin", "Sejung Lee", "Soonmin Bae", "Hwijung Ryu", "Changwon Ok", "Hoyoun Jung", "Hyesung Ji", "Jeehyun Lim", "Jehoon Lee", "Ji-Eun Han", "Jisoo Baik", "Mihyeon Kim", "Riwoo Chung", "Seongmin Lee", "Wonjae Park", "Yoonseok Heo", "Youngkyung Seo", "Seyoun Won", "Boeun Kim", "Cheolhun Heo", "Eunkyeong Lee", "Honghee Lee", "Hyeongju Ju", "Hyeontae Seo", "Jeongyong Shim", "Jisoo Lee", "Junseok Koh", "Junwoo Kim", "Minho Lee", "Minji Kang", "Minju Kim", "Sangha Nam", "Seongheum Park", "Taehyeong Kim", "Euijai Ahn", "Hong Seok Jeung", "Jisu Shin", "Jiyeon Kim", "Seonyeong Song", "Seung Hyun Kong", "Sukjin Hong", "Taeyang Yun", "Yu-Seon Kim", "A-Hyun Lee", "Chae-Jeong Lee", "Hye-Won Yu", "Ji-Hyun Ahn", "Song-Yeon Kim", "Sun-Woo Jung", "Eunju Kim", "Eunji Ha", "Jinwoo Baek", "Yun-ji Lee", "Wanjin Park", "Jeong Yeop Kim", "Eun Mi Kim", "Hyoung Jun Park", "Jung Won Yoon", "Min Sung Noh", "Myung Gyo Oh", "Wongyoung Lee", "Yun Jin Park", "Young S. Kwon", "Hyun Keun Kim", "Jieun Lee", "YeoJoo Park"], "title": "Mi:dm 2.0 Korea-centric Bilingual Language Models", "comment": null, "summary": "We introduce Mi:dm 2.0, a bilingual large language model (LLM) specifically engineered to advance Korea-centric AI. This model goes beyond Korean text processing by integrating the values, reasoning patterns, and commonsense knowledge inherent to Korean society, enabling nuanced understanding of cultural contexts, emotional subtleties, and real-world scenarios to generate reliable and culturally appropriate responses. To address limitations of existing LLMs, often caused by insufficient or low-quality Korean data and lack of cultural alignment, Mi:dm 2.0 emphasizes robust data quality through a comprehensive pipeline that includes proprietary data cleansing, high-quality synthetic data generation, strategic data mixing with curriculum learning, and a custom Korean-optimized tokenizer to improve efficiency and coverage. To realize this vision, we offer two complementary configurations: Mi:dm 2.0 Base (11.5B parameters), built with a depth-up scaling strategy for general-purpose use, and Mi:dm 2.0 Mini (2.3B parameters), optimized for resource-constrained environments and specialized tasks. Mi:dm 2.0 achieves state-of-the-art performance on Korean-specific benchmarks, with top-tier zero-shot results on KMMLU and strong internal evaluation results across language, humanities, and social science tasks. The Mi:dm 2.0 lineup is released under the MIT license to support extensive research and commercial use. By offering accessible and high-performance Korea-centric LLMs, KT aims to accelerate AI adoption across Korean industries, public services, and education, strengthen the Korean AI developer community, and lay the groundwork for the broader vision of K-intelligence. Our models are available at https://huggingface.co/K-intelligence. For technical inquiries, please contact midm-llm@kt.com.", "AI": {"tldr": "Mi:dm 2.0 \u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u63a8\u8fdb\u97e9\u56fd\u4e2d\u5fc3AI\u8bbe\u8ba1\u7684\u53cc\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u97e9\u56fd\u793e\u4f1a\u7684\u4ef7\u503c\u89c2\u3001\u63a8\u7406\u6a21\u5f0f\u548c\u5e38\u8bc6\u77e5\u8bc6\uff0c\u5728\u97e9\u56fd\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u7840\u7248\u548c\u8ff7\u4f60\u7248\u4e24\u79cd\u914d\u7f6e\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u97e9\u56fd\u76f8\u5173\u5185\u5bb9\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u6e90\u4e8e\u97e9\u56fd\u6570\u636e\u4e0d\u8db3\u6216\u8d28\u91cf\u4f4e\u4e0b\u4ee5\u53ca\u7f3a\u4e4f\u6587\u5316\u5bf9\u9f50\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u7406\u89e3\u97e9\u56fd\u6587\u5316\u80cc\u666f\u3001\u60c5\u611f\u7ec6\u5fae\u5dee\u522b\u548c\u73b0\u5b9e\u573a\u666f\uff0c\u65e0\u6cd5\u751f\u6210\u53ef\u9760\u4e14\u6587\u5316\u9002\u5b9c\u7684\u54cd\u5e94\u3002", "method": "\u8be5\u6a21\u578b\u91c7\u7528\u5168\u9762\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u4e13\u6709\u6570\u636e\u6e05\u6d17\u3001\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u7684\u7b56\u7565\u6027\u6570\u636e\u6df7\u5408\uff0c\u4ee5\u53ca\u5b9a\u5236\u7684\u97e9\u56fd\u4f18\u5316\u5206\u8bcd\u5668\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u8986\u76d6\u8303\u56f4\uff1b\u6a21\u578b\u63d0\u4f9b\u4e24\u79cd\u914d\u7f6e\uff1a\u91c7\u7528\u6df1\u5ea6\u6269\u5c55\u7b56\u7565\u7684Mi:dm 2.0 Base\uff08115\u4ebf\u53c2\u6570\uff09\u9002\u7528\u4e8e\u901a\u7528\u573a\u666f\uff0c\u4ee5\u53ca\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u73af\u5883\u548c\u4e13\u95e8\u4efb\u52a1\u4f18\u5316\u7684Mi:dm 2.0 Mini\uff0823\u4ebf\u53c2\u6570\uff09\u3002", "result": "Mi:dm 2.0 \u5728\u97e9\u56fd\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728KMMLU\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u9876\u7ea7\u7684\u96f6\u6837\u672c\u7ed3\u679c\uff0c\u5e76\u5728\u8bed\u8a00\u3001\u4eba\u6587\u548c\u793e\u4f1a\u79d1\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5185\u90e8\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u53ef\u8bbf\u95ee\u4e14\u9ad8\u6027\u80fd\u7684\u97e9\u56fd\u4e2d\u5fc3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u52a0\u901f\u97e9\u56fd\u5404\u884c\u4e1a\u3001\u516c\u5171\u670d\u52a1\u548c\u6559\u80b2\u9886\u57df\u7684AI\u91c7\u7528\uff0c\u52a0\u5f3a\u97e9\u56fdAI\u5f00\u53d1\u8005\u793e\u533a\uff0c\u5e76\u4e3a\u66f4\u5e7f\u6cdb\u7684K-intelligence\u613f\u666f\u5960\u5b9a\u57fa\u7840\uff1b\u6a21\u578b\u4ee5MIT\u8bb8\u53ef\u8bc1\u53d1\u5e03\u652f\u6301\u5e7f\u6cdb\u7684\u7814\u7a76\u548c\u5546\u4e1a\u4f7f\u7528\u3002"}}
{"id": "2601.09105", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09105", "abs": "https://arxiv.org/abs/2601.09105", "authors": ["Wenbin Li", "Jingling Wu", "Xiaoyong Lin. Jing Chen", "Cong Chen"], "title": "AviationLMM: A Large Multimodal Foundation Model for Civil Aviation", "comment": "Accepted by 2025 7th International Conference on Interdisciplinary Computer Science and Engineering (ICICSE 2025) conference, Chongqing, China; 9 pages,1 figure,5 tables", "summary": "Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AviationLMM\u613f\u666f\uff0c\u8fd9\u662f\u4e00\u4e2a\u9762\u5411\u6c11\u822a\u9886\u57df\u7684\u5927\u578b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u7edf\u4e00\u6c11\u822a\u5f02\u6784\u6570\u636e\u6d41\uff0c\u5b9e\u73b0\u7406\u89e3\u3001\u63a8\u7406\u3001\u751f\u6210\u548c\u667a\u80fd\u4f53\u5e94\u7528\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709AI\u89e3\u51b3\u65b9\u6848\u5728\u6c11\u822a\u4e2d\u5b64\u7acb\u3001\u5355\u6a21\u6001\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6c11\u822a\u73b0\u6709AI\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u5b64\u7acb\u548c\u5355\u6a21\u6001\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u8bed\u97f3\u901a\u4fe1\u3001\u96f7\u8fbe\u8f68\u8ff9\u3001\u4f20\u611f\u5668\u6d41\u548c\u6587\u672c\u62a5\u544a\u7b49\u5f02\u6784\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u6001\u52bf\u611f\u77e5\u3001\u9002\u5e94\u6027\u548c\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u80fd\u529b\uff0c\u963b\u788d\u4e86\u6c11\u822a\u5b89\u5168\u3001\u6548\u7387\u548c\u5ba2\u6237\u6ee1\u610f\u5ea6\u7684\u63d0\u5347\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86AviationLMM\u7684\u6a21\u578b\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u80fd\u591f\u5904\u7406\u7a7a-\u5730\u8bed\u97f3\u3001\u76d1\u89c6\u6570\u636e\u3001\u673a\u8f7d\u9065\u6d4b\u3001\u89c6\u9891\u548c\u7ed3\u6784\u5316\u6587\u672c\u7b49\u591a\u6a21\u6001\u8f93\u5165\uff0c\u6267\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u878d\u5408\uff0c\u5e76\u751f\u6210\u4ece\u6001\u52bf\u6458\u8981\u3001\u98ce\u9669\u9884\u8b66\u5230\u9884\u6d4b\u6027\u8bca\u65ad\u548c\u591a\u6a21\u6001\u4e8b\u4ef6\u91cd\u5efa\u7684\u7075\u6d3b\u8f93\u51fa\u3002", "result": "\u8bba\u6587\u672a\u62a5\u544a\u5177\u4f53\u7684\u6027\u80fd\u6307\u6807\u6216\u5b9e\u9a8c\u7ed3\u679c\uff0c\u800c\u662f\u63d0\u51fa\u4e86\u4e00\u4e2a\u7814\u7a76\u613f\u666f\u548c\u6846\u67b6\uff0c\u5e76\u8bc6\u522b\u4e86\u5b9e\u73b0\u8be5\u613f\u666f\u9700\u8981\u89e3\u51b3\u7684\u5173\u952e\u7814\u7a76\u673a\u4f1a\uff0c\u5305\u62ec\u6570\u636e\u83b7\u53d6\u3001\u5bf9\u9f50\u4e0e\u878d\u5408\u3001\u9884\u8bad\u7ec3\u3001\u63a8\u7406\u3001\u53ef\u4fe1\u6027\u3001\u9690\u79c1\u3001\u6a21\u6001\u7f3a\u5931\u9c81\u68d2\u6027\u548c\u5408\u6210\u573a\u666f\u751f\u6210\u7b49\u65b9\u9762\u3002", "conclusion": "\u901a\u8fc7\u9610\u8ff0AviationLMM\u7684\u8bbe\u8ba1\u548c\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u63a8\u52a8\u6c11\u822a\u57fa\u7840\u6a21\u578b\u7684\u8fdb\u5c55\uff0c\u5e76\u50ac\u5316\u534f\u8c03\u7684\u7814\u7a76\u52aa\u529b\uff0c\u4ee5\u6784\u5efa\u4e00\u4e2a\u96c6\u6210\u3001\u53ef\u4fe1\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u6c11\u822aAI\u751f\u6001\u7cfb\u7edf\uff0c\u4e3a\u672a\u6765\u6c11\u822aAI\u53d1\u5c55\u63d0\u4f9b\u660e\u786e\u7684\u7814\u7a76\u65b9\u5411\u548c\u6846\u67b6\u3002"}}
{"id": "2601.09116", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09116", "abs": "https://arxiv.org/abs/2601.09116", "authors": ["Haoyan Gong", "Hongbin Liu"], "title": "LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models", "comment": null, "summary": "Real-world License Plate Recognition (LPR) faces significant challenges from severe degradations such as motion blur, low resolution, and complex illumination. The prevailing \"restoration-then-recognition\" two-stage paradigm suffers from a fundamental flaw: the pixel-level optimization objectives of image restoration models are misaligned with the semantic goals of character recognition, leading to artifact interference and error accumulation. While Vision-Language Models (VLMs) have demonstrated powerful general capabilities, they lack explicit structural modeling for license plate character sequences (e.g., fixed length, specific order). To address this, we propose an end-to-end structure-aware multimodal reasoning framework based on Qwen3-VL. The core innovation lies in the Character-Aware Multimodal Reasoning Module (CMRM), which introduces a set of learnable Character Slot Queries. Through a cross-attention mechanism, these queries actively retrieve fine-grained evidence corresponding to character positions from visual features. Subsequently, we inject these character-aware representations back into the visual tokens via residual modulation, enabling the language model to perform autoregressive generation based on explicit structural priors. Furthermore, combined with the LoRA parameter-efficient fine-tuning strategy, the model achieves domain adaptation while retaining the generalization capabilities of the large model. Extensive experiments on both synthetic and real-world severely degraded datasets demonstrate that our method significantly outperforms existing restoration-recognition combinations and general VLMs, validating the superiority of incorporating structured reasoning into large models for low-quality text recognition tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eQwen3-VL\u7684\u7aef\u5230\u7aef\u7ed3\u6784\u611f\u77e5\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5b57\u7b26\u611f\u77e5\u591a\u6a21\u6001\u63a8\u7406\u6a21\u5757\u548c\u5b57\u7b26\u69fd\u67e5\u8be2\uff0c\u89e3\u51b3\u4e86\u8f66\u724c\u8bc6\u522b\u4e2d\u56fe\u50cf\u6062\u590d\u4e0e\u5b57\u7b26\u8bc6\u522b\u76ee\u6807\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e25\u91cd\u9000\u5316\u573a\u666f\u4e0b\u7684\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u8f66\u724c\u8bc6\u522b\u9762\u4e34\u8fd0\u52a8\u6a21\u7cca\u3001\u4f4e\u5206\u8fa8\u7387\u548c\u590d\u6742\u5149\u7167\u7b49\u4e25\u91cd\u9000\u5316\u6311\u6218\uff0c\u4f20\u7edf\u7684\"\u6062\u590d-\u518d\u8bc6\u522b\"\u4e24\u9636\u6bb5\u8303\u5f0f\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff1a\u56fe\u50cf\u6062\u590d\u6a21\u578b\u7684\u50cf\u7d20\u7ea7\u4f18\u5316\u76ee\u6807\u4e0e\u5b57\u7b26\u8bc6\u522b\u7684\u8bed\u4e49\u76ee\u6807\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u4f2a\u5f71\u5e72\u6270\u548c\u8bef\u5dee\u7d2f\u79ef\u3002\u540c\u65f6\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u5907\u5f3a\u5927\u901a\u7528\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8f66\u724c\u5b57\u7b26\u5e8f\u5217\u56fa\u5b9a\u957f\u5ea6\u548c\u7279\u5b9a\u987a\u5e8f\u7b49\u663e\u5f0f\u7ed3\u6784\u5efa\u6a21\u3002", "method": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8eQwen3-VL\u7684\u7aef\u5230\u7aef\u7ed3\u6784\u611f\u77e5\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u5b57\u7b26\u611f\u77e5\u591a\u6a21\u6001\u63a8\u7406\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5f15\u5165\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u5b57\u7b26\u69fd\u67e5\u8be2\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u4ece\u89c6\u89c9\u7279\u5f81\u4e2d\u4e3b\u52a8\u68c0\u7d22\u5bf9\u5e94\u5b57\u7b26\u4f4d\u7f6e\u7684\u7ec6\u7c92\u5ea6\u8bc1\u636e\u3002\u968f\u540e\u901a\u8fc7\u6b8b\u5dee\u8c03\u5236\u5c06\u8fd9\u4e9b\u5b57\u7b26\u611f\u77e5\u8868\u793a\u6ce8\u5165\u89c6\u89c9\u6807\u8bb0\uff0c\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u57fa\u4e8e\u663e\u5f0f\u7ed3\u6784\u5148\u9a8c\u8fdb\u884c\u81ea\u56de\u5f52\u751f\u6210\u3002\u7ed3\u5408LoRA\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff0c\u6a21\u578b\u5728\u4fdd\u7559\u5927\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u9886\u57df\u9002\u5e94\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u4e25\u91cd\u9000\u5316\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6062\u590d-\u8bc6\u522b\u7ec4\u5408\u65b9\u6cd5\u548c\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5c06\u7ed3\u6784\u5316\u63a8\u7406\u878d\u5165\u5927\u6a21\u578b\u5bf9\u4e8e\u4f4e\u8d28\u91cf\u6587\u672c\u8bc6\u522b\u4efb\u52a1\u7684\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u7ed3\u6784\u5efa\u6a21\u548c\u5b57\u7b26\u611f\u77e5\u63a8\u7406\u673a\u5236\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u4e24\u9636\u6bb5\u65b9\u6cd5\u7684\u8bed\u4e49\u76ee\u6807\u4e0d\u4e00\u81f4\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u8d28\u91cf\u6587\u672c\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u4e0e\u9886\u57df\u7279\u5b9a\u7ed3\u6784\u5148\u9a8c\u76f8\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7c7b\u4f3c\u7684\u7ed3\u6784\u5316\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2601.09119", "categories": ["cs.CL", "econ.GN"], "pdf": "https://arxiv.org/pdf/2601.09119", "abs": "https://arxiv.org/abs/2601.09119", "authors": ["Yongming Sun"], "title": "Contrastive Bi-Encoder Models for Multi-Label Skill Extraction: Enhancing ESCO Ontology Matching with BERT and Attention Mechanisms", "comment": null, "summary": "Fine-grained labor market analysis increasingly relies on mapping unstructured job advertisements to standardized skill taxonomies such as ESCO. This mapping is naturally formulated as an Extreme Multi-Label Classification (XMLC) problem, but supervised solutions are constrained by the scarcity and cost of large-scale, taxonomy-aligned annotations--especially in non-English settings where job-ad language diverges substantially from formal skill definitions. We propose a zero-shot skill extraction framework that eliminates the need for manually labeled job-ad training data. The framework uses a Large Language Model (LLM) to synthesize training instances from ESCO definitions, and introduces hierarchically constrained multi-skill generation based on ESCO Level-2 categories to improve semantic coherence in multi-label contexts. On top of the synthetic corpus, we train a contrastive bi-encoder that aligns job-ad sentences with ESCO skill descriptions in a shared embedding space; the encoder augments a BERT backbone with BiLSTM and attention pooling to better model long, information-dense requirement statements. An upstream RoBERTa-based binary filter removes non-skill sentences to improve end-to-end precision. Experiments show that (i) hierarchy-conditioned generation improves both fluency and discriminability relative to unconstrained pairing, and (ii) the resulting multi-label model transfers effectively to real-world Chinese job advertisements, achieving strong zero-shot retrieval performance (F1@5 = 0.72) and outperforming TF--IDF and standard BERT baselines. Overall, the proposed pipeline provides a scalable, data-efficient pathway for automated skill coding in labor economics and workforce analytics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6280\u80fd\u63d0\u53d6\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4eceESCO\u5b9a\u4e49\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u5f15\u5165\u5c42\u6b21\u7ea6\u675f\u7684\u591a\u6280\u80fd\u751f\u6210\uff0c\u8bad\u7ec3\u5bf9\u6bd4\u53cc\u7f16\u7801\u5668\u5b9e\u73b0\u65e0\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u6280\u80fd\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e2d\u6587\u62db\u8058\u5e7f\u544a\u4e2d\u7684\u6280\u80fd\u63d0\u53d6\u6027\u80fd\u3002", "motivation": "\u7ec6\u7c92\u5ea6\u52b3\u52a8\u529b\u5e02\u573a\u5206\u6790\u9700\u8981\u5c06\u975e\u7ed3\u6784\u5316\u62db\u8058\u5e7f\u544a\u6620\u5c04\u5230\u6807\u51c6\u5316\u6280\u80fd\u5206\u7c7b\u4f53\u7cfb\u5982ESCO\uff0c\u8fd9\u672c\u8d28\u4e0a\u662f\u6781\u7aef\u591a\u6807\u7b7e\u5206\u7c7b\u95ee\u9898\u3002\u7136\u800c\u76d1\u7763\u89e3\u51b3\u65b9\u6848\u53d7\u5230\u5927\u89c4\u6a21\u3001\u5206\u7c7b\u5bf9\u9f50\u6807\u6ce8\u7a00\u7f3a\u4e14\u6210\u672c\u9ad8\u6602\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u975e\u82f1\u8bed\u73af\u5883\u4e2d\uff0c\u62db\u8058\u5e7f\u544a\u8bed\u8a00\u4e0e\u6b63\u5f0f\u6280\u80fd\u5b9a\u4e49\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4eceESCO\u5b9a\u4e49\u5408\u6210\u8bad\u7ec3\u5b9e\u4f8b\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eESCO\u4e8c\u7ea7\u7c7b\u522b\u7684\u5c42\u6b21\u7ea6\u675f\u591a\u6280\u80fd\u751f\u6210\u4ee5\u63d0\u5347\u591a\u6807\u7b7e\u4e0a\u4e0b\u6587\u4e2d\u7684\u8bed\u4e49\u8fde\u8d2f\u6027\u3002\u5728\u5408\u6210\u8bed\u6599\u4e0a\u8bad\u7ec3\u5bf9\u6bd4\u53cc\u7f16\u7801\u5668\uff0c\u5c06\u62db\u8058\u5e7f\u544a\u53e5\u5b50\u4e0eESCO\u6280\u80fd\u63cf\u8ff0\u5bf9\u9f50\u5230\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\uff1b\u7f16\u7801\u5668\u5728BERT\u9aa8\u5e72\u57fa\u7840\u4e0a\u589e\u52a0BiLSTM\u548c\u6ce8\u610f\u529b\u6c60\u5316\u4ee5\u66f4\u597d\u5efa\u6a21\u957f\u800c\u4fe1\u606f\u5bc6\u96c6\u7684\u9700\u6c42\u9648\u8ff0\u3002\u4e0a\u6e38\u57fa\u4e8eRoBERTa\u7684\u4e8c\u5143\u8fc7\u6ee4\u5668\u79fb\u9664\u975e\u6280\u80fd\u53e5\u5b50\u4ee5\u63d0\u9ad8\u7aef\u5230\u7aef\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5c42\u6b21\u6761\u4ef6\u751f\u6210\u76f8\u6bd4\u65e0\u7ea6\u675f\u914d\u5bf9\u5728\u6d41\u7545\u6027\u548c\u53ef\u533a\u5206\u6027\u4e0a\u5747\u6709\u6539\u5584\uff0c\u6240\u5f97\u591a\u6807\u7b7e\u6a21\u578b\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u4e2d\u6587\u62db\u8058\u5e7f\u544a\uff0c\u5b9e\u73b0\u5f3a\u5927\u7684\u96f6\u6837\u672c\u68c0\u7d22\u6027\u80fd\uff08F1@5 = 0.72\uff09\uff0c\u4f18\u4e8eTF-IDF\u548c\u6807\u51c6BERT\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6d41\u6c34\u7ebf\u4e3a\u52b3\u52a8\u7ecf\u6d4e\u5b66\u548c\u52b3\u52a8\u529b\u5206\u6790\u4e2d\u7684\u81ea\u52a8\u5316\u6280\u80fd\u7f16\u7801\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u6570\u636e\u9ad8\u6548\u7684\u9014\u5f84\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u65b9\u6cd5\u514b\u670d\u4e86\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u7279\u522b\u9002\u7528\u4e8e\u975e\u82f1\u8bed\u73af\u5883\u4e0b\u7684\u6280\u80fd\u63d0\u53d6\u4efb\u52a1\u3002"}}
{"id": "2601.09113", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09113", "abs": "https://arxiv.org/abs/2601.09113", "authors": ["Zixia Jia", "Jiaqi Li", "Yipeng Kang", "Yuxuan Wang", "Tong Wu", "Quansen Wang", "Xiaobo Wang", "Shuyi Zhang", "Junzhe Shen", "Qing Li", "Siyuan Qi", "Yitao Liang", "Di He", "Zilong Zheng", "Song-Chun Zhu"], "title": "The AI Hippocampus: How Far are We From Human Memory?", "comment": null, "summary": "Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u673a\u5236\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6db5\u76d6\u9690\u5f0f\u3001\u663e\u5f0f\u548c\u667a\u80fd\u4f53\u8bb0\u5fc6\u8303\u5f0f\u7684\u7ed3\u6784\u5316\u5206\u7c7b\u4f53\u7cfb\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u67b6\u6784\u8fdb\u5c55\u3001\u57fa\u51c6\u4efb\u52a1\u548c\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u9759\u6001\u9884\u6d4b\u5668\u5411\u5177\u5907\u6301\u7eed\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u63a8\u7406\u80fd\u529b\u7684\u4ea4\u4e92\u7cfb\u7edf\u6f14\u8fdb\uff0c\u8bb0\u5fc6\u673a\u5236\u5df2\u6210\u4e3a\u5176\u67b6\u6784\u548c\u529f\u80fd\u53d1\u5c55\u7684\u6838\u5fc3\u4e3b\u9898\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8bb0\u5fc6\u5728LLMs\u548cMLLMs\u4e2d\u4f5c\u7528\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u7ec4\u7ec7\u76f8\u5173\u6587\u732e\u5e76\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u8bb0\u5fc6\u5206\u7c7b\u4f53\u7cfb\uff0c\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u8303\u5f0f\uff1a\u9690\u5f0f\u8bb0\u5fc6\u6307\u9884\u8bad\u7ec3transformer\u5185\u90e8\u53c2\u6570\u4e2d\u5d4c\u5165\u7684\u77e5\u8bc6\uff0c\u6d89\u53ca\u8bb0\u5fc6\u5316\u3001\u5173\u8054\u68c0\u7d22\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff1b\u663e\u5f0f\u8bb0\u5fc6\u6d89\u53ca\u5916\u90e8\u5b58\u50a8\u548c\u68c0\u7d22\u7ec4\u4ef6\uff0c\u5305\u62ec\u6587\u672c\u8bed\u6599\u5e93\u3001\u7a20\u5bc6\u5411\u91cf\u548c\u56fe\u7ed3\u6784\u7b49\u52a8\u6001\u53ef\u67e5\u8be2\u77e5\u8bc6\u8868\u793a\uff1b\u667a\u80fd\u4f53\u8bb0\u5fc6\u5219\u5173\u6ce8\u81ea\u4e3b\u667a\u80fd\u4f53\u4e2d\u7684\u6301\u4e45\u6027\u3001\u65f6\u95f4\u6269\u5c55\u8bb0\u5fc6\u7ed3\u6784\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u957f\u671f\u89c4\u5212\u3001\u81ea\u4e00\u81f4\u6027\u548c\u534f\u4f5c\u884c\u4e3a\u3002", "result": "\u8be5\u7efc\u8ff0\u7cfb\u7edf\u68b3\u7406\u4e86\u8bb0\u5fc6\u673a\u5236\u5728LLMs\u548cMLLMs\u4e2d\u7684\u5173\u952e\u67b6\u6784\u8fdb\u5c55\u3001\u57fa\u51c6\u4efb\u52a1\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u591a\u6a21\u6001\u573a\u666f\u4e0b\u8de8\u89c6\u89c9\u3001\u8bed\u8a00\u3001\u97f3\u9891\u548c\u52a8\u4f5c\u6a21\u6001\u7684\u8fde\u8d2f\u6027\u9700\u6c42\uff0c\u5e76\u8bc6\u522b\u4e86\u8bb0\u5fc6\u5bb9\u91cf\u3001\u5bf9\u9f50\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u8de8\u7cfb\u7edf\u4e92\u64cd\u4f5c\u6027\u7b49\u6838\u5fc3\u6311\u6218\u3002", "conclusion": "\u8bb0\u5fc6\u673a\u5236\u5bf9\u4e8e\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3001\u9002\u5e94\u6027\u548c\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u5177\u6709\u57fa\u7840\u6027\u4f5c\u7528\uff0c\u8be5\u7efc\u8ff0\u5efa\u7acb\u7684\u5206\u7c7b\u6846\u67b6\u4e3a\u7406\u89e3\u4e0d\u540c\u8bb0\u5fc6\u8303\u5f0f\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89c6\u89d2\uff0c\u540c\u65f6\u6307\u51fa\u7684\u5f00\u653e\u6311\u6218\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u81ea\u4e3b\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u8bb0\u5fc6\u96c6\u6210\u65b9\u9762\u3002"}}
{"id": "2601.09118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09118", "abs": "https://arxiv.org/abs/2601.09118", "authors": ["Jackie Alex", "Guoqiang Huan"], "title": "LPCAN: Lightweight Pyramid Cross-Attention Network for Rail Surface Defect Detection Using RGB-D Data", "comment": null, "summary": "This paper addresses the limitations of current vision-based rail defect detection methods, including high computational complexity, excessive parameter counts, and suboptimal accuracy. We propose a Lightweight Pyramid Cross-Attention Network (LPCANet) that leverages RGB-D data for efficient and accurate defect identification. The architecture integrates MobileNetv2 as a backbone for RGB feature extraction with a lightweight pyramid module (LPM) for depth processing, coupled with a cross-attention mechanism (CAM) for multimodal fusion and a spatial feature extractor (SFE) for enhanced structural analysis. Comprehensive evaluations on three unsupervised RGB-D rail datasets (NEU-RSDDS-AUG, RSDD-TYPE1, RSDD-TYPE2) demonstrate that LPCANet achieves state-of-the-art performance with only 9.90 million parameters, 2.50 G FLOPs, and 162.60 fps inference speed. Compared to 18 existing methods, LPCANet shows significant improvements, including +1.48\\% in $S_\u03b1$, +0.86\\% in IOU, and +1.77\\% in MAE over the best-performing baseline. Ablation studies confirm the critical roles of CAM and SFE, while experiments on non-rail datasets (DAGM2007, MT, Kolektor-SDD2) validate its generalization capability. The proposed framework effectively bridges traditional and deep learning approaches, offering substantial practical value for industrial defect inspection. Future work will focus on further model compression for real-time deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u91d1\u5b57\u5854\u4ea4\u53c9\u6ce8\u610f\u529b\u7f51\u7edc\uff08LPCANet\uff09\uff0c\u5229\u7528RGB-D\u6570\u636e\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u8f68\u9053\u7f3a\u9677\u68c0\u6d4b\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u7684\u8f68\u9053\u7f3a\u9677\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u53c2\u6570\u91cf\u5927\u548c\u7cbe\u5ea6\u4e0d\u8db3\u7b49\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u6765\u6ee1\u8db3\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u7684\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u7684LPCANet\u67b6\u6784\u91c7\u7528MobileNetv2\u4f5c\u4e3aRGB\u7279\u5f81\u63d0\u53d6\u7684\u4e3b\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u91d1\u5b57\u5854\u6a21\u5757\uff08LPM\uff09\u5904\u7406\u6df1\u5ea6\u6570\u636e\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff08CAM\uff09\u5b9e\u73b0\u591a\u6a21\u6001\u878d\u5408\uff0c\u5e76\u5229\u7528\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u5668\uff08SFE\uff09\u589e\u5f3a\u7ed3\u6784\u5206\u6790\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u65e0\u76d1\u7763RGB-D\u8f68\u9053\u6570\u636e\u96c6\uff08NEU-RSDDS-AUG\u3001RSDD-TYPE1\u3001RSDD-TYPE2\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cLPCANet\u4ec5\u9700990\u4e07\u53c2\u6570\u548c2.50 G FLOPs\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe162.60 fps\uff0c\u5728S\u03b1\u3001IOU\u548cMAE\u6307\u6807\u4e0a\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u53471.48%\u30010.86%\u548c1.77%\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86CAM\u548cSFE\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5728\u975e\u8f68\u9053\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u6709\u6548\u6865\u63a5\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u4e3a\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5177\u6709\u91cd\u8981\u5b9e\u7528\u4ef7\u503c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u96c6\u4e2d\u4e8e\u8fdb\u4e00\u6b65\u6a21\u578b\u538b\u7f29\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2601.09185", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09185", "abs": "https://arxiv.org/abs/2601.09185", "authors": ["Zeqiang Wang", "Xinyue Wu", "Chenxi Li", "Zixi Chen", "Nishanth Sastry", "Jon Johnson", "Suparna De"], "title": "OrthoGeoLoRA: Geometric Parameter-Efficient Fine-Tuning for Structured Social Science Concept Retrieval on theWeb", "comment": null, "summary": "Large language models and text encoders increasingly power web-based information systems in the social sciences, including digital libraries, data catalogues, and search interfaces used by researchers, policymakers, and civil society. Full fine-tuning is often computationally and energy intensive, which can be prohibitive for smaller institutions and non-profit organizations in the Web4Good ecosystem. Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation (LoRA), reduces this cost by updating only a small number of parameters. We show that the standard LoRA update $\u0394W = BA^\\top$ has geometric drawbacks: gauge freedom, scale ambiguity, and a tendency toward rank collapse. We introduce OrthoGeoLoRA, which enforces an SVD-like form $\u0394W = B\u03a3A^\\top$ by constraining the low-rank factors to be orthogonal (Stiefel manifold). A geometric reparameterization implements this constraint while remaining compatible with standard optimizers such as Adam and existing fine-tuning pipelines. We also propose a benchmark for hierarchical concept retrieval over the European Language Social Science Thesaurus (ELSST), widely used to organize social science resources in digital repositories. Experiments with a multilingual sentence encoder show that OrthoGeoLoRA outperforms standard LoRA and several strong PEFT variants on ranking metrics under the same low-rank budget, offering a more compute- and parameter-efficient path to adapt foundation models in resource-constrained settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOrthoGeoLoRA\uff0c\u4e00\u79cd\u57fa\u4e8eStiefel\u6d41\u5f62\u7ea6\u675f\u7684\u51e0\u4f55\u611f\u77e5\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5236\u4f4e\u79e9\u56e0\u5b50\u6b63\u4ea4\u5316\u6765\u514b\u670d\u6807\u51c6LoRA\u7684\u51e0\u4f55\u7f3a\u9677\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u4e3a\u793e\u4f1a\u79d1\u5b66\u4fe1\u606f\u7cfb\u7edf\u7684\u6a21\u578b\u9002\u914d\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u8def\u5f84\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u7f16\u7801\u5668\u5728\u793e\u4f1a\u79d1\u5b66\u4fe1\u606f\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5b8c\u5168\u5fae\u8c03\u7684\u8ba1\u7b97\u548c\u80fd\u8017\u6210\u672c\u9ad8\u6602\uff0c\u5bf9Web4Good\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5c0f\u578b\u673a\u6784\u548c\u975e\u8425\u5229\u7ec4\u7ec7\u6784\u6210\u969c\u788d\u3002\u6807\u51c6LoRA\u65b9\u6cd5\u5b58\u5728\u51e0\u4f55\u7f3a\u9677\uff0c\u5305\u62ec\u89c4\u8303\u81ea\u7531\u5ea6\u3001\u5c3a\u5ea6\u6a21\u7cca\u6027\u548c\u79e9\u5d29\u6e83\u503e\u5411\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faOrthoGeoLoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5236\u4f4e\u79e9\u56e0\u5b50\u6b63\u4ea4\u5316\u6765\u7ea6\u675f\u53c2\u6570\u66f4\u65b0\u5f62\u5f0f\u4e3a\u0394W = B\u03a3A\u22a4\uff0c\u7c7b\u4f3c\u4e8eSVD\u5206\u89e3\u3002\u8be5\u65b9\u6cd5\u5c06\u4f4e\u79e9\u56e0\u5b50\u7ea6\u675f\u5728Stiefel\u6d41\u5f62\u4e0a\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u91cd\u53c2\u6570\u5316\u5b9e\u73b0\u8fd9\u4e00\u7ea6\u675f\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eAdam\u7b49\u6807\u51c6\u4f18\u5316\u5668\u53ca\u73b0\u6709\u5fae\u8c03\u6d41\u7a0b\u7684\u517c\u5bb9\u6027\u3002\u7814\u7a76\u8fd8\u5efa\u7acb\u4e86\u57fa\u4e8e\u6b27\u6d32\u8bed\u8a00\u793e\u4f1a\u79d1\u5b66\u53d9\u8bcd\u8868\uff08ELSST\uff09\u7684\u5c42\u6b21\u6982\u5ff5\u68c0\u7d22\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u793e\u4f1a\u79d1\u5b66\u6570\u5b57\u8d44\u6e90\u7ec4\u7ec7\u4e2d\u7684\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u591a\u8bed\u8a00\u53e5\u5b50\u7f16\u7801\u5668\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540c\u4f4e\u79e9\u9884\u7b97\u4e0b\uff0cOrthoGeoLoRA\u5728\u6392\u5e8f\u6307\u6807\u4e0a\u4f18\u4e8e\u6807\u51c6LoRA\u548c\u591a\u79cd\u5f3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u53d8\u4f53\u3002\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u9002\u914d\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "OrthoGeoLoRA\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u89e3\u51b3\u4e86\u6807\u51c6LoRA\u7684\u56fa\u6709\u7f3a\u9677\uff0c\u4e3a\u793e\u4f1a\u79d1\u5b66\u4fe1\u606f\u7cfb\u7edf\u4e2d\u7684\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u7a33\u5b9a\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u7814\u7a76\u673a\u6784\u548c\u975e\u8425\u5229\u7ec4\u7ec7\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8Web4Good\u751f\u6001\u7cfb\u7edf\u4e2d\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u516c\u5e73\u83b7\u53d6\u548c\u5e94\u7528\u3002"}}
{"id": "2601.09269", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09269", "abs": "https://arxiv.org/abs/2601.09269", "authors": ["Wencheng Ye", "Liang Peng", "Xiaoyang Yuan", "Yi Bin", "Pengpeng Zeng", "Hengyu Jin", "Heng Tao Shen"], "title": "RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering", "comment": null, "summary": "Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RISER\uff08\u57fa\u4e8e\u8def\u7531\u5668\u7684\u53ef\u5f15\u5bfc\u63a8\u7406\u589e\u5f3a\u6846\u67b6\uff09\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6fc0\u6d3b\u7a7a\u95f4\u5e72\u9884\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7ec4\u5408\u53ef\u590d\u7528\u63a8\u7406\u5411\u91cf\u6765\u81ea\u9002\u5e94\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u9ad8\u6548\u63a8\u7406\u589e\u5f3a\u3002", "motivation": "\u5f53\u524d\u9886\u57df\u7279\u5b9a\u63a8\u7406\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9700\u8981\u53c2\u6570\u66f4\u65b0\u7684\u8bad\u7ec3\u5bc6\u96c6\u578b\u65b9\u6cd5\uff0c\u800c\u73b0\u6709\u7684\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u3001\u624b\u52a8\u5e72\u9884\uff0c\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u63a8\u7406\u7684\u52a8\u6001\u7279\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u53c2\u6570\u9ad8\u6548\u63a8\u7406\u589e\u5f3a\u7684\u9002\u5e94\u6027\u3002", "method": "RISER\u6846\u67b6\u6784\u5efa\u4e86\u53ef\u590d\u7528\u63a8\u7406\u5411\u91cf\u5e93\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u52a8\u6001\u7ec4\u5408\u8fd9\u4e9b\u5411\u91cf\u4ee5\u9002\u5e94\u6bcf\u4e2a\u8f93\u5165\uff1b\u8def\u7531\u5668\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u4efb\u52a1\u7ea7\u5956\u52b1\u4e0b\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u6d8c\u73b0\u548c\u7ec4\u5408\u65b9\u5f0f\u6fc0\u6d3b\u6f5c\u5728\u7684\u8ba4\u77e5\u539f\u8bed\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u6fc0\u6d3b\u7a7a\u95f4\u5e72\u9884\u3002", "result": "\u5728\u4e03\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRISER\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u4e863.4-6.5%\u7684\u5e73\u5747\u96f6\u6837\u672c\u51c6\u786e\u7387\u63d0\u5347\uff0c\u540c\u65f6\u8d85\u8d8a\u4e86\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5\uff0c\u5177\u67092-3\u500d\u66f4\u9ad8\u7684\u6807\u8bb0\u6548\u7387\u5e76\u4fdd\u6301\u7a33\u5065\u7684\u51c6\u786e\u7387\u589e\u76ca\uff1b\u5206\u6790\u8868\u660eRISER\u80fd\u591f\u81ea\u4e3b\u7ec4\u5408\u591a\u4e2a\u5411\u91cf\u5f62\u6210\u53ef\u89e3\u91ca\u7684\u7cbe\u786e\u63a7\u5236\u7b56\u7565\u3002", "conclusion": "RISER\u5c55\u793a\u4e86\u901a\u8fc7\u52a8\u6001\u7ec4\u5408\u63a8\u7406\u5411\u91cf\u5b9e\u73b0\u81ea\u9002\u5e94\u6fc0\u6d3b\u5f15\u5bfc\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u66f4\u53ef\u63a7\u548c\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u8868\u660e\u6d8c\u73b0\u5f0f\u7ec4\u5408\u5e72\u9884\u80fd\u591f\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u7b56\u7565\uff0c\u63a8\u52a8\u4e86\u53c2\u6570\u9ad8\u6548\u63a8\u7406\u589e\u5f3a\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.09136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09136", "abs": "https://arxiv.org/abs/2601.09136", "authors": ["Lijun Liu", "Linwei Chen", "Zhishou Zhang", "Meng Tian", "Hengfu Cui", "Ruiyang Li", "Zhaocheng Liu", "Qiang Ju", "Qianxi Li", "Hong-Yu Zhou"], "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL", "comment": null, "summary": "General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSkinFlow\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u89c6\u89c9\u4fe1\u606f\u4f20\u8f93\u6548\u7387\u800c\u975e\u53c2\u6570\u6269\u5c55\u6765\u89e3\u51b3\u901a\u7528\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u5b66\u4e2d\u7684\"\u5f25\u6563\u6ce8\u610f\u529b\"\u95ee\u9898\uff0c\u57287B\u53c2\u6570\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u8d85\u8d8a\u5927\u89c4\u6a21\u901a\u7528\u6a21\u578b\u7684\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u901a\u7528\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u5b66\u8bca\u65ad\u4e2d\u5b58\u5728\"\u5f25\u6563\u6ce8\u610f\u529b\"\u95ee\u9898\uff0c\u96be\u4ee5\u4ece\u80cc\u666f\u566a\u58f0\u4e2d\u533a\u5206\u7ec6\u5fae\u75c5\u7406\u75c5\u53d8\uff0c\u73b0\u6709\u7814\u7a76\u8fc7\u5ea6\u4f9d\u8d56\u53c2\u6570\u6269\u5c55\u800c\u5ffd\u89c6\u4e86\u4fe1\u606f\u4f20\u8f93\u6548\u7387\u7684\u4f18\u5316\u3002", "method": "\u63d0\u51faSkinFlow\u6846\u67b6\uff0c\u91c7\u7528\u865a\u62df\u5bbd\u5ea6\u52a8\u6001\u89c6\u89c9\u7f16\u7801\u5668\u5728\u4e0d\u589e\u52a0\u7269\u7406\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5c55\u5f00\u590d\u6742\u75c5\u7406\u6d41\u5f62\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u5bf9\u9f50\u663e\u6027\u533b\u5b66\u63cf\u8ff0\uff0c\u7b2c\u4e8c\u9636\u6bb5\u91cd\u5efa\u9690\u6027\u8bca\u65ad\u7eb9\u7406\uff0c\u5e76\u5728\u53d7\u9650\u8bed\u4e49\u7a7a\u95f4\u5185\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728Fitzpatrick17k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7B\u6a21\u578b\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0cTop-1\u51c6\u786e\u7387\u63d0\u534712.06%\uff0cTop-6\u51c6\u786e\u7387\u63d0\u534728.57%\uff0c\u663e\u8457\u8d85\u8d8aQwen3VL-235B\u548cGPT-5.2\u7b49\u5927\u89c4\u6a21\u901a\u7528\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u4f18\u5316\u51e0\u4f55\u5bb9\u91cf\u548c\u4fe1\u606f\u6d41\u6bd4\u539f\u59cb\u53c2\u6570\u6269\u5c55\u80fd\u4ea7\u751f\u66f4\u4f18\u8d8a\u7684\u8bca\u65ad\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u533b\u5b66AI\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u5f3a\u8c03\u8bca\u65ad\u5b89\u5168\u6027\u548c\u5c42\u6b21\u76f8\u5173\u6027\u7684\u4e34\u5e8a\u8bc4\u4f30\u534f\u8bae\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2601.09246", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09246", "abs": "https://arxiv.org/abs/2601.09246", "authors": ["Xiangqian Wang", "Yifan Jia", "Yang Xiang", "Yumin Zhang", "Yanbin Wang", "Ke Liu"], "title": "TeachPro: Multi-Label Qualitative Teaching Evaluation via Cross-View Graph Synergy and Semantic Anchored Evidence Encoding", "comment": null, "summary": "Standardized Student Evaluation of Teaching often suffer from low reliability, restricted response options, and response distortion. Existing machine learning methods that mine open-ended comments usually reduce feedback to binary sentiment, which overlooks concrete concerns such as content clarity, feedback timeliness, and instructor demeanor, and provides limited guidance for instructional improvement.We propose TeachPro, a multi-label learning framework that systematically assesses five key teaching dimensions: professional expertise, instructional behavior, pedagogical efficacy, classroom experience, and other performance metrics. We first propose a Dimension-Anchored Evidence Encoder, which integrates three core components: (i) a pre-trained text encoder that transforms qualitative feedback annotations into contextualized embeddings; (ii) a prompt module that represents five teaching dimensions as learnable semantic anchors; and (iii) a cross-attention mechanism that aligns evidence with pedagogical dimensions within a structured semantic space. We then propose a Cross-View Graph Synergy Network to represent student comments. This network comprises two components: (i) a Syntactic Branch that extracts explicit grammatical dependencies from parse trees, and (ii) a Semantic Branch that models latent conceptual relations derived from BERT-based similarity graphs. BiAffine fusion module aligns syntactic and semantic units, while a differential regularizer disentangles embeddings to encourage complementary representations. Finally, a cross-attention mechanism bridges the dimension-anchored evidence with the multi-view comment representations. We also contribute a novel benchmark dataset featuring expert qualitative annotations and multi-label scores. Extensive experiments demonstrate that TeachPro offers superior diagnostic granularity and robustness across diverse evaluation settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TeachPro\uff0c\u4e00\u4e2a\u591a\u6807\u7b7e\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5f00\u653e\u5f0f\u5b66\u751f\u8bc4\u6559\u4e2d\u7cfb\u7edf\u8bc4\u4f30\u4e94\u4e2a\u5173\u952e\u6559\u5b66\u7ef4\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5c06\u53cd\u9988\u7b80\u5316\u4e3a\u4e8c\u5143\u60c5\u611f\u800c\u5ffd\u89c6\u5177\u4f53\u6559\u5b66\u95ee\u9898\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6807\u51c6\u5316\u7684\u5b66\u751f\u8bc4\u6559\u901a\u5e38\u5b58\u5728\u53ef\u9760\u6027\u4f4e\u3001\u54cd\u5e94\u9009\u9879\u53d7\u9650\u548c\u54cd\u5e94\u5931\u771f\u7b49\u95ee\u9898\u3002\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6316\u6398\u5f00\u653e\u5f0f\u8bc4\u8bba\u65f6\u901a\u5e38\u5c06\u53cd\u9988\u7b80\u5316\u4e3a\u4e8c\u5143\u60c5\u611f\u5206\u6790\uff0c\u8fd9\u5ffd\u89c6\u4e86\u5185\u5bb9\u6e05\u6670\u5ea6\u3001\u53cd\u9988\u53ca\u65f6\u6027\u548c\u6559\u5e08\u6001\u5ea6\u7b49\u5177\u4f53\u6559\u5b66\u95ee\u9898\uff0c\u65e0\u6cd5\u4e3a\u6559\u5b66\u6539\u8fdb\u63d0\u4f9b\u6709\u6548\u6307\u5bfc\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86TeachPro\u591a\u6807\u7b7e\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u7ef4\u5ea6\u951a\u5b9a\u8bc1\u636e\u7f16\u7801\u5668\u548c\u8de8\u89c6\u56fe\u56fe\u534f\u540c\u7f51\u7edc\u3002\u7ef4\u5ea6\u951a\u5b9a\u8bc1\u636e\u7f16\u7801\u5668\u6574\u5408\u4e86\u9884\u8bad\u7ec3\u6587\u672c\u7f16\u7801\u5668\u3001\u8868\u793a\u4e94\u4e2a\u6559\u5b66\u7ef4\u5ea6\u7684\u53ef\u5b66\u4e60\u8bed\u4e49\u951a\u70b9\u6a21\u5757\uff0c\u4ee5\u53ca\u7ed3\u6784\u5316\u8bed\u4e49\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u8bc1\u636e\u4e0e\u6559\u5b66\u7ef4\u5ea6\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3002\u8de8\u89c6\u56fe\u56fe\u534f\u540c\u7f51\u7edc\u5305\u542b\u4ece\u89e3\u6790\u6811\u63d0\u53d6\u663e\u5f0f\u8bed\u6cd5\u4f9d\u8d56\u7684\u53e5\u6cd5\u5206\u652f\u548c\u57fa\u4e8eBERT\u76f8\u4f3c\u56fe\u5efa\u6a21\u6f5c\u5728\u6982\u5ff5\u5173\u7cfb\u7684\u8bed\u4e49\u5206\u652f\uff0c\u901a\u8fc7\u53cc\u4eff\u5c04\u878d\u5408\u6a21\u5757\u5bf9\u9f50\u53e5\u6cd5\u4e0e\u8bed\u4e49\u5355\u5143\uff0c\u5e76\u4f7f\u7528\u5dee\u5206\u6b63\u5219\u5316\u5668\u89e3\u8026\u5d4c\u5165\u4ee5\u83b7\u5f97\u4e92\u8865\u8868\u793a\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTeachPro\u5728\u591a\u6837\u5316\u7684\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u63d0\u4f9b\u4e86\u4f18\u8d8a\u7684\u8bca\u65ad\u7c92\u5ea6\u548c\u9c81\u68d2\u6027\u3002\u4f5c\u8005\u8fd8\u8d21\u732e\u4e86\u4e00\u4e2a\u5305\u542b\u4e13\u5bb6\u5b9a\u6027\u6807\u6ce8\u548c\u591a\u6807\u7b7e\u8bc4\u5206\u7684\u65b0\u578b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6559\u5b66\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u5206\u6790\u5de5\u5177\uff0c\u80fd\u591f\u4ece\u5f00\u653e\u5f0f\u5b66\u751f\u53cd\u9988\u4e2d\u63d0\u53d6\u591a\u7ef4\u5ea6\u7684\u6559\u5b66\u6d1e\u5bdf\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u4e8c\u5143\u60c5\u611f\u5206\u6790\u7684\u5c40\u9650\u6027\u3002TeachPro\u6846\u67b6\u5c55\u793a\u4e86\u5c06\u7ed3\u6784\u5316\u8bed\u4e49\u7a7a\u95f4\u4e0e\u591a\u89c6\u56fe\u6587\u672c\u8868\u793a\u76f8\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6559\u80b2\u6570\u636e\u6316\u6398\u548c\u6559\u5b66\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2601.09278", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09278", "abs": "https://arxiv.org/abs/2601.09278", "authors": ["Xiaohan Yu", "Chao Feng", "Lang Mei", "Chong Chen"], "title": "M$^3$Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning", "comment": null, "summary": "Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesize from real-world web environments. However, existing approaches remain fundamentally limited to text modality. Extending autonomous information-seeking agents to multimodal settings introduces critical challenges: the specialization-generalization trade-off that emerges when training models for multimodal tool-use at scale, and the severe scarcity of training data capturing complex, multi-step multimodal search trajectories. To address these challenges, we propose M$^3$Searcher, a modular multimodal information-seeking agent that explicitly decouples information acquisition from answer derivation. M$^3$Searcher is optimized with a retrieval-oriented multi-objective reward that jointly encourages factual accuracy, reasoning soundness, and retrieval fidelity. In addition, we develop MMSearchVQA, a multimodal multi-hop dataset to support retrieval centric RL training. Experimental results demonstrate that M$^3$Searcher outperforms existing approaches, exhibits strong transfer adaptability and effective reasoning in complex multimodal tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86M\u00b3Searcher\uff0c\u4e00\u79cd\u6a21\u5757\u5316\u7684\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u89e3\u8026\u4fe1\u606f\u83b7\u53d6\u4e0e\u7b54\u6848\u63a8\u5bfc\u8fc7\u7a0b\uff0c\u5e76\u91c7\u7528\u68c0\u7d22\u5bfc\u5411\u7684\u591a\u76ee\u6807\u5956\u52b1\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eDeepResearch\u98ce\u683c\u7684\u667a\u80fd\u4f53\u5728\u81ea\u4e3b\u4fe1\u606f\u83b7\u53d6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ec5\u9650\u4e8e\u6587\u672c\u6a21\u6001\u3002\u5c06\u81ea\u4e3b\u4fe1\u606f\u68c0\u7d22\u6269\u5c55\u5230\u591a\u6a21\u6001\u73af\u5883\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5927\u89c4\u6a21\u8bad\u7ec3\u591a\u6a21\u6001\u5de5\u5177\u4f7f\u7528\u6a21\u578b\u65f6\u51fa\u73b0\u7684\u4e13\u4e1a\u5316\u4e0e\u6cdb\u5316\u6743\u8861\u95ee\u9898\uff0c\u4ee5\u53ca\u6355\u6349\u590d\u6742\u591a\u6b65\u591a\u6a21\u6001\u641c\u7d22\u8f68\u8ff9\u7684\u8bad\u7ec3\u6570\u636e\u4e25\u91cd\u7a00\u7f3a\u3002", "method": "M\u00b3Searcher\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u660e\u786e\u5c06\u4fe1\u606f\u83b7\u53d6\u4e0e\u7b54\u6848\u63a8\u5bfc\u8fc7\u7a0b\u89e3\u8026\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u68c0\u7d22\u5bfc\u5411\u7684\u591a\u76ee\u6807\u5956\u52b1\u51fd\u6570\u8fdb\u884c\u4f18\u5316\uff0c\u8054\u5408\u9f13\u52b1\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u63a8\u7406\u5408\u7406\u6027\u548c\u68c0\u7d22\u4fdd\u771f\u5ea6\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u5f00\u53d1\u4e86MMSearchVQA\u591a\u6a21\u6001\u591a\u8df3\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u68c0\u7d22\u4e2d\u5fc3\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cM\u00b3Searcher\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8fc1\u79fb\u9002\u5e94\u80fd\u529b\u548c\u6709\u6548\u7684\u63a8\u7406\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u6a21\u5757\u5316\u67b6\u6784\u548c\u591a\u76ee\u6807\u5956\u52b1\u4f18\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u89e3\u8026\u4fe1\u606f\u83b7\u53d6\u4e0e\u7b54\u6848\u63a8\u5bfc\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u81ea\u4e3b\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u4e13\u4e1a\u5316-\u6cdb\u5316\u6743\u8861\u95ee\u9898\u3002\u63d0\u51fa\u7684\u591a\u76ee\u6807\u5956\u52b1\u4f18\u5316\u6846\u67b6\u548c\u4e13\u95e8\u6570\u636e\u96c6\u4e3a\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4e3a\u672a\u6765\u66f4\u590d\u6742\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.09147", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09147", "abs": "https://arxiv.org/abs/2601.09147", "authors": ["Chenhao Fu", "Han Fang", "Xiuzheng Zheng", "Wenbo Wei", "Yonghua Li", "Hao Sun", "Xuelong Li"], "title": "SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection", "comment": null, "summary": "Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model's fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3's multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0\\% Image-AUROC and 92.2\\% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u534f\u540c\u8bed\u4e49-\u89c6\u89c9\u63d0\u793a\uff08SSVP\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u878d\u5408\u591a\u6837\u5316\u7684\u89c6\u89c9\u7f16\u7801\u6765\u63d0\u5347\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u5de5\u4e1a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u4e00\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u96be\u4ee5\u540c\u65f6\u517c\u987e\u5168\u5c40\u8bed\u4e49\u6cdb\u5316\u80fd\u529b\u548c\u7ec6\u7c92\u5ea6\u7ed3\u6784\u5224\u522b\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u534f\u540c\u8bed\u4e49-\u89c6\u89c9\u63d0\u793a\uff08SSVP\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u5c42\u6b21\u5316\u8bed\u4e49-\u89c6\u89c9\u534f\u540c\u673a\u5236\uff08HSVS\uff09\u5c06DINOv3\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\u5148\u9a8c\u6df1\u5ea6\u96c6\u6210\u5230CLIP\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff1b\u89c6\u89c9\u6761\u4ef6\u63d0\u793a\u751f\u6210\u5668\uff08VCPG\uff09\u5229\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5f15\u5bfc\u52a8\u6001\u63d0\u793a\u751f\u6210\uff0c\u4f7f\u8bed\u8a00\u67e5\u8be2\u80fd\u7cbe\u786e\u5b9a\u4f4d\u7279\u5b9a\u5f02\u5e38\u6a21\u5f0f\uff1b\u89c6\u89c9-\u6587\u672c\u5f02\u5e38\u6620\u5c04\u5668\uff08VTAM\uff09\u5efa\u7acb\u4e86\u53cc\u95e8\u6821\u51c6\u8303\u5f0f\uff0c\u89e3\u51b3\u5168\u5c40\u8bc4\u5206\u4e0e\u5c40\u90e8\u8bc1\u636e\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "result": "\u5728\u4e03\u4e2a\u5de5\u4e1a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0cSSVP\u5728MVTec-AD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8693.0%\u7684\u56fe\u50cf\u7ea7AUROC\u548c92.2%\u7684\u50cf\u7d20\u7ea7AUROC\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u534f\u540c\u878d\u5408\u591a\u6837\u5316\u89c6\u89c9\u7f16\u7801\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u76d1\u7763\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u8de8\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2601.09270", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09270", "abs": "https://arxiv.org/abs/2601.09270", "authors": ["Yexing Du", "Kaiyuan Liu", "Bihe Zhang", "Youcheng Pan", "Bo Yang", "Liangyu Huo", "Xiyuan Zhang", "Jian Xie", "Daojing He", "Yang Xiang", "Ming Liu", "Bin Qin"], "title": "MCGA: A Multi-task Classical Chinese Literary Genre Audio Corpus", "comment": null, "summary": "With the rapid advancement of Multimodal Large Language Models (MLLMs), their potential has garnered significant attention in Chinese Classical Studies (CCS). While existing research has primarily focused on text and visual modalities, the audio corpus within this domain remains largely underexplored. To bridge this gap, we propose the Multi-task Classical Chinese Literary Genre Audio Corpus (MCGA). It encompasses a diverse range of literary genres across six tasks: Automatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Speech Emotion Captioning (SEC), Spoken Question Answering (SQA), Speech Understanding (SU), and Speech Reasoning (SR). Through the evaluation of ten MLLMs, our experimental results demonstrate that current models still face substantial challenges when processed on the MCGA test set. Furthermore, we introduce an evaluation metric for SEC and a metric to measure the consistency between the speech and text capabilities of MLLMs. We release MCGA and our code to the public to facilitate the development of MLLMs with more robust multidimensional audio capabilities in CCS. MCGA Corpus: https://github.com/yxduir/MCGA", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MCGA\u591a\u4efb\u52a1\u53e4\u5178\u6587\u5b66\u97f3\u9891\u8bed\u6599\u5e93\uff0c\u586b\u8865\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u6587\u53e4\u5178\u7814\u7a76\u97f3\u9891\u6a21\u6001\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u8bc4\u4f30\u5341\u79cdMLLM\u6a21\u578b\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u663e\u8457\u6311\u6218\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u5728\u4e2d\u6587\u53e4\u5178\u7814\u7a76\u9886\u57df\u7684\u6f5c\u529b\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\uff0c\u97f3\u9891\u8bed\u6599\u5e93\u5728\u8be5\u9886\u57df\u4ecd\u5904\u4e8e\u63a2\u7d22\u4e0d\u8db3\u7684\u72b6\u6001\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u591a\u4efb\u52a1\u53e4\u5178\u6587\u5b66\u97f3\u9891\u8bed\u6599\u5e93\uff0c\u6db5\u76d6\u516d\u79cd\u4efb\u52a1\uff1a\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u3001\u8bed\u97f3\u5230\u6587\u672c\u7ffb\u8bd1\u3001\u8bed\u97f3\u60c5\u611f\u63cf\u8ff0\u3001\u53e3\u8bed\u95ee\u7b54\u3001\u8bed\u97f3\u7406\u89e3\u548c\u8bed\u97f3\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u4e86\u8bed\u97f3\u60c5\u611f\u63cf\u8ff0\u7684\u8bc4\u4f30\u6307\u6807\u4ee5\u53ca\u8861\u91cfMLLM\u8bed\u97f3\u4e0e\u6587\u672c\u80fd\u529b\u4e00\u81f4\u6027\u7684\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5bf9\u5341\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406MCGA\u6d4b\u8bd5\u96c6\u65f6\u4ecd\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u7ef4\u97f3\u9891\u80fd\u529b\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u9a8c\u8bc1\u4e86\u8be5\u8bed\u6599\u5e93\u5bf9\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u6587\u53e4\u5178\u7814\u7a76\u97f3\u9891\u5904\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u7684MCGA\u8bed\u6599\u5e93\u548c\u8bc4\u4f30\u6846\u67b6\u4e3a\u5f00\u53d1\u5177\u6709\u66f4\u5f3a\u5927\u591a\u7ef4\u97f3\u9891\u80fd\u529b\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u5e76\u516c\u5f00\u4e86\u8bed\u6599\u5e93\u548c\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.09536", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09536", "abs": "https://arxiv.org/abs/2601.09536", "authors": ["Dongjie Cheng", "Yongqi Li", "Zhixin Ma", "Hongru Cai", "Yupeng Hu", "Wenjie Wang", "Liqiang Nie", "Wenjie Li"], "title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u751f\u6210\u5f0f\u591a\u6a21\u6001\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u751f\u6210\u4e2d\u95f4\u56fe\u50cf\u6765\u7edf\u4e00\u591a\u6837\u7684\u591a\u6a21\u6001\u63a8\u7406\u6280\u80fd\uff0c\u5e76\u5b9e\u4f8b\u5316\u4e3aOmni-R1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5SFT+RL\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8de8\u591a\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u7684\u7edf\u4e00\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u901a\u5e38\u91c7\u7528\u5355\u4e00\u4efb\u52a1\u7279\u5b9a\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5728\u4e0d\u540c\u591a\u6a21\u6001\u4efb\u52a1\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8bb8\u591a\u591a\u6a21\u6001\u4efb\u52a1\u9700\u8981\u591a\u6837\u5316\u7684\u63a8\u7406\u6280\u80fd\uff0c\u5982\u805a\u7126\u7279\u5b9a\u533a\u57df\u6216\u6807\u8bb0\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u7edf\u4e00\u5904\u7406\u8fd9\u4e9b\u591a\u6837\u5316\u7684\u63a8\u7406\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u751f\u6210\u5f0f\u591a\u6a21\u6001\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u751f\u6210\u4e2d\u95f4\u56fe\u50cf\u6765\u7edf\u4e00\u591a\u6837\u5316\u7684\u591a\u6a21\u6001\u63a8\u7406\u6280\u80fd\u3002\u5177\u4f53\u5b9e\u4f8b\u5316\u4e3aOmni-R1\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u76d1\u7763\u5fae\u8c03\u52a0\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5305\u542b\u611f\u77e5\u5bf9\u9f50\u635f\u5931\u548c\u611f\u77e5\u5956\u52b1\u673a\u5236\u4ee5\u5b9e\u73b0\u529f\u80fd\u6027\u56fe\u50cf\u751f\u6210\u3002\u540c\u65f6\u63d0\u51fa\u4e86Omni-R1-Zero\uff0c\u901a\u8fc7\u4ece\u7eaf\u6587\u672c\u63a8\u7406\u6570\u636e\u4e2d\u5f15\u5bfc\u9010\u6b65\u53ef\u89c6\u5316\uff0c\u6d88\u9664\u4e86\u5bf9\u591a\u6a21\u6001\u6807\u6ce8\u7684\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOmni-R1\u80fd\u591f\u5728\u5e7f\u6cdb\u7684\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u5b9e\u73b0\u7edf\u4e00\u7684\u751f\u6210\u5f0f\u63a8\u7406\u3002Omni-R1-Zero\u5728\u5e73\u5747\u6027\u80fd\u4e0a\u80fd\u591f\u5339\u914d\u751a\u81f3\u8d85\u8d8aOmni-R1\uff0c\u8fd9\u663e\u793a\u4e86\u751f\u6210\u5f0f\u591a\u6a21\u6001\u63a8\u7406\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u751f\u6210\u5f0f\u591a\u6a21\u6001\u63a8\u7406\u8303\u5f0f\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u4e2d\u95f4\u56fe\u50cf\u751f\u6210\u7edf\u4e00\u4e86\u591a\u6837\u5316\u7684\u63a8\u7406\u6280\u80fd\u3002Omni-R1-Zero\u7684\u6210\u529f\u8868\u660e\uff0c\u4ece\u7eaf\u6587\u672c\u6570\u636e\u5f15\u5bfc\u591a\u6a21\u6001\u63a8\u7406\u662f\u53ef\u884c\u7684\u65b9\u5411\uff0c\u4e3a\u51cf\u5c11\u5bf9\u6602\u8d35\u591a\u6a21\u6001\u6807\u6ce8\u7684\u4f9d\u8d56\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u5411\u66f4\u901a\u7528\u3001\u66f4\u9ad8\u6548\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2601.09169", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.09169", "abs": "https://arxiv.org/abs/2601.09169", "authors": ["Jamie Magrill", "Leah Gornstein", "Sandra Seekins", "Barry Magrill"], "title": "Architecture inside the mirage: evaluating generative image models on architectural style, elements, and typologies", "comment": "24 pages, 7 figures", "summary": "Generative artificial intelligence (GenAI) text-to-image systems are increasingly used to generate architectural imagery, yet their capacity to reproduce accurate images in a historically rule-bound field remains poorly characterized. We evaluated five widely used GenAI image platforms (Adobe Firefly, DALL-E 3, Google Imagen 3, Microsoft Image Generator, and Midjourney) using 30 architectural prompts spanning styles, typologies, and codified elements. Each prompt-generator pair produced four images (n = 600 images total). Two architectural historians independently scored each image for accuracy against predefined criteria, resolving disagreements by consensus. Set-level performance was summarized as zero to four accurate images per four-image set. Image output from Common prompts was 2.7-fold more accurate than from Rare prompts (p < 0.05). Across platforms, overall accuracy was limited (highest accuracy score 52 percent; lowest 32 percent; mean 42 percent). All-correct (4 out of 4) outcomes were similar across platforms. By contrast, all-incorrect (0 out of 4) outcomes varied substantially, with Imagen 3 exhibiting the fewest failures and Microsoft Image Generator exhibiting the highest number of failures. Qualitative review of the image dataset identified recurring patterns including over-embellishment, confusion between medieval styles and their later revivals, and misrepresentation of descriptive prompts (for example, egg-and-dart, banded column, pendentive). These findings support the need for visible labeling of GenAI synthetic content, provenance standards for future training datasets, and cautious educational use of GenAI architectural imagery.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e94\u79cd\u4e3b\u6d41\u751f\u6210\u5f0fAI\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u5728\u5efa\u7b51\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u8fd9\u4e9b\u7cfb\u7edf\u5728\u751f\u6210\u5386\u53f2\u89c4\u5219\u7ea6\u675f\u7684\u5efa\u7b51\u56fe\u50cf\u65f6\u51c6\u786e\u6027\u6709\u9650\uff0c\u4e14\u6027\u80fd\u56e0\u5e73\u53f0\u548c\u63d0\u793a\u8bcd\u7c7b\u578b\u800c\u5f02\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u5728\u5efa\u7b51\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u5728\u5386\u53f2\u89c4\u5219\u7ea6\u675f\u7684\u5efa\u7b51\u9886\u57df\u4e2d\u751f\u6210\u51c6\u786e\u56fe\u50cf\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8868\u5f81\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e94\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684GenAI\u56fe\u50cf\u5e73\u53f0\uff08Adobe Firefly\u3001DALL-E 3\u3001Google Imagen 3\u3001Microsoft Image Generator\u548cMidjourney\uff09\uff0c\u4f7f\u752830\u4e2a\u6db5\u76d6\u4e0d\u540c\u98ce\u683c\u3001\u7c7b\u578b\u548c\u7f16\u7801\u5143\u7d20\u7684\u5efa\u7b51\u63d0\u793a\u8bcd\uff0c\u6bcf\u4e2a\u63d0\u793a\u8bcd-\u751f\u6210\u5668\u7ec4\u5408\u4ea7\u751f\u56db\u5f20\u56fe\u50cf\uff0c\u5171600\u5f20\u56fe\u50cf\uff0c\u7531\u4e24\u4f4d\u5efa\u7b51\u5386\u53f2\u5b66\u5bb6\u6839\u636e\u9884\u5b9a\u4e49\u6807\u51c6\u72ec\u7acb\u8bc4\u5206\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u5171\u8bc6\u89e3\u51b3\u5206\u6b67\u3002", "result": "\u5e38\u89c1\u63d0\u793a\u8bcd\u7684\u56fe\u50cf\u8f93\u51fa\u51c6\u786e\u6027\u6bd4\u7f55\u89c1\u63d0\u793a\u8bcd\u9ad82.7\u500d\uff0c\u5404\u5e73\u53f0\u603b\u4f53\u51c6\u786e\u6027\u6709\u9650\uff08\u6700\u9ad852%\uff0c\u6700\u4f4e32%\uff0c\u5e73\u574742%\uff09\uff0c\u5168\u6b63\u786e\u7ed3\u679c\u5728\u5404\u5e73\u53f0\u95f4\u76f8\u4f3c\uff0c\u4f46\u5168\u9519\u8bef\u7ed3\u679c\u5dee\u5f02\u663e\u8457\uff0cImagen 3\u5931\u8d25\u6700\u5c11\uff0cMicrosoft Image Generator\u5931\u8d25\u6700\u591a\uff0c\u5b9a\u6027\u5206\u6790\u8bc6\u522b\u51fa\u8fc7\u5ea6\u88c5\u9970\u3001\u4e2d\u4e16\u7eaa\u98ce\u683c\u4e0e\u540e\u671f\u590d\u5174\u6df7\u6dc6\u4ee5\u53ca\u63cf\u8ff0\u6027\u63d0\u793a\u8bcd\u8bef\u89e3\u7b49\u91cd\u590d\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u9700\u8981\u4e3aGenAI\u5408\u6210\u5185\u5bb9\u63d0\u4f9b\u53ef\u89c1\u6807\u7b7e\u3001\u5efa\u7acb\u672a\u6765\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6765\u6e90\u6807\u51c6\uff0c\u5e76\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u8c28\u614e\u4f7f\u7528GenAI\u5efa\u7b51\u56fe\u50cf\uff0c\u8fd9\u4e9b\u53d1\u73b0\u5bf9AI\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u4f26\u7406\u8003\u91cf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.09342", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09342", "abs": "https://arxiv.org/abs/2601.09342", "authors": ["Ewelina Gajewska", "Katarzyna Budzynska", "Jaros\u0142aw A Chudziak"], "title": "Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework", "comment": "This paper has been accepted for the upcoming 18th International Conference on Agents and Artificial Intelligence (ICAART-2026), Marbella, Spain. The final published version will appear in the official conference proceedings", "summary": "This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods (zero-shot prompting, few-shot prompting, chain-of-thought prompting) and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness that accounts for the trade-off between true positive and true negative rates. We demonstrate that our community-driven consultative framework significantly improves both classification accuracy and fairness across all target groups.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9690\u542b\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6846\u67b6\uff0c\u91c7\u7528\u7531\u4e2d\u592e\u4ef2\u88c1\u4ee3\u7406\u548c\u52a8\u6001\u6784\u5efa\u7684\u793e\u533a\u4ee3\u7406\u7ec4\u6210\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u793e\u4f1a\u6587\u5316\u80cc\u666f\u77e5\u8bc6\uff0c\u5728\u5206\u7c7b\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u63d0\u793a\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u9690\u542b\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u793e\u4f1a\u6587\u5316\u80cc\u666f\u7684\u5145\u5206\u8003\u8651\uff0c\u96be\u4ee5\u5b9e\u73b0\u8eab\u4efd\u611f\u77e5\u7684\u9002\u5ea6\u8c03\u8282\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9488\u5bf9\u7279\u5b9a\u4eba\u53e3\u7fa4\u4f53\u7684\u5fae\u5999\u504f\u89c1\u8868\u8fbe\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u516c\u5e73\u548c\u51c6\u786e\u7684\u5206\u7c7b\u6846\u67b6\u3002", "method": "\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u62ec\u4e2d\u592e\u4ef2\u88c1\u4ee3\u7406\u548c\u4ee3\u8868\u7279\u5b9a\u4eba\u53e3\u7fa4\u4f53\u7684\u52a8\u6001\u793e\u533a\u4ee3\u7406\uff0c\u901a\u8fc7\u6574\u5408\u6765\u81ea\u516c\u5f00\u77e5\u8bc6\u6e90\u7684\u793e\u4f1a\u6587\u5316\u80cc\u666f\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u8eab\u4efd\u611f\u77e5\u7684\u9002\u5ea6\u8c03\u8282\uff0c\u5e76\u91c7\u7528\u5e73\u8861\u51c6\u786e\u7387\u4f5c\u4e3a\u5206\u7c7b\u516c\u5e73\u6027\u7684\u6838\u5fc3\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684ToxiGen\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u96f6\u6837\u672c\u63d0\u793a\u3001\u5c11\u6837\u672c\u63d0\u793a\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u7b49\u6700\u5148\u8fdb\u7684\u63d0\u793a\u65b9\u6cd5\u4ee5\u53ca\u5176\u4ed6\u66ff\u4ee3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6240\u6709\u76ee\u6807\u7fa4\u4f53\u7684\u5206\u7c7b\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\uff0c\u5e73\u8861\u51c6\u786e\u7387\u6307\u6807\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u793e\u533a\u9a71\u52a8\u7684\u534f\u5546\u6846\u67b6\u5728\u9690\u542b\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u6574\u5408\u793e\u4f1a\u6587\u5316\u80cc\u666f\u548c\u91c7\u7528\u516c\u5e73\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u4e3a\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u516c\u6b63\u7684\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u654f\u611f\u5185\u5bb9\u8bc6\u522b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.09191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09191", "abs": "https://arxiv.org/abs/2601.09191", "authors": ["Qizhen Lan", "Aaron Choi", "Jun Ma", "Bo Wang", "Zhaogming Zhao", "Xiaoqian Jiang", "Yu-Chun Hsu"], "title": "From Performance to Practice: Knowledge-Distilled Segmentator for On-Premises Clinical Workflows", "comment": null, "summary": "Deploying medical image segmentation models in routine clinical workflows is often constrained by on-premises infrastructure, where computational resources are fixed and cloud-based inference may be restricted by governance and security policies. While high-capacity models achieve strong segmentation accuracy, their computational demands hinder practical deployment and long-term maintainability in hospital environments. We present a deployment-oriented framework that leverages knowledge distillation to translate a high-performing segmentation model into a scalable family of compact student models, without modifying the inference pipeline. The proposed approach preserves architectural compatibility with existing clinical systems while enabling systematic capacity reduction. The framework is evaluated on a multi-site brain MRI dataset comprising 1,104 3D volumes, with independent testing on 101 curated cases, and is further examined on abdominal CT to assess cross-modality generalizability. Under aggressive parameter reduction (94%), the distilled student model preserves nearly all of the teacher's segmentation accuracy (98.7%), while achieving substantial efficiency gains, including up to a 67% reduction in CPU inference latency without additional deployment overhead. These results demonstrate that knowledge distillation provides a practical and reliable pathway for converting research-grade segmentation models into maintainable, deployment-ready components for on-premises clinical workflows in real-world health systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u90e8\u7f72\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u5c06\u9ad8\u6027\u80fd\u5206\u5272\u6a21\u578b\u8f6c\u6362\u4e3a\u53ef\u6269\u5c55\u7684\u7d27\u51d1\u5b66\u751f\u6a21\u578b\u5bb6\u65cf\uff0c\u5728\u4fdd\u6301\u67b6\u6784\u517c\u5bb9\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u7cfb\u7edf\u6027\u7684\u5bb9\u91cf\u7f29\u51cf\uff0c\u4e3a\u4e34\u5e8a\u5de5\u4f5c\u6d41\u63d0\u4f9b\u5b9e\u7528\u7684\u90e8\u7f72\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5728\u5e38\u89c4\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\u90e8\u7f72\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5e38\u53d7\u9650\u4e8e\u672c\u5730\u57fa\u7840\u8bbe\u65bd\uff0c\u5176\u4e2d\u8ba1\u7b97\u8d44\u6e90\u56fa\u5b9a\u4e14\u57fa\u4e8e\u4e91\u7684\u63a8\u7406\u53ef\u80fd\u53d7\u6cbb\u7406\u548c\u5b89\u5168\u7b56\u7565\u9650\u5236\u3002\u867d\u7136\u9ad8\u5bb9\u91cf\u6a21\u578b\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u5206\u5272\u7cbe\u5ea6\uff0c\u4f46\u5176\u8ba1\u7b97\u9700\u6c42\u963b\u788d\u4e86\u5728\u533b\u9662\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u548c\u957f\u671f\u53ef\u7ef4\u62a4\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u90e8\u7f72\u5bfc\u5411\u7684\u6846\u67b6\uff0c\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u5c06\u9ad8\u6027\u80fd\u5206\u5272\u6a21\u578b\u8f6c\u6362\u4e3a\u53ef\u6269\u5c55\u7684\u7d27\u51d1\u5b66\u751f\u6a21\u578b\u5bb6\u65cf\uff0c\u65e0\u9700\u4fee\u6539\u63a8\u7406\u7ba1\u9053\u3002\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86\u4e0e\u73b0\u6709\u4e34\u5e8a\u7cfb\u7edf\u7684\u67b6\u6784\u517c\u5bb9\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u7cfb\u7edf\u6027\u7684\u5bb9\u91cf\u7f29\u51cf\uff0c\u6846\u67b6\u5728\u591a\u7ad9\u70b9\u8111MRI\u6570\u636e\u96c6\uff08\u5305\u542b1,104\u4e2a3D\u4f53\u79ef\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5728\u8179\u90e8CT\u4e0a\u8fdb\u4e00\u6b65\u68c0\u9a8c\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u6fc0\u8fdb\u53c2\u6570\u51cf\u5c1194%\u7684\u60c5\u51b5\u4e0b\uff0c\u84b8\u998f\u540e\u7684\u5b66\u751f\u6a21\u578b\u4fdd\u7559\u4e86\u6559\u5e08\u6a21\u578b\u5206\u5272\u7cbe\u5ea6\u768498.7%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\uff0c\u5305\u62ecCPU\u63a8\u7406\u5ef6\u8fdf\u51cf\u5c11\u9ad8\u8fbe67%\u4e14\u65e0\u9700\u989d\u5916\u90e8\u7f72\u5f00\u9500\u3002\u8be5\u6846\u67b6\u5728101\u4e2a\u7cbe\u9009\u75c5\u4f8b\u7684\u72ec\u7acb\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u8179\u90e8CT\u6570\u636e\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u533b\u7597\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u77e5\u8bc6\u84b8\u998f\u4e3a\u5c06\u7814\u7a76\u7ea7\u5206\u5272\u6a21\u578b\u8f6c\u6362\u4e3a\u53ef\u7ef4\u62a4\u3001\u90e8\u7f72\u5c31\u7eea\u7684\u7ec4\u4ef6\u63d0\u4f9b\u4e86\u5b9e\u7528\u53ef\u9760\u7684\u9014\u5f84\uff0c\u7279\u522b\u9002\u7528\u4e8e\u672c\u5730\u4e34\u5e8a\u5de5\u4f5c\u6d41\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\uff0c\u4e3a\u533b\u7597\u7cfb\u7edf\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.09367", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09367", "abs": "https://arxiv.org/abs/2601.09367", "authors": ["Aidana Aidynkyzy", "O\u011fuz Dikenelli", "Oylum Alatl\u0131", "\u015eebnem Bora"], "title": "Relation Extraction Capabilities of LLMs on Clinical Text: A Bilingual Evaluation for English and Turkish", "comment": null, "summary": "The scarcity of annotated datasets for clinical information extraction in non-English languages hinders the evaluation of large language model (LLM)-based methods developed primarily in English. In this study, we present the first comprehensive bilingual evaluation of LLMs for the clinical Relation Extraction (RE) task in both English and Turkish. To facilitate this evaluation, we introduce the first English-Turkish parallel clinical RE dataset, derived and carefully curated from the 2010 i2b2/VA relation classification corpus. We systematically assess a diverse set of prompting strategies, including multiple in-context learning (ICL) and Chain-of-Thought (CoT) approaches, and compare their performance to fine-tuned baselines such as PURE. Furthermore, we propose Relation-Aware Retrieval (RAR), a novel in-context example selection method based on contrastive learning, that is specifically designed to capture both sentence-level and relation-level semantics. Our results show that prompting-based LLM approaches consistently outperform traditional fine-tuned models. Moreover, evaluations for English performed better than their Turkish counterparts across all evaluated LLMs and prompting techniques. Among ICL methods, RAR achieves the highest performance, with Gemini 1.5 Flash reaching a micro-F1 score of 0.906 in English and 0.888 in Turkish. Performance further improves to 0.918 F1 in English when RAR is combined with a structured reasoning prompt using the DeepSeek-V3 model. These findings highlight the importance of high-quality demonstration retrieval and underscore the potential of advanced retrieval and prompting techniques to bridge resource gaps in clinical natural language processing.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u82f1\u8bed-\u571f\u8033\u5176\u8bed\u5e73\u884c\u4e34\u5e8a\u5173\u7cfb\u62bd\u53d6\u6570\u636e\u96c6\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u5176\u4e2d\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u5173\u7cfb\u611f\u77e5\u68c0\u7d22\u65b9\u6cd5\u5728\u4e34\u5e8a\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u9ad8\u8d28\u91cf\u793a\u4f8b\u68c0\u7d22\u5bf9\u4e8e\u8de8\u8bed\u8a00\u4e34\u5e8a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u975e\u82f1\u8bed\u8bed\u8a00\u4e34\u5e8a\u4fe1\u606f\u62bd\u53d6\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u963b\u788d\u4e86\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\u5f00\u53d1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u7684\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6784\u5efa\u9996\u4e2a\u82f1\u8bed-\u571f\u8033\u5176\u8bed\u5e73\u884c\u4e34\u5e8a\u5173\u7cfb\u62bd\u53d6\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u8de8\u8bed\u8a00\u4e34\u5e8a\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u6709\u6548\u7684\u63d0\u793a\u7b56\u7565\u4ee5\u5f25\u8865\u8d44\u6e90\u5dee\u8ddd\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u9996\u4e2a\u4ece2010 i2b2/VA\u5173\u7cfb\u5206\u7c7b\u8bed\u6599\u5e93\u884d\u751f\u5e76\u7cbe\u5fc3\u7b56\u5212\u7684\u82f1\u8bed-\u571f\u8033\u5176\u8bed\u5e73\u884c\u4e34\u5e8a\u5173\u7cfb\u62bd\u53d6\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u63d0\u793a\u7b56\u7565\u5305\u62ec\u591a\u79cd\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u601d\u7ef4\u94fe\u65b9\u6cd5\uff0c\u5e76\u4e0ePURE\u7b49\u5fae\u8c03\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u7279\u522b\u63d0\u51fa\u4e86\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u5173\u7cfb\u611f\u77e5\u68c0\u7d22\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u6355\u6349\u53e5\u5b50\u7ea7\u548c\u5173\u7cfb\u7ea7\u8bed\u4e49\u3002", "result": "\u57fa\u4e8e\u63d0\u793a\u7684LLM\u65b9\u6cd5\u5728\u6240\u6709\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u6a21\u578b\uff0c\u82f1\u8bed\u8bc4\u4f30\u7ed3\u679c\u5728\u6240\u6709LLM\u548c\u63d0\u793a\u6280\u672f\u4e0a\u5747\u4f18\u4e8e\u571f\u8033\u5176\u8bed\u5bf9\u5e94\u7ed3\u679c\uff0c\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u4e2d\uff0c\u5173\u7cfb\u611f\u77e5\u68c0\u7d22\u8fbe\u5230\u6700\u9ad8\u6027\u80fd\uff0cGemini 1.5 Flash\u5728\u82f1\u8bed\u548c\u571f\u8033\u5176\u8bed\u4e2d\u5206\u522b\u83b7\u5f970.906\u548c0.888\u7684\u5fae\u5e73\u5747F1\u5206\u6570\uff0c\u5f53RAR\u4e0eDeepSeek-V3\u6a21\u578b\u7684\u7ed3\u6784\u5316\u63a8\u7406\u63d0\u793a\u7ed3\u5408\u65f6\uff0c\u82f1\u8bed\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u81f30.918 F1\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u6f14\u793a\u68c0\u7d22\u5bf9\u4e8e\u4e34\u5e8a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u81f3\u5173\u91cd\u8981\uff0c\u5148\u8fdb\u7684\u68c0\u7d22\u548c\u63d0\u793a\u6280\u672f\u5177\u6709\u5f25\u8865\u8d44\u6e90\u5dee\u8ddd\u7684\u6f5c\u529b\uff0c\u5173\u7cfb\u611f\u77e5\u68c0\u7d22\u65b9\u6cd5\u901a\u8fc7\u6355\u6349\u53e5\u5b50\u7ea7\u548c\u5173\u7cfb\u7ea7\u8bed\u4e49\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u4e34\u5e8a\u5173\u7cfb\u62bd\u53d6\u6027\u80fd\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4e34\u5e8a\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.09209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09209", "abs": "https://arxiv.org/abs/2601.09209", "authors": ["Qiang Hu", "Qimei Wang", "Yingjie Guo", "Qiang Li", "Zhiwei Wang"], "title": "Pairing-free Group-level Knowledge Distillation for Robust Gastrointestinal Lesion Classification in White-Light Endoscopy", "comment": "Accepted to AAAI 2026", "summary": "White-Light Imaging (WLI) is the standard for endoscopic cancer screening, but Narrow-Band Imaging (NBI) offers superior diagnostic details. A key challenge is transferring knowledge from NBI to enhance WLI-only models, yet existing methods are critically hampered by their reliance on paired NBI-WLI images of the same lesion, a costly and often impractical requirement that leaves vast amounts of clinical data untapped. In this paper, we break this paradigm by introducing PaGKD, a novel Pairing-free Group-level Knowledge Distillation framework that that enables effective cross-modal learning using unpaired WLI and NBI data. Instead of forcing alignment between individual, often semantically mismatched image instances, PaGKD operates at the group level to distill more complete and compatible knowledge across modalities. Central to PaGKD are two complementary modules: (1) Group-level Prototype Distillation (GKD-Pro) distills compact group representations by extracting modality-invariant semantic prototypes via shared lesion-aware queries; (2) Group-level Dense Distillation (GKD-Den) performs dense cross-modal alignment by guiding group-aware attention with activation-derived relation maps. Together, these modules enforce global semantic consistency and local structural coherence without requiring image-level correspondence. Extensive experiments on four clinical datasets demonstrate that PaGKD consistently and significantly outperforms state-of-the-art methods, achieving relative AUC improvements of 3.3%, 1.1%, 2.8%, and 3.2%, respectively, establishing a new direction for cross-modal learning from unpaired data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPaGKD\u7684\u65e0\u914d\u5bf9\u7ec4\u7ea7\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5185\u7aa5\u955c\u6210\u50cf\u4e2d\u4ece\u7a84\u5e26\u6210\u50cf\uff08NBI\uff09\u5411\u767d\u5149\u6210\u50cf\uff08WLI\uff09\u8fdb\u884c\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u7684\u96be\u9898\uff0c\u8be5\u6846\u67b6\u65e0\u9700\u4f9d\u8d56\u914d\u5bf9\u7684NBI-WLI\u56fe\u50cf\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u4e86WLI-only\u6a21\u578b\u7684\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u5185\u7aa5\u955c\u764c\u75c7\u7b5b\u67e5\u4e2d\uff0c\u7a84\u5e26\u6210\u50cf\uff08NBI\uff09\u76f8\u6bd4\u6807\u51c6\u767d\u5149\u6210\u50cf\uff08WLI\uff09\u80fd\u63d0\u4f9b\u66f4\u4f18\u7684\u8bca\u65ad\u7ec6\u8282\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u540c\u4e00\u75c5\u7076\u7684\u914d\u5bf9NBI-WLI\u56fe\u50cf\uff0c\u8fd9\u79cd\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u5207\u5b9e\u9645\uff0c\u5bfc\u81f4\u5927\u91cf\u4e34\u5e8a\u6570\u636e\u65e0\u6cd5\u88ab\u6709\u6548\u5229\u7528\uff0c\u9650\u5236\u4e86\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51faPaGKD\uff08Pairing-free Group-level Knowledge Distillation\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u6a21\u5757\uff1a\u7ec4\u7ea7\u539f\u578b\u84b8\u998f\uff08GKD-Pro\uff09\u901a\u8fc7\u5171\u4eab\u7684\u75c5\u7076\u611f\u77e5\u67e5\u8be2\u63d0\u53d6\u6a21\u6001\u4e0d\u53d8\u7684\u8bed\u4e49\u539f\u578b\u6765\u84b8\u998f\u7d27\u51d1\u7684\u7ec4\u8868\u793a\uff1b\u7ec4\u7ea7\u5bc6\u96c6\u84b8\u998f\uff08GKD-Den\uff09\u901a\u8fc7\u6fc0\u6d3b\u5bfc\u51fa\u7684\u5173\u7cfb\u56fe\u5f15\u5bfc\u7ec4\u611f\u77e5\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u5bc6\u96c6\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u3002\u8fd9\u4e24\u4e2a\u6a21\u5757\u5171\u540c\u4f5c\u7528\uff0c\u5728\u4e0d\u8981\u6c42\u56fe\u50cf\u7ea7\u5bf9\u5e94\u5173\u7cfb\u7684\u60c5\u51b5\u4e0b\uff0c\u5f3a\u5236\u6267\u884c\u5168\u5c40\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u7ed3\u6784\u8fde\u8d2f\u6027\u3002", "result": "\u5728\u56db\u4e2a\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPaGKD\u6301\u7eed\u4e14\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5206\u522b\u5b9e\u73b0\u4e863.3%\u30011.1%\u30012.8%\u548c3.2%\u7684\u76f8\u5bf9AUC\u63d0\u5347\uff0c\u4e3a\u65e0\u914d\u5bf9\u6570\u636e\u7684\u8de8\u6a21\u6001\u5b66\u4e60\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\u3002", "conclusion": "PaGKD\u901a\u8fc7\u7ec4\u7ea7\u77e5\u8bc6\u84b8\u998f\u6253\u7834\u4e86\u4f20\u7edf\u8de8\u6a21\u6001\u5b66\u4e60\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4e3a\u5229\u7528\u5927\u91cf\u672a\u914d\u5bf9\u4e34\u5e8a\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5f00\u8f9f\u4e86\u65e0\u914d\u5bf9\u8de8\u6a21\u6001\u5b66\u4e60\u7684\u65b0\u65b9\u5411\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u548c\u7814\u7a76\u610f\u4e49\u3002"}}
{"id": "2601.09555", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09555", "abs": "https://arxiv.org/abs/2601.09555", "authors": ["Manyi Zhang", "Ji-Fu Li", "Zhongao Sun", "Haoli Bai", "Hui-Ling Zhen", "Zhenhua Dong", "Xianzhi Yu"], "title": "Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats", "comment": null, "summary": "Microscaling Floating-Point (MXFP) has emerged as a promising low-precision format for large language models (LLMs). Despite various post-training quantization (PTQ) algorithms being proposed, they mostly focus on integer quantization, while their applicability and behavior under MXFP formats remain largely unexplored. To address this gap, this work conducts a systematic investigation of PTQ under MXFP formats, encompassing over 7 PTQ algorithms, 15 evaluation benchmarks, and 3 LLM families. The key findings include: 1) MXFP8 consistently achieves near-lossless performance, while MXFP4 introduces substantial accuracy degradation and remains challenging; 2) PTQ effectiveness under MXFP depends strongly on format compatibility, with some algorithmic paradigms being consistently more effective than others; 3) PTQ performance exhibits highly consistent trends across model families and modalities, in particular, quantization sensitivity is dominated by the language model rather than the vision encoder in multimodal LLMs; 4) The scaling factor of quantization is a critical error source in MXFP4, and a simple pre-scale optimization strategy can significantly mitigate its impact. Together, these results provide practical guidance on adapting existing PTQ methods to MXFP quantization.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u4e86\u540e\u8bad\u7ec3\u91cf\u5316\u5728\u5fae\u7f29\u6d6e\u70b9\u683c\u5f0f\u4e0b\u7684\u9002\u7528\u6027\uff0c\u53d1\u73b0MXFP8\u80fd\u5b9e\u73b0\u8fd1\u4e4e\u65e0\u635f\u7684\u6027\u80fd\uff0c\u800cMXFP4\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5e76\u4e3a\u73b0\u6709PTQ\u65b9\u6cd5\u9002\u914dMXFP\u91cf\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u5c3d\u7ba1\u5fae\u7f29\u6d6e\u70b9\u683c\u5f0f\u5df2\u6210\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u6709\u524d\u666f\u7684\u4f4e\u7cbe\u5ea6\u683c\u5f0f\uff0c\u4f46\u73b0\u6709\u540e\u8bad\u7ec3\u91cf\u5316\u7b97\u6cd5\u4e3b\u8981\u5173\u6ce8\u6574\u6570\u91cf\u5316\uff0c\u5176\u5728MXFP\u683c\u5f0f\u4e0b\u7684\u9002\u7528\u6027\u548c\u884c\u4e3a\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7cfb\u7edf\u6027\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u6db5\u76d6\u8d85\u8fc77\u79cd\u540e\u8bad\u7ec3\u91cf\u5316\u7b97\u6cd5\u300115\u4e2a\u8bc4\u4f30\u57fa\u51c6\u548c3\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u7279\u522b\u5173\u6ce8\u683c\u5f0f\u517c\u5bb9\u6027\u5206\u6790\uff0c\u5e76\u9488\u5bf9MXFP4\u63d0\u51fa\u4e86\u7b80\u5355\u7684\u9884\u7f29\u653e\u4f18\u5316\u7b56\u7565\u6765\u7f13\u89e3\u7f29\u653e\u56e0\u5b50\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0MXFP8\u80fd\u6301\u7eed\u5b9e\u73b0\u8fd1\u4e4e\u65e0\u635f\u7684\u6027\u80fd\u8868\u73b0\uff0c\u800cMXFP4\u5219\u5f15\u5165\u663e\u8457\u7cbe\u5ea6\u4e0b\u964d\uff1bPTQ\u5728MXFP\u4e0b\u7684\u6709\u6548\u6027\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u683c\u5f0f\u517c\u5bb9\u6027\uff0c\u67d0\u4e9b\u7b97\u6cd5\u8303\u5f0f\u59cb\u7ec8\u66f4\u6709\u6548\uff1b\u91cf\u5316\u654f\u611f\u6027\u4e3b\u8981\u7531\u8bed\u8a00\u6a21\u578b\u800c\u975e\u89c6\u89c9\u7f16\u7801\u5668\u4e3b\u5bfc\uff1bMXFP4\u7684\u7f29\u653e\u56e0\u5b50\u662f\u5173\u952e\u8bef\u5dee\u6e90\uff0c\u9884\u7f29\u653e\u4f18\u5316\u80fd\u663e\u8457\u7f13\u89e3\u5176\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u73b0\u6709PTQ\u65b9\u6cd5\u9002\u914dMXFP\u91cf\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u63ed\u793a\u4e86\u683c\u5f0f\u517c\u5bb9\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8868\u660e\u91cf\u5316\u654f\u611f\u6027\u4e3b\u8981\u7531\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u51b3\u5b9a\uff0c\u4e3a\u672a\u6765\u4f4e\u7cbe\u5ea6\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2601.09213", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09213", "abs": "https://arxiv.org/abs/2601.09213", "authors": ["Jialu Li", "Taiyan Zhou"], "title": "SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion", "comment": "Preprint", "summary": "Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.\n  We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSpikeVAEDiff\uff0c\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u6269\u6563\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u795e\u7ecf\u5c16\u5cf0\u4fe1\u53f7\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u573a\u666f\u3002\u8be5\u65b9\u6cd5\u5728Allen\u89c6\u89c9\u7f16\u7801\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u7279\u5b9a\u8111\u533a\u5bf9\u91cd\u5efa\u8d28\u91cf\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4ece\u795e\u7ecf\u6d3b\u52a8\u91cd\u5efa\u81ea\u7136\u89c6\u89c9\u573a\u666f\u7684\u6838\u5fc3\u6311\u6218\uff0c\u7279\u522b\u662f\u5229\u7528\u795e\u7ecf\u5c16\u5cf0\u4fe1\u53f7\u800c\u975efMRI\u6570\u636e\uff0c\u4ee5\u83b7\u53d6\u66f4\u4f18\u8d8a\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u3002\u5f53\u524d\u65b9\u6cd5\u5728\u4ece\u9ad8\u7ef4\u795e\u7ecf\u6570\u636e\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u3001\u8bed\u4e49\u6709\u610f\u4e49\u7684\u56fe\u50cf\u91cd\u5efa\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u7801\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51faSpikeVAEDiff\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u6df1\u5ea6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5c06\u795e\u7ecf\u5c16\u5cf0\u4fe1\u53f7\u6620\u5c04\u5230\u6f5c\u5728\u8868\u793a\uff0c\u751f\u6210\u4f4e\u5206\u8fa8\u7387\u521d\u6b65\u91cd\u5efa\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u5c06\u5c16\u5cf0\u4fe1\u53f7\u6620\u5c04\u5230CLIP\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\uff0c\u5229\u7528Versatile Diffusion\u6a21\u578b\u901a\u8fc7\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u8fdb\u884c\u7cbe\u7ec6\u5316\u91cd\u5efa\u3002", "result": "\u5728Allen\u89c6\u89c9\u7f16\u7801-\u795e\u7ecf\u50cf\u7d20\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cVISI\u8111\u533a\u8868\u73b0\u51fa\u6700\u663e\u8457\u7684\u6fc0\u6d3b\u5e76\u5728\u91cd\u5efa\u8d28\u91cf\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u7279\u5b9a\u8111\u533a\u6570\u636e\u663e\u8457\u63d0\u5347\u91cd\u5efa\u6027\u80fd\uff0c\u4e0efMRI\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5c16\u5cf0\u6570\u636e\u63d0\u4f9b\u4e86\u66f4\u4f18\u8d8a\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e0e\u6269\u6563\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u4e3a\u795e\u7ecf\u89e3\u7801\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002\u5c16\u5cf0\u6570\u636e\u76f8\u5bf9\u4e8efMRI\u7684\u4f18\u8d8a\u5206\u8fa8\u7387\u4f18\u52bf\u5f97\u5230\u9a8c\u8bc1\uff0c\u7279\u5b9a\u8111\u533a\u5bf9\u89c6\u89c9\u91cd\u5efa\u7684\u5173\u952e\u4f5c\u7528\u4e3a\u795e\u7ecf\u7f16\u7801\u673a\u5236\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2601.09631", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09631", "abs": "https://arxiv.org/abs/2601.09631", "authors": ["Stergios Chatzikyriakidis"], "title": "LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation", "comment": null, "summary": "Large Language Models (LLMs), despite their remarkable capabilities across NLP tasks, struggle with phonologically-grounded phenomena like rhyme detection and generation. This is even more evident in lower-resource languages such as Modern Greek. In this paper, we present a hybrid system that combines LLMs with deterministic phonological algorithms to achieve accurate rhyme identification/analysis and generation. Our approach implements a comprehensive taxonomy of Greek rhyme types, including Pure, Rich, Imperfect, Mosaic, and Identical Pre-rhyme Vowel (IDV) patterns, and employs an agentic generation pipeline with phonological verification. We evaluate multiple prompting strategies (zero-shot, few-shot, Chain-of-Thought, and RAG-augmented) across several LLMs including Claude 3.7 and 4.5, GPT-4o, Gemini 2.0 and open-weight models like Llama 3.1 8B and 70B and Mistral Large. Results reveal a significant \"Reasoning Gap\": while native-like models (Claude 3.7) perform intuitively (40\\% accuracy in identification), reasoning-heavy models (Claude 4.5) achieve state-of-the-art performance (54\\%) only when prompted with Chain-of-Thought. Most critically, pure LLM generation fails catastrophically (under 4\\% valid poems), while our hybrid verification loop restores performance to 73.1\\%. We release our system and a crucial, rigorously cleaned corpus of 40,000+ rhymes, derived from the Anemoskala and Interwar Poetry corpora, to support future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u786e\u5b9a\u6027\u97f3\u97f5\u7b97\u6cd5\u7684\u6df7\u5408\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u5728\u5e0c\u814a\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u97f5\u5f8b\u68c0\u6d4b\u4e0e\u751f\u6210\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u97f3\u97f5\u9a8c\u8bc1\u5faa\u73af\u5c06\u8bd7\u6b4c\u751f\u6210\u6709\u6548\u6027\u4ece\u4e0d\u8db34%\u63d0\u5347\u81f373.1%\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u97f5\u5f8b\u76f8\u5173\u73b0\u8c61\uff08\u5982\u62bc\u97f5\u68c0\u6d4b\u4e0e\u751f\u6210\uff09\u4e0a\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u8fd9\u4e00\u95ee\u9898\u5728\u5e0c\u814a\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u5c24\u4e3a\u7a81\u51fa\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u51c6\u786e\u5904\u7406\u97f3\u97f5\u5b66\u4efb\u52a1\u7684\u4e13\u95e8\u7cfb\u7edf\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7cfb\u7edf\uff0c\u5c06LLM\u4e0e\u786e\u5b9a\u6027\u97f3\u97f5\u7b97\u6cd5\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5168\u9762\u7684\u5e0c\u814a\u8bed\u62bc\u97f5\u7c7b\u578b\u5206\u7c7b\uff08\u5305\u62ec\u7eaf\u97f5\u3001\u5bcc\u97f5\u3001\u4e0d\u5b8c\u5168\u97f5\u3001\u9a6c\u8d5b\u514b\u97f5\u548c\u76f8\u540c\u524d\u5143\u97f3\u6a21\u5f0f\uff09\uff0c\u5e76\u91c7\u7528\u5e26\u6709\u97f3\u97f5\u9a8c\u8bc1\u7684\u667a\u80fd\u751f\u6210\u6d41\u7a0b\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u601d\u7ef4\u94fe\u548cRAG\u589e\u5f3a\uff09\u5728\u4e0d\u540cLLM\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5b58\u5728\u663e\u8457\u7684\"\u63a8\u7406\u9e3f\u6c9f\"\uff1a\u7c7b\u4eba\u63a8\u7406\u6a21\u578b\uff08Claude 3.7\uff09\u5728\u62bc\u97f5\u8bc6\u522b\u4e2d\u8fbe\u523040%\u51c6\u786e\u7387\uff0c\u800c\u63a8\u7406\u5bc6\u96c6\u578b\u6a21\u578b\uff08Claude 4.5\uff09\u5728\u4f7f\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u65f6\u8fbe\u523054%\u7684\u5148\u8fdb\u6c34\u5e73\uff1b\u7eafLLM\u751f\u6210\u5b8c\u5168\u5931\u8d25\uff08\u6709\u6548\u8bd7\u6b4c\u4e0d\u8db34%\uff09\uff0c\u800c\u6df7\u5408\u9a8c\u8bc1\u7cfb\u7edf\u5c06\u6027\u80fd\u6062\u590d\u81f373.1%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7eafLLM\u5728\u97f3\u97f5\u5b66\u4efb\u52a1\u4e0a\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u800c\u6df7\u5408\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u7b97\u6cd5\u9a8c\u8bc1\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1b\u7814\u7a76\u53d1\u5e03\u7684\u7cfb\u7edf\u53ca\u5305\u542b4\u4e07\u591a\u4e2a\u62bc\u97f5\u7684\u6e05\u6d01\u8bed\u6599\u5e93\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u97f5\u5f8b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5f3a\u8c03\u4e86\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u4e0eLLM\u7ed3\u5408\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2601.09228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09228", "abs": "https://arxiv.org/abs/2601.09228", "authors": ["Fan Liu", "Ting Wu", "Chuanyi Zhang", "Liang Yao", "Xing Ma", "Yuhui Zheng"], "title": "Disentangle Object and Non-object Infrared Features via Language Guidance", "comment": null, "summary": "Infrared object detection focuses on identifying and locating objects in complex environments (\\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\\textsuperscript{3}FD (83.7\\% mAP), FLIR (86.1\\% mAP). Our code will be publicly available once the paper is accepted.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u8303\u5f0f\u7528\u4e8e\u7ea2\u5916\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u5f15\u5165\u6587\u672c\u76d1\u7763\u6765\u5f15\u5bfc\u5bf9\u8c61\u4e0e\u975e\u5bf9\u8c61\u7279\u5f81\u7684\u89e3\u8026\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7ea2\u5916\u76ee\u6807\u68c0\u6d4b\u5728\u9ed1\u6697\u3001\u96ea\u5929\u3001\u96e8\u5929\u7b49\u590d\u6742\u73af\u5883\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u7531\u4e8e\u7ea2\u5916\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u4f4e\u3001\u8fb9\u7f18\u4fe1\u606f\u5f31\uff0c\u96be\u4ee5\u63d0\u53d6\u5177\u6709\u5224\u522b\u6027\u7684\u76ee\u6807\u7279\u5f81\uff0c\u5bfc\u81f4\u68c0\u6d4b\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u8303\u5f0f\uff0c\u5305\u542b\u8bed\u4e49\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\u5c06\u76ee\u6807\u7279\u5f81\u4e0e\u5bf9\u5e94\u6587\u672c\u7279\u5f81\u5bf9\u9f50\uff0c\u4ee5\u53ca\u5bf9\u8c61\u7279\u5f81\u89e3\u8026\u6a21\u5757\u901a\u8fc7\u6700\u5c0f\u5316\u76f8\u5173\u6027\u6765\u5206\u79bb\u6587\u672c\u5bf9\u9f50\u7684\u76ee\u6807\u7279\u5f81\u4e0e\u975e\u76ee\u6807\u7279\u5f81\uff0c\u6700\u7ec8\u5c06\u89e3\u8026\u540e\u7684\u76ee\u6807\u7279\u5f81\u8f93\u5165\u68c0\u6d4b\u5934\u3002", "result": "\u5728M\u00b3FD\u548cFLIR\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u5206\u522b\u8fbe\u523083.7%\u548c86.1%\u7684mAP\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ea2\u5916\u76ee\u6807\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5f15\u5165\u6587\u672c\u76d1\u7763\u8fdb\u884c\u7279\u5f81\u89e3\u8026\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7ea2\u5916\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u589e\u5f3a\u7279\u5f81\u5224\u522b\u6027\u548c\u51cf\u5c11\u566a\u58f0\u7279\u5f81\uff0c\u663e\u8457\u6539\u5584\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2601.09696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09696", "abs": "https://arxiv.org/abs/2601.09696", "authors": ["Shan Randhawa", "Agha Ali Raza", "Kentaro Toyama", "Julie Hui", "Mustafa Naseem"], "title": "Empathy Applicability Modeling for General Health Queries", "comment": "In Submission to ACL", "summary": "LLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors' responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We release a benchmark of real patient queries, dual-annotated by Humans and GPT-4o. In the subset with human consensus, we also observe substantial human-GPT alignment. To validate EAF, we train classifiers on human-labeled and GPT-only annotations to predict empathy applicability, achieving strong performance and outperforming the heuristic and zero-shot LLM baselines. Error analysis highlights persistent challenges: implicit distress, clinical-severity ambiguity, and contextual hardship, underscoring the need for multi-annotator modeling, clinician-in-the-loop calibration, and culturally diverse annotation. EAF provides a framework for identifying empathy needs before response generation, establishes a benchmark for anticipatory empathy modeling, and enables supporting empathetic communication in asynchronous healthcare.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5171\u60c5\u9002\u7528\u6027\u6846\u67b6\uff08EAF\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7406\u8bba\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u751f\u6210\u533b\u751f\u56de\u590d\u4e4b\u524d\u8bc6\u522b\u60a3\u8005\u67e5\u8be2\u4e2d\u7684\u5171\u60c5\u9700\u6c42\uff0c\u5e76\u5efa\u7acb\u4e86\u7528\u4e8e\u9884\u6d4b\u6027\u5171\u60c5\u5efa\u6a21\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u878d\u5165\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f46\u901a\u5e38\u7f3a\u4e4f\u4e34\u5e8a\u5171\u60c5\u8fd9\u4e00\u533b\u60a3\u6c9f\u901a\u7684\u5173\u952e\u8981\u7d20\u3002\u73b0\u6709NLP\u6846\u67b6\u4e3b\u8981\u5173\u6ce8\u5bf9\u533b\u751f\u56de\u590d\u4e2d\u7684\u5171\u60c5\u8fdb\u884c\u53cd\u5e94\u6027\u6807\u6ce8\uff0c\u800c\u5728\u9884\u6d4b\u6027\u5efa\u6a21\u5171\u60c5\u9700\u6c42\u65b9\u9762\u652f\u6301\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u4e00\u822c\u5065\u5eb7\u67e5\u8be2\u4e2d\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u5171\u60c5\u9002\u7528\u6027\u6846\u67b6\uff08EAF\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u4e34\u5e8a\u3001\u4e0a\u4e0b\u6587\u548c\u8bed\u8a00\u7ebf\u7d22\u7684\u7406\u8bba\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u6839\u636e\u60c5\u611f\u53cd\u5e94\u548c\u89e3\u91ca\u7684\u9002\u7528\u6027\u5bf9\u60a3\u8005\u67e5\u8be2\u8fdb\u884c\u5206\u7c7b\u3002\u7814\u7a76\u8005\u53d1\u5e03\u4e86\u771f\u5b9e\u60a3\u8005\u67e5\u8be2\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7531\u4eba\u7c7b\u548cGPT-4o\u8fdb\u884c\u53cc\u91cd\u6807\u6ce8\uff0c\u5e76\u5728\u4eba\u7c7b\u5171\u8bc6\u5b50\u96c6\u4e0a\u8bad\u7ec3\u5206\u7c7b\u5668\u6765\u9884\u6d4b\u5171\u60c5\u9002\u7528\u6027\u3002", "result": "\u5728\u4eba\u7c7b\u5171\u8bc6\u5b50\u96c6\u4e2d\u89c2\u5bdf\u5230\u663e\u8457\u7684\u4eba\u7c7b-GPT\u5bf9\u9f50\u3002\u57fa\u4e8e\u4eba\u7c7b\u6807\u6ce8\u548cGPT-only\u6807\u6ce8\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u5728\u9884\u6d4b\u5171\u60c5\u9002\u7528\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u96f6\u6837\u672cLLM\u57fa\u7ebf\u3002\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u9690\u5f0f\u56f0\u6270\u3001\u4e34\u5e8a\u4e25\u91cd\u6027\u6a21\u7cca\u6027\u548c\u4e0a\u4e0b\u6587\u56f0\u96be\u7b49\u6301\u7eed\u6311\u6218\u3002", "conclusion": "EAF\u4e3a\u5728\u56de\u590d\u751f\u6210\u524d\u8bc6\u522b\u5171\u60c5\u9700\u6c42\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5efa\u7acb\u4e86\u9884\u6d4b\u6027\u5171\u60c5\u5efa\u6a21\u7684\u57fa\u51c6\uff0c\u5e76\u652f\u6301\u5f02\u6b65\u533b\u7597\u4e2d\u7684\u5171\u60c5\u6c9f\u901a\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6807\u6ce8\u8005\u5efa\u6a21\u3001\u4e34\u5e8a\u533b\u751f\u53c2\u4e0e\u6821\u51c6\u548c\u6587\u5316\u591a\u6837\u6027\u6807\u6ce8\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u672a\u6765\u4e34\u5e8aNLP\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2601.09255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09255", "abs": "https://arxiv.org/abs/2601.09255", "authors": ["Yibo Zhao", "Hengjia Li", "Xiaofei He", "Boxi Wu"], "title": "PhyRPR: Training-Free Physics-Constrained Video Generation", "comment": null, "summary": "Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,\\textit{PhyRPR}:\\textit{Phy\\uline{R}eason}--\\textit{Phy\\uline{P}lan}--\\textit{Phy\\uline{R}efine}, which decouples physical understanding from visual synthesis. Specifically, \\textit{PhyReason} uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; \\textit{PhyPlan} deterministically synthesizes a controllable coarse motion scaffold; and \\textit{PhyRefine} injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PhyRPR\uff0c\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u5c06\u7269\u7406\u7406\u89e3\u4e0e\u89c6\u89c9\u5408\u6210\u89e3\u8026\u6765\u89e3\u51b3\u6269\u6563\u89c6\u9891\u751f\u6210\u6a21\u578b\u96be\u4ee5\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7269\u7406\u63a8\u7406\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u5916\u89c2\u7ec6\u5316\u7684\u5206\u79bb\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u751f\u6210\u8fc7\u7a0b\u4e2d\u5bf9\u7269\u7406\u7684\u663e\u5f0f\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u5408\u6210\u89c6\u89c9\u4e0a\u5408\u7406\u7684\u89c6\u9891\uff0c\u4f46\u5f80\u5f80\u96be\u4ee5\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\u3002\u4e3b\u8981\u539f\u56e0\u662f\u5927\u591a\u6570\u65b9\u6cd5\u91c7\u7528\u5355\u9636\u6bb5\u8bbe\u8ba1\uff0c\u5c06\u9ad8\u5c42\u7269\u7406\u7406\u89e3\u4e0e\u4f4e\u5c42\u89c6\u89c9\u5408\u6210\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u4f7f\u5f97\u9700\u8981\u663e\u5f0f\u7269\u7406\u63a8\u7406\u7684\u5185\u5bb9\u751f\u6210\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u514d\u8bad\u7ec3\u7684\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebfPhyRPR\uff0c\u5305\u542b\u7269\u7406\u63a8\u7406\u3001\u7269\u7406\u89c4\u5212\u548c\u7269\u7406\u7ec6\u5316\u4e09\u4e2a\u9636\u6bb5\u3002PhyReason\u4f7f\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u7269\u7406\u72b6\u6001\u63a8\u7406\u548c\u56fe\u50cf\u751f\u6210\u5668\u5408\u6210\u5173\u952e\u5e27\uff1bPhyPlan\u786e\u5b9a\u6027\u5730\u5408\u6210\u53ef\u63a7\u7684\u7c97\u7565\u8fd0\u52a8\u652f\u67b6\uff1bPhyRefine\u901a\u8fc7\u6f5c\u5728\u878d\u5408\u7b56\u7565\u5c06\u652f\u67b6\u6ce8\u5165\u6269\u6563\u91c7\u6837\u4e2d\uff0c\u5728\u4fdd\u7559\u89c4\u5212\u52a8\u6001\u7684\u540c\u65f6\u7ec6\u5316\u5916\u89c2\u3002", "result": "\u5728\u7269\u7406\u7ea6\u675f\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u63d0\u9ad8\u4e86\u7269\u7406\u5408\u7406\u6027\u548c\u8fd0\u52a8\u53ef\u63a7\u6027\u3002\u4e09\u9636\u6bb5\u8bbe\u8ba1\u4f7f\u5f97\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u80fd\u591f\u5b9e\u73b0\u663e\u5f0f\u7684\u7269\u7406\u63a7\u5236\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u89e3\u8026\u7269\u7406\u7406\u89e3\u4e0e\u89c6\u89c9\u5408\u6210\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u4e2d\u7684\u7269\u7406\u7ea6\u675f\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002\u5206\u9636\u6bb5\u8bbe\u8ba1\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u7269\u7406\u5408\u7406\u6027\uff0c\u8fd8\u589e\u5f3a\u4e86\u8fd0\u52a8\u53ef\u63a7\u6027\uff0c\u4e3a\u9700\u8981\u663e\u5f0f\u7269\u7406\u63a8\u7406\u7684\u751f\u6210\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.09298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09298", "abs": "https://arxiv.org/abs/2601.09298", "authors": ["Lianying Chao", "Haoran Cai", "Xubin Li", "Kai Zhang", "Sijie Wu", "Rui Xu"], "title": "Multi-Modal LLM based Image Captioning in ICT: Bridging the Gap Between General and Industry Domain", "comment": null, "summary": "In the information and communications technology (ICT) industry, training a domain-specific large language model (LLM) or constructing a retrieval-augmented generation system requires a substantial amount of high-value domain knowledge. However, the knowledge is not only hidden in the textual modality but also in the image modality. Traditional methods can parse text from domain documents but dont have image captioning ability. Multi-modal LLM (MLLM) can understand images, but they do not have sufficient domain knowledge. To address the above issues, this paper proposes a multi-stage progressive training strategy to train a Domain-specific Image Captioning Model (DICModel) in ICT, and constructs a standard evaluation system to validate the performance of DICModel. Specifically, this work first synthesizes about 7K image-text pairs by combining the Mermaid tool and LLMs, which are used for the first-stage supervised-fine-tuning (SFT) of DICModel. Then, ICT-domain experts manually annotate about 2K image-text pairs for the second-stage SFT of DICModel. Finally, experts and LLMs jointly synthesize about 1.5K visual question answering data for the instruction-based SFT. Experimental results indicate that our DICModel with only 7B parameters performs better than other state-of-the-art models with 32B parameters. Compared to the SOTA models with 7B and 32B parameters, our DICModel increases the BLEU metric by approximately 56.8% and 20.8%, respectively. On the objective questions constructed by ICT domain experts, our DICModel outperforms Qwen2.5-VL 32B by 1% in terms of accuracy rate. In summary, this work can efficiently and accurately extract the logical text from images, which is expected to promote the development of multimodal models in the ICT domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u8bad\u7ec3ICT\u9886\u57df\u7684\u4e13\u7528\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\uff08DICModel\uff09\uff0c\u8be5\u6a21\u578b\u5728\u4ec5\u4f7f\u75287B\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u8d85\u8d8a\u4e8632B\u53c2\u6570\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9886\u57df\u56fe\u50cf\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5728ICT\u884c\u4e1a\u4e2d\uff0c\u8bad\u7ec3\u9886\u57df\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6216\u6784\u5efa\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u9700\u8981\u5927\u91cf\u9ad8\u4ef7\u503c\u9886\u57df\u77e5\u8bc6\uff0c\u4f46\u8fd9\u4e9b\u77e5\u8bc6\u4e0d\u4ec5\u9690\u85cf\u5728\u6587\u672c\u6a21\u6001\u4e2d\uff0c\u4e5f\u5b58\u5728\u4e8e\u56fe\u50cf\u6a21\u6001\u4e2d\u3002\u4f20\u7edf\u65b9\u6cd5\u53ea\u80fd\u89e3\u6790\u9886\u57df\u6587\u6863\u4e2d\u7684\u6587\u672c\u800c\u7f3a\u4e4f\u56fe\u50cf\u63cf\u8ff0\u80fd\u529b\uff0c\u800c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u7406\u89e3\u56fe\u50cf\uff0c\u5374\u7f3a\u4e4f\u8db3\u591f\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u6765\u8bad\u7ec3ICT\u9886\u57df\u7684\u4e13\u7528\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u3002\u9996\u5148\u4f7f\u7528Mermaid\u5de5\u5177\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5408\u6210\u7ea67K\u56fe\u50cf-\u6587\u672c\u5bf9\u8fdb\u884c\u7b2c\u4e00\u9636\u6bb5\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u7531ICT\u9886\u57df\u4e13\u5bb6\u624b\u52a8\u6807\u6ce8\u7ea62K\u56fe\u50cf-\u6587\u672c\u5bf9\u8fdb\u884c\u7b2c\u4e8c\u9636\u6bb5\u76d1\u7763\u5fae\u8c03\uff0c\u6700\u540e\u4e13\u5bb6\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u8054\u5408\u5408\u6210\u7ea61.5K\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u8fdb\u884c\u57fa\u4e8e\u6307\u4ee4\u7684\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f7f\u75287B\u53c2\u6570\u7684DICModel\u6027\u80fd\u4f18\u4e8e\u5176\u4ed632B\u53c2\u6570\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002\u4e0e7B\u548c32B\u53c2\u6570\u7684SOTA\u6a21\u578b\u76f8\u6bd4\uff0cDICModel\u5c06BLEU\u6307\u6807\u5206\u522b\u63d0\u9ad8\u4e86\u7ea656.8%\u548c20.8%\u3002\u5728ICT\u9886\u57df\u4e13\u5bb6\u6784\u5efa\u7684\u5ba2\u89c2\u95ee\u9898\u4e0a\uff0cDICModel\u5728\u51c6\u786e\u7387\u4e0a\u6bd4Qwen2.5-VL 32B\u9ad8\u51fa1%\u3002", "conclusion": "\u8be5\u7814\u7a76\u80fd\u591f\u9ad8\u6548\u51c6\u786e\u5730\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u903b\u8f91\u6587\u672c\uff0c\u6709\u671b\u4fc3\u8fdbICT\u9886\u57df\u591a\u6a21\u6001\u6a21\u578b\u7684\u53d1\u5c55\u3002\u63d0\u51fa\u7684\u591a\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u548c\u6807\u51c6\u8bc4\u4f30\u7cfb\u7edf\u4e3a\u9886\u57df\u4e13\u7528\u56fe\u50cf\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u5728\u6709\u9650\u53c2\u6570\u89c4\u6a21\u4e0b\u901a\u8fc7\u9488\u5bf9\u6027\u8bad\u7ec3\u53ef\u4ee5\u83b7\u5f97\u8d85\u8d8a\u66f4\u5927\u901a\u7528\u6a21\u578b\u7684\u9886\u57df\u6027\u80fd\u3002"}}
{"id": "2601.09350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09350", "abs": "https://arxiv.org/abs/2601.09350", "authors": ["Mingyu Jeon", "Sungjin Han", "Jinkwon Hwang", "Minchol Kwon", "Jonghee Kim", "Junyeong Kim"], "title": "See More, Store Less: Memory-Efficient Resolution for Video Moment Retrieval", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSMORE\u6846\u67b6\uff0c\u901a\u8fc7\u67e5\u8be2\u5f15\u5bfc\u7684\u8bed\u4e49\u7f16\u7801\u3001\u91cd\u8981\u6027\u8c03\u5236\u548c\u81ea\u9002\u5e94\u5e27\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u9ad8\u4fe1\u606f\u5206\u8fa8\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u7684\u5185\u5b58\u6548\u7387\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u8bc6\u522b\u548c\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u89c6\u9891\u76f8\u5173\u4efb\u52a1\u4ecd\u9762\u4e34\u5185\u5b58\u9650\u5236\u7684\u6311\u6218\uff0c\u73b0\u6709\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u5e27\u91c7\u6837\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\uff0c\u7279\u522b\u662f\u5728\u957f\u89c6\u9891\u5904\u7406\u4e2d\u3002", "method": "SMORE\u6846\u67b6\u91c7\u7528\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a\u4f7f\u7528\u67e5\u8be2\u5f15\u5bfc\u7684\u6807\u9898\u7f16\u7801\u4e0e\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u7684\u8bed\u4e49\uff1b\u5e94\u7528\u67e5\u8be2\u611f\u77e5\u7684\u91cd\u8981\u6027\u8c03\u5236\u7a81\u51fa\u76f8\u5173\u7247\u6bb5\uff1b\u81ea\u9002\u5e94\u538b\u7f29\u5e27\u4ee5\u4fdd\u7559\u5173\u952e\u5185\u5bb9\u540c\u65f6\u51cf\u5c11\u5197\u4f59\uff0c\u5b9e\u73b0\u9ad8\u6548\u89c6\u9891\u7406\u89e3\u800c\u4e0d\u8d85\u51fa\u5185\u5b58\u9884\u7b97\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cSMORE\u5728QVHighlights\u3001Charades-STA\u548cActivityNet-Captions\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u4fe1\u606f\u5206\u8fa8\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5185\u5b58\u6548\u7387\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u548c\u81ea\u9002\u5e94\u538b\u7f29\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u89c6\u9891\u5904\u7406\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u590d\u6742\u7684\u591a\u6a21\u6001\u89c6\u9891\u5206\u6790\u4efb\u52a1\u3002"}}
{"id": "2601.09416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09416", "abs": "https://arxiv.org/abs/2601.09416", "authors": ["Yaxi Chen", "Zi Ye", "Shaheer U. Saeed", "Oliver Yu", "Simin Ni", "Jie Huang", "Yipeng Hu"], "title": "Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma Histology Classification", "comment": null, "summary": "Osteosarcoma (OS) is an aggressive primary bone malignancy. Accurate histopathological assessment of viable versus non-viable tumor regions after neoadjuvant chemotherapy is critical for prognosis and treatment planning, yet manual evaluation remains labor-intensive, subjective, and prone to inter-observer variability. Recent advances in digital pathology have enabled automated necrosis quantification. Evaluating on test data, independently sampled on patient-level, revealed that the deep learning model performance dropped significantly from the tile-level generalization ability reported in previous studies. First, this work proposes the use of radiomic features as additional input in model training. We show that, despite that they are derived from the images, such a multimodal input effectively improved the classification performance, in addition to its added benefits in interpretability. Second, this work proposes to optimize two binary classification tasks with hierarchical classes (i.e. tumor-vs-non-tumor and viable-vs-non-viable), as opposed to the alternative ``flat'' three-class classification task (i.e. non-tumor, non-viable tumor, viable tumor), thereby enabling a hierarchical loss. We show that such a hierarchical loss, with trainable weightings between the two tasks, the per-class performance can be improved significantly. Using the TCIA OS Tumor Assessment dataset, we experimentally demonstrate the benefits from each of the proposed new approaches and their combination, setting a what we consider new state-of-the-art performance on this open dataset for this application. Code and trained models: https://github.com/YaxiiC/RadiomicsOS.git.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u548c\u5206\u5c42\u635f\u5931\u51fd\u6570\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9aa8\u8089\u7624\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u80bf\u7624\u533a\u57df\uff08\u5b58\u6d3b\u4e0e\u975e\u5b58\u6d3b\uff09\u7684\u81ea\u52a8\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u60a3\u8005\u7ea7\u522b\u72ec\u7acb\u91c7\u6837\u6d4b\u8bd5\u6570\u636e\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u9aa8\u8089\u7624\u65b0\u8f85\u52a9\u5316\u7597\u540e\u5b58\u6d3b\u4e0e\u975e\u5b58\u6d3b\u80bf\u7624\u533a\u57df\u7684\u51c6\u786e\u7ec4\u7ec7\u75c5\u7406\u5b66\u8bc4\u4f30\u5bf9\u9884\u540e\u548c\u6cbb\u7597\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4eba\u5de5\u8bc4\u4f30\u5b58\u5728\u52b3\u52a8\u5bc6\u96c6\u3001\u4e3b\u89c2\u6027\u5f3a\u548c\u89c2\u5bdf\u8005\u95f4\u53d8\u5f02\u5927\u7684\u95ee\u9898\u3002\u5148\u524d\u7814\u7a76\u5728\u74e6\u7247\u7ea7\u522b\u6cdb\u5316\u80fd\u529b\u826f\u597d\uff0c\u4f46\u5728\u60a3\u8005\u7ea7\u522b\u72ec\u7acb\u91c7\u6837\u6d4b\u8bd5\u6570\u636e\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u5173\u952e\u65b9\u6cd5\u6539\u8fdb\uff1a\u9996\u5148\u5f15\u5165\u4ece\u56fe\u50cf\u884d\u751f\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u4f5c\u4e3a\u591a\u6a21\u6001\u8f93\u5165\uff0c\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u5e76\u63d0\u5347\u53ef\u89e3\u91ca\u6027\uff1b\u5176\u6b21\u91c7\u7528\u5206\u5c42\u5206\u7c7b\u7b56\u7565\uff0c\u5c06\u5355\u4e00\u7684\u4e09\u5206\u7c7b\u4efb\u52a1\u5206\u89e3\u4e3a\u80bf\u7624\u4e0e\u975e\u80bf\u7624\u3001\u5b58\u6d3b\u4e0e\u975e\u5b58\u6d3b\u4e24\u4e2a\u4e8c\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u53ef\u8bad\u7ec3\u6743\u91cd\u7684\u5206\u5c42\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5316\u5404\u7c7b\u522b\u6027\u80fd\u3002", "result": "\u5728TCIA\u9aa8\u8089\u7624\u80bf\u7624\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u7684\u5f15\u5165\u548c\u5206\u5c42\u635f\u5931\u51fd\u6570\u7684\u5e94\u7528\u5747\u80fd\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u4e24\u8005\u7ed3\u5408\u5b9e\u73b0\u4e86\u8be5\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8be5\u5e94\u7528\u7684\u6700\u4f73\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u60a3\u8005\u7ea7\u522b\u6d4b\u8bd5\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u8f93\u5165\u548c\u5206\u5c42\u5b66\u4e60\u7b56\u7565\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7ec4\u7ec7\u75c5\u7406\u5b66\u81ea\u52a8\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u6846\u67b6\u3002\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u7684\u5f15\u5165\u4e0d\u4ec5\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u8fd8\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u5206\u5c42\u635f\u5931\u51fd\u6570\u5219\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u4f18\u5316\u4e86\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4e3a\u7c7b\u4f3c\u533b\u5b66\u56fe\u50cf\u5206\u6790\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u501f\u9274\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.09430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09430", "abs": "https://arxiv.org/abs/2601.09430", "authors": ["Rui Zhu", "Xin Shen", "Shuchen Wu", "Chenxi Miao", "Xin Yu", "Yang Li", "Weikang Li", "Deguo Xia", "Jizhou Huang"], "title": "Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs", "comment": null, "summary": "Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Video-MSR\uff0c\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u52a8\u6001\u89c6\u9891\u573a\u666f\u4e2d\u591a\u8df3\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5e76\u6784\u5efa\u4e86MSR-9K\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03Qwen-VL\u6a21\u578b\u5b9e\u73b0\u4e86+7.82%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5355\u6b65\u611f\u77e5\u5230\u5224\u65ad\u4efb\u52a1\uff0c\u800c\u9700\u8981\u590d\u6742\u89c6\u89c9-\u7a7a\u95f4\u903b\u8f91\u94fe\u7684\u573a\u666f\u7814\u7a76\u4e25\u91cd\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u52a8\u6001\u89c6\u9891\u4e2d\u7684\u591a\u8df3\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u5b58\u5728\u663e\u8457\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86Video-MSR\u57fa\u51c6\uff0c\u5305\u542b\u7ea6\u675f\u5b9a\u4f4d\u3001\u94fe\u5f0f\u53c2\u8003\u68c0\u7d22\u3001\u8def\u5f84\u89c4\u5212\u548c\u53cd\u4e8b\u5b9e\u7269\u7406\u63a8\u7406\u56db\u4e2a\u4efb\u52a1\uff0c\u5305\u542b3,052\u4e2a\u9ad8\u8d28\u91cf\u89c6\u9891\u5b9e\u4f8b\u548c4,993\u4e2a\u95ee\u7b54\u5bf9\uff0c\u91c7\u7528\u7ed3\u5408\u5148\u8fdb\u6a21\u578b\u751f\u6210\u4e0e\u4e25\u683c\u4eba\u5de5\u9a8c\u8bc1\u7684\u53ef\u6269\u5c55\u89c6\u89c9\u57fa\u7840\u6d41\u7a0b\u6784\u5efa\uff0c\u5e76\u8fdb\u4e00\u6b65\u6784\u5efa\u4e86MSR-9K\u4e13\u7528\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u7528\u4e8e\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u5bf920\u4e2a\u6700\u5148\u8fdbMLLM\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u663e\u8457\u5c40\u9650\u6027\uff1a\u6a21\u578b\u5728\u8868\u5c42\u611f\u77e5\u65b9\u9762\u8868\u73b0\u719f\u7ec3\uff0c\u4f46\u5728MSR\u4efb\u52a1\u4e2d\u6027\u80fd\u660e\u663e\u4e0b\u964d\uff0c\u7ecf\u5e38\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u51fa\u73b0\u7a7a\u95f4\u8ff7\u5931\u548c\u5e7b\u89c9\u95ee\u9898\uff1b\u901a\u8fc7MSR-9K\u6570\u636e\u96c6\u5fae\u8c03Qwen-VL\u6a21\u578b\uff0c\u5728Video-MSR\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86+7.82%\u7684\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u591a\u8df3\u7a7a\u95f4\u6307\u4ee4\u6570\u636e\u7684\u6709\u6548\u6027\u5f97\u5230\u9a8c\u8bc1\uff0cVideo-MSR\u57fa\u51c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524dMLLM\u5728\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u4e13\u95e8\u6307\u4ee4\u8c03\u4f18\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u591a\u8df3\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2601.09433", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09433", "abs": "https://arxiv.org/abs/2601.09433", "authors": ["David Reid", "Ognjen Arandjelovic"], "title": "Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?", "comment": null, "summary": "Automated analysis of ancient coins has the potential to help researchers extract more historical insights from large collections of coins and to help collectors understand what they are buying or selling. Recent research in this area has shown promise in focusing on identification of semantic elements as they are commonly depicted on ancient coins, by using convolutional neural networks (CNNs). This paper is the first to apply the recently proposed Vision Transformer (ViT) deep learning architecture to the task of identification of semantic elements on coins, using fully automatic learning from multi-modal data (images and unstructured text). This article summarises previous research in the area, discusses the training and implementation of ViT and CNN models for ancient coins analysis and provides an evaluation of their performance. The ViT models were found to outperform the newly trained CNN models in accuracy.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06Vision Transformer\u67b6\u6784\u5e94\u7528\u4e8e\u53e4\u94b1\u5e01\u8bed\u4e49\u5143\u7d20\u8bc6\u522b\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u81ea\u52a8\u5b66\u4e60\uff0c\u53d1\u73b0ViT\u6a21\u578b\u5728\u51c6\u786e\u7387\u4e0a\u8d85\u8d8a\u4e86\u65b0\u8bad\u7ec3\u7684CNN\u6a21\u578b\u3002", "motivation": "\u53e4\u94b1\u5e01\u81ea\u52a8\u5206\u6790\u6709\u52a9\u4e8e\u7814\u7a76\u4eba\u5458\u4ece\u5927\u91cf\u94b1\u5e01\u6536\u85cf\u4e2d\u63d0\u53d6\u66f4\u591a\u5386\u53f2\u89c1\u89e3\uff0c\u5e76\u5e2e\u52a9\u6536\u85cf\u8005\u7406\u89e3\u5176\u4ea4\u6613\u5bf9\u8c61\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u94b1\u5e01\u4e0a\u7684\u8bed\u4e49\u5143\u7d20\uff0c\u4f46\u5c1a\u672a\u63a2\u7d22\u6700\u8fd1\u63d0\u51fa\u7684Vision Transformer\u67b6\u6784\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u672c\u7814\u7a76\u9996\u6b21\u5c06Vision Transformer\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5e94\u7528\u4e8e\u53e4\u94b1\u5e01\u8bed\u4e49\u5143\u7d20\u8bc6\u522b\u4efb\u52a1\uff0c\u91c7\u7528\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u591a\u6a21\u6001\u6570\u636e\u5b66\u4e60\u65b9\u6cd5\uff0c\u540c\u65f6\u5904\u7406\u56fe\u50cf\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u3002\u7814\u7a76\u8fd8\u8bad\u7ec3\u4e86CNN\u6a21\u578b\u4f5c\u4e3a\u5bf9\u6bd4\u57fa\u51c6\uff0c\u5e76\u8be6\u7ec6\u8ba8\u8bba\u4e86ViT\u548cCNN\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u5b9e\u73b0\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cVision Transformer\u6a21\u578b\u5728\u53e4\u94b1\u5e01\u8bed\u4e49\u5143\u7d20\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u8d85\u8d8a\u4e86\u65b0\u8bad\u7ec3\u7684CNN\u6a21\u578b\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u4e24\u79cd\u67b6\u6784\u7684\u6027\u80fd\u5bf9\u6bd4\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86ViT\u5728\u8be5\u7279\u5b9a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u4f18\u8d8a\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u8868\u660eVision Transformer\u67b6\u6784\u5728\u53e4\u94b1\u5e01\u5206\u6790\u9886\u57df\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u6587\u5316\u9057\u4ea7\u6570\u5b57\u5316\u548c\u81ea\u52a8\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\u56fe\u50cf\u4e0e\u6587\u672c\u4fe1\u606f\uff0c\u4e3a\u590d\u6742\u5386\u53f2\u6587\u7269\u7684\u667a\u80fd\u8bc6\u522b\u5f00\u8f9f\u4e86\u521b\u65b0\u65b9\u5411\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22Transformer\u67b6\u6784\u5728\u6587\u7269\u5206\u6790\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2601.09449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09449", "abs": "https://arxiv.org/abs/2601.09449", "authors": ["Darya Baranouskaya", "Andrea Cavallaro"], "title": "PrivLEX: Detecting legal concepts in images through Vision-Language Models", "comment": null, "summary": "We present PrivLEX, a novel image privacy classifier that grounds its decisions in legally defined personal data concepts. PrivLEX is the first interpretable privacy classifier aligned with legal concepts that leverages the recognition capabilities of Vision-Language Models (VLMs). PrivLEX relies on zero-shot VLM concept detection to provide interpretable classification through a label-free Concept Bottleneck Model, without requiring explicit concept labels during training. We demonstrate PrivLEX's ability to identify personal data concepts that are present in images. We further analyse the sensitivity of such concepts as perceived by human annotators of image privacy datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPrivLEX\uff0c\u4e00\u79cd\u57fa\u4e8e\u6cd5\u5f8b\u5b9a\u4e49\u7684\u4e2a\u4eba\u6570\u636e\u6982\u5ff5\u8fdb\u884c\u51b3\u7b56\u7684\u65b0\u578b\u56fe\u50cf\u9690\u79c1\u5206\u7c7b\u5668\uff0c\u8fd9\u662f\u9996\u4e2a\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u80fd\u529b\u5e76\u4e0e\u6cd5\u5f8b\u6982\u5ff5\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u9690\u79c1\u5206\u7c7b\u5668\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u9690\u79c1\u5206\u7c7b\u5668\u7f3a\u4e4f\u4e0e\u6cd5\u5f8b\u5b9a\u4e49\u7684\u4e2a\u4eba\u6570\u636e\u6982\u5ff5\u7684\u5bf9\u9f50\uff0c\u5bfc\u81f4\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\u4e14\u96be\u4ee5\u89e3\u91ca\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9690\u79c1\u4fdd\u62a4\u7684\u5b9e\u9645\u6cd5\u5f8b\u9700\u6c42\u3002", "method": "PrivLEX\u91c7\u7528\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6982\u5ff5\u68c0\u6d4b\u6280\u672f\uff0c\u901a\u8fc7\u65e0\u6807\u7b7e\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u5b9e\u73b0\u53ef\u89e3\u91ca\u5206\u7c7b\uff0c\u65e0\u9700\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u663e\u5f0f\u6982\u5ff5\u6807\u6ce8\uff0c\u5c06VLM\u8bc6\u522b\u80fd\u529b\u4e0e\u6cd5\u5f8b\u6982\u5ff5\u6846\u67b6\u76f8\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660ePrivLEX\u80fd\u591f\u6709\u6548\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684\u4e2a\u4eba\u6570\u636e\u6982\u5ff5\uff0c\u5e76\u5206\u6790\u4e86\u4eba\u7c7b\u6807\u6ce8\u8005\u5bf9\u56fe\u50cf\u9690\u79c1\u6570\u636e\u96c6\u4e2d\u6b64\u7c7b\u6982\u5ff5\u654f\u611f\u5ea6\u7684\u611f\u77e5\u5dee\u5f02\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6cd5\u5f8b\u6982\u5ff5\u5bf9\u9f50\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9690\u79c1\u4fdd\u62a4\u9886\u57df\u63d0\u4f9b\u4e86\u9996\u4e2a\u6cd5\u5f8b\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7VLM\u7684\u96f6\u6837\u672c\u80fd\u529b\u5b9e\u73b0\u4e86\u65e0\u9700\u6807\u6ce8\u7684\u6982\u5ff5\u68c0\u6d4b\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u5e94\u7528\u7684\u900f\u660e\u51b3\u7b56\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.09497", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09497", "abs": "https://arxiv.org/abs/2601.09497", "authors": ["Ritabrata Chakraborty", "Hrishit Mitra", "Shivakumara Palaiahnakote", "Umapada Pal"], "title": "Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity", "comment": "15 pages, 4 figures, 6 tables", "summary": "Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This reveals a clear structure in CD-OD: transfer within the same setting type is relatively stable, while transfer across setting types drops substantially and is often asymmetric. The most severe breakdowns occur when transferring from specific sources to agnostic targets, and persist after open-label alignment, indicating that domain shift dominates in the hardest regimes. To disentangle domain shift from label mismatch, we compare closed-label transfer with an open-label protocol that maps predicted classes to the nearest target label using CLIP similarity. Open-label evaluation yields consistent but bounded gains, and many corrected cases correspond to semantic near-misses supported by the image evidence. Overall, we provide a principled characterization of CD-OD under setting specificity and practical guidance for evaluating detectors under distribution shift. Code will be released at \\href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bbe\u5b9a\u7279\u5f02\u6027\u89c6\u89d2\u7cfb\u7edf\u5206\u6790\u4e86\u8de8\u6570\u636e\u96c6\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u5728\u76f8\u540c\u8bbe\u5b9a\u7c7b\u578b\u5185\u8fc1\u79fb\u76f8\u5bf9\u7a33\u5b9a\u800c\u8de8\u7c7b\u578b\u8fc1\u79fb\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u89c4\u5f8b\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u5206\u5e03\u504f\u79fb\u4e0b\u68c0\u6d4b\u5668\u7684\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u5206\u5e03\u5185\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8bbe\u5b9a\u7279\u5f02\u6027\u7684\u89c6\u89d2\u6765\u7cfb\u7edf\u5206\u6790\u8de8\u6570\u636e\u96c6\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u63a2\u7a76\u68c0\u6d4b\u5668\u5728\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u96c6\u95f4\u8fc1\u79fb\u65f6\u7684\u6027\u80fd\u53d8\u5316\u89c4\u5f8b\uff0c\u7279\u522b\u662f\u533a\u5206\u9886\u57df\u504f\u79fb\u548c\u6807\u7b7e\u4e0d\u5339\u914d\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u5c06\u57fa\u51c6\u6570\u636e\u96c6\u5206\u4e3a\u8bbe\u5b9a\u65e0\u5173\u6570\u636e\u96c6\uff08\u5305\u542b\u591a\u6837\u5316\u65e5\u5e38\u573a\u666f\uff09\u548c\u8bbe\u5b9a\u7279\u5b9a\u6570\u636e\u96c6\uff08\u5c40\u9650\u4e8e\u72ed\u7a84\u73af\u5883\uff09\uff0c\u5e76\u8bc4\u4f30\u6807\u51c6\u68c0\u6d4b\u5668\u5bb6\u65cf\u5728\u6240\u6709\u8bad\u7ec3-\u6d4b\u8bd5\u5bf9\u4e0a\u7684\u8868\u73b0\u3002\u4e3a\u4e86\u89e3\u8026\u9886\u57df\u504f\u79fb\u548c\u6807\u7b7e\u4e0d\u5339\u914d\uff0c\u7814\u7a76\u6bd4\u8f83\u4e86\u5c01\u95ed\u6807\u7b7e\u8fc1\u79fb\u4e0e\u5f00\u653e\u6807\u7b7e\u534f\u8bae\uff0c\u540e\u8005\u4f7f\u7528CLIP\u76f8\u4f3c\u6027\u5c06\u9884\u6d4b\u7c7b\u522b\u6620\u5c04\u5230\u6700\u8fd1\u7684\u76ee\u6807\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u8de8\u6570\u636e\u96c6\u76ee\u6807\u68c0\u6d4b\u7684\u6e05\u6670\u7ed3\u6784\uff1a\u76f8\u540c\u8bbe\u5b9a\u7c7b\u578b\u5185\u7684\u8fc1\u79fb\u76f8\u5bf9\u7a33\u5b9a\uff0c\u800c\u8de8\u7c7b\u578b\u8fc1\u79fb\u6027\u80fd\u663e\u8457\u4e0b\u964d\u4e14\u901a\u5e38\u4e0d\u5bf9\u79f0\u3002\u6700\u4e25\u91cd\u7684\u6027\u80fd\u5d29\u6e83\u53d1\u751f\u5728\u4ece\u7279\u5b9a\u6e90\u6570\u636e\u96c6\u8fc1\u79fb\u5230\u65e0\u5173\u76ee\u6807\u6570\u636e\u96c6\u65f6\uff0c\u5373\u4f7f\u5728\u5f00\u653e\u6807\u7b7e\u5bf9\u9f50\u540e\u4ecd\u7136\u5b58\u5728\uff0c\u8868\u660e\u9886\u57df\u504f\u79fb\u5728\u6700\u56f0\u96be\u533a\u57df\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002\u5f00\u653e\u6807\u7b7e\u8bc4\u4f30\u4ea7\u751f\u4e86\u4e00\u81f4\u4f46\u6709\u754c\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bb8\u591a\u6821\u6b63\u6848\u4f8b\u5bf9\u5e94\u4e86\u56fe\u50cf\u8bc1\u636e\u652f\u6301\u7684\u8bed\u4e49\u8fd1\u4f3c\u9519\u8bef\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bbe\u5b9a\u7279\u5f02\u6027\u7684\u8de8\u6570\u636e\u96c6\u76ee\u6807\u68c0\u6d4b\u539f\u5219\u6027\u8868\u5f81\uff0c\u5e76\u4e3a\u8bc4\u4f30\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002\u7814\u7a76\u53d1\u73b0\u9886\u57df\u504f\u79fb\u5728\u6700\u56f0\u96be\u7684\u8fc1\u79fb\u573a\u666f\u4e2d\u8d77\u4e3b\u5bfc\u4f5c\u7528\uff0c\u800c\u5f00\u653e\u6807\u7b7e\u534f\u8bae\u80fd\u591f\u90e8\u5206\u7f13\u89e3\u6807\u7b7e\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4f46\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u7406\u89e3\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5236\u5b9a\u66f4\u6709\u6548\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u7b56\u7565\u3002"}}
{"id": "2601.09613", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09613", "abs": "https://arxiv.org/abs/2601.09613", "authors": ["Yonglin Tian", "Qiyao Zhang", "Wei Xu", "Yutong Wang", "Yihao Wu", "Xinyi Li", "Xingyuan Dai", "Hui Zhang", "Zhiyong Cui", "Baoqing Guo", "Zujun Yu", "Yisheng Lv"], "title": "CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems", "comment": null, "summary": "Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CogRail\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u94c1\u8def\u5165\u4fb5\u611f\u77e5\u4e2d\u7684\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u8054\u5408\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4f4d\u7f6e\u611f\u77e5\u3001\u8fd0\u52a8\u9884\u6d4b\u548c\u5a01\u80c1\u5206\u6790\u4e09\u4e2a\u6838\u5fc3\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u94c1\u8def\u5165\u4fb5\u611f\u77e5\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u56fa\u5b9a\u89c6\u89c9\u8303\u56f4\u5185\u7684\u7269\u4f53\u5206\u7c7b\uff0c\u5e76\u5e94\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u5224\u65ad\u5165\u4fb5\u72b6\u6001\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u5177\u6709\u6f5c\u5728\u5165\u4fb5\u98ce\u9669\u7684\u76ee\u6807\u3002\u51c6\u786e\u9884\u6d4b\u8fd9\u4e9b\u98ce\u9669\u9700\u8981\u7406\u89e3\u611f\u5174\u8da3\u5bf9\u8c61\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u65f6\u5e8f\u52a8\u6001\uff0c\u8fd9\u5bf9\u4f20\u7edf\u89c6\u89c9\u6a21\u578b\u6784\u6210\u4e86\u6311\u6218\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86CogRail\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6574\u5408\u4e86\u7cbe\u9009\u7684\u5f00\u6e90\u6570\u636e\u96c6\u548c\u8ba4\u77e5\u9a71\u52a8\u7684\u95ee\u7b54\u6807\u6ce8\u4ee5\u652f\u6301\u65f6\u7a7a\u63a8\u7406\u548c\u9884\u6d4b\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u5408\u5fae\u8c03\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u4f4d\u7f6e\u611f\u77e5\u3001\u8fd0\u52a8\u9884\u6d4b\u548c\u5a01\u80c1\u5206\u6790\u4e09\u4e2a\u6838\u5fc3\u4efb\u52a1\uff0c\u4fc3\u8fdb\u901a\u7528\u57fa\u7840\u6a21\u578b\u5411\u8ba4\u77e5\u5165\u4fb5\u611f\u77e5\u4e13\u7528\u6a21\u578b\u7684\u9002\u5e94\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u5728\u5904\u7406\u8ba4\u77e5\u5165\u4fb5\u611f\u77e5\u4efb\u52a1\u6240\u9700\u7684\u590d\u6742\u65f6\u7a7a\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7a81\u663e\u4e86\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u8fd9\u4e00\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5c40\u9650\u6027\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u63d0\u51fa\u7684\u8054\u5408\u5fae\u8c03\u6846\u67b6\u901a\u8fc7\u9488\u5bf9\u6027\u5730\u9002\u5e94\u9886\u57df\u7279\u5b9a\u7684\u63a8\u7406\u9700\u6c42\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u663e\u793a\u4e86\u7ed3\u6784\u5316\u591a\u4efb\u52a1\u5b66\u4e60\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u8ba4\u77e5\u9a71\u52a8\u65b9\u6cd5\u5728\u5b89\u5168\u5173\u952e\u611f\u77e5\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u5c06\u901a\u7528\u57fa\u7840\u6a21\u578b\u9002\u5e94\u5230\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u94c1\u8def\u5165\u4fb5\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u590d\u6742\u65f6\u7a7a\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.09524", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09524", "abs": "https://arxiv.org/abs/2601.09524", "authors": ["Lennart Eing", "Cristina Luna-Jim\u00e9nez", "Silvan Mertes", "Elisabeth Andr\u00e9"], "title": "Video Joint-Embedding Predictive Architectures for Facial Expression Recognition", "comment": "To appear in 2025 Proceedings of the 13th International Conference on Affective Computing and Intelligent Interaction (ACII), submitted to IEEE. \\c{opyright} 2025 IEEE", "summary": "This paper introduces a novel application of Video Joint-Embedding Predictive Architectures (V-JEPAs) for Facial Expression Recognition (FER). Departing from conventional pre-training methods for video understanding that rely on pixel-level reconstructions, V-JEPAs learn by predicting embeddings of masked regions from the embeddings of unmasked regions. This enables the trained encoder to not capture irrelevant information about a given video like the color of a region of pixels in the background. Using a pre-trained V-JEPA video encoder, we train shallow classifiers using the RAVDESS and CREMA-D datasets, achieving state-of-the-art performance on RAVDESS and outperforming all other vision-based methods on CREMA-D (+1.48 WAR). Furthermore, cross-dataset evaluations reveal strong generalization capabilities, demonstrating the potential of purely embedding-based pre-training approaches to advance FER. We release our code at https://github.com/lennarteingunia/vjepa-for-fer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff08V-JEPA\uff09\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5165\u9884\u6d4b\u800c\u975e\u50cf\u7d20\u91cd\u5efa\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\uff0c\u5728RAVDESS\u548cCREMA-D\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u50cf\u7d20\u91cd\u5efa\u7684\u89c6\u9891\u7406\u89e3\u9884\u8bad\u7ec3\u65b9\u6cd5\u53ef\u80fd\u6355\u83b7\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u80cc\u666f\u4fe1\u606f\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u7eaf\u5d4c\u5165\u9884\u6d4b\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5bf9\u76f8\u5173\u7279\u5f81\u7684\u63d0\u53d6\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u89c6\u9891\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff08V-JEPA\uff09\uff0c\u901a\u8fc7\u9884\u6d4b\u63a9\u7801\u533a\u57df\u7684\u5d4c\u5165\u8868\u793a\u800c\u975e\u50cf\u7d20\u7ea7\u91cd\u5efa\u6765\u5b66\u4e60\u89c6\u9891\u8868\u793a\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684V-JEPA\u89c6\u9891\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5728RAVDESS\u548cCREMA-D\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6d45\u5c42\u5206\u7c7b\u5668\u8fdb\u884c\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u3002", "result": "\u5728RAVDESS\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728CREMA-D\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u6240\u6709\u5176\u4ed6\u57fa\u4e8e\u89c6\u89c9\u7684\u65b9\u6cd5\uff08\u52a0\u6743\u51c6\u786e\u7387\u63d0\u5347+1.48%\uff09\uff0c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5d4c\u5165\u9884\u6d4b\u9884\u8bad\u7ec3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7eaf\u5d4c\u5165\u9884\u6d4b\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u907f\u514d\u6355\u83b7\u65e0\u5173\u80cc\u666f\u4fe1\u606f\uff0c\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u5177\u6709\u63a8\u52a8FER\u9886\u57df\u53d1\u5c55\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.09708", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09708", "abs": "https://arxiv.org/abs/2601.09708", "authors": ["Chi-Pin Huang", "Yunze Man", "Zhiding Yu", "Min-Hung Chen", "Jan Kautz", "Yu-Chiang Frank Wang", "Fu-En Yang"], "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning", "comment": "Project page: https://jasper0314-huang.github.io/fast-thinkact/", "summary": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFast-ThinkAct\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u8868\u8fbe\u7684\u6f5c\u5728\u63a8\u7406\u5b9e\u73b0\u7d27\u51d1\u800c\u9ad8\u6548\u7684\u89c4\u5212\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u957f\u65f6\u7a0b\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u4efb\u52a1\u9700\u8981\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5bf9\u590d\u6742\u89c6\u89c9\u573a\u666f\u8fdb\u884c\u63a8\u7406\u5e76\u6267\u884c\u9002\u5e94\u6027\u52a8\u4f5c\uff0c\u73b0\u6709\u663e\u5f0f\u601d\u7ef4\u94fe\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u63a8\u7406\u8f68\u8ff9\u8fc7\u957f\u5bfc\u81f4\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u6846\u67b6\u3002", "method": "Fast-ThinkAct\u91c7\u7528\u53ef\u8868\u8fbe\u7684\u6f5c\u5728\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u6559\u5e08\u6a21\u578b\u84b8\u998f\u5b66\u4e60\u6f5c\u5728\u601d\u7ef4\u94fe\uff0c\u5229\u7528\u504f\u597d\u5f15\u5bfc\u76ee\u6807\u5bf9\u9f50\u64cd\u4f5c\u8f68\u8ff9\uff0c\u540c\u65f6\u8fc1\u79fb\u8bed\u8a00\u548c\u89c6\u89c9\u89c4\u5212\u80fd\u529b\uff0c\u5b9e\u73b0\u63a8\u7406\u589e\u5f3a\u7684\u7b56\u7565\u5b66\u4e60\uff0c\u5c06\u7d27\u51d1\u63a8\u7406\u4e0e\u52a8\u4f5c\u6267\u884c\u6709\u6548\u8fde\u63a5\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u5177\u8eab\u64cd\u4f5c\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFast-ThinkAct\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u63a8\u7406VLA\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe89.3%\u7684\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u6548\u7684\u957f\u65f6\u7a0b\u89c4\u5212\u3001\u5c11\u6837\u672c\u9002\u5e94\u548c\u5931\u8d25\u6062\u590d\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u6f5c\u5728\u63a8\u7406\u84b8\u998f\u548c\u504f\u597d\u5bf9\u9f50\uff0c\u53ef\u4ee5\u5728\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u7684\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u89c4\u5212\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e73\u8861\u4e86\u63a8\u7406\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2601.09575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09575", "abs": "https://arxiv.org/abs/2601.09575", "authors": ["Sheng-Yu Huang", "Jaesung Choe", "Yu-Chiang Frank Wang", "Cheng Sun"], "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding", "comment": "project page: https://peterjohnsonhuang.github.io/openvoxel-pages/", "summary": "We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OpenVoxel\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a00\u758f\u4f53\u7d20\u7684\u5206\u7ec4\u548c\u63cf\u8ff0\uff0c\u4ee5\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u76843D\u573a\u666f\u7406\u89e3\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efa\u4fe1\u606f\u4e30\u5bcc\u7684\u573a\u666f\u5730\u56fe\uff0c\u5728\u590d\u6742\u53c2\u8003\u8868\u8fbe\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u7406\u89e3\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u8bad\u7ec3\u8fc7\u7a0b\u6216\u4f9d\u8d56CLIP/BERT\u6587\u672c\u7f16\u7801\u5668\u7684\u5d4c\u5165\u8868\u793a\uff0c\u8fd9\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u5bf9\u7a00\u758f\u4f53\u7d20\u8fdb\u884c\u5206\u7ec4\u548c\u63cf\u8ff0\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3\u3002", "method": "OpenVoxel\u91c7\u7528\u65e0\u9700\u8bad\u7ec3\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8e\u591a\u89c6\u56fe\u56fe\u50cf\u83b7\u5f97\u7684\u7a00\u758f\u4f53\u7d20\u6805\u683c\u5316\u6a21\u578b\uff0c\u5bf9\u7a00\u758f\u4f53\u7d20\u8fdb\u884c\u6709\u610f\u4e49\u7684\u5206\u7ec4\u4ee5\u63cf\u8ff0\u573a\u666f\u4e2d\u7684\u4e0d\u540c\u7269\u4f53\u3002\u8be5\u65b9\u6cd5\u76f4\u63a5\u5229\u7528\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u5230\u6587\u672c\u7684\u641c\u7d22\u65b9\u5f0f\u4e3a\u6bcf\u4e2a\u5206\u7ec4\u751f\u6210\u63cf\u8ff0\u6027\u6807\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5f15\u5165CLIP/BERT\u6587\u672c\u7f16\u7801\u5668\u5d4c\u5165\u7684\u6b65\u9aa4\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0cOpenVoxel\u5728\u590d\u6742\u53c2\u8003\u8868\u8fbe\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u8fd1\u671f\u7814\u7a76\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u6784\u5efa\u4fe1\u606f\u4e30\u5bcc\u7684\u573a\u666f\u5730\u56fe\uff0c\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u548c\u53c2\u8003\u8868\u8fbe\u5206\u5272\u7b49\u8fdb\u4e00\u6b65\u76843D\u573a\u666f\u7406\u89e3\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u4f18\u52bf\u3002", "conclusion": "OpenVoxel\u5c55\u793a\u4e86\u65e0\u9700\u8bad\u7ec3\u65b9\u6cd5\u57283D\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6587\u672c\u5230\u6587\u672c\u641c\u7d22\u7684\u521b\u65b0\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u4e3a\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u5411\u66f4\u5c11\u4f9d\u8d56\u9884\u8bad\u7ec3\u5d4c\u5165\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2601.09606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09606", "abs": "https://arxiv.org/abs/2601.09606", "authors": ["Manning Gao", "Leheng Zhang", "Shiqin Han", "Haifeng Hu", "Yuncheng Jiang", "Sijie Mai"], "title": "GRCF: Two-Stage Groupwise Ranking and Calibration Framework for Multimodal Sentiment Analysis", "comment": null, "summary": "Most Multimodal Sentiment Analysis research has focused on point-wise regression. While straightforward, this approach is sensitive to label noise and neglects whether one sample is more positive than another, resulting in unstable predictions and poor correlation alignment. Pairwise ordinal learning frameworks emerged to address this gap, capturing relative order by learning from comparisons. Yet, they introduce two new trade-offs: First, they assign uniform importance to all comparisons, failing to adaptively focus on hard-to-rank samples. Second, they employ static ranking margins, which fail to reflect the varying semantic distances between sentiment groups. To address this, we propose a Two-Stage Group-wise Ranking and Calibration Framework (GRCF) that adapts the philosophy of Group Relative Policy Optimization (GRPO). Our framework resolves these trade-offs by simultaneously preserving relative ordinal structure, ensuring absolute score calibration, and adaptively focusing on difficult samples. Specifically, Stage 1 introduces a GRPO-inspired Advantage-Weighted Dynamic Margin Ranking Loss to build a fine-grained ordinal structure. Stage 2 then employs an MAE-driven objective to align prediction magnitudes. To validate its generalizability, we extend GRCF to classification tasks, including multimodal humor detection and sarcasm detection. GRCF achieves state-of-the-art performance on core regression benchmarks, while also showing strong generalizability in classification tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u5206\u7ec4\u6392\u5e8f\u4e0e\u6821\u51c6\u6846\u67b6\uff08GRCF\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u4f18\u52bf\u52a0\u6743\u52a8\u6001\u8fb9\u754c\u6392\u5e8f\u635f\u5931\u548cMAE\u9a71\u52a8\u76ee\u6807\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u4f20\u7edf\u6210\u5bf9\u6392\u5e8f\u65b9\u6cd5\u5bf9\u56f0\u96be\u6837\u672c\u5173\u6ce8\u4e0d\u8db3\u548c\u8fb9\u754c\u8bbe\u7f6e\u9759\u6001\u7684\u95ee\u9898\uff0c\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7814\u7a76\u5927\u591a\u5173\u6ce8\u70b9\u5f0f\u56de\u5f52\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5bf9\u6807\u7b7e\u566a\u58f0\u654f\u611f\u4e14\u5ffd\u7565\u4e86\u6837\u672c\u95f4\u7684\u76f8\u5bf9\u987a\u5e8f\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u7a33\u5b9a\u548c\u76f8\u5173\u6027\u5bf9\u9f50\u5dee\u3002\u867d\u7136\u6210\u5bf9\u6392\u5e8f\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u6bd4\u8f83\u5b66\u4e60\u76f8\u5bf9\u987a\u5e8f\u6765\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\uff0c\u4f46\u5b83\u4eec\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u95ee\u9898\uff1a\u4e00\u662f\u5bf9\u6240\u6709\u6bd4\u8f83\u8d4b\u4e88\u7edf\u4e00\u91cd\u8981\u6027\uff0c\u672a\u80fd\u81ea\u9002\u5e94\u5730\u5173\u6ce8\u96be\u4ee5\u6392\u5e8f\u7684\u6837\u672c\uff1b\u4e8c\u662f\u91c7\u7528\u9759\u6001\u6392\u5e8f\u8fb9\u754c\uff0c\u65e0\u6cd5\u53cd\u6620\u60c5\u611f\u7ec4\u95f4\u53d8\u5316\u7684\u8bed\u4e49\u8ddd\u79bb\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u5206\u7ec4\u6392\u5e8f\u4e0e\u6821\u51c6\u6846\u67b6\uff08GRCF\uff09\uff0c\u8be5\u6846\u67b6\u501f\u9274\u4e86\u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u601d\u60f3\u3002\u7b2c\u4e00\u9636\u6bb5\u5f15\u5165\u4e86GRPO\u542f\u53d1\u7684\u4f18\u52bf\u52a0\u6743\u52a8\u6001\u8fb9\u754c\u6392\u5e8f\u635f\u5931\uff0c\u4ee5\u6784\u5efa\u7ec6\u7c92\u5ea6\u7684\u5e8f\u6570\u7ed3\u6784\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528MAE\u9a71\u52a8\u7684\u76ee\u6807\u6765\u5bf9\u9f50\u9884\u6d4b\u5e45\u5ea6\u3002\u4e3a\u4e86\u9a8c\u8bc1\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u4f5c\u8005\u5c06GRCF\u6269\u5c55\u5230\u5206\u7c7b\u4efb\u52a1\uff0c\u5305\u62ec\u591a\u6a21\u6001\u5e7d\u9ed8\u68c0\u6d4b\u548c\u8bbd\u523a\u68c0\u6d4b\u3002", "result": "GRCF\u5728\u6838\u5fc3\u56de\u5f52\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u76f8\u5bf9\u5e8f\u6570\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u786e\u4fdd\u4e86\u7edd\u5bf9\u5206\u6570\u6821\u51c6\uff0c\u5e76\u80fd\u81ea\u9002\u5e94\u5730\u5173\u6ce8\u56f0\u96be\u6837\u672c\uff0c\u4ece\u800c\u5728\u591a\u4e2a\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u5173\u6ce8\u56f0\u96be\u6837\u672c\u548c\u52a8\u6001\u8fb9\u754c\u8bbe\u7f6e\u5bf9\u4e8e\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u5e8f\u6570\u5b66\u4e60\u7684\u91cd\u8981\u6027\u3002GRCF\u6846\u67b6\u4e0d\u4ec5\u89e3\u51b3\u4e86\u4f20\u7edf\u6210\u5bf9\u6392\u5e8f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u8fd8\u5c55\u793a\u4e86\u4ece\u56de\u5f52\u4efb\u52a1\u5230\u5206\u7c7b\u4efb\u52a1\u7684\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u5e8f\u6570\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u89c6\u89d2\u548c\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2601.09647", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09647", "abs": "https://arxiv.org/abs/2601.09647", "authors": ["Ali Naseh", "Yuefeng Peng", "Anshuman Suri", "Harsh Chaudhari", "Alina Oprea", "Amir Houmansadr"], "title": "Identifying Models Behind Text-to-Image Leaderboards", "comment": null, "summary": "Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space, enabling accurate deanonymization without prompt control or training data. Using 22 models and 280 prompts (150K images), our centroid-based method achieves high accuracy and reveals systematic model-specific signatures. We further introduce a prompt-level distinguishability metric and conduct large-scale analyses showing how certain prompts can lead to near-perfect distinguishability. Our findings expose fundamental security flaws in T2I leaderboards and motivate stronger anonymization defenses.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u57fa\u4e8e\u6295\u7968\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u6392\u884c\u699c\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u901a\u8fc7\u5206\u6790\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5f62\u6210\u72ec\u7279\u7684\u805a\u7c7b\u6a21\u5f0f\uff0c\u4f7f\u5f97\u533f\u540d\u5316\u53ef\u4ee5\u88ab\u8f7b\u6613\u7834\u89e3\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u8d28\u91cf\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u57fa\u4e8e\u6295\u7968\u7684\u6392\u884c\u699c\uff0c\u8fd9\u4e9b\u6392\u884c\u699c\u5047\u8bbe\u6a21\u578b\u8f93\u51fa\u7ecf\u8fc7\u533f\u540d\u5316\u5904\u7406\u4ee5\u4fdd\u8bc1\u516c\u5e73\u6027\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u533f\u540d\u5316\u673a\u5236\u7684\u5b89\u5168\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u6b64\u7c7b\u6392\u884c\u699c\u4e2d\u5b58\u5728\u7684\u6f5c\u5728\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d28\u5fc3\u7684\u53bb\u533f\u540d\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u679022\u4e2a\u4e0d\u540c\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728280\u4e2a\u63d0\u793a\u8bcd\u4e0b\u751f\u6210\u768415\u4e07\u5f20\u56fe\u50cf\uff0c\u53d1\u73b0\u6bcf\u4e2a\u6a21\u578b\u7684\u751f\u6210\u7ed3\u679c\u5728\u56fe\u50cf\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5f62\u6210\u72ec\u7279\u7684\u805a\u7c7b\u6a21\u5f0f\u3002\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u63a7\u5236\u63d0\u793a\u8bcd\u6216\u8bbf\u95ee\u8bad\u7ec3\u6570\u636e\uff0c\u4ec5\u5229\u7528\u56fe\u50cf\u5d4c\u5165\u7279\u5f81\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u6a21\u578b\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u8d28\u5fc3\u7684\u53bb\u533f\u540d\u5316\u65b9\u6cd5\u80fd\u591f\u4ee5\u9ad8\u51c6\u786e\u7387\u8bc6\u522b\u4e0d\u540c\u6a21\u578b\u7684\u751f\u6210\u56fe\u50cf\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u7684\u6a21\u578b\u7279\u5b9a\u7279\u5f81\u7b7e\u540d\u3002\u7814\u7a76\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u63d0\u793a\u8bcd\u7ea7\u522b\u7684\u53ef\u533a\u5206\u6027\u5ea6\u91cf\uff0c\u5e76\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5206\u6790\uff0c\u53d1\u73b0\u67d0\u4e9b\u7279\u5b9a\u63d0\u793a\u8bcd\u80fd\u591f\u5bfc\u81f4\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6a21\u578b\u53ef\u533a\u5206\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u66b4\u9732\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u6392\u884c\u699c\u4e2d\u5b58\u5728\u7684\u6839\u672c\u6027\u5b89\u5168\u7f3a\u9677\uff0c\u8868\u660e\u5f53\u524d\u7684\u533f\u540d\u5316\u673a\u5236\u4e0d\u8db3\u4ee5\u4fdd\u62a4\u6a21\u578b\u8eab\u4efd\u3002\u8fd9\u4e00\u53d1\u73b0\u5f3a\u8c03\u4e86\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u7684\u533f\u540d\u5316\u9632\u5fa1\u63aa\u65bd\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\uff0c\u5bf9AI\u751f\u6210\u5185\u5bb9\u8ba4\u8bc1\u548c\u6a21\u578b\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2601.09658", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09658", "abs": "https://arxiv.org/abs/2601.09658", "authors": ["Selim Emir Can", "Jan Ackermann", "Kiyohiro Nakayama", "Ruofan Liu", "Tong Wu", "Yang Zheng", "Hugo Bertiche", "Menglei Chai", "Thabo Beeler", "Gordon Wetzstein"], "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image", "comment": null, "summary": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u524d\u9988\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u50cf\u76f4\u63a5\u4f30\u8ba1\u7269\u7406\u51c6\u786e\u7684\u3001\u53ef\u7528\u4e8e\u4eff\u771f\u7684\u670d\u88c5\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6750\u6599\u5c5e\u6027\u63a8\u65ad\u548c\u8f7b\u91cf\u7ea7\u7269\u7406\u53c2\u6570\u9884\u6d4b\u5668\uff0c\u65e0\u9700\u591a\u89c6\u56fe\u6355\u83b7\u6216\u8fed\u4ee3\u4f18\u5316\u5373\u53ef\u751f\u6210\u4eff\u771f\u5c31\u7eea\u7684\u670d\u88c5\u3002", "motivation": "\u4ece\u5355\u5f20\u56fe\u50cf\u4f30\u8ba1\u7269\u7406\u51c6\u786e\u7684\u4eff\u771f\u5c31\u7eea\u670d\u88c5\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u7f3a\u4e4f\u56fe\u50cf\u5230\u7269\u7406\u7684\u6570\u636e\u96c6\u4ee5\u53ca\u95ee\u9898\u7684\u75c5\u6001\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u591a\u89c6\u56fe\u6355\u83b7\u548c\u6602\u8d35\u7684\u53ef\u5fae\u5206\u4eff\u771f\uff0c\u8981\u4e48\u53ea\u80fd\u9884\u6d4b\u670d\u88c5\u51e0\u4f55\u5f62\u72b6\u800c\u7f3a\u4e4f\u4eff\u771f\u6240\u9700\u7684\u6750\u6599\u7269\u7406\u5c5e\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u524d\u9988\u6846\u67b6\uff0c\u9996\u5148\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u771f\u5b9e\u56fe\u50cf\u63a8\u65ad\u6750\u6599\u6210\u5206\u548c\u7ec7\u7269\u5c5e\u6027\uff0c\u7136\u540e\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9884\u6d4b\u5668\u5c06\u8fd9\u4e9b\u5c5e\u6027\u6620\u5c04\u5230\u76f8\u5e94\u7684\u7269\u7406\u7ec7\u7269\u53c2\u6570\u3002\u6846\u67b6\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\uff08FTAG\u548cT2P\uff09\uff0c\u5e76\u907f\u514d\u4e86\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\uff0c\u76f4\u63a5\u751f\u6210\u4eff\u771f\u5c31\u7eea\u7684\u670d\u88c5\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6750\u6599\u6210\u5206\u4f30\u8ba1\u548c\u7ec7\u7269\u5c5e\u6027\u9884\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002\u901a\u8fc7\u5c06\u8fd9\u4e9b\u9884\u6d4b\u7ed3\u679c\u8f93\u5165\u7269\u7406\u53c2\u6570\u4f30\u8ba1\u5668\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u5230\u670d\u88c5\u65b9\u6cd5\u76f8\u6bd4\uff0c\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u4fdd\u771f\u5ea6\u7684\u4eff\u771f\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7269\u7406\u53c2\u6570\u6620\u5c04\uff0c\u53ef\u4ee5\u4ece\u5355\u5f20\u56fe\u50cf\u6709\u6548\u4f30\u8ba1\u4eff\u771f\u5c31\u7eea\u7684\u670d\u88c5\uff0c\u65e0\u9700\u591a\u89c6\u56fe\u6355\u83b7\u6216\u6602\u8d35\u7684\u4f18\u5316\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u4e3a\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u865a\u62df\u8bd5\u7a7f\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u6a21\u6001\u5b66\u4e60\u5728\u7269\u7406\u5c5e\u6027\u4f30\u8ba1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.09661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09661", "abs": "https://arxiv.org/abs/2601.09661", "authors": ["Aishwarya Agarwal", "Srikrishna Karanam", "Vineet Gandhi"], "title": "LiteEmbed: Adapting CLIP to Rare Classes", "comment": "14 pages, 12 figures", "summary": "Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.", "AI": {"tldr": "LiteEmbed \u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e CLIP \u7684\u5c11\u6837\u672c\u4e2a\u6027\u5316\uff0c\u901a\u8fc7\u5b50\u7a7a\u95f4\u5f15\u5bfc\u7684\u6587\u672c\u5d4c\u5165\u4f18\u5316\uff0c\u4f7f\u65b0\u7c7b\u522b\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u60c5\u51b5\u4e0b\u6dfb\u52a0\u5230\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f55\u89c1\u7c7b\u522b\u548c\u672a\u89c1\u7c7b\u522b\u7684\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5982 CLIP \u5728\u96f6\u6837\u672c\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u9884\u8bad\u7ec3\u671f\u95f4\u7f55\u89c1\u51fa\u73b0\u7684\u7c7b\u522b\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5305\u62ec\u65b0\u51fa\u73b0\u7684\u5b9e\u4f53\u548c\u6587\u5316\u7279\u5b9a\u7c7b\u522b\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u9002\u5e94\u6027\u548c\u8986\u76d6\u8303\u56f4\u3002", "method": "LiteEmbed \u91c7\u7528\u5b50\u7a7a\u95f4\u5f15\u5bfc\u7684\u6587\u672c\u5d4c\u5165\u4f18\u5316\u65b9\u6cd5\uff0c\u57fa\u4e8e PCA \u5206\u89e3\u5c06\u8bed\u4e49\u7a7a\u95f4\u89e3\u8026\u4e3a\u7c97\u7c92\u5ea6\u8bed\u4e49\u65b9\u5411\u548c\u7ec6\u7c92\u5ea6\u53d8\u5316\u65b9\u5411\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u5bf9\u9f50\u548c\u7ec6\u7c92\u5ea6\u5206\u79bb\u4e24\u4e2a\u4e92\u8865\u76ee\u6807\uff0c\u5728\u4fdd\u6301\u5168\u5c40\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u540c\u65f6\u589e\u5f3a\u89c6\u89c9\u76f8\u4f3c\u7c7b\u522b\u4e4b\u95f4\u7684\u533a\u5206\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLiteEmbed \u5728\u5206\u7c7b\u3001\u68c0\u7d22\u3001\u5206\u5272\u548c\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u4e3a CLIP \u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u3001\u7f55\u89c1\u6216\u672a\u89c1\u7c7b\u522b\u7684\u9002\u5e94\u65b9\u9762\u5efa\u7acb\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u5373\u63d2\u5373\u7528\u7684\u5d4c\u5165\u66ff\u6362\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5c11\u6837\u672c\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u7a7a\u95f4\u7684\u4f18\u5316\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u539f\u6709\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u6269\u5c55\u4e86\u5176\u5bf9\u7f55\u89c1\u548c\u65b0\u7c7b\u522b\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.09663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09663", "abs": "https://arxiv.org/abs/2601.09663", "authors": ["Xuyang Fang", "Sion Hannuna", "Edwin Simpson", "Neill Campbell"], "title": "Self-Supervised Animal Identification for Long Videos", "comment": "11 pages, 1 figure", "summary": "Identifying individual animals in long-duration videos is essential for behavioral ecology, wildlife monitoring, and livestock management. Traditional methods require extensive manual annotation, while existing self-supervised approaches are computationally demanding and ill-suited for long sequences due to memory constraints and temporal error propagation. We introduce a highly efficient, self-supervised method that reframes animal identification as a global clustering task rather than a sequential tracking problem. Our approach assumes a known, fixed number of individuals within a single video -- a common scenario in practice -- and requires only bounding box detections and the total count. By sampling pairs of frames, using a frozen pre-trained backbone, and employing a self-bootstrapping mechanism with the Hungarian algorithm for in-batch pseudo-label assignment, our method learns discriminative features without identity labels. We adapt a Binary Cross Entropy loss from vision-language models, enabling state-of-the-art accuracy ($>$97\\%) while consuming less than 1 GB of GPU memory per batch -- an order of magnitude less than standard contrastive methods. Evaluated on challenging real-world datasets (3D-POP pigeons and 8-calves feeding videos), our framework matches or surpasses supervised baselines trained on over 1,000 labeled frames, effectively removing the manual annotation bottleneck. This work enables practical, high-accuracy animal identification on consumer-grade hardware, with broad applicability in resource-constrained research settings. All code written for this paper are \\href{https://huggingface.co/datasets/tonyFang04/8-calves}{here}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u52a8\u7269\u4e2a\u4f53\u8bc6\u522b\u65b9\u6cd5\uff0c\u5c06\u8bc6\u522b\u4efb\u52a1\u91cd\u6784\u4e3a\u5168\u5c40\u805a\u7c7b\u95ee\u9898\u800c\u975e\u5e8f\u5217\u8ddf\u8e2a\uff0c\u4ec5\u9700\u8fb9\u754c\u6846\u68c0\u6d4b\u548c\u4e2a\u4f53\u603b\u6570\uff0c\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc797%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u548c\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u52a8\u7269\u4e2a\u4f53\u8bc6\u522b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\uff0c\u800c\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u9002\u7528\u4e8e\u957f\u89c6\u9891\u5e8f\u5217\uff0c\u5b58\u5728\u5185\u5b58\u9650\u5236\u548c\u65f6\u95f4\u8bef\u5dee\u4f20\u64ad\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7814\u7a76\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u52a8\u7269\u8bc6\u522b\u91cd\u6784\u4e3a\u5168\u5c40\u805a\u7c7b\u4efb\u52a1\uff0c\u5047\u8bbe\u89c6\u9891\u4e2d\u4e2a\u4f53\u6570\u91cf\u5df2\u77e5\u4e14\u56fa\u5b9a\uff0c\u4ec5\u9700\u8fb9\u754c\u6846\u68c0\u6d4b\u548c\u603b\u6570\u4fe1\u606f\uff1b\u901a\u8fc7\u91c7\u6837\u5e27\u5bf9\u3001\u4f7f\u7528\u51bb\u7ed3\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\u3001\u7ed3\u5408\u5308\u7259\u5229\u7b97\u6cd5\u7684\u81ea\u5f15\u5bfc\u673a\u5236\u8fdb\u884c\u6279\u5185\u4f2a\u6807\u7b7e\u5206\u914d\uff0c\u5e76\u91c7\u7528\u6765\u81ea\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u5b66\u4e60\u5224\u522b\u6027\u7279\u5f81\u3002", "result": "\u57283D-POP\u9e3d\u5b50\u548c8\u5934\u5c0f\u725b\u5582\u98df\u89c6\u9891\u7b49\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d85\u8fc797%\u7684\u51c6\u786e\u7387\uff0c\u6bcf\u6279\u6b21GPU\u5185\u5b58\u6d88\u8017\u5c0f\u4e8e1GB\uff0c\u6bd4\u6807\u51c6\u5bf9\u6bd4\u65b9\u6cd5\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u4f7f\u7528\u8d85\u8fc71000\u4e2a\u6807\u6ce8\u5e27\u8bad\u7ec3\u7684\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u6d88\u9664\u4e86\u52a8\u7269\u8bc6\u522b\u4e2d\u7684\u624b\u52a8\u6807\u6ce8\u74f6\u9888\uff0c\u4f7f\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u7684\u9ad8\u7cbe\u5ea6\u52a8\u7269\u8bc6\u522b\u6210\u4e3a\u53ef\u80fd\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7814\u7a76\u73af\u5883\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e3a\u884c\u4e3a\u751f\u6001\u5b66\u3001\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u548c\u755c\u7267\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.09668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09668", "abs": "https://arxiv.org/abs/2601.09668", "authors": ["Ailin Huang", "Chengyuan Yao", "Chunrui Han", "Fanqi Wan", "Hangyu Guo", "Haoran Lv", "Hongyu Zhou", "Jia Wang", "Jian Zhou", "Jianjian Sun", "Jingcheng Hu", "Kangheng Lin", "Liang Zhao", "Mitt Huang", "Song Yuan", "Wenwen Qu", "Xiangfeng Wang", "Yanlin Lai", "Yingxiu Zhao", "Yinmin Zhang", "Yukang Shi", "Yuyang Chen", "Zejia Weng", "Ziyang Meng", "Ang Li", "Aobo Kong", "Bo Dong", "Changyi Wan", "David Wang", "Di Qi", "Dingming Li", "En Yu", "Guopeng Li", "Haiquan Yin", "Han Zhou", "Hanshan Zhang", "Haolong Yan", "Hebin Zhou", "Hongbo Peng", "Jiaran Zhang", "Jiashu Lv", "Jiayi Fu", "Jie Cheng", "Jie Zhou", "Jisheng Yin", "Jingjing Xie", "Jingwei Wu", "Jun Zhang", "Junfeng Liu", "Kaijun Tan", "Kaiwen Yan", "Liangyu Chen", "Lina Chen", "Mingliang Li", "Qian Zhao", "Quan Sun", "Shaoliang Pang", "Shengjie Fan", "Shijie Shang", "Siyuan Zhang", "Tianhao You", "Wei Ji", "Wuxun Xie", "Xiaobo Yang", "Xiaojie Hou", "Xiaoran Jiao", "Xiaoxiao Ren", "Xiangwen Kong", "Xin Huang", "Xin Wu", "Xing Chen", "Xinran Wang", "Xuelin Zhang", "Yana Wei", "Yang Li", "Yanming Xu", "Yeqing Shen", "Yuang Peng", "Yue Peng", "Yu Zhou", "Yusheng Li", "Yuxiang Yang", "Yuyang Zhang", "Zhe Xie", "Zhewei Huang", "Zhenyi Lu", "Zhimin Fan", "Zihui Cheng", "Daxin Jiang", "Qi Han", "Xiangyu Zhang", "Yibo Zhu", "Zheng Ge"], "title": "STEP3-VL-10B Technical Report", "comment": "50 pages", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\\times$-20$\\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "AI": {"tldr": "STEP3-VL-10B\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u5e76\u884c\u534f\u8c03\u63a8\u7406\u673a\u5236\uff0c\u5728\u4ec510B\u53c2\u6570\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u4e0e10-20\u500d\u5927\u578b\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u7d27\u51d1\u6548\u7387\u4e0e\u524d\u6cbf\u591a\u6a21\u6001\u667a\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e2d\u7d27\u51d1\u6548\u7387\u4e0e\u524d\u6cbf\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u73b0\u6709\u6a21\u578b\u901a\u5e38\u9700\u8981\u6781\u5927\u53c2\u6570\u91cf\u624d\u80fd\u8fbe\u5230\u9876\u7ea7\u6027\u80fd\uff0c\u800c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u590d\u6742\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u91cd\u65b0\u5b9a\u4e49\u8fd9\u4e00\u6743\u8861\u5173\u7cfb\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u6218\u7565\u8f6c\u53d8\uff1a\u9996\u5148\u91c7\u7528\u7edf\u4e00\u5b8c\u5168\u89e3\u51bb\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u57281.2T\u591a\u6a21\u6001token\u4e0a\u6574\u5408\u8bed\u8a00\u5bf9\u9f50\u7684\u611f\u77e5\u7f16\u7801\u5668\u548cQwen3-8B\u89e3\u7801\u5668\u4ee5\u5efa\u7acb\u5185\u5728\u89c6\u89c9\u8bed\u8a00\u534f\u540c\uff1b\u5176\u6b21\u5b9e\u65bd\u5305\u542b1000\u591a\u6b21\u8fed\u4ee3\u7684\u5f3a\u5316\u5b66\u4e60\u89c4\u6a21\u5316\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u5f15\u5165\u5e76\u884c\u534f\u8c03\u63a8\u7406\u673a\u5236\u6765\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u3002", "result": "STEP3-VL-10B\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u5353\u8d8a\u6027\u80fd\uff1aMMBench\u8fbe\u523092.2%\uff0cMMMU\u8fbe\u523080.11%\uff0c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2dAIME2025\u8fbe\u523094.43%\uff0cMathVision\u8fbe\u523075.95%\uff0c\u517610B\u53c2\u6570\u89c4\u6a21\u4e0b\u6027\u80fd\u53ef\u5339\u654c\u6216\u8d85\u8d8a10-20\u500d\u5927\u578b\u6a21\u578b\u53ca\u9876\u7ea7\u4e13\u6709\u65d7\u8230\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u63a8\u7406\u673a\u5236\uff0c\u7d27\u51d1\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u524d\u6cbf\u591a\u6a21\u6001\u667a\u80fd\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u4e0a\u8ba4\u4e3a\u9700\u8981\u6781\u5927\u53c2\u6570\u91cf\u624d\u80fd\u83b7\u5f97\u9876\u7ea7\u6027\u80fd\u7684\u5047\u8bbe\uff0c\u63a8\u52a8\u4e86\u9ad8\u6548\u591a\u6a21\u6001AI\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.09697", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09697", "abs": "https://arxiv.org/abs/2601.09697", "authors": ["Jieying Chen", "Jeffrey Hu", "Joan Lasenby", "Ayush Tewari"], "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering", "comment": "Project page: https://ayushtewari.com/projects/srender/", "summary": "Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSRENDER\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u7a00\u758f\u5173\u952e\u5e27\uff0c\u7136\u540e\u5229\u75283D\u91cd\u5efa\u548c\u6e32\u67d3\u5408\u6210\u5b8c\u6574\u89c6\u9891\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u6269\u6563\u6a21\u578b\u5feb40\u500d\u4ee5\u4e0a\u7684\u9ad8\u6548\u89c6\u9891\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u751f\u6210\u51e0\u79d2\u949f\u89c6\u9891\u9700\u8981\u6570\u5206\u949fGPU\u65f6\u95f4\uff0c\u8fd9\u4e25\u91cd\u963b\u788d\u4e86\u5728\u9700\u8981\u5b9e\u65f6\u4ea4\u4e92\u7684\u5e94\u7528\uff08\u5982\u5177\u8eabAI\u548cVR/AR\uff09\u4e2d\u7684\u90e8\u7f72\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u751f\u6210\uff0c\u7279\u522b\u662f\u5728\u9759\u6001\u573a\u666f\u7684\u76f8\u673a\u6761\u4ef6\u89c6\u9891\u751f\u6210\u65b9\u9762\u5b58\u5728\u663e\u8457\u8ba1\u7b97\u74f6\u9888\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u5206\u5c42\u7b56\u7565\uff1a\u9996\u5148\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u7a00\u758f\u7684\u5173\u952e\u5e27\u96c6\u5408\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u5173\u952e\u5e27\u63d0\u5347\u52303D\u8868\u793a\u4e2d\uff0c\u901a\u8fc73D\u91cd\u5efa\u548c\u6e32\u67d3\u6280\u672f\u5408\u6210\u4e2d\u95f4\u89c6\u56fe\u3002\u7cfb\u7edf\u5f15\u5165\u4e86\u4e00\u4e2a\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u7ed9\u5b9a\u76f8\u673a\u8f68\u8ff9\u9884\u6d4b\u6700\u4f18\u5173\u952e\u5e27\u6570\u91cf\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u3002\u6700\u7ec8\u63d0\u51fa\u7684SRENDER\u65b9\u6cd5\u6839\u636e\u76f8\u673a\u8fd0\u52a8\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u5173\u952e\u5e27\u5bc6\u5ea6\uff0c\u7b80\u5355\u8f68\u8ff9\u4f7f\u7528\u6781\u7a00\u758f\u5173\u952e\u5e27\uff0c\u590d\u6742\u8fd0\u52a8\u4f7f\u7528\u8f83\u5bc6\u96c6\u5173\u952e\u5e27\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSRENDER\u5728\u751f\u621020\u79d2\u89c6\u9891\u65f6\u6bd4\u57fa\u4e8e\u6269\u6563\u7684\u57fa\u7ebf\u65b9\u6cd5\u5feb40\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u751f\u6210\u6210\u672c\u5206\u644a\u5230\u6570\u767e\u5e27\u4e2d\u5e76\u5f3a\u5236\u6267\u884c\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\u3002\u81ea\u9002\u5e94\u5173\u952e\u5e27\u5206\u914d\u673a\u5236\u786e\u4fdd\u4e86\u5728\u4e0d\u540c\u76f8\u673a\u8f68\u8ff9\u590d\u6742\u5ea6\u4e0b\u7684\u6700\u4f18\u6027\u80fd\u5e73\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u6a21\u578b\u4e0e3D\u91cd\u5efa\u6280\u672f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u89c6\u9891\u5408\u6210\uff0c\u4e3a\u5b9e\u65f6\u4ea4\u4e92\u5e94\u7528\u4e2d\u7684\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002\u65b9\u6cd5\u7684\u6838\u5fc3\u6d1e\u5bdf\u5728\u4e8e\u5c06\u6602\u8d35\u7684\u751f\u6210\u8fc7\u7a0b\u9650\u5236\u5728\u7a00\u758f\u5173\u952e\u5e27\u4e0a\uff0c\u7136\u540e\u5229\u7528\u51e0\u4f55\u4e00\u81f4\u6027\u8fdb\u884c\u9ad8\u6548\u63d2\u503c\uff0c\u8fd9\u79cd\u5206\u5c42\u7b56\u7565\u4e3a\u672a\u6765\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
