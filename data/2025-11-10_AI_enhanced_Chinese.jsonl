{"id": "2511.04727", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04727", "abs": "https://arxiv.org/abs/2511.04727", "authors": ["Ali Faraz", "Akash", "Shaharukh Khan", "Raja Kolla", "Akshat Patidar", "Suranjan Goswami", "Abhinav Ravi", "Chandra Khatri", "Shubham Agarwal"], "title": "IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs", "comment": null, "summary": "Vision-language models (VLMs) have demonstrated impressive generalization\nacross multimodal tasks, yet most evaluation benchmarks remain Western-centric,\nleaving open questions about their performance in culturally diverse and\nmultilingual settings. To address this gap, we introduce IndicVisionBench, the\nfirst large-scale benchmark centered on the Indian subcontinent. Covering\nEnglish and 10 Indian languages, our benchmark spans 3 multimodal tasks,\nincluding Optical Character Recognition (OCR), Multimodal Machine Translation\n(MMT), and Visual Question Answering (VQA), covering 6 kinds of question types.\nOur final benchmark consists of a total of ~5K images and 37K+ QA pairs across\n13 culturally grounded topics. In addition, we release a paired parallel corpus\nof annotations across 10 Indic languages, creating a unique resource for\nanalyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum\nof 8 models, from proprietary closed-source systems to open-weights medium and\nlarge-scale models. Our experiments reveal substantial performance gaps,\nunderscoring the limitations of current VLMs in culturally diverse contexts. By\ncentering cultural diversity and multilinguality, IndicVisionBench establishes\na reproducible evaluation framework that paves the way for more inclusive\nmultimodal research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86IndicVisionBench\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u5370\u5ea6\u6b21\u5927\u9646\u7684\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u82f1\u8bed\u548c10\u79cd\u5370\u5ea6\u8bed\u8a00\uff0c\u5305\u542b3\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u548c13\u4e2a\u6587\u5316\u4e3b\u9898\uff0c\u63ed\u793a\u4e86\u5f53\u524dVLM\u5728\u6587\u5316\u591a\u6837\u6027\u73af\u5883\u4e2d\u7684\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5927\u591a\u6570\u8bc4\u4f30\u57fa\u51c6\u4ecd\u7136\u4ee5\u897f\u65b9\u4e3a\u4e2d\u5fc3\uff0c\u7f3a\u4e4f\u5bf9\u6587\u5316\u591a\u6837\u6027\u548c\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u5370\u5ea6\u6b21\u5927\u9646\u8fd9\u6837\u5177\u6709\u4e30\u5bcc\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u7684\u5730\u533a\u5b58\u5728\u660e\u663e\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86IndicVisionBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u82f1\u8bed\u548c10\u79cd\u5370\u5ea6\u8bed\u8a00\uff0c\u5305\u542b\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u3001\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\u548c\u89c6\u89c9\u95ee\u7b543\u79cd\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u6d89\u53ca6\u79cd\u95ee\u9898\u7c7b\u578b\uff0c\u5305\u542b\u7ea65K\u56fe\u50cf\u548c37K+\u95ee\u7b54\u5bf9\uff0c\u8986\u76d613\u4e2a\u6587\u5316\u4e3b\u9898\uff0c\u5e76\u53d1\u5e03\u4e8610\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u5e76\u884c\u6807\u6ce8\u8bed\u6599\u5e93\u3002", "result": "\u8bc4\u4f30\u4e868\u4e2a\u4ece\u4e13\u6709\u95ed\u6e90\u7cfb\u7edf\u5230\u5f00\u6e90\u6743\u91cd\u7684\u4e2d\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u591a\u6837\u6027\u73af\u5883\u4e2d\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u7a81\u663e\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8de8\u6587\u5316\u548c\u591a\u8bed\u8a00\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "IndicVisionBench\u901a\u8fc7\u5173\u6ce8\u6587\u5316\u591a\u6837\u6027\u548c\u591a\u8bed\u8a00\u6027\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u66f4\u5305\u5bb9\u7684\u591a\u6a21\u6001\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u80fd\u591f\u66f4\u597d\u9002\u5e94\u5168\u7403\u6587\u5316\u591a\u6837\u6027\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2511.04753", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04753", "abs": "https://arxiv.org/abs/2511.04753", "authors": ["Zonglin Lyu", "Ming Li", "Xinxin Liu", "Chen Chen"], "title": "CPO: Condition Preference Optimization for Controllable Image Generation", "comment": null, "summary": "To enhance controllability in text-to-image generation, ControlNet introduces\nimage-based control signals, while ControlNet++ improves pixel-level cycle\nconsistency between generated images and the input control signal. To avoid the\nprohibitive cost of back-propagating through the sampling process, ControlNet++\noptimizes only low-noise timesteps (e.g., $t < 200$) using a single-step\napproximation, which not only ignores the contribution of high-noise timesteps\nbut also introduces additional approximation errors. A straightforward\nalternative for optimizing controllability across all timesteps is Direct\nPreference Optimization (DPO), a fine-tuning method that increases model\npreference for more controllable images ($I^{w}$) over less controllable ones\n($I^{l}$). However, due to uncertainty in generative models, it is difficult to\nensure that win--lose image pairs differ only in controllability while keeping\nother factors, such as image quality, fixed. To address this, we propose\nperforming preference learning over control conditions rather than generated\nimages. Specifically, we construct winning and losing control signals,\n$\\mathbf{c}^{w}$ and $\\mathbf{c}^{l}$, and train the model to prefer\n$\\mathbf{c}^{w}$. This method, which we term \\textit{Condition Preference\nOptimization} (CPO), eliminates confounding factors and yields a low-variance\ntraining objective. Our approach theoretically exhibits lower contrastive loss\nvariance than DPO and empirically achieves superior results. Moreover, CPO\nrequires less computation and storage for dataset curation. Extensive\nexperiments show that CPO significantly improves controllability over the\nstate-of-the-art ControlNet++ across multiple control types: over $10\\%$ error\nrate reduction in segmentation, $70$--$80\\%$ in human pose, and consistent\n$2$--$5\\%$ reductions in edge and depth maps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6761\u4ef6\u504f\u597d\u4f18\u5316\uff08CPO\uff09\uff0c\u901a\u8fc7\u76f4\u63a5\u5728\u63a7\u5236\u4fe1\u53f7\u800c\u975e\u751f\u6210\u56fe\u50cf\u4e0a\u8fdb\u884c\u504f\u597d\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u4f18\u5316\u53ef\u63a7\u6027\u65f6\u9762\u4e34\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6df7\u6742\u56e0\u7d20\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u63a7\u5236\u7c7b\u578b\u7684\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982ControlNet++\u4ec5\u4f18\u5316\u4f4e\u566a\u58f0\u65f6\u95f4\u6b65\uff0c\u5ffd\u7565\u4e86\u9ad8\u566a\u58f0\u65f6\u95f4\u6b65\u7684\u8d21\u732e\u5e76\u5f15\u5165\u8fd1\u4f3c\u8bef\u5dee\uff0c\u800c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7531\u4e8e\u751f\u6210\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u96be\u4ee5\u786e\u4fdd\u80dc\u8d1f\u56fe\u50cf\u5bf9\u4ec5\u5728\u53ef\u63a7\u6027\u4e0a\u5b58\u5728\u5dee\u5f02\u800c\u4fdd\u6301\u5176\u4ed6\u56e0\u7d20\u4e0d\u53d8\u3002", "method": "\u63d0\u51fa\u6761\u4ef6\u504f\u597d\u4f18\u5316\uff08CPO\uff09\uff0c\u901a\u8fc7\u6784\u5efa\u80dc\u8d1f\u63a7\u5236\u4fe1\u53f7\uff08c^w\u548cc^l\uff09\u5e76\u8bad\u7ec3\u6a21\u578b\u504f\u597dc^w\uff0c\u76f4\u63a5\u5728\u63a7\u5236\u6761\u4ef6\u800c\u975e\u751f\u6210\u56fe\u50cf\u4e0a\u8fdb\u884c\u504f\u597d\u5b66\u4e60\uff0c\u6d88\u9664\u4e86\u6df7\u6742\u56e0\u7d20\u5e76\u83b7\u5f97\u4e86\u4f4e\u65b9\u5dee\u8bad\u7ec3\u76ee\u6807\u3002", "result": "CPO\u5728\u591a\u4e2a\u63a7\u5236\u7c7b\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684ControlNet++\uff1a\u5206\u5272\u4efb\u52a1\u9519\u8bef\u7387\u964d\u4f4e\u8d85\u8fc710%\uff0c\u4eba\u4f53\u59ff\u6001\u4efb\u52a1\u964d\u4f4e70-80%\uff0c\u8fb9\u7f18\u548c\u6df1\u5ea6\u56fe\u4efb\u52a1\u4e00\u81f4\u964d\u4f4e2-5%\u3002", "conclusion": "CPO\u4e0d\u4ec5\u5728\u7406\u8bba\u4e0a\u5c55\u73b0\u51fa\u6bd4DPO\u66f4\u4f4e\u7684\u5bf9\u6bd4\u635f\u5931\u65b9\u5dee\uff0c\u5b9e\u8bc1\u7ed3\u679c\u4e5f\u66f4\u4f18\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6570\u636e\u96c6\u6784\u5efa\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\uff0c\u4e3a\u53ef\u63a7\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2511.04811", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07, 68U10", "I.2.10; I.4.6; J.3"], "pdf": "https://arxiv.org/pdf/2511.04811", "abs": "https://arxiv.org/abs/2511.04811", "authors": ["Shuo Zhao", "Yu Zhou", "Jianxu Chen"], "title": "An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention", "comment": "6 pages, 4 figures, presented at Bildverarbeitung f\\\"ur die Medizin\n  (BVM) 2025, Wiesbaden, Germany", "summary": "Biomedical image segmentation is critical for precise structure delineation\nand downstream analysis. Traditional methods often struggle with noisy data,\nwhile deep learning models such as U-Net have set new benchmarks in\nsegmentation performance. nnU-Net further automates model configuration, making\nit adaptable across datasets without extensive tuning. However, it requires a\nsubstantial amount of annotated data for cross-validation, posing a challenge\nwhen only raw images but no labels are available. Large foundation models offer\nzero-shot generalizability, but may underperform on specific datasets with\nunique characteristics, limiting their direct use for analysis. This work\naddresses these bottlenecks by proposing a data-centric AI workflow that\nleverages active learning and pseudo-labeling to combine the strengths of\ntraditional neural networks and large foundation models while minimizing human\nintervention. The pipeline starts by generating pseudo-labels from a foundation\nmodel, which are then used for nnU-Net's self-configuration. Subsequently, a\nrepresentative core-set is selected for minimal manual annotation, enabling\neffective fine-tuning of the nnU-Net model. This approach significantly reduces\nthe need for manual annotations while maintaining competitive performance,\nproviding an accessible solution for biomedical researchers to apply\nstate-of-the-art AI techniques in their segmentation tasks. The code is\navailable at https://github.com/MMV-Lab/AL_BioMed_img_seg.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u4e2d\u5fc3\u7684AI\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u548c\u4f2a\u6807\u7b7e\u6280\u672f\uff0c\u5c06\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u4e0e\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u4f18\u52bf\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u4eba\u5de5\u6807\u6ce8\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u74f6\u9888\uff1a\u4f20\u7edf\u65b9\u6cd5\u5982nnU-Net\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u800c\u5927\u578b\u57fa\u7840\u6a21\u578b\u867d\u5177\u5907\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u4f46\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u5f53\u53ea\u6709\u539f\u59cb\u56fe\u50cf\u800c\u65e0\u6807\u6ce8\u6570\u636e\u53ef\u7528\u65f6\uff0c\u8fd9\u4e9b\u9650\u5236\u5c24\u4e3a\u7a81\u51fa\uff0c\u963b\u788d\u4e86\u5148\u8fdbAI\u6280\u672f\u5728\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u7684AI\u5de5\u4f5c\u6d41\u7a0b\uff0c\u9996\u5148\u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u8fd9\u4e9b\u4f2a\u6807\u7b7e\u7528\u4e8ennU-Net\u7684\u81ea\u914d\u7f6e\u8fc7\u7a0b\u3002\u968f\u540e\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u9009\u62e9\u4ee3\u8868\u6027\u6838\u5fc3\u96c6\u8fdb\u884c\u6700\u5c0f\u5316\u4eba\u5de5\u6807\u6ce8\uff0c\u5b9e\u73b0nnU-Net\u6a21\u578b\u7684\u6709\u6548\u5fae\u8c03\u3002\u8be5\u6d41\u7a0b\u7ed3\u5408\u4e86\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u5404\u81ea\u4f18\u52bf\uff0c\u540c\u65f6\u6700\u5927\u7a0b\u5ea6\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u624b\u52a8\u6807\u6ce8\u7684\u9700\u6c42\uff0c\u540c\u65f6\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e86\u7ade\u4e89\u6027\u7684\u6027\u80fd\u8868\u73b0\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u80fd\u591f\u5728\u4ec5\u9700\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u63a5\u8fd1\u5168\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u751f\u7269\u533b\u5b66\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8bbf\u95ee\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u4ed6\u4eec\u80fd\u591f\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u5e94\u7528\u6700\u5148\u8fdb\u7684AI\u6280\u672f\u3002\u5de5\u4f5c\u6d41\u7684\u8bbe\u8ba1\u5e73\u8861\u4e86\u81ea\u52a8\u5316\u4e0e\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u51cf\u5c11\u6807\u6ce8\u8d1f\u62c5\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u7814\u7a76\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2511.04948", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04948", "abs": "https://arxiv.org/abs/2511.04948", "authors": ["Haoxin Lv", "Ijazul Haq", "Jin Du", "Jiaxin Ma", "Binnian Zhu", "Xiaobing Dang", "Chaoan Liang", "Ruxu Du", "Yingjie Zhang", "Muhammad Saqib"], "title": "A benchmark multimodal oro-dental dataset for large vision-language models", "comment": null, "summary": "The advancement of artificial intelligence in oral healthcare relies on the\navailability of large-scale multimodal datasets that capture the complexity of\nclinical practice. In this paper, we present a comprehensive multimodal\ndataset, comprising 8775 dental checkups from 4800 patients collected over\neight years (2018-2025), with patients ranging from 10 to 90 years of age. The\ndataset includes 50000 intraoral images, 8056 radiographs, and detailed textual\nrecords, including diagnoses, treatment plans, and follow-up notes. The data\nwere collected under standard ethical guidelines and annotated for\nbenchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large\nvision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks:\nclassification of six oro-dental anomalies and generation of complete\ndiagnostic reports from multimodal inputs. We compared the fine-tuned models\nwith their base counterparts and GPT-4o. The fine-tuned models achieved\nsubstantial gains over these baselines, validating the dataset and underscoring\nits effectiveness in advancing AI-driven oro-dental healthcare solutions. The\ndataset is publicly available, providing an essential resource for future\nresearch in AI dentistry.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u53e3\u8154\u5065\u5eb7\u6570\u636e\u96c6\uff0c\u5305\u542b8775\u6b21\u7259\u79d1\u68c0\u67e5\u6570\u636e\uff0c\u5e76\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u4e86Qwen-VL\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u53e3\u8154\u5f02\u5e38\u5206\u7c7b\u548c\u8bca\u65ad\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u53e3\u8154\u5065\u5eb7\u9886\u57df\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u53d7\u9650\u4e8e\u7f3a\u4e4f\u80fd\u591f\u53cd\u6620\u4e34\u5e8a\u5b9e\u8df5\u590d\u6742\u6027\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u73b0\u6709\u6570\u636e\u8d44\u6e90\u4e0d\u8db3\u4ee5\u652f\u6491\u5148\u8fdbAI\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "method": "\u6536\u96c6\u4e86\u8de8\u8d8a\u516b\u5e74\uff082018-2025\uff09\u76848775\u6b21\u7259\u79d1\u68c0\u67e5\u6570\u636e\uff0c\u5305\u62ec50000\u5f20\u53e3\u5185\u56fe\u50cf\u30018056\u5f20X\u5149\u7247\u548c\u8be6\u7ec6\u6587\u672c\u8bb0\u5f55\uff1b\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5bf9Qwen-VL 3B\u548c7B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728\u516d\u79cd\u53e3\u8154\u5f02\u5e38\u5206\u7c7b\u548c\u4ece\u591a\u6a21\u6001\u8f93\u5165\u751f\u6210\u5b8c\u6574\u8bca\u65ad\u62a5\u544a\u4e24\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u53e3\u8154\u5f02\u5e38\u5206\u7c7b\u548c\u8bca\u65ad\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e0a\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u548cGPT-4o\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u5e76\u5c55\u793a\u4e86\u5176\u5728\u63a8\u8fdbAI\u9a71\u52a8\u53e3\u8154\u5065\u5eb7\u89e3\u51b3\u65b9\u6848\u65b9\u9762\u7684\u4ef7\u503c\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3aAI\u7259\u79d1\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u8bc1\u660e\u4e86\u5927\u89c4\u6a21\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\u5728\u63d0\u5347\u53e3\u8154\u5065\u5eb7AI\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u672a\u6765AI\u7259\u79d1\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u5e76\u6307\u660e\u4e86\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2511.04688", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04688", "abs": "https://arxiv.org/abs/2511.04688", "authors": ["Adrita Anika", "Md Messal Monem Miah"], "title": "Evaluating LLMs' Reasoning Over Ordered Procedural Steps", "comment": "Accepted to IJCNLP-AACL 2025 Findings", "summary": "Reasoning over procedural sequences, where the order of steps directly\nimpacts outcomes, is a critical capability for large language models (LLMs). In\nthis work, we study the task of reconstructing globally ordered sequences from\nshuffled procedural steps, using a curated dataset of food recipes, a domain\nwhere correct sequencing is essential for task success. We evaluate several\nLLMs under zero-shot and few-shot settings and present a comprehensive\nevaluation framework that adapts established metrics from ranking and sequence\nalignment. These include Kendall's Tau, Normalized Longest Common Subsequence\n(NLCS), and Normalized Edit Distance (NED), which capture complementary aspects\nof ordering quality. Our analysis shows that model performance declines with\nincreasing sequence length, reflecting the added complexity of longer\nprocedures. We also find that greater step displacement in the input,\ncorresponding to more severe shuffling, leads to further degradation. These\nfindings highlight the limitations of current LLMs in procedural reasoning,\nespecially with longer and more disordered inputs.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7a0b\u5e8f\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u91cd\u6784\u6253\u4e71\u7684\u98df\u8c31\u6b65\u9aa4\u5e8f\u5217\u6765\u6d4b\u8bd5\u6a21\u578b\u5bf9\u7a0b\u5e8f\u987a\u5e8f\u7684\u7406\u89e3\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u968f\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u4e14\u8f93\u5165\u6b65\u9aa4\u7684\u66f4\u5927\u4f4d\u79fb\u4f1a\u5bfc\u81f4\u8fdb\u4e00\u6b65\u6027\u80fd\u9000\u5316\u3002", "motivation": "\u7a0b\u5e8f\u5e8f\u5217\u63a8\u7406\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5173\u952e\u80fd\u529b\uff0c\u5176\u4e2d\u6b65\u9aa4\u987a\u5e8f\u76f4\u63a5\u5f71\u54cd\u7ed3\u679c\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5f53\u524dLLMs\u5728\u5904\u7406\u7a0b\u5e8f\u5e8f\u5217\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7406\u89e3\u6b65\u9aa4\u95f4\u987a\u5e8f\u4f9d\u8d56\u5173\u7cfb\u7684\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u98df\u8c31\u8fd9\u4e00\u987a\u5e8f\u81f3\u5173\u91cd\u8981\u7684\u9886\u57df\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u7a0b\u5e8f\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u7cbe\u5fc3\u7b56\u5212\u7684\u98df\u8c31\u6570\u636e\u96c6\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u591a\u4e2aLLMs\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u91c7\u7528\u6765\u81ea\u6392\u5e8f\u548c\u5e8f\u5217\u5bf9\u9f50\u7684\u6210\u719f\u6307\u6807\uff0c\u5305\u62ecKendall's Tau\u3001\u5f52\u4e00\u5316\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\u548c\u5f52\u4e00\u5316\u7f16\u8f91\u8ddd\u79bb\uff0c\u8fd9\u4e9b\u6307\u6807\u6355\u6349\u4e86\u6392\u5e8f\u8d28\u91cf\u7684\u4e0d\u540c\u65b9\u9762\u3002", "result": "\u5b9e\u9a8c\u5206\u6790\u8868\u660e\u6a21\u578b\u6027\u80fd\u968f\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u53cd\u6620\u4e86\u66f4\u957f\u7a0b\u5e8f\u5e26\u6765\u7684\u989d\u5916\u590d\u6742\u6027\u3002\u540c\u65f6\u53d1\u73b0\u8f93\u5165\u4e2d\u66f4\u5927\u7684\u6b65\u9aa4\u4f4d\u79fb\uff08\u5bf9\u5e94\u66f4\u4e25\u91cd\u7684\u6253\u4e71\uff09\u4f1a\u5bfc\u81f4\u8fdb\u4e00\u6b65\u6027\u80fd\u9000\u5316\uff0c\u8fd9\u4e9b\u6307\u6807\u5171\u540c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u7a0b\u5e8f\u5e8f\u5217\u91cd\u6784\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u5f53\u524dLLMs\u5728\u7a0b\u5e8f\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u66f4\u957f\u548c\u66f4\u65e0\u5e8f\u8f93\u5165\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u4e3a\u6539\u8fdb\u6a21\u578b\u5bf9\u7a0b\u5e8f\u987a\u5e8f\u7684\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u5728\u590d\u6742\u7a0b\u5e8f\u63a8\u7406\u4efb\u52a1\u4e2d\u9700\u8981\u8fdb\u4e00\u6b65\u53d1\u5c55\u7684\u65b9\u5411\u3002"}}
{"id": "2511.04963", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04963", "abs": "https://arxiv.org/abs/2511.04963", "authors": ["Xiongri Shen", "Jiaqi Wang", "Yi Zhong", "Zhenxi Song", "Leilei Zhao", "Yichen Wei", "Lingyan Liang", "Shuqiang Wang", "Baiying Lei", "Demao Deng", "Zhiguo Zhang"], "title": "Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement", "comment": null, "summary": "Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and\ndiffusion MRI (dMRI), is essential for studying neurodegenerative diseases.\nHowever, missing modalities pose a major barrier to their clinical use.\nAlthough GAN- and diffusion model-based approaches have shown some promise in\nmodality completion, they remain limited in fMRI-dMRI synthesis due to (1)\nsignificant BOLD vs. diffusion-weighted signal differences between fMRI and\ndMRI in time/gradient axis, and (2) inadequate integration of disease-related\nneuroanatomical patterns during generation. To address these challenges, we\npropose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D\ndiffusion framework for cross-modality learning, and (2) a tissue refinement\nnetwork integrated with a efficient microstructure refinement to maintain\nstructural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house\ndatasets, our method achieves state-of-the-art results, with PSNR/SSIM scores\nof 29.83 dB/90.84\\% for fMRI synthesis (+1.54 dB/+4.12\\% over baselines) and\n30.00 dB/77.55\\% for dMRI synthesis (+1.02 dB/+2.2\\%). In clinical validation,\nthe synthesized data show strong diagnostic performance, achieving\n67.92\\%/66.02\\%/64.15\\% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic\nexperiments. Code is available in \\href{https://github.com/SXR3015/PDS}{PDS\nGitHub Repository}", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPDS\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u5f0f\u611f\u77e5\u53cc\u6a21\u60013D\u6269\u6563\u6846\u67b6\u548c\u96c6\u6210\u5fae\u7ed3\u6784\u4f18\u5316\u7684\u7ec4\u7ec7\u7ec6\u5316\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86fMRI-dMRI\u8de8\u6a21\u6001\u5408\u6210\u4e2d\u7684\u4fe1\u53f7\u5dee\u5f02\u548c\u795e\u7ecf\u89e3\u5256\u6a21\u5f0f\u6574\u5408\u4e0d\u8db3\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5408\u6210\u6027\u80fd\u3002", "motivation": "\u529f\u80fd\u78c1\u5171\u632f\u6210\u50cf\uff08fMRI\uff09\u548c\u6269\u6563\u78c1\u5171\u632f\u6210\u50cf\uff08dMRI\uff09\u5728\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7814\u7a76\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6a21\u6001\u7f3a\u5931\u4e25\u91cd\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\u3002\u73b0\u6709\u57fa\u4e8eGAN\u548c\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u5728fMRI-dMRI\u5408\u6210\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u4e00\u662ffMRI\u4e0edMRI\u5728\u65f6\u95f4/\u68af\u5ea6\u8f74\u4e0a\u5b58\u5728\u663e\u8457\u7684BOLD\u4e0e\u6269\u6563\u52a0\u6743\u4fe1\u53f7\u5dee\u5f02\uff0c\u4e8c\u662f\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u672a\u80fd\u5145\u5206\u6574\u5408\u75be\u75c5\u76f8\u5173\u7684\u795e\u7ecf\u89e3\u5256\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u7684PDS\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u4e00\u662f\u6a21\u5f0f\u611f\u77e5\u53cc\u6a21\u60013D\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u6a21\u6001\u5b66\u4e60\uff1b\u4e8c\u662f\u96c6\u6210\u9ad8\u6548\u5fae\u7ed3\u6784\u4f18\u5316\u7684\u7ec4\u7ec7\u7ec6\u5316\u7f51\u7edc\uff0c\u4ee5\u4fdd\u6301\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u7ec6\u8282\u5b8c\u6574\u6027\u3002\u8be5\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9fMRI\u548cdMRI\u4e4b\u95f4\u7684\u4fe1\u53f7\u5dee\u5f02\u8fdb\u884c\u4f18\u5316\u8bbe\u8ba1\u3002", "result": "\u5728OASIS-3\u3001ADNI\u548c\u5185\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1afMRI\u5408\u6210\u7684PSNR/SSIM\u8fbe\u523029.83 dB/90.84%\uff08\u6bd4\u57fa\u7ebf\u63d0\u53471.54 dB/4.12%\uff09\uff0cdMRI\u5408\u6210\u8fbe\u523030.00 dB/77.55%\uff08\u63d0\u53471.02 dB/2.2%\uff09\u3002\u4e34\u5e8a\u9a8c\u8bc1\u4e2d\uff0c\u5408\u6210\u6570\u636e\u5728\u6df7\u5408\u771f\u5b9e-\u5408\u6210\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8bca\u65ad\u6027\u80fd\uff0cNC vs. MCI vs. AD\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523067.92%/66.02%/64.15%\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86PDS\u65b9\u6cd5\u5728\u89e3\u51b3fMRI-dMRI\u8de8\u6a21\u6001\u5408\u6210\u6311\u6218\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u5408\u6210\u8d28\u91cf\u6307\u6807\uff0c\u66f4\u91cd\u8981\u7684\u662f\u5408\u6210\u6570\u636e\u5728\u4e34\u5e8a\u8bca\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6a21\u6001\u8865\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.04754", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04754", "abs": "https://arxiv.org/abs/2511.04754", "authors": ["Nikolai Ilinykh", "Simon Dobnik"], "title": "Surprisal reveals diversity gaps in image captioning and different scorers change the story", "comment": "Accepted and presented at INLG 2025", "summary": "We quantify linguistic diversity in image captioning with surprisal variance\n- the spread of token-level negative log-probabilities within a caption set. On\nthe MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs,\ndecoded with greedy and nucleus sampling, to human captions. Measured with a\ncaption-trained n-gram LM, humans display roughly twice the surprisal variance\nof models, but rescoring the same captions with a general-language model\nreverses the pattern. Our analysis introduces the surprisal-based diversity\nmetric for image captioning. We show that relying on a single scorer can\ncompletely invert conclusions, thus, robust diversity evaluation must report\nsurprisal under several scorers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u57fa\u4e8e\u60ca\u5947\u5ea6\u65b9\u5dee\u7684\u591a\u6837\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u7684\u8bcd\u6c47\u591a\u6837\u6027\uff0c\u5e76\u53d1\u73b0\u4f9d\u8d56\u5355\u4e00\u8bc4\u5206\u6a21\u578b\u4f1a\u5b8c\u5168\u98a0\u5012\u5173\u4e8e\u4eba\u7c7b\u4e0e\u6a21\u578b\u591a\u6837\u6027\u7684\u7ed3\u8bba\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u7f3a\u4e4f\u6709\u6548\u7684\u8bcd\u6c47\u591a\u6837\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u96be\u4ee5\u51c6\u786e\u8861\u91cf\u63cf\u8ff0\u6587\u672c\u7684\u8bed\u8a00\u591a\u6837\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u591a\u6837\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u60ca\u5947\u5ea6\u65b9\u5dee\u7684\u591a\u6837\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4f7f\u7528n-gram\u8bed\u8a00\u6a21\u578b\u548c\u901a\u7528\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5206\u5668\uff0c\u5728MSCOCO\u6d4b\u8bd5\u96c6\u4e0a\u6bd4\u8f83\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\u4e0e\u4eba\u7c7b\u63cf\u8ff0\uff0c\u91c7\u7528\u8d2a\u5a6a\u89e3\u7801\u548c\u6838\u91c7\u6837\u4e24\u79cd\u89e3\u7801\u7b56\u7565\u3002", "result": "\u4f7f\u7528\u63cf\u8ff0\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u8bc4\u5206\u65f6\uff0c\u4eba\u7c7b\u63cf\u8ff0\u7684\u60ca\u5947\u5ea6\u65b9\u5dee\u7ea6\u4e3a\u6a21\u578b\u7684\u4e24\u500d\uff0c\u4f46\u4f7f\u7528\u901a\u7528\u8bed\u8a00\u6a21\u578b\u91cd\u65b0\u8bc4\u5206\u540e\uff0c\u8fd9\u4e00\u6a21\u5f0f\u5b8c\u5168\u53cd\u8f6c\uff0c\u8868\u660e\u8bc4\u5206\u5668\u9009\u62e9\u5bf9\u591a\u6837\u6027\u8bc4\u4f30\u7ed3\u8bba\u5177\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u56fe\u50cf\u63cf\u8ff0\u591a\u6837\u6027\u8bc4\u4f30\u5fc5\u987b\u8003\u8651\u591a\u4e2a\u8bc4\u5206\u5668\u7684\u7ed3\u679c\uff0c\u5355\u4e00\u8bc4\u5206\u5668\u53ef\u80fd\u5bfc\u81f4\u5b8c\u5168\u9519\u8bef\u7684\u7ed3\u8bba\uff0c\u4e3a\u6784\u5efa\u66f4\u7a33\u5065\u7684\u591a\u6837\u6027\u8bc4\u4f30\u6846\u67b6\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2511.04977", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.04977", "abs": "https://arxiv.org/abs/2511.04977", "authors": ["Heng Er Metilda Chee", "Jiayin Wang", "Zhiqiang Guo", "Weizhi Ma", "Min Zhang"], "title": "GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder", "comment": null, "summary": "Stickers have become a popular form of visual communication, yet\nunderstanding their semantic relationships remains challenging due to their\nhighly diverse and symbolic content. In this work, we formally {define the\nSticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark\nfor this task, consisting of 905 human-annotated positive and negative sticker\npairs. Through extensive evaluation, we show that existing pretrained vision\nand multimodal models struggle to capture nuanced sticker semantics. To address\nthis, we propose the {General Sticker Encoder (GSE)}, a lightweight and\nversatile model that learns robust sticker embeddings using both Triple-S and\nadditional datasets. GSE achieves superior performance on unseen stickers, and\ndemonstrates strong results on downstream tasks such as emotion classification\nand sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we\nprovide standardized evaluation tools and robust embeddings, enabling future\nresearch in sticker understanding, retrieval, and multimodal content\ngeneration. The Triple-S benchmark and GSE have been publicly released and are\navailable here.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8d34\u7eb8\u8bed\u4e49\u76f8\u4f3c\u6027\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86\u9996\u4e2a\u57fa\u51c6\u6570\u636e\u96c6Triple-S\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u901a\u7528\u8d34\u7eb8\u7f16\u7801\u5668GSE\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u9c81\u68d2\u7684\u8d34\u7eb8\u5d4c\u5165\u8868\u793a\uff0c\u5728\u672a\u89c1\u8d34\u7eb8\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8d34\u7eb8\u5df2\u6210\u4e3a\u6d41\u884c\u7684\u89c6\u89c9\u4ea4\u6d41\u5f62\u5f0f\uff0c\u4f46\u7531\u4e8e\u5176\u5185\u5bb9\u9ad8\u5ea6\u591a\u6837\u5316\u548c\u7b26\u53f7\u5316\uff0c\u7406\u89e3\u5176\u8bed\u4e49\u5173\u7cfb\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u9884\u8bad\u7ec3\u89c6\u89c9\u548c\u591a\u6a21\u6001\u6a21\u578b\u96be\u4ee5\u6355\u6349\u8d34\u7eb8\u7684\u7ec6\u5fae\u8bed\u4e49\u5dee\u5f02\uff0c\u9700\u8981\u4e13\u95e8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u6b63\u5f0f\u5b9a\u4e49\u4e86\u8d34\u7eb8\u8bed\u4e49\u76f8\u4f3c\u6027\u4efb\u52a1\uff0c\u6784\u5efa\u4e86\u5305\u542b905\u4e2a\u4eba\u5de5\u6807\u6ce8\u6b63\u8d1f\u8d34\u7eb8\u5bf9\u7684Triple-S\u57fa\u51c6\u6570\u636e\u96c6\u3002\u63d0\u51fa\u4e86\u901a\u7528\u8d34\u7eb8\u7f16\u7801\u5668GSE\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u901a\u7528\u7684\u6a21\u578b\uff0c\u5229\u7528Triple-S\u548c\u989d\u5916\u6570\u636e\u96c6\u5b66\u4e60\u9c81\u68d2\u7684\u8d34\u7eb8\u5d4c\u5165\u8868\u793a\u3002", "result": "GSE\u5728\u672a\u89c1\u8d34\u7eb8\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u5982\u60c5\u611f\u5206\u7c7b\u548c\u8d34\u7eb8\u5230\u8d34\u7eb8\u68c0\u7d22\u4e2d\u8868\u73b0\u51fa\u5f3a\u52b2\u7ed3\u679c\u3002\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u6355\u6349\u8d34\u7eb8\u8bed\u4e49\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u800cGSE\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03Triple-S\u57fa\u51c6\u548cGSE\u6a21\u578b\uff0c\u672c\u7814\u7a76\u4e3a\u6807\u51c6\u5316\u7684\u8d34\u7eb8\u7406\u89e3\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u9c81\u68d2\u5d4c\u5165\u8868\u793a\uff0c\u4e3a\u672a\u6765\u8d34\u7eb8\u7406\u89e3\u3001\u68c0\u7d22\u548c\u591a\u6a21\u6001\u5185\u5bb9\u751f\u6210\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u8fd9\u4e9b\u8d44\u6e90\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.04910", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04910", "abs": "https://arxiv.org/abs/2511.04910", "authors": ["Jaehoon Lee", "Sohyun Kim", "Wanggeun Park", "Geon Lee", "Seungkyung Kim", "Minyoung Lee"], "title": "SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents", "comment": "27 pages, 15 figures, 6 tables", "summary": "Existing benchmarks for visual document retrieval (VDR) largely overlook\nnon-English languages and the structural complexity of official publications.\nTo address this critical gap, we introduce SDS KoPub VDR, the first\nlarge-scale, publicly available benchmark for retrieving and understanding\nKorean public documents. The benchmark is built upon a corpus of 361 real-world\ndocuments (40,781 pages), including 256 files under the KOGL Type 1 license and\n105 from official legal portals, capturing complex visual elements like tables,\ncharts, and multi-column layouts. To establish a challenging and reliable\nevaluation set, we constructed 600 query-page-answer triples. These were\ninitially generated using multimodal models (e.g., GPT-4o) and subsequently\nunderwent a rigorous human verification and refinement process to ensure\nfactual accuracy and contextual relevance. The queries span six major public\ndomains and are systematically categorized by the reasoning modality required:\ntext-based, visual-based (e.g., chart interpretation), and cross-modal. We\nevaluate SDS KoPub VDR on two complementary tasks that reflect distinct\nretrieval paradigms: (1) text-only retrieval, which measures a model's ability\nto locate relevant document pages based solely on textual signals, and (2)\nmultimodal retrieval, which assesses retrieval performance when visual features\n(e.g., tables, charts, and layouts) are jointly leveraged alongside text. This\ndual-task evaluation reveals substantial performance gaps, particularly in\nmultimodal scenarios requiring cross-modal reasoning, even for state-of-the-art\nmodels. As a foundational resource, SDS KoPub VDR not only enables rigorous and\nfine-grained evaluation across textual and multimodal retrieval tasks but also\nprovides a clear roadmap for advancing multimodal AI in complex, real-world\ndocument intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SDS KoPub VDR\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u97e9\u6587\u516c\u5171\u6587\u6863\u68c0\u7d22\u7684\u5927\u89c4\u6a21\u516c\u5f00\u57fa\u51c6\uff0c\u5305\u542b361\u4e2a\u771f\u5b9e\u4e16\u754c\u6587\u6863\u548c600\u4e2a\u67e5\u8be2-\u9875\u9762-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u901a\u8fc7\u53cc\u4efb\u52a1\u8bc4\u4f30\u63ed\u793a\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u7684\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u57fa\u51c6\u4e3b\u8981\u5ffd\u89c6\u975e\u82f1\u8bed\u8bed\u8a00\u548c\u5b98\u65b9\u51fa\u7248\u7269\u7684\u7ed3\u6784\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u9488\u5bf9\u97e9\u6587\u516c\u5171\u6587\u6863\u7684\u53ef\u9760\u8bc4\u4f30\u8d44\u6e90\uff0c\u8fd9\u9650\u5236\u4e86\u591a\u6a21\u6001AI\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6587\u6863\u667a\u80fd\u4e2d\u7684\u53d1\u5c55\u3002", "method": "\u57fa\u4e8e361\u4e2a\u771f\u5b9e\u4e16\u754c\u97e9\u6587\u516c\u5171\u6587\u6863\u6784\u5efa\u5927\u89c4\u6a21\u8bed\u6599\u5e93\uff0c\u91c7\u7528\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210600\u4e2a\u67e5\u8be2-\u9875\u9762-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u5e76\u901a\u8fc7\u4e25\u683c\u7684\u4eba\u5de5\u9a8c\u8bc1\u786e\u4fdd\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff0c\u8bc4\u4f30\u6db5\u76d6\u6587\u672c\u68c0\u7d22\u548c\u591a\u6a21\u6001\u68c0\u7d22\u4e24\u79cd\u4e92\u8865\u4efb\u52a1\u3002", "result": "\u53cc\u4efb\u52a1\u8bc4\u4f30\u663e\u793a\u5373\u4f7f\u5728\u6700\u5148\u8fdb\u6a21\u578b\u4e2d\u4e5f\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u8de8\u6a21\u6001\u63a8\u7406\u7684\u591a\u6a21\u6001\u573a\u666f\u4e2d\uff0c\u7a81\u663e\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u89c6\u89c9\u5143\u7d20\u548c\u8de8\u6a21\u6001\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "SDS KoPub VDR\u4e0d\u4ec5\u4e3a\u6587\u672c\u548c\u591a\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e25\u683c\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u8fd8\u4e3a\u63a8\u8fdb\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6587\u6863\u667a\u80fd\u4e2d\u7684\u591a\u6a21\u6001AI\u53d1\u5c55\u6307\u660e\u4e86\u6e05\u6670\u7684\u6280\u672f\u8def\u7ebf\u56fe\u3002"}}
{"id": "2511.05017", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.05017", "abs": "https://arxiv.org/abs/2511.05017", "authors": ["Aakriti Agrawal", "Gouthaman KV", "Rohith Aralikatti", "Gauri Jagatap", "Jiaxin Yuan", "Vijay Kamarshi", "Andrea Fanelli", "Furong Huang"], "title": "Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings", "comment": null, "summary": "In this work, we identify an inherent bias in prevailing LVLM architectures\ntoward the language modality, largely resulting from the common practice of\nsimply appending visual embeddings to the input text sequence. To address this,\nwe propose a simple yet effective method that refines textual embeddings by\nintegrating average-pooled visual features. Our approach demonstrably improves\nvisual grounding and significantly reduces hallucinations on established\nbenchmarks. While average pooling offers a straightforward, robust, and\nefficient means of incorporating visual information, we believe that more\nsophisticated fusion methods could further enhance visual grounding and\ncross-modal alignment. Given that the primary focus of this work is to\nhighlight the modality imbalance and its impact on hallucinations -- and to\nshow that refining textual embeddings with visual information mitigates this\nissue -- we leave exploration of advanced fusion strategies for future work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u4f7f\u7528\u5e73\u5747\u6c60\u5316\u89c6\u89c9\u7279\u5f81\u6765\u7cbe\u70bc\u6587\u672c\u5d4c\u5165\uff0c\u663e\u8457\u6539\u5584\u4e86\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u5e76\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\u3002", "motivation": "\u5f53\u524dLVLM\u67b6\u6784\u5b58\u5728\u56fa\u6709\u7684\u8bed\u8a00\u6a21\u6001\u504f\u5411\u95ee\u9898\uff0c\u8fd9\u4e3b\u8981\u6e90\u4e8e\u5c06\u89c6\u89c9\u5d4c\u5165\u7b80\u5355\u9644\u52a0\u5230\u8f93\u5165\u6587\u672c\u5e8f\u5217\u7684\u5e38\u89c1\u505a\u6cd5\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u4fe1\u606f\u800c\u5ffd\u89c6\u89c6\u89c9\u5185\u5bb9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u5e73\u5747\u6c60\u5316\u7684\u89c6\u89c9\u7279\u5f81\u6765\u7cbe\u70bc\u6587\u672c\u5d4c\u5165\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u63a5\u3001\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89c6\u89c9\u4fe1\u606f\u878d\u5408\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5df2\u5efa\u7acb\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u6539\u5584\u4e86\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\uff0c\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u89c6\u89c9\u4fe1\u606f\u7cbe\u70bc\u6587\u672c\u5d4c\u5165\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u63ed\u793a\u6a21\u6001\u4e0d\u5e73\u8861\u53ca\u5176\u5bf9\u5e7b\u89c9\u7684\u5f71\u54cd\uff0c\u5e76\u8bc1\u660e\u901a\u8fc7\u89c6\u89c9\u4fe1\u606f\u7cbe\u70bc\u6587\u672c\u5d4c\u5165\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u540c\u65f6\u6307\u51fa\u66f4\u590d\u6742\u7684\u878d\u5408\u65b9\u6cd5\u53ef\u80fd\u8fdb\u4e00\u6b65\u6539\u5584\u89c6\u89c9\u5b9a\u4f4d\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.05361", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05361", "abs": "https://arxiv.org/abs/2511.05361", "authors": ["Maria Huynh", "Wilder C. Rodrigues"], "title": "A multimodal multiplex of the mental lexicon for multilingual individuals", "comment": null, "summary": "Historically, bilingualism was often perceived as an additional cognitive\nload that could hinder linguistic and intellectual development. However, over\nthe last three decades, this view has changed considerably. Numerous studies\nhave aimed to model and understand the architecture of the bilingual word\nrecognition system Dijkstra and van Heuven (2002), investigating how parallel\nactivation operates in the brain and how one language influences another Kroll\net al. (2015). Increasingly, evidence suggests that multilinguals, individuals\nwho speak three or more languages, can perform better than monolinguals in\nvarious linguistic and cognitive tasks, such as learning an additional language\nAbu-Rabia and Sanitsky (2010). This research proposal focuses on the study of\nthe mental lexicon and how it may be structured in individuals who speak\nmultiple languages. Building on the work of Stella et al. (2018), who\ninvestigated explosive learning in humans using a multiplex model of the mental\nlexicon, and the Bilingual Interactive Activation (BIA+) framework proposed by\nDijkstra and van Heuven (2002), the present study applies the same multilayer\nnetwork principles introduced by Kivela et al. (2014). Our experimental design\nextends previous research by incorporating multimodality into the multiplex\nmodel, introducing an additional layer that connects visual inputs to their\ncorresponding lexical representations across the multilingual layers of the\nmental lexicon. In this research, we aim to explore how a heritage language\ninfluences the acquisition of another language. Specifically, we ask: Does the\npresence of visual input in a translation task influence participants'\nproficiency and accuracy compared to text-only conditions?", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u591a\u8bed\u8a00\u5fc3\u7406\u8bcd\u5178\u7684\u591a\u5c42\u7f51\u7edc\u6a21\u578b\uff0c\u63a2\u7d22\u89c6\u89c9\u8f93\u5165\u5bf9\u591a\u8bed\u8a00\u4e60\u5f97\u7684\u5f71\u54cd\uff0c\u7279\u522b\u5173\u6ce8\u9057\u4ea7\u8bed\u8a00\u5728\u8bed\u8a00\u4e60\u5f97\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u4e0e\u89c6\u89c9\u6a21\u6001\u7684\u4ea4\u4e92\u6548\u5e94\u3002", "motivation": "\u4f20\u7edf\u4e0a\u53cc\u8bed\u88ab\u89c6\u4e3a\u8ba4\u77e5\u8d1f\u62c5\uff0c\u4f46\u8fd1\u4e09\u5341\u5e74\u7814\u7a76\u8868\u660e\u591a\u8bed\u8a00\u8005\u5728\u8bed\u8a00\u548c\u8ba4\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u591a\u8bed\u8a00\u5fc3\u7406\u8bcd\u5178\u7684\u7ed3\u6784\u673a\u5236\uff0c\u7279\u522b\u5173\u6ce8\u89c6\u89c9\u8f93\u5165\u5982\u4f55\u5f71\u54cd\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u8bed\u8a00\u719f\u7ec3\u5ea6\u548c\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8eStella\u7b49\u4eba\u7684\u5fc3\u7406\u8bcd\u5178\u591a\u8def\u590d\u7528\u6a21\u578b\u548cDijkstra\u7684\u53cc\u8bed\u4ea4\u4e92\u6fc0\u6d3b\u6846\u67b6\uff0c\u91c7\u7528Kivela\u63d0\u51fa\u7684\u591a\u5c42\u7f51\u7edc\u539f\u7406\uff0c\u5728\u6a21\u578b\u4e2d\u5f15\u5165\u89c6\u89c9\u6a21\u6001\u5c42\uff0c\u5c06\u89c6\u89c9\u8f93\u5165\u4e0e\u591a\u8bed\u8a00\u5c42\u7684\u8bcd\u6c47\u8868\u5f81\u76f8\u8fde\u63a5\u3002", "result": "\u5b9e\u9a8c\u8bbe\u8ba1\u6bd4\u8f83\u4e86\u6587\u672c\u6761\u4ef6\u548c\u89c6\u89c9\u8f93\u5165\u6761\u4ef6\u4e0b\u7684\u7ffb\u8bd1\u4efb\u52a1\u8868\u73b0\uff0c\u65e8\u5728\u91cf\u5316\u89c6\u89c9\u6a21\u6001\u5bf9\u591a\u8bed\u8a00\u4e60\u5f97\u7684\u5f71\u54cd\uff0c\u7279\u522b\u5173\u6ce8\u9057\u4ea7\u8bed\u8a00\u5728\u8bed\u8a00\u4e60\u5f97\u8fc7\u7a0b\u4e2d\u7684\u4fc3\u8fdb\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u5fc3\u7406\u8bcd\u5178\u7684\u591a\u5c42\u7f51\u7edc\u7ed3\u6784\u7279\u6027\uff0c\u89c6\u89c9\u6a21\u6001\u7684\u5f15\u5165\u4e3a\u7406\u89e3\u591a\u8bed\u8a00\u4e60\u5f97\u7684\u8ba4\u77e5\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5bf9\u8bed\u8a00\u6559\u80b2\u548c\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u5177\u6709\u91cd\u8981\u542f\u793a\u610f\u4e49\u3002"}}
{"id": "2511.05034", "categories": ["cs.CV", "cs.AI", "I.4.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2511.05034", "abs": "https://arxiv.org/abs/2511.05034", "authors": ["Jing Jin", "Xu Liu", "Te Gao", "Zhihong Shi", "Yixiong Liang", "Ruiqing Zheng", "Hulin Kuang", "Min Zeng", "Shichao Kan"], "title": "Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation", "comment": "8pages, 3figures, published to ACM Digital Library", "summary": "Whole Slide Image (WSI) representation is critical for cancer subtyping,\ncancer recognition and mutation prediction.Training an end-to-end WSI\nrepresentation model poses significant challenges, as a standard gigapixel\nslide can contain tens of thousands of image tiles, making it difficult to\ncompute gradients of all tiles in a single mini-batch due to current GPU\nlimitations. To address this challenge, we propose a method of dynamic residual\nencoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI\nrepresentation. Our approach utilizes a memory bank to store the features of\ntiles across all WSIs in the dataset. During training, a mini-batch usually\ncontains multiple WSIs. For each WSI in the batch, a subset of tiles is\nrandomly sampled and their features are computed using a tile encoder. Then,\nadditional tile features from the same WSI are selected from the memory bank.\nThe representation of each individual WSI is generated using a residual\nencoding technique that incorporates both the sampled features and those\nretrieved from the memory bank. Finally, the slide-level contrastive loss is\ncomputed based on the representations and histopathology reports ofthe WSIs\nwithin the mini-batch. Experiments conducted over cancer subtyping, cancer\nrecognition, and mutation prediction tasks proved the effectiveness of the\nproposed DRE-SLCL method.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6b8b\u5dee\u7f16\u7801\u4e0e\u5207\u7247\u7ea7\u5bf9\u6bd4\u5b66\u4e60\uff08DRE-SLCL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u7684\u5168\u5207\u7247\u56fe\u50cf\u8868\u793a\u5b66\u4e60\uff0c\u901a\u8fc7\u5185\u5b58\u5e93\u5b58\u50a8\u74e6\u7247\u7279\u5f81\u5e76\u7ed3\u5408\u6b8b\u5dee\u7f16\u7801\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86GPU\u5185\u5b58\u9650\u5236\u4e0b\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u6240\u6709\u74e6\u7247\u7684\u95ee\u9898\u3002", "motivation": "\u5168\u5207\u7247\u56fe\u50cf\u8868\u793a\u5728\u764c\u75c7\u5206\u578b\u3001\u8bc6\u522b\u548c\u7a81\u53d8\u9884\u6d4b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u6807\u51c6\u5343\u5146\u50cf\u7d20\u5207\u7247\u5305\u542b\u6570\u4e07\u4e2a\u56fe\u50cf\u74e6\u7247\uff0c\u5728\u5f53\u524dGPU\u9650\u5236\u4e0b\u65e0\u6cd5\u5728\u5355\u4e2a\u5c0f\u6279\u91cf\u4e2d\u8ba1\u7b97\u6240\u6709\u74e6\u7247\u7684\u68af\u5ea6\uff0c\u8fd9\u7ed9\u7aef\u5230\u7aefWSI\u8868\u793a\u6a21\u578b\u7684\u8bad\u7ec3\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u5185\u5b58\u5e93\u5b58\u50a8\u6570\u636e\u96c6\u4e2d\u6240\u6709WSI\u7684\u74e6\u7247\u7279\u5f81\uff0c\u8bad\u7ec3\u65f6\u6bcf\u4e2a\u5c0f\u6279\u91cf\u5305\u542b\u591a\u4e2aWSI\uff0c\u5bf9\u6bcf\u4e2aWSI\u968f\u673a\u91c7\u6837\u90e8\u5206\u74e6\u7247\u5e76\u901a\u8fc7\u74e6\u7247\u7f16\u7801\u5668\u8ba1\u7b97\u7279\u5f81\uff0c\u540c\u65f6\u4ece\u5185\u5b58\u5e93\u4e2d\u9009\u62e9\u540c\u4e00WSI\u7684\u989d\u5916\u74e6\u7247\u7279\u5f81\uff0c\u91c7\u7528\u6b8b\u5dee\u7f16\u7801\u6280\u672f\u7ed3\u5408\u91c7\u6837\u7279\u5f81\u548c\u68c0\u7d22\u7279\u5f81\u751f\u6210\u5355\u4e2aWSI\u7684\u8868\u793a\uff0c\u6700\u540e\u57fa\u4e8e\u5c0f\u6279\u91cf\u5185WSI\u7684\u8868\u793a\u548c\u7ec4\u7ec7\u75c5\u7406\u5b66\u62a5\u544a\u8ba1\u7b97\u5207\u7247\u7ea7\u5bf9\u6bd4\u635f\u5931\u3002", "result": "\u5728\u764c\u75c7\u5206\u578b\u3001\u764c\u75c7\u8bc6\u522b\u548c\u7a81\u53d8\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684DRE-SLCL\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u514b\u670dGPU\u5185\u5b58\u9650\u5236\u5e76\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684WSI\u8868\u793a\u5b66\u4e60\u3002", "conclusion": "DRE-SLCL\u65b9\u6cd5\u4e3a\u7aef\u5230\u7aefWSI\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u6001\u6b8b\u5dee\u7f16\u7801\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u4e0d\u4ec5\u89e3\u51b3\u4e86\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u95ee\u9898\uff0c\u8fd8\u4e3a\u764c\u75c7\u76f8\u5173\u7684\u591a\u79cd\u75c5\u7406\u5b66\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u8868\u793a\u80fd\u529b\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.05044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05044", "abs": "https://arxiv.org/abs/2511.05044", "authors": ["Xinyu Chen", "Yiran Wang", "Gaoyang Pang", "Jiafu Hao", "Chentao Yue", "Luping Zhou", "Yonghui Li"], "title": "Medical Referring Image Segmentation via Next-Token Mask Prediction", "comment": "This work has been submitted to the IEEE Transactions on Medical\n  Imaging for possible publication", "summary": "Medical Referring Image Segmentation (MRIS) involves segmenting target\nregions in medical images based on natural language descriptions. While\nachieving promising results, recent approaches usually involve complex design\nof multimodal fusion or multi-stage decoders. In this work, we propose\nNTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive\nnext-token prediction task over a unified multimodal sequence of tokenized\nimage, text, and mask representations. This formulation streamlines model\ndesign by eliminating the need for modality-specific fusion and external\nsegmentation models, supports a unified architecture for end-to-end training.\nIt also enables the use of pretrained tokenizers from emerging large-scale\nmultimodal models, enhancing generalization and adaptability. More importantly,\nto address challenges under this formulation-such as exposure bias, long-tail\ntoken distributions, and fine-grained lesion edges-we propose three novel\nstrategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative\nprediction errors, (2) Token-level Contrastive Learning (TCL) to enhance\nboundary sensitivity and mitigate long-tail distribution effects, and (3) a\nmemory-based Hard Error Token (HET) optimization strategy that emphasizes\ndifficult tokens during training. Extensive experiments on the QaTa-COV19 and\nMosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art\nperformance, offering a streamlined and effective alternative to traditional\nMRIS pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNTP-MRISeg\u6846\u67b6\uff0c\u5c06\u533b\u5b66\u53c2\u8003\u56fe\u50cf\u5206\u5272\u91cd\u65b0\u8868\u8ff0\u4e3a\u591a\u6a21\u6001\u5e8f\u5217\u4e0a\u7684\u81ea\u56de\u5f52\u4e0b\u4e00\u4ee4\u724c\u9884\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u7b80\u5316\u4e86\u6a21\u578b\u8bbe\u8ba1\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u53c2\u8003\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u901a\u5e38\u6d89\u53ca\u590d\u6742\u7684\u591a\u6a21\u6001\u878d\u5408\u6216\u591a\u9636\u6bb5\u89e3\u7801\u5668\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u6a21\u578b\u67b6\u6784\u590d\u6742\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u7b80\u6d01\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faNTP-MRISeg\u6846\u67b6\uff0c\u5c06MRIS\u91cd\u65b0\u8868\u8ff0\u4e3a\u56fe\u50cf\u3001\u6587\u672c\u548c\u63a9\u7801\u8868\u793a\u7684\u7edf\u4e00\u591a\u6a21\u6001\u5e8f\u5217\u4e0a\u7684\u81ea\u56de\u5f52\u4e0b\u4e00\u4ee4\u724c\u9884\u6d4b\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e09\u79cd\u5173\u952e\u7b56\u7565\uff1a\u4e0b\u4e00k\u4ee4\u724c\u9884\u6d4b\u65b9\u6848\u51cf\u5c11\u7d2f\u79ef\u8bef\u5dee\u3001\u4ee4\u724c\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u8fb9\u754c\u654f\u611f\u6027\u3001\u57fa\u4e8e\u5185\u5b58\u7684\u786c\u9519\u8bef\u4ee4\u724c\u4f18\u5316\u7b56\u7565\u4e13\u6ce8\u4e8e\u56f0\u96be\u4ee4\u724c\u3002", "result": "\u5728QaTa-COV19\u548cMosMedData+\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cNTP-MRISeg\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u533b\u5b66\u53c2\u8003\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u81ea\u56de\u5f52\u4ee4\u724c\u9884\u6d4b\u6846\u67b6\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u533b\u5b66\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\u3002"}}
{"id": "2511.05057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05057", "abs": "https://arxiv.org/abs/2511.05057", "authors": ["Yuanxiang Huangfu", "Chaochao Wang", "Weilei Wang"], "title": "Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach", "comment": null, "summary": "The effectiveness of Contrastive Language-Image Pre-training (CLIP) models\ncritically depends on the semantic diversity and quality of their training\ndata. However, while existing synthetic data generation methods primarily focus\non increasing data volume, such emphasis often leads to limited semantic\ndiversity and redundant or shallow captions. To address this limitation, we\npropose Role-SynthCLIP, a novel data synthesis framework that leverages\nmulti-perspective role-playing prompts (e.g., a compositional analyst, an\ninterpreter of image context) to guide Multimodal Large Language Models (MLLMs)\nin generating semantically diverse captions from distinct viewpoints. This\nmechanism enhances the semantic diversity and fine-grained image-text alignment\nof synthetic pairs, thereby improving caption expressiveness and accuracy while\nkeeping the total number of image-text pairs unchanged. Experimental results\ndemonstrate the effectiveness and efficiency of our method. A CLIP-B/16 model\ntrained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on\nthe MS COCO validation set, surpassing the best existing synthetic data\nbaseline (trained on 5M pairs) by 2.8 percentage points. The code and trained\nmodels are released at https://github.com/huangfu170/Role-SynthCLIP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Role-SynthCLIP\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u89d2\u8272\u626e\u6f14\u63d0\u793a\u5f15\u5bfc\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bed\u4e49\u591a\u6837\u5316\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u4ec5\u4f7f\u7528100\u4e07\u5bf9\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u4e8e500\u4e07\u5bf9\u5408\u6210\u6570\u636e\u7684\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u589e\u52a0\u6570\u636e\u91cf\uff0c\u4f46\u8fd9\u79cd\u5f3a\u8c03\u5f80\u5f80\u5bfc\u81f4\u8bed\u4e49\u591a\u6837\u6027\u6709\u9650\u4ee5\u53ca\u751f\u6210\u5197\u4f59\u6216\u6d45\u663e\u7684\u63cf\u8ff0\u6587\u672c\uff0c\u9650\u5236\u4e86\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6548\u679c\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86Role-SynthCLIP\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u5229\u7528\u591a\u89c6\u89d2\u89d2\u8272\u626e\u6f14\u63d0\u793a\uff08\u5982\u7ec4\u5408\u5206\u6790\u5e08\u3001\u56fe\u50cf\u4e0a\u4e0b\u6587\u89e3\u91ca\u5668\u7b49\uff09\u5f15\u5bfc\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u4e0d\u540c\u89c6\u89d2\u751f\u6210\u8bed\u4e49\u591a\u6837\u5316\u7684\u63cf\u8ff0\u6587\u672c\uff0c\u589e\u5f3a\u5408\u6210\u5bf9\u7684\u8bed\u4e49\u591a\u6837\u6027\u548c\u7ec6\u7c92\u5ea6\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf-\u6587\u672c\u5bf9\u603b\u6570\u4e0d\u53d8\u3002", "result": "\u5728MS COCO\u9a8c\u8bc1\u96c6\u4e0a\uff0c\u4ec5\u4f7f\u7528100\u4e07\u5bf9Role-SynthCLIP\u6570\u636e\u8bad\u7ec3\u7684CLIP-B/16\u6a21\u578b\u5b9e\u73b0\u4e8664.1%\u7684Recall@1\uff0c\u6bd4\u73b0\u6709\u57fa\u4e8e500\u4e07\u5bf9\u5408\u6210\u6570\u636e\u7684\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa2.8\u4e2a\u767e\u5206\u70b9\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u591a\u89c6\u89d2\u89d2\u8272\u626e\u6f14\u63d0\u793a\u589e\u5f3a\u5408\u6210\u6570\u636e\u7684\u8bed\u4e49\u591a\u6837\u6027\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u6570\u636e\u91cf\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u6570\u636e\u8d28\u91cf\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.05271", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05271", "abs": "https://arxiv.org/abs/2511.05271", "authors": ["Jack Hong", "Chenxiao Zhao", "ChengLin Zhu", "Weiheng Lu", "Guohai Xu", "Xing Yu"], "title": "DeepEyesV2: Toward Agentic Multimodal Model", "comment": "Homepage: https://visual-agent.github.io/", "summary": "Agentic multimodal models should not only comprehend text and images, but\nalso actively invoke external tools, such as code execution environments and\nweb search, and integrate these operations into reasoning. In this work, we\nintroduce DeepEyesV2 and explore how to build an agentic multimodal model from\nthe perspectives of data construction, training methods, and model evaluation.\nWe observe that direct reinforcement learning alone fails to induce robust\ntool-use behavior. This phenomenon motivates a two-stage training pipeline: a\ncold-start stage to establish tool-use patterns, and reinforcement learning\nstage to further refine tool invocation. We curate a diverse, moderately\nchallenging training dataset, specifically including examples where tool use is\nbeneficial. We further introduce RealX-Bench, a comprehensive benchmark\ndesigned to evaluate real-world multimodal reasoning, which inherently requires\nthe integration of multiple capabilities, including perception, search, and\nreasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative\nbenchmarks, demonstrating its effectiveness across real-world understanding,\nmathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2\nexhibits task-adaptive tool invocation, tending to use image operations for\nperception tasks and numerical computations for reasoning tasks. Reinforcement\nlearning further enables complex tool combinations and allows model to\nselectively invoke tools based on context. We hope our study can provide\nguidance for community in developing agentic multimodal models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DeepEyesV2\uff0c\u4e00\u79cd\u4ee3\u7406\u5f0f\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff08\u51b7\u542f\u52a8\u9636\u6bb5\u548c\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff09\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u5de5\u5177\u8c03\u7528\u884c\u4e3a\uff0c\u5e76\u5728RealX-Bench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u7406\u89e3\u3001\u6570\u5b66\u63a8\u7406\u548c\u641c\u7d22\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4ee3\u7406\u5f0f\u591a\u6a21\u6001\u6a21\u578b\u4e0d\u4ec5\u9700\u8981\u7406\u89e3\u6587\u672c\u548c\u56fe\u50cf\uff0c\u8fd8\u9700\u8981\u4e3b\u52a8\u8c03\u7528\u5916\u90e8\u5de5\u5177\uff08\u5982\u4ee3\u7801\u6267\u884c\u73af\u5883\u548c\u7f51\u7edc\u641c\u7d22\uff09\u5e76\u5c06\u8fd9\u4e9b\u64cd\u4f5c\u6574\u5408\u5230\u63a8\u7406\u8fc7\u7a0b\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u4ec5\u4f7f\u7528\u76f4\u63a5\u5f3a\u5316\u5b66\u4e60\u65e0\u6cd5\u8bf1\u5bfc\u51fa\u7a33\u5065\u7684\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u5982\u4f55\u6784\u5efa\u6709\u6548\u7684\u4ee3\u7406\u5f0f\u591a\u6a21\u6001\u6a21\u578b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u51b7\u542f\u52a8\u9636\u6bb5\u5efa\u7acb\u5de5\u5177\u4f7f\u7528\u6a21\u5f0f\uff0c\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u8fdb\u4e00\u6b65\u4f18\u5316\u5de5\u5177\u8c03\u7528\u3002\u6784\u5efa\u4e86\u591a\u6837\u5316\u3001\u9002\u5ea6\u6311\u6218\u6027\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u7279\u522b\u5305\u542b\u5de5\u5177\u4f7f\u7528\u6709\u76ca\u7684\u573a\u666f\u3002\u5f15\u5165\u4e86RealX-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u9700\u8981\u6574\u5408\u611f\u77e5\u3001\u641c\u7d22\u548c\u63a8\u7406\u80fd\u529b\u7684\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u63a8\u7406\u3002", "result": "DeepEyesV2\u5728RealX-Bench\u548c\u5176\u4ed6\u4ee3\u8868\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7406\u89e3\u3001\u6570\u5b66\u63a8\u7406\u548c\u641c\u7d22\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u5747\u663e\u793a\u6709\u6548\u6027\u3002\u6a21\u578b\u5c55\u73b0\u51fa\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u5de5\u5177\u8c03\u7528\u884c\u4e3a\uff0c\u503e\u5411\u4e8e\u5728\u611f\u77e5\u4efb\u52a1\u4e2d\u4f7f\u7528\u56fe\u50cf\u64cd\u4f5c\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u4f7f\u7528\u6570\u503c\u8ba1\u7b97\u3002\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u5b9e\u73b0\u4e86\u590d\u6742\u7684\u5de5\u5177\u7ec4\u5408\u548c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u9009\u62e9\u6027\u5de5\u5177\u8c03\u7528\u3002", "conclusion": "\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u662f\u6784\u5efa\u4ee3\u7406\u5f0f\u591a\u6a21\u6001\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u4fc3\u8fdb\u590d\u6742\u5de5\u5177\u7ec4\u5408\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5de5\u5177\u9009\u62e9\u3002\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u4ee3\u7406\u5f0f\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u5c55\u793a\u4e86\u5de5\u5177\u8c03\u7528\u884c\u4e3a\u53ef\u4ee5\u901a\u8fc7\u9002\u5f53\u7684\u8bad\u7ec3\u7b56\u7565\u5f97\u5230\u6709\u6548\u8bf1\u5bfc\u548c\u4f18\u5316\u3002"}}
{"id": "2511.05092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05092", "abs": "https://arxiv.org/abs/2511.05092", "authors": ["Ruolin Li", "Min Liu", "Yuan Bian", "Zhaoyang Li", "Yuzhen Li", "Xueping Wang", "Yaonan Wang"], "title": "A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification", "comment": "10 pages, 6 figures", "summary": "With growing concerns over data privacy, researchers have started using\nvirtual data as an alternative to sensitive real-world images for training\nperson re-identification (Re-ID) models. However, existing virtual datasets\nproduced by game engines still face challenges such as complex construction and\npoor domain generalization, making them difficult to apply in real scenarios.\nTo address these challenges, we propose a Dual-stage Prompt-driven\nPrivacy-preserving Paradigm (DPPP). In the first stage, we generate rich\nprompts incorporating multi-dimensional attributes such as pedestrian\nappearance, illumination, and viewpoint that drive the diffusion model to\nsynthesize diverse data end-to-end, building a large-scale virtual dataset\nnamed GenePerson with 130,519 images of 6,641 identities. In the second stage,\nwe propose a Prompt-driven Disentanglement Mechanism (PDM) to learn\ndomain-invariant generalization features. With the aid of contrastive learning,\nwe employ two textual inversion networks to map images into pseudo-words\nrepresenting style and content, respectively, thereby constructing\nstyle-disentangled content prompts to guide the model in learning\ndomain-invariant content features at the image level. Experiments demonstrate\nthat models trained on GenePerson with PDM achieve state-of-the-art\ngeneralization performance, surpassing those on popular real and virtual Re-ID\ndatasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u9636\u6bb5\u63d0\u793a\u9a71\u52a8\u7684\u9690\u79c1\u4fdd\u62a4\u8303\u5f0f(DPPP)\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u865a\u62df\u6570\u636e\u5e76\u5229\u7528\u63d0\u793a\u9a71\u52a8\u89e3\u8026\u673a\u5236\u5b66\u4e60\u9886\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u5728\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6e38\u620f\u5f15\u64ce\u751f\u6210\u7684\u865a\u62df\u6570\u636e\u96c6\u9762\u4e34\u6784\u5efa\u590d\u6742\u548c\u9886\u57df\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6709\u6548\u5e94\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u80cc\u666f\u4e0b\u4f7f\u7528\u865a\u62df\u6570\u636e\u8bad\u7ec3\u884c\u4eba\u91cd\u8bc6\u522b\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u53cc\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5305\u542b\u884c\u4eba\u5916\u89c2\u3001\u5149\u7167\u548c\u89c6\u89d2\u7b49\u591a\u7ef4\u5c5e\u6027\u7684\u4e30\u5bcc\u63d0\u793a\u9a71\u52a8\u6269\u6563\u6a21\u578b\u7aef\u5230\u7aef\u5408\u6210\u591a\u6837\u5316\u6570\u636e\uff0c\u6784\u5efa\u5305\u542b6,641\u4e2a\u8eab\u4efd\u7684130,519\u5f20\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u865a\u62df\u6570\u636e\u96c6GenePerson\uff1b\u7b2c\u4e8c\u9636\u6bb5\u63d0\u51fa\u63d0\u793a\u9a71\u52a8\u89e3\u8026\u673a\u5236\uff0c\u501f\u52a9\u5bf9\u6bd4\u5b66\u4e60\u4f7f\u7528\u4e24\u4e2a\u6587\u672c\u53cd\u8f6c\u7f51\u7edc\u5c06\u56fe\u50cf\u6620\u5c04\u4e3a\u4ee3\u8868\u98ce\u683c\u548c\u5185\u5bb9\u7684\u4f2a\u8bcd\uff0c\u6784\u5efa\u98ce\u683c\u89e3\u8026\u7684\u5185\u5bb9\u63d0\u793a\u6765\u6307\u5bfc\u6a21\u578b\u5728\u56fe\u50cf\u5c42\u9762\u5b66\u4e60\u9886\u57df\u4e0d\u53d8\u7684\u5185\u5bb9\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728GenePerson\u6570\u636e\u96c6\u4e0a\u4f7f\u7528PDM\u8bad\u7ec3\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5728\u6d41\u884c\u771f\u5b9e\u548c\u865a\u62df\u884c\u4eba\u91cd\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u9a71\u52a8\u8303\u5f0f\u53ef\u4ee5\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u865a\u62df\u6570\u636e\u5e76\u5b66\u4e60\u9886\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u4e0b\u884c\u4eba\u91cd\u8bc6\u522b\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u8bad\u7ec3\u6570\u636e\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.05299", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05299", "abs": "https://arxiv.org/abs/2511.05299", "authors": ["Zhenyu Yang", "Kairui Zhang", "Yuhang Hu", "Bing Wang", "Shengsheng Qian", "Bin Wen", "Fan Yang", "Tingting Gao", "Weiming Dong", "Changsheng Xu"], "title": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding", "comment": "NeurIPS 2025 Accepted", "summary": "Despite significant progress in Video Large Language Models (Video-LLMs) for\noffline video understanding, existing online Video-LLMs typically struggle to\nsimultaneously process continuous frame-by-frame inputs and determine optimal\nresponse timing, often compromising real-time responsiveness and narrative\ncoherence. To address these limitations, we introduce LiveStar, a pioneering\nlive streaming assistant that achieves always-on proactive responses through\nadaptive streaming decoding. Specifically, LiveStar incorporates: (1) a\ntraining strategy enabling incremental video-language alignment for\nvariable-length video streams, preserving temporal consistency across\ndynamically evolving frame sequences; (2) a response-silence decoding framework\nthat determines optimal proactive response timing via a single forward pass\nverification; (3) memory-aware acceleration via peak-end memory compression for\nonline inference on 10+ minute videos, combined with streaming key-value cache\nto achieve 1.53x faster inference. We also construct an OmniStar dataset, a\ncomprehensive dataset for training and benchmarking that encompasses 15 diverse\nreal-world scenarios and 5 evaluation tasks for online video understanding.\nExtensive experiments across three benchmarks demonstrate LiveStar's\nstate-of-the-art performance, achieving an average 19.5% improvement in\nsemantic correctness with 18.1% reduced timing difference compared to existing\nonline Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.\nOur model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.", "AI": {"tldr": "LiveStar\u662f\u4e00\u4e2a\u5f00\u521b\u6027\u7684\u76f4\u64ad\u6d41\u52a9\u624b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6d41\u5f0f\u89e3\u7801\u5b9e\u73b0\u6301\u7eed\u4e3b\u52a8\u54cd\u5e94\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5728\u7ebf\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u65f6\u54cd\u5e94\u6027\u548c\u53d9\u4e8b\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u96be\u4ee5\u540c\u65f6\u5904\u7406\u8fde\u7eed\u5e27\u8f93\u5165\u5e76\u786e\u5b9a\u6700\u4f73\u54cd\u5e94\u65f6\u673a\uff0c\u5f80\u5f80\u5728\u5b9e\u65f6\u54cd\u5e94\u6027\u548c\u53d9\u4e8b\u8fde\u8d2f\u6027\u4e4b\u95f4\u505a\u51fa\u59a5\u534f\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u76f4\u64ad\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "LiveStar\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a\u652f\u6301\u53ef\u53d8\u957f\u5ea6\u89c6\u9891\u6d41\u589e\u91cf\u89c6\u9891-\u8bed\u8a00\u5bf9\u9f50\u7684\u8bad\u7ec3\u7b56\u7565\u3001\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u9a8c\u8bc1\u786e\u5b9a\u4e3b\u52a8\u54cd\u5e94\u65f6\u673a\u7684\u54cd\u5e94-\u9759\u9ed8\u89e3\u7801\u6846\u67b6\uff0c\u4ee5\u53ca\u7ed3\u5408\u5cf0\u503c-\u672b\u7aef\u5185\u5b58\u538b\u7f29\u548c\u6d41\u5f0f\u952e\u503c\u7f13\u5b58\u7684\u8bb0\u5fc6\u611f\u77e5\u52a0\u901f\u673a\u5236\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLiveStar\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u8bed\u4e49\u6b63\u786e\u6027\u4e0a\u5e73\u5747\u63d0\u534719.5%\uff0c\u65f6\u5e8f\u5dee\u5f02\u51cf\u5c1118.1%\uff0c\u540c\u65f6\u5728\u6240\u6709\u4e94\u4e2aOmniStar\u4efb\u52a1\u4e2dFPS\u63d0\u534712.0%\uff0c\u63a8\u7406\u901f\u5ea6\u52a0\u5feb1.53\u500d\u3002", "conclusion": "LiveStar\u901a\u8fc7\u521b\u65b0\u7684\u6d41\u5f0f\u89e3\u7801\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u7684\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u76f4\u64ad\u573a\u666f\u4e0b\u7684\u667a\u80fd\u52a9\u624b\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u6784\u5efa\u7684OmniStar\u6570\u636e\u96c6\u4e3a\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u9886\u57df\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u5efa\u7acb\u4e86\u5168\u9762\u57fa\u51c6\u3002"}}
{"id": "2511.05106", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05106", "abs": "https://arxiv.org/abs/2511.05106", "authors": ["Yasemin Turkan", "F. Boray Tek", "M. Serdar Nazl\u0131", "\u00d6yk\u00fc Eren"], "title": "Early Alzheimer's Disease Detection from Retinal OCT Images: A UK Biobank Study", "comment": null, "summary": "Alterations in retinal layer thickness, measurable using Optical Coherence\nTomography (OCT), have been associated with neurodegenerative diseases such as\nAlzheimer's disease (AD). While previous studies have mainly focused on\nsegmented layer thickness measurements, this study explored the direct\nclassification of OCT B-scan images for the early detection of AD. To our\nknowledge, this is the first application of deep learning to raw OCT B-scans\nfor AD prediction in the literature. Unlike conventional medical image\nclassification tasks, early detection is more challenging than diagnosis\nbecause imaging precedes clinical diagnosis by several years. We fine-tuned and\nevaluated multiple pretrained models, including ImageNet-based networks and the\nOCT-specific RETFound transformer, using subject-level cross-validation\ndatasets matched for age, sex, and imaging instances from the UK Biobank\ncohort. To reduce overfitting in this small, high-dimensional dataset, both\nstandard and OCT-specific augmentation techniques were applied, along with a\nyear-weighted loss function that prioritized cases diagnosed within four years\nof imaging. ResNet-34 produced the most stable results, achieving an AUC of\n0.62 in the 4-year cohort. Although below the threshold for clinical\napplication, our explainability analyses confirmed localized structural\ndifferences in the central macular subfield between the AD and control groups.\nThese findings provide a baseline for OCT-based AD prediction, highlight the\nchallenges of detecting subtle retinal biomarkers years before AD diagnosis,\nand point to the need for larger datasets and multimodal approaches.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5c06\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u4e8e\u539f\u59cbOCT B\u626b\u63cf\u56fe\u50cf\u8fdb\u884c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u9884\u6d4b\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u7279\u5b9a\u589e\u5f3a\u6280\u672f\uff0c\u5728UK Biobank\u961f\u5217\u4e2d\u5b9e\u73b0\u4e860.62\u7684AUC\uff0c\u4e3a\u57fa\u4e8e\u89c6\u7f51\u819c\u6210\u50cf\u7684\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u68c0\u6d4b\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5206\u5272\u540e\u7684\u89c6\u7f51\u819c\u5c42\u539a\u5ea6\u6d4b\u91cf\uff0c\u800c\u672c\u7814\u7a76\u63a2\u7d22\u76f4\u63a5\u5bf9\u539f\u59cbOCT B\u626b\u63cf\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u4ee5\u5b9e\u73b0\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\uff0c\u586b\u8865\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u539f\u59cbOCT\u56fe\u50cf\u4e0a\u5e94\u7528\u4e8eAD\u9884\u6d4b\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u5fae\u8c03\u4e86\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5305\u62ec\u57fa\u4e8eImageNet\u7684\u7f51\u7edc\u548cOCT\u4e13\u7528RETFound transformer\uff0c\u91c7\u7528\u5e74\u9f84\u3001\u6027\u522b\u548c\u6210\u50cf\u5b9e\u4f8b\u5339\u914d\u7684\u53d7\u8bd5\u8005\u7ea7\u522b\u4ea4\u53c9\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u5e94\u7528\u6807\u51c6\u548cOCT\u7279\u5b9a\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u4f7f\u7528\u5e74\u52a0\u6743\u635f\u5931\u51fd\u6570\u4f18\u5148\u5904\u7406\u6210\u50cf\u540e\u56db\u5e74\u5185\u8bca\u65ad\u7684\u75c5\u4f8b\u3002", "result": "ResNet-34\u6a21\u578b\u57284\u5e74\u961f\u5217\u4e2d\u4ea7\u751f\u6700\u7a33\u5b9a\u7ed3\u679c\uff0cAUC\u8fbe\u52300.62\uff0c\u867d\u7136\u4f4e\u4e8e\u4e34\u5e8a\u5e94\u7528\u9608\u503c\uff0c\u4f46\u53ef\u89e3\u91ca\u6027\u5206\u6790\u786e\u8ba4\u4e86AD\u7ec4\u4e0e\u5bf9\u7167\u7ec4\u5728\u4e2d\u592e\u9ec4\u6591\u4e9a\u533a\u5b58\u5728\u5c40\u90e8\u7ed3\u6784\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u4e3a\u57fa\u4e8eOCT\u7684AD\u9884\u6d4b\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u7a81\u663e\u4e86\u5728AD\u8bca\u65ad\u524d\u6570\u5e74\u68c0\u6d4b\u7ec6\u5fae\u89c6\u7f51\u819c\u751f\u7269\u6807\u5fd7\u7269\u7684\u6311\u6218\uff0c\u5e76\u6307\u51fa\u9700\u8981\u66f4\u5927\u6570\u636e\u96c6\u548c\u591a\u6a21\u6001\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u65e9\u671f\u68c0\u6d4b\u96be\u9898\u3002"}}
{"id": "2511.05404", "categories": ["cs.CV", "cs.AI", "I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2511.05404", "abs": "https://arxiv.org/abs/2511.05404", "authors": ["Laura Alejandra Encinar Gonzalez", "John Folkesson", "Rudolph Triebel", "Riccardo Giubilato"], "title": "Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments", "comment": "Under review for ICRA 2026", "summary": "Robust loop closure detection is a critical component of Simultaneous\nLocalization and Mapping (SLAM) algorithms in GNSS-denied environments, such as\nin the context of planetary exploration. In these settings, visual place\nrecognition often fails due to aliasing and weak textures, while LiDAR-based\nmethods suffer from sparsity and ambiguity. This paper presents MPRF, a\nmultimodal pipeline that leverages transformer-based foundation models for both\nvision and LiDAR modalities to achieve robust loop closure in severely\nunstructured environments. Unlike prior work limited to retrieval, MPRF\nintegrates a two-stage visual retrieval strategy with explicit 6-DoF pose\nestimation, combining DINOv2 features with SALAD aggregation for efficient\ncandidate screening and SONATA-based LiDAR descriptors for geometric\nverification. Experiments on the S3LI dataset and S3LI Vulcano dataset show\nthat MPRF outperforms state-of-the-art retrieval methods in precision while\nenhancing pose estimation robustness in low-texture regions. By providing\ninterpretable correspondences suitable for SLAM back-ends, MPRF achieves a\nfavorable trade-off between accuracy, efficiency, and reliability,\ndemonstrating the potential of foundation models to unify place recognition and\npose estimation. Code and models will be released at github.com/DLR-RM/MPRF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMPRF\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u57fa\u7840\u6a21\u578b\u7684\u591a\u6a21\u6001\u95ed\u73af\u68c0\u6d4b\u7ba1\u9053\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u68c0\u7d22\u548c\u663e\u5f0f6-DoF\u59ff\u6001\u4f30\u8ba1\uff0c\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u95ed\u73af\u68c0\u6d4b\u3002", "motivation": "\u5728GNSS\u62d2\u6b62\u73af\u5883\uff08\u5982\u884c\u661f\u63a2\u6d4b\uff09\u4e2d\uff0c\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u56e0\u6df7\u53e0\u548c\u5f31\u7eb9\u7406\u800c\u5931\u6548\uff0c\u800cLiDAR\u65b9\u6cd5\u5219\u53d7\u9650\u4e8e\u7a00\u758f\u6027\u548c\u6a21\u7cca\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5c40\u9650\u4e8e\u68c0\u7d22\u800c\u7f3a\u4e4f\u7cbe\u786e\u7684\u59ff\u6001\u4f30\u8ba1\u80fd\u529b\u3002", "method": "MPRF\u91c7\u7528\u4e24\u9636\u6bb5\u89c6\u89c9\u68c0\u7d22\u7b56\u7565\uff0c\u7ed3\u5408DINOv2\u7279\u5f81\u4e0eSALAD\u805a\u5408\u8fdb\u884c\u5019\u9009\u7b5b\u9009\uff0c\u5e76\u4f7f\u7528SONATA-based LiDAR\u63cf\u8ff0\u7b26\u8fdb\u884c\u51e0\u4f55\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4ece\u68c0\u7d22\u5230\u663e\u5f0f6-DoF\u59ff\u6001\u4f30\u8ba1\u7684\u5b8c\u6574\u6d41\u7a0b\u3002", "result": "\u5728S3LI\u6570\u636e\u96c6\u548cS3LI Vulcano\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMPRF\u5728\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u5e76\u5728\u4f4e\u7eb9\u7406\u533a\u57df\u663e\u8457\u63d0\u5347\u4e86\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MPRF\u901a\u8fc7\u63d0\u4f9b\u9002\u7528\u4e8eSLAM\u540e\u7aef\u7684\u53ef\u89e3\u91ca\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u53ef\u9760\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u5e73\u8861\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u7edf\u4e00\u4f4d\u7f6e\u8bc6\u522b\u548c\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.05293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05293", "abs": "https://arxiv.org/abs/2511.05293", "authors": ["Rui Yan", "Yibo Li", "Han Ding", "Fei Wang"], "title": "Cross-domain EEG-based Emotion Recognition with Contrastive Learning", "comment": "5 pages", "summary": "Electroencephalogram (EEG)-based emotion recognition is vital for affective\ncomputing but faces challenges in feature utilization and cross-domain\ngeneralization. This work introduces EmotionCLIP, which reformulates\nrecognition as an EEG-text matching task within the CLIP framework. A tailored\nbackbone, SST-LegoViT, captures spatial, spectral, and temporal features using\nmulti-scale convolution and Transformer modules. Experiments on SEED and\nSEED-IV datasets show superior cross-subject accuracies of 88.69% and 73.50%,\nand cross-time accuracies of 88.46% and 77.54%, outperforming existing models.\nResults demonstrate the effectiveness of multimodal contrastive learning for\nrobust EEG emotion recognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEmotionCLIP\uff0c\u5c06\u57fa\u4e8e\u8111\u7535\u56fe\u7684\u60c5\u7eea\u8bc6\u522b\u91cd\u65b0\u5b9a\u4e49\u4e3aCLIP\u6846\u67b6\u5185\u7684\u8111\u7535\u56fe-\u6587\u672c\u5339\u914d\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u4e86\u8de8\u88ab\u8bd5\u548c\u8de8\u65f6\u95f4\u7684\u7a33\u5065\u60c5\u7eea\u8bc6\u522b\u3002", "motivation": "\u57fa\u4e8e\u8111\u7535\u56fe\u7684\u60c5\u7eea\u8bc6\u522b\u5728\u7279\u5f81\u5229\u7528\u548c\u8de8\u57df\u6cdb\u5316\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u5e76\u5b9e\u73b0\u7a33\u5065\u7684\u8de8\u57df\u6027\u80fd\u3002", "method": "\u63d0\u51faEmotionCLIP\u6846\u67b6\uff0c\u5c06\u60c5\u7eea\u8bc6\u522b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8111\u7535\u56fe-\u6587\u672c\u5339\u914d\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86SST-LegoViT\u9aa8\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5377\u79ef\u548cTransformer\u6a21\u5757\u6355\u83b7\u7a7a\u95f4\u3001\u9891\u8c31\u548c\u65f6\u95f4\u7279\u5f81\u3002", "result": "\u5728SEED\u548cSEED-IV\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8de8\u88ab\u8bd5\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523088.69%\u548c73.50%\uff0c\u8de8\u65f6\u95f4\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523088.46%\u548c77.54%\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u5728\u8111\u7535\u56fe\u60c5\u7eea\u8bc6\u522b\u4e2d\u5177\u6709\u663e\u8457\u6709\u6548\u6027\uff0c\u4e3a\u7a33\u5065\u7684\u8de8\u57df\u60c5\u611f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u8111\u7535\u56fe-\u6587\u672c\u5bf9\u9f50\u7b56\u7565\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2511.05489", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05489", "abs": "https://arxiv.org/abs/2511.05489", "authors": ["Junwen Pan", "Qizhe Zhang", "Rui Zhang", "Ming Lu", "Xin Wan", "Yuan Zhang", "Chang Liu", "Qi She"], "title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning", "comment": "22 pages, 17 figures. Official code:\n  https://github.com/Time-Search/TimeSearch-R", "summary": "Temporal search aims to identify a minimal set of relevant frames from tens\nof thousands based on a given query, serving as a foundation for accurate\nlong-form video understanding. Existing works attempt to progressively narrow\nthe search space. However, these approaches typically rely on a hand-crafted\nsearch process, lacking end-to-end optimization for learning optimal search\nstrategies. In this paper, we propose TimeSearch-R, which reformulates temporal\nsearch as interleaved text-video thinking, seamlessly integrating searching\nvideo clips into the reasoning process through reinforcement learning (RL).\nHowever, applying RL training methods, such as Group Relative Policy\nOptimization (GRPO), to video reasoning can result in unsupervised intermediate\nsearch decisions. This leads to insufficient exploration of the video content\nand inconsistent logical reasoning. To address these issues, we introduce GRPO\nwith Completeness Self-Verification (GRPO-CSV), which gathers searched video\nframes from the interleaved reasoning process and utilizes the same policy\nmodel to verify the adequacy of searched frames, thereby improving the\ncompleteness of video reasoning. Additionally, we construct datasets\nspecifically designed for the SFT cold-start and RL training of GRPO-CSV,\nfiltering out samples with weak temporal dependencies to enhance task\ndifficulty and improve temporal search capabilities. Extensive experiments\ndemonstrate that TimeSearch-R achieves significant improvements on temporal\nsearch benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as\nlong-form video understanding benchmarks like VideoMME and MLVU. Notably,\nTimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%\nimprovement over the base model Qwen2.5-VL and 2.0% over the advanced video\nreasoning model Video-R1. Our code is available at\nhttps://github.com/Time-Search/TimeSearch-R.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTimeSearch-R\uff0c\u5c06\u65f6\u5e8f\u641c\u7d22\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4ea4\u9519\u5f0f\u6587\u672c-\u89c6\u9891\u601d\u8003\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u89c6\u9891\u7247\u6bb5\u641c\u7d22\u65e0\u7f1d\u96c6\u6210\u5230\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u5e76\u5728\u591a\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u5e8f\u641c\u7d22\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u624b\u5de5\u8bbe\u8ba1\u7684\u641c\u7d22\u8fc7\u7a0b\uff0c\u7f3a\u4e4f\u7aef\u5230\u7aef\u4f18\u5316\u6765\u5b66\u4e60\u6700\u4f18\u641c\u7d22\u7b56\u7565\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u63a2\u7d22\u89c6\u9891\u5185\u5bb9\u5e76\u4fdd\u6301\u903b\u8f91\u63a8\u7406\u7684\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faTimeSearch-R\u6846\u67b6\uff0c\u5c06\u65f6\u5e8f\u641c\u7d22\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4ea4\u9519\u5f0f\u6587\u672c-\u89c6\u9891\u601d\u8003\u8fc7\u7a0b\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u5e26\u6709\u5b8c\u6574\u6027\u81ea\u9a8c\u8bc1\u7684GRPO-CSV\u65b9\u6cd5\uff0c\u5229\u7528\u540c\u4e00\u7b56\u7565\u6a21\u578b\u9a8c\u8bc1\u641c\u7d22\u5e27\u7684\u5145\u5206\u6027\u4ee5\u63d0\u9ad8\u89c6\u9891\u63a8\u7406\u7684\u5b8c\u6574\u6027\u3002", "result": "\u5728Haystack-LVBench\u3001Haystack-Ego4D\u7b49\u65f6\u5e8f\u641c\u7d22\u57fa\u51c6\u4ee5\u53caVideoMME\u3001MLVU\u7b49\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5728LongVideoBench\u4e0a\u76f8\u6bd4\u57fa\u7840\u6a21\u578bQwen2.5-VL\u63d0\u53474.1%\uff0c\u76f8\u6bd4\u5148\u8fdb\u89c6\u9891\u63a8\u7406\u6a21\u578bVideo-R1\u63d0\u53472.0%\uff0c\u521b\u4e0b\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c06\u65f6\u5e8f\u641c\u7d22\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4ea4\u9519\u5f0f\u63a8\u7406\u8fc7\u7a0b\u7684\u6709\u6548\u6027\uff0c\u5f3a\u5316\u5b66\u4e60\u4e0e\u5b8c\u6574\u6027\u81ea\u9a8c\u8bc1\u7684\u7ed3\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u89c6\u9891\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.05393", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05393", "abs": "https://arxiv.org/abs/2511.05393", "authors": ["Zehui Feng", "Tian Qiu", "Tong Wu", "Junxuan Li", "Huayuan Xu", "Ting Han"], "title": "PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization", "comment": "27 pages, 14 figures, under review as a conference paper", "summary": "Visual Quality Assessment (QA) seeks to predict human perceptual judgments of\nvisual fidelity. While recent multimodal large language models (MLLMs) show\npromise in reasoning about image and video quality, existing approaches mainly\nrely on supervised fine-tuning or rank-only objectives, resulting in shallow\nreasoning, poor score calibration, and limited cross-domain generalization. We\npropose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning\nframework that unifies absolute score regression and relative ranking\nconsistency within a single reasoning-driven optimization scheme. Unlike prior\nQA methods, PreResQ-R1 introduces a dual-branch reward formulation that\nseparately models intra-sample response coherence and inter-sample preference\nalignment, optimized via Group Relative Policy Optimization (GRPO). This design\nencourages fine-grained, stable, and interpretable chain-of-thought reasoning\nabout perceptual quality. To extend beyond static imagery, we further design a\nglobal-temporal and local-spatial data flow strategy for Video Quality\nAssessment. Remarkably, with reinforcement fine-tuning on only 6K images and\n28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5\nVQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30%\nand textbf2.15% in IQA task, respectively. Beyond quantitative gains, it\nproduces human-aligned reasoning traces that reveal the perceptual cues\nunderlying quality judgments. Code and model are available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPreResQ-R1\uff0c\u4e00\u79cd\u504f\u597d-\u54cd\u5e94\u89e3\u8026\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7edd\u5bf9\u5206\u6570\u56de\u5f52\u548c\u76f8\u5bf9\u6392\u5e8f\u4e00\u81f4\u6027\u6765\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u75286K\u56fe\u50cf\u548c28K\u89c6\u9891\u8fdb\u884c\u5f3a\u5316\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u591a\u4e2aIQA\u548cVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\u6216\u4ec5\u6392\u5e8f\u76ee\u6807\uff0c\u5bfc\u81f4\u63a8\u7406\u6d45\u5c42\u3001\u5206\u6570\u6821\u51c6\u5dee\u4ee5\u53ca\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u8fd9\u4e9b\u5c40\u9650\u6027\u963b\u788d\u4e86\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u3001\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684\u611f\u77e5\u8d28\u91cf\u63a8\u7406\u3002", "method": "PreResQ-R1\u91c7\u7528\u504f\u597d-\u54cd\u5e94\u89e3\u8026\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5f15\u5165\u53cc\u5206\u652f\u5956\u52b1\u516c\u5f0f\u5206\u522b\u5efa\u6a21\u6837\u672c\u5185\u54cd\u5e94\u4e00\u81f4\u6027\u548c\u6837\u672c\u95f4\u504f\u597d\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u8fdb\u884c\u4f18\u5316\u3002\u5bf9\u4e8e\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff0c\u8bbe\u8ba1\u4e86\u5168\u5c40-\u65f6\u95f4\u548c\u5c40\u90e8-\u7a7a\u95f4\u6570\u636e\u6d41\u7b56\u7565\u6765\u6269\u5c55\u9759\u6001\u56fe\u50cf\u5904\u7406\u80fd\u529b\u3002", "result": "\u5728\u4ec5\u4f7f\u75286K\u56fe\u50cf\u548c28K\u89c6\u9891\u8fdb\u884c\u5f3a\u5316\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0cPreResQ-R1\u572810\u4e2aIQA\u548c5\u4e2aVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5728IQA\u4efb\u52a1\u4e2dSRCC\u548cPLCC\u6307\u6807\u5206\u522b\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd55.30%\u548c2.15%\u3002\u6a21\u578b\u8fd8\u4ea7\u751f\u4e86\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u63ed\u793a\u4e86\u8d28\u91cf\u5224\u65ad\u80cc\u540e\u7684\u611f\u77e5\u7ebf\u7d22\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u7edf\u4e00\u7edd\u5bf9\u548c\u76f8\u5bf9\u8d28\u91cf\u8bc4\u4f30\u76ee\u6807\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u3001\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684\u94fe\u5f0f\u63a8\u7406\u3002\u8d85\u8d8a\u5b9a\u91cf\u6027\u80fd\u63d0\u5347\uff0c\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u4e3a\u7406\u89e3\u4eba\u7c7b\u611f\u77e5\u8d28\u91cf\u5224\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u6d1e\u5bdf\uff0c\u63a8\u52a8\u4e86\u53ef\u89e3\u91ca\u8d28\u91cf\u8bc4\u4f30\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.05474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05474", "abs": "https://arxiv.org/abs/2511.05474", "authors": ["Xian-Hong Huang", "Hui-Kai Su", "Chi-Chia Sun", "Jun-Wei Hsieh"], "title": "Semantic-Guided Natural Language and Visual Fusion for Cross-Modal Interaction Based on Tiny Object Detection", "comment": null, "summary": "This paper introduces a cutting-edge approach to cross-modal interaction for\ntiny object detection by combining semantic-guided natural language processing\nwith advanced visual recognition backbones. The proposed method integrates the\nBERT language model with the CNN-based Parallel Residual Bi-Fusion Feature\nPyramid Network (PRB-FPN-Net), incorporating innovative backbone architectures\nsuch as ELAN, MSP, and CSP to optimize feature extraction and fusion. By\nemploying lemmatization and fine-tuning techniques, the system aligns semantic\ncues from textual inputs with visual features, enhancing detection precision\nfor small and complex objects. Experimental validation using the COCO and\nObjects365 datasets demonstrates that the model achieves superior performance.\nOn the COCO2017 validation set, it attains a 52.6% average precision (AP),\noutperforming YOLO-World significantly while maintaining half the parameter\nconsumption of Transformer-based models like GLIP. Several test on different of\nbackbones such ELAN, MSP, and CSP further enable efficient handling of\nmulti-scale objects, ensuring scalability and robustness in\nresource-constrained environments. This study underscores the potential of\nintegrating natural language understanding with advanced backbone\narchitectures, setting new benchmarks in object detection accuracy, efficiency,\nand adaptability to real-world challenges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u5f15\u5bfc\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e0e\u5148\u8fdb\u89c6\u89c9\u8bc6\u522b\u9aa8\u5e72\u7f51\u7edc\u7684\u8de8\u6a21\u6001\u5fae\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7BERT\u8bed\u8a00\u6a21\u578b\u4e0eCNN-based PRB-FPN-Net\u7684\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u548c\u590d\u6742\u76ee\u6807\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u5fae\u5c0f\u548c\u590d\u6742\u76ee\u6807\u65f6\u7684\u7cbe\u5ea6\u4e0d\u8db3\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5982\u4f55\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u4ee5\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8be5\u65b9\u6cd5\u6574\u5408\u4e86BERT\u8bed\u8a00\u6a21\u578b\u4e0e\u57fa\u4e8eCNN\u7684\u5e76\u884c\u6b8b\u5dee\u53cc\u878d\u5408\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff08PRB-FPN-Net\uff09\uff0c\u91c7\u7528ELAN\u3001MSP\u548cCSP\u7b49\u521b\u65b0\u9aa8\u5e72\u67b6\u6784\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\uff0c\u901a\u8fc7\u8bcd\u5f62\u8fd8\u539f\u548c\u5fae\u8c03\u6280\u672f\u5c06\u6587\u672c\u8f93\u5165\u7684\u8bed\u4e49\u7ebf\u7d22\u4e0e\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u5728COCO\u548cObjects365\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728COCO2017\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523052.6%\u7684\u5e73\u5747\u7cbe\u5ea6\uff08AP\uff09\uff0c\u663e\u8457\u4f18\u4e8eYOLO-World\uff0c\u540c\u65f6\u4ec5\u6d88\u8017Transformer-based\u6a21\u578b\uff08\u5982GLIP\uff09\u4e00\u534a\u7684\u53c2\u6570\uff0c\u4e0d\u540c\u9aa8\u5e72\u7f51\u7edc\uff08ELAN\u3001MSP\u3001CSP\uff09\u7684\u6d4b\u8bd5\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5176\u5904\u7406\u591a\u5c3a\u5ea6\u76ee\u6807\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5c06\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e0e\u5148\u8fdb\u9aa8\u5e72\u67b6\u6784\u96c6\u6210\u7684\u6f5c\u529b\uff0c\u4e3a\u7269\u4f53\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u5b9e\u9645\u5e94\u7528\u9002\u5e94\u6027\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2511.05491", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05491", "abs": "https://arxiv.org/abs/2511.05491", "authors": ["Rui Yang", "Ziyu Zhu", "Yanwei Li", "Jingjia Huang", "Shen Yan", "Siyuan Zhou", "Zhe Liu", "Xiangtai Li", "Shuangye Li", "Wenqian Wang", "Yi Lin", "Hengshuang Zhao"], "title": "Visual Spatial Tuning", "comment": null, "summary": "Capturing spatial relationships from visual inputs is a cornerstone of\nhuman-like general intelligence. Several previous studies have tried to enhance\nthe spatial awareness of Vision-Language Models (VLMs) by adding extra expert\nencoders, which brings extra overhead and usually harms general capabilities.\nTo enhance the spatial ability in general architectures, we introduce Visual\nSpatial Tuning (VST), a comprehensive framework to cultivate VLMs with\nhuman-like visuospatial abilities, from spatial perception to reasoning. We\nfirst attempt to enhance spatial perception in VLMs by constructing a\nlarge-scale dataset termed VST-P, which comprises 4.1 million samples spanning\n19 skills across single views, multiple images, and videos. Then, we present\nVST-R, a curated dataset with 135K samples that instruct models to reason in\nspace. In particular, we adopt a progressive training pipeline: supervised\nfine-tuning to build foundational spatial knowledge, followed by reinforcement\nlearning to further improve spatial reasoning abilities. Without the\nside-effect to general capabilities, the proposed VST consistently achieves\nstate-of-the-art results on several spatial benchmarks, including $34.8\\%$ on\nMMSI-Bench and $61.2\\%$ on VSIBench. It turns out that the\nVision-Language-Action models can be significantly enhanced with the proposed\nspatial tuning paradigm, paving the way for more physically grounded AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u89c6\u89c9\u7a7a\u95f4\u8c03\u4f18\uff08VST\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u7a7a\u95f4\u611f\u77e5\u6570\u636e\u96c6VST-P\u548c\u7a7a\u95f4\u63a8\u7406\u6570\u636e\u96c6VST-R\uff0c\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u5176\u901a\u7528\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u901a\u8fc7\u6dfb\u52a0\u989d\u5916\u4e13\u5bb6\u7f16\u7801\u5668\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u8fd9\u4f1a\u5e26\u6765\u989d\u5916\u5f00\u9500\u5e76\u635f\u5bb3\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u67b6\u6784\u4e0b\u7684\u7a7a\u95f4\u80fd\u529b\u589e\u5f3a\u65b9\u6cd5\uff0c\u4ece\u7a7a\u95f4\u611f\u77e5\u5230\u63a8\u7406\u5168\u9762\u57f9\u517b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u7c7b\u5316\u7a7a\u95f4\u667a\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u89c6\u89c9\u7a7a\u95f4\u8c03\u4f18\uff08VST\uff09\u7efc\u5408\u6846\u67b6\uff0c\u5305\u62ec\u6784\u5efa\u5305\u542b410\u4e07\u6837\u672c\u7684VST-P\u6570\u636e\u96c6\u8986\u76d619\u79cd\u7a7a\u95f4\u6280\u80fd\uff0c\u4ee5\u53ca\u5305\u542b13.5\u4e07\u6837\u672c\u7684VST-R\u63a8\u7406\u6570\u636e\u96c6\u3002\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6d41\u7a0b\uff1a\u9996\u5148\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5efa\u7acb\u57fa\u7840\u7a7a\u95f4\u77e5\u8bc6\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "VST\u6846\u67b6\u5728\u591a\u4e2a\u7a7a\u95f4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5728MMSI-Bench\u4e0a\u8fbe\u523034.8%\uff0c\u5728VSIBench\u4e0a\u8fbe\u523061.2%\u3002\u8be5\u65b9\u6cd5\u5728\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u80fd\u529b\u7684\u540c\u65f6\uff0c\u4e0d\u4f1a\u5bf9\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u7a7a\u95f4\u8c03\u4f18\u8303\u5f0f\u663e\u8457\u589e\u5f3a\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u7269\u7406\u57fa\u7840\u7684\u4eba\u5de5\u667a\u80fd\u94fa\u5e73\u4e86\u9053\u8def\u3002\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u4e0d\u635f\u5bb3\u901a\u7528\u80fd\u529b\u7684\u524d\u63d0\u4e0b\uff0c\u7cfb\u7edf\u6027\u5730\u57f9\u517b\u6a21\u578b\u7a7a\u95f4\u667a\u80fd\u7684\u53ef\u884c\u6027\u3002"}}
