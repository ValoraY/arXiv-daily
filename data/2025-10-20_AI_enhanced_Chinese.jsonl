{"id": "2510.14992", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14992", "abs": "https://arxiv.org/abs/2510.14992", "authors": ["Leela Krishna", "Mengyang Zhao", "Saicharithreddy Pasula", "Harshit Rajgarhia", "Abhishek Mukherji"], "title": "GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments", "comment": null, "summary": "Training robust world models requires large-scale, precisely labeled\nmultimodal datasets, a process historically bottlenecked by slow and expensive\nmanual annotation. We present a production-tested GAZE pipeline that automates\nthe conversion of raw, long-form video into rich, task-ready supervision for\nworld-model training. Our system (i) normalizes proprietary 360-degree formats\ninto standard views and shards them for parallel processing; (ii) applies a\nsuite of AI models (scene understanding, object tracking, audio transcription,\nPII/NSFW/minor detection) for dense, multimodal pre-annotation; and (iii)\nconsolidates signals into a structured output specification for rapid human\nvalidation.\n  The GAZE workflow demonstrably yields efficiency gains (~19 minutes saved per\nreview hour) and reduces human review volume by >80% through conservative\nauto-skipping of low-salience segments. By increasing label density and\nconsistency while integrating privacy safeguards and chain-of-custody metadata,\nour method generates high-fidelity, privacy-aware datasets directly consumable\nfor learning cross-modal dynamics and action-conditioned prediction. We detail\nour orchestration, model choices, and data dictionary to provide a scalable\nblueprint for generating high-quality world model training data without\nsacrificing throughput or governance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aGAZE\u7684\u751f\u4ea7\u7ea7\u6d41\u6c34\u7ebf\uff0c\u80fd\u591f\u81ea\u52a8\u5c06\u539f\u59cb\u957f\u89c6\u9891\u8f6c\u6362\u4e3a\u7528\u4e8e\u4e16\u754c\u6a21\u578b\u8bad\u7ec3\u7684\u4e30\u5bcc\u76d1\u7763\u6570\u636e\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u9884\u6807\u6ce8\u548c\u7ed3\u6784\u5316\u8f93\u51fa\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u6807\u6ce8\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u8bad\u7ec3\u9c81\u68d2\u7684\u4e16\u754c\u6a21\u578b\u9700\u8981\u5927\u89c4\u6a21\u3001\u7cbe\u786e\u6807\u6ce8\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4f46\u4f20\u7edf\u624b\u52a8\u6807\u6ce8\u8fc7\u7a0b\u5b58\u5728\u901f\u5ea6\u6162\u3001\u6210\u672c\u9ad8\u7684\u74f6\u9888\u95ee\u9898\uff0c\u9650\u5236\u4e86\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u83b7\u53d6\u6548\u7387\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528\u4e09\u9636\u6bb5\u5904\u7406\u6d41\u7a0b\uff1a\u9996\u5148\u5c06\u4e13\u6709\u7684360\u5ea6\u89c6\u9891\u683c\u5f0f\u6807\u51c6\u5316\u4e3a\u6807\u51c6\u89c6\u56fe\u5e76\u8fdb\u884c\u5206\u7247\u5e76\u884c\u5904\u7406\uff1b\u7136\u540e\u5e94\u7528\u4e00\u7cfb\u5217AI\u6a21\u578b\u8fdb\u884c\u5bc6\u96c6\u591a\u6a21\u6001\u9884\u6807\u6ce8\uff0c\u5305\u62ec\u573a\u666f\u7406\u89e3\u3001\u76ee\u6807\u8ddf\u8e2a\u3001\u97f3\u9891\u8f6c\u5f55\u4ee5\u53caPII/NSFW/\u672a\u6210\u5e74\u4eba\u68c0\u6d4b\uff1b\u6700\u540e\u5c06\u4fe1\u53f7\u6574\u5408\u4e3a\u7ed3\u6784\u5316\u8f93\u51fa\u89c4\u8303\u4ee5\u4fbf\u5feb\u901f\u4eba\u5de5\u9a8c\u8bc1\u3002", "result": "GAZE\u5de5\u4f5c\u6d41\u7a0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\uff0c\u6bcf\u5ba1\u6838\u5c0f\u65f6\u53ef\u8282\u7701\u7ea619\u5206\u949f\u65f6\u95f4\uff0c\u901a\u8fc7\u4fdd\u5b88\u81ea\u52a8\u8df3\u8fc7\u4f4e\u663e\u8457\u6027\u7247\u6bb5\u5c06\u4eba\u5de5\u5ba1\u6838\u91cf\u51cf\u5c11\u8d85\u8fc780%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6807\u7b7e\u5bc6\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u5e76\u96c6\u6210\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u76d1\u7ba1\u94fe\u5143\u6570\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u4e16\u754c\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u84dd\u56fe\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u541e\u5410\u91cf\u548c\u6cbb\u7406\u8981\u6c42\u7684\u540c\u65f6\uff0c\u76f4\u63a5\u751f\u6210\u9002\u7528\u4e8e\u5b66\u4e60\u8de8\u6a21\u6001\u52a8\u6001\u548c\u52a8\u4f5c\u6761\u4ef6\u9884\u6d4b\u7684\u9ad8\u4fdd\u771f\u3001\u9690\u79c1\u611f\u77e5\u6570\u636e\u96c6\u3002"}}
{"id": "2510.15015", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15015", "abs": "https://arxiv.org/abs/2510.15015", "authors": ["Mor Ventura", "Michael Toker", "Or Patashnik", "Yonatan Belinkov", "Roi Reichart"], "title": "DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models", "comment": null, "summary": "Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable\nto semantic leakage, the unintended transfer of semantically related features\nbetween distinct entities. Existing mitigation strategies are often\noptimization-based or dependent on external inputs. We introduce DeLeaker, a\nlightweight, optimization-free inference-time approach that mitigates leakage\nby directly intervening on the model's attention maps. Throughout the diffusion\nprocess, DeLeaker dynamically reweights attention maps to suppress excessive\ncross-entity interactions while strengthening the identity of each entity. To\nsupport systematic evaluation, we introduce SLIM (Semantic Leakage in IMages),\nthe first dataset dedicated to semantic leakage, comprising 1,130\nhuman-verified samples spanning diverse scenarios, together with a novel\nautomatic evaluation framework. Experiments demonstrate that DeLeaker\nconsistently outperforms all baselines, even when they are provided with\nexternal information, achieving effective leakage mitigation without\ncompromising fidelity or quality. These results underscore the value of\nattention control and pave the way for more semantically precise T2I models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDeLeaker\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u4f18\u5316\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5e72\u9884\u6a21\u578b\u6ce8\u610f\u529b\u56fe\u6765\u7f13\u89e3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u8bed\u4e49\u6cc4\u6f0f\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u6709\u6548\u6291\u5236\u5b9e\u4f53\u95f4\u7684\u610f\u5916\u7279\u5f81\u4f20\u9012\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u867d\u7136\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u8bed\u4e49\u6cc4\u6f0f\u7684\u5f71\u54cd\uff0c\u5373\u4e0d\u540c\u5b9e\u4f53\u4e4b\u95f4\u610f\u5916\u4f20\u9012\u8bed\u4e49\u76f8\u5173\u7279\u5f81\uff0c\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u901a\u5e38\u57fa\u4e8e\u4f18\u5316\u6216\u4f9d\u8d56\u5916\u90e8\u8f93\u5165\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "DeLeaker\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u52a8\u6001\u91cd\u65b0\u52a0\u6743\u6ce8\u610f\u529b\u56fe\uff0c\u6291\u5236\u8fc7\u5ea6\u7684\u8de8\u5b9e\u4f53\u4ea4\u4e92\u540c\u65f6\u589e\u5f3a\u6bcf\u4e2a\u5b9e\u4f53\u7684\u8eab\u4efd\u7279\u5f81\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u4f18\u5316\u4e14\u4e0d\u4f9d\u8d56\u5916\u90e8\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDeLeaker\u5728\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5373\u4f7f\u57fa\u7ebf\u65b9\u6cd5\u4f7f\u7528\u5916\u90e8\u4fe1\u606f\uff0cDeLeaker\u4e5f\u80fd\u5728\u4e0d\u635f\u5bb3\u4fdd\u771f\u5ea6\u6216\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u7f13\u89e3\u8bed\u4e49\u6cc4\u6f0f\uff0c\u540c\u65f6\u4f5c\u8005\u8fd8\u6784\u5efa\u4e86\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bed\u4e49\u6cc4\u6f0f\u8bc4\u4f30\u7684\u6570\u636e\u96c6SLIM\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u6ce8\u610f\u529b\u63a7\u5236\u7684\u4ef7\u503c\uff0c\u4e3a\u5f00\u53d1\u66f4\u8bed\u4e49\u7cbe\u786e\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u8bc1\u660e\u4e86\u76f4\u63a5\u5e72\u9884\u6ce8\u610f\u529b\u673a\u5236\u5728\u89e3\u51b3\u8bed\u4e49\u6cc4\u6f0f\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.15018", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15018", "abs": "https://arxiv.org/abs/2510.15018", "authors": ["Mingxuan Liu", "Honglin He", "Elisa Ricci", "Wayne Wu", "Bolei Zhou"], "title": "UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos", "comment": "Technical report. Project page: https://urbanverseproject.github.io/", "summary": "Urban embodied AI agents, ranging from delivery robots to quadrupeds, are\nincreasingly populating our cities, navigating chaotic streets to provide\nlast-mile connectivity. Training such agents requires diverse, high-fidelity\nurban environments to scale, yet existing human-crafted or procedurally\ngenerated simulation scenes either lack scalability or fail to capture\nreal-world complexity. We introduce UrbanVerse, a data-driven real-to-sim\nsystem that converts crowd-sourced city-tour videos into physics-aware,\ninteractive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a\nrepository of 100k+ annotated urban 3D assets with semantic and physical\nattributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene\nlayouts from video and instantiates metric-scale 3D simulations using retrieved\nassets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed\nscenes from 24 countries, along with a curated benchmark of 10 artist-designed\ntest scenes. Experiments show that UrbanVerse scenes preserve real-world\nsemantics and layouts, achieving human-evaluated realism comparable to manually\ncrafted scenes. In urban navigation, policies trained in UrbanVerse exhibit\nscaling power laws and strong generalization, improving success by +6.3% in\nsimulation and +30.1% in zero-shot sim-to-real transfer comparing to prior\nmethods, accomplishing a 300 m real-world mission with only two interventions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UrbanVerse\uff0c\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u771f\u5b9e\u5230\u4eff\u771f\u7cfb\u7edf\uff0c\u53ef\u5c06\u4f17\u5305\u57ce\u5e02\u6e38\u89c8\u89c6\u9891\u8f6c\u6362\u4e3a\u7269\u7406\u611f\u77e5\u7684\u4ea4\u4e92\u5f0f\u4eff\u771f\u573a\u666f\uff0c\u7528\u4e8e\u8bad\u7ec3\u57ce\u5e02\u5177\u8eabAI\u4ee3\u7406\u3002\u8be5\u7cfb\u7edf\u5305\u542b10\u4e07+\u6807\u6ce8\u76843D\u57ce\u5e02\u8d44\u4ea7\u5e93\u548c\u81ea\u52a8\u573a\u666f\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u7b56\u7565\u5728\u4eff\u771f\u548c\u96f6\u6837\u672c\u771f\u5b9e\u4e16\u754c\u8fc1\u79fb\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5de5\u5236\u4f5c\u6216\u7a0b\u5e8f\u751f\u6210\u7684\u4eff\u771f\u573a\u666f\u5728\u53ef\u6269\u5c55\u6027\u548c\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6027\u6355\u6349\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u57ce\u5e02\u5177\u8eabAI\u4ee3\u7406\u8bad\u7ec3\u9700\u6c42\uff0c\u8fd9\u4e9b\u4ee3\u7406\u9700\u8981\u5728\u6df7\u4e71\u7684\u57ce\u5e02\u8857\u9053\u4e2d\u5bfc\u822a\u4ee5\u63d0\u4f9b\u6700\u540e\u4e00\u516c\u91cc\u8fde\u63a5\u670d\u52a1\u3002", "method": "UrbanVerse\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aUrbanVerse-100K\uff08\u5305\u542b10\u4e07+\u5e26\u6709\u8bed\u4e49\u548c\u7269\u7406\u5c5e\u6027\u6807\u6ce8\u76843D\u57ce\u5e02\u8d44\u4ea7\u5e93\uff09\u548cUrbanVerse-Gen\uff08\u4ece\u89c6\u9891\u4e2d\u63d0\u53d6\u573a\u666f\u5e03\u5c40\u5e76\u4f7f\u7528\u68c0\u7d22\u8d44\u4ea7\u5b9e\u4f8b\u5316\u5ea6\u91cf\u5c3a\u5ea63D\u4eff\u771f\u7684\u81ea\u52a8\u6d41\u6c34\u7ebf\uff09\uff0c\u7cfb\u7edf\u8fd0\u884c\u5728IsaacSim\u5e73\u53f0\u4e0a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eUrbanVerse\u573a\u666f\u4fdd\u6301\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u8bed\u4e49\u548c\u5e03\u5c40\uff0c\u5728\u4eba\u7c7b\u8bc4\u4f30\u7684\u771f\u5b9e\u611f\u65b9\u9762\u4e0e\u4eba\u5de5\u5236\u4f5c\u573a\u666f\u76f8\u5f53\u3002\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u5728UrbanVerse\u4e2d\u8bad\u7ec3\u7684\u7b56\u7565\u5c55\u73b0\u51fa\u7f29\u653e\u5e42\u5f8b\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4eff\u771f\u4e2d\u6210\u529f\u7387\u63d0\u5347+6.3%\uff0c\u96f6\u6837\u672c\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u4e2d\u63d0\u5347+30.1%\uff0c\u4ec5\u9700\u4e24\u6b21\u5e72\u9884\u5373\u53ef\u5b8c\u6210300\u7c73\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u3002", "conclusion": "UrbanVerse\u8bc1\u660e\u4e86\u6570\u636e\u9a71\u52a8\u7684\u771f\u5b9e\u5230\u4eff\u771f\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u57ce\u5e02\u4eff\u771f\u573a\u666f\u7684\u53ef\u6269\u5c55\u6027\u548c\u771f\u5b9e\u6027\u95ee\u9898\uff0c\u4e3a\u5177\u8eabAI\u4ee3\u7406\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u57ce\u5e02\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u90e8\u7f72\u6027\u80fd\u3002"}}
{"id": "2510.15026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15026", "abs": "https://arxiv.org/abs/2510.15026", "authors": ["Mattia Segu", "Marta Tintore Gazulla", "Yongqin Xian", "Luc Van Gool", "Federico Tombari"], "title": "MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning", "comment": "ICCV 2025", "summary": "Scaling up model size and training data has advanced foundation models for\ninstance-level perception, achieving state-of-the-art in-domain and zero-shot\nperformance across object detection and segmentation. However, their high\ncomputational cost limits adoption on resource-constrained platforms. We first\nexamine the limitations of existing architectures in enabling efficient edge\ndeployment without compromising performance. We then introduce MOBIUS, a family\nof foundation models for universal instance segmentation, designed for\nPareto-optimal downscaling to support deployment across devices ranging from\nhigh-end accelerators to mobile hardware. To reduce training and inference\ndemands, we propose: (i) a bottleneck pixel decoder for efficient multi-scale\nand multi-modal fusion, (ii) a language-guided uncertainty calibration loss for\nadaptive decoder pruning, and (iii) a streamlined, unified training strategy.\nUnlike efficient baselines that trade accuracy for reduced complexity, MOBIUS\nreduces pixel and transformer decoder FLOPs by up to 55% and 75%, respectively,\nwhile maintaining state-of-the-art performance in just a third of the training\niterations. MOBIUS establishes a new benchmark for efficient segmentation on\nboth high-performance computing platforms and mobile devices.", "AI": {"tldr": "MOBIUS\u662f\u4e00\u4e2a\u7528\u4e8e\u901a\u7528\u5b9e\u4f8b\u5206\u5272\u7684\u57fa\u7840\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u74f6\u9888\u50cf\u7d20\u89e3\u7801\u5668\u3001\u8bed\u8a00\u5f15\u5bfc\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u635f\u5931\u548c\u7edf\u4e00\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u4ece\u9ad8\u7aef\u52a0\u901f\u5668\u5230\u79fb\u52a8\u8bbe\u5907\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\u867d\u7136\u5728\u5168\u666f\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u57df\u5185\u548c\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u5e94\u7528\uff0c\u9700\u8981\u89e3\u51b3\u6a21\u578b\u6548\u7387\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u74f6\u9888\u50cf\u7d20\u89e3\u7801\u5668\u7528\u4e8e\u9ad8\u6548\u7684\u591a\u5c3a\u5ea6\u548c\u591a\u6a21\u6001\u878d\u5408\uff0c\u8bbe\u8ba1\u4e86\u8bed\u8a00\u5f15\u5bfc\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u635f\u5931\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u89e3\u7801\u5668\u526a\u679d\uff0c\u5e76\u91c7\u7528\u7b80\u5316\u7684\u7edf\u4e00\u8bad\u7ec3\u7b56\u7565\u6765\u964d\u4f4e\u8bad\u7ec3\u548c\u63a8\u7406\u9700\u6c42\u3002", "result": "MOBIUS\u5c06\u50cf\u7d20\u548cTransformer\u89e3\u7801\u5668\u7684FLOPs\u5206\u522b\u964d\u4f4e\u4e8655%\u548c75%\uff0c\u4ec5\u7528\u4e09\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570\u5c31\u4fdd\u6301\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5728\u9ad8\u6548\u5206\u5272\u4efb\u52a1\u4e0a\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e73\u53f0\u548c\u79fb\u52a8\u8bbe\u5907\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u4f8b\u5206\u5272\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u9ad8\u6548AI\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2510.15253", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15253", "abs": "https://arxiv.org/abs/2510.15253", "authors": ["Sensen Gao", "Shanshan Zhao", "Xu Jiang", "Lunhao Duan", "Yong Xien Chng", "Qing-Guo Chen", "Weihua Luo", "Kaifu Zhang", "Jia-Wang Bian", "Mingming Gong"], "title": "Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding", "comment": null, "summary": "Document understanding is critical for applications from financial analysis\nto scientific discovery. Current approaches, whether OCR-based pipelines\nfeeding Large Language Models (LLMs) or native Multimodal LLMs (MLLMs), face\nkey limitations: the former loses structural detail, while the latter struggles\nwith context modeling. Retrieval-Augmented Generation (RAG) helps ground models\nin external data, but documents' multimodal nature, i.e., combining text,\ntables, charts, and layout, demands a more advanced paradigm: Multimodal RAG.\nThis approach enables holistic retrieval and reasoning across all modalities,\nunlocking comprehensive document intelligence. Recognizing its importance, this\npaper presents a systematic survey of Multimodal RAG for document\nunderstanding. We propose a taxonomy based on domain, retrieval modality, and\ngranularity, and review advances involving graph structures and agentic\nframeworks. We also summarize key datasets, benchmarks, and applications, and\nhighlight open challenges in efficiency, fine-grained representation, and\nrobustness, providing a roadmap for future progress in document AI.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u9762\u5411\u6587\u6863\u7406\u89e3\u7684\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u9886\u57df\u3001\u68c0\u7d22\u6a21\u6001\u548c\u7c92\u5ea6\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u603b\u7ed3\u4e86\u56fe\u7ed3\u6784\u548c\u667a\u80fd\u4f53\u6846\u67b6\u7b49\u5173\u952e\u6280\u672f\u8fdb\u5c55\uff0c\u4e3a\u6587\u6863AI\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002", "motivation": "\u5f53\u524d\u6587\u6863\u7406\u89e3\u65b9\u6cd5\u9762\u4e34\u5173\u952e\u9650\u5236\uff1a\u57fa\u4e8eOCR\u7684\u6d41\u6c34\u7ebf\u65b9\u6cd5\u4f1a\u4e22\u5931\u7ed3\u6784\u7ec6\u8282\uff0c\u800c\u539f\u751f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u867d\u7136\u6709\u52a9\u4e8e\u6a21\u578b\u57fa\u4e8e\u5916\u90e8\u6570\u636e\uff0c\u4f46\u6587\u6863\u7684\u591a\u6a21\u6001\u7279\u6027\u9700\u8981\u66f4\u5148\u8fdb\u7684\u8303\u5f0f\u6765\u89e3\u51b3\u6587\u672c\u3001\u8868\u683c\u3001\u56fe\u8868\u548c\u5e03\u5c40\u7684\u5168\u9762\u68c0\u7d22\u4e0e\u63a8\u7406\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u9886\u57df\u3001\u68c0\u7d22\u6a21\u6001\u548c\u7c92\u5ea6\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u7cfb\u7edf\u56de\u987e\u4e86\u6d89\u53ca\u56fe\u7ed3\u6784\u548c\u667a\u80fd\u4f53\u6846\u67b6\u7684\u6280\u672f\u8fdb\u5c55\uff0c\u603b\u7ed3\u4e86\u5173\u952e\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u5e94\u7528\u573a\u666f\uff0c\u91cd\u70b9\u5173\u6ce8\u591a\u6a21\u6001RAG\u5728\u6587\u6863\u7406\u89e3\u4e2d\u7684\u7cfb\u7edf\u5316\u5b9e\u73b0\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u603b\u7ed3\u4e86\u591a\u6a21\u6001RAG\u5728\u6587\u6863\u7406\u89e3\u9886\u57df\u7684\u5173\u952e\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u6548\u7387\u3001\u7ec6\u7c92\u5ea6\u8868\u793a\u548c\u9c81\u68d2\u6027\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\uff0c\u4e3a\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\u3002", "conclusion": "\u591a\u6a21\u6001RAG\u4e3a\u6587\u6863AI\u63d0\u4f9b\u4e86\u5168\u9762\u68c0\u7d22\u548c\u63a8\u7406\u7684\u65b0\u8303\u5f0f\uff0c\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u63ed\u793a\u4e86\u8be5\u9886\u57df\u5728\u6548\u7387\u4f18\u5316\u3001\u7ec6\u7c92\u5ea6\u8868\u793a\u589e\u5f3a\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u65b9\u9762\u7684\u5f00\u653e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\u548c\u53d1\u5c55\u6307\u5357\u3002"}}
{"id": "2510.15261", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15261", "abs": "https://arxiv.org/abs/2510.15261", "authors": ["Jitesh Jain", "Shubham Maheshwari", "Ning Yu", "Wen-mei Hwu", "Humphrey Shi"], "title": "AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory", "comment": "LAW 2025 Workshop at NeurIPS 2025. Work done from late 2023 to early\n  2024", "summary": "Riding on the success of LLMs with retrieval-augmented generation (RAG),\nthere has been a growing interest in augmenting agent systems with external\nmemory databases. However, the existing systems focus on storing text\ninformation in their memory, ignoring the importance of multimodal signals.\nMotivated by the multimodal nature of human memory, we present AUGUSTUS, a\nmultimodal agent system aligned with the ideas of human memory in cognitive\nscience. Technically, our system consists of 4 stages connected in a loop: (i)\nencode: understanding the inputs; (ii) store in memory: saving important\ninformation; (iii) retrieve: searching for relevant context from memory; and\n(iv) act: perform the task. Unlike existing systems that use vector databases,\nwe propose conceptualizing information into semantic tags and associating the\ntags with their context to store them in a graph-structured multimodal\ncontextual memory for efficient concept-driven retrieval. Our system\noutperforms the traditional multimodal RAG approach while being 3.5 times\nfaster for ImageNet classification and outperforming MemGPT on the MSC\nbenchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AUGUSTUS\uff0c\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u4eba\u7c7b\u8bb0\u5fc6\u7406\u8bba\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u5b9e\u73b0\u6982\u5ff5\u9a71\u52a8\u7684\u4fe1\u606f\u68c0\u7d22\uff0c\u5728ImageNet\u5206\u7c7b\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edf\u591a\u6a21\u6001RAG\u65b9\u6cd5\u5feb3.5\u500d\uff0c\u5e76\u5728MSC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eMemGPT\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u4fe1\u606f\u7684\u5b58\u50a8\uff0c\u5ffd\u89c6\u4e86\u591a\u6a21\u6001\u4fe1\u53f7\u7684\u91cd\u8981\u6027\uff0c\u800c\u4eba\u7c7b\u8bb0\u5fc6\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u7684\uff0c\u8fd9\u4fc3\u4f7f\u6211\u4eec\u5f00\u53d1\u4e0e\u8ba4\u77e5\u79d1\u5b66\u4e2d\u4eba\u7c7b\u8bb0\u5fc6\u7406\u8bba\u5bf9\u9f50\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u56db\u9636\u6bb5\u5faa\u73af\u67b6\u6784\uff1a\u7f16\u7801\u7406\u89e3\u8f93\u5165\u3001\u5b58\u50a8\u91cd\u8981\u4fe1\u606f\u5230\u8bb0\u5fc6\u3001\u4ece\u8bb0\u5fc6\u4e2d\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\u3001\u6267\u884c\u4efb\u52a1\uff1b\u4e0d\u540c\u4e8e\u4f20\u7edf\u5411\u91cf\u6570\u636e\u5e93\uff0c\u6211\u4eec\u63d0\u51fa\u5c06\u4fe1\u606f\u6982\u5ff5\u5316\u4e3a\u8bed\u4e49\u6807\u7b7e\uff0c\u5e76\u5c06\u5176\u4e0e\u4e0a\u4e0b\u6587\u5173\u8054\u5b58\u50a8\u5728\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u4e2d\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6982\u5ff5\u9a71\u52a8\u68c0\u7d22\u3002", "result": "\u5728ImageNet\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u7cfb\u7edf\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u591a\u6a21\u6001RAG\u65b9\u6cd5\u4e14\u901f\u5ea6\u5feb3.5\u500d\uff0c\u5728MSC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86MemGPT\u7684\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5c06\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u4eba\u7c7b\u8bb0\u5fc6\u539f\u7406\u878d\u5165\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u80fd\u591f\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u56fe\u7ed3\u6784\u7684\u8bed\u4e49\u6807\u7b7e\u8bb0\u5fc6\u673a\u5236\u4e3a\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8bb0\u5fc6\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.15040", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15040", "abs": "https://arxiv.org/abs/2510.15040", "authors": ["Xinyi Gu", "Jiayuan Mao", "Zhang-Wei Hong", "Zhuoran Yu", "Pengyuan Li", "Dhiraj Joshi", "Rogerio Feris", "Zexue He"], "title": "Composition-Grounded Instruction Synthesis for Visual Reasoning", "comment": null, "summary": "Pretrained multi-modal large language models (MLLMs) demonstrate strong\nperformance on diverse multimodal tasks, but remain limited in reasoning\ncapabilities for domains where annotations are difficult to collect. In this\nwork, we focus on artificial image domains such as charts, rendered documents,\nand webpages, which are abundant in practice yet lack large-scale human\nannotated reasoning datasets. We introduce COGS (COmposition-Grounded\ninstruction Synthesis), a data-efficient framework for equipping MLLMs with\nadvanced reasoning abilities from a small set of seed questions. The key idea\nis to decompose each seed question into primitive perception and reasoning\nfactors, which can then be systematically recomposed with new images to\ngenerate large collections of synthetic question-answer pairs. Each generated\nquestion is paired with subquestions and intermediate answers, enabling\nreinforcement learning with factor-level process rewards. Experiments on chart\nreasoning show that COGS substantially improves performance on unseen\nquestions, with the largest gains on reasoning-heavy and compositional\nquestions. Moreover, training with a factor-level mixture of different seed\ndata yields better transfer across multiple datasets, suggesting that COGS\ninduces generalizable capabilities rather than dataset-specific overfitting. We\nfurther demonstrate that the framework extends beyond charts to other domains\nsuch as webpages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86COGS\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u79cd\u5b50\u95ee\u9898\u5206\u89e3\u4e3a\u611f\u77e5\u548c\u63a8\u7406\u56e0\u5b50\u5e76\u8fdb\u884c\u7cfb\u7edf\u91cd\u7ec4\uff0c\u4ece\u800c\u4ece\u5c11\u91cf\u79cd\u5b50\u6570\u636e\u4e2d\u9ad8\u6548\u751f\u6210\u5927\u89c4\u6a21\u5408\u6210\u95ee\u7b54\u5bf9\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7b49\u4eba\u5de5\u56fe\u50cf\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6837\u5316\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6807\u6ce8\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u7684\u4eba\u5de5\u56fe\u50cf\u9886\u57df\uff08\u5982\u56fe\u8868\u3001\u6e32\u67d3\u6587\u6863\u548c\u7f51\u9875\uff09\u7684\u63a8\u7406\u80fd\u529b\u4ecd\u7136\u6709\u9650\uff0c\u8fd9\u4e9b\u9886\u57df\u5728\u5b9e\u8df5\u4e2d\u4e30\u5bcc\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u7684\u63a8\u7406\u6570\u636e\u96c6\u3002", "method": "COGS\u6846\u67b6\u5c06\u6bcf\u4e2a\u79cd\u5b50\u95ee\u9898\u5206\u89e3\u4e3a\u539f\u59cb\u611f\u77e5\u548c\u63a8\u7406\u56e0\u5b50\uff0c\u7136\u540e\u7cfb\u7edf\u6027\u5730\u4e0e\u65b0\u56fe\u50cf\u91cd\u65b0\u7ec4\u5408\u751f\u6210\u5927\u91cf\u5408\u6210\u95ee\u7b54\u5bf9\uff0c\u6bcf\u4e2a\u751f\u6210\u95ee\u9898\u90fd\u914d\u6709\u5b50\u95ee\u9898\u548c\u4e2d\u95f4\u7b54\u6848\uff0c\u652f\u6301\u57fa\u4e8e\u56e0\u5b50\u7ea7\u8fc7\u7a0b\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u56fe\u8868\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCOGS\u663e\u8457\u63d0\u5347\u4e86\u672a\u89c1\u95ee\u9898\u7684\u6027\u80fd\uff0c\u5728\u63a8\u7406\u5bc6\u96c6\u548c\u7ec4\u5408\u6027\u95ee\u9898\u4e0a\u7684\u63d0\u5347\u6700\u5927\uff0c\u4f7f\u7528\u4e0d\u540c\u79cd\u5b50\u6570\u636e\u7684\u56e0\u5b50\u7ea7\u6df7\u5408\u8bad\u7ec3\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8fc1\u79fb\u6027\u80fd\uff0c\u8868\u660eCOGS\u8bf1\u5bfc\u4e86\u6cdb\u5316\u80fd\u529b\u800c\u975e\u6570\u636e\u96c6\u7279\u5b9a\u7684\u8fc7\u62df\u5408\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660eCOGS\u6846\u67b6\u80fd\u591f\u4ece\u5c11\u91cf\u79cd\u5b50\u6570\u636e\u4e2d\u9ad8\u6548\u6269\u5c55\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u56fe\u8868\u4e4b\u5916\u7684\u7f51\u9875\u7b49\u5176\u4ed6\u9886\u57df\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u9886\u57df\u7684\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.15349", "categories": ["cs.CL", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.15349", "abs": "https://arxiv.org/abs/2510.15349", "authors": ["Baode Wang", "Biao Wu", "Weizhen Li", "Meng Fang", "Zuming Huang", "Jun Huang", "Haozhe Wang", "Yanjie Liang", "Ling Chen", "Wei Chu", "Yuan Qi"], "title": "Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing", "comment": "22 pages, 14 figures,", "summary": "Document parsing from scanned images into structured formats remains a\nsignificant challenge due to its complexly intertwined elements such as text\nparagraphs, figures, formulas, and tables. Existing supervised fine-tuning\nmethods often struggle to generalize across diverse document types, leading to\npoor performance, particularly on out-of-distribution data. This issue is\nfurther exacerbated by the limited availability of high-quality training data\nfor layout-aware parsing tasks. To address these challenges, we introduce\nLayoutRL, a reinforcement learning framework that optimizes layout\nunderstanding through composite rewards integrating normalized edit distance,\nparagraph count accuracy, and reading order preservation. To support this\ntraining, we construct the Infinity-Doc-400K dataset, which we use to train\nInfinity-Parser, a vision-language model demonstrating robust generalization\nacross various domains. Extensive evaluations on benchmarks including\nOmniDocBench, olmOCR-Bench, PubTabNet, and FinTabNet show that Infinity-Parser\nconsistently achieves state-of-the-art performance across a broad range of\ndocument types, languages, and structural complexities, substantially\noutperforming both specialized document parsing systems and general-purpose\nvision-language models. We will release our code, dataset, and model to\nfacilitate reproducible research in document parsing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LayoutRL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548cInfinity-Parser\u6a21\u578b\uff0c\u901a\u8fc7\u590d\u5408\u5956\u52b1\u673a\u5236\u4f18\u5316\u6587\u6863\u5e03\u5c40\u7406\u89e3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6587\u6863\u89e3\u6790\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728\u5904\u7406\u626b\u63cf\u6587\u6863\u89e3\u6790\u65f6\u96be\u4ee5\u6cdb\u5316\u5230\u591a\u6837\u5316\u6587\u6863\u7c7b\u578b\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u540c\u65f6\u9ad8\u8d28\u91cf\u5e03\u5c40\u611f\u77e5\u89e3\u6790\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86LayoutRL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5f52\u4e00\u5316\u7f16\u8f91\u8ddd\u79bb\u3001\u6bb5\u843d\u8ba1\u6570\u51c6\u786e\u6027\u548c\u9605\u8bfb\u987a\u5e8f\u4fdd\u6301\u7684\u590d\u5408\u5956\u52b1\u6765\u4f18\u5316\u5e03\u5c40\u7406\u89e3\uff0c\u5e76\u6784\u5efa\u4e86Infinity-Doc-400K\u6570\u636e\u96c6\u6765\u8bad\u7ec3Infinity-Parser\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728OmniDocBench\u3001olmOCR-Bench\u3001PubTabNet\u548cFinTabNet\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cInfinity-Parser\u5728\u5404\u79cd\u6587\u6863\u7c7b\u578b\u3001\u8bed\u8a00\u548c\u7ed3\u6784\u590d\u6742\u5ea6\u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4e13\u95e8\u7684\u6587\u6863\u89e3\u6790\u7cfb\u7edf\u548c\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u6587\u6863\u89e3\u6790\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u590d\u5408\u5956\u52b1\u673a\u5236\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u80fd\u591f\u5b9e\u73b0\u5f3a\u5927\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6587\u6863\u89e3\u6790\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2510.15306", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15306", "abs": "https://arxiv.org/abs/2510.15306", "authors": ["Kuang-Da Wang", "Zhao Wang", "Yotaro Shimose", "Wei-Yao Wang", "Shingo Takamatsu"], "title": "WebGen-V Bench: Structured Representation for Enhancing Visual Design in LLM-based Web Generation and Evaluation", "comment": null, "summary": "Witnessed by the recent advancements on leveraging LLM for coding and\nmultimodal understanding, we present WebGen-V, a new benchmark and framework\nfor instruction-to-HTML generation that enhances both data quality and\nevaluation granularity. WebGen-V contributes three key innovations: (1) an\nunbounded and extensible agentic crawling framework that continuously collects\nreal-world webpages and can leveraged to augment existing benchmarks; (2) a\nstructured, section-wise data representation that integrates metadata,\nlocalized UI screenshots, and JSON-formatted text and image assets, explicit\nalignment between content, layout, and visual components for detailed\nmultimodal supervision; and (3) a section-level multimodal evaluation protocol\naligning text, layout, and visuals for high-granularity assessment. Experiments\nwith state-of-the-art LLMs and ablation studies validate the effectiveness of\nour structured data and section-wise evaluation, as well as the contribution of\neach component. To the best of our knowledge, WebGen-V is the first work to\nenable high-granularity agentic crawling and evaluation for instruction-to-HTML\ngeneration, providing a unified pipeline from real-world data acquisition and\nwebpage generation to structured multimodal assessment.", "AI": {"tldr": "WebGen-V\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6307\u4ee4\u5230HTML\u751f\u6210\u7684\u65b0\u57fa\u51c6\u548c\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u521b\u65b0\u5b9e\u73b0\u4e86\u6570\u636e\u8d28\u91cf\u548c\u8bc4\u4f30\u7c92\u5ea6\u7684\u53cc\u91cd\u63d0\u5347\uff1a\u65e0\u754c\u53ef\u6269\u5c55\u7684\u4ee3\u7406\u722c\u53d6\u6846\u67b6\u3001\u7ed3\u6784\u5316\u5206\u8282\u6570\u636e\u8868\u793a\u4ee5\u53ca\u5206\u8282\u591a\u6a21\u6001\u8bc4\u4f30\u534f\u8bae\u3002", "motivation": "\u5f53\u524d\u6307\u4ee4\u5230HTML\u751f\u6210\u4efb\u52a1\u9762\u4e34\u6570\u636e\u8d28\u91cf\u4e0d\u8db3\u548c\u8bc4\u4f30\u7c92\u5ea6\u7c97\u7cd9\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u771f\u5b9e\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u548c\u66f4\u7cbe\u7ec6\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u673a\u5236\u6765\u63d0\u5347\u7f51\u9875\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "WebGen-V\u5f15\u5165\u4e86\u4e09\u4e2a\u6838\u5fc3\u6280\u672f\uff1a\u65e0\u754c\u53ef\u6269\u5c55\u7684\u4ee3\u7406\u722c\u53d6\u6846\u67b6\u6301\u7eed\u6536\u96c6\u771f\u5b9e\u7f51\u9875\u6570\u636e\uff1b\u7ed3\u6784\u5316\u5206\u8282\u6570\u636e\u8868\u793a\u6574\u5408\u5143\u6570\u636e\u3001\u5c40\u90e8UI\u622a\u56fe\u548cJSON\u683c\u5f0f\u7684\u6587\u672c\u56fe\u50cf\u8d44\u6e90\uff1b\u5206\u8282\u591a\u6a21\u6001\u8bc4\u4f30\u534f\u8bae\u5bf9\u9f50\u6587\u672c\u3001\u5e03\u5c40\u548c\u89c6\u89c9\u7ec4\u4ef6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u7ed3\u6784\u5316\u6570\u636e\u548c\u5206\u8282\u8bc4\u4f30\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u8d21\u732e\uff0c\u5b9e\u73b0\u4e86\u4ece\u771f\u5b9e\u6570\u636e\u91c7\u96c6\u5230\u7ed3\u6784\u5316\u591a\u6a21\u6001\u8bc4\u4f30\u7684\u7edf\u4e00\u6d41\u7a0b\u3002", "conclusion": "WebGen-V\u662f\u9996\u4e2a\u5b9e\u73b0\u9ad8\u7c92\u5ea6\u4ee3\u7406\u722c\u53d6\u548c\u8bc4\u4f30\u7684\u6307\u4ee4\u5230HTML\u751f\u6210\u5de5\u4f5c\uff0c\u4e3a\u7f51\u9875\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u4ece\u6570\u636e\u83b7\u53d6\u5230\u591a\u6a21\u6001\u8bc4\u4f30\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u7f51\u9875\u7684\u8d28\u91cf\u548c\u8bc4\u4f30\u7cbe\u5ea6\u3002"}}
{"id": "2510.15042", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15042", "abs": "https://arxiv.org/abs/2510.15042", "authors": ["Tassilo Wald", "Ibrahim Ethem Hamamci", "Yuan Gao", "Sam Bond-Taylor", "Harshita Sharma", "Maximilian Ilse", "Cynthia Lo", "Olesya Melnichenko", "Noel C. F. Codella", "Maria Teodora Wetscherek", "Klaus H. Maier-Hein", "Panagiotis Korfiatis", "Valentina Salvatelli", "Javier Alvarez-Valle", "Fernando P\u00e9rez-Garc\u00eda"], "title": "Comprehensive language-image pre-training for 3D medical image understanding", "comment": null, "summary": "Vision-language pre-training, i.e., aligning images with paired text, is a\npowerful paradigm to create encoders that can be directly used for tasks such\nas classification and retrieval, and for downstream tasks such as segmentation\nand report generation. In the 3D medical image domain, these capabilities allow\nvision-language encoders (VLEs) to support radiologists by retrieving patients\nwith similar abnormalities or predicting likelihoods of abnormality. While the\nmethodology holds promise, data availability limits the capabilities of current\n3D VLEs.\n  In this paper, we alleviate the lack of data by injecting additional\ninductive biases: introducing a report generation objective and pairing\nvision-language pre-training with vision-only pre-training. This allows us to\nleverage both image-only and paired image-text 3D datasets, increasing the\ntotal amount of data to which our model is exposed. Through these additional\ninductive biases, paired with best practices of the 3D medical imaging domain,\nwe develop the Comprehensive Language-image Pre-training (COLIPRI) encoder\nfamily. Our COLIPRI encoders achieve state-of-the-art performance in report\ngeneration, classification probing, and zero-shot classification, and remain\ncompetitive for semantic segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86COLIPRI\u7f16\u7801\u5668\u5bb6\u65cf\uff0c\u901a\u8fc7\u5f15\u5165\u62a5\u544a\u751f\u6210\u76ee\u6807\u548c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u4e0e\u7eaf\u89c6\u89c9\u9884\u8bad\u7ec3\u6765\u89e3\u51b33D\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d3D\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u7f16\u7801\u5668\u9762\u4e34\u6570\u636e\u53ef\u7528\u6027\u9650\u5236\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u652f\u6301\u653e\u5c04\u79d1\u533b\u751f\u8fdb\u884c\u76f8\u4f3c\u5f02\u5e38\u75c5\u4f8b\u68c0\u7d22\u548c\u5f02\u5e38\u53ef\u80fd\u6027\u9884\u6d4b\u7b49\u65b9\u9762\u7684\u80fd\u529b\uff0c\u9700\u8981\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5236\u7ea6\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u989d\u5916\u7684\u5f52\u7eb3\u504f\u7f6e\u6765\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u5305\u62ec\u5f15\u5165\u62a5\u544a\u751f\u6210\u76ee\u6807\u4ee5\u53ca\u5c06\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u4e0e\u7eaf\u89c6\u89c9\u9884\u8bad\u7ec3\u76f8\u7ed3\u5408\uff0c\u4ece\u800c\u80fd\u591f\u540c\u65f6\u5229\u7528\u7eaf\u56fe\u50cf\u548c\u914d\u5bf9\u56fe\u50cf\u6587\u672c3D\u6570\u636e\u96c6\uff0c\u5e76\u9075\u5faa3D\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7684\u6700\u4f73\u5b9e\u8df5\u5f00\u53d1\u4e86COLIPRI\u7f16\u7801\u5668\u5bb6\u65cf\u3002", "result": "COLIPRI\u7f16\u7801\u5668\u5728\u62a5\u544a\u751f\u6210\u3001\u5206\u7c7b\u63a2\u6d4b\u548c\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u989d\u5916\u7684\u5f52\u7eb3\u504f\u7f6e\u548c\u7ed3\u5408\u4e0d\u540c\u7c7b\u578b\u7684\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e33D\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u533b\u5b66\u5f71\u50cf\u5206\u6790\u5de5\u5177\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2510.15418", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15418", "abs": "https://arxiv.org/abs/2510.15418", "authors": ["Lee Qi Zun", "Mohamad Zulhilmi Bin Abdul Halim", "Goh Man Fye"], "title": "Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs", "comment": null, "summary": "Retrieval-Augmented Generation systems are essential for providing fact-based\nguidance from Malaysian Clinical Practice Guidelines. However, their\neffectiveness with image-based queries is limited, as general Vision-Language\nModel captions often lack clinical specificity and factual grounding. This\nstudy proposes and validates a framework to specialize the MedGemma model for\ngenerating high-fidelity captions that serve as superior queries. To overcome\ndata scarcity, we employ a knowledge distillation pipeline to create a\nsynthetic dataset across dermatology, fundus, and chest radiography domains,\nand fine-tune MedGemma using the parameter-efficient QLoRA method. Performance\nwas rigorously assessed through a dual framework measuring both classification\naccuracy and, via a novel application of the RAGAS framework, caption\nfaithfulness, relevancy, and correctness. The fine-tuned model demonstrated\nsubstantial improvements in classification performance, while RAGAS evaluation\nconfirmed significant gains in caption faithfulness and correctness, validating\nthe models ability to produce reliable, factually grounded descriptions. This\nwork establishes a robust pipeline for specializing medical VLMs and validates\nthe resulting model as a high-quality query generator, laying the groundwork\nfor enhancing multimodal RAG systems in evidence-based clinical decision\nsupport.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u4e13\u95e8\u5316MedGemma\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u533b\u5b66\u56fe\u50cf\u63cf\u8ff0\u4f5c\u4e3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u4f18\u8d28\u67e5\u8be2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u5728\u5904\u7406\u57fa\u4e8e\u56fe\u50cf\u7684\u533b\u5b66\u67e5\u8be2\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u63cf\u8ff0\u7f3a\u4e4f\u4e34\u5e8a\u7279\u5f02\u6027\u548c\u4e8b\u5b9e\u57fa\u7840\uff0c\u9650\u5236\u4e86\u5176\u5728\u9a6c\u6765\u897f\u4e9a\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\u4e2d\u63d0\u4f9b\u57fa\u4e8e\u4e8b\u5b9e\u6307\u5bfc\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u7ba1\u9053\u521b\u5efa\u76ae\u80a4\u79d1\u3001\u773c\u5e95\u548c\u80f8\u90e8\u653e\u5c04\u5b66\u9886\u57df\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u7684QLoRA\u65b9\u6cd5\u5bf9MedGemma\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u901a\u8fc7\u53cc\u91cd\u8bc4\u4f30\u6846\u67b6\u8861\u91cf\u5206\u7c7b\u51c6\u786e\u6027\u548c\u4f7f\u7528RAGAS\u6846\u67b6\u8bc4\u4f30\u63cf\u8ff0\u5fe0\u5b9e\u6027\u3001\u76f8\u5173\u6027\u548c\u6b63\u786e\u6027\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0cRAGAS\u8bc4\u4f30\u786e\u8ba4\u4e86\u63cf\u8ff0\u5fe0\u5b9e\u6027\u548c\u6b63\u786e\u6027\u7684\u663e\u8457\u6539\u5584\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u751f\u6210\u53ef\u9760\u3001\u4e8b\u5b9e\u57fa\u7840\u63cf\u8ff0\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u4e13\u95e8\u5316\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a33\u5065\u6d41\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u6240\u5f97\u6a21\u578b\u4f5c\u4e3a\u9ad8\u8d28\u91cf\u67e5\u8be2\u751f\u6210\u5668\u7684\u6709\u6548\u6027\uff0c\u4e3a\u589e\u5f3a\u57fa\u4e8e\u8bc1\u636e\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.15317", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15317", "abs": "https://arxiv.org/abs/2510.15317", "authors": ["Tingqiao Xu", "Ziru Zeng", "Jiayu Chen"], "title": "VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal Data", "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "The quality of supervised fine-tuning (SFT) data is crucial for the\nperformance of large multimodal models (LMMs), yet current data enhancement\nmethods often suffer from factual errors and hallucinations due to inadequate\nvisual perception. To address this challenge, we propose VERITAS, a pipeline\nthat systematically integrates vision priors and multiple state-of-the-art LMMs\nwith statistical methods to enhance SFT data quality. VERITAS leverages visual\nrecognition models (RAM++) and OCR systems (PP-OCRv4) to extract structured\nvision priors, which are combined with images, questions, and answers. Three\nLMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) evaluate the original answers,\nproviding critique rationales and scores that are statistically fused into a\nhigh-confidence consensus score serving as ground truth. Using this consensus,\nwe train a lightweight critic model via Group Relative Policy Optimization\n(GRPO), enhancing reasoning capabilities efficiently. Each LMM then refines the\noriginal answers based on the critiques, generating new candidate answers; we\nselect the highest-scoring one as the final refined answer. Experiments across\nsix multimodal benchmarks demonstrate that models fine-tuned with data\nprocessed by VERITAS consistently outperform those using raw data, particularly\nin text-rich and fine-grained reasoning tasks. Our critic model exhibits\nenhanced capability comparable to state-of-the-art LMMs while being\nsignificantly more efficient. We release our pipeline, datasets, and model\ncheckpoints to advance research in multimodal data optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVERITAS\u7ba1\u9053\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u5148\u9a8c\u548c\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u7edf\u8ba1\u878d\u5408\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u63d0\u5347\u76d1\u7763\u5fae\u8c03\u6570\u636e\u7684\u8d28\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u4e8b\u5b9e\u9519\u8bef\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7684\u89c6\u89c9\u611f\u77e5\u4e0d\u8db3\u95ee\u9898\uff0c\u5bfc\u81f4\u4e8b\u5b9e\u9519\u8bef\u548c\u5e7b\u89c9\u9891\u53d1\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6574\u5408\u89c6\u89c9\u4fe1\u606f\u5e76\u786e\u4fdd\u6570\u636e\u51c6\u786e\u6027\u7684\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "VERITAS\u91c7\u7528\u89c6\u89c9\u8bc6\u522b\u6a21\u578bRAM++\u548cOCR\u7cfb\u7edfPP-OCRv4\u63d0\u53d6\u7ed3\u6784\u5316\u89c6\u89c9\u5148\u9a8c\uff0c\u7ed3\u5408\u4e09\u4e2a\u5148\u8fdbLMM\u6a21\u578b\u8fdb\u884c\u7b54\u6848\u8bc4\u4f30\uff0c\u901a\u8fc7\u7edf\u8ba1\u878d\u5408\u751f\u6210\u9ad8\u7f6e\u4fe1\u5ea6\u5171\u8bc6\u5206\u6570\u4f5c\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u5e76\u5229\u7528Group Relative Policy Optimization\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6279\u8bc4\u6a21\u578b\uff0c\u6700\u7ec8\u9009\u62e9\u6700\u9ad8\u5206\u5019\u9009\u7b54\u6848\u4f5c\u4e3a\u7cbe\u70bc\u7ed3\u679c\u3002", "result": "\u5728\u516d\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528VERITAS\u5904\u7406\u6570\u636e\u5fae\u8c03\u7684\u6a21\u578b\u6027\u80fd\u5168\u9762\u8d85\u8d8a\u539f\u59cb\u6570\u636e\uff0c\u5c24\u5176\u5728\u6587\u672c\u4e30\u5bcc\u548c\u7ec6\u7c92\u5ea6\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u6279\u8bc4\u6a21\u578b\u5728\u4fdd\u6301\u4e0e\u5148\u8fdbLMM\u76f8\u5f53\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "conclusion": "VERITAS\u8bc1\u660e\u4e86\u7cfb\u7edf\u6027\u6574\u5408\u89c6\u89c9\u5148\u9a8c\u548c\u591a\u6a21\u578b\u5171\u8bc6\u673a\u5236\u5bf9\u63d0\u5347SFT\u6570\u636e\u8d28\u91cf\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u6570\u636e\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u6279\u8bc4\u6a21\u578b\u5728\u4fdd\u6301\u6027\u80fd\u524d\u63d0\u4e0b\u5b9e\u73b0\u6548\u7387\u4f18\u5316\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.15050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15050", "abs": "https://arxiv.org/abs/2510.15050", "authors": ["Chao Huang", "Zeliang Zhang", "Jiang Liu", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Ze Wang", "Chenliang Xu", "Emad Barsoum", "Zicheng Liu"], "title": "Directional Reasoning Injection for Fine-Tuning MLLMs", "comment": "Project Page: https://wikichao.github.io/DRIFT/", "summary": "Multimodal large language models (MLLMs) are rapidly advancing, yet their\nreasoning ability often lags behind that of strong text-only counterparts.\nExisting methods to bridge this gap rely on supervised fine-tuning over\nlarge-scale multimodal reasoning data or reinforcement learning, both of which\nare resource-intensive. A promising alternative is model merging, which\ninterpolates parameters between reasoning-enhanced LLMs and multimodal\nvariants. However, our analysis shows that naive merging is not always a \"free\nlunch\": its effectiveness varies drastically across model families, with some\n(e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance\ndegradation. To address this, we propose Directional Reasoning Injection for\nFine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning\nknowledge in the gradient space, without destabilizing multimodal alignment.\nDRIFT precomputes a reasoning prior as the parameter-space difference between\nreasoning and multimodal variants, then uses it to bias gradients during\nmultimodal fine-tuning. This approach preserves the simplicity of standard\nsupervised fine-tuning pipelines while enabling efficient reasoning transfer.\nExtensive experiments on multimodal reasoning benchmarks, including MathVista\nand MathVerse, demonstrate that DRIFT consistently improves reasoning\nperformance over naive merging and supervised fine-tuning, while matching or\nsurpassing training-heavy methods at a fraction of the cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDRIFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u7a7a\u95f4\u4e2d\u7684\u63a8\u7406\u77e5\u8bc6\u8f6c\u79fb\u6765\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u76d1\u7763\u5fae\u8c03\u6216\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u591a\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u901a\u5e38\u843d\u540e\u4e8e\u7eaf\u6587\u672c\u6a21\u578b\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u7684\u5927\u89c4\u6a21\u76d1\u7763\u5fae\u8c03\u6216\u5f3a\u5316\u5b66\u4e60\uff0c\u800c\u7b80\u5355\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u4e2d\u6548\u679c\u4e0d\u7a33\u5b9a\uff0c\u90e8\u5206\u6a21\u578b\u751a\u81f3\u51fa\u73b0\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faDRIFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u63a8\u7406\u5148\u9a8c\u4f5c\u4e3a\u63a8\u7406\u53d8\u4f53\u4e0e\u591a\u6a21\u6001\u53d8\u4f53\u5728\u53c2\u6570\u7a7a\u95f4\u7684\u5dee\u5f02\uff0c\u7136\u540e\u5728\u591a\u6a21\u6001\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5229\u7528\u8be5\u5148\u9a8c\u6765\u504f\u7f6e\u68af\u5ea6\uff0c\u5b9e\u73b0\u5728\u68af\u5ea6\u7a7a\u95f4\u4e2d\u7684\u63a8\u7406\u77e5\u8bc6\u8f6c\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5728MathVista\u548cMathVerse\u7b49\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDRIFT\u65b9\u6cd5\u76f8\u6bd4\u6734\u7d20\u878d\u5408\u548c\u76d1\u7763\u5fae\u8c03\u6301\u7eed\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u4ee5\u6781\u4f4e\u6210\u672c\u8fbe\u5230\u6216\u8d85\u8d8a\u8bad\u7ec3\u5bc6\u96c6\u578b\u65b9\u6cd5\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "DRIFT\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u5728\u68af\u5ea6\u7a7a\u95f4\u8fdb\u884c\u77e5\u8bc6\u8f6c\u79fb\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u80fd\u529b\u63d0\u5347\u5f00\u8f9f\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u6d41\u7a0b\u7684\u7b80\u6d01\u6027\u3002"}}
{"id": "2510.15421", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15421", "abs": "https://arxiv.org/abs/2510.15421", "authors": ["Hongcheng Liu", "Pingjie Wang", "Yuhao Wang", "Siqu Ou", "Yanfeng Wang", "Yu Wang"], "title": "When Seeing Is not Enough: Revealing the Limits of Active Reasoning in MLLMs", "comment": "20 pages, 13 figures", "summary": "Multimodal large language models (MLLMs) have shown strong capabilities\nacross a broad range of benchmarks. However, most existing evaluations focus on\npassive inference, where models perform step-by-step reasoning under complete\ninformation. This setup is misaligned with real-world use, where seeing is not\nenough. This raises a fundamental question: Can MLLMs actively acquire missing\nevidence under incomplete information? To bridge this gap, we require the MLLMs\nto actively acquire missing evidence and iteratively refine decisions under\nincomplete information, by selecting a target image from a candidate pool\nwithout task-specific priors. To support systematic study, we propose\nGuessBench, a benchmark with both perception-oriented and knowledge-oriented\nimages for evaluating active reasoning in MLLMs. We evaluate 20 superior MLLMs\nand find that performance on active reasoning lags far behind it on passive\nsettings, indicating substantial room for improvement. Further analysis\nidentifies fine-grained perception and timely decision-making as key\nchallenges. Ablation studies show that perceptual enhancements benefit smaller\nmodels, whereas thinking-oriented methods provide consistent gains across model\nsizes. These results suggest promising directions for future research on\nmultimodal active reasoning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86GuessBench\u57fa\u51c6\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u52a8\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u4e3b\u52a8\u83b7\u53d6\u7f3a\u5931\u8bc1\u636e\u548c\u8fed\u4ee3\u51b3\u7b56\u65b9\u9762\u7684\u80fd\u529b\u8fdc\u4f4e\u4e8e\u88ab\u52a8\u63a8\u7406\uff0c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u4e3b\u52a8\u63a8\u7406\u9886\u57df\u7684\u91cd\u8981\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u88ab\u52a8\u63a8\u7406\u573a\u666f\uff0c\u5373\u5728\u5b8c\u6574\u4fe1\u606f\u4e0b\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0c\u8fd9\u4e0e\u73b0\u5b9e\u4e16\u754c\u4e2d\u4fe1\u606f\u4e0d\u5b8c\u6574\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u5b58\u5728\u504f\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u6a21\u578b\u5728\u4fe1\u606f\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u4e3b\u52a8\u83b7\u53d6\u7f3a\u5931\u8bc1\u636e\u5e76\u8fdb\u884c\u8fed\u4ee3\u51b3\u7b56\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86GuessBench\u57fa\u51c6\uff0c\u5305\u542b\u611f\u77e5\u5bfc\u5411\u548c\u77e5\u8bc6\u5bfc\u5411\u7684\u56fe\u50cf\uff0c\u8981\u6c42\u6a21\u578b\u5728\u6ca1\u6709\u4efb\u52a1\u7279\u5b9a\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\u4ece\u5019\u9009\u6c60\u4e2d\u9009\u62e9\u76ee\u6807\u56fe\u50cf\uff0c\u901a\u8fc7\u4e3b\u52a8\u83b7\u53d6\u7f3a\u5931\u8bc1\u636e\u548c\u8fed\u4ee3\u4f18\u5316\u51b3\u7b56\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u7684\u4e3b\u52a8\u63a8\u7406\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u4e8620\u4e2a\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u4e3b\u52a8\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdc\u4f4e\u4e8e\u88ab\u52a8\u63a8\u7406\u573a\u666f\uff0c\u6d88\u878d\u7814\u7a76\u8868\u660e\u611f\u77e5\u589e\u5f3a\u5bf9\u5c0f\u578b\u6a21\u578b\u6709\u76ca\uff0c\u800c\u601d\u7ef4\u5bfc\u5411\u65b9\u6cd5\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u90fd\u80fd\u5e26\u6765\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u7ec6\u7c92\u5ea6\u611f\u77e5\u548c\u53ca\u65f6\u51b3\u7b56\u662f\u4e3b\u52a8\u63a8\u7406\u7684\u4e3b\u8981\u6311\u6218\uff0c\u611f\u77e5\u589e\u5f3a\u548c\u601d\u7ef4\u5bfc\u5411\u65b9\u6cd5\u4e3a\u672a\u6765\u591a\u6a21\u6001\u4e3b\u52a8\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u8868\u660e\u8be5\u9886\u57df\u4ecd\u6709\u5de8\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2510.15374", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15374", "abs": "https://arxiv.org/abs/2510.15374", "authors": ["Zezhong Tan", "Hang Gao", "Xinhong Ma", "Feng Zhang", "Ziqiang Dong"], "title": "Towards Flash Thinking via Decoupled Advantage Policy Optimization", "comment": null, "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable performance in\nsolving complex problems via supervised fine-tuning (SFT) and reinforcement\nlearning (RL). Although existing RL algorithms significantly enhance model\naccuracy, they still suffer from excessively lengthy responses and overthinking\nissues, resulting in increased inference latency and computational consumption,\nespecially for simple tasks that require minimal reasoning. To address this, we\npropose a novel RL framework, DEPO, to reduce inefficient reasoning for models.\nOur method mainly consists of three core components: (1) an innovative\nadvantage decoupled algorithm to guide model reduction of inefficient tokens;\n(2) a difficulty-aware length penalty to lower the overall length of model\nresponses; (3) an advantage clipping method to prevent bias in policy\noptimization. In our experiments, applied to DeepSeek-Distill-Qwen-7B and\nDeepSeek-Distill-Qwen-1.5B as base models, DEPO achieves a significant\nreduction in sequence length by 39% and reduces excessive reasoning paths in\ninefficient tokens, while outperforming the base model in overall accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDEPO\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u4f4e\u6548\u63a8\u7406\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u52bf\u89e3\u8026\u7b97\u6cd5\u3001\u96be\u5ea6\u611f\u77e5\u957f\u5ea6\u60e9\u7f5a\u548c\u4f18\u52bf\u88c1\u526a\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u54cd\u5e94\u957f\u5ea6\u548c\u8ba1\u7b97\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u867d\u7136\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u4f46\u4ecd\u5b58\u5728\u54cd\u5e94\u8fc7\u957f\u548c\u8fc7\u5ea6\u601d\u8003\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u589e\u52a0\u548c\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7b80\u5355\u4efb\u52a1\u65f6\u5c24\u4e3a\u660e\u663e\u3002\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "DEPO\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u521b\u65b0\u7684\u4f18\u52bf\u89e3\u8026\u7b97\u6cd5\u7528\u4e8e\u6307\u5bfc\u6a21\u578b\u51cf\u5c11\u4f4e\u6548token\uff1b\u96be\u5ea6\u611f\u77e5\u957f\u5ea6\u60e9\u7f5a\u673a\u5236\u4ee5\u964d\u4f4e\u6574\u4f53\u54cd\u5e94\u957f\u5ea6\uff1b\u4f18\u52bf\u88c1\u526a\u65b9\u6cd5\u9632\u6b62\u7b56\u7565\u4f18\u5316\u4e2d\u7684\u504f\u5dee\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8eDeepSeek-Distill-Qwen\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728DeepSeek-Distill-Qwen-7B\u548c1.5B\u6a21\u578b\u4e0a\uff0cDEPO\u5b9e\u73b0\u4e86\u5e8f\u5217\u957f\u5ea6\u51cf\u5c1139%\u7684\u663e\u8457\u6548\u679c\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4f4e\u6548token\u4e2d\u7684\u8fc7\u5ea6\u63a8\u7406\u8def\u5f84\u3002\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u8ba1\u7b97\u6d88\u8017\u7684\u540c\u65f6\uff0c\u6574\u4f53\u51c6\u786e\u7387\u4ecd\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "DEPO\u6846\u67b6\u4e3a\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u4f4e\u6548\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u4f18\u5316\u63a8\u7406\u6548\u7387\u7684\u53ef\u884c\u6027\u3002\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u9ad8\u6548\u63a8\u7406\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.15104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15104", "abs": "https://arxiv.org/abs/2510.15104", "authors": ["Guofeng Zhang", "Angtian Wang", "Jacob Zhiyuan Fang", "Liming Jiang", "Haotian Yang", "Bo Liu", "Yiding Yang", "Guang Chen", "Longyin Wen", "Alan Yuille", "Chongyang Ma"], "title": "TGT: Text-Grounded Trajectories for Locally Controlled Video Generation", "comment": null, "summary": "Text-to-video generation has advanced rapidly in visual fidelity, whereas\nstandard methods still have limited ability to control the subject composition\nof generated scenes. Prior work shows that adding localized text control\nsignals, such as bounding boxes or segmentation masks, can help. However, these\nmethods struggle in complex scenarios and degrade in multi-object settings,\noffering limited precision and lacking a clear correspondence between\nindividual trajectories and visual entities as the number of controllable\nobjects increases. We introduce Text-Grounded Trajectories (TGT), a framework\nthat conditions video generation on trajectories paired with localized text\ndescriptions. We propose Location-Aware Cross-Attention (LACA) to integrate\nthese signals and adopt a dual-CFG scheme to separately modulate local and\nglobal text guidance. In addition, we develop a data processing pipeline that\nproduces trajectories with localized descriptions of tracked entities, and we\nannotate two million high quality video clips to train TGT. Together, these\ncomponents enable TGT to use point trajectories as intuitive motion handles,\npairing each trajectory with text to control both appearance and motion.\nExtensive experiments show that TGT achieves higher visual quality, more\naccurate text alignment, and improved motion controllability compared with\nprior approaches. Website: https://textgroundedtraj.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6587\u672c\u5f15\u5bfc\u8f68\u8ff9\uff08TGT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u70b9\u8f68\u8ff9\u4e0e\u5c40\u90e8\u6587\u672c\u63cf\u8ff0\u914d\u5bf9\u6765\u63a7\u5236\u89c6\u9891\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u591a\u5bf9\u8c61\u573a\u666f\u4e0b\u73b0\u6709\u65b9\u6cd5\u63a7\u5236\u7cbe\u5ea6\u4e0d\u8db3\u548c\u8f68\u8ff9-\u5b9e\u4f53\u5bf9\u5e94\u5173\u7cfb\u6a21\u7cca\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u6587\u672c\u5bf9\u9f50\u548c\u8fd0\u52a8\u53ef\u63a7\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u63a7\u5236\u751f\u6210\u573a\u666f\u7684\u4e3b\u4f53\u6784\u56fe\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u591a\u5bf9\u8c61\u8bbe\u7f6e\u4e0b\uff0c\u73b0\u6709\u57fa\u4e8e\u8fb9\u754c\u6846\u6216\u5206\u5272\u63a9\u7801\u7684\u5c40\u90e8\u6587\u672c\u63a7\u5236\u65b9\u6cd5\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u4e14\u968f\u7740\u53ef\u63a7\u5bf9\u8c61\u6570\u91cf\u589e\u52a0\uff0c\u4e2a\u4f53\u8f68\u8ff9\u4e0e\u89c6\u89c9\u5b9e\u4f53\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u53d8\u5f97\u6a21\u7cca\u4e0d\u6e05\u3002", "method": "\u63d0\u51fa\u4e86\u6587\u672c\u5f15\u5bfc\u8f68\u8ff9\uff08TGT\uff09\u6846\u67b6\uff0c\u91c7\u7528\u4f4d\u7f6e\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b\uff08LACA\uff09\u6765\u6574\u5408\u8f68\u8ff9\u548c\u5c40\u90e8\u6587\u672c\u63cf\u8ff0\u4fe1\u53f7\uff0c\u5e76\u91c7\u7528\u53cc\u91cd\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff08dual-CFG\uff09\u65b9\u6848\u5206\u522b\u8c03\u8282\u5c40\u90e8\u548c\u5168\u5c40\u6587\u672c\u6307\u5bfc\u3002\u5f00\u53d1\u4e86\u6570\u636e\u5904\u7406\u6d41\u6c34\u7ebf\u6765\u751f\u6210\u5e26\u6709\u8ddf\u8e2a\u5b9e\u4f53\u5c40\u90e8\u63cf\u8ff0\u7b26\u7684\u8f68\u8ff9\uff0c\u5e76\u6807\u6ce8\u4e86200\u4e07\u4e2a\u9ad8\u8d28\u91cf\u89c6\u9891\u7247\u6bb5\u7528\u4e8e\u8bad\u7ec3TGT\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTGT\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u6587\u672c\u5bf9\u9f50\u51c6\u786e\u6027\u548c\u8fd0\u52a8\u53ef\u63a7\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u4f7f\u7528\u70b9\u8f68\u8ff9\u4f5c\u4e3a\u76f4\u89c2\u7684\u8fd0\u52a8\u63a7\u5236\u624b\u67c4\uff0c\u5c06\u6bcf\u4e2a\u8f68\u8ff9\u4e0e\u6587\u672c\u914d\u5bf9\u4ee5\u540c\u65f6\u63a7\u5236\u5916\u89c2\u548c\u8fd0\u52a8\u3002", "conclusion": "TGT\u6846\u67b6\u901a\u8fc7\u5c06\u8f68\u8ff9\u4e0e\u5c40\u90e8\u6587\u672c\u63cf\u8ff0\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u591a\u5bf9\u8c61\u573a\u666f\u7684\u63a7\u5236\u7cbe\u5ea6\u548c\u7075\u6d3b\u6027\uff0c\u4e3a\u590d\u6742\u573a\u666f\u7684\u7cbe\u786e\u53ef\u63a7\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u8f68\u8ff9\u4f5c\u4e3a\u8fd0\u52a8\u63a7\u5236\u624b\u67c4\u7684\u76f4\u89c2\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.15543", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.15543", "abs": "https://arxiv.org/abs/2510.15543", "authors": ["Qiyu Wu", "Shuyang Cui", "Satoshi Hayakawa", "Wei-Yao Wang", "Hiromi Wakaki", "Yuki Mitsufuji"], "title": "MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval", "comment": null, "summary": "Multimodal retrieval, which seeks to retrieve relevant content across\nmodalities such as text or image, supports applications from AI search to\ncontents production. Despite the success of separate-encoder approaches like\nCLIP align modality-specific embeddings with contrastive learning, recent\nmultimodal large language models (MLLMs) enable a unified encoder that directly\nprocesses composed inputs. While flexible and advanced, we identify that\nunified encoders trained with conventional contrastive learning are prone to\nlearn modality shortcut, leading to poor robustness under distribution shifts.\nWe propose a modality composition awareness framework to mitigate this issue.\nConcretely, a preference loss enforces multimodal embeddings to outperform\ntheir unimodal counterparts, while a composition regularization objective\naligns multimodal embeddings with prototypes composed from its unimodal parts.\nThese objectives explicitly model structural relationships between the composed\nrepresentation and its unimodal counterparts. Experiments on various benchmarks\nshow gains in out-of-distribution retrieval, highlighting modality composition\nawareness as a effective principle for robust composed multimodal retrieval\nwhen utilizing MLLMs as the unified encoder.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u6001\u7ec4\u5408\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u504f\u597d\u635f\u5931\u548c\u7ec4\u5408\u6b63\u5219\u5316\u76ee\u6807\u6765\u7f13\u89e3\u7edf\u4e00\u7f16\u7801\u5668\u5728\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u7684\u6a21\u6001\u6377\u5f84\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u68c0\u7d22\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u7edf\u4e00\u7f16\u7801\u5668\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u68c0\u7d22\u4e2d\u5c55\u73b0\u51fa\u7075\u6d3b\u6027\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u4f20\u7edf\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u7684\u7edf\u4e00\u7f16\u7801\u5668\u5bb9\u6613\u5b66\u4e60\u6a21\u6001\u6377\u5f84\uff0c\u5bfc\u81f4\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u9c81\u68d2\u6027\u8f83\u5dee\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6a21\u6001\u7ec4\u5408\u611f\u77e5\u6846\u67b6\uff0c\u5305\u542b\u504f\u597d\u635f\u5931\u5f3a\u5236\u591a\u6a21\u6001\u5d4c\u5165\u4f18\u4e8e\u5176\u5355\u6a21\u6001\u5bf9\u5e94\u7269\uff0c\u4ee5\u53ca\u7ec4\u5408\u6b63\u5219\u5316\u76ee\u6807\u5c06\u591a\u6a21\u6001\u5d4c\u5165\u4e0e\u5176\u5355\u6a21\u6001\u90e8\u5206\u7ec4\u5408\u7684\u539f\u578b\u5bf9\u9f50\uff0c\u660e\u786e\u5efa\u6a21\u7ec4\u5408\u8868\u793a\u4e0e\u5355\u6a21\u6001\u5bf9\u5e94\u7269\u4e4b\u95f4\u7684\u7ed3\u6784\u5173\u7cfb\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u68c0\u7d22\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u6a21\u6001\u7ec4\u5408\u611f\u77e5\u4f5c\u4e3a\u5229\u7528MLLMs\u4f5c\u4e3a\u7edf\u4e00\u7f16\u7801\u5668\u65f6\u5b9e\u73b0\u9c81\u68d2\u7ec4\u5408\u591a\u6a21\u6001\u68c0\u7d22\u7684\u6709\u6548\u539f\u5219\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u6a21\u6001\u7ec4\u5408\u611f\u77e5\u662f\u63d0\u5347\u7edf\u4e00\u7f16\u7801\u5668\u5728\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u9c81\u68d2\u6027\u7684\u5173\u952e\u673a\u5236\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5206\u5e03\u504f\u79fb\u548c\u6a21\u6001\u4ea4\u4e92\u65b9\u9762\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2510.15547", "categories": ["cs.AI", "cs.ET", "cs.LG", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.15547", "abs": "https://arxiv.org/abs/2510.15547", "authors": ["Usman Ali", "Ali Zia", "Waqas Ali", "Umer Ramzan", "Abdul Rehman", "Muhammad Tayyab Chaudhry", "Wei Xiang"], "title": "Hypergraph Contrastive Sensor Fusion for Multimodal Fault Diagnosis in Induction Motors", "comment": "Submitted to IEEE Sensors Journal", "summary": "Reliable induction motor (IM) fault diagnosis is vital for industrial safety\nand operational continuity, mitigating costly unplanned downtime. Conventional\napproaches often struggle to capture complex multimodal signal relationships,\nare constrained to unimodal data or single fault types, and exhibit performance\ndegradation under noisy or cross-domain conditions. This paper proposes the\nMultimodal Hypergraph Contrastive Attention Network (MM-HCAN), a unified\nframework for robust fault diagnosis. To the best of our knowledge, MM-HCAN is\nthe first to integrate contrastive learning within a hypergraph topology\nspecifically designed for multimodal sensor fusion, enabling the joint\nmodelling of intra- and inter-modal dependencies and enhancing generalisation\nbeyond Euclidean embedding spaces. The model facilitates simultaneous diagnosis\nof bearing, stator, and rotor faults, addressing the engineering need for\nconsolidated di- agnostic capabilities. Evaluated on three real-world\nbenchmarks, MM-HCAN achieves up to 99.82% accuracy with strong cross-domain\ngeneralisation and resilience to noise, demonstrating its suitability for\nreal-world deployment. An ablation study validates the contribution of each\ncomponent. MM-HCAN provides a scalable and robust solution for comprehensive\nmulti-fault diagnosis, supporting predictive maintenance and extended asset\nlongevity in industrial environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8d85\u56fe\u5bf9\u6bd4\u6ce8\u610f\u529b\u7f51\u7edc\uff08MM-HCAN\uff09\uff0c\u8fd9\u662f\u9996\u4e2a\u5c06\u5bf9\u6bd4\u5b66\u4e60\u96c6\u6210\u5230\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u8d85\u56fe\u62d3\u6251\u4e2d\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u9c81\u68d2\u7684\u611f\u5e94\u7535\u673a\u591a\u6545\u969c\u8bca\u65ad\uff0c\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523099.82%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u611f\u5e94\u7535\u673a\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u591a\u6a21\u6001\u4fe1\u53f7\u5173\u7cfb\uff0c\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u6a21\u6001\u6570\u636e\u6216\u5355\u4e00\u6545\u969c\u7c7b\u578b\uff0c\u4e14\u5728\u566a\u58f0\u6216\u8de8\u57df\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5de5\u4e1a\u5b89\u5168\u6027\u548c\u8fd0\u884c\u8fde\u7eed\u6027\u7684\u9700\u6c42\u3002", "method": "MM-HCAN\u6846\u67b6\u5c06\u5bf9\u6bd4\u5b66\u4e60\u96c6\u6210\u5230\u4e13\u95e8\u4e3a\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u8bbe\u8ba1\u7684\u8d85\u56fe\u62d3\u6251\u4e2d\uff0c\u80fd\u591f\u8054\u5408\u5efa\u6a21\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u8d85\u8d8a\u6b27\u51e0\u91cc\u5f97\u5d4c\u5165\u7a7a\u95f4\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u652f\u6301\u540c\u65f6\u8bca\u65ad\u8f74\u627f\u3001\u5b9a\u5b50\u548c\u8f6c\u5b50\u6545\u969c\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMM-HCAN\u5b9e\u73b0\u4e86\u9ad8\u8fbe99.82%\u7684\u51c6\u786e\u7387\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u548c\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u8d21\u732e\u3002", "conclusion": "MM-HCAN\u4e3a\u5168\u9762\u591a\u6545\u969c\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\u8d44\u4ea7\u5bff\u547d\u5ef6\u957f\uff0c\u5c55\u793a\u4e86\u5728\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u548c\u8de8\u57df\u6cdb\u5316\u65b9\u9762\u7684\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.15148", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15148", "abs": "https://arxiv.org/abs/2510.15148", "authors": ["Xingrui Wang", "Jiang Liu", "Chao Huang", "Xiaodong Yu", "Ze Wang", "Ximeng Sun", "Jialian Wu", "Alan Yuille", "Emad Barsoum", "Zicheng Liu"], "title": "XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models", "comment": null, "summary": "Omni-modal large language models (OLLMs) aim to unify audio, vision, and text\nunderstanding within a single framework. While existing benchmarks primarily\nevaluate general cross-modal question-answering ability, it remains unclear\nwhether OLLMs achieve modality-invariant reasoning or exhibit modality-specific\nbiases. We introduce XModBench, a large-scale tri-modal benchmark explicitly\ndesigned to measure cross-modal consistency. XModBench comprises 60,828\nmultiple-choice questions spanning five task families and systematically covers\nall six modality compositions in question-answer pairs, enabling fine-grained\ndiagnosis of an OLLM's modality-invariant reasoning, modality disparity, and\ndirectional imbalance. Experiments show that even the strongest model, Gemini\n2.5 Pro, (i) struggles with spatial and temporal reasoning, achieving less than\n60% accuracy, (ii) reveals persistent modality disparities, with performance\ndropping substantially when the same semantic content is conveyed through audio\nrather than text, and (iii) shows systematic directional imbalance, exhibiting\nlower consistency when vision serves as context compared to text. These\nfindings indicate that current OLLMs remain far from truly modality-invariant\nreasoning and position XModBench as a fundamental diagnostic tool for\nevaluating and improving cross-modal competence. All data and evaluation tools\nwill be available at https://xingruiwang.github.io/projects/XModBench/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86XModBench\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u4e09\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u6a21\u6001\u4e0d\u53d8\u63a8\u7406\u65b9\u9762\u7684\u663e\u8457\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u8de8\u6a21\u6001\u95ee\u7b54\u80fd\u529b\uff0c\u4f46\u65e0\u6cd5\u786e\u5b9a\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u771f\u6b63\u5b9e\u73b0\u4e86\u6a21\u6001\u4e0d\u53d8\u63a8\u7406\u6216\u5b58\u5728\u6a21\u6001\u7279\u5b9a\u504f\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u8bca\u65ad\u5de5\u5177\u6765\u7cfb\u7edf\u8bc4\u4f30\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86\u5305\u542b60,828\u4e2a\u591a\u9009\u9898\u7684XModBench\u57fa\u51c6\uff0c\u6db5\u76d6\u4e94\u4e2a\u4efb\u52a1\u5bb6\u65cf\u5e76\u7cfb\u7edf\u8986\u76d6\u6240\u6709\u516d\u79cd\u6a21\u6001\u7ec4\u5408\u7684\u95ee\u7b54\u5bf9\uff0c\u80fd\u591f\u5bf9\u6a21\u578b\u7684\u6a21\u6001\u4e0d\u53d8\u63a8\u7406\u80fd\u529b\u3001\u6a21\u6001\u5dee\u5f02\u548c\u65b9\u5411\u4e0d\u5e73\u8861\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bca\u65ad\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u6700\u5f3a\u7684Gemini 2.5 Pro\u6a21\u578b\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4f4e\u4e8e60%\uff0c\u5b58\u5728\u6301\u7eed\u7684\u6a21\u6001\u5dee\u5f02\uff08\u97f3\u9891\u6a21\u6001\u6027\u80fd\u663e\u8457\u4f4e\u4e8e\u6587\u672c\uff09\uff0c\u5e76\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u65b9\u5411\u4e0d\u5e73\u8861\uff08\u89c6\u89c9\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u65f6\u4e00\u81f4\u6027\u4f4e\u4e8e\u6587\u672c\uff09\u3002", "conclusion": "\u5f53\u524d\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8ddd\u79bb\u771f\u6b63\u7684\u6a21\u6001\u4e0d\u53d8\u63a8\u7406\u4ecd\u6709\u5f88\u5927\u5dee\u8ddd\uff0cXModBench\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u8de8\u6a21\u6001\u80fd\u529b\u7684\u57fa\u7840\u8bca\u65ad\u5de5\u5177\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2510.15685", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15685", "abs": "https://arxiv.org/abs/2510.15685", "authors": ["Joshua Wolfe Brook", "Ilia Markov"], "title": "Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection", "comment": "8 pages, 9 figures, submitted to LREC 2026", "summary": "This research introduces a novel approach to textual and multimodal Hate\nSpeech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge\nbases to generate background context and incorporate it into the input of HSD\nclassifiers. Two context generation strategies are examined: one focused on\nnamed entities and the other on full-text prompting. Four methods of\nincorporating context into the classifier input are compared: text\nconcatenation, embedding concatenation, a hierarchical transformer-based\nfusion, and LLM-driven text enhancement. Experiments are conducted on the\ntextual Latent Hatred dataset of implicit hate speech and applied in a\nmultimodal setting on the MAMI dataset of misogynous memes. Results suggest\nthat both the contextual information and the method by which it is incorporated\nare key, with gains of up to 3 and 6 F1 points on textual and multimodal setups\nrespectively, from a zero-context baseline to the highest-performing system,\nbased on embedding concatenation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u52a8\u6001\u77e5\u8bc6\u5e93\u751f\u6210\u80cc\u666f\u4e0a\u4e0b\u6587\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u5206\u7c7b\u5668\u8f93\u5165\u4e2d\u3002\u5b9e\u9a8c\u8868\u660e\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u6574\u5408\u65b9\u6cd5\u5bf9\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5728\u6587\u672c\u548c\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad83\u548c6\u4e2aF1\u5206\u6570\u7684\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u9690\u542b\u4ec7\u6068\u8a00\u8bba\u548c\u591a\u6a21\u6001\u5185\u5bb9\u65f6\u9762\u4e34\u6311\u6218\uff0c\u7f3a\u4e4f\u5bf9\u80cc\u666f\u4e0a\u4e0b\u6587\u7684\u6709\u6548\u5229\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528LLMs\u751f\u6210\u76f8\u5173\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u589e\u5f3a\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9700\u8981\u6df1\u5c42\u7406\u89e3\u7684\u9690\u542b\u8868\u8fbe\u548c\u591a\u6a21\u6001\u5185\u5bb9\u65f6\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u4e0a\u4e0b\u6587\u751f\u6210\u7b56\u7565\uff1a\u57fa\u4e8e\u547d\u540d\u5b9e\u4f53\u7684\u751f\u6210\u548c\u57fa\u4e8e\u5168\u6587\u63d0\u793a\u7684\u751f\u6210\u3002\u6bd4\u8f83\u4e86\u56db\u79cd\u4e0a\u4e0b\u6587\u6574\u5408\u65b9\u6cd5\uff1a\u6587\u672c\u62fc\u63a5\u3001\u5d4c\u5165\u62fc\u63a5\u3001\u57fa\u4e8e\u5c42\u6b21\u5316Transformer\u7684\u878d\u5408\u4ee5\u53caLLM\u9a71\u52a8\u7684\u6587\u672c\u589e\u5f3a\u3002\u5b9e\u9a8c\u5728\u6587\u672c\u9690\u542b\u4ec7\u6068\u6570\u636e\u96c6Latent Hatred\u548c\u591a\u6a21\u6001\u538c\u5973\u8868\u60c5\u5305\u6570\u636e\u96c6MAMI\u4e0a\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u6574\u5408\u65b9\u6cd5\u5bf9\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\u3002\u4ece\u96f6\u4e0a\u4e0b\u6587\u57fa\u7ebf\u5230\u6700\u4f73\u6027\u80fd\u7cfb\u7edf\uff0c\u5728\u6587\u672c\u548c\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad83\u548c6\u4e2aF1\u5206\u6570\u7684\u63d0\u5347\u3002\u57fa\u4e8e\u5d4c\u5165\u62fc\u63a5\u7684\u65b9\u6cd5\u5728\u6240\u6709\u6574\u5408\u7b56\u7565\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8bc1\u660e\u4e86\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6709\u6548\u6574\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5229\u7528LLMs\u751f\u6210\u80cc\u666f\u4e0a\u4e0b\u6587\u80fd\u591f\u6709\u6548\u63d0\u5347\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9690\u542b\u8868\u8fbe\u548c\u591a\u6a21\u6001\u5185\u5bb9\u65f6\u3002\u4e0a\u4e0b\u6587\u6574\u5408\u65b9\u6cd5\u7684\u9009\u62e9\u5bf9\u6700\u7ec8\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5d4c\u5165\u62fc\u63a5\u7b56\u7565\u663e\u793a\u51fa\u6700\u4f73\u6548\u679c\u3002\u8fd9\u4e3a\u672a\u6765\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u65b9\u5411\u3002"}}
{"id": "2510.15748", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15748", "abs": "https://arxiv.org/abs/2510.15748", "authors": ["Minlin Zeng", "Zhipeng Zhou", "Yang Qiu", "Zhiqi Shen"], "title": "Towards Relaxed Multimodal Inputs for Gait-based Parkinson's Disease Assessment", "comment": null, "summary": "Parkinson's disease assessment has garnered growing interest in recent years,\nparticularly with the advent of sensor data and machine learning techniques.\nAmong these, multimodal approaches have demonstrated strong performance by\neffectively integrating complementary information from various data sources.\nHowever, two major limitations hinder their practical application: (1) the need\nto synchronize all modalities during training, and (2) the dependence on all\nmodalities during inference. To address these issues, we propose the first\nParkinson's assessment system that formulates multimodal learning as a\nmulti-objective optimization (MOO) problem. This not only allows for more\nflexible modality requirements during both training and inference, but also\nhandles modality collapse issue during multimodal information fusion. In\naddition, to mitigate the imbalance within individual modalities, we introduce\na margin-based class rebalancing strategy to enhance category learning. We\nconduct extensive experiments on three public datasets under both synchronous\nand asynchronous settings. The results show that our framework-Towards Relaxed\nInPuts (TRIP)-achieves state-of-the-art performance, outperforming the best\nbaselines by 16.48, 6.89, and 11.55 percentage points in the asynchronous\nsetting, and by 4.86 and 2.30 percentage points in the synchronous setting,\nhighlighting its effectiveness and adaptability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5c06\u5e15\u91d1\u68ee\u75c5\u8bc4\u4f30\u4e2d\u7684\u591a\u6a21\u6001\u5b66\u4e60\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u7684\u7cfb\u7edfTRIP\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5bf9\u6a21\u6001\u540c\u6b65\u6027\u548c\u5b8c\u6574\u6027\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5e15\u91d1\u68ee\u75c5\u8bc4\u4f30\u4e2d\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u8bad\u7ec3\u65f6\u9700\u8981\u6240\u6709\u6a21\u6001\u540c\u6b65\u53ef\u7528\uff0c\u63a8\u7406\u65f6\u4f9d\u8d56\u6240\u6709\u6a21\u6001\u5b8c\u6574\u5b58\u5728\uff0c\u8fd9\u4e9b\u9650\u5236\u4e25\u91cd\u963b\u788d\u4e86\u5176\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u63d0\u51faTRIP\u6846\u67b6\u5c06\u591a\u6a21\u6001\u5b66\u4e60\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u57fa\u4e8e\u8fb9\u754c\u7684\u7c7b\u522b\u91cd\u5e73\u8861\u7b56\u7565\u6765\u7f13\u89e3\u6a21\u6001\u5185\u90e8\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u540c\u65f6\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u4e2d\u7684\u6a21\u6001\u5d29\u6e83\u95ee\u9898\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTRIP\u5728\u5f02\u6b65\u8bbe\u7f6e\u4e0b\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e8616.48\u30016.89\u548c11.55\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u540c\u6b65\u8bbe\u7f6e\u4e0b\u63d0\u5347\u4e864.86\u548c2.30\u4e2a\u767e\u5206\u70b9\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u5728\u591a\u6a21\u6001\u533b\u7597\u6570\u636e\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5904\u7406\u4e0d\u5b8c\u6574\u6216\u5f02\u6b65\u6a21\u6001\u6570\u636e\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.15162", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15162", "abs": "https://arxiv.org/abs/2510.15162", "authors": ["Weizhi Wang", "Rongmei Lin", "Shiyang Li", "Colin Lockard", "Ritesh Sarkhel", "Sanket Lokegaonkar", "Jingbo Shang", "Xifeng Yan", "Nasser Zalmout", "Xian Li"], "title": "Train a Unified Multimodal Data Quality Classifier with Synthetic Data", "comment": "EMNLP 2025 Findings", "summary": "The Multimodal Large Language Models (MLLMs) are continually pre-trained on a\nmixture of image-text caption data and interleaved document data, while the\nhigh-quality data filtering towards image-text interleaved document data is\nunder-explored. We propose to train an efficient MLLM as a Unified Mulitmodal\nData Quality Classifier to Filter both high-quality image-text caption and\ninterleaved data (UniFilter). To address the challenge of collecting diverse\nlabeled multimodal data, we introduce a semi-synthetic approach that leverages\nreadily available raw images and generates corresponding text across four\nquality levels. This method enables efficient creation of sample-score pairs\nfor both caption and interleaved document data to train UniFilter. We apply\nUniFilter to curate high-quality caption data from DataComp caption dataset and\ninterleaved data from the OBELICS image-text interleaved dataset. MLLMs\npre-trained on the filtered data demonstrate significantly enhanced\ncapabilities compared to those trained on baseline-filtered data, achieving\nstronger zero-shot reasoning and in-context learning capabilities. After visual\nsupervised fine-tuning, these UniFilter-induced MLLMs achieve stronger\nperformance on various benchmarks, highlighting the downstream benefits of\nhigh-quality multimodal pre-training. We release the synthetic training data\nused for training UniFilter, the UniFilter model checkpoints, and the\nhigh-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to\nthe community for reproduction and further development.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faUniFilter\uff0c\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6570\u636e\u8d28\u91cf\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u7b5b\u9009\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf-\u6587\u672c\u63cf\u8ff0\u6570\u636e\u548c\u4ea4\u9519\u6587\u6863\u6570\u636e\u3002\u901a\u8fc7\u5728UniFilter\u7b5b\u9009\u7684\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u663e\u8457\u589e\u5f3a\u7684\u96f6\u6837\u672c\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u63cf\u8ff0\u6570\u636e\u548c\u4ea4\u9519\u6587\u6863\u6570\u636e\u7684\u6df7\u5408\u9884\u8bad\u7ec3\u4e2d\uff0c\u9ad8\u8d28\u91cf\u6570\u636e\u7b5b\u9009\u65b9\u6cd5\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf-\u6587\u672c\u4ea4\u9519\u6587\u6863\u6570\u636e\u7684\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u6570\u636e\u8d28\u91cf\u5206\u7c7b\u5668\uff0c\u91c7\u7528\u534a\u5408\u6210\u65b9\u6cd5\u5229\u7528\u539f\u59cb\u56fe\u50cf\u751f\u6210\u56db\u79cd\u8d28\u91cf\u7ea7\u522b\u7684\u5bf9\u5e94\u6587\u672c\uff0c\u4e3a\u63cf\u8ff0\u548c\u4ea4\u9519\u6587\u6863\u6570\u636e\u521b\u5efa\u6837\u672c-\u8bc4\u5206\u5bf9\u6765\u8bad\u7ec3UniFilter\u6a21\u578b\u3002", "result": "\u5e94\u7528UniFilter\u4eceDataComp\u63cf\u8ff0\u6570\u636e\u96c6\u548cOBELICS\u56fe\u50cf-\u6587\u672c\u4ea4\u9519\u6570\u636e\u96c6\u4e2d\u7b5b\u9009\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u9884\u8bad\u7ec3\u7684MLLM\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5c55\u73b0\u51fa\u663e\u8457\u589e\u5f3a\u7684\u96f6\u6837\u672c\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u7ecf\u8fc7\u89c6\u89c9\u76d1\u7763\u5fae\u8c03\u540e\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6570\u636e\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5177\u6709\u663e\u8457\u63d0\u5347\u4f5c\u7528\uff0c\u901a\u8fc7UniFilter\u7b5b\u9009\u7684\u6570\u636e\u80fd\u591f\u6709\u6548\u589e\u5f3aMLLM\u7684\u80fd\u529b\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u6570\u636e\u7b5b\u9009\u65b9\u6cd5\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u8d44\u6e90\u3002"}}
{"id": "2510.15851", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15851", "abs": "https://arxiv.org/abs/2510.15851", "authors": ["Kadri Hacioglu", "Manjunath K E", "Andreas Stolcke"], "title": "SpeechLLMs for Large-scale Contextualized Zero-shot Slot Filling", "comment": "13 pages, EMNLP 2025", "summary": "Slot filling is a crucial subtask in spoken language understanding (SLU),\ntraditionally implemented as a cascade of speech recognition followed by one or\nmore natural language understanding (NLU) components. The recent advent of\nspeech-based large language models (speechLLMs), which integrate speech and\ntextual foundation models, has opened new avenues for achieving speech\nunderstanding tasks in a more unified, generative, and instruction-following\nmanner while promising data and compute efficiency with zero-shot abilities,\ngeneralizing to unseen slot labels. We address the slot-filling task by\ncreating an empirical upper bound for the task, identifying performance,\nrobustness, and generalization gaps, and proposing improvements to the training\ndata, architecture, and training strategies to narrow the gap with the upper\nbound result. We show that each of these measures improve performance\nsubstantially, while highlighting practical challenges and providing empirical\nguidance and insights for harnessing these emerging models.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5efa\u7acb\u69fd\u586b\u5145\u4efb\u52a1\u7684\u5b9e\u8bc1\u4e0a\u9650\uff0c\u8bc6\u522b\u4e86\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u7684\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u8bad\u7ec3\u6570\u636e\u3001\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u7684\u6539\u8fdb\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u69fd\u586b\u5145\u4efb\u52a1\u91c7\u7528\u8bed\u97f3\u8bc6\u522b\u4e0e\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7ea7\u8054\u67b6\u6784\uff0c\u800c\u65b0\u5174\u7684\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63d0\u4f9b\u4e86\u7edf\u4e00\u751f\u6210\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u5b58\u5728\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u5e76\u7f29\u5c0f\u4e0e\u7406\u8bba\u4e0a\u9650\u7684\u5dee\u8ddd\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5efa\u7acb\u69fd\u586b\u5145\u4efb\u52a1\u7684\u5b9e\u8bc1\u6027\u80fd\u4e0a\u9650\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u3001\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u9488\u5bf9\u6027\u5730\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\uff0c\u5305\u62ec\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u6784\u9020\u3001\u6a21\u578b\u67b6\u6784\u8c03\u6574\u4ee5\u53ca\u8bad\u7ec3\u7b56\u7565\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5404\u9879\u6539\u8fdb\u63aa\u65bd\u5747\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u6709\u6548\u7f29\u5c0f\u4e86\u4e0e\u5b9e\u8bc1\u4e0a\u9650\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u5229\u7528\u8fd9\u4e9b\u65b0\u5174\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63d0\u5347\u4e86\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u69fd\u586b\u5145\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u66f4\u91cd\u8981\u7684\u662f\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u6539\u8fdb\u6846\u67b6\u548c\u5b9e\u8bc1\u6307\u5bfc\uff0c\u4e3a\u672a\u6765\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\u7684\u7edf\u4e00\u751f\u6210\u5f0f\u89e3\u51b3\u65b9\u6848\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2502.08636", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2502.08636", "abs": "https://arxiv.org/abs/2502.08636", "authors": ["Xingrui Wang", "Wufei Ma", "Tiezheng Zhang", "Celso M de Melo", "Jieneng Chen", "Alan Yuille"], "title": "Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models", "comment": "Published in CVPR 2025 as Highlight. Data and code are released at\n  https://github.com/XingruiWang/Spatial457", "summary": "Although large multimodal models (LMMs) have demonstrated remarkable\ncapabilities in visual scene interpretation and reasoning, their capacity for\ncomplex and precise 3-dimensional spatial reasoning remains uncertain. Existing\nbenchmarks focus predominantly on 2D spatial understanding and lack a framework\nto comprehensively evaluate 6D spatial reasoning across varying complexities.\nTo address this limitation, we present Spatial457, a scalable and unbiased\nsynthetic dataset designed with 4 key capability for spatial reasoning:\nmulti-object recognition, 2D location, 3D location, and 3D orientation. We\ndevelop a cascading evaluation structure, constructing 7 question types across\n5 difficulty levels that range from basic single object recognition to our new\nproposed complex 6D spatial reasoning tasks. We evaluated various large\nmultimodal models (LMMs) on PulseCheck457, observing a general decline in\nperformance as task complexity increases, particularly in 3D reasoning and 6D\nspatial tasks. To quantify these challenges, we introduce the Relative\nPerformance Dropping Rate (RPDR), highlighting key weaknesses in 3D reasoning\ncapabilities. Leveraging the unbiased attribute design of our dataset, we also\nuncover prediction biases across different attributes, with similar patterns\nobserved in real-world image settings. The code and data are released in\nhttps://github.com/XingruiWang/Spatial457.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Spatial457\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u65e0\u504f\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u57286D\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u57283D\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u663e\u8457\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9\u573a\u666f\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u590d\u6742\u7cbe\u786e\u76843\u7ef4\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4ecd\u4e0d\u786e\u5b9a\uff0c\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce82D\u7a7a\u95f4\u7406\u89e3\uff0c\u7f3a\u4e4f\u8bc4\u4f306D\u7a7a\u95f4\u63a8\u7406\u7684\u5168\u9762\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86Spatial457\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b4\u4e2a\u5173\u952e\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff1a\u591a\u76ee\u6807\u8bc6\u522b\u30012D\u5b9a\u4f4d\u30013D\u5b9a\u4f4d\u548c3D\u65b9\u5411\uff0c\u91c7\u7528\u7ea7\u8054\u8bc4\u4f30\u7ed3\u6784\u6784\u5efa\u4e867\u79cd\u95ee\u9898\u7c7b\u578b\u548c5\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u4ece\u57fa\u7840\u5355\u76ee\u6807\u8bc6\u522b\u5230\u65b0\u63d0\u51fa\u7684\u590d\u67426D\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728PulseCheck457\u4e0a\u6027\u80fd\u968f\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u800c\u666e\u904d\u4e0b\u964d\uff0c\u7279\u522b\u662f\u57283D\u63a8\u7406\u548c6D\u7a7a\u95f4\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5f15\u5165\u76f8\u5bf9\u6027\u80fd\u4e0b\u964d\u7387\u91cf\u5316\u4e863D\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u5f31\u70b9\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u5c5e\u6027\u95f4\u7684\u9884\u6d4b\u504f\u5dee\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u57283D\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u663e\u8457\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u53d1\u73b0\u7684\u9884\u6d4b\u504f\u5dee\u6a21\u5f0f\u5728\u771f\u5b9e\u56fe\u50cf\u8bbe\u7f6e\u4e2d\u540c\u6837\u5b58\u5728\uff0c\u5f3a\u8c03\u4e86\u6539\u8fdb3D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.15164", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15164", "abs": "https://arxiv.org/abs/2510.15164", "authors": ["Usman Afzaal", "Ziyu Su", "Usama Sajjad", "Hao Lu", "Mostafa Rezapour", "Metin Nafi Gurcan", "Muhammad Khalid Khan Niazi"], "title": "Hyperparameter Optimization and Reproducibility in Deep Learning Model Training", "comment": null, "summary": "Reproducibility remains a critical challenge in foundation model training for\nhistopathology, often hindered by software randomness, hardware\nnon-determinism, and inconsistent hyperparameter reporting. To investigate\nthese issues, we trained a CLIP model on the QUILT-1M dataset and\nsystematically evaluated the impact of different hyperparameter settings and\naugmentation strategies across three downstream histopathology datasets\n(PatchCamelyon, LC25000-Lung, and LC25000-Colon). Despite variability across\nruns, we identified clear trends: RandomResizedCrop values of 0.7-0.8\noutperformed more aggressive (0.6) or conservative (0.9) settings, distributed\ntraining without local loss improved stability, and learning rates below 5.0e-5\nconsistently degraded performance across all datasets. The LC25000 (Colon)\ndataset consistently provided the most reproducible benchmark. These findings\nhighlight that reproducibility in computational pathology depends not only on\ntransparent documentation but also on carefully chosen experimental\nconfigurations, and we provide practical rules to guide future efforts in\ndeveloping reproducible foundation models for digital pathology.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u7ec4\u7ec7\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u53ef\u590d\u73b0\u6027\u6311\u6218\uff0c\u901a\u8fc7CLIP\u6a21\u578b\u5728QUILT-1M\u6570\u636e\u96c6\u4e0a\u7684\u8bad\u7ec3\u5b9e\u9a8c\uff0c\u786e\u5b9a\u4e86\u5173\u952e\u8d85\u53c2\u6570\u548c\u589e\u5f3a\u7b56\u7565\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u6570\u5b57\u75c5\u7406\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u53ef\u590d\u73b0\u6027\u6307\u5bfc\u539f\u5219\u3002", "motivation": "\u7ec4\u7ec7\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u9762\u4e34\u4e25\u91cd\u7684\u53ef\u590d\u73b0\u6027\u95ee\u9898\uff0c\u4e3b\u8981\u969c\u788d\u5305\u62ec\u8f6f\u4ef6\u968f\u673a\u6027\u3001\u786c\u4ef6\u975e\u786e\u5b9a\u6027\u548c\u8d85\u53c2\u6570\u62a5\u544a\u4e0d\u4e00\u81f4\uff0c\u8fd9\u963b\u788d\u4e86\u7814\u7a76\u7ed3\u679c\u7684\u53ef\u9760\u6bd4\u8f83\u548c\u9a8c\u8bc1\u3002", "method": "\u7814\u7a76\u5728QUILT-1M\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3CLIP\u6a21\u578b\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u8d85\u53c2\u6570\u8bbe\u7f6e\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u5728\u4e09\u4e2a\u4e0b\u6e38\u7ec4\u7ec7\u75c5\u7406\u5b66\u6570\u636e\u96c6\uff08PatchCamelyon\u3001LC25000-Lung\u548cLC25000-Colon\uff09\u4e0a\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0RandomResizedCrop\u503c\u57280.7-0.8\u8303\u56f4\u5185\u8868\u73b0\u6700\u4f73\uff0c\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e0d\u4f7f\u7528\u5c40\u90e8\u635f\u5931\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\uff0c\u5b66\u4e60\u7387\u4f4e\u4e8e5.0e-5\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u5176\u4e2dLC25000\uff08Colon\uff09\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u6700\u53ef\u9760\u7684\u53ef\u590d\u73b0\u6027\u57fa\u51c6\u3002", "conclusion": "\u8ba1\u7b97\u75c5\u7406\u5b66\u7684\u53ef\u590d\u73b0\u6027\u4e0d\u4ec5\u4f9d\u8d56\u4e8e\u900f\u660e\u6587\u6863\u8bb0\u5f55\uff0c\u66f4\u9700\u8981\u7cbe\u5fc3\u9009\u62e9\u7684\u5b9e\u9a8c\u914d\u7f6e\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u53ef\u590d\u73b0\u7684\u6570\u5b57\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6307\u5bfc\u539f\u5219\u548c\u914d\u7f6e\u5efa\u8bae\u3002"}}
{"id": "2502.17092", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2502.17092", "abs": "https://arxiv.org/abs/2502.17092", "authors": ["Syed Abdul Gaffar Shakhadri", "Kruthika KR", "Kartik Basavaraj Angadi"], "title": "Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI", "comment": null, "summary": "We introduce Shakti VLM, a family of vision-language models in the capacity\nof 1B and 4B parameters designed to address data efficiency challenges in\nmultimodal learning. While recent VLMs achieve strong performance through\nextensive training data, Shakti models leverage architectural innovations to\nattain competitive results with fewer tokens. Key advancements include\nQK-Normalization for attention stability, hybrid normalization techniques, and\nenhanced positional encoding. A three-stage training strategy further optimizes\nlearning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and\nShakti-VLM-4B excel in document understanding, Visual Reasoning, OCR\nextraction, and general multimodal reasoning. Our results highlight that high\nperformance can be achieved through model design and training strategy rather\nthan sheer data volume, making Shakti an efficient solution for\nenterprise-scale multimodal tasks.", "AI": {"tldr": "Shakti VLM\u662f\u4e00\u4e2a\u53c2\u6570\u89c4\u6a21\u4e3a1B\u548c4B\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u548c\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7ade\u4e89\u529b\u7684\u591a\u6a21\u6001\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u4f01\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u6548\u7387\u6311\u6218\uff0c\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u6d77\u91cf\u8bad\u7ec3\u6570\u636e\u6765\u83b7\u5f97\u5f3a\u6027\u80fd\uff0c\u800cShakti\u6a21\u578b\u5219\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6a21\u578b\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\u6765\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5b66\u4e60\u3002", "method": "Shakti VLM\u5f15\u5165\u4e86QK-Normalization\u6765\u589e\u5f3a\u6ce8\u610f\u529b\u7a33\u5b9a\u6027\uff0c\u91c7\u7528\u6df7\u5408\u5f52\u4e00\u5316\u6280\u672f\uff0c\u6539\u8fdb\u4e86\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u5e76\u5b9e\u65bd\u4e86\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u6765\u4f18\u5316\u5b66\u4e60\u6548\u7387\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793aShakti-VLM-1B\u548cShakti-VLM-4B\u5728\u6587\u6863\u7406\u89e3\u3001\u89c6\u89c9\u63a8\u7406\u3001OCR\u63d0\u53d6\u548c\u901a\u7528\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5728\u51cf\u5c11\u8bad\u7ec3token\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u7ade\u4e89\u529b\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u9ad8\u6027\u80fd\u53ef\u4ee5\u901a\u8fc7\u6a21\u578b\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\u800c\u975e\u5355\u7eaf\u4f9d\u8d56\u6570\u636e\u91cf\u6765\u5b9e\u73b0\uff0cShakti\u6a21\u578b\u4e3a\u4f01\u4e1a\u7ea7\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86\u67b6\u6784\u521b\u65b0\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.15208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15208", "abs": "https://arxiv.org/abs/2510.15208", "authors": ["Daniela Vega", "Hannah V. Ceballos", "Javier S. Vera", "Santiago Rodriguez", "Alejandra Perez", "Angela Castillo", "Maria Escobar", "Dario Londo\u00f1o", "Luis A. Sarmiento", "Camila I. Castro", "Nadiezhda Rodriguez", "Juan C. Brice\u00f1o", "Pablo Arbel\u00e1ez"], "title": "CARDIUM: Congenital Anomaly Recognition with Diagnostic Images and Unified Medical records", "comment": "Accepted to CVAMD Workshop, ICCV 2025", "summary": "Prenatal diagnosis of Congenital Heart Diseases (CHDs) holds great potential\nfor Artificial Intelligence (AI)-driven solutions. However, collecting\nhigh-quality diagnostic data remains difficult due to the rarity of these\nconditions, resulting in imbalanced and low-quality datasets that hinder model\nperformance. Moreover, no public efforts have been made to integrate multiple\nsources of information, such as imaging and clinical data, further limiting the\nability of AI models to support and enhance clinical decision-making. To\novercome these challenges, we introduce the Congenital Anomaly Recognition with\nDiagnostic Images and Unified Medical records (CARDIUM) dataset, the first\npublicly available multimodal dataset consolidating fetal ultrasound and\nechocardiographic images along with maternal clinical records for prenatal CHD\ndetection. Furthermore, we propose a robust multimodal transformer architecture\nthat incorporates a cross-attention mechanism to fuse feature representations\nfrom image and tabular data, improving CHD detection by 11% and 50% over image\nand tabular single-modality approaches, respectively, and achieving an F1 score\nof 79.8 $\\pm$ 4.8% in the CARDIUM dataset. We will publicly release our dataset\nand code to encourage further research on this unexplored field. Our dataset\nand code are available at https://github.com/BCVUniandes/Cardium, and at the\nproject website https://bcv-uniandes.github.io/CardiumPage/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u516c\u5f00\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6CARDIUM\uff0c\u7528\u4e8e\u80ce\u513f\u5148\u5929\u6027\u5fc3\u810f\u75c5\u4ea7\u524d\u8bca\u65ad\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u6a21\u6001transformer\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u5148\u5929\u6027\u5fc3\u810f\u75c5\u4ea7\u524d\u8bca\u65ad\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u8d28\u91cf\u4f4e\u4e0b\u4ee5\u53ca\u591a\u6e90\u4fe1\u606f\u6574\u5408\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u73b0\u6709AI\u6a21\u578b\u56e0\u6570\u636e\u4e0d\u5e73\u8861\u548c\u5355\u6a21\u6001\u9650\u5236\u800c\u6027\u80fd\u53d7\u9650\uff0c\u7f3a\u4e4f\u516c\u5f00\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u963b\u788d\u4e86\u76f8\u5173\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86CARDIUM\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6574\u5408\u80ce\u513f\u8d85\u58f0\u548c\u8d85\u58f0\u5fc3\u52a8\u56fe\u56fe\u50cf\u4ee5\u53ca\u6bcd\u4f53\u4e34\u5e8a\u8bb0\u5f55\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u6a21\u6001transformer\u67b6\u6784\uff0c\u7528\u4e8e\u878d\u5408\u56fe\u50cf\u548c\u8868\u683c\u6570\u636e\u7684\u7279\u5f81\u8868\u793a\u3002", "result": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u5728CARDIUM\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8679.8\u00b14.8%\u7684F1\u5206\u6570\uff0c\u76f8\u6bd4\u5355\u6a21\u6001\u56fe\u50cf\u548c\u8868\u683c\u65b9\u6cd5\u5206\u522b\u63d0\u5347\u4e8611%\u548c50%\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5355\u6a21\u6001\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u9996\u4e2a\u516c\u5f00\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u4e3a\u4ea7\u524d\u5148\u5929\u6027\u5fc3\u810f\u75c5\u8bca\u65ad\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u7684\u516c\u5f00\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u8fdb\u4e00\u6b65\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2510.15240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15240", "abs": "https://arxiv.org/abs/2510.15240", "authors": ["Aysan Aghazadeh", "Adriana Kovashka"], "title": "The Face of Persuasion: Analyzing Bias and Generating Culture-Aware Ads", "comment": null, "summary": "Text-to-image models are appealing for customizing visual advertisements and\ntargeting specific populations. We investigate this potential by examining the\ndemographic bias within ads for different ad topics, and the disparate level of\npersuasiveness (judged by models) of ads that are identical except for\ngender/race of the people portrayed. We also experiment with a technique to\ntarget ads for specific countries. The code is available at\nhttps://github.com/aysanaghazadeh/FaceOfPersuasion", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u5e7f\u544a\u5b9a\u5236\u4e2d\u7684\u6f5c\u5728\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u5e7f\u544a\u4e3b\u9898\u4e0b\u7684\u4eba\u53e3\u504f\u89c1\u4ee5\u53ca\u76f8\u540c\u5e7f\u544a\u5185\u5bb9\u4e2d\u4eba\u7269\u6027\u522b/\u79cd\u65cf\u5bf9\u8bf4\u670d\u529b\u7684\u5dee\u5f02\u6027\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u4e86\u9488\u5bf9\u7279\u5b9a\u56fd\u5bb6\u7684\u5e7f\u544a\u5b9a\u5411\u6280\u672f\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u5b9a\u5236\u89c6\u89c9\u5e7f\u544a\u548c\u9488\u5bf9\u7279\u5b9a\u4eba\u7fa4\u65b9\u9762\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5b58\u5728\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8fd9\u79cd\u504f\u89c1\u5728\u5e7f\u544a\u4e2d\u7684\u8868\u73b0\u53ca\u5176\u5bf9\u8bf4\u670d\u529b\u7684\u5dee\u5f02\u6027\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5b9e\u9a8c\u65b9\u6cd5\u5206\u6790\u4e0d\u540c\u5e7f\u544a\u4e3b\u9898\u4e0b\u7684\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\uff0c\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u6bd4\u8f83\u4ec5\u4eba\u7269\u6027\u522b/\u79cd\u65cf\u4e0d\u540c\u7684\u76f8\u540c\u5e7f\u544a\u7684\u8bf4\u670d\u529b\u5dee\u5f02\uff0c\u5e76\u5f00\u53d1\u4e86\u9488\u5bf9\u7279\u5b9a\u56fd\u5bb6\u7684\u5e7f\u544a\u5b9a\u5411\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u7684\u5e7f\u544a\u5b58\u5728\u663e\u8457\u7684\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\uff0c\u4e0d\u540c\u5e7f\u544a\u4e3b\u9898\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u504f\u89c1\u6a21\u5f0f\uff0c\u4e14\u76f8\u540c\u5e7f\u544a\u5185\u5bb9\u4e2d\u4eba\u7269\u6027\u522b/\u79cd\u65cf\u7684\u6539\u53d8\u4f1a\u5bfc\u81f4\u6a21\u578b\u5224\u65ad\u7684\u8bf4\u670d\u529b\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u5e7f\u544a\u5e94\u7528\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u4e0d\u516c\u5e73\u7684\u5e7f\u544a\u6295\u653e\u6548\u679c\uff0c\u7814\u7a76\u5f3a\u8c03\u4e86\u5728AI\u9a71\u52a8\u7684\u5e7f\u544a\u7cfb\u7edf\u4e2d\u8003\u8651\u516c\u5e73\u6027\u548c\u504f\u89c1\u7f13\u89e3\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.15264", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15264", "abs": "https://arxiv.org/abs/2510.15264", "authors": ["Weijie Wang", "Jiagang Zhu", "Zeyu Zhang", "Xiaofeng Wang", "Zheng Zhu", "Guosheng Zhao", "Chaojun Ni", "Haoxiao Wang", "Guan Huang", "Xinze Chen", "Yukun Zhou", "Wenkang Qin", "Duochao Shi", "Haoyun Li", "Guanghong Jia", "Jiwen Lu"], "title": "DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion", "comment": "Accepted by NeurIPS Workshop on Next Practices in Video Generation\n  and Evaluation (Short Paper Track)", "summary": "We present DriveGen3D, a novel framework for generating high-quality and\nhighly controllable dynamic 3D driving scenes that addresses critical\nlimitations in existing methodologies. Current approaches to driving scene\nsynthesis either suffer from prohibitive computational demands for extended\ntemporal generation, focus exclusively on prolonged video synthesis without 3D\nrepresentation, or restrict themselves to static single-scene reconstruction.\nOur work bridges this methodological gap by integrating accelerated long-term\nvideo generation with large-scale dynamic scene reconstruction through\nmultimodal conditional control. DriveGen3D introduces a unified pipeline\nconsisting of two specialized components: FastDrive-DiT, an efficient video\ndiffusion transformer for high-resolution, temporally coherent video synthesis\nunder text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a\nfeed-forward reconstruction module that rapidly builds 3D Gaussian\nrepresentations across time, ensuring spatial-temporal consistency. Together,\nthese components enable real-time generation of extended driving videos (up to\n$424\\times800$ at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM\nof 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining\nparameter efficiency.", "AI": {"tldr": "DriveGen3D\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u52a0\u901f\u957f\u65f6\u89c6\u9891\u751f\u6210\u548c\u5927\u89c4\u6a21\u52a8\u6001\u573a\u666f\u91cd\u5efa\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u53ef\u63a7\u6027\u7684\u52a8\u60013D\u9a7e\u9a76\u573a\u666f\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c3D\u8868\u793a\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u9a7e\u9a76\u573a\u666f\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff1a\u8981\u4e48\u56e0\u8ba1\u7b97\u9700\u6c42\u8fc7\u9ad8\u800c\u65e0\u6cd5\u8fdb\u884c\u957f\u65f6\u95f4\u751f\u6210\uff0c\u8981\u4e48\u4ec5\u5173\u6ce8\u957f\u65f6\u95f4\u89c6\u9891\u5408\u6210\u800c\u7f3a\u4e4f3D\u8868\u793a\u80fd\u529b\uff0c\u6216\u8005\u5c40\u9650\u4e8e\u9759\u6001\u5355\u573a\u666f\u91cd\u5efa\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u65b9\u6cd5\u5b66\u5dee\u8ddd\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6761\u4ef6\u63a7\u5236\u5b9e\u73b0\u52a0\u901f\u957f\u65f6\u89c6\u9891\u751f\u6210\u4e0e\u5927\u89c4\u6a21\u52a8\u6001\u573a\u666f\u91cd\u5efa\u7684\u96c6\u6210\u3002", "method": "DriveGen3D\u91c7\u7528\u7edf\u4e00\u6d41\u6c34\u7ebf\uff0c\u5305\u542b\u4e24\u4e2a\u4e13\u95e8\u7ec4\u4ef6\uff1aFastDrive-DiT\u2014\u2014\u57fa\u4e8e\u9ad8\u6548\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u7684\u9ad8\u5206\u8fa8\u7387\u3001\u65f6\u95f4\u4e00\u81f4\u89c6\u9891\u5408\u6210\u6a21\u5757\uff0c\u652f\u6301\u6587\u672c\u548c\u9e1f\u77b0\u56fe\u5e03\u5c40\u5f15\u5bfc\uff1bFastRecon3D\u2014\u2014\u524d\u9988\u91cd\u5efa\u6a21\u5757\uff0c\u5feb\u901f\u6784\u5efa\u8de8\u65f6\u95f4\u76843D\u9ad8\u65af\u8868\u793a\uff0c\u786e\u4fdd\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u751f\u6210\u957f\u65f6\u9a7e\u9a76\u89c6\u9891\uff08\u6700\u9ad8424\u00d7800\u5206\u8fa8\u7387\uff0c12 FPS\uff09\u53ca\u5bf9\u5e94\u7684\u52a8\u60013D\u573a\u666f\uff0c\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u4e0a\u8fbe\u5230SSIM 0.811\u548cPSNR 22.84\u7684\u6027\u80fd\u6307\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53c2\u6570\u6548\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u4e13\u95e8\u5316\u7ec4\u4ef6\u96c6\u6210\u52a0\u901f\u89c6\u9891\u751f\u6210\u4e0e\u52a8\u60013D\u91cd\u5efa\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u548c\u865a\u62df\u73af\u5883\u521b\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u591a\u6a21\u6001\u6761\u4ef6\u63a7\u5236\u5728\u590d\u6742\u573a\u666f\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u52a8\u6001\u573a\u666f\u5408\u6210\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.15430", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15430", "abs": "https://arxiv.org/abs/2510.15430", "authors": ["Shuang Liang", "Zhihao Xu", "Jialing Tao", "Hui Xue", "Xiting Wang"], "title": "Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models", "comment": null, "summary": "Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)\nremain vulnerable to jailbreak attacks, posing serious safety risks. To address\nthis, existing detection methods either learn attack-specific parameters, which\nhinders generalization to unseen attacks, or rely on heuristically sound\nprinciples, which limit accuracy and efficiency. To overcome these limitations,\nwe propose Learning to Detect (LoD), a general framework that accurately\ndetects unknown jailbreak attacks by shifting the focus from attack-specific\nlearning to task-specific learning. This framework includes a Multi-modal\nSafety Concept Activation Vector module for safety-oriented representation\nlearning and a Safety Pattern Auto-Encoder module for unsupervised attack\nclassification. Extensive experiments show that our method achieves\nconsistently higher detection AUROC on diverse unknown attacks while improving\nefficiency. The code is available at\nhttps://anonymous.4open.science/r/Learning-to-Detect-51CB.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5b66\u4e60\u68c0\u6d4b\uff08LoD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u91cd\u70b9\u4ece\u653b\u51fb\u7279\u5b9a\u5b66\u4e60\u8f6c\u5411\u4efb\u52a1\u7279\u5b9a\u5b66\u4e60\uff0c\u51c6\u786e\u68c0\u6d4b\u672a\u77e5\u7684\u8d8a\u72f1\u653b\u51fb\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u591a\u6a21\u6001\u5b89\u5168\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\u6a21\u5757\u7528\u4e8e\u5b89\u5168\u5bfc\u5411\u8868\u793a\u5b66\u4e60\uff0c\u4ee5\u53ca\u5b89\u5168\u6a21\u5f0f\u81ea\u7f16\u7801\u5668\u6a21\u5757\u7528\u4e8e\u65e0\u76d1\u7763\u653b\u51fb\u5206\u7c7b\u3002", "motivation": "\u5c3d\u7ba1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5bf9\u9f50\u5de5\u4f5c\uff0c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\uff0c\u6784\u6210\u4e25\u91cd\u7684\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u8981\u4e48\u5b66\u4e60\u653b\u51fb\u7279\u5b9a\u53c2\u6570\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u5230\u672a\u89c1\u653b\u51fb\u7684\u80fd\u529b\uff0c\u8981\u4e48\u4f9d\u8d56\u542f\u53d1\u5f0f\u539f\u5219\uff0c\u9650\u5236\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5b66\u4e60\u68c0\u6d4b\uff08LoD\uff09\u901a\u7528\u6846\u67b6\uff0c\u5305\u62ec\u591a\u6a21\u6001\u5b89\u5168\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\u6a21\u5757\u7528\u4e8e\u5b89\u5168\u5bfc\u5411\u8868\u793a\u5b66\u4e60\uff0c\u4ee5\u53ca\u5b89\u5168\u6a21\u5f0f\u81ea\u7f16\u7801\u5668\u6a21\u5757\u7528\u4e8e\u65e0\u76d1\u7763\u653b\u51fb\u5206\u7c7b\uff0c\u5b9e\u73b0\u4ece\u653b\u51fb\u7279\u5b9a\u5b66\u4e60\u5230\u4efb\u52a1\u7279\u5b9a\u5b66\u4e60\u7684\u8f6c\u53d8\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u672a\u77e5\u653b\u51fb\u4e0a\u5b9e\u73b0\u4e86\u6301\u7eed\u66f4\u9ad8\u7684\u68c0\u6d4bAUROC\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6548\u7387\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u4efb\u52a1\u7279\u5b9a\u5b66\u4e60\u5728\u8d8a\u72f1\u653b\u51fb\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u5b89\u5168\u5a01\u80c1\u68c0\u6d4b\u573a\u666f\u3002"}}
{"id": "2510.15870", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15870", "abs": "https://arxiv.org/abs/2510.15870", "authors": ["Hanrong Ye", "Chao-Han Huck Yang", "Arushi Goel", "Wei Huang", "Ligeng Zhu", "Yuanhang Su", "Sean Lin", "An-Chieh Cheng", "Zhen Wan", "Jinchuan Tian", "Yuming Lou", "Dong Yang", "Zhijian Liu", "Yukang Chen", "Ambrish Dantrey", "Ehsan Jahangiri", "Sreyan Ghosh", "Daguang Xu", "Ehsan Hosseini-Asl", "Danial Mohseni Taheri", "Vidya Murali", "Sifei Liu", "Jason Lu", "Oluwatobi Olabiyi", "Frank Wang", "Rafael Valle", "Bryan Catanzaro", "Andrew Tao", "Song Han", "Jan Kautz", "Hongxu Yin", "Pavlo Molchanov"], "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM", "comment": "Technical Report. Code: https://github.com/NVlabs/OmniVinci", "summary": "Advancing machine intelligence requires developing the ability to perceive\nacross multiple modalities, much as humans sense the world. We introduce\nOmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We\ncarefully study the design choices across model architecture and data curation.\nFor model architecture, we present three key innovations: (i) OmniAlignNet for\nstrengthening alignment between vision and audio embeddings in a shared\nomni-modal latent space; (ii) Temporal Embedding Grouping for capturing\nrelative temporal alignment between vision and audio signals; and (iii)\nConstrained Rotary Time Embedding for encoding absolute temporal information in\nomni-modal embeddings. We introduce a curation and synthesis pipeline that\ngenerates 24M single-modal and omni-modal conversations. We find that\nmodalities reinforce one another in both perception and reasoning. Our model,\nOmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal\nunderstanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while\nusing just 0.2T training tokens - a 6 times reduction compared to\nQwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream\napplications spanning robotics, medical AI, and smart factory.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86OmniVinci\uff0c\u4e00\u4e2a\u5f3a\u5927\u7684\u5f00\u6e90\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u6570\u636e\u7b56\u5c55\u65b9\u6cd5\uff0c\u5728\u8de8\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff0c\u540c\u65f6\u5c06\u8bad\u7ec3\u6570\u636e\u91cf\u51cf\u5c11\u4e866\u500d\u3002", "motivation": "\u63a8\u8fdb\u673a\u5668\u667a\u80fd\u9700\u8981\u53d1\u5c55\u8de8\u591a\u6a21\u6001\u7684\u611f\u77e5\u80fd\u529b\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u5bf9\u4e16\u754c\u7684\u591a\u611f\u5b98\u611f\u77e5\uff0c\u5f53\u524d\u7814\u7a76\u65e8\u5728\u6784\u5efa\u5f3a\u5927\u7684\u5f00\u6e90\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6765\u89e3\u51b3\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u7406\u89e3\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u67b6\u6784\u521b\u65b0\uff1aOmniAlignNet\u7528\u4e8e\u589e\u5f3a\u89c6\u89c9\u548c\u97f3\u9891\u5d4c\u5165\u5728\u5168\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5bf9\u9f50\uff1b\u65f6\u95f4\u5d4c\u5165\u5206\u7ec4\u7528\u4e8e\u6355\u6349\u89c6\u89c9\u548c\u97f3\u9891\u4fe1\u53f7\u95f4\u7684\u76f8\u5bf9\u65f6\u95f4\u5bf9\u9f50\uff1b\u7ea6\u675f\u65cb\u8f6c\u65f6\u95f4\u5d4c\u5165\u7528\u4e8e\u5728\u5168\u6a21\u6001\u5d4c\u5165\u4e2d\u7f16\u7801\u7edd\u5bf9\u65f6\u95f4\u4fe1\u606f\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u751f\u62102400\u4e07\u5355\u6a21\u6001\u548c\u5168\u6a21\u6001\u5bf9\u8bdd\u7684\u6570\u636e\u7b56\u5c55\u4e0e\u5408\u6210\u6d41\u7a0b\u3002", "result": "OmniVinci\u5728\u8de8\u6a21\u6001\u7406\u89e3\u4efb\u52a1DailyOmni\u4e0a\u8d85\u8d8aQwen2.5-Omni\u8fbe19.05\u5206\uff0c\u5728\u97f3\u9891\u4efb\u52a1MMAR\u4e0a\u63d0\u53471.7\u5206\uff0c\u5728\u89c6\u89c9\u4efb\u52a1Video-MME\u4e0a\u63d0\u53473.9\u5206\uff0c\u4ec5\u4f7f\u75280.2T\u8bad\u7ec3token\uff0c\u76f8\u6bd4Qwen2.5-Omni\u76841.2T\u51cf\u5c11\u4e866\u500d\u8bad\u7ec3\u6570\u636e\u91cf\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6a21\u6001\u5728\u611f\u77e5\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u76f8\u4e92\u589e\u5f3a\uff0c\u8bc1\u660e\u4e86\u5168\u6a21\u6001\u6a21\u578b\u5728\u673a\u5668\u4eba\u3001\u533b\u7597AI\u548c\u667a\u80fd\u5de5\u5382\u7b49\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u591a\u6a21\u6001\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u6d1e\u89c1\u3002"}}
{"id": "2510.15434", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15434", "abs": "https://arxiv.org/abs/2510.15434", "authors": ["Huan Chen", "Ting Han", "Siyu Chen", "Zhihao Guo", "Yiping Chen", "Meiliu Wu"], "title": "Semantic4Safety: Causal Insights from Zero-shot Street View Imagery Segmentation for Urban Road Safety", "comment": "11 pages, 10 figures, The 8th ACM SIGSPATIAL International Workshop\n  on AI for Geographic Knowledge Discovery (GeoAI '25), November 3--6, 2025,\n  Minneapolis, MN, USA", "summary": "Street-view imagery (SVI) offers a fine-grained lens on traffic risk, yet two\nfundamental challenges persist: (1) how to construct street-level indicators\nthat capture accident-related features, and (2) how to quantify their causal\nimpacts across different accident types. To address these challenges, we\npropose Semantic4Safety, a framework that applies zero-shot semantic\nsegmentation to SVIs to derive 11 interpretable streetscape indicators, and\nintegrates road type as contextual information to analyze approximately 30,000\naccident records in Austin. Specifically, we train an eXtreme Gradient Boosting\n(XGBoost) multi-class classifier and use Shapley Additive Explanations (SHAP)\nto interpret both global and local feature contributions, and then apply\nGeneralized Propensity Score (GPS) weighting and Average Treatment Effect (ATE)\nestimation to control confounding and quantify causal effects. Results uncover\nheterogeneous, accident-type-specific causal patterns: features capturing scene\ncomplexity, exposure, and roadway geometry dominate predictive power; larger\ndrivable area and emergency space reduce risk, whereas excessive visual\nopenness can increase it. By bridging predictive modeling with causal\ninference, Semantic4Safety supports targeted interventions and high-risk\ncorridor diagnosis, offering a scalable, data-informed tool for urban road\nsafety planning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Semantic4Safety\u6846\u67b6\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u4ece\u8857\u666f\u56fe\u50cf\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u8857\u9053\u6307\u6807\uff0c\u5e76\u7ed3\u5408\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u91cf\u5316\u4e0d\u540c\u4e8b\u6545\u7c7b\u578b\u7684\u56e0\u679c\u6548\u5e94\uff0c\u4e3a\u57ce\u5e02\u9053\u8def\u5b89\u5168\u89c4\u5212\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u5de5\u5177\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8857\u666f\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u5982\u4f55\u6784\u5efa\u80fd\u591f\u6355\u6349\u4e8b\u6545\u76f8\u5173\u7279\u5f81\u7684\u8857\u9053\u7ea7\u6307\u6807\uff0c\u4ee5\u53ca\u5982\u4f55\u91cf\u5316\u8fd9\u4e9b\u6307\u6807\u5bf9\u4e0d\u540c\u4e8b\u6545\u7c7b\u578b\u7684\u56e0\u679c\u5f71\u54cd\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u8857\u666f\u6570\u636e\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u5b89\u5168\u6307\u6807\u548c\u5efa\u7acb\u56e0\u679c\u8054\u7cfb\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u63d0\u51faSemantic4Safety\u6846\u67b6\uff0c\u5e94\u7528\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u4ece\u8857\u666f\u56fe\u50cf\u4e2d\u63d0\u53d611\u4e2a\u53ef\u89e3\u91ca\u7684\u8857\u9053\u666f\u89c2\u6307\u6807\uff0c\u5e76\u6574\u5408\u9053\u8def\u7c7b\u578b\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u91c7\u7528XGBoost\u591a\u5206\u7c7b\u5668\u7ed3\u5408SHAP\u8fdb\u884c\u7279\u5f81\u8d21\u732e\u5206\u6790\uff0c\u4f7f\u7528\u5e7f\u4e49\u503e\u5411\u5f97\u5206\u52a0\u6743\u548c\u5e73\u5747\u5904\u7406\u6548\u5e94\u4f30\u8ba1\u6765\u63a7\u5236\u6df7\u6742\u56e0\u7d20\u5e76\u91cf\u5316\u56e0\u679c\u6548\u5e94\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f02\u8d28\u6027\u7684\u3001\u4e8b\u6545\u7c7b\u578b\u7279\u5b9a\u7684\u56e0\u679c\u6a21\u5f0f\uff1a\u6355\u6349\u573a\u666f\u590d\u6742\u6027\u3001\u66b4\u9732\u5ea6\u548c\u9053\u8def\u51e0\u4f55\u7684\u7279\u5f81\u4e3b\u5bfc\u9884\u6d4b\u80fd\u529b\uff1b\u66f4\u5927\u7684\u53ef\u9a7e\u9a76\u533a\u57df\u548c\u5e94\u6025\u7a7a\u95f4\u964d\u4f4e\u98ce\u9669\uff0c\u800c\u8fc7\u5ea6\u89c6\u89c9\u5f00\u653e\u6027\u53ef\u80fd\u589e\u52a0\u98ce\u9669\u3002\u57fa\u4e8e\u7ea630,000\u6761\u5965\u65af\u6c40\u4e8b\u6545\u8bb0\u5f55\u7684\u5206\u6790\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9884\u6d4b\u5efa\u6a21\u4e0e\u56e0\u679c\u63a8\u65ad\u76f8\u7ed3\u5408\uff0cSemantic4Safety\u652f\u6301\u9488\u5bf9\u6027\u5e72\u9884\u548c\u9ad8\u98ce\u9669\u8d70\u5eca\u8bca\u65ad\uff0c\u4e3a\u57ce\u5e02\u9053\u8def\u5b89\u5168\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u9a71\u52a8\u5de5\u5177\u3002\u8be5\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u4e0d\u540c\u7c7b\u578b\u4e8b\u6545\u7684\u5173\u952e\u5f71\u54cd\u56e0\u7d20\uff0c\u4e3a\u7cbe\u51c6\u5b89\u5168\u5e72\u9884\u63d0\u4f9b\u79d1\u5b66\u4f9d\u636e\u3002"}}
{"id": "2510.15470", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.15470", "abs": "https://arxiv.org/abs/2510.15470", "authors": ["Jinghao Huang", "Yaxiong Chen", "Ganchao Liu"], "title": "MSAM: Multi-Semantic Adaptive Mining for Cross-Modal Drone Video-Text Retrieval", "comment": null, "summary": "With the advancement of drone technology, the volume of video data increases\nrapidly, creating an urgent need for efficient semantic retrieval. We are the\nfirst to systematically propose and study the drone video-text retrieval (DVTR)\ntask. Drone videos feature overhead perspectives, strong structural\nhomogeneity, and diverse semantic expressions of target combinations, which\nchallenge existing cross-modal methods designed for ground-level views in\neffectively modeling their characteristics. Therefore, dedicated retrieval\nmechanisms tailored for drone scenarios are necessary. To address this issue,\nwe propose a novel approach called Multi-Semantic Adaptive Mining (MSAM). MSAM\nintroduces a multi-semantic adaptive learning mechanism, which incorporates\ndynamic changes between frames and extracts rich semantic information from\nspecific scene regions, thereby enhancing the deep understanding and reasoning\nof drone video content. This method relies on fine-grained interactions between\nwords and drone video frames, integrating an adaptive semantic construction\nmodule, a distribution-driven semantic learning term and a diversity semantic\nterm to deepen the interaction between text and drone video modalities and\nimprove the robustness of feature representation. To reduce the interference of\ncomplex backgrounds in drone videos, we introduce a cross-modal interactive\nfeature fusion pooling mechanism that focuses on feature extraction and\nmatching in target regions, minimizing noise effects. Extensive experiments on\ntwo self-constructed drone video-text datasets show that MSAM outperforms other\nexisting methods in the drone video-text retrieval task. The source code and\ndataset will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u63d0\u51fa\u5e76\u7814\u7a76\u4e86\u65e0\u4eba\u673a\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u8bed\u4e49\u81ea\u9002\u5e94\u6316\u6398\uff08MSAM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u548c\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u89c6\u9891\u8bed\u4e49\u68c0\u7d22\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u89c6\u9891\u6570\u636e\u91cf\u6025\u5267\u589e\u52a0\uff0c\u8feb\u5207\u9700\u8981\u9ad8\u6548\u7684\u8bed\u4e49\u68c0\u7d22\u65b9\u6cd5\u3002\u73b0\u6709\u8de8\u6a21\u6001\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5730\u9762\u89c6\u89d2\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u65e0\u4eba\u673a\u89c6\u9891\u7684\u4fef\u89c6\u89c6\u89d2\u3001\u5f3a\u7ed3\u6784\u540c\u8d28\u6027\u548c\u76ee\u6807\u7ec4\u5408\u7684\u591a\u6837\u5316\u8bed\u4e49\u8868\u8fbe\u7b49\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u65e0\u4eba\u673a\u573a\u666f\u7684\u68c0\u7d22\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u8bed\u4e49\u81ea\u9002\u5e94\u6316\u6398\uff08MSAM\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u591a\u8bed\u4e49\u81ea\u9002\u5e94\u5b66\u4e60\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u5e27\u95f4\u53d8\u5316\u5efa\u6a21\u548c\u7279\u5b9a\u573a\u666f\u533a\u57df\u8bed\u4e49\u63d0\u53d6\u6765\u589e\u5f3a\u5bf9\u65e0\u4eba\u673a\u89c6\u9891\u5185\u5bb9\u7684\u6df1\u5ea6\u7406\u89e3\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u81ea\u9002\u5e94\u8bed\u4e49\u6784\u5efa\u6a21\u5757\u3001\u5206\u5e03\u9a71\u52a8\u7684\u8bed\u4e49\u5b66\u4e60\u9879\u548c\u591a\u6837\u6027\u8bed\u4e49\u9879\uff0c\u5e76\u5f15\u5165\u4e86\u8de8\u6a21\u6001\u4ea4\u4e92\u7279\u5f81\u878d\u5408\u6c60\u5316\u673a\u5236\u4ee5\u51cf\u5c11\u590d\u6742\u80cc\u666f\u5e72\u6270\u3002", "result": "\u5728\u4e24\u4e2a\u81ea\u5efa\u7684\u65e0\u4eba\u673a\u89c6\u9891-\u6587\u672c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMSAM\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u65e0\u4eba\u673a\u89c6\u9891\u8bed\u4e49\u68c0\u7d22\u63d0\u4f9b\u4e86\u4e13\u95e8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u9488\u5bf9\u65e0\u4eba\u673a\u89c6\u9891\u7279\u6027\u7684\u5b9a\u5236\u5316\u68c0\u7d22\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002\u63d0\u51fa\u7684MSAM\u65b9\u6cd5\u901a\u8fc7\u6df1\u5ea6\u6a21\u6001\u4ea4\u4e92\u548c\u80cc\u666f\u566a\u58f0\u6291\u5236\uff0c\u4e3a\u65e0\u4eba\u673a\u89c6\u9891\u5206\u6790\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2510.15510", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15510", "abs": "https://arxiv.org/abs/2510.15510", "authors": ["Heeseong Shin", "Byeongho Heo", "Dongyoon Han", "Seungryong Kim", "Taekyung Kim"], "title": "Exploring Conditions for Diffusion models in Robotic Control", "comment": "Project page: https://orca-rc.github.io/", "summary": "While pre-trained visual representations have significantly advanced\nimitation learning, they are often task-agnostic as they remain frozen during\npolicy learning. In this work, we explore leveraging pre-trained text-to-image\ndiffusion models to obtain task-adaptive visual representations for robotic\ncontrol, without fine-tuning the model itself. However, we find that naively\napplying textual conditions - a successful strategy in other vision domains -\nyields minimal or even negative gains in control tasks. We attribute this to\nthe domain gap between the diffusion model's training data and robotic control\nenvironments, leading us to argue for conditions that consider the specific,\ndynamic visual information required for control. To this end, we propose ORCA,\nwhich introduces learnable task prompts that adapt to the control environment\nand visual prompts that capture fine-grained, frame-specific details. Through\nfacilitating task-adaptive representations with our newly devised conditions,\nour approach achieves state-of-the-art performance on various robotic control\nbenchmarks, significantly surpassing prior methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86ORCA\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u4efb\u52a1\u63d0\u793a\u548c\u89c6\u89c9\u63d0\u793a\u6765\u9002\u914d\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u63d0\u4f9b\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u89c6\u89c9\u8868\u793a\uff0c\u800c\u65e0\u9700\u5fae\u8c03\u6a21\u578b\u672c\u8eab\u3002\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u673a\u5668\u4eba\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u793a\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u901a\u5e38\u4fdd\u6301\u51bb\u7ed3\u72b6\u6001\uff0c\u5bfc\u81f4\u4efb\u52a1\u65e0\u5173\u6027\u3002\u867d\u7136\u6587\u672c\u6761\u4ef6\u5728\u5176\u4ed6\u89c6\u89c9\u9886\u57df\u8868\u73b0\u6210\u529f\uff0c\u4f46\u5728\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u76f4\u63a5\u5e94\u7528\u6548\u679c\u6709\u9650\u751a\u81f3\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u8fd9\u5f52\u56e0\u4e8e\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u73af\u5883\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86ORCA\u65b9\u6cd5\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u4efb\u52a1\u63d0\u793a\u6765\u9002\u5e94\u63a7\u5236\u73af\u5883\uff0c\u4ee5\u53ca\u89c6\u89c9\u63d0\u793a\u6765\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u5e27\u7ea7\u7ec6\u8282\u3002\u901a\u8fc7\u65b0\u8bbe\u8ba1\u7684\u6761\u4ef6\u673a\u5236\u4fc3\u8fdb\u4efb\u52a1\u81ea\u9002\u5e94\u8868\u793a\uff0c\u800c\u65e0\u9700\u5fae\u8c03\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u672c\u8eab\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u673a\u5668\u4eba\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4efb\u52a1\u81ea\u9002\u5e94\u8868\u793a\u80fd\u591f\u6709\u6548\u63d0\u5347\u63a7\u5236\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u8003\u8651\u63a7\u5236\u4efb\u52a1\u6240\u9700\u7684\u7279\u5b9a\u52a8\u6001\u89c6\u89c9\u4fe1\u606f\u7684\u91cd\u8981\u6027\uff0c\u6210\u529f\u5f25\u5408\u4e86\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u73af\u5883\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002\u53ef\u5b66\u4e60\u63d0\u793a\u673a\u5236\u4e3a\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.15557", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.15557", "abs": "https://arxiv.org/abs/2510.15557", "authors": ["Tingyu Lin", "Marco Peer", "Florian Kleber", "Robert Sablatnig"], "title": "ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents", "comment": "18 pages, accepted at ICDAR2025 DALL", "summary": "This paper presents ClapperText, a benchmark dataset for handwritten and\nprinted text recognition in visually degraded and low-resource settings. The\ndataset is derived from 127 World War II-era archival video segments containing\nclapperboards that record structured production metadata such as date,\nlocation, and camera-operator identity. ClapperText includes 9,813 annotated\nframes and 94,573 word-level text instances, 67% of which are handwritten and\n1,566 are partially occluded. Each instance includes transcription, semantic\ncategory, text type, and occlusion status, with annotations available as\nrotated bounding boxes represented as 4-point polygons to support spatially\nprecise OCR applications. Recognizing clapperboard text poses significant\nchallenges, including motion blur, handwriting variation, exposure\nfluctuations, and cluttered backgrounds, mirroring broader challenges in\nhistorical document analysis where structured content appears in degraded,\nnon-standard forms. We provide both full-frame annotations and cropped word\nimages to support downstream tasks. Using a consistent per-video evaluation\nprotocol, we benchmark six representative recognition and seven detection\nmodels under zero-shot and fine-tuned conditions. Despite the small training\nset (18 videos), fine-tuning leads to substantial performance gains,\nhighlighting ClapperText's suitability for few-shot learning scenarios. The\ndataset offers a realistic and culturally grounded resource for advancing\nrobust OCR and document understanding in low-resource archival contexts. The\ndataset and evaluation code are available at\nhttps://github.com/linty5/ClapperText.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ClapperText\uff0c\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u9000\u5316\u548c\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u624b\u5199\u4e0e\u5370\u5237\u6587\u672c\u8bc6\u522b\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6e90\u81ea\u4e8c\u6218\u65f6\u671f\u6863\u6848\u89c6\u9891\u4e2d\u7684\u573a\u8bb0\u677f\uff0c\u5305\u542b9,813\u4e2a\u6807\u6ce8\u5e27\u548c94,573\u4e2a\u5355\u8bcd\u7ea7\u6587\u672c\u5b9e\u4f8b\uff0c\u4e3a\u5386\u53f2\u6587\u6863\u5206\u6790\u63d0\u4f9b\u4e86\u73b0\u5b9e\u4e14\u6587\u5316\u80cc\u666f\u4e30\u5bcc\u7684\u8d44\u6e90\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5386\u53f2\u6863\u6848\u5206\u6790\u4e2d\u89c6\u89c9\u9000\u5316\u6587\u672c\u8bc6\u522b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u573a\u8bb0\u677f\u6587\u672c\u8bc6\u522b\u9762\u4e34\u8fd0\u52a8\u6a21\u7cca\u3001\u624b\u5199\u53d8\u5f02\u3001\u66dd\u5149\u6ce2\u52a8\u548c\u6742\u4e71\u80cc\u666f\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u53cd\u6620\u4e86\u5386\u53f2\u6587\u6863\u5206\u6790\u4e2d\u7ed3\u6784\u5316\u5185\u5bb9\u51fa\u73b0\u5728\u9000\u5316\u3001\u975e\u6807\u51c6\u5f62\u5f0f\u4e2d\u7684\u666e\u904d\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b127\u4e2a\u4e8c\u6218\u65f6\u671f\u6863\u6848\u89c6\u9891\u7247\u6bb5\u7684ClapperText\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e869,813\u4e2a\u6807\u6ce8\u5e27\u548c94,573\u4e2a\u5355\u8bcd\u7ea7\u6587\u672c\u5b9e\u4f8b\uff0c\u5176\u4e2d67%\u4e3a\u624b\u5199\u6587\u672c\uff0c1,566\u4e2a\u5b9e\u4f8b\u5b58\u5728\u90e8\u5206\u906e\u6321\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u5305\u542b\u8f6c\u5f55\u3001\u8bed\u4e49\u7c7b\u522b\u3001\u6587\u672c\u7c7b\u578b\u548c\u906e\u6321\u72b6\u6001\u7b49\u6807\u6ce8\u4fe1\u606f\uff0c\u5e76\u4ee54\u70b9\u591a\u8fb9\u5f62\u8868\u793a\u7684\u65cb\u8f6c\u8fb9\u754c\u6846\u652f\u6301\u7a7a\u95f4\u7cbe\u786e\u7684OCR\u5e94\u7528\u3002", "result": "\u7814\u7a76\u4f7f\u7528\u4e00\u81f4\u7684\u6bcf\u89c6\u9891\u8bc4\u4f30\u534f\u8bae\uff0c\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u6761\u4ef6\u4e0b\u5bf9\u516d\u4e2a\u4ee3\u8868\u6027\u8bc6\u522b\u6a21\u578b\u548c\u4e03\u4e2a\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u5c3d\u7ba1\u8bad\u7ec3\u96c6\u8f83\u5c0f\uff08\u4ec518\u4e2a\u89c6\u9891\uff09\uff0c\u5fae\u8c03\u4ecd\u80fd\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7a81\u51fa\u4e86ClapperText\u5728\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "ClapperText\u6570\u636e\u96c6\u4e3a\u63a8\u8fdb\u4f4e\u8d44\u6e90\u6863\u6848\u573a\u666f\u4e2d\u9c81\u68d2OCR\u548c\u6587\u6863\u7406\u89e3\u63d0\u4f9b\u4e86\u73b0\u5b9e\u4e14\u6587\u5316\u80cc\u666f\u4e30\u5bcc\u7684\u8d44\u6e90\uff0c\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5fae\u8c03\u5728\u5386\u53f2\u6587\u6863\u5206\u6790\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u5904\u7406\u89c6\u89c9\u9000\u5316\u6587\u672c\u7684\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2510.15579", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15579", "abs": "https://arxiv.org/abs/2510.15579", "authors": ["Mohammad Soltaninezhad", "Yashar Rouzbahani", "Jhonatan Contreras", "Rohan Chippalkatti", "Daniel Kwaku Abankwa", "Christian Eggeling", "Thomas Bocklitz"], "title": "Lightweight CycleGAN Models for Cross-Modality Image Transformation and Experimental Quality Assessment in Fluorescence Microscopy", "comment": "17 pages, 8 Figures", "summary": "Lightweight deep learning models offer substantial reductions in\ncomputational cost and environmental impact, making them crucial for scientific\napplications. We present a lightweight CycleGAN for modality transfer in\nfluorescence microscopy (confocal to super-resolution STED/deconvolved STED),\naddressing the common challenge of unpaired datasets. By replacing the\ntraditional channel-doubling strategy in the U-Net-based generator with a fixed\nchannel approach, we drastically reduce trainable parameters from 41.8 million\nto approximately nine thousand, achieving superior performance with faster\ntraining and lower memory usage. We also introduce the GAN as a diagnostic tool\nfor experimental and labeling quality. When trained on high-quality images, the\nGAN learns the characteristics of optimal imaging; deviations between its\ngenerated outputs and new experimental images can reveal issues such as\nphotobleaching, artifacts, or inaccurate labeling. This establishes the model\nas a practical tool for validating experimental accuracy and image fidelity in\nmicroscopy workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7CycleGAN\u7528\u4e8e\u8367\u5149\u663e\u5fae\u955c\u4e2d\u7684\u6a21\u6001\u8f6c\u6362\uff0c\u901a\u8fc7\u56fa\u5b9a\u901a\u9053\u7b56\u7565\u5c06\u53c2\u6570\u4ece4180\u4e07\u51cf\u5c11\u5230\u7ea69000\u4e2a\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u8bad\u7ec3\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u4f7f\u7528\u3002\u8be5\u6a21\u578b\u8fd8\u53ef\u4f5c\u4e3a\u5b9e\u9a8c\u548c\u6807\u8bb0\u8d28\u91cf\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u901a\u8fc7\u6bd4\u8f83\u751f\u6210\u56fe\u50cf\u4e0e\u5b9e\u9a8c\u56fe\u50cf\u6765\u8bc6\u522b\u6210\u50cf\u95ee\u9898\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8367\u5149\u663e\u5fae\u955c\u4e2d\u6a21\u6001\u8f6c\u6362\u9762\u4e34\u7684\u672a\u914d\u5bf9\u6570\u636e\u96c6\u6311\u6218\uff0c\u540c\u65f6\u964d\u4f4e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u548c\u73af\u5883\u5f71\u54cd\u3002\u4f20\u7edf\u65b9\u6cd5\u53c2\u6570\u8fc7\u591a\u4e14\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u8f7b\u91cf\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7CycleGAN\u67b6\u6784\uff0c\u5728\u57fa\u4e8eU-Net\u7684\u751f\u6210\u5668\u4e2d\u7528\u56fa\u5b9a\u901a\u9053\u65b9\u6cd5\u66ff\u4ee3\u4f20\u7edf\u7684\u901a\u9053\u500d\u589e\u7b56\u7565\u3002\u8fd9\u79cd\u8bbe\u8ba1\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u6001\u8f6c\u6362\u7684\u6027\u80fd\u3002\u6a21\u578b\u8fd8\u4f5c\u4e3a\u8bca\u65ad\u5de5\u5177\u7528\u4e8e\u8bc4\u4f30\u5b9e\u9a8c\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u6a21\u578b\u53c2\u6570\u4ece4180\u4e07\u5927\u5e45\u51cf\u5c11\u5230\u7ea69000\u4e2a\uff0c\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\u4e14\u5185\u5b58\u4f7f\u7528\u66f4\u4f4e\u3002\u5728\u9ad8\u8d28\u91cf\u56fe\u50cf\u4e0a\u8bad\u7ec3\u540e\uff0c\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u6700\u4f18\u6210\u50cf\u7279\u5f81\uff0c\u5176\u751f\u6210\u8f93\u51fa\u4e0e\u5b9e\u9a8c\u56fe\u50cf\u7684\u504f\u5dee\u53ef\u63ed\u793a\u5149\u6f02\u767d\u3001\u4f2a\u5f71\u6216\u6807\u8bb0\u4e0d\u51c6\u786e\u7b49\u95ee\u9898\u3002", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7CycleGAN\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6a21\u6001\u8f6c\u6362\uff0c\u8fd8\u5efa\u7acb\u4e86\u4f5c\u4e3a\u5b9e\u9a8c\u9a8c\u8bc1\u5de5\u5177\u7684\u65b0\u5e94\u7528\u8303\u5f0f\u3002\u5b83\u4e3a\u663e\u5fae\u955c\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u56fe\u50cf\u4fdd\u771f\u5ea6\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u79d1\u5b66\u4eea\u5668\u8d28\u91cf\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.15595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15595", "abs": "https://arxiv.org/abs/2510.15595", "authors": ["Zhen Sun", "Lei Tan", "Yunhang Shen", "Chengmao Cai", "Xing Sun", "Pingyang Dai", "Liujuan Cao", "Rongrong Ji"], "title": "FlexiReID: Adaptive Mixture of Expert for Multi-Modal Person Re-Identification", "comment": null, "summary": "Multimodal person re-identification (Re-ID) aims to match pedestrian images\nacross different modalities. However, most existing methods focus on limited\ncross-modal settings and fail to support arbitrary query-retrieval\ncombinations, hindering practical deployment. We propose FlexiReID, a flexible\nframework that supports seven retrieval modes across four modalities: rgb,\ninfrared, sketches, and text. FlexiReID introduces an adaptive\nmixture-of-experts (MoE) mechanism to dynamically integrate diverse modality\nfeatures and a cross-modal query fusion module to enhance multimodal feature\nextraction. To facilitate comprehensive evaluation, we construct CIRS-PEDES, a\nunified dataset extending four popular Re-ID datasets to include all four\nmodalities. Extensive experiments demonstrate that FlexiReID achieves\nstate-of-the-art performance and offers strong generalization in complex\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FlexiReID\uff0c\u4e00\u4e2a\u652f\u6301RGB\u3001\u7ea2\u5916\u3001\u7d20\u63cf\u548c\u6587\u672c\u56db\u79cd\u6a21\u6001\u95f4\u4e03\u79cd\u68c0\u7d22\u6a21\u5f0f\u7684\u7075\u6d3b\u591a\u6a21\u6001\u884c\u4eba\u91cd\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u548c\u8de8\u6a21\u6001\u67e5\u8be2\u878d\u5408\u6a21\u5757\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6709\u9650\u7684\u8de8\u6a21\u6001\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u652f\u6301\u4efb\u610f\u67e5\u8be2-\u68c0\u7d22\u7ec4\u5408\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u7075\u6d3b\u6027\u3002", "method": "FlexiReID\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u6765\u52a8\u6001\u6574\u5408\u4e0d\u540c\u6a21\u6001\u7279\u5f81\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8de8\u6a21\u6001\u67e5\u8be2\u878d\u5408\u6a21\u5757\u4ee5\u589e\u5f3a\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u5728\u6784\u5efa\u7684\u7edf\u4e00\u6570\u636e\u96c6CIRS-PEDES\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFlexiReID\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u590d\u6742\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u652f\u6301\u591a\u79cd\u6a21\u6001\u7ec4\u5408\u7684\u7075\u6d3b\u6846\u67b6\u5728\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u590d\u6742\u591a\u6a21\u6001\u68c0\u7d22\u9700\u6c42\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.15673", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15673", "abs": "https://arxiv.org/abs/2510.15673", "authors": ["Antonyo Musabini", "Rachid Benmokhtar", "Jagdish Bhanushali", "Victor Galizzi", "Bertrand Luvison", "Xavier Perrotton"], "title": "Valeo Near-Field: a novel dataset for pedestrian intent detection", "comment": null, "summary": "This paper presents a novel dataset aimed at detecting pedestrians'\nintentions as they approach an ego-vehicle. The dataset comprises synchronized\nmulti-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic\nsensor readings, and motion capture-based 3D body poses, collected across\ndiverse real-world scenarios. Key contributions include detailed annotations of\n3D body joint positions synchronized with fisheye camera images, as well as\naccurate 3D pedestrian positions extracted from lidar data, facilitating robust\nbenchmarking for perception algorithms. We release a portion of the dataset\nalong with a comprehensive benchmark suite, featuring evaluation metrics for\naccuracy, efficiency, and scalability on embedded systems. By addressing\nreal-world challenges such as sensor occlusions, dynamic environments, and\nhardware constraints, this dataset offers a unique resource for developing and\nevaluating state-of-the-art algorithms in pedestrian detection, 3D pose\nestimation and 4D trajectory and intention prediction. Additionally, we provide\nbaseline performance metrics using custom neural network architectures and\nsuggest future research directions to encourage the adoption and enhancement of\nthe dataset. This work aims to serve as a foundation for researchers seeking to\nadvance the capabilities of intelligent vehicles in near-field scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u884c\u4eba\u610f\u56fe\u7684\u65b0\u578b\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u9c7c\u773c\u76f8\u673a\u3001\u6fc0\u5149\u96f7\u8fbe\u3001\u8d85\u58f0\u6ce2\u4f20\u611f\u5668\u548c\u8fd0\u52a8\u6355\u6349\u6570\u636e\uff0c\u65e8\u5728\u4e3a\u667a\u80fd\u8f66\u8f86\u5728\u8fd1\u573a\u573a\u666f\u4e2d\u7684\u611f\u77e5\u7b97\u6cd5\u63d0\u4f9b\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u667a\u80fd\u8f66\u8f86\u5728\u8fd1\u573a\u573a\u666f\u4e2d\u51c6\u786e\u68c0\u6d4b\u884c\u4eba\u610f\u56fe\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4f20\u611f\u5668\u906e\u6321\u3001\u52a8\u6001\u73af\u5883\u548c\u786c\u4ef6\u7ea6\u675f\u7b49\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u65f6\uff0c\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u540c\u6b65\u591a\u6a21\u6001\u6570\u636e\u548c\u8be6\u7ec63D\u59ff\u6001\u6807\u6ce8\u7684\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u540c\u6b65\u591a\u6a21\u6001\u6570\u636e\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u5305\u62ec\u9c7c\u773c\u76f8\u673a\u89c6\u9891\u3001\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u3001\u8d85\u58f0\u6ce2\u4f20\u611f\u5668\u8bfb\u6570\u548c\u57fa\u4e8e\u8fd0\u52a8\u6355\u6349\u76843D\u4eba\u4f53\u59ff\u6001\uff0c\u63d0\u4f9b\u4e86\u8be6\u7ec6\u76843D\u5173\u8282\u4f4d\u7f6e\u6807\u6ce8\u548c\u7cbe\u786e\u76843D\u884c\u4eba\u4f4d\u7f6e\u4fe1\u606f\u3002", "result": "\u7814\u7a76\u53d1\u5e03\u4e86\u90e8\u5206\u6570\u636e\u96c6\u548c\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u5d4c\u5165\u5f0f\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5b9a\u5236\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u57fa\u7ebf\u6027\u80fd\u6307\u6807\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u884c\u4eba\u68c0\u6d4b\u30013D\u59ff\u6001\u4f30\u8ba1\u4ee5\u53ca4D\u8f68\u8ff9\u548c\u610f\u56fe\u9884\u6d4b\u7b49\u4efb\u52a1\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u8d44\u6e90\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u5148\u8fdb\u7b97\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u4fc3\u8fdb\u6570\u636e\u96c6\u7684\u91c7\u7528\u548c\u6539\u8fdb\u3002"}}
{"id": "2510.15602", "categories": ["cs.CV", "I.4.7; I.2.10; I.3.8"], "pdf": "https://arxiv.org/pdf/2510.15602", "abs": "https://arxiv.org/abs/2510.15602", "authors": ["Andrei-Timotei Ardelean", "Patrick R\u00fcckbeil", "Tim Weyrich"], "title": "Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection", "comment": "13 pages, 10 figures. Published in the 30th Intl. Conference on\n  Vision, Modeling, and Visualization (VMV), 2025", "summary": "Zero-shot anomaly localization is a rising field in computer vision research,\nwith important progress in recent years. This work focuses on the problem of\ndetecting and localizing anomalies in textures, where anomalies can be defined\nas the regions that deviate from the overall statistics, violating the\nstationarity assumption. The main limitation of existing methods is their high\nrunning time, making them impractical for deployment in real-world scenarios,\nsuch as assembly line monitoring. We propose a real-time method, named QFCA,\nwhich implements a quantized version of the feature correspondence analysis\n(FCA) algorithm. By carefully adapting the patch statistics comparison to work\non histograms of quantized values, we obtain a 10x speedup with little to no\nloss in accuracy. Moreover, we introduce a feature preprocessing step based on\nprincipal component analysis, which enhances the contrast between normal and\nanomalous features, improving the detection precision on complex textures. Our\nmethod is thoroughly evaluated against prior art, comparing favorably with\nexisting methods. Project page:\nhttps://reality.tf.fau.de/pub/ardelean2025quantized.html", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQFCA\u7684\u5b9e\u65f6\u96f6\u6837\u672c\u5f02\u5e38\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u7279\u5f81\u5bf9\u5e94\u5206\u6790\u7b97\u6cd5\u5b9e\u73b0\u4e8610\u500d\u52a0\u901f\uff0c\u5728\u7eb9\u7406\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u5f02\u5e38\u5b9a\u4f4d\u65b9\u6cd5\u5728\u7eb9\u7406\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5b58\u5728\u8fd0\u884c\u65f6\u95f4\u8fc7\u957f\u7684\u95ee\u9898\uff0c\u4f7f\u5176\u96be\u4ee5\u5728\u5b9e\u9645\u573a\u666f\u5982\u751f\u4ea7\u7ebf\u76d1\u63a7\u4e2d\u90e8\u7f72\u5e94\u7528\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5b9e\u9645\u90e8\u7f72\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u7684QFCA\u65b9\u6cd5\u5b9e\u73b0\u4e86\u91cf\u5316\u7248\u672c\u7684\u7279\u5f81\u5bf9\u5e94\u5206\u6790\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u8865\u4e01\u7edf\u8ba1\u6bd4\u8f83\u9002\u914d\u5230\u91cf\u5316\u503c\u76f4\u65b9\u56fe\u4e0a\u5de5\u4f5c\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u4e3b\u6210\u5206\u5206\u6790\u7684\u7279\u5f81\u9884\u5904\u7406\u6b65\u9aa4\u6765\u589e\u5f3a\u6b63\u5e38\u4e0e\u5f02\u5e38\u7279\u5f81\u4e4b\u95f4\u7684\u5bf9\u6bd4\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u51c6\u786e\u7387\u51e0\u4e4e\u6ca1\u6709\u635f\u5931\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u4e8610\u500d\u7684\u52a0\u901f\u6548\u679c\uff0c\u5e76\u5728\u590d\u6742\u7eb9\u7406\u4e0a\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5728\u4e0e\u73b0\u6709\u65b9\u6cd5\u7684\u5168\u9762\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "QFCA\u65b9\u6cd5\u8bc1\u660e\u4e86\u901a\u8fc7\u91cf\u5316\u7b56\u7565\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u7684\u8fd0\u884c\u6548\u7387\uff0c\u4e3a\u96f6\u6837\u672c\u5f02\u5e38\u5b9a\u4f4d\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.15684", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15684", "abs": "https://arxiv.org/abs/2510.15684", "authors": ["Gerard Comas-Quiles", "Carles Garcia-Cabrera", "Julia Dietlmeier", "Noel E. O'Connor", "Ferran Marques"], "title": "Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI", "comment": "10 pages, 5 figures, BraTS GoAT 2025 challenge", "summary": "Unsupervised anomaly detection (UAD) presents a complementary alternative to\nsupervised learning for brain tumor segmentation in magnetic resonance imaging\n(MRI), particularly when annotated datasets are limited, costly, or\ninconsistent. In this work, we propose a novel Multimodal Vision Transformer\nAutoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and\nlocalize tumors via reconstruction-based error maps. This unsupervised paradigm\nenables segmentation without reliance on manual labels, addressing a key\nscalability bottleneck in neuroimaging workflows. Our method is evaluated in\nthe BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors\nsuch as gliomas, meningiomas, and pediatric brain tumors. To enhance\nperformance, we introduce a multimodal early-late fusion strategy that\nleverages complementary information across multiple MRI sequences, and a\npost-processing pipeline that integrates the Segment Anything Model (SAM) to\nrefine predicted tumor contours. Despite the known challenges of UAD,\nparticularly in detecting small or non-enhancing lesions, our method achieves\nclinically meaningful tumor localization, with lesion-wise Dice Similarity\nCoefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing\nTumor) on the test set, and an anomaly Detection Rate of 89.4% on the\nvalidation set. These findings highlight the potential of transformer-based\nunsupervised models to serve as scalable, label-efficient tools for\nneuro-oncological imaging.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u89c6\u89c9Transformer\u81ea\u7f16\u7801\u5668\uff08MViT-AE\uff09\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u8111\u80bf\u7624\u5206\u5272\uff0c\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5065\u5eb7\u8111\u90e8MRI\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u91cd\u5efa\u8bef\u5dee\u56fe\u5b9e\u73b0\u80bf\u7624\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff0c\u5728BraTS-GoAT 2025\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u6027\u80fd\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8111\u80bf\u7624\u5206\u5272\u4e2d\u6807\u6ce8\u6570\u636e\u6709\u9650\u3001\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u76d1\u7763\u5b66\u4e60\u7684\u8865\u5145\u65b9\u6848\uff0c\u7279\u522b\u9488\u5bf9\u795e\u7ecf\u5f71\u50cf\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u89c6\u89c9Transformer\u81ea\u7f16\u7801\u5668\uff08MViT-AE\uff09\uff0c\u91c7\u7528\u591a\u6a21\u6001\u65e9\u671f-\u665a\u671f\u878d\u5408\u7b56\u7565\u6574\u5408\u4e0d\u540cMRI\u5e8f\u5217\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u5305\u542bSegment Anything Model\uff08SAM\uff09\u7684\u540e\u5904\u7406\u6d41\u7a0b\u6765\u4f18\u5316\u9884\u6d4b\u7684\u80bf\u7624\u8f6e\u5ed3\u3002", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u75c5\u7076\u7ea7\u522b\u7684Dice\u76f8\u4f3c\u7cfb\u6570\uff1a\u5168\u80bf\u76240.437\u3001\u80bf\u7624\u6838\u5fc30.316\u3001\u589e\u5f3a\u80bf\u76240.350\uff0c\u5728\u9a8c\u8bc1\u96c6\u4e0a\u5f02\u5e38\u68c0\u6d4b\u7387\u8fbe\u523089.4%\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u57fa\u4e8eTransformer\u7684\u65e0\u76d1\u7763\u6a21\u578b\u5177\u6709\u4f5c\u4e3a\u795e\u7ecf\u80bf\u7624\u5f71\u50cf\u53ef\u6269\u5c55\u3001\u6807\u7b7e\u9ad8\u6548\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u5c3d\u7ba1\u5728\u68c0\u6d4b\u5c0f\u75c5\u7076\u6216\u975e\u589e\u5f3a\u75c5\u53d8\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u4f46\u4e3a\u65e0\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2510.15725", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.15725", "abs": "https://arxiv.org/abs/2510.15725", "authors": ["Tingyu Lin", "Armin Dadras", "Florian Kleber", "Robert Sablatnig"], "title": "DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification", "comment": "9 pages, accepted at ACMMM2025 SUMAC", "summary": "Camera movement classification (CMC) models trained on contemporary,\nhigh-quality footage often degrade when applied to archival film, where noise,\nmissing frames, and low contrast obscure motion cues. We bridge this gap by\nassembling a unified benchmark that consolidates two modern corpora into four\ncanonical classes and restructures the HISTORIAN collection into five balanced\ncategories. Building on this benchmark, we introduce DGME-T, a lightweight\nextension to the Video Swin Transformer that injects directional grid motion\nencoding, derived from optical flow, via a learnable and normalised late-fusion\nlayer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and\nits macro F1 from 82.08% to 87.81% on modern clips, while still improving the\ndemanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72%\nto 82.63% macro F1. A cross-domain study further shows that an intermediate\nfine-tuning stage on modern data increases historical performance by more than\nfive percentage points. These results demonstrate that structured motion priors\nand transformer representations are complementary and that even a small,\ncarefully calibrated motion head can substantially enhance robustness in\ndegraded film analysis. Related resources are available at\nhttps://github.com/linty5/DGME-T.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDGME-T\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u9891Swin Transformer\u6269\u5c55\uff0c\u901a\u8fc7\u6ce8\u5165\u65b9\u5411\u6027\u7f51\u683c\u8fd0\u52a8\u7f16\u7801\u6765\u63d0\u5347\u6863\u6848\u5f71\u7247\u4e2d\u7684\u76f8\u673a\u8fd0\u52a8\u5206\u7c7b\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u73b0\u4ee3\u548c\u5386\u53f2\u5f71\u7247\u6570\u636e\u4e0a\u5747\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u548cF1\u5206\u6570\u3002", "motivation": "\u9488\u5bf9\u5728\u5f53\u4ee3\u9ad8\u8d28\u91cf\u89c6\u9891\u4e0a\u8bad\u7ec3\u7684\u76f8\u673a\u8fd0\u52a8\u5206\u7c7b\u6a21\u578b\u5728\u5904\u7406\u6863\u6848\u5f71\u7247\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u7531\u4e8e\u6863\u6848\u5f71\u7247\u5b58\u5728\u566a\u58f0\u3001\u7f3a\u5931\u5e27\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u7b49\u9000\u5316\u56e0\u7d20\u4f1a\u6a21\u7cca\u8fd0\u52a8\u7ebf\u7d22\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u7edf\u4e00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5c06\u4e24\u4e2a\u73b0\u4ee3\u8bed\u6599\u5e93\u6574\u5408\u4e3a\u56db\u4e2a\u89c4\u8303\u7c7b\u522b\uff0c\u5e76\u91cd\u6784HISTORIAN\u96c6\u5408\u4e3a\u4e94\u4e2a\u5e73\u8861\u7c7b\u522b\u3002\u63d0\u51fa\u4e86DGME-T\u65b9\u6cd5\uff0c\u5728Video Swin Transformer\u57fa\u7840\u4e0a\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5f52\u4e00\u5316\u540e\u671f\u878d\u5408\u5c42\u6ce8\u5165\u57fa\u4e8e\u5149\u6d41\u7684\u65b9\u5411\u6027\u7f51\u683c\u8fd0\u52a8\u7f16\u7801\u3002", "result": "DGME-T\u5c06\u9aa8\u5e72\u7f51\u7edc\u7684top-1\u51c6\u786e\u7387\u4ece81.78%\u63d0\u5347\u81f386.14%\uff0c\u5b8f\u89c2F1\u4ece82.08%\u63d0\u5347\u81f387.81%\u3002\u5728\u4e8c\u6218\u6863\u6848\u5f71\u7247\u4e0a\uff0c\u51c6\u786e\u7387\u4ece83.43%\u63d0\u5347\u81f384.62%\uff0c\u5b8f\u89c2F1\u4ece81.72%\u63d0\u5347\u81f382.63%\u3002\u8de8\u57df\u7814\u7a76\u8868\u660e\u5728\u73b0\u4ee3\u6570\u636e\u4e0a\u8fdb\u884c\u4e2d\u95f4\u5fae\u8c03\u53ef\u5c06\u5386\u53f2\u6027\u80fd\u63d0\u5347\u8d85\u8fc7\u4e94\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7ed3\u6784\u5316\u8fd0\u52a8\u5148\u9a8c\u4e0eTransformer\u8868\u793a\u5177\u6709\u4e92\u8865\u6027\uff0c\u5373\u4f7f\u662f\u4e00\u4e2a\u5c0f\u578b\u7cbe\u5fc3\u6821\u51c6\u7684\u8fd0\u52a8\u5934\u4e5f\u80fd\u663e\u8457\u589e\u5f3a\u9000\u5316\u5f71\u7247\u5206\u6790\u7684\u9c81\u68d2\u6027\u3002\u65b9\u5411\u6027\u8fd0\u52a8\u7f16\u7801\u4e0eTransformer\u67b6\u6784\u7684\u7ed3\u5408\u4e3a\u6863\u6848\u5f71\u7247\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.15752", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15752", "abs": "https://arxiv.org/abs/2510.15752", "authors": ["Yitong Sun", "Yao Huang", "Ruochen Zhang", "Huanran Chen", "Shouwei Ruan", "Ranjie Duan", "Xingxing Wei"], "title": "NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation", "comment": "10 pages, 8 figures, accepted by ACMMM 2025", "summary": "Despite the impressive generative capabilities of text-to-image (T2I)\ndiffusion models, they remain vulnerable to generating inappropriate content,\nespecially when confronted with implicit sexual prompts. Unlike explicit\nharmful prompts, these subtle cues, often disguised as seemingly benign terms,\ncan unexpectedly trigger sexual content due to underlying model biases, raising\nsignificant ethical concerns. However, existing detection methods are primarily\ndesigned to identify explicit sexual content and therefore struggle to detect\nthese implicit cues. Fine-tuning approaches, while effective to some extent,\nrisk degrading the model's generative quality, creating an undesirable\ntrade-off. To address this, we propose NDM, the first noise-driven detection\nand mitigation framework, which could detect and mitigate implicit malicious\nintention in T2I generation while preserving the model's original generative\ncapabilities. Specifically, we introduce two key innovations: first, we\nleverage the separability of early-stage predicted noise to develop a\nnoise-based detection method that could identify malicious content with high\naccuracy and efficiency; second, we propose a noise-enhanced adaptive negative\nguidance mechanism that could optimize the initial noise by suppressing the\nprominent region's attention, thereby enhancing the effectiveness of adaptive\nnegative guidance for sexual mitigation. Experimentally, we validate NDM on\nboth natural and adversarial datasets, demonstrating its superior performance\nover existing SOTA methods, including SLD, UCE, and RECE, etc. Code and\nresources are available at https://github.com/lorraine021/NDM.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faNDM\uff0c\u9996\u4e2a\u566a\u58f0\u9a71\u52a8\u7684\u68c0\u6d4b\u4e0e\u7f13\u89e3\u6846\u67b6\uff0c\u80fd\u591f\u68c0\u6d4b\u5e76\u7f13\u89e3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u9690\u542b\u6076\u610f\u610f\u56fe\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u539f\u59cb\u751f\u6210\u80fd\u529b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u566a\u58f0\u5206\u79bb\u6027\u548c\u81ea\u9002\u5e94\u8d1f\u5f15\u5bfc\u673a\u5236\uff0c\u5728\u81ea\u7136\u548c\u5bf9\u6297\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u80fd\u529b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u9690\u542b\u6027\u6697\u793a\u63d0\u793a\u4ecd\u7136\u8106\u5f31\uff0c\u8fd9\u4e9b\u5fae\u5999\u7ebf\u7d22\u5e38\u4f2a\u88c5\u4e3a\u826f\u6027\u672f\u8bed\u5374\u53ef\u80fd\u610f\u5916\u89e6\u53d1\u4e0d\u5f53\u5185\u5bb9\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u663e\u5f0f\u5185\u5bb9\u8bbe\u8ba1\uff0c\u96be\u4ee5\u8bc6\u522b\u9690\u542b\u7ebf\u7d22\uff0c\u800c\u5fae\u8c03\u65b9\u6cd5\u867d\u6709\u6548\u4f46\u4f1a\u635f\u5bb3\u751f\u6210\u8d28\u91cf\uff0c\u5f62\u6210\u4e0d\u826f\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u9996\u5148\u5229\u7528\u65e9\u671f\u9884\u6d4b\u566a\u58f0\u7684\u53ef\u5206\u79bb\u6027\u5f00\u53d1\u57fa\u4e8e\u566a\u58f0\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u7cbe\u5ea6\u9ad8\u6548\u8bc6\u522b\u6076\u610f\u5185\u5bb9\uff1b\u5176\u6b21\u63d0\u51fa\u566a\u58f0\u589e\u5f3a\u7684\u81ea\u9002\u5e94\u8d1f\u5f15\u5bfc\u673a\u5236\uff0c\u901a\u8fc7\u6291\u5236\u663e\u8457\u533a\u57df\u6ce8\u610f\u529b\u6765\u4f18\u5316\u521d\u59cb\u566a\u58f0\uff0c\u4ece\u800c\u589e\u5f3a\u5bf9\u6027\u5185\u5bb9\u7f13\u89e3\u7684\u81ea\u9002\u5e94\u8d1f\u5f15\u5bfc\u6548\u679c\u3002", "result": "\u5728\u81ea\u7136\u548c\u5bf9\u6297\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cNDM\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5305\u62ecSLD\u3001UCE\u548cRECE\u5728\u5185\u7684\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5353\u8d8a\u7684\u68c0\u6d4b\u548c\u7f13\u89e3\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u566a\u58f0\u9a71\u52a8\u65b9\u6cd5\u5728\u68c0\u6d4b\u548c\u7f13\u89e3\u9690\u542b\u6076\u610f\u610f\u56fe\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u5b89\u5168\u9632\u62a4\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2510.15710", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15710", "abs": "https://arxiv.org/abs/2510.15710", "authors": ["Junzhi Ning", "Wei Li", "Cheng Tang", "Jiashi Lin", "Chenglong Ma", "Chaoyang Zhang", "Jiyao Liu", "Ying Chen", "Shujian Gao", "Lihao Liu", "Yuandong Pu", "Huihui Xu", "Chenhui Gou", "Ziyan Huang", "Yi Xin", "Qi Qin", "Zhongying Deng", "Diping Song", "Bin Fu", "Guang Yang", "Yuanfeng Ji", "Tianbin Li", "Yanzhou Su", "Jin Ye", "Shixiang Tang", "Ming Hu", "Junjun He"], "title": "Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis", "comment": null, "summary": "Medical diagnostic applications require models that can process multimodal\nmedical inputs (images, patient histories, lab results) and generate diverse\noutputs including both textual reports and visual content (annotations,\nsegmentation masks, and images). Despite this need, existing medical AI systems\ndisrupt this unified process: medical image understanding models interpret\nimages but cannot generate visual outputs, while medical image generation\nmodels synthesize images but cannot provide textual explanations. This leads to\ngaps in data representation, feature integration, and task-level multimodal\ncapabilities. To this end, we propose a multi-level framework that draws\ninspiration from diagnostic workflows through the\nObservation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation\nlevel, we construct UniMed-5M, a dataset comprising over 5.6M samples that\nreformat diverse unimodal data into multimodal pairs for foundational\nobservation. At the knowledge level, we propose Progressive Curriculum Learning\nthat systematically introduces medical multimodal knowledge. At the analysis\nlevel, we introduce UniMedVL, the first medical unified multimodal model for\nthe simultaneous analysis of image understanding and generation tasks within a\nsingle architecture. UniMedVL achieves superior performance on five medical\nimage understanding benchmarks, while matching specialized models in generation\nquality across eight medical imaging modalities. Crucially, our unified\narchitecture enables bidirectional knowledge sharing: generation tasks enhance\nvisual understanding features, demonstrating that integrating traditionally\nseparate capabilities within a single medical framework unlocks improvements\nacross diverse medical vision-language tasks. Code is available at\nhttps://github.com/uni-medical/UniMedVL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UniMedVL\uff0c\u8fd9\u662f\u9996\u4e2a\u533b\u5b66\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7Observation-Knowledge-Analysis\u6846\u67b6\u5728\u5355\u4e00\u67b6\u6784\u4e2d\u540c\u65f6\u5904\u7406\u533b\u5b66\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5728\u4e94\u4e2a\u7406\u89e3\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5728\u516b\u4e2a\u533b\u5b66\u6210\u50cf\u6a21\u6001\u4e0a\u5339\u914d\u4e13\u7528\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u533b\u5b66AI\u7cfb\u7edf\u5b58\u5728\u7edf\u4e00\u5904\u7406\u6d41\u7a0b\u7684\u65ad\u88c2\uff1a\u533b\u5b66\u56fe\u50cf\u7406\u89e3\u6a21\u578b\u53ea\u80fd\u89e3\u91ca\u56fe\u50cf\u800c\u65e0\u6cd5\u751f\u6210\u89c6\u89c9\u8f93\u51fa\uff0c\u533b\u5b66\u56fe\u50cf\u751f\u6210\u6a21\u578b\u53ea\u80fd\u5408\u6210\u56fe\u50cf\u800c\u65e0\u6cd5\u63d0\u4f9b\u6587\u672c\u89e3\u91ca\uff0c\u8fd9\u5bfc\u81f4\u4e86\u6570\u636e\u8868\u793a\u3001\u7279\u5f81\u96c6\u6210\u548c\u4efb\u52a1\u7ea7\u591a\u6a21\u6001\u80fd\u529b\u65b9\u9762\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eObservation-Knowledge-Analysis\u8303\u5f0f\u7684\u591a\u7ea7\u6846\u67b6\uff0c\u5305\u62ec\u6784\u5efa\u5305\u542b560\u4e07\u6837\u672c\u7684UniMed-5M\u6570\u636e\u96c6\u7528\u4e8e\u57fa\u7840\u89c2\u5bdf\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bfe\u7a0b\u5b66\u4e60\u7cfb\u7edf\u5f15\u5165\u533b\u5b66\u591a\u6a21\u6001\u77e5\u8bc6\uff0c\u4ee5\u53ca\u8bbe\u8ba1UniMedVL\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u5355\u4e00\u67b6\u6784\u4e2d\u540c\u65f6\u5206\u6790\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u3002", "result": "UniMedVL\u5728\u4e94\u4e2a\u533b\u5b66\u56fe\u50cf\u7406\u89e3\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u540c\u65f6\u5728\u516b\u4e2a\u533b\u5b66\u6210\u50cf\u6a21\u6001\u7684\u751f\u6210\u8d28\u91cf\u4e0a\u5339\u914d\u4e13\u7528\u6a21\u578b\uff0c\u7edf\u4e00\u67b6\u6784\u5b9e\u73b0\u4e86\u53cc\u5411\u77e5\u8bc6\u5171\u4eab\uff0c\u751f\u6210\u4efb\u52a1\u589e\u5f3a\u4e86\u89c6\u89c9\u7406\u89e3\u7279\u5f81\u3002", "conclusion": "\u5728\u5355\u4e00\u533b\u5b66\u6846\u67b6\u5185\u6574\u5408\u4f20\u7edf\u5206\u79bb\u7684\u80fd\u529b\u80fd\u591f\u89e3\u9501\u8de8\u591a\u6837\u5316\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u6539\u8fdb\uff0c\u53cc\u5411\u77e5\u8bc6\u5171\u4eab\u673a\u5236\u8bc1\u660e\u751f\u6210\u548c\u7406\u89e3\u4efb\u52a1\u7684\u534f\u540c\u4f5c\u7528\u53ef\u4ee5\u76f8\u4e92\u589e\u5f3a\uff0c\u4e3a\u533b\u5b66\u8bca\u65ad\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5904\u7406\u65b9\u6848\u3002"}}
{"id": "2510.15761", "categories": ["cs.CV", "cs.LG", "68T07, 68U10", "I.2.10; I.4.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.15761", "abs": "https://arxiv.org/abs/2510.15761", "authors": ["Denis Rychkovskiy"], "title": "QSilk: Micrograin Stabilization and Adaptive Quantile Clipping for Detail-Friendly Latent Diffusion", "comment": "Preprint. Qualitative side-by-side comparisons (fixed seeds); 3\n  figures with subfigures; 1 algorithm. CADE 2.5 / SDXL integration; sample\n  images included. Code and presets planned for release upon publication", "summary": "We present QSilk, a lightweight, always-on stabilization layer for latent\ndiffusion that improves high-frequency fidelity while suppressing rare\nactivation spikes. QSilk combines (i) a per-sample micro clamp that gently\nlimits extreme values without washing out texture, and (ii) Adaptive Quantile\nClip (AQClip), which adapts the allowed value corridor per region. AQClip can\noperate in a proxy mode using local structure statistics or in an attention\nentropy guided mode (model confidence). Integrated into the CADE 2.5 rendering\npipeline, QSilk yields cleaner, sharper results at low step counts and\nultra-high resolutions with negligible overhead. It requires no training or\nfine-tuning and exposes minimal user controls. We report consistent qualitative\nimprovements across SD/SDXL backbones and show synergy with CFG/Rescale,\nenabling slightly higher guidance without artifacts.", "AI": {"tldr": "QSilk\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7a33\u5b9a\u5c42\uff0c\u901a\u8fc7\u5fae\u94b3\u4f4d\u548c\u81ea\u9002\u5e94\u5206\u4f4d\u6570\u88c1\u526a\u6280\u672f\uff0c\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u9ad8\u9891\u7ec6\u8282\u4fdd\u771f\u5ea6\u5e76\u6291\u5236\u7f55\u89c1\u6fc0\u6d3b\u5cf0\u503c\uff0c\u5b9e\u73b0\u66f4\u6e05\u6670\u3001\u66f4\u9510\u5229\u7684\u751f\u6210\u7ed3\u679c\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u9ad8\u9891\u7ec6\u8282\u4fdd\u771f\u5ea6\u4e0d\u8db3\u548c\u7f55\u89c1\u6fc0\u6d3b\u5cf0\u503c\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5728\u4f4e\u6b65\u6570\u91c7\u6837\u548c\u8d85\u9ad8\u5206\u8fa8\u7387\u751f\u6210\u65f6\u5c24\u4e3a\u660e\u663e\uff0c\u5f71\u54cd\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u3002", "method": "QSilk\u7ed3\u5408\u4e86\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1a\u6bcf\u6837\u672c\u5fae\u94b3\u4f4d\u6280\u672f\u6e29\u548c\u9650\u5236\u6781\u7aef\u503c\u800c\u4e0d\u635f\u5931\u7eb9\u7406\u7ec6\u8282\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u5206\u4f4d\u6570\u88c1\u526a\uff08AQClip\uff09\u6839\u636e\u533a\u57df\u7279\u6027\u52a8\u6001\u8c03\u6574\u5141\u8bb8\u503c\u8303\u56f4\uff0cAQClip\u53ef\u57fa\u4e8e\u5c40\u90e8\u7ed3\u6784\u7edf\u8ba1\u6216\u6ce8\u610f\u529b\u71b5\u5f15\u5bfc\u4e24\u79cd\u6a21\u5f0f\u8fd0\u884c\u3002", "result": "\u96c6\u6210\u5230CADE 2.5\u6e32\u67d3\u7ba1\u7ebf\u540e\uff0cQSilk\u5728\u4f4e\u6b65\u6570\u91c7\u6837\u548c\u8d85\u9ad8\u5206\u8fa8\u7387\u6761\u4ef6\u4e0b\u4ea7\u751f\u66f4\u6e05\u6670\u3001\u66f4\u9510\u5229\u7684\u7ed3\u679c\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff0c\u5728SD/SDXL\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u83b7\u5f97\u4e00\u81f4\u7684\u5b9a\u6027\u6539\u8fdb\uff0c\u5e76\u80fd\u4e0eCFG/Rescale\u6280\u672f\u534f\u540c\u5de5\u4f5c\uff0c\u652f\u6301\u66f4\u9ad8\u7684\u5f15\u5bfc\u5f3a\u5ea6\u800c\u4e0d\u4ea7\u751f\u4f2a\u5f71\u3002", "conclusion": "QSilk\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u751f\u6210\u8d28\u91cf\uff0c\u5176\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u548c\u6700\u5c0f\u5316\u7528\u6237\u63a7\u5236\u4f7f\u5176\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4e3a\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u540e\u5904\u7406\u589e\u5f3a\u624b\u6bb5\u3002"}}
{"id": "2510.15841", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15841", "abs": "https://arxiv.org/abs/2510.15841", "authors": ["Jiayi Lin", "Jiabo Huang", "Shaogang Gong"], "title": "Neuro-Symbolic Spatial Reasoning in Segmentation", "comment": null, "summary": "Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from\nan open set of categories, requiring generalization to unseen and unlabelled\nobjects. Using vision-language models (VLMs) to correlate local image patches\nwith potential unseen object categories suffers from a lack of understanding of\nspatial relations of objects in a scene. To solve this problem, we introduce\nneuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary\nVLM correlation-based approaches, we propose Relational Segmentor (RelateSeg)\nto impose explicit spatial relational constraints by first order logic (FOL)\nformulated in a neural network architecture. This is the first attempt to\nexplore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically\nextracts spatial relations, e.g., <cat, to-right-of, person>, and encodes them\nas first-order logic formulas using our proposed pseudo categories. Each pixel\nlearns to predict both a semantic category (e.g., \"cat\") and a spatial pseudo\ncategory (e.g., \"right of person\") simultaneously, enforcing relational\nconstraints (e.g., a \"cat\" pixel must lie to the right of a \"person\"). Finally,\nthese logic constraints are formulated in a deep network architecture by fuzzy\nlogic relaxation, enabling end-to-end learning of spatial-relationally\nconsistent segmentation. RelateSeg achieves state-of-the-art performance in\nterms of average mIoU across four benchmark datasets and particularly shows\nclear advantages on images containing multiple categories, with the cost of\nonly introducing a single auxiliary loss function and no additional parameters,\nvalidating the effectiveness of NeSy spatial reasoning in OVSS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RelateSeg\uff0c\u8fd9\u662f\u9996\u4e2a\u5728\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4e2d\u63a2\u7d22\u795e\u7ecf\u7b26\u53f7\u7a7a\u95f4\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u9636\u903b\u8f91\u516c\u5f0f\u5728\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u65bd\u52a0\u663e\u5f0f\u7a7a\u95f4\u5173\u7cfb\u7ea6\u675f\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5173\u8054\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u573a\u666f\u4e2d\u7269\u4f53\u7a7a\u95f4\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u5bfc\u81f4\u5728\u672a\u89c1\u548c\u672a\u6807\u8bb0\u5bf9\u8c61\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u53d7\u9650\uff0c\u9700\u8981\u89e3\u51b3\u7a7a\u95f4\u5173\u7cfb\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Relational Segmentor (RelateSeg)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u9636\u903b\u8f91\u516c\u5f0f\u5728\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u65bd\u52a0\u663e\u5f0f\u7a7a\u95f4\u5173\u7cfb\u7ea6\u675f\uff0c\u81ea\u52a8\u63d0\u53d6\u7a7a\u95f4\u5173\u7cfb\u5e76\u7f16\u7801\u4e3a\u903b\u8f91\u516c\u5f0f\uff0c\u6bcf\u4e2a\u50cf\u7d20\u540c\u65f6\u9884\u6d4b\u8bed\u4e49\u7c7b\u522b\u548c\u7a7a\u95f4\u4f2a\u7c7b\u522b\uff0c\u6700\u540e\u901a\u8fc7\u6a21\u7cca\u903b\u8f91\u677e\u5f1b\u5728\u6df1\u5ea6\u7f51\u7edc\u67b6\u6784\u4e2d\u5b9e\u73b0\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "result": "RelateSeg\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5e73\u5747mIoU\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5305\u542b\u591a\u4e2a\u7c7b\u522b\u7684\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u660e\u663e\u4f18\u52bf\uff0c\u4ec5\u5f15\u5165\u5355\u4e2a\u8f85\u52a9\u635f\u5931\u51fd\u6570\u4e14\u4e0d\u589e\u52a0\u989d\u5916\u53c2\u6570\uff0c\u9a8c\u8bc1\u4e86\u795e\u7ecf\u7b26\u53f7\u7a7a\u95f4\u63a8\u7406\u5728\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u795e\u7ecf\u7b26\u53f7\u7a7a\u95f4\u63a8\u7406\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u663e\u5f0f\u7a7a\u95f4\u5173\u7cfb\u7ea6\u675f\u5b9e\u73b0\u4e86\u7a7a\u95f4\u5173\u7cfb\u4e00\u81f4\u7684\u8bed\u4e49\u5206\u5272\uff0c\u4e3a\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.15857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15857", "abs": "https://arxiv.org/abs/2510.15857", "authors": ["Jiuhai Chen", "Le Xue", "Zhiyang Xu", "Xichen Pan", "Shusheng Yang", "Can Qin", "An Yan", "Honglu Zhou", "Zeyuan Chen", "Lifu Huang", "Tianyi Zhou", "Junnan Li", "Silvio Savarese", "Caiming Xiong", "Ran Xu"], "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation", "comment": null, "summary": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3\nseries that advances the next frontier of native image generation. BLIP3o-NEXT\nunifies text-to-image generation and image editing within a single\narchitecture, demonstrating strong image generation and image editing\ncapabilities. In developing the state-of-the-art native image generation model,\nwe identify four key insights: (1) Most architectural choices yield comparable\nperformance; an architecture can be deemed effective provided it scales\nefficiently and supports fast inference; (2) The successful application of\nreinforcement learning can further push the frontier of native image\ngeneration; (3) Image editing still remains a challenging task, yet instruction\nfollowing and the consistency between generated and reference images can be\nsignificantly enhanced through post-training and data engine; (4) Data quality\nand scale continue to be decisive factors that determine the upper bound of\nmodel performance. Building upon these insights, BLIP3o-NEXT leverages an\nAutoregressive + Diffusion architecture in which an autoregressive model first\ngenerates discrete image tokens conditioned on multimodal inputs, whose hidden\nstates are then used as conditioning signals for a diffusion model to generate\nhigh-fidelity images. This architecture integrates the reasoning strength and\ninstruction following of autoregressive models with the fine-detail rendering\nability of diffusion models, achieving a new level of coherence and realism.\nExtensive evaluations of various text-to-image and image-editing benchmarks\nshow that BLIP3o-NEXT achieves superior performance over existing models.", "AI": {"tldr": "BLIP3o-NEXT\u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684\u56fe\u50cf\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u56de\u5f52+\u6269\u6563\u6df7\u5408\u67b6\u6784\u7edf\u4e00\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u56fe\u50cf\u7f16\u8f91\u529f\u80fd\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u7edf\u4e00\u67b6\u6784\u4e0b\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\u7684\u6311\u6218\uff0c\u63a2\u7d22\u539f\u751f\u56fe\u50cf\u751f\u6210\u7684\u524d\u6cbf\u6280\u672f\u8fb9\u754c\u3002", "method": "BLIP3o-NEXT\u91c7\u7528\u81ea\u56de\u5f52+\u6269\u6563\u6df7\u5408\u67b6\u6784\uff0c\u5176\u4e2d\u81ea\u56de\u5f52\u6a21\u578b\u9996\u5148\u751f\u6210\u57fa\u4e8e\u591a\u6a21\u6001\u8f93\u5165\u7684\u79bb\u6563\u56fe\u50cf\u4ee4\u724c\uff0c\u5176\u9690\u85cf\u72b6\u6001\u968f\u540e\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u8c03\u8282\u4fe1\u53f7\u6765\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\uff0c\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u6269\u6563\u6a21\u578b\u7684\u7ec6\u8282\u6e32\u67d3\u4f18\u52bf\u3002", "result": "\u5728\u591a\u79cd\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\uff0cBLIP3o-NEXT\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e0a\u7684\u5f3a\u5927\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u56db\u4e2a\u5173\u952e\u6d1e\u5bdf\uff1a\u67b6\u6784\u9009\u62e9\u5bf9\u6027\u80fd\u5f71\u54cd\u8f83\u5c0f\u4f46\u9700\u5173\u6ce8\u6269\u5c55\u6548\u7387\uff1b\u5f3a\u5316\u5b66\u4e60\u80fd\u63a8\u52a8\u539f\u751f\u56fe\u50cf\u751f\u6210\u524d\u6cbf\uff1b\u901a\u8fc7\u540e\u8bad\u7ec3\u548c\u6570\u636e\u5f15\u64ce\u53ef\u663e\u8457\u63d0\u5347\u56fe\u50cf\u7f16\u8f91\u80fd\u529b\uff1b\u6570\u636e\u8d28\u91cf\u548c\u89c4\u6a21\u4ecd\u662f\u51b3\u5b9a\u6a21\u578b\u6027\u80fd\u4e0a\u9650\u7684\u51b3\u5b9a\u6027\u56e0\u7d20\u3002"}}
{"id": "2510.15866", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.15866", "abs": "https://arxiv.org/abs/2510.15866", "authors": ["Kaushitha Silva", "Mansitha Eashwara", "Sanduni Ubayasiri", "Ruwan Tennakoon", "Damayanthi Herath"], "title": "BiomedXPro: Prompt Optimization for Explainable Diagnosis with Biomedical Vision Language Models", "comment": "10 Pages + 15 Supplementary Material Pages, 5 figures", "summary": "The clinical adoption of biomedical vision-language models is hindered by\nprompt optimization techniques that produce either uninterpretable latent\nvectors or single textual prompts. This lack of transparency and failure to\ncapture the multi-faceted nature of clinical diagnosis, which relies on\nintegrating diverse observations, limits their trustworthiness in high-stakes\nsettings. To address this, we introduce BiomedXPro, an evolutionary framework\nthat leverages a large language model as both a biomedical knowledge extractor\nand an adaptive optimizer to automatically generate a diverse ensemble of\ninterpretable, natural-language prompt pairs for disease diagnosis. Experiments\non multiple biomedical benchmarks show that BiomedXPro consistently outperforms\nstate-of-the-art prompt-tuning methods, particularly in data-scarce few-shot\nsettings. Furthermore, our analysis demonstrates a strong semantic alignment\nbetween the discovered prompts and statistically significant clinical features,\ngrounding the model's performance in verifiable concepts. By producing a\ndiverse ensemble of interpretable prompts, BiomedXPro provides a verifiable\nbasis for model predictions, representing a critical step toward the\ndevelopment of more trustworthy and clinically-aligned AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BiomedXPro\u8fdb\u5316\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u63d0\u53d6\u5668\u548c\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff0c\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u53ef\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5bf9\u96c6\u5408\uff0c\u7528\u4e8e\u75be\u75c5\u8bca\u65ad\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u751f\u7269\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u5e94\u7528\u53d7\u5230\u9650\u5236\uff0c\u56e0\u4e3a\u73b0\u6709\u7684\u63d0\u793a\u4f18\u5316\u6280\u672f\u8981\u4e48\u4ea7\u751f\u4e0d\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u5411\u91cf\uff0c\u8981\u4e48\u4ec5\u751f\u6210\u5355\u4e00\u6587\u672c\u63d0\u793a\u3002\u8fd9\u79cd\u7f3a\u4e4f\u900f\u660e\u5ea6\u4ee5\u53ca\u65e0\u6cd5\u6355\u6349\u4e34\u5e8a\u8bca\u65ad\u591a\u9762\u6027\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u9ad8\u98ce\u9669\u533b\u7597\u73af\u5883\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "BiomedXPro\u91c7\u7528\u8fdb\u5316\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u63d0\u53d6\u5668\u548c\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff0c\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u53ef\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5bf9\u96c6\u5408\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u7b56\u7565\uff0c\u786e\u4fdd\u751f\u6210\u7684\u63d0\u793a\u80fd\u591f\u6355\u6349\u4e34\u5e8a\u8bca\u65ad\u7684\u591a\u65b9\u9762\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u751f\u7269\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBiomedXPro\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002\u5206\u6790\u663e\u793a\u53d1\u73b0\u7684\u63d0\u793a\u4e0e\u7edf\u8ba1\u663e\u8457\u7684\u4e34\u5e8a\u7279\u5f81\u4e4b\u95f4\u5b58\u5728\u5f3a\u8bed\u4e49\u5bf9\u9f50\uff0c\u4e3a\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u6982\u5ff5\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u53ef\u89e3\u91ca\u63d0\u793a\u96c6\u5408\uff0cBiomedXPro\u4e3a\u6a21\u578b\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u57fa\u7840\uff0c\u4ee3\u8868\u4e86\u5411\u5f00\u53d1\u66f4\u53ef\u4fe1\u4e14\u4e34\u5e8a\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u8fc8\u51fa\u7684\u5173\u952e\u4e00\u6b65\u3002\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002"}}
