<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-20.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 34]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 7]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 6]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-gazegovernance-aware-pre-annotation-for-zero-shot-world-model-environments">[1] <a href="https://arxiv.org/abs/2510.14992">GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments</a></h3>
<p><em>Leela Krishna, Mengyang Zhao, Saicharithreddy Pasula, Harshit Rajgarhia, Abhishek Mukherji</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºGAZEçš„ç”Ÿäº§çº§æµæ°´çº¿ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å°†åŸå§‹é•¿è§†é¢‘è½¬æ¢ä¸ºç”¨äºä¸–ç•Œæ¨¡å‹è®­ç»ƒçš„ä¸°å¯Œç›‘ç£æ•°æ®ï¼Œé€šè¿‡å¤šæ¨¡æ€é¢„æ ‡æ³¨å’Œç»“æ„åŒ–è¾“å‡ºæ˜¾è‘—æå‡äº†æ•°æ®æ ‡æ³¨æ•ˆç‡å’Œè´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è®­ç»ƒé²æ£’çš„ä¸–ç•Œæ¨¡å‹éœ€è¦å¤§è§„æ¨¡ã€ç²¾ç¡®æ ‡æ³¨çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä½†ä¼ ç»Ÿæ‰‹åŠ¨æ ‡æ³¨è¿‡ç¨‹å­˜åœ¨é€Ÿåº¦æ…¢ã€æˆæœ¬é«˜çš„ç“¶é¢ˆé—®é¢˜ï¼Œé™åˆ¶äº†é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„è·å–æ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> è¯¥ç³»ç»Ÿé‡‡ç”¨ä¸‰é˜¶æ®µå¤„ç†æµç¨‹ï¼šé¦–å…ˆå°†ä¸“æœ‰çš„360åº¦è§†é¢‘æ ¼å¼æ ‡å‡†åŒ–ä¸ºæ ‡å‡†è§†å›¾å¹¶è¿›è¡Œåˆ†ç‰‡å¹¶è¡Œå¤„ç†ï¼›ç„¶ååº”ç”¨ä¸€ç³»åˆ—AIæ¨¡å‹è¿›è¡Œå¯†é›†å¤šæ¨¡æ€é¢„æ ‡æ³¨ï¼ŒåŒ…æ‹¬åœºæ™¯ç†è§£ã€ç›®æ ‡è·Ÿè¸ªã€éŸ³é¢‘è½¬å½•ä»¥åŠPII/NSFW/æœªæˆå¹´äººæ£€æµ‹ï¼›æœ€åå°†ä¿¡å·æ•´åˆä¸ºç»“æ„åŒ–è¾“å‡ºè§„èŒƒä»¥ä¾¿å¿«é€Ÿäººå·¥éªŒè¯ã€‚</p>
<p><strong>Result:</strong> GAZEå·¥ä½œæµç¨‹å®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œæ¯å®¡æ ¸å°æ—¶å¯èŠ‚çœçº¦19åˆ†é’Ÿæ—¶é—´ï¼Œé€šè¿‡ä¿å®ˆè‡ªåŠ¨è·³è¿‡ä½æ˜¾è‘—æ€§ç‰‡æ®µå°†äººå·¥å®¡æ ¸é‡å‡å°‘è¶…è¿‡80%ï¼ŒåŒæ—¶æé«˜äº†æ ‡ç­¾å¯†åº¦å’Œä¸€è‡´æ€§ï¼Œå¹¶é›†æˆäº†éšç§ä¿æŠ¤å’Œç›‘ç®¡é“¾å…ƒæ•°æ®ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•ä¸ºç”Ÿæˆé«˜è´¨é‡ä¸–ç•Œæ¨¡å‹è®­ç»ƒæ•°æ®æä¾›äº†å¯æ‰©å±•çš„è“å›¾ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒååé‡å’Œæ²»ç†è¦æ±‚çš„åŒæ—¶ï¼Œç›´æ¥ç”Ÿæˆé€‚ç”¨äºå­¦ä¹ è·¨æ¨¡æ€åŠ¨æ€å’ŒåŠ¨ä½œæ¡ä»¶é¢„æµ‹çš„é«˜ä¿çœŸã€éšç§æ„ŸçŸ¥æ•°æ®é›†ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Training robust world models requires large-scale, precisely labeled
multimodal datasets, a process historically bottlenecked by slow and expensive
manual annotation. We present a production-tested GAZE pipeline that automates
the conversion of raw, long-form video into rich, task-ready supervision for
world-model training. Our system (i) normalizes proprietary 360-degree formats
into standard views and shards them for parallel processing; (ii) applies a
suite of AI models (scene understanding, object tracking, audio transcription,
PII/NSFW/minor detection) for dense, multimodal pre-annotation; and (iii)
consolidates signals into a structured output specification for rapid human
validation.
  The GAZE workflow demonstrably yields efficiency gains (~19 minutes saved per
review hour) and reduces human review volume by &gt;80% through conservative
auto-skipping of low-salience segments. By increasing label density and
consistency while integrating privacy safeguards and chain-of-custody metadata,
our method generates high-fidelity, privacy-aware datasets directly consumable
for learning cross-modal dynamics and action-conditioned prediction. We detail
our orchestration, model choices, and data dictionary to provide a scalable
blueprint for generating high-quality world model training data without
sacrificing throughput or governance.</p>
<h3 id="2-deleaker-dynamic-inference-time-reweighting-for-semantic-leakage-mitigation-in-text-to-image-models">[2] <a href="https://arxiv.org/abs/2510.15015">DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models</a></h3>
<p><em>Mor Ventura, Michael Toker, Or Patashnik, Yonatan Belinkov, Roi Reichart</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDeLeakerï¼Œä¸€ç§è½»é‡çº§ã€æ— éœ€ä¼˜åŒ–çš„æ¨ç†æ—¶æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥å¹²é¢„æ¨¡å‹æ³¨æ„åŠ›å›¾æ¥ç¼“è§£æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­çš„è¯­ä¹‰æ³„æ¼é—®é¢˜ï¼Œåœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶æœ‰æ•ˆæŠ‘åˆ¶å®ä½“é—´çš„æ„å¤–ç‰¹å¾ä¼ é€’ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹è™½ç„¶å‘å±•è¿…é€Ÿï¼Œä½†ä»ç„¶å®¹æ˜“å—åˆ°è¯­ä¹‰æ³„æ¼çš„å½±å“ï¼Œå³ä¸åŒå®ä½“ä¹‹é—´æ„å¤–ä¼ é€’è¯­ä¹‰ç›¸å…³ç‰¹å¾ï¼Œç°æœ‰ç¼“è§£ç­–ç•¥é€šå¸¸åŸºäºä¼˜åŒ–æˆ–ä¾èµ–å¤–éƒ¨è¾“å…¥ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> DeLeakeråœ¨æ‰©æ•£è¿‡ç¨‹ä¸­åŠ¨æ€é‡æ–°åŠ æƒæ³¨æ„åŠ›å›¾ï¼ŒæŠ‘åˆ¶è¿‡åº¦çš„è·¨å®ä½“äº¤äº’åŒæ—¶å¢å¼ºæ¯ä¸ªå®ä½“çš„èº«ä»½ç‰¹å¾ï¼Œè¯¥æ–¹æ³•æ— éœ€ä¼˜åŒ–ä¸”ä¸ä¾èµ–å¤–éƒ¨ä¿¡æ¯ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜DeLeakeråœ¨æ‰€æœ‰åŸºçº¿æ–¹æ³•ä¸­è¡¨ç°æœ€ä¼˜ï¼Œå³ä½¿åŸºçº¿æ–¹æ³•ä½¿ç”¨å¤–éƒ¨ä¿¡æ¯ï¼ŒDeLeakerä¹Ÿèƒ½åœ¨ä¸æŸå®³ä¿çœŸåº¦æˆ–è´¨é‡çš„æƒ…å†µä¸‹æœ‰æ•ˆç¼“è§£è¯­ä¹‰æ³„æ¼ï¼ŒåŒæ—¶ä½œè€…è¿˜æ„å»ºäº†é¦–ä¸ªä¸“é—¨ç”¨äºè¯­ä¹‰æ³„æ¼è¯„ä¼°çš„æ•°æ®é›†SLIMã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ³¨æ„åŠ›æ§åˆ¶çš„ä»·å€¼ï¼Œä¸ºå¼€å‘æ›´è¯­ä¹‰ç²¾ç¡®çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹é“ºå¹³äº†é“è·¯ï¼Œè¯æ˜äº†ç›´æ¥å¹²é¢„æ³¨æ„åŠ›æœºåˆ¶åœ¨è§£å†³è¯­ä¹‰æ³„æ¼é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable
to semantic leakage, the unintended transfer of semantically related features
between distinct entities. Existing mitigation strategies are often
optimization-based or dependent on external inputs. We introduce DeLeaker, a
lightweight, optimization-free inference-time approach that mitigates leakage
by directly intervening on the model's attention maps. Throughout the diffusion
process, DeLeaker dynamically reweights attention maps to suppress excessive
cross-entity interactions while strengthening the identity of each entity. To
support systematic evaluation, we introduce SLIM (Semantic Leakage in IMages),
the first dataset dedicated to semantic leakage, comprising 1,130
human-verified samples spanning diverse scenarios, together with a novel
automatic evaluation framework. Experiments demonstrate that DeLeaker
consistently outperforms all baselines, even when they are provided with
external information, achieving effective leakage mitigation without
compromising fidelity or quality. These results underscore the value of
attention control and pave the way for more semantically precise T2I models.</p>
<h3 id="3-urbanverse-scaling-urban-simulation-by-watching-city-tour-videos">[3] <a href="https://arxiv.org/abs/2510.15018">UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos</a></h3>
<p><em>Mingxuan Liu, Honglin He, Elisa Ricci, Wayne Wu, Bolei Zhou</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UrbanVerseï¼Œä¸€ä¸ªæ•°æ®é©±åŠ¨çš„çœŸå®åˆ°ä»¿çœŸç³»ç»Ÿï¼Œå¯å°†ä¼—åŒ…åŸå¸‚æ¸¸è§ˆè§†é¢‘è½¬æ¢ä¸ºç‰©ç†æ„ŸçŸ¥çš„äº¤äº’å¼ä»¿çœŸåœºæ™¯ï¼Œç”¨äºè®­ç»ƒåŸå¸‚å…·èº«AIä»£ç†ã€‚è¯¥ç³»ç»ŸåŒ…å«10ä¸‡+æ ‡æ³¨çš„3DåŸå¸‚èµ„äº§åº“å’Œè‡ªåŠ¨åœºæ™¯ç”Ÿæˆæµæ°´çº¿ï¼Œæ˜¾è‘—æå‡äº†å¯¼èˆªç­–ç•¥åœ¨ä»¿çœŸå’Œé›¶æ ·æœ¬çœŸå®ä¸–ç•Œè¿ç§»ä¸­çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„äººå·¥åˆ¶ä½œæˆ–ç¨‹åºç”Ÿæˆçš„ä»¿çœŸåœºæ™¯åœ¨å¯æ‰©å±•æ€§å’ŒçœŸå®ä¸–ç•Œå¤æ‚æ€§æ•æ‰æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³æ—¥ç›Šå¢é•¿çš„åŸå¸‚å…·èº«AIä»£ç†è®­ç»ƒéœ€æ±‚ï¼Œè¿™äº›ä»£ç†éœ€è¦åœ¨æ··ä¹±çš„åŸå¸‚è¡—é“ä¸­å¯¼èˆªä»¥æä¾›æœ€åä¸€å…¬é‡Œè¿æ¥æœåŠ¡ã€‚</p>
<p><strong>Method:</strong> UrbanVerseç³»ç»ŸåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šUrbanVerse-100Kï¼ˆåŒ…å«10ä¸‡+å¸¦æœ‰è¯­ä¹‰å’Œç‰©ç†å±æ€§æ ‡æ³¨çš„3DåŸå¸‚èµ„äº§åº“ï¼‰å’ŒUrbanVerse-Genï¼ˆä»è§†é¢‘ä¸­æå–åœºæ™¯å¸ƒå±€å¹¶ä½¿ç”¨æ£€ç´¢èµ„äº§å®ä¾‹åŒ–åº¦é‡å°ºåº¦3Dä»¿çœŸçš„è‡ªåŠ¨æµæ°´çº¿ï¼‰ï¼Œç³»ç»Ÿè¿è¡Œåœ¨IsaacSimå¹³å°ä¸Šã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜UrbanVerseåœºæ™¯ä¿æŒäº†çœŸå®ä¸–ç•Œçš„è¯­ä¹‰å’Œå¸ƒå±€ï¼Œåœ¨äººç±»è¯„ä¼°çš„çœŸå®æ„Ÿæ–¹é¢ä¸äººå·¥åˆ¶ä½œåœºæ™¯ç›¸å½“ã€‚åœ¨å¯¼èˆªä»»åŠ¡ä¸­ï¼Œåœ¨UrbanVerseä¸­è®­ç»ƒçš„ç­–ç•¥å±•ç°å‡ºç¼©æ”¾å¹‚å¾‹å’Œå¼ºæ³›åŒ–èƒ½åŠ›ï¼Œä»¿çœŸä¸­æˆåŠŸç‡æå‡+6.3%ï¼Œé›¶æ ·æœ¬ä»¿çœŸåˆ°çœŸå®è¿ç§»ä¸­æå‡+30.1%ï¼Œä»…éœ€ä¸¤æ¬¡å¹²é¢„å³å¯å®Œæˆ300ç±³çœŸå®ä¸–ç•Œä»»åŠ¡ã€‚</p>
<p><strong>Conclusion:</strong> UrbanVerseè¯æ˜äº†æ•°æ®é©±åŠ¨çš„çœŸå®åˆ°ä»¿çœŸæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£å†³åŸå¸‚ä»¿çœŸåœºæ™¯çš„å¯æ‰©å±•æ€§å’ŒçœŸå®æ€§é—®é¢˜ï¼Œä¸ºå…·èº«AIä»£ç†çš„è®­ç»ƒæä¾›äº†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„åŸå¸‚ç¯å¢ƒï¼Œæ˜¾è‘—æå‡äº†å¯¼èˆªç­–ç•¥åœ¨çœŸå®ä¸–ç•Œä¸­çš„éƒ¨ç½²æ€§èƒ½ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Urban embodied AI agents, ranging from delivery robots to quadrupeds, are
increasingly populating our cities, navigating chaotic streets to provide
last-mile connectivity. Training such agents requires diverse, high-fidelity
urban environments to scale, yet existing human-crafted or procedurally
generated simulation scenes either lack scalability or fail to capture
real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim
system that converts crowd-sourced city-tour videos into physics-aware,
interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a
repository of 100k+ annotated urban 3D assets with semantic and physical
attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene
layouts from video and instantiates metric-scale 3D simulations using retrieved
assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed
scenes from 24 countries, along with a curated benchmark of 10 artist-designed
test scenes. Experiments show that UrbanVerse scenes preserve real-world
semantics and layouts, achieving human-evaluated realism comparable to manually
crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit
scaling power laws and strong generalization, improving success by +6.3% in
simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior
methods, accomplishing a 300 m real-world mission with only two interventions.</p>
<h3 id="4-mobius-big-to-mobile-universal-instance-segmentation-via-multi-modal-bottleneck-fusion-and-calibrated-decoder-pruning">[4] <a href="https://arxiv.org/abs/2510.15026">MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning</a></h3>
<p><em>Mattia Segu, Marta Tintore Gazulla, Yongqin Xian, Luc Van Gool, Federico Tombari</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>MOBIUSæ˜¯ä¸€ä¸ªç”¨äºé€šç”¨å®ä¾‹åˆ†å‰²çš„åŸºç¡€æ¨¡å‹å®¶æ—ï¼Œé€šè¿‡ç“¶é¢ˆåƒç´ è§£ç å™¨ã€è¯­è¨€å¼•å¯¼ä¸ç¡®å®šæ€§æ ¡å‡†æŸå¤±å’Œç»Ÿä¸€è®­ç»ƒç­–ç•¥ï¼Œåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå®ç°äº†ä»é«˜ç«¯åŠ é€Ÿå™¨åˆ°ç§»åŠ¨è®¾å¤‡çš„å¸•ç´¯æ‰˜æœ€ä¼˜éƒ¨ç½²ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºç¡€æ¨¡å‹è™½ç„¶åœ¨å…¨æ™¯åˆ†å‰²å’Œç›®æ ‡æ£€æµ‹é¢†åŸŸå®ç°äº†æœ€å…ˆè¿›çš„åŸŸå†…å’Œé›¶æ ·æœ¬æ€§èƒ½ï¼Œä½†å…¶é«˜è®¡ç®—æˆæœ¬é™åˆ¶äº†åœ¨èµ„æºå—é™å¹³å°ä¸Šçš„éƒ¨ç½²åº”ç”¨ï¼Œéœ€è¦è§£å†³æ¨¡å‹æ•ˆç‡ä¸æ€§èƒ½ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ç“¶é¢ˆåƒç´ è§£ç å™¨ç”¨äºé«˜æ•ˆçš„å¤šå°ºåº¦å’Œå¤šæ¨¡æ€èåˆï¼Œè®¾è®¡äº†è¯­è¨€å¼•å¯¼ä¸ç¡®å®šæ€§æ ¡å‡†æŸå¤±ä»¥å®ç°è‡ªé€‚åº”è§£ç å™¨å‰ªæï¼Œå¹¶é‡‡ç”¨ç®€åŒ–çš„ç»Ÿä¸€è®­ç»ƒç­–ç•¥æ¥é™ä½è®­ç»ƒå’Œæ¨ç†éœ€æ±‚ã€‚</p>
<p><strong>Result:</strong> MOBIUSå°†åƒç´ å’ŒTransformerè§£ç å™¨çš„FLOPsåˆ†åˆ«é™ä½äº†55%å’Œ75%ï¼Œä»…ç”¨ä¸‰åˆ†ä¹‹ä¸€çš„è®­ç»ƒè¿­ä»£æ¬¡æ•°å°±ä¿æŒäº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œåœ¨é«˜æ•ˆåˆ†å‰²ä»»åŠ¡ä¸Šä¸ºé«˜æ€§èƒ½è®¡ç®—å¹³å°å’Œç§»åŠ¨è®¾å¤‡å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŸºç¡€æ¨¡å‹å¯ä»¥åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹å®ç°æ˜¾è‘—çš„è®¡ç®—æ•ˆç‡æå‡ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„å®ä¾‹åˆ†å‰²åº”ç”¨æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†é«˜æ•ˆAIæ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®é™…éƒ¨ç½²ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Scaling up model size and training data has advanced foundation models for
instance-level perception, achieving state-of-the-art in-domain and zero-shot
performance across object detection and segmentation. However, their high
computational cost limits adoption on resource-constrained platforms. We first
examine the limitations of existing architectures in enabling efficient edge
deployment without compromising performance. We then introduce MOBIUS, a family
of foundation models for universal instance segmentation, designed for
Pareto-optimal downscaling to support deployment across devices ranging from
high-end accelerators to mobile hardware. To reduce training and inference
demands, we propose: (i) a bottleneck pixel decoder for efficient multi-scale
and multi-modal fusion, (ii) a language-guided uncertainty calibration loss for
adaptive decoder pruning, and (iii) a streamlined, unified training strategy.
Unlike efficient baselines that trade accuracy for reduced complexity, MOBIUS
reduces pixel and transformer decoder FLOPs by up to 55% and 75%, respectively,
while maintaining state-of-the-art performance in just a third of the training
iterations. MOBIUS establishes a new benchmark for efficient segmentation on
both high-performance computing platforms and mobile devices.</p>
<h3 id="5-composition-grounded-instruction-synthesis-for-visual-reasoning">[5] <a href="https://arxiv.org/abs/2510.15040">Composition-Grounded Instruction Synthesis for Visual Reasoning</a></h3>
<p><em>Xinyi Gu, Jiayuan Mao, Zhang-Wei Hong, Zhuoran Yu, Pengyuan Li, Dhiraj Joshi, Rogerio Feris, Zexue He</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†COGSæ¡†æ¶ï¼Œé€šè¿‡å°†ç§å­é—®é¢˜åˆ†è§£ä¸ºæ„ŸçŸ¥å’Œæ¨ç†å› å­å¹¶è¿›è¡Œç³»ç»Ÿé‡ç»„ï¼Œä»è€Œä»å°‘é‡ç§å­æ•°æ®ä¸­é«˜æ•ˆç”Ÿæˆå¤§è§„æ¨¡åˆæˆé—®ç­”å¯¹ï¼Œæ˜¾è‘—æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾è¡¨ç­‰äººå·¥å›¾åƒé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é¢„è®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šæ ·åŒ–å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ ‡æ³¨æ•°æ®éš¾ä»¥è·å–çš„äººå·¥å›¾åƒé¢†åŸŸï¼ˆå¦‚å›¾è¡¨ã€æ¸²æŸ“æ–‡æ¡£å’Œç½‘é¡µï¼‰çš„æ¨ç†èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œè¿™äº›é¢†åŸŸåœ¨å®è·µä¸­ä¸°å¯Œä½†ç¼ºä¹å¤§è§„æ¨¡äººå·¥æ ‡æ³¨çš„æ¨ç†æ•°æ®é›†ã€‚</p>
<p><strong>Method:</strong> COGSæ¡†æ¶å°†æ¯ä¸ªç§å­é—®é¢˜åˆ†è§£ä¸ºåŸå§‹æ„ŸçŸ¥å’Œæ¨ç†å› å­ï¼Œç„¶åç³»ç»Ÿæ€§åœ°ä¸æ–°å›¾åƒé‡æ–°ç»„åˆç”Ÿæˆå¤§é‡åˆæˆé—®ç­”å¯¹ï¼Œæ¯ä¸ªç”Ÿæˆé—®é¢˜éƒ½é…æœ‰å­é—®é¢˜å’Œä¸­é—´ç­”æ¡ˆï¼Œæ”¯æŒåŸºäºå› å­çº§è¿‡ç¨‹å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> åœ¨å›¾è¡¨æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCOGSæ˜¾è‘—æå‡äº†æœªè§é—®é¢˜çš„æ€§èƒ½ï¼Œåœ¨æ¨ç†å¯†é›†å’Œç»„åˆæ€§é—®é¢˜ä¸Šçš„æå‡æœ€å¤§ï¼Œä½¿ç”¨ä¸åŒç§å­æ•°æ®çš„å› å­çº§æ··åˆè®­ç»ƒåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ›´å¥½çš„è¿ç§»æ€§èƒ½ï¼Œè¡¨æ˜COGSè¯±å¯¼äº†æ³›åŒ–èƒ½åŠ›è€Œéæ•°æ®é›†ç‰¹å®šçš„è¿‡æ‹Ÿåˆã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜COGSæ¡†æ¶èƒ½å¤Ÿä»å°‘é‡ç§å­æ•°æ®ä¸­é«˜æ•ˆæ‰©å±•å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸”è¯¥æ–¹æ³•å¯æ‰©å±•åˆ°å›¾è¡¨ä¹‹å¤–çš„ç½‘é¡µç­‰å…¶ä»–é¢†åŸŸï¼Œä¸ºæ•°æ®ç¨€ç¼ºé¢†åŸŸçš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Pretrained multi-modal large language models (MLLMs) demonstrate strong
performance on diverse multimodal tasks, but remain limited in reasoning
capabilities for domains where annotations are difficult to collect. In this
work, we focus on artificial image domains such as charts, rendered documents,
and webpages, which are abundant in practice yet lack large-scale human
annotated reasoning datasets. We introduce COGS (COmposition-Grounded
instruction Synthesis), a data-efficient framework for equipping MLLMs with
advanced reasoning abilities from a small set of seed questions. The key idea
is to decompose each seed question into primitive perception and reasoning
factors, which can then be systematically recomposed with new images to
generate large collections of synthetic question-answer pairs. Each generated
question is paired with subquestions and intermediate answers, enabling
reinforcement learning with factor-level process rewards. Experiments on chart
reasoning show that COGS substantially improves performance on unseen
questions, with the largest gains on reasoning-heavy and compositional
questions. Moreover, training with a factor-level mixture of different seed
data yields better transfer across multiple datasets, suggesting that COGS
induces generalizable capabilities rather than dataset-specific overfitting. We
further demonstrate that the framework extends beyond charts to other domains
such as webpages.</p>
<h3 id="6-comprehensive-language-image-pre-training-for-3d-medical-image-understanding">[6] <a href="https://arxiv.org/abs/2510.15042">Comprehensive language-image pre-training for 3D medical image understanding</a></h3>
<p><em>Tassilo Wald, Ibrahim Ethem Hamamci, Yuan Gao, Sam Bond-Taylor, Harshita Sharma, Maximilian Ilse, Cynthia Lo, Olesya Melnichenko, Noel C. F. Codella, Maria Teodora Wetscherek, Klaus H. Maier-Hein, Panagiotis Korfiatis, Valentina Salvatelli, Javier Alvarez-Valle, Fernando PÃ©rez-GarcÃ­a</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†COLIPRIç¼–ç å™¨å®¶æ—ï¼Œé€šè¿‡å¼•å…¥æŠ¥å‘Šç”Ÿæˆç›®æ ‡å’Œç»“åˆè§†è§‰-è¯­è¨€é¢„è®­ç»ƒä¸çº¯è§†è§‰é¢„è®­ç»ƒæ¥è§£å†³3DåŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒä¸­æ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰3DåŒ»å­¦è§†è§‰è¯­è¨€ç¼–ç å™¨é¢ä¸´æ•°æ®å¯ç”¨æ€§é™åˆ¶çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æ”¯æŒæ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œç›¸ä¼¼å¼‚å¸¸ç—…ä¾‹æ£€ç´¢å’Œå¼‚å¸¸å¯èƒ½æ€§é¢„æµ‹ç­‰æ–¹é¢çš„èƒ½åŠ›ï¼Œéœ€è¦è§£å†³æ•°æ®ç¨€ç¼ºå¯¹æ¨¡å‹æ€§èƒ½çš„åˆ¶çº¦ã€‚</p>
<p><strong>Method:</strong> é€šè¿‡å¼•å…¥é¢å¤–çš„å½’çº³åç½®æ¥è§£å†³æ•°æ®ä¸è¶³é—®é¢˜ï¼ŒåŒ…æ‹¬å¼•å…¥æŠ¥å‘Šç”Ÿæˆç›®æ ‡ä»¥åŠå°†è§†è§‰è¯­è¨€é¢„è®­ç»ƒä¸çº¯è§†è§‰é¢„è®­ç»ƒç›¸ç»“åˆï¼Œä»è€Œèƒ½å¤ŸåŒæ—¶åˆ©ç”¨çº¯å›¾åƒå’Œé…å¯¹å›¾åƒæ–‡æœ¬3Dæ•°æ®é›†ï¼Œå¹¶éµå¾ª3DåŒ»å­¦å½±åƒé¢†åŸŸçš„æœ€ä½³å®è·µå¼€å‘äº†COLIPRIç¼–ç å™¨å®¶æ—ã€‚</p>
<p><strong>Result:</strong> COLIPRIç¼–ç å™¨åœ¨æŠ¥å‘Šç”Ÿæˆã€åˆ†ç±»æ¢æµ‹å’Œé›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šä¿æŒäº†ç«äº‰åŠ›ï¼Œè¯æ˜äº†æ‰€ææ–¹æ³•åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡å¼•å…¥é¢å¤–çš„å½’çº³åç½®å’Œç»“åˆä¸åŒç±»å‹çš„é¢„è®­ç»ƒæ•°æ®ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£3DåŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„åŒ»å­¦å½±åƒåˆ†æå·¥å…·æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œå¹¶å±•ç¤ºäº†åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹æå‡æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆç­–ç•¥ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Vision-language pre-training, i.e., aligning images with paired text, is a
powerful paradigm to create encoders that can be directly used for tasks such
as classification and retrieval, and for downstream tasks such as segmentation
and report generation. In the 3D medical image domain, these capabilities allow
vision-language encoders (VLEs) to support radiologists by retrieving patients
with similar abnormalities or predicting likelihoods of abnormality. While the
methodology holds promise, data availability limits the capabilities of current
3D VLEs.
  In this paper, we alleviate the lack of data by injecting additional
inductive biases: introducing a report generation objective and pairing
vision-language pre-training with vision-only pre-training. This allows us to
leverage both image-only and paired image-text 3D datasets, increasing the
total amount of data to which our model is exposed. Through these additional
inductive biases, paired with best practices of the 3D medical imaging domain,
we develop the Comprehensive Language-image Pre-training (COLIPRI) encoder
family. Our COLIPRI encoders achieve state-of-the-art performance in report
generation, classification probing, and zero-shot classification, and remain
competitive for semantic segmentation.</p>
<h3 id="7-directional-reasoning-injection-for-fine-tuning-mllms">[7] <a href="https://arxiv.org/abs/2510.15050">Directional Reasoning Injection for Fine-Tuning MLLMs</a></h3>
<p><em>Chao Huang, Zeliang Zhang, Jiang Liu, Ximeng Sun, Jialian Wu, Xiaodong Yu, Ze Wang, Chenliang Xu, Emad Barsoum, Zicheng Liu</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDRIFTæ–¹æ³•ï¼Œé€šè¿‡æ¢¯åº¦ç©ºé—´ä¸­çš„æ¨ç†çŸ¥è¯†è½¬ç§»æ¥å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€å¤§è§„æ¨¡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨å¤šä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šå®ç°æ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›é€šå¸¸è½åäºçº¯æ–‡æœ¬æ¨¡å‹ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–èµ„æºå¯†é›†å‹çš„å¤§è§„æ¨¡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ï¼Œè€Œç®€å•çš„æ¨¡å‹èåˆæ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å®¶æ—ä¸­æ•ˆæœä¸ç¨³å®šï¼Œéƒ¨åˆ†æ¨¡å‹ç”šè‡³å‡ºç°æ€§èƒ½ä¸‹é™ã€‚</p>
<p><strong>Method:</strong> æå‡ºDRIFTæ–¹æ³•ï¼Œé€šè¿‡é¢„è®¡ç®—æ¨ç†å…ˆéªŒä½œä¸ºæ¨ç†å˜ä½“ä¸å¤šæ¨¡æ€å˜ä½“åœ¨å‚æ•°ç©ºé—´çš„å·®å¼‚ï¼Œç„¶ååœ¨å¤šæ¨¡æ€å¾®è°ƒè¿‡ç¨‹ä¸­åˆ©ç”¨è¯¥å…ˆéªŒæ¥åç½®æ¢¯åº¦ï¼Œå®ç°åœ¨æ¢¯åº¦ç©ºé—´ä¸­çš„æ¨ç†çŸ¥è¯†è½¬ç§»ï¼ŒåŒæ—¶ä¿æŒå¤šæ¨¡æ€å¯¹é½çš„ç¨³å®šæ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨MathVistaå’ŒMathVerseç­‰å¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDRIFTæ–¹æ³•ç›¸æ¯”æœ´ç´ èåˆå’Œç›‘ç£å¾®è°ƒæŒç»­æå‡æ¨ç†æ€§èƒ½ï¼ŒåŒæ—¶ä»¥æä½æˆæœ¬è¾¾åˆ°æˆ–è¶…è¶Šè®­ç»ƒå¯†é›†å‹æ–¹æ³•çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> DRIFTæä¾›äº†ä¸€ç§è½»é‡çº§ä¸”é«˜æ•ˆçš„æ¨ç†èƒ½åŠ›å¢å¼ºæ–¹æ¡ˆï¼Œè¯æ˜äº†åœ¨æ¢¯åº¦ç©ºé—´è¿›è¡ŒçŸ¥è¯†è½¬ç§»çš„å¯è¡Œæ€§ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„èƒ½åŠ›æå‡å¼€è¾Ÿäº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼ŒåŒæ—¶ä¿æŒäº†æ ‡å‡†ç›‘ç£å¾®è°ƒæµç¨‹çš„ç®€æ´æ€§ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) are rapidly advancing, yet their
reasoning ability often lags behind that of strong text-only counterparts.
Existing methods to bridge this gap rely on supervised fine-tuning over
large-scale multimodal reasoning data or reinforcement learning, both of which
are resource-intensive. A promising alternative is model merging, which
interpolates parameters between reasoning-enhanced LLMs and multimodal
variants. However, our analysis shows that naive merging is not always a "free
lunch": its effectiveness varies drastically across model families, with some
(e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance
degradation. To address this, we propose Directional Reasoning Injection for
Fine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning
knowledge in the gradient space, without destabilizing multimodal alignment.
DRIFT precomputes a reasoning prior as the parameter-space difference between
reasoning and multimodal variants, then uses it to bias gradients during
multimodal fine-tuning. This approach preserves the simplicity of standard
supervised fine-tuning pipelines while enabling efficient reasoning transfer.
Extensive experiments on multimodal reasoning benchmarks, including MathVista
and MathVerse, demonstrate that DRIFT consistently improves reasoning
performance over naive merging and supervised fine-tuning, while matching or
surpassing training-heavy methods at a fraction of the cost.</p>
<h3 id="8-tgt-text-grounded-trajectories-for-locally-controlled-video-generation">[8] <a href="https://arxiv.org/abs/2510.15104">TGT: Text-Grounded Trajectories for Locally Controlled Video Generation</a></h3>
<p><em>Guofeng Zhang, Angtian Wang, Jacob Zhiyuan Fang, Liming Jiang, Haotian Yang, Bo Liu, Yiding Yang, Guang Chen, Longyin Wen, Alan Yuille, Chongyang Ma</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†æ–‡æœ¬å¼•å¯¼è½¨è¿¹ï¼ˆTGTï¼‰æ¡†æ¶ï¼Œé€šè¿‡å°†ç‚¹è½¨è¿¹ä¸å±€éƒ¨æ–‡æœ¬æè¿°é…å¯¹æ¥æ§åˆ¶è§†é¢‘ç”Ÿæˆï¼Œè§£å†³äº†å¤šå¯¹è±¡åœºæ™¯ä¸‹ç°æœ‰æ–¹æ³•æ§åˆ¶ç²¾åº¦ä¸è¶³å’Œè½¨è¿¹-å®ä½“å¯¹åº”å…³ç³»æ¨¡ç³Šçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€æ–‡æœ¬å¯¹é½å’Œè¿åŠ¨å¯æ§æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•åœ¨æ§åˆ¶ç”Ÿæˆåœºæ™¯çš„ä¸»ä½“æ„å›¾æ–¹é¢èƒ½åŠ›æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å¤šå¯¹è±¡è®¾ç½®ä¸‹ï¼Œç°æœ‰åŸºäºè¾¹ç•Œæ¡†æˆ–åˆ†å‰²æ©ç çš„å±€éƒ¨æ–‡æœ¬æ§åˆ¶æ–¹æ³•ç²¾åº¦ä¸è¶³ï¼Œä¸”éšç€å¯æ§å¯¹è±¡æ•°é‡å¢åŠ ï¼Œä¸ªä½“è½¨è¿¹ä¸è§†è§‰å®ä½“ä¹‹é—´çš„å¯¹åº”å…³ç³»å˜å¾—æ¨¡ç³Šä¸æ¸…ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†æ–‡æœ¬å¼•å¯¼è½¨è¿¹ï¼ˆTGTï¼‰æ¡†æ¶ï¼Œé‡‡ç”¨ä½ç½®æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›ï¼ˆLACAï¼‰æ¥æ•´åˆè½¨è¿¹å’Œå±€éƒ¨æ–‡æœ¬æè¿°ä¿¡å·ï¼Œå¹¶é‡‡ç”¨åŒé‡åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ï¼ˆdual-CFGï¼‰æ–¹æ¡ˆåˆ†åˆ«è°ƒèŠ‚å±€éƒ¨å’Œå…¨å±€æ–‡æœ¬æŒ‡å¯¼ã€‚å¼€å‘äº†æ•°æ®å¤„ç†æµæ°´çº¿æ¥ç”Ÿæˆå¸¦æœ‰è·Ÿè¸ªå®ä½“å±€éƒ¨æè¿°ç¬¦çš„è½¨è¿¹ï¼Œå¹¶æ ‡æ³¨äº†200ä¸‡ä¸ªé«˜è´¨é‡è§†é¢‘ç‰‡æ®µç”¨äºè®­ç»ƒTGTã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTGTåœ¨è§†è§‰è´¨é‡ã€æ–‡æœ¬å¯¹é½å‡†ç¡®æ€§å’Œè¿åŠ¨å¯æ§æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿä½¿ç”¨ç‚¹è½¨è¿¹ä½œä¸ºç›´è§‚çš„è¿åŠ¨æ§åˆ¶æ‰‹æŸ„ï¼Œå°†æ¯ä¸ªè½¨è¿¹ä¸æ–‡æœ¬é…å¯¹ä»¥åŒæ—¶æ§åˆ¶å¤–è§‚å’Œè¿åŠ¨ã€‚</p>
<p><strong>Conclusion:</strong> TGTæ¡†æ¶é€šè¿‡å°†è½¨è¿¹ä¸å±€éƒ¨æ–‡æœ¬æè¿°ç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘ç”Ÿæˆä¸­å¤šå¯¹è±¡åœºæ™¯çš„æ§åˆ¶ç²¾åº¦å’Œçµæ´»æ€§ï¼Œä¸ºå¤æ‚åœºæ™¯çš„ç²¾ç¡®å¯æ§è§†é¢‘ç”Ÿæˆæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†è½¨è¿¹ä½œä¸ºè¿åŠ¨æ§åˆ¶æ‰‹æŸ„çš„ç›´è§‚æ€§å’Œå®ç”¨æ€§ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Text-to-video generation has advanced rapidly in visual fidelity, whereas
standard methods still have limited ability to control the subject composition
of generated scenes. Prior work shows that adding localized text control
signals, such as bounding boxes or segmentation masks, can help. However, these
methods struggle in complex scenarios and degrade in multi-object settings,
offering limited precision and lacking a clear correspondence between
individual trajectories and visual entities as the number of controllable
objects increases. We introduce Text-Grounded Trajectories (TGT), a framework
that conditions video generation on trajectories paired with localized text
descriptions. We propose Location-Aware Cross-Attention (LACA) to integrate
these signals and adopt a dual-CFG scheme to separately modulate local and
global text guidance. In addition, we develop a data processing pipeline that
produces trajectories with localized descriptions of tracked entities, and we
annotate two million high quality video clips to train TGT. Together, these
components enable TGT to use point trajectories as intuitive motion handles,
pairing each trajectory with text to control both appearance and motion.
Extensive experiments show that TGT achieves higher visual quality, more
accurate text alignment, and improved motion controllability compared with
prior approaches. Website: https://textgroundedtraj.github.io.</p>
<h3 id="9-xmodbench-benchmarking-cross-modal-capabilities-and-consistency-in-omni-language-models">[9] <a href="https://arxiv.org/abs/2510.15148">XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models</a></h3>
<p><em>Xingrui Wang, Jiang Liu, Chao Huang, Xiaodong Yu, Ze Wang, Ximeng Sun, Jialian Wu, Alan Yuille, Emad Barsoum, Zicheng Liu</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†XModBenchï¼Œä¸€ä¸ªå¤§è§„æ¨¡ä¸‰æ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è·¨æ¨¡æ€ä¸€è‡´æ€§ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æ¨¡æ€ä¸å˜æ¨ç†æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºå‡†ä¸»è¦è¯„ä¼°å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é€šç”¨è·¨æ¨¡æ€é—®ç­”èƒ½åŠ›ï¼Œä½†æ— æ³•ç¡®å®šè¿™äº›æ¨¡å‹æ˜¯å¦çœŸæ­£å®ç°äº†æ¨¡æ€ä¸å˜æ¨ç†æˆ–å­˜åœ¨æ¨¡æ€ç‰¹å®šåå·®ï¼Œå› æ­¤éœ€è¦ä¸“é—¨çš„è¯Šæ–­å·¥å…·æ¥ç³»ç»Ÿè¯„ä¼°è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«60,828ä¸ªå¤šé€‰é¢˜çš„XModBenchåŸºå‡†ï¼Œæ¶µç›–äº”ä¸ªä»»åŠ¡å®¶æ—å¹¶ç³»ç»Ÿè¦†ç›–æ‰€æœ‰å…­ç§æ¨¡æ€ç»„åˆçš„é—®ç­”å¯¹ï¼Œèƒ½å¤Ÿå¯¹æ¨¡å‹çš„æ¨¡æ€ä¸å˜æ¨ç†èƒ½åŠ›ã€æ¨¡æ€å·®å¼‚å’Œæ–¹å‘ä¸å¹³è¡¡è¿›è¡Œç»†ç²’åº¦è¯Šæ–­ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œå³ä½¿æœ€å¼ºçš„Gemini 2.5 Proæ¨¡å‹åœ¨ç©ºé—´å’Œæ—¶é—´æ¨ç†ä»»åŠ¡ä¸Šå‡†ç¡®ç‡ä½äº60%ï¼Œå­˜åœ¨æŒç»­çš„æ¨¡æ€å·®å¼‚ï¼ˆéŸ³é¢‘æ¨¡æ€æ€§èƒ½æ˜¾è‘—ä½äºæ–‡æœ¬ï¼‰ï¼Œå¹¶è¡¨ç°å‡ºç³»ç»Ÿæ€§æ–¹å‘ä¸å¹³è¡¡ï¼ˆè§†è§‰ä½œä¸ºä¸Šä¸‹æ–‡æ—¶ä¸€è‡´æ€§ä½äºæ–‡æœ¬ï¼‰ã€‚</p>
<p><strong>Conclusion:</strong> å½“å‰å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è·ç¦»çœŸæ­£çš„æ¨¡æ€ä¸å˜æ¨ç†ä»æœ‰å¾ˆå¤§å·®è·ï¼ŒXModBenchå¯ä½œä¸ºè¯„ä¼°å’Œæ”¹è¿›è·¨æ¨¡æ€èƒ½åŠ›çš„åŸºç¡€è¯Šæ–­å·¥å…·ï¼Œä¸ºæœªæ¥æ¨¡å‹å¼€å‘æä¾›é‡è¦åŸºå‡†ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Omni-modal large language models (OLLMs) aim to unify audio, vision, and text
understanding within a single framework. While existing benchmarks primarily
evaluate general cross-modal question-answering ability, it remains unclear
whether OLLMs achieve modality-invariant reasoning or exhibit modality-specific
biases. We introduce XModBench, a large-scale tri-modal benchmark explicitly
designed to measure cross-modal consistency. XModBench comprises 60,828
multiple-choice questions spanning five task families and systematically covers
all six modality compositions in question-answer pairs, enabling fine-grained
diagnosis of an OLLM's modality-invariant reasoning, modality disparity, and
directional imbalance. Experiments show that even the strongest model, Gemini
2.5 Pro, (i) struggles with spatial and temporal reasoning, achieving less than
60% accuracy, (ii) reveals persistent modality disparities, with performance
dropping substantially when the same semantic content is conveyed through audio
rather than text, and (iii) shows systematic directional imbalance, exhibiting
lower consistency when vision serves as context compared to text. These
findings indicate that current OLLMs remain far from truly modality-invariant
reasoning and position XModBench as a fundamental diagnostic tool for
evaluating and improving cross-modal competence. All data and evaluation tools
will be available at https://xingruiwang.github.io/projects/XModBench/.</p>
<h3 id="10-train-a-unified-multimodal-data-quality-classifier-with-synthetic-data">[10] <a href="https://arxiv.org/abs/2510.15162">Train a Unified Multimodal Data Quality Classifier with Synthetic Data</a></h3>
<p><em>Weizhi Wang, Rongmei Lin, Shiyang Li, Colin Lockard, Ritesh Sarkhel, Sanket Lokegaonkar, Jingbo Shang, Xifeng Yan, Nasser Zalmout, Xian Li</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºUniFilterï¼Œä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ•°æ®è´¨é‡åˆ†ç±»å™¨ï¼Œç”¨äºç­›é€‰é«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬æè¿°æ•°æ®å’Œäº¤é”™æ–‡æ¡£æ•°æ®ã€‚é€šè¿‡åœ¨UniFilterç­›é€‰çš„æ•°æ®ä¸Šé¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å±•ç°å‡ºæ˜¾è‘—å¢å¼ºçš„é›¶æ ·æœ¬æ¨ç†å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒ-æ–‡æœ¬æè¿°æ•°æ®å’Œäº¤é”™æ–‡æ¡£æ•°æ®çš„æ··åˆé¢„è®­ç»ƒä¸­ï¼Œé«˜è´¨é‡æ•°æ®ç­›é€‰æ–¹æ³•å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒ-æ–‡æœ¬äº¤é”™æ–‡æ¡£æ•°æ®çš„è´¨é‡è¯„ä¼°æ–¹é¢å­˜åœ¨ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> æå‡ºè®­ç»ƒé«˜æ•ˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºç»Ÿä¸€å¤šæ¨¡æ€æ•°æ®è´¨é‡åˆ†ç±»å™¨ï¼Œé‡‡ç”¨åŠåˆæˆæ–¹æ³•åˆ©ç”¨åŸå§‹å›¾åƒç”Ÿæˆå››ç§è´¨é‡çº§åˆ«çš„å¯¹åº”æ–‡æœ¬ï¼Œä¸ºæè¿°å’Œäº¤é”™æ–‡æ¡£æ•°æ®åˆ›å»ºæ ·æœ¬-è¯„åˆ†å¯¹æ¥è®­ç»ƒUniFilteræ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> åº”ç”¨UniFilterä»DataCompæè¿°æ•°æ®é›†å’ŒOBELICSå›¾åƒ-æ–‡æœ¬äº¤é”™æ•°æ®é›†ä¸­ç­›é€‰é«˜è´¨é‡æ•°æ®ï¼Œé¢„è®­ç»ƒçš„MLLMç›¸æ¯”åŸºçº¿æ–¹æ³•å±•ç°å‡ºæ˜¾è‘—å¢å¼ºçš„é›¶æ ·æœ¬æ¨ç†å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œç»è¿‡è§†è§‰ç›‘ç£å¾®è°ƒååœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ›´ä¼˜ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜é«˜è´¨é‡å¤šæ¨¡æ€é¢„è®­ç»ƒæ•°æ®å¯¹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å…·æœ‰æ˜¾è‘—æå‡ä½œç”¨ï¼Œé€šè¿‡UniFilterç­›é€‰çš„æ•°æ®èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºMLLMçš„èƒ½åŠ›ï¼Œä¸ºç¤¾åŒºæä¾›äº†å¯å¤ç°çš„æ•°æ®ç­›é€‰æ–¹æ³•å’Œé«˜è´¨é‡æ•°æ®é›†èµ„æºã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>The Multimodal Large Language Models (MLLMs) are continually pre-trained on a
mixture of image-text caption data and interleaved document data, while the
high-quality data filtering towards image-text interleaved document data is
under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal
Data Quality Classifier to Filter both high-quality image-text caption and
interleaved data (UniFilter). To address the challenge of collecting diverse
labeled multimodal data, we introduce a semi-synthetic approach that leverages
readily available raw images and generates corresponding text across four
quality levels. This method enables efficient creation of sample-score pairs
for both caption and interleaved document data to train UniFilter. We apply
UniFilter to curate high-quality caption data from DataComp caption dataset and
interleaved data from the OBELICS image-text interleaved dataset. MLLMs
pre-trained on the filtered data demonstrate significantly enhanced
capabilities compared to those trained on baseline-filtered data, achieving
stronger zero-shot reasoning and in-context learning capabilities. After visual
supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger
performance on various benchmarks, highlighting the downstream benefits of
high-quality multimodal pre-training. We release the synthetic training data
used for training UniFilter, the UniFilter model checkpoints, and the
high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to
the community for reproduction and further development.</p>
<h3 id="11-spatial457-a-diagnostic-benchmark-for-6d-spatial-reasoning-of-large-multimodal-models">[11] <a href="https://arxiv.org/abs/2502.08636">Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models</a></h3>
<p><em>Xingrui Wang, Wufei Ma, Tiezheng Zhang, Celso M de Melo, Jieneng Chen, Alan Yuille</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Spatial457æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•ä¸”æ— åçš„åˆæˆæ•°æ®é›†ï¼Œç”¨äºå…¨é¢è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨6Dç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨3Dç©ºé—´æ¨ç†æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰åœºæ™¯ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨å¤æ‚ç²¾ç¡®çš„3ç»´ç©ºé—´æ¨ç†èƒ½åŠ›ä»ä¸ç¡®å®šï¼Œç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨2Dç©ºé—´ç†è§£ï¼Œç¼ºä¹è¯„ä¼°6Dç©ºé—´æ¨ç†çš„å…¨é¢æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> å¼€å‘äº†Spatial457åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«4ä¸ªå…³é”®ç©ºé—´æ¨ç†èƒ½åŠ›ï¼šå¤šç›®æ ‡è¯†åˆ«ã€2Då®šä½ã€3Då®šä½å’Œ3Dæ–¹å‘ï¼Œé‡‡ç”¨çº§è”è¯„ä¼°ç»“æ„æ„å»ºäº†7ç§é—®é¢˜ç±»å‹å’Œ5ä¸ªéš¾åº¦çº§åˆ«ï¼Œä»åŸºç¡€å•ç›®æ ‡è¯†åˆ«åˆ°æ–°æå‡ºçš„å¤æ‚6Dç©ºé—´æ¨ç†ä»»åŠ¡ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°å‘ç°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨PulseCheck457ä¸Šæ€§èƒ½éšä»»åŠ¡å¤æ‚åº¦å¢åŠ è€Œæ™®éä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨3Dæ¨ç†å’Œ6Dç©ºé—´ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå¼•å…¥ç›¸å¯¹æ€§èƒ½ä¸‹é™ç‡é‡åŒ–äº†3Dæ¨ç†èƒ½åŠ›çš„å…³é”®å¼±ç‚¹ï¼Œå¹¶æ­ç¤ºäº†ä¸åŒå±æ€§é—´çš„é¢„æµ‹åå·®ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨3Dç©ºé—´æ¨ç†æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ï¼Œæå‡ºçš„æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ä¸ºæœªæ¥æ¨¡å‹å¼€å‘æä¾›äº†é‡è¦åŸºå‡†ï¼Œå‘ç°çš„é¢„æµ‹åå·®æ¨¡å¼åœ¨çœŸå®å›¾åƒè®¾ç½®ä¸­åŒæ ·å­˜åœ¨ï¼Œå¼ºè°ƒäº†æ”¹è¿›3Dç©ºé—´ç†è§£èƒ½åŠ›çš„å¿…è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Although large multimodal models (LMMs) have demonstrated remarkable
capabilities in visual scene interpretation and reasoning, their capacity for
complex and precise 3-dimensional spatial reasoning remains uncertain. Existing
benchmarks focus predominantly on 2D spatial understanding and lack a framework
to comprehensively evaluate 6D spatial reasoning across varying complexities.
To address this limitation, we present Spatial457, a scalable and unbiased
synthetic dataset designed with 4 key capability for spatial reasoning:
multi-object recognition, 2D location, 3D location, and 3D orientation. We
develop a cascading evaluation structure, constructing 7 question types across
5 difficulty levels that range from basic single object recognition to our new
proposed complex 6D spatial reasoning tasks. We evaluated various large
multimodal models (LMMs) on PulseCheck457, observing a general decline in
performance as task complexity increases, particularly in 3D reasoning and 6D
spatial tasks. To quantify these challenges, we introduce the Relative
Performance Dropping Rate (RPDR), highlighting key weaknesses in 3D reasoning
capabilities. Leveraging the unbiased attribute design of our dataset, we also
uncover prediction biases across different attributes, with similar patterns
observed in real-world image settings. The code and data are released in
https://github.com/XingruiWang/Spatial457.</p>
<h3 id="12-hyperparameter-optimization-and-reproducibility-in-deep-learning-model-training">[12] <a href="https://arxiv.org/abs/2510.15164">Hyperparameter Optimization and Reproducibility in Deep Learning Model Training</a></h3>
<p><em>Usman Afzaal, Ziyu Su, Usama Sajjad, Hao Lu, Mostafa Rezapour, Metin Nafi Gurcan, Muhammad Khalid Khan Niazi</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†ç»„ç»‡ç—…ç†å­¦åŸºç¡€æ¨¡å‹è®­ç»ƒä¸­çš„å¯å¤ç°æ€§æŒ‘æˆ˜ï¼Œé€šè¿‡CLIPæ¨¡å‹åœ¨QUILT-1Mæ•°æ®é›†ä¸Šçš„è®­ç»ƒå®éªŒï¼Œç¡®å®šäº†å…³é”®è¶…å‚æ•°å’Œå¢å¼ºç­–ç•¥å¯¹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„å½±å“ï¼Œä¸ºæ•°å­—ç—…ç†å­¦é¢†åŸŸæä¾›äº†å®ç”¨çš„å¯å¤ç°æ€§æŒ‡å¯¼åŸåˆ™ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç»„ç»‡ç—…ç†å­¦åŸºç¡€æ¨¡å‹è®­ç»ƒé¢ä¸´ä¸¥é‡çš„å¯å¤ç°æ€§é—®é¢˜ï¼Œä¸»è¦éšœç¢åŒ…æ‹¬è½¯ä»¶éšæœºæ€§ã€ç¡¬ä»¶éç¡®å®šæ€§å’Œè¶…å‚æ•°æŠ¥å‘Šä¸ä¸€è‡´ï¼Œè¿™é˜»ç¢äº†ç ”ç©¶ç»“æœçš„å¯é æ¯”è¾ƒå’ŒéªŒè¯ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶åœ¨QUILT-1Mæ•°æ®é›†ä¸Šè®­ç»ƒCLIPæ¨¡å‹ï¼Œç³»ç»Ÿè¯„ä¼°äº†ä¸åŒè¶…å‚æ•°è®¾ç½®å’Œæ•°æ®å¢å¼ºç­–ç•¥åœ¨ä¸‰ä¸ªä¸‹æ¸¸ç»„ç»‡ç—…ç†å­¦æ•°æ®é›†ï¼ˆPatchCamelyonã€LC25000-Lungå’ŒLC25000-Colonï¼‰ä¸Šçš„å½±å“ã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°RandomResizedCropå€¼åœ¨0.7-0.8èŒƒå›´å†…è¡¨ç°æœ€ä½³ï¼Œåˆ†å¸ƒå¼è®­ç»ƒä¸ä½¿ç”¨å±€éƒ¨æŸå¤±æé«˜äº†ç¨³å®šæ€§ï¼Œå­¦ä¹ ç‡ä½äº5.0e-5åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå…¶ä¸­LC25000ï¼ˆColonï¼‰æ•°æ®é›†æä¾›äº†æœ€å¯é çš„å¯å¤ç°æ€§åŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> è®¡ç®—ç—…ç†å­¦çš„å¯å¤ç°æ€§ä¸ä»…ä¾èµ–äºé€æ˜æ–‡æ¡£è®°å½•ï¼Œæ›´éœ€è¦ç²¾å¿ƒé€‰æ‹©çš„å®éªŒé…ç½®ï¼Œç ”ç©¶ç»“æœä¸ºå¼€å‘å¯å¤ç°çš„æ•°å­—ç—…ç†å­¦åŸºç¡€æ¨¡å‹æä¾›äº†å®ç”¨çš„æŒ‡å¯¼åŸåˆ™å’Œé…ç½®å»ºè®®ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Reproducibility remains a critical challenge in foundation model training for
histopathology, often hindered by software randomness, hardware
non-determinism, and inconsistent hyperparameter reporting. To investigate
these issues, we trained a CLIP model on the QUILT-1M dataset and
systematically evaluated the impact of different hyperparameter settings and
augmentation strategies across three downstream histopathology datasets
(PatchCamelyon, LC25000-Lung, and LC25000-Colon). Despite variability across
runs, we identified clear trends: RandomResizedCrop values of 0.7-0.8
outperformed more aggressive (0.6) or conservative (0.9) settings, distributed
training without local loss improved stability, and learning rates below 5.0e-5
consistently degraded performance across all datasets. The LC25000 (Colon)
dataset consistently provided the most reproducible benchmark. These findings
highlight that reproducibility in computational pathology depends not only on
transparent documentation but also on carefully chosen experimental
configurations, and we provide practical rules to guide future efforts in
developing reproducible foundation models for digital pathology.</p>
<h3 id="13-shakti-vlms-scalable-vision-language-models-for-enterprise-ai">[13] <a href="https://arxiv.org/abs/2502.17092">Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI</a></h3>
<p><em>Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>Shakti VLMæ˜¯ä¸€ä¸ªå‚æ•°è§„æ¨¡ä¸º1Bå’Œ4Bçš„è§†è§‰è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œé€šè¿‡æ¶æ„åˆ›æ–°å’Œé«˜æ•ˆè®­ç»ƒç­–ç•¥ï¼Œåœ¨å‡å°‘è®­ç»ƒæ•°æ®é‡çš„æƒ…å†µä¸‹å®ç°ç«äº‰åŠ›çš„å¤šæ¨¡æ€æ€§èƒ½ï¼Œä¸ºå¤§è§„æ¨¡ä¼ä¸šåº”ç”¨æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ•°æ®æ•ˆç‡æŒ‘æˆ˜ï¼Œå½“å‰è§†è§‰è¯­è¨€æ¨¡å‹é€šå¸¸ä¾èµ–æµ·é‡è®­ç»ƒæ•°æ®æ¥è·å¾—å¼ºæ€§èƒ½ï¼Œè€ŒShaktiæ¨¡å‹åˆ™æ¢ç´¢å¦‚ä½•é€šè¿‡æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒç­–ç•¥ä¼˜åŒ–æ¥å®ç°æ•°æ®é«˜æ•ˆçš„å¤šæ¨¡æ€å­¦ä¹ ã€‚</p>
<p><strong>Method:</strong> Shakti VLMå¼•å…¥äº†QK-Normalizationæ¥å¢å¼ºæ³¨æ„åŠ›ç¨³å®šæ€§ï¼Œé‡‡ç”¨æ··åˆå½’ä¸€åŒ–æŠ€æœ¯ï¼Œæ”¹è¿›äº†ä½ç½®ç¼–ç æ–¹æ³•ï¼Œå¹¶å®æ–½äº†ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥ä¼˜åŒ–å­¦ä¹ æ•ˆç‡ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°ç»“æœæ˜¾ç¤ºShakti-VLM-1Bå’ŒShakti-VLM-4Båœ¨æ–‡æ¡£ç†è§£ã€è§†è§‰æ¨ç†ã€OCRæå–å’Œé€šç”¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†åœ¨å‡å°‘è®­ç»ƒtokenæ•°é‡çš„æƒ…å†µä¸‹ä»èƒ½å®ç°ç«äº‰åŠ›æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜é«˜æ€§èƒ½å¯ä»¥é€šè¿‡æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒç­–ç•¥è€Œéå•çº¯ä¾èµ–æ•°æ®é‡æ¥å®ç°ï¼ŒShaktiæ¨¡å‹ä¸ºä¼ä¸šçº§å¤šæ¨¡æ€ä»»åŠ¡æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¼ºè°ƒäº†æ¶æ„åˆ›æ–°åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>We introduce Shakti VLM, a family of vision-language models in the capacity
of 1B and 4B parameters designed to address data efficiency challenges in
multimodal learning. While recent VLMs achieve strong performance through
extensive training data, Shakti models leverage architectural innovations to
attain competitive results with fewer tokens. Key advancements include
QK-Normalization for attention stability, hybrid normalization techniques, and
enhanced positional encoding. A three-stage training strategy further optimizes
learning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and
Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR
extraction, and general multimodal reasoning. Our results highlight that high
performance can be achieved through model design and training strategy rather
than sheer data volume, making Shakti an efficient solution for
enterprise-scale multimodal tasks.</p>
<h3 id="14-cardium-congenital-anomaly-recognition-with-diagnostic-images-and-unified-medical-records">[14] <a href="https://arxiv.org/abs/2510.15208">CARDIUM: Congenital Anomaly Recognition with Diagnostic Images and Unified Medical records</a></h3>
<p><em>Daniela Vega, Hannah V. Ceballos, Javier S. Vera, Santiago Rodriguez, Alejandra Perez, Angela Castillo, Maria Escobar, Dario LondoÃ±o, Luis A. Sarmiento, Camila I. Castro, Nadiezhda Rodriguez, Juan C. BriceÃ±o, Pablo ArbelÃ¡ez</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†é¦–ä¸ªå…¬å¼€çš„å¤šæ¨¡æ€æ•°æ®é›†CARDIUMï¼Œç”¨äºèƒå„¿å…ˆå¤©æ€§å¿ƒè„ç—…äº§å‰è¯Šæ–­ï¼Œå¹¶å¼€å‘äº†ä¸€ç§ç»“åˆäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„å¤šæ¨¡æ€transformeræ¶æ„ï¼Œæ˜¾è‘—æå‡äº†è¯Šæ–­æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å…ˆå¤©æ€§å¿ƒè„ç—…äº§å‰è¯Šæ–­é¢ä¸´æ•°æ®ç¨€ç¼ºã€è´¨é‡ä½ä¸‹ä»¥åŠå¤šæºä¿¡æ¯æ•´åˆä¸è¶³çš„æŒ‘æˆ˜ï¼Œç°æœ‰AIæ¨¡å‹å› æ•°æ®ä¸å¹³è¡¡å’Œå•æ¨¡æ€é™åˆ¶è€Œæ€§èƒ½å—é™ï¼Œç¼ºä¹å…¬å¼€çš„å¤šæ¨¡æ€æ•°æ®é›†é˜»ç¢äº†ç›¸å…³ç ”ç©¶è¿›å±•ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†CARDIUMå¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ•´åˆèƒå„¿è¶…å£°å’Œè¶…å£°å¿ƒåŠ¨å›¾å›¾åƒä»¥åŠæ¯ä½“ä¸´åºŠè®°å½•ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„å¤šæ¨¡æ€transformeræ¶æ„ï¼Œç”¨äºèåˆå›¾åƒå’Œè¡¨æ ¼æ•°æ®çš„ç‰¹å¾è¡¨ç¤ºã€‚</p>
<p><strong>Result:</strong> æ‰€æå‡ºçš„å¤šæ¨¡æ€æ–¹æ³•åœ¨CARDIUMæ•°æ®é›†ä¸Šå®ç°äº†79.8Â±4.8%çš„F1åˆ†æ•°ï¼Œç›¸æ¯”å•æ¨¡æ€å›¾åƒå’Œè¡¨æ ¼æ–¹æ³•åˆ†åˆ«æå‡äº†11%å’Œ50%çš„æ£€æµ‹æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡æä¾›é¦–ä¸ªå…¬å¼€å¤šæ¨¡æ€æ•°æ®é›†å’Œæœ‰æ•ˆçš„å¤šæ¨¡æ€èåˆæ–¹æ³•ï¼Œä¸ºäº§å‰å…ˆå¤©æ€§å¿ƒè„ç—…è¯Šæ–­å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œæ•°æ®é›†å’Œä»£ç çš„å…¬å¼€å°†ä¿ƒè¿›è¯¥é¢†åŸŸè¿›ä¸€æ­¥ç ”ç©¶å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Prenatal diagnosis of Congenital Heart Diseases (CHDs) holds great potential
for Artificial Intelligence (AI)-driven solutions. However, collecting
high-quality diagnostic data remains difficult due to the rarity of these
conditions, resulting in imbalanced and low-quality datasets that hinder model
performance. Moreover, no public efforts have been made to integrate multiple
sources of information, such as imaging and clinical data, further limiting the
ability of AI models to support and enhance clinical decision-making. To
overcome these challenges, we introduce the Congenital Anomaly Recognition with
Diagnostic Images and Unified Medical records (CARDIUM) dataset, the first
publicly available multimodal dataset consolidating fetal ultrasound and
echocardiographic images along with maternal clinical records for prenatal CHD
detection. Furthermore, we propose a robust multimodal transformer architecture
that incorporates a cross-attention mechanism to fuse feature representations
from image and tabular data, improving CHD detection by 11% and 50% over image
and tabular single-modality approaches, respectively, and achieving an F1 score
of 79.8 $\pm$ 4.8% in the CARDIUM dataset. We will publicly release our dataset
and code to encourage further research on this unexplored field. Our dataset
and code are available at https://github.com/BCVUniandes/Cardium, and at the
project website https://bcv-uniandes.github.io/CardiumPage/</p>
<h3 id="15-the-face-of-persuasion-analyzing-bias-and-generating-culture-aware-ads">[15] <a href="https://arxiv.org/abs/2510.15240">The Face of Persuasion: Analyzing Bias and Generating Culture-Aware Ads</a></h3>
<p><em>Aysan Aghazadeh, Adriana Kovashka</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶è°ƒæŸ¥äº†æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¹¿å‘Šå®šåˆ¶ä¸­çš„æ½œåœ¨äººå£ç»Ÿè®¡åè§ï¼Œé€šè¿‡åˆ†æä¸åŒå¹¿å‘Šä¸»é¢˜ä¸‹çš„äººå£åè§ä»¥åŠç›¸åŒå¹¿å‘Šå†…å®¹ä¸­äººç‰©æ€§åˆ«/ç§æ—å¯¹è¯´æœåŠ›çš„å·®å¼‚æ€§å½±å“ï¼Œå¹¶æ¢ç´¢äº†é’ˆå¯¹ç‰¹å®šå›½å®¶çš„å¹¿å‘Šå®šå‘æŠ€æœ¯ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å®šåˆ¶è§†è§‰å¹¿å‘Šå’Œé’ˆå¯¹ç‰¹å®šäººç¾¤æ–¹é¢å…·æœ‰å¸å¼•åŠ›ï¼Œä½†å­˜åœ¨äººå£ç»Ÿè®¡åè§çš„æ½œåœ¨é£é™©ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢è¿™ç§åè§åœ¨å¹¿å‘Šä¸­çš„è¡¨ç°åŠå…¶å¯¹è¯´æœåŠ›çš„å·®å¼‚æ€§å½±å“ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨å®éªŒæ–¹æ³•åˆ†æä¸åŒå¹¿å‘Šä¸»é¢˜ä¸‹çš„äººå£ç»Ÿè®¡åè§ï¼Œé€šè¿‡æ§åˆ¶å˜é‡æ¯”è¾ƒä»…äººç‰©æ€§åˆ«/ç§æ—ä¸åŒçš„ç›¸åŒå¹¿å‘Šçš„è¯´æœåŠ›å·®å¼‚ï¼Œå¹¶å¼€å‘äº†é’ˆå¯¹ç‰¹å®šå›½å®¶çš„å¹¿å‘Šå®šå‘æŠ€æœ¯ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘ç°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆçš„å¹¿å‘Šå­˜åœ¨æ˜¾è‘—çš„äººå£ç»Ÿè®¡åè§ï¼Œä¸åŒå¹¿å‘Šä¸»é¢˜è¡¨ç°å‡ºä¸åŒçš„åè§æ¨¡å¼ï¼Œä¸”ç›¸åŒå¹¿å‘Šå†…å®¹ä¸­äººç‰©æ€§åˆ«/ç§æ—çš„æ”¹å˜ä¼šå¯¼è‡´æ¨¡å‹åˆ¤æ–­çš„è¯´æœåŠ›å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¹¿å‘Šåº”ç”¨ä¸­å­˜åœ¨ç³»ç»Ÿæ€§çš„äººå£ç»Ÿè®¡åè§ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¸å…¬å¹³çš„å¹¿å‘ŠæŠ•æ”¾æ•ˆæœï¼Œç ”ç©¶å¼ºè°ƒäº†åœ¨AIé©±åŠ¨çš„å¹¿å‘Šç³»ç»Ÿä¸­è€ƒè™‘å…¬å¹³æ€§å’Œåè§ç¼“è§£çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Text-to-image models are appealing for customizing visual advertisements and
targeting specific populations. We investigate this potential by examining the
demographic bias within ads for different ad topics, and the disparate level of
persuasiveness (judged by models) of ads that are identical except for
gender/race of the people portrayed. We also experiment with a technique to
target ads for specific countries. The code is available at
https://github.com/aysanaghazadeh/FaceOfPersuasion</p>
<h3 id="16-drivegen3d-boosting-feed-forward-driving-scene-generation-with-efficient-video-diffusion">[16] <a href="https://arxiv.org/abs/2510.15264">DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion</a></h3>
<p><em>Weijie Wang, Jiagang Zhu, Zeyu Zhang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Haoxiao Wang, Guan Huang, Xinze Chen, Yukun Zhou, Wenkang Qin, Duochao Shi, Haoyun Li, Guanghong Jia, Jiwen Lu</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>DriveGen3Dæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆåŠ é€Ÿé•¿æ—¶è§†é¢‘ç”Ÿæˆå’Œå¤§è§„æ¨¡åŠ¨æ€åœºæ™¯é‡å»ºï¼Œå®ç°äº†é«˜è´¨é‡ã€é«˜å¯æ§æ€§çš„åŠ¨æ€3Dé©¾é©¶åœºæ™¯ç”Ÿæˆï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ã€æ—¶é—´ä¸€è‡´æ€§å’Œ3Dè¡¨ç¤ºæ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰é©¾é©¶åœºæ™¯åˆæˆæ–¹æ³•å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼šè¦ä¹ˆå› è®¡ç®—éœ€æ±‚è¿‡é«˜è€Œæ— æ³•è¿›è¡Œé•¿æ—¶é—´ç”Ÿæˆï¼Œè¦ä¹ˆä»…å…³æ³¨é•¿æ—¶é—´è§†é¢‘åˆæˆè€Œç¼ºä¹3Dè¡¨ç¤ºèƒ½åŠ›ï¼Œæˆ–è€…å±€é™äºé™æ€å•åœºæ™¯é‡å»ºã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼¥åˆè¿™ä¸€æ–¹æ³•å­¦å·®è·ï¼Œé€šè¿‡å¤šæ¨¡æ€æ¡ä»¶æ§åˆ¶å®ç°åŠ é€Ÿé•¿æ—¶è§†é¢‘ç”Ÿæˆä¸å¤§è§„æ¨¡åŠ¨æ€åœºæ™¯é‡å»ºçš„é›†æˆã€‚</p>
<p><strong>Method:</strong> DriveGen3Dé‡‡ç”¨ç»Ÿä¸€æµæ°´çº¿ï¼ŒåŒ…å«ä¸¤ä¸ªä¸“é—¨ç»„ä»¶ï¼šFastDrive-DiTâ€”â€”åŸºäºé«˜æ•ˆè§†é¢‘æ‰©æ•£å˜æ¢å™¨çš„é«˜åˆ†è¾¨ç‡ã€æ—¶é—´ä¸€è‡´è§†é¢‘åˆæˆæ¨¡å—ï¼Œæ”¯æŒæ–‡æœ¬å’Œé¸Ÿç°å›¾å¸ƒå±€å¼•å¯¼ï¼›FastRecon3Dâ€”â€”å‰é¦ˆé‡å»ºæ¨¡å—ï¼Œå¿«é€Ÿæ„å»ºè·¨æ—¶é—´çš„3Dé«˜æ–¯è¡¨ç¤ºï¼Œç¡®ä¿æ—¶ç©ºä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> è¯¥æ¡†æ¶å®ç°äº†å®æ—¶ç”Ÿæˆé•¿æ—¶é©¾é©¶è§†é¢‘ï¼ˆæœ€é«˜424Ã—800åˆ†è¾¨ç‡ï¼Œ12 FPSï¼‰åŠå¯¹åº”çš„åŠ¨æ€3Dåœºæ™¯ï¼Œåœ¨æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šè¾¾åˆ°SSIM 0.811å’ŒPSNR 22.84çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒæ—¶ä¿æŒäº†å‚æ•°æ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶è¯æ˜äº†é€šè¿‡ä¸“é—¨åŒ–ç»„ä»¶é›†æˆåŠ é€Ÿè§†é¢‘ç”Ÿæˆä¸åŠ¨æ€3Dé‡å»ºçš„å¯è¡Œæ€§ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ä»¿çœŸå’Œè™šæ‹Ÿç¯å¢ƒåˆ›å»ºæä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶å±•ç¤ºäº†å¤šæ¨¡æ€æ¡ä»¶æ§åˆ¶åœ¨å¤æ‚åœºæ™¯ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥å¤§è§„æ¨¡åŠ¨æ€åœºæ™¯åˆæˆç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>We present DriveGen3D, a novel framework for generating high-quality and
highly controllable dynamic 3D driving scenes that addresses critical
limitations in existing methodologies. Current approaches to driving scene
synthesis either suffer from prohibitive computational demands for extended
temporal generation, focus exclusively on prolonged video synthesis without 3D
representation, or restrict themselves to static single-scene reconstruction.
Our work bridges this methodological gap by integrating accelerated long-term
video generation with large-scale dynamic scene reconstruction through
multimodal conditional control. DriveGen3D introduces a unified pipeline
consisting of two specialized components: FastDrive-DiT, an efficient video
diffusion transformer for high-resolution, temporally coherent video synthesis
under text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a
feed-forward reconstruction module that rapidly builds 3D Gaussian
representations across time, ensuring spatial-temporal consistency. Together,
these components enable real-time generation of extended driving videos (up to
$424\times800$ at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM
of 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining
parameter efficiency.</p>
<h3 id="17-learning-to-detect-unknown-jailbreak-attacks-in-large-vision-language-models">[17] <a href="https://arxiv.org/abs/2510.15430">Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</a></h3>
<p><em>Shuang Liang, Zhihao Xu, Jialing Tao, Hui Xue, Xiting Wang</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºå­¦ä¹ æ£€æµ‹ï¼ˆLoDï¼‰æ¡†æ¶ï¼Œé€šè¿‡å°†é‡ç‚¹ä»æ”»å‡»ç‰¹å®šå­¦ä¹ è½¬å‘ä»»åŠ¡ç‰¹å®šå­¦ä¹ ï¼Œå‡†ç¡®æ£€æµ‹æœªçŸ¥çš„è¶Šç‹±æ”»å‡»ã€‚è¯¥æ–¹æ³•åŒ…å«å¤šæ¨¡æ€å®‰å…¨æ¦‚å¿µæ¿€æ´»å‘é‡æ¨¡å—ç”¨äºå®‰å…¨å¯¼å‘è¡¨ç¤ºå­¦ä¹ ï¼Œä»¥åŠå®‰å…¨æ¨¡å¼è‡ªç¼–ç å™¨æ¨¡å—ç”¨äºæ— ç›‘ç£æ”»å‡»åˆ†ç±»ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡è¿›è¡Œäº†å¹¿æ³›çš„å¯¹é½å·¥ä½œï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ä»ç„¶å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»ï¼Œæ„æˆä¸¥é‡çš„å®‰å…¨é£é™©ã€‚ç°æœ‰æ£€æµ‹æ–¹æ³•è¦ä¹ˆå­¦ä¹ æ”»å‡»ç‰¹å®šå‚æ•°ï¼Œé™åˆ¶äº†æ³›åŒ–åˆ°æœªè§æ”»å‡»çš„èƒ½åŠ›ï¼Œè¦ä¹ˆä¾èµ–å¯å‘å¼åŸåˆ™ï¼Œé™åˆ¶äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> æå‡ºå­¦ä¹ æ£€æµ‹ï¼ˆLoDï¼‰é€šç”¨æ¡†æ¶ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€å®‰å…¨æ¦‚å¿µæ¿€æ´»å‘é‡æ¨¡å—ç”¨äºå®‰å…¨å¯¼å‘è¡¨ç¤ºå­¦ä¹ ï¼Œä»¥åŠå®‰å…¨æ¨¡å¼è‡ªç¼–ç å™¨æ¨¡å—ç”¨äºæ— ç›‘ç£æ”»å‡»åˆ†ç±»ï¼Œå®ç°ä»æ”»å‡»ç‰¹å®šå­¦ä¹ åˆ°ä»»åŠ¡ç‰¹å®šå­¦ä¹ çš„è½¬å˜ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ ·æœªçŸ¥æ”»å‡»ä¸Šå®ç°äº†æŒç»­æ›´é«˜çš„æ£€æµ‹AUROCï¼ŒåŒæ—¶æé«˜äº†æ£€æµ‹æ•ˆç‡ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ä»»åŠ¡ç‰¹å®šå­¦ä¹ åœ¨è¶Šç‹±æ”»å‡»æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºæ›´å®‰å…¨çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†æ–°æ€è·¯ï¼Œæœªæ¥å¯æ‰©å±•åˆ°å…¶ä»–å®‰å…¨å¨èƒæ£€æµ‹åœºæ™¯ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. To address
this, existing detection methods either learn attack-specific parameters, which
hinders generalization to unseen attacks, or rely on heuristically sound
principles, which limit accuracy and efficiency. To overcome these limitations,
we propose Learning to Detect (LoD), a general framework that accurately
detects unknown jailbreak attacks by shifting the focus from attack-specific
learning to task-specific learning. This framework includes a Multi-modal
Safety Concept Activation Vector module for safety-oriented representation
learning and a Safety Pattern Auto-Encoder module for unsupervised attack
classification. Extensive experiments show that our method achieves
consistently higher detection AUROC on diverse unknown attacks while improving
efficiency. The code is available at
https://anonymous.4open.science/r/Learning-to-Detect-51CB.</p>
<h3 id="18-omnivinci-enhancing-architecture-and-data-for-omni-modal-understanding-llm">[18] <a href="https://arxiv.org/abs/2510.15870">OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM</a></h3>
<p><em>Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, Yuanhang Su, Sean Lin, An-Chieh Cheng, Zhen Wan, Jinchuan Tian, Yuming Lou, Dong Yang, Zhijian Liu, Yukang Chen, Ambrish Dantrey, Ehsan Jahangiri, Sreyan Ghosh, Daguang Xu, Ehsan Hosseini-Asl, Danial Mohseni Taheri, Vidya Murali, Sifei Liu, Jason Lu, Oluwatobi Olabiyi, Frank Wang, Rafael Valle, Bryan Catanzaro, Andrew Tao, Song Han, Jan Kautz, Hongxu Yin, Pavlo Molchanov</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†OmniVinciï¼Œä¸€ä¸ªå¼ºå¤§çš„å¼€æºå…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡åˆ›æ–°çš„æ¶æ„è®¾è®¡å’Œæ•°æ®ç­–å±•æ–¹æ³•ï¼Œåœ¨è·¨æ¨¡æ€ç†è§£ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ¨¡å‹ï¼ŒåŒæ—¶å°†è®­ç»ƒæ•°æ®é‡å‡å°‘äº†6å€ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ¨è¿›æœºå™¨æ™ºèƒ½éœ€è¦å‘å±•è·¨å¤šæ¨¡æ€çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œç±»ä¼¼äºäººç±»å¯¹ä¸–ç•Œçš„å¤šæ„Ÿå®˜æ„ŸçŸ¥ï¼Œå½“å‰ç ”ç©¶æ—¨åœ¨æ„å»ºå¼ºå¤§çš„å¼€æºå…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¥è§£å†³å¤šæ¨¡æ€å¯¹é½å’Œç†è§£çš„å…³é”®æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸‰ä¸ªå…³é”®æ¶æ„åˆ›æ–°ï¼šOmniAlignNetç”¨äºå¢å¼ºè§†è§‰å’ŒéŸ³é¢‘åµŒå…¥åœ¨å…¨æ¨¡æ€æ½œåœ¨ç©ºé—´ä¸­çš„å¯¹é½ï¼›æ—¶é—´åµŒå…¥åˆ†ç»„ç”¨äºæ•æ‰è§†è§‰å’ŒéŸ³é¢‘ä¿¡å·é—´çš„ç›¸å¯¹æ—¶é—´å¯¹é½ï¼›çº¦æŸæ—‹è½¬æ—¶é—´åµŒå…¥ç”¨äºåœ¨å…¨æ¨¡æ€åµŒå…¥ä¸­ç¼–ç ç»å¯¹æ—¶é—´ä¿¡æ¯ï¼ŒåŒæ—¶å¼€å‘äº†ç”Ÿæˆ2400ä¸‡å•æ¨¡æ€å’Œå…¨æ¨¡æ€å¯¹è¯çš„æ•°æ®ç­–å±•ä¸åˆæˆæµç¨‹ã€‚</p>
<p><strong>Result:</strong> OmniVinciåœ¨è·¨æ¨¡æ€ç†è§£ä»»åŠ¡DailyOmniä¸Šè¶…è¶ŠQwen2.5-Omniè¾¾19.05åˆ†ï¼Œåœ¨éŸ³é¢‘ä»»åŠ¡MMARä¸Šæå‡1.7åˆ†ï¼Œåœ¨è§†è§‰ä»»åŠ¡Video-MMEä¸Šæå‡3.9åˆ†ï¼Œä»…ä½¿ç”¨0.2Tè®­ç»ƒtokenï¼Œç›¸æ¯”Qwen2.5-Omniçš„1.2Tå‡å°‘äº†6å€è®­ç»ƒæ•°æ®é‡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å‘ç°ä¸åŒæ¨¡æ€åœ¨æ„ŸçŸ¥å’Œæ¨ç†è¿‡ç¨‹ä¸­ç›¸äº’å¢å¼ºï¼Œè¯æ˜äº†å…¨æ¨¡æ€æ¨¡å‹åœ¨æœºå™¨äººã€åŒ»ç–—AIå’Œæ™ºèƒ½å·¥å‚ç­‰ä¸‹æ¸¸åº”ç”¨ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆçš„å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦æ´è§ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Advancing machine intelligence requires developing the ability to perceive
across multiple modalities, much as humans sense the world. We introduce
OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We
carefully study the design choices across model architecture and data curation.
For model architecture, we present three key innovations: (i) OmniAlignNet for
strengthening alignment between vision and audio embeddings in a shared
omni-modal latent space; (ii) Temporal Embedding Grouping for capturing
relative temporal alignment between vision and audio signals; and (iii)
Constrained Rotary Time Embedding for encoding absolute temporal information in
omni-modal embeddings. We introduce a curation and synthesis pipeline that
generates 24M single-modal and omni-modal conversations. We find that
modalities reinforce one another in both perception and reasoning. Our model,
OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal
understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while
using just 0.2T training tokens - a 6 times reduction compared to
Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream
applications spanning robotics, medical AI, and smart factory.</p>
<h3 id="19-semantic4safety-causal-insights-from-zero-shot-street-view-imagery-segmentation-for-urban-road-safety">[19] <a href="https://arxiv.org/abs/2510.15434">Semantic4Safety: Causal Insights from Zero-shot Street View Imagery Segmentation for Urban Road Safety</a></h3>
<p><em>Huan Chen, Ting Han, Siyu Chen, Zhihao Guo, Yiping Chen, Meiliu Wu</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†Semantic4Safetyæ¡†æ¶ï¼Œé€šè¿‡é›¶æ ·æœ¬è¯­ä¹‰åˆ†å‰²ä»è¡—æ™¯å›¾åƒä¸­æå–å¯è§£é‡Šçš„è¡—é“æŒ‡æ ‡ï¼Œå¹¶ç»“åˆå› æœæ¨æ–­æ–¹æ³•é‡åŒ–ä¸åŒäº‹æ•…ç±»å‹çš„å› æœæ•ˆåº”ï¼Œä¸ºåŸå¸‚é“è·¯å®‰å…¨è§„åˆ’æä¾›æ•°æ®é©±åŠ¨çš„å·¥å…·ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è¡—æ™¯å›¾åƒåˆ†æä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šå¦‚ä½•æ„å»ºèƒ½å¤Ÿæ•æ‰äº‹æ•…ç›¸å…³ç‰¹å¾çš„è¡—é“çº§æŒ‡æ ‡ï¼Œä»¥åŠå¦‚ä½•é‡åŒ–è¿™äº›æŒ‡æ ‡å¯¹ä¸åŒäº‹æ•…ç±»å‹çš„å› æœå½±å“ã€‚ç°æœ‰æ–¹æ³•åœ¨ä»è¡—æ™¯æ•°æ®ä¸­æå–å¯è§£é‡Šçš„å®‰å…¨æŒ‡æ ‡å’Œå»ºç«‹å› æœè”ç³»æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºSemantic4Safetyæ¡†æ¶ï¼Œåº”ç”¨é›¶æ ·æœ¬è¯­ä¹‰åˆ†å‰²ä»è¡—æ™¯å›¾åƒä¸­æå–11ä¸ªå¯è§£é‡Šçš„è¡—é“æ™¯è§‚æŒ‡æ ‡ï¼Œå¹¶æ•´åˆé“è·¯ç±»å‹ä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é‡‡ç”¨XGBoostå¤šåˆ†ç±»å™¨ç»“åˆSHAPè¿›è¡Œç‰¹å¾è´¡çŒ®åˆ†æï¼Œä½¿ç”¨å¹¿ä¹‰å€¾å‘å¾—åˆ†åŠ æƒå’Œå¹³å‡å¤„ç†æ•ˆåº”ä¼°è®¡æ¥æ§åˆ¶æ··æ‚å› ç´ å¹¶é‡åŒ–å› æœæ•ˆåº”ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘ç°å¼‚è´¨æ€§çš„ã€äº‹æ•…ç±»å‹ç‰¹å®šçš„å› æœæ¨¡å¼ï¼šæ•æ‰åœºæ™¯å¤æ‚æ€§ã€æš´éœ²åº¦å’Œé“è·¯å‡ ä½•çš„ç‰¹å¾ä¸»å¯¼é¢„æµ‹èƒ½åŠ›ï¼›æ›´å¤§çš„å¯é©¾é©¶åŒºåŸŸå’Œåº”æ€¥ç©ºé—´é™ä½é£é™©ï¼Œè€Œè¿‡åº¦è§†è§‰å¼€æ”¾æ€§å¯èƒ½å¢åŠ é£é™©ã€‚åŸºäºçº¦30,000æ¡å¥¥æ–¯æ±€äº‹æ•…è®°å½•çš„åˆ†æéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡å°†é¢„æµ‹å»ºæ¨¡ä¸å› æœæ¨æ–­ç›¸ç»“åˆï¼ŒSemantic4Safetyæ”¯æŒé’ˆå¯¹æ€§å¹²é¢„å’Œé«˜é£é™©èµ°å»Šè¯Šæ–­ï¼Œä¸ºåŸå¸‚é“è·¯å®‰å…¨è§„åˆ’æä¾›äº†å¯æ‰©å±•çš„æ•°æ®é©±åŠ¨å·¥å…·ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«ä¸åŒç±»å‹äº‹æ•…çš„å…³é”®å½±å“å› ç´ ï¼Œä¸ºç²¾å‡†å®‰å…¨å¹²é¢„æä¾›ç§‘å­¦ä¾æ®ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Street-view imagery (SVI) offers a fine-grained lens on traffic risk, yet two
fundamental challenges persist: (1) how to construct street-level indicators
that capture accident-related features, and (2) how to quantify their causal
impacts across different accident types. To address these challenges, we
propose Semantic4Safety, a framework that applies zero-shot semantic
segmentation to SVIs to derive 11 interpretable streetscape indicators, and
integrates road type as contextual information to analyze approximately 30,000
accident records in Austin. Specifically, we train an eXtreme Gradient Boosting
(XGBoost) multi-class classifier and use Shapley Additive Explanations (SHAP)
to interpret both global and local feature contributions, and then apply
Generalized Propensity Score (GPS) weighting and Average Treatment Effect (ATE)
estimation to control confounding and quantify causal effects. Results uncover
heterogeneous, accident-type-specific causal patterns: features capturing scene
complexity, exposure, and roadway geometry dominate predictive power; larger
drivable area and emergency space reduce risk, whereas excessive visual
openness can increase it. By bridging predictive modeling with causal
inference, Semantic4Safety supports targeted interventions and high-risk
corridor diagnosis, offering a scalable, data-informed tool for urban road
safety planning.</p>
<h3 id="20-msam-multi-semantic-adaptive-mining-for-cross-modal-drone-video-text-retrieval">[20] <a href="https://arxiv.org/abs/2510.15470">MSAM: Multi-Semantic Adaptive Mining for Cross-Modal Drone Video-Text Retrieval</a></h3>
<p><em>Jinghao Huang, Yaxiong Chen, Ganchao Liu</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æå‡ºå¹¶ç ”ç©¶äº†æ— äººæœºè§†é¢‘-æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åä¸ºå¤šè¯­ä¹‰è‡ªé€‚åº”æŒ–æ˜ï¼ˆMSAMï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç»†ç²’åº¦äº¤äº’å’Œè·¨æ¨¡æ€ç‰¹å¾èåˆæœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æ— äººæœºè§†é¢‘è¯­ä¹‰æ£€ç´¢çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€æ— äººæœºæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œè§†é¢‘æ•°æ®é‡æ€¥å‰§å¢åŠ ï¼Œè¿«åˆ‡éœ€è¦é«˜æ•ˆçš„è¯­ä¹‰æ£€ç´¢æ–¹æ³•ã€‚ç°æœ‰è·¨æ¨¡æ€æ–¹æ³•ä¸»è¦é’ˆå¯¹åœ°é¢è§†è§’è®¾è®¡ï¼Œæ— æ³•æœ‰æ•ˆå»ºæ¨¡æ— äººæœºè§†é¢‘çš„ä¿¯è§†è§†è§’ã€å¼ºç»“æ„åŒè´¨æ€§å’Œç›®æ ‡ç»„åˆçš„å¤šæ ·åŒ–è¯­ä¹‰è¡¨è¾¾ç­‰ç‰¹æ€§ï¼Œå› æ­¤éœ€è¦ä¸“é—¨é’ˆå¯¹æ— äººæœºåœºæ™¯çš„æ£€ç´¢æœºåˆ¶ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†å¤šè¯­ä¹‰è‡ªé€‚åº”æŒ–æ˜ï¼ˆMSAMï¼‰æ–¹æ³•ï¼ŒåŒ…å«å¤šè¯­ä¹‰è‡ªé€‚åº”å­¦ä¹ æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€å¸§é—´å˜åŒ–å»ºæ¨¡å’Œç‰¹å®šåœºæ™¯åŒºåŸŸè¯­ä¹‰æå–æ¥å¢å¼ºå¯¹æ— äººæœºè§†é¢‘å†…å®¹çš„æ·±åº¦ç†è§£ã€‚è¯¥æ–¹æ³•æ•´åˆäº†è‡ªé€‚åº”è¯­ä¹‰æ„å»ºæ¨¡å—ã€åˆ†å¸ƒé©±åŠ¨çš„è¯­ä¹‰å­¦ä¹ é¡¹å’Œå¤šæ ·æ€§è¯­ä¹‰é¡¹ï¼Œå¹¶å¼•å…¥äº†è·¨æ¨¡æ€äº¤äº’ç‰¹å¾èåˆæ± åŒ–æœºåˆ¶ä»¥å‡å°‘å¤æ‚èƒŒæ™¯å¹²æ‰°ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸¤ä¸ªè‡ªå»ºçš„æ— äººæœºè§†é¢‘-æ–‡æœ¬æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMSAMæ–¹æ³•åœ¨æ— äººæœºè§†é¢‘-æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­ä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºæ— äººæœºè§†é¢‘è¯­ä¹‰æ£€ç´¢æä¾›äº†ä¸“é—¨çš„è§£å†³æ–¹æ¡ˆï¼Œè¯æ˜äº†é’ˆå¯¹æ— äººæœºè§†é¢‘ç‰¹æ€§çš„å®šåˆ¶åŒ–æ£€ç´¢æœºåˆ¶çš„å¿…è¦æ€§ã€‚æå‡ºçš„MSAMæ–¹æ³•é€šè¿‡æ·±åº¦æ¨¡æ€äº¤äº’å’ŒèƒŒæ™¯å™ªå£°æŠ‘åˆ¶ï¼Œä¸ºæ— äººæœºè§†é¢‘åˆ†æé¢†åŸŸå¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œç›¸å…³ä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€ä»¥ä¿ƒè¿›åç»­ç ”ç©¶ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>With the advancement of drone technology, the volume of video data increases
rapidly, creating an urgent need for efficient semantic retrieval. We are the
first to systematically propose and study the drone video-text retrieval (DVTR)
task. Drone videos feature overhead perspectives, strong structural
homogeneity, and diverse semantic expressions of target combinations, which
challenge existing cross-modal methods designed for ground-level views in
effectively modeling their characteristics. Therefore, dedicated retrieval
mechanisms tailored for drone scenarios are necessary. To address this issue,
we propose a novel approach called Multi-Semantic Adaptive Mining (MSAM). MSAM
introduces a multi-semantic adaptive learning mechanism, which incorporates
dynamic changes between frames and extracts rich semantic information from
specific scene regions, thereby enhancing the deep understanding and reasoning
of drone video content. This method relies on fine-grained interactions between
words and drone video frames, integrating an adaptive semantic construction
module, a distribution-driven semantic learning term and a diversity semantic
term to deepen the interaction between text and drone video modalities and
improve the robustness of feature representation. To reduce the interference of
complex backgrounds in drone videos, we introduce a cross-modal interactive
feature fusion pooling mechanism that focuses on feature extraction and
matching in target regions, minimizing noise effects. Extensive experiments on
two self-constructed drone video-text datasets show that MSAM outperforms other
existing methods in the drone video-text retrieval task. The source code and
dataset will be made publicly available.</p>
<h3 id="21-exploring-conditions-for-diffusion-models-in-robotic-control">[21] <a href="https://arxiv.org/abs/2510.15510">Exploring Conditions for Diffusion models in Robotic Control</a></h3>
<p><em>Heeseong Shin, Byeongho Heo, Dongyoon Han, Seungryong Kim, Taekyung Kim</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ORCAæ–¹æ³•ï¼Œé€šè¿‡å¯å­¦ä¹ çš„ä»»åŠ¡æç¤ºå’Œè§†è§‰æç¤ºæ¥é€‚é…é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä¸ºæœºå™¨äººæ§åˆ¶ä»»åŠ¡æä¾›ä»»åŠ¡è‡ªé€‚åº”çš„è§†è§‰è¡¨ç¤ºï¼Œè€Œæ— éœ€å¾®è°ƒæ¨¡å‹æœ¬èº«ã€‚è¯¥æ–¹æ³•åœ¨å„ç§æœºå™¨äººæ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é¢„è®­ç»ƒè§†è§‰è¡¨ç¤ºåœ¨æ¨¡ä»¿å­¦ä¹ ä¸­é€šå¸¸ä¿æŒå†»ç»“çŠ¶æ€ï¼Œå¯¼è‡´ä»»åŠ¡æ— å…³æ€§ã€‚è™½ç„¶æ–‡æœ¬æ¡ä»¶åœ¨å…¶ä»–è§†è§‰é¢†åŸŸè¡¨ç°æˆåŠŸï¼Œä½†åœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­ç›´æ¥åº”ç”¨æ•ˆæœæœ‰é™ç”šè‡³äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œè¿™å½’å› äºæ‰©æ•£æ¨¡å‹è®­ç»ƒæ•°æ®ä¸æœºå™¨äººæ§åˆ¶ç¯å¢ƒä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ORCAæ–¹æ³•ï¼Œå¼•å…¥å¯å­¦ä¹ çš„ä»»åŠ¡æç¤ºæ¥é€‚åº”æ§åˆ¶ç¯å¢ƒï¼Œä»¥åŠè§†è§‰æç¤ºæ¥æ•æ‰ç»†ç²’åº¦çš„å¸§çº§ç»†èŠ‚ã€‚é€šè¿‡æ–°è®¾è®¡çš„æ¡ä»¶æœºåˆ¶ä¿ƒè¿›ä»»åŠ¡è‡ªé€‚åº”è¡¨ç¤ºï¼Œè€Œæ— éœ€å¾®è°ƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æœ¬èº«ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨å„ç§æœºå™¨äººæ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†å…ˆå‰çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ä»»åŠ¡è‡ªé€‚åº”è¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆæå‡æ§åˆ¶ä»»åŠ¡çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜è€ƒè™‘æ§åˆ¶ä»»åŠ¡æ‰€éœ€çš„ç‰¹å®šåŠ¨æ€è§†è§‰ä¿¡æ¯çš„é‡è¦æ€§ï¼ŒæˆåŠŸå¼¥åˆäº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸æœºå™¨äººæ§åˆ¶ç¯å¢ƒä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚å¯å­¦ä¹ æç¤ºæœºåˆ¶ä¸ºåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæœºå™¨äººæ§åˆ¶æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>While pre-trained visual representations have significantly advanced
imitation learning, they are often task-agnostic as they remain frozen during
policy learning. In this work, we explore leveraging pre-trained text-to-image
diffusion models to obtain task-adaptive visual representations for robotic
control, without fine-tuning the model itself. However, we find that naively
applying textual conditions - a successful strategy in other vision domains -
yields minimal or even negative gains in control tasks. We attribute this to
the domain gap between the diffusion model's training data and robotic control
environments, leading us to argue for conditions that consider the specific,
dynamic visual information required for control. To this end, we propose ORCA,
which introduces learnable task prompts that adapt to the control environment
and visual prompts that capture fine-grained, frame-specific details. Through
facilitating task-adaptive representations with our newly devised conditions,
our approach achieves state-of-the-art performance on various robotic control
benchmarks, significantly surpassing prior methods.</p>
<h3 id="22-clappertext-a-benchmark-for-text-recognition-in-low-resource-archival-documents">[22] <a href="https://arxiv.org/abs/2510.15557">ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents</a></h3>
<p><em>Tingyu Lin, Marco Peer, Florian Kleber, Robert Sablatnig</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ClapperTextï¼Œä¸€ä¸ªç”¨äºè§†è§‰é€€åŒ–å’Œä½èµ„æºåœºæ™¯ä¸‹æ‰‹å†™ä¸å°åˆ·æ–‡æœ¬è¯†åˆ«çš„åŸºå‡†æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æºè‡ªäºŒæˆ˜æ—¶æœŸæ¡£æ¡ˆè§†é¢‘ä¸­çš„åœºè®°æ¿ï¼ŒåŒ…å«9,813ä¸ªæ ‡æ³¨å¸§å’Œ94,573ä¸ªå•è¯çº§æ–‡æœ¬å®ä¾‹ï¼Œä¸ºå†å²æ–‡æ¡£åˆ†ææä¾›äº†ç°å®ä¸”æ–‡åŒ–èƒŒæ™¯ä¸°å¯Œçš„èµ„æºã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å†å²æ¡£æ¡ˆåˆ†æä¸­è§†è§‰é€€åŒ–æ–‡æœ¬è¯†åˆ«çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœºè®°æ¿æ–‡æœ¬è¯†åˆ«é¢ä¸´è¿åŠ¨æ¨¡ç³Šã€æ‰‹å†™å˜å¼‚ã€æ›å…‰æ³¢åŠ¨å’Œæ‚ä¹±èƒŒæ™¯ç­‰é—®é¢˜ï¼Œè¿™äº›é—®é¢˜åæ˜ äº†å†å²æ–‡æ¡£åˆ†æä¸­ç»“æ„åŒ–å†…å®¹å‡ºç°åœ¨é€€åŒ–ã€éæ ‡å‡†å½¢å¼ä¸­çš„æ™®éæŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> è¯¥ç ”ç©¶æ„å»ºäº†åŒ…å«127ä¸ªäºŒæˆ˜æ—¶æœŸæ¡£æ¡ˆè§†é¢‘ç‰‡æ®µçš„ClapperTextæ•°æ®é›†ï¼Œæä¾›äº†9,813ä¸ªæ ‡æ³¨å¸§å’Œ94,573ä¸ªå•è¯çº§æ–‡æœ¬å®ä¾‹ï¼Œå…¶ä¸­67%ä¸ºæ‰‹å†™æ–‡æœ¬ï¼Œ1,566ä¸ªå®ä¾‹å­˜åœ¨éƒ¨åˆ†é®æŒ¡ï¼Œæ¯ä¸ªå®ä¾‹åŒ…å«è½¬å½•ã€è¯­ä¹‰ç±»åˆ«ã€æ–‡æœ¬ç±»å‹å’Œé®æŒ¡çŠ¶æ€ç­‰æ ‡æ³¨ä¿¡æ¯ï¼Œå¹¶ä»¥4ç‚¹å¤šè¾¹å½¢è¡¨ç¤ºçš„æ—‹è½¬è¾¹ç•Œæ¡†æ”¯æŒç©ºé—´ç²¾ç¡®çš„OCRåº”ç”¨ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶ä½¿ç”¨ä¸€è‡´çš„æ¯è§†é¢‘è¯„ä¼°åè®®ï¼Œåœ¨é›¶æ ·æœ¬å’Œå¾®è°ƒæ¡ä»¶ä¸‹å¯¹å…­ä¸ªä»£è¡¨æ€§è¯†åˆ«æ¨¡å‹å’Œä¸ƒä¸ªæ£€æµ‹æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºå°½ç®¡è®­ç»ƒé›†è¾ƒå°ï¼ˆä»…18ä¸ªè§†é¢‘ï¼‰ï¼Œå¾®è°ƒä»èƒ½å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œçªå‡ºäº†ClapperTextåœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­çš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>Conclusion:</strong> ClapperTextæ•°æ®é›†ä¸ºæ¨è¿›ä½èµ„æºæ¡£æ¡ˆåœºæ™¯ä¸­é²æ£’OCRå’Œæ–‡æ¡£ç†è§£æä¾›äº†ç°å®ä¸”æ–‡åŒ–èƒŒæ™¯ä¸°å¯Œçš„èµ„æºï¼Œè¯¥ç ”ç©¶å¼ºè°ƒäº†å¾®è°ƒåœ¨å†å²æ–‡æ¡£åˆ†æä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¤„ç†è§†è§‰é€€åŒ–æ–‡æœ¬çš„æ¨¡å‹å¼€å‘æä¾›äº†æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>This paper presents ClapperText, a benchmark dataset for handwritten and
printed text recognition in visually degraded and low-resource settings. The
dataset is derived from 127 World War II-era archival video segments containing
clapperboards that record structured production metadata such as date,
location, and camera-operator identity. ClapperText includes 9,813 annotated
frames and 94,573 word-level text instances, 67% of which are handwritten and
1,566 are partially occluded. Each instance includes transcription, semantic
category, text type, and occlusion status, with annotations available as
rotated bounding boxes represented as 4-point polygons to support spatially
precise OCR applications. Recognizing clapperboard text poses significant
challenges, including motion blur, handwriting variation, exposure
fluctuations, and cluttered backgrounds, mirroring broader challenges in
historical document analysis where structured content appears in degraded,
non-standard forms. We provide both full-frame annotations and cropped word
images to support downstream tasks. Using a consistent per-video evaluation
protocol, we benchmark six representative recognition and seven detection
models under zero-shot and fine-tuned conditions. Despite the small training
set (18 videos), fine-tuning leads to substantial performance gains,
highlighting ClapperText's suitability for few-shot learning scenarios. The
dataset offers a realistic and culturally grounded resource for advancing
robust OCR and document understanding in low-resource archival contexts. The
dataset and evaluation code are available at
https://github.com/linty5/ClapperText.</p>
<h3 id="23-lightweight-cyclegan-models-for-cross-modality-image-transformation-and-experimental-quality-assessment-in-fluorescence-microscopy">[23] <a href="https://arxiv.org/abs/2510.15579">Lightweight CycleGAN Models for Cross-Modality Image Transformation and Experimental Quality Assessment in Fluorescence Microscopy</a></h3>
<p><em>Mohammad Soltaninezhad, Yashar Rouzbahani, Jhonatan Contreras, Rohan Chippalkatti, Daniel Kwaku Abankwa, Christian Eggeling, Thomas Bocklitz</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§CycleGANç”¨äºè§å…‰æ˜¾å¾®é•œä¸­çš„æ¨¡æ€è½¬æ¢ï¼Œé€šè¿‡å›ºå®šé€šé“ç­–ç•¥å°†å‚æ•°ä»4180ä¸‡å‡å°‘åˆ°çº¦9000ä¸ªï¼ŒåŒæ—¶å®ç°äº†æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ã€‚è¯¥æ¨¡å‹è¿˜å¯ä½œä¸ºå®éªŒå’Œæ ‡è®°è´¨é‡çš„è¯Šæ–­å·¥å…·ï¼Œé€šè¿‡æ¯”è¾ƒç”Ÿæˆå›¾åƒä¸å®éªŒå›¾åƒæ¥è¯†åˆ«æˆåƒé—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è§å…‰æ˜¾å¾®é•œä¸­æ¨¡æ€è½¬æ¢é¢ä¸´çš„æœªé…å¯¹æ•°æ®é›†æŒ‘æˆ˜ï¼ŒåŒæ—¶é™ä½æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®¡ç®—æˆæœ¬å’Œç¯å¢ƒå½±å“ã€‚ä¼ ç»Ÿæ–¹æ³•å‚æ•°è¿‡å¤šä¸”è®­ç»ƒæ•ˆç‡ä½ä¸‹ï¼Œéœ€è¦å¼€å‘æ›´è½»é‡é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨è½»é‡çº§CycleGANæ¶æ„ï¼Œåœ¨åŸºäºU-Netçš„ç”Ÿæˆå™¨ä¸­ç”¨å›ºå®šé€šé“æ–¹æ³•æ›¿ä»£ä¼ ç»Ÿçš„é€šé“å€å¢ç­–ç•¥ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—å‡å°‘äº†æ¨¡å‹å‚æ•°æ•°é‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡æ€è½¬æ¢çš„æ€§èƒ½ã€‚æ¨¡å‹è¿˜ä½œä¸ºè¯Šæ–­å·¥å…·ç”¨äºè¯„ä¼°å®éªŒå›¾åƒè´¨é‡ã€‚</p>
<p><strong>Result:</strong> æ¨¡å‹å‚æ•°ä»4180ä¸‡å¤§å¹…å‡å°‘åˆ°çº¦9000ä¸ªï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ä¸”å†…å­˜ä½¿ç”¨æ›´ä½ã€‚åœ¨é«˜è´¨é‡å›¾åƒä¸Šè®­ç»ƒåï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æœ€ä¼˜æˆåƒç‰¹å¾ï¼Œå…¶ç”Ÿæˆè¾“å‡ºä¸å®éªŒå›¾åƒçš„åå·®å¯æ­ç¤ºå…‰æ¼‚ç™½ã€ä¼ªå½±æˆ–æ ‡è®°ä¸å‡†ç¡®ç­‰é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥è½»é‡çº§CycleGANä¸ä»…å®ç°äº†é«˜æ•ˆçš„æ¨¡æ€è½¬æ¢ï¼Œè¿˜å»ºç«‹äº†ä½œä¸ºå®éªŒéªŒè¯å·¥å…·çš„æ–°åº”ç”¨èŒƒå¼ã€‚å®ƒä¸ºæ˜¾å¾®é•œå·¥ä½œæµç¨‹æä¾›äº†å®ç”¨çš„å›¾åƒä¿çœŸåº¦éªŒè¯æ–¹æ³•ï¼Œå±•ç¤ºäº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç§‘å­¦ä»ªå™¨è´¨é‡æ§åˆ¶ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Lightweight deep learning models offer substantial reductions in
computational cost and environmental impact, making them crucial for scientific
applications. We present a lightweight CycleGAN for modality transfer in
fluorescence microscopy (confocal to super-resolution STED/deconvolved STED),
addressing the common challenge of unpaired datasets. By replacing the
traditional channel-doubling strategy in the U-Net-based generator with a fixed
channel approach, we drastically reduce trainable parameters from 41.8 million
to approximately nine thousand, achieving superior performance with faster
training and lower memory usage. We also introduce the GAN as a diagnostic tool
for experimental and labeling quality. When trained on high-quality images, the
GAN learns the characteristics of optimal imaging; deviations between its
generated outputs and new experimental images can reveal issues such as
photobleaching, artifacts, or inaccurate labeling. This establishes the model
as a practical tool for validating experimental accuracy and image fidelity in
microscopy workflows.</p>
<h3 id="24-flexireid-adaptive-mixture-of-expert-for-multi-modal-person-re-identification">[24] <a href="https://arxiv.org/abs/2510.15595">FlexiReID: Adaptive Mixture of Expert for Multi-Modal Person Re-Identification</a></h3>
<p><em>Zhen Sun, Lei Tan, Yunhang Shen, Chengmao Cai, Xing Sun, Pingyang Dai, Liujuan Cao, Rongrong Ji</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FlexiReIDï¼Œä¸€ä¸ªæ”¯æŒRGBã€çº¢å¤–ã€ç´ æå’Œæ–‡æœ¬å››ç§æ¨¡æ€é—´ä¸ƒç§æ£€ç´¢æ¨¡å¼çš„çµæ´»å¤šæ¨¡æ€è¡Œäººé‡è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”ä¸“å®¶æ··åˆæœºåˆ¶å’Œè·¨æ¨¡æ€æŸ¥è¯¢èåˆæ¨¡å—å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æœ‰é™çš„è·¨æ¨¡æ€è®¾ç½®ï¼Œæ— æ³•æ”¯æŒä»»æ„æŸ¥è¯¢-æ£€ç´¢ç»„åˆï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å®é™…éƒ¨ç½²çš„çµæ´»æ€§ã€‚</p>
<p><strong>Method:</strong> FlexiReIDå¼•å…¥äº†è‡ªé€‚åº”ä¸“å®¶æ··åˆæœºåˆ¶æ¥åŠ¨æ€æ•´åˆä¸åŒæ¨¡æ€ç‰¹å¾ï¼Œå¹¶è®¾è®¡äº†è·¨æ¨¡æ€æŸ¥è¯¢èåˆæ¨¡å—ä»¥å¢å¼ºå¤šæ¨¡æ€ç‰¹å¾æå–èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨æ„å»ºçš„ç»Ÿä¸€æ•°æ®é›†CIRS-PEDESä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlexiReIDå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤æ‚åœºæ™¯ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ”¯æŒå¤šç§æ¨¡æ€ç»„åˆçš„çµæ´»æ¡†æ¶åœ¨è¡Œäººé‡è¯†åˆ«ä¸­çš„é‡è¦æ€§ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„å¤æ‚å¤šæ¨¡æ€æ£€ç´¢éœ€æ±‚æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Multimodal person re-identification (Re-ID) aims to match pedestrian images
across different modalities. However, most existing methods focus on limited
cross-modal settings and fail to support arbitrary query-retrieval
combinations, hindering practical deployment. We propose FlexiReID, a flexible
framework that supports seven retrieval modes across four modalities: rgb,
infrared, sketches, and text. FlexiReID introduces an adaptive
mixture-of-experts (MoE) mechanism to dynamically integrate diverse modality
features and a cross-modal query fusion module to enhance multimodal feature
extraction. To facilitate comprehensive evaluation, we construct CIRS-PEDES, a
unified dataset extending four popular Re-ID datasets to include all four
modalities. Extensive experiments demonstrate that FlexiReID achieves
state-of-the-art performance and offers strong generalization in complex
scenarios.</p>
<h3 id="25-valeo-near-field-a-novel-dataset-for-pedestrian-intent-detection">[25] <a href="https://arxiv.org/abs/2510.15673">Valeo Near-Field: a novel dataset for pedestrian intent detection</a></h3>
<p><em>Antonyo Musabini, Rachid Benmokhtar, Jagdish Bhanushali, Victor Galizzi, Bertrand Luvison, Xavier Perrotton</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºæ£€æµ‹è¡Œäººæ„å›¾çš„æ–°å‹å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«é±¼çœ¼ç›¸æœºã€æ¿€å…‰é›·è¾¾ã€è¶…å£°æ³¢ä¼ æ„Ÿå™¨å’Œè¿åŠ¨æ•æ‰æ•°æ®ï¼Œæ—¨åœ¨ä¸ºæ™ºèƒ½è½¦è¾†åœ¨è¿‘åœºåœºæ™¯ä¸­çš„æ„ŸçŸ¥ç®—æ³•æä¾›åŸºå‡†æµ‹è¯•èµ„æºã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³æ™ºèƒ½è½¦è¾†åœ¨è¿‘åœºåœºæ™¯ä¸­å‡†ç¡®æ£€æµ‹è¡Œäººæ„å›¾çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¼ æ„Ÿå™¨é®æŒ¡ã€åŠ¨æ€ç¯å¢ƒå’Œç¡¬ä»¶çº¦æŸç­‰ç°å®ä¸–ç•Œé—®é¢˜æ—¶ï¼Œç°æœ‰æ•°æ®é›†ç¼ºä¹åŒæ­¥å¤šæ¨¡æ€æ•°æ®å’Œè¯¦ç»†3Då§¿æ€æ ‡æ³¨çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªåŒ…å«åŒæ­¥å¤šæ¨¡æ€æ•°æ®çš„ç»¼åˆæ•°æ®é›†ï¼ŒåŒ…æ‹¬é±¼çœ¼ç›¸æœºè§†é¢‘ã€æ¿€å…‰é›·è¾¾æ‰«æã€è¶…å£°æ³¢ä¼ æ„Ÿå™¨è¯»æ•°å’ŒåŸºäºè¿åŠ¨æ•æ‰çš„3Däººä½“å§¿æ€ï¼Œæä¾›äº†è¯¦ç»†çš„3Då…³èŠ‚ä½ç½®æ ‡æ³¨å’Œç²¾ç¡®çš„3Dè¡Œäººä½ç½®ä¿¡æ¯ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘å¸ƒäº†éƒ¨åˆ†æ•°æ®é›†å’Œå…¨é¢çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…å«å‡†ç¡®æ€§ã€æ•ˆç‡å’ŒåµŒå…¥å¼ç³»ç»Ÿå¯æ‰©å±•æ€§çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æä¾›äº†åŸºäºå®šåˆ¶ç¥ç»ç½‘ç»œæ¶æ„çš„åŸºçº¿æ€§èƒ½æŒ‡æ ‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ•°æ®é›†ä¸ºè¡Œäººæ£€æµ‹ã€3Då§¿æ€ä¼°è®¡ä»¥åŠ4Dè½¨è¿¹å’Œæ„å›¾é¢„æµ‹ç­‰ä»»åŠ¡æä¾›äº†ç‹¬ç‰¹çš„èµ„æºï¼Œä¸ºç ”ç©¶äººå‘˜å¼€å‘å…ˆè¿›ç®—æ³•å¥ å®šäº†åŸºç¡€ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ä»¥ä¿ƒè¿›æ•°æ®é›†çš„é‡‡ç”¨å’Œæ”¹è¿›ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>This paper presents a novel dataset aimed at detecting pedestrians'
intentions as they approach an ego-vehicle. The dataset comprises synchronized
multi-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic
sensor readings, and motion capture-based 3D body poses, collected across
diverse real-world scenarios. Key contributions include detailed annotations of
3D body joint positions synchronized with fisheye camera images, as well as
accurate 3D pedestrian positions extracted from lidar data, facilitating robust
benchmarking for perception algorithms. We release a portion of the dataset
along with a comprehensive benchmark suite, featuring evaluation metrics for
accuracy, efficiency, and scalability on embedded systems. By addressing
real-world challenges such as sensor occlusions, dynamic environments, and
hardware constraints, this dataset offers a unique resource for developing and
evaluating state-of-the-art algorithms in pedestrian detection, 3D pose
estimation and 4D trajectory and intention prediction. Additionally, we provide
baseline performance metrics using custom neural network architectures and
suggest future research directions to encourage the adoption and enhancement of
the dataset. This work aims to serve as a foundation for researchers seeking to
advance the capabilities of intelligent vehicles in near-field scenarios.</p>
<h3 id="26-quantized-fca-efficient-zero-shot-texture-anomaly-detection">[26] <a href="https://arxiv.org/abs/2510.15602">Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection</a></h3>
<p><em>Andrei-Timotei Ardelean, Patrick RÃ¼ckbeil, Tim Weyrich</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºQFCAçš„å®æ—¶é›¶æ ·æœ¬å¼‚å¸¸å®šä½æ–¹æ³•ï¼Œé€šè¿‡é‡åŒ–ç‰¹å¾å¯¹åº”åˆ†æç®—æ³•å®ç°äº†10å€åŠ é€Ÿï¼Œåœ¨çº¹ç†å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­ä¿æŒäº†é«˜ç²¾åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é›¶æ ·æœ¬å¼‚å¸¸å®šä½æ–¹æ³•åœ¨çº¹ç†å¼‚å¸¸æ£€æµ‹ä¸­å­˜åœ¨è¿è¡Œæ—¶é—´è¿‡é•¿çš„é—®é¢˜ï¼Œä½¿å…¶éš¾ä»¥åœ¨å®é™…åœºæ™¯å¦‚ç”Ÿäº§çº¿ç›‘æ§ä¸­éƒ¨ç½²åº”ç”¨ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€å®é™…éƒ¨ç½²ç“¶é¢ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„QFCAæ–¹æ³•å®ç°äº†é‡åŒ–ç‰ˆæœ¬çš„ç‰¹å¾å¯¹åº”åˆ†æç®—æ³•ï¼Œé€šè¿‡å°†è¡¥ä¸ç»Ÿè®¡æ¯”è¾ƒé€‚é…åˆ°é‡åŒ–å€¼ç›´æ–¹å›¾ä¸Šå·¥ä½œï¼Œå¹¶å¼•å…¥åŸºäºä¸»æˆåˆ†åˆ†æçš„ç‰¹å¾é¢„å¤„ç†æ­¥éª¤æ¥å¢å¼ºæ­£å¸¸ä¸å¼‚å¸¸ç‰¹å¾ä¹‹é—´çš„å¯¹æ¯”åº¦ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨ä¿æŒå‡†ç¡®ç‡å‡ ä¹æ²¡æœ‰æŸå¤±çš„æƒ…å†µä¸‹è·å¾—äº†10å€çš„åŠ é€Ÿæ•ˆæœï¼Œå¹¶åœ¨å¤æ‚çº¹ç†ä¸Šæé«˜äº†æ£€æµ‹ç²¾åº¦ï¼Œåœ¨ä¸ç°æœ‰æ–¹æ³•çš„å…¨é¢è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> QFCAæ–¹æ³•è¯æ˜äº†é€šè¿‡é‡åŒ–ç­–ç•¥å¯ä»¥åœ¨ä¸ç‰ºç‰²ç²¾åº¦çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡å¼‚å¸¸æ£€æµ‹ç®—æ³•çš„è¿è¡Œæ•ˆç‡ï¼Œä¸ºé›¶æ ·æœ¬å¼‚å¸¸å®šä½åœ¨å®æ—¶åº”ç”¨ä¸­çš„éƒ¨ç½²æä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Zero-shot anomaly localization is a rising field in computer vision research,
with important progress in recent years. This work focuses on the problem of
detecting and localizing anomalies in textures, where anomalies can be defined
as the regions that deviate from the overall statistics, violating the
stationarity assumption. The main limitation of existing methods is their high
running time, making them impractical for deployment in real-world scenarios,
such as assembly line monitoring. We propose a real-time method, named QFCA,
which implements a quantized version of the feature correspondence analysis
(FCA) algorithm. By carefully adapting the patch statistics comparison to work
on histograms of quantized values, we obtain a 10x speedup with little to no
loss in accuracy. Moreover, we introduce a feature preprocessing step based on
principal component analysis, which enhances the contrast between normal and
anomalous features, improving the detection precision on complex textures. Our
method is thoroughly evaluated against prior art, comparing favorably with
existing methods. Project page:
https://reality.tf.fau.de/pub/ardelean2025quantized.html</p>
<h3 id="27-towards-label-free-brain-tumor-segmentation-unsupervised-learning-with-multimodal-mri">[27] <a href="https://arxiv.org/abs/2510.15684">Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI</a></h3>
<p><em>Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier, Noel E. O'Connor, Ferran Marques</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€è§†è§‰Transformerè‡ªç¼–ç å™¨ï¼ˆMViT-AEï¼‰çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨å¥åº·è„‘éƒ¨MRIæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡é‡å»ºè¯¯å·®å›¾å®ç°è‚¿ç˜¤æ£€æµ‹ä¸å®šä½ï¼Œåœ¨BraTS-GoAT 2025æ•°æ®é›†ä¸Šå–å¾—äº†å…·æœ‰ä¸´åºŠæ„ä¹‰çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è„‘è‚¿ç˜¤åˆ†å‰²ä¸­æ ‡æ³¨æ•°æ®æœ‰é™ã€æˆæœ¬é«˜æ˜‚ä¸”ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œé€šè¿‡æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•æä¾›ç›‘ç£å­¦ä¹ çš„è¡¥å……æ–¹æ¡ˆï¼Œç‰¹åˆ«é’ˆå¯¹ç¥ç»å½±åƒå·¥ä½œæµç¨‹ä¸­çš„å¯æ‰©å±•æ€§ç“¶é¢ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºå¤šæ¨¡æ€è§†è§‰Transformerè‡ªç¼–ç å™¨ï¼ˆMViT-AEï¼‰ï¼Œé‡‡ç”¨å¤šæ¨¡æ€æ—©æœŸ-æ™šæœŸèåˆç­–ç•¥æ•´åˆä¸åŒMRIåºåˆ—çš„äº’è¡¥ä¿¡æ¯ï¼Œå¹¶å¼•å…¥åŒ…å«Segment Anything Modelï¼ˆSAMï¼‰çš„åå¤„ç†æµç¨‹æ¥ä¼˜åŒ–é¢„æµ‹çš„è‚¿ç˜¤è½®å»“ã€‚</p>
<p><strong>Result:</strong> åœ¨æµ‹è¯•é›†ä¸Šè·å¾—ç—…ç¶çº§åˆ«çš„Diceç›¸ä¼¼ç³»æ•°ï¼šå…¨è‚¿ç˜¤0.437ã€è‚¿ç˜¤æ ¸å¿ƒ0.316ã€å¢å¼ºè‚¿ç˜¤0.350ï¼Œåœ¨éªŒè¯é›†ä¸Šå¼‚å¸¸æ£€æµ‹ç‡è¾¾åˆ°89.4%ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜åŸºäºTransformerçš„æ— ç›‘ç£æ¨¡å‹å…·æœ‰ä½œä¸ºç¥ç»è‚¿ç˜¤å½±åƒå¯æ‰©å±•ã€æ ‡ç­¾é«˜æ•ˆå·¥å…·çš„æ½œåŠ›ï¼Œå°½ç®¡åœ¨æ£€æµ‹å°ç—…ç¶æˆ–éå¢å¼ºç—…å˜æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†ä¸ºæ— ç›‘ç£åŒ»å­¦å›¾åƒåˆ†ææä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Unsupervised anomaly detection (UAD) presents a complementary alternative to
supervised learning for brain tumor segmentation in magnetic resonance imaging
(MRI), particularly when annotated datasets are limited, costly, or
inconsistent. In this work, we propose a novel Multimodal Vision Transformer
Autoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and
localize tumors via reconstruction-based error maps. This unsupervised paradigm
enables segmentation without reliance on manual labels, addressing a key
scalability bottleneck in neuroimaging workflows. Our method is evaluated in
the BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors
such as gliomas, meningiomas, and pediatric brain tumors. To enhance
performance, we introduce a multimodal early-late fusion strategy that
leverages complementary information across multiple MRI sequences, and a
post-processing pipeline that integrates the Segment Anything Model (SAM) to
refine predicted tumor contours. Despite the known challenges of UAD,
particularly in detecting small or non-enhancing lesions, our method achieves
clinically meaningful tumor localization, with lesion-wise Dice Similarity
Coefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing
Tumor) on the test set, and an anomaly Detection Rate of 89.4% on the
validation set. These findings highlight the potential of transformer-based
unsupervised models to serve as scalable, label-efficient tools for
neuro-oncological imaging.</p>
<h3 id="28-dgme-t-directional-grid-motion-encoding-for-transformer-based-historical-camera-movement-classification">[28] <a href="https://arxiv.org/abs/2510.15725">DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification</a></h3>
<p><em>Tingyu Lin, Armin Dadras, Florian Kleber, Robert Sablatnig</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDGME-Tï¼Œä¸€ç§è½»é‡çº§è§†é¢‘Swin Transformeræ‰©å±•ï¼Œé€šè¿‡æ³¨å…¥æ–¹å‘æ€§ç½‘æ ¼è¿åŠ¨ç¼–ç æ¥æå‡æ¡£æ¡ˆå½±ç‰‡ä¸­çš„ç›¸æœºè¿åŠ¨åˆ†ç±»æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨ç°ä»£å’Œå†å²å½±ç‰‡æ•°æ®ä¸Šå‡æ˜¾è‘—æå‡äº†åˆ†ç±»å‡†ç¡®ç‡å’ŒF1åˆ†æ•°ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é’ˆå¯¹åœ¨å½“ä»£é«˜è´¨é‡è§†é¢‘ä¸Šè®­ç»ƒçš„ç›¸æœºè¿åŠ¨åˆ†ç±»æ¨¡å‹åœ¨å¤„ç†æ¡£æ¡ˆå½±ç‰‡æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œç”±äºæ¡£æ¡ˆå½±ç‰‡å­˜åœ¨å™ªå£°ã€ç¼ºå¤±å¸§å’Œä½å¯¹æ¯”åº¦ç­‰é€€åŒ–å› ç´ ä¼šæ¨¡ç³Šè¿åŠ¨çº¿ç´¢ï¼Œéœ€è¦å¼€å‘æ›´é²æ£’çš„åˆ†ç±»æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†ç»Ÿä¸€åŸºå‡†æ•°æ®é›†ï¼Œå°†ä¸¤ä¸ªç°ä»£è¯­æ–™åº“æ•´åˆä¸ºå››ä¸ªè§„èŒƒç±»åˆ«ï¼Œå¹¶é‡æ„HISTORIANé›†åˆä¸ºäº”ä¸ªå¹³è¡¡ç±»åˆ«ã€‚æå‡ºäº†DGME-Tæ–¹æ³•ï¼Œåœ¨Video Swin TransformeråŸºç¡€ä¸Šé€šè¿‡å¯å­¦ä¹ çš„å½’ä¸€åŒ–åæœŸèåˆå±‚æ³¨å…¥åŸºäºå…‰æµçš„æ–¹å‘æ€§ç½‘æ ¼è¿åŠ¨ç¼–ç ã€‚</p>
<p><strong>Result:</strong> DGME-Tå°†éª¨å¹²ç½‘ç»œçš„top-1å‡†ç¡®ç‡ä»81.78%æå‡è‡³86.14%ï¼Œå®è§‚F1ä»82.08%æå‡è‡³87.81%ã€‚åœ¨äºŒæˆ˜æ¡£æ¡ˆå½±ç‰‡ä¸Šï¼Œå‡†ç¡®ç‡ä»83.43%æå‡è‡³84.62%ï¼Œå®è§‚F1ä»81.72%æå‡è‡³82.63%ã€‚è·¨åŸŸç ”ç©¶è¡¨æ˜åœ¨ç°ä»£æ•°æ®ä¸Šè¿›è¡Œä¸­é—´å¾®è°ƒå¯å°†å†å²æ€§èƒ½æå‡è¶…è¿‡äº”ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ç»“æ„åŒ–è¿åŠ¨å…ˆéªŒä¸Transformerè¡¨ç¤ºå…·æœ‰äº’è¡¥æ€§ï¼Œå³ä½¿æ˜¯ä¸€ä¸ªå°å‹ç²¾å¿ƒæ ¡å‡†çš„è¿åŠ¨å¤´ä¹Ÿèƒ½æ˜¾è‘—å¢å¼ºé€€åŒ–å½±ç‰‡åˆ†æçš„é²æ£’æ€§ã€‚æ–¹å‘æ€§è¿åŠ¨ç¼–ç ä¸Transformeræ¶æ„çš„ç»“åˆä¸ºæ¡£æ¡ˆå½±ç‰‡åˆ†ææä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>Camera movement classification (CMC) models trained on contemporary,
high-quality footage often degrade when applied to archival film, where noise,
missing frames, and low contrast obscure motion cues. We bridge this gap by
assembling a unified benchmark that consolidates two modern corpora into four
canonical classes and restructures the HISTORIAN collection into five balanced
categories. Building on this benchmark, we introduce DGME-T, a lightweight
extension to the Video Swin Transformer that injects directional grid motion
encoding, derived from optical flow, via a learnable and normalised late-fusion
layer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and
its macro F1 from 82.08% to 87.81% on modern clips, while still improving the
demanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72%
to 82.63% macro F1. A cross-domain study further shows that an intermediate
fine-tuning stage on modern data increases historical performance by more than
five percentage points. These results demonstrate that structured motion priors
and transformer representations are complementary and that even a small,
carefully calibrated motion head can substantially enhance robustness in
degraded film analysis. Related resources are available at
https://github.com/linty5/DGME-T.</p>
<h3 id="29-ndm-a-noise-driven-detection-and-mitigation-framework-against-implicit-sexual-intentions-in-text-to-image-generation">[29] <a href="https://arxiv.org/abs/2510.15752">NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation</a></h3>
<p><em>Yitong Sun, Yao Huang, Ruochen Zhang, Huanran Chen, Shouwei Ruan, Ranjie Duan, Xingxing Wei</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºNDMï¼Œé¦–ä¸ªå™ªå£°é©±åŠ¨çš„æ£€æµ‹ä¸ç¼“è§£æ¡†æ¶ï¼Œèƒ½å¤Ÿæ£€æµ‹å¹¶ç¼“è§£æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„éšå«æ¶æ„æ„å›¾ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å™ªå£°åˆ†ç¦»æ€§å’Œè‡ªé€‚åº”è´Ÿå¼•å¯¼æœºåˆ¶ï¼Œåœ¨è‡ªç„¶å’Œå¯¹æŠ—æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆèƒ½åŠ›æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†å¯¹éšå«æ€§æš—ç¤ºæç¤ºä»ç„¶è„†å¼±ï¼Œè¿™äº›å¾®å¦™çº¿ç´¢å¸¸ä¼ªè£…ä¸ºè‰¯æ€§æœ¯è¯­å´å¯èƒ½æ„å¤–è§¦å‘ä¸å½“å†…å®¹ã€‚ç°æœ‰æ£€æµ‹æ–¹æ³•ä¸»è¦é’ˆå¯¹æ˜¾å¼å†…å®¹è®¾è®¡ï¼Œéš¾ä»¥è¯†åˆ«éšå«çº¿ç´¢ï¼Œè€Œå¾®è°ƒæ–¹æ³•è™½æœ‰æ•ˆä½†ä¼šæŸå®³ç”Ÿæˆè´¨é‡ï¼Œå½¢æˆä¸è‰¯æƒè¡¡ã€‚</p>
<p><strong>Method:</strong> æå‡ºä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šé¦–å…ˆåˆ©ç”¨æ—©æœŸé¢„æµ‹å™ªå£°çš„å¯åˆ†ç¦»æ€§å¼€å‘åŸºäºå™ªå£°çš„æ£€æµ‹æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜ç²¾åº¦é«˜æ•ˆè¯†åˆ«æ¶æ„å†…å®¹ï¼›å…¶æ¬¡æå‡ºå™ªå£°å¢å¼ºçš„è‡ªé€‚åº”è´Ÿå¼•å¯¼æœºåˆ¶ï¼Œé€šè¿‡æŠ‘åˆ¶æ˜¾è‘—åŒºåŸŸæ³¨æ„åŠ›æ¥ä¼˜åŒ–åˆå§‹å™ªå£°ï¼Œä»è€Œå¢å¼ºå¯¹æ€§å†…å®¹ç¼“è§£çš„è‡ªé€‚åº”è´Ÿå¼•å¯¼æ•ˆæœã€‚</p>
<p><strong>Result:</strong> åœ¨è‡ªç„¶å’Œå¯¹æŠ—æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒNDMåœ¨æ€§èƒ½ä¸Šä¼˜äºåŒ…æ‹¬SLDã€UCEå’ŒRECEåœ¨å†…çš„ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„æ£€æµ‹å’Œç¼“è§£èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å™ªå£°é©±åŠ¨æ–¹æ³•åœ¨æ£€æµ‹å’Œç¼“è§£éšå«æ¶æ„æ„å›¾æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å®‰å…¨é˜²æŠ¤æä¾›äº†æ–°æ–¹å‘ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å®‰å…¨æ€§å’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Despite the impressive generative capabilities of text-to-image (T2I)
diffusion models, they remain vulnerable to generating inappropriate content,
especially when confronted with implicit sexual prompts. Unlike explicit
harmful prompts, these subtle cues, often disguised as seemingly benign terms,
can unexpectedly trigger sexual content due to underlying model biases, raising
significant ethical concerns. However, existing detection methods are primarily
designed to identify explicit sexual content and therefore struggle to detect
these implicit cues. Fine-tuning approaches, while effective to some extent,
risk degrading the model's generative quality, creating an undesirable
trade-off. To address this, we propose NDM, the first noise-driven detection
and mitigation framework, which could detect and mitigate implicit malicious
intention in T2I generation while preserving the model's original generative
capabilities. Specifically, we introduce two key innovations: first, we
leverage the separability of early-stage predicted noise to develop a
noise-based detection method that could identify malicious content with high
accuracy and efficiency; second, we propose a noise-enhanced adaptive negative
guidance mechanism that could optimize the initial noise by suppressing the
prominent region's attention, thereby enhancing the effectiveness of adaptive
negative guidance for sexual mitigation. Experimentally, we validate NDM on
both natural and adversarial datasets, demonstrating its superior performance
over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and
resources are available at https://github.com/lorraine021/NDM.</p>
<h3 id="30-unimedvl-unifying-medical-multimodal-understanding-and-generation-through-observation-knowledge-analysis">[30] <a href="https://arxiv.org/abs/2510.15710">Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis</a></h3>
<p><em>Junzhi Ning, Wei Li, Cheng Tang, Jiashi Lin, Chenglong Ma, Chaoyang Zhang, Jiyao Liu, Ying Chen, Shujian Gao, Lihao Liu, Yuandong Pu, Huihui Xu, Chenhui Gou, Ziyan Huang, Yi Xin, Qi Qin, Zhongying Deng, Diping Song, Bin Fu, Guang Yang, Yuanfeng Ji, Tianbin Li, Yanzhou Su, Jin Ye, Shixiang Tang, Ming Hu, Junjun He</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UniMedVLï¼Œè¿™æ˜¯é¦–ä¸ªåŒ»å­¦ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡Observation-Knowledge-Analysisæ¡†æ¶åœ¨å•ä¸€æ¶æ„ä¸­åŒæ—¶å¤„ç†åŒ»å­¦å›¾åƒç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œåœ¨äº”ä¸ªç†è§£åŸºå‡†ä¸Šå–å¾—ä¼˜è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨å…«ä¸ªåŒ»å­¦æˆåƒæ¨¡æ€ä¸ŠåŒ¹é…ä¸“ç”¨æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŒ»å­¦AIç³»ç»Ÿå­˜åœ¨ç»Ÿä¸€å¤„ç†æµç¨‹çš„æ–­è£‚ï¼šåŒ»å­¦å›¾åƒç†è§£æ¨¡å‹åªèƒ½è§£é‡Šå›¾åƒè€Œæ— æ³•ç”Ÿæˆè§†è§‰è¾“å‡ºï¼ŒåŒ»å­¦å›¾åƒç”Ÿæˆæ¨¡å‹åªèƒ½åˆæˆå›¾åƒè€Œæ— æ³•æä¾›æ–‡æœ¬è§£é‡Šï¼Œè¿™å¯¼è‡´äº†æ•°æ®è¡¨ç¤ºã€ç‰¹å¾é›†æˆå’Œä»»åŠ¡çº§å¤šæ¨¡æ€èƒ½åŠ›æ–¹é¢çš„å·®è·ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åŸºäºObservation-Knowledge-AnalysisèŒƒå¼çš„å¤šçº§æ¡†æ¶ï¼ŒåŒ…æ‹¬æ„å»ºåŒ…å«560ä¸‡æ ·æœ¬çš„UniMed-5Mæ•°æ®é›†ç”¨äºåŸºç¡€è§‚å¯Ÿï¼Œé‡‡ç”¨æ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ ç³»ç»Ÿå¼•å…¥åŒ»å­¦å¤šæ¨¡æ€çŸ¥è¯†ï¼Œä»¥åŠè®¾è®¡UniMedVLç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨å•ä¸€æ¶æ„ä¸­åŒæ—¶åˆ†æå›¾åƒç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚</p>
<p><strong>Result:</strong> UniMedVLåœ¨äº”ä¸ªåŒ»å­¦å›¾åƒç†è§£åŸºå‡†ä¸Šå–å¾—ä¼˜è¶Šæ€§èƒ½ï¼ŒåŒæ—¶åœ¨å…«ä¸ªåŒ»å­¦æˆåƒæ¨¡æ€çš„ç”Ÿæˆè´¨é‡ä¸ŠåŒ¹é…ä¸“ç”¨æ¨¡å‹ï¼Œç»Ÿä¸€æ¶æ„å®ç°äº†åŒå‘çŸ¥è¯†å…±äº«ï¼Œç”Ÿæˆä»»åŠ¡å¢å¼ºäº†è§†è§‰ç†è§£ç‰¹å¾ã€‚</p>
<p><strong>Conclusion:</strong> åœ¨å•ä¸€åŒ»å­¦æ¡†æ¶å†…æ•´åˆä¼ ç»Ÿåˆ†ç¦»çš„èƒ½åŠ›èƒ½å¤Ÿè§£é”è·¨å¤šæ ·åŒ–åŒ»å­¦è§†è§‰è¯­è¨€ä»»åŠ¡çš„æ”¹è¿›ï¼ŒåŒå‘çŸ¥è¯†å…±äº«æœºåˆ¶è¯æ˜ç”Ÿæˆå’Œç†è§£ä»»åŠ¡çš„ååŒä½œç”¨å¯ä»¥ç›¸äº’å¢å¼ºï¼Œä¸ºåŒ»å­¦è¯Šæ–­åº”ç”¨æä¾›äº†æ›´ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤„ç†æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Medical diagnostic applications require models that can process multimodal
medical inputs (images, patient histories, lab results) and generate diverse
outputs including both textual reports and visual content (annotations,
segmentation masks, and images). Despite this need, existing medical AI systems
disrupt this unified process: medical image understanding models interpret
images but cannot generate visual outputs, while medical image generation
models synthesize images but cannot provide textual explanations. This leads to
gaps in data representation, feature integration, and task-level multimodal
capabilities. To this end, we propose a multi-level framework that draws
inspiration from diagnostic workflows through the
Observation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation
level, we construct UniMed-5M, a dataset comprising over 5.6M samples that
reformat diverse unimodal data into multimodal pairs for foundational
observation. At the knowledge level, we propose Progressive Curriculum Learning
that systematically introduces medical multimodal knowledge. At the analysis
level, we introduce UniMedVL, the first medical unified multimodal model for
the simultaneous analysis of image understanding and generation tasks within a
single architecture. UniMedVL achieves superior performance on five medical
image understanding benchmarks, while matching specialized models in generation
quality across eight medical imaging modalities. Crucially, our unified
architecture enables bidirectional knowledge sharing: generation tasks enhance
visual understanding features, demonstrating that integrating traditionally
separate capabilities within a single medical framework unlocks improvements
across diverse medical vision-language tasks. Code is available at
https://github.com/uni-medical/UniMedVL.</p>
<h3 id="31-qsilk-micrograin-stabilization-and-adaptive-quantile-clipping-for-detail-friendly-latent-diffusion">[31] <a href="https://arxiv.org/abs/2510.15761">QSilk: Micrograin Stabilization and Adaptive Quantile Clipping for Detail-Friendly Latent Diffusion</a></h3>
<p><em>Denis Rychkovskiy</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>QSilkæ˜¯ä¸€ç§è½»é‡çº§çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ç¨³å®šå±‚ï¼Œé€šè¿‡å¾®é’³ä½å’Œè‡ªé€‚åº”åˆ†ä½æ•°è£å‰ªæŠ€æœ¯ï¼Œåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æå‡é«˜é¢‘ç»†èŠ‚ä¿çœŸåº¦å¹¶æŠ‘åˆ¶ç½•è§æ¿€æ´»å³°å€¼ï¼Œå®ç°æ›´æ¸…æ™°ã€æ›´é”åˆ©çš„ç”Ÿæˆç»“æœã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­å­˜åœ¨çš„é«˜é¢‘ç»†èŠ‚ä¿çœŸåº¦ä¸è¶³å’Œç½•è§æ¿€æ´»å³°å€¼é—®é¢˜ï¼Œè¿™äº›é—®é¢˜åœ¨ä½æ­¥æ•°é‡‡æ ·å’Œè¶…é«˜åˆ†è¾¨ç‡ç”Ÿæˆæ—¶å°¤ä¸ºæ˜æ˜¾ï¼Œå½±å“ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œç¨³å®šæ€§ã€‚</p>
<p><strong>Method:</strong> QSilkç»“åˆäº†ä¸¤ç§å…³é”®æŠ€æœ¯ï¼šæ¯æ ·æœ¬å¾®é’³ä½æŠ€æœ¯æ¸©å’Œé™åˆ¶æç«¯å€¼è€Œä¸æŸå¤±çº¹ç†ç»†èŠ‚ï¼Œä»¥åŠè‡ªé€‚åº”åˆ†ä½æ•°è£å‰ªï¼ˆAQClipï¼‰æ ¹æ®åŒºåŸŸç‰¹æ€§åŠ¨æ€è°ƒæ•´å…è®¸å€¼èŒƒå›´ï¼ŒAQClipå¯åŸºäºå±€éƒ¨ç»“æ„ç»Ÿè®¡æˆ–æ³¨æ„åŠ›ç†µå¼•å¯¼ä¸¤ç§æ¨¡å¼è¿è¡Œã€‚</p>
<p><strong>Result:</strong> é›†æˆåˆ°CADE 2.5æ¸²æŸ“ç®¡çº¿åï¼ŒQSilkåœ¨ä½æ­¥æ•°é‡‡æ ·å’Œè¶…é«˜åˆ†è¾¨ç‡æ¡ä»¶ä¸‹äº§ç”Ÿæ›´æ¸…æ™°ã€æ›´é”åˆ©çš„ç»“æœï¼Œè®¡ç®—å¼€é”€å¯å¿½ç•¥ä¸è®¡ï¼Œåœ¨SD/SDXLéª¨å¹²ç½‘ç»œä¸Šå‡è·å¾—ä¸€è‡´çš„å®šæ€§æ”¹è¿›ï¼Œå¹¶èƒ½ä¸CFG/RescaleæŠ€æœ¯ååŒå·¥ä½œï¼Œæ”¯æŒæ›´é«˜çš„å¼•å¯¼å¼ºåº¦è€Œä¸äº§ç”Ÿä¼ªå½±ã€‚</p>
<p><strong>Conclusion:</strong> QSilkæä¾›äº†ä¸€ç§æ— éœ€è®­ç»ƒæˆ–å¾®è°ƒçš„å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ç¨³å®šæ€§å’Œç”Ÿæˆè´¨é‡ï¼Œå…¶è½»é‡çº§è®¾è®¡å’Œæœ€å°åŒ–ç”¨æˆ·æ§åˆ¶ä½¿å…¶æ˜“äºé›†æˆåˆ°ç°æœ‰å·¥ä½œæµä¸­ï¼Œä¸ºé«˜è´¨é‡å›¾åƒç”Ÿæˆæä¾›äº†æœ‰æ•ˆçš„åå¤„ç†å¢å¼ºæ‰‹æ®µã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>We present QSilk, a lightweight, always-on stabilization layer for latent
diffusion that improves high-frequency fidelity while suppressing rare
activation spikes. QSilk combines (i) a per-sample micro clamp that gently
limits extreme values without washing out texture, and (ii) Adaptive Quantile
Clip (AQClip), which adapts the allowed value corridor per region. AQClip can
operate in a proxy mode using local structure statistics or in an attention
entropy guided mode (model confidence). Integrated into the CADE 2.5 rendering
pipeline, QSilk yields cleaner, sharper results at low step counts and
ultra-high resolutions with negligible overhead. It requires no training or
fine-tuning and exposes minimal user controls. We report consistent qualitative
improvements across SD/SDXL backbones and show synergy with CFG/Rescale,
enabling slightly higher guidance without artifacts.</p>
<h3 id="32-neuro-symbolic-spatial-reasoning-in-segmentation">[32] <a href="https://arxiv.org/abs/2510.15841">Neuro-Symbolic Spatial Reasoning in Segmentation</a></h3>
<p><em>Jiayi Lin, Jiabo Huang, Shaogang Gong</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†RelateSegï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä¸­æ¢ç´¢ç¥ç»ç¬¦å·ç©ºé—´æ¨ç†çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸€é˜¶é€»è¾‘å…¬å¼åœ¨ç¥ç»ç½‘ç»œæ¶æ„ä¸­æ–½åŠ æ˜¾å¼ç©ºé—´å…³ç³»çº¦æŸï¼Œåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹å…³è”çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–¹æ³•ç¼ºä¹å¯¹åœºæ™¯ä¸­ç‰©ä½“ç©ºé—´å…³ç³»çš„ç†è§£ï¼Œå¯¼è‡´åœ¨æœªè§å’Œæœªæ ‡è®°å¯¹è±¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›å—é™ï¼Œéœ€è¦è§£å†³ç©ºé—´å…³ç³»å»ºæ¨¡ä¸è¶³çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†Relational Segmentor (RelateSeg)æ–¹æ³•ï¼Œé€šè¿‡ä¸€é˜¶é€»è¾‘å…¬å¼åœ¨ç¥ç»ç½‘ç»œæ¶æ„ä¸­æ–½åŠ æ˜¾å¼ç©ºé—´å…³ç³»çº¦æŸï¼Œè‡ªåŠ¨æå–ç©ºé—´å…³ç³»å¹¶ç¼–ç ä¸ºé€»è¾‘å…¬å¼ï¼Œæ¯ä¸ªåƒç´ åŒæ—¶é¢„æµ‹è¯­ä¹‰ç±»åˆ«å’Œç©ºé—´ä¼ªç±»åˆ«ï¼Œæœ€åé€šè¿‡æ¨¡ç³Šé€»è¾‘æ¾å¼›åœ¨æ·±åº¦ç½‘ç»œæ¶æ„ä¸­å®ç°ç«¯åˆ°ç«¯å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> RelateSegåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å¹³å‡mIoUçš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ…å«å¤šä¸ªç±»åˆ«çš„å›¾åƒä¸Šè¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼Œä»…å¼•å…¥å•ä¸ªè¾…åŠ©æŸå¤±å‡½æ•°ä¸”ä¸å¢åŠ é¢å¤–å‚æ•°ï¼ŒéªŒè¯äº†ç¥ç»ç¬¦å·ç©ºé—´æ¨ç†åœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ç¥ç»ç¬¦å·ç©ºé—´æ¨ç†èƒ½å¤Ÿæ˜¾è‘—æå‡å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ï¼Œé€šè¿‡æ˜¾å¼ç©ºé—´å…³ç³»çº¦æŸå®ç°äº†ç©ºé—´å…³ç³»ä¸€è‡´çš„è¯­ä¹‰åˆ†å‰²ï¼Œä¸ºç»“åˆç¬¦å·æ¨ç†ä¸æ·±åº¦å­¦ä¹ çš„æ–¹æ³•æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from
an open set of categories, requiring generalization to unseen and unlabelled
objects. Using vision-language models (VLMs) to correlate local image patches
with potential unseen object categories suffers from a lack of understanding of
spatial relations of objects in a scene. To solve this problem, we introduce
neuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary
VLM correlation-based approaches, we propose Relational Segmentor (RelateSeg)
to impose explicit spatial relational constraints by first order logic (FOL)
formulated in a neural network architecture. This is the first attempt to
explore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically
extracts spatial relations, e.g., <cat, to-right-of, person>, and encodes them
as first-order logic formulas using our proposed pseudo categories. Each pixel
learns to predict both a semantic category (e.g., "cat") and a spatial pseudo
category (e.g., "right of person") simultaneously, enforcing relational
constraints (e.g., a "cat" pixel must lie to the right of a "person"). Finally,
these logic constraints are formulated in a deep network architecture by fuzzy
logic relaxation, enabling end-to-end learning of spatial-relationally
consistent segmentation. RelateSeg achieves state-of-the-art performance in
terms of average mIoU across four benchmark datasets and particularly shows
clear advantages on images containing multiple categories, with the cost of
only introducing a single auxiliary loss function and no additional parameters,
validating the effectiveness of NeSy spatial reasoning in OVSS.</p>
<h3 id="33-blip3o-next-next-frontier-of-native-image-generation">[33] <a href="https://arxiv.org/abs/2510.15857">BLIP3o-NEXT: Next Frontier of Native Image Generation</a></h3>
<p><em>Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, Ran Xu</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>BLIP3o-NEXTæ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„å›¾åƒç”ŸæˆåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡è‡ªå›å½’+æ‰©æ•£æ··åˆæ¶æ„ç»Ÿä¸€äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘åŠŸèƒ½ï¼Œåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¼˜äºç°æœ‰æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å½“å‰å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç»Ÿä¸€æ¶æ„ä¸‹åŒæ—¶å®ç°é«˜è´¨é‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘èƒ½åŠ›çš„æŒ‘æˆ˜ï¼Œæ¢ç´¢åŸç”Ÿå›¾åƒç”Ÿæˆçš„å‰æ²¿æŠ€æœ¯è¾¹ç•Œã€‚</p>
<p><strong>Method:</strong> BLIP3o-NEXTé‡‡ç”¨è‡ªå›å½’+æ‰©æ•£æ··åˆæ¶æ„ï¼Œå…¶ä¸­è‡ªå›å½’æ¨¡å‹é¦–å…ˆç”ŸæˆåŸºäºå¤šæ¨¡æ€è¾“å…¥çš„ç¦»æ•£å›¾åƒä»¤ç‰Œï¼Œå…¶éšè—çŠ¶æ€éšåä½œä¸ºæ‰©æ•£æ¨¡å‹çš„è°ƒèŠ‚ä¿¡å·æ¥ç”Ÿæˆé«˜ä¿çœŸå›¾åƒï¼Œç»“åˆäº†è‡ªå›å½’æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ‰©æ•£æ¨¡å‹çš„ç»†èŠ‚æ¸²æŸ“ä¼˜åŠ¿ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šç§æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•çš„å¹¿æ³›è¯„ä¼°ä¸­ï¼ŒBLIP3o-NEXTå±•ç°å‡ºä¼˜äºç°æœ‰æ¨¡å‹çš„å“è¶Šæ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„å¼ºå¤§èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†å››ä¸ªå…³é”®æ´å¯Ÿï¼šæ¶æ„é€‰æ‹©å¯¹æ€§èƒ½å½±å“è¾ƒå°ä½†éœ€å…³æ³¨æ‰©å±•æ•ˆç‡ï¼›å¼ºåŒ–å­¦ä¹ èƒ½æ¨åŠ¨åŸç”Ÿå›¾åƒç”Ÿæˆå‰æ²¿ï¼›é€šè¿‡åè®­ç»ƒå’Œæ•°æ®å¼•æ“å¯æ˜¾è‘—æå‡å›¾åƒç¼–è¾‘èƒ½åŠ›ï¼›æ•°æ®è´¨é‡å’Œè§„æ¨¡ä»æ˜¯å†³å®šæ¨¡å‹æ€§èƒ½ä¸Šé™çš„å†³å®šæ€§å› ç´ ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3
series that advances the next frontier of native image generation. BLIP3o-NEXT
unifies text-to-image generation and image editing within a single
architecture, demonstrating strong image generation and image editing
capabilities. In developing the state-of-the-art native image generation model,
we identify four key insights: (1) Most architectural choices yield comparable
performance; an architecture can be deemed effective provided it scales
efficiently and supports fast inference; (2) The successful application of
reinforcement learning can further push the frontier of native image
generation; (3) Image editing still remains a challenging task, yet instruction
following and the consistency between generated and reference images can be
significantly enhanced through post-training and data engine; (4) Data quality
and scale continue to be decisive factors that determine the upper bound of
model performance. Building upon these insights, BLIP3o-NEXT leverages an
Autoregressive + Diffusion architecture in which an autoregressive model first
generates discrete image tokens conditioned on multimodal inputs, whose hidden
states are then used as conditioning signals for a diffusion model to generate
high-fidelity images. This architecture integrates the reasoning strength and
instruction following of autoregressive models with the fine-detail rendering
ability of diffusion models, achieving a new level of coherence and realism.
Extensive evaluations of various text-to-image and image-editing benchmarks
show that BLIP3o-NEXT achieves superior performance over existing models.</p>
<h3 id="34-biomedxpro-prompt-optimization-for-explainable-diagnosis-with-biomedical-vision-language-models">[34] <a href="https://arxiv.org/abs/2510.15866">BiomedXPro: Prompt Optimization for Explainable Diagnosis with Biomedical Vision Language Models</a></h3>
<p><em>Kaushitha Silva, Mansitha Eashwara, Sanduni Ubayasiri, Ruwan Tennakoon, Damayanthi Herath</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†BiomedXProè¿›åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç”Ÿç‰©åŒ»å­¦çŸ¥è¯†æå–å™¨å’Œè‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–çš„å¯è§£é‡Šè‡ªç„¶è¯­è¨€æç¤ºå¯¹é›†åˆï¼Œç”¨äºç–¾ç—…è¯Šæ–­ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æç¤ºè°ƒä¼˜æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„å°‘æ ·æœ¬åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸´åºŠåº”ç”¨å—åˆ°é™åˆ¶ï¼Œå› ä¸ºç°æœ‰çš„æç¤ºä¼˜åŒ–æŠ€æœ¯è¦ä¹ˆäº§ç”Ÿä¸å¯è§£é‡Šçš„æ½œåœ¨å‘é‡ï¼Œè¦ä¹ˆä»…ç”Ÿæˆå•ä¸€æ–‡æœ¬æç¤ºã€‚è¿™ç§ç¼ºä¹é€æ˜åº¦ä»¥åŠæ— æ³•æ•æ‰ä¸´åºŠè¯Šæ–­å¤šé¢æ€§çš„é—®é¢˜ï¼Œé™åˆ¶äº†è¿™äº›æ–¹æ³•åœ¨é«˜é£é™©åŒ»ç–—ç¯å¢ƒä¸­çš„å¯ä¿¡åº¦ã€‚</p>
<p><strong>Method:</strong> BiomedXProé‡‡ç”¨è¿›åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç”Ÿç‰©åŒ»å­¦çŸ¥è¯†æå–å™¨å’Œè‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–çš„å¯è§£é‡Šè‡ªç„¶è¯­è¨€æç¤ºå¯¹é›†åˆã€‚è¯¥æ–¹æ³•é€šè¿‡é›†æˆå­¦ä¹ ç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆçš„æç¤ºèƒ½å¤Ÿæ•æ‰ä¸´åºŠè¯Šæ–­çš„å¤šæ–¹é¢ç‰¹å¾ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒBiomedXProæŒç»­ä¼˜äºæœ€å…ˆè¿›çš„æç¤ºè°ƒä¼˜æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„å°‘æ ·æœ¬åœºæ™¯ä¸‹è¡¨ç°çªå‡ºã€‚åˆ†ææ˜¾ç¤ºå‘ç°çš„æç¤ºä¸ç»Ÿè®¡æ˜¾è‘—çš„ä¸´åºŠç‰¹å¾ä¹‹é—´å­˜åœ¨å¼ºè¯­ä¹‰å¯¹é½ï¼Œä¸ºæ¨¡å‹æ€§èƒ½æä¾›äº†å¯éªŒè¯çš„æ¦‚å¿µåŸºç¡€ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡ç”Ÿæˆå¤šæ ·åŒ–çš„å¯è§£é‡Šæç¤ºé›†åˆï¼ŒBiomedXProä¸ºæ¨¡å‹é¢„æµ‹æä¾›äº†å¯éªŒè¯çš„åŸºç¡€ï¼Œä»£è¡¨äº†å‘å¼€å‘æ›´å¯ä¿¡ä¸”ä¸´åºŠå¯¹é½çš„AIç³»ç»Ÿè¿ˆå‡ºçš„å…³é”®ä¸€æ­¥ã€‚è¯¥æ–¹æ³•å¢å¼ºäº†æ¨¡å‹åœ¨åŒ»ç–—å†³ç­–ä¸­çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>The clinical adoption of biomedical vision-language models is hindered by
prompt optimization techniques that produce either uninterpretable latent
vectors or single textual prompts. This lack of transparency and failure to
capture the multi-faceted nature of clinical diagnosis, which relies on
integrating diverse observations, limits their trustworthiness in high-stakes
settings. To address this, we introduce BiomedXPro, an evolutionary framework
that leverages a large language model as both a biomedical knowledge extractor
and an adaptive optimizer to automatically generate a diverse ensemble of
interpretable, natural-language prompt pairs for disease diagnosis. Experiments
on multiple biomedical benchmarks show that BiomedXPro consistently outperforms
state-of-the-art prompt-tuning methods, particularly in data-scarce few-shot
settings. Furthermore, our analysis demonstrates a strong semantic alignment
between the discovered prompts and statistically significant clinical features,
grounding the model's performance in verifiable concepts. By producing a
diverse ensemble of interpretable prompts, BiomedXPro provides a verifiable
basis for model predictions, representing a critical step toward the
development of more trustworthy and clinically-aligned AI systems.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="35-scaling-beyond-context-a-survey-of-multimodal-retrieval-augmented-generation-for-document-understanding">[35] <a href="https://arxiv.org/abs/2510.15253">Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding</a></h3>
<p><em>Sensen Gao, Shanshan Zhao, Xu Jiang, Lunhao Duan, Yong Xien Chng, Qing-Guo Chen, Weihua Luo, Kaifu Zhang, Jia-Wang Bian, Mingming Gong</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ç³»ç»Ÿç»¼è¿°äº†é¢å‘æ–‡æ¡£ç†è§£çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œæå‡ºäº†åŸºäºé¢†åŸŸã€æ£€ç´¢æ¨¡æ€å’Œç²’åº¦çš„åˆ†ç±»ä½“ç³»ï¼Œå¹¶æ€»ç»“äº†å›¾ç»“æ„å’Œæ™ºèƒ½ä½“æ¡†æ¶ç­‰å…³é”®æŠ€æœ¯è¿›å±•ï¼Œä¸ºæ–‡æ¡£AIçš„æœªæ¥å‘å±•æä¾›äº†è·¯çº¿å›¾ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æ¡£ç†è§£æ–¹æ³•é¢ä¸´å…³é”®é™åˆ¶ï¼šåŸºäºOCRçš„æµæ°´çº¿æ–¹æ³•ä¼šä¸¢å¤±ç»“æ„ç»†èŠ‚ï¼Œè€ŒåŸç”Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆè™½ç„¶æœ‰åŠ©äºæ¨¡å‹åŸºäºå¤–éƒ¨æ•°æ®ï¼Œä½†æ–‡æ¡£çš„å¤šæ¨¡æ€ç‰¹æ€§éœ€è¦æ›´å…ˆè¿›çš„èŒƒå¼æ¥è§£å†³æ–‡æœ¬ã€è¡¨æ ¼ã€å›¾è¡¨å’Œå¸ƒå±€çš„å…¨é¢æ£€ç´¢ä¸æ¨ç†é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†åŸºäºé¢†åŸŸã€æ£€ç´¢æ¨¡æ€å’Œç²’åº¦çš„åˆ†ç±»ä½“ç³»ï¼Œç³»ç»Ÿå›é¡¾äº†æ¶‰åŠå›¾ç»“æ„å’Œæ™ºèƒ½ä½“æ¡†æ¶çš„æŠ€æœ¯è¿›å±•ï¼Œæ€»ç»“äº†å…³é”®æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•å’Œåº”ç”¨åœºæ™¯ï¼Œé‡ç‚¹å…³æ³¨å¤šæ¨¡æ€RAGåœ¨æ–‡æ¡£ç†è§£ä¸­çš„ç³»ç»ŸåŒ–å®ç°æ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶æ€»ç»“äº†å¤šæ¨¡æ€RAGåœ¨æ–‡æ¡£ç†è§£é¢†åŸŸçš„å…³é”®æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•å’Œå®é™…åº”ç”¨ï¼Œè¯†åˆ«äº†å½“å‰æ–¹æ³•åœ¨æ•ˆç‡ã€ç»†ç²’åº¦è¡¨ç¤ºå’Œé²æ£’æ€§æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸ºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒæ–¹æ³•æä¾›äº†ç³»ç»Ÿæ¡†æ¶ã€‚</p>
<p><strong>Conclusion:</strong> å¤šæ¨¡æ€RAGä¸ºæ–‡æ¡£AIæä¾›äº†å…¨é¢æ£€ç´¢å’Œæ¨ç†çš„æ–°èŒƒå¼ï¼Œæœ¬æ–‡é€šè¿‡ç³»ç»Ÿç»¼è¿°æ­ç¤ºäº†è¯¥é¢†åŸŸåœ¨æ•ˆç‡ä¼˜åŒ–ã€ç»†ç²’åº¦è¡¨ç¤ºå¢å¼ºå’Œç³»ç»Ÿé²æ£’æ€§æ–¹é¢çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†æ¸…æ™°çš„è·¯çº¿å›¾å’Œå‘å±•æŒ‡å—ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Document understanding is critical for applications from financial analysis
to scientific discovery. Current approaches, whether OCR-based pipelines
feeding Large Language Models (LLMs) or native Multimodal LLMs (MLLMs), face
key limitations: the former loses structural detail, while the latter struggles
with context modeling. Retrieval-Augmented Generation (RAG) helps ground models
in external data, but documents' multimodal nature, i.e., combining text,
tables, charts, and layout, demands a more advanced paradigm: Multimodal RAG.
This approach enables holistic retrieval and reasoning across all modalities,
unlocking comprehensive document intelligence. Recognizing its importance, this
paper presents a systematic survey of Multimodal RAG for document
understanding. We propose a taxonomy based on domain, retrieval modality, and
granularity, and review advances involving graph structures and agentic
frameworks. We also summarize key datasets, benchmarks, and applications, and
highlight open challenges in efficiency, fine-grained representation, and
robustness, providing a roadmap for future progress in document AI.</p>
<h3 id="36-infinity-parser-layout-aware-reinforcement-learning-for-scanned-document-parsing">[36] <a href="https://arxiv.org/abs/2510.15349">Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing</a></h3>
<p><em>Baode Wang, Biao Wu, Weizhen Li, Meng Fang, Zuming Huang, Jun Huang, Haozhe Wang, Yanjie Liang, Ling Chen, Wei Chu, Yuan Qi</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†LayoutRLå¼ºåŒ–å­¦ä¹ æ¡†æ¶å’ŒInfinity-Parseræ¨¡å‹ï¼Œé€šè¿‡å¤åˆå¥–åŠ±æœºåˆ¶ä¼˜åŒ–æ–‡æ¡£å¸ƒå±€ç†è§£ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ–‡æ¡£è§£ææ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•åœ¨å¤„ç†æ‰«ææ–‡æ¡£è§£ææ—¶éš¾ä»¥æ³›åŒ–åˆ°å¤šæ ·åŒ–æ–‡æ¡£ç±»å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ï¼ŒåŒæ—¶é«˜è´¨é‡å¸ƒå±€æ„ŸçŸ¥è§£æè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºè¿›ä¸€æ­¥åŠ å‰§äº†è¿™ä¸€æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†LayoutRLå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå½’ä¸€åŒ–ç¼–è¾‘è·ç¦»ã€æ®µè½è®¡æ•°å‡†ç¡®æ€§å’Œé˜…è¯»é¡ºåºä¿æŒçš„å¤åˆå¥–åŠ±æ¥ä¼˜åŒ–å¸ƒå±€ç†è§£ï¼Œå¹¶æ„å»ºäº†Infinity-Doc-400Kæ•°æ®é›†æ¥è®­ç»ƒInfinity-Parserè§†è§‰è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> åœ¨OmniDocBenchã€olmOCR-Benchã€PubTabNetå’ŒFinTabNetç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒInfinity-Parseråœ¨å„ç§æ–‡æ¡£ç±»å‹ã€è¯­è¨€å’Œç»“æ„å¤æ‚åº¦ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºä¸“é—¨çš„æ–‡æ¡£è§£æç³»ç»Ÿå’Œé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨æ–‡æ¡£è§£æä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡å¤åˆå¥–åŠ±æœºåˆ¶å’Œé«˜è´¨é‡æ•°æ®é›†è®­ç»ƒï¼Œèƒ½å¤Ÿå®ç°å¼ºå¤§çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ–‡æ¡£è§£æç ”ç©¶æä¾›äº†å¯å¤ç°çš„åŸºç¡€è®¾æ–½ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>Document parsing from scanned images into structured formats remains a
significant challenge due to its complexly intertwined elements such as text
paragraphs, figures, formulas, and tables. Existing supervised fine-tuning
methods often struggle to generalize across diverse document types, leading to
poor performance, particularly on out-of-distribution data. This issue is
further exacerbated by the limited availability of high-quality training data
for layout-aware parsing tasks. To address these challenges, we introduce
LayoutRL, a reinforcement learning framework that optimizes layout
understanding through composite rewards integrating normalized edit distance,
paragraph count accuracy, and reading order preservation. To support this
training, we construct the Infinity-Doc-400K dataset, which we use to train
Infinity-Parser, a vision-language model demonstrating robust generalization
across various domains. Extensive evaluations on benchmarks including
OmniDocBench, olmOCR-Bench, PubTabNet, and FinTabNet show that Infinity-Parser
consistently achieves state-of-the-art performance across a broad range of
document types, languages, and structural complexities, substantially
outperforming both specialized document parsing systems and general-purpose
vision-language models. We will release our code, dataset, and model to
facilitate reproducible research in document parsing.</p>
<h3 id="37-fine-tuning-medgemma-for-clinical-captioning-to-enhance-multimodal-rag-over-malaysia-cpgs">[37] <a href="https://arxiv.org/abs/2510.15418">Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs</a></h3>
<p><em>Lee Qi Zun, Mohamad Zulhilmi Bin Abdul Halim, Goh Man Fye</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºå¹¶éªŒè¯äº†ä¸€ä¸ªä¸“é—¨åŒ–MedGemmaæ¨¡å‹çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜ä¿çœŸåº¦çš„åŒ»å­¦å›¾åƒæè¿°ä½œä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„ä¼˜è´¨æŸ¥è¯¢ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿçš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿåœ¨å¤„ç†åŸºäºå›¾åƒçš„åŒ»å­¦æŸ¥è¯¢æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æè¿°ç¼ºä¹ä¸´åºŠç‰¹å¼‚æ€§å’Œäº‹å®åŸºç¡€ï¼Œé™åˆ¶äº†å…¶åœ¨é©¬æ¥è¥¿äºšä¸´åºŠå®è·µæŒ‡å—ä¸­æä¾›åŸºäºäº‹å®æŒ‡å¯¼çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨çŸ¥è¯†è’¸é¦ç®¡é“åˆ›å»ºçš®è‚¤ç§‘ã€çœ¼åº•å’Œèƒ¸éƒ¨æ”¾å°„å­¦é¢†åŸŸçš„åˆæˆæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨å‚æ•°é«˜æ•ˆçš„QLoRAæ–¹æ³•å¯¹MedGemmaæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡åŒé‡è¯„ä¼°æ¡†æ¶è¡¡é‡åˆ†ç±»å‡†ç¡®æ€§å’Œä½¿ç”¨RAGASæ¡†æ¶è¯„ä¼°æè¿°å¿ å®æ€§ã€ç›¸å…³æ€§å’Œæ­£ç¡®æ€§ã€‚</p>
<p><strong>Result:</strong> å¾®è°ƒåçš„æ¨¡å‹åœ¨åˆ†ç±»æ€§èƒ½ä¸Šå–å¾—æ˜¾è‘—æå‡ï¼ŒRAGASè¯„ä¼°ç¡®è®¤äº†æè¿°å¿ å®æ€§å’Œæ­£ç¡®æ€§çš„æ˜¾è‘—æ”¹å–„ï¼ŒéªŒè¯äº†æ¨¡å‹ç”Ÿæˆå¯é ã€äº‹å®åŸºç¡€æè¿°çš„èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªä¸“é—¨åŒ–åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹çš„ç¨³å¥æµç¨‹ï¼ŒéªŒè¯äº†æ‰€å¾—æ¨¡å‹ä½œä¸ºé«˜è´¨é‡æŸ¥è¯¢ç”Ÿæˆå™¨çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¢å¼ºåŸºäºè¯æ®çš„ä¸´åºŠå†³ç­–æ”¯æŒä¸­çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>Retrieval-Augmented Generation systems are essential for providing fact-based
guidance from Malaysian Clinical Practice Guidelines. However, their
effectiveness with image-based queries is limited, as general Vision-Language
Model captions often lack clinical specificity and factual grounding. This
study proposes and validates a framework to specialize the MedGemma model for
generating high-fidelity captions that serve as superior queries. To overcome
data scarcity, we employ a knowledge distillation pipeline to create a
synthetic dataset across dermatology, fundus, and chest radiography domains,
and fine-tune MedGemma using the parameter-efficient QLoRA method. Performance
was rigorously assessed through a dual framework measuring both classification
accuracy and, via a novel application of the RAGAS framework, caption
faithfulness, relevancy, and correctness. The fine-tuned model demonstrated
substantial improvements in classification performance, while RAGAS evaluation
confirmed significant gains in caption faithfulness and correctness, validating
the models ability to produce reliable, factually grounded descriptions. This
work establishes a robust pipeline for specializing medical VLMs and validates
the resulting model as a high-quality query generator, laying the groundwork
for enhancing multimodal RAG systems in evidence-based clinical decision
support.</p>
<h3 id="38-when-seeing-is-not-enough-revealing-the-limits-of-active-reasoning-in-mllms">[38] <a href="https://arxiv.org/abs/2510.15421">When Seeing Is not Enough: Revealing the Limits of Active Reasoning in MLLMs</a></h3>
<p><em>Hongcheng Liu, Pingjie Wang, Yuhao Wang, Siqu Ou, Yanfeng Wang, Yu Wang</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†GuessBenchåŸºå‡†æ¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸»åŠ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°ç°æœ‰æ¨¡å‹åœ¨ä¸»åŠ¨è·å–ç¼ºå¤±è¯æ®å’Œè¿­ä»£å†³ç­–æ–¹é¢çš„èƒ½åŠ›è¿œä½äºè¢«åŠ¨æ¨ç†ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€ä¸»åŠ¨æ¨ç†é¢†åŸŸçš„é‡è¦ç ”ç©¶ç©ºç™½ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸»è¦å…³æ³¨è¢«åŠ¨æ¨ç†åœºæ™¯ï¼Œå³åœ¨å®Œæ•´ä¿¡æ¯ä¸‹è¿›è¡Œé€æ­¥æ¨ç†ï¼Œè¿™ä¸ç°å®ä¸–ç•Œä¸­ä¿¡æ¯ä¸å®Œæ•´çš„å®é™…åº”ç”¨åœºæ™¯å­˜åœ¨åå·®ï¼Œå› æ­¤éœ€è¦ç ”ç©¶æ¨¡å‹åœ¨ä¿¡æ¯ä¸å®Œæ•´æƒ…å†µä¸‹ä¸»åŠ¨è·å–ç¼ºå¤±è¯æ®å¹¶è¿›è¡Œè¿­ä»£å†³ç­–çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†GuessBenchåŸºå‡†ï¼ŒåŒ…å«æ„ŸçŸ¥å¯¼å‘å’ŒçŸ¥è¯†å¯¼å‘çš„å›¾åƒï¼Œè¦æ±‚æ¨¡å‹åœ¨æ²¡æœ‰ä»»åŠ¡ç‰¹å®šå…ˆéªŒçš„æƒ…å†µä¸‹ä»å€™é€‰æ± ä¸­é€‰æ‹©ç›®æ ‡å›¾åƒï¼Œé€šè¿‡ä¸»åŠ¨è·å–ç¼ºå¤±è¯æ®å’Œè¿­ä»£ä¼˜åŒ–å†³ç­–æ¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„ä¸»åŠ¨æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°äº†20ä¸ªå…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å®ƒä»¬åœ¨ä¸»åŠ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°è¿œä½äºè¢«åŠ¨æ¨ç†åœºæ™¯ï¼Œæ¶ˆèç ”ç©¶è¡¨æ˜æ„ŸçŸ¥å¢å¼ºå¯¹å°å‹æ¨¡å‹æœ‰ç›Šï¼Œè€Œæ€ç»´å¯¼å‘æ–¹æ³•åœ¨ä¸åŒè§„æ¨¡æ¨¡å‹ä¸Šéƒ½èƒ½å¸¦æ¥ä¸€è‡´æå‡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å‘ç°ç»†ç²’åº¦æ„ŸçŸ¥å’ŒåŠæ—¶å†³ç­–æ˜¯ä¸»åŠ¨æ¨ç†çš„ä¸»è¦æŒ‘æˆ˜ï¼Œæ„ŸçŸ¥å¢å¼ºå’Œæ€ç»´å¯¼å‘æ–¹æ³•ä¸ºæœªæ¥å¤šæ¨¡æ€ä¸»åŠ¨æ¨ç†ç ”ç©¶æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œè¡¨æ˜è¯¥é¢†åŸŸä»æœ‰å·¨å¤§çš„æ”¹è¿›ç©ºé—´ã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) have shown strong capabilities
across a broad range of benchmarks. However, most existing evaluations focus on
passive inference, where models perform step-by-step reasoning under complete
information. This setup is misaligned with real-world use, where seeing is not
enough. This raises a fundamental question: Can MLLMs actively acquire missing
evidence under incomplete information? To bridge this gap, we require the MLLMs
to actively acquire missing evidence and iteratively refine decisions under
incomplete information, by selecting a target image from a candidate pool
without task-specific priors. To support systematic study, we propose
GuessBench, a benchmark with both perception-oriented and knowledge-oriented
images for evaluating active reasoning in MLLMs. We evaluate 20 superior MLLMs
and find that performance on active reasoning lags far behind it on passive
settings, indicating substantial room for improvement. Further analysis
identifies fine-grained perception and timely decision-making as key
challenges. Ablation studies show that perceptual enhancements benefit smaller
models, whereas thinking-oriented methods provide consistent gains across model
sizes. These results suggest promising directions for future research on
multimodal active reasoning.</p>
<h3 id="39-mca-modality-composition-awareness-for-robust-composed-multimodal-retrieval">[39] <a href="https://arxiv.org/abs/2510.15543">MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval</a></h3>
<p><em>Qiyu Wu, Shuyang Cui, Satoshi Hayakawa, Wei-Yao Wang, Hiromi Wakaki, Yuki Mitsufuji</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡æ€ç»„åˆæ„ŸçŸ¥æ¡†æ¶ï¼Œé€šè¿‡åå¥½æŸå¤±å’Œç»„åˆæ­£åˆ™åŒ–ç›®æ ‡æ¥ç¼“è§£ç»Ÿä¸€ç¼–ç å™¨åœ¨å¤šæ¨¡æ€æ£€ç´¢ä¸­çš„æ¨¡æ€æ·å¾„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†åˆ†å¸ƒåç§»ä¸‹çš„æ£€ç´¢é²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡åŸºäºç»Ÿä¸€ç¼–ç å™¨çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç»„åˆæ£€ç´¢ä¸­å±•ç°å‡ºçµæ´»æ€§ï¼Œä½†ç ”ç©¶å‘ç°ä¼ ç»Ÿå¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„ç»Ÿä¸€ç¼–ç å™¨å®¹æ˜“å­¦ä¹ æ¨¡æ€æ·å¾„ï¼Œå¯¼è‡´åœ¨åˆ†å¸ƒåç§»ä¸‹é²æ£’æ€§è¾ƒå·®ï¼Œéœ€è¦è§£å†³è¿™ä¸€å…³é”®é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºæ¨¡æ€ç»„åˆæ„ŸçŸ¥æ¡†æ¶ï¼ŒåŒ…å«åå¥½æŸå¤±å¼ºåˆ¶å¤šæ¨¡æ€åµŒå…¥ä¼˜äºå…¶å•æ¨¡æ€å¯¹åº”ç‰©ï¼Œä»¥åŠç»„åˆæ­£åˆ™åŒ–ç›®æ ‡å°†å¤šæ¨¡æ€åµŒå…¥ä¸å…¶å•æ¨¡æ€éƒ¨åˆ†ç»„åˆçš„åŸå‹å¯¹é½ï¼Œæ˜ç¡®å»ºæ¨¡ç»„åˆè¡¨ç¤ºä¸å•æ¨¡æ€å¯¹åº”ç‰©ä¹‹é—´çš„ç»“æ„å…³ç³»ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å¸ƒå¤–æ£€ç´¢ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æå‡ï¼ŒéªŒè¯äº†æ¨¡æ€ç»„åˆæ„ŸçŸ¥ä½œä¸ºåˆ©ç”¨MLLMsä½œä¸ºç»Ÿä¸€ç¼–ç å™¨æ—¶å®ç°é²æ£’ç»„åˆå¤šæ¨¡æ€æ£€ç´¢çš„æœ‰æ•ˆåŸåˆ™ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯æ˜äº†æ¨¡æ€ç»„åˆæ„ŸçŸ¥æ˜¯æå‡ç»Ÿä¸€ç¼–ç å™¨åœ¨å¤šæ¨¡æ€æ£€ç´¢ä¸­é²æ£’æ€§çš„å…³é”®æœºåˆ¶ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æä¾›äº†é‡è¦æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åˆ†å¸ƒåç§»å’Œæ¨¡æ€äº¤äº’æ–¹é¢å…·æœ‰å¹¿æ³›é€‚ç”¨æ€§ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Multimodal retrieval, which seeks to retrieve relevant content across
modalities such as text or image, supports applications from AI search to
contents production. Despite the success of separate-encoder approaches like
CLIP align modality-specific embeddings with contrastive learning, recent
multimodal large language models (MLLMs) enable a unified encoder that directly
processes composed inputs. While flexible and advanced, we identify that
unified encoders trained with conventional contrastive learning are prone to
learn modality shortcut, leading to poor robustness under distribution shifts.
We propose a modality composition awareness framework to mitigate this issue.
Concretely, a preference loss enforces multimodal embeddings to outperform
their unimodal counterparts, while a composition regularization objective
aligns multimodal embeddings with prototypes composed from its unimodal parts.
These objectives explicitly model structural relationships between the composed
representation and its unimodal counterparts. Experiments on various benchmarks
show gains in out-of-distribution retrieval, highlighting modality composition
awareness as a effective principle for robust composed multimodal retrieval
when utilizing MLLMs as the unified encoder.</p>
<h3 id="40-leveraging-llms-for-context-aware-implicit-textual-and-multimodal-hate-speech-detection">[40] <a href="https://arxiv.org/abs/2510.15685">Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection</a></h3>
<p><em>Joshua Wolfe Brook, Ilia Markov</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»‡æ¨è¨€è®ºæ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºåŠ¨æ€çŸ¥è¯†åº“ç”ŸæˆèƒŒæ™¯ä¸Šä¸‹æ–‡ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°åˆ†ç±»å™¨è¾“å…¥ä¸­ã€‚å®éªŒè¡¨æ˜ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ•´åˆæ–¹æ³•å¯¹æå‡æ£€æµ‹æ€§èƒ½è‡³å…³é‡è¦ï¼Œåœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€è®¾ç½®ä¸‹åˆ†åˆ«å®ç°äº†æœ€é«˜3å’Œ6ä¸ªF1åˆ†æ•°çš„æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ä»‡æ¨è¨€è®ºæ£€æµ‹æ–¹æ³•åœ¨å¤„ç†éšå«ä»‡æ¨è¨€è®ºå’Œå¤šæ¨¡æ€å†…å®¹æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç¼ºä¹å¯¹èƒŒæ™¯ä¸Šä¸‹æ–‡çš„æœ‰æ•ˆåˆ©ç”¨ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¦‚ä½•åˆ©ç”¨LLMsç”Ÿæˆç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å¢å¼ºä»‡æ¨è¨€è®ºæ£€æµ‹ç³»ç»Ÿçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éœ€è¦æ·±å±‚ç†è§£çš„éšå«è¡¨è¾¾å’Œå¤šæ¨¡æ€å†…å®¹æ—¶ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸¤ç§ä¸Šä¸‹æ–‡ç”Ÿæˆç­–ç•¥ï¼šåŸºäºå‘½åå®ä½“çš„ç”Ÿæˆå’ŒåŸºäºå…¨æ–‡æç¤ºçš„ç”Ÿæˆã€‚æ¯”è¾ƒäº†å››ç§ä¸Šä¸‹æ–‡æ•´åˆæ–¹æ³•ï¼šæ–‡æœ¬æ‹¼æ¥ã€åµŒå…¥æ‹¼æ¥ã€åŸºäºå±‚æ¬¡åŒ–Transformerçš„èåˆä»¥åŠLLMé©±åŠ¨çš„æ–‡æœ¬å¢å¼ºã€‚å®éªŒåœ¨æ–‡æœ¬éšå«ä»‡æ¨æ•°æ®é›†Latent Hatredå’Œå¤šæ¨¡æ€åŒå¥³è¡¨æƒ…åŒ…æ•°æ®é›†MAMIä¸Šè¿›è¡Œã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ•´åˆæ–¹æ³•å¯¹æ€§èƒ½æå‡è‡³å…³é‡è¦ã€‚ä»é›¶ä¸Šä¸‹æ–‡åŸºçº¿åˆ°æœ€ä½³æ€§èƒ½ç³»ç»Ÿï¼Œåœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€è®¾ç½®ä¸‹åˆ†åˆ«å®ç°äº†æœ€é«˜3å’Œ6ä¸ªF1åˆ†æ•°çš„æå‡ã€‚åŸºäºåµŒå…¥æ‹¼æ¥çš„æ–¹æ³•åœ¨æ‰€æœ‰æ•´åˆç­–ç•¥ä¸­è¡¨ç°æœ€ä½³ï¼Œè¯æ˜äº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆèƒ½å¤Ÿæ˜¾è‘—æå‡ä»‡æ¨è¨€è®ºæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜åˆ©ç”¨LLMsç”ŸæˆèƒŒæ™¯ä¸Šä¸‹æ–‡èƒ½å¤Ÿæœ‰æ•ˆæå‡ä»‡æ¨è¨€è®ºæ£€æµ‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éšå«è¡¨è¾¾å’Œå¤šæ¨¡æ€å†…å®¹æ—¶ã€‚ä¸Šä¸‹æ–‡æ•´åˆæ–¹æ³•çš„é€‰æ‹©å¯¹æœ€ç»ˆæ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼ŒåµŒå…¥æ‹¼æ¥ç­–ç•¥æ˜¾ç¤ºå‡ºæœ€ä½³æ•ˆæœã€‚è¿™ä¸ºæœªæ¥åŸºäºä¸Šä¸‹æ–‡çš„ä»‡æ¨è¨€è®ºæ£€æµ‹ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒæ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>This research introduces a novel approach to textual and multimodal Hate
Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge
bases to generate background context and incorporate it into the input of HSD
classifiers. Two context generation strategies are examined: one focused on
named entities and the other on full-text prompting. Four methods of
incorporating context into the classifier input are compared: text
concatenation, embedding concatenation, a hierarchical transformer-based
fusion, and LLM-driven text enhancement. Experiments are conducted on the
textual Latent Hatred dataset of implicit hate speech and applied in a
multimodal setting on the MAMI dataset of misogynous memes. Results suggest
that both the contextual information and the method by which it is incorporated
are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups
respectively, from a zero-context baseline to the highest-performing system,
based on embedding concatenation.</p>
<h3 id="41-speechllms-for-large-scale-contextualized-zero-shot-slot-filling">[41] <a href="https://arxiv.org/abs/2510.15851">SpeechLLMs for Large-scale Contextualized Zero-shot Slot Filling</a></h3>
<p><em>Kadri Hacioglu, Manjunath K E, Andreas Stolcke</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é€šè¿‡å»ºç«‹æ§½å¡«å……ä»»åŠ¡çš„å®è¯ä¸Šé™ï¼Œè¯†åˆ«äº†è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹åœ¨æ€§èƒ½ã€é²æ£’æ€§å’Œæ³›åŒ–æ€§æ–¹é¢çš„å·®è·ï¼Œå¹¶æå‡ºäº†è®­ç»ƒæ•°æ®ã€æ¶æ„å’Œè®­ç»ƒç­–ç•¥çš„æ”¹è¿›æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿæ§½å¡«å……ä»»åŠ¡é‡‡ç”¨è¯­éŸ³è¯†åˆ«ä¸è‡ªç„¶è¯­è¨€ç†è§£çº§è”æ¶æ„ï¼Œè€Œæ–°å…´çš„è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹è™½ç„¶æä¾›äº†ç»Ÿä¸€ç”Ÿæˆå¼è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»å­˜åœ¨æ€§èƒ½ã€é²æ£’æ€§å’Œæ³›åŒ–æ€§æ–¹é¢çš„ä¸è¶³ï¼Œéœ€è¦ç³»ç»Ÿæ€§åœ°è¯†åˆ«å¹¶ç¼©å°ä¸ç†è®ºä¸Šé™çš„å·®è·ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é€šè¿‡å»ºç«‹æ§½å¡«å……ä»»åŠ¡çš„å®è¯æ€§èƒ½ä¸Šé™ï¼Œç³»ç»Ÿåˆ†æäº†è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°æ®ã€æ¶æ„å’Œè®­ç»ƒç­–ç•¥æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶é’ˆå¯¹æ€§åœ°æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä¼˜åŒ–è®­ç»ƒæ•°æ®æ„é€ ã€æ¨¡å‹æ¶æ„è°ƒæ•´ä»¥åŠè®­ç»ƒç­–ç•¥æ”¹è¿›ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å„é¡¹æ”¹è¿›æªæ–½å‡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œæœ‰æ•ˆç¼©å°äº†ä¸å®è¯ä¸Šé™çš„å·®è·ï¼ŒåŒæ—¶æ­ç¤ºäº†å®é™…åº”ç”¨ä¸­çš„æŒ‘æˆ˜ï¼Œä¸ºåˆ©ç”¨è¿™äº›æ–°å…´æ¨¡å‹æä¾›äº†å®è¯æŒ‡å¯¼ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ä»…æå‡äº†è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹åœ¨æ§½å¡«å……ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæ›´é‡è¦çš„æ˜¯æä¾›äº†ç³»ç»Ÿæ€§çš„æ”¹è¿›æ¡†æ¶å’Œå®è¯æŒ‡å¯¼ï¼Œä¸ºæœªæ¥è¯­éŸ³ç†è§£ä»»åŠ¡çš„ç»Ÿä¸€ç”Ÿæˆå¼è§£å†³æ–¹æ¡ˆå‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>Slot filling is a crucial subtask in spoken language understanding (SLU),
traditionally implemented as a cascade of speech recognition followed by one or
more natural language understanding (NLU) components. The recent advent of
speech-based large language models (speechLLMs), which integrate speech and
textual foundation models, has opened new avenues for achieving speech
understanding tasks in a more unified, generative, and instruction-following
manner while promising data and compute efficiency with zero-shot abilities,
generalizing to unseen slot labels. We address the slot-filling task by
creating an empirical upper bound for the task, identifying performance,
robustness, and generalization gaps, and proposing improvements to the training
data, architecture, and training strategies to narrow the gap with the upper
bound result. We show that each of these measures improve performance
substantially, while highlighting practical challenges and providing empirical
guidance and insights for harnessing these emerging models.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="42-augustus-an-llm-driven-multimodal-agent-system-with-contextualized-user-memory">[42] <a href="https://arxiv.org/abs/2510.15261">AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory</a></h3>
<p><em>Jitesh Jain, Shubham Maheshwari, Ning Yu, Wen-mei Hwu, Humphrey Shi</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†AUGUSTUSï¼Œä¸€ç§åŸºäºè®¤çŸ¥ç§‘å­¦äººç±»è®°å¿†ç†è®ºçš„å¤šæ¨¡æ€æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡å›¾ç»“æ„çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡è®°å¿†å®ç°æ¦‚å¿µé©±åŠ¨çš„ä¿¡æ¯æ£€ç´¢ï¼Œåœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸­æ¯”ä¼ ç»Ÿå¤šæ¨¡æ€RAGæ–¹æ³•å¿«3.5å€ï¼Œå¹¶åœ¨MSCåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºMemGPTã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ™ºèƒ½ä½“ç³»ç»Ÿä¸»è¦å…³æ³¨æ–‡æœ¬ä¿¡æ¯çš„å­˜å‚¨ï¼Œå¿½è§†äº†å¤šæ¨¡æ€ä¿¡å·çš„é‡è¦æ€§ï¼Œè€Œäººç±»è®°å¿†æœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€çš„ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬å¼€å‘ä¸è®¤çŸ¥ç§‘å­¦ä¸­äººç±»è®°å¿†ç†è®ºå¯¹é½çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“ç³»ç»Ÿæ¥è§£å†³è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> ç³»ç»Ÿé‡‡ç”¨å››é˜¶æ®µå¾ªç¯æ¶æ„ï¼šç¼–ç ç†è§£è¾“å…¥ã€å­˜å‚¨é‡è¦ä¿¡æ¯åˆ°è®°å¿†ã€ä»è®°å¿†ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ã€æ‰§è¡Œä»»åŠ¡ï¼›ä¸åŒäºä¼ ç»Ÿå‘é‡æ•°æ®åº“ï¼Œæˆ‘ä»¬æå‡ºå°†ä¿¡æ¯æ¦‚å¿µåŒ–ä¸ºè¯­ä¹‰æ ‡ç­¾ï¼Œå¹¶å°†å…¶ä¸ä¸Šä¸‹æ–‡å…³è”å­˜å‚¨åœ¨åŸºäºå›¾ç»“æ„çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡è®°å¿†ä¸­ï¼Œå®ç°é«˜æ•ˆçš„æ¦‚å¿µé©±åŠ¨æ£€ç´¢ã€‚</p>
<p><strong>Result:</strong> åœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œç³»ç»Ÿæ€§èƒ½ä¼˜äºä¼ ç»Ÿå¤šæ¨¡æ€RAGæ–¹æ³•ä¸”é€Ÿåº¦å¿«3.5å€ï¼Œåœ¨MSCåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†MemGPTçš„è¡¨ç°ï¼ŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å°†è®¤çŸ¥ç§‘å­¦ä¸­çš„äººç±»è®°å¿†åŸç†èå…¥å¤šæ¨¡æ€æ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡èƒ½å¤Ÿæ˜¾è‘—æå‡æ€§èƒ½ï¼Œå›¾ç»“æ„çš„è¯­ä¹‰æ ‡ç­¾è®°å¿†æœºåˆ¶ä¸ºå¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢æä¾›äº†æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæœªæ¥æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®°å¿†æ¶æ„è®¾è®¡æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>Riding on the success of LLMs with retrieval-augmented generation (RAG),
there has been a growing interest in augmenting agent systems with external
memory databases. However, the existing systems focus on storing text
information in their memory, ignoring the importance of multimodal signals.
Motivated by the multimodal nature of human memory, we present AUGUSTUS, a
multimodal agent system aligned with the ideas of human memory in cognitive
science. Technically, our system consists of 4 stages connected in a loop: (i)
encode: understanding the inputs; (ii) store in memory: saving important
information; (iii) retrieve: searching for relevant context from memory; and
(iv) act: perform the task. Unlike existing systems that use vector databases,
we propose conceptualizing information into semantic tags and associating the
tags with their context to store them in a graph-structured multimodal
contextual memory for efficient concept-driven retrieval. Our system
outperforms the traditional multimodal RAG approach while being 3.5 times
faster for ImageNet classification and outperforming MemGPT on the MSC
benchmark.</p>
<h3 id="43-webgen-v-bench-structured-representation-for-enhancing-visual-design-in-llm-based-web-generation-and-evaluation">[43] <a href="https://arxiv.org/abs/2510.15306">WebGen-V Bench: Structured Representation for Enhancing Visual Design in LLM-based Web Generation and Evaluation</a></h3>
<p><em>Kuang-Da Wang, Zhao Wang, Yotaro Shimose, Wei-Yao Wang, Shingo Takamatsu</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>WebGen-Væå‡ºäº†ä¸€ä¸ªç”¨äºæŒ‡ä»¤åˆ°HTMLç”Ÿæˆçš„æ–°åŸºå‡†å’Œæ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°å®ç°äº†æ•°æ®è´¨é‡å’Œè¯„ä¼°ç²’åº¦çš„åŒé‡æå‡ï¼šæ— ç•Œå¯æ‰©å±•çš„ä»£ç†çˆ¬å–æ¡†æ¶ã€ç»“æ„åŒ–åˆ†èŠ‚æ•°æ®è¡¨ç¤ºä»¥åŠåˆ†èŠ‚å¤šæ¨¡æ€è¯„ä¼°åè®®ã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æŒ‡ä»¤åˆ°HTMLç”Ÿæˆä»»åŠ¡é¢ä¸´æ•°æ®è´¨é‡ä¸è¶³å’Œè¯„ä¼°ç²’åº¦ç²—ç³™çš„é—®é¢˜ï¼Œéœ€è¦æ›´çœŸå®çš„æ•°æ®æ”¶é›†æ–¹æ³•å’Œæ›´ç²¾ç»†çš„å¤šæ¨¡æ€è¯„ä¼°æœºåˆ¶æ¥æå‡ç½‘é¡µç”Ÿæˆçš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Method:</strong> WebGen-Vå¼•å…¥äº†ä¸‰ä¸ªæ ¸å¿ƒæŠ€æœ¯ï¼šæ— ç•Œå¯æ‰©å±•çš„ä»£ç†çˆ¬å–æ¡†æ¶æŒç»­æ”¶é›†çœŸå®ç½‘é¡µæ•°æ®ï¼›ç»“æ„åŒ–åˆ†èŠ‚æ•°æ®è¡¨ç¤ºæ•´åˆå…ƒæ•°æ®ã€å±€éƒ¨UIæˆªå›¾å’ŒJSONæ ¼å¼çš„æ–‡æœ¬å›¾åƒèµ„æºï¼›åˆ†èŠ‚å¤šæ¨¡æ€è¯„ä¼°åè®®å¯¹é½æ–‡æœ¬ã€å¸ƒå±€å’Œè§†è§‰ç»„ä»¶è¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹å®éªŒå’Œæ¶ˆèç ”ç©¶éªŒè¯äº†ç»“æ„åŒ–æ•°æ®å’Œåˆ†èŠ‚è¯„ä¼°çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ï¼Œå®ç°äº†ä»çœŸå®æ•°æ®é‡‡é›†åˆ°ç»“æ„åŒ–å¤šæ¨¡æ€è¯„ä¼°çš„ç»Ÿä¸€æµç¨‹ã€‚</p>
<p><strong>Conclusion:</strong> WebGen-Væ˜¯é¦–ä¸ªå®ç°é«˜ç²’åº¦ä»£ç†çˆ¬å–å’Œè¯„ä¼°çš„æŒ‡ä»¤åˆ°HTMLç”Ÿæˆå·¥ä½œï¼Œä¸ºç½‘é¡µç”Ÿæˆä»»åŠ¡æä¾›äº†ä»æ•°æ®è·å–åˆ°å¤šæ¨¡æ€è¯„ä¼°çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆç½‘é¡µçš„è´¨é‡å’Œè¯„ä¼°ç²¾åº¦ã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>Witnessed by the recent advancements on leveraging LLM for coding and
multimodal understanding, we present WebGen-V, a new benchmark and framework
for instruction-to-HTML generation that enhances both data quality and
evaluation granularity. WebGen-V contributes three key innovations: (1) an
unbounded and extensible agentic crawling framework that continuously collects
real-world webpages and can leveraged to augment existing benchmarks; (2) a
structured, section-wise data representation that integrates metadata,
localized UI screenshots, and JSON-formatted text and image assets, explicit
alignment between content, layout, and visual components for detailed
multimodal supervision; and (3) a section-level multimodal evaluation protocol
aligning text, layout, and visuals for high-granularity assessment. Experiments
with state-of-the-art LLMs and ablation studies validate the effectiveness of
our structured data and section-wise evaluation, as well as the contribution of
each component. To the best of our knowledge, WebGen-V is the first work to
enable high-granularity agentic crawling and evaluation for instruction-to-HTML
generation, providing a unified pipeline from real-world data acquisition and
webpage generation to structured multimodal assessment.</p>
<h3 id="44-veritas-leveraging-vision-priors-and-expert-fusion-to-improve-multimodal-data">[44] <a href="https://arxiv.org/abs/2510.15317">VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal Data</a></h3>
<p><em>Tingqiao Xu, Ziru Zeng, Jiayu Chen</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºVERITASç®¡é“ï¼Œé€šè¿‡æ•´åˆè§†è§‰å…ˆéªŒå’Œå¤šæ¨¡æ€å¤§æ¨¡å‹çš„ç»Ÿè®¡èåˆæ–¹æ³•ï¼Œç³»ç»Ÿæ€§æå‡ç›‘ç£å¾®è°ƒæ•°æ®çš„è´¨é‡ï¼Œæ˜¾è‘—å‡å°‘äº‹å®é”™è¯¯å’Œå¹»è§‰é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§æ¨¡å‹çš„ç›‘ç£å¾®è°ƒæ•°æ®å¢å¼ºæ–¹æ³•å­˜åœ¨ä¸¥é‡çš„è§†è§‰æ„ŸçŸ¥ä¸è¶³é—®é¢˜ï¼Œå¯¼è‡´äº‹å®é”™è¯¯å’Œå¹»è§‰é¢‘å‘ï¼ŒäºŸéœ€ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆæ•´åˆè§†è§‰ä¿¡æ¯å¹¶ç¡®ä¿æ•°æ®å‡†ç¡®æ€§çš„ç³»ç»ŸåŒ–è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> VERITASé‡‡ç”¨è§†è§‰è¯†åˆ«æ¨¡å‹RAM++å’ŒOCRç³»ç»ŸPP-OCRv4æå–ç»“æ„åŒ–è§†è§‰å…ˆéªŒï¼Œç»“åˆä¸‰ä¸ªå…ˆè¿›LMMæ¨¡å‹è¿›è¡Œç­”æ¡ˆè¯„ä¼°ï¼Œé€šè¿‡ç»Ÿè®¡èåˆç”Ÿæˆé«˜ç½®ä¿¡åº¦å…±è¯†åˆ†æ•°ä½œä¸ºçœŸå®æ ‡ç­¾ï¼Œå¹¶åˆ©ç”¨Group Relative Policy Optimizationè®­ç»ƒè½»é‡çº§æ‰¹è¯„æ¨¡å‹ï¼Œæœ€ç»ˆé€‰æ‹©æœ€é«˜åˆ†å€™é€‰ç­”æ¡ˆä½œä¸ºç²¾ç‚¼ç»“æœã€‚</p>
<p><strong>Result:</strong> åœ¨å…­ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨VERITASå¤„ç†æ•°æ®å¾®è°ƒçš„æ¨¡å‹æ€§èƒ½å…¨é¢è¶…è¶ŠåŸå§‹æ•°æ®ï¼Œå°¤å…¶åœ¨æ–‡æœ¬ä¸°å¯Œå’Œç»†ç²’åº¦æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºï¼Œæ‰¹è¯„æ¨¡å‹åœ¨ä¿æŒä¸å…ˆè¿›LMMç›¸å½“èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> VERITASè¯æ˜äº†ç³»ç»Ÿæ€§æ•´åˆè§†è§‰å…ˆéªŒå’Œå¤šæ¨¡å‹å…±è¯†æœºåˆ¶å¯¹æå‡SFTæ•°æ®è´¨é‡çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šæ¨¡æ€æ•°æ®ä¼˜åŒ–æä¾›äº†å¯å¤ç°çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶å±•ç¤ºäº†è½»é‡çº§æ‰¹è¯„æ¨¡å‹åœ¨ä¿æŒæ€§èƒ½å‰æä¸‹å®ç°æ•ˆç‡ä¼˜åŒ–çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>The quality of supervised fine-tuning (SFT) data is crucial for the
performance of large multimodal models (LMMs), yet current data enhancement
methods often suffer from factual errors and hallucinations due to inadequate
visual perception. To address this challenge, we propose VERITAS, a pipeline
that systematically integrates vision priors and multiple state-of-the-art LMMs
with statistical methods to enhance SFT data quality. VERITAS leverages visual
recognition models (RAM++) and OCR systems (PP-OCRv4) to extract structured
vision priors, which are combined with images, questions, and answers. Three
LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) evaluate the original answers,
providing critique rationales and scores that are statistically fused into a
high-confidence consensus score serving as ground truth. Using this consensus,
we train a lightweight critic model via Group Relative Policy Optimization
(GRPO), enhancing reasoning capabilities efficiently. Each LMM then refines the
original answers based on the critiques, generating new candidate answers; we
select the highest-scoring one as the final refined answer. Experiments across
six multimodal benchmarks demonstrate that models fine-tuned with data
processed by VERITAS consistently outperform those using raw data, particularly
in text-rich and fine-grained reasoning tasks. Our critic model exhibits
enhanced capability comparable to state-of-the-art LMMs while being
significantly more efficient. We release our pipeline, datasets, and model
checkpoints to advance research in multimodal data optimization.</p>
<h3 id="45-towards-flash-thinking-via-decoupled-advantage-policy-optimization">[45] <a href="https://arxiv.org/abs/2510.15374">Towards Flash Thinking via Decoupled Advantage Policy Optimization</a></h3>
<p><em>Zezhong Tan, Hang Gao, Xinhong Ma, Feng Zhang, Ziqiang Dong</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDEPOçš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„ä½æ•ˆæ¨ç†é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŠ¿è§£è€¦ç®—æ³•ã€éš¾åº¦æ„ŸçŸ¥é•¿åº¦æƒ©ç½šå’Œä¼˜åŠ¿è£å‰ªä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å“åº”é•¿åº¦å’Œè®¡ç®—æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡äº†å‡†ç¡®ç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¼ºåŒ–å­¦ä¹ ç®—æ³•è™½ç„¶æ˜¾è‘—æå‡äº†å¤§å‹æ¨ç†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä½†ä»å­˜åœ¨å“åº”è¿‡é•¿å’Œè¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†å»¶è¿Ÿå¢åŠ å’Œè®¡ç®—èµ„æºæµªè´¹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç®€å•ä»»åŠ¡æ—¶å°¤ä¸ºæ˜æ˜¾ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Method:</strong> DEPOæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåˆ›æ–°çš„ä¼˜åŠ¿è§£è€¦ç®—æ³•ç”¨äºæŒ‡å¯¼æ¨¡å‹å‡å°‘ä½æ•ˆtokenï¼›éš¾åº¦æ„ŸçŸ¥é•¿åº¦æƒ©ç½šæœºåˆ¶ä»¥é™ä½æ•´ä½“å“åº”é•¿åº¦ï¼›ä¼˜åŠ¿è£å‰ªæ–¹æ³•é˜²æ­¢ç­–ç•¥ä¼˜åŒ–ä¸­çš„åå·®ã€‚è¯¥æ–¹æ³•åº”ç”¨äºDeepSeek-Distill-Qwenç³»åˆ—æ¨¡å‹è¿›è¡ŒéªŒè¯ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨DeepSeek-Distill-Qwen-7Bå’Œ1.5Bæ¨¡å‹ä¸Šï¼ŒDEPOå®ç°äº†åºåˆ—é•¿åº¦å‡å°‘39%çš„æ˜¾è‘—æ•ˆæœï¼ŒåŒæ—¶å‡å°‘äº†ä½æ•ˆtokenä¸­çš„è¿‡åº¦æ¨ç†è·¯å¾„ã€‚è¯¥æ–¹æ³•åœ¨é™ä½è®¡ç®—æ¶ˆè€—çš„åŒæ—¶ï¼Œæ•´ä½“å‡†ç¡®ç‡ä»ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Conclusion:</strong> DEPOæ¡†æ¶ä¸ºè§£å†³å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„ä½æ•ˆæ¨ç†é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œè¯æ˜äº†åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—ä¼˜åŒ–æ¨ç†æ•ˆç‡çš„å¯è¡Œæ€§ã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥é«˜æ•ˆæ¨ç†æ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„åº”ç”¨å…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>Recent Large Reasoning Models (LRMs) have achieved remarkable performance in
solving complex problems via supervised fine-tuning (SFT) and reinforcement
learning (RL). Although existing RL algorithms significantly enhance model
accuracy, they still suffer from excessively lengthy responses and overthinking
issues, resulting in increased inference latency and computational consumption,
especially for simple tasks that require minimal reasoning. To address this, we
propose a novel RL framework, DEPO, to reduce inefficient reasoning for models.
Our method mainly consists of three core components: (1) an innovative
advantage decoupled algorithm to guide model reduction of inefficient tokens;
(2) a difficulty-aware length penalty to lower the overall length of model
responses; (3) an advantage clipping method to prevent bias in policy
optimization. In our experiments, applied to DeepSeek-Distill-Qwen-7B and
DeepSeek-Distill-Qwen-1.5B as base models, DEPO achieves a significant
reduction in sequence length by 39% and reduces excessive reasoning paths in
inefficient tokens, while outperforming the base model in overall accuracy.</p>
<h3 id="46-hypergraph-contrastive-sensor-fusion-for-multimodal-fault-diagnosis-in-induction-motors">[46] <a href="https://arxiv.org/abs/2510.15547">Hypergraph Contrastive Sensor Fusion for Multimodal Fault Diagnosis in Induction Motors</a></h3>
<p><em>Usman Ali, Ali Zia, Waqas Ali, Umer Ramzan, Abdul Rehman, Muhammad Tayyab Chaudhry, Wei Xiang</em></p>
<h4 id="tldr_45">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è¶…å›¾å¯¹æ¯”æ³¨æ„åŠ›ç½‘ç»œï¼ˆMM-HCANï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªå°†å¯¹æ¯”å­¦ä¹ é›†æˆåˆ°å¤šæ¨¡æ€ä¼ æ„Ÿå™¨èåˆè¶…å›¾æ‹“æ‰‘ä¸­çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºå®ç°é²æ£’çš„æ„Ÿåº”ç”µæœºå¤šæ•…éšœè¯Šæ–­ï¼Œåœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°99.82%çš„å‡†ç¡®ç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_45">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿæ„Ÿåº”ç”µæœºæ•…éšœè¯Šæ–­æ–¹æ³•éš¾ä»¥æ•æ‰å¤æ‚çš„å¤šæ¨¡æ€ä¿¡å·å…³ç³»ï¼Œé€šå¸¸å±€é™äºå•æ¨¡æ€æ•°æ®æˆ–å•ä¸€æ•…éšœç±»å‹ï¼Œä¸”åœ¨å™ªå£°æˆ–è·¨åŸŸæ¡ä»¶ä¸‹æ€§èƒ½ä¸‹é™ï¼Œæ— æ³•æ»¡è¶³å·¥ä¸šå®‰å…¨æ€§å’Œè¿è¡Œè¿ç»­æ€§çš„éœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> MM-HCANæ¡†æ¶å°†å¯¹æ¯”å­¦ä¹ é›†æˆåˆ°ä¸“é—¨ä¸ºå¤šæ¨¡æ€ä¼ æ„Ÿå™¨èåˆè®¾è®¡çš„è¶…å›¾æ‹“æ‰‘ä¸­ï¼Œèƒ½å¤Ÿè”åˆå»ºæ¨¡æ¨¡æ€å†…å’Œæ¨¡æ€é—´ä¾èµ–å…³ç³»ï¼Œå¹¶è¶…è¶Šæ¬§å‡ é‡Œå¾—åµŒå…¥ç©ºé—´å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œæ”¯æŒåŒæ—¶è¯Šæ–­è½´æ‰¿ã€å®šå­å’Œè½¬å­æ•…éšœã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMM-HCANå®ç°äº†é«˜è¾¾99.82%çš„å‡†ç¡®ç‡ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›å’Œå™ªå£°é²æ£’æ€§ï¼Œæ¶ˆèç ”ç©¶éªŒè¯äº†å„ç»„ä»¶çš„è´¡çŒ®ã€‚</p>
<p><strong>Conclusion:</strong> MM-HCANä¸ºå…¨é¢å¤šæ•…éšœè¯Šæ–­æä¾›äº†å¯æ‰©å±•ä¸”é²æ£’çš„è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒå·¥ä¸šç¯å¢ƒä¸­çš„é¢„æµ‹æ€§ç»´æŠ¤å’Œèµ„äº§å¯¿å‘½å»¶é•¿ï¼Œå±•ç¤ºäº†åœ¨å¤šæ¨¡æ€ä¼ æ„Ÿå™¨èåˆå’Œè·¨åŸŸæ³›åŒ–æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_45">ğŸ“„ Abstract</h4>
<p>Reliable induction motor (IM) fault diagnosis is vital for industrial safety
and operational continuity, mitigating costly unplanned downtime. Conventional
approaches often struggle to capture complex multimodal signal relationships,
are constrained to unimodal data or single fault types, and exhibit performance
degradation under noisy or cross-domain conditions. This paper proposes the
Multimodal Hypergraph Contrastive Attention Network (MM-HCAN), a unified
framework for robust fault diagnosis. To the best of our knowledge, MM-HCAN is
the first to integrate contrastive learning within a hypergraph topology
specifically designed for multimodal sensor fusion, enabling the joint
modelling of intra- and inter-modal dependencies and enhancing generalisation
beyond Euclidean embedding spaces. The model facilitates simultaneous diagnosis
of bearing, stator, and rotor faults, addressing the engineering need for
consolidated di- agnostic capabilities. Evaluated on three real-world
benchmarks, MM-HCAN achieves up to 99.82% accuracy with strong cross-domain
generalisation and resilience to noise, demonstrating its suitability for
real-world deployment. An ablation study validates the contribution of each
component. MM-HCAN provides a scalable and robust solution for comprehensive
multi-fault diagnosis, supporting predictive maintenance and extended asset
longevity in industrial environments.</p>
<h3 id="47-towards-relaxed-multimodal-inputs-for-gait-based-parkinsons-disease-assessment">[47] <a href="https://arxiv.org/abs/2510.15748">Towards Relaxed Multimodal Inputs for Gait-based Parkinson's Disease Assessment</a></h3>
<p><em>Minlin Zeng, Zhipeng Zhou, Yang Qiu, Zhiqi Shen</em></p>
<h4 id="tldr_46">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†é¦–ä¸ªå°†å¸•é‡‘æ£®ç—…è¯„ä¼°ä¸­çš„å¤šæ¨¡æ€å­¦ä¹ å»ºæ¨¡ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜çš„ç³»ç»ŸTRIPï¼Œè§£å†³äº†ä¼ ç»Ÿå¤šæ¨¡æ€æ–¹æ³•åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå¯¹æ¨¡æ€åŒæ­¥æ€§å’Œå®Œæ•´æ€§çš„ä¾èµ–é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_46">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¸•é‡‘æ£®ç—…è¯„ä¼°ä¸­çš„å¤šæ¨¡æ€æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šè®­ç»ƒæ—¶éœ€è¦æ‰€æœ‰æ¨¡æ€åŒæ­¥å¯ç”¨ï¼Œæ¨ç†æ—¶ä¾èµ–æ‰€æœ‰æ¨¡æ€å®Œæ•´å­˜åœ¨ï¼Œè¿™äº›é™åˆ¶ä¸¥é‡é˜»ç¢äº†å…¶å®é™…ä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Method:</strong> æå‡ºTRIPæ¡†æ¶å°†å¤šæ¨¡æ€å­¦ä¹ å»ºæ¨¡ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œé‡‡ç”¨åŸºäºè¾¹ç•Œçš„ç±»åˆ«é‡å¹³è¡¡ç­–ç•¥æ¥ç¼“è§£æ¨¡æ€å†…éƒ¨çš„ä¸å¹³è¡¡é—®é¢˜ï¼ŒåŒæ—¶å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯èåˆä¸­çš„æ¨¡æ€å´©æºƒé—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTRIPåœ¨å¼‚æ­¥è®¾ç½®ä¸‹æ¯”æœ€ä½³åŸºçº¿æ–¹æ³•æå‡äº†16.48ã€6.89å’Œ11.55ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨åŒæ­¥è®¾ç½®ä¸‹æå‡äº†4.86å’Œ2.30ä¸ªç™¾åˆ†ç‚¹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¤šç›®æ ‡ä¼˜åŒ–æ¡†æ¶åœ¨å¤šæ¨¡æ€åŒ»ç–—æ•°æ®åˆ†æä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤„ç†ä¸å®Œæ•´æˆ–å¼‚æ­¥æ¨¡æ€æ•°æ®æä¾›äº†çµæ´»ä¸”é²æ£’çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_46">ğŸ“„ Abstract</h4>
<p>Parkinson's disease assessment has garnered growing interest in recent years,
particularly with the advent of sensor data and machine learning techniques.
Among these, multimodal approaches have demonstrated strong performance by
effectively integrating complementary information from various data sources.
However, two major limitations hinder their practical application: (1) the need
to synchronize all modalities during training, and (2) the dependence on all
modalities during inference. To address these issues, we propose the first
Parkinson's assessment system that formulates multimodal learning as a
multi-objective optimization (MOO) problem. This not only allows for more
flexible modality requirements during both training and inference, but also
handles modality collapse issue during multimodal information fusion. In
addition, to mitigate the imbalance within individual modalities, we introduce
a margin-based class rebalancing strategy to enhance category learning. We
conduct extensive experiments on three public datasets under both synchronous
and asynchronous settings. The results show that our framework-Towards Relaxed
InPuts (TRIP)-achieves state-of-the-art performance, outperforming the best
baselines by 16.48, 6.89, and 11.55 percentage points in the asynchronous
setting, and by 4.86 and 2.30 percentage points in the synchronous setting,
highlighting its effectiveness and adaptability.</p>
  </article>
</body>
</html>
