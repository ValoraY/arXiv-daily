<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 43]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.AI](#cs.AI) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?](https://arxiv.org/abs/2511.20710)
*David Amebley, Sayanton Dibbo*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¥ç»ç§‘å­¦å¯å‘çš„æ‹“æ‰‘æ­£åˆ™åŒ–æ¡†æ¶ï¼Œç”¨äºå¢å¼ºå¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹å¯¹æˆå‘˜æ¨ç†æ”»å‡»çš„éšç§ä¿æŠ¤èƒ½åŠ›ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½éšç§æ”»å‡»æˆåŠŸç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€å¤šæ¨¡æ€æ¨¡å‹çš„å¹¿æ³›éƒ¨ç½²ï¼Œéšç§æ³„éœ²é£é™©æ—¥ç›Šå‡¸æ˜¾ï¼Œç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨å•æ¨¡æ€ç³»ç»Ÿçš„éšç§æ”»å‡»ï¼Œè€Œå¤šæ¨¡æ€æ¨¡å‹å¯¹éšç§æ”»å‡»çš„è„†å¼±æ€§ç ”ç©¶ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯ç¥ç»ç§‘å­¦å¯å‘çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨éšç§ä¿æŠ¤æ–¹é¢çš„æ½œåŠ›å°šæœªæ¢ç´¢ã€‚

**Method:** æå‡ºäº†ç³»ç»ŸåŒ–çš„ç¥ç»ç§‘å­¦å¯å‘çš„æ‹“æ‰‘æ­£åˆ™åŒ–æ¡†æ¶ï¼Œåœ¨ä¸‰ç§è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬BLIPã€PaliGemma 2å’ŒViT-GPT2ï¼Œä½¿ç”¨COCOã€CC3Må’ŒNoCapsä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œé€šè¿‡tau>0çš„é…ç½®å®šä¹‰ç¥ç»å˜ä½“æ¨¡å‹ã€‚

**Result:** åœ¨BLIPæ¨¡å‹å’ŒCOCOæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œç¥ç»å˜ä½“æ¨¡å‹çš„æˆå‘˜æ¨ç†æ”»å‡»æˆåŠŸç‡å¹³å‡ROC-AUCä¸‹é™24%ï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼çš„æ¨¡å‹æ•ˆç”¨ï¼ŒMPNetå’ŒROUGE-2æŒ‡æ ‡è¡¨æ˜ç”Ÿæˆè´¨é‡æœªæ˜¾è‘—å—æŸï¼Œåœ¨PaliGemma 2å’ŒViT-GPT2æ¨¡å‹ä¸Šçš„æ‰©å±•è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†ç»“æœçš„ç¨³å®šæ€§ã€‚

**Conclusion:** ç¥ç»å¯å‘çš„å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—å¢å¼ºäº†éšç§å¨èƒæŠµå¾¡èƒ½åŠ›ï¼Œä¸ºç†è§£å¤šæ¨¡æ€æ¨¡å‹éšç§é£é™©æä¾›äº†æ–°è§†è§’ï¼Œå¹¶ä¸ºå¼€å‘æ›´å®‰å…¨çš„AIç³»ç»Ÿæä¾›äº†å®è¯ä¾æ®ã€‚

---

#### ğŸ“„ Abstract
In the age of agentic AI, the growing deployment of multi-modal models (MMs) has introduced new attack vectors that can leak sensitive training data in MMs, causing privacy leakage. This paper investigates a black-box privacy attack, i.e., membership inference attack (MIA) on multi-modal vision-language models (VLMs). State-of-the-art research analyzes privacy attacks primarily to unimodal AI-ML systems, while recent studies indicate MMs can also be vulnerable to privacy attacks. While researchers have demonstrated that biologically inspired neural network representations can improve unimodal model resilience against adversarial attacks, it remains unexplored whether neuro-inspired MMs are resilient against privacy attacks. In this work, we introduce a systematic neuroscience-inspired topological regularization (tau) framework to analyze MM VLMs resilience against image-text-based inference privacy attacks. We examine this phenomenon using three VLMs: BLIP, PaliGemma 2, and ViT-GPT2, across three benchmark datasets: COCO, CC3M, and NoCaps. Our experiments compare the resilience of baseline and neuro VLMs (with topological regularization), where the tau > 0 configuration defines the NEURO variant of VLM. Our results on the BLIP model using the COCO dataset illustrate that MIA attack success in NEURO VLMs drops by 24% mean ROC-AUC, while achieving similar model utility (similarities between generated and reference captions) in terms of MPNet and ROUGE-2 metrics. This shows neuro VLMs are comparatively more resilient against privacy attacks, while not significantly compromising model utility. Our extensive evaluation with PaliGemma 2 and ViT-GPT2 models, on two additional datasets: CC3M and NoCaps, further validates the consistency of the findings. This work contributes to the growing understanding of privacy risks in MMs and provides evidence on neuro VLMs privacy threat resilience.


### [2] [DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving](https://arxiv.org/abs/2511.20720)
*Haibo HU, Lianming Huang, Nan Guan, Chun Jason Xue*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†DeeADï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„åŠ¨ä½œå¼•å¯¼æ—©æœŸé€€å‡ºæ¡†æ¶ï¼Œé€šè¿‡è¯„ä¼°ä¸­é—´è½¨è¿¹çš„ç‰©ç†å¯è¡Œæ€§æ¥åŠ é€Ÿè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„è§„åˆ’æ¨ç†ï¼Œåœ¨ä¿æŒè§„åˆ’è´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½æ¨ç†å»¶è¿Ÿã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ç»Ÿä¸€äº†æ„ŸçŸ¥ã€æ¨ç†å’Œè½¨è¿¹ç”Ÿæˆï¼Œä½†ç”±äºæ·±åº¦transformerå †æ ˆå¯¼è‡´æ¨ç†å»¶è¿Ÿæ˜¾è‘—ï¼Œéœ€è¦ä¸€ç§åœ¨ä¸ç‰ºç‰²è§„åˆ’è´¨é‡çš„å‰æä¸‹åŠ é€Ÿæ¨ç†çš„æ–¹æ³•ã€‚

**Method:** DeeADé‡‡ç”¨åŸºäºåŠ¨ä½œçš„æ—©æœŸé€€å‡ºç­–ç•¥ï¼Œå½“é¢„æµ‹è½¨è¿¹ä¸è½»é‡çº§è§„åˆ’å…ˆéªŒï¼ˆå¦‚å¯¼èˆªæˆ–ä½ç²¾åº¦è§„åˆ’ï¼‰åœ¨å¯å®¹å¿åå·®ï¼ˆ<2ç±³ï¼‰å†…å¯¹é½æ—¶ç»ˆæ­¢æ¨ç†ï¼Œå¹¶å¼•å…¥å¤šè·³æ§åˆ¶å™¨æ ¹æ®åˆ†æ•°å˜åŒ–ç‡è‡ªé€‚åº”è·³è¿‡å†—ä½™å±‚ã€‚

**Result:** åœ¨Bench2DriveåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDeeADå®ç°äº†é«˜è¾¾28%çš„transformerå±‚ç¨€ç–æ€§å’Œ29%çš„å»¶è¿Ÿé™ä½ï¼ŒåŒæ—¶ä¿æŒäº†è§„åˆ’è´¨é‡å’Œå®‰å…¨æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜åŸºäºç‰©ç†å¯è¡Œæ€§çš„æ—©æœŸé€€å‡ºç­–ç•¥å¯æœ‰æ•ˆåŠ é€ŸVLAæ¨¡å‹æ¨ç†ï¼Œä¸ºå®æ—¶è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†å®ç”¨çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒå³å¯é›†æˆåˆ°ç°æœ‰æ¨¡å‹ä¸­ã€‚

---

#### ğŸ“„ Abstract
Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.


### [3] [CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design](https://arxiv.org/abs/2511.20737)
*Daeheon Jeong, Seoyeon Byun, Kihoon Son, Dae Hyun Kim, Juho Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CANVASåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸºäºå·¥å…·çš„ç”¨æˆ·ç•Œé¢è®¾è®¡ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°æ¡†æ¶çš„ç©ºç™½ï¼Œå¹¶æ­ç¤ºäº†é¢†å…ˆæ¨¡å‹åœ¨å·¥å…·è°ƒç”¨ç­–ç•¥æ–¹é¢çš„æ”¹è¿›æ½œåŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç¼ºä¹è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸºäºå·¥å…·çš„ç”¨æˆ·ç•Œé¢è®¾è®¡èƒ½åŠ›æ–¹é¢çš„åŸºå‡†æµ‹è¯•ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹æ˜¾ç¤ºå‡ºé€šè¿‡å·¥å…·è°ƒç”¨åœ¨è®¾è®¡è½¯ä»¶ä¸­è¿­ä»£ç¼–è¾‘UIè®¾è®¡çš„æ½œåŠ›ï¼Œä½†è¿™ä¸€èƒ½åŠ›çš„å…·ä½“è¡¨ç°ä»æœªè¢«ç³»ç»Ÿè¯„ä¼°ã€‚

**Method:** CANVASåŸºå‡†åŒ…å«598ä¸ªåŸºäºå·¥å…·çš„è®¾è®¡ä»»åŠ¡ï¼Œé‡‡æ ·è‡ª3.3Kä¸ªç§»åŠ¨UIè®¾è®¡ï¼Œæ¶µç›–30ä¸ªåŠŸèƒ½ç±»åˆ«ï¼ŒåŒ…å«è®¾è®¡å¤åˆ¶å’Œè®¾è®¡ä¿®æ”¹ä¸¤ç§ä»»åŠ¡ç±»å‹ï¼Œæ¨¡å‹é€šè¿‡ä¸Šä¸‹æ–‡å·¥å…·è°ƒç”¨é€æ­¥æ›´æ–°è®¾è®¡ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜é¢†å…ˆæ¨¡å‹å±•ç°å‡ºæ›´å…·ç­–ç•¥æ€§çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†è®¾è®¡è´¨é‡ï¼ŒåŒæ—¶ç ”ç©¶è¯†åˆ«äº†æ¨¡å‹å¸¸è§çš„é”™è¯¯æ¨¡å¼ï¼Œä¸ºæœªæ¥æ”¹è¿›æä¾›äº†æŒ‡å¯¼ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå·¥å…·é©±åŠ¨çš„UIè®¾è®¡èƒ½åŠ›è¯„ä¼°å»ºç«‹äº†æ ‡å‡†åŒ–æ¡†æ¶ï¼Œæ­ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è®¾è®¡åä½œä¸­çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥å¢å¼ºåŸºäºå·¥å…·çš„è®¾è®¡èƒ½åŠ›æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
User interface (UI) design is an iterative process in which designers progressively refine their work with design software such as Figma or Sketch. Recent advances in vision language models (VLMs) with tool invocation suggest these models can operate design software to edit a UI design through iteration. Understanding and enhancing this capacity is important, as it highlights VLMs' potential to collaborate with designers within conventional software. However, as no existing benchmark evaluates tool-based design performance, the capacity remains unknown. To address this, we introduce CANVAS, a benchmark for VLMs on tool-based user interface design. Our benchmark contains 598 tool-based design tasks paired with ground-truth references sampled from 3.3K mobile UI designs across 30 function-based categories (e.g., onboarding, messaging). In each task, a VLM updates the design step-by-step through context-based tool invocations (e.g., create a rectangle as a button background), linked to design software. Specifically, CANVAS incorporates two task types: (i) design replication evaluates the ability to reproduce a whole UI screen; (ii) design modification evaluates the ability to modify a specific part of an existing screen. Results suggest that leading models exhibit more strategic tool invocations, improving design quality. Furthermore, we identify common error patterns models exhibit, guiding future work in enhancing tool-based design capabilities.


### [4] [Text-Guided Semantic Image Encoder](https://arxiv.org/abs/2511.20770)
*Raghuveer Thirukovalluru, Xiaochuang Han, Bhuwan Dhingra, Emily Dinan, Maha Elbayad*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†æ–‡æœ¬å¼•å¯¼è¯­ä¹‰å›¾åƒç¼–ç å™¨ï¼ˆTIEï¼‰ï¼Œé€šè¿‡æ–‡æœ¬æ¡ä»¶åŒ–è®­ç»ƒä½¿è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å›¾åƒç¼–ç å™¨èƒ½å¤Ÿæ ¹æ®è¾“å…¥æ–‡æœ¬æŸ¥è¯¢ç”Ÿæˆæ¡ä»¶åŒ–çš„å›¾åƒè¡¨ç¤ºï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ä»»åŠ¡çš„æ€§èƒ½å¹¶æé«˜äº†æ¨ç†æ•ˆç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å›¾åƒç¼–ç å™¨é€šå¸¸åœ¨ç‹¬ç«‹é¢„è®­ç»ƒåä¸è¯­è¨€æ¨¡å‹å¯¹é½ï¼Œè¿™ç§èŒƒå¼å¯¼è‡´ç¼–ç å™¨ä»¥ä»»åŠ¡æ— å…³çš„æ–¹å¼å¤„ç†å›¾åƒï¼Œæ— æ³•æ ¹æ®å…·ä½“ä¸‹æ¸¸ä»»åŠ¡æˆ–æ–‡æœ¬æŸ¥è¯¢è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ï¼Œé™åˆ¶äº†æ¨¡å‹å¯¹æŸ¥è¯¢ç›¸å…³è§†è§‰ç‰¹å¾çš„æ•æ‰èƒ½åŠ›ã€‚

**Method:** æå‡ºäº†æ–‡æœ¬å¼•å¯¼è¯­ä¹‰å›¾åƒç¼–ç å™¨ï¼ˆTIEï¼‰ï¼Œé€šè¿‡æ–‡æœ¬æ¡ä»¶åŒ–è®­ç»ƒä½¿å›¾åƒç¼–ç å™¨èƒ½å¤Ÿæ ¹æ®è¾“å…¥æ–‡æœ¬æŸ¥è¯¢ç”Ÿæˆæ¡ä»¶åŒ–çš„å›¾åƒè¡¨ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹æ¶æ„ç®€æ´çš„åŒæ—¶å®ç°äº†å›¾åƒè¡¨ç¤ºä¸æ–‡æœ¬æŸ¥è¯¢çš„æ·±åº¦äº¤äº’ã€‚

**Result:** åœ¨1Bå’Œ3Bè§„æ¨¡ä¸‹ï¼Œé…å¤‡TIEçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¹ä¸ªå›¾åƒåˆ°æ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­å¹³å‡åˆ†åˆ«æå‡äº†1.5å’Œ1.3ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨DocVQAå’ŒInfoVQAç­‰ä»»åŠ¡ä¸Šå¢ç›Šé«˜è¾¾6ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶ä»…ä½¿ç”¨ä¸€åŠçš„å›¾åƒåˆ†å—ï¼ˆtokenï¼‰å³å¯è¾¾åˆ°æ›´ä¼˜æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ã€‚

**Conclusion:** æ–‡æœ¬æ¡ä»¶åŒ–è®­ç»ƒæœ‰æ•ˆä¼˜åŒ–äº†ç¼–ç å™¨å¯¹å…³é”®è§†è§‰ç‰¹å¾çš„æ•æ‰èƒ½åŠ›ï¼ŒTIEèƒ½å¤ŸæŒç»­å…³æ³¨æŸ¥è¯¢ç›¸å…³åŒºåŸŸï¼Œå¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’ŒæŸ¥è¯¢ç‰¹å®šçš„åŸºç¡€èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•ä¸ºæ„å»ºæ›´é«˜æ•ˆã€æ›´å…·é€‚åº”æ€§çš„å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Image encoders, a fundamental component of vision-language models (VLMs), are typically pretrained independently before being aligned with a language model. This standard paradigm results in encoders that process images agnostically, without regard to the specific downstream task or text query. To address this limitation, we propose the Text-Guided Semantic Image Encoder (TIE), which generates image representations conditioned on the input text query. VLMs equipped with TIE outperform their conventional counterparts by +1.5 and +1.3 points on average across nine image-to-text benchmarks at the 1B and 3B scales, respectively, with gains reaching up to 6 points on tasks such as DocVQA and InfoVQA. Moreover, TIE-based VLMs attain superior performance while utilizing only half as many image tiles (tokens), resulting in notably improved inference efficiency. TIE also generalizes well with generic queries, indicating that text-conditioned training effectively optimizes the encoder to capture key visual features. Qualitative analysis confirms that TIE consistently attends to query-relevant regions, enhancing both interpretability and query-specific grounding.


### [5] [LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling](https://arxiv.org/abs/2511.20785)
*Zuhao Yang, Sudong Wang, Kaichen Zhang, Keming Wu, Sicong Leng, Yifan Zhang, Chengwei Qin, Shijian Lu, Xingxuan Li, Lidong Bing*

#### ğŸ§© TL;DR
LongVTæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡äº¤é”™çš„å¤šæ¨¡æ€å·¥å…·é“¾æ€ç»´å®ç°é•¿è§†é¢‘æ¨ç†ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å›ºæœ‰çš„æ—¶é—´å®šä½èƒ½åŠ›ä½œä¸ºåŸç”Ÿè§†é¢‘è£å‰ªå·¥å…·ï¼Œåœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿è§†é¢‘ç†è§£åŸºå‡†ä¸ŠæŒç»­ä¼˜äºç°æœ‰å¼ºåŸºçº¿ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨é•¿è§†é¢‘æ¨ç†ä¸­å®¹æ˜“äº§ç”Ÿå¹»è§‰ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯æ®ç¨€ç–ä¸”æ—¶é—´åˆ†æ•£çš„é•¿è§†é¢‘å¤„ç†åœºæ™¯ä¸‹ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå®šä½å’Œåˆ©ç”¨åˆ†æ•£çš„è§†è§‰è¯æ®ã€‚

**Method:** æå‡ºLongVTæ¡†æ¶ï¼Œåˆ©ç”¨LMMsçš„æ—¶é—´å®šä½èƒ½åŠ›ä½œä¸ºåŸç”Ÿè§†é¢‘è£å‰ªå·¥å…·ï¼Œé€šè¿‡å…¨å±€åˆ°å±€éƒ¨çš„æ¨ç†å¾ªç¯é€æ­¥æ”¾å¤§ç‰¹å®šè§†é¢‘ç‰‡æ®µå¹¶é‡æ–°é‡‡æ ·æ›´ç»†ç²’åº¦çš„è§†é¢‘å¸§ï¼Œç›´åˆ°ç­”æ¡ˆåŸºäºæ£€ç´¢åˆ°çš„è§†è§‰è¯æ®ã€‚

**Result:** åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿è§†é¢‘ç†è§£å’Œæ¨ç†åŸºå‡†ä¸ŠæŒç»­ä¼˜äºç°æœ‰å¼ºåŸºçº¿ï¼Œå¹¶æ„å»ºäº†åŒ…å«247.9Kè®­ç»ƒæ ·æœ¬å’Œ1,280ä¸ªQAå¯¹è¯„ä¼°åŸºå‡†çš„VideoSIAHæ•°æ®é›†ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡å·¥å…·é›†æˆå’Œå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒLMMsèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿è§†é¢‘æ¨ç†ä»»åŠ¡ï¼Œä¸ºé•¿è§†é¢‘ç†è§£æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆå’ŒåŸºå‡†æ•°æ®é›†ã€‚

---

#### ğŸ“„ Abstract
Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables "Thinking with Long Videos" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .


### [6] [Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models](https://arxiv.org/abs/2511.20795)
*Souradeep Dutta, Keshav Bulia, Neena S Nair*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶å¯¹KRISPçŸ¥è¯†å¢å¼ºè§†è§‰é—®ç­”æ¨¡å‹è¿›è¡Œäº†è½»é‡åŒ–å¤ç°ï¼Œé€šè¿‡å‡å°‘å‚æ•°æ•°é‡å¼€å‘äº†å¯åœ¨è¾¹ç¼˜è®¾å¤‡è¿è¡Œçš„ç´§å‡‘ç‰ˆæœ¬ï¼ŒåŒæ—¶é€šè¿‡ç³»ç»Ÿæ¶ˆèç ”ç©¶æ­ç¤ºäº†åŸå§‹æ¨¡å‹çš„è®¾è®¡ç¼ºé™·å’Œå®é™…åº”ç”¨é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŸå§‹KRISPæ¨¡å‹è™½ç„¶æœ‰æ•ˆä½†å­˜åœ¨å·¥ä¸šçº§è®­ç»ƒè§„æ¨¡å¤§ã€è®¡ç®—éœ€æ±‚é«˜ã€ä¸å¤§å‹éª¨å¹²ç½‘ç»œç´§å¯†è€¦åˆçš„é—®é¢˜ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨é‡æ–°å®¡è§†è¯¥æ¨¡å‹å¹¶å¼€å‘å‚æ•°æ˜¾è‘—å‡å°‘çš„è½»é‡çº§ç‰ˆæœ¬ï¼ŒåŒæ—¶æ¢ç´¢åœ¨èµ„æºå—é™æ¡ä»¶ä¸‹çŸ¥è¯†å¢å¼ºVQAæ¶æ„çš„å¯æ‰©å±•æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**Method:** é‡‡ç”¨è½»é‡åŒ–å¤ç°ç­–ç•¥æ˜¾è‘—å‡å°‘æ¨¡å‹å‚æ•°ï¼Œé€šè¿‡ç³»ç»Ÿæ¶ˆèç ”ç©¶åˆ†æè®¾è®¡ç¼ºé™·å’Œå®é™…é—®é¢˜ï¼ŒåŒ…æ‹¬åœ¨åˆæˆVQAæ•°æ®ä¸Šçš„æ¦‚å¿µéªŒè¯å’ŒDAQUARæ•°æ®é›†è¯„ä¼°ï¼Œæ¨¡å‹é…ç½®é‡‡ç”¨ä½å‚æ•°è®¾ç½®å¹¶å—å¤–éƒ¨çŸ¥è¯†å›¾è°±é¢†åŸŸçº¦æŸä»¥é˜²æ­¢AIå¹»è§‰ã€‚

**Result:** å¤ç°æ¨¡å‹æ€§èƒ½è¾¾åˆ°åŸå§‹KRISPçš„çº¦75%ï¼ŒåŒæ—¶é€šè¿‡æ¶ˆèç ”ç©¶æ­ç¤ºäº†åŸå§‹è®ºæ–‡æœªå……åˆ†è¦†ç›–çš„è®¾è®¡ç¼ºé™·ã€å®é™…åº”ç”¨é™·é˜±å’Œéšæ€§é—®é¢˜ï¼Œè½»é‡åŒ–è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ™ºèƒ½æ‰‹æœºå’ŒAR-VRç­‰è¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œï¼Œå®ç°ç¦»çº¿è§†è§‰æ¨ç†ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜çŸ¥è¯†å¢å¼ºVQAæ¶æ„åœ¨èµ„æºå—é™æ¡ä»¶ä¸‹ä»èƒ½ä¿æŒæœ‰æ•ˆæ€§èƒ½ï¼Œè½»é‡åŒ–å¤ç°ä¸ä»…æå‡äº†æ¨¡å‹éƒ¨ç½²çµæ´»æ€§ï¼Œè¿˜é€šè¿‡ç³»ç»Ÿåˆ†æä¸ºæœªæ¥ç±»ä¼¼æ¶æ„çš„è®¾è®¡æä¾›äº†é‡è¦è§è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨é˜²æ­¢AIå¹»è§‰å’Œè¾¹ç¼˜è®¡ç®—åº”ç”¨æ–¹é¢å…·æœ‰å®é™…ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Facebook AI Research introduced KRISP [4], which integrates structured external knowledge into pipelines for vision-language reasoning. Despite its effectiveness, the original model has been developed for industrial-scale training, is computationally demanding, and is tightly connected to a large backbone. In this work, we reexamine KRISP from a different angle and offer a lightweight reproduction with significantly fewer parameters. Even though our replicated model performs about 75 % of the original, the replication process uncovers a number of design flaws, real-world pitfalls, and implicit problems that were not fully covered in the original paper. We offer insights into the scalability and efficacy of knowledge-enhanced VQA architectures under resource constraints through systematic ablation studies, which include a proof-of-concept on synthetic VQA data and evaluation on the DAQUAR dataset. Our model, configured with a low parameter setup and constrained by the external Knowledge graph domain, prevents AI hallucinations and generates outputs solely within that domain. Minimal parameters allow us to function on edge devices like smartphones and AR-VR, further improving offline visual reasoning.


### [7] [SPHINX: A Synthetic Environment for Visual Perception and Reasoning](https://arxiv.org/abs/2511.20814)
*Md Tanvirul Alam, Saksham Aggarwal, Justin Yang Chae, Nidhi Rastogi*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†Sphinxåˆæˆç¯å¢ƒï¼Œç”¨äºè§†è§‰æ„ŸçŸ¥ä¸æ¨ç†ä»»åŠ¡è¯„ä¼°ï¼Œé€šè¿‡ç¨‹åºåŒ–ç”ŸæˆåŒ…å«å¤šç§è®¤çŸ¥åŸºå…ƒçš„è°œé¢˜ï¼Œå¹¶å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰æ¨ç†ä»»åŠ¡ç¼ºä¹èƒ½å¤Ÿç²¾ç¡®è¯„ä¼°æ ¸å¿ƒè®¤çŸ¥èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ç¯å¢ƒï¼Œç°æœ‰æ•°æ®é›†åœ¨ä»»åŠ¡å¤šæ ·æ€§å’Œå¯éªŒè¯æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥ç³»ç»ŸåŒ–åœ°è¡¡é‡æ¨¡å‹åœ¨å¯¹ç§°æ£€æµ‹ã€å‡ ä½•å˜æ¢ã€ç©ºé—´æ¨ç†ç­‰åŸºç¡€è®¤çŸ¥èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚

**Method:** å¼€å‘äº†Sphinxåˆæˆç¯å¢ƒï¼Œé€šè¿‡ç¨‹åºåŒ–ç”ŸæˆåŒ…å«å›¾æ¡ˆã€å›¾å—ã€å›¾è¡¨ã€å›¾æ ‡å’Œå‡ ä½•åŸºå…ƒçš„è°œé¢˜ï¼Œæ¯ä¸ªè°œé¢˜éƒ½é…æœ‰å¯éªŒè¯çš„çœŸå®è§£ï¼Œæ„å»ºäº†æ¶µç›–25ç§ä»»åŠ¡ç±»å‹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

**Result:** è¯„ä¼°æ˜¾ç¤ºå½“å‰æœ€å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹GPT-5åœ¨SphinxåŸºå‡†ä¸Šä»…è¾¾åˆ°51.1%çš„å‡†ç¡®ç‡ï¼Œè¿œä½äºäººç±»è¡¨ç°ï¼Œè€ŒRLVRæ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨å¤–éƒ¨è§†è§‰æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æ€§èƒ½å¢ç›Šã€‚

**Conclusion:** SphinxåŸºå‡†æ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŸºç¡€è®¤çŸ¥èƒ½åŠ›ä¸Šçš„æ˜¾è‘—ä¸è¶³ï¼ŒRLVRæ–¹æ³•å±•ç¤ºäº†é€šè¿‡å¯éªŒè¯å¥–åŠ±æœºåˆ¶æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„è§†è§‰æ¨ç†ç³»ç»Ÿæä¾›äº†æ–°çš„è®­ç»ƒèŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.


### [8] [Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion](https://arxiv.org/abs/2511.20821)
*Samuele Dell'Erba, Andrew D. Bagdanov*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¼˜åŒ–çš„è§†è§‰åè½¬æ–¹æ³•ï¼Œæ— éœ€è®­ç»ƒå³å¯æ›¿ä»£æ‰©æ•£æ¨¡å‹ä¸­çš„å…ˆéªŒç½‘ç»œï¼Œé€šè¿‡ä¼˜åŒ–æ½œåœ¨è§†è§‰è¡¨ç¤ºä¸æ–‡æœ¬åµŒå…¥çš„ç›¸ä¼¼æ€§ï¼Œå¹¶ç»“åˆä¸¤ç§æ–°çš„çº¦æŸæŸå¤±æ¥æå‡ç”Ÿæˆè´¨é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ä¾èµ–è®¡ç®—æ˜‚è´µçš„å…ˆéªŒç½‘ç»œå°†æ–‡æœ¬åµŒå…¥æ˜ å°„åˆ°è§†è§‰æµå½¢ï¼Œè¿™äº›å…ˆéªŒéœ€è¦å¤§é‡æ•°æ®å’Œè®­ç»ƒæˆæœ¬ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢æ— éœ€è®­ç»ƒå…ˆéªŒçš„æ›¿ä»£æ–¹æ¡ˆã€‚

**Method:** é‡‡ç”¨åŸºäºä¼˜åŒ–çš„è§†è§‰åè½¬æ–¹æ³•ï¼Œä»éšæœºä¼ªæ ‡è®°åˆå§‹åŒ–æ½œåœ¨è§†è§‰è¡¨ç¤ºï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–æœ€å¤§åŒ–ä¸æ–‡æœ¬æç¤ºåµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¹¶å¼•å…¥é©¬æ°è·ç¦»å’Œæœ€è¿‘é‚»æŸå¤±ä¸¤ç§çº¦æŸæ¥æ­£åˆ™åŒ–ä¼˜åŒ–è¿‡ç¨‹ã€‚

**Result:** åœ¨Kandinsky 2.2ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒOVIå¯ä»¥æ›¿ä»£ä¼ ç»Ÿå…ˆéªŒï¼ŒåŒæ—¶æ­ç¤ºäº†å½“å‰è¯„ä¼°åŸºå‡†çš„ç¼ºé™·ï¼Œçº¦æŸOVIæ–¹æ³•åœ¨è§†è§‰ä¿çœŸåº¦ä¸Šä¼˜äºåŸºçº¿ï¼Œæœ€è¿‘é‚»æ–¹æ³•åœ¨å®šé‡å¾—åˆ†ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡æœ€å…ˆè¿›çš„æ•°æ®é«˜æ•ˆå…ˆéªŒã€‚

**Conclusion:** ç ”ç©¶è¯æ˜äº†æ— éœ€è®­ç»ƒå…ˆéªŒçš„å¯è¡Œæ€§ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›äº†æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼ŒåŒæ—¶æŒ‡å‡ºäº†å½“å‰è¯„ä¼°åŸºå‡†éœ€è¦æ”¹è¿›ï¼ŒåŸºäºä¼˜åŒ–çš„æ–¹æ³•å€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶ä»¥æå‡ç”Ÿæˆè´¨é‡ã€‚

---

#### ğŸ“„ Abstract
Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.


### [9] [Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries](https://arxiv.org/abs/2511.20854)
*Sree Bhattacharyya, Yaman Kumar Singla, Sudhir Yarram, Somesh Kumar Singh, Harini S, James Z. Wang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡æ— ç›‘ç£è§†è§‰è®°å¿†æ€§æ•°æ®é›†ï¼ŒåŒ…å«82,000å¤šä¸ªè§†é¢‘åŠå…¶å›å¿†æè¿°æ•°æ®ï¼Œé€šè¿‡å¾®è°ƒå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›å¿†ç”Ÿæˆå’Œè¯åˆ°å˜´è¾¹æ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†GPT-4oç­‰æœ€å…ˆè¿›æ¨¡å‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†è§‰å†…å®¹è®°å¿†æ€§ç ”ç©¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºä»äººç±»æ”¶é›†è®°å¿†æ€§æ ‡æ³¨çš„æˆæœ¬é«˜æ˜‚ï¼Œè¿™é™åˆ¶äº†æ•°æ®é›†çš„å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ï¼Œç°æœ‰æ•°æ®é›†ä»…æ”¶é›†èšåˆè®°å¿†æ€§åˆ†æ•°è€Œæ— æ³•æ•æ‰è‡ªç„¶å¼€æ”¾å¼å›å¿†æè¿°ä¸­çš„ç»†å¾®è®°å¿†æ€§ä¿¡å·ã€‚

**Method:** åˆ©ç”¨Redditç­‰åœ¨çº¿å¹³å°çš„è¯åˆ°å˜´è¾¹æ£€ç´¢æŸ¥è¯¢æ„å»ºæ— ç›‘ç£æ•°æ®é›†ï¼Œé‡‡ç”¨å¯¹æ¯”è®­ç»ƒç­–ç•¥åˆ›å»ºé¦–ä¸ªèƒ½å¤Ÿæ‰§è¡Œå¤šæ¨¡æ€è¯åˆ°å˜´è¾¹æ£€ç´¢çš„æ¨¡å‹ï¼Œå¹¶é€šè¿‡å¾®è°ƒå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¥å¤„ç†è®°å¿†æ€§ç›¸å…³ä»»åŠ¡ã€‚

**Result:** åœ¨å›å¿†ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒåŸºäºæœ¬æ•°æ®é›†å¾®è°ƒçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè§†è§‰å†…å®¹çš„å¼€æ”¾å¼è®°å¿†æ€§æè¿°æ–¹é¢è¶…è¶Šäº†GPT-4oç­‰æœ€å…ˆè¿›æ¨¡å‹ï¼ŒåŒæ—¶æˆåŠŸå®ç°äº†å¤šæ¨¡æ€è¯åˆ°å˜´è¾¹æ£€ç´¢åŠŸèƒ½ã€‚

**Conclusion:** è¯¥æ•°æ®é›†å’Œæ¨¡å‹ä¸ºè§†è§‰å†…å®¹è®°å¿†æ€§ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œé€šè¿‡æ— ç›‘ç£æ–¹æ³•æœ‰æ•ˆè§£å†³äº†æ•°æ®æ”¶é›†çš„ç“¶é¢ˆé—®é¢˜ï¼Œä¸ºç†è§£äººç±»è®°å¿†æœºåˆ¶å’Œå¢å¼ºå†…å®¹è®¾è®¡æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚

---

#### ğŸ“„ Abstract
Visual content memorability has intrigued the scientific community for decades, with applications ranging widely, from understanding nuanced aspects of human memory to enhancing content design. A significant challenge in progressing the field lies in the expensive process of collecting memorability annotations from humans. This limits the diversity and scalability of datasets for modeling visual content memorability. Most existing datasets are limited to collecting aggregate memorability scores for visual content, not capturing the nuanced memorability signals present in natural, open-ended recall descriptions. In this work, we introduce the first large-scale unsupervised dataset designed explicitly for modeling visual memorability signals, containing over 82,000 videos, accompanied by descriptive recall data. We leverage tip-of-the-tongue (ToT) retrieval queries from online platforms such as Reddit. We demonstrate that our unsupervised dataset provides rich signals for two memorability-related tasks: recall generation and ToT retrieval. Large vision-language models fine-tuned on our dataset outperform state-of-the-art models such as GPT-4o in generating open-ended memorability descriptions for visual content. We also employ a contrastive training strategy to create the first model capable of performing multimodal ToT retrieval. Our dataset and models present a novel direction, facilitating progress in visual content memorability research.


### [10] [Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation](https://arxiv.org/abs/2511.20889)
*Taehoon Kim, Henry Gouk, Timothy Hospedales*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºNull-Text Test-Time Alignment (Null-TTA)ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶ä¼˜åŒ–åˆ†ç±»å™¨æ— å…³å¼•å¯¼ä¸­çš„æ— æ¡ä»¶åµŒå…¥è€Œéæ½œåœ¨å˜é‡ï¼Œè§£å†³äº†æµ‹è¯•æ—¶å¯¹é½ä¸­å­˜åœ¨çš„æ¬ ä¼˜åŒ–å’Œè¿‡ä¼˜åŒ–é—®é¢˜ï¼Œå®ç°äº†åœ¨è¯­ä¹‰ç›¸å¹²æµå½¢ä¸Šçš„å¯¹é½å¹¶é˜²æ­¢å¥–åŠ±ç ´è§£ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æµ‹è¯•æ—¶å¯¹é½æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€‚åº”ç‰¹å®šå¥–åŠ±æ—¶ï¼Œå¾€å¾€å­˜åœ¨æ¬ ä¼˜åŒ–æˆ–è¿‡ä¼˜åŒ–é—®é¢˜ï¼Œåè€…è¡¨ç°ä¸ºå¥–åŠ±ç ´è§£ç°è±¡ï¼Œå³æ¨¡å‹é€šè¿‡åˆ©ç”¨éè¯­ä¹‰å™ªå£°æ¨¡å¼æ¥äººä¸ºæé«˜å¥–åŠ±åˆ†æ•°ï¼Œè€ŒéçœŸæ­£å®ç°è¯­ä¹‰å±‚é¢çš„å¯¹é½ã€‚

**Method:** Null-TTAæ–¹æ³•é€šè¿‡ä¼˜åŒ–åˆ†ç±»å™¨æ— å…³å¼•å¯¼ä¸­çš„æ— æ¡ä»¶åµŒå…¥æ¥å®ç°æ‰©æ•£æ¨¡å‹çš„å¯¹é½ï¼Œç”±äºæ–‡æœ¬åµŒå…¥ç©ºé—´å…·æœ‰ç»“æ„åŒ–çš„è¯­ä¹‰ç‰¹æ€§ï¼Œè¿™ç§ä¼˜åŒ–ç¡®ä¿äº†å¯¹é½å‘ç”Ÿåœ¨è¯­ä¹‰ç›¸å¹²çš„æµå½¢ä¸Šï¼Œå³ä½¿ä¸æ›´æ–°æ¨¡å‹å‚æ•°ä¹Ÿèƒ½ç›´æ¥å¼•å¯¼ç”Ÿæˆåˆ†å¸ƒæœå‘ç›®æ ‡å¥–åŠ±ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜Null-TTAåœ¨ç›®æ ‡æµ‹è¯•æ—¶å¯¹é½ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„è·¨å¥–åŠ±æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†è¯­ä¹‰ç©ºé—´ä¼˜åŒ–ä½œä¸ºä¸€ç§æœ‰æ•ˆä¸”åŸç†æ€§çš„TTAæ–°èŒƒå¼çš„å¯è¡Œæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶ç¡®ç«‹äº†è¯­ä¹‰ç©ºé—´ä¼˜åŒ–ä½œä¸ºæµ‹è¯•æ—¶å¯¹é½çš„æœ‰æ•ˆæ–°èŒƒå¼ï¼Œé€šè¿‡åˆ©ç”¨æ–‡æœ¬åµŒå…¥ç©ºé—´çš„è¯­ä¹‰ç»“æ„ç‰¹æ€§ï¼Œæ—¢å®ç°äº†ç²¾ç¡®çš„ç›®æ ‡å¥–åŠ±å¯¹é½ï¼Œåˆé¿å…äº†å¥–åŠ±ç ´è§£é—®é¢˜ï¼Œä¸ºæ‰©æ•£æ¨¡å‹çš„æ¨ç†æ—¶é€‚åº”æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.


### [11] [BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model](https://arxiv.org/abs/2511.20956)
*Rawa Mohammed, Mina Attin, Bryar Shareef*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºBUSTRï¼Œä¸€ç§æ— éœ€é…å¯¹å›¾åƒ-æŠ¥å‘Šç›‘ç£çš„å¤šä»»åŠ¡è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä¹³è…ºè¶…å£°æŠ¥å‘Šã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“æ„åŒ–æè¿°ç¬¦å’Œæ”¾å°„ç»„å­¦ç‰¹å¾æ„å»ºæŠ¥å‘Šï¼Œåˆ©ç”¨å¤šä»»åŠ¡æŸå¤±è®­ç»ƒå¤šå¤´Swinç¼–ç å™¨å­¦ä¹ æè¿°ç¬¦æ„ŸçŸ¥çš„è§†è§‰è¡¨ç¤ºï¼Œå¹¶é€šè¿‡åŒçº§ç›®æ ‡å¯¹é½è§†è§‰å’Œæ–‡æœ¬æ ‡è®°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¹³è…ºè¶…å£°è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆé¢ä¸´ç¼ºä¹é…å¯¹å›¾åƒ-æŠ¥å‘Šæ•°æ®é›†å’Œå¤§å‹è¯­è¨€æ¨¡å‹äº§ç”Ÿå¹»è§‰é£é™©çš„é™åˆ¶ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–ç›‘ç£å­¦ä¹ éœ€è¦å¤§é‡é…å¯¹æ•°æ®ï¼Œè€Œå®é™…ä¸´åºŠç¯å¢ƒä¸­è¿™ç§æ•°æ®å¾€å¾€ç¨€ç¼ºä¸”è·å–æˆæœ¬é«˜æ˜‚ã€‚

**Method:** BUSTRæ¡†æ¶ä»ç»“æ„åŒ–æè¿°ç¬¦ï¼ˆå¦‚BI-RADSã€ç—…ç†å­¦ã€ç»„ç»‡å­¦ï¼‰å’Œæ”¾å°„ç»„å­¦ç‰¹å¾æ„å»ºæŠ¥å‘Šï¼Œä½¿ç”¨å¤šä»»åŠ¡æŸå¤±åœ¨æ•°æ®é›†ç‰¹å®šæè¿°ç¬¦é›†ä¸Šè®­ç»ƒå¤šå¤´Swinç¼–ç å™¨å­¦ä¹ æè¿°ç¬¦æ„ŸçŸ¥çš„è§†è§‰è¡¨ç¤ºï¼Œå¹¶é€šè¿‡ç»“åˆæ ‡è®°çº§äº¤å‰ç†µä¸è¾“å…¥è¾“å‡ºè¡¨ç¤ºé—´ä½™å¼¦ç›¸ä¼¼åº¦å¯¹é½æŸå¤±çš„åŒçº§ç›®æ ‡å¯¹é½è§†è§‰å’Œæ–‡æœ¬æ ‡è®°ã€‚

**Result:** åœ¨ä¸¤ä¸ªå…¬å…±ä¹³è…ºè¶…å£°æ•°æ®é›†BrEaSTå’ŒBUS-BRAä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒBUSTRåœ¨æ ‡å‡†è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡å’Œä¸´åºŠæ•ˆèƒ½æŒ‡æ ‡ä¸Šå‡å–å¾—ä¸€è‡´æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨BI-RADSç±»åˆ«å’Œç—…ç†å­¦ç­‰å…³é”®ç›®æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒè§„æ¨¡å’Œå¯ç”¨æè¿°ç¬¦çš„æ•°æ®é›†ä¸Šéƒ½å±•ç°å‡ºç¨³å¥æ€§èƒ½ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æè¿°ç¬¦æ„ŸçŸ¥çš„è§†è§‰æ¨¡å‹ç»“åˆæ ‡è®°çº§å’Œå¯¹é½æŸå¤±è®­ç»ƒï¼Œæ— éœ€é…å¯¹å›¾åƒ-æŠ¥å‘Šæ•°æ®å³å¯åŒæ—¶æ”¹å–„è‡ªåŠ¨æŠ¥å‘ŠæŒ‡æ ‡å’Œä¸´åºŠæ•ˆèƒ½ã€‚è¯¥æ–¹æ³•ä¸ºåŒ»å­¦å½±åƒæŠ¥å‘Šç”Ÿæˆæä¾›äº†æ–°çš„æ— ç›‘ç£å­¦ä¹ èŒƒå¼ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR


### [12] [TrafficLens: Multi-Camera Traffic Video Analysis Using LLMs](https://arxiv.org/abs/2511.20965)
*Md Adnan Arefeen, Biplob Debnath, Srimat Chakradhar*

#### ğŸ§© TL;DR
TrafficLensæå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šæ‘„åƒå¤´äº¤é€šè·¯å£çš„å®šåˆ¶ç®—æ³•ï¼Œé€šè¿‡é¡ºåºå¤„ç†æ‘„åƒå¤´é‡å è¦†ç›–åŒºåŸŸå¹¶æ™ºèƒ½ç»•è¿‡å†—ä½™è§†è§‰è¯­è¨€æ¨¡å‹è°ƒç”¨ï¼Œå°†è§†é¢‘åˆ°æ–‡æœ¬è½¬æ¢æ—¶é—´å‡å°‘é«˜è¾¾4å€ï¼ŒåŒæ—¶ä¿æŒä¿¡æ¯å‡†ç¡®æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ‘„åƒå¤´äº¤é€šè·¯å£äº§ç”Ÿå¤§é‡è§†é¢‘æ•°æ®ï¼Œä¼ ç»Ÿæ–¹æ³•éœ€è¦å°†è§†é¢‘æ•°æ®é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºæ–‡æœ¬å†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åˆ†æï¼Œè¿™ä¸€è¿‡ç¨‹è€—æ—¶ä¸”å»¶è¿Ÿäº†äº¤é€šè§†é¢‘çš„å®æ—¶æ´å¯Ÿç”Ÿæˆå’Œäº‹ä»¶è°ƒæŸ¥èƒ½åŠ›ã€‚

**Method:** TrafficLensé‡‡ç”¨é¡ºåºå¤„ç†ç­–ç•¥ï¼Œåˆ©ç”¨æ‘„åƒå¤´é‡å è¦†ç›–åŒºåŸŸï¼Œè¿­ä»£åº”ç”¨ä¸åŒä»¤ç‰Œé™åˆ¶çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå°†å‰åºè¾“å‡ºä½œä¸ºåç»­æ‘„åƒå¤´çš„æç¤ºï¼Œå¹¶é€šè¿‡å¯¹è±¡çº§ç›¸ä¼¼æ€§æ£€æµ‹å™¨æ™ºèƒ½ç»•è¿‡å†—ä½™çš„è§†è§‰è¯­è¨€æ¨¡å‹è°ƒç”¨ã€‚

**Result:** åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTrafficLenså°†è§†é¢‘åˆ°æ–‡æœ¬è½¬æ¢æ—¶é—´å‡å°‘äº†é«˜è¾¾4å€ï¼ŒåŒæ—¶ä¿æŒäº†ä¿¡æ¯å‡†ç¡®æ€§ï¼Œå®ç°äº†é«˜æ•ˆçš„äº¤é€šè§†é¢‘åˆ†æã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡æ™ºèƒ½è°ƒåº¦è§†è§‰è¯­è¨€æ¨¡å‹è°ƒç”¨å’Œåˆ©ç”¨æ‘„åƒå¤´é‡å ä¿¡æ¯ï¼Œå¯ä»¥æ˜¾è‘—æå‡å¤šæ‘„åƒå¤´äº¤é€šè§†é¢‘åˆ†ææ•ˆç‡ï¼Œä¸ºå®æ—¶äº¤é€šç®¡ç†å’Œäº‹ä»¶å“åº”æä¾›äº†å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Traffic cameras are essential in urban areas, playing a crucial role in intelligent transportation systems. Multiple cameras at intersections enhance law enforcement capabilities, traffic management, and pedestrian safety. However, efficiently managing and analyzing multi-camera feeds poses challenges due to the vast amount of data. Analyzing such huge video data requires advanced analytical tools. While Large Language Models (LLMs) like ChatGPT, equipped with retrieval-augmented generation (RAG) systems, excel in text-based tasks, integrating them into traffic video analysis demands converting video data into text using a Vision-Language Model (VLM), which is time-consuming and delays the timely utilization of traffic videos for generating insights and investigating incidents. To address these challenges, we propose TrafficLens, a tailored algorithm for multi-camera traffic intersections. TrafficLens employs a sequential approach, utilizing overlapping coverage areas of cameras. It iteratively applies VLMs with varying token limits, using previous outputs as prompts for subsequent cameras, enabling rapid generation of detailed textual descriptions while reducing processing time. Additionally, TrafficLens intelligently bypasses redundant VLM invocations through an object-level similarity detector. Experimental results with real-world datasets demonstrate that TrafficLens reduces video-to-text conversion time by up to $4\times$ while maintaining information accuracy.


### [13] [GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety Supervision](https://arxiv.org/abs/2511.20994)
*Yuxiao Xiang, Junchi Chen, Zhenchao Jin, Changtao Miao, Haojie Yuan, Qi Chu, Tao Gong, Nenghai Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†GuardTrace-VLï¼Œä¸€ç§è§†è§‰æ„ŸçŸ¥çš„å®‰å…¨å®¡è®¡å™¨ï¼Œé€šè¿‡è”åˆå›¾åƒ-æ–‡æœ¬åˆ†æç›‘æ§å®Œæ•´çš„é—®ç­”-æ€è€ƒ-ç­”æ¡ˆæµç¨‹ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†é˜¶æ®µæ£€æµ‹ä¸å®‰å…¨å†…å®¹ã€‚è¯¥æ–¹æ³•åœ¨å®‰å…¨æ¨ç†æ£€æµ‹ä»»åŠ¡ä¸Šå®ç°äº†93.1%çš„F1åˆ†æ•°ï¼Œæ¯”ç°æœ‰å¤šæ¨¡æ€å®‰å…¨é˜²å¾¡æ–¹æ³•æå‡äº†13.5%ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§å‹æ¨ç†æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­äº§ç”Ÿæ˜¾å¼ä¸­é—´æ¨ç†è¿‡ç¨‹æ—¶ï¼Œå³ä½¿æœ€ç»ˆç­”æ¡ˆæ— å®³ï¼Œæ¨ç†è½¨è¿¹ä¹Ÿå¯èƒ½åŒ…å«ä¸å®‰å…¨å†…å®¹ï¼Œé€ æˆéƒ¨ç½²é£é™©ã€‚ç°æœ‰çš„å¤šæ¨¡æ€å®‰å…¨é˜²æŠ¤ä¸»è¦ä»…è¯„ä¼°è¾“å…¥é—®é¢˜å’Œæœ€ç»ˆç­”æ¡ˆï¼Œå¿½è§†äº†ä¸­é—´æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´åè§æ¨æ–­æˆ–è¿åæ”¿ç­–çš„è§†è§‰ä¸Šä¸‹æ–‡ä½¿ç”¨ç­‰å±å®³æœªè¢«æ£€æµ‹åˆ°ã€‚

**Method:** æå‡ºäº†GuardTrace-VLè§†è§‰æ„ŸçŸ¥å®‰å…¨å®¡è®¡å™¨ï¼Œé€šè¿‡è”åˆå›¾åƒ-æ–‡æœ¬åˆ†æç›‘æ§å®Œæ•´çš„é—®ç­”-æ€è€ƒ-ç­”æ¡ˆæµç¨‹ã€‚æ„å»ºäº†GuardTraceæ•°æ®é›†ï¼Œé‡‡ç”¨å¤šæ ·åŒ–æç¤ºç­–ç•¥ç”Ÿæˆå¹¶é€šè¿‡MLRMå’Œäººå·¥æŠ•ç¥¨éªŒè¯æµç¨‹è¿›è¡Œç²¾ç‚¼ã€‚æå‡ºäº†ä¸‰é˜¶æ®µæ¸è¿›è®­ç»ƒæ–¹æ¡ˆä¸æ•°æ®ç²¾ç‚¼è¿‡ç¨‹ç»“åˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸åŒé£é™©çº§åˆ«å­¦ä¹ ç»†è‡´å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„å®‰å…¨åå¥½ã€‚

**Result:** åœ¨æå‡ºçš„æ¶µç›–åŸŸå†…å’ŒåŸŸå¤–åœºæ™¯çš„æµ‹è¯•é›†ä¸Šï¼ŒGuardTrace-VLæ¨¡å‹åœ¨ä¸å®‰å…¨æ¨ç†æ£€æµ‹ä»»åŠ¡ä¸Šå®ç°äº†93.1%çš„F1åˆ†æ•°ï¼Œç›¸æ¯”ä¹‹å‰æœ€å¼ºçš„å¤šæ¨¡æ€å®‰å…¨é˜²å¾¡æ–¹æ³•æå‡äº†13.5%çš„F1åˆ†æ•°ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ç›‘æ§å¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„å®Œæ•´æ¨ç†æµç¨‹å¯¹äºå…¨é¢å®‰å…¨é˜²æŠ¤è‡³å…³é‡è¦ï¼Œæå‡ºçš„è”åˆå›¾åƒ-æ–‡æœ¬åˆ†æå’Œæ¸è¿›è®­ç»ƒæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°çš„å®‰å…¨é£é™©ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤šæ¨¡æ€AIç³»ç»Ÿçš„å®‰å…¨éƒ¨ç½²æä¾›äº†é‡è¦ä¿éšœï¼Œå¼ºè°ƒäº†ä¸­é—´æ¨ç†è¿‡ç¨‹å®‰å…¨ç›‘æ§çš„å¿…è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Multimodal large reasoning models (MLRMs) are increasingly deployed for vision-language tasks that produce explicit intermediate rationales. However, reasoning traces can contain unsafe content even when the final answer is non-harmful, creating deployment risks. Existing multimodal safety guards primarily evaluate only the input question and the final answer, neglecting the intermediate reasoning process. This oversight allows undetected harm, such as biased inferences or policy-violating use of visual context, to emerge during reasoning. We introduce GuardTrace-VL, a vision-aware safety auditor that monitors the full Question-Thinking-Answer (QTA) pipeline via joint image-text analysis, enabling detection of unsafe content as it emerges in the reasoning stage. To support training and evaluation, we construct the GuardTrace dataset, which is generated through diverse prompting strategies and refined via a MLRM- and human-based voting and verification pipeline. Furthermore, we propose a three-stage progressive training scheme combined with the data refinement process, enabling the model to learn nuanced and context-dependent safety preferences according to different risk levels. On our proposed test set covering both in-domain and out-of-domain scenarios, GuardTrace-VL model achieves an F1 score of 93.1% on unsafe reasoning detection tasks, representing a 13.5% improvement in F1 score compared to the previous strongest multimodal safety defense methods. The codes will be made publicly available.


### [14] [AnchorOPT: Towards Optimizing Dynamic Anchors for Adaptive Prompt Learning](https://arxiv.org/abs/2511.21188)
*Zheng Li, Yibing Song, Xin Zhang, Lei Luo, Xiang Li, Jian Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºAnchorOPTï¼Œä¸€ç§åŠ¨æ€é”šç‚¹æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å­¦ä¹ ä»»åŠ¡ç‰¹å®šçš„é”šç‚¹å€¼å’Œå¯å­¦ä¹ çš„ä½ç½®çŸ©é˜µï¼Œè§£å†³äº†ç°æœ‰CLIPæç¤ºå­¦ä¹ æ–¹æ³•ä¸­é”šç‚¹é™æ€å›ºå®šçš„å±€é™æ€§ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºCLIPçš„æç¤ºå­¦ä¹ æ–¹æ³•ä½¿ç”¨é™æ€æ–‡æœ¬æ ‡è®°ä½œä¸ºé”šç‚¹æ¥æŒ‡å¯¼å¯å­¦ä¹ è½¯æ ‡è®°ï¼Œä½†è¿™äº›é”šç‚¹åœ¨å€¼å’Œä½ç½®ä¸Šéƒ½æ˜¯é™æ€çš„ï¼Œç¼ºä¹è·¨ä»»åŠ¡å’Œé˜¶æ®µè‡ªé€‚åº”çš„çµæ´»æ€§ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Method:** AnchorOPTæ¡†æ¶åœ¨é”šç‚¹å€¼å’Œä½ç½®å…³ç³»ä¸¤ä¸ªç»´åº¦å¼•å…¥åŠ¨æ€æ€§ï¼šé”šç‚¹å€¼ä»ä»»åŠ¡ç‰¹å®šæ•°æ®ä¸­åŠ¨æ€å­¦ä¹ è€Œéä½¿ç”¨æ‰‹å·¥è®¾è®¡çš„æ˜¾å¼æ–‡æœ¬æ ‡è®°ï¼›é”šç‚¹ä¸è½¯æ ‡è®°ä¹‹é—´çš„ä½ç½®å…³ç³»é€šè¿‡å¯å­¦ä¹ çš„ä½ç½®çŸ©é˜µæ ¹æ®è®­ç»ƒé˜¶æ®µå’Œä»»åŠ¡ä¸Šä¸‹æ–‡è‡ªé€‚åº”ä¼˜åŒ–ã€‚è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå…ˆå­¦ä¹ é”šç‚¹æ ‡è®°ï¼Œç„¶åå†»ç»“å¹¶è½¬ç§»åˆ°ç¬¬äºŒé˜¶æ®µä¼˜åŒ–è½¯æ ‡è®°å’Œä½ç½®çŸ©é˜µã€‚

**Result:** å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨ç®€å•çš„å¯å­¦ä¹ é”šç‚¹å’Œä½ç½®çŸ©é˜µå°±èƒ½è¾¾åˆ°ä¸æŸäº›åŒ…å«é¢å¤–å¯å­¦ä¹ æ¨¡å—æˆ–æ­£åˆ™åŒ–æŠ€æœ¯çš„æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚ä½œä¸ºå³æ’å³ç”¨æ¨¡å—ï¼ŒAnchorOPTèƒ½æ— ç¼é›†æˆåˆ°ç°æœ‰æ¡†æ¶ä¸­ï¼Œåœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šå¸¦æ¥ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åŠ¨æ€é”šç‚¹æœºåˆ¶åœ¨æç¤ºå­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ä»»åŠ¡è‡ªé€‚åº”çš„é”šç‚¹å­¦ä¹ å’Œä½ç½®ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†CLIPæ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ï¼Œä¸ºæç¤ºå­¦ä¹ æ–¹æ³•æä¾›äº†æ–°çš„è®¾è®¡æ€è·¯ï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚

---

#### ğŸ“„ Abstract
Existing prompt learning methods, which are built upon CLIP models, leverage textual tokens as anchors to guide the learnable soft tokens. This guidance improves CLIP generalizations. However, these anchors-static in both value and position-lack cross-task and stage-adaptive flexibility. To address this limitation, we propose AnchorOPT, a dynamic anchor-based prompt learning framework. Specifically, AnchorOPT introduces dynamism in two key dimensions: (i) anchor values eschew handcrafted explicit textual tokens (e.g., "shape", "color"), instead learning dynamically from task-specific data; and (ii) the positional relationship between anchor and soft tokens is no longer fixed but adaptively optimized via a learnable position matrix conditioned on the training stage and task context. Training occurs in two stages: we first learn the anchor tokens, then freeze and transfer them to the second stage for optimization of soft tokens and the position matrix. Extensive experiments demonstrate that using only a simple learnable anchor and position matrix achieves performance comparable to or exceeding some methods incorporating additional learnable modules or regularization techniques. As a plug-and-play module, AnchorOPT integrates seamlessly into existing frameworks, yielding consistent performance gains across diverse datasets. Code is publicly available at https://github.com/zhengli97/ATPrompt.


### [15] [From Inpainting to Layer Decomposition: Repurposing Generative Inpainting Models for Image Layer Decomposition](https://arxiv.org/abs/2511.20996)
*Jingxi Chen, Yixiao Zhang, Xiaoye Qian, Zongxia Li, Cornelia Fermuller, Caren Chen, Yiannis Aloimonos*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆ†å±‚åˆ†è§£æ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§å¾®è°ƒå°†ä¿®å¤æ¨¡å‹é€‚é…äºåˆ†å±‚åˆ†è§£ä»»åŠ¡ï¼Œå¹¶å¼•å…¥å¤šæ¨¡æ€ä¸Šä¸‹æ–‡èåˆæ¨¡å—ä»¥ä¿æŒç»†èŠ‚ã€‚è¯¥æ–¹æ³•åœ¨åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œåœ¨ç›®æ ‡ç§»é™¤å’Œé®æŒ¡æ¢å¤æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡ç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å•å¼ å›¾åƒçš„åˆ†å±‚åˆ†è§£ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦å—é™äºæ–¹æ³•å’Œæ•°æ®çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†å›¾åƒä¸­çš„å‰æ™¯å¯¹è±¡ä¸èƒŒæ™¯çš„åˆ†ç¦»ä»¥åŠé®æŒ¡æ¢å¤é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å›¾åƒç¼–è¾‘çš„çµæ´»æ€§å’Œåˆ›é€ æ€§åº”ç”¨ã€‚

**Method:** æœ¬æ–‡è§‚å¯Ÿåˆ°åˆ†å±‚åˆ†è§£ä¸å›¾åƒä¿®å¤ä»»åŠ¡ä¹‹é—´çš„å¼ºå…³è”æ€§ï¼Œæå‡ºé€šè¿‡è½»é‡çº§å¾®è°ƒå°†åŸºäºæ‰©æ•£çš„ä¿®å¤æ¨¡å‹é€‚é…äºåˆ†å±‚åˆ†è§£ä»»åŠ¡ã€‚ä¸ºäº†åœ¨æ½œåœ¨ç©ºé—´ä¸­ä¿æŒç»†èŠ‚ï¼Œå¼•å…¥äº†å…·æœ‰çº¿æ€§æ³¨æ„åŠ›å¤æ‚åº¦çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å‹å®Œå…¨åœ¨å¼€æºèµ„äº§æ„å»ºçš„åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚

**Result:** è¯¥æ–¹æ³•åœ¨ç›®æ ‡ç§»é™¤å’Œé®æŒ¡æ¢å¤ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜è¶Šæ€§èƒ½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ†è§£å›¾åƒä¸ºå‰æ™¯å’ŒèƒŒæ™¯å±‚ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆæ•°æ®é›†ä¸Šçš„è®­ç»ƒæ•ˆæœæ˜¾è‘—ï¼Œä¸ºä¸‹æ¸¸ç¼–è¾‘åº”ç”¨æä¾›äº†å¯é çš„åˆ†å±‚è¡¨ç¤ºã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å°†ä¿®å¤æ¨¡å‹é€‚é…äºåˆ†å±‚åˆ†è§£ä»»åŠ¡çš„å¯è¡Œæ€§ï¼Œæå‡ºçš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡èåˆæ¨¡å—æœ‰æ•ˆè§£å†³äº†æ½œåœ¨ç©ºé—´ä¸­çš„ç»†èŠ‚ä¿æŒé—®é¢˜ã€‚è¿™é¡¹å·¥ä½œä¸ºå›¾åƒç¼–è¾‘å’Œåˆ›é€ æ€§åº”ç”¨å¼€è¾Ÿäº†æ–°å¯èƒ½æ€§ï¼Œè¯æ˜äº†åœ¨æœ‰é™çœŸå®æ•°æ®æƒ…å†µä¸‹åˆ©ç”¨åˆæˆæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚

---

#### ğŸ“„ Abstract
Images can be viewed as layered compositions, foreground objects over background, with potential occlusions. This layered representation enables independent editing of elements, offering greater flexibility for content creation. Despite the progress in large generative models, decomposing a single image into layers remains challenging due to limited methods and data. We observe a strong connection between layer decomposition and in/outpainting tasks, and propose adapting a diffusion-based inpainting model for layer decomposition using lightweight finetuning. To further preserve detail in the latent space, we introduce a novel multi-modal context fusion module with linear attention complexity. Our model is trained purely on a synthetic dataset constructed from open-source assets and achieves superior performance in object removal and occlusion recovery, unlocking new possibilities in downstream editing and creative applications.


### [16] [Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis](https://arxiv.org/abs/2511.21397)
*Jiyun Bae, Hyunjong Ok, Sangwoo Mo, Jaeho Lee*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥Idisæ•°æ®é›†ç³»ç»Ÿç ”ç©¶è§†è§‰å¹²æ‰°ç‰©å¯¹è§†è§‰è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶æ‰©å±•çš„å½±å“ï¼Œå‘ç°è§†è§‰å¹²æ‰°ç‰©ä¸æ–‡æœ¬å¹²æ‰°ç‰©å­˜åœ¨æ ¹æœ¬å·®å¼‚ï¼šè™½ç„¶éƒ½å­˜åœ¨é€†æ‰©å±•æ•ˆåº”ï¼Œä½†è§†è§‰å¹²æ‰°ç‰©ä¼šé™ä½å‡†ç¡®ç‡è€Œä¸å¢åŠ æ¨ç†é•¿åº¦ï¼Œå¹¶æå‡ºåŸºäºå±æ€§è®¡æ•°çš„åˆ†ææ–¹æ³•æ­ç¤ºäº†å¹²æ‰°ç‰©ã€æ¨ç†é•¿åº¦ä¸å‡†ç¡®ç‡ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç ”ç©¶è¡¨æ˜æ–‡æœ¬å¹²æ‰°ç‰©ä¼šå¯¼è‡´è¯­è¨€æ¨¡å‹å‡ºç°é€†æ‰©å±•æ•ˆåº”ï¼Œå³æ›´é•¿çš„æ¨ç†è¿‡ç¨‹åè€Œé™ä½æ•ˆæœï¼Œä½†è§†è§‰å¹²æ‰°ç‰©åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å½±å“æœºåˆ¶å°šä¸æ˜ç¡®ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¤šæ¨¡æ€åœºæ™¯ä¸‹å¹²æ‰°ç‰©å¯¹æ¨¡å‹æµ‹è¯•æ—¶æ‰©å±•çš„å½±å“è§„å¾‹ã€‚

**Method:** æå‡ºIdisè§†è§‰é—®ç­”æ•°æ®é›†ï¼Œç³»ç»Ÿåœ°åœ¨è¯­ä¹‰ã€æ•°å€¼å’Œç©ºé—´ä¸‰ä¸ªç»´åº¦ä¸Šå¼•å…¥è§†è§‰å¹²æ‰°ç‰©ï¼Œé€šè¿‡åˆ†ææ¨ç†è½¨è¿¹ä¸­çš„å±æ€§è®¡æ•°æ¥ç ”ç©¶å¹²æ‰°ç‰©ä¸æ¨ç†è¿‡ç¨‹çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶å¼€å‘äº†ç®€å•çš„æç¤ºç­–ç•¥æ¥ç¼“è§£æ¨ç†æ¨¡å‹ä¸­çš„åè§é©±åŠ¨é¢„æµ‹ã€‚

**Result:** å®éªŒå‘ç°è§†è§‰å¹²æ‰°ç‰©ä¸æ–‡æœ¬å¹²æ‰°ç‰©å­˜åœ¨æœ¬è´¨å·®å¼‚ï¼šè™½ç„¶éƒ½å‡ºç°é€†æ‰©å±•ç°è±¡ï¼Œä½†è§†è§‰å¹²æ‰°ç‰©æ˜¾è‘—é™ä½æ¨¡å‹å‡†ç¡®ç‡è€Œä¸å¢åŠ æ¨ç†é•¿åº¦ï¼Œåœ¨Waterbirdsç­‰è§†è§‰åè§åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè§‚å¯Ÿåˆ°ç±»ä¼¼è¶‹åŠ¿ï¼Œæå‡ºçš„æç¤ºç­–ç•¥æœ‰æ•ˆå‡è½»äº†åè§é©±åŠ¨çš„é¢„æµ‹ã€‚

**Conclusion:** è§†è§‰å¹²æ‰°ç‰©å¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„å½±å“æœºåˆ¶ä¸æ–‡æœ¬å¹²æ‰°ç‰©ä¸åŒï¼Œé€šè¿‡è·Ÿè¸ªæ¨ç†è¿‡ç¨‹ä¸­çš„å±æ€§è®¡æ•°å¯ä»¥æ·±å…¥ç†è§£å¹²æ‰°ç‰©ä½œç”¨æœºåˆ¶ï¼Œè¿™ä¸€å‘ç°ä¸ºç†è§£å’Œæ”¹è¿›å¤šæ¨¡æ€æ¨¡å‹çš„é²æ£’æ€§æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶å±•ç¤ºäº†ç®€å•æç¤ºç­–ç•¥åœ¨ç¼“è§£æ¨¡å‹åè§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

---

#### ğŸ“„ Abstract
How does irrelevant information (i.e., distractors) affect test-time scaling in vision-language models (VLMs)? Prior studies on language models have reported an inverse scaling effect, where textual distractors lead to longer but less effective reasoning. To investigate whether similar phenomena occur in multimodal settings, we introduce Idis (Images with distractors), a visual question-answering dataset that systematically varies distractors along semantic, numerical, and spatial dimensions. Our analyses reveal that visual distractors differ fundamentally from textual ones: although inverse scaling persists, adding visual distractors reduces accuracy without increasing reasoning length. We further show that tracking attribute counts within reasoning traces provides key insights into how distractors, reasoning length, and accuracy interact. Finally, we demonstrate that these trends extend to established visual bias benchmarks such as Waterbirds, and we propose a simple prompting strategy to mitigate bias-driven predictions in reasoning models.


### [17] [Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning](https://arxiv.org/abs/2511.21002)
*Xiaoxing You, Qiang Huang, Lingyu Li, Chi Zhang, Xiaopeng Liu, Min Zhang, Jun Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MERGEï¼Œé¦–ä¸ªç”¨äºæ–°é—»å›¾åƒæè¿°çš„å¤šæ¨¡æ€å®ä½“æ„ŸçŸ¥æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºå®ä½“ä¸­å¿ƒçš„å¤šæ¨¡æ€çŸ¥è¯†åº“å’ŒåŠ¨æ€æ£€ç´¢æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æ–°é—»å›¾åƒæè¿°çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–°é—»å›¾åƒæè¿°æ–¹æ³•é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¿¡æ¯è¦†ç›–ä¸å®Œæ•´ã€è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›å¼±ä»¥åŠè§†è§‰å®ä½“æ¥åœ°æ•ˆæœæ¬ ä½³ï¼Œè¿™äº›é—®é¢˜é™åˆ¶äº†ç”Ÿæˆæè¿°çš„æ–°é—»ä»·å€¼å’Œä¿¡æ¯ä¸°å¯Œåº¦ã€‚

**Method:** MERGEæ¡†æ¶æ„å»ºäº†å®ä½“ä¸­å¿ƒçš„å¤šæ¨¡æ€çŸ¥è¯†åº“ï¼ˆEMKBï¼‰ï¼Œæ•´åˆæ–‡æœ¬ã€è§†è§‰å’Œç»“æ„åŒ–çŸ¥è¯†ï¼›é‡‡ç”¨å¤šé˜¶æ®µå‡è®¾-æè¿°ç­–ç•¥æ”¹è¿›è·¨æ¨¡æ€å¯¹é½ï¼Œå¹¶é€šè¿‡åŸºäºå›¾åƒå†…å®¹çš„åŠ¨æ€æ£€ç´¢æœºåˆ¶å¢å¼ºè§†è§‰å®ä½“åŒ¹é…ã€‚

**Result:** åœ¨GoodNewså’ŒNYTimes800kæ•°æ®é›†ä¸Šï¼ŒMERGEæ˜¾è‘—ä¼˜äºç°æœ‰æœ€ä¼˜æ–¹æ³•ï¼ŒCIDErå¾—åˆ†åˆ†åˆ«æå‡+6.84å’Œ+1.16ï¼Œå‘½åå®ä½“è¯†åˆ«F1åˆ†æ•°åˆ†åˆ«æå‡+4.14å’Œ+2.64ï¼›åœ¨æœªè§è¿‡çš„Visual Newsæ•°æ®é›†ä¸Šè·å¾—+20.17çš„CIDEræå‡å’Œ+6.22çš„F1åˆ†æ•°æå‡ã€‚

**Conclusion:** MERGEå±•ç¤ºäº†å¤šæ¨¡æ€å®ä½“æ„ŸçŸ¥æ£€ç´¢å¢å¼ºç”Ÿæˆåœ¨æ–°é—»å›¾åƒæè¿°ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œé¢†åŸŸé€‚åº”æ€§ä¸ºè·¨æ¨¡æ€å†…å®¹ç”Ÿæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»“åˆä¸°å¯ŒèƒŒæ™¯çŸ¥è¯†çš„åº”ç”¨åœºæ™¯ä¸­ã€‚

---

#### ğŸ“„ Abstract
News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.


### [18] [CaptionQA: Is Your Caption as Useful as the Image Itself?](https://arxiv.org/abs/2511.21025)
*Shijia Yang, Yunong Liu, Bohan Zhai, Ximeng Sun, Zicheng Liu, Emad Barsoum, Manling Li, Chenfeng Xu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CaptionQAåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡è¡¡é‡å›¾åƒæè¿°åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å®ç”¨æ€§æ¥è¯„ä¼°ç”Ÿæˆæè¿°çš„è´¨é‡ï¼Œå¡«è¡¥äº†å½“å‰è¯„ä¼°æ–¹æ³•æ— æ³•éªŒè¯æè¿°èƒ½å¦åœ¨å®é™…ä»»åŠ¡ä¸­æ›¿ä»£å›¾åƒçš„ç©ºç™½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å›¾åƒæè¿°è¯„ä¼°æ–¹æ³•å­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·ï¼Œæ— æ³•éªŒè¯ç”Ÿæˆçš„æè¿°æ˜¯å¦èƒ½åœ¨å®é™…ä¸‹æ¸¸ä»»åŠ¡ä¸­æœ‰æ•ˆæ›¿ä»£åŸå§‹å›¾åƒå†…å®¹ï¼Œè¿™é™åˆ¶äº†å›¾åƒæè¿°åœ¨å¤šæ¨¡æ€ç³»ç»Ÿä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

**Method:** æ„å»ºäº†CaptionQAåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–è‡ªç„¶å›¾åƒã€æ–‡æ¡£ã€ç”µå­å•†åŠ¡å’Œå…·èº«AIå››ä¸ªé¢†åŸŸï¼ŒåŒ…å«25ä¸ªé¡¶çº§ç±»åˆ«å’Œ69ä¸ªå­ç±»åˆ«çš„ç»†ç²’åº¦åˆ†ç±»ä½“ç³»ï¼Œåˆ›å»ºäº†33,027ä¸ªå¯†é›†æ ‡æ³¨çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼Œé€šè¿‡LLMä»…ä½¿ç”¨æè¿°å›ç­”é—®é¢˜çš„æ–¹å¼ç›´æ¥æµ‹é‡æè¿°åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚

**Result:** è¯„ä¼°æ˜¾ç¤ºæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨æè¿°å®ç”¨æ€§ä¸å›¾åƒå®ç”¨æ€§ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œåœ¨ä¼ ç»Ÿå›¾åƒé—®ç­”åŸºå‡†ä¸Šè¡¨ç°ç›¸è¿‘çš„æ¨¡å‹åœ¨æè¿°å®ç”¨æ€§ä¸Šæœ€å¤šä¸‹é™32%ï¼Œæ­ç¤ºäº†å½“å‰å›¾åƒæè¿°ç”ŸæˆæŠ€æœ¯çš„å±€é™æ€§ã€‚

**Conclusion:** å›¾åƒæè¿°çš„è´¨é‡è¯„ä¼°åº”åŸºäºå…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å®é™…æ•ˆç”¨ï¼Œè€Œéä¼ ç»ŸæŒ‡æ ‡ï¼ŒCaptionQAä¸ºå›¾åƒæè¿°ç”ŸæˆæŠ€æœ¯çš„æ”¹è¿›æä¾›äº†å®ç”¨å¯¼å‘çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å¯é€šè¿‡å¼€æºç®¡é“æ‰©å±•åˆ°æ–°é¢†åŸŸã€‚

---

#### ğŸ“„ Abstract
Image captions serve as efficient surrogates for visual content in multimodal systems such as retrieval, recommendation, and multi-step agentic inference pipelines. Yet current evaluation practices miss a fundamental question: Can captions stand-in for images in real downstream tasks? We propose a utility-based benchmark, CaptionQA, to evaluate model-generated captions, where caption quality is measured by how well it supports downstream tasks. CaptionQA is an extensible domain-dependent benchmark covering 4 domains--Natural, Document, E-commerce, and Embodied AI--each with fine-grained taxonomies (25 top-level and 69 subcategories) that identify useful information for domain-specific tasks. CaptionQA builds 33,027 densely annotated multiple-choice questions (50.3 per image on average) that explicitly require visual information to answer, providing a comprehensive probe of caption utility. In our evaluation protocol, an LLM answers these questions using captions alone, directly measuring whether captions preserve image-level utility and are utilizable by a downstream LLM. Evaluating state-of-the-art MLLMs reveals substantial gaps between the image and its caption utility. Notably, models nearly identical on traditional image-QA benchmarks lower by up to 32% in caption utility. We release CaptionQA along with an open-source pipeline for extension to new domains. The code is available at https://github.com/bronyayang/CaptionQA.


### [19] [FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation](https://arxiv.org/abs/2511.21029)
*Kaixing Yang, Xulong Tang, Ziqiao Peng, Xiangyue Zhang, Puwei Wang, Jun He, Hongyan Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºFlowerDanceï¼Œä¸€ç§é«˜æ•ˆçš„éŸ³ä¹åˆ°èˆè¹ˆç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆMeanFlowä¸ç‰©ç†ä¸€è‡´æ€§çº¦æŸï¼Œåœ¨ä»…éœ€å°‘é‡é‡‡æ ·æ­¥éª¤çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è´¨é‡èˆè¹ˆåŠ¨ä½œï¼ŒåŒæ—¶åœ¨æ¨ç†é€Ÿåº¦å’Œå†…å­˜åˆ©ç”¨æ–¹é¢å®ç°æ˜¾è‘—æ•ˆç‡æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰éŸ³ä¹åˆ°èˆè¹ˆç”Ÿæˆæ–¹æ³•å­˜åœ¨ç”Ÿæˆæ•ˆç‡æœ‰é™çš„é—®é¢˜ï¼Œå¯¼è‡´åœ¨é«˜ä¿çœŸ3Dæ¸²æŸ“ä¸­è®¡ç®—ä½™é‡ä¸è¶³ï¼Œä»è€Œé™åˆ¶äº†3Dè§’è‰²åœ¨çœŸå®åº”ç”¨åœºæ™¯ä¸­çš„è¡¨ç°åŠ›ã€‚

**Method:** FlowerDanceç»“åˆMeanFlowä¸ç‰©ç†ä¸€è‡´æ€§çº¦æŸå®ç°é«˜è´¨é‡è¿åŠ¨ç”Ÿæˆï¼Œé‡‡ç”¨åŸºäºBiMambaçš„é«˜æ•ˆæ¨¡å‹æ¶æ„å’Œé€šé“çº§è·¨æ¨¡æ€èåˆï¼Œä»¥éè‡ªå›å½’æ–¹å¼ç”Ÿæˆèˆè¹ˆåŠ¨ä½œï¼ŒåŒæ—¶æ”¯æŒè¿åŠ¨ç¼–è¾‘åŠŸèƒ½ã€‚

**Result:** åœ¨AIST++å’ŒFineDanceæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFlowerDanceåœ¨è¿åŠ¨è´¨é‡å’Œç”Ÿæˆæ•ˆç‡æ–¹é¢å‡è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦å’Œå†…å­˜åˆ©ç”¨ç‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é«˜æ•ˆèˆè¹ˆç”Ÿæˆç³»ç»Ÿçš„å¯è¡Œæ€§ï¼Œä¸ºè™šæ‹Ÿç°å®å’Œæ•°å­—å¨±ä¹åº”ç”¨æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æ”¯æŒäº¤äº’å¼è¿åŠ¨ç¼–è¾‘åŠŸèƒ½å¢å¼ºäº†ç³»ç»Ÿçš„å®ç”¨æ€§ã€‚

---

#### ğŸ“„ Abstract
Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization . Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance.


### [20] [LungNoduleAgent: A Collaborative Multi-Agent System for Precision Diagnosis of Lung Nodules](https://arxiv.org/abs/2511.21042)
*Cheng Yang, Hui Jin, Xinlei Yu, Zhipeng Wang, Yaoqun Liu, Fenglei Fan, Dajiang Lei, Gangyong Jia, Changmiao Wang, Ruiquan Ge*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†LungNoduleAgentï¼Œä¸€ä¸ªç”¨äºåˆ†æè‚ºéƒ¨CTæ‰«æçš„åä½œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡æ¨¡å—åŒ–è¯Šæ–­æµç¨‹æ˜¾è‘—æå‡äº†è‚ºç»“èŠ‚å½¢æ€æè¿°å’Œæ¶æ€§ç¨‹åº¦åˆ†çº§çš„å‡†ç¡®æ€§ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰ä¸»æµæ¨¡å‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è‚ºéƒ¨CTåˆ†æåœ¨å‡†ç¡®æè¿°ç»“èŠ‚å½¢æ€å’Œèå…¥åŒ»å­¦ä¸“ä¸šçŸ¥è¯†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå½±å“äº†ä¸´åºŠåº”ç”¨çš„å¯é æ€§ï¼Œè€Œåä½œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨ç—…ç†å­¦é¢†åŸŸçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚

**Method:** LungNoduleAgenté‡‡ç”¨ä¸‰æ¨¡å—åä½œæ¶æ„ï¼šç»“èŠ‚å®šä½å™¨åè°ƒä¸´åºŠæ£€æµ‹æ¨¡å‹è¯†åˆ«ç»“èŠ‚ï¼Œæ”¾å°„ç§‘åŒ»ç”Ÿæ•´åˆå±€éƒ¨å›¾åƒæè¿°æŠ€æœ¯ç”Ÿæˆå…¨é¢CTæŠ¥å‘Šï¼ŒåŒ»ç”Ÿæ™ºèƒ½ä½“ç³»ç»ŸåŸºäºå›¾åƒå’ŒCTæŠ¥å‘Šè¿›è¡Œæ¶æ€§æ¨ç†ï¼Œå¹¶è¾…ä»¥ç—…ç†çŸ¥è¯†åº“å’Œå¤šæ™ºèƒ½ä½“æ¡†æ¶æ”¯æŒã€‚

**Result:** åœ¨ä¸¤ä¸ªç§æœ‰æ•°æ®é›†å’Œå…¬å¼€LIDC-IDRIæ•°æ®é›†ä¸Šçš„å¹¿æ³›æµ‹è¯•è¡¨æ˜ï¼ŒLungNoduleAgentè¶…è¶Šäº†ä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹ã€æ™ºèƒ½ä½“ç³»ç»Ÿå’Œå…ˆè¿›ä¸“å®¶æ¨¡å‹ï¼Œè¯æ˜äº†åŒºåŸŸçº§è¯­ä¹‰å¯¹é½å’Œå¤šæ™ºèƒ½ä½“åä½œåœ¨ç»“èŠ‚è¯Šæ–­ä¸­çš„é‡è¦æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“åä½œåœ¨åŒ»å­¦å½±åƒåˆ†æä¸­çš„æœ‰æ•ˆæ€§ï¼ŒLungNoduleAgentä½œä¸ºæ”¯æŒè‚ºç»“èŠ‚ä¸´åºŠåˆ†æçš„æœ‰å‰æ™¯çš„åŸºç¡€å·¥å…·ï¼Œå±•ç¤ºäº†åœ¨å¹³è¡¡é€šç”¨æ€§å’Œç²¾ç¡®æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚

---

#### ğŸ“„ Abstract
Diagnosing lung cancer typically involves physicians identifying lung nodules in Computed tomography (CT) scans and generating diagnostic reports based on their morphological features and medical expertise. Although advancements have been made in using multimodal large language models for analyzing lung CT scans, challenges remain in accurately describing nodule morphology and incorporating medical expertise. These limitations affect the reliability and effectiveness of these models in clinical settings. Collaborative multi-agent systems offer a promising strategy for achieving a balance between generality and precision in medical applications, yet their potential in pathology has not been thoroughly explored. To bridge these gaps, we introduce LungNoduleAgent, an innovative collaborative multi-agent system specifically designed for analyzing lung CT scans. LungNoduleAgent streamlines the diagnostic process into sequential components, improving precision in describing nodules and grading malignancy through three primary modules. The first module, the Nodule Spotter, coordinates clinical detection models to accurately identify nodules. The second module, the Radiologist, integrates localized image description techniques to produce comprehensive CT reports. Finally, the Doctor Agent System performs malignancy reasoning by using images and CT reports, supported by a pathology knowledge base and a multi-agent system framework. Extensive testing on two private datasets and the public LIDC-IDRI dataset indicates that LungNoduleAgent surpasses mainstream vision-language models, agent systems, and advanced expert models. These results highlight the importance of region-level semantic alignment and multi-agent collaboration in diagnosing nodules. LungNoduleAgent stands out as a promising foundational tool for supporting clinical analyses of lung nodules.


### [21] [MIRA: Multimodal Iterative Reasoning Agent for Image Editing](https://arxiv.org/abs/2511.21087)
*Ziyun Zeng, Hang Hua, Jiebo Luo*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MIRAï¼ˆå¤šæ¨¡æ€è¿­ä»£æ¨ç†ä»£ç†ï¼‰ï¼Œä¸€ç§è½»é‡çº§å³æ’å³ç”¨çš„å¤šæ¨¡æ€æ¨ç†ä»£ç†ï¼Œé€šè¿‡è¿­ä»£æ„ŸçŸ¥-æ¨ç†-è¡ŒåŠ¨å¾ªç¯æ¥æ‰§è¡Œå›¾åƒç¼–è¾‘ï¼Œæ˜¾è‘—æå‡äº†æ‰©æ•£æ¨¡å‹å¯¹å¤æ‚ç”¨æˆ·æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ‰©æ•£åŸºç¼–è¾‘æ¨¡å‹åœ¨å¤„ç†å¤æ‚ç”¨æˆ·æŒ‡ä»¤æ—¶å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠç»„åˆå…³ç³»ã€ä¸Šä¸‹æ–‡çº¿ç´¢æˆ–æŒ‡ä»£è¡¨è¾¾çš„æŒ‡ä»¤ï¼Œå¯¼è‡´ç¼–è¾‘ç»“æœè¯­ä¹‰æ¼‚ç§»æˆ–æ— æ³•åæ˜ é¢„æœŸæ›´æ”¹ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å‡†ç¡®è§£é‡ŠåŒ…å«å¤šå±‚æ¬¡è¯­ä¹‰çš„å¤æ‚ç¼–è¾‘éœ€æ±‚ã€‚

**Method:** æå‡ºäº†MIRAå¤šæ¨¡æ€æ¨ç†ä»£ç†ï¼Œé‡‡ç”¨è¿­ä»£æ„ŸçŸ¥-æ¨ç†-è¡ŒåŠ¨å¾ªç¯æ¨¡æ‹Ÿäººæœºå¤šè½®äº¤äº’è¿‡ç¨‹ï¼Œé€æ­¥é¢„æµ‹åŸå­ç¼–è¾‘æŒ‡ä»¤å¹¶åˆ©ç”¨è§†è§‰åé¦ˆè¿›è¡Œå†³ç­–ã€‚é€šè¿‡æ„å»º150Kå¤šæ¨¡æ€å·¥å…·ä½¿ç”¨æ•°æ®é›†MIRA-Editingï¼Œç»“åˆä¸¤é˜¶æ®µSFT + GRPOè®­ç»ƒæµç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¯¹å¤æ‚ç¼–è¾‘æŒ‡ä»¤è¿›è¡Œæ¨ç†å’Œç¼–è¾‘ã€‚

**Result:** å½“ä¸å¼€æºå›¾åƒç¼–è¾‘æ¨¡å‹ï¼ˆå¦‚Flux.1-Kontextã€Step1X-Editå’ŒQwen-Image-Editï¼‰é…åˆä½¿ç”¨æ—¶ï¼ŒMIRAæ˜¾è‘—æå‡äº†è¯­ä¹‰ä¸€è‡´æ€§å’Œæ„ŸçŸ¥è´¨é‡ï¼Œåœ¨æ€§èƒ½ä¸Šè¾¾åˆ°ç”šè‡³è¶…è¿‡äº†GPT-Imageå’ŒNano-Bananaç­‰ä¸“æœ‰ç³»ç»Ÿã€‚

**Conclusion:** MIRAå±•ç¤ºäº†è¿­ä»£æ¨ç†æœºåˆ¶åœ¨å¤æ‚å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›äº†æ›´å‡†ç¡®çš„å¤šæ¨¡æ€æŒ‡ä»¤ç†è§£èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ºå¼€æ”¾åŸŸå›¾åƒç¼–è¾‘æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†äººæœºäº¤äº’å¼ç¼–è¾‘çš„å‘å±•æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.


### [22] [Scaling Foundation Models for Radar Scene Understanding](https://arxiv.org/abs/2511.21105)
*Pushkal Mishra, Kshitiz Bansal, Dinesh Bharadia*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†RadarFMï¼Œä¸€ä¸ªé€šè¿‡ç»“æ„åŒ–ç©ºé—´è¯­è¨€ç›‘ç£å­¦ä¹ ç»Ÿä¸€åœºæ™¯çº§è¡¨ç¤ºçš„é›·è¾¾åŸºç¡€æ¨¡å‹ï¼Œè§£å†³äº†ç°æœ‰é›·è¾¾æ–¹æ³•ä»»åŠ¡ç‰¹å®šä¸”æ— æ³•è·¨ä»»åŠ¡è¿ç§»çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ç»“æ„åŒ–æ ‡é¢˜æ¡†æ¶å’Œå“ˆå¸Œæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ç›®æ ‡ï¼Œå®ç°äº†ç»†ç²’åº¦çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰é›·è¾¾æ„ŸçŸ¥æ–¹æ³•å­˜åœ¨ä»»åŠ¡ç‰¹å®šæ€§å’Œæ¶æ„ç¢ç‰‡åŒ–é—®é¢˜ï¼Œæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ä¸åŒçš„æ¶æ„å’Œè®­ç»ƒç›®æ ‡ï¼Œå¯¼è‡´æ— æ³•å®ç°è·¨ä»»åŠ¡çš„çŸ¥è¯†è¿ç§»ã€‚å°½ç®¡åŸºç¡€æ¨¡å‹åœ¨è§†è§‰å’Œè¯­è¨€ç†è§£é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶ä¸é›·è¾¾æ„ŸçŸ¥çš„æ•´åˆä»å¤„äºæ¢ç´¢ä¸è¶³çš„çŠ¶æ€ã€‚

**Method:** æå‡ºäº†ä¸¤ä¸ªå…³é”®è´¡çŒ®ï¼šç»“æ„åŒ–æ ‡é¢˜æ¡†æ¶åœ¨åŸç”Ÿé›·è¾¾åæ ‡ä¸­ç¼–ç è½¦è¾†åˆ†å¸ƒï¼Œä»¥åŠå“ˆå¸Œæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ç›®æ ‡é‡åŒ–è¿ç»­åœºæ™¯ç›¸ä¼¼æ€§è€ŒéäºŒå…ƒåŒ¹é…ã€‚åˆ©ç”¨CARLAæ¨¡æ‹Ÿå™¨ç”Ÿæˆå¤§è§„æ¨¡ã€è‰¯å¥½æ ‡æ³¨çš„é›·è¾¾æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†è¶…è¶Šä¼ ç»Ÿæ£€æµ‹æŒ‡æ ‡çš„å®šä½æ„ŸçŸ¥è¯„ä¼°æŒ‡æ ‡ã€‚

**Result:** é€šè¿‡å¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®é›†éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºçš„å®šä½æ„ŸçŸ¥æŒ‡æ ‡èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°ç©ºé—´ç²¾åº¦ã€‚ç»“æ„åŒ–è¯­è¨€ç›‘ç£å’Œå¯¹æ¯”å­¦ä¹ ç›®æ ‡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ç»Ÿä¸€çš„åœºæ™¯çº§è¡¨ç¤ºï¼Œæ”¯æŒè·¨ä»»åŠ¡çš„è¿ç§»å­¦ä¹ ã€‚

**Conclusion:** RadarFMå±•ç¤ºäº†åŸºç¡€æ¨¡å‹èŒƒå¼åœ¨é›·è¾¾æ„ŸçŸ¥é¢†åŸŸçš„æ½œåŠ›ï¼Œä¸ºå¤šä»»åŠ¡é›·è¾¾ç†è§£æä¾›äº†ç»Ÿä¸€æ¡†æ¶ã€‚ç»“æ„åŒ–ç©ºé—´è¯­è¨€ç›‘ç£å’Œè¿ç»­ç›¸ä¼¼æ€§åº¦é‡æ–¹æ³•ä¸ºé›·è¾¾åŸºç¡€æ¨¡å‹çš„å‘å±•å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œæœ‰æœ›æ¨åŠ¨è‡ªåŠ¨é©¾é©¶åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„å¯é æ„ŸçŸ¥ã€‚

---

#### ğŸ“„ Abstract
Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.


### [23] [EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens](https://arxiv.org/abs/2511.21106)
*Ze Feng, Sen Yang, Boqiang Duan, Wankou Yang, Jingdong Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºEM-KDï¼Œä¸€ç§å¢å¼ºé«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†è’¸é¦æ–°èŒƒå¼ï¼Œé€šè¿‡è§£å†³å¸ˆç”Ÿæ¨¡å‹é—´è§†è§‰ä»¤ç‰Œä¸å¹³è¡¡é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰é«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€šè¿‡å‹ç¼©è§†è§‰ä»¤ç‰Œæ¥å‡å°‘èµ„æºæ¶ˆè€—ï¼Œä½†è§†è§‰ä¿¡æ¯ä¸¢å¤±ä¼šé™ä½ç†è§£èƒ½åŠ›ã€‚è™½ç„¶å…ˆå‰ç ”ç©¶å¼•å…¥çŸ¥è¯†è’¸é¦æ¥å¢å¼ºå­¦ç”Ÿæ¨¡å‹ï¼Œä½†å¿½ç•¥äº†é«˜æ•ˆå­¦ç”Ÿæ¨¡å‹ä¸åŸå§‹æ•™å¸ˆæ¨¡å‹ä¹‹é—´ç”±äºè§†è§‰ä»¤ç‰Œä¸å¹³è¡¡å¯¼è‡´çš„ç»†ç²’åº¦è§†è§‰ç†è§£å·®å¼‚ã€‚

**Method:** EM-KDé¦–å…ˆä½¿ç”¨æ›¼å“ˆé¡¿è·ç¦»è®¡ç®—å¸ˆç”Ÿè§†è§‰logitsä¹‹é—´çš„è·ç¦»ï¼Œå¹¶é€šè¿‡åŒˆç‰™åˆ©åŒ¹é…ç®—æ³•åœ¨ç©ºé—´ç»´åº¦ä¸Šè¿›è¡Œå¯¹é½ã€‚éšåå¼•å…¥ä¸¤ç§è’¸é¦ç­–ç•¥ï¼šè§†è§‰è¯­è¨€äº²å’ŒåŠ›è’¸é¦é€šè¿‡è®¡ç®—æ–‡æœ¬ä»¤ç‰Œä¸å¯¹é½è§†è§‰ä»¤ç‰Œä¹‹é—´çš„äº²å’ŒçŸ©é˜µï¼Œå¹¶æœ€å°åŒ–å¸ˆç”Ÿäº²å’ŒçŸ©é˜µçš„å¹³æ»‘L1è·ç¦»ï¼›è§†è§‰è¯­ä¹‰è’¸é¦åˆ©ç”¨åå‘KLæ•£åº¦åº¦é‡å¯¹é½è§†è§‰logitsåœ¨è¯æ±‡ç©ºé—´ä¸Šçš„ç¦»æ•£æ¦‚ç‡åˆ†å¸ƒå·®å¼‚ã€‚

**Result:** åœ¨å¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒEM-KDè®­ç»ƒæ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºå…ˆå‰çš„é«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚ä¸é…å¤‡ç›¸åŒè§†è§‰ä»¤ç‰ŒåŒ¹é…ç­–ç•¥çš„å…ˆå‰è’¸é¦æ–¹æ³•ç›¸æ¯”ï¼ŒEM-KDä¹Ÿå®ç°äº†æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è§£å†³è§†è§‰ä»¤ç‰Œä¸å¹³è¡¡é—®é¢˜ï¼ŒçŸ¥è¯†è’¸é¦å¯ä»¥æœ‰æ•ˆå¢å¼ºé«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚EM-KDä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„é«˜æ•ˆå‹ç¼©æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œåœ¨ä¿æŒæ¨¡å‹è½»é‡åŒ–çš„åŒæ—¶æ˜¾è‘—æå‡äº†è§†è§‰ç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.


### [24] [Which Layer Causes Distribution Deviation? Entropy-Guided Adaptive Pruning for Diffusion and Flow Models](https://arxiv.org/abs/2511.21122)
*Changlin Li, Jiawei Zhang, Zeyi Shi, Zongxin Yang, Zhihui Li, Xiaojun Chang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºEntPrunerï¼Œä¸€ç§åŸºäºç†µå¼•å¯¼çš„è‡ªåŠ¨æ¸è¿›å¼å‰ªææ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹æ‰©æ•£æ¨¡å‹å’Œæµæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å®ç°é«˜è¾¾2.22å€çš„æ¨ç†åŠ é€Ÿã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§è§„æ¨¡è§†è§‰ç”Ÿæˆæ¨¡å‹ï¼ˆåŒ…æ‹¬æ‰©æ•£æ¨¡å‹å’Œæµæ¨¡å‹ï¼‰åœ¨è¿ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶å­˜åœ¨æ˜¾è‘—çš„å‚æ•°å†—ä½™é—®é¢˜ï¼Œä¼ ç»Ÿå‰ªææ–¹æ³•éš¾ä»¥åœ¨ä¿æŒç”Ÿæˆå¤šæ ·æ€§å’Œæ¡ä»¶ä¿çœŸåº¦çš„åŒæ—¶æœ‰æ•ˆå‹ç¼©æ¨¡å‹ã€‚

**Method:** æå‡ºç†µå¼•å¯¼å‰ªæç­–ç•¥ï¼Œä½¿ç”¨æ•°æ®ä¾èµ–çš„æ¡ä»¶ç†µåå·®ï¼ˆCEDï¼‰ä½œä¸ºå—çº§é‡è¦æ€§è¯„ä¼°æŒ‡æ ‡ï¼›å¼€å‘é›¶æ ·æœ¬è‡ªé€‚åº”å‰ªææ¡†æ¶ï¼ŒåŠ¨æ€ç¡®å®šå‰ªææ—¶æœºå’Œç¨‹åº¦ï¼Œé¿å…ä¸€æ¬¡æ€§å‰ªæå¯¼è‡´çš„æ¨¡å¼å´©æºƒé—®é¢˜ã€‚

**Result:** åœ¨DiTå’ŒSiTæ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒEntPruneråœ¨ImageNetå’Œä¸‰ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šä¿æŒç«äº‰åŠ›çš„ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾2.22å€çš„æ¨ç†åŠ é€Ÿã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç”Ÿæˆæ¨¡å‹å‰ªæéœ€è¦ä¸“é—¨çš„é‡è¦æ€§è¯„ä¼°ç­–ç•¥ï¼Œç†µå¼•å¯¼æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡æ¨¡å‹å‹ç¼©ä¸ç”Ÿæˆè´¨é‡ï¼Œä¸ºé›¶æ ·æœ¬è‡ªé€‚åº”æ¨¡å‹å‹ç¼©æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Large-scale vision generative models, including diffusion and flow models, have demonstrated remarkable performance in visual generation tasks. However, transferring these pre-trained models to downstream tasks often results in significant parameter redundancy. In this paper, we propose EntPruner, an entropy-guided automatic progressive pruning framework for diffusion and flow models. First, we introduce entropy-guided pruning, a block-level importance assessment strategy specifically designed for generative models. Unlike discriminative models, generative models require preserving the diversity and condition-fidelity of the output distribution. As the importance of each module can vary significantly across downstream tasks, EntPruner prioritizes pruning of less important blocks using data-dependent Conditional Entropy Deviation (CED) as a guiding metric. CED quantifies how much the distribution diverges from the learned conditional data distribution after removing a block. Second, we propose a zero-shot adaptive pruning framework to automatically determine when and how much to prune during training. This dynamic strategy avoids the pitfalls of one-shot pruning, mitigating mode collapse, and preserving model performance. Extensive experiments on DiT and SiT models demonstrate the effectiveness of EntPruner, achieving up to 2.22$\times$ inference speedup while maintaining competitive generation quality on ImageNet and three downstream datasets.


### [25] [CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion](https://arxiv.org/abs/2511.21129)
*Dianbing Xi, Jiepeng Wang, Yuanzhi Liang, Xi Qiu, Jialun Liu, Hao Pan, Yuchi Huo, Rui Wang, Haibin Huang, Chi Zhang, Xuelong Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CtrlVDiffï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ··åˆæ¨¡æ€æ§åˆ¶ç­–ç•¥èåˆæ·±åº¦ã€æ³•çº¿ã€åˆ†å‰²ã€è¾¹ç¼˜å’Œå›¾å½¢å­¦æœ¬å¾å±æ€§ç­‰å¤šç§æ¨¡æ€ï¼Œè§£å†³äº†è§†é¢‘ç†è§£å’Œå¯æ§ç”Ÿæˆçš„åŒé‡æŒ‘æˆ˜ï¼Œåœ¨ä¿æŒæ—¶é—´ä¸€è‡´æ€§çš„åŒæ—¶å®ç°äº†ç²¾ç¡®çš„å±‚å¼ç¼–è¾‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºå‡ ä½•çº¿ç´¢çš„è§†é¢‘ç†è§£ä¸ç”Ÿæˆæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œä»…èƒ½æŒ‡å®šå¸ƒå±€ä½†æ— æ³•å……åˆ†çº¦æŸå¤–è§‚ã€æè´¨å’Œå…‰ç…§ï¼Œå¯¼è‡´ç‰©ç†ä¸Šæœ‰æ„ä¹‰çš„ç¼–è¾‘å¦‚é‡å…‰ç…§æˆ–æè´¨æ›¿æ¢éš¾ä»¥å®ç°ï¼Œå¹¶ç»å¸¸å¼•èµ·æ—¶é—´æ¼‚ç§»é—®é¢˜ã€‚

**Method:** æå‡ºäº†CtrlVDiffæ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆæ¨¡æ€æ§åˆ¶ç­–ç•¥æ¥è·¯ç”±å’Œèåˆæ·±åº¦ã€æ³•çº¿ã€åˆ†å‰²ã€è¾¹ç¼˜ä»¥åŠå›¾å½¢å­¦æœ¬å¾å±æ€§ç­‰å¤šç§æ¨¡æ€ç‰¹å¾ï¼Œå¹¶æ„å»ºäº†MMVideoæ··åˆçœŸå®ä¸åˆæˆæ•°æ®é›†æ¥æä¾›è·¨æ¨¡æ€å¯¹é½çš„ç›‘ç£ä¿¡å·ã€‚

**Result:** åœ¨ç†è§£å’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCtrlVDiffå±•ç°å‡ºå“è¶Šçš„å¯æ§æ€§å’Œä¿çœŸåº¦ï¼Œèƒ½å¤Ÿå®ç°å±‚å¼ç¼–è¾‘å¦‚é‡å…‰ç…§ã€æè´¨è°ƒæ•´å’Œç‰©ä½“æ’å…¥ï¼Œå¹¶åœ¨éƒ¨åˆ†æ¨¡æ€ç¼ºå¤±æ—¶ä¿æŒé²æ£’æ€§ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›åŸºçº¿æ–¹æ³•ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œä¸°å¯Œå›¾å½¢å­¦æ¨¡æ€çš„å¼•å…¥ä¸ºè§†é¢‘ç†è§£æä¾›äº†äº’è¡¥çº¦æŸï¼Œæ—¢æ¶ˆé™¤æ­§ä¹‰åˆå®ç°äº†ç²¾ç¡®å¯æ§çš„ç”Ÿæˆï¼Œä¸ºç»Ÿä¸€è§†é¢‘ç†è§£ä¸ç”Ÿæˆæ¡†æ¶çš„å‘å±•æä¾›äº†é‡è¦è§è§£å’Œæ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
We tackle the dual challenges of video understanding and controllable video generation within a unified diffusion framework. Our key insights are two-fold: geometry-only cues (e.g., depth, edges) are insufficient: they specify layout but under-constrain appearance, materials, and illumination, limiting physically meaningful edits such as relighting or material swaps and often causing temporal drift. Enriching the model with additional graphics-based modalities (intrinsics and semantics) provides complementary constraints that both disambiguate understanding and enable precise, predictable control during generation.
  However, building a single model that uses many heterogeneous cues introduces two core difficulties. Architecturally, the model must accept any subset of modalities, remain robust to missing inputs, and inject control signals without sacrificing temporal consistency. Data-wise, training demands large-scale, temporally aligned supervision that ties real videos to per-pixel multimodal annotations.
  We then propose CtrlVDiff, a unified diffusion model trained with a Hybrid Modality Control Strategy (HMCS) that routes and fuses features from depth, normals, segmentation, edges, and graphics-based intrinsics (albedo, roughness, metallic), and re-renders videos from any chosen subset with strong temporal coherence. To enable this, we build MMVideo, a hybrid real-and-synthetic dataset aligned across modalities and captions. Across understanding and generation benchmarks, CtrlVDiff delivers superior controllability and fidelity, enabling layer-wise edits (relighting, material adjustment, object insertion) and surpassing state-of-the-art baselines while remaining robust when some modalities are unavailable.


### [26] [Referring Video Object Segmentation with Cross-Modality Proxy Queries](https://arxiv.org/abs/2511.21139)
*Baoli Sun, Xinzhu Ma, Ning Wang, Zhihui Wang, Zhiyong Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºProxyFormerï¼Œä¸€ç§æ–°é¢–çš„æŒ‡ä»£è§†é¢‘ç›®æ ‡åˆ†å‰²æ¶æ„ï¼Œé€šè¿‡å¼•å…¥ä»£ç†æŸ¥è¯¢æœºåˆ¶æ¥æ•´åˆè§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰ï¼Œè§£å†³ç°æœ‰æ–¹æ³•ä¸­è·¨æ¨¡æ€å¯¹é½ä¸è¶³å’Œæ–‡æœ¬çº¦æŸå»¶è¿Ÿé›†æˆçš„é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºæ¡ä»¶æŸ¥è¯¢çš„æŒ‡ä»£è§†é¢‘ç›®æ ‡åˆ†å‰²æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šæ¡ä»¶æŸ¥è¯¢ç¼ºä¹å¸§é—´ä¾èµ–æ€§å’Œå˜åŒ–å»ºæ¨¡ï¼Œå¯¼è‡´åœ¨å¸§é—´æ˜¾è‘—å˜åŒ–æ—¶éš¾ä»¥å‡†ç¡®è·Ÿè¸ªç›®æ ‡ï¼›æ–‡æœ¬çº¦æŸå»¶è¿Ÿé›†æˆå¯èƒ½ä½¿è§†é¢‘ç‰¹å¾å…³æ³¨éæŒ‡ä»£å¯¹è±¡ï¼Œå½±å“åˆ†å‰²ç²¾åº¦ã€‚

**Method:** æå‡ºProxyFormeræ¶æ„ï¼Œå¼•å…¥ä¸€ç»„ä»£ç†æŸ¥è¯¢æ¥æ•´åˆè§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰å¹¶ä¿ƒè¿›è¯­ä¹‰æµåŠ¨ï¼Œé€šè¿‡å¤šé˜¶æ®µè§†é¢‘ç‰¹å¾ç¼–ç å™¨é€æ­¥æ›´æ–°å’Œä¼ æ’­ä»£ç†æŸ¥è¯¢ï¼Œç¡®ä¿è§†é¢‘ç‰¹å¾èšç„¦äºæ„Ÿå…´è¶£å¯¹è±¡ï¼›é‡‡ç”¨æ—¶ç©ºç»´åº¦è§£è€¦çš„è·¨æ¨¡æ€äº¤äº’æœºåˆ¶é™ä½è®¡ç®—æˆæœ¬ï¼Œå¹¶è®¾è®¡è”åˆè¯­ä¹‰ä¸€è‡´æ€§è®­ç»ƒç­–ç•¥å¯¹é½ä»£ç†æŸ¥è¯¢ä¸è§†é¢‘-æ–‡æœ¬å¯¹çš„è¯­ä¹‰å…±è¯†ã€‚

**Result:** åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„æŒ‡ä»£è§†é¢‘ç›®æ ‡åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å…¨é¢å®éªŒè¡¨æ˜ï¼ŒProxyFormeråœ¨æ€§èƒ½ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

**Conclusion:** ProxyFormeré€šè¿‡åŠ¨æ€æ¼”åŒ–çš„ä»£ç†æŸ¥è¯¢æœºåˆ¶å»ºç«‹äº†å¸§é—´ä¾èµ–å…³ç³»ï¼Œå¢å¼ºäº†ç›®æ ‡è·Ÿè¸ªçš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ï¼›è¯¥ç ”ç©¶ä¸ºè·¨æ¨¡æ€è§†é¢‘ç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„è¯­ä¹‰å¯¹é½èŒƒå¼ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºå’Œåº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Referring video object segmentation (RVOS) is an emerging cross-modality task that aims to generate pixel-level maps of the target objects referred by given textual expressions. The main concept involves learning an accurate alignment of visual elements and language expressions within a semantic space. Recent approaches address cross-modality alignment through conditional queries, tracking the target object using a query-response based mechanism built upon transformer structure. However, they exhibit two limitations: (1) these conditional queries lack inter-frame dependency and variation modeling, making accurate target tracking challenging amid significant frame-to-frame variations; and (2) they integrate textual constraints belatedly, which may cause the video features potentially focus on the non-referred objects. Therefore, we propose a novel RVOS architecture called ProxyFormer, which introduces a set of proxy queries to integrate visual and text semantics and facilitate the flow of semantics between them. By progressively updating and propagating proxy queries across multiple stages of video feature encoder, ProxyFormer ensures that the video features are focused on the object of interest. This dynamic evolution also enables the establishment of inter-frame dependencies, enhancing the accuracy and coherence of object tracking. To mitigate high computational costs, we decouple cross-modality interactions into temporal and spatial dimensions. Additionally, we design a Joint Semantic Consistency (JSC) training strategy to align semantic consensus between the proxy queries and the combined video-text pairs. Comprehensive experiments on four widely used RVOS benchmarks demonstrate the superiority of our ProxyFormer to the state-of-the-art methods.


### [27] [LLaVA-UHD v3: Progressive Visual Compression for Efficient Native-Resolution Encoding in MLLMs](https://arxiv.org/abs/2511.21150)
*Shichu Sun, Yichen Zhang, Haolin Song, Zonghao Guo, Chi Chen, Yidan Zhang, Yuan Yao, Zhiyuan Liu, Maosong Sun*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†LLaVA-UHD v3å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå…¶æ ¸å¿ƒæ˜¯æ¸è¿›å¼è§†è§‰å‹ç¼©æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚è¯¥æ–¹æ³•å°†é¢„è®­ç»ƒçš„ViTé‡æ„ä¸ºViT-UHDï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ä¸MoonViTç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å°†é¦–ä»¤ç‰Œç”Ÿæˆæ—¶é—´å‡å°‘2.4å€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œå…¨å±€åŸç”Ÿåˆ†è¾¨ç‡è§†è§‰ç¼–ç ç›¸æ¯”åˆ‡ç‰‡æ–¹æ³•è¶Šæ¥è¶Šå—é’çï¼Œä½†å…¨å±€ç¼–ç è™½ç„¶å¢å¼ºäº†æ•´ä½“èƒ½åŠ›ï¼Œå´å¸¦æ¥äº†æ›´å¤§çš„è®¡ç®—å¼€é”€ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€æ•ˆç‡é—®é¢˜ï¼Œåœ¨ä¿æŒè§†è§‰ç¼–ç æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚

**Method:** æå‡ºäº†æ¸è¿›å¼è§†è§‰å‹ç¼©æ–¹æ³•ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šç²¾ç»†åŒ–è¡¥ä¸åµŒå…¥æ”¯æŒçµæ´»è¡¥ä¸å¤§å°ç¼©æ”¾ä»¥å®ç°ç»†ç²’åº¦è§†è§‰å»ºæ¨¡ï¼Œä»¥åŠçª—å£åŒ–ä»¤ç‰Œå‹ç¼©åœ¨ViTå±‚é—´åˆ†å±‚éƒ¨ç½²ä»¥é€æ­¥èšåˆå±€éƒ¨ä»¤ç‰Œè¡¨ç¤ºã€‚è¯¥æ–¹æ³•å¯æ— ç¼é›†æˆåˆ°æ ‡å‡†ViTä¸­ï¼Œå®ç°é«˜æ•ˆçš„åŸç”Ÿåˆ†è¾¨ç‡ç¼–ç ã€‚

**Result:** è½¬æ¢åçš„ViT-UHDåœ¨å¹¿æ³›åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸MoonViTç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å°†é¦–ä»¤ç‰Œç”Ÿæˆæ—¶é—´å‡å°‘2.4å€ã€‚åŸºäºViT-UHDæ„å»ºçš„LLaVA-UHD v3è¾¾åˆ°ä¸Qwen2-VLç«äº‰çš„æ€§èƒ½ï¼ŒåŒæ—¶è¿›ä¸€æ­¥å°†é¦–ä»¤ç‰Œç”Ÿæˆæ—¶é—´å‡å°‘1.9å€ã€‚

**Conclusion:** æ¸è¿›å¼è§†è§‰å‹ç¼©æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä¸æ•ˆç‡ï¼Œé€šè¿‡é‡æ„é¢„è®­ç»ƒViTå¯åœ¨ä¿æŒé€šç”¨æ€§çš„åŒæ—¶æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ã€‚è¯¥ç ”ç©¶ä¸ºé«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æœªæ¥å‘å±•æä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„å’ŒåŸºå‡†ã€‚

---

#### ğŸ“„ Abstract
Visual encoding followed by token condensing has become the standard architectural paradigm in multi-modal large language models (MLLMs). Many recent MLLMs increasingly favor global native- resolution visual encoding over slice-based methods. To investigate this trend, we systematically compare their behavior on vision-language understanding and attention patterns, revealing that global encoding enhances overall capability but at the expense of greater computational overhead. To address this issue, we present LLaVA-UHD v3, an MLLM centered upon our proposed Progressive Visual Compression (PVC) method, which can be seamlessly integrated into standard Vision Transformer (ViT) to enable efficient native-resolution encoding. The PVC approach consists of two key modules: (i) refined patch embedding, which supports flexible patch-size scaling for fine-grained visual model- ing, (ii) windowed token compression, hierarchically deployed across ViT layers to progressively aggregate local token representations. Jointly modulated by these two modules, a widely pretrained ViT can be reconfigured into an efficient architecture while largely preserving generality. Evaluated across extensive benchmarks, the transformed ViT, termed ViT-UHD, demonstrates competitive performance with MoonViT while reducing TTFT (time-to-first-token) by 2.4x, when developed within an identical MLLM architecture. Building upon ViT-UHD, LLaVA-UHD v3 also achieves competitive performance to Qwen2-VL, while further reducing TTFT by 1.9x. We will release all code and checkpoints to support future research on efficient MLLMs.


### [28] [Progress by Pieces: Test-Time Scaling for Autoregressive Image Generation](https://arxiv.org/abs/2511.21185)
*Joonhyung Park, Hyeongwon Jang, Joowon Kim, Eunho Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†GridARï¼Œä¸€ç§ä¸“ä¸ºè§†è§‰è‡ªå›å½’æ¨¡å‹è®¾è®¡çš„æµ‹è¯•æ—¶æ‰©å±•æ¡†æ¶ï¼Œé€šè¿‡ç½‘æ ¼åˆ†åŒºæ¸è¿›ç”Ÿæˆå’Œå¸ƒå±€æŒ‡å®šæç¤ºé‡æ„ç­–ç•¥ï¼Œåœ¨æœ‰é™è®¡ç®—é¢„ç®—ä¸‹æ˜¾è‘—æå‡ç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨T2I-CompBench++ä¸Šä»¥N=4è¶…è¶ŠBest-of-Nï¼ˆN=8ï¼‰14.4%ï¼ŒåŒæ—¶é™ä½æˆæœ¬25.6%ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰è‡ªå›å½’æ¨¡å‹åœ¨æµ‹è¯•æ—¶æ‰©å±•æ–¹é¢å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä¼ ç»ŸBest-of-Nç­–ç•¥åœ¨é”™è¯¯ç”Ÿæˆè½¨è¿¹ä¸Šæ¶ˆè€—å®Œæ•´è®¡ç®—èµ„æºï¼Œè€Œå…‰æ …æ‰«æè§£ç æ–¹æ¡ˆç¼ºä¹æ•´ä½“ç”»å¸ƒè“å›¾ï¼Œå¯¼è‡´æ‰©å±•æ•ˆç›Šå—é™ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†è§†è§‰ARæ¨¡å‹å……åˆ†å‘æŒ¥æµ‹è¯•æ—¶æ‰©å±•çš„æ½œåŠ›ã€‚

**Method:** GridARé‡‡ç”¨ç½‘æ ¼åˆ†åŒºæ¸è¿›ç”Ÿæˆæ–¹æ¡ˆï¼Œåœ¨åŒä¸€ç”»å¸ƒä½ç½®ç”Ÿæˆå¤šä¸ªéƒ¨åˆ†å€™é€‰ï¼Œæ—©æœŸå‰ªæä¸å¯è¡Œå€™é€‰ï¼Œå¹¶å°†å¯è¡Œå€™é€‰å›ºå®šä¸ºé”šç‚¹æŒ‡å¯¼åç»­è§£ç ã€‚åŒæ—¶æå‡ºå¸ƒå±€æŒ‡å®šæç¤ºé‡æ„ç­–ç•¥ï¼Œé€šè¿‡æ£€æŸ¥éƒ¨åˆ†è§†å›¾æ¨æ–­å¯è¡Œå¸ƒå±€ï¼Œé‡æ„åçš„æç¤ºæŒ‡å¯¼å›¾åƒç”Ÿæˆä»¥å¼¥è¡¥è“å›¾ç¼ºå¤±ã€‚

**Result:** åœ¨T2I-CompBench++åŸºå‡†ä¸Šï¼ŒGridARåœ¨N=4æ—¶æ€§èƒ½è¶…è¶ŠBest-of-Nï¼ˆN=8ï¼‰14.4%ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬é™ä½25.6%ã€‚åœ¨PIE-Benchå›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç¼–è¾‘è´¨é‡çš„åŒæ—¶ï¼Œè¯­ä¹‰ä¿æŒåº¦æ¯”æ›´å¤§Nçš„åŸºçº¿æå‡13.9%ã€‚

**Conclusion:** GridARè¯æ˜äº†è§†è§‰è‡ªå›å½’æ¨¡å‹æµ‹è¯•æ—¶æ‰©å±•çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡æ™ºèƒ½å€™é€‰ç®¡ç†å’Œå¸ƒå±€å¼•å¯¼è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶ä¸ä»…æå‡äº†ç”Ÿæˆè´¨é‡ï¼Œè¿˜æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä¸ºè§†è§‰ARæ¨¡å‹çš„å®ç”¨åŒ–éƒ¨ç½²æä¾›äº†å¯è¡Œè·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Recent visual autoregressive (AR) models have shown promising capabilities in text-to-image generation, operating in a manner similar to large language models. While test-time computation scaling has brought remarkable success in enabling reasoning-enhanced outputs for challenging natural language tasks, its adaptation to visual AR models remains unexplored and poses unique challenges. Naively applying test-time scaling strategies such as Best-of-N can be suboptimal: they consume full-length computation on erroneous generation trajectories, while the raster-scan decoding scheme lacks a blueprint of the entire canvas, limiting scaling benefits as only a few prompt-aligned candidates are generated. To address these, we introduce GridAR, a test-time scaling framework designed to elicit the best possible results from visual AR models. GridAR employs a grid-partitioned progressive generation scheme in which multiple partial candidates for the same position are generated within a canvas, infeasible ones are pruned early, and viable ones are fixed as anchors to guide subsequent decoding. Coupled with this, we present a layout-specified prompt reformulation strategy that inspects partial views to infer a feasible layout for satisfying the prompt. The reformulated prompt then guides subsequent image generation to mitigate the blueprint deficiency. Together, GridAR achieves higher-quality results under limited test-time scaling: with N=4, it even outperforms Best-of-N (N=8) by 14.4% on T2I-CompBench++ while reducing cost by 25.6%. It also generalizes to autoregressive image editing, showing comparable edit quality and a 13.9% gain in semantic preservation on PIE-Bench over larger-N baselines.


### [29] [Scenes as Tokens: Multi-Scale Normal Distributions Transform Tokenizer for General 3D Vision-Language Understanding](https://arxiv.org/abs/2511.21191)
*Yutao Tang, Cheng Zhao, Gaurav Mittal, Rohith Kukkala, Rama Chellappa, Cheng Peng, Mei Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†NDTokenizer3Dï¼Œä¸€ç§é€šç”¨çš„3Dè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ–°é¢–çš„ä¸‰é˜¶æ®µåœºæ™¯æ ‡è®°åŒ–æµç¨‹å’ŒåŸºäºå¤šå°ºåº¦æ­£æ€åˆ†å¸ƒå˜æ¢çš„è¡¨ç¤ºæ–¹æ³•ï¼Œåœ¨å¤šç§3Dåœºæ™¯ç†è§£ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼ŒåŒæ—¶æ”¯æŒäººæœºäº¤äº’ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰3Dè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å°†3Dåœºæ™¯æœ‰æ•ˆæ ‡è®°åŒ–ä¸ºæ•´ä½“åœºæ™¯æ ‡è®°ï¼Œå¹¶è·¨å¤šç§3Dç†è§£ä»»åŠ¡åˆ©ç”¨è¿™äº›æ ‡è®°æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæ¡¥æ¥è¯­è¨€çº§æ¨ç†ä¸3Dç©ºé—´ç†è§£çš„é€šç”¨è§£å†³æ–¹æ¡ˆã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºå¤šå°ºåº¦æ­£æ€åˆ†å¸ƒå˜æ¢è¡¨ç¤ºçš„ä¸‰é˜¶æ®µåœºæ™¯æ ‡è®°åŒ–æµç¨‹ï¼Œé¦–å…ˆä»åŸå§‹é«˜åˆ†è¾¨ç‡ç‚¹äº‘æ„å»ºå¤šå°ºåº¦NDTè¡¨ç¤ºä»¥ä¿ç•™å…¨å±€ä¸Šä¸‹æ–‡å’Œç»†ç²’åº¦å‡ ä½•ç»†èŠ‚ï¼Œç„¶åé€šè¿‡å¤šå°ºåº¦NDTè§£ç å™¨é€æ­¥èåˆè·¨å°ºåº¦ç‰¹å¾ç”Ÿæˆæ•´ä½“åœºæ™¯æ ‡è®°ï¼Œè¯¥è§£ç å™¨è¿˜è¢«é‡æ–°ç”¨ä½œäººæœºäº¤äº’æç¤ºå’Œåˆ†å‰²æ©ç è§£ç çš„é€šç”¨æ¥å£ã€‚

**Result:** NDTokenizer3Dåœ¨3Då‚è€ƒåˆ†å‰²ã€3Dè§†è§‰é—®ç­”å’Œ3Då¯†é›†æè¿°ç­‰ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œå…¶ç´§å‡‘ç»Ÿä¸€çš„è®¾è®¡ä½¿å…¶æˆä¸ºç»†ç²’åº¦çš„é€šç”¨3Dè§†è§‰è¯­è¨€æ¨¡å‹ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡ç»Ÿä¸€çš„å¤šå°ºåº¦è¡¨ç¤ºå’Œæ ‡è®°åŒ–æ–¹æ³•ï¼Œå¯ä»¥åœ¨å•ä¸€æ¶æ„å†…å®ç°å¤šç§3Dåœºæ™¯ç†è§£ä»»åŠ¡çš„ç»Ÿä¸€å¤„ç†ï¼Œä¸º3Dè§†è§‰è¯­è¨€æ¨¡å‹çš„å‘å±•æä¾›äº†æ–°çš„è®¾è®¡èŒƒå¼ï¼Œå¼ºè°ƒäº†åœºæ™¯è¡¨ç¤ºä¸è¯­è¨€æ¨¡å‹é›†æˆçš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Recent advances in 3D vision-language models (VLMs) highlight a strong potential for 3D scene understanding and reasoning. However, effectively tokenizing 3D scenes into holistic scene tokens, and leveraging these tokens across diverse 3D understanding tasks, remain highly challenging. We present NDTokenizer3D, a generalist 3D VLM that performs a wide range of 3D scene understanding tasks while naturally supporting human interactions, thereby bridging language-level reasoning with 3D spatial understanding. The core of our approach is a novel three-stage scene tokenization pipeline built upon a Multi-Scale Normal Distributions Transform (NDT) representation, paired with a Multi-Scale NDT Decoder (MSDec). Specifically, NDTokenizer3D first constructs a multi-scale NDT representation from raw high-resolution point clouds, preserving both global context and fine-grained geometric details. Next, the MSDec progressively fuses cross-scale NDT features, producing holistic scene tokens consumable by LLM endpoints. Beyond tokenization, MSDec is repurposed as a general interface for human-interactive prompting (points, boxes, masks) and segmentation-mask decoding, unifying diverse 3D scene understanding tasks within a single architecture. With this compact and unified design, NDTokenizer3D offers a fine-grained, general-purpose 3D VLM, achieving remarkable improvements in 3D Referring Segmentation, 3D Visual Question Answering, and 3D Dense Captioning.


### [30] [When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21192)
*Hui Lu, Yi Yu, Yiming Yang, Chenyu Yi, Qixin Zhang, Bingquan Shen, Alex C. Kot, Xudong Jiang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºUPA-RFASï¼Œä¸€ç§é’ˆå¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„é€šç”¨å¯è¿ç§»å¯¹æŠ—è¡¥ä¸æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡å…±äº«ç‰¹å¾ç©ºé—´å­¦ä¹ å•ä¸€ç‰©ç†è¡¥ä¸ï¼Œåœ¨æœªçŸ¥æ¶æ„ã€å¾®è°ƒå˜ä½“å’Œä»¿çœŸåˆ°çœŸå®åœºæ™¯ä¸‹å®ç°è·¨æ¨¡å‹æ”»å‡»ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å®¹æ˜“å—åˆ°å¯¹æŠ—æ”»å‡»ï¼Œä½†é€šç”¨ä¸”å¯è¿ç§»çš„æ”»å‡»æ–¹æ³•ç ”ç©¶ä¸è¶³ï¼Œå¤§å¤šæ•°ç°æœ‰è¡¥ä¸æ–¹æ³•è¿‡åº¦æ‹Ÿåˆå•ä¸€æ¨¡å‹ä¸”åœ¨é»‘ç›’è®¾ç½®ä¸‹å¤±æ•ˆï¼Œè¿™é™åˆ¶äº†å¯¹æŠ—æ”»å‡»åœ¨å®é™…æœºå™¨äººç³»ç»Ÿä¸­çš„é€‚ç”¨æ€§è¯„ä¼°ã€‚

**Method:** UPA-RFASæ¡†æ¶ç»“åˆäº†ç‰¹å¾ç©ºé—´ç›®æ ‡å‡½æ•°ï¼ˆåŒ…å«â„“1åå·®å…ˆéªŒå’Œæ’æ–¥æ€§InfoNCEæŸå¤±ï¼‰ã€é²æ£’æ€§å¢å¼ºçš„ä¸¤é˜¶æ®µmin-maxä¼˜åŒ–ï¼ˆå†…ç¯å­¦ä¹ ä¸å¯è§çš„æ ·æœ¬çº§æ‰°åŠ¨ï¼Œå¤–ç¯é’ˆå¯¹å¼ºåŒ–é‚»åŸŸä¼˜åŒ–é€šç”¨è¡¥ä¸ï¼‰ï¼Œä»¥åŠä¸¤ä¸ªVLAç‰¹å®šæŸå¤±å‡½æ•°ï¼ˆè¡¥ä¸æ³¨æ„åŠ›ä¸»å¯¼å’Œè¡¥ä¸è¯­ä¹‰é”™é…ï¼‰æ¥åŠ«æŒæ–‡æœ¬åˆ°è§†è§‰çš„æ³¨æ„åŠ›å¹¶è¯±å¯¼å›¾åƒ-æ–‡æœ¬ä¸åŒ¹é…ã€‚

**Result:** å®éªŒè¡¨æ˜UPA-RFASåœ¨ä¸åŒVLAæ¨¡å‹ã€æ“ä½œå¥—ä»¶å’Œç‰©ç†æ‰§è¡Œä¸­å‡èƒ½å®ç°è·¨æ¨¡å‹ã€è·¨ä»»åŠ¡å’Œè·¨è§†è§’çš„ä¸€è‡´è¿ç§»æ”»å‡»ï¼Œæš´éœ²äº†åŸºäºè¡¥ä¸çš„å®é™…æ”»å‡»é¢ï¼Œä¸ºæœªæ¥é˜²å¾¡ç ”ç©¶å»ºç«‹äº†å¼ºåŸºå‡†ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†VLAæ¨¡å‹åœ¨å¯¹æŠ—è¡¥ä¸æ”»å‡»ä¸‹çš„å®é™…è„†å¼±æ€§ï¼Œè¯æ˜äº†å•ä¸€è¡¥ä¸åœ¨æœªçŸ¥æ¨¡å‹å’ŒçœŸå®ç¯å¢ƒä¸­çš„å¯è¿ç§»æ€§ï¼Œä¸ºè¯„ä¼°æœºå™¨äººç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¼€å‘æ›´é²æ£’çš„é˜²å¾¡æœºåˆ¶æä¾›äº†é‡è¦è§è§£å’ŒåŸºå‡†æ–¹æ³•ã€‚

---

#### ğŸ“„ Abstract
Vision-Language-Action (VLA) models are vulnerable to adversarial attacks, yet universal and transferable attacks remain underexplored, as most existing patches overfit to a single model and fail in black-box settings. To address this gap, we present a systematic study of universal, transferable adversarial patches against VLA-driven robots under unknown architectures, finetuned variants, and sim-to-real shifts. We introduce UPA-RFAS (Universal Patch Attack via Robust Feature, Attention, and Semantics), a unified framework that learns a single physical patch in a shared feature space while promoting cross-model transfer. UPA-RFAS combines (i) a feature-space objective with an $\ell_1$ deviation prior and repulsive InfoNCE loss to induce transferable representation shifts, (ii) a robustness-augmented two-phase min-max procedure where an inner loop learns invisible sample-wise perturbations and an outer loop optimizes the universal patch against this hardened neighborhood, and (iii) two VLA-specific losses: Patch Attention Dominance to hijack text$\to$vision attention and Patch Semantic Misalignment to induce image-text mismatch without labels. Experiments across diverse VLA models, manipulation suites, and physical executions show that UPA-RFAS consistently transfers across models, tasks, and viewpoints, exposing a practical patch-based attack surface and establishing a strong baseline for future defenses.


### [31] [BotaCLIP: Contrastive Learning for Botany-Aware Representation of Earth Observation Data](https://arxiv.org/abs/2511.21194)
*Selene Cerna, Sara Si-Moussi, Wilfried Thuiller, Hadrien Hendrikx, Vincent Miele*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†BotaCLIPï¼Œä¸€ç§è½»é‡çº§å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå°†é¢†åŸŸä¸“ä¸šçŸ¥è¯†æ³¨å…¥é¢„è®­ç»ƒçš„åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹ï¼Œåœ¨ç”Ÿæ€å»ºæ¨¡ä»»åŠ¡ä¸­å®ç°äº†ä¼˜äºåŸå§‹æ¨¡å‹å’Œç›‘ç£åŸºçº¿çš„æ€§èƒ½è¡¨ç°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºç¡€æ¨¡å‹è™½ç„¶èƒ½å¤Ÿå­¦ä¹ ä¸°å¯Œçš„è·¨æ¨¡æ€è¡¨ç¤ºï¼Œä½†åœ¨é€‚åº”ç‰¹å®šé¢†åŸŸçŸ¥è¯†æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨ä¸é‡æ–°è®­ç»ƒæˆ–æ˜¾è‘—å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œå°†é¢†åŸŸä¸“ä¸šçŸ¥è¯†æ³¨å…¥é¢„è®­ç»ƒæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„ç”Ÿæ€å»ºæ¨¡åœºæ™¯ä¸­ã€‚

**Method:** BotaCLIPé‡‡ç”¨è½»é‡çº§å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å°†é«˜åˆ†è¾¨ç‡èˆªç©ºå½±åƒä¸æ¤ç‰©æ ·æ–¹æ•°æ®è¿›è¡Œå¯¹é½ï¼Œç»“åˆæ­£åˆ™åŒ–ç­–ç•¥ç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä»è€Œåœ¨é¢„è®­ç»ƒçš„DOFAåœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹ä¸­å†…åŒ–ç”Ÿæ€ç»“æ„çŸ¥è¯†ã€‚

**Result:** åœ¨æ¤ç‰©å­˜åœ¨é¢„æµ‹ã€è´è¶å‡ºç°å»ºæ¨¡å’ŒåœŸå£¤è¥å…»ç»„ä¸°åº¦ä¼°è®¡ä¸‰ä¸ªç”Ÿæ€ä»»åŠ¡ä¸­ï¼ŒBotaCLIPè¡¨ç¤ºå‡å–å¾—äº†ä¼˜äºDOFAåŸºç¡€æ¨¡å‹å’Œç›‘ç£åŸºçº¿çš„æ€§èƒ½è¡¨ç°ï¼Œæ˜¾ç¤ºå‡ºåœ¨ç”Ÿæ€å»ºæ¨¡ä»»åŠ¡ä¸­çš„ä¸€è‡´æ”¹è¿›ã€‚

**Conclusion:** è¿™é¡¹å·¥ä½œå±•ç¤ºäº†é¢†åŸŸæ„ŸçŸ¥çš„åŸºç¡€æ¨¡å‹é€‚åº”æ–¹æ³•èƒ½å¤Ÿåœ¨æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸­æ³¨å…¥ä¸“å®¶çŸ¥è¯†ï¼Œå®ç°ç»æµçš„è¡¨ç¤ºå­¦ä¹ ï¼Œä¸ºç‰¹å®šé¢†åŸŸçš„åŸºç¡€æ¨¡å‹å®šåˆ¶åŒ–æä¾›äº†å¯è¡Œè·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Foundation models have demonstrated a remarkable ability to learn rich, transferable representations across diverse modalities such as images, text, and audio. In modern machine learning pipelines, these representations often replace raw data as the primary input for downstream tasks. In this paper, we address the challenge of adapting a pre-trained foundation model to inject domain-specific knowledge, without retraining from scratch or incurring significant computational costs. To this end, we introduce BotaCLIP, a lightweight multimodal contrastive framework that adapts a pre-trained Earth Observation foundation model (DOFA) by aligning high-resolution aerial imagery with botanical relevÃ©s. Unlike generic embeddings, BotaCLIP internalizes ecological structure through contrastive learning with a regularization strategy that mitigates catastrophic forgetting. Once trained, the resulting embeddings serve as transferable representations for downstream predictors. Motivated by real-world applications in biodiversity modeling, we evaluated BotaCLIP representations in three ecological tasks: plant presence prediction, butterfly occurrence modeling, and soil trophic group abundance estimation. The results showed consistent improvements over those derived from DOFA and supervised baselines. More broadly, this work illustrates how domain-aware adaptation of foundation models can inject expert knowledge into data-scarce settings, enabling frugal representation learning.


### [32] [Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting](https://arxiv.org/abs/2511.21265)
*Juncheng Chen, Chao Xu, Yanjun Cao*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMatchGSæ¡†æ¶ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°ä¿®æ­£å¹¶åˆ©ç”¨3Dé«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰å®ç°é²æ£’çš„é›¶æ ·æœ¬å›¾åƒåŒ¹é…ã€‚é€šè¿‡å‡ ä½•ç²¾ç‚¼çš„æ•°æ®ç”Ÿæˆæµç¨‹å’Œ2D-3Dè¡¨ç¤ºå¯¹é½ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒåŒ¹é…å™¨çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŸºäºå­¦ä¹ çš„å›¾åƒåŒ¹é…ä¸¥é‡ä¾èµ–å¤§è§„æ¨¡ã€å¤šæ ·åŒ–ä¸”å‡ ä½•å‡†ç¡®çš„è®­ç»ƒæ•°æ®ã€‚3Dé«˜æ–¯æº…å°„è™½ç„¶èƒ½å¤Ÿå®ç°é€¼çœŸçš„æ–°è§†è§’åˆæˆï¼Œä½†å…¶å‡ ä½•ä¸å‡†ç¡®æ€§å’Œæ·±åº¦æ¸²æŸ“åå·®é˜»ç¢äº†é²æ£’å¯¹åº”å…³ç³»çš„æ ‡æ³¨ã€‚

**Method:** MatchGSæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå‡ ä½•å¿ å®çš„æ•°æ®ç”Ÿæˆæµç¨‹ï¼Œé€šè¿‡ç²¾ç‚¼3DGSå‡ ä½•æ¥äº§ç”Ÿé«˜ç²¾åº¦å¯¹åº”æ ‡ç­¾ï¼›2D-3Dè¡¨ç¤ºå¯¹é½ç­–ç•¥ï¼Œå°†3DGSçš„æ˜¾å¼3DçŸ¥è¯†æ³¨å…¥2DåŒ¹é…å™¨ï¼Œå¼•å¯¼å­¦ä¹ è§†è§’ä¸å˜çš„3Dè¡¨ç¤ºã€‚

**Result:** ç”Ÿæˆçš„å¯¹åº”å…³ç³»å°†æçº¿è¯¯å·®é™ä½äº†40å€ï¼Œæ”¯æŒæç«¯è§†è§’å˜åŒ–çš„ç›‘ç£ï¼Œå¹¶é€šè¿‡é«˜æ–¯å±æ€§æä¾›è‡ªç›‘ç£ä¿¡å·ã€‚ä»…ä½¿ç”¨MatchGSæ•°æ®è®­ç»ƒçš„æœ€å…ˆè¿›åŒ¹é…å™¨åœ¨å…¬å¼€åŸºå‡†ä¸Šå®ç°äº†é«˜è¾¾17.7%çš„é›¶æ ·æœ¬æ€§èƒ½æå‡ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡é€‚å½“çš„å‡ ä½•ç²¾ç‚¼ï¼Œ3DGSå¯ä»¥ä½œä¸ºå¯æ‰©å±•ã€é«˜ä¿çœŸä¸”ç»“æ„ä¸°å¯Œçš„æ•°æ®æºï¼Œä¸ºæ–°ä¸€ä»£é²æ£’é›¶æ ·æœ¬å›¾åƒåŒ¹é…å™¨çš„å‘å±•é“ºå¹³é“è·¯ã€‚

---

#### ğŸ“„ Abstract
Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.


### [33] [The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment](https://arxiv.org/abs/2511.21331)
*Stefanos Koutoupis, Michaela Areti Zervou, Konstantinos Kontras, Maarten De Vos, Panagiotis Tsakalides, Grigorios Tsagatakis*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†å¯¹æ¯”èåˆï¼ˆConFuï¼‰æ¡†æ¶ï¼Œé€šè¿‡å°†å•ä¸ªæ¨¡æ€åŠå…¶èåˆç»„åˆå…±åŒåµŒå…¥åˆ°ç»Ÿä¸€è¡¨ç¤ºç©ºé—´ä¸­ï¼ŒåŒæ—¶æ•è·å¤šæ¨¡æ€é«˜é˜¶ä¾èµ–å…³ç³»å¹¶ä¿æŒå¼ºé…å¯¹å¯¹åº”æ€§ã€‚è¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºç«äº‰æ€§æ€§èƒ½ï¼Œæ”¯æŒç»Ÿä¸€çš„ä¸€å¯¹ä¸€å’Œä¸€å¯¹å¤šæ£€ç´¢ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ä¸»è¦å±€é™äºé…å¯¹è®¾ç½®ï¼Œä»…å¯¹é½ä¸¤ä¸ªæ¨¡æ€ï¼Œè€Œè¿‘æœŸå°è¯•æ•è·é«˜é˜¶äº¤äº’çš„æ–¹æ³•å¾€å¾€å¿½è§†æˆ–æœªèƒ½å……åˆ†ä¿æŒé…å¯¹å…³ç³»ï¼Œé™åˆ¶äº†å…¶åœ¨å•æ¨¡æ€ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚å¤šæ¨¡æ€è”åˆè¡¨ç¤ºå­¦ä¹ ä¸­çš„é«˜é˜¶ä¾èµ–å…³ç³»æ•è·ä¸é…å¯¹å…³ç³»ä¿æŒä¹‹é—´çš„å¹³è¡¡æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚

**Method:** ConFuæ¡†æ¶é€šè¿‡æ‰©å±•ä¼ ç»Ÿé…å¯¹å¯¹æ¯”ç›®æ ‡ï¼Œå¼•å…¥é¢å¤–çš„èåˆæ¨¡æ€å¯¹æ¯”é¡¹ï¼Œå°†æ¨¡æ€å¯¹ä¸ç¬¬ä¸‰ä¸ªæ¨¡æ€çš„è”åˆåµŒå…¥å¯¹é½ã€‚è¯¥æ¡†æ¶å°†å•ä¸ªæ¨¡æ€åŠå…¶èåˆç»„åˆå…±åŒåµŒå…¥åˆ°ç»Ÿä¸€è¡¨ç¤ºç©ºé—´ï¼Œä½¿æ¨¡æ€ä¸å…¶èåˆå¯¹åº”ç‰©å¯¹é½ï¼Œä»è€Œèƒ½å¤Ÿæ•è·ä»…é€šè¿‡é…å¯¹å¯¹é½æ— æ³•æ¢å¤çš„é«˜é˜¶ä¾èµ–å…³ç³»ï¼ˆå¦‚XORç±»å…³ç³»ï¼‰ã€‚

**Result:** åœ¨åˆæˆå’ŒçœŸå®å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒConFuåœ¨æ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡ä¸Šå±•ç°å‡ºç«äº‰æ€§æ€§èƒ½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨è·¨æ¨¡æ€äº’è¡¥æ€§ã€æ•è·é«˜é˜¶ä¾èµ–å…³ç³»ï¼Œå¹¶éšç€å¤šæ¨¡æ€å¤æ‚åº¦çš„å¢åŠ è€Œæ‰©å±•ã€‚è¯¥æ¡†æ¶æ”¯æŒåœ¨å•ä¸€å¯¹æ¯”æ¡†æ¶å†…å®ç°ç»Ÿä¸€çš„ä¸€å¯¹ä¸€å’Œä¸€å¯¹å¤šæ£€ç´¢ã€‚

**Conclusion:** ConFuè¯æ˜äº†åœ¨å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä¸­åŒæ—¶ä¿æŒé«˜é˜¶äº¤äº’å’Œé…å¯¹å¯¹åº”æ€§çš„å¯è¡Œæ€§ï¼Œä¸ºå¤„ç†å¤æ‚å¤šæ¨¡æ€å…³ç³»æä¾›äº†ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€æœºå™¨å­¦ä¹ ä¸­å¹³è¡¡é«˜é˜¶ä¾èµ–æ•è·ä¸åŸºç¡€é…å¯¹å…³ç³»ä¿æŒæä¾›äº†é‡è¦è§è§£ï¼Œæ¨åŠ¨äº†æ›´å…¨é¢çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.


### [34] [Co-Training Vision Language Models for Remote Sensing Multi-task Learning](https://arxiv.org/abs/2511.21272)
*Qingyun Li, Shuran Ma, Junwei Luo, Yi Yu, Yue Zhou, Fengxiang Wang, Xudong Lu, Xiaoxing Wang, Xin He, Yushi Chen, Xue Yang, Junchi Yan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†RSCoVLMï¼Œä¸€ä¸ªç”¨äºé¥æ„Ÿå¤šä»»åŠ¡å­¦ä¹ çš„ç®€å•è€Œçµæ´»çš„è§†è§‰è¯­è¨€æ¨¡å‹åŸºçº¿ï¼Œé€šè¿‡ç»Ÿä¸€çš„æ•°æ®å¼•æ“ã€åŠ¨æ€åˆ†è¾¨ç‡ç­–ç•¥å’ŒZoom-in Chainæœºåˆ¶ï¼Œåœ¨å¤šä¸ªé¥æ„Ÿä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰é¥æ„Ÿé¢†åŸŸè™½ç„¶Transformeråœ¨å•ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹èƒ½å¤Ÿç»Ÿä¸€å¤„ç†å¤šä»»åŠ¡çš„é€šç”¨æ¨¡å‹ï¼›åŒæ—¶ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¥æ„Ÿå›¾åƒç†è§£ã€å®šä½å’Œè¶…é«˜æ¸…å›¾åƒæ¨ç†æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†é¢ä¸´å¤æ‚é¥æ„Ÿæ•°æ®ç¯å¢ƒå’Œå¤šæ ·åŒ–å›¾åƒå°ºåº¦çš„æŒ‘æˆ˜ã€‚

**Method:** æå‡ºäº†å®Œæ•´çš„æ•°æ®ç®¡ç†å¼•æ“ï¼ŒåŒ…æ‹¬æ•°æ®é‡‡é›†ã€ç¦»çº¿å¤„ç†å’Œé›†æˆã€åœ¨çº¿åŠ è½½å’ŒåŠ æƒï¼›è®¾è®¡äº†ç»Ÿä¸€åŠ¨æ€åˆ†è¾¨ç‡ç­–ç•¥å¤„ç†ä¸åŒå°ºåº¦çš„é¥æ„Ÿå›¾åƒï¼›é’ˆå¯¹è¶…é«˜æ¸…å›¾åƒå¼•å…¥äº†Zoom-in Chainæœºåˆ¶åŠå…¶å¯¹åº”æ•°æ®é›†LRS-VQA-Zoomï¼›å¢å¼ºäº†æ¨¡å‹çš„ç›®æ ‡æ£€æµ‹èƒ½åŠ›å¹¶æå‡ºäº†æ–°çš„è¯„ä¼°åè®®ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜RSCoVLMåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„é¥æ„Ÿè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”šè‡³å¯ä¸ä¸“é—¨çš„ä¸“å®¶æ¨¡å‹ç›¸åª²ç¾ï¼›æ‰€æœ‰è®­ç»ƒè¯„ä¼°å·¥å…·ã€æ¨¡å‹æƒé‡å’Œæ•°æ®é›†å‡å·²å¼€æºä»¥ç¡®ä¿å¯å¤ç°æ€§ã€‚

**Conclusion:** è¯¥åŸºçº¿æ¨¡å‹ä¸ºé€šç”¨é¥æ„Ÿæ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦æ¨åŠ¨ï¼Œå±•ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¥æ„Ÿå¤šä»»åŠ¡å­¦ä¹ ä¸­çš„å·¨å¤§æ½œåŠ›ï¼›æå‡ºçš„æ•°æ®å¼•æ“å’Œåˆ†è¾¨ç‡ç­–ç•¥ä¸ºè§£å†³é¥æ„Ÿæ•°æ®å¤æ‚æ€§å’Œè®¡ç®—è´Ÿæ‹…æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model's object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.


### [35] [SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding](https://arxiv.org/abs/2511.21339)
*Tae-Min Choi, Tae Kyeong Jeong, Garam Kim, Jaemin Lee, Yeongyoon Koh, In Cheul Choi, Jae-Ho Chung, Jong Woong Park, Juyoun Park*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†SurgMLLMBenchï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºå¼€å‘å’Œè¯„ä¼°äº¤äº’å¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤–ç§‘åœºæ™¯ç†è§£ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬æ–°æ”¶é›†çš„MAVISæ•°æ®é›†ï¼Œè§£å†³äº†ç°æœ‰å¤–ç§‘æ•°æ®é›†çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¤–ç§‘æ•°æ®é›†ä¸»è¦é‡‡ç”¨è§†è§‰é—®ç­”æ ¼å¼ï¼Œå­˜åœ¨åˆ†ç±»å­¦å¼‚æ„ä¸”ç¼ºä¹åƒç´ çº§åˆ†å‰²æ”¯æŒçš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†ä¸€è‡´æ€§è¯„ä¼°å’Œå®é™…åº”ç”¨ï¼Œå› æ­¤éœ€è¦æ„å»ºç»Ÿä¸€çš„å¤šæ¨¡æ€åŸºå‡†æ¥ä¿ƒè¿›å¤–ç§‘AIç ”ç©¶çš„å‘å±•ã€‚

**Method:** è¯¥ç ”ç©¶æ•´åˆäº†åƒç´ çº§å™¨æ¢°åˆ†å‰²æ©ç å’Œç»“æ„åŒ–è§†è§‰é—®ç­”æ ‡æ³¨ï¼Œæ¶µç›–è…¹è…”é•œã€æœºå™¨äººè¾…åŠ©å’Œæ˜¾å¾®å¤–ç§‘é¢†åŸŸï¼Œå¹¶åœ¨ç»Ÿä¸€åˆ†ç±»å­¦ä¸‹æ„å»ºäº†SurgMLLMBenchåŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒè¶…è¶Šä¼ ç»Ÿè§†è§‰é—®ç­”ä»»åŠ¡çš„å…¨é¢è¯„ä¼°å’Œæ›´ä¸°å¯Œçš„è§†è§‰å¯¹è¯äº¤äº’ã€‚

**Result:** å¹¿æ³›çš„åŸºçº¿å®éªŒè¡¨æ˜ï¼Œåœ¨SurgMLLMBenchä¸Šè®­ç»ƒçš„å•ä¸€æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸå‡èƒ½ä¿æŒä¸€è‡´çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„æ•°æ®é›†ï¼Œè¯æ˜äº†è¯¥åŸºå‡†çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** SurgMLLMBenchå°†ä½œä¸ºå…¬å¼€èµ„æºæ¨åŠ¨å¤šæ¨¡æ€å¤–ç§‘AIç ”ç©¶çš„å‘å±•ï¼Œæ”¯æŒå¯é‡å¤çš„è¯„ä¼°å’Œäº¤äº’å¼å¤–ç§‘æ¨ç†æ¨¡å‹çš„å¼€å‘ï¼Œä¸ºå¤–ç§‘åœºæ™¯ç†è§£æä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶å’Œä¸°å¯Œçš„æ•°æ®æ”¯æŒã€‚

---

#### ğŸ“„ Abstract
Recent advances in multimodal large language models (LLMs) have highlighted their potential for medical and surgical applications. However, existing surgical datasets predominantly adopt a Visual Question Answering (VQA) format with heterogeneous taxonomies and lack support for pixel-level segmentation, limiting consistent evaluation and applicability. We present SurgMLLMBench, a unified multimodal benchmark explicitly designed for developing and evaluating interactive multimodal LLMs for surgical scene understanding, including the newly collected Micro-surgical Artificial Vascular anastomosIS (MAVIS) dataset. It integrates pixel-level instrument segmentation masks and structured VQA annotations across laparoscopic, robot-assisted, and micro-surgical domains under a unified taxonomy, enabling comprehensive evaluation beyond traditional VQA tasks and richer visual-conversational interactions. Extensive baseline experiments show that a single model trained on SurgMLLMBench achieves consistent performance across domains and generalizes effectively to unseen datasets. SurgMLLMBench will be publicly released as a robust resource to advance multimodal surgical AI research, supporting reproducible evaluation and development of interactive surgical reasoning models.


### [36] [Monet: Reasoning in Latent Visual Space Beyond Images and Language](https://arxiv.org/abs/2511.21395)
*Qixun Wang, Yang Shi, Yifei Wang, Yuanxing Zhang, Pengfei Wan, Kun Gai, Xianghua Ying, Yisen Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Monetè®­ç»ƒæ¡†æ¶ï¼Œä½¿å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨æ½œåœ¨è§†è§‰ç©ºé—´ä¸­è¿›è¡Œæ¨ç†ï¼Œé€šè¿‡ç”Ÿæˆè¿ç»­åµŒå…¥ä½œä¸ºä¸­é—´è§†è§‰æ€ç»´ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æŠ½è±¡è§†è§‰æ¨ç†æ–¹é¢çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰æ¨ç†æ–¹æ³•åœ¨æŠ½è±¡è§†è§‰æ€ç»´æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå…¶çµæ´»æ€§å—åˆ°å¤–éƒ¨å·¥å…·çš„æ ¹æœ¬é™åˆ¶ï¼Œæ— æ³•å®ç°ç±»äººçš„æŠ½è±¡è§†è§‰æ¨ç†èƒ½åŠ›ã€‚

**Method:** æå‡ºäº†ä¸‰é˜¶æ®µè’¸é¦å¼ç›‘ç£å¾®è°ƒç®¡é“ï¼ŒåŒ…æ‹¬æ„å»ºé«˜è´¨é‡çš„æ–‡æœ¬-å›¾åƒäº¤é”™æ€ç»´é“¾æ•°æ®é›†Monet-SFT-125Kï¼Œå¹¶è®¾è®¡äº†VLPOï¼ˆè§†è§‰æ½œåœ¨ç­–ç•¥ä¼˜åŒ–ï¼‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå°†æ½œåœ¨åµŒå…¥æ˜¾å¼çº³å…¥ç­–ç•¥æ¢¯åº¦æ›´æ–°ã€‚

**Result:** Monet-7Bæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œæ„ŸçŸ¥å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæŒç»­å¢ç›Šï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æŠ½è±¡è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§çš„åˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºè§†è§‰æ½œåœ¨æ¨ç†çš„æœªæ¥å‘å±•æä¾›äº†é‡è¦è§è§£ï¼Œé€šè¿‡å®è¯åˆ†æå„è®­ç»ƒç»„ä»¶çš„ä½œç”¨å¹¶è®¨è®ºæ—©æœŸå¤±è´¥å°è¯•ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€æ¨ç†å‘æ›´æŠ½è±¡ã€æ›´çµæ´»çš„è§†è§‰æ€ç»´æ–¹å‘å‘å±•ã€‚

---

#### ğŸ“„ Abstract
"Thinking with images" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.


### [37] [Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning](https://arxiv.org/abs/2511.21375)
*Xin Gu, Haoji Zhang, Qihang Fan, Jingxuan Niu, Zhipeng Zhang, Libo Zhang, Guang Chen, Fan Chen, Longyin Wen, Sijie Zhu*

#### ğŸ§© TL;DR
STVG-o1æ˜¯é¦–ä¸ªæ— éœ€æ¶æ„ä¿®æ”¹å³å¯è®©ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶ç©ºè§†é¢‘å®šä½ä»»åŠ¡ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½çš„æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è¾¹ç•Œæ¡†æ€ç»´é“¾æœºåˆ¶å’Œå¤šç»´å¼ºåŒ–å¥–åŠ±å‡½æ•°ï¼Œæ˜¾è‘—æå‡äº†MLLMsåœ¨ç»†ç²’åº¦æ—¶ç©ºå®šä½ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ—¶ç©ºè§†é¢‘å®šä½ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦åŸå› æ˜¯è®­ç»ƒç›®æ ‡ä¸åŒ¹é…ä»¥åŠæ ‡å‡†è§†è§‰ç¼–ç å™¨ä¸­ç»†ç²’åº¦åŒºåŸŸ-è¯è¯­å¯¹é½èƒ½åŠ›è¾ƒå¼±ï¼Œè¿™é™åˆ¶äº†MLLMsåœ¨ç²¾ç¡®æ—¶ç©ºå®šä½ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

**Method:** è¯¥æ–¹æ³•å¼•å…¥äº†è¾¹ç•Œæ¡†æ€ç»´é“¾æœºåˆ¶ï¼Œåœ¨ç”Ÿæˆæœ€ç»ˆé¢„æµ‹å‰æ˜¾å¼æ¨ç†æ—¶ç©ºä½ç½®ä½œä¸ºä¸­é—´æ­¥éª¤ï¼Œå¹¶è®¾è®¡äº†åŒ…å«æ ¼å¼ã€ä¸€è‡´æ€§ã€æ—¶é—´ã€ç©ºé—´å’Œæ€ç»´å¥–åŠ±çš„å¤šç»´å¼ºåŒ–å¥–åŠ±å‡½æ•°ï¼Œé€šè¿‡å¼ºåŒ–å¾®è°ƒæä¾›å‡ ä½•æ„ŸçŸ¥çš„ç›‘ç£ä¿¡å·ã€‚

**Result:** åœ¨HCSTVG-v1/v2å’ŒVidSTGæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒSTVG-o1åœ¨HCSTVGä¸Šåˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œåœ¨HCSTVG-v1ä¸Šä»¥7.3%çš„m_tIoUä¼˜åŠ¿è¶…è¶Šæœ€ä½³ä»»åŠ¡ç‰¹å®šæ–¹æ³•ï¼Œåœ¨VidSTGä¸Šä¸ä¸“ç”¨æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œå¹¶å¤§å¹…è¶…è¶Šæ‰€æœ‰ç°æœ‰çš„åŸºäºMLLMçš„æ–¹æ³•ã€‚

**Conclusion:** è¯¥ç ”ç©¶ç¡®ç«‹äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºç²¾ç¡®æ—¶ç©ºå®šä½ä»»åŠ¡å¯è¡Œä¸”å¼ºå¤§çš„éª¨å¹²ç½‘ç»œï¼Œå±•ç¤ºäº†å¼ºå¤§çš„è·¨æ•°æ®é›†å¼€æ”¾è¯æ±‡æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºMLLMsåœ¨ç»†ç²’åº¦è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Spatio-temporal video grounding (STVG) requires localizing a target object in untrimmed videos both temporally and spatially from natural language descriptions. Despite their strong language understanding, multimodal large language models (MLLMs) underperform on STVG due to misaligned training objectives and weak fine-grained region-word alignment in standard visual encoders. To address this, we propose STVG-o1, the first framework that enables off-the-shelf MLLMs to achieve state-of-the-art STVG performance without any architectural modifications. Our method introduces a bounding-box chain-of-thought mechanism that explicitly reasons about spatio-temporal locations in an intermediate step before producing the final prediction. We further design a multi-dimensional reinforcement reward function consisting of format, consistency, temporal, spatial, and think rewards, which provides geometry-aware supervision through reinforcement fine-tuning. Evaluated on HCSTVG-v1/v2 and VidSTG, STVG-o1 sets new state-of-the-art results on HCSTVG, outperforming the best task-specific method by 7.3\% m\_tIoU on HCSTVG-v1, matching specialized models on VidSTG, and surpassing all existing MLLM-based approaches by large margins. It also demonstrates strong open-vocabulary generalization across datasets, establishing MLLMs as viable and powerful backbones for precise spatio-temporal grounding. Our code and models will be released.


### [38] [From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings](https://arxiv.org/abs/2511.21428)
*Jiajie Zhang, SÃ¶ren Schwertfeger, Alexander Kleiner*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— ç›‘ç£æ¡†æ¶ï¼Œä»è¿ç»­å·¥ä¸šè§†é¢‘æµä¸­è§£é”å¤§é‡æœªæ ‡è®°çš„äººç±»æ¼”ç¤ºæ•°æ®ï¼Œç”¨äºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹é¢„è®­ç»ƒã€‚è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§è¿åŠ¨åˆ†è¯å™¨å’ŒåŸºäºæ½œåœ¨åŠ¨ä½œèƒ½é‡çš„æ— ç›‘ç£åŠ¨ä½œåˆ†å‰²å™¨ï¼Œè‡ªåŠ¨å‘ç°å’Œç»„ç»‡è¯­ä¹‰ä¸€è‡´çš„åŠ¨ä½œåŸºå…ƒã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å·¥ä¸šç¯å¢ƒä¸­å­˜åœ¨å¤§é‡æœªæ ‡è®°çš„äººç±»æ¼”ç¤ºè§†é¢‘æ•°æ®ï¼Œä½†ç¼ºä¹è‡ªåŠ¨åŒ–æ–¹æ³•æ¥æå–ç»“æ„åŒ–æ•°æ®ç”¨äºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹é¢„è®­ç»ƒã€‚ç°æœ‰æ–¹æ³•éœ€è¦äººå·¥æ ‡æ³¨æˆ–ç›‘ç£å­¦ä¹ ï¼Œéš¾ä»¥æ‰©å±•åˆ°å·¥ä¸šè§„æ¨¡çš„è¿ç»­è§†é¢‘æµï¼Œè¿™é™åˆ¶äº†å…·èº«AIåœ¨åˆ¶é€ ä¸šä¸­çš„é›†æˆåº”ç”¨ã€‚

**Method:** è¯¥æ–¹æ³•é¦–å…ˆè®­ç»ƒè½»é‡çº§è¿åŠ¨åˆ†è¯å™¨æ¥ç¼–ç è¿åŠ¨åŠ¨æ€ï¼Œç„¶åé‡‡ç”¨æ— ç›‘ç£åŠ¨ä½œåˆ†å‰²å™¨ï¼Œåˆ©ç”¨æ–°é¢–çš„æ½œåœ¨åŠ¨ä½œèƒ½é‡åº¦é‡æ¥å‘ç°å’Œåˆ†å‰²è¯­ä¹‰ä¸€è‡´çš„åŠ¨ä½œåŸºå…ƒã€‚æ•´ä¸ªæµæ°´çº¿è¾“å‡ºåˆ†å‰²çš„è§†é¢‘ç‰‡æ®µåŠå…¶å¯¹åº”çš„æ½œåœ¨åŠ¨ä½œåºåˆ—ï¼Œä¸ºVLAé¢„è®­ç»ƒæä¾›ç»“æ„åŒ–æ•°æ®ã€‚

**Result:** åœ¨å…¬å…±åŸºå‡†æµ‹è¯•å’Œä¸“æœ‰ç”µæœºè£…é…æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåˆ†å‰²äººç±»åœ¨å·¥ä½œç«™æ‰§è¡Œçš„å…³é”®ä»»åŠ¡ã€‚é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œçš„èšç±»å’Œå®šé‡è¯„ä¼°è¯å®äº†æ‰€å‘ç°åŠ¨ä½œåŸºå…ƒçš„è¯­ä¹‰ä¸€è‡´æ€§ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¿™æ˜¯é¦–ä¸ªä»éç»“æ„åŒ–å·¥ä¸šè§†é¢‘ä¸­æå–å’Œç»„ç»‡VLAé¢„è®­ç»ƒæ•°æ®çš„å…¨è‡ªåŠ¨ç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œä¸ºåˆ¶é€ ä¸šä¸­å…·èº«AIé›†æˆæä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•è§£é”äº†å·¥ä¸šè§†é¢‘æ•°æ®çš„æ½œåŠ›ï¼Œä¸ºå¤§è§„æ¨¡VLAæ¨¡å‹è®­ç»ƒå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel "Latent Action Energy" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.


### [39] [EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation](https://arxiv.org/abs/2511.21439)
*Futian Wang, Fan Zhang, Xiao Wang, Mengqi Wang, Dexing Huang, Jin Tang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¶…å›¾å¼•å¯¼çš„æ—¶ç©ºäº‹ä»¶æµè¡¥å…¨æœºåˆ¶ï¼Œé€šè¿‡è¶…å›¾è¿æ¥ä¸åŒæ—¶é—´å’Œç©ºé—´ä½ç½®çš„äº‹ä»¶ä»¤ç‰Œï¼Œå¹¶åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ä¼ é€’æ¥è¡¥å…¨ç¨€ç–äº‹ä»¶ï¼Œæœ‰æ•ˆè§£å†³äº†äº‹ä»¶ç›¸æœºæ•°æ®ç©ºé—´ç¨€ç–æ€§å¯¼è‡´çš„æ¬ é‡‡æ ·é—®é¢˜ã€‚è¯¥æ–¹æ³•å¯çµæ´»æ•´åˆRGBä»¤ç‰Œå®ç°å¤šæ¨¡æ€ä¿¡æ¯è¡¥å…¨ï¼Œåœ¨å•æ ‡ç­¾å’Œå¤šæ ‡ç­¾äº‹ä»¶åˆ†ç±»ä»»åŠ¡ä¸­å‡éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** äº‹ä»¶ç›¸æœºäº§ç”Ÿçš„å¼‚æ­¥äº‹ä»¶æµå…·æœ‰ç©ºé—´ç¨€ç–ä½†æ—¶é—´å¯†é›†çš„ç‰¹æ€§ï¼Œä¸»æµçš„äº‹ä»¶è¡¨ç¤ºå­¦ä¹ æ–¹æ³•é€šå¸¸ä½¿ç”¨äº‹ä»¶å¸§ã€ä½“ç´ æˆ–å¼ é‡ä½œä¸ºè¾“å…¥ï¼Œè¿™äº›æ–¹æ³•è™½ç„¶å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†éš¾ä»¥è§£å†³ç”±ç©ºé—´ç¨€ç–æ€§å¼•èµ·çš„æ¬ é‡‡æ ·é—®é¢˜ã€‚

**Method:** æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¶…å›¾å¼•å¯¼æ—¶ç©ºäº‹ä»¶æµè¡¥å…¨æœºåˆ¶ï¼Œé€šè¿‡è¶…å›¾è¿æ¥ä¸åŒæ—¶é—´å’Œç©ºé—´ä½ç½®çš„äº‹ä»¶ä»¤ç‰Œï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ¶ˆæ¯ä¼ é€’æ¥è¡¥å…¨è¿™äº›ç¨€ç–äº‹ä»¶ã€‚è¯¥æ–¹æ³•å¯çµæ´»åœ°å°†RGBä»¤ç‰Œä½œä¸ºè¶…å›¾ä¸­çš„èŠ‚ç‚¹æ•´åˆåˆ°è¡¥å…¨æ¡†æ¶ä¸­ï¼Œå®ç°åŸºäºè¶…å›¾çš„å¤šæ¨¡æ€ä¿¡æ¯è¡¥å…¨ï¼Œéšåé€šè¿‡è‡ªæ³¨æ„åŠ›èšåˆä¸åŒæ—¶é—´æ­¥çš„è¶…å›¾èŠ‚ç‚¹ä¿¡æ¯ï¼Œå®ç°å¤šæ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆå­¦ä¹ å’Œèåˆã€‚

**Result:** åœ¨å•æ ‡ç­¾å’Œå¤šæ ‡ç­¾äº‹ä»¶åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒå……åˆ†éªŒè¯äº†æ‰€æå‡ºæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡äº‹ä»¶æ•°æ®çš„è¡¨ç¤ºå­¦ä¹ æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºäº‹ä»¶ç›¸æœºæ•°æ®å¤„ç†æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è¶…å›¾å¼•å¯¼è¡¥å…¨æœºåˆ¶ï¼Œé€šè¿‡æ—¶ç©ºä¸Šä¸‹æ–‡ä¿¡æ¯ä¼ é€’å’Œå¤šæ¨¡æ€ç‰¹å¾èåˆï¼ŒæˆåŠŸè§£å†³äº†äº‹ä»¶æµç©ºé—´ç¨€ç–æ€§é—®é¢˜ï¼Œä¸ºäº‹ä»¶è¡¨ç¤ºå­¦ä¹ å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œå¹¶å±•ç¤ºäº†åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸‹çš„åº”ç”¨æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Event cameras produce asynchronous event streams that are spatially sparse yet temporally dense. Mainstream event representation learning algorithms typically use event frames, voxels, or tensors as input. Although these approaches have achieved notable progress, they struggle to address the undersampling problem caused by spatial sparsity. In this paper, we propose a novel hypergraph-guided spatio-temporal event stream completion mechanism, which connects event tokens across different times and spatial locations via hypergraphs and leverages contextual information message passing to complete these sparse events. The proposed method can flexibly incorporate RGB tokens as nodes in the hypergraph within this completion framework, enabling multi-modal hypergraph-based information completion. Subsequently, we aggregate hypergraph node information across different time steps through self-attention, enabling effective learning and fusion of multi-modal features. Extensive experiments on both single- and multi-label event classification tasks fully validated the effectiveness of our proposed framework. The source code of this paper will be released on https://github.com/Event-AHU/EvRainDrop.


### [40] [Multimodal Robust Prompt Distillation for 3D Point Cloud Models](https://arxiv.org/abs/2511.21574)
*Xiang Gu, Liming Lu, Xu Zheng, Anan Du, Yongbin Zhou, Shuchao Pang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€é²æ£’æç¤ºè’¸é¦æ¡†æ¶ï¼ˆMRPDï¼‰ï¼Œé€šè¿‡å°†å­¦ç”Ÿç‚¹äº‘æ¨¡å‹ä¸æ¥è‡ªè§†è§‰ã€3Då’Œæ–‡æœ¬ç¼–ç å™¨çš„é²æ£’åµŒå…¥å¯¹é½ï¼Œå®ç°äº†å¯¹æŠ—æ”»å‡»ä¸‹çš„é«˜æ•ˆé˜²å¾¡ï¼Œåœ¨è®­ç»ƒé˜¶æ®µå®Œæˆè’¸é¦è€Œæ— éœ€æ¨ç†æ—¶é¢å¤–è®¡ç®—å¼€é”€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰3Dç‚¹äº‘æ¨¡å‹é˜²å¾¡æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šé«˜è®¡ç®—å¼€é”€å’Œè·¨ä¸åŒæ”»å‡»ç±»å‹çš„æ³›åŒ–èƒ½åŠ›å·®ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å…¶åœ¨å®‰å…¨æ•æ„Ÿåº”ç”¨ä¸­çš„å¯é æ€§ã€‚

**Method:** æå‡ºå¤šæ¨¡æ€é²æ£’æç¤ºè’¸é¦æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªä¸åŒæ•™å¸ˆæ¨¡å‹ï¼ˆå¤„ç†æ·±åº¦æŠ•å½±çš„è§†è§‰æ¨¡å‹ã€é«˜æ€§èƒ½3Dæ¨¡å‹å’Œæ–‡æœ¬ç¼–ç å™¨ï¼‰çš„é²æ£’åµŒå…¥æ¥å¯¹é½å­¦ç”Ÿç‚¹äº‘æ¨¡å‹ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨ç½®ä¿¡åº¦é—¨æ§æœºåˆ¶åŠ¨æ€å¹³è¡¡æ‰€æœ‰è¾“å…¥æ¨¡æ€çš„è´¡çŒ®ã€‚

**Result:** å¹¿æ³›å®éªŒè¡¨æ˜MRPDåœ¨ç™½ç›’å’Œé»‘ç›’æ”»å‡»ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›é˜²å¾¡æ–¹æ³•ï¼Œç”šè‡³åœ¨å¹²å‡€æ•°æ®ä¸Šä¹Ÿèƒ½è·å¾—æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** æœ¬ç ”ç©¶é€šè¿‡é«˜æ•ˆåˆ©ç”¨å¤šæ¨¡æ€çŸ¥è¯†ï¼Œä¸ºæ„å»ºé²æ£’çš„3Dè§†è§‰ç³»ç»Ÿæä¾›äº†ä¸€ç§æ–°é¢–å®ç”¨çš„èŒƒå¼ï¼Œå±•ç¤ºäº†åœ¨è®­ç»ƒé˜¶æ®µå®ŒæˆçŸ¥è¯†è’¸é¦çš„ä¼˜è¶Šæ€§ã€‚

---

#### ğŸ“„ Abstract
Adversarial attacks pose a significant threat to learning-based 3D point cloud models, critically undermining their reliability in security-sensitive applications. Existing defense methods often suffer from (1) high computational overhead and (2) poor generalization ability across diverse attack types. To bridge these gaps, we propose a novel yet efficient teacher-student framework, namely Multimodal Robust Prompt Distillation (MRPD) for distilling robust 3D point cloud model. It learns lightweight prompts by aligning student point cloud model's features with robust embeddings from three distinct teachers: a vision model processing depth projections, a high-performance 3D model, and a text encoder. To ensure a reliable knowledge transfer, this distillation is guided by a confidence-gated mechanism which dynamically balances the contribution of all input modalities. Notably, since the distillation is all during the training stage, there is no additional computational cost at inference. Extensive experiments demonstrate that MRPD substantially outperforms state-of-the-art defense methods against a wide range of white-box and black-box attacks, while even achieving better performance on clean data. Our work presents a new, practical paradigm for building robust 3D vision systems by efficiently harnessing multimodal knowledge.


### [41] [E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework](https://arxiv.org/abs/2511.21422)
*Adeela Islam, Stefano Fiorini, Manuel Lecha, Theodore Tsesmelis, Stuart James, Pietro Morerio, Alessio Del Bue*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºE-M3RFï¼Œä¸€ç§ç­‰å˜å¤šæ¨¡æ€3Dé‡ç»„æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå‡ ä½•å’Œé¢œè‰²ç‰¹å¾ä»¥åŠSE(3)æµåŒ¹é…æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†ç¢ç‰‡é‡ç»„ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡ ä½•ä¿¡æ¯ä¸è¶³æˆ–æ¨¡ç³Šçš„æƒ…å†µä¸‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„3Dé‡ç»„æ–¹æ³•ä¸»è¦ä¾èµ–å‡ ä½•ç‰¹å¾ï¼Œåœ¨å‡ ä½•ä¿¡æ¯ä¸è¶³ï¼ˆå¦‚å°ç¢ç‰‡ã€ä¾µèš€ç¢ç‰‡æˆ–å¯¹ç§°ç¢ç‰‡ï¼‰æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸”ç¼ºä¹é˜²æ­¢é‡å ç»„è£…çš„ç‰©ç†çº¦æŸï¼Œè¿™é™åˆ¶äº†å®é™…åº”ç”¨æ•ˆæœã€‚

**Method:** E-M3RFé‡‡ç”¨ç­‰å˜å¤šæ¨¡æ€æ¡†æ¶ï¼Œè¾“å…¥åŒ…å«ä½ç½®å’Œé¢œè‰²çš„ç‚¹äº‘æ•°æ®ï¼Œä½¿ç”¨æ—‹è½¬ç­‰å˜ç¼–ç å™¨æå–å‡ ä½•ç‰¹å¾ï¼ŒTransformeræå–é¢œè‰²ç‰¹å¾ï¼Œç„¶åèåˆå½¢æˆå¤šæ¨¡æ€è¡¨ç¤ºï¼Œæœ€åé€šè¿‡SE(3)æµåŒ¹é…é¢„æµ‹é‡ç»„æ‰€éœ€çš„å˜æ¢ã€‚

**Result:** åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒE-M3RFåœ¨RePAIRæ•°æ®é›†ä¸Šç›¸æ¯”ç«äº‰æ–¹æ³•å°†æ—‹è½¬è¯¯å·®é™ä½23.1%ï¼Œå¹³ç§»è¯¯å·®é™ä½13.2%ï¼ŒChamferè·ç¦»å‡å°‘18.4%ï¼Œåœ¨åˆæˆå’ŒçœŸå®æ–‡åŒ–é—äº§æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ç»“åˆå¤šæ¨¡æ€ç‰¹å¾å’Œç­‰å˜å­¦ä¹ èƒ½æœ‰æ•ˆè§£å†³å‡ ä½•æ¨¡ç³Šæ€§é—®é¢˜ï¼ŒSE(3)æµåŒ¹é…ä¸º3Dé‡ç»„æä¾›äº†å¼ºå¤§çš„å˜æ¢é¢„æµ‹æ¡†æ¶ï¼Œä¸ºæ–‡åŒ–é—äº§ä¿æŠ¤ç­‰å®é™…åº”ç”¨æä¾›äº†æ›´å¯é çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.


### [42] [Video Generation Models Are Good Latent Reward Models](https://arxiv.org/abs/2511.21541)
*Xiaoyue Mi, Wenqing Yu, Jiesong Lian, Shibo Jie, Ruizhe Zhong, Zijun Liu, Guozhen Zhang, Zixiang Zhou, Zhiyong Xu, Yuan Zhou, Qinglin Lu, Fan Tang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºProcess Reward Feedback Learning (PRFL)ï¼Œä¸€ç§åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œåå¥½ä¼˜åŒ–çš„æ¡†æ¶ï¼Œè§£å†³äº†è§†é¢‘ç”Ÿæˆä¸­å¥–åŠ±åé¦ˆå­¦ä¹ çš„å†…å­˜æ¶ˆè€—å’Œè®­ç»ƒæ•ˆç‡é—®é¢˜ï¼Œé€šè¿‡ç›´æ¥åœ¨å™ªå£°æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†ä¸äººç±»åå¥½çš„å¯¹é½æ•ˆæœã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†é¢‘å¥–åŠ±æ¨¡å‹ä¾èµ–ä¸ºåƒç´ ç©ºé—´è¾“å…¥è®¾è®¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå°†ReFLä¼˜åŒ–é™åˆ¶åœ¨è®¡ç®—æ˜‚è´µçš„VAEè§£ç åçš„æ¥è¿‘å®Œå…¨å»å™ªæ­¥éª¤ï¼Œè¿™ç§åƒç´ ç©ºé—´æ–¹æ³•å¯¼è‡´å¤§é‡å†…å­˜å¼€é”€å’Œè®­ç»ƒæ—¶é—´å¢åŠ ï¼Œä¸”åæœŸä¼˜åŒ–ç¼ºä¹æ—©æœŸç›‘ç£ï¼Œä»…èƒ½ä¼˜åŒ–è§†è§‰è´¨é‡è€ŒéåŸºç¡€è¿åŠ¨åŠ¨æ€å’Œç»“æ„è¿è´¯æ€§ã€‚

**Method:** æå‡ºProcess Reward Feedback Learning (PRFL)æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å™ªå£°æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå¥–åŠ±å»ºæ¨¡ï¼Œè¿™äº›æ¨¡å‹ä¸“ä¸ºå¤„ç†ä»»æ„æ—¶é—´æ­¥çš„å™ªå£°æ½œåœ¨è¡¨ç¤ºè€Œè®¾è®¡ï¼Œé€šè¿‡åºåˆ—å»ºæ¨¡èƒ½åŠ›å›ºæœ‰åœ°ä¿ç•™æ—¶é—´ä¿¡æ¯ï¼Œå®ç°æ— éœ€VAEè§£ç çš„å®Œæ•´å»å™ªé“¾æ¢¯åº¦åå‘ä¼ æ’­ã€‚

**Result:** å¹¿æ³›å®éªŒè¡¨æ˜PRFLæ˜¾è‘—æå‡äº†ä¸äººç±»åå¥½çš„å¯¹é½æ•ˆæœï¼ŒåŒæ—¶åœ¨å†…å­˜æ¶ˆè€—å’Œè®­ç»ƒæ—¶é—´æ–¹é¢ç›¸æ¯”RGB ReFLå®ç°äº†å¤§å¹…å‡å°‘ï¼Œè¯æ˜äº†åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œåå¥½ä¼˜åŒ–çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¼˜åŠ¿ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜é¢„è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹å¤©ç„¶é€‚åˆåœ¨å™ªå£°æ½œåœ¨ç©ºé—´è¿›è¡Œå¥–åŠ±å»ºæ¨¡ï¼ŒPRFLæ¡†æ¶ä¸ºè§†é¢‘ç”Ÿæˆçš„å¯¹é½å­¦ä¹ æä¾›äº†æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¼€è¾Ÿäº†åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–çš„æ–°æ–¹å‘ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.


### [43] [Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy](https://arxiv.org/abs/2511.21579)
*Teng Hu, Zhentao Yu, Guozhen Zhang, Zihan Su, Zhengguang Zhou, Youliang Zhang, Yuan Zhou, Qinglin Lu, Ran Yi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Harmonyæ¡†æ¶ï¼Œé€šè¿‡è§£å†³è”åˆæ‰©æ•£è¿‡ç¨‹ä¸­çš„ä¸‰ä¸ªæ ¹æœ¬æŒ‘æˆ˜æ¥æ˜¾è‘—æå‡éŸ³è§†é¢‘åŒæ­¥ç”Ÿæˆè´¨é‡ã€‚è¯¥æ¡†æ¶å¼•å…¥è·¨ä»»åŠ¡ååŒè®­ç»ƒã€å…¨å±€-å±€éƒ¨è§£è€¦äº¤äº’æ¨¡å—å’ŒåŒæ­¥å¢å¼ºCFGï¼Œåœ¨ç”Ÿæˆä¿çœŸåº¦å’Œç»†ç²’åº¦éŸ³è§†é¢‘åŒæ­¥æ–¹é¢è¾¾åˆ°æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¼€æºæ¨¡å‹åœ¨éŸ³è§†é¢‘å†…å®¹åˆæˆä¸­é¢ä¸´éŸ³è§†é¢‘å¯¹é½ä¸ç¨³å®šçš„æ ¸å¿ƒé—®é¢˜ï¼Œè¿™æºäºè”åˆæ‰©æ•£è¿‡ç¨‹çš„ä¸‰ä¸ªæ ¹æœ¬æŒ‘æˆ˜ï¼šå¯¹åº”æ¼‚ç§»ã€ä½æ•ˆçš„å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ä»¥åŠä¼ ç»Ÿæ— åˆ†ç±»å™¨å¼•å¯¼çš„æ¨¡æ€å†…åå·®ã€‚è¿™äº›æŒ‘æˆ˜é˜»ç¢äº†ç¨³å®šçš„å¯¹é½å­¦ä¹ å’Œç»†ç²’åº¦æ—¶é—´çº¿ç´¢çš„æ•æ‰ã€‚

**Method:** Harmonyæ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®æŠ€æœ¯ï¼šè·¨ä»»åŠ¡ååŒè®­ç»ƒèŒƒå¼åˆ©ç”¨éŸ³é¢‘é©±åŠ¨è§†é¢‘å’Œè§†é¢‘é©±åŠ¨éŸ³é¢‘ç”Ÿæˆä»»åŠ¡çš„å¼ºç›‘ç£ä¿¡å·æ¥ç¼“è§£å¯¹åº”æ¼‚ç§»ï¼›å…¨å±€-å±€éƒ¨è§£è€¦äº¤äº’æ¨¡å—å®ç°é«˜æ•ˆç²¾ç¡®çš„æ—¶é—´-é£æ ¼å¯¹é½ï¼›åŒæ­¥å¢å¼ºCFGåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ˜¾å¼åˆ†ç¦»å¹¶æ”¾å¤§å¯¹é½ä¿¡å·ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜Harmonyåœ¨ç”Ÿæˆä¿çœŸåº¦å’Œç»†ç²’åº¦éŸ³è§†é¢‘åŒæ­¥æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„åŒæ­¥æ€§èƒ½å’Œç”Ÿæˆè´¨é‡ã€‚

**Conclusion:** ç ”ç©¶è¯æ˜é€šè¿‡æœºåˆ¶æ€§åœ°å¼ºåˆ¶æ‰§è¡ŒéŸ³è§†é¢‘åŒæ­¥ï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³è”åˆæ‰©æ•£è¿‡ç¨‹ä¸­çš„æ ¹æœ¬æŒ‘æˆ˜ã€‚Harmonyä¸ºç”Ÿæˆå¼AIä¸­çš„å¤šæ¨¡æ€å†…å®¹åˆæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¼ºè°ƒäº†å¯¹é½ä¿¡å·æ˜¾å¼å»ºæ¨¡çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [44] [Structured Definitions and Segmentations for Legal Reasoning in LLMs: A Study on Indian Legal Data](https://arxiv.org/abs/2511.20669)
*Mann Khatri, Mirza Yusuf, Rajiv Ratn Shah, Ponnurangam Kumaraguru*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡é‡ç»„æ³•å¾‹æ–‡æ¡£åŸºäºä¿®è¾è§’è‰²ã€å®šä¹‰æ³•å¾‹æœ¯è¯­ä»¥åŠæ¨¡æ‹Ÿæ³•é™¢é€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬æ³•å¾‹åˆ¤å†³é¢„æµ‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œåœ¨ä¸‰ä¸ªå°åº¦æ³•å¾‹æ•°æ®é›†ä¸Šå®ç°äº†1.5%è‡³4.36%çš„F1åˆ†æ•°æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶åœ¨é€šç”¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ³•å¾‹ç­‰ä¸“ä¸šé¢†åŸŸè¡¨ç°ä¸ä½³ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹é¢†åŸŸç‰¹å®šçš„é¢„è®­ç»ƒã€‚æ³•å¾‹æ–‡æ¡£é€šå¸¸å†—é•¿å¤æ‚ï¼Œä½¿å¾—æ¨¡å‹éš¾ä»¥æœ‰æ•ˆå¤„ç†å®Œæ•´æ–‡æœ¬ï¼Œç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•æ¥å¼¥è¡¥çŸ¥è¯†å·®è·ï¼Œä½†æœªå……åˆ†æ¢ç´¢ç»“æ„åŒ–ä¿¡æ¯å¯¹æ¨¡å‹å†³ç­–çš„å½±å“ã€‚

**Method:** ç ”ç©¶é€šè¿‡ä¸‰ä¸ªå®éªŒæ–¹å‘åˆ†ææ¨¡å‹åœ¨æ³•å¾‹ä»»åŠ¡ä¸­çš„è¡Œä¸ºï¼šåŸºäºä¿®è¾è§’è‰²é‡ç»„æ–‡æ¡£ä»¥è¯„ä¼°ç»“æ„åŒ–ä¿¡æ¯å¯¹é•¿æ–‡æœ¬å¤„ç†å’Œæ¨¡å‹å†³ç­–çš„å½±å“ï¼›å®šä¹‰ä¿®è¾è§’è‰²ä½¿æ¨¡å‹ç†Ÿæ‚‰æ³•å¾‹æœ¯è¯­ï¼›æ¨¡æ‹Ÿæ³•é™¢å…³äºä¿®è¾è§’è‰²çš„é€æ­¥æ¨ç†è¿‡ç¨‹ä»¥å¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚æ‰€æœ‰å®éªŒå‡åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹åœ¨ä¸‰ä¸ªå°åº¦æ³•å¾‹åˆ¤å†³é¢„æµ‹æ•°æ®é›†ä¸Šè¿›è¡Œã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œç»„ç»‡æ•°æ®æˆ–è§£é‡Šå…³é”®æ³•å¾‹æœ¯è¯­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œä¸åŸºçº¿ç›¸æ¯”F1åˆ†æ•°æœ€ä½æå‡çº¦1.5%ï¼Œæœ€é«˜æå‡4.36%ã€‚ç»“æ„åŒ–ä¿¡æ¯å¤„ç†å’Œæœ¯è¯­å®šä¹‰å¯¹æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸçš„è¡¨ç°äº§ç”Ÿäº†å®è´¨æ€§æ”¹è¿›ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜é€šè¿‡é€‚å½“çš„ç»“æ„åŒ–ä¿¡æ¯ç»„ç»‡å’Œé¢†åŸŸæœ¯è¯­å®šä¹‰ï¼Œå¯ä»¥æœ‰æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸“ä¸šæ³•å¾‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œè€Œæ— éœ€è¿›è¡Œå®Œæ•´çš„é¢†åŸŸå¯¹é½è®­ç»ƒã€‚è¿™ç§æ–¹æ³•ä¸ºå°†é€šç”¨è¯­è¨€æ¨¡å‹é€‚é…åˆ°ä¸“ä¸šé¢†åŸŸæä¾›äº†é«˜æ•ˆå¯è¡Œçš„è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Large Language Models (LLMs), trained on extensive datasets from the web, exhibit remarkable general reasoning skills. Despite this, they often struggle in specialized areas like law, mainly because they lack domain-specific pretraining. The legal field presents unique challenges, as legal documents are generally long and intricate, making it hard for models to process the full text efficiently. Previous studies have examined in-context approaches to address the knowledge gap, boosting model performance in new domains without full domain alignment. In our paper, we analyze model behavior on legal tasks by conducting experiments in three areas: (i) reorganizing documents based on rhetorical roles to assess how structured information affects long context processing and model decisions, (ii) defining rhetorical roles to familiarize the model with legal terminology, and (iii) emulating the step-by-step reasoning of courts regarding rhetorical roles to enhance model reasoning. These experiments are conducted in a zero-shot setting across three Indian legal judgment prediction datasets. Our results reveal that organizing data or explaining key legal terms significantly boosts model performance, with a minimum increase of ~1.5% and a maximum improvement of 4.36% in F1 score compared to the baseline.


### [45] [Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic](https://arxiv.org/abs/2511.20677)
*Saleh Almohaimeed, May Alsofyani, Saad Almohaimeed, Mansour Al Ghanim, Liqiang Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Ar-SParCï¼Œé¦–ä¸ªé˜¿æ‹‰ä¼¯è¯­è·¨é¢†åŸŸä¸Šä¸‹æ–‡ç›¸å…³æ–‡æœ¬åˆ°SQLæ•°æ®é›†ï¼ŒåŒ…å«3,450ä¸ªç›¸å…³é—®é¢˜å’Œ10,225ä¸ªæŸ¥è¯¢ï¼Œå¹¶é€šè¿‡GPTæ¨¡å‹å’Œåˆ›æ–°çš„GATæ ¡æ­£å™¨æ–¹æ³•æ˜¾è‘—æå‡äº†é˜¿æ‹‰ä¼¯è¯­SQLç”Ÿæˆæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ–‡æœ¬åˆ°SQLä»»åŠ¡çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­å’Œä¸­æ–‡é¢†åŸŸï¼Œé˜¿æ‹‰ä¼¯è¯­å®Œå…¨ç¼ºä¹ç›¸å…³æ•°æ®é›†å’Œç ”ç©¶ï¼Œè¿™é™åˆ¶äº†é˜¿æ‹‰ä¼¯è¯­ç”¨æˆ·åœ¨æ— éœ€SQLçŸ¥è¯†çš„æƒ…å†µä¸‹ä¸æ•°æ®åº“è¿›è¡Œè‡ªç„¶è¯­è¨€å¯¹è¯çš„èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦å¡«è¡¥è¿™ä¸€é‡è¦ç©ºç™½ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†Ar-SParCæ•°æ®é›†ï¼Œä½¿ç”¨GPT-3.5-turboå’ŒGPT-4.5-turboè¿›è¡Œ40ä¸ªå®éªŒï¼Œåº”ç”¨10ç§æç¤ºå·¥ç¨‹æŠ€æœ¯åŒ…æ‹¬å››ç§é—®é¢˜è¡¨ç¤ºæ–¹æ³•å’Œå…­ç§ä¸Šä¸‹æ–‡å­¦ä¹ æŠ€æœ¯ï¼Œå¹¶å¼€å‘äº†æ–°é¢–çš„GATæ ¡æ­£å™¨æ–¹æ³•æ¥æå‡æ€§èƒ½ã€‚

**Result:** GATæ ¡æ­£å™¨åœ¨æ‰€æœ‰40ä¸ªå®éªŒä¸­å‡æå‡äº†æ€§èƒ½ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹æ‰§è¡Œå‡†ç¡®ç‡ï¼ˆEXï¼‰å’Œäº¤äº’å‡†ç¡®ç‡ï¼ˆIXï¼‰å¹³å‡æé«˜1.9%ï¼Œåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ è®¾ç½®ä¸‹EXå¹³å‡æé«˜1.72%ã€IXæé«˜0.92%ï¼Œå¹¶é€šè¿‡æ¶ˆèç ”ç©¶éªŒè¯äº†å…¶ç›¸å¯¹äºå…ˆå‰GATéªŒè¯å™¨çš„ä¼˜åŠ¿ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¡«è¡¥äº†é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬åˆ°SQLé¢†åŸŸçš„ç©ºç™½ï¼Œè¯æ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­SQLç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒGATæ ¡æ­£å™¨æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºé˜¿æ‹‰ä¼¯è¯­ç‰¹æ€§ï¼Œä¸ºå¤šè¯­è¨€è‡ªç„¶è¯­è¨€æ•°æ®åº“äº¤äº’ç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
In recent years, the task of cross-domain, context-dependent text-to-SQL has received significant attention. Enables users with no prior knowledge of SQL to have a conversation with databases using natural language. However, most of the available datasets and research have been conducted in English, along with some work in Chinese. To this date, no effort has been made to address this task in the Arabic language. In this paper, we introduce Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of interrelated questions, each sequence containing an average of approximately three questions, which results in a total of 10225 questions along with their corresponding SQL queries. We conducted 40 experiments on the Ar-SParC dataset using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques, including four question representation methods and six in-context learning techniques. Furthermore, we developed a novel approach named GAT corrector, which enhanced the performance across all 40 experiments, yielding an average improvement of 1.9% in execution accuracy (EX) and 1.9% in interaction accuracy (IX) under zero-shot settings, and an average increase of 1.72% EX and 0.92% IX under in-context learning settings. Finally, we conducted an ablation study with two more experiments to explain why the GAT corrector outperformed the previous GAT verifier technique, particularly for the Arabic language.


### [46] [Winning with Less for Low Resource Languages: Advantage of Cross-Lingual English_Persian Argument Mining Model over LLM Augmentation](https://arxiv.org/abs/2511.20872)
*Ali Jahan, Masood Ghayoomi, Annette Hautli-Janisz*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºä¸€ç§è·¨è¯­è¨€è®ºè¾©æŒ–æ˜æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºä¸‰ç§è®­ç»ƒåœºæ™¯æ¥è§£å†³ä½èµ„æºè¯­è¨€çš„è®ºè¾©æŒ–æ˜é—®é¢˜ï¼Œå®éªŒè¡¨æ˜è½»é‡çº§è·¨è¯­è¨€æ··åˆæ¨¡å‹æ˜¾è‘—ä¼˜äºèµ„æºå¯†é›†å‹çš„å¢å¼ºæ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è®ºè¾©æŒ–æ˜ä½œä¸ºè‡ªç„¶è¯­è¨€å¤„ç†çš„å­é¢†åŸŸï¼Œæ—¨åœ¨è¯†åˆ«æ–‡æœ¬ä¸­çš„è®ºè¯ç»„ä»¶åŠå…¶å…³ç³»ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹é«˜èµ„æºè¯­è¨€ï¼Œä½èµ„æºè¯­è¨€é¢ä¸´æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢è·¨è¯­è¨€æ–¹æ³•åœ¨è®ºè¾©æŒ–æ˜ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«å…³æ³¨ä½èµ„æºè¯­è¨€å¦‚æ³¢æ–¯è¯­çš„æ•°æ®çŸ­ç¼ºé—®é¢˜ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†ä¸‰ç§è®­ç»ƒåœºæ™¯ï¼šé›¶æ ·æœ¬è¿ç§»ï¼ˆä»…ä½¿ç”¨è‹±è¯­æ•°æ®è®­ç»ƒï¼‰ã€åŸºäºå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆæ ·æœ¬çš„å¢å¼ºè®­ç»ƒã€ä»¥åŠç»“åˆåŸå§‹è‹±è¯­æ•°æ®å’Œäººå·¥ç¿»è¯‘æ³¢æ–¯è¯­å¥å­çš„è·¨è¯­è¨€æ¨¡å‹ã€‚è¯„ä¼°åŸºäºè‹±è¯­Microtextè¯­æ–™åº“åŠå…¶å¹³è¡Œæ³¢æ–¯è¯­ç¿»è¯‘è¿›è¡Œã€‚

**Result:** é›¶æ ·æœ¬è¿ç§»æ¨¡å‹åœ¨è‹±è¯­å’Œæ³¢æ–¯è¯­æµ‹è¯•é›†ä¸Šçš„F1åˆ†æ•°åˆ†åˆ«ä¸º50.2%å’Œ50.7%ã€‚LLMå¢å¼ºæ¨¡å‹å°†æ€§èƒ½æå‡è‡³è‹±è¯­59.2%å’Œæ³¢æ–¯è¯­69.3%ã€‚è·¨è¯­è¨€æ¨¡å‹åœ¨æ³¢æ–¯è¯­æµ‹è¯•é›†ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°74.8%çš„F1åˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜è½»é‡çº§è·¨è¯­è¨€æ··åˆæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—è¶…è¶Šèµ„æºå¯†é›†å‹çš„å¢å¼ºæµç¨‹ï¼Œä¸ºè®ºè¾©æŒ–æ˜ä»»åŠ¡åœ¨ä½èµ„æºè¯­è¨€ä¸Šå…‹æœæ•°æ®çŸ­ç¼ºé—®é¢˜æä¾›äº†å®ç”¨è·¯å¾„ã€‚è¿™ç§æ–¹æ³•å±•ç¤ºäº†è·¨è¯­è¨€è¿ç§»åœ¨è®ºè¾©æŒ–æ˜é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¼˜åŠ¿ã€‚

---

#### ğŸ“„ Abstract
Argument mining is a subfield of natural language processing to identify and extract the argument components, like premises and conclusions, within a text and to recognize the relations between them. It reveals the logical structure of texts to be used in tasks like knowledge extraction. This paper aims at utilizing a cross-lingual approach to argument mining for low-resource languages, by constructing three training scenarios. We examine the models on English, as a high-resource language, and Persian, as a low-resource language. To this end, we evaluate the models based on the English Microtext corpus \citep{PeldszusStede2015}, and its parallel Persian translation. The learning scenarios are as follow: (i) zero-shot transfer, where the model is trained solely with the English data, (ii) English-only training enhanced by synthetic examples generated by Large Language Models (LLMs), and (iii) a cross-lingual model that combines the original English data with manually translated Persian sentences. The zero-shot transfer model attains F1 scores of 50.2\% on the English test set and 50.7\% on the Persian test set. LLM-based augmentation model improves the performance up to 59.2\% on English and 69.3\% on Persian. The cross-lingual model, trained on both languages but evaluated solely on the Persian test set, surpasses the LLM-based variant, by achieving a F1 of 74.8\%. Results indicate that a lightweight cross-lingual blend can outperform considerably the more resource-intensive augmentation pipelines, and it offers a practical pathway for the argument mining task to overcome data resource shortage on low-resource languages.


### [47] [Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels](https://arxiv.org/abs/2511.21038)
*Anantha Padmanaban Krishna Kumar*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡å¯¹æ¯”è‡ªç„¶æ¼”ç¤ºå’Œåè½¬æ¼”ç¤ºå®éªŒï¼Œå‘ç°ä¸Šä¸‹æ–‡å­¦ä¹ ä¸»è¦è°ƒæ•´è¾“å…¥åœ¨é¢„è®­ç»ƒè¯­ä¹‰æ–¹å‘ä¸Šçš„æŠ•å½±ï¼Œè€Œéé‡æ–°æ˜ å°„æ ‡ç­¾å«ä¹‰ï¼Œæ”¯æŒäº†è¯­ä¹‰é”šå®šè§‚ç‚¹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ä¸Šä¸‹æ–‡å­¦ä¹ æ˜¯å¦èƒ½å¤Ÿè¦†ç›–é¢„è®­ç»ƒçš„æ ‡ç­¾è¯­ä¹‰ï¼Œè¿˜æ˜¯ä»…ä»…åœ¨ç°æœ‰è¯­ä¹‰åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒçš„æ ¸å¿ƒé—®é¢˜ï¼Œæ¢ç´¢LLMsåœ¨æç¤ºå­¦ä¹ ä¸­çš„è¯­ä¹‰å¤„ç†æœºåˆ¶ã€‚

**Method:** å°†LLMsè§†ä¸ºæç¤ºè¯±å¯¼åˆ†ç±»å™¨ï¼Œé€šè¿‡å¯¹æ¯”è‡ªç„¶æ¼”ç¤ºå’Œåè½¬æ¼”ç¤ºå®éªŒï¼Œå¼•å…¥ä¸‰ä¸ªå¯¹é½åº¦é‡æŒ‡æ ‡å’Œè¯­ä¹‰è¦†ç›–ç‡ï¼Œåœ¨å…«ä¸ªåˆ†ç±»ä»»åŠ¡å’Œå…«ä¸ªå¼€æºLLMsä¸Šè¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨è‡ªç„¶æ¼”ç¤ºä¸‹ICLæé«˜å‡†ç¡®ç‡åŒæ—¶ä¿æŒå¼ºå…ˆéªŒå¯¹é½ï¼›åœ¨åè½¬æ¼”ç¤ºä¸‹æ¨¡å‹æ— æ³•å­¦ä¹ è¿è´¯çš„åè¯­ä¹‰åˆ†ç±»å™¨ï¼Œè¯­ä¹‰è¦†ç›–ç‡åœ¨1-12Bå‚æ•°è§„æ¨¡ä¸‹ä¿æŒä¸ºé›¶ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†ä¸Šä¸‹æ–‡å­¦ä¹ ä¸»è¦è°ƒæ•´è¾“å…¥åœ¨ç¨³å®šè¯­ä¹‰æ–¹å‘ä¸Šçš„æŠ•å½±ï¼Œè€Œéçµæ´»é‡æ˜ å°„æ ‡ç­¾å«ä¹‰ï¼Œé˜æ˜äº†å°‘æ ·æœ¬æç¤ºçš„åŸºæœ¬é™åˆ¶ï¼Œè¡¨æ˜è¦†ç›–æ ‡ç­¾è¯­ä¹‰éœ€è¦è¶…è¶ŠICLçš„å¹²é¢„æªæ–½ã€‚

---

#### ğŸ“„ Abstract
Can in-context learning (ICL) override pre-trained label semantics, or does it merely refine an existing semantic backbone? We address this question by treating LLMs as prompt-induced classifiers and contrasting their behavior under \emph{natural} demonstrations (with correct labels) and \emph{inverted} demonstrations (systematically flipping label meanings). We decompose ICL behavior into three alignment metrics (truth, prior, and prompt alignment) and introduce a semantic override rate, defined as correctness under flipped semantics. Across eight classification tasks and eight open-source LLMs (1--12B parameters), we find consistent evidence for a semantic anchor view. With natural demonstrations, ICL improves accuracy while maintaining strong prior alignment; most correct predictions coincide with zero-shot behavior, even when the prior is weak. With inverted demonstrations, models cannot learn coherent anti-semantic classifiers: prompt alignment increases only by sacrificing accuracy, and semantic override rates remain exactly zero in our few-shot 1--12B setting. Rather than flexibly remapping label meanings, ICL primarily adjusts how inputs project onto stable semantic directions learned during pre-training, clarifying fundamental limits of few-shot prompting and suggesting that overriding label semantics at these scales requires interventions beyond ICL. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/semantic-anchors-icl.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [48] [AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI](https://arxiv.org/abs/2511.20686)
*Chae-Gyun Lim, Seung-Ho Han, EunYoung Byun, Jeongyun Han, Soohyun Cho, Eojin Joo, Heehyeon Kim, Sieun Kim, Juhoon Lee, Hyunsoo Lee, Dongkun Lee, Jonghwan Hyeon, Yechan Hwang, Young-Jun Lee, Kyeongryul Lee, Minhyeong An, Hyunjun Ahn, Jeongwoo Son, Junho Park, Donggyu Yoon, Taehyung Kim, Jeemin Kim, Dasom Choi, Kwangyoung Lee, Hyunseung Lim, Yeohyun Jung, Jongok Hong, Sooyohn Nam, Joonyoung Park, Sungmin Na, Yubin Choi, Jeanne Choi, Yoojin Hong, Sueun Jang, Youngseok Seo, Somin Park, Seoungung Jo, Wonhye Chae, Yeeun Jo, Eunyoung Kim, Joyce Jiyoung Whang, HwaJung Hong, Joseph Seering, Uichin Lee, Juho Kim, Sunna Choi, Seokyeon Ko, Taeho Kim, Kyunghoon Kim, Myungsik Ha, So Jung Lee, Jemin Hwang, JoonHo Kwak, Ho-Jin Choi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†AssurAIï¼Œä¸€ä¸ªç»è¿‡è´¨é‡æ§åˆ¶çš„éŸ©è¯­å¤šæ¨¡æ€æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆå¼AIçš„å®‰å…¨æ€§ã€‚è¯¥æ•°æ®é›†åŒ…å«11,480ä¸ªå®ä¾‹ï¼Œæ¶µç›–æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘å››ç§æ¨¡æ€ï¼Œä¸“é—¨é’ˆå¯¹éŸ©è¯­ç¤¾ä¼šæ–‡åŒ–èƒŒæ™¯ä¸‹çš„AIé£é™©å› ç´ è¿›è¡Œè¯„ä¼°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰çš„å®‰å…¨è¯„ä¼°æ•°æ®é›†ä¸»è¦é›†ä¸­äºè‹±è¯­ç¯å¢ƒï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰éè‹±è¯­ç¤¾ä¼šæ–‡åŒ–èƒŒæ™¯ï¼ˆå¦‚éŸ©è¯­ï¼‰ä¸­çš„ç‰¹å®šé£é™©ï¼Œä¸”é€šå¸¸ä»…é™äºæ–‡æœ¬æ¨¡æ€ã€‚è¿™ç§å±€é™æ€§é˜»ç¢äº†ç”Ÿæˆå¼AIåœ¨å¤šå…ƒæ–‡åŒ–ç¯å¢ƒä¸­çš„å®‰å…¨æ€§å’Œå¯é æ€§å‘å±•ã€‚

**Method:** ç ”ç©¶é¦–å…ˆé€šè¿‡å¤šå­¦ç§‘ä¸“å®¶ç»„å®šä¹‰äº†35ä¸ªä¸åŒçš„AIé£é™©å› ç´ åˆ†ç±»ä½“ç³»ï¼Œæ¶µç›–æ™®éå±å®³å’ŒéŸ©è¯­ç¤¾ä¼šæ–‡åŒ–ç›¸å…³æ€§ã€‚ç„¶åé‡‡ç”¨ä¸¤é˜¶æ®µæ„å»ºæ–¹æ³•ï¼ˆä¸“å®¶å¼•å¯¼çš„ç§å­é˜¶æ®µå’Œä¼—åŒ…æ‰©å±•é˜¶æ®µï¼‰ï¼Œç»“åˆä¸‰é‡ç‹¬ç«‹æ ‡æ³¨å’Œè¿­ä»£å¼ä¸“å®¶çº¢é˜Ÿæµ‹è¯•å¾ªç¯ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§ã€‚

**Result:** æ„å»ºçš„AssurAIæ•°æ®é›†åŒ…å«11,480ä¸ªå¤šæ¨¡æ€å®ä¾‹ï¼Œè¦†ç›–æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘å››ç§å½¢å¼ã€‚åˆæ­¥ç ”ç©¶éªŒè¯äº†è¯¥æ•°æ®é›†åœ¨è¯„ä¼°æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºéŸ©è¯­ç¤¾åŒºçš„AIå®‰å…¨è¯„ä¼°æä¾›äº†å¯é åŸºå‡†ã€‚

**Conclusion:** AssurAIæ•°æ®é›†å¡«è¡¥äº†éè‹±è¯­å¤šæ¨¡æ€AIå®‰å…¨è¯„ä¼°çš„ç©ºç™½ï¼Œä¿ƒè¿›äº†æ›´å®‰å…¨å¯é çš„ç”Ÿæˆå¼AIç³»ç»Ÿåœ¨éŸ©è¯­ç¤¾åŒºçš„å‘å±•ã€‚è¯¥å·¥ä½œçš„å¤šå­¦ç§‘æ–¹æ³•ã€ä¸¥æ ¼è´¨é‡æ§åˆ¶å’Œé’ˆå¯¹æ€§é£é™©åˆ†ç±»ä¸ºå…¶ä»–è¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯çš„AIå®‰å…¨è¯„ä¼°æä¾›äº†å¯å€Ÿé‰´çš„æ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
The rapid evolution of generative AI necessitates robust safety evaluations. However, current safety datasets are predominantly English-centric, failing to capture specific risks in non-English, socio-cultural contexts such as Korean, and are often limited to the text modality. To address this gap, we introduce AssurAI, a new quality-controlled Korean multimodal dataset for evaluating the safety of generative AI. First, we define a taxonomy of 35 distinct AI risk factors, adapted from established frameworks by a multidisciplinary expert group to cover both universal harms and relevance to the Korean socio-cultural context. Second, leveraging this taxonomy, we construct and release AssurAI, a large-scale Korean multimodal dataset comprising 11,480 instances across text, image, video, and audio. Third, we apply the rigorous quality control process used to ensure data integrity, featuring a two-phase construction (i.e., expert-led seeding and crowdsourced scaling), triple independent annotation, and an iterative expert red-teaming loop. Our pilot study validates AssurAI's effectiveness in assessing the safety of recent LLMs. We release AssurAI to the public to facilitate the development of safer and more reliable generative AI systems for the Korean community.


### [49] [Cross Domain Evaluation of Multimodal Chain-of-Thought Reasoning of different datasets into the Amazon CoT Framework](https://arxiv.org/abs/2511.20701)
*Nitya Tiwari, Parv Maheshwari, Vidisha Agarwal*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶å¯¹å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†è¿›è¡Œäº†è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›çš„ç»¼åˆåˆ†æï¼Œå‘ç°è™½ç„¶è§†è§‰ç‰¹å¾æ•´åˆèƒ½æ˜¾è‘—å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å¹»è§‰ç”Ÿæˆï¼Œä½†æ€ç»´é“¾æ¨ç†çš„æœ‰æ•ˆæ€§åœ¨ä¸åŒé—®é¢˜ç±»å‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¸¸è¯†æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å·²æœ‰å·¥ä½œå°†æ€ç»´é“¾æ‰©å±•åˆ°å¤šæ¨¡æ€åœºæ™¯å¹¶åœ¨ç§‘å­¦é—®ç­”åŸºå‡†ä¸Šå–å¾—äº†å…ˆè¿›ç»“æœï¼Œä½†è¿™äº›æ–¹æ³•åœ¨å¤šæ ·åŒ–é¢†åŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†åœ¨éœ€è¦å¹¿æ³›å¸¸è¯†å’Œä¸–ç•ŒçŸ¥è¯†çš„A-OKVQAã€OKVQAå’ŒChartQAæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

**Method:** æˆ‘ä»¬å®ç°äº†Zhangç­‰äººæå‡ºçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ¨ç†è¿‡ç¨‹ç”Ÿæˆä¸ç­”æ¡ˆæ¨æ–­åˆ†ç¦»ï¼Œå¹¶é€šè¿‡é—¨æ§èåˆæœºåˆ¶å°†è§†è§‰ç‰¹å¾ä¸åŸºäºT5çš„è¯­è¨€æ¨¡å‹é›†æˆã€‚é€šè¿‡ç³»ç»Ÿæ¶ˆèç ”ç©¶åˆ†æäº†è§†è§‰ç‰¹å¾ã€æ¨ç†è´¨é‡å’Œæ¶æ„é€‰æ‹©çš„è´¡çŒ®ã€‚

**Result:** ç ”ç©¶å‘ç°è§†è§‰ç‰¹å¾æ•´åˆæ˜¾è‘—å‡å°‘äº†æ¨ç†ç”Ÿæˆä¸­çš„å¹»è§‰ç°è±¡ï¼Œä½†æ€ç»´é“¾æ¨ç†çš„æœ‰æ•ˆæ€§åœ¨ä¸åŒé—®é¢˜ç±»å‹é—´å·®å¼‚å¾ˆå¤§ï¼Œå…¶ä¸­å¸¸è¯†æ¨ç†ä»»åŠ¡é¢ä¸´ç‰¹åˆ«æŒ‘æˆ˜ã€‚å®éªŒåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** è¿™é¡¹å·¥ä½œä¸ºç ”ç©¶äººå‘˜å®ç°å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæä¾›äº†å®ç”¨è§è§£ï¼Œå¹¶ç¡®å®šäº†è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›æ”¹è¿›çš„å…³é”®æ–¹å‘ï¼Œå¼ºè°ƒäº†é’ˆå¯¹ä¸åŒé—®é¢˜ç±»å‹å®šåˆ¶æ¨ç†ç­–ç•¥çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [3], which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.


### [50] [OpenApps: Simulating Environment Variations to Measure UI-Agent Reliability](https://arxiv.org/abs/2511.20766)
*Karen Ullrich, Jingtong Su, Claudia Shi, Arjun Subramonian, Amir Bar, Ivan Evtimov, Nikolaos Tsilivis, Randall Balestriero, Julia Kempe, Mark Ibrahim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†OpenAppsï¼Œä¸€ä¸ªè½»é‡çº§å¼€æºç”Ÿæ€ç³»ç»Ÿï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€UIä»£ç†åœ¨ä¸åŒåº”ç”¨å˜ä½“ä¸­çš„å¯é æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡åœ¨å›ºå®šåº”ç”¨ç¯å¢ƒä¸­ä»£ç†æ€§èƒ½ç›¸å¯¹ç¨³å®šï¼Œä½†åœ¨åº”ç”¨å˜ä½“ä¸­å¯é æ€§æ³¢åŠ¨æ˜¾è‘—ï¼Œä»»åŠ¡æˆåŠŸç‡å¯å˜åŒ–è¶…è¿‡50%ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è‡ªä¸»UIä»£ç†çš„è¯„ä¼°ä¾èµ–äºå›ºå®šç¯å¢ƒï¼Œé€šå¸¸æ˜¯ç°æœ‰åº”ç”¨çš„å…‹éš†ç‰ˆæœ¬ï¼Œè¿™ç§æ–¹æ³•åªèƒ½æ­ç¤ºä»£ç†åœ¨ç‰¹å®šç¯å¢ƒä¸­å®Œæˆä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œå®é™…éƒ¨ç½²æ—¶ï¼Œä»£ç†å¯èƒ½é‡åˆ°åº”ç”¨è®¾è®¡å’Œå†…å®¹çš„å˜åŒ–ï¼Œè¿™äº›å˜åŒ–ä¼šå½±å“ä»£ç†å®Œæˆä»»åŠ¡çš„èƒ½åŠ›ï¼Œç°æœ‰è¯„ä¼°æ–¹æ³•æ— æ³•æœ‰æ•ˆè¡¡é‡ä»£ç†åœ¨ä¸åŒåº”ç”¨å˜ä½“ä¸­çš„å¯é æ€§ã€‚

**Method:** å¼€å‘äº†OpenAppsè½»é‡çº§å¼€æºç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…å«å…­ä¸ªå¯é…ç½®åº”ç”¨ï¼ˆæ¶ˆæ¯ã€æ—¥å†ã€åœ°å›¾ç­‰ï¼‰ï¼Œåœ¨å†…å®¹å’Œå¤–è§‚ä¸Šå‡å¯é…ç½®ã€‚è¯¥ç³»ç»Ÿä»…éœ€å•ä¸ªCPUå³å¯è¿è¡Œï¼Œèƒ½å¤Ÿè½»æ¾ç”Ÿæˆå’Œéƒ¨ç½²æ•°åƒä¸ªæ¯ä¸ªåº”ç”¨çš„ä¸åŒç‰ˆæœ¬ï¼Œå¹¶è¿›è¡Œäº†è¶…è¿‡10,000æ¬¡ç‹¬ç«‹è¯„ä¼°æ¥ç ”ç©¶ä¸ƒä¸ªé¢†å…ˆå¤šæ¨¡æ€ä»£ç†çš„å¯é æ€§ã€‚

**Result:** ç ”ç©¶å‘ç°ï¼Œåœ¨å›ºå®šåº”ç”¨ç¯å¢ƒä¸­æ ‡å‡†å¯é æ€§ç›¸å¯¹ç¨³å®šï¼Œä½†åœ¨åº”ç”¨å˜ä½“ä¸­æµ‹é‡çš„å¯é æ€§æ³¢åŠ¨æ˜¾è‘—ã€‚è®¸å¤šä»£ç†çš„ä»»åŠ¡æˆåŠŸç‡åœ¨ä¸åŒåº”ç”¨å˜ä½“ä¸­æ³¢åŠ¨è¶…è¿‡50%ï¼Œä¾‹å¦‚Kimi-VL-3Båœ¨æ‰€æœ‰ä»»åŠ¡ä¸­çš„å¹³å‡æˆåŠŸç‡ä»63%æ³¢åŠ¨è‡³ä»…4%ã€‚ä»£ç†è¡Œä¸ºå¦‚å¾ªç¯æˆ–å¹»è§‰åŠ¨ä½œä¹Ÿå› ç¯å¢ƒé…ç½®ä¸åŒè€Œå·®å¼‚å·¨å¤§ã€‚

**Conclusion:** è¿™äº›åˆæ­¥å‘ç°å¼ºè°ƒäº†åœ¨åº”ç”¨å˜ä½“è¿™ä¸€æ–°ç»´åº¦ä¸Šæµ‹é‡å¯é æ€§çš„é‡è¦æ€§ã€‚OpenAppsç”Ÿæ€ç³»ç»Ÿä¸ºè¯„ä¼°UIä»£ç†åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­çš„é²æ£’æ€§æä¾›äº†æœ‰æ•ˆå·¥å…·ï¼Œæ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€ä»£ç†åœ¨é¢å¯¹åº”ç”¨è®¾è®¡å’Œå†…å®¹å˜åŒ–æ—¶çš„è„†å¼±æ€§ï¼Œä¸ºæœªæ¥ä»£ç†å¯é æ€§ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Reliability is key to realizing the promise of autonomous UI-Agents, multimodal agents that directly interact with apps in the same manner as humans, as users must be able to trust an agent to complete a given task. Current evaluations rely on fixed environments, often clones of existing apps, which are limited in that they can only shed light on whether or how often an agent can complete a task within a specific environment. When deployed however, agents are likely to encounter variations in app design and content that can affect an agent's ability to complete a task. To address this blind spot of measuring agent reliability across app variations, we develop OpenApps, a light-weight open-source ecosystem with six apps (messenger, calendar, maps, etc.) that are configurable in appearance and content. OpenApps requires just a single CPU to run, enabling easy generation and deployment of thousands of versions of each app. Specifically, we run more than 10,000 independent evaluations to study reliability across seven leading multimodal agents. We find that while standard reliability within a fixed app is relatively stable, reliability can vary drastically when measured across app variations. Task success rates for many agents can fluctuate by more than $50\%$ across app variations. For example, Kimi-VL-3B's average success across all tasks fluctuates from $63\%$ to just $4\%$ across app versions. We also find agent behaviors such as looping or hallucinating actions can differ drastically depending on the environment configuration. These initial findings highlight the importance of measuring reliability along this new dimension of app variations. OpenApps is available at https://facebookresearch.github.io/OpenApps/


### [51] [ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction](https://arxiv.org/abs/2511.20937)
*Qineng Wang, Wenlong Huang, Yu Zhou, Hang Yin, Tianwei Bao, Jianwen Lyu, Weiyu Liu, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, Manling Li*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºENACTåŸºå‡†æµ‹è¯•ï¼Œå°†å…·èº«è®¤çŸ¥è¯„ä¼°è½¬åŒ–ä¸ºä»¥è§†è§‰é—®ç­”å½¢å¼è¿›è¡Œçš„è‡ªæˆ‘ä¸­å¿ƒäº¤äº’ä¸–ç•Œå»ºæ¨¡ï¼Œæ­ç¤ºäº†å‰æ²¿è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å…·èº«è®¤çŸ¥èƒ½åŠ›æ–¹é¢ä¸äººç±»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å…·èº«è®¤çŸ¥ç†è®ºè®¤ä¸ºæ™ºèƒ½æºäºæ„ŸçŸ¥è¿åŠ¨äº¤äº’è€Œéè¢«åŠ¨è§‚å¯Ÿï¼Œä½†ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ä¸»è¦é‡‡ç”¨éå…·èº«æ–¹å¼è®­ç»ƒï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢è¿™äº›æ¨¡å‹æ˜¯å¦å±•ç°å‡ºå…·èº«è®¤çŸ¥çš„è¿¹è±¡ï¼Œå¡«è¡¥äº†ç°æœ‰è¯„ä¼°æ–¹æ³•åœ¨å…·èº«è®¤çŸ¥èƒ½åŠ›æµ‹è¯•æ–¹é¢çš„ç©ºç™½ã€‚

**Method:** æå‡ºENACTåŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œå°†å…¶æ„å»ºä¸ºéƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ŒåŠ¨ä½œå®šä¹‰ä¸ºåœºæ™¯å›¾å˜åŒ–ï¼ŒåŒ…å«ä¸¤ä¸ªäº’è¡¥çš„åºåˆ—é‡æ’åºä»»åŠ¡ï¼šå‰å‘ä¸–ç•Œå»ºæ¨¡ï¼ˆç»™å®šåŠ¨ä½œé‡æ’æ‰“ä¹±çš„è§‚æµ‹ï¼‰å’Œé€†å‘ä¸–ç•Œå»ºæ¨¡ï¼ˆç»™å®šè§‚æµ‹é‡æ’æ‰“ä¹±çš„åŠ¨ä½œï¼‰ï¼Œé€šè¿‡æœºå™¨äººä»¿çœŸå¹³å°BEHAVIORç”Ÿæˆ8,972ä¸ªé—®ç­”å¯¹ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å®¶åº­è§„æ¨¡é•¿æ—¶ç¨‹æ´»åŠ¨ä¸­çš„è¡¨ç°ã€‚

**Result:** å®éªŒæ˜¾ç¤ºå‰æ²¿è§†è§‰è¯­è¨€æ¨¡å‹ä¸äººç±»ä¹‹é—´å­˜åœ¨æ€§èƒ½å·®è·ï¼Œä¸”éšç€äº¤äº’æ—¶é•¿çš„å¢åŠ å·®è·æ‰©å¤§ï¼Œæ¨¡å‹åœ¨é€†å‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå‰å‘ä»»åŠ¡ï¼Œå¹¶è¡¨ç°å‡ºä»¥äººç±»ä¸ºä¸­å¿ƒçš„åè§ï¼ŒåŒ…æ‹¬åå¥½å³æ‰‹åŠ¨ä½œä»¥åŠåœ¨ç›¸æœºå†…å‚æˆ–è§†è§’åç¦»äººç±»è§†è§‰æ—¶æ€§èƒ½ä¸‹é™ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å…·èº«è®¤çŸ¥èƒ½åŠ›æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§ä¸è¶³ï¼Œæ­ç¤ºäº†æ¨¡å‹å¯¹äººç±»ä¸­å¿ƒåè§çš„æ•æ„Ÿæ€§ï¼Œä¸ºå¼€å‘æ›´é²æ£’çš„å…·èº«æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦åŸºå‡†å’Œæ–¹å‘æŒ‡å¯¼ï¼Œå¼ºè°ƒäº†åœ¨éå…·èº«è®­ç»ƒèŒƒå¼ä¸‹æ¨¡å‹å†…åœ¨è®¤çŸ¥å±€é™çš„è¯„ä¼°å¿…è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.


### [52] [OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection](https://arxiv.org/abs/2511.21064)
*Chujie Wang, Jianyu Lu, Zhiyuan Luo, Xi Chen, Chu He*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºOVOD-Agentæ¡†æ¶ï¼Œå°†å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹ä»è¢«åŠ¨ç±»åˆ«åŒ¹é…è½¬å˜ä¸ºä¸»åŠ¨è§†è§‰æ¨ç†å’Œè‡ªæˆ‘æ¼”åŒ–æ£€æµ‹ï¼Œé€šè¿‡è§†è§‰æ€ç»´é“¾å’Œå¼±é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ˜¾è‘—æå‡äº†æ£€æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹æ–¹æ³•è™½ç„¶åœ¨å¤šæ¨¡æ€æ•°æ®é›†ä¸Šé¢„è®­ç»ƒï¼Œä½†æ¨ç†ä»å—é™äºå›ºå®šç±»åˆ«åç§°ï¼Œå¯¼è‡´å¤šæ¨¡æ€è®­ç»ƒä¸å•æ¨¡æ€æ¨ç†ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œä¸”æ–‡æœ¬è¡¨ç¤ºç©ºé—´å°šæœªå……åˆ†æ¢ç´¢ã€‚

**Method:** æå‡ºOVOD-Agentæ¡†æ¶ï¼Œé‡‡ç”¨è§†è§‰æ€ç»´é“¾èŒƒå¼å°†æ–‡æœ¬ä¼˜åŒ–è¿‡ç¨‹æ‰©å±•ä¸ºå¯è§£é‡Šçš„è§†è§‰æ¨ç†ï¼ŒåŸºäºå¼±é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å»ºæ¨¡è§†è§‰ä¸Šä¸‹æ–‡è½¬æ¢ï¼Œé€šè¿‡Banditæ¨¡å—ç”Ÿæˆæ¢ç´¢ä¿¡å·å¹¶é›†æˆé©¬å°”å¯å¤«è½¬ç§»çŸ©é˜µä¸Banditè½¨è¿¹è¿›è¡Œè‡ªç›‘ç£å¥–åŠ±æ¨¡å‹ä¼˜åŒ–ã€‚

**Result:** åœ¨COCOå’ŒLVISæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒOVOD-Agentåœ¨å„ç§OVODéª¨å¹²ç½‘ç»œä¸Šå‡èƒ½æä¾›ä¸€è‡´æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€æœ‰ç±»åˆ«ä¸Šè¡¨ç°å°¤ä¸ºæ˜¾è‘—ï¼ŒéªŒè¯äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å°†è¢«åŠ¨æ£€æµ‹è½¬å˜ä¸ºä¸»åŠ¨æ¨ç†èŒƒå¼çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡è§†è§‰æ€ç»´é“¾å’Œå¼±é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸ºå¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç¨€æœ‰ç±»åˆ«æ—¶å±•ç°å‡ºå¼ºå¤§æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD's lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent's state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.


### [53] [EWE: An Agentic Framework for Extreme Weather Analysis](https://arxiv.org/abs/2511.21444)
*Zhe Jiang, Jiong Wang, Xiaoyu Yue, Zijie Guo, Wenlong Zhang, Fenghua Ling, Wanli Ouyang, Lei Bai*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªæç«¯å¤©æ°”æ™ºèƒ½è¯Šæ–­ä»£ç†æ¡†æ¶EWEï¼Œé€šè¿‡çŸ¥è¯†å¼•å¯¼çš„è§„åˆ’ã€é—­ç¯æ¨ç†å’Œæ°”è±¡ä¸“ç”¨å·¥å…·åŒ…ï¼Œå®ç°äº†ä»åŸå§‹æ°”è±¡æ•°æ®åˆ°å¤šæ¨¡æ€å¯è§†åŒ–çš„è‡ªåŠ¨è¯Šæ–­åˆ†æï¼Œä¸ºè§£å†³æç«¯å¤©æ°”ç‰©ç†æœºåˆ¶åˆ†æçš„è‡ªåŠ¨åŒ–ç“¶é¢ˆæä¾›äº†è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æç«¯å¤©æ°”äº‹ä»¶å¯¹å…¨çƒç¤¾ä¼šæ„æˆæ—¥ç›Šä¸¥é‡çš„é£é™©ï¼Œä½†å½“å‰ä¸“å®¶é©±åŠ¨ã€åŠ³åŠ¨å¯†é›†çš„è¯Šæ–­èŒƒå¼é€ æˆäº†å…³é”®çš„åˆ†æç“¶é¢ˆï¼Œé˜»ç¢äº†ç§‘å­¦è¿›å±•ã€‚è™½ç„¶AIåœ¨åœ°çƒç§‘å­¦é¢„æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†åŒæ ·é‡è¦çš„è‡ªåŠ¨è¯Šæ–­æ¨ç†æŒ‘æˆ˜ä»åŸºæœ¬æœªè¢«æ¢ç´¢ã€‚

**Method:** EWEæ¡†æ¶é€šè¿‡çŸ¥è¯†å¼•å¯¼çš„è§„åˆ’æ¨¡æ‹Ÿä¸“å®¶å·¥ä½œæµç¨‹ï¼Œé‡‡ç”¨é—­ç¯æ¨ç†æœºåˆ¶å’Œä¸“é—¨å®šåˆ¶çš„æ°”è±¡å·¥å…·åŒ…ï¼Œèƒ½å¤Ÿä»åŸå§‹æ°”è±¡æ•°æ®è‡ªä¸»ç”Ÿæˆå’Œè§£é‡Šå¤šæ¨¡æ€å¯è§†åŒ–ï¼Œå®ç°å…¨é¢çš„è¯Šæ–­åˆ†æã€‚

**Result:** ç ”ç©¶å¼•å…¥äº†è¯¥æ–°å…´é¢†åŸŸçš„é¦–ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«103ä¸ªé«˜å½±å“äº‹ä»¶çš„ç²¾é€‰æ•°æ®é›†å’Œæ–°å‹é€æ­¥è¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºæç«¯å¤©æ°”è‡ªåŠ¨è¯Šæ–­æä¾›äº†æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶ã€‚

**Conclusion:** EWEæ ‡å¿—ç€å‘è‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œå…·æœ‰æ°‘ä¸»åŒ–ä¸“ä¸šçŸ¥è¯†å’Œæ™ºåŠ›èµ„æºçš„æ½œåŠ›ï¼Œç‰¹åˆ«ä¸ºæ˜“å—æç«¯å¤©æ°”å½±å“çš„å‘å±•ä¸­å›½å®¶æä¾›äº†é‡è¦å·¥å…·ï¼Œæ¨åŠ¨äº†æç«¯å¤©æ°”ç‰©ç†æœºåˆ¶åˆ†æèŒƒå¼çš„è½¬å˜ã€‚

---

#### ğŸ“„ Abstract
Extreme weather events pose escalating risks to global society, underscoring the urgent need to unravel their underlying physical mechanisms. Yet the prevailing expert-driven, labor-intensive diagnostic paradigm has created a critical analytical bottleneck, stalling scientific progress. While AI for Earth Science has achieved notable advances in prediction, the equally essential challenge of automated diagnostic reasoning remains largely unexplored. We present the Extreme Weather Expert (EWE), the first intelligent agent framework dedicated to this task. EWE emulates expert workflows through knowledge-guided planning, closed-loop reasoning, and a domain-tailored meteorological toolkit. It autonomously produces and interprets multimodal visualizations from raw meteorological data, enabling comprehensive diagnostic analyses. To catalyze progress, we introduce the first benchmark for this emerging field, comprising a curated dataset of 103 high-impact events and a novel step-wise evaluation metric. EWE marks a step toward automated scientific discovery and offers the potential to democratize expertise and intellectual resources, particularly for developing nations vulnerable to extreme weather.


### [54] [SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition](https://arxiv.org/abs/2511.21471)
*Peiran Xu, Sudong Wang, Yao Zhu, Jianing Li, Yunjian Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ†å±‚ç©ºé—´è®¤çŸ¥æ¡†æ¶å’ŒSpatialBenchåŸºå‡†ï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨æ„ŸçŸ¥å±‚é¢è¡¨ç°è‰¯å¥½ä½†åœ¨ç¬¦å·æ¨ç†å’Œè§„åˆ’æ–¹é¢å­˜åœ¨å±€é™ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºå‡†é€šå¸¸å°†ç©ºé—´è®¤çŸ¥è¿‡åº¦ç®€åŒ–ä¸ºå•ä¸€ç»´åº¦æŒ‡æ ‡ï¼Œæ— æ³•æ•æ‰ç©ºé—´èƒ½åŠ›çš„å±‚æ¬¡ç»“æ„å’Œç›¸äº’ä¾èµ–å…³ç³»ï¼Œè¿™é™åˆ¶äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒäº¤äº’ä¸­çš„å‘å±•æ½œåŠ›ã€‚

**Method:** æå‡ºäº†åˆ†å±‚ç©ºé—´è®¤çŸ¥æ¡†æ¶ï¼Œå°†ç©ºé—´æ™ºèƒ½åˆ†è§£ä¸ºä»åŸºç¡€è§‚å¯Ÿåˆ°é«˜çº§è§„åˆ’çš„äº”ä¸ªæ¸è¿›å¤æ‚åº¦çº§åˆ«ï¼Œå¹¶æ„å»ºäº†SpatialBenchå¤§è§„æ¨¡ç»†ç²’åº¦åŸºå‡†ï¼ŒåŒ…å«ä¸è¿™äº›è®¤çŸ¥çº§åˆ«å¯¹é½çš„15ä¸ªä»»åŠ¡ï¼ŒåŒæ—¶å¼•å…¥äº†é«˜å±‚æ¬¡èƒ½åŠ›å¯¼å‘çš„ç»Ÿä¸€è¯„ä¼°æŒ‡æ ‡ã€‚

**Result:** å¤§è§„æ¨¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®éªŒæ˜¾ç¤ºä¸åŒè®¤çŸ¥çº§åˆ«å­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½åˆ†å±‚ï¼šæ¨¡å‹åœ¨æ„ŸçŸ¥åŸºç¡€æ–¹é¢è¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨ç¬¦å·æ¨ç†ã€å› æœæ¨æ–­å’Œè§„åˆ’æ–¹é¢ä»ç„¶å—é™ï¼›äººç±»æµ‹è¯•è¡¨æ˜äººç±»æ‰§è¡Œé€‰æ‹©æ€§ã€ç›®æ ‡å¯¼å‘çš„æŠ½è±¡ï¼Œè€Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å€¾å‘äºè¿‡åº¦å…³æ³¨è¡¨é¢ç»†èŠ‚è€Œç¼ºä¹è¿è´¯çš„ç©ºé—´æ„å›¾ã€‚

**Conclusion:** è¯¥ç ”ç©¶å»ºç«‹äº†é¦–ä¸ªç³»ç»ŸåŒ–æµ‹é‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­åˆ†å±‚ç©ºé—´è®¤çŸ¥çš„æ¡†æ¶ï¼Œä¸ºæœªæ¥ç©ºé—´æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨é«˜çº§ç©ºé—´æ¨ç†èƒ½åŠ›æ–¹é¢çš„å…³é”®å·®è·ã€‚

---

#### ğŸ“„ Abstract
Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability-oriented metric that reliably assesses a model's overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems.
