<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-29.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 25]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 9]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 10]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-explainable-detection-of-ai-generated-images-with-artifact-localization-using-faster-than-lies-and-vision-language-models-for-edge-devices">[1] <a href="https://arxiv.org/abs/2510.23775">Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices</a></h3>
<p><em>Aryan Mathur, Asaduddin Ahmed, Pushti Amit Vasoya, Simeon Kandan Sonar, Yasir Z, Madesh Kuppusamy</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„å›¾åƒçœŸå®æ€§æ£€æµ‹ç³»ç»Ÿï¼Œç»“åˆè½»é‡çº§å·ç§¯åˆ†ç±»å™¨å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨32Ã—32ä½åˆ†è¾¨ç‡å›¾åƒä¸­å®ç°96.5%çš„å‡†ç¡®ç‡æ£€æµ‹AIç”Ÿæˆå›¾åƒï¼ŒåŒæ—¶æä¾›ä¼ªå½±å®šä½å’Œæ–‡æœ¬è§£é‡Šã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€AIç”Ÿæˆå›¾åƒçš„çœŸå®æ„Ÿä¸æ–­æå‡ï¼ŒéªŒè¯è§†è§‰å†…å®¹çš„çœŸå®æ€§é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿåœ¨ä½åˆ†è¾¨ç‡æ¡ä»¶ä¸‹å‡†ç¡®æ£€æµ‹å¹¶è§£é‡Šå›¾åƒä¼ªé€ ç—•è¿¹çš„æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨è½»é‡çº§å·ç§¯åˆ†ç±»å™¨Faster-Than-Liesä¸è§†è§‰è¯­è¨€æ¨¡å‹Qwen2-VL-7Bç›¸ç»“åˆçš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªç¼–ç å™¨é‡æ„è¯¯å·®å›¾ç”Ÿæˆä¼ªå½±å®šä½çƒ­åŠ›å›¾ï¼Œå¹¶å°†70ç§è§†è§‰ä¼ªå½±ç±»å‹å½’ç±»ä¸ºå…«ä¸ªè¯­ä¹‰ç»„ä»¥å®ç°å¯è§£é‡Šçš„å¼‚å¸¸æ£€æµ‹ã€‚</p>
<p><strong>Result:</strong> åœ¨åŒ…å«å¯¹æŠ—æ€§æ‰°åŠ¨çš„æ‰©å±•CiFAKEæ•°æ®é›†ä¸Šè¾¾åˆ°96.5%çš„å‡†ç¡®ç‡ï¼Œåœ¨8æ ¸CPUä¸Šæ¨ç†æ—¶é—´ä¸º175æ¯«ç§’ï¼Œèƒ½å¤Ÿéƒ¨ç½²åœ¨æœ¬åœ°æˆ–è¾¹ç¼˜è®¾å¤‡ä¸Šï¼ŒåŒæ—¶ç”Ÿæˆä¼ªå½±å®šä½çƒ­åŠ›å›¾å’Œæ–‡æœ¬è§£é‡Šã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†è§†è§‰ä¸è¯­è¨€æ¨ç†ç›¸ç»“åˆåœ¨ä½åˆ†è¾¨ç‡å›¾åƒå¯è§£é‡ŠçœŸå®æ€§æ£€æµ‹ä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºå–è¯ã€å·¥ä¸šæ£€æµ‹å’Œç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ç­‰è·¨é¢†åŸŸåº”ç”¨æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>The increasing realism of AI-generated imagery poses challenges for verifying
visual authenticity. We present an explainable image authenticity detection
system that combines a lightweight convolutional classifier
("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify,
localize, and explain artifacts in 32x32 images. Our model achieves 96.5%
accuracy on the extended CiFAKE dataset augmented with adversarial
perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling
deployment on local or edge devices. Using autoencoder-based reconstruction
error maps, we generate artifact localization heatmaps, which enhance
interpretability for both humans and the VLM. We further categorize 70 visual
artifact types into eight semantic groups and demonstrate explainable text
generation for each detected anomaly. This work highlights the feasibility of
combining visual and linguistic reasoning for interpretable authenticity
detection in low-resolution imagery and outlines potential cross-domain
applications in forensics, industrial inspection, and social media moderation.</p>
<h3 id="2-countformer-a-transformer-framework-for-learning-visual-repetition-and-structure-in-class-agnostic-object-counting">[2] <a href="https://arxiv.org/abs/2510.23785">CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting</a></h3>
<p><em>Md Tanvir Hossain, Akif Islam, Mohd Ruhul Ameen</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCountFormerï¼Œä¸€ç§åŸºäºTransformerçš„ç±»æ— å…³ç‰©ä½“è®¡æ•°æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆè‡ªç›‘ç£åŸºç¡€æ¨¡å‹DINOv2æ¥è¯†åˆ«è§†è§‰é‡å¤å’Œç»“æ„ä¸€è‡´æ€§ï¼Œåœ¨FSC-147æ•°æ®é›†ä¸Šè¾¾åˆ°ä¸å½“å‰æœ€ä¼˜æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç»“æ„å¤æ‚åœºæ™¯ä¸­è¡¨ç°æ›´ä¼˜ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è®¡æ•°æ¨¡å‹éš¾ä»¥å¤åˆ¶äººç±»é€šè¿‡æ„ŸçŸ¥è§†è§‰é‡å¤å’Œç»“æ„å…³ç³»è€Œéç±»åˆ«èº«ä»½æ¥è®¡æ•°çš„èƒ½åŠ›ï¼Œåœ¨ç‰©ä½“å…·æœ‰å¤æ‚å½¢çŠ¶ã€å†…éƒ¨å¯¹ç§°æ€§æˆ–é‡å ç»„ä»¶æ—¶ç»å¸¸è®¡æ•°é”™è¯¯ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿè¯†åˆ«é‡å¤æ€§å’Œç»“æ„ä¸€è‡´æ€§çš„ç±»æ— å…³è®¡æ•°æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> åŸºäºCounTRæ¶æ„æ„å»ºCountFormerï¼Œä½¿ç”¨è‡ªç›‘ç£åŸºç¡€æ¨¡å‹DINOv2æ›¿æ¢è§†è§‰ç¼–ç å™¨ä»¥ç”Ÿæˆæ›´ä¸°å¯Œä¸”ç©ºé—´ä¸€è‡´çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶å¼•å…¥ä½ç½®åµŒå…¥èåˆæ¥ä¿æŒå‡ ä½•å…³ç³»ï¼Œæœ€åé€šè¿‡è½»é‡çº§å·ç§¯è§£ç å™¨å°†ç‰¹å¾è§£ç ä¸ºå¯†åº¦å›¾ã€‚</p>
<p><strong>Result:</strong> åœ¨FSC-147æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹æ€§èƒ½ä¸å½“å‰æœ€ä¼˜æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶åœ¨ç»“æ„å¤æ‚æˆ–å¯†é›†åœºæ™¯ä¸­å±•ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚ç»“æ„æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜æ•´åˆDINOv2ç­‰åŸºç¡€æ¨¡å‹èƒ½ä½¿è®¡æ•°ç³»ç»Ÿæ¥è¿‘äººç±»çš„ç»“æ„æ„ŸçŸ¥èƒ½åŠ›ï¼Œæ¨åŠ¨äº†çœŸæ­£é€šç”¨ä¸”æ— éœ€ç¤ºä¾‹çš„è®¡æ•°èŒƒå¼çš„å‘å±•ï¼Œä¸ºç±»æ— å…³ç‰©ä½“è®¡æ•°æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Humans can effortlessly count diverse objects by perceiving visual repetition
and structural relationships rather than relying on class identity. However,
most existing counting models fail to replicate this ability; they often
miscount when objects exhibit complex shapes, internal symmetry, or overlapping
components. In this work, we introduce CountFormer, a transformer-based
framework that learns to recognize repetition and structural coherence for
class-agnostic object counting. Built upon the CounTR architecture, our model
replaces its visual encoder with the self-supervised foundation model DINOv2,
which produces richer and spatially consistent feature representations. We
further incorporate positional embedding fusion to preserve geometric
relationships before decoding these features into density maps through a
lightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model
achieves performance comparable to current state-of-the-art methods while
demonstrating superior accuracy on structurally intricate or densely packed
scenes. Our findings indicate that integrating foundation models such as DINOv2
enables counting systems to approach human-like structural perception,
advancing toward a truly general and exemplar-free counting paradigm.</p>
<h3 id="3-improving-visual-discriminability-of-clip-for-training-free-open-vocabulary-semantic-segmentation">[3] <a href="https://arxiv.org/abs/2510.23894">Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation</a></h3>
<p><em>Jinxin Zhou, Jiachen Jiang, Zhihui Zhu</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†LHT-CLIPï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿåˆ©ç”¨CLIPåœ¨å±‚ã€å¤´å’Œä»¤ç‰Œçº§åˆ«çš„è§†è§‰åŒºåˆ†èƒ½åŠ›ï¼Œæœ‰æ•ˆè§£å†³äº†CLIPæ¨¡å‹æ‰©å±•åˆ°è¯­ä¹‰åˆ†å‰²æ—¶çš„å¯¹é½åå·®é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°†CLIPæ¨¡å‹æ‰©å±•åˆ°è¯­ä¹‰åˆ†å‰²é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦åŸå› æ˜¯å…¶å›¾åƒçº§é¢„è®­ç»ƒç›®æ ‡ä¸å¯†é›†é¢„æµ‹æ‰€éœ€çš„åƒç´ çº§è§†è§‰ç†è§£ä¹‹é—´å­˜åœ¨ä¸å¯¹é½ã€‚å…ˆå‰æ–¹æ³•è™½ç„¶é€šè¿‡é‡ç»„æœ€ç»ˆå±‚å’Œç‰¹å¾å–å¾—äº†é¼“èˆäººå¿ƒçš„ç»“æœï¼Œä½†å¾€å¾€ç»§æ‰¿äº†å‰å±‚çš„å…¨å±€å¯¹é½åå·®ï¼Œå¯¼è‡´åˆ†å‰²æ€§èƒ½ä¸ç†æƒ³ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸‰ç§äº’è¡¥æŠ€æœ¯ï¼šè¯­ä¹‰ç©ºé—´é‡åŠ æƒã€é€‰æ‹©æ€§å¤´å¢å¼ºå’Œå¼‚å¸¸ä»¤ç‰Œæ›¿æ¢ï¼Œè¿™äº›æ–¹æ³•åŸºäºå¯¹CLIPè§†è§‰åŒºåˆ†èƒ½åŠ›çš„å…¨é¢åˆ†æï¼ŒåŒ…æ‹¬å‘ç°æœ€ç»ˆå±‚ä¸»è¦å¼ºåŒ–å›¾åƒ-æ–‡æœ¬å¯¹é½ä½†ç‰ºç‰²è§†è§‰åŒºåˆ†èƒ½åŠ›ã€éƒ¨åˆ†æ³¨æ„åŠ›å¤´åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¸€è‡´çš„å¼ºè§†è§‰åŒºåˆ†èƒ½åŠ›ï¼Œä»¥åŠå¼‚å¸¸ä»¤ç‰Œç›¸æ¯”æ­£å¸¸ä»¤ç‰Œæ˜¾ç¤ºå‡ºç¨€ç–ä¸”ä¸€è‡´çš„æ¿€æ´»æ¨¡å¼ã€‚</p>
<p><strong>Result:</strong> åœ¨8ä¸ªå¸¸è§è¯­ä¹‰åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLHT-CLIPåœ¨å¤šæ ·åŒ–åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå®é™…éƒ¨ç½²çš„å®ç”¨æ€§ï¼Œä¸”æ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒã€è¾…åŠ©é¢„è®­ç»ƒç½‘ç»œæˆ–å¤§é‡è¶…å‚æ•°è°ƒä¼˜ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†CLIPæ¨¡å‹ä¸­è§†è§‰åŒºåˆ†èƒ½åŠ›çš„å…³é”®ç‰¹æ€§ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä¸ºCLIPåœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨æä¾›äº†é‡è¦è§è§£ï¼Œå±•ç¤ºäº†é€šè¿‡ç³»ç»Ÿåˆ†ææ¨¡å‹å†…éƒ¨æœºåˆ¶æ¥æå‡æ€§èƒ½çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Extending CLIP models to semantic segmentation remains challenging due to the
misalignment between their image-level pre-training objectives and the
pixel-level visual understanding required for dense prediction. While prior
efforts have achieved encouraging results by reorganizing the final layer and
features, they often inherit the global alignment bias of preceding layers,
leading to suboptimal segmentation performance. In this work, we propose
LHT-CLIP, a novel training-free framework that systematically exploits the
visual discriminability of CLIP across layer, head, and token levels. Through
comprehensive analysis, we reveal three key insights: (i) the final layers
primarily strengthen image-text alignment with sacrifice of visual
discriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),
partly due to the emergence of anomalous tokens; (ii) a subset of attention
heads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual
discriminability across datasets; (iii) abnormal tokens display sparse and
consistent activation pattern compared to normal tokens. Based on these
findings, we propose three complementary techniques: semantic-spatial
reweighting, selective head enhancement, and abnormal token replacement to
effectively restore visual discriminability and improve segmentation
performance without any additional training, auxiliary pre-trained networks, or
extensive hyperparameter tuning. Extensive experiments on 8 common semantic
segmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art
performance across diverse scenarios, highlighting its effectiveness and
practicality for real-world deployment.</p>
<h3 id="4-dynastride-dynamic-stride-windowing-with-mmcot-for-instructional-multi-scene-captioning">[4] <a href="https://arxiv.org/abs/2510.23907">DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning</a></h3>
<p><em>Eddison Pham, Prisha Priyadarshini, Adrian Maliackel, Kanishk Bandi, Cristian Meo, Kevin Zhu</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†DynaStrideç®¡é“ï¼Œç”¨äºåœ¨æ— éœ€æ‰‹åŠ¨åœºæ™¯åˆ†å‰²çš„æƒ…å†µä¸‹ç”Ÿæˆè¿è´¯çš„æ•™å­¦è§†é¢‘åœºæ™¯çº§å­—å¹•ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”å¸§é‡‡æ ·ã€å¤šæ¨¡æ€çª—å£å’ŒåŠ¨æ€æ­¥é•¿é€‰æ‹©ç®—æ³•ï¼Œæœ‰æ•ˆå¹³è¡¡æ—¶é—´ä¸Šä¸‹æ–‡ä¸å†—ä½™ï¼Œåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ•™å­¦è§†é¢‘ä¸­çš„åœºæ™¯çº§å­—å¹•éœ€è¦åŒæ—¶ç†è§£è§†è§‰çº¿ç´¢å’Œæ—¶é—´ç»“æ„ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆæ•æ‰è¿™ç§ç»“æ„ï¼Œå¯¼è‡´ç”Ÿæˆçš„å­—å¹•ç¼ºä¹è¿è´¯æ€§å’Œè´¨é‡ï¼Œä»è€Œå½±å“è§†é¢‘çš„æ•™è‚²æ„å›¾ã€‚å½“å‰ç¼ºä¹èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡åœºæ™¯çº§å­—å¹•ä¸”æ— éœ€æ‰‹åŠ¨åœºæ™¯åˆ†å‰²çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> DynaStrideé‡‡ç”¨è‡ªé€‚åº”å¸§é‡‡æ ·å’Œå¤šæ¨¡æ€çª—å£æŠ€æœ¯æ¥æ•æ‰åœºæ™¯å†…çš„å…³é”®è½¬æ¢ï¼Œé€šè¿‡å¤šæ¨¡æ€æ€ç»´é“¾è¿‡ç¨‹ç”Ÿæˆå¤šä¸ªåŠ¨ä½œ-å¯¹è±¡å¯¹ï¼Œå¹¶ä½¿ç”¨åŠ¨æ€æ­¥é•¿çª—å£é€‰æ‹©ç®—æ³•æ¥ä¼˜åŒ–å’Œèåˆè¿™äº›å¯¹ã€‚è¯¥æ–¹æ³•åœ¨YouCookIIæ•°æ®é›†ä¸Šåˆ©ç”¨åœºæ™¯æ ‡æ³¨ï¼Œæœ€ç»ˆå°†è§†è§‰è¯­ä¹‰å’Œæ—¶é—´æ¨ç†æ•´åˆåˆ°å•ä¸ªæ•™å­¦å­—å¹•ä¸­ã€‚</p>
<p><strong>Result:</strong> ä¸VLLaMA3å’ŒGPT-4oç­‰å¼ºåŸºçº¿ç›¸æ¯”ï¼ŒDynaStrideåœ¨N-gramæŒ‡æ ‡ï¼ˆBLEUã€METEORï¼‰å’Œè¯­ä¹‰ç›¸ä¼¼åº¦åº¦é‡ï¼ˆBERTScoreã€CLIPScoreï¼‰ä¸Šå‡å–å¾—ä¸€è‡´æå‡ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„å­—å¹•åœ¨æ—¶é—´è¿è´¯æ€§å’Œä¿¡æ¯ä¸°å¯Œåº¦æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</p>
<p><strong>Conclusion:</strong> DynaStrideå±•ç¤ºäº†é€šè¿‡è‡ªé€‚åº”æ—¶é—´å»ºæ¨¡å’Œå¤šæ¨¡æ€æ¨ç†æ¥æ”¹è¿›AIé©±åŠ¨çš„æ•™å­¦å†…å®¹ç”Ÿæˆçš„å¯è¡Œæ–¹å‘ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒå­—å¹•è¿è´¯æ€§çš„åŒæ—¶æœ‰æ•ˆå¹³è¡¡äº†æ—¶é—´ä¸Šä¸‹æ–‡ä¸å†—ä½™ï¼Œä¸ºæ•™å­¦è§†é¢‘çš„è‡ªåŠ¨å­—å¹•ç”Ÿæˆæä¾›äº†æœ‰å‰æ™¯çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Scene-level captioning in instructional videos can enhance learning by
requiring an understanding of both visual cues and temporal structure. By
aligning visual cues with textual guidance, this understanding supports
procedural learning and multimodal reasoning, providing a richer context for
skill acquisition. However, captions that fail to capture this structure may
lack coherence and quality, which can create confusion and undermine the
video's educational intent. To address this gap, we introduce DynaStride, a
pipeline to generate coherent, scene-level captions without requiring manual
scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride
performs adaptive frame sampling and multimodal windowing to capture key
transitions within each scene. It then employs a multimodal chain-of-thought
process to produce multiple action-object pairs, which are refined and fused
using a dynamic stride window selection algorithm that adaptively balances
temporal context and redundancy. The final scene-level caption integrates
visual semantics and temporal reasoning in a single instructional caption.
Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,
demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and
semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses
further show that DynaStride produces captions that are more temporally
coherent and informative, suggesting a promising direction for improving
AI-powered instructional content generation.</p>
<h3 id="5-planargs-high-fidelity-indoor-3d-gaussian-splatting-guided-by-vision-language-planar-priors">[5] <a href="https://arxiv.org/abs/2510.23930">PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors</a></h3>
<p><em>Xirui Jin, Renbiao Jin, Boying Li, Danping Zou, Wenxian Yu</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†PlanarGSï¼Œä¸€ç§é’ˆå¯¹å®¤å†…åœºæ™¯é‡å»ºçš„3Dé«˜æ–¯æ³¼æº…æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è¯­è¨€æç¤ºå¹³é¢å…ˆéªŒå’Œå‡ ä½•ç›‘ç£æ¥è§£å†³å¤§èŒƒå›´ä½çº¹ç†åŒºåŸŸä¸­3DGSçš„å‡ ä½•æ¨¡ç³Šé—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†3Dè¡¨é¢é‡å»ºè´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿ3Dé«˜æ–¯æ³¼æº…åœ¨å®¤å†…åœºæ™¯ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§èŒƒå›´ä½çº¹ç†åŒºåŸŸï¼Œä»…ä¾èµ–å…‰åº¦æŸå¤±ä¼šå¯¼è‡´å‡ ä½•æ¨¡ç³Šï¼Œæ— æ³•æ¢å¤é«˜ä¿çœŸåº¦çš„3Dè¡¨é¢ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å®¤å†…ç¯å¢ƒé‡å»ºä¸­çš„åº”ç”¨æ•ˆæœã€‚</p>
<p><strong>Method:</strong> æå‡ºPlanarGSæ¡†æ¶ï¼Œè®¾è®¡äº†è¯­è¨€æç¤ºå¹³é¢å…ˆéªŒç®¡çº¿ï¼Œåˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€åˆ†å‰²æ¨¡å‹å¹¶é€šè¿‡è·¨è§†å›¾èåˆå’Œå‡ ä½•å…ˆéªŒæ£€æŸ¥æ¥ä¼˜åŒ–åŒºåŸŸæè®®ï¼›åœ¨3Dé«˜æ–¯ä¼˜åŒ–ä¸­å¼•å…¥å¹³é¢å…ˆéªŒç›‘ç£é¡¹å’Œå‡ ä½•å…ˆéªŒç›‘ç£é¡¹ï¼Œåˆ†åˆ«å¼ºåˆ¶å¹³é¢ä¸€è‡´æ€§å’Œå¼•å¯¼é«˜æ–¯åˆ†å¸ƒæœå‘æ·±åº¦ä¸æ³•çº¿çº¿ç´¢ã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†å®¤å†…åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPlanarGSèƒ½å¤Ÿé‡å»ºå‡†ç¡®ä¸”è¯¦ç»†çš„3Dè¡¨é¢ï¼Œåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä»¥è¾ƒå¤§ä¼˜åŠ¿è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆè¯­è¨€å¼•å¯¼çš„å¹³é¢å…ˆéªŒå’Œå‡ ä½•ç›‘ç£èƒ½å¤Ÿæœ‰æ•ˆè§£å†³3DGSåœ¨å®¤å†…åœºæ™¯ä¸­çš„å‡ ä½•æ¨¡ç³Šé—®é¢˜ï¼Œä¸ºåŸºäºé«˜æ–¯æ³¼æº…çš„å®¤å†…åœºæ™¯é‡å»ºæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an
efficient representation for novel-view synthesis, achieving impressive visual
quality. However, in scenes dominated by large and low-texture regions, common
in indoor environments, the photometric loss used to optimize 3DGS yields
ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome
this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for
indoor scene reconstruction. Specifically, we design a pipeline for
Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language
segmentation model and refines its region proposals via cross-view fusion and
inspection with geometric priors. 3D Gaussians in our framework are optimized
with two additional terms: a planar prior supervision term that enforces planar
consistency, and a geometric prior supervision term that steers the Gaussians
toward the depth and normal cues. We have conducted extensive experiments on
standard indoor benchmarks. The results show that PlanarGS reconstructs
accurate and detailed 3D surfaces, consistently outperforming state-of-the-art
methods by a large margin. Project page: https://planargs.github.io</p>
<h3 id="6-reasoning-visual-language-model-for-chest-x-ray-analysis">[6] <a href="https://arxiv.org/abs/2510.23968">Reasoning Visual Language Model for Chest X-Ray Analysis</a></h3>
<p><em>Andriy Myronenko, Dong Yang, Baris Turkbey, Mariam Aboian, Sena Azamat, Esra Akcicek, Hongxu Yin, Pavlo Molchanov, Marc Edgar, Yufan He, Pengfei Guo, Yucheng Tang, Daguang Xu</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå°†æ€ç»´é“¾æ¨ç†å¼•å…¥èƒ¸éƒ¨Xå…‰è§£è¯»çš„æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆé«˜ä¿çœŸè§†è§‰ç¼–ç ä¸ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œåœ¨ä¿æŒç«äº‰æ€§å¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œä¸´åºŠå¯å®¡è®¡æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­è™½ç„¶è¡¨ç°å‡ºè‰²ï¼Œä½†å¤§å¤šç¼ºä¹é€æ˜åº¦ï¼Œæ— æ³•æä¾›ä¸´åºŠåŒ»ç”Ÿä¾èµ–çš„é€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨ä¸´åºŠå®è·µä¸­çš„å¯ä¿¡åº¦å’Œå®‰å…¨æ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¡†æ¶é‡‡ç”¨é«˜ä¿çœŸè§†è§‰ç¼–ç å™¨ä¸ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆè¿›è¡Œæ¨ç†é£æ ¼çš„ç›‘ç£å¾®è°ƒï¼Œç„¶åä½¿ç”¨åŸºäºXå…‰å¼‚å¸¸åˆ—è¡¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹è¾“å‡ºèƒ½å¤Ÿåæ˜ æ”¾å°„ç§‘åŒ»ç”Ÿç³»ç»Ÿæ€§æ€ç»´è¿‡ç¨‹ã€ä¸ç¡®å®šæ€§å’Œé‰´åˆ«è¯Šæ–­çš„æ¨ç†è½¨è¿¹ã€‚</p>
<p><strong>Result:</strong> åœ¨åˆ†å¸ƒå¤–è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†ç«äº‰æ€§çš„å¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½ï¼›åœ¨ä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿçš„é˜…è¯»ç ”ç©¶ä¸­ï¼Œå®Œæ•´æ¨ç†è½¨è¿¹æé«˜äº†è¯Šæ–­ä¿¡å¿ƒã€æ”¯æŒé”™è¯¯å®¡è®¡ï¼Œå¹¶å‡å°‘äº†æœ€ç»ˆæŠ¥å‘Šå®Œæˆæ—¶é—´ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨åŒ»å­¦å½±åƒä»»åŠ¡ä¸­æ¨ç†è´¨é‡ä¸é¢„æµ‹è´¨é‡åŒç­‰é‡è¦ï¼Œä¸ºæ„å»ºå¯ä¿¡èµ–ã€å¯è§£é‡Šçš„AIç³»ç»Ÿæä¾›äº†é‡è¦æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é€æ˜å†³ç­–è¿‡ç¨‹çš„ä¸´åºŠåº”ç”¨ä¸­ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Vision-language models (VLMs) have shown strong promise for medical image
analysis, but most remain opaque, offering predictions without the transparent,
stepwise reasoning clinicians rely on. We present a framework that brings
chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by
reasoning-first training paradigms, our approach is designed to learn how
experts reason, not just what they conclude, by aligning intermediate steps
with observable image evidence and radiology workflow. Beyond accuracy, the
explicit reasoning traces support clinical auditability: they reveal why a
conclusion was reached, which alternatives were considered, and where
uncertainty remains, enabling quality assurance, error analysis, and safer
human-AI collaboration.
  Our model couples high-fidelity visual encoding with a two-stage training
recipe: a reasoning-style supervised fine-tuning (SFT) followed by
reinforcement learning (RL) that uses verifiable rewards over a list of X-ray
abnormalities. The model outputs reasoning that mirrors radiologists systematic
thought process, uncertainty, and differential diagnosis. In
out-of-distribution evaluation, the approach achieves competitive multi-label
classification while improving interpretability. In a reader study with expert
radiologists, full reasoning traces increased confidence, supported error
auditing, and reduced time to finalize reports. We release code and the model
NV-Reason-CXR-3B to support community progress toward trustworthy, explainable
AI in chest radiography and other medical imaging tasks where reasoning quality
is as critical as prediction quality.</p>
<h3 id="7-teleego-benchmarking-egocentric-ai-assistants-in-the-wild">[7] <a href="https://arxiv.org/abs/2510.23981">TeleEgo: Benchmarking Egocentric AI Assistants in the Wild</a></h3>
<p><em>Jiaqi Yan, Ruilong Ren, Jingren Liu, Shuning Xu, Ling Wang, Yiheng Wang, Yun Wang, Long Zhang, Xiangyu Chen, Changzhi Sun, Jixiang Luo, Dell Zhang, Hao Sun, Chi Zhang, Xuelong Li</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†TeleEgoâ€”â€”ä¸€ä¸ªç”¨äºè¯„ä¼°å…·èº«AIåŠ©æ‰‹çš„é•¿æ—¶ç¨‹ã€æµå¼ã€å…¨æ¨¡æ€åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¶…è¿‡14å°æ—¶çš„åŒæ­¥è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬æ•°æ®ï¼Œå®šä¹‰äº†12ä¸ªè¯Šæ–­æ€§å­ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°å®æ—¶æ€§ã€å¤šæ¨¡æ€å¤„ç†å’Œé•¿æœŸè®°å¿†æ–¹é¢çš„ä¸è¶³ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºå‡†æµ‹è¯•é€šå¸¸å­¤ç«‹è¯„ä¼°å…·èº«AIåŠ©æ‰‹çš„å„é¡¹èƒ½åŠ›ï¼Œç¼ºä¹çœŸå®çš„æµå¼åœºæ™¯æ”¯æŒï¼Œæˆ–ä»…æ”¯æŒçŸ­æœŸä»»åŠ¡ï¼Œæ— æ³•å…¨é¢è¯„ä¼°ç°å®ç¯å¢ƒä¸­æ‰€éœ€çš„å®æ—¶å¤„ç†ã€å¤šæ¨¡æ€è¾“å…¥å’Œé•¿æœŸè®°å¿†ä¿æŒèƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†åŒ…å«å·¥ä½œå­¦ä¹ ã€ç”Ÿæ´»æ–¹å¼ã€ç¤¾äº¤æ´»åŠ¨å’Œå¤–å‡ºæ–‡åŒ–å››ä¸ªé¢†åŸŸçš„åŒæ­¥å¤šæ¨¡æ€æ•°æ®é›†ï¼Œé‡‡ç”¨ç»Ÿä¸€å…¨å±€æ—¶é—´çº¿å¯¹é½ï¼ŒåŒ…å«é«˜è´¨é‡è§†è§‰å™è¿°å’Œè¯­éŸ³è½¬å½•ï¼Œå®šä¹‰äº†è®°å¿†ã€ç†è§£å’Œè·¨è®°å¿†æ¨ç†ä¸‰å¤§æ ¸å¿ƒèƒ½åŠ›çš„12ä¸ªè¯Šæ–­æ€§å­ä»»åŠ¡ï¼ŒåŒ…å«3,291ä¸ªäººå·¥éªŒè¯çš„é—®ç­”é¡¹ç›®ï¼Œå¹¶æå‡ºäº†å®æ—¶å‡†ç¡®ç‡å’Œè®°å¿†æŒä¹…æ—¶é—´ä¸¤ä¸ªå…³é”®è¯„ä¼°æŒ‡æ ‡ã€‚</p>
<p><strong>Result:</strong> TeleEgoåŸºå‡†æµ‹è¯•åŒ…å«æ¯ä½å‚ä¸è€…è¶…è¿‡14å°æ—¶çš„å¤šæ¨¡æ€æ•°æ®ï¼Œæ”¯æŒå¤šç§é—®ç­”æ ¼å¼çš„ä¸¥æ ¼æµå¼è¯„ä¼°ï¼Œé€šè¿‡æå‡ºçš„åŒæŒ‡æ ‡ç³»ç»Ÿèƒ½å¤Ÿè”åˆè¯„ä¼°æ¨¡å‹çš„æ­£ç¡®æ€§ã€æ—¶é—´å“åº”æ€§å’Œé•¿æœŸè®°å¿†ä¿æŒèƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> TeleEgoä¸ºå¼€å‘å®ç”¨çš„AIåŠ©æ‰‹æä¾›äº†ç°å®ä¸”å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡é•¿æ—¶ç¨‹æµå¼å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•çš„å»ºç«‹ï¼Œæ¨åŠ¨äº†å…·èº«AIåŠ©æ‰‹åœ¨çœŸå®æ—¥å¸¸ç¯å¢ƒä¸­çš„èƒ½åŠ›å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æœŸè®°å¿†ä¿æŒå’Œå®æ—¶å“åº”æ–¹é¢çš„æŠ€æœ¯è¿›æ­¥ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Egocentric AI assistants in real-world settings must process multi-modal
inputs (video, audio, text), respond in real time, and retain evolving
long-term memory. However, existing benchmarks typically evaluate these
abilities in isolation, lack realistic streaming scenarios, or support only
short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming,
omni-modal benchmark for evaluating egocentric AI assistants in realistic daily
contexts. The dataset features over 14 hours per participant of synchronized
egocentric video, audio, and text across four domains: work \&amp; study, lifestyle
\&amp; routines, social activities, and outings \&amp; culture. All data is aligned on
a unified global timeline and includes high-quality visual narrations and
speech transcripts, curated through human refinement.TeleEgo defines 12
diagnostic subtasks across three core capabilities: Memory (recalling past
events), Understanding (interpreting the current moment), and Cross-Memory
Reasoning (linking distant events). It contains 3,291 human-verified QA items
spanning multiple question formats (single-choice, binary, multi-choice, and
open-ended), evaluated strictly in a streaming setting. We propose two key
metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess
correctness, temporal responsiveness, and long-term retention. TeleEgo provides
a realistic and comprehensive evaluation to advance the development of
practical AI assistants.</p>
<h3 id="8-mars-bench-a-benchmark-for-evaluating-foundation-models-for-mars-science-tasks">[8] <a href="https://arxiv.org/abs/2510.24010">Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks</a></h3>
<p><em>Mirali Purohit, Bimal Gajera, Vatsal Malaviya, Irish Mehta, Kunal Kasodekar, Jacob Adler, Steven Lu, Umaa Rebbapragada, Hannah Kerner</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Mars-Benchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºç³»ç»Ÿè¯„ä¼°ç«æ˜Ÿç§‘å­¦ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«20ä¸ªæ•°æ®é›†ï¼Œæ¶µç›–åˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡ï¼Œæ—¨åœ¨å»ºç«‹ç«æ˜Ÿæœºå™¨å­¦ä¹ æ¨¡å‹å¼€å‘çš„æ ‡å‡†åŒ–åŸºç¡€ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡åŸºç¡€æ¨¡å‹åœ¨åœ°çƒè§‚æµ‹ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç«æ˜Ÿç§‘å­¦ä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ï¼Œä¸»è¦éšœç¢æ˜¯ç¼ºä¹æ ‡å‡†åŒ–çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ï¼Œè¿™é™åˆ¶äº†ç«æ˜Ÿä¸“ç”¨åŸºç¡€æ¨¡å‹çš„å¼€å‘è¿›ç¨‹ã€‚</p>
<p><strong>Method:</strong> Mars-BenchåŸºå‡†åŒ…å«20ä¸ªæ ‡å‡†åŒ–æ•°æ®é›†ï¼Œæ¶µç›–åˆ†ç±»ã€åˆ†å‰²å’Œç‰©ä½“æ£€æµ‹ä»»åŠ¡ï¼Œä¸“æ³¨äºå…³é”®åœ°è´¨ç‰¹å¾å¦‚é™¨çŸ³å‘ã€é”¥ä½“ã€å·¨çŸ³å’Œéœœå†»ï¼Œå¹¶æä¾›äº†ä½¿ç”¨è‡ªç„¶å›¾åƒã€åœ°çƒå«æ˜Ÿæ•°æ®å’Œæœ€å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„åŸºçº¿è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> æ‰€æœ‰åˆ†æç»“æœè¡¨æ˜ï¼Œç«æ˜Ÿä¸“ç”¨åŸºç¡€æ¨¡å‹ç›¸æ¯”é€šç”¨é¢†åŸŸå¯¹åº”æ¨¡å‹å¯èƒ½å…·æœ‰ä¼˜åŠ¿ï¼Œè¿™æ¿€åŠ±äº†å¯¹é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒçš„è¿›ä¸€æ­¥æ¢ç´¢ï¼ŒåŸºå‡†æµ‹è¯•ä¸ºç«æ˜Ÿç§‘å­¦æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¼€å‘å’Œæ¯”è¾ƒå»ºç«‹äº†æ ‡å‡†åŒ–åŸºç¡€ã€‚</p>
<p><strong>Conclusion:</strong> ç«æ˜Ÿä¸“ç”¨åŸºç¡€æ¨¡å‹åœ¨ç«æ˜Ÿç§‘å­¦ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜äºé€šç”¨æ¨¡å‹çš„æ½œåŠ›ï¼ŒMars-Benchçš„å»ºç«‹ä¸ºæœªæ¥ç«æ˜Ÿæœºå™¨å­¦ä¹ ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶ï¼Œæ¨åŠ¨äº†é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Foundation models have enabled rapid progress across many specialized domains
by leveraging large-scale pre-training on unlabeled data, demonstrating strong
generalization to a variety of downstream tasks. While such models have gained
significant attention in fields like Earth Observation, their application to
Mars science remains limited. A key enabler of progress in other domains has
been the availability of standardized benchmarks that support systematic
evaluation. In contrast, Mars science lacks such benchmarks and standardized
evaluation frameworks, which have limited progress toward developing foundation
models for Martian tasks. To address this gap, we introduce Mars-Bench, the
first benchmark designed to systematically evaluate models across a broad range
of Mars-related tasks using both orbital and surface imagery. Mars-Bench
comprises 20 datasets spanning classification, segmentation, and object
detection, focused on key geologic features such as craters, cones, boulders,
and frost. We provide standardized, ready-to-use datasets and baseline
evaluations using models pre-trained on natural images, Earth satellite data,
and state-of-the-art vision-language models. Results from all analyses suggest
that Mars-specific foundation models may offer advantages over general-domain
counterparts, motivating further exploration of domain-adapted pre-training.
Mars-Bench aims to establish a standardized foundation for developing and
comparing machine learning models for Mars science. Our data, models, and code
are available at: https://mars-bench.github.io/.</p>
<h3 id="9-autoprompt-automated-red-teaming-of-text-to-image-models-via-llm-driven-adversarial-prompts">[9] <a href="https://arxiv.org/abs/2510.24034">AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts</a></h3>
<p><em>Yufan Liu, Wanqian Zhang, Huashan Chen, Lin Wang, Xiaojun Jia, Zheng Lin, Weiping Wang</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†APTï¼ˆAutoPrompTï¼‰ï¼Œä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆäººç±»å¯è¯»å¯¹æŠ—æ€§åç¼€çš„é»‘ç›’æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å®‰å…¨æ¼æ´ã€‚è¯¥æ–¹æ³•é€šè¿‡äº¤æ›¿ä¼˜åŒ–å¾®è°ƒç®¡é“å’ŒåŒé‡è§„é¿ç­–ç•¥ï¼Œèƒ½å¤Ÿç»•è¿‡åŸºäºå›°æƒ‘åº¦çš„è¿‡æ»¤å™¨å’Œé»‘åå•è¯è¿‡æ»¤å™¨ï¼Œå±•ç°å‡ºå“è¶Šçš„çº¢é˜Ÿæµ‹è¯•æ€§èƒ½å’Œé›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å®‰å…¨æœºåˆ¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æç¤ºçš„æ”»å‡»ï¼Œè€Œç°æœ‰çš„çº¢é˜Ÿæµ‹è¯•æ–¹æ³•é€šå¸¸éœ€è¦ç™½ç›’è®¿é—®æƒé™ï¼Œä¾èµ–ä½æ•ˆçš„é€æç¤ºä¼˜åŒ–ï¼Œå¹¶ä¸”ä¸å¯é¿å…åœ°ç”Ÿæˆè¯­ä¹‰æ— æ„ä¹‰çš„æç¤ºï¼Œå®¹æ˜“è¢«è¿‡æ»¤å™¨é˜»æ­¢ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº¤æ›¿ä¼˜åŒ–å¾®è°ƒç®¡é“ï¼Œåœ¨å¯¹æŠ—æ€§åç¼€ä¼˜åŒ–å’Œåˆ©ç”¨ä¼˜åŒ–åç¼€å¾®è°ƒLLMä¹‹é—´äº¤æ›¿è¿›è¡Œï¼›é›†æˆåŒé‡è§„é¿ç­–ç•¥ï¼Œé€šè¿‡è¾…åŠ©LLMå›°æƒ‘åº¦è¯„åˆ†çº¦æŸç”Ÿæˆäººç±»å¯è¯»æç¤ºï¼Œå¹¶å¼•å…¥ç¦æ­¢è¯æƒ©ç½šæ¥æŠ‘åˆ¶é»‘åå•ä¸­æ˜¾å¼ç¦æ­¢è¯çš„ç”Ÿæˆã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„äººç±»å¯è¯»ã€æŠ—è¿‡æ»¤çš„å¯¹æŠ—æ€§æç¤ºå…·æœ‰å‡ºè‰²çš„çº¢é˜Ÿæµ‹è¯•æ€§èƒ½ï¼Œä»¥åŠå“è¶Šçš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼Œèƒ½å¤Ÿå³æ—¶é€‚åº”æœªè§è¿‡çš„æç¤ºï¼Œå¹¶åœ¨å•†ä¸šAPIä¸­æš´éœ²å…³é”®æ¼æ´ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å®‰å…¨æœºåˆ¶çš„è„†å¼±æ€§ï¼Œæå‡ºçš„é»‘ç›’æ¡†æ¶ä¸ºè¯„ä¼°å’Œå¢å¼ºæ¨¡å‹å®‰å…¨æ€§æä¾›äº†æœ‰æ•ˆå·¥å…·ï¼ŒåŒæ—¶è¯æ˜äº†äººç±»å¯è¯»å¯¹æŠ—æ€§æç¤ºåœ¨ç»•è¿‡ç°æœ‰é˜²å¾¡æœºåˆ¶æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Despite rapid advancements in text-to-image (T2I) models, their safety
mechanisms are vulnerable to adversarial prompts, which maliciously generate
unsafe images. Current red-teaming methods for proactively assessing such
vulnerabilities usually require white-box access to T2I models, and rely on
inefficient per-prompt optimization, as well as inevitably generate
semantically meaningless prompts easily blocked by filters. In this paper, we
propose APT (AutoPrompT), a black-box framework that leverages large language
models (LLMs) to automatically generate human-readable adversarial suffixes for
benign prompts. We first introduce an alternating optimization-finetuning
pipeline between adversarial suffix optimization and fine-tuning the LLM
utilizing the optimized suffix. Furthermore, we integrates a dual-evasion
strategy in optimization phase, enabling the bypass of both perplexity-based
filter and blacklist word filter: (1) we constrain the LLM generating
human-readable prompts through an auxiliary LLM perplexity scoring, which
starkly contrasts with prior token-level gibberish, and (2) we also introduce
banned-token penalties to suppress the explicit generation of banned-tokens in
blacklist. Extensive experiments demonstrate the excellent red-teaming
performance of our human-readable, filter-resistant adversarial prompts, as
well as superior zero-shot transferability which enables instant adaptation to
unseen prompts and exposes critical vulnerabilities even in commercial APIs
(e.g., Leonardo.Ai.).</p>
<h3 id="10-enhancing-clip-robustness-via-cross-modality-alignment">[10] <a href="https://arxiv.org/abs/2510.24038">Enhancing CLIP Robustness via Cross-Modality Alignment</a></h3>
<p><em>Xingyu Zhu, Beier Zhu, Shuo Wang, Kesen Zhao, Hanwang Zhang</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCOLAï¼Œä¸€ç§åŸºäºæœ€ä¼˜ä¼ è¾“çš„è·¨æ¨¡æ€å¯¹é½æ¡†æ¶ï¼Œé€šè¿‡æ¢å¤å¯¹æŠ—æ‰°åŠ¨ä¸‹çš„å…¨å±€å›¾åƒ-æ–‡æœ¬å¯¹é½å’Œå±€éƒ¨ç»“æ„ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒä¸”ä¸ç°æœ‰å¾®è°ƒæ¨¡å‹å…¼å®¹ï¼Œåœ¨14ä¸ªé›¶æ ·æœ¬åˆ†ç±»åŸºå‡†ä¸Šå–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹å¯¹æŠ—æ‰°åŠ¨é«˜åº¦è„†å¼±ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å¯¹æŠ—å¾®è°ƒæˆ–æç¤ºä¼˜åŒ–ï¼Œå¿½è§†äº†CLIPç¼–ç ç‰¹å¾ä¸­çš„è·¨æ¨¡æ€å¯¹é½å·®è·ï¼Œè¿™ç§ä¸å¯¹é½åœ¨å¯¹æŠ—æ‰°åŠ¨ä¸‹è¢«æ˜¾è‘—æ”¾å¤§ï¼Œå¯¼è‡´åˆ†ç±»æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚</p>
<p><strong>Method:</strong> COLAæ¡†æ¶é‡‡ç”¨æœ€ä¼˜ä¼ è¾“æ–¹æ³•ï¼Œé¦–å…ˆå°†å¯¹æŠ—å›¾åƒåµŒå…¥æŠ•å½±åˆ°ç±»åˆ«æ–‡æœ¬ç‰¹å¾å¼ æˆçš„å­ç©ºé—´ï¼Œè¿‡æ»¤éè¯­ä¹‰å¤±çœŸåŒæ—¶ä¿ç•™åˆ¤åˆ«ä¿¡æ¯ï¼›ç„¶åå°†å›¾åƒå’Œæ–‡æœ¬å»ºæ¨¡ä¸ºå¤šä¸ªå¢å¼ºè§†å›¾ä¸Šçš„ç¦»æ•£åˆ†å¸ƒï¼Œé€šè¿‡æœ€ä¼˜ä¼ è¾“ç»†åŒ–å¯¹é½ï¼Œå­ç©ºé—´æŠ•å½±æ— ç¼é›†æˆåˆ°æˆæœ¬è®¡ç®—ä¸­ã€‚</p>
<p><strong>Result:</strong> åœ¨14ä¸ªé›¶æ ·æœ¬åˆ†ç±»åŸºå‡†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒCOLAåœ¨PGDå¯¹æŠ—æ”»å‡»ä¸‹åœ¨ImageNetåŠå…¶å˜ä½“ä¸Šå¹³å‡æå‡6.7%ï¼ŒåŒæ—¶åœ¨å¹²å‡€æ ·æœ¬ä¸Šä¿æŒé«˜å‡†ç¡®ç‡ã€‚è¯¥æ–¹æ³•è®­ç»ƒå…è´¹ä¸”ä¸ç°æœ‰å¾®è°ƒæ¨¡å‹å…¼å®¹ã€‚</p>
<p><strong>Conclusion:</strong> COLAé€šè¿‡æ˜¾å¼å¤„ç†å¯¹æŠ—ä¸å¯¹é½é—®é¢˜ï¼Œè¯æ˜äº†è·¨æ¨¡æ€ç‰¹å¾å¯¹é½å¯¹æå‡è§†è§‰è¯­è¨€æ¨¡å‹é²æ£’æ€§çš„é‡è¦æ€§ã€‚è¯¥æ–¹æ³•ä¸ºæ— éœ€é¢å¤–è®­ç»ƒå³å¯å¢å¼ºæ¨¡å‹å¯¹æŠ—é²æ£’æ€§æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œå…·æœ‰å®é™…éƒ¨ç½²ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Vision-language models (VLMs) such as CLIP demonstrate strong generalization
in zero-shot classification but remain highly vulnerable to adversarial
perturbations. Existing methods primarily focus on adversarial fine-tuning or
prompt optimization; they often overlook the gaps in CLIP's encoded features,
which is shown as the text and image features lie far apart from each other.
This misalignment is significantly amplified under adversarial perturbations,
leading to severe degradation in classification performance. To address this
problem, we propose Cross-modality Alignment, dubbed COLA, an optimal
transport-based framework that explicitly addresses adversarial misalignment by
restoring both global image-text alignment and local structural consistency in
the feature space. (1) COLA first projects adversarial image embeddings onto a
subspace spanned by class text features, effectively filtering out non-semantic
distortions while preserving discriminative information. (2) It then models
images and texts as discrete distributions over multiple augmented views and
refines their alignment via OT, with the subspace projection seamlessly
integrated into the cost computation. This design ensures stable cross-modal
alignment even under adversarial conditions. COLA is training-free and
compatible with existing fine-tuned models. Extensive evaluations across 14
zero-shot classification benchmarks demonstrate the effectiveness of COLA,
especially with an average improvement of 6.7% on ImageNet and its variants
under PGD adversarial attacks, while maintaining high accuracy on clean
samples.</p>
<h3 id="11-vc4vg-optimizing-video-captions-for-text-to-video-generation">[11] <a href="https://arxiv.org/abs/2510.24134">VC4VG: Optimizing Video Captions for Text-to-Video Generation</a></h3>
<p><em>Yang Du, Zhuoran Lin, Kaiqiang Song, Biao Wang, Zhicheng Zheng, Tiezheng Ge, Bo Zheng, Qin Jin</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VC4VGæ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¼˜åŒ–è§†é¢‘å­—å¹•ï¼Œé€šè¿‡ç³»ç»Ÿåˆ†æè§†é¢‘é‡å»ºæ‰€éœ€çš„å…³é”®å…ƒç´ å¹¶æ„å»ºå¤šç»´è¯„ä¼°åŸºå‡†ï¼Œæ˜¾è‘—æå‡äº†T2Væ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆé¢†åŸŸè™½ç„¶è®¤è¯†åˆ°é«˜è´¨é‡è§†é¢‘-æ–‡æœ¬å¯¹çš„é‡è¦æ€§ï¼Œä½†ä¸“é—¨é’ˆå¯¹T2Vè®­ç»ƒä¼˜åŒ–çš„è§†é¢‘å­—å¹•ç­–ç•¥ç ”ç©¶ä»æ˜¾ä¸è¶³ï¼Œç¼ºä¹ç³»ç»Ÿæ€§çš„å­—å¹•è®¾è®¡æ–¹æ³•å’Œè¯„ä¼°æ ‡å‡†ã€‚</p>
<p><strong>Method:</strong> æå‡ºVC4VGæ¡†æ¶ï¼Œä»T2Vè§†è§’åˆ†æå­—å¹•å†…å®¹ï¼Œå°†è§†é¢‘é‡å»ºæ‰€éœ€çš„å…³é”®å…ƒç´ åˆ†è§£ä¸ºå¤šä¸ªç»´åº¦ï¼Œå»ºç«‹åŸåˆ™æ€§çš„å­—å¹•è®¾è®¡æ–¹æ³•ï¼Œå¹¶æ„å»ºVC4VG-BenchåŸºå‡†ï¼ŒåŒ…å«ç»†ç²’åº¦ã€å¤šç»´åº¦å’Œå¿…è¦æ€§åˆ†çº§çš„è¯„ä¼°æŒ‡æ ‡ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›çš„T2Vå¾®è°ƒå®éªŒè¡¨æ˜ï¼Œå­—å¹•è´¨é‡çš„æ”¹è¿›ä¸è§†é¢‘ç”Ÿæˆæ€§èƒ½ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸ºT2Væ¨¡å‹è®­ç»ƒæä¾›äº†å¯é çš„å­—å¹•ä¼˜åŒ–æ–¹æ¡ˆã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ç¡®ç«‹äº†è§†é¢‘å­—å¹•ä¼˜åŒ–å¯¹T2Vç”Ÿæˆæ€§èƒ½çš„å…³é”®å½±å“ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†ç³»ç»Ÿçš„è¯„ä¼°å·¥å…·å’Œæ–¹æ³•è®ºï¼Œæ¨åŠ¨äº†æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„æ•°æ®è´¨é‡ä¼˜åŒ–ç ”ç©¶ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Recent advances in text-to-video (T2V) generation highlight the critical role
of high-quality video-text pairs in training models capable of producing
coherent and instruction-aligned videos. However, strategies for optimizing
video captions specifically for T2V training remain underexplored. In this
paper, we introduce VC4VG (Video Captioning for Video Generation), a
comprehensive caption optimization framework tailored to the needs of T2V
models.We begin by analyzing caption content from a T2V perspective,
decomposing the essential elements required for video reconstruction into
multiple dimensions, and proposing a principled caption design methodology. To
support evaluation, we construct VC4VG-Bench, a new benchmark featuring
fine-grained, multi-dimensional, and necessity-graded metrics aligned with
T2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a
strong correlation between improved caption quality and video generation
performance, validating the effectiveness of our approach. We release all
benchmark tools and code at https://github.com/qyr0403/VC4VG to support further
research.</p>
<h3 id="12-beyond-objects-contextual-synthetic-data-generation-for-fine-grained-classification">[12] <a href="https://arxiv.org/abs/2510.24078">Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification</a></h3>
<p><em>William Yang, Xindi Wu, Zhiwei Deng, Esin Tureci, Olga Russakovsky</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºBOBå¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡æå–ç±»æ— å…³å±æ€§å¹¶æ˜¾å¼æ¡ä»¶åŒ–æ¥ç¼“è§£T2Iæ¨¡å‹åœ¨ç»†ç²’åº¦åˆ†ç±»ä»»åŠ¡ä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨ä½æ ·æœ¬ç»†ç²’åº¦åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨åˆæˆæ•°æ®å¢å¼ºæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨åˆæˆæ•°æ®é›†ç”Ÿæˆä¸­çš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œä½†ä¸ºåˆ†ç±»ä»»åŠ¡ç”Ÿæˆæœ‰æ•ˆçš„åˆæˆè®­ç»ƒæ•°æ®ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¾®è°ƒT2Iæ¨¡å‹è™½ç„¶èƒ½æé«˜åˆæˆæ•°æ®è´¨é‡ï¼Œä½†å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå’Œæ ·æœ¬å¤šæ ·æ€§é™ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»†ç²’åº¦åˆ†ç±»ä»»åŠ¡ä¸­è¿™ä¸€é—®é¢˜å°¤ä¸ºçªå‡ºã€‚</p>
<p><strong>Method:</strong> æå‡ºBOBå¾®è°ƒç­–ç•¥ï¼Œé¦–å…ˆä»å°‘é‡çœŸå®æ ·æœ¬ä¸­æå–ç±»æ— å…³å±æ€§ï¼ˆå¦‚åœºæ™¯èƒŒæ™¯å’Œç‰©ä½“å§¿æ€ï¼‰ï¼Œç„¶ååœ¨T2Iæ¨¡å‹å¾®è°ƒè¿‡ç¨‹ä¸­æ˜¾å¼åœ°æ¡ä»¶åŒ–è¿™äº›å±æ€§ï¼Œå¹¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹å®ƒä»¬è¿›è¡Œè¾¹ç¼˜åŒ–å¤„ç†ã€‚è¿™ç§è®¾è®¡ç¼“è§£äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œä¿æŒäº†T2Iæ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒï¼Œå‡å°‘äº†ä¼°è®¡è¯¯å·®ï¼Œå¹¶æœ€å°åŒ–äº†æ„å¤–çš„ç±»é—´å…³è”ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªT2Iæ¨¡å‹ã€éª¨å¹²ç½‘ç»œå’Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä½¿ç”¨åˆæˆæ•°æ®å¢å¼ºçš„ä½æ ·æœ¬ç»†ç²’åº¦åˆ†ç±»ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚BOBåœ¨Aircraftæ•°æ®é›†ä¸Šæ¯”DataDreamæå‡äº†7.4%ï¼Œåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„ä¸‰ä¸ªä¸Šï¼Œä½¿ç”¨5ä¸ªçœŸå®å›¾åƒåŠ ä¸ŠBOBå¢å¼ºçš„å¾®è°ƒæ€§èƒ½ä¼˜äºä½¿ç”¨10ä¸ªçœŸå®å›¾åƒçš„å¾®è°ƒã€‚åœ¨24ä¸ªå®éªŒè®¾ç½®ä¸­çš„18ä¸ªä¸­ä¼˜äºå…ˆå‰æŠ€æœ¯ï¼Œå…¶ä¸­14ä¸ªè®¾ç½®çš„å‡†ç¡®ç‡æå‡è¶…è¿‡2%ã€‚</p>
<p><strong>Conclusion:</strong> BOBæ–¹æ³•é€šè¿‡æ˜¾å¼æ¡ä»¶åŒ–å’Œè¾¹ç¼˜åŒ–ç±»æ— å…³å±æ€§ï¼Œæœ‰æ•ˆç¼“è§£äº†T2Iæ¨¡å‹åœ¨ç»†ç²’åº¦åˆ†ç±»ä»»åŠ¡ä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆå¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•ä¸ºä½¿ç”¨åˆæˆæ•°æ®å¢å¼ºè§£å†³ä½æ ·æœ¬å­¦ä¹ é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Text-to-image (T2I) models are increasingly used for synthetic dataset
generation, but generating effective synthetic training data for classification
remains challenging. Fine-tuning a T2I model with a few real examples can help
improve the quality of synthetic training data; however, it may also cause
overfitting and reduce diversity in the generated samples. We propose a
fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for
fine-grained classification. Given a small set of real examples, we first
extract class-agnostic attributes such as scene background and object pose. We
then explicitly condition on these attributes during fine-tuning of the T2I
model and marginalize them out during generation. This design mitigates
overfitting, preserves the T2I model's generative prior, reduces estimation
errors, and further minimizes unintended inter-class associations. Extensive
experiments across multiple T2I models, backbones, and datasets show that our
method achieves state-of-the-art performance in low-shot fine-grained
classification when augmented with synthetic data. Concretely, BOB outperforms
DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning
a CLIP classifier with five real images augmented with 100 synthetic images).
In three of the four benchmarks, fine-tuning downstream models with 5 real
images augmented with BOB achieves better performance than fine-tuning with 10
real images. Collectively, BOB outperforms prior art in 18 of 24 experimental
settings, with 2+% accuracy improvements in 14 of these settings.</p>
<h3 id="13-compositional-image-synthesis-with-inference-time-scaling">[13] <a href="https://arxiv.org/abs/2510.24133">Compositional Image Synthesis with Inference-Time Scaling</a></h3>
<p><em>Minsuk Ji, Sanghyeok Lee, Namhyuk Ahn</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç»„åˆæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆç›®æ ‡ä¸­å¿ƒåŒ–æ–¹æ³•å’Œè‡ªä¼˜åŒ–æœºåˆ¶æ¥æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å¸ƒå±€å¿ å®åº¦ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹åˆæˆæ˜¾å¼å¸ƒå±€ï¼Œå¹¶é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè¿­ä»£é‡æ’åºï¼Œæ˜¾è‘—æ”¹å–„äº†åœºæ™¯ä¸æç¤ºçš„å¯¹é½è´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°ä»£æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç»„åˆæ€§æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œç»å¸¸æ— æ³•å‡†ç¡®æ¸²æŸ“ç‰©ä½“æ•°é‡ã€å±æ€§å’Œç©ºé—´å…³ç³»ã€‚è¿™ç§å¸ƒå±€å¿ å®åº¦çš„ç¼ºå¤±é™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ç”Ÿæˆä¸­çš„å®é™…åº”ç”¨æ•ˆæœï¼Œéœ€è¦ä¸“é—¨çš„æ–¹æ³•æ¥æå‡ç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æç¤ºçš„å¯¹é½ç¨‹åº¦ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¡†æ¶é‡‡ç”¨æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé¦–å…ˆåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä»è¾“å…¥æç¤ºä¸­åˆæˆæ˜¾å¼å¸ƒå±€ï¼Œç„¶åå°†è¿™äº›å¸ƒå±€æ³¨å…¥å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨ç”Ÿæˆé˜¶æ®µï¼Œé‡‡ç”¨ç›®æ ‡ä¸­å¿ƒåŒ–çš„è§†è§‰è¯­è¨€æ¨¡å‹å¯¹å¤šä¸ªå€™é€‰å›¾åƒè¿›è¡Œè¿­ä»£é‡æ’åºï¼Œé€‰æ‹©ä¸æç¤ºæœ€å¯¹é½çš„ç»“æœï¼Œå®ç°äº†æ˜¾å¼å¸ƒå±€å¼•å¯¼ä¸åŸºäºè‡ªä¼˜åŒ–çš„æ¨ç†æ—¶æ‰©å±•çš„ç»Ÿä¸€ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ç›¸æ¯”æœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨åœºæ™¯ä¸æç¤ºå¯¹é½æ–¹é¢å–å¾—äº†æ›´å¼ºçš„æ€§èƒ½ã€‚é€šè¿‡æ˜¾å¼å¸ƒå±€å¼•å¯¼å’Œè¿­ä»£ä¼˜åŒ–æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒåœ¨ç‰©ä½“æ•°é‡ã€å±æ€§å’Œç©ºé—´å…³ç³»æ–¹é¢çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„ç¾å­¦è´¨é‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆæ˜¾å¼å¸ƒå±€å¼•å¯¼ä¸è‡ªä¼˜åŒ–æœºåˆ¶åœ¨æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå¿ å®åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ä¸ºæ”¹å–„æ¨¡å‹ç»„åˆæ€§æä¾›äº†æ–°æ€è·¯ï¼Œæœªæ¥å¯æ‰©å±•åˆ°æ›´å¤æ‚çš„åœºæ™¯ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ¨åŠ¨æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Despite their impressive realism, modern text-to-image models still struggle
with compositionality, often failing to render accurate object counts,
attributes, and spatial relations. To address this challenge, we present a
training-free framework that combines an object-centric approach with
self-refinement to improve layout faithfulness while preserving aesthetic
quality. Specifically, we leverage large language models (LLMs) to synthesize
explicit layouts from input prompts, and we inject these layouts into the image
generation process, where a object-centric vision-language model (VLM) judge
reranks multiple candidates to select the most prompt-aligned outcome
iteratively. By unifying explicit layout-grounding with self-refine-based
inference-time scaling, our framework achieves stronger scene alignment with
prompts compared to recent text-to-image models. The code are available at
https://github.com/gcl-inha/ReFocus.</p>
<h3 id="14-viper-empowering-the-self-evolution-of-visual-perception-abilities-in-vision-language-model">[14] <a href="https://arxiv.org/abs/2510.24285">ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model</a></h3>
<p><em>Juntian Zhang, Song Jin, Chuanqi Cheng, Yuhan Liu, Yankai Lin, Xun Zhang, Yufei Zhang, Fei Jiang, Guojun Yin, Wei Lin, Rui Yan</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ViPERæ¡†æ¶ï¼Œé€šè¿‡ç²—ç²’åº¦åˆ°ç»†ç²’åº¦çš„æ¸è¿›å¼è§†è§‰æ„ŸçŸ¥å­¦ä¹ å’Œä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè§£å†³äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„ç“¶é¢ˆé—®é¢˜ï¼Œåœ¨ä¿æŒé€šç”¨èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ„ŸçŸ¥æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„æœ‰é™èƒ½åŠ›æ„æˆäº†å®é™…åº”ç”¨ä¸­çš„å…³é”®ç“¶é¢ˆï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨æ˜æ˜¾å±€é™æ€§ï¼šç›‘ç£å¾®è°ƒé€šå¸¸ä¼šæŸå®³æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ï¼Œè€Œå¼ºåŒ–å¾®è°ƒåˆ™ä¼˜å…ˆè€ƒè™‘æ–‡æœ¬æ¨ç†è€Œéè§†è§‰æ„ŸçŸ¥ï¼Œè¿™ä¿ƒä½¿ç ”ç©¶è€…å¯»æ±‚ä¸€ç§èƒ½å¤Ÿå¹³è¡¡æ„ŸçŸ¥èƒ½åŠ›æå‡ä¸é€šç”¨æ€§ä¿æŒçš„æ–°æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ViPERè‡ªä¸¾æ¡†æ¶ï¼Œé€šè¿‡å°†è§†è§‰æ„ŸçŸ¥å­¦ä¹ æ„å»ºä¸ºç²—ç²’åº¦åˆ°ç»†ç²’åº¦çš„æ¸è¿›è¿‡ç¨‹ï¼Œç»“åˆå›¾åƒçº§å’Œå®ä¾‹çº§é‡å»ºä»»åŠ¡ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ç­–ç•¥å»ºç«‹é—­ç¯è®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨å†…éƒ¨åˆæˆæ•°æ®ç›´æ¥é©±åŠ¨æ„ŸçŸ¥èƒ½åŠ›çš„è¿­ä»£è¿›åŒ–ã€‚</p>
<p><strong>Result:</strong> åœ¨Qwen2.5-VLç³»åˆ—æ¨¡å‹ä¸Šåº”ç”¨ViPERæ¡†æ¶äº§ç”Ÿäº†Qwen-Viperç³»åˆ—ï¼Œåœ¨ä¸ƒä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡1.7%ï¼Œåœ¨ç»†ç²’åº¦æ„ŸçŸ¥ä»»åŠ¡ä¸Šæœ€é«˜æå‡6.0%ï¼ŒåŒæ—¶åœ¨ä¸åŒè§†è§‰è¯­è¨€åœºæ™¯ä¸­ä¿æŒä¸€è‡´çš„ä¼˜è¶Šæ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ViPERä¸ä»…å®ç°äº†æ„ŸçŸ¥èƒ½åŠ›çš„è‡ªæˆ‘æå‡ï¼Œè¿˜ä¸ºç”Ÿæˆä¸ç†è§£ä¹‹é—´çš„äº’æƒ å…³ç³»æä¾›äº†å…·ä½“è¯æ®ï¼Œè¿™ä¸€çªç ´ä¸ºå¼€å‘æ›´è‡ªä¸»ã€æ›´å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå±•ç¤ºäº†é—­ç¯è®­ç»ƒèŒƒå¼åœ¨æ¨¡å‹èƒ½åŠ›è¿›åŒ–ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>The limited capacity for fine-grained visual perception presents a critical
bottleneck for Vision-Language Models (VLMs) in real-world applications.
Addressing this is challenging due to the scarcity of high-quality data and the
limitations of existing methods: supervised fine-tuning (SFT) often compromises
general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual
reasoning over visual perception. To bridge this gap, we propose a novel
two-stage task that structures visual perception learning as a coarse-to-fine
progressive process. Based on this task formulation, we develop ViPER, a
self-bootstrapping framework specifically designed to enable iterative
evolution through self-critiquing and self-prediction. By synergistically
integrating image-level and instance-level reconstruction with a two-stage
reinforcement learning strategy, ViPER establishes a closed-loop training
paradigm, where internally synthesized data directly fuel the enhancement of
perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the
Qwen-Viper series. With an average gain of 1.7% on seven comprehensive
benchmarks spanning various tasks and up to 6.0% on fine-grained perception,
Qwen-Viper consistently demonstrates superior performance across different
vision-language scenarios while maintaining generalizability. Beyond enabling
self-improvement in perceptual capabilities, ViPER provides concrete evidence
for the reciprocal relationship between generation and understanding, a
breakthrough to developing more autonomous and capable VLMs.</p>
<h3 id="15-enhancing-vision-language-models-for-autonomous-driving-through-task-specific-prompting-and-spatial-reasoning">[15] <a href="https://arxiv.org/abs/2510.24152">Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning</a></h3>
<p><em>Aodi Wu, Xubo Luo</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£çš„ç³»ç»ŸåŒ–è§†è§‰è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡æ··åˆæç¤ºè·¯ç”±ã€ä»»åŠ¡ç‰¹å®šæç¤ºã€è§†è§‰ç»„è£…æ¨¡å—å’Œä¼˜åŒ–æ¨ç†å‚æ•°ï¼Œåœ¨RoboSenseæŒ‘æˆ˜èµ›ä¸­å®ç°äº†ä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£ä»»åŠ¡ä¸­çš„æ€§èƒ½ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ„ŸçŸ¥ã€é¢„æµ‹ã€è§„åˆ’å’Œå¼‚å¸¸æ£€æµ‹ç­‰å¤šä»»åŠ¡äº¤å‰åœºæ™¯ä¸‹ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†ä¸åŒç±»å‹é—®é¢˜ä¹‹é—´çš„å¹²æ‰°ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æ„å»ºäº†å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæ··åˆæç¤ºè·¯ç”±å™¨å¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»å¹¶åˆ†å‘è‡³ä»»åŠ¡ç‰¹å®šçš„ä¸“å®¶æç¤ºï¼›ä»»åŠ¡ç‰¹å®šæç¤ºåµŒå…¥äº†æ˜¾å¼åæ ‡ç³»ã€ç©ºé—´æ¨ç†è§„åˆ™ã€è§’è‰²æ‰®æ¼”å’Œé“¾å¼/æ ‘çŠ¶æ¨ç†ï¼›è§†è§‰ç»„è£…æ¨¡å—æ ¹æ®é—®é¢˜éœ€æ±‚ç»„åˆå¤šè§†è§’å›¾åƒä¸å¯¹è±¡è£å‰ªï¼›é’ˆå¯¹ä¸åŒä»»åŠ¡ä¼˜åŒ–æ¨¡å‹æ¨ç†å‚æ•°é…ç½®ã€‚</p>
<p><strong>Result:</strong> åœ¨Qwen2.5-VL-72Bæ¨¡å‹ä¸Šå®ç°ï¼Œè¯¥æ–¹æ³•åœ¨Phase-1ï¼ˆå¹²å‡€æ•°æ®ï¼‰ä¸Šè¾¾åˆ°70.87%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œåœ¨Phase-2ï¼ˆæŸåæ•°æ®ï¼‰ä¸Šè¾¾åˆ°72.85%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè¯æ˜äº†ç»“æ„åŒ–æç¤ºå’Œç©ºé—´åŸºç¡€å¯¹å®‰å…¨å…³é”®è‡ªåŠ¨é©¾é©¶ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ç³»ç»ŸåŒ–çš„æç¤ºå·¥ç¨‹å’Œç©ºé—´åŸºç¡€èƒ½å¤Ÿæ˜¾è‘—æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è‡ªåŠ¨é©¾é©¶ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä¸ºå®‰å…¨å…³é”®åº”ç”¨æä¾›äº†å¯é çš„æŠ€æœ¯è·¯å¾„ï¼ŒåŒæ—¶å¼€æºä»£ç å’Œæç¤ºæ¨¡æ¿ä¿ƒè¿›äº†ç›¸å…³ç ”ç©¶çš„å¯å¤ç°æ€§ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>This technical report presents our solution for the RoboSense Challenge at
IROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving
scene understanding across perception, prediction, planning, and corruption
detection tasks. We propose a systematic framework built on four core
components. First, a Mixture-of-Prompts router classifies questions and
dispatches them to task-specific expert prompts, eliminating interference
across diverse question types. Second, task-specific prompts embed explicit
coordinate systems, spatial reasoning rules, role-playing,
Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to
each task. Third, a visual assembly module composes multi-view images with
object crops, magenta markers, and adaptive historical frames based on question
requirements. Fourth, we configure model inference parameters (temperature,
top-p, message roles) per task to optimize output quality. Implemented on
Qwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean
data) and 72.85% on Phase-2 (corrupted data), demonstrating that structured
prompting and spatial grounding substantially enhance VLM performance on
safety-critical autonomous driving tasks. Code and prompt are available at
https://github.com/wuaodi/UCAS-CSU-phase2.</p>
<h3 id="16-latent-sketchpad-sketching-visual-thoughts-to-elicit-multimodal-reasoning-in-mllms">[16] <a href="https://arxiv.org/abs/2510.24514">Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</a></h3>
<p><em>Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Latent Sketchpadæ¡†æ¶ï¼Œé€šè¿‡ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é…å¤‡å†…éƒ¨è§†è§‰è‰ç¨¿æœ¬ï¼Œå°†è§†è§‰ç”Ÿæˆç›´æ¥é›†æˆåˆ°è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä»è€Œå¢å¼ºæ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„è§†è§‰è§„åˆ’å’Œæƒ³è±¡èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è™½ç„¶åœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦è§†è§‰è§„åˆ’å’Œæƒ³è±¡çš„å¤æ‚åœºæ™¯ä¸­å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œä¼ ç»Ÿæ¨¡å‹çš„å†…éƒ¨è§†è§‰è¡¨ç¤ºä»…é™äºæ„ŸçŸ¥ç†è§£ï¼Œç¼ºä¹ç”Ÿæˆæ€§è§†è§‰æ€ç»´çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰å¤´è‡ªå›å½’åœ°ç”Ÿæˆè§†è§‰è¡¨ç¤ºï¼Œä»¥åŠé¢„è®­ç»ƒçš„è‰å›¾è§£ç å™¨å°†è¿™äº›è¡¨ç¤ºæ¸²æŸ“ä¸ºäººç±»å¯è§£é‡Šçš„å›¾åƒï¼Œå°†è§†è§‰ç”Ÿæˆç›´æ¥é›†æˆåˆ°æ¨¡å‹çš„åŸç”Ÿè‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­ã€‚</p>
<p><strong>Result:</strong> åœ¨MazePlanningæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLatent Sketchpadåœ¨å„ç§MLLMsä¸Šå®ç°äº†ä¸éª¨å¹²æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ¨ç†æ€§èƒ½ï¼Œå¹¶åœ¨åŒ…æ‹¬Gemma3å’ŒQwen2.5-VLåœ¨å†…çš„å‰æ²¿MLLMsä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡å°†æ¨¡å‹çš„æ–‡æœ¬æ¨ç†æ‰©å±•åˆ°è§†è§‰æ€ç»´ï¼Œè¯¥æ¡†æ¶ä¸ºäººæœºäº¤äº’å’Œæ›´å¹¿æ³›çš„åº”ç”¨å¼€è¾Ÿäº†æ–°æœºé‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåƒäººç±»ä½¿ç”¨è‰å›¾è¿›è¡Œè§†è§‰æ€è€ƒä¸€æ ·å‘å±•æƒ³æ³•ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>While Multimodal Large Language Models (MLLMs) excel at visual understanding,
they often struggle in complex scenarios that require visual planning and
imagination. Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad. The internal visual
representations of MLLMs have traditionally been confined to perceptual
understanding. We repurpose them to support generative visual thought without
compromising reasoning ability. Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process. It allows the model to interleave textual reasoning with the
generation of visual latents. These latents guide the internal thought process
and can be translated into sketch images for interpretability. To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images. We evaluate the framework on our new dataset
MazePlanning. Experiments across various MLLMs show that Latent Sketchpad
delivers comparable or even superior reasoning performance to their backbone.
It further generalizes across distinct frontier MLLMs, including Gemma3 and
Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications. More details and resources are available on our project
page: https://latent-sketchpad.github.io/.</p>
<h3 id="17-scope-saliency-coverage-oriented-token-pruning-for-efficient-multimodel-llms">[17] <a href="https://arxiv.org/abs/2510.24214">SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs</a></h3>
<p><em>Jinhong Deng, Wen Li, Joey Tianyi Zhou, Yang He</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSCOPEï¼Œä¸€ç§æ–°é¢–çš„è§†è§‰ä»¤ç‰Œå‰ªæç­–ç•¥ï¼Œé€šè¿‡è”åˆå»ºæ¨¡æ˜¾è‘—æ€§å’Œè¦†ç›–åº¦æ¥ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ï¼Œåœ¨ä¿æŒè¯­ä¹‰å®Œæ•´æ€§çš„åŒæ—¶æ˜¾è‘—å‡å°‘è®¡ç®—å¼€é”€ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€šå¸¸å¤„ç†å¤§é‡è§†è§‰ä»¤ç‰Œå¯¼è‡´è®¡ç®—å¼€é”€å·¨å¤§ï¼Œç°æœ‰è§†è§‰ä»¤ç‰Œå‰ªææ–¹æ³•ä¸»è¦åŸºäºæ³¨æ„åŠ›åˆ†æ•°é€‰æ‹©æœ€æ˜¾è‘—ä»¤ç‰Œï¼Œä½†ä¼šå¯¼è‡´æ‰€é€‰ä»¤ç‰Œè¯­ä¹‰ä¸å®Œæ•´çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºSCOPEç­–ç•¥ï¼Œå¼•å…¥åŸºäºä»¤ç‰Œå…³ç³»çš„é›†åˆè¦†ç›–åº¦æ¦‚å¿µï¼Œå®šä¹‰æ¯ä¸ªæœªé€‰ä»¤ç‰Œçš„è¦†ç›–å¢ç›Šï¼Œå°†æ˜¾è‘—æ€§åˆ†æ•°æ•´åˆåˆ°ä»¤ç‰Œè¦†ç›–å¢ç›Šä¸­å½¢æˆSCOPEåˆ†æ•°ï¼Œå¹¶è¿­ä»£é€‰æ‹©å…·æœ‰æœ€é«˜SCOPEåˆ†æ•°çš„ä»¤ç‰Œã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªè§†è§‰è¯­è¨€ç†è§£åŸºå‡†æµ‹è¯•ä¸­ä½¿ç”¨LLaVA-1.5å’ŒLLaVA-Nextæ¨¡å‹è¿›è¡Œå¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•æŒç»­ä¼˜äºå…ˆå‰æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> SCOPEæ–¹æ³•é€šè¿‡è”åˆä¼˜åŒ–æ˜¾è‘—æ€§å’Œè¦†ç›–åº¦ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰è§†è§‰ä»¤ç‰Œå‰ªæä¸­çš„è¯­ä¹‰ä¸å®Œæ•´é—®é¢˜ï¼Œä¸ºé«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›äº†æ–°çš„ä¼˜åŒ–æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) typically process a large number of
visual tokens, leading to considerable computational overhead, even though many
of these tokens are redundant. Existing visual token pruning methods primarily
focus on selecting the most salient tokens based on attention scores, resulting
in the semantic incompleteness of the selected tokens. In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness. Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships. We then define a
token-coverage gain for each unselected token, quantifying how much additional
coverage would be obtained by including it. By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score. We conduct extensive experiments on
multiple vision-language understanding benchmarks using the LLaVA-1.5 and
LLaVA-Next models. Experimental results demonstrate that our method
consistently outperforms prior approaches. Our code is available at
\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.</p>
<h3 id="18-training-free-source-attribution-of-ai-generated-images-via-resynthesis">[18] <a href="https://arxiv.org/abs/2510.24278">Training-free Source Attribution of AI-generated Images via Resynthesis</a></h3>
<p><em>Pietro Bongini, Valentina Molinari, Andrea Costanzo, Benedetta Tondi, Mauro Barni</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒé‡åˆæˆçš„å…è®­ç»ƒå•æ ·æœ¬å½’å› æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆæè¿°å›¾åƒæç¤ºè¯å¹¶åœ¨å€™é€‰ç”Ÿæˆå™¨ä¸Šé‡åˆæˆå›¾åƒï¼Œåœ¨ç‰¹å¾ç©ºé—´ä¸­æ¯”è¾ƒä¸åŸå›¾çš„ç›¸ä¼¼åº¦è¿›è¡Œå½’å› ã€‚è¯¥æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰å°‘æ ·æœ¬æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åˆæˆå›¾åƒå½’å› æ•°æ®é›†ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åˆæˆå›¾åƒæ¥æºå½’å› åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›çš„æƒ…å†µä¸‹ã€‚ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒæ ·æœ¬æœ‰é™æ—¶æ€§èƒ½å—é™ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿåœ¨æå°‘è®­ç»ƒæ•°æ®ä¸‹æœ‰æ•ˆå·¥ä½œçš„å½’å› æŠ€æœ¯ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŸºäºå›¾åƒé‡åˆæˆçš„å…è®­ç»ƒå•æ ·æœ¬å½’å› æ–¹æ³•ï¼šé¦–å…ˆç”Ÿæˆå¾…åˆ†æå›¾åƒçš„æè¿°æç¤ºè¯ï¼Œç„¶åä½¿ç”¨æ‰€æœ‰å€™é€‰ç”Ÿæˆå™¨é‡åˆæˆè¯¥å›¾åƒï¼Œåœ¨é€‚å½“çš„ç‰¹å¾ç©ºé—´ä¸­æ¯”è¾ƒé‡åˆæˆå›¾åƒä¸åŸå›¾çš„ç›¸ä¼¼åº¦ï¼Œå°†å›¾åƒå½’å› äºäº§ç”Ÿæœ€æ¥è¿‘é‡åˆæˆçš„æ¨¡å‹ã€‚åŒæ—¶æ„å»ºäº†åŒ…å«å•†ä¸šå’Œå¼€æºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå™¨çš„äººè„¸å›¾åƒå½’å› æ•°æ®é›†ã€‚</p>
<p><strong>Result:</strong> åœ¨æå‡ºçš„æ–°æ•°æ®é›†ä¸Šï¼Œé‡åˆæˆæ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„å°‘æ ·æœ¬æ–¹æ³•å’Œå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæˆ–å¾®è°ƒæ ·æœ¬æå°‘çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒè¯æ˜è¯¥æ•°æ®é›†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸ºå¼€å‘å’Œè¯„ä¼°æœªæ¥å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬æ–¹æ³•æä¾›äº†æœ‰ä»·å€¼çš„åŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> åŸºäºé‡åˆæˆçš„å…è®­ç»ƒå½’å› æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºåˆæˆå›¾åƒæ¥æºæ£€æµ‹æä¾›äº†æ–°çš„æœ‰æ•ˆé€”å¾„ã€‚æ–°æ„å»ºçš„æ•°æ®é›†ä¸ºå½’å› æ–¹æ³•å¼€å‘æä¾›äº†æ ‡å‡†åŒ–æµ‹è¯•å¹³å°ï¼Œæ¨åŠ¨äº†å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å½’å› æŠ€æœ¯çš„ç ”ç©¶è¿›å±•ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Synthetic image source attribution is a challenging task, especially in data
scarcity conditions requiring few-shot or zero-shot classification
capabilities. We present a new training-free one-shot attribution method based
on image resynthesis. A prompt describing the image under analysis is
generated, then it is used to resynthesize the image with all the candidate
sources. The image is attributed to the model which produced the resynthesis
closest to the original image in a proper feature space. We also introduce a
new dataset for synthetic image attribution consisting of face images from
commercial and open-source text-to-image generators. The dataset provides a
challenging attribution framework, useful for developing new attribution models
and testing their capabilities on different generative architectures. The
dataset structure allows to test approaches based on resynthesis and to compare
them to few-shot methods. Results from state-of-the-art few-shot approaches and
other baselines show that the proposed resynthesis method outperforms existing
techniques when only a few samples are available for training or fine-tuning.
The experiments also demonstrate that the new dataset is a challenging one and
represents a valuable benchmark for developing and evaluating future few-shot
and zero-shot methods.</p>
<h3 id="19-deshadowmamba-deshadowing-as-1d-sequential-similarity">[19] <a href="https://arxiv.org/abs/2510.24260">DeshadowMamba: Deshadowing as 1D Sequential Similarity</a></h3>
<p><em>Zhaotong Yang, Yi Chen, Yanying Li, Shengfeng He, Yangyang Xu, Junyu Dong, Jian Yang, Yong Du</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºDeshadowMambaï¼Œä¸€ç§åŸºäºé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹çš„å›¾åƒé˜´å½±å»é™¤æ–¹æ³•ï¼Œé€šè¿‡CrossGateæ–¹å‘è°ƒåˆ¶æœºåˆ¶å’ŒColorShiftæ­£åˆ™åŒ–ï¼Œè§£å†³äº†ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶åœ¨é˜´å½±å»é™¤ä¸­æ··åˆæ— å…³åŒºåŸŸå…‰ç…§çº¿ç´¢å¯¼è‡´çš„ç»“æ„æ‰­æ›²å’Œé¢œè‰²ä¸ä¸€è‡´é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºæ³¨æ„åŠ›çš„æ·±åº¦å›¾åƒé˜´å½±å»é™¤æ¨¡å‹å­˜åœ¨å›ºå®šæ³¨æ„åŠ›æ¨¡å¼å®¹æ˜“æ··åˆæ— å…³åŒºåŸŸçš„å…‰ç…§çº¿ç´¢ï¼Œå¯¼è‡´ç»“æ„æ‰­æ›²å’Œé¢œè‰²ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹æ³•æ¥ä¿æŒç»“æ„å®Œæ•´æ€§å’Œè‰²å½©ä¸€è‡´æ€§ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨Mambaé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹è¿›è¡Œåºåˆ—å»ºæ¨¡ï¼Œæå‡ºCrossGateæ–¹å‘è°ƒåˆ¶æœºåˆ¶å°†é˜´å½±æ„ŸçŸ¥ç›¸ä¼¼æ€§æ³¨å…¥è¾“å…¥é—¨å®ç°ç›¸å…³ä¸Šä¸‹æ–‡çš„é€‰æ‹©æ€§æ•´åˆï¼Œå¹¶å¼•å…¥åŸºäºå…¨å±€é¢œè‰²ç»Ÿè®¡çš„ColorShiftå¯¹æ¯”å­¦ä¹ æ­£åˆ™åŒ–æ¥æŠ‘åˆ¶é¢œè‰²æ±¡æŸ“ã€‚</p>
<p><strong>Result:</strong> åœ¨å…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDeshadowMambaåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æ€§èƒ½æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ï¼Œå®ç°äº†é²æ£’çš„é¢œè‰²æ¢å¤å’Œç»“æ„ä¿æŒã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†åºåˆ—å»ºæ¨¡åœ¨é˜´å½±å»é™¤ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ï¼Œé€šè¿‡æ–¹å‘çŠ¶æ€è½¬æ¢å’Œå¯¹æ¯”å­¦ä¹ æœºåˆ¶æœ‰æ•ˆè§£å†³äº†ç»“æ„å®Œæ•´æ€§å’Œè‰²å½©ä¸€è‡´æ€§çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºå›¾åƒä¿®å¤ä»»åŠ¡æä¾›äº†æ–°çš„å»ºæ¨¡æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Recent deep models for image shadow removal often rely on attention-based
architectures to capture long-range dependencies. However, their fixed
attention patterns tend to mix illumination cues from irrelevant regions,
leading to distorted structures and inconsistent colors. In this work, we
revisit shadow removal from a sequence modeling perspective and explore the use
of Mamba, a selective state space model that propagates global context through
directional state transitions. These transitions yield an efficient global
receptive field while preserving positional continuity. Despite its potential,
directly applying Mamba to image data is suboptimal, since it lacks awareness
of shadow-non-shadow semantics and remains susceptible to color interference
from nearby regions. To address these limitations, we propose CrossGate, a
directional modulation mechanism that injects shadow-aware similarity into
Mamba's input gate, allowing selective integration of relevant context along
transition axes. To further ensure appearance fidelity, we introduce ColorShift
regularization, a contrastive learning objective driven by global color
statistics. By synthesizing structured informative negatives, it guides the
model to suppress color contamination and achieve robust color restoration.
Together, these components adapt sequence modeling to the structural integrity
and chromatic consistency required for shadow removal. Extensive experiments on
public benchmarks demonstrate that DeshadowMamba achieves state-of-the-art
visual quality and strong quantitative performance.</p>
<h3 id="20-few-shot-remote-sensing-image-scene-classification-with-clip-and-prompt-learning">[20] <a href="https://arxiv.org/abs/2510.24321">Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning</a></h3>
<p><em>Ivica Dimitrovski, Vlatko Spasev, Ivan Kitanovski</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ç³»ç»Ÿæ¢ç´¢äº†æç¤ºå­¦ä¹ ä½œä¸ºé¥æ„Ÿå›¾åƒåœºæ™¯åˆ†ç±»çš„é«˜æ•ˆé€‚åº”ç­–ç•¥ï¼Œåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿æ–¹æ³•ï¼Œå…¶ä¸­å¸¦è‡ªçº¦æŸçš„æç¤ºå­¦ä¹ æ–¹æ³•åœ¨è·¨åŸŸæ³›åŒ–æ–¹é¢è¡¨ç°æœ€ä¸ºé²æ£’ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é¥æ„Ÿåº”ç”¨ä¸­çš„æ·±åº¦å­¦ä¹ æ€§èƒ½å—åˆ°æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œè·¨åŸŸæ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é™åˆ¶ï¼Œè€Œç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPç›´æ¥åº”ç”¨äºé¥æ„Ÿé¢†åŸŸå­˜åœ¨æ˜¾è‘—çš„é¢†åŸŸå·®è·å’Œä»»åŠ¡è¯­ä¹‰é€‚åº”éœ€æ±‚ä¸è¶³çš„é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶è¯„ä¼°äº†å¤šç§ä»£è¡¨æ€§æç¤ºå­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€æ¡ä»¶ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€å¤šæ¨¡æ€æç¤ºå­¦ä¹ ä»¥åŠå¸¦è‡ªçº¦æŸçš„æç¤ºå­¦ä¹ ï¼Œè¿™äº›æ–¹æ³•æ¶µç›–äº†ä»é™æ€ä¸Šä¸‹æ–‡ä¼˜åŒ–åˆ°æ¡ä»¶æç¤ºå¢å¼ºæ³›åŒ–ã€å¤šæ¨¡æ€è”åˆé€‚åº”ä»¥åŠè¯­ä¹‰æ­£åˆ™åŒ–ç¨³å®šå­¦ä¹ çš„è®¾è®¡ç†å¿µã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªé¥æ„ŸåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæç¤ºå­¦ä¹ æ–¹æ³•åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹æŒç»­ä¼˜äºé›¶æ ·æœ¬CLIPå’ŒåŸºäºå†»ç»“ç‰¹å¾çš„çº¿æ€§æ¢æµ‹åŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨æ•°æ®é›†æ³›åŒ–æµ‹è¯•ä¸­ï¼Œå¸¦è‡ªçº¦æŸçš„æç¤ºå­¦ä¹ æ–¹æ³•å®ç°äº†æœ€é²æ£’çš„è·¨åŸŸæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœå¼ºè°ƒäº†æç¤ºå­¦ä¹ ä½œä¸ºè¿æ¥å«æ˜Ÿå’Œèˆªç©ºå½±åƒé¢†åŸŸå·®è·çš„å¯æ‰©å±•é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä¸ºæœªæ¥è¯¥é¢†åŸŸç ”ç©¶æä¾›äº†åšå®åŸºç¡€ï¼Œå±•ç¤ºäº†è½»é‡çº§é€‚åº”ç­–ç•¥åœ¨é¥æ„Ÿåœºæ™¯åˆ†ç±»ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Remote sensing applications increasingly rely on deep learning for scene
classification. However, their performance is often constrained by the scarcity
of labeled data and the high cost of annotation across diverse geographic and
sensor domains. While recent vision-language models like CLIP have shown
promise by learning transferable representations at scale by aligning visual
and textual modalities, their direct application to remote sensing remains
suboptimal due to significant domain gaps and the need for task-specific
semantic adaptation. To address this critical challenge, we systematically
explore prompt learning as a lightweight and efficient adaptation strategy for
few-shot remote sensing image scene classification. We evaluate several
representative methods, including Context Optimization, Conditional Context
Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating
Constraints. These approaches reflect complementary design philosophies: from
static context optimization to conditional prompts for enhanced generalization,
multi-modal prompts for joint vision-language adaptation, and semantically
regularized prompts for stable learning without forgetting. We benchmark these
prompt-learning methods against two standard baselines: zero-shot CLIP with
hand-crafted prompts and a linear probe trained on frozen CLIP features.
Through extensive experiments on multiple benchmark remote sensing datasets,
including cross-dataset generalization tests, we demonstrate that prompt
learning consistently outperforms both baselines in few-shot scenarios.
Notably, Prompting with Self-Regulating Constraints achieves the most robust
cross-domain performance. Our findings underscore prompt learning as a scalable
and efficient solution for bridging the domain gap in satellite and aerial
imagery, providing a strong foundation for future research in this field.</p>
<h3 id="21-when-are-radiology-reports-useful-for-training-medical-image-classifiers">[21] <a href="https://arxiv.org/abs/2510.24385">When are radiology reports useful for training medical image classifiers?</a></h3>
<p><em>Herman BergstrÃ¶m, Zhongqi Yue, Fredrik D. Johansson</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†æ”¾å°„å­¦æŠ¥å‘Šåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»è®­ç»ƒä¸­çš„ä½¿ç”¨æ—¶æœºå’Œæ–¹æ³•ï¼Œå‘ç°åœ¨æ ‡ç­¾ä¸æ–‡æœ¬å¼ºç›¸å…³æ—¶é¢„è®­ç»ƒæœ‰ç›Šï¼Œè€Œåœ¨å¼±ç›¸å…³æ—¶å¯èƒ½æœ‰å®³ï¼ŒåŒæ—¶å¾®è°ƒé˜¶æ®µä½¿ç”¨æŠ¥å‘Šèƒ½å¸¦æ¥æ˜¾è‘—æ”¹è¿›ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŒ»å­¦å›¾åƒåˆ†ç±»ç ”ç©¶ä¸»è¦ä¾èµ–æ”¾å°„å­¦æŠ¥å‘Šä½œä¸ºä¸“å®¶æ³¨é‡Šï¼Œä½†å®é™…åº”ç”¨ä¸­éœ€è¦æ”¾å°„ç§‘åŒ»ç”Ÿæ‰‹åŠ¨æ’°å†™æŠ¥å‘Šï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šä½•æ—¶ä»¥åŠå¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆåˆ©ç”¨æ”¾å°„å­¦æŠ¥å‘Šæ¥æ”¹è¿›ä»…åŸºäºå›¾åƒçš„åˆ†ç±»æ€§èƒ½ã€‚ç°æœ‰ç ”ç©¶å±€é™äºä½¿ç”¨é¢„è®­ç»ƒå›¾åƒè¡¨ç¤ºè¿›è¡Œå¾®è°ƒï¼Œå¿½ç•¥äº†æ ‡ç­¾ä¸æ–‡æœ¬å¼±å…³è”çš„ä»»åŠ¡åœºæ™¯ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶é‡‡ç”¨ç³»ç»Ÿæ€§å®éªŒè®¾è®¡ï¼Œè€ƒå¯Ÿæ”¾å°„å­¦æŠ¥å‘Šåœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µçš„ä½¿ç”¨ç­–ç•¥ï¼Œè¦†ç›–è¯Šæ–­æ€§å’Œé¢„åæ€§ä»»åŠ¡ï¼ˆå¦‚12ä¸ªæœˆå†å…¥é™¢é¢„æµ‹ï¼‰ï¼Œå¹¶åœ¨ä¸åŒè®­ç»ƒé›†è§„æ¨¡ä¸‹è¿›è¡Œè¯„ä¼°ã€‚é‡ç‚¹æ¯”è¾ƒäº†æ˜¾å¼å›¾åƒ-æ–‡æœ¬å¯¹é½é¢„è®­ç»ƒæ–¹æ³•åœ¨ä¸åŒä»»åŠ¡åœºæ™¯ä¸‹çš„æ•ˆæœã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼šå½“æ ‡ç­¾ä¸æ–‡æœ¬å†…å®¹å¼ºç›¸å…³æ—¶ï¼Œé¢„è®­ç»ƒé˜¶æ®µåˆ©ç”¨æŠ¥å‘Šèƒ½æå‡ä¸‹æ¸¸åˆ†ç±»æ€§èƒ½ï¼›ä½†åœ¨æ ‡ç­¾ä¸æ–‡æœ¬å¼±å…³è”çš„åœºæ™¯ä¸­ï¼Œæ˜¾å¼å›¾åƒ-æ–‡æœ¬å¯¹é½é¢„è®­ç»ƒåè€Œä¼šäº§ç”Ÿè´Ÿé¢å½±å“ã€‚æ­¤å¤–ï¼Œå¾®è°ƒé˜¶æ®µä½¿ç”¨æŠ¥å‘Šèƒ½å¸¦æ¥æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨æŸäº›è®¾ç½®ä¸‹å…¶å½±å“ç”šè‡³è¶…è¿‡é¢„è®­ç»ƒæ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ä¸ºåŒ»å­¦å›¾åƒåˆ†ç±»å™¨è®­ç»ƒä¸­å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨ç‰¹æƒæ–‡æœ¬æ•°æ®æä¾›äº†å…·ä½“æŒ‡å¯¼ï¼Œå¼ºè°ƒäº†æ ¹æ®ä»»åŠ¡ç‰¹æ€§é€‰æ‹©é€‚å½“è®­ç»ƒç­–ç•¥çš„é‡è¦æ€§ï¼ŒåŒæ—¶æŒ‡å‡ºäº†å½“å‰ç ”ç©¶åœ¨æ ‡ç­¾-æ–‡æœ¬å…³è”åº¦å½±å“æ–¹é¢çš„è®¤çŸ¥ç©ºç™½ï¼Œä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Medical images used to train machine learning models are often accompanied by
radiology reports containing rich expert annotations. However, relying on these
reports as inputs for clinical prediction requires the timely manual work of a
trained radiologist. This raises a natural question: when can radiology reports
be leveraged during training to improve image-only classification? Prior works
are limited to evaluating pre-trained image representations by fine-tuning them
to predict diagnostic labels, often extracted from reports, ignoring tasks with
labels that are weakly associated with the text. To address this gap, we
conduct a systematic study of how radiology reports can be used during both
pre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,
12-month readmission), and under varying training set sizes. Our findings
reveal that: (1) Leveraging reports during pre-training is beneficial for
downstream classification tasks where the label is well-represented in the
text; however, pre-training through explicit image-text alignment can be
detrimental in settings where it's not; (2) Fine-tuning with reports can lead
to significant improvements and even have a larger impact than the pre-training
method in certain settings. These results provide actionable insights into when
and how to leverage privileged text data to train medical image classifiers
while highlighting gaps in current research.</p>
<h3 id="22-does-object-binding-naturally-emerge-in-large-pretrained-vision-transformers">[22] <a href="https://arxiv.org/abs/2510.24709">Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?</a></h3>
<p><em>Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æ­ç¤ºäº†è‡ªç›‘ç£é¢„è®­ç»ƒçš„è§†è§‰Transformerï¼ˆViTï¼‰èƒ½å¤Ÿè‡ªç„¶æ¶Œç°å‡ºç‰©ä½“ç»‘å®šèƒ½åŠ›ï¼Œé€šè¿‡ç›¸ä¼¼æ€§æ¢é’ˆè§£ç IsSameObjectå±æ€§ï¼Œå‡†ç¡®ç‡è¶…è¿‡90%ï¼ŒæŒ‘æˆ˜äº†ViTç¼ºä¹ç‰©ä½“ç»‘å®šçš„ä¼ ç»Ÿè§‚ç‚¹ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å…ˆå‰å·¥ä½œå¸¸é€šè¿‡æ˜¾å¼æ–½åŠ ç‰©ä½“ä¸­å¿ƒæ³¨æ„åŠ›æ¥æ¢ç´¢ç‰©ä½“ç»‘å®šçš„ç›Šå¤„ï¼Œä½†å°šä¸æ¸…æ¥šè¿™ç§èƒ½åŠ›æ˜¯å¦åœ¨é¢„è®­ç»ƒçš„è§†è§‰Transformerä¸­è‡ªç„¶æ¶Œç°ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶ViTæ˜¯å¦èƒ½å¤Ÿè‡ªå‘åœ°å­¦ä¹ è¯†åˆ«å“ªäº›å›¾åƒå—å±äºåŒä¸€ç‰©ä½“ï¼Œä»¥åŠè¿™ç§èƒ½åŠ›å¦‚ä½•å—åˆ°ä¸åŒé¢„è®­ç»ƒç›®æ ‡çš„å½±å“ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨ç›¸ä¼¼æ€§æ¢é’ˆä»ViTå„å±‚çš„å›¾åƒå—åµŒå…¥ä¸­è§£ç IsSameObjectå±æ€§ï¼Œåˆ†æè‡ªç›‘ç£æ¨¡å‹ï¼ˆDINOã€MAEã€CLIPï¼‰ä¸ImageNetç›‘ç£æ¨¡å‹åœ¨ç‰©ä½“ç»‘å®šèƒ½åŠ›ä¸Šçš„å·®å¼‚ï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒéªŒè¯IsSameObjectä¿¡å·å¯¹æ³¨æ„åŠ›æœºåˆ¶çš„å¼•å¯¼ä½œç”¨ã€‚</p>
<p><strong>Result:</strong> ç›¸ä¼¼æ€§æ¢é’ˆåœ¨è§£ç IsSameObjectå±æ€§æ—¶è¾¾åˆ°è¶…è¿‡90%çš„å‡†ç¡®ç‡ï¼Œä¸”è¯¥èƒ½åŠ›åœ¨è‡ªç›‘ç£ViTä¸­å¯é æ¶Œç°ï¼Œè€Œåœ¨ImageNetç›‘ç£æ¨¡å‹ä¸­æ˜¾è‘—è¾ƒå¼±ã€‚IsSameObjectè¢«ç¼–ç åœ¨ç‰©ä½“ç‰¹å¾ä¹‹ä¸Šçš„ä½ç»´å­ç©ºé—´ä¸­ï¼Œå¹¶ä¸»åŠ¨å¼•å¯¼æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¶ˆèè¯¥ä¿¡å·ä¼šé™ä½ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜ç‰©ä½“ç»‘å®šèƒ½åŠ›å¹¶éViTæ¶æ„çš„ç®€å•äº§ç‰©ï¼Œè€Œæ˜¯é€šè¿‡ç‰¹å®šé¢„è®­ç»ƒç›®æ ‡ä¹ å¾—çš„èƒ½åŠ›ã€‚è¿™ç§æ¶Œç°çš„ç¬¦å·çŸ¥è¯†æŒ‘æˆ˜äº†è¿æ¥ä¸»ä¹‰ç³»ç»Ÿç¼ºä¹ç»“æ„åŒ–è¡¨å¾çš„ä¼ ç»Ÿè§‚ç‚¹ï¼Œæ­ç¤ºäº†ViTèƒ½å¤Ÿè‡ªç„¶å­¦ä¹ "å“ªäº›éƒ¨åˆ†å±äºä¸€èµ·"çš„æŠ½è±¡æ¦‚å¿µã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Object binding, the brain's ability to bind the many features that
collectively represent an object into a coherent whole, is central to human
cognition. It groups low-level perceptual features into high-level object
representations, stores those objects efficiently and compositionally in
memory, and supports human reasoning about individual object instances. While
prior work often imposes object-centric attention (e.g., Slot Attention)
explicitly to probe these benefits, it remains unclear whether this ability
naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they
could: recognizing which patches belong to the same object should be useful for
downstream prediction and thus guide attention. Motivated by the quadratic
nature of self-attention, we hypothesize that ViTs represent whether two
patches belong to the same object, a property we term IsSameObject. We decode
IsSameObject from patch embeddings across ViT layers using a similarity probe,
which reaches over 90% accuracy. Crucially, this object-binding capability
emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker
in ImageNet-supervised models, suggesting that binding is not a trivial
architectural artifact, but an ability acquired through specific pretraining
objectives. We further discover that IsSameObject is encoded in a
low-dimensional subspace on top of object features, and that this signal
actively guides attention. Ablating IsSameObject from model activations
degrades downstream performance and works against the learning objective,
implying that emergent object binding naturally serves the pretraining
objective. Our findings challenge the view that ViTs lack object binding and
highlight how symbolic knowledge of "which parts belong together" emerges
naturally in a connectionist system.</p>
<h3 id="23-osworld-mcp-benchmarking-mcp-tool-invocation-in-computer-use-agents">[23] <a href="https://arxiv.org/abs/2510.24563">OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents</a></h3>
<p><em>Hongrui Jia, Jitong Liao, Xi Zhang, Haiyang Xu, Tianbao Xie, Chaoya Jiang, Ming Yan, Si Liu, Wei Ye, Fei Huang</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†OSWorld-MCPï¼Œè¿™æ˜¯é¦–ä¸ªå…¨é¢ä¸”å…¬å¹³çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºåœ¨çœŸå®ç¯å¢ƒä¸­è¯„ä¼°è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„å·¥å…·è°ƒç”¨ã€GUIæ“ä½œå’Œå†³ç­–èƒ½åŠ›ï¼Œå¡«è¡¥äº†å¤šæ¨¡æ€ä»£ç†å·¥å…·è°ƒç”¨èƒ½åŠ›è¯„ä¼°çš„ç©ºç™½ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€ä»£ç†è¯„ä¼°ä¸»è¦å…³æ³¨GUIäº¤äº’æŠ€èƒ½ï¼Œè€Œç”±æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å®ç°çš„å·¥å…·è°ƒç”¨èƒ½åŠ›è¢«ä¸¥é‡å¿½è§†ï¼Œå¯¼è‡´é›†æˆå·¥å…·è°ƒç”¨çš„ä»£ç†ä¸ä»…è¯„ä¼°GUIäº¤äº’çš„ä»£ç†ä¹‹é—´å­˜åœ¨ä¸å…¬å¹³æ¯”è¾ƒã€‚</p>
<p><strong>Method:</strong> è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„è‡ªåŠ¨ä»£ç ç”Ÿæˆæµæ°´çº¿æ¥åˆ›å»ºå·¥å…·ï¼Œå¹¶ç»“åˆç°æœ‰å·¥å…·çš„ç²¾é€‰é›†åˆï¼Œé€šè¿‡ä¸¥æ ¼çš„äººå·¥éªŒè¯äº§ç”Ÿäº†158ä¸ªé«˜è´¨é‡å·¥å…·ï¼Œæ¶µç›–7ä¸ªå¸¸è§åº”ç”¨ç¨‹åºï¼Œæ¯ä¸ªå·¥å…·éƒ½éªŒè¯äº†åŠŸèƒ½æ­£ç¡®æ€§ã€å®ç”¨æ€§å’Œå¤šåŠŸèƒ½æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨OSWorld-MCPä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒMCPå·¥å…·æ™®éæé«˜äº†ä»»åŠ¡æˆåŠŸç‡ï¼ˆä¾‹å¦‚OpenAI o3åœ¨15æ­¥æ—¶ä»8.3%æå‡è‡³20.4%ï¼ŒClaude 4 Sonnetåœ¨50æ­¥æ—¶ä»40.1%æå‡è‡³43.3%ï¼‰ï¼Œä½†å³ä½¿æ˜¯æœ€å¼ºæ¨¡å‹çš„å·¥å…·è°ƒç”¨ç‡ä¹Ÿç›¸å¯¹è¾ƒä½ï¼Œä»…ä¸º36.3%ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡æ˜ç¡®æµ‹é‡MCPå·¥å…·ä½¿ç”¨æŠ€èƒ½ï¼ŒOSWorld-MCPåŠ æ·±äº†å¯¹å¤šæ¨¡æ€ä»£ç†çš„ç†è§£ï¼Œå¹¶ä¸ºè¯„ä¼°å¤æ‚å·¥å…·è¾…åŠ©ç¯å¢ƒä¸­çš„æ€§èƒ½è®¾ç«‹äº†æ–°æ ‡å‡†ï¼ŒåŒæ—¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å·¥å…·è°ƒç”¨èƒ½åŠ›æ–¹é¢ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>With advances in decision-making and reasoning capabilities, multimodal
agents show strong potential in computer application scenarios. Past
evaluations have mainly assessed GUI interaction skills, while tool invocation
abilities, such as those enabled by the Model Context Protocol (MCP), have been
largely overlooked. Comparing agents with integrated tool invocation to those
evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,
the first comprehensive and fair benchmark for assessing computer-use agents'
tool invocation, GUI operation, and decision-making abilities in a real-world
environment. We design a novel automated code-generation pipeline to create
tools and combine them with a curated selection from existing tools. Rigorous
manual validation yields 158 high-quality tools (covering 7 common
applications), each verified for correct functionality, practical
applicability, and versatility. Extensive evaluations of state-of-the-art
multimodal agents on OSWorld-MCP show that MCP tools generally improve task
success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%
to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of
assessing tool invocation capabilities. However, even the strongest models have
relatively low tool invocation rates, Only 36.3%, indicating room for
improvement and highlighting the benchmark's challenge. By explicitly measuring
MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents
and sets a new standard for evaluating performance in complex, tool-assisted
environments. Our code, environment, and data are publicly available at
https://osworld-mcp.github.io.</p>
<h3 id="24-a-dual-branch-cnn-for-robust-detection-of-ai-generated-facial-forgeries">[24] <a href="https://arxiv.org/abs/2510.24640">A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries</a></h3>
<p><em>Xin Zhang, Yuqi Song, Fei Zuo</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„åŒåˆ†æ”¯å·ç§¯ç¥ç»ç½‘ç»œç”¨äºäººè„¸ä¼ªé€ æ£€æµ‹ï¼Œé€šè¿‡ç»“åˆç©ºé—´åŸŸå’Œé¢‘åŸŸçš„äº’è¡¥çº¿ç´¢ï¼Œåœ¨DiFFåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²å¹¶è¶…è¶Šäººç±»å¹³å‡å‡†ç¡®ç‡ï¼Œä¸ºAIå®‰å…¨ç”Ÿæ€ç³»ç»Ÿæä¾›æœ‰æ•ˆçš„è§†è§‰ä¼ªé€ é˜²å¾¡æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç”Ÿæˆå¼AIçš„å¿«é€Ÿå‘å±•ä½¿å¾—ä¼ªé€ äººè„¸å›¾åƒå˜å¾—é«˜åº¦é€¼çœŸï¼Œå¯¹AIå®‰å…¨ã€æ•°å­—åª’ä½“å®Œæ•´æ€§å’Œå…¬ä¼—ä¿¡ä»»æ„æˆä¸¥é‡å¨èƒã€‚å½“å‰äººè„¸ä¼ªé€ æŠ€æœ¯åŒ…æ‹¬äººè„¸äº¤æ¢ã€å±æ€§ç¼–è¾‘å’ŒåŸºäºæ‰©æ•£çš„å›¾åƒåˆæˆç­‰æ–¹æ³•ï¼Œæ­£è¢«æ¶æ„ç”¨äºè™šå‡ä¿¡æ¯ã€èº«ä»½æ¬ºè¯ˆå’Œè¯½è°¤ç­‰ç›®çš„ï¼ŒäºŸéœ€å¼€å‘é²æ£’ä¸”æ³›åŒ–æ€§å¼ºçš„äººè„¸ä¼ªé€ æ£€æµ‹æ–¹æ³•ä½œä¸ºAIå®‰å…¨åŸºç¡€è®¾æ–½çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚</p>
<p><strong>Method:</strong> æå‡ºä¸€ç§æ–°é¢–çš„åŒåˆ†æ”¯å·ç§¯ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­RGBåˆ†æ”¯æ•è·è¯­ä¹‰ä¿¡æ¯ï¼Œé¢‘ç‡åˆ†æ”¯ä¸“æ³¨äºç”Ÿæˆæ¨¡å‹éš¾ä»¥æŠ‘åˆ¶çš„é«˜é¢‘ä¼ªå½±ã€‚å¼•å…¥é€šé“æ³¨æ„åŠ›æ¨¡å—è‡ªé€‚åº”èåˆè¿™äº›å¼‚æ„ç‰¹å¾ï¼Œçªå‡ºæœ€å…·ä¿¡æ¯é‡çš„ä¼ªé€ åˆ¤åˆ«é€šé“ã€‚è®¾è®¡ç»Ÿä¸€çš„FSCæŸå¤±å‡½æ•°ï¼Œç»“åˆç„¦ç‚¹æŸå¤±ã€ç›‘ç£å¯¹æ¯”æŸå¤±å’Œé¢‘ç‡ä¸­å¿ƒè¾¹ç•ŒæŸå¤±ï¼Œä»¥å¢å¼ºç±»åˆ«å¯åˆ†æ€§å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨åŒ…å«æ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€äººè„¸äº¤æ¢å’Œäººè„¸ç¼–è¾‘å››ç§ä»£è¡¨æ€§æ–¹æ³•ç”Ÿæˆçš„ä¼ªé€ å›¾åƒçš„DiFFåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰ç±»åˆ«ä¸Šå‡è¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ï¼Œè¶…è¶Šäº†äººç±»å¹³å‡å‡†ç¡®ç‡ã€‚å®éªŒç»“æœéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå¯¹è§†è§‰ä¼ªé€ æ”»å‡»çš„é˜²å¾¡æ½œåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆç©ºé—´å’Œé¢‘åŸŸçº¿ç´¢çš„åŒåˆ†æ”¯æ¶æ„åœ¨æ£€æµ‹ç”Ÿæˆå¼AIä¼ªé€ å†…å®¹æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºæ›´å®‰å…¨çš„AIç”Ÿæ€ç³»ç»Ÿæä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚æ¨¡å‹åœ¨å¤šç§ä¼ªé€ æŠ€æœ¯ä¸Šçš„æ³›åŒ–èƒ½åŠ›è¡¨æ˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œæœ‰åŠ©äºåº”å¯¹æ—¥ç›Šå¤æ‚çš„è§†è§‰ä¼ªé€ å¨èƒã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>The rapid advancement of generative AI has enabled the creation of highly
realistic forged facial images, posing significant threats to AI security,
digital media integrity, and public trust. Face forgery techniques, ranging
from face swapping and attribute editing to powerful diffusion-based image
synthesis, are increasingly being used for malicious purposes such as
misinformation, identity fraud, and defamation. This growing challenge
underscores the urgent need for robust and generalizable face forgery detection
methods as a critical component of AI security infrastructure. In this work, we
propose a novel dual-branch convolutional neural network for face forgery
detection that leverages complementary cues from both spatial and frequency
domains. The RGB branch captures semantic information, while the frequency
branch focuses on high-frequency artifacts that are difficult for generative
models to suppress. A channel attention module is introduced to adaptively fuse
these heterogeneous features, highlighting the most informative channels for
forgery discrimination. To guide the network's learning process, we design a
unified loss function, FSC Loss, that combines focal loss, supervised
contrastive loss, and a frequency center margin loss to enhance class
separability and robustness. We evaluate our model on the DiFF benchmark, which
includes forged images generated from four representative methods:
text-to-image, image-to-image, face swap, and face edit. Our method achieves
strong performance across all categories and outperforms average human
accuracy. These results demonstrate the model's effectiveness and its potential
contribution to safeguarding AI ecosystems against visual forgery attacks.</p>
<h3 id="25-sage-structure-aware-generative-video-transitions-between-diverse-clips">[25] <a href="https://arxiv.org/abs/2510.24667">SAGE: Structure-Aware Generative Video Transitions between Diverse Clips</a></h3>
<p><em>Mia Kan, Yilin Liu, Niloy Mitra</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SAGEï¼ˆStructure-Aware Generative vidEo transitionsï¼‰ï¼Œä¸€ç§é›¶æ ·æœ¬çš„è§†é¢‘è¿‡æ¸¡æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆç»“æ„å¼•å¯¼å’Œç”Ÿæˆåˆæˆï¼Œåœ¨å¤šæ ·è§†é¢‘ç‰‡æ®µä¹‹é—´å®ç°å¹³æ»‘ã€è¯­ä¹‰ä¸€è‡´çš„è¿‡æ¸¡ã€‚è¯¥æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œç”¨æˆ·ç ”ç©¶ä¸­å‡ä¼˜äºç°æœ‰ç»å…¸å’Œç”ŸæˆåŸºçº¿æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘è¿‡æ¸¡æ–¹æ³•åœ¨å¤„ç†å…·æœ‰å¤§æ—¶é—´é—´éš”æˆ–æ˜¾è‘—è¯­ä¹‰å·®å¼‚çš„å¤šæ ·åŒ–è§†é¢‘ç‰‡æ®µæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¼ ç»ŸæŠ€æœ¯å¦‚äº¤å‰æ·¡å…¥æ·¡å‡ºã€å˜å½¢å’Œå¸§æ’å€¼ä»¥åŠæœ€è¿‘çš„ç”Ÿæˆä¸­é—´å¸§æ–¹æ³•éš¾ä»¥åœ¨ä¿æŒå†…å®¹æ„ŸçŸ¥å’Œè§†è§‰è¿è´¯æ€§çš„åŒæ—¶æ¡¥æ¥è¿™äº›å·®å¼‚ã€‚</p>
<p><strong>Method:</strong> SAGEé‡‡ç”¨é›¶æ ·æœ¬æ–¹æ³•ï¼Œç»“åˆç»“æ„å¼•å¯¼ï¼ˆé€šè¿‡çº¿æ¡†å›¾å’Œè¿åŠ¨æµæä¾›ï¼‰ä¸ç”Ÿæˆåˆæˆï¼Œæ— éœ€å¾®è°ƒå³å¯å®ç°å¹³æ»‘è¿‡æ¸¡ã€‚è¯¥æ–¹æ³•å€Ÿé‰´è‰ºæœ¯å·¥ä½œæµç¨‹ï¼Œé€šè¿‡å¯¹é½è½®å»“å’Œæ’å€¼æ˜¾è‘—ç‰¹å¾æ¥ä¿æŒç»“æ„å’Œæ„ŸçŸ¥è¿ç»­æ€§ã€‚</p>
<p><strong>Result:</strong> ä¸ç°æœ‰æ›¿ä»£æ–¹æ³•ï¼ˆFILMã€TVGã€DiffMorpherã€VACEã€GIï¼‰çš„å¹¿æ³›å®éªŒæ¯”è¾ƒè¡¨æ˜ï¼ŒSAGEåœ¨å®šé‡æŒ‡æ ‡å’Œç”¨æˆ·ç ”ç©¶ä¸­å‡ä¼˜äºç»å…¸å’Œç”ŸæˆåŸºçº¿æ–¹æ³•ï¼Œèƒ½å¤Ÿä¸ºå¤šæ ·åŒ–è§†é¢‘ç‰‡æ®µç”Ÿæˆæ›´ä¼˜è´¨çš„è¿‡æ¸¡æ•ˆæœã€‚</p>
<p><strong>Conclusion:</strong> SAGEé€šè¿‡ç»“æ„æ„ŸçŸ¥çš„ç”Ÿæˆæ–¹æ³•æˆåŠŸè§£å†³äº†å¤šæ ·åŒ–è§†é¢‘ç‰‡æ®µé—´çš„è¿‡æ¸¡æŒ‘æˆ˜ï¼Œè¯æ˜äº†ç»“åˆç»“æ„å¼•å¯¼ä¸ç”Ÿæˆåˆæˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºé›¶æ ·æœ¬è§†é¢‘è¿‡æ¸¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨ä¸“ä¸šè§†é¢‘åˆ¶ä½œä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Video transitions aim to synthesize intermediate frames between two clips,
but naive approaches such as linear blending introduce artifacts that limit
professional use or break temporal coherence. Traditional techniques
(cross-fades, morphing, frame interpolation) and recent generative inbetweening
methods can produce high-quality plausible intermediates, but they struggle
with bridging diverse clips involving large temporal gaps or significant
semantic differences, leaving a gap for content-aware and visually coherent
transitions. We address this challenge by drawing on artistic workflows,
distilling strategies such as aligning silhouettes and interpolating salient
features to preserve structure and perceptual continuity. Building on this, we
propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot
approach that combines structural guidance, provided via line maps and motion
flow, with generative synthesis, enabling smooth, semantically consistent
transitions without fine-tuning. Extensive experiments and comparison with
current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate
that SAGE outperforms both classical and generative baselines on quantitative
metrics and user studies for producing transitions between diverse clips. Code
to be released on acceptance.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="26-success-and-cost-elicit-convention-formation-for-efficient-communication">[26] <a href="https://arxiv.org/abs/2510.24023">Success and Cost Elicit Convention Formation for Efficient Communication</a></h3>
<p><em>Saujas Vaduguru, Yilun Hua, Yoav Artzi, Daniel Fried</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è®­ç»ƒå¤§å‹å¤šæ¨¡æ€æ¨¡å‹å½¢æˆè¯­è¨€çº¦å®šçš„æ–¹æ³•ï¼Œé€šè¿‡æ¨¡æ‹Ÿå‚è€ƒæ¸¸æˆä½¿æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œé«˜æ•ˆé€šä¿¡ã€‚è¯¥æ–¹æ³•åœ¨æ¶‰åŠç…§ç‰‡å’Œä¸ƒå·§æ¿å›¾åƒçš„é‡å¤å‚è€ƒæ¸¸æˆä¸­ï¼Œå°†æ¶ˆæ¯é•¿åº¦å‡å°‘è¾¾41%åŒæ—¶æé«˜æˆåŠŸç‡15%ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> äººç±»é€šè¿‡å…±äº«å¯¹è¯ä¸Šä¸‹æ–‡é€æ¸å½¢æˆé«˜æ•ˆé€šä¿¡èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å½¢æˆä¸´æ—¶è¯­è¨€çº¦å®šæ¥åè°ƒç®€çŸ­ã€ä½æˆæœ¬çš„è¡¨è¾¾ã€‚å½“å‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç¼ºä¹è¿™ç§é€šè¿‡ä¸Šä¸‹æ–‡å½¢æˆçº¦å®šçš„èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶é€šä¿¡æ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨æ¨¡æ‹Ÿå‚è€ƒæ¸¸æˆçš„æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¹‹é—´è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€é¢å¤–äººå·¥æ•°æ®ã€‚è¯¥æ–¹æ³•åŸºäºé‡å¤äº¤äº’ä¸­çš„æˆåŠŸç‡å’Œé€šä¿¡æˆæœ¬è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œä¿ƒä½¿æ¨¡å‹è‡ªå‘å½¢æˆé«˜æ•ˆé€šä¿¡çº¦å®šã€‚</p>
<p><strong>Result:</strong> åœ¨ç…§ç‰‡å’Œä¸ƒå·§æ¿å›¾åƒçš„å‚è€ƒæ¸¸æˆä¸­ï¼Œæ¨¡å‹é€šä¿¡æ¶ˆæ¯é•¿åº¦å‡å°‘é«˜è¾¾41%ï¼ŒåŒæ—¶äº¤äº’æˆåŠŸç‡æé«˜15%ã€‚äººç±»å¬ä¼—ä¸å½¢æˆçº¦å®šçš„æ¨¡å‹äº¤äº’æ—¶å“åº”é€Ÿåº¦æ›´å¿«ï¼Œè¯æ˜é€šä¿¡æ•ˆç‡æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Conclusion:</strong> ä»…åŸºäºæˆåŠŸç‡æˆ–é€šä¿¡æˆæœ¬çš„å•ä¸€è®­ç»ƒç›®æ ‡ä¸è¶³ä»¥æ¿€å‘çº¦å®šå½¢æˆï¼Œå¿…é¡»åŒæ—¶ä¼˜åŒ–ä¸¤è€…æ‰èƒ½å®ç°é«˜æ•ˆé€šä¿¡ã€‚è¯¥æ–¹æ³•ä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„é«˜æ•ˆäººæœºäº¤äº’æä¾›äº†æ–°é€”å¾„ï¼Œå±•ç¤ºäº†é€šè¿‡æ¨¡æ‹Ÿäº¤äº’å­¦ä¹ é€šä¿¡çº¦å®šçš„å¯è¡Œæ€§ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Humans leverage shared conversational context to become increasingly
successful and efficient at communicating over time. One manifestation of this
is the formation of ad hoc linguistic conventions, which allow people to
coordinate on short, less costly utterances that are understood using shared
conversational context. We present a method to train large multimodal models to
form conventions, enabling efficient communication. Our approach uses simulated
reference games between models, and requires no additional human-produced data.
In repeated reference games involving photographs and tangram images, our
method enables models to communicate efficiently with people: reducing the
message length by up to 41% while increasing success by 15% over the course of
the interaction. Human listeners respond faster when interacting with our model
that forms conventions. We also show that training based on success or cost
alone is insufficient - both are necessary to elicit convention formation.</p>
<h3 id="27-musag-a-multimodal-german-sarcasm-dataset-with-full-modal-annotations">[27] <a href="https://arxiv.org/abs/2510.24178">MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations</a></h3>
<p><em>Aaron Scott, Maike ZÃ¼fle, Jan Niehues</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†MuSaGï¼Œé¦–ä¸ªå¾·è¯­å¤šæ¨¡æ€è®½åˆºæ£€æµ‹æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªå¾·å›½ç”µè§†èŠ‚ç›®çš„33åˆ†é’Ÿäººå·¥æ ‡æ³¨æ•°æ®ï¼Œå¹¶è¯„ä¼°äº†å¤šç§æ¨¡å‹åœ¨æ–‡æœ¬ã€éŸ³é¢‘ã€è§†è§‰å’Œå¤šæ¨¡æ€è®¾ç½®ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è®½åˆºä½œä¸ºä¸€ç§å¤æ‚çš„æ¯”å–»è¯­è¨€å½¢å¼ï¼Œåœ¨ç¤¾äº¤åª’ä½“å’Œæµè¡Œæ–‡åŒ–ä¸­æ™®éå­˜åœ¨ï¼Œå¯¹è‡ªç„¶è¯­è¨€ç†è§£ã€æƒ…æ„Ÿåˆ†æå’Œå†…å®¹å®¡æ ¸æ„æˆäº†æŒç»­æŒ‘æˆ˜ã€‚éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼Œè®½åˆºæ£€æµ‹éœ€è¦è¶…è¶Šæ–‡æœ¬èŒƒå›´ï¼Œæ•´åˆæ¥è‡ªéŸ³é¢‘å’Œè§†è§‰çš„çº¿ç´¢ï¼Œè€Œå¾·è¯­é¢†åŸŸç¼ºä¹ç›¸åº”çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†MuSaGæ•°æ®é›†ï¼ŒåŒ…å«33åˆ†é’Ÿæ¥è‡ªå¾·å›½ç”µè§†èŠ‚ç›®çš„äººå·¥ç­›é€‰å’Œæ ‡æ³¨è¯­å¥ï¼Œæ¯ä¸ªå®ä¾‹æä¾›å¯¹é½çš„æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘æ¨¡æ€ï¼Œåˆ†åˆ«ç”±äººå·¥æ ‡æ³¨ã€‚ç ”ç©¶å¯¹ä¹ç§å¼€æºå’Œå•†ä¸šæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–æ–‡æœ¬ã€éŸ³é¢‘ã€è§†è§‰å’Œå¤šæ¨¡æ€æ¶æ„ï¼Œå¹¶å°†å®ƒä»¬çš„æ€§èƒ½ä¸äººå·¥æ ‡æ³¨è¿›è¡Œæ¯”è¾ƒã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œäººç±»åœ¨å¯¹è¯ç¯å¢ƒä¸­ä¸»è¦ä¾èµ–éŸ³é¢‘çº¿ç´¢è¿›è¡Œè®½åˆºæ£€æµ‹ï¼Œè€Œæ¨¡å‹åœ¨æ–‡æœ¬æ¨¡æ€ä¸Šè¡¨ç°æœ€ä½³ã€‚è¿™æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯æ–¹é¢å­˜åœ¨å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ©ç”¨éŸ³é¢‘çº¿ç´¢æ–¹é¢è¡¨ç°ä¸è¶³ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘æ›´é€‚åˆç°å®åœºæ™¯çš„å¤šæ¨¡æ€æ¨¡å‹çš„å¿…è¦æ€§ï¼ŒMuSaGæ•°æ®é›†çš„å‘å¸ƒå°†æ”¯æŒæœªæ¥åœ¨å¤šæ¨¡æ€è®½åˆºæ£€æµ‹å’Œäººæœºå¯¹é½æ–¹é¢çš„ç ”ç©¶ï¼Œä¸ºæ”¹è¿›å¤šæ¨¡æ€ç†è§£æ¨¡å‹æä¾›äº†é‡è¦åŸºå‡†å’Œèµ„æºã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Sarcasm is a complex form of figurative language in which the intended
meaning contradicts the literal one. Its prevalence in social media and popular
culture poses persistent challenges for natural language understanding,
sentiment analysis, and content moderation. With the emergence of multimodal
large language models, sarcasm detection extends beyond text and requires
integrating cues from audio and vision. We present MuSaG, the first German
multimodal sarcasm detection dataset, consisting of 33 minutes of manually
selected and human-annotated statements from German television shows. Each
instance provides aligned text, audio, and video modalities, annotated
separately by humans, enabling evaluation in unimodal and multimodal settings.
We benchmark nine open-source and commercial models, spanning text, audio,
vision, and multimodal architectures, and compare their performance to human
annotations. Our results show that while humans rely heavily on audio in
conversational settings, models perform best on text. This highlights a gap in
current multimodal models and motivates the use of MuSaG for developing models
better suited to realistic scenarios. We release MuSaG publicly to support
future research on multimodal sarcasm detection and human-model alignment.</p>
<h3 id="28-abjad-ai-at-nadi-2025-catt-whisper-multimodal-diacritic-restoration-using-text-and-speech-representations">[28] <a href="https://arxiv.org/abs/2510.24247">Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using Text and Speech Representations</a></h3>
<p><em>Ahmad Ghannam, Naif Alharthi, Faris Alasmary, Kholood Al Tabash, Shouq Sadah, Lahouari Ghouti</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§èåˆæ–‡æœ¬å’Œè¯­éŸ³ä¿¡æ¯çš„è·¨æ¨¡æ€æ–¹æ³•æ¥è§£å†³é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€çš„å˜éŸ³ç¬¦å·æ¢å¤ä»»åŠ¡ï¼Œé€šè¿‡ä¸¤ç§é›†æˆç­–ç•¥å®ç°äº†ä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥æ–¹æ³•åœ¨å¼€å‘é›†ä¸Šè¾¾åˆ°äº†0.25çš„è¯é”™è¯¯ç‡å’Œ0.9çš„å­—ç¬¦é”™è¯¯ç‡ï¼Œåœ¨æµ‹è¯•é›†ä¸Šåˆ†åˆ«è¾¾åˆ°0.55å’Œ0.13ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€å¥å­ä¸­å˜éŸ³ç¬¦å·æ¢å¤ä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†è¯­éŸ³æ¨¡æ€æä¾›çš„ä¸°å¯ŒéŸµå¾‹å’Œå‘éŸ³ç‰¹å¾ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨æ–¹è¨€ç¯å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„æ¨¡å‹é‡‡ç”¨è·¨æ¨¡æ€èåˆæ–¹æ³•ï¼Œæ–‡æœ¬æ¨¡æ€ä½¿ç”¨è‡ªç ”é¢„è®­ç»ƒæ¨¡å‹CATTçš„ç¼–ç å™¨ï¼Œè¯­éŸ³æ¨¡æ€ä½¿ç”¨OpenAI WhisperåŸºç¡€æ¨¡å‹çš„ç¼–ç å™¨æ¨¡å—ã€‚è®¾è®¡äº†ä¸¤ç§é›†æˆç­–ç•¥ï¼šæ—©æœŸèåˆç­–ç•¥å°†1500å¸§éŸ³é¢‘æ®µå¹³å‡ä¸º150ä¸ªè¯­éŸ³æ ‡è®°ï¼Œé€šè¿‡çº¿æ€§æŠ•å½±å±‚å¤„ç†åä¸æ–‡æœ¬æ ‡è®°åˆå¹¶ï¼›äº¤å‰æ³¨æ„åŠ›ç­–ç•¥é€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶èåˆæ–‡æœ¬å’Œè¯­éŸ³åµŒå…¥ï¼Œè¾“å‡ºé€å…¥CATTåˆ†ç±»å¤´è¿›è¡Œæ ‡è®°çº§é¢„æµ‹ã€‚è®­ç»ƒæ—¶éšæœºç¦ç”¨è¯­éŸ³è¾“å…¥ä»¥å¢å¼ºæ¨¡å‹é²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¼€å‘é›†ä¸Šå–å¾—äº†0.25çš„è¯é”™è¯¯ç‡å’Œ0.9çš„å­—ç¬¦é”™è¯¯ç‡ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¯é”™è¯¯ç‡å’Œå­—ç¬¦é”™è¯¯ç‡åˆ†åˆ«è¾¾åˆ°0.55å’Œ0.13ï¼Œè¯æ˜äº†è·¨æ¨¡æ€èåˆæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜èåˆæ–‡æœ¬å’Œè¯­éŸ³ä¿¡æ¯èƒ½å¤Ÿæ˜¾è‘—æå‡é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€å˜éŸ³ç¬¦å·æ¢å¤çš„æ€§èƒ½ï¼Œè·¨æ¨¡æ€æ–¹æ³•ä¸ºæ–¹è¨€å¤„ç†ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚éšæœºç¦ç”¨è¯­éŸ³è¾“å…¥çš„è®­ç»ƒç­–ç•¥å¢å¼ºäº†æ¨¡å‹åœ¨ä»…æœ‰æ–‡æœ¬è¾“å…¥æ—¶çš„é²æ£’æ€§ï¼Œä¸ºå®é™…åº”ç”¨åœºæ™¯æä¾›äº†çµæ´»æ€§ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>In this work, we tackle the Diacritic Restoration (DR) task for Arabic
dialectal sentences using a multimodal approach that combines both textual and
speech information. We propose a model that represents the text modality using
an encoder extracted from our own pre-trained model named CATT. The speech
component is handled by the encoder module of the OpenAI Whisper base model.
Our solution is designed following two integration strategies. The former
consists of fusing the speech tokens with the input at an early stage, where
the 1500 frames of the audio segment are averaged over 10 consecutive frames,
resulting in 150 speech tokens. To ensure embedding compatibility, these
averaged tokens are processed through a linear projection layer prior to
merging them with the text tokens. Contextual encoding is guaranteed by the
CATT encoder module. The latter strategy relies on cross-attention, where text
and speech embeddings are fused. The cross-attention output is then fed to the
CATT classification head for token-level diacritic prediction. To further
improve model robustness, we randomly deactivate the speech input during
training, allowing the model to perform well with or without speech. Our
experiments show that the proposed approach achieves a word error rate (WER) of
0.25 and a character error rate (CER) of 0.9 on the development set. On the
test set, our model achieved WER and CER scores of 0.55 and 0.13, respectively.</p>
<h3 id="29-beyond-mcq-an-open-ended-arabic-cultural-qa-benchmark-with-dialect-variants">[29] <a href="https://arxiv.org/abs/2510.24328">Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants</a></h3>
<p><em>Hunzalah Hassan Bhatti, Firoj Alam</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»¼åˆæ–¹æ³•æ¥è¯„ä¼°LLMsåœ¨é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€å’Œæ–‡åŒ–å†…å®¹ä¸Šçš„è¡¨ç°ï¼Œé€šè¿‡å°†ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­å¤šé€‰é¢˜è½¬æ¢ä¸ºè‹±è¯­å’Œå¤šç§é˜¿æ‹‰ä¼¯æ–¹è¨€çš„å¼€æ”¾å¼é—®é¢˜ï¼Œå¹¶åˆ©ç”¨æ€ç»´é“¾å¾®è°ƒæ¨¡å‹è¿›è¡Œé€æ­¥æ¨ç†ï¼Œæ­ç¤ºäº†LLMsåœ¨æ–¹è¨€çŸ¥è¯†æ–¹é¢çš„æŒç»­å·®è·ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ—¥å¸¸é—®ç­”ä¸­åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†åœ¨æ–‡åŒ–åŸºç¡€å’Œæ–¹è¨€å†…å®¹ä¸Šçš„è¡¨ç°å­˜åœ¨è¯­è¨€é—´çš„ä¸å‡è¡¡ï¼Œç‰¹åˆ«æ˜¯é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€çš„çŸ¥è¯†è¦†ç›–ä¸è¶³ï¼Œéœ€è¦ç³»ç»Ÿè¯„ä¼°å’Œæ”¹è¿›ã€‚</p>
<p><strong>Method:</strong> æå‡ºç»¼åˆæ–¹æ³•åŒ…æ‹¬å°†ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­å¤šé€‰é¢˜ç¿»è¯‘ä¸ºè‹±è¯­å’Œå¤šç§é˜¿æ‹‰ä¼¯æ–¹è¨€ï¼Œè½¬æ¢ä¸ºå¼€æ”¾å¼é—®é¢˜ï¼Œåœ¨é›¶æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸‹å¯¹å¤šç§LLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶ç”Ÿæˆæ€ç»´é“¾æ¨ç†æ¥å¾®è°ƒæ¨¡å‹è¿›è¡Œé€æ­¥æ¨ç†ã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯æ–¹è¨€ä¸Šè¡¨ç°è¾ƒå·®ï¼Œæ­ç¤ºæ–‡åŒ–åŸºç¡€å’Œæ–¹è¨€ç‰¹å®šçŸ¥è¯†çš„æŒç»­å·®è·ï¼›é˜¿æ‹‰ä¼¯ä¸­å¿ƒæ¨¡å‹åœ¨å¤šé€‰é¢˜ä¸Šè¡¨ç°è‰¯å¥½ä½†åœ¨å¼€æ”¾å¼é—®é¢˜ä¸Šå›°éš¾ï¼›æ€ç»´é“¾å¾®è°ƒæé«˜äº†åˆ¤æ–­æ­£ç¡®æ€§ä½†n-gramæŒ‡æ ‡ç»“æœä¸ä¸€ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†LLMsåœ¨æ–¹è¨€å’Œæ–‡åŒ–å†…å®¹ç†è§£ä¸Šçš„ç³»ç»Ÿæ€§ç¼ºé™·ï¼Œå¼€å‘çš„æ•°æ®é›†å°†å…¬å¼€æ”¯æŒæ–‡åŒ–è¯­è¨€åŒ…å®¹æ€§è¯„ä¼°ç ”ç©¶ï¼Œæ€ç»´é“¾æ–¹æ³•å¯¹æ¨ç†èƒ½åŠ›æå‡æœ‰æ•ˆä½†éœ€æ”¹è¿›è¯„ä¼°æŒ‡æ ‡ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Large Language Models (LLMs) are increasingly used to answer everyday
questions, yet their performance on culturally grounded and dialectal content
remains uneven across languages. We propose a comprehensive method that (i)
translates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into
English and several Arabic dialects, (ii) converts them into open-ended
questions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs
under both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)
rationales to fine-tune models for step-by-step reasoning. Using this method,
we extend an existing dataset in which QAs are parallelly aligned across
multiple language varieties, making it, to our knowledge, the first of its
kind. We conduct extensive experiments with both open and closed models. Our
findings show that (i) models underperform on Arabic dialects, revealing
persistent gaps in culturally grounded and dialect-specific knowledge; (ii)
Arabic-centric models perform well on MCQs but struggle with OEQs; and (iii)
CoT improves judged correctness while yielding mixed n-gram-based metrics. The
developed dataset will be publicly released to support further research on
culturally and linguistically inclusive evaluation.</p>
<h3 id="30-sparta-evaluating-reasoning-segmentation-robustness-through-black-box-adversarial-paraphrasing-in-text-autoencoder-latent-space">[30] <a href="https://arxiv.org/abs/2510.24446">SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space</a></h3>
<p><em>Viktoriia Zinkovich, Anton Antonov, Andrei Spiridonov, Denis Shepelev, Andrey Moskalenko, Daria Pugacheva, Elena Tutubalina, Andrey Kuznetsov, Vlad Shakhuro</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯¹æŠ—æ€§æ”¹å†™ä»»åŠ¡ï¼Œé€šè¿‡ç”Ÿæˆè¯­ä¹‰ç­‰ä»·ä½†èƒ½é™ä½åˆ†å‰²æ€§èƒ½çš„æ–‡æœ¬æŸ¥è¯¢æ¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§ï¼Œå¹¶å¼€å‘äº†SPARTAæ–¹æ³•åœ¨è¯­ä¹‰æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œé»‘ç›’ä¼˜åŒ–ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨å›¾åƒè¾“å…¥çš„æ‰°åŠ¨ï¼Œè€Œè¯­ä¹‰ç­‰ä»·çš„æ–‡æœ¬æ”¹å†™åœ¨å®é™…åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œå› ä¸ºç”¨æˆ·å¯èƒ½ä»¥ä¸åŒæ–¹å¼è¡¨è¾¾ç›¸åŒæ„å›¾ï¼Œè¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</p>
<p><strong>Method:</strong> æˆ‘ä»¬å¼•å…¥äº†SPARTAæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é»‘ç›’ã€å¥å­çº§åˆ«çš„ä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨æ–‡æœ¬è‡ªç¼–ç å™¨çš„ä½ç»´è¯­ä¹‰æ½œåœ¨ç©ºé—´ä¸­æ“ä½œï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡ŒæŒ‡å¯¼ï¼ŒåŒæ—¶å¼€å‘äº†å…¨é¢çš„è‡ªåŠ¨è¯„ä¼°åè®®æ¥éªŒè¯å¯¹æŠ—æ€§æ”¹å†™çš„è´¨é‡ã€‚</p>
<p><strong>Result:</strong> SPARTAåœ¨ReasonSegå’ŒLLMSeg-40kæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ›´é«˜çš„æˆåŠŸç‡ï¼Œæ¯”ç°æœ‰æ–¹æ³•é«˜å‡ºæœ€å¤š2å€ï¼Œæ­ç¤ºäº†å…ˆè¿›æ¨ç†åˆ†å‰²æ¨¡å‹å³ä½¿åœ¨ä¸¥æ ¼çš„è¯­ä¹‰å’Œè¯­æ³•çº¦æŸä¸‹ä»ç„¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”¹å†™çš„æ”»å‡»ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†åˆ†å‰²ä»»åŠ¡ä¸­å¯¹è¯­ä¹‰ç­‰ä»·çš„å¯¹æŠ—æ€§æ”¹å†™å­˜åœ¨æ˜¾è‘—è„†å¼±æ€§ï¼Œè¿™ä¸ºæ¨¡å‹é²æ£’æ€§è¯„ä¼°æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶å¼ºè°ƒäº†åœ¨å®é™…éƒ¨ç½²ä¸­è€ƒè™‘å¤šæ ·åŒ–ç”¨æˆ·è¡¨è¾¾çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) have shown impressive capabilities
in vision-language tasks such as reasoning segmentation, where models generate
segmentation masks based on textual queries. While prior work has primarily
focused on perturbing image inputs, semantically equivalent textual
paraphrases-crucial in real-world applications where users express the same
intent in varied ways-remain underexplored. To address this gap, we introduce a
novel adversarial paraphrasing task: generating grammatically correct
paraphrases that preserve the original query meaning while degrading
segmentation performance. To evaluate the quality of adversarial paraphrases,
we develop a comprehensive automatic evaluation protocol validated with human
studies. Furthermore, we introduce SPARTA-a black-box, sentence-level
optimization method that operates in the low-dimensional semantic latent space
of a text autoencoder, guided by reinforcement learning. SPARTA achieves
significantly higher success rates, outperforming prior methods by up to 2x on
both the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive
baselines to assess the robustness of advanced reasoning segmentation models.
We reveal that they remain vulnerable to adversarial paraphrasing-even under
strict semantic and grammatical constraints. All code and data will be released
publicly upon acceptance.</p>
<h3 id="31-talk2ref-a-dataset-for-reference-prediction-from-scientific-talks">[31] <a href="https://arxiv.org/abs/2510.24478">Talk2Ref: A Dataset for Reference Prediction from Scientific Talks</a></h3>
<p><em>Frederik Broy, Maike ZÃ¼fle, Jan Niehues</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†æ¼”è®²å¼•ç”¨é¢„æµ‹ä»»åŠ¡ï¼Œæ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡Talk2Refæ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†åŒç¼–ç å™¨æ¶æ„ï¼Œæ˜¾è‘—æå‡äº†ä»ç§‘å­¦æ¼”è®²ä¸­è‡ªåŠ¨è¯†åˆ«ç›¸å…³æ–‡çŒ®çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç§‘å­¦æ¼”è®²æ­£æˆä¸ºä¼ æ’­ç ”ç©¶çš„é‡è¦åª’ä»‹ï¼Œä½†è‡ªåŠ¨è¯†åˆ«èƒ½å¤Ÿæ”¯æ’‘æˆ–ä¸°å¯Œæ¼”è®²å†…å®¹çš„ç›¸å…³æ–‡çŒ®ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç ”ç©¶äººå‘˜å’Œå­¦ç”Ÿéœ€è¦èƒ½å¤Ÿä»éç»“æ„åŒ–çš„é•¿ç¯‡ç§‘å­¦æ¼”è®²ä¸­å‡†ç¡®æ˜ å°„åˆ°ç›¸å…³è®ºæ–‡çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åŸºäºåŒç¼–ç å™¨æ¶æ„çš„æ¨¡å‹ï¼Œåœ¨Talk2Refæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ¢ç´¢äº†å¤„ç†é•¿æ–‡æœ¬è½¬å½•æœ¬çš„ç­–ç•¥ä»¥åŠé¢†åŸŸè‡ªé€‚åº”è®­ç»ƒæ–¹æ³•ï¼ŒåŒæ—¶è¯„ä¼°äº†æœ€å…ˆè¿›æ–‡æœ¬åµŒå…¥æ¨¡å‹åœ¨é›¶æ ·æœ¬æ£€ç´¢åœºæ™¯ä¸­çš„è¡¨ç°ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Talk2Refæ•°æ®é›†ä¸Šçš„å¾®è°ƒæ˜¾è‘—æå‡äº†å¼•ç”¨é¢„æµ‹æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥ä»»åŠ¡çš„æŠ€æœ¯æŒ‘æˆ˜æ€§ä»¥åŠä»å£è¯­ç§‘å­¦å†…å®¹ä¸­å­¦ä¹ è¯­ä¹‰è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å°†å£è¯­ç§‘å­¦äº¤æµæ•´åˆåˆ°å¼•ç”¨æ¨èç³»ç»Ÿä¸­çš„å¯è¡Œæ€§ï¼Œå‘å¸ƒçš„å¼€æ”¾æ•°æ®é›†å’Œè®­ç»ƒæ¨¡å‹ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€ï¼Œæ¨åŠ¨äº†ç§‘å­¦æ¼”è®²å†…å®¹è‡ªåŠ¨åˆ†æçš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Scientific talks are a growing medium for disseminating research, and
automatically identifying relevant literature that grounds or enriches a talk
would be highly valuable for researchers and students alike. We introduce
Reference Prediction from Talks (RPT), a new task that maps long, and
unstructured scientific presentations to relevant papers. To support research
on RPT, we present Talk2Ref, the first large-scale dataset of its kind,
containing 6,279 talks and 43,429 cited papers (26 per talk on average), where
relevance is approximated by the papers cited in the talk's corresponding
source publication. We establish strong baselines by evaluating
state-of-the-art text embedding models in zero-shot retrieval scenarios, and
propose a dual-encoder architecture trained on Talk2Ref. We further explore
strategies for handling long transcripts, as well as training for domain
adaptation. Our results show that fine-tuning on Talk2Ref significantly
improves citation prediction performance, demonstrating both the challenges of
the task and the effectiveness of our dataset for learning semantic
representations from spoken scientific content. The dataset and trained models
are released under an open license to foster future research on integrating
spoken scientific communication into citation recommendation systems.</p>
<h3 id="32-zero-shot-cross-lingual-transfer-using-prefix-based-adaptation">[32] <a href="https://arxiv.org/abs/2510.24619">Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation</a></h3>
<p><em>Snegha A, Sayambhu Sen, Piyush Singh Pasi, Abhishek Singhania, Preethi Jyothi</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†ä¸‰ç§å‰ç¼€æ–¹æ³•åœ¨é›¶æ ·æœ¬è·¨è¯­è¨€è¿ç§»ä¸­çš„è¡¨ç°ï¼Œå‘ç°åœ¨Llama 3.1 8Bå’ŒMistral v0.3 7Bæ¨¡å‹ä¸Šï¼Œå‰ç¼€æ–¹æ³•æ¯”LoRAåŸºçº¿åœ¨BelebeleåŸºå‡†ä¸Šæå‡äº†é«˜è¾¾6%ï¼Œä»…ä½¿ç”¨1.23Må­¦ä¹ å‚æ•°å³å¯å®ç°ä¸€è‡´çš„æ€§èƒ½æ”¹è¿›ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡Llamaå’ŒMistralç­‰ä»…è§£ç å™¨å¤§è¯­è¨€æ¨¡å‹å…·å¤‡å¤šè¯­è¨€é¢„è®­ç»ƒå’Œå¼ºæ³›åŒ–èƒ½åŠ›ï¼Œä½†å°†å…¶é€‚åº”åˆ°è·¨è¯­è¨€æ–°ä»»åŠ¡ä»å…·æŒ‘æˆ˜æ€§ï¼›è™½ç„¶å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯å¦‚LoRAè¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†åŸºäºå‰ç¼€çš„æŠ€æœ¯å¦‚è½¯æç¤ºè°ƒä¼˜ã€å‰ç¼€è°ƒä¼˜å’ŒLlama Adapteråœ¨ä»…è§£ç å™¨æ¨¡å‹çš„é›¶æ ·æœ¬è¿ç§»ä¸­ç ”ç©¶è¾ƒå°‘ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶å¯¹ä¸‰ç§å‰ç¼€æ–¹æ³•è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼ŒåŒ…æ‹¬è½¯æç¤ºè°ƒä¼˜ã€å‰ç¼€è°ƒä¼˜å’ŒLlama Adapterï¼Œç”¨äºä»è‹±è¯­åˆ°35+ç§é«˜èµ„æºå’Œä½èµ„æºè¯­è¨€çš„é›¶æ ·æœ¬è·¨è¯­è¨€è¿ç§»ï¼›åˆ†æè¿˜æ¢è®¨äº†è·¨è¯­è¨€å®¶æ—å’Œæ–‡å­—ç³»ç»Ÿçš„è¿ç§»æ•ˆæœï¼Œä»¥åŠæ¨¡å‹è§„æ¨¡ä»1Båˆ°24Bç¼©æ”¾çš„å½±å“ã€‚</p>
<p><strong>Result:</strong> åœ¨Llama 3.1 8Bæ¨¡å‹ä¸Šï¼Œå‰ç¼€æ–¹æ³•åœ¨BelebeleåŸºå‡†ä¸Šæ¯”LoRAåŸºçº¿é«˜å‡ºé«˜è¾¾6%ï¼›Mistral v0.3 7Bæ¨¡å‹ä¹Ÿè§‚å¯Ÿåˆ°ç±»ä¼¼çš„æ”¹è¿›ï¼›å°½ç®¡å‰ç¼€è°ƒä¼˜ä»…ä½¿ç”¨1.23Må­¦ä¹ å‚æ•°ï¼Œä½†åœ¨å¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Conclusion:</strong> è¿™äº›å‘ç°çªæ˜¾äº†å‰ç¼€æŠ€æœ¯ä½œä¸ºLoRAçš„æœ‰æ•ˆä¸”å¯æ‰©å±•æ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºå¤šè¯­è¨€è®¾ç½®ä¸­ï¼›å‰ç¼€æ–¹æ³•åœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå®ç°æ›´å¥½çš„è·¨è¯­è¨€è¿ç§»æ€§èƒ½ï¼Œä¸ºå¤šè¯­è¨€NLPåº”ç”¨æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>With the release of new large language models (LLMs) like Llama and Mistral,
zero-shot cross-lingual transfer has become increasingly feasible due to their
multilingual pretraining and strong generalization capabilities. However,
adapting these decoder-only LLMs to new tasks across languages remains
challenging. While parameter-efficient fine-tuning (PeFT) techniques like
Low-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as
soft prompt tuning, prefix tuning, and Llama Adapter are less explored,
especially for zero-shot transfer in decoder-only models. We present a
comprehensive study of three prefix-based methods for zero-shot cross-lingual
transfer from English to 35+ high- and low-resource languages. Our analysis
further explores transfer across linguistic families and scripts, as well as
the impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix
methods outperform LoRA-baselines by up to 6% on the Belebele benchmark.
Similar improvements were observed with Mistral v0.3 7B as well. Despite using
only 1.23M learning parameters with prefix tuning, we achieve consistent
improvements across diverse benchmarks. These findings highlight the potential
of prefix-based techniques as an effective and scalable alternative to LoRA,
particularly in low-resource multilingual settings.</p>
<h3 id="33-mm-wat-detecting-other-initiated-repair-requests-in-dialogue">[33] <a href="https://arxiv.org/abs/2510.24628">"Mm, Wat?" Detecting Other-initiated Repair Requests in Dialogue</a></h3>
<p><em>Anh Ngo, Nicolas Rollet, Catherine Pelachaud, Chloe Clavel</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡æ•´åˆåŸºäºä¼šè¯åˆ†æçš„è¯­éŸ³å­¦å’Œè¯­è¨€å­¦ç‰¹å¾ï¼Œè‡ªåŠ¨æ£€æµ‹è·å…°è¯­å¯¹è¯ä¸­çš„ä¿®å¤å‘èµ·ã€‚ç»“æœè¡¨æ˜è¯­éŸ³çº¿ç´¢è¡¥å……äº†è¯­è¨€ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†é¢„è®­ç»ƒæ–‡æœ¬å’ŒéŸ³é¢‘åµŒå…¥çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨äººç±»å¯¹è¯ä¸­ç»´æŒç›¸äº’ç†è§£æ˜¯é¿å…å¯¹è¯ä¸­æ–­çš„å…³é”®ï¼Œå…¶ä¸­ä¿®å¤ç‰¹åˆ«æ˜¯ä»–äººå‘èµ·ä¿®å¤èµ·ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œä¼šè¯ä»£ç†ä»ç„¶æ— æ³•è¯†åˆ«ç”¨æˆ·çš„ä¿®å¤å‘èµ·ï¼Œå¯¼è‡´å¯¹è¯ä¸­æ–­æˆ–ç”¨æˆ·è„±ç¦»ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡æ•´åˆåŸºäºä¼šè¯åˆ†æçš„è¯­è¨€å­¦å’Œè¯­éŸ³å­¦ç‰¹å¾ï¼Œè‡ªåŠ¨æ£€æµ‹è·å…°è¯­å¯¹è¯ä¸­çš„ä¿®å¤å‘èµ·ã€‚è¯¥æ–¹æ³•ç»“åˆäº†é¢„è®­ç»ƒçš„æ–‡æœ¬å’ŒéŸ³é¢‘åµŒå…¥ï¼Œå¹¶åˆ©ç”¨è¯­éŸ³çº¿ç´¢è¡¥å……è¯­è¨€ç‰¹å¾ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºè¯­éŸ³çº¿ç´¢æ˜¾è‘—è¡¥å……äº†è¯­è¨€ç‰¹å¾ï¼Œå¹¶æ˜¾è‘—æå‡äº†é¢„è®­ç»ƒæ–‡æœ¬å’ŒéŸ³é¢‘åµŒå…¥çš„æ€§èƒ½ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†ä¸åŒç‰¹å¾ä¹‹é—´çš„äº¤äº’æœºåˆ¶ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€ä¿®å¤å‘èµ·æ£€æµ‹æä¾›äº†æœ‰æ•ˆæ–¹æ³•ï¼Œæœªæ¥æ–¹å‘åŒ…æ‹¬æ•´åˆè§†è§‰çº¿ç´¢ã€æ¢ç´¢å¤šè¯­è¨€å’Œè·¨ä¸Šä¸‹æ–‡è¯­æ–™åº“ä»¥è¯„ä¼°æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Maintaining mutual understanding is a key component in human-human
conversation to avoid conversation breakdowns, in which repair, particularly
Other-Initiated Repair (OIR, when one speaker signals trouble and prompts the
other to resolve), plays a vital role. However, Conversational Agents (CAs)
still fail to recognize user repair initiation, leading to breakdowns or
disengagement. This work proposes a multimodal model to automatically detect
repair initiation in Dutch dialogues by integrating linguistic and prosodic
features grounded in Conversation Analysis. The results show that prosodic cues
complement linguistic features and significantly improve the results of
pretrained text and audio embeddings, offering insights into how different
features interact. Future directions include incorporating visual cues,
exploring multilingual and cross-context corpora to assess the robustness and
generalizability.</p>
<h3 id="34-optimizing-retrieval-for-rag-via-reinforced-contrastive-learning">[34] <a href="https://arxiv.org/abs/2510.24652">Optimizing Retrieval for RAG via Reinforced Contrastive Learning</a></h3>
<p><em>Jiawei Zhou, Lei Chen</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºR3æ¡†æ¶ï¼Œä¸€ç§é€šè¿‡è¯•é”™åé¦ˆå¼ºåŒ–å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ£€ç´¢æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€äººå·¥æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹åŠ¨æ€ä¼˜åŒ–æ£€ç´¢å™¨åœ¨RAGç¯å¢ƒä¸­çš„ç›¸å…³æ€§åˆ¤æ–­èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å¹¿æ³›åº”ç”¨ï¼Œä¿¡æ¯æ£€ç´¢çš„è§’è‰²ä»ä¸ºäººç±»ç”¨æˆ·æ£€ç´¢ä¿¡æ¯è½¬å˜ä¸ºä¸ºAIç³»ç»Ÿæ£€ç´¢ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œå…¶ä¸­ç›¸å…³æ€§éš¾ä»¥é¢„å…ˆå®šä¹‰æˆ–æ ‡æ³¨ï¼Œè¿™æ„æˆäº†ç°æœ‰æ–¹æ³•çš„ä¸»è¦æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> R3æ¡†æ¶é‡‡ç”¨åŸºäºè¯•é”™åé¦ˆçš„å¼ºåŒ–å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä½¿æ£€ç´¢å™¨èƒ½å¤Ÿåœ¨RAGç¯å¢ƒä¸­åŠ¨æ€æ¢ç´¢å’Œä¼˜åŒ–ç›¸å…³æ€§ï¼Œé€šè¿‡æ£€ç´¢ç»“æœä¸ç¯å¢ƒäº¤äº’äº§ç”Ÿå¯¹æ¯”ä¿¡å·æ¥è‡ªåŠ¨æŒ‡å¯¼æ£€ç´¢å™¨çš„è‡ªæˆ‘æ”¹è¿›ï¼Œæ— éœ€ä¾èµ–æ ‡æ³¨æˆ–åˆæˆæ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒR3å°†RAGæ€§èƒ½ç›¸æ¯”åŸå§‹æ£€ç´¢å™¨æå‡5.2%ï¼Œè¶…è¶Šæœ€å…ˆè¿›æ£€ç´¢å™¨4.9%ï¼ŒåŒæ—¶è¾¾åˆ°ä¸åŸºäºåè®­ç»ƒæˆ–æŒ‡ä»¤è°ƒä¼˜LLMçš„LLMå¢å¼ºæ£€ç´¢å’ŒRAGç³»ç»Ÿç›¸å½“çš„ç»“æœï¼Œè®­ç»ƒä»…éœ€4ä¸ªGPUå¹¶åœ¨å•æ—¥å†…å®Œæˆã€‚</p>
<p><strong>Conclusion:</strong> R3è¯æ˜äº†åœ¨RAGç¯å¢ƒä¸­é€šè¿‡å¼ºåŒ–å¯¹æ¯”å­¦ä¹ å®ç°æ£€ç´¢å™¨è‡ªä¼˜åŒ–çš„å¯è¡Œæ€§ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆå®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨æ— éœ€å¤§é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡æ£€ç´¢è´¨é‡ï¼Œä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„ä¼˜åŒ–å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>As retrieval-augmented generation (RAG) becomes increasingly widespread, the
role of information retrieval (IR) is shifting from retrieving information for
human users to retrieving contextual knowledge for artificial intelligence (AI)
systems, where relevance becomes difficult to define or annotate beforehand. To
address this challenge, we propose R3, a Retrieval framework optimized for RAG
through trialand-feedback Reinforced contrastive learning. Unlike prior
approaches that rely on annotated or synthetic data for supervised fine-tuning,
R3 enables the retriever to dynamically explore and optimize relevance within
the RAG environment. During training, the retrieved results interact with the
environment to produce contrastive signals that automatically guide the
retriever's self-improvement. Extensive experiments across diverse tasks
demonstrate that R3 improves RAG performance by 5.2% over the original
retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving
comparable results to LLM-augmented retrieval and RAG systems built on
post-trained or instruction-tuned LLMs. It is both efficient and practical,
requiring only 4 GPUs and completing training within a single day.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="35-game-tars-pretrained-foundation-models-for-scalable-generalist-multimodal-game-agents">[35] <a href="https://arxiv.org/abs/2510.23691">Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents</a></h3>
<p><em>Zihao Wang, Xujing Li, Yining Ye, Junjie Fang, Haoming Wang, Longxiang Liu, Shihao Liang, Junting Lu, Zhiyong Wu, Jiazhan Feng, Wanjun Zhong, Zili Li, Yu Wang, Yu Miao, Bo Zhou, Yuanfan Li, Hao Wang, Zhongkai Zhao, Faming Wu, Zhengxuan Jiang, Weihao Tan, Heyuan Yao, Shi Yan, Xiangyang Li, Yitao Liang, Yujia Qin, Guang Shi</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>Game-TARSæ˜¯ä¸€ä¸ªé€šç”¨æ¸¸æˆæ™ºèƒ½ä½“ï¼Œé€šè¿‡ç»Ÿä¸€å¯æ‰©å±•çš„é”®ç›˜é¼ æ ‡åŠ¨ä½œç©ºé—´è¿›è¡Œè®­ç»ƒï¼Œåœ¨å¤§è§„æ¨¡è·¨åŸŸé¢„è®­ç»ƒä¸­å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œåœ¨å¤šä¸ªæ¸¸æˆåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹å’Œäººç±»æ°´å¹³ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¸¸æˆæ™ºèƒ½ä½“ä¾èµ–ç‰¹å®šAPIæˆ–GUIæ¥å£çš„é™åˆ¶ï¼Œè¿™äº›æ–¹æ³•éš¾ä»¥å®ç°å¤§è§„æ¨¡è·¨åŸŸæŒç»­é¢„è®­ç»ƒã€‚ç ”ç©¶è€…å¸Œæœ›å¼€å‘ä¸€ç§åŸºäºäººç±»å¯¹é½çš„é”®ç›˜é¼ æ ‡è¾“å…¥çš„ç»Ÿä¸€åŠ¨ä½œç©ºé—´ï¼Œä»¥æ”¯æŒåœ¨æ“ä½œç³»ç»Ÿã€ç½‘é¡µå’Œæ¨¡æ‹Ÿæ¸¸æˆç­‰å¼‚æ„é¢†åŸŸä¸­è¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒã€‚</p>
<p><strong>Method:</strong> Game-TARSé‡‡ç”¨ç»Ÿä¸€å¯æ‰©å±•çš„åŠ¨ä½œç©ºé—´é”šå®šäºäººç±»å¯¹é½çš„é”®ç›˜é¼ æ ‡è¾“å…¥ï¼Œé€šè¿‡è¶…è¿‡500B tokençš„å¤šæ ·åŒ–è½¨è¿¹å’Œå¤šæ¨¡æ€æ•°æ®è¿›è¡Œé¢„è®­ç»ƒã€‚å…³é”®æŠ€æœ¯åŒ…æ‹¬è¡°å‡æŒç»­æŸå¤±ä»¥å‡å°‘å› æœæ··æ·†ï¼Œä»¥åŠé«˜æ•ˆçš„ç¨€ç–æ€ç»´ç­–ç•¥æ¥å¹³è¡¡æ¨ç†æ·±åº¦å’Œæ¨ç†æˆæœ¬ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGame-TARSåœ¨å¼€æ”¾ä¸–ç•ŒMinecraftä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æ¯”ä¹‹å‰æœ€å…ˆè¿›æ¨¡å‹æé«˜çº¦2å€ï¼Œåœ¨æœªè§è¿‡çš„ç½‘é¡µ3Dæ¸¸æˆä¸­æ¥è¿‘äººç±»æ–°æ‰‹æ°´å¹³ï¼Œåœ¨FPSåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†GPT-5ã€Gemini-2.5-Proå’ŒClaude-4-Sonnetã€‚è®­ç»ƒæ—¶é—´å’Œæµ‹è¯•æ—¶é—´çš„æ‰©å±•ç»“æœè¯å®ç»Ÿä¸€åŠ¨ä½œç©ºé—´åœ¨è·¨æ¸¸æˆå’Œå¤šæ¨¡æ€æ•°æ®æ‰©å±•æ—¶èƒ½æŒç»­æå‡æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œç®€å•å¯æ‰©å±•çš„åŠ¨ä½œè¡¨ç¤ºä¸å¤§è§„æ¨¡é¢„è®­ç»ƒç›¸ç»“åˆï¼Œä¸ºå®ç°å…·æœ‰å¹¿æ³›è®¡ç®—æœºä½¿ç”¨èƒ½åŠ›çš„é€šç”¨æ™ºèƒ½ä½“æä¾›äº†ä¸€æ¡æœ‰å‰æ™¯çš„è·¯å¾„ã€‚ç»Ÿä¸€åŠ¨ä½œç©ºé—´èŒƒå¼æ”¯æŒåœ¨å¼‚æ„é¢†åŸŸä¸­è¿›è¡Œå¤§è§„æ¨¡æŒç»­é¢„è®­ç»ƒï¼Œä¸ºå¼€å‘æ›´é€šç”¨çš„AIç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>We present Game-TARS, a generalist game agent trained with a unified,
scalable action space anchored to human-aligned native keyboard-mouse inputs.
Unlike API- or GUI-based approaches, this paradigm enables large-scale
continual pre-training across heterogeneous domains, including OS, web, and
simulation games. Game-TARS is pre-trained on over 500B tokens with diverse
trajectories and multimodal data. Key techniques include a decaying continual
loss to reduce causal confusion and an efficient Sparse-Thinking strategy that
balances reasoning depth and inference cost. Experiments show that Game-TARS
achieves about 2 times the success rate over the previous sota model on
open-world Minecraft tasks, is close to the generality of fresh humans in
unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet
in FPS benchmarks. Scaling results on training-time and test-time confirm that
the unified action space sustains improvements when scaled to cross-game and
multimodal data. Our results demonstrate that simple, scalable action
representations combined with large-scale pre-training provide a promising path
toward generalist agents with broad computer-use abilities.</p>
<h3 id="36-why-foundation-models-in-pathology-are-failing">[36] <a href="https://arxiv.org/abs/2510.23807">Why Foundation Models in Pathology Are Failing</a></h3>
<p><em>Hamid R. Tizhoosh</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†è®¡ç®—ç—…ç†å­¦ä¸­åŸºç¡€æ¨¡å‹å­˜åœ¨çš„æ ¹æœ¬æ€§ç¼ºé™·ï¼ŒæŒ‡å‡ºè¿™äº›æ¨¡å‹ä¸ç»„ç»‡å½¢æ€å­¦æœ¬è´¨å­˜åœ¨æ¦‚å¿µæ€§ä¸åŒ¹é…ï¼Œå¹¶è¯†åˆ«äº†ä¸ƒä¸ªç›¸äº’å…³è”çš„å¤±è´¥åŸå› ï¼Œå‘¼åå¯¹è¯¥èŒƒå¼è¿›è¡Œæ ¹æœ¬æ€§é‡æ–°æ€è€ƒã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡åŸºç¡€æ¨¡å‹åœ¨éåŒ»å­¦é¢†åŸŸå–å¾—äº†é©å‘½æ€§çªç ´ï¼Œä½†åœ¨è®¡ç®—ç—…ç†å­¦ä¸­çš„å¿«é€Ÿåº”ç”¨å¹¶æœªå®ç°é¢„æœŸçš„ç™Œç—‡è¯Šæ–­ã€é¢„åé¢„æµ‹å’Œå¤šæ¨¡æ€æ£€ç´¢çªç ´ï¼Œåè€Œæš´éœ²å‡ºè¯Šæ–­å‡†ç¡®æ€§ä½ã€é²æ£’æ€§å·®ã€å‡ ä½•ä¸ç¨³å®šæ€§ã€è®¡ç®—éœ€æ±‚å¤§å’Œå®‰å…¨æ¼æ´ç­‰æ ¹æœ¬æ€§å¼±ç‚¹ï¼Œéœ€è¦æ·±å…¥æ¢ç©¶è¿™äº›å¤±è´¥çš„æ ¹æœ¬åŸå› ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é‡‡ç”¨ç³»ç»Ÿæ€§è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡æ‰¹åˆ¤æ€§åˆ†æè¯†åˆ«äº†åŸºç¡€æ¨¡å‹åœ¨è®¡ç®—ç—…ç†å­¦ä¸­å¤±è´¥çš„ä¸ƒä¸ªæ ¸å¿ƒåŸå› ï¼šç”Ÿç‰©å¤æ‚æ€§ã€æ— æ•ˆçš„è‡ªç›‘ç£å­¦ä¹ ã€è¿‡åº¦æ³›åŒ–ã€è¿‡åº¦å¤æ‚çš„æ¶æ„ã€ç¼ºä¹é¢†åŸŸç‰¹å®šåˆ›æ–°ã€æ•°æ®ä¸è¶³ä»¥åŠä¸ç»„ç»‡å—å¤§å°ç›¸å…³çš„åŸºæœ¬è®¾è®¡ç¼ºé™·ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘ç°å½“å‰ç—…ç†å­¦åŸºç¡€æ¨¡å‹å­˜åœ¨è¯Šæ–­å‡†ç¡®ç‡ä½ã€é²æ£’æ€§å·®ã€å‡ ä½•ä¸ç¨³å®šã€è®¡ç®—éœ€æ±‚ç¹é‡å’Œå®‰å…¨æ¼æ´ç­‰ç³»ç»Ÿæ€§ç¼ºé™·ï¼Œè¿™äº›ç¼ºé™·æºäºæ¨¡å‹å‡è®¾ä¸äººç±»ç»„ç»‡å†…åœ¨å¤æ‚æ€§ä¹‹é—´çš„æ ¹æœ¬æ€§ä¸åŒ¹é…ï¼Œè€Œéç®€å•çš„æŠ€æœ¯ä¼˜åŒ–é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“è®ºè¡¨æ˜å½“å‰ç—…ç†å­¦åŸºç¡€æ¨¡å‹åœ¨æ¦‚å¿µå±‚é¢ä¸ç»„ç»‡å½¢æ€å­¦æœ¬è´¨å­˜åœ¨æ ¹æœ¬æ€§é”™é…ï¼Œéœ€è¦å½»åº•é‡æ–°æ€è€ƒåŸºç¡€æ¨¡å‹èŒƒå¼æœ¬èº«ï¼Œè€Œéä»…ä»…è¿›è¡Œæ¸è¿›å¼æ”¹è¿›ï¼Œè¿™ä¸ºæœªæ¥å¼€å‘çœŸæ­£é€‚ç”¨äºè®¡ç®—ç—…ç†å­¦çš„æ–°å‹å»ºæ¨¡æ–¹æ³•æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>In non-medical domains, foundation models (FMs) have revolutionized computer
vision and language processing through large-scale self-supervised and
multimodal learning. Consequently, their rapid adoption in computational
pathology was expected to deliver comparable breakthroughs in cancer diagnosis,
prognostication, and multimodal retrieval. However, recent systematic
evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor
robustness, geometric instability, heavy computational demands, and concerning
safety vulnerabilities. This short paper examines these shortcomings and argues
that they stem from deeper conceptual mismatches between the assumptions
underlying generic foundation modeling in mainstream AI and the intrinsic
complexity of human tissue. Seven interrelated causes are identified:
biological complexity, ineffective self-supervision, overgeneralization,
excessive architectural complexity, lack of domain-specific innovation,
insufficient data, and a fundamental design flaw related to tissue patch size.
These findings suggest that current pathology foundation models remain
conceptually misaligned with the nature of tissue morphology and call for a
fundamental rethinking of the paradigm itself.</p>
<h3 id="37-latent-chain-of-thought-for-visual-reasoning">[37] <a href="https://arxiv.org/abs/2510.23925">Latent Chain-of-Thought for Visual Reasoning</a></h3>
<p><em>Guohao Sun, Hang Hua, Jian Wang, Jiebo Luo, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‘Šé”€å˜åˆ†æ¨ç†çš„å¯æ‰©å±•è®­ç»ƒç®—æ³•ï¼Œå°†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†é‡æ–°è¡¨è¿°ä¸ºåéªŒæ¨æ–­é—®é¢˜ï¼Œé€šè¿‡å¤šæ ·æ€§å¯»æ±‚å¼ºåŒ–å­¦ä¹ å’Œè´å¶æ–¯æ¨ç†ç¼©æ”¾ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä¸ƒä¸ªæ¨ç†åŸºå‡†ä¸Šçš„æ€§èƒ½ã€æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è®­ç»ƒç®—æ³•å¦‚SFTã€PPOå’ŒGRPOåœ¨æœªè§æ¨ç†ä»»åŠ¡ä¸Šæ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œä¸”ä¸¥é‡ä¾èµ–æœ‰åè§çš„å¥–åŠ±æ¨¡å‹ï¼Œè¿™é™åˆ¶äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§å‘å±•ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨æ‘Šé”€å˜åˆ†æ¨ç†æ¡†æ¶å°†æ¨ç†é‡æ–°è¡¨è¿°ä¸ºåéªŒæ¨æ–­é—®é¢˜ï¼Œå¼•å…¥åŸºäºå¤šæ ·æ€§å¯»æ±‚å¼ºåŒ–å­¦ä¹ çš„ç¨€ç–å¥–åŠ±å‡½æ•°ï¼Œåœ¨tokençº§åˆ«æä¾›å­¦ä¹ ä¿¡å·ä»¥é¼“åŠ±å¤šæ ·åŒ–ã€é«˜ä¼¼ç„¶åº¦çš„æ½œåœ¨æ€ç»´é“¾ï¼Œå¹¶ä½¿ç”¨è´å¶æ–¯æ¨ç†ç¼©æ”¾ç­–ç•¥é€šè¿‡è¾¹é™…ä¼¼ç„¶æ›¿ä»£æ˜‚è´µçš„Best-of-Nå’ŒBeamæœç´¢æ¥é«˜æ•ˆæ’åºæœ€ä¼˜æ¨ç†è·¯å¾„å’Œç­”æ¡ˆã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸ƒä¸ªæ¨ç†åŸºå‡†ä¸Šçš„å®è¯ç ”ç©¶è¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨æœ‰æ•ˆæ€§ã€æ³›åŒ–æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å°†æ¨ç†å»ºæ¨¡ä¸ºåéªŒæ¨æ–­é—®é¢˜çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºçš„å˜åˆ†æ¨ç†æ¡†æ¶å’Œå¤šæ ·æ€§å¥–åŠ±æœºåˆ¶ä¸ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ¨ç†æä¾›äº†æ–°èŒƒå¼ï¼ŒåŒæ—¶è´å¶æ–¯ç¼©æ”¾ç­–ç•¥ä¸ºé«˜æ•ˆæ¨ç†è·¯å¾„é€‰æ‹©æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„ç†è®ºä»·å€¼å’Œå®é™…åº”ç”¨å‰æ™¯ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.</p>
<h3 id="38-histolens-an-interactive-xai-toolkit-for-verifying-and-mitigating-flaws-in-vision-language-models-for-histopathology">[38] <a href="https://arxiv.org/abs/2510.24115">HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology</a></h3>
<p><em>Sandeep Vissapragada, Vikrant Sahu, Gagan Raj Gupta, Vandita Singh</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶å¼€å‘äº†HistoLensç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘ç—…ç†å­¦å®¶çš„é€æ˜AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’å’Œè§†è§‰è§£é‡ŠåŠŸèƒ½å¢å¼ºè¯Šæ–­è¿‡ç¨‹ä¸­çš„ä¿¡ä»»ä¸åˆä½œã€‚ç³»ç»Ÿé€šè¿‡æ™ºèƒ½ç¿»è¯‘ç”¨æˆ·æŸ¥è¯¢ã€ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Šå¹¶æä¾›çƒ­åŠ›å›¾å¯è§†åŒ–ï¼Œä½¿AIæ¨ç†è¿‡ç¨‹å®Œå…¨é€æ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŒ»ç–—AIç³»ç»Ÿæ™®éå­˜åœ¨é»‘ç®±é—®é¢˜ï¼ŒåŒ»ç”Ÿéš¾ä»¥ç†è§£AIçš„æ¨ç†è¿‡ç¨‹ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„å¯ä¿¡åº¦å’Œåº”ç”¨ä»·å€¼ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ªé€æ˜çš„AIåŠ©æ‰‹ï¼Œä½¿ç—…ç†å­¦å®¶èƒ½å¤Ÿåƒå’¨è¯¢åŒäº‹ä¸€æ ·ä¸AIç³»ç»Ÿè¿›è¡Œè‡ªç„¶äº¤äº’ï¼Œä»è€Œå»ºç«‹çœŸæ­£çš„ä¿¡ä»»å…³ç³»ã€‚</p>
<p><strong>Method:</strong> ç³»ç»Ÿé‡‡ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å°†ç—…ç†å­¦å®¶çš„è‹±æ–‡é—®é¢˜è½¬åŒ–ä¸ºç²¾ç¡®çš„AIæŸ¥è¯¢ï¼Œç”Ÿæˆç»“æ„åŒ–è¯Šæ–­æŠ¥å‘Šã€‚å…³é”®åˆ›æ–°åœ¨äºæä¾›å³æ—¶è§†è§‰è§£é‡ŠåŠŸèƒ½ï¼Œé€šè¿‡çƒ­åŠ›å›¾ç²¾ç¡®å®šä½åˆ†ææ‰€ä¾æ®çš„ç»†èƒå’Œç»„ç»‡åŒºåŸŸã€‚åŒæ—¶è®­ç»ƒAIæ¨¡å‹ä¸“æ³¨äºæ‚£è€…ç»„ç»‡ç‰¹å¾ï¼Œè‡ªåŠ¨å¿½ç•¥èƒŒæ™¯å™ªå£°å¹²æ‰°ã€‚</p>
<p><strong>Result:</strong> HistoLenså®ç°äº†ç—…ç†å­¦å®¶ä¸AIç³»ç»Ÿä¹‹é—´çš„é€æ˜åä½œå·¥ä½œæµç¨‹ï¼ŒåŒ»ç”Ÿèƒ½å¤Ÿä¿æŒä¸“å®¶ä¸»å¯¼åœ°ä½ï¼ŒåŒæ—¶åˆ©ç”¨å¯ä¿¡çš„AIåŠ©æ‰‹éªŒè¯è¯Šæ–­è§è§£ã€‚ç³»ç»Ÿæ˜¾è‘—æå‡äº†è¯Šæ–­æ•ˆç‡å’Œä¿¡å¿ƒï¼Œä½¿åŒ»ç”Ÿèƒ½å¤Ÿåšå‡ºæ›´å¿«ã€æ›´å¯é çš„è¯Šæ–­å†³ç­–ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€æ˜AIç³»ç»Ÿåœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„å…³é”®ä»·å€¼ï¼Œé€šè¿‡æä¾›å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹å’Œè§†è§‰è¯æ®ï¼ŒæˆåŠŸå»ºç«‹äº†åŒ»ç”Ÿå¯¹AIçš„ä¿¡ä»»ã€‚è¿™ç§åä½œæ¨¡å¼ä¸ºåŒ»ç–—AIçš„å®é™…åº”ç”¨æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œå¼ºè°ƒäº†ä¿æŒäººç±»ä¸“å®¶ä¸»å¯¼åœ°ä½çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>For doctors to truly trust artificial intelligence, it can't be a black box.
They need to understand its reasoning, almost as if they were consulting a
colleague. We created HistoLens1 to be that transparent, collaborative partner.
It allows a pathologist to simply ask a question in plain English about a
tissue slide--just as they would ask a trainee. Our system intelligently
translates this question into a precise query for its AI engine, which then
provides a clear, structured report. But it doesn't stop there. If a doctor
ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any
finding--a heatmap that points to the exact cells and regions the AI used for
its analysis. We've also ensured the AI focuses only on the patient's tissue,
just like a trained pathologist would, by teaching it to ignore distracting
background noise. The result is a workflow where the pathologist remains the
expert in charge, using a trustworthy AI assistant to verify their insights and
make faster, more confident diagnoses.</p>
<h3 id="39-blm_1-a-boundless-large-model-for-cross-space-cross-task-and-cross-embodiment-learning">[39] <a href="https://arxiv.org/abs/2510.24161">BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning</a></h3>
<p><em>Wentao Tan, Bowen Wang, Heng Zhi, Chenyu Liu, Zhe Li, Jian Liu, Zengrong Lin, Yukun Dai, Yipeng Chen, Wenjie Yang, Enci Xie, Hao Xue, Baixu Ji, Chen Xu, Zhibin Wang, Tianshi Wang, Lei Zhu, Heng Tao Shen</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†è¾¹ç•Œå¤§æ¨¡å‹BLMâ‚ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç©ºé—´åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼å®ç°äº†è·¨ç©ºé—´è¿ç§»ã€è·¨ä»»åŠ¡å­¦ä¹ å’Œè·¨å…·èº«æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æ•°å­—å’Œç‰©ç†ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰æ¨¡å‹å®¶æ—ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­—-ç‰©ç†ç©ºé—´å’Œä¸åŒå…·èº«ç³»ç»Ÿé—´çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä»…èƒ½äº§ç”Ÿä½çº§åŠ¨ä½œè€Œç¼ºä¹é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œå¤§å¤šæ•°å…·èº«å¤§è¯­è¨€æ¨¡å‹å±€é™äºæ•°å­—ç©ºé—´ä¸”éš¾ä»¥æ³›åŒ–åˆ°ç‰©ç†ä¸–ç•Œï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿåœ¨æ•°å­—å’Œç‰©ç†ç©ºé—´æ— ç¼æ“ä½œå¹¶è·¨å…·èº«ç³»ç»Ÿå’Œä»»åŠ¡æ³›åŒ–çš„ç»Ÿä¸€æ¨¡å‹ã€‚</p>
<p><strong>Method:</strong> BLMâ‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡ç²¾é€‰æ•°å­—è¯­æ–™åº“å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ³¨å…¥å…·èº«çŸ¥è¯†åŒæ—¶ä¿æŒè¯­è¨€èƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡æ„å›¾æ¡¥æ¥æ¥å£è®­ç»ƒç­–ç•¥æ¨¡å—ï¼Œä»å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­æå–é«˜çº§è¯­ä¹‰æ¥æŒ‡å¯¼æ§åˆ¶è€Œä¸å¾®è°ƒä¸»å¹²ç½‘ç»œï¼Œè¯¥æ–¹æ³•åŸºäºè‡ªæ”¶é›†çš„è·¨å…·èº«æ¼”ç¤ºå¥—ä»¶ï¼Œæ¶µç›–å››ç§æœºå™¨äººå…·èº«å’Œå…­ä¸ªæ¸è¿›æŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚</p>
<p><strong>Result:</strong> åœ¨æ•°å­—å’Œç‰©ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œå•ä¸ªBLMâ‚å®ä¾‹åœ¨æ•°å­—ä»»åŠ¡ä¸­å®ç°äº†çº¦6%çš„æ€§èƒ½æå‡ï¼Œåœ¨ç‰©ç†ä»»åŠ¡ä¸­å®ç°äº†çº¦3%çš„æ€§èƒ½æå‡ï¼Œä¼˜äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€å…·èº«å¤§è¯­è¨€æ¨¡å‹ã€è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å’Œé€šç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å››ä¸ªæ¨¡å‹å®¶æ—ã€‚</p>
<p><strong>Conclusion:</strong> BLMâ‚è¯æ˜äº†é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼å¯ä»¥å®ç°è·¨ç©ºé—´ã€è·¨ä»»åŠ¡å’Œè·¨å…·èº«çš„ç»Ÿä¸€å»ºæ¨¡ï¼Œä¸ºæ„å»ºåœ¨æ•°å­—å’Œç‰©ç†ä¸–ç•Œé—´æ— ç¼æ“ä½œçš„å…·èº«æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆè·¯å¾„ï¼ŒåŒæ—¶ä¿æŒäº†æŒ‡ä»¤è·Ÿéšå’Œæ¨ç†èƒ½åŠ›ï¼Œæ”¯æŒé²æ£’çš„è·¨å…·èº«æ§åˆ¶ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) have advanced vision-language
reasoning and are increasingly deployed in embodied agents. However,
significant limitations remain: MLLMs generalize poorly across digital-physical
spaces and embodiments; vision-language-action models (VLAs) produce low-level
actions yet lack robust high-level embodied reasoning; and most embodied large
language models (ELLMs) are constrained to digital-space with poor
generalization to the physical world. Thus, unified models that operate
seamlessly across digital and physical spaces while generalizing across
embodiments and tasks remain absent. We introduce the \textbf{Boundless Large
Model (BLM$_1$)}, a multimodal spatial foundation model that preserves
instruction following and reasoning, incorporates embodied knowledge, and
supports robust cross-embodiment control. BLM$_1$ integrates three key
capabilities -- \textit{cross-space transfer, cross-task learning, and
cross-embodiment generalization} -- via a two-stage training paradigm. Stage I
injects embodied knowledge into the MLLM through curated digital corpora while
maintaining language competence. Stage II trains a policy module through an
intent-bridging interface that extracts high-level semantics from the MLLM to
guide control, without fine-tuning the MLLM backbone. This process is supported
by a self-collected cross-embodiment demonstration suite spanning four robot
embodiments and six progressively challenging tasks. Evaluations across digital
and physical benchmarks show that a single BLM$_1$ instance outperforms four
model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving
$\sim!\textbf{6%}$ gains in digital tasks and $\sim!\textbf{3%}$ in physical
tasks.</p>
<h3 id="40-mga-memory-driven-gui-agent-for-observation-centric-interaction">[40] <a href="https://arxiv.org/abs/2510.24168">MGA: Memory-Driven GUI Agent for Observation-Centric Interaction</a></h3>
<p><em>Weihua Cheng, Ersheng Ni, Wenlong Wang, Yifei Sun, Junming Liu, Wangyu Shen, Yirong Chen, Botian Shi, Ding Wang</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†è®°å¿†é©±åŠ¨çš„GUIæ™ºèƒ½ä½“ï¼ˆMGAï¼‰ï¼Œé€šè¿‡'å…ˆè§‚å¯Ÿåå†³ç­–'çš„èŒƒå¼è§£å†³äº†ç°æœ‰GUIæ™ºèƒ½ä½“å¯¹å†å²è½¨è¿¹çš„ä¾èµ–å’Œå±€éƒ¨æ¢ç´¢åå·®é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†é²æ£’æ€§ã€æ³›åŒ–æ€§å’Œæ•ˆç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰GUIæ™ºèƒ½ä½“é€šå¸¸å°†ä»»åŠ¡å»ºæ¨¡ä¸ºé•¿é“¾æ‰§è¡Œï¼Œå°†å†å²è½¨è¿¹ä¸²è”åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼Œå­˜åœ¨ä¸¤ä¸ªæŒç»­æ€§é—®é¢˜ï¼šå¯¹å†å²è½¨è¿¹çš„ä¾èµ–ä¼šæ”¾å¤§é”™è¯¯ä¼ æ’­ï¼Œä»¥åŠ'å†³ç­–ä¼˜å…ˆã€è§‚å¯Ÿæ»å'æœºåˆ¶å¯¼è‡´çš„å±€éƒ¨æ¢ç´¢åå·®ä¼šå¿½ç•¥å…³é”®ç•Œé¢çº¿ç´¢ã€‚</p>
<p><strong>Method:</strong> MGAå°†GUIäº¤äº’é‡æ–°æ„å»ºä¸º'å…ˆè§‚å¯Ÿåå†³ç­–'çš„åŸåˆ™ï¼Œå°†æ¯ä¸ªæ­¥éª¤å»ºæ¨¡ä¸ºç‹¬ç«‹çš„ã€ä¸Šä¸‹æ–‡ä¸°å¯Œçš„ç¯å¢ƒçŠ¶æ€ï¼Œç”±ä¸‰å…ƒç»„è¡¨ç¤ºï¼šå½“å‰å±å¹•æˆªå›¾ã€ä»»åŠ¡æ— å…³çš„ç©ºé—´ä¿¡æ¯ä»¥åŠåŠ¨æ€æ›´æ–°çš„ç»“æ„åŒ–è®°å¿†ã€‚</p>
<p><strong>Result:</strong> åœ¨OSworldåŸºå‡†æµ‹è¯•ã€çœŸå®æ¡Œé¢åº”ç”¨ç¨‹åºï¼ˆChromeã€VSCodeã€VLCï¼‰å’Œè·¨ä»»åŠ¡è¿ç§»å®éªŒä¸­çš„ç»“æœè¡¨æ˜ï¼ŒMGAç›¸æ¯”æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•åœ¨é²æ£’æ€§ã€æ³›åŒ–æ€§å’Œæ•ˆç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Conclusion:</strong> MGAé€šè¿‡é‡æ„GUIäº¤äº’èŒƒå¼ï¼Œè¯æ˜äº†'å…ˆè§‚å¯Ÿåå†³ç­–'æ–¹æ³•åœ¨è§£å†³ç°æœ‰GUIæ™ºèƒ½ä½“æ ¸å¿ƒé™åˆ¶æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºæ›´ç¨³å¥å’Œé€šç”¨çš„ç•Œé¢äº¤äº’ç³»ç»Ÿæä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>The rapid progress of Large Language Models (LLMs) and their multimodal
extensions (MLLMs) has enabled agentic systems capable of perceiving and acting
across diverse environments. A challenging yet impactful frontier is the
development of GUI agents, which must navigate complex desktop and web
interfaces while maintaining robustness and generalization. Existing paradigms
typically model tasks as long-chain executions, concatenating historical
trajectories into the context. While approaches such as Mirage and GTA1 refine
planning or introduce multi-branch action selection, they remain constrained by
two persistent issues: Dependence on historical trajectories, which amplifies
error propagation. And Local exploration bias, where "decision-first,
observation-later" mechanisms overlook critical interface cues. We introduce
the Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the
principle of observe first, then decide. MGA models each step as an
independent, context-rich environment state represented by a triad: current
screenshot, task-agnostic spatial information, and a dynamically updated
structured memory. Experiments on OSworld benchmarks, real desktop applications
(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves
substantial gains in robustness, generalization, and efficiency compared to
state-of-the-art baselines. The code is publicly available at:
{https://anonymous.4open.science/r/MGA-3571}.</p>
<h3 id="41-a-unified-geometric-space-bridging-ai-models-and-the-human-brain">[41] <a href="https://arxiv.org/abs/2510.24342">A Unified Geometric Space Bridging AI Models and the Human Brain</a></h3>
<p><em>Silin Chen, Yuzhong Chen, Zifan Wang, Junhao Wang, Zifeng Jia, Keith M Kendrick, Tuo Zhang, Lin Zhao, Dezhong Yao, Tianming Liu, Xi Jiang</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†è„‘ç›¸ä¼¼ç©ºé—´è¿™ä¸€å¼€åˆ›æ€§æ¦‚å¿µï¼Œé€šè¿‡å°†AIæ¨¡å‹çš„å†…åœ¨ç©ºé—´æ³¨æ„åŠ›æ‹“æ‰‘æ˜ å°„åˆ°äººç±»åŠŸèƒ½æ€§è„‘ç½‘ç»œä¸Šï¼Œæ„å»ºäº†ç»Ÿä¸€çš„å‡ ä½•ç©ºé—´æ¥é‡åŒ–æ¯”è¾ƒä¸åŒæ¨¡æ€AIæ¨¡å‹çš„è„‘ç›¸ä¼¼åº¦ï¼Œæ­ç¤ºäº†æ¨¡å‹ç»„ç»‡ä¸å¤§è„‘åŠŸèƒ½ç½‘ç»œä¹‹é—´çš„æ·±å±‚å¯¹åº”å…³ç³»ã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è„‘-AIå¯¹é½ç ”ç©¶è™½ç„¶æ˜¾ç¤ºäº†ä¸¤è€…ä¹‹é—´çš„æ˜¾è‘—å¯¹åº”å…³ç³»ï¼Œä½†è¿™äº›æ¯”è¾ƒå±€é™äºç‰¹å®šè¾“å…¥å’Œä»»åŠ¡ï¼Œç¼ºä¹ä¸€ä¸ªèƒ½å¤Ÿè·¨æ¨¡æ€ã€è·¨ä»»åŠ¡ç»Ÿä¸€æ¯”è¾ƒä¸åŒAIæ¨¡å‹å†…åœ¨ç»„ç»‡ç»“æ„çš„å…±åŒæ¡†æ¶ï¼Œæ— æ³•ç³»ç»Ÿè¯„ä¼°è§†è§‰ã€è¯­è¨€æˆ–å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šä¸å¤§è„‘çš„ç»„ç»‡æ–¹å¼ç›¸ä¼¼ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†è„‘ç›¸ä¼¼ç©ºé—´çš„æ¦‚å¿µæ¡†æ¶ï¼Œé€šè¿‡å°†Transformeræ¨¡å‹çš„å†…åœ¨ç©ºé—´æ³¨æ„åŠ›æ‹“æ‰‘ç»„ç»‡æ˜ å°„åˆ°æ ‡å‡†äººç±»åŠŸèƒ½æ€§è„‘ç½‘ç»œä¸Šï¼Œæ„å»ºç»Ÿä¸€çš„å‡ ä½•ç©ºé—´ï¼›è¯¥æ–¹æ³•åˆ†æäº†151ä¸ªåŸºäºTransformerçš„æ¨¡å‹ï¼Œæ¶µç›–æœ€å…ˆè¿›çš„å¤§è§„æ¨¡è§†è§‰æ¨¡å‹ã€è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼Œæ¯”è¾ƒäº†ä¸åŒé¢„è®­ç»ƒèŒƒå¼å’Œä½ç½®ç¼–ç æ–¹æ¡ˆå¯¹è„‘ç›¸ä¼¼åº¦çš„å½±å“ã€‚</p>
<p><strong>Result:</strong> åœ¨è„‘ç›¸ä¼¼ç©ºé—´ä¸­å‘ç°äº†ä¸€ä¸ªè¿ç»­çš„å¼§å½¢å‡ ä½•ç»“æ„ï¼Œåæ˜ äº†è„‘ç›¸ä¼¼åº¦çš„é€æ¸å¢åŠ ï¼›ä¸åŒæ¨¡å‹åœ¨è¯¥å‡ ä½•ä¸­å±•ç°å‡ºä¸ä¸åŒè„‘ç›¸ä¼¼åº¦ç›¸å…³çš„åˆ†å¸ƒæ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼ä¸ä»…å—æ¨¡æ€å½±å“ï¼Œæ›´å…³é”®åœ°å–å†³äºé¢„è®­ç»ƒèŒƒå¼æ˜¯å¦å¼ºè°ƒå…¨å±€è¯­ä¹‰æŠ½è±¡ä»¥åŠä½ç½®ç¼–ç æ–¹æ¡ˆæ˜¯å¦ä¿ƒè¿›è·¨æ¨¡æ€æ·±åº¦èåˆï¼›æ¨¡å‹è„‘ç›¸ä¼¼åº¦ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å¹¶éå®Œå…¨ä¸€è‡´ã€‚</p>
<p><strong>Conclusion:</strong> è„‘ç›¸ä¼¼ç©ºé—´ä¸ºè·¨é¢†åŸŸæ™ºèƒ½çš„å®šä½ã€é‡åŒ–å’Œæ¯”è¾ƒæä¾›äº†é¦–ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ­ç¤ºäº†è¿æ¥æœºå™¨ä¸å¤§è„‘çš„æ·±å±‚ç»„ç»‡åŸåˆ™ï¼›ç ”ç©¶è¡¨æ˜æ¨¡å‹çš„å†…åœ¨ç»„ç»‡æ–¹å¼åæ˜ äº†å…¶ä¸å¤§è„‘åŠŸèƒ½ç½‘ç»œçš„å¯¹åº”ç¨‹åº¦ï¼Œè€Œä¸ä»…ä»…æ˜¯ä»»åŠ¡æ€§èƒ½ï¼Œè¿™ä¸ºç†è§£æ™ºèƒ½çš„æœ¬è´¨å’Œæ„å»ºæ›´ç±»è„‘çš„AIç³»ç»Ÿæä¾›äº†æ–°çš„ç†è®ºåŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>For decades, neuroscientists and computer scientists have pursued a shared
ambition: to understand intelligence and build it. Modern artificial neural
networks now rival humans in language, perception, and reasoning, yet it is
still largely unknown whether these artificial systems organize information as
the brain does. Existing brain-AI alignment studies have shown the striking
correspondence between the two systems, but such comparisons remain bound to
specific inputs and tasks, offering no common ground for comparing how AI
models with different kinds of modalities-vision, language, or multimodal-are
intrinsically organized. Here we introduce a groundbreaking concept of
Brain-like Space: a unified geometric space in which every AI model can be
precisely situated and compared by mapping its intrinsic spatial attention
topological organization onto canonical human functional brain networks,
regardless of input modality, task, or sensory domain. Our extensive analysis
of 151 Transformer-based models spanning state-of-the-art large vision models,
large language models, and large multimodal models uncovers a continuous
arc-shaped geometry within this space, reflecting a gradual increase of
brain-likeness; different models exhibit distinct distribution patterns within
this geometry associated with different degrees of brain-likeness, shaped not
merely by their modality but by whether the pretraining paradigm emphasizes
global semantic abstraction and whether the positional encoding scheme
facilitates deep fusion across different modalities. Moreover, the degree of
brain-likeness for a model and its downstream task performance are not
"identical twins". The Brain-like Space provides the first unified framework
for situating, quantifying, and comparing intelligence across domains,
revealing the deep organizational principles that bridge machines and the
brain.</p>
<h3 id="42-os-sentinel-towards-safety-enhanced-mobile-gui-agents-via-hybrid-validation-in-realistic-workflows">[42] <a href="https://arxiv.org/abs/2510.24411">OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows</a></h3>
<p><em>Qiushi Sun, Mukai Li, Zhoumianze Liu, Zhihui Xie, Fangzhi Xu, Zhangyue Yin, Kanzhi Cheng, Zehao Li, Zichen Ding, Qi Liu, Zhiyong Wu, Zhuosheng Zhang, Ben Kao, Lingpeng Kong</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MobileRisk-LiveåŠ¨æ€æ²™ç›’ç¯å¢ƒå’ŒOS-Sentinelæ··åˆå®‰å…¨æ£€æµ‹æ¡†æ¶ï¼Œç”¨äºè§£å†³ç§»åŠ¨ç¯å¢ƒä¸­åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä»£ç†çš„å®‰å…¨æ“ä½œé—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆå½¢å¼åŒ–éªŒè¯å™¨å’Œä¸Šä¸‹æ–‡é£é™©è¯„ä¼°ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ¯”ç°æœ‰æ–¹æ³•æå‡10%-30%çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è®¡ç®—æœºä½¿ç”¨ä»£ç†åœ¨ç§»åŠ¨å¹³å°ç­‰æ•°å­—ç¯å¢ƒä¸­å±•ç°å‡ºç±»äººèƒ½åŠ›ï¼Œä½†å…¶æ½œåœ¨çš„ä¸å®‰å…¨æ“ä½œï¼ˆå¦‚ç³»ç»Ÿç ´åå’Œéšç§æ³„éœ²ï¼‰å¼•å‘äº†ä¸¥é‡æ‹…å¿§ã€‚åœ¨ç§»åŠ¨ç¯å¢ƒå¹¿é˜”å¤æ‚çš„æ“ä½œç©ºé—´ä¸­æ£€æµ‹è¿™äº›å®‰å…¨é—®é¢˜æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ï¼Œç›®å‰ä»å¤„äºä¸¥é‡æœªæ¢ç´¢çŠ¶æ€ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†OS-Sentinelæ··åˆå®‰å…¨æ£€æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒç»“åˆå½¢å¼åŒ–éªŒè¯å™¨ç”¨äºæ£€æµ‹æ˜¾å¼ç³»ç»Ÿçº§è¿è§„ï¼Œä»¥åŠåŸºäºVLMçš„ä¸Šä¸‹æ–‡åˆ¤æ–­å™¨ç”¨äºè¯„ä¼°ä¸Šä¸‹æ–‡é£é™©å’Œä»£ç†è¡Œä¸ºã€‚åŒæ—¶æ„å»ºäº†MobileRisk-LiveåŠ¨æ€æ²™ç›’ç¯å¢ƒåŠå…¶å®‰å…¨æ£€æµ‹åŸºå‡†ï¼ŒåŒ…å«å…·æœ‰ç»†ç²’åº¦æ ‡æ³¨çš„ç°å®è½¨è¿¹ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼ŒOS-Sentinelåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ¯”ç°æœ‰æ–¹æ³•å®ç°äº†10%-30%çš„æ€§èƒ½æå‡ã€‚è¯¥æ¡†æ¶åœ¨æ£€æµ‹ç§»åŠ¨ä»£ç†å®‰å…¨é£é™©æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºå®‰å…¨æ£€æµ‹æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Conclusion:</strong> è¿›ä¸€æ­¥åˆ†ææä¾›äº†å…³é”®è§è§£ï¼Œä¿ƒè¿›äº†æ›´å®‰å…¨å¯é çš„è‡ªä¸»ç§»åŠ¨ä»£ç†çš„å‘å±•ã€‚è¯¥ç ”ç©¶ä¸ºç§»åŠ¨ä»£ç†å®‰å…¨ç ”ç©¶å»ºç«‹äº†åŸºç¡€ï¼Œæå‡ºçš„æ··åˆæ£€æµ‹æ–¹æ³•ä¸ºè§£å†³å¤æ‚ç¯å¢ƒä¸­çš„å®‰å…¨é—®é¢˜æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>Computer-using agents powered by Vision-Language Models (VLMs) have
demonstrated human-like capabilities in operating digital environments like
mobile platforms. While these agents hold great promise for advancing digital
automation, their potential for unsafe operations, such as system compromise
and privacy leakage, is raising significant concerns. Detecting these safety
concerns across the vast and complex operational space of mobile environments
presents a formidable challenge that remains critically underexplored. To
establish a foundation for mobile agent safety research, we introduce
MobileRisk-Live, a dynamic sandbox environment accompanied by a safety
detection benchmark comprising realistic trajectories with fine-grained
annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety
detection framework that synergistically combines a Formal Verifier for
detecting explicit system-level violations with a VLM-based Contextual Judge
for assessing contextual risks and agent actions. Experiments show that
OS-Sentinel achieves 10%-30% improvements over existing approaches across
multiple metrics. Further analysis provides critical insights that foster the
development of safer and more reliable autonomous mobile agents.</p>
<h3 id="43-generative-ai-for-healthcare-fundamentals-challenges-and-perspectives">[43] <a href="https://arxiv.org/abs/2510.24551">Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives</a></h3>
<p><em>Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è®¾è®¡èŒƒå¼ï¼Œå°†åŒ»ç–—æ•°æ®ç”Ÿæ€ç³»ç»Ÿé‡æ–°å®šä½ä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åŸºç¡€æ”¯æ’‘ï¼Œé€šè¿‡è¯­ä¹‰å‘é‡æœç´¢å’Œä¸Šä¸‹æ–‡æŸ¥è¯¢ç­‰é«˜æ•ˆæ•°æ®å¤„ç†ç®¡é“ï¼Œæ”¯æŒä¸Šæ¸¸æ¨¡å‹ç»„ä»¶å’Œä¸‹æ¸¸ä¸´åºŠåº”ç”¨ã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„éƒ¨ç½²éœ€è¦æ·±å…¥ç†è§£åŒ»ç–—ä»»åŠ¡çš„èƒ½åŠ›è¾¹ç•Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹åŒ»ç–—æ•°æ®ç”Ÿå‘½å‘¨æœŸçš„ç³»ç»Ÿæ€§æ•´åˆï¼Œæ— æ³•æŒç»­æ”¯æŒå¤šæ ·åŒ–åŒ»ç–—æ•°æ®å’ŒçŸ¥è¯†çš„é›†æˆã€è¡¨ç¤ºä¸æ£€ç´¢ã€‚</p>
<p><strong>Method:</strong> æå‡ºä»¥åŒ»ç–—æ•°æ®ç”Ÿæ€ç³»ç»Ÿä¸ºåŸºçŸ³çš„ç”Ÿæˆå¼åŒ»ç–—ç³»ç»Ÿè®¾è®¡èŒƒå¼ï¼Œé€šè¿‡è¯­ä¹‰å‘é‡æœç´¢å’Œä¸Šä¸‹æ–‡æŸ¥è¯¢ç­‰é«˜æ•ˆæ•°æ®å¤„ç†ç®¡é“ï¼Œæ”¯æŒå¤šæ¨¡æ€åŒ»ç–—æ•°æ®çš„æ•´åˆä¸æ£€ç´¢ï¼Œä¸ºä¸Šæµæ¨¡å‹é¢„è®­ç»ƒå’Œé¢†åŸŸå¾®è°ƒæä¾›é«˜è´¨é‡æ•°æ®ï¼ŒåŒæ—¶ä½œä¸ºçŸ¥è¯†æ£€ç´¢åç«¯æ”¯æ’‘ä»»åŠ¡ç‰¹å®šæ¨ç†ã€‚</p>
<p><strong>Result:</strong> è¯¥ç”Ÿæ€ç³»ç»Ÿèƒ½å¤ŸæŒç»­æ”¯æŒå¤šæ ·åŒ–åŒ»ç–—æ•°æ®å’ŒçŸ¥è¯†çš„é›†æˆä¸æ£€ç´¢ï¼Œä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹æä¾›é«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®ç”¨äºå¤§è§„æ¨¡é¢„è®­ç»ƒå’Œé¢†åŸŸç‰¹å®šå¾®è°ƒï¼Œå¹¶é€šè¿‡ä»£ç†å±‚æ”¯æŒä»»åŠ¡ç‰¹å®šçš„æ¨ç†åº”ç”¨ã€‚</p>
<p><strong>Conclusion:</strong> ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„åŒ»ç–—ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç³»ç»Ÿè®¾è®¡èŒƒå¼èƒ½å¤Ÿå®ç°é«˜è´¨é‡æœ‰æ•ˆçš„åŒ»ç–—æœåŠ¡äº¤ä»˜ï¼Œé€šè¿‡é‡æ–°å®šä½æ•°æ®ç”Ÿå‘½å‘¨æœŸå’Œæ„å»ºå¯æŒç»­çš„åŒ»ç–—æ•°æ®ç”Ÿæ€ç³»ç»Ÿï¼Œä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„å¯é éƒ¨ç½²æä¾›äº†ç³»ç»Ÿæ€§è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>Generative Artificial Intelligence (GenAI) is taking the world by storm. It
promises transformative opportunities for advancing and disrupting existing
practices, including healthcare. From large language models (LLMs) for clinical
note synthesis and conversational assistance to multimodal systems that
integrate medical imaging, electronic health records, and genomic data for
decision support, GenAI is transforming the practice of medicine and the
delivery of healthcare, such as diagnosis and personalized treatments, with
great potential in reducing the cognitive burden on clinicians, thereby
improving overall healthcare delivery. However, GenAI deployment in healthcare
requires an in-depth understanding of healthcare tasks and what can and cannot
be achieved. In this paper, we propose a data-centric paradigm in the design
and deployment of GenAI systems for healthcare. Specifically, we reposition the
data life cycle by making the medical data ecosystem as the foundational
substrate for generative healthcare systems. This ecosystem is designed to
sustainably support the integration, representation, and retrieval of diverse
medical data and knowledge. With effective and efficient data processing
pipelines, such as semantic vector search and contextual querying, it enables
GenAI-powered operations for upstream model components and downstream clinical
applications. Ultimately, it not only supplies foundation models with
high-quality, multimodal data for large-scale pretraining and domain-specific
fine-tuning, but also serves as a knowledge retrieval backend to support
task-specific inference via the agentic layer. The ecosystem enables the
deployment of GenAI for high-quality and effective healthcare delivery.</p>
<h3 id="44-advancing-site-specific-disease-and-pest-management-in-precision-agriculture-from-reasoning-driven-foundation-models-to-adaptive-feedback-based-learning">[44] <a href="https://arxiv.org/abs/2510.24650">Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning</a></h3>
<p><em>Nitin Rai, Daeun, Choi, Nathan S. Boyd, Arnold W. Schumann</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬ç»¼è¿°åˆ†æäº†åŸºç¡€æ¨¡å‹åœ¨ä½œç‰©å®šç‚¹ç—…å®³ç®¡ç†ä¸­çš„åº”ç”¨è¿›å±•ï¼Œé‡ç‚¹è€ƒå¯Ÿäº†è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªé€‚åº”å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ åŠæ•°å­—å­ªç”Ÿæ¡†æ¶ä¸­çš„ä½œç”¨ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ç»“åˆå®æ—¶åé¦ˆå°†æ¨åŠ¨ä¸‹ä¸€ä»£æ™ºèƒ½å†œä¸šçš„å‘å±•ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿä½œç‰©ç—…å®³ç®¡ç†æ–¹æ³•å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œèµ„æºæµªè´¹é—®é¢˜ï¼Œç ”ç©¶æ—¨åœ¨æ¢ç´¢åŸºç¡€æ¨¡å‹å¦‚ä½•é€šè¿‡æ•´åˆè§†è§‰ä¸æ–‡æœ¬æ•°æ®ã€ç†è§£ç—‡çŠ¶-ç®¡ç†å…³ç³»ä»¥åŠæ”¯æŒäº¤äº’å¼é—®ç­”æ¥é©æ–°å®šç‚¹ç—…å®³ç®¡ç†ï¼Œè§£å†³ä¼ ç»Ÿç¥ç»ç½‘ç»œåœ¨å†œä¸šåº”ç”¨ä¸­å­˜åœ¨çš„å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶ç³»ç»Ÿç­›é€‰äº†çº¦40ç¯‡ç›¸å…³æ–‡çŒ®ï¼Œé‡ç‚¹åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å†œä¸šé¢†åŸŸçš„åº”ç”¨ï¼Œæ¢è®¨äº†è¿™äº›æ¨¡å‹åœ¨è‡ªé€‚åº”å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ä»¥åŠæ•°å­—å­ªç”Ÿæ¡†æ¶ä¸­çš„é›†æˆæ–¹å¼ï¼Œç‰¹åˆ«å…³æ³¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å¦‚ä½•å®ç°ç—‡çŠ¶è¯†åˆ«ä¸ç®¡ç†å†³ç­–çš„ååŒä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘ç°åŸºç¡€æ¨¡å‹åº”ç”¨å‘ˆç°å¿«é€Ÿå¢é•¿è¶‹åŠ¿ï¼Œ2023-24å¹´æ–‡çŒ®æ•°é‡æ¿€å¢ï¼›è§†è§‰è¯­è¨€æ¨¡å‹å‘å±•é€Ÿåº¦è¿œè¶…å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘è¡¨é‡å¢é•¿5-10å€ï¼›å¼ºåŒ–å­¦ä¹ å’Œè‡ªé€‚åº”å­¦ä¹ åœ¨æ™ºèƒ½å–·æ´’é¢†åŸŸä»å¤„äºèµ·æ­¥é˜¶æ®µï¼›æ•°å­—å­ªç”Ÿç»“åˆå¼ºåŒ–å­¦ä¹ å¯æœ‰æ•ˆæ¨¡æ‹Ÿé¶å‘å–·æ´’è¿‡ç¨‹ï¼›äººæœºåä½œç‰¹åˆ«æ˜¯äººåœ¨ç¯æ–¹æ³•åº”ç”¨æœ‰é™ã€‚</p>
<p><strong>Conclusion:</strong> åŸºç¡€æ¨¡å‹æ­£æˆä¸ºå†œä¸šæ™ºèƒ½åŒ–çš„å…³é”®é©±åŠ¨åŠ›ï¼Œå¤šæ¨¡æ€æ¨¡å‹ç»“åˆå®æ—¶åé¦ˆå°†å¡‘é€ ä¸‹ä¸€ä»£å®šç‚¹ç—…å®³ç®¡ç†ç³»ç»Ÿï¼›è§£å†³æ¨¡æ‹Ÿåˆ°ç°å®çš„è½¬æ¢å·®è·æ˜¯å®ç°å®é™…éƒ¨ç½²çš„å…³é”®æŒ‘æˆ˜ï¼›åŠ å¼ºäººæœºåä½œç‰¹åˆ«æ˜¯äººç±»ä¸“å®¶å¯¹ä¸ç¡®å®šç—…ä¾‹çš„éªŒè¯æœºåˆ¶æ˜¯æœªæ¥é‡è¦å‘å±•æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>Site-specific disease management (SSDM) in crops has advanced rapidly through
machine and deep learning (ML and DL) for real-time computer vision. Research
evolved from handcrafted feature extraction to large-scale automated feature
learning. With foundation models (FMs), crop disease datasets are now processed
in fundamentally new ways. Unlike traditional neural networks, FMs integrate
visual and textual data, interpret symptoms in text, reason about
symptom-management relationships, and support interactive QA for growers and
educators. Adaptive and imitation learning in robotics further enables
field-based disease management. This review screened approx. 40 articles on FM
applications for SSDM, focusing on large-language models (LLMs) and
vision-language models (VLMs), and discussing their role in adaptive learning
(AL), reinforcement learning (RL), and digital twin frameworks for targeted
spraying. Key findings: (a) FMs are gaining traction with surging literature in
2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL
and AL are still nascent for smart spraying; (d) digital twins with RL can
simulate targeted spraying virtually; (e) addressing the sim-to-real gap is
critical for real-world deployment; (f) human-robot collaboration remains
limited, especially in human-in-the-loop approaches where robots detect early
symptoms and humans validate uncertain cases; (g) multi-modal FMs with
real-time feedback will drive next-gen SSDM. For updates, resources, and
contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to
submit papers, code, or datasets.</p>
  </article>
</body>
</html>
