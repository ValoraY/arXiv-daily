{"id": "2512.19864", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19864", "abs": "https://arxiv.org/abs/2512.19864", "authors": ["Shashi Kant Gupta", "Arijeet Pramanik", "Jerrin John Thomas", "Regina Schwind", "Lauren Wiener", "Avi Raju", "Jeremy Kornbluth", "Yanshan Wang", "Zhaohui Su", "Hrituraj Singh"], "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data", "comment": "39 Pages, Supplementary Included", "summary": "Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u80bf\u7624\u5b66\u6570\u636e\uff0c\u5728\u5305\u542b40\u4e07\u4efd\u4e34\u5e8a\u6587\u6863\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5e73\u5747F1\u5206\u65700.93\u7684\u4f18\u5f02\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u4e30\u5bcc\u7684\u80bf\u7624\u6cbb\u7597\u4fe1\u606f\uff0c\u4f46\u7531\u4e8e\u672f\u8bed\u4e13\u4e1a\u6027\u5f3a\u3001\u6587\u6863\u683c\u5f0f\u4e0d\u4e00\u81f4\u4ee5\u53ca\u4fe1\u606f\u77db\u76fe\u7b49\u95ee\u9898\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u573a\u666f\u6216\u53d8\u91cf\uff0c\u65e0\u6cd5\u5b9e\u73b0\u8de8\u6587\u6863\u7684\u60a3\u8005\u7ea7\u7efc\u5408\u6570\u636e\u63d0\u53d6\uff0c\u800c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u590d\u6742\u7684\u80bf\u7624\u5b66\u6570\u636e\u63d0\u53d6\u4efb\u52a1\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u3001\u81ea\u9002\u5e94\u7684\u5b50\u4efb\u52a1\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u914d\u5907\u4e0a\u4e0b\u6587\u654f\u611f\u68c0\u7d22\u548c\u8fed\u4ee3\u5408\u6210\u80fd\u529b\uff0c\u4ece\u771f\u5b9e\u4e16\u754c\u7684\u80bf\u7624\u5b66\u7b14\u8bb0\u4e2d\u5168\u9762\u63d0\u53d6\u7ed3\u6784\u5316\u4e34\u5e8a\u53d8\u91cf\u3002", "result": "\u5728\u5305\u542b40\u4e07\u4efd\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u548c\u626b\u63cfPDF\u62a5\u544a\u3001\u6db5\u76d62250\u540d\u764c\u75c7\u60a3\u8005\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747F1\u5206\u6570\u8fbe\u52300.93\uff0c103\u4e2a\u80bf\u7624\u7279\u5f02\u6027\u4e34\u5e8a\u53d8\u91cf\u4e2d\u6709100\u4e2a\u8d85\u8fc70.85\uff0c\u5173\u952e\u53d8\u91cf\u5982\u751f\u7269\u6807\u5fd7\u7269\u548c\u836f\u7269\u4fe1\u606f\u8d85\u8fc70.95\uff0c\u96c6\u6210\u5230\u6570\u636e\u6574\u7406\u5de5\u4f5c\u6d41\u540e\u83b7\u5f97\u4e860.94\u7684\u76f4\u63a5\u4eba\u5de5\u6279\u51c6\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8eLLM\u667a\u80fd\u4f53\u7684\u7aef\u5230\u7aef\u7ed3\u6784\u5316\u80bf\u7624\u5b66\u6570\u636e\u63d0\u53d6\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86\u667a\u80fd\u4f53\u6846\u67b6\u5728\u5904\u7406\u590d\u6742\u533b\u7597\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u4e34\u5e8a\u6570\u636e\u81ea\u52a8\u5316\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u5de5\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u6570\u636e\u63d0\u53d6\u7684\u5168\u9762\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2512.19903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19903", "abs": "https://arxiv.org/abs/2512.19903", "authors": ["Kirk Vanacore", "Rene F. Kizilcec"], "title": "How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse", "comment": null, "summary": "Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u9700\u5b9a\u5236\u5316\u7684\u60c5\u51b5\u4e0b\u5bf9\u771f\u5b9e\u6559\u80b2\u573a\u666f\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u53d1\u73b0\u57fa\u7840\u6a21\u578b\u5728\u5206\u7c7b\u6559\u5b66\u884c\u4e3a\u65b9\u9762\u8868\u73b0\u51fa\u6709\u610f\u4e49\u4f46\u6709\u9650\u7684\u80fd\u529b\uff0c\u63d0\u793a\u8bbe\u8ba1\u80fd\u63d0\u5347\u6027\u80fd\u4f46\u65e0\u6cd5\u6d88\u9664\u6839\u672c\u7684\u53ef\u9760\u6027\u9650\u5236\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u6280\u672f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u4f18\u5316\uff0c\u800c\u5bf9\u6a21\u578b\u5728\u65e0\u9700\u663e\u8457\u5b9a\u5236\u5316\u7684\u60c5\u51b5\u4e0b\u89e3\u91ca\u771f\u5b9e\u6559\u80b2\u573a\u666f\u7684\u80fd\u529b\u4e86\u89e3\u4e0d\u8db3\u3002\u5728LLM\u7cfb\u7edf\u88ab\u5b66\u4e60\u8005\u548c\u6559\u80b2\u8005\u5e7f\u6cdb\u91c7\u7528\u7684\u80cc\u666f\u4e0b\uff0c\u7406\u89e3\u5176\u5f00\u7bb1\u5373\u7528\u7684\u80fd\u529b\u5bf9\u4e8e\u8bbe\u5b9a\u671f\u671b\u548c\u5efa\u7acb\u57fa\u51c6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u516d\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u8bfe\u5802\u8f6c\u5f55\u672c\u4e2d\u5206\u7c7b\u6559\u5b66\u884c\u4e3a\u7684\u57fa\u7840\u6027\u80fd\u3002\u8bc4\u4f30\u4e86\u5178\u578b\u7684\u63d0\u793a\u65b9\u6cd5\uff1a\u96f6\u6837\u672c\u3001\u5355\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\uff0c\u901a\u8fc7\u4e13\u5bb6\u7f16\u7801\u6ce8\u91ca\u4f5c\u4e3a\u57fa\u51c6\u6765\u6d4b\u91cf\u6a21\u578b\u7684\u5206\u7c7b\u51c6\u786e\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u96f6\u6837\u672c\u6027\u80fd\u4e2d\u7b49\uff0c\u4f46\u63d0\u4f9b\u5168\u9762\u793a\u4f8b\u7684\u5c11\u6837\u672c\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6700\u5f3a\u914d\u7f6e\u8fbe\u5230Cohen's Kappa = 0.58\u3002\u7136\u800c\u6539\u8fdb\u5e76\u4e0d\u5747\u5300\u6216\u5b8c\u5168\uff1a\u6027\u80fd\u56e0\u6559\u5b66\u884c\u4e3a\u7c7b\u578b\u800c\u5f02\uff0c\u66f4\u9ad8\u7684\u53ec\u56de\u7387\u901a\u5e38\u4ee5\u589e\u52a0\u8bef\u62a5\u4e3a\u4ee3\u4ef7\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5728\u89e3\u91ca\u6559\u5b66\u8bdd\u8bed\u65b9\u9762\u8868\u73b0\u51fa\u6709\u610f\u4e49\u4f46\u6709\u9650\u7684\u80fd\u529b\uff0c\u63d0\u793a\u8bbe\u8ba1\u6709\u52a9\u4e8e\u5c55\u73b0\u6a21\u578b\u6f5c\u529b\u4f46\u65e0\u6cd5\u6d88\u9664\u6839\u672c\u7684\u53ef\u9760\u6027\u7ea6\u675f\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u771f\u5b9e\u6559\u80b2\u5e94\u7528\u4e2d\u9700\u8981\u8c28\u614e\u8bc4\u4f30LLM\u6027\u80fd\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u9700\u8981\u89e3\u51b3\u6a21\u578b\u5728\u7279\u5b9a\u6559\u5b66\u884c\u4e3a\u5206\u7c7b\u4e0a\u7684\u4e0d\u4e00\u81f4\u6027\u3002"}}
{"id": "2512.19933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.19933", "abs": "https://arxiv.org/abs/2512.19933", "authors": ["Zhixiang Lu", "Xueyuan Deng", "Yiran Liu", "Yulong Li", "Qiang Yan", "Imran Razzak", "Jionglong Su"], "title": "PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation", "comment": null, "summary": "Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PRISM\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ed3\u5408\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u548c\u4e2a\u6027\u6761\u4ef6\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u5728\u7ebf\u6781\u5316\u4e2d\u7684\u5fc3\u7406\u5f02\u8d28\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u683c\u4e00\u81f4\u6027\u5e76\u6210\u529f\u590d\u73b0\u4e86\u7406\u6027\u6291\u5236\u548c\u60c5\u611f\u5171\u9e23\u7b49\u6d8c\u73b0\u73b0\u8c61\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4ee3\u7406\u7684\u610f\u89c1\u52a8\u6001\u6a21\u578b\u56e0\u91c7\u7528\u7b80\u5316\u7684\u540c\u8d28\u6027\u5047\u8bbe\u800c\u65e0\u6cd5\u6355\u6349\u9a71\u52a8\u5728\u7ebf\u6781\u5316\u7684\u5fc3\u7406\u5f02\u8d28\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u4e2a\u4f53\u8ba4\u77e5\u504f\u5dee\u4e0e\u4fe1\u606f\u4f20\u64ad\u4e4b\u95f4\u5173\u952e\u76f8\u4e92\u4f5c\u7528\u7684\u7406\u89e3\uff0c\u963b\u788d\u4e86\u5bf9\u610f\u8bc6\u5f62\u6001\u5206\u6b67\u653e\u5927\u673a\u5236\u7684\u7406\u89e3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e2a\u6027\u6298\u5c04\u667a\u80fd\u4eff\u771f\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u5c06\u7528\u4e8e\u8fde\u7eed\u60c5\u7eea\u6f14\u5316\u7684\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u4e0e\u7528\u4e8e\u79bb\u6563\u51b3\u7b56\u7684\u4e2a\u6027\u6761\u4ef6\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u76f8\u7ed3\u5408\uff0c\u8be5\u6a21\u578b\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5206\u914d\u57fa\u4e8e\u8fc8\u5c14\u65af-\u5e03\u91cc\u683c\u65af\u7c7b\u578b\u6307\u6807\u7684\u8ba4\u77e5\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u7684\u6570\u636e\u9a71\u52a8\u5148\u9a8c\u8fdb\u884c\u521d\u59cb\u5316\u3002", "result": "PRISM\u6a21\u578b\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u771f\u5b9e\u60c5\u51b5\u4e00\u81f4\u7684\u4eba\u683c\u4e00\u81f4\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u7684\u540c\u8d28\u6027\u548c\u5927\u4e94\u4eba\u683c\u57fa\u51c6\uff0c\u8be5\u6846\u67b6\u6709\u6548\u590d\u73b0\u4e86\u7406\u6027\u6291\u5236\u548c\u60c5\u611f\u5171\u9e23\u7b49\u6d8c\u73b0\u73b0\u8c61\uff0c\u4e3a\u5206\u6790\u590d\u6742\u793e\u4ea4\u5a92\u4f53\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u6846\u67b6\u6765\u5206\u6790\u590d\u6742\u7684\u793e\u4ea4\u5a92\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u8fde\u7eed\u60c5\u7eea\u6f14\u5316\u548c\u79bb\u6563\u51b3\u7b56\u8fc7\u7a0b\uff0c\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u4e2a\u4f53\u5fc3\u7406\u5f02\u8d28\u6027\u5982\u4f55\u9a71\u52a8\u5728\u7ebf\u6781\u5316\u73b0\u8c61\uff0c\u4e3a\u793e\u4f1a\u79d1\u5b66\u8ba1\u7b97\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u5de5\u5177\u3002"}}
{"id": "2512.20136", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20136", "abs": "https://arxiv.org/abs/2512.20136", "authors": ["Hyeongcheol Park", "Jiyoung Seo", "Jaewon Mun", "Hogun Park", "Wonmin Byeon", "Sung June Kim", "Hyeonsoo Im", "JeungSub Lee", "Sangpil Kim"], "title": "M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faM\u00b3KG-RAG\uff0c\u4e00\u79cd\u591a\u8df3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u5b9e\u4f53\u4e09\u5143\u7ec4\u548c\u5f15\u5165GRASP\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u97f3\u9891-\u89c6\u89c9\u9886\u57df\u7684\u63a8\u7406\u6df1\u5ea6\u548c\u7b54\u6848\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u97f3\u9891-\u89c6\u89c9\u9886\u57df\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u4e00\u662f\u73b0\u6709\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u7684\u6a21\u6001\u8986\u76d6\u6709\u9650\u4e14\u591a\u8df3\u8fde\u63a5\u4e0d\u8db3\uff0c\u4e8c\u662f\u4ec5\u57fa\u4e8e\u5171\u4eab\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u7684\u76f8\u4f3c\u6027\u68c0\u7d22\u65e0\u6cd5\u6709\u6548\u8fc7\u6ee4\u65e0\u5173\u6216\u5197\u4f59\u77e5\u8bc6\uff0c\u5bfc\u81f4\u63a8\u7406\u6df1\u5ea6\u548c\u7b54\u6848\u5fe0\u5b9e\u5ea6\u53d7\u9650\u3002", "method": "\u672c\u6587\u63d0\u51faM\u00b3KG-RAG\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u9996\u5148\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u591a\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\u6784\u5efa\u591a\u8df3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u5176\u4e2d\u5305\u542b\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u5b9e\u4f53\u4e09\u5143\u7ec4\uff0c\u652f\u6301\u57fa\u4e8e\u8f93\u5165\u67e5\u8be2\u7684\u6a21\u6001\u611f\u77e5\u68c0\u7d22\uff1b\u5176\u6b21\u5f15\u5165GRASP\u673a\u5236\uff0c\u786e\u4fdd\u5b9e\u4f53\u4e0e\u67e5\u8be2\u7684\u7cbe\u786e\u5bf9\u9f50\uff0c\u8bc4\u4f30\u7b54\u6848\u652f\u6301\u76f8\u5173\u6027\uff0c\u5e76\u526a\u679d\u5197\u4f59\u4e0a\u4e0b\u6587\uff0c\u4ec5\u4fdd\u7559\u751f\u6210\u54cd\u5e94\u6240\u5fc5\u9700\u7684\u77e5\u8bc6\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cM\u00b3KG-RAG\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u548c\u5b9e\u4f53\u5bf9\u9f50\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u97f3\u9891-\u89c6\u89c9\u9886\u57df\u7684\u77e5\u8bc6\u68c0\u7d22\u548c\u7b54\u6848\u751f\u6210\u8d28\u91cf\u65b9\u9762\u53d6\u5f97\u4e86\u660e\u663e\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u8df3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u4e0e\u9009\u62e9\u6027\u68c0\u7d22\u526a\u679d\u673a\u5236\u76f8\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u5e7f\u6cdb\u7684\u591a\u6a21\u6001\u5e94\u7528\u573a\u666f\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f18\u5316\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u548c\u68c0\u7d22\u6548\u7387\u3002"}}
{"id": "2512.19957", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19957", "abs": "https://arxiv.org/abs/2512.19957", "authors": ["Luciano Araujo Dourado Filho", "Almir Moreira da Silva Neto", "Rodrigo Pereira David", "Rodrigo Tripodi Calumby"], "title": "Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification", "comment": null, "summary": "This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \\href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8ePlantClef 2025\u7ec6\u7c92\u5ea6\u591a\u6807\u7b7e\u7269\u79cd\u8bc6\u522b\u6311\u6218\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u7c7b\u522b\u539f\u578b\u4f5c\u4e3a\u4ee3\u7406\u6307\u5bfc\uff0c\u8bad\u7ec3\u5206\u5272\u89c6\u89c9Transformer\u5728\u6d4b\u8bd5\u96c6\u56fe\u50cf\u4e0a\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\uff0c\u6700\u7ec8\u5728\u7ade\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e94\u540d\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3PlantClef 2025\u6311\u6218\u4e2d\u7684\u7ec6\u7c92\u5ea6\u591a\u6807\u7b7e\u7269\u79cd\u8bc6\u522b\u95ee\u9898\uff0c\u8be5\u4efb\u52a1\u9700\u8981\u5728\u5305\u542b\u591a\u79cd\u7269\u79cd\u7684\u9ad8\u5206\u8fa8\u7387\u690d\u88ab\u56fe\u50cf\u4e2d\u8fdb\u884c\u7cbe\u786e\u8bc6\u522b\uff0c\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u5982\u4f55\u4ece\u591a\u7c7b\u522b\u4e2a\u4f53\u7269\u79cd\u8bc6\u522b\u9002\u5e94\u5230\u9ad8\u5206\u8fa8\u7387\u690d\u88ab\u5730\u5757\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u3002", "method": "\u65b9\u6cd5\u91c7\u7528\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u7684\u7c7b\u522b\u539f\u578b\u4f5c\u4e3a\u4ee3\u7406\u6307\u5bfc\uff0c\u901a\u8fc7K-Means\u805a\u7c7b\uff08K\u7b49\u4e8e\u6570\u636e\u96c6\u7c7b\u522b\u6570\uff09\u4ece\u8bad\u7ec3\u56fe\u50cf\u7279\u5f81\u4e2d\u521b\u5efa\u7c7b\u522b\u8868\u793a\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5b9a\u5236\u5316\u7684\u7a84\u89c6\u89c9Transformer\uff0c\u7528\u51bb\u7ed3\u7684DinoV2\u66ff\u6362\u4e86\u539f\u59cb\u8865\u4e01\u5d4c\u5165\u5c42\uff0c\u8be5\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u4e2a\u4f53\u7269\u79cd\u5206\u7c7b\u7684\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u8bad\u7ec3\u8be5\u6a21\u578b\u4ece\u6d4b\u8bd5\u6570\u636e\u96c6\u56fe\u50cf\u4e2d\u91cd\u5efa\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u7c7b\u522b\u539f\u578b\uff0c\u5229\u7528\u83b7\u5f97\u7684\u6ce8\u610f\u529b\u5206\u6570\u6765\u8bc6\u522b\u548c\u5b9a\u4f4d\u611f\u5174\u8da3\u533a\u57df\u4ee5\u6307\u5bfc\u5206\u7c7b\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728PlantCLEF 2025\u6311\u6218\u8d5b\u7684\u79c1\u6709\u6392\u884c\u699c\u4e0a\u83b7\u5f97\u7b2c\u4e94\u540d\uff0cF1\u5206\u6570\u4e3a0.33331\uff0c\u4e0e\u6700\u4f73\u63d0\u4ea4\u7ed3\u679c\u4ec5\u76f8\u5dee0.03\u5206\uff0c\u8868\u660e\u5728\u57fa\u51c6\u4efb\u52a1\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u4f7f\u7528\u7c7b\u522b\u539f\u578b\u4f5c\u4e3a\u4ee3\u7406\u6307\u5bfc\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u5c06\u591a\u7c7b\u522b\u4e2a\u4f53\u7269\u79cd\u8bc6\u522b\u9002\u5e94\u5230\u9ad8\u5206\u8fa8\u7387\u690d\u88ab\u5730\u5757\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\uff0c\u8be5\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u690d\u7269\u8bc6\u522b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u7ade\u4e89\u6027\u80fd\uff0c\u4e3a\u7c7b\u4f3c\u7684\u591a\u6807\u7b7e\u89c6\u89c9\u8bc6\u522b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u601d\u8def\u3002"}}
{"id": "2512.19918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.19918", "abs": "https://arxiv.org/abs/2512.19918", "authors": ["Houston H. Zhang", "Tao Zhang", "Baoze Lin", "Yuanqi Xue", "Yincheng Zhu", "Huan Liu", "Li Gu", "Linfeng Ye", "Ziqiang Wang", "Xinxin Zuo", "Yang Wang", "Yuanhao Yu", "Zhixiang Chi"], "title": "Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs", "comment": "Code: https://github.com/Djanghao/widget2code", "summary": "User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Widget-to-Code\uff08Widget2Code\uff09\u4efb\u52a1\uff0c\u9488\u5bf9\u7d27\u51d1\u3001\u65e0\u4e0a\u4e0b\u6587\u7684\u5c0f\u90e8\u4ef6\u754c\u9762\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u5e76\u5f00\u53d1\u4e86WidgetFactory\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u542b\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u548c\u7f16\u8bd1\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709UI2Code\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7f51\u9875\u548c\u79fb\u52a8\u754c\u9762\uff0c\u800c\u5c0f\u90e8\u4ef6\u4f5c\u4e3a\u7d27\u51d1\u3001\u65e0\u4e0a\u4e0b\u6587\u7684\u5fae\u754c\u9762\uff0c\u5177\u6709\u5bc6\u96c6\u5e03\u5c40\u548c\u56fe\u6807\u5316\u7279\u5f81\uff0c\u4e14\u7f3a\u4e4f\u53ef\u8bbf\u95ee\u7684\u6807\u8bb0\u6570\u636e\uff0c\u8fd9\u4e00\u9886\u57df\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u8054\u5408\u63a8\u8fdb\u611f\u77e5\u7406\u89e3\u548c\u7ed3\u6784\u5316\u4ee3\u7801\u751f\u6210\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u611f\u77e5\u5c42\u9762\u9075\u5faa\u5c0f\u90e8\u4ef6\u8bbe\u8ba1\u539f\u5219\u5c06\u539f\u5b50\u7ec4\u4ef6\u7ec4\u88c5\u4e3a\u5b8c\u6574\u5e03\u5c40\uff0c\u914d\u5907\u56fe\u6807\u68c0\u7d22\u548c\u53ef\u91cd\u7528\u53ef\u89c6\u5316\u6a21\u5757\uff1b\u5728\u7cfb\u7edf\u5c42\u9762\u8bbe\u8ba1\u4e86\u7aef\u5230\u7aef\u57fa\u7840\u8bbe\u65bdWidgetFactory\uff0c\u5305\u542b\u6846\u67b6\u65e0\u5173\u7684\u5c0f\u90e8\u4ef6\u9886\u57df\u7279\u5b9a\u8bed\u8a00WidgetDSL\u548c\u53ef\u7f16\u8bd1\u4e3a\u591a\u79cd\u524d\u7aef\u5b9e\u73b0\u7684\u7f16\u8bd1\u5668\uff0c\u81ea\u9002\u5e94\u6e32\u67d3\u6a21\u5757\u8fdb\u4e00\u6b65\u4f18\u5316\u7a7a\u95f4\u7ef4\u5ea6\u4ee5\u6ee1\u8db3\u7d27\u51d1\u6027\u7ea6\u675f\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u5c3d\u7ba1\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u4e8e\u4e13\u95e8\u7684UI2Code\u65b9\u6cd5\uff0c\u4f46\u4ecd\u4ea7\u751f\u4e0d\u53ef\u9760\u4e14\u89c6\u89c9\u4e0d\u4e00\u81f4\u7684\u4ee3\u7801\uff1b\u63d0\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5\u901a\u8fc7WidgetFactory\u57fa\u7840\u8bbe\u65bd\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u4e3aWidget2Code\u7814\u7a76\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f62\u5f0f\u5316\u4e86Widget2Code\u4efb\u52a1\u5e76\u5efa\u7acb\u4e86\u9996\u4e2a\u4ec5\u56fe\u50cf\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u611f\u77e5\u7406\u89e3\u548c\u7ed3\u6784\u5316\u4ee3\u7801\u751f\u6210\u89e3\u51b3\u4e86\u5c0f\u90e8\u4ef6\u7279\u6709\u7684\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u65b9\u6cd5\u548c\u7edf\u4e00\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u63a8\u52a8\u4e86\u7d27\u51d1\u754c\u9762\u4ee3\u7801\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.20145", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20145", "abs": "https://arxiv.org/abs/2512.20145", "authors": ["Xiang Chen", "Yixin Ou", "Quan Feng", "Lei Li", "Piji Li", "Haibo Ye", "Sheng-Jun Huang", "Shuofei Qiao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models", "comment": "IEEE/ACM Transactions on Audio, Speech and Language Processing", "summary": "The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRetroPrompt\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u68c0\u7d22\u673a\u5236\u548c\u89e3\u8026\u8bb0\u5fc6\u4e0e\u6cdb\u5316\uff0c\u89e3\u51b3\u4f20\u7edf\u63d0\u793a\u5b66\u4e60\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u8bb0\u5fc6\u5316\u7684\u95ee\u9898\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4ecd\u9075\u5faa\u53c2\u6570\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u5728\u8bb0\u5fc6\u5316\u548c\u673a\u68b0\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u6cdb\u5316\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u96be\u4ee5\u5145\u5206\u5229\u7528\u975e\u5178\u578b\u5b9e\u4f8b\u5e76\u907f\u514d\u5728\u6709\u9650\u6570\u636e\u4e0b\u5bf9\u6d45\u5c42\u6a21\u5f0f\u7684\u8fc7\u62df\u5408\u3002", "method": "RetroPrompt\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u77e5\u8bc6\u4e0e\u8bb0\u5fc6\u5316\uff0c\u5728\u8f93\u5165\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u751f\u6210\u7684\u516c\u5f00\u77e5\u8bc6\u5e93\u68c0\u7d22\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4e3b\u52a8\u4ece\u8bed\u6599\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\u4fe1\u606f\u4ee5\u589e\u5f3a\u53ef\u7528\u7ebf\u7d22\u3002", "result": "\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cRetroPrompt\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u901a\u8fc7\u8bb0\u5fc6\u6a21\u5f0f\u5206\u6790\u8bc1\u5b9e\u5176\u6709\u6548\u51cf\u5c11\u4e86\u5bf9\u673a\u68b0\u8bb0\u5fc6\u7684\u4f9d\u8d56\u3002", "conclusion": "RetroPrompt\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u673a\u5236\u5e73\u8861\u8bb0\u5fc6\u4e0e\u6cdb\u5316\uff0c\u4e3a\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u63d0\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u8868\u660e\u89e3\u8026\u77e5\u8bc6\u8bb0\u5fc6\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.20074", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20074", "abs": "https://arxiv.org/abs/2512.20074", "authors": ["H M Quamran Hasan", "Housam Khalifa Bashier", "Jiayi Dai", "Mi-Young Kim", "Randy Goebel"], "title": "Reason2Decide: Rationale-Driven Multi-Task Learning", "comment": null, "summary": "Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Reason2Decide\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u89e3\u51b3\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u9884\u6d4b\u4e0e\u89e3\u91ca\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u5728\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u53ef\u89e3\u91ca\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u5f53\u524d\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u9762\u4e34\u5173\u952e\u6311\u6218\uff1a\u5728\u5b9e\u73b0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u751f\u6210\u4e0e\u9884\u6d4b\u5bf9\u9f50\u7684\u89e3\u91ca\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u66b4\u9732\u504f\u5dee\u95ee\u9898\uff0c\u5bfc\u81f4\u89e3\u91ca\u4e0e\u9884\u6d4b\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u89e3\u51b3\u81ea\u89e3\u91ca\u4efb\u52a1\u4e2d\u7684\u66b4\u9732\u504f\u5dee\u548c\u4efb\u52a1\u5206\u79bb\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51faReason2Decide\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u751f\u6210\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8054\u5408\u8bad\u7ec3\u6807\u7b7e\u9884\u6d4b\u548c\u63a8\u7406\u751f\u6210\uff0c\u5e94\u7528\u8ba1\u5212\u91c7\u6837\u6280\u672f\u9010\u6b65\u4ece\u57fa\u4e8e\u9ec4\u91d1\u6807\u7b7e\u7684\u6761\u4ef6\u8f6c\u6362\u5230\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u7684\u6761\u4ef6\uff0c\u6709\u6548\u89e3\u51b3\u66b4\u9732\u504f\u5dee\u95ee\u9898\u3002", "result": "\u5728\u4e09\u4e2a\u533b\u7597\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cReason2Decide\u5728\u9884\u6d4b\u6027\u80fd\uff08F1\uff09\u548c\u63a8\u7406\u4fdd\u771f\u5ea6\uff08BERTScore\u3001BLEU\u3001LLM-as-a-Judge\uff09\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u5fae\u8c03\u57fa\u7ebf\u548c\u90e8\u5206\u96f6\u6837\u672cLLM\u3002\u5728\u5206\u8bca\u4efb\u52a1\u4e2d\uff0c\u8be5\u6846\u67b6\u5bf9LLM\u751f\u6210\u3001\u62a4\u58eb\u64b0\u5199\u548c\u62a4\u58eb\u540e\u5904\u7406\u7684\u63a8\u7406\u5747\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u4e14\u4ec5\u4f7f\u7528LLM\u751f\u6210\u63a8\u7406\u8fdb\u884c\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u5c31\u80fd\u8d85\u8d8a\u5176\u4ed6\u53d8\u4f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660eLLM\u751f\u6210\u7684\u63a8\u7406\u9002\u5408\u7528\u4e8e\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u51cf\u5c11\u5bf9\u4eba\u7c7b\u6807\u6ce8\u7684\u4f9d\u8d56\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cReason2Decide\u4f7f\u7528\u6bd4\u5f53\u4ee3\u57fa\u7840\u6a21\u578b\u5c0f40\u500d\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u8fd9\u4e9b\u6027\u80fd\u63d0\u5347\uff0c\u4f7f\u4e34\u5e8a\u63a8\u7406\u5728\u8d44\u6e90\u53d7\u9650\u7684\u90e8\u7f72\u4e2d\u66f4\u52a0\u53ef\u53ca\uff0c\u540c\u65f6\u4ecd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2512.19934", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19934", "abs": "https://arxiv.org/abs/2512.19934", "authors": ["Wentao Wu", "Xiao Wang", "Chenglong Li", "Jin Tang", "Bin Luo"], "title": "Vehicle-centric Perception via Multimodal Structured Pre-training", "comment": "Journal extension of VehicleMAE (AAAI 2024)", "summary": "Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVehicleMAE-V2\uff0c\u4e00\u79cd\u9762\u5411\u8f66\u8f86\u611f\u77e5\u7684\u9884\u8bad\u7ec3\u5927\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u8f66\u8f86\u76f8\u5173\u7684\u7ed3\u6784\u5316\u5148\u9a8c\u77e5\u8bc6\u6765\u6307\u5bfc\u63a9\u7801\u4ee4\u724c\u91cd\u5efa\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f66\u8f86\u4e2d\u5fc3\u611f\u77e5\u4efb\u52a1\u7684\u6cdb\u5316\u8868\u793a\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u7f3a\u4e4f\u5bf9\u8f66\u8f86\u76f8\u5173\u77e5\u8bc6\u7684\u6709\u6548\u5b66\u4e60\uff0c\u5bfc\u81f4\u5728\u5efa\u6a21\u901a\u7528\u8f66\u8f86\u611f\u77e5\u8868\u793a\u65f6\u80fd\u529b\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u8f66\u8f86\u7279\u6709\u7684\u7ed3\u6784\u7279\u5f81\u548c\u8bed\u4e49\u4fe1\u606f\u3002", "method": "\u8be5\u65b9\u6cd5\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u5bf9\u79f0\u5f15\u5bfc\u63a9\u7801\u6a21\u5757\u5229\u7528\u8f66\u8f86\u5bf9\u79f0\u6027\u7ea6\u675f\u9009\u62e9\u9ad8\u8d28\u91cf\u63a9\u7801\u56fe\u50cf\u5757\u5e76\u51cf\u5c11\u4fe1\u606f\u5197\u4f59\uff1b\u8f6e\u5ed3\u5f15\u5bfc\u8868\u793a\u6a21\u5757\u901a\u8fc7\u6700\u5c0f\u5316\u8f6e\u5ed3\u7279\u5f81\u4e0e\u91cd\u5efa\u7279\u5f81\u7684\u6982\u7387\u5206\u5e03\u5dee\u5f02\u6765\u4fdd\u7559\u6574\u4f53\u8f66\u8f86\u7ed3\u6784\u4fe1\u606f\uff1b\u8bed\u4e49\u5f15\u5bfc\u8868\u793a\u6a21\u5757\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u84b8\u998f\u5bf9\u9f50\u56fe\u50cf-\u6587\u672c\u7279\u5f81\u4ee5\u89e3\u51b3\u63a9\u7801\u91cd\u5efa\u4e2d\u7684\u8bed\u4e49\u6df7\u6dc6\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u6784\u5efa\u4e86\u5305\u542b\u7ea6400\u4e07\u8f66\u8f86\u56fe\u50cf\u548c12,693\u6761\u6587\u672c\u63cf\u8ff0\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6Autobot4M\uff0c\u5e76\u5728\u4e94\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86VehicleMAE-V2\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u5229\u7528\u8f66\u8f86\u7279\u6709\u7684\u7ed3\u6784\u5316\u5148\u9a8c\u77e5\u8bc6\uff08\u5bf9\u79f0\u6027\u3001\u8f6e\u5ed3\u3001\u8bed\u4e49\uff09\u53ef\u4ee5\u6709\u6548\u6307\u5bfc\u63a9\u7801\u81ea\u7f16\u7801\u5668\u7684\u9884\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4e3a\u8f66\u8f86\u4e2d\u5fc3\u611f\u77e5\u4efb\u52a1\u5b66\u4e60\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u8868\u793a\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u7684\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u9884\u8bad\u7ec3\u8303\u5f0f\u3002"}}
{"id": "2512.20578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20578", "abs": "https://arxiv.org/abs/2512.20578", "authors": ["Amirhosein Ghasemabadi", "Di Niu"], "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits", "comment": null, "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGnosis\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u81ea\u611f\u77e5\u673a\u5236\uff0c\u4f7f\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u89e3\u7801\u9690\u85cf\u72b6\u6001\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u4fe1\u53f7\u8fdb\u884c\u5185\u5728\u81ea\u6211\u9a8c\u8bc1\uff0c\u4ee5\u9884\u6d4b\u81ea\u8eab\u751f\u6210\u9519\u8bef\uff0c\u4ec5\u589e\u52a0\u7ea6500\u4e07\u53c2\u6570\u4e14\u4e0e\u5e8f\u5217\u957f\u5ea6\u65e0\u5173\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6d41\u7545\u590d\u6742\u7684\u8f93\u51fa\u4f46\u7ecf\u5e38\u65e0\u6cd5\u8bc6\u522b\u81ea\u8eab\u9519\u8bef\u548c\u5e7b\u89c9\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8bc4\u5224\u5668\u3001\u591a\u6837\u672c\u4e00\u81f4\u6027\u6216\u57fa\u4e8e\u6587\u672c\u7684\u81ea\u6211\u6279\u5224\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8981\u4e48\u589e\u52a0\u989d\u5916\u8ba1\u7b97\u6210\u672c\uff0c\u8981\u4e48\u4e0e\u771f\u5b9e\u6b63\u786e\u6027\u76f8\u5173\u6027\u8f83\u5f31\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u662f\u5426\u80fd\u591f\u901a\u8fc7\u63a8\u7406\u8fc7\u7a0b\u4e2d\u68c0\u67e5\u5185\u90e8\u72b6\u6001\u6765\u9884\u6d4b\u81ea\u8eab\u5931\u8d25\u3002", "method": "\u5f15\u5165Gnosis\u8f7b\u91cf\u7ea7\u81ea\u611f\u77e5\u673a\u5236\uff0c\u4f7f\u51bb\u7ed3\u7684LLMs\u80fd\u591f\u8fdb\u884c\u5185\u5728\u81ea\u6211\u9a8c\u8bc1\uff0c\u901a\u8fc7\u88ab\u52a8\u89c2\u5bdf\u5185\u90e8\u8f68\u8ff9\uff0c\u5c06\u5176\u538b\u7f29\u4e3a\u56fa\u5b9a\u9884\u7b97\u63cf\u8ff0\u7b26\uff0c\u5e76\u4ee5\u53ef\u5ffd\u7565\u7684\u63a8\u7406\u6210\u672c\u9884\u6d4b\u6b63\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u4ec5\u589e\u52a0\u7ea6500\u4e07\u53c2\u6570\uff0c\u72ec\u7acb\u4e8e\u5e8f\u5217\u957f\u5ea6\u8fd0\u884c\uff0c\u89e3\u7801\u9690\u85cf\u72b6\u6001\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u4fe1\u53f7\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u5f00\u653e\u57df\u95ee\u7b54\u548c\u5b66\u672f\u77e5\u8bc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9488\u5bf9\u4ece17\u4ebf\u5230200\u4ebf\u53c2\u6570\u7684\u51bb\u7ed3\u9aa8\u5e72\u6a21\u578b\uff0cGnosis\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u5185\u90e8\u57fa\u7ebf\u548c\u5927\u578b\u5916\u90e8\u8bc4\u5224\u5668\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u90e8\u5206\u751f\u6210\uff0c\u5b9e\u73b0\u5931\u8d25\u8f68\u8ff9\u7684\u65e9\u671f\u68c0\u6d4b\u548c\u8ba1\u7b97\u611f\u77e5\u63a7\u5236\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u53ef\u9760\u7684\u6b63\u786e\u6027\u7ebf\u7d22\u5185\u5728\u4e8e\u751f\u6210\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u5916\u90e8\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u63d0\u53d6\u3002Gnosis\u5c55\u793a\u4e86LLMs\u5185\u90e8\u72b6\u6001\u5305\u542b\u4e30\u5bcc\u7684\u81ea\u6211\u8bc4\u4f30\u4fe1\u606f\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u81ea\u6211\u76d1\u63a7\u7684\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2512.20140", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20140", "abs": "https://arxiv.org/abs/2512.20140", "authors": ["Xingyou Yin", "Ceyao Zhang", "Min Hu", "Kai Chen"], "title": "Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection", "comment": "9 pages,3 figures", "summary": "Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6807\u8bb0\u5316\u524d\u6ce8\u5165\u566a\u58f0\uff0c\u6765\u63d0\u5347\u51bb\u7ed3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u63a8\u7406\u65f6\u589e\u5f3a\u624b\u6bb5\uff0c\u8feb\u4f7f\u6a21\u578b\u57fa\u4e8e\u9c81\u68d2\u7684\u65f6\u95f4\u6a21\u5f0f\u800c\u975e\u8868\u9762\u6570\u503c\u4f2a\u5f71\u8fdb\u884c\u5916\u63a8\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8bd5\u56fe\u5229\u7528\u5b8c\u5168\u51bb\u7ed3\u7684\u3001\u672a\u7ecf\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4f46\u5176\u6027\u80fd\u5bf9\u8f93\u5165\u6570\u636e\u7684\u6587\u672c\u8868\u793a\u6781\u5176\u654f\u611f\uff0c\u56e0\u4e3a\u6a21\u578b\u53c2\u6570\u65e0\u6cd5\u9002\u5e94\u5206\u5e03\u504f\u79fb\u3002\u8fd9\u4e9b\u5b8c\u5168\u51bb\u7ed3\u6a21\u578b\u7684\u8868\u73b0\u8106\u5f31\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u5e72\u9884\u6765\u514b\u670d\u8fd9\u79cd\u8106\u5f31\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u7b56\u7565\uff1a\u5728\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6807\u8bb0\u5316\u4e3a\u6587\u672c\u8868\u793a\u4e4b\u524d\u6ce8\u5165\u566a\u58f0\u3002\u8fd9\u79cd\u975e\u4fb5\u5165\u6027\u5e72\u9884\u4f5c\u4e3a\u4e00\u79cd\u63a8\u7406\u65f6\u589e\u5f3a\u624b\u6bb5\uff0c\u8feb\u4f7f\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u9c81\u68d2\u7684\u57fa\u7840\u65f6\u95f4\u6a21\u5f0f\u800c\u975e\u8868\u9762\u7684\u6570\u503c\u4f2a\u5f71\u8fdb\u884c\u5916\u63a8\u3002\u8be5\u65b9\u6cd5\u4e0d\u6d89\u53ca\u4efb\u4f55\u6a21\u578b\u5fae\u8c03\uff0c\u4ec5\u901a\u8fc7\u7b56\u7565\u6027\u7684\u6570\u636e\u9884\u5904\u7406\u6765\u63d0\u5347\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5e76\u59cb\u7ec8\u89c2\u5bdf\u5230\u6027\u80fd\u63d0\u5347\u3002\u4e3a\u4e86\u5b8c\u5168\u6d88\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u671f\u95f4\u6570\u636e\u6c61\u67d3\u53ef\u80fd\u5e26\u6765\u7684\u504f\u5dee\uff0c\u7814\u7a76\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u9896\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u5b8c\u5168\u8d85\u51fa\u4e86\u6240\u6709\u4f7f\u7528\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u8303\u56f4\uff0c\u5e76\u5728\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0a\u4e00\u81f4\u89c2\u5bdf\u5230\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u76f4\u63a5\u5229\u7528\u73b0\u6210\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u8fdb\u4e00\u6b65\u7684\u8fdb\u5c55\u3002\u566a\u58f0\u6ce8\u5165\u4f5c\u4e3a\u4e00\u79cd\u63a8\u7406\u65f6\u589e\u5f3a\u624b\u6bb5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u51bb\u7ed3\u6a21\u578b\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6570\u503c\u5e8f\u5217\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u8bba\u542f\u793a\u3002"}}
{"id": "2512.19943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.19943", "abs": "https://arxiv.org/abs/2512.19943", "authors": ["Haoyi Zhong", "Fang-Lue Zhang", "Andrew Chalmers", "Taehyun Rhee"], "title": "SE360: Semantic Edit in 360$^\\circ$ Panoramas via Hierarchical Data Construction", "comment": null, "summary": "While instruction-based image editing is emerging, extending it to 360$^\\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SE360\uff0c\u4e00\u4e2a\u7528\u4e8e360\u00b0\u5168\u666f\u56fe\u50cf\u591a\u6761\u4ef6\u5f15\u5bfc\u5bf9\u8c61\u7f16\u8f91\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e3b\u6570\u636e\u751f\u6210\u7ba1\u9053\u548c\u4e24\u9636\u6bb5\u6570\u636e\u7cbe\u70bc\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5728\u6587\u672c\u3001\u63a9\u7801\u6216\u53c2\u8003\u56fe\u50cf\u5f15\u5bfc\u4e0b\u7684\u7075\u6d3b\u5bf9\u8c61\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u6269\u5c55\u5230360\u00b0\u5168\u666f\u56fe\u50cf\u65f6\u9762\u4e34\u989d\u5916\u6311\u6218\uff0c\u5728\u7b49\u8ddd\u67f1\u72b6\u6295\u5f71\u548c\u900f\u89c6\u89c6\u56fe\u4e2d\u5e38\u4ea7\u751f\u4e0d\u5408\u7406\u7ed3\u679c\uff0c\u9700\u8981\u89e3\u51b3\u5168\u666f\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u6838\u5fc3\u5305\u62ec\u65b0\u9896\u7684\u4ece\u7c97\u5230\u7ec6\u7684\u81ea\u4e3b\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u9002\u5e94\u6295\u5f71\u8c03\u6574\u8fdb\u884c\u5206\u5c42\u5206\u6790\uff0c\u786e\u4fdd\u5bf9\u8c61\u53ca\u5176\u7269\u7406\u73af\u5883\u7684\u6574\u4f53\u5206\u5272\uff1b\u540c\u65f6\u5f15\u5165\u7ecf\u6d4e\u9ad8\u6548\u7684\u4e24\u9636\u6bb5\u6570\u636e\u7cbe\u70bc\u7b56\u7565\uff0c\u63d0\u5347\u6570\u636e\u771f\u5b9e\u611f\u5e76\u51cf\u8f7b\u6a21\u578b\u5bf9\u64e6\u9664\u4f2a\u5f71\u7684\u8fc7\u62df\u5408\uff1b\u57fa\u4e8e\u6784\u5efa\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u8bed\u4e49\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u6570\u636e\u5bf9\u65e2\u5177\u6709\u8bed\u4e49\u610f\u4e49\u53c8\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u5373\u4f7f\u6765\u81ea\u672a\u6807\u8bb0\u7684\u5168\u666f\u56fe\u50cf\u4e5f\u80fd\u4fdd\u8bc1\u7f16\u8f91\u7ed3\u679c\u7684\u5408\u7406\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u81ea\u4e3b\u6570\u636e\u751f\u6210\u7ba1\u9053\u5728\u89e3\u51b3\u5168\u666f\u56fe\u50cf\u7f16\u8f91\u6311\u6218\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u7684\u591a\u6761\u4ef6\u5f15\u5bfc\u7f16\u8f91\u6846\u67b6\u4e3a360\u00b0\u5168\u666f\u56fe\u50cf\u7684\u7075\u6d3b\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u540c\u65f6\u6570\u636e\u7cbe\u70bc\u7b56\u7565\u4e3a\u51cf\u5c11\u6a21\u578b\u8fc7\u62df\u5408\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20595", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20595", "abs": "https://arxiv.org/abs/2512.20595", "authors": ["Dhruv Anand", "Ehsan Shareghi"], "title": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs", "comment": "27 pages, 5 figures, 9 tables. Cube available at https://github.com/dana-23/cube-bench", "summary": "We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Cube Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9b54\u65b9\u89e3\u8c1c\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u4e0e\u5e8f\u5217\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u4e94\u4e2a\u5206\u89e3\u6280\u80fd\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u63ed\u793a\u4e86\u6a21\u578b\u5728\u590d\u6742\u5e8f\u5217\u51b3\u7b56\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u4e0e\u5e8f\u5217\u63a8\u7406\u80fd\u529b\u7684\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u591a\u6b65\u51b3\u7b56\u548c\u9519\u8bef\u6062\u590d\u7684\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u8861\u91cf\u6a21\u578b\u7684\u5b9e\u9645\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86Cube Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c06\u9b54\u65b9\u89e3\u8c1c\u4efb\u52a1\u5206\u89e3\u4e3a\u4e94\u4e2a\u6838\u5fc3\u6280\u80fd\uff1a\u4ece\u56fe\u50cf\u548c\u6587\u672c\u91cd\u5efa\u9b54\u65b9\u9762\u3001\u9009\u62e9\u6700\u4f18\u4e0b\u4e00\u6b65\u79fb\u52a8\u3001\u9884\u6d4b\u5019\u9009\u79fb\u52a8\u7ed3\u679c\u800c\u4e0d\u6267\u884c\u3001\u6267\u884c\u591a\u6b65\u8ba1\u5212\u5e76\u4ece\u4e2d\u65ad\u6062\u590d\u3001\u68c0\u6d4b\u5e76\u4fee\u6b63\u81ea\u8eab\u9519\u8bef\u3002\u91c7\u7528\u7edf\u4e00\u7684\u52a0\u6270\u72b6\u6001\u3001\u63d0\u793a\u8bcd\u548c\u89e3\u6790\u5668\uff0c\u4ee5\u53ca\u5355\u4e00\u7684\u8ddd\u79bb\u89e3\u51b3\u5ea6\u91cf\u6807\u51c6\uff0c\u5728\u4e0d\u540c\u52a0\u6270\u6df1\u5ea6\u4e0b\u7cfb\u7edf\u6bd4\u8f83\u591a\u4e2aMLLM\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e03\u4e2aMLLM\u6a21\u578b\u663e\u793a\uff0c\u968f\u7740\u52a0\u6270\u6df1\u5ea6\u589e\u52a0\uff0c\u6a21\u578b\u51c6\u786e\u7387\u6025\u5267\u4e0b\u964d\uff1b\u4e00\u65e6\u8f68\u8ff9\u505c\u6ede\u6216\u53d1\u6563\uff0c\u6a21\u578b\u5f88\u5c11\u80fd\u6062\u590d\uff1b\u9ad8\u9762\u91cd\u5efa\u51c6\u786e\u7387\u4e0d\u80fd\u4fdd\u8bc1\u6709\u6548\u7684\u52a8\u4f5c\u9009\u62e9\u6216\u591a\u6b65\u6267\u884c\u80fd\u529b\u3002\u95ed\u6e90\u4e0e\u5f00\u6e90\u6a21\u578b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1a\u6700\u5f3a\u95ed\u6e90\u6a21\u578b\u5728\u5355\u6b65\u611f\u77e5\u548c\u591a\u6b65\u63a7\u5236\u4efb\u52a1\u4e2d\u5747\u9886\u5148\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u5728\u6700\u56f0\u96be\u8bbe\u7f6e\u4e0b\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff1b\u5373\u4f7f\u6700\u4f73MLLM\u5728\u66f4\u9ad8\u9b54\u65b9\u590d\u6742\u5ea6\u4e0b\u6027\u80fd\u4e5f\u4f1a\u9000\u5316\u3002\u7b80\u5355\u7684\u81ea\u6211\u6821\u6b63\u901a\u8fc7\u53cd\u601d\u601d\u7ef4\u5e26\u6765\u9002\u5ea6\u63d0\u5347\uff0c\u4f46\u4e5f\u53ef\u80fd\u5f15\u5165\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "conclusion": "Cube Bench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u3001\u53ef\u590d\u73b0\u7684\u5e8f\u5217\u7a7a\u95f4\u63a8\u7406\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86MLLM\u5728\u590d\u6742\u591a\u6b65\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u9519\u8bef\u6062\u590d\u80fd\u529b\u548c\u957f\u671f\u89c4\u5212\u80fd\u529b\u7684\u4e0d\u8db3\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\u5f53\u524dMLLM\u5728\u7a7a\u95f4\u5e8f\u5217\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u663e\u8457\u63d0\u5347\u7a7a\u95f4\uff0c\u4e14\u95ed\u6e90\u6a21\u578b\u5728\u6b64\u7c7b\u4efb\u52a1\u4e0a\u660e\u663e\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u8bc4\u4f30\u65b9\u5411\u3002"}}
{"id": "2512.20206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20206", "abs": "https://arxiv.org/abs/2512.20206", "authors": ["Zhe Sun", "Kunlun Wu", "Chuanjian Fu", "Zeming Song", "Langyong Shi", "Zihe Xue", "Bohan Jing", "Ying Yang", "Xiaomeng Gao", "Aijia Li", "Tianyu Guo", "Huiying Li", "Xueyuan Yang", "Rongkai Liu", "Xinyi He", "Yuxi Wang", "Yue Li", "Mingyuan Liu", "Yujie Lu", "Hongzhao Xie", "Shiyun Zhao", "Bo Dai", "Wei Wang", "Tao Yuan", "Song-Chun Zhu", "Yujia Peng", "Zhenliang Zhang"], "title": "TongSIM: A General Platform for Simulating Intelligent Machines", "comment": null, "summary": "As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TongSIM\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u5177\u8eab\u667a\u80fd\u4f53\u7684\u9ad8\u4fdd\u771f\u901a\u7528\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u4eff\u771f\u73af\u5883\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u3001\u7f3a\u4e4f\u901a\u7528\u8bad\u7ec3\u5e73\u53f0\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u4f9b\u591a\u6837\u5316\u7684\u5ba4\u5185\u5916\u573a\u666f\u548c\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u6765\u52a0\u901f\u901a\u7528\u5177\u8eab\u667a\u80fd\u7684\u53d1\u5c55\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7279\u522b\u662f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7814\u7a76\u91cd\u70b9\u6b63\u4ece\u5355\u6a21\u6001\u6587\u672c\u5904\u7406\u8f6c\u5411\u66f4\u590d\u6742\u7684\u591a\u6a21\u6001\u548c\u5177\u8eab\u667a\u80fd\u9886\u57df\u3002\u7136\u800c\uff0c\u73b0\u6709\u4eff\u771f\u5e73\u53f0\u5927\u591a\u8bbe\u8ba1\u72ed\u7a84\uff0c\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u5b9a\u5236\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u591f\u652f\u6301\u4ece\u4f4e\u7ea7\u5177\u8eab\u5bfc\u822a\u5230\u9ad8\u7ea7\u590d\u5408\u6d3b\u52a8\uff08\u5982\u591a\u667a\u80fd\u4f53\u793e\u4f1a\u4eff\u771f\u548c\u4eba\u673a\u534f\u4f5c\uff09\u7684\u901a\u7528\u8bad\u7ec3\u73af\u5883\u3002", "method": "TongSIM\u5e73\u53f0\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u3001\u901a\u7528\u7684\u5177\u8eab\u667a\u80fd\u4f53\u8bad\u7ec3\u548c\u8bc4\u4f30\u73af\u5883\uff0c\u5305\u542b\u8d85\u8fc7100\u4e2a\u591a\u6837\u5316\u7684\u591a\u623f\u95f4\u5ba4\u5185\u573a\u666f\u4ee5\u53ca\u4e00\u4e2a\u5f00\u653e\u5f0f\u3001\u4ea4\u4e92\u4e30\u5bcc\u7684\u6237\u5916\u57ce\u9547\u4eff\u771f\u3002\u5e73\u53f0\u5177\u5907\u5b9a\u5236\u573a\u666f\u3001\u4efb\u52a1\u81ea\u9002\u5e94\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u5316\u667a\u80fd\u4f53\u7c7b\u578b\u548c\u52a8\u6001\u73af\u5883\u4eff\u771f\u7b49\u7279\u5f81\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "TongSIM\u5e73\u53f0\u5b9e\u73b0\u4e86\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u80fd\u591f\u652f\u6301\u611f\u77e5\u3001\u8ba4\u77e5\u3001\u51b3\u7b56\u3001\u4eba\u673a\u534f\u4f5c\u4ee5\u53ca\u7a7a\u95f4\u548c\u793e\u4f1a\u63a8\u7406\u7b49\u591a\u79cd\u80fd\u529b\u8bc4\u4f30\u3002\u5176\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5e73\u53f0\uff0c\u52a0\u901f\u4e86\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u901a\u7528\u5177\u8eab\u667a\u80fd\u7684\u53d1\u5c55\u8fdb\u7a0b\u3002", "conclusion": "TongSIM\u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u901a\u7528\u5e73\u53f0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u4eff\u771f\u73af\u5883\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u63d0\u4f9b\u591a\u6837\u5316\u7684\u5ba4\u5185\u5916\u573a\u666f\u548c\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u52a0\u901f\u901a\u7528\u5177\u8eab\u667a\u80fd\u7684\u7814\u7a76\u548c\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8bbe\u65bd\u3002\u8be5\u5e73\u53f0\u7684\u8bbe\u8ba1\u5f3a\u8c03\u7075\u6d3b\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u6709\u671b\u63a8\u52a8\u5177\u8eab\u667a\u80fd\u4ece\u7279\u5b9a\u4efb\u52a1\u5411\u901a\u7528\u80fd\u529b\u7684\u8f6c\u53d8\u3002"}}
{"id": "2512.20011", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20011", "abs": "https://arxiv.org/abs/2512.20011", "authors": ["Blessing Agyei Kyem", "Joshua Kofi Asamoah", "Anthony Dontoh", "Andrews Danyo", "Eugene Denteh", "Armstrong Aboah"], "title": "PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification", "comment": null, "summary": "Automated pavement defect detection often struggles to generalize across diverse real-world conditions due to the lack of standardized datasets. Existing datasets differ in annotation styles, distress type definitions, and formats, limiting their integration for unified training. To address this gap, we introduce a comprehensive benchmark dataset that consolidates multiple publicly available sources into a standardized collection of 52747 images from seven countries, with 135277 bounding box annotations covering 13 distinct distress types. The dataset captures broad real-world variation in image quality, resolution, viewing angles, and weather conditions, offering a unique resource for consistent training and evaluation. Its effectiveness was demonstrated through benchmarking with state-of-the-art object detection models including YOLOv8-YOLOv12, Faster R-CNN, and DETR, which achieved competitive performance across diverse scenarios. By standardizing class definitions and annotation formats, this dataset provides the first globally representative benchmark for pavement defect detection and enables fair comparison of models, including zero-shot transfer to new environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u5168\u7403\u4ee3\u8868\u6027\u7684\u8def\u9762\u7f3a\u9677\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u516c\u5f00\u6570\u636e\u6e90\u5e76\u6807\u51c6\u5316\u6807\u6ce8\u683c\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u6807\u6ce8\u98ce\u683c\u3001\u7f3a\u9677\u5b9a\u4e49\u548c\u683c\u5f0f\u4e0a\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3a\u6a21\u578b\u8bad\u7ec3\u548c\u516c\u5e73\u6bd4\u8f83\u63d0\u4f9b\u4e86\u7edf\u4e00\u8d44\u6e90\u3002", "motivation": "\u81ea\u52a8\u5316\u8def\u9762\u7f3a\u9677\u68c0\u6d4b\u5728\u591a\u6837\u5316\u771f\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u6807\u51c6\u5316\u6570\u636e\u96c6\u3002\u73b0\u6709\u6570\u636e\u96c6\u5728\u6807\u6ce8\u98ce\u683c\u3001\u7f3a\u9677\u7c7b\u578b\u5b9a\u4e49\u548c\u683c\u5f0f\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u6574\u5408\u7528\u4e8e\u7edf\u4e00\u8bad\u7ec3\uff0c\u963b\u788d\u4e86\u6a21\u578b\u7684\u516c\u5e73\u6bd4\u8f83\u548c\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u7efc\u5408\u6027\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86\u591a\u4e2a\u516c\u5f00\u6570\u636e\u6e90\uff0c\u5305\u542b\u6765\u81ea\u4e03\u4e2a\u56fd\u5bb6\u768452747\u5f20\u56fe\u50cf\u548c135277\u4e2a\u8fb9\u754c\u6846\u6807\u6ce8\uff0c\u8986\u76d613\u79cd\u4e0d\u540c\u7684\u7f3a\u9677\u7c7b\u578b\u3002\u6570\u636e\u96c6\u6807\u51c6\u5316\u4e86\u7c7b\u522b\u5b9a\u4e49\u548c\u6807\u6ce8\u683c\u5f0f\uff0c\u5e76\u91c7\u7528\u6700\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08\u5305\u62ecYOLOv8-YOLOv12\u3001Faster R-CNN\u548cDETR\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5176\u5728\u591a\u6837\u5316\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u652f\u6301\u591a\u79cd\u5148\u8fdb\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\u3002\u6570\u636e\u96c6\u6355\u83b7\u4e86\u56fe\u50cf\u8d28\u91cf\u3001\u5206\u8fa8\u7387\u3001\u62cd\u6444\u89d2\u5ea6\u548c\u5929\u6c14\u6761\u4ef6\u7b49\u65b9\u9762\u7684\u5e7f\u6cdb\u771f\u5b9e\u4e16\u754c\u53d8\u5316\uff0c\u4e3a\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u65b0\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u8bc4\u4f30\u5e73\u53f0\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u5728\u591a\u6837\u5316\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u7403\u4ee3\u8868\u6027\u7684\u8def\u9762\u7f3a\u9677\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u6807\u6ce8\u89e3\u51b3\u4e86\u6570\u636e\u96c6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u7684\u516c\u5e73\u6bd4\u8f83\u3002\u6570\u636e\u96c6\u652f\u6301\u96f6\u6837\u672c\u8fc1\u79fb\u8bc4\u4f30\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u5316\u8def\u9762\u68c0\u6d4b\u6280\u672f\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002"}}
{"id": "2512.20276", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20276", "abs": "https://arxiv.org/abs/2512.20276", "authors": ["Yuntao Dai", "Hang Gu", "Teng Wang", "Qianyu Cheng", "Yifei Zheng", "Zhiyong Qiu", "Lei Gong", "Wenqi Lou", "Xuehai Zhou"], "title": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ActionFlow\uff0c\u4e00\u79cd\u9762\u5411\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u5e73\u53f0\u7684\u7cfb\u7edf\u7ea7\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u8bf7\u6c42\u6d41\u6c34\u7ebf\u8c03\u5ea6\u7b56\u7565\u548c\u5185\u5b58\u4f18\u5316\u6280\u672f\uff0c\u5c06VLA\u6a21\u578b\u7684\u63a8\u7406\u5ef6\u8fdf\u663e\u8457\u964d\u4f4e\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u52a8\u6001\u64cd\u4f5c\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u52a8\u6001\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u53d7\u5230\u9ad8\u63a8\u7406\u5ef6\u8fdf\u7684\u4e25\u91cd\u963b\u788d\uff0c\u867d\u7136\u6d41\u7545\u7684\u673a\u5668\u4eba\u4ea4\u4e92\u9700\u898120-30Hz\u7684\u63a7\u5236\u9891\u7387\uff0c\u4f46\u5f53\u524dVLA\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u901a\u5e38\u53ea\u80fd\u4ee53-5Hz\u8fd0\u884c\uff0c\u8fd9\u4e3b\u8981\u6e90\u4e8e\u81ea\u56de\u5f52\u89e3\u7801\u7684\u5185\u5b58\u53d7\u9650\u7279\u6027\uff0c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u5f80\u5f80\u9700\u8981\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\u6216\u4f1a\u635f\u5bb3\u6a21\u578b\u7cbe\u5ea6\u3002", "method": "ActionFlow\u7684\u6838\u5fc3\u662f\u8de8\u8bf7\u6c42\u6d41\u6c34\u7ebf\u7b56\u7565\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8c03\u5ea6\u5668\uff0c\u5c06VLA\u63a8\u7406\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5fae\u8bf7\u6c42\u7684\u5b8f\u6d41\u6c34\u7ebf\uff0c\u8be5\u7b56\u7565\u667a\u80fd\u5730\u5c06\u5185\u5b58\u53d7\u9650\u7684\u89e3\u7801\u9636\u6bb5\u4e0e\u8ba1\u7b97\u53d7\u9650\u7684\u9884\u586b\u5145\u9636\u6bb5\u5728\u8fde\u7eed\u65f6\u95f4\u6b65\u4e0a\u8fdb\u884c\u6279\u5904\u7406\u4ee5\u6700\u5927\u5316\u786c\u4ef6\u5229\u7528\u7387\uff0c\u6b64\u5916\u8fd8\u63d0\u51fa\u4e86\u8de8\u8bf7\u6c42\u72b6\u6001\u6253\u5305\u524d\u5411\u7b97\u5b50\u548c\u7edf\u4e00KV\u73af\u5f62\u7f13\u51b2\u533a\uff0c\u5c06\u788e\u7247\u5316\u7684\u5185\u5b58\u64cd\u4f5c\u878d\u5408\u4e3a\u9ad8\u6548\u7684\u5bc6\u96c6\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cActionFlow\u5728OpenVLA-7B\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e862.55\u500d\u7684FPS\u63d0\u5347\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u8fd9\u4f7f\u5f97\u5728\u8fb9\u7f18\u786c\u4ef6\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u52a8\u6001\u64cd\u4f5c\u6210\u4e3a\u53ef\u80fd\uff0c\u8be5\u6846\u67b6\u663e\u8457\u6539\u5584\u4e86VLA\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u63a8\u7406\u6548\u7387\u3002", "conclusion": "ActionFlow\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\u6210\u529f\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5ef6\u8fdf\u74f6\u9888\uff0c\u5176\u8de8\u8bf7\u6c42\u8c03\u5ea6\u548c\u5185\u5b58\u4f18\u5316\u6280\u672f\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u5c55\u793a\u4e86\u7cfb\u7edf\u7ea7\u4f18\u5316\u5728\u63d0\u5347AI\u6a21\u578b\u5b9e\u9645\u90e8\u7f72\u6548\u7387\u65b9\u9762\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2512.20025", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20025", "abs": "https://arxiv.org/abs/2512.20025", "authors": ["Anthony Dontoh", "Stephanie Ivey", "Armstrong Aboah"], "title": "A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments", "comment": null, "summary": "Despite increasing interest in computer vision-based distracted driving detection, most existing models rely exclusively on driver-facing views and overlook crucial environmental context that influences driving behavior. This study investigates whether incorporating road-facing views alongside driver-facing footage improves distraction detection accuracy in naturalistic driving conditions. Using synchronized dual-camera recordings from real-world driving, we benchmark three leading spatiotemporal action recognition architectures: SlowFast-R50, X3D-M, and SlowOnly-R50. Each model is evaluated under two input configurations: driver-only and stacked dual-view. Results show that while contextual inputs can improve detection in certain models, performance gains depend strongly on the underlying architecture. The single-pathway SlowOnly model achieved a 9.8 percent improvement with dual-view inputs, while the dual-pathway SlowFast model experienced a 7.2 percent drop in accuracy due to representational conflicts. These findings suggest that simply adding visual context is not sufficient and may lead to interference unless the architecture is specifically designed to support multi-view integration. This study presents one of the first systematic comparisons of single- and dual-view distraction detection models using naturalistic driving data and underscores the importance of fusion-aware design for future multimodal driver monitoring systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u5206\u5fc3\u9a7e\u9a76\u68c0\u6d4b\u4e2d\u7ed3\u5408\u9a7e\u9a76\u5458\u89c6\u89d2\u548c\u9053\u8def\u89c6\u89d2\u7684\u53cc\u89c6\u56fe\u65b9\u6cd5\uff0c\u53d1\u73b0\u6027\u80fd\u63d0\u5347\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\uff0c\u5176\u4e2d\u5355\u8def\u5f84SlowOnly\u6a21\u578b\u83b7\u5f979.8%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u800c\u53cc\u8def\u5f84SlowFast\u6a21\u578b\u56e0\u8868\u5f81\u51b2\u7a81\u5bfc\u81f47.2%\u7684\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5206\u5fc3\u9a7e\u9a76\u68c0\u6d4b\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u9a7e\u9a76\u5458\u89c6\u89d2\uff0c\u5ffd\u7565\u4e86\u5f71\u54cd\u9a7e\u9a76\u884c\u4e3a\u7684\u5173\u952e\u73af\u5883\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7ed3\u5408\u9053\u8def\u89c6\u89d2\u4e0e\u9a7e\u9a76\u5458\u89c6\u89d2\u7684\u53cc\u89c6\u56fe\u8f93\u5165\u662f\u5426\u80fd\u63d0\u5347\u81ea\u7136\u9a7e\u9a76\u6761\u4ef6\u4e0b\u7684\u5206\u5fc3\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u771f\u5b9e\u9a7e\u9a76\u73af\u5883\u4e2d\u540c\u6b65\u7684\u53cc\u6444\u50cf\u5934\u8bb0\u5f55\u6570\u636e\uff0c\u5bf9\u4e09\u79cd\u9886\u5148\u7684\u65f6\u7a7a\u52a8\u4f5c\u8bc6\u522b\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff1aSlowFast-R50\u3001X3D-M\u548cSlowOnly-R50\uff0c\u6bcf\u79cd\u6a21\u578b\u5728\u4e24\u79cd\u8f93\u5165\u914d\u7f6e\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff1a\u4ec5\u9a7e\u9a76\u5458\u89c6\u89d2\u548c\u5806\u53e0\u7684\u53cc\u89c6\u56fe\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0a\u4e0b\u6587\u8f93\u5165\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u5f71\u54cd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5e95\u5c42\u67b6\u6784\uff0c\u5355\u8def\u5f84SlowOnly\u6a21\u578b\u5728\u53cc\u89c6\u56fe\u8f93\u5165\u4e0b\u83b7\u5f979.8%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u800c\u53cc\u8def\u5f84SlowFast\u6a21\u578b\u56e0\u8868\u5f81\u51b2\u7a81\u5bfc\u81f47.2%\u7684\u51c6\u786e\u7387\u4e0b\u964d\uff0cX3D-M\u6a21\u578b\u7684\u8868\u73b0\u5219\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7b80\u5355\u5730\u6dfb\u52a0\u89c6\u89c9\u4e0a\u4e0b\u6587\u5e76\u4e0d\u8db3\u4ee5\u63d0\u5347\u5206\u5fc3\u9a7e\u9a76\u68c0\u6d4b\u6027\u80fd\uff0c\u53cd\u800c\u53ef\u80fd\u5bfc\u81f4\u5e72\u6270\uff0c\u9664\u975e\u67b6\u6784\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u652f\u6301\u591a\u89c6\u56fe\u96c6\u6210\uff0c\u8fd9\u5f3a\u8c03\u4e86\u878d\u5408\u611f\u77e5\u8bbe\u8ba1\u5bf9\u672a\u6765\u591a\u6a21\u6001\u9a7e\u9a76\u5458\u76d1\u63a7\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u5355\u89c6\u56fe\u4e0e\u53cc\u89c6\u56fe\u68c0\u6d4b\u6a21\u578b\u7684\u7cfb\u7edf\u6bd4\u8f83\u63d0\u4f9b\u4e86\u9996\u6279\u5b9e\u8bc1\u8bc1\u636e\u3002"}}
{"id": "2512.20174", "categories": ["cs.CV", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.20174", "abs": "https://arxiv.org/abs/2512.20174", "authors": ["Hao Guo", "Xugong Qin", "Jun Jie Ou Yang", "Peng Zhang", "Gangyan Zeng", "Yubo Li", "Hailun Lin"], "title": "Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark", "comment": "CVPR 2025", "summary": "Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u6587\u6863\u56fe\u50cf\u68c0\u7d22\uff08NL-DIR\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u4e30\u5bcc\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u6765\u89e3\u51b3\u73b0\u6709\u6587\u6863\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u8bed\u4e49\u68c0\u7d22\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5305\u542b41K\u771f\u5b9e\u6587\u6863\u56fe\u50cf\u548c\u9ad8\u8d28\u91cf\u67e5\u8be2\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u6587\u6863\u56fe\u50cf\u68c0\u7d22\uff08DIR\uff09\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u56fe\u50cf\u67e5\u8be2\uff0c\u53ea\u80fd\u68c0\u7d22\u76f8\u540c\u7c97\u7c92\u5ea6\u8bed\u4e49\u7c7b\u522b\uff08\u5982\u62a5\u7eb8\u6216\u6536\u636e\uff09\u7684\u6587\u6863\uff0c\u4f46\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u901a\u5e38\u63d0\u4f9b\u5177\u6709\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7684\u6587\u672c\u67e5\u8be2\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u68c0\u7d22\u3002\u4e3a\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u9700\u8981\u5efa\u7acb\u80fd\u591f\u5904\u7406\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u67e5\u8be2\u7684\u6587\u6863\u56fe\u50cf\u68c0\u7d22\u57fa\u51c6\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u81ea\u7136\u8bed\u8a00\u6587\u6863\u56fe\u50cf\u68c0\u7d22\uff08NL-DIR\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b41K\u771f\u5b9e\u6587\u6863\u56fe\u50cf\uff0c\u6bcf\u4e2a\u56fe\u50cf\u914d\u6709\u4e94\u4e2a\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e76\u7ecf\u4eba\u5de5\u9a8c\u8bc1\u7684\u9ad8\u8d28\u91cf\u7ec6\u7c92\u5ea6\u8bed\u4e49\u67e5\u8be2\u3002\u8bc4\u4f30\u4e86\u73b0\u6709\u4e3b\u6d41\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u65e0\u9700OCR\u7684\u89c6\u89c9\u6587\u6863\u7406\u89e3\u6a21\u578b\u7684\u96f6\u6837\u672c\u548c\u5fae\u8c03\u6027\u80fd\uff0c\u5e76\u8fdb\u4e00\u6b65\u7814\u7a76\u4e86\u4e24\u9636\u6bb5\u68c0\u7d22\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u65f6\u95f4\u548c\u7a7a\u95f4\u6548\u7387\u3002", "result": "NL-DIR\u6570\u636e\u96c6\u5305\u542b41K\u771f\u5b9e\u6587\u6863\u56fe\u50cf\uff0c\u6bcf\u4e2a\u56fe\u50cf\u914d\u6709\u4e94\u4e2a\u9ad8\u8d28\u91cf\u7ec6\u7c92\u5ea6\u8bed\u4e49\u67e5\u8be2\u3002\u7814\u7a76\u5bf9\u73b0\u6709\u4e3b\u6d41\u6a21\u578b\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bc4\u4f30\uff0c\u5e76\u5c55\u793a\u4e86\u4e24\u9636\u6bb5\u68c0\u7d22\u65b9\u6cd5\u5728\u6027\u80fd\u63d0\u5347\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u65f6\u95f4\u548c\u7a7a\u95f4\u6548\u7387\u7684\u5e73\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684NL-DIR\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u89c6\u89c9\u6587\u6863\u7406\u89e3\u793e\u533a\u5e26\u6765\u4e86\u65b0\u7684\u7814\u7a76\u673a\u4f1a\uff0c\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u4e30\u5bcc\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u6863\u56fe\u50cf\u68c0\u7d22\u5728\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5339\u914d\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u8be5\u57fa\u51c6\u5c06\u4fc3\u8fdb\u6587\u6863\u56fe\u50cf\u68c0\u7d22\u9886\u57df\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5904\u7406\u590d\u6742\u8bed\u4e49\u67e5\u8be2\u7684\u80fd\u529b\u3002"}}
{"id": "2512.20344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20344", "abs": "https://arxiv.org/abs/2512.20344", "authors": ["Yaowei Bai", "Ruiheng Zhang", "Yu Lei", "Xuhua Duan", "Jingfeng Yao", "Shuguang Ju", "Chaoyang Wang", "Wei Yao", "Yiwan Guo", "Guilin Zhang", "Chao Wan", "Qian Yuan", "Lei Chen", "Wenjuan Tang", "Biqiang Zhu", "Xinggang Wang", "Tao Sun", "Wei Zhou", "Dacheng Tao", "Yongchao Xu", "Chuansheng Zheng", "Huangxuan Zhao", "Bo Du"], "title": "A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.19493", "summary": "A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86Janus-Pro-CXR\uff081B\uff09\u80f8\u90e8X\u5149\u89e3\u8bfb\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u4e2d\u5fc3\u524d\u77bb\u6027\u4e34\u5e8a\u8bd5\u9a8c\u9a8c\u8bc1\u5176\u5728\u62a5\u544a\u751f\u6210\u548c\u5173\u952e\u653e\u5c04\u5b66\u53d1\u73b0\u68c0\u6d4b\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u8bca\u65ad\u53ef\u9760\u6027\u548c\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u3002", "motivation": "\u5168\u7403\u653e\u5c04\u79d1\u533b\u751f\u77ed\u7f3a\u95ee\u9898\u56e0\u80f8\u90e8X\u5149\u5de5\u4f5c\u91cf\u5de8\u5927\u800c\u52a0\u5267\uff0c\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u81ea\u52a8\u5316\u6307\u6807\u6216\u56de\u987e\u6027\u5206\u6790\uff0c\u7f3a\u4e4f\u4e25\u683c\u7684\u524d\u77bb\u6027\u4e34\u5e8a\u9a8c\u8bc1\uff0c\u9700\u8981\u5f00\u53d1\u7ecf\u8fc7\u4e34\u5e8a\u9a8c\u8bc1\u7684AI\u8f85\u52a9\u653e\u5c04\u5b66\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eDeepSeek Janus-Pro\u6a21\u578b\u5f00\u53d1\u4e86Janus-Pro-CXR\uff081B\uff09\u80f8\u90e8X\u5149\u89e3\u8bfb\u7cfb\u7edf\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u67b6\u6784\u548c\u9886\u57df\u7279\u5b9a\u4f18\u5316\uff0c\u901a\u8fc7\u591a\u4e2d\u5fc3\u524d\u77bb\u6027\u4e34\u5e8a\u8bd5\u9a8c\uff08NCT07117266\uff09\u8fdb\u884c\u4e25\u683c\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u5305\u62ecChatGPT 4o\uff08200B\u53c2\u6570\uff09\u5728\u5185\u7684\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "Janus-Pro-CXR\u5728\u81ea\u52a8\u62a5\u544a\u751f\u6210\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5305\u62ec\u66f4\u5927\u89c4\u6a21\u7684ChatGPT 4o\uff0c\u53ef\u9760\u68c0\u6d4b\u516d\u79cd\u4e34\u5e8a\u5173\u952e\u653e\u5c04\u5b66\u53d1\u73b0\uff1b\u524d\u77bb\u6027\u4e34\u5e8a\u90e8\u7f72\u4e2d\uff0cAI\u8f85\u52a9\u663e\u8457\u63d0\u9ad8\u62a5\u544a\u8d28\u91cf\u8bc4\u5206\uff0c\u51cf\u5c1118.3%\u89e3\u8bfb\u65f6\u95f4\uff08P<0.001\uff09\uff0c54.3%\u75c5\u4f8b\u4e2d\u4e13\u5bb6\u66f4\u504f\u597dAI\u8f85\u52a9\u7ed3\u679c\u3002", "conclusion": "Janus-Pro-CXR\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67b6\u6784\u548c\u9886\u57df\u4f18\u5316\u663e\u8457\u63d0\u5347\u8bca\u65ad\u53ef\u9760\u6027\u548c\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff1b\u6a21\u578b\u67b6\u6784\u548c\u5b9e\u65bd\u6846\u67b6\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdbAI\u8f85\u52a9\u653e\u5c04\u5b66\u89e3\u51b3\u65b9\u6848\u7684\u4e34\u5e8a\u8f6c\u5316\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u8303\u4f8b\u3002"}}
{"id": "2512.20026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20026", "abs": "https://arxiv.org/abs/2512.20026", "authors": ["Ziwei Qin", "Xuhui Song", "Deqing Huang", "Na Qin", "Jun Li"], "title": "MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis", "comment": "Accepted by Proceedings of the AAAI Conference on Artificial Intelligence 40 (AAAI-26)", "summary": "Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u591a\u6fc0\u6d3b\u5e73\u9762\u4ea4\u4e92\u56fe\u795e\u7ecf\u7f51\u7edc\uff08MAPI-GNN\uff09\uff0c\u901a\u8fc7\u4ece\u8bed\u4e49\u89e3\u7f20\u7684\u7279\u5f81\u5b50\u7a7a\u95f4\u5b66\u4e60\u591a\u9762\u56fe\u914d\u7f6e\u6587\u4ef6\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u5355\u9759\u6001\u56fe\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u533b\u7597\u8bca\u65ad\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u6a21\u6001\u533b\u7597\u8bca\u65ad\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4ece\u975e\u533a\u5206\u6027\u7279\u5f81\u6784\u5efa\u7684\u5355\u4e00\u9759\u6001\u56fe\uff0c\u8fd9\u79cd\u8303\u5f0f\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u60a3\u8005\u7279\u5f02\u6027\u75c5\u7406\u5173\u7cfb\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u5bfc\u81f4\u8bca\u65ad\u6548\u679c\u53d7\u9650\u3002", "method": "MAPI-GNN\u6846\u67b6\u9996\u5148\u901a\u8fc7\u591a\u7ef4\u5224\u522b\u5668\u63ed\u793a\u6f5c\u5728\u7684\u56fe\u611f\u77e5\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u6307\u5bfc\u52a8\u6001\u6784\u5efa\u4e00\u7cfb\u5217\u6fc0\u6d3b\u56fe\uff0c\u6700\u7ec8\u901a\u8fc7\u5173\u7cfb\u878d\u5408\u5f15\u64ce\u805a\u5408\u548c\u60c5\u5883\u5316\u8fd9\u4e9b\u591a\u9762\u56fe\u914d\u7f6e\u6587\u4ef6\u4ee5\u5b9e\u73b0\u7a33\u5065\u8bca\u65ad\u3002", "result": "\u5728\u4e24\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6db5\u76d6\u8d85\u8fc71300\u4e2a\u60a3\u8005\u6837\u672c\uff0c\u8868\u660eMAPI-GNN\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u591a\u9762\u56fe\u5efa\u6a21\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u8bed\u4e49\u89e3\u7f20\u7279\u5f81\u5b50\u7a7a\u95f4\u6784\u5efa\u591a\u9762\u56fe\u914d\u7f6e\u6587\u4ef6\u80fd\u591f\u66f4\u6709\u6548\u5730\u6355\u6349\u60a3\u8005\u7279\u5f02\u6027\u75c5\u7406\u5173\u7cfb\uff0c\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u52a8\u6001\u56fe\u6784\u5efa\u8303\u5f0f\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2512.20387", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20387", "abs": "https://arxiv.org/abs/2512.20387", "authors": ["YuChe Hsu", "AnJui Wang", "TsaiChing Ni", "YuanFu Yang"], "title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems", "comment": "10 pages, 9 figures", "summary": "We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u8bed\u8a00\u4eff\u771f\u6a21\u578b\uff08VLSM\uff09\uff0c\u901a\u8fc7\u7edf\u4e00\u89c6\u89c9\u548c\u6587\u672c\u7406\u89e3\u4ece\u5e03\u5c40\u8349\u56fe\u548c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u751f\u6210\u53ef\u6267\u884c\u7684FlexScript\uff0c\u4e3a\u5de5\u4e1a\u4eff\u771f\u7cfb\u7edf\u5efa\u7acb\u4e86\u751f\u6210\u5f0f\u6570\u5b57\u5b6a\u751f\u7684\u65b0\u8303\u5f0f\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5de5\u4e1a\u4eff\u771f\u7cfb\u7edf\u4e2d\u8de8\u6a21\u6001\u63a8\u7406\u7684\u6311\u6218\uff0c\u5373\u5982\u4f55\u5c06\u89c6\u89c9\u5e03\u5c40\u8349\u56fe\u4e0e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u76f8\u7ed3\u5408\uff0c\u751f\u6210\u53ef\u76f4\u63a5\u6267\u884c\u7684\u4eff\u771f\u903b\u8f91\u4ee3\u7801\uff0c\u4ece\u800c\u586b\u8865\u751f\u6210\u5f0f\u6570\u5b57\u5b6a\u751f\u9886\u57df\u7f3a\u4e4f\u7edf\u4e00\u89c6\u89c9-\u8bed\u8a00-\u4ee3\u7801\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u89c6\u89c9\u8bed\u8a00\u4eff\u771f\u6a21\u578b\uff08VLSM\uff09\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u67b6\u6784\u6574\u5408\u89c6\u89c9\u7f16\u7801\u5668\u3001\u8fde\u63a5\u5668\u548c\u4ee3\u7801\u9884\u8bad\u7ec3\u8bed\u8a00\u9aa8\u5e72\u7f51\u7edc\uff0c\u80fd\u591f\u4ece\u5e03\u5c40\u8349\u56fe\u548c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u751f\u6210\u53ef\u6267\u884c\u7684FlexScript\u4ee3\u7801\uff1b\u540c\u65f6\u6784\u5efa\u4e86\u5305\u542b\u8d85\u8fc712\u4e07\u6761\u63d0\u793a-\u8349\u56fe-\u4ee3\u7801\u4e09\u5143\u7ec4\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u4e13\u95e8\u8bbe\u8ba1\u4e86\u7ed3\u6784\u6709\u6548\u6027\u7387\uff08SVR\uff09\u3001\u53c2\u6570\u5339\u914d\u7387\uff08PMR\uff09\u548c\u6267\u884c\u6210\u529f\u7387\uff08ESR\uff09\u4e09\u4e2a\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u6d88\u878d\u5b9e\u9a8c\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u89c6\u89c9\u7f16\u7801\u5668\u3001\u8fde\u63a5\u5668\u548c\u4ee3\u7801\u9884\u8bad\u7ec3\u8bed\u8a00\u9aa8\u5e72\u7f51\u7edc\u7684\u4e0d\u540c\u914d\u7f6e\u4e0b\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u7ed3\u6784\u51c6\u786e\u6027\u548c\u9ad8\u5ea6\u7684\u6267\u884c\u9c81\u68d2\u6027\uff1b\u4e09\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6307\u6807\u5168\u9762\u9a8c\u8bc1\u4e86\u751f\u6210\u4ee3\u7801\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u3001\u53c2\u6570\u4fdd\u771f\u5ea6\u548c\u4eff\u771f\u5668\u53ef\u6267\u884c\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u751f\u6210\u5f0f\u6570\u5b57\u5b6a\u751f\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5c06\u89c6\u89c9\u63a8\u7406\u548c\u8bed\u8a00\u7406\u89e3\u6574\u5408\u5230\u53ef\u6267\u884c\u5de5\u4e1a\u4eff\u771f\u7cfb\u7edf\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u5de5\u4e1a\u81ea\u52a8\u5316\u3001\u667a\u80fd\u5236\u9020\u548c\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u7684\u667a\u80fd\u5316\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u548c\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.20029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20029", "abs": "https://arxiv.org/abs/2512.20029", "authors": ["Lin Li", "Jiahui Li", "Jiaming Lei", "Jun Xiao", "Feifei Shao", "Long Chen"], "title": "$\\text{H}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning", "comment": null, "summary": "Compositional zero-shot learning (CZSL) aims to recognize unseen state-object compositions by generalizing from a training set of their primitives (state and object). Current methods often overlook the rich hierarchical structures, such as the semantic hierarchy of primitives (e.g., apple fruit) and the conceptual hierarchy between primitives and compositions (e.g, sliced apple apple). A few recent efforts have shown effectiveness in modeling these hierarchies through loss regularization within Euclidean space. In this paper, we argue that they fail to scale to the large-scale taxonomies required for real-world CZSL: the space's polynomial volume growth in flat geometry cannot match the exponential structure, impairing generalization capacity. To this end, we propose H2em, a new framework that learns Hierarchical Hyperbolic EMbeddings for CZSL. H2em leverages the unique properties of hyperbolic geometry, a space naturally suited for embedding tree-like structures with low distortion. However, a naive hyperbolic mapping may suffer from hierarchical collapse and poor fine-grained discrimination. We further design two learning objectives to structure this space: a Dual-Hierarchical Entailment Loss that uses hyperbolic entailment cones to enforce the predefined hierarchies, and a Discriminative Alignment Loss with hard negative mining to establish a large geodesic distance between semantically similar compositions. Furthermore, we devise Hyperbolic Cross-Modal Attention to realize instance-aware cross-modal infusion within hyperbolic geometry. Extensive ablations on three benchmarks demonstrate that H2em establishes a new state-of-the-art in both closed-world and open-world scenarios. Our codes will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faH2em\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u66f2\u51e0\u4f55\u5d4c\u5165\u89e3\u51b3\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\u5efa\u6a21\u95ee\u9898\uff0c\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u7684\u6307\u6570\u4f53\u79ef\u589e\u957f\u7279\u6027\u5339\u914d\u7ec4\u5408\u8bed\u4e49\u7684\u6307\u6570\u7ed3\u6784\uff0c\u5728\u5c01\u95ed\u4e16\u754c\u548c\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e30\u5bcc\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5982\u57fa\u5143\u7684\u8bed\u4e49\u5c42\u6b21\u548c\u57fa\u5143\u4e0e\u7ec4\u5408\u4e4b\u95f4\u7684\u6982\u5ff5\u5c42\u6b21\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u901a\u8fc7\u635f\u5931\u6b63\u5219\u5316\u5efa\u6a21\u8fd9\u4e9b\u5c42\u6b21\uff0c\u4f46\u65e0\u6cd5\u6269\u5c55\u5230\u73b0\u5b9e\u4e16\u754cCZSL\u6240\u9700\u7684\u5927\u89c4\u6a21\u5206\u7c7b\u4f53\u7cfb\uff0c\u56e0\u4e3a\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7684\u591a\u9879\u5f0f\u4f53\u79ef\u589e\u957f\u65e0\u6cd5\u5339\u914d\u7ec4\u5408\u8bed\u4e49\u7684\u6307\u6570\u7ed3\u6784\uff0c\u4ece\u800c\u635f\u5bb3\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faH2em\u6846\u67b6\uff0c\u5229\u7528\u53cc\u66f2\u51e0\u4f55\u7684\u81ea\u7136\u7279\u6027\u5d4c\u5165\u6811\u72b6\u7ed3\u6784\u3002\u8bbe\u8ba1\u53cc\u91cd\u5c42\u6b21\u8574\u542b\u635f\u5931\uff0c\u4f7f\u7528\u53cc\u66f2\u8574\u542b\u9525\u5f3a\u5236\u6267\u884c\u9884\u5b9a\u4e49\u7684\u5c42\u6b21\u7ed3\u6784\uff1b\u8bbe\u8ba1\u5224\u522b\u5bf9\u9f50\u635f\u5931\u4e0e\u56f0\u96be\u8d1f\u6837\u672c\u6316\u6398\uff0c\u5728\u8bed\u4e49\u76f8\u4f3c\u7684\u7ec4\u5408\u4e4b\u95f4\u5efa\u7acb\u8f83\u5927\u7684\u6d4b\u5730\u8ddd\u79bb\uff1b\u5f00\u53d1\u53cc\u66f2\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u53cc\u66f2\u51e0\u4f55\u5185\u5b9e\u73b0\u5b9e\u4f8b\u611f\u77e5\u7684\u8de8\u6a21\u6001\u878d\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0cH2em\u5728\u5c01\u95ed\u4e16\u754c\u548c\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u5747\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5c42\u6b21\u5d29\u6e83\u548c\u7ec6\u7c92\u5ea6\u533a\u5206\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u53cc\u66f2\u51e0\u4f55\u4e3a\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u7684\u5c42\u6b21\u7ed3\u6784\u5efa\u6a21\u63d0\u4f9b\u4e86\u81ea\u7136\u4e14\u6709\u6548\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5176\u6307\u6570\u4f53\u79ef\u589e\u957f\u7279\u6027\u80fd\u591f\u5339\u914d\u7ec4\u5408\u8bed\u4e49\u7684\u6307\u6570\u7ed3\u6784\u3002\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u5206\u7c7b\u4f53\u7cfb\u4e0b\u7684\u7ec4\u5408\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u8bc1\u660e\u4e86\u53cc\u66f2\u7a7a\u95f4\u5728\u590d\u6742\u8bed\u4e49\u5173\u7cfb\u5efa\u6a21\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2512.20548", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20548", "abs": "https://arxiv.org/abs/2512.20548", "authors": ["Zhiyi Duan", "Xiangren Wang", "Hongyu Yuan", "Qianli Xing"], "title": "Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model", "comment": null, "summary": "Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6559\u5e08\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u6570\u636e\u96c6T-MED\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u5bf9\u79f0\u6ce8\u610f\u529b\u7684\u591a\u6a21\u6001\u6559\u5e08\u60c5\u611f\u5206\u6790\u6a21\u578bAAM-TSA\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6559\u5e08\u60c5\u611f\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u6559\u5e08\u60c5\u611f\uff0c\u8fd9\u65e2\u56e0\u4e3a\u6559\u5e08\u60c5\u611f\u5177\u6709\u8868\u6f14\u6027\u7279\u5f81\uff0c\u4e5f\u56e0\u4e3a\u5ffd\u89c6\u4e86\u6559\u5b66\u4fe1\u606f\u5bf9\u60c5\u611f\u8868\u8fbe\u7684\u5173\u952e\u5f71\u54cd\uff0c\u5bfc\u81f4\u5728\u771f\u5b9e\u6559\u80b2\u573a\u666f\u4e2d\u6559\u5e08\u60c5\u611f\u5206\u6790\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7814\u7a76\u9996\u5148\u6784\u5efa\u4e86\u5305\u542b14,938\u4e2a\u5b9e\u4f8b\u7684\u6559\u5e08\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u6570\u636e\u96c6T-MED\uff0c\u6db5\u76d611\u4e2a\u5b66\u79d1\u4eceK-12\u5230\u9ad8\u7b49\u6559\u80b2\u7684250\u4e2a\u771f\u5b9e\u8bfe\u5802\uff0c\u6574\u5408\u4e86\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u9891\u548c\u6559\u5b66\u4fe1\u606f\u7b49\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u91c7\u7528\u4eba\u673a\u534f\u540c\u6807\u6ce8\u6d41\u7a0b\u786e\u4fdd\u6807\u6ce8\u8d28\u91cf\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u57fa\u4e8e\u975e\u5bf9\u79f0\u6ce8\u610f\u529b\u7684\u591a\u6a21\u6001\u6559\u5e08\u60c5\u611f\u5206\u6790\u6a21\u578bAAM-TSA\uff0c\u8be5\u6a21\u578b\u5f15\u5165\u4e86\u975e\u5bf9\u79f0\u6ce8\u610f\u529b\u673a\u5236\u548c\u5206\u5c42\u95e8\u63a7\u5355\u5143\uff0c\u5b9e\u73b0\u4e86\u5dee\u5f02\u5316\u7684\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u548c\u7cbe\u786e\u7684\u60c5\u611f\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAAM-TSA\u6a21\u578b\u5728T-MED\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u6559\u5e08\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u63d0\u51fa\u521b\u65b0\u6a21\u578b\uff0c\u4e3a\u6559\u5e08\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86\u6559\u5b66\u4fe1\u606f\u5728\u60c5\u611f\u5206\u6790\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u6559\u80b2\u60c5\u611f\u8ba1\u7b97\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u7ec6\u7c92\u5ea6\u7684\u60c5\u611f\u5206\u6790\u548c\u4e2a\u6027\u5316\u6559\u5b66\u652f\u6301\u5e94\u7528\u3002"}}
{"id": "2512.20042", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20042", "abs": "https://arxiv.org/abs/2512.20042", "authors": ["Nguyen Lam Phu Quy", "Pham Phu Hoa", "Tran Chi Nguyen", "Dao Sy Duy Minh", "Nguyen Hoang Minh Ngoc", "Huynh Trung Kiet"], "title": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva", "comment": "7 pages, 5 figures. System description for the EVENTA Grand Challenge (Track 1) at ACM MM'25", "summary": "Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u7ba1\u9053\uff0c\u901a\u8fc7\u6574\u5408\u5916\u90e8\u6587\u672c\u77e5\u8bc6\u6765\u589e\u5f3a\u56fe\u50cf\u63cf\u8ff0\uff0c\u751f\u6210\u5305\u542b\u4e8b\u4ef6\u80cc\u666f\u3001\u65f6\u95f4\u7ebf\u7d22\u548c\u547d\u540d\u5b9e\u4f53\u7b49\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u63cf\u8ff0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f20\u7edf\u56fe\u50cf\u63cf\u8ff0\u65b9\u6cd5\u7684\u6df1\u5ea6\u548c\u4fe1\u606f\u91cf\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u56fe\u50cf\u63cf\u8ff0\u901a\u5e38\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u6df1\u5ea6\uff0c\u5ffd\u7565\u4e86\u4e8b\u4ef6\u80cc\u666f\u3001\u65f6\u95f4\u7ebf\u7d22\u3001\u7ed3\u679c\u548c\u4e0d\u53ef\u89c6\u7684\u547d\u540d\u5b9e\u4f53\u7b49\u5173\u952e\u7ec6\u8282\uff0c\u8fd9\u9650\u5236\u4e86\u56fe\u50cf\u7406\u89e3\u5728\u65b0\u95fb\u3001\u6559\u80b2\u548c\u6570\u5b57\u6863\u6848\u7b49\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u8fd9\u4e9b\u9886\u57df\u9700\u8981\u66f4\u4e30\u5bcc\u3001\u4fe1\u606f\u91cf\u66f4\u5927\u7684\u63cf\u8ff0\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u591a\u6a21\u6001\u7ba1\u9053\uff0c\u4f7f\u7528BEIT-3\uff08Flickr30k-384\u548cCOCO-384\uff09\u548cSigLIP So-384\u68c0\u7d22\u8bed\u4e49\u76f8\u4f3c\u56fe\u50cf\uff0c\u901a\u8fc7ORB\u548cSIFT\u8fdb\u884c\u51e0\u4f55\u5bf9\u9f50\u91cd\u6392\u5e8f\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u641c\u7d22\u4ece\u76f8\u5173\u6587\u7ae0\u4e2d\u63d0\u53d6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u7136\u540e\u4f7f\u7528QLoRA\u5fae\u8c03\u7684Qwen3\u6a21\u578b\u5c06\u4e0a\u4e0b\u6587\u4e0eInstruct BLIP\uff08Vicuna-7B\uff09\u751f\u6210\u7684\u57fa\u7840\u63cf\u8ff0\u6574\u5408\uff0c\u751f\u6210\u4e8b\u4ef6\u4e30\u5bcc\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63cf\u8ff0\u3002", "result": "\u5728OpenEvents v1\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u63cf\u8ff0\u4fe1\u606f\u91cf\u663e\u8457\u66f4\u5927\uff0c\u663e\u793a\u51fa\u5728\u9700\u8981\u66f4\u6df1\u5c42\u6b21\u89c6\u89c9-\u6587\u672c\u7406\u89e3\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u5f3a\u5927\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6574\u5408\u5916\u90e8\u6587\u672c\u77e5\u8bc6\u5bf9\u4e8e\u589e\u5f3a\u56fe\u50cf\u63cf\u8ff0\u4e0a\u4e0b\u6587\u6df1\u5ea6\u7684\u6709\u6548\u6027\uff0c\u4e3a\u65b0\u95fb\u3001\u6559\u80b2\u548c\u6570\u5b57\u6863\u6848\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u77e5\u8bc6\u6574\u5408\u5728\u63d0\u5347\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u65b9\u9762\u7684\u4ef7\u503c\u3002"}}
{"id": "2512.20618", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.20618", "abs": "https://arxiv.org/abs/2512.20618", "authors": ["Runtao Liu", "Ziyi Liu", "Jiaqi Tang", "Yue Ma", "Renjie Pi", "Jipeng Zhang", "Qifeng Chen"], "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "comment": null, "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u957f\u89c6\u9891\u95ee\u7b54\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5176\u4e2d\u4e3b\u63a7LLM\u534f\u8c03\u5b9a\u4f4d\u667a\u80fd\u4f53\u548c\u89c6\u89c9\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001LLM\u548c\u957f\u89c6\u9891\u95ee\u7b54\u7cfb\u7edf\u5728\u5904\u7406\u5c0f\u65f6\u7ea7\u89c6\u9891\u65f6\uff0c\u901a\u5e38\u91c7\u7528\u6709\u635f\u538b\u7f29\u6458\u8981\u6216\u4f9d\u8d56\u6709\u9650\u5de5\u5177\u96c6\uff0c\u8fd9\u524a\u5f31\u4e86\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\u5e76\u9057\u6f0f\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ebf\u7d22\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u65f6\u5e8f\u5b9a\u4f4d\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5176\u4e2d\u4e3b\u63a7LLM\u534f\u8c03\u4e24\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff1a\u5b9a\u4f4d\u667a\u80fd\u4f53\u8d1f\u8d23\u5b9a\u4f4d\u95ee\u9898\u76f8\u5173\u89c6\u9891\u7247\u6bb5\uff0c\u89c6\u89c9\u667a\u80fd\u4f53\u8d1f\u8d23\u63d0\u53d6\u9488\u5bf9\u6027\u6587\u672c\u89c2\u5bdf\u3002\u4e3b\u63a7\u667a\u80fd\u4f53\u91c7\u7528\u6b65\u6570\u9650\u5236\u8fdb\u884c\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4ee5\u4fc3\u8fdb\u7b80\u6d01\u3001\u6b63\u786e\u4e14\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3002", "result": "\u5728\u63d0\u51fa\u7684LongTVQA\u548cLongTVQA+\u6570\u636e\u96c6\uff08\u57fa\u4e8eTVQA/TVQA+\u6784\u5efa\u7684\u5267\u96c6\u7ea7\u6570\u636e\u96c6\uff09\u4e0a\uff0c\u8be5\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u663e\u8457\u4f18\u4e8e\u5f3a\u975e\u667a\u80fd\u4f53\u57fa\u7ebf\u3002\u5b9e\u9a8c\u8868\u660e\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u8bad\u7ec3\u540e\u667a\u80fd\u4f53\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u4e13\u95e8\u667a\u80fd\u4f53\u7684\u5206\u5de5\u534f\u4f5c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u65f6\u5e8f\u5b9a\u4f4d\u548c\u89c6\u89c9\u7ec6\u8282\u8865\u5145\uff0c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u6548\u7387\uff0c\u4e3a\u957f\u89c6\u9891\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9\u3002"}}
{"id": "2512.20088", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20088", "abs": "https://arxiv.org/abs/2512.20088", "authors": ["Jinyoung Choi", "Youngchae Kwon", "Injung Kim"], "title": "Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts", "comment": "This is a pre-print of an article published in Applied Intelligence. The final authenticated version is available online at: https://doi.org/10.1007/s10489-024-05683-9", "summary": "Fashion style classification is a challenging task because of the large visual variation within the same style and the existence of visually similar styles.\n  Styles are expressed not only by the global appearance, but also by the attributes of individual items and their combinations.\n  In this study, we propose an item region-based fashion style classification network (IRSN) to effectively classify fashion styles by analyzing item-specific features and their combinations in addition to global features.\n  IRSN extracts features of each item region using item region pooling (IRP), analyzes them separately, and combines them using gated feature fusion (GFF).\n  In addition, we improve the feature extractor by applying a dual-backbone architecture that combines a domain-specific feature extractor and a general feature extractor pre-trained with a large-scale image-text dataset.\n  In experiments, applying IRSN to six widely-used backbones, including EfficientNet, ConvNeXt, and Swin Transformer, improved style classification accuracy by an average of 6.9% and a maximum of 14.5% on the FashionStyle14 dataset and by an average of 7.6% and a maximum of 15.1% on the ShowniqV3 dataset. Visualization analysis also supports that the IRSN models are better than the baseline models at capturing differences between similar style classes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u54c1\u533a\u57df\u7684\u65f6\u5c1a\u98ce\u683c\u5206\u7c7b\u7f51\u7edc\uff08IRSN\uff09\uff0c\u901a\u8fc7\u5206\u6790\u7269\u54c1\u7279\u5b9a\u7279\u5f81\u53ca\u5176\u7ec4\u5408\u6765\u6539\u8fdb\u65f6\u5c1a\u98ce\u683c\u5206\u7c7b\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u7269\u54c1\u533a\u57df\u6c60\u5316\u3001\u95e8\u63a7\u7279\u5f81\u878d\u5408\u548c\u53cc\u9aa8\u5e72\u67b6\u6784\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u65f6\u5c1a\u98ce\u683c\u5206\u7c7b\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u540c\u4e00\u98ce\u683c\u5185\u5b58\u5728\u8f83\u5927\u7684\u89c6\u89c9\u5dee\u5f02\uff0c\u4ee5\u53ca\u4e0d\u540c\u98ce\u683c\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u89c6\u89c9\u76f8\u4f3c\u6027\u3002\u98ce\u683c\u4e0d\u4ec5\u7531\u6574\u4f53\u5916\u89c2\u8868\u8fbe\uff0c\u8fd8\u53d6\u51b3\u4e8e\u5355\u4e2a\u7269\u54c1\u7684\u5c5e\u6027\u53ca\u5176\u7ec4\u5408\u65b9\u5f0f\uff0c\u56e0\u6b64\u9700\u8981\u540c\u65f6\u8003\u8651\u5168\u5c40\u7279\u5f81\u548c\u7269\u54c1\u7ea7\u7279\u5f81\u3002", "method": "IRSN\u91c7\u7528\u7269\u54c1\u533a\u57df\u6c60\u5316\uff08IRP\uff09\u63d0\u53d6\u6bcf\u4e2a\u7269\u54c1\u533a\u57df\u7684\u7279\u5f81\uff0c\u5206\u522b\u8fdb\u884c\u5206\u6790\uff0c\u7136\u540e\u901a\u8fc7\u95e8\u63a7\u7279\u5f81\u878d\u5408\uff08GFF\uff09\u8fdb\u884c\u7ec4\u5408\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u53cc\u9aa8\u5e72\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u9886\u57df\u7279\u5b9a\u7279\u5f81\u63d0\u53d6\u5668\u548c\u5728\u5927\u89c4\u6a21\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u901a\u7528\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4ee5\u589e\u5f3a\u7279\u5f81\u8868\u793a\u80fd\u529b\u3002", "result": "\u5728FashionStyle14\u548cShowniqV3\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIRSN\u5e94\u7528\u4e8e\u5305\u62ecEfficientNet\u3001ConvNeXt\u548cSwin Transformer\u5728\u5185\u7684\u516d\u79cd\u9aa8\u5e72\u7f51\u7edc\u65f6\uff0c\u5e73\u5747\u5206\u522b\u63d0\u5347\u4e866.9%\u548c7.6%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6700\u5927\u63d0\u5347\u5206\u522b\u8fbe\u523014.5%\u548c15.1%\u3002\u53ef\u89c6\u5316\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u5b9eIRSN\u6a21\u578b\u80fd\u66f4\u597d\u5730\u6355\u6349\u76f8\u4f3c\u98ce\u683c\u7c7b\u522b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "conclusion": "IRSN\u901a\u8fc7\u540c\u65f6\u5efa\u6a21\u5168\u5c40\u7279\u5f81\u3001\u7269\u54c1\u7ea7\u7279\u5f81\u53ca\u5176\u7ec4\u5408\u5173\u7cfb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u5c1a\u98ce\u683c\u5206\u7c7b\u4e2d\u7684\u89c6\u89c9\u53d8\u5f02\u548c\u76f8\u4f3c\u6027\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u4e3a\u7ec6\u7c92\u5ea6\u65f6\u5c1a\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u6846\u67b6\uff0c\u5176\u53cc\u9aa8\u5e72\u67b6\u6784\u548c\u95e8\u63a7\u878d\u5408\u673a\u5236\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u9700\u8981\u591a\u5c42\u6b21\u7279\u5f81\u5efa\u6a21\u7684\u89c6\u89c9\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2512.20117", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.20117", "abs": "https://arxiv.org/abs/2512.20117", "authors": ["Jingqi Tian", "Yiheng Du", "Haoji Zhang", "Yuji Wang", "Isaac Ning Lee", "Xulong Bai", "Tianrui Zhu", "Jingxuan Niu", "Yansong Tang"], "title": "DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation", "comment": "https://trilarflagz.github.io/DDAVS-page/", "summary": "Audio-Visual Segmentation (AVS) aims to localize sound-producing objects at the pixel level by jointly leveraging auditory and visual information. However, existing methods often suffer from multi-source entanglement and audio-visual misalignment, which lead to biases toward louder or larger objects while overlooking weaker, smaller, or co-occurring sources. To address these challenges, we propose DDAVS, a Disentangled Audio Semantics and Delayed Bidirectional Alignment framework. To mitigate multi-source entanglement, DDAVS employs learnable queries to extract audio semantics and anchor them within a structured semantic space derived from an audio prototype memory bank. This is further optimized through contrastive learning to enhance discriminability and robustness. To alleviate audio-visual misalignment, DDAVS introduces dual cross-attention with delayed modality interaction, improving the robustness of multimodal alignment. Extensive experiments on the AVS-Objects and VPO benchmarks demonstrate that DDAVS consistently outperforms existing approaches, exhibiting strong performance across single-source, multi-source, and multi-instance scenarios. These results validate the effectiveness and generalization ability of our framework under challenging real-world audio-visual segmentation conditions. Project page: https://trilarflagz.github.io/DDAVS-page/", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDDAVS\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u97f3\u9891\u8bed\u4e49\u4e0e\u5ef6\u8fdf\u53cc\u5411\u5bf9\u9f50\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u97f3\u9891-\u89c6\u89c9\u5206\u5272\u4e2d\u7684\u591a\u6e90\u7ea0\u7f20\u548c\u89c6\u542c\u9519\u4f4d\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u97f3\u9891-\u89c6\u89c9\u5206\u5272\u4efb\u52a1\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u542c\u89c9\u548c\u89c6\u89c9\u4fe1\u606f\u5728\u50cf\u7d20\u7ea7\u522b\u5b9a\u4f4d\u53d1\u58f0\u7269\u4f53\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5e38\u53d7\u591a\u6e90\u7ea0\u7f20\u548c\u89c6\u542c\u9519\u4f4d\u95ee\u9898\u7684\u56f0\u6270\uff0c\u5bfc\u81f4\u6a21\u578b\u504f\u5411\u4e8e\u66f4\u54cd\u4eae\u6216\u66f4\u5927\u7684\u7269\u4f53\uff0c\u800c\u5ffd\u7565\u8f83\u5f31\u3001\u8f83\u5c0f\u6216\u5171\u73b0\u7684\u58f0\u6e90\u3002", "method": "DDAVS\u6846\u67b6\u91c7\u7528\u89e3\u8026\u97f3\u9891\u8bed\u4e49\u548c\u5ef6\u8fdf\u53cc\u5411\u5bf9\u9f50\u673a\u5236\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u67e5\u8be2\u4ece\u97f3\u9891\u539f\u578b\u8bb0\u5fc6\u5e93\u4e2d\u63d0\u53d6\u97f3\u9891\u8bed\u4e49\u5e76\u5c06\u5176\u951a\u5b9a\u5728\u7ed3\u6784\u5316\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u5224\u522b\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5f15\u5165\u5177\u6709\u5ef6\u8fdf\u6a21\u6001\u4ea4\u4e92\u7684\u53cc\u91cd\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6765\u6539\u5584\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728AVS-Objects\u548cVPO\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDDAVS\u5728\u5355\u6e90\u3001\u591a\u6e90\u548c\u591a\u5b9e\u4f8b\u573a\u666f\u4e2d\u5747\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u97f3\u9891-\u89c6\u89c9\u5206\u5272\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u89e3\u8026\u97f3\u9891\u8bed\u4e49\u548c\u5ef6\u8fdf\u53cc\u5411\u5bf9\u9f50\u673a\u5236\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u97f3\u9891-\u89c6\u89c9\u5206\u5272\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u5904\u7406\u590d\u6742\u591a\u6e90\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u591a\u6a21\u6001\u5206\u5272\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.20217", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20217", "abs": "https://arxiv.org/abs/2512.20217", "authors": ["Xiangxuan Ren", "Zhongdao Wang", "Pin Tang", "Guoqing Wang", "Jilai Zheng", "Chao Ma"], "title": "LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation", "comment": "13 pages, 9 figures, 8 tables", "summary": "3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLiteFusion\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u60013D\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u5c06LiDAR\u6570\u636e\u4f5c\u4e3a\u51e0\u4f55\u4fe1\u606f\u7684\u8865\u5145\u6e90\u6765\u589e\u5f3a\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u68c0\u6d4b\uff0c\u5b8c\u5168\u6d88\u9664\u4e86\u5bf93D\u4e3b\u5e72\u7f51\u7edc\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u90e8\u7f72\u53cb\u597d\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u60013D\u76ee\u6807\u68c0\u6d4b\u5668\u4e25\u91cd\u4f9d\u8d56LiDAR\u4f20\u611f\u5668\uff0c\u5728LiDAR\u7f3a\u5931\u65f6\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u4e14\u7531\u4e8e\u4f9d\u8d56\u4e3b\u8981\u9488\u5bf9NVIDIA GPU\u4f18\u5316\u76843D\u7a00\u758f\u5377\u79ef\u7b97\u5b50\uff0c\u96be\u4ee5\u90e8\u7f72\u5230NPU\u548cFPGA\u7b49\u591a\u6837\u5316\u786c\u4ef6\u5e73\u53f0\u4e0a\uff0c\u8fd9\u5f71\u54cd\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "LiteFusion\u91cd\u65b0\u601d\u8003\u4e86LiDAR\u5728\u6444\u50cf\u5934-LiDAR\u878d\u5408\u8303\u5f0f\u4e2d\u7684\u4f5c\u7528\uff0c\u5c06LiDAR\u6570\u636e\u4f5c\u4e3a\u51e0\u4f55\u4fe1\u606f\u7684\u8865\u5145\u6e90\u6765\u589e\u5f3a\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u68c0\u6d4b\uff0c\u800c\u975e\u5c06\u5176\u89c6\u4e3a\u5177\u6709\u72ec\u7acb\u7279\u5f81\u63d0\u53d6\u4e3b\u5e72\u7f51\u7edc\u7684\u72ec\u7acb\u6a21\u6001\u3002\u8be5\u65b9\u6cd5\u5728\u56db\u5143\u6570\u7a7a\u95f4\u4e2d\u96c6\u6210LiDAR\u70b9\u5230\u56fe\u50cf\u7279\u5f81\u4e2d\uff0c\u5176\u4e2d\u6b63\u4ea4\u7ea6\u675f\u5728\u7f51\u7edc\u8bad\u7ec3\u671f\u95f4\u5f97\u5230\u826f\u597d\u4fdd\u6301\uff0c\u6709\u52a9\u4e8e\u5efa\u6a21\u8de8\u6a21\u6001\u7684\u9886\u57df\u7279\u5b9a\u5173\u7cfb\uff0c\u4ea7\u751f\u7d27\u51d1\u7684\u8de8\u6a21\u6001\u5d4c\u5165\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLiteFusion\u5c06\u57fa\u4e8e\u89c6\u89c9\u7684\u57fa\u7ebf\u68c0\u6d4b\u5668\u7684mAP\u63d0\u9ad8\u4e86+20.4%\uff0cNDS\u63d0\u9ad8\u4e86+19.7%\uff0c\u800c\u53c2\u6570\u4ec5\u589e\u52a01.1%\uff0c\u4e14\u672a\u4f7f\u7528\u4e13\u7528\u7684LiDAR\u7f16\u7801\u5668\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5373\u4f7f\u5728LiDAR\u8f93\u5165\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\uff0cLiteFusion\u4ecd\u80fd\u4fdd\u6301\u5f3a\u52b2\u7ed3\u679c\uff0c\u7a81\u663e\u4e86\u5176\u5728\u4e0d\u540c\u878d\u5408\u8303\u5f0f\u548c\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u826f\u597d\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u91cd\u65b0\u601d\u8003LiDAR\u5728\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u90e8\u7f72\u53cb\u597d\u4e14\u9c81\u68d2\u76843D\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5b8c\u5168\u6d88\u9664\u4e86\u5bf93D\u4e3b\u5e72\u7f51\u7edc\u7684\u4f9d\u8d56\uff0c\u4e3a\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u5177\u6709\u66f4\u597d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2512.20257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20257", "abs": "https://arxiv.org/abs/2512.20257", "authors": ["Daniele Cardullo", "Simone Teglia", "Irene Amerini"], "title": "LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation", "comment": null, "summary": "With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LADLE-MM\uff0c\u4e00\u79cd\u5728\u6709\u9650\u6807\u6ce8\u548c\u8bad\u7ec3\u8d44\u6e90\u4e0b\u5de5\u4f5c\u7684\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u6a21\u578b\u96c6\u6210\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u5728\u51cf\u5c1160.3%\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u540c\u65f6\uff0c\u5728DGM4\u548cVERITE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u591a\u5a92\u4f53\u5185\u5bb9\u751f\u6210\u548c\u64cd\u7eb5\u5de5\u5177\u7684\u666e\u53ca\uff0c\u8de8\u591a\u6a21\u6001\u7684\u903c\u771f\u5408\u6210\u7be1\u6539\u5df2\u6210\u4e3a\u5e7f\u6cdb\u5a01\u80c1\uff0c\u5e38\u88ab\u7528\u4e8e\u626d\u66f2\u91cd\u8981\u4e8b\u4ef6\u53d9\u4e8b\u548c\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u578b\u67b6\u6784\u6216\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5e94\u7528\u3002", "method": "LADLE-MM\u91c7\u7528\u6a21\u578b\u96c6\u6210\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5355\u6a21\u6001\u5206\u652f\u548c\u4e00\u4e2a\u591a\u6a21\u6001\u5206\u652f\u3002\u591a\u6a21\u6001\u5206\u652f\u901a\u8fc7\u4eceBLIP\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u56fa\u5b9a\u591a\u6a21\u6001\u5d4c\u5165\u6765\u589e\u5f3a\u56fe\u50cf\u548c\u6587\u672c\u8868\u793a\uff0c\u8fd9\u4e9b\u5d4c\u5165\u4f5c\u4e3a\u53c2\u8003\u7a7a\u95f4\u3002\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u6807\u6ce8\u8bbe\u7f6e\u4e0b\u5de5\u4f5c\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u3002", "result": "\u5728DGM4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLADLE-MM\u5728\u4e8c\u8fdb\u5236\u548c\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5c3d\u7ba1\u53ef\u8bad\u7ec3\u53c2\u6570\u6bd4\u5148\u524d\u6700\u5148\u8fdb\u6a21\u578b\u51cf\u5c1160.3%\u3002\u5728\u6ca1\u6709\u57fa\u7840\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u65f6\uff0c\u5b83\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002\u5728VERITE\u6570\u636e\u96c6\u4e0a\uff0cLADLE-MM\u8d85\u8d8a\u4e86\u4f7f\u7528\u66f4\u590d\u6742\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u7684\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u5f00\u653e\u96c6\u8bbe\u7f6e\u4e2d\u7684\u6709\u6548\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u5355\u6a21\u6001\u504f\u89c1\u7684\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6a21\u578b\u96c6\u6210\u521d\u59cb\u5316\u548c\u591a\u6a21\u6001\u8868\u793a\u589e\u5f3a\uff0c\u53ef\u4ee5\u5728\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5f3a\u5927\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u591a\u6a21\u6001\u5185\u5bb9\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5f00\u653e\u96c6\u573a\u666f\u4e2d\u7684\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u5bf9\u5b9e\u9645\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2512.20296", "categories": ["cs.CV", "cs.AI", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.20296", "abs": "https://arxiv.org/abs/2512.20296", "authors": ["Ji-Hoon Kim", "Junseok Ahn", "Doyeop Kwak", "Joon Son Chung", "Shinji Watanabe"], "title": "TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation", "comment": "Project page: https://mm.kaist.ac.kr/projects/TAVID", "summary": "The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TAVID\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u6587\u672c\u548c\u53c2\u8003\u56fe\u50cf\u4e2d\u540c\u6b65\u751f\u6210\u4ea4\u4e92\u5f0f\u9762\u90e8\u89c6\u9891\u548c\u5bf9\u8bdd\u8bed\u97f3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u89c6\u542c\u6a21\u6001\u5206\u79bb\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u7684\u4eba\u7c7b\u5bf9\u8bdd\u6a21\u62df\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5b64\u7acb\u5730\u63a2\u7d22\u8bf4\u8bdd\u5934\u751f\u6210\u6216\u5bf9\u8bdd\u8bed\u97f3\u751f\u6210\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u5bf9\u8bdd\u4e2d\u7d27\u5bc6\u8026\u5408\u7684\u89c6\u542c\u4ea4\u4e92\u7279\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u6784\u5efa\u7c7b\u4eba\u5bf9\u8bdd\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u6b65\u751f\u6210\u4ea4\u4e92\u5f0f\u9762\u90e8\u548c\u5bf9\u8bdd\u8bed\u97f3\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86TAVID\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u8de8\u6a21\u6001\u6620\u5c04\u5668\uff08\u8fd0\u52a8\u6620\u5c04\u5668\u548c\u8bf4\u8bdd\u8005\u6620\u5c04\u5668\uff09\u6574\u5408\u9762\u90e8\u548c\u8bed\u97f3\u751f\u6210\u6d41\u7a0b\uff0c\u5b9e\u73b0\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u4e4b\u95f4\u7684\u53cc\u5411\u4fe1\u606f\u4ea4\u6362\uff0c\u4ece\u800c\u540c\u6b65\u751f\u6210\u4ea4\u4e92\u5f0f\u9762\u90e8\u548c\u5bf9\u8bdd\u8bed\u97f3\u3002", "result": "\u5b9e\u9a8c\u5728\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\uff1a\u8bf4\u8bdd\u9762\u90e8\u771f\u5b9e\u6027\u3001\u503e\u542c\u5934\u90e8\u54cd\u5e94\u6027\u3001\u4e8c\u5143\u4ea4\u4e92\u6d41\u7545\u6027\u548c\u8bed\u97f3\u8d28\u91cf\uff0c\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u65b9\u9762\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u542c\u540c\u6b65\u751f\u6210\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u7edf\u4e00\u89c6\u542c\u751f\u6210\u6846\u67b6\u5728\u6784\u5efa\u7c7b\u4eba\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u4fe1\u606f\u4ea4\u6362\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u7684\u4ea4\u4e92\u4f53\u9a8c\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2512.20362", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20362", "abs": "https://arxiv.org/abs/2512.20362", "authors": ["V. Kovalev", "A. Kuvshinov", "A. Buzovkin", "D. Pokidov", "D. Timonin"], "title": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation", "comment": "37 pages, 42 figures", "summary": "Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.\n  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.\n  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CRAFT\u6846\u67b6\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u63a8\u7406\u65f6\u95f4\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u7ea6\u675f\u9a71\u52a8\u53cd\u9988\u6765\u63d0\u5347\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u7684\u53ef\u9760\u6027\u548c\u53ef\u63a7\u6027\u3002\u8be5\u6846\u67b6\u5c06\u63d0\u793a\u5206\u89e3\u4e3a\u4f9d\u8d56\u7ed3\u6784\u5316\u7684\u89c6\u89c9\u95ee\u9898\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7LLM\u4ee3\u7406\u8fdb\u884c\u9488\u5bf9\u6027\u63d0\u793a\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65f6\u95f4\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9690\u5f0f\u3001\u6574\u4f53\u7684\u6279\u8bc4\u6216\u65e0\u7ea6\u675f\u7684\u63d0\u793a\u91cd\u5199\uff0c\u5bfc\u81f4\u5176\u884c\u4e3a\u96be\u4ee5\u89e3\u91ca\u3001\u63a7\u5236\u6216\u53ef\u9760\u505c\u6b62\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5df2\u4ece\u57fa\u4e8e\u9a8c\u8bc1\u3001\u9488\u5bf9\u6027\u4fee\u6b63\u548c\u65e9\u671f\u505c\u6b62\u7684\u663e\u5f0f\u7ed3\u6784\u5316\u601d\u7ef4\u5f62\u5f0f\u4e2d\u53d7\u76ca\uff0c\u800c\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u9886\u57df\u7f3a\u4e4f\u7c7b\u4f3c\u7684\u7cfb\u7edf\u5316\u63a8\u7406\u6846\u67b6\u3002", "method": "CRAFT\u6846\u67b6\u5c06\u63d0\u793a\u5206\u89e3\u4e3a\u4f9d\u8d56\u7ed3\u6784\u5316\u7684\u89c6\u89c9\u95ee\u9898\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7LLM\u4ee3\u7406\u5728\u7ea6\u675f\u5931\u8d25\u5904\u5e94\u7528\u9488\u5bf9\u6027\u63d0\u793a\u7f16\u8f91\u3002\u8be5\u8fc7\u7a0b\u5728\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u6761\u4ef6\u65f6\u91c7\u7528\u663e\u5f0f\u505c\u6b62\u51c6\u5219\u8fdb\u884c\u8fed\u4ee3\uff0c\u5f62\u6210\u4e00\u4e2a\u53ef\u89e3\u91ca\u4e14\u53ef\u63a7\u7684\u63a8\u7406\u65f6\u95f4\u4f18\u5316\u5faa\u73af\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u4e14\u4e0e\u6a21\u578b\u65e0\u5173\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u7cfb\u5217\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRAFT\u6301\u7eed\u63d0\u5347\u4e86\u7ec4\u5408\u51c6\u786e\u6027\u3001\u6587\u672c\u6e32\u67d3\u548c\u57fa\u4e8e\u504f\u597d\u7684\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u8f7b\u91cf\u7ea7\u751f\u6210\u5668\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002\u8fd9\u4e9b\u6539\u8fdb\u4ec5\u5e26\u6765\u53ef\u5ffd\u7565\u7684\u63a8\u7406\u65f6\u95f4\u5f00\u9500\uff0c\u4f7f\u8f83\u5c0f\u6216\u8f83\u4fbf\u5b9c\u7684\u6a21\u578b\u80fd\u591f\u63a5\u8fd1\u66f4\u6602\u8d35\u7cfb\u7edf\u7684\u8d28\u91cf\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u663e\u5f0f\u7ed3\u6784\u5316\u3001\u7ea6\u675f\u9a71\u52a8\u7684\u63a8\u7406\u65f6\u95f4\u4f18\u5316\u662f\u63d0\u5347\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u53ef\u9760\u6027\u7684\u5173\u952e\u8981\u7d20\u3002CRAFT\u6846\u67b6\u4e3a\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u63a7\u7684\u63a8\u7406\u5faa\u73af\uff0c\u4f7f\u8d44\u6e90\u53d7\u9650\u7684\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u667a\u80fd\u63a8\u7406\u8fbe\u5230\u66f4\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\uff0c\u4e3a\u672a\u6765\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.20417", "categories": ["cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.20417", "abs": "https://arxiv.org/abs/2512.20417", "authors": ["Pedro Domingos", "Jo\u00e3o Pereira", "Vasco Lopes", "Jo\u00e3o Neves", "David Semedo"], "title": "Chain-of-Anomaly Thoughts with Large Vision-Language Models", "comment": "2 pages, 3 figures, 1 table. Accepted for RECPAD 2025", "summary": "Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Chain-of-Anomaly-Thoughts (CoAT)\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5f52\u7eb3\u6027\u72af\u7f6a\u504f\u7f6e\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u76d1\u63a7\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u89c6\u9891\u76d1\u63a7\u4e2d\u5b58\u5728\u56fa\u6709\u7684\u6b63\u5e38\u6027\u504f\u7f6e\uff0c\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u72af\u7f6a\u884c\u4e3a\uff0c\u800c\u73b0\u6709\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u7b56\u7565\u7531\u4e8e\u7f3a\u4e4f\u5f52\u7eb3\u6027\u5f02\u5e38\u504f\u7f6e\uff0c\u8fdb\u4e00\u6b65\u5c06\u6a21\u578b\u5bfc\u5411\u6b63\u5e38\u89e3\u91ca\uff0c\u8fd9\u9650\u5236\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Chain-of-Anomaly-Thoughts (CoAT)\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5f15\u5165\u5f52\u7eb3\u6027\u72af\u7f6a\u504f\u7f6e\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u5f02\u5e38\u68c0\u6d4b\u7684\u6700\u7ec8\u5206\u7c7b\u5c42\uff0c\u4ece\u800c\u7cfb\u7edf\u6027\u5730\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u5f02\u5e38\u6a21\u5f0f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u7247\u6bb5\u4e0a\u5c06\u5f02\u5e38\u68c0\u6d4b\u7684F1\u5206\u6570\u63d0\u5347\u4e8611.8\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u4e2d\u5c06\u5f02\u5e38\u5206\u7c7b\u6027\u80fd\u63d0\u5347\u4e863.78\u4e2a\u767e\u5206\u70b9\uff0c\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u5728\u6311\u6218\u6027\u76d1\u63a7\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5728\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\u4e2d\u5f15\u5165\u5f52\u7eb3\u6027\u5f02\u5e38\u504f\u7f6e\uff0c\u53ef\u4ee5\u6709\u6548\u514b\u670d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6b63\u5e38\u6027\u504f\u7f6e\u95ee\u9898\uff0c\u4e3a\u89c6\u9891\u76d1\u63a7\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.20451", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20451", "abs": "https://arxiv.org/abs/2512.20451", "authors": ["Anh Dao", "Manh Tran", "Yufei Zhang", "Xiaoming Liu", "Zijun Cui"], "title": "Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding", "comment": null, "summary": "Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u5c06\u7269\u7406\u63a8\u65ad\u7684\u5173\u8282\u9a71\u52a8\u529b\u6574\u5408\u5230\u4eba\u4f53\u8fd0\u52a8\u7406\u89e3\u6d41\u7a0b\u4e2d\uff0c\u901a\u8fc7\u5728\u6b65\u6001\u8bc6\u522b\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u89c6\u9891\u63cf\u8ff0\u4e09\u5927\u4efb\u52a1\u4e0a\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u529b\u4fe1\u53f7\u5728\u52a8\u6001\u3001\u906e\u6321\u6216\u5916\u89c2\u53d8\u5316\u6761\u4ef6\u4e0b\u80fd\u663e\u8457\u589e\u5f3a\u73b0\u6709\u89c6\u89c9\u548c\u8fd0\u52a8\u5b66\u7279\u5f81\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u4eba\u4f53\u8fd0\u52a8\u7406\u89e3\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bc6\u522b\u3001\u8ddf\u8e2a\u548c\u63cf\u8ff0\u4efb\u52a1\uff0c\u4f46\u666e\u904d\u5ffd\u7565\u4e86\u5173\u8282\u9a71\u52a8\u529b\u7b49\u7269\u7406\u7ebf\u7d22\uff0c\u800c\u8fd9\u4e9b\u7ebf\u7d22\u5728\u751f\u7269\u529b\u5b66\u4e2d\u5177\u6709\u57fa\u7840\u6027\u4f5c\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7269\u7406\u63a8\u65ad\u7684\u529b\u4fe1\u53f7\u662f\u5426\u4ee5\u53ca\u4f55\u65f6\u80fd\u591f\u589e\u5f3a\u8fd0\u52a8\u7406\u89e3\u80fd\u529b\uff0c\u586b\u8865\u5f53\u524d\u65b9\u6cd5\u5728\u7269\u7406\u7ebf\u7d22\u6574\u5408\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u5c06\u7269\u7406\u63a8\u65ad\u7684\u529b\u4fe1\u53f7\u6574\u5408\u5230\u73b0\u6709\u8fd0\u52a8\u7406\u89e3\u6d41\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u4e09\u5927\u4efb\u52a1\u4e0a\u7684\u5f71\u54cd\uff1a\u6b65\u6001\u8bc6\u522b\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u7ec6\u7c92\u5ea6\u89c6\u9891\u63cf\u8ff0\u3002\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u8fc7\u5c06\u529b\u4fe1\u53f7\u4e0e\u57fa\u7ebf\u6a21\u578b\u7ed3\u5408\uff0c\u6784\u5efa\u4e86\u5305\u542b\u7269\u7406\u7ebf\u7d22\u7684\u589e\u5f3a\u578b\u8fd0\u52a8\u7406\u89e3\u7ba1\u9053\uff0c\u91cd\u70b9\u5173\u6ce8\u529b\u4fe1\u53f7\u5728\u52a8\u6001\u3001\u906e\u6321\u548c\u5916\u89c2\u53d8\u5316\u6761\u4ef6\u4e0b\u7684\u8865\u5145\u4f5c\u7528\u3002", "result": "\u5728CASIA-B\u6b65\u6001\u8bc6\u522b\u57fa\u51c6\u4e0a\uff0cRank-1\u51c6\u786e\u7387\u4ece89.52%\u63d0\u5347\u81f390.39%\uff08+0.87%\uff09\uff0c\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\u63d0\u5347\u66f4\u663e\u8457\uff1a\u7a7f\u5916\u5957\u65f6\u63d0\u5347+2.7%\uff0c\u4fa7\u89c6\u56fe\u65f6\u63d0\u5347+3.0%\u3002Gait3D\u57fa\u51c6\u4e0a\u6027\u80fd\u4ece46.0%\u63d0\u5347\u81f347.3%\uff08+1.3%\uff09\u3002\u52a8\u4f5c\u8bc6\u522b\u65b9\u9762\uff0cCTR-GCN\u5728Penn Action\u4e0a\u63d0\u5347+2.00%\uff0c\u9ad8\u5f3a\u5ea6\u52a8\u4f5c\u5982\u51fb\u6253/\u62cd\u6253\u7c7b\u63d0\u5347+6.96%\u3002\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u4e2d\uff0cQwen2.5-VL\u7684ROUGE-L\u5f97\u5206\u4ece0.310\u63d0\u5347\u81f30.339\uff08+0.029\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7269\u7406\u63a8\u65ad\u7684\u529b\u4fe1\u53f7\u80fd\u591f\u663e\u8457\u8865\u5145\u89c6\u89c9\u548c\u8fd0\u52a8\u5b66\u7279\u5f81\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u3001\u906e\u6321\u6216\u5916\u89c2\u53d8\u5316\u7684\u6761\u4ef6\u4e0b\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u8fd0\u52a8\u7406\u89e3\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7269\u7406\u7ebf\u7d22\u6574\u5408\u8303\u5f0f\uff0c\u8868\u660e\u751f\u7269\u529b\u5b66\u4fe1\u606f\u80fd\u591f\u589e\u5f3a\u73b0\u6709\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u8bed\u4e49\u4e30\u5bcc\u6027\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u8fd0\u52a8\u7406\u89e3\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.20479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20479", "abs": "https://arxiv.org/abs/2512.20479", "authors": ["Yiming Zhao", "Yuanpeng Gao", "Yuxuan Luo", "Jiwei Duan", "Shisong Lin", "Longfei Xiong", "Zhouhui Lian"], "title": "UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images", "comment": "22 pages, 25 figures, SIGGRAPH Asia 2025, Conference Paper", "summary": "AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UTDesign\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bbe\u8ba1\u56fe\u50cf\u4e2d\u9ad8\u7cbe\u5ea6\u98ce\u683c\u5316\u6587\u672c\u7f16\u8f91\u548c\u6761\u4ef6\u6587\u672c\u751f\u6210\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u652f\u6301\u82f1\u6587\u548c\u4e2d\u6587\u811a\u672c\uff0c\u901a\u8fc7\u96c6\u6210\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001\u6761\u4ef6\u7f16\u7801\u5668\u5b9e\u73b0\u4e86\u98ce\u683c\u4e00\u81f4\u4e14\u51c6\u786e\u7684\u6587\u672c\u5408\u6210\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u89c6\u89c9\u5185\u5bb9\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6587\u672c\u6e32\u67d3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5c0f\u89c4\u6a21\u6392\u7248\u548c\u975e\u62c9\u4e01\u6587\u5b57\uff08\u5982\u4e2d\u6587\uff09\u4ecd\u7136\u6709\u9650\uff0c\u8fd9\u9650\u5236\u4e86AI\u8f85\u52a9\u56fe\u5f62\u8bbe\u8ba1\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "UTDesign\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u9996\u5148\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDiT\u7684\u6587\u672c\u98ce\u683c\u8fc1\u79fb\u6a21\u578b\uff0c\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u4ece\u5934\u8bad\u7ec3\uff0c\u80fd\u591f\u751f\u6210\u4fdd\u7559\u53c2\u8003\u5b57\u5f62\u98ce\u683c\u7684\u900f\u660eRGBA\u6587\u672c\u524d\u666f\uff1b\u5176\u6b21\uff0c\u901a\u8fc7\u5728\u591a\u6a21\u6001\u6761\u4ef6\u7f16\u7801\u5668\u4e0a\u8bad\u7ec3\uff0c\u6269\u5c55\u4e3a\u6761\u4ef6\u6587\u672c\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u57fa\u4e8e\u80cc\u666f\u56fe\u50cf\u3001\u63d0\u793a\u8bcd\u548c\u5e03\u5c40\u89c4\u683c\u7684\u51c6\u786e\u6587\u672c\u5408\u6210\uff1b\u6700\u540e\uff0c\u5c06\u65b9\u6cd5\u96c6\u6210\u5230\u5168\u81ea\u52a8\u6587\u672c\u5230\u8bbe\u8ba1\u7ba1\u9053\u4e2d\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u548c\u57fa\u4e8eMLLM\u7684\u5e03\u5c40\u89c4\u5212\u5668\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUTDesign\u5728\u5f00\u6e90\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u98ce\u683c\u4e00\u81f4\u6027\u548c\u6587\u672c\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u4e13\u6709\u5546\u4e1a\u65b9\u6cd5\u76f8\u6bd4\u4e5f\u5c55\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u652f\u6301\u4e2d\u82f1\u6587\u811a\u672c\u7684\u9ad8\u7cbe\u5ea6\u6587\u672c\u751f\u6210\u65b9\u9762\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aAI\u8f85\u52a9\u56fe\u5f62\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6587\u672c\u5904\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u62c9\u4e01\u6587\u5b57\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u901a\u8fc7\u6761\u4ef6\u751f\u6210\u548c\u81ea\u52a8\u5316\u7ba1\u9053\u7684\u7ed3\u5408\uff0c\u4e3a\u5b9e\u9645\u8bbe\u8ba1\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u591a\u8bed\u8a00\u6587\u672c\u751f\u6210\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.20501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20501", "abs": "https://arxiv.org/abs/2512.20501", "authors": ["Gorjan Radevski"], "title": "Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition", "comment": "Ph.D. manuscript; Supervisors/Mentors: Marie-Francine Moens and Tinne Tuytelaars", "summary": "This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.\n  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.\n  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.\n  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.\n  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.\n  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.\n  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u591a\u6a21\u6001\u5bf9\u9f50\u3001\u7ffb\u8bd1\u3001\u878d\u5408\u548c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e94\u4e2a\u7ae0\u8282\u5206\u522b\u89e3\u51b3\u4e86\u7a7a\u95f4\u8bed\u8a00\u7406\u89e3\u3001\u533b\u5b66\u6587\u672c\u5bfc\u822a\u3001\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u3001\u52a8\u4f5c\u8bc6\u522b\u878d\u5408\u4ee5\u53ca\u591a\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7b49\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u7cfb\u7edf\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u8f93\u5165\u7684\u80fd\u529b\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u591a\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u5c06\u6587\u672c\u7a7a\u95f4\u5173\u7cfb\u8f6c\u6362\u4e3a\u89c6\u89c9\u6392\u5217\u3001\u533b\u5b66\u6587\u672c\u4e0e\u89e3\u5256\u56fe\u8c31\u7684\u7cbe\u786e\u6620\u5c04\u3001\u7ed3\u6784\u5316\u6587\u672c\u5230\u77e5\u8bc6\u56fe\u8c31\u4e8b\u5b9e\u7684\u94fe\u63a5\u3001\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u7684\u591a\u6a21\u6001\u878d\u5408\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u77e5\u8bc6\u8fc1\u79fb\u4f7f\u5355\u6a21\u6001\u6a21\u578b\u83b7\u5f97\u591a\u6a21\u6001\u80fd\u529b\uff0c\u4ece\u800c\u589e\u5f3a\u8ba1\u7b97\u7cfb\u7edf\u5bf9\u590d\u6742\u591a\u6a21\u6001\u8f93\u5165\u7684\u7406\u89e3\u548c\u5904\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e94\u79cd\u6838\u5fc3\u6280\u672f\u65b9\u6cd5\uff1a\u7b2c\u4e09\u7ae0\u5f00\u53d1\u4e86Spatial-Reasoning Bert\u6a21\u578b\uff0c\u5c06\u57fa\u4e8e\u6587\u672c\u7684\u7a7a\u95f4\u5173\u7cfb\u7ffb\u8bd1\u4e3a\u4e8c\u7ef4\u6392\u5217\uff1b\u7b2c\u56db\u7ae0\u5f15\u5165\u5229\u7528\u533b\u5b66\u672f\u8bed\u7a7a\u95f4\u5171\u73b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u533b\u5b66\u6587\u672c\u5230\u89e3\u5256\u56fe\u8c31\u4e09\u7ef4\u4f4d\u7f6e\u7684\u6620\u5c04\uff1b\u7b2c\u4e94\u7ae0\u5efa\u7acb\u4e86\u5c06\u7ed3\u6784\u5316\u6587\u672c\u94fe\u63a5\u5230\u77e5\u8bc6\u56fe\u8c31\u5b9e\u4f53\u548c\u8c13\u8bcd\u7684\u57fa\u51c6\uff1b\u7b2c\u516d\u7ae0\u63d0\u51fa\u4e86\u878d\u5408\u89c6\u9891\u5e27\u548c\u7269\u4f53\u68c0\u6d4b\u8868\u793a\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff1b\u7b2c\u4e03\u7ae0\u63a2\u7d22\u4e86\u591a\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u4f7fRGB-only\u6a21\u578b\u80fd\u591f\u6a21\u4eff\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u5b9e\u73b0\u4e86\u591a\u9879\u91cd\u8981\u6210\u679c\uff1a\u7a7a\u95f4\u8bed\u8a00\u5230\u89c6\u89c9\u6392\u5217\u7684\u6709\u6548\u89e3\u7801\uff0c\u4e3a\u81ea\u52a8\u5316\u573a\u666f\u751f\u6210\u5960\u5b9a\u4e86\u57fa\u7840\uff1b\u533b\u5b66\u6587\u672c\u5bfc\u822a\u6027\u663e\u8457\u589e\u5f3a\uff0c\u521b\u5efa\u4e86\u53ef\u89e3\u91ca\u7684\u6620\u5c04\u5173\u7cfb\uff1b\u5efa\u7acb\u4e86\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u7684\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u63d0\u53d6\u4e2d\u7684\u6b67\u4e49\u95ee\u9898\uff1b\u52a8\u4f5c\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u5f97\u5230\u63d0\u5347\uff1b\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\uff0cRGB-only\u6a21\u578b\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "\u8be5\u7814\u7a76\u5728\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u9886\u57df\u505a\u51fa\u4e86\u7cfb\u7edf\u6027\u8d21\u732e\uff0c\u63a8\u8fdb\u4e86\u7a7a\u95f4\u8bed\u8a00\u7406\u89e3\u3001\u533b\u5b66\u6587\u672c\u89e3\u91ca\u3001\u77e5\u8bc6\u56fe\u8c31\u4e30\u5bcc\u5316\u548c\u52a8\u4f5c\u8bc6\u522b\u7684\u65b9\u6cd5\u8bba\u53d1\u5c55\u3002\u8fd9\u4e9b\u65b9\u6cd5\u589e\u5f3a\u4e86\u8ba1\u7b97\u7cfb\u7edf\u8de8\u591a\u6837\u5316\u5e94\u7528\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u8f93\u5165\u7684\u80fd\u529b\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u9700\u6c42\u3002"}}
{"id": "2512.20556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20556", "abs": "https://arxiv.org/abs/2512.20556", "authors": ["Mingwei Tang", "Jiahao Nie", "Guang Yang", "Ziqing Cui", "Jie Li"], "title": "Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios", "comment": "Accepted to WACV 2026", "summary": "Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7c92\u5ea6\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\uff08MTIF\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u7ec6\u7c92\u5ea6\u3001\u7ed3\u6784\u6027\u548c\u8bed\u4e49\u6027\u7684\u591a\u5c42\u6b21\u6587\u672c\u63cf\u8ff0\uff0c\u7ed3\u5408\u5206\u5c42\u8de8\u6a21\u6001\u8c03\u5236\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u66dd\u5149\u548c\u591a\u7126\u70b9\u56fe\u50cf\u878d\u5408\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u7c97\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\u4f5c\u4e3a\u8f85\u52a9\u6307\u5bfc\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u56fe\u50cf\u7ec6\u7c92\u5ea6\u7ec6\u8282\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u7ed9\u8de8\u6a21\u6001\u5bf9\u9f50\u5e26\u6765\u4e86\u6311\u6218\uff0c\u5bfc\u81f4\u878d\u5408\u8d28\u91cf\u53d7\u9650\u3002", "method": "MTIF\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\u5f15\u5165\u591a\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\u5206\u522b\u6355\u6349\u7ec6\u8282\u3001\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff1b\u91c7\u7528\u5206\u5c42\u8de8\u6a21\u6001\u8c03\u5236\u6a21\u5757\u5b9e\u73b0\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u878d\u5408\uff1b\u5728\u6bcf\u4e2a\u7c92\u5ea6\u7ea7\u522b\u6dfb\u52a0\u76d1\u7763\u4fe1\u53f7\u4ee5\u4fc3\u8fdb\u89c6\u89c9-\u6587\u672c\u7279\u5f81\u5bf9\u9f50\uff1b\u4ee5\u53ca\u4f7f\u7528\u663e\u8457\u6027\u9a71\u52a8\u7684\u6570\u636e\u589e\u5f3a\u6a21\u5757\u6765\u4e30\u5bcc\u8bad\u7ec3\u6570\u636e\u7684\u8bed\u4e49\u5185\u5bb9\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMTIF\u5728\u591a\u66dd\u5149\u548c\u591a\u7126\u70b9\u56fe\u50cf\u878d\u5408\u4efb\u52a1\u4e0a\u5747\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u591a\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\u548c\u5206\u5c42\u8de8\u6a21\u6001\u8c03\u5236\u5728\u63d0\u5347\u56fe\u50cf\u878d\u5408\u8d28\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u7c92\u5ea6\u6587\u672c\u6307\u5bfc\u5728\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u3001\u7ed3\u6784\u6027\u548c\u8bed\u4e49\u6027\u63cf\u8ff0\u7684\u5c42\u6b21\u5316\u6574\u5408\uff0c\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4e3a\u6587\u672c\u5f15\u5bfc\u7684\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u878d\u5408\u8303\u5f0f\u3002"}}
{"id": "2512.20557", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20557", "abs": "https://arxiv.org/abs/2512.20557", "authors": ["Shengchao Zhou", "Yuxin Chen", "Yuying Ge", "Wei Huang", "Jiehong Lin", "Ying Shan", "Xiaojuan Qi"], "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "comment": null, "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DSR Suite\uff0c\u4e00\u4e2a\u9488\u5bf9\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\uff08DSR\uff09\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u548c\u6a21\u578b\u589e\u5f3a\u5957\u4ef6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7ba1\u9053\u4ece\u91ce\u5916\u89c6\u9891\u751f\u6210\u51e0\u4f55\u611f\u77e5\u7684\u95ee\u7b54\u5bf9\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u51e0\u4f55\u9009\u62e9\u6a21\u5757\u5c06\u51e0\u4f55\u5148\u9a8c\u96c6\u6210\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\uff08DSR\uff09\u65b9\u9762\u4ecd\u7136\u8584\u5f31\uff0c\u5373\u63a8\u74063D\u7a7a\u95f4\u4e2d\u7269\u4f53\u51e0\u4f55\u548c\u5173\u7cfb\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u80fd\u529b\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u7f3a\u4e4f\u53ef\u6269\u5c55\u76844D\u611f\u77e5\u8bad\u7ec3\u8d44\u6e90\u3002\u73b0\u6709\u5de5\u4f5c\u96be\u4ee5\u6ee1\u8db3\u5bf9\u91ce\u5916\u89c6\u9891\u6e90\u3001\u7269\u4f53\u548c\u573a\u666f\u7ea73D\u8981\u6c42\u3001\u89c6\u70b9\u53d8\u6362\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u4ee5\u53ca\u7ec6\u7c92\u5ea6\u7a0b\u5e8f\u6027\u7b54\u6848\u7684\u9700\u6c42\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86DSR Suite\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u9996\u5148\uff0c\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u7ba1\u9053\u4ece\u91ce\u5916\u89c6\u9891\u751f\u6210\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u7684\u591a\u9009\u95ee\u7b54\u5bf9\uff0c\u5229\u7528\u73b0\u4ee3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u5305\u62ec\u76f8\u673a\u59ff\u6001\u3001\u5c40\u90e8\u70b9\u4e91\u3001\u7269\u4f53\u63a9\u7801\u3001\u65b9\u5411\u548c3D\u8f68\u8ff9\uff1b\u5176\u6b21\uff0c\u6784\u5efa\u4e86\u7528\u4e8e\u8bad\u7ec3\u7684DSR-Train\u6570\u636e\u96c6\u548c\u7ecf\u8fc7\u4eba\u5de5\u7cbe\u70bc\u7684\u8bc4\u4f30\u57fa\u51c6DSR-Bench\uff1b\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u51e0\u4f55\u9009\u62e9\u6a21\u5757\uff0c\u5c06\u51e0\u4f55\u5148\u9a8c\u65e0\u7f1d\u96c6\u6210\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u8be5\u6a21\u5757\u538b\u7f29\u95ee\u9898\u8bed\u4e49\u5e76\u4ece\u9884\u8bad\u7ec3\u76844D\u91cd\u5efa\u5148\u9a8c\u4e2d\u63d0\u53d6\u95ee\u9898\u76f8\u5173\u77e5\u8bc6\u5230\u7d27\u51d1\u7684\u51e0\u4f55\u6807\u8bb0\u96c6\u5408\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06DSR-Train\u548c\u51e0\u4f55\u9009\u62e9\u6a21\u5757\u96c6\u6210\u5230Qwen2.5-VL-7B\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5176\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u5728\u901a\u7528\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002\u4e0e\u5148\u524d\u5de5\u4f5c\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u91ce\u5916\u89c6\u9891\u6e90\u3001\u7269\u4f53\u548c\u573a\u666f\u7ea73D\u8981\u6c42\u3001\u89c6\u70b9\u53d8\u6362\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u4ee5\u53ca\u7ec6\u7c92\u5ea6\u7a0b\u5e8f\u6027\u7b54\u6848\u7b49\u65b9\u9762\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7DSR Suite\u5728\u6570\u636e\u96c6\u3001\u57fa\u51c6\u548c\u6a21\u578b\u4e09\u4e2a\u5c42\u9762\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e86\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u7684\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u5c06\u51e0\u4f55\u5148\u9a8c\u6709\u6548\u96c6\u6210\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53ef\u884c\u6027\u3002\u51e0\u4f55\u9009\u62e9\u6a21\u5757\u7684\u8bbe\u8ba1\u907f\u514d\u4e86\u7528\u65e0\u5173\u77e5\u8bc6\u6df9\u6ca1\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9488\u5bf9\u6027\u77e5\u8bc6\u63d0\u53d6\uff0c\u4e3a\u672a\u67654D\u611f\u77e5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u65b9\u5411\u3002"}}
{"id": "2512.20561", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20561", "abs": "https://arxiv.org/abs/2512.20561", "authors": ["Kaitong Cai", "Jusheng Zhang", "Jing Yang", "Yijia Fan", "Pengtao Xie", "Jian Wang", "Keze Wang"], "title": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models", "comment": "Under submission", "summary": "Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.\n  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.\n  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFlashVLM\uff0c\u4e00\u79cd\u6587\u672c\u5f15\u5bfc\u7684\u89c6\u89c9\u4ee4\u724c\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u67e5\u8be2\u7684\u89c6\u89c9\u8f93\u5165\u538b\u7f29\uff0c\u5728\u663e\u8457\u51cf\u5c11\u89c6\u89c9\u4ee4\u724c\u7684\u540c\u65f6\u5b9e\u73b0\u8d85\u8d8a\u65e0\u635f\u538b\u7f29\u7684\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u9700\u8981\u5904\u7406\u6570\u767e\u81f3\u6570\u5343\u4e2a\u89c6\u89c9\u4ee4\u724c\uff0c\u5bfc\u81f4\u4e8c\u6b21\u6ce8\u610f\u529b\u6210\u672c\u548c\u5927\u91cf\u5197\u4f59\u3002\u73b0\u6709\u4ee4\u724c\u7f29\u51cf\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u6587\u672c\u67e5\u8be2\u6216\u4f9d\u8d56\u4e0d\u7a33\u5b9a\u7684\u6df1\u5ea6\u6ce8\u610f\u529b\u56fe\uff0c\u5728\u6fc0\u8fdb\u526a\u679d\u4e0b\u4f1a\u5bfc\u81f4\u8bed\u4e49\u5bf9\u9f50\u9000\u5316\u3002", "method": "FlashVLM\u91c7\u7528\u6587\u672c\u5f15\u5bfc\u7684\u89c6\u89c9\u4ee4\u724c\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u6295\u5f71\u56fe\u50cf\u4ee4\u724c\u4e0e\u5f52\u4e00\u5316\u6587\u672c\u5d4c\u5165\u5728\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u4e2d\u7684\u663e\u5f0f\u8de8\u6a21\u6001\u76f8\u4f3c\u5ea6\uff0c\u5c06\u5916\u5728\u76f8\u5173\u6027\u4e0e\u5185\u5728\u89c6\u89c9\u663e\u8457\u6027\u901a\u8fc7\u5bf9\u6570\u57df\u52a0\u6743\u548c\u6e29\u5ea6\u63a7\u5236\u9510\u5316\u878d\u5408\uff0c\u5e76\u91c7\u7528\u591a\u6837\u6027\u4fdd\u6301\u5206\u533a\u4fdd\u7559\u6700\u5c0f\u4f46\u5177\u4ee3\u8868\u6027\u7684\u80cc\u666f\u4ee4\u724c\u4ee5\u7ef4\u6301\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u76f8\u540c\u4ee4\u724c\u9884\u7b97\u548c\u8bc4\u4f30\u534f\u8bae\u4e0b\uff0cFlashVLM\u5728LLaVA 1.5\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8d8a\u65e0\u635f\u538b\u7f29\u7684\u6027\u80fd\uff0c\u5728\u526a\u679d\u9ad8\u8fbe77.8%\u89c6\u89c9\u4ee4\u724c\u65f6\u7565\u5fae\u8d85\u8fc7\u672a\u526a\u679d\u57fa\u7ebf\uff0c\u5373\u4f7f\u572894.4%\u538b\u7f29\u7387\u4e0b\u4ecd\u4fdd\u630192.8%\u51c6\u786e\u7387\u3002\u572814\u4e2a\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6548\u7387-\u6027\u80fd\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u663e\u5f0f\u8de8\u6a21\u6001\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u548c\u591a\u6837\u6027\u4fdd\u6301\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u5927\u5e45\u51cf\u5c11\u89c6\u89c9\u4ee4\u724c\u7684\u540c\u65f6\u7ef4\u6301\u751a\u81f3\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2512.20617", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20617", "abs": "https://arxiv.org/abs/2512.20617", "authors": ["Yuxi Xiao", "Longfei Li", "Shen Yan", "Xinhang Liu", "Sida Peng", "Yunchao Wei", "Xiaowei Zhou", "Bingyi Kang"], "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "comment": "webpage: https://spatialtree.github.io/", "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86SpatialTree\uff0c\u4e00\u4e2a\u53d7\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\u7684\u56db\u5c42\u6b21\u7a7a\u95f4\u80fd\u529b\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u5c42\u6b21\u5316\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002\u7814\u7a76\u53d1\u73b0\u7a7a\u95f4\u80fd\u529b\u5728MLLMs\u4e2d\u5448\u73b0\u6e05\u6670\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u8de8\u5c42\u6b21\u8fc1\u79fb\u7684\u52a8\u6001\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u81ea\u52a8\u601d\u8003\u7b56\u7565\u6765\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u5728\u6240\u6709\u5c42\u6b21\u7684\u8868\u73b0\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u7a7a\u95f4\u80fd\u529b\u5c42\u6b21\u7ed3\u6784\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u5f53\u524d\u7814\u7a76\u5927\u591a\u5173\u6ce8\u72ed\u7a84\u7684\u4efb\u52a1\u96c6\uff0c\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u80fd\u529b\u4ece\u4f4e\u7ea7\u611f\u77e5\u5230\u9ad8\u7ea7\u63a8\u7406\u4e0e\u4ea4\u4e92\u7684\u6e10\u8fdb\u53d1\u5c55\u5c42\u6b21\u7684\u7cfb\u7edf\u6027\u8ba4\u77e5\u79d1\u5b66\u7406\u89e3\uff0c\u8fd9\u963b\u788d\u4e86\u5bf9MLLMs\u7a7a\u95f4\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\u4e0e\u7cfb\u7edf\u6027\u63d0\u5347\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86SpatialTree\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u53d7\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\u7684\u56db\u5c42\u6b21\u7a7a\u95f4\u80fd\u529b\u5206\u7c7b\u4f53\u7cfb\uff1a\u4f4e\u7ea7\u611f\u77e5\uff08L1\uff09\u3001\u5fc3\u7406\u6620\u5c04\uff08L2\uff09\u3001\u6a21\u62df\uff08L3\uff09\u548c\u667a\u80fd\u4f53\u80fd\u529b\uff08L4\uff09\u3002\u57fa\u4e8e\u6b64\u5206\u7c7b\u6cd5\uff0c\u6784\u5efa\u4e86\u9996\u4e2a\u4ee5\u80fd\u529b\u4e3a\u4e2d\u5fc3\u7684\u5c42\u6b21\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5168\u9762\u8bc4\u4f30\u4e3b\u6d41MLLMs\u572827\u4e2a\u5b50\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u6709\u76d1\u7763\u5fae\u8c03\u4e0b\u7684\u8fc1\u79fb\u52a8\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u81ea\u52a8\u601d\u8003\u7b56\u7565\u6765\u6291\u5236\u4e0d\u5fc5\u8981\u7684\u6df1\u601d\u719f\u8651\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u4e00\u81f4\u5730\u63d0\u5347\u6240\u6709\u5c42\u6b21\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u4e86\u4e00\u4e2a\u6e05\u6670\u7684\u7ed3\u6784\uff1aL1\u6280\u80fd\u57fa\u672c\u6b63\u4ea4\uff0c\u800c\u66f4\u9ad8\u7ea7\u522b\u7684\u6280\u80fd\u5219\u5f3a\u76f8\u5173\uff0c\u8868\u660e\u968f\u7740\u5c42\u6b21\u63d0\u5347\uff0c\u80fd\u529b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u589e\u5f3a\u3002\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u6709\u76d1\u7763\u5fae\u8c03\uff0c\u53d1\u73b0\u4e86\u4ee4\u4eba\u60ca\u8bb6\u7684\u8fc1\u79fb\u52a8\u6001\uff1aL1\u5185\u90e8\u5b58\u5728\u8d1f\u8fc1\u79fb\uff0c\u4f46\u4ece\u4f4e\u5c42\u5230\u9ad8\u5c42\u80fd\u529b\u5b58\u5728\u5f3a\u8de8\u5c42\u8fc1\u79fb\u5e76\u8868\u73b0\u51fa\u663e\u8457\u7684\u534f\u540c\u6548\u5e94\u3002\u5b9e\u9a8c\u8fd8\u53d1\u73b0\uff0c\u9f13\u52b1\u5e7f\u6cdb\"\u601d\u8003\"\u7684\u6734\u7d20\u5f3a\u5316\u5b66\u4e60\u4e0d\u53ef\u9760\uff1a\u5b83\u6709\u52a9\u4e8e\u590d\u6742\u63a8\u7406\u4f46\u635f\u5bb3\u76f4\u89c9\u611f\u77e5\uff0c\u800c\u63d0\u51fa\u7684\u81ea\u52a8\u601d\u8003\u7b56\u7565\u80fd\u591f\u4f7f\u5f3a\u5316\u5b66\u4e60\u5728\u6240\u6709\u5c42\u6b21\u4e0a\u4e00\u81f4\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efaSpatialTree\uff0c\u4e3a\u7406\u89e3\u548c\u7cfb\u7edf\u6027\u6269\u5c55MLLMs\u4e2d\u7684\u7a7a\u95f4\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u6846\u67b6\u3002\u7814\u7a76\u63ed\u793a\u4e86\u7a7a\u95f4\u80fd\u529b\u5728MLLMs\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\u7279\u6027\uff0c\u53d1\u73b0\u4e86\u8de8\u5c42\u6b21\u8fc1\u79fb\u7684\u52a8\u6001\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u4f18\u5316\u7b56\u7565\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u5168\u9762\u3001\u5c42\u6b21\u5316\u7684\u591a\u6a21\u6001\u80fd\u529b\u8bc4\u4f30\u4f53\u7cfb\u4ee5\u53ca\u8bbe\u8ba1\u9488\u5bf9\u6027\u7684\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
