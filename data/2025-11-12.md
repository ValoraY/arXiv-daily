<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 37]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.AI](#cs.AI) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label Cross-Modal Retrieval](https://arxiv.org/abs/2511.07780)
*Likang Peng, Chao Su, Wenyuan Wu, Yuan Sun, Dezhong Peng, Xi Peng, Xu Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯­ä¹‰ä¸€è‡´åŒå‘å¯¹æ¯”å“ˆå¸Œæ¡†æ¶ï¼ˆSCBCHï¼‰ï¼Œé€šè¿‡è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§åˆ†ç±»å’ŒåŒå‘è½¯å¯¹æ¯”å“ˆå¸Œæ¨¡å—ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ ‡ç­¾è·¨æ¨¡æ€æ£€ç´¢ä¸­çš„æ ‡ç­¾å™ªå£°å’Œè¯­ä¹‰é‡å é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è·¨æ¨¡æ€å“ˆå¸Œæ–¹æ³•ä¸¥é‡ä¾èµ–å…¨æ ‡æ³¨æ•°æ®é›†ï¼Œè€Œåœ¨å®é™…å¤šæ ‡ç­¾åœºæ™¯ä¸­æ ‡ç­¾å™ªå£°æ™®éå­˜åœ¨ä¸”ä¼šæ˜¾è‘—é™ä½æ£€ç´¢æ€§èƒ½ï¼ŒåŒæ—¶ç°æœ‰æ–¹æ³•é€šå¸¸å¿½ç•¥äº†å¤šæ ‡ç­¾æ•°æ®ä¸­å›ºæœ‰çš„éƒ¨åˆ†è¯­ä¹‰é‡å é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Method:** æå‡ºçš„SCBCHæ¡†æ¶åŒ…å«ä¸¤ä¸ªäº’è¡¥æ¨¡å—ï¼šè·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§åˆ†ç±»ï¼ˆCSCCï¼‰åˆ©ç”¨è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§ä¼°è®¡æ ·æœ¬å¯é æ€§ä»¥å‡å°‘å™ªå£°æ ‡ç­¾å½±å“ï¼›åŒå‘è½¯å¯¹æ¯”å“ˆå¸Œï¼ˆBSCHï¼‰åŸºäºå¤šæ ‡ç­¾è¯­ä¹‰é‡å åŠ¨æ€ç”Ÿæˆè½¯å¯¹æ¯”æ ·æœ¬å¯¹ï¼Œå®ç°è·¨æ¨¡æ€è¯­ä¹‰ç›¸ä¼¼ä¸ä¸ç›¸ä¼¼æ ·æœ¬é—´çš„è‡ªé€‚åº”å¯¹æ¯”å­¦ä¹ ã€‚

**Result:** åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„è·¨æ¨¡æ€æ£€ç´¢åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ï¼Œåœ¨å™ªå£°å¤šæ ‡ç­¾æ¡ä»¶ä¸‹å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§å’Œè‡ªé€‚åº”å¯¹æ¯”å­¦ä¹ èƒ½å¤Ÿæœ‰æ•ˆæå‡å™ªå£°å¤šæ ‡ç­¾åœºæ™¯ä¸‹çš„è·¨æ¨¡æ€æ£€ç´¢æ€§èƒ½ï¼Œä¸ºå¤„ç†ç°å®ä¸–ç•Œä¸­çš„ä¸å®Œç¾æ ‡æ³¨æ•°æ®æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Cross-modal hashing (CMH) facilitates efficient retrieval across different modalities (e.g., image and text) by encoding data into compact binary representations. While recent methods have achieved remarkable performance, they often rely heavily on fully annotated datasets, which are costly and labor-intensive to obtain. In real-world scenarios, particularly in multi-label datasets, label noise is prevalent and severely degrades retrieval performance. Moreover, existing CMH approaches typically overlook the partial semantic overlaps inherent in multi-label data, limiting their robustness and generalization. To tackle these challenges, we propose a novel framework named Semantic-Consistent Bidirectional Contrastive Hashing (SCBCH). The framework comprises two complementary modules: (1) Cross-modal Semantic-Consistent Classification (CSCC), which leverages cross-modal semantic consistency to estimate sample reliability and reduce the impact of noisy labels; (2) Bidirectional Soft Contrastive Hashing (BSCH), which dynamically generates soft contrastive sample pairs based on multi-label semantic overlap, enabling adaptive contrastive learning between semantically similar and dissimilar samples across modalities. Extensive experiments on four widely-used cross-modal retrieval benchmarks validate the effectiveness and robustness of our method, consistently outperforming state-of-the-art approaches under noisy multi-label conditions.


### [2] [Cancer-Net PCa-MultiSeg: Multimodal Enhancement of Prostate Cancer Lesion Segmentation Using Synthetic Correlated Diffusion Imaging](https://arxiv.org/abs/2511.07816)
*Jarett Dewbury, Chi-en Amy Tai, Alexander Wong*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºå°†åˆæˆç›¸å…³æ‰©æ•£æˆåƒï¼ˆCDI^sï¼‰ä½œä¸ºæ ‡å‡†æ‰©æ•£æˆåƒåè®®çš„å¢å¼ºæ–¹æ³•ï¼Œåœ¨å…­ç§æœ€å…ˆè¿›çš„åˆ†å‰²æ¶æ„ä¸ŠéªŒè¯äº†CDI^sèƒ½å¤Ÿå¯é æå‡å‰åˆ—è…ºç™Œç—…ç¶åˆ†å‰²æ€§èƒ½ï¼Œæœ€é«˜å®ç°72.5%çš„ç›¸å¯¹æ”¹è¿›ï¼Œä¸”æ— éœ€é¢å¤–æ‰«ææ—¶é—´å³å¯éƒ¨ç½²åˆ°ä¸´åºŠå·¥ä½œæµä¸­ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºæ·±åº¦å­¦ä¹ çš„å‰åˆ—è…ºç™Œç—…ç¶åˆ†å‰²æ–¹æ³•åœ¨å¤§è§„æ¨¡æ‚£è€…é˜Ÿåˆ—ä¸­è¡¨ç°æœ‰é™ï¼ŒDiceåˆ†æ•°ä»…ä¸º0.32æˆ–æ›´ä½ï¼Œè¿™å‡¸æ˜¾äº†éœ€è¦æ›´æœ‰æ•ˆçš„æˆåƒå¢å¼ºç­–ç•¥æ¥æå‡åˆ†å‰²æ€§èƒ½ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨åˆæˆç›¸å…³æ‰©æ•£æˆåƒï¼ˆCDI^sï¼‰ä½œä¸ºæ ‡å‡†æ‰©æ•£æˆåƒåè®®çš„å¢å¼ºï¼Œåœ¨200åæ‚£è€…çš„å…±é…å‡†CDI^sã€æ‰©æ•£åŠ æƒæˆåƒå’Œè¡¨è§‚æ‰©æ•£ç³»æ•°åºåˆ—ä¸Šï¼Œç³»ç»Ÿè¯„ä¼°äº†å…­ç§æœ€å…ˆè¿›çš„åˆ†å‰²æ¶æ„çš„æ€§èƒ½è¡¨ç°ã€‚

**Result:** CDI^sé›†æˆåœ¨94%çš„è¯„ä¼°é…ç½®ä¸­å¯é åœ°æå‡æˆ–ä¿æŒäº†åˆ†å‰²æ€§èƒ½ï¼Œä¸ªåˆ«æ¶æ„ç›¸æ¯”åŸºçº¿æ¨¡æ€å®ç°äº†é«˜è¾¾72.5%çš„ç»Ÿè®¡æ˜¾è‘—ç›¸å¯¹æ”¹è¿›ï¼Œå…¶ä¸­CDI^s + DWIç»„åˆåœ¨ä¸€åŠè¯„ä¼°æ¶æ„ä¸­å®ç°æ˜¾è‘—æ”¹è¿›ä¸”æ— æ€§èƒ½ä¸‹é™å®ä¾‹ã€‚

**Conclusion:** CDI^sä½œä¸ºç°æœ‰DWIé‡‡é›†çš„è¡ç”ŸæŠ€æœ¯ï¼Œæ— éœ€é¢å¤–æ‰«ææ—¶é—´æˆ–æ¶æ„ä¿®æ”¹å³å¯å®ç°å³æ—¶ä¸´åºŠéƒ¨ç½²ï¼Œä¸ºå‰åˆ—è…ºç™Œç—…ç¶åˆ†å‰²ä»»åŠ¡å»ºç«‹äº†ç»è¿‡éªŒè¯çš„å®ç”¨å¢å¼ºé›†æˆè·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Current deep learning approaches for prostate cancer lesion segmentation achieve limited performance, with Dice scores of 0.32 or lower in large patient cohorts. To address this limitation, we investigate synthetic correlated diffusion imaging (CDI$^s$) as an enhancement to standard diffusion-based protocols. We conduct a comprehensive evaluation across six state-of-the-art segmentation architectures using 200 patients with co-registered CDI$^s$, diffusion-weighted imaging (DWI) and apparent diffusion coefficient (ADC) sequences. We demonstrate that CDI$^s$ integration reliably enhances or preserves segmentation performance in 94% of evaluated configurations, with individual architectures achieving up to 72.5% statistically significant relative improvement over baseline modalities. CDI$^s$ + DWI emerges as the safest enhancement pathway, achieving significant improvements in half of evaluated architectures with zero instances of degradation. Since CDI$^s$ derives from existing DWI acquisitions without requiring additional scan time or architectural modifications, it enables immediate deployment in clinical workflows. Our results establish validated integration pathways for CDI$^s$ as a practical drop-in enhancement for PCa lesion segmentation tasks across diverse deep learning architectures.


### [3] [Visual Bridge: Universal Visual Perception Representations Generating](https://arxiv.org/abs/2511.07877)
*Yilin Gao, Shuguang Dou, Junzhou Li, Zhiheng Yu, Yin Li, Dongsheng Jiang, Shugong Xu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…çš„é€šç”¨è§†è§‰æ„ŸçŸ¥æ¡†æ¶ï¼Œèƒ½å¤Ÿè·¨å¤šä¸ªä»»åŠ¡ç”Ÿæˆå¤šæ ·åŒ–çš„è§†è§‰è¡¨ç¤ºï¼Œçªç ´äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„å•ä»»åŠ¡é™åˆ¶ï¼Œå®ç°äº†ç«äº‰æ€§çš„é›¶æ ·æœ¬å’Œå¾®è°ƒæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ·±åº¦ä¼°è®¡å’Œå…‰æµç­‰å­¤ç«‹è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å—åˆ°â€œå•ä»»åŠ¡-å•æ¨¡å‹â€èŒƒå¼çš„é™åˆ¶ï¼Œä¸¥é‡åˆ¶çº¦äº†å…¶åœ¨å¤šä»»åŠ¡åœºæ™¯ä¸­çš„æ³›åŒ–æ€§å’Œå¯æ‰©å±•æ€§ã€‚å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›çš„å¯å‘ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤šä»»åŠ¡è§†è§‰æ„ŸçŸ¥ä¸­çš„é€šç”¨æ€§é—®é¢˜ã€‚

**Method:** è¯¥æ–¹æ³•å°†è§†è§‰è¡¨ç¤ºç”Ÿæˆè¿‡ç¨‹æ„å»ºä¸ºä»å›¾åƒå—æ ‡è®°åˆ°ä»»åŠ¡ç‰¹å®šè¡¨ç¤ºçš„é€šç”¨æµåŒ¹é…é—®é¢˜ï¼Œè€Œéç‹¬ç«‹çš„ç”Ÿæˆæˆ–å›å½’é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨å¼ºå¤§çš„è‡ªç›‘ç£åŸºç¡€æ¨¡å‹ä½œä¸ºé”šç‚¹ï¼Œå¹¶å¼•å…¥å¤šå°ºåº¦å¾ªç¯ä»»åŠ¡åµŒå…¥æœºåˆ¶ï¼Œå­¦ä¹ ä¸€ä¸ªé€šç”¨é€Ÿåº¦åœºæ¥æ¡¥æ¥å¼‚æ„ä»»åŠ¡ä¹‹é—´çš„å·®è·ï¼Œæ”¯æŒé«˜æ•ˆçµæ´»çš„è¡¨å¾è¿ç§»ã€‚

**Result:** åœ¨åˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ã€æ·±åº¦ä¼°è®¡å’Œå›¾åƒ-æ–‡æœ¬æ£€ç´¢ç­‰ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸‹å‡å®ç°äº†ç«äº‰æ€§æ€§èƒ½ï¼Œè¶…è¶Šäº†å…ˆå‰é€šç”¨æ¨¡å‹å’Œå¤šä¸ªä¸“ç”¨æ¨¡å‹ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¡†æ¶çš„é²æ£’æ€§ã€å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** æœ¬ç ”ç©¶æ ‡å¿—ç€å‘é€šç”¨è§†è§‰æ„ŸçŸ¥è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œä¸ºæœªæ¥é€šç”¨è§†è§‰å»ºæ¨¡ç ”ç©¶æä¾›äº†åšå®åŸºç¡€ã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†é€šè¿‡æµåŒ¹é…æ–¹æ³•å®ç°è·¨ä»»åŠ¡è§†è§‰è¡¨ç¤ºç”Ÿæˆçš„å¯è¡Œæ€§ï¼Œä¸ºæ„å»ºæ›´é€šç”¨çš„è§†è§‰ç³»ç»Ÿå¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Recent advances in diffusion models have achieved remarkable success in isolated computer vision tasks such as text-to-image generation, depth estimation, and optical flow. However, these models are often restricted by a ``single-task-single-model'' paradigm, severely limiting their generalizability and scalability in multi-task scenarios. Motivated by the cross-domain generalization ability of large language models, we propose a universal visual perception framework based on flow matching that can generate diverse visual representations across multiple tasks. Our approach formulates the process as a universal flow-matching problem from image patch tokens to task-specific representations rather than an independent generation or regression problem. By leveraging a strong self-supervised foundation model as the anchor and introducing a multi-scale, circular task embedding mechanism, our method learns a universal velocity field to bridge the gap between heterogeneous tasks, supporting efficient and flexible representation transfer. Extensive experiments on classification, detection, segmentation, depth estimation, and image-text retrieval demonstrate that our model achieves competitive performance in both zero-shot and fine-tuned settings, outperforming prior generalist and several specialist models. Ablation studies further validate the robustness, scalability, and generalization of our framework. Our work marks a significant step towards general-purpose visual perception, providing a solid foundation for future research in universal vision modeling.


### [4] [Exploring the Underwater World Segmentation without Extra Training](https://arxiv.org/abs/2511.07923)
*Bingyu Li, Tao Huo, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡ç»†ç²’åº¦æ°´ä¸‹å¼€æ”¾è¯æ±‡åˆ†å‰²æ•°æ®é›†AquaOV255å’ŒåŸºå‡†UOVSBenchï¼Œå¹¶å¼€å‘äº†æ— éœ€è®­ç»ƒçš„åœ°çƒåˆ°æµ·æ´‹è¿ç§»æ¡†æ¶Earth2Oceanï¼Œé€šè¿‡å‡ ä½•å¼•å¯¼è§†è§‰æ©ç ç”Ÿæˆå™¨å’Œç±»åˆ«-è§†è§‰è¯­ä¹‰å¯¹é½æ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†æ°´ä¸‹å¼€æ”¾è¯æ±‡åˆ†å‰²æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åˆ†å‰²æ•°æ®é›†å’Œæ¨¡å‹ä¸»è¦å±€é™äºé™†åœ°åœºæ™¯ï¼Œç¼ºä¹é’ˆå¯¹æ°´ä¸‹ç¯å¢ƒçš„å¤§è§„æ¨¡ç»†ç²’åº¦åˆ†å‰²èµ„æºï¼Œè¿™é™åˆ¶äº†æµ·æ´‹ç”Ÿç‰©å¤šæ ·æ€§ç›‘æµ‹å’Œç”Ÿæ€è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œéœ€è¦æ„å»ºä¸“é—¨çš„æ°´ä¸‹å¼€æ”¾è¯æ±‡åˆ†å‰²åŸºå‡†å¹¶å¼€å‘æœ‰æ•ˆçš„è¿ç§»æ–¹æ³•ã€‚

**Method:** æå‡ºäº†Earth2Oceanæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå‡ ä½•å¼•å¯¼è§†è§‰æ©ç ç”Ÿæˆå™¨é€šè¿‡è‡ªç›¸ä¼¼æ€§å‡ ä½•å…ˆéªŒä¼˜åŒ–è§†è§‰ç‰¹å¾ä»¥å¢å¼ºå±€éƒ¨ç»“æ„æ„ŸçŸ¥ï¼Œç±»åˆ«-è§†è§‰è¯­ä¹‰å¯¹é½æ¨¡å—åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¨ç†å’Œåœºæ™¯æ„ŸçŸ¥æ¨¡æ¿æ„å»ºæ¥å¢å¼ºæ–‡æœ¬åµŒå…¥ï¼Œå®ç°æ— éœ€é¢å¤–æ°´ä¸‹è®­ç»ƒçš„é™†åœ°è§†è§‰-è¯­è¨€æ¨¡å‹å‘æ°´ä¸‹é¢†åŸŸçš„è¿ç§»ã€‚

**Result:** åœ¨UOVSBenchåŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒEarth2Oceanåœ¨ä¿æŒé«˜æ•ˆæ¨ç†çš„åŒæ—¶å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯¥åŸºå‡†æ•´åˆäº†AquaOV255æ•°æ®é›†å’Œäº”ä¸ªé¢å¤–æ°´ä¸‹æ•°æ®é›†ï¼ŒåŒ…å«255ä¸ªç±»åˆ«å’Œè¶…è¿‡20,000å¼ å›¾åƒï¼Œä¸ºæ°´ä¸‹å¼€æ”¾è¯æ±‡åˆ†å‰²æä¾›äº†å…¨é¢çš„è¯„ä¼°å¹³å°ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è¿ç§»æ¡†æ¶ï¼Œé™†åœ°é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥æœ‰æ•ˆé€‚åº”æ°´ä¸‹åˆ†å‰²ä»»åŠ¡è€Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œä¸ºæµ·æ´‹è§†è§‰åˆ†æå¼€è¾Ÿäº†æ–°é€”å¾„ï¼ŒåŒæ—¶å»ºç«‹çš„æ•°æ®é›†å’ŒåŸºå‡†å°†ä¸ºæœªæ¥æ°´ä¸‹è®¡ç®—æœºè§†è§‰ç ”ç©¶æä¾›é‡è¦åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Accurate segmentation of marine organisms is vital for biodiversity monitoring and ecological assessment, yet existing datasets and models remain largely limited to terrestrial scenes. To bridge this gap, we introduce \textbf{AquaOV255}, the first large-scale and fine-grained underwater segmentation dataset containing 255 categories and over 20K images, covering diverse categories for open-vocabulary (OV) evaluation. Furthermore, we establish the first underwater OV segmentation benchmark, \textbf{UOVSBench}, by integrating AquaOV255 with five additional underwater datasets to enable comprehensive evaluation. Alongside, we present \textbf{Earth2Ocean}, a training-free OV segmentation framework that transfers terrestrial vision--language models (VLMs) to underwater domains without any additional underwater training. Earth2Ocean consists of two core components: a Geometric-guided Visual Mask Generator (\textbf{GMG}) that refines visual features via self-similarity geometric priors for local structure perception, and a Category-visual Semantic Alignment (\textbf{CSA}) module that enhances text embeddings through multimodal large language model reasoning and scene-aware template construction. Extensive experiments on the UOVSBench benchmark demonstrate that Earth2Ocean achieves significant performance improvement on average while maintaining efficient inference.


### [5] [Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification](https://arxiv.org/abs/2511.07929)
*Yihang Wu, Ahmad Chaddad*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºCLIPçš„è”é‚¦å­¦ä¹ æ–¹æ³•FedMedCLIPï¼Œé€šè¿‡æ©ç ç‰¹å¾é€‚é…æ¨¡å—å’Œæœ¬åœ°åˆ†ç±»å™¨è®¾è®¡ï¼Œåœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å®ç°äº†é«˜æ€§èƒ½å’Œä½èµ„æºæ¶ˆè€—çš„è”é‚¦å­¦ä¹ æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å½±åƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¼ ç»Ÿè®­ç»ƒéœ€è¦æºæ•°æ®ï¼Œå­˜åœ¨éšç§æ³„éœ²é£é™©ã€‚è”é‚¦å­¦ä¹ è™½æä¾›åˆ†æ•£å¼è§£å†³æ–¹æ¡ˆï¼Œä½†æ•°æ®å¼‚æ„æ€§å’Œèµ„æºæˆæœ¬é™åˆ¶äº†å…¶éƒ¨ç½²ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ—¶ã€‚

**Method:** æå‡ºFedMedCLIPæ¡†æ¶ï¼Œé‡‡ç”¨æ©ç ç‰¹å¾é€‚é…æ¨¡å—ä½œä¸ºé€šä¿¡æ¨¡å—å‡å°‘é€šä¿¡è´Ÿè½½ï¼Œå†»ç»“CLIPç¼–ç å™¨é™ä½è®¡ç®—å¼€é”€ã€‚è®¾è®¡æ©ç MLPä½œä¸ºæœ¬åœ°åˆ†ç±»å™¨é€‚åº”å®¢æˆ·ç«¯ä»»åŠ¡ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”KLæ•£åº¦è’¸é¦æ­£åˆ™åŒ–æ–¹æ³•ä¿ƒè¿›æ¨¡å—é—´ç›¸äº’å­¦ä¹ ã€‚

**Result:** åœ¨å››ä¸ªå…¬å¼€åŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ISIC2019æ•°æ®é›†ä¸Šæ¯”æ¬¡ä¼˜åŸºçº¿æ€§èƒ½æå‡8%ï¼ŒåŒæ—¶èµ„æºæ•ˆç‡æ˜¾è‘—æå‡ï¼Œè®­ç»ƒé€Ÿåº¦æ¯”FedAVGå¿«120å€ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºCLIPçš„è”é‚¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„å¯è¡Œæ€§ï¼Œé€šè¿‡æ¨¡å—åŒ–è®¾è®¡å’Œæ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…é™ä½äº†é€šä¿¡å’Œè®¡ç®—æˆæœ¬ï¼Œä¸ºéšç§ä¿æŠ¤çš„åŒ»ç–—AIåº”ç”¨æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Despite the remarkable performance of deep models in medical imaging, they still require source data for training, which limits their potential in light of privacy concerns. Federated learning (FL), as a decentralized learning framework that trains a shared model with multiple hospitals (a.k.a., FL clients), provides a feasible solution. However, data heterogeneity and resource costs hinder the deployment of FL models, especially when using vision language models (VLM). To address these challenges, we propose a novel contrastive language-image pre-training (CLIP) based FL approach for medical image classification (FedMedCLIP). Specifically, we introduce a masked feature adaptation module (FAM) as a communication module to reduce the communication load while freezing the CLIP encoders to reduce the computational overhead. Furthermore, we propose a masked multi-layer perceptron (MLP) as a private local classifier to adapt to the client tasks. Moreover, we design an adaptive Kullback-Leibler (KL) divergence-based distillation regularization method to enable mutual learning between FAM and MLP. Finally, we incorporate model compression to transmit the FAM parameters while using ensemble predictions for classification. Extensive experiments on four publicly available medical datasets demonstrate that our model provides feasible performance (e.g., 8\% higher compared to second best baseline on ISIC2019) with reasonable resource cost (e.g., 120$\times$ faster than FedAVG).


### [6] [Laytrol: Preserving Pretrained Knowledge in Layout Control for Multimodal Diffusion Transformers](https://arxiv.org/abs/2511.07934)
*Sida Huang, Siqi Huang, Ping Luo, Hongyuan Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºLayout Control (Laytrol)ç½‘ç»œå’ŒLayout Synthesis (LaySyn)æ•°æ®é›†ï¼Œé€šè¿‡ç»§æ‰¿é¢„è®­ç»ƒå‚æ•°å’Œä¸“ç”¨åˆå§‹åŒ–æ–¹æ¡ˆè§£å†³æ‰©æ•£æ¨¡å‹ä¸­å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡çš„è§†è§‰è´¨é‡ä¸‹é™å’Œé£æ ¼ä¸ä¸€è‡´é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•é€šå¸¸é€šè¿‡é€‚é…å™¨æ¨¡å—å¼•å…¥å¸ƒå±€æ¡ä»¶ï¼Œä½†ç”Ÿæˆçš„å›¾åƒå¾€å¾€è§†è§‰è´¨é‡è¾ƒä½ä¸”ä¸åŸºç¡€æ¨¡å‹é£æ ¼ä¸ä¸€è‡´ï¼Œè¡¨æ˜é¢„è®­ç»ƒçŸ¥è¯†å­˜åœ¨æŸå¤±ï¼Œéœ€è¦è§£å†³åˆ†å¸ƒåç§»é—®é¢˜ã€‚

**Method:** æå‡ºLayout Control (Laytrol)ç½‘ç»œï¼Œå…¶å‚æ•°ä»MM-DiTç»§æ‰¿ä»¥ä¿ç•™åŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒçŸ¥è¯†ï¼›é‡‡ç”¨ä¸“ç”¨åˆå§‹åŒ–æ–¹æ¡ˆï¼Œå°†å¸ƒå±€ç¼–ç å™¨åˆå§‹åŒ–ä¸ºçº¯æ–‡æœ¬ç¼–ç å™¨ï¼Œå¸ƒå±€æ§åˆ¶ç½‘ç»œè¾“å‡ºåˆå§‹åŒ–ä¸ºé›¶ï¼›åº”ç”¨å¯¹è±¡çº§æ—‹è½¬ä½ç½®åµŒå…¥ä¸ºå¸ƒå±€ä»¤ç‰Œæä¾›ç²—ç•¥ä½ç½®ä¿¡æ¯ã€‚

**Result:** å®šæ€§å’Œå®šé‡å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç”Ÿæˆçš„å›¾åƒåœ¨è§†è§‰è´¨é‡å’Œç©ºé—´ä¸€è‡´æ€§æ–¹é¢å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** é€šè¿‡ç»§æ‰¿é¢„è®­ç»ƒå‚æ•°å’Œç²¾å¿ƒè®¾è®¡çš„åˆå§‹åŒ–æ–¹æ¡ˆï¼Œè¯¥æ–¹æ³•æˆåŠŸç¼“è§£äº†åˆ†å¸ƒåç§»é—®é¢˜ï¼Œä¸ºæ‰©æ•£æ¨¡å‹çš„ç©ºé—´å¯æ§æ€§å¢å¼ºæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨ä¿æŒé¢„è®­ç»ƒçŸ¥è¯†çš„åŒæ—¶å®ç°ç²¾ç¡®å¸ƒå±€æ§åˆ¶çš„å¯èƒ½æ€§ã€‚

---

#### ğŸ“„ Abstract
With the development of diffusion models, enhancing spatial controllability in text-to-image generation has become a vital challenge. As a representative task for addressing this challenge, layout-to-image generation aims to generate images that are spatially consistent with the given layout condition. Existing layout-to-image methods typically introduce the layout condition by integrating adapter modules into the base generative model. However, the generated images often exhibit low visual quality and stylistic inconsistency with the base model, indicating a loss of pretrained knowledge. To alleviate this issue, we construct the Layout Synthesis (LaySyn) dataset, which leverages images synthesized by the base model itself to mitigate the distribution shift from the pretraining data. Moreover, we propose the Layout Control (Laytrol) Network, in which parameters are inherited from MM-DiT to preserve the pretrained knowledge of the base model. To effectively activate the copied parameters and avoid disturbance from unstable control conditions, we adopt a dedicated initialization scheme for Laytrol. In this scheme, the layout encoder is initialized as a pure text encoder to ensure that its output tokens remain within the data domain of MM-DiT. Meanwhile, the outputs of the layout control network are initialized to zero. In addition, we apply Object-level Rotary Position Embedding to the layout tokens to provide coarse positional information. Qualitative and quantitative experiments demonstrate the effectiveness of our method.


### [7] [Libra-MIL: Multimodal Prototypes Stereoscopic Infused with Task-specific Language Priors for Few-shot Whole Slide Image Classification](https://arxiv.org/abs/2511.07941)
*Zhenfeng Zhuang, Fangyu Zhou, Liansheng Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€åŸå‹å¤šç¤ºä¾‹å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºä»»åŠ¡ç‰¹å¼‚æ€§ç—…ç†å®ä½“åŸå‹å’ŒåŒå‘äº¤äº’æœºåˆ¶ï¼Œè§£å†³äº†è®¡ç®—ç—…ç†å­¦ä¸­å…¨åˆ‡ç‰‡å›¾åƒå»ºæ¨¡çš„è®¡ç®—æŒ‘æˆ˜å’Œæ ‡ç­¾ç¨€ç–æ€§é—®é¢˜ï¼Œåœ¨å¤šä¸ªç™Œç—‡æ•°æ®é›†ä¸Šå±•ç°äº†ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è®¡ç®—ç—…ç†å­¦ä¸­å…¨åˆ‡ç‰‡å›¾åƒçš„é«˜è®¡ç®—æˆæœ¬éœ€è¦ä½¿ç”¨å¤šç¤ºä¾‹å­¦ä¹ è¿›è¡Œå»ºæ¨¡ï¼Œä½†ç—…ç†ä»»åŠ¡é€šå¸¸ä»…æä¾›åŒ…çº§æ ‡ç­¾ï¼Œè€Œç”±å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å®ä¾‹çº§æè¿°ç”±äºç¼ºä¹ç»†ç²’åº¦åŒ»å­¦çŸ¥è¯†å¾€å¾€å­˜åœ¨åå·®ã€‚ç°æœ‰è§†è§‰è¯­è¨€å¤šç¤ºä¾‹å­¦ä¹ æ–¹æ³•é€šå¸¸é‡‡ç”¨å•å‘æŒ‡å¯¼ï¼Œé™åˆ¶äº†è·¨æ¨¡æ€ååŒæ•ˆåº”ã€‚

**Method:** æå‡ºå¤šæ¨¡æ€åŸå‹å¤šç¤ºä¾‹å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨å†»ç»“çš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆä»»åŠ¡ç‰¹å¼‚æ€§ç—…ç†å®ä½“æè¿°ä½œä¸ºæ–‡æœ¬åŸå‹ï¼ŒåŒæ—¶è§†è§‰åˆ†æ”¯å­¦ä¹ å®ä¾‹çº§åŸå‹ä»¥å‡å°‘å¯¹å†—ä½™æ•°æ®çš„ä¾èµ–ã€‚åœ¨èåˆé˜¶æ®µé‡‡ç”¨åŸºäºç›¸ä¼¼åº¦åº¦é‡çš„ç«‹ä½“æœ€ä¼˜ä¼ è¾“ç®—æ³•ï¼Œä¿ƒè¿›é«˜ç»´ç©ºé—´ä¸­çš„è¯­ä¹‰å¯¹é½ã€‚

**Result:** åœ¨ä¸‰ä¸ªä¸åŒçš„ç™Œç—‡æ•°æ®é›†ä¸Šè¿›è¡Œäº†å°‘æ ·æœ¬åˆ†ç±»å’Œå¯è§£é‡Šæ€§å®éªŒï¼Œç»“æœè¡¨æ˜æ‰€ææ–¹æ³•å…·æœ‰ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ç—…ç†å›¾åƒåˆ†æä»»åŠ¡ä¸­å±•ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚

**Conclusion:** æ„å»ºä»»åŠ¡ç‰¹å¼‚æ€§ç—…ç†å®ä½“åŸå‹å¯¹äºå­¦ä¹ å¯æ³›åŒ–ç‰¹å¾å’Œå¢å¼ºæ¨¡å‹å¯è§£é‡Šæ€§è‡³å…³é‡è¦ï¼ŒåŒå‘äº¤äº’æœºåˆ¶é€šè¿‡å¹³è¡¡ä¿¡æ¯å‹ç¼©æ–¹æ¡ˆæœ‰æ•ˆä¿ƒè¿›äº†è·¨æ¨¡æ€ååŒï¼Œä¸ºè®¡ç®—ç—…ç†å­¦ä¸­çš„å¤šæ¨¡æ€å­¦ä¹ æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
While Large Language Models (LLMs) are emerging as a promising direction in computational pathology, the substantial computational cost of giga-pixel Whole Slide Images (WSIs) necessitates the use of Multi-Instance Learning (MIL) to enable effective modeling. A key challenge is that pathological tasks typically provide only bag-level labels, while instance-level descriptions generated by LLMs often suffer from bias due to a lack of fine-grained medical knowledge. To address this, we propose that constructing task-specific pathological entity prototypes is crucial for learning generalizable features and enhancing model interpretability. Furthermore, existing vision-language MIL methods often employ unidirectional guidance, limiting cross-modal synergy. In this paper, we introduce a novel approach, Multimodal Prototype-based Multi-Instance Learning, that promotes bidirectional interaction through a balanced information compression scheme. Specifically, we leverage a frozen LLM to generate task-specific pathological entity descriptions, which are learned as text prototypes. Concurrently, the vision branch learns instance-level prototypes to mitigate the model's reliance on redundant data. For the fusion stage, we employ the Stereoscopic Optimal Transport (SOT) algorithm, which is based on a similarity metric, thereby facilitating broader semantic alignment in a higher-dimensional space. We conduct few-shot classification and explainability experiments on three distinct cancer datasets, and the results demonstrate the superior generalization capabilities of our proposed method.


### [8] [Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection](https://arxiv.org/abs/2511.07966)
*Shenao Zhao, Pengpeng Liang, Zhoufan Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMMAssistæ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€è¾…åŠ©æå‡åŸºäºLiDARçš„3Dæ— ç›‘ç£åŸŸè‡ªé€‚åº”æ£€æµ‹æ€§èƒ½ï¼Œåˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä½œä¸ºæ¡¥æ¢è¿›è¡Œè·¨åŸŸç‰¹å¾å¯¹é½ï¼Œåœ¨ä¸‰ä¸ªä¸»æµ3Dæ£€æµ‹æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡ç‚¹äº‘å’Œå›¾åƒé€šå¸¸åŒæ—¶é‡‡é›†ï¼Œä½†åœ¨3Dæ— ç›‘ç£åŸŸè‡ªé€‚åº”è®­ç»ƒä¸­å›¾åƒæ•°æ®çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œç°æœ‰åŸºäºå¸ˆç”Ÿæ¶æ„å’Œä¼ªæ ‡ç­¾çš„æ–¹æ³•å¾ˆå°‘åˆ©ç”¨å¤šæ¨¡æ€ä¿¡æ¯æ¥æå‡åŸŸè‡ªé€‚åº”æ€§èƒ½ã€‚

**Method:** æå‡ºMMAssistæ–¹æ³•ï¼Œé€šè¿‡å°†3Dè¾¹ç•Œæ¡†æŠ•å½±åˆ°å›¾åƒè·å–2Dæ¡†ï¼Œä½¿ç”¨é¢„è®­ç»ƒè§†è§‰éª¨å¹²æå–å›¾åƒç‰¹å¾ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æè¿°å¹¶é€šè¿‡æ–‡æœ¬ç¼–ç å™¨è·å–æ–‡æœ¬ç‰¹å¾ï¼Œåœ¨æºåŸŸå’Œç›®æ ‡åŸŸè®­ç»ƒä¸­é€šè¿‡ç‰¹å¾å¯¹é½å’ŒåŠ æƒèåˆå®ç°3Dç‰¹å¾ä¸å¤šæ¨¡æ€ç‰¹å¾çš„ååŒå­¦ä¹ ï¼ŒåŒæ—¶ç»“åˆ2Dæ£€æµ‹å™¨å¢å¼ºä¼ªæ ‡ç­¾è´¨é‡ã€‚

**Result:** åœ¨ä¸‰ä¸ªæµè¡Œçš„3Dç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šçš„ä¸‰ä¸ªåŸŸè‡ªé€‚åº”ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”æœ€å…ˆè¿›æ–¹æ³•å–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å¤šæ¨¡æ€è¾…åŠ©åœ¨3Dæ— ç›‘ç£åŸŸè‡ªé€‚åº”ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜å›¾åƒå’Œæ–‡æœ¬ç­‰å¤šæ¨¡æ€ä¿¡æ¯å¯ä½œä¸ºæœ‰æ•ˆçš„æ¡¥æ¢ä¿ƒè¿›3Dç‰¹å¾çš„è·¨åŸŸå¯¹é½ï¼Œä¸º3Dæ— ç›‘ç£åŸŸè‡ªé€‚åº”æä¾›äº†æ–°çš„å¤šæ¨¡æ€èåˆèŒƒå¼ï¼Œæœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢æ›´ç²¾ç»†çš„å¤šæ¨¡æ€äº¤äº’æœºåˆ¶ã€‚

---

#### ğŸ“„ Abstract
Unsupervised domain adaptation for LiDAR-based 3D object detection (3D UDA) based on the teacher-student architecture with pseudo labels has achieved notable improvements in recent years. Although it is quite popular to collect point clouds and images simultaneously, little attention has been paid to the usefulness of image data in 3D UDA when training the models. In this paper, we propose an approach named MMAssist that improves the performance of 3D UDA with multi-modal assistance. A method is designed to align 3D features between the source domain and the target domain by using image and text features as bridges. More specifically, we project the ground truth labels or pseudo labels to the images to get a set of 2D bounding boxes. For each 2D box, we extract its image feature from a pre-trained vision backbone. A large vision-language model (LVLM) is adopted to extract the box's text description, and a pre-trained text encoder is used to obtain its text feature. During the training of the model in the source domain and the student model in the target domain, we align the 3D features of the predicted boxes with their corresponding image and text features, and the 3D features and the aligned features are fused with learned weights for the final prediction. The features between the student branch and the teacher branch in the target domain are aligned as well. To enhance the pseudo labels, we use an off-the-shelf 2D object detector to generate 2D bounding boxes from images and estimate their corresponding 3D boxes with the aid of point cloud, and these 3D boxes are combined with the pseudo labels generated by the teacher model. Experimental results show that our approach achieves promising performance compared with state-of-the-art methods in three domain adaptation tasks on three popular 3D object detection datasets. The code is available at https://github.com/liangp/MMAssist.


### [9] [Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection via LLMs](https://arxiv.org/abs/2511.07429)
*Hari Lee*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†åŸºäºæ–‡æœ¬çš„å¯è§£é‡Šè§†é¢‘å¼‚å¸¸æ£€æµ‹æ¡†æ¶TbVADï¼Œè¯¥æ¡†æ¶å®Œå…¨åœ¨æ–‡æœ¬åŸŸå†…æ‰§è¡Œå¼‚å¸¸æ£€æµ‹å’Œè§£é‡Šï¼Œé€šè¿‡è¯­è¨€è¡¨ç¤ºè§†é¢‘è¯­ä¹‰å®ç°å¯è§£é‡Šçš„çŸ¥è¯†é©±åŠ¨æ¨ç†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿå¼±ç›‘ç£è§†é¢‘å¼‚å¸¸æ£€æµ‹æ¨¡å‹ä¾èµ–æ˜¾å¼è§†è§‰ç‰¹å¾ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ï¼ŒTbVADæ—¨åœ¨é€šè¿‡è¯­è¨€é©±åŠ¨æ¡†æ¶è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ç°å¯è§£é‡Šä¸”åŸºäºçŸ¥è¯†çš„å¼‚å¸¸æ£€æµ‹æ¨ç†ã€‚

**Method:** TbVADé‡‡ç”¨ä¸‰é˜¶æ®µæ¡†æ¶ï¼šé¦–å…ˆä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å°†è§†é¢‘å†…å®¹è½¬æ¢ä¸ºç»†ç²’åº¦æè¿°ï¼Œç„¶åå°†æè¿°ç»„ç»‡ä¸ºå››ä¸ªè¯­ä¹‰æ§½ï¼ˆåŠ¨ä½œã€å¯¹è±¡ã€ä¸Šä¸‹æ–‡ã€ç¯å¢ƒï¼‰æ„å»ºç»“æ„åŒ–çŸ¥è¯†ï¼Œæœ€åç”Ÿæˆæ§½çº§è§£é‡Šä»¥æ­ç¤ºå“ªäº›è¯­ä¹‰å› ç´ å¯¹å¼‚å¸¸å†³ç­–è´¡çŒ®æœ€å¤§ã€‚

**Result:** åœ¨UCF-Crimeå’ŒXD-Violenceä¸¤ä¸ªå…¬å¼€åŸºå‡†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæ–‡æœ¬çŸ¥è¯†æ¨ç†ä¸ºçœŸå®ä¸–ç•Œç›‘æ§åœºæ™¯æä¾›äº†å¯è§£é‡Šä¸”å¯é çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜åŸºäºè¯­è¨€çš„è§†é¢‘è¡¨ç¤ºèƒ½å¤Ÿå®ç°å¯è§£é‡Šçš„å¼‚å¸¸æ£€æµ‹ï¼Œä¸ºç›‘æ§ç³»ç»Ÿæä¾›äº†çŸ¥è¯†é©±åŠ¨çš„æ¨ç†æ¡†æ¶ï¼Œå¼€è¾Ÿäº†æ–‡æœ¬é©±åŠ¨è§†é¢‘ç†è§£çš„æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
We introduce Text-based Explainable Video Anomaly Detection (TbVAD), a language-driven framework for weakly supervised video anomaly detection that performs anomaly detection and explanation entirely within the textual domain. Unlike conventional WSVAD models that rely on explicit visual features, TbVAD represents video semantics through language, enabling interpretable and knowledge-grounded reasoning. The framework operates in three stages: (1) transforming video content into fine-grained captions using a vision-language model, (2) constructing structured knowledge by organizing the captions into four semantic slots (action, object, context, environment), and (3) generating slot-wise explanations that reveal which semantic factors contribute most to the anomaly decision. We evaluate TbVAD on two public benchmarks, UCF-Crime and XD-Violence, demonstrating that textual knowledge reasoning provides interpretable and reliable anomaly detection for real-world surveillance scenarios.


### [10] [ChexFract: From General to Specialized - Enhancing Fracture Description Generation](https://arxiv.org/abs/2511.07983)
*Nikolay Nechaev, Evgeniia Przhezdzetskaia, Dmitry Umerenkov, Dmitry V. Dylov*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é’ˆå¯¹èƒ¸éƒ¨Xå…‰æŠ¥å‘Šä¸­ç½•è§éª¨æŠ˜ç—…ç†æè¿°ä¸è¶³çš„é—®é¢˜ï¼Œå¼€å‘äº†ä¸“é—¨çš„éª¨æŠ˜æ£€æµ‹ä¸æè¿°è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†éª¨æŠ˜æè¿°çš„å‡†ç¡®æ€§ï¼Œå¹¶å…¬å¼€äº†æœ€ä½³æ¨¡å‹ä»¥ä¿ƒè¿›ç½•è§ç—…ç†æŠ¥å‘Šç ”ç©¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆä¸­è™½ç„¶æ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æè¿°ç½•è§ä½†ä¸´åºŠé‡è¦çš„ç—…ç†ï¼ˆå¦‚éª¨æŠ˜ï¼‰æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„å¯é åº”ç”¨ã€‚

**Method:** åŸºäºMAIRA-2å’ŒCheXagentçš„ç¼–ç å™¨ï¼Œè®­ç»ƒäº†ä¸“é—¨é’ˆå¯¹éª¨æŠ˜ç—…ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¸“ä¸šåŒ–æ¶æ„è®¾è®¡æå‡å¯¹éª¨æŠ˜ç‰¹å¾çš„è¯†åˆ«å’Œæè¿°èƒ½åŠ›ã€‚

**Result:** ä¸“ä¸šéª¨æŠ˜æ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®éª¨æŠ˜æè¿°æ–¹é¢æ˜¾è‘—ä¼˜äºé€šç”¨æ¨¡å‹ï¼Œé€šè¿‡å¯¹éª¨æŠ˜ç±»å‹ã€ä½ç½®å’Œå¹´é¾„çš„åˆ†ææ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹æ¶æ„çš„ç‰¹å®šä¼˜åŠ¿å’Œå±€é™æ€§ã€‚

**Conclusion:** ä¸“ä¸šåŒ–æ¨¡å‹åœ¨ç½•è§ç—…ç†æŠ¥å‘Šç”Ÿæˆä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œå½“å‰æ¶æ„åœ¨ä¸åŒéª¨æŠ˜ç‰¹å¾æè¿°ä¸Šå­˜åœ¨å·®å¼‚ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ä»¥æå‡ä¸´åºŠå®ç”¨æ€§ï¼Œå…¬å¼€æ¨¡å‹å°†æ¨åŠ¨è¯¥é¢†åŸŸç ”ç©¶å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Generating accurate and clinically meaningful radiology reports from chest X-ray images remains a significant challenge in medical AI. While recent vision-language models achieve strong results in general radiology report generation, they often fail to adequately describe rare but clinically important pathologies like fractures. This work addresses this gap by developing specialized models for fracture pathology detection and description. We train fracture-specific vision-language models with encoders from MAIRA-2 and CheXagent, demonstrating significant improvements over general-purpose models in generating accurate fracture descriptions. Analysis of model outputs by fracture type, location, and age reveals distinct strengths and limitations of current vision-language model architectures. We publicly release our best-performing fracture-reporting model, facilitating future research in accurate reporting of rare pathologies.


### [11] [Multi-modal Deepfake Detection and Localization with FPN-Transformer](https://arxiv.org/abs/2511.08031)
*Chende Zheng, Ruiqi Suo, Zhoulin Ji, Jingyi Deng, Fangbin Yi, Chenhao Lin, Chao Shen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾é‡‘å­—å¡”å˜æ¢å™¨çš„å¤šæ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸å®šä½æ¡†æ¶ï¼Œé€šè¿‡è·¨æ¨¡æ€ç‰¹å¾èåˆå’Œæ—¶åºè¾¹ç•Œå›å½’ï¼Œåœ¨IJCAI'25 DDL-AVåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†0.7535çš„ä¼˜å¼‚æ€§èƒ½ï¼Œä¸ºé€šç”¨æ·±åº¦ä¼ªé€ æ£€æµ‹æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å•æ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•æ— æ³•æœ‰æ•ˆåˆ©ç”¨è·¨æ¨¡æ€å…³è”æ€§ï¼Œä¸”éš¾ä»¥ç²¾ç¡®å®šä½ä¼ªé€ ç‰‡æ®µï¼Œåœ¨é¢å¯¹å¤æ‚ç²¾ç»†çš„åˆæˆåª’ä½“æ—¶å­˜åœ¨æ˜æ˜¾å±€é™æ€§ï¼ŒäºŸéœ€å¼€å‘èƒ½å¤ŸåŒæ—¶å®ç°è·¨æ¨¡æ€æ³›åŒ–å’Œæ—¶åºå®šä½çš„æ£€æµ‹æ¡†æ¶ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨é¢„è®­ç»ƒè‡ªç›‘ç£æ¨¡å‹æå–å±‚æ¬¡åŒ–æ—¶åºç‰¹å¾ï¼Œé€šè¿‡å…·æœ‰å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶çš„R-TLMæ¨¡å—æ„å»ºå¤šå°ºåº¦ç‰¹å¾é‡‘å­—å¡”ï¼Œåˆ©ç”¨åŒåˆ†æ”¯é¢„æµ‹å¤´åŒæ—¶é¢„æµ‹ä¼ªé€ æ¦‚ç‡å’Œç²¾ç‚¼è¢«ç¯¡æ”¹ç‰‡æ®µçš„æ—¶åºåç§»é‡ï¼Œå®ç°å¸§çº§å®šä½ç²¾åº¦ã€‚

**Result:** åœ¨IJCAI'25 DDL-AVåŸºå‡†æµ‹è¯•é›†ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†0.7535çš„æœ€ç»ˆå¾—åˆ†ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶åœ¨æŒ‘æˆ˜æ€§ç¯å¢ƒä¸‹å¯¹è·¨æ¨¡æ€æ·±åº¦ä¼ªé€ æ£€æµ‹å’Œå®šä½å…·æœ‰æ˜¾è‘—æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯å®äº†å¤šæ¨¡æ€ç‰¹å¾èåˆå’Œæ—¶åºè¾¹ç•Œå›å½’åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„é‡è¦æ€§ï¼Œä¸ºé€šç”¨æ·±åº¦ä¼ªé€ æ£€æµ‹æä¾›äº†åˆ›æ–°è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†è·¨æ¨¡æ€åˆ†ææ–¹æ³•åœ¨å¤„ç†å¤æ‚åˆæˆåª’ä½“å¨èƒæ–¹é¢çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
The rapid advancement of generative adversarial networks (GANs) and diffusion models has enabled the creation of highly realistic deepfake content, posing significant threats to digital trust across audio-visual domains. While unimodal detection methods have shown progress in identifying synthetic media, their inability to leverage cross-modal correlations and precisely localize forged segments limits their practicality against sophisticated, fine-grained manipulations. To address this, we introduce a multi-modal deepfake detection and localization framework based on a Feature Pyramid-Transformer (FPN-Transformer), addressing critical gaps in cross-modal generalization and temporal boundary regression. The proposed approach utilizes pre-trained self-supervised models (WavLM for audio, CLIP for video) to extract hierarchical temporal features. A multi-scale feature pyramid is constructed through R-TLM blocks with localized attention mechanisms, enabling joint analysis of cross-context temporal dependencies. The dual-branch prediction head simultaneously predicts forgery probabilities and refines temporal offsets of manipulated segments, achieving frame-level localization precision. We evaluate our approach on the test set of the IJCAI'25 DDL-AV benchmark, showing a good performance with a final score of 0.7535 for cross-modal deepfake detection and localization in challenging environments. Experimental results confirm the effectiveness of our approach and provide a novel way for generalized deepfake detection. Our code is available at https://github.com/Zig-HS/MM-DDL


### [12] [WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation](https://arxiv.org/abs/2511.08036)
*Gongshu Wang, Zhirui Wang, Kan Yang*

#### ğŸ§© TL;DR
WEDepthæå‡ºäº†ä¸€ç§æ— éœ€ä¿®æ”¹è§†è§‰åŸºç¡€æ¨¡å‹ç»“æ„å’Œé¢„è®­ç»ƒæƒé‡çš„å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•ï¼Œé€šè¿‡å°†VFMä½œä¸ºå¤šçº§ç‰¹å¾å¢å¼ºå™¨æ¥æœ‰æ•ˆæ¿€å‘å’Œåˆ©ç”¨å…¶å†…åœ¨å…ˆéªŒçŸ¥è¯†ï¼Œåœ¨NYU-Depth v2å’ŒKITTIæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å•ç›®æ·±åº¦ä¼°è®¡ç”±äºä»å•å¼ 2Då›¾åƒé‡å»º3Dåœºæ™¯çš„å›ºæœ‰ä¸é€‚å®šæ€§è€Œæå…·æŒ‘æˆ˜æ€§ï¼Œç°ä»£è§†è§‰åŸºç¡€æ¨¡å‹åœ¨å¤§è§„æ¨¡å¤šæ ·åŒ–æ•°æ®é›†ä¸Šé¢„è®­ç»ƒåå±•ç°å‡ºå“è¶Šçš„ä¸–ç•Œç†è§£èƒ½åŠ›ï¼Œä½†å¦‚ä½•åœ¨ä¸ä¿®æ”¹æ¨¡å‹ç»“æ„å’Œé¢„è®­ç»ƒæƒé‡çš„æƒ…å†µä¸‹æœ‰æ•ˆåˆ©ç”¨è¿™äº›å…ˆéªŒçŸ¥è¯†è¿›è¡Œæ·±åº¦ä¼°è®¡ä»æ˜¯ä¸€ä¸ªå¼€æ”¾é—®é¢˜ã€‚

**Method:** è¯¥æ–¹æ³•å°†è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºå¤šçº§ç‰¹å¾å¢å¼ºå™¨ï¼Œåœ¨ä¸åŒè¡¨ç¤ºå±‚æ¬¡ä¸Šç³»ç»Ÿæ€§åœ°æ³¨å…¥å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡è¿™ç§ç»“æ„ä¿æŒçš„æ–¹å¼æœ‰æ•ˆæ¿€å‘VFMçš„å†…åœ¨å…ˆéªŒï¼ŒåŒæ—¶é¿å…äº†æ¨¡å‹ç»“æ„å’Œé¢„è®­ç»ƒæƒé‡çš„ä¿®æ”¹ã€‚

**Result:** åœ¨NYU-Depth v2å’ŒKITTIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒWEDepthå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œç›¸æ¯”éœ€è¦å¤šæ¬¡å‰å‘ä¼ æ’­çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•å’Œåœ¨ç›¸å¯¹æ·±åº¦ä¸Šé¢„è®­ç»ƒçš„æ–¹æ³•éƒ½å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœï¼ŒåŒæ—¶å±•ç°å‡ºè·¨å¤šæ ·åŒ–åœºæ™¯çš„å¼ºå¤§é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜è§†è§‰åŸºç¡€æ¨¡å‹çš„å†…åœ¨å…ˆéªŒçŸ¥è¯†å¯ä»¥æœ‰æ•ˆè¿ç§»åˆ°å•ç›®æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸­ï¼Œæ— éœ€æ¨¡å‹ç»“æ„ä¿®æ”¹å³å¯å®ç°å“è¶Šæ€§èƒ½ï¼Œä¸ºé›¶æ ·æœ¬æ·±åº¦ä¼°è®¡å’Œè·¨åŸŸè¿ç§»å­¦ä¹ æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå±•ç¤ºäº†é¢„è®­ç»ƒè§†è§‰æ¨¡å‹åœ¨å‡ ä½•ç†è§£ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Monocular depth estimation (MDE) has widely applicable but remains highly challenging due to the inherently ill-posed nature of reconstructing 3D scenes from single 2D images. Modern Vision Foundation Models (VFMs), pre-trained on large-scale diverse datasets, exhibit remarkable world understanding capabilities that benefit for various vision tasks. Recent studies have demonstrated significant improvements in MDE through fine-tuning these VFMs. Inspired by these developments, we propose WEDepth, a novel approach that adapts VFMs for MDE without modi-fying their structures and pretrained weights, while effec-tively eliciting and leveraging their inherent priors. Our method employs the VFM as a multi-level feature en-hancer, systematically injecting prior knowledge at differ-ent representation levels. Experiments on NYU-Depth v2 and KITTI datasets show that WEDepth establishes new state-of-the-art (SOTA) performance, achieving competi-tive results compared to both diffusion-based approaches (which require multiple forward passes) and methods pre-trained on relative depth. Furthermore, we demonstrate our method exhibits strong zero-shot transfer capability across diverse scenarios.


### [13] [Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance](https://arxiv.org/abs/2511.07499)
*Kwanyoung Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†å¯¹æŠ—æ€§Sinkhornæ³¨æ„åŠ›å¼•å¯¼ï¼ˆASAGï¼‰ï¼Œä¸€ç§åŸºäºæœ€ä¼˜ä¼ è¾“ç†è®ºçš„æ–°é¢–æ‰©æ•£æ¨¡å‹å¼•å¯¼æ–¹æ³•ï¼Œé€šè¿‡æ•…æ„ç ´åè‡ªæ³¨æ„åŠ›å±‚ä¸­çš„ä¼ è¾“æˆæœ¬æ¥æå‡ç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å±•ç°å‡ºç¨³å®šæ”¹è¿›ï¼Œå¹¶å¢å¼ºäº†IP-Adapterå’ŒControlNetç­‰ä¸‹æ¸¸åº”ç”¨çš„å¯æ§æ€§å’Œä¿çœŸåº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ‰©æ•£æ¨¡å‹å¼•å¯¼æ–¹æ³•å¦‚æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰é€šå¸¸é€šè¿‡å¯å‘å¼æ‰°åŠ¨å‡½æ•°æ•…æ„åŠ£åŒ–æ— æ¡ä»¶è¾“å‡ºæ¥æå‡ç›®æ ‡è¾“å‡ºè´¨é‡ï¼Œä½†è¿™äº›æ–¹æ³•ç¼ºä¹ç†è®ºåŸºç¡€ä¸”ä¾èµ–äººå·¥è®¾è®¡çš„å¤±çœŸç­–ç•¥ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºæ³¨æ„åŠ›æœºåˆ¶æä¾›æ›´åŸåˆ™æ€§çš„å¼•å¯¼æ¡†æ¶ï¼Œè§£å†³ç°æœ‰æ–¹æ³•åœ¨ç†è®ºåŸºç¡€å’Œä¼˜åŒ–æ•ˆç‡æ–¹é¢çš„å±€é™æ€§ã€‚

**Method:** ASAGæ–¹æ³•ä»æœ€ä¼˜ä¼ è¾“è§’åº¦é‡æ–°è§£é‡Šæ‰©æ•£æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œé€šè¿‡Sinkhornç®—æ³•åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­æ³¨å…¥å¯¹æŠ—æ€§æˆæœ¬æ¥é™ä½æŸ¥è¯¢å’Œé”®ä¹‹é—´çš„åƒç´ çº§ç›¸ä¼¼æ€§ã€‚è¿™ç§æœ‰æ„çš„åŠ£åŒ–ç­–ç•¥å‰Šå¼±äº†è¯¯å¯¼æ€§çš„æ³¨æ„åŠ›å¯¹é½ï¼Œä»è€Œæå‡æ¡ä»¶ç”Ÿæˆå’Œæ— æ¡ä»¶ç”Ÿæˆçš„æ ·æœ¬è´¨é‡ã€‚è¯¥æ–¹æ³•å…·æœ‰è½»é‡çº§ã€å³æ’å³ç”¨çš„ç‰¹ç‚¹ï¼Œæ— éœ€æ¨¡å‹é‡æ–°è®­ç»ƒã€‚

**Result:** ASAGåœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ä»»åŠ¡ä¸­å±•ç°å‡ºç¨³å®šçš„è´¨é‡æ”¹è¿›ï¼ŒåŒæ—¶æ˜¾è‘—æå‡äº†IP-Adapterå’ŒControlNetç­‰ä¸‹æ¸¸åº”ç”¨çš„å¯æ§æ€§å’Œä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ”¹å–„ç”Ÿæˆæ ·æœ¬çš„å¯é æ€§å’Œè´¨é‡ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** ASAGä¸ºæ‰©æ•£æ¨¡å‹å¼•å¯¼æä¾›äº†åŸºäºæœ€ä¼˜ä¼ è¾“çš„ç†è®ºåŸºç¡€ï¼Œå±•ç¤ºäº†é€šè¿‡æ•…æ„ç ´åæ³¨æ„åŠ›ä¼ è¾“æˆæœ¬æ¥æå‡ç”Ÿæˆè´¨é‡çš„å¯è¡Œæ€§ã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†ç”Ÿæˆå¯é æ€§ï¼Œè¿˜ä¸ºæœªæ¥æ‰©æ•£æ¨¡å‹ä¼˜åŒ–å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³¨æ„åŠ›æœºåˆ¶çš„ç†è®ºè§£é‡Šå’Œä¼˜åŒ–ç­–ç•¥æ–¹é¢å…·æœ‰é‡è¦å¯ç¤ºæ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.


### [14] [CLIP is All You Need for Human-like Semantic Representations in Stable Diffusion](https://arxiv.org/abs/2511.08075)
*Cameron Braunstein, Mariya Toneva, Eddy Ilg*

#### ğŸ§© TL;DR
æœ¬æ–‡ç ”ç©¶å‘ç°ï¼ŒStable Diffusionç­‰æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„è¯­ä¹‰ç†è§£ä¸»è¦æ¥æºäºCLIPæ–‡æœ¬ç¼–ç å™¨ï¼Œè€Œéåå‘æ‰©æ•£è¿‡ç¨‹ï¼Œæ‰©æ•£è¿‡ç¨‹ä¸»è¦æ‰¿æ‹…è§†è§‰è§£ç å™¨çš„è§’è‰²ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œä½†è¿™äº›æ¨¡å‹å¯¹æ‰€ç”Ÿæˆå›¾åƒçš„è¯­ä¹‰ç†è§£ç¨‹åº¦å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹å†…éƒ¨è¡¨ç¤ºæ˜¯å¦åŒ…å«å¯¹äººç±»æœ‰æ„ä¹‰çš„è¯­ä¹‰ä¿¡æ¯è¿™ä¸€å…³é”®é—®é¢˜äºŸå¾…æ¢ç´¢ã€‚

**Method:** é€šè¿‡åœ¨Stable Diffusionä¸Šæ‰§è¡Œæ¢æµ‹åˆ†æï¼Œä½¿ç”¨ç®€å•çš„å›å½’å±‚é¢„æµ‹å¯¹è±¡è¯­ä¹‰å±æ€§ï¼Œå¹¶å°†è¿™äº›é¢„æµ‹ä¸äººç±»æ ‡æ³¨è¿›è¡Œæ¯”è¾ƒï¼Œé‡ç‚¹æ¯”è¾ƒCLIPæ–‡æœ¬ç¼–ç ä¸åå‘æ‰©æ•£è¿‡ç¨‹åœ¨è¯­ä¹‰è¡¨ç¤ºä¸­çš„ç›¸å¯¹è´¡çŒ®ã€‚

**Result:** ç ”ç©¶å‘ç°è¯­ä¹‰ç†è§£çš„æˆåŠŸä¸»è¦å½’å› äºCLIPçš„æ–‡æœ¬ç¼–ç è€Œéåå‘æ‰©æ•£è¿‡ç¨‹ï¼Œä¸åŒè¯­ä¹‰å±æ€§çš„è§£ç å‡†ç¡®ç‡å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”åœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­å±æ€§é—´çš„åŒºåˆ†åº¦é€æ¸é™ä½ï¼Œè¡¨æ˜CLIPå…·æœ‰æœ€å¼ºçš„è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ç‹¬ç«‹è®­ç»ƒçš„CLIPè§†è§‰è¯­è¨€æ¨¡å‹å†³å®šäº†ç±»äººçš„è¯­ä¹‰è¡¨ç¤ºï¼Œè€Œæ‰©æ•£è¿‡ç¨‹ä¸»è¦æ‰¿æ‹…è§†è§‰è§£ç å™¨çš„åŠŸèƒ½ï¼Œè¿™å¯¹ç†è§£æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„è¯­ä¹‰èƒ½åŠ›åˆ†å¸ƒå…·æœ‰é‡è¦æ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
Latent diffusion models such as Stable Diffusion achieve state-of-the-art results on text-to-image generation tasks. However, the extent to which these models have a semantic understanding of the images they generate is not well understood. In this work, we investigate whether the internal representations used by these models during text-to-image generation contain semantic information that is meaningful to humans. To do so, we perform probing on Stable Diffusion with simple regression layers that predict semantic attributes for objects and evaluate these predictions against human annotations. Surprisingly, we find that this success can actually be attributed to the text encoding occurring in CLIP rather than the reverse diffusion process. We demonstrate that groups of specific semantic attributes have markedly different decoding accuracy than the average, and are thus represented to different degrees. Finally, we show that attributes become more difficult to disambiguate from one another during the inverse diffusion process, further demonstrating the strongest semantic representation of object attributes in CLIP. We conclude that the separately trained CLIP vision-language model is what determines the human-like semantic representation, and that the diffusion process instead takes the role of a visual decoder.


### [15] [OTSNet: A Neurocognitive-Inspired Observation-Thinking-Spelling Pipeline for Scene Text Recognition](https://arxiv.org/abs/2511.08133)
*Lixu Sun, Nurmemet Yolwas, Wushour Silamu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºOTSNetï¼Œä¸€ç§å—ç¥ç»è®¤çŸ¥å¯å‘çš„ä¸‰é˜¶æ®µç½‘ç»œï¼Œé€šè¿‡è§‚å¯Ÿ-æ€è€ƒ-æ‹¼å†™æµç¨‹å®ç°ç»Ÿä¸€çš„åœºæ™¯æ–‡æœ¬è¯†åˆ«ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­åˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½è®°å½•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åœºæ™¯æ–‡æœ¬è¯†åˆ«æ¡†æ¶ä¸­çš„è§£è€¦è§†è§‰-è¯­è¨€ä¼˜åŒ–ä¼šé€šè¿‡è·¨æ¨¡æ€é”™ä½æ”¾å¤§é”™è¯¯ä¼ æ’­ï¼Œè§†è§‰ç¼–ç å™¨å¯¹èƒŒæ™¯å¹²æ‰°ç‰©å­˜åœ¨æ³¨æ„åŠ›åå·®ï¼Œè€Œè§£ç å™¨åœ¨è§£æå‡ ä½•å˜å½¢æ–‡æœ¬æ—¶é­å—ç©ºé—´é”™ä½ï¼Œè¿™äº›å› ç´ å…±åŒé™ä½äº†ä¸è§„åˆ™æ¨¡å¼ä¸‹çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚

**Method:** OTSNeté‡‡ç”¨ä¸‰é˜¶æ®µæ¶æ„ï¼ŒåŒ…æ‹¬åŒæ³¨æ„åŠ›é©¬å¡é¾™ç¼–ç å™¨é€šè¿‡å·®åˆ†æ³¨æ„åŠ›å›¾ç»†åŒ–è§†è§‰ç‰¹å¾ï¼Œä½ç½®æ„ŸçŸ¥æ¨¡å—å’Œè¯­ä¹‰é‡åŒ–å™¨é€šè¿‡è‡ªé€‚åº”é‡‡æ ·æ•´åˆç©ºé—´ä¸Šä¸‹æ–‡ä¸å­—å½¢çº§è¯­ä¹‰æŠ½è±¡ï¼Œä»¥åŠå¤šæ¨¡æ€åä½œéªŒè¯å™¨é€šè¿‡è§†è§‰ã€è¯­ä¹‰å’Œå­—ç¬¦çº§ç‰¹å¾çš„è·¨æ¨¡æ€èåˆå®ç°è‡ªæ ¡æ­£ã€‚

**Result:** åœ¨æŒ‘æˆ˜æ€§çš„Union14M-LåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°83.5%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œåœ¨ä¸¥é‡é®æŒ¡çš„OSTæ•°æ®é›†ä¸Šè¾¾åˆ°79.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨14ä¸ªè¯„ä¼°åœºæ™¯ä¸­çš„9ä¸ªåœºæ™¯ä¸Šåˆ›é€ äº†æ–°çš„è®°å½•ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç¥ç»è®¤çŸ¥å¯å‘çš„åˆ†å±‚å¤„ç†æµç¨‹åœ¨åœºæ™¯æ–‡æœ¬è¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ç»Ÿä¸€çš„è·¨æ¨¡æ€å¯¹é½å’Œè‡ªæ ¡æ­£æœºåˆ¶æ˜¾è‘—æå‡äº†å¤æ‚åœºæ™¯ä¸‹çš„è¯†åˆ«é²æ£’æ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€èåˆç ”ç©¶æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Scene Text Recognition (STR) remains challenging due to real-world complexities, where decoupled visual-linguistic optimization in existing frameworks amplifies error propagation through cross-modal misalignment. Visual encoders exhibit attention bias toward background distractors, while decoders suffer from spatial misalignment when parsing geometrically deformed text-collectively degrading recognition accuracy for irregular patterns. Inspired by the hierarchical cognitive processes in human visual perception, we propose OTSNet, a novel three-stage network embodying a neurocognitive-inspired Observation-Thinking-Spelling pipeline for unified STR modeling. The architecture comprises three core components: (1) a Dual Attention Macaron Encoder (DAME) that refines visual features through differential attention maps to suppress irrelevant regions and enhance discriminative focus; (2) a Position-Aware Module (PAM) and Semantic Quantizer (SQ) that jointly integrate spatial context with glyph-level semantic abstraction via adaptive sampling; and (3) a Multi-Modal Collaborative Verifier (MMCV) that enforces self-correction through cross-modal fusion of visual, semantic, and character-level features. Extensive experiments demonstrate that OTSNet achieves state-of-the-art performance, attaining 83.5% average accuracy on the challenging Union14M-L benchmark and 79.1% on the heavily occluded OST dataset-establishing new records across 9 out of 14 evaluation scenarios.


### [16] [PEOD: A Pixel-Aligned Event-RGB Benchmark for Object Detection under Challenging Conditions](https://arxiv.org/abs/2511.08140)
*Luoping Cui, Hanqing Liu, Mingjie Liu, Endian Lin, Donghong Jiang, Yuhao Wang, Chuang Zhu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PEODï¼Œé¦–ä¸ªå¤§è§„æ¨¡ã€åƒç´ å¯¹é½çš„é«˜åˆ†è¾¨ç‡äº‹ä»¶-RGBæ•°æ®é›†ï¼Œç”¨äºæŒ‘æˆ˜æ¡ä»¶ä¸‹çš„ç›®æ ‡æ£€æµ‹ï¼ŒåŒ…å«130+æ—¶ç©ºå¯¹é½åºåˆ—å’Œ34ä¸‡æ‰‹åŠ¨æ ‡æ³¨æ¡†ï¼Œå…¶ä¸­57%æ•°æ®åœ¨ä½å…‰ç…§ã€è¿‡æ›å’Œé«˜é€Ÿè¿åŠ¨æ¡ä»¶ä¸‹é‡‡é›†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰äº‹ä»¶-RGBæ•°æ®é›†åœ¨æç«¯æ¡ä»¶è¦†ç›–ç¨€ç–ä¸”ç©ºé—´åˆ†è¾¨ç‡ä½ï¼ˆâ‰¤640Ã—480ï¼‰ï¼Œæ— æ³•å…¨é¢è¯„ä¼°æŒ‘æˆ˜åœºæ™¯ä¸‹çš„æ£€æµ‹å™¨æ€§èƒ½ï¼Œè¿™é™åˆ¶äº†é²æ£’ç›®æ ‡æ£€æµ‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å‘å±•ã€‚

**Method:** æ„å»ºäº†PEODæ•°æ®é›†ï¼ŒåŒ…å«130å¤šä¸ªæ—¶ç©ºå¯¹é½åºåˆ—å’Œ340,000ä¸ªæ‰‹åŠ¨æ ‡æ³¨è¾¹ç•Œæ¡†ï¼Œ57%æ•°æ®åœ¨ä½å…‰ç…§ã€è¿‡æ›å’Œé«˜é€Ÿè¿åŠ¨æ¡ä»¶ä¸‹é‡‡é›†ï¼Œå¹¶å¯¹14ç§æ–¹æ³•åœ¨ä¸‰ç§è¾“å…¥é…ç½®ï¼ˆäº‹ä»¶ã€RGBå’Œäº‹ä»¶-RGBèåˆï¼‰ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

**Result:** åœ¨å®Œæ•´æµ‹è¯•é›†å’Œæ­£å¸¸å­é›†ä¸Šï¼Œèåˆæ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼›åœ¨å…‰ç…§æŒ‘æˆ˜å­é›†ä¸­ï¼Œé¡¶çº§äº‹ä»¶æ¨¡å‹ä¼˜äºæ‰€æœ‰èåˆæ¨¡å‹ï¼Œè€Œèåˆæ¨¡å‹ä»ä¼˜äºRGBæ¨¡å‹ï¼Œè¡¨æ˜å½“å¸§æ¨¡æ€ä¸¥é‡é€€åŒ–æ—¶ç°æœ‰èåˆæ–¹æ³•å­˜åœ¨å±€é™ã€‚

**Conclusion:** PEODä¸ºå¤šæ¨¡æ€æ„ŸçŸ¥å»ºç«‹äº†çœŸå®ã€é«˜è´¨é‡çš„åŸºå‡†ï¼Œæ­ç¤ºäº†ç°æœ‰èåˆæ–¹æ³•åœ¨å¸§æ¨¡æ€ä¸¥é‡é€€åŒ–æ—¶çš„å±€é™æ€§ï¼Œä¿ƒè¿›äº†æœªæ¥åœ¨æŒ‘æˆ˜æ¡ä»¶ä¸‹é²æ£’ç›®æ ‡æ£€æµ‹çš„ç ”ç©¶ã€‚

---

#### ğŸ“„ Abstract
Robust object detection for challenging scenarios increasingly relies on event cameras, yet existing Event-RGB datasets remain constrained by sparse coverage of extreme conditions and low spatial resolution (<= 640 x 480), which prevents comprehensive evaluation of detectors under challenging scenarios. To address these limitations, we propose PEOD, the first large-scale, pixel-aligned and high-resolution (1280 x 720) Event-RGB dataset for object detection under challenge conditions. PEOD contains 130+ spatiotemporal-aligned sequences and 340k manual bounding boxes, with 57% of data captured under low-light, overexposure, and high-speed motion. Furthermore, we benchmark 14 methods across three input configurations (Event-based, RGB-based, and Event-RGB fusion) on PEOD. On the full test set and normal subset, fusion-based models achieve the excellent performance. However, in illumination challenge subset, the top event-based model outperforms all fusion models, while fusion models still outperform their RGB-based counterparts, indicating limits of existing fusion methods when the frame modality is severely degraded. PEOD establishes a realistic, high-quality benchmark for multimodal perception and facilitates future research.


### [17] [Boomda: Balanced Multi-objective Optimization for Multimodal Domain Adaptation](https://arxiv.org/abs/2511.08152)
*Jun Sun, Xinxin Zhang, Simin Hong, Jian Zhu, Xiang Gao*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºBoomdaæ–¹æ³•ï¼Œé€šè¿‡å¤šç›®æ ‡ä¼˜åŒ–å®ç°å¼‚æ„å¤šæ¨¡æ€é¢†åŸŸè‡ªé€‚åº”ï¼Œæœ‰æ•ˆå¹³è¡¡ä¸åŒæ¨¡æ€é—´çš„é¢†åŸŸåç§»ï¼Œåœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„å¤šæ¨¡æ€åœºæ™¯ä¸­å®ç°é«˜æ•ˆé¢†åŸŸé€‚åº”ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å­¦ä¹ é¢ä¸´æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œè€Œç°æœ‰çš„æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”æ–¹æ³•ä¸»è¦é’ˆå¯¹å•æ¨¡æ€åœºæ™¯ï¼Œåœ¨å¤šæ¨¡æ€è®¾ç½®ä¸­ç ”ç©¶è¾ƒå°‘ï¼Œç‰¹åˆ«æ˜¯å½“ä¸åŒæ¨¡æ€åœ¨æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´å­˜åœ¨ä¸åŒç¨‹åº¦çš„é¢†åŸŸåç§»æ—¶ï¼Œéœ€è¦è§£å†³å¼‚æ„å¤šæ¨¡æ€é¢†åŸŸè‡ªé€‚åº”é—®é¢˜ã€‚

**Method:** é¦–å…ˆå¼•å…¥ä¿¡æ¯ç“¶é¢ˆæ–¹æ³•ç‹¬ç«‹å­¦ä¹ æ¯ä¸ªæ¨¡æ€çš„è¡¨ç¤ºï¼Œç„¶åé€šè¿‡ç›¸å…³æ€§å¯¹é½åœ¨è¡¨ç¤ºç©ºé—´ä¸­å¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸï¼Œå°†é—®é¢˜å»ºæ¨¡ä¸ºå¤šç›®æ ‡ä¼˜åŒ–ä»»åŠ¡ä»¥è·å¾—å¸•ç´¯æ‰˜æœ€ä¼˜è§£ï¼Œé€šè¿‡æ¨¡å‹ç‰¹å®šæ€§è´¨å°†é—®é¢˜ç®€åŒ–ä¸ºäºŒæ¬¡è§„åˆ’é—®é¢˜å¹¶æ¨å¯¼å‡ºé—­å¼è§£ï¼Œæœ€ç»ˆå½¢æˆé«˜æ•ˆçš„æ¨¡æ€å¹³è¡¡å¤šæ¨¡æ€é¢†åŸŸè‡ªé€‚åº”ç®—æ³•Boomdaã€‚

**Result:** å¤§é‡å®è¯ç»“æœè¡¨æ˜æ‰€ææ–¹æ³•å…·æœ‰æ˜¾è‘—æœ‰æ•ˆæ€§ï¼ŒBoomdaåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç«äº‰æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€é¢†åŸŸè‡ªé€‚åº”ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€é¢†åŸŸè‡ªé€‚åº”æä¾›äº†æœ‰æ•ˆçš„æ¨¡æ€å¹³è¡¡è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å¤šç›®æ ‡ä¼˜åŒ–æ¡†æ¶æˆåŠŸå¤„ç†äº†ä¸åŒæ¨¡æ€é—´çš„å¼‚æ„é¢†åŸŸåç§»é—®é¢˜ï¼Œä¸ºç¼ºä¹æ ‡æ³¨æ•°æ®çš„å¤šæ¨¡æ€å­¦ä¹ åœºæ™¯å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Multimodal learning, while contributing to numerous success stories across various fields, faces the challenge of prohibitively expensive manual annotation. To address the scarcity of annotated data, a popular solution is unsupervised domain adaptation, which has been extensively studied in unimodal settings yet remains less explored in multimodal settings. In this paper, we investigate heterogeneous multimodal domain adaptation, where the primary challenge is the varying domain shifts of different modalities from the source to the target domain. We first introduce the information bottleneck method to learn representations for each modality independently, and then match the source and target domains in the representation space with correlation alignment. To balance the domain alignment of all modalities, we formulate the problem as a multi-objective task, aiming for a Pareto optimal solution. By exploiting the properties specific to our model, the problem can be simplified to a quadratic programming problem. Further approximation yields a closed-form solution, leading to an efficient modality-balanced multimodal domain adaptation algorithm. The proposed method features \textbf{B}alanced multi-\textbf{o}bjective \textbf{o}ptimization for \textbf{m}ultimodal \textbf{d}omain \textbf{a}daptation, termed \textbf{Boomda}. Extensive empirical results showcase the effectiveness of the proposed approach and demonstrate that Boomda outperforms the competing schemes. The code is is available at: https://github.com/sunjunaimer/Boomda.git.


### [18] [Non-Aligned Reference Image Quality Assessment for Novel View Synthesis](https://arxiv.org/abs/2511.08155)
*Abhijay Ghildyal, Rajesh Sureddi, Nabajeet Barman, Saman Zadtootaghaj, Alan Bovik*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–°è§†è§’åˆæˆå›¾åƒçš„éå¯¹é½å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œåˆæˆå¤±çœŸè®­ç»ƒï¼Œåœ¨ä¸å¯¹é½å‚è€ƒåœºæ™¯ä¸‹å®ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ–°è§†è§’åˆæˆå›¾åƒçš„æ„ŸçŸ¥è´¨é‡è¯„ä¼°é¢ä¸´å…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹åƒç´ å¯¹é½çš„çœŸå®å‚è€ƒå›¾åƒæ—¶ï¼Œå…¨å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•åœ¨ä¸å¯¹é½æƒ…å†µä¸‹å¤±æ•ˆï¼Œè€Œæ— å‚è€ƒæ–¹æ³•åˆ™å­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚

**Method:** æ„å»ºäº†åŒ…å«é’ˆå¯¹æ—¶é—´æ„Ÿå…´è¶£åŒºåŸŸåˆæˆå¤±çœŸçš„å¤§è§„æ¨¡å›¾åƒæ•°æ®é›†ï¼Œé‡‡ç”¨åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ¡†æ¶ï¼Œç»“åˆLoRAå¢å¼ºçš„DINOv2åµŒå…¥è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨ç°æœ‰IQAæ–¹æ³•è¿›è¡Œç›‘ç£è®­ç»ƒï¼Œä¸“é—¨åœ¨åˆæˆå¤±çœŸæ•°æ®ä¸Šè®­ç»ƒä»¥é¿å…å¯¹ç‰¹å®šçœŸå®NVSæ ·æœ¬çš„è¿‡æ‹Ÿåˆã€‚

**Result:** æ‰€ææ¨¡å‹åœ¨ä¸å¯¹é½å‚è€ƒåœºæ™¯ä¸‹è¶…è¶Šäº†æœ€å…ˆè¿›çš„å…¨å‚è€ƒã€æ— å‚è€ƒå’Œéå¯¹é½å‚è€ƒIQAæ–¹æ³•ï¼Œåœ¨å¯¹é½å’Œä¸å¯¹é½å‚è€ƒæƒ…å†µä¸‹å‡è¡¨ç°å‡ºé²æ£’æ€§èƒ½ï¼Œä¸”ä¸æ”¶é›†çš„ä¸»è§‚è¯„åˆ†å…·æœ‰å¼ºç›¸å…³æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åœ¨åˆæˆå¤±çœŸæ•°æ®ä¸Šè®­ç»ƒçš„éå¯¹é½å‚è€ƒIQAæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºNVSè´¨é‡è¯„ä¼°æä¾›äº†æ–°èŒƒå¼ï¼Œå¹¶é€šè¿‡ç”¨æˆ·ç ”ç©¶éªŒè¯äº†æ¨¡å‹é¢„æµ‹ä¸äººç±»åå¥½çš„å¼ºç›¸å…³æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
Evaluating the perceptual quality of Novel View Synthesis (NVS) images remains a key challenge, particularly in the absence of pixel-aligned ground truth references. Full-Reference Image Quality Assessment (FR-IQA) methods fail under misalignment, while No-Reference (NR-IQA) methods struggle with generalization. In this work, we introduce a Non-Aligned Reference (NAR-IQA) framework tailored for NVS, where it is assumed that the reference view shares partial scene content but lacks pixel-level alignment. We constructed a large-scale image dataset containing synthetic distortions targeting Temporal Regions of Interest (TROI) to train our NAR-IQA model. Our model is built on a contrastive learning framework that incorporates LoRA-enhanced DINOv2 embeddings and is guided by supervision from existing IQA methods. We train exclusively on synthetically generated distortions, deliberately avoiding overfitting to specific real NVS samples and thereby enhancing the model's generalization capability. Our model outperforms state-of-the-art FR-IQA, NR-IQA, and NAR-IQA methods, achieving robust performance on both aligned and non-aligned references. We also conducted a novel user study to gather data on human preferences when viewing non-aligned references in NVS. We find strong correlation between our proposed quality prediction model and the collected subjective ratings. For dataset and code, please visit our project page: https://stootaghaj.github.io/nova-project/


### [19] [LandSegmenter: Towards a Flexible Foundation Model for Land Use and Land Cover Mapping](https://arxiv.org/abs/2511.08156)
*Chenying Liu, Wei Huang, Xiao Xiang Zhu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºLandSegmenterï¼Œä¸€ç§ç”¨äºåœŸåœ°åˆ©ç”¨åœŸåœ°è¦†ç›–ï¼ˆLULCï¼‰æ˜ å°„çš„ä¸“ç”¨åŸºç¡€æ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡å¼±ç›‘ç£å­¦ä¹ è§£å†³é¥æ„Ÿé¢†åŸŸæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œåœ¨é›¶æ ·æœ¬å’Œè¿ç§»å­¦ä¹ åœºæ™¯ä¸‹å®ç°äº†ä¼˜è¶Šæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰LULCæ¨¡å‹é€šå¸¸é’ˆå¯¹ç‰¹å®šæ¨¡æ€å’Œå›ºå®šç±»åˆ«åˆ†ç±»æ³•å¼€å‘ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›å’Œå¹¿æ³›åº”ç”¨æ€§ã€‚ä»»åŠ¡æ— å…³åŸºç¡€æ¨¡å‹éœ€è¦å¾®è°ƒï¼Œè€Œä»»åŠ¡ä¸“ç”¨åŸºç¡€æ¨¡å‹ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œè¿™åœ¨é¥æ„Ÿé¢†åŸŸæˆæœ¬é«˜æ˜‚ä¸”ä¸åˆ‡å®é™…ã€‚

**Method:** æå‡ºLandSegmenterä¸‰é˜¶æ®µæ¡†æ¶ï¼šè¾“å…¥å±‚é¢æ„å»ºLASå¤§è§„æ¨¡å¤šæ¨¡æ€å¤šæºæ•°æ®é›†ï¼Œä½¿ç”¨ç°æœ‰LULCäº§å“çš„å¼±æ ‡ç­¾ï¼›æ¨¡å‹å±‚é¢é›†æˆé¥æ„Ÿä¸“ç”¨é€‚é…å™¨è¿›è¡Œè·¨æ¨¡æ€ç‰¹å¾æå–å’Œæ–‡æœ¬ç¼–ç å™¨å¢å¼ºè¯­ä¹‰æ„ŸçŸ¥ï¼›è¾“å‡ºå±‚é¢é‡‡ç”¨ç±»åˆ«ç½®ä¿¡åº¦å¼•å¯¼èåˆç­–ç•¥ç¼“è§£è¯­ä¹‰é—æ¼ã€‚

**Result:** åœ¨å…­ä¸ªç²¾ç¡®æ ‡æ³¨çš„LULCæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¹¿æ³›çš„è¿ç§»å­¦ä¹ å’Œé›¶æ ·æœ¬å®éªŒè¡¨æ˜ï¼ŒLandSegmenterå®ç°äº†ç«äº‰æ€§æˆ–ä¼˜è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è½¬ç§»åˆ°æœªè§æ•°æ®é›†æ—¶çš„é›¶æ ·æœ¬è®¾ç½®ä¸­è¡¨ç°çªå‡ºã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ä»¥åŠå¼±ç›‘ç£åœ¨æ„å»ºä»»åŠ¡ä¸“ç”¨åŸºç¡€æ¨¡å‹ä¸­çš„å®ç”¨æ€§ï¼Œä¸ºé¥æ„Ÿé¢†åŸŸçš„é€šç”¨æ¨¡å‹å¼€å‘æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œæ˜¾è‘—é™ä½äº†æ•°æ®æ ‡æ³¨æˆæœ¬å¹¶æå‡äº†æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
Land Use and Land Cover (LULC) mapping is a fundamental task in Earth Observation (EO). However, current LULC models are typically developed for a specific modality and a fixed class taxonomy, limiting their generability and broader applicability. Recent advances in foundation models (FMs) offer promising opportunities for building universal models. Yet, task-agnostic FMs often require fine-tuning for downstream applications, whereas task-specific FMs rely on massive amounts of labeled data for training, which is costly and impractical in the remote sensing (RS) domain. To address these challenges, we propose LandSegmenter, an LULC FM framework that resolves three-stage challenges at the input, model, and output levels. From the input side, to alleviate the heavy demand on labeled data for FM training, we introduce LAnd Segment (LAS), a large-scale, multi-modal, multi-source dataset built primarily with globally sampled weak labels from existing LULC products. LAS provides a scalable, cost-effective alternative to manual annotation, enabling large-scale FM training across diverse LULC domains. For model architecture, LandSegmenter integrates an RS-specific adapter for cross-modal feature extraction and a text encoder for semantic awareness enhancement. At the output stage, we introduce a class-wise confidence-guided fusion strategy to mitigate semantic omissions and further improve LandSegmenter's zero-shot performance. We evaluate LandSegmenter on six precisely annotated LULC datasets spanning diverse modalities and class taxonomies. Extensive transfer learning and zero-shot experiments demonstrate that LandSegmenter achieves competitive or superior performance, particularly in zero-shot settings when transferred to unseen datasets. These results highlight the efficacy of our proposed framework and the utility of weak supervision for building task-specific FMs.


### [20] [Multi-Granularity Mutual Refinement Network for Zero-Shot Learning](https://arxiv.org/abs/2511.08163)
*Ning Wang, Long Yu, Cong Hua, Guangming Zhu, Lin Mei, Syed Afaq Ali Shah, Mohammed Bennamoun, Liang Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šç²’åº¦ç›¸äº’ç²¾ç‚¼ç½‘ç»œï¼ˆMg-MRNï¼‰ï¼Œé€šè¿‡è§£è€¦å¤šç²’åº¦ç‰¹å¾å­¦ä¹ å’Œè·¨ç²’åº¦ç‰¹å¾äº¤äº’æ¥ç²¾ç‚¼åˆ¤åˆ«æ€§å’Œå¯è¿ç§»çš„è§†è§‰ç‰¹å¾ï¼Œä»¥è§£å†³é›¶æ ·æœ¬å­¦ä¹ ä¸­å±€éƒ¨åŒºåŸŸç‰¹å¾é—´å†…åœ¨äº¤äº’è¢«å¿½è§†çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰é›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•é€šå¸¸å°†å…¨å±€è§†è§‰ç‰¹å¾ä¸è¯­ä¹‰ä¿¡æ¯å…³è”æˆ–å°†å±€éƒ¨è§†è§‰åŒºåŸŸç‰¹å¾ä¸å¯¹åº”å±æ€§å¯¹é½ï¼Œä½†å¾€å¾€å¿½è§†äº†å±€éƒ¨åŒºåŸŸç‰¹å¾ä¹‹é—´çš„å†…åœ¨äº¤äº’ä½œç”¨ï¼Œè¿™äº›äº¤äº’å¯ä»¥è¿›ä¸€æ­¥æé«˜å¯è¿ç§»å’Œæ˜¾å¼è§†è§‰ç‰¹å¾çš„è·å–èƒ½åŠ›ã€‚

**Method:** è®¾è®¡äº†å¤šç²’åº¦ç‰¹å¾æå–æ¨¡å—é€šè¿‡è§£è€¦åŒºåŸŸç‰¹å¾æŒ–æ˜å­¦ä¹ åŒºåŸŸçº§åˆ¤åˆ«ç‰¹å¾ï¼Œå¹¶æ„å»ºè·¨ç²’åº¦ç‰¹å¾èåˆæ¨¡å—æ¥åŠ å¼ºä¸åŒç²’åº¦åŒºåŸŸç‰¹å¾ä¹‹é—´çš„å†…åœ¨äº¤äº’ï¼Œé€šè¿‡æ•´åˆç›¸é‚»å±‚æ¬¡ç»“æ„çš„åŒºåŸŸè¡¨ç¤ºæ¥å¢å¼ºæ¯ä¸ªç²’åº¦çº§åˆ«çš„è¡¨ç¤ºåˆ¤åˆ«èƒ½åŠ›ã€‚

**Result:** åœ¨ä¸‰ä¸ªæµè¡Œçš„é›¶æ ·æœ¬å­¦ä¹ åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†æ‰€æå‡ºçš„Mg-MRNæ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œç«äº‰åŠ›ï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬è¯†åˆ«æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†å±€éƒ¨åŒºåŸŸç‰¹å¾é—´äº¤äº’åœ¨é›¶æ ·æœ¬å­¦ä¹ ä¸­çš„é‡è¦æ€§ï¼Œæå‡ºçš„å¤šç²’åº¦ç›¸äº’ç²¾ç‚¼æ¡†æ¶ä¸ºå­¦ä¹ æ›´å…·åˆ¤åˆ«æ€§å’Œå¯è¿ç§»æ€§çš„è§†è§‰ç‰¹å¾æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œå¯¹æå‡é›¶æ ·æœ¬è¯†åˆ«æ€§èƒ½å…·æœ‰é‡è¦ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Zero-shot learning (ZSL) aims to recognize unseen classes with zero samples by transferring semantic knowledge from seen classes. Current approaches typically correlate global visual features with semantic information (i.e., attributes) or align local visual region features with corresponding attributes to enhance visual-semantic interactions. Although effective, these methods often overlook the intrinsic interactions between local region features, which can further improve the acquisition of transferable and explicit visual features. In this paper, we propose a network named Multi-Granularity Mutual Refinement Network (Mg-MRN), which refine discriminative and transferable visual features by learning decoupled multi-granularity features and cross-granularity feature interactions. Specifically, we design a multi-granularity feature extraction module to learn region-level discriminative features through decoupled region feature mining. Then, a cross-granularity feature fusion module strengthens the inherent interactions between region features of varying granularities. This module enhances the discriminability of representations at each granularity level by integrating region representations from adjacent hierarchies, further improving ZSL recognition performance. Extensive experiments on three popular ZSL benchmark datasets demonstrate the superiority and competitiveness of our proposed Mg-MRN method. Our code is available at https://github.com/NingWang2049/Mg-MRN.


### [21] [Distributed Zero-Shot Learning for Visual Recognition](https://arxiv.org/abs/2511.08170)
*Zhi Chen, Yadan Luo, Zi Huang, Jingjing Li, Sen Wang, Xin Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†åˆ†å¸ƒå¼é›¶æ ·æœ¬å­¦ä¹ æ¡†æ¶DistZSLï¼Œé€šè¿‡è·¨èŠ‚ç‚¹å±æ€§æ­£åˆ™åŒ–å’Œå…¨å±€å±æ€§-è§†è§‰å…±è¯†æœºåˆ¶ï¼Œæœ‰æ•ˆåˆ©ç”¨åˆ†å¸ƒå¼æ•°æ®å­¦ä¹ æœªè§ç±»åˆ«çš„æ¨¡å‹ï¼Œåœ¨åˆ†å¸ƒå¼æ•°æ®å­¦ä¹ æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³åˆ†å¸ƒå¼ç¯å¢ƒä¸‹é›¶æ ·æœ¬å­¦ä¹ é¢ä¸´çš„æ•°æ®å¼‚æ„æ€§é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨åˆ†æ•£åœ¨ä¸åŒèŠ‚ç‚¹ä¸Šçš„æ•°æ®æ¥å­¦ä¹ æœ‰æ•ˆçš„æœªè§ç±»åˆ«æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®åˆ†å¸ƒä¸ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œè§†è§‰åˆ°å±æ€§çš„æ˜ å°„å…³ç³»å®¹æ˜“äº§ç”Ÿåå·®ã€‚

**Method:** æå‡ºäº†DistZSLæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šè·¨èŠ‚ç‚¹å±æ€§æ­£åˆ™åŒ–ç¡®ä¿ä¸åŒèŠ‚ç‚¹é—´å±æ€§ç‰¹å¾è·ç¦»çš„ç›¸ä¼¼æ€§ï¼Œç¨³å®šæ•´ä½“å±æ€§ç‰¹å¾ç©ºé—´ï¼›å…¨å±€å±æ€§-è§†è§‰å…±è¯†æœºåˆ¶é€šè¿‡å¼ºåˆ¶å±æ€§ä¸è§†è§‰ç‰¹å¾åˆ†å¸ƒçš„åŒå‘æ˜ å°„åœ¨ä¸åŒèŠ‚ç‚¹é—´ä¿æŒä¸€è‡´ï¼Œå‡è½»ä¸ªä½“èŠ‚ç‚¹å­¦ä¹ çš„V2Aæ˜ å°„åå·®ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDistZSLåœ¨åˆ†å¸ƒå¼æ•°æ®å­¦ä¹ æ–¹é¢å®ç°äº†ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è·¨ä¸åŒèŠ‚ç‚¹çš„é›¶æ ·æœ¬å­¦ä¹ æ•ˆæœï¼ŒéªŒè¯äº†æ‰€ææ¡†æ¶åœ¨å¤„ç†æ•°æ®å¼‚æ„æ€§é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡é€‚å½“çš„æ­£åˆ™åŒ–å’Œå…±è¯†æœºåˆ¶å¯ä»¥æœ‰æ•ˆè§£å†³åˆ†å¸ƒå¼é›¶æ ·æœ¬å­¦ä¹ ä¸­çš„æ•°æ®å¼‚æ„æ€§æŒ‘æˆ˜ï¼Œä¸ºåˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„çŸ¥è¯†è¿ç§»æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
In this paper, we propose a Distributed Zero-Shot Learning (DistZSL) framework that can fully exploit decentralized data to learn an effective model for unseen classes. Considering the data heterogeneity issues across distributed nodes, we introduce two key components to ensure the effective learning of DistZSL: a cross-node attribute regularizer and a global attribute-to-visual consensus. Our proposed cross-node attribute regularizer enforces the distances between attribute features to be similar across different nodes. In this manner, the overall attribute feature space would be stable during learning, and thus facilitate the establishment of visual-to-attribute(V2A) relationships. Then, we introduce the global attribute-tovisual consensus to mitigate biased V2A mappings learned from individual nodes. Specifically, we enforce the bilateral mapping between the attribute and visual feature distributions to be consistent across different nodes. Thus, the learned consistent V2A mapping can significantly enhance zero-shot learning across different nodes. Extensive experiments demonstrate that DistZSL achieves superior performance to the state-of-the-art in learning from distributed data.


### [22] [Remodeling Semantic Relationships in Vision-Language Fine-Tuning](https://arxiv.org/abs/2511.08238)
*Xiangyang Wu, Liu Liu, Baosheng Yu, Jiayan Qiu, Zhenwei Shi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰å’Œå…³ç³»å¢å¼ºçš„å¤šæ¨¡æ€å¯¹é½ä¸èåˆæ–¹æ³•ï¼Œé€šè¿‡æå–å¤šå±‚çº§è¯­ä¹‰ç‰¹å¾ã€å­¦ä¹ è¯­ä¹‰åˆ†ç»„ä»¥åŠä½¿ç”¨å¯ç»§æ‰¿çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æ‰€æœ‰æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰çš„è§†è§‰è¯­è¨€å¾®è°ƒæ–¹æ³•é€šå¸¸å¿½è§†äº†æ–‡æœ¬ä¸Šä¸‹æ–‡ä¸­å¼ºè°ƒçš„å›¾åƒå†…éƒ¨è¯­ä¹‰å…³ç³»ä¿¡æ¯ï¼Œå¯¼è‡´è§†è§‰ä¸è¯­è¨€å¯¹é½æ•ˆæœä¸ä½³ã€‚è¿™ç§å¯¹è¯­ä¹‰å…³ç³»çš„å¿½ç•¥é™åˆ¶äº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½æå‡ï¼Œéœ€è¦å¼€å‘èƒ½å¤ŸåŒæ—¶åˆ©ç”¨è¯­ä¹‰å’Œå…³ç³»ä¿¡æ¯çš„æ–°æ–¹æ³•æ¥è§£å†³è¿™ä¸€å±€é™æ€§ã€‚

**Method:** è¯¥æ–¹æ³•é¦–å…ˆä»ä¸åŒè§†è§‰ç¼–ç å™¨ä¸­æå–å¤šå±‚çº§è¯­ä¹‰ç‰¹å¾ä»¥æ•æ‰æ›´ä¸°å¯Œçš„è§†è§‰å…³ç³»çº¿ç´¢ï¼Œç„¶åå­¦ä¹ å°†è§†è§‰ç‰¹å¾æŠ•å½±åˆ°ç›¸å…³çš„è¯­ä¹‰åˆ†ç»„ä¸­ï¼Œæœ€åä½¿ç”¨å¯ç»§æ‰¿çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶èåˆè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼Œé€šè¿‡ä¸¢å¼ƒç›¸å…³æ€§ä½çš„è§†è§‰-è¯­è¨€ç‰¹å¾å¯¹æ¥å…¨å±€å»é™¤å†—ä½™çš„è§†è§‰å…³ç³»ã€‚

**Result:** åœ¨å…«ä¸ªåŸºç¡€æ¨¡å‹å’Œä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†æ‰€æœ‰ç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ¨¡æ€å¯¹é½å’Œèåˆæ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åŒæ—¶åˆ©ç”¨è¯­ä¹‰å’Œå…³ç³»ä¿¡æ¯å¯¹äºæå‡å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ï¼Œæå‡ºçš„æ–¹æ³•ä¸ºè§†è§‰è¯­è¨€å¯¹é½æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¹¶ä¸ºæ„å»ºæ›´å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£ç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Vision-language fine-tuning has emerged as an efficient paradigm for constructing multimodal foundation models. While textual context often highlights semantic relationships within an image, existing fine-tuning methods typically overlook this information when aligning vision and language, thus leading to suboptimal performance. Toward solving this problem, we propose a method that can improve multimodal alignment and fusion based on both semantics and relationships.Specifically, we first extract multilevel semantic features from different vision encoder to capture more visual cues of the relationships. Then, we learn to project the vision features to group related semantics, among which are more likely to have relationships. Finally, we fuse the visual features with the textual by using inheritable cross-attention, where we globally remove the redundant visual relationships by discarding visual-language feature pairs with low correlation. We evaluate our proposed method on eight foundation models and two downstream tasks, visual question answering and image captioning, and show that it outperforms all existing methods.


### [23] [VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion](https://arxiv.org/abs/2511.08173)
*Samet Hicsonmez, Abd El Rahman Shabayek, Djamila Aouada*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºVLMDiffï¼Œä¸€ç§æ–°é¢–çš„æ— ç›‘ç£å¤šç±»åˆ«è§†è§‰å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡é›†æˆæ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨VLMç”Ÿæˆçš„å›¾åƒæè¿°ä½œä¸ºé¢å¤–æ¡ä»¶æ¥å¢å¼ºå¼‚å¸¸å®šä½å’Œæ£€æµ‹èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨Real-IADå’ŒCOCO-ADæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ‰©æ•£åŸºæ–¹æ³•ï¼Œæ— éœ€äººå·¥æ ‡æ³¨æˆ–é€ç±»åˆ«æ¨¡å‹è®­ç»ƒå³å¯å®ç°å¤šç±»åˆ«å¼‚å¸¸æ£€æµ‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºæ‰©æ•£çš„è§†è§‰å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–åˆæˆå™ªå£°ç”Ÿæˆï¼Œè¿™é™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ä¸”éœ€è¦é€ç±»åˆ«æ¨¡å‹è®­ç»ƒï¼Œä¸¥é‡é˜»ç¢äº†æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚å¤šç±»åˆ«çœŸå®ä¸–ç•Œå›¾åƒä¸­çš„è§†è§‰å¼‚å¸¸æ£€æµ‹é¢ä¸´å¤šæ ·æ€§æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆçš„å¤šç±»åˆ«å¼‚å¸¸å®šä½å’Œæ£€æµ‹ã€‚

**Method:** VLMDiffæ¡†æ¶é›†æˆæ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨é¢„è®­ç»ƒVLMé€šè¿‡ç®€å•æç¤ºæå–è¯¦ç»†å›¾åƒæè¿°ä½œä¸ºLDMè®­ç»ƒçš„é¢å¤–æ¡ä»¶ã€‚è¯¥æ–¹æ³•æ— éœ€äººå·¥æ ‡æ³¨æˆ–é¢å¤–è®­ç»ƒå³å¯ä»VLMè·å–æ­£å¸¸å›¾åƒæè¿°ï¼Œè¿™äº›æè¿°æ¡ä»¶åŒ–æ‰©æ•£æ¨¡å‹ä»¥å­¦ä¹ é²æ£’çš„æ­£å¸¸å›¾åƒç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œå®ç°å¤šç±»åˆ«å¼‚å¸¸æ£€æµ‹ã€‚

**Result:** åœ¨Real-IADæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å°†åƒç´ çº§Per-Region-OverlapæŒ‡æ ‡æå‡äº†é«˜è¾¾25ä¸ªç‚¹ï¼Œåœ¨COCO-ADæ•°æ®é›†ä¸Šæå‡äº†8ä¸ªç‚¹ï¼Œæ˜¾è‘—è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šç±»åˆ«è§†è§‰å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­å…·æœ‰ç«äº‰ä¼˜åŠ¿ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†VLMä¸LDMçš„ååŒé›†æˆèƒ½å¤Ÿæœ‰æ•ˆè§£å†³å¤šç±»åˆ«è§†è§‰å¼‚å¸¸æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œæ— éœ€é€ç±»åˆ«è®­ç»ƒå³å¯å®ç°å¯æ‰©å±•çš„å¼‚å¸¸æ£€æµ‹ã€‚æ–¹æ³•å±•ç¤ºäº†åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹æä¾›è¯­ä¹‰æ¡ä»¶æ¥å¢å¼ºæ‰©æ•£æ¨¡å‹è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›çš„æ½œåŠ›ï¼Œä¸ºæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Detecting visual anomalies in diverse, multi-class real-world images is a significant challenge. We introduce \ours, a novel unsupervised multi-class visual anomaly detection framework. It integrates a Latent Diffusion Model (LDM) with a Vision-Language Model (VLM) for enhanced anomaly localization and detection. Specifically, a pre-trained VLM with a simple prompt extracts detailed image descriptions, serving as additional conditioning for LDM training. Current diffusion-based methods rely on synthetic noise generation, limiting their generalization and requiring per-class model training, which hinders scalability. \ours, however, leverages VLMs to obtain normal captions without manual annotations or additional training. These descriptions condition the diffusion model, learning a robust normal image feature representation for multi-class anomaly detection. Our method achieves competitive performance, improving the pixel-level Per-Region-Overlap (PRO) metric by up to 25 points on the Real-IAD dataset and 8 points on the COCO-AD dataset, outperforming state-of-the-art diffusion-based approaches. Code is available at https://github.com/giddyyupp/VLMDiff.


### [24] [NERVE: Neighbourhood & Entropy-guided Random-walk for training free open-Vocabulary sEgmentation](https://arxiv.org/abs/2511.08248)
*Kunal Mahatha, Jose Dolz, Christian Desrosiers*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†NERVEï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²å¼ºåŸºçº¿æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆå…¨å±€ä¸å±€éƒ¨ä¿¡æ¯ã€éšæœºæ¸¸èµ°ä¼˜åŒ–äº²å’Œåº¦ä»¥åŠåŸºäºç†µçš„ä¸ç¡®å®šæ€§é€‰æ‹©æœºåˆ¶ï¼Œåœ¨7ä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬åˆ†å‰²æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ— éœ€è®­ç»ƒçš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–¹æ³•å­˜åœ¨å¤šä¸ªå±€é™æ€§ï¼šè®¡ç®—æˆæœ¬é«˜æ˜‚çš„äº²å’Œåº¦ä¼˜åŒ–ç­–ç•¥ã€å› ç­‰æƒé‡æˆ–å›ºå®šå¤§å°é«˜æ–¯æ ¸å¯¼è‡´çš„Transformeræ³¨æ„åŠ›å›¾èåˆæ•ˆç‡ä½ä¸‹ï¼Œä»¥åŠå¼ºåˆ¶å„å‘åŒæ€§é‚»åŸŸçš„é—®é¢˜ã€‚

**Method:** NERVEæ–¹æ³•ç‹¬ç‰¹åœ°æ•´åˆäº†å…¨å±€å’Œç»†ç²’åº¦å±€éƒ¨ä¿¡æ¯ï¼Œåˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹è‡ªæ³¨æ„åŠ›å±‚çš„é‚»åŸŸç»“æ„ï¼Œå¼•å…¥éšæœºæ¸¸èµ°è¿›è¡Œäº²å’Œåº¦ä¼˜åŒ–è€Œéä¾èµ–å›ºå®šå¤§å°é«˜æ–¯æ ¸ï¼Œå¹¶ä½¿ç”¨åŸºäºç†µçš„ä¸ç¡®å®šæ€§æ¥é€‰æ‹©æœ€ç›¸å…³çš„æ³¨æ„åŠ›å›¾ï¼Œæ— éœ€ä¼ ç»Ÿåå¤„ç†æŠ€æœ¯ã€‚

**Result:** åœ¨7ä¸ªæµè¡Œçš„è¯­ä¹‰åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œå®éªŒï¼Œè¯¥æ–¹æ³•å®ç°äº†æ•´ä½“æœ€å…ˆè¿›çš„é›¶æ ·æœ¬åˆ†å‰²æ€§èƒ½ï¼Œä¸ºå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡æ•´åˆå…¨å±€å±€éƒ¨ä¿¡æ¯ã€éšæœºæ¸¸èµ°ä¼˜åŒ–å’Œä¸ç¡®å®šæ€§é€‰æ‹©æœºåˆ¶ï¼Œå¯ä»¥æ„å»ºé«˜æ•ˆçš„æ— éœ€è®­ç»ƒå¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•ï¼Œæœ‰æ•ˆå¤„ç†ä»»æ„å½¢çŠ¶ç‰©ä½“çš„åˆ†å‰²é—®é¢˜ï¼Œä¸ºç›¸å…³é¢†åŸŸæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Despite recent advances in Open-Vocabulary Semantic Segmentation (OVSS), existing training-free methods face several limitations: use of computationally expensive affinity refinement strategies, ineffective fusion of transformer attention maps due to equal weighting or reliance on fixed-size Gaussian kernels to reinforce local spatial smoothness, enforcing isotropic neighborhoods. We propose a strong baseline for training-free OVSS termed as NERVE (Neighbourhood \& Entropy-guided Random-walk for open-Vocabulary sEgmentation), which uniquely integrates global and fine-grained local information, exploiting the neighbourhood structure from the self-attention layer of a stable diffusion model. We also introduce a stochastic random walk for refining the affinity rather than relying on fixed-size Gaussian kernels for local context. This spatial diffusion process encourages propagation across connected and semantically related areas, enabling it to effectively delineate objects with arbitrary shapes. Whereas most existing approaches treat self-attention maps from different transformer heads or layers equally, our method uses entropy-based uncertainty to select the most relevant maps. Notably, our method does not require any conventional post-processing techniques like Conditional Random Fields (CRF) or Pixel-Adaptive Mask Refinement (PAMR). Experiments are performed on 7 popular semantic segmentation benchmarks, yielding an overall state-of-the-art zero-shot segmentation performance, providing an effective approach to open-vocabulary semantic segmentation.


### [25] [UI2Code$^\text{N}$: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation](https://arxiv.org/abs/2511.08195)
*Zhen Yang, Wenyi Hong, Mingde Xu, Xinyue Fan, Weihan Wang, Jiele Cheng, Xiaotao Gu, Jie Tang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†UI2Code$^\text{N}$ï¼Œä¸€ç§é€šè¿‡åˆ†é˜¶æ®µé¢„è®­ç»ƒã€å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†äº¤äº’å¼UIåˆ°ä»£ç ç”Ÿæˆçš„æ–°èŒƒå¼ï¼Œåœ¨UIåˆ°ä»£ç å’ŒUIä¼˜åŒ–åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°å¼€æºæ¨¡å‹çš„æ–°æœ€ä¼˜æ°´å¹³ï¼Œæ€§èƒ½ä¸é¢†å…ˆçš„é—­æºæ¨¡å‹ç›¸å½“ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰UIç¼–ç¨‹æ–¹æ³•é¢ä¸´ä¸¤ä¸ªå…³é”®é™åˆ¶ï¼šå¤šæ¨¡æ€ç¼–ç èƒ½åŠ›å‘å±•ä¸è¶³ï¼Œä»¥åŠå•è½®èŒƒå¼å¾ˆå°‘åˆ©ç”¨è¿­ä»£è§†è§‰åé¦ˆã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡äº¤äº’å¼UIåˆ°ä»£ç èŒƒå¼æ›´å¥½åœ°åæ˜ å®é™…å·¥ä½œæµç¨‹å¹¶æé«˜å¯å®ç°çš„æ€§èƒ½ä¸Šé™ã€‚

**Method:** é‡‡ç”¨åˆ†é˜¶æ®µé¢„è®­ç»ƒã€å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹UI2Code$^\text{N}$ï¼Œç»Ÿä¸€äº†ä¸‰ä¸ªå…³é”®èƒ½åŠ›ï¼šUIåˆ°ä»£ç ç”Ÿæˆã€UIç¼–è¾‘å’ŒUIä¼˜åŒ–ï¼Œå¹¶æ¢ç´¢äº†äº¤äº’å¼ç”Ÿæˆçš„æµ‹è¯•æ—¶æ‰©å±•ï¼Œå®ç°å¤šè½®åé¦ˆçš„ç³»ç»Ÿæ€§ä½¿ç”¨ã€‚

**Result:** åœ¨UIåˆ°ä»£ç å’ŒUIä¼˜åŒ–åŸºå‡†æµ‹è¯•ä¸­ï¼ŒUI2Code$^\text{N}$åœ¨å¼€æºæ¨¡å‹ä¸­å»ºç«‹äº†æ–°çš„æœ€ä¼˜æ°´å¹³ï¼Œæ€§èƒ½ä¸é¢†å…ˆçš„é—­æºæ¨¡å‹å¦‚Claude-4-Sonnetå’ŒGPT-5ç›¸å½“ã€‚

**Conclusion:** äº¤äº’å¼UIåˆ°ä»£ç èŒƒå¼èƒ½å¤Ÿæ˜¾è‘—æå‡å¤šæ¨¡æ€ç¼–ç æ€§èƒ½ï¼Œé€šè¿‡ç»Ÿä¸€ç”Ÿæˆã€ç¼–è¾‘å’Œä¼˜åŒ–èƒ½åŠ›ä»¥åŠå¤šè½®åé¦ˆæœºåˆ¶ï¼Œä¸ºå®é™…UIå¼€å‘å·¥ä½œæµç¨‹æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†æµ‹è¯•æ—¶æ‰©å±•åœ¨äº¤äº’å¼ç”Ÿæˆä¸­çš„ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code$^\text{N}$, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code$^\text{N}$ establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.


### [26] [ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation](https://arxiv.org/abs/2511.08263)
*Yue Min, Shaobo Wang, Jiaze Li, Tianle Niu, Junxin Fan, Yongliang Miao, Lijin Yang, Linfeng Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ImageBindDCï¼Œä¸€ç§åœ¨ImageBindç»Ÿä¸€ç‰¹å¾ç©ºé—´ä¸­æ“ä½œçš„æ–°å‹æ•°æ®å‹ç¼©æ¡†æ¶ï¼Œé€šè¿‡ç‰¹å¾å‡½æ•°æŸå¤±å®ç°ç²¾ç¡®çš„ç»Ÿè®¡å¯¹é½ï¼Œåœ¨è·¨æ¨¡æ€æ•°æ®å‹ç¼©æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›æ€§èƒ½ã€‚è¯¥æ¡†æ¶åœ¨NYU-v2æ•°æ®é›†ä¸Šä»…ä½¿ç”¨æ¯ç±»5ä¸ªå‹ç¼©æ•°æ®ç‚¹å³å¯å®ç°ä¸å®Œæ•´æ•°æ®é›†ç›¸å½“çš„æ— æŸæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿæ•°æ®å‹ç¼©æŠ€æœ¯åœ¨å•æ¨¡æ€åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­å¾€å¾€å¤±æ•ˆï¼Œå› ä¸ºéš¾ä»¥ä¿æŒå¤æ‚çš„æ¨¡æ€é—´ä¾èµ–å…³ç³»ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è·¨æ¨¡æ€æ•°æ®æ—¶æ— æ³•æœ‰æ•ˆæ•æ‰ä¸åŒæ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰å…³è”å’Œç»Ÿè®¡ç‰¹æ€§ã€‚

**Method:** ImageBindDCæ¡†æ¶åœ¨ImageBindç»Ÿä¸€ç‰¹å¾ç©ºé—´ä¸­æ“ä½œï¼Œé‡‡ç”¨ç‰¹å¾å‡½æ•°æŸå¤±åœ¨å‚…é‡Œå¶åŸŸå®ç°ç²¾ç¡®çš„æ— é™çŸ©åŒ¹é…ã€‚è¯¥æ–¹æ³•å¼ºåˆ¶æ‰§è¡Œä¸‰ä¸ªå…³é”®çº§åˆ«çš„åˆ†å¸ƒä¸€è‡´æ€§ï¼šå•æ¨¡æ€å¯¹é½åŒ¹é…å„æ¨¡æ€å†…çš„ç»Ÿè®¡ç‰¹æ€§ï¼Œè·¨æ¨¡æ€å¯¹é½é€šè¿‡æ··åˆçœŸå®-åˆæˆæ•°æ®å¯¹ä¿æŒæˆå¯¹è¯­ä¹‰ï¼Œè”åˆæ¨¡æ€å¯¹é½é€šè¿‡å¯¹é½çœŸå®æ•°æ®å¯¹ä¸åˆæˆå¯¹åº”ç‰©çš„è”åˆåˆ†å¸ƒæ¥æ•æ‰å®Œæ•´å¤šå…ƒæ•°æ®ç»“æ„ã€‚

**Result:** åœ¨NYU-v2æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨æ¯ç±»5ä¸ªå‹ç¼©æ•°æ®ç‚¹è®­ç»ƒçš„æ¨¡å‹å³å¯å®ç°ä¸å®Œæ•´æ•°æ®é›†ç›¸å½“çš„æ— æŸæ€§èƒ½ï¼Œç›¸æ¯”ä¹‹å‰æœ€ä½³æ–¹æ³•å–å¾—8.2%çš„ç»å¯¹æ”¹è¿›ï¼Œä¸”å‹ç¼©æ—¶é—´å‡å°‘è¶…è¿‡4å€ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚

**Conclusion:** ImageBindDCé€šè¿‡ç»Ÿä¸€ç‰¹å¾ç©ºé—´å’Œç‰¹å¾å‡½æ•°æŸå¤±æœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€æ•°æ®å‹ç¼©ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œè¯æ˜äº†åœ¨ä¿æŒè·¨æ¨¡æ€ä¾èµ–å…³ç³»çš„åŒæ—¶å®ç°é«˜æ•ˆæ•°æ®å‹ç¼©çš„å¯è¡Œæ€§ã€‚è¯¥æ–¹æ³•ä¸ºå¤šæ¨¡æ€æœºå™¨å­¦ä¹ ä¸­çš„é«˜æ•ˆè®­ç»ƒå¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Data condensation techniques aim to synthesize a compact dataset from a larger one to enable efficient model training, yet while successful in unimodal settings, they often fail in multimodal scenarios where preserving intricate inter-modal dependencies is crucial. To address this, we introduce ImageBindDC, a novel data condensation framework operating within the unified feature space of ImageBind. Our approach moves beyond conventional distribution-matching by employing a powerful Characteristic Function (CF) loss, which operates in the Fourier domain to facilitate a more precise statistical alignment via exact infinite moment matching. We design our objective to enforce three critical levels of distributional consistency: (i) uni-modal alignment, which matches the statistical properties of synthetic and real data within each modality; (ii) cross-modal alignment, which preserves pairwise semantics by matching the distributions of hybrid real-synthetic data pairs; and (iii) joint-modal alignment, which captures the complete multivariate data structure by aligning the joint distribution of real data pairs with their synthetic counterparts. Extensive experiments highlight the effectiveness of ImageBindDC: on the NYU-v2 dataset, a model trained on just 5 condensed datapoints per class achieves lossless performance comparable to one trained on the full dataset, achieving a new state-of-the-art with an 8.2\% absolute improvement over the previous best method and more than 4$\times$ less condensation time.


### [27] [Evaluating Gemini LLM in Food Image-Based Recipe and Nutrition Description with EfficientNet-B4 Visual Backbone](https://arxiv.org/abs/2511.08215)
*Rizal Khoirul Anam*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è§£è€¦çš„å¤šæ¨¡æ€é£Ÿç‰©è¯†åˆ«ç®¡é“ï¼Œé€šè¿‡ç³»ç»Ÿè¯„ä¼°è§†è§‰éª¨å¹²ç½‘ç»œä¸ç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹çš„ç»„åˆæ€§èƒ½ï¼Œæ­ç¤ºäº†è§†è§‰å‰ç«¯æ„ŸçŸ¥ç²¾åº¦å¯¹ç³»ç»Ÿæ•´ä½“æ•ˆç”¨çš„ç“¶é¢ˆé™åˆ¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ•°å­—é£Ÿå“åº”ç”¨çš„æ™®åŠéœ€è¦å¯é çš„è‡ªåŠ¨åŒ–è¥å…»åˆ†æå’Œçƒ¹é¥ªæŒ‡å¯¼æ–¹æ³•ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å…¬å¼€æ•°æ®é›†ä¸­å­˜åœ¨çš„æ–‡åŒ–åè§é—®é¢˜ï¼Œå¹¶è¯„ä¼°è§†è§‰åˆ†ç±»ç²¾åº¦ä¸ç”Ÿæˆè¾“å‡ºè´¨é‡ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚

**Method:** é‡‡ç”¨è§£è€¦çš„å¤šæ¨¡æ€ç®¡é“æ¶æ„ï¼Œé›†æˆä¸“ç”¨è§†è§‰éª¨å¹²ç½‘ç»œï¼ˆEfficientNet-B4ï¼‰ä¸ç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹ï¼ˆGemini LLMï¼‰ï¼Œå¹¶å¼•å…¥è¯­ä¹‰é”™è¯¯ä¼ æ’­ï¼ˆSEPï¼‰å½¢å¼åŒ–åˆ†ææ¡†æ¶æ¥è¯„ä¼°åˆ†ç±»è¯¯å·®åœ¨ç”Ÿæˆè¾“å‡ºä¸­çš„çº§è”æ•ˆåº”ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜EfficientNet-B4åœ¨Top-1å‡†ç¡®ç‡è¾¾åˆ°89.0%æ—¶æä¾›äº†æœ€ä½³ç²¾åº¦ä¸æ•ˆç‡å¹³è¡¡ï¼ŒGeminiåœ¨äº‹å®å‡†ç¡®æ€§æ–¹é¢è¾¾åˆ°9.2/10çš„ä¼˜å¼‚è¡¨ç°ï¼Œä½†ç³»ç»Ÿæ•´ä½“æ€§èƒ½å—åˆ°è§†è§‰å‰ç«¯æ„ŸçŸ¥ç²¾åº¦çš„æ ¹æœ¬æ€§é™åˆ¶ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†é«˜è¯­ä¹‰ç›¸ä¼¼åº¦æ˜¯ç³»ç»Ÿæœ€å…³é”®å¤±æ•ˆæ¨¡å¼ï¼Œè§†è§‰æ¨¡å—çš„åˆ†ç±»å‡†ç¡®æ€§æ˜¯å†³å®šå¤šæ¨¡æ€é£Ÿç‰©è¯†åˆ«ç³»ç»Ÿæ•ˆç”¨çš„å…³é”®ç“¶é¢ˆï¼Œä¸ºæœªæ¥æ”¹è¿›æ–¹å‘æä¾›äº†é‡è¦è§è§£ã€‚

---

#### ğŸ“„ Abstract
The proliferation of digital food applications necessitates robust methods for automated nutritional analysis and culinary guidance. This paper presents a comprehensive comparative evaluation of a decoupled, multimodal pipeline for food recognition. We evaluate a system integrating a specialized visual backbone (EfficientNet-B4) with a powerful generative large language model (Google's Gemini LLM). The core objective is to evaluate the trade-offs between visual classification accuracy, model efficiency, and the quality of generative output (nutritional data and recipes). We benchmark this pipeline against alternative vision backbones (VGG-16, ResNet-50, YOLOv8) and a lightweight LLM (Gemma). We introduce a formalization for "Semantic Error Propagation" (SEP) to analyze how classification inaccuracies from the visual module cascade into the generative output. Our analysis is grounded in a new Custom Chinese Food Dataset (CCFD) developed to address cultural bias in public datasets. Experimental results demonstrate that while EfficientNet-B4 (89.0\% Top-1 Acc.) provides the best balance of accuracy and efficiency, and Gemini (9.2/10 Factual Accuracy) provides superior generative quality, the system's overall utility is fundamentally bottlenecked by the visual front-end's perceptive accuracy. We conduct a detailed per-class analysis, identifying high semantic similarity as the most critical failure mode.


### [28] [Text-based Aerial-Ground Person Retrieval](https://arxiv.org/abs/2511.08369)
*Xinyu Zhou, Yu Wu, Jiayao Ma, Wenhao Wang, Min Cao, Mang Ye*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†åŸºäºæ–‡æœ¬çš„ç©ºåœ°è¡Œäººæ£€ç´¢ä»»åŠ¡ï¼ˆTAG-PRï¼‰ï¼Œé€šè¿‡æ„å»ºTAG-PEDESæ•°æ®é›†å’Œå¼€å‘TAG-CLIPæ£€ç´¢æ¡†æ¶ï¼Œè§£å†³äº†è·¨å¼‚æ„è§†è§’çš„è¡Œäººå›¾åƒæ£€ç´¢é—®é¢˜ï¼Œåœ¨ä¼ ç»Ÿåœ°é¢è§†è§’æ£€ç´¢åŸºç¡€ä¸Šå¼•å…¥äº†æ›´å…·å®ç”¨ä»·å€¼çš„ç©ºä¸­è§†è§’ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»ŸåŸºäºæ–‡æœ¬çš„è¡Œäººæ£€ç´¢ï¼ˆT-PRï¼‰ä»…å…³æ³¨åœ°é¢è§†è§’å›¾åƒï¼Œå­˜åœ¨è§†è§’å•ä¸€çš„é™åˆ¶ï¼Œè€Œç©ºåœ°è¡Œäººæ£€ç´¢ï¼ˆTAG-PRï¼‰å¼•å…¥äº†ç©ºä¸­è§†è§’ï¼Œå…·æœ‰æ›´å¤§çš„å®é™…åº”ç”¨ä»·å€¼ï¼Œä½†ç”±äºå›¾åƒé—´å­˜åœ¨æ˜¾è‘—çš„è§†è§’å·®å¼‚ï¼Œå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚

**Method:** æå‡ºäº†TAG-CLIPæ£€ç´¢æ¡†æ¶ï¼Œé‡‡ç”¨åˆ†å±‚è·¯ç”±çš„ä¸“å®¶æ··åˆæ¨¡å—æ¥å­¦ä¹ è§†è§’ç‰¹å®šå’Œè§†è§’æ— å…³çš„ç‰¹å¾ï¼Œå¹¶é€šè¿‡è§†è§’è§£è€¦ç­–ç•¥åˆ†ç¦»è§†è§’ç‰¹å®šç‰¹å¾ä»¥å®ç°æ›´å¥½çš„è·¨æ¨¡æ€å¯¹é½ï¼›åŒæ—¶æ„å»ºäº†TAG-PEDESæ•°æ®é›†ï¼Œé‡‡ç”¨å¤šæ ·åŒ–æ–‡æœ¬ç”ŸæˆèŒƒå¼ç¡®ä¿åœ¨è§†è§’å¼‚æ„æ€§ä¸‹çš„é²æ£’æ€§ã€‚

**Result:** åœ¨æå‡ºçš„TAG-PEDESæ•°æ®é›†å’Œç°æœ‰T-PRåŸºå‡†ä¸Šè¯„ä¼°äº†TAG-CLIPçš„æœ‰æ•ˆæ€§ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è·¨å¼‚æ„è§†è§’çš„è¡Œäººæ£€ç´¢ä»»åŠ¡ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºè·¨è§†è§’è¡Œäººæ£€ç´¢æä¾›äº†æ–°çš„ä»»åŠ¡å®šä¹‰å’Œè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡è§†è§’è§£è€¦å’Œç‰¹å¾å­¦ä¹ ç­–ç•¥æœ‰æ•ˆç¼“è§£äº†è§†è§’å·®å¼‚å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä¸ºç©ºåœ°ååŒçš„æ™ºèƒ½ç›‘æ§å’Œæœç´¢æ•‘æ´ç­‰åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
This work introduces Text-based Aerial-Ground Person Retrieval (TAG-PR), which aims to retrieve person images from heterogeneous aerial and ground views with textual descriptions. Unlike traditional Text-based Person Retrieval (T-PR), which focuses solely on ground-view images, TAG-PR introduces greater practical significance and presents unique challenges due to the large viewpoint discrepancy across images. To support this task, we contribute: (1) TAG-PEDES dataset, constructed from public benchmarks with automatically generated textual descriptions, enhanced by a diversified text generation paradigm to ensure robustness under view heterogeneity; and (2) TAG-CLIP, a novel retrieval framework that addresses view heterogeneity through a hierarchically-routed mixture of experts module to learn view-specific and view-agnostic features and a viewpoint decoupling strategy to decouple view-specific features for better cross-modal alignment. We evaluate the effectiveness of TAG-CLIP on both the proposed TAG-PEDES dataset and existing T-PR benchmarks. The dataset and code are available at https://github.com/Flame-Chasers/TAG-PR.


### [29] [Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation](https://arxiv.org/abs/2511.08402)
*Difei Gu, Yunhe Gao, Mu Zhou, Dimitris Metaxas*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºAnatomy-VLMï¼Œä¸€ç§ç»†ç²’åº¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ•´åˆå¤šå°ºåº¦åŒ»å­¦ä¿¡æ¯å’Œè§£å‰–ç»“æ„å®šä½ï¼Œå®ç°äº†ä¸“å®¶çº§çš„æ”¾å°„å­¦ç–¾ç—…è¯Šæ–­èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå¹¶æ”¯æŒé›¶æ ·æœ¬è§£å‰–ç»“æ„è§£é‡Šã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹å°†åŒ»å­¦å›¾åƒè§†ä¸ºæ•´ä½“å®ä½“ï¼Œå¿½ç•¥äº†ç–¾ç—…è¯Šæ–­è‡³å…³é‡è¦çš„ç»†ç²’åº¦å›¾åƒç»†èŠ‚ã€‚æ”¾å°„ç§‘åŒ»ç”Ÿé€šè¿‡ç»“åˆå…ˆéªŒåŒ»å­¦çŸ¥è¯†è¯†åˆ«å…³é”®è§£å‰–ç»“æ„ä½œä¸ºæ„Ÿå…´è¶£åŒºåŸŸè¿›è¡Œè¯Šæ–­ï¼Œè€Œç°æœ‰æ¨¡å‹ç¼ºä¹è¿™ç§ç»†ç²’åº¦åˆ†æèƒ½åŠ›ã€‚

**Method:** Anatomy-VLMè®¾è®¡äº†æ¨¡å‹ç¼–ç å™¨æ¥å®šä½åŒ»å­¦å›¾åƒä¸­çš„å…³é”®è§£å‰–ç‰¹å¾ï¼Œå°†è¿™äº›åŒºåŸŸä¸ç»“æ„åŒ–çŸ¥è¯†ç»“åˆè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥è§£é‡Šï¼Œå¹¶é€šè¿‡å¤šå°ºåº¦åŒ»å­¦ä¿¡æ¯å¯¹é½ç”Ÿæˆä¸´åºŠå¯è§£é‡Šçš„ç–¾ç—…é¢„æµ‹ã€‚æ¨¡å‹é‡‡ç”¨ç»†ç²’åº¦è§†è§‰è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œæ•´åˆäº†è§£å‰–ç»“æ„å®šä½å’ŒçŸ¥è¯†å¢å¼ºæœºåˆ¶ã€‚

**Result:** Anatomy-VLMåœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–æ•°æ®é›†ä¸Šå‡å–å¾—ä¼˜å¼‚æ€§èƒ½ï¼Œå¹¶åœ¨ä¸‹æ¸¸å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­éªŒè¯äº†å…¶ç»†ç²’åº¦å¯¹é½èƒ½å¤Ÿæ•è·è§£å‰–å’Œç—…ç†ç›¸å…³çŸ¥è¯†ã€‚æ¨¡å‹ç¼–ç å™¨æ”¯æŒé›¶æ ·æœ¬è§£å‰–ç»“æ„è§£é‡Šï¼Œå±•ç°äº†å¼ºå¤§çš„ä¸´åºŠè§£é‡Šèƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜æ•´åˆç»†ç²’åº¦è§£å‰–ä¿¡æ¯å’Œå¤šå°ºåº¦å¯¹é½å¯¹äºåŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ï¼ŒAnatomy-VLMçš„ä¸“å®¶çº§è¯Šæ–­èƒ½åŠ›éªŒè¯äº†äººç±»ä¸­å¿ƒåŒ–å·¥ä½œæµç¨‹åœ¨åŒ»å­¦AIä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¸´åºŠå¯è§£é‡ŠAIè¯Šæ–­æä¾›äº†æ–°èŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
Accurate disease interpretation from radiology remains challenging due to imaging heterogeneity. Achieving expert-level diagnostic decisions requires integration of subtle image features with clinical knowledge. Yet major vision-language models (VLMs) treat images as holistic entities and overlook fine-grained image details that are vital for disease diagnosis. Clinicians analyze images by utilizing their prior medical knowledge and identify anatomical structures as important region of interests (ROIs). Inspired from this human-centric workflow, we introduce Anatomy-VLM, a fine-grained, vision-language model that incorporates multi-scale information. First, we design a model encoder to localize key anatomical features from entire medical images. Second, these regions are enriched with structured knowledge for contextually-aware interpretation. Finally, the model encoder aligns multi-scale medical information to generate clinically-interpretable disease prediction. Anatomy-VLM achieves outstanding performance on both in- and out-of-distribution datasets. We also validate the performance of Anatomy-VLM on downstream image segmentation tasks, suggesting that its fine-grained alignment captures anatomical and pathology-related knowledge. Furthermore, the Anatomy-VLM's encoder facilitates zero-shot anatomy-wise interpretation, providing its strong expert-level clinical interpretation capabilities.


### [30] [Large Sign Language Models: Toward 3D American Sign Language Translation](https://arxiv.org/abs/2511.08535)
*Sen Zhang, Xiaoxiao He, Di Liu, Zhaoyang Xia, Mingyu Zhao, Chaowei Tan, Vivian Li, Bo Liu, Dimitris N. Metaxas, Mubbasir Kapadia*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†å¤§å‹æ‰‹è¯­æ¨¡å‹ï¼ˆLSLMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºéª¨å¹²ç½‘ç»œæ¥ç¿»è¯‘3Dç¾å›½æ‰‹è¯­çš„æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿæ”¹å–„å¬åŠ›éšœç¢è€…çš„è™šæ‹Ÿäº¤æµã€‚è¯¥æ¡†æ¶é€šè¿‡ç›´æ¥å¤„ç†3Dæ‰‹è¯­æ•°æ®æ¥æ•æ‰ä¸°å¯Œçš„ç©ºé—´ã€å§¿æ€å’Œæ·±åº¦ä¿¡æ¯ï¼Œå®ç°äº†æ›´å‡†ç¡®å’Œé²æ£’çš„æ‰‹è¯­ç¿»è¯‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ‰‹è¯­è¯†åˆ«æ–¹æ³•ä¸»è¦ä¾èµ–2Dè§†é¢‘æ•°æ®ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨3Dåœºæ™¯ä¸­çš„ä¸°å¯Œç©ºé—´ã€å§¿æ€å’Œæ·±åº¦ä¿¡æ¯ï¼Œé™åˆ¶äº†æ‰‹è¯­ç¿»è¯‘çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€å±€é™æ€§ï¼ŒåŒæ—¶æ¢ç´¢å°†å¤æ‚çš„å¤šæ¨¡æ€è¯­è¨€æ•´åˆåˆ°LLMå¤„ç†èƒ½åŠ›ä¸­ï¼Œè¶…è¶Šçº¯æ–‡æœ¬è¾“å…¥ä»¥æ‰©å±•å¯¹äººç±»äº¤æµçš„ç†è§£ã€‚

**Method:** è¯¥ç ”ç©¶æå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„3Dæ‰‹è¯­ç¿»è¯‘æ¡†æ¶ï¼Œç›´æ¥åˆ©ç”¨3Dæ‰‹è¯­æ•°æ®æ•æ‰ç©ºé—´ã€å§¿æ€å’Œæ·±åº¦ä¿¡æ¯ã€‚ç ”ç©¶äº†ä¸¤ç§ç¿»è¯‘æ–¹å¼ï¼šç›´æ¥ä»3Dæ‰‹åŠ¿ç‰¹å¾åˆ°æ–‡æœ¬çš„ç¿»è¯‘ï¼Œä»¥åŠé€šè¿‡å¤–éƒ¨æç¤ºè°ƒèŠ‚çš„æŒ‡ä»¤å¼•å¯¼ç¿»è¯‘è®¾ç½®ï¼Œæä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚

**Result:** è¯¥æ–¹æ³•å®ç°äº†æ›´å‡†ç¡®å’Œé²æ£’çš„3Dæ‰‹è¯­ç¿»è¯‘ï¼Œå¢å¼ºäº†æ•°å­—äº¤æµå¯¹å¬åŠ›éšœç¢ç¾¤ä½“çš„å¯è®¿é—®æ€§ã€‚é€šè¿‡å°†å¤æ‚çš„å¤šæ¨¡æ€è¯­è¨€æ•´åˆåˆ°LLMä¸­ï¼Œæ‰©å±•äº†æ¨¡å‹å¯¹äººç±»äº¤æµå½¢å¼çš„ç†è§£èƒ½åŠ›ã€‚

**Conclusion:** è¿™é¡¹å·¥ä½œä¸ºå®ç°åŒ…å®¹æ€§å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ï¼Œè¿™äº›ç³»ç»Ÿèƒ½å¤Ÿç†è§£å¤šæ ·åŒ–çš„è¯­è¨€å½¢å¼ã€‚ç ”ç©¶å±•ç¤ºäº†LLMåœ¨å¤„ç†å¤æ‚å…·èº«å¤šæ¨¡æ€è¯­è¨€æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥æ›´å¹¿æ³›çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
We present Large Sign Language Models (LSLM), a novel framework for translating 3D American Sign Language (ASL) by leveraging Large Language Models (LLMs) as the backbone, which can benefit hearing-impaired individuals' virtual communication. Unlike existing sign language recognition methods that rely on 2D video, our approach directly utilizes 3D sign language data to capture rich spatial, gestural, and depth information in 3D scenes. This enables more accurate and resilient translation, enhancing digital communication accessibility for the hearing-impaired community. Beyond the task of ASL translation, our work explores the integration of complex, embodied multimodal languages into the processing capabilities of LLMs, moving beyond purely text-based inputs to broaden their understanding of human communication. We investigate both direct translation from 3D gesture features to text and an instruction-guided setting where translations can be modulated by external prompts, offering greater flexibility. This work provides a foundational step toward inclusive, multimodal intelligent systems capable of understanding diverse forms of language.


### [31] [Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation](https://arxiv.org/abs/2511.08258)
*Jae Joong Lee, Bedrich Benes*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºTop2Groundï¼Œä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿç›´æ¥ä»èˆªæ‹å›¾åƒç”Ÿæˆé€¼çœŸçš„åœ°é¢è§†è§’å›¾åƒï¼Œæ— éœ€ä¾èµ–æ·±åº¦å›¾æˆ–3Dä½“ç´ ç­‰ä¸­é—´è¡¨ç¤ºã€‚è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå¹³å‡SSIMæå‡7.3%ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä»èˆªæ‹è§†è§’ç”Ÿæˆåœ°é¢å›¾åƒé¢ä¸´æç«¯è§†è§’å·®å¼‚ã€é®æŒ¡å’Œæœ‰é™è§†é‡ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–ä¸­é—´è¡¨ç¤ºå¦‚æ·±åº¦å›¾æˆ–3Dä½“ç´ ï¼Œé™åˆ¶äº†ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚

**Method:** Top2Groundé‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œå°†å»å™ªè¿‡ç¨‹åŸºäºVAEç¼–ç çš„ç©ºé—´ç‰¹å¾ï¼ˆæ¥è‡ªèˆªæ‹RGBå›¾åƒå’Œä¼°è®¡é«˜åº¦å›¾ï¼‰ä¸CLIPè¯­ä¹‰åµŒå…¥çš„è”åˆè¡¨ç¤ºè¿›è¡Œæ¡ä»¶åŒ–ï¼Œç¡®ä¿ç”Ÿæˆç»“æœæ—¢å—åœºæ™¯3Dç»“æ„å‡ ä½•çº¦æŸåˆä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚

**Result:** åœ¨CVUSAã€CVACTå’ŒAuto Arboristä¸‰ä¸ªå¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒTop2Groundåœ¨SSIMæŒ‡æ ‡ä¸Šå¹³å‡æå‡7.3%ï¼Œèƒ½å¤Ÿç¨³å¥å¤„ç†å®½çª„è§†é‡åœºæ™¯ï¼ŒéªŒè¯äº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** Top2Groundè¯æ˜äº†ç›´æ¥ç«¯åˆ°ç«¯ç”Ÿæˆæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ— éœ€ä¸­é—´è¡¨ç¤ºå³å¯å®ç°å‡ ä½•ç²¾ç¡®å’Œè¯­ä¹‰ä¸€è‡´çš„åœ°é¢å›¾åƒç”Ÿæˆï¼Œä¸ºè·¨è§†è§’å›¾åƒåˆæˆæä¾›äº†æ–°æ€è·¯ï¼Œå…·æœ‰å®é™…åº”ç”¨æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Generating ground-level images from aerial views is a challenging task due to extreme viewpoint disparity, occlusions, and a limited field of view. We introduce Top2Ground, a novel diffusion-based method that directly generates photorealistic ground-view images from aerial input images without relying on intermediate representations such as depth maps or 3D voxels. Specifically, we condition the denoising process on a joint representation of VAE-encoded spatial features (derived from aerial RGB images and an estimated height map) and CLIP-based semantic embeddings. This design ensures the generation is both geometrically constrained by the scene's 3D structure and semantically consistent with its content. We evaluate Top2Ground on three diverse datasets: CVUSA, CVACT, and the Auto Arborist. Our approach shows 7.3% average improvement in SSIM across three benchmark datasets, showing Top2Ground can robustly handle both wide and narrow fields of view, highlighting its strong generalization capabilities.


### [32] [SENCA-st: Integrating Spatial Transcriptomics and Histopathology with Cross Attention Shared Encoder for Region Identification in Cancer Pathology](https://arxiv.org/abs/2511.08573)
*Shanaka Liyanaarachchi, Chathurya Wijethunga, Shihab Aaquil Ahamed, Akthas Absar, Ranga Rodrigo*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSENCA-stæ¶æ„ï¼Œé€šè¿‡å…±äº«ç¼–ç å™¨å’Œé‚»åŸŸäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆæ•´åˆç»„ç»‡ç—…ç†å­¦å›¾åƒå’Œç©ºé—´è½¬å½•ç»„å­¦æ•°æ®ï¼Œåœ¨è‚¿ç˜¤å¼‚è´¨æ€§å’Œè‚¿ç˜¤å¾®ç¯å¢ƒåŒºåŸŸæ£€æµ‹æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿä¿ç•™ä¸¤ç§æ¨¡æ€çš„ç‰¹å¾ï¼Œç‰¹åˆ«å…³æ³¨ç»“æ„ç›¸ä¼¼ä½†åŠŸèƒ½ä¸åŒçš„åŒºåŸŸã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰çš„ç»„ç»‡ç—…ç†å­¦-ç©ºé—´è½¬å½•ç»„å­¦åŒºåŸŸåˆ†å‰²æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªæç«¯é—®é¢˜ï¼šè¦ä¹ˆè¿‡åº¦ä¾èµ–ç©ºé—´è½¬å½•ç»„å­¦æ•°æ®è€Œä»…å°†ç»„ç»‡ç—…ç†å­¦ç‰¹å¾ä½œä¸ºè¾…åŠ©å¤„ç†ï¼Œè¦ä¹ˆä½¿ç”¨æ™®é€šå¯¹æ¯”å­¦ä¹ æ–¹æ³•å¯¼è‡´ç»„ç»‡ç—…ç†å­¦å›¾åƒç‰¹å¾è¿‡äºçªå‡ºè€Œä¸¢å¤±åŠŸèƒ½ä¿¡æ¯ã€‚è¿™ä¸¤ç§æƒ…å†µéƒ½ä¼šå¯¼è‡´æ¨¡å‹è¦ä¹ˆè¿·å¤±åœ¨ç©ºé—´è½¬å½•ç»„å­¦çš„å™ªå£°ä¸­ï¼Œè¦ä¹ˆè¿‡åº¦å¹³æ»‘è€Œä¸¢å¤±å…³é”®ä¿¡æ¯ã€‚

**Method:** æå‡ºæ–°é¢–çš„SENCA-stæ¶æ„ï¼Œé‡‡ç”¨å…±äº«ç¼–ç å™¨ç»“åˆé‚»åŸŸäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚è¯¥æ¶æ„èƒ½å¤ŸåŒæ—¶ä¿ç•™ä¸¤ç§æ¨¡æ€çš„ç‰¹å¾ï¼Œç‰¹åˆ«é‡è¦çš„æ˜¯é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å¼ºè°ƒåœ¨ç»„ç»‡ç—…ç†å­¦ä¸Šç»“æ„ç›¸ä¼¼ä½†åœ¨ç©ºé—´è½¬å½•ç»„å­¦ä¸ŠåŠŸèƒ½ä¸åŒçš„åŒºåŸŸã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ£€æµ‹è‚¿ç˜¤å¼‚è´¨æ€§å’Œè‚¿ç˜¤å¾®ç¯å¢ƒåŒºåŸŸæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›åŒºåŸŸåœ¨ä¸´åºŠåº”ç”¨ä¸­å…·æœ‰å…³é”®é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç™Œç—‡è€è¯æ€§ç ”ç©¶ä¸­ã€‚

**Conclusion:** SENCA-stæ¶æ„æˆåŠŸè§£å†³äº†å¤šæ¨¡æ€æ•°æ®æ•´åˆä¸­çš„å¹³è¡¡é—®é¢˜ï¼Œä¸ºè‚¿ç˜¤å¼‚è´¨æ€§åˆ†ææä¾›äº†æ›´æœ‰æ•ˆçš„å·¥å…·ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨æ•´åˆç»“æ„å’ŒåŠŸèƒ½ä¿¡æ¯æ—¶ä¿æŒç‰¹å¾å¹³è¡¡çš„é‡è¦æ€§ï¼Œä¸ºç™Œç—‡ç ”ç©¶å’Œç²¾å‡†åŒ»ç–—å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Spatial transcriptomics is an emerging field that enables the identification of functional regions based on the spatial distribution of gene expression. Integrating this functional information present in transcriptomic data with structural data from histopathology images is an active research area with applications in identifying tumor substructures associated with cancer drug resistance. Current histopathology-spatial-transcriptomic region segmentation methods suffer due to either making spatial transcriptomics prominent by using histopathology features just to assist processing spatial transcriptomics data or using vanilla contrastive learning that make histopathology images prominent due to only promoting common features losing functional information. In both extremes, the model gets either lost in the noise of spatial transcriptomics or overly smoothed, losing essential information. Thus, we propose our novel architecture SENCA-st (Shared Encoder with Neighborhood Cross Attention) that preserves the features of both modalities. More importantly, it emphasizes regions that are structurally similar in histopathology but functionally different on spatial transcriptomics using cross-attention. We demonstrate the superior performance of our model that surpasses state-of-the-art methods in detecting tumor heterogeneity and tumor micro-environment regions, a clinically crucial aspect.


### [33] [Re-coding for Uncertainties: Edge-awareness Semantic Concordance for Resilient Event-RGB Segmentation](https://arxiv.org/abs/2511.08269)
*Nan Bao, Yifan Zhao, Lin Zhu, Jia Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§è¾¹ç¼˜æ„ŸçŸ¥è¯­ä¹‰ä¸€è‡´æ€§æ¡†æ¶ï¼Œé€šè¿‡æŒ–æ˜äº‹ä»¶å’ŒRGBæ¨¡æ€çš„è¾¹ç¼˜ç‰¹å¾è¿›è¡Œå¼¹æ€§èåˆï¼Œè§£å†³äº†æç«¯æ¡ä»¶ä¸‹å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²ä¸­çš„å¼‚æ„ç‰¹å¾ä¸åŒ¹é…é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨DERS-XSæ•°æ®é›†ä¸Šå®ç°äº†2.55%çš„mIoUæå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è¯­ä¹‰åˆ†å‰²æ–¹æ³•åœ¨æç«¯æ¡ä»¶ä¸‹ï¼ˆå¦‚å…‰ç…§ä¸è¶³ã€å‰§çƒˆç›¸æœºè¿åŠ¨ï¼‰å­˜åœ¨æ˜¾è‘—çš„RGBä¿¡æ¯ä¸¢å¤±é—®é¢˜ï¼Œä¸¥é‡æŸå®³åˆ†å‰²ç»“æœã€‚è™½ç„¶å·²æœ‰ç ”ç©¶åˆ©ç”¨é«˜é€Ÿé«˜åŠ¨æ€èŒƒå›´çš„äº‹ä»¶æ¨¡æ€ä½œä¸ºè¡¥å……ï¼Œä½†äº‹ä»¶ä¸RGBæ¨¡æ€çš„å¤©ç„¶å¼‚æ„æ€§å¯¼è‡´ç‰¹å¾çº§ä¸åŒ¹é…ï¼Œç°æœ‰å¤šæ¨¡æ€æ–¹æ³•ä¼˜åŒ–æ•ˆæœä¸ä½³ã€‚

**Method:** æå‡ºè¾¹ç¼˜æ„ŸçŸ¥è¯­ä¹‰ä¸€è‡´æ€§æ¡†æ¶ï¼ŒåŒ…å«è¾¹ç¼˜æ„ŸçŸ¥æ½œåœ¨é‡ç¼–ç å’Œé‡ç¼–ç æ•´åˆä¸ä¸ç¡®å®šæ€§ä¼˜åŒ–ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ã€‚å‰è€…é€šè¿‡é‡ç¼–ç åˆ†å¸ƒå¼•å¯¼å°†äº‹ä»¶-RGBç‰¹å¾é‡æ–°å¯¹é½åˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ï¼Œå¹¶åˆ©ç”¨é¢„å»ºç«‹çš„è¾¹ç¼˜å­—å…¸ä½œä¸ºçº¿ç´¢å°†äº‹ä»¶-RGBåˆ†å¸ƒè½¬æ¢ä¸ºé‡ç¼–ç ç‰¹å¾ï¼›åè€…åˆ©ç”¨é‡ç¼–ç è¾¹ç¼˜ç‰¹å¾å’Œä¸ç¡®å®šæ€§æŒ‡æ ‡è§£å†³æç«¯æ¡ä»¶ä¸‹çš„å¼‚æ„äº‹ä»¶-RGBèåˆé—®é¢˜ã€‚

**Result:** åœ¨ä¸¤ä¸ªåˆæˆæ•°æ®é›†å’Œä¸€ä¸ªçœŸå®ä¸–ç•Œäº‹ä»¶-RGBè¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æå‡ºçš„DERS-XSæ•°æ®é›†ä¸Šæ¯”æœ€å…ˆè¿›æ–¹æ³•æé«˜äº†2.55%çš„mIoUï¼Œå¹¶åœ¨ç©ºé—´é®æŒ¡æ¡ä»¶ä¸‹è¡¨ç°å‡ºä¼˜è¶Šçš„å¼¹æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡æŒ–æ˜å¤šæ¨¡æ€çš„è¾¹ç¼˜ç‰¹å¾è¿›è¡Œå¼‚æ„ç‰¹å¾ç»Ÿä¸€çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæç«¯æ¡ä»¶ä¸‹çš„é²æ£’è¯­ä¹‰åˆ†å‰²æä¾›äº†æ–°æ€è·¯ã€‚æ‰€æå‡ºçš„è¾¹ç¼˜æ„ŸçŸ¥è¯­ä¹‰ä¸€è‡´æ€§æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè§£å†³äº‹ä»¶å’ŒRGBæ¨¡æ€çš„å¼‚æ„èåˆé—®é¢˜ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Semantic segmentation has achieved great success in ideal conditions. However, when facing extreme conditions (e.g., insufficient light, fierce camera motion), most existing methods suffer from significant information loss of RGB, severely damaging segmentation results. Several researches exploit the high-speed and high-dynamic event modality as a complement, but event and RGB are naturally heterogeneous, which leads to feature-level mismatch and inferior optimization of existing multi-modality methods. Different from these researches, we delve into the edge secret of both modalities for resilient fusion and propose a novel Edge-awareness Semantic Concordance framework to unify the multi-modality heterogeneous features with latent edge cues. In this framework, we first propose Edge-awareness Latent Re-coding, which obtains uncertainty indicators while realigning event-RGB features into unified semantic space guided by re-coded distribution, and transfers event-RGB distributions into re-coded features by utilizing a pre-established edge dictionary as clues. We then propose Re-coded Consolidation and Uncertainty Optimization, which utilize re-coded edge features and uncertainty indicators to solve the heterogeneous event-RGB fusion issues under extreme conditions. We establish two synthetic and one real-world event-RGB semantic segmentation datasets for extreme scenario comparisons. Experimental results show that our method outperforms the state-of-the-art by a 2.55% mIoU on our proposed DERS-XS, and possesses superior resilience under spatial occlusion. Our code and datasets are publicly available at https://github.com/iCVTEAM/ESC.


### [34] [VideoChain: A Transformer-Based Framework for Multi-hop Video Question Generation](https://arxiv.org/abs/2511.08348)
*Arpan Phukan, Anupam Pandey, Deepjyoti Bodo, Asif Ekbal*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†VideoChainæ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¤šè·³è§†é¢‘é—®é¢˜ç”Ÿæˆ(MVQG)çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿç”Ÿæˆéœ€è¦è·¨å¤šä¸ªæ—¶é—´åˆ†ç¦»è§†é¢‘ç‰‡æ®µè¿›è¡Œæ¨ç†çš„é—®é¢˜ï¼Œåœ¨TVQA+æ•°æ®é›†ä¸Šæ„å»ºäº†å¤§è§„æ¨¡MVQ-60æ•°æ®é›†å¹¶å±•ç¤ºäº†ä¼˜è¶Šçš„ç”Ÿæˆæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šè·³é—®é¢˜ç”Ÿæˆä¸»è¦å±€é™äºæ–‡æœ¬é¢†åŸŸï¼Œè€Œè§†é¢‘é—®é¢˜ç”Ÿæˆä»…é™äºå•ç‰‡æ®µé›¶è·³é—®é¢˜ï¼Œç¼ºä¹èƒ½å¤Ÿå¤„ç†è·¨å¤šä¸ªæ—¶é—´åˆ†ç¦»è§†é¢‘ç‰‡æ®µè¿›è¡Œå¤æ‚æ¨ç†çš„å¤šè·³è§†é¢‘é—®é¢˜ç”Ÿæˆæ–¹æ³•ï¼Œè¿™é™åˆ¶äº†è§†é¢‘ç†è§£èƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚

**Method:** VideoChainé‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼ŒåŸºäºæ”¹è¿›çš„BARTéª¨å¹²ç½‘ç»œå¹¶å¢å¼ºè§†é¢‘åµŒå…¥ï¼Œèƒ½å¤ŸåŒæ—¶æ•æ‰æ–‡æœ¬å’Œè§†è§‰ä¾èµ–å…³ç³»ï¼Œåˆ©ç”¨TVQA+æ•°æ®é›†è‡ªåŠ¨æ„å»ºå¤§è§„æ¨¡MVQ-60æ•°æ®é›†ï¼Œé€šè¿‡åˆå¹¶é›¶è·³é—®ç­”å¯¹ç¡®ä¿å¯æ‰©å±•æ€§å’Œå¤šæ ·æ€§ã€‚

**Result:** è¯„ä¼°æ˜¾ç¤ºVideoChainåœ¨æ ‡å‡†ç”ŸæˆæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼šROUGE-L(0.6454)ã€ROUGE-1(0.6854)ã€BLEU-1(0.6711)ã€BERTScore-F1(0.7967)å’Œè¯­ä¹‰ç›¸ä¼¼åº¦(0.8110)ï¼Œè¯æ˜äº†æ¨¡å‹ç”Ÿæˆè¿è´¯ã€ä¸Šä¸‹æ–‡ç›¸å…³ä¸”éœ€è¦æ¨ç†çš„é—®é¢˜çš„èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å¤šè·³è§†é¢‘é—®é¢˜ç”Ÿæˆçš„å¯è¡Œæ€§ï¼ŒVideoChainæ¡†æ¶ä¸ºè§†é¢‘ç†è§£æä¾›äº†æ›´å…¨é¢çš„è¯„ä¼°å·¥å…·ï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡ä¸ºæœªæ¥è§†é¢‘æ¨ç†ä»»åŠ¡çš„å‘å±•å¥ å®šäº†åŸºç¡€ï¼Œæ¨åŠ¨äº†è§†é¢‘é—®ç­”é¢†åŸŸå‘æ›´å¤æ‚æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Multi-hop Question Generation (QG) effectively evaluates reasoning but remains confined to text; Video Question Generation (VideoQG) is limited to zero-hop questions over single segments. To address this, we introduce VideoChain, a novel Multi-hop Video Question Generation (MVQG) framework designed to generate questions that require reasoning across multiple, temporally separated video segments. VideoChain features a modular architecture built on a modified BART backbone enhanced with video embeddings, capturing textual and visual dependencies. Using the TVQA+ dataset, we automatically construct the large-scale MVQ-60 dataset by merging zero-hop QA pairs, ensuring scalability and diversity. Evaluations show VideoChain's strong performance across standard generation metrics: ROUGE-L (0.6454), ROUGE-1 (0.6854), BLEU-1 (0.6711), BERTScore-F1 (0.7967), and semantic similarity (0.8110). These results highlight the model's ability to generate coherent, contextually grounded, and reasoning-intensive questions.


### [35] [Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding](https://arxiv.org/abs/2511.08480)
*Da Li, Yuxiao Luo, Keping Bi, Jiafeng Guo, Wei Yuan, Biao Yang, Yan Wang, Fan Yang, Tingting Gao, Guorui Zhou*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºCoMaæ–¹æ³•ï¼Œé€šè¿‡å‹ç¼©é¢„è®­ç»ƒé˜¶æ®µä½œä¸ºå¯¹æ¯”å­¦ä¹ çš„é¢„çƒ­ï¼Œå°†è§†è§‰è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºå…·æœ‰ç«äº‰åŠ›çš„åµŒå…¥æ¨¡å‹ã€‚è¯¥æ–¹æ³•ä»…éœ€å°‘é‡é¢„è®­ç»ƒæ•°æ®å³å¯å®ç°æ•ˆç‡å’Œæ•ˆæœçš„åŒé‡ä¼˜åŒ–ï¼Œåœ¨MMEBåŸºå‡†ä¸Šè¾¾åˆ°åŒç±»è§„æ¨¡æ¨¡å‹çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡å¯¹æ¯”å­¦ä¹ åŒæ—¶ä¼˜åŒ–ä¸¤ä¸ªäº’è¡¥ç›®æ ‡ï¼Œä½†ä½œè€…è®¤ä¸ºå…¨é¢ç†è§£è¾“å…¥ä¸ä¸‹æ¸¸ä»»åŠ¡åˆ¤åˆ«æ€§ç‰¹å¾å¯ä»¥è§£è€¦ã€‚ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡æ•°æ®è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œé™åˆ¶äº†æ¨¡å‹æ•ˆç‡å’Œé€‚åº”æ€§ï¼Œå› æ­¤éœ€è¦æ¢ç´¢æ›´é«˜æ•ˆçš„é¢„è®­ç»ƒç­–ç•¥ã€‚

**Method:** æå‡ºCoMaå‹ç¼©é¢„è®­ç»ƒé˜¶æ®µä½œä¸ºå¯¹æ¯”å­¦ä¹ çš„é¢„çƒ­ï¼Œè¯¥æ–¹æ³•å°†è§†è§‰è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºåµŒå…¥æ¨¡å‹ï¼Œé€šè¿‡è§£è€¦è¯­ä¹‰å†…å®¹ä¿ç•™å’Œåˆ¤åˆ«æ€§ç‰¹å¾å¼ºè°ƒä¸¤ä¸ªç›®æ ‡ï¼Œä»…éœ€å°‘é‡é¢„è®­ç»ƒæ•°æ®å³å¯å®ç°æœ‰æ•ˆä¼˜åŒ–ã€‚

**Result:** åœ¨MMEBåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCoMaå®ç°äº†åŒç±»è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šçš„åŒé‡ä¼˜åŒ–ï¼Œä»…éœ€å°‘é‡é¢„è®­ç»ƒæ•°æ®å°±èƒ½è·å¾—ç«äº‰åŠ›çš„åµŒå…¥è´¨é‡ã€‚

**Conclusion:** ç ”ç©¶è¯æ˜è§†è§‰è¯­è¨€æ¨¡å‹çš„åµŒå…¥èƒ½åŠ›å¯ä»¥é€šè¿‡è§£è€¦é¢„è®­ç»ƒç›®æ ‡æ¥ä¼˜åŒ–ï¼Œå‹ç¼©é¢„è®­ç»ƒé˜¶æ®µèƒ½æœ‰æ•ˆæå‡å¯¹æ¯”å­¦ä¹ æ•ˆç‡ã€‚è¿™ä¸€å‘ç°ä¸ºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æä¾›äº†æ–°çš„ä¼˜åŒ–è·¯å¾„ï¼Œè¡¨æ˜æ¨¡å‹æ€§èƒ½æå‡ä¸ä¸€å®šéœ€è¦å¤§è§„æ¨¡æ•°æ®æ”¯æŒã€‚

---

#### ğŸ“„ Abstract
Vision-language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that VLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform a VLM into a competitive embedding model. CoMa achieves new state-of-the-art results among VLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.


### [36] [UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist](https://arxiv.org/abs/2511.08521)
*Zhengyang Liang, Daoan Zhang, Huichi Zhou, Rui Huang, Bobo Li, Yuechen Zhang, Shengqiong Wu, Xiaohan Wang, Jiebo Luo, Lizi Liao, Hao Fei*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†UniVAï¼Œä¸€ä¸ªå¼€æºçš„ã€å…¨èƒ½åŠ›å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºç»Ÿä¸€è§†é¢‘ç†è§£ã€åˆ†å‰²ã€ç¼–è¾‘å’Œç”Ÿæˆåˆ°è¿è´¯çš„å·¥ä½œæµä¸­ï¼Œé€šè¿‡Plan-and-ActåŒæ™ºèƒ½ä½“æ¶æ„å’Œå±‚æ¬¡åŒ–å¤šçº§å†…å­˜å®ç°äº¤äº’å¼å’Œè‡ªæˆ‘åæ€çš„è§†é¢‘åˆ›ä½œã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ä¸“ä¸šAIæ¨¡å‹åœ¨å­¤ç«‹è§†é¢‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç°å®åº”ç”¨éœ€è¦ç»“åˆè¿™äº›èƒ½åŠ›çš„å¤æ‚è¿­ä»£å·¥ä½œæµï¼Œè€Œå½“å‰ç¼ºä¹èƒ½å¤Ÿç»Ÿä¸€è§†é¢‘ç†è§£ã€åˆ†å‰²ã€ç¼–è¾‘å’Œç”Ÿæˆèƒ½åŠ›çš„ç»¼åˆæ€§æ¡†æ¶ã€‚

**Method:** UniVAé‡‡ç”¨Plan-and-ActåŒæ™ºèƒ½ä½“æ¶æ„ï¼šè§„åˆ’æ™ºèƒ½ä½“è§£é‡Šç”¨æˆ·æ„å›¾å¹¶åˆ†è§£ä¸ºç»“æ„åŒ–è§†é¢‘å¤„ç†æ­¥éª¤ï¼Œæ‰§è¡Œæ™ºèƒ½ä½“é€šè¿‡æ¨¡å—åŒ–MCPå·¥å…·æœåŠ¡å™¨æ‰§è¡Œè¿™äº›æ­¥éª¤ï¼ŒåŒæ—¶é€šè¿‡å±‚æ¬¡åŒ–å¤šçº§å†…å­˜ï¼ˆå…¨å±€çŸ¥è¯†ã€ä»»åŠ¡ä¸Šä¸‹æ–‡å’Œç”¨æˆ·ç‰¹å®šåå¥½ï¼‰ç»´æŒé•¿è§†é‡æ¨ç†å’Œä¸Šä¸‹æ–‡è¿ç»­æ€§ã€‚

**Result:** è¯¥æ¡†æ¶å®ç°äº†è¿­ä»£å’Œä»»æ„æ¡ä»¶è§†é¢‘å·¥ä½œæµï¼Œå¹¶å¼•å…¥äº†UniVA-BenchåŸºå‡†å¥—ä»¶æ¥ä¸¥æ ¼è¯„ä¼°æ­¤ç±»æ™ºèƒ½è§†é¢‘ç³»ç»Ÿï¼Œæ¶µç›–ç†è§£ã€ç¼–è¾‘ã€åˆ†å‰²å’Œç”Ÿæˆç­‰å¤šæ­¥éª¤è§†é¢‘ä»»åŠ¡ã€‚

**Conclusion:** UniVAå’ŒUniVA-Benchçš„å¼€æºå‘å¸ƒæ—¨åœ¨æ¨åŠ¨äº¤äº’å¼ã€æ™ºèƒ½åŒ–å’Œé€šç”¨è§†é¢‘æ™ºèƒ½çš„ç ”ç©¶ï¼Œä¸ºä¸‹ä¸€ä»£å¤šæ¨¡æ€AIç³»ç»Ÿçš„å‘å±•æä¾›é‡è¦åŸºç¡€ï¼Œè§£å†³äº†ä¼ ç»Ÿå•ç”¨é€”æ¨¡å‹æˆ–å•ä½“è§†é¢‘è¯­è¨€æ¨¡å‹éš¾ä»¥å®ç°çš„å¤æ‚å·¥ä½œæµé—®é¢˜ã€‚

---

#### ğŸ“„ Abstract
While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation $\rightarrow$ multi-round editing $\rightarrow$ object segmentation $\rightarrow$ compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)


### [37] [3D4D: An Interactive, Editable, 4D World Model via 3D Video Generation](https://arxiv.org/abs/2511.08536)
*Yunhong He, Zhengqing Yuan, Zhengzhong Tu, Yanfang Ye, Lichao Sun*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†3D4Däº¤äº’å¼4Då¯è§†åŒ–æ¡†æ¶ï¼Œé€šè¿‡é›†æˆWebGLä¸Supersplatæ¸²æŸ“æŠ€æœ¯ï¼Œå°†é™æ€å›¾åƒå’Œæ–‡æœ¬è½¬æ¢ä¸ºè¿è´¯çš„4Dåœºæ™¯ï¼Œå®ç°äº†é«˜æ•ˆçš„å¤šæ¨¡æ€å®æ—¶äº¤äº’ã€‚è¯¥æ¡†æ¶æ”¯æŒç”¨æˆ·é©±åŠ¨çš„è‡ªé€‚åº”å¤æ‚4Dç¯å¢ƒæ¢ç´¢ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰4Då¯è§†åŒ–ç³»ç»Ÿåœ¨å®æ—¶äº¤äº’æ•ˆç‡å’Œç”¨æˆ·é©±åŠ¨æ¢ç´¢æ–¹é¢å­˜åœ¨å±€é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚4Dç¯å¢ƒæ—¶ç¼ºä¹æœ‰æ•ˆçš„å¤šæ¨¡æ€äº¤äº’èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³é™æ€å†…å®¹å‘åŠ¨æ€4Dåœºæ™¯è½¬æ¢çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œä»¥åŠå¤§è§„æ¨¡4Dæ•°æ®å®æ—¶æ¸²æŸ“çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚

**Method:** è¯¥æ¡†æ¶é‡‡ç”¨WebGLä¸Supersplatæ¸²æŸ“æŠ€æœ¯é›†æˆæ–¹æ¡ˆï¼Œæ„å»ºäº†å››ä¸ªæ ¸å¿ƒå¤„ç†æ¨¡å—ï¼Œå¹¶å¼•å…¥äº†æ³¨è§†ç‚¹æ¸²æŸ“ç­–ç•¥æ¥ä¼˜åŒ–è®¡ç®—èµ„æºåˆ†é…ã€‚é€šè¿‡å¤šæ¨¡æ€è¾“å…¥å¤„ç†ç®¡é“ï¼Œå®ç°äº†ä»é™æ€å›¾åƒå’Œæ–‡æœ¬åˆ°è¿è´¯4Dåœºæ™¯çš„ç«¯åˆ°ç«¯è½¬æ¢ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œ3D4Dæ¡†æ¶èƒ½å¤Ÿå®ç°é«˜æ•ˆçš„å®æ—¶å¤šæ¨¡æ€äº¤äº’ï¼Œæ˜¾è‘—æå‡äº†4Dåœºæ™¯æ¸²æŸ“æ€§èƒ½ã€‚æ³¨è§†ç‚¹æ¸²æŸ“ç­–ç•¥æœ‰æ•ˆé™ä½äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†è§†è§‰è´¨é‡ï¼Œæ”¯æŒç”¨æˆ·å¯¹å¤æ‚4Dç¯å¢ƒçš„è‡ªé€‚åº”æ¢ç´¢ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é›†æˆWebGLä¸Supersplatæ¸²æŸ“åœ¨4Då¯è§†åŒ–ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºäº¤äº’å¼4Då†…å®¹åˆ›å»ºæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚æ¡†æ¶çš„å¯æ‰©å±•æ¶æ„ä¸ºæœªæ¥å¤šæ¨¡æ€4Däº¤äº’ç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

---

#### ğŸ“„ Abstract
We introduce 3D4D, an interactive 4D visualization framework that integrates WebGL with Supersplat rendering. It transforms static images and text into coherent 4D scenes through four core modules and employs a foveated rendering strategy for efficient, real-time multi-modal interaction. This framework enables adaptive, user-driven exploration of complex 4D environments. The project page and code are available at https://yunhonghe1021.github.io/NOVA/.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [38] [Motif 2 12.7B technical report](https://arxiv.org/abs/2511.07464)
*Junghwan Lim, Sungmin Lee, Dongseok Kim, Taehyun Kim, Eunhwan Park, Jeesoo Lee, Jeongdoo Lee, Junhyeok Lee, Wai Ting Cheung, Dahye Choi, Jaeheui Her, Jaeyeon Huh, Hanbin Jung, Changjin Kang, Beomgyu Kim, Minjae Kim, Taewhan Kim, Youngrok Kim, Hyukjin Kweon, Haesol Lee, Kungyu Lee, Dongpin Oh, Yeongjae Park, Bokki Ryu, Dongjoo Weon*

#### ğŸ§© TL;DR
Motif-2-12.7Bæ˜¯ä¸€ä¸ªæ–°çš„å¼€æºåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡æ¶æ„åˆ›æ–°å’Œç³»ç»Ÿçº§ä¼˜åŒ–æ¨è¿›å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡å‰æ²¿ï¼Œåœ¨å—é™è®¡ç®—é¢„ç®—ä¸‹å®ç°å¯æ‰©å±•çš„è¯­è¨€ç†è§£å’Œé²æ£’æŒ‡ä»¤æ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨è®¡ç®—èµ„æºå—é™æƒ…å†µä¸‹çš„æ•ˆç‡é—®é¢˜ï¼Œé€šè¿‡ä¼˜åŒ–æ¶æ„å’Œè®­ç»ƒç³»ç»Ÿæ¥æå‡æ¨¡å‹æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›çš„åŸºå‡†è¡¨ç°ï¼ŒæŒ‘æˆ˜æ›´å¤§è§„æ¨¡æ¨¡å‹çš„èƒ½åŠ›ã€‚

**Method:** æ¨¡å‹åŸºäºMotif-2.6Bæ„å»ºï¼Œé›†æˆäº†åˆ†ç»„å·®åˆ†æ³¨æ„åŠ›æœºåˆ¶æ¥åˆ†ç¦»ä¿¡å·å’Œå™ªå£°æ§åˆ¶æ³¨æ„åŠ›è·¯å¾„ï¼Œä½¿ç”¨è¯¾ç¨‹é©±åŠ¨çš„æ•°æ®è°ƒåº¦å™¨åœ¨5.5ä¸‡äº¿tokenä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é‡‡ç”¨MuonClipä¼˜åŒ–å™¨å’Œè‡ªå®šä¹‰é«˜æ€§èƒ½å†…æ ¸ï¼ŒåŒ…æ‹¬èåˆPolyNormæ¿€æ´»å’Œå¹¶è¡ŒMuonç®—æ³•ã€‚

**Result:** Motif-2-12.7Båœ¨å¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºç«äº‰åŠ›è¡¨ç°ï¼Œè¡¨æ˜ç»è¿‡æ·±æ€ç†Ÿè™‘çš„æ¶æ„æ‰©å±•å’Œä¼˜åŒ–è®­ç»ƒè®¾è®¡èƒ½å¤ŸåŒ¹æ•Œæ›´å¤§è§„æ¨¡æ¨¡å‹çš„èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­å®ç°äº†æ˜¾è‘—çš„ååé‡å’Œå†…å­˜æ•ˆç‡æå‡ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æ¶æ„åˆ›æ–°å’Œç³»ç»Ÿä¼˜åŒ–çš„ç»“åˆï¼Œå¯ä»¥åœ¨ä¸æ˜¾è‘—å¢åŠ æ¨¡å‹è§„æ¨¡çš„æƒ…å†µä¸‹å®ç°ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè¿™ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹å¼€å‘æä¾›äº†é‡è¦å¯ç¤ºå’Œæ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.


### [39] [State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?](https://arxiv.org/abs/2511.07989)
*Taja Kuzman PungerÅ¡ek, Peter Rupnik, Ivan Porupski, Vuk DiniÄ‡, Nikola LjubeÅ¡iÄ‡*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªå—æ–¯æ‹‰å¤«è¯­è¨€æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå‘ç°LLMsåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå¯ä¸å¾®è°ƒçš„BERTç±»æ¨¡å‹ç›¸åª²ç¾ï¼Œä½†åœ¨è¾“å‡ºç¨³å®šæ€§ã€æ¨ç†é€Ÿåº¦å’Œè®¡ç®—æˆæœ¬æ–¹é¢å­˜åœ¨æ˜¾è‘—é™åˆ¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€æŒ‡ä»¤è°ƒä¼˜çš„è§£ç å™¨æ¨¡å‹å…´èµ·ï¼Œæ–‡æœ¬åˆ†ç±»é¢†åŸŸé€æ¸è½¬å‘é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºæ–¹æ³•ï¼Œä½†LLMsåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºè¾ƒå°‘çš„è¯­è¨€ä¸Šï¼Œä»ç„¶ç¼ºä¹å……åˆ†æ¢ç´¢ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚

**Method:** ç ”ç©¶æ¯”è¾ƒäº†å…¬å¼€å¯ç”¨çš„å¾®è°ƒBERTç±»æ¨¡å‹ä¸å¼€æºå’Œé—­æºLLMsåœ¨å—æ–¯æ‹‰å¤«è¯­è¨€ä¸Šçš„è¡¨ç°ï¼Œæ¶µç›–ä¸‰ä¸ªé¢†åŸŸçš„ä¸‰ä¸ªä»»åŠ¡ï¼šè®®ä¼šæ¼”è®²ä¸­çš„æƒ…æ„Ÿåˆ†ç±»ã€æ–°é—»æ–‡ç« å’Œè®®ä¼šæ¼”è®²ä¸­çš„ä¸»é¢˜åˆ†ç±»ï¼Œä»¥åŠç½‘ç»œæ–‡æœ¬çš„ä½“è£è¯†åˆ«ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMså±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œé€šå¸¸èƒ½å¤ŸåŒ¹é…ç”šè‡³è¶…è¶Šå¾®è°ƒçš„BERTç±»æ¨¡å‹ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼ŒLLMsåœ¨å—æ–¯æ‹‰å¤«è¯­è¨€å’Œè‹±è¯­ä¸Šçš„è¡¨ç°ç›¸å½“ï¼Œä½†å­˜åœ¨è¾“å‡ºä¸å¯é¢„æµ‹æ€§ã€æ¨ç†é€Ÿåº¦æ˜¾è‘—è¾ƒæ…¢å’Œè®¡ç®—æˆæœ¬æ›´é«˜ç­‰å…³é”®ç¼ºç‚¹ã€‚

**Conclusion:** å°½ç®¡LLMsåœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºè¾“å‡ºç¨³å®šæ€§å·®ã€æ¨ç†é€Ÿåº¦æ…¢å’Œè®¡ç®—æˆæœ¬é«˜ç­‰é™åˆ¶ï¼Œå¾®è°ƒçš„BERTç±»æ¨¡å‹åœ¨å¤§è§„æ¨¡è‡ªåŠ¨æ–‡æœ¬æ ‡æ³¨åœºæ™¯ä¸­ä»ç„¶æ˜¯æ›´å®ç”¨çš„é€‰æ‹©ï¼Œè¿™ä¸ºèµ„æºå—é™è¯­è¨€çš„å¤„ç†æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚

---

#### ğŸ“„ Abstract
Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.


### [40] [VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context](https://arxiv.org/abs/2511.08230)
*Heyang Liu, Ziyang Cheng, Yuhao Wang, Hongcheng Liu, Yiqi Li, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†VocalBench-zhï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹æ™®é€šè¯ç¯å¢ƒçš„è¯­éŸ³äº¤äº’è¯„ä¼°å¥—ä»¶ï¼ŒåŒ…å«10ä¸ªå­é›†å’Œè¶…è¿‡10Ké«˜è´¨é‡å®ä¾‹ï¼Œè¦†ç›–12ä¸ªç”¨æˆ·å¯¼å‘ç‰¹æ€§ï¼Œé€šè¿‡å¯¹14ä¸ªä¸»æµæ¨¡å‹çš„è¯„ä¼°æ­ç¤ºäº†å½“å‰æ–¹æ³•çš„å…±åŒæŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œæ™®é€šè¯ä½œä¸ºå…¨çƒä½¿ç”¨æœ€å¹¿æ³›çš„è¯­è¨€ä¹‹ä¸€ï¼Œè™½ç„¶å¾—åˆ°äº†å¤§å¤šæ•°æ¨¡å‹çš„æ”¯æŒï¼Œä½†ç¼ºä¹å…¨é¢çš„è¯­éŸ³åˆ°è¯­éŸ³åŸºå‡†æµ‹è¯•é˜»ç¢äº†å¼€å‘è€…çš„ç³»ç»Ÿæ€§è¯„ä¼°å’Œç”¨æˆ·çš„å…¬å¹³æ¨¡å‹æ¯”è¾ƒï¼Œè¿™ç§è¯„ä¼°èµ„æºçš„ç¨€ç¼ºé™åˆ¶äº†æ™®é€šè¯è¯­éŸ³äº¤äº’ç³»ç»Ÿçš„è¿›æ­¥ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†VocalBench-zhè¯„ä¼°å¥—ä»¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºèƒ½åŠ›çº§åˆ«åˆ’åˆ†çš„æ™®é€šè¯è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«10ä¸ªç²¾å¿ƒè®¾è®¡çš„å­é›†å’Œè¶…è¿‡10,000ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œè¦†ç›–äº†12ä¸ªç”¨æˆ·å¯¼å‘çš„ç‰¹æ€§ç»´åº¦ï¼Œä¸ºæ™®é€šè¯è¯­éŸ³äº¤äº’æä¾›äº†å…¨é¢çš„è¯„ä¼°æ ‡å‡†ã€‚

**Result:** é€šè¿‡å¯¹14ä¸ªä¸»æµæ¨¡å‹çš„è¯„ä¼°å®éªŒï¼Œç ”ç©¶å‘ç°å½“å‰æŠ€æœ¯è·¯çº¿é¢ä¸´å…±åŒçš„æŒ‘æˆ˜ï¼Œè¯„ä¼°ç»“æœä¸ä»…æ­ç¤ºäº†ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œè¿˜å¼ºè°ƒäº†æ–°ä¸€ä»£è¯­éŸ³äº¤äº’ç³»ç»Ÿéœ€è¦æ–°çš„è®¾è®¡æ€è·¯å’Œåˆ›æ–°æ–¹æ³•ã€‚

**Conclusion:** è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†å¼€å‘ä¸“é—¨é’ˆå¯¹æ™®é€šè¯ç¯å¢ƒçš„è¯„ä¼°åŸºå‡†çš„é‡è¦æ€§ï¼Œä¸ºä¸‹ä¸€ä»£è¯­éŸ³äº¤äº’ç³»ç»Ÿçš„å‘å±•æä¾›äº†å…³é”®è§è§£ï¼Œè¯„ä¼°ä»£ç å’Œæ•°æ®é›†çš„å…¬å¼€å¯ç”¨æ€§å°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå…¬å¹³æ¯”è¾ƒã€‚

---

#### ğŸ“„ Abstract
The development of multi-modal large language models (LLMs) leads to intelligent approaches capable of speech interactions. As one of the most widely spoken languages globally, Mandarin is supported by most models to enhance their applicability and reach. However, the scarcity of comprehensive speech-to-speech (S2S) benchmarks in Mandarin contexts impedes systematic evaluation for developers and hinders fair model comparison for users. In this work, we propose VocalBench-zh, an ability-level divided evaluation suite adapted to Mandarin context consisting of 10 well-crafted subsets and over 10K high-quality instances, covering 12 user-oriented characters. The evaluation experiment on 14 mainstream models reveals the common challenges for current routes, and highlights the need for new insights into next-generation speech interactive systems. The evaluation codes and datasets will be available at https://github.com/SJTU-OmniAgent/VocalBench-zh.


### [41] [REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment](https://arxiv.org/abs/2511.07458)
*Priyanka Mudgal*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†REFLEXï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹åˆ¤æ–­çš„æ— å‚è€ƒæ—¥å¿—æ‘˜è¦è¯„ä¼°æŒ‡æ ‡ï¼Œè§£å†³äº†ä¼ ç»ŸæŒ‡æ ‡ä¾èµ–è¯æ±‡é‡å å’Œç¼ºä¹é«˜è´¨é‡å‚è€ƒæ‘˜è¦çš„é—®é¢˜ï¼Œä¸ºæ—¥å¿—æ‘˜è¦è¯„ä¼°æä¾›äº†å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ—¥å¿—æ‘˜è¦ç³»ç»Ÿè¯„ä¼°é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºç¼ºä¹é«˜è´¨é‡å‚è€ƒæ‘˜è¦ä»¥åŠç°æœ‰æŒ‡æ ‡ï¼ˆå¦‚ROUGEå’ŒBLEUï¼‰çš„å±€é™æ€§ï¼Œè¿™äº›æŒ‡æ ‡ä¾èµ–äºè¡¨é¢å±‚æ¬¡çš„è¯æ±‡é‡å ï¼Œæ— æ³•å‡†ç¡®åæ˜ æ‘˜è¦è´¨é‡ã€‚

**Method:** REFLEXä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºé›¶æ ·æœ¬è¯„ä¼°å™¨ï¼Œåœ¨æ— éœ€é»„é‡‘æ ‡å‡†å‚è€ƒæˆ–äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œä»ç›¸å…³æ€§ã€ä¿¡æ¯é‡å’Œè¿è´¯æ€§ç­‰ç»´åº¦è¯„ä¼°æ‘˜è¦è´¨é‡ï¼Œå®ç°äº†æ— å‚è€ƒçš„è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚

**Result:** å®éªŒè¡¨æ˜REFLEXåœ¨å¤šä¸ªæ—¥å¿—æ‘˜è¦æ•°æ®é›†ä¸Šäº§ç”Ÿç¨³å®šã€å¯è§£é‡Šä¸”ç»†ç²’åº¦çš„è¯„ä¼°ç»“æœï¼Œç›¸æ¯”ä¼ ç»ŸæŒ‡æ ‡èƒ½æ›´æœ‰æ•ˆåœ°åŒºåˆ†ä¸åŒæ¨¡å‹çš„è¾“å‡ºè´¨é‡ã€‚

**Conclusion:** REFLEXä¸ºç°å®åœºæ™¯ä¸­å‚è€ƒæ•°æ®ç¨€ç¼ºæˆ–ä¸å¯ç”¨çš„æƒ…å†µæä¾›äº†å¯æ‰©å±•çš„è¯„ä¼°è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†æ—¥å¿—æ‘˜è¦è¯„ä¼°æ–¹æ³•çš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free evaluation metric for log summarization based on large language model (LLM) judgment. REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations. We show that REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization dataset, and more effectively distinguishes model outputs than traditional metrics. REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [42] [Versatile and Risk-Sensitive Cardiac Diagnosis via Graph-Based ECG Signal Representation](https://arxiv.org/abs/2511.07973)
*Yue Wang, Yuyang Xu, Renjun Hu, Fanqi Shen, Hanyun Jiang, Jun Wang, Jintai Chen, Danny Z. Chen, Jian Wu, Haochao Ying*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºVARSæ–¹æ³•ï¼Œé€šè¿‡åŸºäºå›¾çš„è¡¨ç¤ºç»Ÿä¸€å»ºæ¨¡å¼‚æ„å¿ƒç”µå›¾ä¿¡å·ï¼Œè§£å†³äº†ä¼ ç»Ÿæ·±åº¦å­¦ä¹ å¿ƒç”µå›¾è¯Šæ–­æ–¹æ³•åœ¨å¤„ç†å¤šæ ·åŒ–ä¿¡å·é…ç½®å’Œæ£€æµ‹é£é™©ä¿¡å·æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ï¼Œå¹¶æ˜¾è‘—æé«˜äº†é£é™©ä¿¡å·çš„è¯†åˆ«èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºæ·±åº¦å­¦ä¹ çš„å¿ƒç”µå›¾è¯Šæ–­æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦éšœç¢ï¼šç¼ºä¹å¤„ç†å¤šæ ·åŒ–ä¿¡å·é…ç½®çš„é€šç”¨æ€§ï¼Œä»¥åŠç”±äºæ ·æœ¬ä¸å¹³è¡¡å¯¼è‡´é£é™©ä¿¡å·æ£€æµ‹ä¸è¶³ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†è¿™äº›æ–¹æ³•åœ¨ä¸´åºŠå®è·µä¸­çš„å¹¿æ³›åº”ç”¨ã€‚

**Method:** VARSé‡‡ç”¨åŸºäºå›¾çš„è¡¨ç¤ºæ–¹æ³•ï¼Œå°†å¿ƒç”µå›¾ä¿¡å·è½¬æ¢ä¸ºé€šç”¨çš„å›¾ç»“æ„ï¼Œæ•æ‰å…³é”®è¯Šæ–­ç‰¹å¾è€Œä¸å—å¯¼è”æ•°ã€é‡‡æ ·é¢‘ç‡å’ŒæŒç»­æ—¶é—´ç­‰ä¿¡å·å¤šæ ·æ€§çš„å½±å“ã€‚è¯¥æ–¹æ³•ç»“åˆå»å™ªé‡å»ºå’Œå¯¹æ¯”å­¦ä¹ ï¼Œåœ¨ä¿ç•™åŸå§‹å¿ƒç”µå›¾ä¿¡æ¯çš„åŒæ—¶çªå‡ºç—…ç†æ€§æ¨¡å¼ã€‚

**Result:** åœ¨ä¸‰ä¸ªä¸åŒçš„å¿ƒç”µå›¾æ•°æ®é›†ä¸Šçš„ä¸¥æ ¼è¯„ä¼°è¡¨æ˜ï¼ŒVARSåœ¨æ‰€æœ‰æ•°æ®é›†ä¸ŠæŒç»­è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ï¼Œå¹¶åœ¨è¯†åˆ«é£é™©ä¿¡å·æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚è¯¥æ–¹æ³•è¿˜æä¾›å¯è§£é‡Šæ€§ï¼Œèƒ½å¤Ÿç²¾ç¡®å®šä½å¯¼è‡´ç‰¹å®šæ¨¡å‹è¾“å‡ºçš„æ³¢å½¢ã€‚

**Conclusion:** VARSæœ‰æœ›æˆä¸ºå…¨é¢å¿ƒè„å¥åº·è¯„ä¼°çš„å®è´µå·¥å…·ï¼Œå…¶åŸºäºå›¾çš„è¡¨ç¤ºæ–¹æ³•ä¸ä»…æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§ï¼Œè¿˜å¢å¼ºäº†ä¸´åºŠå†³ç­–æ”¯æŒèƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ºå¤„ç†å¿ƒç”µå›¾ä¿¡å·å¤šæ ·æ€§é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨åŒ»ç–—AIé¢†åŸŸçš„å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚

---

#### ğŸ“„ Abstract
Despite the rapid advancements of electrocardiogram (ECG) signal diagnosis and analysis methods through deep learning, two major hurdles still limit their clinical adoption: the lack of versatility in processing ECG signals with diverse configurations, and the inadequate detection of risk signals due to sample imbalances. Addressing these challenges, we introduce VersAtile and Risk-Sensitive cardiac diagnosis (VARS), an innovative approach that employs a graph-based representation to uniformly model heterogeneous ECG signals. VARS stands out by transforming ECG signals into versatile graph structures that capture critical diagnostic features, irrespective of signal diversity in the lead count, sampling frequency, and duration. This graph-centric formulation also enhances diagnostic sensitivity, enabling precise localization and identification of abnormal ECG patterns that often elude standard analysis methods. To facilitate representation transformation, our approach integrates denoising reconstruction with contrastive learning to preserve raw ECG information while highlighting pathognomonic patterns. We rigorously evaluate the efficacy of VARS on three distinct ECG datasets, encompassing a range of structural variations. The results demonstrate that VARS not only consistently surpasses existing state-of-the-art models across all these datasets but also exhibits substantial improvement in identifying risk signals. Additionally, VARS offers interpretability by pinpointing the exact waveforms that lead to specific model outputs, thereby assisting clinicians in making informed decisions. These findings suggest that our VARS will likely emerge as an invaluable tool for comprehensive cardiac health assessment.


### [43] [National Institute on Aging PREPARE Challenge: Early Detection of Cognitive Impairment Using Speech - The SpeechCARE Solution](https://arxiv.org/abs/2511.08132)
*Maryam Zolnoori, Hossein Azadmaleki, Yasaman Haghbin, Ali Zolnour, Mohammad Javad Momeni Nezhad, Sina Rashidi, Mehdi Naserian, Elyas Esmaeili, Sepehr Karimi Arpanahi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†SpeechCAREï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€è¯­éŸ³å¤„ç†çš„æ–°å‹è®¤çŸ¥éšœç¢æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡æ··åˆä¸“å®¶æ¶æ„èåˆå£°å­¦ã€è¯­è¨€å­¦å’Œäººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼Œåœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…å’Œç›¸å…³ç—´å‘†ç—‡çš„æ—©æœŸæ£€æµ‹ä¸­å®ç°äº†ä¼˜å¼‚çš„åˆ†ç±»æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é˜¿å°”èŒ¨æµ·é»˜ç—…å’Œç›¸å…³ç—´å‘†ç—‡å½±å“è¶…è¿‡60å²äººç¾¤çš„äº”åˆ†ä¹‹ä¸€ï¼Œä½†è¶…è¿‡ä¸€åŠçš„è®¤çŸ¥è¡°é€€æ‚£è€…æœªè¢«è¯Šæ–­ã€‚ç°æœ‰åŸºäºè¯­éŸ³çš„è¯„ä¼°æ–¹æ³•å­˜åœ¨æ€§èƒ½æœ‰é™å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œä¼ ç»Ÿè¯­éŸ³å¤„ç†æµç¨‹ä½¿ç”¨æ‰‹å·¥ç‰¹å¾æˆ–é€šç”¨éŸ³é¢‘åˆ†ç±»å™¨éš¾ä»¥æ•æ‰è®¤çŸ¥éšœç¢ç›¸å…³çš„ç»†å¾®è¯­éŸ³å˜åŒ–ã€‚

**Method:** SpeechCAREé‡‡ç”¨å¤šæ¨¡æ€è¯­éŸ³å¤„ç†æµç¨‹ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€å£°å­¦å’Œè¯­è¨€è½¬æ¢å™¨æ¨¡å‹æ•è·è®¤çŸ¥éšœç¢ç›¸å…³çš„è¯­éŸ³çº¿ç´¢ã€‚å—æ··åˆä¸“å®¶èŒƒå¼å¯å‘ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŠ¨æ€èåˆæ¶æ„åŠ æƒå¤„ç†åŸºäºè½¬æ¢å™¨çš„å£°å­¦ã€è¯­è¨€å­¦å’Œäººå£ç»Ÿè®¡å­¦è¾“å…¥ï¼Œå¹¶åŒ…å«è‡ªåŠ¨è½¬å½•ã€åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹å’Œä»»åŠ¡è¯†åˆ«ç­‰é²æ£’é¢„å¤„ç†æ­¥éª¤ï¼Œä»¥åŠåŸºäºSHAPçš„å¯è§£é‡Šæ€§æ¨¡å—ã€‚

**Result:** SpeechCAREåœ¨è®¤çŸ¥å¥åº·ã€è½»åº¦è®¤çŸ¥éšœç¢å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°AUC=0.88å’ŒF1=0.72ï¼Œåœ¨è½»åº¦è®¤çŸ¥éšœç¢æ£€æµ‹ä¸­è¾¾åˆ°AUC=0.90å’ŒF1=0.62ã€‚åå€šåˆ†ææ˜¾ç¤ºé™¤80å²ä»¥ä¸Šæˆå¹´äººå¤–å·®å¼‚æœ€å°ï¼Œé€šè¿‡è¿‡é‡‡æ ·å’ŒåŠ æƒæŸå¤±æŠ€æœ¯è¿›è¡Œäº†ç¼“è§£ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€è¯­éŸ³åˆ†æåœ¨è®¤çŸ¥éšœç¢æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŠ¨æ€èåˆæ¶æ„æ”¯æŒé¢å¤–æ¨¡æ€çš„é›†æˆå¹¶å¢å¼ºäº†è·¨ä»»åŠ¡çš„é²æ£’æ€§ã€‚æœªæ¥å·¥ä½œåŒ…æ‹¬åœ¨çœŸå®æŠ¤ç†ç¯å¢ƒä¸­éƒ¨ç½²ã€ä¸ç”µå­å¥åº·è®°å½•é›†æˆçš„å¯è§£é‡Šæ€§åˆ†æï¼Œä»¥åŠå…³æ³¨çº½çº¦å¸‚ä»£è¡¨æ€§ä¸è¶³äººç¾¤çš„åº”ç”¨æ‰©å±•ã€‚

---

#### ğŸ“„ Abstract
Alzheimer's disease and related dementias (ADRD) affect one in five adults over 60, yet more than half of individuals with cognitive decline remain undiagnosed. Speech-based assessments show promise for early detection, as phonetic motor planning deficits alter acoustic features (e.g., pitch, tone), while memory and language impairments lead to syntactic and semantic errors. However, conventional speech-processing pipelines with hand-crafted features or general-purpose audio classifiers often exhibit limited performance and generalizability. To address these limitations, we introduce SpeechCARE, a multimodal speech processing pipeline that leverages pretrained, multilingual acoustic and linguistic transformer models to capture subtle speech-related cues associated with cognitive impairment. Inspired by the Mixture of Experts (MoE) paradigm, SpeechCARE employs a dynamic fusion architecture that weights transformer-based acoustic, linguistic, and demographic inputs, allowing integration of additional modalities (e.g., social factors, imaging) and enhancing robustness across diverse tasks. Its robust preprocessing includes automatic transcription, large language model (LLM)-based anomaly detection, and task identification. A SHAP-based explainability module and LLM reasoning highlight each modality's contribution to decision-making. SpeechCARE achieved AUC = 0.88 and F1 = 0.72 for classifying cognitively healthy, MCI, and AD individuals, with AUC = 0.90 and F1 = 0.62 for MCI detection. Bias analysis showed minimal disparities, except for adults over 80. Mitigation techniques included oversampling and weighted loss. Future work includes deployment in real-world care settings (e.g., VNS Health, Columbia ADRC) and EHR-integrated explainability for underrepresented populations in New York City.


### [44] [oboro: Text-to-Image Synthesis on Limited Data using Flow-based Diffusion Transformer with MMH Attention](https://arxiv.org/abs/2511.08168)
*Ryusuke Mizutani, Kazuaki Matano, Tsugumi Kadowaki, Haruki Tenya, Layris, nuigurumi, Koki Hashimoto, Yu Tanaka*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶å¼€å‘äº†æ—¥æœ¬é¦–ä¸ªå¼€æºå•†ä¸šå›¾åƒç”Ÿæˆæ¨¡å‹'oboro:'ï¼Œè¯¥æ¨¡å‹å®Œå…¨ä»é›¶å¼€å§‹è®­ç»ƒï¼Œä»…ä½¿ç”¨ç‰ˆæƒæ¸…ç†å›¾åƒï¼Œæ—¨åœ¨è§£å†³æ—¥æœ¬åŠ¨ç”»äº§ä¸šåŠ³åŠ¨åŠ›çŸ­ç¼ºé—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é¡¹ç›®æ—¨åœ¨è§£å†³æ—¥æœ¬åŠ¨ç”»åˆ¶ä½œè¡Œä¸šé¢ä¸´çš„åŠ³åŠ¨åŠ›çŸ­ç¼ºç­‰æŒ‘æˆ˜ï¼Œé€šè¿‡å¼€å‘ä»é›¶å¼€å§‹çš„å›¾åƒç”Ÿæˆæ¨¡å‹æ¥æ”¯æŒå›½å†…åˆ›æ„äº§ä¸šï¼Œè¿™æ˜¯æ—¥æœ¬é¦–ä¸ªé¢å‘å•†ä¸šåº”ç”¨çš„å¼€æºå›¾åƒç”ŸæˆAIé¡¹ç›®ã€‚

**Method:** å¼€å‘äº†æ–°å‹å›¾åƒç”Ÿæˆæ¨¡å‹'oboro:'ï¼Œé‡‡ç”¨ä¸“ä¸ºæœ‰é™æ•°æ®é›†è®¾è®¡çš„æ–°æ¶æ„ï¼Œèƒ½å¤Ÿä»å°‘é‡è®­ç»ƒæ•°æ®ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œæ‰€æœ‰è®­ç»ƒå›¾åƒå‡ç»è¿‡ç‰ˆæƒæ¸…ç†ç¡®ä¿åˆè§„æ€§ã€‚

**Result:** æˆåŠŸå¼€å‘å¹¶å…¬å¼€å‘å¸ƒäº†'oboro:'åŸºç¡€æ¨¡å‹æƒé‡å’Œæ¨ç†ä»£ç ï¼Œè¿™æ˜¯æ—¥æœ¬é¦–ä¸ªå®Œå…¨è‡ªä¸»ç ”å‘çš„å¼€æºå•†ä¸šå›¾åƒç”ŸæˆAIï¼Œä¸ºå›½å†…AIç”Ÿæ€ç³»ç»Ÿæä¾›äº†é‡è¦æŠ€æœ¯èµ„æºã€‚

**Conclusion:** è¯¥é¡¹ç›®é€šè¿‡ä¿æŒå¼€å‘è¿‡ç¨‹é€æ˜åº¦ï¼Œä¸ºæ—¥æœ¬AIç ”ç©¶ç¤¾åŒºåšå‡ºè´¡çŒ®ï¼Œä¿ƒè¿›äº†å›½å†…AIå¼€å‘ç”Ÿæ€ç³»ç»Ÿçš„å‘å±•ï¼Œæ ‡å¿—ç€æ—¥æœ¬åœ¨ç”Ÿæˆå¼AIé¢†åŸŸçš„é‡è¦é‡Œç¨‹ç¢‘ã€‚

---

#### ğŸ“„ Abstract
This project was conducted as a 2nd-term adopted project of the "Post-5G Information and Communication System Infrastructure Enhancement R&D Project Development of Competitive Generative AI Foundation Models (GENIAC)," a business of the Ministry of Economy, Trade and Industry (METI) and the New Energy and Industrial Technology Development Organization (NEDO). To address challenges such as labor shortages in Japan's anime production industry, this project aims to develop an image generation model from scratch. This report details the technical specifications of the developed image generation model, "oboro:." We have developed "oboro:," a new image generation model built from scratch, using only copyright-cleared images for training. A key characteristic is its architecture, designed to generate high-quality images even from limited datasets. The foundation model weights and inference code are publicly available alongside this report. This project marks the first release of an open-source, commercially-oriented image generation AI fully developed in Japan. AiHUB originated from the OSS community; by maintaining transparency in our development process, we aim to contribute to Japan's AI researcher and engineer community and promote the domestic AI development ecosystem.


### [45] [An Efficient Training Pipeline for Reasoning Graphical User Interface Agents](https://arxiv.org/abs/2511.08172)
*Georgios Pantazopoulos, Eda B. Ã–zyiÄŸit*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§†è§‰å®šä½è®­ç»ƒæµç¨‹ï¼Œé€šè¿‡æ¨¡å‹é©±åŠ¨çš„æ•°æ®è¿‡æ»¤å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œä»480ä¸‡åˆæˆæ ·æœ¬ä¸­ç­›é€‰å‡º1.2ä¸‡ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­åŒ¹é…æˆ–è¶…è¶Šäº†æ›´å¤§è§„æ¨¡åŸºçº¿æ¨¡å‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰å®šä½æ–¹æ³•ä¸¥é‡ä¾èµ–å¤§è§„æ¨¡å™ªå£°åˆæˆæ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†å›¾å½¢ç”¨æˆ·ç•Œé¢æ™ºèƒ½ä»£ç†çš„æ¨ç†èƒ½åŠ›å‘å±•ï¼Œéœ€è¦æ›´é«˜æ•ˆçš„æ•°æ®ç­›é€‰å’Œè®­ç»ƒç­–ç•¥æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

**Method:** é‡‡ç”¨æ¨¡å‹é©±åŠ¨çš„æ•°æ®è¿‡æ»¤æ–¹æ³•ï¼Œé¦–å…ˆè¯†åˆ«æŒ‘æˆ˜æ€§æ¡ˆä¾‹ï¼Œç§»é™¤æœªå¯¹é½æ ·æœ¬ï¼Œç„¶åé€‰æ‹©å¤šæ ·åŒ–çš„å¤šæ¨¡æ€å®ä¾‹ï¼›åœ¨ç­›é€‰æ•°æ®ä¸Šå¯¹30äº¿å‚æ•°è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œç›‘ç£å¾®è°ƒã€æ€ç»´é“¾å¢å¼ºå¾®è°ƒå’ŒåŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ ä¸‰ç§è®­ç»ƒç­–ç•¥ã€‚

**Result:** åœ¨ScreenSpotã€Multimodal-Mind2Webå’ŒAndroidControlç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨è¿‡æ»¤æ•°æ®å’Œè½»é‡çº§è®­ç»ƒç­–ç•¥çš„æ¨¡å‹åŒ¹é…æˆ–è¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†é«˜æ•ˆæ•°æ®ç­›é€‰çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼ŒåŸåˆ™æ€§çš„æ•°æ®ç­›é€‰å’Œé²æ£’çš„é€‚åº”ç­–ç•¥å¯ä»¥åª²ç¾å¤§è§„æ¨¡è®­ç»ƒï¼Œèƒ½å¤Ÿå¼€å‘å‡ºç´§å‡‘ä½†å…·å¤‡å¼ºå¤§èƒ½åŠ›çš„å¤šæ¨¡æ€æ¨ç†æ™ºèƒ½ä½“ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ¨¡å‹éƒ¨ç½²æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Visual grounding is the task of localising image regions from natural language queries and is critical for reasoning capable Graphical User Interface agents. Many existing methods rely on massive, noisy synthetic datasets.This work introduces an efficient training pipeline that combines model-based data filtering with parameter-efficient fine-tuning. From 4.8M synthetic examples, 12K clean and diverse instances are curated by first identifying challenging cases, removing misaligned and then selecting a diverse set of multimodal instances. On this data, a 3B-parameter Vision-Language Model is trained under three regimes: supervised fine-tuning, chain-of-thought- augmented fine-tuning, and reinforcement learning via Group Relative Policy Optimization. Models trained with the filtered data and lightweight training strategies match or surpass larger baselines on benchmarks such as ScreenSpot, Multimodal-Mind2Web, and AndroidControl. These results demonstrate that principled data curation and robust adaptation can rival large-scale training, enabling compact yet capable multimodal reasoning agents.


### [46] [Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning](https://arxiv.org/abs/2511.08246)
*Ziyu Ma, Chenhui Gou, Yiming Hu, Yong Wang, Xiangxiang Chu, Bohan Zhuang, Jianfei Cai*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•æ„Ÿæ€§æ„ŸçŸ¥ä»»åŠ¡å‘é‡æ’å…¥æ¡†æ¶ï¼ˆSTVï¼‰ï¼Œé€šè¿‡è¯†åˆ«æ¿€æ´»å·®å¼‚çš„ç»“æ„æ¨¡å¼æ¥ç¡®å®šä»»åŠ¡å‘é‡çš„æ’å…¥ä½ç½®å’Œå†…å®¹ï¼Œè§£å†³äº†å¤šæ¨¡æ€æ¨¡å‹ä¸­å¤šæ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ é¢ä¸´çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶å’Œæ¨ç†æˆæœ¬é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤šæ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­é¢ä¸´ä¸Šä¸‹æ–‡é•¿åº¦æœ‰é™å’Œæ¨ç†æˆæœ¬é«˜çš„é—®é¢˜ï¼Œç°æœ‰åŸºäºä»»åŠ¡å‘é‡çš„æ–¹æ³•è¦ä¹ˆå¿½ç•¥äº†æ’å…¥ä½ç½®çš„é‡è¦æ€§ï¼Œè¦ä¹ˆéš¾ä»¥ç¡®å®šæ¯ä¸ªä½ç½®åˆé€‚çš„æ’å…¥å€¼ã€‚

**Method:** æå‡ºæ•æ„Ÿæ€§æ„ŸçŸ¥ä»»åŠ¡å‘é‡æ’å…¥æ¡†æ¶ï¼ˆSTVï¼‰ï¼Œé€šè¿‡åˆ†ææŸ¥è¯¢-ä¸Šä¸‹æ–‡å¯¹ä¹‹é—´çš„æ¿€æ´»å·®å¼‚ç»“æ„æ¨¡å¼æ¥è¯†åˆ«æ•æ„Ÿä½ç½®ï¼Œä¸ºæ¯ä¸ªä½ç½®æ„å»ºé¢„èšç±»æ¿€æ´»åº“ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ é€‰æ‹©æœ€åˆé€‚çš„æ’å…¥å€¼ã€‚

**Result:** åœ¨å¤šç§å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚Qwen-VLã€Idefics-2ï¼‰å’Œä»»åŠ¡ï¼ˆå¦‚VizWizã€OK-VQAï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSTVç›¸æ¯”ä¹‹å‰çš„ä»»åŠ¡å‘é‡æ–¹æ³•å…·æœ‰ä¸€è‡´æ”¹è¿›å’Œå¼ºæ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** æ¿€æ´»å·®å¼‚çš„ç»“æ„æ¨¡å¼ä¸ºä»»åŠ¡å‘é‡æ’å…¥æä¾›äº†å¯é çº¿ç´¢ï¼ŒSTVæ¡†æ¶é€šè¿‡ç»“åˆæ•æ„Ÿæ€§åˆ†æå’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œåœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­å®ç°äº†æ›´æœ‰æ•ˆçš„ä»»åŠ¡å‘é‡æ’å…¥ç­–ç•¥ã€‚

---

#### ğŸ“„ Abstract
Large Multimodal Models (LMMs) have shown promising in-context learning (ICL) capabilities, but scaling to many-shot settings remains difficult due to limited context length and high inference cost. To address these challenges, task-vector-based methods have been explored by inserting compact representations of many-shot in-context demonstrations into model activations. However, existing task-vector-based methods either overlook the importance of where to insert task vectors or struggle to determine suitable values for each location. To this end, we propose a novel Sensitivity-aware Task Vector insertion framework (STV) to figure out where and what to insert. Our key insight is that activation deltas across query-context pairs exhibit consistent structural patterns, providing a reliable cue for insertion. Based on the identified sensitive-aware locations, we construct a pre-clustered activation bank for each location by clustering the activation values, and then apply reinforcement learning to choose the most suitable one to insert. We evaluate STV across a range of multimodal models (e.g., Qwen-VL, Idefics-2) and tasks (e.g., VizWiz, OK-VQA), demonstrating its effectiveness and showing consistent improvements over previous task-vector-based methods with strong generalization.


### [47] [FaithAct: Faithfulness Planning and Acting in MLLMs](https://arxiv.org/abs/2511.08409)
*Junxian Li, Xinyue Xu, Sai Ma, Sichao Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†FaithEvalè¯„ä¼°æ¡†æ¶å’ŒFaithActè§„åˆ’æ‰§è¡Œæ¡†æ¶ï¼Œé€šè¿‡åŒºåˆ†è¡Œä¸ºå¿ å®æ€§å’Œæ„ŸçŸ¥å¿ å®æ€§ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼ºåˆ¶è¯æ®åŸºç¡€ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€æ¨ç†çš„å¿ å®æ€§è€Œä¸é™ä½ä»»åŠ¡å‡†ç¡®ç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨å¿ å®æ€§é—®é¢˜ï¼Œç»å¸¸äº§ç”Ÿçœ‹ä¼¼åˆç†ä½†ç¼ºä¹ä¾æ®çš„æ¨ç†é“¾ï¼Œè¿™äº›æ¨ç†é“¾ä¸æ„ŸçŸ¥è¯æ®æˆ–æœ€ç»ˆç»“è®ºå­˜åœ¨åˆ†æ­§ï¼Œå› æ­¤éœ€è¦è§£å†³æ¨ç†è¿‡ç¨‹ä¸è¾“å…¥è¯æ®ä¹‹é—´çš„å¯¹é½é—®é¢˜ã€‚

**Method:** æå‡ºäº†FaithEvalè¯„ä¼°æ¡†æ¶ç”¨äºé‡åŒ–æ­¥éª¤çº§å’Œé“¾çº§å¿ å®æ€§ï¼Œè¯„ä¼°æ¯ä¸ªå£°ç§°å¯¹è±¡æ˜¯å¦å¾—åˆ°å›¾åƒè§†è§‰æ”¯æŒï¼›å¹¶å¼€å‘äº†FaithActæ¡†æ¶ï¼Œä»¥å¿ å®æ€§ä¸ºé¦–è¦åŸåˆ™çš„è§„åˆ’å’Œæ‰§è¡Œæ–¹æ³•ï¼Œåœ¨æ¯ä¸€æ­¥æ¨ç†ä¸­å¼ºåˆ¶è¯æ®åŸºç¡€ã€‚

**Result:** åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFaithActç›¸æ¯”åŸºäºæç¤ºå’Œå·¥å…·å¢å¼ºçš„åŸºçº¿æ–¹æ³•ï¼Œå°†æ„ŸçŸ¥å¿ å®æ€§æå‡äº†é«˜è¾¾26%ï¼ŒåŒæ—¶ä¸é™ä½ä»»åŠ¡å‡†ç¡®ç‡ï¼Œå¹¶äº§ç”Ÿäº†æ›´ç¨³å®šçš„æ¨ç†è½¨è¿¹ã€‚

**Conclusion:** å°†å¿ å®æ€§ä½œä¸ºæŒ‡å¯¼åŸåˆ™ä¸ä»…èƒ½å¤Ÿå‡è½»å¹»è§‰é—®é¢˜ï¼Œè¿˜èƒ½äº§ç”Ÿæ›´ç¨³å®šçš„æ¨ç†è½¨è¿¹ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†ä¸­çš„å¿ å®æ€§è¯„ä¼°å’Œæ‰§è¡Œå»ºç«‹äº†ç»Ÿä¸€æ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.


### [48] [Simulating the Visual World with Artificial Intelligence: A Roadmap](https://arxiv.org/abs/2511.08585)
*Jingtong Yue, Ziqi Huang, Zhaoxi Chen, Xintao Wang, Pengfei Wan, Ziwei Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡ç³»ç»Ÿç»¼è¿°äº†è§†é¢‘ç”Ÿæˆå‘è§†é¢‘åŸºç¡€æ¨¡å‹çš„æ¼”è¿›ï¼Œæå‡ºå°†ç°ä»£è§†é¢‘åŸºç¡€æ¨¡å‹æ¦‚å¿µåŒ–ä¸ºéšå¼ä¸–ç•Œæ¨¡å‹å’Œè§†é¢‘æ¸²æŸ“å™¨çš„ç»„åˆï¼Œæ—¨åœ¨æ„å»ºå…·å¤‡ç‰©ç†åˆç†æ€§å’Œäº¤äº’èƒ½åŠ›çš„è™šæ‹Ÿç¯å¢ƒã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†é¢‘ç”Ÿæˆé¢†åŸŸæ­£ä»å•çº¯ç”Ÿæˆè§†è§‰å¸å¼•åŠ›çš„ç‰‡æ®µè½¬å‘æ„å»ºæ”¯æŒäº¤äº’å¹¶ä¿æŒç‰©ç†åˆç†æ€§çš„è™šæ‹Ÿç¯å¢ƒï¼Œè¿™æŒ‡å‘äº†è§†é¢‘åŸºç¡€æ¨¡å‹çš„å‘å±•ï¼Œè¿™äº›æ¨¡å‹ä¸ä»…ä½œä¸ºè§†è§‰ç”Ÿæˆå™¨ï¼Œè¿˜ä½œä¸ºèƒ½å¤Ÿæ¨¡æ‹Ÿç‰©ç†åŠ¨åŠ›å­¦ã€æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’å’Œä»»åŠ¡è§„åˆ’çš„éšå¼ä¸–ç•Œæ¨¡å‹ã€‚

**Method:** ç ”ç©¶å°†ç°ä»£è§†é¢‘åŸºç¡€æ¨¡å‹æ¦‚å¿µåŒ–ä¸ºä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶çš„ç»„åˆï¼šéšå¼ä¸–ç•Œæ¨¡å‹å’Œè§†é¢‘æ¸²æŸ“å™¨ã€‚ä¸–ç•Œæ¨¡å‹ç¼–ç å…³äºä¸–ç•Œçš„ç»“æ„åŒ–çŸ¥è¯†ï¼ŒåŒ…æ‹¬ç‰©ç†å®šå¾‹ã€äº¤äº’åŠ¨åŠ›å­¦å’Œæ™ºèƒ½ä½“è¡Œä¸ºï¼Œä½œä¸ºæ½œåœ¨æ¨¡æ‹Ÿå¼•æ“ï¼›è§†é¢‘æ¸²æŸ“å™¨åˆ™å°†è¿™ç§æ½œåœ¨æ¨¡æ‹Ÿè½¬æ¢ä¸ºé€¼çœŸçš„è§†è§‰è§‚å¯Ÿã€‚

**Result:** ç ”ç©¶è¿½è¸ªäº†è§†é¢‘ç”Ÿæˆçš„å››ä»£æ¼”è¿›è¿‡ç¨‹ï¼Œæ¯ä»£æ ¸å¿ƒèƒ½åŠ›é€æ­¥æå‡ï¼Œæœ€ç»ˆå½¢æˆå…·å¤‡å†…åœ¨ç‰©ç†åˆç†æ€§ã€å®æ—¶å¤šæ¨¡æ€äº¤äº’ä»¥åŠè·¨å¤šä¸ªæ—¶ç©ºå°ºåº¦è§„åˆ’èƒ½åŠ›çš„ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶åˆ†æäº†åœ¨æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶å’Œäº¤äº’æ¸¸æˆç­‰é¢†åŸŸçš„åº”ç”¨ã€‚

**Conclusion:** è§†é¢‘åŸºç¡€æ¨¡å‹çš„å‘å±•æ ‡å¿—ç€å‘æ„å»ºä½œä¸ºéšå¼ä¸–ç•Œæ¨¡å‹çš„ç³»ç»Ÿçš„è½¬å˜ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿå¤æ‚çš„ç‰©ç†å’Œäº¤äº’åŠ¨æ€ï¼Œä¸ºä¸‹ä¸€ä»£ä¸–ç•Œæ¨¡å‹çš„è®¾è®¡åŸåˆ™å’ŒæŒ‘æˆ˜æä¾›äº†é‡è¦è§è§£ï¼ŒåŒ…æ‹¬æ™ºèƒ½ä½“æ™ºèƒ½åœ¨å¡‘é€ å’Œè¯„ä¼°è¿™äº›ç³»ç»Ÿä¸­çš„ä½œç”¨ã€‚

---

#### ğŸ“„ Abstract
The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a "window" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.
