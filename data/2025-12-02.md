<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 92]
- [cs.CL](#cs.CL) [Total: 19]
- [cs.AI](#cs.AI) [Total: 12]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SO-Bench: A Structural Output Evaluation of Multimodal LLMs](https://arxiv.org/abs/2511.21750)
*Di Feng, Kaixin Ma, Feng Nan, Haofeng Chen, Bohan Zhai, David Griffiths, Mingfei Gao, Zhe Gan, Eshan Verma, Yinfei Yang, Zhifeng Chen, Afshin Dehghan*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†SO-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç¼ºä¹ç³»ç»Ÿæ€§è¯„ä¼°åŸºå‡†çš„ç©ºç™½ï¼Œå¹¶é€šè¿‡è®­ç»ƒå®éªŒæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç»“æ„åŒ–è¾“å‡ºæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡æ–‡æœ¬é¢†åŸŸçš„ç»“æ„åŒ–ç”Ÿæˆå·²å–å¾—è¿›å±•ï¼Œä½†ç›®å‰ç¼ºä¹èƒ½å¤Ÿç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¾“å…¥ä¸Šè¿›è¡Œæ¨¡å¼é©±åŠ¨çš„ä¿¡æ¯æå–å’Œæ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œè€Œç°å®ä¸–ç•Œä¸­éƒ¨ç½²çš„MLLMéœ€è¦ç”Ÿæˆæ—¢æ­£ç¡®åˆç¬¦åˆé¢„å®šä¹‰æ•°æ®æ¨¡å¼çš„è¾“å‡ºã€‚

**Method:** æœ¬ç ”ç©¶è®¾è®¡äº†SO-BenchåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–UIç•Œé¢ã€è‡ªç„¶å›¾åƒã€æ–‡æ¡£å’Œå›¾è¡¨å››ä¸ªè§†è§‰é¢†åŸŸï¼ŒåŸºäºè¶…è¿‡6,500ä¸ªå¤šæ ·åŒ–JSONæ¨¡å¼å’Œ1,800ä¸ªäººå·¥éªŒè¯è´¨é‡çš„å›¾åƒ-æ¨¡å¼å¯¹æ„å»ºï¼Œå¹¶è¿›è¡Œäº†åŸºå‡†æµ‹è¯•å’Œè®­ç»ƒå®éªŒä»¥æå‡æ¨¡å‹çš„ç»“æ„åŒ–è¾“å‡ºèƒ½åŠ›ã€‚

**Result:** å¯¹å¼€æºå’Œå‰æ²¿ä¸“æœ‰æ¨¡å‹çš„åŸºå‡†æµ‹è¯•æ­ç¤ºäº†åœ¨é¢„æµ‹å‡†ç¡®ã€ç¬¦åˆæ¨¡å¼è¦æ±‚çš„è¾“å‡ºæ–¹é¢å­˜åœ¨æŒç»­å·®è·ï¼Œçªæ˜¾äº†æ”¹è¿›å¤šæ¨¡æ€ç»“æ„åŒ–æ¨ç†çš„å¿…è¦æ€§ï¼ŒåŒæ—¶è®­ç»ƒå®éªŒæ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç»“æ„åŒ–è¾“å‡ºèƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ä¸è¶³ï¼Œæå‡ºçš„SO-BenchåŸºå‡†ä¸ºç³»ç»Ÿè¯„ä¼°æä¾›äº†æ ‡å‡†ï¼Œè®­ç»ƒå®éªŒè¡¨æ˜æ¨¡å‹æ€§èƒ½å¯é€šè¿‡ä¸“é—¨è®­ç»ƒæ˜¾è‘—æå‡ï¼Œä¸ºæœªæ¥æ”¹è¿›å¤šæ¨¡æ€ç»“æ„åŒ–æ¨ç†æä¾›äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.


### [2] [PathReasoning: A multimodal reasoning agent for query-based ROI navigation on whole-slide images](https://arxiv.org/abs/2511.21902)
*Kunpeng Zhang, Hanwen Xu, Sheng Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PathReasoningï¼Œä¸€ç§å¤šæ¨¡æ€æ¨ç†æ™ºèƒ½ä½“ï¼Œé€šè¿‡è¿­ä»£æ¨ç†å’Œç»†åŒ–åœ¨WSIä¸­å¯¼èˆªï¼Œå°†æ•´ä¸ªç—…ç†åˆ‡ç‰‡è½¬æ¢ä¸ºé—®é¢˜å¼•å¯¼çš„è§†å›¾åºåˆ—ï¼Œä»è€Œé«˜æ•ˆå®šä½è¯Šæ–­ç›¸å…³åŒºåŸŸï¼Œæ— éœ€å¯†é›†åƒç´ çº§æ ‡æ³¨ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰è™½ç„¶æä¾›äº†ç™Œç—‡çš„å…¨é¢ä¿¡æ¯ï¼Œä½†å…¶å·¨å¤§çš„å°ºå¯¸ï¼ˆå¯è¾¾100äº¿åƒç´ ï¼‰ä½¿å¾—å¯¼èˆªåˆ°ç›¸åº”åŒºåŸŸä»¥æ”¯æŒå¤šæ ·åŒ–ä¸´åºŠæ£€æŸ¥å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ä¸”è€—æ—¶ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥é«˜æ•ˆå®šä½è¯Šæ–­ç›¸å…³åŒºåŸŸï¼Œè€Œç—…ç†å­¦å®¶é€šå¸¸é€šè¿‡é‡‡æ ·ã€æ¨ç†å’Œè‡ªåæ€çš„ç»„åˆåœ¨WSIä¸­è¿›è¡Œå¯¼èˆªï¼Œè¿™å¯å‘äº†æœ¬ç ”ç©¶ã€‚

**Method:** PathReasoningæ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨ç†æ™ºèƒ½ä½“ï¼Œé€šè¿‡å¤šè½®æ¨ç†å’Œç»†åŒ–è¿­ä»£å¯¼èˆªWSIã€‚è¯¥æ–¹æ³•ä»éšæœºé‡‡æ ·çš„å€™é€‰åŒºåŸŸå¼€å§‹ï¼Œé€šè¿‡è‡ªåæ€è¯„ä¼°å½“å‰é€‰æ‹©ï¼Œæ¨ç†è§†è§‰è§‚å¯Ÿä¸ä¸´åºŠé—®é¢˜ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå¹¶æœ€ç»ˆæå‡ºæ–°çš„æ¢ç´¢åŒºåŸŸã€‚é€šè¿‡æ„å»ºé€æ¸å°†æ³¨æ„åŠ›å¼•å¯¼åˆ°è¯Šæ–­ç›¸å…³åŒºåŸŸçš„æ¨ç†é“¾ï¼Œå°†æ•´ä¸ªåˆ‡ç‰‡è½¬æ¢ä¸ºé—®é¢˜å¼•å¯¼çš„è§†å›¾åºåˆ—ã€‚

**Result:** PathReasoningåœ¨äºšå‹åˆ†ç±»å’Œçºµå‘åˆ†æä»»åŠ¡ä¸Šåˆ†åˆ«æ¯”å¼ºå¤§çš„ROIé€‰æ‹©æ–¹æ³•é«˜å‡º6.7%å’Œ3.1%çš„AUROCã€‚é«˜è´¨é‡ROIè¿›ä¸€æ­¥æ”¯æŒä¹³è…ºç™Œçš„å‡†ç¡®æŠ¥å‘Šç”Ÿæˆï¼Œåœ¨å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†GPT-4oè¾¾10%ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å›ºå®šæ­¥æ•°å†…é«˜æ•ˆæ‰¾åˆ°ä¿¡æ¯ä¸°å¯Œçš„ROIï¼Œæ— éœ€å¯†é›†åƒç´ çº§æ ‡æ³¨ã€‚

**Conclusion:** PathReasoningä¼˜å…ˆè€ƒè™‘é—®é¢˜ç‰¹å®šåŒºåŸŸå¹¶æ„å»ºå¯è§£é‡Šçš„æ¨ç†é“¾ï¼Œæ”¯æŒæ•°å­—ç—…ç†å­¦ä¸­çš„é«˜æ•ˆåˆ‡ç‰‡å®¡æŸ¥ã€ä¸€è‡´è¯Šæ–­è§£é‡Šã€å…¨é¢æŠ¥å‘Šå’Œè¯æ®å¯è¿½æº¯æ€§ã€‚è¯¥æ–¹æ³•æ¨¡ä»¿ç—…ç†å­¦å®¶çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œä¸ºå¤§è§„æ¨¡WSIåˆ†ææä¾›äº†æœ‰æ•ˆçš„å¯¼èˆªæ¡†æ¶ï¼Œæ˜¾è‘—æé«˜äº†è¯Šæ–­æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

---

#### ğŸ“„ Abstract
Deciphering tumor microenvironment from Whole Slide Images (WSIs) is intriguing as it is key to cancer diagnosis, prognosis and treatment response. While these gigapixel images on one hand offer a comprehensive portrait of cancer, on the other hand, the extremely large size, as much as more than 10 billion pixels, make it challenging and time-consuming to navigate to corresponding regions to support diverse clinical inspection. Inspired by pathologists who conducted navigation on WSIs with a combination of sampling, reasoning and self-reflection, we proposed "PathReasoning", a multi-modal reasoning agent that iteratively navigates across WSIs through multiple rounds of reasoning and refinements. Specifically, starting with randomly sampled candidate regions, PathReasoning reviews current selections with self-reflection, reasoning over the correspondence between visual observations and clinical questions, and concludes by proposing new regions to explore. Across rounds, PathReasoning builds a reasoning chain that gradually directs attention to diagnostically relevant areas. PathReasoning turns each whole slide into a sequence of question-guided views, allowing the model to efficiently find informative ROIs within a fixed number of steps, without the need for dense pixel-level annotations. PathReasoning can substantially outperform strong ROI-selection approaches by 6.7% and 3.1% of AUROC on subtyping and longitudinal analysis tasks. The high-quality ROIs further support accurate report generation on breast cancer, significantly outperforming the standard GPT-4o by 10% in accuracy. PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, supporting efficient slide review, consistent diagnostic interpretations, comprehensive reporting, and evidence traceability in digital pathology.


### [3] [Interpretable Multimodal Cancer Prototyping with Whole Slide Images and Incompletely Paired Genomics](https://arxiv.org/abs/2511.21937)
*Yupei Zhang, Yating Huang, Wanming Hu, Lequan Yu, Hujun Yin, Chao Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§çµæ´»çš„å¤šæ¨¡æ€åŸå‹æ¡†æ¶ï¼Œç”¨äºæ•´åˆå…¨åˆ‡ç‰‡å›¾åƒå’Œä¸å®Œæ•´çš„åŸºå› ç»„æ•°æ®ä»¥æ”¯æŒç²¾å‡†è‚¿ç˜¤å­¦ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿç‰©åŸå‹æ„å»ºã€å¤šè§†å›¾å¯¹é½ã€äºŒåˆ†èåˆå’Œè¯­ä¹‰åŸºå› ç»„æ’è¡¥ç­‰å…³é”®ç»„ä»¶ï¼Œæœ‰æ•ˆè§£å†³äº†æ¨¡æ€å†…è¡¨ç¤ºè´¨é‡å’Œæ¨¡æ€é—´æ•´åˆçš„æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€æ–¹æ³•æ•´åˆç»„ç»‡å­¦å’ŒåŸºå› ç»„å­¦åœ¨ç²¾å‡†è‚¿ç˜¤å­¦ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†è¡¨å‹å’ŒåŸºå› å‹å¼‚è´¨æ€§é™åˆ¶äº†æ¨¡æ€å†…è¡¨ç¤ºè´¨é‡å¹¶é˜»ç¢äº†æœ‰æ•ˆçš„æ¨¡æ€é—´æ•´åˆã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å¤§å¤šå¿½ç•¥äº†ç°å®ä¸´åºŠåœºæ™¯ä¸­åŸºå› ç»„æ•°æ®å¯èƒ½éƒ¨åˆ†ç¼ºå¤±æˆ–å®Œå…¨ä¸å¯ç”¨çš„æƒ…å†µï¼Œè¿™é™åˆ¶äº†æ–¹æ³•åœ¨å®é™…åŒ»ç–—ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

**Method:** è¯¥æ–¹æ³•æå‡ºäº†ä¸€ä¸ªçµæ´»çš„å¤šæ¨¡æ€åŸå‹æ¡†æ¶ï¼ŒåŒ…å«å››ä¸ªå…³é”®ç»„ä»¶ï¼š1ï¼‰ä½¿ç”¨æ–‡æœ¬æç¤ºå’ŒåŸå‹åŠ æƒè¿›è¡Œç”Ÿç‰©åŸå‹æ„å»ºï¼›2ï¼‰é€šè¿‡æ ·æœ¬çº§å’Œåˆ†å¸ƒçº§å¯¹é½å®ç°å¤šè§†å›¾å¯¹é½ï¼›3ï¼‰é‡‡ç”¨äºŒåˆ†èåˆæœºåˆ¶æ•è·å…±äº«å’Œæ¨¡æ€ç‰¹å®šä¿¡æ¯ä»¥è¿›è¡Œå¤šæ¨¡æ€èåˆï¼›4ï¼‰è¯­ä¹‰åŸºå› ç»„æ’è¡¥æŠ€æœ¯å¤„ç†ç¼ºå¤±æ•°æ®ã€‚è¯¥æ¡†æ¶ç‰¹åˆ«è®¾è®¡ç”¨äºæ•´åˆå…¨åˆ‡ç‰‡å›¾åƒå’Œä¸å®Œæ•´çš„åŸºå› ç»„æ•°æ®ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šç›¸æ¯”å…¶ä»–æœ€å…ˆè¿›æ–¹æ³•è¡¨ç°å‡ºæŒç»­çš„ä¼˜åŠ¿ã€‚è¯¥æ–¹æ³•åœ¨åŸºå› ç»„æ•°æ®ä¸å®Œæ•´æˆ–ç¼ºå¤±çš„ä¸´åºŠåœºæ™¯ä¸­å±•ç°å‡ºé²æ£’æ€§å’Œæœ‰æ•ˆæ€§ï¼ŒéªŒè¯äº†å…¶åœ¨ç°å®åŒ»ç–—ç¯å¢ƒä¸­çš„å®ç”¨ä»·å€¼ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºç²¾å‡†è‚¿ç˜¤å­¦ä¸­çš„å¤šæ¨¡æ€æ•°æ®æ•´åˆæä¾›äº†çµæ´»ä¸”é²æ£’çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚ç”¨äºåŸºå› ç»„æ•°æ®ä¸å®Œæ•´çš„ç°å®ä¸´åºŠåœºæ™¯ã€‚æå‡ºçš„æ¡†æ¶é€šè¿‡åŸå‹æ„å»ºã€å¤šè§†å›¾å¯¹é½å’Œç¼ºå¤±æ•°æ®å¤„ç†æœºåˆ¶ï¼Œä¸ºå¤šæ¨¡æ€åŒ»å­¦æ•°æ®åˆ†æå¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨å‰æ™¯ã€‚

---

#### ğŸ“„ Abstract
Multimodal approaches that integrate histology and genomics hold strong potential for precision oncology. However, phenotypic and genotypic heterogeneity limits the quality of intra-modal representations and hinders effective inter-modal integration. Furthermore, most existing methods overlook real-world clinical scenarios where genomics may be partially missing or entirely unavailable. We propose a flexible multimodal prototyping framework to integrate whole slide images and incomplete genomics for precision oncology. Our approach has four key components: 1) Biological Prototyping using text prompting and prototype-wise weighting; 2) Multiview Alignment through sample- and distribution-wise alignments; 3) Bipartite Fusion to capture both shared and modality-specific information for multimodal fusion; and 4) Semantic Genomics Imputation to handle missing data. Extensive experiments demonstrate the consistent superiority of the proposed method compared to other state-of-the-art approaches on multiple downstream tasks. The code is available at https://github.com/helenypzhang/Interpretable-Multimodal-Prototyping.


### [4] [WalkCLIP: Multimodal Learning for Urban Walkability Prediction](https://arxiv.org/abs/2511.21947)
*Shilong Xiang, JangHyeon Lee, Min Namgung, Yao-Yi Chiang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†WalkCLIPï¼Œä¸€ç§å¤šæ¨¡æ€æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå«æ˜Ÿå›¾åƒã€è¡—æ™¯å›¾åƒå’Œäººå£åŠ¨æ€æ•°æ®æ¥é¢„æµ‹åŸå¸‚æ­¥è¡Œæ€§ï¼Œå…‹æœäº†å•æºæ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶åœ¨æ˜å°¼é˜¿æ³¢åˆ©æ–¯-åœ£ä¿ç½—åœ°åŒºå–å¾—äº†ä¼˜äºåŸºå‡†æ–¹æ³•çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿæ­¥è¡Œæ€§è¯„ä¼°æ–¹æ³•ä¾èµ–è°ƒæŸ¥å’Œå®åœ°å®¡æ ¸ï¼Œæˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•ã€‚ç°æœ‰ç ”ç©¶ä½¿ç”¨å«æ˜Ÿå›¾åƒã€è¡—æ™¯å›¾åƒæˆ–äººå£æŒ‡æ ‡ç­‰å•æºæ–¹æ³•ï¼Œä½†è¿™äº›æ–¹æ³•å„è‡ªå­˜åœ¨å±€é™æ€§ï¼šå«æ˜Ÿæ•°æ®ç¼ºä¹è¡Œäººè§†è§’ï¼Œè¡—æ™¯å›¾åƒç¼ºå°‘ç©ºé—´ä¸Šä¸‹æ–‡ï¼Œäººå£åŠ¨æ€æ•°æ®æ— æ³•åæ˜ ç¯å¢ƒè§†è§‰å½¢æ€ã€‚å› æ­¤éœ€è¦æ•´åˆè¿™äº›äº’è¡¥è§†è§’æ¥å…¨é¢è¯„ä¼°æ­¥è¡Œç¯å¢ƒã€‚

**Method:** WalkCLIPæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼Œé¦–å…ˆä»GPT-4oç”Ÿæˆçš„å›¾åƒæè¿°ä¸­å­¦ä¹ æ­¥è¡Œæ€§æ„ŸçŸ¥çš„è§†è§‰-è¯­è¨€è¡¨ç¤ºï¼Œç„¶åé€šè¿‡ç©ºé—´èšåˆæ¨¡å—æ•´åˆé‚»åŸŸä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ä¼˜åŒ–è¿™äº›è¡¨ç¤ºï¼Œæœ€åå°†å¾—åˆ°çš„ç‰¹å¾ä¸äººå£åŠ¨æ€åŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºè¿›è¡Œèåˆï¼Œä»è€Œç»¼åˆé¢„æµ‹åŸå¸‚æ­¥è¡Œæ€§ã€‚

**Result:** åœ¨æ˜å°¼é˜¿æ³¢åˆ©æ–¯-åœ£ä¿ç½—åœ°åŒº4,660ä¸ªåœ°ç‚¹è¿›è¡Œè¯„ä¼°ï¼ŒWalkCLIPåœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œç©ºé—´å¯¹é½æ–¹é¢å‡ä¼˜äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€åŸºå‡†æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ•´åˆè§†è§‰å’Œè¡Œä¸ºä¿¡å·èƒ½å¤Ÿå¯é åœ°é¢„æµ‹æ­¥è¡Œç¯å¢ƒè´¨é‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ï¼Œæ•´åˆäº’è¡¥çš„å¤šæ¨¡æ€æ•°æ®æºï¼ˆåŒ…æ‹¬è§†è§‰å’Œè¡Œä¸ºä¿¡å·ï¼‰èƒ½å¤Ÿæ›´å…¨é¢ã€å¯é åœ°è¯„ä¼°åŸå¸‚æ­¥è¡Œæ€§ã€‚WalkCLIPæ¡†æ¶ä¸ºå¯æ‰©å±•çš„åŸå¸‚ç¯å¢ƒè¯„ä¼°æä¾›äº†æ–°æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å¤šæ¨¡æ€å­¦ä¹ åœ¨åŸå¸‚åˆ†æä¸­çš„æ½œåŠ›ï¼Œä¸ºåŸå¸‚è§„åˆ’ã€å…¬å…±å¥åº·å’Œå¯æŒç»­å‘å±•æä¾›äº†æ•°æ®æ”¯æŒã€‚

---

#### ğŸ“„ Abstract
Urban walkability is a cornerstone of public health, sustainability, and quality of life. Traditional walkability assessments rely on surveys and field audits, which are costly and difficult to scale. Recent studies have used satellite imagery, street view imagery, or population indicators to estimate walkability, but these single-source approaches capture only one dimension of the walking environment. Satellite data describe the built environment from above, but overlook the pedestrian perspective. Street view imagery captures conditions at the ground level, but lacks broader spatial context. Population dynamics reveal patterns of human activity but not the visual form of the environment. We introduce WalkCLIP, a multimodal framework that integrates these complementary viewpoints to predict urban walkability. WalkCLIP learns walkability-aware vision-language representations from GPT-4o generated image captions, refines these representations with a spatial aggregation module that incorporates neighborhood context, and fuses the resulting features with representations from a population dynamics foundation model. Evaluated at 4,660 locations throughout Minneapolis-Saint Paul, WalkCLIP outperforms unimodal and multimodal baselines in both predictive accuracy and spatial alignment. These results show that the integration of visual and behavioral signals yields reliable predictions of the walking environment.


### [5] [PAT3D: Physics-Augmented Text-to-3D Scene Generation](https://arxiv.org/abs/2511.21978)
*Guying Lin, Kemeng Huang, Michael Liu, Ruihan Gao, Hanke Chen, Lyuhao Chen, Beijia Lu, Taku Komura, Yuan Liu, Jun-Yan Zhu, Minchen Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PAT3Dï¼Œé¦–ä¸ªå°†è§†è§‰è¯­è¨€æ¨¡å‹ä¸ç‰©ç†ä»¿çœŸç›¸ç»“åˆçš„ç‰©ç†å¢å¼ºæ–‡æœ¬åˆ°3Dåœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆç‰©ç†åˆç†ã€ä»¿çœŸå°±ç»ªä¸”æ— ç›¸äº¤çš„3Dåœºæ™¯ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–‡æœ¬åˆ°3Dåœºæ™¯ç”Ÿæˆæ–¹æ³•é€šå¸¸å¿½ç•¥ç‰©ç†åˆç†æ€§ï¼Œå¯¼è‡´ç”Ÿæˆçš„åœºæ™¯å­˜åœ¨ç‰©ä½“ç©¿é€ã€ç‰©ç†ä¸ç¨³å®šç­‰é—®é¢˜ï¼Œæ— æ³•ç›´æ¥ç”¨äºä¸‹æ¸¸ä»¿çœŸä»»åŠ¡ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œé€šè¿‡é›†æˆç‰©ç†ä»¿çœŸæ¥ç”Ÿæˆç‰©ç†åˆç†ä¸”ä»¿çœŸå°±ç»ªçš„3Dåœºæ™¯ã€‚

**Method:** PAT3Dæ¡†æ¶é¦–å…ˆæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆ3Dç‰©ä½“å¹¶æ¨æ–­å…¶ç©ºé—´å…³ç³»ï¼Œç»„ç»‡æˆå±‚æ¬¡åŒ–åœºæ™¯æ ‘ï¼Œç„¶åè½¬æ¢ä¸ºä»¿çœŸåˆå§‹æ¡ä»¶ã€‚é‡‡ç”¨å¯å¾®åˆ†åˆšä½“ä»¿çœŸå™¨ç¡®ä¿ç‰©ä½“åœ¨é‡åŠ›ä¸‹çš„çœŸå®äº¤äº’ï¼Œé©±åŠ¨åœºæ™¯è¾¾åˆ°é™æ€å¹³è¡¡ä¸”æ— ç©¿é€ã€‚è¿›ä¸€æ­¥å¼•å…¥ä»¿çœŸå¾ªç¯ä¼˜åŒ–è¿‡ç¨‹ï¼Œåœ¨ä¿è¯ç‰©ç†ç¨³å®šæ€§å’Œéç›¸äº¤æ€§çš„åŒæ—¶æå‡ä¸è¾“å…¥æç¤ºçš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚

**Result:** å®éªŒè¡¨æ˜ï¼ŒPAT3Dåœ¨ç‰©ç†åˆç†æ€§ã€è¯­ä¹‰ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥æ¡†æ¶ä¸ä»…ç”Ÿæˆé«˜è´¨é‡åœºæ™¯ï¼Œè¿˜èƒ½äº§ç”Ÿå¯ç›´æ¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åœºæ™¯ç¼–è¾‘å’Œæœºå™¨äººæ“ä½œï¼‰çš„ä»¿çœŸå°±ç»ª3Dåœºæ™¯ã€‚

**Conclusion:** PAT3Dé€šè¿‡å°†ç‰©ç†ä»¿çœŸé›†æˆåˆ°æ–‡æœ¬åˆ°3Dç”Ÿæˆæµç¨‹ä¸­ï¼Œå¼€åˆ›äº†ç‰©ç†å¢å¼ºåœºæ™¯ç”Ÿæˆçš„æ–°èŒƒå¼ã€‚è¯¥æ¡†æ¶ä¸ä»…è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨ç‰©ç†åˆç†æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œè¿˜ä¸ºä»¿çœŸå°±ç»ª3Då†…å®¹çš„åˆ›å»ºæä¾›äº†æ–°é€”å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
We introduce PAT3D, the first physics-augmented text-to-3D scene generation framework that integrates vision-language models with physics-based simulation to produce physically plausible, simulation-ready, and intersection-free 3D scenes. Given a text prompt, PAT3D generates 3D objects, infers their spatial relations, and organizes them into a hierarchical scene tree, which is then converted into initial conditions for simulation. A differentiable rigid-body simulator ensures realistic object interactions under gravity, driving the scene toward static equilibrium without interpenetrations. To further enhance scene quality, we introduce a simulation-in-the-loop optimization procedure that guarantees physical stability and non-intersection, while improving semantic consistency with the input prompt. Experiments demonstrate that PAT3D substantially outperforms prior approaches in physical plausibility, semantic consistency, and visual quality. Beyond high-quality generation, PAT3D uniquely enables simulation-ready 3D scenes for downstream tasks such as scene editing and robotic manipulation. Code and data will be released upon acceptance.


### [6] [DialBench: Towards Accurate Reading Recognition of Pointer Meter using Large Foundation Models](https://arxiv.org/abs/2511.21982)
*Futian Wang, Chaoliu Weng, Xiao Wang, Zhen Chen, Zhicheng Zhao, Jin Tang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºæŒ‡é’ˆå¼ä»ªè¡¨è¯»æ•°è¯†åˆ«çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†RPM-10Kï¼Œå¹¶åŸºäºç‰©ç†å…³ç³»æ³¨å…¥æ„å»ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰è¯­è¨€æ¨¡å‹MRLMï¼Œè¯¥æ¨¡å‹é€šè¿‡å‡ ä½•ä¸å› æœå…³ç³»ç¼–ç å®ç°äº†æ„ŸçŸ¥ä¸ç‰©ç†æ¨ç†çš„å¯¹é½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ™ºèƒ½ç”µåŠ›ç³»ç»Ÿä¸­æŒ‡é’ˆå¼ä»ªè¡¨çš„ç²¾ç¡®è¯»æ•°è¯†åˆ«é¢ä¸´åå°„ã€é®æŒ¡ã€åŠ¨æ€è§†è§’ä»¥åŠæŒ‡é’ˆä¸åˆ»åº¦æ ‡è®°ä¹‹é—´è¿‡äºæ¥è¿‘ç­‰æŒ‘æˆ˜ï¼Œè€Œç°æœ‰æ–¹æ³•å¯¹æ­¤è¾ƒä¸ºè„†å¼±ã€‚è¯¥é¢†åŸŸç›®å‰ç¼ºä¹å¤§è§„æ¨¡æ•°æ®é›†æ¥æ”¯æŒé²æ£’ç®—æ³•çš„å¼€å‘ï¼Œè¿™é™åˆ¶äº†ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ä¸åº”ç”¨ã€‚

**Method:** æœ¬æ–‡é¦–å…ˆæ„å»ºäº†åŒ…å«10730å¼ ä»ªè¡¨å›¾åƒçš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†RPM-10Kï¼Œå……åˆ†åæ˜ äº†å®é™…åº”ç”¨ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç‰©ç†å…³ç³»æ³¨å…¥çš„è§†è§‰è¯­è¨€æ¨¡å‹MRLMï¼Œè¯¥æ¨¡å‹æ˜¾å¼ç¼–ç æŒ‡é’ˆä¸åˆ»åº¦ä¹‹é—´çš„å‡ ä½•å’Œå› æœå…³ç³»ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›èåˆå’Œè‡ªé€‚åº”ä¸“å®¶é€‰æ‹©æœºåˆ¶ï¼Œå°†æ„ŸçŸ¥ä¸ç‰©ç†æ¨ç†å¯¹é½ï¼Œä»¥ä¸–ç•Œæ¨¡å‹çš„è§†è§’è§£é‡Šä»ªè¡¨é…ç½®å¹¶ç”Ÿæˆç²¾ç¡®æ•°å€¼è¯»æ•°ã€‚

**Result:** åœ¨æ–°æå‡ºçš„åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå……åˆ†éªŒè¯äº†æ‰€æå‡ºæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMRLMæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹åå°„ã€é®æŒ¡ã€åŠ¨æ€è§†è§’ç­‰å¤æ‚åœºæ™¯ï¼Œåœ¨æŒ‡é’ˆå¼ä»ªè¡¨è¯»æ•°è¯†åˆ«ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œè¯æ˜äº†ç‰©ç†å…³ç³»æ³¨å…¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ä»…å¡«è¡¥äº†æŒ‡é’ˆå¼ä»ªè¡¨è¯»æ•°è¯†åˆ«é¢†åŸŸå¤§è§„æ¨¡æ•°æ®é›†çš„ç©ºç™½ï¼Œæ›´é‡è¦çš„æ˜¯æå‡ºäº†ä¸€ç§å°†ç‰©ç†å…³ç³»æ˜¾å¼ç¼–ç åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ–°èŒƒå¼ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å°†æ„ŸçŸ¥ä¸ç‰©ç†æ¨ç†å¯¹é½ï¼Œä¸ºå¤æ‚åœºæ™¯ä¸‹çš„ä»ªè¡¨è¯»æ•°è¯†åˆ«æä¾›äº†æ›´é²æ£’çš„è§£å†³æ–¹æ¡ˆï¼Œå¯¹æœªæ¥æ™ºèƒ½ç”µåŠ›ç³»ç»Ÿå’Œå…¶ä»–å·¥ä¸šè§†è§‰åº”ç”¨å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
The precise reading recognition of pointer meters plays a key role in smart power systems, but existing approaches remain fragile due to challenges like reflections, occlusions, dynamic viewing angles, and overly between thin pointers and scale markings. Up to now, this area still lacks large-scale datasets to support the development of robust algorithms. To address these challenges, this paper first presents a new large-scale benchmark dataset for dial reading, termed RPM-10K, which contains 10730 meter images that fully reflect the aforementioned key challenges. Built upon the dataset, we propose a novel vision-language model for pointer meter reading recognition, termed MRLM, based on physical relation injection. Instead of exhaustively learning image-level correlations, MRLM explicitly encodes the geometric and causal relationships between the pointer and the scale, aligning perception with physical reasoning in the spirit of world-model perspectives. Through cross-attentional fusion and adaptive expert selection, the model learns to interpret dial configurations and generate precise numeric readings. Extensive experiments fully validated the effectiveness of our proposed framework on the newly proposed benchmark dataset. Both the dataset and source code will be released on https://github.com/Event-AHU/DialBench


### [7] [PPBoost: Progressive Prompt Boosting for Text-Driven Medical Image Segmentation](https://arxiv.org/abs/2511.21984)
*Xuchen Li, Hengrui Gu, Mohan Zhang, Qin Liu, Zhen Tan, Xinyuan Zhu, Huixue Zhou, Tianlong Chen, Kaixiong Zhou*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºPPBoostæ¡†æ¶ï¼Œé€šè¿‡å°†å¼±æ–‡æœ¬æç¤ºè½¬åŒ–ä¸ºå¼ºç©ºé—´è§†è§‰æç¤ºï¼Œåœ¨é›¶æ ·æœ¬æ¡ä»¶ä¸‹æ˜¾è‘—æå‡åŒ»å­¦å›¾åƒåˆ†å‰²æ€§èƒ½ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆå§‹ä¼ªè¾¹ç•Œæ¡†ï¼Œå¹¶é€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥è¿‡æ»¤å’Œä¼ªæ ‡ç­¾æ£€æµ‹å™¨è®­ç»ƒï¼Œæœ€ç»ˆå¢å¼ºç°æœ‰åˆ†å‰²æ¨¡å‹çš„å®šä½èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼ŒåŸºäºæ–‡æœ¬æç¤ºçš„åŸºç¡€æ¨¡å‹è™½ç„¶ç›´è§‚ä½†ç¼ºä¹ç©ºé—´ç²¾åº¦ä¸”æ˜“å—åŸŸåç§»å½±å“ï¼Œè€ŒåŸºäºè§†è§‰æç¤ºçš„æ¨¡å‹è™½æ€§èƒ½ä¼˜è¶Šå´éœ€è¦ç²¾ç¡®è¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œè¿™åœ¨ä¸´åºŠå®è·µä¸­æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥è·å–ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³æ–‡æœ¬æç¤ºæ¨¡å‹ç©ºé—´ç²¾åº¦ä¸è¶³ä¸è§†è§‰æç¤ºæ¨¡å‹æ ‡æ³¨éœ€æ±‚è‹›åˆ»ä¹‹é—´çš„çŸ›ç›¾ã€‚

**Method:** PPBoostæ¡†æ¶é‡‡ç”¨æ¸è¿›å¼æç¤ºå¢å¼ºç­–ç•¥ï¼Œé¦–å…ˆåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹åŸºäºæ–‡æœ¬æè¿°ç”Ÿæˆåˆå§‹ä¼ªè¾¹ç•Œæ¡†ï¼Œå¹¶é€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ ‡å‡†è¿‡æ»¤ä¸å¯é é¢„æµ‹ã€‚ä¿ç•™çš„å›¾åƒ-è¾¹ç•Œæ¡†å¯¹ç”¨äºè®­ç»ƒä¼ªæ ‡ç­¾æ£€æµ‹å™¨ä»¥ç”Ÿæˆé«˜è´¨é‡è¾¹ç•Œæ¡†ï¼Œæ¨ç†æ—¶è¿›ä¸€æ­¥é€šè¿‡é€‚å½“æ‰©å±•è¾¹ç•Œæ¡†æ¥ç´§å¯†è¦†ç›–ç›®æ ‡è§£å‰–ç»“æ„ï¼Œæœ€ç»ˆå°†å¢å¼ºçš„ç©ºé—´å®šä½è¾¹ç•Œæ¡†æç¤ºç”¨äºæŒ‡å¯¼ç°æœ‰åˆ†å‰²æ¨¡å‹ç”Ÿæˆå¯†é›†æ©ç ã€‚

**Result:** åœ¨æ¶µç›–å¤šç§æ¨¡æ€å’Œè§£å‰–ç»“æ„çš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šï¼ŒPPBooståœ¨Diceç³»æ•°å’Œå½’ä¸€åŒ–è¡¨é¢è·ç¦»æŒ‡æ ‡ä¸ŠæŒç»­ä¼˜äºæ–‡æœ¬æç¤ºå’Œè§†è§‰æç¤ºåŸºçº¿æ–¹æ³•ï¼Œä¸”æ˜¾è‘—è¶…è¶Šå°‘æ ·æœ¬åˆ†å‰²æ¨¡å‹è€Œæ— éœ€ä½¿ç”¨æ ‡æ³¨æ•°æ®ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ³›åŒ–åˆ°å¤šç§å…¸å‹çš„è§†è§‰åˆ†å‰²æ¨¡å‹éª¨å¹²ç½‘ç»œï¼Œå±•ç°å‡ºå¼ºå¤§çš„è·¨åŸŸé€‚åº”èƒ½åŠ›ã€‚

**Conclusion:** PPBoostæˆåŠŸåœ°å°†å¼±æ–‡æœ¬ä¿¡å·è½¬åŒ–ä¸ºå¼ºç©ºé—´è§†è§‰æç¤ºï¼Œåœ¨ä¸¥æ ¼é›¶æ ·æœ¬æ¡ä»¶ä¸‹å®ç°äº†åŒ»å­¦å›¾åƒåˆ†å‰²æ€§èƒ½çš„æ˜¾è‘—æå‡ã€‚è¯¥ç ”ç©¶ä¸ºç»“åˆæ–‡æœ¬å’Œè§†è§‰æç¤ºçš„ä¼˜åŠ¿æä¾›äº†æœ‰æ•ˆæ¡†æ¶ï¼Œå±•ç¤ºäº†åœ¨ç¼ºä¹ç²¾ç¡®æ ‡æ³¨çš„ä¸´åºŠåœºæ™¯ä¸­åˆ©ç”¨å¼±ç›‘ç£ä¿¡å·å®ç°é«˜è´¨é‡åˆ†å‰²çš„å¯è¡Œæ€§ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æä¸­çš„æç¤ºå·¥ç¨‹å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Text-prompted foundation models for medical image segmentation offer an intuitive way to delineate anatomical structures from natural language queries, but their predictions often lack spatial precision and degrade under domain shift. In contrast, visual-prompted models achieve strong segmentation performance across diverse modalities by leveraging spatial cues of precise bounding-box (bbox) prompts to guide the segmentation of target lesions. However, it is costly and challenging to obtain the precise visual prompts in clinical practice. We propose PPBoost (Progressive Prompt-Boosting), a framework that bridges these limitations by transforming weak text-derived signals into strong, spatially grounded visual prompts, operating under a strict zero-shot regime with no image- or pixel-level segmentation labels. PPBoost first uses a vision-language model to produce initial pseudo-bboxes conditioned on the textual object descriptions and applies an uncertainty-aware criterion to filter unreliable predictions. The retained image-bboxes pairs are then leveraged to train a pseudo-labeled detector, producing the high-quality bboxes for the query images. During inference, PPBoost further refines the generated bboxes by appropriately expanding them to tightly cover the target anatomical structures. The enhanced spatially-grounding bbox prompts guide existing segmentation models to generate final dense masks, effectively amplifying weak text cues into strong spatial guidance. Across three datasets spanning diverse modalities and anatomies, PPBoost consistently improves Dice and Normalized Surface Distance over text- and visual-prompted baselines and, notably, surpasses few-shot segmentation models without using labeled data. PPBoost can generalize to multiple typical visual segmentation model backbones.


### [8] [Can Multi-Modal LLMs Provide Live Step-by-Step Task Guidance?](https://arxiv.org/abs/2511.21998)
*Apratim Bhattacharyya, Bicheng Xu, Sanjay Haresh, Reza Pourreza, Litian Liu, Sunny Panchal, Pulkit Madan, Leonid Sigal, Roland Memisevic*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Qualcomm Interactive CookingåŸºå‡†æ•°æ®é›†å’ŒLiveMambaæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å®æ—¶äº¤äº’å¼åˆ†æ­¥æŒ‡å¯¼æ–¹é¢çš„ä¸è¶³ï¼Œä¸ºå®æ—¶æƒ…å¢ƒåŒ–æ•™å­¦æä¾›äº†é¦–ä¸ªä¸“ç”¨åŸºå‡†å’Œå¼ºåŸºçº¿ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¯¹è¯èƒ½åŠ›ä¸Šè™½æœ‰è¿›æ­¥ï¼Œä½†ç¼ºä¹æä¾›å®æ—¶äº¤äº’å¼åˆ†æ­¥æŒ‡å¯¼çš„å…³é”®èƒ½åŠ›ï¼Œè¿™éœ€è¦æ¨¡å‹ä¸ä»…èƒ½æä¾›æŒ‡ä»¤ï¼Œè¿˜èƒ½æ£€æµ‹æ‰§è¡ŒæˆåŠŸã€è¯†åˆ«é”™è¯¯å¹¶åŠæ—¶æé†’ç”¨æˆ·ï¼Œæ‰€æœ‰è¿™äº›éƒ½å¿…é¡»åœ¨å®æ—¶è§†é¢‘æµä¸­å¼‚æ­¥å“åº”å®Œæˆã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†åŸºäºCaptainCook4Dæ„å»ºçš„Qualcomm Interactive CookingåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«ç”¨æˆ·åœ¨ä»»åŠ¡æ‰§è¡Œä¸­çš„é”™è¯¯è®°å½•ï¼Œå…·æœ‰å¯†é›†æ ‡æ³¨çš„å®šæ—¶æŒ‡ä»¤å’Œåé¦ˆä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯ä¸è§†é¢‘ä¸­è§†è§‰é”™è¯¯å‘ç”Ÿæ—¶é—´ç²¾ç¡®å¯¹é½çš„é”™è¯¯æé†’ã€‚åŒæ—¶æå‡ºäº†LiveMambaï¼Œä¸€ç§ä¸“ä¸ºäº¤äº’å¼æ•™å­¦æŒ‡å¯¼è®¾è®¡çš„æµå¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚

**Result:** åœ¨Qualcomm Interactive CookingåŸºå‡†ä¸Šè¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†LiveMambaä½œä¸ºå®æ—¶äº¤äº’æŒ‡å¯¼çš„å¼ºåŸºçº¿æ€§èƒ½ï¼Œè¯¥åŸºå‡†æä¾›äº†é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å®æ—¶æƒ…å¢ƒåŒ–æ•™å­¦èƒ½åŠ›çš„æµ‹è¯•å¹³å°ã€‚

**Conclusion:** è¿™é¡¹å·¥ä½œä¸ºå¼€å‘å’Œè¯„ä¼°å®æ—¶æƒ…å¢ƒåŒ–æ•™å­¦ç³»ç»Ÿæä¾›äº†é¦–ä¸ªä¸“ç”¨åŸºå‡†å’Œå¼ºåŸºçº¿ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å‘å®æ—¶äº¤äº’æŒ‡å¯¼èƒ½åŠ›çš„å‘å±•ï¼Œä¸ºæœªæ¥AIåŠ©æ‰‹çš„å…³é”®åŠŸèƒ½å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Multi-modal Large Language Models (LLM) have advanced conversational abilities but struggle with providing live, interactive step-by-step guidance, a key capability for future AI assistants. Effective guidance requires not only delivering instructions but also detecting their successful execution, as well as identifying and alerting users to mistakes, all of which has to happen in real-time. This requires models that are not turn-based, but that can react asynchronously to a video stream, as well as video data showing users performing tasks including mistakes and their corrections. To this end, we introduce Qualcomm Interactive Cooking, a new benchmark and dataset built upon CaptainCook4D, which contains user mistakes during task execution. Our dataset and benchmark features densely annotated, timed instructions and feedback messages, specifically including mistake alerts precisely timestamped to their visual occurrence in the video. We evaluate state-of-the-art multi-modal LLMs on the Qualcomm Interactive Cooking benchmark and introduce LiveMamba, a streaming multi-modal LLM designed for interactive instructional guidance. This work provides the first dedicated benchmark and a strong baseline for developing and evaluating on live, situated coaching.


### [9] [MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis](https://arxiv.org/abs/2511.22018)
*Chunzheng Zhu, Yangfang Lin, Shen Chen, Yijun Wang, Jianxin Lin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMedEyesï¼Œä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å»ºæ¨¡ä¸´åºŠåŒ»ç”Ÿé£æ ¼çš„è¯Šæ–­æ¨ç†è¿‡ç¨‹ï¼Œè§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—è¯Šæ–­ä¸­äº§ç”Ÿè¡¨é¢è¿è´¯ä½†ä¸´åºŠä¸å‡†ç¡®æ¨ç†è·¯å¾„çš„é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŒ»ç–—VQAåŸºå‡†ä¸Šå®ç°å¹³å‡8.5%çš„æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å‡†ç¡®çš„åŒ»ç–—è¯Šæ–­é€šå¸¸æ¶‰åŠæ¸è¿›å¼è§†è§‰èšç„¦å’Œè¿­ä»£æ¨ç†ï¼Œè¿™ä¸ä¸´åºŠå·¥ä½œæµç¨‹ç‰¹å¾ç›¸ç¬¦ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±çš„è§†è§‰è¯­è¨€æ¨¡å‹é‡‡ç”¨çº¯åœ¨çº¿ç­–ç•¥å­¦ä¹ èŒƒå¼ï¼Œå€¾å‘äºå¼ºåŒ–è¡¨é¢è¿è´¯ä½†ä¸´åºŠä¸å‡†ç¡®çš„æ¨ç†è·¯å¾„ï¼Œæ— æ³•æœ‰æ•ˆå»ºæ¨¡ä¸´åºŠåŒ»ç”Ÿé£æ ¼çš„è¯Šæ–­æ¨ç†è¿‡ç¨‹ã€‚

**Method:** MedEyesæ¡†æ¶é€šè¿‡æ•´åˆç¦»çº¿ç­–ç•¥ä¸“å®¶æŒ‡å¯¼ï¼Œå°†ä¸“å®¶è§†è§‰æœç´¢è½¨è¿¹è½¬åŒ–ä¸ºç»“æ„åŒ–å¤–éƒ¨è¡Œä¸ºä¿¡å·ï¼Œå¼•å¯¼æ¨¡å‹å®ç°ä¸´åºŠå¯¹é½çš„è§†è§‰æ¨ç†ã€‚è®¾è®¡äº†å‡è§†å¼•å¯¼æ¨ç†å¯¼èˆªå™¨ï¼Œé‡‡ç”¨åŒæ¨¡å¼æ¢ç´¢ç­–ç•¥æ¨¡æ‹Ÿè¯Šæ–­è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç³»ç»Ÿæ€§å¼‚å¸¸å®šä½æ‰«æå’Œè¯¦ç»†åŒºåŸŸåˆ†æé’»å–ã€‚å¼•å…¥ç½®ä¿¡åº¦å€¼é‡‡æ ·å™¨ï¼Œé€šè¿‡æ ¸å¿ƒé‡‡æ ·å’Œè‡ªé€‚åº”ç»ˆæ­¢åˆ›å»ºå¤šæ ·ä¸”å¯ä¿¡çš„æ¢ç´¢è·¯å¾„ã€‚æœ€åï¼ŒåŒæµGRPOä¼˜åŒ–æ¡†æ¶è§£è€¦åœ¨çº¿å’Œç¦»çº¿ç­–ç•¥å­¦ä¹ ä¿¡å·ï¼Œç¼“è§£å¥–åŠ±åŒåŒ–å’Œç†µå´©æºƒé—®é¢˜ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒMedEyesåœ¨å¤šä¸ªåŒ»ç–—è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¹³å‡8.5%çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨æ„å»ºå¯è§£é‡ŠåŒ»ç–—AIç³»ç»Ÿæ–¹é¢çš„æ½œåŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸“å®¶æŒ‡å¯¼çš„è§†è§‰æ¨ç†æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†åŒ»ç–—è¯Šæ–­çš„å‡†ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡æ•´åˆä¸“å®¶è§†è§‰æœç´¢è½¨è¿¹å’Œç»“æ„åŒ–è¡Œä¸ºä¿¡å·ï¼Œå¯ä»¥æœ‰æ•ˆå¼•å¯¼AIæ¨¡å‹å®ç°ä¸´åºŠå¯¹é½çš„è¯Šæ–­æ¨ç†ã€‚MedEyesæ¡†æ¶ä¸ä»…æå‡äº†åŒ»ç–—VQAä»»åŠ¡çš„æ€§èƒ½ï¼Œè¿˜ä¸ºæ„å»ºå¯è§£é‡Šã€ç¬¦åˆä¸´åºŠå·¥ä½œæµç¨‹çš„åŒ»ç–—AIç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¹³è¡¡äº†ä¸“å®¶æ¨¡ä»¿ä¸è‡ªä¸»å‘ç°çš„å…³ç³»ã€‚

---

#### ğŸ“„ Abstract
Accurate medical diagnosis often involves progressive visual focusing and iterative reasoning, characteristics commonly observed in clinical workflows. While recent vision-language models demonstrate promising chain-of-thought (CoT) reasoning capabilities via reinforcement learning with verifiable rewards (RLVR), their purely on-policy learning paradigm tends to reinforce superficially coherent but clinically inaccurate reasoning paths. We propose MedEyes, a novel reinforcement learning framework that dynamically models clinician-style diagnostic reasoning by progressively attending to and interpreting relevant medical image regions. By incorporating off-policy expert guidance, MedEyes converts expert visual search trajectories into structured external behavioral signals, guiding the model toward clinically aligned visual reasoning. We design the Gaze-guided Reasoning Navigator (GRN) to emulate the diagnostic process through a dual-mode exploration strategy, scanning for systematic abnormality localization and drilling for detailed regional analysis. To balance expert imitation and autonomous discovery, we introduce the Confidence Value Sampler (CVS), which employs nucleus sampling and adaptive termination to create diverse yet credible exploration paths. Finally, the dual-stream GRPO optimization framework decouples on-policy and off-policy learning signals, mitigating reward assimilation and entropy collapse. Experiments demonstrate that MedEyes achieves an average performance improvement of +8.5\% across multiple medical VQA benchmarks, validating MedEyes's potential in building interpretable medical AI systems.


### [10] [Intra-Class Probabilistic Embeddings for Uncertainty Estimation in Vision-Language Models](https://arxiv.org/abs/2511.22019)
*Zhenxiang Lin, Maryam Haghighat, Will Browne, Dimity Miller*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„åå¤„ç†ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•ICPEï¼Œç”¨äºæå‡å¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ€§ï¼Œé€šè¿‡æµ‹é‡ç±»å†…è§†è§‰ç‰¹å¾ä¸€è‡´æ€§æ¥æ£€æµ‹é”™è¯¯é¢„æµ‹ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨å¼€æ”¾è¯æ±‡åˆ†ç±»ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®¹æ˜“å¯¹é”™è¯¯åˆ†ç±»åˆ†é…é«˜ç½®ä¿¡åº¦åˆ†æ•°ï¼Œè¿™åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­é™åˆ¶äº†å…¶å¯é æ€§ï¼Œå› æ­¤éœ€è¦æœ‰æ•ˆçš„é”™è¯¯æ£€æµ‹æœºåˆ¶ã€‚

**Method:** æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„åå¤„ç†ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•ï¼Œé€šè¿‡ç‰¹å¾æŠ•å½±ç»“åˆå¤šå…ƒé«˜æ–¯åˆ†å¸ƒåˆ›å»ºç±»ç‰¹å®šçš„æ¦‚ç‡åµŒå…¥ï¼Œæµ‹é‡ç±»å†…è§†è§‰ç‰¹å¾ä¸€è‡´æ€§ï¼Œè¯¥æ–¹æ³•ä¸VLMæ— å…³ä¸”æ— éœ€å¾®è°ƒã€‚

**Result:** åœ¨ImageNetã€Flowers102ã€Food101ã€EuroSATå’ŒDTDæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é”™è¯¯æ£€æµ‹æ€§èƒ½ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºç¡®å®šæ€§å’Œæ¦‚ç‡æ€§VLMåŸºçº¿ï¼Œä¸”ä»…éœ€æ¯ç±»10å¼ è®­ç»ƒå›¾åƒå³å¯æœ‰æ•ˆå·¥ä½œã€‚

**Conclusion:** è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åå¤„ç†è§£å†³æ–¹æ¡ˆï¼Œå¢å¼ºäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åˆ†å¸ƒåç§»ä¸‹çš„é²æ£’æ€§ï¼Œä¸ºå®‰å…¨å…³é”®åº”ç”¨ä¸­çš„å¯é éƒ¨ç½²æä¾›äº†å®ç”¨å·¥å…·ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹æ— å…³æ€§å’Œæ— éœ€è®­ç»ƒçš„ä¼˜åŠ¿ã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLMs), such as CLIP, have gained popularity for their strong open vocabulary classification performance, but they are prone to assigning high confidence scores to misclassifications, limiting their reliability in safety-critical applications. We introduce a training-free, post-hoc uncertainty estimation method for contrastive VLMs that can be used to detect erroneous predictions. The key to our approach is to measure visual feature consistency within a class, using feature projection combined with multivariate Gaussians to create class-specific probabilistic embeddings. Our method is VLM-agnostic, requires no fine-tuning, demonstrates robustness to distribution shift, and works effectively with as few as 10 training images per class. Extensive experiments on ImageNet, Flowers102, Food101, EuroSAT and DTD show state-of-the-art error detection performance, significantly outperforming both deterministic and probabilistic VLM baselines. Code is available at https://github.com/zhenxianglin/ICPE.


### [11] [Layover or Direct Flight: Rethinking Audio-Guided Image Segmentation](https://arxiv.org/abs/2511.22025)
*Joel Alberto Santos, Zongwei Wu, Xavier Alameda-Pineda, Radu Timofte*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç›´æ¥ä»éŸ³é¢‘è¿›è¡Œç‰©ä½“å®šä½çš„æ–°æ–¹æ³•ï¼ŒæŒ‘æˆ˜äº†ä¾èµ–æ–‡æœ¬è½¬å½•çš„ä¼ ç»ŸéŸ³é¢‘-è§†è§‰å¯¹é½èŒƒå¼ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨ç‰¹å®šæƒ…å†µä¸‹ç”šè‡³ä¼˜äºåŸºäºè½¬å½•çš„æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤„ç†è¯­è¨€å¤šæ ·æ€§æ–¹é¢è¡¨ç°æ›´é²æ£’ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºæ–‡æœ¬è½¬å½•çš„éŸ³é¢‘-è§†è§‰ç‰©ä½“å®šä½æ–¹æ³•å­˜åœ¨æ•ˆç‡å’Œé²æ£’æ€§é—®é¢˜ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å°†è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬ã€æå–å…³é”®è¯ï¼Œç„¶ååˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬-è§†è§‰æ¨¡å‹è¿›è¡Œå®šä½ï¼Œä½†ç ”ç©¶è€…è´¨ç–‘è¿™ç§è½¬å½•æµç¨‹çš„æ•ˆç‡å’Œé²æ£’æ€§ï¼Œæ¢ç´¢æ˜¯å¦èƒ½å¤Ÿå®ç°ä¸ä¾èµ–æ–‡æœ¬çš„ç›´æ¥éŸ³é¢‘-è§†è§‰å¯¹é½ã€‚

**Method:** ç ”ç©¶é€šè¿‡ç®€åŒ–ä»»åŠ¡ï¼Œä¸“æ³¨äºåŸºäºå•è¯è¯­éŸ³æŒ‡ä»¤çš„ç‰©ä½“å®šä½ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°çš„éŸ³é¢‘å®šä½æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§ç‰©ä½“å’Œå¤šæ ·åŒ–çš„äººç±»å£éŸ³ï¼Œç„¶åä»ç›¸å…³éŸ³é¢‘-è§†è§‰é¢†åŸŸé€‚é…å¹¶åŸºå‡†æµ‹è¯•äº†å¤šç§æ¨¡å‹ï¼Œæ¢ç´¢ç›´æ¥éŸ³é¢‘å®šä½çš„å¯è¡Œæ€§ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œç›´æ¥ä»éŸ³é¢‘è¿›è¡Œç‰©ä½“å®šä½ä¸ä»…æ˜¯å¯è¡Œçš„ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³ä¼˜äºåŸºäºè½¬å½•çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¯­è¨€å¤šæ ·æ€§æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œè¿™æŒ‘æˆ˜äº†ä¼ ç»Ÿä¾èµ–æ–‡æœ¬ä¸­é—´è¡¨ç¤ºçš„èŒƒå¼ã€‚

**Conclusion:** è¯¥ç ”ç©¶é¼“åŠ±å¯¹ç›´æ¥éŸ³é¢‘å®šä½çš„é‡æ–°å…³æ³¨ï¼Œä¸ºæ›´é²æ£’å’Œé«˜æ•ˆçš„å¤šæ¨¡æ€ç†è§£ç³»ç»Ÿé“ºå¹³äº†é“è·¯ï¼Œè¡¨æ˜ç»•è¿‡æ–‡æœ¬è½¬å½•å¯ä»¥å®ç°æ›´ç›´æ¥å’Œé²æ£’çš„éŸ³é¢‘-è§†è§‰å¯¹é½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šæ ·åŒ–å£éŸ³å’Œè¯­è¨€å˜ä½“æ—¶å…·æœ‰ä¼˜åŠ¿ã€‚

---

#### ğŸ“„ Abstract
Understanding human instructions is essential for enabling smooth human-robot interaction. In this work, we focus on object grounding, i.e., localizing an object of interest in a visual scene (e.g., an image) based on verbal human instructions. Despite recent progress, a dominant research trend relies on using text as an intermediate representation. These approaches typically transcribe speech to text, extract relevant object keywords, and perform grounding using models pretrained on large text-vision datasets. However, we question both the efficiency and robustness of such transcription-based pipelines. Specifically, we ask: Can we achieve direct audio-visual alignment without relying on text? To explore this possibility, we simplify the task by focusing on grounding from single-word spoken instructions. We introduce a new audio-based grounding dataset that covers a wide variety of objects and diverse human accents. We then adapt and benchmark several models from the closely audio-visual field. Our results demonstrate that direct grounding from audio is not only feasible but, in some cases, even outperforms transcription-based methods, especially in terms of robustness to linguistic variability. Our findings encourage a renewed interest in direct audio grounding and pave the way for more robust and efficient multimodal understanding systems.


### [12] [ICM-SR: Image-Conditioned Manifold Regularization for Image Super-Resoultion](https://arxiv.org/abs/2511.22048)
*Junoh Kang, Donghun Ryu, Bohyung Han*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºå›¾åƒæ¡ä»¶æµå½¢æ­£åˆ™åŒ–ï¼ˆICMï¼‰æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é¢œè‰²æ˜ å°„å’ŒCannyè¾¹ç¼˜çš„ç»“æ„ä¿¡æ¯æ¥æ­£åˆ™åŒ–è¾“å‡ºï¼Œè§£å†³äº†ç°æœ‰åŸºäºæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•ä¸­å­˜åœ¨çš„æ¦‚å¿µé”™ä½å’Œç”Ÿæˆå…ˆéªŒç¼ºé™·é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•é€šå¸¸åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒï¼Œä½†é»˜è®¤é‡‡ç”¨æ–‡æœ¬æ¡ä»¶æµå½¢å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™ï¼šæ¦‚å¿µä¸Šä¸ä»»åŠ¡ç›®æ ‡ä¸åŒ¹é…ï¼ˆè¶…åˆ†è¾¨ç‡åº”ç”Ÿæˆä¸ä½è´¨é‡å›¾åƒç›´æ¥ç›¸å…³çš„é«˜è´¨é‡å›¾åƒï¼‰ï¼Œå®è·µä¸­æ•™å¸ˆæ¨¡å‹å¸¸äº§ç”Ÿé¢œè‰²å¤±çœŸå’Œè¾¹ç¼˜æ¨¡ç³Šçš„å›¾åƒï¼Œè¡¨æ˜å…¶ç”Ÿæˆå…ˆéªŒå­˜åœ¨ç¼ºé™·ã€‚

**Method:** æœ¬æ–‡æå‡ºå›¾åƒæ¡ä»¶æµå½¢æ­£åˆ™åŒ–ï¼ˆICMï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†è¾“å‡ºæ­£åˆ™åŒ–åˆ°ä»¥ç¨€ç–ä½†å…³é”®çš„ç»“æ„ä¿¡æ¯ä¸ºæ¡ä»¶çš„æµå½¢ä¸Šï¼Œå…·ä½“ç»“åˆé¢œè‰²æ˜ å°„å’ŒCannyè¾¹ç¼˜ä¿¡æ¯ï¼Œé¿å…äº†ç›´æ¥å¯¹å¯†é›†åŸå§‹å›¾åƒè¿›è¡Œæ¡ä»¶åŒ–æ—¶çš„ä¸ç¨³å®šæ€§ï¼ŒåŒæ—¶æä¾›äº†ä»»åŠ¡å¯¹é½çš„ç¨³å®šæ­£åˆ™åŒ–ä¿¡å·ã€‚

**Result:** å®éªŒéªŒè¯è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ­£åˆ™åŒ–æ–¹æ³•æ˜¾è‘—æå‡äº†è¶…åˆ†è¾¨ç‡æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä½œè€…æ‰¿è¯ºå°†å¼€æºä»£ç ä»¥ç¡®ä¿å¯å¤ç°æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†ä¸ºç‰¹å®šä»»åŠ¡é€‰æ‹©åˆé€‚æ­£åˆ™åŒ–æµå½¢çš„é‡è¦æ€§ï¼Œé€šè¿‡ç»“åˆç¨€ç–ç»“æ„ä¿¡æ¯è€Œéå¯†é›†åŸå§‹å›¾åƒï¼ŒICMæ–¹æ³•æ—¢ä¿æŒäº†æ•°å€¼ç¨³å®šæ€§åˆå®ç°äº†æ¦‚å¿µå¯¹é½ï¼Œä¸ºåŸºäºæ‰©æ•£æ¨¡å‹çš„çœŸå®å›¾åƒè¶…åˆ†è¾¨ç‡æä¾›äº†æ›´æœ‰æ•ˆçš„æ­£åˆ™åŒ–æ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
Real world image super-resolution (Real-ISR) often leverages the powerful generative priors of text-to-image diffusion models by regularizing the output to lie on their learned manifold. However, existing methods often overlook the importance of the regularizing manifold, typically defaulting to a text-conditioned manifold. This approach suffers from two key limitations. Conceptually, it is misaligned with the Real-ISR task, which is to generate high quality (HQ) images directly tied to the low quality (LQ) images. Practically, the teacher model often reconstructs images with color distortions and blurred edges, indicating a flawed generative prior for this task. To correct these flaws and ensure conceptual alignment, a more suitable manifold must incorporate information from the images. While the most straightforward approach is to condition directly on the raw input images, their high information densities make the regularization process numerically unstable. To resolve this, we propose image-conditioned manifold regularization (ICM), a method that regularizes the output towards a manifold conditioned on the sparse yet essential structural information: a combination of colormap and Canny edges. ICM provides a task-aligned and stable regularization signal, thereby avoiding the instability of dense-conditioning and enhancing the final super-resolution quality. Our experiments confirm that the proposed regularization significantly enhances super-resolution performance, particularly in perceptual quality, demonstrating its effectiveness for real-world applications. We will release the source code of our work for reproducibility.


### [13] [OralGPT-Omni: A Versatile Dental Multimodal Large Language Model](https://arxiv.org/abs/2511.22055)
*Jing Hao, Yuci Liang, Lizhuo Lin, Yuxuan Fan, Wenkai Zhou, Kaixin Guo, Zanting Ye, Yanpeng Sun, Xinyu Zhang, Yanqi Yang, Qiankun Li, Hao Tang, James Kit-Hon Tsoi, Linlin Shen, Kuo Feng Hung*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†OralGPT-Omniï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘ç‰™ç§‘é¢†åŸŸçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ„å»ºTRACE-CoTä¸´åºŠæ¨ç†æ•°æ®é›†å’Œå››é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œæ˜¾è‘—æå‡äº†ç‰™ç§‘å›¾åƒåˆ†æä¸ç†è§£èƒ½åŠ›ï¼Œå¹¶åœ¨æ–°æå‡ºçš„MMOral-UniåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚è¡¨ç°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¼—å¤šåŒ»å­¦ä¸“ä¸šé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç‰™ç§‘é¢†åŸŸä»æœªè¢«å……åˆ†æ¢ç´¢ï¼Œä¸»è¦ç”±äºé¢†åŸŸç‰¹å®šæ•°æ®æœ‰é™ã€ç‰™ç§‘ä¸“å®¶æ ‡æ³¨ç¨€ç¼ºã€æ¨¡æ€ç‰¹å®šå»ºæ¨¡ä¸è¶³ä»¥åŠå¯é æ€§æŒ‘æˆ˜ç­‰é—®é¢˜ï¼Œå› æ­¤éœ€è¦å¼€å‘ä¸“é—¨é’ˆå¯¹ç‰™ç§‘çš„ç»¼åˆå¯ä¿¡åˆ†æç³»ç»Ÿã€‚

**Method:** ç ”ç©¶æ„å»ºäº†TRACE-CoTä¸´åºŠæ¨ç†æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¨¡æ‹Ÿç‰™ç§‘æ”¾å°„ç§‘åŒ»å¸ˆçš„å†³ç­–è¿‡ç¨‹ï¼Œä»¥æ˜¾å¼æ•æ‰ç‰™åŒ»çš„è¯Šæ–­æ¨ç†ï¼›åŒæ—¶æå‡ºäº†å››é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œç»“åˆæ¨ç†ç›‘ç£æ¥å¢å¼ºæ¨¡å‹å¯¹ç‰™ç§‘å›¾åƒçš„ç†è§£ä¸åˆ†æèƒ½åŠ›ï¼›æ­¤å¤–è¿˜åˆ›å»ºäº†MMOral-UniåŸºå‡†ï¼Œè¿™æ˜¯é¦–ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€ç‰™ç§‘å›¾åƒåˆ†æåŸºå‡†ï¼ŒåŒ…å«2,809ä¸ªå¼€æ”¾æ€§é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œæ¶µç›–äº”ç§æ¨¡æ€å’Œäº”é¡¹ä»»åŠ¡ã€‚

**Result:** OralGPT-Omniåœ¨MMOral-UniåŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†51.84çš„æ€»åˆ†ï¼Œåœ¨MMOral-OPGåŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†45.31åˆ†ï¼Œæ˜¾è‘—è¶…è¶Šäº†GPT-5çš„è¡¨ç°ï¼Œè¯æ˜äº†è¯¥æ¨¡å‹åœ¨ç‰™ç§‘å›¾åƒåˆ†æä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ¨åŠ¨äº†æ™ºèƒ½ç‰™ç§‘çš„å‘å±•ï¼Œä¸ºç‰™ç§‘å›¾åƒåˆ†æçš„æœªæ¥è¿›å±•é“ºå¹³äº†é“è·¯ï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„æ¨¡å‹æ¶æ„ã€ä¸´åºŠæ¨ç†æ•°æ®é›†å’Œç»¼åˆè¯„ä¼°åŸºå‡†ï¼Œä¸ºè§£å†³ç‰™ç§‘é¢†åŸŸå¤šæ¨¡æ€åˆ†æä¸­çš„æŒ‘æˆ˜æä¾›äº†ç³»ç»ŸåŒ–è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPT-Omni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists' diagnostic reasoning, we construct TRACE-CoT, a clinically grounded chain-of-thought dataset that mirrors dental radiologists' decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the model's capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended question-answer pairs spanning five modalities and five tasks, offering a comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT-5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available.


### [14] [MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation](https://arxiv.org/abs/2511.22102)
*Simon Joseph ClÃ©ment CrÃªte, Marta Kersten-Oertel, Yiming Xiao*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é¦–æ¬¡å°†ç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸Rank-N-ContrastæŸå¤±å‡½æ•°åº”ç”¨äºåŸºäºT1wç»“æ„MRIçš„è„‘é¾„ä¼°è®¡ï¼Œæå‡ºäº†ä¸€ç§èƒ½å¤Ÿæ›´å¥½æ•æ‰ç¥ç»å½¢æ€è¿ç»­å˜åŒ–çš„æ–¹æ³•ï¼Œå¹¶åœ¨ç¥ç»é€€è¡Œæ€§ç–¾ç—…ä¸­éªŒè¯äº†è„‘é¾„å·®ä½œä¸ºç”Ÿç‰©æ ‡å¿—ç‰©çš„æ½œåŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„è„‘é¾„ä¼°è®¡æ–¹æ³•å¾€å¾€éš¾ä»¥æ•æ‰ç¥ç»å½¢æ€å˜åŒ–çš„è¿ç»­æ€§ï¼Œå¯èƒ½å¯¼è‡´æ¬¡ä¼˜çš„ç‰¹å¾è¡¨ç¤ºå’Œç»“æœã€‚ç¥ç»é€€è¡Œæ€§ç–¾ç—…ç­‰å› ç´ ä¼šåŠ é€Ÿå¤§è„‘è¡°è€ï¼Œå‡†ç¡®æµ‹é‡è¿™ä¸€ç°è±¡å¯¹äºä¸´åºŠç”Ÿç‰©æ ‡å¿—ç‰©åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚

**Method:** æœ¬ç ”ç©¶é¦–æ¬¡å°†ç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸Rank-N-ContrastæŸå¤±å‡½æ•°åº”ç”¨äºåŸºäºT1wç»“æ„MRIçš„è„‘é¾„ä¼°è®¡ï¼Œé‡‡ç”¨ResNetä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œå¹¶åˆ©ç”¨Grad-RAMå¯¹å›å½’ç»“æœè¿›è¡Œå¯è§†åŒ–è§£é‡Šï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

**Result:** åœ¨æœ‰é™è®­ç»ƒæ ·æœ¬çš„æ•°æ®é›†ä¸Šï¼Œæ‰€ææ–¹æ³•å®ç°äº†4.27å¹´çš„å¹³å‡ç»å¯¹è¯¯å·®å’Œ0.93çš„RÂ²åˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºä½¿ç”¨ç›¸åŒResNetéª¨å¹²çš„ä¼ ç»Ÿæ·±åº¦å›å½’æ–¹æ³•ï¼Œå¹¶ä¸ä½¿ç”¨æ›´å¤§è®­ç»ƒæ•°æ®çš„å…ˆè¿›æ–¹æ³•è¡¨ç°ç›¸å½“æˆ–æ›´å¥½ã€‚Grad-RAMå¯è§†åŒ–æ˜¾ç¤ºRNCæŸå¤±èƒ½æ•æ‰æ¯”ä¼ ç»Ÿå›å½’æ›´ç»†å¾®çš„å¹´é¾„ç›¸å…³ç‰¹å¾ã€‚

**Conclusion:** è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†è„‘é¾„ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œè¿˜é€šè¿‡è„‘é¾„å·®åˆ†ææ­ç¤ºäº†é˜¿å°”èŒ¨æµ·é»˜ç—…å’Œå¸•é‡‘æ£®ç—…æ‚£è€…è„‘é¾„å·®ä¸ç–¾ç—…ä¸¥é‡ç¨‹åº¦çš„ç›¸å…³æ€§ï¼Œè¯æ˜äº†å…¶ä½œä¸ºç¥ç»é€€è¡Œæ€§ç–¾ç—…ç”Ÿç‰©æ ‡å¿—ç‰©çš„æ½œåœ¨ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
MRI-based brain age estimation models aim to assess a subject's biological brain age based on information, such as neuroanatomical features. Various factors, including neurodegenerative diseases, can accelerate brain aging and measuring this phenomena could serve as a potential biomarker for clinical applications. While deep learning (DL)-based regression has recently attracted major attention, existing approaches often fail to capture the continuous nature of neuromorphological changes, potentially resulting in sub-optimal feature representation and results. To address this, we propose to use supervised contrastive learning with the recent Rank-N-Contrast (RNC) loss to estimate brain age based on widely used T1w structural MRI for the first time and leverage Grad-RAM to visually explain regression results. Experiments show that our proposed method achieves a mean absolute error (MAE) of 4.27 years and an $R^2$ of 0.93 with a limited dataset of training samples, significantly outperforming conventional deep regression with the same ResNet backbone while performing better or comparably with the state-of-the-art methods with significantly larger training data. Furthermore, Grad-RAM revealed more nuanced features related to age regression with the RNC loss than conventional deep regression. As an exploratory study, we employed the proposed method to estimate the gap between the biological and chronological brain ages in Alzheimer's Disease and Parkinson's disease patients, and revealed the correlation between the brain age gap and disease severity, demonstrating its potential as a biomarker in neurodegenerative disorders.


### [15] [MoE3D: Mixture of Experts meets Multi-Modal 3D Understanding](https://arxiv.org/abs/2511.22103)
*Yu Li, Yuenan Hou, Yingmei Wei, Xinge Zhu, Yuexin Ma, Wenqi Shao, Yanming Guo*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMoE3Dï¼Œä¸€ç§åŸºäºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„å¤šæ¨¡æ€3Dç†è§£æ¡†æ¶ï¼Œé€šè¿‡éƒ¨ç½²ä¸“é—¨åŒ–çš„ä¸“å®¶ç½‘ç»œå¤„ç†ä¸åŒæ¨¡æ€æˆ–è·¨æ¨¡æ€äº¤äº’ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€èåˆæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¤šæ¨¡æ€èåˆæ–¹æ³•é€šå¸¸é‡‡ç”¨å•ä¸€å¯†é›†èåˆç½‘ç»œï¼Œéš¾ä»¥å¤„ç†æ¨¡æ€é—´çš„æ˜¾è‘—å¼‚è´¨æ€§å’Œå¤æ‚æ€§ï¼Œå¯¼è‡´æ€§èƒ½æ¬ ä½³ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶æ¥è§£å†³è¿™ä¸€å±€é™æ€§ã€‚

**Method:** æå‡ºMoE3Dæ¡†æ¶ï¼Œå°†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰é›†æˆåˆ°å¤šæ¨¡æ€å­¦ä¹ ä¸­ï¼Œéƒ¨ç½²ä¸€ç»„ä¸“é—¨åŒ–ä¸“å®¶ç½‘ç»œå¤„ç†ç‰¹å®šæ¨¡æ€æˆ–è·¨æ¨¡æ€äº¤äº’ï¼›è®¾è®¡åŸºäºMoEçš„Transformerä»¥æ›´å¥½åˆ©ç”¨è§†è§‰ç‰¹å¾ä¸­çš„äº’è¡¥ä¿¡æ¯ï¼›å¼•å…¥ä¿¡æ¯èšåˆæ¨¡å—å¢å¼ºèåˆæ€§èƒ½ï¼›é‡‡ç”¨Top-1é—¨æ§æœºåˆ¶ç¡®ä¿é«˜æ•ˆå¤„ç†ï¼›æå‡ºæ¸è¿›å¼é¢„è®­ç»ƒç­–ç•¥ä»¥åˆ©ç”¨è¯­ä¹‰å’Œ2Då…ˆéªŒçŸ¥è¯†è¿›è¡Œè‰¯å¥½åˆå§‹åŒ–ã€‚

**Result:** MoE3Dåœ¨å››ä¸ªä¸»æµ3Dç†è§£ä»»åŠ¡ä¸Šå–å¾—ç«äº‰æ€§æ€§èƒ½ï¼Œåœ¨Multi3DReferåŸºå‡†ä¸Šè¶…è¶Šæœ€ä½³å¯¹æ¯”æ–¹æ³•6.1 mIoUï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ¨¡æ€3Dç†è§£ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜æ··åˆä¸“å®¶æ¶æ„èƒ½æœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€å¼‚è´¨æ€§ï¼Œæ¸è¿›å¼é¢„è®­ç»ƒç­–ç•¥æœ‰åŠ©äºåˆ©ç”¨å…ˆéªŒçŸ¥è¯†ï¼Œä¸ºå¤šæ¨¡æ€3Dç†è§£æä¾›äº†æ–°çš„é«˜æ•ˆèåˆæ¡†æ¶ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Multi-modal 3D understanding is a fundamental task in computer vision. Previous multi-modal fusion methods typically employ a single, dense fusion network, struggling to handle the significant heterogeneity and complexity across modalities, leading to suboptimal performance. In this paper, we propose MoE3D, which integrates Mixture of Experts (MoE) into the multi-modal learning framework. The core is that we deploy a set of specialized "expert" networks, each adept at processing a specific modality or a mode of cross-modal interaction. Specifically, the MoE-based transformer is designed to better utilize the complementary information hidden in the visual features. Information aggregation module is put forward to further enhance the fusion performance. Top-1 gating is employed to make one expert process features with expert groups, ensuring high efficiency. We further propose a progressive pre-training strategy to better leverage the semantic and 2D prior, thus equipping the network with good initialization. Our MoE3D achieves competitive performance across four prevalent 3D understanding tasks. Notably, our MoE3D surpasses the top-performing counterpart by 6.1 mIoU on Multi3DRefer.


### [16] [HyperST: Hierarchical Hyperbolic Learning for Spatial Transcriptomics Prediction](https://arxiv.org/abs/2511.22107)
*Chen Zhang, Yilu An, Ying Chen, Hao Li, Xitong Ling, Lihao Liu, Junjun He, Yuxiang Lin, Zihui Wang, Rongshan Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºHyperSTæ¡†æ¶ï¼Œé€šè¿‡åŒæ›²ç©ºé—´å»ºæ¨¡ç©ºé—´è½¬å½•ç»„æ•°æ®çš„å›ºæœ‰å±‚æ¬¡ç»“æ„ï¼Œå®ç°ä»ç»„ç»‡ç—…ç†å›¾åƒåˆ°åŸºå› è¡¨è¾¾çš„å¤šå±‚æ¬¡è·¨æ¨¡æ€é¢„æµ‹ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç‚¹æ°´å¹³çš„å›¾åƒ-åŸºå› åŒ¹é…ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ç©ºé—´è½¬å½•ç»„æ•°æ®çš„å®Œæ•´å±‚æ¬¡ç»“æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºå› è¡¨è¾¾ä¾§ã€‚æ­¤å¤–ï¼ŒåŸºå› è¡¨è¾¾è°±åŒ…å«æ›´å¤šåˆ†å­ç»†èŠ‚ï¼Œè¿™äº›ç»†èŠ‚å¯èƒ½åœ¨ç»„ç»‡å­¦å›¾åƒä¸­ç¼ºä¹æ˜¾è‘—çš„è§†è§‰å¯¹åº”ç‰©ï¼Œå¯¼è‡´æ¨¡æ€é—´ä¿¡æ¯ä¸å¯¹ç§°ï¼Œéœ€è¦å¤æ‚çš„è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ¥å¼¥åˆè¿™ä¸€å·®è·ã€‚

**Method:** HyperSTæ¡†æ¶åœ¨åŒæ›²ç©ºé—´ä¸­å»ºæ¨¡æ•°æ®çš„å›ºæœ‰å±‚æ¬¡ç»“æ„ï¼Œé¦–å…ˆè®¾è®¡å¤šçº§è¡¨ç¤ºæå–å™¨æ•è·æ¯ä¸ªæ¨¡æ€çš„ç‚¹æ°´å¹³å’Œç”Ÿæ€ä½æ°´å¹³è¡¨ç¤ºï¼Œæä¾›è¶…è¶Šå•ä¸ªç‚¹æ°´å¹³å›¾åƒ-åŸºå› å¯¹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¿¡æ¯ï¼›å…¶æ¬¡å¼•å…¥å±‚æ¬¡åŒæ›²å¯¹é½æ¨¡å—ç»Ÿä¸€è¿™äº›è¡¨ç¤ºï¼Œåœ¨æ‰§è¡Œç©ºé—´å¯¹é½çš„åŒæ—¶å±‚æ¬¡åŒ–åœ°ç»“æ„åŒ–å›¾åƒå’ŒåŸºå› åµŒå…¥ï¼Œè¯¥å¯¹é½ç­–ç•¥ç”¨åˆ†å­è¯­ä¹‰ä¸°å¯Œå›¾åƒè¡¨ç¤ºã€‚

**Result:** HyperSTåœ¨æ¥è‡ªä¸åŒç»„ç»‡çš„å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æ”¹å–„äº†è·¨æ¨¡æ€é¢„æµ‹ï¼Œä¸ºæ›´å¯æ‰©å±•å’Œå‡†ç¡®çš„ç©ºé—´è½¬å½•ç»„å­¦é¢„æµ‹é“ºå¹³äº†é“è·¯ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡å»ºæ¨¡ç©ºé—´è½¬å½•ç»„æ•°æ®çš„å›ºæœ‰å±‚æ¬¡ç»“æ„å¹¶åˆ©ç”¨åŒæ›²ç©ºé—´çš„å‡ ä½•ç‰¹æ€§ï¼ŒæˆåŠŸå¼¥åˆäº†ç»„ç»‡ç—…ç†å›¾åƒä¸åŸºå› è¡¨è¾¾ä¹‹é—´çš„æ¨¡æ€å·®è·ï¼Œä¸ºæˆæœ¬æ•ˆç›Šé«˜çš„ç©ºé—´è½¬å½•ç»„å­¦é¢„æµ‹æä¾›äº†æ–°æ¡†æ¶ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸå‘æ›´å¯æ‰©å±•å’Œå‡†ç¡®çš„æ–¹å‘å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Spatial Transcriptomics (ST) merges the benefits of pathology images and gene expression, linking molecular profiles with tissue structure to analyze spot-level function comprehensively. Predicting gene expression from histology images is a cost-effective alternative to expensive ST technologies. However, existing methods mainly focus on spot-level image-to-gene matching but fail to leverage the full hierarchical structure of ST data, especially on the gene expression side, leading to incomplete image-gene alignment. Moreover, a challenge arises from the inherent information asymmetry: gene expression profiles contain more molecular details that may lack salient visual correlates in histological images, demanding a sophisticated representation learning approach to bridge this modality gap. We propose HyperST, a framework for ST prediction that learns multi-level image-gene representations by modeling the data's inherent hierarchy within hyperbolic space, a natural geometric setting for such structures. First, we design a Multi-Level Representation Extractors to capture both spot-level and niche-level representations from each modality, providing context-aware information beyond individual spot-level image-gene pairs. Second, a Hierarchical Hyperbolic Alignment module is introduced to unify these representations, performing spatial alignment while hierarchically structuring image and gene embeddings. This alignment strategy enriches the image representations with molecular semantics, significantly improving cross-modal prediction. HyperST achieves state-of-the-art performance on four public datasets from different tissues, paving the way for more scalable and accurate spatial transcriptomics prediction.


### [17] [PROMPTMINER: Black-Box Prompt Stealing against Text-to-Image Generative Models via Reinforcement Learning and Fuzz Optimization](https://arxiv.org/abs/2511.22119)
*Mingzhe Li, Renhao Zhang, Zhiyang Wen, Siqi Pan, Bruno Castro da Silva, Juan Zhai, Shiqing Ma*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PROMPTMINERï¼Œä¸€ç§ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„é»‘ç›’æç¤ºçªƒå–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å’Œæ¨¡ç³Šæœç´¢ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œèƒ½å¤Ÿä»æœªçŸ¥ç”Ÿæˆå™¨çš„å›¾åƒä¸­æœ‰æ•ˆæ¢å¤åŸå§‹æ–‡æœ¬æç¤ºï¼Œåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ‰©æ•£æ¨¡å‹ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å‘å±•ï¼Œç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯å·²æˆä¸ºå…·æœ‰ä»·å€¼çš„æ•°å­—èµ„äº§ï¼Œä½†é¢ä¸´ç€å®‰å…¨æ€§å’ŒçŸ¥è¯†äº§æƒé£é™©ï¼Œç‰¹åˆ«æ˜¯æç¤ºçªƒå–æ”»å‡»çš„å¨èƒã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦ç™½ç›’æ¢¯åº¦è®¿é—®ã€å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†è¿›è¡Œç›‘ç£è®­ç»ƒï¼Œæˆ–ä»…ä¾èµ–å›¾åƒæè¿°è€Œä¸è¿›è¡Œæ˜¾å¼ä¼˜åŒ–ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§å’Œé€‚åº”æ€§ã€‚

**Method:** PROMPTMINERé‡‡ç”¨ä¸¤é˜¶æ®µé»‘ç›’æç¤ºçªƒå–æ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•é‡å»ºå›¾åƒçš„ä¸»è¦ä¸»é¢˜ï¼Œç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ¨¡ç³Šé©±åŠ¨çš„æœç´¢æ–¹æ³•æ¢å¤é£æ ¼ä¿®é¥°ç¬¦ã€‚è¯¥æ¡†æ¶ä¸ä¾èµ–æ¢¯åº¦è®¿é—®æˆ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®ï¼Œé€šè¿‡è§£è€¦ä¸»é¢˜å’Œé£æ ¼æ¢å¤ä»»åŠ¡æ¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

**Result:** åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ‰©æ•£æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPROMPTMINERå®ç°äº†CLIPç›¸ä¼¼åº¦é«˜è¾¾0.958å’ŒSBERTæ–‡æœ¬å¯¹é½åº¦è¾¾0.751çš„ä¼˜å¼‚æ€§èƒ½ï¼Œè¶…è¶Šäº†æ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚å¯¹äºæœªçŸ¥ç”Ÿæˆå™¨çš„çœŸå®ä¸–ç•Œå›¾åƒï¼Œå…¶CLIPç›¸ä¼¼åº¦æ¯”æœ€å¼ºåŸºçº¿é«˜å‡º7.5%ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚å³ä½¿åœ¨é˜²å¾¡æ€§æ‰°åŠ¨ä¸‹ï¼Œè¯¥æ¡†æ¶ä»ä¿æŒå¼ºå¤§çš„é²æ£’æ€§ã€‚

**Conclusion:** PROMPTMINERå±•ç¤ºäº†é»‘ç›’æç¤ºçªƒå–çš„å®é™…å¯è¡Œæ€§ï¼Œä¸ºæç¤ºå®‰å…¨æ€§å’ŒçŸ¥è¯†äº§æƒä¿æŠ¤æä¾›äº†é‡è¦è§è§£ã€‚è¯¥æ¡†æ¶ä¸ä»…å¯ç”¨äºæ¶æ„æ”»å‡»æ£€æµ‹ï¼Œè¿˜èƒ½æ”¯æŒæ•°æ®å½’å±ã€æ¨¡å‹æ¥æºåˆ†æå’Œæ°´å°éªŒè¯ç­‰æœ‰ç›Šåº”ç”¨ï¼Œä¸ºç”Ÿæˆæ¨¡å‹çš„å®‰å…¨è¯„ä¼°å’Œå®¡è®¡å·¥å…·å¼€å‘å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Text-to-image (T2I) generative models such as Stable Diffusion and FLUX can synthesize realistic, high-quality images directly from textual prompts. The resulting image quality depends critically on well-crafted prompts that specify both subjects and stylistic modifiers, which have become valuable digital assets. However, the rising value and ubiquity of high-quality prompts expose them to security and intellectual-property risks. One key threat is the prompt stealing attack, i.e., the task of recovering the textual prompt that generated a given image. Prompt stealing enables unauthorized extraction and reuse of carefully engineered prompts, yet it can also support beneficial applications such as data attribution, model provenance analysis, and watermarking validation. Existing approaches often assume white-box gradient access, require large-scale labeled datasets for supervised training, or rely solely on captioning without explicit optimization, limiting their practicality and adaptability. To address these challenges, we propose PROMPTMINER, a black-box prompt stealing framework that decouples the task into two phases: (1) a reinforcement learning-based optimization phase to reconstruct the primary subject, and (2) a fuzzing-driven search phase to recover stylistic modifiers. Experiments across multiple datasets and diffusion backbones demonstrate that PROMPTMINER achieves superior results, with CLIP similarity up to 0.958 and textual alignment with SBERT up to 0.751, surpassing all baselines. Even when applied to in-the-wild images with unknown generators, it outperforms the strongest baseline by 7.5 percent in CLIP similarity, demonstrating better generalization. Finally, PROMPTMINER maintains strong performance under defensive perturbations, highlighting remarkable robustness. Code: https://github.com/aaFrostnova/PromptMiner


### [18] [From Compound Figures to Composite Understanding: Developing a Multi-Modal LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation](https://arxiv.org/abs/2511.22232)
*Zhen Chen, Yihang Fu, Gabriel Madera, Mauro Giuffre, Serina Applebaum, Hyunjae Kim, Hua Xu, Qingyu Chen*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºM3LLMï¼Œä¸€ç§èƒ½å¤Ÿè¿›è¡Œå¤šå›¾åƒå¤åˆç†è§£çš„åŒ»å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡åˆ©ç”¨ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®ä¸­çš„å¤åˆå›¾åƒæ•°æ®è§£å†³äº†åŒ»ç–—MLLMsåœ¨å¤šå›¾åƒåˆ†ææ–¹é¢è®­ç»ƒæ•°æ®åŒ®ä¹çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŒ»å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸»è¦å±€é™äºå•å›¾åƒç†è§£ï¼Œæ— æ³•æ»¡è¶³ä¸´åºŠå®è·µä¸­éœ€è¦ç»¼åˆå¤šæ¨¡æ€æˆ–å¤šæ—¶é—´ç‚¹å›¾åƒä¿¡æ¯çš„è¯Šæ–­éœ€æ±‚ï¼Œä¸”ç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡çš„å¤šå›¾åƒæ ‡æ³¨è®­ç»ƒæ•°æ®é˜»ç¢äº†ç›¸å…³æ¨¡å‹çš„å‘å±•ã€‚

**Method:** æå‡ºåŸºäºè®¸å¯å…è®¸çš„ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®å¤åˆå›¾åƒä½œä¸ºæ•°æ®æºï¼Œè®¾è®¡äº”é˜¶æ®µä¸Šä¸‹æ–‡æ„ŸçŸ¥æŒ‡ä»¤ç”ŸæˆèŒƒå¼ï¼Œé‡‡ç”¨åˆ†æ²»ç­–ç•¥å°†å¤šå›¾åƒåˆ†æåˆ†è§£ä¸ºå¯ç®¡ç†çš„å­ä»»åŠ¡ï¼Œé€šè¿‡è§£æè¶…è¿‡237,000ä¸ªå¤åˆå›¾åƒåŠå…¶ä¸Šä¸‹æ–‡æ–‡æœ¬æ„å»ºM3LLMæ¨¡å‹ã€‚

**Result:** M3LLMåœ¨å¤šå›¾åƒã€å•å›¾åƒã€çº¯æ–‡æœ¬å’Œå¤šé€‰æ‹©åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºé€šç”¨å’Œä¸“ä¸šåŒ»å­¦MLLMsï¼Œåœ¨PMC-MI-BenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨MIMICæ•°æ®é›†ä¸Šçš„çºµå‘èƒ¸éƒ¨Xå…‰åˆ†æä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** è¯¥å·¥ä½œå»ºç«‹äº†å¯æ‰©å±•ä¸”é«˜æ•ˆçš„åŒ»å­¦MLLMså¼€å‘èŒƒå¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¤åˆå›¾åƒä¸­å¤æ‚çš„ç©ºé—´ã€æ—¶é—´å’Œè·¨æ¨¡æ€å…³ç³»ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦æ–‡çŒ®ä¸çœŸå®ä¸´åºŠåº”ç”¨ä¹‹é—´æ¶èµ·äº†æ¡¥æ¢ï¼Œæ¨åŠ¨äº†åŒ»ç–—AIå‘å¤åˆæ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Multi-modal large language models (MLLMs) have shown promise in advancing healthcare. However, most existing models remain confined to single-image understanding, which greatly limits their applicability in clinical workflows. In practice, medical diagnosis and progression often require synthesizing information across multiple images from different modalities or time points. The development of medical MLLMs capable of such multi-image understanding has been hindered by the lack of large-scale, high-quality annotated training data. To address this limitation, we propose a novel framework that leverages license-permissive compound images in biomedical literature, as a rich yet underutilized data source for multi-image analysis. Specifically, we design a five-stage, context-aware instruction generation paradigm underpinned by a divide-and-conquer strategy. By decomposing multi-image analysis into manageable sub-tasks, this paradigm empowers MLLMs to move beyond single-panel analysis and provide a composite understanding by learning the complex spatial, temporal, and cross-modal relationships inherent in these compound figures. By parsing over 237,000 compound figures and their contextual text for instruction generation, we develop M3LLM, a medical multi-image multi-modal large language model. For benchmarking, we construct PMC-MI-Bench for composite understanding, manually validated by medical experts. Extensive experiments show that M3LLM significantly outperforms both general-purpose and specialized medical MLLMs across multi-image, single-image, text-only, and multi-choice scenarios. Notably, M3LLM exhibits strong generalization to longitudinal chest X-ray analysis using the MIMIC dataset. This work establishes a scalable and efficient paradigm for developing medical MLLMs capable of composite reasoning, bridging the gap between biomedical literature and real-world clinical applications.


### [19] [GA2-CLIP: Generic Attribute Anchor for Efficient Prompt Tuningin Video-Language Models](https://arxiv.org/abs/2511.22125)
*Bin Wang, Ruotong Hu, Wenqian Wang, Wentong Li, Mingliang Gao, Runmin Cong, Wei Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„è€¦åˆæç¤ºå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å¤–éƒ¨ç›‘ç£æç¤ºæ¥ç¼“è§£è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ä»»åŠ¡å¾®è°ƒä¸­çš„è¯­ä¹‰ç©ºé—´çª„åŒ–é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æœªè§ç±»åˆ«ä¸Šçš„æ³›åŒ–æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ä¼šæŸå®³æ¨¡å‹å¯¹æœªè§ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¼ ç»Ÿæ–¹æ³•é€šè¿‡æ­£åˆ™åŒ–æ‰‹å·¥æç¤ºä¸è½¯æç¤ºä¹‹é—´çš„å·®è·æ¥ç¼“è§£é—å¿˜æ•ˆåº”ï¼Œä½†è¿™ä¼šå‰Šå¼±è½¯æç¤ºçš„å­¦ä¹ èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦è§£å†³å¾®è°ƒè¿‡ç¨‹ä¸­è¯­ä¹‰ç©ºé—´çª„åŒ–çš„é—®é¢˜ã€‚

**Method:** æœ¬æ–‡æå‡ºè€¦åˆæç¤ºå­¦ä¹ æ¡†æ¶ï¼Œåœ¨æ–‡æœ¬æç¤ºä¸­å¼•å…¥æ¥è‡ªå…¶ä»–æ•°æ®é›†çš„é¢„è®­ç»ƒæç¤ºä½œä¸ºç¡¬æç¤ºæ ‡è®°ï¼Œä¸è½¯æç¤ºæ ‡è®°æ‹¼æ¥å¹¶é€šè¿‡å¯å­¦ä¹ çš„æ˜ å°„å±‚è€¦åˆï¼Œå½¢æˆç«äº‰æ€§æç¤ºæœºåˆ¶é˜²æ­¢è¯­ä¹‰ç©ºé—´è¿‡åº¦æ‹Ÿåˆç›‘ç£ç±»åˆ«ï¼›åŒæ—¶å¼•å…¥ç²¾å¿ƒè®¾è®¡çš„æ— å…³è§†é¢‘é›†å’Œè´Ÿæç¤ºä½œä¸ºé€šç”¨å±æ€§é”šç‚¹ï¼Œä¿æŒé¢„è®­ç»ƒè¯­ä¹‰ç©ºé—´ä¸­å±æ€§çš„é€šç”¨ç›¸å…³æ€§ã€‚

**Result:** åœ¨è§†é¢‘ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ³›åŒ–åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æç¤ºè°ƒä¼˜æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºç¡€åˆ°æ–°ç±»åˆ«çš„é¢„æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹å¯¹æœªè§ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡ç«äº‰æ€§æç¤ºæœºåˆ¶å’Œé€šç”¨å±æ€§é”šç‚¹çš„å¼•å…¥ï¼ŒæˆåŠŸç¼“è§£äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ä»»åŠ¡å¾®è°ƒä¸­çš„è¯­ä¹‰ç©ºé—´çª„åŒ–é—®é¢˜ï¼Œä¸ºæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œè¡¨æ˜å¤–éƒ¨ç›‘ç£æç¤ºå’Œå±æ€§ä¿æŒæœºåˆ¶åœ¨æç¤ºå­¦ä¹ ä¸­çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Visual and textual soft prompt tuning can effectively improve the adaptability of Vision-Language Models (VLMs) in downstream tasks. However, fine-tuning on video tasks impairs the model's generalization ability to unseen classes. Existing methods attempt to mitigate this forgetting effect by regularizing the gap between hand-crafted prompts and soft prompts, but this also weakens the learning ability of soft prompts. To address this challenge, we propose a plug-and-play coupling prompt learning framework to optimize the generalization performance of V-L models in video tasks, with the core motivation of mitigating semantic space narrowing during fine-tuning by introducing an externally supervised prompt. Specifically, for textual prompts, we introduce pre-trained prompts from other datasets as hard prompt tokens. These are concatenated with soft prompt tokens and coupled via a learnable mapping layer. This competitive prompting approach prevents the semantic space from overfitting to supervised categories. In addition, we introduce a set of well-designed irrelevant video sets and negative prompts as generic attribute anchors to maintain the generic relevance of the attributes in the pre-trained semantic space, thus preserving the generalization ability. Experiments on video tasks demonstrate that our method significantly outperforms state-of-the-art prompt tuning approaches across generalization benchmarks, particularly on base-to-new class prediction.


### [20] [ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2511.22715)
*Alberto Compagnoni, Marco Morini, Sara Sarto, Federico Cocchi, Davide Caffagni, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºReAGï¼Œä¸€ç§æ–°é¢–çš„æ¨ç†å¢å¼ºå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆç²—ç²’åº¦ä¸ç»†ç²’åº¦æ£€ç´¢ä»¥åŠæ‰¹åˆ¤æ¨¡å‹è¿‡æ»¤æ— å…³æ®µè½ï¼Œæ˜¾è‘—æå‡äº†çŸ¥è¯†å¯†é›†å‹è§†è§‰é—®ç­”ä»»åŠ¡çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é¢†åŸŸç‰¹å®šæˆ–çŸ¥è¯†å¯†é›†å‹æŸ¥è¯¢ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œç°æœ‰çš„åŸºäºæ£€ç´¢çš„è§†è§‰é—®ç­”æ–¹æ³•å­˜åœ¨æ£€ç´¢ç²¾åº¦ä½ã€å™ªå£°æ®µè½å¤šå’Œæ¨ç†èƒ½åŠ›æœ‰é™çš„é—®é¢˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„æ£€ç´¢å¢å¼ºæ–¹æ³•æ¥æä¾›é«˜è´¨é‡ä¸Šä¸‹æ–‡ã€‚

**Method:** ReAGé‡‡ç”¨ç²—ç²’åº¦ä¸ç»†ç²’åº¦æ£€ç´¢ç›¸ç»“åˆçš„ç­–ç•¥ï¼Œå¼•å…¥æ‰¹åˆ¤æ¨¡å‹è¿‡æ»¤æ— å…³æ®µè½ä»¥ç¡®ä¿é«˜è´¨é‡ä¸Šä¸‹æ–‡ï¼Œé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¢å¼ºå¯¹æ£€ç´¢å†…å®¹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œç›‘ç£å¾®è°ƒä»…ä½œä¸ºå†·å¯åŠ¨ã€‚

**Result:** åœ¨Encyclopedic-VQAå’ŒInfoSeekæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒReAGæ˜¾è‘—ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œæé«˜äº†ç­”æ¡ˆå‡†ç¡®æ€§ï¼Œå¹¶æä¾›äº†åŸºäºæ£€ç´¢è¯æ®çš„å¯è§£é‡Šæ¨ç†ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜æ¨ç†å¢å¼ºçš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•èƒ½æœ‰æ•ˆè§£å†³çŸ¥è¯†å¯†é›†å‹å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„ä¿¡æ¯ä¸è¶³é—®é¢˜ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°æ€è·¯ï¼Œå¼ºè°ƒäº†é«˜è´¨é‡æ£€ç´¢ä¸Šä¸‹æ–‡ä¸å¼ºåŒ–å­¦ä¹ æ¨ç†ç›¸ç»“åˆçš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.


### [21] [DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action](https://arxiv.org/abs/2511.22134)
*Zhen Fang, Zhuoyang Liu, Jiaming Liu, Hao Chen, Yu Zeng, Shiting Huang, Zehui Chen, Lin Chen, Shanghang Zhang, Feng Zhao*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºDualVLAæ–¹æ³•è§£å†³è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­åŠ¨ä½œé€€åŒ–é—®é¢˜ï¼Œé€šè¿‡åŒé‡æ•°æ®å‰ªæå’ŒåŒæ•™å¸ˆè‡ªé€‚åº”è’¸é¦ç­–ç•¥ï¼Œåœ¨ä¿æŒæ¨ç†èƒ½åŠ›çš„åŒæ—¶å¢å¼ºåŠ¨ä½œæ€§èƒ½ï¼Œå¹¶å¼•å…¥VLA Scoreè¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ„å»ºé€šç”¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ—¶å­˜åœ¨åŠ¨ä½œé€€åŒ–é—®é¢˜ï¼šåœ¨å°†ä¸“å®¶VLAæ¨¡å‹ä¸å¤šæ¨¡æ€æ•°æ®æ··åˆå¾®è°ƒä»¥æ¢å¤æ¨ç†èƒ½åŠ›çš„è¿‡ç¨‹ä¸­ï¼ŒåŠ¨ä½œæ€§èƒ½ç›¸æ¯”ä¸“å®¶æ¨¡å‹æ˜¾è‘—ä¸‹é™ï¼Œè¿™é™åˆ¶äº†é€šç”¨VLAæ¨¡å‹çš„å®é™…åº”ç”¨æ•ˆæœã€‚

**Method:** æå‡ºDualVLAæ¡†æ¶ï¼Œé‡‡ç”¨åŒé‡æ•°æ®å‰ªææ–¹æ³•å»é™¤å†—ä½™çš„å…·èº«æ¨ç†æ•°æ®ä»¥é¿å…å¯¹åŠ¨ä½œå­¦ä¹ äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå¹¶è®¾è®¡åŒæ•™å¸ˆè‡ªé€‚åº”è’¸é¦ç­–ç•¥ä¸ºä¸åŒæ•°æ®åŸŸåˆ†é…ä¸åŒçš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶ä¿æŒæ¨ç†èƒ½åŠ›ï¼›æ­¤å¤–è¿˜æå‡ºVLA Scoreè¯„ä¼°æŒ‡æ ‡ï¼Œå°†VLAèƒ½åŠ›è§£è€¦ä¸ºæ¨ç†ã€æ„å›¾ã€åŠ¨ä½œå’Œå¯¹é½å››ä¸ªç»´åº¦è¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚

**Result:** å®éªŒè¡¨æ˜DualVLAåœ¨SimplerEnvä¸­è¾¾åˆ°61.0%çš„å¹³å‡æˆåŠŸç‡ï¼Œåœ¨å…«ä¸ªç«äº‰æ€§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è·å¾—65.4çš„å¹³å‡åˆ†æ•°ï¼Œåœ¨ç²¾ç¡®åŠ¨ä½œæ‰§è¡Œå’Œå¤šæ¨¡æ€ç†è§£ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ï¼Œæ˜¾è‘—ç¼“è§£äº†åŠ¨ä½œé€€åŒ–é—®é¢˜ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†é€šç”¨VLAæ¨¡å‹ä¸­åŠ¨ä½œé€€åŒ–çš„æ ¹æœ¬åŸå› ï¼Œå¹¶æå‡ºæœ‰æ•ˆçš„åè®­ç»ƒç­–ç•¥æ¥å¹³è¡¡æ¨ç†èƒ½åŠ›å’ŒåŠ¨ä½œæ€§èƒ½ï¼Œä¸ºæ„å»ºæ›´å®ç”¨çš„å…·èº«æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦æ–¹æ³•è®ºï¼›VLA Scoreè¯„ä¼°æ¡†æ¶ä¹Ÿä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ›´å…¨é¢çš„æ€§èƒ½è¯„ä¼°æ ‡å‡†ã€‚

---

#### ğŸ“„ Abstract
To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.


### [22] [Partially Shared Concept Bottleneck Models](https://arxiv.org/abs/2511.22170)
*Delong Zhao, Qiang Huang, Di Yan, Yiqun Sun, Jun Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PS-CBMï¼ˆéƒ¨åˆ†å…±äº«æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼‰ï¼Œé€šè¿‡å¤šæ¨¡æ€æ¦‚å¿µç”Ÿæˆå™¨ã€éƒ¨åˆ†å…±äº«æ¦‚å¿µç­–ç•¥å’Œæ¦‚å¿µæ•ˆç‡å‡†ç¡®åº¦æŒ‡æ ‡ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¦‚å¿µç“¶é¢ˆæ¨¡å‹åœ¨è§†è§‰åŸºç¡€ã€æ¦‚å¿µå†—ä½™å’Œå¹³è¡¡å‡†ç¡®æ€§-ç´§å‡‘æ€§æ–¹é¢çš„ä¸‰å¤§æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡ç°æœ‰æ–¹æ³•ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆæ¦‚å¿µï¼Œä½†æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ä»é¢ä¸´ä¸‰ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šè§†è§‰åŸºç¡€è–„å¼±ã€æ¦‚å¿µå†—ä½™ä¸¥é‡ï¼Œä»¥åŠç¼ºä¹å¹³è¡¡é¢„æµ‹å‡†ç¡®æ€§å’Œæ¦‚å¿µç´§å‡‘æ€§çš„åŸåˆ™æ€§åº¦é‡æŒ‡æ ‡ã€‚

**Method:** PS-CBMæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¤šæ¨¡æ€æ¦‚å¿µç”Ÿæˆå™¨æ•´åˆLLMè¯­ä¹‰ä¸åŸºäºç¤ºä¾‹çš„è§†è§‰çº¿ç´¢ï¼›éƒ¨åˆ†å…±äº«æ¦‚å¿µç­–ç•¥åŸºäºæ¿€æ´»æ¨¡å¼åˆå¹¶æ¦‚å¿µä»¥å¹³è¡¡ç‰¹å¼‚æ€§å’Œç´§å‡‘æ€§ï¼›æ¦‚å¿µæ•ˆç‡å‡†ç¡®åº¦ä½œä¸ºäº‹ååº¦é‡æŒ‡æ ‡è”åˆæ•æ‰é¢„æµ‹å‡†ç¡®æ€§å’Œæ¦‚å¿µç´§å‡‘æ€§ã€‚

**Result:** åœ¨11ä¸ªå¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPS-CBMæŒç»­ä¼˜äºæœ€å…ˆè¿›çš„æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼Œåˆ†ç±»å‡†ç¡®ç‡æå‡1.0%-7.4%ï¼Œæ¦‚å¿µæ•ˆç‡å‡†ç¡®åº¦æå‡2.0%-9.5%ï¼ŒåŒæ—¶æ‰€éœ€æ¦‚å¿µæ•°é‡æ˜¾è‘—å‡å°‘ã€‚

**Conclusion:** PS-CBMé€šè¿‡è§£å†³æ¦‚å¿µç“¶é¢ˆæ¨¡å‹çš„å…³é”®é™åˆ¶ï¼Œåœ¨ä¿æŒé«˜å‡†ç¡®æ€§çš„åŒæ—¶å®ç°äº†å¼ºå¯è§£é‡Šæ€§ï¼Œä¸ºå¹³è¡¡æ¨¡å‹æ€§èƒ½å’Œæ¦‚å¿µç´§å‡‘æ€§æä¾›äº†ç³»ç»Ÿæ¡†æ¶ï¼Œæ¨åŠ¨äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Concept Bottleneck Models (CBMs) enhance interpretability by introducing a layer of human-understandable concepts between inputs and predictions. While recent methods automate concept generation using Large Language Models (LLMs) and Vision-Language Models (VLMs), they still face three fundamental challenges: poor visual grounding, concept redundancy, and the absence of principled metrics to balance predictive accuracy and concept compactness. We introduce PS-CBM, a Partially Shared CBM framework that addresses these limitations through three core components: (1) a multimodal concept generator that integrates LLM-derived semantics with exemplar-based visual cues; (2) a Partially Shared Concept Strategy that merges concepts based on activation patterns to balance specificity and compactness; and (3) Concept-Efficient Accuracy (CEA), a post-hoc metric that jointly captures both predictive accuracy and concept compactness. Extensive experiments on eleven diverse datasets show that PS-CBM consistently outperforms state-of-the-art CBMs, improving classification accuracy by 1.0%-7.4% and CEA by 2.0%-9.5%, while requiring significantly fewer concepts. These results underscore PS-CBM's effectiveness in achieving both high accuracy and strong interpretability.


### [23] [Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning](https://arxiv.org/abs/2511.22172)
*Zhaoyang Wei, Wenchao Ding, Yanchao Hao, Xi Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†GRiPï¼ˆå¼•å¯¼æ¨ç†ä¸æ„ŸçŸ¥ï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•è§£å†³è§†è§‰åŸºç¡€æ¨ç†ä¸­çš„ç¨³å®šæ€§ä¸çµæ´»æ€§å›°å¢ƒï¼Œåœ¨Qwen2.5-VL-7Bæ¨¡å‹åŸºç¡€ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°å¼€æºæ¨¡å‹çš„æœ€å…ˆè¿›æ°´å¹³ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€AIä¸­å®ç°"å›¾åƒæ€ç»´"èƒ½åŠ›çš„æ–¹æ³•é¢ä¸´æ ¹æœ¬å›°å¢ƒï¼šç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•å­˜åœ¨ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œè€Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åˆ™ç¼ºä¹è®¤çŸ¥çµæ´»æ€§ï¼Œå¯¼è‡´æ¨¡å‹è¦ä¹ˆéš¾ä»¥æœ‰æ•ˆå­¦ä¹ ï¼Œè¦ä¹ˆæ— æ³•åº”å¯¹å¤æ‚çœŸå®åœºæ™¯çš„æ¨ç†éœ€æ±‚ï¼Œéœ€è¦åœ¨ç¨³å®šæ€§å’Œçµæ´»æ€§ä¹‹é—´æ‰¾åˆ°æ›´å¥½çš„å¹³è¡¡ç‚¹ã€‚

**Method:** GRiPæ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œæ ¸å¿ƒåˆ›æ–°åœ¨äºè®¤çŸ¥å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šæ˜¾è‘—æ€§åŠ æƒIoUå¥–åŠ±æœºåˆ¶ï¼Œæ¿€åŠ±æ¨¡å‹ä¼˜å…ˆå®šä½ä»»åŠ¡å…³é”®å¯¹è±¡è€Œéæ— å…³å¹²æ‰°ç‰©ï¼›å¤šå¯å‘å¼å¥–åŠ±æœºåˆ¶ï¼Œé€šè¿‡å¥–åŠ±å¤šæ ·ä½†é€»è¾‘æœ‰æ•ˆçš„æ¨ç†è·¯å¾„æ¥ä¿ƒè¿›è®¤çŸ¥çµæ´»æ€§ã€‚è¯¥æ¡†æ¶åŸºäºQwen2.5-VL-7Bæ¨¡å‹è¿›è¡Œåˆå§‹åŒ–ã€‚

**Result:** GRiPåœ¨å¤šä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ï¼Œåœ¨é«˜åº¦å¤æ‚çš„TreeBenchå’ŒV* BenchåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å¼€æºæ¨¡å‹çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†æ¡†æ¶åœ¨å¼•å¯¼æ¨¡å‹æ„ŸçŸ¥ç„¦ç‚¹å’Œé€»è¾‘è·¯å¾„æ–¹é¢çš„ä¼˜åŠ¿ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œè¶…è¶Šç®€å•å¥–åŠ±æœºåˆ¶ï¼Œé‡‡ç”¨è®¤çŸ¥å¯å‘çš„ä¿¡å·æ¥å¼•å¯¼æ¨¡å‹"çœ‹ä»€ä¹ˆ"å’Œ"å¦‚ä½•æ€è€ƒ"å¯¹äºè§£é”ä¸‹ä¸€ä»£å¤šæ¨¡æ€æ™ºèƒ½è‡³å…³é‡è¦ã€‚GRiPæ¡†æ¶çš„æˆåŠŸéªŒè¯äº†é€šè¿‡æ˜¾å¼å¼•å¯¼æ„ŸçŸ¥ç„¦ç‚¹å’Œé€»è¾‘è·¯å¾„æ¥åŸ¹å…»ç¨³å¥çµæ´»è§†è§‰åŸºç¡€æ¨ç†çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šæ¨¡æ€AIçš„å‘å±•æä¾›äº†é‡è¦æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Models capable of "thinking with images" by dynamically grounding their reasoning in visual evidence represent a major leap in multimodal AI. However, replicating and advancing this ability is non-trivial, with current methods often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT). This leads to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes. To navigate this dilemma, we introduce GRiP (Guided Reasoning and Perception), a novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model's perceptual focus and logical pathways. GRiP's core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward that incentivizes the model to prioritize the localization of mission-critical objects over trivial distractors, and (2) a Multi-Heuristic Reward that encourages cognitive flexibility by rewarding diverse yet logically valid reasoning pathways. Initialized from the Qwen2.5-VL-7B model, GRiP demonstrates significant performance gains across multiple challenging benchmarks. It achieves state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench, proving its effectiveness in complex visual reasoning. Our work demonstrates that moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence. The code will be made publicly available.


### [24] [Enhanced Graph Convolutional Network with Chebyshev Spectral Graph and Graph Attention for Autism Spectrum Disorder Classification](https://arxiv.org/abs/2511.22178)
*Adnan Ferdous Ashrafi, Hasanul Kabir*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆåˆ‡æ¯”é›ªå¤«è°±å›¾å·ç§¯å’Œå›¾æ³¨æ„åŠ›ç½‘ç»œçš„å›¾å·ç§¯ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºæé«˜è‡ªé—­ç—‡è°±ç³»éšœç¢çš„å¤šæ¨¡æ€ç¥ç»å½±åƒå’Œè¡¨å‹æ•°æ®åˆ†ç±»å‡†ç¡®æ€§ï¼Œåœ¨ABIDE Iæ•°æ®é›†ä¸Šå®ç°äº†74.82%çš„æµ‹è¯•å‡†ç¡®ç‡å’Œ0.82çš„AUCå€¼ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è‡ªé—­ç—‡è°±ç³»éšœç¢æ˜¯ä¸€ç§å¤æ‚çš„ç¥ç»å‘è‚²éšœç¢ï¼Œå…¶ç—‡çŠ¶è¡¨ç°å’Œç¥ç»åŸºç¡€å­˜åœ¨æ˜¾è‘—å¼‚è´¨æ€§ï¼Œè¿™ä½¿å¾—æ—©æœŸå®¢è§‚è¯Šæ–­æä¸ºå›°éš¾ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€ç¥ç»å½±åƒæ•°æ®å’Œæ•æ‰ä¸ªä½“é—´å…³ç³»æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„åˆ†ç±»æ¨¡å‹æ¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ç§å›¾å·ç§¯ç½‘ç»œæ¨¡å‹ï¼Œæ•´åˆäº†åˆ‡æ¯”é›ªå¤«è°±å›¾å·ç§¯å’Œå›¾æ³¨æ„åŠ›ç½‘ç»œæ¥å¤„ç†å¤šæ¨¡æ€æ•°æ®ã€‚æ¨¡å‹é‡‡ç”¨å¤šåˆ†æ”¯æ¶æ„åˆ†åˆ«å¤„ç†é™æ¯æ€åŠŸèƒ½ç£å…±æŒ¯æˆåƒã€ç»“æ„ç£å…±æŒ¯æˆåƒå’Œè¡¨å‹å˜é‡ï¼Œç„¶åé€šè¿‡æ‹¼æ¥è¿›è¡Œèåˆã€‚åŸºäºç«™ç‚¹ç›¸ä¼¼æ€§æ„å»ºç¾¤ä½“å›¾ç»“æ„ï¼Œåˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼æ»¤æ³¢å™¨æä¾›å±€éƒ¨è°±å­¦ä¹ ï¼Œè€ŒGATå±‚é€šè¿‡æ³¨æ„åŠ›åŠ æƒèšåˆé‚»å±…ä¿¡æ¯å¢å¼ºèŠ‚ç‚¹è¡¨ç¤ºã€‚æ¨¡å‹ä½¿ç”¨åˆ†å±‚äº”æŠ˜äº¤å‰éªŒè¯è¿›è¡Œè®­ç»ƒï¼Œæ¯ä¸ªä¸ªä½“è¾“å…¥ç»´åº¦ä¸º5,206ä¸ªç‰¹å¾ã€‚

**Result:** åœ¨åŒ…å«870åæ‚£è€…çš„ABIDE Iæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹åœ¨å®Œæ•´æ•°æ®é›†ä¸Šå®ç°äº†74.82%çš„æµ‹è¯•å‡†ç¡®ç‡å’Œ0.82çš„AUCå€¼ã€‚è¯¥æ€§èƒ½è¶…è¶Šäº†å¤šä¸ªæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿå›¾å·ç§¯ç½‘ç»œã€åŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„æ·±åº¦ç¥ç»ç½‘ç»œå’Œå¤šæ¨¡æ€å·ç§¯ç¥ç»ç½‘ç»œï¼Œè¯æ˜äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€è‡ªé—­ç—‡åˆ†ç±»ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ•´åˆåˆ‡æ¯”é›ªå¤«è°±å›¾å·ç§¯å’Œå›¾æ³¨æ„åŠ›ç½‘ç»œçš„å¤šæ¨¡æ€å›¾å·ç§¯ç½‘ç»œåœ¨è‡ªé—­ç—‡è¯Šæ–­ä¸­çš„æœ‰æ•ˆæ€§ã€‚åŸºäºç«™ç‚¹ç›¸ä¼¼æ€§çš„å›¾ç»“æ„æ„å»ºæ–¹æ³•æœ‰åŠ©äºæ•æ‰ä¸ªä½“é—´å…³ç³»ï¼Œè€Œå¤šæ¨¡æ€èåˆç­–ç•¥èƒ½å¤Ÿå……åˆ†åˆ©ç”¨ç¥ç»å½±åƒå’Œè¡¨å‹ä¿¡æ¯çš„äº’è¡¥æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºç¥ç»å‘è‚²éšœç¢çš„å®¢è§‚è¯Šæ–­æä¾›äº†æœ‰å‰æ™¯çš„è®¡ç®—æ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†å›¾ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åº”ç”¨æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
ASD is a complicated neurodevelopmental disorder marked by variation in symptom presentation and neurological underpinnings, making early and objective diagnosis extremely problematic. This paper presents a Graph Convolutional Network (GCN) model, incorporating Chebyshev Spectral Graph Convolution and Graph Attention Networks (GAT), to increase the classification accuracy of ASD utilizing multimodal neuroimaging and phenotypic data. Leveraging the ABIDE I dataset, which contains resting-state functional MRI (rs-fMRI), structural MRI (sMRI), and phenotypic variables from 870 patients, the model leverages a multi-branch architecture that processes each modality individually before merging them via concatenation. Graph structure is encoded using site-based similarity to generate a population graph, which helps in understanding relationship connections across individuals. Chebyshev polynomial filters provide localized spectral learning with lower computational complexity, whereas GAT layers increase node representations by attention-weighted aggregation of surrounding information. The proposed model is trained using stratified five-fold cross-validation with a total input dimension of 5,206 features per individual. Extensive trials demonstrate the enhanced model's superiority, achieving a test accuracy of 74.82\% and an AUC of 0.82 on the entire dataset, surpassing multiple state-of-the-art baselines, including conventional GCNs, autoencoder-based deep neural networks, and multimodal CNNs.


### [25] [MTR-VP: Towards End-to-End Trajectory Planning through Context-Driven Image Encoding and Multiple Trajectory Prediction](https://arxiv.org/abs/2511.22181)
*Maitrayee Keskar, Mohan Trivedi, Ross Greer*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MTR-VPæ–¹æ³•ï¼Œä¸€ç§åŸºäºè§†è§‰çš„è‡ªåŠ¨é©¾é©¶è½¨è¿¹è§„åˆ’æ–¹æ³•ï¼Œé€šè¿‡ViTç¼–ç å™¨å­¦ä¹ å›¾åƒä¸Šä¸‹æ–‡åµŒå…¥æ¥æ›¿ä»£ä¼ ç»Ÿåœ°å›¾ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¿åŠ¨é¢„æµ‹æ¡†æ¶è¿›è¡Œå¤šæ¨¡æ€è½¨è¿¹è§„åˆ’ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶è½¨è¿¹è§„åˆ’ä¸­ä¼ ç»Ÿåœ°å›¾ç‰¹å¾ä¾èµ–çš„é—®é¢˜ï¼Œæ¢ç´¢å¦‚ä½•åˆ©ç”¨è§†è§‰ä¿¡æ¯æ›¿ä»£åœ°å›¾ç‰¹å¾æ¥ç”Ÿæˆæœ‰æ•ˆçš„åœºæ™¯ä¸Šä¸‹æ–‡åµŒå…¥ï¼ŒåŒæ—¶ç ”ç©¶å¤šæ¨¡æ€è½¨è¿¹é¢„æµ‹åœ¨è§„åˆ’æ€§èƒ½ä¸­çš„ä½œç”¨ã€‚

**Method:** è¯¥æ–¹æ³•æå‡ºMTR-VPæ¶æ„ï¼Œä½¿ç”¨ViTç¼–ç å™¨å¤„ç†åŸå§‹å›¾åƒå’Œè¿‡å»è¿åŠ¨çŠ¶æ€ï¼Œç”Ÿæˆä¸MTRç¼–ç å™¨ç±»ä¼¼çš„ä¸Šä¸‹æ–‡åµŒå…¥ï¼›é‡‡ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å°†æ„å›¾ä¸ä¸Šä¸‹æ–‡åµŒå…¥ç»“åˆï¼Œæ›¿ä»£MTRè§£ç å™¨ä¸­çš„å¯å­¦ä¹ æ„å›¾æŸ¥è¯¢ï¼›åœ¨Waymoç«¯åˆ°ç«¯é©¾é©¶æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œé€šè¿‡æ¶ˆèç ”ç©¶åˆ†æå›¾åƒè¾“å…¥å’Œå¤šè½¨è¿¹è¾“å‡ºçš„å½±å“ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºTransformerçš„æ–¹æ³•åœ¨ç»“åˆè§†è§‰ç‰¹å¾å’Œè¿åŠ¨ç‰¹å¾æ–¹é¢æ•ˆæœæœ‰é™ï¼Œå³ä½¿ä½¿ç”¨CLIPå’ŒDINOv2ç­‰åŸºç¡€æ¨¡å‹å¢å¼ºæ„å›¾åµŒå…¥ï¼Œä¹Ÿæ— æ³•æœ‰æ•ˆç”Ÿæˆæœ‰ç”¨çš„åœºæ™¯ä¸Šä¸‹æ–‡åµŒå…¥ï¼›ç„¶è€Œï¼Œé¢„æµ‹å¤šä¸ªæœªæ¥è½¨è¿¹åˆ†å¸ƒè€Œéå•ä¸€è½¨è¿¹æ˜¾è‘—æå‡äº†è§„åˆ’æ€§èƒ½ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†è§†è§‰ç‰¹å¾ä¸è¿åŠ¨ç‰¹å¾èåˆçš„æŒ‘æˆ˜ï¼Œè¡¨æ˜å½“å‰Transformeræ¶æ„åœ¨è·¨æ¨¡æ€ä¿¡æ¯æ•´åˆæ–¹é¢å­˜åœ¨å±€é™ï¼›åŒæ—¶éªŒè¯äº†å¤šæ¨¡æ€è½¨è¿¹é¢„æµ‹å¯¹è§„åˆ’æ€§èƒ½çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥åŸºäºè§†è§‰çš„è‡ªåŠ¨é©¾é©¶è§„åˆ’ç³»ç»Ÿè®¾è®¡æä¾›äº†é‡è¦è§è§£ã€‚

---

#### ğŸ“„ Abstract
We present a method for trajectory planning for autonomous driving, learning image-based context embeddings that align with motion prediction frameworks and planning-based intention input. Within our method, a ViT encoder takes raw images and past kinematic state as input and is trained to produce context embeddings, drawing inspiration from those generated by the recent MTR (Motion Transformer) encoder, effectively substituting map-based features with learned visual representations. MTR provides a strong foundation for multimodal trajectory prediction by localizing agent intent and refining motion iteratively via motion query pairs; we name our approach MTR-VP (Motion Transformer for Vision-based Planning), and instead of the learnable intention queries used in the MTR decoder, we use cross attention on the intent and the context embeddings, which reflect a combination of information encoded from the driving scene and past vehicle states. We evaluate our methods on the Waymo End-to-End Driving Dataset, which requires predicting the agent's future 5-second trajectory in bird's-eye-view coordinates using prior camera images, agent pose history, and routing goals. We analyze our architecture using ablation studies, removing input images and multiple trajectory output. Our results suggest that transformer-based methods that are used to combine the visual features along with the kinetic features such as the past trajectory features are not effective at combining both modes to produce useful scene context embeddings, even when intention embeddings are augmented with foundation-model representations of scene context from CLIP and DINOv2, but that predicting a distribution over multiple futures instead of a single future trajectory boosts planning performance.


### [26] [Controllable 3D Object Generation with Single Image Prompt](https://arxiv.org/abs/2511.22194)
*Jaeseok Lee, Jaekoo Lee*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€æ–‡æœ¬åè½¬çš„3Då¯¹è±¡ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡ç°æˆçš„å›¾åƒé€‚é…å™¨å’Œæ·±åº¦æ¡ä»¶é¢„çƒ­ç­–ç•¥ï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å¢å¼ºäº†3Dä¸€è‡´æ€§å’Œæ§åˆ¶èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºæ–‡æœ¬åè½¬çš„3Då¯¹è±¡ç”Ÿæˆæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šéœ€è¦é¢å¤–çš„è®­ç»ƒæ—¶é—´ä¸”ç¼ºä¹å¯¹ç”Ÿæˆè¿‡ç¨‹çš„æ§åˆ¶èƒ½åŠ›ã€‚ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹è™½ç„¶é€šè¿‡æ–‡æœ¬åè½¬å­¦ä¹ ç›®æ ‡å¯¹è±¡çš„æ¦‚å¿µæˆ–é£æ ¼ï¼Œä½†è¿™ç§æ–¹æ³•é™åˆ¶äº†ç”¨æˆ·å¯¹æ·±åº¦ã€å§¿æ€ç­‰æ¡ä»¶çš„ç²¾ç¡®æ§åˆ¶ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸¤ç§åˆ›æ–°æ–¹æ³•ï¼šé¦–å…ˆï¼Œé‡‡ç”¨ç°æˆçš„å›¾åƒé€‚é…å™¨ç›´æ¥ç”Ÿæˆ3Då¯¹è±¡ï¼Œæ— éœ€ä¾èµ–æ–‡æœ¬åè½¬ï¼Œä»è€Œå®ç°å¯¹æ·±åº¦ã€å§¿æ€å’Œæ–‡æœ¬ç­‰å¤šç§æ¡ä»¶çš„å¢å¼ºæ§åˆ¶ï¼›å…¶æ¬¡ï¼Œå¼•å…¥æ·±åº¦æ¡ä»¶é¢„çƒ­ç­–ç•¥ï¼Œä¸“é—¨è®¾è®¡ç”¨äºæå‡ç”Ÿæˆç»“æœçš„3Dä¸€è‡´æ€§ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å‡å±•ç°å‡ºä¸ç°æœ‰åŸºäºæ–‡æœ¬åè½¬æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨3Dä¸€è‡´æ€§æ–¹é¢æœ‰æ‰€æå‡ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ–¹æ³•åœ¨è¾“å…¥å›¾åƒåŒ¹é…åº¦å’Œ3Dä¸€è‡´æ€§ä¿æŒæ–¹é¢å‡ä¼˜äºç°æœ‰æ›¿ä»£æ–¹æ¡ˆï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸º3Då¯¹è±¡ç”Ÿæˆæä¾›äº†ä¸€ç§æ›´é«˜æ•ˆä¸”å¯æ§çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ¶ˆé™¤æ–‡æœ¬åè½¬çš„éœ€æ±‚å¹¶å¢å¼ºæ¡ä»¶æ§åˆ¶èƒ½åŠ›ï¼Œæ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰ä¸­3Dç”Ÿæˆä»»åŠ¡çš„å‘å±•ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†3Dä¸€è‡´æ€§ï¼Œä¸ºæœªæ¥çš„3Då†…å®¹åˆ›ä½œå·¥å…·æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Recently, the impressive generative capabilities of diffusion models have been demonstrated, producing images with remarkable fidelity. Particularly, existing methods for the 3D object generation tasks, which is one of the fastest-growing segments in computer vision, pre-dominantly use text-to-image diffusion models with textual inversion which train a pseudo text prompt to describe the given image. In practice, various text-to-image generative models employ textual inversion to learn concepts or styles of target object in the pseudo text prompt embedding space, thereby generating sophisticated outputs. However, textual inversion requires additional training time and lacks control ability. To tackle this issues, we propose two innovative methods: (1) using an off-the-shelf image adapter that generates 3D objects without textual inversion, offering enhanced control over conditions such as depth, pose, and text. (2) a depth conditioned warmup strategy to enhance 3D consistency. In experimental results, ours show qualitatively and quantitatively comparable performance and improved 3D consistency to the existing text-inversion-based alternatives. Furthermore, we conduct a user study to assess (i) how well results match the input image and (ii) whether 3D consistency is maintained. User study results show that our model outperforms the alternatives, validating the effectiveness of our approaches. Our code is available at GitHub repository:https://github.com/Seooooooogi/Control3D_IP/


### [27] [TTSnap: Test-Time Scaling of Diffusion Models via Noise-Aware Pruning](https://arxiv.org/abs/2511.22242)
*Qingtao Yu, Changlin Song, Minghao Sun, Zhengyang Yu, Vinay Kumar Verma, Soumya Roy, Sumit Negi, Hongdong Li, Dylan Campbell*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†TTSnapæ¡†æ¶ï¼Œé€šè¿‡å™ªå£°æ„ŸçŸ¥å‰ªæåœ¨æµ‹è¯•æ—¶æ‰©å±•æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨è‡ªè’¸é¦è®­ç»ƒå™ªå£°æ„ŸçŸ¥å¥–åŠ±æ¨¡å‹æ¥å¯¹é½ä¸­é—´ä¼°è®¡ä¸æœ€ç»ˆå¹²å‡€å›¾åƒçš„å¥–åŠ±ï¼Œä»è€Œåœ¨ä¸å®Œå…¨å»å™ªçš„æƒ…å†µä¸‹é«˜æ•ˆå‰ªæä½è´¨é‡å€™é€‰æ ·æœ¬ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºå¤šå™ªå£°ç§å­æœç´¢çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•é¢ä¸´è®¡ç®—æ•ˆç‡ç“¶é¢ˆï¼Œå› ä¸ºæ¯ä¸ªå€™é€‰æ ·æœ¬å¿…é¡»å®Œå…¨å»å™ªåæ‰èƒ½è®¡ç®—å¥–åŠ±å‡½æ•°ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å›ºå®šè®¡ç®—é¢„ç®—ä¸‹å¯æ¢ç´¢çš„æ ·æœ¬æ•°é‡ï¼Œéœ€è¦ä¸€ç§èƒ½åœ¨ä¸å®Œå…¨å»å™ªé˜¶æ®µå°±è¯†åˆ«å’Œå‰ªæä½è´¨é‡å€™é€‰çš„é«˜æ•ˆæ–¹æ³•ã€‚

**Method:** æå‡ºæµ‹è¯•æ—¶æ‰©å±•ä¸å™ªå£°æ„ŸçŸ¥å‰ªææ¡†æ¶TTSnapï¼Œé€šè¿‡è‡ªè’¸é¦è®­ç»ƒå™ªå£°æ„ŸçŸ¥å¥–åŠ±æ¨¡å‹æ¥å¯¹é½ä¸­é—´ä¼°è®¡ä¸æœ€ç»ˆå¹²å‡€å›¾åƒçš„å¥–åŠ±é¢„æµ‹ï¼Œé‡‡ç”¨è¯¾ç¨‹è®­ç»ƒç­–ç•¥é€æ­¥ä»å¹²å‡€å›¾åƒåŸŸè¿‡æ¸¡åˆ°å™ªå£°å›¾åƒåŸŸä»¥ç¨³å®šå­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥æ–°çš„å¥–åŠ±å¯¹é½ä¸è®¡ç®—é¢„ç®—åˆ©ç”¨ç‡åº¦é‡æŒ‡æ ‡ã€‚

**Result:** å®éªŒè¡¨æ˜TTSnapç›¸æ¯”ç°æœ‰æ–¹æ³•æ€§èƒ½æå‡è¶…è¿‡16%ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„æµ‹è¯•æ—¶æ‰©å±•ï¼ŒåŒæ—¶åœ¨ä¸åè®­ç»ƒæŠ€æœ¯å’Œå±€éƒ¨æµ‹è¯•æ—¶ä¼˜åŒ–æ–¹æ³•ç»“åˆæ—¶èƒ½æä¾›æ­£äº¤å¢ç›Šï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—é¢„ç®—åˆ©ç”¨ç‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å™ªå£°æ„ŸçŸ¥å¥–åŠ±æ¨¡å‹åœ¨æµ‹è¯•æ—¶æ‰©å±•ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ¨ç†æä¾›äº†æ–°æ–¹å‘ï¼Œå±•ç¤ºäº†è‡ªè’¸é¦å’Œè¯¾ç¨‹è®­ç»ƒç­–ç•¥åœ¨å¤„ç†è·¨åŸŸå¥–åŠ±å¯¹é½é—®é¢˜ä¸Šçš„ä¼˜åŠ¿ï¼Œå¹¶ä¸ºç»“åˆå…¶ä»–ä¼˜åŒ–æŠ€æœ¯æä¾›äº†å…¼å®¹æ€§æ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
A prominent approach to test-time scaling for text-to-image diffusion models formulates the problem as a search over multiple noise seeds, selecting the one that maximizes a certain image-reward function. The effectiveness of this strategy heavily depends on the number and diversity of noise seeds explored. However, verifying each candidate is computationally expensive, because each must be fully denoised before a reward can be computed. This severely limits the number of samples that can be explored under a fixed budget. We propose test-time scaling with noise-aware pruning (TTSnap), a framework that prunes low-quality candidates without fully denoising them. The key challenge is that reward models are learned in the clean image domain, and the ranking of rewards predicted for intermediate estimates are often inconsistent with those predicted for clean images. To overcome this, we train noise-aware reward models via self-distillation to align the reward for intermediate estimates with that of the final clean images. To stabilize learning across different noise levels, we adopt a curriculum training strategy that progressively shifts the data domain from clean images to noise images. In addition, we introduce a new metric that measures reward alignment and computational budget utilization. Experiments demonstrate that our approach improves performance by over 16\% compared with existing methods, enabling more efficient and effective test-time scaling. It also provides orthogonal gains when combined with post-training techniques and local test-time optimization. Code: https://github.com/TerrysLearning/TTSnap/.


### [28] [Semantic Anchoring for Robust Personalization in Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.22245)
*Seoyun Yang, Gihoon Kim, Taesup Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰é”šå®šçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œé€šè¿‡å°†æ–°æ¦‚å¿µé”šå®šåˆ°å…¶å¯¹åº”åˆ†å¸ƒæ¥å¼•å¯¼æ¨¡å‹é€‚åº”ï¼Œåœ¨ä¿æŒé¢„è®­ç»ƒè¯­ä¹‰å…ˆéªŒçš„åŒæ—¶å­¦ä¹ ç”¨æˆ·ç‰¹å®šçš„è§†è§‰æ¦‚å¿µï¼Œå®ç°äº†ç¨³å®šçš„ä¸ªæ€§åŒ–ç”Ÿæˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ä¸ªæ€§åŒ–æ–¹é¢é¢ä¸´å…³é”®æŒ‘æˆ˜ï¼šå½“æ¨¡å‹ä¸“æ³¨äºä¸»ä½“ä¿çœŸåº¦æ—¶å®¹æ˜“å¯¹å°‘é‡å‚è€ƒå›¾åƒè¿‡æ‹Ÿåˆï¼Œè€Œå¼ºè°ƒå…ˆéªŒä¿æŒåˆ™é˜»ç¢å­¦ä¹ æ–°çš„ä¸ªæ€§åŒ–å±æ€§ï¼Œéœ€è¦åœ¨å­¦ä¹ æ–°è§†è§‰æ¦‚å¿µä¸ä¿æŒé¢„è®­ç»ƒè¯­ä¹‰å…ˆéªŒä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**Method:** è¯¥æ–¹æ³•æå‡ºé€šè¿‡è¯­ä¹‰é”šå®šå¼•å¯¼ä¸ªæ€§åŒ–è¿‡ç¨‹ï¼Œå°†ä¸ªæ€§åŒ–é‡æ–°è¡¨è¿°ä¸ºé€šè¿‡è¯­ä¹‰é”šå®šåœ¨é¢‘ç¹å¯¹åº”æ¦‚å¿µçš„æŒ‡å¯¼ä¸‹å­¦ä¹ ç¨€æœ‰æ¦‚å¿µçš„è¿‡ç¨‹ï¼Œè¿™ç§é”šå®šæœºåˆ¶é¼“åŠ±æ¨¡å‹ä»¥ç¨³å®šå¯æ§çš„æ–¹å¼é€‚åº”æ–°æ¦‚å¿µï¼Œåœ¨æ‰©å±•é¢„è®­ç»ƒåˆ†å¸ƒçš„åŒæ—¶ä¿æŒå…¶è¯­ä¹‰ç»“æ„ã€‚

**Result:** å®éªŒè¡¨æ˜æ‰€ææ–¹æ³•å®ç°äº†ç¨³å®šçš„é€‚åº”ï¼Œåœ¨ä¸»ä½“ä¿çœŸåº¦å’Œæ–‡æœ¬å›¾åƒå¯¹é½æ–¹é¢å‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¿æ³›çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†æ‰€æé”šå®šç­–ç•¥çš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡è¯­ä¹‰é”šå®šæ¡†æ¶è§£å†³äº†æ‰©æ•£æ¨¡å‹ä¸ªæ€§åŒ–ä¸­çš„è¿‡æ‹Ÿåˆä¸å…ˆéªŒä¿æŒçš„æƒè¡¡é—®é¢˜ï¼Œä¸ºç¨³å®šå¯æ§çš„æ¦‚å¿µé€‚åº”æä¾›äº†æ–°æ€è·¯ï¼Œæ‰©å±•äº†é¢„è®­ç»ƒåˆ†å¸ƒå‘ä¸ªæ€§åŒ–åŒºåŸŸçš„åŒæ—¶ä¿æŒäº†è¯­ä¹‰ç»“æ„çš„ä¸€è‡´æ€§ã€‚

---

#### ğŸ“„ Abstract
Text-to-image diffusion models have achieved remarkable progress in generating diverse and realistic images from textual descriptions. However, they still struggle with personalization, which requires adapting a pretrained model to depict user-specific subjects from only a few reference images. The key challenge lies in learning a new visual concept from a limited number of reference images while preserving the pretrained semantic prior that maintains text-image alignment. When the model focuses on subject fidelity, it tends to overfit the limited reference images and fails to leverage the pretrained distribution. Conversely, emphasizing prior preservation maintains semantic consistency but prevents the model from learning new personalized attributes. Building on these observations, we propose the personalization process through a semantic anchoring that guides adaptation by grounding new concepts in their corresponding distributions. We therefore reformulate personalization as the process of learning a rare concept guided by its frequent counterpart through semantic anchoring. This anchoring encourages the model to adapt new concepts in a stable and controlled manner, expanding the pretrained distribution toward personalized regions while preserving its semantic structure. As a result, the proposed method achieves stable adaptation and consistent improvements in both subject fidelity and text-image alignment compared to baseline methods. Extensive experiments and ablation studies further demonstrate the robustness and effectiveness of the proposed anchoring strategy.


### [29] [UMind-VL: A Generalist Ultrasound Vision-Language Model for Unified Grounded Perception and Comprehensive Interpretation](https://arxiv.org/abs/2511.22256)
*Dengbo Chen, Ziwei Zhao, Kexin Zhang, Shishuang Zhao, Junjie Hou, Yaqian Wang, Nianxi Liao, Anlan Sun, Fei Gao, Jia Ding, Yuhang Liu, Dong Wang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†UMind-VLï¼Œä¸€ä¸ªç»Ÿä¸€çš„è¶…å£°åŒ»å­¦åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨å¼¥åˆä½å±‚æ¬¡è¶…å£°æ„ŸçŸ¥ï¼ˆå¦‚åˆ†å‰²ã€å®šä½ï¼‰ä¸é«˜å±‚æ¬¡ä¸´åºŠè§£é‡Šï¼ˆå¦‚è¯Šæ–­ã€æ¨ç†ï¼‰ä¹‹é—´çš„é¸¿æ²Ÿï¼Œé€šè¿‡å•ä¸€æ¡†æ¶å®ç°åƒç´ çº§ç»“æ„ç†è§£ä¸å¤æ‚ä¸´åºŠæ¨ç†çš„ååŒã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡åŒ»å­¦åŸºç¡€æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¶…å£°é¢†åŸŸç¼ºä¹èƒ½å¤Ÿè¿æ¥ä½å±‚æ¬¡è¶…å£°åŸºç¡€æ„ŸçŸ¥ï¼ˆå¦‚åˆ†å‰²ã€å®šä½ï¼‰ä¸é«˜å±‚æ¬¡è¶…å£°ç»¼åˆè§£é‡Šï¼ˆå¦‚è¯Šæ–­ã€æ¨ç†ï¼‰çš„å…¨é¢è§£å†³æ–¹æ¡ˆï¼Œç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•´åˆåƒç´ çº§ç»“æ„ç†è§£ä¸ä¸´åºŠæ¨ç†èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶é¦–å…ˆæ„å»ºäº†UMind-DSå¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«120ä¸‡è¶…å£°å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œæ¶µç›–16ä¸ªè§£å‰–åŒºåŸŸï¼Œå¹¶åŒ…å«åƒç´ çº§æ ‡æ³¨å’Œä¸´åºŠéªŒè¯çš„æ¨ç†ä¾æ®ã€‚æ¨¡å‹æ¶æ„ä¸Šï¼ŒUMind-VLå¼•å…¥äº†è½»é‡çº§åŠ¨æ€å·ç§¯æ©ç è§£ç å™¨ï¼Œé€šè¿‡åŸºäºå¤§è¯­è¨€æ¨¡å‹è¾“å‡ºçš„åŠ¨æ€æ ¸ç”Ÿæˆæ©ç ï¼Œç»“åˆä»»åŠ¡ç‰¹å®šä»¤ç‰Œï¼Œåœ¨å•ä¸€æ¡†æ¶å†…ç»Ÿä¸€äº†åˆ†å‰²ã€æ£€æµ‹ã€å‡ ä½•æµ‹é‡å’Œè¯Šæ–­ä»»åŠ¡ã€‚

**Result:** å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒUMind-VLåœ¨åˆ†å‰²ã€æ£€æµ‹ã€å…³é”®ç‚¹å®šä½å’Œè¯Šæ–­æ¨ç†ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„é€šç”¨å¤šæ¨¡æ€æ¨¡å‹ï¼Œå…¶æ€§èƒ½è¾¾åˆ°ç”šè‡³è¶…è¶Šäº†æœ€å…ˆè¿›çš„ä¸“ç”¨æ¨¡å‹æ°´å¹³ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç»Ÿä¸€è¶…å£°åŸºç¡€æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œé€šè¿‡æ•´åˆåƒç´ çº§æ„ŸçŸ¥ä¸ä¸´åºŠæ¨ç†èƒ½åŠ›ï¼Œä¸ºè¶…å£°åŒ»å­¦äººå·¥æ™ºèƒ½æä¾›äº†å…¨é¢è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å•ä¸€æ¨¡å‹åœ¨å¤šç§è¶…å£°ä»»åŠ¡ä¸Šçš„å“è¶Šæ€§èƒ½ï¼Œä¸ºæœªæ¥åŒ»å­¦å¤šæ¨¡æ€æ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚

---

#### ğŸ“„ Abstract
Despite significant strides in medical foundation models, the ultrasound domain lacks a comprehensive solution capable of bridging low-level Ultrasound Grounded Perception (e.g., segmentation, localization) and high-level Ultrasound Comprehensive Interpretation (e.g., diagnosis, reasoning). To bridge this gap, we propose UMind-VL, a unified foundation model designed to synergize pixel-level structural understanding with complex clinical reasoning. We first introduce UMind-DS, a large-scale multimodal dataset comprising 1.2 million ultrasound image-text pairs across 16 anatomical regions, enriching standard data with pixel-level annotations and clinician-validated rationales. Architecturally, UMind-VL incorporates a lightweight Dynamic Convolutional Mask Decoder that generates masks via dynamic kernels conditioned on LLM outputs. This design, combined with task-specific tokens, unifies segmentation, detection, geometric measurement, and diagnosis tasks within a single framework. Extensive evaluations demonstrate that UMind-VL significantly outperforms existing generalist multimodal models and achieves performance on par with, or superior to, state-of-the-art specialist models across segmentation, detection, keypoint localization, and diagnostic reasoning benchmarks, while maintaining strong generalization ability. We demonstrate the capability of UMind-VL in Figure 1.


### [30] [Asking like Socrates: Socrates helps VLMs understand remote sensing images](https://arxiv.org/abs/2511.22396)
*Run Shao, Ziyu Li, Zhaoyang Zhang, Linrui Xu, Xinran He, Hongyuan Yuan, Bolei He, Yongxing Dai, Yiming Yan, Yijun Chen, Wang Guo, Haifeng Li*

#### ğŸ§© TL;DR
æœ¬æ–‡é’ˆå¯¹é¥æ„Ÿè§†è§‰é—®ç­”ä¸­å­˜åœ¨çš„ä¼ªæ¨ç†é—®é¢˜ï¼Œæå‡ºäº†RS-EoTï¼ˆé¥æ„Ÿè¯æ®æ€ç»´ï¼‰èŒƒå¼ï¼Œé€šè¿‡è¯­è¨€é©±åŠ¨çš„è¿­ä»£è§†è§‰è¯æ®æœç´¢æœºåˆ¶ï¼Œç»“åˆSocraticAgentå¤šæ™ºèƒ½ä½“ç³»ç»Ÿå’Œæ¸è¿›å¼å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†é¥æ„Ÿè§†è§‰æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰é¥æ„Ÿå¤šæ¨¡æ€æ¨ç†æ¨¡å‹æ™®éå­˜åœ¨ä¼ªæ¨ç†é—®é¢˜ï¼Œå³æ¨¡å‹ä»…æè¿°æ¨ç†è¿‡ç¨‹è€ŒéåŸºäºè§†è§‰è¯æ®è¿›è¡ŒçœŸå®æ¨ç†ã€‚ä½œè€…å°†æ­¤å½’å› äºGlance Effectç°è±¡ï¼šå¯¹å¤§è§„æ¨¡é¥æ„Ÿå›¾åƒçš„ç²—ç•¥å•æ¬¡æ„ŸçŸ¥å¯¼è‡´ä¸å®Œæ•´ç†è§£ï¼Œä½¿å¾—æ¨ç†åŸºäºè¯­è¨€è‡ªæ´½æ€§è€Œéè§†è§‰è¯æ®ã€‚

**Method:** æå‡ºäº†RS-EoTèŒƒå¼ï¼Œè¿™æ˜¯ä¸€ç§è¯­è¨€é©±åŠ¨çš„è¿­ä»£è§†è§‰è¯æ®æœç´¢æœºåˆ¶ã€‚ä¸ºå®ç°è¯¥èŒƒå¼ï¼Œè®¾è®¡äº†SocraticAgentå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡æ¨ç†å’Œè§†è§‰æ£€æŸ¥çš„äº¤æ›¿å¾ªç¯åˆæˆæ¨ç†è½¨è¿¹ã€‚ä¸ºå¢å¼ºå’Œæ³›åŒ–è¿™äº›æ¨¡å¼ï¼Œæå‡ºäº†ä¸¤é˜¶æ®µæ¸è¿›å¼å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼šé¦–å…ˆåœ¨ç»†ç²’åº¦Groundingä»»åŠ¡ä¸Šè¿›è¡ŒRLä»¥å¢å¼ºRS-EoTèƒ½åŠ›ï¼Œç„¶ååœ¨RS VQAä»»åŠ¡ä¸Šè¿›è¡ŒRLä»¥æ³›åŒ–åˆ°æ›´å¹¿æ³›çš„ç†è§£åœºæ™¯ã€‚

**Result:** å®éªŒè¡¨æ˜RS-EoTåœ¨å¤šä¸ªRS VQAå’ŒGroundingåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åˆ†ææ­ç¤ºäº†æ¸…æ™°çš„æ¨ç†å’Œè¯æ®æœç´¢è¿­ä»£å¾ªç¯ï¼Œè¯å®äº†RS-EoTèƒ½å¤Ÿç¼“è§£Glance Effectå¹¶å®ç°çœŸæ­£çš„è¯æ®åŸºç¡€æ¨ç†ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†é¥æ„Ÿå¤šæ¨¡æ€æ¨ç†ä¸­çš„ä¼ªæ¨ç†é—®é¢˜åŠå…¶æ ¹æºï¼Œæå‡ºçš„RS-EoTèŒƒå¼é€šè¿‡è¿­ä»£è¯æ®æœç´¢æœºåˆ¶æœ‰æ•ˆè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸ä»…æå‡äº†æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œä¸ºé¥æ„Ÿè§†è§‰æ¨ç†æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘å’Œå®ç”¨æ¡†æ¶ã€‚

---

#### ğŸ“„ Abstract
Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates


### [31] [Match-and-Fuse: Consistent Generation from Unstructured Image Sets](https://arxiv.org/abs/2511.22287)
*Kate Feingold, Omri Kaduri, Tali Dekel*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMatch-and-Fuseï¼Œä¸€ç§é›¶æ ·æœ¬ã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå…·æœ‰ä¸€è‡´æ€§çš„éç»“æ„åŒ–å›¾åƒé›†åˆï¼Œè¿™äº›é›†åˆå…±äº«å…±åŒè§†è§‰å…ƒç´ ä½†åœ¨è§†è§’ã€æ‹æ‘„æ—¶é—´å’Œå‘¨å›´å†…å®¹ä¸Šå­˜åœ¨å·®å¼‚ï¼Œå®ç°äº†é›†åˆåˆ°é›†åˆçš„ç”Ÿæˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•ä¸»è¦å¤„ç†å•å¼ å›¾åƒæˆ–å¯†é›†é‡‡æ ·çš„è§†é¢‘ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†éç»“æ„åŒ–å›¾åƒé›†åˆçš„ç”Ÿæˆé—®é¢˜ï¼Œè¿™äº›é›†åˆä¸­çš„å›¾åƒå…±äº«å…±åŒè§†è§‰å…ƒç´ ä½†åœ¨è§†è§’ã€æ—¶é—´å’Œå…¶ä»–å†…å®¹ä¸Šå­˜åœ¨å·®å¼‚ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿä¿æŒè·¨å›¾åƒä¸€è‡´æ€§çš„é›†åˆåˆ°é›†åˆç”Ÿæˆæ¡†æ¶ã€‚

**Method:** è¯¥æ–¹æ³•å°†ä»»åŠ¡å»ºæ¨¡ä¸ºå›¾ç»“æ„ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”ä¸€å¼ å›¾åƒï¼Œæ¯æ¡è¾¹è§¦å‘å›¾åƒå¯¹çš„è”åˆç”Ÿæˆï¼Œé€šè¿‡èåˆè·¨å›¾åƒå¯¹çš„å†…éƒ¨ç‰¹å¾å¹¶åœ¨å¯†é›†è¾“å…¥å¯¹åº”å…³ç³»çš„æŒ‡å¯¼ä¸‹å®ç°å±€éƒ¨ä¸€è‡´æ€§å’Œå…¨å±€è¿è´¯æ€§ï¼Œæ— éœ€æ©ç æˆ–äººå·¥ç›‘ç£ï¼ŒåŒæ—¶åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­æ–°å…´çš„å…ˆéªŒçŸ¥è¯†ï¼Œé¼“åŠ±å¤šä¸ªè§†å›¾åœ¨å…±äº«ç”»å¸ƒä¸Šç”Ÿæˆè¿è´¯å†…å®¹ã€‚

**Result:** Match-and-Fuseåœ¨ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼Œèƒ½å¤Ÿä»å›¾åƒé›†åˆä¸­åˆ›å»ºæ–°å†…å®¹ï¼Œå®ç°äº†é›†åˆåˆ°é›†åˆçš„ç”Ÿæˆèƒ½åŠ›ï¼Œåœ¨ä¿æŒå…±äº«å†…å®¹è·¨å›¾åƒä¸€è‡´æ€§çš„åŒæ—¶ç”Ÿæˆé«˜è´¨é‡çš„æ–°å›¾åƒé›†åˆã€‚

**Conclusion:** è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§ç»Ÿä¸€çš„æ¡†æ¶æ¥å¤„ç†éç»“æ„åŒ–å›¾åƒé›†åˆçš„ç”Ÿæˆé—®é¢˜ï¼Œé€šè¿‡å›¾å»ºæ¨¡å’Œç‰¹å¾èåˆå®ç°äº†å±€éƒ¨ä¸€è‡´æ€§å’Œå…¨å±€è¿è´¯æ€§ï¼Œä¸ºä»å›¾åƒé›†åˆè¿›è¡Œå†…å®¹åˆ›ä½œå¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå±•ç¤ºäº†é›¶æ ·æœ¬æ–¹æ³•åœ¨å¤æ‚ç”Ÿæˆä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
We present Match-and-Fuse - a zero-shot, training-free method for consistent controlled generation of unstructured image sets - collections that share a common visual element, yet differ in viewpoint, time of capture, and surrounding content. Unlike existing methods that operate on individual images or densely sampled videos, our framework performs set-to-set generation: given a source set and user prompts, it produces a new set that preserves cross-image consistency of shared content. Our key idea is to model the task as a graph, where each node corresponds to an image and each edge triggers a joint generation of image pairs. This formulation consolidates all pairwise generations into a unified framework, enforcing their local consistency while ensuring global coherence across the entire set. This is achieved by fusing internal features across image pairs, guided by dense input correspondences, without requiring masks or manual supervision. It also allows us to leverage an emergent prior in text-to-image models that encourages coherent generation when multiple views share a single canvas. Match-and-Fuse achieves state-of-the-art consistency and visual quality, and unlocks new capabilities for content creation from image collections.


### [32] [DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA](https://arxiv.org/abs/2511.22521)
*Ahmad Mohammadshirazi, Pinaki Prasad Guha Neogi, Dheeraj Kulshrestha, Rajiv Ramnath*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºDocVALæ¡†æ¶ï¼Œé€šè¿‡éªŒè¯å¼æ€ç»´é“¾è’¸é¦å°†å¤§å‹æ•™å¸ˆæ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›è¿ç§»åˆ°å¯éƒ¨ç½²çš„å­¦ç”Ÿè§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œåœ¨DocVQAä»»åŠ¡ä¸Šå®ç°äº†é«˜ç²¾åº¦ä¸é«˜æ•ˆç‡çš„å¹³è¡¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ–‡æ¡£è§†è§‰é—®ç­”ç³»ç»Ÿé¢ä¸´ç²¾åº¦ä¸æ•ˆç‡çš„å°–é”æƒè¡¡ï¼šå¤§å‹æ•™å¸ˆæ¨¡å‹å…·æœ‰å¼ºå¤§çš„ç©ºé—´å®šä½èƒ½åŠ›ä½†éƒ¨ç½²æˆæœ¬è¿‡é«˜ï¼Œè€Œç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹åœ¨å®šä½æ€§èƒ½ä¸Šæ˜¾è‘—ä¸‹é™ï¼Œéœ€è¦è§£å†³è¿™ä¸€éƒ¨ç½²ç“¶é¢ˆã€‚

**Method:** DocVALæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŸºäºéªŒè¯æ—¶æ–‡æœ¬æ£€æµ‹çš„æ•™å¸ˆç›‘ç£ä»¥è¿‡æ»¤å’Œå»å™ªè®­ç»ƒä¿¡å·ï¼›å¤šæ¨¡å—éªŒè¯å™¨ç¡®ä¿ç­”æ¡ˆæ­£ç¡®æ€§å’Œå‡ ä½•ä¸€è‡´æ€§å¹¶æä¾›åƒç´ çº§é”™è¯¯åé¦ˆï¼›ä¸¤é˜¶æ®µå­¦ç”Ÿè®­ç»ƒæ–¹æ¡ˆï¼Œå…ˆå­¦ä¹ éªŒè¯åçš„æ€ç»´é“¾è½¨è¿¹ï¼Œå†é€šè¿‡éªŒè¯å™¨åé¦ˆè¿›è¡Œè¿­ä»£ç²¾ç‚¼ã€‚

**Result:** å­¦ç”Ÿæ¨¡å‹ï¼ˆGemma-3 12Bï¼‰åœ¨DocVQAä¸Šè¾¾åˆ°91.4% ANLSå’Œ82.4% mAPï¼Œä½œä¸ºçº¯è§†è§‰è¯­è¨€æ¨¡å‹æ— éœ€æ¨ç†æ—¶çš„æ–‡æœ¬æ£€æµ‹æˆ–OCRã€‚æ¶ˆèå®éªŒæ˜¾ç¤ºéªŒè¯åé¦ˆè´¡çŒ®6.3 mAPå¢ç›Šï¼Œè¿­ä»£ç²¾ç‚¼å¸¦æ¥9.7 mAPæå‡ã€‚

**Conclusion:** ç ”ç©¶è¯æ˜äº†éªŒè¯å¼è’¸é¦åœ¨è¿ç§»ç©ºé—´æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œé‡Šæ”¾çš„9.5ä¸‡æ¡é«˜è´¨é‡éªŒè¯æ€ç»´é“¾è½¨è¿¹å°†æ¨åŠ¨æ–‡æ¡£ç†è§£ä¸­çš„ç©ºé—´æ¨ç†ç ”ç©¶ï¼Œä¸ºå®é™…éƒ¨ç½²æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Document visual question answering (DocVQA) requires models to jointly reason over textual content and spatial layout, yet current systems exhibit a sharp accuracy--efficiency trade-off: large teacher models achieve strong grounding but are too expensive for deployment, while compact students suffer substantial drops in localization performance. We propose DocVAL, a validated chain-of-thought distillation framework that transfers the spatial reasoning ability of a large teacher into a deployable student VLM through three key components: (1) teacher supervision with validation-time text detection to filter and denoise training signals, (2) a multi-module validator (VAL) that enforces answer correctness and geometric consistency while producing fine-grained, pixel-level error feedback, and (3) a two-stage student training scheme that first learns from validated CoT traces and then undergoes iterative refinement driven by VAL feedback. Our student (Gemma-3 12B) achieves 91.4\% ANLS and 82.4\% mAP on DocVQA as a pure VLM requiring no text detection or OCR at inference. Extensive ablations demonstrate that validated feedback contributes 6.3 mAP gain and iterative refinement accounts for 9.7 mAP improvement. We release 95k high-quality, validator-verified CoT traces to advance spatial reasoning research in document understanding.


### [33] [Structure is Supervision: Multiview Masked Autoencoders for Radiology](https://arxiv.org/abs/2511.22294)
*Sonia Laguna, Andrea Agostini, Alain Ryser, Samuel Ruiperez-Campillo, Irene Cannistraci, Moritz Vandenhirtz, Stephan Mandt, Nicolas Deperrois, Farhad Nooralahzadeh, Michael Krauthammer, Thomas M. Sutter, Julia E. Vogt*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†MVMAEå’ŒMVMAE-V2Tä¸¤ç§è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨æ”¾å°„å­¦ç ”ç©¶çš„è‡ªç„¶å¤šè§†å›¾ç»„ç»‡å’Œæ”¾å°„å­¦æŠ¥å‘Šæ¥å­¦ä¹ è§†å›¾ä¸å˜ä¸”ä¸ç–¾ç—…ç›¸å…³çš„åŒ»å­¦å›¾åƒè¡¨ç¤ºï¼Œåœ¨å¤šä¸ªå¤§è§„æ¨¡å…¬å…±æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç›‘ç£å­¦ä¹ å’Œè§†è§‰è¯­è¨€åŸºçº¿æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ„å»ºé²æ£’çš„åŒ»å­¦æœºå™¨å­¦ä¹ ç³»ç»Ÿéœ€è¦èƒ½å¤Ÿåˆ©ç”¨ä¸´åºŠæ•°æ®å†…åœ¨ç»“æ„çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯æ”¾å°„å­¦ç ”ç©¶ä¸­çš„è‡ªç„¶å¤šè§†å›¾ç»„ç»‡ï¼Œä»¥åŠå¦‚ä½•å°†è¿™äº›ç»“æ„ä¿¡æ¯è½¬åŒ–ä¸ºæœ‰æ•ˆçš„è‡ªç›‘ç£ä¿¡å·æ¥å­¦ä¹ è§†å›¾ä¸å˜ä¸”ä¸ç–¾ç—…ç›¸å…³çš„è¡¨ç¤ºã€‚

**Method:** ç ”ç©¶æå‡ºäº†å¤šè§†å›¾æ©ç è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œç»“åˆæ©ç å›¾åƒé‡å»ºä¸è·¨è§†å›¾å¯¹é½ï¼Œå°†ä¸åŒæŠ•å½±é—´çš„ä¸´åºŠå†—ä½™è½¬åŒ–ä¸ºè‡ªç›‘ç£ä¿¡å·ï¼›è¿›ä¸€æ­¥æ‰©å±•ä¸ºMVMAE-V2Tï¼Œå¼•å…¥æ”¾å°„å­¦æŠ¥å‘Šä½œä¸ºè¾…åŠ©æ–‡æœ¬å­¦ä¹ ä¿¡å·ä»¥å¢å¼ºè¯­ä¹‰åŸºç¡€ï¼ŒåŒæ—¶ä¿æŒå®Œå…¨åŸºäºè§†è§‰çš„æ¨ç†èƒ½åŠ›ã€‚

**Result:** åœ¨MIMIC-CXRã€CheXpertå’ŒPadChestä¸‰ä¸ªå¤§è§„æ¨¡å…¬å…±æ•°æ®é›†çš„ä¸‹æ¸¸ç–¾ç—…åˆ†ç±»ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒMVMAEæŒç»­ä¼˜äºç›‘ç£å­¦ä¹ å’Œè§†è§‰è¯­è¨€åŸºçº¿æ–¹æ³•ï¼›MVMAE-V2Tæä¾›äº†é¢å¤–æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ ‡ç­¾æƒ…å†µä¸‹ï¼Œç»“æ„åŒ–æ–‡æœ¬ç›‘ç£çš„æ•ˆç›Šæœ€ä¸ºæ˜¾è‘—ã€‚

**Conclusion:** è¯¥ç ”ç©¶ç¡®ç«‹äº†ç»“æ„å’Œæ–‡æœ¬ç›‘ç£ä½œä¸ºæ„å»ºå¯æ‰©å±•ã€ä¸´åºŠåŸºç¡€çš„åŒ»å­¦åŸºç¡€æ¨¡å‹çš„äº’è¡¥è·¯å¾„ï¼Œè¯æ˜äº†åˆ©ç”¨æ”¾å°„å­¦ç ”ç©¶çš„è‡ªç„¶å¤šè§†å›¾ç»„ç»‡å’Œæ”¾å°„å­¦æŠ¥å‘Šèƒ½å¤Ÿæ˜¾è‘—æå‡åŒ»å­¦å›¾åƒè¡¨ç¤ºå­¦ä¹ çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹ã€‚

---

#### ğŸ“„ Abstract
Building robust medical machine learning systems requires pretraining strategies that exploit the intrinsic structure present in clinical data. We introduce Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn view-invariant and disease-relevant representations. MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal. We further extend this approach with MVMAE-V2T, which incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference. Evaluated on a downstream disease classification task on three large-scale public datasets, MIMIC-CXR, CheXpert, and PadChest, MVMAE consistently outperforms supervised and vision-language baselines. Furthermore, MVMAE-V2T provides additional gains, particularly in low-label regimes where structured textual supervision is most beneficial. Together, these results establish the importance of structural and textual supervision as complementary paths toward scalable, clinically grounded medical foundation models.


### [34] [CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving](https://arxiv.org/abs/2511.22532)
*Zhaohui Wang, Tengbo Yu, Hao Tang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CoT4ADï¼Œä¸€ç§ç”¨äºè‡ªåŠ¨é©¾é©¶çš„æ–°å‹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ€ç»´é“¾æ¨ç†æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ•°å€¼æ¨ç†å’Œå› æœæ¨ç†èƒ½åŠ›ï¼Œåœ¨å¤æ‚é©¾é©¶åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­å­˜åœ¨æ•°å€¼æ¨ç†èƒ½åŠ›æœ‰é™å’Œè¾“å…¥-è¾“å‡ºæ˜ å°„è¿‡äºç®€åŒ–çš„é—®é¢˜ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨éœ€è¦é€æ­¥å› æœæ¨ç†çš„å¤æ‚é©¾é©¶åœºæ™¯ä¸­çš„è¡¨ç°ï¼Œå› æ­¤éœ€è¦å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä»¥åº”å¯¹åŠ¨æ€ç¯å¢ƒä¸­çš„å†³ç­–æŒ‘æˆ˜ã€‚

**Method:** CoT4ADæ¡†æ¶é€šè¿‡æ•´åˆè§†è§‰è§‚å¯Ÿå’Œè¯­è¨€æŒ‡ä»¤æ¥æ‰§è¡Œè¯­ä¹‰æ¨ç†ã€åœºæ™¯ç†è§£å’Œè½¨è¿¹è§„åˆ’ï¼Œåœ¨è®­ç»ƒé˜¶æ®µæ˜¾å¼å»ºæ¨¡æ„ŸçŸ¥-é—®é¢˜-é¢„æµ‹-åŠ¨ä½œçš„æ€ç»´é“¾ï¼Œä»¥å¯¹é½å¤šä¸ªé©¾é©¶ä»»åŠ¡ä¸­çš„æ¨ç†ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ï¼Œåœ¨æ¨ç†é˜¶æ®µæ‰§è¡Œéšå¼æ€ç»´é“¾æ¨ç†ä»¥å®ç°åŠ¨æ€ç¯å¢ƒä¸­çš„ä¸€è‡´æ•°å€¼æ¨ç†å’Œé²æ£’å†³ç­–ã€‚

**Result:** åœ¨nuSceneså’ŒBench2Driveç­‰çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCoT4ADåœ¨å¼€ç¯å’Œé—­ç¯è¯„ä¼°ä¸­éƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†æ€ç»´é“¾æ¨ç†åœ¨å¢å¼ºè‡ªåŠ¨é©¾é©¶æ¨¡å‹æ•°å€¼å’Œå› æœæ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜æ€ç»´é“¾æ¨ç†æœºåˆ¶èƒ½å¤Ÿæ˜¾è‘—æå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„æ¨ç†èƒ½åŠ›å’Œå†³ç­–é²æ£’æ€§ï¼Œä¸ºå¤æ‚åŠ¨æ€ç¯å¢ƒä¸­çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†æ–°çš„æ¡†æ¶è®¾è®¡æ€è·¯ï¼Œå¹¶å±•ç¤ºäº†è·¨ä»»åŠ¡æ¨ç†ç©ºé—´å¯¹é½çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Vision-Language-Action (VLA) models have recently attracted growing attention in end-to-end autonomous driving for their strong reasoning capabilities and rich world knowledge. However, existing VLAs often suffer from limited numerical reasoning ability and overly simplified input-output mappings, which hinder their performance in complex driving scenarios requiring step-by-step causal reasoning. To address these challenges, we propose CoT4AD, a novel VLA framework that introduces Chain-of-Thought (CoT) reasoning for autonomous driving to enhance both numerical and causal reasoning in Vision-Language Models (VLMs). CoT4AD integrates visual observations and language instructions to perform semantic reasoning, scene understanding, and trajectory planning. During training, it explicitly models a perception-question-prediction-action CoT to align the reasoning space with the action space across multiple driving tasks. During inference, it performs implicit CoT reasoning to enable consistent numerical reasoning and robust decision-making in dynamic environments. Extensive experiments on both real-world and simulated benchmarks, including nuScenes and Bench2Drive, demonstrate that CoT4AD achieves state-of-the-art performance in both open-loop and closed-loop evaluations. Code will be released upon paper acceptance.


### [35] [Unexplored flaws in multiple-choice VQA evaluations](https://arxiv.org/abs/2511.22341)
*Fabio Rosenthal, Sebastian Schmidt, Thorsten Graf, Thorsten Bagodonat, Stephan GÃ¼nnemann, Leo Schwinn*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šé€‰é¢˜è§†è§‰é—®ç­”è¯„ä¼°ä¸­å­˜åœ¨æ˜¾è‘—çš„æç¤ºæ ¼å¼æ•æ„Ÿæ€§ï¼Œå³ä½¿è¯­ä¹‰ä¸­æ€§çš„æ ¼å¼å˜åŒ–ä¹Ÿä¼šå¯¼è‡´æ€§èƒ½æ³¢åŠ¨ï¼Œè€Œç°æœ‰åç½®ç¼“è§£ç­–ç•¥æ— æ³•è§£å†³è¿™äº›æ–°å‘ç°çš„åç½®é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å…ˆå‰ç ”ç©¶å·²å‘ç°å¤šé€‰é¢˜è§†è§‰é—®ç­”åŸºå‡†å¯¹ç­”æ¡ˆé€‰é¡¹é¡ºåºæ•æ„Ÿï¼Œä½†æœ¬ç ”ç©¶æŒ‡å‡ºå­˜åœ¨æ›´å¤šæœªè¢«æ¢ç´¢çš„æç¤ºæ ¼å¼åç½®é—®é¢˜ï¼Œè¿™äº›åç½®è´¨ç–‘å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å¯é æ€§ï¼Œéœ€è¦ç³»ç»Ÿæ€§åœ°è¯†åˆ«å’Œåˆ†æè¿™äº›æ ¼å¼å˜åŒ–å› ç´ å¯¹è¯„ä¼°ç»“æœçš„å½±å“ã€‚

**Method:** ç ”ç©¶é€šè¿‡å¤§è§„æ¨¡å®éªŒåˆ†æï¼Œè¯†åˆ«äº†æç¤ºæ ¼å¼ä¸­çš„ä¸‰ä¸ªå…³é”®å˜åŒ–å› ç´ ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæ¶‰åŠä¸ƒä¸ªä¸åŒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œäº”ä¸ªè§†è§‰é—®ç­”æ•°æ®é›†ï¼Œå…±è®¡48ç§ä¸åŒçš„æç¤ºæ ¼å¼å˜ä½“ï¼Œç³»ç»Ÿæ€§åœ°è€ƒå¯Ÿäº†æ ¼å¼å˜åŒ–å¯¹æ¨¡å‹è¯„ä¼°ç»“æœçš„å½±å“ã€‚

**Result:** ç ”ç©¶å‘ç°å¤šé€‰é¢˜è§†è§‰é—®ç­”å¯¹å¾®å°çš„æç¤ºæ ¼å¼å˜åŒ–é«˜åº¦æ•æ„Ÿï¼Œå³ä½¿è¿™äº›å˜åŒ–åœ¨è¯­ä¹‰ä¸Šæ˜¯ä¸­æ€§çš„ï¼Œè¿™ç§æ•æ„Ÿæ€§ç‹¬ç«‹äºå·²çŸ¥çš„é¡ºåºåç½®æˆ–æ¨¡å‹å¯¹æ­£ç¡®ç­”æ¡ˆçš„ç½®ä¿¡åº¦ï¼Œå¹¶ä¸”ç°æœ‰åç½®ç¼“è§£ç­–ç•¥æ— æ³•æœ‰æ•ˆè§£å†³è¿™äº›æ–°å‘ç°çš„æ ¼å¼åç½®é—®é¢˜ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­å­˜åœ¨çš„ç³»ç»Ÿæ€§åç½®é—®é¢˜ï¼Œæç¤ºæ ¼å¼çš„å¾®å°å˜åŒ–ä¼šæ˜¾è‘—å½±å“è¯„ä¼°ç»“æœï¼Œè¿™è¦æ±‚æœªæ¥ç ”ç©¶éœ€è¦å¼€å‘æ›´ç¨³å¥çš„è¯„ä¼°æ–¹æ³•ï¼Œå¹¶é‡æ–°å®¡è§†ç°æœ‰åŸºå‡†çš„å¯é æ€§ï¼Œä¸ºæ„å»ºæ›´å…¬å¹³å’Œå¯é çš„æ¨¡å‹è¯„ä¼°æ¡†æ¶æä¾›äº†é‡è¦è§è§£ã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Language Models (MLLMs) demonstrate strong capabilities in handling image-text inputs. A common way to assess this ability is through multiple-choice Visual Question Answering (VQA). Earlier works have already revealed that these benchmarks are sensitive to answer choice order, a limitation that can be mitigated through careful design. Yet, we highlight additional, unexplored biases in prompt formatting that question the reliability of current MLLM evaluations. Specifically, we identify three key variation factors in prompt formatting and analyze their impact through a large-scale study involving $\mathbf{\text{seven}}$ MLLMs and $\mathbf{\text{five}}$ VQA datasets, spanning $\mathbf{48}$ distinct $\mathbf{\text{prompt format variations}}$. Our findings reveal that multiple-choice VQA is highly sensitive to minor prompt format changes, even when these changes are semantically neutral. We further demonstrate that these biases persist independently of known order biases or the MLLM's confidence in the correct answer. Finally, we demonstrate that existing bias mitigation strategies fail to address these newly identified biases.


### [36] [Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization](https://arxiv.org/abs/2511.22586)
*Yifan Du, Kun Zhou, Yingqian Min, Yue Ling, Wayne Xin Zhao, Youbin Wu*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†ä¸åŒæ€ç»´é“¾è®¾è®¡å¯¹è§†è§‰è¯­è¨€æ¨¡å‹æ³›åŒ–è§†è§‰æ¨ç†èƒ½åŠ›çš„å½±å“ï¼Œå‘ç°ç®€æ´ä¸”ä»…åŒ…å«å¿…è¦ç©ºé—´å®šä½æ­¥éª¤çš„æ€ç»´é“¾åœ¨è¿·å®«æ±‚è§£ç­‰è§†è§‰æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œæ­ç¤ºäº†'çŸ­å³é•¿'æ•ˆåº”ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡æ€ç»´é“¾æ•°æ®ï¼ˆå°¤å…¶æ˜¯é•¿é“¾æˆ–è§†è§‰æ€ç»´é“¾ï¼‰å·²è¢«å¹¿æ³›ç”¨äºç›‘ç£ä¸­é—´æ¨ç†è¿‡ç¨‹ï¼Œä½†ä¸åŒæ€ç»´é“¾è®¾è®¡å¦‚ä½•å½±å“è§†è§‰è¯­è¨€æ¨¡å‹è·å¾—æ³›åŒ–è§†è§‰æ¨ç†èƒ½åŠ›çš„å…·ä½“æœºåˆ¶å°šä¸æ˜ç¡®ï¼Œéœ€è¦ç³»ç»Ÿè¯„ä¼°å“ªäº›è®¾è®¡çœŸæ­£æ”¯æŒå¯æ³›åŒ–çš„æ¨ç†èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨å¯æ§çš„è¿·å®«æ±‚è§£åŸºå‡†ï¼Œå…¶ä¸­æ¨ç†è§„åˆ™å®Œå…¨è§†è§‰åŒ–ï¼Œéš¾åº¦å¯é€šè¿‡ç½‘æ ¼å¤§å°è°ƒèŠ‚ï¼Œæ‰€æœ‰ä¸­é—´æ­¥éª¤å¯è‡ªåŠ¨ç”Ÿæˆã€‚ä½¿ç”¨Qwen2.5-VL-7Bæ¨¡å‹åœ¨æ ‡å‡†SFT-then-RLæµç¨‹ä¸‹ï¼Œæ¯”è¾ƒäº†ä¸‰ç§ä»£è¡¨æ€§æ€ç»´é“¾æ ¼å¼ï¼šè¯­è¨€æ€ç»´é“¾ã€ç©ºé—´å®šä½æ€ç»´é“¾ï¼ˆå«åæ ‡è½¨è¿¹ï¼‰å’Œè§†è§‰æ€ç»´é“¾ï¼ˆå«å›¾åƒæ“ä½œï¼‰ã€‚

**Result:** å®éªŒè¡¨æ˜è§†è§‰å’Œè¾ƒé•¿çš„æ€ç»´é“¾ä¸»è¦åŠ é€Ÿæ”¶æ•›ä½†ä¸æå‡æœ€ç»ˆæ€§èƒ½ä¸Šé™ï¼›ä»…åŒ…å«å¿…è¦å®šä½æ­¥éª¤çš„ç®€æ´æ€ç»´é“¾ä¼˜äºè¾ƒé•¿è½¨è¿¹ï¼›ä»…ä¿ç•™æœ€å°å®šä½ç»“æœçš„æ€ç»´é“¾åœ¨ä¸åŒè¿·å®«å°ºå¯¸ä¸Šæ³›åŒ–èƒ½åŠ›æœ€ä½³ã€‚è¿™äº›å‘ç°åœ¨å…¶ä»–è§†è§‰ä¸­å¿ƒä»»åŠ¡ä¸Šå¾—åˆ°è¿›ä¸€æ­¥éªŒè¯ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†æ€ç»´é“¾è®¾è®¡ä¸­çš„'çŸ­å³é•¿'æ•ˆåº”ï¼Œè¡¨æ˜ç®€æ´ä¸”ä»…åŒ…å«å¿…è¦ç©ºé—´å®šä½ä¿¡æ¯çš„æ€ç»´é“¾æœ€èƒ½ä¿ƒè¿›æ³›åŒ–è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä¸ºæ„å»ºæ›´å¯æ³›åŒ–çš„ç›‘ç£å¾®è°ƒæ•°æ®é›†æä¾›äº†å®ç”¨æŒ‡å¯¼ï¼Œå¼ºè°ƒäº†åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ä¼˜åŒ–æ€ç»´é“¾è®¾è®¡çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as "think with image", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a "short is long" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.


### [37] [INSIGHT: An Interpretable Neural Vision-Language Framework for Reasoning of Generative Artifacts](https://arxiv.org/abs/2511.22351)
*Anshul Bagaria*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºINSIGHTæ¡†æ¶ï¼Œä¸€ä¸ªç”¨äºAIç”Ÿæˆå›¾åƒæ£€æµ‹å’Œè§£é‡Šçš„ç»Ÿä¸€å¤šæ¨¡æ€ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨æä½åˆ†è¾¨ç‡ä¸‹å®ç°é²æ£’æ£€æµ‹å¹¶æä¾›é€æ˜è§£é‡Šï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒå–è¯çš„å¯ä¿¡åº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿåœ¨ç°å®ä¸–ç•Œæ¡ä»¶ä¸‹è¡¨ç°æ€¥å‰§ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸¥é‡ä¸‹é‡‡æ ·ã€å‹ç¼©å’Œè·¨åŸŸåˆ†å¸ƒåç§»æ—¶ï¼Œä¸”å¤§å¤šæ•°æ£€æµ‹å™¨ä½œä¸ºä¸é€æ˜åˆ†ç±»å™¨è¿è¡Œï¼Œæ— æ³•è§£é‡Šä¸ºä½•å›¾åƒè¢«æ ‡è®°ä¸ºåˆæˆï¼Œè¿™å‰Šå¼±äº†ä¿¡ä»»å¹¶é˜»ç¢äº†é«˜é£é™©åœºæ™¯çš„é‡‡ç”¨ã€‚

**Method:** INSIGHTæ¡†æ¶ç»“åˆäº†åˆ†å±‚è¶…åˆ†è¾¨ç‡ä»¥æ”¾å¤§ç»†å¾®å–è¯çº¿ç´¢è€Œä¸å¼•å…¥è¯¯å¯¼æ€§ä¼ªå½±ï¼ŒGrad-CAMé©±åŠ¨çš„å¤šå°ºåº¦å®šä½æ¥æ­ç¤ºæŒ‡ç¤ºç”Ÿæˆæ¨¡å¼çš„ç©ºé—´åŒºåŸŸï¼Œä»¥åŠCLIPå¼•å¯¼çš„è¯­ä¹‰å¯¹é½å°†è§†è§‰å¼‚å¸¸æ˜ å°„åˆ°äººç±»å¯è§£é‡Šçš„æè¿°ç¬¦ï¼Œæœ€åä½¿ç”¨ç»“æ„åŒ–ReAct+æ€ç»´é“¾åè®®æç¤ºè§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸€è‡´ã€ç»†ç²’åº¦çš„è§£é‡Šï¼Œå¹¶é€šè¿‡åŒé˜¶æ®µG-Eval+LLM-as-a-judgeæµç¨‹éªŒè¯ä»¥æœ€å°åŒ–å¹»è§‰å¹¶ç¡®ä¿äº‹å®æ€§ã€‚

**Result:** åœ¨åŒ…æ‹¬åŠ¨ç‰©ã€è½¦è¾†å’ŒæŠ½è±¡åˆæˆåœºæ™¯åœ¨å†…çš„å¤šæ ·åŒ–é¢†åŸŸä¸­ï¼ŒINSIGHTåœ¨æç«¯é€€åŒ–æ¡ä»¶ä¸‹æ˜¾è‘—æé«˜äº†æ£€æµ‹é²æ£’æ€§å’Œè§£é‡Šè´¨é‡ï¼Œä¼˜äºå…ˆå‰çš„æ£€æµ‹å™¨å’Œé»‘ç›’è§†è§‰è¯­è¨€æ¨¡å‹åŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æä½åˆ†è¾¨ç‡ï¼ˆ16x16-64x64ï¼‰ä¸‹è¡¨ç°å‡ºè‰²ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºé€æ˜ã€å¯é çš„AIç”Ÿæˆå›¾åƒå–è¯æä¾›äº†ä¸€æ¡å®ç”¨è·¯å¾„ï¼Œç¡®ç«‹äº†INSIGHTä½œä¸ºå¯ä¿¡å¤šæ¨¡æ€å†…å®¹éªŒè¯çš„é‡è¦è¿›å±•ï¼Œå¼ºè°ƒäº†å¯è§£é‡Šæ€§åœ¨ç”Ÿæˆå›¾åƒæ£€æµ‹ä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶ä¸ºé«˜é£é™©åº”ç”¨ä¸­çš„å¯ä¿¡é‡‡ç”¨å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
The growing realism of AI-generated images produced by recent GAN and diffusion models has intensified concerns over the reliability of visual media. Yet, despite notable progress in deepfake detection, current forensic systems degrade sharply under real-world conditions such as severe downsampling, compression, and cross-domain distribution shifts. Moreover, most detectors operate as opaque classifiers, offering little insight into why an image is flagged as synthetic, undermining trust and hindering adoption in high-stakes settings.
  We introduce INSIGHT (Interpretable Neural Semantic and Image-based Generative-forensic Hallucination Tracing), a unified multimodal framework for robust detection and transparent explanation of AI-generated images, even at extremely low resolutions (16x16 - 64x64). INSIGHT combines hierarchical super-resolution for amplifying subtle forensic cues without inducing misleading artifacts, Grad-CAM driven multi-scale localization to reveal spatial regions indicative of generative patterns, and CLIP-guided semantic alignment to map visual anomalies to human-interpretable descriptors. A vision-language model is then prompted using a structured ReAct + Chain-of-Thought protocol to produce consistent, fine-grained explanations, verified through a dual-stage G-Eval + LLM-as-a-judge pipeline to minimize hallucinations and ensure factuality.
  Across diverse domains, including animals, vehicles, and abstract synthetic scenes, INSIGHT substantially improves both detection robustness and explanation quality under extreme degradation, outperforming prior detectors and black-box VLM baselines. Our results highlight a practical path toward transparent, reliable AI-generated image forensics and establish INSIGHT as a step forward in trustworthy multimodal content verification.


### [38] [HarmoCLIP: Harmonizing Global and Regional Representations in Contrastive Vision-Language Models](https://arxiv.org/abs/2511.22594)
*Haoxi Zeng, Haoxuan Li, Yi Bin, Pengpeng Zeng, Xing Xu, Yang Yang, Heng Tao Shen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†HarmoCLIPæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³CLIPæ¨¡å‹ä¸­å…¨å±€ä¸å±€éƒ¨è¡¨å¾ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œé€šè¿‡å¼•å…¥ç»†ç²’åº¦è¯­ä¹‰ç›‘ç£å’ŒåŒºåŸŸ-è¯­è¨€å¯¹é½ç­–ç•¥ï¼Œåœ¨ä¿æŒå…¨å±€ä¸€è‡´æ€§çš„åŒæ—¶å¢å¼ºå±€éƒ¨è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** CLIPæ¨¡å‹è™½ç„¶å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ç”±äºç¼ºä¹åŒºåŸŸçº§ç›‘ç£ï¼Œå…¶ç»†ç²’åº¦è¯­ä¹‰ç†è§£èƒ½åŠ›æœ‰é™ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å¾€å¾€ç ´åäº†å…¨å±€å¯¹é½ï¼Œå¯¼è‡´æ”¹å–„å±€éƒ¨æ„ŸçŸ¥çš„åŒæ—¶ä¼šé™ä½å…¨å±€ä¸€è‡´æ€§ï¼Œå½¢æˆäº†æŒç»­çš„æƒè¡¡å›°å¢ƒã€‚

**Method:** HarmoCLIPæ¡†æ¶é¦–å…ˆè¯†åˆ«å‡ºå±€éƒ¨æ–‡æœ¬ä¸è§†è§‰è¯­ä¹‰ä¹‹é—´ç¼ºä¹ç›´æ¥å¯¹é½æ˜¯æƒè¡¡é—®é¢˜çš„æ ¹æœ¬åŸå› ã€‚ä¸ºæ­¤ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†æ˜¾å¼çš„ç»†ç²’åº¦è¯­ä¹‰ç›‘ç£é¡¹ï¼Œç›´æ¥å¯¹é½æ–‡æœ¬ç‰‡æ®µä¸å…¶å¯¹åº”çš„è§†è§‰åŒºåŸŸï¼Œæœ‰æ•ˆæ¡¥æ¥å›¾åƒåŒºåŸŸç©ºé—´ä¸æ–‡æœ¬ç©ºé—´ã€‚ä¸ºè¿›ä¸€æ­¥å¢å¼ºå±€éƒ¨è¡¨å¾èƒ½åŠ›ï¼Œè¿˜æå‡ºäº†æ–°é¢–çš„åŒºåŸŸ-è¯­è¨€å¯¹é½ç›‘ç£ç­–ç•¥ï¼Œåœ¨ä¸æŸå®³å…¨å±€è¯­ä¹‰ä¸€è‡´æ€§çš„å‰æä¸‹ä¿ƒè¿›ç»†ç²’åº¦è¯­ä¹‰å­¦ä¹ ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHarmoCLIPåœ¨æ£€ç´¢è¿™ä¸€å…¨å±€ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼ˆæå‡é«˜è¾¾69.78%ï¼‰ï¼Œåœ¨è¾¹ç•Œæ¡†åˆ†ç±»è¿™ä¸€åŒºåŸŸä»»åŠ¡ä¸Šå®ç°äº†3.2%çš„Top-1å‡†ç¡®ç‡æ˜¾è‘—æå‡ã€‚è¯¥æ–¹æ³•æŒç»­ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œä¸ºCLIPä¸­çš„å…¨å±€-å±€éƒ¨æƒè¡¡é—®é¢˜æä¾›äº†å¹³è¡¡ã€é«˜æ•ˆä¸”å³æ’å³ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†CLIPæ¨¡å‹ä¸­å…¨å±€ä¸å±€éƒ¨è¡¨å¾æƒè¡¡çš„æ ¹æœ¬åŸå› ï¼Œå¹¶æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„åè°ƒæœºåˆ¶ã€‚HarmoCLIPæ¡†æ¶ä¸ä»…æå‡äº†ç»†ç²’åº¦è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†å…¨å±€å¯¹é½çš„å®Œæ•´æ€§ï¼Œä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„ç»†ç²’åº¦ç†è§£æä¾›äº†æ–°çš„è®¾è®¡æ€è·¯ï¼Œå…·æœ‰å³æ’å³ç”¨çš„å®ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Contrastive Language-Image Pre-training (CLIP) has demonstrated remarkable generalization ability and strong performance across a wide range of vision-language tasks. However, due to the lack of region-level supervision, CLIP exhibits limited fine-grained semantic understanding. Although several methods attempt to mitigate this issue, they unintentionally disrupt the global alignment, resulting in a persistent trade-off where improving local perception simultaneously degrades global coherence. In this paper, we propose HarmoCLIP, a novel framework designed to harmonize global and region representations within CLIP. We first identify that the absence of direct alignment between local textual and visual semantics is the fundamental cause of the trade-off. To address this, HarmoCLIP introduces an explicit fine-grained semantic supervision term that directly aligns textual segments with their corresponding visual regions, effectively bridging the image region space and the textual space. To further strengthen the representation capability at the local level, our method introduces a novel Region-Language Alignment supervision strategy that promotes fine-grained semantic learning without compromising global semantic consistency. Extensive experiments demonstrate that HarmoCLIP achieves state-of-the-art (improvement up to 69.78%) performance on the global task of retrieval and yields a substantial 3.2% improvement in Top-1 accuracy on the region task of bounding-box classification, consistently outperforming prior approaches while providing a balanced, efficient, and plug-and-play solution to the global-local trade-off in CLIP. Code is available at https://github.com/Erosist/HarmoCLIP.


### [39] [UAV-MM3D: A Large-Scale Synthetic Benchmark for 3D Perception of Unmanned Aerial Vehicles with Multi-Modal Data](https://arxiv.org/abs/2511.22404)
*Longkun Zou, Jiale Wang, Rongqin Liang, Hai Wu, Ke Chen, Yaowei Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†UAV-MM3Dï¼Œä¸€ä¸ªç”¨äºä½ç©ºæ— äººæœºæ„ŸçŸ¥ä¸è¿åŠ¨ç†è§£çš„é«˜ä¿çœŸå¤šæ¨¡æ€åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«40ä¸‡å¸§åŒæ­¥æ•°æ®ï¼Œæ¶µç›–å¤šç§åœºæ™¯ã€å¤©æ°”æ¡ä»¶å’Œä¼ æ„Ÿå™¨æ¨¡æ€ï¼Œå¹¶æä¾›äº†ç›¸åº”çš„åŸºå‡†æ¨¡å‹ä»¥ä¿ƒè¿›ç›¸å…³ç ”ç©¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ— äººæœºåœ¨å¤æ‚ä½ç©ºç¯å¢ƒä¸­çš„ç²¾ç¡®æ„ŸçŸ¥å¯¹ç©ºåŸŸå®‰å…¨å’Œæ™ºèƒ½ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œä½†ç°å®ä¸–ç•Œæ— äººæœºæ•°æ®æ”¶é›†é¢ä¸´ç©ºåŸŸç®¡åˆ¶ã€éšç§é—®é¢˜å’Œç¯å¢ƒå˜åŒ–ç­‰å›ºæœ‰çº¦æŸï¼ŒåŒæ—¶3Då§¿æ€å’Œè·¨æ¨¡æ€å¯¹åº”å…³ç³»çš„æ‰‹åŠ¨æ ‡æ³¨è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ï¼Œç°æœ‰æ•°æ®é›†éš¾ä»¥æ»¡è¶³å¯é è§£å†³æ–¹æ¡ˆçš„å¼€å‘éœ€æ±‚ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†UAV-MM3Dåˆæˆæ•°æ®é›†ï¼ŒåŒ…å«40ä¸‡å¸§åŒæ­¥æ•°æ®ï¼Œæ¶µç›–åŸå¸‚ã€éƒŠåŒºã€æ£®æ—ã€æ²¿æµ·ç­‰å¤šæ ·åœºæ™¯å’Œæ™´ã€é˜´ã€é›¨ã€é›¾ç­‰å¤©æ°”æ¡ä»¶ï¼ŒåŒ…å«å¾®ã€å°ã€ä¸­å‹å¤šç§æ— äººæœºæ¨¡å‹ï¼Œæä¾›RGBã€çº¢å¤–ã€æ¿€å…‰é›·è¾¾ã€é›·è¾¾å’ŒåŠ¨æ€è§†è§‰ä¼ æ„Ÿå™¨äº”ç§æ¨¡æ€æ•°æ®ï¼Œæ¯å¸§åŒ…å«2D/3Dè¾¹ç•Œæ¡†ã€6è‡ªç”±åº¦å§¿æ€å’Œå®ä¾‹çº§æ ‡æ³¨ï¼Œå¹¶æå‡ºäº†LiDARå¼•å¯¼çš„å¤šæ¨¡æ€èåˆåŸºå‡†ç½‘ç»œLGFusionNetå’Œä¸“ç”¨çš„æ— äººæœºè½¨è¿¹é¢„æµ‹åŸºå‡†æ¨¡å‹ã€‚

**Result:** UAV-MM3Dæ•°æ®é›†æ”¯æŒæ— äººæœº3Dæ£€æµ‹ã€å§¿æ€ä¼°è®¡ã€ç›®æ ‡è·Ÿè¸ªå’ŒçŸ­æœŸè½¨è¿¹é¢„æµ‹ç­‰æ ¸å¿ƒä»»åŠ¡ï¼Œé€šè¿‡å¯æ§çš„ä»¿çœŸç¯å¢ƒã€å…¨é¢çš„åœºæ™¯è¦†ç›–å’Œä¸°å¯Œçš„æ ‡æ³¨ä¿¡æ¯ï¼Œä¸ºæ— äººæœº3Dæ„ŸçŸ¥ç ”ç©¶æä¾›äº†å…¬å¼€çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œä¿ƒè¿›äº†ç›¸å…³ç®—æ³•çš„å‘å±•ä¸è¯„ä¼°ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡æ„å»ºå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å¤šæ¨¡æ€åˆæˆæ•°æ®é›†ï¼Œæœ‰æ•ˆè§£å†³äº†æ— äººæœºæ„ŸçŸ¥ç ”ç©¶ä¸­çœŸå®æ•°æ®è·å–å›°éš¾çš„é—®é¢˜ï¼Œä¸ºä½ç©ºæ— äººæœºæ„ŸçŸ¥å’Œè¿åŠ¨ç†è§£æä¾›äº†é‡è¦çš„æ•°æ®èµ„æºå’ŒåŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ¨åŠ¨äº†ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ï¼ŒåŒæ—¶å±•ç¤ºäº†åˆæˆæ•°æ®åœ¨è§£å†³ç°å®ä¸–ç•Œæ•°æ®çº¦æŸæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Accurate perception of UAVs in complex low-altitude environments is critical for airspace security and related intelligent systems. Developing reliable solutions requires large-scale, accurately annotated, and multimodal data. However, real-world UAV data collection faces inherent constraints due to airspace regulations, privacy concerns, and environmental variability, while manual annotation of 3D poses and cross-modal correspondences is time-consuming and costly. To overcome these challenges, we introduce UAV-MM3D, a high-fidelity multimodal synthetic dataset for low-altitude UAV perception and motion understanding. It comprises 400K synchronized frames across diverse scenes (urban areas, suburbs, forests, coastal regions) and weather conditions (clear, cloudy, rainy, foggy), featuring multiple UAV models (micro, small, medium-sized) and five modalities - RGB, IR, LiDAR, Radar, and DVS (Dynamic Vision Sensor). Each frame provides 2D/3D bounding boxes, 6-DoF poses, and instance-level annotations, enabling core tasks related to UAVs such as 3D detection, pose estimation, target tracking, and short-term trajectory forecasting. We further propose LGFusionNet, a LiDAR-guided multimodal fusion baseline, and a dedicated UAV trajectory prediction baseline to facilitate benchmarking. With its controllable simulation environment, comprehensive scenario coverage, and rich annotations, UAV3D offers a public benchmark for advancing 3D perception of UAVs.


### [40] [All Centers Are at most a Few Tokens Apart: Knowledge Distillation with Domain Invariant Prompt Tuning](https://arxiv.org/abs/2511.22739)
*Amir Mohammad Ezzati, Alireza Malekhosseini, Armin Khosravi, Mohammad Hossein Rohban*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè®¡ç®—ç—…ç†å­¦é¢†åŸŸæ³›åŒ–çš„é¢†åŸŸä¸å˜æç¤ºè°ƒä¼˜æ–¹æ³•ï¼Œé€šè¿‡ä»ç—…ç†å­¦è§†è§‰è¯­è¨€æ¨¡å‹ä¸­è’¸é¦çŸ¥è¯†ï¼Œå­¦ä¹ é¢†åŸŸä¸å˜æç¤ºä»¥å¢å¼ºæ¨¡å‹åœ¨å¼‚æ„ä¸´åºŠæ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è®¡ç®—ç—…ç†å­¦ä¸­ç”±äºæŸ“è‰²åè®®ã€æ‰«æè®¾å¤‡å’Œæˆåƒè®¾ç½®çš„å˜åŒ–å¯¼è‡´æ˜¾è‘—çš„é¢†åŸŸåç§»ï¼Œè€Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å¯¹æç¤ºå˜åŒ–æ•æ„Ÿï¼Œä¸”ç—…ç†å­¦å›¾åƒç¼ºä¹è‡ªç„¶å›¾åƒä¸­çš„è¯­ä¹‰æè¿°ç¬¦ï¼Œéš¾ä»¥å®šä¹‰é¢†åŸŸç‰¹å®šçš„æç¤ºï¼Œéœ€è¦æ•°æ®é©±åŠ¨çš„æ–¹æ³•æ¥å­¦ä¹ é¢†åŸŸä¸å˜æç¤ºã€‚

**Method:** æå‡ºé¢†åŸŸä¸å˜æç¤ºè°ƒä¼˜æ–¹æ³•ï¼Œä¸ºæ¯ä¸ªé¢†åŸŸå­¦ä¹ å¤šä¸ªè¾“å…¥æ ‡è®°ï¼Œè¿™äº›æ ‡è®°åˆ†åˆ«é’ˆå¯¹æ¯ä¸ªé¢†åŸŸè¿›è¡Œè®­ç»ƒï¼Œç„¶åè·¨é¢†åŸŸå¹³å‡ä»¥äº§ç”Ÿé¢†åŸŸä¸å˜æç¤ºï¼›å­¦ç”Ÿæ¨¡å‹é€šè¿‡åˆ©ç”¨DIPTå­¦ä¹ çš„æç¤ºä»PLIPçš„æ–‡æœ¬ç¼–ç å™¨è’¸é¦çŸ¥è¯†ï¼Œä½¿è§†è§‰ç‰¹å¾ä¸é¢†åŸŸä¸å˜åµŒå…¥å¯¹é½ã€‚

**Result:** è¯¥æ–¹æ³•åœ¨ç»„ç»‡ç—…ç†å­¦æ•°æ®é›†ä¸Šçš„é¢†åŸŸæ³›åŒ–ä»»åŠ¡ä¸­ï¼Œç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œåœ¨å¹³å‡F1åˆ†æ•°ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œè¯æ˜äº†å…¶åœ¨å¼‚æ„æ•°æ®æºä¸Šçš„ä¼˜è¶Šæ³›åŒ–æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºåœ¨ç°å®ä¸–ç•Œä¸´åºŠé—®é¢˜ä¸­éƒ¨ç½²é²æ£’çš„è®¡ç®—ç—…ç†å­¦æ¨¡å‹æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œé€šè¿‡é¢†åŸŸä¸å˜æç¤ºå­¦ä¹ å®ç°äº†æ›´å¥½çš„è·¨ä¸­å¿ƒæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºè§£å†³åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é¢†åŸŸåç§»é—®é¢˜æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Domain generalization is critical in computational pathology (CPath) due to inherent domain shifts caused by variations in staining protocols, scanner devices, and imaging settings across clinical centers. Vision-language models (VLMs), such as PLIP-a pathology-tuned CLIP-trained on image-text pairs across diverse domains, serve as strong knowledge distillation sources. However, their zero-shot performance with predefined prompts remains limited due to sensitivity to prompt variations. Moreover, unlike natural images, histopathology centers lack semantic descriptors (e.g., 'sketch'), making it difficult to define domain-specific prompts for clinical centers. This requires a data-driven approach for learning domain-specific and ultimately class-generic continuous prompts. We propose Domain Invariant Prompt Tuning (DIPT) for knowledge distillation process, a novel step that learns multiple input tokens for each domain. These tokens are trained separately for each domain and are averaged across domains, leading to domain-invariant prompts. Our student model then distills knowledge from PLIP's text encoder by leveraging the prompts learned by DIPT. This leads to alignment of visual features with domain-invariant embeddings, enhancing generalization by training on multiple domains. Our method adds a significant improvement in average F1-score to existing state-of-the-art (SOTA) knowledge distillation approaches in domain generalization with histopathology datasets. This work helps the way of deploying robust CPath models in real-world clinical problems with heterogeneous data sources.


### [41] [SkeletonAgent: An Agentic Interaction Framework for Skeleton-based Action Recognition](https://arxiv.org/abs/2511.22433)
*Hongda Liu, Yunfan Liu, Changlu Wang, Yunlong Wang, Zhenan Sun*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSkeletonAgentæ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªåä½œä»£ç†ï¼ˆQuestionerå’ŒSelectorï¼‰å°†éª¨æ¶åŠ¨ä½œè¯†åˆ«æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹æ¡¥æ¥ï¼Œä½¿LLMèƒ½å¤Ÿæä¾›æ›´å…·åˆ¤åˆ«æ€§çš„è¯­ä¹‰å…ˆéªŒï¼Œä»è€Œæ˜¾è‘—æå‡å¯¹ç›¸ä¼¼åŠ¨ä½œçš„åŒºåˆ†èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºéª¨æ¶çš„åŠ¨ä½œè¯†åˆ«æ–¹æ³•è™½ç„¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æä¾›è¯­ä¹‰å…ˆéªŒï¼Œä½†LLMé€šå¸¸ä¸è¯†åˆ«æ¨¡å‹éš”ç¦»è¿è¡Œä¸”ç¼ºä¹æ€§èƒ½åé¦ˆï¼Œå¯¼è‡´å…¶æ— æ³•æä¾›åŒºåˆ†ç›¸ä¼¼åŠ¨ä½œæ‰€éœ€çš„å…³é”®åˆ¤åˆ«æ€§çº¿ç´¢ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„è¿›ä¸€æ­¥æå‡ã€‚

**Method:** SkeletonAgentæ¡†æ¶åŒ…å«ä¸¤ä¸ªåä½œä»£ç†ï¼šQuestionerè¯†åˆ«æœ€æ˜“æ··æ·†çš„åŠ¨ä½œç±»åˆ«å¹¶å°†å…¶ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™LLMä»¥è·å¾—é’ˆå¯¹æ€§æŒ‡å¯¼ï¼›Selectorè§£æLLMçš„å“åº”ï¼Œæå–ç²¾ç¡®çš„å…³èŠ‚çº§çº¦æŸå¹¶åé¦ˆç»™è¯†åˆ«å™¨ï¼Œå®ç°ç»†ç²’åº¦çš„è·¨æ¨¡æ€å¯¹é½ã€‚

**Result:** åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆNTU RGB+Dã€NTU RGB+D 120ã€Kinetics-Skeletonã€FineGYMå’ŒUAV-Humanï¼‰ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒSkeletonAgentå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„åŸºå‡†æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æå‡éª¨æ¶åŠ¨ä½œè¯†åˆ«æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡åä½œä»£ç†æœºåˆ¶æ¡¥æ¥è¯†åˆ«æ¨¡å‹ä¸LLMçš„æœ‰æ•ˆæ€§ï¼Œä½¿LLMèƒ½å¤Ÿæä¾›æ›´å…·åˆ¤åˆ«æ€§çš„è¯­ä¹‰æŒ‡å¯¼ï¼Œä¸ºåŸºäºéª¨æ¶çš„åŠ¨ä½œè¯†åˆ«é¢†åŸŸæä¾›äº†ä¸€ç§æ–°çš„è·¨æ¨¡æ€äº¤äº’èŒƒå¼ï¼Œå¹¶è¡¨æ˜ç»†ç²’åº¦çš„å…³èŠ‚çº§çº¦æŸæå–å¯¹æå‡æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚

---

#### ğŸ“„ Abstract
Recent advances in skeleton-based action recognition increasingly leverage semantic priors from Large Language Models (LLMs) to enrich skeletal representations. However, the LLM is typically queried in isolation from the recognition model and receives no performance feedback. As a result, it often fails to deliver the targeted discriminative cues critical to distinguish similar actions. To overcome these limitations, we propose SkeletonAgent, a novel framework that bridges the recognition model and the LLM through two cooperative agents, i.e., Questioner and Selector. Specifically, the Questioner identifies the most frequently confused classes and supplies them to the LLM as context for more targeted guidance. Conversely, the Selector parses the LLM's response to extract precise joint-level constraints and feeds them back to the recognizer, enabling finer-grained cross-modal alignment. Comprehensive evaluations on five benchmarks, including NTU RGB+D, NTU RGB+D 120, Kinetics-Skeleton, FineGYM, and UAV-Human, demonstrate that SkeletonAgent consistently outperforms state-of-the-art benchmark methods. The code is available at https://github.com/firework8/SkeletonAgent.


### [42] [Leveraging Textual Compositional Reasoning for Robust Change Captioning](https://arxiv.org/abs/2511.22903)
*Kyu Ri Park, Jiyoung Park, Seong Tae Kim, Hong Joo Lee, Jung Uk Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CORTEXæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆè§†è§‰è¯­è¨€æ¨¡å‹æä¾›çš„æ–‡æœ¬çº¿ç´¢æ¥å¢å¼ºå˜åŒ–æè¿°ä»»åŠ¡ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ•æ‰åƒç´ çº§å·®å¼‚å’Œåœºæ™¯çº§æ–‡æœ¬çŸ¥è¯†ï¼Œä»è€Œè§£å†³ç°æœ‰æ–¹æ³•å› ä»…ä¾èµ–è§†è§‰ç‰¹å¾è€Œéš¾ä»¥è¯†åˆ«ç»†å¾®ä½†æœ‰æ„ä¹‰å˜åŒ–çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å˜åŒ–æè¿°æ–¹æ³•ä¸»è¦ä¾èµ–è§†è§‰ç‰¹å¾ï¼Œä½†ç”±äºç¼ºä¹å¯¹ç»“æ„åŒ–ä¿¡æ¯ï¼ˆå¦‚å¯¹è±¡å…³ç³»å’Œç»„åˆè¯­ä¹‰ï¼‰çš„æ˜¾å¼è¡¨ç¤ºèƒ½åŠ›ï¼Œå¾€å¾€éš¾ä»¥æ•æ‰ç»†å¾®ä½†æœ‰æ„ä¹‰çš„å˜åŒ–ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹å¯¹å¤æ‚åœºæ™¯å˜åŒ–çš„å‡†ç¡®ç†è§£ä¸æè¿°ã€‚

**Method:** CORTEXæ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šå›¾åƒçº§å˜åŒ–æ£€æµ‹å™¨ç”¨äºè¯†åˆ«é…å¯¹å›¾åƒé—´çš„ä½çº§è§†è§‰å·®å¼‚ï¼›æ¨ç†æ„ŸçŸ¥æ–‡æœ¬æå–æ¨¡å—åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆéšå«åœ¨è§†è§‰ç‰¹å¾ä¸­çš„ç»„åˆæ¨ç†æè¿°ï¼›å›¾åƒ-æ–‡æœ¬åŒé‡å¯¹é½æ¨¡å—å°†è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾å¯¹é½ä»¥å®ç°ç»†ç²’åº¦å…³ç³»æ¨ç†ï¼Œä»è€Œå®ç°å¯¹è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„è”åˆæ¨ç†ã€‚

**Result:** è™½ç„¶æ‘˜è¦æœªæä¾›å…·ä½“æ€§èƒ½æŒ‡æ ‡ï¼Œä½†è®ºæ–‡è¡¨æ˜CORTEXæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆæ•´åˆåƒç´ çº§å·®å¼‚å’Œåœºæ™¯çº§æ–‡æœ¬çŸ¥è¯†ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹æå–æ›´ä¸°å¯Œçš„å›¾åƒæ–‡æœ¬ä¿¡å·ï¼Œæ­ç¤ºæ½œåœ¨çš„ç»„åˆæ¨ç†ï¼Œä»è€Œæ•æ‰ä»…å‡­è§†è§‰ç‰¹å¾éš¾ä»¥è¯†åˆ«çš„æ¨¡ç³Šå˜åŒ–ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†æ•´åˆæ–‡æœ¬çº¿ç´¢å¯¹äºå¢å¼ºå˜åŒ–ç†è§£çš„é‡è¦æ€§ï¼ŒCORTEXæ¡†æ¶é€šè¿‡ç»“åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„äº’è¡¥ä¿¡æ¯ï¼Œä¸ºè§£å†³å˜åŒ–æè¿°ä¸­ç»†å¾®å˜åŒ–è¯†åˆ«éš¾é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†åœ¨è§†è§‰å˜åŒ–åˆ†æä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Change captioning aims to describe changes between a pair of images. However, existing works rely on visual features alone, which often fail to capture subtle but meaningful changes because they lack the ability to represent explicitly structured information such as object relationships and compositional semantics. To alleviate this, we present CORTEX (COmpositional Reasoning-aware TEXt-guided), a novel framework that integrates complementary textual cues to enhance change understanding. In addition to capturing cues from pixel-level differences, CORTEX utilizes scene-level textual knowledge provided by Vision Language Models (VLMs) to extract richer image text signals that reveal underlying compositional reasoning. CORTEX consists of three key modules: (i) an Image-level Change Detector that identifies low-level visual differences between paired images, (ii) a Reasoning-aware Text Extraction (RTE) module that use VLMs to generate compositional reasoning descriptions implicit in visual features, and (iii) an Image-Text Dual Alignment (ITDA) module that aligns visual and textual features for fine-grained relational reasoning. This enables CORTEX to reason over visual and textual features and capture changes that are otherwise ambiguous in visual features alone.


### [43] [ABounD: Adversarial Boundary-Driven Few-Shot Learning for Multi-Class Anomaly Detection](https://arxiv.org/abs/2511.22436)
*Runzhi Deng, Yundi Hu, Xinshuang Zhang, Zhao Wang, Xixi Liu, Wang-Zhou Dai, Caifeng Shan, Fang Zhao*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºABounDï¼Œä¸€ç§å¯¹æŠ—æ€§è¾¹ç•Œé©±åŠ¨çš„å°‘æ ·æœ¬å¤šç±»åˆ«å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€æ¦‚å¿µèåˆæ¨¡å—ç”Ÿæˆç±»åˆ«è‡ªé€‚åº”æç¤ºï¼Œå¹¶ç»“åˆå¯¹æŠ—æ€§è¾¹ç•Œé”»é€ æŠ€æœ¯æ¥å¡‘é€ æ›´ç²¾ç¡®çš„å†³ç­–è¾¹ç•Œï¼Œåœ¨MVTec-ADå’ŒVisAæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°‘æ ·æœ¬å¤šç±»åˆ«å·¥ä¸šå¼‚å¸¸æ£€æµ‹é¢ä¸´æ•°æ®ç¨€ç¼ºå¯¼è‡´æ­£å¸¸ä¸å¼‚å¸¸çŠ¶æ€è¾¹ç•Œæ¨¡ç³Šçš„æŒ‘æˆ˜ï¼Œè¿™ç§æ¨¡ç³Šæ€§å¯¼è‡´éš¾ä»¥æ£€æµ‹ç»†å¾®ç¼ºé™·å¹¶å¯èƒ½é”™è¯¯æ‹’ç»éå…¸å‹æ­£å¸¸æ ·æœ¬ï¼Œç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹éœ€è¦åœ¨ç±»åˆ«é€‚åº”æ€§å’Œåˆ¤åˆ«èƒ½åŠ›ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

**Method:** ABounDæ¡†æ¶æ•´åˆäº†è¯­ä¹‰æ¦‚å¿µå­¦ä¹ ä¸å†³ç­–è¾¹ç•Œå¡‘é€ ï¼ŒåŒ…å«åŠ¨æ€æ¦‚å¿µèåˆæ¨¡å—é€šè¿‡èåˆå¯æ³›åŒ–å…ˆéªŒä¸ç±»åˆ«ç‰¹å®šçº¿ç´¢ç”Ÿæˆç±»åˆ«è‡ªé€‚åº”æç¤ºï¼Œä»¥åŠå¯¹æŠ—æ€§è¾¹ç•Œé”»é€ æ¨¡å—é€šè¿‡PGDé£æ ¼æ‰°åŠ¨ç”Ÿæˆè¾¹ç•Œçº§å›´æ ç‰¹å¾æ¥å¡‘é€ ç²¾ç¡®å†³ç­–è¾¹ç•Œï¼Œé‡‡ç”¨å•é˜¶æ®µè®­ç»ƒå’Œæ¦‚å¿µ-è¾¹ç•ŒæŸå¤±å‡½æ•°ï¼Œå…¶ä¸­ABFæä¾›ä¸»è¦ç›‘ç£ä¿¡å·ï¼Œè¯­ä¹‰-ç©ºé—´æ­£åˆ™åŒ–å™¨ç¨³å®šä¼˜åŒ–è¿‡ç¨‹ã€‚

**Result:** åœ¨MVTec-ADå’ŒVisAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ ·æœ¬å¤šç±»åˆ«å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”Ÿæˆçš„å†³ç­–è¾¹ç•Œèƒ½å¤Ÿç´§å¯†è·Ÿéšæ­£å¸¸æ•°æ®åˆ†å¸ƒï¼ŒåŒæ—¶ä¿æŒçµæ´»æ€§å’Œé²æ£’çš„è¯­ä¹‰å¯¹é½èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†è¯­ä¹‰æ¦‚å¿µå­¦ä¹ ä¸å†³ç­–è¾¹ç•Œå¡‘é€ çš„ååŒä½œç”¨èƒ½å¤Ÿæœ‰æ•ˆè§£å†³å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä¸­çš„è¾¹ç•Œæ¨¡ç³Šé—®é¢˜ï¼Œå¯¹æŠ—æ€§è¾¹ç•Œé”»é€ æŠ€æœ¯ä¸ºç²¾ç¡®å†³ç­–è¾¹ç•Œå­¦ä¹ æä¾›äº†æœ‰æ•ˆç›‘ç£ä¿¡å·ï¼Œè¯¥ç»Ÿä¸€æ¡†æ¶ä¸ºå·¥ä¸šå¼‚å¸¸æ£€æµ‹ä¸­çš„ç±»åˆ«é€‚åº”æ€§å’Œåˆ¤åˆ«èƒ½åŠ›å¹³è¡¡æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Few-shot multi-class industrial anomaly detection remains a challenging task. Vision-language models need to be both category-adaptive and sharply discriminative, yet data scarcity often blurs the boundary between normal and abnormal states. This ambiguity leads to missed subtle defects and the rejection of atypical normal samples. We propose ABounD, an Adversarial Boundary-Driven few-shot learning for multi-class anomaly detection, which is a unified learning framework that integrates semantic concept learning with decision boundary shaping. The Dynamic Concept Fusion (DCF) module produces class-adaptive prompts by fusing generalizable priors with class-specific cues, conditioned on image features. Meanwhile, Adversarial Boundary Forging (ABF) sculpts a more precise decision margin by generating boundary-level fence features via PGD-style perturbations. Training is conducted in a single stage under a Concept-Boundary Loss, where ABF provides the main supervisory signal and semantic-spatial regularizers stabilize the optimization. This synergy yields a decision boundary that closely follows normal data while preserving flexibility and robust semantic alignment. Experiments on MVTec-AD and VisA datasets demonstrate state-of-the-art performance in the task of few-shot multi-class anomaly detection.


### [44] [Ovis-Image Technical Report](https://arxiv.org/abs/2511.22982)
*Guo-Hua Wang, Liangfu Cao, Tianyu Cui, Minghao Fu, Xiaohao Chen, Pengxin Zhan, Jianshan Zhao, Lan Li, Bowen Fu, Jiaqi Liu, Qing-Guo Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Ovis-Imageï¼Œä¸€ä¸ªä¸“é—¨é’ˆå¯¹é«˜è´¨é‡æ–‡æœ¬æ¸²æŸ“ä¼˜åŒ–çš„7Bå‚æ•°æ–‡ç”Ÿå›¾æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸¥æ ¼çš„è®¡ç®—çº¦æŸä¸‹é«˜æ•ˆè¿è¡Œï¼Œåœ¨æ–‡æœ¬æ¸²æŸ“æ€§èƒ½ä¸Šè¾¾åˆ°ä¸æ›´å¤§å¼€æºæ¨¡å‹ç›¸å½“çš„æ°´å¹³ï¼ŒåŒæ—¶ä¿æŒå•GPUå¯éƒ¨ç½²æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å‰æ²¿æ–‡æœ¬æ¸²æŸ“æ¨¡å‹é€šå¸¸è§„æ¨¡åºå¤§æˆ–ä¾èµ–é—­æºç³»ç»Ÿï¼Œå¯¼è‡´éƒ¨ç½²æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥åœ¨æœ‰é™è®¡ç®—èµ„æºä¸‹å®ç°é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ªç´§å‡‘é«˜æ•ˆçš„æ–‡ç”Ÿå›¾æ¨¡å‹ï¼Œä¸“é—¨ä¼˜åŒ–æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›ï¼Œç¼©å°å‰æ²¿æ–‡æœ¬æ¸²æŸ“æŠ€æœ¯ä¸å®é™…éƒ¨ç½²å¯è¡Œæ€§ä¹‹é—´çš„å·®è·ã€‚

**Method:** åŸºäºå…ˆå‰Ovis-U1æ¡†æ¶ï¼ŒOvis-Imageæ•´åˆäº†æ‰©æ•£å¼è§†è§‰è§£ç å™¨ä¸æ›´å¼ºçš„Ovis 2.5å¤šæ¨¡æ€éª¨å¹²ç½‘ç»œï¼Œé‡‡ç”¨ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„è®­ç»ƒæµç¨‹ï¼Œç»“åˆå¤§è§„æ¨¡é¢„è®­ç»ƒä¸ç²¾å¿ƒè®¾è®¡çš„åè®­ç»ƒç²¾è°ƒç­–ç•¥ï¼Œä¸“é—¨é’ˆå¯¹æ–‡æœ¬æ¸²æŸ“ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚

**Result:** å°½ç®¡æ¶æ„ç´§å‡‘ï¼ŒOvis-Imageåœ¨æ–‡æœ¬æ¸²æŸ“æ€§èƒ½ä¸Šä¸Qwen-Imageç­‰æ˜¾è‘—æ›´å¤§çš„å¼€æºæ¨¡å‹ç›¸å½“ï¼Œå¹¶æ¥è¿‘Seedreamå’ŒGPT4oç­‰é—­æºç³»ç»Ÿã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å•ä¸ªé«˜ç«¯GPUä¸Šéƒ¨ç½²ï¼Œä»…éœ€ä¸­ç­‰å†…å­˜éœ€æ±‚ï¼Œå®ç°äº†å‰æ²¿æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›ä¸å®ç”¨éƒ¨ç½²å¯è¡Œæ€§ä¹‹é—´çš„å¹³è¡¡ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œå°†å¼ºå¤§çš„å¤šæ¨¡æ€éª¨å¹²ç½‘ç»œä¸ç²¾å¿ƒè®¾è®¡çš„æ–‡æœ¬ä¸­å¿ƒåŒ–è®­ç»ƒæ–¹æ¡ˆç›¸ç»“åˆï¼Œè¶³ä»¥å®ç°å¯é çš„åŒè¯­æ–‡æœ¬æ¸²æŸ“ï¼Œæ— éœ€ä¾èµ–è¶…å¤§æ¨¡å‹æˆ–ä¸“æœ‰ç³»ç»Ÿã€‚è¿™ä¸€å‘ç°ä¸ºå¼€å‘é«˜æ•ˆå¯éƒ¨ç½²çš„æ–‡æœ¬æ¸²æŸ“æ¨¡å‹æä¾›äº†æ–°è·¯å¾„ï¼Œæœ‰åŠ©äºé™ä½é«˜è´¨é‡æ–‡æœ¬ç”Ÿæˆçš„æŠ€æœ¯é—¨æ§›ã€‚

---

#### ğŸ“„ Abstract
We introduce $\textbf{Ovis-Image}$, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models.


### [45] [Do You See What I Say? Generalizable Deepfake Detection based on Visual Speech Recognition](https://arxiv.org/abs/2511.22443)
*Maheswar Bora, Tashvik Dhamija, Shukesh Reddy, Baptiste Chopin, Pranav Balaji, Abhijit Das, Antitza Dantcheva*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒè§†è§‰è¯­éŸ³è¯†åˆ«ç‰¹å¾çš„æ·±åº¦ä¼ªé€ æ£€æµ‹ç½‘ç»œFauxNetï¼Œè¯¥ç½‘ç»œåœ¨é›¶æ ·æœ¬æ£€æµ‹è®¾ç½®ä¸­æŒç»­ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶èƒ½è¯†åˆ«è§†é¢‘çš„ç”Ÿæˆæ¥æºã€‚ç ”ç©¶è¿˜å‘å¸ƒäº†åŒ…å«çº¦38,000ä¸ªçœŸå®å’Œä¼ªé€ è§†é¢‘çš„Authenticaæ•°æ®é›†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ·±åº¦ä¼ªé€ ç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†é«˜åº¦é€¼çœŸçš„ç”Ÿæˆå›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ï¼ŒåŒæ—¶ä¹Ÿå¼•å‘äº†å…³äºæ“çºµåª’ä½“æ»¥ç”¨çš„ä¸¥é‡æ‹…å¿§ã€‚ä¸ºå‡è½»æ­¤ç±»æ»¥ç”¨ï¼Œè¿«åˆ‡éœ€è¦é²æ£’ä¸”å¯é çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é›¶æ ·æœ¬æ£€æµ‹ï¼ˆå³å¯æ³›åŒ–æ£€æµ‹ï¼‰è¿™ä¸€å…³é”®æŒ‘æˆ˜ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ·±åº¦ä¼ªé€ æ£€æµ‹ç½‘ç»œFauxNetï¼Œè¯¥ç½‘ç»œåŸºäºé¢„è®­ç»ƒçš„è§†è§‰è¯­éŸ³è¯†åˆ«ç‰¹å¾ã€‚é€šè¿‡ä»è§†é¢‘ä¸­æå–æ—¶åºVSRç‰¹å¾ï¼Œç³»ç»Ÿèƒ½å¤Ÿè¯†åˆ«å¹¶åˆ†ç¦»çœŸå®è§†é¢‘ä¸æ“çºµè§†é¢‘ã€‚ç ”ç©¶è¿˜åˆ›å»ºäº†åŒ…å«çº¦38,000ä¸ªçœŸå®å’Œä¼ªé€ è§†é¢‘çš„æ–°æ•°æ®é›†Authentica-Voxå’ŒAuthentica-HDTFï¼Œåè€…ä½¿ç”¨å…­ç§æœ€æ–°çš„æ·±åº¦ä¼ªé€ ç”ŸæˆæŠ€æœ¯åˆ›å»ºã€‚

**Result:** FauxNetåœ¨é›¶æ ·æœ¬æ£€æµ‹è®¾ç½®ä¸­æŒç»­ä¼˜äºç°æœ‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶èƒ½å¤Ÿè¿›è¡Œå±æ€§è¯†åˆ«â€”â€”åŒºåˆ†è§†é¢‘æ¥æºçš„ç”ŸæˆæŠ€æœ¯ã€‚åœ¨Authenticaæ•°æ®é›†å’ŒFaceForensics++ä¸Šçš„å¹¿æ³›åˆ†æå’Œç»“æœè¯æ˜äº†FauxNetçš„ä¼˜è¶Šæ€§ï¼ŒAuthenticaæ•°æ®é›†å°†å…¬å¼€æä¾›ä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜åŸºäºè§†è§‰è¯­éŸ³è¯†åˆ«ç‰¹å¾çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ä»…èƒ½å¤Ÿæ£€æµ‹ä¼ªé€ è§†é¢‘ï¼Œè¿˜èƒ½è¯†åˆ«å…¶ç”Ÿæˆæ¥æºã€‚å‘å¸ƒçš„Authenticaæ•°æ®é›†ä¸ºæ·±åº¦ä¼ªé€ æ£€æµ‹ç ”ç©¶æä¾›äº†æ–°çš„åŸºå‡†èµ„æºï¼Œæ¨åŠ¨äº†å¯æ³›åŒ–æ£€æµ‹æŠ€æœ¯çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Deepfake generation has witnessed remarkable progress, contributing to highly realistic generated images, videos, and audio. While technically intriguing, such progress has raised serious concerns related to the misuse of manipulated media. To mitigate such misuse, robust and reliable deepfake detection is urgently needed. Towards this, we propose a novel network FauxNet, which is based on pre-trained Visual Speech Recognition (VSR) features. By extracting temporal VSR features from videos, we identify and segregate real videos from manipulated ones. The holy grail in this context has to do with zero-shot detection, i.e., generalizable detection, which we focus on in this work. FauxNet consistently outperforms the state-of-the-art in this setting. In addition, FauxNet is able to attribute - distinguish between generation techniques from which the videos stem. Finally, we propose new datasets, referred to as Authentica-Vox and Authentica-HDTF, comprising about 38,000 real and fake videos in total, the latter created with six recent deepfake generation techniques. We provide extensive analysis and results on the Authentica datasets and FaceForensics++, demonstrating the superiority of FauxNet. The Authentica datasets will be made publicly available.


### [46] [From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning](https://arxiv.org/abs/2511.23031)
*Changpeng Wang, Haozhe Wang, Xi Chen, Junhan Liu, Taofeng Xue, Chong Peng, Donglian Qi, Fangzhen Lin, Yunfeng Yan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºè§†è§‰ç†æ€§å­¦ä¹ ï¼ˆViRLï¼‰èŒƒå¼ï¼Œå°†è§†è§‰åŠ¨ä½œé‡æ„ä¸ºæ ¸å¿ƒæ¨ç†åŸè¯­è€Œéå¯é€‰å·¥å…·ï¼Œé€šè¿‡ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹åŸºäºè§†è§‰è¯æ®è¿›è¡Œæ¨ç†ï¼Œåœ¨æ„ŸçŸ¥ã€å¹»è§‰å’Œæ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨ç†æ¡†æ¶å°†è§†è§‰åŠ¨ä½œè§†ä¸ºå¯é€‰å·¥å…·ï¼Œå¯¼è‡´æ¨¡å‹æ¨ç†ç¼ºä¹è§†è§‰åŸºç¡€ï¼Œäº§ç”Ÿ"å›¾åƒæ€è€ƒå¹»è§‰"â€”â€”æ¨¡å‹çœ‹ä¼¼åŸºäºè§†è§‰è¯æ®ï¼Œå®åˆ™ä¾èµ–ä¸ä¸Šä¸‹æ–‡æ— å…³çš„åŠ¨ä½œï¼Œæ—¢æœªç²¾ç‚¼æ„ŸçŸ¥ä¹Ÿæœªå¼•å¯¼æ¨ç†èµ°å‘æ­£ç¡®ç­”æ¡ˆã€‚

**Method:** æå‡ºè§†è§‰ç†æ€§å­¦ä¹ ï¼ˆViRLï¼‰èŒƒå¼ï¼Œå°†è§†è§‰åŠ¨ä½œé‡æ„ä¸ºè§†è§‰ç†æ€§åŒ–ï¼ˆè§†è§‰ç±»æ¯”äºæ–‡æœ¬æ€ç»´é“¾ï¼‰çš„æ ¸å¿ƒæ¨ç†åŸè¯­ã€‚ViRLåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šåŸºäºçœŸå®ç†æ€§çš„è¿‡ç¨‹ç›‘ç£ã€é€šè¿‡æ­¥éª¤çº§å¥–åŠ±å¡‘å½¢çš„ç›®æ ‡å¯¹é½ï¼Œä»¥åŠåŒºåˆ†æ­£ç¡®ã€å†—ä½™å’Œé”™è¯¯åŠ¨ä½œçš„ç»†ç²’åº¦ä¿¡ç”¨åˆ†é…æœºåˆ¶ã€‚

**Result:** é€šè¿‡çº¯ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒViRLåœ¨æ¶µç›–æ„ŸçŸ¥ã€å¹»è§‰å’Œæ¨ç†çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯äº†è§†è§‰ç†æ€§åŒ–èŒƒå¼åœ¨æå‡æ¨¡å‹é€æ˜åº¦å’Œå¯éªŒè¯æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶ç¡®ç«‹äº†è§†è§‰ç†æ€§åŒ–ä½œä¸ºä»»åŠ¡æ— å…³ã€è¿‡ç¨‹åŸºç¡€çš„èŒƒå¼ï¼Œä¸ºæ„å»ºé€æ˜ã€å¯éªŒè¯ä¸”å¯ä¿¡èµ–çš„è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†æ–°æ–¹å‘ï¼Œç¡®ä¿æ¯ä¸ªè§†è§‰åŠ¨ä½œéƒ½èƒ½æœ‰æ„ä¹‰åœ°è´¡çŒ®äºæ¨ç†é“¾ï¼Œä½¿æ¨¡å‹"åŸºäºæ­£ç¡®çš„è§†è§‰ç†ç”±å¾—å‡ºæ­£ç¡®ç­”æ¡ˆ"ã€‚

---

#### ğŸ“„ Abstract
Recent advances in vision-language reasoning underscore the importance of thinking with images, where models actively ground their reasoning in visual evidence. Yet, prevailing frameworks treat visual actions as optional tools, boosting metrics but leaving reasoning ungrounded and crops ineffective. This gap gives rise to the illusion of thinking with images: models seem visually grounded but rely on context-agnostic actions that neither refine perception nor guide reasoning toward correct answers. We address this problem by reframing visual actions as core reasoning primitives rather than optional tools, which we term visual rationalization, the visual analogue of textual Chain-of-Thought. Building on this insight, we propose Visual Rationale Learning (ViRL), an end-to-end paradigm that grounds training in the visual rationale itself. ViRL integrates (1) Process Supervision with ground-truth rationales, (2) Objective Alignment via step-level reward shaping, and (3) Fine-Grained Credit Assignment to distinguish correct, redundant, and erroneous actions. By ensuring each action contributes meaningfully to the reasoning chain, ViRL enables models to "get the right answer for the right visual reason". Trained purely with end-to-end RL, ViRL achieves state-of-the-art results across benchmarks spanning perception, hallucination, and reasoning. This work establishes visual rationalization as a task-agnostic, process-grounded paradigm for building transparent, verifiable, and trustworthy vision-language models.


### [47] [Beyond Real versus Fake Towards Intent-Aware Video Analysis](https://arxiv.org/abs/2511.22455)
*Saurabh Atreya, Nabyl Quignon, Baptiste Chopin, Abhijit Das, Antitza Dantcheva*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†IntentHQåŸºå‡†ï¼Œå°†æ·±åº¦ä¼ªé€ æ£€æµ‹èŒƒå¼ä»çœŸå®æ€§éªŒè¯è½¬å‘æ„å›¾åˆ†æï¼Œé€šè¿‡å¤šæ¨¡æ€æ¨¡å‹è¯†åˆ«è§†é¢‘èƒŒåçš„23ç§ç»†ç²’åº¦æ„å›¾ç±»åˆ«ï¼Œä¸ºç†è§£æ“çºµè§†é¢‘çš„åŠ¨æœºæä¾›äº†æ–°æ¡†æ¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€ç”Ÿæˆæ¨¡å‹å¿«é€Ÿå‘å±•ï¼Œæ·±åº¦ä¼ªé€ è§†é¢‘æ—¥ç›Šé€¼çœŸï¼Œå¸¦æ¥ä¸¥é‡ç¤¾ä¼šå’Œå®‰å…¨é£é™©ã€‚ç°æœ‰æ£€æµ‹æ–¹æ³•ä»…å…³æ³¨åŒºåˆ†çœŸå‡è§†é¢‘ï¼Œæœªèƒ½è§£å†³ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼šæ“çºµè§†é¢‘èƒŒåçš„æ„å›¾æ˜¯ä»€ä¹ˆï¼Ÿæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œå°†ç ”ç©¶èŒƒå¼ä»çœŸå®æ€§éªŒè¯è½¬å‘è§†é¢‘çš„ä¸Šä¸‹æ–‡ç†è§£ã€‚

**Method:** ç ”ç©¶æå‡ºäº†IntentHQåŸºå‡†ï¼ŒåŒ…å«5168ä¸ªç»è¿‡ç²¾å¿ƒæ”¶é›†å’Œæ ‡æ³¨çš„è§†é¢‘ï¼Œæ¶µç›–23ç§ç»†ç²’åº¦æ„å›¾ç±»åˆ«ã€‚é‡‡ç”¨ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ•´åˆæ—¶ç©ºè§†é¢‘ç‰¹å¾ã€éŸ³é¢‘å¤„ç†å’Œæ–‡æœ¬åˆ†æï¼Œä»¥æ¨æ–­è§†é¢‘èƒŒåçš„æ½œåœ¨åŠ¨æœºå’Œç›®æ ‡ã€‚æå‡ºçš„æ¨¡å‹ç»è¿‡ä¼˜åŒ–ï¼Œèƒ½å¤ŸåŒºåˆ†å¹¿æ³›çš„æ„å›¾ç±»åˆ«ã€‚

**Result:** ç ”ç©¶æ„å»ºäº†åŒ…å«5168ä¸ªè§†é¢‘çš„IntentHQåŸºå‡†æ•°æ®é›†ï¼Œæ ‡æ³¨äº†åŒ…æ‹¬"é‡‘èæ¬ºè¯ˆ"ã€"é—´æ¥è¥é”€"ã€"æ”¿æ²»å®£ä¼ "å’Œ"ææƒ§ç…½åŠ¨"åœ¨å†…çš„23ç§æ„å›¾ç±»åˆ«ã€‚é€šè¿‡å¤šæ¨¡æ€æ¨¡å‹å®ç°äº†å¯¹è§†é¢‘æ„å›¾çš„è¯†åˆ«ï¼Œä¸ºäººç±»ä¸­å¿ƒæ„å›¾åˆ†ææä¾›äº†æ–°çš„è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†ã€‚

**Conclusion:** è¯¥ç ”ç©¶å°†æ·±åº¦ä¼ªé€ æ£€æµ‹ä»ç®€å•çš„çœŸå®æ€§åˆ¤æ–­æå‡åˆ°æ„å›¾ç†è§£å±‚é¢ï¼Œä¸ºç†è§£æ“çºµè§†é¢‘çš„ç¤¾ä¼šå½±å“æä¾›äº†æ›´å…¨é¢çš„æ¡†æ¶ã€‚IntentHQåŸºå‡†ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦æ•°æ®é›†å’Œè¯„ä¼°æ ‡å‡†ï¼Œæ¨åŠ¨äº†è§†é¢‘å†…å®¹åˆ†æå‘æ›´ç»†ç²’åº¦çš„æ„å›¾è¯†åˆ«æ–¹å‘å‘å±•ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
The rapid advancement of generative models has led to increasingly realistic deepfake videos, posing significant societal and security risks. While existing detection methods focus on distinguishing real from fake videos, such approaches fail to address a fundamental question: What is the intent behind a manipulated video? Towards addressing this question, we introduce IntentHQ: a new benchmark for human-centered intent analysis, shifting the paradigm from authenticity verification to contextual understanding of videos. IntentHQ consists of 5168 videos that have been meticulously collected and annotated with 23 fine-grained intent-categories, including "Financial fraud", "Indirect marketing", "Political propaganda", as well as "Fear mongering". We perform intent recognition with supervised and self-supervised multi-modality models that integrate spatio-temporal video features, audio processing, and text analysis to infer underlying motivations and goals behind videos. Our proposed model is streamlined to differentiate between a wide range of intent-categories.


### [48] [SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2511.23075)
*Ruosen Zhao, Zhikang Zhang, Jialei Xu, Jiahao Chang, Dong Chen, Lingyun Li, Weijian Sun, Zizhuang Wei*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†SpaceMindï¼Œä¸€ç§ä¸“ä¸ºç©ºé—´æ¨ç†è®¾è®¡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œä»…ä»RGBè¾“å…¥å³å¯å®ç°3Dç©ºé—´ç†è§£ã€‚è¯¥æ¨¡å‹é€šè¿‡ç›¸æœºå¼•å¯¼çš„æ¨¡æ€èåˆæ¨¡å—ï¼Œå°†ç›¸æœºè¡¨ç¤ºä½œä¸ºä¸»åŠ¨å¼•å¯¼æ¨¡æ€è€Œéè¢«åŠ¨å…ƒæ•°æ®ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨3Dç©ºé—´æ¨ç†ï¼ˆå¦‚è·ç¦»ä¼°è®¡ã€å°ºå¯¸æ¯”è¾ƒå’Œè·¨è§†å›¾ä¸€è‡´æ€§ï¼‰æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚ç°æœ‰3Dæ„ŸçŸ¥æ–¹æ³•è¦ä¹ˆä¾èµ–è¾…åŠ©3Dä¿¡æ¯ï¼Œè¦ä¹ˆä»…é€šè¿‡æµ…å±‚ç‰¹å¾èåˆå¢å¼ºRGB-onlyæ¨¡å‹ï¼Œç¼ºä¹å¯¹ç›¸æœºè¡¨ç¤ºçš„ä¸»åŠ¨åˆ©ç”¨ã€‚

**Method:** SpaceMindé‡‡ç”¨åŒç¼–ç å™¨æ¶æ„ï¼Œé›†æˆVGGTä½œä¸ºç©ºé—´ç†è§£ç¼–ç å™¨å’ŒInternViTä½œä¸º2Dè§†è§‰ç¼–ç å™¨ã€‚æ ¸å¿ƒåˆ›æ–°æ˜¯å¼•å…¥è½»é‡çº§ç›¸æœºå¼•å¯¼æ¨¡æ€èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è¯­è¨€æ¨¡å‹ä¹‹å‰æ›¿æ¢æµ…å±‚èåˆï¼Œé€šè¿‡ç›¸æœºæ¡ä»¶åç½®å¤„ç†ç©ºé—´æ ‡è®°ï¼Œåˆ†é…åæ˜ å‡ ä½•é‡è¦æ€§çš„æŸ¥è¯¢æ— å…³æƒé‡ï¼Œå¹¶ä½¿ç”¨ç›¸æœºåµŒå…¥é—¨æ§èåˆè¡¨ç¤ºã€‚

**Result:** SpaceMindåœ¨VSI-Benchã€SQA3Då’ŒSPBenchåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚åœ¨VSI-Benchå’ŒSPBenchä¸Šå¤§å¹…è¶…è¶Šå¼€æºå’Œä¸“æœ‰ç³»ç»Ÿï¼Œåœ¨SQA3Dä¸Šä¹Ÿè¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¯æ˜äº†ç›¸æœºå¼•å¯¼æ¨¡æ€èåˆçš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ç›¸æœºå¼•å¯¼çš„æ¨¡æ€èåˆæ˜¯ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹æä¾›çœŸæ­£ç©ºé—´åŸºç¡€æ™ºèƒ½çš„æœ‰æ•ˆä¸”å®ç”¨çš„å½’çº³åç½®ã€‚è¯¥æ–¹æ³•ä»…éœ€RGBè¾“å…¥å³å¯å®ç°å¼ºå¤§çš„3Dç©ºé—´æ¨ç†ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦æ–¹å‘ï¼Œä½œè€…å°†å‘å¸ƒä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä»¥æ”¯æŒåç»­å·¥ä½œã€‚

---

#### ğŸ“„ Abstract
Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.


### [49] [RoadSceneBench: A Lightweight Benchmark for Mid-Level Road Scene Understanding](https://arxiv.org/abs/2511.22466)
*Xiyan Liu, Han Wang, Yuhu Wang, Junjie Cai, Zhe Cao, Jianzhong Yang, Zhen Lu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†RoadSceneBenchåŸºå‡†å’ŒHRRP-Tè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨é©¾é©¶åŸºå‡†ç¼ºä¹é“è·¯æ‹“æ‰‘å’ŒåŠ¨æ€åœºæ™¯ç»“æ„æ¨ç†èƒ½åŠ›çš„é—®é¢˜ï¼Œé€šè¿‡å¼ºè°ƒå…³ç³»ç†è§£å’Œç»“æ„ä¸€è‡´æ€§æ¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é“è·¯ç¯å¢ƒä¸­çš„æ¨ç†å¯é æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è‡ªåŠ¨é©¾é©¶åŸºå‡†ä¸»è¦å…³æ³¨æ£€æµ‹æˆ–åˆ†å‰²ç­‰æ„ŸçŸ¥ä»»åŠ¡ï¼Œå¿½è§†äº†æ¨ç†é“è·¯æ‹“æ‰‘å’ŒåŠ¨æ€åœºæ™¯ç»“æ„æ‰€éœ€çš„èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹ç†è§£è¿æ¥ä½å±‚æ„ŸçŸ¥ä¸é«˜å±‚è§„åˆ’çš„ä¸­å±‚é“è·¯è¯­ä¹‰çš„èƒ½åŠ›ï¼Œè€Œä¸­å±‚é“è·¯è¯­ä¹‰å¯¹äºå¯é çš„è‡ªåŠ¨é©¾é©¶å’Œæ•°å­—åœ°å›¾æ„å»ºè‡³å…³é‡è¦ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†RoadSceneBenchåŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä½†ä¿¡æ¯ä¸°å¯Œçš„åŸºå‡†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤æ‚é“è·¯ç¯å¢ƒä¸­çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼›åŒæ—¶æå‡ºäº†åˆ†å±‚å…³ç³»å¥–åŠ±ä¼ æ’­ä¸æ—¶åºä¸€è‡´æ€§æ¡†æ¶ï¼Œè¯¥è®­ç»ƒæ¡†æ¶é€šè¿‡è‡ªé€‚åº”å¥–åŠ±ä¿¡å·ä¿ƒè¿›ç©ºé—´ä¸€è‡´æ€§å’Œè¯­ä¹‰å¯¹é½ï¼Œä½¿è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå®ç°å‡ ä½•æ„ŸçŸ¥å’Œæ—¶åºä¸€è‡´çš„æ¨ç†ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šç§é“è·¯é…ç½®ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒRoadSceneBenchåŸºå‡†ä¸ºç ”ç©¶ä¸­å±‚é“è·¯è¯­ä¹‰å’ŒåŸ¹å…»ç»“æ„æ„ŸçŸ¥çš„è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥æä¾›äº†ç´§å‡‘è€Œå¼ºå¤§çš„åŸºç¡€ï¼Œç›¸å…³æ•°æ®é›†å·²åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†å…³ç³»ç†è§£å’Œç»“æ„ä¸€è‡´æ€§åœ¨è‡ªåŠ¨é©¾é©¶æ¨ç†ä¸­çš„é‡è¦æ€§ï¼Œæå‡ºçš„åŸºå‡†å’Œè®­ç»ƒæ¡†æ¶ä½¿æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šé™æ€è¯†åˆ«ï¼Œå®ç°å‡ ä½•æ„ŸçŸ¥å’Œæ—¶åºä¸€è‡´çš„æ¨ç†ï¼Œä¸ºç»“æ„æ„ŸçŸ¥çš„è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€ï¼Œå¹¶æ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é“è·¯åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
Understanding mid-level road semantics, which capture the structural and contextual cues that link low-level perception to high-level planning, is essential for reliable autonomous driving and digital map construction. However, existing benchmarks primarily target perception tasks such as detection or segmentation, overlooking the reasoning capabilities required to infer road topology and dynamic scene structure. To address this gap, we present RoadSceneBench, a lightweight yet information-rich benchmark designed to evaluate and advance visual reasoning in complex road environments. Unlike large-scale perception datasets, RoadSceneBench emphasizes relational understanding and structural consistency, encouraging models to capture the underlying logic of real-world road scenes. Furthermore, to enhance reasoning reliability, we propose Hierarchical Relational Reward Propagation with Temporal Consistency (HRRP-T), a training framework for Vision-Language Models (VLMs) in which reward signals adaptively promote spatial coherence and semantic alignment throughout the reasoning process. This paradigm enables models to move beyond static recognition toward geometry-aware and temporally consistent reasoning. Extensive experiments demonstrate that our method achieves state-of-the-art performance across diverse road configurations. RoadSceneBench thus provides a compact yet powerful foundation for studying mid-level road semantics and fostering structure-aware autonomous perception. Our dataset is available at https://github.com/XiyanLiu/RoadSceneBench.


### [50] [Hybrid, Unified and Iterative: A Novel Framework for Text-based Person Anomaly Retrieval](https://arxiv.org/abs/2511.22470)
*Tien-Huy Nguyen, Huu-Loc Tran, Huu-Phong Phan-Nguyen, Quang-Vinh Dinh*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ–‡æœ¬è¡Œäººå¼‚å¸¸æ£€ç´¢çš„å±€éƒ¨-å…¨å±€æ··åˆè§†è§’æ¨¡å—ï¼Œç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’Œç»Ÿä¸€å›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼Œé€šè¿‡è¿­ä»£é›†æˆç­–ç•¥å’Œç‰¹å¾é€‰æ‹©ç®—æ³•æ˜¾è‘—æå‡äº†ç»†ç²’åº¦ç‰¹å¾æå–èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºæ–‡æœ¬çš„è¡Œäººå¼‚å¸¸æ£€ç´¢æ–¹æ³•å¤§å¤šä¾èµ–å¤æ‚çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå­˜åœ¨ç»†ç²’åº¦ç‰¹å¾æå–ä¸è¶³çš„é—®é¢˜ï¼Œéœ€è¦æ¢ç´¢å¦‚ä½•ä¼˜åŒ–æ¨¡å‹ä»¥è·å¾—æ›´ç²¾ç»†çš„ç‰¹å¾è¡¨ç¤ºã€‚

**Method:** æå‡ºäº†å±€éƒ¨-å…¨å±€æ··åˆè§†è§’æ¨¡å—ä¸è§†è§‰è¯­è¨€æ¨¡å‹é›†æˆï¼Œå¼€å‘äº†ç»Ÿä¸€å›¾åƒ-æ–‡æœ¬æ¨¡å‹ç»“åˆITCã€ITMã€MLMå’ŒMIMå¤šç›®æ ‡æŸå¤±å‡½æ•°ï¼Œè®¾è®¡äº†æ–°é¢–çš„è¿­ä»£é›†æˆç­–ç•¥è€Œéä¼ ç»Ÿå¹¶è¡Œé›†æˆï¼Œå¹¶åŸºäºLHPæ¨¡å‹æŒ‡å¯¼æå‡ºäº†ç‰¹å¾é€‰æ‹©ç®—æ³•ã€‚

**Result:** åœ¨PABæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸æ¯”å…ˆå‰å·¥ä½œï¼ŒR@1æå‡äº†9.70%ï¼ŒR@5æå‡äº†1.77%ï¼ŒR@10æå‡äº†1.01%ï¼Œè¯æ˜äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ç»“åˆç»†ç²’åº¦ä¸ç²—ç²’åº¦ç‰¹å¾çš„å±€éƒ¨-å…¨å±€æ··åˆè§†è§’èƒ½æ˜¾è‘—æå‡æ–‡æœ¬è¡Œäººå¼‚å¸¸æ£€ç´¢æ€§èƒ½ï¼Œè¿­ä»£é›†æˆç­–ç•¥å’ŒåŸºäºæ¨¡å‹æŒ‡å¯¼çš„ç‰¹å¾é€‰æ‹©ç®—æ³•ä¸ºç›¸å…³ä»»åŠ¡æä¾›äº†æ–°çš„ä¼˜åŒ–æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Text-based person anomaly retrieval has emerged as a challenging task, with most existing approaches relying on complex deep-learning techniques. This raises a research question: How can the model be optimized to achieve greater fine-grained features? To address this, we propose a Local-Global Hybrid Perspective (LHP) module integrated with a Vision-Language Model (VLM), designed to explore the effectiveness of incorporating both fine-grained features alongside coarse-grained features. Additionally, we investigate a Unified Image-Text (UIT) model that combines multiple objective loss functions, including Image-Text Contrastive (ITC), Image-Text Matching (ITM), Masked Language Modeling (MLM), and Masked Image Modeling (MIM) loss. Beyond this, we propose a novel iterative ensemble strategy, by combining iteratively instead of using model results simultaneously like other ensemble methods. To take advantage of the superior performance of the LHP model, we introduce a novel feature selection algorithm based on its guidance, which helps improve the model's performance. Extensive experiments demonstrate the effectiveness of our method in achieving state-of-the-art (SOTA) performance on PAB dataset, compared with previous work, with a 9.70\% improvement in R@1, 1.77\% improvement in R@5, and 1.01\% improvement in R@10.


### [51] [REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection](https://arxiv.org/abs/2511.23158)
*Huangsen Cao, Qin Mei, Zhiheng Li, Yuxi Li, Ying Zhang, Chen Li, Zhimeng Zhang, Xin Ding, Yongwei Wang, Jing Lyu, Fei Wu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†REVEAL-BenchåŸºå‡†å’ŒREVEALæ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºè¯æ®é“¾æ¨ç†çš„å¤šæ¨¡æ€AIç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡ä¸“å®¶æ¨¡å‹é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ æœºåˆ¶ï¼Œåœ¨æå‡æ£€æµ‹å‡†ç¡®æ€§çš„åŒæ—¶ç”Ÿæˆå¯éªŒè¯çš„è§£é‡Šæ€§æ¨ç†è¿‡ç¨‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼ŒAIç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒè¶Šæ¥è¶Šéš¾ä»¥åŒºåˆ†ï¼Œå¯¹ä¿¡æ¯å®Œæ•´æ€§æ„æˆä¸¥é‡å¨èƒã€‚ç°æœ‰å¯è§£é‡Šå–è¯æ–¹æ³•ä¸»è¦ä¾èµ–äº‹ååˆç†åŒ–æˆ–è§†è§‰åˆ¤åˆ«ï¼Œç¼ºä¹å¯éªŒè¯çš„è¯æ®é“¾ï¼Œå¯¼è‡´è§£é‡Šç¼ºä¹å› æœåŸºç¡€ä¸”æ³›åŒ–èƒ½åŠ›å·®ï¼Œè¿«åˆ‡éœ€è¦å»ºç«‹åŸºäºè¯æ®é“¾çš„å¯éªŒè¯å–è¯æ¡†æ¶ã€‚

**Method:** ç ”ç©¶æå‡ºäº†REVEAL-BenchåŸºå‡†ï¼Œè¿™æ˜¯é¦–ä¸ªå›´ç»•è¯æ®é“¾æ„å»ºçš„æ¨ç†å¢å¼ºå¤šæ¨¡æ€AIç”Ÿæˆå›¾åƒæ£€æµ‹åŸºå‡†ï¼ŒåŸºäºå¤šä¸ªè½»é‡çº§ä¸“å®¶æ¨¡å‹ç”Ÿæˆé€æ­¥æ¨ç†è½¨è¿¹å’Œè¯æ®ä¾æ®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†REVEALæ¡†æ¶ï¼Œé‡‡ç”¨ä¸“å®¶é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå…¶å¥–åŠ±æœºåˆ¶ä¸“é—¨è®¾è®¡ç”¨äºè”åˆä¼˜åŒ–æ£€æµ‹å‡†ç¡®æ€§ã€è§£é‡Šä¿çœŸåº¦å’ŒåŸºäºæ˜ç¡®å–è¯è¯æ®çš„é€»è¾‘è¿è´¯æ€§ã€‚

**Result:** å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒREVEALåœ¨æ£€æµ‹å‡†ç¡®æ€§ã€è§£é‡Šä¿çœŸåº¦å’Œè·¨æ¨¡å‹æ³›åŒ–é²æ£’æ€§æ–¹é¢å‡æ˜¾è‘—æå‡ï¼Œä¸ºå¯è§£é‡Šå›¾åƒå–è¯å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›åŸºå‡†ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆç»†ç²’åº¦ã€å¯è§£é‡Šä¸”å¯éªŒè¯çš„æ¨ç†é“¾ï¼ŒåŒæ—¶å®ç°ä¼˜å¼‚çš„æ£€æµ‹æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥åŸºäºè¯æ®é“¾çš„æ¨ç†å¢å¼ºæ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰å¯è§£é‡Šå–è¯æ–¹æ³•ç¼ºä¹å¯éªŒè¯æ€§çš„å…³é”®å±€é™ï¼Œä¸ºAIç”Ÿæˆå›¾åƒæ£€æµ‹æä¾›äº†æ–°çš„èŒƒå¼ã€‚REVEALæ¡†æ¶ä¸ä»…æå‡äº†æ£€æµ‹æ€§èƒ½ï¼Œæ›´é‡è¦çš„æ˜¯å»ºç«‹äº†å¯éªŒè¯çš„è§£é‡Šæœºåˆ¶ï¼Œä¸ºæ„å»ºå¯ä¿¡èµ–çš„å–è¯ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ï¼Œæ¨åŠ¨äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½åœ¨å›¾åƒå–è¯é¢†åŸŸçš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
With the rapid advancement of generative models, visually realistic AI-generated images have become increasingly difficult to distinguish from authentic ones, posing severe threats to social trust and information integrity. Consequently, there is an urgent need for efficient and truly explainable image forensic methods. Recent detection paradigms have shifted towards explainable forensics. However, state-of-the-art approaches primarily rely on post-hoc rationalizations or visual discrimination, lacking a verifiable chain of evidence. This reliance on surface-level pattern matching limits the generation of causally grounded explanations and often results in poor generalization. To bridge this critical gap, we introduce \textbf{REVEAL-Bench}, the first reasoning-enhanced multimodal benchmark for AI-generated image detection that is explicitly structured around a chain-of-evidence derived from multiple lightweight expert models, then records step-by-step reasoning traces and evidential justifications. Building upon this dataset, we propose \textbf{REVEAL} (\underline{R}easoning-\underline{e}nhanced Forensic E\underline{v}id\underline{e}nce \underline{A}na\underline{l}ysis), an effective and explainable forensic framework that integrates detection with a novel expert-grounded reinforcement learning. Our reward mechanism is specially tailored to jointly optimize detection accuracy, explanation fidelity, and logical coherence grounded in explicit forensic evidence, enabling REVEAL to produce fine-grained, interpretable, and verifiable reasoning chains alongside its detection outcomes. Extensive experimental results demonstrate that REVEAL significantly enhances detection accuracy, explanation fidelity, and robust cross-model generalization, benchmarking a new state of the art for explainable image forensics.


### [52] [Text Condition Embedded Regression Network for Automated Dental Abutment Design](https://arxiv.org/abs/2511.22578)
*Mianjie Zheng, Xinquan Yang, Xuguang Li, Xiaoling Luo, Xuefen Liu, Kun Tang, He Meng, Linlin Shen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–‡æœ¬æ¡ä»¶åµŒå…¥çš„ç‰™ç§‘ç§æ¤ä½“åŸºå°è®¾è®¡æ¡†æ¶ï¼ˆTCEADï¼‰ï¼Œé€šè¿‡å¼•å…¥æ–‡æœ¬å¼•å¯¼å®šä½æ¨¡å—å’Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå®ç°äº†ç‰™ç§‘åŸºå°è®¾è®¡çš„è‡ªåŠ¨åŒ–ï¼Œæ˜¾è‘—æé«˜äº†å®šä½ç²¾åº¦å’Œè®¾è®¡æ•ˆç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿç‰™ç§‘ç§æ¤ä½“åŸºå°è®¾è®¡è¿‡ç¨‹è€—æ—¶è€—åŠ›ï¼Œä¸”é•¿æœŸä½¿ç”¨ä¸åˆé€‚çš„åŸºå°å¯èƒ½å¯¼è‡´ç§æ¤ä½“å‘¨å›´ç‚ç­‰å¹¶å‘ç—‡ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆï¼Œä¸”åŸºå°åŒºåŸŸå®šä½ç²¾åº¦ä¸è¶³ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿå¿«é€Ÿå®šä½å¹¶è®¾è®¡é€‚é…åŸºå°çš„æ™ºèƒ½æ–¹æ³•ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†æ–‡æœ¬æ¡ä»¶åµŒå…¥åŸºå°è®¾è®¡æ¡†æ¶ï¼ˆTCEADï¼‰ï¼Œæ‰©å±•äº†ç½‘æ ¼æ©ç è‡ªç¼–ç å™¨ï¼ˆMeshMAEï¼‰çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œå¼•å…¥äº†æ–‡æœ¬å¼•å¯¼å®šä½ï¼ˆTGLï¼‰æ¨¡å—ã€‚è¯¥æ¡†æ¶é€šè¿‡CLIPæ–‡æœ¬ç¼–ç å™¨å¼•å…¥åŸºå°åŒºåŸŸæè¿°ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿå¿«é€Ÿå®šä½åŸºå°åŒºåŸŸï¼Œå¹¶åˆ©ç”¨å£è…”æ‰«ææ•°æ®é¢„è®­ç»ƒç¼–ç å™¨ä»¥å¢å¼ºå±€éƒ¨ç»†ç²’åº¦ç‰¹å¾æå–èƒ½åŠ›ã€‚

**Result:** åœ¨å¤§å‹åŸºå°è®¾è®¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTCEADç›¸æ¯”å…¶ä»–ä¸»æµæ–¹æ³•åœ¨äº¤å¹¶æ¯”ï¼ˆIoUï¼‰æŒ‡æ ‡ä¸Šæå‡äº†0.8%-12.85%ï¼ŒéªŒè¯äº†å…¶åœ¨è‡ªåŠ¨åŒ–ç‰™ç§‘åŸºå°è®¾è®¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚æ–‡æœ¬å¼•å¯¼å®šä½æ¨¡å—æœ‰æ•ˆæé«˜äº†åŸºå°åŒºåŸŸçš„å®šä½ç²¾åº¦ã€‚

**Conclusion:** TCEADæ¡†æ¶å±•ç¤ºäº†æ–‡æœ¬å¼•å¯¼å®šä½ä¸è‡ªç›‘ç£å­¦ä¹ åœ¨ç‰™ç§‘åŸºå°è®¾è®¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè‡ªåŠ¨åŒ–ç‰™ç§‘ä¿®å¤ä½“è®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†è®¾è®¡æ•ˆç‡ï¼Œè¿˜é€šè¿‡ç²¾å‡†å®šä½å‡å°‘äº†å¹¶å‘ç—‡é£é™©ï¼Œå…·æœ‰ä¸´åºŠè½¬åŒ–æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
The abutment is an important part of artificial dental implants, whose design process is time-consuming and labor-intensive. Long-term use of inappropriate dental implant abutments may result in implant complications, including peri-implantitis. Using artificial intelligence to assist dental implant abutment design can quickly improve the efficiency of abutment design and enhance abutment adaptability. In this paper, we propose a text condition embedded abutment design framework (TCEAD), the novel automated abutment design solution available in literature. The proposed study extends the self-supervised learning framework of the mesh mask autoencoder (MeshMAE) by introducing a text-guided localization (TGL) module to facilitate abutment area localization. As the parameter determination of the abutment is heavily dependent on local fine-grained features (the width and height of the implant and the distance to the opposing tooth), we pre-train the encoder using oral scan data to improve the model's feature extraction ability. Moreover, considering that the abutment area is only a small part of the oral scan data, we designed a TGL module, which introduces the description of the abutment area through the text encoder of Contrastive Language-Image Pre-training (CLIP), enabling the network to quickly locate the abutment area. We validated the performance of TCEAD on a large abutment design dataset. Extensive experiments demonstrate that TCEAD achieves an Intersection over Union (IoU) improvement of 0.8%-12.85% over other mainstream methods, underscoring its potential in automated dental abutment design.


### [53] [AnoRefiner: Anomaly-Aware Group-Wise Refinement for Zero-Shot Industrial Anomaly Detection](https://arxiv.org/abs/2511.22595)
*Dayou Huang, Feng Xue, Xurui Li, Yu Zhou*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºAnoRefinerï¼Œä¸€ç§å¯æ’å…¥å¼å¼‚å¸¸æ„ŸçŸ¥ç»†åŒ–å™¨ï¼Œé€šè¿‡åˆ©ç”¨å¼‚å¸¸åˆ†æ•°å›¾çš„äº’è¡¥ç©ºé—´çº¿ç´¢ï¼Œå°†é›¶æ ·æœ¬å·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„è¡¥ä¸çº§å¼‚å¸¸å›¾æå‡è‡³åƒç´ çº§ç²¾åº¦ï¼Œæ˜¾è‘—æå‡äº†æ£€æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é›¶æ ·æœ¬å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ–¹æ³•é€šå¸¸åªèƒ½ç”Ÿæˆç²—ç³™çš„è¡¥ä¸çº§å¼‚å¸¸å›¾ï¼Œç°æœ‰æ–¹æ³•å°è¯•ä½¿ç”¨åˆæˆå¼‚å¸¸æ•°æ®è¿›è¡Œç»†åŒ–ä½†ä»éš¾ä»¥å‡†ç¡®æ¢å¤ç»†ç²’åº¦å¼‚å¸¸ï¼Œä¸»è¦å› ä¸ºåˆæˆå¼‚å¸¸ä¸çœŸå®å¼‚å¸¸ä¹‹é—´å­˜åœ¨å·®è·ã€‚ç ”ç©¶å‘ç°å¼‚å¸¸åˆ†æ•°å›¾æä¾›äº†å›¾åƒç‰¹å¾ä¸­ç¼ºä¹çš„äº’è¡¥ç©ºé—´çº¿ç´¢ï¼Œè¿™ä¸€äº‹å®å…ˆå‰è¢«å¿½è§†ã€‚

**Method:** æå‡ºAnoRefineræ¡†æ¶ï¼ŒåŒ…å«å¼‚å¸¸ç»†åŒ–è§£ç å™¨ï¼Œé€šè¿‡æ¸è¿›å¼åˆ©ç”¨å¼‚å¸¸åˆ†æ•°å›¾å¢å¼ºå›¾åƒç‰¹å¾ï¼Œå‡å°‘å¯¹åˆæˆå¼‚å¸¸æ•°æ®çš„ä¾èµ–ã€‚åŒæ—¶æå‡ºæ¸è¿›å¼åˆ†ç»„æµ‹è¯•æ—¶è®­ç»ƒç­–ç•¥ï¼Œåœ¨æ¯ä¸ªäº§å“ç»„ä¸­è®­ç»ƒARDç”¨äºä¸‹ä¸€ç»„çš„ç»†åŒ–è¿‡ç¨‹ï¼Œä¿æŒä¸ä»»æ„ZSADæ–¹æ³•çš„å…¼å®¹æ€§ã€‚

**Result:** åœ¨MVTec ADå’ŒVisAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAnoRefinerèƒ½å¤Ÿå°†å¤šç§ZSADæ¨¡å‹çš„åƒç´ çº§APæŒ‡æ ‡æå‡é«˜è¾¾5.2%ï¼Œè¿™ä¸€æ”¹è¿›åœ¨å¤§é‡å¯è§†åŒ–ç»“æœä¸­å¯ç›´æ¥è§‚å¯Ÿåˆ°ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†å¼‚å¸¸åˆ†æ•°å›¾ä½œä¸ºäº’è¡¥ç©ºé—´çº¿ç´¢çš„é‡è¦ä»·å€¼ï¼Œæå‡ºçš„AnoRefineræ¡†æ¶ä¸ºå·¥ä¸šå¼‚å¸¸æ£€æµ‹æä¾›äº†æœ‰æ•ˆçš„åƒç´ çº§ç»†åŒ–è§£å†³æ–¹æ¡ˆï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶èƒ½å¤Ÿä¸ç°æœ‰ZSADæ–¹æ³•æ— ç¼é›†æˆï¼Œå…·æœ‰å®é™…åº”ç”¨æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Zero-shot industrial anomaly detection (ZSAD) methods typically yield coarse anomaly maps as vision transformers (ViTs) extract patch-level features only. To solve this, recent solutions attempt to predict finer anomalies using features from ZSAD, but they still struggle to recover fine-grained anomalies without missed detections, mainly due to the gap between randomly synthesized training anomalies and real ones. We observe that anomaly score maps exactly provide complementary spatial cues that are largely absent from ZSAD's image features, a fact overlooked before.
  Inspired by this, we propose an anomaly-aware refiner (AnoRefiner) that can be plugged into most ZSAD models and improve patch-level anomaly maps to the pixel level. First, we design an anomaly refinement decoder (ARD) that progressively enhances image features using anomaly score maps, reducing the reliance on synthetic anomaly data. Second, motivated by the mass production paradigm, we propose a progressive group-wise test-time training (PGT) strategy that trains ARD in each product group for the refinement process in the next group, while staying compatible with any ZSAD method.
  Experiments on the MVTec AD and VisA datasets show that AnoRefiner boosts various ZSAD models by up to a 5.2\% gain in pixel-AP metrics, which can also be directly observed in many visualizations. The code will be available at https://github.com/HUST-SLOW/AnoRefiner.


### [54] [MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory](https://arxiv.org/abs/2511.22609)
*Bo Wang, Jiehong Lin, Chenzhi Liu, Xinting Hu, Yifei Yu, Tianjia Liu, Zhongrui Wang, Xiaojuan Qi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MG-Navï¼ˆè®°å¿†å¼•å¯¼å¯¼èˆªï¼‰ï¼Œä¸€ç§ç”¨äºé›¶æ ·æœ¬è§†è§‰å¯¼èˆªçš„åŒå°ºåº¦æ¡†æ¶ï¼Œé€šè¿‡å…¨å±€è®°å¿†å¼•å¯¼è§„åˆ’ä¸å±€éƒ¨å‡ ä½•å¢å¼ºæ§åˆ¶çš„ç»Ÿä¸€ï¼Œå®ç°äº†åœ¨åŠ¨æ€é‡æ’å’Œæœªè§åœºæ™¯æ¡ä»¶ä¸‹çš„é²æ£’å¯¼èˆªã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰å¯¼èˆªæ–¹æ³•åœ¨é•¿è§†é‡è§„åˆ’ã€åŠ¨æ€ç¯å¢ƒé€‚åº”å’Œæœªè§åœºæ™¯æ³›åŒ–æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹éœ€è¦åŒæ—¶å¤„ç†å…¨å±€è·¯å¾„è§„åˆ’å’Œå±€éƒ¨ç²¾ç¡®æ§åˆ¶çš„ç»Ÿä¸€é—®é¢˜ã€‚

**Method:** MG-Navæ¡†æ¶åŒ…å«ç¨€ç–ç©ºé—´è®°å¿†å›¾ï¼ˆSMGï¼‰ä½œä¸ºç´§å‡‘çš„åŒºåŸŸä¸­å¿ƒè®°å¿†è¡¨ç¤ºï¼Œé‡‡ç”¨å…¨å±€å›¾åƒåˆ°å®ä¾‹æ··åˆæ£€ç´¢è¿›è¡Œè·¯å¾„è§„åˆ’ï¼Œå±€éƒ¨ä½¿ç”¨å¯¼èˆªåŸºç¡€ç­–ç•¥æ‰§è¡Œç‚¹ç›®æ ‡æ¨¡å¼ï¼Œå¹¶å¼•å…¥VGGT-adapterå‡ ä½•æ¨¡å—åœ¨å…±äº«3Dæ„ŸçŸ¥ç©ºé—´ä¸­å¯¹é½è§‚æµ‹å’Œç›®æ ‡ç‰¹å¾ã€‚

**Result:** åœ¨HM3Då®ä¾‹-å›¾åƒ-ç›®æ ‡å’ŒMP3Då›¾åƒ-ç›®æ ‡åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMG-Navå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œåœ¨åŠ¨æ€é‡æ’å’Œæœªè§åœºæ™¯æ¡ä»¶ä¸‹ä¿æŒé²æ£’æ€§ï¼Œè¯æ˜äº†åŒå°ºåº¦æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å…¨å±€è®°å¿†å¼•å¯¼è§„åˆ’ä¸å±€éƒ¨å‡ ä½•å¢å¼ºæ§åˆ¶ç»Ÿä¸€æ¡†æ¶çš„ä¼˜è¶Šæ€§ï¼Œä¸ºè§†è§‰å¯¼èˆªç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿è§†é‡ä»»åŠ¡å’Œç¯å¢ƒå˜åŒ–æ–¹é¢å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.


### [55] [REASONEDIT: Towards Reasoning-Enhanced Image Editing Models](https://arxiv.org/abs/2511.22625)
*Fukun Yin, Shiyu Liu, Yucheng Han, Zhibo Wang, Peng Xing, Rui Wang, Wei Cheng, Yingming Wang, Aojie Li, Zixin Yin, Pengtao Chen, Xiangyu Zhang, Daxin Jiang, Xianfang Zeng, Gang Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨ç†çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œé€šè¿‡è§£é”å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ¥æå‡ç¼–è¾‘æ€§èƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ€è€ƒ-ç¼–è¾‘-åæ€å¾ªç¯æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒç¼–è¾‘çš„å‡†ç¡®æ€§å’ŒæŒ‡ä»¤ç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å›¾åƒç¼–è¾‘æ¨¡å‹é€šå¸¸å°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç¼–ç å™¨ä¸æ‰©æ•£è§£ç å™¨è€¦åˆï¼Œä½†MLLMåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒå†»ç»“çŠ¶æ€ï¼Œè¿™é™åˆ¶äº†å…¶æ¨ç†èƒ½åŠ›çš„å‘æŒ¥ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢è§£é”MLLMæ¨ç†èƒ½åŠ›æ˜¯å¦èƒ½è¿›ä¸€æ­¥æå‡å›¾åƒç¼–è¾‘æ¨¡å‹çš„æ€§èƒ½è¾¹ç•Œï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£æŠ½è±¡æŒ‡ä»¤å’Œæå‡ç¼–è¾‘å‡†ç¡®æ€§æ–¹é¢ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºæ¨ç†çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œé‡‡ç”¨æ€è€ƒ-ç¼–è¾‘-åæ€å¾ªç¯æœºåˆ¶ã€‚æ€è€ƒæœºåˆ¶åˆ©ç”¨MLLMçš„ä¸–ç•ŒçŸ¥è¯†æ¥è§£é‡ŠæŠ½è±¡æŒ‡ä»¤ï¼Œåæ€æœºåˆ¶åˆ™å®¡æŸ¥ç¼–è¾‘ç»“æœã€è‡ªåŠ¨çº æ­£æ„å¤–æ“ä½œå¹¶ç¡®å®šåœæ­¢è½®æ¬¡ã€‚è¯¥æ¡†æ¶æ¢ç´¢äº†ä¸¤ç§æ¨ç†æœºåˆ¶ï¼šæ€è€ƒç”¨äºå¢å¼ºæŒ‡ä»¤ç†è§£ï¼Œåæ€ç”¨äºæé«˜ç¼–è¾‘å‡†ç¡®æ€§ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨ç†æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚å½“ä»Step1X-Editåˆå§‹åŒ–DiTæ—¶ï¼Œåœ¨ImgEditä¸Šæå‡äº†4.3%ï¼Œåœ¨GEditä¸Šæå‡äº†4.7%ï¼Œåœ¨Krisä¸Šæå‡äº†8.2%ã€‚å½“ä¸Qwen-Image-Edité›†æˆæ—¶ï¼Œåœ¨GEditå’ŒKrisåŸºå‡†æµ‹è¯•ä¸Šä¹Ÿä¼˜äºä¹‹å‰çš„å¼€æºæ–¹æ³•ã€‚

**Conclusion:** æœ¬ç ”ç©¶è¯æ˜äº†è§£é”å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å¯ä»¥æ˜¾è‘—æå‡å›¾åƒç¼–è¾‘æ¨¡å‹çš„æ€§èƒ½ã€‚æ€è€ƒ-ç¼–è¾‘-åæ€å¾ªç¯æœºåˆ¶ä¸ºå›¾åƒç¼–è¾‘ä»»åŠ¡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ¨ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†æŠ½è±¡æŒ‡ä»¤å¹¶æé«˜ç¼–è¾‘å‡†ç¡®æ€§ï¼Œä¸ºæœªæ¥åŸºäºMLLMçš„å›¾åƒç¼–è¾‘ç³»ç»Ÿè®¾è®¡æä¾›äº†æ–°çš„æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q).


### [56] [GeoZero: Incentivizing Reasoning from Scratch on Geospatial Scenes](https://arxiv.org/abs/2511.22645)
*Di Wang, Shunyu Liu, Wentao Jiang, Fengxiang Wang, Yi Liu, Xiaolei Qin, Zhiming Luo, Chaoyang Zhou, Haonan Guo, Jing Zhang, Bo Du, Dacheng Tao, Liangpei Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†GeoZeroæ¡†æ¶ï¼Œä½¿å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨æ— éœ€é¢„å®šä¹‰æ€ç»´é“¾ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œåœ°ç†ç©ºé—´æ¨ç†ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µæ¿€å‘æ¨¡å‹çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰é¥æ„Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€šå¸¸é€šè¿‡ç²¾å¿ƒç­–åˆ’çš„æ€ç»´é“¾æ•°æ®è¿›è¡Œå†·å¯åŠ¨è®­ç»ƒæ¥å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œè¿™ç§æ–¹æ³•ä¸ä»…äº§ç”Ÿé«˜æ˜‚çš„æ ‡æ³¨æˆæœ¬ï¼Œè¿˜å¼•å…¥äº†å¯èƒ½é™åˆ¶æ¨¡å‹æ¨ç†å¤šæ ·æ€§çš„äººç±»åè§ï¼Œå› æ­¤éœ€è¦å¼€å‘æ— éœ€é¢„å®šä¹‰æ€ç»´é“¾ç›‘ç£çš„åœ°ç†ç©ºé—´æ¨ç†æ–¹æ³•ã€‚

**Method:** æå‡ºçš„GeoZeroæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ•°æ®é›†æ„å»ºï¼šGeoZero-Instructé€šè¿‡ç›‘ç£å¾®è°ƒè®©æ¨¡å‹è·å–åˆæ­¥åœ°ç†ç©ºé—´çŸ¥è¯†ï¼ŒGeoZero-Hardåœ¨åç»­å¼ºåŒ–å­¦ä¹ é˜¶æ®µæ¿€å‘æ·±åº¦æ¨ç†ï¼›åŒæ—¶å¼•å…¥äº†ç­”æ¡ˆé”šå®šç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡æ¨¡å‹è‡ªèº«ç­”æ¡ˆå¯¹æ¨ç†è¿‡ç¨‹è¿›è¡Œæ­£åˆ™åŒ–ï¼Œé¼“åŠ±å¤šæ ·è€Œå‡†ç¡®çš„æ€è€ƒã€‚

**Result:** åœ¨å¤šä¸ªé¥æ„Ÿè§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGeoZeroä¸ä»…è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¿˜åœ¨å¤šæ ·åŒ–çš„åœ°ç†ç©ºé—´ä»»åŠ¡ä¸­åŸ¹å…»äº†é€šç”¨çš„æ¶Œç°æ¨ç†èƒ½åŠ›ï¼Œè¯æ˜äº†æ— éœ€é¢„å®šä¹‰æ€ç»´é“¾ç›‘ç£çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†æ— éœ€äººå·¥æ ‡æ³¨æ€ç»´é“¾æ•°æ®çš„åœ°ç†ç©ºé—´æ¨ç†æ¡†æ¶çš„å¯è¡Œæ€§ï¼Œé€šè¿‡æ¨¡å‹è‡ªèº«ç­”æ¡ˆå¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæ¿€å‘å¤šæ ·ä¸”å‡†ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œä¸ºé¥æ„Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•æä¾›äº†æ–°çš„æ— ç›‘ç£æ¨ç†èŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
Multimodal large language models (MLLMs) have undergone rapid development in advancing geospatial scene understanding. Recent studies have sought to enhance the reasoning capabilities of remote sensing MLLMs, typically through cold-start training with elaborately curated chain-of-thought (CoT) data. However, this approach not only incurs substantial annotation costs but also introduces human biases that may limit the diversity of model reasoning. To address these challenges, we propose GeoZero, a framework that enables MLLMs to perform geospatial reasoning without any predefined CoT supervision. Specifically, we construct two datasets, GeoZero-Instruct and GeoZero-Hard. GeoZero-Instruct allows the model to acquire preliminary geospatial knowledge through supervised fine-tuning, while GeoZero-Hard stimulates deep reasoning during the subsequent reinforcement learning stage. Furthermore, we introduce Answer-Anchored Group Relative Policy Optimization (A$^2$GRPO), where the reasoning process is regularized by the model's own answers, encouraging diverse yet accurate thinking. Extensive experiments on multiple remote sensing vision-language benchmarks demonstrate that GeoZero not only surpasses existing state-of-the-art methods but also fosters universal emergent reasoning capabilities across diverse geospatial tasks. Code,data,and models will be publicly available at https://github.com/MiliLab/GeoZero.


### [57] [Architecture Decoupling Is Not All You Need For Unified Multimodal Model](https://arxiv.org/abs/2511.22663)
*Dian Zheng, Manyuan Zhang, Hongyu Li, Kai Zou, Hongbo Liu, Ziyu Guo, Kaituo Feng, Yexin Liu, Ying Luo, Yan Feng, Peng Pei, Xunliang Cai, Hongsheng Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ³¨æ„åŠ›äº¤äº’å¯¹é½ï¼ˆAIAï¼‰æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨ç¼“è§£å¤šæ¨¡æ€æ¨¡å‹ä¸­ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„å†²çªï¼Œè€Œæ— éœ€è¿›è¡Œæ¨¡å‹è§£è€¦ï¼Œä»è€Œåœ¨ä¿æŒç»Ÿä¸€æ¨¡å‹äº¤é”™ç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶æå‡æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹é¢ä¸´ç†è§£ä¸ç”Ÿæˆä»»åŠ¡é—´ç›®æ ‡å†²çªçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•é€šè¿‡ä¸åŒç¨‹åº¦çš„æ¨¡å‹è§£è€¦ï¼ˆå¦‚åŒå›¾åƒç¼–ç å™¨ã€MOE/MOTæ¶æ„æˆ–å†»ç»“MLLMï¼‰æ¥ç¼“è§£å†²çªï¼Œä½†è¿‡åº¦è§£è€¦ä¼šæŸå®³æ¨¡å‹çš„äº¤é”™ç”Ÿæˆèƒ½åŠ›ï¼Œè¿èƒŒäº†ç»Ÿä¸€æ¨¡å‹çš„åˆè¡·ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¦‚ä½•åœ¨ä¸è¿›è¡Œæ¨¡å‹è§£è€¦çš„æƒ…å†µä¸‹ç¼“è§£ä»»åŠ¡å†²çªã€‚

**Method:** é¦–å…ˆé€šè¿‡åˆ†ææ¨¡å‹è§£è€¦å¦‚ä½•ç¼“è§£å†²çªï¼Œç ”ç©¶æ¨¡å‹çš„è·¨æ¨¡æ€æ³¨æ„åŠ›è¡Œä¸ºï¼Œå‘ç°æ¨¡å‹è§£è€¦æœ¬è´¨ä¸Šé©±åŠ¨æ¨¡å‹è¶‹å‘ä»»åŠ¡ç‰¹å®šçš„å¤šæ¨¡æ€äº¤äº’æ¨¡å¼ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæå‡ºäº†æ³¨æ„åŠ›äº¤äº’å¯¹é½ï¼ˆAIAï¼‰æŸå¤±å‡½æ•°ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾å¼å­¦ä¹ ä»»åŠ¡ç‰¹å®šçš„å¤šæ¨¡æ€äº¤äº’æ¨¡å¼ã€‚ä¸ºäº†éªŒè¯AIAæŸå¤±çš„é€šç”¨æ€§ï¼Œå°†å…¶åˆ†åˆ«åº”ç”¨äºEmu3å’ŒJanus-Proæ¨¡å‹ï¼Œåœ¨SFTå’ŒåæœŸè®­ç»ƒé˜¶æ®µè¿›è¡Œå®éªŒã€‚

**Result:** AIAæŸå¤±ä¸ä»…ç»†åŒ–äº†è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å¼ï¼Œè¿˜åŒæ—¶æå‡äº†ç”Ÿæˆå’Œç†è§£æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸å¼•å…¥é¢å¤–å¤æ‚ç»“æ„çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆç¼“è§£äº†ä»»åŠ¡å†²çªï¼Œä¿æŒäº†æ¨¡å‹çš„äº¤é”™ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå–å¾—äº†æ”¹è¿›ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†æ¨¡å‹è§£è€¦ç¼“è§£ä»»åŠ¡å†²çªçš„æœºåˆ¶æ˜¯é€šè¿‡å¼•å¯¼æ¨¡å‹å­¦ä¹ ä»»åŠ¡ç‰¹å®šçš„äº¤äº’æ¨¡å¼ï¼Œæå‡ºçš„AIAæŸå¤±æä¾›äº†ä¸€ç§æ— éœ€æ¨¡å‹è§£è€¦çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„è®­ç»ƒèŒƒå¼æä¾›äº†æ–°æ€è·¯ï¼Œæœ‰åŠ©äºåœ¨ä¿æŒæ¨¡å‹ç»Ÿä¸€æ€§çš„åŒæ—¶æå‡å¤šä»»åŠ¡æ€§èƒ½ã€‚

---

#### ğŸ“„ Abstract
Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.


### [58] [VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.22664)
*Silin Cheng, Kai Han*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å˜åˆ†å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¡†æ¶VaMPï¼Œé€šè¿‡ä»å­¦ä¹ åˆ°çš„åéªŒåˆ†å¸ƒä¸­é‡‡æ ·ç”Ÿæˆå®ä¾‹æ¡ä»¶åŒ–çš„æç¤ºï¼Œå®ç°äº†æ ·æœ¬ç‰¹å®šã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼Œåœ¨å°‘æ ·æœ¬å’Œé¢†åŸŸæ³›åŒ–åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ–¹æ³•é€šå¸¸ä¾èµ–å›ºå®šã€å…±äº«çš„æç¤ºå’Œç¡®å®šæ€§å‚æ•°ï¼Œé™åˆ¶äº†å®ƒä»¬æ•æ‰å®ä¾‹çº§å˜åŒ–æˆ–è·¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸå»ºæ¨¡ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ï¼Œè¿™é˜»ç¢äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æœ‰é™ç›‘ç£ä¸‹é€‚åº”ä¸‹æ¸¸ä»»åŠ¡çš„æ½œåŠ›ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†å˜åˆ†å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¡†æ¶VaMPï¼Œé€šè¿‡ä»å­¦ä¹ åˆ°çš„åéªŒåˆ†å¸ƒä¸­é‡‡æ ·ç”Ÿæˆå®ä¾‹æ¡ä»¶åŒ–çš„æç¤ºï¼Œå®ç°æ ·æœ¬ç‰¹å®šçš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æç¤ºè°ƒä¼˜ï¼›å¼•å…¥åŸºäºå®ä¾‹è¡¨ç¤ºå’Œç±»åˆ«åŸå‹çš„ç±»æ„ŸçŸ¥å…ˆéªŒï¼Œå¢å¼ºå±€éƒ¨å’Œå…¨å±€è¯­ä¹‰çš„æ•´åˆï¼›å°†æç¤ºè°ƒä¼˜å½¢å¼åŒ–ä¸ºæ½œåœ¨æç¤ºè¡¨ç¤ºçš„å˜åˆ†æ¨æ–­ï¼Œå¹¶é€šè¿‡é‡å‚æ•°åŒ–é‡‡æ ·è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚

**Result:** åœ¨å°‘æ ·æœ¬å­¦ä¹ å’Œé¢†åŸŸæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVaMPæ¡†æ¶å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå®éªŒç»“æœè¡¨æ˜å»ºæ¨¡ä¸ç¡®å®šæ€§å’Œä»»åŠ¡ç»“æ„å¯¹æå‡å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ•ˆæœå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å˜åˆ†æ¨æ–­å®ç°å®ä¾‹ç‰¹å®šã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æç¤ºç”Ÿæˆèƒ½å¤Ÿæœ‰æ•ˆæå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æœ‰é™ç›‘ç£ä¸‹çš„é€‚åº”èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ä¸­çš„æç¤ºè°ƒä¼˜æä¾›äº†æ–°çš„ç†è®ºæ¡†æ¶å’Œå®è·µæ–¹æ³•ã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLMs), such as CLIP, have shown strong generalization under zero-shot settings, yet adapting them to downstream tasks with limited supervision remains a significant challenge. Existing multi-modal prompt learning methods typically rely on fixed, shared prompts and deterministic parameters, which limits their ability to capture instance-level variation or model uncertainty across diverse tasks and domains. To tackle this issue, we propose a novel Variational Multi-Modal Prompt Learning (VaMP) framework that enables sample-specific, uncertainty-aware prompt tuning in multi-modal representation learning. VaMP generates instance-conditioned prompts by sampling from a learned posterior distribution, allowing the model to personalize its behavior based on input content. To further enhance the integration of local and global semantics, we introduce a class-aware prior derived from the instance representation and class prototype. Building upon these, we formulate prompt tuning as variational inference over latent prompt representations and train the entire framework end-to-end through reparameterized sampling. Experiments on few-shot and domain generalization benchmarks show that VaMP achieves state-of-the-art performance, highlighting the benefits of modeling both uncertainty and task structure in our method. Project page: https://visual-ai.github.io/vamp


### [59] [Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield](https://arxiv.org/abs/2511.22677)
*Dongyang Liu, Peng Gao, David Liu, Ruoyi Du, Zhen Li, Qilong Wu, Xin Jin, Sihan Cao, Shifeng Zhang, Hongsheng Li, Steven Hoi*

#### ğŸ§© TL;DR
æœ¬æ–‡æŒ‘æˆ˜äº†æ‰©æ•£æ¨¡å‹è’¸é¦ä¸­åˆ†å¸ƒåŒ¹é…æœºåˆ¶çš„ä¼ ç»Ÿç†è§£ï¼Œé€šè¿‡å¯¹DMDè®­ç»ƒç›®æ ‡çš„åˆ†è§£ï¼Œæ­ç¤ºäº†åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç­‰å¤æ‚ä»»åŠ¡ä¸­ï¼ŒCFGå¢å¼ºè€Œéåˆ†å¸ƒåŒ¹é…æ‰æ˜¯å°‘æ­¥è’¸é¦çš„ä¸»è¦é©±åŠ¨åŠ›ï¼Œå¹¶æå‡ºäº†è§£è€¦å™ªå£°è°ƒåº¦ç­‰æ”¹è¿›æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨æŒ‘æˆ˜æ‰©æ•£æ¨¡å‹è’¸é¦ä¸­å…³äºåˆ†å¸ƒåŒ¹é…è’¸é¦åŠå…¶å˜ä½“æ€§èƒ½æ¥æºçš„ä¼ ç»Ÿç†è§£ï¼Œä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºè¿™äº›æ–¹æ³•çš„ä¼˜å¼‚æ€§èƒ½æºäºå­¦ç”Ÿæ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¸é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹çš„åŒ¹é…æœºåˆ¶ï¼Œä½†æœ¬æ–‡é€šè¿‡æ·±å…¥åˆ†æå‘ç°è¿™ç§ç†è§£å¯èƒ½ä¸å‡†ç¡®ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦CFGçš„å¤æ‚ä»»åŠ¡ä¸­ã€‚

**Method:** æœ¬æ–‡é€šè¿‡å¯¹DMDè®­ç»ƒç›®æ ‡è¿›è¡Œä¸¥æ ¼çš„æ•°å­¦åˆ†è§£ï¼Œè¯†åˆ«å‡ºCFGå¢å¼ºè¿™ä¸€å…ˆå‰è¢«å¿½è§†çš„æ ¸å¿ƒç»„ä»¶ï¼Œå¹¶ç³»ç»Ÿåˆ†æäº†åˆ†å¸ƒåŒ¹é…é¡¹ä¸CFGå¢å¼ºé¡¹çš„ä¸åŒä½œç”¨ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†è¿™ç§è§£è€¦å…³ç³»ï¼Œé€šè¿‡å¼•å…¥æ›´ç®€å•çš„éå‚æ•°çº¦æŸæˆ–åŸºäºGANçš„ç›®æ ‡ä½œä¸ºæ›¿ä»£æ­£åˆ™åŒ–å™¨ï¼Œå¹¶æå‡ºäº†è§£è€¦å¼•æ“ä¸æ­£åˆ™åŒ–å™¨å™ªå£°è°ƒåº¦ç­‰æ”¹è¿›æ–¹æ³•ã€‚

**Result:** ç ”ç©¶ç»“æœè¡¨æ˜CFGå¢å¼ºé¡¹æ˜¯å°‘æ­¥è’¸é¦çš„æ ¸å¿ƒ"å¼•æ“"ï¼Œè€Œåˆ†å¸ƒåŒ¹é…é¡¹ä¸»è¦ä½œä¸º"æ­£åˆ™åŒ–å™¨"ç¡®ä¿è®­ç»ƒç¨³å®šæ€§å¹¶å‡å°‘ä¼ªå½±ï¼Œå®éªŒè¯æ˜åˆ†å¸ƒåŒ¹é…é¡¹å¹¶éå”¯ä¸€æœ‰æ•ˆçš„æ­£åˆ™åŒ–å™¨ï¼Œæ›´ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆä¹Ÿèƒ½å®ç°ç±»ä¼¼åŠŸèƒ½ä½†å…·æœ‰ä¸åŒæƒè¡¡ï¼ŒåŸºäºæ–°ç†è§£æå‡ºçš„æ”¹è¿›æ–¹æ³•å·²è¢«Z-Imageé¡¹ç›®é‡‡ç”¨ï¼ŒæˆåŠŸå¼€å‘å‡ºé¡¶çº§çš„8æ­¥å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚

**Conclusion:** æœ¬ç ”ç©¶æä¾›äº†å¯¹æ‰©æ•£æ¨¡å‹è’¸é¦æœºåˆ¶æ›´ç³»ç»Ÿæ·±å…¥çš„ç†è§£ï¼Œæ­ç¤ºäº†CFGå¢å¼ºè€Œéåˆ†å¸ƒåŒ¹é…æ‰æ˜¯å¤æ‚ä»»åŠ¡ä¸­å°‘æ­¥è’¸é¦çš„ä¸»è¦é©±åŠ¨åŠ›ï¼Œè¿™ç§è§£è€¦åˆ†æä¸ºè’¸é¦è¿‡ç¨‹æä¾›äº†æ›´åŸåˆ™æ€§çš„æ¡†æ¶ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿæ›´ç³»ç»Ÿåœ°åˆ†æä¸¤ä¸ªç»„ä»¶çš„ç‰¹æ€§ï¼Œå¹¶ä¸ºæ”¹è¿›è’¸é¦æ–¹æ³•æä¾›äº†ç†è®ºåŸºç¡€ï¼Œæœ€ç»ˆæ¨åŠ¨äº†æ›´é«˜æ•ˆå›¾åƒç”Ÿæˆæ¨¡å‹çš„å¼€å‘ã€‚

---

#### ğŸ“„ Abstract
Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.


### [60] [Ar2Can: An Architect and an Artist Leveraging a Canvas for Multi-Human Generation](https://arxiv.org/abs/2511.22690)
*Shubhankar Borse, Phuc Pham, Farzad Farhadzadeh, Seokeon Choi, Phong Ha Nguyen, Anh Tuan Tran, Sungrack Yun, Munawar Hayat, Fatih Porikli*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Ar2Canï¼Œä¸€ä¸ªç”¨äºå¤šäººç”Ÿæˆåœºæ™¯çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡è§£è€¦ç©ºé—´è§„åˆ’ä¸èº«ä»½æ¸²æŸ“æ¥è§£å†³ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤šäººåœºæ™¯ä¸­é‡å¤é¢éƒ¨ã€åˆå¹¶èº«ä»½æˆ–è®¡æ•°é”™è¯¯çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯è¿‘æœŸå–å¾—è¿›å±•ï¼Œç°æœ‰æ¨¡å‹åœ¨ç”Ÿæˆå¤šäººåœºæ™¯æ—¶å§‹ç»ˆå­˜åœ¨å¯é æ€§é—®é¢˜ï¼Œç»å¸¸å‡ºç°é¢éƒ¨é‡å¤ã€èº«ä»½åˆå¹¶æˆ–ä¸ªä½“è®¡æ•°é”™è¯¯ï¼Œè¿™æ„æˆäº†å½“å‰ç ”ç©¶éœ€è¦è§£å†³çš„å…³é”®æŠ€æœ¯ç“¶é¢ˆã€‚

**Method:** Ar2Cané‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶ï¼šArchitectæ¨¡å—é¢„æµ‹ç»“æ„åŒ–å¸ƒå±€ä»¥ç¡®å®šæ¯ä¸ªäººçš„ä½ç½®ï¼ŒArtistæ¨¡å—åŸºäºæ‰©æ•£æ¨¡å‹åˆæˆé€¼çœŸå›¾åƒï¼Œå¹¶é€šè¿‡ç»“åˆåŒˆç‰™åˆ©ç©ºé—´å¯¹é½ä¸ArcFaceèº«ä»½ç›¸ä¼¼æ€§çš„ç©ºé—´æ¥åœ°é¢éƒ¨åŒ¹é…å¥–åŠ±è¿›è¡Œå¼•å¯¼ï¼ŒåŒæ—¶ä½¿ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰é€šè¿‡ç»„åˆå¥–åŠ±ä¼˜åŒ–è®¡æ•°å‡†ç¡®æ€§ã€å›¾åƒè´¨é‡å’Œèº«ä»½åŒ¹é…ã€‚

**Result:** åœ¨MultiHuman-Testbenchè¯„ä¼°ä¸­ï¼ŒAr2Canåœ¨è®¡æ•°å‡†ç¡®æ€§å’Œèº«ä»½ä¿æŒæ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ„ŸçŸ¥è´¨é‡ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯è¯¥æ–¹æ³•ä¸»è¦ä½¿ç”¨åˆæˆæ•°æ®å®ç°è¿™äº›ç»“æœï¼Œæ— éœ€çœŸå®å¤šäººå›¾åƒã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡è§£è€¦ç©ºé—´è§„åˆ’ä¸èº«ä»½æ¸²æŸ“çš„æ¡†æ¶è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆè§£å†³å¤šäººåœºæ™¯ç”Ÿæˆçš„å¯é æ€§é—®é¢˜ï¼ŒåŒæ—¶è¯æ˜äº†ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤æ‚åœºæ™¯ç”Ÿæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Despite recent advances in text-to-image generation, existing models consistently fail to produce reliable multi-human scenes, often duplicating faces, merging identities, or miscounting individuals. We present Ar2Can, a novel two-stage framework that disentangles spatial planning from identity rendering for multi-human generation. The Architect module predicts structured layouts, specifying where each person should appear. The Artist module then synthesizes photorealistic images, guided by a spatially-grounded face matching reward that combines Hungarian spatial alignment with ArcFace identity similarity. This approach ensures faces are rendered at correct locations and faithfully preserve reference identities. We develop two Architect variants, seamlessly integrated with our diffusion-based Artist model and optimized via Group Relative Policy Optimization (GRPO) using compositional rewards for count accuracy, image quality, and identity matching. Evaluated on the MultiHuman-Testbench, Ar2Can achieves substantial improvements in both count accuracy and identity preservation, while maintaining high perceptual quality. Notably, our method achieves these results using primarily synthetic data, without requiring real multi-human images.


### [61] [Alzheimer's Disease Prediction Using EffNetViTLoRA and BiLSTM with Multimodal Longitudinal MRI Data](https://arxiv.org/abs/2511.22774)
*Mahdieh Behjat Khatooni, Mohsen Soryani*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…é¢„æµ‹çš„å¹¿ä¹‰ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡æ•´åˆå·ç§¯ç¥ç»ç½‘ç»œå’Œè§†è§‰Transformeræ¥æ•æ‰MRIæ‰«æçš„å±€éƒ¨ç©ºé—´ç‰¹å¾ä¸å…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–ï¼Œå¹¶åˆ©ç”¨åŒå‘LSTMå¤„ç†æ—¶åºæ•°æ®ï¼Œå®ç°äº†å¯¹è½»åº¦è®¤çŸ¥éšœç¢å‘é˜¿å°”èŒ¨æµ·é»˜ç—…è½¬åŒ–çš„é«˜ç²¾åº¦é¢„æµ‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é˜¿å°”èŒ¨æµ·é»˜ç—…æ˜¯ä¸€ç§ä¸å¯é€†çš„ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œæ—©æœŸé¢„æµ‹å¯¹åŠæ—¶å¹²é¢„è‡³å…³é‡è¦ã€‚è½»åº¦è®¤çŸ¥éšœç¢ä½œä¸ºè®¤çŸ¥æ­£å¸¸ä¸é˜¿å°”èŒ¨æµ·é»˜ç—…ä¹‹é—´çš„è¿‡æ¸¡é˜¶æ®µï¼Œå…¶å‘é˜¿å°”èŒ¨æµ·é»˜ç—…çš„è½¬åŒ–é¢„æµ‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå¹¶éæ‰€æœ‰MCIæ‚£è€…éƒ½ä¼šå‘å±•ä¸ºé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼Œç°æœ‰æ–¹æ³•åœ¨åŒºåˆ†ç¨³å®šMCIä¸è¿›å±•æ€§MCIæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚

**Method:** æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆæ¶æ„ï¼Œæ•´åˆäº†å·ç§¯ç¥ç»ç½‘ç»œå’Œè§†è§‰Transformerï¼Œä»¥ä»ç£å…±æŒ¯æˆåƒæ‰«æä¸­æ•æ‰å±€éƒ¨ç©ºé—´ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–ã€‚ä¸ºçº³å…¥æ—¶åºè¿›å±•ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥é‡‡ç”¨åŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œå¤„ç†è¿ç»­å››ä¸ªæ—¶é—´ç‚¹çš„MRIç‰¹å¾åŠå…¶ä»–éå½±åƒç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œé¢„æµ‹å—è¯•è€…åœ¨ç¬¬48ä¸ªæœˆçš„è®¤çŸ¥çŠ¶æ€ã€‚

**Result:** è¯¥å¤šæ¨¡æ€æ¨¡å‹åœ¨åŒºåˆ†ç¨³å®šMCIä¸è¿›å±•æ€§MCIæ–¹é¢å®ç°äº†95.05%çš„å¹³å‡è¿›å±•é¢„æµ‹å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ç°æœ‰é˜¿å°”èŒ¨æµ·é»˜ç—…é¢„æµ‹ç ”ç©¶çš„è¡¨ç°ã€‚è¯¥å·¥ä½œå±•ç¤ºäº†åœ¨çºµå‘é˜¿å°”èŒ¨æµ·é»˜ç—…é¢„æµ‹ä¸­çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** æœ¬ç ”ç©¶è¯æ˜äº†ç»“åˆç©ºé—´ä¸æ—¶åºå»ºæ¨¡åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…æ—©æœŸæ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„é¢„æµ‹æä¾›äº†å¼ºå¤§çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ··åˆæ¶æ„èƒ½å¤ŸåŒæ—¶æ•æ‰å½±åƒæ•°æ®çš„å±€éƒ¨ç»†èŠ‚ä¸å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¹¶ç»“åˆæ—¶åºåŠ¨æ€ä¿¡æ¯ï¼Œä¸ºä¸´åºŠæ—©æœŸå¹²é¢„æä¾›äº†å¯é çš„å·¥å…·ã€‚

---

#### ğŸ“„ Abstract
Alzheimer's disease (AD) is a prevalent neurodegenerative disorder that progressively impairs memory, decision-making, and overall cognitive function. As AD is irreversible, early prediction is critical for timely intervention and management. Mild Cognitive Impairment (MCI), a transitional stage between cognitively normal (CN) aging and AD, plays a significant role in early AD diagnosis. However, predicting MCI progression remains a significant challenge, as not all individuals with MCI convert to AD. MCI subjects are categorized into stable MCI (sMCI) and progressive MCI (pMCI) based on conversion status. In this study, we propose a generalized, end-to-end deep learning model for AD prediction using MCI cases from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our hybrid architecture integrates Convolutional Neural Networks and Vision Transformers to capture both local spatial features and global contextual dependencies from Magnetic Resonance Imaging (MRI) scans. To incorporate temporal progression, we further employ Bidirectional Long Short-Term Memory (BiLSTM) networks to process features extracted from four consecutive MRI timepoints along with some other non-image biomarkers, predicting each subject's cognitive status at month 48. Our multimodal model achieved an average progression prediction accuracy of 95.05\% between sMCI and pMCI, outperforming existing studies in AD prediction. This work demonstrates state-of-the-art performance in longitudinal AD prediction and highlights the effectiveness of combining spatial and temporal modeling for the early detection of Alzheimer's disease.


### [62] [World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models](https://arxiv.org/abs/2511.22787)
*Eunsu Kim, Junyeong Park, Na Min An, Junseong Kim, Hitesh Laxmichand Patel, Jiho Jin, Julia Kruk, Amit Agarwal, Srikant Panda, Fenal Ashokbhai Ilasariya, Hyunjung Shim, Alice Oh*

#### ğŸ§© TL;DR
æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–‡åŒ–æ··åˆåœºæ™¯ä¸­çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œæ„å»ºäº†CultureMixåŸºå‡†æµ‹è¯•å¹¶å‘ç°ç°æœ‰æ¨¡å‹åœ¨ä¿æŒæ–‡åŒ–èº«ä»½ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒç­–ç•¥æœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨å¤šå…ƒæ–‡åŒ–ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åœ¨å…¨çƒåŒ–çš„è§†è§‰åœºæ™¯ä¸­ï¼Œæ¥è‡ªä¸åŒæ–‡åŒ–çš„å…ƒç´ ç»å¸¸åŒæ—¶å‡ºç°å½¢æˆæ–‡åŒ–æ··åˆåœºæ™¯ï¼Œç„¶è€Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¦‚ä½•æ„ŸçŸ¥è¿™äº›æ··åˆæ–‡åŒ–åœºæ™¯ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ç³»ç»Ÿæ€§åœ°åˆ†æLVLMsåœ¨å¤šå…ƒæ–‡åŒ–å…ƒç´ å…±å­˜æ—¶çš„è¡Œä¸ºæ¨¡å¼ï¼Œå¡«è¡¥å½“å‰ç ”ç©¶åœ¨æ–‡åŒ–æ··åˆåœºæ™¯è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†CultureMixåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«23kä¸ªæ‰©æ•£ç”Ÿæˆå¹¶ç»äººå·¥éªŒè¯çš„æ–‡åŒ–æ··åˆå›¾åƒï¼Œæ¶µç›–å››ä¸ªå­ä»»åŠ¡ï¼šçº¯é£Ÿç‰©ã€é£Ÿç‰©+é£Ÿç‰©ã€é£Ÿç‰©+èƒŒæ™¯ã€é£Ÿç‰©+é£Ÿç‰©+èƒŒæ™¯ã€‚è¯„ä¼°äº†10ä¸ªå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶æ¢ç´¢äº†ä¸‰ç§é²æ£’æ€§ç­–ç•¥ï¼Œå…¶ä¸­ç›‘ç£å¾®è°ƒä½¿ç”¨å¤šæ ·åŒ–çš„æ–‡åŒ–æ··åˆæ•°æ®é›†æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

**Result:** å®éªŒå‘ç°ç°æœ‰æ¨¡å‹åœ¨æ–‡åŒ–æ··åˆåœºæ™¯ä¸­æ™®éæ— æ³•ä¿æŒä¸ªä½“æ–‡åŒ–èº«ä»½çš„ä¸€è‡´æ€§ï¼Œè¡¨ç°å‡ºå¼ºçƒˆçš„èƒŒæ™¯ä¾èµ–æ€§â€”â€”å½“æ–‡åŒ–èƒŒæ™¯æ·»åŠ åˆ°çº¯é£Ÿç‰©åŸºå‡†æ—¶å‡†ç¡®ç‡ä¸‹é™14%ã€‚æ¨¡å‹å¯¹ç›¸åŒé£Ÿç‰©åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„é¢„æµ‹ç»“æœä¸ä¸€è‡´ï¼Œè€Œç›‘ç£å¾®è°ƒç­–ç•¥æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„ä¸€è‡´æ€§å¹¶é™ä½äº†èƒŒæ™¯æ•æ„Ÿæ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜æ–‡åŒ–æ··åˆåœºæ™¯æ˜¯LVLMsé¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œéœ€è¦æ›´å¤šå…³æ³¨ä»¥å¼€å‘èƒ½å¤Ÿåœ¨æ–‡åŒ–å¤šæ ·æ€§ç°å®ç¯å¢ƒä¸­å¯é è¿è¡Œçš„æ¨¡å‹ã€‚ç›‘ç£å¾®è°ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ä¸ºæå‡æ¨¡å‹æ–‡åŒ–æ„ŸçŸ¥èƒ½åŠ›æä¾›äº†å¯è¡Œè·¯å¾„ï¼Œå¼ºè°ƒäº†æ„å»ºåŒ…å®¹æ€§AIç³»ç»Ÿæ—¶è€ƒè™‘æ–‡åŒ–å¤æ‚æ€§çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
In a globalized world, cultural elements from diverse origins frequently appear together within a single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as a critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, a food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using a diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as a critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments.


### [63] [From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images](https://arxiv.org/abs/2511.22805)
*Yiming Chen, Junlin Han, Tianyi Bai, Shengbang Tong, Filippos Kokkinos, Philip Torr*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºCogIP-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒè®¤çŸ¥å±æ€§ï¼ˆå¦‚è®°å¿†æ€§ã€è¶£å‘³æ€§ã€å®¡ç¾æ€§ï¼‰ä¸Šä¸äººç±»æ„ŸçŸ¥çš„å¯¹é½ç¨‹åº¦ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åè®­ç»ƒæ–¹æ³•æ˜¾è‘—æå‡æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ï¼Œè¯æ˜è¿™ç§è®¤çŸ¥å¯¹é½å¯è¿ç§»åˆ°ä¸‹æ¸¸åˆ›æ„ä»»åŠ¡ä¸­ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ“…é•¿è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“å’Œæè¿°åœºæ™¯ï¼Œä½†ç¼ºä¹ç†è§£å›¾åƒå¯¹äººç±»è§‚å¯Ÿè€…ä¸»è§‚æ„Ÿå—çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è®°å¿†æ€§ã€è¶£å‘³æ€§ã€å®¡ç¾æ€§å’Œæƒ…æ„Ÿå”¤èµ·ç­‰è®¤çŸ¥å±æ€§æ–¹é¢ä¸äººç±»æ„ŸçŸ¥å­˜åœ¨æ˜¾è‘—å·®è·ã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†CogIP-BenchåŸºå‡†æµ‹è¯•æ¥ç³»ç»Ÿè¯„ä¼°MLLMsåœ¨å›¾åƒè®¤çŸ¥å±æ€§ä¸Šçš„è¡¨ç°ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åè®­ç»ƒé˜¶æ®µæ–¹æ³•ï¼Œé€šè¿‡ä¸“é—¨çš„è®­ç»ƒè¿‡ç¨‹å¢å¼ºæ¨¡å‹ä¸äººç±»æ„ŸçŸ¥çš„å¯¹é½èƒ½åŠ›ï¼ŒåŒæ—¶å°†è®¤çŸ¥å¯¹é½æ¨¡å‹é›†æˆåˆ°å›¾åƒç”Ÿæˆæµç¨‹ä¸­ä»¥æŒ‡å¯¼åˆæˆè¿‡ç¨‹ã€‚

**Result:** è¯„ä¼°æ˜¾ç¤ºå½“å‰æ¨¡å‹åœ¨å›¾åƒè®¤çŸ¥å±æ€§ä¸Šä¸äººç±»æ„ŸçŸ¥çš„å¯¹é½ç¨‹åº¦è¾ƒå·®ï¼Œä½†åè®­ç»ƒæ–¹æ³•èƒ½æœ‰æ•ˆç¼©å°è¿™ä¸€å·®è·ï¼Œæ˜¾è‘—æå‡æ¨¡å‹ä¸äººç±»åˆ¤æ–­çš„å¯¹é½åº¦ï¼Œä¸”å­¦ä¹ åˆ°çš„è®¤çŸ¥å¯¹é½å…·æœ‰å¯è¿ç§»æ€§ï¼Œèƒ½æŒ‡å¯¼å›¾åƒç”Ÿæˆæµç¨‹äº§ç”Ÿæ›´å…·è®°å¿†æ€§æˆ–è§†è§‰å¸å¼•åŠ›çš„å›¾åƒã€‚

**Conclusion:** è¯¥ç ”ç©¶æä¾›äº†æµ‹é‡äººç±»æ„ŸçŸ¥çš„åŸºå‡†æµ‹è¯•ã€å¢å¼ºå¯¹é½çš„åè®­ç»ƒæµç¨‹ï¼Œå¹¶è¯æ˜è¿™ç§è®¤çŸ¥å¯¹é½èƒ½è§£é”æ›´ä»¥äººä¸ºæœ¬çš„äººå·¥æ™ºèƒ½ï¼Œä¸ºå¼€å‘æ›´å…·äººç±»æ„ŸçŸ¥èƒ½åŠ›çš„å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†ç³»ç»Ÿæ–¹æ³•å’Œå®è¯åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
While Multimodal Large Language Models (MLLMs) are adept at answering what is in an image-identifying objects and describing scenes-they often lack the ability to understand how an image feels to a human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, a comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals a significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that a post-training phase can effectively bridge this gap, significantly enhancing the model's alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides a benchmark to measure this human-like perception, a post-training pipeline to enhance it, and a demonstration that this alignment unlocks more human-centric AI.


### [64] [Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs](https://arxiv.org/abs/2511.22826)
*Tianle Chen, Chaitanya Chakka, Arjun Reddy Akula, Xavier Thomas, Deepti Ghadiyaram*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†MMA-BenchåŸºå‡†æ¥è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹çŸ›ç›¾æ¨¡æ€çš„é²æ£’æ€§ï¼Œå¹¶å¼€å‘äº†ä¸€ç§æ¨¡æ€å¯¹é½å¾®è°ƒç­–ç•¥ä»¥å¢å¼ºæ¨¡å‹çš„è·¨æ¨¡æ€æ¨ç†å¯é æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å¯¹çŸ›ç›¾æ¨¡æ€çš„é²æ£’æ€§ä»æ˜¯ä¸€ä¸ªæœªè§£å†³çš„åŸºæœ¬é—®é¢˜ï¼Œå½“å‰æ¨¡å‹åœ¨éŸ³é¢‘-è§†è§‰å¯¹é”™ä½å’Œç®€å•è¯¯å¯¼æ–‡æœ¬æƒ…å†µä¸‹ç¼ºä¹ç¨³å¥çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†MMA-BenchåŸºå‡†ï¼ŒåŒ…å«è§†é¢‘å’Œä»»åŠ¡æ¥æ¢æµ‹æ¨¡å‹å¯¹ç‰¹å®šæ¨¡æ€çš„ä¾èµ–ï¼Œé‡‡ç”¨é»‘ç›’å’Œç™½ç›’å¯è§£é‡Šæ€§æŠ€æœ¯åˆ†ææ¨¡å‹è„†å¼±æ€§ï¼Œå¹¶æå‡ºæ¨¡æ€å¯¹é½å¾®è°ƒç­–ç•¥æ¥æ•™å¯¼æ¨¡å‹ä½•æ—¶ä¼˜å…ˆè€ƒè™‘ã€åˆ©ç”¨æˆ–å¿½ç•¥ç‰¹å®šæ¨¡æ€çº¿ç´¢ã€‚

**Result:** å®éªŒè¡¨æ˜å½“å‰å¼€æºå’Œé—­æºMLLMåœ¨é”™ä½çš„éŸ³é¢‘-è§†è§‰å¯¹å’Œç®€å•è¯¯å¯¼æ–‡æœ¬ä¸‹è¡¨ç°è„†å¼±ï¼Œè€Œæå‡ºçš„å¯¹é½å¾®è°ƒç­–ç•¥èƒ½æ˜¾è‘—å¢å¼ºå¤šæ¨¡æ€åŸºç¡€èƒ½åŠ›ï¼Œäº§ç”Ÿæ›´å¯é çš„è·¨æ¨¡æ€æ¨ç†ã€‚

**Conclusion:** è¯¥å·¥ä½œæä¾›äº†å¯è§£é‡Šæ€§å·¥å…·å’Œæ˜ç¡®çš„è·¯å¾„æ¥å¼€å‘å…·æœ‰å†…åœ¨å¯é è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›çš„MLLMï¼Œæ¨¡æ€å¯¹é½å¾®è°ƒæ˜¯å®ç°è¿™ä¸€ç›®æ ‡çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å¯ç”¨ã€‚

---

#### ğŸ“„ Abstract
Despite remarkable advancements in Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe a model's reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide a critical analysis of the brittleness of both open- and closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audio-visual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both interpretability tools and a clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be publicly available.


### [65] [Breaking the Visual Shortcuts in Multimodal Knowledge-Based Visual Question Answering](https://arxiv.org/abs/2511.22843)
*Dosung Lee, Sangwon Jung, Boyoung Kim, Minyoung Kim, Sungyeon Kim, Junyoung Sung, Paul Hongsuck Seo*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æ­ç¤ºäº†ç°æœ‰å¤šæ¨¡æ€çŸ¥è¯†åº“è§†è§‰é—®ç­”åŸºå‡†å­˜åœ¨çš„"è§†è§‰æ·å¾„"é—®é¢˜ï¼Œå¹¶æå‡ºäº†RETINAåŸºå‡†å’ŒMIMIRæ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚RETINAé€šè¿‡LLMé©±åŠ¨çš„æµç¨‹è‡ªåŠ¨æ„å»ºï¼ŒåŒ…å«12ä¸‡è®­ç»ƒæ ·æœ¬å’Œ2åƒäººå·¥æ ‡æ³¨æµ‹è¯•é›†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºæ¶ˆé™¤è§†è§‰æ·å¾„ï¼›MIMIRåˆ™é€šè¿‡å¢å¼ºå¤šä¸ªç›¸å…³å®ä½“çš„å›¾åƒæ¥ä¸°å¯Œæ–‡æ¡£åµŒå…¥è¡¨ç¤ºã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¤šæ¨¡æ€çŸ¥è¯†åº“è§†è§‰é—®ç­”åŸºå‡†å­˜åœ¨ä¸¥é‡çš„"è§†è§‰æ·å¾„"é—®é¢˜ï¼Œå³æŸ¥è¯¢å›¾åƒé€šå¸¸ä¸ç›®æ ‡æ–‡æ¡£çš„ä¸»è¦ä¸»é¢˜å®ä½“ç›´æ¥åŒ¹é…ï¼Œå¯¼è‡´æ¨¡å‹å¯ä»¥ä»…åˆ©ç”¨è§†è§‰çº¿ç´¢å°±è·å¾—å¯æ¯”ç»“æœï¼Œè€Œæ— éœ€çœŸæ­£ç†è§£å¤šæ¨¡æ€çŸ¥è¯†ã€‚è¿™ç§è®¾è®¡ç¼ºé™·ä½¿å¾—ç°æœ‰åŸºå‡†æ— æ³•å‡†ç¡®è¯„ä¼°æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œæ©ç›–äº†æ¨¡å‹å¯¹è§†è§‰æ·å¾„çš„ä¾èµ–ã€‚

**Method:** ç ”ç©¶æå‡ºäº†ä¸¤ä¸ªä¸»è¦è´¡çŒ®ï¼šé¦–å…ˆï¼Œå¼€å‘äº†RETINAåŸºå‡†ï¼Œé‡‡ç”¨LLMé©±åŠ¨çš„è‡ªåŠ¨åŒ–æµç¨‹æ„å»ºï¼ŒåŒ…å«12ä¸‡è®­ç»ƒæ ·æœ¬å’Œ2åƒäººå·¥æ ‡æ³¨æµ‹è¯•é›†ï¼Œä¸“é—¨è®¾è®¡æŸ¥è¯¢å¼•ç”¨æ¬¡è¦ä¸»é¢˜ï¼ˆç›¸å…³å®ä½“ï¼‰å¹¶é…å¯¹ç›¸å…³å®ä½“çš„å›¾åƒï¼Œä»è€Œæ¶ˆé™¤è§†è§‰æ·å¾„ã€‚å…¶æ¬¡ï¼Œæå‡ºäº†MIMIRæ–¹æ³•ï¼Œé€šè¿‡å¢å¼ºå¤šä¸ªç›¸å…³å®ä½“çš„å›¾åƒæ¥ä¸°å¯Œæ–‡æ¡£åµŒå…¥è¡¨ç¤ºï¼Œä¸å…ˆå‰ä»…ä½¿ç”¨å•ä¸ªå›¾åƒçš„æ–¹æ³•ä¸åŒï¼ŒMIMIRèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†RETINAä¸­çš„å¤æ‚å¤šå®ä½“å…³ç³»ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œå½“åœ¨RETINAåŸºå‡†ä¸Šè¯„ä¼°æ—¶ï¼Œç°æœ‰æ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¯å®äº†å®ƒä»¬å¯¹è§†è§‰æ·å¾„çš„ä¾èµ–ã€‚MIMIRæ–¹æ³•åœ¨RETINAåŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•æ— æ³•å¤„ç†çš„å¤æ‚å¤šå®ä½“å…³ç³»é—®é¢˜ã€‚ç ”ç©¶éªŒè¯äº†ç°æœ‰åŸºå‡†çš„å±€é™æ€§ï¼Œå¹¶è¯æ˜äº†RETINAåŸºå‡†å’ŒMIMIRæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€çŸ¥è¯†åº“è§†è§‰é—®ç­”é¢†åŸŸåŸºå‡†è®¾è®¡çš„é‡è¦ç¼ºé™·ï¼Œæå‡ºçš„RETINAåŸºå‡†ä¸ºè¯„ä¼°æ¨¡å‹çœŸå®å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æä¾›äº†æ›´å¯é çš„æµ‹è¯•å¹³å°ã€‚MIMIRæ–¹æ³•å±•ç¤ºäº†é€šè¿‡å¤šå›¾åƒå¢å¼ºæ¥å¤„ç†å¤æ‚å®ä½“å…³ç³»çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€çŸ¥è¯†è¡¨ç¤ºå’Œæ¨ç†ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†åœ¨åŸºå‡†è®¾è®¡ä¸­æ¶ˆé™¤æ·å¾„çš„é‡è¦æ€§ï¼Œå¹¶æ¨åŠ¨äº†æ›´é²æ£’çš„å¤šæ¨¡æ€æ¨¡å‹å¼€å‘ã€‚

---

#### ğŸ“„ Abstract
Existing Multimodal Knowledge-Based Visual Question Answering (MKB-VQA) benchmarks suffer from "visual shortcuts", as the query image typically matches the primary subject entity of the target document. We demonstrate that models can exploit these shortcuts, achieving comparable results using visual cues alone. To address this, we introduce Relational Entity Text-Image kNowledge Augmented (RETINA) benchmark, automatically constructed using an LLM-driven pipeline, consisting of 120k training and 2k human-curated test set. RETINA contains queries referencing secondary subjects (i.e. related entities) and pairs them with images of these related entities, removing the visual shortcut. When evaluated on RETINA existing models show significantly degraded performance, confirming their reliance on the shortcut. Furthermore, we propose Multi-Image MultImodal Retriever (MIMIR), which enriches document embeddings by augmenting images of multiple related entities, effectively handling RETINA, unlike prior work that uses only a single image per document. Our experiments validate the limitations of existing benchmarks and demonstrate the effectiveness of RETINA and MIMIR. Our project is available at: Project Page.


### [66] [Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding](https://arxiv.org/abs/2511.22850)
*Keliang Liu, Zizhi Chen, Mingcheng Li, Jingqun Tang, Dingkang Yang, Lihua Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSLEUTHï¼Œä¸€ä¸ªç”¨äºé•¿æ–‡æ¡£ç†è§£çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚ç²¾ç‚¼è¿‡ç¨‹è¯†åˆ«å…³é”®å¤šæ¨¡æ€çº¿ç´¢å¹¶è¿‡æ»¤å†—ä½™ä¿¡æ¯ï¼Œä¸å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ç»“åˆæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å•é¡µæ–‡æ¡£ä»»åŠ¡ä¸­è¡¨ç°æœ‰æ•ˆï¼Œä½†åœ¨å¤„ç†é•¿æ–‡æ¡£æ—¶æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºçº¿ç´¢åˆ†æ•£åœ¨å¤šä¸ªé¡µé¢å’Œæ¨¡æ€ä¸­ï¼Œä¸”å†—é•¿è¾“å…¥ä¸­çš„å†—ä½™ä¼šæŸå®³æ¨¡å‹åˆ¤æ–­ï¼Œè€Œæ£€ç´¢å¢å¼ºç”Ÿæˆè™½ç„¶èƒ½è¿‡æ»¤é—®é¢˜ç›¸å…³å†…å®¹ï¼Œä½†æ£€ç´¢ç»“æœä»åŒ…å«å¤§é‡å†—ä½™ä¿¡æ¯ã€‚

**Method:** SLEUTHæ˜¯ä¸€ä¸ªæ¨¡å‹æ— å…³ä¸”å¯æ‰©å±•çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåè°ƒæ£€ç´¢å™¨å’Œå››ä¸ªåä½œæ™ºèƒ½ä½“æ‰§è¡Œä»ç²—åˆ°ç²¾çš„å¤„ç†æµç¨‹ï¼Œè¯¥æ¡†æ¶è¯†åˆ«æ£€ç´¢é¡µé¢ä¸­çš„å…³é”®æ–‡æœ¬å’Œè§†è§‰çº¿ç´¢ï¼Œè¿‡æ»¤è¡¨æ ¼å’Œå›¾è¡¨ç­‰æ˜¾è‘—è§†è§‰è¯æ®ï¼Œåˆ†ææŸ¥è¯¢ä»¥åˆ¶å®šæ¨ç†ç­–ç•¥ï¼Œæœ€ç»ˆåˆæˆç»è¿‡æç‚¼çš„è¯æ®å¯†é›†å‹å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä»¥ç”Ÿæˆæœ€ç»ˆé¢„æµ‹ã€‚

**Result:** å½“ä¸å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸»å¹²ç»“åˆæ—¶ï¼ŒSLEUTHåœ¨å¤šä¸ªé•¿æ–‡æ¡£åŸºå‡†æµ‹è¯•ä¸­æŒç»­æå‡æ€§èƒ½å¹¶å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ¶ˆèç ”ç©¶éªŒè¯äº†æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ç¡®è®¤äº†åˆ†å±‚ç²¾ç‚¼èŒƒå¼çš„ä¼˜åŠ¿ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶åœ¨è§£å†³é•¿æ–‡æ¡£ç†è§£æŒ‘æˆ˜ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡åˆ†å±‚ç²¾ç‚¼è¿‡ç¨‹å¤„ç†å¤šæ¨¡æ€çº¿ç´¢å’Œå†—ä½™é—®é¢˜ï¼Œä¸ºæ–‡æ¡£ç†è§£é¢†åŸŸæä¾›äº†å¯æ‰©å±•ä¸”æ¨¡å‹æ— å…³çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶éªŒè¯äº†æ™ºèƒ½ä½“åä½œåœ¨å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Document understanding is a long standing practical task. Vision Language Models (VLMs) have gradually become a primary approach in this domain, demonstrating effective performance on single page tasks. However, their effectiveness diminishes when handling long documents. In such scenarios, clues are often scattered across multiple pages and modalities, and redundancy from lengthy inputs can impair the models judgment. While retrieval augmented generation mitigates this issue by filtering for question relevant content, the retrieved results still contain substantial redundancy. To address these limitations, we propose SLEUTH, a multi agent framework. Concretely, SLEUTH orchestrates a retriever and four collaborative agents in a coarse to fine process. The framework identifies key textual and visual clues within the retrieved pages, filters for salient visual evidence such as tables and charts, and analyzes the query to devise a reasoning strategy. It ultimately synthesizes a distilled, evidence dense multimodal context to generate the final prediction. SLEUTH is model agnostic and scalable. When paired with advanced VLM backbones, it consistently improves performance on multiple long document benchmarks, achieving state of the art results. Ablation studies verify each modules effectiveness and confirm the benefits of our hierarchical refinement paradigm.


### [67] [CoordSpeaker: Exploiting Gesture Captioning for Coordinated Caption-Empowered Co-Speech Gesture Generation](https://arxiv.org/abs/2511.22863)
*Fengyi Fang, Sicheng Yang, Wenming Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CoordSpeakeræ¡†æ¶ï¼Œé€šè¿‡æ‰‹åŠ¿æè¿°ç”Ÿæˆå’Œå¤šç²’åº¦æ§åˆ¶ï¼Œè§£å†³äº†ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆä¸­è¯­ä¹‰å…ˆéªŒç¼ºå¤±å’Œè·¨æ¨¡æ€åè°ƒæ§åˆ¶çš„æŒ‘æˆ˜ï¼Œå®ç°äº†é«˜è´¨é‡ã€è¯­ä¹‰ä¸€è‡´çš„æ‰‹åŠ¿åˆæˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆæ–¹æ³•é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¸€æ˜¯ç”±äºæ‰‹åŠ¿æ•°æ®é›†ä¸­ç¼ºä¹æè¿°æ€§æ–‡æœ¬æ ‡æ³¨å¯¼è‡´çš„è¯­ä¹‰å…ˆéªŒå·®è·ï¼ŒäºŒæ˜¯éš¾ä»¥å®ç°æ‰‹åŠ¿ç”Ÿæˆçš„åè°ƒå¤šæ¨¡æ€æ§åˆ¶ï¼Œç‰¹åˆ«æ˜¯æ— æ³•ç”Ÿæˆæ–‡æœ¬é©±åŠ¨çš„éè‡ªå‘æ‰‹åŠ¿ï¼ˆå¦‚è¯´è¯æ—¶é èº¬ï¼‰ã€‚

**Method:** æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¦–å…ˆæå‡ºæ–°é¢–çš„æ‰‹åŠ¿æè¿°æ¡†æ¶ï¼Œåˆ©ç”¨è¿åŠ¨-è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šç²’åº¦æè¿°æ€§æ ‡é¢˜æ¥å¼¥åˆè¯­ä¹‰å…ˆéªŒå·®è·ï¼›ç„¶åæ„å»ºæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨ç»Ÿä¸€è·¨æ•°æ®é›†è¿åŠ¨è¡¨ç¤ºå’Œåˆ†å±‚æ§åˆ¶å»å™ªå™¨ï¼Œå®ç°é«˜åº¦å¯æ§çš„åè°ƒæ‰‹åŠ¿ç”Ÿæˆã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½ç”Ÿæˆé«˜è´¨é‡æ‰‹åŠ¿ï¼Œæ—¢ä¸è¯­éŸ³èŠ‚å¥åŒæ­¥ï¼Œåˆä¸ä»»æ„æ ‡é¢˜è¯­ä¹‰ä¸€è‡´ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•å®ç°äº†æ›´ä¼˜æ€§èƒ½ä¸”æ•ˆç‡æ›´é«˜ï¼Œåœ¨åè°ƒæ§åˆ¶å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢è¡¨ç°çªå‡ºã€‚

**Conclusion:** è¯¥ç ”ç©¶é¦–æ¬¡æ¢ç´¢æ‰‹åŠ¿ç†è§£å’Œæè¿°ç”Ÿæˆæ¥è§£å†³æ‰‹åŠ¿ç”Ÿæˆä¸­çš„è¯­ä¹‰å·®è·ï¼Œæä¾›äº†æ‰‹åŠ¿-æ–‡æœ¬åŒå‘æ˜ å°„çš„æ–°è§†è§’ï¼Œä¸ºå¯æ§ååŒæ‰‹åŠ¿åˆæˆå¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œæ˜¾è‘—æå‡äº†äººæœºäº¤äº’çš„è‡ªç„¶æ€§å’Œè¡¨ç°åŠ›ã€‚

---

#### ğŸ“„ Abstract
Co-speech gesture generation has significantly advanced human-computer interaction, yet speaker movements remain constrained due to the omission of text-driven non-spontaneous gestures (e.g., bowing while talking). Existing methods face two key challenges: 1) the semantic prior gap due to the lack of descriptive text annotations in gesture datasets, and 2) the difficulty in achieving coordinated multimodal control over gesture generation. To address these challenges, this paper introduces CoordSpeaker, a comprehensive framework that enables coordinated caption-empowered co-speech gesture synthesis. Our approach first bridges the semantic prior gap through a novel gesture captioning framework, leveraging a motion-language model to generate descriptive captions at multiple granularities. Building upon this, we propose a conditional latent diffusion model with unified cross-dataset motion representation and a hierarchically controlled denoiser to achieve highly controlled, coordinated gesture generation. CoordSpeaker pioneers the first exploration of gesture understanding and captioning to tackle the semantic gap in gesture generation while offering a novel perspective of bidirectional gesture-text mapping. Extensive experiments demonstrate that our method produces high-quality gestures that are both rhythmically synchronized with speeches and semantically coherent with arbitrary captions, achieving superior performance with higher efficiency compared to existing approaches.


### [68] [CNN-Based Framework for Pedestrian Age and Gender Classification Using Far-View Surveillance in Mixed-Traffic Intersections](https://arxiv.org/abs/2511.22873)
*Shisir Shahriar Arif, Md. Muhtashim Shahrier, Nazmul Haque, Md Asif Raihan, Md. Hadiuzzaman*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ¡†æ¶ï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œä»è¿œè§†è§’è·¯å£ç›‘æ§è§†é¢‘ä¸­å®æ—¶åˆ†ç±»è¡Œäººçš„å¹´é¾„ç»„åˆ«å’Œæ€§åˆ«ï¼Œæ— éœ€ä¾èµ–é¢éƒ¨è¯†åˆ«æˆ–é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œä¸ºæ··åˆäº¤é€šç¯å¢ƒä¸‹çš„è¡Œäººå®‰å…¨ç›‘æµ‹æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åœ¨ä½æ”¶å…¥å’Œä¸­ç­‰æ”¶å…¥å›½å®¶çš„æ‹¥æŒ¤åŸå¸‚äº¤å‰å£ï¼Œè¡Œäººå®‰å…¨æ˜¯ä¸€ä¸ªç´§è¿«é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¤šæ¨¡å¼äº¤é€šå’Œç¼ºä¹æ­£å¼æ§åˆ¶çš„åŸºç¡€è®¾æ–½ç¯å¢ƒã€‚å¹´é¾„å’Œæ€§åˆ«ç­‰äººå£ç»Ÿè®¡å› ç´ æ˜¾è‘—å½±å“è¡Œäººè„†å¼±æ€§ï¼Œä½†å®æ—¶ç›‘æ§ç³»ç»Ÿå¾ˆå°‘æ•è·æ­¤ç±»ä¿¡æ¯ï¼Œå¯¼è‡´ä¼ ç»Ÿäº¤é€šæ•°æ®ä¸­ç¼ºä¹å…³é”®çš„äººå£ç»Ÿè®¡æ´å¯Ÿã€‚

**Method:** ç ”ç©¶æå‡ºäº†ä¸€ç§æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œä»è¿œè§†è§’è·¯å£ç›‘æ§è§†é¢‘ä¸­åˆ†ç±»è¡Œäººå¹´é¾„ç»„åˆ«å’Œæ€§åˆ«ã€‚è¯¥æ–¹æ³•å°†åˆ†ç±»æ„å»ºä¸ºç»Ÿä¸€çš„å…­ç±»é—®é¢˜ï¼ŒåŒºåˆ†æˆå¹´ã€é’å°‘å¹´å’Œå„¿ç«¥è¡Œäººä»¥åŠç”·æ€§å’Œå¥³æ€§ï¼ŒåŸºäºå…¨èº«è§†è§‰çº¿ç´¢è€Œéé¢éƒ¨è¯†åˆ«ã€‚ç ”ç©¶å®ç°äº†ä¸¤ç§CNNæ¶æ„ï¼šåœ¨ImageNetä¸Šé¢„è®­ç»ƒçš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œResNet50ï¼Œä»¥åŠä¸ºè®¡ç®—æ•ˆç‡ä¼˜åŒ–çš„è‡ªå®šä¹‰è½»é‡çº§CNNã€‚é€šè¿‡å…«ç§æ¨¡å‹å˜ä½“æ¢ç´¢äº†æ± åŒ–ç­–ç•¥å’Œä¼˜åŒ–å™¨çš„ä¸åŒç»„åˆã€‚

**Result:** ResNet50ç»“åˆæœ€å¤§æ± åŒ–å’ŒSGDä¼˜åŒ–å™¨å®ç°äº†æœ€é«˜å‡†ç¡®ç‡ï¼ˆ86.19%ï¼‰ï¼Œè€Œè‡ªå®šä¹‰CNNåœ¨å‚æ•°æ›´å°‘ã€è®­ç»ƒæ›´å¿«çš„æƒ…å†µä¸‹è¡¨ç°ç›¸å½“ï¼ˆ84.15%ï¼‰ã€‚æ¨¡å‹çš„é«˜æ•ˆè®¾è®¡ä½¿å…¶èƒ½å¤Ÿåœ¨æ ‡å‡†ç›‘æ§è§†é¢‘æµä¸Šå®ç°å®æ—¶æ¨ç†ã€‚æ•°æ®æ”¶é›†è‡ªå­ŸåŠ æ‹‰å›½è¾¾å¡çš„ä¸‰ä¸ªé«˜é£é™©äº¤å‰å£ï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨çœŸå®æ··åˆäº¤é€šç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ã€‚

**Conclusion:** è¯¥æ¡†æ¶ä¸ºä»ä¸šè€…æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€ç»æµé«˜æ•ˆçš„å·¥å…·ï¼Œå¯åˆ©ç”¨ç°æœ‰æ‘„åƒå¤´åŸºç¡€è®¾æ–½ç›‘æµ‹äº¤å‰å£è¡Œäººäººå£ç»Ÿè®¡ç‰¹å¾ã€‚å…¶è¾“å‡ºå¯ç”¨äºä¼˜åŒ–äº¤å‰å£è®¾è®¡ã€è°ƒæ•´ä¿¡å·æ—¶åºï¼Œå¹¶ä¸ºå„¿ç«¥æˆ–è€å¹´äººç­‰å¼±åŠ¿ç¾¤ä½“å®æ–½æœ‰é’ˆå¯¹æ€§çš„å®‰å…¨å¹²é¢„æªæ–½ã€‚é€šè¿‡æä¾›ä¼ ç»Ÿäº¤é€šæ•°æ®ä¸­å¸¸ç¼ºå¤±çš„äººå£ç»Ÿè®¡æ´å¯Ÿï¼Œè¯¥æ¡†æ¶æ”¯æŒåœ¨æ··åˆäº¤é€šç¯å¢ƒä¸­è¿›è¡Œæ›´å…·åŒ…å®¹æ€§ã€æ•°æ®é©±åŠ¨çš„è§„åˆ’ã€‚

---

#### ğŸ“„ Abstract
Pedestrian safety remains a pressing concern in congested urban intersections, particularly in low- and middle-income countries where traffic is multimodal, and infrastructure often lacks formal control. Demographic factors like age and gender significantly influence pedestrian vulnerability, yet real-time monitoring systems rarely capture this information. To address this gap, this study proposes a deep learning framework that classifies pedestrian age group and gender from far-view intersection footage using convolutional neural networks (CNNs), without relying on facial recognition or high-resolution imagery. The classification is structured as a unified six-class problem, distinguishing adult, teenager, and child pedestrians for both males and females, based on full-body visual cues. Video data was collected from three high-risk intersections in Dhaka, Bangladesh. Two CNN architectures were implemented: ResNet50, a deep convolutional neural network pretrained on ImageNet, and a custom lightweight CNN optimized for computational efficiency. Eight model variants explored combinations of pooling strategies and optimizers. ResNet50 with Max Pooling and SGD achieved the highest accuracy (86.19%), while the custom CNN performed comparably (84.15%) with fewer parameters and faster training. The model's efficient design enables real-time inference on standard surveillance feeds. For practitioners, this system provides a scalable, cost-effective tool to monitor pedestrian demographics at intersections using existing camera infrastructure. Its outputs can shape intersection design, optimize signal timing, and enable targeted safety interventions for vulnerable groups such as children or the elderly. By offering demographic insights often missing in conventional traffic data, the framework supports more inclusive, data-driven planning in mixed-traffic environments.


### [69] [DM$^3$T: Harmonizing Modalities via Diffusion for Multi-Object Tracking](https://arxiv.org/abs/2511.22896)
*Weiran Li, Yeqiang Liu, Yijie Wei, Mina Han, Qiannan Guo, Zhenbo Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºDMÂ³Tï¼Œä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å¤šæ¨¡æ€å¤šç›®æ ‡è·Ÿè¸ªæ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£ç‰¹å¾å¯¹é½å®ç°å¯è§å…‰ä¸çƒ­çº¢å¤–æ¨¡æ€çš„æœ‰æ•ˆèåˆï¼Œåœ¨VT-MOTåŸºå‡†ä¸Šå®ç°äº†41.7 HOTAçš„æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤šç›®æ ‡è·Ÿè¸ªä¸­å¯è§å…‰ä¸çƒ­çº¢å¤–ä¿¡æ¯çš„æœ‰æ•ˆèåˆé¢ä¸´æŒ‘æˆ˜ï¼Œä¼ ç»Ÿçš„æ‹¼æ¥æˆ–ç›¸åŠ ç­‰ç®€å•ç­–ç•¥éš¾ä»¥å¼¥åˆç‰¹å¾è¡¨ç¤ºé—´çš„éçº¿æ€§åˆ†å¸ƒå·®å¼‚ï¼Œå®¹æ˜“å¯¼è‡´æ¨¡æ€å†²çªå¹¶é™ä½è·Ÿè¸ªç²¾åº¦ï¼Œéœ€è¦æ›´å…ˆè¿›çš„èåˆæ–¹æ³•æ¥è§£å†³è¿™ä¸€å…³é”®é—®é¢˜ã€‚

**Method:** æœ¬æ–‡æå‡ºDMÂ³Tæ¡†æ¶ï¼Œå°†å¤šæ¨¡æ€èåˆé‡æ–°å®šä¹‰ä¸ºè¿­ä»£ç‰¹å¾å¯¹é½è¿‡ç¨‹ï¼Œé€šè¿‡è·¨æ¨¡æ€æ‰©æ•£èåˆæ¨¡å—å®ç°è¿­ä»£çš„è·¨æ¨¡æ€åè°ƒï¼Œä½¿ä¸¤ç§æ¨¡æ€ç‰¹å¾ç›¸äº’å¼•å¯¼å¹¶æŠ•å½±åˆ°å…±äº«ä¸€è‡´çš„ç‰¹å¾æµå½¢ä¸Šï¼ŒåŒæ—¶å¼•å…¥å¯æ’æ‹”çš„æ‰©æ•£ç²¾ç‚¼å™¨å¢å¼ºç»Ÿä¸€ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶è®¾è®¡åˆ†å±‚è·Ÿè¸ªå™¨è‡ªé€‚åº”å¤„ç†ç½®ä¿¡åº¦ä¼°è®¡ã€‚

**Result:** åœ¨VT-MOTåŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDMÂ³Tæ–¹æ³•å®ç°äº†41.7 HOTAçš„æ€§èƒ½æŒ‡æ ‡ï¼Œç›¸å¯¹äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•è·å¾—äº†1.54%çš„ç›¸å¯¹æ€§èƒ½æå‡ï¼Œå°†ç›®æ ‡æ£€æµ‹ã€çŠ¶æ€ä¼°è®¡å’Œæ•°æ®å…³è”ç»Ÿä¸€åˆ°æ— éœ€å¤æ‚åå¤„ç†çš„åœ¨çº¿è·Ÿè¸ªæ¡†æ¶ä¸­ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹è¿­ä»£ç»†åŒ–æ€æƒ³åœ¨å¤šæ¨¡æ€èåˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºçš„æ¡†æ¶èƒ½å¤Ÿå­¦ä¹ äº’è¡¥ä¿¡æ¯å¹¶å®ç°æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´æ·±å±‚æ¬¡çš„èåˆï¼Œä¸ºé²æ£’è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†æ›´å‡†ç¡®å’Œæ—¶åºä¸€è‡´çš„ç›®æ ‡è½¨è¿¹ç”Ÿæˆæ–¹æ³•ã€‚

---

#### ğŸ“„ Abstract
Multi-object tracking (MOT) is a fundamental task in computer vision with critical applications in autonomous driving and robotics. Multimodal MOT that integrates visible light and thermal infrared information is particularly essential for robust autonomous driving systems. However, effectively fusing these heterogeneous modalities is challenging. Simple strategies like concatenation or addition often fail to bridge the significant non-linear distribution gap between their feature representations, which can lead to modality conflicts and degrade tracking accuracy. Drawing inspiration from the connection between multimodal MOT and the iterative refinement in diffusion models, this paper proposes DM$^3$T, a novel framework that reformulates multimodal fusion as an iterative feature alignment process to generate accurate and temporally coherent object trajectories. Our approach performs iterative cross-modal harmonization through a proposed Cross-Modal Diffusion Fusion (C-MDF) module. In this process, features from both modalities provide mutual guidance, iteratively projecting them onto a shared, consistent feature manifold. This enables the learning of complementary information and achieves deeper fusion compared to conventional methods. Additionally, we introduce a plug-and-play Diffusion Refiner (DR) to enhance and refine the unified feature representation. To further improve tracking robustness, we design a Hierarchical Tracker that adaptively handles confidence estimation. DM$^3$T unifies object detection, state estimation, and data association into a comprehensive online tracking framework without complex post-processing. Extensive experiments on the VT-MOT benchmark demonstrate that our method achieves 41.7 HOTA, representing a 1.54% relative improvement over existing state-of-the-art methods. The code and models are available at https://vranlee.github.io/DM-3-T/.


### [70] [From Points to Clouds: Learning Robust Semantic Distributions for Multi-modal Prompts](https://arxiv.org/abs/2511.22897)
*Weiran Li, Yeqiang Liu, Yijie Wei, Mina Han, Xin Liu, Zhenbo Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Points-to-Clouds (P2C)æ¡†æ¶ï¼Œé€šè¿‡å°†å¤šæ¨¡æ€æç¤ºå­¦ä¹ é‡æ„ä¸ºåŠ¨æ€å»å™ªä»»åŠ¡ï¼Œå°†ä¼ ç»Ÿçš„é™æ€ç‚¹è¡¨ç¤ºå­¦ä¹ æ‰©å±•ä¸ºè¯­ä¹‰äº‘åˆ†å¸ƒå­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–°ç±»åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ–¹æ³•å—é™äºä¼˜åŒ–å•ä¸€é™æ€ç‚¹è¡¨ç¤ºï¼Œè¿™ç§èŒƒå¼å­˜åœ¨å›ºæœ‰è„†å¼±æ€§ï¼Œå®¹æ˜“åœ¨åŸºç¡€ç±»åˆ«ä¸Šè¿‡æ‹Ÿåˆï¼Œä¸”åœ¨æ–°ç±»åˆ«æˆ–æ¨¡ç³Šç±»åˆ«ä¸Šæ³›åŒ–èƒ½åŠ›å·®ã€‚ç ”ç©¶æŒ‘æˆ˜äº†è¿™ç§ç‚¹èŒƒå¼ï¼Œè®¤ä¸ºé²æ£’æ³›åŒ–éœ€è¦å­¦ä¹ è¯­ä¹‰äº‘ï¼ˆå³åµŒå…¥ç©ºé—´ä¸Šçš„åˆ†å¸ƒï¼‰ã€‚

**Method:** æå‡ºäº†Points-to-Clouds (P2C)æ¡†æ¶ï¼Œå—æ‰©æ•£æ¨¡å‹å¯å‘ï¼Œå°†æç¤ºå­¦ä¹ é‡æ„ä¸ºåŠ¨æ€å»å™ªä»»åŠ¡ã€‚æ ¸å¿ƒæ˜¯åŒé‡å»å™ªæœºåˆ¶ï¼šåŠ¨æ€æç¤ºå»å™ªæœºåˆ¶é€šè¿‡å¤æ‚çš„é€€ç«å™ªå£°æ‰°åŠ¨æ–‡æœ¬æç¤ºä»¥å­¦ä¹ æ›´å¹³æ»‘çš„è¯­ä¹‰æ™¯è§‚ï¼›è¾…åŠ©çš„V-Læ˜ å°„å™¨å»å™ªæŸå¤±å°†æ˜ å°„å™¨é‡æ–°ä»»åŠ¡åŒ–ä¸ºå»å™ªè‡ªç¼–ç å™¨ï¼Œå¼ºåˆ¶å…¶ä»å™ªå£°æ–‡æœ¬è¾“å…¥é‡æ„å¹²å‡€çš„è§†è§‰æç¤ºï¼Œç¡®ä¿é²æ£’çš„è·¨æ¨¡æ€å¯¹é½ã€‚

**Result:** åœ¨11ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒP2Cå§‹ç»ˆä¼˜äºå¼ºåŸºçº¿æ–¹æ³•ã€‚åœ¨åŸºç¡€åˆ°æ–°ç±»åˆ«çš„æ³›åŒ–åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†79.7%çš„è°ƒå’Œå¹³å‡å€¼ï¼Œç›¸å¯¹äºåŸºçº¿æœ‰1.4%çš„ç›¸å¯¹æ”¹è¿›ã€‚

**Conclusion:** ç ”ç©¶è¯æ˜äº†å°†å¤šæ¨¡æ€æç¤ºå­¦ä¹ ä»é™æ€ç‚¹è¡¨ç¤ºæ‰©å±•åˆ°è¯­ä¹‰äº‘åˆ†å¸ƒçš„æœ‰æ•ˆæ€§ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„é²æ£’æ³›åŒ–æä¾›äº†æ–°èŒƒå¼ã€‚P2Cæ¡†æ¶é€šè¿‡å»å™ªæœºåˆ¶å­¦ä¹ æ›´å¹³æ»‘çš„è¯­ä¹‰æ™¯è§‚ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æœªè§ç±»åˆ«ä¸Šçš„æ€§èƒ½ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€å­¦ä¹ ç ”ç©¶æä¾›äº†é‡è¦æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Multimodal Prompt Learning (MPL) has emerged as a pivotal technique for adapting large-scale Visual Language Models (VLMs). However, current MPL methods are fundamentally limited by their optimization of a single, static point representation. This paradigm is inherently brittle, leads to overfitting on base classes, and generalizes poorly to novel or ambiguous categories. We challenge this point paradigm, proposing that robust generalization requires learning a semantic cloud (i.e., a distribution over the embedding space). To achieve this, we introduce Points-to-Clouds (P2C), a novel framework inspired by diffusion models that reframes prompt learning as a dynamic denoising task. At the core of P2C is a dual denoising mechanism: a Dynamic Prompt Denoising (DPD) mechanism perturbs text prompts with sophisticated, annealed noise to learn a smoother semantic landscape, while an auxiliary V-L Mapper denoising loss re-tasks the mapper as a denoising autoencoder. This forces the mapper to reconstruct clean visual prompts from noisy text inputs, ensuring robust cross-modal alignment. Extensive experiments across 11 datasets demonstrate that P2C consistently outperforms strong baselines. On the base-to-novel generalization benchmark, our method achieves a Harmonic Mean of 79.7%, representing a relative improvement of 1.4% over the baseline. The code and models are available at https://vranlee.github.io/P2C/.


### [71] [See, Rank, and Filter: Important Word-Aware Clip Filtering via Scene Understanding for Moment Retrieval and Highlight Detection](https://arxiv.org/abs/2511.22906)
*YuEun Lee, Jung Uk Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†é¢‘æ—¶åˆ»æ£€ç´¢ä¸é«˜å…‰æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«æŸ¥è¯¢ä¸­çš„é‡è¦è¯æ±‡å®ç°ç»†ç²’åº¦è§†é¢‘ç‰‡æ®µè¿‡æ»¤ï¼Œæ˜¾è‘—æå‡äº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•æ•´åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åœºæ™¯ç†è§£èƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†ç‰¹å¾å¢å¼ºæ¨¡å—å’ŒåŸºäºæ’åçš„è¿‡æ»¤æ¨¡å—ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†é¢‘æ—¶åˆ»æ£€ç´¢ä¸é«˜å…‰æ£€æµ‹æ–¹æ³•å°†æ•´ä¸ªæ–‡æœ¬æŸ¥è¯¢å’Œè§†é¢‘ç‰‡æ®µè§†ä¸ºé»‘ç›’ï¼Œå¿½è§†äº†å•ä¸ªè¯æ±‡çš„é‡è¦æ€§ï¼Œè¿™é˜»ç¢äº†å¯¹è§†é¢‘å†…å®¹çš„ä¸Šä¸‹æ–‡ç†è§£ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œé€šè¿‡è¯†åˆ«æŸ¥è¯¢ä¸­çš„é‡è¦è¯æ±‡æ¥å®ç°æ›´ç²¾ç»†çš„è§†é¢‘ç‰‡æ®µè¿‡æ»¤ã€‚

**Method:** æœ¬æ–‡æ–¹æ³•é€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ•´åˆå›¾åƒ-æ–‡æœ¬åœºæ™¯ç†è§£ï¼Œå¢å¼ºè§†é¢‘ç‰‡æ®µçš„è¯­ä¹‰ç†è§£ã€‚æ ¸å¿ƒåŒ…æ‹¬ç‰¹å¾å¢å¼ºæ¨¡å—ç”¨äºæ•è·æŸ¥è¯¢ä¸­çš„é‡è¦è¯æ±‡ï¼Œä»¥åŠåŸºäºæ’åçš„è¿‡æ»¤æ¨¡å—ç”¨äºæ ¹æ®è¿™äº›é‡è¦è¯æ±‡çš„ç›¸å…³æ€§è¿­ä»£ç»†åŒ–è§†é¢‘ç‰‡æ®µï¼Œå®ç°ç»†ç²’åº¦çš„ç‰‡æ®µè¿‡æ»¤ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘æ—¶åˆ»æ£€ç´¢å’Œé«˜å…‰æ£€æµ‹ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå–å¾—äº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚å…·ä½“å®éªŒéªŒè¯äº†æ‰€ææ¨¡å—çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†ç»†ç²’åº¦è¯æ±‡å…³æ³¨æœºåˆ¶å¯¹æå‡è§†é¢‘ç†è§£ä»»åŠ¡æ€§èƒ½çš„é‡è¦æ€§ã€‚

**Conclusion:** æœ¬ç ”ç©¶å¼ºè°ƒäº†åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å…³æ³¨æŸ¥è¯¢è¯æ±‡ç²’åº¦çš„é‡è¦æ€§ï¼Œä¸ºè§†é¢‘æ—¶åˆ»æ£€ç´¢å’Œé«˜å…‰æ£€æµ‹æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚æ‰€æå‡ºçš„ç»†ç²’åº¦è¿‡æ»¤æ¡†æ¶å±•ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸ä¸“é—¨è®¾è®¡çš„è¿‡æ»¤æ¨¡å—ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥è§†é¢‘è¯­ä¹‰ç†è§£ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

---

#### ğŸ“„ Abstract
Video moment retrieval (MR) and highlight detection (HD) with natural language queries aim to localize relevant moments and key highlights in a video clips. However, existing methods overlook the importance of individual words, treating the entire text query and video clips as a black-box, which hinders contextual understanding. In this paper, we propose a novel approach that enables fine-grained clip filtering by identifying and prioritizing important words in the query. Our method integrates image-text scene understanding through Multimodal Large Language Models (MLLMs) and enhances the semantic understanding of video clips. We introduce a feature enhancement module (FEM) to capture important words from the query and a ranking-based filtering module (RFM) to iteratively refine video clips based on their relevance to these important words. Extensive experiments demonstrate that our approach significantly outperforms existing state-of-the-art methods, achieving superior performance in both MR and HD tasks. Our code is available at: https://github.com/VisualAIKHU/SRF.


### [72] [Contrastive Heliophysical Image Pretraining for Solar Dynamics Observatory Records](https://arxiv.org/abs/2511.22958)
*Shiyu Shen, Zhe Gao, Taifeng Chai, Yang Huang, Bin Pan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†SolarCHIPï¼Œä¸€ä¸ªä¸“ä¸ºå¤ªé˜³åŠ¨åŠ›å­¦å¤©æ–‡å°å¤šä»ªå™¨è§‚æµ‹è®¾è®¡çš„å¯¹æ¯”é¢„è®­ç»ƒè§†è§‰éª¨å¹²ç½‘ç»œå®¶æ—ï¼Œé€šè¿‡å¤šç²’åº¦å¯¹æ¯”å­¦ä¹ è§£å†³å¤ªé˜³å›¾åƒåˆ†æä¸­çš„æ¨¡æ€å·®å¼‚ã€ç±»é—´ç›¸ä¼¼æ€§å’Œç±»å†…å˜å¼‚æ€§é—®é¢˜ï¼Œå¹¶åœ¨è·¨æ¨¡æ€ç¿»è¯‘å’Œè€€æ–‘åˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¤ªé˜³å›¾åƒåˆ†æä¸­é€šå¸¸ä»å¤´è®­ç»ƒä»»åŠ¡ç‰¹å®šç¼–ç å™¨æˆ–ä¾èµ–å¿½ç•¥å¤ªé˜³æ•°æ®ç‹¬ç‰¹æ€§çš„è‡ªç„¶å›¾åƒé¢„è®­ç»ƒï¼Œè€Œå¤ªé˜³åŠ¨åŠ›å­¦å¤©æ–‡å°æ•°æ®é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šè·¨AIAå’ŒHMIä»ªå™¨çš„å¤šæ¨¡æ€æ„ŸçŸ¥ã€ç”±äºç¼“æ…¢æ—¶é—´æ¼”åŒ–å¯¼è‡´çš„å¼±ç±»é—´å¯åˆ†ç¦»æ€§ï¼Œä»¥åŠå…·æœ‰ç¨€ç–æ´»åŠ¨ä¿¡å·çš„å¼ºç±»å†…å˜å¼‚æ€§ã€‚

**Method:** SolarCHIPé‡‡ç”¨å¤šç²’åº¦å¯¹æ¯”é¢„è®­ç»ƒæ¡†æ¶ï¼Œè”åˆå¯¹é½ä¸‰ä¸ªå±‚æ¬¡çš„ç‰¹å¾ï¼šè·¨å…±æ—¶AIA-HMIå¯¹çš„å…¨å±€ç±»åˆ«æ ‡è®°ä»¥å¢å¼ºæ—¶é—´åŒºåˆ†èƒ½åŠ›ï¼Œå›ºå®šç©ºé—´ç´¢å¼•çš„å±€éƒ¨è¡¥ä¸æ ‡è®°ä»¥å¼ºåˆ¶ä½ç½®ä¸€è‡´ä¸”æ¨¡æ€ä¸å˜çš„ç‰¹å¾ï¼Œä»¥åŠä¸åŒç©ºé—´ä½ç½®çš„æ ·æœ¬å†…è¡¥ä¸ä»¥ä¿ç•™ç»†ç²’åº¦ç©ºé—´ç»“æ„ã€‚è¯¥æ–¹æ³•è®­ç»ƒäº†åŸºäºCNNå’Œè§†è§‰Transformerçš„è‡ªç¼–ç å™¨ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒSolarCHIPåœ¨è·¨æ¨¡æ€ç¿»è¯‘ï¼ˆé€šè¿‡ControlNetå®ç°HMIä¸AIAé€šé“é—´è½¬æ¢ï¼‰å’Œå…¨ç›˜è€€æ–‘åˆ†ç±»ä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡è®°æ•°æ®æœ‰é™çš„ä½èµ„æºè®¾ç½®ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚æ¶ˆèç ”ç©¶è¯å®æ¯ä¸ªå¯¹æ¯”ç»„ä»¶åœ¨ä¸åŒç²’åº¦ä¸Šè´¡çŒ®äº†å¿…è¦çš„åˆ¤åˆ«èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºæ—¥çƒç‰©ç†å­¦ç¤¾åŒºæä¾›äº†ä¸€ä¸ªå®ç”¨ã€å³æ’å³ç”¨çš„ç‰¹å¾æå–å™¨ï¼Œé€šè¿‡å…¬å¼€é¢„è®­ç»ƒæƒé‡å’Œè®­ç»ƒä»£ç ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—éœ€æ±‚ï¼Œæé«˜äº†æ ‡ç­¾æ•ˆç‡ï¼Œå¹¶ä¸ºå¤šæ ·åŒ–çš„å¤ªé˜³æˆåƒåº”ç”¨å»ºç«‹äº†å¯é‡ç”¨çš„åŸºç¡€ã€‚å¤šç²’åº¦å¯¹æ¯”å­¦ä¹ æ–¹æ³•æœ‰æ•ˆè§£å†³äº†å¤ªé˜³å›¾åƒç‰¹æœ‰çš„æ¨¡æ€å¯¹é½å’Œç‰¹å¾å­¦ä¹ æŒ‘æˆ˜ã€‚

---

#### ğŸ“„ Abstract
Deep learning has revolutionized solar image analysis, yet most approaches train task-specific encoders from scratch or rely on natural-image pretraining that ignores the unique characteristics of Solar Dynamics Observatory (SDO) data. We introduce SolarCHIP, a family of contrastively pretrained visual backbones tailored to multi-instrument SDO observations. SolarCHIP addresses three key challenges in solar imaging: multimodal sensing across AIA and HMI instruments, weak inter-class separability due to slow temporal evolution, and strong intra-class variability with sparse activity signals. Our pretraining framework employs a multi-granularity contrastive objective that jointly aligns (1) global class tokens across co-temporal AIA-HMI pairs to enhance temporal discrimination, (2) local patch tokens at fixed spatial indices to enforce position-consistent, modality-invariant features, and (3) intra-sample patches across different spatial locations to preserve fine-grained spatial structure. We train both CNN- and Vision Transformer-based autoencoders and demonstrate their effectiveness on two downstream tasks: cross-modal translation between HMI and AIA passbands via ControlNet, and full-disk flare classification. Experimental results show that SolarCHIP achieves state-of-the-art performance across both tasks, with particularly strong gains in low-resource settings where labeled data is limited. Ablation studies confirm that each contrastive component contributes essential discriminative capacity at different granularities. By publicly releasing pretrained weights and training code, we provide the heliophysics community with a practical, plug-and-play feature extractor that reduces computational requirements, improves label efficiency, and establishes a reusable foundation for diverse solar imaging applications.


### [73] [HMR3D: Hierarchical Multimodal Representation for 3D Scene Understanding with Large Vision-Language Model](https://arxiv.org/abs/2511.22961)
*Chen Li, Eric Peh, Basura Fernando*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†å±‚å¤šæ¨¡æ€è¡¨ç¤ºæ–¹æ³•ï¼Œç”¨äº3Dåœºæ™¯æ¨ç†ï¼Œé€šè¿‡åœ¨è¾“å…¥ç©ºé—´æ˜¾å¼å¯¹é½å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨å¤šè§†è§’å›¾åƒå’ŒåŒ…å«3Dåæ ‡çš„æ–‡æœ¬æè¿°æ¥æå‡æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„3Dåœºæ™¯ç†è§£æ–¹æ³•é€šå¸¸å°†3Dåœºæ™¯ç‰¹å¾ä¸VLMçš„åµŒå…¥ç©ºé—´å¯¹é½ï¼Œä½†è¿™ç§éšå¼å¯¹é½ç”±äº3Dæ•°æ®ç¨€ç¼ºå’Œ3Dç¯å¢ƒä¸­ç©ºé—´å…³ç³»çš„å›ºæœ‰å¤æ‚æ€§ï¼Œå¾€å¾€å¯¼è‡´æ¬¡ä¼˜æ€§èƒ½ã€‚

**Method:** æå‡ºäº†ä¸€ç§åˆ†å±‚å¤šæ¨¡æ€è¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡åœ¨è¾“å…¥ç©ºé—´æ˜¾å¼å¯¹é½VLMï¼Œåˆ©ç”¨å¤šè§†è§’å›¾åƒï¼ˆåŒ…æ‹¬ä¿¯è§†å›¾å’Œå››ä¸ªæ–¹å‘è§†å›¾ï¼‰å’ŒåŒ…å«æ£€æµ‹å¯¹è±¡3Dåæ ‡çš„æ–‡æœ¬æè¿°æ¥æ•è·ç©ºé—´å…³ç³»ï¼Œå¹¶å¼•å…¥åˆ†å±‚ç‰¹å¾è¡¨ç¤ºï¼Œå°†å›¾åƒå—çº§ç‰¹å¾èšåˆä¸ºè§†å›¾çº§å’Œåœºæ™¯çº§è¡¨ç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å±€éƒ¨å’Œå…¨å±€åœºæ™¯ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ¨ç†ã€‚

**Result:** åœ¨æƒ…å¢ƒåŒ–3Dé—®ç­”å’Œé€šç”¨3Dé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨3Dåœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼ŒéªŒè¯äº†æ˜¾å¼è¾“å…¥ç©ºé—´å¯¹é½å’Œåˆ†å±‚å¤šæ¨¡æ€è¡¨ç¤ºçš„ä¼˜è¶Šæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡æ˜¾å¼è¾“å…¥ç©ºé—´å¯¹é½å’Œåˆ†å±‚å¤šæ¨¡æ€è¡¨ç¤ºå¯ä»¥æ˜¾è‘—æå‡3Dåœºæ™¯æ¨ç†æ€§èƒ½ï¼Œä¸º3Dåœºæ™¯ç†è§£æä¾›äº†æ–°çš„æœ‰æ•ˆæ–¹æ³•ï¼ŒåŒæ—¶å¼ºè°ƒäº†ç»¼åˆåˆ©ç”¨å¤šè§†è§’è§†è§‰ä¿¡æ¯å’ŒåŒ…å«ç©ºé—´åæ ‡çš„æ–‡æœ¬æè¿°å¯¹äºæ•è·å¤æ‚3Dç©ºé—´å…³ç³»çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Recent advances in large vision-language models (VLMs) have shown significant promise for 3D scene understanding. Existing VLM-based approaches typically align 3D scene features with the VLM's embedding space. However, this implicit alignment often yields suboptimal performance due to the scarcity of 3D data and the inherent complexity of spatial relationships in 3D environments. To address these limitations, we propose a novel hierarchical multimodal representation for 3D scene reasoning that explicitly aligns with VLMs at the input space by leveraging both multi-view images and text descriptions. The text descriptions capture spatial relationships by referencing the 3D coordinates of detected objects, while the multi-view images include a top-down perspective and four directional views (forward, left, right, and backward), ensuring comprehensive scene coverage. Additionally, we introduce a hierarchical feature representation that aggregates patch-level image features into view-level and scene-level representations, enabling the model to reason over both local and global scene context. Experimental results on both situated 3D Q&A and general 3D Q&A benchmarks demonstrate the effectiveness of our approach.


### [74] [MultiBanana: A Challenging Benchmark for Multi-Reference Text-to-Image Generation](https://arxiv.org/abs/2511.22989)
*Yuta Oshima, Daiki Miyake, Kohsei Matsutani, Yusuke Iwasawa, Masahiro Suzuki, Yutaka Matsuo, Hiroki Furuta*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MultiBananaåŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°å¤šå‚è€ƒå›¾åƒç”Ÿæˆä¸ç¼–è¾‘æ¨¡å‹çš„æ€§èƒ½è¾¹ç•Œï¼Œé€šè¿‡è¦†ç›–äº”ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šå‚è€ƒåœºæ™¯æ¥æ­ç¤ºç°æœ‰æ¨¡å‹çš„ä¼˜åŠ¿ä¸å±€é™ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹è™½å·²å…·å¤‡å¤šå‚è€ƒç”Ÿæˆä¸ç¼–è¾‘èƒ½åŠ›ï¼Œä½†å½“å‰åŸºå‡†æ•°æ®é›†ä¸»è¦å…³æ³¨å•å‚è€ƒæˆ–å°‘é‡å‚è€ƒå›¾åƒåœºæ™¯ï¼Œæ— æ³•ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒå¤šå‚è€ƒæ¡ä»¶ä¸‹çš„æ€§èƒ½è¿›å±•ä¸å¼±ç‚¹ã€‚æ­¤å¤–ï¼Œç°æœ‰ä»»åŠ¡å®šä¹‰è¾ƒä¸ºæ¨¡ç³Šï¼Œé€šå¸¸å±€é™äº"ç¼–è¾‘ä»€ä¹ˆ"æˆ–"å‚è€ƒæ•°é‡"ç­‰ç®€å•ç»´åº¦ï¼Œæœªèƒ½æ•æ‰å¤šå‚è€ƒè®¾ç½®çš„å†…åœ¨éš¾åº¦ã€‚

**Method:** æœ¬æ–‡è®¾è®¡äº†MultiBananaåŸºå‡†æ•°æ®é›†ï¼Œé€šè¿‡å¤§è§„æ¨¡è¦†ç›–äº”ç§å¤šå‚è€ƒç‰¹å®šé—®é¢˜æ¥è¯„ä¼°æ¨¡å‹èƒ½åŠ›è¾¹ç•Œï¼šå‚è€ƒæ•°é‡å˜åŒ–ã€å‚è€ƒé—´é¢†åŸŸä¸åŒ¹é…ï¼ˆå¦‚ç…§ç‰‡ä¸åŠ¨æ¼«ï¼‰ã€å‚è€ƒä¸ç›®æ ‡åœºæ™¯å°ºåº¦ä¸åŒ¹é…ã€å‚è€ƒåŒ…å«ç½•è§æ¦‚å¿µï¼ˆå¦‚çº¢è‰²é¦™è•‰ï¼‰ä»¥åŠå¤šè¯­è¨€æ–‡æœ¬å‚è€ƒæ¸²æŸ“ã€‚è¯¥åŸºå‡†æ—¨åœ¨å»ºç«‹æ ‡å‡†åŒ–è¯„ä¼°åŸºç¡€ï¼Œæ¨åŠ¨å¤šå‚è€ƒå›¾åƒç”Ÿæˆé¢†åŸŸçš„å…¬å¹³æ¯”è¾ƒã€‚

**Result:** é€šè¿‡å¯¹å¤šç§æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨å¤šå‚è€ƒåœºæ™¯ä¸‹çš„ä¼˜è¶Šæ€§èƒ½ã€å…¸å‹å¤±è´¥æ¨¡å¼ä»¥åŠéœ€è¦æ”¹è¿›çš„é¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨ä¸åŒå¤šå‚è€ƒæ¡ä»¶ä¸‹è¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢†åŸŸä¸åŒ¹é…å’Œç½•è§æ¦‚å¿µå¤„ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾å±€é™æ€§ã€‚

**Conclusion:** MultiBananaåŸºå‡†çš„å¼•å…¥ä¸ºå¤šå‚è€ƒå›¾åƒç”Ÿæˆç ”ç©¶æä¾›äº†ç³»ç»Ÿè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«æ¨¡å‹çš„å…·ä½“å¼±ç‚¹ä¸ºæœªæ¥æ”¹è¿›æŒ‡æ˜äº†æ–¹å‘ã€‚è¯¥å¼€æ”¾åŸºå‡†å°†æ¨åŠ¨é¢†åŸŸè¾¹ç•Œæ‰©å±•ï¼Œå¹¶å»ºç«‹æ ‡å‡†åŒ–æ¯”è¾ƒåŸºç¡€ï¼Œä¿ƒè¿›å¤šå‚è€ƒç”ŸæˆæŠ€æœ¯çš„å…¬å¹³è¯„ä¼°ä¸æŒç»­å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Recent text-to-image generation models have acquired the ability of multi-reference generation and editing; the ability to inherit the appearance of subjects from multiple reference images and re-render them under new contexts. However, the existing benchmark datasets often focus on the generation with single or a few reference images, which prevents us from measuring the progress on how model performance advances or pointing out their weaknesses, under different multi-reference conditions. In addition, their task definitions are still vague, typically limited to axes such as "what to edit" or "how many references are given", and therefore fail to capture the intrinsic difficulty of multi-reference settings. To address this gap, we introduce $\textbf{MultiBanana}$, which is carefully designed to assesses the edge of model capabilities by widely covering multi-reference-specific problems at scale: (1) varying the number of references, (2) domain mismatch among references (e.g., photo vs. anime), (3) scale mismatch between reference and target scenes, (4) references containing rare concepts (e.g., a red banana), and (5) multilingual textual references for rendering. Our analysis among a variety of text-to-image models reveals their superior performances, typical failure modes, and areas for improvement. MultiBanana will be released as an open benchmark to push the boundaries and establish a standardized basis for fair comparison in multi-reference image generation. Our data and code are available at https://github.com/matsuolab/multibanana .


### [75] [MrGS: Multi-modal Radiance Fields with 3D Gaussian Splatting for RGB-Thermal Novel View Synthesis](https://arxiv.org/abs/2511.22997)
*Minseong Kweon, Janghyun Kim, Ukcheol Shin, Jinsun Park*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMrGSï¼Œä¸€ç§åŸºäº3Dé«˜æ–¯æ³¼æº…çš„å¤šæ¨¡æ€è¾å°„åœºæ–¹æ³•ï¼Œèƒ½å¤ŸåŒæ—¶é‡å»ºRGBå’Œçƒ­çº¢å¤–3Dåœºæ™¯ï¼Œé€šè¿‡ç‰©ç†å…ˆéªŒå»ºæ¨¡çƒ­ä¼ å¯¼å’Œè¾å°„ç‰¹æ€§ï¼Œåœ¨ä¿æŒé«˜ä¿çœŸåº¦çš„åŒæ—¶å‡å°‘äº†é«˜æ–¯æ•°é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºNeRFå’Œ3DGSçš„æ–¹æ³•åœ¨RGBåœºæ™¯é‡å»ºæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†èåˆçƒ­çº¢å¤–å›¾åƒçš„å¤šæ¨¡æ€æ¸²æŸ“ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äº†çƒ­ä¼ å¯¼å’Œæœ—ä¼¯ç‰¹æ€§ç­‰ç‹¬ç‰¹çš„çƒ­å­¦ç‰¹å¾ï¼Œè¿™é™åˆ¶äº†å¤šæ¨¡æ€åœºæ™¯é‡å»ºçš„å‡†ç¡®æ€§å’Œç‰©ç†åˆç†æ€§ã€‚

**Method:** MrGSé€šè¿‡æ­£äº¤ç‰¹å¾æå–ä»å•ä¸€å¤–è§‚ç‰¹å¾ä¸­åˆ†ç¦»RGBå’Œçƒ­ç›¸å…³ä¿¡æ¯ï¼Œå¹¶æ ¹æ®å„æ¨¡æ€çš„æœ—ä¼¯åå°„ç¨‹åº¦é‡‡ç”¨è§†å›¾ä¾èµ–æˆ–è§†å›¾ç‹¬ç«‹çš„åµŒå…¥ç­–ç•¥ã€‚è¯¥æ–¹æ³•æ•´åˆäº†ä¸¤ä¸ªç‰©ç†åŸç†ï¼šåœ¨alphaæ··åˆå‰åº”ç”¨å‚…é‡Œå¶çƒ­ä¼ å¯¼å®šå¾‹æ¥å»ºæ¨¡ç›¸é‚»é«˜æ–¯ä¹‹é—´çš„çƒ­ä¼ å¯¼å¼ºåº¦æ’å€¼ï¼Œä»¥åŠç»“åˆæ–¯ç‰¹è—©-ç»å°”å…¹æ›¼å®šå¾‹å’Œå¹³æ–¹åæ¯”å®šå¾‹æ¥æ„å»ºæ·±åº¦æ„ŸçŸ¥çš„çƒ­è¾å°„å›¾ï¼Œä¸ºçƒ­æ¸²æŸ“æ–½åŠ é¢å¤–çš„å‡ ä½•çº¦æŸã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒMrGSèƒ½å¤Ÿå®ç°é«˜ä¿çœŸåº¦çš„RGB-Tåœºæ™¯é‡å»ºï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„é«˜æ–¯æ•°é‡ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒé‡å»ºè´¨é‡çš„å‰æä¸‹ï¼Œæœ‰æ•ˆå»ºæ¨¡äº†çƒ­ä¼ å¯¼å’Œè¾å°„ç°è±¡ï¼ŒéªŒè¯äº†ç‰©ç†å…ˆéªŒåœ¨å¤šæ¨¡æ€æ¸²æŸ“ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å°†ç‰©ç†å®šå¾‹æ•´åˆåˆ°å¤šæ¨¡æ€è¾å°„åœºä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºRGB-Tåœºæ™¯é‡å»ºæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ç»“åˆçƒ­ä¼ å¯¼å’Œè¾å°„çš„ç‰©ç†çº¦æŸï¼ŒMrGSä¸ä»…æé«˜äº†çƒ­çº¢å¤–é‡å»ºçš„å‡†ç¡®æ€§ï¼Œè¿˜å‡å°‘äº†æ¨¡å‹å¤æ‚åº¦ï¼Œä¸ºå¤šæ¨¡æ€3Dé‡å»ºå¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Recent advances in Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved considerable performance in RGB scene reconstruction. However, multi-modal rendering that incorporates thermal infrared imagery remains largely underexplored. Existing approaches tend to neglect distinctive thermal characteristics, such as heat conduction and the Lambertian property. In this study, we introduce MrGS, a multi-modal radiance field based on 3DGS that simultaneously reconstructs both RGB and thermal 3D scenes. Specifically, MrGS derives RGB- and thermal-related information from a single appearance feature through orthogonal feature extraction and employs view-dependent or view-independent embedding strategies depending on the degree of Lambertian reflectance exhibited by each modality. Furthermore, we leverage two physics-based principles to effectively model thermal-domain phenomena. First, we integrate Fourier's law of heat conduction prior to alpha blending to model intensity interpolation caused by thermal conduction between neighboring Gaussians. Second, we apply the Stefan-Boltzmann law and the inverse-square law to formulate a depth-aware thermal radiation map that imposes additional geometric constraints on thermal rendering. Experimental results demonstrate that the proposed MrGS achieves high-fidelity RGB-T scene reconstruction while reducing the number of Gaussians.


### [76] [JarvisEvo: Towards a Self-Evolving Photo Editing Agent with Synergistic Editor-Evaluator Optimization](https://arxiv.org/abs/2511.23002)
*Yunlong Lin, Linqing Wang, Kunjie Lin, Zixu Lin, Kaixiong Gong, Wenbo Li, Bin Lin, Zhenxi Li, Shiyi Zhang, Yuyang Peng, Wenxun Dai, Xinghao Ding, Chunyu Wang, Qinglin Lu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºJarvisEvoï¼Œä¸€ç§ç»Ÿä¸€çš„å›¾åƒç¼–è¾‘æ™ºèƒ½ä½“ï¼Œé€šè¿‡äº¤ç»‡å¼å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†å’ŒååŒç¼–è¾‘-è¯„ä¼°ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œè§£å†³äº†æŒ‡ä»¤å¹»è§‰å’Œå¥–åŠ±æ”»å‡»é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç¼–è¾‘çš„å‡†ç¡®æ€§å’Œå†…å®¹ä¿çœŸåº¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºæ™ºèƒ½ä½“çš„ç¼–è¾‘æ¨¡å‹é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¸€æ˜¯æŒ‡ä»¤å¹»è§‰é—®é¢˜ï¼Œçº¯æ–‡æœ¬æ€ç»´é“¾æ¨ç†ç”±äºå›ºæœ‰ä¿¡æ¯ç“¶é¢ˆæ— æ³•å®Œå…¨é˜²æ­¢äº‹å®é”™è¯¯ï¼›äºŒæ˜¯å¥–åŠ±æ”»å‡»é—®é¢˜ï¼Œé’ˆå¯¹é™æ€å¥–åŠ±æ¨¡å‹çš„åŠ¨æ€ç­–ç•¥ä¼˜åŒ–ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåˆ©ç”¨å¥–åŠ±å‡½æ•°ç¼ºé™·ï¼Œå¯¼è‡´ç¼–è¾‘è´¨é‡ä¸‹é™ã€‚

**Method:** JarvisEvoé‡‡ç”¨äº¤ç»‡å¼å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†æœºåˆ¶å¢å¼ºæŒ‡ä»¤éµå¾ªå’Œç¼–è¾‘è´¨é‡ï¼Œå¹¶æå‡ºååŒç¼–è¾‘-è¯„ä¼°ç­–ç•¥ä¼˜åŒ–æ¡†æ¶å®ç°æ— éœ€å¤–éƒ¨å¥–åŠ±çš„è‡ªæˆ‘æ”¹è¿›ï¼Œæœ‰æ•ˆç¼“è§£å¥–åŠ±æ”»å‡»é—®é¢˜ï¼ŒåŒæ—¶é€šè¿‡æ— ç¼é›†æˆAdobe Lightroomæ”¯æŒå…¨å±€å’Œå±€éƒ¨ç²¾ç»†ç¼–è¾‘ã€‚

**Result:** åœ¨ArtEdit-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒJarvisEvoåœ¨ä¿æŠ¤æ€§ç¼–è¾‘æŒ‡æ ‡ä¸Šå¹³å‡ä¼˜äºNano-Banana 18.95%ï¼Œå…¶ä¸­åƒç´ çº§å†…å®¹ä¿çœŸåº¦æå‡è¾¾44.96%ï¼Œæ˜¾è‘—æé«˜äº†ç¼–è¾‘å‡†ç¡®æ€§å’Œå†…å®¹ä¸€è‡´æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜å¤šæ¨¡æ€æ¨ç†ä¸ååŒç­–ç•¥ä¼˜åŒ–çš„ç»“åˆèƒ½æœ‰æ•ˆè§£å†³æ™ºèƒ½ä½“ç¼–è¾‘ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸ºæ„å»ºæ›´å¯é ã€è‡ªæ”¹è¿›çš„ç¼–è¾‘ç³»ç»Ÿæä¾›äº†æ–°èŒƒå¼ï¼Œå…·æœ‰æ¨åŠ¨åˆ›æ„AIå·¥å…·å‘å±•çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Agent-based editing models have substantially advanced interactive experiences, processing quality, and creative flexibility. However, two critical challenges persist: (1) instruction hallucination, text-only chain-of-thought (CoT) reasoning cannot fully prevent factual errors due to inherent information bottlenecks; (2) reward hacking, dynamic policy optimization against static reward models allows agents to exploit flaws in reward functions. To address these issues, we propose JarvisEvo, a unified image editing agent that emulates an expert human designer by iteratively editing, selecting appropriate tools, evaluating results, and reflecting on its own decisions to refine outcomes. JarvisEvo offers three key advantages: (1) an interleaved multimodal chain-of-thought (iMCoT) reasoning mechanism that enhances instruction following and editing quality; (2) a synergistic editor-evaluator policy optimization (SEPO) framework that enables self-improvement without external rewards, effectively mitigating reward hacking; and (3) support for both global and local fine-grained editing through seamless integration of Adobe Lightroom. On ArtEdit-Bench, JarvisEvo outperforms Nano-Banana by an average of 18.95% on preservative editing metrics, including a substantial 44.96% improvement in pixel-level content fidelity.


### [77] [Buffer replay enhances the robustness of multimodal learning under missing-modality](https://arxiv.org/abs/2511.23070)
*Hongye Zhu, Xuan Liu, Yanwen Ba, Jingye Xue, Shigeng Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†REplay Prompting (REP)æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºæ¨¡æ€ç‰¹å¾ç¼“å†²åŒºã€è§£è€¦ç§æœ‰-å…±äº«ç‰¹å¾ä»¥åŠåŠ¨æ€åˆå§‹åŒ–æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€æ¨¡å‹ä¸­æ¨¡æ€ç¼ºå¤±å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œåœ¨å¤šç§æ¨¡æ€ç¼ºå¤±åœºæ™¯ä¸‹å®ç°äº†è½»é‡çº§ä¸”é²æ£’çš„æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€æ¨¡å‹ä¸­æ¨¡æ€ç¼ºå¤±é€šå¸¸å¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œè¦ä¹ˆä»…ä¾èµ–ç›¸é‚»å±‚ç‰¹å¾è€Œå¿½ç•¥äº†é•¿è·ç¦»ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯åœ¨æ¨¡æ€ç¼ºå¤±æ—¶å¯èƒ½æä¾›é¢å¤–çš„å®¹é”™èƒ½åŠ›ã€‚

**Method:** REPæ–¹æ³•åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé€šè¿‡æ®‹å·®æ—è·¯æ„å»ºæ¨¡æ€ç‰¹å¾ç¼“å†²åŒºä»¥ç¼“å­˜æ—©æœŸå±‚è¡¨ç¤ºå¹¶åœ¨æ·±å±‚é‡æ”¾ï¼›é‡‡ç”¨ç§æœ‰-å…±äº«ç‰¹å¾è§£è€¦ç­–ç•¥ï¼Œç§æœ‰ç¼“å†²åŒºä¿ç•™æ¨¡æ€ç‰¹å®šä¿¡å·ï¼Œå…±äº«ç¼“å†²åŒºç¼–ç è·¨æ¨¡æ€è¯­ä¹‰ï¼›è®¾è®¡ä»»åŠ¡æ„ŸçŸ¥çš„åŠ¨æ€åˆå§‹åŒ–æœºåˆ¶ï¼Œæ ¹æ®ä¸åŒç¼ºå¤±æ¨¡æ€æ¡ä»¶é…ç½®ç¼“å†²åŒºä»¥æé«˜ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Result:** åœ¨è§†è§‰-è¯­è¨€ã€è§†è§‰-è¯­è¨€-éŸ³é¢‘ä»¥åŠæ—¶åºå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒREPåœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€ç¼ºå¤±åœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä»…å¼•å…¥å¯å¿½ç•¥çš„å‚æ•°å¼€é”€ï¼Œè¯æ˜äº†å…¶åœ¨æŒ‘æˆ˜æ€§æ¨¡æ€ç¼ºå¤±ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œè½»é‡çº§ç‰¹æ€§ã€‚

**Conclusion:** REPä¸ºé²æ£’å¤šæ¨¡æ€å­¦ä¹ æä¾›äº†ä¸€ä¸ªè½»é‡çº§ä¸”æœ‰æ•ˆçš„èŒƒå¼ï¼Œé€šè¿‡ç‰¹å¾é‡æ”¾å’Œè§£è€¦æœºåˆ¶æœ‰æ•ˆç¼“è§£äº†æ·±åº¦ç½‘ç»œä¸­çš„ä¿¡æ¯æŸå¤±é—®é¢˜ï¼Œä¸ºå¤„ç†ç°å®ä¸–ç•Œä¸­ä¸å®Œæ•´å¤šæ¨¡æ€æ•°æ®æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments.


### [78] [NumeriKontrol: Adding Numeric Control to Diffusion Transformers for Instruction-based Image Editing](https://arxiv.org/abs/2511.23105)
*Zhenyu Xu, Xiaoqi Shen, Haotian Nan, Xinyu Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†NumeriKontrolæ¡†æ¶ï¼Œé€šè¿‡è¿ç»­æ ‡é‡å€¼å®ç°åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘çš„ç²¾ç¡®å¼ºåº¦æ§åˆ¶ï¼Œè§£å†³äº†æ–‡æœ¬æŒ‡ä»¤åœ¨ç»†ç²’åº¦ç¼–è¾‘å¼ºåº¦æ§åˆ¶æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¯æ’æ‹”çš„æ•°å€¼é€‚é…å™¨ï¼Œæ”¯æŒé›¶æ ·æœ¬å¤šæ¡ä»¶ç¼–è¾‘ï¼Œå¹¶åœ¨å¤šæ ·åŒ–å±æ€§ç¼–è¾‘åœºæ™¯ä¸­å®ç°äº†å‡†ç¡®ã€è¿ç»­ä¸”ç¨³å®šçš„å°ºåº¦æ§åˆ¶ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘è™½ç„¶èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€å‘½ä»¤å®ç°ç›´è§‚æ“ä½œï¼Œä½†æ–‡æœ¬æŒ‡ä»¤æœ¬èº«å¾€å¾€ç¼ºä¹å¯¹ç¼–è¾‘å¼ºåº¦çš„ç²¾ç¡®æ§åˆ¶èƒ½åŠ›ï¼Œéš¾ä»¥å®ç°ç»†ç²’åº¦çš„å±æ€§è°ƒæ•´ã€‚ç°æœ‰æ–¹æ³•åœ¨ç¼–è¾‘å¼ºåº¦æ§åˆ¶æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæä¾›è¿ç»­ã€ç²¾ç¡®å°ºåº¦æ§åˆ¶çš„è§£å†³æ–¹æ¡ˆã€‚

**Method:** NumeriKontrolæ¡†æ¶é€šè¿‡æœ‰æ•ˆçš„æ•°å€¼é€‚é…å™¨ç¼–ç æ•°å€¼ç¼–è¾‘å°ºåº¦ï¼Œå¹¶ä»¥å³æ’å³ç”¨æ–¹å¼æ³¨å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä»»åŠ¡åˆ†ç¦»è®¾è®¡ï¼Œæ”¯æŒé›¶æ ·æœ¬å¤šæ¡ä»¶ç¼–è¾‘ï¼Œå…è®¸ç”¨æˆ·ä»¥ä»»æ„é¡ºåºæŒ‡å®šå¤šä¸ªæŒ‡ä»¤ã€‚ä¸ºæä¾›é«˜è´¨é‡ç›‘ç£ï¼Œç ”ç©¶å›¢é˜Ÿä»é«˜ä¿çœŸæ¸²æŸ“å¼•æ“å’Œæ•°ç å•åç›¸æœºç­‰å¯é æ¥æºåˆæˆç²¾ç¡®è®­ç»ƒæ•°æ®ï¼Œæ„å»ºäº†è¦†ç›–å¤šæ ·åŒ–å±æ€§æ“ä½œçš„é€šç”¨å±æ€§å˜æ¢æ•°æ®é›†ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼ŒNumeriKontrolåœ¨å¹¿æ³›çš„å±æ€§ç¼–è¾‘åœºæ™¯ä¸­å®ç°äº†å‡†ç¡®ã€è¿ç»­ä¸”ç¨³å®šçš„å°ºåº¦æ§åˆ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä½œä¸ºç®€å•è€Œå¼ºå¤§çš„äº¤äº’å¼ç¼–è¾‘å·¥ä½œå®¤ï¼Œåœ¨å¤šæ ·åŒ–å±æ€§æ“ä½œä¸­æä¾›ç²¾ç¡®çš„ç¼–è¾‘æ•ˆæœï¼Œæ”¯æŒç”¨æˆ·é€šè¿‡è¿ç»­æ ‡é‡å€¼è¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚

**Conclusion:** NumeriKontrolé€šè¿‡å¼•å…¥è¿ç»­æ ‡é‡æ§åˆ¶æœºåˆ¶ï¼Œæ˜¾è‘—æ¨è¿›äº†åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æŠ€æœ¯ï¼Œå®ç°äº†ç²¾ç¡®ã€å¯æ‰©å±•ä¸”ç”¨æˆ·å¯æ§çš„å›¾åƒæ“ä½œã€‚è¯¥ç ”ç©¶ä¸ºå›¾åƒç¼–è¾‘æä¾›äº†æ–°çš„äº¤äº’èŒƒå¼ï¼Œä½¿ç¼–è¾‘å¼ºåº¦æ§åˆ¶æ›´åŠ ç›´è§‚å’Œç²¾ç¡®ï¼Œä¸ºæœªæ¥å¯æ§åˆ¶ç”Ÿæˆæ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Instruction-based image editing enables intuitive manipulation through natural language commands. However, text instructions alone often lack the precision required for fine-grained control over edit intensity. We introduce NumeriKontrol, a framework that allows users to precisely adjust image attributes using continuous scalar values with common units. NumeriKontrol encodes numeric editing scales via an effective Numeric Adapter and injects them into diffusion models in a plug-and-play manner. Thanks to a task-separated design, our approach supports zero-shot multi-condition editing, allowing users to specify multiple instructions in any order. To provide high-quality supervision, we synthesize precise training data from reliable sources, including high-fidelity rendering engines and DSLR cameras. Our Common Attribute Transform (CAT) dataset covers diverse attribute manipulations with accurate ground-truth scales, enabling NumeriKontrol to function as a simple yet powerful interactive editing studio. Extensive experiments show that NumeriKontrol delivers accurate, continuous, and stable scale control across a wide range of attribute editing scenarios. These contributions advance instruction-based image editing by enabling precise, scalable, and user-controllable image manipulation.


### [79] [MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?](https://arxiv.org/abs/2511.23112)
*Yuandong Wang, Yao Cui, Yuxin Zhao, Zhen Yang, Yangfu Zhu, Zhenzhou Shao*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†MathSightåŸºå‡†æµ‹è¯•ï¼Œç”¨äºé‡åŒ–è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­è§†è§‰ä¿¡æ¯çš„å®é™…è´¡çŒ®ï¼Œå‘ç°éšç€é—®é¢˜éš¾åº¦å¢åŠ ï¼Œè§†è§‰ä¿¡æ¯çš„ä½œç”¨æ˜¾è‘—å‡å¼±ï¼Œç”šè‡³çº¯æ–‡æœ¬æ¨¡å‹ä¼˜äºå¤šæ¨¡æ€å˜ä½“ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è§†è§‰ä¿¡æ¯å¯¹æ¨ç†çš„å®é™…è´¡çŒ®ç¨‹åº¦å°šä¸æ˜ç¡®ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•æŠ¥å‘Šäº†å¼ºå¤§çš„æ•´ä½“æ€§èƒ½ï¼Œä½†å¾ˆå°‘åˆ†ç¦»å›¾åƒæ¨¡æ€çš„ä½œç”¨ï¼Œæ— æ³•ç¡®å®šæ¨¡å‹æ˜¯å¦çœŸæ­£åˆ©ç”¨è§†è§‰ç†è§£è¿˜æ˜¯ä»…ä»…ä¾èµ–è¯­è¨€å…ˆéªŒçŸ¥è¯†ã€‚

**Method:** ç ”ç©¶æå‡ºäº†MathSightåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å­¦çº§åˆ«çš„å¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†ï¼Œæ—¨åœ¨åˆ†ç¦»å’Œé‡åŒ–è§†è§‰è¾“å…¥çš„å½±å“ã€‚æ¯ä¸ªé—®é¢˜åŒ…å«å¤šä¸ªè§†è§‰å˜ä½“â€”â€”åŸå§‹å›¾åƒã€æ‰‹ç»˜å›¾åƒã€ç…§ç‰‡æ‹æ‘„å›¾åƒâ€”â€”ä»¥åŠä¸€ä¸ªçº¯æ–‡æœ¬æ¡ä»¶ç”¨äºå—æ§æ¯”è¾ƒï¼Œä»è€Œç³»ç»Ÿè¯„ä¼°è§†è§‰ä¿¡æ¯çš„ä½œç”¨ã€‚

**Result:** åœ¨æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒæ­ç¤ºäº†ä¸€è‡´è¶‹åŠ¿ï¼šè§†è§‰ä¿¡æ¯çš„è´¡çŒ®éšç€é—®é¢˜éš¾åº¦çš„å¢åŠ è€Œå‡å°‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ²¡æœ‰ä»»ä½•å›¾åƒè¾“å…¥çš„Qwen3-VLæ¨¡å‹è¶…è¶Šäº†å…¶å¤šæ¨¡æ€å˜ä½“å’ŒGPT-5ï¼Œè¿™è¡¨æ˜å½“å‰æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„ä¾èµ–æœ‰é™ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­å¹¶æœªå……åˆ†åˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œè€Œæ˜¯è¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒçŸ¥è¯†ã€‚MathSightåŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°å’Œæ¨è¿›çœŸæ­£çš„è§†è§‰åŸºç¡€æ¨ç†æä¾›äº†å¿…è¦å·¥å…·ï¼Œå¼ºè°ƒäº†æœªæ¥æ¨¡å‹éœ€è¦æ›´æœ‰æ•ˆåœ°æ•´åˆè§†è§‰ç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
Recent advances in Vision-Language Models (VLMs) have achieved impressive progress in multimodal mathematical reasoning. Yet, how much visual information truly contributes to reasoning remains unclear. Existing benchmarks report strong overall performance but seldom isolate the role of the image modality, leaving open whether VLMs genuinely leverage visual understanding or merely depend on linguistic priors. To address this, we present MathSight, a university-level multimodal mathematical reasoning benchmark designed to disentangle and quantify the effect of visual input. Each problem includes multiple visual variants -- original, hand-drawn, photo-captured -- and a text-only condition for controlled comparison. Experiments on state-of-the-art VLMs reveal a consistent trend: the contribution of visual information diminishes with increasing problem difficulty. Remarkably, Qwen3-VL without any image input surpasses both its multimodal variants and GPT-5, underscoring the need for benchmarks like MathSight to advance genuine vision-grounded reasoning in future models.


### [80] [Learning to Refuse: Refusal-Aware Reinforcement Fine-Tuning for Hard-Irrelevant Queries in Video Temporal Grounding](https://arxiv.org/abs/2511.23151)
*Jin-Seop Lee, SungJoon Lee, SeongJun Jung, Boyang Li, Jee-Hyong Lee*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºæ‹’ç»æ„ŸçŸ¥å¼ºåŒ–å¾®è°ƒï¼ˆRA-RFTï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³è§†é¢‘æ—¶åºå®šä½ä¸­ç¡¬ä¸ç›¸å…³æŸ¥è¯¢çš„æ‹’ç»é—®é¢˜ï¼Œè¯¥æ–¹æ³•åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¡†æ¶å¹¶æ•´åˆå››ç§å¥–åŠ±ç›®æ ‡ï¼ŒåŒæ—¶æ„å»ºäº†åŒ…å«ç¡¬ä¸ç›¸å…³æŸ¥è¯¢çš„HI-VTGæ•°æ®é›†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†é¢‘æ—¶åºå®šä½æ¨¡å‹å‡è®¾æŸ¥è¯¢æ€»æ˜¯ä¸è§†é¢‘ç›¸å…³ï¼Œå¯¼è‡´å³ä½¿æŸ¥è¯¢ä¸ç›¸å…³ä¹Ÿä¼šé¢„æµ‹ç›®æ ‡ç‰‡æ®µï¼Œè€Œè¿‘æœŸæ–¹æ³•åªèƒ½æ‹’ç»å®Œå…¨ä¸ç›¸å…³çš„æŸ¥è¯¢ï¼Œæ— æ³•å¤„ç†è¯­ä¹‰ç›¸ä¼¼ä½†å®é™…ä¸ç›¸å…³çš„ç¡¬ä¸ç›¸å…³æŸ¥è¯¢ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚

**Method:** æå‡ºæ‹’ç»æ„ŸçŸ¥å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼ŒåŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œæ•´åˆæ ¼å¼ã€æ‹’ç»äº¤å¹¶æ¯”ã€è§£é‡Šå’ŒæŸ¥è¯¢ä¿®æ­£å››ç§å¥–åŠ±ç›®æ ‡ï¼Œä»¥æå‡ç›¸å…³æ€§åˆ¤åˆ«å’Œç»†ç²’åº¦è¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶æ„å»ºåŒ…å«ç¡¬ä¸ç›¸å…³æŸ¥è¯¢åŠå…¶æ‹’ç»ç­”æ¡ˆçš„HI-VTGæ•°æ®é›†æ¥æ”¯æŒæ–¹æ³•è®­ç»ƒã€‚

**Result:** æ–¹æ³•åœ¨å¤šç§ç›¸å…³æ€§æ„ŸçŸ¥è§†é¢‘æ—¶åºå®šä½åœºæ™¯ä¸­éªŒè¯æœ‰æ•ˆï¼ŒåŒ…æ‹¬ç¡¬ä¸ç›¸å…³VTGã€ç®€å•é‡æ’RA-VTGå’Œäººå·¥æ ‡æ³¨RA-VTGè®¾ç½®ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•å¯æ‰©å±•åº”ç”¨äºå¤šç§åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„VTGæ¨¡å‹ï¼Œå¹¶åœ¨æ‹’ç»ç¡¬ä¸ç›¸å…³æŸ¥è¯¢æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºè§£å†³è§†é¢‘æ—¶åºå®šä½ä¸­ç¡¬ä¸ç›¸å…³æŸ¥è¯¢çš„æ‹’ç»é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’Œå¤šç›®æ ‡å¥–åŠ±è®¾è®¡æå‡äº†æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œæ„å»ºçš„æ•°æ®é›†ä¸ºç›¸å…³ç ”ç©¶æä¾›äº†åŸºå‡†ï¼Œæ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Video Temporal Grounding (VTG) aims to localize a temporal segment in a video corresponding to a natural language query. However, existing VTG models assume that a relevant segment always exists, causing them to always predict a target segment even when the query is irrelevant to the video. While recent approaches attempt to handle irrelevant queries, they can only reject those that are entirely unrelated to the video and still fail to handle hard-irrelevant queries that are semantically similar but not actually relevant. To address this, we propose Refusal-Aware Reinforcement Fine-Tuning (RA-RFT) to effectively refuse hard-irrelevant queries in VTG. Our method is based on the Group Relative Policy Optimization (GRPO) framework and integrates four reward objectives-format, refuse-IoU, explain, and query correction-to improve both relevance discrimination and fine-grained semantic reasoning. In addition, to effectively support RA-RFT, we construct a Hard-Irrelevant VTG (HI-VTG) dataset, which includes hard-irrelevant queries and their refusal answers. We demonstrate the effectiveness of our method across various relevance-aware VTG scenarios, including hard-irrelevant VTG, simply-shuffled RA-VTG, and human-annotated RA-VTG settings. We also show that the proposed method is scalable by applying it to various LVLM-based VTG models. Our code is available at https://github.com/JINSUBY/RA-RFT.


### [81] [PowerCLIP: Powerset Alignment for Contrastive Pre-Training](https://arxiv.org/abs/2511.23170)
*Masaki Kawamura, Nakamasa Inoue, Rintaro Yanagi, Hirokatsu Kataoka, Rio Yokota*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºPowerCLIPï¼Œä¸€ç§åŸºäºå¹‚é›†å¯¹é½çš„å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–å›¾åƒåŒºåŸŸä¸æ–‡æœ¬è§£ææ ‘ä¹‹é—´çš„å¹‚é›†å¯¹é½æ¥å¢å¼ºç»†ç²’åº¦ç»„åˆè¯­ä¹‰ç†è§£ï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬åˆ†ç±»å’Œæ£€ç´¢æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶å¦‚CLIPåœ¨ç»†ç²’åº¦ç»„åˆç†è§£æ–¹é¢å­˜åœ¨å±€é™ï¼Œç‰¹åˆ«æ˜¯éš¾ä»¥æ•æ‰è·¨è¶Šå¤šä¸ªå›¾åƒåŒºåŸŸçš„ç»„åˆè¯­ä¹‰ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹å¯¹å¤æ‚è§†è§‰åœºæ™¯çš„æ·±åº¦ç†è§£èƒ½åŠ›ã€‚

**Method:** PowerCLIPé‡‡ç”¨å¹‚é›†å¯¹é½æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–å›¾åƒåŒºåŸŸå­é›†ä¸æ–‡æœ¬è§£ææ ‘ä¹‹é—´çš„æŸå¤±æ¥ä¼˜åŒ–åŒºåŸŸåˆ°çŸ­è¯­çš„å¯¹é½ï¼›ä¸ºè§£å†³å¹‚é›†æ„é€ å¸¦æ¥çš„æŒ‡æ•°è®¡ç®—å¤æ‚åº¦ï¼Œå¼•å…¥äº†é«˜æ•ˆéçº¿æ€§èšåˆå™¨ï¼Œå°†å¤æ‚åº¦ä»O(2^M)é™ä½åˆ°O(M)ï¼ŒåŒæ—¶èƒ½ä»¥ä»»æ„ç²¾åº¦é€¼è¿‘ç²¾ç¡®æŸå¤±å€¼ã€‚

**Result:** å®éªŒè¡¨æ˜PowerCLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶ç»„åˆæ€§å’Œé²æ£’æ€§ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»†ç²’åº¦è¯­ä¹‰ç†è§£çš„å¤æ‚è§†è§‰åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚

**Conclusion:** PowerCLIPé€šè¿‡å¹‚é›†å¯¹é½æœºåˆ¶æœ‰æ•ˆè§£å†³äº†å¤šåŒºåŸŸç»„åˆè¯­ä¹‰çš„å»ºæ¨¡é—®é¢˜ï¼Œä¸ºè§†è§‰è¯­è¨€ç†è§£æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…¶é«˜æ•ˆéçº¿æ€§èšåˆå™¨è®¾è®¡ä¸ºå¤„ç†ç»„åˆçˆ†ç‚¸é—®é¢˜æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†ç»†ç²’åº¦è·¨æ¨¡æ€å¯¹é½ç ”ç©¶çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Contrastive vision-language pre-training frameworks such as CLIP have demonstrated impressive zero-shot performance across a range of vision-language tasks. Recent studies have shown that aligning individual text tokens with specific image patches or regions enhances fine-grained compositional understanding. However, it remains challenging to capture compositional semantics that span multiple image regions. To address this limitation, we propose PowerCLIP, a novel contrastive pre-training framework enhanced by powerset alignment, which exhaustively optimizes region-to-phrase alignments by minimizing the loss defined between powersets of image regions and textual parse trees. Since the naive powerset construction incurs exponential computational cost due to the combinatorial explosion in the number of region subsets, we introduce efficient non-linear aggregators (NLAs) that reduce complexity from O(2^M) to O(M) with respect to the number of regions M, while approximating the exact loss value with arbitrary precision. Our extensive experiments demonstrate that PowerCLIP outperforms state-of-the-art methods in zero-shot classification and retrieval tasks, underscoring the compositionality and robustness of our approach. Our code will be made publicly available.


### [82] [Zero-Shot Multi-Criteria Visual Quality Inspection for Semi-Controlled Industrial Environments via Real-Time 3D Digital Twin Simulation](https://arxiv.org/abs/2511.23214)
*Jose Moises Araya-Martinez, Gautham Mohan, Kenichi Hayakawa BolaÃ±os, Roberto Mendieta, Sarvenaz Sardari, Jens Lambrecht, JÃ¶rg KrÃ¼ger*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å§¿æ€æ— å…³çš„é›¶æ ·æœ¬è´¨é‡æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡å°†çœŸå®åœºæ™¯ä¸å®æ—¶æ•°å­—å­ªç”Ÿåœ¨RGB-Dç©ºé—´ä¸­è¿›è¡Œæ¯”è¾ƒï¼Œå®ç°äº†åŠæ§åˆ¶å·¥ä¸šç¯å¢ƒä¸‹çš„é«˜æ•ˆç¼ºé™·æ£€æµ‹ã€‚è¯¥æ–¹æ³•åŸºäºå·²çŸ¥CADæ¨¡å‹çš„ç‰©ä½“æ£€æµ‹å’Œå§¿æ€ä¼°è®¡ï¼Œä¸ºåŠ¨æ€åˆ¶é€ ç¯å¢ƒä¸­çš„é€šç”¨åŒ–ã€ä½æ•°æ®ç¼ºé™·æ£€æµ‹æ–¹æ³•å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°ä»£å·¥ä¸šç¯å¢ƒä¸­ï¼Œæ—©æœŸè§†è§‰è´¨é‡æ£€æµ‹å¯¹äºå®ç°é›¶ç¼ºé™·åˆ¶é€ å’Œæœ€å°åŒ–ç”Ÿäº§æµªè´¹è‡³å…³é‡è¦ï¼Œä½†é²æ£’è§†è§‰æ£€æµ‹ç³»ç»Ÿçš„å¤æ‚æ€§åŠå…¶å¹¿æ³›çš„æ•°æ®éœ€æ±‚é˜»ç¢äº†å…¶åœ¨åŠæ§åˆ¶å·¥ä¸šç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨ã€‚å› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿåœ¨åŠ¨æ€åˆ¶é€ ç¯å¢ƒä¸­å®ç°é€šç”¨åŒ–ã€ä½æ•°æ®ç¼ºé™·æ£€æµ‹çš„æ–¹æ³•ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†ä¸€ç§å§¿æ€æ— å…³çš„é›¶æ ·æœ¬è´¨é‡æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡ç‰©ä½“æ£€æµ‹å’Œå·²çŸ¥CADæ¨¡å‹çš„å§¿æ€ä¼°è®¡å¯¹å·¥ä¸šåœºæ™¯è¿›è¡Œè¯­ä¹‰æè¿°ï¼Œå®ç°å®æ—¶æ•°å­—å­ªç”Ÿåœ¨RGB-Dç©ºé—´ä¸­çš„é«˜æ•ˆæ¸²æŸ“ã€‚è¯¥æ–¹æ³•æä¾›äº†å¯æ‰©å±•çš„åˆ†å±‚æ³¨é‡Šç­–ç•¥ï¼Œç”¨äºå¤šæ ‡å‡†ç¼ºé™·æ£€æµ‹ï¼Œå°†å§¿æ€æ ‡æ³¨ä¸é€»è¾‘å’Œç»“æ„ç¼ºé™·æ³¨é‡Šç»Ÿä¸€èµ·æ¥ï¼Œå¹¶åŸºå‡†æµ‹è¯•äº†å®æ—¶å¤šæ¨¡æ€RGB-Dæ•°å­—å­ªç”Ÿåˆ›å»ºå·¥å…·ï¼ŒåŒæ—¶è·Ÿè¸ªè®¡ç®—èµ„æºæ¶ˆè€—ã€‚

**Result:** åŸºäºè½´å‘ç£é€šç”µæœºè´¨é‡æ£€æµ‹çš„æ±½è½¦ç”¨ä¾‹ï¼Œè¯¥æ¡†æ¶åœ¨åŠæ§åˆ¶å·¥ä¸šæ¡ä»¶ä¸‹è¡¨ç°å‡ºè‰²ï¼Œå³ä½¿ä½¿ç”¨ç®€å•çš„è·ç¦»æµ‹é‡ï¼Œä¸çœŸå®æ©ç ç›¸æ¯”ä¹Ÿèƒ½è¾¾åˆ°é«˜è¾¾63.3%çš„äº¤å¹¶æ¯”åˆ†æ•°ã€‚è¯¥æ–¹æ³•åœ¨åŠ¨æ€åˆ¶é€ ç¯å¢ƒä¸­å±•ç¤ºäº†æœ‰æ•ˆçš„æ£€æµ‹æ€§èƒ½ï¼Œä¸ºä½æ•°æ®ç¼ºé™·æ£€æµ‹æ–¹æ³•æä¾›äº†å®è¯æ”¯æŒã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºåŠ¨æ€åˆ¶é€ ç¯å¢ƒä¸­é€šç”¨åŒ–ã€ä½æ•°æ®ç¼ºé™·æ£€æµ‹æ–¹æ³•çš„æœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œé€šè¿‡å®æ—¶æ•°å­—å­ªç”Ÿä¸çœŸå®åœºæ™¯çš„æ¯”è¾ƒæ¡†æ¶ï¼Œè§£å†³äº†åŠæ§åˆ¶å·¥ä¸šç¯å¢ƒä¸­è§†è§‰è´¨é‡æ£€æµ‹çš„å¤æ‚æ€§å’Œæ•°æ®éœ€æ±‚é—®é¢˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿ä½¿ç”¨ç›¸å¯¹ç®€å•çš„æµ‹é‡æ–¹æ³•ï¼Œè¯¥æ¡†æ¶ä¹Ÿèƒ½åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å·¥ä¸šæ¡ä»¶ä¸‹å®ç°æœ‰æ•ˆçš„ç¼ºé™·æ£€æµ‹ã€‚

---

#### ğŸ“„ Abstract
Early-stage visual quality inspection is vital for achieving Zero-Defect Manufacturing and minimizing production waste in modern industrial environments. However, the complexity of robust visual inspection systems and their extensive data requirements hinder widespread adoption in semi-controlled industrial settings. In this context, we propose a pose-agnostic, zero-shot quality inspection framework that compares real scenes against real-time Digital Twins (DT) in the RGB-D space. Our approach enables efficient real-time DT rendering by semantically describing industrial scenes through object detection and pose estimation of known Computer-Aided Design models. We benchmark tools for real-time, multimodal RGB-D DT creation while tracking consumption of computational resources. Additionally, we provide an extensible and hierarchical annotation strategy for multi-criteria defect detection, unifying pose labelling with logical and structural defect annotations. Based on an automotive use case featuring the quality inspection of an axial flux motor, we demonstrate the effectiveness of our framework. Our results demonstrate detection performace, achieving intersection-over-union (IoU) scores of up to 63.3% compared to ground-truth masks, even if using simple distance measurements under semi-controlled industrial conditions. Our findings lay the groundwork for future research on generalizable, low-data defect detection methods in dynamic manufacturing settings.


### [83] [Unlocking Multilingual Reasoning Capability of LLMs and LVLMs through Representation Engineering](https://arxiv.org/abs/2511.23231)
*Qiming Li, Xiaocheng Feng, Yixuan Ma, Zekai Ye, Ruihan Chen, Xiachong Feng, Bing Qin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€åœ¨æ¨ç†æ—¶å¢å¼ºå¤šè¯­è¨€æ¨ç†èƒ½åŠ›çš„æ–¹æ³•MRREï¼Œé€šè¿‡è¡¨ç¤ºå·¥ç¨‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¡ºåºæ³¨å…¥ä¸¤ä¸ªé¢„è®¡ç®—å‘é‡ï¼Œæ˜¾è‘—æå‡äº†ä½èµ„æºè¯­è¨€çš„æ¨ç†æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è¾“å…¥è¾“å‡ºè¯­è¨€çš„ä¸€è‡´æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‹±è¯­ä¸Šçš„æ¨ç†èƒ½åŠ›æ˜¾è‘—ä¼˜äºä½èµ„æºè¯­è¨€ï¼Œå¯¼è‡´å¤šè¯­è¨€åº”ç”¨ä¸­çš„å…¬å¹³æ€§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–æ˜‚è´µçš„å¤šè¯­è¨€è®­ç»ƒï¼Œè¦ä¹ˆä½¿ç”¨å¤–éƒ¨ç¿»è¯‘å·¥å…·è¿›è¡Œæç¤ºï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½èµ„æºå¯†é›†ä¸”å¯¹ç¿»è¯‘è´¨é‡æ•æ„Ÿã€‚

**Method:** MRREæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒæ•°æ®çš„æ¨ç†æ—¶æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†å¤„ç†çš„ç‰¹å®šå±‚é¡ºåºæ³¨å…¥ä¸¤ä¸ªé¢„è®¡ç®—å‘é‡ï¼šè·¨è¯­è¨€æ¨ç†å¢å¼ºå‘é‡å°†éè‹±è¯­æ¨ç†è¡¨ç¤ºå¼•å¯¼è‡³è‹±è¯­ç©ºé—´ä»¥è§£é”å¤šè¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œç›®æ ‡è¯­è¨€è¾“å‡ºé”šå®šå‘é‡æ¢å¤ç›®æ ‡è¯­è¨€çš„åˆ†å¸ƒä»¥ä¿æŒè¾“å…¥è¾“å‡ºè¯­è¨€ä¸€è‡´æ€§ã€‚

**Result:** åœ¨å…­ä¸ªå…ˆè¿›LLMå’ŒLVLMä¸Šçš„å››ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMRREå°†éè‹±è¯­æ¨ç†èƒ½åŠ›å¹³å‡æå‡äº†5.48%ï¼Œåœ¨ä½èµ„æºè¯­è¨€ï¼ˆæ³°è¯­å’Œæ–¯ç“¦å¸Œé‡Œè¯­ï¼‰ä¸Šæœ€é«˜æå‡è¾¾7.54%ï¼ŒåŒæ—¶å°†è¾“å…¥è¾“å‡ºè¯­è¨€ä¸€è‡´æ€§æé«˜äº†3.78%ã€‚

**Conclusion:** MRREæä¾›äº†ä¸€ç§é«˜æ•ˆã€æ— éœ€è®­ç»ƒçš„å¤šè¯­è¨€æ¨ç†å¢å¼ºæ–¹æ¡ˆï¼Œé€šè¿‡è¡¨ç¤ºå·¥ç¨‹æœ‰æ•ˆç¼“è§£äº†è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„æ€§èƒ½å·®è·ï¼Œä¸ºå¤šè¯­è¨€AIç³»ç»Ÿçš„å…¬å¹³æ€§æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶é¿å…äº†ä¼ ç»Ÿæ–¹æ³•çš„é«˜æˆæœ¬å’Œç¿»è¯‘è´¨é‡ä¾èµ–é—®é¢˜ã€‚

---

#### ğŸ“„ Abstract
Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) demonstrate strong reasoning capabilities, yet their performance in English significantly outperforms that in low-resource languages, raising fairness concerns in multilingual applications. Existing approaches either rely on costly multilingual training or employ prompting with external translation tools, both of which are resource-intensive and sensitive to translation quality. To address these limitations, we propose a training-free inference-time method to enhance Multilingual Reasoning capabilities via Representation Engineering (MRRE) without using any additional training data or tools. MRRE sequentially injects two precomputed vectors at specific layers during inference processing: cross-lingual reasoning enhancement vectors, which steer non-English reasoning representations toward English space to unlock multilingual reasoning, and target-language output anchoring vectors, which restore the distribution of the target language to preserve input-output language consistency. Comprehensive experiments across six advanced LLMs and LVLMs on four reasoning benchmarks demonstrate that MRRE consistently enhances non-English reasoning by an average gain of 5.48% and up to 7.54% in low-resource languages (Thai and Swahili), while improving input-output language consistency by 3.78%.


### [84] [UniGeoSeg: Towards Unified Open-World Segmentation for Geospatial Scenes](https://arxiv.org/abs/2511.23332)
*Shuo Ni, Di Wang, He Chen, Haonan Guo, Ning Zhang, Jing Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†GeoSeg-1Mï¼Œé¦–ä¸ªç™¾ä¸‡è§„æ¨¡çš„é¥æ„ŸæŒ‡ä»¤é©±åŠ¨åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶æ„å»ºäº†ç»Ÿä¸€çš„UniGeoSegæ¡†æ¶ï¼Œåœ¨é¥æ„ŸæŒ‡ä»¤åˆ†å‰²ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°å’Œå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰é¥æ„ŸæŒ‡ä»¤é©±åŠ¨åˆ†å‰²æ–¹æ³•é¢ä¸´ä»»åŠ¡è¡¨è¿°ç¢ç‰‡åŒ–å’ŒæŒ‡ä»¤æ•°æ®æœ‰é™çš„é—®é¢˜ï¼Œè¿™é˜»ç¢äº†æ¨¡å‹çš„æœ‰æ•ˆç†è§£å’Œæ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦å¤§è§„æ¨¡æ•°æ®é›†å’Œç»Ÿä¸€æ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚

**Method:** ç ”ç©¶æå‡ºäº†è‡ªåŠ¨æ©ç è¿‡æ»¤å’ŒæŒ‡ä»¤ç”Ÿæˆæµæ°´çº¿ï¼Œä»å¤šä¸ªå…¬å…±æ•°æ®é›†åˆæˆå‚è€ƒã€äº¤äº’å’Œæ¨ç†åˆ†å‰²æŒ‡ä»¤ï¼Œæ„å»ºäº†åŒ…å«590Kå›¾åƒå’Œ1.1Mä¸‰å…ƒç»„çš„GeoSeg-1Mæ•°æ®é›†ï¼›è¿›ä¸€æ­¥è®¾è®¡äº†UniGeoSegç»Ÿä¸€æ¡†æ¶ï¼ŒåŒ…å«ä»»åŠ¡æ„ŸçŸ¥æ–‡æœ¬å¢å¼ºã€æ½œåœ¨çŸ¥è¯†è®°å¿†å’Œæ¸è¿›å¼è®­ç»ƒç­–ç•¥ä»¥ä¿ƒè¿›å¤šä»»åŠ¡å­¦ä¹ ã€‚

**Result:** UniGeoSegåœ¨GeoSeg-BenchåŸºå‡†æµ‹è¯•å’Œå¤šä¸ªå…¬å…±åŸºå‡†ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†å¤§è§„æ¨¡æ•°æ®é›†å’Œç»Ÿä¸€æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†å’Œç»Ÿä¸€æ¡†æ¶ï¼Œæ˜¾è‘—æ¨è¿›äº†é¥æ„ŸæŒ‡ä»¤åˆ†å‰²é¢†åŸŸçš„å‘å±•ï¼Œä¸ºå¤æ‚åœ°ç†ç©ºé—´åœºæ™¯ä¸‹çš„ä¸Šä¸‹æ–‡ç†è§£å’Œæ¨ç†èƒ½åŠ›è¯„ä¼°æä¾›äº†é‡è¦åŸºå‡†ï¼Œå¹¶ä¸ºé€šç”¨é¥æ„Ÿè§†è§‰ç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Instruction-driven segmentation in remote sensing generates masks from guidance, offering great potential for accessible and generalizable applications. However, existing methods suffer from fragmented task formulations and limited instruction data, hindering effective understanding and generalization. To address these issues, we introduce GeoSeg-1M, the first million-scale dataset for remote sensing instruction-driven segmentation, constructed via an automatic mask filtering and instruction generation pipeline that synthesizes referring, interactive, and reasoning segmentation instructions from multiple public datasets. GeoSeg-1M contains 590K images, 117 categories, and 1.1M image-mask-instruction triplets. Building upon this foundation, we further curate GeoSeg-Bench, a challenging benchmark designed to evaluate contextual understanding and reasoning capabilities across diverse instruction-driven tasks and complex geospatial scenes. Furthermore, we present UniGeoSeg, a unified framework that serves as a strong baseline, incorporating task-aware text enhancement, latent knowledge memory, and a progressive training strategy to facilitate multi-task learning. Extensive experiments demonstrate the state-of-the-art performance of UniGeoSeg across GeoSeg-Bench and diverse public benchmarks, while exhibiting strong zero-shot generalization. Datasets and source code were released at https://github.com/MiliLab/UniGeoSeg.


### [85] [DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline](https://arxiv.org/abs/2511.23377)
*Rui Zhang, Hongxia Wang, Hangqing Liu, Yang Zhou, Qiang Zeng*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†DEAL-300Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡30ä¸‡å¼ æ ‡æ³¨å›¾åƒçš„å¤§è§„æ¨¡æ‰©æ•£ç¼–è¾‘å®šä½åŸºå‡†ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åŸºäºè§†è§‰åŸºç¡€æ¨¡å‹å’Œå¤šé¢‘ç‡æç¤ºè°ƒä¼˜çš„å®šä½æ¡†æ¶ï¼Œä¸ºæ‰©æ•£ç¼–è¾‘åŒºåŸŸæ£€æµ‹æä¾›äº†å®ç”¨åŸºç¡€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŸºäºæ‰©æ•£çš„å›¾åƒç¼–è¾‘æŠ€æœ¯è™½ç„¶å®ç°äº†è¯­ä¹‰çº§å›¾åƒæ“ä½œï¼Œä½†ä¹Ÿäº§ç”Ÿäº†éš¾ä»¥å®šä½çš„çœŸå®å±€éƒ¨ä¼ªé€ å†…å®¹ã€‚ç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨ç”Ÿæˆå›¾åƒçš„äºŒå…ƒæ£€æµ‹æˆ–æ‰‹åŠ¨ç¼–è¾‘åŒºåŸŸçš„å®šä½ï¼Œæœªèƒ½åæ˜ æ‰©æ•£ç¼–è¾‘å¹³æ»‘èå…¥åŸå§‹å†…å®¹çš„ç‰¹æ€§ï¼Œç¼ºä¹ä¸“é—¨é’ˆå¯¹æ‰©æ•£ç¼–è¾‘å®šä½çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†DEAL-300Kæ•°æ®é›†ï¼Œä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç¼–è¾‘æŒ‡ä»¤ï¼Œæ— æ©ç æ‰©æ•£ç¼–è¾‘å™¨ç”Ÿæˆæ“ä½œå›¾åƒï¼Œä»¥åŠä¸»åŠ¨å­¦ä¹ å˜åŒ–æ£€æµ‹æµç¨‹è·å–åƒç´ çº§æ ‡æ³¨ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†åŸºäºå†»ç»“è§†è§‰åŸºç¡€æ¨¡å‹å’Œå¤šé¢‘ç‡æç¤ºè°ƒä¼˜çš„å®šä½æ¡†æ¶ï¼Œä»¥åŒæ—¶æ•æ‰ç¼–è¾‘åŒºåŸŸçš„è¯­ä¹‰å’Œé¢‘åŸŸç‰¹å¾ã€‚

**Result:** åœ¨DEAL-300Kæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ–¹æ³•åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†82.56%çš„åƒç´ çº§F1åˆ†æ•°ï¼Œåœ¨å¤–éƒ¨CoCoGlideåŸºå‡†ä¸Šè¾¾åˆ°äº†80.97%çš„F1åˆ†æ•°ï¼Œä¸ºæ‰©æ•£ç¼–è¾‘å®šä½ç ”ç©¶æä¾›äº†å¼ºæœ‰åŠ›çš„åŸºçº¿æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¡«è¡¥äº†æ‰©æ•£ç¼–è¾‘å®šä½é¢†åŸŸçš„æ•°æ®é›†ç©ºç™½ï¼Œæå‡ºçš„å®šä½æ¡†æ¶å±•ç°äº†æ•æ‰ç¼–è¾‘åŒºåŸŸè¯­ä¹‰å’Œé¢‘åŸŸç‰¹å¾çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥DIMLç ”ç©¶æä¾›äº†å®ç”¨çš„æ•°æ®é›†å’ŒåŸºå‡†æ–¹æ³•ï¼Œæœ‰åŠ©äºæå‡æ‰©æ•£ä¼ªé€ å†…å®¹çš„æ£€æµ‹èƒ½åŠ›ã€‚

---

#### ğŸ“„ Abstract
Diffusion-based image editing has made semantic level image manipulation easy for general users, but it also enables realistic local forgeries that are hard to localize. Existing benchmarks mainly focus on the binary detection of generated images or the localization of manually edited regions and do not reflect the properties of diffusion-based edits, which often blend smoothly into the original content. We present Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a large scale dataset for diffusion-based image manipulation localization (DIML) with more than 300,000 annotated images. We build DEAL-300K by using a multi-modal large language model to generate editing instructions, a mask-free diffusion editor to produce manipulated images, and an active-learning change detection pipeline to obtain pixel-level annotations. On top of this dataset, we propose a localization framework that uses a frozen Visual Foundation Model (VFM) together with Multi Frequency Prompt Tuning (MFPT) to capture both semantic and frequency-domain cues of edited regions. Trained on DEAL-300K, our method reaches a pixel-level F1 score of 82.56% on our test split and 80.97% on the external CoCoGlide benchmark, providing strong baselines and a practical foundation for future DIML research.The dataset can be accessed via https://github.com/ymhzyj/DEAL-300K.


### [86] [VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction](https://arxiv.org/abs/2511.23386)
*Sinan Du, Jiahao Guo, Bo Li, Shuhao Cui, Zhengzhuo Xu, Yifu Luo, Yongxian Wei, Kun Gai, Xinggang Wang, Kai Wu, Chun Yuan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºVQRAEï¼Œä¸€ç§å‘é‡é‡åŒ–çš„è¡¨ç¤ºè‡ªç¼–ç å™¨ï¼Œé¦–æ¬¡æ¢ç´¢åœ¨ç»Ÿä¸€æ ‡è®°å™¨ä¸­åŒæ—¶ç”Ÿæˆç”¨äºå›¾åƒç†è§£çš„è¿ç»­è¯­ä¹‰ç‰¹å¾å’Œç”¨äºè§†è§‰ç”Ÿæˆçš„ç¦»æ•£æ ‡è®°ï¼Œè§£å†³äº†å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆå’Œé‡å»ºè¡¨ç¤ºç»Ÿä¸€çš„å…³é”®æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ„å»ºç»Ÿä¸€æ¨¡å‹çš„å…³é”®æŒ‘æˆ˜åœ¨äºå¦‚ä½•åœ¨å•ä¸€æ ‡è®°å™¨ä¸­ç»Ÿä¸€å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆå’Œé‡å»ºè¡¨ç¤ºã€‚å…ˆå‰ç ”ç©¶ä¸»è¦é‡‡ç”¨åŒç¼–ç å™¨èŒƒå¼ï¼Œä¾‹å¦‚åˆ†åˆ«ä½¿ç”¨ä¸åŒç¼–ç å™¨è¿›è¡Œç†è§£å’Œç”Ÿæˆï¼Œæˆ–é€šè¿‡å¯¹æ¯”æŸå¤±å¹³è¡¡è¯­ä¹‰è¡¨ç¤ºä¸ä½çº§ç‰¹å¾ï¼Œç¼ºä¹çœŸæ­£çš„ç»Ÿä¸€è¡¨ç¤ºæ–¹æ³•ã€‚

**Method:** VQRAEåŸºäºé¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹æ„å»ºï¼Œé‡‡ç”¨å¯¹ç§°ViTè§£ç å™¨å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µå†»ç»“ç¼–ç å™¨ï¼Œé€šè¿‡åƒç´ é‡å»ºç›®æ ‡å­¦ä¹ é«˜ç»´è¯­ä¹‰VQç æœ¬ï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡è‡ªè’¸é¦çº¦æŸè”åˆä¼˜åŒ–ç¼–ç å™¨ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ç»Ÿä¸€æ ‡è®°å™¨ä¸­ç”Ÿæˆç”¨äºç†è§£çš„è¿ç»­è¯­ä¹‰ç‰¹å¾å’Œç”¨äºç”Ÿæˆçš„ç¦»æ•£æ ‡è®°ã€‚

**Result:** VQRAEåœ¨å¤šä¸ªè§†è§‰ç†è§£ã€ç”Ÿæˆå’Œé‡å»ºåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå…¶è¯­ä¹‰VQç æœ¬åœ¨1536ç»´åº¦ä¸‹èƒ½è¾¾åˆ°100%åˆ©ç”¨ç‡ï¼Œå¹¶åœ¨è‡ªå›å½’èŒƒå¼ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ‰©å±•æ€§ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒå¤šæ¨¡æ€ç†è§£èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†ç»†ç²’åº¦é‡å»ºå’Œç”Ÿæˆå…¼å®¹æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†é‡åŒ–è¯­ä¹‰ç¼–ç å™¨æ—¶é«˜ç»´ç æœ¬çš„é‡è¦æ€§ï¼Œä¸å…ˆå‰å›¾åƒé‡å»ºä¸­å¸¸ç”¨çš„ä½ç»´ç æœ¬å®è·µå½¢æˆå¯¹æ¯”ã€‚VQRAEä¸ºæ„å»ºç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°èŒƒå¼ï¼Œå…¶ç¦»æ•£ç‰¹æ€§ç‰¹åˆ«é€‚åˆè‡ªå›å½’ç”Ÿæˆä»»åŠ¡ï¼Œä¸ºæœªæ¥ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ å¼€è¾Ÿäº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.


### [87] [MANTA: Physics-Informed Generalized Underwater Object Tracking](https://arxiv.org/abs/2511.23405)
*Suhas Srinath, Hemang Jamadagni, Aditya Chadrasekar, Prathosh AP*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMANTAæ¡†æ¶ï¼Œä¸€ç§é’ˆå¯¹æ°´ä¸‹ç‰©ä½“è·Ÿè¸ªçš„ç‰©ç†æ„ŸçŸ¥æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè¡¨ç¤ºå­¦ä¹ ä¸è·Ÿè¸ªè®¾è®¡ï¼Œåˆ©ç”¨åŒæ­£å¯¹æ¯”å­¦ä¹ ç­–ç•¥å’Œç‰©ç†å¢å¼ºçš„äºŒæ¬¡å…³è”ç®—æ³•ï¼Œæ˜¾è‘—æå‡äº†æ°´ä¸‹è·Ÿè¸ªçš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ°´ä¸‹ç‰©ä½“è·Ÿè¸ªé¢ä¸´æ³¢é•¿ä¾èµ–çš„è¡°å‡å’Œæ•£å°„ç­‰ç‰©ç†é€€åŒ–é—®é¢˜ï¼Œå¯¼è‡´ç›®æ ‡å¤–è§‚åœ¨ä¸åŒæ·±åº¦å’Œæ°´å†µä¸‹ä¸¥é‡å¤±çœŸï¼Œç°æœ‰åŸºäºé™†åœ°æ•°æ®è®­ç»ƒçš„è·Ÿè¸ªå™¨éš¾ä»¥æ³›åŒ–åˆ°è¿™äº›ç‰©ç†é©±åŠ¨çš„é€€åŒ–åœºæ™¯ï¼Œéœ€è¦ä¸“é—¨é’ˆå¯¹æ°´ä¸‹ç¯å¢ƒè®¾è®¡çš„è·Ÿè¸ªæ¡†æ¶ã€‚

**Method:** MANTAæ¡†æ¶é‡‡ç”¨åŒæ­£å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œå°†æ—¶é—´ä¸€è‡´æ€§çº¦æŸä¸Beer-Lambertç‰©ç†å¢å¼ºç›¸ç»“åˆï¼Œç”Ÿæˆå¯¹æ—¶é—´å’Œæ°´ä¸‹å¤±çœŸå‡é²æ£’çš„ç‰¹å¾è¡¨ç¤ºï¼›è¿›ä¸€æ­¥æå‡ºå¤šé˜¶æ®µè·Ÿè¸ªæµç¨‹ï¼Œåœ¨åŸºäºè¿åŠ¨çš„è·Ÿè¸ªåŸºç¡€ä¸Šå¼•å…¥ç‰©ç†æ„ŸçŸ¥çš„äºŒæ¬¡å…³è”ç®—æ³•ï¼Œæ•´åˆå‡ ä½•ä¸€è‡´æ€§å’Œå¤–è§‚ç›¸ä¼¼æ€§ä»¥åº”å¯¹é®æŒ¡å’Œæ¼‚ç§»æƒ…å†µï¼›åŒæ—¶æå‡ºä¸­å¿ƒå°ºåº¦ä¸€è‡´æ€§å’Œå‡ ä½•å¯¹é½åˆ†æ•°ä½œä¸ºå‡ ä½•ä¿çœŸåº¦çš„è¯„ä¼°æŒ‡æ ‡ã€‚

**Result:** åœ¨å››ä¸ªæ°´ä¸‹åŸºå‡†æ•°æ®é›†ï¼ˆWebUOT-1Mã€UOT32ã€UTB180ã€UWCOT220ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMANTAå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°†Success AUCæŒ‡æ ‡æå‡é«˜è¾¾6%ï¼ŒåŒæ—¶ç¡®ä¿äº†ç¨³å®šçš„é•¿æœŸæ³›åŒ–æ°´ä¸‹è·Ÿè¸ªèƒ½åŠ›å’Œé«˜æ•ˆçš„è¿è¡Œæ—¶æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜å°†ç‰©ç†æ„ŸçŸ¥æœºåˆ¶æ•´åˆåˆ°è·Ÿè¸ªæ¡†æ¶ä¸­èƒ½æœ‰æ•ˆåº”å¯¹æ°´ä¸‹ç¯å¢ƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒæ­£å¯¹æ¯”å­¦ä¹ ç­–ç•¥å’Œç‰©ç†å¢å¼ºçš„å…³è”ç®—æ³•ä¸ºæ°´ä¸‹è§†è§‰ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆæ–¹å‘ï¼Œæ‰€æå‡ºçš„å‡ ä½•è¯„ä¼°æŒ‡æ ‡ä¸ºæ°´ä¸‹è·Ÿè¸ªæ€§èƒ½æä¾›äº†æ›´å…¨é¢çš„è¡¡é‡æ ‡å‡†ã€‚

---

#### ğŸ“„ Abstract
Underwater object tracking is challenging due to wavelength dependent attenuation and scattering, which severely distort appearance across depths and water conditions. Existing trackers trained on terrestrial data fail to generalize to these physics-driven degradations. We present MANTA, a physics-informed framework integrating representation learning with tracking design for underwater scenarios. We propose a dual-positive contrastive learning strategy coupling temporal consistency with Beer-Lambert augmentations to yield features robust to both temporal and underwater distortions. We further introduce a multi-stage pipeline augmenting motion-based tracking with a physics-informed secondary association algorithm that integrates geometric consistency and appearance similarity for re-identification under occlusion and drift. To complement standard IoU metrics, we propose Center-Scale Consistency (CSC) and Geometric Alignment Score (GAS) to assess geometric fidelity. Experiments on four underwater benchmarks (WebUOT-1M, UOT32, UTB180, UWCOT220) show that MANTA achieves state-of-the-art performance, improving Success AUC by up to 6 percent, while ensuring stable long-term generalized underwater tracking and efficient runtime.


### [88] [DisMo: Disentangled Motion Representations for Open-World Motion Transfer](https://arxiv.org/abs/2511.23428)
*Thomas Ressler-Antal, Frank Fundel, Malek Ben Alaya, Stefan Andreas Baumann, Felix Krause, Ming Gui, BjÃ¶rn Ommer*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºDisMoï¼Œä¸€ç§é€šè¿‡å›¾åƒç©ºé—´é‡å»ºç›®æ ‡ä»åŸå§‹è§†é¢‘æ•°æ®ä¸­å­¦ä¹ æŠ½è±¡è¿åŠ¨è¡¨ç¤ºçš„æ–°èŒƒå¼ï¼Œè¯¥è¡¨ç¤ºä¸é™æ€ä¿¡æ¯è§£è€¦ï¼Œæ”¯æŒå¼€æ”¾ä¸–ç•Œè¿åŠ¨è¿ç§»ï¼Œå¹¶èƒ½ä¸ç°æœ‰è§†é¢‘ç”Ÿæˆå™¨ç»“åˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘æ¨¡å‹ç¼ºä¹å°†è¿åŠ¨ä¸å†…å®¹åˆ†ç¦»çš„æ˜¾å¼è¡¨ç¤ºï¼Œé™åˆ¶äº†å†…å®¹åˆ›ä½œçš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•åœ¨è¿åŠ¨ä¿çœŸåº¦å’Œæç¤ºéµå¾ªä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œå®¹æ˜“è¿‡åº¦æ‹Ÿåˆæºç»“æ„æˆ–åç¦»æè¿°åŠ¨ä½œï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿè§£è€¦è¿åŠ¨è¯­ä¹‰ä¸å¤–è§‚çš„é€šç”¨è¡¨ç¤ºã€‚

**Method:** æå‡ºDisMoèŒƒå¼ï¼Œé€šè¿‡å›¾åƒç©ºé—´é‡å»ºç›®æ ‡ç›´æ¥ä»åŸå§‹è§†é¢‘æ•°æ®å­¦ä¹ æŠ½è±¡è¿åŠ¨è¡¨ç¤ºã€‚è¯¥è¡¨ç¤ºä¸å¤–è§‚ã€ç‰©ä½“èº«ä»½æˆ–å§¿æ€ç­‰é™æ€ä¿¡æ¯æ— å…³ï¼Œæ”¯æŒå¼€æ”¾ä¸–ç•Œè¿åŠ¨è¿ç§»ï¼Œæ— éœ€ç‰©ä½“å¯¹åº”å…³ç³»ã€‚é€šè¿‡è½»é‡çº§é€‚é…å™¨å¯ä¸ä»»ä½•ç°æœ‰è§†é¢‘ç”Ÿæˆå™¨ç»“åˆï¼Œåˆ©ç”¨è§†é¢‘æ¨¡å‹çš„æœªæ¥è¿›å±•ã€‚

**Result:** æ–¹æ³•åœ¨å¤šæ ·åŒ–è¿åŠ¨è¿ç§»ä»»åŠ¡ä¸­å±•ç¤ºæœ‰æ•ˆæ€§ï¼Œå­¦ä¹ åˆ°çš„è¡¨ç¤ºåœ¨ä¸‹æ¸¸è¿åŠ¨ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨Something-Something v2å’ŒJesterç­‰åŸºå‡†ä¸Šï¼Œé›¶æ ·æœ¬åŠ¨ä½œåˆ†ç±»æ€§èƒ½æŒç»­è¶…è¶ŠV-JEPAç­‰æœ€å…ˆè¿›çš„è§†é¢‘è¡¨ç¤ºæ¨¡å‹ã€‚

**Conclusion:** DisMoæä¾›äº†ä¸€ç§é€šç”¨çš„è¿åŠ¨è¡¨ç¤ºæ–¹æ³•ï¼ŒæˆåŠŸè§£è€¦è¿åŠ¨è¯­ä¹‰ä¸å¤–è§‚ï¼Œæ”¯æŒè·¨è¯­ä¹‰æ— å…³å®ä½“çš„å‡†ç¡®è¿åŠ¨è¿ç§»ã€‚è¯¥æ–¹æ³•æ¡†æ¶çµæ´»ï¼Œå¯ä¸ç°æœ‰è§†é¢‘ç”Ÿæˆå™¨é›†æˆï¼Œä¸ºå†…å®¹åˆ›ä½œå’Œè¿åŠ¨ç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œå±•ç¤ºäº†å­¦ä¹ åˆ°çš„è¡¨ç¤ºåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚

---

#### ğŸ“„ Abstract
Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo


### [89] [Visual Generation Tuning](https://arxiv.org/abs/2511.23469)
*Jiahao Guo, Sinan Du, Jingfeng Yao, Wenyu Liu, Bo Li, Haoxiang Cao, Kun Gai, Chun Yuan, Kai Wu, Xinggang Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºVGTï¼ˆè§†è§‰ç”Ÿæˆè°ƒä¼˜ï¼‰èŒƒå¼ï¼Œé€šè¿‡é«˜æ•ˆè°ƒä¼˜é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹æ¥æ¿€å‘å…¶è§†è§‰ç”Ÿæˆæ½œåŠ›ï¼Œæ˜¾è‘—é™ä½å¯¹é½æˆæœ¬å¹¶åŠ é€Ÿè¿ç»­ç©ºé—´è‡ªå›å½’å»ºæ¨¡ï¼Œä¸ºæ¢ç´¢ä¸‹ä¸€ä»£ç»Ÿä¸€å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å¼€è¾Ÿæ–°é€”å¾„ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹é€šè¿‡å¹¿æ³›é¢„è®­ç»ƒè·å¾—äº†ä¸è¯­è¨€å¯¹é½çš„å¤æ‚è§†è§‰è¡¨ç¤ºï¼Œä½†è¿™äº›é’ˆå¯¹å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¼˜åŒ–çš„è¡¨ç¤ºæ˜¯å¦è•´å«è§†è§‰ç”Ÿæˆæ½œåŠ›ä»æœªå……åˆ†æ¢ç´¢ï¼Œå½“å‰ç ”ç©¶æ—¨åœ¨æ¿€å‘ä»»ä½•è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æ½œåœ¨çš„è§†è§‰ç”Ÿæˆèƒ½åŠ›ã€‚

**Method:** æå‡ºVGTï¼ˆè§†è§‰ç”Ÿæˆè°ƒä¼˜ï¼‰èŒƒå¼ï¼Œæ‘’å¼ƒä¸ºæ‰©æ•£å˜æ¢å™¨è®¾è®¡çš„çº ç¼ åƒç´ çº§VAEï¼Œé€šè¿‡å°†é¢„è®­ç»ƒVLMçš„è¯­ä¹‰ç¼–ç å™¨ä¸åƒç´ è§£ç å™¨çš„æ½œåœ¨è¡¨ç¤ºå¯¹é½æ¥æ„å»ºVGT-AEï¼Œå®ç°é«˜æ•ˆè§†è§‰ç”Ÿæˆè°ƒä¼˜ï¼Œæ˜¾è‘—é™ä½å¯¹é½æˆæœ¬å¹¶åŠ é€Ÿè¿ç»­ç©ºé—´è‡ªå›å½’å»ºæ¨¡æ”¶æ•›é€Ÿåº¦è¾¾20å€ã€‚

**Result:** åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸­ï¼ŒVGTåœ¨28å€å‹ç¼©æ¯”ä¸‹è¾¾åˆ°26.67 PSNRå’Œ0.50 rFIDï¼Œè¶…è¶Šä¸“ç”¨VAEï¼›åœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œåœ¨è‡ªå›å½’æ¨¡å‹ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼ŒGenEvalå¾—åˆ†ä¸º0.77ï¼ŒDPG-Benchå¾—åˆ†ä¸º78.73ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ‰©å±•æ½œåŠ›ã€‚

**Conclusion:** VGTèŒƒå¼ä¸ºæ¢ç´¢ä¸‹ä¸€ä»£ç»Ÿä¸€å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œèƒ½å¤Ÿä¸ºä»»ä½•ä¸ºå¤šæ¨¡æ€ç†è§£è®­ç»ƒçš„VLMèµ‹äºˆè§†è§‰ç”Ÿæˆèƒ½åŠ›ï¼Œå±•ç¤ºäº†å°†ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ç»Ÿä¸€äºå•ä¸€æ¨¡å‹çš„å·¨å¤§æ½œåŠ›ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„å‘å±•æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.


### [90] [AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement](https://arxiv.org/abs/2511.23475)
*Zhizhou Zhong, Yicheng Ji, Zhe Kong, Yiying Liu, Jiarui Wang, Jiasun Feng, Lupeng Liu, Xiangyi Wang, Yanjia Li, Yuqing She, Ying Qin, Huan Li, Shuiyang Mao, Wei Liu, Wenhan Luo*

#### ğŸ§© TL;DR
AnyTalkeræ˜¯ä¸€ä¸ªå¤šäººç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å¯æ‰©å±•çš„å¤šæµå¤„ç†æ¶æ„å’Œèº«ä»½æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†éŸ³é¢‘é©±åŠ¨çš„å¤šäººè¯´è¯è§†é¢‘ç”Ÿæˆï¼Œåœ¨æ•°æ®æˆæœ¬å’Œèº«ä»½å¯æ‰©å±•æ€§ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰éŸ³é¢‘é©±åŠ¨çš„å¤šäººè¯´è¯è§†é¢‘ç”Ÿæˆé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå¤šæ ·åŒ–å¤šäººæ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œä»¥åŠéš¾ä»¥é©±åŠ¨å¤šä¸ªèº«ä»½å®ç°è¿è´¯çš„äº¤äº’æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™äº›æŒ‘æˆ˜æ—¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ›´é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚

**Method:** AnyTalkeré‡‡ç”¨å¯æ‰©å±•çš„å¤šæµå¤„ç†æ¶æ„ï¼Œé€šè¿‡æ‰©å±•Diffusion Transformerçš„æ³¨æ„åŠ›å—ï¼Œå¼•å…¥æ–°é¢–çš„èº«ä»½æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿­ä»£å¤„ç†èº«ä»½-éŸ³é¢‘å¯¹ï¼Œå®ç°ä»»æ„æ•°é‡çš„å¯é©±åŠ¨èº«ä»½ã€‚è®­ç»ƒæµç¨‹ä»…ä¾èµ–å•äººè§†é¢‘å­¦ä¹ å¤šäººè¯´è¯æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨å°‘é‡çœŸå®å¤šäººç‰‡æ®µç²¾ç‚¼äº¤äº’æ€§ã€‚

**Result:** å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAnyTalkeråœ¨å”‡éƒ¨åŒæ­¥ã€è§†è§‰è´¨é‡å’Œè‡ªç„¶äº¤äº’æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚è¯¥æ¡†æ¶è¿˜è´¡çŒ®äº†ä¸“é—¨è®¾è®¡çš„è¯„ä¼°æŒ‡æ ‡å’Œæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆçš„å¤šäººè§†é¢‘çš„è‡ªç„¶æ€§å’Œäº¤äº’æ€§è´¨é‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡åˆ›æ–°çš„æ¶æ„è®¾è®¡å’Œé«˜æ•ˆçš„æ•°æ®åˆ©ç”¨ç­–ç•¥ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–å¤§è§„æ¨¡å¤šäººæ•°æ®é›†çš„æƒ…å†µä¸‹å®ç°é«˜è´¨é‡çš„å¤šäººè§†é¢‘ç”Ÿæˆã€‚èº«ä»½æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ä¸ºå¤šäººç”Ÿæˆä»»åŠ¡æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæœªæ¥å¤šäººäº¤äº’è§†é¢‘ç”Ÿæˆç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚

---

#### ğŸ“„ Abstract
Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.


### [91] [Video-CoM: Interactive Video Reasoning via Chain of Manipulations](https://arxiv.org/abs/2511.23477)
*Hanoona Rasheed, Mohammed Zumri, Muhammad Maaz, Ming-Hsuan Yang, Fahad Shahbaz Khan, Salman Khan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†äº¤äº’å¼è§†é¢‘æ¨ç†æ–°èŒƒå¼Video CoMï¼Œé€šè¿‡æ“ä½œé“¾æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿä¸»åŠ¨ä¸è§†é¢‘äº¤äº’è¿›è¡Œæ¨ç†ï¼Œåœ¨ä¹ä¸ªè§†é¢‘æ¨ç†åŸºå‡†ä¸Šå¹³å‡æ€§èƒ½æå‡3.6%ï¼ŒåŒæ—¶ä»…éœ€å°‘é‡è®­ç»ƒæ•°æ®ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£ä¸­é‡‡ç”¨è¢«åŠ¨èŒƒå¼ï¼Œå°†è§†è§‰è¾“å…¥è§†ä¸ºé™æ€ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´è¯­ä¹‰ç“¶é¢ˆï¼šæ¨¡å‹æ— æ³•å›çœ‹ã€é‡æ–°èšç„¦æˆ–éªŒè¯è¯æ®ï¼Œåœ¨éœ€è¦ç»†ç²’åº¦æ—¶ç©ºç†è§£çš„ä»»åŠ¡ä¸Šåªèƒ½è¿›è¡Œæµ…å±‚è§†è§‰æ¨ç†ã€‚

**Method:** æå‡ºäº¤äº’å¼è§†é¢‘æ¨ç†èŒƒå¼ï¼Œå°†è§†é¢‘è½¬åŒ–ä¸ºä¸»åŠ¨è®¤çŸ¥å·¥ä½œç©ºé—´ï¼›å¼€å‘Video CoMæ¨¡å‹ï¼Œé€šè¿‡æ“ä½œé“¾æœºåˆ¶æ‰§è¡Œè¿­ä»£è§†è§‰åŠ¨ä½œæ¥æ”¶é›†å’Œç²¾ç‚¼è¯æ®ï¼›æ„å»ºåŒ…å«18Kæ ·æœ¬çš„Video CoM InstructæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼›é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ“ä½œç­–ç•¥ï¼Œå¼•å…¥å…·æœ‰æ¨ç†æ„ŸçŸ¥çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•å’Œæ­¥éª¤çº§æ¨ç†å¥–åŠ±ã€‚

**Result:** åœ¨ä¹ä¸ªè§†é¢‘æ¨ç†åŸºå‡†ä¸Šå–å¾—å¼ºåŠ²ç»“æœï¼Œå¹³å‡æ€§èƒ½æ¯”æœ€æ–°SOTAæ¨¡å‹æå‡3.6%ï¼›ä»…ä½¿ç”¨25Kç›‘ç£å¾®è°ƒå’Œ3K GRPOè§†é¢‘æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—å°‘äºå¯æ¯”çš„å¤§è§„æ¨¡æ¨¡å‹ï¼›æ¶ˆèç ”ç©¶è¡¨æ˜æ¨ç†æ„ŸçŸ¥å¥–åŠ±åŒæ—¶æé«˜äº†å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚

**Conclusion:** äº¤äº’å¼è§†é¢‘æ¨ç†èŒƒå¼çªç ´äº†ä¼ ç»Ÿè¢«åŠ¨è§†é¢‘ç†è§£çš„é™åˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸»åŠ¨ä¸è§†è§‰å†…å®¹äº¤äº’è¿›è¡Œæ·±å…¥æ¨ç†ï¼›æ“ä½œé“¾æœºåˆ¶å’Œæ¨ç†æ„ŸçŸ¥å¥–åŠ±çš„ç»“åˆä¸ºç»†ç²’åº¦æ—¶ç©ºç†è§£æä¾›äº†æ–°æ–¹æ³•ï¼›è¯¥æ¡†æ¶åœ¨æ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸ºæœªæ¥è§†é¢‘ç†è§£ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still "think about videos" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to "think with videos". Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM


### [92] [Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models](https://arxiv.org/abs/2511.23478)
*Muhammad Maaz, Hanoona Rasheed, Fahad Shahbaz Khan, Salman Khan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºVideo R2æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¢å¼ºè§†é¢‘æ¨ç†çš„æ—¶é—´å¯¹é½å’Œé€»è¾‘ä¸€è‡´æ€§ï¼Œè§£å†³äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€è§†è§‰å†…å®¹æ¨ç†ä¸­å­˜åœ¨çš„é€»è¾‘ä¸ä¸€è‡´å’Œè§†è§‰è¯æ®ä¾èµ–ä¸è¶³çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€è§†è§‰å†…å®¹æ¨ç†ä¸­å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼šç”Ÿæˆçš„æ¨ç†è½¨è¿¹è™½ç„¶çœ‹ä¼¼åˆç†ï¼Œä½†å®é™…ä¸Šç»å¸¸å‡ºç°é€»è¾‘ä¸ä¸€è‡´æˆ–ç¼ºä¹è§†è§‰è¯æ®æ”¯æ’‘çš„æƒ…å†µã€‚ç ”ç©¶é€šè¿‡ä¸¤ä¸ªè¯Šæ–­æŒ‡æ ‡ï¼ˆæ€ç»´ç­”æ¡ˆä¸€è‡´æ€§å’Œè§†é¢‘æ³¨æ„åŠ›åˆ†æ•°ï¼‰å½¢å¼åŒ–è¿™äº›é—®é¢˜ï¼Œå‘ç°ç°æœ‰æ¨¡å‹è¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒè€Œéè§†è§‰å†…å®¹ã€‚

**Method:** æå‡ºä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆæ—¶é—´æˆ³æ„ŸçŸ¥çš„ç›‘ç£å¾®è°ƒå’ŒåŸºäºæ–°å‹æ—¶é—´å¯¹é½å¥–åŠ±çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŒé˜¶æ®µåè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡æ—¶é—´å¯¹é½å¥–åŠ±å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ—¶é—´å¯¹é½ä¸”å› æœè¿è´¯çš„è§†é¢‘æ¨ç†ï¼Œå¢å¼ºæ¨ç†çš„è§†è§‰ä¾èµ–æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ã€‚

**Result:** åœ¨11ä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVideo R2æ¨¡å‹åœ¨æ€ç»´ç­”æ¡ˆä¸€è‡´æ€§ã€è§†é¢‘æ³¨æ„åŠ›åˆ†æ•°å’Œå‡†ç¡®ç‡æ–¹é¢å‡å–å¾—æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ—¶é—´å¯¹é½å’Œæ¨ç†ä¸€è‡´æ€§çš„æ”¹è¿›ç›´æ¥è½¬åŒ–ä¸ºæ›´å‡†ç¡®å’Œå¯ä¿¡çš„è§†é¢‘ç†è§£æ€§èƒ½ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ—¶é—´å¯¹é½å’Œæ¨ç†ä¸€è‡´æ€§èƒ½å¤Ÿæœ‰æ•ˆæå‡è§†é¢‘æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ã€‚è¯¥æ–¹æ³•ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†é¢‘ç†è§£æä¾›äº†æ–°çš„ä¼˜åŒ–æ–¹å‘ï¼Œå¼ºè°ƒäº†è§†è§‰è¯æ®ä¾èµ–å’Œé€»è¾‘ä¸€è‡´æ€§çš„é‡è¦æ€§ï¼Œç›¸å…³ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å°†å¼€æºã€‚

---

#### ğŸ“„ Abstract
Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [93] [Insight-A: Attribution-aware for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.21705)
*Junjie Wu, Yumeng Fu, Chen Gong, Guohong Fu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºInsight-Aæ¡†æ¶ï¼Œé€šè¿‡æ¢ç´¢å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å½’å› èƒ½åŠ›æ¥æ£€æµ‹AIGCç”Ÿæˆçš„å¤šæ¨¡æ€è™šå‡ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•ä¸ä»…è¯†åˆ«è™šå‡å†…å®¹ï¼Œè¿˜å°†å…¶å½’å› äºç‰¹å®šçš„ä¼ªé€ æ¥æºã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** AIGCæŠ€æœ¯å·²æˆä¸ºç¤¾äº¤åª’ä½“ä¸Šå¤šæ¨¡æ€è™šå‡ä¿¡æ¯çš„ä¸»è¦æ¥æºï¼Œå¯¹ç¤¾ä¼šå®‰å…¨æ„æˆä¸¥é‡å¨èƒã€‚ç°æœ‰åŸºäºæ ‡å‡†æç¤ºçš„æ–¹æ³•åˆ©ç”¨MLLMsæ£€æµ‹è™šå‡ä¿¡æ¯ï¼Œä½†å¿½ç•¥äº†è™šå‡ä¿¡æ¯çš„å½’å› åˆ†æï¼Œæ— æ³•è¿½è¸ªä¼ªé€ æ¥æºå’Œç”Ÿæˆæ¨¡å¼ã€‚

**Method:** Insight-Aæ¡†æ¶é‡‡ç”¨åˆ†å±‚æ¨ç†ç®¡é“ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šäº¤å‰å½’å› æç¤ºï¼ˆCAPï¼‰ç”¨äºå»ºæ¨¡æ„ŸçŸ¥ä¸æ¨ç†ä¹‹é—´çš„å¤æ‚å…³è”ï¼Œå°†è™šå‡ä¿¡æ¯å½’å› äºåŸºäºç”Ÿæˆæ¨¡å¼çš„ä¼ªé€ ç—•è¿¹ï¼›è‡ªåŠ¨å½’å› å»åæç¤ºï¼ˆADPï¼‰ç”¨äºå‡å°‘äººå·¥æ ‡æ³¨æç¤ºçš„ä¸»è§‚æ€§ï¼Œå®ç°MLLMsçš„ä»»åŠ¡é€‚åº”ã€‚åŒæ—¶è®¾è®¡å›¾åƒæè¿°ï¼ˆICï¼‰æ¨¡å—æå–è§†è§‰ç»†èŠ‚ï¼Œå¢å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§æ£€æŸ¥ã€‚

**Result:** å¤§é‡å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œåœ¨æ£€æµ‹AIGCç”Ÿæˆçš„å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¡†æ¶ä¸ºè™šå‡ä¿¡æ¯æ£€æµ‹æä¾›äº†æ–°çš„èŒƒå¼ï¼Œç‰¹åˆ«æ˜¯åœ¨å½’å› åˆ†æå’Œè·¨æ¨¡æ€ä¸€è‡´æ€§éªŒè¯æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºAIGCæ—¶ä»£çš„å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹æä¾›äº†æ–°èŒƒå¼ï¼Œå¼ºè°ƒå½’å› åˆ†æçš„é‡è¦æ€§ã€‚é€šè¿‡å°†æ£€æµ‹ä¸ä¼ªé€ æ¥æºè¿½è¸ªç›¸ç»“åˆï¼ŒInsight-Aä¸ä»…æé«˜äº†æ£€æµ‹å‡†ç¡®æ€§ï¼Œè¿˜ä¸ºç†è§£è™šå‡ä¿¡æ¯çš„ç”Ÿæˆæœºåˆ¶å’Œä¼ æ’­è·¯å¾„æä¾›äº†æ–°è§†è§’ã€‚

---

#### ğŸ“„ Abstract
AI-generated content (AIGC) technology has emerged as a prevalent alternative to create multimodal misinformation on social media platforms, posing unprecedented threats to societal safety. However, standard prompting leverages multimodal large language models (MLLMs) to identify the emerging misinformation, which ignores the misinformation attribution. To this end, we present Insight-A, exploring attribution with MLLM insights for detecting multimodal misinformation. Insight-A makes two efforts: I) attribute misinformation to forgery sources, and II) an effective pipeline with hierarchical reasoning that detects distortions across modalities. Specifically, to attribute misinformation to forgery traces based on generation patterns, we devise cross-attribution prompting (CAP) to model the sophisticated correlations between perception and reasoning. Meanwhile, to reduce the subjectivity of human-annotated prompts, automatic attribution-debiased prompting (ADP) is used for task adaptation on MLLMs. Additionally, we design image captioning (IC) to achieve visual details for enhancing cross-modal consistency checking. Extensive experiments demonstrate the superiority of our proposal and provide a new paradigm for multimodal misinformation detection in the era of AIGC.


### [94] [An Optimized Machine Learning Classifier for Detecting Fake Reviews Using Extracted Features](https://arxiv.org/abs/2511.21716)
*Shabbir Anees, Anshuman, Ayush Chaurasia, Prathmesh Bogar*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„é«˜çº§ç³»ç»Ÿï¼Œç”¨äºé«˜ç²¾åº¦æ£€æµ‹äººå·¥æ™ºèƒ½ç”Ÿæˆçš„è™šå‡è¯„è®ºï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬é¢„å¤„ç†ã€å¤šæ¨¡æ€ç‰¹å¾æå–ã€å“ˆé‡Œæ–¯é¹°ä¼˜åŒ–ç‰¹å¾é€‰æ‹©å’Œå †å é›†æˆåˆ†ç±»å™¨ï¼Œåœ¨å…¬å¼€æ•°æ®é›†ä¸Šå®ç°äº†95.40%çš„å‡†ç¡®ç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åœ¨çº¿è´­ç‰©ä¸­è™šå‡è¯„è®ºæŸå®³äº†å¹³å°çš„å¯ä¿¡åº¦ï¼Œç‰¹åˆ«æ˜¯äººå·¥æ™ºèƒ½ç”Ÿæˆçš„è¯„è®ºä¸äººç±»æ’°å†™çš„è¯„è®ºæ··åˆå‡ºç°ï¼Œä½¿å¾—æ¶ˆè´¹è€…éš¾ä»¥è¾¨åˆ«çœŸä¼ªï¼Œè¿™ç§æ–°å‹çš„è®¡ç®—æœºç”Ÿæˆè¯„è®ºå¯¹ç°æœ‰æ£€æµ‹æ–¹æ³•æå‡ºäº†æŒ‘æˆ˜ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨å…ˆè¿›çš„æ–‡æœ¬é¢„å¤„ç†æŠ€æœ¯ï¼Œç»“åˆå¤šæ¨¡æ€ç‰¹å¾æå–ä»è¯„è®ºä¸­è·å–ä¸°å¯Œç‰¹å¾ï¼Œä½¿ç”¨å“ˆé‡Œæ–¯é¹°ä¼˜åŒ–ç®—æ³•è¿›è¡Œç‰¹å¾é€‰æ‹©ä»¥é™ä½ç»´åº¦ï¼Œæœ€åæ„å»ºå †å é›†æˆåˆ†ç±»å™¨è¿›è¡Œæœ€ç»ˆåˆ†ç±»å†³ç­–ã€‚

**Result:** åœ¨åŒ…å«40,432æ¡åŸå§‹è¯„è®ºå’Œè®¡ç®—æœºç”Ÿæˆè¯„è®ºçš„å…¬å¼€æ•°æ®é›†ä¸Šï¼Œå“ˆé‡Œæ–¯é¹°ä¼˜åŒ–ç®—æ³•ä»åˆå§‹13,539ä¸ªç‰¹å¾ä¸­é€‰æ‹©äº†1,368ä¸ªæœ€ç›¸å…³ç‰¹å¾ï¼Œå®ç°äº†89.9%çš„ç»´åº¦ç¼©å‡ï¼Œæœ€ç»ˆå †å æ¨¡å‹è¾¾åˆ°äº†95.40%å‡†ç¡®ç‡ã€92.81%ç²¾ç¡®ç‡ã€95.01%å¬å›ç‡å’Œ93.90% F1åˆ†æ•°ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜é›†æˆå­¦ä¹ ä¸ç”Ÿç‰©å¯å‘å¼ä¼˜åŒ–çš„ç»“åˆæ˜¯æ£€æµ‹æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„æœ‰æ•ˆæ–¹æ³•ï¼Œè€ƒè™‘åˆ°å¤§è§„æ¨¡è¯„è®ºåˆ†æé€šå¸¸åœ¨äº‘å¹³å°ä¸Šè¿›è¡Œï¼Œéœ€è¦é‡‡ç”¨å·®åˆ†éšç§å’Œå®‰å…¨å¤–åŒ…ç­‰éšç§ä¿æŠ¤æŠ€æœ¯æ¥ä¿æŠ¤ç”¨æˆ·æ•°æ®ã€‚

---

#### ğŸ“„ Abstract
It is well known that fraudulent reviews cast doubt on the legitimacy and dependability of online purchases. The most recent development that leads customers towards darkness is the appearance of human reviews in computer-generated (CG) ones. In this work, we present an advanced machine-learning-based system that analyses these reviews produced by AI with remarkable precision. Our method integrates advanced text preprocessing, multi-modal feature extraction, Harris Hawks Optimization (HHO) for feature selection, and a stacking ensemble classifier. We implemented this methodology on a public dataset of 40,432 Original (OR) and Computer-Generated (CG) reviews. From an initial set of 13,539 features, HHO selected the most applicable 1,368 features, achieving an 89.9% dimensionality reduction. Our final stacking model achieved 95.40% accuracy, 92.81% precision, 95.01% recall, and a 93.90% F1-Score, which demonstrates that the combination of ensemble learning and bio-inspired optimisation is an effective method for machine-generated text recognition. Because large-scale review analytics commonly run on cloud platforms, privacy-preserving techniques such as differential approaches and secure outsourcing are essential to protect user data in these systems.


### [95] [CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution](https://arxiv.org/abs/2511.21717)
*Baoliang Tian, Yuxuan Si, Jilong Wang, Lingyao Li, Zhongyuan Bao, Zineng Zhou, Tao Wang, Sixu Li, Ziyao Xu, Mingze Wang, Zhouzhuo Zhang, Zhihao Wang, Yike Yun, Ke Tian, Ning Yang, Minghui Qiu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CrossCheck-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹å’Œè§£å†³è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§æ–¹é¢èƒ½åŠ›çš„è¯Šæ–­æ€§åŸºå‡†ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨é€»è¾‘çŸ›ç›¾æ£€æµ‹æ–¹é¢çš„æ˜¾è‘—æ€§èƒ½ä¸‹é™ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸»è¦åœ¨å¯¹é½çš„å›¾åƒ-æ–‡æœ¬å¯¹ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œè¿™å¯¼è‡´å®ƒä»¬åœ¨æ£€æµ‹å’Œè§£å†³ç°å®ä¸–ç•Œä¸­çš„è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨å¼€æ”¾åŸŸåº”ç”¨ä¸­ï¼Œè§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ç»å¸¸å‘ç”Ÿå†²çªï¼Œéœ€è¦æ¨¡å‹è¿›è¡Œè¶…è¶Šè¡¨é¢å±‚é¢å¯¹é½çš„ç»“æ„åŒ–æ¨ç†ã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†CrossCheck-Benchè¯Šæ–­åŸºå‡†ï¼Œé‡‡ç”¨åˆ†å±‚ä»»åŠ¡æ¡†æ¶è¦†ç›–ä¸‰ä¸ªæ¨ç†å¤æ‚åº¦çº§åˆ«ï¼Œå¹¶å®šä¹‰äº†è§£å†³è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§æ‰€éœ€çš„ä¸ƒç§åŸå­èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«15kä¸ªä»çœŸå®ä¸–ç•Œæ•°æ®æºè·å–çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œå¹¶é€šè¿‡åˆæˆæ³¨å…¥çŸ›ç›¾çš„æ–¹å¼æ„å»ºï¼Œé‡‡ç”¨å¤šé˜¶æ®µæ ‡æ³¨æµç¨‹ï¼Œæ¶‰åŠè¶…è¿‡450ä¸ªä¸“å®¶å°æ—¶ä»¥ç¡®ä¿è¯­ä¹‰æœ‰æ•ˆæ€§å’Œæ ¡å‡†éš¾åº¦ã€‚

**Result:** è¯„ä¼°äº†13ä¸ªæœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œè§‚å¯Ÿåˆ°éšç€ä»»åŠ¡ä»æ„ŸçŸ¥åŒ¹é…è½¬å‘é€»è¾‘çŸ›ç›¾æ£€æµ‹ï¼Œæ€§èƒ½å‡ºç°ä¸€è‡´ä¸‹é™ã€‚å¤§å¤šæ•°æ¨¡å‹åœ¨å­¤ç«‹å®ä½“è¯†åˆ«ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦ç»¼åˆå¤šä¸ªçº¿ç´¢è¿›è¡Œå†²çªæ¨ç†æ—¶å¤±è´¥ã€‚èƒ½åŠ›å±‚çº§åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†æŠ€èƒ½è·å–çš„ä¸å‡è¡¡æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šæ­¥æ¨ç†æˆ–åŸºäºè§„åˆ™éªŒè¯çš„ä»»åŠ¡ä¸­ã€‚ä¼ ç»Ÿæç¤ºç­–ç•¥å¦‚æ€ç»´é“¾å’Œæ ‡è®°é›†ä»…å¸¦æ¥è¾¹é™…æ”¹è¿›ï¼Œè€Œå°†ç¬¦å·æ¨ç†ä¸åŸºç¡€è§†è§‰å¤„ç†äº¤ç»‡çš„æ–¹æ³•åˆ™å®ç°æ›´ç¨³å®šçš„æå‡ã€‚

**Conclusion:** ç ”ç©¶ç»“æœçªæ˜¾äº†å¤šæ¨¡æ€æ¨ç†ä¸­å­˜åœ¨çš„æŒç»­ç“¶é¢ˆï¼Œè¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨è·¨æ¨¡æ€éªŒè¯èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚è¿™ä¸ºæ„å»ºèƒ½å¤Ÿè¿›è¡Œç¨³å¥è·¨æ¨¡æ€éªŒè¯çš„æ¨¡å‹æŒ‡å‡ºäº†æ–°æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯éœ€è¦å¼€å‘èƒ½å¤Ÿæœ‰æ•ˆæ•´åˆç¬¦å·æ¨ç†ä¸åŸºç¡€è§†è§‰å¤„ç†çš„æ–¹æ³•ã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Language Models are primarily trained and evaluated on aligned image-text pairs, which leaves their ability to detect and resolve real-world inconsistencies largely unexplored. In open-domain applications visual and textual cues often conflict, requiring models to perform structured reasoning beyond surface-level alignment. We introduce CrossCheck-Bench, a diagnostic benchmark for evaluating contradiction detection in multimodal inputs. The benchmark adopts a hierarchical task framework covering three levels of reasoning complexity and defines seven atomic capabilities essential for resolving cross-modal inconsistencies. CrossCheck-Bench includes 15k question-answer pairs sourced from real-world artifacts with synthetically injected contradictions. The dataset is constructed through a multi-stage annotation pipeline involving more than 450 expert hours to ensure semantic validity and calibrated difficulty across perception, integration, and reasoning. We evaluate 13 state-of-the-art vision-language models and observe a consistent performance drop as tasks shift from perceptual matching to logical contradiction detection. Most models perform well on isolated entity recognition but fail when multiple clues must be synthesized for conflict reasoning. Capability-level analysis further reveals uneven skill acquisition, especially in tasks requiring multi-step inference or rule-based validation. Additional probing shows that conventional prompting strategies such as Chain-of-Thought and Set-of-Mark yield only marginal gains. By contrast, methods that interleave symbolic reasoning with grounded visual processing achieve more stable improvements. These results highlight a persistent bottleneck in multimodal reasoning and suggest new directions for building models capable of robust cross-modal verification.


### [96] [Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue](https://arxiv.org/abs/2511.21728)
*Lin Yu, Xiaofei Han, Yifei Kang, Chiung-Yi Tseng, Danyang Zhang, Ziqian Bi, Zhimo Han*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºAffectMindï¼Œä¸€ç§å¤šæ¨¡æ€æƒ…æ„Ÿå¯¹è¯ä»£ç†ï¼Œé€šè¿‡ä¸»åŠ¨æ¨ç†å’ŒåŠ¨æ€çŸ¥è¯†åŸºç¡€æ¥ç»´æŒæƒ…æ„Ÿå¯¹é½çš„è¯´æœæ€§äº’åŠ¨ï¼Œè§£å†³äº†ç°æœ‰LLMåœ¨æƒ…æ„Ÿä¸°å¯Œã€ç›®æ ‡å¯¼å‘åœºæ™¯ä¸­ååº”æ€§å’Œæƒ…æ„Ÿä¸€è‡´æ€§ä¸è¶³çš„é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æµç•…å¯¹è¯ç³»ç»Ÿæ–¹é¢å–å¾—è¿›å±•ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹åœ¨æƒ…æ„Ÿä¸°å¯Œã€ç›®æ ‡å¯¼å‘çš„åœºæ™¯ï¼ˆå¦‚è¥é”€å¯¹è¯ï¼‰ä¸­ä»ä¿æŒè¢«åŠ¨ååº”æ€§ï¼Œç¼ºä¹æƒ…æ„Ÿå¯¹é½å’Œä¸»åŠ¨è¯´æœèƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å•†ä¸šå¤šæ¨¡æ€ä»£ç†ä¸­çš„åº”ç”¨æ•ˆæœã€‚

**Method:** AffectMindåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸»åŠ¨çŸ¥è¯†åŸºç¡€ç½‘ç»œï¼ˆPKGNï¼‰ä»æ–‡æœ¬ã€è§†è§‰å’ŒéŸµå¾‹ä¸­æŒç»­æ›´æ–°äº‹å®å’Œæƒ…æ„Ÿä¸Šä¸‹æ–‡ï¼›æƒ…æ„Ÿ-æ„å›¾å¯¹é½æ¨¡å‹ï¼ˆEIAMï¼‰è”åˆå»ºæ¨¡ç”¨æˆ·æƒ…æ„Ÿå’Œè´­ä¹°æ„å›¾ä»¥è°ƒæ•´è¯´æœç­–ç•¥ï¼›å¼ºåŒ–è¯è¯­å¾ªç¯ï¼ˆRDLï¼‰é€šè¿‡ç”¨æˆ·å“åº”çš„å¼ºåŒ–ä¿¡å·ä¼˜åŒ–æƒ…æ„Ÿè¿è´¯æ€§å’Œå‚ä¸åº¦ã€‚

**Result:** åœ¨ä¸¤ä¸ªæ–°æ„å»ºçš„è¥é”€å¯¹è¯æ•°æ®é›†MM-ConvMarketå’ŒAffectPromoä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAffectMindåœ¨æƒ…æ„Ÿä¸€è‡´æ€§ï¼ˆ+26%ï¼‰ã€è¯´æœæˆåŠŸç‡ï¼ˆ+19%ï¼‰å’Œé•¿æœŸç”¨æˆ·å‚ä¸åº¦ï¼ˆ+23%ï¼‰æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºäºLLMçš„å¼ºåŸºçº¿æ¨¡å‹ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜æƒ…æ„ŸåŸºç¡€çš„ä¸»åŠ¨æ€§æ˜¯å•†ä¸šå¤šæ¨¡æ€ä»£ç†çš„å…³é”®èƒ½åŠ›ï¼Œé€šè¿‡å¤šæ¨¡æ€æƒ…æ„Ÿå»ºæ¨¡å’Œä¸»åŠ¨æ¨ç†å¯ä»¥æ˜¾è‘—æå‡å¯¹è¯ç³»ç»Ÿçš„æƒ…æ„Ÿå¯¹é½å’Œè¯´æœæ•ˆæœï¼Œä¸ºæƒ…æ„Ÿæ™ºèƒ½å¯¹è¯ç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\%), persuasive success rate (+19\%), and long-term user engagement (+23\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.


### [97] [HUMORCHAIN: Theory-Guided Multi-Stage Reasoning for Interpretable Multimodal Humor Generation](https://arxiv.org/abs/2511.21732)
*Jiajun Zhang, Shijia Luo, Ruikang Zhang, Qi Su*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†HUMORCHAINï¼Œä¸€ç§ç†è®ºæŒ‡å¯¼çš„å¤šé˜¶æ®µæ¨ç†æ¡†æ¶ï¼Œé¦–æ¬¡å°†å¹½é»˜ç†è®ºçš„è®¤çŸ¥ç»“æ„æ˜¾å¼åµŒå…¥å¤šæ¨¡æ€å¹½é»˜ç”Ÿæˆä¸­ï¼Œé€šè¿‡ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ä»è§†è§‰ç†è§£åˆ°å¹½é»˜åˆ›é€ ï¼Œæ˜¾è‘—æå‡äº†AIç”Ÿæˆå¹½é»˜çš„è´¨é‡å’Œäººç±»åå¥½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ•°æ®é©±åŠ¨æ–¹æ³•ç¼ºä¹å¯¹å¹½é»˜çš„æ˜¾å¼å»ºæ¨¡æˆ–ç†è®ºåŸºç¡€ï¼Œé€šå¸¸äº§ç”Ÿå­—é¢æè¿°è€Œæ— æ³•æ•æ‰å…¶åº•å±‚è®¤çŸ¥æœºåˆ¶ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒæè¿°æµç•…ä½†ç¼ºä¹çœŸæ­£çš„å¹½é»˜æˆ–è®¤çŸ¥æ·±åº¦ã€‚å¤šæ¨¡æ€å¹½é»˜å·²æˆä¸ºåœ¨çº¿äº¤æµçš„æ™®éå½¢å¼ï¼Œç‰¹åˆ«æ˜¯Zä¸–ä»£ï¼Œå‡¸æ˜¾äº†éœ€è¦èƒ½å¤Ÿæ•´åˆè§†è§‰ç†è§£ä¸å¹½é»˜è¯­è¨€ç”Ÿæˆçš„AIç³»ç»Ÿã€‚

**Method:** æœ¬æ–‡æå‡ºäº†HUMORCHAINï¼ˆHUmor-guided Multi-step Orchestrated Reasoning Chain for Image Captioningï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç†è®ºæŒ‡å¯¼çš„å¤šé˜¶æ®µæ¨ç†æ¡†æ¶ï¼Œé›†æˆäº†è§†è§‰è¯­ä¹‰è§£æã€åŸºäºå¹½é»˜å’Œå¿ƒç†å­¦çš„æ¨ç†ï¼Œä»¥åŠç”¨äºå¹½é»˜è¯„ä¼°çš„å¾®è°ƒåˆ¤åˆ«å™¨ï¼Œå½¢æˆä¸€ä¸ªå¯è§£é‡Šä¸”å¯æ§çš„è®¤çŸ¥æ¨ç†é“¾ã€‚è¯¥æ¡†æ¶é¦–æ¬¡å°†å¹½é»˜ç†è®ºçš„è®¤çŸ¥ç»“æ„æ˜¾å¼åµŒå…¥å¤šæ¨¡æ€å¹½é»˜ç”Ÿæˆä¸­ï¼Œå®ç°äº†ä»è§†è§‰ç†è§£åˆ°å¹½é»˜åˆ›é€ çš„ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ã€‚

**Result:** åœ¨Meme-Image-No-Textã€Oogiri-GOå’ŒOxfordTVG-HICæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHUMORCHAINåœ¨äººç±»å¹½é»˜åå¥½ã€Elo/BTåˆ†æ•°å’Œè¯­ä¹‰å¤šæ ·æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚è¿™äº›ç»“æœè¯æ˜äº†ç†è®ºé©±åŠ¨çš„ç»“æ„åŒ–æ¨ç†èƒ½å¤Ÿä½¿å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸äººç±»æ„ŸçŸ¥ä¸€è‡´çš„å¹½é»˜å†…å®¹ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å°†è®¤çŸ¥ç†è®ºç»“æ„æ˜¾å¼åµŒå…¥å¤šæ¨¡æ€å¹½é»˜ç”Ÿæˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºAIå¹½é»˜ç”Ÿæˆæä¾›äº†ç†è®ºæŒ‡å¯¼çš„ç»“æ„åŒ–æ–¹æ³•ã€‚HUMORCHAINæ¡†æ¶çš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ä¸ºæœªæ¥åœ¨æ›´å¹¿æ³›åˆ›æ„ä»»åŠ¡ä¸­åº”ç”¨è®¤çŸ¥æ¨ç†é“¾å¥ å®šäº†åŸºç¡€ï¼Œè¡¨æ˜ç†è®ºé©±åŠ¨çš„ç»“æ„åŒ–æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æå‡ç”Ÿæˆå†…å®¹çš„è´¨é‡å’Œäººç±»å¯¹é½æ€§ã€‚

---

#### ğŸ“„ Abstract
Humor, as both a creative human activity and a social binding mechanism, has long posed a major challenge for AI generation. Although producing humor requires complex cognitive reasoning and social understanding, theories of humor suggest that it follows learnable patterns and structures, making it theoretically possible for generative models to acquire them implicitly. In recent years, multimodal humor has become a prevalent form of online communication, especially among Gen Z, highlighting the need for AI systems capable of integrating visual understanding with humorous language generation. However, existing data-driven approaches lack explicit modeling or theoretical grounding of humor, often producing literal descriptions that fail to capture its underlying cognitive mechanisms, resulting in the generated image descriptions that are fluent but lack genuine humor or cognitive depth. To address this limitation, we propose HUMORCHAIN (HUmor-guided Multi-step Orchestrated Reasoning Chain for Image Captioning), a theory-guided multi-stage reasoning framework. It integrates visual semantic parsing, humor- and psychology-based reasoning, and a fine-tuned discriminator for humor evaluation, forming an interpretable and controllable cognitive reasoning chain. To the best of our knowledge, this is the first work to explicitly embed cognitive structures from humor theories into multimodal humor generation, enabling a structured reasoning process from visual understanding to humor creation. Experiments on Meme-Image-No-Text, Oogiri-GO, and OxfordTVG-HIC datasets show that HUMORCHAIN outperforms state-of-the-art baselines in human humor preference, Elo/BT scores, and semantic diversity, demonstrating that theory-driven structured reasoning enables large language models to generate humor aligned with human perception.


### [98] [Closing the Performance Gap Between AI and Radiologists in Chest X-Ray Reporting](https://arxiv.org/abs/2511.21735)
*Harshita Sharma, Maxwell C. Reynolds, Valentina Salvatelli, Anne-Marie G. Sykes, Kelly K. Horst, Anton Schwaighofer, Maximilian Ilse, Olesya Melnichenko, Sam Bond-Taylor, Fernando PÃ©rez-GarcÃ­a, Vamshi K. Mugu, Alex Chan, Ceylan Colak, Shelby A. Swartz, Motassem B. Nashawaty, Austin J. Gonzalez, Heather A. Ouellette, Selnur B. Erdal, Beth A. Schueler, Maria T. Wetscherek, Noel Codella, Mohit Jain, Shruthi Bannur, Kenza Bouzid, Daniel C. Castro, Stephanie Hyland, Panos Korfiatis, Ashish Khandelwal, Javier Alvarez-Valle*

#### ğŸ§© TL;DR
æœ¬æ–‡ä»‹ç»äº†MAIRA-Xï¼Œä¸€ç§ç”¨äºçºµå‘èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆçš„å¤šæ¨¡æ€AIæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä¸´åºŠè¯„ä¼°ä¸­æ˜¾è‘—æå‡äº†AIç”ŸæˆæŠ¥å‘Šåœ¨è¯æ±‡è´¨é‡ã€ä¸´åºŠæ­£ç¡®æ€§å’Œç®¡çº¿æŠ¥å‘Šæ–¹é¢çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡é¦–æ¬¡å›é¡¾æ€§ç”¨æˆ·ç ”ç©¶éªŒè¯äº†å…¶ä¸´åºŠå®ç”¨æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€ç­›æŸ¥æŒ‡å—æ‰©å±•ã€å¤æ‚ç—…ä¾‹å¢åŠ å’Œæ”¾å°„ç§‘åŒ»ç”ŸçŸ­ç¼ºï¼ŒAIè¾…åŠ©æŠ¥å‘Šç”Ÿæˆæœ‰æœ›å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œè´Ÿæ‹…ï¼ŒåŒæ—¶ä¿æŒè¯Šæ–­å‡†ç¡®æ€§ã€‚ç‰¹åˆ«æ˜¯èƒ¸éƒ¨Xå…‰æŠ¥å‘Šä¸­æè¿°ç—…ç†å‘ç°å’Œè§£è¯»ç®¡çº¿ï¼ˆL&Tï¼‰çš„ä»»åŠ¡æ—¢ç¹é‡åˆé‡å¤ï¼Œåœ¨é«˜æ‚£è€…é‡æƒ…å†µä¸‹å°¤ä¸ºçªå‡ºï¼Œéœ€è¦å¼€å‘èƒ½å¤ŸåŒæ—¶å¤„ç†ä¸´åºŠå‘ç°å’Œç®¡çº¿æŠ¥å‘Šçš„ç»¼åˆAIç³»ç»Ÿã€‚

**Method:** ç ”ç©¶å¼€å‘äº†MAIRA-Xå¤šæ¨¡æ€AIæ¨¡å‹ï¼Œä¸“é—¨ç”¨äºçºµå‘èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆï¼Œæ¶µç›–ä¸´åºŠå‘ç°å’Œç®¡çº¿æŠ¥å‘Šã€‚æ¨¡å‹åŸºäºæ¢…å¥¥è¯Šæ‰€çš„å¤§è§„æ¨¡ã€å¤šä¸­å¿ƒã€çºµå‘æ•°æ®é›†è®­ç»ƒï¼ŒåŒ…å«310ä¸‡é¡¹ç ”ç©¶ï¼ˆæ¥è‡ª80.6ä¸‡æ‚£è€…çš„600ä¸‡å¼ å›¾åƒï¼‰ã€‚ç ”ç©¶è¿˜å¼€å‘äº†æ–°é¢–çš„ç®¡çº¿ç‰¹å®šè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°ç®¡çº¿ç±»å‹ã€çºµå‘å˜åŒ–å’Œæ”¾ç½®ä½ç½®ç­‰å±æ€§çš„æŠ¥å‘Šå‡†ç¡®æ€§ï¼Œå¹¶è¿›è¡Œäº†é¦–æ¬¡å›é¡¾æ€§ç”¨æˆ·è¯„ä¼°ç ”ç©¶ã€‚

**Result:** åœ¨ä¸‰ä¸ªä¿ç•™æµ‹è¯•é›†å’Œå…¬å¼€MIMIC-CXRæ•°æ®é›†ä¸Šï¼ŒMAIRA-Xåœ¨è¯æ±‡è´¨é‡ã€ä¸´åºŠæ­£ç¡®æ€§å’Œç®¡çº¿ç›¸å…³å…ƒç´ æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚ç”¨æˆ·ç ”ç©¶æ¶‰åŠ9åä¸åŒç»éªŒæ°´å¹³çš„æ”¾å°„ç§‘åŒ»ç”Ÿå¯¹600é¡¹ç ”ç©¶è¿›è¡Œç›²å®¡ï¼Œç»“æœæ˜¾ç¤ºå…³é”®é”™è¯¯ç‡ç›¸å½“ï¼ˆåŸå§‹æŠ¥å‘Š3.0% vs AIç”ŸæˆæŠ¥å‘Š4.6%ï¼‰ï¼Œå¯æ¥å—å¥å­ç‡ç›¸ä¼¼ï¼ˆåŸå§‹97.8% vs AIç”Ÿæˆ97.4%ï¼‰ï¼Œç›¸æ¯”å…ˆå‰ç”¨æˆ·ç ”ç©¶æœ‰æ˜¾è‘—æ”¹è¿›ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜MAIRA-Xèƒ½å¤Ÿæœ‰æ•ˆè¾…åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜å®¹é‡ä¸´åºŠç¯å¢ƒä¸­ã€‚è¯¥æ¨¡å‹åœ¨ä¿æŒä¸´åºŠå®‰å…¨æ€§çš„åŒæ—¶å‡è½»äº†å·¥ä½œè´Ÿæ‹…ï¼Œä¸ºAIè¾…åŠ©æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„å®é™…ä¸´åºŠåº”ç”¨æä¾›äº†æœ‰åŠ›è¯æ®ï¼Œæ ‡å¿—ç€è¯¥é¢†åŸŸå‘ä¸´åºŠå®ç”¨åŒ–è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚

---

#### ğŸ“„ Abstract
AI-assisted report generation offers the opportunity to reduce radiologists' workload stemming from expanded screening guidelines, complex cases and workforce shortages, while maintaining diagnostic accuracy. In addition to describing pathological findings in chest X-ray reports, interpreting lines and tubes (L&T) is demanding and repetitive for radiologists, especially with high patient volumes. We introduce MAIRA-X, a clinically evaluated multimodal AI model for longitudinal chest X-ray (CXR) report generation, that encompasses both clinical findings and L&T reporting. Developed using a large-scale, multi-site, longitudinal dataset of 3.1 million studies (comprising 6 million images from 806k patients) from Mayo Clinic, MAIRA-X was evaluated on three holdout datasets and the public MIMIC-CXR dataset, where it significantly improved AI-generated reports over the state of the art on lexical quality, clinical correctness, and L&T-related elements. A novel L&T-specific metrics framework was developed to assess accuracy in reporting attributes such as type, longitudinal change and placement. A first-of-its-kind retrospective user evaluation study was conducted with nine radiologists of varying experience, who blindly reviewed 600 studies from distinct subjects. The user study found comparable rates of critical errors (3.0% for original vs. 4.6% for AI-generated reports) and a similar rate of acceptable sentences (97.8% for original vs. 97.4% for AI-generated reports), marking a significant improvement over prior user studies with larger gaps and higher error rates. Our results suggest that MAIRA-X can effectively assist radiologists, particularly in high-volume clinical settings.


### [99] [Decoding inner speech with an end-to-end brain-to-text neural interface](https://arxiv.org/abs/2511.21740)
*Yizi Zhang, Linyang He, Chaofei Fan, Tingkai Liu, Han Yu, Trung Le, Jingyuan Li, Scott Linderman, Lea Duncker, Francis R Willett, Nima Mesgarani, Liam Paninski*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„è„‘åˆ°æ–‡æœ¬ï¼ˆBITï¼‰æ¡†æ¶ï¼Œé€šè¿‡å•ä¸€å¯å¾®åˆ†ç¥ç»ç½‘ç»œå°†ç¥ç»æ´»åŠ¨ç›´æ¥ç¿»è¯‘ä¸ºè¿è´¯å¥å­ï¼Œæ˜¾è‘—æå‡äº†è¯­éŸ³è„‘æœºæ¥å£çš„è§£ç æ€§èƒ½ï¼Œå¹¶å®ç°äº†è·¨ä»»åŠ¡å’Œè·¨ç‰©ç§çš„è¡¨ç¤ºè¿ç§»ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è¯­éŸ³è„‘æœºæ¥å£å¤§å¤šé‡‡ç”¨çº§è”æ¡†æ¶ï¼Œå…ˆè§£ç éŸ³ç´ å†é€šè¿‡n-gramè¯­è¨€æ¨¡å‹ç»„è£…å¥å­ï¼Œè¿™ç§æ¶æ„é˜»ç¢äº†æ‰€æœ‰é˜¶æ®µçš„è”åˆä¼˜åŒ–ï¼Œé™åˆ¶äº†ç³»ç»Ÿæ€§èƒ½çš„è¿›ä¸€æ­¥æå‡ã€‚

**Method:** è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯è·¨ä»»åŠ¡ã€è·¨ç‰©ç§é¢„è®­ç»ƒçš„ç¥ç»ç¼–ç å™¨ï¼Œå…¶è¡¨ç¤ºå¯è¿ç§»åˆ°å°è¯•æ€§å’Œæƒ³è±¡æ€§è¯­éŸ³ï¼›è¯¥ç¼–ç å™¨ä¸éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ç«¯åˆ°ç«¯é›†æˆï¼Œå¹¶é€šè¿‡å¯¹æ¯”å­¦ä¹ è¿›è¡Œè·¨æ¨¡æ€å¯¹é½ï¼Œå½¢æˆå®Œæ•´çš„Brain-to-Textæ¡†æ¶ã€‚

**Result:** åœ¨çº§è”è®¾ç½®ä¸­ï¼Œé¢„è®­ç»ƒç¼–ç å™¨åœ¨Brain-to-Text '24å’Œ'25åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼›ç«¯åˆ°ç«¯é›†æˆåï¼Œå°†å…ˆå‰ç«¯åˆ°ç«¯æ–¹æ³•çš„è¯é”™è¯¯ç‡ä»24.69%æ˜¾è‘—é™ä½è‡³10.22%ï¼ŒåŒæ—¶å‘ç°å°è§„æ¨¡éŸ³é¢‘LLMèƒ½æ˜¾è‘—æ”¹å–„ç«¯åˆ°ç«¯è§£ç æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ä»…å®ç°äº†è®°å½•æ€§çš„æ€§èƒ½æå‡ï¼Œè¿˜é€šè¿‡å¯¹é½å°è¯•æ€§å’Œæƒ³è±¡æ€§è¯­éŸ³åµŒå…¥å®ç°äº†è·¨ä»»åŠ¡æ³›åŒ–ï¼Œä¸ºæ•´åˆå¤§è§„æ¨¡å¤šæ ·åŒ–ç¥ç»æ•°æ®é›†ã€æ„å»ºæ”¯æŒæ— ç¼å¯å¾®åˆ†ä¼˜åŒ–çš„ç«¯åˆ°ç«¯è§£ç æ¡†æ¶å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.


### [100] [A Multiscale Geometric Method for Capturing Relational Topic Alignment](https://arxiv.org/abs/2511.21741)
*Conrad D. Hougen, Karl T. Pazdernik, Alfred O. Hero*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å‡ ä½•æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆå¤šæ¨¡æ€æ–‡æœ¬å’Œåˆè‘—è€…ç½‘ç»œæ•°æ®ï¼Œæ„å»ºå±‚æ¬¡åŒ–ä¸»é¢˜æ ‘çŠ¶å›¾ï¼Œæœ‰æ•ˆè¯†åˆ«ç§‘å­¦æ–‡çŒ®ä¸­çš„ç¨€æœ‰ä¸»é¢˜å¹¶å¯è§†åŒ–ä¸»é¢˜éšæ—¶é—´çš„å¹³æ»‘æ¼‚ç§»ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºå¯†é›†TransformeråµŒå…¥çš„ä¸»é¢˜æ¨¡å‹åœ¨å¤„ç†ç§‘å­¦æ–‡çŒ®æ—¶å­˜åœ¨æ˜æ˜¾å±€é™ï¼Œå®ƒä»¬å¾€å¾€æ— æ³•æ•æ‰ç¨€æœ‰ä¸»é¢˜ï¼Œå¯¼è‡´åœ¨è¿½è¸ªåˆè‘—è€…ç¤¾åŒºç ”ç©¶å…´è¶£æ¼”å˜æ—¶ç¼ºä¹å¹³æ»‘çš„æ—¶é—´å¯¹é½èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«è¢«å¿½è§†çš„åˆ©åŸºä¸»é¢˜æ–¹é¢è¡¨ç°ä¸è¶³ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨å‡ ä½•æ¡†æ¶æ•´åˆå¤šæ¨¡æ€æ–‡æœ¬å’Œåˆè‘—è€…ç½‘ç»œæ•°æ®ï¼Œä½¿ç”¨Hellingerè·ç¦»å’ŒWardè¿æ¥ç®—æ³•æ„å»ºå±‚æ¬¡åŒ–ä¸»é¢˜æ ‘çŠ¶å›¾ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶æ•æ‰å±€éƒ¨å’Œå…¨å±€ç»“æ„ï¼Œæ”¯æŒè·¨è¯­ä¹‰å’Œæ—¶é—´ç»´åº¦çš„å¤šå°ºåº¦å­¦ä¹ ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆè¯†åˆ«ç¨€æœ‰ä¸»é¢˜ç»“æ„å¹¶å¯è§†åŒ–ä¸»é¢˜éšæ—¶é—´çš„å¹³æ»‘æ¼‚ç§»ï¼Œå±•ç¤ºäº†å¯è§£é‡Šçš„è¯è¢‹æ¨¡å‹ä¸åŸåˆ™æ€§å‡ ä½•å¯¹é½ç›¸ç»“åˆçš„ä¼˜åŠ¿ï¼Œåœ¨æ•æ‰ç§‘å­¦æ–‡çŒ®ä¸­æ–°é¢–å’Œä»£è¡¨æ€§ä¸è¶³çš„ä¸»é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†å¯è§£é‡Šçš„è¯è¢‹æ¨¡å‹ä¸å‡ ä½•å¯¹é½æ–¹æ³•ç›¸ç»“åˆçš„ä»·å€¼ï¼Œä¸ºè¿½è¸ªç§‘å­¦ç¤¾åŒºç ”ç©¶å…´è¶£æ¼”å˜æä¾›äº†æœ‰æ•ˆå·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«ç¨€æœ‰ä¸»é¢˜å’Œå¯è§†åŒ–ä¸»é¢˜æ¼‚ç§»æ–¹é¢å…·æœ‰é‡è¦åº”ç”¨å‰æ™¯ï¼Œä¸ºå¤šæ¨¡æ€æ•°æ®æ•´åˆçš„ä¸»é¢˜å»ºæ¨¡å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Interpretable topic modeling is essential for tracking how research interests evolve within co-author communities. In scientific corpora, where novelty is prized, identifying underrepresented niche topics is particularly important. However, contemporary models built from dense transformer embeddings tend to miss rare topics and therefore also fail to capture smooth temporal alignment. We propose a geometric method that integrates multimodal text and co-author network data, using Hellinger distances and Ward's linkage to construct a hierarchical topic dendrogram. This approach captures both local and global structure, supporting multiscale learning across semantic and temporal dimensions. Our method effectively identifies rare-topic structure and visualizes smooth topic drift over time. Experiments highlight the strength of interpretable bag-of-words models when paired with principled geometric alignment.


### [101] [DELTA: Language Diffusion-based EEG-to-Text Architecture](https://arxiv.org/abs/2511.21746)
*Mingyu Jeon, Hyobin Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºDELTAæ¨¡å‹ï¼Œé€šè¿‡æ®‹å·®å‘é‡é‡åŒ–EEGåˆ†è¯å™¨å’Œæ©ç è¯­è¨€æ‰©æ•£æ¨¡å‹ï¼Œè§£å†³EEGåˆ°æ–‡æœ¬è½¬æ¢ä¸­çš„å™ªå£°ã€ä¸ªä½“å·®å¼‚å’Œè‡ªå›å½’è§£ç è¯¯å·®ç´¯ç§¯é—®é¢˜ï¼Œåœ¨ZuCoæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†è¯­ä¹‰å¯¹é½æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è„‘ç”µå›¾åˆ°æ–‡æœ¬è½¬æ¢é¢ä¸´é«˜ç»´å™ªå£°ã€å—è¯•è€…ä¸ªä½“å·®å¼‚ä»¥åŠè‡ªå›å½’è§£ç ä¸­è¯¯å·®ç´¯ç§¯çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è¯­ä¹‰å¯¹é½å’Œå¯é æ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„EEGè¡¨å¾å’Œæ–‡æœ¬ç”Ÿæˆæ¡†æ¶ã€‚

**Method:** DELTAé‡‡ç”¨æ®‹å·®å‘é‡é‡åŒ–å°†è¿ç»­EEGä¿¡å·ç¦»æ•£åŒ–ä¸ºå¤šå±‚tokenä»¥å‡å°‘å™ªå£°å’Œä¸ªä½“å·®å¼‚ï¼ŒåŒæ—¶ç»“åˆæ©ç è¯­è¨€æ‰©æ•£æ¨¡å‹é€šè¿‡éé¡ºåºå»å™ªè¿‡ç¨‹é‡æ„å¥å­ï¼Œé¿å…äº†ä¼ ç»Ÿè‡ªå›å½’æ–¹æ³•çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ã€‚

**Result:** åœ¨ZuCoæ•°æ®é›†ä¸Šï¼ŒDELTAç›¸æ¯”è‡ªå›å½’åŸºçº¿å°†è¯­ä¹‰å¯¹é½æå‡äº†æœ€é«˜5.37åˆ†ï¼Œåœ¨è¯çº§æ¡ä»¶ä¸‹è¾¾åˆ°BLEU-1 21.9å’ŒROUGE-1 F 17.2ï¼Œå®ç°äº†ä»å°è§„æ¨¡EEG-æ–‡æœ¬æ•°æ®é›†ä¸­å¯é ç”Ÿæˆæ–‡æœ¬çš„ç›®æ ‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆç¦»æ•£åŒ–è¡¨å¾å’Œæ‰©æ•£æ¨¡å‹åœ¨EEGåˆ°æ–‡æœ¬è½¬æ¢ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºå¯æ‰©å±•çš„å¤šæ¨¡æ€EEG-è¯­è¨€æ¨¡å‹æä¾›äº†æ–°æ–¹å‘ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹å®ç°å¯é çš„æ–‡æœ¬ç”Ÿæˆã€‚

---

#### ğŸ“„ Abstract
Electroencephalogram (EEG)-to-text remains challenging due to high-dimensional noise, subject variability, and error accumulation in autoregressive decoding. We introduce DELTA, which pairs a Residual Vector Quantization (RVQ) EEG tokenizer with a masked language diffusion model (LLaDA). RVQ discretizes continuous EEG into multi-layer tokens to reduce noise and individual differences, while LLaDA reconstructs sentences via non-sequential denoising. On ZuCo, DELTA improves semantic alignment by up to 5.37 points over autoregressive baselines, achieving BLEU-1 21.9 and ROUGE-1 F 17.2 under word-level conditions. These results enable reliable text generation from small EEG-text datasets and point toward scalable multimodal EEG-language models.


### [102] [fMRI-LM: Towards a Universal Foundation Model for Language-Aligned fMRI Understanding](https://arxiv.org/abs/2511.21760)
*Yuxiang Wei, Yanteng Zhang, Xi Xiao, Chengxuan Qian, Tianyang Wang, Vince D. Calhoun*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†fMRI-LMï¼Œè¿™æ˜¯ä¸€ä¸ªå°†åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒä¸è¯­è¨€æ¨¡å‹è¿æ¥çš„åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ä¸‰é˜¶æ®µæ¡†æ¶å®ç°äº†å¤§è„‘æ´»åŠ¨ä¸è¯­ä¹‰è®¤çŸ¥çš„ç»Ÿä¸€å»ºæ¨¡ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘çš„ç»Ÿä¸€æ¨ç†æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å°†å…¶èƒ½åŠ›æ‰©å±•åˆ°è„‘æˆåƒé¢†åŸŸä»æœªè¢«å……åˆ†æ¢ç´¢ï¼Œè€Œå¼¥åˆè¿™ä¸€å·®è·å¯¹äºè¿æ¥ç¥ç»æ´»åŠ¨ä¸è¯­ä¹‰è®¤çŸ¥ä»¥åŠå¼€å‘è·¨æ¨¡æ€å¤§è„‘è¡¨å¾è‡³å…³é‡è¦ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨ä¸‰é˜¶æ®µæ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µå­¦ä¹ ç¥ç»åˆ†è¯å™¨ï¼Œå°†fMRIæ˜ å°„åˆ°è¯­è¨€ä¸€è‡´ç©ºé—´ä¸­çš„ç¦»æ•£æ ‡è®°ï¼›ç¬¬äºŒé˜¶æ®µé€‚é…é¢„è®­ç»ƒLLMï¼Œè”åˆå»ºæ¨¡fMRIæ ‡è®°å’Œæ–‡æœ¬ï¼Œå°†å¤§è„‘æ´»åŠ¨è§†ä¸ºå¯é¢„æµ‹å’Œæè¿°çš„åºåˆ—ï¼›ç¬¬ä¸‰é˜¶æ®µè¿›è¡Œå¤šä»»åŠ¡ã€å¤šèŒƒå¼æŒ‡ä»¤å¾®è°ƒï¼Œèµ‹äºˆæ¨¡å‹é«˜çº§è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶é€šè¿‡æ„å»ºå¤§å‹æè¿°æ€§è¯­æ–™åº“è§£å†³è‡ªç„¶fMRI-æ–‡æœ¬å¯¹ç¼ºä¹çš„é—®é¢˜ã€‚

**Result:** fMRI-LMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ï¼Œå¹¶é€šè¿‡å‚æ•°é«˜æ•ˆè°ƒä¼˜ï¼ˆLoRAï¼‰å®ç°äº†é«˜æ•ˆé€‚é…ï¼Œä¸ºfMRIçš„ç»“æ„å’Œè¯­ä¹‰ç†è§£å»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•çš„è¯­è¨€å¯¹é½é€šç”¨æ¨¡å‹è·¯å¾„ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºè¿æ¥å¤§è„‘æ´»åŠ¨ä¸è¯­è¨€ç†è§£æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„åŸºç¡€æ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡å°†fMRIè½¬åŒ–ä¸ºè¯­è¨€ä¸€è‡´çš„è¡¨ç¤ºï¼Œå®ç°äº†è·¨æ¨¡æ€çš„å¤§è„‘è¯­ä¹‰å»ºæ¨¡ï¼Œä¸ºç¥ç»ç§‘å­¦å’Œäººå·¥æ™ºèƒ½çš„äº¤å‰ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œæ”¯æŒå¤šæ ·åŒ–çš„ä¸‹æ¸¸åº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
Recent advances in multimodal large language models (LLMs) have enabled unified reasoning across images, audio, and video, but extending such capability to brain imaging remains largely unexplored. Bridging this gap is essential to link neural activity with semantic cognition and to develop cross-modal brain representations. To this end, we present fMRI-LM, a foundational model that bridges functional MRI (fMRI) and language through a three-stage framework. In Stage 1, we learn a neural tokenizer that maps fMRI into discrete tokens embedded in a language-consistent space. In Stage 2, a pretrained LLM is adapted to jointly model fMRI tokens and text, treating brain activity as a sequence that can be temporally predicted and linguistically described. To overcome the lack of natural fMRI-text pairs, we construct a large descriptive corpus that translates diverse imaging-based features into structured textual descriptors, capturing the low-level organization of fMRI signals. In Stage 3, we perform multi-task, multi-paradigm instruction tuning to endow fMRI-LM with high-level semantic understanding, supporting diverse downstream applications. Across various benchmarks, fMRI-LM achieves strong zero-shot and few-shot performance, and adapts efficiently with parameter-efficient tuning (LoRA), establishing a scalable pathway toward a language-aligned, universal model for structural and semantic understanding of fMRI.


### [103] [A Comparative Study of LLM Prompting and Fine-Tuning for Cross-genre Authorship Attribution on Chinese Lyrics](https://arxiv.org/abs/2511.21930)
*Yuxin Li, Lorraine Xu, Meng Fan Wang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é’ˆå¯¹ä¸­æ–‡æ­Œè¯ä½œè€…å½’å±é¢†åŸŸç¼ºä¹å…¬å¼€æ•°æ®é›†çš„é—®é¢˜ï¼Œåˆ›å»ºäº†é¦–ä¸ªè·¨æµæ´¾å¹³è¡¡æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†é¢†åŸŸç‰¹å®šæ¨¡å‹ï¼Œé€šè¿‡å¯¹æ¯”å¾®è°ƒæ¨¡å‹ä¸é›¶æ ·æœ¬LLMåŸºçº¿çš„æ€§èƒ½ï¼Œæ­ç¤ºäº†æµæ´¾æ•æ„Ÿæ€§å¯¹ä½œè€…å½’å±å‡†ç¡®æ€§çš„å…³é”®å½±å“ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¸­æ–‡æ­Œè¯ä½œè€…å½’å±é¢†åŸŸé¢ä¸´ä¸¤å¤§æ ¸å¿ƒé—®é¢˜ï¼šä¸€æ˜¯ç¼ºä¹å¹²å‡€ã€å…¬å¼€çš„æ•°æ®é›†ï¼ŒäºŒæ˜¯ç°æœ‰æ–¹æ³•åœ¨è·¨æµæ´¾è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ä¸æ˜ç¡®ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œé€šè¿‡åˆ›å»ºé¦–ä¸ªå¹³è¡¡çš„ä¸­æ–‡æ­Œè¯æ•°æ®é›†ï¼Œå¹¶ç³»ç»Ÿè¯„ä¼°é¢†åŸŸç‰¹å®šæ¨¡å‹åœ¨ä¸åŒæµæ´¾ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚

**Method:** ç ”ç©¶æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ–¹é¢ï¼šé¦–å…ˆåˆ›å»ºäº†ä¸€ä¸ªè·¨å¤šä¸ªæµæ´¾çš„å¹³è¡¡ä¸­æ–‡æ­Œè¯æ•°æ®é›†ï¼›å…¶æ¬¡å¼€å‘å¹¶å¾®è°ƒäº†é¢†åŸŸç‰¹å®šæ¨¡å‹ï¼Œå°†å…¶æ€§èƒ½ä¸ä½¿ç”¨DeepSeek LLMçš„é›¶æ ·æœ¬æ¨ç†è¿›è¡Œå¯¹æ¯”ã€‚å®éªŒè®¾è®¡æµ‹è¯•äº†ä¸¤ä¸ªæ ¸å¿ƒå‡è®¾ï¼šå¾®è°ƒæ¨¡å‹æ˜¯å¦ä¼˜äºé›¶æ ·æœ¬åŸºçº¿ï¼Œä»¥åŠæ€§èƒ½æ˜¯å¦å…·æœ‰æµæ´¾ä¾èµ–æ€§ã€‚

**Result:** å®éªŒç»“æœå¼ºçƒˆæ”¯æŒç¬¬äºŒä¸ªå‡è®¾ï¼šç»“æ„åŒ–æµæ´¾ï¼ˆå¦‚æ°‘ä¿—ä¸ä¼ ç»Ÿï¼‰çš„ä½œè€…å½’å±å‡†ç¡®ç‡æ˜¾è‘—é«˜äºæŠ½è±¡æµæ´¾ï¼ˆå¦‚çˆ±æƒ…ä¸æµªæ¼«ï¼‰ã€‚ç¬¬ä¸€ä¸ªå‡è®¾ä»…å¾—åˆ°éƒ¨åˆ†æ”¯æŒï¼šåœ¨çœŸå®ä¸–ç•Œæ•°æ®å’Œå›°éš¾æµæ´¾çš„Test1ä¸­ï¼Œå¾®è°ƒæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨åˆæˆå¢å¼ºçš„Test2ä¸­å¢ç›Šæœ‰é™æˆ–æ¨¡ç³Šã€‚ç ”ç©¶å‘ç°Test2çš„è®¾è®¡å±€é™æ€§ï¼ˆå¦‚æ ‡ç­¾ä¸å¹³è¡¡ã€æµ…å±‚è¯æ±‡å·®å¼‚å’Œç‹­çª„æµæ´¾é‡‡æ ·ï¼‰å¯èƒ½æ©ç›–å¾®è°ƒçš„çœŸå®æ•ˆæœã€‚

**Conclusion:** æœ¬ç ”ç©¶å»ºç«‹äº†é¦–ä¸ªè·¨æµæ´¾ä¸­æ–‡æ­Œè¯ä½œè€…å½’å±åŸºå‡†ï¼Œå¼ºè°ƒäº†æµæ´¾æ•æ„Ÿè¯„ä¼°çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å…¬å¼€æ•°æ®é›†å’Œåˆ†ææ¡†æ¶ã€‚ä¸»è¦å»ºè®®åŒ…æ‹¬ï¼šæ‰©å¤§å’Œå¤šæ ·åŒ–æµ‹è¯•é›†ã€å‡å°‘å¯¹è¯çº§æ•°æ®å¢å¼ºçš„ä¾èµ–ã€å¹³è¡¡è·¨æµæ´¾çš„ä½œè€…ä»£è¡¨æ€§ï¼Œä»¥åŠæ¢ç´¢é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒä½œä¸ºæ”¹è¿›å½’å±æ€§èƒ½çš„é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
We propose a novel study on authorship attribution for Chinese lyrics, a domain where clean, public datasets are sorely lacking. Our contributions are twofold: (1) we create a new, balanced dataset of Chinese lyrics spanning multiple genres, and (2) we develop and fine-tune a domain-specific model, comparing its performance against zero-shot inference using the DeepSeek LLM.
  We test two central hypotheses. First, we hypothesize that a fine-tuned model will outperform a zero-shot LLM baseline. Second, we hypothesize that performance is genre-dependent. Our experiments strongly confirm Hypothesis 2: structured genres (e.g. Folklore & Tradition) yield significantly higher attribution accuracy than more abstract genres (e.g. Love & Romance). Hypothesis 1 receives only partial support: fine-tuning improves robustness and generalization in Test1 (real-world data and difficult genres), but offers limited or ambiguous gains in Test2, a smaller, synthetically-augmented set. We show that the design limitations of Test2 (e.g., label imbalance, shallow lexical differences, and narrow genre sampling) can obscure the true effectiveness of fine-tuning.
  Our work establishes the first benchmark for cross-genre Chinese lyric attribution, highlights the importance of genre-sensitive evaluation, and provides a public dataset and analytical framework for future research. We conclude with recommendations: enlarge and diversify test sets, reduce reliance on token-level data augmentation, balance author representation across genres, and investigate domain-adaptive pretraining as a pathway for improved attribution performance.


### [104] [ResearchArcade: Graph Interface for Academic Tasks](https://arxiv.org/abs/2511.22036)
*Jingjun Xu, Chongshan Lin, Haofei Yu, Tao Feng, Jiaxuan You*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ResearchArcadeï¼Œä¸€ç§åŸºäºå›¾çš„ç»Ÿä¸€æ•°æ®æ¥å£ï¼Œç”¨äºè¿æ¥å¤šç§å­¦æœ¯æ•°æ®æºå¹¶æ”¯æŒå¤šæ ·åŒ–çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œæ—¨åœ¨åŠ é€ŸçŸ¥è¯†å‘ç°è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šæ¨¡æ€å›¾ç»“æ„ç»„ç»‡å­¦æœ¯æ•°æ®ï¼Œå¹¶åœ¨å…­ä¸ªå­¦æœ¯ä»»åŠ¡ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€ç ”ç©¶äººå‘˜è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨æœºå™¨å­¦ä¹ è¾…åŠ©ç ”ç©¶ä»»åŠ¡ï¼Œå­¦æœ¯é¢†åŸŸäº§ç”Ÿäº†å¤šæ ·åŒ–çš„æ•°æ®æºï¼Œä½†ç¼ºä¹ç»Ÿä¸€çš„æ¥å£æ¥æ”¯æŒè·¨æ•°æ®æºçš„æœºå™¨å­¦ä¹ æ¨¡å‹å¼€å‘ã€‚è¿™ç§åˆ†æ•£æ€§é™åˆ¶äº†æ¨¡å‹åœ¨æ•´ä¸ªç ”ç©¶è¿‡ç¨‹ä¸­çš„æ”¯æŒèƒ½åŠ›ï¼Œé˜»ç¢äº†çŸ¥è¯†å‘ç°çš„åŠ é€Ÿã€‚

**Method:** ResearchArcadeé‡‡ç”¨åŸºäºå›¾çš„æ¥å£è®¾è®¡ï¼Œä½¿ç”¨è¿è´¯çš„å¤šè¡¨æ ¼å¼å’Œå›¾ç»“æ„ç»„ç»‡æ¥è‡ªä¸åŒæ¥æºçš„æ•°æ®ï¼ŒåŒ…æ‹¬ArXivå­¦æœ¯è¯­æ–™åº“å’ŒOpenReviewåŒè¡Œè¯„å®¡æ•°æ®ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šæ¨¡æ€ä¿¡æ¯ï¼ˆæ–‡æœ¬ã€å›¾è¡¨ã€è¡¨æ ¼ï¼‰çš„æ•´åˆï¼Œå¹¶ä¿ç•™äº†æ‰‹ç¨¿å’Œç¤¾åŒºå±‚é¢çš„æ—¶é—´æ¼”åŒ–ä¿¡æ¯ï¼ŒåŒæ—¶ç»Ÿä¸€äº†å¤šæ ·åŒ–çš„å­¦æœ¯ä»»åŠ¡å®šä¹‰ä»¥æ”¯æŒä¸åŒè¾“å…¥éœ€æ±‚çš„æ¨¡å‹ã€‚

**Result:** åœ¨å…­ä¸ªå­¦æœ¯ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»“åˆè·¨æºå’Œå¤šæ¨¡æ€ä¿¡æ¯èƒ½å¤Ÿæ”¯æŒæ›´å¹¿æ³›çš„ä»»åŠ¡èŒƒå›´ï¼Œè€Œå›¾ç»“æ„çš„å¼•å…¥æŒç»­æå‡äº†åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœéªŒè¯äº†ResearchArcadeåœ¨æ•´åˆå¼‚æ„å­¦æœ¯æ•°æ®æ–¹é¢çš„æœ‰æ•ˆæ€§åŠå…¶å¯¹ä»»åŠ¡æ€§èƒ½çš„ç§¯æå½±å“ã€‚

**Conclusion:** ResearchArcadeå±•ç¤ºäº†æ„å»ºç»Ÿä¸€å­¦æœ¯æ•°æ®æ¥å£çš„å¯è¡Œæ€§ï¼Œå…¶å›¾ç»“æ„å’Œå¤šæ¨¡æ€æ•´åˆæ–¹æ³•ä¸ºå­¦æœ¯æœºå™¨å­¦ä¹ æä¾›äº†æ›´å…¨é¢çš„æ•°æ®æ”¯æŒã€‚è¯¥æ¡†æ¶å…·æœ‰åŠ é€Ÿç ”ç©¶è¿›ç¨‹çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥å¼€å‘æ›´å¼ºå¤§çš„ç ”ç©¶è¾…åŠ©å·¥å…·å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Academic research generates diverse data sources, and as researchers increasingly use machine learning to assist research tasks, a crucial question arises: Can we build a unified data interface to support the development of machine learning models for various academic tasks? Models trained on such a unified interface can better support human researchers throughout the research process, eventually accelerating knowledge discovery. In this work, we introduce ResearchArcade, a graph-based interface that connects multiple academic data sources, unifies task definitions, and supports a wide range of base models to address key academic challenges. ResearchArcade utilizes a coherent multi-table format with graph structures to organize data from different sources, including academic corpora from ArXiv and peer reviews from OpenReview, while capturing information with multiple modalities, such as text, figures, and tables. ResearchArcade also preserves temporal evolution at both the manuscript and community levels, supporting the study of paper revisions as well as broader research trends over time. Additionally, ResearchArcade unifies diverse academic task definitions and supports various models with distinct input requirements. Our experiments across six academic tasks demonstrate that combining cross-source and multi-modal information enables a broader range of tasks, while incorporating graph structures consistently improves performance over baseline methods. This highlights the effectiveness of ResearchArcade and its potential to advance research progress.


### [105] [Bridging the Modality Gap by Similarity Standardization with Pseudo-Positive Samples](https://arxiv.org/abs/2511.22141)
*Shuhei Yamashita, Daiki Shirafuji, Tatsuhiko Saito*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¼ªæ•°æ®æ„å»ºçš„ç›¸ä¼¼åº¦æ ‡å‡†åŒ–æ–¹æ³•ï¼Œç”¨äºè§£å†³è·¨æ¨¡æ€æ£€ç´¢ä¸­çš„æ¨¡æ€é—´éš™é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è®¡ç®—æ¨¡æ€ç‰¹å®šçš„ç»Ÿè®¡é‡æ¥æ ‡å‡†åŒ–ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œä»è€Œåœ¨å¤šæ¨¡æ€é—®ç­”åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†æ£€ç´¢æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“æ–‡æœ¬å’Œå›¾åƒåŒæ—¶å­˜åœ¨äºæ•°æ®åº“ä¸­æ—¶ï¼Œè·¨æ¨¡æ€æ£€ç´¢é¢ä¸´æ¨¡æ€é—´éš™é—®é¢˜ï¼Œå³ä¸åŒæ¨¡æ€çš„ç›¸ä¼¼åº¦åˆ†æ•°å­˜åœ¨å°ºåº¦å·®å¼‚ï¼Œè¿™é˜»ç¢äº†å‡†ç¡®çš„æ£€ç´¢ã€‚ç°æœ‰ç ”ç©¶å¤§å¤šä¾èµ–äººå·¥æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œè€Œæœ¬æ–‡æ—¨åœ¨é€šè¿‡æ— ç›‘ç£æ–¹æ³•è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚

**Method:** æœ¬æ–‡æå‡ºç›¸ä¼¼åº¦æ ‡å‡†åŒ–æ–¹æ³•ï¼Œé¦–å…ˆè®¡ç®—æ¯ä¸ªæŸ¥è¯¢ä¸å…¶é…å¯¹æ–‡æœ¬æˆ–å›¾åƒæ•°æ®ä¹‹é—´ç›¸ä¼¼åº¦åˆ†æ•°çš„å‡å€¼å’Œæ–¹å·®ã€‚åˆ©ç”¨è¿™äº›æ¨¡æ€ç‰¹å®šçš„ç»Ÿè®¡é‡ï¼Œå°†æ‰€æœ‰ç›¸ä¼¼åº¦åˆ†æ•°æ ‡å‡†åŒ–åˆ°è·¨æ¨¡æ€å¯æ¯”è¾ƒçš„å…±åŒå°ºåº¦ä¸Šã€‚è¿™äº›ç»Ÿè®¡é‡é€šè¿‡ä¼ªæ•°æ®å¯¹è®¡ç®—ï¼Œä¼ªæ•°æ®å¯¹é€šè¿‡æ£€ç´¢ä¸æ¯ä¸ªæŸ¥è¯¢å…·æœ‰æœ€é«˜ä½™å¼¦ç›¸ä¼¼åº¦çš„æ–‡æœ¬å’Œå›¾åƒå€™é€‰æ„å»ºè€Œæˆã€‚

**Result:** åœ¨ä¸ƒä¸ªè§†è§‰è¯­è¨€æ¨¡å‹å’Œä¸¤ä¸ªå¤šæ¨¡æ€é—®ç­”åŸºå‡†ï¼ˆMMQAå’ŒWebQAï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ£€ç´¢æ€§èƒ½ã€‚å½“æŸ¥è¯¢ä¸ç›®æ ‡æ•°æ®å±äºä¸åŒæ¨¡æ€æ—¶ï¼Œåœ¨MMQAä¸Šå®ç°äº†å¹³å‡64%çš„Recall@20æå‡ï¼Œåœ¨WebQAä¸Šå®ç°äº†28%çš„æå‡ã€‚ä¸é€šè¿‡å›¾åƒæè¿°è§£å†³æ¨¡æ€é—´éš™çš„E5-Vç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•æ›´æœ‰æ•ˆåœ°å¼¥åˆäº†æ¨¡æ€é—´éš™ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ä¼ªæ•°æ®æ„å»ºçš„ç›¸ä¼¼åº¦æ ‡å‡†åŒ–æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£å†³è·¨æ¨¡æ€æ£€ç´¢ä¸­çš„æ¨¡æ€é—´éš™é—®é¢˜ï¼Œæ— éœ€ä¾èµ–äººå·¥æ ‡æ³¨æ•°æ®ã€‚è¯¥æ–¹æ³•ä¸ºå¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿæä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ— ç›‘ç£è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ï¼Œå¹¶ä¸ºæœªæ¥è·¨æ¨¡æ€å¯¹é½ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Advances in vision-language models (VLMs) have enabled effective cross-modality retrieval. However, when both text and images exist in the database, similarity scores would differ in scale by modality. This phenomenon, known as the modality gap, hinders accurate retrieval. Most existing studies address this issue with manually labeled data, e.g., by fine-tuning VLMs on them. In this work, we propose a similarity standardization approach with pseudo data construction. We first compute the mean and variance of the similarity scores between each query and its paired data in text or image modality. Using these modality-specific statistics, we standardize all similarity scores to compare on a common scale across modalities. These statistics are calculated from pseudo pairs, which are constructed by retrieving the text and image candidates with the highest cosine similarity to each query. We evaluate our method across seven VLMs using two multi-modal QA benchmarks (MMQA and WebQA), where each question requires retrieving either text or image data. Our experimental results show that our method significantly improves retrieval performance, achieving average Recall@20 gains of 64% on MMQA and 28% on WebQA when the query and the target data belong to different modalities. Compared to E5-V, which addresses the modality gap through image captioning, we confirm that our method more effectively bridges the modality gap.


### [106] [Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information](https://arxiv.org/abs/2511.22176)
*Lukas Struppek, Dominik Hintersdorf, Hannah Struppek, Daniel Neider, Kristian Kersting*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€ä»¥è¾“å…¥ä¸ºä¸­å¿ƒçš„Focused Chain-of-Thoughtæ–¹æ³•ï¼Œé€šè¿‡å°†ä¿¡æ¯æå–ä¸æ¨ç†è¿‡ç¨‹åˆ†ç¦»ï¼Œåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶å°†ç”Ÿæˆä»¤ç‰Œå‡å°‘2-3å€ï¼Œæ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡ç”Ÿæˆè¯¦ç»†çš„æ€ç»´é“¾è½¨è¿¹å®ç°å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™é€šå¸¸å¯¼è‡´ä»¤ç‰Œä½¿ç”¨è¿‡å¤šå’Œæ¨ç†å»¶è¿Ÿè¿‡é«˜ã€‚ä¼ ç»Ÿæ•ˆç‡æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡å‹ä¸­å¿ƒå¹²é¢„ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ æˆ–ç›‘ç£å¾®è°ƒæ¥å‡å°‘å†—ä½™ï¼Œè€Œæœ¬ç ”ç©¶æ¢ç´¢è®­ç»ƒè‡ªç”±çš„è¾“å…¥ä¸­å¿ƒæ–¹æ³•æ¥è§£å†³è¿™ä¸€æ•ˆç‡ç“¶é¢ˆã€‚

**Method:** å—è®¤çŸ¥å¿ƒç†å­¦å¯å‘ï¼Œæœ¬æ–‡æå‡ºFocused Chain-of-Thoughtæ–¹æ³•ï¼Œå°†ä¿¡æ¯æå–ä¸æ¨ç†è¿‡ç¨‹åˆ†ç¦»ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»æŸ¥è¯¢ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç»„ç»‡æˆç®€æ´çš„ç»“æ„åŒ–ä¸Šä¸‹æ–‡ï¼Œç„¶åå¼•å¯¼æ¨¡å‹ä»…åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæ¨ç†ï¼Œé€šè¿‡é˜²æ­¢å…³æ³¨æ— å…³ç»†èŠ‚è‡ªç„¶äº§ç”Ÿæ›´çŸ­çš„æ¨ç†è·¯å¾„ã€‚

**Result:** åœ¨ç®—æœ¯æ–‡å­—é—®é¢˜å®éªŒä¸­ï¼ŒF-CoTå°†ç”Ÿæˆä»¤ç‰Œå‡å°‘2-3å€ï¼ŒåŒæ—¶ä¿æŒä¸æ ‡å‡†é›¶æ ·æœ¬æ€ç»´é“¾ç›¸å½“çš„å‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡æ•ˆç‡çš„åŒæ—¶ä¸ä¼šç‰ºç‰²æ¨ç†æ€§èƒ½ï¼ŒéªŒè¯äº†ç»“æ„åŒ–è¾“å…¥ä½œä¸ºé«˜æ•ˆæ¨ç†æ æ†çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** æœ¬ç ”ç©¶è¯æ˜äº†ç»“æ„åŒ–è¾“å…¥ä½œä¸ºç®€å•è€Œæœ‰æ•ˆçš„æ æ†ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚F-CoTæ–¹æ³•å±•ç¤ºäº†è®­ç»ƒè‡ªç”±ã€è¾“å…¥ä¸­å¿ƒæ–¹æ³•çš„æ½œåŠ›ï¼Œä¸ºå‡å°‘æ¨ç†å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬æä¾›äº†æ–°æ–¹å‘ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§ã€‚

---

#### ğŸ“„ Abstract
Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.


### [107] [Improving LLM-based Ontology Matching with fine-tuning on synthetic data](https://arxiv.org/abs/2511.22612)
*Guilherme Sousa, Rinaldo Lima, Cassia Trojahn*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè‡ªåŠ¨æ•°æ®é›†ç”Ÿæˆä¸å¾®è°ƒçš„ç­–ç•¥ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ‰§è¡Œæœ¬ä½“åŒ¹é…ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡æœç´¢ç©ºé—´ç¼©å‡å’Œåˆæˆæ•°æ®ç”Ÿæˆï¼Œæ˜¾è‘—æå‡äº†LLMåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æœ¬ä½“å¯¹é½æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³LLMåœ¨æœ¬ä½“åŒ¹é…ä»»åŠ¡ä¸­é¢ä¸´çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç¼ºä¹è¶³å¤Ÿçš„å‚è€ƒå¯¹é½æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥åŠå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨LLMç›´æ¥å¤„ç†æœ¬ä½“æ¨¡å—å¹¶ç”Ÿæˆå¯¹åº”å¯¹é½ã€‚ç°æœ‰æ–¹æ³•åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æ€§èƒ½æœ‰é™ï¼Œä¸”ç¼ºä¹ä¸“é—¨é’ˆå¯¹æœ¬ä½“åŒ¹é…ä»»åŠ¡çš„å¾®è°ƒç­–ç•¥ã€‚

**Method:** è¯¥æ–¹æ³•åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæŠ€æœ¯ï¼šé¦–å…ˆé‡‡ç”¨æœç´¢ç©ºé—´ç¼©å‡æŠ€æœ¯ä»æºå’Œç›®æ ‡æœ¬ä½“ä¸­é€‰æ‹©ç›¸å…³å­é›†ï¼Œå¹¶è‡ªåŠ¨æ„å»ºæç¤ºï¼›å…¶æ¬¡å¼•å…¥ä¸€ç§æ–°é¢–çš„LLMåŸºæ–¹æ³•ç”Ÿæˆåˆæˆæ•°æ®é›†ï¼Œåˆ›å»ºæœ¬ä½“å­æ¨¡å—å¯¹åŠå…¶å‚è€ƒå¯¹é½çš„è¯­æ–™åº“ï¼›æœ€åè®¾è®¡ä¸“é—¨çš„å¾®è°ƒç­–ç•¥ï¼Œåˆ©ç”¨åˆæˆæ•°æ®è®­ç»ƒLLMä»¥é€‚åº”æœ¬ä½“åŒ¹é…ä»»åŠ¡ã€‚

**Result:** åœ¨OAEIå¤æ‚èµ›é“çš„Conferenceã€Geolinkã€Enslavedã€Taxonå’ŒHydrographyæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒåŸºäºåˆæˆæ•°æ®å¾®è°ƒçš„LLMç›¸æ¯”æœªå¾®è°ƒçš„åŸºç¡€æ¨¡å‹è¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸæœ¬ä½“åŒ¹é…ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** æœ¬ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ç§ç»“åˆè‡ªåŠ¨æ•°æ®é›†ç”Ÿæˆä¸å¾®è°ƒçš„ç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³äº†LLMåœ¨æœ¬ä½“åŒ¹é…ä»»åŠ¡ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚è¯¥æ–¹æ³•ä¸ºLLMåœ¨çŸ¥è¯†è¡¨ç¤ºå’Œè¯­ä¹‰é›†æˆé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°æ€è·¯ï¼Œå±•ç¤ºäº†åˆæˆæ•°æ®ç”Ÿæˆåœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡é€‚é…ä¸­çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Large Language Models (LLMs) are increasingly being integrated into various components of Ontology Matching pipelines. This paper investigates the capability of LLMs to perform ontology matching directly on ontology modules and generate the corresponding alignments. Furthermore, it is explored how a dedicated fine-tuning strategy can enhance the model's matching performance in a zero-shot setting. The proposed method incorporates a search space reduction technique to select relevant subsets from both source and target ontologies, which are then used to automatically construct prompts. Recognizing the scarcity of reference alignments for training, a novel LLM-based approach is introduced for generating a synthetic dataset. This process creates a corpus of ontology submodule pairs and their corresponding reference alignments, specifically designed to fine-tune an LLM for the ontology matching task. The proposed approach was evaluated on the Conference, Geolink, Enslaved, Taxon, and Hydrography datasets from the OAEI complex track. The results demonstrate that the LLM fine-tuned on the synthetically generated data exhibits superior performance compared to the non-fine-tuned base model. The key contribution is a strategy that combines automatic dataset generation with fine-tuning to effectively adapt LLMs for ontology matching tasks.


### [108] [Visual Puns from Idioms: An Iterative LLM-T2IM-MLLM Framework](https://arxiv.org/abs/2511.22943)
*Kelaiti Xiao, Liang Yang, Dongyu Zhang, Paerhati Tulajiang, Hongfei Lin*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¿­ä»£æ¡†æ¶çš„è§†è§‰åŒå…³è¯­è‡ªåŠ¨ç”Ÿæˆä¸è¯„ä¼°ç³»ç»Ÿï¼Œé€šè¿‡åè°ƒå¤§è¯­è¨€æ¨¡å‹ã€æ–‡ç”Ÿå›¾æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†æˆè¯­è§†è§‰åŒå…³çš„åˆæˆä¸ç†è§£ï¼Œå¹¶æ„å»ºäº†ç›¸åº”çš„åŸºå‡†æ•°æ®é›†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³æˆè¯­è§†è§‰åŒå…³è¯­è‡ªåŠ¨ç”Ÿæˆä¸è¯„ä¼°çš„æŒ‘æˆ˜ï¼Œæ¢ç´¢å¦‚ä½•åè°ƒä¸åŒæ¨¡æ€çš„AIæ¨¡å‹æ¥åˆ›å»ºæ—¢ä½“ç°æˆè¯­å­—é¢æ„ä¹‰åˆä¼ è¾¾å…¶æ¯”å–»æ„ä¹‰çš„å›¾åƒï¼Œå¹¶å»ºç«‹ç›¸åº”çš„åŸºå‡†æ•°æ®é›†ç”¨äºæ¨¡å‹æ€§èƒ½è¯„ä¼°ã€‚

**Method:** ç ”ç©¶æå‡ºäº†ä¸€ä¸ªè¿­ä»£æ¡†æ¶ï¼Œåè°ƒå¤§è¯­è¨€æ¨¡å‹ã€æ–‡ç”Ÿå›¾æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡ŒååŒå·¥ä½œã€‚ç³»ç»Ÿé‡‡ç”¨å››æ­¥è¿­ä»£æµç¨‹ï¼šç”Ÿæˆè¯¦ç»†è§†è§‰æç¤ºã€åˆæˆå›¾åƒã€ä»å›¾åƒæ¨æ–­æˆè¯­ã€ä¼˜åŒ–æç¤ºç›´è‡³è¯†åˆ«æˆåŠŸæˆ–è¾¾åˆ°æ­¥æ•°é™åˆ¶ï¼Œä½¿ç”¨1000ä¸ªæˆè¯­ä½œä¸ºè¾“å…¥æ„å»ºäº†è§†è§‰åŒå…³å›¾åƒæ•°æ®é›†ã€‚

**Result:** å®éªŒæ¶‰åŠ10ä¸ªå¤§è¯­è¨€æ¨¡å‹ã€10ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œ1ä¸ªæ–‡ç”Ÿå›¾æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€‰æ‹©æ˜¯æ€§èƒ½çš„ä¸»è¦é©±åŠ¨å› ç´ ï¼šGPTè·å¾—æœ€é«˜å‡†ç¡®ç‡ï¼ŒGeminiæ¬¡ä¹‹ï¼Œæœ€ä½³å¼€æºæ¨¡å‹Gemmaä¸éƒ¨åˆ†é—­æºæ¨¡å‹ç«äº‰ã€‚åœ¨å¤§è¯­è¨€æ¨¡å‹æ–¹é¢ï¼ŒClaudeåœ¨æç¤ºç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€AIç³»ç»Ÿåœ¨åˆ›æ„å†…å®¹ç”Ÿæˆä»»åŠ¡ä¸­çš„ååŒæ½œåŠ›ï¼Œä¸ºè§†è§‰è¯­è¨€ç†è§£æä¾›äº†æ–°çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼ŒåŒæ—¶æ­ç¤ºäº†ä¸åŒæ¨¡å‹æ¶æ„åœ¨å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æ€§èƒ½å·®å¼‚ï¼Œä¸ºæœªæ¥è·¨æ¨¡æ€ç”Ÿæˆä¸ç†è§£ç³»ç»Ÿçš„è®¾è®¡æä¾›äº†é‡è¦å‚è€ƒã€‚

---

#### ğŸ“„ Abstract
We study idiom-based visual puns--images that align an idiom's literal and figurative meanings--and present an iterative framework that coordinates a large language model (LLM), a text-to-image model (T2IM), and a multimodal LLM (MLLM) for automatic generation and evaluation. Given an idiom, the system iteratively (i) generates detailed visual prompts, (ii) synthesizes an image, (iii) infers the idiom from the image, and (iv) refines the prompt until recognition succeeds or a step limit is reached. Using 1,000 idioms as inputs, we synthesize a corresponding dataset of visual pun images with paired prompts, enabling benchmarking of both generation and understanding. Experiments across 10 LLMs, 10 MLLMs, and one T2IM (Qwen-Image) show that MLLM choice is the primary performance driver: GPT achieves the highest accuracies, Gemini follows, and the best open-source MLLM (Gemma) is competitive with some closed models. On the LLM side, Claude attains the strongest average performance for prompt generation.


### [109] [Mind Reading or Misreading? LLMs on the Big Five Personality Test](https://arxiv.org/abs/2511.23101)
*Francesco Di Cursi, Chiara Boldrini, Marco Conti, Andrea Passarella*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹åŸºäºæ–‡æœ¬è¿›è¡Œè‡ªåŠ¨äººæ ¼é¢„æµ‹çš„èƒ½åŠ›ï¼Œå‘ç°å½“å‰ç°æˆçš„LLMåœ¨äºŒå…ƒäº”å› ç´ æ¨¡å‹äººæ ¼é¢„æµ‹ä»»åŠ¡ä¸­å°šæœªè¾¾åˆ°å¯é æ°´å¹³ï¼Œä¸”æç¤ºè®¾è®¡å’Œè¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©å¯¹ç»“æœè§£é‡Šè‡³å…³é‡è¦ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨äººæ ¼é¢„æµ‹ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§é—®é¢˜ï¼Œæ¢ç´¢LLMåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹åŸºäºæ–‡æœ¬è¿›è¡Œäººæ ¼ç‰¹è´¨é¢„æµ‹çš„èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨ä¸åŒæ¨¡å‹ã€æ•°æ®é›†å’Œæç¤ºç­–ç•¥å¯¹é¢„æµ‹æ€§èƒ½çš„å½±å“ï¼Œä»¥å¡«è¡¥ç°æœ‰ç ”ç©¶åœ¨LLMäººæ ¼é¢„æµ‹ç³»ç»Ÿè¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨äº”ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4å’Œè½»é‡çº§å¼€æºæ›¿ä»£æ–¹æ¡ˆï¼‰ï¼Œåœ¨ä¸‰ä¸ªå¼‚æ„æ•°æ®é›†ï¼ˆEssaysã€MyPersonalityã€Pandoraï¼‰ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ¯”è¾ƒä¸¤ç§æç¤ºç­–ç•¥ï¼šæœ€å°åŒ–æç¤ºä¸åŒ…å«è¯­è¨€å­¦å’Œå¿ƒç†å­¦çº¿ç´¢çš„ä¸°å¯Œæç¤ºï¼Œæ‰€æœ‰å®éªŒå‡åœ¨äºŒå…ƒäº”å› ç´ æ¨¡å‹æ¡†æ¶ä¸‹è¿›è¡Œé›¶æ ·æœ¬é¢„æµ‹ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œä¸°å¯Œæç¤ºè™½èƒ½å‡å°‘æ— æ•ˆè¾“å‡ºå¹¶æ”¹å–„ç±»åˆ«å¹³è¡¡ï¼Œä½†ä¼šå¼•å…¥ç³»ç»Ÿæ€§åå‘ç‰¹è´¨å­˜åœ¨çš„é¢„æµ‹åå·®ï¼›å¼€æ”¾æ€§ç‰¹è´¨å’Œå®œäººæ€§ç‰¹è´¨ç›¸å¯¹è¾ƒæ˜“æ£€æµ‹ï¼Œè€Œå¤–å‘æ€§å’Œç¥ç»è´¨ç‰¹è´¨é¢„æµ‹ä»å…·æŒ‘æˆ˜æ€§ï¼›å¼€æºæ¨¡å‹æœ‰æ—¶æ¥è¿‘GPT-4å’Œå…ˆå‰åŸºå‡†ï¼Œä½†æ— ä»»ä½•é…ç½®èƒ½åœ¨é›¶æ ·æœ¬äºŒå…ƒè®¾ç½®ä¸­äº§ç”ŸæŒç»­å¯é çš„é¢„æµ‹ï¼›èšåˆæŒ‡æ ‡å¦‚å‡†ç¡®ç‡å’Œå®F1åˆ†æ•°æ©ç›–äº†æ˜¾è‘—çš„ä¸å¯¹ç§°æ€§ï¼Œè€Œæ¯ç±»å¬å›ç‡æä¾›æ›´æ¸…æ™°çš„è¯Šæ–­ä»·å€¼ã€‚

**Conclusion:** ç ”ç©¶å‘ç°å½“å‰ç°æˆçš„LLMå°šä¸é€‚åˆè‡ªåŠ¨äººæ ¼é¢„æµ‹ä»»åŠ¡ï¼Œæç¤ºè®¾è®¡ã€ç‰¹è´¨æ¡†æ¶å’Œè¯„ä¼°æŒ‡æ ‡éœ€è¦ç²¾å¿ƒåè°ƒæ‰èƒ½è·å¾—å¯è§£é‡Šçš„ç»“æœï¼›ç ”ç©¶å¼ºè°ƒäº†åœ¨äººæ ¼é¢„æµ‹åº”ç”¨ä¸­è°¨æ…ä½¿ç”¨LLMçš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºéœ€è¦æ›´ç»†è‡´çš„è¯„ä¼°æ–¹æ³•å’Œæ›´ä¸“é—¨åŒ–çš„æ¨¡å‹è®¾è®¡æ¥æå‡é¢„æµ‹å¯é æ€§ã€‚

---

#### ğŸ“„ Abstract
We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. enriched with linguistic and psychological cues). Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Performance varies substantially: Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging. Although open-source models sometimes approach GPT-4 and prior benchmarks, no configuration yields consistently reliable predictions in zero-shot binary settings. Moreover, aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value. These findings show that current out-of-the-box LLMs are not yet suitable for APPT, and that careful coordination of prompt design, trait framing, and evaluation metrics is essential for interpretable results.


### [110] [Tourism Question Answer System in Indian Language using Domain-Adapted Foundation Models](https://arxiv.org/abs/2511.23235)
*Praveen Gatla, Anushka, Nikita Kanwar, Gouri Sahoo, Rajesh Kumar Mundotiya*

#### ğŸ§© TL;DR
æœ¬æ–‡é¦–æ¬¡ä¸ºå°åœ°è¯­æ—…æ¸¸é¢†åŸŸæ„å»ºäº†æŠ½å–å¼é—®ç­”ç³»ç»ŸåŸºå‡†ï¼Œä¸“æ³¨äºç“¦æ‹‰çº³è¥¿æ–‡åŒ–åœºæ™¯ï¼Œé€šè¿‡æ„å»ºæ•°æ®é›†å¹¶åˆ©ç”¨BERTå’ŒRoBERTaç­‰åŸºç¡€æ¨¡å‹ç»“åˆLoRAå¾®è°ƒï¼Œåœ¨ä½èµ„æºç¯å¢ƒä¸‹å®ç°äº†å‚æ•°æ•ˆç‡ä¸æ€§èƒ½çš„å¹³è¡¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å°åœ°è¯­æ—…æ¸¸é¢†åŸŸç¼ºä¹è¯­è¨€ç‰¹å®šé—®ç­”èµ„æºçš„ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç“¦æ‹‰çº³è¥¿è¿™ä¸€å…·æœ‰ä¸°å¯Œæ–‡åŒ–å†…æ¶µï¼ˆå¦‚Bhakti-Bhaavç²¾ç¥ï¼‰çš„æ—…æ¸¸ç›®çš„åœ°ï¼Œç°æœ‰ç³»ç»Ÿæ— æ³•å¤„ç†æ–‡åŒ–ç»†å¾®å·®åˆ«å’Œé¢†åŸŸç‰¹å®šæŸ¥è¯¢ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†åŒ…å«7,715ä¸ªå°åœ°è¯­é—®ç­”å¯¹çš„æ•°æ®é›†ï¼Œå¹¶é€šè¿‡Llamaé›¶æ ·æœ¬æç¤ºç”Ÿæˆäº†27,455ä¸ªå¢å¼ºå¯¹ï¼›æå‡ºåŸºäºBERTå’ŒRoBERTaç­‰åŸºç¡€æ¨¡å‹çš„æ¡†æ¶ï¼Œé‡‡ç”¨ç›‘ç£å¾®è°ƒå’Œä½ç§©é€‚åº”ä¸¤ç§å¾®è°ƒç­–ç•¥ï¼Œè¯„ä¼°äº†åŒ…æ‹¬å°åœ°è¯­BERTåœ¨å†…çš„å¤šç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å˜ä½“ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºLoRAçš„å¾®è°ƒåœ¨å‡å°‘98%å¯è®­ç»ƒå‚æ•°çš„åŒæ—¶å®ç°äº†85.3%çš„F1åˆ†æ•°ï¼Œåœ¨æ•ˆç‡ä¸å‡†ç¡®æ€§é—´å–å¾—å¹³è¡¡ï¼›RoBERTaç»“åˆç›‘ç£å¾®è°ƒåœ¨æ•æ‰æ–‡åŒ–åµŒå…¥æœ¯è¯­ï¼ˆå¦‚Aartiã€Kundï¼‰çš„ä¸Šä¸‹æ–‡ç»†å¾®å·®åˆ«æ–¹é¢ä¼˜äºBERTå˜ä½“ï¼ŒF1ã€BLEUå’ŒROUGE-LæŒ‡æ ‡æ­ç¤ºäº†ç­”æ¡ˆç²¾ç¡®æ€§ä¸è¯­è¨€æµç•…æ€§ä¹‹é—´çš„æƒè¡¡ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå°åœ°è¯­æ—…æ¸¸é—®ç­”ç³»ç»Ÿå»ºç«‹äº†åŸºç¡€åŸºå‡†ï¼Œå¼ºè°ƒäº†LoRAåœ¨ä½èµ„æºç¯å¢ƒä¸­çš„é‡è¦ä½œç”¨ï¼Œå¹¶æŒ‡å‡ºæ—…æ¸¸é¢†åŸŸéœ€è¦æ–‡åŒ–æƒ…å¢ƒåŒ–çš„NLPæ¡†æ¶ï¼›å·¥ä½œå±•ç¤ºäº†å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—éœ€æ±‚ï¼Œä¸ºç±»ä¼¼ä½èµ„æºé¢†åŸŸç‰¹å®šåº”ç”¨æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
This article presents the first comprehensive study on designing a baseline extractive question-answering (QA) system for the Hindi tourism domain, with a specialized focus on the Varanasi-a cultural and spiritual hub renowned for its Bhakti-Bhaav (devotional ethos). Targeting ten tourism-centric subdomains-Ganga Aarti, Cruise, Food Court, Public Toilet, Kund, Museum, General, Ashram, Temple and Travel, the work addresses the absence of language-specific QA resources in Hindi for culturally nuanced applications. In this paper, a dataset comprising 7,715 Hindi QA pairs pertaining to Varanasi tourism was constructed and subsequently augmented with 27,455 pairs generated via Llama zero-shot prompting. We propose a framework leveraging foundation models-BERT and RoBERTa, fine-tuned using Supervised Fine-Tuning (SFT) and Low-Rank Adaptation (LoRA), to optimize parameter efficiency and task performance. Multiple variants of BERT, including pre-trained languages (e.g., Hindi-BERT), are evaluated to assess their suitability for low-resource domain-specific QA. Evaluation metrics - F1, BLEU, and ROUGE-L - highlight trade-offs between answer precision and linguistic fluency. Experiments demonstrate that LoRA-based fine-tuning achieves competitive performance (85.3\% F1) while reducing trainable parameters by 98\% compared to SFT, striking a balance between efficiency and accuracy. Comparative analysis across models reveals that RoBERTa with SFT outperforms BERT variants in capturing contextual nuances, particularly for culturally embedded terms (e.g., Aarti, Kund). This work establishes a foundational baseline for Hindi tourism QA systems, emphasizing the role of LORA in low-resource settings and underscoring the need for culturally contextualized NLP frameworks in the tourism domain.


### [111] [Optimizing Multimodal Language Models through Attention-based Interpretability](https://arxiv.org/abs/2511.23375)
*Alexander Sergeev, Evgeny Kotelnikov*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œé€šè¿‡åˆ†æå›¾åƒå…³é”®å¯¹è±¡çš„æ³¨æ„åŠ›å¾—åˆ†æ¥è¯†åˆ«é‡è¦æ³¨æ„åŠ›å¤´ï¼Œå¹¶åˆ©ç”¨è¯¥ä¿¡æ¯æŒ‡å¯¼å‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œåœ¨ä»…å¾®è°ƒçº¦0.01%å‚æ•°çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡å›¾åƒç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è™½ç„¶åŠŸèƒ½å¼ºå¤§ä½†éš¾ä»¥è§£é‡Šï¼Œä½¿å¾—åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒä¸­éš¾ä»¥ç¡®å®šå“ªäº›ç»„ä»¶æœ€æœ‰æ•ˆè®­ç»ƒä»¥å¹³è¡¡æ•ˆç‡ä¸æ€§èƒ½ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹æ¨¡å‹å†…éƒ¨æ³¨æ„åŠ›æœºåˆ¶ä¸å›¾åƒå…³é”®å¯¹è±¡å…³è”çš„ç³»ç»Ÿåˆ†æã€‚

**Method:** æå‡ºåŸºäºæ³¨æ„åŠ›çš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œé€šè¿‡åˆ†ææ³¨æ„åŠ›å¾—åˆ†ç›¸å¯¹äºå›¾åƒä»¤ç‰Œçš„å…³ç³»æ¥è¯†åˆ«å…³æ³¨å›¾åƒå…³é”®å¯¹è±¡çš„æ³¨æ„åŠ›å¤´ï¼Œè®¡ç®—å¤´éƒ¨å½±å“åˆ†æ•°é‡åŒ–æ³¨æ„åŠ›å¤´å¯¹å…³é”®å¯¹è±¡çš„å…³æ³¨ç¨‹åº¦ï¼Œå¹¶åˆ©ç”¨è¯¥ä¿¡æ¯é€‰æ‹©æœ€ä¼˜æ¨¡å‹ç»„ä»¶è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚

**Result:** åœ¨20-30äº¿å‚æ•°çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸ŠéªŒè¯äº†æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œå®éªŒè¡¨æ˜å¾®è°ƒå…·æœ‰æœ€é«˜HIåˆ†æ•°çš„å±‚ç›¸æ¯”é¢„è®­ç»ƒã€éšæœºé€‰æ‹©æˆ–æœ€ä½HIåˆ†æ•°å±‚èƒ½å¸¦æ¥æœ€æ˜¾è‘—çš„æŒ‡æ ‡å˜åŒ–ï¼Œä»…å¾®è°ƒçº¦0.01%å‚æ•°å³å¯æ˜¾è‘—å½±å“å›¾åƒç†è§£èƒ½åŠ›ã€‚

**Conclusion:** è¯¥æ–¹æ³•ä¸ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æä¾›äº†å¯è§£é‡Šæ€§å·¥å…·ï¼Œè¯æ˜äº†åŸºäºæ³¨æ„åŠ›åˆ†æé€‰æ‹©å¾®è°ƒç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå‚æ•°é«˜æ•ˆå¾®è°ƒæä¾›äº†æ•°æ®é©±åŠ¨çš„ç»„ä»¶é€‰æ‹©ç­–ç•¥ï¼ŒåŒæ—¶åˆ›å»ºäº†åŒ…å«å›¾åƒã€å…³é”®å¯¹è±¡æ©ç å’Œæ–‡æœ¬æè¿°çš„æ–°æ•°æ®é›†æ”¯æŒåç»­ç ”ç©¶ã€‚

---

#### ğŸ“„ Abstract
Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance efficiency and performance. We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens. The core idea is to identify attention heads that focus on image key objects. We utilize this information to select optimal model components for PEFT in multimodal models. Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions. We conducted experiments on MLMs with 2-3 billion parameters to validate the method's effectiveness. By calculating Head Impact (HI) scores we quantify an attention head's focus on key objects, indicating its significance in image understanding. Our fine-tuning experiments demonstrate that adapting layers with the highest HI scores leads to the most significant shifts in metrics compared to pre-trained, randomly selected, or lowest-HI-score layers. This indicates that fine-tuning a small percentage (around 0.01%) of parameters in these crucial layers can substantially influence image understanding capabilities.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [112] [Evaluating Strategies for Synthesizing Clinical Notes for Medical Multimodal AI](https://arxiv.org/abs/2511.21827)
*Niccolo Marini, Zhaohui Liang, Sivaramakrishnan Rajaraman, Zhiyun Xue, Sameer Antani*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æ¢ç´¢äº†åœ¨ç”Ÿç‰©åŒ»å­¦å¤šæ¨¡æ€å­¦ä¹ ä¸­åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆä¸´åºŠæ–‡æœ¬çš„ç­–ç•¥ï¼Œé€šè¿‡ä¼˜åŒ–æç¤ºè®¾è®¡å’ŒåŒ»å­¦å…ƒæ•°æ®æ•´åˆï¼Œå¢å¼ºçš®è‚¤ç—…å˜åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç”Ÿç‰©åŒ»å­¦å¤šæ¨¡æ€å­¦ä¹ é¢ä¸´å¤§è§„æ¨¡å¼‚è´¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨çš®è‚¤ç—…å­¦é¢†åŸŸï¼Œçš®è‚¤ç—…å˜æ•°æ®é›†é€šå¸¸ä»…åŒ…å«å›¾åƒå’Œå°‘é‡å…ƒæ•°æ®ï¼Œé™åˆ¶äº†å¤šæ¨¡æ€æ•°æ®æ•´åˆçš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹è™½èƒ½ç”Ÿæˆå›¾åƒæ–‡æœ¬æè¿°ï¼Œä½†æœªç»åŒ»å­¦é¢†åŸŸä¸“é—¨è®­ç»ƒï¼Œå­˜åœ¨ä¸´åºŠç›¸å…³å¹»è§‰é£é™©ï¼Œéœ€è¦ç ”ç©¶æœ‰æ•ˆçš„åˆæˆä¸´åºŠæ–‡æœ¬ç”Ÿæˆç­–ç•¥ã€‚

**Method:** æœ¬ç ”ç©¶æå‡ºäº†é’ˆå¯¹åˆæˆä¸´åºŠæ–‡æœ¬ç”Ÿæˆçš„ç­–ç•¥ï¼Œé‡ç‚¹å…³æ³¨æç¤ºè®¾è®¡å’ŒåŒ»å­¦å…ƒæ•°æ®æ•´åˆæ–¹æ³•ã€‚é€šè¿‡ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æç¤ºå·¥ç¨‹ï¼Œç»“åˆåŒ»å­¦é¢†åŸŸç‰¹å®šçš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„åˆæˆä¸´åºŠç¬”è®°ã€‚åœ¨å¤šæ¨¡æ€æ¶æ„ä¸­æ•´åˆå›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºï¼Œè¯„ä¼°å…¶åœ¨åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚

**Result:** åœ¨å¤šä¸ªå¼‚è´¨çš®è‚¤ç—…æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåˆæˆä¸´åºŠç¬”è®°æ˜¾è‘—æå‡äº†åˆ†ç±»æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢†åŸŸåç§»æƒ…å†µä¸‹è¡¨ç°æ›´ä¸ºçªå‡ºã€‚æ­¤å¤–ï¼Œåˆæˆæ–‡æœ¬è¿˜è§£é”äº†è·¨æ¨¡æ€æ£€ç´¢èƒ½åŠ›ï¼Œè¿™ä¸€ä¸‹æ¸¸ä»»åŠ¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹¶æœªè¢«æ˜ç¡®ä¼˜åŒ–ã€‚ç ”ç©¶éªŒè¯äº†ä¼˜åŒ–æç¤ºè®¾è®¡å’ŒåŒ»å­¦å…ƒæ•°æ®æ•´åˆå¯¹æå‡å¤šæ¨¡æ€å­¦ä¹ æ•ˆæœçš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** æœ¬ç ”ç©¶è¯æ˜äº†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºå’ŒåŒ»å­¦å…ƒæ•°æ®æ•´åˆï¼Œå¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åˆæˆä¸´åºŠæ–‡æœ¬ï¼Œæœ‰æ•ˆå¢å¼ºç”Ÿç‰©åŒ»å­¦å¤šæ¨¡æ€å­¦ä¹ æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ³•ä¸ä»…æå‡äº†åˆ†ç±»ä»»åŠ¡çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè¿˜è§£é”äº†æ–°çš„è·¨æ¨¡æ€æ£€ç´¢åŠŸèƒ½ï¼Œä¸ºåŒ»ç–—AIåº”ç”¨ä¸­æ•°æ®ç¨€ç¼ºé—®é¢˜çš„è§£å†³æä¾›äº†åˆ›æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Multimodal (MM) learning is emerging as a promising paradigm in biomedical artificial intelligence (AI) applications, integrating complementary modality, which highlight different aspects of patient health. The scarcity of large heterogeneous biomedical MM data has restrained the development of robust models for medical AI applications. In the dermatology domain, for instance, skin lesion datasets typically include only images linked to minimal metadata describing the condition, thereby limiting the benefits of MM data integration for reliable and generalizable predictions. Recent advances in Large Language Models (LLMs) enable the synthesis of textual description of image findings, potentially allowing the combination of image and text representations. However, LLMs are not specifically trained for use in the medical domain, and their naive inclusion has raised concerns about the risk of hallucinations in clinically relevant contexts. This work investigates strategies for generating synthetic textual clinical notes, in terms of prompt design and medical metadata inclusion, and evaluates their impact on MM architectures toward enhancing performance in classification and cross-modal retrieval tasks. Experiments across several heterogeneous dermatology datasets demonstrate that synthetic clinical notes not only enhance classification performance, particularly under domain shift, but also unlock cross-modal retrieval capabilities, a downstream task that is not explicitly optimized during training.


### [113] [Hybrid Stackelberg Game and Diffusion-based Auction for Two-tier Agentic AI Task Offloading in Internet of Agents](https://arxiv.org/abs/2511.22076)
*Yue Zhong, Yongju Tong, Jiawen Kang, Minghui Dai, Hong-Ning Dai, Zhou Su, Dusit Niyato*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘æ™ºèƒ½ä½“ç‰©è”ç½‘çš„ä¸¤å±‚ä¼˜åŒ–æ–¹æ¡ˆï¼Œé€šè¿‡Stackelbergåšå¼ˆå’ŒåŒè·å…°æ‹å–æœºåˆ¶è§£å†³æ— çº¿æ™ºèƒ½ä½“è®¡ç®—å¯†é›†å‹ä»»åŠ¡å¸è½½é—®é¢˜ï¼Œå¹¶é‡‡ç”¨åŸºäºæ‰©æ•£çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œæ±‚è§£ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ™ºèƒ½ä½“ç‰©è”ç½‘ä¸­æ— çº¿æ™ºèƒ½ä½“å…·æœ‰æœ‰é™æ¿è½½èµ„æºï¼Œéœ€è¦å°†è®¡ç®—å¯†é›†å‹AIæœåŠ¡å¸è½½åˆ°é™„è¿‘æœåŠ¡å™¨ï¼Œä½†ç°æœ‰æ–¹æ¡ˆæœªèƒ½æœ‰æ•ˆè§£å†³ç§»åŠ¨æ™ºèƒ½ä½“åŠ¨æ€è¿æ¥é™åˆ¶ä¸å›ºå®šæ™ºèƒ½ä½“èµ„æºè¿‡è½½æƒ…å†µä¸‹çš„ååŒå¸è½½ä¼˜åŒ–é—®é¢˜ã€‚

**Method:** æå‡ºä¸¤å±‚ä¼˜åŒ–æ–¹æ³•ï¼šç¬¬ä¸€å±‚é‡‡ç”¨å¤šé¢†å¯¼è€…å¤šè·Ÿéšè€…Stackelbergåšå¼ˆï¼Œç§»åŠ¨æ™ºèƒ½ä½“å’Œå›ºå®šæ™ºèƒ½ä½“ä½œä¸ºé¢†å¯¼è€…è®¾å®šèµ„æºä»·æ ¼ï¼Œæ— çº¿æ™ºèƒ½ä½“ä½œä¸ºè·Ÿéšè€…ç¡®å®šä»»åŠ¡å¸è½½æ¯”ä¾‹ï¼›ç¬¬äºŒå±‚å¼•å…¥åŒè·å…°æ‹å–æ¨¡å‹ï¼Œè¿‡è½½å›ºå®šæ™ºèƒ½ä½“ä½œä¸ºä¹°æ–¹è¯·æ±‚èµ„æºï¼Œç©ºä¸­æ™ºèƒ½ä½“ä½œä¸ºå–æ–¹æä¾›èµ„æºï¼Œå¹¶å¼€å‘åŸºäºæ‰©æ•£çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•æ±‚è§£æ¨¡å‹ã€‚

**Result:** æ•°å€¼ç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ¡ˆåœ¨ä¿ƒè¿›ä»»åŠ¡å¸è½½æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåè°ƒç§»åŠ¨æ™ºèƒ½ä½“ã€å›ºå®šæ™ºèƒ½ä½“å’Œç©ºä¸­æ™ºèƒ½ä½“ä¹‹é—´çš„èµ„æºåˆ†é…ï¼Œä¼˜åŒ–æ•´ä½“ç³»ç»Ÿæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºæ™ºèƒ½ä½“ç‰©è”ç½‘ä¸­çš„èµ„æºä¼˜åŒ–æä¾›äº†åˆ›æ–°æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚åšå¼ˆå’Œæ‹å–æœºåˆ¶å®ç°äº†å¼‚æ„æ™ºèƒ½ä½“é—´çš„ååŒå¸è½½ï¼Œä¸ºæœªæ¥å¤§è§„æ¨¡äº’è”æ™ºèƒ½ç³»ç»Ÿçš„å®é™…éƒ¨ç½²æä¾›äº†ç†è®ºåŸºç¡€å’Œç®—æ³•æ”¯æŒã€‚

---

#### ğŸ“„ Abstract
The Internet of Agents (IoA) is rapidly gaining prominence as a foundational architecture for interconnected intelligent systems, designed to facilitate seamless discovery, communication, and collaborative reasoning among a vast network of Artificial Intelligence (AI) agents. Powered by Large Language and Vision-Language Models, IoA enables the development of interactive, rational agents capable of complex cooperation, moving far beyond traditional isolated models. IoA involves physical entities, i.e., Wireless Agents (WAs) with limited onboard resources, which need to offload their compute-intensive agentic AI services to nearby servers. Such servers can be Mobile Agents (MAs), e.g., vehicle agents, or Fixed Agents (FAs), e.g., end-side units agents. Given their fixed geographical locations and stable connectivity, FAs can serve as reliable communication gateways and task aggregation points. This stability allows them to effectively coordinate with and offload to an Aerial Agent (AA) tier, which has an advantage not affordable for highly mobile MAs with dynamic connectivity limitations. As such, we propose a two-tier optimization approach. The first tier employs a multi-leader multi-follower Stackelberg game. In the game, MAs and FAs act as the leaders who set resource prices. WAs are the followers to determine task offloading ratios. However, when FAs become overloaded, they can further offload tasks to available aerial resources. Therefore, the second tier introduces a Double Dutch Auction model where overloaded FAs act as the buyers to request resources, and AAs serve as the sellers for resource provision. We then develop a diffusion-based Deep Reinforcement Learning algorithm to solve the model. Numerical results demonstrate the superiority of our proposed scheme in facilitating task offloading.


### [114] [WearVQA: A Visual Question Answering Benchmark for Wearables in Egocentric Authentic Real-world scenarios](https://arxiv.org/abs/2511.22154)
*Eun Chang, Zhuangqun Huang, Yiwei Liao, Sagar Ravi Bhavsar, Amogh Param, Tammy Stark, Adel Ahmadyan, Xiao Yang, Jiaqi Wang, Ahsan Abdullah, Giang Nguyen, Akil Iyer, David Hall, Elissa Li, Shane Moon, Nicolas Scheffer, Kirmani Ahmed, Babak Damavandi, Rakesh Wanga, Anuj Kumar, Rohit Patel, Xin Luna Dong*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†WearVQAï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨è¯„ä¼°å¯ç©¿æˆ´è®¾å¤‡ä¸Šå¤šæ¨¡æ€AIåŠ©æ‰‹è§†è§‰é—®ç­”èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥åŸºå‡†åŒ…å«2,520ä¸ªç²¾å¿ƒç­–åˆ’çš„å›¾åƒ-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„ï¼Œæ¶µç›–äº†å¯ç©¿æˆ´è®¾å¤‡ç‰¹æœ‰çš„è§†è§‰æŒ‘æˆ˜å’Œç°å®ä½¿ç”¨åœºæ™¯ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰é—®ç­”åŸºå‡†ä¸»è¦å…³æ³¨é«˜è´¨é‡ã€ç¬¬ä¸‰äººç§°å›¾åƒï¼Œæ— æ³•åæ˜ å¯ç©¿æˆ´è®¾å¤‡ä¸Šä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒäº¤äº’çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è§†è§‰è¾“å…¥å¯èƒ½è¢«é®æŒ¡ã€å…‰ç…§ä¸ä½³ã€æœªç¼©æ”¾æˆ–æ¨¡ç³Šç­‰é—®é¢˜ï¼Œä¸”ç¼ºä¹é’ˆå¯¹å¯ç©¿æˆ´è®¾å¤‡å®é™…ä½¿ç”¨åœºæ™¯çš„è¯„ä¼°æ¡†æ¶ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«2,520ä¸ªå›¾åƒ-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„çš„WearVQAåŸºå‡†ï¼Œæ¶µç›–7ä¸ªä¸åŒå›¾åƒé¢†åŸŸï¼ˆåŒ…æ‹¬æ–‡æœ¬ä¸­å¿ƒå’Œä¸€èˆ¬åœºæ™¯ï¼‰ã€10ç§è®¤çŸ¥ä»»åŠ¡ç±»å‹ï¼ˆä»åŸºæœ¬è¯†åˆ«åˆ°å„ç§æ¨ç†å½¢å¼ï¼‰ä»¥åŠ6ç§å¸¸è§çš„å¯ç©¿æˆ´è®¾å¤‡å›¾åƒè´¨é‡é—®é¢˜ï¼Œå¹¶é…å¤‡äº†å‡†ç¡®ç‡è¾¾96%çš„LLM-as-a-judgeè¯„ä¼°æ¡†æ¶ã€‚

**Result:** å¼€æºå’Œä¸“æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨WearVQAä¸Šçš„é—®ç­”å‡†ç¡®ç‡ä»…ä¸º24-52%ï¼Œåœ¨ä½è´¨é‡å›¾åƒå’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œè¿™è¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¯ç©¿æˆ´è®¾å¤‡ç‰¹æœ‰çš„è§†è§‰æŒ‘æˆ˜æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚

**Conclusion:** WearVQAä½œä¸ºä¸€ä¸ªå…¨é¢ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€AIç³»ç»Ÿåœ¨å¯ç©¿æˆ´è®¾å¤‡å®é™…åº”ç”¨åœºæ™¯ä¸­çš„å±€é™æ€§ï¼Œä¸ºå¼€å‘æ›´é²æ£’ã€å®ç”¨çš„å¯ç©¿æˆ´AIç³»ç»Ÿæä¾›äº†é‡è¦çš„è¯„ä¼°å·¥å…·å’ŒæŠ€æœ¯æŒ‡å¯¼æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
We introduce WearVQA, the first benchmark specifically designed to evaluate the Visual Question Answering (VQA) capabilities of multi-model AI assistant on wearable devices like smart glasses. Unlike prior benchmarks that focus on high-quality, third-person imagery, WearVQA reflects the unique challenges of ego-centric interaction-where visual inputs may be occluded, poorly lit, unzoomed, or blurry, and questions are grounded in realistic wearable use cases. The benchmark comprises 2,520 carefully curated image-question-answer triplets, spanning 7 diverse image domains including both text-centric and general scenes, 10 cognitive task types ranging from basic recognition to various forms of reasoning, and 6 common wearables-specific image quality issues. All questions are designed to be answerable using only the visual input and common senses. WearVQA is paired with a rigorous LLM-as-a-judge evaluation framework with 96% labeling accuracy. Open-source and proprietary multi-model LLMs achieved a QA accuracy as low as 24-52% on WearVQA, with substantial drops on lower-quality images and reasoning-heavy tasks. These observations position WearVQA as a comprehensive and challenging benchmark for guiding technical advancement towards robust, real-world multi-model wearables AI systems.


### [115] [Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation](https://arxiv.org/abs/2511.22235)
*Zehao Deng, Tianjie Ju, Zheng Wu, Zhuosheng Zhang, Gongshen Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Coordinator-Executor-State Tracker (CES)å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒé«˜å±‚è°ƒåº¦æ¨¡å‹æ¥è§£å†³GUIæ™ºèƒ½ä½“åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­çš„è´£ä»»è€¦åˆä¸çŠ¶æ€ç®¡ç†é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†å„ç§åº•å±‚æ‰§è¡Œå™¨çš„é•¿æ—¶ç¨‹ä»»åŠ¡å¤„ç†èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹é©±åŠ¨çš„GUIæ™ºèƒ½ä½“åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡å¤„ç†ä¸Šé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå•æ™ºèƒ½ä½“æ¨¡å‹éš¾ä»¥å¹³è¡¡é«˜å±‚è§„åˆ’èƒ½åŠ›ä¸åº•å±‚æ‰§è¡Œèƒ½åŠ›ï¼Œå­˜åœ¨è´£ä»»è€¦åˆå’Œèƒ½åŠ›å†²çªé—®é¢˜ï¼›æ™ºèƒ½ä½“ç¼ºä¹ä»»åŠ¡çŠ¶æ€æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¯¼è‡´é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­è¿›åº¦ä¸¢å¤±ã€‚

**Method:** æå‡ºåˆ†é˜¶æ®µæ‰§è¡Œ-åé¦ˆå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¸“æ³¨äºè®­ç»ƒé«˜å±‚è°ƒåº¦æ¨¡å‹è€Œéç»Ÿä¸€ç­–ç•¥æ¨¡å‹ã€‚æ„å»ºäº†Coordinator-Executor-State Tracker (CES)å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ŒåŒ…å«è´Ÿè´£æˆ˜ç•¥è§„åˆ’å’Œä»»åŠ¡åˆ†è§£çš„Coordinatorï¼Œä»¥åŠè´Ÿè´£ä¸Šä¸‹æ–‡å‹ç¼©å’Œä¿¡æ¯ç®¡ç†çš„State Trackerï¼Œè¯¥æ¡†æ¶å¯ä¸ä»»ä½•åº•å±‚Executoræ¨¡å‹é›†æˆã€‚

**Result:** åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCESæ¡†æ¶æ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿçš„è§„åˆ’èƒ½åŠ›å’ŒçŠ¶æ€ç®¡ç†èƒ½åŠ›ã€‚åˆ†æè¯å®è®­ç»ƒå¾—åˆ°çš„é«˜å±‚è°ƒåº¦æ¨¡å—å…·æœ‰é€šç”¨æ€§ï¼Œå¯ä½œä¸ºå³æ’å³ç”¨æ¨¡å—æ˜¾è‘—æå‡å„ç§Executorçš„é•¿æ—¶ç¨‹ä»»åŠ¡å¤„ç†èƒ½åŠ›ã€‚

**Conclusion:** CESæ¡†æ¶é€šè¿‡è§£è€¦é«˜å±‚è°ƒåº¦ä¸åº•å±‚æ‰§è¡Œï¼Œæœ‰æ•ˆè§£å†³äº†GUIæ™ºèƒ½ä½“åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­çš„è´£ä»»è€¦åˆé—®é¢˜ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ¶æ„åœ¨å¤æ‚ä»»åŠ¡å¤„ç†ä¸­çš„ä¼˜åŠ¿ï¼Œä¸ºé€šç”¨GUIæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡æä¾›äº†æ–°çš„èŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
The rapid development of large vision-language model (VLM) has greatly promoted the research of GUI agent. However, GUI agents still face significant challenges in handling long-horizon tasks. First, single-agent models struggle to balance high-level capabilities and low-level execution capability, facing prevalent issues of responsibility coupling and capability conflicts. Second, agents lack awareness of the task state, leading to progress loss in long-horizon tasks. To address these challenges, we propose a staged execution-feedback reinforcement learning algorithm. Unlike training a unified policy model, we focus on training high-level scheduling models. Specifically, we propose and train two agents: a Coordinator, responsible for the strategic planning and task decomposition; and a State Tracker, responsible for context compression and information management to maintain the task's state and coherence. Based on this, we built the Coordinator-Executor-State Tracker (CES) multi-agent framework, which can be integrated with any low-level Executor model, assisting the Executor in solving long-horizon tasks through task scheduling and state management. Experiments on long-horizon task benchmarks demonstrate that CES significantly enhances the system's planning and state management capabilities. Furthermore, analysis confirms that our trained high-level scheduling module is a generalizable, plug-and-play module that significantly enhances the long-horizon capabilities of various Executors. Code can be available at https://github.com/hehehahi4/CES.


### [116] [Structured Extraction from Business Process Diagrams Using Vision-Language Models](https://arxiv.org/abs/2511.22448)
*Pritam Deka, Barry Devereux*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä»BPMNæµç¨‹å›¾åƒä¸­ç›´æ¥æå–ç»“æ„åŒ–JSONè¡¨ç¤ºçš„ç®¡é“ï¼Œæ— éœ€æºæ¨¡å‹æ–‡ä»¶æˆ–æ–‡æœ¬æ ‡æ³¨ï¼Œå¹¶é€šè¿‡OCRè¿›è¡Œæ–‡æœ¬å¢å¼ºä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** BPMNä½œä¸ºå¹¿æ³›é‡‡ç”¨çš„ä¸šåŠ¡æµç¨‹å»ºæ¨¡æ ‡å‡†ï¼Œå…¶å›¾è¡¨é€šå¸¸ä»¥è§†è§‰å›¾åƒå½¢å¼äº¤æ¢ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–XMLè¡¨ç¤ºè¿›è¡Œè®¡ç®—åˆ†æï¼Œå½“åŸå§‹æºæ–‡ä»¶ä¸å¯ç”¨æ—¶ç¼ºä¹æœ‰æ•ˆçš„å›¾åƒè§£ææ–¹æ³•ã€‚

**Method:** è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«çš„å¤„ç†ç®¡é“ï¼Œåˆ©ç”¨VLMsç›´æ¥ä»BPMNå›¾åƒä¸­æå–ç»“æ„åŒ–JSONè¡¨ç¤ºï¼Œå¹¶é€šè¿‡OCRè¿›è¡Œæ–‡æœ¬å†…å®¹å¢å¼ºï¼ŒåŒæ—¶è®¾è®¡äº†åŸºäºæºXMLæ–‡ä»¶çš„çœŸå®æ•°æ®è¯„ä¼°æ¡†æ¶ã€‚

**Result:** å®éªŒå¯¹å¤šä¸ªVLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°OCRæ–‡æœ¬å¢å¼ºèƒ½æ˜¾è‘—æå‡å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶é€šè¿‡å¹¿æ³›çš„ç»Ÿè®¡åˆ†ææ­ç¤ºäº†OCRå¢å¼ºæ–¹æ³•å’Œæç¤ºæ¶ˆèç ”ç©¶å¯¹æ¨¡å‹æ€§èƒ½çš„å…·ä½“å½±å“æ¨¡å¼ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç›´æ¥ä»BPMNå›¾åƒä¸­æå–ç»“æ„åŒ–è¡¨ç¤ºçš„å¯è¡Œæ€§ï¼Œä¸ºæºæ–‡ä»¶ä¸å¯ç”¨åœºæ™¯ä¸‹çš„ä¸šåŠ¡æµç¨‹åˆ†ææä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶é˜æ˜äº†æ–‡æœ¬å¢å¼ºæŠ€æœ¯åœ¨è§†è§‰è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­çš„é‡è¦ä½œç”¨ã€‚

---

#### ğŸ“„ Abstract
Business Process Model and Notation (BPMN) is a widely adopted standard for representing complex business workflows. While BPMN diagrams are often exchanged as visual images, existing methods primarily rely on XML representations for computational analysis. In this work, we present a pipeline that leverages Vision-Language Models (VLMs) to extract structured JSON representations of BPMN diagrams directly from images, without requiring source model files or textual annotations. We also incorporate optical character recognition (OCR) for textual enrichment and evaluate the generated element lists against ground truth data derived from the source XML files. Our approach enables robust component extraction in scenarios where original source files are unavailable. We benchmark multiple VLMs and observe performance improvements in several models when OCR is used for text enrichment. In addition, we conducted extensive statistical analyses of OCR-based enrichment methods and prompt ablation studies, providing a clearer understanding of their impact on model performance.


### [117] [Agentic AI Framework for Individuals with Disabilities and Neurodivergence: A Multi-Agent System for Healthy Eating, Daily Routines, and Inclusive Well-Being](https://arxiv.org/abs/2511.22737)
*Salman Jan, Toqeer Ali Syed, Gohar Ali, Ali Akarma, Mohammad Riyaz Belgaum, Ahmad Ali*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘æ®‹éšœäººå£«å’Œç¥ç»å¤šæ ·æ€§äººç¾¤çš„æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œé€šè¿‡å¤šå±‚æ¶æ„å’Œå››ä¸ªä¸“ç”¨æ™ºèƒ½ä½“çš„ååŒå·¥ä½œï¼Œæä¾›ä¸ªæ€§åŒ–ã€è‡ªé€‚åº”ä¸”é€æ˜çš„å¥åº·ç”Ÿæ´»æ”¯æŒç³»ç»Ÿã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿè¾…åŠ©ç³»ç»Ÿåœ¨åŒ…å®¹æ€§ã€ä¸ªæ€§åŒ–å’Œå¯è®¿é—®æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥æ»¡è¶³æ®‹éšœäººå£«å’Œç¥ç»å¤šæ ·æ€§äººç¾¤çš„å¤æ‚éœ€æ±‚ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿæ”¯æŒä»–ä»¬å®ç°æ›´å¥åº·ç”Ÿæ´»å’Œè§„å¾‹æ—¥å¸¸çš„æ™ºèƒ½ä½“AIç³»ç»Ÿã€‚

**Method:** é‡‡ç”¨ä¸‰å±‚æ¶æ„è®¾è®¡ï¼šåº”ç”¨ä¸æ¥å£å±‚ã€æ™ºèƒ½ä½“å±‚å’Œæ•°æ®æºå±‚ï¼Œæ ¸å¿ƒæ˜¯æ··åˆæ¨ç†å¼•æ“åŒæ­¥å››ä¸ªä¸“ç”¨æ™ºèƒ½ä½“ï¼ˆè†³é£Ÿè§„åˆ’ã€æé†’ã€é£Ÿç‰©æŒ‡å¯¼å’Œç›‘æµ‹ï¼‰ï¼Œé€šè¿‡é»‘æ¿/äº‹ä»¶æ€»çº¿å®ç°è‡ªä¸»äº¤äº’å’Œå®æ—¶åé¦ˆï¼Œå¹¶æ•´åˆç”µå­å¥åº·è®°å½•ã€è¥å…»æ•°æ®åº“ã€å¯ç©¿æˆ´è®¾å¤‡å’Œæ™ºèƒ½å¨æˆ¿ç‰©è”ç½‘ç­‰å¤šæ¨¡æ€æ•°æ®æºã€‚

**Result:** è¯¥æ¡†æ¶å®ç°äº†è‡ªé€‚åº”ã€é€æ˜ä¸”åŒ…å®¹çš„æ”¯æŒç³»ç»Ÿï¼Œé€šè¿‡å¯è§£é‡ŠAIæ¨¡å—æä¾›å†³ç­–è§£é‡Šï¼Œæ”¿ç­–æ§åˆ¶å±‚ç¡®ä¿æ•°æ®å®‰å…¨å’Œåˆè§„æ€§ï¼Œä¸´åºŠåŒ»ç”Ÿä»ªè¡¨æ¿æ”¯æŒååŒç›‘ç£ï¼Œå¤šåª’ä½“ç”¨æˆ·ç•Œé¢å®ç°å®æ—¶äº¤äº’åé¦ˆã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ™ºèƒ½ä½“æ¨ç†ã€å¤šæ¨¡æ€ç•Œé¢å’Œä»¥äººä¸ºä¸­å¿ƒè®¾è®¡çš„äº¤å‰èåˆï¼Œè¶…è¶Šäº†ä¼ ç»Ÿè¾…åŠ©ç³»ç»Ÿï¼Œä¸ºä¿ƒè¿›æ®‹éšœäººå£«å’Œç¥ç»å¤šæ ·æ€§äººç¾¤çš„è‡ªä¸»æ€§ã€å¥åº·å’Œæ•°å­—å…¬å¹³æä¾›äº†åˆ›æ–°æ¡†æ¶ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šåŒ…å®¹æ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
The paper presents a detailed Agentic Artificial Intelligence (AI) model that would enable people with disabilities and neurodivergence to lead healthier lives and have more regular days. The system will use a multi-layer structure; it will include an Application and Interface Layer, an Agents Layer, and a Data Source Layer to provide adaptive, transparent, and inclusive support. Fundamentally, a hybrid reasoning engine will synchronize four special-purpose agents, which include: a personalized-nutrition-based, called a Meal Planner Agent; an adaptive-scheduling-based, called a Reminder Agent; interactive assistance during grocery shopping and cooking, called a Food Guidance Agent; and a continuous-intake-and-physiological-tracking, called a Monitoring Agent. All the agents interact through a central communicative system called the Blackboard/Event Bus, which allows autonomous interaction and real-time feedback loops with multimedia user interfaces. Privacy-sensitive data sources, including electronic health records (EHRs), nutritional databases, wearable sensors, and smart kitchen Internet of Things, are also included in the framework and placed into a policy-controlled layer, which ensures data safety and compliance with consent. Collaborative care and clinician dashboards allow common supervision, and discussable artificial intelligence (XAI) modules give brief explanations of why a decision was made, making users responsible and reliant. The proposed agentic AI framework is an extension beyond traditional assistive systems since it incorporates inclusiveness, personalization, and accessibility at all levels. It displays the intersection of multi-agent reasoning, multi-modal interfaces, and human-centered design that will enable the development of autonomy, health, and digital equity among people with disabilities and neurodivergence.


### [118] [TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM](https://arxiv.org/abs/2511.22998)
*Peng Kuang, Xiangxiang Wang, Wentao Liu, Jian Dong, Kaidi Xu, Haohan Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†TIM-PRMï¼ˆå·¥å…·é›†æˆå¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼‰ï¼Œä¸€ç§å°†éªŒè¯ä»è¢«åŠ¨åˆ†ç±»ä»»åŠ¡è½¬å˜ä¸ºä¸»åŠ¨å·¥å…·å¢å¼ºè°ƒæŸ¥çš„ä»£ç†æ¡†æ¶ï¼Œé€šè¿‡ç‹¬ç«‹æé—®æœºåˆ¶æ¶ˆé™¤ç¡®è®¤åå·®ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€æ•°å­¦æ¨ç†çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»æ˜“å—è§†è§‰å¹»è§‰å’Œé€»è¾‘ä¸ä¸€è‡´æ€§çš„å½±å“ï¼Œè€ŒåŸºäºç»“æœçš„ç›‘ç£æ–¹æ³•æ— æ³•æœ‰æ•ˆç¼“è§£è¿™äº›é—®é¢˜ã€‚ç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹é€šå¸¸ä½œä¸ºæ ‡é‡è¯„åˆ†å™¨æˆ–ç”Ÿæˆå¼æ‰¹è¯„è€…ï¼Œå­˜åœ¨è°„åªšå€¾å‘ï¼Œç›²ç›®éªŒè¯æœ‰ç¼ºé™·çš„å‡è®¾è€Œéå°†å…¶åŸºäºè§†è§‰ç°å®ï¼Œå› æ­¤éœ€è¦æ–°çš„éªŒè¯æ¡†æ¶æ¥æ¶ˆé™¤ç¡®è®¤åå·®ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†TIM-PRMæ¡†æ¶ï¼Œå°†éªŒè¯ä»è¢«åŠ¨åˆ†ç±»ä»»åŠ¡è½¬å˜ä¸ºä¸»åŠ¨å·¥å…·å¢å¼ºè°ƒæŸ¥ã€‚è¯¥æ–¹æ³•è®­ç»ƒæ¨¡å‹æ˜¾å¼è§„åˆ’éªŒè¯ç­–ç•¥ï¼Œå¹¶é‡‡ç”¨ç‹¬ç«‹æé—®æœºåˆ¶é€šè¿‡å¤–éƒ¨å·¥å…·æŸ¥è¯¢è¯æ®ï¼Œæœ‰æ•ˆè§£è€¦éªŒè¯ä¸æ¨ç†ä¸Šä¸‹æ–‡ä»¥æ¶ˆé™¤ç¡®è®¤åå·®ã€‚é€šè¿‡ç­–åˆ’é«˜è´¨é‡çš„å·¥å…·é›†æˆéªŒè¯è½¨è¿¹æ•°æ®é›†æ¥å®ä¾‹åŒ–è¯¥æ–¹æ³•ï¼Œæ„å»ºäº†8Bå‚æ•°æ¨¡å‹ã€‚

**Result:** åœ¨VisualProcessBenchä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œ8Bå‚æ•°çš„TIM-PRMæ¨¡å‹è¶…è¶Šäº†ç°æœ‰çš„å¼€æºå¤šæ¨¡æ€PRMæ¨¡å‹ï¼Œæ˜¾è‘—ä¼˜äºQwen2.5-72Bå’ŒInternVL-78Bç­‰æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚è¯¥æ–¹æ³•ä¸ä»…æä¾›äº†å“è¶Šçš„æ€§èƒ½ï¼Œè¿˜æä¾›äº†éªŒè¯è¿‡ç¨‹çš„å¯è§£é‡Šæ€§æ´å¯Ÿï¼Œå±•ç¤ºäº†å°è§„æ¨¡æ¨¡å‹é€šè¿‡å·¥å…·å¢å¼ºéªŒè¯ç­–ç•¥å¯ä»¥è¶…è¶Šå¤§è§„æ¨¡æ¨¡å‹çš„æ½œåŠ›ã€‚

**Conclusion:** TIM-PRMæ¡†æ¶é€šè¿‡å°†éªŒè¯é‡æ–°å®šä¹‰ä¸ºä¸»åŠ¨è°ƒæŸ¥è¿‡ç¨‹ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€æ¨ç†ä¸­çš„ç¡®è®¤åå·®é—®é¢˜ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†å·¥å…·å¢å¼ºéªŒè¯ç­–ç•¥çš„é‡è¦æ€§ï¼Œä»¥åŠå°è§„æ¨¡æ¨¡å‹é€šè¿‡é€‚å½“çš„æ¶æ„è®¾è®¡å¯ä»¥è¶…è¶Šå¤§è§„æ¨¡æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶ä¸ºæ„å»ºæ›´å¯é ã€å¯è§£é‡Šçš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿæä¾›äº†æ–°æ–¹å‘ï¼Œå¼ºè°ƒäº†è¿‡ç¨‹éªŒè¯ä¸­ä¸»åŠ¨è¯æ®æ”¶é›†çš„ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Language Models (MLLMs) have achieved impressive performances in mathematical reasoning, yet they remain vulnerable to visual hallucinations and logical inconsistencies that standard outcome-based supervision fails to mitigate. While Process Reward Models (PRMs) promise step-by-step verification, current approaches typically operate as scalar scorers or generative critics that suffer from sycophancy, blindly validating the flawed hypotheses rather than grounding them in visual reality. To bridge this gap, we introduce TIM-PRM (Tool-Integrated Multimodal PRM), a novel agentic framework that transforms verification from a passive classification task into an active, tool-augmented investigation. TIM-PRM is trained to explicitly plan verification strategies and utilizes a mechanism of Independent Question Asking to query evidence via external tools, effectively decoupling verification from the reasoning context to eliminate confirmation bias. We instantiate this method by curating a high-quality dataset of tool-integrated verification trajectories. Extensive experiments on VisualProcessBench demonstrate that our 8B parameter model surpasses existing open-source multimodal PRMs, significantly outperforming much larger models like Qwen2.5-72B and InternVL-78B, while offering interpretable insights into the verification process.


### [119] [MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents](https://arxiv.org/abs/2511.23055)
*Ruoxuan Zhang, Qiyun Zheng, Zhiyu Zhou, Ziqi Liao, Siyu Wu, Jian-Yu Jiang-Lin, Bin Wen, Hongxia Xie, Jianlong Fu, Wen-Huang Cheng*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMindPoweræ¡†æ¶ï¼Œä¸€ä¸ªé›†æˆäº†æ„ŸçŸ¥ã€å¿ƒæ™ºæ¨ç†ã€å†³ç­–ä¸æ‰§è¡Œçš„æœºå™¨äººä¸­å¿ƒåŒ–æ¶æ„ï¼Œé€šè¿‡å¼•å…¥Mind-Rewardä¼˜åŒ–ç›®æ ‡ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€å…·èº«æ™ºèƒ½ä½“åœ¨åŸºäºå¿ƒæ™ºç†è®ºæ¨ç†çš„å†³ç­–ä¸è¡ŒåŠ¨ç”Ÿæˆèƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€å…·èº«æ™ºèƒ½ä½“ç¼ºä¹åŸºäºå¿ƒæ™ºç†è®ºçš„å†³ç­–èƒ½åŠ›ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•ä»…å…³æ³¨äººç±»å¿ƒæ™ºçŠ¶æ€è€Œå¿½ç•¥æ™ºèƒ½ä½“è‡ªèº«è§†è§’ï¼Œå¯¼è‡´å†³ç­–ä¸è¡ŒåŠ¨ç”Ÿæˆä¸è¿è´¯ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€å±€é™æ€§ã€‚

**Method:** æå‡ºMindPoweræœºå™¨äººä¸­å¿ƒåŒ–æ¡†æ¶ï¼ŒåŒ…å«æ„ŸçŸ¥ã€å¿ƒæ™ºæ¨ç†ã€å†³ç­–ä¸æ‰§è¡Œå››ä¸ªæ¨¡å—ï¼Œé¦–å…ˆæ„ŸçŸ¥ç¯å¢ƒä¸äººç±»çŠ¶æ€ï¼Œç„¶åè¿›è¡Œå¿ƒæ™ºç†è®ºæ¨ç†ä»¥å»ºæ¨¡è‡ªæˆ‘ä¸ä»–äººå¿ƒæ™ºçŠ¶æ€ï¼Œæœ€ååŸºäºæ¨ç†ç»“æœç”Ÿæˆå†³ç­–ä¸è¡ŒåŠ¨ï¼Œå¹¶å¼•å…¥Mind-Rewardä¼˜åŒ–ç›®æ ‡æ¥ä¿ƒè¿›è§†è§‰è¯­è¨€æ¨¡å‹äº§ç”Ÿä¸€è‡´çš„å¿ƒæ™ºæ¨ç†ä¸è¡Œä¸ºã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒMindPoweråœ¨å†³ç­–åˆ¶å®šæ–¹é¢è¶…è¶ŠGPT-4oè¾¾12.77%ï¼Œåœ¨è¡ŒåŠ¨ç”Ÿæˆæ–¹é¢è¶…è¶ŠGPT-4oè¾¾12.49%ï¼Œæ˜¾è‘—æå‡äº†åŸºäºå¿ƒæ™ºç†è®ºæ¨ç†çš„å…·èº«æ™ºèƒ½ä½“æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é›†æˆè‡ªæˆ‘ä¸ä»–äººå¿ƒæ™ºçŠ¶æ€å»ºæ¨¡å¯¹äºå…·èº«æ™ºèƒ½ä½“å†³ç­–è¿è´¯æ€§çš„é‡è¦æ€§ï¼ŒMind-Rewardä¼˜åŒ–ç›®æ ‡ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸€è‡´æ€§æ¨ç†ä¸è¡Œä¸ºç”Ÿæˆæä¾›äº†æœ‰æ•ˆæœºåˆ¶ï¼Œä¸ºæœªæ¥å…·èº«äººå·¥æ™ºèƒ½çš„å¿ƒæ™ºç†è®ºèƒ½åŠ›å‘å±•æä¾›äº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Theory of Mind (ToM) refers to the ability to infer others' mental states, such as beliefs, desires, and intentions. Current vision-language embodied agents lack ToM-based decision-making, and existing benchmarks focus solely on human mental states while ignoring the agent's own perspective, hindering coherent decision and action generation. To address this, we propose MindPower, a Robot-Centric framework integrating Perception, Mental Reasoning, Decision Making and Action. Given multimodal inputs, MindPower first perceives the environment and human states, then performs ToM Reasoning to model both self and others, and finally generates decisions and actions guided by inferred mental states. Furthermore, we introduce Mind-Reward, a novel optimization objective that encourages VLMs to produce consistent ToM Reasoning and behavior. Our model outperforms GPT-4o by 12.77% in decision making and 12.49% in action generation.


### [120] [AgriCoT: A Chain-of-Thought Benchmark for Evaluating Reasoning in Vision-Language Models for Agriculture](https://arxiv.org/abs/2511.23253)
*Yibin Wen, Qingmei Li, Zi Ye, Jiarui Zhang, Jing Wu, Zurong Mai, Shuohong Lou, Yuhang Chen, Henglian Huang, Xiaoya Fan, Yang Zhang, Lingyuan Zhao, Haohuan Fu, Huang Jianxi, Juepeng Zheng*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†AgriCoTï¼Œä¸€ä¸ªåŒ…å«æ€ç»´é“¾æ¨ç†çš„è§†è§‰é—®ç­”æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å†œä¸šé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤æ‚å†œä¸šåœºæ™¯ä¸­æ¨ç†èƒ½åŠ›çš„æ˜¾è‘—ä¸è¶³ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡ç°æœ‰è§†è§‰é—®ç­”æ•°æ®é›†å·²ç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•å……åˆ†è¯„ä¼°å¤æ‚å†œä¸šåœºæ™¯ä¸­æ‰€éœ€çš„å…³é”®æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨ç²¾å‡†å†œä¸šã€ä½œç‰©ç›‘æµ‹ã€ç—…è™«å®³æ£€æµ‹ç­‰å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†AgriCoTæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«4,535ä¸ªç²¾å¿ƒç­–åˆ’çš„æ ·æœ¬ï¼Œä¸“é—¨æ•´åˆäº†æ€ç»´é“¾æ¨ç†æœºåˆ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„é€»è¾‘æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›ï¼Œå¹¶å¯¹26ä¸ªä»£è¡¨æ€§æ¨¡å‹ï¼ˆåŒ…æ‹¬ä¸“æœ‰å’Œå¼€æºæ¨¡å‹ï¼‰è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚

**Result:** è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æŸäº›ä¸“æœ‰æ¨¡å‹åœ¨å›ç­”é—®é¢˜æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸”æ˜æ˜¾çš„å·®è·ï¼Œè¿™çªæ˜¾äº†æ€ç»´é“¾æ¨ç†å¯¹äºæ›´ç²¾ç¡®è¯„ä¼°çš„é‡è¦æ€§ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å†œä¸šæ¨ç†ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­æ•´åˆæ€ç»´é“¾æ¨ç†çš„å¿…è¦æ€§ï¼Œä¸ºå†œä¸šäººå·¥æ™ºèƒ½é¢†åŸŸæä¾›äº†æ›´ç²¾ç¡®çš„è¯„ä¼°åŸºå‡†ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥æ¨¡å‹å¼€å‘éœ€è¦é‡ç‚¹å…³æ³¨é€»è¾‘æ¨ç†å’Œå¤æ‚é—®é¢˜è§£å†³èƒ½åŠ›çš„æå‡æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Recent advancements in Vision-Language Models (VLMs) have significantly transformed various industries. In agriculture, these dual-modal capabilities offer promising applications such as precision farming, crop monitoring, pest detection, and environmental sustainability. While several Visual Question Answering (VQA) datasets and benchmarks have been developed to evaluate VLM performance, they often fail to adequately assess the critical reasoning and problem-solving skills required in complex agricultural contexts. To address this gap, we introduce AgriCoT, a VQA dataset that incorporates Chain-of-Thought (CoT) reasoning, specifically designed to evaluate the reasoning capabilities of VLMs. With 4,535 carefully curated samples, AgriCoT offers a comprehensive and robust evaluation of reasoning abilities for VLMs, particularly in zero-shot scenarios, by focusing on their capacity to engage in logical reasoning and effective problem-solving. Our evaluations, conducted with 26 representative VLMs, including both proprietary and open-source models, reveal that while some proprietary models excel at answering questions, there is a notable and significant gap in their reasoning capabilities. This underscores the importance of incorporating CoT for more precise and effective assessments. Our dataset are available at https://huggingface.co/datasets/wenyb/AgriCoT.


### [121] [Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning](https://arxiv.org/abs/2511.23262)
*Yang Li, Zhiyuan He, Yuxuan Huang, Zhuhanling Xiao, Chao Yu, Meng Fang, Kun Shao, Jun Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†å…ƒè®¤çŸ¥æµ‹è¯•æ—¶æ¨ç†ï¼ˆMCTRï¼‰æ¡†æ¶ï¼Œé€šè¿‡èµ‹äºˆè§†è§‰è¯­è¨€æ¨¡å‹å…ƒè®¤çŸ¥è‡ªæˆ‘æ›´æ–°èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶å­¦ä¹ ã€é€‚åº”å’Œæ”¹è¿›ï¼Œä»è€Œå¼¥åˆæ¨¡å‹ä¸äººç±»åœ¨é€‚åº”æ–°ä»»åŠ¡æ–¹é¢çš„å·®è·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹è™½ç„¶å±•ç°å‡ºå¼ºå¤§çš„æ„ŸçŸ¥æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æµ‹è¯•æ—¶é‡åˆ°æ–°ä»»åŠ¡æ—¶å¾€å¾€éš¾ä»¥é«˜æ•ˆé€‚åº”ï¼Œè€Œäººç±»åˆ™èƒ½å¤Ÿåˆ©ç”¨å…·æœ‰è®°å¿†çš„å…ƒè®¤çŸ¥æ¨¡å‹ï¼Œé€šè¿‡å…ƒè®¤çŸ¥æ§åˆ¶æŒç»­ä¼˜åŒ–ç­–ç•¥æ¥åº”å¯¹æ–°æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¼¥åˆè¿™ä¸€å·®è·ã€‚

**Method:** MCTRæ¡†æ¶åŒ…å«å…ƒçº§å’Œå¯¹è±¡çº§VLMæ¨ç†æ¨¡å—ï¼Œæ¯ä¸ªæ¨¡å—é…å¤‡ä¸“ç”¨è®°å¿†ç³»ç»Ÿè¿›è¡Œåˆ†å±‚è‡ªé€‚åº”æ¨ç†ï¼šå…ƒæ¨ç†æ¨¡å—é€šè¿‡ä»æµ‹è¯•æ—¶è§‚å¯Ÿä¸­å‘ç°å¹¶å­˜å‚¨ä»»åŠ¡ç›¸å…³è§„åˆ™ã€ç¯å¢ƒæ¨¡å¼å’ŒåŠ¨ä½œ-ç»“æœå…³ç³»ä½œä¸ºè‡ªç„¶è¯­è¨€æè¿°æ¥å¢é‡æ„å»ºç»“æ„åŒ–è®°å¿†ï¼›åŠ¨ä½œæ¨ç†æ¨¡å—é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ„ŸçŸ¥å’Œç­–ç•¥æ¨ç†åŠ¨æ€æ£€ç´¢å¹¶æ•´åˆè®°å¿†çŸ¥è¯†æ¥ç¡®å®šæœ€ä¼˜åŠ¨ä½œï¼Œå¹¶é€šè¿‡æå‡ºçš„å…ƒè®¤çŸ¥æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ æŒç»­æ›´æ–°ç­–ç•¥ã€‚

**Result:** åœ¨45ä¸ªAtariæ¸¸æˆï¼ˆ33ä¸ªå·²è§ï¼Œ12ä¸ªæœªè§ï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒMCTRå±•ç°å‡ºå¼ºå¤§çš„æµ‹è¯•æ—¶é€‚åº”èƒ½åŠ›ï¼Œåœ¨æœªè§æ¸¸æˆä¸­ç›¸æ¯”åŸºçº¿è·å¾—äº†9/12çš„top-1ç»“æœï¼›é€šè¿‡æ¶ˆèå®éªŒã€å­¦ä¹ åŠ¨æ€å’Œæ¡ˆä¾‹ç ”ç©¶åˆ†ææ­ç¤ºäº†ä¸¤ä¸ªç»„ä»¶çš„äº’è¡¥è´¡çŒ®ï¼Œå¹¶æ˜¾ç¤ºå…ƒæ¨ç†é€æ¸æ¼”å˜ä¸ºç±»ä¼¼äººç±»çš„é€‚åº”ç­–ç•¥ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹é…å¤‡å…ƒè®¤çŸ¥èƒ½åŠ›èƒ½å¤Ÿæ˜¾è‘—æå‡å…¶åœ¨æµ‹è¯•æ—¶çš„é€‚åº”æ€§èƒ½ï¼Œå…ƒè®¤çŸ¥æµ‹è¯•æ—¶æ¨ç†æ¡†æ¶é€šè¿‡åˆ†å±‚è®°å¿†ç³»ç»Ÿå’Œè‡ªé€‚åº”å­¦ä¹ æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿåƒäººç±»ä¸€æ ·æŒç»­æ”¹è¿›ç­–ç•¥ï¼Œä¸ºå¼€å‘æ›´å…·é€‚åº”æ€§å’Œé€šç”¨æ€§çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Recent Vision-Language Models (VLMs) exhibit strong perceptual reasoning abilities, yet they often struggle to adapt efficiently when encountering novel tasks at test time. In contrast, humans leverage the metacognitive model with memory, enabling continuous strategy refinement through metacognitive control when faced with new challenges. To bridge this gap, we propose metacognitive test-time reasoning (MCTR), a framework that equips models with the ability to learn, adapt, and improve during test time through metacognitive self-updating. Inspired by the dual structure of human metacognition, MCTR comprises meta-level and object-level VLM reasoning modules, each equipped with dedicated memory systems for hierarchical adaptive reasoning. Specifically, MCTR consists of (1) a meta-reasoning module which incrementally builds a structured memory by discovering and storing task-relevant rules, environmental patterns, and action-outcome relationships from test-time observations as natural language descriptions; and (2) an action-reasoning module that determines optimal actions through context-aware perception and strategic reasoning by dynamically retrieving and integrating knowledge from memory. The action-reasoning module continuously updates its policy through proposed metacognitive test-time reinforcement learning, adapting as knowledge memory evolves. We evaluate MCTR on 45 Atari games (33 seen, 12 unseen). MCTR demonstrates robust test-time adaptation, achieving 9/12 top-1 results on unseen games compared with baselines. Analyses through ablations, learning dynamics, and case studies reveal the complementary contributions of both components and show meta-reasoning evolving toward human-like adaptation strategies.


### [122] [OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning](https://arxiv.org/abs/2511.23269)
*Timothy Ossowski, Sheng Zhang, Qianchu Liu, Guanghui Qin, Reuben Tan, Tristan Naumann, Junjie Hu, Hoifung Poon*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºåŒ»å­¦å¤§è¯­è¨€æ¨¡å‹çš„æ•°æ®é…æ–¹ç­–ç•¥ï¼Œé€šè¿‡åˆ©ç”¨ç»“æ„åŒ–æ¨ç†è½¨è¿¹è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œåœ¨è¶…è¿‡800ä¸‡æ ·æœ¬å’Œ68äº¿å“åº”ä»¤ç‰Œçš„æ•°æ®é›†ä¸Šå®ç°äº†å¼€æºæ¨¡å‹åœ¨å¤šæ ·åŒ–åŒ»å­¦åŸºå‡†ä»»åŠ¡ä¸Šçš„æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é«˜è´¨é‡ä¸”ç²¾å¿ƒç­–åˆ’çš„æ•°æ®æ˜¯è®­ç»ƒåŒ»å­¦å¤§è¯­è¨€æ¨¡å‹çš„åŸºçŸ³ï¼Œç›´æ¥å½±å“æ¨¡å‹å¯¹æœªè§ä¸´åºŠä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢è®­ç»ƒå’Œæ•°æ®ç­–åˆ’ç­–ç•¥ï¼Œä»¥å¼€å‘åŒ»å­¦é¢†åŸŸä¸­é²æ£’çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œç‰¹åˆ«å…³æ³¨å¦‚ä½•é€šè¿‡æ•°æ®é…æ–¹æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶ä¸»è¦é‡‡ç”¨ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œæ¢ç´¢åˆ©ç”¨ç»“æ„åŒ–æ¨ç†è½¨è¿¹çš„æ•°æ®é…æ–¹ç­–ç•¥ã€‚è¯¥æ–¹æ³•æ¶‰åŠç²¾å¿ƒç­–åˆ’åŒ…å«ä¸åŒé•¿åº¦ç»“æ„åŒ–æ¨ç†è½¨è¿¹çš„é«˜è´¨é‡ã€å¤šæ ·åŒ–è®­ç»ƒæ•°æ®é›†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸‹æ¸¸ä»»åŠ¡è‡ªæˆ‘æ ¡å‡†å…¶æ¨ç†è½¨è¿¹é•¿åº¦ï¼Œè€Œæ— éœ€æ˜¾å¼ç›‘ç£ã€‚

**Result:** ä½¿ç”¨æå‡ºçš„æ•°æ®é…æ–¹ï¼Œç ”ç©¶å°†å®éªŒæ‰©å±•åˆ°åŒ…å«è¶…è¿‡800ä¸‡ä¸ªç¤ºä¾‹å’Œ68äº¿å“åº”ä»¤ç‰Œçš„æ•°æ®é›†ï¼Œåœ¨å¤šæ ·åŒ–çš„åˆ†å¸ƒå¤–åŒ»å­¦åŸºå‡†ä»»åŠ¡ä¸Šå®ç°äº†å¼€æºæ¨¡å‹ä¸­çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œç­–åˆ’å…·æœ‰ä¸åŒç»“æ„åŒ–æ¨ç†è½¨è¿¹é•¿åº¦çš„è®­ç»ƒæ•°æ®é›†èƒ½ä½¿å¾®è°ƒæ¨¡å‹æ ¹æ®ä¸‹æ¸¸ä»»åŠ¡è‡ªæˆ‘æ ¡å‡†æ¨ç†è½¨è¿¹é•¿åº¦ã€‚

**Conclusion:** è¯¥ç ”ç©¶æä¾›äº†å…³äºåŒ»å­¦å¤§è¯­è¨€æ¨¡å‹æ•°æ®ç­–åˆ’çš„å…³é”®è§è§£ï¼Œå±•ç¤ºäº†ç»“æ„åŒ–æ¨ç†è½¨è¿¹åœ¨æå‡æ¨¡å‹é²æ£’æ€§æ–¹é¢çš„é‡è¦æ€§ã€‚ç ”ç©¶ç»“æœä¸ºå¼€å‘é²æ£’çš„åŒ»å­¦è§†è§‰è¯­è¨€æ¨ç†ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ï¼Œå¹¶æŒ‡å‡ºäº†é€šè¿‡æ•°æ®é…æ–¹å®ç°æ¨¡å‹è‡ªæˆ‘æ ¡å‡†æ¨ç†èƒ½åŠ›çš„æœªæ¥æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.


### [123] [Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering](https://arxiv.org/abs/2511.23304)
*Zijian Fu, Changsheng Lv, Mengshi Qi, Huadong Ma*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSHRIKEæ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥å¤šæ¨¡æ€åœºæ™¯å›¾æ˜¾å¼å»ºæ¨¡è§†å¬åœºæ™¯ä¸­çš„å¯¹è±¡åŠå…¶å…³ç³»ï¼Œå¹¶è®¾è®¡åŸºäºKolmogorov-Arnoldç½‘ç»œçš„ä¸“å®¶æ··åˆæœºåˆ¶ï¼Œåœ¨MUSIC-AVQAåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è§†å¬é—®ç­”æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†å¬é—®ç­”æ–¹æ³•æœªèƒ½æœ‰æ•ˆæ•æ‰è§†é¢‘ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œä¸”å¯¹å¤šæ¨¡æ€ç‰¹å¾çš„ç»†ç²’åº¦å»ºæ¨¡ä¸è¶³ï¼Œå¯¼è‡´éš¾ä»¥ä»å¤æ‚çš„è§†å¬å†…å®¹ä¸­è¯†åˆ«é—®é¢˜ç›¸å…³çº¿ç´¢ï¼Œé™åˆ¶äº†äººç±»æ¨ç†èƒ½åŠ›çš„æ¨¡ä»¿æ•ˆæœã€‚

**Method:** æå‡ºä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€åœºæ™¯å›¾ï¼Œæ˜¾å¼å»ºæ¨¡è§†å¬åœºæ™¯ä¸­çš„å¯¹è±¡åŠå…¶å…³ç³»ï¼Œå½¢æˆè§†è§‰åŸºç¡€çš„åœºæ™¯ç»“æ„åŒ–è¡¨ç¤ºï¼›è®¾è®¡åŸºäºKolmogorov-Arnoldç½‘ç»œçš„ä¸“å®¶æ··åˆæœºåˆ¶ï¼Œå¢å¼ºæ—¶é—´æ•´åˆé˜¶æ®µçš„è¡¨è¾¾èƒ½åŠ›ï¼Œå®ç°é—®é¢˜æ„ŸçŸ¥çš„èåˆè§†å¬è¡¨ç¤ºä¸­è·¨æ¨¡æ€äº¤äº’çš„ç»†ç²’åº¦å»ºæ¨¡ã€‚

**Result:** åœ¨MUSIC-AVQAå’ŒMUSIC-AVQA v2åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSHRIKEæ¨¡å‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€åœºæ™¯å›¾å’ŒKAN-based MoEæœºåˆ¶åœ¨æå‡è§†å¬é—®ç­”ä»»åŠ¡ä¸­æ—¶é—´æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡ç»“æ„åŒ–åœºæ™¯è¡¨ç¤ºå’Œç»†ç²’åº¦è·¨æ¨¡æ€äº¤äº’å»ºæ¨¡ï¼Œæ˜¾è‘—æå‡äº†è§†å¬é—®ç­”ç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€ç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„æŠ€æœ¯æ¡†æ¶ï¼ŒåŒæ—¶å…¬å¼€ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

---

#### ğŸ“„ Abstract
In this paper, we propose a novel Multi-Modal Scene Graph with Kolmogorov-Arnold Expert Network for Audio-Visual Question Answering (SHRIKE). The task aims to mimic human reasoning by extracting and fusing information from audio-visual scenes, with the main challenge being the identification of question-relevant cues from the complex audio-visual content. Existing methods fail to capture the structural information within video, and suffer from insufficient fine-grained modeling of multi-modal features. To address these issues, we are the first to introduce a new multi-modal scene graph that explicitly models the objects and their relationship as a visually grounded, structured representation of the audio-visual scene. Furthermore, we design a Kolmogorov-Arnold Network~(KAN)-based Mixture of Experts (MoE) to enhance the expressive power of the temporal integration stage. This enables more fine-grained modeling of cross-modal interactions within the question-aware fused audio-visual representation, leading to capture richer and more nuanced patterns and then improve temporal reasoning performance. We evaluate the model on the established MUSIC-AVQA and MUSIC-AVQA v2 benchmarks, where it achieves state-of-the-art performance. Code and model checkpoints will be publicly released.
