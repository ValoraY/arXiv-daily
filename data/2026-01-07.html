<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2026-01-07.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 31]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 7]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 3]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-expert-guided-explainable-few-shot-learning-with-active-sample-selection-for-medical-image-analysis">[1] <a href="https://arxiv.org/abs/2601.02409">Expert-Guided Explainable Few-Shot Learning with Active Sample Selection for Medical Image Analysis</a></h3>
<p><em>Longwei Wang, Ifrat Ikhtear Uddin, KC Santosh</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒæ¡†æ¶è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ä¸“å®¶å¼•å¯¼çš„å¯è§£é‡Šæ€§å°‘æ ·æœ¬å­¦ä¹ ï¼ˆEGxFSLï¼‰å’Œå¯è§£é‡Šæ€§å¼•å¯¼çš„ä¸»åŠ¨å­¦ä¹ ï¼ˆxGALï¼‰ï¼Œå…±åŒè§£å†³åŒ»å­¦å›¾åƒåˆ†æä¸­æ•°æ®ç¨€ç¼ºå’Œæ¨¡å‹å¯è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šå®ç°äº†æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŒ»å­¦å›¾åƒåˆ†æé¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ ‡è®°æ•°æ®ç¨€ç¼ºå’Œæ¨¡å‹å¯è§£é‡Šæ€§ä¸è¶³ï¼Œè¿™ä¸¤è€…éƒ½é˜»ç¢äº†ä¸´åºŠAIçš„éƒ¨ç½²ã€‚å°‘æ ·æœ¬å­¦ä¹ è™½ç„¶èƒ½è§£å†³æ•°æ®é™åˆ¶é—®é¢˜ï¼Œä½†ç¼ºä¹é¢„æµ‹é€æ˜åº¦ï¼›ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ä¼˜åŒ–æ•°æ®è·å–ï¼Œä½†å¿½è§†äº†è·å–æ ·æœ¬çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºåŒæ¡†æ¶è§£å†³æ–¹æ¡ˆï¼šä¸“å®¶å¼•å¯¼çš„å¯è§£é‡Šæ€§å°‘æ ·æœ¬å­¦ä¹ ï¼ˆEGxFSLï¼‰å’Œå¯è§£é‡Šæ€§å¼•å¯¼çš„ä¸»åŠ¨å­¦ä¹ ï¼ˆxGALï¼‰ã€‚EGxFSLé€šè¿‡åŸºäºGrad-CAMçš„DiceæŸå¤±å°†æ”¾å°„ç§‘åŒ»ç”Ÿå®šä¹‰çš„æ„Ÿå…´è¶£åŒºåŸŸä½œä¸ºç©ºé—´ç›‘ç£ï¼Œä¸åŸå‹åˆ†ç±»è”åˆä¼˜åŒ–ä»¥å®ç°å¯è§£é‡Šçš„å°‘æ ·æœ¬å­¦ä¹ ã€‚xGALå¼•å…¥è¿­ä»£æ ·æœ¬è·å–ç­–ç•¥ï¼Œä¼˜å…ˆè€ƒè™‘é¢„æµ‹ä¸ç¡®å®šæ€§å’Œæ³¨æ„åŠ›é”™ä½ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¯è§£é‡Šæ€§æŒ‡å¯¼è®­ç»ƒå’Œæ ·æœ¬é€‰æ‹©çš„é—­ç¯æ¡†æ¶ã€‚</p>
<p><strong>Result:</strong> åœ¨BraTSï¼ˆMRIï¼‰ã€VinDr-CXRï¼ˆèƒ¸éƒ¨Xå…‰ï¼‰å’ŒSIIM-COVID-19ï¼ˆèƒ¸éƒ¨Xå…‰ï¼‰æ•°æ®é›†ä¸Šï¼Œåˆ†åˆ«å®ç°äº†92%ã€76%å’Œ62%çš„å‡†ç¡®ç‡ï¼Œåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºéå¼•å¯¼åŸºçº¿ã€‚åœ¨ä¸¥é‡æ•°æ®é™åˆ¶ä¸‹ï¼ŒxGALä»…ç”¨680ä¸ªæ ·æœ¬å°±å®ç°äº†76%çš„å‡†ç¡®ç‡ï¼Œè€Œéšæœºé‡‡æ ·ä»…ä¸º57%ã€‚Grad-CAMå¯è§†åŒ–æ˜¾ç¤ºå¼•å¯¼æ¨¡å‹èšç„¦äºè¯Šæ–­ç›¸å…³åŒºåŸŸï¼Œåœ¨ä¹³è…ºè¶…å£°ä¸Šçš„æ³›åŒ–éªŒè¯è¯å®äº†è·¨æ¨¡æ€é€‚ç”¨æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å°†å¯è§£é‡Šæ€§æœºåˆ¶æ•´åˆåˆ°å°‘æ ·æœ¬å­¦ä¹ å’Œä¸»åŠ¨å­¦ä¹ æ¡†æ¶ä¸­çš„æœ‰æ•ˆæ€§ï¼Œåˆ›å»ºäº†ä¸€ä¸ªååŒç³»ç»Ÿï¼Œå…¶ä¸­å¯è§£é‡Šæ€§ä¸ä»…å¢å¼ºäº†æ¨¡å‹é€æ˜åº¦ï¼Œè¿˜æŒ‡å¯¼äº†è®­ç»ƒè¿‡ç¨‹å’Œæ ·æœ¬é€‰æ‹©ç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŒ»å­¦å½±åƒæ¨¡æ€ä¸Šå±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºä¸´åºŠAIéƒ¨ç½²æä¾›äº†æ›´å¯é å’Œé€æ˜çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Medical image analysis faces two critical challenges: scarcity of labeled data and lack of model interpretability, both hindering clinical AI deployment. Few-shot learning (FSL) addresses data limitations but lacks transparency in predictions. Active learning (AL) methods optimize data acquisition but overlook interpretability of acquired samples. We propose a dual-framework solution: Expert-Guided Explainable Few-Shot Learning (EGxFSL) and Explainability-Guided AL (xGAL). EGxFSL integrates radiologist-defined regions-of-interest as spatial supervision via Grad-CAM-based Dice loss, jointly optimized with prototypical classification for interpretable few-shot learning. xGAL introduces iterative sample acquisition prioritizing both predictive uncertainty and attention misalignment, creating a closed-loop framework where explainability guides training and sample selection synergistically. On the BraTS (MRI), VinDr-CXR (chest X-ray), and SIIM-COVID-19 (chest X-ray) datasets, we achieve accuracies of 92\%, 76\%, and 62\%, respectively, consistently outperforming non-guided baselines across all datasets. Under severe data constraints, xGAL achieves 76\% accuracy with only 680 samples versus 57\% for random sampling. Grad-CAM visualizations demonstrate guided models focus on diagnostically relevant regions, with generalization validated on breast ultrasound confirming cross-modality applicability.</p>
<h3 id="2-miar-modality-interaction-and-alignment-representation-fuison-for-multimodal-emotion">[2] <a href="https://arxiv.org/abs/2601.02414">MIAR: Modality Interaction and Alignment Representation Fuison for Multimodal Emotion</a></h3>
<p><em>Jichao Zhu, Jun Yu</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ¨¡æ€äº¤äº’ä¸å¯¹é½è¡¨ç¤ºï¼ˆMIARï¼‰çš„æ–°å‹å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•ï¼Œé€šè¿‡ç‰¹å¾äº¤äº’å’Œå¯¹æ¯”å­¦ä¹ ç­–ç•¥è§£å†³æ¨¡æ€é—´åˆ†å¸ƒå·®å¼‚å’Œè´¡çŒ®åº¦ä¸å¹³è¡¡é—®é¢˜ï¼Œåœ¨CMU-MOSIå’ŒCMU-MOSEIåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡æ€èåˆï¼Œä½†æœªèƒ½å……åˆ†è§£å†³æ¨¡æ€é—´æ˜¾è‘—çš„åˆ†å¸ƒå·®å¼‚é—®é¢˜ï¼Œä¹Ÿæœªè€ƒè™‘ä¸åŒæ¨¡æ€å¯¹ä»»åŠ¡è´¡çŒ®åº¦çš„å·®å¼‚ï¼ŒåŒæ—¶ç¼ºä¹å¯¹å¤šæ ·åŒ–æ–‡æœ¬æ¨¡å‹ç‰¹å¾çš„é²æ£’æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œé™åˆ¶äº†åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„MIARç½‘ç»œé€šè¿‡ç‰¹å¾äº¤äº’æœºåˆ¶æ•´åˆä¸åŒæ¨¡æ€çš„ä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œç”Ÿæˆä»£è¡¨å…¨å±€è¡¨ç¤ºçš„ç‰¹å¾ä»¤ç‰Œï¼Œè¿™äº›ä»¤ç‰Œæ•æ‰æ¯ä¸ªæ¨¡æ€ä»å…¶ä»–æ¨¡æ€æå–ä¿¡æ¯çš„æ–¹å¼ï¼ŒåŒæ—¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ å’Œå½’ä¸€åŒ–ç­–ç•¥å¯¹é½ä¸åŒæ¨¡æ€çš„è¡¨ç¤ºç©ºé—´ã€‚</p>
<p><strong>Result:</strong> åœ¨CMU-MOSIå’ŒCMU-MOSEIä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMIARæ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨å¤„ç†æ¨¡æ€åˆ†å¸ƒå·®å¼‚å’Œæå‡æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­è€ƒè™‘æ¨¡æ€é—´åˆ†å¸ƒå·®å¼‚å’Œè´¡çŒ®åº¦å·®å¼‚çš„é‡è¦æ€§ï¼Œæå‡ºçš„äº¤äº’å¯¹é½æ¡†æ¶ä¸ºå¤„ç†æ¨¡æ€å¼‚è´¨æ€§æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Multimodal Emotion Recognition (MER) aims to perceive human emotions through three modes: language, vision, and audio. Previous methods primarily focused on modal fusion without adequately addressing significant distributional differences among modalities or considering their varying contributions to the task. They also lacked robust generalization capabilities across diverse textual model features, thus limiting performance in multimodal scenarios. Therefore, we propose a novel approach called Modality Interaction and Alignment Representation (MIAR). This network integrates contextual features across different modalities using a feature interaction to generate feature tokens to represent global representations of this modality extracting information from other modalities. These four tokens represent global representations of how each modality extracts information from others. MIAR aligns different modalities using contrastive learning and normalization strategies. We conduct experiments on two benchmarks: CMU-MOSI and CMU-MOSEI datasets, experimental results demonstrate the MIAR outperforms state-of-the-art MER methods.</p>
<h3 id="3-multimodal-sentiment-analysis-based-on-multi-channel-and-symmetric-mutual-promotion-feature-fusion">[3] <a href="https://arxiv.org/abs/2601.02415">Multimodal Sentiment Analysis based on Multi-channel and Symmetric Mutual Promotion Feature Fusion</a></h3>
<p><em>Wangyuan Zhu, Jun Yu</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æçš„å¯¹ç§°äº’ä¿ƒç‰¹å¾èåˆæ–¹æ³•ï¼Œé€šè¿‡æå–å¤šé€šé“ç‰¹å¾å¢å¼ºæ¨¡æ€å†…è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨å¯¹ç§°äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¿ƒè¿›æ¨¡æ€é—´ä¿¡æ¯äº¤äº’ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­ç‰¹å¾æå–ä¸è¶³å’Œæ¨¡æ€èåˆä¸å……åˆ†çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯ä»å•æ¨¡æ€æ•°æ®ä¸­æå–çš„ç‰¹å¾æœ‰é™ä¸”ä¸å¤Ÿä¸°å¯Œï¼›äºŒæ˜¯ç°æœ‰ç ”ç©¶å¤§å¤šåªå…³æ³¨æ¨¡æ€é—´ç‰¹å¾ä¿¡æ¯çš„ä¸€è‡´æ€§ï¼Œè€Œå¿½ç•¥äº†ç‰¹å¾é—´çš„å·®å¼‚æ€§ï¼Œå¯¼è‡´ç‰¹å¾ä¿¡æ¯èåˆä¸å……åˆ†ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™äº›é™åˆ¶ï¼Œæå‡å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æçš„æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> é¦–å…ˆæå–å¤šé€šé“ç‰¹å¾ä»¥è·å¾—æ›´å…¨é¢çš„ç‰¹å¾ä¿¡æ¯ï¼Œåœ¨è§†è§‰å’Œå¬è§‰æ¨¡æ€ä¸­é‡‡ç”¨åŒé€šé“ç‰¹å¾å¢å¼ºæ¨¡æ€å†…ç‰¹å¾è¡¨ç¤ºã€‚å…¶æ¬¡æå‡ºå¯¹ç§°äº’ä¿ƒï¼ˆSMPï¼‰çš„æ¨¡æ€é—´ç‰¹å¾èåˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆå¯¹ç§°äº¤å‰æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­äº¤å‰æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ä»å…¶ä»–æ¨¡æ€æ•è·æœ‰ç”¨ä¿¡æ¯ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¿ƒè¿›æ¨¡æ€é—´æœ‰ç”¨ä¿¡æ¯çš„äº¤æ¢ï¼Œä»è€ŒåŠ å¼ºæ¨¡æ€é—´äº¤äº’ã€‚æœ€åæ•´åˆæ¨¡æ€å†…ç‰¹å¾å’Œæ¨¡æ€é—´èåˆç‰¹å¾ï¼Œå……åˆ†åˆ©ç”¨æ¨¡æ€é—´ç‰¹å¾ä¿¡æ¯çš„äº’è¡¥æ€§ï¼ŒåŒæ—¶è€ƒè™‘ç‰¹å¾ä¿¡æ¯å·®å¼‚ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å¤šé€šé“ç‰¹å¾æå–å’Œå¯¹ç§°äº’ä¿ƒèåˆç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶é€šè¿‡å¢å¼ºæ¨¡æ€å†…ç‰¹å¾è¡¨ç¤ºå’Œä¿ƒè¿›æ¨¡æ€é—´ä¿¡æ¯äº¤äº’ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„ç‰¹å¾æå–å’Œèåˆé—®é¢˜ã€‚å¯¹ç§°äº’ä¿ƒèåˆæ–¹æ³•ä¸ä»…è€ƒè™‘äº†æ¨¡æ€é—´çš„ä¸€è‡´æ€§ï¼Œè¿˜å……åˆ†åˆ©ç”¨äº†ç‰¹å¾å·®å¼‚çš„äº’è¡¥æ€§ï¼Œä¸ºå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææä¾›äº†æ–°çš„æŠ€æœ¯æ€è·¯ï¼Œå¯¹æå‡äººæœºäº¤äº’å’Œæƒ…æ„Ÿè®¡ç®—çš„æ€§èƒ½å…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Multimodal sentiment analysis is a key technology in the fields of human-computer interaction and affective computing. Accurately recognizing human emotional states is crucial for facilitating smooth communication between humans and machines. Despite some progress in multimodal sentiment analysis research, numerous challenges remain. The first challenge is the limited and insufficiently rich features extracted from single modality data. Secondly, most studies focus only on the consistency of inter-modal feature information, neglecting the differences between features, resulting in inadequate feature information fusion. In this paper, we first extract multi-channel features to obtain more comprehensive feature information. We employ dual-channel features in both the visual and auditory modalities to enhance intra-modal feature representation. Secondly, we propose a symmetric mutual promotion (SMP) inter-modal feature fusion method. This method combines symmetric cross-modal attention mechanisms and self-attention mechanisms, where the cross-modal attention mechanism captures useful information from other modalities, and the self-attention mechanism models contextual information. This approach promotes the exchange of useful information between modalities, thereby strengthening inter-modal interactions. Furthermore, we integrate intra-modal features and inter-modal fused features, fully leveraging the complementarity of inter-modal feature information while considering feature information differences. Experiments conducted on two benchmark datasets demonstrate the effectiveness and superiority of our proposed method.</p>
<h3 id="4-watch-wider-and-think-deeper-collaborative-cross-modal-chain-of-thought-for-complex-visual-reasoning">[4] <a href="https://arxiv.org/abs/2601.02422">Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning</a></h3>
<p><em>Wenting Lu, Didi Zhu, Tao Shen, Donglin Zhu, Ayong Ye, Chao Wu</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCoCoTï¼ˆåä½œè·¨æ¨¡æ€æ€ç»´ï¼‰æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å¤šåŒºåŸŸå®šä½å’Œå…³ç³»æ„ŸçŸ¥æ¨ç†è§£å†³ç°æœ‰è·¨æ¨¡æ€æ€ç»´é“¾æ–¹æ³•åœ¨è§†è§‰-è¯­è¨€æ¨ç†ä¸­çš„å±€é™æ€§ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚è§†è§‰æ¨ç†æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è·¨æ¨¡æ€æ€ç»´é“¾æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šä¸€æ˜¯è¿‡åº¦ä¾èµ–å•ä¸€ç²—ç²’åº¦å›¾åƒåŒºåŸŸï¼ŒäºŒæ˜¯è¿ç»­æ¨ç†æ­¥éª¤ä¹‹é—´å­˜åœ¨è¯­ä¹‰ç¢ç‰‡åŒ–é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å¤šæ¨¡æ€æ¨ç†ä¸­è§†è§‰ä¸è¯­è¨€çº¿ç´¢çš„æ— ç¼æ•´åˆèƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> CoCoTæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼šåŠ¨æ€å¤šåŒºåŸŸå®šä½æ ¹æ®é—®é¢˜è‡ªé€‚åº”æ£€æµ‹æœ€ç›¸å…³çš„å›¾åƒåŒºåŸŸï¼Œå…³ç³»æ„ŸçŸ¥æ¨ç†é€šè¿‡è¿­ä»£å¯¹é½è§†è§‰çº¿ç´¢å®ç°å¤šåŒºåŸŸåä½œï¼Œæ„å»ºè¿è´¯çš„é€»è¾‘æ€ç»´é“¾ã€‚åŒæ—¶æ„å»ºäº†åŒ…å«74,691ä¸ªé«˜è´¨é‡æ ·æœ¬çš„CoCoT-70Kæ•°æ®é›†ï¼ŒåŒ…å«å¤šåŒºåŸŸæ ‡æ³¨å’Œç»“æ„åŒ–æ¨ç†é“¾ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜CoCoTæ˜¾è‘—å¢å¼ºäº†å¤æ‚è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œåœ¨LLaVA-1.5ä¸Šå¹³å‡å‡†ç¡®ç‡æå‡15.4%ï¼Œåœ¨Qwen2-VLä¸Šæå‡4.0%ï¼Œåœ¨å…­ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŠ¨æ€å¤šåŒºåŸŸå®šä½å’Œå…³ç³»æ„ŸçŸ¥æ¨ç†åœ¨è·¨æ¨¡æ€æ€ç»´é“¾ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†æä¾›äº†æ›´ç²¾ç»†çš„è§†è§‰-è¯­è¨€å¯¹é½æœºåˆ¶ï¼Œæ¨åŠ¨äº†å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡çš„å‘å±•ï¼Œç›¸å…³æ•°æ®å’Œä»£ç å·²å¼€æºä¾›ç¤¾åŒºä½¿ç”¨ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) frame- work, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively align- ing visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual rea- soning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.</p>
<h3 id="5-understanding-pure-textual-reasoning-for-blind-image-quality-assessment">[5] <a href="https://arxiv.org/abs/2601.02441">Understanding Pure Textual Reasoning for Blind Image Quality Assessment</a></h3>
<p><em>Yuan Li, Shin'ya Nishida</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶ä»ä¿¡æ¯æµè§’åº¦åˆ†ææ–‡æœ¬æ¨ç†åœ¨ç›²å›¾åƒè´¨é‡è¯„ä¼°ä¸­çš„ä½œç”¨ï¼Œé€šè¿‡æ¯”è¾ƒä¸‰ç§å­¦ä¹ å›¾åƒ-æ–‡æœ¬-åˆ†æ•°å…³ç³»çš„èŒƒå¼ï¼Œæ­ç¤ºäº†æ–‡æœ¬ä¿¡æ¯å¯¹è´¨é‡é¢„æµ‹çš„è´¡çŒ®ç¨‹åº¦åŠä¼˜åŒ–æ–¹å‘ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡æ–‡æœ¬æ¨ç†åœ¨ç›²å›¾åƒè´¨é‡è¯„ä¼°ä¸­å·²è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†æ–‡æœ¬ä¿¡æ¯å¦‚ä½•è´¡çŒ®äºè´¨é‡é¢„æµ‹ä»¥åŠæ–‡æœ¬èƒ½åœ¨å¤šå¤§ç¨‹åº¦ä¸Šè¡¨ç¤ºä¸åˆ†æ•°ç›¸å…³çš„å›¾åƒå†…å®¹ä»ä¸æ˜ç¡®ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨ä»ä¿¡æ¯æµè§’åº¦è§£å†³è¿™äº›é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ¯”è¾ƒäº†ç°æœ‰BIQAæ¨¡å‹ä¸ä¸‰ç§ä¸“é—¨è®¾è®¡ç”¨äºå­¦ä¹ å›¾åƒ-æ–‡æœ¬-åˆ†æ•°å…³ç³»çš„èŒƒå¼ï¼šæ€ç»´é“¾èŒƒå¼ã€è‡ªä¸€è‡´æ€§èŒƒå¼å’Œè‡ªç¼–ç å™¨èŒƒå¼ï¼Œä»ä¿¡æ¯æµè§’åº¦åˆ†æä¸åŒèŒƒå¼çš„è¡¨ç°å·®å¼‚ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨æ–‡æœ¬ä¿¡æ¯æ—¶ç°æœ‰æ¨¡å‹çš„åˆ†æ•°é¢„æµ‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼›æ€ç»´é“¾èŒƒå¼å¯¹BIQAæ€§èƒ½æå‡æœ‰é™ï¼Œè€Œè‡ªä¸€è‡´æ€§èŒƒå¼æ˜¾è‘—ç¼©å°äº†å›¾åƒä¸æ–‡æœ¬æ¡ä»¶é¢„æµ‹ä¹‹é—´çš„å·®è·ï¼Œå°†PLCC/SRCCå·®å¼‚ç¼©å°è‡³0.02/0.03ï¼›è‡ªç¼–ç å™¨èŒƒå¼åœ¨ç¼©å°å›¾åƒ-æ–‡æœ¬å·®è·æ–¹é¢æ•ˆæœè¾ƒå·®ï¼Œä½†æ­ç¤ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–çš„æ–¹å‘ã€‚</p>
<p><strong>Conclusion:</strong> è¿™äº›å‘ç°ä¸ºæ”¹è¿›ç›²å›¾åƒè´¨é‡è¯„ä¼°åŠé«˜çº§è§†è§‰ä»»åŠ¡ä¸­çš„æ–‡æœ¬æ¨ç†æä¾›äº†é‡è¦è§è§£ï¼Œç‰¹åˆ«æ˜¯è‡ªä¸€è‡´æ€§èŒƒå¼åœ¨å¼¥åˆå›¾åƒä¸æ–‡æœ¬è¡¨ç¤ºå·®è·æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†ä¼˜åŒ–æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Textual reasoning has recently been widely adopted in Blind Image Quality Assessment (BIQA). However, it remains unclear how textual information contributes to quality prediction and to what extent text can represent the score-related image contents. This work addresses these questions from an information-flow perspective by comparing existing BIQA models with three paradigms designed to learn the image-text-score relationship: Chain-of-Thought, Self-Consistency, and Autoencoder. Our experiments show that the score prediction performance of the existing model significantly drops when only textual information is used for prediction. Whereas the Chain-of-Thought paradigm introduces little improvement in BIQA performance, the Self-Consistency paradigm significantly reduces the gap between image- and text-conditioned predictions, narrowing the PLCC/SRCC difference to 0.02/0.03. The Autoencoder-like paradigm is less effective in closing the image-text gap, yet it reveals a direction for further optimization. These findings provide insights into how to improve the textual reasoning for BIQA and high-level vision tasks.</p>
<h3 id="6-evaluating-the-diagnostic-classification-ability-of-multimodal-large-language-models-insights-from-the-osteoarthritis-initiative">[6] <a href="https://arxiv.org/abs/2601.02443">Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative</a></h3>
<p><em>Li Wang, Xi Chen, XiangWen Deng, HuaHui Yi, ZeKun Jiang, Kang Li, Jian Li</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è†å…³èŠ‚éª¨å…³èŠ‚ç‚Xå…‰ç‰‡åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°å®Œæ•´çš„MLLMæ¶æ„åœ¨ç‰¹å®šåŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œè€Œä¼˜åŒ–è§†è§‰ç¼–ç å™¨å’Œç²¾å¿ƒç­–åˆ’æ•°æ®é›†æ›´ä¸ºå…³é”®ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦è§†è§‰é—®ç­”å’ŒæŠ¥å‘Šç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶ç”Ÿæˆå’Œè§£é‡Šèƒ½åŠ›å¹¶ä¸èƒ½å¯é åœ°è¿ç§»åˆ°ç–¾ç—…ç‰¹å¼‚æ€§åˆ†ç±»ä»»åŠ¡ä¸­ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°MLLMåœ¨è†å…³èŠ‚éª¨å…³èŠ‚ç‚Xå…‰ç‰‡åˆ†ç±»è¿™ä¸€ä»£è¡¨æ€§ä¸è¶³ä½†å½±å“å…¨çƒæ•°äº¿äººçš„åŒ»å­¦ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ¢ç´¢å„ç»„ä»¶å¯¹è¯Šæ–­å‡†ç¡®æ€§çš„è´¡çŒ®ã€‚</p>
<p><strong>Method:</strong> é€šè¿‡ç³»ç»Ÿæ¶ˆèç ”ç©¶ï¼Œæ“çºµè§†è§‰ç¼–ç å™¨ã€è¿æ¥å™¨å’Œå¤§è¯­è¨€æ¨¡å‹ç»„ä»¶ï¼Œå¹¶é‡‡ç”¨å¤šæ ·åŒ–çš„è®­ç»ƒç­–ç•¥ã€‚ç ”ç©¶æ¯”è¾ƒäº†ä¸åŒè®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬LoRAå¾®è°ƒåœ¨å°å‹å¹³è¡¡æ•°æ®é›†ï¼ˆ500å¼ å›¾åƒï¼‰ä¸å¤§å‹ä¸å¹³è¡¡æ•°æ®é›†ï¼ˆ5,778å¼ å›¾åƒï¼‰ä¸Šçš„è¡¨ç°ï¼Œè¯„ä¼°å„ç»„ä»¶å¯¹åˆ†ç±»å‡†ç¡®æ€§çš„å½±å“ã€‚</p>
<p><strong>Result:</strong> åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå•ç‹¬è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šèƒ½å¤Ÿè¶…è¶Šå®Œæ•´çš„MLLMæµç¨‹ï¼Œè€Œå¾®è°ƒLLMç›¸æ¯”åŸºäºæç¤ºçš„æŒ‡å¯¼å¹¶æœªå¸¦æ¥æœ‰æ„ä¹‰çš„æ”¹è¿›ã€‚LoRAå¾®è°ƒåœ¨å°å‹å¹³è¡¡æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå¤§å‹ä¸å¹³è¡¡æ•°æ®é›†ï¼Œè¡¨æ˜æ•°æ®å¹³è¡¡æ€§å’Œè´¨é‡æ¯”åŸå§‹è§„æ¨¡æ›´ä¸ºé‡è¦ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹äºé¢†åŸŸç‰¹å®šçš„åŒ»å­¦åˆ†ç±»ä»»åŠ¡ï¼ŒLLMæ›´é€‚åˆä½œä¸ºè§£é‡Šå™¨å’ŒæŠ¥å‘Šç”Ÿæˆå™¨è€Œéä¸»è¦åˆ†ç±»å™¨ã€‚MLLMæ¶æ„å¯¹äºéœ€è¦é«˜ç¡®å®šæ€§çš„åŒ»å­¦å›¾åƒè¯Šæ–­åˆ†ç±»ä»»åŠ¡ä¸å¤ªé€‚ç”¨ï¼Œå»ºè®®åœ¨å¼€å‘ä¸´åºŠé€‚ç”¨ç³»ç»Ÿæ—¶ä¼˜å…ˆä¼˜åŒ–è§†è§‰ç¼–ç å™¨å¹¶è¿›è¡Œç²¾ç»†çš„æ•°æ®é›†ç­–åˆ’ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.</p>
<h3 id="7-patchalign3d-local-feature-alignment-for-dense-3d-shape-understanding">[7] <a href="https://arxiv.org/abs/2601.02457">PatchAlign3D: Local Feature Alignment for Dense 3D Shape understanding</a></h3>
<p><em>Souhail Hadgi, Bingchen Gong, Ramana Sundararaman, Emery Pierson, Lei Li, Peter Wonka, Maks Ovsjanikov</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¼–ç å™¨ä¸“ç”¨çš„3Dæ¨¡å‹ï¼Œèƒ½å¤Ÿç›´æ¥ä»ç‚¹äº‘ç”Ÿæˆè¯­è¨€å¯¹é½çš„è¡¥ä¸çº§ç‰¹å¾ï¼Œå®ç°äº†æ— éœ€å¤šè§†è§’æ¸²æŸ“çš„é›¶æ ·æœ¬3Déƒ¨ä»¶åˆ†å‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„åŸºäºæ¸²æŸ“çš„æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰3Då½¢çŠ¶åŸºç¡€æ¨¡å‹åœ¨å…¨å±€ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å±€éƒ¨éƒ¨ä»¶çº§æ¨ç†ä¸Šè¿ç§»èƒ½åŠ›è¾ƒå·®ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¤šè§†è§’æ¸²æŸ“å’Œæ–‡æœ¬æŸ¥è¯¢ï¼Œéœ€è¦æ˜‚è´µçš„æ¨ç†æˆæœ¬ã€å¤æ‚çš„LLMæç¤ºå·¥ç¨‹ï¼Œä¸”æœªèƒ½å……åˆ†åˆ©ç”¨3Då½¢çŠ¶çš„å›ºæœ‰å‡ ä½•ç»“æ„ã€‚</p>
<p><strong>Method:</strong> æ–¹æ³•é‡‡ç”¨ç¼–ç å™¨ä¸“ç”¨3Dæ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µé¢„è®­ç»ƒï¼šé¦–å…ˆä»DINOv2ç­‰è§†è§‰ç¼–ç å™¨è’¸é¦å¯†é›†2Dç‰¹å¾åˆ°3Dè¡¥ä¸ï¼Œç„¶åé€šè¿‡å¤šæ­£å¯¹æ¯”ç›®æ ‡å°†è¿™äº›è¡¥ä¸åµŒå…¥ä¸éƒ¨ä»¶çº§æ–‡æœ¬åµŒå…¥å¯¹é½ã€‚æ¨¡å‹ä½¿ç”¨ç‚¹äº‘Transformerç¼–ç å™¨ï¼Œåˆ©ç”¨ç°æœ‰æ•°æ®å¼•æ“ç”Ÿæˆçš„éƒ¨ä»¶æ ‡æ³¨3Då½¢çŠ¶æ•°æ®ã€‚</p>
<p><strong>Result:</strong> è¯¥3Dç¼–ç å™¨å®ç°äº†é›¶æ ·æœ¬3Déƒ¨ä»¶åˆ†å‰²ï¼Œä»…éœ€å•æ¬¡å‰å‘æ¨ç†è€Œæ— éœ€æµ‹è¯•æ—¶å¤šè§†è§’æ¸²æŸ“ï¼Œåœ¨å¤šä¸ª3Déƒ¨ä»¶åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†å…ˆå‰åŸºäºæ¸²æŸ“å’Œå‰é¦ˆæ–¹æ³•çš„è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å±•ç¤ºäº†ç›´æ¥ä»ç‚¹äº‘å­¦ä¹ è¯­è¨€å¯¹é½3Dç‰¹å¾çš„æœ‰æ•ˆæ€§ï¼Œä¸º3Dç†è§£ä»»åŠ¡æä¾›äº†æ›´é«˜æ•ˆã€å‡ ä½•æ„ŸçŸ¥çš„è§£å†³æ–¹æ¡ˆï¼Œå‡å°‘äº†å¯¹å¤–éƒ¨æ¸²æŸ“å’Œå¤æ‚æç¤ºå·¥ç¨‹çš„ä¾èµ–ï¼Œæ¨åŠ¨äº†3DåŸºç¡€æ¨¡å‹å‘å±€éƒ¨æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Current foundation models for 3D shapes excel at global tasks (retrieval, classification) but transfer poorly to local part-level reasoning. Recent approaches leverage vision and language foundation models to directly solve dense tasks through multi-view renderings and text queries. While promising, these pipelines require expensive inference over multiple renderings, depend heavily on large language-model (LLM) prompt engineering for captions, and fail to exploit the inherent 3D geometry of shapes. We address this gap by introducing an encoder-only 3D model that produces language-aligned patch-level features directly from point clouds. Our pre-training approach builds on existing data engines that generate part-annotated 3D shapes by pairing multi-view SAM regions with VLM captioning. Using this data, we train a point cloud transformer encoder in two stages: (1) distillation of dense 2D features from visual encoders such as DINOv2 into 3D patches, and (2) alignment of these patch embeddings with part-level text embeddings through a multi-positive contrastive objective. Our 3D encoder achieves zero-shot 3D part segmentation with fast single-pass inference without any test-time multi-view rendering, while significantly outperforming previous rendering-based and feed-forward approaches across several 3D part segmentation benchmarks. Project website: https://souhail-hadgi.github.io/patchalign3dsite/</p>
<h3 id="8-movierecapsqa-a-multimodal-open-ended-video-question-answering-benchmark">[8] <a href="https://arxiv.org/abs/2601.02536">MovieRecapsQA: A Multimodal Open-Ended Video Question-Answering Benchmark</a></h3>
<p><em>Shaden Shaar, Bradon Thymes, Sirawut Chaixanien, Claire Cardie, Bharath Hariharan</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MovieRecapsQAï¼Œä¸€ä¸ªåŸºäºç”µå½±è§£è¯´è§†é¢‘æ„å»ºçš„æ–°å‹å¼€æ”¾å¼å¤šæ¨¡æ€è§†é¢‘é—®ç­”åŸºå‡†ï¼Œè¯¥åŸºå‡†é€šè¿‡åŒæ­¥çš„è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€æä¾›æ˜¾å¼æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘é—®ç­”åŸºå‡†éš¾ä»¥æ•æ‰çœŸå®ä¸–ç•Œè§†é¢‘ï¼ˆå¦‚ç”µå½±ï¼‰æ‰€éœ€çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä¸”å¤§å¤šä¸æ˜¯å¼€æ”¾å¼çš„ï¼Œä¸»è¦å› ä¸ºè‡ªç”±å½¢å¼ç­”æ¡ˆçš„è¯„ä¼°å›°éš¾ã€‚ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œåˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿè¯„ä¼°æ¨¡å‹æ•´åˆè§†è§‰å’Œå¯¹è¯çº¿ç´¢ä»¥å›ç­”å¤æ‚é—®é¢˜çš„å¼€æ”¾å¼å¤šæ¨¡æ€è§†é¢‘é—®ç­”åŸºå‡†ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶åˆ©ç”¨ç”µå½±è§£è¯´è§†é¢‘è¿™ä¸€ç‹¬ç‰¹çš„YouTubeå†…å®¹ç±»å‹ï¼Œé€šè¿‡åŒæ­¥çš„è§†è§‰ï¼ˆè§£è¯´è§†é¢‘ï¼‰å’Œæ–‡æœ¬ï¼ˆè§£è¯´æ‘˜è¦ï¼‰æ¨¡æ€æ„å»ºåŸºå‡†ã€‚ä½¿ç”¨è§£è¯´æ‘˜è¦ç”Ÿæˆçº¦8.2Kä¸ªä¸ç”µå½±å­—å¹•å¯¹é½çš„é—®ç­”å¯¹ï¼Œå¹¶æä¾›éªŒè¯ç­”æ¡ˆæ‰€éœ€çš„"äº‹å®"ä¿¡æ¯ï¼Œå®ç°æ— å‚è€ƒè¯„ä¼°ã€‚åŸºå‡†æä¾›å¤šç§é•¿åº¦è§†é¢‘ï¼ˆè§£è¯´ç‰‡æ®µã€ç”µå½±ç‰‡æ®µï¼‰å’Œé—®é¢˜åˆ†ç±»ï¼ˆæŒ‰æ¨¡æ€å’Œç±»å‹ï¼‰ï¼Œæ”¯æŒç»†ç²’åº¦åˆ†æã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°äº†ä¸ƒç§æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå‘ç°ï¼š1ï¼‰çº¯è§†è§‰é—®é¢˜æœ€å…·æŒ‘æˆ˜æ€§ï¼›2ï¼‰æ¨¡å‹åœ¨æœ‰æ–‡æœ¬è¾“å…¥æ—¶å€¾å‘äºä¾èµ–æ–‡æœ¬ï¼›3ï¼‰ä»è§†é¢‘å†…å®¹ä¸­æå–äº‹å®å‡†ç¡®ä¿¡æ¯å¯¹æ‰€æœ‰æ¨¡å‹ä»å¾ˆå›°éš¾ï¼›4ï¼‰ä¸“æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹åœ¨è§†é¢‘ä¾èµ–é—®é¢˜ä¸Šè¡¨ç°ç›¸å½“ã€‚åŸºå‡†æä¾›äº†é¦–ä¸ªæä¾›è¾“å…¥æ˜¾å¼æ–‡æœ¬ä¸Šä¸‹æ–‡çš„å¼€æ”¾å¼è§†é¢‘é—®ç­”è¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Conclusion:</strong> MovieRecapsQAåŸºå‡†æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯è§†è§‰æ¨ç†èƒ½åŠ›çš„ä¸è¶³å’Œè¿‡åº¦ä¾èµ–æ–‡æœ¬è¾“å…¥çš„å€¾å‘ã€‚ç ”ç©¶ä¸ºè¯„ä¼°å¤æ‚å¤šæ¨¡æ€æ¨ç†æä¾›äº†æ–°å·¥å…·ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥æ¨¡å‹éœ€è¦æ”¹è¿›çš„æ–¹å‘ï¼ŒåŒ…æ‹¬æå‡çº¯è§†è§‰ä¿¡æ¯æå–èƒ½åŠ›å’Œå¹³è¡¡å¤šæ¨¡æ€èåˆç­–ç•¥ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Understanding real-world videos such as movies requires integrating visual and dialogue cues to answer complex questions. Yet existing VideoQA benchmarks struggle to capture this multimodal reasoning and are largely not open-ended, given the difficulty of evaluating free-form answers. In this paper, we introduce a novel open-ended multi-modal VideoQA benchmark, MovieRecapsQA created using movie recap videos--a distinctive type of YouTube content that summarizes a film by presenting its key events through synchronized visual (recap video) and textual (recap summary) modalities. Using the recap summary, we generate $\approx 8.2$ K question-answer (QA) pairs (aligned with movie-subtitles) and provide the necessary "facts" needed to verify an answer in a reference-free manner. To our knowledge, this is the first open-ended VideoQA benchmark that supplies explicit textual context of the input (video and/or text); which we use for evaluation. Our benchmark provides videos of multiple lengths (i.e., recap-segments, movie-segments) and categorizations of questions (by modality and type) to enable fine-grained analysis. We evaluate the performance of seven state-of-the-art MLLMs using our benchmark and observe that: 1) visual-only questions remain the most challenging; 2) models default to textual inputs whenever available; 3) extracting factually accurate information from video content is still difficult for all models; and 4) proprietary and open-source models perform comparably on video-dependent questions.</p>
<h3 id="9-unveiling-and-bridging-the-functional-perception-gap-in-mllms-atomic-visual-alignment-and-hierarchical-evaluation-via-pet-bench">[9] <a href="https://arxiv.org/abs/2601.02737">Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench</a></h3>
<p><em>Zanting Ye, Xiaolong Niu, Xuanbin Wu, Xu Han, Shengyuan Liu, Jing Hao, Zhihao Peng, Hao Sun, Jieqin Lv, Fanghu Wang, Yanchao Huang, Hubing Wu, Yixuan Yuan, Habib Zaidi, Arman Rahmim, Yefeng Zheng, Lijun Lu</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŠŸèƒ½æˆåƒé¢†åŸŸå­˜åœ¨çš„åŠŸèƒ½æ€§æ„ŸçŸ¥é¸¿æ²Ÿï¼Œå¹¶æå‡ºäº†PET-BenchåŸºå‡†å’ŒåŸå­è§†è§‰å¯¹é½æ–¹æ³•ï¼ŒæˆåŠŸå°†æ€ç»´é“¾ä»å¹»è§‰æ¥æºè½¬åŒ–ä¸ºå¯é çš„æ¨ç†å·¥å…·ï¼Œå°†è¯Šæ–­å‡†ç¡®ç‡æå‡é«˜è¾¾14.83%ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§£å‰–æ¨¡æ€å¼‚å¸¸æ£€æµ‹å’ŒæŠ¥å‘Šç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŠŸèƒ½æˆåƒé¢†åŸŸçš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯†åˆ«å’Œé‡åŒ–ä¸€ä¸ªæ ¹æœ¬æ€§çš„åŠŸèƒ½æ€§æ„ŸçŸ¥é¸¿æ²Ÿï¼šç°æœ‰è§†è§‰ç¼–ç å™¨æ— æ³•ç‹¬ç«‹äºå½¢æ€å­¦å…ˆéªŒè§£ç åŠŸèƒ½æ€§ç¤ºè¸ªå‰‚ç”Ÿç‰©åˆ†å¸ƒï¼Œç‰¹åˆ«ä»¥æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æä½œä¸ºç ”ç©¶è¿™ä¸€è„±èŠ‚çš„å…¸å‹æ¨¡æ€ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†PET-Benchï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡åŠŸèƒ½æˆåƒåŸºå‡†ï¼ŒåŒ…å«æ¥è‡ª9,732é¡¹å¤šä¸­å¿ƒã€å¤šç¤ºè¸ªå‰‚PETç ”ç©¶çš„52,308ä¸ªåˆ†å±‚é—®ç­”å¯¹ã€‚é’ˆå¯¹19ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¹¿æ³›è¯„ä¼°åï¼Œæå‡ºäº†åŸå­è§†è§‰å¯¹é½æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„å¾®è°ƒç­–ç•¥ï¼Œå¼ºåˆ¶æ¨¡å‹åœ¨é«˜çº§è¯Šæ–­æ¨ç†ä¹‹å‰æŒæ¡ä½å±‚æ¬¡åŠŸèƒ½æ€§æ„ŸçŸ¥èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°æ­ç¤ºäº†æ€ç»´é“¾å¹»è§‰é™·é˜±è¿™ä¸€å…³é”®å®‰å…¨éšæ‚£ï¼šæ ‡å‡†æ€ç»´é“¾æç¤ºåœ¨PETä¸­ä¼šçŸ›ç›¾åœ°è§£è€¦è¯­è¨€ç”Ÿæˆä¸è§†è§‰è¯æ®ï¼Œäº§ç”Ÿä¸´åºŠæµç•…ä½†äº‹å®æ— æ ¹æ®çš„è¯Šæ–­ã€‚åŸå­è§†è§‰å¯¹é½æ–¹æ³•æœ‰æ•ˆå¼¥åˆäº†æ„ŸçŸ¥é¸¿æ²Ÿï¼Œå°†æ€ç»´é“¾ä»å¹»è§‰æ¥æºè½¬åŒ–ä¸ºç¨³å¥çš„æ¨ç†å·¥å…·ï¼Œå°†è¯Šæ–­å‡†ç¡®ç‡æå‡é«˜è¾¾14.83%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŠŸèƒ½æˆåƒé¢†åŸŸå­˜åœ¨çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯æ€ç»´é“¾æç¤ºå¯èƒ½å¯¼è‡´çš„å¹»è§‰é£é™©ã€‚åŸå­è§†è§‰å¯¹é½æ–¹æ³•é€šè¿‡å¼ºåˆ¶ä½å±‚æ¬¡è§†è§‰ç†è§£ä¼˜å…ˆäºé«˜çº§æ¨ç†ï¼Œä¸ºè§£å†³è¿™ä¸€å®‰å…¨é—®é¢˜æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œä¸ºåŒ»å­¦å½±åƒåˆ†æä¸­çš„å¯é å¤šæ¨¡æ€ç³»ç»Ÿè®¾è®¡æä¾›äº†é‡è¦è§è§£ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>While Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, transforming CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.</p>
<h3 id="10-towards-zero-shot-point-cloud-registration-across-diverse-scales-scenes-and-sensor-setups">[10] <a href="https://arxiv.org/abs/2601.02759">Towards Zero-Shot Point Cloud Registration Across Diverse Scales, Scenes, and Sensor Setups</a></h3>
<p><em>Hyungtae Lim, Minkyun Seo, Luca Carlone, Jaesik Park</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºBUFFER-Xï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„é›¶æ ·æœ¬æ³›åŒ–ç‚¹äº‘é…å‡†æ¡†æ¶ï¼Œé€šè¿‡å‡ ä½•å¼•å¯¼çš„è¶…å‚æ•°ä¼°è®¡ã€åˆ†å¸ƒæ„ŸçŸ¥é‡‡æ ·å’Œè¡¥ä¸çº§åæ ‡å½’ä¸€åŒ–è§£å†³ç°æœ‰æ–¹æ³•è·¨åŸŸæ³›åŒ–ä¸è¶³çš„é—®é¢˜ï¼Œå¹¶åœ¨12ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºäºæ·±åº¦å­¦ä¹ çš„ç‚¹äº‘é…å‡†æ–¹æ³•åœ¨é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™ï¼Œé€šå¸¸éœ€è¦é’ˆå¯¹æ–°ç¯å¢ƒè¿›è¡Œæ•°æ®é›†ç‰¹å®šçš„è¶…å‚æ•°è°ƒæ•´æˆ–é‡æ–°è®­ç»ƒã€‚ç ”ç©¶è¯†åˆ«äº†ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šå›ºå®šç”¨æˆ·å®šä¹‰å‚æ•°æ— æ³•é€‚åº”ä¸åŒå°ºåº¦å˜åŒ–ï¼Œå­¦ä¹ å‹å…³é”®ç‚¹æ£€æµ‹å™¨è·¨åŸŸè¿ç§»èƒ½åŠ›å·®ï¼Œä»¥åŠç»å¯¹åæ ‡ä¼šæ”¾å¤§æ•°æ®é›†é—´çš„å°ºåº¦ä¸åŒ¹é…ã€‚</p>
<p><strong>Method:</strong> BUFFER-Xæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå‡ ä½•å¼•å¯¼çš„è¶…å‚æ•°è‡ªåŠ¨ä¼°è®¡ï¼Œç”¨äºæ›¿ä»£å­¦ä¹ å‹æ£€æµ‹å™¨çš„åˆ†å¸ƒæ„ŸçŸ¥æœ€è¿œç‚¹é‡‡æ ·ï¼Œä»¥åŠç¡®ä¿å°ºåº¦ä¸€è‡´æ€§çš„è¡¥ä¸çº§åæ ‡å½’ä¸€åŒ–ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åˆ†å±‚å¤šå°ºåº¦åŒ¹é…ç­–ç•¥ï¼Œåœ¨å±€éƒ¨ã€ä¸­é—´å’Œå…¨å±€æ„Ÿå—é‡ä¸­æå–å¯¹åº”å…³ç³»ï¼ŒåŒæ—¶æå‡ºäº†è®¡ç®—æ•ˆç‡æ›´é«˜çš„BUFFER-X-Liteç‰ˆæœ¬ï¼Œé€šè¿‡æ—©æœŸé€€å‡ºç­–ç•¥å’Œå¿«é€Ÿä½å§¿æ±‚è§£å™¨å‡å°‘43%çš„è®¡ç®—æ—¶é—´ã€‚</p>
<p><strong>Result:</strong> åœ¨åŒ…å«12ä¸ªæ•°æ®é›†çš„ç»¼åˆåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¶µç›–ç‰©ä½“å°ºåº¦ã€å®¤å†…å’Œå®¤å¤–åœºæ™¯ï¼ŒåŒ…æ‹¬å¼‚æ„LiDARé…ç½®é—´çš„è·¨ä¼ æ„Ÿå™¨é…å‡†ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•æ— éœ€æ‰‹åŠ¨è°ƒå‚æˆ–æµ‹è¯•åŸŸå…ˆéªŒçŸ¥è¯†å³å¯æœ‰æ•ˆæ³›åŒ–ã€‚BUFFER-X-Liteåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶å°†æ€»è®¡ç®—æ—¶é—´ç›¸å¯¹BUFFER-Xå‡å°‘äº†43%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ— éœ€è®­ç»ƒçš„é›¶æ ·æœ¬æ³›åŒ–ç‚¹äº‘é…å‡†çš„å¯è¡Œæ€§ï¼Œé€šè¿‡å‡ ä½•å¼•å¯¼æ–¹æ³•æ›¿ä»£å­¦ä¹ å‹ç»„ä»¶å’Œæ‰‹åŠ¨å‚æ•°è°ƒæ•´ã€‚BUFFER-Xæ¡†æ¶ä¸ºè·¨åŸŸç‚¹äº‘é…å‡†æä¾›äº†é€šç”¨è§£å†³æ–¹æ¡ˆï¼Œå…¶è½»é‡çº§ç‰ˆæœ¬BUFFER-X-Liteè¿›ä¸€æ­¥æå‡äº†å®é™…åº”ç”¨çš„æ•ˆç‡ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººç­‰é¢†åŸŸçš„ç¯å¢ƒæ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Some deep learning-based point cloud registration methods struggle with zero-shot generalization, often requiring dataset-specific hyperparameter tuning or retraining for new environments. We identify three critical limitations: (a) fixed user-defined parameters (e.g., voxel size, search radius) that fail to generalize across varying scales, (b) learned keypoint detectors exhibit poor cross-domain transferability, and (c) absolute coordinates amplify scale mismatches between datasets. To address these three issues, we present BUFFER-X, a training-free registration framework that achieves zero-shot generalization through: (a) geometric bootstrapping for automatic hyperparameter estimation, (b) distribution-aware farthest point sampling to replace learned detectors, and (c) patch-level coordinate normalization to ensure scale consistency. Our approach employs hierarchical multi-scale matching to extract correspondences across local, middle, and global receptive fields, enabling robust registration in diverse environments. For efficiency-critical applications, we introduce BUFFER-X-Lite, which reduces total computation time by 43% (relative to BUFFER-X) through early exit strategies and fast pose solvers while preserving accuracy. We evaluate on a comprehensive benchmark comprising 12 datasets spanning object-scale, indoor, and outdoor scenes, including cross-sensor registration between heterogeneous LiDAR configurations. Results demonstrate that our approach generalizes effectively without manual tuning or prior knowledge of test domains. Code: https://github.com/MIT-SPARK/BUFFER-X.</p>
<h3 id="11-anydepth-depth-estimation-made-easy">[11] <a href="https://arxiv.org/abs/2601.02760">AnyDepth: Depth Estimation Made Easy</a></h3>
<p><em>Zeyu Ren, Zeyu Zhang, Wukai Li, Qingxiang Liu, Hao Tang</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§ã€ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„é›¶æ ·æœ¬å•ç›®æ·±åº¦ä¼°è®¡æ¡†æ¶ï¼Œé‡‡ç”¨DINOv3ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œå¹¶è®¾è®¡äº†å‚æ•°å‡å°‘85%-89%çš„Simple Depth Transformerè§£ç å™¨ï¼Œé€šè¿‡è´¨é‡è¿‡æ»¤ç­–ç•¥æå‡è®­ç»ƒè´¨é‡ï¼Œåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†DPTçš„ç²¾åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•ä¾èµ–å¤§è§„æ¨¡æ•°æ®é›†å’Œå¤æ‚è§£ç å™¨ï¼Œé™åˆ¶äº†å…¶æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºä¸€ä¸ªè½»é‡çº§ä¸”ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„é›¶æ ·æœ¬æ·±åº¦ä¼°è®¡æ¡†æ¶ï¼Œä»¥å¹³è¡¡æ¨¡å‹è®¾è®¡å’Œæ•°æ®è´¨é‡ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨DINOv3ä½œä¸ºè§†è§‰ç¼–ç å™¨è·å–é«˜è´¨é‡å¯†é›†ç‰¹å¾ï¼Œå¹¶è®¾è®¡äº†Simple Depth Transformerï¼ˆSDTï¼‰ä½œä¸ºç´§å‡‘çš„åŸºäºTransformerçš„è§£ç å™¨ï¼Œé‡‡ç”¨å•è·¯å¾„ç‰¹å¾èåˆå’Œä¸Šé‡‡æ ·è¿‡ç¨‹å‡å°‘è·¨å°ºåº¦ç‰¹å¾èåˆçš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶æå‡ºåŸºäºè´¨é‡çš„è¿‡æ»¤ç­–ç•¥æ¥ç­›é€‰æœ‰å®³æ ·æœ¬ï¼Œå‡å°‘æ•°æ®é›†è§„æ¨¡å¹¶æå‡è®­ç»ƒè´¨é‡ã€‚</p>
<p><strong>Result:</strong> åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç²¾åº¦ä¸Šè¶…è¶Šäº†DPTï¼ŒåŒæ—¶SDTè§£ç å™¨ç›¸æ¯”DPTå‡å°‘äº†çº¦85%-89%çš„å‚æ•°æ•°é‡ï¼Œåœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶å¼ºè°ƒäº†å¹³è¡¡æ¨¡å‹è®¾è®¡å’Œæ•°æ®è´¨é‡å¯¹äºå®ç°é«˜æ•ˆä¸”å¯æ³›åŒ–çš„é›¶æ ·æœ¬æ·±åº¦ä¼°è®¡çš„é‡è¦æ€§ï¼Œæå‡ºçš„è½»é‡çº§æ¡†æ¶å±•ç¤ºäº†åœ¨å‡å°‘å‚æ•°å’Œè®¡ç®—å¼€é”€çš„åŒæ—¶ä¿æŒç”šè‡³æå‡æ€§èƒ½çš„å¯è¡Œæ€§ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„èµ„æºå—é™åœºæ™¯æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.</p>
<h3 id="12-clearair-a-human-visual-perception-inspired-all-in-one-image-restoration">[12] <a href="https://arxiv.org/abs/2601.02763">ClearAIR: A Human-Visual-Perception-Inspired All-in-One Image Restoration</a></h3>
<p><em>Xu Zhang, Huan Zhang, Guoli Wang, Qian Zhang, Lefei Zhang</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºClearAIRï¼Œä¸€ç§å—äººç±»è§†è§‰æ„ŸçŸ¥å¯å‘çš„å…¨åˆä¸€å›¾åƒä¿®å¤æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚ç²—åˆ°ç»†çš„ä¿®å¤ç­–ç•¥è§£å†³ç°æœ‰æ–¹æ³•è¿‡åº¦å¹³æ»‘å’Œä¼ªå½±é—®é¢˜ï¼Œåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡å–å¾—ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å…¨åˆä¸€å›¾åƒä¿®å¤æ–¹æ³•ä¸¥é‡ä¾èµ–é€€åŒ–ç‰¹å®šè¡¨ç¤ºï¼Œå¸¸å¯¼è‡´è¿‡åº¦å¹³æ»‘å’Œä¼ªå½±é—®é¢˜ï¼Œéš¾ä»¥å‡†ç¡®å¤„ç†å¤æ‚çš„å¤åˆé€€åŒ–åœºæ™¯ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„æ¡†æ¶æ¥æå‡ä¿®å¤è´¨é‡ã€‚</p>
<p><strong>Method:</strong> ClearAIRé‡‡ç”¨åˆ†å±‚ç²—åˆ°ç»†ä¿®å¤ç­–ç•¥ï¼šé¦–å…ˆåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å›¾åƒè´¨é‡è¯„ä¼°è¿›è¡Œå…¨å±€è¯„ä¼°ï¼›å…¶æ¬¡é€šè¿‡åŒºåŸŸæ„ŸçŸ¥å’Œä»»åŠ¡è¯†åˆ«æµç¨‹ï¼Œç»“åˆè¯­ä¹‰äº¤å‰æ³¨æ„åŠ›å’Œé€€åŒ–æ„ŸçŸ¥æ¨¡å—è¿›è¡Œå±€éƒ¨ä¿®å¤ï¼›æœ€åå¼•å…¥è‡ªç›‘ç£çš„å†…éƒ¨çº¿ç´¢é‡ç”¨æœºåˆ¶æŒ–æ˜å›¾åƒå†…åœ¨ä¿¡æ¯ä»¥æ¢å¤ç»†èŠ‚ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒClearAIRåœ¨å¤šæ ·åŒ–çš„åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡å–å¾—ä¼˜è¶Šæ€§èƒ½ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚å¤åˆé€€åŒ–ï¼Œæ˜¾è‘—æå‡å›¾åƒä¿®å¤è´¨é‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†äººç±»è§†è§‰æ„ŸçŸ¥å¯å‘çš„åˆ†å±‚ä¿®å¤ç­–ç•¥åœ¨å…¨åˆä¸€å›¾åƒä¿®å¤ä¸­çš„æœ‰æ•ˆæ€§ï¼Œè·¨æ¨¡æ€ç†è§£å’Œå†…éƒ¨çº¿ç´¢é‡ç”¨æœºåˆ¶ä¸ºå¤„ç†å¤æ‚é€€åŒ–æä¾›äº†æ–°æ€è·¯ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„å›¾åƒä¿®å¤ä»»åŠ¡æä¾›äº†æ›´é²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>All-in-One Image Restoration (AiOIR) has advanced significantly, offering promising solutions for complex real-world degradations. However, most existing approaches rely heavily on degradation-specific representations, often resulting in oversmoothing and artifacts. To address this, we propose ClearAIR, a novel AiOIR framework inspired by Human Visual Perception (HVP) and designed with a hierarchical, coarse-to-fine restoration strategy. First, leveraging the global priority of early HVP, we employ a Multimodal Large Language Model (MLLM)-based Image Quality Assessment (IQA) model for overall evaluation. Unlike conventional IQA, our method integrates cross-modal understanding to more accurately characterize complex, composite degradations. Building upon this overall assessment, we then introduce a region awareness and task recognition pipeline. A semantic cross-attention, leveraging semantic guidance unit, first produces coarse semantic prompts. Guided by this regional context, a degradation-aware module implicitly captures region-specific degradation characteristics, enabling more precise local restoration. Finally, to recover fine details, we propose an internal clue reuse mechanism. It operates in a self-supervised manner to mine and leverage the intrinsic information of the image itself, substantially enhancing detail restoration. Experimental results show that ClearAIR achieves superior performance across diverse synthetic and real-world datasets.</p>
<h3 id="13-abductivemllm-boosting-visual-abductive-reasoning-within-mllms">[13] <a href="https://arxiv.org/abs/2601.02771">AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs</a></h3>
<p><em>Boyu Chang, Qi Wang, Xi Guo, Zhixiong Nan, Yazhou Yao, Tianfei Zhou</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºAbductiveMLLMï¼Œä¸€ç§å—äººç±»è®¤çŸ¥å¯å‘çš„è§†è§‰æº¯å› æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè¯­è¨€å’Œå›¾åƒåŒæ¨¡æ€æ¨ç†æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æº¯å› ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é€šç”¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§†è§‰æº¯å› æ¨ç†ä»»åŠ¡ä¸Šä»è¿œä¸åŠäººç±»æ°´å¹³ï¼Œå­˜åœ¨æ˜æ˜¾çš„æ¨ç†èƒ½åŠ›å·®è·ã€‚è§†è§‰æº¯å› æ¨ç†è¦æ±‚ä»éƒ¨åˆ†è§†è§‰è§‚å¯Ÿä¸­æ¨æ–­æœ€å¯èƒ½çš„è§£é‡Šï¼Œè¿™å¯¹AIç³»ç»Ÿæå‡ºäº†æ›´é«˜å±‚æ¬¡çš„è®¤çŸ¥æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> AbductiveMLLMåŒ…å«ä¸¤ä¸ªååŒç»„ä»¶ï¼šREASONERåœ¨è¯­è¨€åŸŸå·¥ä½œï¼Œå…ˆä½¿ç”¨ç›²LLMæ¢ç´¢å¹¿æ³›çš„å¯èƒ½è§£é‡Šç©ºé—´ï¼Œç„¶ååŸºäºè·¨æ¨¡æ€å› æœå¯¹é½ä¿®å‰ªè§†è§‰ä¸ä¸€è‡´å‡è®¾ï¼Œå°†å‰©ä½™å‡è®¾ä½œä¸ºç›®æ ‡å…ˆéªŒå¼•å…¥MLLMï¼›IMAGINERæ¨¡æ‹Ÿäººç±»å›¾åƒæ€ç»´ï¼ŒåŸºäºè¾“å…¥è§†é¢‘å’ŒREASONERè¾“å‡ºåµŒå…¥æ¡ä»¶åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆä¸è¯­è¨€è§£é‡Šå¯¹åº”çš„è§†è§‰åœºæ™¯ã€‚ä¸¤ä¸ªç»„ä»¶ä»¥ç«¯åˆ°ç«¯æ–¹å¼è”åˆè®­ç»ƒã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†è§†è§‰æº¯å› æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAbductiveMLLMå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸€è‡´ä¼˜äºä¼ ç»Ÿè§£å†³æ–¹æ¡ˆå’Œå…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡æ¨¡å‹åœ¨è§†è§‰æº¯å› ä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡æ¨¡ä»¿äººç±»è¯­è¨€ä¸å›¾åƒæº¯å› çš„äº¤äº’è®¤çŸ¥æœºåˆ¶ï¼ŒæˆåŠŸæå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æº¯å› æ¨ç†èƒ½åŠ›ã€‚è¿™è¡¨æ˜ç»“åˆåŒæ¨¡æ€æ¨ç†ç­–ç•¥æ˜¯å¢å¼ºAIç³»ç»Ÿé«˜çº§è®¤çŸ¥åŠŸèƒ½çš„æœ‰æ•ˆé€”å¾„ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER's output embeddings to "imagine" plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs' contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.</p>
<h3 id="14-earthvl-a-progressive-earth-vision-language-understanding-and-generation-framework">[14] <a href="https://arxiv.org/abs/2601.02783">EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework</a></h3>
<p><em>Junjue Wang, Yanfei Zhong, Zihang Chen, Zhuo Zheng, Ailong Ma, Liangpei Zhang</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¸è¿›å¼åœ°çƒè§†è§‰-è¯­è¨€ç†è§£ä¸ç”Ÿæˆæ¡†æ¶ï¼ŒåŒ…æ‹¬å¤šä»»åŠ¡æ•°æ®é›†EarthVLSetå’Œè¯­ä¹‰å¼•å¯¼ç½‘ç»œEarthVLNetï¼Œæ—¨åœ¨è§£å†³åœ°çƒè§†è§‰ä¸­å¯¹è±¡å…³ç³»æ¨ç†çš„ä¸è¶³ï¼Œå®ç°ä»è¯­ä¹‰åˆ†å‰²åˆ°å…³ç³»æ¨ç†å†åˆ°å…¨é¢ç†è§£çš„æ¸è¿›å¼åœºæ™¯ç†è§£ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ°çƒè§†è§‰åœ¨é¥æ„Ÿå¯¹è±¡è¯†åˆ«æ–¹é¢å–å¾—äº†é‡Œç¨‹ç¢‘è¿›å±•ï¼Œä½†ç¼ºä¹å¯¹å¯¹è±¡å…³ç³»æ¨ç†çš„æ¢ç´¢ï¼Œè¿™é™åˆ¶äº†å…¨é¢çš„åœºæ™¯ç†è§£èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†æ•´åˆå›¾åƒã€æ©ç å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸå¸‚è§„åˆ’åº”ç”¨ä¸­ï¼Œéœ€è¦æ›´å…¨é¢çš„åœ°ç†å¯¹è±¡å…³ç³»ç†è§£æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†æ¸è¿›å¼åœ°çƒè§†è§‰-è¯­è¨€ç†è§£ä¸ç”Ÿæˆæ¡†æ¶ï¼ŒåŒ…æ‹¬å¤šä»»åŠ¡æ•°æ®é›†EarthVLSetï¼ˆåŒ…å«10.9käºšç±³çº§åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒã€åœŸåœ°è¦†ç›–æ©ç å’Œ761.5kæ–‡æœ¬å¯¹ï¼‰å’Œè¯­ä¹‰å¼•å¯¼ç½‘ç»œEarthVLNetã€‚EarthVLNeté‡‡ç”¨å¯¹è±¡ä¸­å¿ƒæ–¹æ³•ï¼Œé€šè¿‡ä¸‰ä¸ªé˜¶æ®µå®ç°æ¸è¿›ç†è§£ï¼šé¦–å…ˆè¿›è¡ŒåœŸåœ°è¦†ç›–è¯­ä¹‰åˆ†å‰²ç”Ÿæˆå¯¹è±¡è¯­ä¹‰ï¼Œç„¶ååŸºäºåƒç´ çº§è¯­ä¹‰å¼•å¯¼çš„å¯¹è±¡æ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå…³ç³»æ¨ç†å’ŒçŸ¥è¯†æ€»ç»“ï¼Œæœ€åé€šè¿‡æ•°å€¼å·®å¼‚æŸå¤±å‡½æ•°åŠ¨æ€æ·»åŠ å·®å¼‚æƒ©ç½šä»¥å¤„ç†ä¸åŒå¯¹è±¡çš„ç»Ÿè®¡ç‰¹æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆè¯­ä¹‰åˆ†å‰²ã€å¤šé¡¹é€‰æ‹©VQAå’Œå¼€æ”¾å¼VQAï¼‰ä¸­ï¼ŒEarthVLNetè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚å®éªŒå‘ç°ä¸‰ä¸ªå…³é”®æ–¹å‘ï¼š1ï¼‰åˆ†å‰²ç‰¹å¾å³ä½¿åœ¨è·¨æ•°æ®é›†åœºæ™¯ä¸­ä¹Ÿèƒ½æŒç»­å¢å¼ºVQAæ€§èƒ½ï¼›2ï¼‰å¤šé¡¹é€‰æ‹©ä»»åŠ¡å¯¹è§†è§‰ç¼–ç å™¨çš„æ•æ„Ÿæ€§å¤§äºè¯­è¨€è§£ç å™¨ï¼›3ï¼‰å¼€æ”¾å¼ä»»åŠ¡éœ€è¦å…ˆè¿›çš„è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€è§£ç å™¨æ‰èƒ½è·å¾—æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè¿æ¥"å›¾åƒ-æ©ç -æ–‡æœ¬"æä¾›äº†æœ‰ç›ŠåŸºå‡†ï¼Œæ¨åŠ¨äº†åœ°çƒè§†è§‰çš„åœ°ç†åº”ç”¨å‘å±•ã€‚æå‡ºçš„æ•°æ®é›†å’Œæ–¹æ³•æ¡†æ¶å¡«è¡¥äº†åœ°çƒè§†è§‰ä¸­å¯¹è±¡å…³ç³»æ¨ç†çš„ç©ºç™½ï¼Œä¸ºåŸå¸‚è§„åˆ’ç­‰åº”ç”¨æä¾›äº†æ›´å…¨é¢çš„åœºæ™¯ç†è§£èƒ½åŠ›ã€‚ç ”ç©¶æ­ç¤ºäº†åˆ†å‰²ç‰¹å¾å¯¹VQAä»»åŠ¡çš„é‡è¦æ€§ä»¥åŠä¸åŒä»»åŠ¡ç±»å‹å¯¹æ¨¡å‹ç»„ä»¶çš„æ•æ„Ÿæ€§å·®å¼‚ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects' statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects ''image-mask-text'', advancing geographical applications for Earth vision.</p>
<h3 id="15-topology-aware-pathological-consistency-matching-for-weakly-paired-ihc-virtual-staining">[15] <a href="https://arxiv.org/abs/2601.02806">Topology-aware Pathological Consistency Matching for Weakly-Paired IHC Virtual Staining</a></h3>
<p><em>Mingzhou Jiang, Jiaying Zhou, Nan Zeng, Mickael Li, Qijie Tang, Chao He, Huazhu Fu, Honghui He</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ‹“æ‰‘æ„ŸçŸ¥æ¡†æ¶ç”¨äºH&amp;Eåˆ°IHCçš„è™šæ‹ŸæŸ“è‰²ï¼Œé€šè¿‡æ‹“æ‰‘æ„ŸçŸ¥ä¸€è‡´æ€§åŒ¹é…å’Œæ‹“æ‰‘çº¦æŸç—…ç†åŒ¹é…æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†ç›¸é‚»åˆ‡ç‰‡æ•°æ®ç©ºé—´é”™ä½å’Œå±€éƒ¨å˜å½¢å¸¦æ¥çš„å¼±é…å¯¹é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†è™šæ‹ŸæŸ“è‰²çš„ç”Ÿæˆè´¨é‡å’Œä¸´åºŠç›¸å…³æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å…ç–«ç»„åŒ–æŸ“è‰²åœ¨ç™Œç—‡ä¸´åºŠæ£€æŸ¥ä¸­è‡³å…³é‡è¦ï¼Œä½†ç›¸æ¯”å¸¸ç”¨çš„H&amp;EæŸ“è‰²ï¼Œå…¶æµç¨‹å¤æ‚ã€è€—æ—¶ä¸”æ˜‚è´µï¼Œé™åˆ¶äº†ä¸´åºŠåº”ç”¨ã€‚è™šæ‹ŸæŸ“è‰²æŠ€æœ¯å¯å°†H&amp;Eå›¾åƒè½¬æ¢ä¸ºIHCå›¾åƒï¼Œç„¶è€Œä½¿ç”¨ç›¸é‚»åˆ‡ç‰‡ä½œä¸ºçœŸå®æ ‡ç­¾ä¼šå¯¼è‡´å¼±é…å¯¹æ•°æ®ï¼Œå­˜åœ¨ç©ºé—´é”™ä½å’Œå±€éƒ¨å˜å½¢é—®é¢˜ï¼Œé˜»ç¢äº†æœ‰æ•ˆçš„ç›‘ç£å­¦ä¹ ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ‹“æ‰‘æ„ŸçŸ¥æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæœºåˆ¶ï¼šæ‹“æ‰‘æ„ŸçŸ¥ä¸€è‡´æ€§åŒ¹é…æœºåˆ¶é‡‡ç”¨å›¾å¯¹æ¯”å­¦ä¹ å’Œæ‹“æ‰‘æ‰°åŠ¨æ¥å­¦ä¹ é²æ£’çš„åŒ¹é…æ¨¡å¼ï¼Œç¡®ä¿ç»“æ„ä¸€è‡´æ€§ï¼›æ‹“æ‰‘çº¦æŸç—…ç†åŒ¹é…æœºåˆ¶åŸºäºèŠ‚ç‚¹é‡è¦æ€§å¯¹é½ç—…ç†é˜³æ€§åŒºåŸŸï¼Œå¢å¼ºç—…ç†ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶ä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†ç©ºé—´é”™ä½å’Œå±€éƒ¨å˜å½¢é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å››ä¸ªæŸ“è‰²ä»»åŠ¡ä¸­è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„ç”Ÿæˆè´¨é‡å’Œä¸´åºŠç›¸å…³æ€§ã€‚å®éªŒéªŒè¯äº†æ‹“æ‰‘æ„ŸçŸ¥æœºåˆ¶åœ¨è§£å†³å¼±é…å¯¹æ•°æ®é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†åœ¨å¤šä¸ªæŸ“è‰²ä»»åŠ¡ä¸­çš„ä¸€è‡´ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ‹“æ‰‘æ„ŸçŸ¥æ¡†æ¶åœ¨H&amp;Eåˆ°IHCè™šæ‹ŸæŸ“è‰²ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡å¤„ç†ç©ºé—´é”™ä½å’Œå±€éƒ¨å˜å½¢é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•ä¸ºä¸´åºŠæä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ï¼Œå¹¶ä¸ºå¤„ç†å¼±é…å¯¹åŒ»å­¦å›¾åƒæ•°æ®æä¾›äº†æ–°çš„æŠ€æœ¯æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Immunohistochemical (IHC) staining provides crucial molecular characterization of tissue samples and plays an indispensable role in the clinical examination and diagnosis of cancers. However, compared with the commonly used Hematoxylin and Eosin (H&amp;E) staining, IHC staining involves complex procedures and is both time-consuming and expensive, which limits its widespread clinical use. Virtual staining converts H&amp;E images to IHC images, offering a cost-effective alternative to clinical IHC staining. Nevertheless, using adjacent slides as ground truth often results in weakly-paired data with spatial misalignment and local deformations, hindering effective supervised learning. To address these challenges, we propose a novel topology-aware framework for H&amp;E-to-IHC virtual staining. Specifically, we introduce a Topology-aware Consistency Matching (TACM) mechanism that employs graph contrastive learning and topological perturbations to learn robust matching patterns despite spatial misalignments, ensuring structural consistency. Furthermore, we propose a Topology-constrained Pathological Matching (TCPM) mechanism that aligns pathological positive regions based on node importance to enhance pathological consistency. Extensive experiments on two benchmarks across four staining tasks demonstrate that our method outperforms state-of-the-art approaches, achieving superior generation quality with higher clinical relevance.</p>
<h3 id="16-ta-prompting-enhancing-video-large-language-models-for-dense-video-captioning-via-temporal-anchors">[16] <a href="https://arxiv.org/abs/2601.02908">TA-Prompting: Enhancing Video Large Language Models for Dense Video Captioning via Temporal Anchors</a></h3>
<p><em>Wei-Yuan Cheng, Kai-Po Chang, Chi-Pin Huang, Fu-En Yang, Yu-Chiang Frank Wang</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºTA-Promptingæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ—¶é—´é”šç‚¹å¢å¼ºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„äº‹ä»¶å®šä½èƒ½åŠ›ï¼Œå¹¶ç»“åˆäº‹ä»¶ä¸€è‡´æ€§é‡‡æ ·ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†å¯†é›†è§†é¢‘æè¿°å’Œæ—¶é—´ç†è§£ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è§†é¢‘ç†è§£æ–¹æ³•åœ¨æœªä¿®å‰ªè§†é¢‘ä¸­éš¾ä»¥ç²¾ç¡®è¯†åˆ«äº‹ä»¶è¾¹ç•Œï¼Œå¯¼è‡´ç”Ÿæˆçš„æè¿°ç¼ºä¹å‡†ç¡®çš„æ—¶é—´å®šä½åŸºç¡€ï¼Œè¿™é™åˆ¶äº†å¯†é›†è§†é¢‘æè¿°ä»»åŠ¡çš„å®é™…åº”ç”¨æ•ˆæœã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æå‡ºTA-Promptingæ¡†æ¶ï¼Œé€šè¿‡æ—¶é—´é”šç‚¹å­¦ä¹ ç²¾ç¡®çš„äº‹ä»¶å®šä½ï¼Œå¹¶æç¤ºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ—¶é—´æ„ŸçŸ¥çš„è§†é¢‘äº‹ä»¶ç†è§£ï¼›åœ¨æ¨ç†é˜¶æ®µå¼•å…¥äº‹ä»¶ä¸€è‡´æ€§é‡‡æ ·ç­–ç•¥ï¼ŒåŸºäºæ—¶é—´äº‹ä»¶é—´çš„è¿è´¯æ€§å’Œè·¨æ¨¡æ€ç›¸ä¼¼æ€§é€‰æ‹©äº‹ä»¶æè¿°ã€‚</p>
<p><strong>Result:</strong> åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTA-Promptingæ–¹æ³•åœ¨å¯†é›†è§†é¢‘æè¿°å’Œæ—¶é—´ç†è§£ä»»åŠ¡ï¼ˆåŒ…æ‹¬æ—¶åˆ»æ£€ç´¢å’ŒTemporalQAï¼‰ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œå–å¾—äº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ—¶é—´é”šç‚¹æœºåˆ¶å¯¹äºæå‡è§†é¢‘å¤§è¯­è¨€æ¨¡å‹äº‹ä»¶å®šä½èƒ½åŠ›çš„é‡è¦æ€§ï¼Œäº‹ä»¶ä¸€è‡´æ€§é‡‡æ ·ç­–ç•¥æœ‰æ•ˆè§£å†³äº†è§†é¢‘ä¸­ä»»æ„æ•°é‡äº‹ä»¶çš„æè¿°ç”Ÿæˆé—®é¢˜ï¼Œä¸ºå¯†é›†è§†é¢‘ç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Dense video captioning aims to interpret and describe all temporally localized events throughout an input video. Recent state-of-the-art methods leverage large language models (LLMs) to provide detailed moment descriptions for video data. However, existing VideoLLMs remain challenging in identifying precise event boundaries in untrimmed videos, causing the generated captions to be not properly grounded. In this paper, we propose TA-Prompting, which enhances VideoLLMs via Temporal Anchors that learn to precisely localize events and prompt the VideoLLMs to perform temporal-aware video event understanding. During inference, in order to properly determine the output caption sequence from an arbitrary number of events presented within a video, we introduce an event coherent sampling strategy to select event captions with sufficient coherence across temporal events and cross-modal similarity with the given video. Through extensive experiments on benchmark datasets, we show that our TA-Prompting is favorable against state-of-the-art VideoLLMs, yielding superior performance on dense video captioning and temporal understanding tasks including moment retrieval and temporalQA.</p>
<h3 id="17-sketchthinker-r1-towards-efficient-sketch-style-reasoning-in-large-multimodal-models">[17] <a href="https://arxiv.org/abs/2601.02825">SketchThinker-R1: Towards Efficient Sketch-Style Reasoning in Large Multimodal Models</a></h3>
<p><em>Ruiyang Zhang, Dongzhan Zhou, Zhedong Zheng</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSketchThinker-R1æ–¹æ³•ï¼Œé€šè¿‡æ¿€åŠ±å¤§è¯­è¨€æ¨¡å‹é‡‡ç”¨è‰å›¾å¼æ¨ç†æ¥æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ï¼Œåœ¨ä¿æŒç­”æ¡ˆå‡†ç¡®æ€§çš„åŒæ—¶å®ç°äº†è¶…è¿‡64%çš„æ¨ç†ä»¤ç‰Œæˆæœ¬å‡å°‘ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§è¯­è¨€æ¨¡å‹å¹¿æ³›é‡‡ç”¨é€æ­¥æ¨ç†æ–¹æ³•ï¼Œè™½ç„¶ç»éªŒä¸Šæœ‰æ•ˆä½†å¯¼è‡´è®¡ç®—å¼€é”€æ˜¾è‘—å¢åŠ ï¼ŒåŒ…æ‹¬æ›´é«˜çš„ä»¤ç‰Œæˆæœ¬å’Œå“åº”æ—¶é—´ï¼Œä»è€ŒæŸå®³æ¨ç†æ•ˆç‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»å¸¸é‡‡ç”¨è‰å›¾å¼æ¨ç†â€”â€”ä¸€ç§ç®€æ´ã€ç›®æ ‡å¯¼å‘çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œèƒ½å¤Ÿä¼˜å…ˆå¤„ç†å…³é”®ä¿¡æ¯å¹¶å®ç°é«˜æ•ˆé—®é¢˜è§£å†³ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šè‰å›¾æ¨¡å¼å†·å¯åŠ¨é˜¶æ®µå°†æ ‡å‡†é•¿æ¨ç†è¿‡ç¨‹è½¬æ¢ä¸ºè‰å›¾å¼æ¨ç†å¹¶å¾®è°ƒåŸºç¡€å¤šæ¨¡æ€æ¨¡å‹ï¼›è®­ç»ƒSketchJudgeå¥–åŠ±æ¨¡å‹ï¼Œæ˜¾å¼è¯„ä¼°æ¨¡å‹æ€ç»´è¿‡ç¨‹å¹¶ä¸ºè‰å›¾å¼æ¨ç†åˆ†é…æ›´é«˜åˆ†æ•°ï¼›åœ¨SketchJudgeç›‘ç£ä¸‹è¿›è¡Œè‰å›¾æ€ç»´å¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥æ³›åŒ–è‰å›¾å¼æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒSketchThinker-R1å®ç°äº†è¶…è¿‡64%çš„æ¨ç†ä»¤ç‰Œæˆæœ¬å‡å°‘ï¼ŒåŒæ—¶ä¸æŸå®³æœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜è‰å›¾å¼æ¨ç†åœ¨é—®é¢˜è§£å†³è¿‡ç¨‹ä¸­æ›´ä¸“æ³¨äºå…³é”®çº¿ç´¢ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡æ¨¡ä»¿äººç±»è®¤çŸ¥æ•ˆç‡æ¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„å¯è¡Œæ€§ï¼Œè‰å›¾å¼æ¨ç†æ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ï¼Œä¸ºé«˜æ•ˆå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿè®¾è®¡æä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Despite the empirical success of extensive, step-by-step reasoning in large multimodal models, long reasoning processes inevitably incur substantial computational overhead, i.e., in terms of higher token costs and increased response time, which undermines inference efficiency. In contrast, humans often employ sketch-style reasoning: a concise, goal-directed cognitive process that prioritizes salient information and enables efficient problem-solving. Inspired by this cognitive efficiency, we propose SketchThinker-R1, which incentivizes sketch-style reasoning ability in large multimodal models. Our method consists of three primary stages. In the Sketch-Mode Cold Start stage, we convert standard long reasoning process into sketch-style reasoning and finetune base multimodal model, instilling initial sketch-style reasoning capability. Next, we train SketchJudge Reward Model, which explicitly evaluates thinking process of model and assigns higher scores to sketch-style reasoning. Finally, we conduct Sketch-Thinking Reinforcement Learning under supervision of SketchJudge to further generalize sketch-style reasoning ability. Experimental evaluation on four benchmarks reveals that our SketchThinker-R1 achieves over 64% reduction in reasoning token cost without compromising final answer accuracy. Qualitative analysis further shows that sketch-style reasoning focuses more on key cues during problem solving.</p>
<h3 id="18-dcg-reid-disentangling-collaboration-and-guidance-fusion-representations-for-multi-modal-vehicle-re-identification">[18] <a href="https://arxiv.org/abs/2601.02924">DCG ReID: Disentangling Collaboration and Guidance Fusion Representations for Multi-modal Vehicle Re-Identification</a></h3>
<p><em>Aihua Zheng, Ya Gao, Shihao Li, Chenglong Li, Jin Tang</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDCG-ReIDæ–¹æ³•ï¼Œé€šè¿‡è§£è€¦å¼‚æ„è´¨é‡åˆ†å¸ƒæ¨¡æ€æ•°æ®çš„èåˆéœ€æ±‚ï¼Œè®¾è®¡äº†åŠ¨æ€ç½®ä¿¡åº¦è§£è€¦åŠ æƒæœºåˆ¶å’Œä¸¤ç§åœºæ™¯ç‰¹å¼‚æ€§èåˆç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šæ¨¡æ€è½¦è¾†é‡è¯†åˆ«ä¸­å¹³è¡¡ä¸ä¸å¹³è¡¡è´¨é‡åˆ†å¸ƒæ•°æ®çš„å†²çªèåˆé—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€è½¦è¾†é‡è¯†åˆ«é¢ä¸´æ¨¡æ€è´¨é‡åˆ†å¸ƒä¸ç¡®å®šæ€§çš„æŒ‘æˆ˜ï¼Œç”±äºRGBã€è¿‘çº¢å¤–å’Œçƒ­çº¢å¤–æ¨¡æ€é—´çš„å›ºæœ‰å·®å¼‚ï¼Œå¯¼è‡´å¹³è¡¡ä¸ä¸å¹³è¡¡è´¨é‡åˆ†å¸ƒæ•°æ®å…·æœ‰ä¸åŒçš„å†²çªèåˆéœ€æ±‚ã€‚ç°æœ‰æ–¹æ³•å°†æ‰€æœ‰å¤šæ¨¡æ€æ•°æ®ç½®äºå•ä¸€èåˆæ¨¡å‹ä¸­å¤„ç†ï¼Œå¿½è§†äº†ä¸¤ç§æ•°æ®ç±»å‹çš„ä¸åŒéœ€æ±‚ï¼Œéš¾ä»¥è§£è€¦ç±»å†…ä¸€è‡´æ€§ä¸æ¨¡æ€é—´å¼‚è´¨æ€§ä¹‹é—´çš„å†²çªã€‚</p>
<p><strong>Method:</strong> æå‡ºDCG-ReIDæ–¹æ³•ï¼Œé¦–å…ˆè®¾è®¡åŠ¨æ€ç½®ä¿¡åº¦è§£è€¦åŠ æƒæœºåˆ¶ï¼Œé€šè¿‡äº¤äº’å¯¼å‡ºçš„æ¨¡æ€ç½®ä¿¡åº¦åŠ¨æ€é‡åŠ æƒä¸‰æ¨¡æ€è´¡çŒ®ï¼Œæ„å»ºè§£è€¦èåˆæ¡†æ¶ã€‚åœ¨æ­¤åŸºç¡€ä¸Šå¼€å‘ä¸¤ç§åœºæ™¯ç‰¹å¼‚æ€§èåˆç­–ç•¥ï¼šé’ˆå¯¹å¹³è¡¡è´¨é‡åˆ†å¸ƒï¼Œåä½œèåˆæ¨¡å—æŒ–æ˜æˆå¯¹å…±è¯†ç‰¹å¾ä»¥æ•è·å…±äº«åˆ¤åˆ«ä¿¡æ¯ï¼›é’ˆå¯¹ä¸å¹³è¡¡åˆ†å¸ƒï¼Œå¼•å¯¼èåˆæ¨¡å—å®æ–½æ¨¡æ€åˆ¤åˆ«å·®å¼‚çš„å·®å¼‚åŒ–æ”¾å¤§ï¼Œå¼ºåŒ–ä¸»å¯¼æ¨¡æ€ä¼˜åŠ¿å¹¶å¼•å¯¼è¾…åŠ©æ¨¡æ€æŒ–æ˜äº’è¡¥åˆ¤åˆ«ä¿¡æ¯ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªå¤šæ¨¡æ€é‡è¯†åˆ«åŸºå‡†æ•°æ®é›†ï¼ˆWMVeID863ã€MSVR310ã€RGBNT100ï¼‰ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€è½¦è¾†é‡è¯†åˆ«ä¸­çš„æ¨¡æ€è´¨é‡åˆ†å¸ƒä¸ç¡®å®šæ€§é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†è¯†åˆ«æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€è½¦è¾†é‡è¯†åˆ«ä¸­æ¨¡æ€è´¨é‡åˆ†å¸ƒå·®å¼‚å¯¹èåˆç­–ç•¥çš„é‡è¦å½±å“ï¼Œæå‡ºçš„è§£è€¦åä½œä¸å¼•å¯¼èåˆæ¡†æ¶ä¸ºè§£å†³å¹³è¡¡ä¸ä¸å¹³è¡¡è´¨é‡åˆ†å¸ƒæ•°æ®çš„å†²çªèåˆéœ€æ±‚æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä¸ºå¤šæ¨¡æ€è§†è§‰ä»»åŠ¡ä¸­çš„å¼‚æ„æ•°æ®èåˆæä¾›äº†æ–°çš„æ€è·¯ï¼Œå…·æœ‰æ‰©å±•åˆ°å…¶ä»–å¤šæ¨¡æ€è¯†åˆ«ä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Multi-modal vehicle Re-Identification (ReID) aims to leverage complementary information from RGB, Near Infrared (NIR), and Thermal Infrared (TIR) modalities to retrieve the same vehicle. The challenges of multi-modal vehicle ReID arise from the uncertainty of modality quality distribution induced by inherent discrepancies across modalities, resulting in distinct conflicting fusion requirements for data with balanced and unbalanced quality distributions. Existing methods handle all multi-modal data within a single fusion model, overlooking the different needs of the two data types and making it difficult to decouple the conflict between intra-class consistency and inter-modal heterogeneity. To this end, we propose Disentangle Collaboration and Guidance Fusion Representations for Multi-modal Vehicle ReID (DCG-ReID). Specifically, to disentangle heterogeneous quality-distributed modal data without mutual interference, we first design the Dynamic Confidence-based Disentangling Weighting (DCDW) mechanism: dynamically reweighting three-modal contributions via interaction-derived modal confidence to build a disentangled fusion framework. Building on DCDW, we develop two scenario-specific fusion strategies: (1) for balanced quality distributions, Collaboration Fusion Module (CFM) mines pairwise consensus features to capture shared discriminative information and boost intra-class consistency; (2) for unbalanced distributions, Guidance Fusion Module (GFM) implements differential amplification of modal discriminative disparities to reinforce dominant modality advantages, guide auxiliary modalities to mine complementary discriminative info, and mitigate inter-modal divergence to boost multi-modal joint decision performance. Extensive experiments on three multi-modal ReID benchmarks (WMVeID863, MSVR310, RGBNT100) validate the effectiveness of our method. Code will be released upon acceptance.</p>
<h3 id="19-dga-net-enhancing-sam-with-depth-prompting-and-graph-anchor-guidance-for-camouflaged-object-detection">[19] <a href="https://arxiv.org/abs/2601.02831">DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection</a></h3>
<p><em>Yuetong Li, Qing Zhang, Yilin Zhao, Gongyang Li, Zeming Liu</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDGA-Netï¼Œä¸€ç§é€šè¿‡æ–°é¢–çš„"æ·±åº¦æç¤º"èŒƒå¼é€‚é…Segment Anything Model (SAM)çš„ä¼ªè£…ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡è·¨æ¨¡æ€å›¾å¢å¼ºå’Œé”šç‚¹å¼•å¯¼ç»†åŒ–æ¨¡å—ï¼Œæœ‰æ•ˆåˆ©ç”¨æ·±åº¦çº¿ç´¢æå‡æ£€æµ‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ä¼ªè£…ç›®æ ‡æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–ç¨€ç–æç¤ºï¼ˆå¦‚ç‚¹æˆ–æ¡†ï¼‰ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨æ·±åº¦çº¿ç´¢ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³å¦‚ä½•æœ‰æ•ˆæ•´åˆRGBè¯­ä¹‰ä¿¡æ¯å’Œæ·±åº¦å‡ ä½•ä¿¡æ¯ï¼Œä»¥æ„å»ºå¯†é›†æ·±åº¦æç¤ºå¹¶ä¼ æ’­åˆ°æ•´ä¸ªç½‘ç»œï¼Œä»è€Œæå‡ä¼ªè£…ç›®æ ‡æ£€æµ‹çš„ç²¾åº¦å’Œä¸€è‡´æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºDGA-Netæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šè·¨æ¨¡æ€å›¾å¢å¼ºæ¨¡å—é€šè¿‡å¼‚æ„å›¾èåˆRGBè¯­ä¹‰å’Œæ·±åº¦å‡ ä½•ä¿¡æ¯ï¼Œç”Ÿæˆç»Ÿä¸€å¼•å¯¼ä¿¡å·ï¼›é”šç‚¹å¼•å¯¼ç»†åŒ–æ¨¡å—åˆ›å»ºå…¨å±€é”šç‚¹å¹¶å»ºç«‹éå±€éƒ¨è·¯å¾„ï¼Œå°†å¼•å¯¼ä¿¡å·ä»æ·±å±‚å¹¿æ’­åˆ°æµ…å±‚ï¼Œä»¥ç¼“è§£ç‰¹å¾å±‚æ¬¡ä¸­çš„ä¿¡æ¯è¡°å‡é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> å®šé‡å’Œå®šæ€§å®éªŒç»“æœè¡¨æ˜ï¼ŒDGA-Netåœ¨ä¼ªè£…ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†æ·±åº¦æç¤ºèŒƒå¼çš„æœ‰æ•ˆæ€§ä»¥åŠæ‰€ææ¨¡å—åœ¨æå‡åˆ†å‰²ç²¾åº¦å’Œä¸€è‡´æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é€šè¿‡å¯†é›†æ·±åº¦æç¤ºæœºåˆ¶æœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯çš„æ½œåŠ›ï¼Œä¸ºSAMåœ¨ç‰¹å®šè§†è§‰ä»»åŠ¡ä¸Šçš„é€‚é…æä¾›äº†æ–°æ€è·¯ã€‚æ‰€æå‡ºçš„å›¾å¢å¼ºå’Œé”šç‚¹å¼•å¯¼æ–¹æ³•ä¸ºè§£å†³ç‰¹å¾å±‚æ¬¡ä¿¡æ¯è¡°å‡é—®é¢˜æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¯¹å¤æ‚åœºæ™¯ä¸‹çš„ç›®æ ‡æ£€æµ‹å…·æœ‰é‡è¦å‚è€ƒä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting" paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.</p>
<h3 id="20-prismvau-prompt-refined-inference-system-for-multimodal-video-anomaly-understanding">[20] <a href="https://arxiv.org/abs/2601.02927">PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding</a></h3>
<p><em>IÃ±aki Erregue, Kamal Nasrollahi, Sergio Escalera</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºPrismVAUï¼Œä¸€ç§è½»é‡çº§å®æ—¶è§†é¢‘å¼‚å¸¸ç†è§£ç³»ç»Ÿï¼Œå®ƒåˆ©ç”¨å•ä¸ªç°æˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¼‚å¸¸è¯„åˆ†ã€è§£é‡Šå’Œæç¤ºä¼˜åŒ–ï¼Œæ— éœ€æŒ‡ä»¤å¾®è°ƒæˆ–å¤–éƒ¨æ¨¡å—å³å¯å®ç°ç«äº‰æ€§æ£€æµ‹æ€§èƒ½å’Œå¯è§£é‡Šçš„å¼‚å¸¸è§£é‡Šã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘å¼‚å¸¸ç†è§£æ–¹æ³•é€šå¸¸ä¾èµ–å¾®è°ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æˆ–å¤–éƒ¨æ¨¡å—å¦‚è§†é¢‘æè¿°å™¨ï¼Œè¿™äº›æ–¹æ³•å¼•å…¥äº†æ˜‚è´µçš„æ ‡æ³¨æˆæœ¬ã€å¤æ‚çš„è®­ç»ƒæµç¨‹å’Œé«˜æ¨ç†å¼€é”€ï¼Œå› æ­¤éœ€è¦ä¸€ç§è½»é‡çº§ä¸”é«˜æ•ˆçš„å®æ—¶è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> PrismVAUé‡‡ç”¨ä¸¤é˜¶æ®µäº’è¡¥æ¶æ„ï¼šç²—ç²’åº¦å¼‚å¸¸è¯„åˆ†æ¨¡å—é€šè¿‡æ–‡æœ¬é”šç‚¹çš„ç›¸ä¼¼æ€§è®¡ç®—å¸§çº§å¼‚å¸¸åˆ†æ•°ï¼Œä»¥åŠåŸºäºMLLMçš„ç»†åŒ–æ¨¡å—é€šè¿‡ç³»ç»Ÿå’Œç”¨æˆ·æç¤ºå¯¹å¼‚å¸¸è¿›è¡Œä¸Šä¸‹æ–‡ç†è§£ï¼›æ–‡æœ¬é”šç‚¹å’Œæç¤ºé€šè¿‡å¼±ç›‘ç£è‡ªåŠ¨æç¤ºå·¥ç¨‹æ¡†æ¶è¿›è¡Œä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†è§†é¢‘å¼‚å¸¸æ£€æµ‹åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPrismVAUåœ¨æ— éœ€æŒ‡ä»¤å¾®è°ƒã€å¸§çº§æ ‡æ³¨ã€å¤–éƒ¨æ¨¡å—æˆ–å¯†é›†å¤„ç†çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ç«äº‰æ€§çš„æ£€æµ‹æ€§èƒ½å’Œå¯è§£é‡Šçš„å¼‚å¸¸è§£é‡Šã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§é«˜æ•ˆå®ç”¨çš„è§†é¢‘å¼‚å¸¸ç†è§£è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å•ä¸ªç°æˆMLLMå®ç°äº†å®æ—¶æ€§èƒ½ï¼Œé™ä½äº†æ ‡æ³¨å’Œè®¡ç®—æˆæœ¬ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„è½»é‡çº§å¼‚å¸¸ç†è§£ç³»ç»Ÿè®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.</p>
<h3 id="21-lams-edit-latent-and-attention-mixing-with-schedulers-for-improved-content-preservation-in-diffusion-based-image-and-style-editing">[21] <a href="https://arxiv.org/abs/2601.02987">LAMS-Edit: Latent and Attention Mixing with Schedulers for Improved Content Preservation in Diffusion-Based Image and Style Editing</a></h3>
<p><em>Wingwa Fu, Takayuki Okatani</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºLAMS-Editæ¡†æ¶ï¼Œé€šè¿‡èåˆå»å™ªè¿‡ç¨‹ä¸­çš„æ½œåœ¨è¡¨ç¤ºå’Œæ³¨æ„åŠ›å›¾ï¼Œç»“åˆè°ƒåº¦å™¨æ§åˆ¶æ’å€¼æƒé‡ï¼Œæœ‰æ•ˆè§£å†³äº†æ–‡æœ¬åˆ°å›¾åƒç¼–è¾‘ä¸­å†…å®¹ä¿æŒä¸ç¼–è¾‘åº”ç”¨ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åˆ°å›¾åƒç¼–è¾‘é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯éš¾ä»¥åœ¨å†…å®¹ä¿æŒä¸ç¼–è¾‘åº”ç”¨ä¹‹é—´å–å¾—å¹³è¡¡ï¼ŒäºŒæ˜¯å¤„ç†çœŸå®å›¾åƒç¼–è¾‘æ—¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰æ–¹æ³•åœ¨ä¿æŒåŸå§‹å›¾åƒå†…å®¹çš„åŒæ—¶æœ‰æ•ˆåº”ç”¨ç¼–è¾‘æŒ‡ä»¤æ–¹é¢å­˜åœ¨å±€é™ï¼Œç‰¹åˆ«æ˜¯åœ¨çœŸå®å›¾åƒç¼–è¾‘åœºæ™¯ä¸­ã€‚</p>
<p><strong>Method:</strong> æå‡ºLAMS-Editæ¡†æ¶ï¼Œæ ¸å¿ƒæ˜¯Latent and Attention Mixing with SchedulersæŠ€æœ¯ï¼Œåˆ©ç”¨åè½¬è¿‡ç¨‹ä¸­çš„ä¸­é—´çŠ¶æ€è¿›è¡Œç¼–è¾‘å›¾åƒç”Ÿæˆã€‚å…·ä½“æ–¹æ³•æ˜¯åœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­ï¼Œé€šè¿‡åŠ æƒæ’å€¼èåˆåè½¬è¿‡ç¨‹å’Œç¼–è¾‘ç”Ÿæˆè¿‡ç¨‹çš„æ½œåœ¨è¡¨ç¤ºä¸æ³¨æ„åŠ›å›¾ï¼Œæƒé‡ç”±è°ƒåº¦å™¨æ§åˆ¶ã€‚è¯¥æ¡†æ¶ä¸Prompt-to-Prompté›†æˆï¼Œæ”¯æŒåŒºåŸŸæ©ç çš„ç²¾ç¡®ç¼–è¾‘ï¼Œå¹¶å¯é€šè¿‡LoRAå®ç°é£æ ¼è¿ç§»ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLAMS-Editåœ¨å†…å®¹ä¿æŒä¸ç¼–è¾‘åº”ç”¨ä¹‹é—´å–å¾—äº†æœ‰æ•ˆå¹³è¡¡ã€‚è¯¥æ–¹æ³•åœ¨çœŸå®å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿç²¾ç¡®åº”ç”¨ç¼–è¾‘æŒ‡ä»¤åŒæ—¶ä¿æŒåŸå§‹å›¾åƒçš„å…³é”®å†…å®¹ç‰¹å¾ï¼Œæ”¯æŒåŒºåŸŸæ©ç ç¼–è¾‘å’Œé£æ ¼è¿ç§»åŠŸèƒ½ã€‚</p>
<p><strong>Conclusion:</strong> LAMS-Editæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„ç¼–è¾‘æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨åè½¬è¿‡ç¨‹çš„ä¸­é—´çŠ¶æ€ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†æ–‡æœ¬åˆ°å›¾åƒç¼–è¾‘çš„è´¨é‡å’Œå¯æ§æ€§ã€‚è¯¥æ–¹æ³•ä¸ºå¹³è¡¡å†…å®¹ä¿æŒä¸ç¼–è¾‘åº”ç”¨æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºæœªæ¥åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘ç ”ç©¶æä¾›äº†æ–°çš„æŠ€æœ¯æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Text-to-Image editing using diffusion models faces challenges in balancing content preservation with edit application and handling real-image editing. To address these, we propose LAMS-Edit, leveraging intermediate states from the inversion process--an essential step in real-image editing--during edited image generation. Specifically, latent representations and attention maps from both processes are combined at each step using weighted interpolation, controlled by a scheduler. This technique, Latent and Attention Mixing with Schedulers (LAMS), integrates with Prompt-to-Prompt (P2P) to form LAMS-Edit--an extensible framework that supports precise editing with region masks and enables style transfer via LoRA. Extensive experiments demonstrate that LAMS-Edit effectively balances content preservation and edit application.</p>
<h3 id="22-towards-faithful-reasoning-in-comics-for-small-mllms">[22] <a href="https://arxiv.org/abs/2601.02991">Towards Faithful Reasoning in Comics for Small MLLMs</a></h3>
<p><em>Chengcheng Feng, Haojie Yin, Yucheng Jin, Kaizhu Huang</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¼«ç”»æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ€ç»´é“¾æç¤ºåœ¨æ¼«ç”»è§†è§‰é—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œé€šè¿‡æ¨¡å—åŒ–æ¨ç†ç”Ÿæˆå’Œå¼ºåŒ–å¾®è°ƒï¼Œä½¿å°å‹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æŠ½è±¡è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šå®ç°æ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ¼«ç”»è§†è§‰é—®ç­”ä»»åŠ¡å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æå‡ºäº†ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œæ¶‰åŠç¬¦å·æŠ½è±¡ã€å™äº‹é€»è¾‘å’Œå¹½é»˜ç†è§£ç­‰å¤æ‚æ¨ç†ã€‚ç ”ç©¶å‘ç°ä¼ ç»Ÿæ€ç»´é“¾æç¤ºåœ¨CVQAä»»åŠ¡ä¸­åè€Œä¼šé™ä½æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å°å‹æ¨¡å‹ä¸­ï¼Œä¸»è¦é—®é¢˜åŒ…æ‹¬çŠ¶æ€çº ç¼ ã€è™šå‡è½¬ç§»å’Œæ¢ç´¢æ•ˆç‡ä½ä¸‹ï¼Œè¿™äº›ç¼ºé™·åœ¨èµ„æºå—é™çš„å°å‹æ¨¡å‹ä¸­å°¤ä¸ºçªå‡ºã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¼«ç”»æ¨ç†æ¡†æ¶ï¼Œç»“åˆäº†æ¨¡å—åŒ–æ€ç»´é“¾ç”Ÿæˆä¸åŸºäºGRPOçš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œå¹¶è®¾è®¡äº†æ–°å‹ç»“æ„åŒ–å¥–åŠ±æœºåˆ¶ã€‚è¯¥æ¡†æ¶ä¸“é—¨é’ˆå¯¹å°å‹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–ï¼Œæ—¨åœ¨ç”Ÿæˆæ›´å¿ å®ä¸”å¯è¿ç§»çš„æ¨ç†é“¾ï¼ŒåŒæ—¶å°†æ–¹æ³•æ‰©å±•åˆ°æ›´å¹¿æ³›çš„å¹½é»˜ä¸­å¿ƒå’ŒæŠ½è±¡è§†è§‰æ¨ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬è¡¨æƒ…åŒ…ç†è§£å’Œç¤¾è®ºæ¼«ç”»è§£è¯»ã€‚</p>
<p><strong>Result:</strong> åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œæå‡ºçš„3Bå‚æ•°æ¨¡å‹è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œæ’ä»¶å®éªŒåœ¨ä¸åŒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†å¹³å‡12.1%çš„é¢å¤–æ€§èƒ½æå‡ã€‚è¯¥æ–¹æ³•ä¸ä»…åœ¨æ¼«ç”»VQAä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨æ›´å¹¿æ³›çš„å¹½é»˜ä¸­å¿ƒå’ŒæŠ½è±¡è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šä¹Ÿå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†ä¼ ç»Ÿæ€ç»´é“¾æç¤ºåœ¨å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹å°å‹æ¨¡å‹çš„è´Ÿé¢å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æ‰€æå‡ºçš„æ¼«ç”»æ¨ç†æ¡†æ¶ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„æŠ½è±¡è§†è§‰ç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¯¹å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿçš„ä¼˜åŒ–è®¾è®¡å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Comic-based visual question answering (CVQA) poses distinct challenges to multimodal large language models (MLLMs) due to its reliance on symbolic abstraction, narrative logic, and humor, which differ from conventional VQA tasks. Although Chain-of-Thought (CoT) prompting is widely used to enhance MLLM reasoning, surprisingly, its direct application to CVQA often degrades performance, especially in small-scale models. Our theoretical and empirical analyses reveal that standard CoT in CVQA suffers from state entanglement, spurious transitions, and exploration inefficiency, with small models particularly vulnerable in resource-constrained settings. To address these issues, we propose a novel comic reasoning framework, designed to produce more faithful and transferable reasoning chains in small MLLMs. Specifically, our framework combines modular CoT generation with GRPO-based reinforcement fine-tuning and a novel structured reward. Beyond comic VQA, we further evaluate our approach on a broader class of humor-centric and abstract visual reasoning tasks, including meme understanding and editorial cartoon interpretation. Across five challenging benchmarks, our 3B model outperforms state-of-the-art methods, and plug-in experiments yield an additional average improvement of $\mathbf{12.1\%}$ across different MLLMs.</p>
<h3 id="23-ibisagent-reinforcing-pixel-level-visual-reasoning-in-mllms-for-universal-biomedical-object-referring-and-segmentation">[23] <a href="https://arxiv.org/abs/2601.03054">IBISAgent: Reinforcing Pixel-Level Visual Reasoning in MLLMs for Universal Biomedical Object Referring and Segmentation</a></h3>
<p><em>Yankai Jiang, Qiaoru Li, Binlu Xu, Haoran Sun, Chao Ding, Junting Dong, Yuxiang Cai, Xuhong Zhang, Jianwei Yin</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºIBISAgentçš„æ–°å‹ä»£ç†å¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå°†åŒ»å­¦å›¾åƒåˆ†å‰²é‡æ–°å®šä¹‰ä¸ºè§†è§‰ä¸­å¿ƒçš„å¤šæ­¥å†³ç­–è¿‡ç¨‹ï¼Œé€šè¿‡è¿­ä»£æ¨ç†å’Œæ–‡æœ¬ç‚¹å‡»åŠ¨ä½œç”Ÿæˆé«˜è´¨é‡åˆ†å‰²æ©ç ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŒ»å­¦å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åƒç´ çº§ç†è§£æ–¹é¢é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯å¼•å…¥éšå¼åˆ†å‰²æ ‡è®°å¹¶éœ€è¦åŒæ—¶å¾®è°ƒMLLMå’Œå¤–éƒ¨åƒç´ è§£ç å™¨ï¼Œå¢åŠ äº†ç¾éš¾æ€§é—å¿˜é£é™©å¹¶é™åˆ¶äº†æ³›åŒ–èƒ½åŠ›ï¼›äºŒæ˜¯å¤§å¤šæ•°æ–¹æ³•ä¾èµ–å•æ¬¡æ¨ç†ï¼Œç¼ºä¹è¿­ä»£ä¼˜åŒ–åˆ†å‰²ç»“æœçš„èƒ½åŠ›ï¼Œå¯¼è‡´æ€§èƒ½æ¬ ä½³ã€‚</p>
<p><strong>Method:</strong> IBISAgentå°†åˆ†å‰²é‡æ–°å®šä¹‰ä¸ºè§†è§‰ä¸­å¿ƒçš„å¤šæ­¥å†³ç­–è¿‡ç¨‹ï¼Œä½¿MLLMèƒ½å¤Ÿç”Ÿæˆäº¤é”™æ¨ç†å’ŒåŸºäºæ–‡æœ¬çš„ç‚¹å‡»åŠ¨ä½œï¼Œè°ƒç”¨åˆ†å‰²å·¥å…·å¹¶äº§ç”Ÿé«˜è´¨é‡æ©ç è€Œæ— éœ€æ¶æ„ä¿®æ”¹ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬å†·å¯åŠ¨ç›‘ç£å¾®è°ƒå’Œå…·æœ‰å®šåˆ¶ç»†ç²’åº¦å¥–åŠ±çš„ä»£ç†å¼ºåŒ–å­¦ä¹ ï¼Œå¢å¼ºæ¨¡å‹åœ¨å¤æ‚åŒ»å­¦æŒ‡ä»£å’Œæ¨ç†åˆ†å‰²ä»»åŠ¡ä¸­çš„é²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒIBISAgentåœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºé—­æºå’Œå¼€æºçš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨å¤æ‚åŒ»å­¦æŒ‡ä»£å’Œæ¨ç†åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ï¼ŒéªŒè¯äº†å…¶å¤šæ­¥è¿­ä»£æ¨ç†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å°†åˆ†å‰²é‡æ–°å®šä¹‰ä¸ºå¤šæ­¥å†³ç­–è¿‡ç¨‹çš„å¯è¡Œæ€§ï¼Œé€šè¿‡ä»£ç†å¼MLLMå®ç°æ— éœ€æ¶æ„ä¿®æ”¹çš„é«˜è´¨é‡åˆ†å‰²ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†ææä¾›äº†æ–°çš„åƒç´ çº§è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸ºå¤æ‚åŒ»ç–—åˆ†å‰²ä»»åŠ¡ä¸­çš„è¿­ä»£ä¼˜åŒ–æä¾›äº†æœ‰æ•ˆæ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Recent research on medical MLLMs has gradually shifted its focus from image-level understanding to fine-grained, pixel-level comprehension. Although segmentation serves as the foundation for pixel-level understanding, existing approaches face two major challenges. First, they introduce implicit segmentation tokens and require simultaneous fine-tuning of both the MLLM and external pixel decoders, which increases the risk of catastrophic forgetting and limits generalization to out-of-domain scenarios. Second, most methods rely on single-pass reasoning and lack the capability to iteratively refine segmentation results, leading to suboptimal performance. To overcome these limitations, we propose a novel agentic MLLM, named IBISAgent, that reformulates segmentation as a vision-centric, multi-step decision-making process. IBISAgent enables MLLMs to generate interleaved reasoning and text-based click actions, invoke segmentation tools, and produce high-quality masks without architectural modifications. By iteratively performing multi-step visual reasoning on masked image features, IBISAgent naturally supports mask refinement and promotes the development of pixel-level visual reasoning capabilities. We further design a two-stage training framework consisting of cold-start supervised fine-tuning and agentic reinforcement learning with tailored, fine-grained rewards, enhancing the model's robustness in complex medical referring and reasoning segmentation tasks. Extensive experiments demonstrate that IBISAgent consistently outperforms both closed-source and open-source SOTA methods. All datasets, code, and trained models will be released publicly.</p>
<h3 id="24-text-guided-layer-fusion-mitigates-hallucination-in-multimodal-llms">[24] <a href="https://arxiv.org/abs/2601.03100">Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs</a></h3>
<p><em>Chenchen Lin, Sanbao Su, Rachel Luo, Yuxiao Chen, Yan Wang, Marco Pavone, Fei Miao</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†TGIFï¼ˆæ–‡æœ¬å¼•å¯¼çš„å±‚é—´èåˆï¼‰ï¼Œä¸€ç§è½»é‡çº§æ¨¡å—ï¼Œé€šè¿‡é¢„æµ‹ä¸æŸ¥è¯¢ç›¸å…³çš„è§†è§‰ç‰¹å¾èåˆæ¥å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰åŸºç¡€èƒ½åŠ›ï¼Œæœ‰æ•ˆå‡å°‘å¹»è§‰ç°è±¡ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€šå¸¸ä»…ä½¿ç”¨å†»ç»“è§†è§‰ç¼–ç å™¨çš„å•ä¸€æ·±å±‚ç‰¹å¾ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ç¼–ç å™¨ä¸°å¯Œçš„è§†è§‰å±‚æ¬¡ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹å®¹æ˜“äº§ç”Ÿè§†è§‰æœªåŸºç¡€çš„å¹»è§‰ï¼Œè¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒè€Œéå›¾åƒè¯æ®ã€‚ç°æœ‰å¤šå±‚èåˆæ–¹æ³•è™½éƒ¨åˆ†è§£å†³äº†è¿™ä¸€é™åˆ¶ï¼Œä½†ä»æ˜¯é™æ€çš„ï¼Œæ— è®ºæŸ¥è¯¢å†…å®¹å¦‚ä½•éƒ½åº”ç”¨ç›¸åŒçš„å±‚æ··åˆç­–ç•¥ã€‚</p>
<p><strong>Method:</strong> TGIFå°†è§†è§‰ç¼–ç å™¨çš„å„å±‚è§†ä¸ºæ·±åº¦æ–¹å‘çš„"ä¸“å®¶"ï¼Œé€šè¿‡è½»é‡çº§æ¨¡å—é¢„æµ‹ä¸æ–‡æœ¬æç¤ºç›¸å…³çš„è§†è§‰ç‰¹å¾èåˆã€‚è¯¥æ–¹æ³•éµå¾ªç›´æ¥å¤–éƒ¨èåˆåŸåˆ™ï¼Œæ— éœ€æ›´æ–°è§†è§‰ç¼–ç å™¨å‚æ•°ï¼Œä»…å¢åŠ æå°çš„è®¡ç®—å¼€é”€ã€‚TGIFè¢«é›†æˆåˆ°LLaVA-1.5-7Bæ¨¡å‹ä¸­ï¼Œå®ç°äº†æŸ¥è¯¢æ¡ä»¶åŒ–çš„å±‚æ¬¡æ„ŸçŸ¥ç‰¹å¾èåˆã€‚</p>
<p><strong>Result:</strong> åœ¨LLaVA-1.5-7Bä¸­é›†æˆTGIFåï¼Œåœ¨å¹»è§‰æ£€æµ‹ã€OCRè¯†åˆ«å’Œè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å‡è·å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶åœ¨ScienceQAã€GQAå’ŒMMBenchç­‰åŸºå‡†ä¸Šä¿æŒæˆ–æ”¹è¿›äº†åŸæœ‰æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå¢å¼ºè§†è§‰åŸºç¡€èƒ½åŠ›å¹¶å‡å°‘å¹»è§‰ç°è±¡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼ŒæŸ¥è¯¢æ¡ä»¶åŒ–çš„å±‚æ¬¡æ„ŸçŸ¥èåˆæ˜¯å¢å¼ºç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è§†è§‰åŸºç¡€èƒ½åŠ›å’Œå‡å°‘å¹»è§‰çš„æœ‰æ•ˆé€”å¾„ã€‚è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§æ¨¡å—å……åˆ†åˆ©ç”¨è§†è§‰ç¼–ç å™¨çš„å±‚æ¬¡ä¿¡æ¯ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹è®¾è®¡æä¾›äº†æ–°çš„æ–¹å‘ï¼Œè¡¨æ˜åŠ¨æ€ç‰¹å¾é€‰æ‹©æ¯”é™æ€èåˆç­–ç•¥æ›´å…·ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder's rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than image evidence. While many prior mitigation strategies operate on the text side, they leave the visual representation unchanged and do not exploit the rich hierarchy of features encoded across vision layers. Existing multi-layer fusion methods partially address this limitation but remain static, applying the same layer mixture regardless of the query. In this work, we introduce TGIF (Text-Guided Inter-layer Fusion), a lightweight module that treats encoder layers as depth-wise "experts" and predicts a prompt-dependent fusion of visual features. TGIF follows the principle of direct external fusion, requires no vision-encoder updates, and adds minimal overhead. Integrated into LLaVA-1.5-7B, TGIF provides consistent improvements across hallucination, OCR, and VQA benchmarks, while preserving or improving performance on ScienceQA, GQA, and MMBench. These results suggest that query-conditioned, hierarchy-aware fusion is an effective way to strengthen visual grounding and reduce hallucination in modern MLLMs.</p>
<h3 id="25-unified-thinker-a-general-reasoning-modular-core-for-image-generation">[25] <a href="https://arxiv.org/abs/2601.03127">Unified Thinker: A General Reasoning Modular Core for Image Generation</a></h3>
<p><em>Sashuai Zhou, Qiang Zhou, Jijin Hu, Hanqing Yang, Yue Cao, Junpeng Ma, Yinchao Ma, Jun Song, Tiezheng Ge, Cheng Yu, Bo Zheng, Zhou Zhao</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºUnified Thinkerï¼Œä¸€ç§ä»»åŠ¡æ— å…³çš„æ¨ç†æ¶æ„ï¼Œé€šè¿‡å°†ä¸“ç”¨æ¨ç†æ¨¡å—ä¸å›¾åƒç”Ÿæˆå™¨è§£è€¦ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ¨¡å‹åœ¨é€»è¾‘å¯†é›†å‹æŒ‡ä»¤è·Ÿéšæ–¹é¢çš„èƒ½åŠ›ï¼Œå¼¥åˆäº†æ¨ç†ä¸æ‰§è¡Œä¹‹é—´çš„å·®è·ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡é«˜ä¿çœŸå›¾åƒåˆæˆå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç”Ÿæˆæ¨¡å‹åœ¨é€»è¾‘å¯†é›†å‹æŒ‡ä»¤è·Ÿéšæ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œæš´éœ²å‡ºæŒç»­çš„æ¨ç†-æ‰§è¡Œå·®è·ã€‚åŒæ—¶ï¼Œé—­æºç³»ç»Ÿåœ¨æ¨ç†é©±åŠ¨çš„å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œçªæ˜¾äº†å½“å‰å¼€æºæ¨¡å‹çš„æ˜æ˜¾ä¸è¶³ã€‚æœ¬æ–‡è®¤ä¸ºå¼¥åˆè¿™ä¸€å·®è·ä¸ä»…éœ€è¦æ›´å¥½çš„è§†è§‰ç”Ÿæˆå™¨ï¼Œæ›´éœ€è¦å¯æ‰§è¡Œçš„æ¨ç†èƒ½åŠ›ï¼Œå³å°†é«˜çº§æ„å›¾åˆ†è§£ä¸ºå¯ç›´æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹çš„å…·ä½“ã€å¯éªŒè¯è®¡åˆ’ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºUnified Thinkerï¼Œä¸€ç§ä»»åŠ¡æ— å…³çš„é€šç”¨å›¾åƒç”Ÿæˆæ¨ç†æ¶æ„ï¼Œè®¾è®¡ä¸ºç»Ÿä¸€çš„è§„åˆ’æ ¸å¿ƒï¼Œå¯æ’å…¥ä¸åŒçš„ç”Ÿæˆå™¨å’Œå·¥ä½œæµç¨‹ã€‚è¯¥æ¶æ„å°†ä¸“ç”¨æ¨ç†æ¨¡å—ä¸å›¾åƒç”Ÿæˆå™¨è§£è€¦ï¼Œå®ç°æ¨ç†èƒ½åŠ›çš„æ¨¡å—åŒ–å‡çº§è€Œæ— éœ€é‡æ–°è®­ç»ƒæ•´ä¸ªç”Ÿæˆæ¨¡å‹ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼šé¦–å…ˆä¸ºæ¨ç†æ¨¡å—æ„å»ºç»“æ„åŒ–è§„åˆ’æ¥å£ï¼Œç„¶ååº”ç”¨å¼ºåŒ–å­¦ä¹ å°†å…¶ç­–ç•¥åŸºäºåƒç´ çº§åé¦ˆè¿›è¡Œä¼˜åŒ–ï¼Œé¼“åŠ±ç”Ÿæˆä¼˜åŒ–è§†è§‰æ­£ç¡®æ€§è€Œéæ–‡æœ¬åˆç†æ€§çš„è®¡åˆ’ã€‚</p>
<p><strong>Result:</strong> åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒUnified Thinkeræ˜¾è‘—æå‡äº†å›¾åƒæ¨ç†å’Œç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨é€»è¾‘å¯†é›†å‹æŒ‡ä»¤è·Ÿéšæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæœ‰æ•ˆå¼¥åˆäº†å¼€æºæ¨¡å‹ä¸é—­æºç³»ç»Ÿåœ¨æ¨ç†é©±åŠ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„æ€§èƒ½å·®è·ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å°†æ¨ç†ä¸ç”Ÿæˆè¿‡ç¨‹è§£è€¦å¹¶é‡‡ç”¨åŸºäºåƒç´ åé¦ˆçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå¯ä»¥æ˜¾è‘—æå‡ç”Ÿæˆæ¨¡å‹åœ¨å¤æ‚é€»è¾‘ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¿™ç§æ¨¡å—åŒ–æ¶æ„ä¸ºæœªæ¥ç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å‡çº§æä¾›äº†çµæ´»æ¡†æ¶ï¼Œå¼ºè°ƒäº†å¯æ‰§è¡Œæ¨ç†åœ¨å¼¥åˆæ¨ç†-æ‰§è¡Œå·®è·ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.</p>
<h3 id="26-reccur-a-recursive-corner-case-curation-framework-for-robust-vision-language-understanding-in-open-and-edge-scenarios">[26] <a href="https://arxiv.org/abs/2601.03011">ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios</a></h3>
<p><em>Yihan Wei, Shenghai Yuan, Tianchen Deng, Boyang Lou, Enwen Hu</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºReCCuræ¡†æ¶ï¼Œä¸€ç§ä½è®¡ç®—æˆæœ¬çš„é€’å½’è§’ç‚¹æ¡ˆä¾‹æ ‡æ³¨æ–¹æ³•ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“æµæ°´çº¿å°†å™ªå£°ç½‘ç»œå›¾åƒè½¬åŒ–ä¸ºå¯å®¡è®¡çš„ç»†ç²’åº¦æ ‡ç­¾ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„ä¸‹æ¸¸è®­ç»ƒå’Œè¯„ä¼°æä¾›å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§’ç‚¹æ¡ˆä¾‹æ˜¯é©±åŠ¨ç°å®ä¸–ç•Œæ•…éšœçš„ç½•è§æˆ–æç«¯åœºæ™¯ï¼Œä½†éš¾ä»¥å¤§è§„æ¨¡æ ‡æ³¨ï¼šç½‘ç»œæ•°æ®å™ªå£°å¤§ã€æ ‡ç­¾è„†å¼±ï¼Œä¸”è¾¹ç¼˜éƒ¨ç½²ç¯å¢ƒé™åˆ¶äº†å¤§è§„æ¨¡é‡æ–°è®­ç»ƒã€‚ç°æœ‰æ–¹æ³•é¢ä¸´æ•°æ®è·å–å›°éš¾ã€æ ‡æ³¨è´¨é‡ä¸ç¨³å®šä»¥åŠè®¡ç®—èµ„æºéœ€æ±‚é«˜ç­‰æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> ReCCuræ¡†æ¶é‡‡ç”¨ä¸‰çº§é€’å½’æµæ°´çº¿ï¼šé¦–å…ˆé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡æ•°æ®é‡‡é›†å’Œè¿‡æ»¤ï¼Œæ‰©å±•é¢†åŸŸè¯æ±‡å¹¶æ‰§è¡Œä¸‰æ¨¡æ€ä¸€è‡´æ€§æ£€æŸ¥ï¼›å…¶æ¬¡é‡‡ç”¨ä¸“å®¶æ··åˆçŸ¥è¯†è’¸é¦ï¼Œåˆ©ç”¨äº’è¡¥ç¼–ç å™¨è¿›è¡ŒkNNæŠ•ç¥¨å’Œä¸ç¡®å®šæ€§é‡‡æ ·ï¼›æœ€åé€šè¿‡åŒºåŸŸè¯æ®VLMå¯¹æŠ—æ ‡æ³¨ï¼Œç»“åˆæè®®å™¨å’ŒéªŒè¯å™¨ç”Ÿæˆå¯è§£é‡Šæ ‡ç­¾ã€‚</p>
<p><strong>Result:</strong> åœ¨çœŸå®è§’ç‚¹æ¡ˆä¾‹åœºæ™¯ï¼ˆå¦‚æ´ªæ°´è½¦è¾†æ£€æµ‹ï¼‰ä¸­ï¼ŒReCCuråœ¨æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œï¼ŒæŒç»­æå‡æ•°æ®çº¯åº¦å’Œå¯åˆ†ç¦»æ€§ï¼ŒåŒæ—¶ä»…éœ€æœ€å°åŒ–äººå·¥ç›‘ç£ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å™ªå£°ç½‘ç»œå›¾åƒï¼Œç”Ÿæˆé«˜è´¨é‡å¯å®¡è®¡æ ‡ç­¾ã€‚</p>
<p><strong>Conclusion:</strong> ReCCurä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„è§’ç‚¹æ¡ˆä¾‹æ•°æ®æ ‡æ³¨æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡é€’å½’å¤šæ™ºèƒ½ä½“æ¡†æ¶æ˜¾è‘—é™ä½äººå·¥ç›‘ç£éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡æ ‡æ³¨ã€‚è¯¥æ¡†æ¶ä¸ºä¸‹æ¸¸è®­ç»ƒå’Œè¯„ä¼°æä¾›äº†å¯é æ•°æ®åŸºç¡€ï¼Œå¹¶å°†åœ¨å®é™…éƒ¨ç½²ä¸­é‡Šæ”¾ä»£ç å’Œæ•°æ®é›†ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Corner cases are rare or extreme scenarios that drive real-world failures, but they are difficult to curate at scale: web data are noisy, labels are brittle, and edge deployments preclude large retraining. We present ReCCur (Recursive Corner-Case Curation), a low-compute framework that converts noisy web imagery into auditable fine-grained labels via a multi-agent recursive pipeline. First, large-scale data acquisition and filtering expands a domain vocabulary with a vision-language model (VLM), crawls the web, and enforces tri-modal (image, description, keyword) consistency with light human spot checks to yield refined candidates. Next, mixture-of-experts knowledge distillation uses complementary encoders (e.g., CLIP, DINOv2, BEiT) for kNN voting with dual-confidence activation and uncertainty sampling, converging to a high-precision set. Finally, region-evidence VLM adversarial labeling pairs a proposer (multi-granularity regions and semantic cues) with a validator (global and local chained consistency) to produce explainable labels and close the loop. On realistic corner-case scenarios (e.g., flooded-car inspection), ReCCur runs on consumer-grade GPUs, steadily improves purity and separability, and requires minimal human supervision, providing a practical substrate for downstream training and evaluation under resource constraints. Code and dataset will be released.</p>
<h3 id="27-anatomix-an-anatomy-aware-grounded-multimodal-large-language-model-for-chest-x-ray-interpretation">[27] <a href="https://arxiv.org/abs/2601.03191">AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation</a></h3>
<p><em>Anees Ur Rehman Hashmi, Numan Saeed, Christoph Lippert</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºAnatomiXï¼Œä¸€ç§ä¸“ä¸ºèƒ¸éƒ¨Xå…‰è§£å‰–å­¦åŸºç¡€è§£é‡Šè®¾è®¡çš„å¤šä»»åŠ¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•æ˜¾è‘—æå‡è§£å‰–å­¦æ¨ç†èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æå‡è¶…è¿‡25%ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€åŒ»å­¦å¤§è¯­è¨€æ¨¡å‹åœ¨èƒ¸éƒ¨Xå…‰è§£è¯»ä¸­è™½å–å¾—è¿›å±•ï¼Œä½†åœ¨ç©ºé—´æ¨ç†å’Œè§£å‰–å­¦ç†è§£æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰åŸºç¡€æŠ€æœ¯å¸¸æ— æ³•å»ºç«‹çœŸæ­£çš„è§£å‰–å­¦å¯¹åº”å…³ç³»ï¼Œå¯¼è‡´åŒ»å­¦é¢†åŸŸè§£å‰–å­¦ç†è§£é”™è¯¯ï¼Œéœ€è¦è§£å†³è¿™ä¸€å…³é”®å·®è·ã€‚</p>
<p><strong>Method:</strong> AnatomiXé‡‡ç”¨å—æ”¾å°„å­¦å·¥ä½œæµç¨‹å¯å‘çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼šé¦–å…ˆè¯†åˆ«è§£å‰–ç»“æ„å¹¶æå–å…¶ç‰¹å¾ï¼Œç„¶ååˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æ‰§è¡Œå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬çŸ­è¯­å®šä½ã€æŠ¥å‘Šç”Ÿæˆã€è§†è§‰é—®ç­”å’Œå›¾åƒç†è§£ï¼Œå®ç°è§£å‰–å­¦åŸºç¡€çš„å¤šä»»åŠ¡å¤„ç†ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAnatomiXå®ç°äº†å“è¶Šçš„è§£å‰–å­¦æ¨ç†èƒ½åŠ›ï¼Œåœ¨è§£å‰–å­¦åŸºç¡€ã€çŸ­è¯­å®šä½ã€åŸºç¡€è¯Šæ–­å’ŒåŸºç¡€æè¿°ä»»åŠ¡ä¸Šç›¸æ¯”ç°æœ‰æ–¹æ³•æ€§èƒ½æå‡è¶…è¿‡25%ï¼Œä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å¼€æºã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡è§£å‰–å­¦åŸºç¡€çš„å¤šä»»åŠ¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è®¾è®¡ï¼Œæ˜¾è‘—æå‡äº†åŒ»å­¦å›¾åƒè§£é‡Šçš„è§£å‰–å­¦å‡†ç¡®æ€§ï¼Œä¸ºåŒ»å­¦AIç³»ç»Ÿæä¾›äº†æ›´å¯é çš„è§£å‰–å­¦æ¨ç†æ¡†æ¶ï¼Œæ¨åŠ¨äº†åŒ»å­¦å›¾åƒç†è§£å‘æ›´ç²¾ç¡®çš„è§£å‰–å­¦å¯¹åº”æ–¹å‘å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at https://github.com/aneesurhashmi/anatomix</p>
<h3 id="28-understanding-multi-agent-reasoning-with-large-language-models-for-cartoon-vqa">[28] <a href="https://arxiv.org/abs/2601.03073">Understanding Multi-Agent Reasoning with Large Language Models for Cartoon VQA</a></h3>
<p><em>Tong Wu, Thanet Markchom</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¡é€šå›¾åƒè§†è§‰é—®ç­”çš„å¤šæ™ºèƒ½ä½“LLMæ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªä¸“é—¨åŒ–æ™ºèƒ½ä½“çš„ååŒå·¥ä½œæ¥è§£å†³å¡é€šå›¾åƒä¸­è§†è§‰æŠ½è±¡å’Œå™äº‹ä¸Šä¸‹æ–‡å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¹¶åœ¨Pororoå’ŒSimpsonsæ•°æ®é›†ä¸Šè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¡é€šå›¾åƒçš„è§†è§‰é—®ç­”é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤¸å¼ çš„è§†è§‰æŠ½è±¡å’Œå™äº‹é©±åŠ¨çš„ä¸Šä¸‹æ–‡ç†è§£ï¼Œè¿™äº›æŒ‘æˆ˜æ— æ³•è¢«åŸºäºè‡ªç„¶å›¾åƒè®­ç»ƒçš„æ ‡å‡†å¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ•ˆå¤„ç†ï¼Œå› æ­¤éœ€è¦ä¸“é—¨çš„æ–¹æ³•æ¥è§£å†³å¡é€šVQAä¸­çš„è¿™äº›å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“LLMæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªä¸“é—¨åŒ–æ™ºèƒ½ä½“ï¼šè§†è§‰æ™ºèƒ½ä½“è´Ÿè´£å¤„ç†è§†è§‰æŠ½è±¡ç‰¹å¾ï¼Œè¯­è¨€æ™ºèƒ½ä½“å¤„ç†æ–‡æœ¬ä¿¡æ¯ï¼Œæ‰¹è¯„æ™ºèƒ½ä½“è¿›è¡Œç»¼åˆæ¨ç†ï¼Œè¿™ä¸‰ä¸ªæ™ºèƒ½ä½“é€šè¿‡åä½œé›†æˆè§†è§‰çº¿ç´¢å’Œå™äº‹ä¸Šä¸‹æ–‡æ¥æ”¯æŒç»“æ„åŒ–æ¨ç†ã€‚</p>
<p><strong>Result:</strong> è¯¥æ¡†æ¶åœ¨Pororoå’ŒSimpsonsä¸¤ä¸ªå¡é€šVQAæ•°æ®é›†ä¸Šè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå®éªŒç»“æœè¯¦ç»†åˆ†æäº†æ¯ä¸ªæ™ºèƒ½ä½“å¯¹æœ€ç»ˆé¢„æµ‹çš„è´¡çŒ®ï¼Œæä¾›äº†å¯¹LLMåŸºå¤šæ™ºèƒ½ä½“åœ¨å¡é€šVQAå’Œå¤šæ¨¡æ€æ¨ç†ä¸­è¡Œä¸ºçš„æ·±å…¥ç†è§£ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å±•ç¤ºäº†å¤šæ™ºèƒ½ä½“æ¡†æ¶åœ¨è§£å†³å¡é€šå›¾åƒç‹¬ç‰¹æŒ‘æˆ˜æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºç†è§£LLMåŸºå¤šæ™ºèƒ½ä½“åœ¨å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„è¡Œä¸ºæä¾›äº†é‡è¦è§è§£ï¼Œå¹¶ä¸ºæœªæ¥é’ˆå¯¹ç‰¹å®šé¢†åŸŸè§†è§‰æŠ½è±¡çš„å¤šæ¨¡æ€AIç³»ç»Ÿè®¾è®¡æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>Visual Question Answering (VQA) for stylised cartoon imagery presents challenges, such as interpreting exaggerated visual abstraction and narrative-driven context, which are not adequately addressed by standard large language models (LLMs) trained on natural images. To investigate this issue, a multi-agent LLM framework is introduced, specifically designed for VQA tasks in cartoon imagery. The proposed architecture consists of three specialised agents: visual agent, language agent and critic agent, which work collaboratively to support structured reasoning by integrating visual cues and narrative context. The framework was systematically evaluated on two cartoon-based VQA datasets: Pororo and Simpsons. Experimental results provide a detailed analysis of how each agent contributes to the final prediction, offering a deeper understanding of LLM-based multi-agent behaviour in cartoon VQA and multimodal inference.</p>
<h3 id="29-unicorn-towards-self-improving-unified-multimodal-models-through-self-generated-supervision">[29] <a href="https://arxiv.org/abs/2601.03193">UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision</a></h3>
<p><em>Ruiyan Han, Zhen Fang, XinYu Sun, Yuchen Ma, Ziheng Wang, Yu Zeng, Zehui Chen, Lin Chen, Wenxuan Huang, Wei-Jie Xu, Yi Cao, Feng Zhao</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºUniCornæ¡†æ¶ï¼Œé€šè¿‡å°†ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åˆ’åˆ†ä¸ºä¸‰ä¸ªåä½œè§’è‰²ï¼ˆæè®®è€…ã€æ±‚è§£è€…å’Œè¯„åˆ¤è€…ï¼‰å¹¶è¿›è¡Œè‡ªåšå¼ˆï¼Œè§£å†³äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„ä¼ å¯¼æ€§å¤±è¯­é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨è·¨æ¨¡æ€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨åˆ©ç”¨å†…éƒ¨çŸ¥è¯†è¿›è¡Œé«˜è´¨é‡ç”Ÿæˆæ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œè¿™ç§ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ä¹‹é—´çš„ä¸ä¸€è‡´è¢«å½¢å¼åŒ–ä¸ºä¼ å¯¼æ€§å¤±è¯­ç°è±¡ï¼Œå³æ¨¡å‹èƒ½å‡†ç¡®è§£é‡Šå¤šæ¨¡æ€è¾“å…¥å´éš¾ä»¥å°†è¿™ç§ç†è§£è½¬åŒ–ä¸ºå¿ å®å¯æ§çš„åˆæˆè¾“å‡ºã€‚</p>
<p><strong>Method:</strong> æå‡ºUniCornè‡ªæ”¹è¿›æ¡†æ¶ï¼Œæ— éœ€å¤–éƒ¨æ•°æ®æˆ–æ•™å¸ˆç›‘ç£ï¼Œé€šè¿‡å°†å•ä¸ªç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åˆ’åˆ†ä¸ºä¸‰ä¸ªåä½œè§’è‰²ï¼šæè®®è€…ã€æ±‚è§£è€…å’Œè¯„åˆ¤è€…ï¼Œé€šè¿‡è‡ªåšå¼ˆç”Ÿæˆé«˜è´¨é‡äº¤äº’ï¼Œå¹¶é‡‡ç”¨è®¤çŸ¥æ¨¡å¼é‡æ„å°†æ½œåœ¨ç†è§£æç‚¼ä¸ºæ˜¾å¼ç”Ÿæˆä¿¡å·ï¼ŒåŒæ—¶å¼•å…¥UniCycleåŸºå‡†ï¼ŒåŸºäºæ–‡æœ¬åˆ°å›¾åƒåˆ°æ–‡æœ¬çš„é‡å»ºå¾ªç¯æ¥éªŒè¯å¤šæ¨¡æ€ä¸€è‡´æ€§æ¢å¤ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜UniCornåœ¨å…­ä¸ªé€šç”¨å›¾åƒç”ŸæˆåŸºå‡†ä¸Šç›¸æ¯”åŸºç¡€æ¨¡å‹å®ç°äº†å…¨é¢ä¸”æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨TIIF(73.8)ã€DPG(86.8)ã€CompBench(88.5)å’ŒUniCycleä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶åœ¨WISEå’ŒOneIGä¸Šåˆ†åˆ«è·å¾—+5.0å’Œ+6.5çš„æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„åŒæ—¶ä¿æŒäº†é²æ£’çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å®Œå…¨è‡ªç›‘ç£ç²¾ç‚¼æ–¹æ³•åœ¨ç»Ÿä¸€å¤šæ¨¡æ€æ™ºèƒ½ä¸­çš„å¯æ‰©å±•æ€§ï¼Œé€šè¿‡è‡ªæˆ‘æ”¹è¿›æ¡†æ¶æœ‰æ•ˆå¼¥åˆäº†å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å·®è·ï¼Œä¸ºæå‡ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›æä¾›äº†æ–°é€”å¾„ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹åŸæœ‰çš„ç†è§£èƒ½åŠ›ï¼Œå®ç°äº†ç†è§£ä¸ç”Ÿæˆçš„ååŒæå‡ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.</p>
<h3 id="30-ltx-2-efficient-joint-audio-visual-foundation-model">[30] <a href="https://arxiv.org/abs/2601.03233">LTX-2: Efficient Joint Audio-Visual Foundation Model</a></h3>
<p><em>Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, Eitan Richardson, Guy Shiran, Itay Chachy, Jonathan Chetboun, Michael Finkelson, Michael Kupchick, Nir Zabari, Nitzan Guetta, Noa Kotler, Ofir Bibi, Ori Gordon, Poriya Panet, Roi Benita, Shahar Armon, Victor Kulikov, Yaron Inger, Yonatan Shiftan, Zeev Melumian, Zeev Farbman</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºLTX-2ï¼Œä¸€ç§å¼€æºåŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿä»¥ç»Ÿä¸€æ–¹å¼ç”Ÿæˆé«˜è´¨é‡ã€æ—¶é—´åŒæ­¥çš„è§†å¬å†…å®¹ï¼Œè§£å†³äº†ç°æœ‰æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç¼ºä¹éŸ³é¢‘ç”Ÿæˆèƒ½åŠ›çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½ç”Ÿæˆå¼•äººæ³¨ç›®çš„è§†é¢‘åºåˆ—ï¼Œä½†ç¼ºä¹éŸ³é¢‘ç”Ÿæˆèƒ½åŠ›ï¼Œæ— æ³•æä¾›è¯­ä¹‰ã€æƒ…æ„Ÿå’Œæ°›å›´çº¿ç´¢ï¼Œè¿™é™åˆ¶äº†ç”Ÿæˆå†…å®¹çš„æ²‰æµ¸æ„Ÿå’Œå®Œæ•´æ€§ã€‚</p>
<p><strong>Method:</strong> LTX-2é‡‡ç”¨éå¯¹ç§°åŒæµTransformeræ¶æ„ï¼ŒåŒ…å«140äº¿å‚æ•°çš„è§†é¢‘æµå’Œ50äº¿å‚æ•°çš„éŸ³é¢‘æµï¼Œé€šè¿‡åŒå‘éŸ³é¢‘-è§†é¢‘äº¤å‰æ³¨æ„åŠ›å±‚è¿æ¥ï¼Œå¹¶é…å¤‡æ—¶é—´ä½ç½®åµŒå…¥å’Œè·¨æ¨¡æ€AdaLNç”¨äºå…±äº«æ—¶é—´æ­¥æ¡ä»¶ã€‚æ¨¡å‹é‡‡ç”¨å¤šè¯­è¨€æ–‡æœ¬ç¼–ç å™¨å¢å¼ºæç¤ºç†è§£ï¼Œå¹¶å¼•å…¥æ¨¡æ€æ„ŸçŸ¥åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼æœºåˆ¶ä»¥æ”¹å–„è§†å¬å¯¹é½å’Œå¯æ§æ€§ã€‚</p>
<p><strong>Result:</strong> è¯¥æ¨¡å‹åœ¨è¯„ä¼°ä¸­å®ç°äº†å¼€æºç³»ç»Ÿä¸­æœ€ä½³çš„è§†å¬è´¨é‡å’Œæç¤ºéµå¾ªæ€§èƒ½ï¼ŒåŒæ—¶ä»¥æ˜¾è‘—é™ä½çš„è®¡ç®—æˆæœ¬å’Œæ¨ç†æ—¶é—´è¾¾åˆ°äº†ä¸ä¸“æœ‰æ¨¡å‹ç›¸å½“çš„ç»“æœã€‚LTX-2ä¸ä»…èƒ½ç”Ÿæˆè¯­éŸ³ï¼Œè¿˜èƒ½äº§ç”Ÿä¸°å¯Œã€è¿è´¯çš„éŸ³é¢‘è½¨é“ï¼ŒåŒ…å«è‡ªç„¶çš„èƒŒæ™¯éŸ³å’Œæ‹ŸéŸ³å…ƒç´ ã€‚</p>
<p><strong>Conclusion:</strong> LTX-2å±•ç¤ºäº†ç»Ÿä¸€è§†å¬ç”Ÿæˆæ¨¡å‹çš„å¯è¡Œæ€§ï¼Œé€šè¿‡éå¯¹ç§°æ¶æ„è®¾è®¡å®ç°äº†é«˜æ•ˆè®­ç»ƒå’Œæ¨ç†ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡è¾“å‡ºã€‚è¯¥ç ”ç©¶ä¸ºå¼€æºå¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹æä¾›äº†é‡è¦è¿›å±•ï¼Œæ‰€æœ‰æ¨¡å‹æƒé‡å’Œä»£ç å‡å·²å…¬å¼€å‘å¸ƒï¼Œä¿ƒè¿›äº†è¯¥é¢†åŸŸçš„å¯è®¿é—®æ€§å’Œè¿›ä¸€æ­¥å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.</p>
<h3 id="31-a-versatile-multimodal-agent-for-multimedia-content-generation">[31] <a href="https://arxiv.org/abs/2601.03250">A Versatile Multimodal Agent for Multimedia Content Generation</a></h3>
<p><em>Daoan Zhang, Wenlin Yao, Xiaoyang Wang, Yebowen Hu, Jiebo Luo, Dong Yu</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ™ºèƒ½ä½“çš„å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆç³»ç»ŸMultiMedia-Agentï¼Œé€šè¿‡å¼•å…¥æŠ€èƒ½ä¹ å¾—ç†è®ºå’Œä¸¤é˜¶æ®µå…³è”ç­–ç•¥ï¼Œå®ç°äº†å¤æ‚å¤šåª’ä½“å†…å®¹çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–ç”Ÿæˆï¼Œç›¸æ¯”ç°æœ‰æ¨¡å‹èƒ½äº§ç”Ÿæ›´ä¼˜è´¨çš„å¤šåª’ä½“å†…å®¹ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰AIGCæ¨¡å‹å¤§å¤šåªèƒ½ä½œä¸ºç‰¹å®šåº”ç”¨åœºæ™¯ä¸­çš„ç‹¬ç«‹ç»„ä»¶ï¼Œæ— æ³•åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å®Œæˆç«¯åˆ°ç«¯ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥å’Œè¾“å‡ºæ–¹é¢å­˜åœ¨å±€é™ï¼Œè€Œç°å®åº”ç”¨ä¸­ç¼–è¾‘ä¸“å®¶éœ€è¦å¤„ç†å¤šæ ·åŒ–çš„å›¾åƒè§†é¢‘è¾“å…¥å¹¶ç”ŸæˆåŒ…å«éŸ³é¢‘ã€æ–‡æœ¬ç­‰å¤šå…ƒç´ çš„å¤šæ¨¡æ€è¾“å‡ºã€‚</p>
<p><strong>Method:</strong> æå‡ºMultiMedia-Agentç³»ç»Ÿï¼ŒåŒ…å«æ•°æ®ç”Ÿæˆç®¡é“ã€å†…å®¹åˆ›å»ºå·¥å…·åº“å’Œåå¥½å¯¹é½è¯„ä¼°æŒ‡æ ‡é›†ï¼Œå¼•å…¥æŠ€èƒ½ä¹ å¾—ç†è®ºæ¥å»ºæ¨¡è®­ç»ƒæ•°æ®ç®¡ç†å’Œæ™ºèƒ½ä½“è®­ç»ƒï¼Œè®¾è®¡äº†ä¸¤é˜¶æ®µå…³è”ç­–ç•¥è¿›è¡Œè®¡åˆ’ä¼˜åŒ–ï¼ŒåŒ…æ‹¬è‡ªå…³è”å’Œæ¨¡å‹åå¥½å…³è”ï¼Œå¹¶é€šè¿‡ä¸‰é˜¶æ®µæ–¹æ³•è®­ç»ƒæ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬åŸºç¡€/æˆåŠŸè®¡åˆ’å¾®è°ƒå’Œåå¥½ä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> å¯¹æ¯”å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æœ‰æ•ˆï¼ŒMultiMedia-Agentç›¸æ¯”æ–°é¢–æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´ä¼˜è´¨çš„å¤šåª’ä½“å†…å®¹ï¼Œè¯æ˜äº†æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚å†…å®¹ç”Ÿæˆä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Conclusion:</strong> åŸºäºæ™ºèƒ½ä½“çš„ç³»ç»Ÿä¸ºè§£å†³å¤æ‚å¤šåª’ä½“å†…å®¹ç”Ÿæˆæä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œé€šè¿‡æ•´åˆå¤šæ¨¡æ€å¤„ç†èƒ½åŠ›å’Œç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æµç¨‹ï¼Œå¡«è¡¥äº†å½“å‰AIGCæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥æ™ºèƒ½å†…å®¹åˆ›ä½œç³»ç»Ÿçš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs -- a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks. To deal with the complex scenarios, in this paper, we propose a MultiMedia-Agent designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. Additionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to novel models.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="32-adversarial-question-answering-robustness-a-multi-level-error-analysis-and-mitigation-study">[32] <a href="https://arxiv.org/abs/2601.02700">Adversarial Question Answering Robustness: A Multi-Level Error Analysis and Mitigation Study</a></h3>
<p><em>Agniv Roy Choudhury, Vignesh Ponselvan Rajasingh</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†Transformeræ¨¡å‹åœ¨å¯¹æŠ—æ€§QAä»»åŠ¡ä¸­çš„é²æ£’æ€§ï¼Œé€šè¿‡å¤šå±‚çº§é”™è¯¯åˆ†æè¯†åˆ«ä¸»è¦å¤±è´¥æ¨¡å¼ï¼Œå¹¶æå‡ºäº†åŸºäºå®ä½“æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ çš„é’ˆå¯¹æ€§ç¼“è§£ç­–ç•¥ï¼Œåœ¨ELECTRA-baseæ¨¡å‹ä¸Šå®ç°äº†å¯¹æŠ—æ€§èƒ½ä¸å¹²å‡€æ€§èƒ½çš„æ¥è¿‘æŒå¹³ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡é—®ç­”ç³»ç»Ÿåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å¦‚SQuADä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¯¹å¯¹æŠ—æ€§ç¤ºä¾‹ä»ç„¶è„†å¼±ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³Transformeræ¨¡å‹åœ¨AddSentå¯¹æŠ—æ•°æ®é›†ä¸Šçš„é²æ£’æ€§é—®é¢˜ï¼Œæ¢ç´¢æ¨¡å‹è§„æ¨¡ä¸é’ˆå¯¹æ€§ç¼“è§£ç­–ç•¥å¯¹å¯¹æŠ—æ€§è„†å¼±æ€§çš„å½±å“ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨ç³»ç»Ÿå®éªŒæ–¹æ³•ï¼Œåœ¨ELECTRA-smallåˆ°ELECTRA-baseä¸åŒè§„æ¨¡æ¨¡å‹ä¸Šè¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡äº”ç§äº’è¡¥çš„åˆ†ç±»æ–¹æ¡ˆè¿›è¡Œå¤šå±‚çº§é”™è¯¯åˆ†æï¼Œè¯†åˆ«ä¸»è¦å¤±è´¥æ¨¡å¼ã€‚ç³»ç»Ÿè¯„ä¼°å¯¹æŠ—æ€§å¾®è°ƒæ¯”ä¾‹ï¼Œå¹¶å®æ–½ä¸‰ç§é’ˆå¯¹æ€§ç¼“è§£ç­–ç•¥ï¼Œå…¶ä¸­å®ä½“æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ä¸ºæ ¸å¿ƒæ–¹æ³•ï¼Œç»“åˆå‘½åå®ä½“è¯†åˆ«æŒ‡å¯¼çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘ç°80%å¹²å‡€æ•°æ®+20%å¯¹æŠ—æ•°æ®ä¸ºæœ€ä¼˜å¾®è°ƒæ¯”ä¾‹ã€‚æ¨¡å‹è§„æ¨¡ä»ELECTRA-smallæ‰©å±•åˆ°ELECTRA-baseæ¶ˆé™¤äº†é²æ£’æ€§-å‡†ç¡®æ€§æƒè¡¡ï¼Œåœ¨å¹²å‡€å’Œå¯¹æŠ—æ•°æ®ä¸Šéƒ½å–å¾—æ˜¾è‘—æå‡ã€‚å®ä½“æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ å®ç°æœ€ä½³æ€§èƒ½ï¼šAddSent Exact Matchè¾¾åˆ°89.89%ï¼ŒSQuAD EMè¾¾åˆ°90.73%ï¼Œå¯¹æŠ—æ€§å·®è·ç¼©å°94.9%ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜é’ˆå¯¹æ€§ç¼“è§£ç­–ç•¥èƒ½å¤Ÿå®ç°å¹²å‡€æ€§èƒ½ä¸å¯¹æŠ—æ€§èƒ½çš„æ¥è¿‘æŒå¹³ï¼Œæ¨¡å‹è§„æ¨¡æ‰©å±•å¯æ¶ˆé™¤é²æ£’æ€§-å‡†ç¡®æ€§æƒè¡¡ã€‚è¿™æ˜¯é¦–ä¸ªå°†å…¨é¢è¯­è¨€é”™è¯¯åˆ†æä¸NERæŒ‡å¯¼çš„å¯¹æ¯”å­¦ä¹ ç›¸ç»“åˆç”¨äºå¯¹æŠ—æ€§QAçš„ç ”ç©¶ï¼Œä¸ºæ„å»ºæ›´é²æ£’çš„é—®ç­”ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆæ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Question answering (QA) systems achieve impressive performance on standard benchmarks like SQuAD, but remain vulnerable to adversarial examples. This project investigates the adversarial robustness of transformer models on the AddSent adversarial dataset through systematic experimentation across model scales and targeted mitigation strategies. We perform comprehensive multi-level error analysis using five complementary categorization schemes, identifying negation confusion and entity substitution as the primary failure modes. Through systematic evaluation of adversarial fine-tuning ratios, we identify 80% clean + 20% adversarial data as optimal. Data augmentation experiments reveal a capacity bottleneck in small models. Scaling from ELECTRA-small (14M parameters) to ELECTRA-base (110M parameters) eliminates the robustness-accuracy trade-off, achieving substantial improvements on both clean and adversarial data. We implement three targeted mitigation strategies, with Entity-Aware contrastive learning achieving best performance: 89.89% AddSent Exact Match (EM) and 90.73% SQuAD EM, representing 94.9% closure of the adversarial gap. To our knowledge, this is the first work integrating comprehensive linguistic error analysis with Named Entity Recognition (NER)-guided contrastive learning for adversarial QA, demonstrating that targeted mitigation can achieve near-parity between clean and adversarial performance.</p>
<h3 id="33-revisiting-data-compression-with-language-modeling">[33] <a href="https://arxiv.org/abs/2601.02875">Revisiting Data Compression with Language Modeling</a></h3>
<p><em>Chen-Han Tsai</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°æ®å‹ç¼©ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡ä¼˜åŒ–é…ç½®æ–¹æ³•åœ¨enwik9æ•°æ®é›†ä¸Šå®ç°äº†çº¦18%çš„è°ƒæ•´å‹ç¼©ç‡ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼ŒåŒæ—¶éªŒè¯äº†LLMåœ¨éè‡ªç„¶æ–‡æœ¬åºåˆ—å‹ç¼©ä¸­çš„ç«äº‰åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å…ˆå‰ç ”ç©¶å·²è¯æ˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®å‹ç¼©æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»å­˜åœ¨è‹¥å¹²æŒ‘æˆ˜ï¼Œé˜»ç¢å…¶æ›¿ä»£ç°æœ‰æ•°æ®å‹ç¼©ç®—æ³•ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¦‚ä½•é™ä½LLMä½œä¸ºæ•°æ®å‹ç¼©å™¨çš„è°ƒæ•´å‹ç¼©ç‡ï¼Œå¹¶è§£å†³å®é™…éƒ¨ç½²ä¸­çš„å…³é”®é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶æ¢ç´¢äº†å¤šç§ä¼˜åŒ–æ–¹æ³•ä»¥é™ä½å¤§å‹è¯­è¨€æ¨¡å‹çš„è°ƒæ•´å‹ç¼©ç‡ï¼Œé‡ç‚¹ç ”ç©¶äº†æ— éœ€é¢å¤–æ¨¡å‹è®­ç»ƒçš„é…ç½®ç­–ç•¥ã€‚ç ”ç©¶ç‰¹åˆ«å…³æ³¨äº†LLMåœ¨éè‹±è¯­æ•°æ®ã€ä»£ç æ•°æ®å’Œå­—èŠ‚æµåºåˆ—å‹ç¼©ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡ä¸åŒçš„é…ç½®æ–¹å¼è¯„ä¼°å…¶å‹ç¼©æ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> åœ¨enwik9æ•°æ®é›†ä¸Šï¼Œç ”ç©¶å®ç°äº†çº¦18%çš„è°ƒæ•´å‹ç¼©ç‡ï¼Œåˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œä¸”æ— éœ€é¢å¤–æ¨¡å‹è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒLLMåœ¨æ–‡æœ¬ä¸»å¯¼é¢†åŸŸçš„æ•°æ®å‹ç¼©æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè€Œåœ¨éè‡ªç„¶æ–‡æœ¬åºåˆ—å‹ç¼©ä¸­ï¼Œé€šè¿‡é€‚å½“é…ç½®ä»èƒ½ä¿æŒç«äº‰åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°æ®å‹ç¼©é¢†åŸŸå…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ä¸»å¯¼é¢†åŸŸè¡¨ç°ä¼˜å¼‚ã€‚ç ”ç©¶è¯å®é€šè¿‡é€‚å½“çš„é…ç½®ç­–ç•¥ï¼ŒLLMèƒ½å¤Ÿæœ‰æ•ˆå‹ç¼©éè‡ªç„¶æ–‡æœ¬åºåˆ—ï¼Œä¸ºæ›¿ä»£ä¼ ç»Ÿæ•°æ®å‹ç¼©ç®—æ³•æä¾›äº†å¯è¡Œæ€§ä¾æ®ï¼Œä½†å®é™…éƒ¨ç½²ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>In this report, we investigate the potential use of large language models (LLM's) in the task of data compression. Previous works have demonstrated promising results in applying LLM's towards compressing not only text, but also a wide range of multi-modal data. Despite the favorable performance achieved, there still remains several practical questions that pose a challenge towards replacing existing data compression algorithms with LLM's. In this work, we explore different methods to achieve a lower adjusted compression rate using LLM's as data compressors. In comparison to previous works, we were able to achieve a new state-of-the-art (SOTA) adjusted compression rate of around $18\%$ on the enwik9 dataset without additional model training. Furthermore, we explore the use of LLM's in compressing non-English data, code data, byte stream sequences. We show that while LLM's excel in compressing data in text-dominant domains, their ability in compressing non-natural text sequences still remain competitive if configured in the right way.</p>
<h3 id="34-linear-script-representations-in-speech-foundation-models-enable-zero-shot-transliteration">[34] <a href="https://arxiv.org/abs/2601.02906">Linear Script Representations in Speech Foundation Models Enable Zero-Shot Transliteration</a></h3>
<p><em>Ryan Soh-Eun Shim, Kwanghee Choi, Kalvin Chang, Ming-Hao Hsu, Florian Eichin, Zhizheng Wu, Alane Suhr, Michael A. Hedderich, David Harwath, David R. Mortensen, Barbara Plank</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åå¤„ç†æ–¹æ³•ï¼Œé€šè¿‡ä¿®æ”¹å¤šè¯­è¨€è¯­éŸ³æ¨¡å‹åœ¨æ¨ç†æ—¶çš„æ¿€æ´»å‘é‡ï¼Œå®ç°å¯¹è¯­éŸ³è¯†åˆ«è¾“å‡ºè„šæœ¬çš„ç›´æ¥æ§åˆ¶ï¼Œè§£å†³äº†ä¸åŒåœ°åŒºå˜ä½“ä½¿ç”¨ä¸åŒè„šæœ¬å¯¼è‡´çš„è¾“å‡ºä¸ç¡®å®šæ€§é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šè¯­è¨€è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Whisperï¼‰åœ¨ç½‘é¡µè§„æ¨¡æ•°æ®ä¸Šè®­ç»ƒï¼Œä½†åŒä¸€è¯­è¨€çš„ä¸åŒåœ°åŒºå˜ä½“å¾€å¾€ä½¿ç”¨ä¸åŒè„šæœ¬ä¹¦å†™ï¼Œå¯¼è‡´è¯­éŸ³è¯†åˆ«è¾“å‡ºè„šæœ¬å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œè¿™ç»™ä¸‹æ¸¸åº”ç”¨å¸¦æ¥æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å‘ç°è„šæœ¬ä¿¡æ¯åœ¨å¤šè¯­è¨€è¯­éŸ³æ¨¡å‹çš„æ¿€æ´»ç©ºé—´ä¸­å‘ˆçº¿æ€§ç¼–ç ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶å‘æ¿€æ´»å‘é‡æ·»åŠ ç‰¹å®šçš„è„šæœ¬å‘é‡ï¼Œå¯ä»¥ç›´æ¥æ§åˆ¶è¾“å‡ºè„šæœ¬ï¼Œè¯¥æ–¹æ³•æ”¯æŒéå¸¸è§„çš„è¯­è¨€-è„šæœ¬é…å¯¹è½¬æ¢ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè¯±å¯¼è„šæœ¬å˜æ›´ï¼ŒåŒ…æ‹¬éå¸¸è§„çš„è¯­è¨€-è„šæœ¬é…å¯¹ï¼ˆå¦‚æ„å¤§åˆ©è¯­ä½¿ç”¨è¥¿é‡Œå°”å­—æ¯ã€æ—¥è¯­ä½¿ç”¨æ‹‰ä¸å­—æ¯ï¼‰ï¼Œåœ¨Whisperæ‰€æœ‰æ¨¡å‹è§„æ¨¡ä¸Šå‡è¡¨ç°å‡ºç«äº‰æ€§æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šè¯­è¨€è¯­éŸ³æ¨¡å‹ä¸­è„šæœ¬ä¿¡æ¯çš„çº¿æ€§ç¼–ç ç‰¹æ€§ï¼Œæä¾›äº†ä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒå³å¯å®ç°è„šæœ¬æ§åˆ¶çš„åå¤„ç†æ–¹æ³•ï¼Œä¸ºè¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„è„šæœ¬æ ‡å‡†åŒ–å’Œå®šåˆ¶åŒ–åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>Multilingual speech foundation models such as Whisper are trained on web-scale data, where data for each language consists of a myriad of regional varieties. However, different regional varieties often employ different scripts to write the same language, rendering speech recognition output also subject to non-determinism in the output script. To mitigate this problem, we show that script is linearly encoded in the activation space of multilingual speech models, and that modifying activations at inference time enables direct control over output script. We find the addition of such script vectors to activations at test time can induce script change even in unconventional language-script pairings (e.g. Italian in Cyrillic and Japanese in Latin script). We apply this approach to inducing post-hoc control over the script of speech recognition output, where we observe competitive performance across all model sizes of Whisper.</p>
<h3 id="35-mmformalizer-multimodal-autoformalization-in-the-wild">[35] <a href="https://arxiv.org/abs/2601.03017">MMFormalizer: Multimodal Autoformalization in the Wild</a></h3>
<p><em>Jing Xiong, Qi Han, Yunta Hsieh, Hui Shen, Huajian Xin, Chaofan Tao, Chenyang Zhao, Hengyuan Zhang, Taiqiang Wu, Zhen Zhang, Haochen Wang, Zhongwei Wan, Lingpeng Kong, Ngai Wong</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMMFormalizerï¼Œä¸€ç§å¤šæ¨¡æ€è‡ªåŠ¨å½¢å¼åŒ–æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”å®ä½“åŸºç¡€å°†è‡ªç„¶è¯­è¨€æ•°å­¦ä¸è§†è§‰å…ƒç´ ç›¸ç»“åˆï¼Œå®ç°ä»ç‰©ç†ä¸–ç•Œåˆ°å½¢å¼åŒ–é™ˆè¿°çš„è½¬æ¢ï¼Œå¹¶æ„å»ºäº†PhyX-AFåŸºå‡†è¿›è¡Œè¯„ä¼°ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªåŠ¨å½¢å¼åŒ–é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºç‰©ç†ä¸–ç•Œçš„å¤šæ¨¡æ€ç‰¹æ€§ï¼Œå…¶ä¸­ç‰©ç†æ¨ç†éœ€è¦ä»è§†è§‰å…ƒç´ æ¨æ–­éšè—çº¦æŸï¼ˆå¦‚è´¨é‡æˆ–èƒ½é‡ï¼‰ï¼Œè€Œç°æœ‰æ–¹æ³•ä¸»è¦å±€é™äºæ–‡æœ¬é¢†åŸŸï¼Œæ— æ³•å¤„ç†ç°å®ä¸–ç•Œæ•°å­¦å’Œç‰©ç†é¢†åŸŸä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯ã€‚</p>
<p><strong>Method:</strong> MMFormalizeré€šè¿‡è‡ªé€‚åº”åŸºç¡€å°†è‡ªåŠ¨å½¢å¼åŒ–æ‰©å±•åˆ°æ–‡æœ¬ä¹‹å¤–ï¼Œä»æ„ŸçŸ¥åŸºç¡€çš„åŸè¯­é€’å½’æ„å»ºå½¢å¼å‘½é¢˜ï¼Œé‡‡ç”¨é€’å½’åŸºç¡€å’Œå…¬ç†ç»„åˆæ–¹æ³•ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”é€’å½’ç»ˆæ­¢ç¡®ä¿æ¯ä¸ªæŠ½è±¡éƒ½å¾—åˆ°è§†è§‰è¯æ®æ”¯æŒå¹¶é”šå®šåœ¨ç»´åº¦æˆ–å…¬ç†åŸºç¡€ä¸Šã€‚</p>
<p><strong>Result:</strong> åœ¨åŒ…å«115ä¸ªæ ·æœ¬çš„æ–°åŸºå‡†PhyX-AFä¸Šè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå‰æ²¿æ¨¡å‹å¦‚GPT-5å’ŒGemini-3-Proåœ¨ç¼–è¯‘å’Œè¯­ä¹‰å‡†ç¡®æ€§æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå…¶ä¸­GPT-5åœ¨ç‰©ç†æ¨ç†æ–¹é¢è¡¨ç°çªå‡ºï¼Œè€Œå‡ ä½•é¢†åŸŸä»ç„¶æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</p>
<p><strong>Conclusion:</strong> MMFormalizerä¸ºç»Ÿä¸€çš„å¤šæ¨¡æ€è‡ªåŠ¨å½¢å¼åŒ–æä¾›äº†å¯æ‰©å±•æ¡†æ¶ï¼Œè¿æ¥äº†æ„ŸçŸ¥ä¸å½¢å¼æ¨ç†ï¼Œé¦–æ¬¡å®ç°äº†å¤„ç†ç»å…¸åŠ›å­¦ï¼ˆæºè‡ªå“ˆå¯†é¡¿é‡ï¼‰ã€ç›¸å¯¹è®ºã€é‡å­åŠ›å­¦å’Œçƒ­åŠ›å­¦çš„å¤šæ¨¡æ€è‡ªåŠ¨å½¢å¼åŒ–æ–¹æ³•ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„é‡è¦ç©ºç™½ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io</p>
<h3 id="36-twist-training-free-and-label-free-short-text-clustering-through-iterative-vector-updating-with-llms">[36] <a href="https://arxiv.org/abs/2510.06747">TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs</a></h3>
<p><em>I-Fan Lin, Faegheh Hasibi, Suzan Verberne</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒå’Œæ ‡æ³¨çš„çŸ­æ–‡æœ¬èšç±»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯åœ¨ä»»ä½•ç°æœ‰åµŒå…¥å™¨ä¹‹ä¸Šä½¿ç”¨ï¼Œé€šè¿‡è¿­ä»£å‘é‡æ›´æ–°å’ŒLLMæŒ‡å¯¼å®ç°æ„å›¾èšç±»ï¼Œåœ¨å•†ä¸šåœºæ™¯ä¸­å–å¾—äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨é¢å‘å®¢æˆ·çš„èŠå¤©æœºå™¨äººåœºæ™¯ä¸­ï¼Œä¼ä¸šéœ€è¦å¤„ç†å¤§é‡ç”¨æˆ·è¯è¯­å¹¶æŒ‰æ„å›¾è¿›è¡Œèšç±»ï¼Œä½†å•†ä¸šç¯å¢ƒä¸­é€šå¸¸æ²¡æœ‰æ ‡æ³¨æ•°æ®ä¸”èšç±»æ•°é‡æœªçŸ¥ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¯¹æ¯”å­¦ä¹ æˆ–å…ˆéªŒçŸ¥è¯†ï¼Œæ— æ³•é€‚åº”è¿™ç§æ— æ ‡ç­¾ã€æ— å…ˆéªŒçš„ä½èµ„æºè®¾ç½®ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åŸºäºè¿­ä»£å‘é‡æ›´æ–°æœºåˆ¶ï¼Œé¦–å…ˆåŸºäºä»£è¡¨æ€§æ–‡æœ¬æ„å»ºç¨€ç–å‘é‡ï¼Œç„¶åé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡å¯¼è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæ•´ä¸ªè¿‡ç¨‹æ— éœ€è®­ç»ƒå’Œæ ‡æ³¨ï¼Œä¸”ä¸å‡è®¾å·²çŸ¥èšç±»æ•°é‡æˆ–æ ‡ç­¾ä¿¡æ¯ï¼Œå¯åº”ç”¨äºä»»ä½•åµŒå…¥å™¨ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šå–å¾—äº†ä¸ä½¿ç”¨å¯¹æ¯”å­¦ä¹ çš„æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜çš„ç»“æœï¼ŒåŒæ—¶è¯æ˜å…¶å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œå¯åº”ç”¨äºä¸åŒåµŒå…¥å™¨å’Œè¾ƒå°è§„æ¨¡çš„LLMï¼Œä¸”èƒ½å¤Ÿæ‰©å±•åˆ°å¤§å‹æ•°æ®é›†å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•åœ¨ä½èµ„æºã€é€‚åº”æ€§å¼ºçš„è®¾ç½®ä¸‹è¡¨ç°å‡ºè‰²ï¼Œå…¶å¯æ‰©å±•æ€§ä½¿å…¶æ¯”ç°æœ‰èšç±»æ–¹æ³•æ›´ç¬¦åˆå®é™…å•†ä¸šåœºæ™¯ï¼Œä¸ºæ— æ ‡æ³¨æ•°æ®ç¯å¢ƒä¸‹çš„æ„å›¾å‘ç°æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†LLMæŒ‡å¯¼åœ¨æ— ç›‘ç£èšç±»ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>In this paper, we propose a training-free and label-free method for short text clustering that can be used on top of any existing embedder. In the context of customer-facing chatbots, companies are dealing with large amounts of user utterances that need to be clustered according to their intent. In these commercial settings, no labeled data is typically available, and the number of clusters is not known. Our method is based on iterative vector updating: it constructs sparse vectors based on representative texts, and then iteratively refines them through LLM guidance. Our method achieves comparable or superior results to state-of-the-art methods that use contrastive learning, but without assuming prior knowledge of clusters or labels. Experiments on diverse datasets and smaller LLMs show that our method is model agnostic and can be applied to any embedder, with relatively small LLMs, and different clustering methods. We also show that our method scales to large datasets, reducing the computational cost of the LLM. These low-resource, adaptable settings and the scalability of our method make it more aligned with real-world scenarios than existing clustering methods.</p>
<h3 id="37-limited-linguistic-diversity-in-embodied-ai-datasets">[37] <a href="https://arxiv.org/abs/2601.03136">Limited Linguistic Diversity in Embodied AI Datasets</a></h3>
<p><em>Selma Wanna, Agnes Luhtaru, Jonathan Salfity, Ryan Barron, Juston Moore, Cynthia Matuszek, Mitch Pryor</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶å¯¹å¹¿æ³›ä½¿ç”¨çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ•°æ®é›†è¿›è¡Œäº†ç³»ç»Ÿæ€§å®¡è®¡ï¼Œé‡åŒ–äº†æŒ‡ä»¤è¯­è¨€çš„è¯æ±‡å¤šæ ·æ€§ã€é‡å¤æ€§ã€è¯­ä¹‰ç›¸ä¼¼æ€§å’Œå¥æ³•å¤æ‚æ€§ï¼Œæ­ç¤ºäº†å½“å‰æ•°æ®é›†ä¸­æŒ‡ä»¤è¯­è¨€çš„é«˜åº¦é‡å¤æ€§å’Œæœ‰é™ç»“æ„å˜åŒ–é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨è¯­è¨€ç†è§£æ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œä½†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°è¿™äº›ç³»ç»Ÿçš„æ•°æ®é›†çš„çœŸå®è¯­è¨€ç‰¹å¾ç¼ºä¹ç³»ç»Ÿæ€§çš„æ–‡æ¡£è®°å½•ï¼Œç ”ç©¶äººå‘˜å¯¹æ•°æ®é›†å®é™…åŒ…å«çš„æŒ‡ä»¤ç±»å‹å’Œè¯­è¨€å¤šæ ·æ€§äº†è§£æœ‰é™ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„è¯­è¨€ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨ç³»ç»Ÿæ€§æ•°æ®é›†å®¡è®¡æ–¹æ³•ï¼Œå¯¹å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„VLAè¯­æ–™åº“è¿›è¡Œé‡åŒ–åˆ†æï¼Œä»äº’è¡¥ç»´åº¦è¯„ä¼°æŒ‡ä»¤è¯­è¨€ç‰¹å¾ï¼ŒåŒ…æ‹¬è¯æ±‡å¤šæ ·æ€§ã€é‡å¤å’Œé‡å ç¨‹åº¦ã€è¯­ä¹‰ç›¸ä¼¼æ€§ä»¥åŠå¥æ³•å¤æ‚æ€§ï¼Œé€šè¿‡å¤šç»´åº¦æŒ‡æ ‡å…¨é¢åˆ»ç”»æ•°æ®é›†çš„è¯­è¨€åˆ†å¸ƒç‰¹å¾ã€‚</p>
<p><strong>Result:</strong> åˆ†æç»“æœæ˜¾ç¤ºè®¸å¤šæ•°æ®é›†ä¾èµ–é«˜åº¦é‡å¤ã€æ¨¡æ¿åŒ–çš„æŒ‡ä»¤ï¼Œç»“æ„å˜åŒ–æœ‰é™ï¼Œå¯¼è‡´æŒ‡ä»¤å½¢å¼åˆ†å¸ƒç‹­çª„ï¼Œæ•°æ®é›†ä¸­çš„è¯­è¨€ä¿¡å·å‘ˆç°å‡ºæ˜æ˜¾çš„æ¨¡å¼åŒ–ç‰¹å¾ï¼Œç¼ºä¹è¶³å¤Ÿçš„è¯­è¨€å¤šæ ·æ€§å’Œå¤æ‚æ€§æ¥æ”¯æŒæ¨¡å‹å¯¹è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„å…¨é¢ç†è§£ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå½“å‰VLAè®­ç»ƒå’Œè¯„ä¼°æ•°æ®çš„è¯­è¨€ç‰¹å¾æä¾›äº†æè¿°æ€§æ–‡æ¡£ï¼Œæ”¯æŒæ›´è¯¦ç»†çš„æ•°æ®é›†æŠ¥å‘Šã€æ›´åŸåˆ™æ€§çš„æ•°æ®é›†é€‰æ‹©ï¼Œä»¥åŠé’ˆå¯¹æ€§çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œæ—¨åœ¨æ‰©å¤§è¯­è¨€è¦†ç›–èŒƒå›´ï¼Œä¸ºæ„å»ºæ›´å…·è¯­è¨€å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„VLAæ¨¡å‹æä¾›æ•°æ®åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>Language plays a critical role in Vision-Language-Action (VLA) models, yet the linguistic characteristics of the datasets used to train and evaluate these systems remain poorly documented. In this work, we present a systematic dataset audit of several widely used VLA corpora, aiming to characterize what kinds of instructions these datasets actually contain and how much linguistic variety they provide. We quantify instruction language along complementary dimensions-including lexical variety, duplication and overlap, semantic similarity, and syntactic complexity. Our analysis shows that many datasets rely on highly repetitive, template-like commands with limited structural variation, yielding a narrow distribution of instruction forms. We position these findings as descriptive documentation of the language signal available in current VLA training and evaluation data, intended to support more detailed dataset reporting, more principled dataset selection, and targeted curation or augmentation strategies that broaden language coverage.</p>
<h3 id="38-multi-rads-synthetic-radiology-report-dataset-and-head-to-head-benchmarking-of-41-open-weight-and-proprietary-language-models">[38] <a href="https://arxiv.org/abs/2601.03232">Multi-RADS Synthetic Radiology Report Dataset and Head-to-Head Benchmarking of 41 Open-Weight and Proprietary Language Models</a></h3>
<p><em>Kartik Bose, Abhinandan Kumar, Raghuraman Soundararajan, Priya Mudgil, Samonee Ralmilay, Niharika Dutta, Manphool Singhal, Arun Kumar, Saugata Sen, Anurima Patra, Priya Ghosh, Abanti Das, Amit Gupta, Ashish Verma, Dipin Sudhakaran, Ekta Dhamija, Himangi Unde, Ishan Kumar, Krithika Rangarajan, Prerna Garg, Rachel Sequeira, Sudhin Shylendran, Taruna Yadav, Tej Pal, Pankaj Gupta</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶åˆ›å»ºäº†RXL-RADSetâ€”â€”ä¸€ä¸ªç»è¿‡æ”¾å°„ç§‘åŒ»ç”ŸéªŒè¯çš„å¤šRADSåˆæˆåŸºå‡†æ•°æ®é›†ï¼Œå¹¶ç³»ç»Ÿæ¯”è¾ƒäº†å¼€æºå°è¯­è¨€æ¨¡å‹ä¸ä¸“æœ‰æ¨¡å‹åœ¨æ”¾å°„å­¦æŠ¥å‘Šç»“æ„åŒ–åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°20-32Bå‚æ•°è§„æ¨¡çš„å°è¯­è¨€æ¨¡å‹åœ¨å¼•å¯¼æç¤ºä¸‹å¯æ¥è¿‘ä¸“æœ‰æ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ”¾å°„å­¦æŠ¥å‘Šå’Œæ•°æ®åˆ†æç³»ç»Ÿï¼ˆRADSï¼‰è™½ç„¶æ ‡å‡†åŒ–äº†é£é™©æ²Ÿé€šï¼Œä½†ä»å™è¿°æ€§æŠ¥å‘Šä¸­è‡ªåŠ¨åˆ†é…RADSç±»åˆ«é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æŒ‡å—å¤æ‚æ€§ã€è¾“å‡ºæ ¼å¼çº¦æŸã€è·¨RADSæ¡†æ¶å’Œæ¨¡å‹è§„æ¨¡çš„åŸºå‡†æµ‹è¯•æœ‰é™ï¼Œä»¥åŠç¼ºä¹ç»è¿‡éªŒè¯çš„å¤šRADSåŸºå‡†æ•°æ®é›†ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶åˆ›å»ºäº†åŒ…å«1,600ä»½åˆæˆæ”¾å°„å­¦æŠ¥å‘Šçš„RXL-RADSetåŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–10ç§RADSæ ‡å‡†å’Œå¤šç§æˆåƒæ¨¡æ€ï¼ŒæŠ¥å‘Šé€šè¿‡å¤§è¯­è¨€æ¨¡å‹åŸºäºåœºæ™¯è®¡åˆ’æ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿé£æ ¼ç”Ÿæˆï¼Œå¹¶ç»è¿‡ä¸¤é˜¶æ®µæ”¾å°„ç§‘åŒ»ç”ŸéªŒè¯ï¼›è¯„ä¼°äº†41ä¸ªé‡åŒ–å°è¯­è¨€æ¨¡å‹ï¼ˆ12ä¸ªå®¶æ—ï¼Œ0.135-32Bå‚æ•°ï¼‰å’ŒGPT-5.2ï¼Œé‡‡ç”¨å›ºå®šå¼•å¯¼æç¤ºç­–ç•¥ï¼Œä¸»è¦ç»ˆç‚¹ä¸ºæœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ï¼Œæ¬¡è¦åˆ†ææ¯”è¾ƒäº†å¼•å¯¼æç¤ºä¸é›¶æ ·æœ¬æç¤ºã€‚</p>
<p><strong>Result:</strong> åœ¨å¼•å¯¼æç¤ºä¸‹ï¼ŒGPT-5.2è¾¾åˆ°99.8%æœ‰æ•ˆæ€§å’Œ81.1%å‡†ç¡®æ€§ï¼›å°è¯­è¨€æ¨¡å‹æ€»ä½“è¾¾åˆ°96.8%æœ‰æ•ˆæ€§å’Œ61.1%å‡†ç¡®æ€§ï¼Œå…¶ä¸­20-32Bå‚æ•°èŒƒå›´çš„é¡¶çº§æ¨¡å‹è¾¾åˆ°çº¦99%æœ‰æ•ˆæ€§å’Œä¸­é«˜70%å‡†ç¡®æ€§ï¼›æ€§èƒ½éšæ¨¡å‹è§„æ¨¡æ‰©å±•ï¼ˆ&lt;1Bä¸â‰¥10Bä¹‹é—´å­˜åœ¨æ‹ç‚¹ï¼‰ï¼ŒéšRADSå¤æ‚æ€§å¢åŠ è€Œä¸‹é™ä¸»è¦æºäºåˆ†ç±»éš¾åº¦è€Œéæ— æ•ˆè¾“å‡ºï¼›å¼•å¯¼æç¤ºç›¸æ¯”é›¶æ ·æœ¬æç¤ºæ˜¾è‘—æ”¹å–„äº†æœ‰æ•ˆæ€§ï¼ˆ99.2% vs 96.7%ï¼‰å’Œå‡†ç¡®æ€§ï¼ˆ78.5% vs 69.6%ï¼‰ã€‚</p>
<p><strong>Conclusion:</strong> RXL-RADSetä¸ºå¤šRADSè‡ªåŠ¨åˆ†ç±»æä¾›äº†ç»è¿‡éªŒè¯çš„åŸºå‡†æ•°æ®é›†ï¼Œå¤§å‹å°è¯­è¨€æ¨¡å‹ï¼ˆ20-32Bï¼‰åœ¨å¼•å¯¼æç¤ºä¸‹å¯æ¥è¿‘ä¸“æœ‰æ¨¡å‹æ€§èƒ½ï¼Œä½†å¯¹äºæ›´é«˜å¤æ‚åº¦çš„åˆ†ç±»æ–¹æ¡ˆä»å­˜åœ¨å·®è·ï¼Œæ¨¡å‹è§„æ¨¡æ‰©å±•å’Œæç¤ºå·¥ç¨‹å¯¹æ€§èƒ½æå‡è‡³å…³é‡è¦ï¼Œä¸ºä¸´åºŠç¯å¢ƒä¸­éƒ¨ç½²é«˜æ•ˆå°è¯­è¨€æ¨¡å‹æä¾›äº†å®è¯ä¾æ®ã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>Background: Reporting and Data Systems (RADS) standardize radiology risk communication but automated RADS assignment from narrative reports is challenging because of guideline complexity, output-format constraints, and limited benchmarking across RADS frameworks and model sizes. Purpose: To create RXL-RADSet, a radiologist-verified synthetic multi-RADS benchmark, and compare validity and accuracy of open-weight small language models (SLMs) with a proprietary model for RADS assignment. Materials and Methods: RXL-RADSet contains 1,600 synthetic radiology reports across 10 RADS (BI-RADS, CAD-RADS, GB-RADS, LI-RADS, Lung-RADS, NI-RADS, O-RADS, PI-RADS, TI-RADS, VI-RADS) and multiple modalities. Reports were generated by LLMs using scenario plans and simulated radiologist styles and underwent two-stage radiologist verification. We evaluated 41 quantized SLMs (12 families, 0.135-32B parameters) and GPT-5.2 under a fixed guided prompt. Primary endpoints were validity and accuracy; a secondary analysis compared guided versus zero-shot prompting. Results: Under guided prompting GPT-5.2 achieved 99.8% validity and 81.1% accuracy (1,600 predictions). Pooled SLMs (65,600 predictions) achieved 96.8% validity and 61.1% accuracy; top SLMs in the 20-32B range reached ~99% validity and mid-to-high 70% accuracy. Performance scaled with model size (inflection between &lt;1B and &gt;=10B) and declined with RADS complexity primarily due to classification difficulty rather than invalid outputs. Guided prompting improved validity (99.2% vs 96.7%) and accuracy (78.5% vs 69.6%) compared with zero-shot. Conclusion: RXL-RADSet provides a radiologist-verified multi-RADS benchmark; large SLMs (20-32B) can approach proprietary-model performance under guided prompting, but gaps remain for higher-complexity schemes.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="39-learning-from-prompt-itself-the-hierarchical-attribution-prompt-optimization">[39] <a href="https://arxiv.org/abs/2601.02683">Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization</a></h3>
<p><em>Dongyu Chen, Jian Ma, Xianpeng Zhang, Lei Zhang, Haonan Lu, Chen Chen, Chuangchuang Wang, Kai Tang</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†åˆ†å±‚å½’å› æç¤ºä¼˜åŒ–ï¼ˆHAPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å½’å› æœºåˆ¶å’Œè¯­ä¹‰å•å…ƒä¼˜åŒ–è§£å†³ç°æœ‰æç¤ºä¼˜åŒ–æ–¹æ³•ä¸­çš„æç¤ºæ¼‚ç§»å’Œå¯è§£é‡Šæ€§é™ä½é—®é¢˜ï¼Œåœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜æ•ˆçš„æç¤ºä¼˜åŒ–ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æç¤ºä¼˜åŒ–æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯æç¤ºæ¼‚ç§»ç°è±¡ï¼Œå³æ–°æç¤ºä¿®å¤å…ˆå‰å¤±è´¥ä»»åŠ¡çš„åŒæ—¶ä¼šæŸå®³å…ˆå‰æˆåŠŸä»»åŠ¡çš„æ€§èƒ½ï¼›äºŒæ˜¯ä»å¤´ç”Ÿæˆæç¤ºä¼šé™ä½å¯è§£é‡Šæ€§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™äº›é™åˆ¶ï¼Œå¼€å‘ä¸€ç§æ—¢èƒ½ä¿æŒæ€§èƒ½ä¸€è‡´æ€§åˆèƒ½ç»´æŒå¯è§£é‡Šæ€§çš„ç»“æ„åŒ–æç¤ºä¼˜åŒ–æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> HAPOæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼šåŠ¨æ€å½’å› æœºåˆ¶é’ˆå¯¹è®­ç»ƒæ•°æ®å’Œæç¤ºå†å²ä¸­çš„é”™è¯¯æ¨¡å¼è¿›è¡Œä¼˜åŒ–ï¼›è¯­ä¹‰å•å…ƒä¼˜åŒ–æ–¹æ³•ç¼–è¾‘åŠŸèƒ½æ€§æç¤ºç‰‡æ®µè€Œéæ•´ä¸ªæç¤ºï¼›å¤šæ¨¡æ€å‹å¥½è¿›å±•æ”¯æŒç«¯åˆ°ç«¯LLMå’ŒLLM-MLLMä¸¤ç§å·¥ä½œæµç¨‹ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åˆ†å±‚ç»“æ„ï¼Œèƒ½å¤Ÿç³»ç»Ÿæ€§åœ°æ”¹è¿›æç¤ºè®¾è®¡è¿‡ç¨‹ã€‚</p>
<p><strong>Result:</strong> åœ¨å•/å¤šå›¾åƒé—®ç­”ï¼ˆå¦‚OCRV2ï¼‰å’Œå¤æ‚ä»»åŠ¡åˆ†æï¼ˆå¦‚BBHï¼‰ç­‰åº”ç”¨åœºæ™¯ä¸­ï¼ŒHAPOè¡¨ç°å‡ºå¢å¼ºçš„ä¼˜åŒ–æ•ˆç‡ï¼Œè¶…è¶Šäº†å¯æ¯”è¾ƒçš„è‡ªåŠ¨åŒ–æç¤ºä¼˜åŒ–æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶åœ¨ä¿æŒæç¤ºå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†æç¤ºæ¼‚ç§»é—®é¢˜ï¼Œå¹¶å»ºç«‹äº†å¯æ‰©å±•çš„æç¤ºå·¥ç¨‹èŒƒå¼ã€‚</p>
<p><strong>Conclusion:</strong> HAPOæ¡†æ¶ä¸ºå¤§è§„æ¨¡æç¤ºå·¥ç¨‹æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„èŒƒå¼ï¼Œé€šè¿‡ç»“æ„åŒ–ä¼˜åŒ–æ–¹æ³•å¹³è¡¡äº†æ€§èƒ½æ”¹è¿›ä¸å¯è§£é‡Šæ€§éœ€æ±‚ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†é’ˆå¯¹é”™è¯¯æ¨¡å¼çš„åŠ¨æ€å½’å› å’Œè¯­ä¹‰å•å…ƒç¼–è¾‘åœ¨æç¤ºä¼˜åŒ–ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æç¤ºå·¥ç¨‹ç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.</p>
<h3 id="40-m3mad-bench-are-multi-agent-debates-really-effective-across-domains-and-modalities">[40] <a href="https://arxiv.org/abs/2601.02854">M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?</a></h3>
<p><em>Ao Li, Jinghui Zhang, Luyu Li, Yuxiang Duan, Lang Gao, Mingcai Chen, Weijun Qin, Shaopeng Li, Fengxian Ji, Ning Liu, Lizhen Cui, Xiuying Chen, Yuntao Du</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†M3MAD-Benchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ™ºèƒ½ä½“è¾©è®ºæ–¹æ³•çš„ç»Ÿä¸€ä¸”å¯æ‰©å±•çš„åŸºå‡†ï¼Œè¦†ç›–å¤šé¢†åŸŸä»»åŠ¡ã€å¤šæ¨¡æ€è¾“å…¥å’Œå¤šç»´åº¦æŒ‡æ ‡ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ç¢ç‰‡åŒ–å’Œå•æ¨¡æ€é™åˆ¶é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤šæ™ºèƒ½ä½“è¾©è®ºç ”ç©¶å­˜åœ¨ä¸¤ä¸ªæ ¹æœ¬æ€§å±€é™ï¼šè¯„ä¼°åœ¨ç¢ç‰‡åŒ–ä¸”ä¸ä¸€è‡´çš„è®¾ç½®ä¸‹è¿›è¡Œï¼Œé˜»ç¢äº†å…¬å¹³æ¯”è¾ƒï¼›è¯„ä¼°ä¸»è¦å±€é™äºä¾èµ–çº¯æ–‡æœ¬è¾“å…¥çš„å•æ¨¡æ€åœºæ™¯ï¼Œç¼ºä¹å¯¹å¤šæ¨¡æ€è¾“å…¥çš„è¦†ç›–ã€‚</p>
<p><strong>Method:</strong> M3MAD-Benchå»ºç«‹äº†äº”ä¸ªæ ¸å¿ƒä»»åŠ¡é¢†åŸŸçš„æ ‡å‡†åŒ–åè®®ï¼šçŸ¥è¯†ã€æ•°å­¦ã€åŒ»å­¦ã€è‡ªç„¶ç§‘å­¦å’Œå¤æ‚æ¨ç†ï¼Œå¹¶ç³»ç»Ÿè¦†ç›–äº†çº¯æ–‡æœ¬å’Œè§†è§‰è¯­è¨€æ•°æ®é›†ï¼Œæ”¯æŒå—æ§çš„è·¨æ¨¡æ€æ¯”è¾ƒã€‚è¯¥åŸºå‡†åœ¨ä¹ä¸ªä¸åŒæ¶æ„ã€è§„æ¨¡å’Œæ¨¡æ€èƒ½åŠ›çš„åŸºç¡€æ¨¡å‹ä¸Šè¯„ä¼°MADæ–¹æ³•ï¼Œå¹¶çº³å…¥äº†é¢å‘æ•ˆç‡çš„æŒ‡æ ‡å¦‚ä»¤ç‰Œæ¶ˆè€—å’Œæ¨ç†æ—¶é—´ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›å®éªŒäº§ç”Ÿäº†å…³äºMADåœ¨çº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€åœºæ™¯ä¸‹æœ‰æ•ˆæ€§ã€é²æ£’æ€§å’Œæ•ˆç‡çš„ç³»ç»Ÿæ€§è§è§£ã€‚åŸºå‡†æä¾›äº†æ€§èƒ½-æˆæœ¬æƒè¡¡çš„æ•´ä½“è§†å›¾ï¼Œæ­ç¤ºäº†ä¸åŒè®¾ç½®ä¸‹MADæ–¹æ³•çš„ç›¸å¯¹ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</p>
<p><strong>Conclusion:</strong> M3MAD-Benchä¸ºæœªæ¥æ ‡å‡†åŒ–MADè¯„ä¼°ç ”ç©¶æä¾›äº†å¯é åŸºç¡€ï¼Œé€šè¿‡ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ä¿ƒè¿›äº†å…¬å¹³æ¯”è¾ƒï¼Œå¹¶ä¸ºå¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“è¾©è®ºç³»ç»Ÿçš„å¼€å‘æä¾›äº†ç³»ç»Ÿæ€§çš„æ€§èƒ½æ´å¯Ÿã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench.</p>
<h3 id="41-rationale-grounded-in-context-learning-for-time-series-reasoning-with-multimodal-large-language-models">[41] <a href="https://arxiv.org/abs/2601.02968">Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models</a></h3>
<p><em>Qingxiang Liu, Zhiqing Cui, Xiaoliang Luo, Yuqian Wu, Zhuoyang Jiang, Huaiyu Wan, Sheng Sun, Lvchun Wang, Wei Yu, Yuxuan Liang</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºRationaleTSæ–¹æ³•ï¼Œé€šè¿‡åŸºäºåŸç†çš„æƒ…å¢ƒå­¦ä¹ æ¥è§£å†³æ—¶é—´åºåˆ—æ¨ç†é—®é¢˜ï¼Œå…¶ä¸­åŸç†ä½œä¸ºæŒ‡å¯¼æ€§æ¨ç†å•å…ƒè€Œéäº‹åè§£é‡Šï¼Œä»è€Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—æ¨ç†ä¸­çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç”¨äºæ—¶é—´åºåˆ—æ¨ç†çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¡¨ç°ä¸ä½³ï¼Œå…¶æ ¹æœ¬åŸå› åœ¨äºç¼ºä¹è¿æ¥æ—¶é—´è§‚æµ‹ä¸ä¸‹æ¸¸ç»“æœçš„åŸç†å…ˆéªŒï¼Œå¯¼è‡´æ¨¡å‹ä¾èµ–è¡¨é¢æ¨¡å¼åŒ¹é…è€ŒéåŸåˆ™æ€§æ¨ç†ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é¦–å…ˆè¯±å¯¼æ ‡ç­¾æ¡ä»¶åŒ–åŸç†ï¼Œæ„å»ºä»å¯è§‚æµ‹è¯æ®åˆ°æ½œåœ¨ç»“æœçš„æ¨ç†è·¯å¾„ï¼›ç„¶åè®¾è®¡æ··åˆæ£€ç´¢æœºåˆ¶ï¼Œé€šè¿‡å¹³è¡¡æ—¶é—´æ¨¡å¼å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡æ¥æ£€ç´¢ç›¸å…³åŸç†å…ˆéªŒï¼Œæœ€ç»ˆå¯¹æ–°æ ·æœ¬è¿›è¡Œæƒ…å¢ƒæ¨ç†ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªé¢†åŸŸçš„æ—¶é—´åºåˆ—æ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†RationaleTSæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æå‡æ—¶é—´åºåˆ—æ¨ç†æ€§èƒ½æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜å°†åŸç†ä½œä¸ºæŒ‡å¯¼æ€§æ¨ç†å•å…ƒè€Œéäº‹åè§£é‡Šèƒ½å¤Ÿæ˜¾è‘—æå‡æ—¶é—´åºåˆ—æ¨ç†æ€§èƒ½ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ–¹æ³•è®ºæ¡†æ¶ï¼Œå¹¶è®¡åˆ’å¼€æºä»£ç ä»¥ä¾›å¤ç°ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.</p>
  </article>
</body>
</html>
