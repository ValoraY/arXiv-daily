<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-11-15.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 2]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-towards-blind-and-low-vision-accessibility-of-lightweight-vlms-and-custom-llm-evals">[1] <a href="https://arxiv.org/abs/2511.10615">Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals</a></h3>
<p><em>Shruti Singh Baghel, Yash Pratap Singh Rathore, Sushovan Jena, Anurag Pradhan, Amit Shukla, Arnav Bhavsar, Pawan Goyal</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç›²äººå’Œä½è§†åŠ›ç”¨æˆ·è¾…åŠ©åº”ç”¨ä¸­é¢ä¸´çš„é«˜èµ„æºéœ€æ±‚é—®é¢˜ï¼Œæå‡ºäº†ä¸“é—¨çš„å¯è®¿é—®æ€§è¯„ä¼°æ¡†æ¶ï¼Œå¹¶ç³»ç»Ÿè¯„ä¼°äº†ä¸åŒè§„æ¨¡SmolVLM2æ¨¡å‹åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æè¿°ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é«˜å†…å­˜ã€è®¡ç®—å’Œéƒ¨ç½²éœ€æ±‚é™åˆ¶äº†å®é™…åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä¾èµ–è¯¦ç»†ä¸Šä¸‹æ–‡æ„ŸçŸ¥æè¿°çš„ç›²äººå’Œä½è§†åŠ›ç”¨æˆ·ç¾¤ä½“ï¼Œéœ€è¦ç ”ç©¶æ¨¡å‹è§„æ¨¡å¯¹å¯è®¿é—®æ€§æè¿°è´¨é‡çš„å½±å“ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶è¯„ä¼°äº†500Må’Œ2.2Bå‚æ•°çš„SmolVLM2å˜ä½“åœ¨ä¸¤ä¸ªå¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå¹¶å¼•å…¥äº†ä¸¤ä¸ªä¸“é—¨é’ˆå¯¹BLVå¯è®¿é—®æ€§è¯„ä¼°çš„æ–°æ¡†æ¶ï¼šå¤šä¸Šä¸‹æ–‡BLVæ¡†æ¶è¯„ä¼°ç©ºé—´å®šå‘ã€ç¤¾äº¤äº’åŠ¨ã€åŠ¨ä½œäº‹ä»¶å’Œç¯å¢ƒèƒŒæ™¯ï¼›å¯¼èˆªè¾…åŠ©æ¡†æ¶ä¸“æ³¨äºç§»åŠ¨å…³é”®ä¿¡æ¯ã€‚åŒæ—¶ç³»ç»Ÿè¯„ä¼°äº†å››ç§ä¸åŒçš„æç¤ºè®¾è®¡ç­–ç•¥ï¼Œå¹¶åœ¨æ™ºèƒ½æ‰‹æœºä¸Šéƒ¨ç½²äº†FP32å’ŒINT8ç²¾åº¦å˜ä½“ã€‚</p>
<p><strong>Result:</strong> å®éªŒåœ¨AVCapsï¼ˆæˆ·å¤–ï¼‰å’ŒCharadesï¼ˆå®¤å†…ï¼‰ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œè¯„ä¼°äº†ä¸åŒè§„æ¨¡æ¨¡å‹åœ¨èµ„æºå—é™ç§»åŠ¨è®¾å¤‡ä¸Šçš„å®é™…æ€§èƒ½çº¦æŸï¼ŒåŒ…æ‹¬è®¡ç®—æ•ˆç‡ã€å†…å­˜ä½¿ç”¨å’Œæ¨ç†é€Ÿåº¦ç­‰å…³é”®æŒ‡æ ‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¯è®¿é—®æ€§åº”ç”¨ä¸­çš„ä¼˜åŒ–æä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œè¡¨æ˜é€šè¿‡é€‚å½“çš„æ¨¡å‹å‹ç¼©å’Œç²¾åº¦é‡åŒ–æŠ€æœ¯ï¼Œå¯ä»¥åœ¨ä¿æŒæè¿°è´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½èµ„æºéœ€æ±‚ï¼Œä¸ºBLVç”¨æˆ·çš„å®é™…éƒ¨ç½²å¼€è¾Ÿäº†å¯è¡Œè·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.</p>
<h3 id="2-enhancing-the-outcome-reward-based-rl-training-of-mllms-with-self-consistency-sampling">[2] <a href="https://arxiv.org/abs/2511.10648">Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling</a></h3>
<p><em>Jiahao Wang, Weiye Xu, Aijun Yang, Wengang Zhou, Lewei Lu, Houqiang Li, Xiaohua Wang, Jinguo Zhu</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†è‡ªæ´½é‡‡æ ·ï¼ˆSCSï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥è§†è§‰æ‰°åŠ¨å’Œè½¨è¿¹é‡é‡‡æ ·æ¥çº æ­£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­ç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ å­˜åœ¨çš„ä¸å¯é è½¨è¿¹é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æå‡äº†æ¨ç†å‡†ç¡®æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ é¢ä¸´ä¸€ä¸ªè¢«å¿½è§†çš„å…³é”®é—®é¢˜ï¼šå³ä½¿æ¨ç†é“¾å­˜åœ¨é”™è¯¯ä½†æœ€ç»ˆçŒœå¯¹æ­£ç¡®ç­”æ¡ˆçš„ä¸å¯é è½¨è¿¹ï¼Œä¸çœŸæ­£æ­£ç¡®æ¨ç†çš„è½¨è¿¹è·å¾—ç›¸åŒå¥–åŠ±ï¼Œè¿™ç§å¥–åŠ±åˆ†é…çš„ä¸å…¬å¹³æ€§ä¸¥é‡å½±å“äº†æ¨¡å‹çš„å­¦ä¹ æ•ˆæœã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„è‡ªæ´½é‡‡æ ·ï¼ˆSCSï¼‰æ–¹æ³•åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼šé¦–å…ˆå¯¹è¾“å…¥å›¾åƒå¼•å…¥å¾®å°è§†è§‰æ‰°åŠ¨ï¼Œç„¶åå¯¹åˆå§‹æ¨ç†è½¨è¿¹è¿›è¡Œé‡å¤æˆªæ–­å’Œé‡é‡‡æ ·ï¼›é€šè¿‡è®¡ç®—è¿™äº›æ‰°åŠ¨è½¨è¿¹ä¹‹é—´çš„ä¸€è‡´æ€§å¾—åˆ†ï¼Œåœ¨ç­–ç•¥æ›´æ–°æ—¶é™ä½ä¸å¯é è½¨è¿¹çš„æƒé‡ã€‚</p>
<p><strong>Result:</strong> åœ¨Qwen2.5-VL-7B-Instructæ¨¡å‹ä¸Šï¼Œå°†SCSé›†æˆåˆ°RLOOã€GRPOå’ŒREINFORCE++ç³»åˆ—æ–¹æ³•ä¸­ï¼Œåœ¨å…­ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šå‡†ç¡®ç‡æœ€é«˜æå‡7.7ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”è®¡ç®—å¼€é”€å¯å¿½ç•¥ä¸è®¡ï¼›è¯¥æ–¹æ³•åœ¨Qwen2.5-VL-3B-Instructå’ŒInternVL3-8Bæ¨¡å‹ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—æ•ˆæœæå‡ã€‚</p>
<p><strong>Conclusion:</strong> SCSä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§ç®€å•é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œæƒ©ç½šä¸å¯é çš„æ¨ç†è½¨è¿¹ï¼Œæ˜¾è‘—æå‡æ¨¡å‹çš„æ¨ç†å¯é æ€§å’Œå‡†ç¡®æ€§ï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§å’Œå®ç”¨æ€§ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.</p>
  </article>
</body>
</html>
