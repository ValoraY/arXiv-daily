{"id": "2601.04339", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04339", "abs": "https://arxiv.org/abs/2601.04339", "authors": ["Jiahui Chen", "Philippe Hansen-Estruch", "Xiaochuang Han", "Yushi Hu", "Emily Dinan", "Amita Kamath", "Michal Drozdzal", "Reyhane Askari-Hemmat", "Luke Zettlemoyer", "Marjan Ghazvininejad"], "title": "Unified Text-Image Generation with Weakness-Targeted Post-Training", "comment": null, "summary": "Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u540e\u8bad\u7ec3\u5b9e\u73b0\u5b8c\u5168\u7edf\u4e00\u7684\u6587\u672c-\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u5355\u6b21\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u4e3b\u5730\u4ece\u6587\u672c\u63a8\u7406\u8fc7\u6e21\u5230\u89c6\u89c9\u5408\u6210\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u52a0\u6743\u548c\u7b56\u7565\u6027\u8bbe\u8ba1\u7684\u540e\u8bad\u7ec3\u6570\u636e\u5728\u591a\u4e2aT2I\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u67b6\u6784\u901a\u5e38\u4f9d\u8d56\u663e\u5f0f\u7684\u6a21\u6001\u5207\u6362\u673a\u5236\uff0c\u9700\u8981\u5148\u751f\u6210\u63a8\u7406\u6587\u672c\u518d\u624b\u52a8\u5207\u6362\u5230\u56fe\u50cf\u751f\u6210\uff0c\u8fd9\u79cd\u5206\u79bb\u7684\u987a\u5e8f\u63a8\u7406\u8fc7\u7a0b\u9650\u5236\u4e86\u8de8\u6a21\u6001\u8026\u5408\u5e76\u963b\u788d\u4e86\u81ea\u52a8\u5316\u7684\u591a\u6a21\u6001\u751f\u6210\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5b9e\u73b0\u5b8c\u5168\u7edf\u4e00\u7684\u6587\u672c-\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u540e\u8bad\u7ec3\u65b9\u6cd5\u5b9e\u73b0\u5b8c\u5168\u7edf\u4e00\u7684\u6587\u672c-\u56fe\u50cf\u751f\u6210\uff0c\u63a2\u7d22\u4e86\u8054\u5408\u6587\u672c-\u56fe\u50cf\u751f\u6210\u5bf9T2I\u6027\u80fd\u7684\u5f71\u54cd\u4ee5\u53ca\u540e\u8bad\u7ec3\u4e2d\u5404\u6a21\u6001\u7684\u76f8\u5bf9\u91cd\u8981\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u4e0d\u540c\u7684\u540e\u8bad\u7ec3\u6570\u636e\u7b56\u7565\uff0c\u5305\u62ec\u4f7f\u7528\u9488\u5bf9\u7279\u5b9a\u9650\u5236\u7684\u6709\u9488\u5bf9\u6027\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u91c7\u7528\u79bb\u7ebf\u5956\u52b1\u52a0\u6743\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u5b8c\u5168\u81ea\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5e7f\u6cdb\u7684\u56fe\u50cf-\u6807\u9898\u8bed\u6599\u5e93\u6216\u57fa\u51c6\u5bf9\u9f50\u6570\u636e\u76f8\u6bd4\uff0c\u9488\u5bf9\u7279\u5b9a\u9650\u5236\u7684\u6709\u9488\u5bf9\u6027\u6570\u636e\u96c6\u80fd\u591f\u53d6\u5f97\u66f4\u4f18\u7ed3\u679c\uff0c\u901a\u8fc7\u5956\u52b1\u52a0\u6743\u4e24\u4e2a\u6a21\u6001\u548c\u7b56\u7565\u6027\u8bbe\u8ba1\u7684\u540e\u8bad\u7ec3\u6570\u636e\uff0c\u8be5\u65b9\u6cd5\u5728\u56db\u4e2a\u4e0d\u540c\u7684T2I\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u7684\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u5b9e\u73b0\u5b8c\u5168\u7edf\u4e00\u7684\u6587\u672c-\u56fe\u50cf\u751f\u6210\u662f\u53ef\u884c\u7684\uff0c\u5956\u52b1\u52a0\u6743\u4e24\u4e2a\u6a21\u6001\u548c\u7b56\u7565\u6027\u8bbe\u8ba1\u7684\u540e\u8bad\u7ec3\u6570\u636e\u5bf9\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u751f\u6210\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e3a\u6784\u5efa\u66f4\u81ea\u4e3b\u3001\u8026\u5408\u6027\u66f4\u5f3a\u7684\u591a\u6a21\u6001\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2601.04359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04359", "abs": "https://arxiv.org/abs/2601.04359", "authors": ["Kunyang Li", "Mubarak Shah", "Yuzhang Shang"], "title": "PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache", "comment": null, "summary": "A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPackCache\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684KV\u7f13\u5b58\u7ba1\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u538b\u7f29\u7edf\u4e00\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u7684KV\u7f13\u5b58\uff0c\u89e3\u51b3\u4e86KV\u7f13\u5b58\u968f\u751f\u6210\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u5bfc\u81f4\u7684\u63a8\u7406\u6548\u7387\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u7edf\u4e00\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f9d\u8d56KV\u7f13\u5b58\u673a\u5236\u5c06\u6ce8\u610f\u529b\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(T\u00b2)\u964d\u4f4e\u5230O(T)\uff0c\u4f46KV\u7f13\u5b58\u5927\u5c0f\u968f\u751f\u6210\u4ee4\u724c\u6570\u91cf\u7ebf\u6027\u589e\u957f\uff0c\u6210\u4e3a\u9650\u5236\u63a8\u7406\u6548\u7387\u548c\u751f\u6210\u957f\u5ea6\u7684\u4e3b\u8981\u74f6\u9888\u3002\u7814\u7a76\u53d1\u73b0KV\u7f13\u5b58\u4ee4\u724c\u5177\u6709\u660e\u663e\u7684\u65f6\u7a7a\u7279\u6027\uff1a\u6587\u672c\u548c\u6761\u4ef6\u56fe\u50cf\u4ee4\u724c\u4f5c\u4e3a\u6301\u4e45\u8bed\u4e49\u951a\u70b9\u6301\u7eed\u83b7\u5f97\u9ad8\u6ce8\u610f\u529b\uff0c\u800c\u5bf9\u5148\u524d\u5e27\u7684\u6ce8\u610f\u529b\u968f\u65f6\u95f4\u8ddd\u79bb\u81ea\u7136\u8870\u51cf\u3002", "method": "PackCache\u901a\u8fc7\u4e09\u79cd\u534f\u8c03\u673a\u5236\u52a8\u6001\u538b\u7f29KV\u7f13\u5b58\uff1a\u6761\u4ef6\u951a\u5b9a\u4fdd\u7559\u8bed\u4e49\u53c2\u8003\uff0c\u8de8\u5e27\u8870\u51cf\u5efa\u6a21\u6839\u636e\u65f6\u95f4\u8ddd\u79bb\u5206\u914d\u7f13\u5b58\u9884\u7b97\uff0c\u7a7a\u95f4\u4fdd\u6301\u4f4d\u7f6e\u5d4c\u5165\u5728\u7f13\u5b58\u79fb\u9664\u65f6\u7ef4\u6301\u8fde\u8d2f\u76843D\u7ed3\u6784\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u6709\u7edf\u4e00\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002", "result": "\u572848\u5e27\u957f\u5e8f\u5217\u4e0a\uff0cPackCache\u5c06\u7aef\u5230\u7aef\u751f\u6210\u901f\u5ea6\u63d0\u53471.7-2.2\u500d\u3002\u5bf9\u4e8e\u53d7KV\u7f13\u5b58\u6269\u5c55\u5f71\u54cd\u6700\u5927\u7684\u6700\u540e\u56db\u5e27\uff08\u89c6\u9891\u4e2d\u6700\u6602\u8d35\u7684\u90e8\u5206\uff09\uff0c\u5728A40\u548cH200\u4e0a\u5206\u522b\u5b9e\u73b02.6\u500d\u548c3.7\u500d\u7684\u52a0\u901f\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u5e8f\u5217\u89c6\u9891\u751f\u6210\u7684\u6548\u7387\u3002", "conclusion": "PackCache\u901a\u8fc7\u5229\u7528KV\u7f13\u5b58\u4ee4\u724c\u7684\u65f6\u7a7a\u7279\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7edf\u4e00\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u4e2d\u7684\u63a8\u7406\u6548\u7387\u74f6\u9888\u3002\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u52a8\u6001\u7ba1\u7406\u7f13\u5b58\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u957f\u5e8f\u5217\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4f18\u5316\u65b9\u6848\uff0c\u5177\u6709\u6269\u5c55\u5230\u5176\u4ed6\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.04376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04376", "abs": "https://arxiv.org/abs/2601.04376", "authors": ["Paraskevi Valergaki", "Vassilis C. Nicodemou", "Iason Oikonomidis", "Antonis Argyros", "Anastasios Roussos"], "title": "Combining facial videos and biosignals for stress estimation during driving", "comment": "UNDER SUBMISSION TO ICPR 2026", "summary": "Reliable stress recognition from facial videos is challenging due to stress's subjective nature and voluntary facial control. While most methods rely on Facial Action Units, the role of disentangled 3D facial geometry remains underexplored. We address this by analyzing stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Paired hypothesis tests between baseline and stressor phases reveal that 41 of 56 coefficients show consistent, phase-specific stress responses comparable to physiological markers. Building on this, we propose a Transformer-based temporal modeling framework and assess unimodal, early-fusion, and cross-modal attention strategies. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92\\%, Accuracy 86.7\\%), with EMOCA-gaze fusion also competitive (AUROC 91.8\\%). This highlights the effectiveness of temporal modeling and cross-modal attention for stress recognition.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u80263D\u9762\u90e8\u51e0\u4f55\u7279\u5f81\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u7684\u9a7e\u9a76\u538b\u529b\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790EMOCA\u63d0\u53d6\u76843D\u8868\u60c5\u548c\u59ff\u6001\u7cfb\u6570\uff0c\u7ed3\u5408Transformer\u65f6\u5e8f\u5efa\u6a21\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u538b\u529b\u72b6\u6001\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9762\u90e8\u52a8\u4f5c\u5355\u5143\u7684\u538b\u529b\u8bc6\u522b\u65b9\u6cd5\u9762\u4e34\u4e3b\u89c2\u6027\u548c\u81ea\u4e3b\u9762\u90e8\u63a7\u5236\u7684\u6311\u6218\uff0c\u800c\u89e3\u8026\u76843D\u9762\u90e8\u51e0\u4f55\u7279\u5f81\u5728\u538b\u529b\u8bc6\u522b\u4e2d\u7684\u4f5c\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u7279\u522b\u5173\u6ce8\u5206\u5fc3\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u538b\u529b\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u7814\u7a76\u91c7\u7528EMOCA\u6a21\u578b\u63d0\u53d63D\u8868\u60c5\u548c\u59ff\u6001\u7cfb\u6570\uff0c\u901a\u8fc7\u914d\u5bf9\u5047\u8bbe\u68c0\u9a8c\u5206\u6790\u57fa\u7ebf\u548c\u538b\u529b\u9636\u6bb5\u7684\u5dee\u5f02\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u57fa\u4e8eTransformer\u7684\u65f6\u5e8f\u5efa\u6a21\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u5355\u6a21\u6001\u3001\u65e9\u671f\u878d\u5408\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u4e09\u79cd\u878d\u5408\u7b56\u7565\uff0c\u7279\u522b\u5173\u6ce8EMOCA\u7279\u5f81\u4e0e\u751f\u7406\u4fe1\u53f7\u3001\u6ce8\u89c6\u4fe1\u53f7\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b056\u4e2aEMOCA\u7cfb\u6570\u4e2d\u670941\u4e2a\u5728\u538b\u529b\u9636\u6bb5\u8868\u73b0\u51fa\u663e\u8457\u4e14\u4e00\u81f4\u7684\u54cd\u5e94\u6a21\u5f0f\uff0c\u4e0e\u751f\u7406\u6807\u8bb0\u7269\u76f8\u5f53\u3002\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0cEMOCA\u4e0e\u751f\u7406\u4fe1\u53f7\u878d\u5408\u8fbe\u5230AUROC 92%\u548c\u51c6\u786e\u738786.7%\uff0cEMOCA\u4e0e\u6ce8\u89c6\u4fe1\u53f7\u878d\u5408\u4e5f\u8fbe\u5230AUROC 91.8%\u7684\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u89e3\u80263D\u9762\u90e8\u51e0\u4f55\u7279\u5f81\u5728\u538b\u529b\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u5728\u65f6\u5e8f\u5efa\u6a21\u4e2d\u7684\u4f18\u52bf\u3002\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u89c6\u89c9\u7684\u538b\u529b\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9a7e\u9a76\u76d1\u63a7\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u8868\u660e\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2601.04404", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04404", "abs": "https://arxiv.org/abs/2601.04404", "authors": ["Jusheng Zhang", "Yijia Fan", "Zimo Wen", "Jian Wang", "Keze Wang"], "title": "3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation", "comment": "Accepted at NeurIPS 2025", "summary": "Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTri-MARF\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u54082D\u591a\u89c6\u89d2\u56fe\u50cf\u3001\u6587\u672c\u63cf\u8ff0\u548c3D\u70b9\u4e91\u7684\u4e09\u6a21\u6001\u8f93\u5165\uff0c\u5e76\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a213D\u5bf9\u8c61\u6807\u6ce8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "3D\u5bf9\u8c61\u6807\u6ce8\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u5e94\u7528\u4e2d\u9762\u4e34\u7a7a\u95f4\u590d\u6742\u6027\u3001\u906e\u6321\u548c\u89c6\u89d2\u4e0d\u4e00\u81f4\u7b49\u6311\u6218\uff0c\u73b0\u6709\u57fa\u4e8e\u5355\u4e00\u6a21\u578b\u7684\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u591a\u6a21\u6001\u534f\u4f5c\u6846\u67b6\u6765\u63d0\u5347\u6807\u6ce8\u6027\u80fd\u3002", "method": "Tri-MARF\u6846\u67b6\u6574\u5408\u4e09\u6a21\u6001\u8f93\u5165\uff082D\u591a\u89c6\u89d2\u56fe\u50cf\u3001\u6587\u672c\u63cf\u8ff0\u548c3D\u70b9\u4e91\uff09\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u67b6\u6784\uff0c\u5305\u62ec\u4e09\u4e2a\u4e13\u95e8\u5316\u667a\u80fd\u4f53\uff1a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u7528\u4e8e\u751f\u6210\u591a\u89c6\u89d2\u63cf\u8ff0\uff0c\u4fe1\u606f\u805a\u5408\u667a\u80fd\u4f53\u7528\u4e8e\u9009\u62e9\u6700\u4f18\u63cf\u8ff0\uff0c\u4ee5\u53ca\u95e8\u63a7\u667a\u80fd\u4f53\u7528\u4e8e\u5bf9\u9f50\u6587\u672c\u8bed\u4e49\u4e0e3D\u51e0\u4f55\u4fe1\u606f\u4ee5\u5b9e\u73b0\u7cbe\u7ec6\u5316\u6807\u6ce8\u3002", "result": "\u5728Objaverse\u3001LVIS\u3001Objaverse XL\u548cABO\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTri-MARF\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cCLIPScore\u8fbe\u523088.7\u5206\uff0cViLT R@5\u68c0\u7d22\u51c6\u786e\u7387\u5206\u522b\u4e3a45.2%\u548c43.8%\uff0c\u5728\u5355\u4e2aNVIDIA A100 GPU\u4e0a\u5b9e\u73b0\u9ad8\u8fbe\u6bcf\u5c0f\u65f612000\u4e2a\u5bf9\u8c61\u7684\u5904\u7406\u541e\u5410\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u8f93\u5165\u4e0e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u67b6\u6784\u57283D\u5bf9\u8c61\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u590d\u67423D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7387\u5927\u89c4\u6a21\u6807\u6ce8\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.04500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04500", "abs": "https://arxiv.org/abs/2601.04500", "authors": ["Yifei Gao", "Jiang Wu", "Xiaoyi Chen", "Yifan Yang", "Zhe Cui", "Tianyi Ma", "Jiaming Zhang", "Jitao Sang"], "title": "GUITester: Enabling GUI Agents for Exploratory Defect Discovery", "comment": null, "summary": "Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \\textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \\textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \\textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \\textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \\textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \\textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\\footnote{Our code is now available in~\\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGUITester\uff0c\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u63a2\u7d22\u5f0fGUI\u6d4b\u8bd5\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u65e0\u6cd5\u81ea\u4e3b\u53d1\u73b0\u7f3a\u9677\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u5bfc\u822a\u4e0e\u9a8c\u8bc1\uff0c\u5728GUITestBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8648.90%\u7684F1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5f0fGUI\u6d4b\u8bd5\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u9ad8\u6602\u7684\u4eba\u5de5\u6210\u672c\u3002\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\u800c\u65e0\u6cd5\u81ea\u4e3b\u53d1\u73b0\u7f3a\u9677\uff1a\u76ee\u6807\u5bfc\u5411\u906e\u853d\uff08\u667a\u80fd\u4f53\u4f18\u5148\u5b8c\u6210\u4efb\u52a1\u800c\u975e\u62a5\u544a\u5f02\u5e38\uff09\u548c\u6267\u884c\u504f\u5dee\u5f52\u56e0\uff08\u7cfb\u7edf\u7f3a\u9677\u88ab\u8bef\u5224\u4e3a\u667a\u80fd\u4f53\u9519\u8bef\uff09\u3002", "method": "\u7814\u7a76\u9996\u5148\u5f15\u5165\u4e86GUITestBench\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u8be5\u4efb\u52a1\u7684\u4ea4\u4e92\u5f0f\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b26\u79cd\u7f3a\u9677\u7c7b\u578b\u7684143\u4e2a\u4efb\u52a1\u3002\u968f\u540e\u63d0\u51fa\u4e86GUITester\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u6a21\u5757\u89e3\u8026\u5bfc\u822a\u4e0e\u9a8c\u8bc1\uff1a\u89c4\u5212\u6267\u884c\u6a21\u5757\u901a\u8fc7\u5d4c\u5165\u6d4b\u8bd5\u610f\u56fe\u4e3b\u52a8\u63a2\u6d4b\u7f3a\u9677\uff1b\u5206\u5c42\u53cd\u601d\u6a21\u5757\u901a\u8fc7\u4ea4\u4e92\u5386\u53f2\u5206\u6790\u89e3\u51b3\u5f52\u56e0\u6a21\u7cca\u6027\u95ee\u9898\u3002", "result": "GUITester\u5728GUITestBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8648.90%\u7684F1\u5206\u6570\uff08Pass@3\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u768433.35%\u3002\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u76ee\u6807\u5bfc\u5411\u906e\u853d\u548c\u6267\u884c\u504f\u5dee\u5f52\u56e0\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u81ea\u4e3b\u63a2\u7d22\u5f0f\u6d4b\u8bd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u81ea\u4e3b\u63a2\u7d22\u5f0fGUI\u6d4b\u8bd5\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765GUI\u8d28\u91cf\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002\u901a\u8fc7\u89e3\u8026\u5bfc\u822a\u4e0e\u9a8c\u8bc1\u7684\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86MLLM\u667a\u80fd\u4f53\u5728\u7f3a\u9677\u53d1\u73b0\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u81ea\u52a8\u5316\u8f6f\u4ef6\u6d4b\u8bd5\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.04200", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04200", "abs": "https://arxiv.org/abs/2601.04200", "authors": ["Virginia Negri", "V\u00edctor Mart\u00ednez G\u00f3mez", "Sergio A. Balanya", "Subburam Rajaram"], "title": "Attribute-Aware Controlled Product Generation with LLMs for E-commerce", "comment": "AAAI'26 Workshop on Shaping Responsible Synthetic Data in the Era of Foundation Models (RSD)", "summary": "Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u7535\u5546\u4ea7\u54c1\u6570\u636e\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u53d7\u63a7\u4fee\u6539\u7b56\u7565\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5728MAVE\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u5f53\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u4ea7\u54c1\u4fe1\u606f\u63d0\u53d6\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u7535\u5546\u4ea7\u54c1\u4fe1\u606f\u63d0\u53d6\u5bf9\u7535\u5546\u670d\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u83b7\u53d6\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u5408\u6210\u7535\u5546\u4ea7\u54c1\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u91c7\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d7\u63a7\u4fee\u6539\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u7b56\u7565\uff1a\u5c5e\u6027\u4fdd\u7559\u4fee\u6539\u3001\u53d7\u63a7\u8d1f\u4f8b\u751f\u6210\u548c\u7cfb\u7edf\u5c5e\u6027\u79fb\u9664\uff0c\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u914d\u5408\u5c5e\u6027\u611f\u77e5\u63d0\u793a\uff0c\u5728\u4fdd\u6301\u4ea7\u54c1\u8fde\u8d2f\u6027\u7684\u540c\u65f6\u5f3a\u5236\u6267\u884c\u5e97\u94fa\u7ea6\u675f\u3002", "result": "\u4eba\u5de5\u8bc4\u4f302000\u4e2a\u5408\u6210\u4ea7\u54c1\u663e\u793a\u9ad8\u5ea6\u6709\u6548\u6027\uff0c99.6%\u88ab\u8bc4\u4e3a\u81ea\u7136\uff0c96.5%\u5305\u542b\u6709\u6548\u5c5e\u6027\u503c\uff0c\u8d85\u8fc790%\u663e\u793a\u4e00\u81f4\u7684\u5c5e\u6027\u4f7f\u7528\uff1b\u5728\u516c\u5f00MAVE\u6570\u636e\u96c6\u4e0a\uff0c\u5408\u6210\u6570\u636e\u8fbe\u523060.5%\u51c6\u786e\u7387\uff0c\u4e0e\u771f\u5b9e\u8bad\u7ec3\u6570\u636e\uff0860.8%\uff09\u8868\u73b0\u76f8\u5f53\uff0c\u663e\u8457\u4f18\u4e8e13.4%\u7684\u96f6\u6837\u672c\u57fa\u7ebf\uff1b\u7ed3\u5408\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u7684\u6df7\u5408\u914d\u7f6e\u8fdb\u4e00\u6b65\u5c06\u6027\u80fd\u63d0\u5347\u81f368.8%\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u589e\u5f3a\u7535\u5546\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u573a\u666f\uff0c\u8bc1\u660e\u4e86\u5408\u6210\u6570\u636e\u5728\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\u80fd\u591f\u8fbe\u5230\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u6df7\u5408\u914d\u7f6e\u7684\u8fdb\u4e00\u6b65\u6539\u8fdb\u8868\u660e\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u7ed3\u5408\u5177\u6709\u534f\u540c\u6548\u5e94\u3002"}}
{"id": "2601.04442", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04442", "abs": "https://arxiv.org/abs/2601.04442", "authors": ["Xingjian Diao", "Zheyuan Liu", "Chunhui Zhang", "Weiyi Wu", "Keyi Kong", "Lin Shi", "Kaize Ding", "Soroush Vosoughi", "Jiang Gui"], "title": "Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGPRO\uff08\u95e8\u63a7\u611f\u77e5-\u63a8\u7406\u4f18\u5316\uff09\uff0c\u4e00\u79cd\u5143\u63a8\u7406\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u8ba1\u7b97\u8def\u5f84\u6765\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u8fc7\u5ea6\u601d\u8003\u7684\u95ee\u9898\uff0c\u5728\u63d0\u5347\u4efb\u52a1\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u601d\u7ef4\u94fe\u673a\u5236\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd9\u7c7b\u6162\u601d\u8003\u65b9\u6cd5\u5e38\u5bfc\u81f4\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5373\u6a21\u578b\u5bf9\u7b80\u5355\u67e5\u8be2\u751f\u6210\u8fc7\u4e8e\u5197\u957f\u7684\u54cd\u5e94\uff0c\u9020\u6210\u6d4b\u8bd5\u65f6\u6548\u7387\u4f4e\u4e0b\u751a\u81f3\u51c6\u786e\u7387\u4e0b\u964d\u3002\u5148\u524d\u7814\u7a76\u5c1d\u8bd5\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u7406\u7b56\u7565\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5927\u591a\u5ffd\u89c6\u4e86\u4e00\u4e2a\u6839\u672c\u74f6\u9888\uff1a\u89c6\u89c9\u611f\u77e5\u5931\u8d25\u3002\u6211\u4eec\u8ba4\u4e3a\u7a33\u5b9a\u63a8\u7406\u5173\u952e\u4f9d\u8d56\u4e8e\u4f4e\u5c42\u6b21\u89c6\u89c9\u57fa\u7840\uff0c\u63a8\u7406\u9519\u8bef\u5f80\u5f80\u6e90\u4e8e\u4e0d\u5b8c\u7f8e\u7684\u611f\u77e5\u800c\u975e\u4e0d\u8db3\u7684\u6df1\u601d\u719f\u8651\u3002", "method": "\u4e3a\u89e3\u51b3\u4e0a\u8ff0\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51faGPRO\uff08\u95e8\u63a7\u611f\u77e5-\u63a8\u7406\u4f18\u5316\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5143\u63a8\u7406\u63a7\u5236\u5668\uff0c\u5728\u6bcf\u4e2a\u751f\u6210\u6b65\u9aa4\u52a8\u6001\u8def\u7531\u8ba1\u7b97\u5230\u4e09\u4e2a\u51b3\u7b56\u8def\u5f84\uff1a\u8f7b\u91cf\u7ea7\u5feb\u901f\u8def\u5f84\u3001\u7528\u4e8e\u91cd\u65b0\u68c0\u67e5\u89c6\u89c9\u8f93\u5165\u7684\u6162\u611f\u77e5\u8def\u5f84\uff0c\u4ee5\u53ca\u7528\u4e8e\u5185\u90e8\u81ea\u6211\u53cd\u601d\u7684\u6162\u63a8\u7406\u8def\u5f84\u3002\u4e3a\u5b66\u4e60\u8fd9\u79cd\u533a\u5206\uff0c\u6211\u4eec\u4ece\u7ea679\u4e07\u4e2a\u6837\u672c\u4e2d\u63a8\u5bfc\u51fa\u5927\u89c4\u6a21\u5931\u8d25\u5f52\u56e0\u76d1\u7763\uff0c\u4f7f\u7528\u6559\u5e08\u6a21\u578b\u6765\u533a\u5206\u611f\u77e5\u5e7b\u89c9\u4e0e\u63a8\u7406\u9519\u8bef\u3002\u7136\u540e\u901a\u8fc7\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63a7\u5236\u5668\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u4f18\u5316\u4efb\u52a1\u51c6\u786e\u7387\u4e0e\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGPRO\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u548c\u6548\u7387\uff0c\u4f18\u4e8e\u6700\u8fd1\u7684\u6162\u601d\u8003\u65b9\u6cd5\uff0c\u540c\u65f6\u751f\u6210\u660e\u663e\u66f4\u77ed\u7684\u54cd\u5e94\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u4efb\u52a1\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u5e73\u8861\u4f18\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u89c9\u611f\u77e5\u5931\u8d25\u662f\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9519\u8bef\u7684\u91cd\u8981\u6839\u6e90\uff0c\u800c\u975e\u4ec5\u4ec5\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002GPRO\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u5b9e\u73b0\u4e86\u611f\u77e5\u4e0e\u63a8\u7406\u7684\u534f\u540c\u4f18\u5316\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u7684\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u6574\u5408\u4f4e\u7ea7\u89c6\u89c9\u57fa\u7840\u4e0e\u9ad8\u7ea7\u8ba4\u77e5\u8fc7\u7a0b\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u81ea\u9002\u5e94\u8ba1\u7b97\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2601.04502", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04502", "abs": "https://arxiv.org/abs/2601.04502", "authors": ["Jingyi Wang", "Fanggang Wang"], "title": "Specific Emitter Identification via Active Learning", "comment": null, "summary": "With the rapid growth of wireless communications, specific emitter identification (SEI) is significant for communication security. However, its model training relies heavily on the large-scale labeled data, which are costly and time-consuming to obtain. To address this challenge, we propose an SEI approach enhanced by active learning (AL), which follows a three-stage semi-supervised training scheme. In the first stage, self-supervised contrastive learning is employed with a dynamic dictionary update mechanism to extract robust representations from large amounts of the unlabeled data. In the second stage, supervised training on a small labeled dataset is performed, where the contrastive and cross-entropy losses are jointly optimized to improve the feature separability and strengthen the classification boundaries. In the third stage, an AL module selects the most valuable samples from the unlabeled data for annotation based on the uncertainty and representativeness criteria, further enhancing generalization under limited labeling budgets. Experimental results on the ADS-B and WiFi datasets demonstrate that the proposed SEI approach significantly outperforms the conventional supervised and semi-supervised methods under limited annotation conditions, achieving higher recognition accuracy with lower labeling cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u7279\u5b9a\u8f90\u5c04\u6e90\u8bc6\u522b\u65b9\u6cd5\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u534a\u76d1\u7763\u8bad\u7ec3\u65b9\u6848\uff0c\u5728\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u7279\u5b9a\u8f90\u5c04\u6e90\u8bc6\u522b\u5728\u65e0\u7ebf\u901a\u4fe1\u5b89\u5168\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u6a21\u578b\u8bad\u7ec3\u4e25\u91cd\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u9650\u6807\u6ce8\u6761\u4ef6\u4e0b\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u5e76\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u4e09\u9636\u6bb5\u534a\u76d1\u7763\u8bad\u7ec3\u65b9\u6848\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5e26\u52a8\u6001\u5b57\u5178\u66f4\u65b0\u673a\u5236\u7684\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u4ece\u672a\u6807\u6ce8\u6570\u636e\u4e2d\u63d0\u53d6\u9c81\u68d2\u8868\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\uff0c\u8054\u5408\u4f18\u5316\u5bf9\u6bd4\u635f\u5931\u548c\u4ea4\u53c9\u71b5\u635f\u5931\u4ee5\u589e\u5f3a\u7279\u5f81\u53ef\u5206\u6027\u548c\u5206\u7c7b\u8fb9\u754c\uff1b\u7b2c\u4e09\u9636\u6bb5\u5f15\u5165\u4e3b\u52a8\u5b66\u4e60\u6a21\u5757\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u548c\u4ee3\u8868\u6027\u51c6\u5219\u4ece\u672a\u6807\u6ce8\u6570\u636e\u4e2d\u9009\u62e9\u6700\u6709\u4ef7\u503c\u7684\u6837\u672c\u8fdb\u884c\u6807\u6ce8\uff0c\u5728\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u8fdb\u4e00\u6b65\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728ADS-B\u548cWiFi\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u6807\u6ce8\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u76d1\u7763\u548c\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8bc6\u522b\u51c6\u786e\u7387\u548c\u66f4\u4f4e\u7684\u6807\u6ce8\u6210\u672c\u3002\u5177\u4f53\u6027\u80fd\u63d0\u5347\u4f53\u73b0\u5728\u4e0d\u540c\u6807\u6ce8\u6bd4\u4f8b\u4e0b\u7684\u7a33\u5b9a\u4f18\u52bf\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u4e09\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\u548c\u4e3b\u52a8\u5b66\u4e60\u9009\u62e9\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u3001\u534a\u76d1\u7763\u8bad\u7ec3\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u591a\u9636\u6bb5\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7279\u5b9a\u8f90\u5c04\u6e90\u8bc6\u522b\u4e2d\u7684\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4e3a\u901a\u4fe1\u5b89\u5168\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5728\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u901a\u8fc7\u667a\u80fd\u6837\u672c\u9009\u62e9\u548c\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u7b56\u7565\u5b9e\u73b0\u9ad8\u6027\u80fd\u8bc6\u522b\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.04202", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04202", "abs": "https://arxiv.org/abs/2601.04202", "authors": ["Anas Ezzakri", "Nicola Piovesan", "Mohamed Sana", "Antonio De Domenico", "Fadhel Ayed", "Haozhe Zhang"], "title": "TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation", "comment": null, "summary": "Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TeleTables\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7535\u4fe1\u6280\u672f\u89c4\u8303\u4e2d\u5bf9\u8868\u683c\u7684\u9690\u5f0f\u77e5\u8bc6\u548c\u663e\u5f0f\u89e3\u91ca\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5904\u74063GPP\u6807\u51c6\u4e2d\u5bc6\u96c6\u8868\u683c\u4fe1\u606f\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u9886\u57df\u4e13\u4e1a\u5316\u5fae\u8c03\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7535\u4fe1\u884c\u4e1a\u5e94\u7528\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u5904\u74063GPP\u6280\u672f\u89c4\u8303\u65f6\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u6807\u51c6\u4e2d\u5bc6\u96c6\u5305\u542b\u5927\u91cf\u8868\u683c\u5448\u73b0\u5173\u952e\u4fe1\u606f\uff0c\u4f46\u6a21\u578b\u5bf9\u8fd9\u4e9b\u8868\u683c\u7684\u77e5\u8bc6\u50a8\u5907\u548c\u89e3\u91ca\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u8bc4\u4f30\uff0c\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u963b\u788d\u4e86LLM\u5728\u7535\u4fe1\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u5e94\u7528\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86TeleTables\u57fa\u51c6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u591a\u9636\u6bb5\u6570\u636e\u751f\u6210\u6d41\u7a0b\u6784\u5efa\u8bc4\u4f30\u6570\u636e\u96c6\u3002\u8be5\u65b9\u6cd5\u4ece3GPP\u6807\u51c6\u4e2d\u63d0\u53d6\u8868\u683c\uff0c\u5229\u7528\u591a\u6a21\u6001\u548c\u63a8\u7406\u5bfc\u5411\u7684\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e76\u9a8c\u8bc1\u95ee\u9898\uff0c\u6700\u7ec8\u521b\u5efa\u4e86\u5305\u542b500\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684\u95ee\u7b54\u5bf9\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u95ee\u9898\u90fd\u5173\u8054\u591a\u79cd\u683c\u5f0f\u7684\u5bf9\u5e94\u8868\u683c\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u8f83\u5c0f\u6a21\u578b\uff08\u53c2\u6570\u5c11\u4e8e100\u4ebf\uff09\u57283GPP\u77e5\u8bc6\u56de\u5fc6\u548c\u8868\u683c\u89e3\u91ca\u65b9\u9762\u5747\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u5176\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7535\u4fe1\u6807\u51c6\u66b4\u9732\u4e0d\u8db3\u4e14\u7f3a\u4e4f\u5904\u7406\u590d\u6742\u6280\u672f\u6750\u6599\u7684\u5f52\u7eb3\u504f\u7f6e\u3002\u8f83\u5927\u6a21\u578b\u5728\u8868\u683c\u89e3\u91ca\u65b9\u9762\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u6574\u4f53\u8868\u73b0\u4ecd\u663e\u793a\u9886\u57df\u4e13\u4e1a\u5316\u4e0d\u8db3\u3002", "conclusion": "TeleTables\u57fa\u51c6\u63ed\u793a\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7535\u4fe1\u9886\u57df\u6280\u672f\u89c4\u8303\u5904\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5bf9\u8868\u683c\u4fe1\u606f\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u9886\u57df\u4e13\u4e1a\u5316\u5fae\u8c03\u5bf9\u4e8e\u53ef\u9760\u89e3\u91ca\u548c\u63a8\u7406\u7535\u4fe1\u6807\u51c6\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u7535\u4fe1\u4e13\u7528\u6a21\u578b\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u65b9\u5411\u6307\u5bfc\u3002"}}
{"id": "2601.04453", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04453", "abs": "https://arxiv.org/abs/2601.04453", "authors": ["Zhexiao Xiong", "Xin Ye", "Burhan Yaman", "Sheng Cheng", "Yiren Lu", "Jingru Luo", "Nathan Jacobs", "Liu Ren"], "title": "UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving", "comment": "Project Page: https://unidrive-wm.github.io/UniDrive-WM", "summary": "World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUniDrive-WM\uff0c\u4e00\u79cd\u7edf\u4e00\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e16\u754c\u6a21\u578b\uff0c\u5c06\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u3001\u8f68\u8ff9\u89c4\u5212\u548c\u8f68\u8ff9\u6761\u4ef6\u672a\u6765\u56fe\u50cf\u751f\u6210\u96c6\u6210\u5230\u5355\u4e00\u67b6\u6784\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u901a\u5e38\u5c06\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4f5c\u4e3a\u72ec\u7acb\u6a21\u5757\u5904\u7406\uff0c\u8fd9\u79cd\u5206\u79bb\u9650\u5236\u4e86\u7cfb\u7edf\u6027\u80fd\u3002\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5212\u65b9\u6cd5\u672a\u80fd\u5b9e\u73b0\u8fd9\u4e9b\u529f\u80fd\u7684\u7edf\u4e00\u96c6\u6210\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8054\u5408\u6267\u884c\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u3001\u8f68\u8ff9\u89c4\u5212\u548c\u672a\u6765\u56fe\u50cf\u751f\u6210\u7684\u7edf\u4e00\u4e16\u754c\u6a21\u578b\u3002", "method": "UniDrive-WM\u91c7\u7528\u7edf\u4e00\u7684VLM\u67b6\u6784\uff0c\u5176\u8f68\u8ff9\u89c4\u5212\u5668\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\uff0c\u8be5\u8f68\u8ff9\u968f\u540e\u6761\u4ef6\u5316VLM\u56fe\u50cf\u751f\u6210\u5668\u4ee5\u751f\u6210\u5408\u7406\u7684\u672a\u6765\u5e27\u3002\u8fd9\u4e9b\u9884\u6d4b\u63d0\u4f9b\u989d\u5916\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u589e\u5f3a\u573a\u666f\u7406\u89e3\u5e76\u8fed\u4ee3\u4f18\u5316\u8f68\u8ff9\u751f\u6210\u3002\u7814\u7a76\u8fd8\u6bd4\u8f83\u4e86\u672a\u6765\u56fe\u50cf\u9884\u6d4b\u7684\u79bb\u6563\u548c\u8fde\u7eed\u8f93\u51fa\u8868\u793a\uff0c\u5206\u6790\u5b83\u4eec\u5bf9\u4e0b\u6e38\u9a7e\u9a76\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniDrive-WM\u751f\u6210\u9ad8\u4fdd\u771f\u672a\u6765\u56fe\u50cf\uff0c\u5e76\u5c06\u89c4\u5212\u6027\u80fd\u63d0\u53475.9%\u7684L2\u8f68\u8ff9\u8bef\u5dee\u548c9.2%\u7684\u78b0\u649e\u7387\uff0c\u4f18\u4e8e\u5148\u524d\u6700\u4f73\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u5728\u8054\u5408\u63a8\u7406\u3001\u89c4\u5212\u548c\u751f\u6210\u5efa\u6a21\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7d27\u5bc6\u96c6\u6210VLM\u9a71\u52a8\u7684\u63a8\u7406\u3001\u89c4\u5212\u548c\u751f\u6210\u4e16\u754c\u5efa\u6a21\u5bf9\u81ea\u52a8\u9a7e\u9a76\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002\u7edf\u4e00\u67b6\u6784\u901a\u8fc7\u8f68\u8ff9\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u989d\u5916\u76d1\u7763\u4fe1\u53f7\uff0c\u8fed\u4ee3\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.04518", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04518", "abs": "https://arxiv.org/abs/2601.04518", "authors": ["Shogo Nakayama", "Masahiro Okuda"], "title": "Integrating Distribution Matching into Semi-Supervised Contrastive Learning for Labeled and Unlabeled Data", "comment": "ITC-CSCC accepted", "summary": "The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u5e03\u5339\u914d\u7684\u4f2a\u6807\u7b7e\u534a\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u6807\u8bb0\u6570\u636e\u4e0e\u672a\u6807\u8bb0\u6570\u636e\u7684\u7279\u5f81\u5206\u5e03\u6765\u63d0\u5347\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7684\u76d1\u7763\u56fe\u50cf\u5206\u7c7b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u5b9e\u9645\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u800c\u5b8c\u5168\u65e0\u6807\u6ce8\u7684\u573a\u666f\u8f83\u5c11\uff0c\u534a\u76d1\u7763\u5b66\u4e60\u5728\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4e0e\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u5171\u5b58\u7684\u60c5\u51b5\u4e0b\u66f4\u5177\u5b9e\u9645\u610f\u4e49\u3002\u73b0\u6709\u57fa\u4e8e\u4f2a\u6807\u7b7e\u7684\u534a\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u5e03\u5339\u914d\u6280\u672f\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u4ece\u800c\u6539\u5584\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u8be5\u65b9\u6cd5\u5728\u4f2a\u6807\u7b7e\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4e2d\u5f15\u5165\u4e86\u5206\u5e03\u5339\u914d\u673a\u5236\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u6807\u8bb0\u6570\u636e\u4e0e\u672a\u6807\u8bb0\u6570\u636e\u7684\u7279\u5f81\u5d4c\u5165\u5206\u5e03\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u65b9\u6cd5\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5f3a\u5236\u6807\u8bb0\u6837\u672c\u4e0e\u672a\u6807\u8bb0\u6837\u672c\u7684\u5206\u5e03\u4e00\u81f4\u6027\uff0c\u5229\u7528\u5206\u5e03\u5339\u914d\u635f\u5931\u51fd\u6570\u4f18\u5316\u7279\u5f81\u8868\u793a\uff0c\u4ece\u800c\u751f\u6210\u66f4\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u5728\u591a\u4e2a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u51c6\u786e\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u4f2a\u6807\u7b7e\u65b9\u6cd5\u548c\u57fa\u51c6\u534a\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u3002\u901a\u8fc7\u5206\u5e03\u5339\u914d\u673a\u5236\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u534a\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u5f15\u5165\u5206\u5e03\u5339\u914d\u673a\u5236\u80fd\u591f\u6709\u6548\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u4ece\u800c\u6539\u5584\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002\u8fd9\u4e00\u65b9\u6cd5\u4e3a\u534a\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u590d\u6742\u7684\u5206\u5e03\u5bf9\u9f50\u7b56\u7565\u3002"}}
{"id": "2601.04203", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.04203", "abs": "https://arxiv.org/abs/2601.04203", "authors": ["Xueqing Wu", "Zihan Xue", "Da Yin", "Shuyan Zhou", "Kai-Wei Chang", "Nanyun Peng", "Yeming Wen"], "title": "FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback", "comment": null, "summary": "We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FronTalk\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u524d\u7aef\u4ee3\u7801\u751f\u6210\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\u591a\u6a21\u6001\u53cd\u9988\u7684\u5bf9\u8bdd\u5f0f\u4ee3\u7801\u751f\u6210\u8fd9\u4e00\u72ec\u7279\u4ea4\u4e92\u52a8\u6001\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u7279\u5f81\u9057\u5fd8\u548c\u89c6\u89c9\u53cd\u9988\u7406\u89e3\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u6311\u6218\u3002", "motivation": "\u524d\u7aef\u5f00\u53d1\u4e2d\uff0c\u8349\u56fe\u3001\u7ebf\u6846\u56fe\u548c\u6807\u6ce8\u622a\u56fe\u7b49\u89c6\u89c9\u5de5\u4ef6\u5bf9\u4e8e\u4f20\u8fbe\u8bbe\u8ba1\u610f\u56fe\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b83\u4eec\u5728\u591a\u8f6e\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u4f5c\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u7279\u522b\u5173\u6ce8\u524d\u7aef\u5f00\u53d1\u4efb\u52a1\u4e2d\u591a\u6a21\u6001\u53cd\u9988\u7684\u5bf9\u8bdd\u5f0f\u4ee3\u7801\u751f\u6210\u8fd9\u4e00\u72ec\u7279\u4ea4\u4e92\u52a8\u6001\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86FronTalk\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b100\u4e2a\u4ece\u65b0\u95fb\u3001\u91d1\u878d\u548c\u827a\u672f\u7b49\u4e0d\u540c\u9886\u57df\u771f\u5b9e\u7f51\u7ad9\u63d0\u53d6\u7684\u591a\u8f6e\u5bf9\u8bdd\u3002\u6bcf\u4e2a\u5bf9\u8bdd\u8f6e\u6b21\u540c\u65f6\u5305\u542b\u6587\u672c\u6307\u4ee4\u548c\u7b49\u6548\u7684\u89c6\u89c9\u6307\u4ee4\uff0c\u4ee3\u8868\u76f8\u540c\u7684\u7528\u6237\u610f\u56fe\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u57fa\u4e8e\u4ee3\u7406\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528Web\u4ee3\u7406\u6a21\u62df\u7528\u6237\u63a2\u7d22\u7f51\u7ad9\uff0c\u4ece\u800c\u540c\u65f6\u8861\u91cf\u529f\u80fd\u6b63\u786e\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002\u9488\u5bf9\u7279\u5f81\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u51fa\u4e86AceCoder\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u4e3bWeb\u4ee3\u7406\u5bf9\u6bcf\u4e2a\u8fc7\u53bb\u6307\u4ee4\u7684\u5b9e\u73b0\u8fdb\u884c\u6279\u5224\u6027\u5206\u6790\u3002", "result": "\u5bf920\u4e2a\u6a21\u578b\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u6a21\u578b\u4f1a\u8986\u76d6\u5148\u524d\u5b9e\u73b0\u7684\u529f\u80fd\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u7684\u7279\u5f81\u9057\u5fd8\u95ee\u9898\uff0c\u4ee5\u53ca\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u91ca\u89c6\u89c9\u53cd\u9988\u65b9\u9762\u7684\u6301\u7eed\u56f0\u96be\u3002\u63d0\u51fa\u7684AceCoder\u57fa\u7ebf\u65b9\u6cd5\u5c06\u7279\u5f81\u9057\u5fd8\u7387\u663e\u8457\u964d\u4f4e\u81f3\u63a5\u8fd1\u96f6\uff0c\u5e76\u5c06\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe9.3%\uff08\u4ece56.0%\u63d0\u5347\u81f365.3%\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u524d\u7aef\u5f00\u53d1\u548c\u591a\u8f6e\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u7684\u4e00\u822c\u4ea4\u4e92\u52a8\u6001\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7279\u5f81\u9057\u5fd8\u548c\u89c6\u89c9\u53cd\u9988\u7406\u89e3\u662f\u5f53\u524d\u6a21\u578b\u9762\u4e34\u7684\u91cd\u8981\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6846\u67b6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u548c\u65b9\u5411\u3002"}}
{"id": "2601.04497", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04497", "abs": "https://arxiv.org/abs/2601.04497", "authors": ["James Brock", "Ce Zhang", "Nantheera Anantrasirichai"], "title": "Vision-Language Agents for Interactive Forest Change Analysis", "comment": "5 pages, 4 figures, Submitted to IGARSS 2026", "summary": "Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u96c6\u6210\u68ee\u6797\u53d8\u5316\u5206\u6790\u667a\u80fd\u4f53\uff0c\u652f\u6301\u8de8\u591a\u4efb\u52a1\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u5e76\u5f15\u5165\u4e86\u5305\u542b\u591a\u7c92\u5ea6\u8bed\u4e49\u53d8\u5316\u63cf\u8ff0\u7684Forest-Change\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68ee\u6797\u53d8\u5316\u76d1\u6d4b\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u68ee\u6797\u76d1\u6d4b\u9762\u4e34\u50cf\u7d20\u7ea7\u53d8\u5316\u68c0\u6d4b\u548c\u590d\u6742\u68ee\u6797\u52a8\u6001\u8bed\u4e49\u53d8\u5316\u63cf\u8ff0\u7684\u6301\u7eed\u6311\u6218\uff0c\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u6b63\u88ab\u7528\u4e8e\u4ea4\u4e92\u5f0f\u6570\u636e\u63a2\u7d22\uff0c\u4f46\u5176\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9065\u611f\u56fe\u50cf\u53d8\u5316\u89e3\u91ca\u9886\u57df\u7684\u96c6\u6210\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u96c6\u6210\u68ee\u6797\u53d8\u5316\u5206\u6790\u667a\u80fd\u4f53\uff0c\u8be5\u7cfb\u7edf\u6784\u5efa\u5728\u591a\u7ea7\u53d8\u5316\u89e3\u91ca\u89c6\u89c9\u8bed\u8a00\u9aa8\u5e72\u7f51\u7edc\u4e4b\u4e0a\uff0c\u91c7\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4efb\u52a1\u7f16\u6392\uff0c\u5e76\u5f15\u5165\u4e86Forest-Change\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u53cc\u65f6\u76f8\u536b\u661f\u5f71\u50cf\u3001\u50cf\u7d20\u7ea7\u53d8\u5316\u63a9\u7801\u4ee5\u53ca\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\u548c\u89c4\u5219\u65b9\u6cd5\u751f\u6210\u7684\u591a\u7c92\u5ea6\u8bed\u4e49\u53d8\u5316\u63cf\u8ff0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7cfb\u7edf\u5728Forest-Change\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8667.10%\u7684mIoU\u548c40.17%\u7684BLEU-4\u5206\u6570\uff0c\u5728LEVIR-MCI-Trees\u57fa\u51c6\uff08\u4e13\u6ce8\u4e8e\u6811\u6728\u7684\u8054\u5408\u53d8\u5316\u68c0\u6d4b\u4e0e\u63cf\u8ff0\u5b50\u96c6\uff09\u4e0a\u5206\u522b\u8fbe\u523088.13%\u548c34.41%\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4ea4\u4e92\u5f0f\u3001\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u9065\u611f\u56fe\u50cf\u53d8\u5316\u89e3\u91ca\u7cfb\u7edf\u5728\u63d0\u5347\u68ee\u6797\u53d8\u5316\u5206\u6790\u53ef\u8bbf\u95ee\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u9065\u611f\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u4ea4\u53c9\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u6846\u67b6\uff0c\u6240\u6709\u6570\u636e\u548c\u4ee3\u7801\u5747\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2601.04566", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04566", "abs": "https://arxiv.org/abs/2601.04566", "authors": ["Yunhao Feng", "Yige Li", "Yutao Wu", "Yingshui Tan", "Yanming Guo", "Yifan Ding", "Kun Zhai", "Xingjun Ma", "Yugang Jiang"], "title": "BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents", "comment": null, "summary": "Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \\textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \\textbf{planning attacks}, \\textbf{memory attacks}, and \\textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \\textbf{Agent QA}, \\textbf{Agent Code}, \\textbf{Agent Web}, and \\textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \\textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\\% of planning attacks, 77.97\\% of memory attacks, and 60.28\\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BackdoorAgent\u6846\u67b6\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u4e2d\u7684\u540e\u95e8\u5a01\u80c1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u3001\u9762\u5411\u667a\u80fd\u4f53\u7684\u89c6\u89d2\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u3001\u9636\u6bb5\u611f\u77e5\u7684\u8bbe\u8ba1\u7cfb\u7edf\u5206\u6790\u4e86\u540e\u95e8\u89e6\u53d1\u5668\u5728\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e0d\u540c\u9636\u6bb5\u95f4\u7684\u6fc0\u6d3b\u4e0e\u4f20\u64ad\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9LLM\u667a\u80fd\u4f53\u4e2d\u7684\u540e\u95e8\u5a01\u80c1\u5206\u6790\u8f83\u4e3a\u96f6\u6563\uff0c\u901a\u5e38\u5b64\u7acb\u5730\u5206\u6790\u5355\u4e2a\u653b\u51fb\u5411\u91cf\uff0c\u7f3a\u4e4f\u4ece\u667a\u80fd\u4f53\u89d2\u5ea6\u7406\u89e3\u540e\u95e8\u89e6\u53d1\u5668\u5728\u4e0d\u540c\u9636\u6bb5\u95f4\u7684\u4ea4\u4e92\u4e0e\u4f20\u64ad\u673a\u5236\uff0c\u8fd9\u9650\u5236\u4e86\u6211\u4eec\u5bf9\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5b89\u5168\u6f0f\u6d1e\u7684\u7cfb\u7edf\u6027\u8ba4\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86BackdoorAgent\u6846\u67b6\uff0c\u5c06\u653b\u51fb\u9762\u7ed3\u6784\u5316\u4e3a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u4e09\u4e2a\u529f\u80fd\u9636\u6bb5\uff1a\u89c4\u5212\u653b\u51fb\u3001\u8bb0\u5fc6\u653b\u51fb\u548c\u5de5\u5177\u4f7f\u7528\u653b\u51fb\uff0c\u5e76\u901a\u8fc7\u68c0\u6d4b\u667a\u80fd\u4f53\u6267\u884c\u8fc7\u7a0b\u6765\u7cfb\u7edf\u5206\u6790\u89e6\u53d1\u5668\u5728\u4e0d\u540c\u9636\u6bb5\u7684\u6fc0\u6d3b\u4e0e\u4f20\u64ad\uff0c\u6784\u5efa\u4e86\u6db5\u76d6Agent QA\u3001Agent Code\u3001Agent Web\u548cAgent Drive\u56db\u4e2a\u4ee3\u8868\u6027\u5e94\u7528\u7684\u6807\u51c6\u5316\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u5206\u6790\u8868\u660e\uff0c\u690d\u5165\u5355\u4e2a\u9636\u6bb5\u7684\u89e6\u53d1\u5668\u53ef\u4ee5\u8de8\u591a\u4e2a\u6b65\u9aa4\u6301\u7eed\u5b58\u5728\u5e76\u901a\u8fc7\u4e2d\u95f4\u72b6\u6001\u4f20\u64ad\uff0c\u5728\u4f7f\u7528GPT\u57fa\u5ea7\u6a21\u578b\u65f6\uff0c\u89c4\u5212\u653b\u51fb\u7684\u89e6\u53d1\u5668\u6301\u7eed\u7387\u4e3a43.58%\uff0c\u8bb0\u5fc6\u653b\u51fb\u4e3a77.97%\uff0c\u5de5\u5177\u9636\u6bb5\u653b\u51fb\u4e3a60.28%\uff0c\u51f8\u663e\u4e86\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u672c\u8eab\u5bf9\u540e\u95e8\u5a01\u80c1\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86LLM\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u540e\u95e8\u5a01\u80c1\u7684\u7cfb\u7edf\u6027\u98ce\u9669\uff0c\u5f3a\u8c03\u4e86\u8de8\u9636\u6bb5\u89e6\u53d1\u5668\u4f20\u64ad\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u548c\u57fa\u51c6\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7840\uff0c\u5e76\u6307\u51fa\u4e86\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u6765\u4fdd\u62a4\u591a\u9636\u6bb5\u667a\u80fd\u4f53\u7cfb\u7edf\u3002"}}
{"id": "2601.04350", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04350", "abs": "https://arxiv.org/abs/2601.04350", "authors": ["Joseph James", "Chenghao Xiao", "Yucheng Li", "Nafise Sadat Moosavi", "Chenghua Lin"], "title": "RIGOURATE: Quantifying Scientific Exaggeration with Evidence-Aligned Claim Evaluation", "comment": null, "summary": "Scientific rigour tends to be sidelined in favour of bold statements, leading authors to overstate claims beyond what their results support. We present RIGOURATE, a two-stage multimodal framework that retrieves supporting evidence from a paper's body and assigns each claim an overstatement score. The framework consists of a dataset of over 10K claim-evidence sets from ICLR and NeurIPS papers, annotated using eight LLMs, with overstatement scores calibrated using peer-review comments and validated through human evaluation. It employes a fine-tuned reranker for evidence retrieval and a fine-tuned model to predict overstatement scores with justification. Compared to strong baselines, RIGOURATE enables improved evidence retrieval and overstatement detection. Overall, our work operationalises evidential proportionality and supports clearer, more transparent scientific communication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RIGOURATE\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u8bba\u6587\u6b63\u6587\u4e2d\u68c0\u7d22\u652f\u6301\u8bc1\u636e\u5e76\u4e3a\u6bcf\u4e2a\u4e3b\u5f20\u5206\u914d\u5938\u5927\u9648\u8ff0\u5206\u6570\uff0c\u65e8\u5728\u64cd\u4f5c\u5316\u8bc1\u636e\u6bd4\u4f8b\u6027\u5e76\u652f\u6301\u66f4\u6e05\u6670\u3001\u900f\u660e\u7684\u79d1\u5b66\u4ea4\u6d41\u3002", "motivation": "\u79d1\u5b66\u4e25\u8c28\u6027\u5f80\u5f80\u88ab\u8fb9\u7f18\u5316\uff0c\u4f5c\u8005\u503e\u5411\u4e8e\u505a\u51fa\u8d85\u51fa\u5176\u7814\u7a76\u7ed3\u679c\u652f\u6301\u7684\u5938\u5927\u9648\u8ff0\uff0c\u8fd9\u5bfc\u81f4\u4e86\u79d1\u5b66\u4ea4\u6d41\u4e2d\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u8bc1\u636e\u6bd4\u4f8b\u6027\u7684\u95ee\u9898\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5305\u62ec\u4eceICLR\u548cNeurIPS\u8bba\u6587\u4e2d\u6784\u5efa\u7684\u8d85\u8fc710K\u4e2a\u4e3b\u5f20-\u8bc1\u636e\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u516b\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6807\u6ce8\uff0c\u5e76\u901a\u8fc7\u540c\u884c\u8bc4\u5ba1\u8bc4\u8bba\u6821\u51c6\u5938\u5927\u9648\u8ff0\u5206\u6570\u3002\u6846\u67b6\u91c7\u7528\u5fae\u8c03\u7684\u91cd\u65b0\u6392\u5e8f\u5668\u8fdb\u884c\u8bc1\u636e\u68c0\u7d22\uff0c\u4ee5\u53ca\u5fae\u8c03\u6a21\u578b\u6765\u9884\u6d4b\u5e26\u6709\u7406\u7531\u7684\u5938\u5927\u9648\u8ff0\u5206\u6570\u3002", "result": "\u4e0e\u5f3a\u57fa\u7ebf\u76f8\u6bd4\uff0cRIGOURATE\u5728\u8bc1\u636e\u68c0\u7d22\u548c\u5938\u5927\u9648\u8ff0\u68c0\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e86\u6539\u8fdb\u6027\u80fd\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u79d1\u5b66\u8bba\u6587\u4e2d\u8bc6\u522b\u5938\u5927\u4e3b\u5f20\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u64cd\u4f5c\u5316\u4e86\u8bc1\u636e\u6bd4\u4f8b\u6027\u7684\u6982\u5ff5\uff0c\u4e3a\u66f4\u6e05\u6670\u3001\u900f\u660e\u7684\u79d1\u5b66\u4ea4\u6d41\u63d0\u4f9b\u4e86\u652f\u6301\u5de5\u5177\u3002\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u51cf\u5c11\u79d1\u5b66\u8bba\u6587\u4e2d\u7684\u5938\u5927\u9648\u8ff0\uff0c\u4fc3\u8fdb\u57fa\u4e8e\u8bc1\u636e\u7684\u79d1\u5b66\u8bba\u8bc1\u548c\u66f4\u4e25\u8c28\u7684\u5b66\u672f\u4ea4\u6d41\u5b9e\u8df5\u3002"}}
{"id": "2601.04567", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04567", "abs": "https://arxiv.org/abs/2601.04567", "authors": ["Ziyou Jiang", "Mingyang Li", "Junjie Wang", "Yuekai Huang", "Jie Huang", "Zhiyuan Chang", "Zhaoyang Li", "Qing Wang"], "title": "All Changes May Have Invariant Principles: Improving Ever-Shifting Harmful Meme Detection via Design Concept Reproduction", "comment": "18 pages, 11 figures", "summary": "Harmful memes are ever-shifting in the Internet communities, which are difficult to analyze due to their type-shifting and temporal-evolving nature. Although these memes are shifting, we find that different memes may share invariant principles, i.e., the underlying design concept of malicious users, which can help us analyze why these memes are harmful. In this paper, we propose RepMD, an ever-shifting harmful meme detection method based on the design concept reproduction. We first refer to the attack tree to define the Design Concept Graph (DCG), which describes steps that people may take to design a harmful meme. Then, we derive the DCG from historical memes with design step reproduction and graph pruning. Finally, we use DCG to guide the Multimodal Large Language Model (MLLM) to detect harmful memes. The evaluation results show that RepMD achieves the highest accuracy with 81.1% and has slight accuracy decreases when generalized to type-shifting and temporal-evolving memes. Human evaluation shows that RepMD can improve the efficiency of human discovery on harmful memes, with 15$\\sim$30 seconds per meme.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRepMD\u65b9\u6cd5\uff0c\u901a\u8fc7\u518d\u73b0\u6076\u610f\u7528\u6237\u7684\u8bbe\u8ba1\u6982\u5ff5\u6765\u68c0\u6d4b\u4e0d\u65ad\u6f14\u53d8\u7684\u7f51\u7edc\u6709\u5bb3\u6a21\u56e0\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u8bbe\u8ba1\u6982\u5ff5\u56fe\u6307\u5bfc\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u68c0\u6d4b\uff0c\u5728\u7c7b\u578b\u6f14\u53d8\u548c\u65f6\u95f4\u6f14\u53d8\u7684\u6a21\u56e0\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7f51\u7edc\u6709\u5bb3\u6a21\u56e0\u5177\u6709\u7c7b\u578b\u6f14\u53d8\u548c\u65f6\u95f4\u6f14\u53d8\u7684\u7279\u6027\uff0c\u96be\u4ee5\u5206\u6790\u68c0\u6d4b\uff0c\u5c3d\u7ba1\u5177\u4f53\u6a21\u56e0\u4e0d\u65ad\u53d8\u5316\uff0c\u4f46\u4e0d\u540c\u6a21\u56e0\u53ef\u80fd\u5171\u4eab\u4e0d\u53d8\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u5373\u6076\u610f\u7528\u6237\u80cc\u540e\u7684\u8bbe\u8ba1\u6982\u5ff5\uff0c\u8fd9\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u56e0\u4e3a\u4f55\u6709\u5bb3\u5e76\u5b9e\u73b0\u6709\u6548\u68c0\u6d4b\u3002", "method": "RepMD\u65b9\u6cd5\u9996\u5148\u53c2\u8003\u653b\u51fb\u6811\u5b9a\u4e49\u8bbe\u8ba1\u6982\u5ff5\u56fe\u6765\u63cf\u8ff0\u8bbe\u8ba1\u6709\u5bb3\u6a21\u56e0\u7684\u6b65\u9aa4\uff0c\u7136\u540e\u901a\u8fc7\u8bbe\u8ba1\u6b65\u9aa4\u518d\u73b0\u548c\u56fe\u526a\u679d\u4ece\u5386\u53f2\u6a21\u56e0\u4e2d\u63a8\u5bfcDCG\uff0c\u6700\u540e\u4f7f\u7528DCG\u6307\u5bfc\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6709\u5bb3\u6a21\u56e0\u68c0\u6d4b\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793aRepMD\u8fbe\u523081.1%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u5728\u6cdb\u5316\u5230\u7c7b\u578b\u6f14\u53d8\u548c\u65f6\u95f4\u6f14\u53d8\u7684\u6a21\u56e0\u65f6\u51c6\u786e\u7387\u4ec5\u6709\u8f7b\u5fae\u4e0b\u964d\uff0c\u4eba\u5de5\u8bc4\u4f30\u8868\u660eRepMD\u80fd\u5c06\u4eba\u7c7b\u53d1\u73b0\u6709\u5bb3\u6a21\u56e0\u7684\u6548\u7387\u63d0\u9ad8\u5230\u6bcf\u6a21\u56e015-30\u79d2\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6355\u6349\u6076\u610f\u7528\u6237\u7684\u4e0d\u53d8\u8bbe\u8ba1\u6982\u5ff5\u800c\u975e\u5177\u4f53\u5185\u5bb9\uff0c\u4e3a\u68c0\u6d4b\u4e0d\u65ad\u6f14\u53d8\u7684\u7f51\u7edc\u6709\u5bb3\u6a21\u56e0\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u6982\u5ff5\u56fe\u6846\u67b6\u80fd\u591f\u6307\u5bfcMLLM\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u52a8\u6001\u7f51\u7edc\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.04571", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.04571", "abs": "https://arxiv.org/abs/2601.04571", "authors": ["Delong Zeng", "Yuexiang Xie", "Yaliang Li", "Ying Shen"], "title": "Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment", "comment": "Accepted by ACL'2025", "summary": "Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCIEA\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e92\u8865\u4fe1\u606f\u63d0\u53d6\u4e0e\u5bf9\u9f50\u673a\u5236\uff0c\u5c06\u6587\u672c\u548c\u56fe\u50cf\u8f6c\u6362\u5230\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u8bbe\u8ba1\u4e92\u8865\u4fe1\u606f\u63d0\u53d6\u5668\u6765\u8bc6\u522b\u548c\u4fdd\u7559\u56fe\u50cf\u8868\u793a\u4e2d\u7684\u5dee\u5f02\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u68c0\u7d22\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6355\u6349\u591a\u6a21\u6001\u6570\u636e\u4e2d\u4e0e\u914d\u5bf9\u6587\u672c\u76f8\u4f3c\u7684\u4fe1\u606f\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u6570\u636e\u4e2d\u5305\u542b\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u8fd9\u79cd\u4fe1\u606f\u7f3a\u5931\u9650\u5236\u4e86\u68c0\u7d22\u7cfb\u7edf\u7684\u5168\u9762\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "CIEA\u91c7\u7528\u4e92\u8865\u4fe1\u606f\u63d0\u53d6\u4e0e\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5c06\u6587\u6863\u4e2d\u7684\u6587\u672c\u548c\u56fe\u50cf\u8f6c\u6362\u5230\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\uff0c\u8bbe\u8ba1\u4e92\u8865\u4fe1\u606f\u63d0\u53d6\u5668\u6765\u8bc6\u522b\u548c\u4fdd\u7559\u56fe\u50cf\u8868\u793a\u4e2d\u7684\u5dee\u5f02\uff0c\u5e76\u4f7f\u7528\u4e24\u79cd\u4e92\u8865\u5bf9\u6bd4\u635f\u5931\u8fdb\u884c\u4f18\u5316\u4ee5\u786e\u4fdd\u8bed\u4e49\u5b8c\u6574\u6027\u5e76\u6709\u6548\u6355\u6349\u56fe\u50cf\u4e2d\u7684\u4e92\u8865\u4fe1\u606f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eCIEA\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u5206\u6cbb\u6a21\u578b\u548c\u901a\u7528\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u3001\u8fdb\u4e00\u6b65\u8ba8\u8bba\u548c\u6848\u4f8b\u7814\u7a76\u7a81\u51fa\u4e86CIEA\u6240\u53d6\u5f97\u7684\u8fdb\u5c55\uff0c\u6e90\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u6355\u6349\u4e92\u8865\u4fe1\u606f\u7684\u91cd\u8981\u6027\uff0cCIEA\u65b9\u6cd5\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5f00\u6e90\u4ee3\u7801\u5c06\u4fc3\u8fdb\u793e\u533a\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u7684\u540c\u65f6\u6709\u6548\u5229\u7528\u56fe\u50cf\u4e2d\u7684\u5dee\u5f02\u4fe1\u606f\uff0c\u4e3a\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2601.04548", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04548", "abs": "https://arxiv.org/abs/2601.04548", "authors": ["Wenjie Li", "Guansong Pang", "Hezhe Qiao", "Debin Gao", "David Lo"], "title": "Identifying Good and Bad Neurons for Task-Level Controllable LLMs", "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNeuronLLM\uff0c\u4e00\u79cd\u57fa\u4e8e\u529f\u80fd\u62ee\u6297\u539f\u7406\u7684\u4efb\u52a1\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4fc3\u8fdb\u4efb\u52a1\u5b8c\u6210\u7684\"\u597d\u795e\u7ecf\u5143\"\u548c\u6291\u5236\u4efb\u52a1\u7684\"\u574f\u795e\u7ecf\u5143\"\uff0c\u5b9e\u73b0\u5bf9LLM\u795e\u7ecf\u5143\u529f\u80fd\u7684\u5168\u9762\u5efa\u6a21\uff0c\u5e76\u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u795e\u7ecf\u5143\u53ef\u89e3\u91ca\u6027\u7684\u7814\u7a76\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u80fd\u529b\u7279\u5b9a\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u9700\u8981\u591a\u79cd\u80fd\u529b\u534f\u8c03\u7684\u4efb\u52a1\u573a\u666f\uff1b\u4ec5\u5173\u6ce8\u4e0e\u4efb\u52a1\u6b63\u76f8\u5173\u7684\u652f\u6301\u6027\u795e\u7ecf\u5143\uff0c\u5ffd\u7565\u6291\u5236\u6027\u795e\u7ecf\u5143\u7b49\u5176\u4ed6\u89d2\u8272\uff1b\u7531\u4e8eLLM\u7684\u5076\u7136\u6b63\u786e\u884c\u4e3a\u5bfc\u81f4\u795e\u7ecf\u5143\u5f52\u56e0\u9519\u8bef\uff0c\u8fd9\u4e9b\u95ee\u9898\u963b\u788d\u4e86\u5bf9LLM\u529f\u80fd\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "NeuronLLM\u91c7\u7528\u751f\u7269\u5b66\u529f\u80fd\u62ee\u6297\u539f\u7406\uff0c\u5c06\u4efb\u52a1\u6027\u80fd\u5efa\u6a21\u4e3a\u7531\u4fc3\u8fdb\u4efb\u52a1\u5b8c\u6210\u7684\"\u597d\u795e\u7ecf\u5143\"\u548c\u6291\u5236\u4efb\u52a1\u7684\"\u574f\u795e\u7ecf\u5143\"\u5171\u540c\u51b3\u5b9a\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u540c\u65f6\u5efa\u6a21\u8fd9\u4e24\u79cd\u5bf9\u7acb\u89d2\u8272\uff0c\u5e76\u5229\u7528\u589e\u5f3a\u95ee\u9898\u96c6\u6765\u51cf\u8f7bLLM\u4e2d\u7684\u5076\u7136\u6b63\u786e\u884c\u4e3a\uff0c\u5b9e\u73b0\u5bf9\u795e\u7ecf\u5143\u529f\u80fd\u7684\u5168\u9762\u8bc6\u522b\u548c\u5206\u6790\u3002", "result": "\u5728\u4e0d\u540c\u89c4\u6a21\u548c\u5bb6\u65cf\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cNeuronLLM\u5728\u56db\u4e2aNLP\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u795e\u7ecf\u5143\u8bc6\u522b\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u4e3a\u7406\u89e3LLM\u7684\u529f\u80fd\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9e\u8bc1\u652f\u6301\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u4efb\u52a1\u6027\u80fd\u7531\u5bf9\u7acb\u795e\u7ecf\u5143\u89d2\u8272\u5171\u540c\u51b3\u5b9a\u7684\u91cd\u8981\u673a\u5236\uff0c\u63d0\u51fa\u7684\u529f\u80fd\u62ee\u6297\u6846\u67b6\u4e3aLLM\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u548c\u8c03\u63a7\u6a21\u578b\u5185\u90e8\u529f\u80fd\u5355\u5143\uff0c\u5bf9\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u7814\u7a76\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2601.04589", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04589", "abs": "https://arxiv.org/abs/2601.04589", "authors": ["Zihao Lin", "Wanrong Zhu", "Jiuxiang Gu", "Jihyung Kil", "Christopher Tensmeyer", "Lin Zhang", "Shilong Liu", "Ruiyi Zhang", "Lifu Huang", "Vlad I. Morariu", "Tong Sun"], "title": "MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing", "comment": null, "summary": "Real-world design documents (e.g., posters) are inherently multi-layered, combining decoration, text, and images. Editing them from natural-language instructions requires fine-grained, layer-aware reasoning to identify relevant layers and coordinate modifications. Prior work largely overlooks multi-layer design document editing, focusing instead on single-layer image editing or multi-layer generation, which assume a flat canvas and lack the reasoning needed to determine what and where to modify. To address this gap, we introduce the Multi-Layer Document Editing Agent (MiLDEAgent), a reasoning-based framework that combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. To systematically benchmark this setting, we introduce the MiLDEBench, a human-in-the-loop corpus of over 20K design documents paired with diverse editing instructions. The benchmark is complemented by a task-specific evaluation protocol, MiLDEEval, which spans four dimensions including instruction following, layout consistency, aesthetics, and text rendering. Extensive experiments on 14 open-source and 2 closed-source models reveal that existing approaches fail to generalize: open-source models often cannot complete multi-layer document editing tasks, while closed-source models suffer from format violations. In contrast, MiLDEAgent achieves strong layer-aware reasoning and precise editing, significantly outperforming all open-source baselines and attaining performance comparable to closed-source models, thereby establishing the first strong baseline for multi-layer document editing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MiLDEAgent\uff0c\u4e00\u4e2a\u57fa\u4e8e\u63a8\u7406\u7684\u591a\u5c42\u8bbe\u8ba1\u6587\u6863\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408RL\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u63a8\u7406\u5668\u548c\u56fe\u50cf\u7f16\u8f91\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5c42\u6587\u6863\u7f16\u8f91\u4e2d\u7684\u5c40\u9650\u6027\u3002\u7814\u7a76\u8fd8\u5f15\u5165\u4e86MiLDEBench\u57fa\u51c6\u6570\u636e\u96c6\u548cMiLDEEval\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3a\u591a\u5c42\u6587\u6863\u7f16\u8f91\u5efa\u7acb\u4e86\u9996\u4e2a\u5f3a\u57fa\u7ebf\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8bbe\u8ba1\u6587\u6863\uff08\u5982\u6d77\u62a5\uff09\u672c\u8d28\u4e0a\u662f\u591a\u5c42\u7684\uff0c\u5305\u542b\u88c5\u9970\u3001\u6587\u672c\u548c\u56fe\u50cf\u7b49\u591a\u79cd\u5143\u7d20\u3002\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7f16\u8f91\u8fd9\u4e9b\u6587\u6863\u9700\u8981\u7ec6\u7c92\u5ea6\u7684\u3001\u5c42\u611f\u77e5\u7684\u63a8\u7406\u80fd\u529b\u6765\u8bc6\u522b\u76f8\u5173\u5c42\u5e76\u534f\u8c03\u4fee\u6539\u3002\u5148\u524d\u7684\u7814\u7a76\u5927\u591a\u5ffd\u89c6\u4e86\u591a\u5c42\u8bbe\u8ba1\u6587\u6863\u7f16\u8f91\u95ee\u9898\uff0c\u4e3b\u8981\u5173\u6ce8\u5355\u5c42\u56fe\u50cf\u7f16\u8f91\u6216\u591a\u5c42\u751f\u6210\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5047\u8bbe\u5e73\u9762\u753b\u5e03\u4e14\u7f3a\u4e4f\u786e\u5b9a\u4fee\u6539\u5185\u5bb9\u548c\u4f4d\u7f6e\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u591a\u5c42\u6587\u6863\u7f16\u8f91\u4ee3\u7406\uff08MiLDEAgent\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a8\u7406\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86RL\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u63a8\u7406\u5668\u7528\u4e8e\u5c42\u611f\u77e5\u7406\u89e3\u548c\u56fe\u50cf\u7f16\u8f91\u5668\u7528\u4e8e\u76ee\u6807\u4fee\u6539\u3002\u7814\u7a76\u8fd8\u5efa\u7acb\u4e86MiLDEBench\u57fa\u51c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u8d85\u8fc720K\u8bbe\u8ba1\u6587\u6863\u548c\u591a\u6837\u5316\u7f16\u8f91\u6307\u4ee4\u7684\u4eba\u5de5\u53c2\u4e0e\u8bed\u6599\u5e93\uff0c\u5e76\u914d\u5957\u4e86MiLDEEval\u8bc4\u4f30\u534f\u8bae\uff0c\u6db5\u76d6\u6307\u4ee4\u9075\u5faa\u3001\u5e03\u5c40\u4e00\u81f4\u6027\u3001\u7f8e\u5b66\u548c\u6587\u672c\u6e32\u67d3\u56db\u4e2a\u7ef4\u5ea6\u3002", "result": "\u572814\u4e2a\u5f00\u6e90\u6a21\u578b\u548c2\u4e2a\u95ed\u6e90\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\uff1a\u5f00\u6e90\u6a21\u578b\u901a\u5e38\u65e0\u6cd5\u5b8c\u6210\u591a\u5c42\u6587\u6863\u7f16\u8f91\u4efb\u52a1\uff0c\u800c\u95ed\u6e90\u6a21\u578b\u5b58\u5728\u683c\u5f0f\u8fdd\u89c4\u95ee\u9898\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cMiLDEAgent\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u5c42\u611f\u77e5\u63a8\u7406\u548c\u7cbe\u786e\u7f16\u8f91\uff0c\u663e\u8457\u4f18\u4e8e\u6240\u6709\u5f00\u6e90\u57fa\u7ebf\uff0c\u5e76\u8fbe\u5230\u4e0e\u95ed\u6e90\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4ece\u800c\u4e3a\u591a\u5c42\u6587\u6863\u7f16\u8f91\u5efa\u7acb\u4e86\u9996\u4e2a\u5f3a\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u5c42\u8bbe\u8ba1\u6587\u6863\u7f16\u8f91\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u63a8\u7406\u7684\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u591a\u5c42\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002MiLDEAgent\u7684\u6210\u529f\u8868\u660e\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u63a8\u7406\u4e0e\u76ee\u6807\u56fe\u50cf\u7f16\u8f91\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5c42\u611f\u77e5\u7f16\u8f91\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u6587\u6863\u7f16\u8f91\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2601.04709", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04709", "abs": "https://arxiv.org/abs/2601.04709", "authors": ["Gijun Park"], "title": "Bridging Temporal and Textual Modalities: A Multimodal Framework for Automated Cloud Failure Root Cause Analysis", "comment": null, "summary": "Root cause analysis in modern cloud infrastructure demands sophisticated understanding of heterogeneous data sources, particularly time-series performance metrics that involve core failure signatures. While large language models demonstrate remarkable capabilities in textual reasoning, their discrete token-based architecture creates fundamental incompatibilities with continuous numerical sequences exhibiting temporal dependencies. Current methodologies inadequately address this modality mismatch, constraining the potential of language model-driven automation in incident management workflows. This paper presents a multimodal diagnostic framework that harmonizes time-series representations with pretrained language model embedding spaces. Our approach contributes three technical advances: (1) a semantic compression technique that distills temporal segments into single-token abstractions while preserving pattern semantics, (2) an alignment encoder utilizing gated cross-attention to project time-series features into language model latent space, and (3) a retrieval-augmented diagnostic pipeline that synthesizes aligned embeddings with historical incident knowledge for expert-level failure attribution. Comprehensive evaluation across six cloud system benchmarks demonstrates that our framework achieves leading performance, reaching 48.75% diagnostic accuracy with notable improvements on scenarios involving compound failure modes. The results validate embedding-space alignment as an effective strategy for enabling language models to reason over multimodal telemetry data in production incident response contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u538b\u7f29\u548c\u5bf9\u9f50\u7f16\u7801\u6280\u672f\u5c06\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u4e0e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u7a7a\u95f4\u76f8\u534f\u8c03\uff0c\u89e3\u51b3\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u4e91\u57fa\u7840\u8bbe\u65bd\u6839\u56e0\u5206\u6790\u4e2d\u5904\u7406\u8fde\u7eed\u6570\u503c\u5e8f\u5217\u7684\u6a21\u6001\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u4e91\u57fa\u7840\u8bbe\u65bd\u6839\u56e0\u5206\u6790\u9700\u8981\u7406\u89e3\u5f02\u6784\u6570\u636e\u6e90\uff0c\u7279\u522b\u662f\u6d89\u53ca\u6838\u5fc3\u6545\u969c\u7279\u5f81\u7684\u65f6\u95f4\u5e8f\u5217\u6027\u80fd\u6307\u6807\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u57fa\u4e8e\u79bb\u6563\u4ee4\u724c\u7684\u67b6\u6784\u4e0e\u5177\u6709\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u8fde\u7eed\u6570\u503c\u5e8f\u5217\u5b58\u5728\u6839\u672c\u6027\u4e0d\u517c\u5bb9\uff0c\u5f53\u524d\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u89e3\u51b3\u8fd9\u79cd\u6a21\u6001\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u4e8b\u4ef6\u7ba1\u7406\u5de5\u4f5c\u6d41\u4e2d\u7684\u81ea\u52a8\u5316\u6f5c\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u8bca\u65ad\u6846\u67b6\uff0c\u5305\u542b\u4e09\u9879\u6280\u672f\u8d21\u732e\uff1a\u4e00\u662f\u8bed\u4e49\u538b\u7f29\u6280\u672f\uff0c\u5c06\u65f6\u95f4\u7247\u6bb5\u84b8\u998f\u4e3a\u5355\u4ee4\u724c\u62bd\u8c61\u540c\u65f6\u4fdd\u7559\u6a21\u5f0f\u8bed\u4e49\uff1b\u4e8c\u662f\u4f7f\u7528\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u5bf9\u9f50\u7f16\u7801\u5668\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\u6295\u5f71\u5230\u8bed\u8a00\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\uff1b\u4e09\u662f\u68c0\u7d22\u589e\u5f3a\u7684\u8bca\u65ad\u7ba1\u9053\uff0c\u5c06\u5bf9\u9f50\u5d4c\u5165\u4e0e\u5386\u53f2\u4e8b\u4ef6\u77e5\u8bc6\u76f8\u7ed3\u5408\u8fdb\u884c\u4e13\u5bb6\u7ea7\u6545\u969c\u5f52\u56e0\u3002", "result": "\u5728\u516d\u4e2a\u4e91\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5168\u9762\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u9886\u5148\u6027\u80fd\uff0c\u8fbe\u523048.75%\u7684\u8bca\u65ad\u51c6\u786e\u7387\uff0c\u5728\u6d89\u53ca\u590d\u5408\u6545\u969c\u6a21\u5f0f\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\u4f5c\u4e3a\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u751f\u4ea7\u4e8b\u4ef6\u54cd\u5e94\u4e0a\u4e0b\u6587\u4e2d\u5bf9\u591a\u6a21\u6001\u9065\u6d4b\u6570\u636e\u8fdb\u884c\u63a8\u7406\u7684\u6709\u6548\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u5b9e\u4e86\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\u662f\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u4e0e\u8fde\u7eed\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6a21\u6001\u4e0d\u5339\u914d\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u4e91\u57fa\u7840\u8bbe\u65bd\u81ea\u52a8\u5316\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u5408\u6545\u969c\u6a21\u5f0f\u65b9\u9762\u5c55\u73b0\u51fa\u4f18\u52bf\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u5728\u4e8b\u4ef6\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2601.04582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04582", "abs": "https://arxiv.org/abs/2601.04582", "authors": ["Mizanur Rahman", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Shafiq Joty", "Enamul Hoque"], "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization", "comment": "Accepted to EACL Main Conference", "summary": "Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRL-Text2Vis\uff0c\u9996\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6587\u672c\u5230\u53ef\u89c6\u5316\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u5956\u52b1\u51fd\u6570\u8054\u5408\u4f18\u5316\u6587\u672c\u51c6\u786e\u6027\u3001\u4ee3\u7801\u6709\u6548\u6027\u548c\u53ef\u89c6\u5316\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u8868\u751f\u6210\u8d28\u91cf\u4e0e\u4ee3\u7801\u6267\u884c\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u53ef\u89c6\u5316\u7cfb\u7edf\u4e2d\uff0c\u95ed\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u56fe\u8868\u5e38\u7f3a\u4e4f\u8bed\u4e49\u5bf9\u9f50\u548c\u6e05\u6670\u5ea6\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u5219\u9891\u7e41\u4ea7\u751f\u4e0d\u53ef\u6267\u884c\u6216\u89c6\u89c9\u8d28\u91cf\u5dee\u7684\u8f93\u51fa\u3002\u5c3d\u7ba1\u76d1\u7763\u5fae\u8c03\u80fd\u6539\u5584\u4ee3\u7801\u53ef\u6267\u884c\u6027\uff0c\u4f46\u65e0\u6cd5\u63d0\u5347\u6574\u4f53\u53ef\u89c6\u5316\u8d28\u91cf\uff0c\u56e0\u4e3a\u4f20\u7edfSFT\u635f\u5931\u65e0\u6cd5\u6355\u6349\u6267\u884c\u540e\u53cd\u9988\u3002", "method": "\u672c\u6587\u63d0\u51faRL-Text2Vis\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff0c\u91c7\u7528\u65b0\u9896\u7684\u591a\u76ee\u6807\u5956\u52b1\u51fd\u6570\uff0c\u8054\u5408\u4f18\u5316\u6587\u672c\u51c6\u786e\u6027\u3001\u4ee3\u7801\u6709\u6548\u6027\u548c\u53ef\u89c6\u5316\u8d28\u91cf\uff0c\u5229\u7528\u6267\u884c\u540e\u53cd\u9988\u8fdb\u884c\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u5728Qwen2.5\u6a21\u578b\uff087B\u548c14B\uff09\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e13\u95e8\u9488\u5bf9\u6587\u672c\u5230\u53ef\u89c6\u5316\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\u3002", "result": "RL-Text2Vis\u5728Text2Vis\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4GPT-4o\u5b9e\u73b0\u4e8622%\u7684\u56fe\u8868\u8d28\u91cf\u76f8\u5bf9\u63d0\u5347\uff0c\u4ee3\u7801\u6267\u884c\u6210\u529f\u7387\u4ece\u96f6\u6837\u672c\u57fa\u7ebf\u768478%\u63d0\u5347\u81f397%\u3002\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5f3a\u96f6\u6837\u672c\u548c\u76d1\u7763\u57fa\u7ebf\uff0c\u5e76\u5728VIS-Eval\u548cNVBench\u7b49\u57df\u5916\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u786e\u7acb\u4e86GRPO\u4f5c\u4e3a\u53ef\u89c6\u5316\u751f\u6210\u4e2d\u7ed3\u6784\u5316\u591a\u6a21\u6001\u63a8\u7406\u7684\u6709\u6548\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u63d0\u5347\u6587\u672c\u5230\u53ef\u89c6\u5316\u7cfb\u7edf\u8d28\u91cf\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u4e3a\u590d\u6742\u7ed3\u6784\u5316\u8f93\u51fa\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\u3002"}}
{"id": "2601.04614", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04614", "abs": "https://arxiv.org/abs/2601.04614", "authors": ["Wenzhi Chen", "Bo Hu", "Leida Li", "Lihuo He", "Wen Lu", "Xinbo Gao"], "title": "HyperAlign: Hyperbolic Entailment Cones for Adaptive Text-to-Image Alignment Assessment", "comment": null, "summary": "With the rapid development of text-to-image generation technology, accurately assessing the alignment between generated images and text prompts has become a critical challenge. Existing methods rely on Euclidean space metrics, neglecting the structured nature of semantic alignment, while lacking adaptive capabilities for different samples. To address these limitations, we propose HyperAlign, an adaptive text-to-image alignment assessment framework based on hyperbolic entailment geometry. First, we extract Euclidean features using CLIP and map them to hyperbolic space. Second, we design a dynamic-supervision entailment modeling mechanism that transforms discrete entailment logic into continuous geometric structure supervision. Finally, we propose an adaptive modulation regressor that utilizes hyperbolic geometric features to generate sample-level modulation parameters, adaptively calibrating Euclidean cosine similarity to predict the final score. HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks, fully validating the effectiveness of hyperbolic geometric modeling for image-text alignment assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHyperAlign\uff0c\u4e00\u79cd\u57fa\u4e8e\u53cc\u66f2\u8574\u542b\u51e0\u4f55\u7684\u81ea\u9002\u5e94\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5c06CLIP\u7279\u5f81\u6620\u5c04\u5230\u53cc\u66f2\u7a7a\u95f4\u5e76\u8bbe\u8ba1\u52a8\u6001\u76d1\u7763\u8574\u542b\u5efa\u6a21\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u4e0e\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u51c6\u786e\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u4e0e\u6587\u672c\u63d0\u793a\u4e4b\u95f4\u7684\u5bf9\u9f50\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u5ea6\u91cf\uff0c\u5ffd\u89c6\u4e86\u8bed\u4e49\u5bf9\u9f50\u7684\u7ed3\u6784\u5316\u7279\u6027\uff0c\u540c\u65f6\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u6837\u672c\u7684\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u8fd9\u4e9b\u5c40\u9650\u6027\u4fc3\u4f7f\u7814\u7a76\u8005\u5f00\u53d1\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "HyperAlign\u6846\u67b6\u9996\u5148\u4f7f\u7528CLIP\u63d0\u53d6\u6b27\u51e0\u91cc\u5f97\u7279\u5f81\u5e76\u5c06\u5176\u6620\u5c04\u5230\u53cc\u66f2\u7a7a\u95f4\uff1b\u5176\u6b21\u8bbe\u8ba1\u52a8\u6001\u76d1\u7763\u8574\u542b\u5efa\u6a21\u673a\u5236\uff0c\u5c06\u79bb\u6563\u7684\u8574\u542b\u903b\u8f91\u8f6c\u5316\u4e3a\u8fde\u7eed\u7684\u51e0\u4f55\u7ed3\u6784\u76d1\u7763\uff1b\u6700\u540e\u63d0\u51fa\u81ea\u9002\u5e94\u8c03\u5236\u56de\u5f52\u5668\uff0c\u5229\u7528\u53cc\u66f2\u51e0\u4f55\u7279\u5f81\u751f\u6210\u6837\u672c\u7ea7\u8c03\u5236\u53c2\u6570\uff0c\u81ea\u9002\u5e94\u6821\u51c6\u6b27\u51e0\u91cc\u5f97\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4ee5\u9884\u6d4b\u6700\u7ec8\u5f97\u5206\u3002", "result": "HyperAlign\u5728\u5355\u6570\u636e\u5e93\u8bc4\u4f30\u548c\u8de8\u6570\u636e\u5e93\u6cdb\u5316\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u9ad8\u5ea6\u7ade\u4e89\u529b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5145\u5206\u9a8c\u8bc1\u4e86\u53cc\u66f2\u51e0\u4f55\u5efa\u6a21\u5728\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u4f20\u7edf\u57fa\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u5ea6\u91cf\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u53cc\u66f2\u51e0\u4f55\u5efa\u6a21\u80fd\u591f\u6709\u6548\u6355\u6349\u8bed\u4e49\u5bf9\u9f50\u7684\u7ed3\u6784\u5316\u7279\u6027\uff0c\u4e3a\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u51e0\u4f55\u89c6\u89d2\u3002\u81ea\u9002\u5e94\u8c03\u5236\u673a\u5236\u89e3\u51b3\u4e86\u6837\u672c\u95f4\u5dee\u5f02\u95ee\u9898\uff0c\u4e3a\u751f\u6210\u5f0fAI\u7684\u8d28\u91cf\u8bc4\u4f30\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.04819", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04819", "abs": "https://arxiv.org/abs/2601.04819", "authors": ["Aleksei Kondratenko", "Mussie Birhane", "Houssame E. Hsain", "Guido Maciocci"], "title": "AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding", "comment": null, "summary": "AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86AECV-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5efa\u7b51\u3001\u5de5\u7a0b\u548c\u65bd\u5de5\u56fe\u7eb8\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u6587\u6863\u8f85\u52a9\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u56fe\u7eb8\u7b26\u53f7\u7406\u89e3\u548c\u8ba1\u6570\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u73b0\u4ee3\u591a\u6a21\u6001\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5efa\u7b51\u3001\u5de5\u7a0b\u548c\u65bd\u5de5\u56fe\u7eb8\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u8fd9\u4e9b\u56fe\u7eb8\u901a\u8fc7\u7b26\u53f7\u3001\u5e03\u5c40\u7ea6\u5b9a\u548c\u5bc6\u96c6\u6807\u6ce8\u7f16\u7801\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u80fd\u53ef\u9760\u89e3\u91ca\u8fd9\u79cd\u56fe\u5f62\u8bed\u8a00\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86AECV-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u7528\u4f8b\uff1a\u5728120\u4e2a\u9ad8\u8d28\u91cf\u5e73\u9762\u56fe\u4e0a\u8fdb\u884c\u5bf9\u8c61\u8ba1\u6570\u4efb\u52a1\uff0c\u4ee5\u53ca\u57fa\u4e8e192\u4e2a\u95ee\u7b54\u5bf9\u7684\u56fe\u7eb8\u6587\u6863\u95ee\u7b54\u4efb\u52a1\uff0c\u540e\u8005\u6db5\u76d6\u6587\u672c\u63d0\u53d6\u3001\u5b9e\u4f8b\u8ba1\u6570\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u6bd4\u8f83\u63a8\u7406\uff0c\u91c7\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u8bc4\u5206\u6d41\u7a0b\u548c\u9488\u5bf9\u8fb9\u7f18\u60c5\u51b5\u7684\u4eba\u5de5\u88c1\u51b3\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u7a33\u5b9a\u7684\u80fd\u529b\u68af\u5ea6\uff1aOCR\u548c\u6587\u672c\u4e2d\u5fc3\u6587\u6863\u95ee\u7b54\u8868\u73b0\u6700\u5f3a\uff0c\u7a7a\u95f4\u63a8\u7406\u4e2d\u7b49\uff0c\u800c\u7b26\u53f7\u4e2d\u5fc3\u56fe\u7eb8\u7406\u89e3\u7279\u522b\u662f\u95e8\u7a97\u53ef\u9760\u8ba1\u6570\u4ecd\u7136\u672a\u89e3\u51b3\uff0c\u5bf9\u8c61\u8ba1\u6570\u4f7f\u7528\u6bcf\u5b57\u6bb5\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\u548cMAPE\u7ed3\u679c\uff0c\u6587\u6863\u95ee\u7b54\u4f7f\u7528\u6574\u4f53\u51c6\u786e\u7387\u548c\u6bcf\u7c7b\u522b\u7ec6\u5206\u3002", "conclusion": "\u5f53\u524d\u7cfb\u7edf\u4f5c\u4e3a\u6587\u6863\u52a9\u624b\u8868\u73b0\u826f\u597d\u4f46\u7f3a\u4e4f\u7a33\u5065\u7684\u56fe\u7eb8\u7406\u89e3\u80fd\u529b\uff0c\u8fd9\u6fc0\u52b1\u4e86\u9886\u57df\u7279\u5b9a\u8868\u793a\u548c\u5de5\u5177\u589e\u5f3a\u3001\u4eba\u5728\u73af\u7684\u5de5\u4f5c\u6d41\u7a0b\u5f00\u53d1\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684AEC\u81ea\u52a8\u5316\uff0c\u5f3a\u8c03\u4e86\u7b26\u53f7\u4e2d\u5fc3\u56fe\u7eb8\u7406\u89e3\u7684\u6311\u6218\u6027\u3002"}}
{"id": "2601.04609", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04609", "abs": "https://arxiv.org/abs/2601.04609", "authors": ["Rhea Kapur", "Robert Hawkins", "Elisa Kreiss"], "title": "When More Words Say Less: Decoupling Length and Specificity in Image Description Evaluation", "comment": null, "summary": "Vision-language models (VLMs) are increasingly used to make visual content accessible via text-based descriptions. In current systems, however, description specificity is often conflated with their length. We argue that these two concepts must be disentangled: descriptions can be concise yet dense with information, or lengthy yet vacuous. We define specificity relative to a contrast set, where a description is more specific to the extent that it picks out the target image better than other possible images. We construct a dataset that controls for length while varying information content, and validate that people reliably prefer more specific descriptions regardless of length. We find that controlling for length alone cannot account for differences in specificity: how the length budget is allocated makes a difference. These results support evaluation approaches that directly prioritize specificity over verbosity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6307\u51fa\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u63cf\u8ff0\u7279\u5f02\u6027\u5e38\u4e0e\u957f\u5ea6\u6df7\u6dc6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u7279\u5f02\u6027\u5e94\u76f8\u5bf9\u4e8e\u5bf9\u6bd4\u96c6\u6765\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u6784\u5efa\u63a7\u5236\u957f\u5ea6\u7684\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u4eba\u4eec\u66f4\u504f\u597d\u7279\u5f02\u6027\u63cf\u8ff0\u800c\u975e\u5197\u957f\u63cf\u8ff0\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u4e2d\uff0c\u63cf\u8ff0\u7684\u7279\u5f02\u6027\u5e38\u88ab\u9519\u8bef\u5730\u4e0e\u63cf\u8ff0\u957f\u5ea6\u6df7\u4e3a\u4e00\u8c08\uff0c\u5bfc\u81f4\u63cf\u8ff0\u53ef\u80fd\u5197\u957f\u4f46\u4fe1\u606f\u7a7a\u6d1e\uff0c\u6216\u7b80\u6d01\u4f46\u4fe1\u606f\u5bc6\u96c6\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u5c06\u8fd9\u4e24\u4e2a\u6982\u5ff5\u89e3\u8026\u5e76\u660e\u786e\u5b9a\u4e49\u63cf\u8ff0\u7279\u5f02\u6027\u3002", "method": "\u7814\u7a76\u5c06\u63cf\u8ff0\u7279\u5f02\u6027\u5b9a\u4e49\u4e3a\u76f8\u5bf9\u4e8e\u5bf9\u6bd4\u96c6\u7684\u6982\u5ff5\uff0c\u5373\u63cf\u8ff0\u80fd\u66f4\u597d\u5730\u533a\u5206\u76ee\u6807\u56fe\u50cf\u4e0e\u5176\u4ed6\u53ef\u80fd\u56fe\u50cf\u7684\u7a0b\u5ea6\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u63a7\u5236\u63cf\u8ff0\u957f\u5ea6\u540c\u65f6\u53d8\u5316\u4fe1\u606f\u5185\u5bb9\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4eba\u7c7b\u504f\u597d\u5b9e\u9a8c\u9a8c\u8bc1\u7279\u5f02\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4eba\u4eec\u786e\u5b9e\u66f4\u504f\u597d\u7279\u5f02\u6027\u63cf\u8ff0\u800c\u975e\u5197\u957f\u63cf\u8ff0\uff0c\u4e14\u4ec5\u63a7\u5236\u957f\u5ea6\u4e0d\u8db3\u4ee5\u89e3\u91ca\u7279\u5f02\u6027\u5dee\u5f02\uff0c\u957f\u5ea6\u9884\u7b97\u7684\u5206\u914d\u65b9\u5f0f\u5bf9\u63cf\u8ff0\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd\uff0c\u652f\u6301\u76f4\u63a5\u4f18\u5148\u8003\u8651\u7279\u5f02\u6027\u800c\u975e\u5197\u957f\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e2d\u533a\u5206\u63cf\u8ff0\u7279\u5f02\u6027\u4e0e\u957f\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5bf9\u6bd4\u96c6\u7684\u7279\u5f02\u6027\u5b9a\u4e49\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u56fe\u50cf\u63cf\u8ff0\u8bc4\u4f30\u6307\u6807\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u5de5\u4f5c\u5e94\u76f4\u63a5\u4f18\u5316\u63cf\u8ff0\u7684\u4fe1\u606f\u5bc6\u5ea6\u800c\u975e\u5355\u7eaf\u63a7\u5236\u957f\u5ea6\u3002"}}
{"id": "2601.04672", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04672", "abs": "https://arxiv.org/abs/2601.04672", "authors": ["Wentao Zhang", "Lifei Wang", "Lina Lu", "MingKun Xu", "Shangyang Li", "Yanchao Yang", "Tao Fang"], "title": "Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning", "comment": "This paper is submitted for review to ACL 2026. It is 17 pages long and includes 5 figures. The corresponding authors are Tao Fang and Lina Lu", "summary": "Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \\textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\\% relative gain in disease recognition accuracy, +33.3\\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAgri-R1\uff0c\u4e00\u79cd\u9762\u5411\u519c\u4e1a\u7684\u63a8\u7406\u589e\u5f3a\u5927\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\u751f\u6210\u548c\u521b\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u4ec5\u4f7f\u752819%\u53ef\u7528\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u519c\u4e1a\u75c5\u5bb3\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u519c\u4e1a\u75c5\u5bb3\u8bca\u65ad\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u6311\u6218\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u73b0\u6709\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4e13\u5bb6\u6807\u6ce8\uff0c\u96be\u4ee5\u5904\u7406\u519c\u4e1a\u67e5\u8be2\u7684\u5f00\u653e\u6027\u548c\u591a\u6837\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u519c\u4e1a\u4e13\u7528\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u3002", "method": "\u63d0\u51faAgri-R1\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u5408\u6210\u548c\u57fa\u4e8eLLM\u7684\u8fc7\u6ee4\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\uff0c\u4ec5\u4f7f\u752819%\u53ef\u7528\u6837\u672c\u3002\u91c7\u7528Group Relative Policy Optimization\uff08GRPO\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u63d0\u51fa\u65b0\u9896\u7684\u5956\u52b1\u51fd\u6570\uff0c\u6574\u5408\u9886\u57df\u7279\u5b9a\u8bcd\u5178\u548c\u6a21\u7cca\u5339\u914d\u6765\u8bc4\u4f30\u5f00\u653e\u56de\u7b54\u7684\u6b63\u786e\u6027\u548c\u8bed\u8a00\u7075\u6d3b\u6027\u3002", "result": "\u5728CDDMBench\u8bc4\u4f30\u4e2d\uff0c3B\u53c2\u6570\u6a21\u578b\u6027\u80fd\u4e0e7B\u81f313B\u53c2\u6570\u57fa\u7ebf\u6a21\u578b\u76f8\u5f53\uff0c\u5728\u75c5\u5bb3\u8bc6\u522b\u51c6\u786e\u7387\u4e0a\u5b9e\u73b0+23.2%\u76f8\u5bf9\u63d0\u5347\uff0c\u519c\u4e1a\u77e5\u8bc6\u95ee\u7b54\u63d0\u5347+33.3%\uff0c\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u6bd4\u6807\u51c6\u5fae\u8c03\u63d0\u9ad826.10\u5206\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u7ed3\u6784\u5316\u63a8\u7406\u6570\u636e\u4e0eGRPO\u9a71\u52a8\u7684\u63a2\u7d22\u534f\u540c\u4f5c\u7528\u652f\u6491\u4e86\u8fd9\u4e9b\u589e\u76ca\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u81ea\u52a8\u5316\u63a8\u7406\u6570\u636e\u751f\u6210\u4e0e\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u519c\u4e1aAI\u6a21\u578b\u7684\u8bca\u65ad\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u95ee\u9898\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u6548\u679c\u66f4\u663e\u8457\u3002\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u519c\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u9886\u57df\u7279\u5b9a\u5956\u52b1\u51fd\u6570\u5728\u5f00\u653e\u56de\u7b54\u8bc4\u4f30\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.24556", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.24556", "abs": "https://arxiv.org/abs/2512.24556", "authors": ["Muhammad Abdullahi Said", "Muhammad Sammani Sani"], "title": "Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs", "comment": null, "summary": "As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the narrative of the multilingual safety gap. Instead of a simple degradation in low-resource settings, we identified a complex interference mechanism in which safety is determined by the intersection of variables. Although the models exhibited a reverse linguistic vulnerability with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal, they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u5ba1\u8ba1\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u4e2d\u5b58\u5728\u590d\u6742\u7684\u5e72\u6270\u673a\u5236\u800c\u975e\u7b80\u5355\u7684\u6027\u80fd\u9000\u5316\uff0c\u63ed\u793a\u4e86\u5b89\u5168\u6027\u80fd\u53d7\u8bed\u8a00\u548c\u65f6\u6001\u6846\u67b6\u4ea4\u4e92\u5f71\u54cd\u7684\u52a8\u6001\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e0d\u53d8\u5bf9\u9f50\u7684\u65b0\u8303\u5f0f\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5b58\u5728\u4e00\u4e2a\u5371\u9669\u76f2\u533a\uff0c\u5373\u5047\u8bbe\u5b89\u5168\u5bf9\u9f50\u80fd\u591f\u4ece\u82f1\u8bed\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u5176\u4ed6\u8bed\u8a00\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6b63\u88ab\u96c6\u6210\u5230\u5173\u952e\u5168\u7403\u57fa\u7840\u8bbe\u65bd\u4e2d\uff0c\u8fd9\u79cd\u5047\u8bbe\u53ef\u80fd\u5bfc\u81f4\u5168\u7403\u5357\u65b9\u7528\u6237\u9762\u4e34\u672c\u5730\u5316\u5371\u5bb3\u98ce\u9669\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7cfb\u7edf\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8e\u897f\u975e\u5a01\u80c1\u573a\u666f\u6784\u5efa\u7684\u65b0\u578b\u5bf9\u6297\u6570\u636e\u96c6HausaSafety\uff0c\u5bf9GPT-5.1\u3001Gemini 3 Pro\u548cClaude 4.5 Opus\u4e09\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c1,440\u6b21\u8bc4\u4f30\uff0c\u91c7\u75282\u00d74\u56e0\u5b50\u8bbe\u8ba1\u68c0\u9a8c\u8bed\u8a00\u4e0e\u65f6\u6001\u6846\u67b6\u7684\u975e\u7ebf\u6027\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u591a\u8bed\u8a00\u5b89\u5168\u5dee\u8ddd\u7684\u7b80\u5355\u53d9\u4e8b\uff0c\u53d1\u73b0\u590d\u6742\u5e72\u6270\u673a\u5236\u51b3\u5b9a\u5b89\u5168\u6027\u80fd\uff0cClaude 4.5 Opus\u5728\u8c6a\u8428\u8bed\u4e2d\u5b89\u5168\u6027\u663e\u8457\u9ad8\u4e8e\u82f1\u8bed\uff0c\u540c\u65f6\u6a21\u578b\u5728\u65f6\u6001\u63a8\u7406\u65b9\u9762\u5b58\u5728\u707e\u96be\u6027\u5931\u8d25\uff0c\u8fc7\u53bb\u65f6\u6846\u67b6\u7ed5\u8fc7\u9632\u5fa1\u800c\u672a\u6765\u65f6\u573a\u666f\u89e6\u53d1\u8fc7\u5ea6\u4fdd\u5b88\u62d2\u7edd\uff0c\u6700\u5b89\u5168\u4e0e\u6700\u8106\u5f31\u914d\u7f6e\u95f4\u5b58\u57289.2\u500d\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f53\u524d\u6a21\u578b\u4f9d\u8d56\u8868\u9762\u542f\u53d1\u5f0f\u800c\u975e\u7a33\u5065\u8bed\u4e49\u7406\u89e3\uff0c\u5f62\u6210\u5b89\u5168\u76f2\u70b9\u4f7f\u5168\u7403\u5357\u65b9\u7528\u6237\u9762\u4e34\u672c\u5730\u5316\u5371\u5bb3\uff0c\u9700\u8981\u5411\u4e0d\u53d8\u5bf9\u9f50\u8303\u5f0f\u8f6c\u53d8\u4ee5\u786e\u4fdd\u8de8\u8bed\u8a00\u548c\u65f6\u6001\u53d8\u5316\u7684\u5b89\u5168\u7a33\u5b9a\u6027\uff0c\u5b89\u5168\u4e0d\u662f\u56fa\u5b9a\u5c5e\u6027\u800c\u662f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u72b6\u6001\u3002"}}
{"id": "2601.04692", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04692", "abs": "https://arxiv.org/abs/2601.04692", "authors": ["Naquee Rizwan", "Subhankar Swain", "Paramananda Bhaskar", "Gagan Aryan", "Shehryaar Shah Khan", "Animesh Mukherjee"], "title": "See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation", "comment": null, "summary": "In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u901a\u7528\u4ec7\u6068\u8868\u60c5\u5305\u68c0\u6d4b\u3001\u89e3\u91ca\u4e0e\u5e72\u9884\u6846\u67b6\uff0c\u9996\u6b21\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u4e09\u4efb\u52a1\u7684\u7edf\u4e00\u5904\u7406\uff0c\u5e76\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u751f\u6210\u5f0f\u591a\u6a21\u6001\u4ee3\u7406\u548c\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u7684\u5c11\u6837\u672c\u9002\u5e94\u80fd\u529b\u6765\u5e94\u5bf9\u4e0d\u540c\u7c7b\u578b\u8868\u60c5\u5305\u3002", "motivation": "\u5f53\u524d\u4ec7\u6068\u8868\u60c5\u5305\u7814\u7a76\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u68c0\u6d4b\u3001\u89e3\u91ca\u548c\u5e72\u9884\u901a\u5e38\u88ab\u5206\u5f00\u7814\u7a76\uff0c\u4e0d\u7b26\u5408\u73b0\u5b9e\u573a\u666f\u9700\u6c42\uff1b\u6784\u5efa\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u6210\u672c\u8fc7\u9ad8\uff1b\u7f3a\u4e4f\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u901a\u7528\u7684\u4ec7\u6068\u8868\u60c5\u5305\u6cbb\u7406\u65b9\u6848\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e9b\u7814\u7a76\u7a7a\u767d\uff0c\u5b9e\u73b0\u4e09\u4efb\u52a1\u7684\u7edf\u4e00\u5904\u7406\u5e76\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u4fdd\u6301\u6709\u6548\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u7684\u751f\u6210\u5f0f\u591a\u6a21\u6001\u4ee3\u7406\u548c\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u7684\u5c11\u6837\u672c\u9002\u5e94\u80fd\u529b\u6765\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u8868\u60c5\u5305\u3002\u8be5\u6846\u67b6\u5728\u751f\u6210\u5f0fAI\u6a21\u578b\u57fa\u7840\u4e0a\u6784\u5efa\u591a\u79cd\u7b56\u7565\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u68c0\u6d4b\u3001\u89e3\u91ca\u548c\u5e72\u9884\u4e09\u4e2a\u4e92\u8865\u4efb\u52a1\uff0c\u7279\u522b\u9488\u5bf9\u6570\u636e\u6709\u9650\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u8be5\u7814\u7a76\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u5bf9\u4ec7\u6068\u8868\u60c5\u5305\u7684\u901a\u7528\u6cbb\u7406\uff0c\u6846\u67b6\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u751f\u4ea7\u573a\u666f\u4e2d\u90e8\u7f72\u7684\u5f3a\u5927\u6f5c\u529b\u3002\u901a\u8fc7\u6574\u5408\u68c0\u6d4b\u3001\u89e3\u91ca\u548c\u5e72\u9884\u529f\u80fd\uff0c\u7cfb\u7edf\u80fd\u591f\u66f4\u5168\u9762\u5730\u5e94\u5bf9\u4ec7\u6068\u8868\u60c5\u5305\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u5206\u79bb\u5904\u7406\u65b9\u6cd5\u66f4\u7b26\u5408\u73b0\u5b9e\u9700\u6c42\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4ec7\u6068\u8868\u60c5\u5305\u6cbb\u7406\u63d0\u4f9b\u4e86\u9996\u4e2a\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c06\u68c0\u6d4b\u3001\u89e3\u91ca\u548c\u5e72\u9884\u4e09\u4e2a\u4efb\u52a1\u7edf\u4e00\u5904\u7406\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u610f\u4e49\u3002\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u591a\u6a21\u6001\u5185\u5bb9\u6cbb\u7406\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2601.04706", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04706", "abs": "https://arxiv.org/abs/2601.04706", "authors": ["Yanbing Zeng", "Jia Wang", "Hanghang Ma", "Junqiang Wu", "Jie Zhu", "Xiaoming Wei", "Jie Hu"], "title": "Forge-and-Quench: Enhancing Image Generation for Higher Fidelity in Unified Multimodal Models", "comment": null, "summary": "Integrating image generation and understanding into a single framework has become a pivotal goal in the multimodal domain. However, how understanding can effectively assist generation has not been fully explored. Unlike previous works that focus on leveraging reasoning abilities and world knowledge from understanding models, this paper introduces a novel perspective: leveraging understanding to enhance the fidelity and detail richness of generated images. To this end, we propose Forge-and-Quench, a new unified framework that puts this principle into practice. In the generation process of our framework, an MLLM first reasons over the entire conversational context, including text instructions, to produce an enhanced text instruction. This refined instruction is then mapped to a virtual visual representation, termed the Bridge Feature, via a novel Bridge Adapter. This feature acts as a crucial link, forging insights from the understanding model to quench and refine the generation process. It is subsequently injected into the T2I backbone as a visual guidance signal, alongside the enhanced text instruction that replaces the original input. To validate this paradigm, we conduct comprehensive studies on the design of the Bridge Feature and Bridge Adapter. Our framework demonstrates exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant savings in training overhead, all without compromising the MLLM's inherent multimodal understanding capabilities. Experiments show that Forge-and-Quench significantly improves image fidelity and detail across multiple models, while also maintaining instruction-following accuracy and enhancing world knowledge application. Models and codes are available at https://github.com/YanbingZeng/Forge-and-Quench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faForge-and-Quench\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u89e3\u6a21\u578b\u589e\u5f3a\u751f\u6210\u6a21\u578b\u7684\u4fdd\u771f\u5ea6\u548c\u7ec6\u8282\u4e30\u5bcc\u5ea6\uff0c\u5229\u7528MLLM\u751f\u6210\u589e\u5f3a\u6587\u672c\u6307\u4ee4\u5e76\u6620\u5c04\u4e3a\u6865\u63a5\u7279\u5f81\uff0c\u4f5c\u4e3a\u89c6\u89c9\u5f15\u5bfc\u4fe1\u53f7\u6ce8\u5165T2I\u4e3b\u5e72\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u9886\u57df\u5c06\u56fe\u50cf\u751f\u6210\u4e0e\u7406\u89e3\u96c6\u6210\u5230\u5355\u4e00\u6846\u67b6\u6210\u4e3a\u5173\u952e\u76ee\u6807\uff0c\u4f46\u7406\u89e3\u5982\u4f55\u6709\u6548\u8f85\u52a9\u751f\u6210\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5229\u7528\u7406\u89e3\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u4e16\u754c\u77e5\u8bc6\uff0c\u800c\u672c\u6587\u5f15\u5165\u65b0\u89c6\u89d2\uff1a\u5229\u7528\u7406\u89e3\u6765\u589e\u5f3a\u751f\u6210\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\u548c\u7ec6\u8282\u4e30\u5bcc\u5ea6\u3002", "method": "\u63d0\u51faForge-and-Quench\u7edf\u4e00\u6846\u67b6\uff0c\u5176\u4e2dMLLM\u9996\u5148\u5bf9\u6574\u4e2a\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u8fdb\u884c\u63a8\u7406\u751f\u6210\u589e\u5f3a\u6587\u672c\u6307\u4ee4\uff0c\u7136\u540e\u901a\u8fc7\u65b0\u9896\u7684\u6865\u63a5\u9002\u914d\u5668\u5c06\u5176\u6620\u5c04\u4e3a\u865a\u62df\u89c6\u89c9\u8868\u793a\u2014\u2014\u6865\u63a5\u7279\u5f81\u3002\u8be5\u7279\u5f81\u4f5c\u4e3a\u5173\u952e\u94fe\u63a5\uff0c\u5c06\u7406\u89e3\u6a21\u578b\u7684\u6d1e\u5bdf\u6ce8\u5165\u751f\u6210\u8fc7\u7a0b\uff0c\u968f\u540e\u4e0e\u589e\u5f3a\u6587\u672c\u6307\u4ee4\u4e00\u8d77\u4f5c\u4e3a\u89c6\u89c9\u5f15\u5bfc\u4fe1\u53f7\u6ce8\u5165T2I\u4e3b\u5e72\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eForge-and-Quench\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u4e2a\u6a21\u578b\u7684\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u7ec6\u8282\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6307\u4ee4\u8ddf\u968f\u51c6\u786e\u6027\u5e76\u589e\u5f3a\u4e86\u4e16\u754c\u77e5\u8bc6\u5e94\u7528\u3002\u8be5\u6846\u67b6\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u5728\u4e0d\u540cMLLM\u548cT2I\u6a21\u578b\u95f4\u9ad8\u6548\u8fc1\u79fb\uff0c\u663e\u8457\u8282\u7701\u8bad\u7ec3\u5f00\u9500\u4e14\u4e0d\u635f\u5bb3MLLM\u56fa\u6709\u7684\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u8f85\u52a9\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6865\u63a5\u7279\u5f81\u6709\u6548\u8fde\u63a5\u7406\u89e3\u4e0e\u751f\u6210\u8fc7\u7a0b\u3002\u6846\u67b6\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u7406\u89e3\u6a21\u578b\u6d1e\u5bdf\u5411\u751f\u6210\u6a21\u578b\u7684\u76f4\u63a5\u4f20\u9012\uff0c\u4e3a\u591a\u6a21\u6001\u7edf\u4e00\u6846\u67b6\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u8def\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u53ef\u8fc1\u79fb\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2601.04720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04720", "abs": "https://arxiv.org/abs/2601.04720", "authors": ["Mingxin Li", "Yanzhao Zhang", "Dingkun Long", "Keqin Chen", "Sibo Song", "Shuai Bai", "Zhibo Yang", "Pengjun Xie", "An Yang", "Dayiheng Liu", "Jingren Zhou", "Junyang Lin"], "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "comment": null, "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\\textbf{2B}$ and $\\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Qwen3-VL-Embedding\u548cQwen3-VL-Reranker\u6a21\u578b\u7cfb\u5217\uff0c\u5b83\u4eec\u57fa\u4e8eQwen3-VL\u57fa\u7840\u6a21\u578b\u6784\u5efa\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u5b9e\u73b0\u4e86\u8de8\u6587\u672c\u3001\u56fe\u50cf\u3001\u6587\u6863\u56fe\u50cf\u548c\u89c6\u9891\u7684\u7edf\u4e00\u8868\u793a\u5b66\u4e60\uff0c\u5728\u591a\u6a21\u6001\u5d4c\u5165\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u641c\u7d22\u4e2d\u4e0d\u540c\u6a21\u6001\u6570\u636e\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u6587\u6863\u56fe\u50cf\u3001\u89c6\u9891\uff09\u7684\u7edf\u4e00\u8868\u793a\u95ee\u9898\uff0c\u6784\u5efa\u7aef\u5230\u7aef\u7684\u9ad8\u7cbe\u5ea6\u591a\u6a21\u6001\u641c\u7d22\u7ba1\u9053\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u548c\u7ec6\u7c92\u5ea6\u76f8\u5173\u6027\u8bc4\u4f30\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "Qwen3-VL-Embedding\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u5bf9\u6bd4\u9884\u8bad\u7ec3\u548c\u91cd\u6392\u5e8f\u6a21\u578b\u84b8\u998f\uff0c\u652f\u6301Matryoshka\u8868\u793a\u5b66\u4e60\u4ee5\u5b9e\u73b0\u7075\u6d3b\u7684\u5d4c\u5165\u7ef4\u5ea6\uff0c\u5904\u7406\u957f\u8fbe32k\u4ee4\u724c\u7684\u8f93\u5165\uff1bQwen3-VL-Reranker\u91c7\u7528\u4ea4\u53c9\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u67e5\u8be2-\u6587\u6863\u5bf9\u7684\u7ec6\u7c92\u5ea6\u76f8\u5173\u6027\u4f30\u8ba1\uff1b\u4e24\u4e2a\u7cfb\u5217\u5747\u7ee7\u627fQwen3-VL\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u652f\u630130\u591a\u79cd\u8bed\u8a00\uff0c\u5e76\u63d0\u4f9b2B\u548c8B\u53c2\u6570\u89c4\u6a21\u4ee5\u9002\u5e94\u4e0d\u540c\u90e8\u7f72\u9700\u6c42\u3002", "result": "Qwen3-VL-Embedding\u7cfb\u5217\u5728\u591a\u6a21\u6001\u5d4c\u5165\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5176\u4e2dQwen3-VL-Embedding-8B\u5728MMEB-V2\u57fa\u51c6\u4e0a\u83b7\u5f9777.8\u7684\u7efc\u5408\u5f97\u5206\uff0c\u5728\u6240\u6709\u6a21\u578b\u4e2d\u6392\u540d\u7b2c\u4e00\uff08\u622a\u81f32025\u5e741\u67088\u65e5\uff09\uff1b\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u3001\u89c6\u89c9\u95ee\u7b54\u548c\u89c6\u9891-\u6587\u672c\u5339\u914d\u7b49\u591a\u79cd\u591a\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u548c\u7edf\u4e00\u8868\u793a\u7a7a\u95f4\u5728\u591a\u6a21\u6001\u5d4c\u5165\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u9645\u591a\u6a21\u6001\u641c\u7d22\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff1bMatryoshka\u8868\u793a\u5b66\u4e60\u548c\u7075\u6d3b\u53c2\u6570\u89c4\u6a21\u8bbe\u8ba1\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u90e8\u7f72\u7075\u6d3b\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2601.04715", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04715", "abs": "https://arxiv.org/abs/2601.04715", "authors": ["Xiao Guo", "Jie Zhu", "Anil Jain", "Xiaoming Liu"], "title": "On the Holistic Approach for Detecting Human Image Forgery", "comment": "6 figures, 5 tables", "summary": "The rapid advancement of AI-generated content (AIGC) has escalated the threat of deepfakes, from facial manipulations to the synthesis of entire photorealistic human bodies. However, existing detection methods remain fragmented, specializing either in facial-region forgeries or full-body synthetic images, and consequently fail to generalize across the full spectrum of human image manipulations. We introduce HuForDet, a holistic framework for human image forgery detection, which features a dual-branch architecture comprising: (1) a face forgery detection branch that employs heterogeneous experts operating in both RGB and frequency domains, including an adaptive Laplacian-of-Gaussian (LoG) module designed to capture artifacts ranging from fine-grained blending boundaries to coarse-scale texture irregularities; and (2) a contextualized forgery detection branch that leverages a Multi-Modal Large Language Model (MLLM) to analyze full-body semantic consistency, enhanced with a confidence estimation mechanism that dynamically weights its contribution during feature fusion. We curate a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans. Extensive experiments show that our HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHuForDet\uff0c\u4e00\u79cd\u7528\u4e8e\u4eba\u7c7b\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u7684\u6574\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u7ed3\u5408\u9762\u90e8\u4f2a\u9020\u68c0\u6d4b\u548c\u4e0a\u4e0b\u6587\u8bed\u4e49\u4e00\u81f4\u6027\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9762\u90e8\u533a\u57df\u4f2a\u9020\u4e0e\u5168\u8eab\u5408\u6210\u56fe\u50cf\u68c0\u6d4b\u4e4b\u95f4\u7684\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "AI\u751f\u6210\u5185\u5bb9\u7684\u5feb\u901f\u53d1\u5c55\u52a0\u5267\u4e86\u6df1\u5ea6\u4f2a\u9020\u5a01\u80c1\uff0c\u4ece\u9762\u90e8\u64cd\u7eb5\u5230\u5168\u8eab\u903c\u771f\u4eba\u4f53\u5408\u6210\uff0c\u4f46\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u4e13\u95e8\u9488\u5bf9\u9762\u90e8\u533a\u57df\u4f2a\u9020\u6216\u5168\u8eab\u5408\u6210\u56fe\u50cf\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u5b8c\u6574\u7684\u4eba\u7c7b\u56fe\u50cf\u64cd\u7eb5\u8c31\u7cfb\u3002", "method": "HuForDet\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff1a\u9762\u90e8\u4f2a\u9020\u68c0\u6d4b\u5206\u652f\u5728RGB\u548c\u9891\u57df\u4e2d\u91c7\u7528\u5f02\u6784\u4e13\u5bb6\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u62c9\u666e\u62c9\u65af-\u9ad8\u65af\u6a21\u5757\u4ee5\u6355\u83b7\u4ece\u7ec6\u7c92\u5ea6\u6df7\u5408\u8fb9\u754c\u5230\u7c97\u5c3a\u5ea6\u7eb9\u7406\u5f02\u5e38\u7684\u4f2a\u5f71\uff1b\u4e0a\u4e0b\u6587\u4f2a\u9020\u68c0\u6d4b\u5206\u652f\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u5168\u8eab\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5e76\u914d\u5907\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u673a\u5236\u5728\u7279\u5f81\u878d\u5408\u4e2d\u52a8\u6001\u52a0\u6743\u5176\u8d21\u732e\u3002", "result": "\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u7684\u4eba\u7c7b\u56fe\u50cf\u4f2a\u9020\u6570\u636e\u96c6\uff0c\u5c06\u73b0\u6709\u9762\u90e8\u4f2a\u9020\u6570\u636e\u4e0e\u65b0\u7684\u5168\u8eab\u5408\u6210\u4eba\u4f53\u8bed\u6599\u5e93\u7ed3\u5408\uff0c\u5b9e\u9a8c\u8868\u660eHuForDet\u5728\u591a\u6837\u5316\u4eba\u7c7b\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u7ec6\u7c92\u5ea6\u9762\u90e8\u4f2a\u5f71\u68c0\u6d4b\u4e0e\u4e0a\u4e0b\u6587\u8bed\u4e49\u5206\u6790\u7684\u6574\u4f53\u6846\u67b6\u5728\u4eba\u7c7b\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684AIGC\u4f2a\u9020\u5a01\u80c1\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u5206\u6790\u548c\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.04736", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04736", "abs": "https://arxiv.org/abs/2601.04736", "authors": ["Han Zhu", "Jiale Chen", "Chengkun Cai", "Shengjie Sun", "Haoran Li", "Yujin Zhou", "Chi-Min Chan", "Pengcheng Wen", "Lei Li", "Sirui Han", "Yike Guo"], "title": "AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs", "comment": null, "summary": "Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\\% in harmless dimension and over 13\\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86InterSafe-V\u591a\u6a21\u6001\u5bf9\u8bdd\u5b89\u5168\u6570\u636e\u96c6\u548cAM\u00b3Safety\u6846\u67b6\uff0c\u901a\u8fc7\u51b7\u542f\u52a8\u62d2\u7edd\u9636\u6bb5\u548c\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u5bf9\u8bdd\u7ea7\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ea4\u4e92\u5e94\u7528\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5728\u591a\u8f6e\u591a\u6a21\u6001\u573a\u666f\u4e2d\uff0c\u6709\u5bb3\u610f\u56fe\u53ef\u80fd\u9010\u6b65\u91cd\u5efa\u800c\u5b89\u5168\u534f\u8bae\u9010\u6e10\u5931\u6548\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u8f6e\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff0c\u4e14\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u504f\u597d\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u5176\u5728\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86InterSafe-V\u5f00\u6e90\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5305\u542b11,270\u4e2a\u5bf9\u8bdd\u548c500\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7684\u62d2\u7edd\u89c6\u89c9\u95ee\u7b54\u6837\u672c\uff0c\u901a\u8fc7\u6a21\u578b\u95f4\u4ea4\u4e92\u6784\u5efa\u4ee5\u66f4\u51c6\u786e\u53cd\u6620\u771f\u5b9e\u573a\u666f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86AM\u00b3Safety\u6846\u67b6\uff0c\u7ed3\u5408\u51b7\u542f\u52a8\u62d2\u7edd\u9636\u6bb5\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u4f7f\u7528\u57fa\u4e8e\u6574\u4e2a\u5bf9\u8bdd\u7684\u8f6e\u6b21\u611f\u77e5\u53cc\u76ee\u6807\u5956\u52b1\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728Qwen2.5-VL-7B-Instruct\u548cLLaVA-NeXT-7B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u8d85\u8fc710%\uff0c\u5728\u591a\u6a21\u6001\u591a\u8f6e\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65e0\u5bb3\u7ef4\u5ea6\u63d0\u5347\u81f3\u5c118%\uff0c\u6709\u5e2e\u52a9\u7ef4\u5ea6\u63d0\u5347\u8d85\u8fc713%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u4e13\u95e8\u6784\u5efa\u7684\u591a\u8f6e\u5bf9\u8bdd\u5b89\u5168\u6570\u636e\u96c6\u548c\u5bf9\u8bdd\u7ea7\u5fae\u8c03\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u5b89\u5168\u6027\u63d0\u5347\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.04734", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04734", "abs": "https://arxiv.org/abs/2601.04734", "authors": ["Yunqing Hu", "Zheming Yang", "Chang Zhao", "Qi Guo", "Meng Gao", "Pengcheng Li", "Wen Ji"], "title": "AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection", "comment": null, "summary": "Multimodal large language models (MLLMs) demonstrate exceptional capabilities in semantic understanding and visual reasoning, yet they still face challenges in precise object localization and resource-constrained edge-cloud deployment. To address this, this paper proposes the AIVD framework, which achieves unified precise localization and high-quality semantic generation through the collaboration between lightweight edge detectors and cloud-based MLLMs. To enhance the cloud MLLM's robustness against edge cropped-box noise and scenario variations, we design an efficient fine-tuning strategy with visual-semantic collaborative augmentation, significantly improving classification accuracy and semantic consistency. Furthermore, to maintain high throughput and low latency across heterogeneous edge devices and dynamic network conditions, we propose a heterogeneous resource-aware dynamic scheduling algorithm. Experimental results demonstrate that AIVD substantially reduces resource consumption while improving MLLM classification performance and semantic generation quality. The proposed scheduling strategy also achieves higher throughput and lower latency across diverse scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AIVD\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8fb9\u7f18\u68c0\u6d4b\u5668\u4e0e\u4e91\u7aefMLLM\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u76ee\u6807\u5b9a\u4f4d\u4e0e\u9ad8\u8d28\u91cf\u8bed\u4e49\u751f\u6210\u7684\u7edf\u4e00\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86\u5f02\u6784\u8d44\u6e90\u611f\u77e5\u7684\u52a8\u6001\u8c03\u5ea6\u7b97\u6cd5\u4ee5\u4f18\u5316\u8fb9\u7f18-\u4e91\u7aef\u90e8\u7f72\u6548\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u7406\u89e3\u548c\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7cbe\u786e\u76ee\u6807\u5b9a\u4f4d\u548c\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18-\u4e91\u7aef\u90e8\u7f72\u573a\u666f\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u8fb9\u7f18\u88c1\u526a\u6846\u566a\u58f0\u3001\u573a\u666f\u53d8\u5316\u4ee5\u53ca\u5f02\u6784\u8bbe\u5907\u52a8\u6001\u7f51\u7edc\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86AIVD\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8fb9\u7f18\u68c0\u6d4b\u5668\u4e0e\u4e91\u7aefMLLM\u7684\u534f\u4f5c\u5b9e\u73b0\u7edf\u4e00\u7cbe\u786e\u5b9a\u4f4d\u548c\u9ad8\u8d28\u91cf\u8bed\u4e49\u751f\u6210\uff1b\u8bbe\u8ba1\u4e86\u89c6\u89c9-\u8bed\u4e49\u534f\u540c\u589e\u5f3a\u7684\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\u4ee5\u63d0\u5347\u4e91\u7aefMLLM\u5bf9\u8fb9\u7f18\u88c1\u526a\u6846\u566a\u58f0\u548c\u573a\u666f\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff1b\u5f00\u53d1\u4e86\u5f02\u6784\u8d44\u6e90\u611f\u77e5\u7684\u52a8\u6001\u8c03\u5ea6\u7b97\u6cd5\u4ee5\u7ef4\u6301\u9ad8\u541e\u5410\u91cf\u548c\u4f4e\u5ef6\u8fdf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAIVD\u6846\u67b6\u5728\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86MLLM\u7684\u5206\u7c7b\u6027\u80fd\u548c\u8bed\u4e49\u751f\u6210\u8d28\u91cf\uff1b\u6240\u63d0\u51fa\u7684\u8c03\u5ea6\u7b56\u7565\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7cfb\u7edf\u6574\u4f53\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u8fb9\u7f18-\u4e91\u7aef\u534f\u540c\u67b6\u6784\u548c\u9488\u5bf9\u6027\u4f18\u5316\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3MLLM\u5728\u7cbe\u786e\u5b9a\u4f4d\u548c\u8fb9\u7f18\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u591a\u6a21\u6001AI\u7cfb\u7edf\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u548c\u6027\u80fd\u4f18\u5316\u601d\u8def\u3002"}}
{"id": "2601.04833", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04833", "abs": "https://arxiv.org/abs/2601.04833", "authors": ["Ke Sun", "Guangsheng Bao", "Han Cui", "Yue Zhang"], "title": "When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection", "comment": null, "summary": "Zero-shot detection methods for AI-generated text typically aggregate token-level statistics across entire sequences, overlooking the temporal dynamics inherent to autoregressive generation. We analyze over 120k text samples and reveal Late-Stage Volatility Decay: AI-generated text exhibits rapidly stabilizing log probability fluctuations as generation progresses, while human writing maintains higher variability throughout. This divergence peaks in the second half of sequences, where AI-generated text shows 24--32\\% lower volatility. Based on this finding, we propose two simple features: Derivative Dispersion and Local Volatility, which computed exclusively from late-stage statistics. Without perturbation sampling or additional model access, our method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86AI\u751f\u6210\u6587\u672c\u7684\u665a\u671f\u6ce2\u52a8\u8870\u51cf\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u665a\u671f\u7edf\u8ba1\u7279\u5f81\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u4e0d\u4f9d\u8d56\u6270\u52a8\u91c7\u6837\u6216\u989d\u5916\u6a21\u578b\u8bbf\u95ee\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672cAI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5728\u6574\u4e2a\u5e8f\u5217\u4e0a\u805a\u5408\u8bcd\u5143\u7ea7\u7edf\u8ba1\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u65f6\u95f4\u52a8\u6001\u7279\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u68c0\u6d4b\u6027\u80fd\u7684\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8d85\u8fc712\u4e07\u4e2a\u6587\u672c\u6837\u672c\uff0c\u53d1\u73b0\u4e86\u665a\u671f\u6ce2\u52a8\u8870\u51cf\u73b0\u8c61\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u4e24\u4e2a\u7b80\u5355\u7279\u5f81\uff1a\u5bfc\u6570\u79bb\u6563\u5ea6\u548c\u5c40\u90e8\u6ce2\u52a8\u6027\uff0c\u8fd9\u4e9b\u7279\u5f81\u4ec5\u4ece\u665a\u671f\u7edf\u8ba1\u4fe1\u606f\u8ba1\u7b97\u5f97\u51fa\uff0c\u65e0\u9700\u6270\u52a8\u91c7\u6837\u6216\u989d\u5916\u6a21\u578b\u8bbf\u95ee\u3002", "result": "\u8be5\u65b9\u6cd5\u5728EvoBench\u548cMAGE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cAI\u751f\u6210\u6587\u672c\u5728\u5e8f\u5217\u540e\u534a\u90e8\u5206\u8868\u73b0\u51fa24-32%\u7684\u66f4\u4f4e\u6ce2\u52a8\u6027\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u5168\u5c40\u65b9\u6cd5\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u4e92\u8865\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86AI\u751f\u6210\u6587\u672c\u7684\u65f6\u95f4\u52a8\u6001\u7279\u6027\u5dee\u5f02\uff0c\u8bc1\u660e\u4e86\u665a\u671f\u7edf\u8ba1\u7279\u5f81\u7684\u68c0\u6d4b\u4ef7\u503c\uff0c\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u7b80\u5355\u7279\u5f81\u4e0e\u590d\u6742\u65b9\u6cd5\u7684\u4e92\u8865\u6f5c\u529b\u3002"}}
{"id": "2601.04777", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04777", "abs": "https://arxiv.org/abs/2601.04777", "authors": ["Shurong Zheng", "Yousong Zhu", "Hongyin Zhao", "Fan Yang", "Yufei Zhan", "Ming Tang", "Jinqiao Wang"], "title": "GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGeM-VG\uff0c\u4e00\u79cd\u80fd\u591f\u8fdb\u884c\u5e7f\u4e49\u591a\u56fe\u50cf\u89c6\u89c9\u5b9a\u4f4d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u4efb\u52a1\u5206\u7c7b\u3001\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6MG-Data-240K\u4ee5\u53ca\u8bbe\u8ba1\u6df7\u5408\u5f3a\u5316\u5fae\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u6837\u5316\u591a\u56fe\u50cf\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u56fe\u50cf\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u76ee\u6807\u5b9a\u4f4d\u548c\u6709\u9650\u7684\u4efb\u52a1\u7c7b\u578b\uff0c\u7f3a\u4e4f\u5bf9\u5e7f\u4e49\u5b9a\u4f4d\u4efb\u52a1\u7684\u7edf\u4e00\u5efa\u6a21\uff0c\u8fd9\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u573a\u666f\u4e0b\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u9996\u5148\u6839\u636e\u8de8\u56fe\u50cf\u7ebf\u7d22\u548c\u63a8\u7406\u9700\u6c42\u5bf9\u591a\u56fe\u50cf\u5b9a\u4f4d\u4efb\u52a1\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\uff0c\u6784\u5efa\u4e86\u5305\u542b240K\u6837\u672c\u7684MG-Data-240K\u6570\u636e\u96c6\u4ee5\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u76ee\u6807\u6570\u91cf\u548c\u56fe\u50cf\u5173\u7cfb\u65b9\u9762\u7684\u9650\u5236\u3002\u4e3a\u89e3\u51b3\u591a\u6837\u5316\u591a\u56fe\u50cf\u5b9a\u4f4d\u4efb\u52a1\u7684\u9c81\u68d2\u5904\u7406\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u76f4\u63a5\u56de\u7b54\u7684\u6df7\u5408\u5f3a\u5316\u5fae\u8c03\u7b56\u7565\uff0c\u91c7\u7528\u57fa\u4e8e\u89c4\u5219\u5956\u52b1\u5f15\u5bfc\u7684R1-like\u7b97\u6cd5\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6574\u4f53\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGeM-VG\u5728\u591a\u56fe\u50cf\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u5728MIG-Bench\u548cMC-Bench\u57fa\u51c6\u4e0a\u5206\u522b\u6bd4\u5148\u524d\u9886\u5148\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u4e862.0%\u548c9.7%\u3002\u5728\u5355\u56fe\u50cf\u5b9a\u4f4d\u4efb\u52a1\u4e2d\uff0c\u76f8\u8f83\u4e8e\u57fa\u7840\u6a21\u578b\u5728ODINW\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e869.1%\u7684\u6539\u8fdb\u3002\u540c\u65f6\uff0c\u6a21\u578b\u5728\u901a\u7528\u591a\u56fe\u50cf\u7406\u89e3\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7edf\u4e00\u5efa\u6a21\u6846\u67b6\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u6df7\u5408\u5fae\u8c03\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u56fe\u50cf\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u7684\u6cdb\u5316\u6311\u6218\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u56fe\u50cf\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5728\u4fdd\u6301\u901a\u7528\u7406\u89e3\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u7279\u5b9a\u5b9a\u4f4d\u6027\u80fd\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2601.04857", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04857", "abs": "https://arxiv.org/abs/2601.04857", "authors": ["Zhiwei Liu", "Paul Thompson", "Jiaqi Rong", "Baojie Qu", "Runteng Guo", "Min Peng", "Qianqian Xie", "Sophia Ananiadou"], "title": "MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News", "comment": "Work in progress", "summary": "Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MisSpans\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u7ec6\u7c92\u5ea6\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e0e\u5206\u6790\u7684\u591a\u9886\u57df\u3001\u4eba\u5de5\u6807\u6ce8\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e09\u4e2a\u4e92\u8865\u4efb\u52a1\uff1a\u865a\u5047\u7247\u6bb5\u8bc6\u522b\u3001\u865a\u5047\u7c7b\u578b\u5206\u7c7b\u548c\u57fa\u4e8e\u7247\u6bb5\u7684\u89e3\u91ca\u751f\u6210\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u53e5\u5b50\u7ea7\u522b\u865a\u5047\u4fe1\u606f\u5b9a\u4f4d\u548c\u89e3\u91ca\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u57fa\u51c6\u548c\u65b9\u6cd5\u901a\u5e38\u5728\u6574\u4e2a\u58f0\u660e\u6216\u6bb5\u843d\u7ea7\u522b\u4f7f\u7528\u7c97\u7c92\u5ea6\u7684\u4e8c\u5143\u6807\u7b7e\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u63a9\u76d6\u4e86\u771f\u5b9e\u548c\u865a\u5047\u7ec6\u8282\u7ecf\u5e38\u5728\u5355\u4e2a\u53e5\u5b50\u4e2d\u5171\u5b58\u7684\u73b0\u8c61\u3002\u8fd9\u4e9b\u7b80\u5316\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\uff1a\u5168\u5c40\u89e3\u91ca\u65e0\u6cd5\u8bc6\u522b\u54ea\u4e9b\u5177\u4f53\u7247\u6bb5\u5177\u6709\u8bef\u5bfc\u6027\uff0c\u4e5f\u65e0\u6cd5\u533a\u5206\u7ec6\u8282\u865a\u5047\u7684\u65b9\u5f0f\uff08\u4f8b\u5982\u626d\u66f2\u4e0e\u634f\u9020\uff09\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86MisSpans\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u914d\u5bf9\u7684\u771f\u5b9e\u548c\u865a\u5047\u65b0\u95fb\u6545\u4e8b\uff0c\u5b9a\u4e49\u4e86\u4e09\u4e2a\u4e92\u8865\u4efb\u52a1\uff1aMisSpansIdentity\u7528\u4e8e\u5728\u53e5\u5b50\u5185\u7cbe\u786e\u5b9a\u4f4d\u865a\u5047\u7247\u6bb5\uff0cMisSpansType\u7528\u4e8e\u6309\u865a\u5047\u4fe1\u606f\u7c7b\u578b\u5bf9\u865a\u5047\u7247\u6bb5\u8fdb\u884c\u5206\u7c7b\uff0cMisSpansExplanation\u7528\u4e8e\u57fa\u4e8e\u5df2\u8bc6\u522b\u7247\u6bb5\u63d0\u4f9b\u7406\u6027\u89e3\u91ca\u3002\u4e13\u5bb6\u6807\u6ce8\u8005\u9075\u5faa\u6807\u51c6\u5316\u6307\u5357\u548c\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u3002\u7814\u7a76\u8bc4\u4f30\u4e8615\u4e2a\u4ee3\u8868\u6027\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u63a8\u7406\u589e\u5f3a\u548c\u975e\u63a8\u7406\u53d8\u4f53\uff0c\u5728\u96f6\u6837\u672c\u548c\u5355\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ec6\u7c92\u5ea6\u865a\u5047\u4fe1\u606f\u8bc6\u522b\u548c\u5206\u6790\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u591a\u4e2a\u4ea4\u4e92\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u6027\u80fd\uff0c\u5305\u62ec\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u53ca\u9886\u57df\u7279\u5b9a\u7684\u6587\u672c\u7279\u5f81\u3002\u6807\u6ce8\u8fc7\u7a0b\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "MisSpans\u57fa\u51c6\u4e3a\u7ec6\u7c92\u5ea6\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u786e\u8bc6\u522b\u548c\u89e3\u91ca\u865a\u5047\u4fe1\u606f\u7247\u6bb5\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u8003\u8651\u6a21\u578b\u5927\u5c0f\u3001\u63a8\u7406\u80fd\u529b\u548c\u9886\u57df\u7279\u5f81\u7b49\u591a\u91cd\u56e0\u7d20\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u7cbe\u786e\u3001\u53ef\u89e3\u91ca\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.04778", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.04778", "abs": "https://arxiv.org/abs/2601.04778", "authors": ["Tobia Poppi", "Burak Uzkent", "Amanmeet Garg", "Lucas Porto", "Garin Kessler", "Yezhou Yang", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara", "Florian Schiffers"], "title": "CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models", "comment": null, "summary": "Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u53cd\u4e8b\u5b9e\u89c6\u9891\u751f\u6210\u6846\u67b6CounterVid\uff0c\u7528\u4e8e\u7f13\u89e3\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u52a8\u4f5c\u548c\u65f6\u5e8f\u63a8\u7406\uff0c\u5e76\u5f15\u5165MixDPO\u65b9\u6cd5\u8054\u5408\u5229\u7528\u6587\u672c\u548c\u89c6\u89c9\u504f\u597d\u8fdb\u884c\u4f18\u5316\u3002", "motivation": "\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u52a8\u4f5c\u548c\u65f6\u5e8f\u63a8\u7406\u65b9\u9762\u4ecd\u7136\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u5982\u6587\u672c\u8fc7\u6ee4\u6216\u968f\u673a\u89c6\u9891\u6270\u52a8\u5f80\u5f80\u672a\u80fd\u89e3\u51b3\u6839\u672c\u539f\u56e0\uff1a\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u800c\u975e\u7ec6\u7c92\u5ea6\u89c6\u89c9\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u53ef\u6269\u5c55\u7684\u53cd\u4e8b\u5b9e\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001LLM\u8fdb\u884c\u52a8\u4f5c\u63d0\u8bae\u548c\u7f16\u8f91\u6307\u5bfc\uff0c\u5229\u7528\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u548c\u89c6\u9891\u6a21\u578b\u751f\u6210\u5927\u89c4\u6a21\u8bed\u4e49\u786c\u8d1f\u6837\u672c\uff1b\u6784\u5efaCounterVid\u5408\u6210\u6570\u636e\u96c6\u5305\u542b\u7ea626k\u504f\u597d\u5bf9\uff0c\u5e76\u5f15\u5165MixDPO\u7edf\u4e00\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u8054\u5408\u5229\u7528\u6587\u672c\u548c\u89c6\u89c9\u504f\u597d\u3002", "result": "\u4f7f\u7528MixDPO\u5fae\u8c03Qwen2.5-VL\u6a21\u578b\u5e26\u6765\u4e00\u81f4\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u65f6\u5e8f\u6392\u5e8f\u65b9\u9762\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u5e76\u6709\u6548\u8fc1\u79fb\u5230\u6807\u51c6\u89c6\u9891\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u53cd\u4e8b\u5b9e\u89c6\u9891\u751f\u6210\u548c\u6df7\u5408\u504f\u597d\u4f18\u5316\uff0c\u4e3a\u89e3\u51b3\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u7279\u522b\u662f\u5728\u52a8\u4f5c\u548c\u65f6\u5e8f\u63a8\u7406\u65b9\u9762\uff0c\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.04897", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.04897", "abs": "https://arxiv.org/abs/2601.04897", "authors": ["Ziteng Wang", "Yujie He", "Guanliang Li", "Siqi Yang", "Jiaqi Xiong", "Songxiang Liu"], "title": "V-FAT: Benchmarking Visual Fidelity Against Text-bias", "comment": "12 pages, 6 figures", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize \"lucky\" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faV-FAT\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bca\u65ad\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6587\u672c\u504f\u89c1\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u89c6\u89c9\u8bc1\u636e\u4e0e\u6587\u672c\u4fe1\u606f\u51b2\u7a81\u65f6\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u6377\u5f84\u800c\u975e\u771f\u5b9e\u89c6\u89c9\u57fa\u7840\u7684\u73b0\u8c61\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6807\u51c6\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u6377\u5f84\u800c\u975e\u771f\u5b9e\u89c6\u89c9\u57fa\u7840\u7684\u95ee\u9898\uff0c\u5373\u6587\u672c\u504f\u89c1\u73b0\u8c61\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u89c6\u89c9\u611f\u77e5\u4e0e\u8bed\u8a00\u5148\u9a8c\u4e4b\u95f4\u7684\u6839\u672c\u5f20\u529b\uff0c\u5e76\u5c06\u504f\u89c1\u6765\u6e90\u89e3\u8026\u4e3a\u5185\u90e8\u8bed\u6599\u504f\u89c1\u548c\u5916\u90e8\u6307\u4ee4\u504f\u89c1\u4e24\u4e2a\u7ef4\u5ea6\u3002", "method": "\u7814\u7a76\u5f15\u5165V-FAT\u8bca\u65ad\u57fa\u51c6\uff0c\u5305\u542b4,026\u4e2aVQA\u5b9e\u4f8b\uff0c\u6db5\u76d6\u516d\u4e2a\u8bed\u4e49\u9886\u57df\u3002\u91c7\u7528\u4e09\u7ea7\u8bc4\u4f30\u6846\u67b6\u7cfb\u7edf\u6027\u5730\u589e\u52a0\u89c6\u89c9\u8bc1\u636e\u4e0e\u6587\u672c\u4fe1\u606f\u4e4b\u95f4\u7684\u51b2\u7a81\uff1aL1\u7ea7\u5904\u7406\u975e\u5178\u578b\u56fe\u50cf\u5f15\u53d1\u7684\u5185\u90e8\u504f\u89c1\uff0cL2\u7ea7\u5904\u7406\u8bef\u5bfc\u6027\u6307\u4ee4\u5f15\u53d1\u7684\u5916\u90e8\u504f\u89c1\uff0cL3\u7ea7\u5904\u7406\u4e24\u8005\u534f\u540c\u7684\u504f\u89c1\u3002\u540c\u65f6\u63d0\u51fa\u89c6\u89c9\u9c81\u68d2\u6027\u8bc4\u5206\u6307\u6807\uff0c\u65e8\u5728\u60e9\u7f5a\"\u5e78\u8fd0\"\u7684\u8bed\u8a00\u731c\u6d4b\u5e76\u5956\u52b1\u771f\u5b9e\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "result": "\u5bf912\u4e2a\u524d\u6cbfMLLM\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5c3d\u7ba1\u6a21\u578b\u5728\u73b0\u6709\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9ad8\u8bed\u8a00\u4e3b\u5bfc\u6027\u6761\u4ef6\u4e0b\u4f1a\u51fa\u73b0\u663e\u8457\u7684\u89c6\u89c9\u5d29\u6e83\u3002V-FAT\u57fa\u51c6\u63ed\u793a\u4e86\u6a21\u578b\u5728\u9762\u5bf9\u89c6\u89c9\u8bc1\u636e\u4e0e\u6587\u672c\u4fe1\u606f\u51b2\u7a81\u65f6\u7684\u7cfb\u7edf\u6027\u5f31\u70b9\uff0c\u7279\u522b\u662f\u5728L2\u548cL3\u7ea7\u8bc4\u4f30\u4e2d\u8868\u73b0\u660e\u663e\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f53\u524dMLLM\u5b58\u5728\u4e25\u91cd\u7684\u6587\u672c\u504f\u89c1\u95ee\u9898\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u800c\u975e\u89c6\u89c9\u8bc1\u636e\u3002\u8fd9\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u65b0\u65b9\u6cd5\u8bba\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\uff0c\u9700\u8981\u66f4\u597d\u5730\u5e73\u8861\u89c6\u89c9\u4e0e\u8bed\u8a00\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2601.04791", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04791", "abs": "https://arxiv.org/abs/2601.04791", "authors": ["Lee Hyoseok", "Sohwi Lim", "Eunju Cha", "Tae-Hyun Oh"], "title": "Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers", "comment": "Under Review", "summary": "With recent advances in generative models, diffusion models have emerged as powerful priors for solving inverse problems in each domain. Since Latent Diffusion Models (LDMs) provide generic priors, several studies have explored their potential as domain-agnostic zero-shot inverse solvers. Despite these efforts, existing latent diffusion inverse solvers suffer from their instability, exhibiting undesirable artifacts and degraded quality. In this work, we first identify the instability as a discrepancy between the solver's and true reverse diffusion dynamics, and show that reducing this gap stabilizes the solver. Building on this, we introduce Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies the LDM-based inverse solvers through measurement-consistent Langevin updates. Compared to prior approaches that rely on linear manifold assumptions, which often do not hold in latent space, MCLC operates without this assumption, leading to more stable and reliable behavior. We experimentally demonstrate the effectiveness of MCLC and its compatibility with existing solvers across diverse image restoration tasks. Additionally, we analyze blob artifacts and offer insights into their underlying causes. We highlight that MCLC is a key step toward more robust zero-shot inverse problem solvers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6d4b\u91cf\u4e00\u81f4\u6717\u4e4b\u4e07\u6821\u6b63\u5668\uff08MCLC\uff09\uff0c\u4e00\u79cd\u7406\u8bba\u57fa\u7840\u7684\u5373\u63d2\u5373\u7528\u6821\u6b63\u6a21\u5757\uff0c\u7528\u4e8e\u7a33\u5b9a\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u96f6\u6837\u672c\u9006\u95ee\u9898\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u51cf\u5c11\u6c42\u89e3\u5668\u4e0e\u771f\u5b9e\u53cd\u5411\u6269\u6563\u52a8\u6001\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u96f6\u6837\u672c\u9006\u95ee\u9898\u6c42\u89e3\u5668\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u8868\u73b0\u4e3a\u4e0d\u5e0c\u671b\u7684\u4f2a\u5f71\u548c\u9000\u5316\u8d28\u91cf\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u6e90\u4e8e\u6c42\u89e3\u5668\u4e0e\u771f\u5b9e\u53cd\u5411\u6269\u6563\u52a8\u6001\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u7ebf\u6027\u6d41\u5f62\u5047\u8bbe\u7684\u66f4\u7a33\u5b9a\u6821\u6b63\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6839\u672c\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u6d4b\u91cf\u4e00\u81f4\u6717\u4e4b\u4e07\u6821\u6b63\u5668\uff08MCLC\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7406\u8bba\u57fa\u7840\u7684\u5373\u63d2\u5373\u7528\u6821\u6b63\u6a21\u5757\uff0c\u901a\u8fc7\u6d4b\u91cf\u4e00\u81f4\u7684\u6717\u4e4b\u4e07\u66f4\u65b0\u6765\u4fee\u6b63\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u9006\u95ee\u9898\u6c42\u89e3\u5668\u3002\u4e0e\u4f9d\u8d56\u7ebf\u6027\u6d41\u5f62\u5047\u8bbe\u7684\u5148\u524d\u65b9\u6cd5\u4e0d\u540c\uff0cMCLC\u65e0\u9700\u6b64\u5047\u8bbe\uff0c\u80fd\u591f\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u66f4\u7a33\u5b9a\u53ef\u9760\u7684\u884c\u4e3a\uff0c\u76f4\u63a5\u9488\u5bf9\u6c42\u89e3\u5668\u4e0e\u771f\u5b9e\u53cd\u5411\u6269\u6563\u52a8\u6001\u4e4b\u95f4\u7684\u5dee\u5f02\u8fdb\u884c\u6821\u6b63\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMCLC\u5728\u591a\u79cd\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u6c42\u89e3\u5668\u517c\u5bb9\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u6591\u70b9\u4f2a\u5f71\u73b0\u8c61\uff0c\u5e76\u6df1\u5165\u63a2\u8ba8\u4e86\u5176\u6839\u672c\u539f\u56e0\uff0c\u9a8c\u8bc1\u4e86MCLC\u5728\u63d0\u9ad8\u6c42\u89e3\u5668\u7a33\u5b9a\u6027\u548c\u51cf\u5c11\u4f2a\u5f71\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "MCLC\u4ee3\u8868\u4e86\u5411\u66f4\u7a33\u5065\u7684\u96f6\u6837\u672c\u9006\u95ee\u9898\u6c42\u89e3\u5668\u8fc8\u51fa\u7684\u5173\u952e\u4e00\u6b65\uff0c\u5176\u65e0\u9700\u7ebf\u6027\u6d41\u5f62\u5047\u8bbe\u7684\u65b9\u6cd5\u4e3a\u89e3\u51b3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u9006\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u65b9\u6848\u3002\u8be5\u7814\u7a76\u4e0d\u4ec5\u63d0\u51fa\u4e86\u6709\u6548\u7684\u6821\u6b63\u673a\u5236\uff0c\u8fd8\u4e3a\u7406\u89e3\u4f2a\u5f71\u73b0\u8c61\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2601.04960", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.04960", "abs": "https://arxiv.org/abs/2601.04960", "authors": ["Qing Wang", "Zehan Li", "Yaodong Song", "Hongjie Chen", "Jian Kang", "Jie Lian", "Jie Li", "Yongxiang Li", "Xuelong Li"], "title": "A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction", "comment": null, "summary": "This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u60c5\u611f\u667a\u80fd\u53e3\u8bed\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6ce8\u5165\u5f0f\u60c5\u611f\u5f52\u56e0\u601d\u7ef4\u6570\u636e\u6784\u5efa\u7b56\u7565\uff0c\u5c06\u7528\u6237\u60c5\u611f\u72b6\u6001\u53ca\u5176\u6210\u56e0\u878d\u5165\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u60c5\u611f\u611f\u77e5\u63a8\u7406\u7684\u5185\u5316\u800c\u975e\u663e\u5f0f\u76d1\u7763\u3002", "motivation": "\u73b0\u6709\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\u5728\u60c5\u611f\u667a\u80fd\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u901a\u5e38\u5c06\u60c5\u611f\u611f\u77e5\u4f5c\u4e3a\u5916\u90e8\u76d1\u7763\u800c\u975e\u6a21\u578b\u5185\u90e8\u63a8\u7406\u80fd\u529b\uff0c\u7f3a\u4e4f\u5bf9\u7528\u6237\u60c5\u611f\u72b6\u6001\u53ca\u5176\u6210\u56e0\u7684\u6df1\u5ea6\u7406\u89e3\u4e0e\u6574\u5408\uff0c\u8fd9\u9650\u5236\u4e86\u7cfb\u7edf\u5728\u60c5\u611f\u8f68\u8ff9\u5efa\u6a21\u3001\u60c5\u611f\u63a8\u7406\u548c\u5171\u60c5\u56de\u5e94\u751f\u6210\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u6ce8\u5165\u5f0f\u60c5\u611f\u5f52\u56e0\u601d\u7ef4\u6570\u636e\u6784\u5efa\u7b56\u7565\uff0c\u5c06\u7528\u6237\u60c5\u611f\u72b6\u6001\u53ca\u5176\u6210\u56e0\u878d\u5165\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u81ea\u84b8\u998f\u8fdb\u884c\u8bed\u97f3-\u6587\u672c\u5bf9\u9f50\u548c\u60c5\u611f\u5c5e\u6027\u5efa\u6a21\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u884c\u7aef\u5230\u7aef\u8de8\u6a21\u6001\u8054\u5408\u4f18\u5316\u4ee5\u786e\u4fdd\u6587\u672c\u4e0e\u53e3\u8bed\u60c5\u611f\u8868\u8fbe\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728Human-like Spoken Dialogue Systems Challenge\u60c5\u611f\u667a\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u60c5\u611f\u8f68\u8ff9\u5efa\u6a21\u3001\u60c5\u611f\u63a8\u7406\u548c\u5171\u60c5\u56de\u5e94\u751f\u6210\u65b9\u9762\u5747\u53d6\u5f97\u9876\u7ea7\u6027\u80fd\uff0c\u5728\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "IEAT\u7b56\u7565\u901a\u8fc7\u5c06\u60c5\u611f\u5f52\u56e0\u601d\u7ef4\u6ce8\u5165\u6a21\u578b\u5185\u90e8\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u60c5\u611f\u611f\u77e5\u80fd\u529b\u7684\u5185\u5316\uff0c\u4e3a\u6784\u5efa\u66f4\u81ea\u7136\u3001\u66f4\u5177\u60c5\u611f\u667a\u80fd\u7684\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u8de8\u6a21\u6001\u60c5\u611f\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.04798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04798", "abs": "https://arxiv.org/abs/2601.04798", "authors": ["Tamara R. Lenhard", "Andreas Weinmann", "Hichem Snoussi", "Tobias Koch"], "title": "Detector-Augmented SAMURAI for Long-Duration Drone Tracking", "comment": "Accepted at the WACV 2026 Workshop on \"Real World Surveillance: Applications and Challenges\"", "summary": "Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86SAMURAI\u57fa\u7840\u6a21\u578b\u5728\u65e0\u4eba\u673a\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u5668\u589e\u5f3a\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u957f\u671f\u65e0\u4eba\u673a\u8ddf\u8e2a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u68c0\u6d4b\u5668\u7684\u65e0\u4eba\u673a\u8ddf\u8e2a\u65b9\u6cd5\u867d\u7136\u5e27\u7ea7\u7cbe\u5ea6\u8f83\u9ad8\uff0c\u4f46\u5b58\u5728\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u548c\u9891\u7e41\u68c0\u6d4b\u4e22\u5931\u7684\u95ee\u9898\uff0c\u800cRGB\u65e0\u4eba\u673a\u8ddf\u8e2a\u7814\u7a76\u4ecd\u6709\u9650\u4e14\u4f9d\u8d56\u4f20\u7edf\u8fd0\u52a8\u6a21\u578b\u3002\u5c3d\u7ba1SAMURAI\u7b49\u57fa\u7840\u6a21\u578b\u5728\u5176\u4ed6\u9886\u57df\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7c7b\u522b\u65e0\u5173\u8ddf\u8e2a\u6027\u80fd\uff0c\u4f46\u5176\u5728\u65e0\u4eba\u673a\u7279\u5b9a\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u5c1a\u672a\u5f97\u5230\u7814\u7a76\uff0c\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u4fc3\u4f7f\u672c\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u5bf9SAMURAI\u57fa\u7840\u6a21\u578b\u5728\u65e0\u4eba\u673a\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u968f\u540e\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u5668\u589e\u5f3a\u7684SAMURAI\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u68c0\u6d4b\u5668\u7ebf\u7d22\u6765\u51cf\u8f7b\u5bf9\u8fb9\u754c\u6846\u521d\u59cb\u5316\u548c\u5e8f\u5217\u957f\u5ea6\u7684\u654f\u611f\u6027\uff0c\u4ece\u800c\u63d0\u5347\u8ddf\u8e2a\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u68c0\u6d4b\u5668\u589e\u5f3a\u6269\u5c55\u65b9\u6cd5\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u5e8f\u5217\u548c\u65e0\u4eba\u673a\u9000\u51fa-\u91cd\u5165\u4e8b\u4ef6\u4e2d\u8868\u73b0\u7a81\u51fa\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6307\u6807\u4e0a\u76f8\u6bd4SAMURAI\u7684\u96f6\u6837\u672c\u6027\u80fd\u5747\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u6210\u529f\u7387\u6700\u9ad8\u63d0\u5347+0.393\uff0c\u8bef\u62a5\u7387\u6700\u9ad8\u964d\u4f4e-0.475\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u5b9e\u4e86\u57fa\u7840\u6a21\u578b\u5728\u65e0\u4eba\u673a\u8ddf\u8e2a\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u68c0\u6d4b\u5668\u589e\u5f3a\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u754c\u6846\u521d\u59cb\u5316\u548c\u5e8f\u5217\u957f\u5ea6\u654f\u611f\u6027\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e0b\u7684\u957f\u671f\u65e0\u4eba\u673a\u76d1\u63a7\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u65e0\u4eba\u673a\u8ddf\u8e2a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u65b9\u5411\u3002"}}
{"id": "2601.05062", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05062", "abs": "https://arxiv.org/abs/2601.05062", "authors": ["Gorjan Radevski", "Kiril Gashteovski", "Giwon Hong", "Carolin Lawrence", "Goran Glava\u0161"], "title": "Compositional Steering of Large Language Models with Steering Tokens", "comment": null, "summary": "Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \\textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \\emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \\textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \\textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \\textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u7ec4\u5408\u5f15\u5bfc\u4ee4\u724c\"\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u884c\u4e3a\u7ec4\u5408\u63a7\u5236\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u84b8\u998f\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7f16\u7801\u4e3a\u4e13\u7528\u4ee4\u724c\uff0c\u5e76\u8bad\u7ec3\u7ec4\u5408\u4ee4\u724c\u6765\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u884c\u4e3a\u7ec4\u5408\uff0c\u4ece\u800c\u5728\u8f93\u5165\u4ee4\u724c\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u96f6\u6837\u672c\u7ec4\u5408\u5f15\u5bfc\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u90e8\u7f72LLM\u9700\u8981\u6ee1\u8db3\u591a\u4e2a\u671f\u671b\u7684\u53ef\u63a7\u8f93\u51fa\uff0c\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u884c\u4e3a\u7684\u5f15\u5bfc\uff0c\u800c\u7ec4\u5408\u5f15\u5bfc\u2014\u2014\u5373\u540c\u65f6\u5f15\u5bfcLLM\u671d\u5411\u591a\u4e2a\u884c\u4e3a\u2014\u2014\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002\u5f53\u524d\u65b9\u6cd5\u5927\u591a\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u5bf9\u591a\u884c\u4e3a\u7ec4\u5408\u7684\u6709\u6548\u96f6\u6837\u672c\u63a7\u5236\u80fd\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u901a\u8fc7\u81ea\u84b8\u998f\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8868\u8fbe\u7684\u4e2a\u4f53\u884c\u4e3a\u5d4c\u5165\u5230\u4e13\u7528\u4ee4\u724c\u4e2d\uff0c\u4f7f\u884c\u4e3a\u5f15\u5bfc\u5728\u8f93\u5165\u4ee4\u724c\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u800c\u975e\u6fc0\u6d3b\u7a7a\u95f4\u3002\u968f\u540e\u8bad\u7ec3\u4e13\u7528\u7684\u7ec4\u5408\u4ee4\u724c\u6765\u5904\u7406\u884c\u4e3a\u5bf9\uff0c\u8be5\u4ee4\u724c\u80fd\u591f\u6210\u529f\u6355\u6349\u7ec4\u5408\u6982\u5ff5\uff0c\u5e76\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u884c\u4e3a\u7ec4\u5408\uff0c\u5305\u62ec\u5305\u542b\u672a\u89c1\u884c\u4e3a\u7684\u7ec4\u5408\u4ee5\u53ca\u5177\u6709\u672a\u89c1\u884c\u4e3a\u6570\u91cf\u7684\u7ec4\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u540cLLM\u67b6\u6784\u4e0a\uff0c\u5f15\u5bfc\u4ee4\u724c\u65b9\u6cd5\u76f8\u6bd4\u7ade\u4e89\u65b9\u6cd5\uff08\u6307\u4ee4\u3001\u6fc0\u6d3b\u5f15\u5bfc\u548cLoRA\u5408\u5e76\uff09\u5728\u591a\u884c\u4e3a\u63a7\u5236\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002\u6b64\u5916\uff0c\u5f15\u5bfc\u4ee4\u724c\u4e0e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5177\u6709\u4e92\u8865\u6027\uff0c\u4e8c\u8005\u7684\u7ec4\u5408\u80fd\u591f\u5e26\u6765\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7ec4\u5408\u4ee4\u724c\u80fd\u591f\u6709\u6548\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u884c\u4e3a\u7ec4\u5408\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u8f93\u5165\u4ee4\u724c\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u7ec4\u5408\u5f15\u5bfc\u7684\u6709\u6548\u6027\uff0c\u4e3aLLM\u7684\u591a\u884c\u4e3a\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002\u7ec4\u5408\u4ee4\u724c\u7684\u6cdb\u5316\u80fd\u529b\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5f15\u5bfc\u4ee4\u724c\u4e0e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u4e92\u8865\u6027\u4e3a\u672a\u6765\u6df7\u5408\u63a7\u5236\u65b9\u6cd5\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u7ec4\u5408\u63a7\u5236\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.04824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04824", "abs": "https://arxiv.org/abs/2601.04824", "authors": ["Oriol Rabasseda", "Zenjie Li", "Kamal Nasrollahi", "Sergio Escalera"], "title": "SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models", "comment": "This work has been accepted at Real World Surveillance: Applications and Challenges, 6th (in WACV Workshops)", "summary": "Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.\n  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SOVABench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u76d1\u63a7\u89c6\u9891\u4e2d\u8f66\u8f86\u52a8\u4f5c\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u514d\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u89e3\u91ca\u7684\u63cf\u8ff0\u5d4c\u5165\u6765\u63d0\u5347\u52a8\u4f5c\u533a\u5206\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5185\u5bb9\u7684\u89c6\u9891\u68c0\u7d22\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u573a\u666f\u7ea7\u76f8\u4f3c\u6027\uff0c\u7f3a\u4e4f\u5bf9\u76d1\u63a7\u573a\u666f\u4e2d\u52a8\u4f5c\u533a\u5206\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u8f66\u8f86\u76f8\u5173\u52a8\u4f5c\u7684\u8bc6\u522b\u5b58\u5728\u7814\u7a76\u7a7a\u767d\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u534f\u8bae\u6765\u8861\u91cf\u8de8\u52a8\u4f5c\u533a\u5206\u548c\u65f6\u95f4\u65b9\u5411\u7406\u89e3\u80fd\u529b\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86SOVABench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e24\u79cd\u8bc4\u4f30\u534f\u8bae\uff08inter-pair\u548cintra-pair\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u514d\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528MLLM\u7684\u89c6\u89c9\u63a8\u7406\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u751f\u6210\u56fe\u50cf\u548c\u89c6\u9891\u7684\u53ef\u89e3\u91ca\u63cf\u8ff0\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u548c\u591a\u6a21\u6001\u6a21\u578b\u5728\u52a8\u4f5c\u533a\u5206\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u63d0\u51fa\u7684\u514d\u8bad\u7ec3\u6846\u67b6\u5728SOVABench\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u5f3a\u52b2\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u5931\u8d25\u7684\u7a7a\u95f4\u548c\u8ba1\u6570\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u76d1\u63a7\u89c6\u9891\u4e2d\u52a8\u4f5c\u533a\u5206\u5bf9\u73b0\u6709\u6a21\u578b\u7684\u6311\u6218\u6027\uff0c\u8bc1\u660e\u4e86MLLM\u751f\u6210\u63cf\u8ff0\u5d4c\u5165\u7684\u6709\u6548\u6027\uff0c\u4e3a\u76d1\u63a7\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u65b9\u6cd5\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u53ef\u89e3\u91ca\u89c6\u9891\u7406\u89e3\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.05075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05075", "abs": "https://arxiv.org/abs/2601.05075", "authors": ["Ziyang Chen", "Zhenxuan Huang", "Yile Wang", "Weiqin Wang", "Lu Yin", "Hui Huang"], "title": "SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment", "comment": null, "summary": "Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSemPA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u504f\u597d\u5bf9\u9f50\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53e5\u5b50\u8868\u793a\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u751f\u6210\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u53e5\u5b50\u7ea7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5728\u91ca\u4e49\u751f\u6210\u4efb\u52a1\u4e0a\u9ad8\u6548\u4f18\u5316LLMs\uff0c\u5728\u4fdd\u6301\u56fa\u6709\u751f\u6210\u80fd\u529b\u7684\u540c\u65f6\u5b66\u4e60\u533a\u5206\u8bed\u4e49\u7b49\u6548\u53e5\u5b50\u3002", "motivation": "\u4f20\u7edf\u53e5\u5b50\u5d4c\u5165\u65b9\u6cd5\u5728\u975e\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\u4f7f\u7528token\u7ea7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u800c\u57fa\u4e8e\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u56fa\u5b9a\u63d0\u793a\u6a21\u677f\u7f3a\u4e4f\u6a21\u578b\u8fdb\u4e00\u6b65\u4f18\u5316\u5bfc\u81f4\u6027\u80fd\u6709\u9650\uff0c\u4fee\u6539\u6a21\u578b\u67b6\u6784\u4f1a\u6539\u53d8\u5185\u90e8\u8ba1\u7b97\u673a\u5236\u4ece\u800c\u635f\u5bb3\u751f\u6210\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u589e\u5f3a\u53e5\u5b50\u8868\u793a\u53c8\u4e0d\u727a\u7272LLMs\u751f\u6210\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSemPA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u504f\u597d\u5bf9\u9f50\u589e\u5f3a\u53e5\u5b50\u8868\u793a\u540c\u65f6\u4fdd\u6301LLMs\u751f\u6210\u80fd\u529b\u3002\u91c7\u7528\u53e5\u5b50\u7ea7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5728\u91ca\u4e49\u751f\u6210\u4efb\u52a1\u4e0a\u9ad8\u6548\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u6a21\u578b\u5b66\u4e60\u533a\u5206\u8bed\u4e49\u7b49\u6548\u53e5\u5b50\u3002\u5728\u7406\u8bba\u5c42\u9762\uff0c\u5728Plackett-Luce\u6a21\u578b\u6846\u67b6\u4e0b\u5efa\u7acb\u4e86DPO\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u4e4b\u95f4\u7684\u5f62\u5f0f\u5316\u8054\u7cfb\u3002", "result": "\u5728\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u4efb\u52a1\u548c\u5404\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSemPA\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8bed\u4e49\u8868\u793a\uff0c\u540c\u65f6\u4e0d\u727a\u7272LLMs\u56fa\u6709\u7684\u751f\u6210\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6a21\u578b\u751f\u6210\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u53e5\u5b50\u5d4c\u5165\u7684\u8d28\u91cf\u548c\u6548\u679c\u3002", "conclusion": "SemPA\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u5e73\u8861\u53e5\u5b50\u8868\u793a\u589e\u5f3a\u4e0e\u751f\u6210\u80fd\u529b\u4fdd\u6301\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u504f\u597d\u5bf9\u9f50\u5b9e\u73b0\u4e86\u4e24\u65b9\u9762\u7684\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u4e3a\u5728\u4fdd\u6301\u5927\u8bed\u8a00\u6a21\u578b\u6838\u5fc3\u80fd\u529b\u7684\u540c\u65f6\u6539\u8fdb\u5176\u5d4c\u5165\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2601.04860", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04860", "abs": "https://arxiv.org/abs/2601.04860", "authors": ["Ayush Pande"], "title": "DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation", "comment": null, "summary": "Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360\u00b0 and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DivAS\uff0c\u4e00\u79cd\u65e0\u9700\u4f18\u5316\u7684\u5b8c\u5168\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u5272\u795e\u7ecf\u8f90\u5c04\u573a\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f15\u5bfc\u76842D\u63a9\u7801\u805a\u5408\u5b9e\u73b0\u5b9e\u65f63D\u5206\u5272\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u6240\u9700\u7684\u9010\u573a\u666f\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f18\u5316\u7684NeRF\u5206\u5272\u65b9\u6cd5\u9700\u8981\u7f13\u6162\u7684\u9010\u573a\u666f\u8bad\u7ec3\uff0c\u727a\u7272\u4e862D\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u4ea4\u4e92\u5f0f\u5206\u5272\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "method": "DivAS\u91c7\u7528\u57fa\u4e8e\u5feb\u901fGUI\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5229\u7528\u7528\u6237\u70b9\u63d0\u793a\u751f\u62102D SAM\u63a9\u7801\uff0c\u5e76\u901a\u8fc7NeRF\u6df1\u5ea6\u5148\u9a8c\u8fdb\u884c\u7cbe\u70bc\u4ee5\u63d0\u9ad8\u51e0\u4f55\u7cbe\u5ea6\u548c\u524d\u666f-\u80cc\u666f\u5206\u79bb\uff1b\u6838\u5fc3\u8d21\u732e\u662f\u81ea\u5b9a\u4e49CUDA\u5185\u6838\uff0c\u53ef\u5728200\u6beb\u79d2\u5185\u5c06\u7cbe\u70bc\u7684\u591a\u89c6\u89d2\u63a9\u7801\u805a\u5408\u5230\u7edf\u4e00\u76843D\u4f53\u7d20\u7f51\u683c\u4e2d\u3002", "result": "\u5728Mip-NeRF 360\u00b0\u548cLLFF\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDivAS\u5b9e\u73b0\u4e86\u4e0e\u4f18\u5316\u65b9\u6cd5\u76f8\u5f53\u7684\u5206\u5272\u8d28\u91cf\uff0c\u7aef\u5230\u7aef\u901f\u5ea6\u63d0\u9ad82-2.5\u500d\uff0c\u6392\u9664\u7528\u6237\u63d0\u793a\u65f6\u95f4\u540e\u901f\u5ea6\u63d0\u5347\u53ef\u8fbe\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u65e0\u9700\u4f18\u5316\u7684\u5b9e\u65f6NeRF\u5206\u5272\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6df1\u5ea6\u4ea4\u4e92\u7684\u4f53\u7d20\u805a\u5408\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e862D\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\u548c3D\u51e0\u4f55\u5148\u9a8c\uff0c\u4e3a\u4ea4\u4e92\u5f0f3D\u573a\u666f\u7406\u89e3\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.04946", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04946", "abs": "https://arxiv.org/abs/2601.04946", "authors": ["Subhadeep Roy", "Gagan Bhatia", "Steffen Eger"], "title": "Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics", "comment": "First version", "summary": "Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \\emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \\textsc{\\textbf{ProtoBias}} (\\textit{\\textbf{Proto}typical \\textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \\textbf{\\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u8bc4\u4f30\u4e2d\u5b58\u5728\u7684\u539f\u578b\u6027\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5ProtoBias\u6765\u91cf\u5316\u8fd9\u4e00\u504f\u5dee\uff0c\u540c\u65f6\u5f00\u53d1\u4e86ProtoScore\u6307\u6807\u6765\u663e\u8457\u51cf\u5c11\u8bc4\u4f30\u5931\u8d25\u7387\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u901a\u5e38\u66ff\u4ee3\u4eba\u7c7b\u5224\u65ad\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u548c\u5927\u89c4\u6a21\u7b5b\u9009\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u662f\u5426\u771f\u6b63\u4f18\u5148\u8003\u8651\u8bed\u4e49\u6b63\u786e\u6027\uff0c\u8fd8\u662f\u503e\u5411\u4e8e\u4ece\u6709\u504f\u6570\u636e\u5206\u5e03\u4e2d\u5b66\u4e60\u5230\u7684\u89c6\u89c9\u548c\u793e\u4f1a\u539f\u578b\u56fe\u50cf\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u548c\u7814\u7a76\u591a\u6a21\u6001\u8bc4\u4f30\u4e2d\u7684\u539f\u578b\u6027\u504f\u5dee\u8fd9\u4e00\u7cfb\u7edf\u6027\u5931\u6548\u6a21\u5f0f\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u53d7\u63a7\u5bf9\u6bd4\u57fa\u51c6ProtoBias\uff0c\u6db5\u76d6\u52a8\u7269\u3001\u7269\u4f53\u548c\u4eba\u53e3\u7edf\u8ba1\u56fe\u50cf\uff0c\u5176\u4e2d\u8bed\u4e49\u6b63\u786e\u4f46\u975e\u539f\u578b\u7684\u56fe\u50cf\u4e0e\u8bed\u4e49\u9519\u8bef\u4f46\u539f\u578b\u7684\u5bf9\u6297\u5bf9\u5e94\u56fe\u50cf\u914d\u5bf9\u3002\u8fd9\u79cd\u8bbe\u7f6e\u80fd\u591f\u5b9a\u5411\u8bc4\u4f30\u6307\u6807\u662f\u9075\u5faa\u6587\u672c\u8bed\u4e49\u8fd8\u662f\u9ed8\u8ba4\u539f\u578b\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u7814\u7a76\u63d0\u51fa\u4e86ProtoScore\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u670970\u4ebf\u53c2\u6570\u7684\u9c81\u68d2\u6027\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5e7f\u6cdb\u4f7f\u7528\u7684\u6307\u6807\u5305\u62ecCLIPScore\u3001PickScore\u548c\u57fa\u4e8eVQA\u7684\u5206\u6570\u7ecf\u5e38\u9519\u8bef\u6392\u5e8f\u8fd9\u4e9b\u914d\u5bf9\uff0c\u800c\u5373\u4f7f\u662fLLM-as-Judge\u7cfb\u7edf\u5728\u793e\u4f1a\u57fa\u7840\u6848\u4f8b\u4e2d\u4e5f\u8868\u73b0\u51fa\u4e0d\u5747\u5300\u7684\u9c81\u68d2\u6027\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4eba\u7c7b\u8bc4\u4f30\u59cb\u7ec8\u66f4\u503e\u5411\u4e8e\u8bed\u4e49\u6b63\u786e\u6027\u4e14\u5177\u6709\u66f4\u5927\u7684\u51b3\u7b56\u8fb9\u754c\u3002ProtoScore\u6307\u6807\u663e\u8457\u964d\u4f4e\u4e86\u5931\u8d25\u7387\u5e76\u6291\u5236\u4e86\u9519\u8bef\u6392\u5e8f\uff0c\u540c\u65f6\u8fd0\u884c\u901f\u5ea6\u6bd4GPT-5\u7684\u63a8\u7406\u65f6\u95f4\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u63a5\u8fd1\u66f4\u5927\u89c4\u6a21\u95ed\u6e90\u8bc4\u4f30\u5668\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u8bc4\u4f30\u4e2d\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u539f\u578b\u6027\u504f\u5dee\u95ee\u9898\uff0c\u8868\u660e\u5f53\u524d\u81ea\u52a8\u6307\u6807\u53ef\u80fd\u8fc7\u5ea6\u4f9d\u8d56\u6709\u504f\u6570\u636e\u5206\u5e03\u4e2d\u7684\u539f\u578b\u6a21\u5f0f\u3002\u63d0\u51fa\u7684ProtoBias\u57fa\u51c6\u4e3a\u8bc4\u4f30\u6307\u6807\u504f\u5dee\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6d4b\u8bd5\u6846\u67b6\uff0c\u800cProtoScore\u6307\u6807\u5c55\u793a\u4e86\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8bc4\u4f30\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u8bc4\u4f30\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2601.05192", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05192", "abs": "https://arxiv.org/abs/2601.05192", "authors": ["Samy Haffoudhi", "Fabian M. Suchanek", "Nils Holzenberger"], "title": "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation", "comment": null, "summary": "Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LELA\uff0c\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u6a21\u5757\u5316\u7531\u7c97\u5230\u7cbe\u5b9e\u4f53\u94fe\u63a5\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\uff0c\u5728\u4e0d\u540c\u9886\u57df\u3001\u77e5\u8bc6\u5e93\u548cLLM\u4e0a\u5747\u80fd\u5de5\u4f5c\uff0c\u5e76\u5728\u591a\u79cd\u5b9e\u4f53\u94fe\u63a5\u8bbe\u7f6e\u4e2d\u5c55\u73b0\u51fa\u4e0e\u5fae\u8c03\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u5b9e\u4f53\u94fe\u63a5\u662f\u5c06\u6587\u672c\u4e2d\u7684\u6a21\u7cca\u63d0\u53ca\u6620\u5c04\u5230\u77e5\u8bc6\u5e93\u5b9e\u4f53\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u5bf9\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u3001\u95ee\u7b54\u548c\u4fe1\u606f\u63d0\u53d6\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u6216\u77e5\u8bc6\u5e93\u8fdb\u884c\u5fae\u8c03\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u4e14\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u3001\u80fd\u8de8\u4e0d\u540c\u9886\u57df\u548c\u77e5\u8bc6\u5e93\u5de5\u4f5c\u7684\u7075\u6d3b\u5b9e\u4f53\u94fe\u63a5\u65b9\u6cd5\u3002", "method": "LELA\u91c7\u7528\u6a21\u5757\u5316\u7531\u7c97\u5230\u7cbe\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u8fdb\u884c\u5b9e\u4f53\u94fe\u63a5\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4efb\u4f55\u5fae\u8c03\u9636\u6bb5\uff0c\u80fd\u591f\u4e0e\u4e0d\u540c\u7684\u76ee\u6807\u9886\u57df\u3001\u77e5\u8bc6\u5e93\u548c\u5927\u8bed\u8a00\u6a21\u578b\u534f\u540c\u5de5\u4f5c\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u5904\u7406\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u5b9e\u4f53\u6d88\u6b67\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLELA\u5728\u5404\u79cd\u5b9e\u4f53\u94fe\u63a5\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u7ecf\u8fc7\u5fae\u8c03\u7684\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u9ad8\u5ea6\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u663e\u8457\u4f18\u4e8e\u6240\u6709\u672a\u7ecf\u5fae\u8c03\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u9886\u57df\u548c\u77e5\u8bc6\u5e93\u4e0a\u5747\u5c55\u73b0\u51fa\u7a33\u5b9a\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "LELA\u8bc1\u660e\u4e86\u65e0\u9700\u5fae\u8c03\u7684\u6a21\u5757\u5316\u5b9e\u4f53\u94fe\u63a5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8de8\u9886\u57df\u5b9e\u4f53\u94fe\u63a5\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u5b9e\u4f53\u94fe\u63a5\u7cfb\u7edf\u7684\u90e8\u7f72\u548c\u7ef4\u62a4\u6210\u672c\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2601.04891", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04891", "abs": "https://arxiv.org/abs/2601.04891", "authors": ["Suyash Mishra", "Qiang Li", "Srikanth Patil", "Satyanarayan Pati", "Baddu Narendra"], "title": "Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform", "comment": "Submitted to the Industry Track of Top Tier Conference; currently under peer review", "summary": "Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new \"A+B\" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u5de5\u4e1a\u573a\u666f\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u9488\u5bf9\u5236\u836f\u9886\u57df\u7684\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u5728\u4e25\u683c\u7684\u8ba1\u7b97\u8d44\u6e90\u7ea6\u675f\u4e0b\u8bc4\u4f30\u4e8640\u591a\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5c40\u9650\u6027\u3001\u6548\u7387\u6743\u8861\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5927\u591a\u6570\u8bc4\u4f30\u96c6\u4e2d\u4e8e\u77ed\u89c6\u9891\u4e14\u5047\u8bbe\u8ba1\u7b97\u8d44\u6e90\u4e0d\u53d7\u9650\u5236\uff0c\u800c\u5de5\u4e1a\u573a\u666f\u5982\u5236\u836f\u5185\u5bb9\u7406\u89e3\u9700\u8981\u5904\u7406\u957f\u89c6\u9891\u5e76\u9762\u4e34\u4e25\u683c\u7684GPU\u3001\u5ef6\u8fdf\u548c\u6210\u672c\u7ea6\u675f\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5b9e\u9645\u90e8\u7f72\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u6781\u9650\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5de5\u4e1a\u7ea7\u751f\u6210\u5f0fAI\u6846\u67b6\uff0c\u5904\u7406\u4e86\u8d85\u8fc720\u4e07\u4efdPDF\u300125,326\u4e2a\u6db5\u76d6\u516b\u79cd\u683c\u5f0f\u7684\u89c6\u9891\u4ee5\u53ca888\u4e2a\u591a\u8bed\u8a00\u97f3\u9891\u6587\u4ef6\uff0c\u6784\u5efa\u4e86\u5236\u836f\u9886\u57df\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u7406\u67b6\u6784\uff0c\u5e76\u5728Video-MME\u548cMMBench\u4e24\u4e2a\u9886\u5148\u57fa\u51c6\u4ee5\u53ca\u5305\u542b14\u79cd\u75be\u75c5\u9886\u57df\u7684\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u5bf940\u591a\u4e2aVLMs\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5546\u7528GPU\u4e0a\u4f7f\u7528SDPA\u6ce8\u610f\u529b\u673a\u5236\u53ef\u83b7\u5f973-8\u500d\u7684\u6548\u7387\u63d0\u5347\uff0c\u591a\u6a21\u6001\u6027\u57288/12\u4e2a\u4efb\u52a1\u9886\u57df\uff08\u7279\u522b\u662f\u957f\u5ea6\u76f8\u5173\u4efb\u52a1\uff09\u4e2d\u5e26\u6765\u6539\u8fdb\uff0c\u540c\u65f6\u5728\u5f00\u6e90\u548c\u95ed\u6e90VLMs\u4e2d\u90fd\u53d1\u73b0\u4e86\u65f6\u95f4\u5bf9\u9f50\u548c\u5173\u952e\u5e27\u68c0\u6d4b\u7684\u660e\u663e\u74f6\u9888\uff0c\u63ed\u793a\u4e86\u957f\u89c6\u9891\u63a8\u7406\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u6743\u8861\u3001\u65f6\u95f4\u63a8\u7406\u9650\u5236\u548c\u89c6\u9891\u5206\u5272\u6311\u6218\u7b49\u5173\u952e\u53d1\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u6ca1\u6709\u63d0\u51fa\u65b0\u7684\"A+B\"\u6a21\u578b\uff0c\u800c\u662f\u7cfb\u7edf\u523b\u753b\u4e86\u5f53\u524dVLMs\u5728\u5b9e\u9645\u90e8\u7f72\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\u6781\u9650\u3001\u6743\u8861\u53d6\u820d\u548c\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u5de5\u4e1a\u7ea7\u957f\u89c6\u9891\u7406\u89e3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\uff0c\u7279\u522b\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u6027\u3001\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u548c\u65f6\u95f4\u5bf9\u9f50\u5904\u7406\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.05125", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05125", "abs": "https://arxiv.org/abs/2601.05125", "authors": ["Ignacio de Rodrigo", "Alvaro J. Lopez-Lopez", "Jaime Boal"], "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding", "comment": null, "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VERSE\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u548c\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u63a2\u7d22\u89c6\u89c9\u5d4c\u5165\u7a7a\u95f4\u6765\u8bc6\u522b\u95ee\u9898\u533a\u57df\u5e76\u751f\u6210\u5408\u6210\u6570\u636e\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u89c6\u89c9\u5d4c\u5165\u7a7a\u95f4\u7684\u7cfb\u7edf\u5206\u6790\u65b9\u6cd5\uff0c\u96be\u4ee5\u8bc6\u522b\u5bfc\u81f4\u9519\u8bef\u7684\u89c6\u89c9\u7279\u5f81\u5e76\u9488\u5bf9\u6027\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "method": "VERSE\u65b9\u6cd5\u901a\u8fc7\u53ef\u89c6\u5316\u6f5c\u5728\u8868\u793a\u6765\u8bc4\u4f30\u6a21\u578b\u53ef\u884c\u6027\uff0c\u8bc6\u522b\u95ee\u9898\u533a\u57df\u5e76\u6307\u5bfc\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u4f7f\u7528MERIT\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u5e76\u5728MERIT Secret\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f18\u5316\u4e86Donut\u548cIdefics2\u7b49\u672c\u5730\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eVERSE\u80fd\u6709\u6548\u63ed\u793a\u4e0e\u9519\u8bef\u805a\u7c7b\u76f8\u5173\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u4f7f\u7528\u5305\u542b\u8fd9\u4e9b\u7279\u5f81\u7684\u6837\u672c\u91cd\u65b0\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86F1\u6027\u80fd\u4e14\u672a\u635f\u5bb3\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u5316\u540e\u7684\u672c\u5730\u6a21\u578b\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86GPT-4\u548cPixtral\u7b49SaaS\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "VERSE\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u5206\u6790\u548c\u6539\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u6570\u636e\u589e\u5f3a\u7b56\u7565\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u672c\u5730\u6a21\u578b\u7ecf\u8fc7\u9002\u5f53\u4f18\u5316\u540e\u80fd\u591f\u4e0e\u5546\u4e1aSaaS\u89e3\u51b3\u65b9\u6848\u7ade\u4e89\uff0c\u4e3a\u6587\u6863\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6a21\u578b\u8bca\u65ad\u548c\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2601.05059", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05059", "abs": "https://arxiv.org/abs/2601.05059", "authors": ["Suyash Mishra", "Qiang Li", "Srikanth Patil", "Anubhav Girdhar"], "title": "From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)", "comment": "Contributed original research to top tier conference in VLM; currently undergoing peer review", "summary": "Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).\n  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5236\u836f\u884c\u4e1a\u7684\u9886\u57df\u81ea\u9002\u5e94\u89c6\u9891\u7247\u6bb5\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4e2a\u6027\u5316\u7684\u591a\u6a21\u6001\u5185\u5bb9\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u6458\u8981\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5236\u836f\u884c\u4e1a\u5728\u5904\u7406\u5f02\u6784\u591a\u6a21\u6001\u6570\u636e\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u3001\u97f3\u9891\u548c\u7f51\u9875\u94fe\u63a5\uff09\u65f6\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u3001\u8d28\u91cf\u4e0b\u964d\u548c\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u957f\u89c6\u9891\u548c\u97f3\u9891\u6570\u636e\uff08\u5982\u4e34\u5e8a\u8bd5\u9a8c\u8bbf\u8c08\u548c\u6559\u80b2\u7814\u8ba8\u4f1a\uff09\u7684\u5904\u7406\u6311\u6218\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u4e2a\u9886\u57df\u81ea\u9002\u5e94\u7684\u89c6\u9891\u5230\u89c6\u9891\u7247\u6bb5\u751f\u6210\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u8d21\u732e\uff1a\u53ef\u590d\u73b0\u7684Cut & Merge\u7b97\u6cd5\uff08\u652f\u6301\u6de1\u5165\u6de1\u51fa\u548c\u65f6\u95f4\u6233\u5f52\u4e00\u5316\u4ee5\u786e\u4fdd\u5e73\u6ed1\u8fc7\u6e21\u548c\u97f3\u89c6\u9891\u5bf9\u9f50\uff09\uff1b\u57fa\u4e8e\u89d2\u8272\u5b9a\u4e49\u548c\u63d0\u793a\u6ce8\u5165\u7684\u4e2a\u6027\u5316\u673a\u5236\uff08\u9488\u5bf9\u8425\u9500\u3001\u57f9\u8bad\u3001\u76d1\u7ba1\u7b49\u4e0d\u540c\u9700\u6c42\uff09\uff1b\u4ee5\u53ca\u5e73\u8861ALM/VLM\u589e\u5f3a\u5904\u7406\u7684\u6210\u672c\u9ad8\u6548\u7aef\u5230\u7aef\u6d41\u6c34\u7ebf\u7b56\u7565\u3002", "result": "\u5728Video MME\u57fa\u51c6\u6d4b\u8bd5\uff08900\u4e2a\u89c6\u9891\uff09\u548c\u5305\u542b16,159\u4e2a\u5236\u836f\u89c6\u9891\u7684\u4e13\u6709\u6570\u636e\u96c6\uff08\u6db5\u76d614\u4e2a\u75be\u75c5\u9886\u57df\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e863-4\u500d\u7684\u901f\u5ea6\u63d0\u5347\u548c4\u500d\u7684\u6210\u672c\u964d\u4f4e\uff0c\u540c\u65f6\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7247\u6bb5\u8d28\u91cf\u3002\u4e0eGemini 2.5 Pro\u7b49\u6700\u5148\u8fdb\u7684VLM\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u7247\u6bb5\u8fde\u8d2f\u6027\u5f97\u5206\uff080.348\uff09\u548c\u4fe1\u606f\u6027\u5f97\u5206\uff080.721\uff09\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u900f\u660e\u3001\u53ef\u5b9a\u5236\u4e14\u652f\u6301\u5408\u89c4\u6027\u7684\u89c6\u9891\u6458\u8981\u65b9\u6cd5\u5728\u751f\u547d\u79d1\u5b66\u9886\u57df\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u548c\u6210\u672c\u8282\u7ea6\uff0c\u8fd8\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u548c\u4e2a\u6027\u5316\u673a\u5236\u63d0\u9ad8\u4e86\u8f93\u51fa\u8d28\u91cf\uff0c\u4e3a\u5236\u836f\u884c\u4e1a\u7684\u6570\u5b57\u5316\u8f6c\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.05159", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05159", "abs": "https://arxiv.org/abs/2601.05159", "authors": ["Shuliang Liu", "Songbo Yang", "Dong Fang", "Sihang Jia", "Yuqi Tang", "Lingfeng Su", "Ruoshui Peng", "Yibo Yan", "Xin Zou", "Xuming Hu"], "title": "Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering", "comment": null, "summary": "Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVision-Language Introspection (VLI)\u7684\u8bad\u7ec3\u514d\u8d39\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u5143\u8ba4\u77e5\u81ea\u6211\u7ea0\u6b63\u8fc7\u7a0b\u6765\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c5e\u6027\u5185\u7701\u8bca\u65ad\u5e7b\u89c9\u98ce\u9669\uff0c\u5e76\u91c7\u7528\u53ef\u89e3\u91ca\u7684\u53cc\u56e0\u679c\u5f15\u5bfc\u52a8\u6001\u8c03\u6574\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u53ef\u9760\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u4e25\u91cd\u635f\u5bb3\u4e86\u5176\u53ef\u9760\u6027\uff0c\u8fd9\u4e3b\u8981\u6e90\u4e8e\u6a21\u578b\u5728\u8ba4\u77e5\u5185\u7701\u65b9\u9762\u7684\u6839\u672c\u6027\u5931\u8d25\uff0c\u5373\u6a21\u578b\u76f2\u76ee\u4fe1\u4efb\u8bed\u8a00\u5148\u9a8c\u800c\u975e\u5177\u4f53\u89c6\u89c9\u8bc1\u636e\u3002\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u5b58\u5728\u660e\u663e\u5c40\u9650\uff1a\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u4ec5\u8868\u9762\u64cd\u4f5c\u800c\u672a\u7ea0\u6b63\u5185\u90e8\u8bed\u4e49\u9519\u4f4d\uff0c\u800c\u5f53\u524d\u6f5c\u5728\u5f15\u5bfc\u65b9\u6cd5\u4f9d\u8d56\u7f3a\u4e4f\u5b9e\u4f8b\u7279\u5b9a\u7cbe\u5ea6\u7684\u9759\u6001\u5411\u91cf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Vision-Language Introspection (VLI)\u8bad\u7ec3\u514d\u8d39\u63a8\u7406\u6846\u67b6\uff0c\u6a21\u62df\u5143\u8ba4\u77e5\u81ea\u6211\u7ea0\u6b63\u8fc7\u7a0b\u3002VLI\u9996\u5148\u6267\u884c\u5c5e\u6027\u5185\u7701\uff0c\u901a\u8fc7\u6982\u7387\u51b2\u7a81\u68c0\u6d4b\u8bca\u65ad\u5e7b\u89c9\u98ce\u9669\u5e76\u5b9a\u4f4d\u56e0\u679c\u89c6\u89c9\u951a\u70b9\uff1b\u7136\u540e\u91c7\u7528\u53ef\u89e3\u91ca\u7684\u53cc\u56e0\u679c\u5f15\u5bfc\uff0c\u4e3b\u52a8\u8c03\u5236\u63a8\u7406\u8fc7\u7a0b\uff0c\u52a8\u6001\u9694\u79bb\u89c6\u89c9\u8bc1\u636e\u4e0e\u80cc\u666f\u566a\u58f0\uff0c\u540c\u65f6\u901a\u8fc7\u81ea\u9002\u5e94\u6821\u51c6\u6d88\u9664\u76f2\u76ee\u7f6e\u4fe1\u3002", "result": "VLI\u5728\u5148\u8fdb\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728MMHal-Bench\u4e0a\u5c06\u7269\u4f53\u5e7b\u89c9\u7387\u964d\u4f4e\u4e8612.67%\uff0c\u5728POPE\u4e0a\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u4e865.8%\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7269\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u8bad\u7ec3\u514d\u8d39\u63a8\u7406\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u6a21\u62df\u5143\u8ba4\u77e5\u8fc7\u7a0b\u89e3\u51b3\u591a\u6a21\u6001\u5e7b\u89c9\u95ee\u9898\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8bad\u7ec3\u514d\u8d39\u5e72\u9884\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002VLI\u6846\u67b6\u901a\u8fc7\u5185\u7701\u8bca\u65ad\u548c\u52a8\u6001\u5f15\u5bfc\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u7684\u7cbe\u7ec6\u8c03\u63a7\uff0c\u4e3a\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u5f00\u8f9f\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2601.05105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.05105", "abs": "https://arxiv.org/abs/2601.05105", "authors": ["Filippo Ghilotti", "Samuel Brucker", "Nahku Saidy", "Matteo Matteucci", "Mario Bijelic", "Felix Heide"], "title": "UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition", "comment": null, "summary": "Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u591a\u6a21\u6001\u4f2a\u6807\u7b7e\u65b9\u6cd5\uff0c\u5229\u7528\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u7684\u65f6\u95f4\u51e0\u4f55\u4e00\u81f4\u6027\u5c06\u6587\u672c\u548c2D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7ebf\u7d22\u76f4\u63a5\u63d0\u5347\u52303D\u7a7a\u95f4\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5b9e\u73b0\u4e863D\u8bed\u4e49\u6807\u7b7e\u3001\u8fb9\u754c\u6846\u548c\u5bc6\u96c6\u70b9\u4e91\u7684\u81ea\u52a8\u751f\u6210\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u672a\u6807\u6ce8\u7684\u6fc0\u5149\u96f7\u8fbe\u65e5\u5fd7\u867d\u7136\u5305\u542b\u4e30\u5bcc\u76843D\u51e0\u4f55\u4fe1\u606f\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u4eba\u5de5\u6807\u6ce8\u800c\u96be\u4ee5\u5229\u7528\uff0c\u5f62\u6210\u4e86\u611f\u77e5\u7814\u7a76\u7684\u4e3b\u8981\u6210\u672c\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u4eba\u5de5\u76d1\u7763\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u5229\u7528\u6548\u7387\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u4ece\u65f6\u95f4\u7d2f\u79ef\u7684\u6fc0\u5149\u96f7\u8fbe\u5730\u56fe\u4e2d\u5b66\u4e60\u7684\u5f3a\u51e0\u4f55\u5148\u9a8c\uff0c\u901a\u8fc7\u65f6\u95f4\u51e0\u4f55\u4e00\u81f4\u6027\u8de8\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u63d0\u5347\u548c\u878d\u5408\u6587\u672c\u4e0e2D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7ebf\u7d22\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8fed\u4ee3\u66f4\u65b0\u89c4\u5219\uff0c\u5f3a\u5236\u5b9e\u65bd\u8054\u5408\u51e0\u4f55-\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u79fb\u52a8\u7269\u4f53\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u751f\u62103D\u8bed\u4e49\u6807\u7b7e\u30013D\u8fb9\u754c\u6846\u548c\u5bc6\u96c6\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u8bed\u4e49\u5206\u5272\u548c\u7269\u4f53\u68c0\u6d4b\u4f2a\u6807\u7b7e\u65b9\u6cd5\u3002\u5373\u4f7f\u4f7f\u7528\u4e00\u5c0f\u90e8\u5206\u51e0\u4f55\u4e00\u81f4\u7684\u7a20\u5bc6\u5316\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\uff0c\u4e5f\u80fd\u572880-150\u7c73\u548c150-250\u7c73\u8303\u56f4\u5185\u5206\u522b\u5c06\u6df1\u5ea6\u9884\u6d4bMAE\u63d0\u534751.5%\u548c22.0%\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5229\u7528\u65f6\u95f4\u51e0\u4f55\u4e00\u81f4\u6027\u5b9e\u73b0\u65e0\u76d1\u77633D\u611f\u77e5\u4f2a\u6807\u7b7e\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u964d\u4f4e\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7814\u7a76\u7684\u6807\u6ce8\u6210\u672c\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002\u51e0\u4f55\u4e00\u81f4\u7684\u7a20\u5bc6\u5316\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u9884\u6d4b\u6027\u80fd\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u63a8\u52a8\u5927\u89c4\u6a21\u65e0\u76d1\u77633D\u611f\u77e5\u5b66\u4e60\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2601.05201", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05201", "abs": "https://arxiv.org/abs/2601.05201", "authors": ["William Rudman", "Michal Golovanevsky", "Dana Arad", "Yonatan Belinkov", "Ritambhara Singh", "Carsten Eickhoff", "Kyle Mahowald"], "title": "Mechanisms of Prompt-Induced Hallucination in Vision-Language Models", "comment": null, "summary": "Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u673a\u5236\u5206\u6790\u63ed\u793a\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u793a\u8bf1\u5bfc\u5e7b\u89c9\u7684\u6545\u969c\u6a21\u5f0f\uff0c\u53d1\u73b0\u901a\u8fc7\u6d88\u878d\u5c11\u91cf\u6ce8\u610f\u529b\u5934\u53ef\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u884c\u4e3a\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u964d\u4f4e\u81f3\u5c1140%\u7684\u5e7b\u89c9\u7387\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\uff0c\u503e\u5411\u4e8e\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u800c\u5ffd\u7565\u89c6\u89c9\u8bc1\u636e\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8fd9\u79cd\u63d0\u793a\u8bf1\u5bfc\u5e7b\u89c9\u7684\u6545\u969c\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u5728\u53d7\u63a7\u7684\u5bf9\u8c61\u8ba1\u6570\u573a\u666f\u4e2d\uff0c\u5f53\u63d0\u793a\u9ad8\u4f30\u56fe\u50cf\u4e2d\u5bf9\u8c61\u6570\u91cf\u65f6\u6a21\u578b\u7684\u884c\u4e3a\u53d8\u5316\u3002", "method": "\u7814\u7a76\u91c7\u7528\u53d7\u63a7\u7684\u5bf9\u8c61\u8ba1\u6570\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6587\u672c\u63d0\u793a\u4e0e\u89c6\u89c9\u8bc1\u636e\u7684\u5dee\u5f02\u6765\u8bc4\u4f30\u6a21\u578b\u884c\u4e3a\u3002\u5bf9\u4e09\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u673a\u5236\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u5bfc\u81f4\u63d0\u793a\u8bf1\u5bfc\u5e7b\u89c9\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u8fd9\u4e9b\u6ce8\u610f\u529b\u5934\u6765\u9a8c\u8bc1\u5176\u4f5c\u7528\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5728\u4f4e\u5bf9\u8c61\u6570\u91cf\u65f6\u6a21\u578b\u80fd\u591f\u7ea0\u6b63\u9ad8\u4f30\uff0c\u4f46\u968f\u7740\u5bf9\u8c61\u6570\u91cf\u589e\u52a0\uff0c\u6a21\u578b\u8d8a\u6765\u8d8a\u503e\u5411\u4e8e\u9075\u5faa\u63d0\u793a\u800c\u5ffd\u7565\u89c6\u89c9\u5dee\u5f02\u3002\u6d88\u878d\u8bc6\u522b\u51fa\u7684\u5c11\u91cf\u6ce8\u610f\u529b\u5934\u53ef\u5c06\u63d0\u793a\u8bf1\u5bfc\u5e7b\u89c9\u51cf\u5c11\u81f3\u5c1140%\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002\u4e0d\u540c\u6a21\u578b\u4e2d\uff0c\u8fd9\u4e9b\u5e7b\u89c9\u5934\u4ee5\u6a21\u578b\u7279\u5b9a\u7684\u65b9\u5f0f\u4ecb\u5bfc\u63d0\u793a\u590d\u5236\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u793a\u8bf1\u5bfc\u5e7b\u89c9\u7684\u5185\u90e8\u673a\u5236\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u5b9e\u73b0\u8fd9\u4e9b\u884c\u4e3a\u65f6\u7684\u7279\u5b9a\u5dee\u5f02\u3002\u6d88\u878d\u5e7b\u89c9\u6ce8\u610f\u529b\u5934\u80fd\u591f\u589e\u5f3a\u6a21\u578b\u5bf9\u89c6\u89c9\u8bc1\u636e\u7684\u6821\u6b63\u80fd\u529b\uff0c\u4e3a\u7406\u89e3\u548c\u7f13\u89e3\u6a21\u578b\u5e7b\u89c9\u63d0\u4f9b\u4e86\u673a\u5236\u5c42\u9762\u7684\u89c1\u89e3\u3002"}}
{"id": "2601.05124", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.05124", "abs": "https://arxiv.org/abs/2601.05124", "authors": ["Runze He", "Yiji Cheng", "Tiankai Hang", "Zhimin Li", "Yu Xu", "Zijin Yin", "Shiyi Zhang", "Wenxun Dai", "Penghui Du", "Ao Ma", "Chunyu Wang", "Qinglin Lu", "Jizhong Han", "Jiao Dai"], "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing", "comment": "13 pages, 9 figures, project page: https://github.com/hrz2000/realign", "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRe-Align\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u5f15\u5bfc\u7684\u5bf9\u9f50\u673a\u5236\u5f25\u5408\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7406\u89e3\u4e0e\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0a\u4e0b\u6587\u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u7406\u89e3\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8fd9\u4e9b\u4f18\u52bf\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u8fc1\u79fb\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\u4efb\u52a1\u4e2d\u7528\u6237\u610f\u56fe\u7684\u7406\u89e3\u4e0e\u5fe0\u5b9e\u6267\u884c\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "Re-Align\u6846\u67b6\u7684\u6838\u5fc3\u662f\u4e0a\u4e0b\u6587\u601d\u7ef4\u94fe\u7ed3\u6784\u5316\u63a8\u7406\u8303\u5f0f\uff0c\u5b83\u5c06\u8bed\u4e49\u5f15\u5bfc\u4e0e\u53c2\u8003\u5173\u8054\u89e3\u8026\uff0c\u63d0\u4f9b\u6e05\u6670\u7684\u6587\u672c\u76ee\u6807\u5e76\u51cf\u5c11\u53c2\u8003\u56fe\u50cf\u95f4\u7684\u6df7\u6dc6\uff1b\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u4ee3\u7406\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6848\uff0c\u901a\u8fc7\u8861\u91cf\u7ed3\u6784\u5316\u63a8\u7406\u6587\u672c\u4e0e\u751f\u6210\u56fe\u50cf\u4e4b\u95f4\u7684\u5bf9\u9f50\u5ea6\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cRe-Align\u5728\u6a21\u578b\u89c4\u6a21\u548c\u8ba1\u7b97\u8d44\u6e90\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u4e0a\u4e0b\u6587\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u5f25\u5408\u7406\u89e3\u4e0e\u751f\u6210\u5dee\u8ddd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u5f15\u5bfc\u7684\u5bf9\u9f50\u673a\u5236\u5728\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u63d0\u5347\u4e0a\u4e0b\u6587\u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u4f18\u5316\u751f\u6210\u5bf9\u9f50\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.05241", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.05241", "abs": "https://arxiv.org/abs/2601.05241", "authors": ["Boyang Wang", "Haoran Zhang", "Shujie Zhang", "Jinkun Hao", "Mingda Jia", "Qi Lv", "Yucheng Mao", "Zhaoyang Lyu", "Jia Zeng", "Xudong Xu", "Jiangmiao Pang"], "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation", "comment": null, "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u8eab\u4efd\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u4f9b\u793a\u4f8b\u56fe\u50cf\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u6765\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u751f\u6210\u6240\u9700\u7684\u573a\u666f\u8bbe\u7f6e\uff0c\u4ece\u800c\u589e\u5f3a\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u63d0\u793a\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u7531\u4e8e\u786c\u4ef6\u548c\u7269\u7406\u8bbe\u7f6e\u7684\u9650\u5236\uff0c\u6536\u96c6\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u6570\u636e\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u96be\u4ee5\u6269\u5c55\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e86\u6700\u5148\u8fdb\u7b56\u7565\u6a21\u578b\u6240\u9700\u7684\u591a\u89c6\u89d2\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u89c2\u6d4b\u9700\u6c42\uff0c\u4e14\u4ec5\u9760\u6587\u672c\u63d0\u793a\u65e0\u6cd5\u53ef\u9760\u5730\u6307\u5b9a\u573a\u666f\u8bbe\u7f6e\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u89c6\u89c9\u8eab\u4efd\u63d0\u793a\u65b9\u6cd5\uff0c\u5c06\u793a\u4f8b\u56fe\u50cf\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u63d0\u4f9b\u7ed9\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u5f15\u5bfc\u751f\u6210\u6240\u9700\u7684\u573a\u666f\u8bbe\u7f6e\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6d41\u7a0b\uff0c\u4ece\u5927\u578b\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e2d\u7b56\u5212\u89c6\u89c9\u8eab\u4efd\u6c60\uff0c\u7528\u4e8e\u589e\u5f3a\u64cd\u4f5c\u6570\u636e\u3002", "result": "\u4f7f\u7528\u589e\u5f3a\u7684\u64cd\u4f5c\u6570\u636e\u8bad\u7ec3\u4e0b\u6e38\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u548c\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u6a21\u578b\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5747\u83b7\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u89c6\u89c9\u8eab\u4efd\u63d0\u793a\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u573a\u666f\u63a7\u5236\u673a\u5236\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e2d\u7684\u89c6\u89c9\u8eab\u4efd\u6c60\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u7684\u89c2\u6d4b\u6570\u636e\uff0c\u4ece\u800c\u63d0\u5347\u7b56\u7565\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.05143", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05143", "abs": "https://arxiv.org/abs/2601.05143", "authors": ["Md. Zahid Hossain", "Most. Sharmin Sultana Samu", "Md. Rakibul Islam", "Md. Siam Ansary"], "title": "A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering", "comment": "Preprint, manuscript is under review", "summary": "Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u53f6\u7247\u56fe\u50cf\u4e2d\u8fdb\u884c\u4f5c\u7269\u548c\u75c5\u5bb3\u8bc6\u522b\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86Swin Transformer\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5e8f\u5217\u5230\u5e8f\u5217\u8bed\u8a00\u89e3\u7801\u5668\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u4f5c\u7269\u75c5\u5bb3\u89c6\u89c9\u95ee\u7b54\u9700\u8981\u51c6\u786e\u7684\u89c6\u89c9\u7406\u89e3\u548c\u53ef\u9760\u7684\u8bed\u8a00\u751f\u6210\u80fd\u529b\uff0c\u73b0\u6709\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u91cf\u8fc7\u5927\uff0c\u7f3a\u4e4f\u9488\u5bf9\u519c\u4e1a\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u7684\u8f7b\u91cf\u7ea7\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528Swin Transformer\u4f5c\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u5e8f\u5217\u5230\u5e8f\u5217\u8bed\u8a00\u89e3\u7801\u5668\u6784\u5efa\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5e76\u4f7f\u7528Grad-CAM\u548c\u8bcd\u5143\u7ea7\u5f52\u56e0\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "\u5728\u5927\u89c4\u6a21\u4f5c\u7269\u75c5\u5bb3\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4f5c\u7269\u548c\u75c5\u5bb3\u8bc6\u522b\u65b9\u9762\u5747\u8fbe\u5230\u9ad8\u51c6\u786e\u7387\uff0c\u5728BLEU\u3001ROUGE\u548cBERTScore\u7b49\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u663e\u8457\u5c11\u4e8e\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u4efb\u52a1\u7279\u5b9a\u7684\u89c6\u89c9\u9884\u8bad\u7ec3\u5bf9\u4f5c\u7269\u75c5\u5bb3\u89c6\u89c9\u95ee\u7b54\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u8f7b\u91cf\u7ea7\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53c2\u6570\u6548\u7387\uff0c\u4e3a\u519c\u4e1a\u9886\u57df\u7684\u89c6\u89c9\u8bed\u8a00\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.05175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.05175", "abs": "https://arxiv.org/abs/2601.05175", "authors": ["Shuming Liu", "Mingchen Zhuge", "Changsheng Zhao", "Jun Chen", "Lemeng Wu", "Zechun Liu", "Chenchen Zhu", "Zhipeng Cai", "Chong Zhou", "Haozhe Liu", "Ernie Chang", "Saksham Suri", "Hongyu Xu", "Qi Qian", "Wei Wen", "Balakrishnan Varadarajan", "Zhuang Liu", "Hu Xu", "Florian Bordes", "Raghuraman Krishnamoorthi", "Bernard Ghanem", "Vikas Chandra", "Yunyang Xiong"], "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice", "comment": "Project page: https://ivul-kaust.github.io/projects/videoauto-r1/", "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVideoAuto-R1\u6846\u67b6\uff0c\u901a\u8fc7\"\u4ec5\u5728\u5fc5\u8981\u65f6\u63a8\u7406\"\u7684\u7b56\u7565\u4f18\u5316\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u5728\u4fdd\u6301\u6700\u5148\u8fdb\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u5c06\u5e73\u5747\u54cd\u5e94\u957f\u5ea6\u51cf\u5c11\u7ea63.3\u500d\u3002", "motivation": "\u5c3d\u7ba1\u601d\u7ef4\u94fe\u63a8\u7406\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5176\u76f8\u5bf9\u4e8e\u76f4\u63a5\u56de\u7b54\u7684\u5fc5\u8981\u6027\u548c\u4f18\u52bf\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u89c6\u9891\u6a21\u578b\uff0c\u76f4\u63a5\u56de\u7b54\u7684\u6027\u80fd\u5f80\u5f80\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u800c\u540e\u8005\u9700\u8981\u66f4\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\u3002\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u7b56\u7565\u3002", "method": "\u63d0\u51faVideoAuto-R1\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u91c7\u7528\"\u4ec5\u5728\u5fc5\u8981\u65f6\u63a8\u7406\"\u7684\u7b56\u7565\u3002\u8bad\u7ec3\u9636\u6bb5\u9075\u5faa\"\u601d\u8003\u4e00\u6b21\uff0c\u56de\u7b54\u4e24\u6b21\"\u8303\u5f0f\uff1a\u6a21\u578b\u9996\u5148\u751f\u6210\u521d\u59cb\u7b54\u6848\uff0c\u7136\u540e\u8fdb\u884c\u63a8\u7406\uff0c\u6700\u540e\u8f93\u51fa\u7ecf\u8fc7\u5ba1\u67e5\u7684\u7b54\u6848\uff0c\u4e24\u4e2a\u7b54\u6848\u90fd\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u8fdb\u884c\u76d1\u7763\u3002\u63a8\u7406\u9636\u6bb5\u4f7f\u7528\u521d\u59cb\u7b54\u6848\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u51b3\u5b9a\u662f\u5426\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u89c6\u9891\u95ee\u7b54\u548c\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideoAuto-R1\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u5c06\u5e73\u5747\u54cd\u5e94\u957f\u5ea6\u51cf\u5c11\u7ea63.3\u500d\uff08\u4f8b\u5982\u4ece149\u4e2a\u6807\u8bb0\u51cf\u5c11\u523044\u4e2a\u6807\u8bb0\uff09\u3002\u7814\u7a76\u8fd8\u89c2\u5bdf\u5230\uff0c\u5728\u611f\u77e5\u5bfc\u5411\u4efb\u52a1\u4e2d\u601d\u7ef4\u6a21\u5f0f\u6fc0\u6d3b\u7387\u8f83\u4f4e\uff0c\u800c\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u6fc0\u6d3b\u7387\u8f83\u9ad8\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u663e\u5f0f\u7684\u57fa\u4e8e\u8bed\u8a00\u7684\u63a8\u7406\u901a\u5e38\u6709\u76ca\u4f46\u5e76\u975e\u603b\u662f\u5fc5\u8981\u3002VideoAuto-R1\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u7406\u7b56\u7565\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u591a\u6a21\u6001\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u5bf9\u63a8\u7406\u9700\u6c42\u7684\u5dee\u5f02\u6027\uff0c\u4e3a\u672a\u6765\u9ad8\u6548\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2601.05237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.05237", "abs": "https://arxiv.org/abs/2601.05237", "authors": ["Rustin Soraki", "Homanga Bharadhwaj", "Ali Farhadi", "Roozbeh Mottaghi"], "title": "ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos", "comment": "Preprint. Project Website: objectforesight.github.io", "summary": "Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ObjectForesight\uff0c\u4e00\u79cd\u4ece\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u5e8f\u5217\u9884\u6d4b\u521a\u4f53\u7269\u4f53\u672a\u67656\u81ea\u7531\u5ea6\u4f4d\u59ff\u548c\u8f68\u8ff9\u76843D\u7269\u4f53\u4e2d\u5fc3\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u663e\u5f0f3D\u7269\u4f53\u7ea7\u8868\u793a\u5b9e\u73b0\u51e0\u4f55\u57fa\u7840\u548c\u65f6\u5e8f\u4e00\u81f4\u7684\u9884\u6d4b\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u8f7b\u677e\u9884\u6d4b\u7269\u4f53\u901a\u8fc7\u4ea4\u4e92\u53ef\u80fd\u53d1\u751f\u7684\u8fd0\u52a8\u6216\u53d8\u5316\uff0c\u4f46\u73b0\u6709\u8ba1\u7b97\u7cfb\u7edf\u7f3a\u4e4f\u76f4\u63a5\u4ece\u88ab\u52a8\u89c6\u89c9\u89c2\u5bdf\u4e2d\u9884\u6d4b\u5408\u7406\u672a\u6765\u7269\u4f53\u8fd0\u52a8\u7684\u80fd\u529b\u3002\u4f20\u7edf\u4e16\u754c\u6a21\u578b\u6216\u52a8\u529b\u5b66\u6a21\u578b\u5728\u50cf\u7d20\u6216\u6f5c\u5728\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u65e0\u6cd5\u63d0\u4f9b\u51e0\u4f55\u57fa\u7840\u548c\u65f6\u5e8f\u4e00\u81f4\u7684\u7269\u4f53\u7ea7\u9884\u6d4b\uff0c\u8fd9\u9650\u5236\u4e86\u7cfb\u7edf\u5bf9\u7269\u4f53\u529f\u80fd\u6027\u548c\u8f68\u8ff9\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "ObjectForesight\u91c7\u75283D\u7269\u4f53\u4e2d\u5fc3\u52a8\u529b\u5b66\u6a21\u578b\u67b6\u6784\uff0c\u4ece\u77ed\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u5e8f\u5217\u9884\u6d4b\u521a\u4f53\u7269\u4f53\u7684\u672a\u67656\u81ea\u7531\u5ea6\u4f4d\u59ff\u548c\u8f68\u8ff9\u3002\u8be5\u65b9\u6cd5\u5728\u7269\u4f53\u7ea7\u522b\u663e\u5f0f\u8868\u793a3D\u4e16\u754c\uff0c\u800c\u975e\u4f20\u7edf\u7684\u50cf\u7d20\u6216\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u3002\u4e3a\u5927\u89c4\u6a21\u8bad\u7ec3\u8be5\u6a21\u578b\uff0c\u7814\u7a76\u5229\u7528\u5206\u5272\u3001\u7f51\u683c\u91cd\u5efa\u548c3D\u4f4d\u59ff\u4f30\u8ba1\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6784\u5efa\u4e86\u5305\u542b200\u591a\u4e07\u4e2a\u77ed\u7247\u6bb5\u548c\u4f2a\u5730\u9762\u771f\u503c3D\u7269\u4f53\u8f68\u8ff9\u7684\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cObjectForesight\u5728\u51c6\u786e\u6027\u3001\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u80fd\u591f\u6709\u6548\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u548c\u573a\u666f\u3002\u8be5\u6a21\u578b\u80fd\u591f\u6355\u6349\u7269\u4f53\u529f\u80fd\u6027\u548c\u8f68\u8ff9\uff0c\u5728\u9884\u6d4b\u672a\u6765\u7269\u4f53\u8fd0\u52a8\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u5b66\u4e60\u7269\u7406\u57fa\u7840\u7684\u7269\u4f53\u4e2d\u5fc3\u52a8\u529b\u5b66\u6a21\u578b\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002", "conclusion": "ObjectForesight\u4e3a\u76f4\u63a5\u4ece\u89c2\u5bdf\u4e2d\u5b66\u4e60\u7269\u7406\u57fa\u7840\u7684\u7269\u4f53\u4e2d\u5fc3\u52a8\u529b\u5b66\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5176\u663e\u5f0f3D\u7269\u4f53\u7ea7\u8868\u793a\u5b9e\u73b0\u4e86\u51e0\u4f55\u57fa\u7840\u548c\u65f6\u5e8f\u4e00\u81f4\u7684\u9884\u6d4b\u3002\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u4f2a\u5730\u9762\u771f\u503c\u6570\u636e\u5728\u8bad\u7ec3\u590d\u6742\u52a8\u529b\u5b66\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u5728\u673a\u5668\u4eba\u3001\u589e\u5f3a\u73b0\u5b9e\u548c\u4ea4\u4e92\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.05239", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.05239", "abs": "https://arxiv.org/abs/2601.05239", "authors": ["Xiao Fu", "Shitao Tang", "Min Shi", "Xian Liu", "Jinwei Gu", "Ming-Yu Liu", "Dahua Lin", "Chen-Hsuan Lin"], "title": "Plenoptic Video Generation", "comment": "Project Page: https://research.nvidia.com/labs/dir/plenopticdreamer/", "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PlenopticDreamer\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u6b65\u751f\u6210\u5e7b\u89c9\u6765\u4fdd\u6301\u65f6\u7a7a\u8bb0\u5fc6\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u89c6\u9891\u91cd\u6e32\u67d3\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76f8\u673a\u63a7\u5236\u7684\u751f\u6210\u89c6\u9891\u91cd\u6e32\u67d3\u65b9\u6cd5\uff08\u5982ReCamMaster\uff09\u5728\u5355\u89c6\u89d2\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u591a\u89c6\u89d2\u573a\u666f\u4e2d\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u751f\u6210\u6a21\u578b\u7684\u968f\u673a\u6027\u4f7f\u5f97\u5e7b\u89c9\u533a\u57df\u7684\u65f6\u7a7a\u8fde\u8d2f\u6027\u96be\u4ee5\u4fdd\u8bc1\uff0c\u8fd9\u6784\u6210\u4e86\u5f53\u524d\u7814\u7a76\u7684\u4e3b\u8981\u5c40\u9650\u6027\u3002", "method": "PlenopticDreamer\u6846\u67b6\u7684\u6838\u5fc3\u662f\u8bad\u7ec3\u4e00\u4e2a\u591a\u8f93\u5165\u5355\u8f93\u51fa\u7684\u89c6\u9891\u6761\u4ef6\u6a21\u578b\uff0c\u91c7\u7528\u81ea\u56de\u5f52\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u8f85\u4ee5\u76f8\u673a\u5f15\u5bfc\u7684\u89c6\u9891\u68c0\u7d22\u7b56\u7565\u81ea\u9002\u5e94\u9009\u62e9\u5148\u524d\u751f\u6210\u7684\u663e\u8457\u89c6\u9891\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u3002\u8bad\u7ec3\u8fc7\u7a0b\u8fd8\u5305\u542b\u6e10\u8fdb\u5f0f\u4e0a\u4e0b\u6587\u7f29\u653e\u4ee5\u6539\u5584\u6536\u655b\u6027\uff0c\u81ea\u6761\u4ef6\u673a\u5236\u4ee5\u589e\u5f3a\u5bf9\u8bef\u5dee\u7d2f\u79ef\u5bfc\u81f4\u7684\u957f\u7a0b\u89c6\u89c9\u9000\u5316\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u957f\u89c6\u9891\u6761\u4ef6\u673a\u5236\u4ee5\u652f\u6301\u6269\u5c55\u89c6\u9891\u751f\u6210\u3002", "result": "\u5728Basic\u548cAgibot\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPlenopticDreamer\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u9891\u91cd\u6e32\u67d3\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u89c6\u89d2\u540c\u6b65\u3001\u9ad8\u4fdd\u771f\u89c6\u89c9\u6548\u679c\u3001\u7cbe\u786e\u7684\u76f8\u673a\u63a7\u5236\u4ee5\u53ca\u591a\u6837\u5316\u7684\u89c6\u89d2\u8f6c\u6362\uff08\u5982\u7b2c\u4e09\u4eba\u79f0\u5230\u7b2c\u4e09\u4eba\u79f0\uff0c\u4ee5\u53ca\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5934\u90e8\u89c6\u89d2\u5230\u5939\u722a\u89c6\u89d2\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u540c\u6b65\u751f\u6210\u5e7b\u89c9\u6765\u4fdd\u6301\u65f6\u7a7a\u8bb0\u5fc6\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u89c6\u9891\u91cd\u6e32\u67d3\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u548c\u76f8\u673a\u63a7\u5236\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u89c6\u89d2\u540c\u6b65\uff0c\u4e3a\u751f\u6210\u89c6\u9891\u91cd\u6e32\u67d3\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u5e94\u7528\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
