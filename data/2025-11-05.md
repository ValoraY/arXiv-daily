<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 26]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.AI](#cs.AI) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning](https://arxiv.org/abs/2511.02280)
*Fangxun Shu, Yongjie Ye, Yue Liao, Zijian Kang, Weijie Yin, Jiacong Wang, Xiao Liang, Shuicheng Yan, Chao Feng*

#### ğŸ§© TL;DR
SAIL-RLæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡æ•™å¯¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä½•æ—¶æ€è€ƒä»¥åŠå¦‚ä½•æ€è€ƒæ¥å¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒå¥–åŠ±ç³»ç»Ÿè¯„ä¼°æ¨ç†è´¨é‡å¹¶è‡ªé€‚åº”åœ°å†³å®šæ·±åº¦æ¨ç†æˆ–ç›´æ¥å›ç­”çš„é€‚ç”¨åœºæ™¯ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•å—é™äºä»…åŸºäºç»“æœçš„ç›‘ç£æœºåˆ¶ï¼Œä»…å¥–åŠ±æ­£ç¡®ç­”æ¡ˆè€Œä¸ç¡®ä¿æ¨ç†è¿‡ç¨‹çš„åˆç†æ€§ï¼ŒåŒæ—¶é‡‡ç”¨ç»Ÿä¸€çš„æ€è€ƒç­–ç•¥å¯¼è‡´ç®€å•ä»»åŠ¡è¿‡åº¦æ€è€ƒè€Œå¤æ‚ä»»åŠ¡æ€è€ƒä¸è¶³çš„é—®é¢˜ã€‚

**Method:** SAIL-RLæå‡ºåŒå¥–åŠ±ç³»ç»Ÿï¼šæ€è€ƒå¥–åŠ±é€šè¿‡äº‹å®åŸºç¡€ã€é€»è¾‘ä¸€è‡´æ€§å’Œç­”æ¡ˆä¸€è‡´æ€§è¯„ä¼°æ¨ç†è´¨é‡ï¼Œåˆ¤æ–­å¥–åŠ±è‡ªé€‚åº”åœ°å†³å®šä½•æ—¶éœ€è¦æ·±åº¦æ¨ç†æˆ–ç›´æ¥å›ç­”ã€‚è¯¥æ¡†æ¶åŸºäºå¼ºåŒ–å­¦ä¹ è¿›è¡Œåè®­ç»ƒä¼˜åŒ–ã€‚

**Result:** åœ¨SAIL-VL2æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAIL-RLåœ¨4Bå’Œ8Bè§„æ¨¡ä¸Šå‡æå‡äº†æ¨ç†å’Œå¤šæ¨¡æ€ç†è§£åŸºå‡†æ€§èƒ½ï¼Œè¾¾åˆ°ä¸GPT-4oç­‰å•†ä¸šé—­æºæ¨¡å‹ç«äº‰çš„æ°´å¹³ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚

**Conclusion:** SAIL-RLä¸ºæ„å»ºæ›´å¯é å’Œè‡ªé€‚åº”çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›äº†åŸåˆ™æ€§æ¡†æ¶ï¼Œé€šè¿‡æ•™å¯¼æ¨¡å‹ä½•æ—¶æ€è€ƒä»¥åŠå¦‚ä½•æ€è€ƒï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æ¨ç†è´¨é‡å’Œæ•ˆç‡æ–¹é¢çš„å±€é™æ€§ã€‚

---

#### ğŸ“„ Abstract
We introduce SAIL-RL, a reinforcement learning (RL) post-training framework
that enhances the reasoning capabilities of multimodal large language models
(MLLMs) by teaching them when and how to think. Existing approaches are limited
by outcome-only supervision, which rewards correct answers without ensuring
sound reasoning, and by uniform thinking strategies, which often lead to
overthinking on simple tasks and underthinking on complex ones. SAIL-RL
addresses these challenges with a dual reward system: the Thinking Reward,
which evaluates reasoning quality through factual grounding, logical coherence,
and answer consistency, and the Judging Reward, which adaptively determines
whether deep reasoning or direct answering is appropriate. Experiments on the
state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal
understanding benchmarks at both 4B and 8B scales, achieving competitive
performance against commercial closed-source models such as GPT-4o, and
substantially reduces hallucinations, establishing it as a principled framework
for building more reliable and adaptive MLLMs. The code will be available at
https://github.com/BytedanceDouyinContent/SAIL-RL.


### [2] [iFlyBot-VLA Technical Report](https://arxiv.org/abs/2511.01914)
*Yuan Zhang, Chenyu Xue, Wenjie Xu, Chao Ji, Jiajia wu, Jia Pan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†iFlyBot-VLAï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å¤§è§„æ¨¡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ä¸‹è®­ç»ƒçš„å¤§æ¨¡å‹ï¼Œé€šè¿‡åŒçº§åŠ¨ä½œè¡¨ç¤ºå’Œæ··åˆè®­ç»ƒç­–ç•¥ï¼Œåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å­˜åœ¨åŠ¨ä½œè¡¨ç¤ºä¸è§†è§‰è¯­è¨€è¡¨ç¤ºç©ºé—´å¯¹é½ä¸è¶³çš„é—®é¢˜ï¼Œéœ€è¦å¼€å‘èƒ½å¤ŸåŒæ—¶æ•æ‰é«˜å±‚æ„å›¾å’Œä½å±‚åŠ¨æ€çš„åŠ¨ä½œè¡¨ç¤ºæ¡†æ¶ã€‚

**Method:** æå‡ºåŒçº§åŠ¨ä½œè¡¨ç¤ºæ¡†æ¶ï¼Œç»“åˆæ½œåœ¨åŠ¨ä½œæ¨¡å‹å’Œç»“æ„åŒ–ç¦»æ•£åŠ¨ä½œæ ‡è®°ï¼Œæ½œåœ¨åŠ¨ä½œä»è·¨å…·èº«æ“ä½œæ•°æ®ä¸­å­¦ä¹ éšå«é«˜å±‚æ„å›¾ï¼Œç¦»æ•£åŠ¨ä½œæ ‡è®°é€šè¿‡é¢‘åŸŸå˜æ¢ç¼–ç æ˜¾å¼ä½å±‚åŠ¨æ€ï¼Œé‡‡ç”¨æ··åˆè®­ç»ƒç­–ç•¥ç»“åˆæœºå™¨äººè½¨è¿¹æ•°æ®ä¸é€šç”¨é—®ç­”æ•°æ®é›†ã€‚

**Result:** åœ¨LIBERO FrankaåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒçœŸå®ä¸–ç•Œè¯„ä¼°æ˜¾ç¤ºåœ¨å¤šæ ·åŒ–æŒ‘æˆ˜æ€§æ“ä½œä»»åŠ¡ä¸­è¾¾åˆ°ç«äº‰æ€§æˆåŠŸç‡ï¼Œè¯æ˜äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åŒçº§åŠ¨ä½œè¡¨ç¤ºå’Œæ··åˆè®­ç»ƒç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆå¯¹é½è¯­è¨€ã€è§†è§‰å’ŒåŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°èŒƒå¼ï¼Œè®¡åˆ’å¼€æºéƒ¨åˆ†è‡ªå»ºæ•°æ®é›†ä»¥ä¿ƒè¿›ç¤¾åŒºç ”ç©¶ã€‚

---

#### ğŸ“„ Abstract
We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model
trained under a novel framework. The main contributions are listed as follows:
(1) a latent action model thoroughly trained on large-scale human and robotic
manipulation videos; (2) a dual-level action representation framework that
jointly supervises both the Vision-Language Model (VLM) and the action expert
during training; (3) a mixed training strategy that combines robot trajectory
data with general QA and spatial QA datasets, effectively enhancing the 3D
perceptual and reasoning capabilities of the VLM backbone. Specifically, the
VLM is trained to predict two complementary forms of actions: latent actions,
derived from our latent action model pretrained on cross-embodiment
manipulation data, which capture implicit high-level intentions; and structured
discrete action tokens, obtained through frequency-domain transformations of
continuous control signals, which encode explicit low-level dynamics. This dual
supervision aligns the representation spaces of language, vision, and action,
enabling the VLM to directly contribute to action generation. Experimental
results on the LIBERO Franka benchmark demonstrate the superiority of our
frame-work, while real-world evaluations further show that iFlyBot-VLA achieves
competitive success rates across diverse and challenging manipulation tasks.
Furthermore, we plan to open-source a portion of our self-constructed dataset
to support future research in the community


### [3] [CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning](https://arxiv.org/abs/2511.02360)
*Jizheng Ma, Xiaofei Zhou, Yanlong Song, Han Yan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CoCoVaæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è¿ç»­è·¨æ¨¡æ€æ¨ç†æœºåˆ¶æ¥å¼¥åˆè§†è§‰æ„ŸçŸ¥çš„ä¸°å¯Œé«˜ç»´ç‰¹æ€§ä¸è¯­è¨€æ¨¡å‹ç¦»æ•£æ¨ç†ç©ºé—´ä¹‹é—´çš„é¸¿æ²Ÿï¼Œåœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå®ç°äº†æ›´é«˜æ•ˆå’Œå‡†ç¡®çš„æ¨ç†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹å—é™äºåœ¨ç¦»æ•£è¯­è¨€æ ‡è®°ç©ºé—´ä¸­è¿›è¡Œæ¨ç†ï¼Œæ— æ³•å……åˆ†è¡¨è¾¾äººç±»è®¤çŸ¥ä¸­é‚£äº›éš¾ä»¥è¨€ä¼ çš„æ€ç»´è¿‡ç¨‹ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹å¯¹ä¸°å¯Œé«˜ç»´è§†è§‰ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ã€‚

**Method:** CoCoVaæ¡†æ¶é‡‡ç”¨è¿­ä»£æ¨ç†å¾ªç¯æœºåˆ¶ï¼Œå…¶ä¸­æ–°å‹Latent Q-Formerä½œä¸ºåŠ¨æ€æ¨ç†å¼•æ“ï¼Œé€šè¿‡è·¨æ¨¡æ€èåˆè¿­ä»£ä¼˜åŒ–æ½œåœ¨æ€ç»´å‘é‡é“¾ï¼Œå¹¶é…åˆåŠ¨æ€æ ‡è®°é€‰æ‹©æœºåˆ¶èšç„¦å…³é”®è§†è§‰åŒºåŸŸï¼ŒåŒæ—¶ä½¿ç”¨å¯¹æ¯”å­¦ä¹ å’ŒåŸºäºæ‰©æ•£çš„é‡æ„å¤šä»»åŠ¡ç›®æ ‡æ¥ç¡®ä¿æ½œåœ¨è¡¨ç¤ºçš„è·¨æ¨¡æ€å¯¹é½ã€‚

**Result:** å®éªŒè¡¨æ˜CoCoVaåœ¨å‡†ç¡®æ€§å’Œæ ‡è®°æ•ˆç‡ä¸Šå‡ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œä½¿ç”¨1.5Béª¨å¹²ç½‘ç»œæ—¶åœ¨å‡ ä¹æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–è¶…è¶Š7B-9Bå¤§æ¨¡å‹çš„æ€§èƒ½ï¼Œå½“æ‰©å±•åˆ°7Béª¨å¹²æ—¶ä»èƒ½ä¸æœ€å…ˆè¿›æ¨¡å‹ä¿æŒç«äº‰åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜å­¦ä¹ çš„æ½œåœ¨ç©ºé—´èƒ½å¤Ÿæ•è·å¯è§£é‡Šçš„ç»“æ„åŒ–æ¨ç†æ¨¡å¼ï¼Œè¯æ˜äº†CoCoVaåœ¨å¼¥åˆç¦»æ•£è¯­è¨€å¤„ç†ä¸è¿ç»­è§†è§‰ç†è§£ä¹‹é—´è¡¨å¾é¸¿æ²Ÿæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨ç†å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
In human cognition, there exist numerous thought processes that are tacit and
beyond verbal expression, enabling us to understand and interact with the world
in multiple ways. However, contemporary Vision-Language Models (VLMs) remain
constrained to reasoning within the discrete and rigid space of linguistic
tokens, thereby bottlenecking the rich, high-dimensional nature of visual
perception. To bridge this gap, we propose CoCoVa (Chain of Continuous
Vision-Language Thought), a novel framework for vision-language model that
leverages continuous cross-modal reasoning for diverse vision-language tasks.
The core of CoCoVa is an iterative reasoning cycle, where a novel Latent
Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a
chain of latent thought vectors through cross-modal fusion. To focus this
process, a token selection mechanism dynamically identifies salient visual
regions, mimicking attentional focus. To ensure these latent thoughts remain
grounded, we train the model with a multi-task objective that combines
contrastive learning and diffusion-based reconstruction, enforcing alignment
between latent representations and both visual and textual modalities.
Evaluations show CoCoVa improves accuracy and token efficiency over strong
baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B
models on almost all benchmarks. When scaled to 7B LLM backbones, it remains
competitive with state-of-the-art models. Qualitative analysis validates that
learned latent space captures interpretable and structured reasoning patterns,
highlighting the potential of CoCoVa to bridge the representational gap between
discrete language processing and the continuous nature of visual understanding.


### [4] [Towards Selection of Large Multimodal Models as Engines for Burned-in Protected Health Information Detection in Medical Images](https://arxiv.org/abs/2511.02014)
*Tuan Truong, Guillermo Jimenez Perez, Pedro Osorio, Matthias Lenga*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒä¸­å—ä¿æŠ¤å¥åº·ä¿¡æ¯æ£€æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°LMMåœ¨OCRå‡†ç¡®ç‡ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ä½†æ•´ä½“PHIæ£€æµ‹æ€§èƒ½æå‡æœ‰é™ï¼Œå¹¶ä¸ºä¸åŒåº”ç”¨åœºæ™¯æä¾›äº†æ¨¡å‹é€‰æ‹©å’Œéƒ¨ç½²ç­–ç•¥å»ºè®®ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»ŸåŒ»ç–—å›¾åƒPHIæ£€æµ‹ä¸»è¦ä¾èµ–OCRæ¨¡å‹ç»“åˆå‘½åå®ä½“è¯†åˆ«æ–¹æ³•ï¼Œå­˜åœ¨æ€§èƒ½ç“¶é¢ˆï¼Œè€Œæ–°å…´çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸ºæ–‡æœ¬æå–å’Œè¯­ä¹‰åˆ†ææä¾›äº†æ–°çš„æŠ€æœ¯æœºä¼šï¼Œéœ€è¦ç³»ç»Ÿè¯„ä¼°å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚

**Method:** ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†GPT-4oã€Gemini 2.5 Flashå’ŒQwen 2.5 7Bä¸‰ä¸ªä¸»æµLMMï¼Œé‡‡ç”¨ä¸¤ç§å¤„ç†æµç¨‹ï¼šçº¯æ–‡æœ¬åˆ†ææµç¨‹ä»¥åŠOCRä¸è¯­ä¹‰åˆ†æç»“åˆçš„æ··åˆæµç¨‹ï¼Œä¸ä¼ ç»ŸEasyOCRæ–¹æ³•è¿›è¡Œå¯¹æ¯”åˆ†æã€‚

**Result:** LMMåœ¨OCRå‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼ˆWERï¼š0.03-0.05ï¼ŒCERï¼š0.02-0.03ï¼‰ï¼Œä½†åœ¨å¤æ‚å°è®°æ¨¡å¼æµ‹è¯•æ¡ˆä¾‹ä¸­è¡¨ç°æœ€ä½³ï¼Œå¯¹äºé«˜å¯¹æ¯”åº¦å¯è¯»æ–‡æœ¬åŒºåŸŸï¼Œä¸åŒæµç¨‹é…ç½®æ•ˆæœç›¸è¿‘ï¼ŒOCRæ€§èƒ½æå‡å¹¶æœªä¸€è‡´è½¬åŒ–ä¸ºæ•´ä½“PHIæ£€æµ‹å‡†ç¡®ç‡çš„æé«˜ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†LMMåœ¨åŒ»ç–—å›¾åƒPHIæ£€æµ‹ä¸­çš„ä¼˜åŠ¿ä¸å±€é™ï¼Œä¸ºä¸åŒæ“ä½œçº¦æŸä¸‹çš„æ¨¡å‹é€‰æ‹©æä¾›äº†å®è¯ä¾æ®ï¼Œå¹¶æå‡ºäº†åŸºäºå¯æ‰©å±•æ¨¡å—åŒ–åŸºç¡€è®¾æ–½çš„éƒ¨ç½²ç­–ç•¥ï¼Œå¯¹å®é™…åŒ»ç–—éšç§ä¿æŠ¤åº”ç”¨å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
The detection of Protected Health Information (PHI) in medical imaging is
critical for safeguarding patient privacy and ensuring compliance with
regulatory frameworks. Traditional detection methodologies predominantly
utilize Optical Character Recognition (OCR) models in conjunction with named
entity recognition. However, recent advancements in Large Multimodal Model
(LMM) present new opportunities for enhanced text extraction and semantic
analysis. In this study, we systematically benchmark three prominent closed and
open-sourced LMMs, namely GPT-4o, Gemini 2.5 Flash, and Qwen 2.5 7B, utilizing
two distinct pipeline configurations: one dedicated to text analysis alone and
another integrating both OCR and semantic analysis. Our results indicate that
LMM exhibits superior OCR efficacy (WER: 0.03-0.05, CER: 0.02-0.03) compared to
conventional models like EasyOCR. However, this improvement in OCR performance
does not consistently correlate with enhanced overall PHI detection accuracy.
The strongest performance gains are observed on test cases with complex imprint
patterns. In scenarios where text regions are well readable with sufficient
contrast, and strong LMMs are employed for text analysis after OCR, different
pipeline configurations yield similar results. Furthermore, we provide
empirically grounded recommendations for LMM selection tailored to specific
operational constraints and propose a deployment strategy that leverages
scalable and modular infrastructure.


### [5] [DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding](https://arxiv.org/abs/2511.02495)
*Zixuan Liu, Siavash H. Khajavi, Guangkai Jiang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†DetectiumFireï¼Œä¸€ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€ç«ç¾æ•°æ®é›†ï¼ŒåŒ…å«22.5Ké«˜åˆ†è¾¨ç‡å›¾åƒå’Œ2.5KçœŸå®ä¸–ç•Œè§†é¢‘ï¼Œæ—¨åœ¨è§£å†³ç«ç¾é¢†åŸŸç¼ºä¹é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„é—®é¢˜ï¼Œæ¨åŠ¨ç«ç¾ç›¸å…³AIç ”ç©¶çš„å‘å±•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç«ç¾é¢†åŸŸçš„åº”ç”¨ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹å…¬å¼€å¯ç”¨çš„é«˜è´¨é‡ç«ç¾é¢†åŸŸæ ‡æ³¨æ•°æ®é›†ï¼Œé™åˆ¶äº†ç«ç¾ç›¸å…³AIæŠ€æœ¯çš„å¼€å‘å’Œåº”ç”¨ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†DetectiumFireæ•°æ®é›†ï¼ŒåŒ…å«22.5Ké«˜åˆ†è¾¨ç‡ç«ç¾å›¾åƒå’Œ2.5KçœŸå®ä¸–ç•Œç«ç¾è§†é¢‘ï¼Œè¦†ç›–å¤šç§ç«ç¾ç±»å‹ã€ç¯å¢ƒå’Œé£é™©ç­‰çº§ï¼Œæ•°æ®æ ‡æ³¨åŒ…æ‹¬ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ ‡ç­¾ï¼ˆå¦‚è¾¹ç•Œæ¡†ï¼‰å’Œè¯¦ç»†æ–‡æœ¬æç¤ºæè¿°åœºæ™¯ï¼Œæ”¯æŒåˆæˆæ•°æ®ç”Ÿæˆå’Œç«ç¾é£é™©æ¨ç†ç­‰åº”ç”¨ã€‚

**Result:** DetectiumFireåœ¨è§„æ¨¡ã€å¤šæ ·æ€§å’Œæ•°æ®è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ï¼Œæœ‰æ•ˆå‡å°‘äº†æ•°æ®å†—ä½™å¹¶å¢å¼ºäº†çœŸå®åœºæ™¯è¦†ç›–åº¦ï¼Œåœ¨ç›®æ ‡æ£€æµ‹ã€åŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆå’Œè§†è§‰è¯­è¨€æ¨ç†ç­‰å¤šä¸ªä»»åŠ¡ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥æ•°æ®é›†å…·æœ‰æ¨åŠ¨ç«ç¾ç›¸å…³ç ”ç©¶å’Œæ™ºèƒ½å®‰å…¨ç³»ç»Ÿå¼€å‘çš„æ½œåŠ›ï¼Œå…¶å…¬å¼€å‘å¸ƒå°†ä¿ƒè¿›AIç¤¾åŒºå¯¹ç«ç¾ç†è§£ç ”ç©¶çš„å¹¿æ³›æ¢ç´¢ï¼Œä¸ºç«ç¾é¢„é˜²å’Œåº”æ€¥å“åº”æä¾›é‡è¦æ•°æ®æ”¯æ’‘ã€‚

---

#### ğŸ“„ Abstract
Recent advances in multi-modal models have demonstrated strong performance in
tasks such as image generation and reasoning. However, applying these models to
the fire domain remains challenging due to the lack of publicly available
datasets with high-quality fire domain annotations. To address this gap, we
introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k
high-resolution fire-related images and 2.5k real-world fire-related videos
covering a wide range of fire types, environments, and risk levels. The data
are annotated with both traditional computer vision labels (e.g., bounding
boxes) and detailed textual prompts describing the scene, enabling applications
such as synthetic data generation and fire risk reasoning. DetectiumFire offers
clear advantages over existing benchmarks in scale, diversity, and data
quality, significantly reducing redundancy and enhancing coverage of real-world
scenarios. We validate the utility of DetectiumFire across multiple tasks,
including object detection, diffusion-based image generation, and
vision-language reasoning. Our results highlight the potential of this dataset
to advance fire-related research and support the development of intelligent
safety systems. We release DetectiumFire to promote broader exploration of fire
understanding in the AI community. The dataset is available at
https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890


### [6] [Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis](https://arxiv.org/abs/2511.02046)
*Soham Joshi, Shwet Kamal Mishra, Viswanath Gopalakrishnan*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªè‡ªåŠ¨åŒ–åˆæˆæ–‡æœ¬è§†è§‰é—®ç­”æ•°æ®é›†çš„ç«¯åˆ°ç«¯æµç¨‹ï¼Œé€šè¿‡æ•´åˆOCRæ£€æµ‹ã€åŒºåŸŸè¯†åˆ«ã€å›¾åƒæè¿°ç”Ÿæˆå’Œé—®é¢˜ç”Ÿæˆç­‰æ¨¡å‹ï¼ŒæˆåŠŸæ„å»ºäº†åŒ…å«çº¦72Ké—®ç­”å¯¹çš„å¤§è§„æ¨¡æ–‡æœ¬-VQAæ•°æ®é›†ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰é¢å‘åœºæ™¯æ–‡æœ¬çš„è§†è§‰é—®ç­”ä»»åŠ¡éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨ï¼Œè¿‡ç¨‹ç¹çä¸”å…·æœ‰æŒ‘æˆ˜æ€§ï¼ŒäºŸéœ€å»ºç«‹èƒ½å¤ŸåŸºäºå›¾åƒä¸­åœºæ™¯æ–‡æœ¬è‡ªåŠ¨åˆæˆé—®ç­”å¯¹çš„ç«¯åˆ°ç«¯æµç¨‹ã€‚

**Method:** è¯¥æ–¹æ³•æ•´åˆäº†OCRæ£€æµ‹ä¸è¯†åˆ«ã€æ„Ÿå…´è¶£åŒºåŸŸæ£€æµ‹ã€å›¾åƒæè¿°ç”Ÿæˆå’Œé—®é¢˜ç”Ÿæˆç­‰å¤šä¸ªæ¨¡å‹ä¸ç®—æ³•ï¼Œå°†è¿™äº›ç»„ä»¶æµçº¿åŒ–æ•´åˆä¸ºç»Ÿä¸€çš„è‡ªåŠ¨åŒ–é—®ç­”å¯¹åˆæˆä¸éªŒè¯æµç¨‹ã€‚

**Result:** è¯¥æµç¨‹æˆåŠŸæ„å»ºäº†åŒ…å«çº¦44Kå›¾åƒå’Œ72Ké—®ç­”å¯¹çš„å¤§è§„æ¨¡æ–‡æœ¬-VQAæ•°æ®é›†ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥è¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿè‡ªåŠ¨åˆæˆå’ŒéªŒè¯å¤§è§„æ¨¡æ–‡æœ¬-VQAæ•°æ®é›†çš„å®Œæ•´æµç¨‹ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨åŸºç¡€æ¨¡å‹å’Œæˆç†ŸOCRæŠ€æœ¯æ„å»ºè‡ªåŠ¨åŒ–æ–‡æœ¬-VQAæ•°æ®é›†åˆæˆæµç¨‹çš„å¯è¡Œæ€§ï¼Œä¸ºå¤§è§„æ¨¡è§†è§‰è¯­è¨€ä»»åŠ¡çš„æ•°æ®é›†æ„å»ºæä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚

---

#### ğŸ“„ Abstract
Creation of large-scale databases for Visual Question Answering tasks
pertaining to the text data in a scene (text-VQA) involves skilful human
annotation, which is tedious and challenging. With the advent of foundation
models that handle vision and language modalities, and with the maturity of OCR
systems, it is the need of the hour to establish an end-to-end pipeline that
can synthesize Question-Answer (QA) pairs based on scene-text from a given
image. We propose a pipeline for automated synthesis for text-VQA dataset that
can produce faithful QA pairs, and which scales up with the availability of
scene text data. Our proposed method harnesses the capabilities of multiple
models and algorithms involving OCR detection and recognition (text spotting),
region of interest (ROI) detection, caption generation, and question
generation. These components are streamlined into a cohesive pipeline to
automate the synthesis and validation of QA pairs. To the best of our
knowledge, this is the first pipeline proposed to automatically synthesize and
validate a large-scale text-VQA dataset comprising around 72K QA pairs based on
around 44K images.


### [7] [UniChange: Unifying Change Detection with Multimodal Large Language Model](https://arxiv.org/abs/2511.02607)
*Xu Zhang, Danyang Li, Xiaohang Dong, Tianhao Wu, Hualong Yu, Jianye Wang, Qicheng Li, Xiang Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†UniChangeï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€å˜åŒ–æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç‰¹æ®Šä»¤ç‰Œå’Œæ–‡æœ¬æç¤ºæœºåˆ¶ï¼ŒæˆåŠŸç»Ÿä¸€äº†äºŒå…ƒå˜åŒ–æ£€æµ‹å’Œè¯­ä¹‰å˜åŒ–æ£€æµ‹ä»»åŠ¡ï¼Œå¹¶åœ¨å¤šä¸ªå…¬å¼€åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å˜åŒ–æ£€æµ‹æ¨¡å‹å­˜åœ¨å…³é”®å±€é™æ€§ï¼Œé€šå¸¸åªèƒ½ä»å•ä¸€ç±»å‹æ ‡æ³¨æ•°æ®ä¸­è·å–æœ‰é™çŸ¥è¯†ï¼Œæ— æ³•åŒæ—¶åˆ©ç”¨å¤šæ ·çš„äºŒå…ƒå˜åŒ–æ£€æµ‹å’Œè¯­ä¹‰å˜åŒ–æ£€æµ‹æ•°æ®é›†ï¼Œè¿™å¯¼è‡´äº†æ³›åŒ–èƒ½åŠ›å·®å’ŒåŠŸèƒ½æœ‰é™çš„é—®é¢˜ã€‚

**Method:** UniChangeåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯­è¨€å…ˆéªŒå’Œç»Ÿä¸€èƒ½åŠ›ï¼Œå°†ç”Ÿæˆå¼è¯­è¨€èƒ½åŠ›ä¸ä¸“é—¨çš„å˜åŒ–æ£€æµ‹åŠŸèƒ½ç›¸ç»“åˆï¼Œé€šè¿‡å¼•å…¥ä¸‰ä¸ªç‰¹æ®Šä»¤ç‰Œ[T1]ã€[T2]å’Œ[CHANGE]æ¥ç»Ÿä¸€äºŒå…ƒå˜åŒ–æ£€æµ‹å’Œè¯­ä¹‰å˜åŒ–æ£€æµ‹ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬æç¤ºæ¥æŒ‡å¯¼å˜åŒ–ç±»åˆ«çš„è¯†åˆ«ï¼Œæ¶ˆé™¤äº†å¯¹é¢„å®šä¹‰åˆ†ç±»å¤´çš„ä¾èµ–ã€‚

**Result:** åœ¨å››ä¸ªå…¬å¼€åŸºå‡†ï¼ˆWHU-CDã€S2Lookingã€LEVIR-CD+å’ŒSECONDï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUniChangeå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåˆ†åˆ«è·å¾—äº†90.41ã€53.04ã€78.87å’Œ57.62çš„IoUåˆ†æ•°ï¼Œè¶…è¶Šäº†æ‰€æœ‰å…ˆå‰çš„æ–¹æ³•ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸ºç»Ÿä¸€å˜åŒ–æ£€æµ‹æ¡†æ¶æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼ŒUniChangeçš„è®¾è®¡ä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆä»å¤šæºæ•°æ®é›†ä¸­è·å–çŸ¥è¯†ï¼Œå³ä½¿è¿™äº›æ•°æ®é›†çš„ç±»åˆ«å®šä¹‰å­˜åœ¨å†²çªï¼Œè¿™ä¸ºå˜åŒ–æ£€æµ‹é¢†åŸŸå¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Change detection (CD) is a fundamental task for monitoring and analyzing land
cover dynamics. While recent high performance models and high quality datasets
have significantly advanced the field, a critical limitation persists. Current
models typically acquire limited knowledge from single-type annotated data and
cannot concurrently leverage diverse binary change detection (BCD) and semantic
change detection (SCD) datasets. This constraint leads to poor generalization
and limited versatility. The recent advancements in Multimodal Large Language
Models (MLLMs) introduce new possibilities for a unified CD framework. We
leverage the language priors and unification capabilities of MLLMs to develop
UniChange, the first MLLM-based unified change detection model. UniChange
integrates generative language abilities with specialized CD functionalities.
Our model successfully unifies both BCD and SCD tasks through the introduction
of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange
utilizes text prompts to guide the identification of change categories,
eliminating the reliance on predefined classification heads. This design allows
UniChange to effectively acquire knowledge from multi-source datasets, even
when their class definitions conflict. Experiments on four public benchmarks
(WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance,
achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively,
surpassing all previous methods. The code is available at
https://github.com/Erxucomeon/UniChange.


### [8] [Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models](https://arxiv.org/abs/2511.02182)
*Jinhwan Seo, Yoonki Cho, Junhyug Noh, Sung-eui Yoon*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºè§£å†³æ¥åœ°è§†é¢‘é—®ç­”ä»»åŠ¡çš„ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è§¦å‘æ—¶åˆ»æ¦‚å¿µæ˜¾è‘—æå‡äº†æ—¶ç©ºæ¥åœ°å’Œè·Ÿè¸ªæ€§èƒ½ï¼Œåœ¨GVQAä»»åŠ¡ä¸Šå®ç°äº†HOTAåˆ†æ•°0.4968çš„é‡å¤§æ”¹è¿›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** GVQAä»»åŠ¡éœ€è¦å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹æ¥å¤„ç†è§†é¢‘å†…å®¹çš„å¤æ‚æ¨ç†ï¼Œå°†ç­”æ¡ˆç»“æœè¿›è¡Œè§†è§‰æ¥åœ°ï¼Œå¹¶åœ¨æ—¶é—´ç»´åº¦ä¸Šè·Ÿè¸ªå‚è€ƒå¯¹è±¡ï¼Œç°æœ‰æ–¹æ³•åœ¨è¿™äº›èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ã€‚

**Method:** æå‡ºçš„æ–¹æ³•å°†GVQAä»»åŠ¡åˆ†è§£ä¸ºä¸‰é˜¶æ®µæµæ°´çº¿ï¼šè§†é¢‘æ¨ç†ä¸é—®ç­”ã€æ—¶ç©ºæ¥åœ°å’Œè·Ÿè¸ªï¼Œå…³é”®è´¡çŒ®æ˜¯å¼•å…¥åŸºäºCORTEXæç¤ºçš„è§¦å‘æ—¶åˆ»æ¦‚å¿µï¼Œè¯¥æ—¶åˆ»ç¡®å®šç›®æ ‡å¯¹è±¡æœ€å¯è§çš„å•ä¸€å¸§ä½œä¸ºæ¥åœ°å’Œè·Ÿè¸ªçš„é²æ£’é”šç‚¹ã€‚

**Result:** åœ¨GVQAä»»åŠ¡ä¸Šå®ç°äº†HOTAåˆ†æ•°0.4968ï¼Œç›¸æ¯”å‰ä¸€å¹´è·èƒœåˆ†æ•°0.2704æœ‰æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** é€šè¿‡ä»»åŠ¡åˆ†è§£å’Œè§¦å‘æ—¶åˆ»çš„å¼•å…¥ï¼Œè¯¥ç ”ç©¶ä¸ºå¤æ‚è§†é¢‘ç†è§£ä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œè¯æ˜äº†å¤šé˜¶æ®µæ–¹æ³•å’Œç²¾ç¡®æ—¶åˆ»å®šä½åœ¨æå‡è§†è§‰æ¥åœ°æ€§èƒ½æ–¹é¢çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
In this technical report, we introduce a framework to address Grounded Video
Question Answering (GVQA) task for the ICCV 2025 Perception Test Challenge. The
GVQA task demands robust multimodal models capable of complex reasoning over
video content, grounding the resulting answers visually, and tracking the
referenced objects temporally. To achieve this capability, our proposed
approach decomposes the GVQA task into a three-stage pipeline: (1) Video
Reasoning \& QA, (2) Spatio-temporal Grounding and (3) Tracking. Our key
contribution is the introduction of a trigger moment, derived from our proposed
CORTEX prompt, which pinpoints the single most visible frame of a target object
to serve as a robust anchor for grounding and tracking. To this end, we achieve
the HOTA score of 0.4968, which marks a significant improvement over the
previous year's winning score of 0.2704 on GVQA task.


### [9] [VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation](https://arxiv.org/abs/2511.02778)
*Kevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex Jinpeng Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºVCodeåŸºå‡†æµ‹è¯•ï¼Œå°†å¤šæ¨¡æ€ç†è§£é‡æ–°å®šä¹‰ä¸ºSVGä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¼€å‘VCoderä»£ç†æ¡†æ¶é€šè¿‡è¿­ä»£ä¿®è®¢å’Œè§†è§‰å·¥å…·å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ä¸­å¿ƒç¼–ç ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰AIç ”ç©¶åœ¨ä»£ç ä½œä¸ºæ¨ç†å’Œæ‰§è¡Œåª’ä»‹æ–¹é¢ä¸»è¦å…³æ³¨è¯­è¨€ä¸­å¿ƒä»»åŠ¡å¦‚ç¨‹åºåˆæˆå’Œè°ƒè¯•ï¼Œè€Œè§†è§‰ä¸­å¿ƒç¼–ç é¢†åŸŸå°šæœªå……åˆ†æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹èƒ½å¤Ÿä¿ç•™ç¬¦å·æ„ä¹‰è¿›è¡Œä¸‹æ¸¸æ¨ç†çš„ç´§å‡‘ã€å¯è§£é‡Šè§†è§‰è¡¨ç¤ºæ–¹æ³•ã€‚

**Method:** æå‡ºVCodeåŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œå°†å¤šæ¨¡æ€ç†è§£é‡æ„ä¸ºSVGä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¼€å‘VCoderä»£ç†æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŸºäºä¿®è®¢çš„è¿­ä»£åˆ†ææœºåˆ¶å’Œåˆ©ç”¨æ£€æµ‹å™¨ä¸è§£æå™¨æä¾›ç»“æ„åŒ–è§†è§‰çº¿ç´¢çš„è§†è§‰å·¥å…·å¢å¼ºç­–ç•¥ã€‚

**Result:** å®éªŒè¡¨æ˜å‰æ²¿è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå¿ å®SVGæ–¹é¢è¡¨ç°ä¸ä½³ï¼ŒVCoderæ¡†æ¶åœ¨åŸºå‡†æµ‹è¯•ä¸­ç›¸æ¯”è¡¨ç°æœ€ä½³çš„Claude-4-Opuså®ç°äº†12.3åˆ†çš„æ€»ä½“æå‡ï¼Œäººç±»ç ”ç©¶æ˜¾ç¤ºæ¸²æŸ“SVGå¯¹äººå’Œæ¨¡å‹éƒ½å…·æœ‰æŒ‘æˆ˜æ€§ä½†ä¸€è‡´æ€§æ­ç¤ºäº†ç¬¦å·è§†è§‰è¡¨ç¤ºçš„æ½œåŠ›ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†è¯­è¨€ä¸­å¿ƒä¸è§†è§‰ä¸­å¿ƒç¼–ç ä¹‹é—´çš„æŒç»­å·®è·ï¼Œè¯æ˜äº†ç¬¦å·è§†è§‰è¡¨ç¤ºåœ¨æ¨ç†ä¸­çš„ä»·å€¼ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€ç†è§£æä¾›äº†æ–°çš„è¯„ä¼°èŒƒå¼å’Œæ”¹è¿›æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸“ä¸šçŸ¥è¯†å’Œ3Dæ¨ç†æ–¹é¢ä»éœ€è¿›ä¸€æ­¥æ¢ç´¢ã€‚

---

#### ğŸ“„ Abstract
Code has emerged as a precise and executable medium for reasoning and action
in the agent era. Yet, progress has largely focused on language-centric tasks
such as program synthesis and debugging, leaving visual-centric coding
underexplored. Inspired by how humans reason over sketches, we advocate SVG
code as a compact, interpretable, and executable visual representation. We
introduce VCode, a benchmark that reframes multimodal understanding as code
generation: given an image, a model must produce SVG that preserves symbolic
meaning for downstream reasoning. VCode covers three domains - general
commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric
perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel
evaluation protocol in which a policy model answers questions over rendered
SVGs; correct answers indicate faithful symbolic preservation. Empirically,
frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap
between language-centric and visual-centric coding. To close this gap, we
introduce VCoder, an agentic framework that augments VLMs along two axes: (i)
Thinking with Revision, which iteratively analyzes discrepancies and refines
SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply
structured cues such as objects, shapes, and text beyond the model's intrinsic
capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities
score well overall yet remain limited in professional knowledge and 3D
reasoning. VCoder delivers a 12.3-point overall gain over the top-performing
Claude-4-Opus. Human studies show that both humans and VLMs perform worse on
rendered SVGs, their consistency reveals the promise of symbolic visual
representation. The benchmark and code are available at
https://github.com/CSU-JPG/VCode.


### [10] [Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers](https://arxiv.org/abs/2511.02206)
*Zhengjie Zhang, Xiaoxie Mao, Qihao Guo, Shaoting Zhang, Qi Huang, Mu Zhou, Fang Xie, Mianxin Liu*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹å¢å¼ºçš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿä»è¡€æ¶²ç”Ÿç‰©æ ‡å¿—ç‰©å’ŒMRIæ‰«æä¸­åˆæˆæ·€ç²‰æ ·è›‹ç™½PETå›¾åƒï¼Œä¸ºé˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­æä¾›äº†ä¸€ç§æˆæœ¬æ›´ä½ã€æ›´æ˜“è·å–çš„æ›¿ä»£æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­ä¸»è¦ä¾èµ–æ·€ç²‰æ ·è›‹ç™½æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼Œä½†è¯¥æ–¹æ³•æˆæœ¬é«˜æ˜‚ä¸”å¯åŠæ€§æœ‰é™ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢æ˜¯å¦èƒ½å¤Ÿä»è¡€æ¶²ç”Ÿç‰©æ ‡å¿—ç‰©å’ŒMRIæ‰«æä¸­é¢„æµ‹æ·€ç²‰æ ·è›‹ç™½PETçš„ç©ºé—´åˆ†å¸ƒæ¨¡å¼ã€‚

**Method:** ç ”ç©¶æ”¶é›†äº†566åå‚ä¸è€…çš„æ·€ç²‰æ ·è›‹ç™½PETå›¾åƒã€T1åŠ æƒMRIæ‰«æå’Œè¡€æ¶²ç”Ÿç‰©æ ‡å¿—ç‰©æ•°æ®ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€ä¿¡æ¯èåˆçš„è¯­è¨€å¢å¼ºç”Ÿæˆæ¨¡å‹æ¥åˆæˆPETå›¾åƒï¼Œå¹¶æ„å»ºäº†å…¨è‡ªåŠ¨è¯Šæ–­æµç¨‹è¿›è¡Œè¯„ä¼°ã€‚

**Result:** åˆæˆçš„PETå›¾åƒåœ¨ç»“æ„ç»†èŠ‚ä¸Šä¸çœŸå®PETæ‰«æé«˜åº¦ç›¸ä¼¼ï¼Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°è¾¾åˆ°0.920Â±0.003ï¼ŒåŒºåŸŸæ¨¡å¼ç›¸å…³æ€§ä¸º0.955Â±0.007ï¼ŒåŸºäºåˆæˆPETçš„è¯Šæ–­ä¸çœŸå®PETè¯Šæ–­çš„ä¸€è‡´æ€§å‡†ç¡®ç‡è¾¾åˆ°0.80ï¼ŒåˆæˆPETæ¨¡å‹åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­ä¸­çš„AUCä¸º0.78ï¼Œä¼˜äºåŸºäºT1çš„æ¨¡å‹å’ŒåŸºäºè¡€æ¶²ç”Ÿç‰©æ ‡å¿—ç‰©çš„æ¨¡å‹ã€‚

**Conclusion:** è¯­è¨€å¢å¼ºç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿåˆæˆé€¼çœŸçš„PETå›¾åƒï¼Œæ˜¾è‘—æå‡äº†MRIå’Œè¡€æ¶²ç”Ÿç‰©æ ‡å¿—ç‰©åœ¨æ·€ç²‰æ ·è›‹ç™½ç©ºé—´æ¨¡å¼è¯„ä¼°ä¸­çš„å®ç”¨æ€§ï¼Œæ”¹å–„äº†é˜¿å°”èŒ¨æµ·é»˜ç—…çš„è¯Šæ–­æµç¨‹ï¼Œä¸ºä½æˆæœ¬ã€å¯æ‰©å±•çš„ç¥ç»é€€è¡Œæ€§ç–¾ç—…è¯Šæ–­æä¾›äº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Background: Alzheimer's disease (AD) diagnosis heavily relies on amyloid-beta
positron emission tomography (Abeta-PET), which is limited by high cost and
limited accessibility. This study explores whether Abeta-PET spatial patterns
can be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We
collected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566
participants. A language-enhanced generative model, driven by a large language
model (LLM) and multimodal information fusion, was developed to synthesize PET
images. Synthesized images were evaluated for image quality, diagnostic
consistency, and clinical applicability within a fully automated diagnostic
pipeline. Findings: The synthetic PET images closely resemble real PET scans in
both structural details (SSIM = 0.920 +/- 0.003) and regional patterns
(Pearson's r = 0.955 +/- 0.007). Diagnostic outcomes using synthetic PET show
high agreement with real PET-based diagnoses (accuracy = 0.80). Using synthetic
PET, we developed a fully automatic AD diagnostic pipeline integrating PET
synthesis and classification. The synthetic PET-based model (AUC = 0.78)
outperforms T1-based (AUC = 0.68) and BBM-based (AUC = 0.73) models, while
combining synthetic PET and BBMs further improved performance (AUC = 0.79).
Ablation analysis supports the advantages of LLM integration and prompt
engineering. Interpretation: Our language-enhanced generative model synthesizes
realistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial
pattern assessment and improving the diagnostic workflow for Alzheimer's
disease.


### [11] [Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2511.02228)
*Delin Ma, Menghui Zhou, Jun Qi, Yun Yang, Po Yang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­çš„åä½œæ³¨æ„åŠ›å’Œä¸€è‡´æ€§å¼•å¯¼èåˆæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆMRIå’ŒPETå¤šæ¨¡æ€ç¥ç»å½±åƒæ•°æ®ï¼Œåœ¨ADNIæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜äºç°æœ‰èåˆæ–¹æ³•çš„è¯Šæ–­æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯å­¦ä¹ å‚æ•°è¡¨ç¤ºå—è¡¥å¿ç¼ºå¤±æ¨¡æ€ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨ä¸€è‡´æ€§å¼•å¯¼æœºåˆ¶æ˜¾å¼å¯¹é½è·¨æ¨¡æ€æ½œåœ¨åˆ†å¸ƒã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€ç¥ç»å½±åƒèåˆæ–¹æ³•ä¸»è¦å¼ºè°ƒè·¨æ¨¡æ€äº’è¡¥æ€§ï¼Œä½†å¿½è§†äº†æ¨¡æ€ç‰¹å¼‚æ€§ç‰¹å¾åœ¨è¯Šæ–­ä¸­çš„é‡è¦æ€§ï¼Œä¸”æ¨¡æ€é—´å›ºæœ‰çš„åˆ†å¸ƒå·®å¼‚å¾€å¾€å¯¼è‡´æœ‰åå’Œå™ªå£°è¡¨ç¤ºï¼Œä»è€Œé™ä½åˆ†ç±»æ€§èƒ½ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ—©æœŸè¯Šæ–­å‡†ç¡®æ€§ã€‚

**Method:** æå‡ºåä½œæ³¨æ„åŠ›å’Œä¸€è‡´æ€§å¼•å¯¼èåˆæ¡†æ¶ï¼ŒåŒ…å«å¯å­¦ä¹ å‚æ•°è¡¨ç¤ºå—ç”¨äºè¡¥å¿ç¼ºå¤±æ¨¡æ€ä¿¡æ¯ï¼Œå…±äº«ç¼–ç å™¨å’Œæ¨¡æ€ç‹¬ç«‹ç¼–ç å™¨ä»¥ä¿ç•™å…±äº«å’Œç‰¹å®šè¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨ä¸€è‡´æ€§å¼•å¯¼æœºåˆ¶æ˜¾å¼å¯¹é½è·¨æ¨¡æ€æ½œåœ¨åˆ†å¸ƒã€‚è¯¥æ¨¡å‹æœ‰æ•ˆæ•´åˆäº†MRIå’ŒPETæ•°æ®çš„å¤šå°ºåº¦äº’è¡¥ç‰¹å¾ã€‚

**Result:** åœ¨ADNIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ç°æœ‰èåˆç­–ç•¥å®ç°äº†æ›´ä¼˜è¶Šçš„è¯Šæ–­æ€§èƒ½ï¼ŒéªŒè¯äº†æ‰€ææ¡†æ¶åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ¨¡å‹é€šè¿‡åŒæ—¶è€ƒè™‘è·¨æ¨¡æ€äº’è¡¥æ€§å’Œæ¨¡æ€ç‰¹å¼‚æ€§ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†è¯Šæ–­å‡†ç¡®ç‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†åŒæ—¶ä¿ç•™å…±äº«å’Œç‰¹å®šæ¨¡æ€è¡¨ç¤ºçš„é‡è¦æ€§ï¼Œä»¥åŠæ˜¾å¼å¯¹é½è·¨æ¨¡æ€åˆ†å¸ƒå¯¹æå‡è¯Šæ–­æ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚æ‰€æå‡ºçš„èåˆæ¡†æ¶ä¸ºå¤šæ¨¡æ€ç¥ç»å½±åƒåˆ†ææä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ—©æœŸè¯Šæ–­å…·æœ‰é‡è¦ä¸´åºŠæ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
Alzheimer's disease (AD) is the most prevalent form of dementia, and its
early diagnosis is essential for slowing disease progression. Recent studies on
multimodal neuroimaging fusion using MRI and PET have achieved promising
results by integrating multi-scale complementary features. However, most
existing approaches primarily emphasize cross-modal complementarity while
overlooking the diagnostic importance of modality-specific features. In
addition, the inherent distributional differences between modalities often lead
to biased and noisy representations, degrading classification performance. To
address these challenges, we propose a Collaborative Attention and
Consistent-Guided Fusion framework for MRI and PET based AD diagnosis. The
proposed model introduces a learnable parameter representation (LPR) block to
compensate for missing modality information, followed by a shared encoder and
modality-independent encoders to preserve both shared and specific
representations. Furthermore, a consistency-guided mechanism is employed to
explicitly align the latent distributions across modalities. Experimental
results on the ADNI dataset demonstrate that our method achieves superior
diagnostic performance compared with existing fusion strategies.


### [12] [Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework](https://arxiv.org/abs/2511.02271)
*Yucheng Song, Yifan Ge, Junhao Li, Zhining Liao, Zhifang Liao*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºHTSC-CIFæ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚ä»»åŠ¡åˆ†è§£æ–¹æ³•è§£å†³åŒ»å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„ä¸‰ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šé¢†åŸŸçŸ¥è¯†ç†è§£ä¸è¶³ã€æ–‡æœ¬-è§†è§‰å®ä½“åµŒå…¥å¯¹é½ä¸ä½³ä»¥åŠè·¨æ¨¡æ€åå·®å¯¼è‡´çš„ä¼ªç›¸å…³æ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åŒ»å­¦æŠ¥å‘Šç”Ÿæˆæ¨¡å‹åœ¨ç—…ç¶æè¿°æ–¹é¢é¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šé¢†åŸŸçŸ¥è¯†ç†è§£ä¸è¶³ã€æ–‡æœ¬-è§†è§‰å®ä½“åµŒå…¥å¯¹é½ä¸ä½³ä»¥åŠè·¨æ¨¡æ€åå·®å¯¼è‡´çš„ä¼ªç›¸å…³æ€§ï¼Œç°æœ‰å·¥ä½œä»…èƒ½è§£å†³å•ä¸ªæŒ‘æˆ˜è€Œæ— æ³•å…¨é¢åº”å¯¹æ‰€æœ‰é—®é¢˜ã€‚

**Method:** æå‡ºHTSC-CIFæ¡†æ¶é‡‡ç”¨åˆ†å±‚ä»»åŠ¡åˆ†è§£æ–¹æ³•ï¼šä½å±‚ä»»åŠ¡å°†åŒ»å­¦å®ä½“ç‰¹å¾ä¸ç©ºé—´ä½ç½®å¯¹é½ä»¥å¢å¼ºè§†è§‰ç¼–ç å™¨çš„é¢†åŸŸçŸ¥è¯†ï¼›ä¸­å±‚ä»»åŠ¡ä½¿ç”¨å‰ç¼€è¯­è¨€å»ºæ¨¡å’Œæ©ç å›¾åƒå»ºæ¨¡é€šè¿‡ç›¸äº’æŒ‡å¯¼æå‡è·¨æ¨¡æ€å¯¹é½ï¼›é«˜å±‚ä»»åŠ¡é€šè¿‡å‰é—¨å¹²é¢„çš„è·¨æ¨¡æ€å› æœå¹²é¢„æ¨¡å—å‡å°‘æ··æ‚å› ç´ å¹¶æé«˜å¯è§£é‡Šæ€§ã€‚

**Result:** å¹¿æ³›å®éªŒéªŒè¯äº†HTSC-CIFæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œåœ¨åŒ»å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åˆ†å±‚ä»»åŠ¡åˆ†è§£æ–¹æ³•åœ¨è§£å†³åŒ»å­¦æŠ¥å‘Šç”Ÿæˆå¤šæŒ‘æˆ˜é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä¸ºè·¨æ¨¡æ€åŒ»å­¦AIç³»ç»Ÿæä¾›äº†æ–°çš„æ¡†æ¶è®¾è®¡æ€è·¯ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Medical Report Generation (MRG) is a key part of modern medical diagnostics,
as it automatically generates reports from radiological images to reduce
radiologists' burden. However, reliable MRG models for lesion description face
three main challenges: insufficient domain knowledge understanding, poor
text-visual entity embedding alignment, and spurious correlations from
cross-modal biases. Previous work only addresses single challenges, while this
paper tackles all three via a novel hierarchical task decomposition approach,
proposing the HTSC-CIF framework. HTSC-CIF classifies the three challenges into
low-, mid-, and high-level tasks: 1) Low-level: align medical entity features
with spatial locations to enhance domain knowledge for visual encoders; 2)
Mid-level: use Prefix Language Modeling (text) and Masked Image Modeling
(images) to boost cross-modal alignment via mutual guidance; 3) High-level: a
cross-modal causal intervention module (via front-door intervention) to reduce
confounders and improve interpretability. Extensive experiments confirm
HTSC-CIF's effectiveness, significantly outperforming state-of-the-art (SOTA)
MRG methods. Code will be made public upon paper acceptance.


### [13] [RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning](https://arxiv.org/abs/2511.02384)
*Jiahe Song, Chuang Wang, Bowen Jiang, Yinfan Wang, Hao Zheng, Xingjian Wei, Chengjin Liu, Junyuan Gao, Yubin Wang, Lijun Wu, Jiang Wu, Qian Yu, Conghui He*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†RxnCaptionæ¡†æ¶ï¼Œå°†åŒ–å­¦ååº”å›¾è§£æé‡æ–°æ„å»ºä¸ºå›¾åƒæè¿°é—®é¢˜ï¼Œé€šè¿‡BIVPç­–ç•¥å’ŒMolYOLOåˆ†å­æ£€æµ‹å™¨æ˜¾è‘—æå‡äº†ç»“æ„æå–è´¨é‡ï¼Œå¹¶æ„å»ºäº†å¤§è§„æ¨¡RxnCaption-11kæ•°æ®é›†ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŒ–å­¦ååº”æ•°æ®é€šå¸¸ä»¥è®ºæ–‡ä¸­çš„å›¾åƒå½¢å¼å­˜åœ¨ï¼Œè¿™äº›æ•°æ®æ— æ³•è¢«æœºå™¨è¯»å–ï¼Œä¹Ÿæ— æ³•ç”¨äºè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè¿™é™åˆ¶äº†AIåœ¨åŒ–å­¦ç ”ç©¶ä¸­çš„åº”ç”¨ã€‚

**Method:** æå‡ºäº†RxnCaptionæ¡†æ¶ï¼Œå°†ä¼ ç»Ÿçš„åæ ‡é¢„æµ‹é©±åŠ¨è§£æè¿‡ç¨‹é‡æ–°æ„å»ºä¸ºå›¾åƒæè¿°é—®é¢˜ï¼Œé‡‡ç”¨BIVPç­–ç•¥ï¼Œä½¿ç”¨æœ€å…ˆè¿›çš„åˆ†å­æ£€æµ‹å™¨MolYOLOåœ¨è¾“å…¥å›¾åƒä¸Šé¢„ç»˜åˆ¶åˆ†å­è¾¹ç•Œæ¡†å’Œç´¢å¼•ï¼Œå°†ä¸‹æ¸¸è§£æè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°é—®é¢˜ã€‚

**Result:** å¹¿æ³›çš„å®éªŒè¡¨æ˜BIVPç­–ç•¥æ˜¾è‘—æé«˜äº†ç»“æ„æå–è´¨é‡å¹¶ç®€åŒ–äº†æ¨¡å‹è®¾è®¡ï¼Œæ„å»ºçš„RxnCaption-11kæ•°æ®é›†æ¯”å…ˆå‰çœŸå®ä¸–ç•Œæ–‡çŒ®åŸºå‡†å¤§ä¸€ä¸ªæ•°é‡çº§ï¼ŒRxnCaption-VLåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†æœ€å…ˆè¿›æ€§èƒ½ã€‚

**Conclusion:** è¯¥æ–¹æ³•ã€æ•°æ®é›†å’Œæ¨¡å‹å°†æ¨åŠ¨ä»åŒ–å­¦æ–‡çŒ®ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯çš„è¿›å±•ï¼Œå¹¶å‚¬åŒ–æ›´å¹¿æ³›çš„AIåœ¨åŒ–å­¦ä¸­çš„åº”ç”¨ï¼Œæ•°æ®ã€æ¨¡å‹å’Œä»£ç å°†åœ¨GitHubä¸Šå‘å¸ƒã€‚

---

#### ğŸ“„ Abstract
Large-scale chemical reaction datasets are crucial for AI research in
chemistry. However, existing chemical reaction data often exist as images
within papers, making them not machine-readable and unusable for training
machine learning models. In response to this challenge, we propose the
RxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP).
Our framework reformulates the traditional coordinate prediction driven parsing
process into an image captioning problem, which Large Vision-Language Models
(LVLMs) handle naturally. We introduce a strategy termed "BBox and Index as
Visual Prompt" (BIVP), which uses our state-of-the-art molecular detector,
MolYOLO, to pre-draw molecular bounding boxes and indices directly onto the
input image. This turns the downstream parsing into a natural-language
description problem. Extensive experiments show that the BIVP strategy
significantly improves structural extraction quality while simplifying model
design. We further construct the RxnCaption-11k dataset, an order of magnitude
larger than prior real-world literature benchmarks, with a balanced test subset
across four layout archetypes. Experiments demonstrate that RxnCaption-VL
achieves state-of-the-art performance on multiple metrics. We believe our
method, dataset, and models will advance structured information extraction from
chemical literature and catalyze broader AI applications in chemistry. We will
release data, models, and code on GitHub.


### [14] [ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension](https://arxiv.org/abs/2511.02415)
*Duo Xu, Hao Cheng, Xin Lin, Zhen Xie, Hao Wang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–å¤šé˜¶æ®µä»£ç é©±åŠ¨ç®¡é“ï¼Œç”¨äºç³»ç»Ÿç”Ÿæˆå¤æ‚å›¾è¡¨ç†è§£æ•°æ®é›†ChartMÂ³ï¼Œè¯¥æ•°æ®é›†æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å›¾è¡¨æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä½¿è¾ƒå°æ¨¡å‹èƒ½å¤Ÿè¾¾åˆ°ä¸æ›´å¤§è§„æ¨¡æ¨¡å‹ç›¸å½“çš„å¤æ‚å›¾è¡¨ç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å›¾è¡¨ç†è§£ä»»åŠ¡ä¸­å­˜åœ¨è¦†ç›–åœºæ™¯æœ‰é™å’Œè®¡ç®—å¯†é›†å‹æ¨ç†ä»»åŠ¡æ”¯æŒä¸è¶³çš„é—®é¢˜ï¼Œéš¾ä»¥æ»¡è¶³å®é™…åº”ç”¨ä¸­å¯¹é«˜çº§è§†è§‰è¯†åˆ«å’Œæ¨ç†èƒ½åŠ›çš„éœ€æ±‚ã€‚

**Method:** é‡‡ç”¨è‡ªåŠ¨åŒ–å¤šé˜¶æ®µä»£ç é©±åŠ¨ç®¡é“ï¼Œé›†æˆæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯è·å–ä¸“ä¸šå›¾è¡¨æ¨¡æ¿ï¼Œå¹¶åˆ©ç”¨æ€ç»´é“¾ç­–ç•¥ç”Ÿæˆæ¨¡æ‹ŸçœŸå®æ•°æ®åˆ†å¸ƒçš„æ¨ç†ä»£ç ï¼Œé©±åŠ¨å›¾è¡¨æ¸²æŸ“å’Œé—®é¢˜ç›¸å…³ç»Ÿè®¡è®¡ç®—ï¼Œé€šè¿‡åŸºäºæ¨¡å‹çš„è¯„ä¼°æå‡å›¾è¡¨å¤šæ ·æ€§å’Œæ•°æ®è´¨é‡ã€‚

**Result:** æ„å»ºäº†åŒ…å«38Kå›¾è¡¨å’Œ142Ké—®ç­”å¯¹çš„ChartMÂ³æ•°æ®é›†ï¼Œç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ å®éªŒè¡¨æ˜è¯¥æ•°æ®é›†æ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›å’Œè·¨é¢†åŸŸæ³›åŒ–æ€§èƒ½ï¼Œä½¿è¾ƒå°æ¨¡å‹åœ¨å¤æ‚å›¾è¡¨ç†è§£ä»»åŠ¡ä¸­èƒ½å¤Ÿè¾¾åˆ°ä¸æ›´å¤§è§„æ¨¡æ¨¡å‹ç›¸å½“çš„è¡¨ç°ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆç®¡é“åœ¨æ„å»ºé«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®é›†æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæå‡æ¨¡å‹åœ¨å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†æ•°æ®è´¨é‡å¯¹æ¨¡å‹èƒ½åŠ›æå‡çš„å…³é”®ä½œç”¨ã€‚

---

#### ğŸ“„ Abstract
Complex chart understanding tasks demand advanced visual recognition and
reasoning capabilities from multimodal large language models (MLLMs). However,
current research provides limited coverage of complex chart scenarios and
computation-intensive reasoning tasks prevalent in real-world applications.
This study proposes an automated multi-stage code-driven pipeline for
systematically generating visual reasoning datasets to address these
limitations. The pipeline integrates retrieval-augmented generation (RAG) to
retrieve professional chart templates and employs chain-of-thought (CoT)
strategies to generate reasoning codes that simulate real data distributions,
thereby driving chart rendering and question-related statistical computations.
Through model-based evaluation, the pipeline enhances chart diversity and data
quality. Using this framework, we construct ChartM$^3$, a multi-dimensional and
multi-step dataset containing 38K charts and 142K Q&A pairs for training, along
with 2,871 high-quality evaluation samples for enabling practical performance
assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL)
experiments demonstrate that our dataset significantly improves reasoning
capabilities and cross-domain generalization performance, enabling smaller
models to achieve performance comparable to larger-scale models in complex
chart comprehension.


### [15] [A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding](https://arxiv.org/abs/2511.02565)
*Jingyu Lu, Haonan Wang, Qixiang Zhang, Xiaomeng Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºVCFlowï¼Œä¸€ç§æ–°é¢–çš„å±‚æ¬¡åŒ–è§£ç æ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡äººç±»è§†è§‰ç³»ç»Ÿçš„è…¹ä¾§-èƒŒä¾§æ¶æ„æ¥å­¦ä¹ å¤šç»´è¡¨å¾ï¼Œå®ç°æ— éœ€ç‰¹å®šå—è¯•è€…è®­ç»ƒçš„è¿ç»­è§†è§‰ä½“éªŒé‡å»ºï¼Œåœ¨ä»…ç‰ºç‰²7%å‡†ç¡®ç‡çš„åŒæ—¶å°†é‡å»ºé€Ÿåº¦æå‡è‡³10ç§’æ¯è§†é¢‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¸»é¢˜æ— å…³çš„è„‘è§£ç æ—¨åœ¨æ— éœ€ç‰¹å®šå—è¯•è€…è®­ç»ƒçš„æƒ…å†µä¸‹ä»fMRIé‡å»ºè¿ç»­è§†è§‰ä½“éªŒï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨æ½œåŠ›ï¼Œä½†ç”±äºè·¨å—è¯•è€…æ³›åŒ–æŒ‘æˆ˜å’Œè„‘ä¿¡å·çš„å¤æ‚æ€§ï¼Œè¯¥æ–¹å‘ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚

**Method:** æå‡ºè§†è§‰çš®å±‚æµæ¶æ„VCFlowï¼Œé€šè¿‡è§£è€¦å’Œåˆ©ç”¨æ—©æœŸè§†è§‰çš®å±‚ã€è…¹ä¾§æµå’ŒèƒŒä¾§æµçš„ç‰¹å¾æ¥å­¦ä¹ å¤šç»´è¡¨å¾ï¼Œå¹¶å¼•å…¥ç‰¹å¾çº§å¯¹æ¯”å­¦ä¹ ç­–ç•¥å¢å¼ºä¸»é¢˜ä¸å˜è¯­ä¹‰è¡¨å¾çš„æå–ã€‚

**Result:** ä¸ä¼ ç»Ÿéœ€è¦è¶…è¿‡12å°æ—¶æ¯å—è¯•è€…æ•°æ®å’Œå¤§é‡è®¡ç®—çš„æ–¹æ³•ç›¸æ¯”ï¼ŒVCFlowå¹³å‡ä»…ç‰ºç‰²7%å‡†ç¡®ç‡ï¼Œå´èƒ½åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ä»¥æ¯è§†é¢‘10ç§’çš„é€Ÿåº¦ç”Ÿæˆé‡å»ºç»“æœã€‚

**Conclusion:** VCFlowæä¾›äº†ä¸€ç§å¿«é€Ÿä¸”ä¸´åºŠå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡è§†è§‰ç³»ç»Ÿå±‚æ¬¡ç»“æ„æˆåŠŸå®ç°äº†è·¨å—è¯•è€…çš„è„‘ä¿¡å·è§£ç ï¼Œä¸ºä¸´åºŠè„‘æœºæ¥å£åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Subject-agnostic brain decoding, which aims to reconstruct continuous visual
experiences from fMRI without subject-specific training, holds great potential
for clinical applications. However, this direction remains underexplored due to
challenges in cross-subject generalization and the complex nature of brain
signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a
novel hierarchical decoding framework that explicitly models the ventral-dorsal
architecture of the human visual system to learn multi-dimensional
representations. By disentangling and leveraging features from early visual
cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary
cognitive information essential for visual reconstruction. Furthermore, we
introduce a feature-level contrastive learning strategy to enhance the
extraction of subject-invariant semantic representations, thereby enhancing
subject-agnostic applicability to previously unseen subjects. Unlike
conventional pipelines that need more than 12 hours of per-subject data and
heavy computation, VCFlow sacrifices only 7\% accuracy on average yet generates
each reconstructed video in 10 seconds without any retraining, offering a fast
and clinically scalable solution. The source code will be released upon
acceptance of the paper.


### [16] [From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics](https://arxiv.org/abs/2511.02427)
*Nicolas Schuler, Lea Dewald, Nick Baldig, JÃ¼rgen Graf*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯„ä¼°äº†ç”¨äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²çš„å°å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åœºæ™¯ç†è§£å’ŒåŠ¨ä½œè¯†åˆ«ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨ç§»åŠ¨æœºå™¨äººåº”ç”¨åœºæ™¯ä¸‹çš„å‡†ç¡®æ€§ä¸æ¨ç†æ—¶é—´æƒè¡¡é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£ã€åœºæ™¯è§£é‡Šå’Œå¸¸è¯†æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶è®¡ç®—å¤æ‚æ€§å¯¹è¾¹ç¼˜è®¾å¤‡å’Œç§»åŠ¨æœºå™¨äººåº”ç”¨æ„æˆäº†æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡†ç¡®æ€§ä¸æ¨ç†æ—¶é—´ä¹‹é—´çš„æƒè¡¡é—®é¢˜ä¸Šã€‚

**Method:** æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è¯„ä¼°æµç¨‹ï¼Œä¸“é—¨ç ”ç©¶èƒ½å¤Ÿåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²çš„å°å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åœºæ™¯è§£é‡Šå’ŒåŠ¨ä½œè¯†åˆ«ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œå¹¶åœ¨åŒ…å«çœŸå®ä¸–ç•ŒåŸå¸‚æ™¯è§‚ã€æ ¡å›­å’Œå®¤å†…åœºæ™¯çš„å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ã€‚

**Result:** å®éªŒè¯„ä¼°æ·±å…¥æ¢è®¨äº†å°å‹æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ½œåŠ›ï¼Œç‰¹åˆ«å…³æ³¨äº†è¿™äº›æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ã€å¼±ç‚¹ã€å›ºæœ‰åè§ä»¥åŠæ‰€è·ä¿¡æ¯çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºè¾¹ç¼˜è®¡ç®—ç¯å¢ƒä¸­çš„è§†è§‰è¯­è¨€æ¨¡å‹éƒ¨ç½²æä¾›äº†é‡è¦è§è§£ï¼Œæ­ç¤ºäº†å°å‹æ¨¡å‹åœ¨ç§»åŠ¨æœºå™¨äººåº”ç”¨ä¸­çš„å®é™…å¯è¡Œæ€§å’Œå±€é™æ€§ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ™ºèƒ½æ„ŸçŸ¥ç³»ç»Ÿè®¾è®¡æä¾›äº†æŒ‡å¯¼ã€‚

---

#### ğŸ“„ Abstract
Video Understanding, Scene Interpretation and Commonsense Reasoning are
highly challenging tasks enabling the interpretation of visual information,
allowing agents to perceive, interact with and make rational decisions in its
environment. Large Language Models (LLMs) and Visual Language Models (VLMs)
have shown remarkable advancements in these areas in recent years, enabling
domain-specific applications as well as zero-shot open vocabulary tasks,
combining multiple domains. However, the required computational complexity
poses challenges for their application on edge devices and in the context of
Mobile Robotics, especially considering the trade-off between accuracy and
inference time. In this paper, we investigate the capabilities of
state-of-the-art VLMs for the task of Scene Interpretation and Action
Recognition, with special regard to small VLMs capable of being deployed to
edge devices in the context of Mobile Robotics. The proposed pipeline is
evaluated on a diverse dataset consisting of various real-world cityscape,
on-campus and indoor scenarios. The experimental evaluation discusses the
potential of these small models on edge devices, with particular emphasis on
challenges, weaknesses, inherent model biases and the application of the gained
information. Supplementary material is provided via the following repository:
https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/


### [17] [TAUE: Training-free Noise Transplant and Cultivation Diffusion Model](https://arxiv.org/abs/2511.02580)
*Daichi Nagai, Ryugo Morita, Shunsuke Kitada, Hitoshi Iyatomi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†TAUEæ¡†æ¶ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„å±‚å¼å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡å™ªå£°ç§»æ¤ä¸åŸ¹è‚²æŠ€æœ¯å®ç°å‰æ™¯ã€èƒŒæ™¯å’Œåˆæˆå±‚çš„è¯­ä¹‰ç»“æ„ä¸€è‡´æ€§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å±‚å¼æ§åˆ¶æ–¹é¢çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä»…èƒ½ç”Ÿæˆå•ä¸€å¹³é¢å›¾åƒï¼Œè¿™æˆä¸ºä¸“ä¸šåº”ç”¨ä¸­éœ€è¦å±‚å¼æ§åˆ¶çš„å…³é”®ç“¶é¢ˆã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆè¦ä¹ˆä¾èµ–å¤§è§„æ¨¡ä¸å¯å¾—æ•°æ®é›†çš„å¾®è°ƒï¼Œè¦ä¹ˆæ˜¯æ— éœ€è®­ç»ƒä½†ä»…é™äºç”Ÿæˆå­¤ç«‹å‰æ™¯å…ƒç´ ï¼Œæ— æ³•äº§ç”Ÿå®Œæ•´è¿è´¯çš„åœºæ™¯ã€‚

**Method:** æˆ‘ä»¬æå‡ºäº†å™ªå£°ç§»æ¤ä¸åŸ¹è‚²æŠ€æœ¯ï¼Œä»å‰æ™¯å’Œåˆæˆç”Ÿæˆè¿‡ç¨‹ä¸­æå–ä¸­é—´æ½œåœ¨è¡¨ç¤ºï¼Œå°†å…¶ç§»æ¤åˆ°åç»­å±‚çš„åˆå§‹å™ªå£°ä¸­ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†å‰æ™¯ã€èƒŒæ™¯å’Œåˆæˆå±‚ä¹‹é—´çš„è¯­ä¹‰å’Œç»“æ„ä¸€è‡´æ€§ï¼Œæ— éœ€å¾®è°ƒæˆ–è¾…åŠ©æ•°æ®é›†å³å¯å®ç°ä¸€è‡´çš„å±‚å¼è¾“å‡ºã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ— éœ€è®­ç»ƒæ–¹æ³•è¾¾åˆ°äº†ä¸å¾®è°ƒæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œåœ¨ä¿æŒé«˜å›¾åƒè´¨é‡å’Œä¿çœŸåº¦çš„åŒæ—¶å¢å¼ºäº†å±‚å¼ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•æ¶ˆé™¤äº†æ˜‚è´µçš„è®­ç»ƒå’Œæ•°æ®é›†éœ€æ±‚ï¼ŒåŒæ—¶è§£é”äº†å¤æ‚ç»„åˆç¼–è¾‘ç­‰æ–°é¢–ä¸‹æ¸¸åº”ç”¨ã€‚

**Conclusion:** TAUEä¸ä»…æ¶ˆé™¤äº†æˆæœ¬é«˜æ˜‚çš„è®­ç»ƒå’Œæ•°æ®é›†éœ€æ±‚ï¼Œè¿˜ä¸ºæ›´æ˜“è®¿é—®å’Œå¯æ§çš„ç”Ÿæˆå·¥ä½œæµç¨‹å¼€è¾Ÿäº†é“è·¯ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†æ— éœ€è®­ç»ƒå³å¯å®ç°ä¸“ä¸šçº§å±‚å¼å›¾åƒç”Ÿæˆçš„å¯è¡Œæ€§ï¼Œä¸ºç”Ÿæˆå¼AIçš„å®é™…åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

---

#### ğŸ“„ Abstract
Despite the remarkable success of text-to-image diffusion models, their
output of a single, flattened image remains a critical bottleneck for
professional applications requiring layer-wise control. Existing solutions
either rely on fine-tuning with large, inaccessible datasets or are
training-free yet limited to generating isolated foreground elements, failing
to produce a complete and coherent scene. To address this, we introduce the
Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a
novel framework for zero-shot, layer-wise image generation. Our core technique,
Noise Transplantation and Cultivation (NTC), extracts intermediate latent
representations from both foreground and composite generation processes,
transplanting them into the initial noise for subsequent layers. This ensures
semantic and structural coherence across foreground, background, and composite
layers, enabling consistent, multi-layered outputs without requiring
fine-tuning or auxiliary datasets. Extensive experiments show that our
training-free method achieves performance comparable to fine-tuned methods,
enhancing layer-wise consistency while maintaining high image quality and
fidelity. TAUE not only eliminates costly training and dataset requirements but
also unlocks novel downstream applications, such as complex compositional
editing, paving the way for more accessible and controllable generative
workflows.


### [18] [LLEXICORP: End-user Explainability of Convolutional Neural Networks](https://arxiv.org/abs/2511.02720)
*VojtÄ›ch KÅ¯r, Adam Bajger, Adam KukuÄka, Marek Hradil, VÃ­t Musil, TomÃ¡Å¡ BrÃ¡zdil*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºLLEXICORPï¼Œä¸€ç§å°†æ¦‚å¿µç›¸å…³æ€§ä¼ æ’­ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç›¸ç»“åˆçš„æ¨¡å—åŒ–ç®¡é“ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ä¸ºæ¦‚å¿µåŸå‹åˆ†é…æè¿°æ€§åç§°å¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šï¼Œæ˜¾è‘—é™ä½äº†æ·±åº¦ç¥ç»ç½‘ç»œè§£é‡Šçš„é—¨æ§›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ¦‚å¿µç›¸å…³æ€§ä¼ æ’­å·¥ä½œæµç¨‹ä¸»è¦ä¾èµ–äººå·¥æ“ä½œï¼Œä¸“å®¶éœ€è¦æ£€æŸ¥æ¿€æ´»å›¾åƒæ¥å‘½åå‘ç°çš„æ¦‚å¿µï¼Œå¹¶ä»ç›¸å…³æ€§å›¾ä¸­åˆæˆå†—é•¿çš„è§£é‡Šï¼Œè¿™é™åˆ¶äº†è§£é‡Šçš„å¯è®¿é—®æ€§å’Œå¯æ‰©å±•æ€§ã€‚

**Method:** è¯¥æ–¹æ³•å°†æ¦‚å¿µç›¸å…³æ€§ä¼ æ’­ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è€¦åˆï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯æ•™ç»™è¯­è¨€æ¨¡å‹CRPçš„è¯­ä¹‰ï¼Œå¹¶å¼ºåˆ¶åˆ†ç¦»å‘½åå’Œè§£é‡Šä»»åŠ¡ï¼Œç”Ÿæˆçš„æ–‡æœ¬å¯ä»¥æ ¹æ®ä¸åŒå—ä¼—è¿›è¡Œå®šåˆ¶ã€‚

**Result:** åœ¨ImageNetæ•°æ®é›†ä¸Šçš„VGG16æ¨¡å‹ä¸Šè¿›è¡Œå®šæ€§è¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæè¿°æ€§æ¦‚å¿µåç§°å’Œç›´è§‚çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œå°†å®šé‡ç›¸å…³æ€§åˆ†å¸ƒè½¬åŒ–ä¸ºæ˜“äºç†è§£çš„å™è¿°ã€‚

**Conclusion:** å°†åŸºäºæ¦‚å¿µçš„å¯å½’å› æ–¹æ³•ä¸å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆå¯ä»¥æ˜¾è‘—é™ä½è§£é‡Šæ·±åº¦ç¥ç»ç½‘ç»œçš„é—¨æ§›ï¼Œä¸ºæ›´é€æ˜çš„AIç³»ç»Ÿé“ºå¹³é“è·¯ï¼Œç”Ÿæˆçš„è§£é‡Šå¯ä»¥æ ¹æ®æŠ€æœ¯èƒŒæ™¯ä¸ºä¸åŒå—ä¼—æä¾›é€‚å½“è¯¦ç»†ç¨‹åº¦çš„ä¿¡æ¯ã€‚

---

#### ğŸ“„ Abstract
Convolutional neural networks (CNNs) underpin many modern computer vision
systems. With applications ranging from common to critical areas, a need to
explain and understand the model and its decisions (XAI) emerged. Prior works
suggest that in the top layers of CNNs, the individual channels can be
attributed to classifying human-understandable concepts. Concept relevance
propagation (CRP) methods can backtrack predictions to these channels and find
images that most activate these channels. However, current CRP workflows are
largely manual: experts must inspect activation images to name the discovered
concepts and must synthesize verbose explanations from relevance maps, limiting
the accessibility of the explanations and their scalability.
  To address these issues, we introduce Large Language model EXplaIns COncept
Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a
multimodal large language model. Our approach automatically assigns descriptive
names to concept prototypes and generates natural-language explanations that
translate quantitative relevance distributions into intuitive narratives. To
ensure faithfulness, we craft prompts that teach the language model the
semantics of CRP through examples and enforce a separation between naming and
explanation tasks. The resulting text can be tailored to different audiences,
offering low-level technical descriptions for experts and high-level summaries
for non-technical stakeholders.
  We qualitatively evaluate our method on various images from ImageNet on a
VGG16 model. Our findings suggest that integrating concept-based attribution
methods with large language models can significantly lower the barrier to
interpreting deep neural networks, paving the way for more transparent AI
systems.


### [19] [Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes](https://arxiv.org/abs/2511.02503)
*Robinson Umeike, Neil Getty, Yin Xiangyu, Yi Jiang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PtychoBenchåŸºå‡†æµ‹è¯•ï¼Œç³»ç»Ÿæ¯”è¾ƒäº†ç›‘ç£å¾®è°ƒ(SFT)å’Œä¸Šä¸‹æ–‡å­¦ä¹ (ICL)ä¸¤ç§é¢†åŸŸé€‚åº”ç­–ç•¥åœ¨ç§‘å­¦æ˜¾å¾®é•œå·¥ä½œæµè‡ªåŠ¨åŒ–ä¸­çš„åº”ç”¨ï¼Œå‘ç°æœ€ä¼˜ç­–ç•¥å–å†³äºä»»åŠ¡æ¨¡æ€ï¼šè§†è§‰ä»»åŠ¡ä¸­SFTä¸ICLäº’è¡¥ï¼Œæ–‡æœ¬ä»»åŠ¡ä¸­ICLè¡¨ç°æ›´ä¼˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å…ˆè¿›æ˜¾å¾®é•œå·¥ä½œæµè‡ªåŠ¨åŒ–æ˜¯é‡è¦ç›®æ ‡ï¼ŒåŸºç¡€æ¨¡å‹å¦‚LLMså’ŒVLMså±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å°†è¿™äº›é€šç”¨æ¨¡å‹é€‚åº”ä¸“ä¸šç§‘å­¦ä»»åŠ¡è‡³å…³é‡è¦ï¼Œè€Œæœ€ä¼˜é¢†åŸŸé€‚åº”ç­–ç•¥å°šä¸æ˜ç¡®ã€‚

**Method:** å¼•å…¥PtychoBenchå¤šæ¨¡æ€å¤šä»»åŠ¡åŸºå‡†æµ‹è¯•ï¼Œç³»ç»Ÿæ¯”è¾ƒç›‘ç£å¾®è°ƒ(SFT)å’Œä¸Šä¸‹æ–‡å­¦ä¹ (ICL)ä¸¤ç§ä¸“ä¸šåŒ–ç­–ç•¥ï¼Œåœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹è¯„ä¼°VLMsçš„è§†è§‰ä¼ªå½±æ£€æµ‹ä»»åŠ¡å’ŒLLMsçš„æ–‡æœ¬å‚æ•°æ¨èä»»åŠ¡ã€‚

**Result:** ç ”ç©¶å‘ç°æœ€ä¼˜ä¸“ä¸šåŒ–è·¯å¾„å…·æœ‰ä»»åŠ¡ä¾èµ–æ€§ï¼šè§†è§‰ä»»åŠ¡ä¸­SFTå’ŒICLé«˜åº¦äº’è¡¥ï¼Œå¾®è°ƒæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç¤ºä¾‹æŒ‡å¯¼ä¸‹è¾¾åˆ°æœ€é«˜æ€§èƒ½ï¼ˆMicro-F1 0.728ï¼‰ï¼›æ–‡æœ¬ä»»åŠ¡ä¸­å¤§å‹åŸºç¡€æ¨¡å‹çš„ICLæ˜¯æ›´ä¼˜ç­–ç•¥ï¼Œè¾¾åˆ°å³°å€¼Micro-F1 0.847ï¼Œä¼˜äºå¼ºå¤§çš„"è¶…çº§ä¸“å®¶"SFTæ¨¡å‹ï¼ˆ0-shot Micro-F1 0.839ï¼‰ã€‚

**Conclusion:** ç ”ç©¶ç»“æœä¸ºç§‘å­¦AIæä¾›äº†å…³é”®è§‚å¯Ÿï¼šåŸºå‡†æµ‹è¯•ä¸­æœ€ä¼˜ä¸“ä¸šåŒ–è·¯å¾„å–å†³äºä»»åŠ¡æ¨¡æ€ï¼Œä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„åŸºäºç§‘å­¦çš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†æ¸…æ™°æ¡†æ¶ï¼ŒåŒæ—¶ç¡®è®¤äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥æç¤ºçš„ä¼˜è¶Šæ€§å¹¶è¯†åˆ«äº†å¾®è°ƒæ¨¡å‹ä¸­ä¸€è‡´çš„ä¸Šä¸‹æ–‡å¹²æ‰°ç°è±¡ã€‚

---

#### ğŸ“„ Abstract
The automation of workflows in advanced microscopy is a key goal where
foundation models like Language Models (LLMs) and Vision-Language Models (VLMs)
show great potential. However, adapting these general-purpose models for
specialized scientific tasks is critical, and the optimal domain adaptation
strategy is often unclear. To address this, we introduce PtychoBench, a new
multi-modal, multi-task benchmark for ptychographic analysis. Using this
benchmark, we systematically compare two specialization strategies: Supervised
Fine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies
on a visual artifact detection task with VLMs and a textual parameter
recommendation task with LLMs in a data-scarce regime. Our findings reveal that
the optimal specialization pathway is task-dependent. For the visual task, SFT
and ICL are highly complementary, with a fine-tuned model guided by
context-aware examples achieving the highest mean performance (Micro-F1 of
0.728). Conversely, for the textual task, ICL on a large base model is the
superior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a
powerful "super-expert" SFT model (0-shot Micro-F1 of 0.839). We also confirm
the superiority of context-aware prompting and identify a consistent contextual
interference phenomenon in fine-tuned models. These results, benchmarked
against strong baselines including GPT-4o and a DINOv3-based classifier, offer
key observations for AI in science: the optimal specialization path in our
benchmark is dependent on the task modality, offering a clear framework for
developing more effective science-based agentic systems.


### [20] [Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems](https://arxiv.org/abs/2511.02507)
*Nicolas Schuler, Lea Dewald, JÃ¼rgen Graf*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç§»åŠ¨æœºå™¨äººçš„è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆæµæ°´çº¿ï¼Œè¯¥æ–¹æ¡ˆå®Œå…¨ä¾èµ–æœ¬åœ°æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œï¼Œä¿æŠ¤ç”¨æˆ·éšç§å¹¶æ— éœ€å¤–éƒ¨æœåŠ¡ï¼Œé€šè¿‡å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®ç”Ÿæˆè‡ªç„¶è¯­è¨€æŠ¥å‘Šã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€æ·±åº¦å­¦ä¹ åœ¨ç¡¬ä»¶è®¤çŸ¥ç³»ç»Ÿä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶å’ŒæœåŠ¡æœºå™¨äººç­‰å…³é”®ä»»åŠ¡ä¸­ï¼Œéœ€è¦è¯„ä¼°å¤§é‡å¼‚æ„æ•°æ®ã€‚è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆå¯¹äºä¿ƒè¿›æ­¤ç±»ç³»ç»Ÿåœ¨å„ä¸ªé¢†åŸŸçš„è¯„ä¼°å’Œæ¥å—åº¦è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å¤–éƒ¨æœåŠ¡ä¸”å­˜åœ¨éšç§é£é™©ã€‚

**Method:** æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€ä¼ æ„Ÿå™¨çš„è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆæµæ°´çº¿ï¼Œè¯¥æ–¹æ¡ˆå®Œå…¨é‡‡ç”¨æœ¬åœ°æ¨¡å‹éƒ¨ç½²åœ¨è¾¹ç¼˜è®¡ç®—è®¾å¤‡ä¸Šï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨äº‘æœåŠ¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å„ç§ä¼ æ„Ÿå™¨æ•°æ®ç”Ÿæˆè‡ªç„¶è¯­è¨€æŠ¥å‘Šï¼Œç¡®ä¿æ‰€æœ‰å‚ä¸è€…çš„éšç§ä¿æŠ¤ã€‚

**Result:** åœ¨æ¶µç›–å®¤å†…ã€å®¤å¤–å’ŒåŸå¸‚ç¯å¢ƒç­‰å¤šç§é¢†åŸŸçš„å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæä¾›äº†å®šé‡å’Œå®šæ€§è¯„ä¼°ç»“æœã€‚ç”Ÿæˆçš„ç¤ºä¾‹æŠ¥å‘Šå’Œè¡¥å……ææ–™é€šè¿‡å…¬å…±å­˜å‚¨åº“æä¾›ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å®Œå…¨åŸºäºè¾¹ç¼˜è®¡ç®—çš„è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿçš„å¯è¡Œæ€§ï¼Œä¸ºç§»åŠ¨æœºå™¨äººç³»ç»Ÿæä¾›äº†ä¸€ç§éšç§ä¿æŠ¤çš„è¯„ä¼°è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰åœ¨å…³é”®ä»»åŠ¡åº”ç”¨ä¸­æ¨å¹¿çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥æœ¬åœ°åŒ–AIç³»ç»Ÿçš„å‘å±•æä¾›äº†å‚è€ƒã€‚

---

#### ğŸ“„ Abstract
Recent advancements in Deep Learning enable hardware-based cognitive systems,
that is, mechatronic systems in general and robotics in particular with
integrated Artificial Intelligence, to interact with dynamic and unstructured
environments. While the results are impressive, the application of such systems
to critical tasks like autonomous driving as well as service and care robotics
necessitate the evaluation of large amount of heterogeneous data. Automated
report generation for Mobile Robotics can play a crucial role in facilitating
the evaluation and acceptance of such systems in various domains. In this
paper, we propose a pipeline for generating automated reports in natural
language utilizing various multi-modal sensors that solely relies on local
models capable of being deployed on edge computing devices, thus preserving the
privacy of all actors involved and eliminating the need for external services.
In particular, we evaluate our implementation on a diverse dataset spanning
multiple domains including indoor, outdoor and urban environments, providing
quantitative as well as qualitative evaluation results. Various generated
example reports and other supplementary materials are available via a public
repository.


### [21] [Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification](https://arxiv.org/abs/2511.02564)
*Md Rashidunnabi, Kailash A. Hambarde, Vasco Lopes, Joao C. Neves, Hugo Proenca*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MTF-CVReIDæ¡†æ¶ï¼Œé€šè¿‡ä¸ƒä¸ªå‚æ•°é«˜æ•ˆçš„æ¨¡å—å¢å¼ºViT-B/16éª¨å¹²ç½‘ç»œï¼Œæ˜¾è‘—æå‡äº†è·¨è§†è§’è§†é¢‘è¡Œäººé‡è¯†åˆ«çš„æ€§èƒ½ï¼Œåœ¨ä¿æŒå®æ—¶æ•ˆç‡çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è·¨è§†è§’è§†é¢‘è¡Œäººé‡è¯†åˆ«é¢ä¸´æç«¯è§†è§’å˜åŒ–ã€å°ºåº¦å·®å¼‚å’Œæ—¶é—´ä¸ä¸€è‡´æ€§ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç©ºä¸­-åœ°é¢ç›‘æ§ç­‰åœºæ™¯æ—¶æ€§èƒ½å—é™ï¼Œéœ€è¦å¼€å‘èƒ½å¤ŸåŒæ—¶è§£å†³è¿™äº›é—®é¢˜çš„é²æ£’æ¡†æ¶ã€‚

**Method:** åŸºäºViT-B/16éª¨å¹²ç½‘ç»œï¼Œå¼•å…¥äº†ä¸ƒä¸ªäº’è¡¥æ¨¡å—ï¼šè·¨æµç‰¹å¾å½’ä¸€åŒ–ç”¨äºçº æ­£ç›¸æœºå’Œè§†è§’åå·®ï¼Œå¤šåˆ†è¾¨ç‡ç‰¹å¾åè°ƒç”¨äºå°ºåº¦ç¨³å®šï¼Œèº«ä»½æ„ŸçŸ¥è®°å¿†æ¨¡å—å¼ºåŒ–èº«ä»½ç‰¹å¾ï¼Œæ—¶é—´åŠ¨æ€å»ºæ¨¡è¿›è¡Œè¿åŠ¨æ„ŸçŸ¥ç¼–ç ï¼Œè·¨è§†è§’ç‰¹å¾å¯¹é½å®ç°è§†è§’ä¸å˜è¡¨ç¤ºï¼Œåˆ†å±‚æ—¶é—´æ¨¡å¼å­¦ä¹ æ•è·å¤šå°ºåº¦æ—¶é—´è§„å¾‹ï¼Œå¤šè§†è§’èº«ä»½ä¸€è‡´æ€§å­¦ä¹ é€šè¿‡å¯¹æ¯”å­¦ä¹ å¼ºåˆ¶è·¨è§†è§’èº«ä»½ä¸€è‡´æ€§ã€‚

**Result:** MTF-CVReIDä»…å¢åŠ çº¦200ä¸‡å‚æ•°å’Œ0.7 GFLOPsï¼Œä¿æŒå®æ—¶æ•ˆç‡ï¼ˆ189 FPSï¼‰ï¼Œåœ¨AG-VPReIDåŸºå‡†æµ‹è¯•çš„æ‰€æœ‰é«˜åº¦çº§åˆ«ä¸Šå®ç°æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶åœ¨G2A-VReIDå’ŒMARSæ•°æ®é›†ä¸Šå±•ç°å‡ºå¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** ç²¾å¿ƒè®¾è®¡çš„åŸºäºé€‚é…å™¨çš„æ¨¡å—å¯ä»¥åœ¨ä¸ç‰ºç‰²è®¡ç®—æ•ˆç‡çš„æƒ…å†µä¸‹æ˜¾è‘—å¢å¼ºè·¨è§†è§’é²æ£’æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œè¯æ˜äº†å‚æ•°é«˜æ•ˆæ–¹æ³•åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®é™…éƒ¨ç½²æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Video-based person re-identification (ReID) in cross-view domains (for
example, aerial-ground surveillance) remains an open problem because of extreme
viewpoint shifts, scale disparities, and temporal inconsistencies. To address
these challenges, we propose MTF-CVReID, a parameter-efficient framework that
introduces seven complementary modules over a ViT-B/16 backbone. Specifically,
we include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera and
view biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scale
stabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) to
reinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) for
motion-aware short-term temporal encoding; (5) Inter-View Feature Alignment
(IVFA) for perspective-invariant representation alignment; (6) Hierarchical
Temporal Pattern Learning (HTPL) to capture multi-scale temporal regularities;
and (7) Multi-View Identity Consistency Learning (MVICL) that enforces
cross-view identity coherence using a contrastive learning paradigm. Despite
adding only about 2 million parameters and 0.7 GFLOPs over the baseline,
MTF-CVReID maintains real-time efficiency (189 FPS) and achieves
state-of-the-art performance on the AG-VPReID benchmark across all altitude
levels, with strong cross-dataset generalization to G2A-VReID and MARS
datasets. These results show that carefully designed adapter-based modules can
substantially enhance cross-view robustness and temporal consistency without
compromising computational efficiency. The source code is available at
https://github.com/MdRashidunnabi/MTF-CVReID


### [22] [Zero-Shot Multi-Animal Tracking in the Wild](https://arxiv.org/abs/2511.02591)
*Jan Frederik Meier, Timo LÃ¼ddecke*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰åŸºç¡€æ¨¡å‹çš„é›¶æ ·æœ¬å¤šåŠ¨ç‰©è¿½è¸ªæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆGrounding Dinoç›®æ ‡æ£€æµ‹å™¨å’ŒSAM 2è·Ÿè¸ªå™¨ï¼Œå®ç°äº†æ— éœ€é‡æ–°è®­ç»ƒæˆ–è¶…å‚æ•°è°ƒæ•´å³å¯åº”ç”¨äºæ–°æ•°æ®é›†çš„åŠ¨ç‰©è¿½è¸ªã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šåŠ¨ç‰©è¿½è¸ªåœ¨ç†è§£åŠ¨ç‰©ç”Ÿæ€å’Œè¡Œä¸ºä¸­è‡³å…³é‡è¦ï¼Œä½†ç”±äºæ –æ¯åœ°å˜åŒ–ã€è¿åŠ¨æ¨¡å¼å’Œç‰©ç§å¤–è§‚çš„å·®å¼‚ï¼Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦ä¸ºæ¯ä¸ªåº”ç”¨åœºæ™¯è¿›è¡Œå¤§é‡æ¨¡å‹å¾®è°ƒå’Œå¯å‘å¼è®¾è®¡ï¼Œè¿™é™åˆ¶äº†æ–¹æ³•çš„é€šç”¨æ€§å’Œæ•ˆç‡ã€‚

**Method:** è¯¥æ–¹æ³•å°†Grounding Dinoç›®æ ‡æ£€æµ‹å™¨ä¸Segment Anything Model 2è·Ÿè¸ªå™¨ç›¸ç»“åˆï¼Œå¹¶è®¾è®¡äº†ç²¾å¿ƒä¼˜åŒ–çš„å¯å‘å¼ç­–ç•¥ï¼Œæ„å»ºäº†ä¸€ä¸ªæ— éœ€é‡æ–°è®­ç»ƒæˆ–è¶…å‚æ•°é€‚åº”çš„é›¶æ ·æœ¬è¿½è¸ªæ¡†æ¶ã€‚

**Result:** åœ¨ChimpActã€Bird Flock Trackingã€AnimalTrackå’ŒGMOT-40å­é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒç‰©ç§å’Œç¯å¢ƒæ¡ä»¶ä¸‹å‡è¡¨ç°å‡ºå¼ºå¤§ä¸”ä¸€è‡´çš„è¿½è¸ªæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†è§†è§‰åŸºç¡€æ¨¡å‹åœ¨é›¶æ ·æœ¬å¤šåŠ¨ç‰©è¿½è¸ªä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºç”Ÿæ€å­¦å’ŒåŠ¨ç‰©è¡Œä¸ºç ”ç©¶æä¾›äº†ä¸€ç§é€šç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå‡å°‘äº†ä¼ ç»Ÿæ–¹æ³•å¯¹åœºæ™¯ç‰¹å®šè°ƒæ•´çš„ä¾èµ–ã€‚

---

#### ğŸ“„ Abstract
Multi-animal tracking is crucial for understanding animal ecology and
behavior. However, it remains a challenging task due to variations in habitat,
motion patterns, and species appearance. Traditional approaches typically
require extensive model fine-tuning and heuristic design for each application
scenario. In this work, we explore the potential of recent vision foundation
models for zero-shot multi-animal tracking. By combining a Grounding Dino
object detector with the Segment Anything Model 2 (SAM 2) tracker and carefully
designed heuristics, we develop a tracking framework that can be applied to new
datasets without any retraining or hyperparameter adaptation. Evaluations on
ChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstrate
strong and consistent performance across diverse species and environments. The
code is available at https://github.com/ecker-lab/SAM2-Animal-Tracking.


### [23] [Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models](https://arxiv.org/abs/2511.02650)
*Tianfan Peng, Yuntao Du, Pengzhou Ji, Shijie Dong, Kailin Jiang, Mingchuan Ma, Yijun Tian, Jinhe Bi, Qian Li, Wei Du, Feng Xiao, Lizhen Cui*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†UniPruneBenchï¼Œä¸€ä¸ªç”¨äºå¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­è§†è§‰ä»¤ç‰Œå‰ªæçš„ç»Ÿä¸€å¯æ‰©å±•åŸºå‡†ï¼Œé€šè¿‡æ ‡å‡†åŒ–è¯„ä¼°åè®®æ­ç¤ºäº†å‰ªææ–¹æ³•çš„å…³é”®å‘ç°ï¼ŒåŒ…æ‹¬éšæœºå‰ªæä½œä¸ºå¼ºåŸºçº¿ã€æ–¹æ³•æ€§èƒ½ä¸ä¸€è‡´æ€§ä»¥åŠå‰ªææ¯”ä¾‹çš„ä¸»å¯¼ä½œç”¨ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§æ¨¡å‹ç”±äºå›¾åƒç¼–ç å™¨å¼•å…¥çš„å¤§é‡è§†è§‰ä»¤ç‰Œå¯¼è‡´ä¸¥é‡çš„æ¨ç†æ•ˆç‡é—®é¢˜ï¼Œè€Œç°æœ‰çš„ä»¤ç‰Œå‹ç¼©æ–¹æ³•è¯„ä¼°å­˜åœ¨ç¢ç‰‡åŒ–å’Œä¸ä¸€è‡´æ€§ï¼Œç¼ºä¹ç»Ÿä¸€çš„åŸºå‡†æ¥ç³»ç»Ÿæ¯”è¾ƒä¸åŒå‰ªæç®—æ³•çš„æ€§èƒ½ã€‚

**Method:** UniPruneBenchæä¾›äº†è·¨å…­ä¸ªèƒ½åŠ›ç»´åº¦å’Œåä¸ªæ•°æ®é›†çš„æ ‡å‡†åŒ–è¯„ä¼°åè®®ï¼Œæ¶µç›–åç§ä»£è¡¨æ€§å‹ç¼©ç®—æ³•å’Œä¸‰ä¸ªLMMå®¶æ—ï¼ˆLLaVA-v1.5ã€Intern-VL3å’ŒQwen2.5-VLï¼‰ï¼Œé™¤äº†ä»»åŠ¡ç²¾åº¦å¤–è¿˜æ•´åˆäº†è¿è¡Œæ—¶å’Œå‰å¡«å……å»¶è¿Ÿç­‰ç³»ç»Ÿçº§æŒ‡æ ‡ã€‚

**Result:** å®éªŒå‘ç°éšæœºå‰ªææ˜¯ä¸€ä¸ªä»¤äººæƒŠè®¶çš„å¼ºåŸºçº¿ï¼Œæ²¡æœ‰å•ä¸€æ–¹æ³•åœ¨æ‰€æœ‰åœºæ™¯ä¸­æŒç»­ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå‰ªææ•æ„Ÿæ€§åœ¨ä¸åŒä»»åŠ¡é—´å·®å¼‚æ˜¾è‘—ï¼ˆOCRæœ€è„†å¼±ï¼‰ï¼Œä¸”å‰ªææ¯”ä¾‹æ˜¯æ€§èƒ½ä¸‹é™çš„ä¸»å¯¼å› ç´ ã€‚

**Conclusion:** UniPruneBenchä¸ºé«˜æ•ˆå¤šæ¨¡æ€å»ºæ¨¡çš„æœªæ¥ç ”ç©¶æä¾›äº†å¯é åŸºç¡€ï¼Œæ­ç¤ºäº†å‰ªææ–¹æ³•é€‰æ‹©éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡åœºæ™¯è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶å¼ºè°ƒäº†å‰ªææ¯”ä¾‹æ§åˆ¶çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Large multimodal models (LMMs) often suffer from severe inference
inefficiency due to the large number of visual tokens introduced by image
encoders. While recent token compression methods, such as pruning and merging,
have shown promise in reducing redundancy, their evaluation remains fragmented
and inconsistent. In this work, we present UniPruneBench, a unified and
extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench
provides standardized protocols across six ability dimensions and ten datasets,
covering ten representative compression algorithms and three families of LMMs
(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates
system-level metrics such as runtime and prefilling latency to provide a
holistic view. Our experiments uncover several key findings: (1) random pruning
is a surprisingly strong baseline, (2) no single method consistently
outperforms others across scenarios, (3) pruning sensitivity varies
significantly across tasks, with OCR being most vulnerable, and (4) pruning
ratio is the dominant factor governing performance degradation. We believe
UniPruneBench will serve as a reliable foundation for future research on
efficient multimodal modeling.


### [24] [Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.02685)
*Chao Yuan, Zanwu Liu, Guiwei Zhang, Haoxuan Xu, Yujian Zhao, Guanglin Niu, Bo Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨¡æ€è½¬æ¢è¡¨ç¤ºå­¦ä¹ ï¼ˆMTRLï¼‰çš„å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆä¸­é—´å›¾åƒä½œä¸ºæ¨¡æ€è½¬æ¢å™¨ï¼Œæ— éœ€é¢å¤–å‚æ•°å³å¯æœ‰æ•ˆå¯¹é½è·¨æ¨¡æ€ç‰¹å¾ï¼Œåœ¨ä¸‰ä¸ªå…¸å‹æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æœ€ä¼˜æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«æ–¹æ³•ä¸»è¦ä¾èµ–ä¸­é—´è¡¨ç¤ºæ¥å¯¹é½è·¨æ¨¡æ€ç‰¹å¾ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸é€šè¿‡ç”Ÿæˆä¸­é—´å›¾åƒæˆ–èåˆä¸­é—´ç‰¹å¾å®ç°ï¼Œå­˜åœ¨å‚æ•°è¿‡å¤šã€å¯è§£é‡Šæ€§å·®çš„é—®é¢˜ï¼Œä¸”æœªèƒ½å……åˆ†åˆ©ç”¨ä¸­é—´ç‰¹å¾çš„æœ‰æ•ˆä¿¡æ¯ã€‚

**Method:** æå‡ºæ¨¡æ€è½¬æ¢è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç”Ÿæˆä¸­é—´å›¾åƒä½œä¸ºå¯è§å…‰åˆ°çº¢å¤–æ¨¡æ€çš„è½¬æ¢å™¨ï¼Œè¯¥å›¾åƒä¸åŸå§‹å¯è§å…‰å›¾åƒå®Œå…¨å¯¹é½ä¸”ä¸çº¢å¤–æ¨¡æ€ç›¸ä¼¼ï¼›é‡‡ç”¨æ¨¡æ€è½¬æ¢å¯¹æ¯”æŸå¤±å’Œæ¨¡æ€æŸ¥è¯¢æ­£åˆ™åŒ–æŸå¤±è¿›è¡Œè®­ç»ƒï¼Œæœ‰æ•ˆå¯¹é½è·¨æ¨¡æ€ç‰¹å¾ä¸”æ— éœ€é¢å¤–å‚æ•°ã€‚

**Result:** åœ¨ä¸‰ä¸ªå…¸å‹å¯è§å…‰-çº¢å¤–è¡Œäººé‡è¯†åˆ«æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€ææ¨¡å‹æ˜¾è‘—ä¸”æŒç»­åœ°ä¼˜äºç°æœ‰æœ€ä¼˜æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒä¸éª¨å¹²ç½‘ç»œç›¸åŒçš„æ¨ç†é€Ÿåº¦ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¨¡æ€è½¬æ¢è¡¨ç¤ºå­¦ä¹ å¯ä»¥æœ‰æ•ˆè§£å†³å¯è§å…‰-çº¢å¤–æ¨¡æ€é—´çš„ç‰¹å¾å¯¹é½é—®é¢˜ï¼Œæ— éœ€å¢åŠ æ¨¡å‹å¤æ‚åº¦å³å¯æå‡æ€§èƒ½ï¼Œä¸ºè·¨æ¨¡æ€è¡Œäººé‡è¯†åˆ«æä¾›äº†æ–°çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Visible-infrared person re-identification (VI-ReID) technique could associate
the pedestrian images across visible and infrared modalities in the practical
scenarios of background illumination changes. However, a substantial gap
inherently exists between these two modalities. Besides, existing methods
primarily rely on intermediate representations to align cross-modal features of
the same person. The intermediate feature representations are usually create by
generating intermediate images (kind of data enhancement), or fusing
intermediate features (more parameters, lack of interpretability), and they do
not make good use of the intermediate features. Thus, we propose a novel
VI-ReID framework via Modality-Transition Representation Learning (MTRL) with a
middle generated image as a transmitter from visible to infrared modals, which
are fully aligned with the original visible images and similar to the infrared
modality. After that, using a modality-transition contrastive loss and a
modality-query regularization loss for training, which could align the
cross-modal features more effectively. Notably, our proposed framework does not
need any additional parameters, which achieves the same inference speed to the
backbone while improving its performance on VI-ReID task. Extensive
experimental results illustrate that our model significantly and consistently
outperforms existing SOTAs on three typical VI-ReID datasets.


### [25] [Dynamic Reflections: Probing Video Representations with Text Alignment](https://arxiv.org/abs/2511.02767)
*Tyler Zhu, Tengda Han, Leonidas Guibas, Viorica PÄƒtrÄƒucean, Maks Ovsjanikov*

#### ğŸ§© TL;DR
æœ¬æ–‡é¦–æ¬¡å¯¹è§†é¢‘-æ–‡æœ¬è¡¨ç¤ºå¯¹é½è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œæå‡ºäº†å‚æ•°åŒ–æµ‹è¯•æ—¶ç¼©æ”¾å®šå¾‹æ¥æ•æ‰è·¨æ¨¡æ€å¯¹é½è¡Œä¸ºï¼Œå¹¶æ­ç¤ºäº†è¯­ä¹‰å¯¹é½ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä¸ºæ—¶ç©ºæ•°æ®çš„è¡¨ç¤ºèƒ½åŠ›æä¾›äº†é›¶æ ·æœ¬æ¢æµ‹æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å›¾åƒä¸æ–‡æœ¬çš„å¯¹é½ç ”ç©¶å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è§†é¢‘æ•°æ®çš„æ—¶åºç‰¹æ€§åœ¨æ­¤èƒŒæ™¯ä¸‹ä»æœªè¢«å……åˆ†æ¢ç´¢ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è§†é¢‘-æ–‡æœ¬è¡¨ç¤ºå¯¹é½è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œç³»ç»Ÿè¯„ä¼°ç°ä»£è§†é¢‘å’Œè¯­è¨€ç¼–ç å™¨çš„èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶æå‡ºäº†å‚æ•°åŒ–æµ‹è¯•æ—¶ç¼©æ”¾å®šå¾‹æ¥æ•æ‰è·¨æ¨¡æ€å¯¹é½è¡Œä¸ºï¼Œç³»ç»Ÿåˆ†æäº†è§†è§‰æ•°æ®ï¼ˆé™æ€å›¾åƒvså¤šå¸§è§†é¢‘ï¼‰å’Œæ–‡æœ¬æ•°æ®ï¼ˆå•å­—å¹•vsé›†åˆï¼‰ä¸°å¯Œåº¦å¯¹å¯¹é½æ•ˆæœçš„å½±å“ï¼Œå¹¶æ¢è®¨äº†è¯­ä¹‰å¯¹é½ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„å…³è”æ€§ã€‚

**Result:** å®éªŒå‘ç°è·¨æ¨¡æ€å¯¹é½é«˜åº¦ä¾èµ–äºæµ‹è¯•æ—¶æä¾›çš„è§†è§‰å’Œæ–‡æœ¬æ•°æ®ä¸°å¯Œåº¦ï¼Œæå‡ºçš„ç¼©æ”¾å®šå¾‹å±•ç°å‡ºä¸ç»éªŒè§‚å¯Ÿçš„æ˜¾è‘—é¢„æµ‹èƒ½åŠ›ï¼ŒåŒæ—¶æ­ç¤ºäº†å¼ºæ–‡æœ¬ç¼–ç å™¨å¯¹é½å¯èƒ½ä¸é€šç”¨è§†é¢‘è¡¨ç¤ºå’Œç†è§£èƒ½åŠ›ç›¸å…³ã€‚

**Conclusion:** æœ¬ç ”ç©¶ç¡®ç«‹äº†è§†é¢‘-æ–‡æœ¬å¯¹é½ä½œä¸ºæ¢æµ‹æ—¶ç©ºæ•°æ®è¡¨ç¤ºèƒ½åŠ›çš„æœ‰æ•ˆé›¶æ ·æœ¬æ–¹æ³•ï¼Œä¸ºç†è§£è§†é¢‘ç¼–ç å™¨çš„è¡¨ç¤ºèƒ½åŠ›å’Œè·¨æ¨¡æ€å¯¹é½æœºåˆ¶æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ—¶åºæ¨ç†èƒ½åŠ›è¯„ä¼°å»ºç«‹äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•åŸºå‡†ã€‚

---

#### ğŸ“„ Abstract
The alignment of representations from different modalities has recently been
shown to provide insights on the structural similarities and downstream
capabilities of different encoders across diverse data types. While significant
progress has been made in aligning images with text, the temporal nature of
video data remains largely unexplored in this context. In this work, we conduct
the first comprehensive study of video-text representation alignment, probing
the capabilities of modern video and language encoders. Our findings reveal
several key insights. First, we demonstrate that cross-modal alignment highly
depends on the richness of both visual (static images vs. multi-frame videos)
and text (single caption vs. a collection) data provided at test time,
especially when using state-of-the-art video encoders. We propose parametric
test-time scaling laws that capture this behavior and show remarkable
predictive power against empirical observations. Secondly, we investigate the
correlation between semantic alignment and performance on both semantic and
non-semantic downstream tasks, providing initial evidence that strong alignment
against text encoders may be linked to general-purpose video representation and
understanding. Finally, we correlate temporal reasoning with cross-modal
alignment providing a challenging test-bed for vision and language models.
Overall, our work introduces video-text alignment as an informative zero-shot
way to probe the representation power of different encoders for spatio-temporal
data. Project page can be found at https://video-prh.github.io/


### [26] [When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought](https://arxiv.org/abs/2511.02779)
*Yiyang Zhou, Haoqin Tu, Zijun Wang, Zeyu Wang, Niklas Muennighoff, Fan Nie, Yejin Choi, James Zou, Chaorui Deng, Shen Yan, Haoqi Fan, Cihang Xie, Huaxiu Yao, Qinghao Ye*

#### ğŸ§© TL;DR
MIRAæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨éœ€è¦ç”Ÿæˆä¸­é—´è§†è§‰å›¾åƒä»¥è¿›è¡ŒæˆåŠŸæ¨ç†çš„åœºæ™¯ä¸­çš„èƒ½åŠ›ï¼Œå¼ºè°ƒè§†è§‰æ€ç»´åœ¨å¤æ‚æ¨ç†ä¸­çš„å…³é”®ä½œç”¨ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿæ€ç»´é“¾æ–¹æ³•ä»…ä¾èµ–æ–‡æœ¬çš„å±€é™æ€§ï¼Œæ¢ç´¢åœ¨éœ€è¦ç”Ÿæˆå’Œåˆ©ç”¨ä¸­é—´è§†è§‰å›¾åƒï¼ˆå¦‚è‰å›¾ã€ç»“æ„å›¾æˆ–è·¯å¾„å›¾ï¼‰æ¥æŒ‡å¯¼æ¨ç†è¿‡ç¨‹çš„å¤æ‚ä»»åŠ¡ä¸­æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿™ç§è®¾ç½®æ›´è´´è¿‘äººç±»é€šè¿‡"ç»˜å›¾æ€è€ƒ"è§£å†³å¤æ‚é—®é¢˜çš„æ–¹å¼ã€‚

**Method:** MIRAåŸºå‡†åŒ…å«546ä¸ªå¤šæ¨¡æ€é—®é¢˜ï¼Œå¹¶æ ‡æ³¨äº†ä¸­é—´è§†è§‰å›¾åƒå’Œæœ€ç»ˆç­”æ¡ˆï¼›æå‡ºäº†ç»Ÿä¸€çš„è¯„ä¼°åè®®ï¼Œæ¶µç›–ä¸‰ä¸ªè¯„ä¼°è¾“å…¥çº§åˆ«ï¼šä»…å›¾åƒå’Œé—®é¢˜çš„ç›´æ¥è¾“å…¥ã€å¸¦å›¾åƒå’Œæ€ç»´æç¤ºçš„çº¯æ–‡æœ¬CoTè¾“å…¥ï¼Œä»¥åŠå¸¦æ³¨é‡Šå›¾åƒçº¿ç´¢å’Œæ–‡æœ¬æ€ç»´æç¤ºçš„Visual-CoTè¾“å…¥ï¼›è¿˜æŠ¥å‘Šäº†ä¸åŒkè®¾ç½®ä¸‹çš„pass@kå’Œå¤šæ•°æŠ•ç¥¨å‡†ç¡®ç‡ä»¥æ¢æµ‹æ¨¡å‹èƒ½åŠ›ä¸Šé™ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä»…ä¾èµ–æ–‡æœ¬æç¤ºæ—¶è¡¨ç°è¾ƒå·®ï¼Œä½†å½“æä¾›ä¸­é—´è§†è§‰çº¿ç´¢æ—¶ï¼Œæ¨¡å‹æ€§èƒ½ä¸€è‡´æå‡ï¼Œåœ¨æ‰€æœ‰æ¨¡å‹å’Œä»»åŠ¡ä¸­å¹³å‡ç›¸å¯¹å¢ç›Šè¾¾åˆ°33.7%ï¼›é€šè¿‡æ‰©å¤§æœç´¢ç©ºé—´å’Œè®¾è®¡ä¸Visual-CoTå¯¹é½çš„æ–‡æœ¬æç¤ºæ¥æ¢æµ‹ä¸Šé™ï¼Œä½†ç›¸æ¯”Visual-CoTè®¾ç½®ä»…è·å¾—æœ‰é™æ”¹è¿›ã€‚

**Conclusion:** è¿™äº›ç»“æœå¼ºè°ƒäº†æƒ³è±¡è§†è§‰ä¿¡æ¯åœ¨MIRAä¸Šå®ç°æˆåŠŸæ¨ç†çš„å…³é”®ä½œç”¨ï¼Œè¡¨æ˜è§†è§‰æ€ç»´å¯¹äºå¤„ç†æ¶‰åŠå¤æ‚ç»“æ„ã€ç©ºé—´å…³ç³»æˆ–éš¾ä»¥ä»…é€šè¿‡è¯­è¨€è¡¨è¾¾çš„æ¨ç†æ­¥éª¤çš„ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘å’ŒåŸºå‡†ã€‚

---

#### ğŸ“„ Abstract
We propose MIRA, a new benchmark designed to evaluate models in scenarios
where generating intermediate visual images is essential for successful
reasoning. Unlike traditional CoT methods that rely solely on text, tasks in
MIRA require models to generate and utilize intermediate images - such as
sketches, structural diagrams, or path drawings - to guide their reasoning
process. This setup closely mirrors how humans solve complex problems through
"drawing to think". To solve this, MIRA focuses on tasks that are intrinsically
challenging and involve complex structures, spatial relationships, or reasoning
steps that are difficult to express through language alone. To ensure that our
evaluation data is of high-quality, we include 546 multimodal problems,
annotated with intermediate visual images and final answers. We also propose a
unified evaluation protocol for MIRA that spans three levels of evaluation
input: direct input with image and question only, text-only CoT input with
image and thinking prompts, and Visual-CoT input with both annotated image
clues and textual thinking prompts. To probe the upper bound of model capacity
on our benchmark, we also report pass@k and majority voting accuracies under
different k settings. Experimental results show that existing multimodal large
language models, including strongest private models as well as strong
open-weight models, perform poorly when relying solely on textual prompts.
However, when intermediate visual cues are provided, model performance improves
consistently, yielding an average relative gain of 33.7% across all models and
tasks. We also probe the upper bound by expanding the search space and
designing textual prompts aligned with Visual-CoT, but both yield only limited
improvements compared to our Visual-CoT setting. These results underscore the
critical role of imagined visual information in enabling successful reasoning
on MIRA.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [27] [Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation](https://arxiv.org/abs/2511.02358)
*Wongyu Kim, Hochang Lee, Sanghak Lee, Yoonsung Kim, Jaehyun Park*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†M-Solomonï¼Œä¸€ç§é€šç”¨å¤šæ¨¡æ€åµŒå…¥å™¨ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°å†³å®šä½•æ—¶è¿›è¡ŒæŸ¥è¯¢å¢å¼ºï¼Œè§£å†³äº†ç°æœ‰LLMåµŒå…¥å™¨ä¸­ç›²ç›®å¢å¼ºæ‰€æœ‰æŸ¥è¯¢å¯¼è‡´çš„å»¶è¿Ÿé—®é¢˜å’Œæ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œå¹¶åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºLLMçš„åµŒå…¥å™¨å¯¹æ‰€æœ‰æŸ¥è¯¢è¿›è¡Œå¢å¼ºä¼šå¯¼è‡´æ˜¾è‘—çš„åµŒå…¥å»¶è¿Ÿï¼Œä¸”æŸäº›æŸ¥è¯¢çš„å¢å¼ºåè€Œä¼šæŸå®³æ€§èƒ½ï¼ŒåŒæ—¶å…ˆå‰æ–¹æ³•æœªåœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­è¿›è¡Œæ¢ç´¢ï¼Œè¿™äº›å±€é™æ€§ä¿ƒä½¿æœ¬ç ”ç©¶å¼€å‘èƒ½å¤Ÿè‡ªé€‚åº”å†³å®šå¢å¼ºæ—¶æœºçš„å¤šæ¨¡æ€åµŒå…¥æ–¹æ³•ã€‚

**Method:** è¯¥æ–¹æ³•é¦–å…ˆåœ¨æ•°æ®é›†å±‚é¢å°†è®­ç»ƒæŸ¥è¯¢åˆ†ä¸ºéœ€è¦å¢å¼ºå’Œä¸éœ€è¦å¢å¼ºä¸¤ç»„ï¼Œåˆ©ç”¨å¼ºå¤§çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸ºéœ€è¦å¢å¼ºçš„æŸ¥è¯¢ç”Ÿæˆé€‚å½“çš„å¢å¼ºå†…å®¹ï¼Œç„¶åé€šè¿‡è‡ªé€‚åº”æŸ¥è¯¢å¢å¼ºæœºåˆ¶ï¼Œä»…å¯¹éœ€è¦å¢å¼ºçš„æŸ¥è¯¢ç”Ÿæˆå¸¦æœ‰/augmentå‰ç¼€çš„åˆæˆå¢å¼ºï¼Œå¯¹å…¶ä»–æŸ¥è¯¢ç”Ÿæˆç®€å•çš„/embedå­—ç¬¦ä¸²ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒM-Solomonä¸ä»…å¤§å¹…è¶…è¶Šäº†æ— å¢å¼ºçš„åŸºçº¿æ–¹æ³•ï¼Œä¹Ÿä¼˜äºå§‹ç»ˆä½¿ç”¨å¢å¼ºçš„åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶æä¾›äº†æ›´å¿«çš„åµŒå…¥å»¶è¿Ÿï¼Œåœ¨å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜è‡ªé€‚åº”æŸ¥è¯¢å¢å¼ºç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡æ£€ç´¢æ€§èƒ½ä¸è®¡ç®—æ•ˆç‡ï¼Œä¸ºå¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿæä¾›äº†æ›´æ™ºèƒ½çš„æŸ¥è¯¢å¤„ç†æ–¹æ¡ˆï¼Œè¯æ˜äº†é€‰æ‹©æ€§å¢å¼ºæ¯”ç›²ç›®å¢å¼ºæ‰€æœ‰æŸ¥è¯¢æ›´å…·ä¼˜åŠ¿ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€åµŒå…¥æ–¹æ³•çš„è®¾è®¡æä¾›äº†é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
Query augmentation makes queries more meaningful by appending further
information to the queries to find relevant documents. Current studies have
proposed Large Language Model (LLM)-based embedders, which learn representation
for embedding and generation for query augmentation in a multi-task manner by
leveraging the generative capabilities of LLM. During inference, these jointly
trained embedders have conducted query augmentation followed by embedding,
showing effective results. However, augmenting every query leads to substantial
embedding latency and query augmentation can be detrimental to performance for
some queries. Also, previous methods have not been explored in multimodal
environments. To tackle these problems, we propose M-Solomon, a universal
multimodal embedder that can adaptively determine when to augment queries. Our
approach first divides the queries of the training datasets into two groups at
the dataset level. One includes queries that require augmentation and the other
includes queries that do not. Then, we introduces a synthesis process that
generates appropriate augmentations for queries that require them by leveraging
a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.
Through this step, M-Solomon can conduct query augmentation only when necessary
by learning to generate synthetic augmentations with the prefix /augment for
queries that demand them and to generate the simple string /embed for others.
Experimental results showed that M-Solomon not only surpassed the baseline
without augmentation by a large margin but also outperformed the baseline that
always used augmentation, providing much faster embedding latency.


### [28] [LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context](https://arxiv.org/abs/2511.02366)
*Yudong Li, Zhongliang Yang, Kejiang Chen, Wenxuan Wang, Tianxin Zhang, Sifang Wan, Kecheng Wang, Haitian Li, Xu Wang, Lefan Cheng, Youdan Yang, Baocheng Chen, Ziyu Liu, Yufei Sun, Liyan Wu, Wenya Wen, Xingchi Gu, Peiru Yang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†LiveSecBenchï¼Œä¸€ä¸ªé’ˆå¯¹ä¸­æ–‡LLMåº”ç”¨åœºæ™¯çš„åŠ¨æ€æŒç»­æ›´æ–°å®‰å…¨åŸºå‡†ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å…­ä¸ªå…³é”®å®‰å…¨ç»´åº¦ä¸Šçš„è¡¨ç°ï¼Œå¹¶å·²å¯¹18ä¸ªæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç¼ºä¹ä¸“é—¨é’ˆå¯¹ä¸­æ–‡è¯­è¨€ç¯å¢ƒå’Œå¤§è¯­è¨€æ¨¡å‹åº”ç”¨åœºæ™¯çš„åŠ¨æ€å®‰å…¨è¯„ä¼°åŸºå‡†ï¼Œéœ€è¦å»ºç«‹ä¸€ä¸ªèƒ½å¤ŸæŒç»­æ›´æ–°ã€åæ˜ ä¸­æ–‡æ³•å¾‹å’Œç¤¾ä¼šæ¡†æ¶çš„å®‰å…¨è¯„ä¼°ä½“ç³»ã€‚

**Method:** æå‡ºäº†LiveSecBenchåŸºå‡†ï¼ŒåŒ…å«åˆæ³•æ€§ã€ä¼¦ç†ã€äº‹å®æ€§ã€éšç§ã€å¯¹æŠ—é²æ£’æ€§å’Œæ¨ç†å®‰å…¨å…­ä¸ªå…³é”®ç»´åº¦ï¼Œé‡‡ç”¨åŠ¨æ€æ›´æ–°æœºåˆ¶çº³å…¥æ–°çš„å¨èƒå‘é‡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå®‰å…¨å’Œæ™ºèƒ½ä½“å®‰å…¨ã€‚

**Result:** LiveSecBenchï¼ˆv251030ï¼‰å·²è¯„ä¼°äº†18ä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼Œæä¾›äº†ä¸­æ–‡è¯­å¢ƒä¸‹AIå®‰å…¨çš„å…¨æ™¯å›¾ï¼ŒåŸºå‡†æ’è¡Œæ¦œå·²å…¬å¼€å¯è®¿é—®ã€‚

**Conclusion:** è¯¥åŸºå‡†ä¸ºä¸­æ–‡LLMå®‰å…¨è¯„ä¼°æä¾›äº†æ ‡å‡†åŒ–æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€æ›´æ–°æœºåˆ¶ç¡®ä¿è¯„ä¼°çš„æ—¶æ•ˆæ€§ï¼Œä¸ºAIå®‰å…¨ç ”ç©¶å’Œæ¨¡å‹å¼€å‘æä¾›äº†é‡è¦å‚è€ƒä¾æ®ã€‚

---

#### ğŸ“„ Abstract
In this work, we propose LiveSecBench, a dynamic and continuously updated
safety benchmark specifically for Chinese-language LLM application scenarios.
LiveSecBench evaluates models across six critical dimensions (Legality, Ethics,
Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in
the Chinese legal and social frameworks. This benchmark maintains relevance
through a dynamic update schedule that incorporates new threat vectors, such as
the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in
the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,
providing a landscape of AI safety in the context of Chinese language. The
leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.


### [29] [Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval](https://arxiv.org/abs/2511.02770)
*Hung-Ting Chen, Xiang Liu, Shauli Ravfogel, Eunsol Choi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªå›å½’å¤šåµŒå…¥æ£€ç´¢å™¨ï¼ˆAMERï¼‰ï¼Œé€šè¿‡ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å‘é‡æ¥æ•è·ç›¸å…³æ–‡æ¡£çš„å¤šæ¨¡æ€åˆ†å¸ƒï¼Œç›¸æ¯”ä¼ ç»Ÿå•å‘é‡æ£€ç´¢å™¨åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–‡æœ¬æ£€ç´¢å™¨é€šå¸¸åªç”Ÿæˆå•ä¸ªæŸ¥è¯¢å‘é‡æ¥æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œä½†ç›¸å…³æ–‡æ¡£çš„æ¡ä»¶åˆ†å¸ƒå¯èƒ½æ˜¯å¤šæ¨¡æ€çš„ï¼Œä»£è¡¨æŸ¥è¯¢çš„ä¸åŒè§£é‡Šã€‚ç ”ç©¶å‘ç°ç°æœ‰æ£€ç´¢å™¨åœ¨ç›®æ ‡æ–‡æ¡£åµŒå…¥è·ç¦»è¾ƒå¤§æ—¶è¡¨ç°è¾ƒå·®ï¼Œè¿™é™åˆ¶äº†å…¶å¤„ç†å¤æ‚æŸ¥è¯¢çš„èƒ½åŠ›ã€‚

**Method:** æå‡ºäº†ä¸€ç§æ–°çš„æ£€ç´¢å™¨æ¶æ„â€”â€”è‡ªå›å½’å¤šåµŒå…¥æ£€ç´¢å™¨ï¼ˆAMERï¼‰ï¼Œè¯¥æ¨¡å‹è‡ªå›å½’åœ°ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å‘é‡ï¼Œæ‰€æœ‰é¢„æµ‹çš„æŸ¥è¯¢å‘é‡éƒ½ç”¨äºä»è¯­æ–™åº“ä¸­æ£€ç´¢æ–‡æ¡£ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•è·å¤šæ¨¡æ€çš„ç›®æ ‡åˆ†å¸ƒã€‚

**Result:** åœ¨åˆæˆå‘é‡åŒ–æ•°æ®ä¸Šï¼Œæ‰€ææ–¹æ³•èƒ½å¤Ÿå®Œç¾æ•è·å¤šä¸ªç›®æ ‡åˆ†å¸ƒï¼Œæ€§èƒ½æ¯”å•åµŒå…¥æ¨¡å‹æå‡4å€ã€‚åœ¨çœŸå®ä¸–ç•Œå¤šç­”æ¡ˆæ£€ç´¢æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒAMERåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šåˆ†åˆ«æ¯”å•åµŒå…¥åŸºçº¿ç›¸å¯¹æå‡4%å’Œ21%ã€‚åœ¨ç›®æ ‡æ–‡æ¡£åµŒå…¥ç›¸ä¼¼åº¦è¾ƒä½çš„æ•°æ®å­é›†ä¸Šï¼Œæ€§èƒ½æå‡æ›´ä¸ºæ˜¾è‘—ã€‚

**Conclusion:** ç ”ç©¶è¯æ˜äº†ä½¿ç”¨å¤šæŸ¥è¯¢å‘é‡æ£€ç´¢å™¨çš„æ½œåŠ›ï¼Œä¸ºè§£å†³å¤æ‚æŸ¥è¯¢çš„å¤šæ¨¡æ€åˆ†å¸ƒé—®é¢˜å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚è¯¥æ–¹æ³•åœ¨å¤„ç†ç›®æ ‡æ–‡æ¡£åµŒå…¥å·®å¼‚è¾ƒå¤§çš„åœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œä¸ºæœªæ¥æ£€ç´¢ç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
Most text retrievers generate \emph{one} query vector to retrieve relevant
documents. Yet, the conditional distribution of relevant documents for the
query may be multimodal, e.g., representing different interpretations of the
query. We first quantify the limitations of existing retrievers. All retrievers
we evaluate struggle more as the distance between target document embeddings
grows. To address this limitation, we develop a new retriever architecture,
\emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER).
Our model autoregressively generates multiple query vectors, and all the
predicted query vectors are used to retrieve documents from the corpus. We show
that on the synthetic vectorized data, the proposed method could capture
multiple target distributions perfectly, showing 4x better performance than
single embedding model. We also fine-tune our model on real-world multi-answer
retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative
gains over single-embedding baselines on two datasets we evaluate on.
Furthermore, we consistently observe larger gains on the subset of dataset
where the embeddings of the target documents are less similar to each other. We
demonstrate the potential of using a multi-query vector retriever and open up a
new direction for future work.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [30] [Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing](https://arxiv.org/abs/2511.02071)
*Xinyi Lin, Yuyang Zhang, Yuanhang Gan, Juntao Chen, Hao Shen, Yichun He, Lijun Li, Ze Yuan, Shuang Wang, Chaohao Wang, Rui Zhang, Na Li, Jia Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§äººæœºååŒå…·èº«æ™ºèƒ½æ–°èŒƒå¼ï¼Œå°†äººç±»ç”¨æˆ·ã€æ™ºèƒ½ä½“AIå’Œå¯ç©¿æˆ´ç¡¬ä»¶é›†æˆåˆ°ç»Ÿä¸€ç³»ç»Ÿä¸­ï¼Œç”¨äºç°å®ä¸–ç•Œçš„å®éªŒå’Œæ™ºèƒ½åˆ¶é€ ã€‚é€šè¿‡APEXç³»ç»Ÿå±•ç¤ºäº†æ™ºèƒ½ä½“æ¨ç†ä¸ç‰©ç†æ‰§è¡Œçš„ç»“åˆï¼Œå®ç°äº†è¶…è¶Šé€šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç§‘å­¦å®éªŒå’Œåˆ¶é€ ä¾èµ–å¤æ‚å¤šæ­¥éª¤ç¨‹åºï¼Œéœ€è¦æŒç»­çš„äººç±»ä¸“ä¸šçŸ¥è¯†è¿›è¡Œç²¾ç¡®æ‰§è¡Œå’Œå†³ç­–ã€‚å°½ç®¡æœºå™¨å­¦ä¹ å’Œè‡ªåŠ¨åŒ–æŠ€æœ¯å–å¾—è¿›å±•ï¼Œä¼ ç»Ÿæ¨¡å‹ä»å±€é™äºè™šæ‹Ÿé¢†åŸŸï¼Œè€Œç°å®ä¸–ç•Œçš„å®éªŒå’Œåˆ¶é€ ä»ä¾èµ–äººç±»ç›‘ç£å’Œä¸“ä¸šçŸ¥è¯†ã€‚æœºå™¨æ™ºèƒ½ä¸ç‰©ç†æ‰§è¡Œä¹‹é—´çš„å·®è·é™åˆ¶äº†ç§‘å­¦å’Œåˆ¶é€ å·¥ä½œæµç¨‹çš„å¯é‡å¤æ€§ã€å¯æ‰©å±•æ€§å’Œå¯è®¿é—®æ€§ã€‚

**Method:** æå‡ºäººæœºååŒå…·èº«æ™ºèƒ½èŒƒå¼ï¼Œå°†äººç±»ç”¨æˆ·ã€æ™ºèƒ½ä½“AIå’Œå¯ç©¿æˆ´ç¡¬ä»¶é›†æˆåˆ°ç»Ÿä¸€ç³»ç»Ÿä¸­ã€‚å¼€å‘äº†APEXç³»ç»Ÿï¼Œé€šè¿‡æ··åˆç°å®æŠ€æœ¯å°†æ™ºèƒ½ä½“æ¨ç†ä¸ç‰©ç†æ‰§è¡Œç›¸ç»“åˆï¼Œç³»ç»Ÿèƒ½å¤Ÿè§‚å¯Ÿå’Œè§£é‡Šäººç±»åŠ¨ä½œï¼Œä¸æ ‡å‡†æ“ä½œç¨‹åºå¯¹é½ï¼Œæä¾›3Dè§†è§‰æŒ‡å¯¼ï¼Œå¹¶åˆ†ææ¯ä¸ªæ­¥éª¤ã€‚

**Result:** åœ¨æ´å‡€å®¤æŸ”æ€§ç”µå­åˆ¶é€ ç¯å¢ƒä¸­å®ç°çš„APEXç³»ç»Ÿï¼Œå…¶ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†å‡†ç¡®ç‡è¶…è¿‡é€šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå®æ—¶çº æ­£é”™è¯¯ï¼Œå¹¶å°†ä¸“ä¸šçŸ¥è¯†ä¼ é€’ç»™åˆå­¦è€…ã€‚ç³»ç»Ÿå®ç°äº†è‡ªä¸»ã€å¯è¿½æº¯ã€å¯è§£é‡Šå’Œå¯æ‰©å±•çš„å®éªŒå’Œåˆ¶é€ æµç¨‹ã€‚

**Conclusion:** è¿™é¡¹ç ”ç©¶å»ºç«‹äº†ä¸€ç±»æ–°çš„æ™ºèƒ½ä½“-ç‰©ç†-äººç±»æ™ºèƒ½ç³»ç»Ÿï¼Œå°†æ™ºèƒ½ä½“æ¨ç†ä»è®¡ç®—é¢†åŸŸæ‰©å±•åˆ°ç‰©ç†é¢†åŸŸã€‚è¯¥èŒƒå¼å°†ç§‘å­¦ç ”ç©¶å’Œåˆ¶é€ è½¬å˜ä¸ºè‡ªä¸»ã€å¯è¿½æº¯ã€å¯è§£é‡Šå’Œå¯æ‰©å±•çš„è¿‡ç¨‹ï¼Œä¸ºäººæœºååŒæ™ºèƒ½åœ¨ç‰©ç†ä¸–ç•Œçš„åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Scientific experiment and manufacture rely on complex, multi-step procedures
that demand continuous human expertise for precise execution and
decision-making. Despite advances in machine learning and automation,
conventional models remain confined to virtual domains, while real-world
experiment and manufacture still rely on human supervision and expertise. This
gap between machine intelligence and physical execution limits reproducibility,
scalability, and accessibility across scientific and manufacture workflows.
Here, we introduce human-AI co-embodied intelligence, a new form of physical AI
that unites human users, agentic AI, and wearable hardware into an integrated
system for real-world experiment and intelligent manufacture. In this paradigm,
humans provide precise execution and control, while agentic AI contributes
memory, contextual reasoning, adaptive planning, and real-time feedback. The
wearable interface continuously captures the experimental and manufacture
processes, facilitates seamless communication between humans and AI for
corrective guidance and interpretable collaboration. As a demonstration, we
present Agentic-Physical Experimentation (APEX) system, coupling agentic
reasoning with physical execution through mixed-reality. APEX observes and
interprets human actions, aligns them with standard operating procedures,
provides 3D visual guidance, and analyzes every step. Implemented in a
cleanroom for flexible electronics fabrication, APEX system achieves
context-aware reasoning with accuracy exceeding general multimodal large
language models, corrects errors in real time, and transfers expertise to
beginners. These results establish a new class of agentic-physical-human
intelligence that extends agentic reasoning beyond computation into the
physical domain, transforming scientific research and manufacturing into
autonomous, traceable, interpretable, and scalable processes.


### [31] [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)
*Zhuoran Zhang, Tengyue Wang, Xilin Gong, Yang Shi, Haotian Wang, Di Wang, Lijie Hu*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ†è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­æ¨¡æ€è·Ÿéšè¡Œä¸ºçš„æ–°æ¡†æ¶ï¼Œæ­ç¤ºäº†ç›¸å¯¹æ¨ç†ä¸ç¡®å®šæ€§å’Œå†…åœ¨æ¨¡æ€åå¥½è¿™ä¸¤ä¸ªåŸºæœ¬å› ç´ å¦‚ä½•å…±åŒå†³å®šæ¨¡å‹åœ¨å†²çªä¿¡æ¯ä¸‹çš„å†³ç­–è¿‡ç¨‹ã€‚é€šè¿‡æ„å»ºå¯æ§æ•°æ®é›†å’Œå¼•å…¥ç†µä½œä¸ºç»†ç²’åº¦ä¸ç¡®å®šæ€§åº¦é‡ï¼Œå‘ç°äº†æ¨¡æ€è·Ÿéšæ¦‚ç‡éšç›¸å¯¹ä¸ç¡®å®šæ€§å•è°ƒä¸‹é™çš„æ™®éè§„å¾‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ç ”ç©¶ä»…ä½¿ç”¨ç²—ç²’åº¦çš„æ•°æ®é›†çº§ç»Ÿè®¡æ¥æµ‹é‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¨¡æ€è·Ÿéšè¡Œä¸ºï¼Œå¿½ç•¥äº†æ¨¡å‹åœ¨å•æ¨¡æ€æ¨ç†ä¸­çš„ç½®ä¿¡åº¦å½±å“ã€‚è¿™ç§ç®€åŒ–æ–¹æ³•æ— æ³•æ­ç¤ºæ¨¡æ€å†²çªè§£å†³çš„å†…åœ¨æœºåˆ¶ï¼Œç‰¹åˆ«æ˜¯å½“ä¸åŒæ¨¡æ€æä¾›çŸ›ç›¾ä¿¡æ¯æ—¶æ¨¡å‹å†³ç­–çš„æ·±å±‚åŸç†ã€‚

**Method:** æœ¬ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªå¯æ§æ•°æ®é›†ï¼Œç³»ç»Ÿæ€§åœ°æ”¹å˜è§†è§‰å’Œæ–‡æœ¬è¾“å…¥çš„æ¨ç†éš¾åº¦ï¼Œå¹¶ä½¿ç”¨ç†µä½œä¸ºç»†ç²’åº¦çš„ä¸ç¡®å®šæ€§åº¦é‡æŒ‡æ ‡ã€‚é€šè¿‡åˆ†æå±‚é—´é¢„æµ‹ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å¹³è¡¡ç‚¹é™„è¿‘åŒºåŸŸå†…çš„å†…éƒ¨æŒ¯è¡æœºåˆ¶ã€‚

**Result:** å®éªŒå‘ç°äº†ä¸€ä¸ªæ™®éè§„å¾‹ï¼šæ¨¡æ€è·Ÿéšæ¦‚ç‡éšå…¶ç›¸å¯¹ä¸ç¡®å®šæ€§çš„å¢åŠ è€Œå•è°ƒä¸‹é™ã€‚åœ¨å¹³è¡¡ç‚¹å¤„ï¼Œæ¨¡å‹å€¾å‘äºä»¥å¯æ¯”è¾ƒçš„æ¦‚ç‡è·Ÿéšä¸¤ç§æ¨¡æ€ï¼Œè¿™ä¸ºå†…åœ¨æ¨¡æ€åå¥½æä¾›äº†å®ç”¨æŒ‡æ ‡ã€‚å±‚é—´åˆ†ææ˜¾ç¤ºï¼Œåœ¨å¹³è¡¡ç‚¹é™„è¿‘çš„æ¨¡ç³ŠåŒºåŸŸï¼Œæ¨¡å‹ä¼šåœ¨ä¸åŒå±‚é—´åœ¨æ¨¡æ€ä¹‹é—´æŒ¯è¡ã€‚

**Conclusion:** ç›¸å¯¹ä¸ç¡®å®šæ€§å’Œå†…åœ¨åå¥½æ˜¯æ¨¡æ€è·Ÿéšçš„ä¸¤ä¸ªä¸»å¯¼åŸåˆ™ï¼Œä¸ºç†è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¦‚ä½•è§£å†³å†²çªä¿¡æ¯æä¾›äº†å®šé‡æ¡†æ¶å’Œæœºåˆ¶æ€§æ´è§ã€‚è¿™ä¸€å‘ç°æä¾›äº†æ¯”ä¼ ç»Ÿå®è§‚æ¯”ç‡æ›´åŸåˆ™åŒ–ã€æ›´å°‘æ··æ·†çš„æ–¹å¼æ¥è¡¨å¾æ¨¡æ€åå·®ï¼Œå°†å…¶ä¸å•æ¨¡æ€èƒ½åŠ›å’Œæ•°æ®é›†ä¼ªå½±åˆ†ç¦»å¼€æ¥ã€‚

---

#### ğŸ“„ Abstract
Multimodal large language models (MLLMs) must resolve conflicts when
different modalities provide contradictory information, a process we term
modality following. Prior work measured this behavior only with coarse
dataset-level statistics, overlooking the influence of model's confidence in
unimodal reasoning. In this paper, we introduce a new framework that decomposes
modality following into two fundamental factors: relative reasoning uncertainty
(the case-specific confidence gap between unimodal predictions) and inherent
modality preference( a model's stable bias when uncertainties are balanced). To
validate this framework, we construct a controllable dataset that
systematically varies the reasoning difficulty of visual and textual inputs.
Using entropy as a fine-grained uncertainty metric, we uncover a universal law:
the probability of following a modality decreases monotonically as its relative
uncertainty increases. At the relative difficulty level where the model tends
to follow both modalities with comparable probability what we call the balance
point, a practical indicator of the model's inherent preference. Unlike
traditional macro-level ratios, this measure offers a more principled and less
confounded way to characterize modality bias, disentangling it from unimodal
capabilities and dataset artifacts. Further, by probing layer-wise predictions,
we reveal the internal mechanism of oscillation: in ambiguous regions near the
balance point, models vacillate between modalities across layers, explaining
externally observed indecision. Together, these findings establish relative
uncertainty and inherent preference as the two governing principles of modality
following, offering both a quantitative framework and mechanistic insight into
how MLLMs resolve conflicting information.


### [32] [Chronic Kidney Disease Prognosis Prediction Using Transformer](https://arxiv.org/abs/2511.02340)
*Yohan Lee, DongGyun Kang, SeHoon Park, Sa-Yoon Park, Kwangsoo Kim*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„æ¡†æ¶ProQ-BERTï¼Œç”¨äºé¢„æµ‹æ…¢æ€§è‚¾è„ç—…çš„è¿›å±•ï¼Œé€šè¿‡æ•´åˆå¤šæ¨¡æ€ç”µå­å¥åº·è®°å½•æ•°æ®ï¼Œåœ¨çŸ­æœŸé¢„æµ‹ä¸­å®ç°äº†é«˜è¾¾0.995çš„ROC-AUCæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ…¢æ€§è‚¾è„ç—…å½±å“å…¨çƒè¿‘10%çš„äººå£ï¼Œå¸¸è¿›å±•è‡³ç»ˆæœ«æœŸè‚¾è¡°ç«­ï¼Œå‡†ç¡®çš„é¢„åé¢„æµ‹å¯¹äºåŠæ—¶å¹²é¢„å’Œèµ„æºä¼˜åŒ–è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨å¤šæ¨¡æ€ç”µå­å¥åº·è®°å½•æ•°æ®è¿›è¡Œç²¾ç¡®é¢„æµ‹æ–¹é¢å­˜åœ¨å±€é™ã€‚

**Method:** æå‡ºäº†ProQ-BERTæ¡†æ¶ï¼ŒåŸºäºTransformeræ¶æ„æ•´åˆäººå£ç»Ÿè®¡å­¦ã€ä¸´åºŠå’Œå®éªŒå®¤æ•°æ®ï¼Œé‡‡ç”¨åŸºäºé‡åŒ–çš„æ ‡è®°åŒ–æ–¹æ³•å¤„ç†è¿ç»­å®éªŒå®¤æ•°å€¼ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºå¯è§£é‡Šæ€§ã€‚æ¨¡å‹é€šè¿‡æ©ç è¯­è¨€å»ºæ¨¡è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é’ˆå¯¹ä»3aæœŸåˆ°5æœŸçš„äºŒå…ƒåˆ†ç±»ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚

**Result:** åœ¨91,816åæ‚£è€…é˜Ÿåˆ—ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹æŒç»­ä¼˜äºCEHR-BERTï¼Œåœ¨çŸ­æœŸé¢„æµ‹ä¸­ROC-AUCé«˜è¾¾0.995ï¼ŒPR-AUCé«˜è¾¾0.989ï¼Œå±•ç°äº†å“è¶Šçš„é¢„æµ‹æ€§èƒ½ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜Transformeræ¶æ„å’Œæ—¶é—´è®¾è®¡é€‰æ‹©åœ¨ä¸´åºŠé¢„åå»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¸ªæ€§åŒ–æ…¢æ€§è‚¾è„ç—…æŠ¤ç†æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†å¤šæ¨¡æ€æ•°æ®æ•´åˆå’Œæ³¨æ„åŠ›æœºåˆ¶åœ¨åŒ»ç–—é¢„æµ‹ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and
often progresses to end-stage renal failure. Accurate prognosis prediction is
vital for timely interventions and resource optimization. We present a
transformer-based framework for predicting CKD progression using multi-modal
electronic health records (EHR) from the Seoul National University Hospital
OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates
demographic, clinical, and laboratory data, employing quantization-based
tokenization for continuous lab values and attention mechanisms for
interpretability. The model was pretrained with masked language modeling and
fine-tuned for binary classification tasks predicting progression from stage 3a
to stage 5 across varying follow-up and assessment periods. Evaluated on a
cohort of 91,816 patients, our model consistently outperformed CEHR-BERT,
achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.
These results highlight the effectiveness of transformer architectures and
temporal design choices in clinical prognosis modeling, offering a promising
direction for personalized CKD care.


### [33] [Agentic AI for Mobile Network RAN Management and Optimization](https://arxiv.org/abs/2511.02532)
*Jorge Pellejero, Luis A. HernÃ¡ndez GÃ³mez, Luis Mendo TomÃ¡s, Zoraida Frias Barroso*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé¢å‘5Gå’Œ6Gç½‘ç»œçš„Agentic AIæ¡†æ¶ï¼Œé€šè¿‡å°†å¤§å‹AIæ¨¡å‹ä¸æ ¸å¿ƒè®¾è®¡æ¨¡å¼ç›¸ç»“åˆï¼Œå®ç°æ— çº¿æ¥å…¥ç½‘ç»œçš„è‡ªä¸»ä¼˜åŒ–å†³ç­–ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç¼ºä¹ç³»ç»Ÿæ€§æ¡†æ¶çš„ç©ºç™½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** 5GåŠæœªæ¥6Gç½‘ç»œçš„å¤æ‚æ€§ä½¿å¾—äººå·¥ä¼˜åŒ–å˜å¾—ä½æ•ˆï¼Œè€ŒAgentic AIè™½ç„¶å¿«é€Ÿå‘å±•ï¼Œä½†ç¼ºä¹ç»Ÿä¸€çš„å®šä¹‰å’Œç³»ç»Ÿæ€§æ¡†æ¶æ¥æŒ‡å¯¼å…¶åœ¨åŠ¨æ€æ— çº¿æ¥å…¥ç½‘ç»œç¯å¢ƒä¸­çš„å†³ç­–è‡ªåŠ¨åŒ–åº”ç”¨ã€‚

**Method:** æå‡ºäº†åŸºäºå¤§å‹AIæ¨¡å‹çš„Agentic AIç³»ç»Ÿï¼Œé‡‡ç”¨åæ€ã€è§„åˆ’ã€å·¥å…·ä½¿ç”¨å’Œå¤šæ™ºèƒ½ä½“åä½œç­‰æ ¸å¿ƒè®¾è®¡æ¨¡å¼ï¼Œç»“åˆæ—¶é—´åºåˆ—åˆ†æå’ŒKPIé©±åŠ¨çš„è‡ªä¸»å†³ç­–æœºåˆ¶æ¥åè°ƒæ™ºèƒ½è¡Œä¸ºã€‚

**Result:** é€šè¿‡ä¸€ä¸ªå®é™…çš„5G RANæ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†æ—¶é—´åºåˆ—åˆ†æä¸LAMé©±åŠ¨æ™ºèƒ½ä½“å¦‚ä½•åä½œå®ç°åŸºäºKPIçš„è‡ªä¸»å†³ç­–ï¼ŒéªŒè¯äº†Agentic AIåœ¨ç½‘ç»œä¼˜åŒ–ä¸­çš„å¯è¡Œæ€§ã€‚

**Conclusion:** Agentic AIä¸º5G/6Gç½‘ç»œè‡ªåŠ¨åŒ–æä¾›äº†æ–°èŒƒå¼ï¼Œé€šè¿‡æ•´åˆå¤šæ¨¡æ€æ„ŸçŸ¥ã€è§„åˆ’ã€è®°å¿†å’Œæ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿå®ç°ç½‘ç»œç›®æ ‡çš„è‡ªä¸»åˆ†è§£ã€ä¸Šä¸‹æ–‡ä¿æŒå’ŒåŠ¨æ€é€‚åº”ï¼Œä¸ºæœªæ¥ç½‘ç»œæ™ºèƒ½åŒ–å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Agentic AI represents a new paradigm for automating complex systems by using
Large AI Models (LAMs) to provide human-level cognitive abilities with
multimodal perception, planning, memory, and reasoning capabilities. This will
lead to a new generation of AI systems that autonomously decompose goals,
retain context over time, learn continuously, operate across tools and
environments, and adapt dynamically. The complexity of 5G and upcoming 6G
networks renders manual optimization ineffective, pointing to Agentic AI as a
method for automating decisions in dynamic RAN environments. However, despite
its rapid advances, there is no established framework outlining the
foundational components and operational principles of Agentic AI systems nor a
universally accepted definition.
  This paper contributes to ongoing research on Agentic AI in 5G and 6G
networks by outlining its core concepts and then proposing a practical use case
that applies Agentic principles to RAN optimization. We first introduce Agentic
AI, tracing its evolution from classical agents and discussing the progress
from workflows and simple AI agents to Agentic AI. Core design
patterns-reflection, planning, tool use, and multi-agent collaboration-are then
described to illustrate how intelligent behaviors are orchestrated. These
theorical concepts are grounded in the context of mobile networks, with a focus
on RAN management and optimization. A practical 5G RAN case study shows how
time-series analytics and LAM-driven agents collaborate for KPI-based
autonomous decision-making.


### [34] [When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning](https://arxiv.org/abs/2511.02794)
*Chenyu Zhang, Minsol Kim, Shohreh Ghorbani, Jingyao Wu, Rosalind Picard, Patricia Maes, Paul Pu Liang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†æ¨¡æ€ç ´åè¿™ä¸€è¯Šæ–­æ€§å¤±æ•ˆæ¨¡å¼ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªè½»é‡çº§ã€æ¨¡å‹æ— å…³çš„è¯„ä¼°å±‚æ¥æ­ç¤ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­å„æ¨¡æ€çš„æ¨ç†åŠ¨æ€ï¼Œé€šè¿‡å°†æ¯ä¸ªæ¨¡æ€è§†ä¸ºæ™ºèƒ½ä½“æ¥è¯†åˆ«è´¡çŒ®è€…å’Œç ´åè€…ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å‘å±•è¿…é€Ÿï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹ä»ç„¶ä¸é€æ˜ï¼šéš¾ä»¥ç¡®å®šå“ªä¸ªæ¨¡æ€é©±åŠ¨é¢„æµ‹ã€å¦‚ä½•è§£å†³æ¨¡æ€é—´å†²çªã€ä»¥åŠä½•æ—¶æŸä¸ªæ¨¡æ€ä¸»å¯¼å†³ç­–ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­ç¼ºä¹é€æ˜åº¦å’Œè¯Šæ–­èƒ½åŠ›çš„é—®é¢˜ã€‚

**Method:** æå‡ºäº†ä¸€ç§è½»é‡çº§ã€æ¨¡å‹æ— å…³çš„è¯„ä¼°å±‚ï¼Œå°†æ¯ä¸ªæ¨¡æ€è§†ä¸ºæ™ºèƒ½ä½“ï¼Œç”Ÿæˆå€™é€‰æ ‡ç­¾å’Œç®€è¦çš„è‡ªæˆ‘è¯„ä¼°ç”¨äºå®¡è®¡ã€‚é‡‡ç”¨ç®€å•çš„èåˆæœºåˆ¶èšåˆè¿™äº›è¾“å‡ºï¼Œä»è€Œæš´éœ²è´¡çŒ®è€…ï¼ˆæ”¯æŒæ­£ç¡®ç»“æœçš„æ¨¡æ€ï¼‰å’Œç ´åè€…ï¼ˆè¯¯å¯¼å†³ç­–çš„æ¨¡æ€ï¼‰ã€‚

**Result:** åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«åŸºå‡†æµ‹è¯•çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œåº”ç”¨è¯¥è¯Šæ–­å±‚æ­ç¤ºäº†ç³»ç»Ÿæ€§çš„å¯é æ€§ç‰¹å¾ï¼Œæä¾›äº†å…³äºå¤±è´¥æ˜¯æºäºæ•°æ®é›†ä¼ªå½±è¿˜æ˜¯æ¨¡å‹å±€é™æ€§çš„è§è§£ã€‚

**Conclusion:** è¯¥æ¡†æ¶ä¸ºå¤šæ¨¡æ€æ¨ç†æä¾›äº†è¯Šæ–­æ€§æ”¯æ¶ï¼Œæ”¯æŒå¯¹èåˆåŠ¨æ€çš„åŸåˆ™æ€§å®¡è®¡ï¼Œå¹¶ä¸ºå¯èƒ½çš„å¹²é¢„æªæ–½æä¾›äº†ä¿¡æ¯ï¼Œæœ‰åŠ©äºç†è§£å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„å†³ç­–è¿‡ç¨‹å’Œå¤±æ•ˆæœºåˆ¶ã€‚

---

#### ğŸ“„ Abstract
Despite rapid growth in multimodal large language models (MLLMs), their
reasoning traces remain opaque: it is often unclear which modality drives a
prediction, how conflicts are resolved, or when one stream dominates. In this
paper, we introduce modality sabotage, a diagnostic failure mode in which a
high-confidence unimodal error overrides other evidence and misleads the fused
result. To analyze such dynamics, we propose a lightweight, model-agnostic
evaluation layer that treats each modality as an agent, producing candidate
labels and a brief self-assessment used for auditing. A simple fusion mechanism
aggregates these outputs, exposing contributors (modalities supporting correct
outcomes) and saboteurs (modalities that mislead). Applying our diagnostic
layer in a case study on multimodal emotion recognition benchmarks with
foundation models revealed systematic reliability profiles, providing insight
into whether failures may arise from dataset artifacts or model limitations.
More broadly, our framework offers a diagnostic scaffold for multimodal
reasoning, supporting principled auditing of fusion dynamics and informing
possible interventions.


### [35] [Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything](https://arxiv.org/abs/2511.02834)
*Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Agent-Omniæ¡†æ¶ï¼Œé€šè¿‡ä¸»ä»£ç†ç³»ç»Ÿåè°ƒç°æœ‰åŸºç¡€æ¨¡å‹ï¼Œå®ç°æ— éœ€é‡æ–°è®­ç»ƒçš„å¤šæ¨¡æ€æ¨ç†ï¼Œåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å±€é™äºå›ºå®šæ¨¡æ€ç»„åˆï¼Œéœ€è¦å¤§é‡å¯¹é½æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæ„å»ºèƒ½å¤Ÿé›†æˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘çš„å…¨èƒ½æ¨¡å‹ä»ä¸å®ç”¨ä¸”ç¼ºä¹å¼ºå¤§çš„æ¨ç†æ”¯æŒã€‚

**Method:** é‡‡ç”¨ä¸»ä»£ç†ç³»ç»Ÿæ¡†æ¶ï¼Œä¸»ä»£ç†è§£é‡Šç”¨æˆ·æ„å›¾ï¼Œå°†å­ä»»åŠ¡å§”æ‰˜ç»™ç‰¹å®šæ¨¡æ€ä»£ç†ï¼Œå¹¶æ•´åˆå®ƒä»¬çš„è¾“å‡ºå½¢æˆè¿è´¯å“åº”ï¼Œå®ç°çµæ´»çš„å¤šæ¨¡æ€æ¨ç†è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚

**Result:** åœ¨æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œå…¨èƒ½åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAgent-Omniå§‹ç»ˆè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤æ‚è·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚

**Conclusion:** åŸºäºä»£ç†çš„è®¾è®¡èƒ½å¤Ÿæ— ç¼é›†æˆä¸“é—¨çš„åŸºç¡€æ¨¡å‹ï¼Œç¡®ä¿å¯¹å¤šæ ·åŒ–è¾“å…¥çš„é€‚åº”æ€§ï¼ŒåŒæ—¶ä¿æŒé€æ˜æ€§å’Œå¯è§£é‡Šæ€§ï¼›æ¡†æ¶å…·æœ‰æ¨¡å—åŒ–å’Œæ˜“æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿéšç€æ›´å¼ºæ¨¡å‹çš„å¯ç”¨æ€§è¿›è¡Œæœªæ¥æ”¹è¿›ã€‚

---

#### ğŸ“„ Abstract
Multimodal large language models (MLLMs) have shown strong capabilities but
remain limited to fixed modality pairs and require costly fine-tuning with
large aligned datasets. Building fully omni-capable models that can integrate
text, images, audio, and video remains impractical and lacks robust reasoning
support. In this paper, we propose an Agent-Omni framework that coordinates
existing foundation models through a master-agent system, enabling flexible
multimodal reasoning without retraining. The master agent interprets user
intent, delegates subtasks to modality-specific agents, and integrates their
outputs into coherent responses. Extensive experiments across text, image,
audio, video, and omni benchmarks show that Agent-Omni consistently achieves
state-of-the-art performance, particularly on tasks requiring complex
cross-modal reasoning. Its agent-based design enables seamless integration of
specialized foundation models, ensuring adaptability to diverse inputs while
maintaining transparency and interpretability. In addition, the framework is
modular and easily extensible, allowing future improvements as stronger models
become available. %We release an open-source implementation to support
continued research on scalable and reliable omni-modal reasoning.
