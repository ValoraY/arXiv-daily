<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 32]
- [cs.CL](#cs.CL) [Total: 10]
- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Fine-Grained Human Motion Video Captioning](https://arxiv.org/abs/2510.24767)
*Guorui Song, Guocun Wang, Zhe Huang, Jing Lin, Xuefei Zhe, Jian Li, Haoqian Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†è¿åŠ¨å¢å¼ºå­—å¹•æ¨¡å‹ï¼ˆM-ACMï¼‰ï¼Œé€šè¿‡ç»“åˆåŸºäºäººä½“ç½‘æ ¼æ¢å¤çš„è¿åŠ¨æ„ŸçŸ¥è§£ç æ¥æå‡è§†é¢‘å­—å¹•è´¨é‡ï¼Œå¹¶å‘å¸ƒäº†ä¸“æ³¨äºäººä½“è¿åŠ¨çš„HMIæ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œæ˜¾è‘—æ”¹è¿›äº†å¤æ‚äººä½“åŠ¨ä½œæè¿°çš„å‡†ç¡®æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†é¢‘å­—å¹•æ¨¡å‹åœ¨æ•æ‰ç»†ç²’åº¦è¿åŠ¨ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´ç”Ÿæˆçš„å­—å¹•æ¨¡ç³Šæˆ–è¯­ä¹‰ä¸ä¸€è‡´ï¼Œæ— æ³•å‡†ç¡®æè¿°äººä½“åŠ¨ä½œçš„åŠ¨æ€ç‰¹å¾ã€‚

**Method:** M-ACMæ¡†æ¶åˆ©ç”¨ä»äººä½“ç½‘æ ¼æ¢å¤ä¸­æå–çš„è¿åŠ¨è¡¨ç¤ºæ¥æ˜¾å¼çªå‡ºäººä½“åŠ¨æ€ï¼Œé€šè¿‡è¿åŠ¨æ„ŸçŸ¥è§£ç æœºåˆ¶å‡å°‘å¹»è§‰å¹¶æ”¹å–„ç”Ÿæˆå­—å¹•çš„è¯­ä¹‰ä¿çœŸåº¦å’Œç©ºé—´å¯¹é½ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜M-ACMåœ¨å‡†ç¡®æè¿°å¤æ‚äººä½“åŠ¨ä½œå’Œç»†å¾®æ—¶é—´å˜åŒ–æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œä¸ºè¿åŠ¨ä¸­å¿ƒè§†é¢‘å­—å¹•è®¾å®šäº†æ–°æ ‡å‡†ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†è¿åŠ¨è¡¨ç¤ºåœ¨è§†é¢‘å­—å¹•ä¸­çš„é‡è¦æ€§ï¼Œæå‡ºçš„æ¡†æ¶å’Œæ•°æ®é›†ä¸ºè¿åŠ¨æ„ŸçŸ¥è§†é¢‘ç†è§£å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œæ¨åŠ¨äº†ç»†ç²’åº¦åŠ¨ä½œæè¿°æŠ€æœ¯çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Generating accurate descriptions of human actions in videos remains a
challenging task for video captioning models. Existing approaches often
struggle to capture fine-grained motion details, resulting in vague or
semantically inconsistent captions. In this work, we introduce the
Motion-Augmented Caption Model (M-ACM), a novel generative framework that
enhances caption quality by incorporating motion-aware decoding. At its core,
M-ACM leverages motion representations derived from human mesh recovery to
explicitly highlight human body dynamics, thereby reducing hallucinations and
improving both semantic fidelity and spatial alignment in the generated
captions. To support research in this area, we present the Human Motion Insight
(HMI) Dataset, comprising 115K video-description pairs focused on human
movement, along with HMI-Bench, a dedicated benchmark for evaluating
motion-focused video captioning. Experimental results demonstrate that M-ACM
significantly outperforms previous methods in accurately describing complex
human motions and subtle temporal variations, setting a new standard for
motion-centric video captioning.


### [2] [Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2510.24777)
*Yujie Nie, Jianzhang Ni, Yonglong Ye, Yuan-Ting Zhang, Yun Kwok Wing, Xiangqing Xu, Xin Ma, Lizhou Fan*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€äº¤å‰å¢å¼ºèåˆæ¡†æ¶ï¼Œé€šè¿‡ååŒåˆ©ç”¨çœ¼åŠ¨è¿½è¸ªå’Œé¢éƒ¨ç‰¹å¾è¿›è¡Œé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹ï¼Œè¯¥æ¡†æ¶åœ¨åŒºåˆ†ADæ‚£è€…ä¸å¥åº·å¯¹ç…§æ—¶è¾¾åˆ°äº†95.11%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰é˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­ä¸­ï¼Œè™½ç„¶å¤šæ¨¡æ€æ–¹æ³•é€šè¿‡æ•´åˆè¡Œä¸ºå’Œæ„ŸçŸ¥é¢†åŸŸçš„äº’è¡¥ä¿¡æ¯å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å¾ˆå°‘æœ‰ç ”ç©¶æ¢ç´¢çœ¼åŠ¨è¿½è¸ªå’Œé¢éƒ¨ç‰¹å¾çš„è”åˆé›†æˆç”¨äºè¾…åŠ©ADè¯Šæ–­ï¼Œè¿™é™åˆ¶äº†è¯Šæ–­çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

**Method:** è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šäº¤å‰å¢å¼ºèåˆæ³¨æ„åŠ›æ¨¡å—é€šè¿‡äº¤å‰æ³¨æ„åŠ›å’Œå…¨å±€å¢å¼ºå»ºæ¨¡æ¨¡æ€é—´äº¤äº’ï¼Œæ–¹å‘æ„ŸçŸ¥å·ç§¯æ¨¡å—é€šè¿‡æ°´å¹³-å‚ç›´æ„Ÿå—é‡æ•è·ç»†ç²’åº¦æ–¹å‘æ€§é¢éƒ¨ç‰¹å¾ï¼Œå…±åŒå®ç°è‡ªé€‚åº”å’Œåˆ¤åˆ«æ€§çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚

**Result:** åœ¨åŒ…å«25åADæ‚£è€…å’Œ25åå¥åº·å¯¹ç…§çš„åŒæ­¥å¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ä¼˜äºä¼ ç»Ÿçš„åæœŸèåˆå’Œç‰¹å¾æ‹¼æ¥æ–¹æ³•ï¼Œåœ¨åŒºåˆ†ADä¸HCæ—¶è¾¾åˆ°95.11%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå‡ºé€šè¿‡æ˜¾å¼å»ºæ¨¡æ¨¡æ€é—´ä¾èµ–å…³ç³»å’Œæ¨¡æ€ç‰¹å®šè´¡çŒ®çš„ä¼˜è¶Šé²æ£’æ€§å’Œè¯Šæ–­æ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡æ˜¾å¼å»ºæ¨¡æ¨¡æ€é—´äº¤äº’å’Œæ•è·ç»†ç²’åº¦ç‰¹å¾çš„å¤šæ¨¡æ€èåˆæ¡†æ¶åœ¨ADè¯Šæ–­ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¼€å‘æ›´å‡†ç¡®ã€é²æ£’çš„è¾…åŠ©è¯Šæ–­å·¥å…·æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶æ„å»ºäº†ç”Ÿæ€æœ‰æ•ˆçš„å¤šæ¨¡æ€èµ„æºç”¨äºè¯„ä¼°é›†æˆç­–ç•¥ã€‚

---

#### ğŸ“„ Abstract
Accurate diagnosis of Alzheimer's disease (AD) is essential for enabling
timely intervention and slowing disease progression. Multimodal diagnostic
approaches offer considerable promise by integrating complementary information
across behavioral and perceptual domains. Eye-tracking and facial features, in
particular, are important indicators of cognitive function, reflecting
attentional distribution and neurocognitive state. However, few studies have
explored their joint integration for auxiliary AD diagnosis. In this study, we
propose a multimodal cross-enhanced fusion framework that synergistically
leverages eye-tracking and facial features for AD detection. The framework
incorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module
(CEFAM), which models inter-modal interactions through cross-attention and
global enhancement, and (b) a Direction-Aware Convolution Module (DACM), which
captures fine-grained directional facial features via horizontal-vertical
receptive fields. Together, these modules enable adaptive and discriminative
multimodal representation learning. To support this work, we constructed a
synchronized multimodal dataset, including 25 patients with AD and 25 healthy
controls (HC), by recording aligned facial video and eye-tracking sequences
during a visual memory-search paradigm, providing an ecologically valid
resource for evaluating integration strategies. Extensive experiments on this
dataset demonstrate that our framework outperforms traditional late fusion and
feature concatenation methods, achieving a classification accuracy of 95.11% in
distinguishing AD from HC, highlighting superior robustness and diagnostic
performance by explicitly modeling inter-modal dependencies and
modality-specific contributions.


### [3] [PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models](https://arxiv.org/abs/2510.24792)
*Patrick Haller, Fabio Barth, Jonas Golde, Georg Rehm, Alan Akbik*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PISA-Benchï¼Œä¸€ä¸ªåŸºäºä¸“å®¶åˆ›å»ºçš„PISAæµ‹è¯•æ„å»ºçš„å¤šè¯­è¨€è§†è§‰è¯­è¨€åŸºå‡†ï¼ŒåŒ…å«å…­ä¸ªè¯­è¨€çš„å¹³è¡Œè¯­æ–™åº“ï¼Œç”¨äºè¯„ä¼°å¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†å­˜åœ¨é«˜è´¨é‡äººå·¥éªŒè¯æ ·æœ¬ä¸è¶³çš„é—®é¢˜ï¼Œè®¸å¤šæ•°æ®é›†ä¾èµ–å¤§è¯­è¨€æ¨¡å‹åˆæˆç”Ÿæˆå†…å®¹ï¼Œä¸”å¤§å¤šæ•°ä»…é™äºè‹±è¯­ï¼Œå¤šè¯­è¨€ç¿»è¯‘æ ·æœ¬çš„è´¨é‡ä¿è¯è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ã€‚

**Method:** åŸºäºè‹±è¯­PISAæµ‹è¯•ä¸“å®¶åˆ›å»ºçš„ä¾‹å­æ„å»ºå¤šè¯­è¨€åŸºå‡†ï¼Œæ¯ä¸ªä¾‹å­åŒ…å«äººå·¥æå–çš„æŒ‡ä»¤ã€é—®é¢˜ã€ç­”æ¡ˆé€‰é¡¹å’Œå›¾åƒï¼Œå¹¶æ·»åŠ é—®é¢˜ç±»å‹åˆ†ç±»ï¼Œä»è‹±è¯­ç¿»è¯‘åˆ°äº”ç§é¢å¤–è¯­è¨€ï¼ˆè¥¿ç­ç‰™è¯­ã€å¾·è¯­ã€ä¸­æ–‡ã€æ³•è¯­å’Œæ„å¤§åˆ©è¯­ï¼‰ï¼Œå½¢æˆè¦†ç›–å…­ç§è¯­è¨€çš„å®Œå…¨å¹³è¡Œè¯­æ–™åº“ã€‚

**Result:** è¯„ä¼°æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹å‘ç°ï¼Œç‰¹åˆ«æ˜¯å°å‹æ¨¡å‹ï¼ˆ<200äº¿å‚æ•°ï¼‰æ— æ³•è·å¾—é«˜æµ‹è¯•åˆ†æ•°ï¼Œåœ¨éè‹±è¯­åˆ†å‰²ä¸Šå­˜åœ¨æ˜¾è‘—æ€§èƒ½ä¸‹é™ï¼Œåœ¨ç©ºé—´å’Œå‡ ä½•æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºé«˜é”™è¯¯ç‡ã€‚

**Conclusion:** é€šè¿‡å‘å¸ƒæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œä¸ºæ¨è¿›å¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†èµ„æºï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤šè¯­è¨€å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œä¸ºæœªæ¥æ¨¡å‹æ”¹è¿›æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLMs) have demonstrated remarkable progress in
multimodal reasoning. However, existing benchmarks remain limited in terms of
high-quality, human-verified examples. Many current datasets rely on
synthetically generated content by large language models (LLMs). Furthermore,
most datasets are limited to English, as manual quality assurance of translated
samples is time-consuming and costly. To fill this gap, we introduce
PISA-Bench, a multilingual benchmark derived from English examples of the
expert-created PISA tests, a unified framework for the assessment of student
competencies in over eighty countries. Each example consists of human-extracted
instructions, questions, answer options, and images, enriched with question
type categories, and has been translated from English into five additional
languages (Spanish, German, Chinese, French, and Italian), resulting in a fully
parallel corpus covering six languages. We evaluate state-of-the-art
vision-language models on PISA-Bench and find that especially small models
(<20B parameters) fail to achieve high test scores. We further find substantial
performance degradation on non-English splits as well as high error-rates when
models are tasked with spatial and geometric reasoning. By releasing the
dataset and evaluation framework, we provide a resource for advancing research
on multilingual multimodal reasoning.


### [4] [A Survey on Efficient Vision-Language-Action Models](https://arxiv.org/abs/2510.24795)
*Zhaoshu Yu, Bo Wang, Pengpeng Zeng, Haonan Zhang, Ji Zhang, Lianli Gao, Jingkuan Song, Nicu Sebe, Heng Tao Shen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªå…³äºé«˜æ•ˆè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œé€šè¿‡å»ºç«‹ç»Ÿä¸€åˆ†ç±»æ³•å°†ç°æœ‰æŠ€æœ¯ç»„ç»‡ä¸ºä¸‰ä¸ªæ ¸å¿ƒæ”¯æŸ±ï¼Œä¸ºç¤¾åŒºå»ºç«‹äº†åŸºç¡€å‚è€ƒæ¡†æ¶å¹¶è§„åˆ’äº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å…·èº«æ™ºèƒ½é¢†åŸŸå±•ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼Œä½†å…¶éƒ¨ç½²å—åˆ°åº•å±‚å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹å·¨å¤§è®¡ç®—å’Œæ•°æ®éœ€æ±‚çš„ä¸¥é‡åˆ¶çº¦ï¼Œè¿«åˆ‡éœ€è¦è§£å†³è¿™äº›æ•ˆç‡æŒ‘æˆ˜ã€‚

**Method:** å¼•å…¥ç»Ÿä¸€åˆ†ç±»æ³•å°†é«˜æ•ˆVLAæŠ€æœ¯ç³»ç»Ÿç»„ç»‡ä¸ºä¸‰ä¸ªæ ¸å¿ƒæ”¯æŸ±ï¼šé«˜æ•ˆæ¨¡å‹è®¾è®¡ï¼ˆå…³æ³¨é«˜æ•ˆæ¶æ„å’Œæ¨¡å‹å‹ç¼©ï¼‰ã€é«˜æ•ˆè®­ç»ƒï¼ˆå‡å°‘æ¨¡å‹å­¦ä¹ è¿‡ç¨‹ä¸­çš„è®¡ç®—è´Ÿæ‹…ï¼‰ä»¥åŠé«˜æ•ˆæ•°æ®æ”¶é›†ï¼ˆè§£å†³æœºå™¨äººæ•°æ®è·å–å’Œåˆ©ç”¨çš„ç“¶é¢ˆï¼‰ã€‚

**Result:** é€šè¿‡åœ¨æ­¤æ¡†æ¶å†…å¯¹æœ€å…ˆè¿›æ–¹æ³•è¿›è¡Œæ‰¹åˆ¤æ€§å›é¡¾ï¼Œä¸ä»…ä¸ºç¤¾åŒºå»ºç«‹äº†åŸºç¡€å‚è€ƒï¼Œè¿˜æ€»ç»“äº†ä»£è¡¨æ€§åº”ç”¨ï¼Œç•Œå®šäº†å…³é”®æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶ç»˜åˆ¶äº†è·¯çº¿å›¾ã€‚

**Conclusion:** è¯¥è°ƒæŸ¥ç¡®ç«‹äº†é«˜æ•ˆVLAé¢†åŸŸçš„ç³»ç»Ÿæ€§åˆ†ææ¡†æ¶ï¼Œé€šè¿‡ä¸‰æ”¯æŸ±åˆ†ç±»æ³•æ•´åˆäº†åˆ†æ•£çš„ç ”ç©¶å·¥ä½œï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ˜ç¡®æ–¹å‘ï¼Œå¹¶ç»´æŠ¤æŒç»­æ›´æ–°çš„é¡¹ç›®é¡µé¢ä»¥è·Ÿè¸ªæœ€æ–°è¿›å±•ã€‚

---

#### ğŸ“„ Abstract
Vision-Language-Action models (VLAs) represent a significant frontier in
embodied intelligence, aiming to bridge digital knowledge with physical-world
interaction. While these models have demonstrated remarkable generalist
capabilities, their deployment is severely hampered by the substantial
computational and data requirements inherent to their underlying large-scale
foundation models. Motivated by the urgent need to address these challenges,
this survey presents the first comprehensive review of Efficient
Vision-Language-Action models (Efficient VLAs) across the entire
data-model-training process. Specifically, we introduce a unified taxonomy to
systematically organize the disparate efforts in this domain, categorizing
current techniques into three core pillars: (1) Efficient Model Design,
focusing on efficient architectures and model compression; (2) Efficient
Training, which reduces computational burdens during model learning; and (3)
Efficient Data Collection, which addresses the bottlenecks in acquiring and
utilizing robotic data. Through a critical review of state-of-the-art methods
within this framework, this survey not only establishes a foundational
reference for the community but also summarizes representative applications,
delineates key challenges, and charts a roadmap for future research. We
maintain a continuously updated project page to track our latest developments:
https://evla-survey.github.io/


### [5] [Conflict Adaptation in Vision-Language Models](https://arxiv.org/abs/2510.24804)
*Xiaoyang Hu*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡é¡ºåºStroopä»»åŠ¡å‘ç°12ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºä¸äººç±»å†²çªé€‚åº”ä¸€è‡´çš„è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨åœ¨InternVL 3.5 4Bä¸­è¯†åˆ«å‡ºè´Ÿè´£å†²çªè°ƒåˆ¶çš„å…³é”®ç¥ç»å…ƒï¼Œæ­ç¤ºäº†VLMsè®¤çŸ¥æ§åˆ¶æœºåˆ¶çš„ç¥ç»åŸºç¡€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢è§†è§‰è¯­è¨€æ¨¡å‹æ˜¯å¦è¡¨ç°å‡ºç±»ä¼¼äººç±»è®¤çŸ¥æ§åˆ¶çš„å†²çªé€‚åº”ç°è±¡ï¼Œå³åœ¨é«˜å†²çªè¯•æ¬¡åæ€§èƒ½æå‡çš„è¡Œä¸ºæ¨¡å¼ï¼Œä»¥ç†è§£è¿™äº›æ¨¡å‹å¦‚ä½•åŠ¨æ€è°ƒæ•´å…¶ç¨€ç¼ºçš„è®¤çŸ¥èµ„æºã€‚

**Method:** ç ”ç©¶é‡‡ç”¨é¡ºåºStroopä»»åŠ¡è¯„ä¼°13ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºè¡¨ç°ï¼Œå¹¶ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨åœ¨InternVL 3.5 4Bæ¨¡å‹ä¸­è¯†åˆ«ä»»åŠ¡ç›¸å…³çš„è¶…èŠ‚ç‚¹ï¼Œé€šè¿‡æ¶ˆèå®éªŒéªŒè¯ç‰¹å®šç¥ç»å…ƒçš„åŠŸèƒ½é‡è¦æ€§ã€‚

**Result:** å®éªŒå‘ç°12ä¸ªVLMsè¡¨ç°å‡ºæ˜¾è‘—çš„å†²çªé€‚åº”è¡Œä¸ºï¼Œä»…æœ‰ä¸€ä¸ªæ¨¡å‹å› å¤©èŠ±æ¿æ•ˆåº”æœªæ˜¾ç¤ºè¯¥æ¨¡å¼ï¼›åœ¨InternVL 3.5 4Bä¸­è¯†åˆ«å‡ºæ—©æœŸå’Œæ™šæœŸå±‚ä¸­éƒ¨åˆ†é‡å çš„æ–‡æœ¬å’Œé¢œè‰²è¶…èŠ‚ç‚¹ï¼Œå…¶ç›¸å¯¹å¤§å°åæ˜ äº†äººç±»é˜…è¯»ä¸é¢œè‰²å‘½åçš„è‡ªåŠ¨æ€§ä¸å¯¹ç§°ï¼Œå¹¶åœ¨24-25å±‚å‘ç°å†²çªè°ƒåˆ¶è¶…èŠ‚ç‚¹ï¼Œå…¶æ¶ˆèæ˜¾è‘—å¢åŠ Stroopé”™è¯¯ç‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶é¦–æ¬¡åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ç³»ç»Ÿæ€§åœ°è¯æ˜äº†ç±»ä¼¼äººç±»çš„è®¤çŸ¥æ§åˆ¶æœºåˆ¶ï¼Œæ­ç¤ºäº†VLMså†…éƒ¨è¡¨å¾ç»“æ„ä¸äººç±»è®¤çŸ¥è¿‡ç¨‹çš„ç›¸ä¼¼æ€§ï¼Œä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›æä¾›äº†ç¥ç»è®¡ç®—åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
A signature of human cognitive control is conflict adaptation: improved
performance on a high-conflict trial following another high-conflict trial.
This phenomenon offers an account for how cognitive control, a scarce resource,
is recruited. Using a sequential Stroop task, we find that 12 of 13
vision-language models (VLMs) tested exhibit behavior consistent with conflict
adaptation, with the lone exception likely reflecting a ceiling effect. To
understand the representational basis of this behavior, we use sparse
autoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B.
Partially overlapping supernodes emerge for text and color in both early and
late layers, and their relative sizes mirror the automaticity asymmetry between
reading and color naming in humans. We further isolate a conflict-modulated
supernode in layers 24-25 whose ablation significantly increases Stroop errors
while minimally affecting congruent trials.


### [6] [DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts](https://arxiv.org/abs/2510.24813)
*Binbin Li, Guimiao Yang, Zisen Qi, Haiping Wang, Yu Ding*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†DualCapæ–¹æ³•ï¼Œé€šè¿‡åŒæ£€ç´¢æœºåˆ¶ç”Ÿæˆè§†è§‰æç¤ºæ¥å¢å¼ºå›¾åƒæè¿°æ¨¡å‹çš„è§†è§‰è¡¨ç¤ºï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä»…å°†æ£€ç´¢æ•°æ®ç”¨ä½œæ–‡æœ¬æç¤ºè€Œå¿½ç•¥åŸå§‹è§†è§‰ç‰¹å¾å¢å¼ºçš„è¯­ä¹‰é¸¿æ²Ÿé—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è½»é‡çº§æ£€ç´¢å¢å¼ºå›¾åƒæè¿°æ¨¡å‹é€šå¸¸ä»…å°†æ£€ç´¢æ•°æ®ç”¨ä½œæ–‡æœ¬æç¤ºï¼Œå¯¼è‡´åŸå§‹è§†è§‰ç‰¹å¾æœªå¾—åˆ°å¢å¼ºï¼Œåœ¨å¯¹è±¡ç»†èŠ‚å’Œå¤æ‚åœºæ™¯ç†è§£æ–¹é¢å­˜åœ¨è¯­ä¹‰é¸¿æ²Ÿã€‚

**Method:** æå‡ºDualCapæ–¹æ³•ï¼Œé‡‡ç”¨åŒæ£€ç´¢æœºåˆ¶ï¼šæ ‡å‡†å›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢ç”¨äºæ–‡æœ¬æç¤ºï¼Œæ–°é¢–çš„å›¾åƒåˆ°å›¾åƒæ£€ç´¢ç”¨äºè·å–è§†è§‰ç›¸ä¼¼åœºæ™¯ã€‚ä»è§†è§‰ç›¸ä¼¼åœºæ™¯çš„æ ‡é¢˜ä¸­æå–å…³é”®è¯è¯­å’ŒçŸ­è¯­ï¼Œé€šè¿‡è½»é‡çº§å¯è®­ç»ƒç‰¹å¾èåˆç½‘ç»œå°†è¿™äº›æ–‡æœ¬ç‰¹å¾ç¼–ç å¹¶ä¸åŸå§‹å›¾åƒç‰¹å¾é›†æˆã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç«äº‰åŠ›çš„æ€§èƒ½åŒæ—¶ï¼Œç›¸æ¯”ä¹‹å‰çš„è§†è§‰æç¤ºæè¿°æ–¹æ³•éœ€è¦æ›´å°‘çš„å¯è®­ç»ƒå‚æ•°ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡åŒæ£€ç´¢æœºåˆ¶ç”Ÿæˆè§†è§‰æç¤ºèƒ½æœ‰æ•ˆå¢å¼ºå›¾åƒæè¿°æ¨¡å‹çš„è§†è§‰è¡¨ç¤ºèƒ½åŠ›ï¼Œä¸ºè½»é‡çº§æ£€ç´¢å¢å¼ºæ¨¡å‹æä¾›äº†æ–°çš„è®¾è®¡æ€è·¯ï¼Œåœ¨å‚æ•°æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡ã€‚

---

#### ğŸ“„ Abstract
Recent lightweight retrieval-augmented image caption models often utilize
retrieved data solely as text prompts, thereby creating a semantic gap by
leaving the original visual features unenhanced, particularly for object
details or complex scenes. To address this limitation, we propose $DualCap$, a
novel approach that enriches the visual representation by generating a visual
prompt from retrieved similar images. Our model employs a dual retrieval
mechanism, using standard image-to-text retrieval for text prompts and a novel
image-to-image retrieval to source visually analogous scenes. Specifically,
salient keywords and phrases are derived from the captions of visually similar
scenes to capture key objects and similar details. These textual features are
then encoded and integrated with the original image features through a
lightweight, trainable feature fusion network. Extensive experiments
demonstrate that our method achieves competitive performance while requiring
fewer trainable parameters compared to previous visual-prompting captioning
approaches.


### [7] [Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection](https://arxiv.org/abs/2510.24816)
*Cui Yakun, Fushuo Huo, Weijie Shi, Juntao Dai, Hang Du, Zhenghao Zhu, Sirui Han, Yike Guo*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†MVFNDBåŸºå‡†æµ‹è¯•ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘å‡æ–°é—»æ£€æµ‹ä¸­çš„æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†MVFND-CoTæ¡†æ¶æ¥éªŒè¯å¤šç‰¹å¾èåˆå¯¹æ£€æµ‹ç»“æœçš„å½±å“ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿè§†é¢‘å‡æ–°é—»æ£€æµ‹åŸºå‡†ä¸»è¦å…³æ³¨æœ€ç»ˆå†³ç­–çš„å‡†ç¡®æ€§ï¼Œç¼ºä¹å¯¹æ•´ä¸ªæ£€æµ‹è¿‡ç¨‹çš„ç»†ç²’åº¦è¯„ä¼°ï¼Œä½¿å¾—æ£€æµ‹è¿‡ç¨‹æˆä¸ºé»‘ç®±ï¼Œæ— æ³•æ·±å…¥ç†è§£æ¨¡å‹çš„æ„ŸçŸ¥ã€ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶åŸºäºç»éªŒåˆ†ææ„å»ºäº†MVFNDBåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«10ä¸ªä»»åŠ¡å’Œ9730ä¸ªäººå·¥æ ‡æ³¨çš„è§†é¢‘ç›¸å…³é—®é¢˜ï¼Œå¹¶è®¾è®¡äº†MVFND-CoTæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†åˆ›ä½œè€…æ·»åŠ å†…å®¹å’ŒåŸå§‹æ‹æ‘„ç´ æçš„æ¨ç†è¿‡ç¨‹ã€‚

**Result:** ç ”ç©¶å¯¹å½±å“æ£€æµ‹å‡†ç¡®æ€§çš„æ·±å±‚å› ç´ è¿›è¡Œäº†æ·±å…¥åˆ†æï¼ŒåŒ…æ‹¬è§†é¢‘å¤„ç†ç­–ç•¥ä»¥åŠè§†é¢‘ç‰¹å¾ä¸æ¨¡å‹èƒ½åŠ›ä¹‹é—´çš„å¯¹é½å…³ç³»ï¼ŒéªŒè¯äº†å¤šç‰¹å¾èåˆå¯¹æœ€ç»ˆç»“æœçš„å½±å“ã€‚

**Conclusion:** è¯¥åŸºå‡†æµ‹è¯•ä¸ºæœªæ¥å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘å‡æ–°é—»æ£€æµ‹é¢†åŸŸçš„è¯„ä¼°å’Œå‘å±•å¥ å®šäº†åšå®åŸºç¡€ï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸç ”ç©¶çš„ç³»ç»ŸåŒ–å’Œæ·±å…¥åŒ–å‘å±•ã€‚

---

#### ğŸ“„ Abstract
The advent of multi-modal large language models (MLLMs) has greatly advanced
research into applications for Video fake news detection (VFND) tasks.
Traditional video-based FND benchmarks typically focus on the accuracy of the
final decision, often failing to provide fine-grained assessments for the
entire detection process, making the detection process a black box. Therefore,
we introduce the MVFNDB (Multi-modal Video Fake News Detection Benchmark) based
on the empirical analysis, which provides foundation for tasks definition. The
benchmark comprises 10 tasks and is meticulously crafted to probe MLLMs'
perception, understanding, and reasoning capacities during detection, featuring
9730 human-annotated video-related questions based on a carefully constructed
taxonomy ability of VFND. To validate the impact of combining multiple features
on the final results, we design a novel framework named MVFND-CoT, which
incorporates both creator-added content and original shooting footage
reasoning. Building upon the benchmark, we conduct an in-depth analysis of the
deeper factors influencing accuracy, including video processing strategies and
the alignment between video features and model capabilities. We believe this
benchmark will lay a solid foundation for future evaluations and advancements
of MLLMs in the domain of video fake news detection.


### [8] [SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing](https://arxiv.org/abs/2510.24820)
*Ruiyang Zhang, Jiahao Luo, Xiaoru Feng, Qiufan Pang, Yaodong Yang, Juntao Dai*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¤šè½®å®‰å…¨ç¼–è¾‘æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºMR-SafeEditæ•°æ®é›†å’Œå¼€å‘SafeEditoræ¨¡å‹ï¼Œä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæä¾›æ¨¡å‹æ— å…³çš„å®‰å…¨å¯¹é½è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—å‡å°‘äº†è¿‡åº¦æ‹’ç»å¹¶æ”¹å–„äº†å®‰å…¨æ€§ä¸å®ç”¨æ€§çš„å¹³è¡¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å®‰å…¨æ–¹æ³•ä¸»è¦åˆ†ä¸ºè®­ç»ƒæ—¶å’Œæ¨ç†æ—¶ä¸¤ç±»ï¼Œå…¶ä¸­æ¨ç†æ—¶æ–¹æ³•å› æˆæœ¬æ•ˆç›Šè€Œè¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†å­˜åœ¨è¿‡åº¦æ‹’ç»ä»¥åŠå®‰å…¨æ€§ä¸å®ç”¨æ€§ä¹‹é—´å¹³è¡¡ä¸è¶³çš„é—®é¢˜ï¼Œéœ€è¦å¼€å‘æ›´æœ‰æ•ˆçš„å®‰å…¨å¯¹é½æ–¹æ¡ˆã€‚

**Method:** æå‡ºäº†ä¸€ä¸ªå¤šè½®å®‰å…¨ç¼–è¾‘æ¡†æ¶ä½œä¸ºæ¨¡å‹æ— å…³çš„å³æ’å³ç”¨æ¨¡å—ï¼Œæ ¸å¿ƒæ˜¯ä¸“é—¨ä¸ºå®‰å…¨ç¼–è¾‘æ„å»ºçš„MR-SafeEditå¤šè½®å›¾æ–‡äº¤é”™æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†SafeEditorç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨åéªŒå®‰å…¨ç¼–è¾‘èŒƒå¼æ¨¡æ‹Ÿäººç±»è¯†åˆ«å’Œä¼˜åŒ–ä¸å®‰å…¨å†…å®¹çš„è®¤çŸ¥è¿‡ç¨‹ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒSafeEditorè¶…è¶Šäº†å…ˆå‰çš„å®‰å…¨æ–¹æ³•ï¼Œåœ¨å‡å°‘è¿‡åº¦æ‹’ç»çš„åŒæ—¶å®ç°äº†æ›´ä¼˜çš„å®‰å…¨æ€§ä¸å®ç”¨æ€§å¹³è¡¡ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå®‰å…¨å¯¹é½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šè½®å®‰å…¨ç¼–è¾‘æ¡†æ¶åœ¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å®‰å…¨å¯¹é½ä¸­çš„æ½œåŠ›ï¼Œä¸ºæ¨¡å‹å®‰å…¨æä¾›äº†æ–°çš„åéªŒç¼–è¾‘èŒƒå¼ï¼Œæœªæ¥å¯æ‰©å±•åˆ°æ›´å¹¿æ³›çš„å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå®ç°æ›´å…¨é¢çš„å®‰å…¨é˜²æŠ¤ã€‚

---

#### ğŸ“„ Abstract
With the rapid advancement of text-to-image (T2I) models, ensuring their
safety has become increasingly critical. Existing safety approaches can be
categorized into training-time and inference-time methods. While inference-time
methods are widely adopted due to their cost-effectiveness, they often suffer
from limitations such as over-refusal and imbalance between safety and utility.
To address these challenges, we propose a multi-round safety editing framework
that functions as a model-agnostic, plug-and-play module, enabling efficient
safety alignment for any text-to-image model. Central to this framework is
MR-SafeEdit, a multi-round image-text interleaved dataset specifically
constructed for safety editing in text-to-image generation. We introduce a
post-hoc safety editing paradigm that mirrors the human cognitive process of
identifying and refining unsafe content. To instantiate this paradigm, we
develop SafeEditor, a unified MLLM capable of multi-round safety editing on
generated images. Experimental results show that SafeEditor surpasses prior
safety approaches by reducing over-refusal while achieving a more favorable
safety-utility balance.


### [9] [Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation](https://arxiv.org/abs/2510.24821)
*Inclusion AI, :, Bowen Ma, Cheng Zou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianing Li, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jianping Jiang, Jun Peng, Kaixiang Ji, Kaimeng Ren, Libin Wang, Lixiang Ru, Longhua Tan, Lan Wang, Mochen Bai, Ning Gao, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Ruobing Zheng, Sirui Gao, Tianqi Li, Tinghao Liu, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaolong Wang, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yuting Xiao, Yunxiao Sun, Yipeng Chen, Yifan Mao, Yifei Wu, Yongjie Lyu, Ziping Ma, Zhiqiang Fang, Zhihao Qiu, Ziyuan Huang, Zizheng Yang, Zhengyu He*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Ming-Flash-Omniï¼Œè¿™æ˜¯Ming-Omniçš„å‡çº§ç‰ˆæœ¬ï¼Œé‡‡ç”¨åŸºäºLing-Flash-2.0çš„ç¨€ç–æ··åˆä¸“å®¶æ¶æ„ï¼Œå…·æœ‰1000äº¿æ€»å‚æ•°ä½†æ¯tokenä»…æ¿€æ´»61äº¿å‚æ•°ï¼Œåœ¨ç»Ÿä¸€å¤šæ¨¡æ€æ™ºèƒ½æ–¹é¢å®ç°äº†æ˜¾è‘—è¿›æ­¥ï¼Œå¹¶åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€ç”Ÿæˆå¼åˆ†å‰²å’Œä¸Šä¸‹æ–‡è¯­éŸ³è¯†åˆ«ç­‰å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹å®¹é‡æ‰©å±•ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ï¼ŒåŒæ—¶æ¨åŠ¨ç»Ÿä¸€å¤šæ¨¡æ€æ™ºèƒ½çš„å‘å±•ï¼Œæ¶µç›–è§†è§‰ã€è¯­éŸ³å’Œè¯­è¨€ç­‰å¤šä¸ªæ¨¡æ€ï¼Œä¸ºå®ç°é€šç”¨äººå·¥æ™ºèƒ½è¿ˆå‡ºå…³é”®ä¸€æ­¥ã€‚

**Method:** è¯¥æ–¹æ³•åŸºäºLing-Flash-2.0çš„ç¨€ç–æ··åˆä¸“å®¶å˜ä½“æ„å»ºï¼Œæ€»å‚æ•°é‡è¾¾1000äº¿ä½†æ¯tokenä»…æ¿€æ´»61äº¿å‚æ•°ï¼Œé€šè¿‡è¿™ç§æ¶æ„å®ç°äº†é«˜æ•ˆçš„å¯æ‰©å±•æ€§ï¼Œå¹¶å¼•å…¥äº†ç”Ÿæˆå¼åˆ†å‰²ç­‰æ–°èƒ½åŠ›æ¥å¢å¼ºç©ºé—´æ§åˆ¶å’Œç¼–è¾‘ä¸€è‡´æ€§ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢ç›¸æ¯”å‰ä»£æœ‰æ˜¾è‘—æå‡ï¼Œåœ¨ä¸Šä¸‹æ–‡ASRä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½å¹¶åœ¨æ–¹è¨€æ„ŸçŸ¥ASRä¸Šè·å¾—é«˜åº¦ç«äº‰åŠ›ï¼Œåœ¨å›¾åƒç”Ÿæˆä¸­å®ç°äº†é«˜ä¿çœŸæ–‡æœ¬æ¸²æŸ“ï¼Œå¹¶åœ¨åœºæ™¯ä¸€è‡´æ€§å’Œèº«ä»½ä¿æŒæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶åœ¨æ‰€æœ‰12ä¸ªä¸Šä¸‹æ–‡ASRåŸºå‡†æµ‹è¯•ä¸­éƒ½åˆ›é€ äº†æ–°è®°å½•ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ç¨€ç–æ··åˆä¸“å®¶æ¶æ„èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡è®¡ç®—æ•ˆç‡ä¸æ¨¡å‹å®¹é‡ï¼Œç»Ÿä¸€çš„æ¶æ„è®¾è®¡ä¸ºå¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿæä¾›äº†å¯è¡Œè·¯å¾„ï¼Œç”Ÿæˆå¼åˆ†å‰²ç­‰æ–°èƒ½åŠ›ä¸ä»…æå‡äº†åˆ†å‰²æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†å›¾åƒç”Ÿæˆçš„ç©ºé—´æ§åˆ¶ï¼Œä¸ºé€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚

---

#### ğŸ“„ Abstract
We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a
sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion
total parameters, of which only 6.1 billion are active per token. This
architecture enables highly efficient scaling (dramatically improving
computational efficiency while significantly expanding model capacity) and
empowers stronger unified multimodal intelligence across vision, speech, and
language, representing a key step toward Artificial General Intelligence (AGI).
Compared to its predecessor, the upgraded version exhibits substantial
improvements across multimodal understanding and generation. We significantly
advance speech recognition capabilities, achieving state-of-the-art performance
in contextual ASR and highly competitive results in dialect-aware ASR. In image
generation, Ming-Flash-Omni introduces high-fidelity text rendering and
demonstrates marked gains in scene consistency and identity preservation during
image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,
a capability that not only achieves strong standalone segmentation performance
but also enhances spatial control in image generation and improves editing
consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in
text-to-image generation and generative segmentation, and sets new records on
all 12 contextual ASR benchmarks, all within a single unified architecture.


### [10] [MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal Interaction for Multimodal Emotion Recognition](https://arxiv.org/abs/2510.24827)
*Haoyang Zhang, Zhou Yang, Ke Sun, Yucai Pang, Guoliang Xu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šè·¯å¾„è·¨æ¨¡æ€äº¤äº’çš„æ··åˆç½‘ç»œæ¨¡å‹MCIHNï¼Œé€šè¿‡å¯¹æŠ—è‡ªç¼–ç å™¨å’Œè·¨æ¨¡æ€é—¨æ§æœºåˆ¶è§£å†³å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ä¸­çš„æ¨¡æ€å·®å¼‚å’Œæƒ…æ„Ÿç‰¹å¾è¡¨å¾éš¾é¢˜ï¼Œåœ¨å…¬å¼€æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜è¶Šæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«åœ¨äººæœºäº¤äº’ä¸­è‡³å…³é‡è¦ï¼Œä½†å½“å‰é¢ä¸´æ¨¡æ€é—´å·®å¼‚æ˜¾è‘—ä»¥åŠå•æ¨¡æ€æƒ…æ„Ÿä¿¡æ¯è¡¨å¾å›°éš¾ä¸¤å¤§æŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ã€‚

**Method:** æå‡ºMCIHNæ··åˆç½‘ç»œæ¨¡å‹ï¼Œé¦–å…ˆä¸ºæ¯ä¸ªæ¨¡æ€æ„å»ºå¯¹æŠ—è‡ªç¼–ç å™¨å­¦ä¹ åˆ¤åˆ«æ€§æƒ…æ„Ÿç‰¹å¾å¹¶è¿›è¡Œé‡æ„å¢å¼ºï¼Œç„¶åé€šè¿‡é¢„å®šä¹‰çš„è·¨æ¨¡æ€é—¨æ§æœºåˆ¶CGMMå‡å°‘æ¨¡æ€å·®å¼‚å¹¶å»ºç«‹æ¨¡æ€é—´æƒ…æ„Ÿå…³ç³»ï¼Œæœ€åä½¿ç”¨ç‰¹å¾èåˆæ¨¡å—FFMè¿›è¡Œå¤šæ¨¡æ€èåˆã€‚

**Result:** åœ¨å…¬å¼€å¯ç”¨çš„SIMSå’ŒMOSIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMCIHNæ¨¡å‹å®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡å¯¹æŠ—è‡ªç¼–ç å™¨å­¦ä¹ åˆ¤åˆ«ç‰¹å¾å’Œè·¨æ¨¡æ€äº¤äº’æœºåˆ¶å‡å°‘æ¨¡æ€å·®å¼‚çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Multimodal emotion recognition is crucial for future human-computer
interaction. However, accurate emotion recognition still faces significant
challenges due to differences between different modalities and the difficulty
of characterizing unimodal emotional information. To solve these problems, a
hybrid network model based on multipath cross-modal interaction (MCIHN) is
proposed. First, adversarial autoencoders (AAE) are constructed separately for
each modality. The AAE learns discriminative emotion features and reconstructs
the features through a decoder to obtain more discriminative information about
the emotion classes. Then, the latent codes from the AAE of different
modalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to
reduce the discrepancy between modalities, establish the emotional relationship
between interacting modalities, and generate the interaction features between
different modalities. Multimodal fusion using the Feature Fusion module (FFM)
for better emotion recognition. Experiments were conducted on publicly
available SIMS and MOSI datasets, demonstrating that MCIHN achieves superior
performance.


### [11] [Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient Modulation for Harmonized Multimodal Learning](https://arxiv.org/abs/2510.24919)
*Hossein R. Nowdeh, Jie Ji, Xiaolong Ma, Fatemeh Afghah*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†æ¨¡æ€æ„ŸçŸ¥é”åº¦æ„ŸçŸ¥æœ€å°åŒ–ï¼ˆM-SAMï¼‰æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«ä¸»å¯¼æ¨¡æ€å¹¶è°ƒåˆ¶æŸå¤±å‡½æ•°æ¥å¹³è¡¡å¤šæ¨¡æ€å­¦ä¹ ï¼Œæ˜¾è‘—æå‡æ¨¡å‹é²æ£’æ€§å’Œæ€§èƒ½ã€‚è¯¥æ¨¡å‹æ— å…³æ–¹æ³•åœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€ä¼˜åŒ–å’Œæ¢¯åº¦æ“ä½œæ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å­¦ä¹ ä¸­ï¼Œä¸»å¯¼æ¨¡æ€å¾€å¾€ä¼šå‹åˆ¶å…¶ä»–æ¨¡æ€çš„è´¡çŒ®ï¼Œå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¹³è¡¡ä¸åŒæ¨¡æ€ä¹‹é—´çš„å­¦ä¹ åŠ¨æ€ï¼Œé™åˆ¶äº†æ¨¡å‹ä»äº’è¡¥ç‰¹å¾ä¸­è·ç›Šçš„èƒ½åŠ›ã€‚

**Method:** M-SAMæ¡†æ¶é‡‡ç”¨ä¸‰é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼šé¦–å…ˆåŸºäºShapleyå€¼è¯†åˆ«ä¸»å¯¼æ¨¡æ€ï¼Œç„¶åé€šè¿‡æŸå¤±å‡½æ•°åˆ†è§£è°ƒåˆ¶æŸå¤±æ™¯è§‚ä»¥å¢å¼ºä¸»å¯¼æ¨¡æ€çš„é²æ£’æ€§ï¼Œæœ€åé€šè¿‡è°ƒåˆ¶æ¢¯åº¦çš„åå‘ä¼ æ’­æ›´æ–°æƒé‡ã€‚è¯¥æ–¹æ³•æ”¯æŒæ—©æœŸå’Œæ™šæœŸèåˆåœºæ™¯ï¼Œé€‚ç”¨äºå¤šç§æ¨¡æ€ç±»å‹ã€‚

**Result:** åœ¨å››ä¸ªå¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒM-SAMæ˜¾è‘—ä¼˜äºæœ€æ–°çš„æœ€ä¼˜åŒ–å’Œæ¢¯åº¦æ“ä½œæ–¹æ³•ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå¹³è¡¡äº†å¤šæ¨¡æ€å­¦ä¹ è¿‡ç¨‹ï¼ŒåŒæ—¶æå‡äº†æ•´ä½“æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†å…¶åœ¨å¢å¼ºæ¨¡å‹é²æ£’æ€§å’Œåˆ©ç”¨äº’è¡¥ç‰¹å¾æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** M-SAMé€šè¿‡æ¨¡æ€æ„ŸçŸ¥çš„æŸå¤±è°ƒåˆ¶æœºåˆ¶ï¼ŒæˆåŠŸè§£å†³äº†å¤šæ¨¡æ€å­¦ä¹ ä¸­ä¸»å¯¼æ¨¡æ€å‹åˆ¶é—®é¢˜ã€‚è¯¥æ¡†æ¶ä¸ºå¤šæ¨¡æ€å­¦ä¹ æä¾›äº†æ–°çš„ä¼˜åŒ–è§†è§’ï¼Œèƒ½å¤Ÿä¿ƒè¿›æ¨¡å‹æ›´å¥½åœ°æ¢ç´¢å’Œåˆ©ç”¨ä¸åŒæ¨¡æ€é—´çš„äº’è¡¥ç‰¹å¾ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
In multimodal learning, dominant modalities often overshadow others, limiting
generalization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM),
a model-agnostic framework that applies to many modalities and supports early
and late fusion scenarios. In every iteration, M-SAM in three steps optimizes
learning. \textbf{First, it identifies the dominant modality} based on
modalities' contribution in the accuracy using Shapley. \textbf{Second, it
decomposes the loss landscape}, or in another language, it modulates the loss
to prioritize the robustness of the model in favor of the dominant modality,
and \textbf{third, M-SAM updates the weights} by backpropagation of modulated
gradients. This ensures robust learning for the dominant modality while
enhancing contributions from others, allowing the model to explore and exploit
complementary features that strengthen overall performance. Extensive
experiments on four diverse datasets show that M-SAM outperforms the latest
state-of-the-art optimization and gradient manipulation methods and
significantly balances and improves multimodal learning.


### [12] [FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for Pressure Ulcer Severity Classification with Reasoning](https://arxiv.org/abs/2510.24980)
*Reza Saadati Fard, Emmanuel Agu, Palawat Busaranuvong, Deepak Kumar, Shefalika Gautam, Bengisu Tulu, Diane Strong, Lorraine Loretz*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†FT-ARMï¼Œä¸€ç§åŸºäºå¾®è°ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è‡ªåå¼æ™ºèƒ½ä½“æ¨¡å‹ï¼Œé€šè¿‡è¿­ä»£æ¨ç†æœºåˆ¶åœ¨å‹åŠ›æ€§æŸä¼¤ä¸¥é‡ç¨‹åº¦åˆ†ç±»ä»»åŠ¡ä¸­å®ç°äº†85%çš„å‡†ç¡®ç‡ï¼Œæ¯”ç°æœ‰CNNæ–¹æ³•æå‡4%ï¼ŒåŒæ—¶æä¾›ä¸´åºŠå¯è§£é‡Šçš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å‹åŠ›æ€§æŸä¼¤ä¸¥é‡ç¨‹åº¦åˆ†ç±»å­˜åœ¨è§†è§‰ç‰¹å¾ç»†å¾®å·®å¼‚å’Œä¸»è§‚åˆ¤æ–­å˜å¼‚æ€§ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰åŸºäºCNNå’ŒViTçš„AIæ–¹æ³•è™½ç„¶å‡†ç¡®ç‡è¾ƒé«˜ä½†å¯è§£é‡Šæ€§ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³ä¸´åºŠéƒ¨ç½²å¯¹é€æ˜åº¦å’Œä¸€è‡´æ€§çš„éœ€æ±‚ã€‚

**Method:** FT-ARMåŸºäºLLaMA 3.2 90Bè¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œé›†æˆäº†æ™ºèƒ½ä½“è‡ªåå¼æœºåˆ¶ï¼Œé€šè¿‡è¿­ä»£æ¨ç†è¿‡ç¨‹å¯¹è§†è§‰ç‰¹å¾å’Œç¼–ç çš„ä¸´åºŠçŸ¥è¯†è¿›è¡Œç»¼åˆåˆ†æï¼Œæ¨¡æ‹Ÿä¸´åºŠåŒ»ç”Ÿçš„è¯Šæ–­å†è¯„ä¼°è¿‡ç¨‹ã€‚

**Result:** åœ¨å…¬å¼€å‹åŠ›æ€§æŸä¼¤å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒFT-ARMåœ¨I-IVæœŸå‹åŠ›æ€§æŸä¼¤åˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°85%çš„å‡†ç¡®ç‡ï¼Œæ¯”å…ˆå‰CNNæ¨¡å‹æå‡4%ï¼Œå¹¶åœ¨å®æ—¶æ¨ç†åœºæ™¯ä¸‹éªŒè¯äº†æ€§èƒ½ï¼ŒåŒæ—¶ç”ŸæˆåŸºäºä¸´åºŠçŸ¥è¯†çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚

**Conclusion:** FT-ARMé€šè¿‡ç»“åˆå¾®è°ƒå’Œå¤šæ¨¡æ€è‡ªåå¼æ¨ç†ï¼Œæ˜¾è‘—æå‡äº†è‡ªåŠ¨åŒ–ä¼¤å£è¯„ä¼°ç³»ç»Ÿçš„å¯é æ€§ã€é€æ˜åº¦å’Œä¸´åºŠåº”ç”¨ä»·å€¼ï¼Œä¸ºè§£å†³å‹åŠ›æ€§æŸä¼¤åˆ†æœŸçš„ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§éœ€æ±‚æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Pressure ulcers (PUs) are a serious and prevalent healthcare concern.
Accurate classification of PU severity (Stages I-IV) is essential for proper
treatment but remains challenging due to subtle visual distinctions and
subjective interpretation, leading to variability among clinicians. Prior
AI-based approaches using Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs) achieved promising accuracy but offered limited
interpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal
model), a fine-tuned multimodal large language model (MLLM) with an agentic
self-reflection mechanism for pressure ulcer severity classification. Inspired
by clinician-style diagnostic reassessment, FT-ARM iteratively refines its
predictions by reasoning over visual features and encoded clinical knowledge
from text, enhancing both accuracy and consistency. On the publicly available
Pressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B,
achieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based
models by +4%. Unlike earlier CNN/ViT studies that relied solely on offline
evaluations, FT-ARM is designed and tested for live inference, reflecting
real-time deployment conditions. Furthermore, it produces clinically grounded
natural-language explanations, improving interpretability and trust. By
integrating fine-tuning and reflective reasoning across multimodal inputs,
FT-ARM advances the reliability, transparency, and clinical applicability of
automated wound assessment systems, addressing the critical need for consistent
and explainable PU staging to support improved patient care.


### [13] [Efficient License Plate Recognition via Pseudo-Labeled Supervision with Grounding DINO and YOLOv8](https://arxiv.org/abs/2510.25032)
*Zahra Ebrahimi Vargoorani, Amir Mohammad Ghoreyshi, Ching Yee Suen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºYOLOv8å’ŒåŠç›‘ç£å­¦ä¹ çš„è‡ªåŠ¨è½¦ç‰Œè¯†åˆ«ç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆGrounding DINOç”Ÿæˆçš„ä¼ªæ ‡ç­¾ä¸äººå·¥æ ‡æ³¨æ•°æ®ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜å¼‚çš„å¬å›ç‡å’Œå­—ç¬¦é”™è¯¯ç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è‡ªåŠ¨è½¦ç‰Œè¯†åˆ«ç³»ç»Ÿé¢ä¸´ç¯å¢ƒå› ç´ ï¼ˆå¦‚å…‰ç…§ã€é›¨æ°´ã€ç°å°˜ï¼‰ã€é«˜é€Ÿè½¦è¾†ã€å¤šå˜æ‘„åƒå¤´è§’åº¦ä»¥åŠä½è´¨é‡å›¾åƒç­‰æŒ‘æˆ˜ï¼Œè¿™äº›å› ç´ é™åˆ¶äº†ç°æœ‰ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›æŠ€æœ¯éš¾é¢˜ï¼Œæå‡ALPRç³»ç»Ÿåœ¨å¤æ‚å®é™…åœºæ™¯ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚

**Method:** é‡‡ç”¨åŸºäºYOLOv8çš„æ·±åº¦å­¦ä¹ ç­–ç•¥è¿›è¡Œè½¦ç‰Œæ£€æµ‹ä¸è¯†åˆ«ï¼Œå¹¶å¼•å…¥åŠç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆå°‘é‡äººå·¥æ ‡æ³¨æ•°æ®å’ŒGrounding DINOè§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾æ¥è®­ç»ƒæ£€æµ‹æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªåŠ¨æ ‡æ³¨å¤§é‡å›¾åƒå‡å°‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒæ ‡ç­¾è´¨é‡ã€‚

**Result:** åœ¨CENPARMIæ•°æ®é›†ä¸Šè¾¾åˆ°94%çš„å¬å›ç‡ï¼Œåœ¨UFPR-ALPRæ•°æ®é›†ä¸Šè¾¾åˆ°91%çš„å¬å›ç‡ï¼ŒåŒæ—¶æŠ¥å‘Šäº†ä¸¤ä¸ªæ•°æ®é›†çš„å­—ç¬¦é”™è¯¯ç‡ï¼Œä¸ºç³»ç»Ÿæ€§èƒ½æä¾›äº†å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ï¼Œç»“åˆåŠç›‘ç£å­¦ä¹ å’Œè§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæå‡è½¦ç‰Œè¯†åˆ«ç³»ç»Ÿçš„æ€§èƒ½ï¼Œå‡å°‘äººå·¥æ ‡æ³¨æˆæœ¬ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„å¤§è§„æ¨¡éƒ¨ç½²æä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶æ˜¾è‘—æé«˜äº†æ•°æ®æ ‡æ³¨æ•ˆç‡ã€‚

---

#### ğŸ“„ Abstract
Developing a highly accurate automatic license plate recognition system
(ALPR) is challenging due to environmental factors such as lighting, rain, and
dust. Additional difficulties include high vehicle speeds, varying camera
angles, and low-quality or low-resolution images. ALPR is vital in traffic
control, parking, vehicle tracking, toll collection, and law enforcement
applications. This paper proposes a deep learning strategy using YOLOv8 for
license plate detection and recognition tasks. This method seeks to enhance the
performance of the model using datasets from Ontario, Quebec, California, and
New York State. It achieved an impressive recall rate of 94% on the dataset
from the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and
91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised
learning framework, combining a small set of manually labeled data with
pseudo-labels generated by Grounding DINO to train our detection model.
Grounding DINO, a powerful vision-language model, automatically annotates many
images with bounding boxes for license plates, thereby minimizing the reliance
on labor-intensive manual labeling. By integrating human-verified and
model-generated annotations, we can scale our dataset efficiently while
maintaining label quality, which significantly enhances the training process
and overall model performance. Furthermore, it reports character error rates
for both datasets, providing additional insight into system performance.


### [14] [Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference Models](https://arxiv.org/abs/2510.25051)
*Shunjie-Fabian Zheng, Hyeonjun Lee, Thijs Kooi, Ali Diba*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œé€šè¿‡å°†2Dä¹³è…ºXçº¿æ‘„å½±çš„è§†è§‰ç‰¹å¾ä¸ä¸´åºŠå…ƒæ•°æ®å’Œåˆæˆæ”¾å°„å­¦æŠ¥å‘Šçš„ç»“æ„åŒ–æ–‡æœ¬æè¿°ç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡äº†ä¹³è…ºç™Œæ£€æµ‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨ç™Œç—‡æ£€æµ‹å’Œé’™åŒ–è¯†åˆ«æ–¹é¢ä¼˜äºå•æ¨¡æ€åŸºçº¿ï¼Œä¸ºå¼€å‘ä¸´åºŠå¯è¡Œçš„è§†è§‰è¯­è¨€æ¨¡å‹è¾…åŠ©è¯Šæ–­ç³»ç»Ÿå»ºç«‹äº†æ–°èŒƒå¼ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿåœ¨ä¸´åºŠéƒ¨ç½²ä¸­å­˜åœ¨å…³é”®å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šæ¨¡æ€æ•°æ®çš„ç»†å¾®è§£é‡Šæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä¸”ç”±äºéœ€è¦å…ˆå‰çš„ä¸´åºŠå†å²è€Œç¼ºä¹å¯è¡Œæ€§ã€‚ä¹³è…ºç™Œä½œä¸ºå‘è¾¾å›½å®¶å¥³æ€§æœ€å¸¸è§çš„æ¶æ€§è‚¿ç˜¤ï¼Œæ—©æœŸæ£€æµ‹å¯¹é™ä½æ­»äº¡ç‡è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨å¯è·å–çš„ä¸´åºŠä¿¡æ¯å’Œå½±åƒæ•°æ®çš„ååŒæ•ˆåº”ã€‚

**Method:** æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–æ¡†æ¶ï¼Œé€šè¿‡åˆ›æ–°çš„æ ‡è®°åŒ–æ¨¡å—å°†2Dä¹³è…ºXçº¿æ‘„å½±çš„è§†è§‰ç‰¹å¾ä¸æ¥è‡ªæ˜“è·å–ä¸´åºŠå…ƒæ•°æ®å’Œåˆæˆæ”¾å°„å­¦æŠ¥å‘Šçš„ç»“æ„åŒ–æ–‡æœ¬æè¿°è¿›è¡ŒååŒæ•´åˆã€‚è¯¥æ–¹æ³•ç­–ç•¥æ€§åœ°å°†å·ç§¯ç¥ç»ç½‘ç»œä¸è¯­è¨€è¡¨ç¤ºç›¸ç»“åˆï¼Œåœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒçš„åŒæ—¶å®ç°äº†ä¼˜äºåŸºäºè§†è§‰å˜æ¢å™¨æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶æ”¯æŒåœ¨ä¸åŒäººç¾¤ä¸­çš„å®é™…éƒ¨ç½²ã€‚

**Result:** é€šè¿‡åœ¨è·¨å›½é˜Ÿåˆ—ç­›æŸ¥ä¹³è…ºXçº¿æ‘„å½±æ•°æ®ä¸Šçš„è¯„ä¼°ï¼Œè¯¥å¤šæ¨¡æ€æ–¹æ³•åœ¨ç™Œç—‡æ£€æµ‹å’Œé’™åŒ–è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºä¼˜äºå•æ¨¡æ€åŸºçº¿çš„å“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šæ”¹è¿›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚è¯¥æ–¹æ³•è¯æ˜äº†è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬æè¿°çš„æœ‰æ•ˆèåˆèƒ½å¤Ÿæ˜¾è‘—æå‡è¯Šæ–­å‡†ç¡®æ€§å’Œä¸´åºŠå®ç”¨æ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå¼€å‘ä¸´åºŠå¯è¡Œçš„åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿå»ºç«‹äº†æ–°èŒƒå¼ï¼Œé€šè¿‡æœ‰æ•ˆçš„èåˆæœºåˆ¶å……åˆ†åˆ©ç”¨å½±åƒæ•°æ®å’Œä¸Šä¸‹æ–‡æ‚£è€…ä¿¡æ¯ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†å¤šæ¨¡æ€æ•´åˆåœ¨åŒ»ç–—å½±åƒåˆ†æä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæœªæ¥ä¸´åºŠéƒ¨ç½²æä¾›äº†å®ç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Breast cancer remains the most commonly diagnosed malignancy among women in
the developed world. Early detection through mammography screening plays a
pivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)
systems have shown promise in assisting radiologists, existing approaches face
critical limitations in clinical deployment - particularly in handling the
nuanced interpretation of multi-modal data and feasibility due to the
requirement of prior clinical history. This study introduces a novel framework
that synergistically combines visual features from 2D mammograms with
structured textual descriptors derived from easily accessible clinical metadata
and synthesized radiological reports through innovative tokenization modules.
Our proposed methods in this study demonstrate that strategic integration of
convolutional neural networks (ConvNets) with language representations achieves
superior performance to vision transformer-based models while handling
high-resolution images and enabling practical deployment across diverse
populations. By evaluating it on multi-national cohort screening mammograms,
our multi-modal approach achieves superior performance in cancer detection and
calcification identification compared to unimodal baselines, with particular
improvements. The proposed method establishes a new paradigm for developing
clinically viable VLM-based CAD systems that effectively leverage imaging data
and contextual patient information through effective fusion mechanisms.


### [15] [DRIP: Dynamic patch Reduction via Interpretable Pooling](https://arxiv.org/abs/2510.25067)
*Yusen Peng, Sachin Kumar*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºåŠ¨æ€è¡¥ä¸ç¼©å‡å¯è§£é‡Šæ± åŒ–æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”åˆå¹¶æ·±å±‚è§†è§‰ç¼–ç å™¨ä¸­çš„ä»¤ç‰Œï¼Œåœ¨ä¿æŒåˆ†ç±»å’Œé›¶æ ·æœ¬æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—å¤æ‚åº¦ã€‚è¯¥æ–¹æ³•åœ¨ImageNetä»å¤´è®­ç»ƒå’ŒCLIPå¯¹æ¯”é¢„è®­ç»ƒä¸­å‡éªŒè¯äº†æœ‰æ•ˆæ€§ï¼Œå¹¶æˆåŠŸåº”ç”¨äºç”Ÿç‰©å­¦é¢†åŸŸçš„å¤§è§„æ¨¡æŒç»­é¢„è®­ç»ƒã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹è™½ç„¶å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºå¤§è§„æ¨¡é¢„è®­ç»ƒçš„è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œç ”ç©¶è€…å¾€å¾€é¿å…ä»å¤´å¼€å§‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œé€šè¿‡é™ä½è®¡ç®—å¤æ‚åº¦æ¥æ¨åŠ¨å¤šæ¨¡æ€AIçš„å‘å±•ã€‚

**Method:** æå‡ºåŠ¨æ€è¡¥ä¸ç¼©å‡å¯è§£é‡Šæ± åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ®è¾“å…¥å›¾åƒè‡ªé€‚åº”åœ°åœ¨è§†è§‰ç¼–ç å™¨çš„æ·±å±‚åˆå¹¶ä»¤ç‰Œã€‚è¿™ç§åŠ¨æ€åˆå¹¶æœºåˆ¶èƒ½å¤Ÿæ˜¾è‘—å‡å°‘è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œé€‚ç”¨äºä¸åŒçš„é¢„è®­ç»ƒåœºæ™¯ã€‚

**Result:** åœ¨ImageNetä»å¤´è®­ç»ƒå’ŒCLIPå¯¹æ¯”é¢„è®­ç»ƒå®éªŒä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„GFLOPsé™ä½ï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¯”çš„åˆ†ç±»å’Œé›¶æ ·æœ¬æ€§èƒ½ã€‚åœ¨å¤§å‹ç”Ÿç‰©å­¦æ•°æ®é›†ä¸Šçš„æŒç»­é¢„è®­ç»ƒè¿›ä¸€æ­¥éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨ç§‘å­¦é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜åŠ¨æ€è¡¥ä¸ç¼©å‡æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡è®¡ç®—æ•ˆç‡ä¸æ¨¡å‹æ€§èƒ½ï¼Œä¸ºå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæä¾›äº†å¯è¡Œçš„æ•ˆç‡ä¼˜åŒ–æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä¸ä»…é€‚ç”¨äºé€šç”¨é¢†åŸŸï¼Œè¿˜èƒ½æ‰©å±•åˆ°ç§‘å­¦è®¡ç®—ç­‰ä¸“ä¸šé¢†åŸŸï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

---

#### ğŸ“„ Abstract
Recently, the advances in vision-language models, including contrastive
pretraining and instruction tuning, have greatly pushed the frontier of
multimodal AI. However, owing to the large-scale and hence expensive
pretraining, the efficiency concern has discouraged researchers from attempting
to pretrain a vision language model from scratch. In this work, we propose
Dynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the
input images and dynamically merges tokens in the deeper layers of a visual
encoder. Our results on both ImageNet training from scratch and CLIP
contrastive pretraining demonstrate a significant GFLOP reduction while
maintaining comparable classification/zero-shot performance. To further
validate our proposed method, we conduct continual pretraining on a large
biology dataset, extending its impact into scientific domains.


### [16] [Vision-Language Integration for Zero-Shot Scene Understanding in Real-World Environments](https://arxiv.org/abs/2510.25070)
*Manjunath Prasad Holenarasipura Rajiv, B. M. Vidyavathi*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è§†è§‰-è¯­è¨€é›†æˆæ¡†æ¶ï¼Œé€šè¿‡ç»Ÿä¸€é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°è§†è§‰ä¸æ–‡æœ¬æ¨¡æ€çš„è¯­ä¹‰å¯¹é½ï¼Œæ˜¾è‘—æå‡äº†é›¶æ ·æœ¬åœºæ™¯ç†è§£èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„é›¶æ ·æœ¬ç†è§£é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºè‡ªç„¶åœºæ™¯çš„å¤æ‚æ€§å’Œå¤šå˜æ€§è¦æ±‚æ¨¡å‹åœ¨æ²¡æœ‰å…ˆéªŒæ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹è¯†åˆ«æ–°å¯¹è±¡ã€åŠ¨ä½œå’Œä¸Šä¸‹æ–‡ï¼Œè¿™éœ€è¦è§£å†³è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚

**Method:** è¯¥æ–¹æ³•å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ï¼Œå°†è§†è§‰è¾“å…¥å’Œæ–‡æœ¬æç¤ºåµŒå…¥åˆ°å…±äº«ç©ºé—´ä¸­ï¼Œéšåé€šè¿‡å¤šæ¨¡æ€èåˆå’Œæ¨ç†å±‚è¿›è¡Œä¸Šä¸‹æ–‡è§£é‡Šï¼Œé›†æˆäº†CLIPã€ViTç­‰é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ä¸GPTæ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

**Result:** åœ¨Visual Genomeã€COCOã€ADE20Kå’Œè‡ªå®šä¹‰çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‰©ä½“è¯†åˆ«ã€æ´»åŠ¨æ£€æµ‹å’Œåœºæ™¯æè¿°ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ¨¡å‹ï¼Œå®ç°äº†é«˜è¾¾18%çš„top-1å‡†ç¡®ç‡æå‡å’Œè¯­ä¹‰è¿è´¯æ€§æŒ‡æ ‡çš„æ˜¾è‘—å¢ç›Šã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†è·¨æ¨¡æ€å¯¹é½å’Œè¯­è¨€æ¥åœ°åœ¨å¢å¼ºçœŸå®ä¸–ç•Œåœºæ™¯ç†è§£æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºæ›´é²æ£’çš„é›¶æ ·æœ¬è§†è§‰ç†è§£ç³»ç»Ÿæä¾›äº†é‡è¦è§è§£ï¼Œå¹¶å±•ç¤ºäº†è§†è§‰-è¯­è¨€é›†æˆæ¡†æ¶åœ¨å¤æ‚åœºæ™¯ç†è§£ä¸­çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Zero-shot scene understanding in real-world settings presents major
challenges due to the complexity and variability of natural scenes, where
models must recognize new objects, actions, and contexts without prior labeled
examples. This work proposes a vision-language integration framework that
unifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models
(e.g., GPT-based architectures) to achieve semantic alignment between visual
and textual modalities. The goal is to enable robust zero-shot comprehension of
scenes by leveraging natural language as a bridge to generalize over unseen
categories and contexts. Our approach develops a unified model that embeds
visual inputs and textual prompts into a shared space, followed by multimodal
fusion and reasoning layers for contextual interpretation. Experiments on
Visual Genome, COCO, ADE20K, and custom real-world datasets demonstrate
significant gains over state-of-the-art zero-shot models in object recognition,
activity detection, and scene captioning. The proposed system achieves up to
18% improvement in top-1 accuracy and notable gains in semantic coherence
metrics, highlighting the effectiveness of cross-modal alignment and language
grounding in enhancing generalization for real-world scene understanding.


### [17] [Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection](https://arxiv.org/abs/2510.25094)
*Chanhyeong Yang, Taehoon Song, Jihwan Park, Hyunwoo J. Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†VDRPæ¡†æ¶ï¼Œä¸€ç§é’ˆå¯¹é›¶æ ·æœ¬äºº-ç‰©äº¤äº’æ£€æµ‹çš„è§†è§‰å¤šæ ·æ€§å’ŒåŒºåŸŸæ„ŸçŸ¥æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥è§†è§‰å¤šæ ·æ€§æ„ŸçŸ¥æç¤ºå­¦ä¹ å’ŒåŒºåŸŸç‰¹å®šæ¦‚å¿µæ£€ç´¢ï¼Œæœ‰æ•ˆè§£å†³äº†åŒç±»è§†è§‰å¤šæ ·æ€§å’Œå¼‚ç±»è§†è§‰çº ç¼ é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰åŸºäºCLIPç­‰é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬äºº-ç‰©äº¤äº’æ£€æµ‹æ–¹æ³•åœ¨å¤„ç†äº¤äº’çš„è§†è§‰å¤æ‚æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯æ— æ³•æœ‰æ•ˆåº”å¯¹åŒç±»è§†è§‰å¤šæ ·æ€§ï¼ˆåŒä¸€åŠ¨è¯åœ¨ä¸åŒå§¿æ€å’Œä¸Šä¸‹æ–‡ä¸­çš„è§†è§‰è¡¨ç°å·®å¼‚ï¼‰å’Œå¼‚ç±»è§†è§‰çº ç¼ ï¼ˆä¸åŒåŠ¨è¯äº§ç”Ÿç›¸ä¼¼è§†è§‰æ¨¡å¼ï¼‰è¿™ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚

**Method:** VDRPæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè§†è§‰å¤šæ ·æ€§æ„ŸçŸ¥æç¤ºå­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å°†åˆ†ç»„è§†è§‰æ–¹å·®æ³¨å…¥ä¸Šä¸‹æ–‡åµŒå…¥å¹¶åº”ç”¨é«˜æ–¯æ‰°åŠ¨æ¥æ•æ‰åŠ¨è¯çš„å¤šæ ·åŒ–è§†è§‰å˜åŒ–ï¼›åŒºåŸŸç‰¹å®šæ¦‚å¿µæ£€ç´¢æœºåˆ¶ï¼Œä»äººã€ç‰©å’Œè”åˆåŒºåŸŸæå–æ¦‚å¿µæ¥å¢å¼ºå¤šæ ·æ€§æ„ŸçŸ¥æç¤ºåµŒå…¥ï¼Œç”Ÿæˆèƒ½å¤Ÿæå‡åŠ¨è¯çº§åˆ«åŒºåˆ†åº¦çš„åŒºåŸŸæ„ŸçŸ¥æç¤ºã€‚

**Result:** åœ¨HICO-DETåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››ç§é›¶æ ·æœ¬è¯„ä¼°è®¾ç½®ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œæœ‰æ•ˆè§£å†³äº†åŒç±»è§†è§‰å¤šæ ·æ€§å’Œå¼‚ç±»è§†è§‰çº ç¼ é—®é¢˜ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚è§†è§‰äº¤äº’æ¨¡å¼æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç»“åˆè§†è§‰å¤šæ ·æ€§å»ºæ¨¡å’ŒåŒºåŸŸæ„ŸçŸ¥æç¤ºå­¦ä¹ ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡é›¶æ ·æœ¬äºº-ç‰©äº¤äº’æ£€æµ‹çš„æ€§èƒ½ï¼Œä¸ºå¤„ç†å¤æ‚è§†è§‰äº¤äº’æ¨¡å¼æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¹¶ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦è§†è§‰ç†è§£ä»»åŠ¡ä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚

---

#### ğŸ“„ Abstract
Zero-shot Human-Object Interaction detection aims to localize humans and
objects in an image and recognize their interaction, even when specific
verb-object pairs are unseen during training. Recent works have shown promising
results using prompt learning with pretrained vision-language models such as
CLIP, which align natural language prompts with visual features in a shared
embedding space. However, existing approaches still fail to handle the visual
complexity of interaction, including (1) intra-class visual diversity, where
instances of the same verb appear in diverse poses and contexts, and (2)
inter-class visual entanglement, where distinct verbs yield visually similar
patterns. To address these challenges, we propose VDRP, a framework for Visual
Diversity and Region-aware Prompt learning. First, we introduce a visual
diversity-aware prompt learning strategy that injects group-wise visual
variance into the context embedding. We further apply Gaussian perturbation to
encourage the prompts to capture diverse visual variations of a verb. Second,
we retrieve region-specific concepts from the human, object, and union regions.
These are used to augment the diversity-aware prompt embeddings, yielding
region-aware prompts that enhance verb-level discrimination. Experiments on the
HICO-DET benchmark demonstrate that our method achieves state-of-the-art
performance under four zero-shot evaluation settings, effectively addressing
both intra-class diversity and inter-class visual entanglement. Code is
available at https://github.com/mlvlab/VDRP.


### [18] [EA3D: Online Open-World 3D Object Extraction from Streaming Videos](https://arxiv.org/abs/2510.25146)
*Xiaoyu Zhou, Jingqi Wang, Yuang Jia, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ExtractAnything3Dï¼ˆEA3Dï¼‰ï¼Œä¸€ä¸ªç”¨äºå¼€æ”¾ä¸–ç•Œ3Dç‰©ä½“æå–çš„ç»Ÿä¸€åœ¨çº¿æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶å®ç°å‡ ä½•é‡å»ºå’Œæ•´ä½“åœºæ™¯ç†è§£ï¼Œé€šè¿‡åŠ¨æ€é›†æˆè§†è§‰è¯­è¨€çŸ¥è¯†å’Œåœ¨çº¿é«˜æ–¯ç‰¹å¾æ›´æ–°æ¥æ”¯æŒå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰3Dåœºæ™¯ç†è§£æ–¹æ³•å—é™äºç¦»çº¿æ”¶é›†çš„å¤šè§†è§’æ•°æ®æˆ–é¢„æ„å»ºçš„3Då‡ ä½•ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è§£å†³åœ¨çº¿åŠ¨æ€ç¯å¢ƒä¸‹åŒæ—¶è¿›è¡Œå‡ ä½•é‡å»ºå’Œè¯­ä¹‰ç†è§£çš„æŒ‘æˆ˜ï¼Œå¡«è¡¥å¼€æ”¾ä¸–ç•Œ3Dç‰©ä½“æå–æ¡†æ¶çš„ç©ºç™½ã€‚

**Method:** EA3Dä½¿ç”¨è§†è§‰è¯­è¨€å’Œ2Dè§†è§‰åŸºç¡€ç¼–ç å™¨åŠ¨æ€è§£é‡Šè§†é¢‘æµå¸§ï¼Œé€šè¿‡å‰é¦ˆåœ¨çº¿æ›´æ–°ç­–ç•¥å°†ç‰©ä½“çº§çŸ¥è¯†é›†æˆåˆ°é«˜æ–¯ç‰¹å¾å›¾ä¸­ï¼Œç»“åˆè¿­ä»£è§†è§‰é‡Œç¨‹è®¡ä¼°è®¡å’Œå¢é‡ç‰¹å¾æ›´æ–°ï¼Œå¹¶é‡‡ç”¨å¾ªç¯è”åˆä¼˜åŒ–æ¨¡å—å¼•å¯¼æ¨¡å‹å…³æ³¨æ„Ÿå…´è¶£åŒºåŸŸã€‚

**Result:** åœ¨å¤šæ ·åŒ–åŸºå‡†å’Œä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒEA3Dåœ¨ç…§ç‰‡çº§çœŸå®æ„Ÿæ¸²æŸ“ã€è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ã€3Dè¾¹ç•Œæ¡†å’Œè¯­ä¹‰å æ®ä¼°è®¡ä»¥åŠ3Dç½‘æ ¼ç”Ÿæˆç­‰ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼ŒéªŒè¯äº†æ¡†æ¶çš„ç»Ÿä¸€æ€§å’Œé«˜æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€é«˜æ•ˆçš„åœ¨çº¿3Dé‡å»ºå’Œæ•´ä½“åœºæ™¯ç†è§£æ¡†æ¶ï¼Œä¸ºå¤šç§ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†åŸºç¡€æ”¯æŒï¼Œæ¨åŠ¨äº†å¼€æ”¾ä¸–ç•Œ3Dåœºæ™¯ç†è§£çš„å‘å±•æ–¹å‘ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Current 3D scene understanding methods are limited by offline-collected
multi-view data or pre-constructed 3D geometry. In this paper, we present
ExtractAnything3D (EA3D), a unified online framework for open-world 3D object
extraction that enables simultaneous geometric reconstruction and holistic
scene understanding. Given a streaming video, EA3D dynamically interprets each
frame using vision-language and 2D vision foundation encoders to extract
object-level knowledge. This knowledge is integrated and embedded into a
Gaussian feature map via a feed-forward online update strategy. We then
iteratively estimate visual odometry from historical frames and incrementally
update online Gaussian features with new observations. A recurrent joint
optimization module directs the model's attention to regions of interest,
simultaneously enhancing both geometric reconstruction and semantic
understanding. Extensive experiments across diverse benchmarks and tasks,
including photo-realistic rendering, semantic and instance segmentation, 3D
bounding box and semantic occupancy estimation, and 3D mesh generation,
demonstrate the effectiveness of EA3D. Our method establishes a unified and
efficient framework for joint online 3D reconstruction and holistic scene
understanding, enabling a broad range of downstream tasks.


### [19] [Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD Generation](https://arxiv.org/abs/2510.25163)
*Wenhao Zheng, Chenwei Sun, Wenbo Zhang, Jiancheng Lv, Xianggen Liu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ç›®æ ‡å¼•å¯¼è´å¶æ–¯æµç½‘ç»œï¼ˆTGBFNï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå®šé‡çº¦æŸCADç”Ÿæˆçš„æ–°æ¡†æ¶ï¼Œé¦–æ¬¡åœ¨ç»Ÿä¸€çš„è¿ç»­å¯å¾®åˆ†å‚æ•°ç©ºé—´ä¸­å¤„ç†CADåºåˆ—çš„å¤šæ¨¡æ€ç‰¹æ€§ï¼Œå¹¶åœ¨å•æ¡ä»¶å’Œå¤šæ¡ä»¶çº¦æŸç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒå’ŒéŸ³é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†é’ˆå¯¹å¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚å‚æ•°åŒ–CADåºåˆ—ï¼‰çš„ç”Ÿæˆå»ºæ¨¡æŠ€æœ¯å‘å±•æ»åï¼Œä¸»è¦æŒ‘æˆ˜åœ¨äºå¤„ç†é•¿ç¨‹çº¦æŸå’Œå‚æ•°æ•æ„Ÿæ€§ï¼Œè¿™é™åˆ¶äº†CADç”Ÿæˆçš„è´¨é‡å’Œå¯æ§æ€§ã€‚

**Method:** TGBFNæ¡†æ¶é€šè¿‡å°†CADåºåˆ—çš„ç¦»æ•£å‘½ä»¤å’Œè¿ç»­å‚æ•°ç»Ÿä¸€æ˜ å°„åˆ°è¿ç»­å¯å¾®åˆ†å‚æ•°ç©ºé—´æ¥å¤„ç†å¤šæ¨¡æ€é—®é¢˜ï¼Œå¹¶å¼•å…¥å¼•å¯¼è´å¶æ–¯æµæœºåˆ¶æ¥ç©¿é€å‚æ•°æ›´æ–°æ ¸ï¼Œä»è€Œå®ç°å¯¹CADå±æ€§çš„ç²¾ç¡®æ§åˆ¶ã€‚

**Result:** åœ¨æ–°å»ºçš„å®šé‡çº¦æŸCADç”Ÿæˆæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTGBFNåœ¨å•æ¡ä»¶å’Œå¤šæ¡ä»¶çº¦æŸç”Ÿæˆä»»åŠ¡ä¸­å‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦ä¸”æ¡ä»¶æ„ŸçŸ¥çš„CADåºåˆ—ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åœ¨ç»Ÿä¸€è¿ç»­ç©ºé—´ä¸­å¤„ç†CADå¤šæ¨¡æ€æ•°æ®çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå‚æ•°åŒ–CADç”Ÿæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¹¶å±•ç¤ºäº†å¼•å¯¼è´å¶æ–¯æµåœ¨æ§åˆ¶ç”Ÿæˆå±æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºå¤æ‚å·¥ç¨‹è®¾è®¡çš„è‡ªåŠ¨åŒ–ç”Ÿæˆå¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Deep generative models, such as diffusion models, have shown promising
progress in image generation and audio generation via simplified continuity
assumptions. However, the development of generative modeling techniques for
generating multi-modal data, such as parametric CAD sequences, still lags
behind due to the challenges in addressing long-range constraints and parameter
sensitivity. In this work, we propose a novel framework for quantitatively
constrained CAD generation, termed Target-Guided Bayesian Flow Network (TGBFN).
For the first time, TGBFN handles the multi-modality of CAD sequences (i.e.,
discrete commands and continuous parameters) in a unified continuous and
differentiable parameter space rather than in the discrete data space. In
addition, TGBFN penetrates the parameter update kernel and introduces a guided
Bayesian flow to control the CAD properties. To evaluate TGBFN, we construct a
new dataset for quantitatively constrained CAD generation. Extensive
comparisons across single-condition and multi-condition constrained generation
tasks demonstrate that TGBFN achieves state-of-the-art performance in
generating high-fidelity, condition-aware CAD sequences. The code is available
at https://github.com/scu-zwh/TGBFN.


### [20] [MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding](https://arxiv.org/abs/2510.25327)
*Runxi Huang, Mingxuan Yu, Mingyu Tsoi, Xiaomin Ouyang*

#### ğŸ§© TL;DR
MMEdgeæ˜¯ä¸€ä¸ªåŸºäºæµæ°´çº¿æ„ŸçŸ¥å’Œç¼–ç çš„ç«¯ä¾§å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡ç»†ç²’åº¦å¢é‡è®¡ç®—å’Œè·¨æ¨¡æ€ä¼˜åŒ–ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—é™ä½ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•é€šå¸¸å¿½ç•¥äº†æ„ŸçŸ¥åŠ¨æ€ä¸æ¨¡å‹æ‰§è¡Œä¹‹é—´çš„ç´§å¯†è€¦åˆä»¥åŠå¤æ‚çš„æ¨¡æ€é—´ä¾èµ–å…³ç³»ï¼Œè€Œè¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶å¤šæ¨¡æ€æ¨ç†å¯¹äºè‡ªåŠ¨é©¾é©¶ã€äººæœºäº¤äº’å’Œç§»åŠ¨å¥åº·ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚

**Method:** MMEdgeå°†æ•´ä¸ªæ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºä¸€ç³»åˆ—ç»†ç²’åº¦çš„æ„ŸçŸ¥å’Œç¼–ç å•å…ƒï¼Œé‡‡ç”¨å¢é‡è®¡ç®—æ–¹å¼å¤„ç†åˆ°è¾¾æ•°æ®ï¼›å¼•å…¥è½»é‡çº§æ—¶é—´èšåˆæ¨¡å—æ•è·è·¨æµæ°´çº¿å•å…ƒçš„ä¸°å¯Œæ—¶é—´åŠ¨æ€ï¼›åŒ…å«è‡ªé€‚åº”å¤šæ¨¡æ€é…ç½®ä¼˜åŒ–å™¨å’Œè·¨æ¨¡æ€æ¨æµ‹è·³è¿‡æœºåˆ¶ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ä¼˜é…ç½®å¹¶åœ¨é¢„æµ‹ç½®ä¿¡åº¦è¶³å¤Ÿæ—¶è·³è¿‡è¾ƒæ…¢æ¨¡æ€çš„æœªæ¥å•å…ƒã€‚

**Result:** åœ¨ä¸¤ä¸ªå…¬å…±å¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„è¯„ä¼°ä»¥åŠåœ¨çœŸå®æ— äººæœºå¤šæ¨¡æ€æµ‹è¯•å¹³å°ä¸Šçš„éƒ¨ç½²ç»“æœè¡¨æ˜ï¼ŒMMEdgeåœ¨å„ç§ç³»ç»Ÿå’Œæ•°æ®åŠ¨æ€ä¸‹æ˜¾è‘—é™ä½äº†ç«¯åˆ°ç«¯å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒäº†é«˜ä»»åŠ¡ç²¾åº¦ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æµæ°´çº¿æ„ŸçŸ¥è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆè§£è€¦å¤šæ¨¡æ€æ¨ç†ä¸­çš„æ„ŸçŸ¥ä¸è®¡ç®—ï¼Œä¸ºèµ„æºå—é™è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶å¤šæ¨¡æ€åº”ç”¨æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†è·¨æ¨¡æ€ä¼˜åŒ–å’Œæ—©æœŸå†³ç­–çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Real-time multimodal inference on resource-constrained edge devices is
essential for applications such as autonomous driving, human-computer
interaction, and mobile health. However, prior work often overlooks the tight
coupling between sensing dynamics and model execution, as well as the complex
inter-modality dependencies. In this paper, we propose MMEdge, an new on-device
multi-modal inference framework based on pipelined sensing and encoding.
Instead of waiting for complete sensor inputs, MMEdge decomposes the entire
inference process into a sequence of fine-grained sensing and encoding units,
allowing computation to proceed incrementally as data arrive. MMEdge also
introduces a lightweight but effective temporal aggregation module that
captures rich temporal dynamics across different pipelined units to maintain
accuracy performance. Such pipelined design also opens up opportunities for
fine-grained cross-modal optimization and early decision-making during
inference. To further enhance system performance under resource variability and
input data complexity, MMEdge incorporates an adaptive multimodal configuration
optimizer that dynamically selects optimal sensing and model configurations for
each modality under latency constraints, and a cross-modal speculative skipping
mechanism that bypasses future units of slower modalities when early
predictions reach sufficient confidence. We evaluate MMEdge using two public
multimodal datasets and deploy it on a real-world unmanned aerial vehicle
(UAV)-based multimodal testbed. The results show that MMEdge significantly
reduces end-to-end latency while maintaining high task accuracy across various
system and data dynamics.


### [21] [$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction](https://arxiv.org/abs/2510.25173)
*Kejing Xia, Jidong Jia, Ke Jin, Yucai Bai, Li Sun, Dacheng Tao, Youjian Zhang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€LiDARçš„åŸå¸‚åœºæ™¯é‡å»ºæ¡†æ¶DÂ²GSï¼Œé€šè¿‡å¤šè§†è§’æ·±åº¦é¢„æµ‹å’Œæ‰©æ•£å…ˆéªŒè·å¾—æ¯”LiDARæ›´å¯†é›†å‡†ç¡®çš„å‡ ä½•å…ˆéªŒï¼Œåœ¨Waymoæ•°æ®é›†ä¸Šè¶…è¶Šäº†åŒ…æ‹¬ä½¿ç”¨çœŸå®LiDARæ•°æ®çš„æ–¹æ³•åœ¨å†…çš„ç°æœ‰æœ€ä½³æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸå¸‚åœºæ™¯é‡å»ºæ–¹æ³•é€šå¸¸ä¾èµ–å¤šæ¨¡æ€ä¼ æ„Ÿå™¨è¾“å…¥ï¼ˆå¦‚LiDARå’Œå›¾åƒï¼‰ï¼Œä½†è·å–ç²¾ç¡®LiDARæ•°æ®å­˜åœ¨æŒ‘æˆ˜ï¼šéœ€è¦ç²¾ç¡®çš„æ—¶ç©ºæ ‡å®šï¼Œä¸”LiDARä¸ç›¸æœºå®‰è£…ä½ç½®ä¸åŒä¼šäº§ç”Ÿé‡æŠ•å½±è¯¯å·®ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›é™åˆ¶ï¼Œå¼€å‘æ— éœ€LiDARçš„é«˜è´¨é‡åŸå¸‚åœºæ™¯é‡å»ºæ–¹æ³•ã€‚

**Method:** æå‡ºDÂ²GSæ¡†æ¶ï¼Œé¦–å…ˆé€šè¿‡å¤šè§†è§’åº¦é‡æ·±åº¦é¢„æµ‹åæŠ•å½±åˆå§‹åŒ–å¯†é›†ç‚¹äº‘ï¼Œé‡‡ç”¨æ¸è¿›å¼å‰ªæç­–ç•¥ä¼˜åŒ–å…¨å±€ä¸€è‡´æ€§ï¼›å…¶æ¬¡é€šè¿‡æ·±åº¦å¢å¼ºå™¨è”åˆä¼˜åŒ–é«˜æ–¯å‡ ä½•å’Œé¢„æµ‹æ·±åº¦ï¼Œåˆ©ç”¨æ·±åº¦åŸºç¡€æ¨¡å‹çš„æ‰©æ•£å…ˆéªŒå¢å¼ºé«˜æ–¯æ¸²æŸ“çš„æ·±åº¦å›¾ï¼›æœ€ååœ¨é“è·¯åŒºåŸŸçº¦æŸé«˜æ–¯å½¢çŠ¶å’Œæ³•å‘é‡å±æ€§ä»¥æ”¹è¿›åœ°é¢å‡ ä½•ç²¾åº¦ã€‚

**Result:** åœ¨Waymoæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå³ä½¿ä¸ä½¿ç”¨çœŸå®LiDARæ•°æ®çš„æ–¹æ³•ç›¸æ¯”ï¼Œä¹Ÿèƒ½äº§ç”Ÿæ›´å‡†ç¡®çš„å‡ ä½•é‡å»ºç»“æœã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ— éœ€LiDARä¼ æ„Ÿå™¨å³å¯å®ç°é«˜è´¨é‡åŸå¸‚åœºæ™¯é‡å»ºçš„å¯è¡Œæ€§ï¼Œé€šè¿‡æ·±åº¦é¢„æµ‹å’Œæ‰©æ•£å…ˆéªŒçš„ç»„åˆå¯ä»¥äº§ç”Ÿæ¯”å®é™…LiDARæ•°æ®æ›´å¯†é›†å‡†ç¡®çš„å‡ ä½•å…ˆéªŒï¼Œä¸ºè‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åœºæ™¯é‡å»ºæä¾›äº†æ›´å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Recently, Gaussian Splatting (GS) has shown great potential for urban scene
reconstruction in the field of autonomous driving. However, current urban scene
reconstruction methods often depend on multimodal sensors as inputs,
\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR
point clouds can largely mitigate ill-posedness in reconstruction, acquiring
such accurate LiDAR data is still challenging in practice: i) precise
spatiotemporal calibration between LiDAR and other sensors is required, as they
may not capture data simultaneously; ii) reprojection errors arise from spatial
misalignment when LiDAR and cameras are mounted at different locations. To
avoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a
LiDAR-free urban scene reconstruction framework. In this work, we obtain
geometry priors that are as effective as LiDAR while being denser and more
accurate. $\textbf{First}$, we initialize a dense point cloud by
back-projecting multi-view metric depth predictions. This point cloud is then
optimized by a Progressive Pruning strategy to improve the global consistency.
$\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense
metric depth via a Depth Enhancer. Specifically, we leverage diffusion priors
from a depth foundation model to enhance the depth maps rendered by Gaussians.
In turn, the enhanced depths provide stronger geometric constraints during
Gaussian training. $\textbf{Finally}$, we improve the accuracy of ground
geometry by constraining the shape and normal attributes of Gaussians within
road regions. Extensive experiments on the Waymo dataset demonstrate that our
method consistently outperforms state-of-the-art methods, producing more
accurate geometry even when compared with those using ground-truth LiDAR data.


### [22] [Test-Time Adaptive Object Detection with Foundation Model](https://arxiv.org/abs/2510.25175)
*Yingjie Gao, Yanan Zhang, Zhi Cai, Di Huang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªåŸºäºåŸºç¡€æ¨¡å‹çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€æç¤ºè°ƒä¼˜å’Œå®ä¾‹åŠ¨æ€è®°å¿†æ¨¡å—ï¼Œåœ¨æ— éœ€æºæ•°æ®çš„æƒ…å†µä¸‹å®ç°äº†è·¨åŸŸå’Œè·¨ç±»åˆ«çš„è‡ªé€‚åº”æ£€æµ‹ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•ä¸¥é‡ä¾èµ–æºåŸŸç»Ÿè®¡ç‰¹å¾ï¼Œå¹¶å‡è®¾æºåŸŸå’Œç›®æ ‡åŸŸå…·æœ‰ç›¸åŒçš„ç±»åˆ«ç©ºé—´ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨çœŸå®å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨æ¶ˆé™¤å¯¹æºæ•°æ®çš„ä¾èµ–å¹¶å…‹æœä¼ ç»Ÿé—­é›†é™åˆ¶ï¼Œå®ç°æ›´çµæ´»çš„è‡ªé€‚åº”æ£€æµ‹ã€‚

**Method:** æå‡ºå¤šæ¨¡æ€æç¤ºå‡å€¼æ•™å¸ˆæ¡†æ¶ï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰æç¤ºè°ƒä¼˜ä»¥å‚æ•°é«˜æ•ˆæ–¹å¼é€‚åº”æµ‹è¯•æ•°æ®çš„è¯­è¨€å’Œè§†è§‰è¡¨ç¤ºç©ºé—´ï¼›è®¾è®¡æµ‹è¯•æ—¶çƒ­å¯åŠ¨ç­–ç•¥ä¿æŠ¤è§†è§‰åˆ†æ”¯è¡¨ç¤ºèƒ½åŠ›ï¼›æ„å»ºå®ä¾‹åŠ¨æ€è®°å¿†æ¨¡å—å­˜å‚¨é«˜è´¨é‡ä¼ªæ ‡ç­¾ï¼Œå¹¶æå‡ºè®°å¿†å¢å¼ºå’Œè®°å¿†å¹»è§‰ç­–ç•¥æå‡é¢„æµ‹è´¨é‡ã€‚

**Result:** åœ¨è·¨æŸåå’Œè·¨æ•°æ®é›†åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æŒç»­ä¼˜äºå…ˆå‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œèƒ½å¤Ÿé€‚åº”ä»»æ„è·¨åŸŸå’Œè·¨ç±»åˆ«çš„ç›®æ ‡æ•°æ®ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚

**Conclusion:** è¯¥æ–¹æ³•é¦–æ¬¡å®ç°äº†æ— éœ€æºæ•°æ®çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹ï¼Œçªç ´äº†ä¼ ç»Ÿé—­é›†å‡è®¾é™åˆ¶ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹çš„ç›®æ ‡æ£€æµ‹æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨è‡ªé€‚åº”æ£€æµ‹ä»»åŠ¡ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
In recent years, test-time adaptive object detection has attracted increasing
attention due to its unique advantages in online domain adaptation, which
aligns more closely with real-world application scenarios. However, existing
approaches heavily rely on source-derived statistical characteristics while
making the strong assumption that the source and target domains share an
identical category space. In this paper, we propose the first foundation
model-powered test-time adaptive object detection method that eliminates the
need for source data entirely and overcomes traditional closed-set limitations.
Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework for
vision-language detector-driven test-time adaptation, which incorporates text
and visual prompt tuning to adapt both language and vision representation
spaces on the test data in a parameter-efficient manner. Correspondingly, we
propose a Test-time Warm-start strategy tailored for the visual prompts to
effectively preserve the representation capability of the vision branch.
Furthermore, to guarantee high-quality pseudo-labels in every test batch, we
maintain an Instance Dynamic Memory (IDM) module that stores high-quality
pseudo-labels from previous test samples, and propose two novel
strategies-Memory Enhancement and Memory Hallucination-to leverage IDM's
high-quality instances for enhancing original predictions and hallucinating
images without available pseudo-labels, respectively. Extensive experiments on
cross-corruption and cross-dataset benchmarks demonstrate that our method
consistently outperforms previous state-of-the-art methods, and can adapt to
arbitrary cross-domain and cross-category target data. Code is available at
https://github.com/gaoyingjay/ttaod_foundation.


### [23] [AI-Powered Early Detection of Critical Diseases using Image Processing and Audio Analysis](https://arxiv.org/abs/2510.25199)
*Manisha More, Kavya Bhand, Kaustubh Mukdam, Kavya Sharma, Manas Kawtikwar, Hridayansh Kaware, Prajwal Kavhar*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€äººå·¥æ™ºèƒ½è¯Šæ–­æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå›¾åƒåˆ†æã€çƒ­æˆåƒå’ŒéŸ³é¢‘ä¿¡å·å¤„ç†æŠ€æœ¯ï¼Œå®ç°äº†å¯¹çš®è‚¤ç™Œã€è¡€ç®¡è¡€æ “å’Œå¿ƒè‚ºå¼‚å¸¸çš„æ—©æœŸæ£€æµ‹ã€‚è¯¥ç³»ç»Ÿåœ¨ä¿æŒè½»é‡çº§çš„åŒæ—¶å®ç°äº†ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸ç«äº‰çš„æ€§èƒ½ï¼Œä¸ºå¯æ‰©å±•çš„å®æ—¶AIé¢„è¯Šæ–­åŒ»ç–—è§£å†³æ–¹æ¡ˆæä¾›äº†å¯è¡Œè·¯å¾„ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è¯Šæ–­æŠ€æœ¯é€šå¸¸æˆæœ¬é«˜æ˜‚ã€å…·æœ‰ä¾µå…¥æ€§ä¸”åœ¨ä½èµ„æºåœ°åŒºéš¾ä»¥è·å–ï¼Œè€Œæ—©æœŸè¯Šæ–­å¯¹äºæé«˜æ‚£è€…ç”Ÿå­˜ç‡å’Œé™ä½æ²»ç–—æˆæœ¬è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€åŒ»ç–—å¯åŠæ€§é—®é¢˜ï¼Œå¼€å‘ä¸€ç§èƒ½å¤Ÿåœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²çš„å¤šæ¨¡æ€AIè¯Šæ–­æ–¹æ¡ˆã€‚

**Method:** é‡‡ç”¨å¤šæ¨¡æ€AIæ¡†æ¶æ•´åˆä¸‰ç§è¯Šæ–­æ¨¡å¼ï¼šä½¿ç”¨åœ¨ISIC 2019æ•°æ®é›†ä¸Šå¾®è°ƒçš„MobileNetV2å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œçš®è‚¤ç—…å˜åˆ†ç±»ï¼›é‡‡ç”¨æ”¯æŒå‘é‡æœºç»“åˆæ‰‹å·¥ç‰¹å¾è¿›è¡Œçƒ­æˆåƒè¡€æ “æ£€æµ‹ï¼›åˆ©ç”¨Melé¢‘ç‡å€’è°±ç³»æ•°ç‰¹å¾æå–å’Œéšæœºæ£®æ—åˆ†ç±»å™¨å¤„ç†å¿ƒè‚ºå£°éŸ³æ•°æ®ã€‚

**Result:** çš®è‚¤ç™Œæ£€æµ‹è¾¾åˆ°89.3%å‡†ç¡®ç‡ã€91.6%çµæ•åº¦å’Œ88.2%ç‰¹å¼‚æ€§ï¼›çƒ­æˆåƒè¡€æ “æ£€æµ‹åœ¨åˆæˆå’Œä¸´åºŠæ•°æ®ä¸Šè·å¾—86.4%å‡†ç¡®ç‡å’Œ0.89 AUCï¼›å¿ƒè‚ºå¼‚å¸¸åˆ†æè¾¾åˆ°87.2%å‡†ç¡®ç‡å’Œ85.7%çµæ•åº¦ã€‚ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿåœ¨ä¿æŒè½»é‡çº§çš„åŒæ—¶å®ç°äº†ç«äº‰æ€§æ€§èƒ½ã€‚

**Conclusion:** è¯¥å¤šæ¨¡æ€AIæ¡†æ¶ä¸ºå¯æ‰©å±•ã€å®æ—¶ä¸”æ˜“äºè·å–çš„é¢„è¯Šæ–­åŒ»ç–—è§£å†³æ–¹æ¡ˆæä¾›äº†æœ‰å‰æ™¯çš„æŠ€æœ¯è·¯å¾„ï¼Œç‰¹åˆ«é€‚åˆåœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ•´åˆå¤šç§è¯Šæ–­æ¨¡å¼ï¼Œå¯ä»¥åœ¨ä¿æŒæ¨¡å‹è½»é‡åŒ–çš„åŒæ—¶å®ç°å‡†ç¡®çš„æ—©æœŸç–¾ç—…æ£€æµ‹ï¼Œä¸ºæ”¹å–„å…¨çƒåŒ»ç–—å¯åŠæ€§æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚

---

#### ğŸ“„ Abstract
Early diagnosis of critical diseases can significantly improve patient
survival and reduce treatment costs. However, existing diagnostic techniques
are often costly, invasive, and inaccessible in low-resource regions. This
paper presents a multimodal artificial intelligence (AI) diagnostic framework
integrating image analysis, thermal imaging, and audio signal processing for
early detection of three major health conditions: skin cancer, vascular blood
clots, and cardiopulmonary abnormalities. A fine-tuned MobileNetV2
convolutional neural network was trained on the ISIC 2019 dataset for skin
lesion classification, achieving 89.3% accuracy, 91.6% sensitivity, and 88.2%
specificity. A support vector machine (SVM) with handcrafted features was
employed for thermal clot detection, achieving 86.4% accuracy (AUC = 0.89) on
synthetic and clinical data. For cardiopulmonary analysis, lung and heart sound
datasets from PhysioNet and Pascal were processed using Mel-Frequency Cepstral
Coefficients (MFCC) and classified via Random Forest, reaching 87.2% accuracy
and 85.7% sensitivity. Comparative evaluation against state-of-the-art models
demonstrates that the proposed system achieves competitive results while
remaining lightweight and deployable on low-cost devices. The framework
provides a promising step toward scalable, real-time, and accessible AI-based
pre-diagnostic healthcare solutions.


### [24] [DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis](https://arxiv.org/abs/2510.25237)
*Yinqi Cai, Jichang Li, Zhaolun Li, Weikai Chen, Rushi Lan, Xi Xie, Xiaonan Luo, Guanbin Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†DeepShieldæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå±€éƒ¨è¡¥ä¸å¼•å¯¼å’Œå…¨å±€ä¼ªé€ å¤šæ ·åŒ–æŠ€æœ¯ï¼Œåœ¨CLIP-ViTç¼–ç å™¨åŸºç¡€ä¸Šæ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿå¹³è¡¡å±€éƒ¨æ•æ„Ÿæ€§å’Œå…¨å±€æ³›åŒ–èƒ½åŠ›çš„æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº†åœ¨æœªè§ä¼ªé€ æŠ€æœ¯ä¸Šçš„æ£€æµ‹é²æ£’æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨åœ¨åŸŸå†…åœºæ™¯è¡¨ç°è‰¯å¥½ï¼Œä½†ç”±äºè¿‡åº¦ä¾èµ–ç‰¹å®šä¼ªé€ ä¼ªå½±è€Œéš¾ä»¥æ³›åŒ–åˆ°å¤šæ ·åŒ–çš„æ“çºµæŠ€æœ¯ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„å®ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹æœªçŸ¥ä¼ªé€ æ”»å‡»æ—¶çš„æ£€æµ‹èƒ½åŠ›ã€‚

**Method:** DeepShieldæ¡†æ¶åŸºäºCLIP-ViTç¼–ç å™¨ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå±€éƒ¨è¡¥ä¸å¼•å¯¼é€šè¿‡æ—¶ç©ºä¼ªå½±å»ºæ¨¡å’Œé€è¡¥ä¸ç›‘ç£æ•è·ç»†ç²’åº¦ä¸ä¸€è‡´æ€§ï¼›å…¨å±€ä¼ªé€ å¤šæ ·åŒ–é€šè¿‡é¢†åŸŸç‰¹å¾å¢å¼ºã€é¢†åŸŸæ¡¥æ¥å’Œè¾¹ç•Œæ‰©å±•ç‰¹å¾ç”Ÿæˆåˆæˆå¤šæ ·åŒ–ä¼ªé€ æ ·æœ¬ï¼Œç¼“è§£è¿‡æ‹Ÿåˆå¹¶æå‡è·¨åŸŸé€‚åº”æ€§ã€‚

**Result:** åœ¨è·¨æ•°æ®é›†å’Œè·¨æ“çºµæŠ€æœ¯çš„è¯„ä¼°ä¸­ï¼ŒDeepShieldè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç°å‡ºå¯¹æœªè§æ·±åº¦ä¼ªé€ æ”»å‡»çš„å“è¶Šé²æ£’æ€§ï¼Œè¯æ˜äº†å…¶åœ¨æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ç»“åˆæ–°é¢–çš„å±€éƒ¨å’Œå…¨å±€åˆ†æç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæå‡æ·±åº¦ä¼ªé€ æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ„å»ºæ›´é²æ£’çš„ä¼ªé€ æ£€æµ‹ç³»ç»Ÿæä¾›äº†é‡è¦æ€è·¯ï¼Œå¹¶å¼ºè°ƒäº†åœ¨æ£€æµ‹æ¡†æ¶ä¸­å¹³è¡¡å±€éƒ¨æ•æ„Ÿæ€§å’Œå…¨å±€æ³›åŒ–çš„å…³é”®ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Recent advances in deep generative models have made it easier to manipulate
face videos, raising significant concerns about their potential misuse for
fraud and misinformation. Existing detectors often perform well in in-domain
scenarios but fail to generalize across diverse manipulation techniques due to
their reliance on forgery-specific artifacts. In this work, we introduce
DeepShield, a novel deepfake detection framework that balances local
sensitivity and global generalization to improve robustness across unseen
forgeries. DeepShield enhances the CLIP-ViT encoder through two key components:
Local Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG
applies spatiotemporal artifact modeling and patch-wise supervision to capture
fine-grained inconsistencies often overlooked by global models. GFD introduces
domain feature augmentation, leveraging domain-bridging and boundary-expanding
feature generation to synthesize diverse forgeries, mitigating overfitting and
enhancing cross-domain adaptability. Through the integration of novel local and
global analysis for deepfake detection, DeepShield outperforms state-of-the-art
methods in cross-dataset and cross-manipulation evaluations, achieving superior
robustness against unseen deepfake attacks.


### [25] [VADB: A Large-Scale Video Aesthetic Database with Professional and Multi-Dimensional Annotations](https://arxiv.org/abs/2510.25238)
*Qianqian Qiao, DanDan Zheng, Yihang Bo, Bao Peng, Heng Huang, Longteng Jiang, Huaye Wang, Jingdong Chen, Jun Zhou, Xin Jin*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†VADBâ€”â€”æœ€å¤§çš„è§†é¢‘ç¾å­¦è¯„ä¼°æ•°æ®åº“ï¼ŒåŒ…å«10,490ä¸ªå¤šæ ·åŒ–è§†é¢‘ï¼Œå¹¶å¼€å‘äº†VADB-NetåŒæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ˜¾è‘—æå‡äº†è§†é¢‘ç¾å­¦è¯„ä¼°æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è§†é¢‘ç¾å­¦è¯„ä¼°ä½œä¸ºå¤šåª’ä½“è®¡ç®—çš„é‡è¦é¢†åŸŸï¼Œå…¶å‘å±•å—åˆ°æ ‡å‡†åŒ–æ•°æ®é›†ç¼ºä¹å’Œé²æ£’æ¨¡å‹ä¸è¶³çš„é™åˆ¶ï¼Œè§†é¢‘çš„æ—¶åºåŠ¨æ€ç‰¹æ€§å’Œå¤šæ¨¡æ€èåˆæŒ‘æˆ˜é˜»ç¢äº†åŸºäºå›¾åƒæ–¹æ³•çš„ç›´æ¥åº”ç”¨ã€‚

**Method:** æå‡ºäº†VADB-NetåŒæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è§†é¢‘ç¾å­¦è¯„ä¼°ä¸­çš„å¤šæ¨¡æ€ä¿¡æ¯èåˆé—®é¢˜ã€‚

**Result:** VADB-Netåœ¨è¯„åˆ†ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„è§†é¢‘è´¨é‡è¯„ä¼°æ¨¡å‹ï¼Œå¹¶æ”¯æŒä¸‹æ¸¸è§†é¢‘ç¾å­¦è¯„ä¼°ä»»åŠ¡ï¼Œå®éªŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ä»…æä¾›äº†å¤§è§„æ¨¡æ ‡å‡†åŒ–è§†é¢‘ç¾å­¦æ•°æ®åº“ï¼Œè¿˜å¼€å‘äº†æœ‰æ•ˆçš„åŒæ¨¡æ€é¢„è®­ç»ƒæ¡†æ¶ï¼Œä¸ºè§†é¢‘ç¾å­¦è¯„ä¼°é¢†åŸŸçš„å‘å±•æä¾›äº†é‡è¦åŸºç¡€è®¾æ–½å’Œæ–¹æ³•è®ºæ”¯æŒã€‚

---

#### ğŸ“„ Abstract
Video aesthetic assessment, a vital area in multimedia computing, integrates
computer vision with human cognition. Its progress is limited by the lack of
standardized datasets and robust models, as the temporal dynamics of video and
multimodal fusion challenges hinder direct application of image-based methods.
This study introduces VADB, the largest video aesthetic database with 10,490
diverse videos annotated by 37 professionals across multiple aesthetic
dimensions, including overall and attribute-specific aesthetic scores, rich
language comments and objective tags. We propose VADB-Net, a dual-modal
pre-training framework with a two-stage training strategy, which outperforms
existing video quality assessment models in scoring tasks and supports
downstream video aesthetic assessment tasks. The dataset and source code are
available at https://github.com/BestiVictory/VADB.


### [26] [LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation](https://arxiv.org/abs/2510.25263)
*Yang Miao, Jan-Nico Zaech, Xi Wang, Fabien Despinoy, Danda Pani Paudel, Luc Van Gool*

#### ğŸ§© TL;DR
LangHOPSæ˜¯é¦–ä¸ªåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¼€æ”¾è¯æ±‡å¯¹è±¡-éƒ¨ä»¶å®ä¾‹åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡è¯­è¨€ç©ºé—´ä¸­çš„å±‚æ¬¡ç»“æ„å®ç°å¯¹è±¡å’Œéƒ¨ä»¶çš„è”åˆæ£€æµ‹ä¸åˆ†å‰²ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•ä¾èµ–å¯å‘å¼æˆ–å¯å­¦ä¹ çš„è§†è§‰åˆ†ç»„ç­–ç•¥ï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†å¼€æ”¾è¯æ±‡çš„å¯¹è±¡-éƒ¨ä»¶å±‚æ¬¡ç»“æ„è§£æé—®é¢˜ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿåˆ©ç”¨è¯­è¨€çŸ¥è¯†æ¥å»ºç«‹å¤šç²’åº¦æ¦‚å¿µé—´è”ç³»çš„æ–°æ–¹æ³•ã€‚

**Method:** æå‡ºåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œå°†MLLMé›†æˆåˆ°å¯¹è±¡-éƒ¨ä»¶è§£ææµç¨‹ä¸­ï¼Œåˆ©ç”¨å…¶ä¸°å¯Œçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨è¯­è¨€ç©ºé—´ä¸­å»ºç«‹å¯¹è±¡-éƒ¨ä»¶å±‚æ¬¡ç»“æ„ï¼Œå¹¶é‡‡ç”¨MLLMé©±åŠ¨çš„éƒ¨ä»¶æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥ã€‚

**Result:** åœ¨PartImageNetæ•°æ®é›†ä¸Šï¼ŒLangHOPSåœ¨åŸŸå†…å’Œè·¨æ•°æ®é›†å¯¹è±¡-éƒ¨ä»¶å®ä¾‹åˆ†å‰²ä¸­åˆ†åˆ«ä»¥5.5%å’Œ4.8%çš„å¹³å‡ç²¾åº¦ä¼˜åŠ¿è¶…è¶Šå…ˆå‰æ–¹æ³•ï¼Œåœ¨ADE20Kçš„é›¶æ ·æœ¬è¯­ä¹‰åˆ†å‰²ä¸­æœªè§å¯¹è±¡éƒ¨ä»¶ä¸Šè·å¾—2.5% mIOUæå‡ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜åŸºäºè¯­è¨€å±‚æ¬¡ç»“æ„çš„æ–¹æ³•èƒ½æœ‰æ•ˆå¤„ç†å¯¹è±¡-éƒ¨ä»¶è§£æä»»åŠ¡ï¼ŒMLLMçš„çŸ¥è¯†æ¨ç†èƒ½åŠ›å¯¹å¤šç²’åº¦æ¦‚å¿µé“¾æ¥å…·æœ‰å…³é”®ä½œç”¨ï¼Œä¸ºå¼€æ”¾è¯æ±‡çš„å±‚æ¬¡åŒ–è§†è§‰ç†è§£æä¾›äº†æ–°æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based
framework for open-vocabulary object-part instance segmentation. Given an
image, LangHOPS can jointly detect and segment hierarchical object and part
instances from open-vocabulary candidate categories. Unlike prior approaches
that rely on heuristic or learnable visual grouping, our approach grounds
object-part hierarchies in language space. It integrates the MLLM into the
object-part parsing pipeline to leverage its rich knowledge and reasoning
capabilities, and link multi-granularity concepts within the hierarchies. We
evaluate LangHOPS across multiple challenging scenarios, including in-domain
and cross-dataset object-part instance segmentation, and zero-shot semantic
segmentation. LangHOPS achieves state-of-the-art results, surpassing previous
methods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on
the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K
(zero-shot). Ablation studies further validate the effectiveness of the
language-grounded hierarchy and MLLM driven part query refinement strategy. The
code will be released here.


### [27] [Prototype-Driven Adaptation for Few-Shot Object Detection](https://arxiv.org/abs/2510.25318)
*Yushen Huang, Zhiming Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºåŸå‹é©±åŠ¨å¯¹é½ï¼ˆPDAï¼‰ï¼Œä¸€ç§è½»é‡çº§æ’ä»¶å¼åº¦é‡å¤´ï¼Œé€šè¿‡æä¾›ä¸çº¿æ€§åˆ†ç±»å™¨äº’è¡¥çš„åŸå‹åŒ–â€œç¬¬äºŒæ„è§â€æ¥è§£å†³å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ä¸­çš„åŸºç¡€ç±»åå·®å’Œæ ¡å‡†ä¸ç¨³å®šé—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨VOC FSODå’ŒGFSODåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ–°ç±»æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒåŸºç¡€ç±»æ€§èƒ½å¹¶ä»…å¼•å…¥å¯å¿½ç•¥çš„è®¡ç®—å¼€é”€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFSODï¼‰åœ¨ä»…æœ‰å°‘é‡æ–°ç±»æ ·æœ¬å¯ç”¨æ—¶ï¼Œå¸¸å¸¸é¢ä¸´åŸºç¡€ç±»åå·®å’Œæ ¡å‡†ä¸ç¨³å®šçš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æœ‰é™çš„æ–°ç±»æ•°æ®ä¸‹éš¾ä»¥å¹³è¡¡åŸºç¡€ç±»å’Œæ–°ç±»çš„æ£€æµ‹æ€§èƒ½ï¼Œä¸”çº¿æ€§åˆ†ç±»å™¨åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹å®¹æ˜“äº§ç”Ÿåå·®é¢„æµ‹ã€‚

**Method:** PDAåœ¨DeFRCNæ¡†æ¶ä¸­å¼•å…¥æ”¯æŒé›†åŸå‹ç»´æŠ¤æœºåˆ¶ï¼Œåœ¨å¯å­¦ä¹ çš„èº«ä»½åˆå§‹åŒ–æŠ•å½±ç©ºé—´ä¸­æ„å»ºåŸå‹è¡¨ç¤ºï¼Œå¹¶å¯é€‰åœ°åº”ç”¨åŸå‹æ¡ä»¶RoIå¯¹é½ä»¥å‡å°‘å‡ ä½•ä¸åŒ¹é…ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ›´æ–°æ ‡è®°å‰æ™¯RoIæ¥é€‚åº”åŸå‹ï¼Œæ— éœ€å¼•å…¥ç±»ç‰¹å®šå‚æ•°ï¼Œå¹¶åœ¨æ¨ç†æ—¶å†»ç»“åŸå‹ä»¥ç¡®ä¿åè®®åˆè§„æ€§ã€‚PDAä½¿ç”¨æœ€ä½³KåŒ¹é…æ–¹æ¡ˆæ•æ‰ç±»å†…å¤šæ¨¡æ€æ€§ï¼Œå¹¶é€šè¿‡æ¸©åº¦ç¼©æ”¾èåˆå°†åº¦é‡ç›¸ä¼¼åº¦ä¸æ£€æµ‹å™¨é€»è¾‘å€¼ç»“åˆã€‚

**Result:** åœ¨VOC FSODå’ŒGFSODåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPDAèƒ½å¤ŸæŒç»­æå‡æ–°ç±»æ£€æµ‹æ€§èƒ½ï¼ŒåŒæ—¶å¯¹åŸºç¡€ç±»æ€§èƒ½å½±å“æå°ã€‚è¯¥æ–¹æ³•ä»¥å¯å¿½ç•¥çš„è®¡ç®—å¼€é”€å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼ŒéªŒè¯äº†åŸå‹é©±åŠ¨æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** PDAè¯æ˜äº†åŸå‹åŒ–åº¦é‡å­¦ä¹ ä½œä¸ºçº¿æ€§åˆ†ç±»å™¨è¡¥å……çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹æä¾›äº†ç¨³å®šå¯é çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†åœ¨ä¿æŒåŸºç¡€ç±»æ€§èƒ½çš„åŒæ—¶æå‡æ–°ç±»æ£€æµ‹èƒ½åŠ›çš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥å°‘æ ·æœ¬å­¦ä¹ ç ”ç©¶æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’Œè®¾è®¡æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Few-shot object detection (FSOD) often suffers from base-class bias and
unstable calibration when only a few novel samples are available. We propose
Prototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN
that provides a prototype-based "second opinion" complementary to the linear
classifier. PDA maintains support-only prototypes in a learnable
identity-initialized projection space and optionally applies
prototype-conditioned RoI alignment to reduce geometric mismatch. During
fine-tuning, prototypes can be adapted via exponential moving average(EMA)
updates on labeled foreground RoIs-without introducing class-specific
parameters-and are frozen at inference to ensure strict protocol compliance.
PDA employs a best-of-K matching scheme to capture intra-class multi-modality
and temperature-scaled fusion to combine metric similarities with detector
logits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently
improves novel-class performance with minimal impact on base classes and
negligible computational overhead.


### [28] [StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA](https://arxiv.org/abs/2510.25332)
*Yuhang Hu, Zhenyu Yang, Shihan Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Changsheng Xu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†StreamingCoTï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ä¸ºæµå¼è§†é¢‘é—®ç­”å’Œå¤šæ¨¡æ€æ€ç»´é“¾ä»»åŠ¡è®¾è®¡çš„å…·æœ‰æ—¶é—´æ¼”åŒ–æ¨ç†èƒ½åŠ›çš„æ•°æ®é›†ï¼Œé€šè¿‡åŠ¨æ€å±‚æ¬¡æ ‡æ³¨æ¶æ„å’Œæ˜¾å¼æ¨ç†é“¾ç”ŸæˆèŒƒå¼è§£å†³äº†ç°æœ‰VideoQAæ•°æ®é›†åœ¨æ—¶é—´åŠ¨æ€æ€§å’Œæ¨ç†é€æ˜åº¦æ–¹é¢çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†é¢‘é—®ç­”æ•°æ®é›†å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šé™æ€æ ‡æ³¨æœºåˆ¶æ— æ³•æ•æ‰æ—¶é—´è§†é¢‘æµä¸­ç­”æ¡ˆçš„æ¼”åŒ–ç‰¹æ€§ï¼Œä»¥åŠç¼ºä¹æ˜¾å¼æ¨ç†è¿‡ç¨‹æ ‡æ³¨é™åˆ¶äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œè¿™é˜»ç¢äº†å¤šæ¨¡æ€æ¨¡å‹åœ¨æµå¼è§†é¢‘åº”ç”¨ä¸­å¯¹æ—¶é—´åŠ¨æ€ç†è§£å’Œå¤æ‚æ¨ç†èƒ½åŠ›çš„æå‡ã€‚

**Method:** ç ”ç©¶æå‡ºäº†åŠ¨æ€å±‚æ¬¡æ ‡æ³¨æ¶æ„ï¼Œç”Ÿæˆæ¯ç§’å¯†é›†æè¿°å¹¶é€šè¿‡ç›¸ä¼¼æ€§èåˆæ„å»ºæ—¶é—´ä¾èµ–çš„è¯­ä¹‰ç‰‡æ®µï¼ŒåŒæ—¶è®¾è®¡æ—¶é—´æ¼”åŒ–æ¨¡å¼çº¦æŸçš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼›è¿›ä¸€æ­¥æå‡ºæ˜¾å¼æ¨ç†é“¾ç”ŸæˆèŒƒå¼ï¼Œé€šè¿‡å…³é”®å¸§è¯­ä¹‰å¯¹é½æå–æ—¶ç©ºå¯¹è±¡ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”ŸæˆåŸºäºå¯¹è±¡çŠ¶æ€è½¬æ¢çš„æ¨ç†è·¯å¾„ï¼Œå¹¶é€šè¿‡äººå·¥éªŒè¯ç¡®ä¿é€»è¾‘ä¸€è‡´æ€§ã€‚

**Result:** StreamingCoTæ•°æ®é›†å»ºç«‹äº†æµå¼è§†é¢‘ç†è§£ã€å¤æ‚æ—¶é—´æ¨ç†å’Œå¤šæ¨¡æ€æ¨ç†ç ”ç©¶çš„åŸºç¡€ï¼Œæä¾›äº†é¦–ä¸ªå…·æœ‰æ—¶é—´æ¼”åŒ–æ¨ç†èƒ½åŠ›çš„VideoQAæ•°æ®é›†ï¼ŒåŒ…å«åŠ¨æ€å±‚æ¬¡æ ‡æ³¨å’Œæ˜¾å¼æ¨ç†é“¾æ ‡æ³¨ï¼Œä¸ºç›¸å…³é¢†åŸŸç ”ç©¶æä¾›äº†æ ‡å‡†åŸºå‡†å’Œå·¥å…·æ”¯æŒã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºæµå¼è§†é¢‘ç†è§£ã€å¤æ‚æ—¶é—´æ¨ç†å’Œå¤šæ¨¡æ€æ¨ç†é¢†åŸŸå»ºç«‹äº†é‡è¦åŸºç¡€ï¼Œæå‡ºçš„æ•°æ®é›†å’Œæ„å»ºå·¥å…·å°†æ¨åŠ¨è§†é¢‘é—®ç­”æ¨¡å‹åœ¨æ—¶é—´åŠ¨æ€ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¿›æ­¥ï¼Œå¹¶ä¸ºå¯è§£é‡Šæ€§å¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
The rapid growth of streaming video applications demands multimodal models
with enhanced capabilities for temporal dynamics understanding and complex
reasoning. However, current Video Question Answering (VideoQA) datasets suffer
from two critical limitations: 1) Static annotation mechanisms fail to capture
the evolving nature of answers in temporal video streams, and 2) The absence of
explicit reasoning process annotations restricts model interpretability and
logical deduction capabilities. To address these challenges, We introduce
StreamingCoT, the first dataset explicitly designed for temporally evolving
reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our
framework first establishes a dynamic hierarchical annotation architecture that
generates per-second dense descriptions and constructs temporally-dependent
semantic segments through similarity fusion, paired with question-answer sets
constrained by temporal evolution patterns. We further propose an explicit
reasoning chain generation paradigm that extracts spatiotemporal objects via
keyframe semantic alignment, derives object state transition-based reasoning
paths using large language models, and ensures logical coherence through
human-verified validation. This dataset establishes a foundation for advancing
research in streaming video understanding, complex temporal reasoning, and
multimodal inference. Our StreamingCoT and its construction toolkit can be
accessed at https://github.com/Fleeting-hyh/StreamingCoT.


### [29] [Instance-Level Composed Image Retrieval](https://arxiv.org/abs/2510.25387)
*Bill Psomas, George Retsinas, Nikos Efthymiadis, Panagiotis Filntisis, Yannis Avrithis, Petros Maragos, Ondrej Chum, Giorgos Tolias*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†i-CIRè¯„ä¼°æ•°æ®é›†å’ŒBASICè®­ç»ƒæ— å…³æ–¹æ³•ï¼Œè§£å†³äº†ç»„åˆå›¾åƒæ£€ç´¢é¢†åŸŸé«˜è´¨é‡æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚BASICæ–¹æ³•é€šè¿‡åˆ†åˆ«ä¼°è®¡è§†è§‰å’Œæ–‡æœ¬æŸ¥è¯¢ä¸å›¾åƒçš„ç›¸ä¼¼åº¦å¹¶è¿›è¡ŒåæœŸèåˆï¼Œåœ¨å¤šä¸ªCIRæ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç»„åˆå›¾åƒæ£€ç´¢ç ”ç©¶çš„è¿›å±•å—åˆ°é«˜è´¨é‡è®­ç»ƒå’Œè¯„ä¼°æ•°æ®ç¼ºä¹çš„é™åˆ¶ï¼Œç°æœ‰æ•°æ®é›†ä¸»è¦å…³æ³¨è¯­ä¹‰çº§åˆ«çš„ç±»åˆ«å®šä¹‰ï¼Œè€Œç¼ºä¹é’ˆå¯¹ç‰¹å®šå®ä¾‹çº§åˆ«å¯¹è±¡çš„æ£€ç´¢è¯„ä¼°æ•°æ®ã€‚

**Method:** æå‡ºäº†BASICè®­ç»ƒæ— å…³æ–¹æ³•ï¼Œåˆ†åˆ«è®¡ç®—æŸ¥è¯¢å›¾åƒåˆ°å›¾åƒå’ŒæŸ¥è¯¢æ–‡æœ¬åˆ°å›¾åƒçš„ç›¸ä¼¼åº¦ï¼Œé€šè¿‡åæœŸèåˆå¯¹åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæŸ¥è¯¢çš„å›¾åƒè¿›è¡ŒåŠ æƒï¼Œå¹¶å¼•å…¥ç®€å•ç›´è§‚çš„ç»„ä»¶æ¥æ”¹è¿›å„ä¸ªç›¸ä¼¼åº¦ä¼°è®¡ã€‚

**Result:** BASICæ–¹æ³•åœ¨æå‡ºçš„i-CIRæ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶åœ¨éµå¾ªè¯­ä¹‰çº§åˆ«ç±»åˆ«å®šä¹‰çš„ç°æœ‰CIRæ•°æ®é›†ä¸Šä¹Ÿè¾¾åˆ°äº†æœ€ä½³è¡¨ç°ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„æ— è®­ç»ƒæ–¹æ³•å¯ä»¥æœ‰æ•ˆè§£å†³ç»„åˆå›¾åƒæ£€ç´¢é—®é¢˜ï¼Œi-CIRæ•°æ®é›†ä¸ºå®ä¾‹çº§åˆ«æ£€ç´¢ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–è¯„ä¼°åŸºå‡†ï¼Œä¸ºæœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
The progress of composed image retrieval (CIR), a popular research direction
in image retrieval, where a combined visual and textual query is used, is held
back by the absence of high-quality training and evaluation data. We introduce
a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an
instance-level class definition. The goal is to retrieve images that contain
the same particular object as the visual query, presented under a variety of
modifications defined by textual queries. Its design and curation process keep
the dataset compact to facilitate future research, while maintaining its
challenge-comparable to retrieval among more than 40M random
distractors-through a semi-automated selection of hard negatives.
  To overcome the challenge of obtaining clean, diverse, and suitable training
data, we leverage pre-trained vision-and-language models (VLMs) in a
training-free approach called BASIC. The method separately estimates
query-image-to-image and query-text-to-image similarities, performing late
fusion to upweight images that satisfy both queries, while down-weighting those
that exhibit high similarity with only one of the two. Each individual
similarity is further improved by a set of components that are simple and
intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR
datasets that follow a semantic-level class definition. Project page:
https://vrg.fel.cvut.cz/icir/.


### [30] [SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time, Monocular Depth Estimation in Underwater Environments](https://arxiv.org/abs/2510.25463)
*Hongjie Zhang, Gideon Billings, Stefan B. Williams*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºSPADEï¼šç¨€ç–è‡ªé€‚åº”æ·±åº¦ä¼°è®¡å™¨ï¼Œä¸€ç§ç»“åˆé¢„è®­ç»ƒç›¸å¯¹æ·±åº¦ä¼°è®¡å™¨ä¸ç¨€ç–æ·±åº¦å…ˆéªŒçš„å•ç›®æ·±åº¦ä¼°è®¡ç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆå¯†é›†çš„åº¦é‡å°ºåº¦æ·±åº¦å›¾ï¼Œæ˜¾è‘—æå‡æ°´ä¸‹åŸºç¡€è®¾æ–½æ£€æŸ¥çš„è‡ªä¸»æ€§å’Œå®‰å…¨æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ°´ä¸‹åŸºç¡€è®¾æ–½æ£€æŸ¥ä¾èµ–äººç±»æ½œæ°´å‘˜æˆ–é¥æ§æ“ä½œè½¦è¾†ï¼Œé¢ä¸´å¤æ‚ç»“æ„å’Œæµ‘æµŠæ°´åŸŸä¸­çš„æ„ŸçŸ¥ä¸æ“ä½œæŒ‘æˆ˜ï¼Œéœ€è¦å¢å¼ºæ°´ä¸‹è½¦è¾†çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ä»¥é™ä½æ“æ§é£é™©å¹¶æé«˜è‡ªä¸»æ€§ã€‚

**Method:** é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼šé¦–å…ˆä½¿ç”¨ç¨€ç–æ·±åº¦ç‚¹å¯¹ç›¸å¯¹æ·±åº¦å›¾è¿›è¡Œå°ºåº¦ç¼©æ”¾ï¼Œç„¶åé€šè¿‡æå‡ºçš„çº§è”å·ç§¯-å¯å˜å½¢Transformerå—å¯¹æœ€ç»ˆåº¦é‡é¢„æµ‹è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚

**Result:** è¯¥æ–¹æ³•åœ¨ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹ï¼Œåœ¨åµŒå…¥å¼ç¡¬ä»¶ä¸Šè¿è¡Œæ•ˆç‡è¶…è¿‡15 FPSï¼Œä¸ºå®é™…æ°´ä¸‹æ£€æŸ¥ä»»åŠ¡æä¾›äº†å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚

**Conclusion:** SPADEæ¡†æ¶é€šè¿‡æœ‰æ•ˆç»“åˆç›¸å¯¹æ·±åº¦ä¼°è®¡ä¸ç¨€ç–æ·±åº¦å…ˆéªŒï¼Œå®ç°äº†é«˜æ•ˆå‡†ç¡®çš„åº¦é‡æ·±åº¦ä¼°è®¡ï¼Œä¸ºæ°´ä¸‹è‡ªä¸»æ£€æŸ¥ç³»ç»Ÿçš„å¼€å‘æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Underwater infrastructure requires frequent inspection and maintenance due to
harsh marine conditions. Current reliance on human divers or remotely operated
vehicles is limited by perceptual and operational challenges, especially around
complex structures or in turbid water. Enhancing the spatial awareness of
underwater vehicles is key to reducing piloting risks and enabling greater
autonomy. To address these challenges, we present SPADE: SParsity Adaptive
Depth Estimator, a monocular depth estimation pipeline that combines
pre-trained relative depth estimator with sparse depth priors to produce dense,
metric scale depth maps. Our two-stage approach first scales the relative depth
map with the sparse depth points, then refines the final metric prediction with
our proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves
improved accuracy and generalisation over state-of-the-art baselines and runs
efficiently at over 15 FPS on embedded hardware, promising to support practical
underwater inspection and intervention. This work has been submitted to IEEE
Journal of Oceanic Engineering Special Issue of AUV 2026.


### [31] [Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image Generation](https://arxiv.org/abs/2510.25739)
*Zhi-Kai Chen, Jun-Peng Jiang, Han-Jia Ye, De-Chuan Zhan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Hawkæ–¹æ³•ï¼Œåˆ©ç”¨å›¾åƒçš„ç©ºé—´ç»“æ„æ¥å¼•å¯¼æ¨æµ‹æ¨¡å‹è¿›è¡Œæ›´å‡†ç¡®å’Œé«˜æ•ˆçš„é¢„æµ‹ï¼Œåœ¨ä¿æŒå›¾åƒä¿çœŸåº¦å’Œå¤šæ ·æ€§çš„åŒæ—¶ï¼Œç›¸æ¯”æ ‡å‡†è‡ªå›å½’æ¨¡å‹å®ç°äº†1.71å€çš„åŠ é€Ÿã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹è™½ç„¶èƒ½å¤Ÿäº§ç”Ÿé«˜ä¿çœŸå›¾åƒï¼Œä½†ç”±äºå…¶å›ºæœ‰çš„é¡ºåºã€é€ä¸ªä»¤ç‰Œçš„è§£ç è¿‡ç¨‹ï¼Œé€šå¸¸å­˜åœ¨æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚æ¨æµ‹è§£ç åœ¨æ–‡æœ¬ç”Ÿæˆä¸­å·²æ˜¾ç¤ºå‡ºåŠ é€Ÿæ½œåŠ›ï¼Œä½†åœ¨å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨ä»æœªè¢«å……åˆ†æ¢ç´¢ï¼Œä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬æ›´å¤§çš„é‡‡æ ·ç©ºé—´å¯¼è‡´è‰ç¨¿æ¨¡å‹ä¸ç›®æ ‡æ¨¡å‹è¾“å‡ºå¯¹é½å›°éš¾ï¼Œä»¥åŠæœªèƒ½å……åˆ†åˆ©ç”¨å›¾åƒçš„äºŒç»´ç©ºé—´ç»“æ„æ¥å»ºæ¨¡å±€éƒ¨ä¾èµ–å…³ç³»ã€‚

**Method:** æœ¬æ–‡æå‡ºäº†Hawkæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å›¾åƒçš„ç©ºé—´ç»“æ„æ¥å¼•å¯¼æ¨æµ‹æ¨¡å‹è¿›è¡Œæ›´å‡†ç¡®å’Œé«˜æ•ˆçš„é¢„æµ‹ã€‚è¯¥æ–¹æ³•é€šè¿‡æ›´å¥½åœ°å»ºæ¨¡å±€éƒ¨ä¾èµ–å…³ç³»ï¼Œå…‹æœäº†ä¼ ç»Ÿæ¨æµ‹è§£ç åœ¨å›¾åƒç”Ÿæˆä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é‡‡æ ·ç©ºé—´å¤§å’Œç©ºé—´ç»“æ„åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚

**Result:** åœ¨å¤šä¸ªæ–‡æœ¬åˆ°å›¾åƒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHawkæ–¹æ³•ç›¸æ¯”æ ‡å‡†è‡ªå›å½’æ¨¡å‹å®ç°äº†1.71å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒçš„ä¿çœŸåº¦å’Œå¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•åœ¨åŠ é€Ÿæ¨ç†çš„åŒæ—¶æ²¡æœ‰ç‰ºç‰²ç”Ÿæˆè´¨é‡ï¼Œè¯æ˜äº†å…¶åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** Hawkæ–¹æ³•æˆåŠŸåœ°å°†æ¨æµ‹è§£ç æŠ€æœ¯æ‰©å±•åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸï¼Œé€šè¿‡åˆ©ç”¨ç©ºé—´ç»“æ„æŒ‡å¯¼å®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚è¿™é¡¹ç ”ç©¶ä¸ºåŠ é€Ÿè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ï¼Œè¡¨æ˜ç©ºé—´æ„ŸçŸ¥çš„æ¨æµ‹è§£ç æ˜¯è§£å†³å›¾åƒç”Ÿæˆæ¨ç†æ•ˆç‡é—®é¢˜çš„æœ‰æ•ˆé€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Autoregressive (AR) image generation models are capable of producing
high-fidelity images but often suffer from slow inference due to their
inherently sequential, token-by-token decoding process. Speculative decoding,
which employs a lightweight draft model to approximate the output of a larger
AR model, has shown promise in accelerating text generation without
compromising quality. However, its application to image generation remains
largely underexplored. The challenges stem from a significantly larger sampling
space, which complicates the alignment between the draft and target model
outputs, coupled with the inadequate use of the two-dimensional spatial
structure inherent in images, thereby limiting the modeling of local
dependencies. To overcome these challenges, we introduce Hawk, a new approach
that harnesses the spatial structure of images to guide the speculative model
toward more accurate and efficient predictions. Experimental results on
multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR
models, while preserving both image fidelity and diversity.


### [32] [Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks](https://arxiv.org/abs/2510.25760)
*Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu*

#### ğŸ§© TL;DR
æœ¬ç»¼è¿°å¯¹å¤šæ¨¡æ€ç©ºé—´æ¨ç†ä»»åŠ¡è¿›è¡Œäº†ç³»ç»Ÿæ€§å›é¡¾ï¼Œæ¶µç›–äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œå¹¶å¼•å…¥äº†å¼€æ”¾åŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°ï¼Œä¸ºè¿™ä¸€æ–°å…´é¢†åŸŸå»ºç«‹äº†åšå®åŸºç¡€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡äººç±»å…·å¤‡é€šè¿‡è§†è§‰å’Œå£°éŸ³ç­‰å¤šæ¨¡æ€è§‚å¯Ÿç†è§£ç©ºé—´çš„èƒ½åŠ›ï¼Œä¸”å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†é’ˆå¯¹è¿™äº›æ¨¡å‹çš„ç³»ç»Ÿæ€§ç»¼è¿°å’Œå…¬å¼€å¯ç”¨åŸºå‡†æµ‹è¯•ä»ç„¶æœ‰é™ï¼Œéœ€è¦å»ºç«‹å…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚

**Method:** æœ¬ç»¼è¿°ç³»ç»Ÿæ€§åœ°åˆ†ç±»äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„è¿›å±•ï¼Œé‡ç‚¹å…³æ³¨åè®­ç»ƒæŠ€æœ¯ã€å¯è§£é‡Šæ€§å’Œæ¶æ„è®¾è®¡ï¼Œæ¶µç›–äº†ä»ç»å…¸2Dä»»åŠ¡åˆ°3Dç©ºé—´ä¸­çš„è§†è§‰é—®ç­”ä¸å®šä½ï¼Œä»¥åŠå…·èº«AIä¸­çš„è§†è§‰è¯­è¨€å¯¼èˆªå’ŒåŠ¨ä½œæ¨¡å‹ã€‚

**Result:** ç ”ç©¶å»ºç«‹äº†å¤šæ¨¡æ€ç©ºé—´æ¨ç†çš„å¼€æ”¾åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†ç©ºé—´å…³ç³»æ¨ç†ã€åœºæ™¯ä¸å¸ƒå±€ç†è§£ã€3Dç©ºé—´ä¸­çš„è§†è§‰é—®ç­”ä¸å®šä½ç­‰ä»»åŠ¡ï¼Œå¹¶è€ƒè™‘äº†éŸ³é¢‘å’Œè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç­‰æ–°å…´æ¨¡æ€å¯¹ç©ºé—´ç†è§£çš„æ–°è´¡çŒ®ã€‚

**Conclusion:** æœ¬ç»¼è¿°ä¸ºå¤šæ¨¡æ€ç©ºé—´æ¨ç†é¢†åŸŸå¥ å®šäº†åšå®åŸºç¡€ï¼Œæä¾›äº†å¯¹è¯¥é¢†åŸŸå‘å±•çš„æ·±åˆ»è§è§£ï¼Œé€šè¿‡ç³»ç»Ÿåˆ†ç±»å’ŒåŸºå‡†æµ‹è¯•çš„å¼•å…¥ï¼Œä¿ƒè¿›äº†è¯¥é¢†åŸŸç ”ç©¶çš„æ ‡å‡†åŒ–å’Œå¯æ¯”æ€§ã€‚

---

#### ğŸ“„ Abstract
Humans possess spatial reasoning abilities that enable them to understand
spaces through multimodal observations, such as vision and sound. Large
multimodal reasoning models extend these abilities by learning to perceive and
reason, showing promising performance across diverse spatial tasks. However,
systematic reviews and publicly available benchmarks for these models remain
limited. In this survey, we provide a comprehensive review of multimodal
spatial reasoning tasks with large models, categorizing recent progress in
multimodal large language models (MLLMs) and introducing open benchmarks for
evaluation. We begin by outlining general spatial reasoning, focusing on
post-training techniques, explainability, and architecture. Beyond classical 2D
tasks, we examine spatial relationship reasoning, scene and layout
understanding, as well as visual question answering and grounding in 3D space.
We also review advances in embodied AI, including vision-language navigation
and action models. Additionally, we consider emerging modalities such as audio
and egocentric video, which contribute to novel spatial understanding through
new sensors. We believe this survey establishes a solid foundation and offers
insights into the growing field of multimodal spatial reasoning. Updated
information about this survey, codes and implementation of the open benchmarks
can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [33] [Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments](https://arxiv.org/abs/2510.24760)
*Mengyuan Chen, Chengjun Dai, Xinyang Dong, Chengzhe Feng, Kewei Fu, Jianshe Li, Zhihan Peng, Yongqi Tong, Junshao Zhang, Hong Zhu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é’‰é’‰DeepResearchï¼Œä¸€ä¸ªé¢å‘ä¼ä¸šç¯å¢ƒçš„ç»Ÿä¸€å¤šæ™ºèƒ½ä½“æ™ºèƒ½æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°æ·±åº¦ç ”ç©¶ã€å¼‚æ„è¡¨æ ¼æ¨ç†å’Œå¤šæ¨¡æ€æŠ¥å‘Šç”Ÿæˆã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ä¼ä¸šç¯å¢ƒä¸­å¤æ‚ä¿¡æ¯å¤„ç†å’Œåˆ†æçš„æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ä¼ä¸šç¯å¢ƒé¢ä¸´ä¿¡æ¯å¤„ç†å¤æ‚ã€å¤šæºå¼‚æ„æ•°æ®æ•´åˆå›°éš¾ä»¥åŠæ·±åº¦åˆ†æèƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿç»Ÿä¸€å¤„ç†å¤šæ ·åŒ–ä¼ä¸šæ™ºèƒ½ä»»åŠ¡çš„æ¡†æ¶æ¥æå‡ä¼ä¸šå†³ç­–æ•ˆç‡å’Œåˆ†æèƒ½åŠ›ã€‚

**Method:** è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ™ºèƒ½ä½“æ™ºèƒ½æ¡†æ¶ï¼Œæ•´åˆäº†æ·±åº¦ç ”ç©¶ã€å¼‚æ„è¡¨æ ¼æ¨ç†å’Œå¤šæ¨¡æ€æŠ¥å‘Šç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡æ™ºèƒ½ä½“ååŒå·¥ä½œæ¥å¤„ç†ä¼ä¸šç¯å¢ƒä¸­çš„å¤æ‚ä¿¡æ¯åˆ†æä»»åŠ¡ã€‚

**Result:** æ¡†æ¶åœ¨å®é™…ä¼ä¸šç¯å¢ƒä¸­å±•ç¤ºäº†å¼ºå¤§çš„ä¿¡æ¯å¤„ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå¤šæºæ•°æ®ã€è¿›è¡Œæ·±åº¦åˆ†æå¹¶ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€ç ”ç©¶æŠ¥å‘Šï¼Œæå‡äº†ä¼ä¸šæ™ºèƒ½å†³ç­–çš„æ•ˆç‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å¤šæ™ºèƒ½ä½“æ¡†æ¶åœ¨ä¼ä¸šæ™ºèƒ½åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºç»Ÿä¸€çš„ä¼ä¸šæ™ºèƒ½åˆ†æå¹³å°æä¾›äº†æ–°æ€è·¯ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œæ¨å¹¿æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
We present Dingtalk DeepResearch, a unified multi agent intelligence
framework for real world enterprise environments, delivering deep research,
heterogeneous table reasoning, and multimodal report generation.


### [34] [Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2510.24870)
*Alexander Martin, William Walden, Reno Kriz, Dengjia Zhang, Kate Sanders, Eugene Yang, Chihsheng Jin, Benjamin Van Durme*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MiRAGEï¼Œä¸€ä¸ªç”¨äºå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥InfoF1å’ŒCiteF1æŒ‡æ ‡æ¥è§£å†³ç°æœ‰æ–‡æœ¬ä¸­å¿ƒè¯„ä¼°æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨ç†åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€è§†å¬åª’ä½“æˆä¸ºåœ¨çº¿ä¿¡æ¯çš„é‡è¦æ¥æºï¼ŒRAGç³»ç»Ÿéœ€è¦æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯è¿›è¡Œç”Ÿæˆï¼Œä½†ç°æœ‰çš„RAGè¯„ä¼°æ–¹æ³•ä¸»è¦é’ˆå¯¹æ–‡æœ¬ä¸­å¿ƒåœºæ™¯ï¼Œæ— æ³•éªŒè¯å¤šæ¨¡æ€æ¥æºçš„ä¿¡æ¯æ”¯æŒï¼Œé™åˆ¶äº†åœ¨å¤šæ¨¡æ€æ¨ç†å¯†é›†å‹ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

**Method:** MiRAGEé‡‡ç”¨å£°æ˜ä¸­å¿ƒçš„è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…å«InfoF1æŒ‡æ ‡è¯„ä¼°äº‹å®æ€§å’Œä¿¡æ¯è¦†ç›–ç‡ï¼Œä»¥åŠCiteF1æŒ‡æ ‡è¡¡é‡å¼•ç”¨æ”¯æŒå’Œå®Œæ•´æ€§ï¼›åŒæ—¶å¼•å…¥äº†MiRAGEçš„è‡ªåŠ¨å˜ä½“å’Œä¸‰ç§ä¸»æµTextRAGæŒ‡æ ‡ï¼ˆACLEã€ARGUEã€RAGASï¼‰è¿›è¡Œå¯¹æ¯”åˆ†æã€‚

**Result:** å®éªŒè¡¨æ˜ï¼Œäººå·¥åº”ç”¨MiRAGEæ¡†æ¶æ—¶ä¸å¤–éƒ¨è´¨é‡åˆ¤æ–­é«˜åº¦ä¸€è‡´ï¼›é€šè¿‡å¯¹æ¯”æ–‡æœ¬ä¸­å¿ƒæ–¹æ³•ï¼Œæ­ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­çš„å±€é™æ€§ï¼Œä¸ºè‡ªåŠ¨è¯„ä¼°å¥ å®šäº†åŸºç¡€ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€RAGè¯„ä¼°æä¾›äº†ç³»ç»Ÿæ¡†æ¶ï¼Œå¼€æºå®ç°ä¿ƒè¿›äº†è¯¥é¢†åŸŸçš„å‘å±•ï¼Œå¹¶æ˜ç¡®äº†å¦‚ä½•æœ‰æ•ˆè¯„ä¼°å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„æ€§èƒ½å’Œè´¨é‡ã€‚

---

#### ğŸ“„ Abstract
We introduce MiRAGE, an evaluation framework for retrieval-augmented
generation (RAG) from multimodal sources. As audiovisual media becomes a
prevalent source of information online, it is essential for RAG systems to
integrate information from these sources into generation. However, existing
evaluations for RAG are text-centric, limiting their applicability to
multimodal, reasoning intensive settings because they don't verify information
against sources. MiRAGE is a claim-centric approach to multimodal RAG
evaluation, consisting of InfoF1, evaluating factuality and information
coverage, and CiteF1, measuring citation support and completeness. We show that
MiRAGE, when applied by humans, strongly aligns with extrinsic quality
judgments. We additionally introduce automatic variants of MiRAGE and three
prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the
limitations of text-centric work and laying the groundwork for automatic
evaluation. We release open-source implementations and outline how to assess
multimodal RAG.


### [35] [Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation to a Parameter-Efficient Student](https://arxiv.org/abs/2510.25303)
*Soumyadeep Jana, Sanasam Ranbir Singh*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºPEKDæ¡†æ¶ï¼Œé€šè¿‡ä»å¤§è§„æ¨¡è®½åˆºæ•°æ®è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹ä¸­æå–çŸ¥è¯†æ¥å¢å¼ºå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œè§£å†³äº†å°‘æ ·æœ¬å¤šæ¨¡æ€è®½åˆºæ£€æµ‹ä¸­ç›‘ç£ä¿¡å·ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥ç†µæ„ŸçŸ¥é—¨æ§æœºåˆ¶åŠ¨æ€è°ƒæ•´è’¸é¦å¼ºåº¦ï¼Œåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹æ˜¾è‘—æå‡äº†PEFTæ–¹æ³•çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€è®½åˆºæ£€æµ‹åœ¨ä½èµ„æºç¯å¢ƒä¸‹é¢ä¸´æŒ‘æˆ˜ï¼Œç”±äºæ ‡æ³¨æ•°æ®ç¨€ç¼ºå¯¼è‡´æ¨¡å‹éš¾ä»¥å­¦ä¹ å›¾åƒ-æ–‡æœ¬é—´çš„å¾®å¦™çŸ›ç›¾ã€‚ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•è™½ç„¶å‡å°‘äº†è¿‡æ‹Ÿåˆï¼Œä½†åœ¨å°‘æ ·æœ¬æ•°æ®ä¸‹å› ç›‘ç£ä¿¡å·æœ‰é™è€Œæ— æ³•è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚

**Method:** æå‡ºPEKDç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡ä»å¤§è§„æ¨¡è®½åˆºæ•°æ®è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦æ¥å¢å¼ºPEFTæ–¹æ³•ã€‚å¼•å…¥ç†µæ„ŸçŸ¥é—¨æ§æœºåˆ¶ï¼Œæ ¹æ®æ•™å¸ˆæ¨¡å‹çš„ç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´è’¸é¦å¼ºåº¦ï¼Œä»¥ç¼“è§£æ¥è‡ªæ•™å¸ˆæ¨¡å‹çš„ä¸å¯é ä¿¡å·ã€‚

**Result:** åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPEKDæ¡†æ¶ä½¿PEFTæ–¹æ³•åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹è¶…è¶Šäº†å…ˆå‰çš„å‚æ•°é«˜æ•ˆæ–¹æ³•å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œå–å¾—äº†å¼ºåŠ²çš„æ€§èƒ½è¡¨ç°ã€‚

**Conclusion:** è¯¥æ¡†æ¶å…·æœ‰æ¨¡å—åŒ–ç‰¹æ€§ï¼Œå¯é€‚åº”å¹¿æ³›çš„å¤šæ¨¡æ€æ¨¡å‹å’Œä»»åŠ¡ã€‚ç ”ç©¶è¯æ˜äº†é€šè¿‡çŸ¥è¯†è’¸é¦å¢å¼ºå‚æ•°é«˜æ•ˆå¾®è°ƒåœ¨ä½èµ„æºå¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ¨¡å‹ä¼˜åŒ–æä¾›äº†æ–°æ€è·¯ã€‚

---

#### ğŸ“„ Abstract
Multimodal sarcasm detection is challenging, especially in low-resource
settings where subtle image-text contradictions are hard to learn due to scarce
annotated data, which hinders the model's performance. Parameter-efficient
fine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce
overfitting but struggle to reach optimal performance due to limited
supervision from few-shot data. We propose PEKD, a unified framework that
enhances PEFT methods via distillation from an expert model trained on
large-scale sarcasm data, which acts as the teacher. To mitigate unreliable
signals from the teacher, we introduce an entropy-aware gating mechanism that
dynamically adjusts the distillation strength based on teacher confidence.
Experiments on two public datasets demonstrate that our PEKD framework enables
PEFT methods to outperform both prior parameter-efficient approaches and large
multimodal models, achieving strong results in the few-shot scenario. The
framework is modular and adaptable to a wide range of multimodal models and
tasks.


### [36] [CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs](https://arxiv.org/abs/2510.25364)
*Luca Capone, Alessandro Bondielli, Alessandro Lenci*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æ¢è®¨äº†å°å‹è¯­è¨€æ¨¡å‹èƒ½å¦ä»æŒ‡ä»¤è°ƒä¼˜ä¸­å—ç›Šï¼Œå‘ç°æŒ‡ä»¤è°ƒä¼˜åœ¨å¾®è°ƒåœºæ™¯ä¸­å¸¦æ¥å°å¹…ä½†ä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œä½†æ”¹è¿›å¹¶ä¸ä¸€è‡´åœ°è¿ç§»åˆ°é›¶æ ·æœ¬ä»»åŠ¡ï¼Œæ­ç¤ºäº†äº¤äº’å¯¼å‘é€‚åº”ä¸å¹¿æ³›è¯­è¨€æ³›åŒ–ä¹‹é—´çš„æƒè¡¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å°å‹è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿä»æŒ‡ä»¤è°ƒä¼˜ä¸­å—ç›Šçš„é—®é¢˜ï¼Œæ¢ç´¢åœ¨æœ‰é™èµ„æºæ¡ä»¶ä¸‹å¦‚ä½•é€šè¿‡äººç±»å¯å‘å¼å­¦ä¹ ç­–ç•¥æ¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«å…³æ³¨æŒ‡ä»¤è°ƒä¼˜å¯¹ä½å‚æ•°é‡æ¨¡å‹çš„é€‚ç”¨æ€§å’Œå±€é™æ€§ã€‚

**Method:** ç ”ç©¶æ¯”è¾ƒäº†å¯¹è¯å¼å’Œé—®ç­”å¼æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œé‡‡ç”¨åˆå¹¶æˆ–é¡ºåºè¯¾ç¨‹ä¸¤ç§ç­–ç•¥ï¼Œä½¿ç”¨100Må’Œ140Må‚æ•°çš„ä»…è§£ç å™¨æ¨¡å‹ï¼Œåœ¨å¾®è°ƒå’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºæŒ‡ä»¤è°ƒä¼˜åœ¨å¾®è°ƒåœºæ™¯ï¼ˆSuperGLUEï¼‰ä¸­äº§ç”Ÿå°å¹…ä½†ä¸€è‡´çš„æ€§èƒ½å¢ç›Šï¼Œé¡ºåºè¯¾ç¨‹ç­–ç•¥ä¼˜äºåˆå¹¶æ•°æ®æ–¹æ³•ï¼›ç„¶è€Œè¿™äº›æ”¹è¿›åœ¨é›¶æ ·æœ¬ä»»åŠ¡ï¼ˆBLiMPã€EWoKã€WUGsç­‰ï¼‰ä¸­å¹¶ä¸ä¸€è‡´åœ°è¿ç§»ï¼Œè¡¨æ˜å­˜åœ¨ç‰¹å®šæƒè¡¡ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†å°†äººç±»å¯å‘å¼å­¦ä¹ ç­–ç•¥åº”ç”¨äºä½èµ„æºè¯­è¨€æ¨¡å‹çš„æ½œåŠ›å’Œçº¦æŸï¼ŒæŒ‡å‡ºäº†åœ¨ç”Ÿæ€è®­ç»ƒé™åˆ¶ä¸‹é€šè¿‡æ··åˆè¯¾ç¨‹æ–¹æ³•å¢å¼ºæ³›åŒ–èƒ½åŠ›çš„æ–¹å‘ï¼Œä¸ºå°å‹æ¨¡å‹ä¼˜åŒ–æä¾›äº†é‡è¦è§è§£ã€‚

---

#### ğŸ“„ Abstract
This work investigates whether small-scale LMs can benefit from instruction
tuning. We compare conversational and question-answering instruction tuning
datasets, applied either in a merged or sequential curriculum, using
decoder-only models with 100M and 140M parameters. Evaluation spans both
fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and
psycholinguistic correlation) settings. Results show that instruction tuning
yields small but consistent gains in fine-tuning scenarios, with sequential
curricula outperforming merged data; however, improvements do not consistently
transfer to zero-shot tasks, suggesting a trade-off between interaction-focused
adaptation and broad linguistic generalization. These results highlight both
the potential and the constraints of adapting human-inspired learning
strategies to low-resource LMs, and point toward hybrid, curriculum-based
approaches for enhancing generalization under ecological training limits.


### [37] [Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media](https://arxiv.org/abs/2510.25413)
*Shakib Yazdani, Yasser Hamidullah, Cristina EspaÃ±a-Bonet, Josef van Genabith*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å®ç°æ‰‹è¯­ç¿»è¯‘æ•°æ®é›†è‡ªåŠ¨æ ‡æ³¨å’Œè¿‡æ»¤çš„æ¡†æ¶ï¼Œæ˜¾è‘—å‡å°‘äººå·¥æ ‡æ³¨ä¾èµ–ï¼Œå¹¶æ„å»ºäº†æ¶µç›–å…«ç§æ‰‹è¯­çš„å¤§è§„æ¨¡æ•°æ®é›†TikTok-SL-8ï¼Œä¸ºæ‰‹è¯­ç¿»è¯‘æ¨¡å‹æä¾›äº†å¯æ‰©å±•çš„å¼±ç›‘ç£é¢„è®­ç»ƒæ•°æ®æºã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ‰‹è¯­ç¿»è¯‘æ•°æ®é›†æ™®éå­˜åœ¨è§„æ¨¡æœ‰é™ã€å¤šè¯­è¨€è¦†ç›–ä¸è¶³çš„é—®é¢˜ï¼Œä¸”ä¾èµ–ä¸“å®¶æ ‡æ³¨å’Œå—æ§å½•åˆ¶ç¯å¢ƒå¯¼è‡´æˆæœ¬é«˜æ˜‚ï¼Œè€Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ‰‹è¯­æ•°æ®è·å–æ–¹é¢çš„æ½œåŠ›å°šæœªè¢«å……åˆ†æŒ–æ˜ã€‚

**Method:** æå‡ºåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–æ ‡æ³¨è¿‡æ»¤æ¡†æ¶ï¼ŒåŒ…å«äººè„¸å¯è§æ€§æ£€æµ‹ã€æ‰‹è¯­æ´»åŠ¨è¯†åˆ«ã€è§†é¢‘æ–‡æœ¬æå–ä»¥åŠè§†é¢‘æ–‡æœ¬å¯¹é½éªŒè¯å››ä¸ªæ­¥éª¤ï¼Œåº”ç”¨äºTikTokå…«ç§æ‰‹è¯­è§†é¢‘å’ŒYouTube-SL-25å¾·è¯­æ‰‹è¯­æ•°æ®é›†ã€‚

**Result:** æ„å»ºäº†TikTok-SL-8å¤šè¯­è¨€æ‰‹è¯­æ•°æ®é›†ï¼Œå¹¶åœ¨è¿‡æ»¤åçš„å¾·è¯­å’Œç¾å›½æ‰‹è¯­æ•°æ®ä¸Šè¯„ä¼°äº†ä¸¤ä¸ªç°æˆæ‰‹è¯­ç¿»è¯‘æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸ºè‡ªåŠ¨æå–çš„å¸¦å™ªå£°æ•°æ®å»ºç«‹äº†åŸºå‡†æµ‹è¯•ã€‚

**Conclusion:** è¯¥å·¥ä½œå®ç°äº†æ‰‹è¯­ç¿»è¯‘çš„å¯æ‰©å±•å¼±ç›‘ç£é¢„è®­ç»ƒï¼Œä¿ƒè¿›äº†ä»ç¤¾äº¤åª’ä½“è·å–æ‰‹è¯­æ•°æ®çš„èƒ½åŠ›ï¼Œä¸ºæ‰‹è¯­ç¿»è¯‘ç ”ç©¶æä¾›äº†é«˜æ•ˆçš„æ•°æ®è·å–æ–°èŒƒå¼ã€‚

---

#### ğŸ“„ Abstract
Most existing sign language translation (SLT) datasets are limited in scale,
lack multilingual coverage, and are costly to curate due to their reliance on
expert annotation and controlled recording setup. Recently, Vision Language
Models (VLMs) have demonstrated strong capabilities as evaluators and real-time
assistants. Despite these advancements, their potential remains untapped in the
context of sign language dataset acquisition. To bridge this gap, we introduce
the first automated annotation and filtering framework that utilizes VLMs to
reduce reliance on manual effort while preserving data quality. Our method is
applied to TikTok videos across eight sign languages and to the already curated
YouTube-SL-25 dataset in German Sign Language for the purpose of additional
evaluation. Our VLM-based pipeline includes a face visibility detection, a sign
activity recognition, a text extraction from video content, and a judgment step
to validate alignment between video and text, implementing generic filtering,
annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we
assess the performance of two off-the-shelf SLT models on our filtered dataset
for German and American Sign Languages, with the goal of establishing baselines
and evaluating the robustness of recent models on automatically extracted,
slightly noisy data. Our work enables scalable, weakly supervised pretraining
for SLT and facilitates data acquisition from social media.


### [38] [A Critical Study of Automatic Evaluation in Sign Language Translation](https://arxiv.org/abs/2510.25434)
*Shakib Yazdani, Yasser Hamidullah, Cristina EspaÃ±a-Bonet, Eleftherios Avramidis, Josef van Genabith*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶ç³»ç»Ÿåˆ†æäº†æ‰‹è¯­ç¿»è¯‘è¯„ä¼°ä¸­æ–‡æœ¬æŒ‡æ ‡çš„å±€é™æ€§ï¼Œå‘ç°ä¼ ç»Ÿè¯æ±‡é‡å æŒ‡æ ‡å­˜åœ¨ä¸è¶³ï¼Œè€ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°å™¨è™½èƒ½æ›´å¥½æ•æ‰è¯­ä¹‰å¯¹ç­‰ï¼Œä½†å¯¹LLMç”Ÿæˆçš„é‡Šä¹‰å­˜åœ¨åè§ï¼Œæ­ç¤ºäº†æ„å»ºå¤šæ¨¡æ€è¯„ä¼°æ¡†æ¶çš„å¿…è¦æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰æ‰‹è¯­ç¿»è¯‘è¯„ä¼°ä¸»è¦ä¾èµ–BLEUã€ROUGEç­‰çº¯æ–‡æœ¬æŒ‡æ ‡ï¼Œä½†è¿™äº›æŒ‡æ ‡åœ¨å¤šå¤§ç¨‹åº¦ä¸Šèƒ½å¯é è¯„ä¼°æ‰‹è¯­ç¿»è¯‘è´¨é‡å°šä¸æ˜ç¡®ï¼Œéœ€è¦ç³»ç»Ÿåˆ†ææ–‡æœ¬æŒ‡æ ‡åœ¨æ‰‹è¯­ç¿»è¯‘è¯„ä¼°ä¸­çš„å±€é™æ€§å’Œå¯é æ€§ã€‚

**Method:** ç ”ç©¶åˆ†æäº†å…­ç§è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬BLEUã€chrFã€ROUGEå’ŒBLEURTç­‰ä¼ ç»ŸæŒ‡æ ‡ï¼Œä»¥åŠåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„G-Evalå’ŒGEMBAé›¶æ ·æœ¬ç›´æ¥è¯„ä¼°æ–¹æ³•ï¼Œå¹¶åœ¨é‡Šä¹‰ã€æ¨¡å‹è¾“å‡ºå¹»è§‰å’Œå¥å­é•¿åº¦å˜åŒ–ä¸‰ç§å—æ§æ¡ä»¶ä¸‹è¯„ä¼°è¿™äº›æŒ‡æ ‡çš„ä¸€è‡´æ€§å’Œé²æ£’æ€§ã€‚

**Result:** åˆ†æè¡¨æ˜è¯æ±‡é‡å æŒ‡æ ‡å­˜åœ¨æ˜æ˜¾å±€é™ï¼ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è¯„ä¼°å™¨èƒ½æ›´å¥½æ•æ‰ä¼ ç»ŸæŒ‡æ ‡å¸¸å¿½ç•¥çš„è¯­ä¹‰å¯¹ç­‰ï¼Œä½†å¯¹LLMç”Ÿæˆçš„é‡Šä¹‰å­˜åœ¨åè§ï¼›æ‰€æœ‰æŒ‡æ ‡éƒ½èƒ½æ£€æµ‹å¹»è§‰ï¼Œä½†BLEUè¿‡äºæ•æ„Ÿï¼Œè€ŒBLEURTå’ŒLLMè¯„ä¼°å™¨å¯¹ç»†å¾®å¹»è§‰æ¡ˆä¾‹ç›¸å¯¹å®½æ¾ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†çº¯æ–‡æœ¬è¯„ä¼°æŒ‡æ ‡åœ¨æ‰‹è¯­ç¿»è¯‘è¯„ä¼°ä¸­çš„æ ¹æœ¬å±€é™æ€§ï¼Œå¼ºè°ƒäº†å¼€å‘è¶…è¶Šæ–‡æœ¬æŒ‡æ ‡çš„å¤šæ¨¡æ€è¯„ä¼°æ¡†æ¶çš„å¿…è¦æ€§ï¼Œä»¥å®ç°å¯¹æ‰‹è¯­ç¿»è¯‘è¾“å‡ºçš„æ›´å…¨é¢è¯„ä¼°ï¼Œæ¨åŠ¨æ‰‹è¯­ç¿»è¯‘é¢†åŸŸçš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Automatic evaluation metrics are crucial for advancing sign language
translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are
only text-based, and it remains unclear to what extent text-based metrics can
reliably capture the quality of SLT outputs. To address this gap, we
investigate the limitations of text-based SLT evaluation metrics by analyzing
six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one
hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA
zero-shot direct assessment on the other hand. Specifically, we assess the
consistency and robustness of these metrics under three controlled conditions:
paraphrasing, hallucinations in model outputs, and variations in sentence
length. Our analysis highlights the limitations of lexical overlap metrics and
demonstrates that while LLM-based evaluators better capture semantic
equivalence often missed by conventional metrics, they can also exhibit bias
toward LLM-paraphrased translations. Moreover, although all metrics are able to
detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and
LLM-based evaluators are comparatively lenient toward subtle cases. This
motivates the need for multimodal evaluation frameworks that extend beyond
text-based metrics to enable a more holistic assessment of SLT outputs.


### [39] [EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis](https://arxiv.org/abs/2510.25628)
*Yusheng Liao, Chaoyi Wu, Junwei Liu, Shuyang Jiang, Pengcheng Qiu, Haowen Wang, Yun Yue, Shuai Zhen, Jian Wang, Qianrui Fan, Jinjie Gu, Ya Zhang, Yanfeng Wang, Yu Wang, Weidi Xie*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†EHR-Inså¤§è§„æ¨¡ç”µå­å¥åº·è®°å½•æ¨ç†æŒ‡ä»¤æ•°æ®é›†ã€EHR-R1æ¨ç†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ä»¥åŠEHR-Benchè¯„ä¼°åŸºå‡†ï¼Œæ˜¾è‘—æå‡äº†LLMåœ¨EHRåˆ†æä¸­çš„æ¨ç†èƒ½åŠ›å’Œä¸´åºŠç›¸å…³æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”µå­å¥åº·è®°å½•åˆ†æä¸­å­˜åœ¨ä»»åŠ¡è¦†ç›–èŒƒå›´æœ‰é™å’Œç¼ºä¹é¢å‘EHRçš„æ¨ç†èƒ½åŠ›ç­‰å…³é”®é™åˆ¶ï¼Œé˜»ç¢äº†å…¶åœ¨ä¸´åºŠå†³ç­–ä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚

**Method:** é‡‡ç”¨æ€ç»´å›¾é©±åŠ¨æ¡†æ¶ç”Ÿæˆå¤§è§„æ¨¡é«˜è´¨é‡æ¨ç†æ•°æ®ï¼Œé€šè¿‡é¢†åŸŸé€‚åº”ã€æ¨ç†å¢å¼ºå’Œå¼ºåŒ–å­¦ä¹ çš„å¤šé˜¶æ®µè®­ç»ƒèŒƒå¼å¼€å‘å‚æ•°é«˜è¾¾720äº¿çš„EHR-R1æ¨¡å‹ç³»åˆ—ï¼Œå¹¶æ„å»ºæ¶µç›–42ä¸ªä»»åŠ¡çš„EHR-Benchè¯„ä¼°åŸºå‡†ã€‚

**Result:** EHR-R1åœ¨MIMIC-Benchä¸Šè¶…è¶ŠGPT-4oè¶…è¿‡30åˆ†ï¼Œåœ¨EHRSHOTä¸Šå®ç°10%çš„é›¶æ ·æœ¬AUROCæå‡ï¼Œæ˜¾è‘—ä¼˜äºåŒ…æ‹¬DeepSeek-V3å’ŒGPT-4oåœ¨å†…çš„æœ€å…ˆè¿›å•†ä¸šå’Œå¼€æºLLMã€‚

**Conclusion:** EHR-Insã€EHR-R1å’ŒEHR-Benchå…±åŒæ¨åŠ¨äº†æ›´å¯é å’Œä¸´åºŠç›¸å…³çš„EHRåˆ†æå‘å±•ï¼Œä¸ºåŒ»ç–—AIç³»ç»Ÿæä¾›äº†ç³»ç»Ÿæ€§çš„é¢†åŸŸçŸ¥è¯†è·å–å’Œå¤šæ ·åŒ–æ¨ç†èƒ½åŠ›å¢å¼ºæ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Electronic Health Records (EHRs) contain rich yet complex information, and
their automated analysis is critical for clinical decision-making. Despite
recent advances of large language models (LLMs) in clinical workflows, their
ability to analyze EHRs remains limited due to narrow task coverage and lack of
EHR-oriented reasoning capabilities. This paper aims to bridge the gap,
specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning
instruction dataset, comprising 300k high-quality reasoning cases and 4M
non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a
thinking-graph-driven framework that enables to generate high-quality reasoning
data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced
LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage
training paradigm, including domain adaptation, reasoning enhancement, and
reinforcement learning, EHR-R1 systematically acquires domain knowledge and
diverse reasoning capabilities, enabling accurate and robust EHR analysis.
Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning
42 tasks, to comprehensively assess reasoning and prediction across EHR
scenarios. In experiments, we show that the resulting EHR-R1 consistently
outperforms state-of-the-art commercial and open-source LLMs (including
DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and
achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,
EHR-R1, and EHR-Bench have significantly advanced the development for more
reliable and clinically relevant EHR analysis.


### [40] [PairUni: Pairwise Training for Unified Multimodal Language Models](https://arxiv.org/abs/2510.25682)
*Jiani Zheng, Zhiyang Teng, Xiangtai Li, Anran Wang, Yu Tian, Kunpeng Qiu, Ye Tian, Haochen Wang, Zhuochen Wang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PairUniæ¡†æ¶ï¼Œé€šè¿‡å°†è§†è§‰è¯­è¨€æ¨¡å‹æ•°æ®é‡ç»„ä¸ºç†è§£-ç”Ÿæˆå¯¹ï¼Œå¹¶å¼€å‘Pair-GPROä¼˜åŒ–æ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ç†è§£ä¸ç”Ÿæˆä»»åŠ¡åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„å¹³è¡¡é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹éœ€è¦åœ¨å•ä¸€æ¶æ„ä¸­åŒæ—¶æ‰§è¡Œç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œä½†è¿™äº›ä»»åŠ¡ä¾èµ–äºå¼‚æ„æ•°æ®å’Œç›‘ç£ä¿¡å·ï¼Œå¯¼è‡´åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­éš¾ä»¥å®ç°ä»»åŠ¡é—´çš„å¹³è¡¡ä¼˜åŒ–ã€‚

**Method:** æå‡ºPairUniæ¡†æ¶ï¼Œé¦–å…ˆä½¿ç”¨GPT-4oå¢å¼ºå•ä»»åŠ¡æ•°æ®ï¼Œä¸ºç†è§£æ ·æœ¬ç”Ÿæˆæè¿°ã€ä¸ºç”Ÿæˆæ ·æœ¬ç”Ÿæˆé—®ç­”å¯¹ï¼Œå½¢æˆå¯¹é½çš„å®ä¾‹å¯¹ï¼›åŒæ—¶é€šè¿‡æ£€ç´¢è¯­ä¹‰ç›¸å…³çš„ç†è§£ç¤ºä¾‹æ„å»ºæ£€ç´¢å¯¹ã€‚åŸºäºæ­¤å¼€å‘Pair-GPROæ–¹æ³•ï¼Œé€šè¿‡ç›¸ä¼¼æ€§è¯„åˆ†è°ƒèŠ‚ä¼˜åŠ¿å‡½æ•°ï¼Œå¼ºåŒ–å¯¹é½è‰¯å¥½çš„æ ·æœ¬å­¦ä¹ å¹¶å‡å°‘ä»»åŠ¡å¹²æ‰°ã€‚

**Result:** æ„å»ºäº†åŒ…å«16Kä¸ªç†è§£-ç”Ÿæˆå¯¹çš„é«˜è´¨é‡æ•°æ®é›†PairUGï¼Œåœ¨å¼ºå¤§çš„Janus-Proç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šè¯„ä¼°ï¼Œç›¸æ¯”ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•å®ç°äº†æ›´å¹³è¡¡çš„æ€§èƒ½æå‡ï¼Œåœ¨å„ç§ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šéƒ½è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡æ•°æ®é‡ç»„å’Œé…å¯¹æ„ŸçŸ¥ä¼˜åŒ–ç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆç¼“è§£ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å¤šä»»åŠ¡å­¦ä¹ çš„å†²çªé—®é¢˜ï¼Œä¸ºå®ç°æ›´å¹³è¡¡çš„å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒæä¾›äº†æ–°æ€è·¯ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Unified vision-language models (UVLMs) must perform both understanding and
generation within a single architecture, but these tasks rely on heterogeneous
data and supervision, making it difficult to balance them during reinforcement
learning (RL). We propose PairUni, a unified framework that reorganizes data
into understanding-generation (UG) pairs and aligns optimization accordingly.
We first use GPT-o3 to augment single-task data, generating captions for
understanding samples and question-answer (QA) pairs for generation samples,
forming aligned pairs from the same instance. Additionally, for each generation
sample, we retrieve a semantically related understanding example to form a
retrieved pair, linking different but related data points. These paired
structures expose cross-task semantic correspondences and support consistent
policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware
variant based on Group Relative Policy Optimization. It assigns a similarity
score to each pair to modulate the advantage, strengthening learning from
well-aligned examples and reducing task interference. We curate a high-quality
dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on
the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on
various UVLMs, outperforming strong UVLM RL baselines. Code:
\href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}


### [41] [Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?](https://arxiv.org/abs/2510.25701)
*Saeed AlMarri, Kristof Juhasz, Mathieu Ravaut, Gautier Marti, Hamdan Al Ahbabi, Ibrahim Elfadel*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶ç³»ç»Ÿæ¯”è¾ƒäº†é›¶æ ·æœ¬LLMåˆ†ç±»å™¨ä¸LightGBMåœ¨é‡‘èé£é™©é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°LLMåœ¨ç»“æ„åŒ–é‡‘èæ•°æ®ä¸Šå­˜åœ¨å±€é™æ€§ä¸”è‡ªè§£é‡Šå¯é æ€§ä¸è¶³ï¼Œå¼ºè°ƒäº†åœ¨é£é™©æ•æ„Ÿé‡‘èç¯å¢ƒä¸­éƒ¨ç½²LLMæ—¶éœ€è¦å¯è§£é‡Šæ€§å®¡è®¡å’Œäººå·¥ç›‘ç£ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºåˆ†ç±»ä»»åŠ¡çš„çµæ´»æ›¿ä»£æ–¹æ¡ˆåœ¨é›¶æ ·æœ¬æç¤ºä¸‹è¢«å¹¿æ³›æ¢ç´¢ï¼Œä½†å…¶åœ¨ç»“æ„åŒ–è¡¨æ ¼æ•°æ®ç‰¹åˆ«æ˜¯é«˜é£é™©é‡‘èåº”ç”¨å¦‚é‡‘èé£é™©è¯„ä¼°ä¸­çš„é€‚ç”¨æ€§ä»æœªå……åˆ†ç ”ç©¶ï¼Œéœ€è¦ç³»ç»Ÿè¯„ä¼°LLMåœ¨æ­¤ç±»å…³é”®ä»»åŠ¡ä¸­çš„å®é™…è¡¨ç°å’Œå¯é æ€§ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨ç³»ç»Ÿæ¯”è¾ƒæ–¹æ³•ï¼Œåœ¨çœŸå®ä¸–ç•Œè´·æ¬¾è¿çº¦é¢„æµ‹ä»»åŠ¡ä¸­å¯¹æ¯”é›¶æ ·æœ¬LLMåˆ†ç±»å™¨ä¸æœ€å…ˆè¿›çš„æ¢¯åº¦æå‡æ¨¡å‹LightGBMï¼Œä½¿ç”¨SHAPè¿›è¡Œç‰¹å¾å½’å› åˆ†æï¼Œå¹¶è¯„ä¼°LLMç”Ÿæˆè‡ªè§£é‡Šçš„å¯é æ€§ï¼Œå…¨é¢è€ƒå¯Ÿæ¨¡å‹é¢„æµ‹æ€§èƒ½ã€ç‰¹å¾é‡è¦æ€§å’Œè§£é‡Šä¸€è‡´æ€§ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºLLMèƒ½å¤Ÿè¯†åˆ«å…³é”®é‡‘èé£é™©æŒ‡æ ‡ï¼Œä½†å…¶ç‰¹å¾é‡è¦æ€§æ’åºä¸LightGBMå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”LLMçš„è‡ªè§£é‡Šå¾€å¾€æ— æ³•ä¸ç»éªŒSHAPå½’å› ä¿æŒä¸€è‡´ï¼Œè¡¨æ˜LLMåœ¨ç»“æ„åŒ–é‡‘èé£é™©é¢„æµ‹ä¸­ä½œä¸ºç‹¬ç«‹æ¨¡å‹å­˜åœ¨æ˜æ˜¾å±€é™æ€§ã€‚

**Conclusion:** ç ”ç©¶å¼ºè°ƒäº†åœ¨é£é™©æ•æ„Ÿé‡‘èç¯å¢ƒä¸­éƒ¨ç½²LLMæ—¶éœ€è¦å¯è§£é‡Šæ€§å®¡è®¡ã€ä¸å¯è§£é‡Šæ¨¡å‹çš„åŸºçº¿æ¯”è¾ƒä»¥åŠäººå·¥ç›‘ç£çš„å¿…è¦æ€§ï¼Œè¿™äº›å‘ç°å¯¹é‡‘èé¢†åŸŸAIç³»ç»Ÿçš„å¯ä¿¡éƒ¨ç½²å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ï¼Œæé†’ä¸šç•Œå…³æ³¨LLMè‡ªè§£é‡Šçš„å¯é æ€§é—®é¢˜ã€‚

---

#### ğŸ“„ Abstract
Large Language Models (LLMs) are increasingly explored as flexible
alternatives to classical machine learning models for classification tasks
through zero-shot prompting. However, their suitability for structured tabular
data remains underexplored, especially in high-stakes financial applications
such as financial risk assessment. This study conducts a systematic comparison
between zero-shot LLM-based classifiers and LightGBM, a state-of-the-art
gradient-boosting model, on a real-world loan default prediction task. We
evaluate their predictive performance, analyze feature attributions using SHAP,
and assess the reliability of LLM-generated self-explanations. While LLMs are
able to identify key financial risk indicators, their feature importance
rankings diverge notably from LightGBM, and their self-explanations often fail
to align with empirical SHAP attributions. These findings highlight the
limitations of LLMs as standalone models for structured financial risk
prediction and raise concerns about the trustworthiness of their self-generated
explanations. Our results underscore the need for explainability audits,
baseline comparisons with interpretable models, and human-in-the-loop oversight
when deploying LLMs in risk-sensitive financial environments.


### [42] [DiagramEval: Evaluating LLM-Generated Diagrams via Graphs](https://arxiv.org/abs/2510.25761)
*Chumeng Liang, Jiaxuan You*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†DiagramEvalï¼Œä¸€ç§æ–°é¢–çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°LLMç”Ÿæˆçš„æ¼”ç¤ºå›¾è´¨é‡ã€‚è¯¥æ–¹æ³•å°†å›¾è¡¨ç¤ºä¸ºå›¾ç»“æ„ï¼Œé€šè¿‡èŠ‚ç‚¹å¯¹é½å’Œè·¯å¾„å¯¹é½ä¸¤ä¸ªæ–°æŒ‡æ ‡ç»„æ¥é‡åŒ–è¯„ä¼°å›¾çš„è´¨é‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç ”ç©¶æ—¨åœ¨è§£å†³LLMç”Ÿæˆå›¾è¯„ä¼°ä¸­ç¼ºä¹è¶³å¤ŸåŒºåˆ†æ€§å’Œå¯è§£é‡Šæ€§æŒ‡æ ‡çš„é—®é¢˜ã€‚å°½ç®¡å›¾åœ¨è®ºæ–‡ä¸­ä½œä¸ºå›¾åƒå‘ˆç°ï¼Œä½†æ ‡å‡†å›¾åƒç”Ÿæˆæ¨¡å‹éš¾ä»¥ç”Ÿæˆå…·æœ‰æ˜ç¡®ç»“æ„çš„æ¸…æ™°å›¾ï¼Œè€ŒLLMç›´æ¥ç”ŸæˆSVGæ ¼å¼çš„å›¾æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œä½†ç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•ã€‚

**Method:** DiagramEvalå°†å›¾æ¦‚å¿µåŒ–ä¸ºå›¾ç»“æ„ï¼Œå°†æ–‡æœ¬å…ƒç´ è§†ä¸ºèŠ‚ç‚¹ï¼Œè¿æ¥å…³ç³»è§†ä¸ºæœ‰å‘è¾¹ã€‚è¯¥æ–¹æ³•æå‡ºäº†ä¸¤ä¸ªæ–°çš„æŒ‡æ ‡ç»„ï¼šèŠ‚ç‚¹å¯¹é½å’Œè·¯å¾„å¯¹é½ï¼Œé€šè¿‡å›¾è¡¨ç¤ºæ¥è¯„ä¼°å›¾çš„è´¨é‡ã€‚

**Result:** ç ”ç©¶é¦–æ¬¡æœ‰æ•ˆè¯„ä¼°äº†æœ€å…ˆè¿›LLMåœ¨è¿‘æœŸç ”ç©¶æ–‡çŒ®ä¸Šç”Ÿæˆçš„å›¾ï¼Œå®šé‡è¯æ˜äº†æ‰€ææŒ‡æ ‡çš„æœ‰æ•ˆæ€§ã€‚å¢å¼ºçš„å¯è§£é‡Šæ€§ä¸ºLLMç”Ÿæˆå›¾çš„ç‰¹å¾æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

**Conclusion:** DiagramEvalä¸ºLLMç”Ÿæˆå›¾çš„è¯„ä¼°æä¾›äº†é‡åŒ–æ¡†æ¶ï¼Œå…¶å¢å¼ºçš„å¯è§£é‡Šæ€§æœ‰åŠ©äºæ·±å…¥ç†è§£LLMç”Ÿæˆå›¾çš„ç‰¹æ€§ï¼Œä¸ºæœªæ¥å›¾ç”Ÿæˆç ”ç©¶æä¾›äº†é‡è¦çš„è¯„ä¼°å·¥å…·å’Œæ´å¯Ÿã€‚

---

#### ğŸ“„ Abstract
Diagrams play a central role in research papers for conveying ideas, yet they
are often notoriously complex and labor-intensive to create. Although diagrams
are presented as images, standard image generative models struggle to produce
clear diagrams with well-defined structure. We argue that a promising direction
is to generate demonstration diagrams directly in textual form as SVGs, which
can leverage recent advances in large language models (LLMs). However, due to
the complexity of components and the multimodal nature of diagrams,
sufficiently discriminative and explainable metrics for evaluating the quality
of LLM-generated diagrams remain lacking. In this paper, we propose
DiagramEval, a novel evaluation metric designed to assess demonstration
diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams
as graphs, treating text elements as nodes and their connections as directed
edges, and evaluates diagram quality using two new groups of metrics: node
alignment and path alignment. For the first time, we effectively evaluate
diagrams produced by state-of-the-art LLMs on recent research literature,
quantitatively demonstrating the validity of our metrics. Furthermore, we show
how the enhanced explainability of our proposed metrics offers valuable
insights into the characteristics of LLM-generated diagrams. Code:
https://github.com/ulab-uiuc/diagram-eval.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured Mixture of Experts](https://arxiv.org/abs/2510.25091)
*Peilin Tan, Liang Xie, Churan Zhi, Dian Tu, Chuanqi Shi*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºH3M-SSMoEsæ¨¡å‹ï¼Œé€šè¿‡è¶…å›¾å¤šæ¨¡æ€æ¶æ„ç»“åˆLLMæ¨ç†å’Œé£æ ¼ç»“æ„åŒ–ä¸“å®¶æ··åˆï¼Œè§£å†³äº†è‚¡ç¥¨é¢„æµ‹ä¸­å¤æ‚æ—¶ç©ºä¾èµ–ã€å¼‚æ„æ¨¡æ€å’ŒåŠ¨æ€è‚¡ç¥¨å…³ç³»çš„ç»Ÿä¸€å»ºæ¨¡é—®é¢˜ï¼Œåœ¨å¤šä¸ªä¸»è¦è‚¡ç¥¨å¸‚åœºå®ç°äº†é¢„æµ‹ç²¾åº¦å’ŒæŠ•èµ„æ€§èƒ½çš„æ˜¾è‘—æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è‚¡ç¥¨è¿åŠ¨é¢„æµ‹é¢ä¸´å¤æ‚æ—¶ç©ºä¾èµ–ã€å¼‚æ„æ¨¡æ€å’ŒåŠ¨æ€æ¼”åŒ–çš„è‚¡ç¥¨é—´å…³ç³»çš„æ ¹æœ¬æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨å¯æ‰©å±•æ¡†æ¶å†…ç»Ÿä¸€ç»“æ„ã€è¯­ä¹‰å’Œæœºåˆ¶è‡ªé€‚åº”å»ºæ¨¡ï¼Œå­˜åœ¨å»ºæ¨¡èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚

**Method:** æå‡ºH3M-SSMoEsæ¶æ„ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®åˆ›æ–°ï¼šå¤šä¸Šä¸‹æ–‡å¤šæ¨¡æ€è¶…å›¾é€šè¿‡å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡è¶…å›¾åˆ†å±‚æ•æ‰æ—¶ç©ºåŠ¨æ€å’ŒæŒä¹…è‚¡ç¥¨ä¾èµ–ï¼Œé‡‡ç”¨å…±äº«è·¨æ¨¡æ€è¶…è¾¹å’ŒJensen-Shannonæ•£åº¦åŠ æƒæœºåˆ¶ï¼›LLMå¢å¼ºæ¨ç†æ¨¡å—åˆ©ç”¨å†»ç»“å¤§è¯­è¨€æ¨¡å‹å’Œè½»é‡é€‚é…å™¨è¯­ä¹‰èåˆé‡åŒ–ä¸æ–‡æœ¬æ¨¡æ€ï¼›é£æ ¼ç»“æ„åŒ–ä¸“å®¶æ··åˆç»“åˆå…±äº«å¸‚åœºä¸“å®¶å’Œè¡Œä¸šä¸“ä¸šä¸“å®¶ï¼Œé€šè¿‡å¯å­¦ä¹ é£æ ¼å‘é‡å®ç°æœºåˆ¶æ„ŸçŸ¥ä¸“ä¸šåŒ–ã€‚

**Result:** åœ¨ä¸‰ä¸ªä¸»è¦è‚¡ç¥¨å¸‚åœºçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒH3M-SSMoEsåœ¨é¢„æµ‹ç²¾åº¦å’ŒæŠ•èµ„æ€§èƒ½ä¸Šå‡è¶…è¶Šæœ€å…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶å±•ç°å‡ºæœ‰æ•ˆçš„é£é™©æ§åˆ¶èƒ½åŠ›ï¼Œè¯æ˜äº†æ¨¡å‹åœ¨çœŸå®å¸‚åœºç¯å¢ƒä¸­çš„ä¼˜è¶Šæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†ç»Ÿä¸€ç»“æ„ã€è¯­ä¹‰å’Œæœºåˆ¶è‡ªé€‚åº”å»ºæ¨¡åœ¨å¤æ‚é‡‘èé¢„æµ‹ä»»åŠ¡ä¸­çš„é‡è¦æ€§ï¼Œä¸ºå¤šæ¨¡æ€æ—¶åºé¢„æµ‹æä¾›äº†å¯æ‰©å±•æ¡†æ¶ï¼ŒåŒæ—¶éªŒè¯äº†LLMå¢å¼ºæ¨ç†å’Œé£æ ¼ç»“æ„åŒ–ä¸“å®¶æ··åˆåœ¨é‡‘èé¢†åŸŸåº”ç”¨çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥æ™ºèƒ½æŠ•èµ„å†³ç­–ç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚

---

#### ğŸ“„ Abstract
Stock movement prediction remains fundamentally challenging due to complex
temporal dependencies, heterogeneous modalities, and dynamically evolving
inter-stock relationships. Existing approaches often fail to unify structural,
semantic, and regime-adaptive modeling within a scalable framework. This work
introduces H3M-SSMoEs, a novel Hypergraph-based MultiModal architecture with
LLM reasoning and Style-Structured Mixture of Experts, integrating three key
innovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically
captures fine-grained spatiotemporal dynamics via a Local Context Hypergraph
(LCH) and persistent inter-stock dependencies through a Global Context
Hypergraph (GCH), employing shared cross-modal hyperedges and Jensen-Shannon
Divergence weighting mechanism for adaptive relational learning and cross-modal
alignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large
language model with lightweight adapters to semantically fuse and align
quantitative and textual modalities, enriching representations with
domain-specific financial knowledge; and (3) a Style-Structured Mixture of
Experts (SSMoEs) that combines shared market experts and industry-specialized
experts, each parameterized by learnable style vectors enabling regime-aware
specialization under sparse activation. Extensive experiments on three major
stock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in
both superior predictive accuracy and investment performance, while exhibiting
effective risk control. Datasets, source code, and model weights are available
at our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs.


### [44] [KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA](https://arxiv.org/abs/2510.25101)
*Zhuo Chen, Fei Wang, Zixuan Li, Zhao Zhang, Weiwei Ding, Chuanguang Yang, Yongjun Xu, Xiaolong Jin, Jiafeng Guo*

#### ğŸ§© TL;DR
KnowCoder-A1æå‡ºäº†ä¸€ç§åŸºäºç»“æœç›‘ç£çš„å¤šé˜¶æ®µè¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒLLMåœ¨çŸ¥è¯†åº“ä¸Šè¿›è¡Œè‡ªä¸»ä»£ç†æ¨ç†ï¼Œæ˜¾è‘—æå‡äº†KBQAæ€§èƒ½ï¼Œåœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹å®ç°äº†11.1%çš„ç›¸å¯¹æ”¹è¿›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰KBQAæ–¹æ³•é€šå¸¸é€šè¿‡è¿‡ç¨‹ç›‘ç£å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œè¿™ç§ç›‘ç£æ–¹å¼æä¾›è¾ƒå¼±çš„æ¢ç´¢æ¿€åŠ±ï¼Œæ— æ³•æœ‰æ•ˆå¢å¼ºä»£ç†æ¨ç†èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿæ¿€åŠ±è‡ªä¸»æ¢ç´¢çš„è®­ç»ƒæ–¹æ³•ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨å¤šé˜¶æ®µè¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé¦–å…ˆé€šè¿‡åŸºäºç»“æœçš„æ‹’ç»é‡‡æ ·è·å¾—é«˜è´¨é‡è½¨è¿¹è¿›è¡ŒåŸºç¡€å¾®è°ƒï¼Œç„¶ååº”ç”¨ä»æ˜“åˆ°éš¾çš„å¥–åŠ±è°ƒåº¦ç­–ç•¥æ¥ç¼“è§£ç»“æœç›‘ç£ä¸­çš„å¥–åŠ±ç¨€ç–æ€§é—®é¢˜ã€‚

**Result:** KnowCoder-A1åœ¨ä¸‰ä¸ªä¸»æµæ•°æ®é›†ä¸ŠæŒç»­ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨GrailQAçš„é›¶æ ·æœ¬å­é›†ä¸Šå®ç°äº†11.1%çš„ç›¸å¯¹æ”¹è¿›ï¼ŒåŒæ—¶ä»…ä½¿ç”¨åäºŒåˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜åŸºäºç»“æœç›‘ç£çš„è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæœ‰æ•ˆåŸ¹å…»LLMçš„è‡ªä¸»ä»£ç†æ¨ç†èƒ½åŠ›ï¼Œä¸ºçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿæä¾›äº†æ›´é«˜æ•ˆçš„è®­ç»ƒèŒƒå¼ï¼Œå±•ç¤ºäº†åœ¨æœ‰é™ç›‘ç£ä¸‹å®ç°å¼ºå¤§æ¨ç†æ€§èƒ½çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Knowledge Base Question Answering (KBQA) aims to answer natural-language
questions over a structured Knowledge Base (KB). Recent work improves KBQA by
adopting an agentic reasoning paradigm, in which Large Language Models (LLMs)
iteratively decompose a question, generate its corresponding logical queries,
and interact with the KB to derive the answer. However, these methods typically
fine-tune LLMs on reasoning trajectories synthesized via process supervision,
which offers weak incentives for exploration and thus fails to strengthen the
agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that
can autonomously perform agentic reasoning on KBs to obtain answers. To
incentivize autonomous exploration, KnowCoder-A1 trains the LLM under
outcome-only supervision via a multi-stage curriculum reinforcement learning
with an easy-to-hard curriculum. To establish foundational agentic
capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of
high-quality trajectories obtained through outcome-based rejection sampling.
Then, to alleviate the reward sparsity inherent in outcome-only supervision, it
applies multi-stage curriculum RL with reward schedules that progress from easy
to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful
reasoning behaviors and consistently outperforms prior approaches across three
mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1
achieves up to an 11.1% relative improvement while using only one-twelfth of
the training data, demonstrating strong agentic reasoning capabilities.


### [45] [Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models](https://arxiv.org/abs/2510.25179)
*Juan Ren, Mark Dras, Usman Naseem*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Agentic Moderationæ¡†æ¶ï¼Œåˆ©ç”¨ä¸“ä¸šåŒ–çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ¥é˜²å¾¡å¤šæ¨¡æ€ç³»ç»Ÿå¯¹æŠ—è¶Šç‹±æ”»å‡»ï¼Œé€šè¿‡åŠ¨æ€åä½œçš„æ™ºèƒ½ä½“å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œå¯è§£é‡Šçš„å†…å®¹å®¡æ ¸ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å®‰å…¨å¯¹é½æ–¹æ³•é€šå¸¸ä½œä¸ºé™æ€å±‚åº”ç”¨äºè¾“å…¥æˆ–è¾“å‡ºï¼Œä»…æä¾›äºŒå…ƒåˆ†ç±»ï¼ˆå®‰å…¨æˆ–ä¸å®‰å…¨ï¼‰ï¼Œç¼ºä¹åŠ¨æ€æ€§ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œå¯è§£é‡Šæ€§ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹å¤æ‚çš„è¶Šç‹±æ”»å‡»ã€‚

**Method:** æå‡ºäº†Agentic Moderationæ¡†æ¶ï¼ŒåŒ…å«Shieldã€Responderã€Evaluatorå’ŒReflectorå››ä¸ªåŠ¨æ€åä½œæ™ºèƒ½ä½“ï¼Œå®ç°æ¨¡å‹æ— å…³çš„å¤šæ¨¡æ€ç³»ç»Ÿå®‰å…¨é˜²å¾¡ï¼Œæä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œå¯è§£é‡Šçš„å®¡æ ¸æœºåˆ¶ã€‚

**Result:** åœ¨äº”ä¸ªæ•°æ®é›†å’Œå››ä¸ªä»£è¡¨æ€§å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å°†æ”»å‡»æˆåŠŸç‡é™ä½7-19%ï¼Œä¿æŒç¨³å®šçš„ä¸è·Ÿéšç‡ï¼Œå¹¶å°†æ‹’ç»ç‡æé«˜4-20%ï¼Œå®ç°äº†é²æ£’ã€å¯è§£é‡Šä¸”å¹³è¡¡çš„å®‰å…¨æ€§èƒ½ã€‚

**Conclusion:** é€šè¿‡åˆ©ç”¨æ™ºèƒ½ä½“æ¶æ„çš„çµæ´»æ€§å’Œæ¨ç†èƒ½åŠ›ï¼ŒAgentic Moderationæä¾›äº†æ¨¡å—åŒ–ã€å¯æ‰©å±•å’Œç»†ç²’åº¦çš„å®‰å…¨æ‰§è¡Œï¼Œçªæ˜¾äº†æ™ºèƒ½ä½“ç³»ç»Ÿä½œä¸ºè‡ªåŠ¨åŒ–å®‰å…¨æ²»ç†åŸºç¡€çš„æ›´å¹¿æ³›æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Agentic methods have emerged as a powerful and autonomous paradigm that
enhances reasoning, collaboration, and adaptive control, enabling systems to
coordinate and independently solve complex tasks. We extend this paradigm to
safety alignment by introducing Agentic Moderation, a model-agnostic framework
that leverages specialised agents to defend multimodal systems against
jailbreak attacks. Unlike prior approaches that apply as a static layer over
inputs or outputs and provide only binary classifications (safe or unsafe), our
method integrates dynamic, cooperative agents, including Shield, Responder,
Evaluator, and Reflector, to achieve context-aware and interpretable
moderation. Extensive experiments across five datasets and four representative
Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the
Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),
and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,
and well-balanced safety performance. By harnessing the flexibility and
reasoning capacity of agentic architectures, Agentic Moderation provides
modular, scalable, and fine-grained safety enforcement, highlighting the
broader potential of agentic systems as a foundation for automated safety
governance.


### [46] [ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents](https://arxiv.org/abs/2510.25668)
*Tianyu Yang, Terry Ruas, Yijun Tian, Jan Philip Wahle, Daniel Kurzawe, Bela Gipp*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ALDENï¼Œä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šè½®äº¤äº’æ¡†æ¶ï¼Œé€šè¿‡å°†è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒä¸ºä¸»åŠ¨å¯¼èˆªé•¿æ–‡æ¡£çš„æ™ºèƒ½ä½“ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨é•¿æ–‡æ¡£ç†è§£ä¸­çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥æ–°é¢–çš„é¡µé¢ç´¢å¼•è®¿é—®åŠ¨ä½œå’Œè§†è§‰è¯­ä¹‰é”šå®šæœºåˆ¶ï¼Œåœ¨äº”ä¸ªé•¿æ–‡æ¡£åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦è·¨å¤šé¡µåˆ†æå’Œä¿¡æ¯æ•´åˆçš„é•¿è€Œå¤æ‚çš„æ–‡æ¡£æ—¶è¡¨ç°ä¸ä½³ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–å›ºå®šçš„æ¨ç†æ¨¡æ¿æˆ–åˆšæ€§æµç¨‹ï¼Œè¿«ä½¿æ¨¡å‹å¤„äºè¢«åŠ¨è§’è‰²ï¼Œé™åˆ¶äº†æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

**Method:** ALDENæ¡†æ¶é‡‡ç”¨å¤šè½®å¼ºåŒ–å­¦ä¹ å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¼•å…¥é¡µé¢ç´¢å¼•è®¿é—®åŠ¨ä½œä»¥åˆ©ç”¨æ–‡æ¡£ç»“æ„ï¼Œæå‡ºåŸºäºè§„åˆ™çš„è·¨å±‚çº§å¥–åŠ±æœºåˆ¶è¿›è¡Œå¯†é›†è¿‡ç¨‹ç›‘ç£ï¼Œå¹¶è®¾è®¡è§†è§‰è¯­ä¹‰é”šå®šæœºåˆ¶é€šè¿‡åŒè·¯å¾„KLæ•£åº¦çº¦æŸåˆ†åˆ«ç¨³å®šè§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºä»¥è§£å†³è®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚

**Result:** åœ¨åŸºäºä¸‰ä¸ªå¼€æºæ•°æ®é›†æ„å»ºçš„è¯­æ–™åº“ä¸Šè®­ç»ƒåï¼ŒALDENåœ¨äº”ä¸ªé•¿æ–‡æ¡£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œæ˜¾è‘—æå‡äº†é•¿æ–‡æ¡£ç†è§£çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

**Conclusion:** ALDENæ ‡å¿—ç€ä»è¢«åŠ¨æ–‡æ¡£é˜…è¯»å‘èƒ½å¤Ÿè‡ªä¸»å¯¼èˆªå’Œè·¨é•¿æ–‡æ¡£æ¨ç†çš„æ™ºèƒ½ä½“çš„é‡è¦è¿›å±•ï¼Œä¸ºæ›´å‡†ç¡®é«˜æ•ˆçš„é•¿æ–‡æ¡£ç†è§£æä¾›äº†ç¨³å¥è·¯å¾„ï¼Œå±•ç¤ºäº†ä¸»åŠ¨äº¤äº’å¼æ–‡æ¡£ç†è§£æ–¹æ³•çš„å·¨å¤§æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLMs) excel at interpreting text-rich images but
struggle with long, visually complex documents that demand analysis and
integration of information spread across multiple pages. Existing approaches
typically rely on fixed reasoning templates or rigid pipelines, which force
VLMs into a passive role and hinder both efficiency and generalization. We
present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement
learning framework that fine-tunes VLMs as interactive agents capable of
actively navigating long, visually rich documents. ALDEN introduces a novel
fetch action that directly accesses the page by index, complementing the
classic search action and better exploiting document structure. For dense
process supervision and efficient training, we propose a rule-based cross-level
reward that provides both turn- and token-level signals. To address the
empirically observed training instability caused by numerous visual tokens from
long documents, we further propose a visual-semantic anchoring mechanism that
applies a dual-path KL-divergence constraint to stabilize visual and textual
representations separately during training. Trained on a corpus constructed
from three open-source datasets, ALDEN achieves state-of-the-art performance on
five long-document benchmarks. Overall, ALDEN marks a step beyond passive
document reading toward agents that autonomously navigate and reason across
long, visually rich documents, offering a robust path to more accurate and
efficient long-document understanding.
