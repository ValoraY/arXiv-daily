{"id": "2510.07546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07546", "abs": "https://arxiv.org/abs/2510.07546", "authors": ["Soroush Mehraban", "Vida Adeli", "Jacob Rommann", "Babak Taati", "Kyryl Truskovskyi"], "title": "PickStyle: Video-to-Video Style Transfer with Context-Style Adapters", "comment": null, "summary": "We address the task of video style transfer with diffusion models, where the\ngoal is to preserve the context of an input video while rendering it in a\ntarget style specified by a text prompt. A major challenge is the lack of\npaired video data for supervision. We propose PickStyle, a video-to-video style\ntransfer framework that augments pretrained video diffusion backbones with\nstyle adapters and benefits from paired still image data with source-style\ncorrespondences for training. PickStyle inserts low-rank adapters into the\nself-attention layers of conditioning modules, enabling efficient\nspecialization for motion-style transfer while maintaining strong alignment\nbetween video content and style. To bridge the gap between static image\nsupervision and dynamic video, we construct synthetic training clips from\npaired images by applying shared augmentations that simulate camera motion,\nensuring temporal priors are preserved. In addition, we introduce Context-Style\nClassifier-Free Guidance (CS-CFG), a novel factorization of classifier-free\nguidance into independent text (style) and video (context) directions. CS-CFG\nensures that context is preserved in generated video while the style is\neffectively transferred. Experiments across benchmarks show that our approach\nachieves temporally coherent, style-faithful, and content-preserving video\ntranslations, outperforming existing baselines both qualitatively and\nquantitatively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PickStyle\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u914d\u5668\u548c\u4e0a\u4e0b\u6587-\u98ce\u683c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u98ce\u683c\u8fc1\u79fb\u4e2d\u7f3a\u4e4f\u914d\u5bf9\u76d1\u7763\u6570\u636e\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65f6\u95f4\u4e00\u81f4\u3001\u98ce\u683c\u5fe0\u5b9e\u4e14\u5185\u5bb9\u4fdd\u6301\u7684\u89c6\u9891\u8f6c\u6362\u3002", "motivation": "\u89c6\u9891\u98ce\u683c\u8fc1\u79fb\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u7f3a\u4e4f\u914d\u5bf9\u7684\u89c6\u9891\u6570\u636e\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u4fdd\u6301\u89c6\u9891\u5185\u5bb9\u7684\u540c\u65f6\u6709\u6548\u4f20\u9012\u76ee\u6807\u98ce\u683c\uff0c\u4e14\u96be\u4ee5\u4fdd\u8bc1\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faPickStyle\u6846\u67b6\uff0c\u5728\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u63d2\u5165\u4f4e\u79e9\u9002\u914d\u5668\u5230\u81ea\u6ce8\u610f\u529b\u5c42\uff0c\u5229\u7528\u914d\u5bf9\u9759\u6001\u56fe\u50cf\u6570\u636e\u6784\u5efa\u5408\u6210\u8bad\u7ec3\u7247\u6bb5\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587-\u98ce\u683c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u6280\u672f\u5c06\u6587\u672c\u98ce\u683c\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\u5f15\u5bfc\u65b9\u5411\u72ec\u7acb\u5206\u89e3\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65f6\u95f4\u4e00\u81f4\u3001\u98ce\u683c\u5fe0\u5b9e\u4e14\u5185\u5bb9\u4fdd\u6301\u7684\u89c6\u9891\u8f6c\u6362\u6548\u679c\uff0c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u4f4e\u79e9\u9002\u914d\u5668\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u5229\u7528\u9759\u6001\u56fe\u50cf\u6570\u636e\u6709\u6548\u89e3\u51b3\u89c6\u9891\u98ce\u683c\u8fc1\u79fb\u95ee\u9898\uff0c\u4e3a\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u548c\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2510.07550", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07550", "abs": "https://arxiv.org/abs/2510.07550", "authors": ["Saman Motamed", "Minghao Chen", "Luc Van Gool", "Iro Laina"], "title": "TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility", "comment": null, "summary": "Despite impressive visual fidelity, modern video generative models frequently\nproduce sequences that violate intuitive physical laws, such as objects\nfloating, teleporting, or morphing in ways that defy causality. While humans\ncan easily detect such implausibilities, there remains no robust method for\nquantitatively assessing physical realism in video. In this work, we explore\nwhether Video-Language Models (VLMs) can be trained to serve as reliable judges\nof physical plausibility. We find that existing VLMs struggle to identify\nphysics violations, exposing fundamental limitations in their temporal and\ncausal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe\nthat combines a balanced training dataset with a trajectory-aware attention\nmodule to improve motion encoding and discrimination in VLMs. To evaluate\nphysical reasoning more rigorously, we propose ImplausiBench, a benchmark of\n300 videos (150 real, 150 generated) that removes linguistic biases and\nisolates visual-temporal understanding. Performance is reported both with\ngold-standard human judgments and stricter LLM-as-judge metrics. Together,\nTRAVL and ImplausiBench offer a unified framework for probing and improving\nphysical plausibility in multimodal models, shedding light on a challenging and\nunderexplored aspect of visual-temporal understanding.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86TRAVL\u5fae\u8c03\u65b9\u6cd5\u548cImplausiBench\u57fa\u51c6\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u5408\u7406\u6027\u8bc4\u4f30\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u89c6\u9891\u751f\u6210\u6a21\u578b\u8fdd\u53cd\u7269\u7406\u89c4\u5f8b\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\u3002", "motivation": "\u73b0\u4ee3\u89c6\u9891\u751f\u6210\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u8fdd\u53cd\u7269\u7406\u89c4\u5f8b\u7684\u5e8f\u5217\uff0c\u5982\u7269\u4f53\u6f02\u6d6e\u3001\u77ac\u79fb\u6216\u8fdd\u80cc\u56e0\u679c\u5173\u7cfb\u7684\u5f62\u53d8\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5b9a\u91cf\u8bc4\u4f30\u7269\u7406\u771f\u5b9e\u6027\u7684\u9c81\u68d2\u65b9\u6cd5\uff0c\u73b0\u6709\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u7269\u7406\u8fdd\u89c4\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u66b4\u9732\u4e86\u5176\u65f6\u5e8f\u548c\u56e0\u679c\u63a8\u7406\u7684\u6839\u672c\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86TRAVL\u5fae\u8c03\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e73\u8861\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u8f68\u8ff9\u611f\u77e5\u6ce8\u610f\u529b\u6a21\u5757\u6765\u6539\u8fdb\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8fd0\u52a8\u7f16\u7801\u548c\u5224\u522b\u80fd\u529b\uff0c\u540c\u65f6\u5f15\u5165\u4e86ImplausiBench\u57fa\u51c6\uff0c\u5305\u542b300\u4e2a\u89c6\u9891\uff08150\u4e2a\u771f\u5b9e\u3001150\u4e2a\u751f\u6210\uff09\uff0c\u65e8\u5728\u6d88\u9664\u8bed\u8a00\u504f\u89c1\u5e76\u9694\u79bb\u89c6\u89c9-\u65f6\u5e8f\u7406\u89e3\u3002", "result": "\u6027\u80fd\u8bc4\u4f30\u540c\u65f6\u91c7\u7528\u9ec4\u91d1\u6807\u51c6\u7684\u4eba\u7c7b\u5224\u65ad\u548c\u66f4\u4e25\u683c\u7684LLM-as-judge\u6307\u6807\uff0cTRAVL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u7269\u7406\u8fdd\u89c4\u7684\u8bc6\u522b\u80fd\u529b\uff0cImplausiBench\u4e3a\u7269\u7406\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "TRAVL\u548cImplausiBench\u5171\u540c\u4e3a\u63a2\u7d22\u548c\u6539\u8fdb\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u7269\u7406\u5408\u7406\u6027\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u89c6\u89c9-\u65f6\u5e8f\u7406\u89e3\u4e2d\u5177\u6709\u6311\u6218\u6027\u4e14\u672a\u5145\u5206\u63a2\u7d22\u7684\u65b9\u9762\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u7269\u7406\u771f\u5b9e\u6027\u8bc4\u4f30\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.07556", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07556", "abs": "https://arxiv.org/abs/2510.07556", "authors": ["Rafin Hassan", "Zarin Tasnim Roshni", "Rafiqul Bari", "Alimul Islam", "Nabeel Mohammed", "Moshiur Farazi", "Shafin Rahman"], "title": "Label Semantics for Robust Hyperspectral Image Classification", "comment": "This work has been accepted for publication in the proceedings of\n  IJCNN 2025", "summary": "Hyperspectral imaging (HSI) classification is a critical tool with widespread\napplications across diverse fields such as agriculture, environmental\nmonitoring, medicine, and materials science. Due to the limited availability of\nhigh-quality training samples and the high dimensionality of spectral data, HSI\nclassification models are prone to overfitting and often face challenges in\nbalancing accuracy and computational complexity. Furthermore, most of HSI\nclassification models are monomodal, where it solely relies on spectral-spatial\ndata to learn decision boundaries in the high dimensional embedding space. To\naddress this, we propose a general-purpose Semantic Spectral-Spatial Fusion\nNetwork (S3FN) that uses contextual, class specific textual descriptions to\ncomplement the training of an HSI classification model. Specifically, S3FN\nleverages LLMs to generate comprehensive textual descriptions for each class\nlabel that captures their unique characteristics and spectral behaviors. These\ndescriptions are then embedded into a vector space using a pre-trained text\nencoder such as BERT or RoBERTa to extract meaningful label semantics which in\nturn leads to a better feature-label alignment for improved classification\nperformance. To demonstrate the effectiveness of our approach, we evaluate our\nmodel on three diverse HSI benchmark datasets - Hyperspectral Wood,\nHyperspectralBlueberries, and DeepHS-Fruit and report significant performance\nboost. Our results highlight the synergy between textual semantics and\nspectral-spatial data, paving the way for further advancements in semantically\naugmented HSI classification models. Codes are be available in:\nhttps://github.com/milab-nsu/S3FN", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8bed\u4e49\u5149\u8c31-\u7a7a\u95f4\u878d\u5408\u7f51\u7edc\uff08S3FN\uff09\uff0c\u901a\u8fc7\u5229\u7528LLM\u751f\u6210\u7684\u7c7b\u522b\u6587\u672c\u63cf\u8ff0\u6765\u589e\u5f3a\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5355\u6a21\u6001\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5149\u8c31\u7a7a\u95f4\u6570\u636e\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u9762\u4e34\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6837\u672c\u7a00\u7f3a\u3001\u6570\u636e\u7ef4\u5ea6\u9ad8\u5bfc\u81f4\u6a21\u578b\u6613\u8fc7\u62df\u5408\u7684\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u5355\u6a21\u6001\uff0c\u4ec5\u4f9d\u8d56\u5149\u8c31\u7a7a\u95f4\u6570\u636e\u5728\u9ad8\u7ef4\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b66\u4e60\u51b3\u7b56\u8fb9\u754c\uff0c\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\u7684\u8865\u5145\u3002", "method": "S3FN\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u6807\u7b7e\u751f\u6210\u5168\u9762\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u6355\u6349\u5176\u72ec\u7279\u7279\u5f81\u548c\u5149\u8c31\u884c\u4e3a\uff0c\u7136\u540e\u4f7f\u7528\u9884\u8bad\u7ec3\u6587\u672c\u7f16\u7801\u5668\uff08\u5982BERT\u6216RoBERTa\uff09\u5c06\u8fd9\u4e9b\u63cf\u8ff0\u5d4c\u5165\u5411\u91cf\u7a7a\u95f4\uff0c\u63d0\u53d6\u6709\u610f\u4e49\u7684\u6807\u7b7e\u8bed\u4e49\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u7279\u5f81-\u6807\u7b7e\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u9ad8\u5149\u8c31\u57fa\u51c6\u6570\u636e\u96c6\uff08Hyperspectral Wood\u3001HyperspectralBlueberries\u548cDeepHS-Fruit\uff09\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6587\u672c\u8bed\u4e49\u4e0e\u5149\u8c31\u7a7a\u95f4\u6570\u636e\u4e4b\u95f4\u7684\u534f\u540c\u6548\u5e94\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u8bed\u4e49\u589e\u5f3a\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u8bed\u4e49\u589e\u5f3aHSI\u5206\u7c7b\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u878d\u5408\u5728\u89e3\u51b3\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.07567", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07567", "abs": "https://arxiv.org/abs/2510.07567", "authors": ["Karuna Bhaila", "Aneesh Komanduri", "Minh-Hao Van", "Xintao Wu"], "title": "Cross-Modal Attention Guided Unlearning in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated immense capabilities in\nmulti-modal understanding and inference tasks such as Visual Question Answering\n(VQA), which requires models to infer outputs based on visual and textual\ncontext simultaneously. Such inference abilities of large-scale pretrained\nmodels are often attributed to the massive scale of pre-training data collected\nacross several domains. However, the models may memorize private and/or\nsensitive information during training and regurgitate it in inference.\nRecently, machine unlearning has been leveraged to address the leakage of\nprivate data in LLMs. VLMs add a layer of complexity to this process, as the\nvisual context in the query may also contain sensitive information in addition\nto the text. To address this issue, we explore unlearning for vision-language\nmodels, specifically for the VQA task. We explore the role of visual tokens for\noutput generation in VLMs using cross-modal attention and utilize it to\nformulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and\nefficient VLM unlearning framework. In contrast to computationally expensive\nmodel finetuning methods, CAGUL utilizes external modules to encode unlearning\ninformation in visual tokens of low importance for relevant queries. We find\nthat the transformed visual tokens not only prevent leakage but also retain\nreference model behavior. Experimental results show that our method performs\nbetter or on par with finetuning-based baselines without altering the\npre-trained model parameters or incurring retraining costs, making it a\npractical and effective unlearning solution for VLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CAGUL\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u89c6\u89c9\u6807\u8bb0\u8f6c\u6362\u6765\u9632\u6b62\u654f\u611f\u4fe1\u606f\u6cc4\u9732\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u8003\u6a21\u578b\u884c\u4e3a\uff0c\u65e0\u9700\u4fee\u6539\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u6216\u4ea7\u751f\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u8bb0\u5fc6\u5e76\u6cc4\u9732\u79c1\u6709\u6216\u654f\u611f\u4fe1\u606f\uff0c\u800c\u73b0\u6709\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u8bed\u8a00\u6a21\u578b\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7531\u4e8e\u89c6\u89c9\u4e0a\u4e0b\u6587\u4e5f\u53ef\u80fd\u5305\u542b\u654f\u611f\u4fe1\u606f\u800c\u589e\u52a0\u4e86\u590d\u6742\u6027\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9VQA\u4efb\u52a1\u7684\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63a2\u7d22\u4e86\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u751f\u6210\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5f15\u5bfc\u9057\u5fd8\u6846\u67b6CAGUL\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5916\u90e8\u6a21\u5757\u5728\u76f8\u5173\u67e5\u8be2\u7684\u4f4e\u91cd\u8981\u6027\u89c6\u89c9\u6807\u8bb0\u4e2d\u7f16\u7801\u9057\u5fd8\u4fe1\u606f\uff0c\u907f\u514d\u4e86\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u7684\u4fee\u6539\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCAGUL\u65b9\u6cd5\u5728\u9632\u6b62\u4fe1\u606f\u6cc4\u9732\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u57fa\u4e8e\u5fae\u8c03\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53c2\u8003\u6a21\u578b\u7684\u884c\u4e3a\u7279\u6027\uff0c\u4e14\u4e0d\u4ea7\u751f\u91cd\u65b0\u8bad\u7ec3\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "CAGUL\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u6709\u6548\u7684\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u5916\u90e8\u6a21\u5757\u5b9e\u73b0\u4e86\u654f\u611f\u4fe1\u606f\u4fdd\u62a4\uff0c\u540c\u65f6\u7ef4\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u9690\u79c1\u4fdd\u62a4\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.07545", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07545", "abs": "https://arxiv.org/abs/2510.07545", "authors": ["Md Tahmid Rahman Laskar", "Mohammed Saidul Islam", "Ridwan Mahbub", "Mizanur Rahman", "Amran Bhuiyan", "Israt Jahan", "Mir Tafseer Nayeem", "Shafiq Joty", "Enamul Hoque", "Jimmy Huang"], "title": "Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices", "comment": "Accepted to the EMNLP 2025 Industry Track", "summary": "Large Vision-Language Models (LVLMs) with only 7B parameters have shown\npromise as automated judges in chart comprehension tasks. However, tiny models\n(<=2B parameters) still perform poorly as judges, limiting their real-world use\nin resource-constrained settings. To address this, we propose two approaches to\nensure cost-efficient evaluation: (i) multi-criteria prompting, which combines\nseparate evaluation criteria into a single query, and (ii) domain-adaptive\ntransfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic\njudgments in a chart dataset to create the ChartJudge. Experiments show that\nmulti-criteria prompting exposes robustness gaps, which led to a huge drop in\nperformance for 7B models, including specialized LVLM judges like LLaVA-Critic.\nIn addition, we find that our tiny LVLM (ChartJudge) can effectively transfer\nknowledge from one dataset to another to make it a more specialized model. Our\nfine-grained analysis across chart types and query complexities offers\nactionable insights into trade-offs between model size, prompt design, and\ntransferability, enabling scalable, low-cost evaluation for chart reasoning\ntasks. Our code and the data will be made publicly available.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u6210\u672c\u9ad8\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\uff1a\u591a\u6807\u51c6\u63d0\u793a\u548c\u9886\u57df\u81ea\u9002\u5e94\u8fc1\u79fb\u5b66\u4e60\uff0c\u6210\u529f\u5c062B\u53c2\u6570\u7684\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e3a\u4e13\u95e8\u7684\u56fe\u8868\u7406\u89e3\u8bc4\u4f30\u5668ChartJudge\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u4f4e\u6210\u672c\u8bc4\u4f30\u3002", "motivation": "\u5c3d\u7ba17B\u53c2\u6570\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f5c\u4e3a\u81ea\u52a8\u5316\u8bc4\u4f30\u5668\u7684\u6f5c\u529b\uff0c\u4f462B\u53c2\u6570\u53ca\u4ee5\u4e0b\u7684\u5fae\u578b\u6a21\u578b\u8868\u73b0\u4ecd\u7136\u8f83\u5dee\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u6210\u672c\u9ad8\u6548\u7684\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u6838\u5fc3\u65b9\u6cd5\uff1a\u591a\u6807\u51c6\u63d0\u793a\u5c06\u591a\u4e2a\u8bc4\u4f30\u6807\u51c6\u6574\u5408\u5230\u5355\u4e2a\u67e5\u8be2\u4e2d\uff0c\u4ee5\u53ca\u9886\u57df\u81ea\u9002\u5e94\u8fc1\u79fb\u5b66\u4e60\uff0c\u901a\u8fc7\u5728\u56fe\u8868\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u5408\u6210\u5224\u65ad\u5bf92B\u53c2\u6570\u7684LVLM\u8fdb\u884c\u5fae\u8c03\uff0c\u521b\u5efa\u4e13\u95e8\u7684ChartJudge\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u591a\u6807\u51c6\u63d0\u793a\u66b4\u9732\u4e86\u9c81\u68d2\u6027\u5dee\u8ddd\uff0c\u5bfc\u81f4\u5305\u62ecLLaVA-Critic\u5728\u5185\u76847B\u6a21\u578b\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff1b\u540c\u65f6\uff0cChartJudge\u80fd\u591f\u6709\u6548\u5730\u5c06\u77e5\u8bc6\u4ece\u4e00\u4e2a\u6570\u636e\u96c6\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u6210\u4e3a\u66f4\u4e13\u4e1a\u5316\u7684\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u56fe\u8868\u7c7b\u578b\u548c\u67e5\u8be2\u590d\u6742\u5ea6\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u6a21\u578b\u5927\u5c0f\u3001\u63d0\u793a\u8bbe\u8ba1\u548c\u53ef\u8fc1\u79fb\u6027\u4e4b\u95f4\u6743\u8861\u7684\u53ef\u64cd\u4f5c\u89c1\u89e3\uff0c\u4e3a\u56fe\u8868\u63a8\u7406\u4efb\u52a1\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u4f4e\u6210\u672c\u8bc4\u4f30\u65b9\u6848\u3002"}}
{"id": "2510.07432", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07432", "abs": "https://arxiv.org/abs/2510.07432", "authors": ["Penghang Liu", "Elizabeth Fons", "Svitlana Vyetrenko", "Daniel Borrajo", "Vamsi Potluru", "Manuela Veloso"], "title": "TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering", "comment": "NeurIPS 2025 Workshop on Foundations of Reasoning in Language Models", "summary": "Large language models (LLMs) have shown strong abilities in reasoning and\nproblem solving, but recent studies reveal that they still struggle with time\nseries reasoning tasks, where outputs are often affected by hallucination or\nknowledge leakage. In this work we propose TS-Agent, a time series reasoning\nagent that leverages LLMs strictly for what they excel at, i.e., gathering\nevidence and synthesizing it into conclusions through step-by-step reasoning,\nwhile delegating the extraction of statistical and structural information to\ntime series analytical tools. Instead of mapping time series into text tokens,\nimages, or embeddings, our agent interacts with raw numeric sequences through\natomic operators, records outputs in an explicit evidence log, and iteratively\nrefines its reasoning under the guidance of a self-critic and a final quality\ngate. This design avoids multi-modal alignment training, preserves the native\nform of time series, ensures interpretability and verifiability, and mitigates\nknowledge leakage or hallucination. Empirically, we evaluate the agent on\nestablished benchmarks. Our experiments show that TS-Agent achieves performance\ncomparable to state-of-the-art LLMs on understanding benchmarks, and delivers\nsignificant improvements on reasoning tasks, where existing models often rely\non memorization and fail in zero-shot settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TS-Agent\uff0c\u4e00\u79cd\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5c06LLMs\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u5de5\u5177\u76f8\u7ed3\u5408\uff0c\u4e13\u95e8\u89e3\u51b3LLMs\u5728\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u548c\u77e5\u8bc6\u6cc4\u9732\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u6700\u8fd1\u7814\u7a76\u53d1\u73b0\u5b83\u4eec\u5728\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u7136\u5b58\u5728\u56f0\u96be\uff0c\u8f93\u51fa\u7ecf\u5e38\u53d7\u5230\u5e7b\u89c9\u6216\u77e5\u8bc6\u6cc4\u9732\u7684\u5f71\u54cd\uff0c\u8fd9\u9650\u5236\u4e86LLMs\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "TS-Agent\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4e25\u683c\u5229\u7528LLMs\u64c5\u957f\u8bc1\u636e\u6536\u96c6\u548c\u7ed3\u8bba\u5408\u6210\u7684\u80fd\u529b\uff0c\u540c\u65f6\u5c06\u7edf\u8ba1\u548c\u7ed3\u6784\u4fe1\u606f\u63d0\u53d6\u59d4\u6258\u7ed9\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u5de5\u5177\uff1b\u667a\u80fd\u4f53\u901a\u8fc7\u539f\u5b50\u64cd\u4f5c\u7b26\u4e0e\u539f\u59cb\u6570\u503c\u5e8f\u5217\u4ea4\u4e92\uff0c\u5728\u663e\u5f0f\u8bc1\u636e\u65e5\u5fd7\u4e2d\u8bb0\u5f55\u8f93\u51fa\uff0c\u5e76\u5728\u81ea\u6211\u6279\u8bc4\u548c\u6700\u7ec8\u8d28\u91cf\u95e8\u7684\u6307\u5bfc\u4e0b\u8fed\u4ee3\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTS-Agent\u5728\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdbLLMs\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u73b0\u6709\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u8bb0\u5fc6\u800c\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u5c06LLMs\u4e25\u683c\u9650\u5b9a\u5728\u5176\u4f18\u52bf\u9886\u57df\uff0c\u540c\u65f6\u5c06\u4e13\u4e1a\u4efb\u52a1\u59d4\u6258\u7ed9\u4e13\u7528\u5de5\u5177\uff0c\u53ef\u4ee5\u6709\u6548\u907f\u514d\u591a\u6a21\u6001\u5bf9\u9f50\u8bad\u7ec3\uff0c\u4fdd\u6301\u65f6\u95f4\u5e8f\u5217\u7684\u539f\u59cb\u5f62\u5f0f\uff0c\u786e\u4fdd\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u5e76\u4e3a\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6846\u67b6\u3002"}}
{"id": "2510.07631", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07631", "abs": "https://arxiv.org/abs/2510.07631", "authors": ["Shreshth Saini", "Shashank Gupta", "Alan C. Bovik"], "title": "Rectified-CFG++ for Flow Based Models", "comment": "Accepted at NeurIPS 2025", "summary": "Classifier-free guidance (CFG) is the workhorse for steering large diffusion\nmodels toward text-conditioned targets, yet its native application to rectified\nflow (RF) based models provokes severe off-manifold drift, yielding visual\nartifacts, text misalignment, and brittle behaviour. We present\nRectified-CFG++, an adaptive predictor-corrector guidance that couples the\ndeterministic efficiency of rectified flows with a geometry-aware conditioning\nrule. Each inference step first executes a conditional RF update that anchors\nthe sample near the learned transport path, then applies a weighted conditional\ncorrection that interpolates between conditional and unconditional velocity\nfields. We prove that the resulting velocity field is marginally consistent and\nthat its trajectories remain within a bounded tubular neighbourhood of the data\nmanifold, ensuring stability across a wide range of guidance strengths.\nExtensive experiments on large-scale text-to-image models (Flux, Stable\nDiffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms\nstandard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and\nT2I-CompBench. Project page: https://rectified-cfgpp.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Rectified-CFG++\uff0c\u4e00\u79cd\u7528\u4e8e\u6574\u6d41\u6d41\u6a21\u578b\u7684\u9002\u5e94\u6027\u9884\u6d4b-\u6821\u6b63\u5f15\u5bfc\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6807\u51c6\u5206\u7c7b\u5668\u65e0\u5173\u5f15\u5bfc\u5728\u6574\u6d41\u6d41\u6a21\u578b\u4e2d\u5f15\u8d77\u7684\u79bb\u6d41\u5f62\u6f02\u79fb\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u5bf9\u9f50\u7684\u751f\u6210\u6548\u679c\u3002", "motivation": "\u6807\u51c6\u5206\u7c7b\u5668\u65e0\u5173\u5f15\u5bfc\u5728\u6574\u6d41\u6d41\u6a21\u578b\u4e2d\u7684\u76f4\u63a5\u5e94\u7528\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u79bb\u6d41\u5f62\u6f02\u79fb\uff0c\u4ea7\u751f\u89c6\u89c9\u4f2a\u5f71\u3001\u6587\u672c\u5bf9\u9f50\u5931\u8d25\u548c\u8106\u5f31\u884c\u4e3a\uff0c\u8fd9\u9650\u5236\u4e86\u6574\u6d41\u6d41\u6a21\u578b\u5728\u6587\u672c\u6761\u4ef6\u751f\u6210\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u9884\u6d4b-\u6821\u6b63\u5f15\u5bfc\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u9996\u5148\u6267\u884c\u6761\u4ef6\u6574\u6d41\u6d41\u66f4\u65b0\u5c06\u6837\u672c\u951a\u5b9a\u5728\u5b66\u4e60\u7684\u4f20\u8f93\u8def\u5f84\u9644\u8fd1\uff0c\u7136\u540e\u5e94\u7528\u52a0\u6743\u6761\u4ef6\u6821\u6b63\uff0c\u5728\u6761\u4ef6\u548c\u65e0\u6761\u4ef6\u901f\u5ea6\u573a\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\uff0c\u786e\u4fdd\u901f\u5ea6\u573a\u7684\u8fb9\u9645\u4e00\u81f4\u6027\u3002", "result": "\u5728Flux\u3001Stable Diffusion 3/3.5\u3001Lumina\u7b49\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRectified-CFG++\u5728MS-COCO\u3001LAION-Aesthetic\u548cT2I-CompBench\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u6807\u51c6\u5206\u7c7b\u5668\u65e0\u5173\u5f15\u5bfc\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u6574\u6d41\u6d41\u8f68\u8ff9\u80fd\u591f\u4fdd\u6301\u5728\u6570\u636e\u6d41\u5f62\u7684\u6709\u754c\u7ba1\u72b6\u90bb\u57df\u5185\uff0c\u786e\u4fdd\u4e86\u5728\u5404\u79cd\u5f15\u5bfc\u5f3a\u5ea6\u4e0b\u7684\u7a33\u5b9a\u6027\uff0c\u4e3a\u6574\u6d41\u6d41\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5f15\u5bfc\u673a\u5236\u3002"}}
{"id": "2510.07737", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07737", "abs": "https://arxiv.org/abs/2510.07737", "authors": ["Fu Chen", "Peng Wang", "Xiyin Li", "Wen Li", "Shichi Lei", "Dongdong Xiang"], "title": "ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs", "comment": null, "summary": "Training Large Language Models (LLMs) with Group Relative Policy Optimization\n(GRPO) encounters a significant challenge: models often fail to produce\naccurate responses, particularly in small-scale architectures. This limitation\nnot only diminishes performance improvements and undermines the potential of\nGRPO but also frequently leads to mid-training collapse, adversely affecting\nstability and final efficacy. To address these issues, we propose ToolExpander,\na novel framework that advances tool-oriented reinforcement learning for\nresource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round\nHard Sampling, which dynamically substitutes challenging samples(those without\ncorrect outputs over 10 rollouts) with high-quality few-shot demonstrations\nduring training, coupled with an exponential learning rate decay strategy to\nmitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO\nframework that eliminates KL divergence and incorporates adjusted clipping\ncoefficients, encouraging models to autonomously generate and analyze few-shot\nexamples via a minimal additional reward (0.01).Experimental results\ndemonstrate that ToolExpander significantly enhances tool-using capabilities in\nLLMs, especially in weaker small-scale models, improving both training\nstability and overall performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ToolExpander\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u591a\u8f6e\u786c\u91c7\u6837\u548c\u81ea\u793a\u4f8b\u601d\u7ef4\u4e24\u9879\u521b\u65b0\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650LLM\u5728GRPO\u8bad\u7ec3\u4e2d\u54cd\u5e94\u4e0d\u51c6\u786e\u548c\u8bad\u7ec3\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u89c4\u6a21\u6a21\u578b\u4f7f\u7528\u5de5\u5177\u7684\u80fd\u529b\u3002", "motivation": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f7f\u7528GRPO\u65b9\u6cd5\u65f6\u9762\u4e34\u663e\u8457\u6311\u6218\uff1a\u6a21\u578b\u7ecf\u5e38\u65e0\u6cd5\u4ea7\u751f\u51c6\u786e\u54cd\u5e94\uff0c\u7279\u522b\u662f\u5728\u5c0f\u89c4\u6a21\u67b6\u6784\u4e2d\uff0c\u8fd9\u4e0d\u4ec5\u964d\u4f4e\u4e86\u6027\u80fd\u6539\u8fdb\u5e76\u524a\u5f31\u4e86GRPO\u7684\u6f5c\u529b\uff0c\u8fd8\u7ecf\u5e38\u5bfc\u81f4\u8bad\u7ec3\u4e2d\u671f\u5d29\u6e83\uff0c\u4e25\u91cd\u5f71\u54cd\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6548\u679c\u3002", "method": "ToolExpander\u6846\u67b6\u5305\u542b\u4e24\u9879\u5173\u952e\u6280\u672f\uff1a\u52a8\u6001\u591a\u8f6e\u786c\u91c7\u6837\u5728\u8bad\u7ec3\u671f\u95f4\u52a8\u6001\u66ff\u6362\u56f0\u96be\u6837\u672c\uff0810\u6b21rollout\u65e0\u6b63\u786e\u8f93\u51fa\u7684\u6837\u672c\uff09\u4e3a\u9ad8\u8d28\u91cf\u5c11\u6837\u672c\u6f14\u793a\uff0c\u5e76\u91c7\u7528\u6307\u6570\u5b66\u4e60\u7387\u8870\u51cf\u7b56\u7565\u7f13\u89e3\u9707\u8361\uff1b\u81ea\u793a\u4f8b\u601d\u7ef4\u662f\u589e\u5f3a\u7684GRPO\u6846\u67b6\uff0c\u6d88\u9664\u4e86KL\u6563\u5ea6\u5e76\u6574\u5408\u8c03\u6574\u540e\u7684\u88c1\u526a\u7cfb\u6570\uff0c\u901a\u8fc7\u6700\u5c0f\u989d\u5916\u5956\u52b1\uff080.01\uff09\u9f13\u52b1\u6a21\u578b\u81ea\u4e3b\u751f\u6210\u548c\u5206\u6790\u5c11\u6837\u672c\u793a\u4f8b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cToolExpander\u663e\u8457\u589e\u5f3a\u4e86LLM\u4f7f\u7528\u5de5\u5177\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8f83\u5f31\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u4e2d\uff0c\u540c\u65f6\u6539\u5584\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u52a8\u6001\u6837\u672c\u66ff\u6362\u548c\u81ea\u4e3b\u793a\u4f8b\u751f\u6210\u673a\u5236\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u8d44\u6e90\u53d7\u9650LLM\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u5c0f\u89c4\u6a21\u6a21\u578b\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.07489", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07489", "abs": "https://arxiv.org/abs/2510.07489", "authors": ["Akhil Kumar", "Jianliang Leon Zhao", "Om Dobariya"], "title": "Evaluation of LLMs for Process Model Analysis and Optimization", "comment": "15 pages, 5 tables, 4 figures; full research paper currently under\n  review for the Workshop on Information Technologies and Systems (WITS) 2025.\n  The paper presents a comprehensive evaluation of large language models (LLMs)\n  for business process model analysis and optimization, including error\n  detection, reasoning, and scenario-based redesign", "summary": "In this paper, we report our experience with several LLMs for their ability\nto understand a process model in an interactive, conversational style, find\nsyntactical and logical errors in it, and reason with it in depth through a\nnatural language (NL) interface. Our findings show that a vanilla, untrained\nLLM like ChatGPT (model o3) in a zero-shot setting is effective in\nunderstanding BPMN process models from images and answering queries about them\nintelligently at syntactic, logic, and semantic levels of depth. Further,\ndifferent LLMs vary in performance in terms of their accuracy and\neffectiveness. Nevertheless, our empirical analysis shows that LLMs can play a\nvaluable role as assistants for business process designers and users. We also\nstudy the LLM's \"thought process\" and ability to perform deeper reasoning in\nthe context of process analysis and optimization. We find that the LLMs seem to\nexhibit anthropomorphic properties.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7406\u89e3BPMN\u4e1a\u52a1\u6d41\u7a0b\u6a21\u578b\u3001\u68c0\u6d4b\u8bed\u6cd5\u903b\u8f91\u9519\u8bef\u4ee5\u53ca\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u672a\u7ecf\u4e13\u95e8\u8bad\u7ec3\u7684LLM\u80fd\u591f\u6709\u6548\u5206\u6790\u6d41\u7a0b\u6a21\u578b\u5e76\u5728\u8bed\u6cd5\u3001\u903b\u8f91\u548c\u8bed\u4e49\u5c42\u9762\u63d0\u4f9b\u667a\u80fd\u56de\u7b54\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u65b9\u5f0f\u7406\u89e3\u4e1a\u52a1\u6d41\u7a0b\u6a21\u578b\uff0c\u68c0\u6d4b\u5176\u4e2d\u7684\u8bed\u6cd5\u548c\u903b\u8f91\u9519\u8bef\uff0c\u5e76\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\uff0c\u586b\u8865\u4e86LLM\u5728\u4e1a\u52a1\u6d41\u7a0b\u5206\u6790\u9886\u57df\u5e94\u7528\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u91c7\u7528\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u591a\u79cdLLM\uff08\u5305\u62ecChatGPT o3\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u5bf9BPMN\u6d41\u7a0b\u6a21\u578b\u56fe\u50cf\u8fdb\u884c\u7406\u89e3\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u8bed\u6cd5\u3001\u903b\u8f91\u548c\u8bed\u4e49\u5c42\u9762\u7684\u5206\u6790\u80fd\u529b\uff0c\u5e76\u7814\u7a76\u5176\"\u601d\u7ef4\u8fc7\u7a0b\"\u548c\u6df1\u5ea6\u63a8\u7406\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672a\u7ecf\u4e13\u95e8\u8bad\u7ec3\u7684LLM\u80fd\u591f\u6709\u6548\u7406\u89e3BPMN\u6d41\u7a0b\u6a21\u578b\u56fe\u50cf\uff0c\u5728\u8bed\u6cd5\u3001\u903b\u8f91\u548c\u8bed\u4e49\u5c42\u9762\u63d0\u4f9b\u51c6\u786e\u56de\u7b54\uff0c\u4e0d\u540cLLM\u5728\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u5b58\u5728\u6027\u80fd\u5dee\u5f02\uff0c\u4f46\u603b\u4f53\u4e0a\u5c55\u73b0\u51fa\u4f5c\u4e3a\u4e1a\u52a1\u6d41\u7a0b\u8bbe\u8ba1\u52a9\u624b\u7684\u91cd\u8981\u4ef7\u503c\u3002", "conclusion": "LLM\u5728\u4e1a\u52a1\u6d41\u7a0b\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u7c7b\u4eba\u5316\u7684\u63a8\u7406\u7279\u6027\uff0c\u80fd\u591f\u4f5c\u4e3a\u4e1a\u52a1\u6d41\u7a0b\u8bbe\u8ba1\u8005\u548c\u7528\u6237\u7684\u6709\u6548\u52a9\u624b\uff0c\u4e3a\u6d41\u7a0b\u5206\u6790\u548c\u4f18\u5316\u63d0\u4f9b\u6df1\u5ea6\u63a8\u7406\u652f\u6301\uff0c\u63ed\u793a\u4e86LLM\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.07636", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07636", "abs": "https://arxiv.org/abs/2510.07636", "authors": ["Shashank Gupta", "Gregoire Phillips", "Alan C. Bovik"], "title": "PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment", "comment": "Oral presentation at ICIP 2025", "summary": "Large Multimodal Models (LMMs) have recently enabled considerable advances in\nthe realm of image and video quality assessment, but this progress has yet to\nbe fully explored in the domain of 3D assets. We are interested in using these\nmodels to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where\nthe aim is to automatically evaluate the perceptual quality of a point cloud in\nabsence of a reference. We begin with the observation that different modalities\nof data - text descriptions, 2D projections, and 3D point cloud views - provide\ncomplementary information about point cloud quality. We then construct PIT-QMM,\na novel LMM for NR-PCQA that is capable of consuming text, images and point\nclouds end-to-end to predict quality scores. Extensive experimentation shows\nthat our proposed method outperforms the state-of-the-art by significant\nmargins on popular benchmarks with fewer training iterations. We also\ndemonstrate that our framework enables distortion localization and\nidentification, which paves a new way forward for model explainability and\ninteractivity. Code and datasets are available at\nhttps://www.github.com/shngt/pit-qmm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PIT-QMM\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7528\u4e8e\u65e0\u53c2\u8003\u70b9\u4e91\u8d28\u91cf\u8bc4\u4f30\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u548c\u70b9\u4e91\u6570\u636e\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u57283D\u8d44\u4ea7\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u65e0\u53c2\u8003\u70b9\u4e91\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86PIT-QMM\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u7aef\u5230\u7aef\u5730\u5904\u7406\u6587\u672c\u63cf\u8ff0\u30012D\u6295\u5f71\u548c3D\u70b9\u4e91\u89c6\u56fe\u7b49\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u5229\u7528\u4e0d\u540c\u6a21\u6001\u63d0\u4f9b\u7684\u4e92\u8865\u4fe1\u606f\u6765\u9884\u6d4b\u70b9\u4e91\u8d28\u91cf\u5206\u6570\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4ee5\u663e\u8457\u4f18\u52bf\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\uff0c\u4e14\u9700\u8981\u66f4\u5c11\u7684\u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570\uff0c\u540c\u65f6\u6846\u67b6\u8fd8\u652f\u6301\u5931\u771f\u5b9a\u4f4d\u548c\u8bc6\u522b\u529f\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u70b9\u4e91\u8d28\u91cf\u8bc4\u4f30\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4ea4\u4e92\u6027\uff0c\u4e3a3D\u8d44\u4ea7\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07793", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07793", "abs": "https://arxiv.org/abs/2510.07793", "authors": ["Sajib Acharjee Dip", "Adrika Zafor", "Bikash Kumar Paul", "Uddip Acharjee Shuvo", "Muhit Islam Emon", "Xuan Wang", "Liqing Zhang"], "title": "LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology", "comment": "34 pages, 5 figures, 7 tables", "summary": "Large language models (LLMs) and emerging agentic frameworks are beginning to\ntransform single-cell biology by enabling natural-language reasoning,\ngenerative annotation, and multimodal data integration. However, progress\nremains fragmented across data modalities, architectures, and evaluation\nstandards. LLM4Cell presents the first unified survey of 58 foundation and\nagentic models developed for single-cell research, spanning RNA, ATAC,\nmulti-omic, and spatial modalities. We categorize these methods into five\nfamilies-foundation, text-bridge, spatial, multimodal, epigenomic, and\nagentic-and map them to eight key analytical tasks including annotation,\ntrajectory and perturbation modeling, and drug-response prediction. Drawing on\nover 40 public datasets, we analyze benchmark suitability, data diversity, and\nethical or scalability constraints, and evaluate models across 10 domain\ndimensions covering biological grounding, multi-omics alignment, fairness,\nprivacy, and explainability. By linking datasets, models, and evaluation\ndomains, LLM4Cell provides the first integrated view of language-driven\nsingle-cell intelligence and outlines open challenges in interpretability,\nstandardization, and trustworthy model development.", "AI": {"tldr": "LLM4Cell\u63d0\u51fa\u4e86\u9996\u4e2a\u7edf\u4e00\u7684\u5927\u8bed\u8a00\u6a21\u578b\u548c\u667a\u80fd\u4f53\u6846\u67b6\u5728\u5355\u7ec6\u80de\u751f\u7269\u5b66\u9886\u57df\u7684\u7efc\u8ff0\uff0c\u7cfb\u7edf\u5206\u7c7b\u4e8658\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u6db5\u76d6\u4e86RNA\u3001ATAC\u3001\u591a\u7ec4\u5b66\u548c\u7a7a\u95f4\u6a21\u6001\uff0c\u4e3a\u8bed\u8a00\u9a71\u52a8\u7684\u5355\u7ec6\u80de\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u7efc\u5408\u89c6\u56fe\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u548c\u65b0\u5174\u667a\u80fd\u4f53\u6846\u67b6\u6b63\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u3001\u751f\u6210\u5f0f\u6ce8\u91ca\u548c\u591a\u6a21\u6001\u6570\u636e\u96c6\u6210\u6539\u53d8\u5355\u7ec6\u80de\u751f\u7269\u5b66\u7814\u7a76\uff0c\u4f46\u8fdb\u5c55\u5728\u6570\u636e\u6a21\u6001\u3001\u67b6\u6784\u548c\u8bc4\u4f30\u6807\u51c6\u65b9\u9762\u4ecd\u7136\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5206\u7c7b\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u8be5\u7814\u7a76\u7cfb\u7edf\u7efc\u8ff0\u4e8658\u4e2a\u4e3a\u5355\u7ec6\u80de\u7814\u7a76\u5f00\u53d1\u7684\u57fa\u7840\u6a21\u578b\u548c\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u5c06\u5176\u5206\u4e3a\u57fa\u7840\u6a21\u578b\u3001\u6587\u672c\u6865\u63a5\u3001\u7a7a\u95f4\u6a21\u578b\u3001\u591a\u6a21\u6001\u3001\u8868\u89c2\u57fa\u56e0\u7ec4\u548c\u667a\u80fd\u4f53\u516d\u4e2a\u5bb6\u65cf\uff0c\u5e76\u6620\u5c04\u5230\u516b\u4e2a\u5173\u952e\u5206\u6790\u4efb\u52a1\uff0c\u5305\u62ec\u7ec6\u80de\u6ce8\u91ca\u3001\u8f68\u8ff9\u5efa\u6a21\u3001\u6270\u52a8\u5efa\u6a21\u548c\u836f\u7269\u53cd\u5e94\u9884\u6d4b\u7b49\u3002", "result": "\u57fa\u4e8e40\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u7684\u5206\u6790\u663e\u793a\uff0c\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u6a21\u578b\u572810\u4e2a\u9886\u57df\u7ef4\u5ea6\u7684\u8868\u73b0\uff0c\u5305\u62ec\u751f\u7269\u5b66\u57fa\u7840\u6027\u3001\u591a\u7ec4\u5b66\u5bf9\u9f50\u3001\u516c\u5e73\u6027\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u5206\u6790\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7684\u9002\u7528\u6027\u3001\u6570\u636e\u591a\u6837\u6027\u4ee5\u53ca\u4f26\u7406\u548c\u53ef\u6269\u5c55\u6027\u7ea6\u675f\u3002", "conclusion": "LLM4Cell\u901a\u8fc7\u8fde\u63a5\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u9886\u57df\uff0c\u4e3a\u8bed\u8a00\u9a71\u52a8\u7684\u5355\u7ec6\u80de\u667a\u80fd\u63d0\u4f9b\u4e86\u9996\u4e2a\u96c6\u6210\u89c6\u56fe\uff0c\u5e76\u6307\u51fa\u4e86\u5728\u53ef\u89e3\u91ca\u6027\u3001\u6807\u51c6\u5316\u548c\u53ef\u4fe1\u6a21\u578b\u5f00\u53d1\u65b9\u9762\u7684\u5f00\u653e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\u548c\u65b9\u5411\u6307\u5f15\u3002"}}
{"id": "2510.07551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07551", "abs": "https://arxiv.org/abs/2510.07551", "authors": ["Harshit Rajgarhia", "Suryam Gupta", "Asif Shaik", "Gulipalli Praveen Kumar", "Y Santhoshraj", "Sanka Nithya Tanvy Nishitha", "Abhishek Mukherji"], "title": "An Evaluation Study of Hybrid Methods for Multilingual PII Detection", "comment": null, "summary": "The detection of Personally Identifiable Information (PII) is critical for\nprivacy compliance but remains challenging in low-resource languages due to\nlinguistic diversity and limited annotated data. We present RECAP, a hybrid\nframework that combines deterministic regular expressions with context-aware\nlarge language models (LLMs) for scalable PII detection across 13 low-resource\nlocales. RECAP's modular design supports over 300 entity types without\nretraining, using a three-phase refinement pipeline for disambiguation and\nfiltering. Benchmarked with nervaluate, our system outperforms fine-tuned NER\nmodels by 82% and zero-shot LLMs by 17% in weighted F1-score. This work offers\na scalable and adaptable solution for efficient PII detection in\ncompliance-focused applications.", "AI": {"tldr": "RECAP\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u786e\u5b9a\u6027\u6b63\u5219\u8868\u8fbe\u5f0f\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u572813\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u7684PII\u68c0\u6d4b\u3002\u8be5\u7cfb\u7edf\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u652f\u6301300\u591a\u79cd\u5b9e\u4f53\u7c7b\u578b\uff0c\u5728\u52a0\u6743F1\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u68c0\u6d4b\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\u9762\u4e34\u8bed\u8a00\u591a\u6837\u6027\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u6311\u6218\uff0c\u8fd9\u7ed9\u9690\u79c1\u5408\u89c4\u5e26\u6765\u4e86\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u591a\u79cd\u8bed\u8a00\u73af\u5883\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "RECAP\u91c7\u7528\u6df7\u5408\u6846\u67b6\u8bbe\u8ba1\uff0c\u5c06\u786e\u5b9a\u6027\u6b63\u5219\u8868\u8fbe\u5f0f\u4e0e\u4e0a\u4e0b\u6587\u611f\u77e5LLMs\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u652f\u6301\u8d85\u8fc7300\u79cd\u5b9e\u4f53\u7c7b\u578b\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u4e09\u9636\u6bb5\u7cbe\u70bc\u6d41\u7a0b\u8fdb\u884c\u6d88\u6b67\u548c\u8fc7\u6ee4\u5904\u7406\u3002", "result": "\u4f7f\u7528nervaluate\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u52a0\u6743F1\u5206\u6570\u4e0a\u6bd4\u5fae\u8c03NER\u6a21\u578b\u9ad8\u51fa82%\uff0c\u6bd4\u96f6\u6837\u672cLLMs\u9ad8\u51fa17%\uff0c\u572813\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u5408\u89c4\u5bfc\u5411\u5e94\u7528\u4e2d\u7684PII\u68c0\u6d4b\u9700\u6c42\uff0c\u4e3a\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2510.07703", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07703", "abs": "https://arxiv.org/abs/2510.07703", "authors": ["Xiaoxu Ma", "Runhao Li", "Zhenyu Weng"], "title": "Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision", "comment": null, "summary": "Deep hashing has been widely adopted for large-scale image retrieval, with\nnumerous strategies proposed to optimize hash function learning. Pairwise-based\nmethods are effective in learning hash functions that preserve local similarity\nrelationships, whereas center-based methods typically achieve superior\nperformance by more effectively capturing global data distributions. However,\nthe strength of center-based methods in modeling global structures often comes\nat the expense of underutilizing important local similarity information. To\naddress this limitation, we propose Mutual Learning for Hashing (MLH), a novel\nweak-to-strong framework that enhances a center-based hashing branch by\ntransferring knowledge from a weaker pairwise-based branch. MLH consists of two\nbranches: a strong center-based branch and a weaker pairwise-based branch.\nThrough an iterative mutual learning process, the center-based branch leverages\nlocal similarity cues learned by the pairwise-based branch. Furthermore,\ninspired by the mixture-of-experts paradigm, we introduce a novel\nmixture-of-hash-experts module that enables effective cross-branch interaction,\nfurther enhancing the performance of both branches. Extensive experiments\ndemonstrate that MLH consistently outperforms state-of-the-art hashing methods\nacross multiple benchmark datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MLH\uff08Mutual Learning for Hashing\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u5f31\u5230\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u57fa\u4e8e\u6210\u5bf9\u76f8\u4f3c\u6027\u7684\u5f31\u5206\u652f\u5411\u57fa\u4e8e\u4e2d\u5fc3\u7684\u5f3a\u5206\u652f\u8f6c\u79fb\u77e5\u8bc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e2d\u5fc3\u54c8\u5e0c\u65b9\u6cd5\u5728\u5efa\u6a21\u5168\u5c40\u7ed3\u6784\u65f6\u5ffd\u7565\u5c40\u90e8\u76f8\u4f3c\u6027\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u54c8\u5e0c\u65b9\u6cd5\u4e2d\uff0c\u57fa\u4e8e\u6210\u5bf9\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4fdd\u6301\u5c40\u90e8\u76f8\u4f3c\u5173\u7cfb\uff0c\u800c\u57fa\u4e8e\u4e2d\u5fc3\u7684\u65b9\u6cd5\u5728\u6355\u83b7\u5168\u5c40\u6570\u636e\u5206\u5e03\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u540e\u8005\u5728\u5efa\u6a21\u5168\u5c40\u7ed3\u6784\u65f6\u5f80\u5f80\u672a\u80fd\u5145\u5206\u5229\u7528\u91cd\u8981\u7684\u5c40\u90e8\u76f8\u4f3c\u6027\u4fe1\u606f\uff0c\u8fd9\u4e00\u5c40\u9650\u6027\u9650\u5236\u4e86\u54c8\u5e0c\u68c0\u7d22\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "MLH\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5206\u652f\uff1a\u4e00\u4e2a\u57fa\u4e8e\u4e2d\u5fc3\u7684\u5f3a\u5206\u652f\u548c\u4e00\u4e2a\u57fa\u4e8e\u6210\u5bf9\u76f8\u4f3c\u6027\u7684\u5f31\u5206\u652f\uff0c\u901a\u8fc7\u8fed\u4ee3\u5f0f\u76f8\u4e92\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5f3a\u5206\u652f\u80fd\u591f\u5229\u7528\u5f31\u5206\u652f\u5b66\u5230\u7684\u5c40\u90e8\u76f8\u4f3c\u6027\u7ebf\u7d22\uff1b\u6b64\u5916\uff0c\u53d7\u6df7\u5408\u4e13\u5bb6\u8303\u5f0f\u542f\u53d1\uff0c\u5f15\u5165\u4e86\u6df7\u5408\u54c8\u5e0c\u4e13\u5bb6\u6a21\u5757\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u8de8\u5206\u652f\u4ea4\u4e92\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e24\u4e2a\u5206\u652f\u7684\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMLH\u65b9\u6cd5\u5728\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u4e00\u81f4\u6027\u5730\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u54c8\u5e0c\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u5728\u7ed3\u5408\u5168\u5c40\u7ed3\u6784\u548c\u5c40\u90e8\u76f8\u4f3c\u6027\u4fe1\u606f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u5f31\u5230\u5f3a\u7684\u76f8\u4e92\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u6574\u5408\u4e0d\u540c\u54c8\u5e0c\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u6df7\u5408\u54c8\u5e0c\u4e13\u5bb6\u6a21\u5757\u7684\u5f15\u5165\u4e3a\u8de8\u5206\u652f\u77e5\u8bc6\u8f6c\u79fb\u63d0\u4f9b\u4e86\u6709\u6548\u673a\u5236\uff0c\u4e3a\u672a\u6765\u54c8\u5e0c\u65b9\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u5373\u901a\u8fc7\u591a\u5206\u652f\u534f\u4f5c\u6765\u5e73\u8861\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u7684\u5229\u7528\u3002"}}
{"id": "2510.07881", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07881", "abs": "https://arxiv.org/abs/2510.07881", "authors": ["Heyang Liu", "Yuhao Wang", "Ziyang Cheng", "Ronghua Wu", "Qunshan Gu", "Yanfeng Wang", "Yu Wang"], "title": "CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching", "comment": null, "summary": "The advancement of multimodal large language models has accelerated the\ndevelopment of speech-to-speech interaction systems. While natural monolingual\ninteraction has been achieved, we find existing models exhibit deficiencies in\nlanguage alignment. In our proposed Code-Switching Speech-to-Speech Benchmark\n(CS3-Bench), experiments on 7 mainstream models demonstrate a relative\nperformance drop of up to 66% in knowledge-intensive question answering and\nvarying degrees of misunderstanding in open-ended conversations. Starting from\na model with severe performance deterioration, we propose both data\nconstructions and training approaches to improve the language alignment\ncapabilities, specifically employing Chain of Recognition (CoR) to enhance\nunderstanding and Keyword Highlighting (KH) to guide generation. Our approach\nimproves the knowledge accuracy from 25.14% to 46.13%, with open-ended\nunderstanding rate from 64.5% to 86.5%, and significantly reduces pronunciation\nerrors in the secondary language. CS3-Bench is available at\nhttps://huggingface.co/datasets/VocalNet/CS3-Bench.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u5230\u8bed\u97f3\u57fa\u51c6\uff08CS3-Bench\uff09\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u5bf9\u9f50\u65b9\u9762\u7684\u4e25\u91cd\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u94fe\u5f0f\u8bc6\u522b\u548c\u5173\u952e\u8bcd\u9ad8\u4eae\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8de8\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u5230\u8bed\u97f3\u4ea4\u4e92\u7cfb\u7edf\u4e2d\u867d\u7136\u5b9e\u73b0\u4e86\u81ea\u7136\u5355\u8bed\u4ea4\u4e92\uff0c\u4f46\u5728\u8bed\u8a00\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7801\u5207\u6362\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\u548c\u8bef\u89e3\u95ee\u9898\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86CS3-Bench\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u6539\u8fdb\u65b9\u6cd5\uff1a\u4f7f\u7528\u94fe\u5f0f\u8bc6\u522b\uff08CoR\uff09\u589e\u5f3a\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u53ca\u91c7\u7528\u5173\u952e\u8bcd\u9ad8\u4eae\uff08KH\uff09\u6280\u672f\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u57287\u4e2a\u4e3b\u6d41\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u7b54\u6027\u80fd\u76f8\u5bf9\u4e0b\u964d\u9ad8\u8fbe66%\uff0c\u800c\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06\u77e5\u8bc6\u51c6\u786e\u7387\u4ece25.14%\u63d0\u5347\u81f346.13%\uff0c\u5f00\u653e\u7406\u89e3\u7387\u4ece64.5%\u63d0\u5347\u81f386.5%\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u7b2c\u4e8c\u8bed\u8a00\u7684\u53d1\u97f3\u9519\u8bef\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8bed\u8a00\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u6570\u636e\u6784\u5efa\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e3a\u6539\u5584\u8de8\u8bed\u8a00\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u591a\u8bed\u8a00\u8bed\u97f3\u4ea4\u4e92\u7814\u7a76\u5efa\u7acb\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2510.07632", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07632", "abs": "https://arxiv.org/abs/2510.07632", "authors": ["Yinglun Zhu", "Jiancheng Zhang", "Fuzhi Tang"], "title": "Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models", "comment": null, "summary": "Frontier AI models have achieved remarkable progress, yet recent studies\nsuggest they struggle with compositional reasoning, often performing at or\nbelow random chance on established benchmarks. We revisit this problem and show\nthat widely used evaluation metrics systematically underestimate model\ncapability. To address this, we introduce a group matching score that better\nexploits group structure and reveals substantial hidden capability in both\ncontrastive vision-language models (VLMs) and multimodal large language models\n(MLLMs). Moreover, simply overfitting to the induced group matchings at test\ntime transfers this hidden capability into higher scores under standard\nevaluation metrics, closing much of the reported gap. This adjustment enables\nSigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first\nresult surpassing estimated human performance on Winoground.\n  Building on this insight, we propose Test-Time Matching (TTM), an iterative,\nself-improving algorithm that further bootstraps model performance without any\nexternal supervision. TTM delivers additional, non-trivial improvements: for\nexample, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a\nnew state of the art. Importantly, TTM remains broadly effective even on\nbenchmarks without metric-induced effects or group structures, achieving\nrelative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16\ndataset variants spanning diverse setups, our experiments demonstrate that TTM\nconsistently improves model performance and advances the frontier of\ncompositional reasoning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u7cfb\u7edf\u6027\u4f4e\u4f30\u4e86\u524d\u6cbfAI\u6a21\u578b\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u7ec4\u5339\u914d\u5206\u6570\u548c\u6d4b\u8bd5\u65f6\u5339\u914d\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7ec4\u5408\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u4eba\u7c7b\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u524d\u6cbfAI\u6a21\u578b\u5728\u7ec4\u5408\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u7cfb\u7edf\u6027\u4f4e\u4f30\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u88ab\u4e25\u91cd\u4f4e\u4f30\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6027\u80fd\u63d0\u5347\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u7ec4\u5339\u914d\u5206\u6570\u6765\u66f4\u597d\u5730\u5229\u7528\u7ec4\u7ed3\u6784\u63ed\u793a\u6a21\u578b\u7684\u9690\u85cf\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u6d4b\u8bd5\u65f6\u5339\u914d\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u8fed\u4ee3\u81ea\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u8fc7\u62df\u5408\u8bf1\u5bfc\u7684\u7ec4\u5339\u914d\u6765\u63d0\u5347\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff1aSigLIP-B16\u8d85\u8d8a\u6240\u6709\u5148\u524d\u7ed3\u679c\uff0cGPT-4.1\u5728Winoground\u4e0a\u9996\u6b21\u8d85\u8d8a\u4f30\u8ba1\u7684\u4eba\u7c7b\u6027\u80fd\uff0cTTM\u4f7fSigLIP-B16\u5728MMVP-VLM\u4e0a\u8d85\u8d8aGPT-4.1\u5efa\u7acb\u65b0SOTA\uff0c\u572816\u4e2a\u6570\u636e\u96c6\u53d8\u4f53\u4e0a\u5747\u5b9e\u73b0\u4e00\u81f4\u6539\u8fdb\uff0c\u5728WhatsUp\u7b49\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u76f8\u5bf9\u589e\u76ca\u9ad8\u8fbe85.7%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u6d4b\u8bd5\u65f6\u8c03\u6574\u53ef\u4ee5\u663e\u8457\u91ca\u653e\u6a21\u578b\u7684\u9690\u85cf\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff0c\u6d4b\u8bd5\u65f6\u5339\u914d\u7b97\u6cd5\u4e3a\u63d0\u5347\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u7ec4\u5408\u63a8\u7406\u7814\u7a76\u7684\u524d\u6cbf\u53d1\u5c55\u3002"}}
{"id": "2510.07791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07791", "abs": "https://arxiv.org/abs/2510.07791", "authors": ["Qinghongbing Xie", "Zhaoyuan Xia", "Feng Zhu", "Lijun Gong", "Ziyue Li", "Rui Zhao", "Long Zeng"], "title": "GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models", "comment": "20 pages, 13 figures", "summary": "Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has\nattracted much attention due to its importance for Autonomous Driving, Embodied\nAI and General Artificial Intelligence. Existing spatial-temporal benchmarks\nmainly focus on egocentric perspective reasoning with images/video context, or\ngeographic perspective reasoning with graphics context (eg. a map), thus fail\nto assess VLMs' geographic spatial-temporal intelligence with both images/video\nand graphics context, which is important for areas like traffic management and\nemergency response. To address the gaps, we introduce Geo-Temporal Reasoning\nbenchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of\nmoving targets in a large-scale camera network. GTR-Bench is more challenging\nas it requires multiple perspective switches between maps and videos, joint\nreasoning across multiple videos with non-overlapping fields of view, and\ninference over spatial-temporal regions that are unobserved by any video\ncontext. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that\neven the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags\nbehind human performance (78.61%) on geo-temporal reasoning. Moreover, our\ncomprehensive analysis on GTR-Bench reveals three primary deficiencies of\ncurrent models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by\nan imbalanced utilization of spatial-temporal context. (2) VLMs are weak in\ntemporal forecasting, which leads to worse performance on temporal-emphasized\ntasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to\ncomprehend or align the map data with multi-view video inputs. We believe\nGTR-Bench offers valuable insights and opens up new opportunities for research\nand applications in spatial-temporal intelligence. Benchmark and code will be\nreleased at https://github.com/X-Luffy/GTR-Bench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Geo-Temporal Reasoning\u57fa\u51c6\uff08GTR-Bench\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5927\u89c4\u6a21\u6444\u50cf\u5934\u7f51\u7edc\u4e2d\u79fb\u52a8\u76ee\u6807\u7684\u5730\u7406\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u5728\u878d\u5408\u56fe\u50cf/\u89c6\u9891\u4e0e\u56fe\u5f62\u4e0a\u4e0b\u6587\u8fdb\u884c\u5730\u7406\u65f6\u7a7a\u667a\u80fd\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u65f6\u7a7a\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u4ee5\u56fe\u50cf/\u89c6\u9891\u4e3a\u4e0a\u4e0b\u6587\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u63a8\u7406\uff0c\u6216\u4ee5\u56fe\u5f62\u4e3a\u4e0a\u4e0b\u6587\u7684\u5730\u7406\u89c6\u89d2\u63a8\u7406\uff0c\u4f46\u7f3a\u4e4f\u540c\u65f6\u5229\u7528\u56fe\u50cf/\u89c6\u9891\u548c\u56fe\u5f62\u4e0a\u4e0b\u6587\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5730\u7406\u65f6\u7a7a\u667a\u80fd\u7684\u57fa\u51c6\uff0c\u8fd9\u79cd\u80fd\u529b\u5bf9\u4e8e\u4ea4\u901a\u7ba1\u7406\u548c\u5e94\u6025\u54cd\u5e94\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86GTR-Bench\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u8981\u6c42\u6a21\u578b\u5728\u5730\u56fe\u548c\u89c6\u9891\u4e4b\u95f4\u8fdb\u884c\u591a\u89c6\u89d2\u5207\u6362\uff0c\u5bf9\u5177\u6709\u975e\u91cd\u53e0\u89c6\u91ce\u7684\u591a\u4e2a\u89c6\u9891\u8fdb\u884c\u8054\u5408\u63a8\u7406\uff0c\u5e76\u5728\u89c6\u9891\u4e0a\u4e0b\u6587\u672a\u89c2\u6d4b\u5230\u7684\u65f6\u7a7a\u533a\u57df\u8fdb\u884c\u63a8\u65ad\uff0c\u4ece\u800c\u5168\u9762\u8bc4\u4f30\u5730\u7406\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5bf910\u591a\u4e2a\u6d41\u884c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u6700\u4f73\u4e13\u6709\u6a21\u578bGemini-2.5-Pro\uff0834.9%\uff09\u4e5f\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0878.61%\uff09\uff0c\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5730\u7406\u65f6\u7a7a\u63a8\u7406\u4e2d\u7684\u4e09\u4e2a\u4e3b\u8981\u7f3a\u9677\uff1a\u65f6\u7a7a\u4e0a\u4e0b\u6587\u5229\u7528\u4e0d\u5e73\u8861\u3001\u65f6\u95f4\u9884\u6d4b\u80fd\u529b\u5f31\u3001\u5730\u56fe\u4e0e\u591a\u89c6\u89d2\u89c6\u9891\u8f93\u5165\u7406\u89e3\u5bf9\u9f50\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "GTR-Bench\u4e3a\u65f6\u7a7a\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u548c\u65b0\u673a\u9047\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5730\u7406\u65f6\u7a7a\u63a8\u7406\u65b9\u9762\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u65f6\u7a7a\u4e0a\u4e0b\u6587\u5229\u7528\u4e0d\u5e73\u8861\u3001\u65f6\u95f4\u9884\u6d4b\u80fd\u529b\u4e0d\u8db3\u4ee5\u53ca\u5730\u56fe\u4e0e\u89c6\u9891\u6570\u636e\u878d\u5408\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.07931", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07931", "abs": "https://arxiv.org/abs/2510.07931", "authors": ["Madis J\u00fcrviste", "Joonatan Jakobson"], "title": "Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German Dictionaries from the 17th and 18th Centuries", "comment": null, "summary": "This article presents research conducted at the Institute of the Estonian\nLanguage between 2022 and 2025 on the application of large language models\n(LLMs) to the study of 17th and 18th century Estonian dictionaries. The authors\naddress three main areas: enriching historical dictionaries with modern word\nforms and meanings; using vision-enabled LLMs to perform text recognition on\nsources printed in Gothic script (Fraktur); and preparing for the creation of a\nunified, cross-source dataset. Initial experiments with J. Gutslaff's 1648\ndictionary indicate that LLMs have significant potential for semi-automatic\nenrichment of dictionary information. When provided with sufficient context,\nClaude 3.7 Sonnet accurately provided meanings and modern equivalents for 81%\nof headword entries. In a text recognition experiment with A. T. Helle's 1732\ndictionary, a zero-shot method successfully identified and structured 41% of\nheadword entries into error-free JSON-formatted output. For digitising the\nEstonian-German dictionary section of A. W. Hupel's 1780 grammar, overlapping\ntiling of scanned image files is employed, with one LLM being used for text\nrecognition and a second for merging the structured output. These findings\ndemonstrate that even for minor languages LLMs have a significant potential for\nsaving time and financial resources.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u572817-18\u4e16\u7eaa\u7231\u6c99\u5c3c\u4e9a\u8bed\u5386\u53f2\u8bcd\u5178\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86LLMs\u5728\u8bcd\u5178\u4fe1\u606f\u534a\u81ea\u52a8\u4e30\u5bcc\u3001\u54e5\u7279\u4f53\u6587\u672c\u8bc6\u522b\u548c\u8de8\u6e90\u6570\u636e\u96c6\u6784\u5efa\u65b9\u9762\u7684\u663e\u8457\u6f5c\u529b\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5386\u53f2\u8bcd\u5178\u7814\u7a76\u4e2d\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\uff1a\u5982\u4f55\u9ad8\u6548\u4e30\u5bcc\u5386\u53f2\u8bcd\u5178\u7684\u73b0\u4ee3\u8bcd\u5f62\u548c\u8bcd\u4e49\uff0c\u5982\u4f55\u81ea\u52a8\u8bc6\u522b\u54e5\u7279\u4f53\u5370\u5237\u6587\u672c\uff0c\u4ee5\u53ca\u5982\u4f55\u4e3a\u6784\u5efa\u7edf\u4e00\u8de8\u6e90\u6570\u636e\u96c6\u505a\u51c6\u5907\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8d44\u6e90\u76f8\u5bf9\u532e\u4e4f\u7684\u5c0f\u8bed\u79cd\u7814\u7a76\u3002", "method": "\u7814\u7a76\u91c7\u7528\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u5305\u62ec\u4f7f\u7528Claude 3.7 Sonnet\u8fdb\u884c\u8bcd\u5178\u4fe1\u606f\u7684\u534a\u81ea\u52a8\u4e30\u5bcc\uff0c\u8fd0\u7528\u89c6\u89c9\u589e\u5f3aLLMs\u8fdb\u884c\u54e5\u7279\u4f53\u6587\u672c\u7684\u96f6\u6837\u672c\u8bc6\u522b\uff0c\u4ee5\u53ca\u91c7\u7528\u91cd\u53e0\u5206\u5757\u626b\u63cf\u56fe\u50cf\u5904\u7406\u6280\u672f\u7ed3\u5408\u591a\u4e2aLLM\u8fdb\u884c\u6587\u672c\u8bc6\u522b\u548c\u7ed3\u6784\u5316\u8f93\u51fa\u5408\u5e76\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5145\u8db3\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\uff0cClaude 3.7 Sonnet\u80fd\u591f\u4e3a81%\u7684\u8bcd\u76ee\u51c6\u786e\u63d0\u4f9b\u8bcd\u4e49\u548c\u73b0\u4ee3\u5bf9\u5e94\u8bcd\uff1b\u5728\u6587\u672c\u8bc6\u522b\u5b9e\u9a8c\u4e2d\uff0c\u96f6\u6837\u672c\u65b9\u6cd5\u6210\u529f\u8bc6\u522b\u5e76\u7ed3\u6784\u531641%\u7684\u8bcd\u76ee\u4e3a\u65e0\u9519\u8bef\u7684JSON\u683c\u5f0f\u8f93\u51fa\uff1b\u91cd\u53e0\u5206\u5757\u6280\u672f\u6709\u6548\u652f\u6301\u4e86\u8bcd\u5178\u7684\u6570\u5b57\u5316\u5904\u7406\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5bf9\u4e8e\u5c0f\u8bed\u79cd\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e5f\u5177\u6709\u663e\u8457\u7684\u65f6\u95f4\u548c\u8d44\u6e90\u8282\u7701\u6f5c\u529b\uff0c\u4e3a\u5386\u53f2\u8bed\u8a00\u5b66\u7814\u7a76\u548c\u6587\u5316\u9057\u4ea7\u6570\u5b57\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6280\u672f\u8def\u5f84\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u591aLLM\u534f\u540c\u5de5\u4f5c\u6d41\u7a0b\u5728\u590d\u6742\u6587\u672c\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.07709", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.07709", "abs": "https://arxiv.org/abs/2510.07709", "authors": ["Alhim Vera", "Karen Sanchez", "Carlos Hinojosa", "Haidar Bin Hamid", "Donghoon Kim", "Bernard Ghanem"], "title": "Multimodal Safety Evaluation in Generative Agent Social Simulations", "comment": null, "summary": "Can generative agents be trusted in multimodal environments? Despite advances\nin large language and vision-language models that enable agents to act\nautonomously and pursue goals in rich settings, their ability to reason about\nsafety, coherence, and trust across modalities remains limited. We introduce a\nreproducible simulation framework for evaluating agents along three dimensions:\n(1) safety improvement over time, including iterative plan revisions in\ntext-visual scenarios; (2) detection of unsafe activities across multiple\ncategories of social situations; and (3) social dynamics, measured as\ninteraction counts and acceptance ratios of social exchanges. Agents are\nequipped with layered memory, dynamic planning, multimodal perception, and are\ninstrumented with SocialMetrics, a suite of behavioral and structural metrics\nthat quantifies plan revisions, unsafe-to-safe conversions, and information\ndiffusion across networks. Experiments show that while agents can detect direct\nmultimodal contradictions, they often fail to align local revisions with global\nsafety, reaching only a 55 percent success rate in correcting unsafe plans.\nAcross eight simulation runs with three models - Claude, GPT-4o mini, and\nQwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75,\n55, and 58 percent, respectively. Overall performance ranged from 20 percent in\nmulti-risk scenarios with GPT-4o mini to 98 percent in localized contexts such\nas fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted\nwhen paired with misleading visuals, showing a strong tendency to overtrust\nimages. These findings expose critical limitations in current architectures and\nprovide a reproducible platform for studying multimodal safety, coherence, and\nsocial dynamics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u4eff\u771f\u6846\u67b6\uff0c\u901a\u8fc7\u793e\u4f1a\u884c\u4e3a\u6307\u6807\u8bc4\u4f30\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u5728\u5b89\u5168\u3001\u4e00\u81f4\u6027\u548c\u4fe1\u4efb\u65b9\u9762\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u5bf9\u9f50\u7f3a\u9677\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u4e30\u5bcc\u73af\u5883\u4e2d\u81ea\u4e3b\u884c\u52a8\u5e76\u8ffd\u6c42\u76ee\u6807\uff0c\u4f46\u5b83\u4eec\u5728\u8de8\u6a21\u6001\u5b89\u5168\u3001\u4e00\u81f4\u6027\u548c\u4fe1\u4efb\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u7136\u6709\u9650\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u53ef\u590d\u73b0\u7684\u4eff\u771f\u6846\u67b6\uff0c\u914d\u5907\u5206\u5c42\u8bb0\u5fc6\u3001\u52a8\u6001\u89c4\u5212\u548c\u591a\u6a21\u6001\u611f\u77e5\u80fd\u529b\u7684\u667a\u80fd\u4f53\uff0c\u5e76\u91c7\u7528SocialMetrics\u884c\u4e3a\u6307\u6807\u5957\u4ef6\u6765\u91cf\u5316\u8ba1\u5212\u4fee\u8ba2\u3001\u4e0d\u5b89\u5168\u5230\u5b89\u5168\u8f6c\u6362\u4ee5\u53ca\u7f51\u7edc\u4e2d\u7684\u4fe1\u606f\u6269\u6563\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u667a\u80fd\u4f53\u5728\u7ea0\u6b63\u4e0d\u5b89\u5168\u8ba1\u5212\u65b9\u9762\u4ec5\u8fbe\u523055%\u6210\u529f\u7387\uff0c\u4e09\u4e2a\u6a21\u578b\uff08Claude\u3001GPT-4o mini\u3001Qwen-VL\uff09\u7684\u5e73\u5747\u4e0d\u5b89\u5168\u5230\u5b89\u5168\u8f6c\u6362\u7387\u5206\u522b\u4e3a75%\u300155%\u548c58%\uff0c\u5728\u8bef\u5bfc\u6027\u89c6\u89c9\u4fe1\u606f\u4e0b45%\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\u88ab\u63a5\u53d7\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524d\u67b6\u6784\u5728\u591a\u6a21\u6001\u5b89\u5168\u5bf9\u9f50\u65b9\u9762\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u4e3a\u7814\u7a76\u591a\u6a21\u6001\u5b89\u5168\u3001\u4e00\u81f4\u6027\u548c\u793e\u4f1a\u52a8\u6001\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u5e73\u53f0\uff0c\u5f3a\u8c03\u4e86\u667a\u80fd\u4f53\u5bf9\u56fe\u50cf\u4fe1\u606f\u7684\u8fc7\u5ea6\u4fe1\u4efb\u95ee\u9898\u3002"}}
{"id": "2510.07856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07856", "abs": "https://arxiv.org/abs/2510.07856", "authors": ["Haochen Yu", "Qiankun Liu", "Hongyuan Liu", "Jianfei Jiang", "Juntao Lyu", "Jiansheng Chen", "Huimin Ma"], "title": "XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method", "comment": "Project page: https://yuyuyu223.github.io/XYZCYlinder-projectpage/", "summary": "Recently, more attention has been paid to feedforward reconstruction\nparadigms, which mainly learn a fixed view transformation implicitly and\nreconstruct the scene with a single representation. However, their\ngeneralization capability and reconstruction accuracy are still limited while\nreconstructing driving scenes, which results from two aspects: (1) The fixed\nview transformation fails when the camera configuration changes, limiting the\ngeneralization capability across different driving scenes equipped with\ndifferent camera configurations. (2) The small overlapping regions between\nsparse views of the $360^\\circ$ panorama and the complexity of driving scenes\nincrease the learning difficulty, reducing the reconstruction accuracy. To\nhandle these difficulties, we propose \\textbf{XYZCylinder}, a feedforward model\nbased on a unified cylinder lifting method which involves camera modeling and\nfeature lifting. Specifically, to improve the generalization capability, we\ndesign a Unified Cylinder Camera Modeling (UCCM) strategy, which avoids the\nlearning of viewpoint-dependent spatial correspondence and unifies different\ncamera configurations with adjustable parameters. To improve the reconstruction\naccuracy, we propose a hybrid representation with several dedicated modules\nbased on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D image\nfeatures to 3D space. Experimental results show that XYZCylinder achieves\nstate-of-the-art performance under different evaluation settings, and can be\ngeneralized to other driving scenes in a zero-shot manner. Project page:\n\\href{https://yuyuyu223.github.io/XYZCYlinder-projectpage/}{here}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faXYZCylinder\uff0c\u4e00\u79cd\u57fa\u4e8e\u7edf\u4e00\u5706\u67f1\u4f53\u63d0\u5347\u65b9\u6cd5\u7684\u524d\u9988\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u76f8\u673a\u5efa\u6a21\u548c\u6df7\u5408\u8868\u793a\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u9a7e\u9a76\u573a\u666f\u91cd\u5efa\u4e2d\u56e0\u76f8\u673a\u914d\u7f6e\u53d8\u5316\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u7cbe\u5ea6\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u524d\u9988\u91cd\u5efa\u8303\u5f0f\u5728\u9a7e\u9a76\u573a\u666f\u91cd\u5efa\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u56fa\u5b9a\u89c6\u56fe\u53d8\u6362\u5728\u76f8\u673a\u914d\u7f6e\u53d8\u5316\u65f6\u5931\u6548\uff0c\u9650\u5236\u4e86\u4e0d\u540c\u9a7e\u9a76\u573a\u666f\u95f4\u7684\u6cdb\u5316\u80fd\u529b\uff1b360\u5ea6\u5168\u666f\u7a00\u758f\u89c6\u56fe\u95f4\u91cd\u53e0\u533a\u57df\u5c0f\u4e14\u9a7e\u9a76\u573a\u666f\u590d\u6742\uff0c\u589e\u52a0\u4e86\u5b66\u4e60\u96be\u5ea6\u5e76\u964d\u4f4e\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u5706\u67f1\u4f53\u76f8\u673a\u5efa\u6a21\u7b56\u7565\u907f\u514d\u5b66\u4e60\u89c6\u70b9\u4f9d\u8d56\u7684\u7a7a\u95f4\u5bf9\u5e94\u5173\u7cfb\uff0c\u4f7f\u7528\u53ef\u8c03\u53c2\u6570\u7edf\u4e00\u4e0d\u540c\u76f8\u673a\u914d\u7f6e\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u5706\u67f1\u5e73\u9762\u7279\u5f81\u7ec4\u7684\u6df7\u5408\u8868\u793a\uff0c\u901a\u8fc7\u4e13\u7528\u6a21\u5757\u5c062D\u56fe\u50cf\u7279\u5f81\u63d0\u5347\u52303D\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cXYZCylinder\u5728\u4e0d\u540c\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u591f\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u6cdb\u5316\u5230\u5176\u4ed6\u9a7e\u9a76\u573a\u666f\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7edf\u4e00\u76f8\u673a\u5efa\u6a21\u548c\u6df7\u5408\u8868\u793a\u5728\u63d0\u5347\u9a7e\u9a76\u573a\u666f\u91cd\u5efa\u6cdb\u5316\u80fd\u529b\u548c\u7cbe\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5904\u7406\u590d\u6742\u591a\u76f8\u673a\u914d\u7f6e\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07993", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07993", "abs": "https://arxiv.org/abs/2510.07993", "authors": ["Watcharapong Timklaypachara", "Monrada Chiewhawan", "Nopporn Lekuthai", "Titipat Achakulvisut"], "title": "Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge", "comment": null, "summary": "Scientific figure captions require both accuracy and stylistic consistency to\nconvey visual information. Here, we present a domain-specific caption\ngeneration system for the 3rd SciCap Challenge that integrates figure-related\ntextual context with author-specific writing styles using the LaMP-Cap dataset.\nOur approach uses a two-stage pipeline: Stage 1 combines context filtering,\ncategory-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption\ncandidate selection; Stage 2 applies few-shot prompting with profile figures\nfor stylistic refinement. Our experiments demonstrate that category-specific\nprompts outperform both zero-shot and general optimized approaches, improving\nROUGE-1 recall by +8.3\\% while limiting precision loss to -2.8\\% and BLEU-4\nreduction to -10.9\\%. Profile-informed stylistic refinement yields 40--48\\%\ngains in BLEU scores and 25--27\\% in ROUGE. Overall, our system demonstrates\nthat combining contextual understanding with author-specific stylistic\nadaptation can generate captions that are both scientifically accurate and\nstylistically faithful to the source paper.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u79d1\u5b66\u56fe\u8868\u6807\u9898\u751f\u6210\u7684\u9886\u57df\u7279\u5b9a\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u548c\u4f5c\u8005\u7279\u5b9a\u5199\u4f5c\u98ce\u683c\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u65e2\u79d1\u5b66\u51c6\u786e\u53c8\u98ce\u683c\u5fe0\u5b9e\u4e8e\u6e90\u8bba\u6587\u7684\u6807\u9898\u751f\u6210\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u5728SciCap\u6311\u6218\u4e2d\u8bc1\u660e\u4e86\u7c7b\u522b\u7279\u5b9a\u63d0\u793a\u548c\u98ce\u683c\u5316\u7cbe\u70bc\u7684\u6709\u6548\u6027\u3002", "motivation": "\u79d1\u5b66\u56fe\u8868\u6807\u9898\u9700\u8981\u540c\u65f6\u5177\u5907\u51c6\u786e\u6027\u548c\u98ce\u683c\u4e00\u81f4\u6027\u4ee5\u4f20\u8fbe\u89c6\u89c9\u4fe1\u606f\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u4fdd\u6301\u4f5c\u8005\u7279\u5b9a\u5199\u4f5c\u98ce\u683c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u79d1\u5b66\u56fe\u8868\u6807\u9898\u751f\u6210\u4e2d\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0e\u98ce\u683c\u9002\u5e94\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9886\u57df\u7279\u5b9a\u7684SciCap\u6311\u6218\u80cc\u666f\u4e0b\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u7ed3\u5408\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u3001\u57fa\u4e8eDSPy\u7684MIPROv2\u548cSIMBA\u8fdb\u884c\u7c7b\u522b\u7279\u5b9a\u63d0\u793a\u4f18\u5316\u4ee5\u53ca\u6807\u9898\u5019\u9009\u9009\u62e9\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528\u5c11\u91cf\u6837\u672c\u63d0\u793a\u7ed3\u5408\u914d\u7f6e\u6587\u4ef6\u8fdb\u884c\u98ce\u683c\u5316\u7cbe\u70bc\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u56fe\u8868\u76f8\u5173\u6587\u672c\u4e0a\u4e0b\u6587\u4e0e\u4f5c\u8005\u7279\u5b9a\u5199\u4f5c\u98ce\u683c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7c7b\u522b\u7279\u5b9a\u63d0\u793a\u5728\u96f6\u6837\u672c\u548c\u901a\u7528\u4f18\u5316\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5c06ROUGE-1\u53ec\u56de\u7387\u63d0\u5347+8.3%\uff0c\u540c\u65f6\u5c06\u7cbe\u5ea6\u635f\u5931\u9650\u5236\u5728-2.8%\uff0cBLEU-4\u51cf\u5c11\u63a7\u5236\u5728-10.9%\u3002\u57fa\u4e8e\u914d\u7f6e\u6587\u4ef6\u7684\u98ce\u683c\u5316\u7cbe\u70bc\u4f7fBLEU\u5f97\u5206\u63d0\u534740-48%\uff0cROUGE\u5f97\u5206\u63d0\u534725-27%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7ed3\u5408\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0e\u4f5c\u8005\u7279\u5b9a\u98ce\u683c\u9002\u5e94\u80fd\u591f\u751f\u6210\u65e2\u79d1\u5b66\u51c6\u786e\u53c8\u98ce\u683c\u5fe0\u5b9e\u7684\u6807\u9898\u3002\u8be5\u65b9\u6cd5\u4e3a\u9886\u57df\u7279\u5b9a\u7684\u79d1\u5b66\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u7c7b\u522b\u7279\u5b9a\u4f18\u5316\u548c\u4e2a\u6027\u5316\u98ce\u683c\u8c03\u6574\u5728\u6280\u672f\u6587\u6863\u751f\u6210\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.07852", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07852", "abs": "https://arxiv.org/abs/2510.07852", "authors": ["Shuangyan Deng", "Haizhou Peng", "Jiachen Xu", "Rui Mao", "Ciprian Doru Giurc\u0103neanu", "Jiamou Liu"], "title": "FinMR: A Knowledge-Intensive Multimodal Benchmark for Advanced Financial Reasoning", "comment": "This paper has been accept by ICAIF 2025", "summary": "Multimodal Large Language Models (MLLMs) have made substantial progress in\nrecent years. However, their rigorous evaluation within specialized domains\nlike finance is hindered by the absence of datasets characterized by\nprofessional-level knowledge intensity, detailed annotations, and advanced\nreasoning complexity. To address this critical gap, we introduce FinMR, a\nhigh-quality, knowledge-intensive multimodal dataset explicitly designed to\nevaluate expert-level financial reasoning capabilities at a professional\nanalyst's standard. FinMR comprises over 3,200 meticulously curated and\nexpertly annotated question-answer pairs across 15 diverse financial topics,\nensuring broad domain diversity and integrating sophisticated mathematical\nreasoning, advanced financial knowledge, and nuanced visual interpretation\ntasks across multiple image types. Through comprehensive benchmarking with\nleading closed-source and open-source MLLMs, we highlight significant\nperformance disparities between these models and professional financial\nanalysts, uncovering key areas for model advancement, such as precise image\nanalysis, accurate application of complex financial formulas, and deeper\ncontextual financial understanding. By providing richly varied visual content\nand thorough explanatory annotations, FinMR establishes itself as an essential\nbenchmark tool for assessing and advancing multimodal financial reasoning\ntoward professional analyst-level competence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FinMR\uff0c\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u77e5\u8bc6\u5bc6\u96c6\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u4e13\u4e1a\u5206\u6790\u5e08\u7ea7\u522b\u7684\u91d1\u878d\u63a8\u7406\u80fd\u529b\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b3200\u591a\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u4e13\u5bb6\u6807\u6ce8\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d615\u4e2a\u91d1\u878d\u4e3b\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u9886\u57df\u7684\u4e13\u4e1a\u7ea7\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u51c6\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u4e25\u683c\u8bc4\u4f30\u53d7\u5230\u9650\u5236\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5177\u6709\u4e13\u4e1a\u7ea7\u77e5\u8bc6\u5f3a\u5ea6\u3001\u8be6\u7ec6\u6807\u6ce8\u548c\u9ad8\u7ea7\u63a8\u7406\u590d\u6742\u5ea6\u7684\u6570\u636e\u96c6\u3002\u73b0\u6709\u6570\u636e\u96c6\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u6a21\u578b\u5728\u4e13\u4e1a\u91d1\u878d\u5206\u6790\u5e08\u6807\u51c6\u4e0b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u963b\u788d\u4e86\u6a21\u578b\u5728\u91d1\u878d\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u548c\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86FinMR\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc73200\u4e2a\u7cbe\u5fc3\u7b56\u5212\u548c\u4e13\u5bb6\u6807\u6ce8\u7684\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d615\u4e2a\u4e0d\u540c\u7684\u91d1\u878d\u4e3b\u9898\u3002\u8be5\u6570\u636e\u96c6\u6574\u5408\u4e86\u590d\u6742\u7684\u6570\u5b66\u63a8\u7406\u3001\u9ad8\u7ea7\u91d1\u878d\u77e5\u8bc6\u548c\u7ec6\u5fae\u7684\u56fe\u50cf\u89e3\u91ca\u4efb\u52a1\uff0c\u6d89\u53ca\u591a\u79cd\u56fe\u50cf\u7c7b\u578b\uff0c\u786e\u4fdd\u5e7f\u6cdb\u7684\u9886\u57df\u591a\u6837\u6027\u548c\u4e13\u4e1a\u7ea7\u7684\u77e5\u8bc6\u5f3a\u5ea6\u3002", "result": "\u901a\u8fc7\u5bf9\u9886\u5148\u7684\u95ed\u6e90\u548c\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u4e0e\u4e13\u4e1a\u91d1\u878d\u5206\u6790\u5e08\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u6d4b\u8bd5\u63ed\u793a\u4e86\u6a21\u578b\u5728\u7cbe\u786e\u56fe\u50cf\u5206\u6790\u3001\u590d\u6742\u91d1\u878d\u516c\u5f0f\u7684\u51c6\u786e\u5e94\u7528\u4ee5\u53ca\u66f4\u6df1\u5c42\u6b21\u4e0a\u4e0b\u6587\u91d1\u878d\u7406\u89e3\u7b49\u5173\u952e\u9886\u57df\u9700\u8981\u6539\u8fdb\u3002", "conclusion": "FinMR\u901a\u8fc7\u63d0\u4f9b\u4e30\u5bcc\u591a\u6837\u7684\u89c6\u89c9\u5185\u5bb9\u548c\u8be6\u5c3d\u7684\u89e3\u91ca\u6027\u6807\u6ce8\uff0c\u786e\u7acb\u4e86\u4f5c\u4e3a\u8bc4\u4f30\u548c\u63a8\u8fdb\u591a\u6a21\u6001\u91d1\u878d\u63a8\u7406\u5411\u4e13\u4e1a\u5206\u6790\u5e08\u6c34\u5e73\u53d1\u5c55\u7684\u5173\u952e\u57fa\u51c6\u5de5\u5177\u3002\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u9886\u57df\u7684\u4e13\u4e1a\u7ea7\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u5e76\u6307\u660e\u4e86\u6a21\u578b\u6539\u8fdb\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2510.07915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07915", "abs": "https://arxiv.org/abs/2510.07915", "authors": ["Peiran Wu", "Zhuorui Yu", "Yunze Liu", "Chi-Hao Wu", "Enmin Zhou", "Junxiao Shen"], "title": "MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding", "comment": null, "summary": "The rapid progress of large language models (LLMs) has laid the foundation\nfor multimodal models. However, visual language models (VLMs) still face heavy\ncomputational costs when extended from images to videos due to high frame rates\nand long durations. Token compression is a promising solution, yet most\nexisting training-free methods cause information loss and performance\ndegradation. To overcome this, we propose \\textbf{Memory-Augmented\nReinforcement Learning-based Token Compression (MARC)}, which integrates\nstructured retrieval and RL-based distillation. MARC adopts a\n\\textit{retrieve-then-compress} strategy using a \\textbf{Visual Memory\nRetriever (VMR)} to select key clips and a \\textbf{Compression Group Relative\nPolicy Optimization (C-GRPO)} framework to distil reasoning ability from a\nteacher to a student model. Experiments on six video benchmarks show that MARC\nachieves near-baseline accuracy using only one frame's tokens -- reducing\nvisual tokens by \\textbf{95\\%}, GPU memory by \\textbf{72\\%}, and latency by\n\\textbf{23.9\\%}. This demonstrates its potential for efficient, real-time video\nunderstanding in resource-constrained settings such as video QA, surveillance,\nand autonomous driving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMARC\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u68c0\u7d22\u548c\u5f3a\u5316\u5b66\u4e60\u84b8\u998f\u5b9e\u73b0\u89c6\u9891token\u538b\u7f29\uff0c\u5728\u4ec5\u4f7f\u7528\u5355\u5e27token\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u63a5\u8fd1\u57fa\u7ebf\u51c6\u786e\u7387\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u56fe\u50cf\u6269\u5c55\u5230\u89c6\u9891\u65f6\u9762\u4e34\u9ad8\u5e27\u7387\u548c\u957f\u6301\u7eed\u65f6\u95f4\u5e26\u6765\u7684\u6c89\u91cd\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u73b0\u6709\u514d\u8bad\u7ec3token\u538b\u7f29\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faMARC\u65b9\u6cd5\uff0c\u91c7\u7528\u68c0\u7d22-\u538b\u7f29\u7b56\u7565\uff0c\u5305\u542b\u89c6\u89c9\u8bb0\u5fc6\u68c0\u7d22\u5668\u9009\u62e9\u5173\u952e\u7247\u6bb5\uff0c\u4ee5\u53ca\u538b\u7f29\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u6846\u67b6\u4ece\u6559\u5e08\u6a21\u578b\u5411\u5b66\u751f\u6a21\u578b\u84b8\u998f\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u516d\u4e2a\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMARC\u4ec5\u4f7f\u7528\u5355\u5e27token\u5373\u53ef\u8fbe\u5230\u63a5\u8fd1\u57fa\u7ebf\u51c6\u786e\u7387\uff0c\u89c6\u89c9token\u51cf\u5c1195%\uff0cGPU\u5185\u5b58\u964d\u4f4e72%\uff0c\u5ef6\u8fdf\u51cf\u5c1123.9%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u89c6\u9891\u95ee\u7b54\u3001\u76d1\u63a7\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u9886\u57df\u3002"}}
{"id": "2510.08002", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08002", "abs": "https://arxiv.org/abs/2510.08002", "authors": ["Cheng Yang", "Xuemeng Yang", "Licheng Wen", "Daocheng Fu", "Jianbiao Mei", "Rong Wu", "Pinlong Cai", "Yufan Shen", "Nianchen Deng", "Botian Shi", "Yu Qiao", "Haifeng Li"], "title": "Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks", "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities across\ndiverse domains, yet significant challenges persist when deploying them as AI\nagents for real-world long-horizon tasks. Existing LLM agents suffer from a\ncritical limitation: they are test-time static and cannot learn from\nexperience, lacking the ability to accumulate knowledge and continuously\nimprove on the job. To address this challenge, we propose MUSE, a novel agent\nframework that introduces an experience-driven, self-evolving system centered\naround a hierarchical Memory Module. MUSE organizes diverse levels of\nexperience and leverages them to plan and execute long-horizon tasks across\nmultiple applications. After each sub-task execution, the agent autonomously\nreflects on its trajectory, converting the raw trajectory into structured\nexperience and integrating it back into the Memory Module. This mechanism\nenables the agent to evolve beyond its static pretrained parameters, fostering\ncontinuous learning and self-evolution. We evaluate MUSE on the long-horizon\nproductivity benchmark TAC. It achieves new SOTA performance by a significant\nmargin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments\ndemonstrate that as the agent autonomously accumulates experience, it exhibits\nincreasingly superior task completion capabilities, as well as robust\ncontinuous learning and self-evolution capabilities. Moreover, the accumulated\nexperience from MUSE exhibits strong generalization properties, enabling\nzero-shot improvement on new tasks. MUSE establishes a new paradigm for AI\nagents capable of real-world productivity task automation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MUSE\uff0c\u4e00\u79cd\u7ecf\u9a8c\u9a71\u52a8\u7684\u81ea\u8fdb\u5316AI\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8bb0\u5fc6\u6a21\u5757\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u548c\u81ea\u6211\u8fdb\u5316\uff0c\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5b58\u5728\u6d4b\u8bd5\u65f6\u9759\u6001\u5316\u7684\u5173\u952e\u5c40\u9650\uff0c\u65e0\u6cd5\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u7f3a\u4e4f\u77e5\u8bc6\u79ef\u7d2f\u548c\u6301\u7eed\u6539\u8fdb\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u90e8\u7f72\u6548\u679c\u3002", "method": "MUSE\u6846\u67b6\u5f15\u5165\u4e86\u4ee5\u5206\u5c42\u8bb0\u5fc6\u6a21\u5757\u4e3a\u6838\u5fc3\u7684\u7ecf\u9a8c\u9a71\u52a8\u81ea\u8fdb\u5316\u7cfb\u7edf\uff0c\u5c06\u539f\u59cb\u8f68\u8ff9\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u7ecf\u9a8c\u5e76\u6574\u5408\u56de\u8bb0\u5fc6\u6a21\u5757\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u8d85\u8d8a\u5176\u9759\u6001\u9884\u8bad\u7ec3\u53c2\u6570\u5b9e\u73b0\u6301\u7eed\u8fdb\u5316\u3002", "result": "\u5728\u957f\u65f6\u7a0b\u751f\u4ea7\u529b\u57fa\u51c6TAC\u4e0a\uff0cMUSE\u4ec5\u4f7f\u7528\u8f7b\u91cf\u7ea7Gemini-2.5 Flash\u6a21\u578b\u5c31\u5b9e\u73b0\u4e86\u65b0\u7684SOTA\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u968f\u7740\u7ecf\u9a8c\u79ef\u7d2f\u5c55\u73b0\u51fa\u8d8a\u6765\u8d8a\u5f3a\u7684\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MUSE\u5efa\u7acb\u4e86\u4e00\u4e2a\u80fd\u591f\u8fdb\u884c\u73b0\u5b9e\u4e16\u754c\u751f\u4ea7\u529b\u4efb\u52a1\u81ea\u52a8\u5316\u7684AI\u667a\u80fd\u4f53\u65b0\u8303\u5f0f\uff0c\u5176\u79ef\u7d2f\u7684\u7ecf\u9a8c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u7279\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u65b0\u4efb\u52a1\u7684\u96f6\u6837\u672c\u6539\u8fdb\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.07858", "categories": ["cs.AI", "cs.LG", "62M10", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.07858", "abs": "https://arxiv.org/abs/2510.07858", "authors": ["Zhiqing Cui", "Binwu Wang", "Qingxiang Liu", "Yeqiang Wang", "Zhengyang Zhou", "Yuxuan Liang", "Yang Wang"], "title": "Augur: Modeling Covariate Causal Associations in Time Series via Large Language Models", "comment": "22 pages, 9 figures", "summary": "Large language models (LLM) have emerged as a promising avenue for time\nseries forecasting, offering the potential to integrate multimodal data.\nHowever, existing LLM-based approaches face notable limitations-such as\nmarginalized role in model architectures, reliance on coarse statistical text\nprompts, and lack of interpretability. In this work, we introduce Augur, a\nfully LLM driven time series forecasting framework that exploits LLM causal\nreasoning to discover and use directed causal associations among covariates.\nAugur uses a two stage teacher student architecture where a powerful teacher\nLLM infers a directed causal graph from time series using heuristic search\ntogether with pairwise causality testing. A lightweight student agent then\nrefines the graph and fine tune on high confidence causal associations that are\nencoded as rich textual prompts to perform forecasting. This design improves\npredictive accuracy while yielding transparent, traceable reasoning about\nvariable interactions. Extensive experiments on real-world datasets with 25\nbaselines demonstrate that Augur achieves competitive performance and robust\nzero-shot generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAugur\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b8c\u5168\u9a71\u52a8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u53d1\u73b0\u534f\u53d8\u91cf\u95f4\u7684\u6709\u5411\u56e0\u679c\u5173\u7cfb\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5728\u6a21\u578b\u67b6\u6784\u4e2d\u7684\u8fb9\u7f18\u5316\u89d2\u8272\u3001\u4f9d\u8d56\u7c97\u7cd9\u7edf\u8ba1\u6587\u672c\u63d0\u793a\u4ee5\u53ca\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86LLM\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u53d1\u6325\u3002", "method": "Augur\u91c7\u7528\u4e24\u9636\u6bb5\u5e08\u751f\u67b6\u6784\uff0c\u9996\u5148\u7531\u5f3a\u5927\u7684\u6559\u5e08LLM\u901a\u8fc7\u542f\u53d1\u5f0f\u641c\u7d22\u548c\u6210\u5bf9\u56e0\u679c\u68c0\u9a8c\u4ece\u65f6\u95f4\u5e8f\u5217\u63a8\u65ad\u6709\u5411\u56e0\u679c\u56fe\uff0c\u7136\u540e\u7531\u8f7b\u91cf\u7ea7\u5b66\u751f\u4ee3\u7406\u7cbe\u70bc\u8be5\u56fe\u5e76\u57fa\u4e8e\u9ad8\u7f6e\u4fe1\u5ea6\u56e0\u679c\u5173\u8054\u8fdb\u884c\u5fae\u8c03\uff0c\u8fd9\u4e9b\u5173\u8054\u88ab\u7f16\u7801\u4e3a\u4e30\u5bcc\u6587\u672c\u63d0\u793a\u7528\u4e8e\u9884\u6d4b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAugur\u572825\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86LLM\u56e0\u679c\u63a8\u7406\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u8fd8\u63d0\u4f9b\u4e86\u900f\u660e\u53ef\u8ffd\u6eaf\u7684\u53d8\u91cf\u4ea4\u4e92\u63a8\u7406\uff0c\u4e3a\u53ef\u89e3\u91caAI\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.07940", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.07940", "abs": "https://arxiv.org/abs/2510.07940", "authors": ["Leigang Qu", "Ziyang Wang", "Na Zheng", "Wenjie Wang", "Liqiang Nie", "Tat-Seng Chua"], "title": "TTOM: Test-Time Optimization and Memorization for Compositional Video Generation", "comment": "Project page: https://ttom-t2v.github.io/", "summary": "Video Foundation Models (VFMs) exhibit remarkable visual generation\nperformance, but struggle in compositional scenarios (e.g., motion, numeracy,\nand spatial relation). In this work, we introduce Test-Time Optimization and\nMemorization (TTOM), a training-free framework that aligns VFM outputs with\nspatiotemporal layouts during inference for better text-image alignment. Rather\nthan direct intervention to latents or attention per-sample in existing work,\nwe integrate and optimize new parameters guided by a general layout-attention\nobjective. Furthermore, we formulate video generation within a streaming\nsetting, and maintain historical optimization contexts with a parametric memory\nmechanism that supports flexible operations, such as insert, read, update, and\ndelete. Notably, we found that TTOM disentangles compositional world knowledge,\nshowing powerful transferability and generalization. Experimental results on\nthe T2V-CompBench and Vbench benchmarks establish TTOM as an effective,\npractical, scalable, and efficient framework to achieve cross-modal alignment\nfor compositional video generation on the fly.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TTOM\uff08\u6d4b\u8bd5\u65f6\u4f18\u5316\u4e0e\u8bb0\u5fc6\u5316\uff09\u6846\u67b6\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5e03\u5c40\u5f15\u5bfc\u7684\u6ce8\u610f\u529b\u4f18\u5316\u548c\u53c2\u6570\u5316\u8bb0\u5fc6\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u57fa\u7840\u6a21\u578b\u5728\u7ec4\u5408\u573a\u666f\u4e0b\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u89c6\u9891\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7ec4\u5408\u573a\u666f\uff08\u5982\u8fd0\u52a8\u3001\u6570\u91cf\u5173\u7cfb\u548c\u7a7a\u95f4\u5173\u7cfb\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u4f18\u5316\u6a21\u578b\u8f93\u51fa\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86TTOM\u6846\u67b6\uff0c\u5305\u542b\u6d4b\u8bd5\u65f6\u4f18\u5316\u548c\u8bb0\u5fc6\u5316\u673a\u5236\uff0c\u901a\u8fc7\u5e03\u5c40\u5f15\u5bfc\u7684\u6ce8\u610f\u529b\u76ee\u6807\u51fd\u6570\u4f18\u5316\u65b0\u53c2\u6570\uff0c\u5e76\u91c7\u7528\u53c2\u6570\u5316\u8bb0\u5fc6\u673a\u5236\u652f\u6301\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5386\u53f2\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u652f\u6301\u63d2\u5165\u3001\u8bfb\u53d6\u3001\u66f4\u65b0\u548c\u5220\u9664\u7b49\u7075\u6d3b\u64cd\u4f5c\u3002", "result": "\u5728T2V-CompBench\u548cVbench\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTTOM\u80fd\u591f\u6709\u6548\u89e3\u8026\u7ec4\u5408\u6027\u4e16\u754c\u77e5\u8bc6\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u8fc1\u79fb\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u6210\u4e3a\u5b9e\u73b0\u7ec4\u5408\u89c6\u9891\u751f\u6210\u4e2d\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u6709\u6548\u3001\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u6846\u67b6\u3002", "conclusion": "TTOM\u6846\u67b6\u8bc1\u660e\u4e86\u5728\u63a8\u7406\u9636\u6bb5\u901a\u8fc7\u5e03\u5c40\u5f15\u5bfc\u4f18\u5316\u548c\u8bb0\u5fc6\u673a\u5236\u80fd\u591f\u663e\u8457\u63d0\u5347\u89c6\u9891\u57fa\u7840\u6a21\u578b\u7684\u7ec4\u5408\u80fd\u529b\uff0c\u4e3a\u52a8\u6001\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.08049", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08049", "abs": "https://arxiv.org/abs/2510.08049", "authors": ["Congming Zheng", "Jiachen Zhu", "Zhuoying Ou", "Yuxiang Chen", "Kangning Zhang", "Rong Shan", "Zeyu Zheng", "Mengyue Yang", "Jianghao Lin", "Yong Yu", "Weinan Zhang"], "title": "A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models", "comment": null, "summary": "Although Large Language Models (LLMs) exhibit advanced reasoning ability,\nconventional alignment remains largely dominated by outcome reward models\n(ORMs) that judge only final answers. Process Reward Models(PRMs) address this\ngap by evaluating and guiding reasoning at the step or trajectory level. This\nsurvey provides a systematic overview of PRMs through the full loop: how to\ngenerate process data, build PRMs, and use PRMs for test-time scaling and\nreinforcement learning. We summarize applications across math, code, text,\nmultimodal reasoning, robotics, and agents, and review emerging benchmarks. Our\ngoal is to clarify design spaces, reveal open challenges, and guide future\nresearch toward fine-grained, robust reasoning alignment.", "AI": {"tldr": "\u672c\u6587\u5bf9\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u901a\u8fc7\u5b8c\u6574\u6d41\u7a0b\u5c55\u793a\u4e86\u5982\u4f55\u751f\u6210\u8fc7\u7a0b\u6570\u636e\u3001\u6784\u5efaPRMs\u4ee5\u53ca\u5c06\u5176\u7528\u4e8e\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u65e8\u5728\u63a8\u52a8\u7ec6\u7c92\u5ea6\u3001\u9c81\u68d2\u7684\u63a8\u7406\u5bf9\u9f50\u7814\u7a76\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u5148\u8fdb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u4e3b\u8981\u88ab\u4ec5\u8bc4\u5224\u6700\u7ec8\u7b54\u6848\u7684\u7ed3\u679c\u5956\u52b1\u6a21\u578b\uff08ORMs\uff09\u4e3b\u5bfc\uff0c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u901a\u8fc7\u8bc4\u4f30\u548c\u6307\u5bfc\u6b65\u9aa4\u6216\u8f68\u8ff9\u7ea7\u522b\u7684\u63a8\u7406\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u8be5\u7efc\u8ff0\u7cfb\u7edf\u6027\u5730\u6982\u8ff0\u4e86PRMs\u7684\u5b8c\u6574\u5faa\u73af\uff1a\u5305\u62ec\u8fc7\u7a0b\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3001PRMs\u6784\u5efa\u6280\u672f\u4ee5\u53ca\u5c06PRMs\u7528\u4e8e\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\u7b56\u7565\u3002", "result": "\u7814\u7a76\u603b\u7ed3\u4e86PRMs\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001\u6587\u672c\u3001\u591a\u6a21\u6001\u63a8\u7406\u3001\u673a\u5668\u4eba\u6280\u672f\u548c\u667a\u80fd\u4f53\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u56de\u987e\u4e86\u65b0\u5174\u7684\u57fa\u51c6\u6d4b\u8bd5\u4f53\u7cfb\uff0c\u63ed\u793a\u4e86\u8bbe\u8ba1\u7a7a\u95f4\u548c\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u9610\u660e\u4e86PRMs\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u9762\u4e34\u7684\u5f00\u653e\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u65b9\u5411\uff0c\u63a8\u52a8\u5411\u66f4\u7ec6\u7c92\u5ea6\u3001\u9c81\u68d2\u7684\u63a8\u7406\u5bf9\u9f50\u65b9\u6cd5\u53d1\u5c55\u3002"}}
{"id": "2510.08238", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08238", "abs": "https://arxiv.org/abs/2510.08238", "authors": ["Jiyang Qiu", "Xinbei Ma", "Yunqing Xu", "Zhuosheng Zhang", "Hai Zhao"], "title": "Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness", "comment": null, "summary": "The rapid deployment of large language model (LLM)-based agents in real-world\napplications has raised serious concerns about their trustworthiness. In this\nwork, we reveal the security and robustness vulnerabilities of these agents\nthrough backdoor attacks. Distinct from traditional backdoors limited to\nsingle-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a\nmulti-step backdoor attack designed for long-horizon agentic control. CoTri\nrelies on an ordered sequence. It starts with an initial trigger, and\nsubsequent ones are drawn from the environment, allowing multi-step\nmanipulation that diverts the agent from its intended task. Experimental\nresults show that CoTri achieves a near-perfect attack success rate (ASR) while\nmaintaining a near-zero false trigger rate (FTR). Due to training data modeling\nthe stochastic nature of the environment, the implantation of CoTri\nparadoxically enhances the agent's performance on benign tasks and even\nimproves its robustness against environmental distractions. We further validate\nCoTri on vision-language models (VLMs), confirming its scalability to\nmultimodal agents. Our work highlights that CoTri achieves stable, multi-step\ncontrol within agents, improving their inherent robustness and task\ncapabilities, which ultimately makes the attack more stealthy and raises\npotential safty risks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faChain-of-Trigger Backdoor (CoTri)\u653b\u51fb\u65b9\u6cd5\uff0c\u9488\u5bf9LLM\u667a\u80fd\u4f53\u5b9e\u73b0\u591a\u6b65\u9aa4\u540e\u95e8\u63a7\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u653b\u51fb\u6210\u529f\u7387\u7684\u540c\u65f6\u589e\u5f3a\u667a\u80fd\u4f53\u5728\u826f\u6027\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u5b89\u5168\u6027\u7684\u6f5c\u5728\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5feb\u901f\u90e8\u7f72\uff0c\u5176\u53ef\u4fe1\u8d56\u6027\u5f15\u53d1\u4e25\u91cd\u62c5\u5fe7\u3002\u4f20\u7edf\u540e\u95e8\u653b\u51fb\u4ec5\u9650\u4e8e\u5355\u6b65\u63a7\u5236\uff0c\u65e0\u6cd5\u5e94\u5bf9\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u591a\u6b65\u9aa4\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5b58\u5728\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u6f0f\u6d1e\u9700\u8981\u6df1\u5165\u63a2\u7d22\u3002", "method": "\u63d0\u51faChain-of-Trigger Backdoor (CoTri)\u591a\u6b65\u9aa4\u540e\u95e8\u653b\u51fb\u6846\u67b6\uff0c\u91c7\u7528\u6709\u5e8f\u89e6\u53d1\u5e8f\u5217\u7b56\u7565\uff0c\u521d\u59cb\u89e6\u53d1\u540e\u4ece\u73af\u5883\u4e2d\u63d0\u53d6\u540e\u7eed\u89e6\u53d1\u6761\u4ef6\uff0c\u5b9e\u73b0\u5bf9\u667a\u80fd\u4f53\u7684\u591a\u6b65\u9aa4\u64cd\u63a7\uff0c\u4f7f\u5176\u504f\u79bb\u539f\u59cb\u4efb\u52a1\u76ee\u6807\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u5efa\u6a21\u73af\u5883\u7684\u968f\u673a\u6027\u7279\u6027\u6765\u589e\u5f3a\u653b\u51fb\u7684\u9690\u853d\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aCoTri\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u653b\u51fb\u6210\u529f\u7387(ASR)\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u96f6\u7684\u8bef\u89e6\u53d1\u7387(FTR)\u3002\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u5bf9\u73af\u5883\u968f\u673a\u6027\u7684\u5efa\u6a21\uff0cCoTri\u690d\u5165\u53cd\u800c\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u826f\u6027\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u589e\u5f3a\u4e86\u5176\u5bf9\u6297\u73af\u5883\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3002\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u9a8c\u8bc1\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5bf9\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "CoTri\u5b9e\u73b0\u4e86\u667a\u80fd\u4f53\u5185\u7a33\u5b9a\u7684\u591a\u6b65\u9aa4\u63a7\u5236\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u5176\u56fa\u6709\u9c81\u68d2\u6027\u548c\u4efb\u52a1\u80fd\u529b\uff0c\u8fd9\u4f7f\u5f97\u653b\u51fb\u66f4\u52a0\u9690\u853d\u5e76\u5e26\u6765\u6f5c\u5728\u5b89\u5168\u98ce\u9669\u3002\u7814\u7a76\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u5b89\u5168\u6027\u7684\u6df1\u5c42\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u5728\u90e8\u7f72\u524d\u8fdb\u884c\u4e25\u683c\u5b89\u5168\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u672a\u6765\u9632\u5fa1\u673a\u5236\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.08003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08003", "abs": "https://arxiv.org/abs/2510.08003", "authors": ["Weihuang Lin", "Yiwei Ma", "Jiayi Ji", "Xiaoshuai Sun", "Rongrong Ji"], "title": "CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning", "comment": null, "summary": "Composed Image Retrieval (CIR), which aims to find a target image from a\nreference image and a modification text, presents the core challenge of\nperforming unified reasoning across visual and semantic modalities. While\ncurrent approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more\nrecent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown\nprogress, they predominantly function as ``black boxes.\" This inherent opacity\nnot only prevents users from understanding the retrieval rationale but also\nrestricts the models' ability to follow complex, fine-grained instructions. To\novercome these limitations, we introduce CIR-CoT, the first end-to-end\nretrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT)\nreasoning. By compelling the model to first generate an interpretable reasoning\nchain, CIR-CoT enhances its ability to capture crucial cross-modal\ninteractions, leading to more accurate retrieval while making its decision\nprocess transparent. Since existing datasets like FashionIQ and CIRR lack the\nnecessary reasoning data, a key contribution of our work is the creation of\nstructured CoT annotations using a three-stage process involving a caption,\nreasoning, and conclusion. Our model is then fine-tuned to produce this\nstructured output before encoding its final retrieval intent into a dedicated\nembedding. Comprehensive experiments show that CIR-CoT achieves highly\ncompetitive performance on in-domain datasets (FashionIQ, CIRR) and\ndemonstrates remarkable generalization on the out-of-domain CIRCO dataset,\nestablishing a new path toward more effective and trustworthy retrieval\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CIR-CoT\uff0c\u8fd9\u662f\u9996\u4e2a\u96c6\u6210\u663e\u5f0f\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u7aef\u5230\u7aef\u68c0\u7d22\u5bfc\u5411\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\u6765\u589e\u5f3a\u8de8\u6a21\u6001\u4ea4\u4e92\u7406\u89e3\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u68c0\u7d22\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u51b3\u7b56\u8fc7\u7a0b\u900f\u660e\u5316\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u4e3b\u8981\u4f5c\u4e3a\u9ed1\u7bb1\u8fd0\u884c\uff0c\u8fd9\u79cd\u4e0d\u900f\u660e\u6027\u4e0d\u4ec5\u963b\u788d\u7528\u6237\u7406\u89e3\u68c0\u7d22\u539f\u7406\uff0c\u8fd8\u9650\u5236\u4e86\u6a21\u578b\u9075\u5faa\u590d\u6742\u7ec6\u7c92\u5ea6\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u53ef\u89e3\u91ca\u6027\u548c\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "CIR-CoT\u901a\u8fc7\u5f3a\u5236\u6a21\u578b\u9996\u5148\u751f\u6210\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\u6765\u589e\u5f3a\u8de8\u6a21\u6001\u4ea4\u4e92\u6355\u83b7\u80fd\u529b\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u6807\u6ce8\u6d41\u7a0b\u521b\u5efa\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u6ce8\u91ca\uff0c\u5305\u62ec\u63cf\u8ff0\u3001\u63a8\u7406\u548c\u7ed3\u8bba\uff0c\u7136\u540e\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4ee5\u4ea7\u751f\u8fd9\u79cd\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u6700\u540e\u5c06\u5176\u68c0\u7d22\u610f\u56fe\u7f16\u7801\u5230\u4e13\u7528\u5d4c\u5165\u4e2d\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cCIR-CoT\u5728\u9886\u57df\u5185\u6570\u636e\u96c6\uff08FashionIQ\u3001CIRR\uff09\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u9886\u57df\u5916CIRCO\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u66f4\u6709\u6548\u548c\u53ef\u4fe1\u7684\u68c0\u7d22\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002", "conclusion": "\u8be5\u7814\u7a76\u786e\u7acb\u4e86\u901a\u8fc7\u663e\u5f0f\u601d\u7ef4\u94fe\u63a8\u7406\u589e\u5f3a\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u7684\u65b0\u8303\u5f0f\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u66f4\u91cd\u8981\u7684\u662f\u5b9e\u73b0\u4e86\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u900f\u660e\u548c\u53ef\u4fe1\u7684\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2510.08152", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08152", "abs": "https://arxiv.org/abs/2510.08152", "authors": ["Elena Khasanova", "Harsh Saini", "Md Tahmid Rahman Laskar", "Xue-Yong Fu", "Cheng Chen", "Shashi Bhushan TN"], "title": "DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations", "comment": "Accepted to the EMNLP 2025 Industry Track. Equal contribution from\n  the first four authors", "summary": "The rapid advancements in Large Language Models (LLMs) have enabled their\nadoption in real-world industrial scenarios for various natural language\nprocessing tasks. However, the high inference cost of large-scale LLMs makes\ntheir deployment impractical, necessitating the use of smaller models. Despite\ntheir efficiency, smaller LLMs lack robust zero-shot instruction-following\ncapabilities across diverse domains, limiting their adaptability to dynamic\nuser requirements. Traditional fine-tuning approaches exacerbate this issue by\ninducing catastrophic forgetting, reducing the model's generalization ability\nfor unseen tasks. In this paper, we propose Domain Adaptive Continual\nInstruction Pre-Training via Reading Comprehension (DACIP-RC), a continual\npre-training technique that enhances smaller LLMs' domain adaptability for\nbusiness conversational tasks. Unlike conventional pre-training approaches that\nrely on next-token prediction, DACIP-RC generates diverse task instructions and\nresponses via reading comprehension on conversation transcripts, enabling\nbetter instruction generalization. Our empirical evaluations demonstrate that\nDACIP-RC significantly improves zero-shot generalization across a wide range of\nbusiness conversational tasks, including meeting summarization, action item\ngeneration, and call purpose identification. To the best of our knowledge, this\nis the first work to apply instruction pre-training on business conversational\ndata, providing insights into how industries can leverage proprietary datasets\nfor domain adaptation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDACIP-RC\u65b9\u6cd5\uff0c\u901a\u8fc7\u9605\u8bfb\u7406\u89e3\u5f0f\u6301\u7eed\u9884\u8bad\u7ec3\u589e\u5f3a\u5c0f\u578bLLM\u5728\u5546\u4e1a\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u9886\u57df\u9002\u5e94\u6027\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5fae\u8c03\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21LLM\u7684\u9ad8\u63a8\u7406\u6210\u672c\u4f7f\u5176\u5728\u5de5\u4e1a\u90e8\u7f72\u4e2d\u4e0d\u5207\u5b9e\u9645\uff0c\u800c\u5c0f\u578bLLM\u7f3a\u4e4f\u8de8\u9886\u57df\u7684\u96f6\u6837\u672c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u4f1a\u5f15\u53d1\u707e\u96be\u6027\u9057\u5fd8\u5e76\u964d\u4f4e\u6a21\u578b\u5bf9\u672a\u89c1\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u9886\u57df\u81ea\u9002\u5e94\u6301\u7eed\u6307\u4ee4\u9884\u8bad\u7ec3\u65b9\u6cd5DACIP-RC\uff0c\u901a\u8fc7\u9605\u8bfb\u7406\u89e3\u5bf9\u8bdd\u8bb0\u5f55\u751f\u6210\u591a\u6837\u5316\u4efb\u52a1\u6307\u4ee4\u548c\u54cd\u5e94\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\u9884\u8bad\u7ec3\u65b9\u5f0f\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6307\u4ee4\u6cdb\u5316\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660eDACIP-RC\u5728\u591a\u79cd\u5546\u4e1a\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u4f1a\u8bae\u6458\u8981\u3001\u884c\u52a8\u9879\u751f\u6210\u548c\u901a\u8bdd\u76ee\u7684\u8bc6\u522b\u7b49\u4efb\u52a1\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u5546\u4e1a\u5bf9\u8bdd\u6570\u636e\u4e0a\u5e94\u7528\u6307\u4ee4\u9884\u8bad\u7ec3\u7684\u5de5\u4f5c\uff0c\u4e3a\u884c\u4e1a\u5982\u4f55\u5229\u7528\u4e13\u6709\u6570\u636e\u96c6\u8fdb\u884c\u9886\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u5b9e\u8df5\u89c1\u89e3\uff0c\u8bc1\u660e\u4e86\u6301\u7eed\u9884\u8bad\u7ec3\u5728\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.08470", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08470", "abs": "https://arxiv.org/abs/2510.08470", "authors": ["Bianca-Mihaela Ganescu", "Suchir Salhan", "Andrew Caines", "Paula Buttery"], "title": "Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling", "comment": "Accepted to the EMNLP 2025 BabyLM Workshop", "summary": "Training vision-language models on cognitively-plausible amounts of data\nrequires rethinking how models integrate multimodal information. Within the\nconstraints of the Vision track for the BabyLM Challenge 2025, we propose a\nlightweight decoder-based architecture with (1) token-wise dynamic gating for\nadaptive fusion of linguistic and visual cues, (2) feature modulation and\nchannel attention to maximise the utility of limited visual information and (3)\nauxiliary contrastive objectives for visual grounding. Evaluation on five\nbenchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows\ncompetitive or superior performance to multimodal baselines. More notably, our\ndynamic gate discovers interpretable patterns without explicit supervision,\nfavouring visual cues for content words and linguistic cues for function words.\nWhile we identify limitations in the Challenge constraints, such as the\ninformation bottleneck created by global image embeddings and training\ninstability from the dataset split, our findings establish dynamic gating as a\npowerful tool for efficient multimodal learning, offering both interpretability\nand performance even under severe constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u95e8\u63a7\u673a\u5236\u5b9e\u73b0\u8bed\u8a00\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u81ea\u9002\u5e94\u878d\u5408\uff0c\u5728\u8ba4\u77e5\u5408\u7406\u7684\u6570\u636e\u91cf\u7ea6\u675f\u4e0b\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u591a\u6a21\u6001\u5b66\u4e60\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u52a8\u6001\u95e8\u63a7\u673a\u5236\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u5373\u53ef\u53d1\u73b0\u53ef\u89e3\u91ca\u7684\u6a21\u5f0f\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5728\u8ba4\u77e5\u5408\u7406\u6570\u636e\u91cf\u7ea6\u675f\u4e0b\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5373\u5728\u6709\u9650\u7684\u591a\u6a21\u6001\u4fe1\u606f\u4e0b\u91cd\u65b0\u601d\u8003\u6a21\u578b\u5982\u4f55\u6709\u6548\u6574\u5408\u8bed\u8a00\u548c\u89c6\u89c9\u7ebf\u7d22\u3002BabyLM Challenge 2025\u7684\u89c6\u89c9\u8d5b\u9053\u7ea6\u675f\u6761\u4ef6\u4fc3\u4f7f\u7814\u7a76\u8005\u5f00\u53d1\u80fd\u591f\u5728\u4e25\u91cd\u6570\u636e\u9650\u5236\u4e0b\u4ecd\u80fd\u9ad8\u6548\u5b66\u4e60\u7684\u591a\u6a21\u6001\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a\u4ee4\u724c\u7ea7\u52a8\u6001\u95e8\u63a7\u7528\u4e8e\u8bed\u8a00\u548c\u89c6\u89c9\u7ebf\u7d22\u7684\u81ea\u9002\u5e94\u878d\u5408\uff1b\u7279\u5f81\u8c03\u5236\u548c\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u6700\u5927\u5316\u6709\u9650\u89c6\u89c9\u4fe1\u606f\u7684\u6548\u7528\uff1b\u8f85\u52a9\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u7528\u4e8e\u89c6\u89c9\u57fa\u7840\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u5171\u540c\u4f5c\u7528\u5728\u4e25\u683c\u7684\u6570\u636e\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u591a\u6a21\u6001\u5b66\u4e60\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08BLiMP\u3001BLiMP Supplement\u3001EWoK\u3001Winoground\u548cVQA\uff09\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u57fa\u7ebf\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u6027\u80fd\u3002\u52a8\u6001\u95e8\u63a7\u673a\u5236\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u5373\u53ef\u53d1\u73b0\u53ef\u89e3\u91ca\u6a21\u5f0f\uff1a\u5bf9\u5185\u5bb9\u8bcd\u504f\u597d\u89c6\u89c9\u7ebf\u7d22\uff0c\u5bf9\u529f\u80fd\u8bcd\u504f\u597d\u8bed\u8a00\u7ebf\u7d22\u3002\u540c\u65f6\u8bc6\u522b\u4e86\u6311\u6218\u7ea6\u675f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5982\u5168\u5c40\u56fe\u50cf\u5d4c\u5165\u9020\u6210\u7684\u4fe1\u606f\u74f6\u9888\u548c\u6570\u636e\u96c6\u5206\u5272\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u786e\u7acb\u4e86\u52a8\u6001\u95e8\u63a7\u4f5c\u4e3a\u9ad8\u6548\u591a\u6a21\u6001\u5b66\u4e60\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u5373\u4f7f\u5728\u4e25\u683c\u7ea6\u675f\u4e0b\u4e5f\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002\u52a8\u6001\u95e8\u63a7\u7684\u81ea\u9002\u5e94\u7279\u6027\u4e3a\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5176\u53d1\u73b0\u7684\u6a21\u5f0f\u4e0e\u8ba4\u77e5\u8bed\u8a00\u5b66\u539f\u7406\u4e00\u81f4\uff0c\u4e3a\u672a\u6765\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u591a\u6a21\u6001\u5b66\u4e60\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.08094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08094", "abs": "https://arxiv.org/abs/2510.08094", "authors": ["Ziqi Zhou", "Menghao Deng", "Yufei Song", "Hangtao Zhang", "Wei Wan", "Shengshan Hu", "Minghui Li", "Leo Yu Zhang", "Dezhong Yao"], "title": "DarkHash: A Data-Free Backdoor Attack Against Deep Hashing", "comment": "Accepted by TIFS 2025", "summary": "Benefiting from its superior feature learning capabilities and efficiency,\ndeep hashing has achieved remarkable success in large-scale image retrieval.\nRecent studies have demonstrated the vulnerability of deep hashing models to\nbackdoor attacks. Although these studies have shown promising attack results,\nthey rely on access to the training dataset to implant the backdoor. In the\nreal world, obtaining such data (e.g., identity information) is often\nprohibited due to privacy protection and intellectual property concerns.\nEmbedding backdoors into deep hashing models without access to the training\ndata, while maintaining retrieval accuracy for the original task, presents a\nnovel and challenging problem. In this paper, we propose DarkHash, the first\ndata-free backdoor attack against deep hashing. Specifically, we design a novel\nshadow backdoor attack framework with dual-semantic guidance. It embeds\nbackdoor functionality and maintains original retrieval accuracy by fine-tuning\nonly specific layers of the victim model using a surrogate dataset. We consider\nleveraging the relationship between individual samples and their neighbors to\nenhance backdoor attacks during training. By designing a topological alignment\nloss, we optimize both individual and neighboring poisoned samples toward the\ntarget sample, further enhancing the attack capability. Experimental results on\nfour image datasets, five model architectures, and two hashing methods\ndemonstrate the high effectiveness of DarkHash, outperforming existing\nstate-of-the-art backdoor attack methods. Defense experiments show that\nDarkHash can withstand existing mainstream backdoor defense methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DarkHash\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u6df1\u5ea6\u54c8\u5e0c\u6a21\u578b\u7684\u65e0\u6570\u636e\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5177\u6709\u53cc\u8bed\u4e49\u6307\u5bfc\u7684\u5f71\u5b50\u540e\u95e8\u653b\u51fb\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u4ee3\u7406\u6570\u636e\u96c6\u5fae\u8c03\u7279\u5b9a\u5c42\u5373\u53ef\u5728\u4fdd\u6301\u539f\u59cb\u68c0\u7d22\u7cbe\u5ea6\u7684\u540c\u65f6\u5d4c\u5165\u540e\u95e8\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u54c8\u5e0c\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u9700\u8981\u8bbf\u95ee\u8bad\u7ec3\u6570\u636e\u96c6\u6765\u690d\u5165\u540e\u95e8\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7531\u4e8e\u9690\u79c1\u4fdd\u62a4\u548c\u77e5\u8bc6\u4ea7\u6743\u8003\u8651\uff0c\u83b7\u53d6\u6b64\u7c7b\u6570\u636e\u5f80\u5f80\u88ab\u7981\u6b62\uff0c\u56e0\u6b64\u5f00\u53d1\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5d4c\u5165\u540e\u95e8\u5e76\u4fdd\u6301\u539f\u59cb\u68c0\u7d22\u7cbe\u5ea6\u7684\u653b\u51fb\u65b9\u6cd5\u6210\u4e3a\u4e00\u4e2a\u65b0\u9896\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f71\u5b50\u540e\u95e8\u653b\u51fb\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u8bed\u4e49\u6307\u5bfc\u673a\u5236\uff0c\u901a\u8fc7\u4ec5\u5fae\u8c03\u53d7\u5bb3\u8005\u6a21\u578b\u7684\u7279\u5b9a\u5c42\u5e76\u4f7f\u7528\u4ee3\u7406\u6570\u636e\u96c6\u6765\u5d4c\u5165\u540e\u95e8\u529f\u80fd\uff0c\u540c\u65f6\u8bbe\u8ba1\u4e86\u62d3\u6251\u5bf9\u9f50\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u6837\u672c\u4e0e\u5176\u90bb\u5c45\u4e4b\u95f4\u7684\u5173\u7cfb\u6765\u4f18\u5316\u4e2a\u4f53\u548c\u76f8\u90bb\u4e2d\u6bd2\u6837\u672c\u671d\u5411\u76ee\u6807\u6837\u672c\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u653b\u51fb\u80fd\u529b\u3002", "result": "\u5728\u56db\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\u3001\u4e94\u79cd\u6a21\u578b\u67b6\u6784\u548c\u4e24\u79cd\u54c8\u5e0c\u65b9\u6cd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDarkHash\u5177\u6709\u9ad8\u5ea6\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u4e14\u9632\u5fa1\u5b9e\u9a8c\u663e\u793aDarkHash\u80fd\u591f\u62b5\u5fa1\u73b0\u6709\u7684\u4e3b\u6d41\u540e\u95e8\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6df1\u5ea6\u54c8\u5e0c\u6a21\u578b\u5728\u65e0\u6570\u636e\u573a\u666f\u4e0b\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u540e\u95e8\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u6709\u6548\u7684\u65e0\u6570\u636e\u540e\u95e8\u653b\u51fb\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6df1\u5ea6\u54c8\u5e0c\u6a21\u578b\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u4e3a\u672a\u6765\u9632\u5fa1\u673a\u5236\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.08163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08163", "abs": "https://arxiv.org/abs/2510.08163", "authors": ["Jian Xie", "Zhendong Chu", "Aoxiao Zhong", "Kai Zhang", "Mingzhe Han", "Xin Fang", "Jialie Shen", "Qingsong Wen"], "title": "ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code", "comment": "Work in Progress", "summary": "Large Reasoning Models (LRMs) often suffer from the ``over-thinking''\nproblem, generating unnecessarily long reasoning on simple tasks. Some\nstrategies have been proposed to mitigate this issue, such as length penalties\nor routing mechanisms, but they are typically heuristic and task-specific,\nlacking a general framework for adaptive reasoning. In this paper, we present\nARM2, a unified model that adaptively balances reasoning performance and\nefficiency across multiple formats through a reinforcement learning framework\naugmented with length-aware optimization. Beyond conventional natural language\ninference, ARM2 integrates vision understanding, extending its applicability to\nmultimodal. Moreover, ARM2 integrates executable code into reasoning, enabling\nsubstantial reductions in token cost while preserving task performance compared\nto long CoT. Experiments demonstrate that ARM2 achieves performance on par with\ntraditional reasoning models trained with GRPO, while reducing token usage by\nover 70% on average. We further conduct extensive analyses to validate the\neffectiveness of ARM2 and the soundness of its design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ARM2\uff0c\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u957f\u5ea6\u611f\u77e5\u4f18\u5316\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5e73\u8861\u63a8\u7406\u6027\u80fd\u4e0e\u6548\u7387\u3002\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u4e0e\u4f20\u7edf\u63a8\u7406\u6a21\u578b\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5e73\u5747\u51cf\u5c11\u8d85\u8fc770%\u7684token\u4f7f\u7528\u91cf\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u666e\u904d\u5b58\u5728\"\u8fc7\u5ea6\u601d\u8003\"\u95ee\u9898\uff0c\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u751f\u6210\u4e0d\u5fc5\u8981\u7684\u5197\u957f\u63a8\u7406\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5982\u957f\u5ea6\u60e9\u7f5a\u6216\u8def\u7531\u673a\u5236\u901a\u5e38\u662f\u542f\u53d1\u5f0f\u4e14\u4efb\u52a1\u7279\u5b9a\u7684\uff0c\u7f3a\u4e4f\u81ea\u9002\u5e94\u63a8\u7406\u7684\u901a\u7528\u6846\u67b6\u3002", "method": "ARM2\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u957f\u5ea6\u611f\u77e5\u4f18\u5316\uff0c\u7edf\u4e00\u5e73\u8861\u591a\u79cd\u683c\u5f0f\u7684\u63a8\u7406\u6027\u80fd\u4e0e\u6548\u7387\u3002\u8be5\u6a21\u578b\u4e0d\u4ec5\u652f\u6301\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff0c\u8fd8\u6574\u5408\u4e86\u89c6\u89c9\u7406\u89e3\u548c\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u6269\u5c55\u4e86\u591a\u6a21\u6001\u5e94\u7528\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eARM2\u5728\u6027\u80fd\u4e0a\u4e0e\u4f7f\u7528GRPO\u8bad\u7ec3\u7684\u4f20\u7edf\u63a8\u7406\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u5e73\u5747\u51cf\u5c11\u8d85\u8fc770%\u7684token\u4f7f\u7528\u91cf\u3002\u5e7f\u6cdb\u7684\u6d88\u878d\u5206\u6790\u9a8c\u8bc1\u4e86ARM2\u7684\u6709\u6548\u6027\u548c\u8bbe\u8ba1\u5408\u7406\u6027\u3002", "conclusion": "ARM2\u4e3a\u81ea\u9002\u5e94\u63a8\u7406\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6574\u5408\u4ee3\u7801\u6267\u884c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002\u8be5\u7814\u7a76\u4e3a\u9ad8\u6548\u591a\u6a21\u6001\u63a8\u7406\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2510.08564", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08564", "abs": "https://arxiv.org/abs/2510.08564", "authors": ["Zhen Zhu", "Yiming Gong", "Yao Xiao", "Yaoyao Liu", "Derek Hoiem"], "title": "How to Teach Large Multimodal Models New Skills", "comment": "In submission. Code is available at\n  https://github.com/jessemelpolio/LMM_CL", "summary": "How can we teach large multimodal models (LMMs) new skills without erasing\nprior abilities? We study sequential fine-tuning on five target skills while\nmonitoring general ability on eight held-out benchmarks across three model\nfamilies. We observe that apparent \"forgetting\" on held-out tasks after narrow\nfine-tuning can partly recover at later stages. We trace this behavior to a\nmeasurable shift in the output token distribution, manifested through a simple\ncounting-bias probe that co-varies with forgetting. Guided by this picture, we\nidentify two simple, robust tuning recipes that learn strongly while limiting\ndrift: (i) updating only the self-attention projection layers, and (ii)\nupdating only the MLP Gate&Up while freezing the Down projection. Across models\nand tasks, these choices deliver strong target gains while largely preserving\nheld-out performance. Code is available at\nhttps://github.com/jessemelpolio/LMM_CL", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u7b80\u5355\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u66f4\u65b0\u81ea\u6ce8\u610f\u529b\u6295\u5f71\u5c42\u6216\u4ec5\u66f4\u65b0MLP Gate&Up\u6295\u5f71\u5c42\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u987a\u5e8f\u5fae\u8c03\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u76ee\u6807\u6280\u80fd\u5b66\u4e60\u4e0e\u901a\u7528\u80fd\u529b\u4fdd\u6301\u7684\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u987a\u5e8f\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5373\u5b66\u4e60\u65b0\u6280\u80fd\u65f6\u53ef\u80fd\u64e6\u9664\u5148\u524d\u83b7\u5f97\u7684\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u7b80\u5355\u800c\u9c81\u68d2\u7684\u5fae\u8c03\u65b9\u6cd5\uff1a\u4e00\u662f\u4ec5\u66f4\u65b0\u81ea\u6ce8\u610f\u529b\u6295\u5f71\u5c42\uff0c\u4e8c\u662f\u4ec5\u66f4\u65b0MLP\u4e2d\u7684Gate&Up\u6295\u5f71\u5c42\u540c\u65f6\u51bb\u7ed3Down\u6295\u5f71\u5c42\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u9650\u5236\u53c2\u6570\u66f4\u65b0\u8303\u56f4\u6765\u51cf\u5c11\u8f93\u51fa\u5206\u5e03\u7684\u6f02\u79fb\u3002", "result": "\u5b9e\u9a8c\u5728\u4e09\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e0a\u8fdb\u884c\uff0c\u6db5\u76d6\u4e94\u4e2a\u76ee\u6807\u6280\u80fd\u548c\u516b\u4e2a\u4fdd\u6301\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u6240\u63d0\u51fa\u7684\u5fae\u8c03\u65b9\u6cd5\u5728\u83b7\u5f97\u5f3a\u5927\u76ee\u6807\u6280\u80fd\u63d0\u5347\u7684\u540c\u65f6\uff0c\u663e\u8457\u4fdd\u6301\u4e86\u4fdd\u6301\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u9057\u5fd8\u73b0\u8c61\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u9057\u5fd8\u884c\u4e3a\u4e0e\u8f93\u51fa\u6807\u8bb0\u5206\u5e03\u6f02\u79fb\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5e76\u8bc1\u660e\u4e86\u901a\u8fc7\u9009\u62e9\u6027\u53c2\u6570\u66f4\u65b0\u7b56\u7565\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u6301\u7eed\u5b66\u4e60\uff0c\u4e3a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u987a\u5e8f\u5fae\u8c03\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08138", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.08138", "abs": "https://arxiv.org/abs/2510.08138", "authors": ["Chengzhi Li", "Heyan Huang", "Ping Jian", "Zhen Yang", "Yaning Tian"], "title": "Improving Temporal Understanding Logic Consistency in Video-Language Models via Attention Enhancement", "comment": null, "summary": "Large language models (LLMs) often generate self-contradictory outputs, which\nseverely impacts their reliability and hinders their adoption in practical\napplications. In video-language models (Video-LLMs), this phenomenon recently\ndraws the attention of researchers. Specifically, these models fail to provide\nlogically consistent responses to rephrased questions based on their grounding\noutputs. However, the underlying causes of this phenomenon remain\nunderexplored. In this work, we adopt an interpretability-driven approach to\nanalyze, statistically summarize, and intervention the potential factors of the\nphenomenon. We find that one of the primary reasons for the inconsistency in\nresponses lies in the inability of cross-modal attention heads to effectively\ndistinguish video tokens across different timestamps. To address this, we\npropose an attention enhancement method called Temporally Conditioned Attention\nSharpening (TCAS), which constructs an enhancement objective based on attention\ndistinctions to enhance the model's temporal resolution capability, thereby\nimproving its temporal understanding logic consistency. Experimental results\ndemonstrate that our method significantly enhances the temporal logic\nconsistency of Video-LLMs. Further interpretability analyses reveal that our\nmethod indeed improves the temporal discriminability of attention heads,\nvalidating our conclusions. Additionally, our method achieves performance\nimprovements in general video temporal grounding tasks, highlighting that\ntemporal logic consistency is a bottleneck in temporal understanding. By\nenhancing consistency, our method drives significant progress in video temporal\nunderstanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u65f6\u95f4\u6761\u4ef6\u6ce8\u610f\u529b\u9510\u5316\uff08TCAS\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5934\u7684\u65f6\u95f4\u5206\u8fa8\u80fd\u529b\u6765\u89e3\u51b3\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u65f6\u95f4\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u65f6\u95f4\u7406\u89e3\u4e00\u81f4\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u8f93\u51fa\u81ea\u76f8\u77db\u76fe\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5bf9\u57fa\u4e8e\u5176\u57fa\u7840\u8f93\u51fa\u7684\u91cd\u8ff0\u95ee\u9898\u65f6\u65e0\u6cd5\u63d0\u4f9b\u903b\u8f91\u4e00\u81f4\u7684\u54cd\u5e94\uff0c\u8fd9\u79cd\u73b0\u8c61\u4e25\u91cd\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u5e76\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u4f46\u5176\u6839\u672c\u539f\u56e0\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u53ef\u89e3\u91ca\u6027\u9a71\u52a8\u7684\u65b9\u6cd5\u5206\u6790\u3001\u7edf\u8ba1\u603b\u7ed3\u5e76\u5e72\u9884\u8be5\u73b0\u8c61\u7684\u53ef\u80fd\u56e0\u7d20\uff0c\u53d1\u73b0\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5934\u65e0\u6cd5\u6709\u6548\u533a\u5206\u4e0d\u540c\u65f6\u95f4\u6233\u7684\u89c6\u9891\u6807\u8bb0\u662f\u54cd\u5e94\u4e0d\u4e00\u81f4\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4e3a\u6b64\u63d0\u51fa\u4e86\u65f6\u95f4\u6761\u4ef6\u6ce8\u610f\u529b\u9510\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u57fa\u4e8e\u6ce8\u610f\u529b\u5dee\u5f02\u7684\u589e\u5f3a\u76ee\u6807\u6765\u63d0\u5347\u6a21\u578b\u7684\u65f6\u95f4\u5206\u8fa8\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u786e\u5b9e\u6539\u5584\u4e86\u6ce8\u610f\u529b\u5934\u7684\u65f6\u95f4\u533a\u5206\u80fd\u529b\uff0c\u540c\u65f6\u5728\u901a\u7528\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u8868\u660e\u65f6\u95f4\u903b\u8f91\u4e00\u81f4\u6027\u662f\u65f6\u95f4\u7406\u89e3\u7684\u74f6\u9888\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u65f6\u95f4\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u8be5\u65b9\u6cd5\u63a8\u52a8\u4e86\u89c6\u9891\u65f6\u95f4\u7406\u89e3\u7684\u663e\u8457\u8fdb\u5c55\uff0c\u8bc1\u660e\u4e86\u65f6\u95f4\u5206\u8fa8\u80fd\u529b\u662f\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e00\u81f4\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u6539\u8fdb\u591a\u6a21\u6001\u6a21\u578b\u7684\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.08404", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08404", "abs": "https://arxiv.org/abs/2510.08404", "authors": ["Noor Ul Zain", "Mohsin Raza", "Ahsan Adeel"], "title": "Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT", "comment": null, "summary": "We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two\nheads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$\nis the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2\n(124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two\nepochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude\ngreater training efficiency on 10M tokens, demonstrating highly sample\nefficient pretraining. Using the BabyLM challenge evaluation pipeline across\ncomplex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning\nperformance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out\nof 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out\nof 7 metrics in both cases. These results suggest the need to rethink\nprevailing deep learning paradigms and associated scaling laws.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCo\u2074\u7684\u6781\u5c0f\u6a21\u578b\uff0c\u4ec5\u5305\u542b\u5355\u5c42\u3001\u53cc\u5934\u548c8M\u53c2\u6570\uff0c\u4ee5\u8fd1\u4f3cO(N)\u7684\u8ba1\u7b97\u6210\u672c\u5728BabyLM\u6311\u6218\u4e2d\u8d85\u8d8a\u4e86GPT-2\u548cGPT-BERT\u57fa\u7ebf\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u6781\u9ad8\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\u666e\u904d\u4f9d\u8d56\u6df1\u5c42\u7f51\u7edc\u548cO(N\u00b2)\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6837\u672c\u6548\u7387\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528Co\u2074\u673a\u5668\u67b6\u6784\uff0c\u4ec5\u5305\u542b\u5355\u5c42transformer\u3001\u4e24\u4e2a\u6ce8\u610f\u529b\u5934\u548c800\u4e07\u53c2\u6570\uff0c\u901a\u8fc7\u8fd1\u4f3cO(N)\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\uff0c\u76f8\u6bd4\u4f20\u7edfO(N\u00b2)\u590d\u6742\u5ea6\u6a21\u578b\u5177\u6709\u663e\u8457\u8ba1\u7b97\u4f18\u52bf\u3002", "result": "\u5728\u4ec5\u8bad\u7ec3\u4e24\u4e2a\u5468\u671f\u540e\uff0cCo\u2074\u572810M tokens\u4e0a\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u66f4\u9ad8\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u5728BabyLM\u8bc4\u4f30\u4e2d\u96f6\u6837\u672c\u6027\u80fd\u8d85\u8d8aGPT-2\u76845/7\u6307\u6807\u548cGPT-BERT\u76844/7\u6307\u6807\uff0c\u5fae\u8c03\u6027\u80fd\u8d85\u8d8aGPT-2\u76846/7\u4efb\u52a1\u548cGPT-BERT\u76844/7\u6307\u6807\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u9700\u8981\u91cd\u65b0\u601d\u8003\u5f53\u524d\u4e3b\u6d41\u7684\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\u548c\u6269\u5c55\u5b9a\u5f8b\uff0c\u6781\u5c0f\u6a21\u578b\u901a\u8fc7\u9ad8\u6548\u67b6\u6784\u8bbe\u8ba1\u53ef\u4ee5\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548AI\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.08143", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08143", "abs": "https://arxiv.org/abs/2510.08143", "authors": ["Shian Du", "Menghan Xia", "Chang Liu", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Xiangyang Ji"], "title": "UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution", "comment": null, "summary": "Cascaded video super-resolution has emerged as a promising technique for\ndecoupling the computational burden associated with generating high-resolution\nvideos using large foundation models. Existing studies, however, are largely\nconfined to text-to-video tasks and fail to leverage additional generative\nconditions beyond text, which are crucial for ensuring fidelity in multi-modal\nvideo generation. We address this limitation by presenting UniMMVSR, the first\nunified generative video super-resolution framework to incorporate hybrid-modal\nconditions, including text, images, and videos. We conduct a comprehensive\nexploration of condition injection strategies, training schemes, and data\nmixture techniques within a latent video diffusion model. A key challenge was\ndesigning distinct data construction and condition utilization methods to\nenable the model to precisely utilize all condition types, given their varied\ncorrelations with the target video. Our experiments demonstrate that UniMMVSR\nsignificantly outperforms existing methods, producing videos with superior\ndetail and a higher degree of conformity to multi-modal conditions. We also\nvalidate the feasibility of combining UniMMVSR with a base model to achieve\nmulti-modal guided generation of 4K video, a feat previously unattainable with\nexisting techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UniMMVSR\uff0c\u8fd9\u662f\u9996\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u751f\u6210\u5f0f\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u80fd\u591f\u6574\u5408\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u7b49\u591a\u79cd\u751f\u6210\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u6761\u4ef6\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7ea7\u8054\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u6587\u672c\u5230\u89c6\u9891\u4efb\u52a1\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6587\u672c\u4e4b\u5916\u7684\u591a\u6a21\u6001\u751f\u6210\u6761\u4ef6\uff0c\u8fd9\u5728\u786e\u4fdd\u591a\u6a21\u6001\u89c6\u9891\u751f\u6210\u4fdd\u771f\u5ea6\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86UniMMVSR\u7edf\u4e00\u6846\u67b6\uff0c\u5728\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u7cfb\u7edf\u63a2\u7d22\u4e86\u6761\u4ef6\u6ce8\u5165\u7b56\u7565\u3001\u8bad\u7ec3\u65b9\u6848\u548c\u6570\u636e\u6df7\u5408\u6280\u672f\uff0c\u9488\u5bf9\u4e0d\u540c\u6761\u4ef6\u7c7b\u578b\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u6570\u636e\u6784\u5efa\u548c\u6761\u4ef6\u5229\u7528\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eUniMMVSR\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u89c6\u9891\u5177\u6709\u66f4\u4e30\u5bcc\u7684\u7ec6\u8282\u548c\u66f4\u9ad8\u7684\u591a\u6a21\u6001\u6761\u4ef6\u4e00\u81f4\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4e0e\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u5b9e\u73b04K\u89c6\u9891\u591a\u6a21\u6001\u5f15\u5bfc\u751f\u6210\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u6761\u4ef6\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u65b9\u6cd5\u57284K\u89c6\u9891\u751f\u6210\u65b9\u9762\u7684\u9650\u5236\u3002"}}
{"id": "2510.08457", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08457", "abs": "https://arxiv.org/abs/2510.08457", "authors": ["Shuang Chen", "Yue Guo", "Yimeng Ye", "Shijue Huang", "Wenbo Hu", "Haoxi Li", "Manyuan Zhang", "Jiayu Chen", "Song Guo", "Nanyun Peng"], "title": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping", "comment": null, "summary": "Recent advances in multimodal large reasoning models (MLRMs) have\nsubstantially improved their ability to solve complex textual and visual tasks.\nHowever, these models tend to overthink on simple problems, producing\nunnecessarily lengthy reasoning traces, while under-exploring on challenging\nones, leading to missed solutions. To address this imbalance, we propose ARES,\na unified open-source framework for adaptive reasoning that dynamically\nallocates exploration effort based on task difficulty. Our approach is\nmotivated by two key empirical findings: (i) while single-token entropy is\nnoisy, high window-entropy (HWE) tokens (token-level entropies averaged under a\nsliding window) can reliably capture reasoning-critical moments; and (ii)\nreducing HWE usage benefits easy problems, while increasing it is essential for\nsolving hard ones. Building on these insights, ARES introduces a two-stage\ntraining pipeline. In the Adaptive Cold-Start stage, we curate multimodal and\ntextual data paired with reasoning traces of length proportional to problem\ndifficulty, equipping the model with initial difficulty awareness. In the\nsecond stage, we develop Adaptive Entropy Policy Optimization (AEPO), which\nuses HWE tokens as exploration triggers to decide when to explore, and a\nhierarchical entropy reward with dynamic KL control to decide how much to\nexplore. Extensive experiments demonstrate that ARES achieves superior\nperformance and reasoning efficiency across diverse mathematical, logical, and\nmultimodal benchmarks, while closing the gap to leading commercial systems\nunder significantly lower inference costs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faARES\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u7406\u673a\u5236\u52a8\u6001\u5206\u914d\u63a2\u7d22\u52aa\u529b\u6765\u89e3\u51b3\u591a\u6a21\u6001\u5927\u63a8\u7406\u6a21\u578b\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u8fc7\u5ea6\u601d\u8003\u3001\u5728\u56f0\u96be\u95ee\u9898\u4e0a\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5229\u7528\u9ad8\u7a97\u53e3\u71b5\u4ee4\u724c\u4f5c\u4e3a\u63a2\u7d22\u89e6\u53d1\u5668\uff0c\u5728\u591a\u4e2a\u6570\u5b66\u3001\u903b\u8f91\u548c\u591a\u6a21\u6001\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u63a8\u7406\u6a21\u578b\u5b58\u5728\u63a8\u7406\u52aa\u529b\u5206\u914d\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff1a\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u5197\u957f\u63a8\u7406\u8f68\u8ff9\uff08\u8fc7\u5ea6\u601d\u8003\uff09\uff0c\u800c\u5728\u56f0\u96be\u95ee\u9898\u4e0a\u63a2\u7d22\u4e0d\u8db3\u5bfc\u81f4\u9519\u5931\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u79cd\u4e0d\u5e73\u8861\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "ARES\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u81ea\u9002\u5e94\u51b7\u542f\u52a8\u9636\u6bb5\u901a\u8fc7\u6309\u95ee\u9898\u96be\u5ea6\u6bd4\u4f8b\u914d\u5bf9\u7684\u63a8\u7406\u8f68\u8ff9\u6570\u636e\u8d4b\u4e88\u6a21\u578b\u521d\u59cb\u96be\u5ea6\u611f\u77e5\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u63d0\u51fa\u81ea\u9002\u5e94\u71b5\u7b56\u7565\u4f18\u5316\uff0c\u4f7f\u7528\u9ad8\u7a97\u53e3\u71b5\u4ee4\u724c\u4f5c\u4e3a\u63a2\u7d22\u89e6\u53d1\u5668\u51b3\u5b9a\u4f55\u65f6\u63a2\u7d22\uff0c\u5e76\u91c7\u7528\u5e26\u52a8\u6001KL\u63a7\u5236\u7684\u5206\u5c42\u71b5\u5956\u52b1\u51b3\u5b9a\u63a2\u7d22\u7a0b\u5ea6\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cARES\u5728\u591a\u6837\u5316\u6570\u5b66\u3001\u903b\u8f91\u548c\u591a\u6a21\u6001\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u5728\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u7f29\u5c0f\u4e86\u4e0e\u9886\u5148\u5546\u4e1a\u7cfb\u7edf\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u57fa\u4e8e\u9ad8\u7a97\u53e3\u71b5\u4ee4\u724c\u7684\u81ea\u9002\u5e94\u63a8\u7406\u673a\u5236\u80fd\u6709\u6548\u5e73\u8861\u6a21\u578b\u5728\u4e0d\u540c\u96be\u5ea6\u95ee\u9898\u4e0a\u7684\u63a2\u7d22\u52aa\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u3001\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u591a\u6a21\u6001\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u52a8\u6001\u96be\u5ea6\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.08157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08157", "abs": "https://arxiv.org/abs/2510.08157", "authors": ["Zhentao Zou", "Zhengrong Yue", "Kunpeng Du", "Binlei Bao", "Hanting Li", "Haizhen Xie", "Guozheng Xu", "Yue Zhou", "Yali Wang", "Jie Hu", "Xue Jiang", "Xinghao Chen"], "title": "Beyond Textual CoT: Interleaved Text-Image Chains with Deep Confidence Reasoning for Image Editing", "comment": "25pages,20figures", "summary": "Image editing with natural language has gained significant popularity, yet\nexisting methods struggle with intricate object intersections and fine-grained\nspatial relationships due to the lack of an explicit reasoning process. While\nChain-of-Thought (CoT) has been explored to enhance reasoning, purely textual\nCoT or CoT augmented with coordinate information is fundamentally limited in\nits ability to represent intricate visual layouts and lacks the necessary\nvisual cues to guide the generation of fine-grained, pixel-level details. To\naddress these challenges, we propose Multimodal Reasoning Edit (MURE), a novel\nframework that shifts the visual editing process from purely text-based\nreasoning to a series of interleaved textual and visual rationales. Our\nframework performs image editing using a natively multimodal, interleaved\ntext-image CoT. This approach generates a step-by-step chain of reasoning where\na textual description is followed by a corresponding visual cue, such as a\npositional mask that defined intended edited regions or a representation of new\ncontent. Furthermore, to mitigate the hallucination phenomenon of large\nlanguage models, we introduce Multimodal Deep Confidence (MMDC) reasoning\nparadigm. This paradigm explores a tree of visual reasoning paths at each step.\nBy pruning low-quality branches using a deep confidence score from a reward\nmodel, it ensures the model consistently follows a high-quality trajectory\ntowards the final edited result. The proposed method decomposes complex editing\ntasks into interdependent sub-tasks, achieving greater precision at each stage\nand yielding high-fidelity edited results. We define the formulation for\ninterleaved text-image chains and release the first CoT-Edit-14K dataset,\ncomprising 14K high-quality editing examples. Extensive experiments show that\nour method yields significant improvements across three image editing\nbenchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MURE\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u6a21\u6001\u4ea4\u9519\u6587\u672c-\u56fe\u50cf\u601d\u7ef4\u94fe\uff0c\u5c06\u89c6\u89c9\u7f16\u8f91\u8fc7\u7a0b\u4ece\u7eaf\u6587\u672c\u63a8\u7406\u8f6c\u5411\u6587\u672c\u4e0e\u89c6\u89c9\u7406\u6027\u4ea4\u66ff\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u7684\u7cbe\u5ea6\u548c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u5bf9\u8c61\u4ea4\u53c9\u548c\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u5173\u7cfb\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\u8fc7\u7a0b\u3002\u7eaf\u6587\u672c\u601d\u7ef4\u94fe\u6216\u5750\u6807\u589e\u5f3a\u601d\u7ef4\u94fe\u5728\u8868\u793a\u590d\u6742\u89c6\u89c9\u5e03\u5c40\u65b9\u9762\u5b58\u5728\u6839\u672c\u9650\u5236\uff0c\u4e14\u7f3a\u4e4f\u8db3\u591f\u7684\u89c6\u89c9\u7ebf\u7d22\u6765\u6307\u5bfc\u50cf\u7d20\u7ea7\u7ec6\u8282\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7f16\u8f91\u6846\u67b6\uff0c\u91c7\u7528\u539f\u751f\u591a\u6a21\u6001\u4ea4\u9519\u6587\u672c-\u56fe\u50cf\u601d\u7ef4\u94fe\uff0c\u5728\u63a8\u7406\u94fe\u7684\u6bcf\u4e00\u6b65\u4e2d\uff0c\u6587\u672c\u63cf\u8ff0\u540e\u8ddf\u968f\u76f8\u5e94\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u5982\u4f4d\u7f6e\u63a9\u7801\u6216\u65b0\u5185\u5bb9\u8868\u793a\u3002\u540c\u65f6\u5f15\u5165\u4e86\u591a\u6a21\u6001\u6df1\u5ea6\u7f6e\u4fe1\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u7684\u6df1\u5ea6\u7f6e\u4fe1\u5206\u6570\u4fee\u526a\u4f4e\u8d28\u91cf\u5206\u652f\uff0c\u786e\u4fdd\u6a21\u578b\u59cb\u7ec8\u6cbf\u7740\u9ad8\u8d28\u91cf\u8f68\u8ff9\u8fdb\u884c\u7f16\u8f91\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u56fe\u50cf\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u5206\u89e3\u4e3a\u76f8\u4e92\u4f9d\u8d56\u7684\u5b50\u4efb\u52a1\uff0c\u5728\u6bcf\u4e2a\u9636\u6bb5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7cbe\u5ea6\uff0c\u5e76\u4ea7\u751f\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u7f16\u8f91\u7ed3\u679c\u3002\u540c\u65f6\u53d1\u5e03\u4e86\u9996\u4e2aCoT-Edit-14K\u6570\u636e\u96c6\uff0c\u5305\u542b14K\u4e2a\u9ad8\u8d28\u91cf\u7f16\u8f91\u793a\u4f8b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u4ece\u7eaf\u6587\u672c\u63a8\u7406\u8f6c\u5411\u591a\u6a21\u6001\u4ea4\u9519\u63a8\u7406\u5728\u89c6\u89c9\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u663e\u5f0f\u5206\u89e3\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u548c\u89c6\u89c9\u7ebf\u7d22\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u7cbe\u5ea6\u548c\u7ed3\u679c\u8d28\u91cf\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.08181", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08181", "abs": "https://arxiv.org/abs/2510.08181", "authors": ["Haoran Yu", "Yi Shi"], "title": "InstructUDrag: Joint Text Instructions and Object Dragging for Interactive Image Editing", "comment": null, "summary": "Text-to-image diffusion models have shown great potential for image editing,\nwith techniques such as text-based and object-dragging methods emerging as key\napproaches. However, each of these methods has inherent limitations: text-based\nmethods struggle with precise object positioning, while object dragging methods\nare confined to static relocation. To address these issues, we propose\nInstructUDrag, a diffusion-based framework that combines text instructions with\nobject dragging, enabling simultaneous object dragging and text-based image\nediting. Our framework treats object dragging as an image reconstruction\nprocess, divided into two synergistic branches. The moving-reconstruction\nbranch utilizes energy-based gradient guidance to move objects accurately,\nrefining cross-attention maps to enhance relocation precision. The text-driven\nediting branch shares gradient signals with the reconstruction branch, ensuring\nconsistent transformations and allowing fine-grained control over object\nattributes. We also employ DDPM inversion and inject prior information into\nnoise maps to preserve the structure of moved objects. Extensive experiments\ndemonstrate that InstructUDrag facilitates flexible, high-fidelity image\nediting, offering both precision in object relocation and semantic control over\nimage content.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86InstructUDrag\u6846\u67b6\uff0c\u5c06\u6587\u672c\u6307\u4ee4\u4e0e\u5bf9\u8c61\u62d6\u62fd\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u540c\u65f6\u8fdb\u884c\u5bf9\u8c61\u62d6\u62fd\u548c\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u786e\u5b9a\u4f4d\u548c\u8bed\u4e49\u63a7\u5236\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u56fa\u6709\u5c40\u9650\uff1a\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u5bf9\u8c61\u5b9a\u4f4d\uff0c\u800c\u5bf9\u8c61\u62d6\u62fd\u65b9\u6cd5\u4ec5\u9650\u4e8e\u9759\u6001\u91cd\u5b9a\u4f4d\uff0c\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u7cbe\u786e\u79fb\u52a8\u548c\u8bed\u4e49\u5c5e\u6027\u63a7\u5236\u3002", "method": "\u63d0\u51faInstructUDrag\u6846\u67b6\uff0c\u5c06\u5bf9\u8c61\u62d6\u62fd\u89c6\u4e3a\u56fe\u50cf\u91cd\u5efa\u8fc7\u7a0b\uff0c\u5206\u4e3a\u4e24\u4e2a\u534f\u540c\u5206\u652f\uff1a\u79fb\u52a8\u91cd\u5efa\u5206\u652f\u4f7f\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u68af\u5ea6\u5f15\u5bfc\u7cbe\u786e\u5b9a\u4f4d\u5bf9\u8c61\uff0c\u6587\u672c\u9a71\u52a8\u7f16\u8f91\u5206\u652f\u5171\u4eab\u68af\u5ea6\u4fe1\u53f7\u786e\u4fdd\u4e00\u81f4\u53d8\u6362\uff1b\u91c7\u7528DDPM\u53cd\u6f14\u548c\u5148\u9a8c\u4fe1\u606f\u6ce8\u5165\u4ee5\u4fdd\u6301\u79fb\u52a8\u5bf9\u8c61\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cInstructUDrag\u5b9e\u73b0\u4e86\u7075\u6d3b\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u5728\u5bf9\u8c61\u91cd\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u56fe\u50cf\u5185\u5bb9\u8bed\u4e49\u63a7\u5236\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u7cbe\u786e\u5b9a\u4f4d\u548c\u7ec6\u7c92\u5ea6\u5c5e\u6027\u63a7\u5236\u7684\u9700\u6c42\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u6587\u672c\u6307\u4ee4\u548c\u5bf9\u8c61\u62d6\u62fd\u7684\u534f\u540c\u6846\u67b6\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u7cbe\u786e\u7684\u7f16\u8f91\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4ea4\u4e92\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4e2d\u7684\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2510.08278", "categories": ["cs.CV", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08278", "abs": "https://arxiv.org/abs/2510.08278", "authors": ["Fevziye Irem Eyiokur", "Dogucan Yaman", "Haz\u0131m Kemal Ekenel", "Alexander Waibel"], "title": "A Multimodal Depth-Aware Method For Embodied Reference Understanding", "comment": null, "summary": "Embodied Reference Understanding requires identifying a target object in a\nvisual scene based on both language instructions and pointing cues. While prior\nworks have shown progress in open-vocabulary object detection, they often fail\nin ambiguous scenarios where multiple candidate objects exist in the scene. To\naddress these challenges, we propose a novel ERU framework that jointly\nleverages LLM-based data augmentation, depth-map modality, and a depth-aware\ndecision module. This design enables robust integration of linguistic and\nembodied cues, improving disambiguation in complex or cluttered environments.\nExperimental results on two datasets demonstrate that our approach\nsignificantly outperforms existing baselines, achieving more accurate and\nreliable referent detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5177\u8eab\u53c2\u8003\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5229\u7528LLM\u6570\u636e\u589e\u5f3a\u3001\u6df1\u5ea6\u56fe\u6a21\u6001\u548c\u6df1\u5ea6\u611f\u77e5\u51b3\u7b56\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u76ee\u6807\u7269\u4f53\u8bc6\u522b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u573a\u666f\u4e2d\u5b58\u5728\u591a\u4e2a\u5019\u9009\u7269\u4f53\u7684\u6a21\u7cca\u60c5\u51b5\u4e0b\u5f80\u5f80\u5931\u6548\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5177\u8eab\u53c2\u8003\u7406\u89e3\u4e2d\u7684\u6b67\u4e49\u6027\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u590d\u6742\u6216\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u63d0\u51fa\u7684ERU\u6846\u67b6\u6574\u5408\u4e86\u57fa\u4e8eLLM\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u3001\u6df1\u5ea6\u56fe\u6a21\u6001\u4fe1\u606f\u4ee5\u53ca\u6df1\u5ea6\u611f\u77e5\u51b3\u7b56\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u8bed\u8a00\u6307\u4ee4\u548c\u5177\u8eab\u7ebf\u7d22\u7684\u9c81\u68d2\u878d\u5408\uff0c\u7279\u522b\u589e\u5f3a\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6d88\u6b67\u80fd\u529b\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u548c\u53ef\u9760\u7684\u53c2\u8003\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u548c\u6df1\u5ea6\u611f\u77e5\u673a\u5236\u5728\u5177\u8eab\u53c2\u8003\u7406\u89e3\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5904\u7406\u590d\u6742\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.08316", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08316", "abs": "https://arxiv.org/abs/2510.08316", "authors": ["Yu Huang", "Zelin Peng", "Changsong Wen", "Xiaokang Yang", "Wei Shen"], "title": "Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge", "comment": "Work in process", "summary": "Affordance segmentation aims to parse 3D objects into functionally distinct\nparts, bridging recognition and interaction for applications in robotic\nmanipulation, embodied AI, and AR. While recent studies leverage visual or\ntextual prompts to guide this process, they often rely on point cloud encoders\nas generic feature extractors, overlooking the intrinsic challenges of 3D data\nsuch as sparsity, noise, and geometric ambiguity. As a result, 3D features\nlearned in isolation frequently lack clear and semantically consistent\nfunctional boundaries. To address this bottleneck, we propose a\nsemantic-grounded learning paradigm that transfers rich semantic knowledge from\nlarge-scale 2D Vision Foundation Models (VFMs) into the 3D domain.\nSpecifically, We introduce Cross-Modal Affinity Transfer (CMAT), a pre-training\nstrategy that aligns a 3D encoder with lifted 2D semantics and jointly\noptimizes reconstruction, affinity, and diversity to yield semantically\norganized representations. Building on this backbone, we further design the\nCross-modal Affordance Segmentation Transformer (CAST), which integrates\nmulti-modal prompts with CMAT-pretrained features to generate precise,\nprompt-aware segmentation maps. Extensive experiments on standard benchmarks\ndemonstrate that our framework establishes new state-of-the-art results for 3D\naffordance segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u57fa\u7840\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u4eb2\u548c\u529b\u8fc1\u79fb\u5c06\u5927\u89c4\u6a212D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4e30\u5bcc\u8bed\u4e49\u77e5\u8bc6\u8f6c\u79fb\u52303D\u9886\u57df\uff0c\u57283D\u529f\u80fd\u5206\u5272\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u529f\u80fd\u5206\u5272\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u70b9\u4e91\u7f16\u7801\u5668\u4f5c\u4e3a\u901a\u7528\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4f46\u5ffd\u89c6\u4e863D\u6570\u636e\u56fa\u6709\u7684\u7a00\u758f\u6027\u3001\u566a\u58f0\u548c\u51e0\u4f55\u6a21\u7cca\u6027\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u5b66\u4e60\u5230\u76843D\u7279\u5f81\u7f3a\u4e4f\u6e05\u6670\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u529f\u80fd\u8fb9\u754c\u3002", "method": "\u63d0\u51fa\u4e86\u8de8\u6a21\u6001\u4eb2\u548c\u529b\u8fc1\u79fb\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5c063D\u7f16\u7801\u5668\u4e0e\u63d0\u5347\u76842D\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u8054\u5408\u4f18\u5316\u91cd\u6784\u3001\u4eb2\u548c\u529b\u548c\u591a\u6837\u6027\u4ee5\u4ea7\u751f\u8bed\u4e49\u7ec4\u7ec7\u7684\u8868\u793a\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1\u4e86\u8de8\u6a21\u6001\u529f\u80fd\u5206\u5272\u53d8\u6362\u5668\uff0c\u96c6\u6210\u591a\u6a21\u6001\u63d0\u793a\u4e0eCMAT\u9884\u8bad\u7ec3\u7279\u5f81\u4ee5\u751f\u6210\u7cbe\u786e\u7684\u63d0\u793a\u611f\u77e5\u5206\u5272\u56fe\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u57283D\u529f\u80fd\u5206\u5272\u4efb\u52a1\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5c062D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u77e5\u8bc6\u8fc1\u79fb\u52303D\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u4e3a3D\u529f\u80fd\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u57fa\u7840\u5b66\u4e60\u8303\u5f0f\uff0c\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u5177\u8eabAI\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.08352", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08352", "abs": "https://arxiv.org/abs/2510.08352", "authors": ["Nikos Theodoridis", "Tim Brophy", "Reenu Mohandas", "Ganesh Sistu", "Fiachra Collins", "Anthony Scanlan", "Ciaran Eising"], "title": "Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception", "comment": null, "summary": "Vision-Language Models (VLMs) are becoming increasingly powerful,\ndemonstrating strong performance on a variety of tasks that require both visual\nand textual understanding. Their strong generalisation abilities make them a\npromising component for automated driving systems, which must handle unexpected\ncorner cases. However, to be trusted in such safety-critical applications, a\nmodel must first possess a reliable perception system. Moreover, since critical\nobjects and agents in traffic scenes are often at a distance, we require\nsystems that are not \"shortsighted\", i.e., systems with strong perception\ncapabilities at both close (up to 20 meters) and long (30+ meters) range. With\nthis in mind, we introduce Distance-Annotated Traffic Perception Question\nAnswering (DTPQA), the first Visual Question Answering (VQA) benchmark focused\nsolely on perception-based questions in traffic scenes, enriched with distance\nannotations. By excluding questions that require reasoning, we ensure that\nmodel performance reflects perception capabilities alone. Since automated\ndriving hardware has limited processing power and cannot support large VLMs,\nour study centers on smaller VLMs. More specifically, we evaluate several\nstate-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the\nsimplicity of the questions, these models significantly underperform compared\nto humans (~60% average accuracy for the best-performing small VLM versus ~85%\nhuman performance). However, it is important to note that the human sample size\nwas relatively small, which imposes statistical limitations. We also identify\nspecific perception tasks, such as distinguishing left from right, that remain\nparticularly challenging for these models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u6ce8\u4e8e\u4ea4\u901a\u573a\u666f\u611f\u77e5\u7684\u8ddd\u79bb\u6807\u6ce8\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6DTPQA\uff0c\u901a\u8fc7\u8bc4\u4f30\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d1\u73b0\u5176\u5728\u611f\u77e5\u4efb\u52a1\u4e0a\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u8fdc\u8ddd\u79bb\u611f\u77e5\u548c\u65b9\u5411\u8fa8\u522b\u7b49\u5173\u952e\u4efb\u52a1\u4e0a\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7f3a\u4e4f\u53ef\u9760\u7684\u611f\u77e5\u7cfb\u7edf\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8fdc\u8ddd\u79bb\uff0830\u7c73\u4ee5\u4e0a\uff09\u4ea4\u901a\u573a\u666f\u7684\u611f\u77e5\u80fd\u529b\u4e0d\u8db3\uff0c\u73b0\u6709\u57fa\u51c6\u5f80\u5f80\u5305\u542b\u63a8\u7406\u4efb\u52a1\u800c\u65e0\u6cd5\u51c6\u786e\u8861\u91cf\u7eaf\u611f\u77e5\u6027\u80fd\uff0c\u4e14\u81ea\u52a8\u9a7e\u9a76\u786c\u4ef6\u9650\u5236\u65e0\u6cd5\u652f\u6301\u5927\u578b\u6a21\u578b\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u8ddd\u79bb\u6807\u6ce8\u4ea4\u901a\u611f\u77e5\u95ee\u7b54\u57fa\u51c6DTPQA\uff0c\u8be5\u57fa\u51c6\u4e13\u95e8\u9488\u5bf9\u4ea4\u901a\u573a\u666f\u8bbe\u8ba1\uff0c\u6392\u9664\u9700\u8981\u63a8\u7406\u7684\u95ee\u9898\u4ee5\u786e\u4fdd\u4ec5\u8bc4\u4f30\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u8ddd\u79bb\u6807\u6ce8\u533a\u5206\u8fd1\u8ddd\u79bb\uff0820\u7c73\u5185\uff09\u548c\u8fdc\u8ddd\u79bb\uff0830\u7c73\u4ee5\u4e0a\uff09\u611f\u77e5\u4efb\u52a1\uff0c\u5e76\u91cd\u70b9\u8bc4\u4f30\u4e86\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6700\u4f73\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728DTPQA\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u4ec5\u4e3a\u7ea660%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u7ea685%\u7684\u8868\u73b0\uff0c\u6a21\u578b\u5728\u8fdc\u8ddd\u79bb\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u533a\u5206\u5de6\u53f3\u65b9\u5411\u7b49\u57fa\u672c\u611f\u77e5\u4efb\u52a1\u4e0a\u5b58\u5728\u660e\u663e\u56f0\u96be\uff0c\u5c3d\u7ba1\u4eba\u7c7b\u6837\u672c\u6570\u91cf\u6709\u9650\u5b58\u5728\u7edf\u8ba1\u9650\u5236\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f53\u524d\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4ea4\u901a\u573a\u666f\u611f\u77e5\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u8fdc\u8ddd\u79bb\u611f\u77e5\u548c\u65b9\u5411\u8fa8\u522b\u7b49\u5173\u952e\u80fd\u529b\u4e0a\uff0c\u8fd9\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u51fa\u4e86\u91cd\u8981\u6311\u6218\uff0c\u672a\u6765\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u5c0f\u578b\u6a21\u578b\u548c\u4e13\u95e8\u7684\u611f\u77e5\u4f18\u5316\u6280\u672f\u3002"}}
{"id": "2510.08377", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08377", "abs": "https://arxiv.org/abs/2510.08377", "authors": ["Cong Wei", "Quande Liu", "Zixuan Ye", "Qiulin Wang", "Xintao Wang", "Pengfei Wan", "Kun Gai", "Wenhu Chen"], "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos", "comment": "Project Website https://congwei1230.github.io/UniVideo/", "summary": "Unified multimodal models have shown promising results in multimodal content\ngeneration and editing but remain largely limited to the image domain. In this\nwork, we present UniVideo, a versatile framework that extends unified modeling\nto the video domain. UniVideo adopts a dual-stream design, combining a\nMultimodal Large Language Model (MLLM) for instruction understanding with a\nMultimodal DiT (MMDiT) for video generation. This design enables accurate\ninterpretation of complex multimodal instructions while preserving visual\nconsistency. Built on this architecture, UniVideo unifies diverse video\ngeneration and editing tasks under a single multimodal instruction paradigm and\nis jointly trained across them. Extensive experiments demonstrate that UniVideo\nmatches or surpasses state-of-the-art task-specific baselines in\ntext/image-to-video generation, in-context video generation and in-context\nvideo editing. Notably, the unified design of UniVideo enables two forms of\ngeneralization. First, UniVideo supports task composition, such as combining\nediting with style transfer, by integrating multiple capabilities within a\nsingle instruction. Second, even without explicit training on free-form video\nediting, UniVideo transfers its editing capability from large-scale image\nediting data to this setting, handling unseen instructions such as\ngreen-screening characters or changing materials within a video. Beyond these\ncore capabilities, UniVideo also supports visual-prompt-based video generation,\nwhere the MLLM interprets visual prompts and guides the MMDiT during synthesis.\nTo foster future research, we will release our model and code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UniVideo\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u89c6\u9891\u751f\u6210\u4e0e\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6d41\u67b6\u6784\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u591a\u6a21\u6001DiT\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5904\u7406\u591a\u79cd\u89c6\u9891\u4efb\u52a1\u7684\u7edf\u4e00\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u56fe\u50cf\u9886\u57df\uff0c\u89c6\u9891\u9886\u57df\u7684\u7edf\u4e00\u5efa\u6a21\u4ecd\u5b58\u5728\u7a7a\u767d\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u591a\u6a21\u6001\u6307\u4ee4\u548c\u4fdd\u6301\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "method": "UniVideo\u91c7\u7528\u53cc\u6d41\u8bbe\u8ba1\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6307\u4ee4\u7406\u89e3\uff0c\u4ee5\u53ca\u591a\u6a21\u6001DiT\u8fdb\u884c\u89c6\u9891\u751f\u6210\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u7edf\u4e00\u5904\u7406\u6587\u672c/\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u3001\u4e0a\u4e0b\u6587\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u7b49\u591a\u79cd\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eUniVideo\u5728\u591a\u4e2a\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u4efb\u52a1\u4e13\u7528\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5c55\u73b0\u51fa\u4efb\u52a1\u7ec4\u5408\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UniVideo\u7684\u7edf\u4e00\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u4efb\u52a1\u7ec4\u5408\u548c\u8de8\u6a21\u6001\u80fd\u529b\u8fc1\u79fb\uff0c\u4e3a\u89c6\u9891\u9886\u57df\u7684\u7edf\u4e00\u591a\u6a21\u6001\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5176\u6846\u67b6\u5c06\u516c\u5f00\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.08482", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08482", "abs": "https://arxiv.org/abs/2510.08482", "authors": ["Onur Kele\u015f", "Asl\u0131 \u00d6zy\u00fcrek", "Gerardo Ortega", "Kadir G\u00f6kg\u00f6", "Esam Ghaleb"], "title": "The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping", "comment": null, "summary": "Iconicity, the resemblance between linguistic form and meaning, is pervasive\nin signed languages, offering a natural testbed for visual grounding. For\nvision-language models (VLMs), the challenge is to recover such essential\nmappings from dynamic human motion rather than static context. We introduce the\n\\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts\npsycholinguistic measures to evaluate VLMs on three tasks: (i) phonological\nsign-form prediction (e.g., handshape, location), (ii) transparency (inferring\nmeaning from visual form), and (iii) graded iconicity ratings. We assess $13$\nstate-of-the-art VLMs in zero- and few-shot settings on Sign Language of the\nNetherlands and compare them to human baselines. On \\textit{phonological form\nprediction}, VLMs recover some handshape and location detail but remain below\nhuman performance; on \\textit{transparency}, they are far from human baselines;\nand only top models correlate moderately with human \\textit{iconicity ratings}.\nInterestingly, \\textit{models with stronger phonological form prediction\ncorrelate better with human iconicity judgment}, indicating shared sensitivity\nto visually grounded structure. Our findings validate these diagnostic tasks\nand motivate human-centric signals and embodied learning methods for modelling\niconicity and improving visual grounding in multimodal models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u89c6\u89c9\u8c61\u4f3c\u6027\u6311\u6218\u57fa\u51c6\uff0c\u901a\u8fc7\u8bc4\u4f3013\u79cd\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u624b\u8bed\u8c61\u4f3c\u6027\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u65b9\u9762\u4ecd\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u66f4\u5f3a\u7684\u8bed\u97f3\u5f62\u5f0f\u9884\u6d4b\u80fd\u529b\u4e0e\u4eba\u7c7b\u8c61\u4f3c\u6027\u5224\u65ad\u5b58\u5728\u76f8\u5173\u6027\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u52a8\u6001\u4eba\u4f53\u8fd0\u52a8\u800c\u975e\u9759\u6001\u4e0a\u4e0b\u6587\u4e2d\u6062\u590d\u8bed\u8a00\u5f62\u5f0f\u4e0e\u610f\u4e49\u4e4b\u95f4\u672c\u8d28\u6620\u5c04\u7684\u6311\u6218\uff0c\u624b\u8bed\u4e2d\u7684\u8c61\u4f3c\u6027\u4e3a\u89c6\u89c9\u57fa\u7840\u63d0\u4f9b\u4e86\u81ea\u7136\u6d4b\u8bd5\u5e73\u53f0\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u89c6\u89c9\u8c61\u4f3c\u6027\u6311\u6218\u57fa\u51c6\uff0c\u91c7\u7528\u5fc3\u7406\u8bed\u8a00\u5b66\u6d4b\u91cf\u65b9\u6cd5\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1a\u8bed\u97f3\u7b26\u53f7\u5f62\u5f0f\u9884\u6d4b\u3001\u900f\u660e\u5ea6\u63a8\u7406\u548c\u5206\u7ea7\u8c61\u4f3c\u6027\u8bc4\u5206\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u5bf913\u79cd\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u8bed\u97f3\u5f62\u5f0f\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6062\u590d\u90e8\u5206\u624b\u5f62\u548c\u4f4d\u7f6e\u7ec6\u8282\u4f46\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff1b\u5728\u900f\u660e\u5ea6\u4efb\u52a1\u4e2d\u8fdc\u672a\u8fbe\u5230\u4eba\u7c7b\u57fa\u7ebf\uff1b\u53ea\u6709\u9876\u7ea7\u6a21\u578b\u4e0e\u4eba\u7c7b\u8c61\u4f3c\u6027\u8bc4\u5206\u5448\u4e2d\u7b49\u76f8\u5173\u6027\uff0c\u4e14\u8bed\u97f3\u5f62\u5f0f\u9884\u6d4b\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u4e0e\u4eba\u7c7b\u8c61\u4f3c\u6027\u5224\u65ad\u76f8\u5173\u6027\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u8bca\u65ad\u4efb\u52a1\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u6a21\u578b\u5bf9\u89c6\u89c9\u57fa\u7840\u7ed3\u6784\u5177\u6709\u5171\u4eab\u654f\u611f\u6027\uff0c\u5f3a\u8c03\u4e86\u91c7\u7528\u4ee5\u4eba\u4e3a\u672c\u7684\u4fe1\u53f7\u548c\u5177\u8eab\u5b66\u4e60\u65b9\u6cd5\u5bf9\u4e8e\u5efa\u6a21\u8c61\u4f3c\u6027\u548c\u6539\u8fdb\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u89c6\u89c9\u57fa\u7840\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.08398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08398", "abs": "https://arxiv.org/abs/2510.08398", "authors": ["Zeqing Wang", "Xinyu Wei", "Bairui Li", "Zhen Guo", "Jinrui Zhang", "Hongyang Wei", "Keze Wang", "Lei Zhang"], "title": "VideoVerse: How Far is Your T2V Generator from a World Model?", "comment": "24 Pages, 8 Figures, 11 Tables", "summary": "The recent rapid advancement of Text-to-Video (T2V) generation technologies,\nwhich are critical to build ``world models'', makes the existing benchmarks\nincreasingly insufficient to evaluate state-of-the-art T2V models. First,\ncurrent evaluation dimensions, such as per-frame aesthetic quality and temporal\nconsistency, are no longer able to differentiate state-of-the-art T2V models.\nSecond, event-level temporal causality, which not only distinguishes video from\nother modalities but also constitutes a crucial component of world models, is\nseverely underexplored in existing benchmarks. Third, existing benchmarks lack\na systematic assessment of world knowledge, which are essential capabilities\nfor building world models. To address these issues, we introduce VideoVerse, a\ncomprehensive benchmark that focuses on evaluating whether a T2V model could\nunderstand complex temporal causality and world knowledge in the real world. We\ncollect representative videos across diverse domains (e.g., natural landscapes,\nsports, indoor scenes, science fiction, chemical and physical experiments) and\nextract their event-level descriptions with inherent temporal causality, which\nare then rewritten into text-to-video prompts by independent annotators. For\neach prompt, we design a suite of binary evaluation questions from the\nperspective of dynamic and static properties, with a total of ten carefully\ndefined evaluation dimensions. In total, our VideoVerse comprises 300 carefully\ncurated prompts, involving 815 events and 793 binary evaluation questions.\nConsequently, a human preference aligned QA-based evaluation pipeline is\ndeveloped by using modern vision-language models. Finally, we perform a\nsystematic evaluation of state-of-the-art open-source and closed-source T2V\nmodels on VideoVerse, providing in-depth analysis on how far the current T2V\ngenerators are from world models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VideoVerse\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u5173\u6ce8\u4e8b\u4ef6\u7ea7\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u548c\u4e16\u754c\u77e5\u8bc6\u6765\u8bc4\u4f30T2V\u6a21\u578b\u6784\u5efa\u4e16\u754c\u6a21\u578b\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5f53\u524d\u8bc4\u4f30\u7ef4\u5ea6\u65e0\u6cd5\u533a\u5206\u6700\u5148\u8fdb\u7684T2V\u6a21\u578b\uff1b\u4e8b\u4ef6\u7ea7\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u8fd9\u4e00\u5173\u952e\u7ef4\u5ea6\u88ab\u4e25\u91cd\u5ffd\u89c6\uff1b\u7f3a\u4e4f\u5bf9\u6784\u5efa\u4e16\u754c\u6a21\u578b\u6240\u9700\u4e16\u754c\u77e5\u8bc6\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6536\u96c6\u8de8\u9886\u57df\u4ee3\u8868\u6027\u89c6\u9891\u5e76\u63d0\u53d6\u5177\u6709\u5185\u5728\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u7684\u4e8b\u4ef6\u7ea7\u63cf\u8ff0\uff0c\u901a\u8fc7\u72ec\u7acb\u6807\u6ce8\u8005\u91cd\u5199\u4e3a\u6587\u672c\u5230\u89c6\u9891\u63d0\u793a\uff0c\u8bbe\u8ba1\u4e86\u5305\u542b\u5341\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u7684\u4e8c\u5143\u8bc4\u4f30\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50QA\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "VideoVerse\u57fa\u51c6\u5305\u542b300\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u63d0\u793a\uff0c\u6d89\u53ca815\u4e2a\u4e8b\u4ef6\u548c793\u4e2a\u4e8c\u5143\u8bc4\u4f30\u95ee\u9898\uff0c\u5bf9\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u95ed\u6e90T2V\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u6df1\u5165\u5206\u6790\u4e86\u5f53\u524dT2V\u751f\u6210\u5668\u4e0e\u4e16\u754c\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f53\u524dT2V\u751f\u6210\u5668\u5728\u7406\u89e3\u590d\u6742\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u548c\u4e16\u754c\u77e5\u8bc6\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0cVideoVerse\u4e3a\u8bc4\u4f30T2V\u6a21\u578b\u6784\u5efa\u4e16\u754c\u6a21\u578b\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.08510", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08510", "abs": "https://arxiv.org/abs/2510.08510", "authors": ["Jiayun Luo", "Wan-Cyuan Fan", "Lyuyang Wang", "Xiangteng He", "Tanzila Rahman", "Purang Abolmaesumi", "Leonid Sigal"], "title": "To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models", "comment": "Preprint. Project page: https://davidhalladay.github.io/diysink_demo", "summary": "Large Vision Language Models (LVLMs) have recently emerged as powerful\narchitectures capable of understanding and reasoning over both visual and\ntextual information. These models typically rely on two key components: a\nVision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual\ncontent into a sequence of image tokens and serves as the perceptual front-end\n-- the eyes of the model. In contrast, the LLM interprets these tokens to\nperform high-level reasoning, generates responses, and functions as the\ncognitive core -- the brain of the model. However, it remains unclear which\nvisual tokens contribute most significantly to understanding and reasoning, and\nhow effectively these signals are propagated from ViT to the LLM. While most\nexisting works have focused on identifying attention sinks, low-semantic tokens\nreceiving disproportionately high attention, within the LLM, we shift the focus\nto the vision encoder by identifying a class of high-norm visual tokens from\nViT, referred to as ViT attention sinks -- a problem that has been rarely\nstudied but is indeed very important for LVLMs. Our findings show that these\nViT sinks encapsulate high-level semantic concepts from images, allowing the\nLLM to perform more effective understanding and reasoning. Despite their\nimportance, these sink tokens are often overlooked in existing LVLM\narchitectures. To explore their contribution, we present both qualitative and\nquantitative analyses of the information embedded in these sink tokens. We also\npropose both training-free and training-based approaches to better leverage how\nthis information is interpreted by the LLM, and to what extent. By explicitly\nutilizing these tokens, we demonstrate substantial improvements across a range\nof LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT\nattention sinks in enhancing visual reasoning.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u5e76\u7814\u7a76\u4e86ViT\u6ce8\u610f\u529b\u6c47\u805a\u70b9\u2014\u2014\u9ad8\u8303\u6570\u89c6\u89c9\u6807\u8bb0\uff0c\u8fd9\u4e9b\u6807\u8bb0\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\u4f46\u5e38\u88ab\u73b0\u6709LVLM\u67b6\u6784\u5ffd\u7565\u3002\u901a\u8fc7\u663e\u5f0f\u5229\u7528\u8fd9\u4e9b\u6807\u8bb0\uff0c\u7814\u7a76\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cdLVLM\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLM\u5185\u90e8\u7684\u6ce8\u610f\u529b\u6c47\u805a\u70b9\uff0c\u800c\u5ffd\u7565\u4e86\u89c6\u89c9\u7f16\u7801\u5668ViT\u4e2d\u9ad8\u8303\u6570\u89c6\u89c9\u6807\u8bb0\u7684\u91cd\u8981\u4f5c\u7528\u3002\u8fd9\u4e9bViT\u6ce8\u610f\u529b\u6c47\u805a\u70b9\u5305\u542b\u9ad8\u5c42\u6b21\u8bed\u4e49\u6982\u5ff5\uff0c\u4f46\u5f53\u524dLVLM\u67b6\u6784\u672a\u80fd\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u5173\u952e\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u89c6\u89c9\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790ViT\u6c47\u805a\u70b9\u6807\u8bb0\u4e2d\u5d4c\u5165\u7684\u4fe1\u606f\uff0c\u5e76\u5f00\u53d1\u4e86\u65e0\u9700\u8bad\u7ec3\u548c\u57fa\u4e8e\u8bad\u7ec3\u7684\u4e24\u79cd\u65b9\u6cd5\u6765\u4f18\u5316LLM\u5bf9\u8fd9\u4e9b\u6807\u8bb0\u7684\u89e3\u8bfb\u3002\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u5229\u7528ViT\u6ce8\u610f\u529b\u6c47\u805a\u70b9\u6765\u589e\u5f3a\u89c6\u89c9\u4fe1\u606f\u5411\u8bed\u8a00\u6a21\u578b\u7684\u4f20\u64ad\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u663e\u5f0f\u5229\u7528ViT\u6ce8\u610f\u529b\u6c47\u805a\u70b9\u53ef\u5728\u591a\u79cdLVLM\u67b6\u6784\u548c\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u8fd9\u4e9b\u9ad8\u8303\u6570\u89c6\u89c9\u6807\u8bb0\u88ab\u8bc1\u5b9e\u5305\u542b\u5173\u952e\u8bed\u4e49\u4fe1\u606f\uff0c\u80fd\u591f\u6709\u6548\u589e\u5f3a\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "ViT\u6ce8\u610f\u529b\u6c47\u805a\u70b9\u662fLVLM\u4e2d\u672a\u88ab\u5145\u5206\u6316\u6398\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u5176\u5305\u542b\u7684\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\u5bf9\u89c6\u89c9\u63a8\u7406\u81f3\u5173\u91cd\u8981\u3002\u672a\u6765LVLM\u8bbe\u8ba1\u5e94\u66f4\u5173\u6ce8\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u7684\u5173\u952e\u6807\u8bb0\uff0c\u4f18\u5316\u89c6\u89c9\u4fe1\u606f\u5411\u8bed\u8a00\u6a21\u578b\u7684\u4f20\u64ad\u673a\u5236\u3002"}}
{"id": "2510.08431", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08431", "abs": "https://arxiv.org/abs/2510.08431", "authors": ["Kaiwen Zheng", "Yuji Wang", "Qianli Ma", "Huayu Chen", "Jintao Zhang", "Yogesh Balaji", "Jianfei Chen", "Ming-Yu Liu", "Jun Zhu", "Qinsheng Zhang"], "title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency", "comment": null, "summary": "This work represents the first effort to scale up continuous-time consistency\ndistillation to general application-level image and video diffusion models.\nAlthough continuous-time consistency model (sCM) is theoretically principled\nand empirically powerful for accelerating academic-scale diffusion, its\napplicability to large-scale text-to-image and video tasks remains unclear due\nto infrastructure challenges in Jacobian-vector product (JVP) computation and\nthe limitations of standard evaluation benchmarks. We first develop a\nparallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on\nmodels with over 10 billion parameters and high-dimensional video tasks. Our\ninvestigation reveals fundamental quality limitations of sCM in fine-detail\ngeneration, which we attribute to error accumulation and the \"mode-covering\"\nnature of its forward-divergence objective. To remedy this, we propose the\nscore-regularized continuous-time consistency model (rCM), which incorporates\nscore distillation as a long-skip regularizer. This integration complements sCM\nwith the \"mode-seeking\" reverse divergence, effectively improving visual\nquality while maintaining high generation diversity. Validated on large-scale\nmodels (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM\nmatches or surpasses the state-of-the-art distillation method DMD2 on quality\nmetrics while offering notable advantages in diversity, all without GAN tuning\nor extensive hyperparameter searches. The distilled models generate\nhigh-fidelity samples in only $1\\sim4$ steps, accelerating diffusion sampling\nby $15\\times\\sim50\\times$. These results position rCM as a practical and\ntheoretically grounded framework for advancing large-scale diffusion\ndistillation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u5206\u6570\u6b63\u5219\u5316\u8fde\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u6a21\u578b\uff08rCM\uff09\uff0c\u9996\u6b21\u5c06\u8fde\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u84b8\u998f\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u5206\u6570\u84b8\u998f\u4f5c\u4e3a\u957f\u8df3\u8dc3\u6b63\u5219\u5316\u5668\uff0c\u5728\u4fdd\u6301\u9ad8\u751f\u6210\u591a\u6837\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u8fde\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u6a21\u578b\u5728\u5b66\u672f\u89c4\u6a21\u6269\u6563\u52a0\u901f\u65b9\u9762\u5177\u6709\u7406\u8bba\u4f18\u52bf\u548c\u5b9e\u8bc1\u6548\u679c\uff0c\u4f46\u5176\u5728\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u548c\u89c6\u9891\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u4ecd\u4e0d\u660e\u786e\uff0c\u4e3b\u8981\u9762\u4e34\u96c5\u53ef\u6bd4\u5411\u91cf\u79ef\u8ba1\u7b97\u7684\u57fa\u7840\u8bbe\u65bd\u6311\u6218\u4ee5\u53ca\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u7cbe\u7ec6\u7ec6\u8282\u751f\u6210\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u7684\u8d28\u91cf\u9650\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u5e76\u884c\u517c\u5bb9\u7684FlashAttention-2\u96c5\u53ef\u6bd4\u5411\u91cf\u79ef\u8ba1\u7b97\u5185\u6838\uff0c\u652f\u6301\u8d85\u8fc7100\u4ebf\u53c2\u6570\u6a21\u578b\u548c\u9ad8\u7ef4\u89c6\u9891\u4efb\u52a1\u7684\u8bad\u7ec3\uff1b\u63d0\u51fa\u5206\u6570\u6b63\u5219\u5316\u8fde\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u5c06\u5206\u6570\u84b8\u998f\u4f5c\u4e3a\u957f\u8df3\u8dc3\u6b63\u5219\u5316\u5668\u96c6\u6210\u5230sCM\u4e2d\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u5f0f\u8986\u76d6\u7684\u524d\u5411\u6563\u5ea6\u548c\u6a21\u5f0f\u5bfb\u6c42\u7684\u53cd\u5411\u6563\u5ea6\u6765\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u3002", "result": "\u5728\u9ad8\u8fbe140\u4ebf\u53c2\u6570\u7684Cosmos-Predict2\u3001Wan2.1\u7b49\u5927\u89c4\u6a21\u6a21\u578b\u548c5\u79d2\u89c6\u9891\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0crCM\u5728\u8d28\u91cf\u6307\u6807\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5DMD2\uff0c\u540c\u65f6\u5728\u591a\u6837\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u65e0\u9700GAN\u8c03\u4f18\u6216\u5927\u91cf\u8d85\u53c2\u6570\u641c\u7d22\uff1b\u84b8\u998f\u540e\u7684\u6a21\u578b\u4ec5\u97001-4\u6b65\u5373\u53ef\u751f\u6210\u9ad8\u4fdd\u771f\u6837\u672c\uff0c\u5c06\u6269\u6563\u91c7\u6837\u52a0\u901f15-50\u500d\u3002", "conclusion": "rCM\u4f5c\u4e3a\u4e00\u4e2a\u5b9e\u7528\u4e14\u7406\u8bba\u57fa\u7840\u7684\u6846\u67b6\uff0c\u4e3a\u63a8\u8fdb\u5927\u89c4\u6a21\u6269\u6563\u84b8\u998f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4e92\u8865\u7684\u6a21\u5f0f\u8986\u76d6\u548c\u6a21\u5f0f\u5bfb\u6c42\u76ee\u6807\u51fd\u6570\uff0c\u5728\u4fdd\u6301\u591a\u6837\u6027\u7684\u540c\u65f6\u89e3\u51b3\u4e86\u7cbe\u7ec6\u7ec6\u8282\u751f\u6210\u7684\u8d28\u91cf\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u5927\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u63a8\u7406\u65b9\u6848\u3002"}}
{"id": "2510.08531", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08531", "abs": "https://arxiv.org/abs/2510.08531", "authors": ["Hongxing Li", "Dingming Li", "Zixuan Wang", "Yuchen Yan", "Hang Wu", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models", "comment": "Project Page: https://zju-real.github.io/SpatialLadder/ Code:\n  https://github.com/ZJU-REAL/SpatialLadder", "summary": "Spatial reasoning remains a fundamental challenge for Vision-Language Models\n(VLMs), with current approaches struggling to achieve robust performance\ndespite recent advances. We identify that this limitation stems from a critical\ngap: existing methods attempt to learn spatial reasoning directly without\nestablishing the hierarchical foundations of perception and understanding. To\naddress this challenge, we present a comprehensive methodology for building\nspatial intelligence progressively. We introduce SpatialLadder-26k, a\nmultimodal dataset containing 26,610 samples spanning object localization,\nsingle image, multi-view, and video spatial reasoning tasks, constructed\nthrough a standardized pipeline that ensures systematic coverage across\nmodalities. Building on this dataset, we design a three-stage progressive\ntraining framework that (1) establishes spatial perception through object\nlocalization, (2) develops spatial understanding through multi-dimensional\nspatial tasks, and (3) strengthens complex reasoning via reinforcement learning\nwith verifiable rewards. This approach yields SpatialLadder, a 3B-parameter\nmodel that achieves state-of-the-art performance on spatial reasoning\nbenchmarks, with 23.4% average improvement over the base model, surpassing\nGPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains\nstrong generalization with 7.2% improvement on out-of-domain benchmarks,\ndemonstrating that progressive training from perception to reasoning is\nessential for robust spatial intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6846\u67b6SpatialLadder\uff0c\u901a\u8fc7\u5efa\u7acb\u7a7a\u95f4\u611f\u77e5\u3001\u7a7a\u95f4\u7406\u89e3\u548c\u590d\u6742\u63a8\u7406\u7684\u4e09\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86GPT-4o\u548cGemini-2.0-Flash\u7b49\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u76f4\u63a5\u5b66\u4e60\u7a7a\u95f4\u63a8\u7406\u800c\u7f3a\u4e4f\u5efa\u7acb\u611f\u77e5\u4e0e\u7406\u89e3\u7684\u5c42\u6b21\u5316\u57fa\u7840\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b26,610\u4e2a\u6837\u672c\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6SpatialLadder-26k\uff0c\u6db5\u76d6\u7269\u4f53\u5b9a\u4f4d\u3001\u5355\u56fe\u50cf\u3001\u591a\u89c6\u89d2\u548c\u89c6\u9891\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6e10\u8fdb\u5f0f\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u901a\u8fc7\u7269\u4f53\u5b9a\u4f4d\u5efa\u7acb\u7a7a\u95f4\u611f\u77e5\uff0c\u901a\u8fc7\u591a\u7ef4\u7a7a\u95f4\u4efb\u52a1\u53d1\u5c55\u7a7a\u95f4\u7406\u89e3\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u52a0\u5f3a\u590d\u6742\u63a8\u7406\u3002", "result": "SpatialLadder\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5e73\u5747\u63d0\u534723.4%\uff0c\u8d85\u8d8aGPT-4o 20.8%\u548cGemini-2.0-Flash 10.1%\uff0c\u5728\u9886\u57df\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u53477.2%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u4ece\u611f\u77e5\u5230\u63a8\u7406\u7684\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u5bf9\u4e8e\u6784\u5efa\u9c81\u68d2\u7684\u7a7a\u95f4\u667a\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u8be5\u65b9\u6cd5\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.08442", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08442", "abs": "https://arxiv.org/abs/2510.08442", "authors": ["Andrew Lee", "Ian Chuang", "Dechen Gao", "Kai Fukazawa", "Iman Soltani"], "title": "Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning", "comment": "Project page: https://andrewcwlee.github.io/gaze-on-the-prize", "summary": "Visual Reinforcement Learning (RL) agents must learn to act based on\nhigh-dimensional image data where only a small fraction of the pixels is\ntask-relevant. This forces agents to waste exploration and computational\nresources on irrelevant features, leading to sample-inefficient and unstable\nlearning. To address this, inspired by human visual foveation, we introduce\nGaze on the Prize. This framework augments visual RL with a learnable foveal\nattention mechanism (Gaze), guided by a self-supervised signal derived from the\nagent's experience pursuing higher returns (the Prize). Our key insight is that\nreturn differences reveal what matters most: If two similar representations\nproduce different outcomes, their distinguishing features are likely\ntask-relevant, and the gaze should focus on them accordingly. This is realized\nthrough return-guided contrastive learning that trains the attention to\ndistinguish between the features relevant to success and failure. We group\nsimilar visual representations into positives and negatives based on their\nreturn differences and use the resulting labels to construct contrastive\ntriplets. These triplets provide the training signal that teaches the attention\nmechanism to produce distinguishable representations for states associated with\ndifferent outcomes. Our method achieves up to 2.4x improvement in sample\nefficiency and can solve tasks that the baseline fails to learn, demonstrated\nacross a suite of manipulation tasks from the ManiSkill3 benchmark, all without\nmodifying the underlying algorithm or hyperparameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Gaze on the Prize\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u4e2d\u592e\u51f9\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u56de\u62a5\u5dee\u5f02\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u6765\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u5728ManiSkill3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad82.4\u500d\u7684\u6837\u672c\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5fc5\u987b\u57fa\u4e8e\u9ad8\u7ef4\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u5b66\u4e60\uff0c\u4f46\u53ea\u6709\u5c11\u91cf\u50cf\u7d20\u4e0e\u4efb\u52a1\u76f8\u5173\uff0c\u8fd9\u5bfc\u81f4\u4ee3\u7406\u5728\u65e0\u5173\u7279\u5f81\u4e0a\u6d6a\u8d39\u63a2\u7d22\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u9020\u6210\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u548c\u5b66\u4e60\u4e0d\u7a33\u5b9a\u3002", "method": "\u8be5\u6846\u67b6\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u4e2d\u592e\u51f9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u57fa\u4e8e\u56de\u62a5\u5dee\u5f02\u7684\u81ea\u76d1\u7763\u4fe1\u53f7\u8fdb\u884c\u5f15\u5bfc\uff0c\u91c7\u7528\u56de\u62a5\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u76f8\u4f3c\u89c6\u89c9\u8868\u793a\u6839\u636e\u56de\u62a5\u5dee\u5f02\u5206\u7ec4\u4e3a\u6b63\u8d1f\u6837\u672c\uff0c\u6784\u5efa\u5bf9\u6bd4\u4e09\u5143\u7ec4\u6765\u8bad\u7ec3\u6ce8\u610f\u529b\u673a\u5236\u4ea7\u751f\u53ef\u533a\u5206\u7684\u72b6\u6001\u8868\u793a\u3002", "result": "\u5728ManiSkill3\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e00\u7cfb\u5217\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad82.4\u500d\u7684\u6837\u672c\u6548\u7387\u63d0\u5347\uff0c\u5e76\u4e14\u80fd\u591f\u89e3\u51b3\u57fa\u7ebf\u65b9\u6cd5\u65e0\u6cd5\u5b66\u4e60\u7684\u4efb\u52a1\uff0c\u6240\u6709\u8fd9\u4e9b\u6539\u8fdb\u90fd\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u7b97\u6cd5\u6216\u8d85\u53c2\u6570\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u56de\u62a5\u5dee\u5f02\u80fd\u591f\u6709\u6548\u63ed\u793a\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u57fa\u4e8e\u56de\u62a5\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u5904\u7406\u9ad8\u7ef4\u89c6\u89c9\u8f93\u5165\u4e2d\u7684\u6ce8\u610f\u529b\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08543", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.08543", "abs": "https://arxiv.org/abs/2510.08543", "authors": ["Nikhil Reddy Varimalla", "Yunfei Xu", "Arkadiy Saakyan", "Meng Fan Wang", "Smaranda Muresan"], "title": "VideoNorms: Benchmarking Cultural Awareness of Video Language Models", "comment": "24 pages, 5 figures, under review", "summary": "As Video Large Language Models (VideoLLMs) are deployed globally, they\nrequire understanding of and grounding in the relevant cultural background. To\nproperly assess these models' cultural awareness, adequate benchmarks are\nneeded. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm)\npairs from US and Chinese cultures annotated with socio-cultural norms grounded\nin speech act theory, norm adherence and violations labels, and verbal and\nnon-verbal evidence. To build VideoNorms, we use a human-AI collaboration\nframework, where a teacher model using theoretically-grounded prompting\nprovides candidate annotations and a set of trained human experts validate and\ncorrect the annotations. We benchmark a variety of open-weight VideoLLMs on the\nnew dataset which highlight several common trends: 1) models performs worse on\nnorm violation than adherence; 2) models perform worse w.r.t Chinese culture\ncompared to the US culture; 3) models have more difficulty in providing\nnon-verbal evidence compared to verbal for the norm adhere/violation label and\nstruggle to identify the exact norm corresponding to a speech-act; and 4)\nunlike humans, models perform worse in formal, non-humorous contexts. Our\nfindings emphasize the need for culturally-grounded video language model\ntraining - a gap our benchmark and framework begin to address.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VideoNorms\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b1000\u591a\u4e2a\u89c6\u9891\u7247\u6bb5\u4e0e\u793e\u4f1a\u6587\u5316\u89c4\u8303\u914d\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u610f\u8bc6\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6587\u5316\u89c4\u8303\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u4e2d\u56fd\u6587\u5316\u7406\u89e3\u3001\u975e\u8bed\u8a00\u8bc1\u636e\u8bc6\u522b\u548c\u89c4\u8303\u8fdd\u53cd\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u968f\u7740\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5168\u7403\u90e8\u7f72\uff0c\u5b83\u4eec\u9700\u8981\u7406\u89e3\u548c\u878d\u5165\u76f8\u5173\u6587\u5316\u80cc\u666f\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9002\u5f53\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u7684\u6587\u5316\u610f\u8bc6\u80fd\u529b\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6355\u6349\u6a21\u578b\u5bf9\u6587\u5316\u89c4\u8303\u7684\u7406\u89e3\u548c\u63a5\u5730\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8de8\u6587\u5316\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u6846\u67b6\u6784\u5efaVideoNorms\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u4f7f\u7528\u57fa\u4e8e\u8a00\u8bed\u884c\u4e3a\u7406\u8bba\u7684\u7406\u8bba\u9a71\u52a8\u63d0\u793a\u7684\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u5019\u9009\u6807\u6ce8\uff0c\u7136\u540e\u7531\u8bad\u7ec3\u6709\u7d20\u7684\u4eba\u7c7b\u4e13\u5bb6\u9a8c\u8bc1\u548c\u4fee\u6b63\u6807\u6ce8\u3002\u6570\u636e\u96c6\u5305\u542b\u6765\u81ea\u7f8e\u56fd\u548c\u4e2d\u56fd\u6587\u5316\u7684\u89c6\u9891\u7247\u6bb5\u4e0e\u793e\u4f1a\u6587\u5316\u89c4\u8303\u914d\u5bf9\uff0c\u6807\u6ce8\u4e86\u89c4\u8303\u9075\u5b88\u4e0e\u8fdd\u53cd\u6807\u7b7e\u4ee5\u53ca\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u8bc1\u636e\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u591a\u4e2a\u5f00\u653e\u6743\u91cd\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5171\u540c\u8d8b\u52bf\uff1a\u5728\u89c4\u8303\u8fdd\u53cd\u68c0\u6d4b\u4e0a\u8868\u73b0\u8f83\u5dee\uff1b\u5bf9\u4e2d\u56fd\u6587\u5316\u7684\u7406\u89e3\u4e0d\u5982\u7f8e\u56fd\u6587\u5316\uff1b\u63d0\u4f9b\u975e\u8bed\u8a00\u8bc1\u636e\u6bd4\u8bed\u8a00\u8bc1\u636e\u66f4\u56f0\u96be\uff1b\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u4e0e\u8a00\u8bed\u884c\u4e3a\u5bf9\u5e94\u7684\u5177\u4f53\u89c4\u8303\uff1b\u5728\u6b63\u5f0f\u3001\u975e\u5e7d\u9ed8\u8bed\u5883\u4e2d\u8868\u73b0\u4e0d\u5982\u4eba\u7c7b\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6587\u5316\u63a5\u5730\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u5fc5\u8981\u6027\uff0c\u73b0\u6709\u6a21\u578b\u5728\u8de8\u6587\u5316\u7406\u89e3\u548c\u89c4\u8303\u8bc6\u522b\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002VideoNorms\u57fa\u51c6\u548c\u6846\u67b6\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u63d0\u4f9b\u4e86\u8d77\u70b9\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u6587\u5316\u610f\u8bc6\u7684\u89c6\u9891\u7406\u89e3\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6587\u5316\u573a\u666f\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.08480", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08480", "abs": "https://arxiv.org/abs/2510.08480", "authors": ["Zhenlong Yuan", "Xiangyan Qu", "Chengxuan Qian", "Rui Chen", "Jing Tang", "Lei Sun", "Xiangxiang Chu", "Dapeng Zhang", "Yiwei Wang", "Yujun Cai", "Shuo Li"], "title": "Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools", "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\npotential in bridging visual and textual reasoning, yet their reliance on\ntext-centric priors often limits their ability to disentangle semantically\nsimilar actions in open-vocabulary scenarios. To address this, we propose\nVideo-STAR, a framework that harmonizes contextual sub-motion decomposition\nwith tool-augmented reinforcement learning for open-vocabulary action\nrecognition (OVAR). Unlike prior methods that treat actions as monolithic\nentities, our approach innovatively decomposes actions into discriminative\nsub-motions for fine-grained matching while dynamically invoking\ndomain-specific tools for cross-modal interleaving, thereby enabling\ncategory-specific reasoning capacity and reducing cross-modal hallucination.\nMoreover, by designing a hierarchical reward that balances tool-usage\nefficiency, sub-motion relevance, and structural coherence in reasoning, our\nmethod autonomously leverages external tools to prioritize sub-motion patterns\nwithout explicit supervision, transmitting from text-centric reasoning to\nvisually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2,\nKinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art\nperformance, outperforming existing methods in distinguishing fine-grained\nactions and handling cross-modal hallucination, validating our excellent\nrobustness and generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVideo-STAR\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b50\u52a8\u4f5c\u5206\u89e3\u4e0e\u5de5\u5177\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u7684\u534f\u540c\u673a\u5236\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u8bed\u4e49\u6df7\u6dc6\u548c\u8de8\u6a21\u6001\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u5339\u914d\u548c\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9-\u6587\u672c\u63a8\u7406\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u6f5c\u529b\uff0c\u4f46\u5176\u5bf9\u6587\u672c\u4e2d\u5fc3\u5148\u9a8c\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u5728\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u4e0b\u533a\u5206\u8bed\u4e49\u76f8\u4f3c\u52a8\u4f5c\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc6\u522b\u548c\u51cf\u5c11\u8de8\u6a21\u6001\u5e7b\u89c9\u65b9\u9762\u5b58\u5728\u660e\u663e\u5c40\u9650\u3002", "method": "\u63d0\u51faVideo-STAR\u6846\u67b6\uff0c\u521b\u65b0\u6027\u5730\u5c06\u52a8\u4f5c\u5206\u89e3\u4e3a\u5177\u6709\u533a\u5206\u6027\u7684\u5b50\u52a8\u4f5c\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5339\u914d\uff0c\u540c\u65f6\u52a8\u6001\u8c03\u7528\u9886\u57df\u7279\u5b9a\u5de5\u5177\u8fdb\u884c\u8de8\u6a21\u6001\u4ea4\u7ec7\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5e73\u8861\u5de5\u5177\u4f7f\u7528\u6548\u7387\u3001\u5b50\u52a8\u4f5c\u76f8\u5173\u6027\u548c\u7ed3\u6784\u8fde\u8d2f\u6027\u7684\u5206\u5c42\u5956\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u5730\u4f18\u5148\u8003\u8651\u5b50\u52a8\u4f5c\u6a21\u5f0f\u3002", "result": "\u5728HMDB-51\u3001UCF-101\u3001SSv2\u3001Kinetics-400\u548cKinetics-600\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u533a\u5206\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u548c\u5904\u7406\u8de8\u6a21\u6001\u5e7b\u89c9\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u79c0\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b50\u52a8\u4f5c\u5206\u89e3\u548c\u5de5\u5177\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u7684\u534f\u540c\u673a\u5236\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u6587\u672c\u4e2d\u5fc3\u63a8\u7406\u5411\u89c6\u89c9\u57fa\u7840\u63a8\u7406\u7684\u8f6c\u53d8\uff0c\u4e3a\u5f00\u653e\u8bcd\u6c47\u52a8\u4f5c\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5728\u51cf\u5c11\u8de8\u6a21\u6001\u5e7b\u89c9\u548c\u63d0\u5347\u7c7b\u522b\u7279\u5b9a\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.08567", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08567", "abs": "https://arxiv.org/abs/2510.08567", "authors": ["Tajamul Ashraf", "Umair Nawaz", "Abdelrahman M. Shaker", "Rao Anwer", "Philip Torr", "Fahad Shahbaz Khan", "Salman Khan"], "title": "MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning", "comment": null, "summary": "Vision language models (VLMs) are increasingly deployed as controllers with\naccess to external tools for complex reasoning and decision-making, yet their\neffectiveness remains limited by the scarcity of high-quality multimodal\ntrajectories and the cost of manual annotation. We address this challenge with\na vision-centric agent tuning framework that automatically synthesizes\nmultimodal trajectories, generates step-wise preference pairs, and trains a VLM\ncontroller for robust tool-use reasoning. Our pipeline first constructs\nM-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified\ntrajectories, enabling imitation-based trajectory tuning. Building on this, we\ndevelop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool\nreasoning. To achieve finer alignment, we further introduce Pref-X, a set of\n11K automatically generated preference pairs, and optimize MATRIX on it via\nstep-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,\nMATRIX consistently surpasses both open- and closed-source VLMs, demonstrating\nscalable and effective multimodal tool use. Our data and code is avaliable at\nhttps://github.com/mbzuai-oryx/MATRIX.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u89c6\u89c9\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5408\u6210\u591a\u6a21\u6001\u8f68\u8ff9\u548c\u751f\u6210\u9010\u6b65\u504f\u597d\u5bf9\uff0c\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a7\u5236\u5668\u4ee5\u5b9e\u73b0\u9c81\u68d2\u7684\u5de5\u5177\u4f7f\u7528\u63a8\u7406\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u5f00\u6e90\u548c\u95ed\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u63a7\u5236\u5668\u8bbf\u95ee\u5916\u90e8\u5de5\u5177\u8fdb\u884c\u590d\u6742\u63a8\u7406\u548c\u51b3\u7b56\u65f6\uff0c\u9762\u4e34\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u8f68\u8ff9\u7a00\u7f3a\u548c\u624b\u52a8\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u9650\u5236\uff0c\u8fd9\u963b\u788d\u4e86\u6a21\u578b\u5728\u5de5\u5177\u4f7f\u7528\u63a8\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u89c6\u89c9\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u8c03\u4f18\u6846\u67b6\uff0c\u5305\u62ec\u6784\u5efa\u5305\u542b28.5K\u591a\u6a21\u6001\u4efb\u52a1\u548c177K\u5df2\u9a8c\u8bc1\u8f68\u8ff9\u7684M-TRACE\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684MATRIX Agent\u63a7\u5236\u5668\uff0c\u4ee5\u53ca\u901a\u8fc7Pref-X\u751f\u6210\u768411K\u81ea\u52a8\u504f\u597d\u5bf9\u8fdb\u884c\u9010\u6b65\u504f\u597d\u5b66\u4e60\u4f18\u5316\u3002", "result": "\u5728Agent-X\u3001GTA\u548cGAIA\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMATRIX\u63a7\u5236\u5668\u6301\u7eed\u8d85\u8d8a\u4e86\u5f00\u6e90\u548c\u95ed\u6e90\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u7684\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u81ea\u52a8\u5408\u6210\u591a\u6a21\u6001\u8f68\u8ff9\u548c\u504f\u597d\u5b66\u4e60\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u591a\u6a21\u6001\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.08485", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08485", "abs": "https://arxiv.org/abs/2510.08485", "authors": ["Chong Mou", "Qichao Sun", "Yanze Wu", "Pengze Zhang", "Xinghui Li", "Fulong Ye", "Songtao Zhao", "Qian He"], "title": "InstructX: Towards Unified Visual Editing with MLLM Guidance", "comment": null, "summary": "With recent advances in Multimodal Large Language Models (MLLMs) showing\nstrong visual understanding and reasoning, interest is growing in using them to\nimprove the editing performance of diffusion models. Despite rapid progress,\nmost studies lack an in-depth analysis of MLLM design choices. Moreover, the\nintegration of MLLMs and diffusion models remains an open challenge in some\ndifficult tasks, such as video editing. In this paper, we present InstructX, a\nunified framework for image and video editing. Specifically, we conduct a\ncomprehensive study on integrating MLLMs and diffusion models for\ninstruction-driven editing across diverse tasks. Building on this study, we\nanalyze the cooperation and distinction between images and videos in unified\nmodeling. (1) We show that training on image data can lead to emergent video\nediting capabilities without explicit supervision, thereby alleviating the\nconstraints imposed by scarce video training data. (2) By incorporating\nmodality-specific MLLM features, our approach effectively unifies image and\nvideo editing tasks within a single model. Extensive experiments demonstrate\nthat our method can handle a broad range of image and video editing tasks and\nachieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86InstructX\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6269\u6563\u6a21\u578b\u7684\u96c6\u6210\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u7684\u7edf\u4e00\u5efa\u6a21\uff0c\u5728\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u6d8c\u73b0\u51fa\u89c6\u9891\u7f16\u8f91\u80fd\u529b\uff0c\u5e76\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5c06\u5176\u4e0e\u6269\u6563\u6a21\u578b\u96c6\u6210\u7528\u4e8e\u7f16\u8f91\u4efb\u52a1\u7684\u7814\u7a76\u7f3a\u4e4f\u6df1\u5165\u7684\u8bbe\u8ba1\u9009\u62e9\u5206\u6790\uff0c\u7279\u522b\u662f\u5728\u89c6\u9891\u7f16\u8f91\u7b49\u56f0\u96be\u4efb\u52a1\u4e2d\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6269\u6563\u6a21\u578b\u7684\u96c6\u6210\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86InstructX\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u9762\u7814\u7a76\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6269\u6563\u6a21\u578b\u5728\u6307\u4ee4\u9a71\u52a8\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u96c6\u6210\u7b56\u7565\uff0c\u5206\u6790\u56fe\u50cf\u548c\u89c6\u9891\u5728\u7edf\u4e00\u5efa\u6a21\u4e2d\u7684\u534f\u540c\u4e0e\u5dee\u5f02\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u6a21\u6001\u7279\u5b9a\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7279\u5f81\u6765\u7edf\u4e00\u5904\u7406\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u5373\u53ef\u5728\u6ca1\u6709\u663e\u5f0f\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u6d8c\u73b0\u51fa\u89c6\u9891\u7f16\u8f91\u80fd\u529b\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u89c6\u9891\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u540c\u65f6\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u5e7f\u6cdb\u7684\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5728\u56fe\u50cf\u6570\u636e\u4e0a\u8bad\u7ec3\u53ef\u4ee5\u81ea\u7136\u83b7\u5f97\u89c6\u9891\u7f16\u8f91\u80fd\u529b\u7684\u6d8c\u73b0\u73b0\u8c61\uff0c\u4e3a\u7edf\u4e00\u5efa\u6a21\u56fe\u50cf\u548c\u89c6\u9891\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u7684\u7279\u5f81\u96c6\u6210\u7b56\u7565\u6210\u529f\u5b9e\u73b0\u4e86\u5355\u4e00\u6a21\u578b\u5bf9\u591a\u79cd\u7f16\u8f91\u4efb\u52a1\u7684\u5904\u7406\uff0c\u4e3a\u591a\u6a21\u6001\u7f16\u8f91\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.08508", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08508", "abs": "https://arxiv.org/abs/2510.08508", "authors": ["Lu Liu", "Chunlei Cai", "Shaocheng Shen", "Jianfeng Liang", "Weimin Ouyang", "Tianxiao Ye", "Jian Mao", "Huiyu Duan", "Jiangchao Yao", "Xiaoyun Zhang", "Qiang Hu", "Guangtao Zhai"], "title": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration", "comment": null, "summary": "Real-world videos often suffer from complex degradations, such as noise,\ncompression artifacts, and low-light distortions, due to diverse acquisition\nand transmission conditions. Existing restoration methods typically require\nprofessional manual selection of specialized models or rely on monolithic\narchitectures that fail to generalize across varying degradations. Inspired by\nexpert experience, we propose MoA-VR, the first\n\\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo\n\\underline{R}estoration system that mimics the reasoning and processing\nprocedures of human professionals through three coordinated agents: Degradation\nIdentification, Routing and Restoration, and Restoration Quality Assessment.\nSpecifically, we construct a large-scale and high-resolution video degradation\nrecognition benchmark and build a vision-language model (VLM) driven\ndegradation identifier. We further introduce a self-adaptive router powered by\nlarge language models (LLMs), which autonomously learns effective restoration\nstrategies by observing tool usage patterns. To assess intermediate and final\nprocessed video quality, we construct the \\underline{Res}tored\n\\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated\nVLM-based video quality assessment (VQA) model tailored for restoration tasks.\nExtensive experiments demonstrate that MoA-VR effectively handles diverse and\ncompound degradations, consistently outperforming existing baselines in terms\nof both objective metrics and perceptual quality. These results highlight the\npotential of integrating multimodal intelligence and modular reasoning in\ngeneral-purpose video restoration systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MoA-VR\uff0c\u9996\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u6df7\u5408\u67b6\u6784\u7684\u89c6\u9891\u4fee\u590d\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u4e2a\u534f\u540c\u5de5\u4f5c\u7684\u667a\u80fd\u4f53\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u6548\u5904\u7406\u590d\u6742\u591a\u6837\u7684\u89c6\u9891\u9000\u5316\u95ee\u9898\uff0c\u5728\u5ba2\u89c2\u6307\u6807\u548c\u611f\u77e5\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u5e38\u56e0\u91c7\u96c6\u548c\u4f20\u8f93\u6761\u4ef6\u4e0d\u540c\u800c\u906d\u53d7\u566a\u58f0\u3001\u538b\u7f29\u4f2a\u5f71\u548c\u4f4e\u5149\u5931\u771f\u7b49\u590d\u6742\u9000\u5316\uff0c\u73b0\u6709\u4fee\u590d\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4eba\u5de5\u9009\u62e9\u4e13\u7528\u6a21\u578b\u6216\u91c7\u7528\u5355\u4e00\u67b6\u6784\u96be\u4ee5\u6cdb\u5316\u5904\u7406\u5404\u79cd\u9000\u5316\u7c7b\u578b\uff0c\u5b58\u5728\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faMoA-VR\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u8c03\u667a\u80fd\u4f53\uff1a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9000\u5316\u8bc6\u522b\u5668\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u8def\u7531\u5668\u548c\u4e13\u95e8\u4e3a\u4fee\u590d\u4efb\u52a1\u8bbe\u8ba1\u7684\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u89c6\u9891\u9000\u5316\u8bc6\u522b\u57fa\u51c6\u548c\u6062\u590d\u89c6\u9891\u8d28\u91cf\u6570\u636e\u96c6\u6765\u652f\u6301\u7cfb\u7edf\u8bad\u7ec3\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eMoA-VR\u80fd\u6709\u6548\u5904\u7406\u591a\u6837\u5316\u548c\u590d\u5408\u9000\u5316\uff0c\u5728\u5ba2\u89c2\u6307\u6807\u548c\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u89c6\u9891\u9000\u5316\u95ee\u9898\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u667a\u80fd\u548c\u6a21\u5757\u5316\u63a8\u7406\u5728\u901a\u7528\u89c6\u9891\u4fee\u590d\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u667a\u80fd\u3001\u81ea\u9002\u5e94\u7684\u89c6\u9891\u5904\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u63a8\u52a8\u4e86\u57fa\u4e8e\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u89c6\u9891\u4fee\u590d\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2510.08559", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08559", "abs": "https://arxiv.org/abs/2510.08559", "authors": ["Andong Deng", "Taojiannan Yang", "Shoubin Yu", "Lincoln Spencer", "Mohit Bansal", "Chen Chen", "Serena Yeung-Levy", "Xiaohan Wang"], "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models", "comment": null, "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress across\nvarious capabilities; however, complex video reasoning in the scientific domain\nremains a significant and challenging frontier. Current video benchmarks\npredominantly target general scenarios where perception/recognition is heavily\nrelied on, while with relatively simple reasoning tasks, leading to saturation\nand thus failing to effectively evaluate advanced multimodal cognitive skills.\nTo address this critical gap, we introduce SciVideoBench, a rigorous benchmark\nspecifically designed to assess advanced video reasoning in scientific\ncontexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice\nquestions derived from cutting-edge scientific experimental videos spanning\nover 25 specialized academic subjects and verified by a semi-automatic system.\nEach question demands sophisticated domain-specific knowledge, precise\nspatiotemporal perception, and intricate logical reasoning, effectively\nchallenging models' higher-order cognitive abilities. Our evaluation highlights\nsignificant performance deficits in state-of-the-art proprietary and\nopen-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating\nsubstantial room for advancement in video reasoning capabilities. Detailed\nanalyses of critical factors such as reasoning complexity and visual grounding\nprovide valuable insights and clear direction for future developments in LMMs,\ndriving the evolution of truly capable multimodal AI co-scientists. We hope\nSciVideoBench could fit the interests of the community and help to push the\nboundary of cutting-edge AI for border science.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86SciVideoBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u89c6\u9891\u63a8\u7406\u80fd\u529b\u7684\u4e25\u683c\u57fa\u51c6\uff0c\u5305\u542b1000\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u591a\u9009\u9898\uff0c\u8986\u76d625\u4e2a\u4e13\u4e1a\u5b66\u79d1\uff0c\u65e8\u5728\u63a8\u52a8\u591a\u6a21\u6001\u6a21\u578b\u5728\u590d\u6742\u79d1\u5b66\u63a8\u7406\u65b9\u9762\u7684\u53d1\u5c55\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u901a\u7528\u573a\u666f\uff0c\u4f9d\u8d56\u611f\u77e5\u8bc6\u522b\u800c\u63a8\u7406\u4efb\u52a1\u76f8\u5bf9\u7b80\u5355\uff0c\u5bfc\u81f4\u6027\u80fd\u9971\u548c\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u5148\u8fdb\u7684\u591a\u6a21\u6001\u8ba4\u77e5\u6280\u80fd\uff0c\u7279\u522b\u662f\u5728\u79d1\u5b66\u9886\u57df\u7684\u590d\u6742\u89c6\u9891\u63a8\u7406\u4ecd\u662f\u4e00\u4e2a\u91cd\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u524d\u6cbf\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u534a\u81ea\u52a8\u7cfb\u7edf\u9a8c\u8bc1\uff0c\u4ece\u5c16\u7aef\u79d1\u5b66\u5b9e\u9a8c\u89c6\u9891\u4e2d\u7cbe\u5fc3\u6784\u5efa\u4e861000\u4e2a\u591a\u9009\u9898\uff0c\u6db5\u76d6\u8d85\u8fc725\u4e2a\u4e13\u4e1a\u5b66\u672f\u9886\u57df\uff0c\u6bcf\u4e2a\u95ee\u9898\u90fd\u9700\u8981\u590d\u6742\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3001\u7cbe\u786e\u7684\u65f6\u7a7a\u611f\u77e5\u548c\u7cbe\u7ec6\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5305\u62ecGemini 2.5 Pro\u548cQwen2.5-VL\u5728\u5185\u7684\u6700\u5148\u8fdb\u4e13\u6709\u548c\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u7f3a\u9677\uff0c\u8868\u660e\u5728\u89c6\u9891\u63a8\u7406\u80fd\u529b\u65b9\u9762\u4ecd\u6709\u5de8\u5927\u7684\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "\u8be6\u7ec6\u5206\u6790\u63a8\u7406\u590d\u6742\u6027\u548c\u89c6\u89c9\u57fa\u7840\u7b49\u5173\u952e\u56e0\u7d20\u4e3a\u672a\u6765\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u660e\u786e\u65b9\u5411\uff0c\u63a8\u52a8\u771f\u6b63\u6709\u80fd\u529b\u7684\u591a\u6a21\u6001AI\u5171\u540c\u79d1\u5b66\u5bb6\u7684\u6f14\u8fdb\uff0c\u8be5\u57fa\u51c6\u6709\u671b\u6ee1\u8db3\u793e\u533a\u5174\u8da3\u5e76\u5e2e\u52a9\u63a8\u52a8\u524d\u6cbfAI\u5728\u66f4\u5e7f\u6cdb\u79d1\u5b66\u9886\u57df\u7684\u8fb9\u754c\u6269\u5c55\u3002"}}
{"id": "2510.08540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08540", "abs": "https://arxiv.org/abs/2510.08540", "authors": ["Xiangyu Zhao", "Junming Lin", "Tianhao Liang", "Yifan Zhou", "Wenhao Chai", "Yuzhe Gu", "Weiyun Wang", "Kai Chen", "Gen Luo", "Wenwei Zhang", "Junchi Yan", "Hua Yang", "Haodong Duan", "Xue Yang"], "title": "MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization", "comment": null, "summary": "While current Multimodal Large Language Models (MLLMs) have demonstrated\nproficiency in reasoning tasks such as mathematics and logic, their capacity\nfor long-chain reflective reasoning, a prerequisite for solving complex\nreal-world problems, remains largely underexplored. In this work, we first\nconduct an extensive empirical investigation to evaluate this capability.\nLeveraging a carefully designed data synthesis engine, we construct MM-HELIX, a\nmultimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks\nthat require iterative thinking and backtracking. Empirical results on this\nbenchmark reveal that existing MLLMs exhibit significant performance deficits\nin long-chain reflective reasoning. To address this limitation, we generate\npost-training data and further explore learning paradigms for exploiting such\ndata. We first develop the Step-Elicited Response Generation pipeline to create\nMM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning\ntraces for instruction-tuning stage. Given that standard Reinforcement Learning\nfails on complex tasks due to sparse reward signals and catastrophic forgetting\nafter Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization\n(AHPO), a novel training strategy that dynamically unifies offline supervision\nand online optimization into a single stage. This strategy enables the model to\nlearn from expert data when rewards are sparse and conduct independent\nexploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our\nmethod achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and\ndemonstrates strong generalization with a +5.7\\% average performance gain on\ngeneral mathematic and logic tasks. Our work demonstrate that reflective\nreasoning in MLLMs can be effectively learned and generalized, paving the way\nfor developing more capable MLLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MM-HELIX\u57fa\u51c6\u548c\u81ea\u9002\u5e94\u6df7\u5408\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u94fe\u53cd\u601d\u63a8\u7406\u80fd\u529b\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8618.6%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u957f\u94fe\u53cd\u601d\u63a8\u7406\u80fd\u529b\u2014\u2014\u89e3\u51b3\u590d\u6742\u73b0\u5b9e\u95ee\u9898\u7684\u5173\u952e\u524d\u63d0\u2014\u2014\u4ecd\u7136\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u9996\u5148\u6784\u5efa\u4e86\u5305\u542b1,260\u4e2a\u6837\u672c\u7684MM-HELIX\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u968f\u540e\u5f00\u53d1\u4e86\u6b65\u9aa4\u5f15\u5bfc\u54cd\u5e94\u751f\u6210\u6d41\u6c34\u7ebf\u521b\u5efaMM-HELIX-100K\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u6df7\u5408\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u79bb\u7ebf\u76d1\u7763\u548c\u5728\u7ebf\u4f18\u5316\u52a8\u6001\u7edf\u4e00\u5230\u5355\u4e00\u8bad\u7ec3\u9636\u6bb5\u4e2d\u3002", "result": "\u5728MM-HELIX\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u5728Qwen2.5-VL-7B\u57fa\u7ebf\u4e0a\u5b9e\u73b0\u4e8618.6%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u540c\u65f6\u5728\u901a\u7528\u6570\u5b66\u548c\u903b\u8f91\u4efb\u52a1\u4e0a\u83b7\u5f97\u4e865.7%\u7684\u5e73\u5747\u6027\u80fd\u589e\u76ca\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u601d\u63a8\u7406\u80fd\u529b\u53ef\u4ee5\u901a\u8fc7\u6709\u6548\u5b66\u4e60\u83b7\u5f97\u5e76\u5b9e\u73b0\u6cdb\u5316\uff0c\u8fd9\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u8bc1\u660e\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u52a8\u6001\u8bad\u7ec3\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.08561", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08561", "abs": "https://arxiv.org/abs/2510.08561", "authors": ["Maham Tanveer", "Yang Zhou", "Simon Niklaus", "Ali Mahdavi Amiri", "Hao Zhang", "Krishna Kumar Singh", "Nanxuan Zhao"], "title": "MultiCOIN: Multi-Modal COntrollable Video INbetweening", "comment": "Project website: https://multicoinx.github.io/multicoin/", "summary": "Video inbetweening creates smooth and natural transitions between two image\nframes, making it an indispensable tool for video editing and long-form video\nsynthesis. Existing works in this domain are unable to generate large, complex,\nor intricate motions. In particular, they cannot accommodate the versatility of\nuser intents and generally lack fine control over the details of intermediate\nframes, leading to misalignment with the creative mind. To fill these gaps, we\nintroduce \\modelname{}, a video inbetweening framework that allows multi-modal\ncontrols, including depth transition and layering, motion trajectories, text\nprompts, and target regions for movement localization, while achieving a\nbalance between flexibility, ease of use, and precision for fine-grained video\ninterpolation. To achieve this, we adopt the Diffusion Transformer (DiT)\narchitecture as our video generative model, due to its proven capability to\ngenerate high-quality long videos. To ensure compatibility between DiT and our\nmulti-modal controls, we map all motion controls into a common sparse and\nuser-friendly point-based representation as the video/noise input. Further, to\nrespect the variety of controls which operate at varying levels of granularity\nand influence, we separate content controls and motion controls into two\nbranches to encode the required features before guiding the denoising process,\nresulting in two generators, one for motion and the other for content. Finally,\nwe propose a stage-wise training strategy to ensure that our model learns the\nmulti-modal controls smoothly. Extensive qualitative and quantitative\nexperiments demonstrate that multi-modal controls enable a more dynamic,\ncustomizable, and contextually accurate visual narrative.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u652f\u6301\u591a\u6a21\u6001\u63a7\u5236\u7684\u89c6\u9891\u63d2\u5e27\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8fd0\u52a8\u63a7\u5236\u6620\u5c04\u4e3a\u7edf\u4e00\u7684\u70b9\u57fa\u8868\u793a\uff0c\u5e76\u91c7\u7528\u53cc\u5206\u652f\u751f\u6210\u5668\u5206\u522b\u5904\u7406\u5185\u5bb9\u548c\u8fd0\u52a8\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u3001\u7cbe\u786e\u7684\u89c6\u9891\u8fc7\u6e21\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u63d2\u5e27\u65b9\u6cd5\u65e0\u6cd5\u751f\u6210\u5927\u89c4\u6a21\u3001\u590d\u6742\u6216\u7cbe\u7ec6\u7684\u8fd0\u52a8\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u7528\u6237\u610f\u56fe\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5bf9\u4e2d\u95f4\u5e27\u7ec6\u8282\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u5bfc\u81f4\u4e0e\u521b\u610f\u6784\u601d\u4e0d\u4e00\u81f4\u3002", "method": "\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\u4f5c\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u5c06\u6240\u6709\u8fd0\u52a8\u63a7\u5236\u6620\u5c04\u4e3a\u7edf\u4e00\u7684\u7a00\u758f\u70b9\u57fa\u8868\u793a\u4f5c\u4e3a\u89c6\u9891/\u566a\u58f0\u8f93\u5165\uff0c\u5c06\u5185\u5bb9\u63a7\u5236\u548c\u8fd0\u52a8\u63a7\u5236\u5206\u79bb\u4e3a\u4e24\u4e2a\u5206\u652f\u8fdb\u884c\u7279\u5f81\u7f16\u7801\uff0c\u5e76\u91c7\u7528\u5206\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u786e\u4fdd\u591a\u6a21\u6001\u63a7\u5236\u7684\u5e73\u6ed1\u5b66\u4e60\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u6a21\u6001\u63a7\u5236\u80fd\u591f\u5b9e\u73b0\u66f4\u52a8\u6001\u3001\u53ef\u5b9a\u5236\u4e14\u4e0a\u4e0b\u6587\u51c6\u786e\u7684\u89c6\u89c9\u53d9\u4e8b\u6548\u679c\u3002", "conclusion": "\u591a\u6a21\u6001\u63a7\u5236\u4e3a\u89c6\u9891\u63d2\u5e27\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u7cbe\u786e\u6027\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u70b9\u57fa\u8868\u793a\u548c\u53cc\u5206\u652f\u67b6\u6784\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u8fd0\u52a8\u7684\u6709\u6548\u5efa\u6a21\uff0c\u4e3a\u89c6\u9891\u7f16\u8f91\u548c\u957f\u89c6\u9891\u5408\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.08565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08565", "abs": "https://arxiv.org/abs/2510.08565", "authors": ["Changyao Tian", "Hao Li", "Gen Luo", "Xizhou Zhu", "Weijie Su", "Hanming Deng", "Jinguo Zhu", "Jie Shao", "Ziran Zhu", "Yunpeng Liu", "Lewei Lu", "Wenhai Wang", "Hongsheng Li", "Jifeng Dai"], "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints", "comment": "Accepted by NeurIPS 2025. 22 pages, link:\n  https://github.com/OpenGVLab/NaViL", "summary": "Compositional training has been the de-facto paradigm in existing Multimodal\nLarge Language Models (MLLMs), where pre-trained vision encoders are connected\nwith pre-trained LLMs through continuous multimodal pre-training. However, the\nmultimodal scaling property of this paradigm remains difficult to explore due\nto the separated training. In this paper, we focus on the native training of\nMLLMs in an end-to-end manner and systematically study its design space and\nscaling property under a practical setting, i.e., data constraint. Through\ncareful study of various choices in MLLM, we obtain the optimal\nmeta-architecture that best balances performance and training cost. After that,\nwe further explore the scaling properties of the native MLLM and indicate the\npositively correlated scaling relationship between visual encoders and LLMs.\nBased on these findings, we propose a native MLLM called NaViL, combined with a\nsimple and cost-effective recipe. Experimental results on 14 multimodal\nbenchmarks confirm the competitive performance of NaViL against existing MLLMs.\nBesides that, our findings and results provide in-depth insights for the future\nstudy of native MLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNaViL\u7684\u539f\u751f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u7cfb\u7edf\u7814\u7a76\u4e86MLLM\u7684\u8bbe\u8ba1\u7a7a\u95f4\u548c\u6269\u5c55\u7279\u6027\uff0c\u572814\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u539f\u751fMLLM\u7814\u7a76\u63d0\u4f9b\u4e86\u6df1\u5165\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u666e\u904d\u91c7\u7528\u7ec4\u5408\u5f0f\u8bad\u7ec3\u8303\u5f0f\uff0c\u5176\u4e2d\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u901a\u8fc7\u8fde\u7eed\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u4e0e\u9884\u8bad\u7ec3LLM\u8fde\u63a5\uff0c\u4f46\u8fd9\u79cd\u5206\u79bb\u8bad\u7ec3\u65b9\u5f0f\u4f7f\u5f97\u591a\u6a21\u6001\u6269\u5c55\u7279\u6027\u96be\u4ee5\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7814\u7a76\u805a\u7126\u4e8e\u539f\u751f\u7aef\u5230\u7aef\u8bad\u7ec3MLLM\uff0c\u5728\u6570\u636e\u7ea6\u675f\u7684\u5b9e\u9645\u8bbe\u7f6e\u4e0b\u7cfb\u7edf\u63a2\u7d22\u5176\u8bbe\u8ba1\u7a7a\u95f4\u548c\u6269\u5c55\u7279\u6027\uff0c\u901a\u8fc7\u4ed4\u7ec6\u7814\u7a76MLLM\u4e2d\u7684\u5404\u79cd\u9009\u62e9\uff0c\u83b7\u5f97\u4e86\u5728\u6027\u80fd\u548c\u8bad\u7ec3\u6210\u672c\u4e4b\u95f4\u6700\u4f73\u5e73\u8861\u7684\u5143\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u572814\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNaViL\u6a21\u578b\u76f8\u5bf9\u4e8e\u73b0\u6709MLLM\u5c55\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u4e0eLLM\u4e4b\u95f4\u6b63\u76f8\u5173\u7684\u6269\u5c55\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u539f\u751fMLLM\u7814\u7a76\u63d0\u4f9b\u4e86\u6df1\u5165\u89c1\u89e3\uff0c\u63d0\u51fa\u7684\u7b80\u5355\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8bad\u7ec3\u65b9\u6848\u4ee5\u53ca\u53d1\u73b0\u7684\u6269\u5c55\u89c4\u5f8b\u5bf9\u540e\u7eed\u591a\u6a21\u6001\u6a21\u578b\u53d1\u5c55\u5177\u6709\u91cd\u8981\u6307\u5bfc\u610f\u4e49\u3002"}}
