{"id": "2511.02946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02946", "abs": "https://arxiv.org/abs/2511.02946", "authors": ["Srikumar Sastry", "Subash Khanal", "Aayush Dhakal", "Jiayu Lin", "Dan Cher", "Phoenix Jarosz", "Nathan Jacobs"], "title": "ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology", "comment": "21 pages, 16 figures", "summary": "We introduce ProM3E, a probabilistic masked multimodal embedding model for\nany-to-any generation of multimodal representations for ecology. ProM3E is\nbased on masked modality reconstruction in the embedding space, learning to\ninfer missing modalities given a few context modalities. By design, our model\nsupports modality inversion in the embedding space. The probabilistic nature of\nour model allows us to analyse the feasibility of fusing various modalities for\ngiven downstream tasks, essentially learning what to fuse. Using these features\nof our model, we propose a novel cross-modal retrieval approach that mixes\ninter-modal and intra-modal similarities to achieve superior performance across\nall retrieval tasks. We further leverage the hidden representation from our\nmodel to perform linear probing tasks and demonstrate the superior\nrepresentation learning capability of our model. All our code, datasets and\nmodel will be released at https://vishu26.github.io/prom3e.", "AI": {"tldr": "ProM3E\u662f\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u63a9\u7801\u591a\u6a21\u6001\u5d4c\u5165\u7684\u751f\u6001\u5b66\u8868\u793a\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u5d4c\u5165\u7a7a\u95f4\u7684\u6a21\u6001\u91cd\u6784\u652f\u6301\u4efb\u610f\u6a21\u6001\u95f4\u7684\u751f\u6210\u4e0e\u68c0\u7d22\uff0c\u5e76\u63d0\u51fa\u65b0\u9896\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u6cd5\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u751f\u6001\u5b66\u4e2d\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5982\u4f55\u4ece\u90e8\u5206\u4e0a\u4e0b\u6587\u6a21\u6001\u63a8\u65ad\u7f3a\u5931\u6a21\u6001\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u6a21\u6001\u878d\u5408\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u672c\u8d28\u4e0a\u5b66\u4e60\u4f55\u65f6\u878d\u5408\u4f55\u79cd\u6a21\u6001\u3002", "method": "\u8be5\u6a21\u578b\u91c7\u7528\u57fa\u4e8e\u5d4c\u5165\u7a7a\u95f4\u7684\u63a9\u7801\u6a21\u6001\u91cd\u6784\u65b9\u6cd5\uff0c\u5b66\u4e60\u7ed9\u5b9a\u5c11\u91cf\u4e0a\u4e0b\u6587\u6a21\u6001\u65f6\u63a8\u65ad\u7f3a\u5931\u6a21\u6001\u7684\u80fd\u529b\uff0c\u5176\u6982\u7387\u6027\u8d28\u652f\u6301\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u7684\u53cd\u8f6c\uff0c\u5e76\u63d0\u51fa\u6df7\u5408\u6a21\u6001\u95f4\u548c\u6a21\u6001\u5185\u76f8\u4f3c\u5ea6\u7684\u65b0\u578b\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u5728\u6240\u6709\u68c0\u7d22\u4efb\u52a1\u4e2d\u5747\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\uff0c\u5176\u9690\u85cf\u8868\u793a\u5728\u7ebf\u6027\u63a2\u6d4b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u8868\u5f81\u5b66\u4e60\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u591a\u6a21\u6001\u878d\u5408\u548c\u8868\u793a\u5b66\u4e60\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u751f\u6001\u5b66\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5176\u6982\u7387\u5efa\u6a21\u548c\u6a21\u6001\u53cd\u8f6c\u80fd\u529b\u4e3a\u7406\u89e3\u6a21\u6001\u878d\u5408\u53ef\u884c\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63d0\u51fa\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.02996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02996", "abs": "https://arxiv.org/abs/2511.02996", "authors": ["Ailar Mahdizadeh", "Puria Azadi Moghadam", "Xiangteng He", "Shahriar Mirabbasi", "Panos Nasiopoulos", "Leonid Sigal"], "title": "SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics", "comment": null, "summary": "Vision-language models (VLMs) have demonstrated strong cross-modal\ncapabilities, yet most work remains limited to 2D data and assumes binary\nsupervision (i.e., positive vs. negative pairs), overlooking the continuous and\nstructured dependencies present in volumetric data such as CT. Existing\napproaches often treat volumetric scans as independent 2D slices, compromising\nspatial coherence and underutilizing rich clinical semantics. We propose\nSCALE-VLP, a soft-weighted contrastive vision-language pre-training framework\nthat integrates (i) volumetric spatial semantics to preserve anatomical\nstructure and (ii) domain-aware, knowledge-infused semantics (e.g.,\nradiological ontologies) to guide alignment. This yields structurally\nconsistent and semantically grounded representations under limited supervision,\ndemonstrating strong cross-task transferability (retrieval, report generation,\nand classification), and cross-domain generalizability with consistent gains\nwithout further fine-tuning. In particular, compared to the previous state of\nthe art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval,\nimproves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and\nBERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an\nout-of-domain external dataset, we observe consistent gains, indicating the\ncross-task and cross-domain generalization ability of SCALE-VLP.", "AI": {"tldr": "SCALE-VLP\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u52a0\u6743\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4f53\u79ef\u7a7a\u95f4\u8bed\u4e49\u548c\u9886\u57df\u611f\u77e5\u77e5\u8bc6\u6ce8\u5165\u8bed\u4e49\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4f53\u79ef\u6570\u636e\u65f6\u5ffd\u7565\u8fde\u7eed\u7ed3\u6784\u5316\u4f9d\u8d56\u5173\u7cfb\u7684\u95ee\u9898\uff0c\u5728CT\u56fe\u50cf-\u62a5\u544a\u68c0\u7d22\u3001\u5f02\u5e38\u5206\u7c7b\u548c\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e2D\u6570\u636e\u5e76\u91c7\u7528\u4e8c\u5143\u76d1\u7763\uff0c\u5ffd\u7565\u4e86CT\u7b49\u4f53\u79ef\u6570\u636e\u4e2d\u5b58\u5728\u7684\u8fde\u7eed\u7ed3\u6784\u5316\u4f9d\u8d56\u5173\u7cfb\uff0c\u540c\u65f6\u73b0\u6709\u65b9\u6cd5\u5c06\u4f53\u79ef\u626b\u63cf\u89c6\u4e3a\u72ec\u7acb2D\u5207\u7247\u5904\u7406\uff0c\u635f\u5bb3\u4e86\u7a7a\u95f4\u4e00\u81f4\u6027\u5e76\u672a\u80fd\u5145\u5206\u5229\u7528\u4e30\u5bcc\u7684\u4e34\u5e8a\u8bed\u4e49\u4fe1\u606f\u3002", "method": "SCALE-VLP\u6846\u67b6\u6574\u5408\u4e86\u4f53\u79ef\u7a7a\u95f4\u8bed\u4e49\u4ee5\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\uff0c\u4ee5\u53ca\u9886\u57df\u611f\u77e5\u7684\u77e5\u8bc6\u6ce8\u5165\u8bed\u4e49\uff08\u5982\u653e\u5c04\u5b66\u672c\u4f53\uff09\uff0c\u901a\u8fc7\u8f6f\u52a0\u6743\u5bf9\u6bd4\u5b66\u4e60\u5728\u6709\u9650\u76d1\u7763\u4e0b\u751f\u6210\u7ed3\u6784\u4e00\u81f4\u4e14\u8bed\u4e49\u57fa\u7840\u7684\u8868\u5f81\u3002", "result": "\u76f8\u6bd4\u5148\u524d\u6700\u4f18\u65b9\u6cd5\uff0cSCALE-VLP\u5728CT-\u62a5\u544a\u68c0\u7d22\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad84.3\u500d\u7684top-1\u6027\u80fd\u63d0\u5347\uff0c\u5f02\u5e38\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u9ad810\u4e2a\u767e\u5206\u70b9\uff0c\u62a5\u544a\u751f\u6210\u4efb\u52a1\u8fbe\u5230ROUGE-L 0.44\u548cBERT-F1 0.89\uff0c\u5728\u96f6\u6837\u672c\u8de8\u57df\u8bc4\u4f30\u4e2d\u4e5f\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660eSCALE-VLP\u5177\u6709\u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u53ef\u8fc1\u79fb\u6027\u548c\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u8fdb\u4e00\u6b65\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u5904\u7406\u4f53\u79ef\u533b\u5b66\u6570\u636e\u63d0\u4f9b\u4e86\u7ed3\u6784\u4e00\u81f4\u4e14\u8bed\u4e49\u57fa\u7840\u7684\u8868\u5f81\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2511.03019", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03019", "abs": "https://arxiv.org/abs/2511.03019", "authors": ["Wenbo Lu"], "title": "SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment", "comment": "Capstone Paper", "summary": "Vision-Language Pretraining (VLP) has achieved remarkable success across\nvarious downstream tasks, but such gains are largely driven by scaling up on\ntraining data. Yet, literature methods treat image-text pairs as isolated\ntraining examples; this neglects the rich relational structure naturally\npresent in many domains, such as e-commerce product co-purchase graphs and\nsocial recommendation networks. Inspired by neuroscientific evidence that human\nencodes knowledge as relationship cognitive maps, we introduce Structure-aware\nLanguage-Image Pretraining (SLIP). SLIP integrates a structural contrastive\nloss to align modalities while also modeling relationships between neighboring\nentities in a structured graph. To support this paradigm, we construct a\nlarge-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling\nstructured cross-modality supervision at scale. Experiment results show that\nSLIP consistently outperforms CLIP on cross-modal retrieval and classification\ntasks in both zero-shot and few-shot settings, showing the value of relational\nsupervision for cross-modal alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7ed3\u6784\u611f\u77e5\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\uff08SLIP\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u5bf9\u6bd4\u635f\u5931\u6765\u5efa\u6a21\u56fe\u50cf-\u6587\u672c\u5bf9\u4e4b\u95f4\u7684\u5173\u8054\u5173\u7cfb\uff0c\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eCLIP\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5173\u7cfb\u76d1\u7763\u5bf9\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u91cd\u8981\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\u5c06\u56fe\u50cf-\u6587\u672c\u5bf9\u89c6\u4e3a\u5b64\u7acb\u8bad\u7ec3\u6837\u672c\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u9886\u57df\u4e2d\u4e30\u5bcc\u7684\u5173\u8054\u7ed3\u6784\uff0c\u5982\u7535\u5546\u4ea7\u54c1\u5171\u8d2d\u56fe\u548c\u793e\u4f1a\u63a8\u8350\u7f51\u7edc\uff0c\u800c\u795e\u7ecf\u79d1\u5b66\u8bc1\u636e\u8868\u660e\u4eba\u7c7b\u5c06\u77e5\u8bc6\u7f16\u7801\u4e3a\u5173\u7cfb\u8ba4\u77e5\u56fe\u8c31\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5efa\u6a21\u7ed3\u6784\u5316\u5173\u7cfb\u7684\u8de8\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "SLIP\u65b9\u6cd5\u6574\u5408\u4e86\u7ed3\u6784\u5316\u5bf9\u6bd4\u635f\u5931\uff0c\u5728\u4fdd\u6301\u6a21\u6001\u5bf9\u9f50\u7684\u540c\u65f6\u5efa\u6a21\u7ed3\u6784\u5316\u56fe\u4e2d\u76f8\u90bb\u5b9e\u4f53\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u4e9a\u9a6c\u900a\u4ea7\u54c1\u5171\u8d2d\u591a\u6a21\u6001\u56fe\u6570\u636e\u96c6\u6765\u652f\u6301\u7ed3\u6784\u5316\u8de8\u6a21\u6001\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSLIP\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8eCLIP\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5173\u7cfb\u76d1\u7763\u5bf9\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u5173\u7cfb\u76d1\u7763\u5728\u8de8\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5229\u7528\u9886\u57df\u7279\u5b9a\u5173\u7cfb\u7ed3\u6784\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5e76\u5c55\u793a\u4e86\u5173\u7cfb\u8ba4\u77e5\u56fe\u8c31\u5728\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.03156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03156", "abs": "https://arxiv.org/abs/2511.03156", "authors": ["Sagar Shrestha", "Gopal Sharma", "Luowei Zhou", "Suren Kumar"], "title": "Finetuning-Free Personalization of Text to Image Generation via Hypernetworks", "comment": null, "summary": "Personalizing text-to-image diffusion models has traditionally relied on\nsubject-specific fine-tuning approaches such as\nDreamBooth~\\cite{ruiz2023dreambooth}, which are computationally expensive and\nslow at inference. Recent adapter- and encoder-based methods attempt to reduce\nthis overhead but still depend on additional fine-tuning or large backbone\nmodels for satisfactory results. In this work, we revisit an orthogonal\ndirection: fine-tuning-free personalization via Hypernetworks that predict\nLoRA-adapted weights directly from subject images. Prior hypernetwork-based\napproaches, however, suffer from costly data generation or unstable attempts to\nmimic base model optimization trajectories. We address these limitations with\nan end-to-end training objective, stabilized by a simple output regularization,\nyielding reliable and effective hypernetworks. Our method removes the need for\nper-subject optimization at test time while preserving both subject fidelity\nand prompt alignment. To further enhance compositional generalization at\ninference time, we introduce Hybrid-Model Classifier-Free Guidance (HM-CFG),\nwhich combines the compositional strengths of the base diffusion model with the\nsubject fidelity of personalized models during sampling. Extensive experiments\non CelebA-HQ, AFHQ-v2, and DreamBench demonstrate that our approach achieves\nstrong personalization performance and highlights the promise of hypernetworks\nas a scalable and effective direction for open-category personalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u514d\u5fae\u8c03\u4e2a\u6027\u5316\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u76ee\u6807\u9884\u6d4bLoRA\u9002\u914d\u6743\u91cd\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u6bcf\u4e2a\u4e3b\u4f53\u90fd\u9700\u8981\u4f18\u5316\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e3b\u4f53\u4fdd\u771f\u5ea6\u548c\u63d0\u793a\u5bf9\u9f50\u3002", "motivation": "\u4f20\u7edf\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2a\u6027\u5316\u65b9\u6cd5\u5982DreamBooth\u4f9d\u8d56\u4e8e\u4e3b\u4f53\u7279\u5b9a\u7684\u5fae\u8c03\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u63a8\u7406\u901f\u5ea6\u6162\u3002\u73b0\u6709\u7684\u9002\u914d\u5668\u548c\u7f16\u7801\u5668\u65b9\u6cd5\u867d\u7136\u5c1d\u8bd5\u51cf\u5c11\u5f00\u9500\uff0c\u4f46\u4ecd\u9700\u8981\u989d\u5916\u5fae\u8c03\u6216\u5927\u578b\u9aa8\u5e72\u6a21\u578b\u3002\u8d85\u7f51\u7edc\u65b9\u6cd5\u867d\u7136\u907f\u514d\u4e86\u9010\u4e3b\u4f53\u4f18\u5316\uff0c\u4f46\u9762\u4e34\u6570\u636e\u751f\u6210\u6210\u672c\u9ad8\u548c\u4f18\u5316\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u8bad\u7ec3\u76ee\u6807\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u8f93\u51fa\u6b63\u5219\u5316\u5b9e\u73b0\u7a33\u5b9a\u5316\uff0c\u6784\u5efa\u53ef\u9760\u7684\u8d85\u7f51\u7edc\u76f4\u63a5\u4ece\u4e3b\u4f53\u56fe\u50cf\u9884\u6d4bLoRA\u9002\u914d\u6743\u91cd\u3002\u5728\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u4e86\u6df7\u5408\u6a21\u578b\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff08HM-CFG\uff09\uff0c\u7ed3\u5408\u57fa\u7840\u6269\u6563\u6a21\u578b\u7684\u7ec4\u5408\u80fd\u529b\u548c\u4e2a\u6027\u5316\u6a21\u578b\u7684\u4e3b\u4f53\u4fdd\u771f\u5ea6\u8fdb\u884c\u91c7\u6837\u3002", "result": "\u5728CelebA-HQ\u3001AFHQ-v2\u548cDreamBench\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u4e2a\u6027\u5316\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8d85\u7f51\u7edc\u4f5c\u4e3a\u53ef\u6269\u5c55\u548c\u6709\u6548\u7684\u5f00\u653e\u7c7b\u522b\u4e2a\u6027\u5316\u65b9\u5411\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u6d4b\u8bd5\u65f6\u9010\u4e3b\u4f53\u4f18\u5316\u7684\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e3b\u4f53\u4fdd\u771f\u5ea6\u548c\u63d0\u793a\u5bf9\u9f50\u3002\u7814\u7a76\u8868\u660e\u8d85\u7f51\u7edc\u662f\u6784\u5efa\u53ef\u6269\u5c55\u4e2a\u6027\u5316\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u4e3a\u5f00\u653e\u7c7b\u522b\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03001", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03001", "abs": "https://arxiv.org/abs/2511.03001", "authors": ["Gyeom Hwangbo", "Hyungjoo Chae", "Minseok Kang", "Hyeonjong Ju", "Soohyun Oh", "Jinyoung Yeo"], "title": "LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation", "comment": "Work in Progress", "summary": "Despite recent progress in using Large Language Models (LLMs) for\nautomatically generating 3D scenes, generated scenes often lack realistic\nspatial layouts and object attributes found in real-world environments. As this\nproblem stems from insufficiently detailed, coarse-grained instructions,\nadvancing 3D scene synthesis guided by more detailed, fine-grained instructions\nthat reflect real-world environments becomes crucial. Without such realistic\nscenes, training embodied agents in unrealistic environments can lead them to\nlearn priors that diverge significantly from real-world physics and semantics,\ndegrading their performance when deployed. Thus, verifying the alignment\nbetween the fine-grained instruction and the generated scene is essential for\neffective learning. However, current evaluation methods, such as CLIPScore and\nvision-language models (VLMs), often fail to reliably assess such alignment.\nThis shortcoming arises primarily from their shallow understanding of 3D\nscenes, which often leads to improperly grounded scene components. To address\nthis, we introduce LEGO-Eval, an evaluation framework equipped with diverse\ntools designed to explicitly ground scene components, enabling more accurate\nalignment assessments. We also present LEGO-Bench, a benchmark of detailed\ninstructions that specify complex layouts and attributes of real-world\nenvironments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge\nby 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with\nLEGO-Bench reveals significant limitations in current generation methods.\nAcross all evaluated approaches, success rates reached at most 10% in\ngenerating scenes that fully align with fine-grained instructions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LEGO-Eval\u8bc4\u4f30\u6846\u67b6\u548cLEGO-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u751f\u62103D\u573a\u666f\u65f6\u7f3a\u4e4f\u771f\u5b9e\u7a7a\u95f4\u5e03\u5c40\u548c\u5bf9\u8c61\u5c5e\u6027\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u5730\u63a5\u5730\u573a\u666f\u7ec4\u4ef6\u6765\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u573a\u666f\u4e0e\u7ec6\u7c92\u5ea6\u6307\u4ee4\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u76843D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u5e03\u5c40\u548c\u5bf9\u8c61\u5c5e\u6027\u4e0d\u771f\u5b9e\u7684\u95ee\u9898\uff0c\u8fd9\u6e90\u4e8e\u6307\u4ee4\u7c92\u5ea6\u4e0d\u591f\u7ec6\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u573a\u666f\u4e0e\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u5b58\u5728\u504f\u5dee\uff0c\u8fdb\u800c\u5f71\u54cd\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u5177\u8eab\u667a\u80fd\u4f53\u7684\u6027\u80fd\uff0c\u800c\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u5982CLIPScore\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf93D\u573a\u666f\u7406\u89e3\u4e0d\u8db3\uff0c\u65e0\u6cd5\u53ef\u9760\u8bc4\u4f30\u573a\u666f\u4e0e\u7ec6\u7c92\u5ea6\u6307\u4ee4\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86LEGO-Eval\u8bc4\u4f30\u6846\u67b6\uff0c\u914d\u5907\u591a\u6837\u5316\u5de5\u5177\u6765\u663e\u5f0f\u5730\u63a5\u5730\u573a\u666f\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u5bf9\u9f50\u8bc4\u4f30\uff1b\u540c\u65f6\u6784\u5efa\u4e86LEGO-Bench\u57fa\u51c6\uff0c\u5305\u542b\u8be6\u7ec6\u6307\u4ee4\u4ee5\u6307\u5b9a\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u7684\u590d\u6742\u5e03\u5c40\u548c\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLEGO-Eval\u5728\u8bc4\u4f30\u573a\u666f-\u6307\u4ee4\u5bf9\u9f50\u65b9\u9762\u6bd4VLM-as-a-judge\u65b9\u6cd5\u9ad8\u51fa0.41 F1\u5206\u6570\uff1b\u4f7f\u7528LEGO-Bench\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5f53\u524d\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u6240\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u751f\u6210\u5b8c\u5168\u7b26\u5408\u7ec6\u7c92\u5ea6\u6307\u4ee4\u7684\u573a\u666f\u65f6\u6210\u529f\u7387\u6700\u9ad8\u4ec5\u4e3a10%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d3D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u6307\u4ee4\u65b9\u9762\u7684\u4e25\u91cd\u4e0d\u8db3\uff0c\u63d0\u51fa\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u51c6\u4e3a\u6539\u8fdb3D\u573a\u666f\u751f\u6210\u8d28\u91cf\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u751f\u6210\u65b9\u6cd5\u6765\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u5efa\u6a21\u7684\u9700\u6c42\u3002"}}
{"id": "2511.03137", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03137", "abs": "https://arxiv.org/abs/2511.03137", "authors": ["Shipeng Cen", "Ying Tan"], "title": "Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks", "comment": null, "summary": "As optimization problems grow increasingly complex and diverse, advancements\nin optimization techniques and paradigm innovations hold significant\nimportance. The challenges posed by optimization problems are primarily\nmanifested in their non-convexity, high-dimensionality, black-box nature, and\nother unfavorable characteristics. Traditional zero-order or first-order\nmethods, which are often characterized by low efficiency, inaccurate gradient\ninformation, and insufficient utilization of optimization information, are\nill-equipped to address these challenges effectively. In recent years, the\nrapid development of large language models (LLM) has led to substantial\nimprovements in their language understanding and code generation capabilities.\nConsequently, the design of optimization algorithms leveraging large language\nmodels has garnered increasing attention from researchers. In this study, we\nchoose the fireworks algorithm(FWA) as the basic optimizer and propose a novel\napproach to assist the design of the FWA by incorporating multi-modal large\nlanguage model(MLLM). To put it simply, we propose the concept of Critical\nPart(CP), which extends FWA to complex high-dimensional tasks, and further\nutilizes the information in the optimization process with the help of the\nmulti-modal characteristics of large language models. We focus on two specific\ntasks: the \\textit{traveling salesman problem }(TSP) and \\textit{electronic\ndesign automation problem} (EDA). The experimental results show that FWAs\ngenerated under our new framework have achieved or surpassed SOTA results on\nmany problem instances.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u7684\u70df\u82b1\u7b97\u6cd5\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5173\u952e\u90e8\u4ef6\u6982\u5ff5\u6269\u5c55\u70df\u82b1\u7b97\u6cd5\u5904\u7406\u590d\u6742\u9ad8\u7ef4\u4f18\u5316\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5728\u65c5\u884c\u5546\u95ee\u9898\u548c\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u6216\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u96f6\u9636\u6216\u4e00\u9636\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u975e\u51f8\u3001\u9ad8\u7ef4\u3001\u9ed1\u7bb1\u7b49\u590d\u6742\u4f18\u5316\u95ee\u9898\u65f6\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u3001\u68af\u5ea6\u4fe1\u606f\u4e0d\u51c6\u786e\u548c\u4f18\u5316\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u7b49\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u73b0\u4ee3\u4f18\u5316\u95ee\u9898\u7684\u6311\u6218\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u7406\u89e3\u548c\u4ee3\u7801\u751f\u6210\u80fd\u529b\u4e0a\u7684\u663e\u8457\u63d0\u5347\u4e3a\u4f18\u5316\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u672c\u7814\u7a76\u4ee5\u70df\u82b1\u7b97\u6cd5\u4e3a\u57fa\u7840\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u5173\u952e\u90e8\u4ef6\u6982\u5ff5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u7279\u6027\u5145\u5206\u6316\u6398\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u606f\uff0c\u5c06\u70df\u82b1\u7b97\u6cd5\u6269\u5c55\u5230\u590d\u6742\u9ad8\u7ef4\u4efb\u52a1\u4e2d\uff0c\u7279\u522b\u9488\u5bf9\u65c5\u884c\u5546\u95ee\u9898\u548c\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\u95ee\u9898\u8fdb\u884c\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u65b0\u6846\u67b6\u4e0b\u751f\u6210\u7684\u70df\u82b1\u7b97\u6cd5\u5728\u591a\u4e2a\u95ee\u9898\u5b9e\u4f8b\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u6216\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u590d\u6742\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f18\u5316\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u5904\u7406\u590d\u6742\u9ad8\u7ef4\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u7ed3\u5408\u4f20\u7edf\u4f18\u5316\u7b97\u6cd5\u4e0e\u5148\u8fdb\u4eba\u5de5\u667a\u80fd\u6280\u672f\u80fd\u591f\u4ea7\u751f\u534f\u540c\u6548\u5e94\uff0c\u4e3a\u672a\u6765\u4f18\u5316\u7b97\u6cd5\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2511.03178", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03178", "abs": "https://arxiv.org/abs/2511.03178", "authors": ["Shreyas C. Dhake", "Jiayuan Huang", "Runlong He", "Danyal Z. Khan", "Evangelos B. Mazomenos", "Sophia Bano", "Hani J. Marcus", "Danail Stoyanov", "Matthew J. Clarkson", "Mobarak I. Hoque"], "title": "SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention", "comment": "12 pages", "summary": "Anticipating forthcoming surgical events is vital for real-time assistance in\nendonasal transsphenoidal pituitary surgery, where visibility is limited and\nworkflow changes rapidly. Most visual question answering (VQA) systems reason\non isolated frames with static vision language alignment, providing little\nsupport for forecasting next steps or instrument needs. Existing surgical VQA\ndatasets likewise center on the current scene rather than the near future. We\nintroduce PitVQA-Anticipation, the first VQA dataset designed for forward\nlooking surgical reasoning. It comprises 33.5 hours of operative video and\n734,769 question answer pairs built from temporally grouped clips and expert\nannotations across four tasks: predicting the future phase, next step, upcoming\ninstrument, and remaining duration. We further propose SurgAnt-ViVQA, a video\nlanguage model that adapts a large language model using a GRU Gated Temporal\nCross-Attention module. A bidirectional GRU encodes frame to frame dynamics,\nwhile an adaptive gate injects visual context into the language stream at the\ntoken level. Parameter efficient fine tuning customizes the language backbone\nto the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and\nEndoVis datasets, surpassing strong image and video based baselines. Ablations\nshow that temporal recurrence and gated fusion drive most of the gains. A frame\nbudget study indicates a trade-off: 8 frames maximize fluency, whereas 32\nframes slightly reduce BLEU but improve numeric time estimation. By pairing a\ntemporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA\nadvances surgical VQA from retrospective description to proactive anticipation.\nPitVQA-Anticipation offers a comprehensive benchmark for this setting and\nhighlights the importance of targeted temporal modeling for reliable, future\naware surgical assistance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PitVQA-Anticipation\u6570\u636e\u96c6\u548cSurgAnt-ViVQA\u6a21\u578b\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u624b\u672f\u9884\u89c1\u6027\u63a8\u7406\u7684\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u65f6\u5e8f\u5efa\u6a21\u548c\u95e8\u63a7\u878d\u5408\u673a\u5236\u5b9e\u73b0\u4e86\u4ece\u56de\u987e\u6027\u63cf\u8ff0\u5411\u524d\u77bb\u6027\u9884\u6d4b\u7684\u8f6c\u53d8\u3002", "motivation": "\u73b0\u6709\u624b\u672f\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u5b64\u7acb\u5e27\u7684\u9759\u6001\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\uff0c\u7f3a\u4e4f\u5bf9\u672a\u6765\u6b65\u9aa4\u6216\u5668\u68b0\u9700\u6c42\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u800c\u73b0\u6709\u624b\u672fVQA\u6570\u636e\u96c6\u4e5f\u96c6\u4e2d\u4e8e\u5f53\u524d\u573a\u666f\u800c\u975e\u8fd1\u671f\u672a\u6765\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5185\u7aa5\u955c\u7ecf\u9f3b\u8776\u5782\u4f53\u624b\u672f\u7b49\u89c6\u91ce\u53d7\u9650\u3001\u5de5\u4f5c\u6d41\u5feb\u901f\u53d8\u5316\u573a\u666f\u7684\u5b9e\u65f6\u8f85\u52a9\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86SurgAnt-ViVQA\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528GRU\u95e8\u63a7\u65f6\u5e8f\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u53cc\u5411GRU\u7f16\u7801\u5e27\u95f4\u52a8\u6001\uff0c\u81ea\u9002\u5e94\u95e8\u63a7\u5728token\u7ea7\u522b\u5c06\u89c6\u89c9\u4e0a\u4e0b\u6587\u6ce8\u5165\u8bed\u8a00\u6d41\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5b9a\u5236\u8bed\u8a00\u9aa8\u5e72\u7f51\u7edc\u4ee5\u9002\u5e94\u624b\u672f\u9886\u57df\u3002", "result": "\u5728PitVQA-Anticipation\u548cEndoVis\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0cSurgAnt-ViVQA\u8d85\u8d8a\u4e86\u5f3a\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u7ebf\uff0c\u6d88\u878d\u7814\u7a76\u663e\u793a\u65f6\u5e8f\u5faa\u73af\u548c\u95e8\u63a7\u878d\u5408\u8d21\u732e\u4e86\u4e3b\u8981\u6027\u80fd\u63d0\u5347\uff0c\u5e27\u9884\u7b97\u7814\u7a76\u53d1\u73b08\u5e27\u6700\u5927\u5316\u6d41\u7545\u6027\uff0c32\u5e27\u7565\u5fae\u964d\u4f4eBLEU\u4f46\u6539\u5584\u6570\u503c\u65f6\u95f4\u4f30\u8ba1\u3002", "conclusion": "\u901a\u8fc7\u5c06\u65f6\u5e8f\u611f\u77e5\u7f16\u7801\u5668\u4e0e\u7ec6\u7c92\u5ea6\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u7ed3\u5408\uff0cSurgAnt-ViVQA\u5c06\u624b\u672fVQA\u4ece\u56de\u987e\u6027\u63cf\u8ff0\u63a8\u8fdb\u5230\u524d\u77bb\u6027\u9884\u6d4b\uff0cPitVQA-Anticipation\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0c\u5e76\u5f3a\u8c03\u4e86\u9488\u5bf9\u6027\u65f6\u5e8f\u5efa\u6a21\u5bf9\u4e8e\u53ef\u9760\u3001\u672a\u6765\u611f\u77e5\u624b\u672f\u8f85\u52a9\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.03146", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03146", "abs": "https://arxiv.org/abs/2511.03146", "authors": ["Kaiyuan Zhang", "Chenghao Yang", "Zhoufutu Wen", "Sihang Yuan", "Qiuyue Wang", "Chaoyi Huang", "Guosheng Zhu", "He Wang", "Huawenyu Lu", "Jianing Wen", "Jianpeng Jiao", "Lishu Luo", "Longxiang Liu", "Sijin Wu", "Xiaolei Zhu", "Xuanliang Zhang", "Ge Zhang", "Yi Lin", "Guang Shi", "Chaoyou Fu", "Wenhao Huang"], "title": "MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity", "comment": null, "summary": "As reasoning models scale rapidly, the essential role of multimodality in\nhuman cognition has come into sharp relief, driving a growing need to probe\nvision-centric cognitive behaviors. Yet, existing multimodal benchmarks either\noveremphasize textual reasoning or fall short of systematically capturing\nvision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs\ninsufficiently assessed. To address this limitation, we introduce MME-CC\n(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded\nbenchmark that organizes 11 representative reasoning tasks into three\nfundamental categories of visual information: spatial, geometric, and\nknowledge-based reasoning, and provides fine-grained analyses of MLLMs'\ncognitive capacity across these dimensions. Based on MME-CC, we conduct\nextensive experiments over 16 representative MLLMs. Our study reveals that\nclosed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.\n30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak\n(less than or equal to 30%). We further identify common error patterns,\nincluding orientation mistakes, fragile cross-view identity persistence, and\npoor adherence to counterfactual instructions, and observe that\nChain-of-Thought typically follows a three-stage process (extract -> reason ->\nverify) with heavy reliance on visual extraction. We hope this work catalyzes a\nshift toward treating the cognitive capacity of MLLMs as central to both\nevaluation and model design.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MME-CC\u591a\u6a21\u6001\u8ba4\u77e5\u80fd\u529b\u8bc4\u4f30\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4e2d\u5fc3\u8ba4\u77e5\u884c\u4e3a\u4e0a\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u5728\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u57fa\u51c6\u8fc7\u5ea6\u5f3a\u8c03\u6587\u672c\u63a8\u7406\uff0c\u672a\u80fd\u7cfb\u7edf\u6355\u6349\u89c6\u89c9\u4e2d\u5fc3\u8ba4\u77e5\u884c\u4e3a\uff0c\u5bfc\u81f4\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba4\u77e5\u80fd\u529b\u8bc4\u4f30\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u89c6\u89c9\u4fe1\u606f\u5904\u7406\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efa\u4e86MME-CC\u57fa\u51c6\uff0c\u5c0611\u4e2a\u4ee3\u8868\u6027\u63a8\u7406\u4efb\u52a1\u7ec4\u7ec7\u4e3a\u7a7a\u95f4\u63a8\u7406\u3001\u51e0\u4f55\u63a8\u7406\u548c\u77e5\u8bc6\u63a8\u7406\u4e09\u4e2a\u57fa\u672c\u7c7b\u522b\uff0c\u5bf916\u4e2a\u4ee3\u8868\u6027\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "result": "\u95ed\u6e90\u6a21\u578b\u8868\u73b0\u9886\u5148\uff08\u5982Gemini-2.5-Pro\u5f97\u5206\u4e3a42.66\uff0cGLM-4.5V\u4e3a30.45\uff09\uff0c\u7a7a\u95f4\u548c\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u666e\u904d\u8f83\u5f31\uff08\u226430%\uff09\uff0c\u8bc6\u522b\u51fa\u65b9\u5411\u9519\u8bef\u3001\u8de8\u89c6\u56fe\u8eab\u4efd\u6301\u7eed\u6027\u8106\u5f31\u3001\u53cd\u4e8b\u5b9e\u6307\u4ee4\u9075\u5faa\u5dee\u7b49\u5e38\u89c1\u9519\u8bef\u6a21\u5f0f\u3002", "conclusion": "\u601d\u7ef4\u94fe\u901a\u5e38\u9075\u5faa\u63d0\u53d6-\u63a8\u7406-\u9a8c\u8bc1\u7684\u4e09\u9636\u6bb5\u8fc7\u7a0b\u4e14\u4e25\u91cd\u4f9d\u8d56\u89c6\u89c9\u63d0\u53d6\uff0c\u7814\u7a76\u547c\u5401\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba4\u77e5\u80fd\u529b\u4f5c\u4e3a\u8bc4\u4f30\u548c\u6a21\u578b\u8bbe\u8ba1\u7684\u6838\u5fc3\u8003\u91cf\uff0c\u63a8\u52a8\u8ba4\u77e5\u80fd\u529b\u5bfc\u5411\u7684\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2511.03235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03235", "abs": "https://arxiv.org/abs/2511.03235", "authors": ["Yi-Fei Liu", "Yi-Long Lu", "Di He", "Hang Zhang"], "title": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers", "comment": null, "summary": "Psychological constructs within individuals are widely believed to be\ninterconnected. We investigated whether and how Large Language Models (LLMs)\ncan model the correlational structure of human psychological traits from\nminimal quantitative inputs. We prompted various LLMs with Big Five Personality\nScale responses from 816 human individuals to role-play their responses on nine\nother psychological scales. LLMs demonstrated remarkable accuracy in capturing\nhuman psychological structure, with the inter-scale correlation patterns from\nLLM-generated responses strongly aligning with those from human data $(R^2 >\n0.89)$. This zero-shot performance substantially exceeded predictions based on\nsemantic similarity and approached the accuracy of machine learning algorithms\ntrained directly on the dataset. Analysis of reasoning traces revealed that\nLLMs use a systematic two-stage process: First, they transform raw Big Five\nresponses into natural language personality summaries through information\nselection and compression, analogous to generating sufficient statistics.\nSecond, they generate target scale responses based on reasoning from these\nsummaries. For information selection, LLMs identify the same key personality\nfactors as trained algorithms, though they fail to differentiate item\nimportance within factors. The resulting compressed summaries are not merely\nredundant representations but capture synergistic information--adding them to\noriginal scores enhances prediction alignment, suggesting they encode emergent,\nsecond-order patterns of trait interplay. Our findings demonstrate that LLMs\ncan precisely predict individual participants' psychological traits from\nminimal data through a process of abstraction and reasoning, offering both a\npowerful tool for psychological simulation and valuable insights into their\nemergent reasoning capabilities.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc1\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4ece\u5c11\u91cf\u4eba\u683c\u91cf\u8868\u8f93\u5165\u4e2d\u51c6\u786e\u5efa\u6a21\u4eba\u7c7b\u5fc3\u7406\u7279\u8d28\u7684\u5173\u8054\u7ed3\u6784\uff0c\u901a\u8fc7\u62bd\u8c61\u548c\u63a8\u7406\u8fc7\u7a0b\u5b9e\u73b0\u96f6\u6837\u672c\u5fc3\u7406\u6a21\u62df\uff0c\u5176\u6027\u80fd\u63a5\u8fd1\u5728\u6570\u636e\u96c6\u4e0a\u76f4\u63a5\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u4ece\u6700\u5c0f\u5316\u7684\u5b9a\u91cf\u8f93\u5165\u4e2d\u5efa\u6a21\u4eba\u7c7b\u5fc3\u7406\u7279\u8d28\u7684\u5173\u8054\u7ed3\u6784\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u4e13\u4e1a\u77e5\u8bc6\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u63ed\u793aLLMs\u5728\u5fc3\u7406\u6a21\u62df\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5\uff0c\u8ba9\u591a\u79cdLLMs\u57fa\u4e8e816\u540d\u4eba\u7c7b\u4e2a\u4f53\u7684\u4e94\u5927\u6027\u683c\u91cf\u8868\u54cd\u5e94\uff0c\u5728\u4e5d\u4e2a\u5176\u4ed6\u5fc3\u7406\u91cf\u8868\u4e0a\u8fdb\u884c\u89d2\u8272\u626e\u6f14\u54cd\u5e94\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u63ed\u793aLLMs\u4f7f\u7528\u7684\u4e24\u9636\u6bb5\u5904\u7406\u8fc7\u7a0b\uff1a\u4fe1\u606f\u9009\u62e9\u538b\u7f29\u548c\u57fa\u4e8e\u6458\u8981\u7684\u63a8\u7406\u3002", "result": "LLMs\u5728\u6355\u6349\u4eba\u7c7b\u5fc3\u7406\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u751f\u6210\u54cd\u5e94\u4e0e\u4eba\u7c7b\u6570\u636e\u95f4\u7684\u91cf\u8868\u95f4\u76f8\u5173\u6027\u6a21\u5f0f\u9ad8\u5ea6\u4e00\u81f4\uff08R\u00b2 > 0.89\uff09\uff0c\u96f6\u6837\u672c\u6027\u80fd\u663e\u8457\u8d85\u8fc7\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u9884\u6d4b\uff0c\u63a5\u8fd1\u76f4\u63a5\u5728\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7cbe\u5ea6\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0LLMs\u901a\u8fc7\u62bd\u8c61\u548c\u63a8\u7406\u8fc7\u7a0b\u80fd\u591f\u7cbe\u786e\u9884\u6d4b\u4e2a\u4f53\u5fc3\u7406\u7279\u8d28\uff0c\u5176\u751f\u6210\u7684\u538b\u7f29\u6458\u8981\u4e0d\u4ec5\u6355\u83b7\u4e86\u534f\u540c\u4fe1\u606f\uff0c\u8fd8\u7f16\u7801\u4e86\u7279\u8d28\u4e92\u52a8\u7684\u6d8c\u73b0\u4e8c\u9636\u6a21\u5f0f\uff0c\u4e3a\u5fc3\u7406\u6a21\u62df\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u540c\u65f6\u63ed\u793a\u4e86LLMs\u7684\u6d8c\u73b0\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.03194", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03194", "abs": "https://arxiv.org/abs/2511.03194", "authors": ["Le Xue", "Gang Feng", "Wenbo Zhang", "Yichi Zhang", "Lanlan Li", "Shuqi Wang", "Liling Peng", "Sisi Peng", "Xin Gao"], "title": "PETWB-REP: A Multi-Cancer Whole-Body FDG PET/CT and Radiology Report Dataset for Medical Imaging Research", "comment": null, "summary": "Publicly available, large-scale medical imaging datasets are crucial for\ndeveloping and validating artificial intelligence models and conducting\nretrospective clinical research. However, datasets that combine functional and\nanatomical imaging with detailed clinical reports across multiple cancer types\nremain scarce. Here, we present PETWB-REP, a curated dataset comprising\nwhole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography/Computed\nTomography (PET/CT) scans and corresponding radiology reports from 490 patients\ndiagnosed with various malignancies. The dataset primarily includes common\ncancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and\novarian cancer. This dataset includes paired PET and CT images, de-identified\ntextual reports, and structured clinical metadata. It is designed to support\nresearch in medical imaging, radiomics, artificial intelligence, and\nmulti-modal learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86PETWB-REP\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b490\u540d\u591a\u79cd\u764c\u75c7\u60a3\u8005\u5168\u8eabFDG PET/CT\u626b\u63cf\u548c\u76f8\u5e94\u653e\u5c04\u5b66\u62a5\u544a\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301\u533b\u5b66\u5f71\u50cf\u3001\u653e\u5c04\u7ec4\u5b66\u548c\u591a\u6a21\u6001\u5b66\u4e60\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7ed3\u5408\u529f\u80fd\u4e0e\u89e3\u5256\u6210\u50cf\u53ca\u8be6\u7ec6\u4e34\u5e8a\u62a5\u544a\u7684\u591a\u764c\u79cd\u516c\u5f00\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u5f00\u53d1\u548c\u9a8c\u8bc1\u4ee5\u53ca\u56de\u987e\u6027\u4e34\u5e8a\u7814\u7a76\u7684\u5f00\u5c55\u3002", "method": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u5168\u8eab18F-FDG PET/CT\u626b\u63cf\u3001\u914d\u5bf9\u7684PET\u548cCT\u56fe\u50cf\u3001\u53bb\u6807\u8bc6\u5316\u7684\u6587\u672c\u62a5\u544a\u4ee5\u53ca\u7ed3\u6784\u5316\u7684\u4e34\u5e8a\u5143\u6570\u636e\uff0c\u6db5\u76d6\u80ba\u764c\u3001\u809d\u764c\u3001\u4e73\u817a\u764c\u3001\u524d\u5217\u817a\u764c\u548c\u5375\u5de2\u764c\u7b49\u5e38\u89c1\u764c\u75c7\u7c7b\u578b\u3002", "result": "\u6570\u636e\u96c6\u6210\u529f\u6574\u5408\u4e86490\u540d\u60a3\u8005\u7684\u591a\u79cd\u6076\u6027\u80bf\u7624\u5f71\u50cf\u6570\u636e\u548c\u4e34\u5e8a\u4fe1\u606f\uff0c\u63d0\u4f9b\u4e86\u914d\u5bf9\u7684PET-CT\u56fe\u50cf\u3001\u653e\u5c04\u5b66\u62a5\u544a\u548c\u7ed3\u6784\u5316\u5143\u6570\u636e\uff0c\u4e3a\u591a\u6a21\u6001\u533b\u5b66AI\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002", "conclusion": "PETWB-REP\u6570\u636e\u96c6\u586b\u8865\u4e86\u591a\u764c\u79cd\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u6570\u636e\u7684\u7a7a\u767d\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u3001\u653e\u5c04\u7ec4\u5b66\u7814\u7a76\u3001\u4eba\u5de5\u667a\u80fd\u7b97\u6cd5\u5f00\u53d1\u548c\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5c06\u63a8\u52a8\u7cbe\u51c6\u533b\u7597\u548c\u4e34\u5e8a\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.03180", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03180", "abs": "https://arxiv.org/abs/2511.03180", "authors": ["Shahriyar Zaman Ridoy", "Azmine Toushik Wasi", "Koushik Ahamed Tonmoy"], "title": "BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture", "comment": "This manuscript is a preprint currently under review", "summary": "As multilingual Large Language Models (LLMs) gain traction across South Asia,\ntheir alignment with local ethical norms, particularly for Bengali, which is\nspoken by over 285 million people and ranked 6th globally, remains\nunderexplored. Existing ethics benchmarks are largely English-centric and\nshaped by Western frameworks, overlooking cultural nuances critical for\nreal-world deployment. To address this, we introduce BengaliMoralBench, the\nfirst large-scale ethics benchmark for the Bengali language and socio-cultural\ncontexts. It covers five moral domains, Daily Activities, Habits, Parenting,\nFamily Relationships, and Religious Activities, subdivided into 50 culturally\nrelevant subtopics. Each scenario is annotated via native-speaker consensus\nusing three ethical lenses: Virtue, Commonsense, and Justice ethics. We conduct\nsystematic zero-shot evaluation of prominent multilingual LLMs, including\nLlama, Gemma, Qwen, and DeepSeek, using a unified prompting protocol and\nstandard metrics. Performance varies widely (50-91% accuracy), with qualitative\nanalysis revealing consistent weaknesses in cultural grounding, commonsense\nreasoning, and moral fairness. BengaliMoralBench provides a foundation for\nresponsible localization, enabling culturally aligned evaluation and supporting\nthe deployment of ethically robust AI in diverse, low-resource multilingual\nsettings such as Bangladesh.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BengaliMoralBench\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u7684\u5927\u89c4\u6a21\u4f26\u7406\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u4f26\u7406\u5bf9\u9f50\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u5357\u4e9a\u5730\u533a\u7684\u8d1f\u8d23\u4efbAI\u90e8\u7f72\u63d0\u4f9b\u4e86\u8bc4\u4f30\u57fa\u7840\u3002", "motivation": "\u968f\u7740\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5357\u4e9a\u5730\u533a\u7684\u666e\u53ca\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e0e\u5f53\u5730\u4f26\u7406\u89c4\u8303\u7684\u5951\u5408\u5ea6\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5168\u7403\u4f7f\u7528\u4eba\u6570\u6392\u540d\u7b2c\u516d\u7684\u5b5f\u52a0\u62c9\u8bed\u3002\u73b0\u6709\u7684\u4f26\u7406\u57fa\u51c6\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\u548c\u897f\u65b9\u6846\u67b6\uff0c\u5ffd\u89c6\u4e86\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u5bf9\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86\u6db5\u76d6\u4e94\u4e2a\u9053\u5fb7\u9886\u57df\u7684\u5927\u89c4\u6a21\u4f26\u7406\u57fa\u51c6\uff0c\u5305\u62ec\u65e5\u5e38\u6d3b\u52a8\u3001\u4e60\u60ef\u3001\u80b2\u513f\u3001\u5bb6\u5ead\u5173\u7cfb\u548c\u5b97\u6559\u6d3b\u52a8\uff0c\u7ec6\u5206\u4e3a50\u4e2a\u6587\u5316\u76f8\u5173\u5b50\u4e3b\u9898\u3002\u6bcf\u4e2a\u573a\u666f\u901a\u8fc7\u6bcd\u8bed\u8005\u5171\u8bc6\u8fdb\u884c\u6807\u6ce8\uff0c\u91c7\u7528\u7f8e\u5fb7\u4f26\u7406\u3001\u5e38\u8bc6\u4f26\u7406\u548c\u6b63\u4e49\u4f26\u7406\u4e09\u79cd\u4f26\u7406\u89c6\u89d2\uff0c\u5e76\u5bf9\u4e3b\u6d41\u591a\u8bed\u8a00LLM\u8fdb\u884c\u7cfb\u7edf\u6027\u96f6\u6837\u672c\u8bc4\u4f30\u3002", "result": "\u4e0d\u540c\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u51c6\u786e\u7387\u8303\u56f4\u4e3a50-91%\u3002\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u5728\u6587\u5316\u57fa\u7840\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u9053\u5fb7\u516c\u5e73\u6027\u65b9\u9762\u5b58\u5728\u4e00\u81f4\u7684\u5f31\u70b9\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5bf9\u5b5f\u52a0\u62c9\u6587\u5316\u80cc\u666f\u7684\u7406\u89e3\u4ecd\u6709\u660e\u663e\u4e0d\u8db3\u3002", "conclusion": "BengaliMoralBench\u4e3a\u8d1f\u8d23\u4efb\u7684\u672c\u571f\u5316\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u652f\u6301\u5728\u591a\u6837\u5316\u3001\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u8fdb\u884c\u6587\u5316\u5bf9\u9f50\u8bc4\u4f30\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5f00\u53d1\u6587\u5316\u654f\u611f\u7684\u4f26\u7406\u57fa\u51c6\u5bf9\u4e8e\u5728\u975e\u897f\u65b9\u73af\u5883\u4e2d\u90e8\u7f72\u7a33\u5065AI\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u591a\u8bed\u8a00\u4f26\u7406\u5bf9\u9f50\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.03471", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.03471", "abs": "https://arxiv.org/abs/2511.03471", "authors": ["Ming Gu", "Ziwei Wang", "Sicen Lai", "Zirui Gao", "Sheng Zhou", "Jiajun Bu"], "title": "Towards Scalable Web Accessibility Audit with MLLMs as Copilots", "comment": "15 pages. Accepted by AAAI 2026 AISI", "summary": "Ensuring web accessibility is crucial for advancing social welfare, justice,\nand equality in digital spaces, yet the vast majority of website user\ninterfaces remain non-compliant, due in part to the resource-intensive and\nunscalable nature of current auditing practices. While WCAG-EM offers a\nstructured methodology for site-wise conformance evaluation, it involves great\nhuman efforts and lacks practical support for execution at scale. In this work,\nwe present an auditing framework, AAA, which operationalizes WCAG-EM through a\nhuman-AI partnership model. AAA is anchored by two key innovations: GRASP, a\ngraph-based multimodal sampling method that ensures representative page\ncoverage via learned embeddings of visual, textual, and relational cues; and\nMaC, a multimodal large language model-based copilot that supports auditors\nthrough cross-modal reasoning and intelligent assistance in high-effort tasks.\nTogether, these components enable scalable, end-to-end web accessibility\nauditing, empowering human auditors with AI-enhanced assistance for real-world\nimpact. We further contribute four novel datasets designed for benchmarking\ncore stages of the audit pipeline. Extensive experiments demonstrate the\neffectiveness of our methods, providing insights that small-scale language\nmodels can serve as capable experts when fine-tuned.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AAA\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u6a21\u5f0f\u5b9e\u73b0\u53ef\u6269\u5c55\u7684Web\u53ef\u8bbf\u95ee\u6027\u5ba1\u8ba1\uff0c\u6838\u5fc3\u521b\u65b0\u5305\u62ec\u57fa\u4e8e\u56fe\u7684\u591a\u6a21\u6001\u91c7\u6837\u65b9\u6cd5GRASP\u548c\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u52a9\u624bMaC\uff0c\u80fd\u591f\u7aef\u5230\u7aef\u5730\u652f\u6301\u5927\u89c4\u6a21\u7f51\u7ad9\u53ef\u8bbf\u95ee\u6027\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u7f51\u7ad9\u7528\u6237\u754c\u9762\u7684\u53ef\u8bbf\u95ee\u6027\u5408\u89c4\u6027\u666e\u904d\u4e0d\u8db3\uff0c\u4e3b\u8981\u7531\u4e8e\u73b0\u6709\u5ba1\u8ba1\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u96be\u4ee5\u6269\u5c55\uff0cWCAG-EM\u6807\u51c6\u867d\u7136\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6295\u5165\u4e14\u7f3a\u4e4f\u89c4\u6a21\u5316\u6267\u884c\u7684\u5b9e\u9645\u652f\u6301\u3002", "method": "AAA\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1aGRASP\u57fa\u4e8e\u56fe\u7684\u591a\u6a21\u6001\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u89c6\u89c9\u3001\u6587\u672c\u548c\u5173\u7cfb\u7ebf\u7d22\u7684\u5d4c\u5165\u8868\u793a\u786e\u4fdd\u4ee3\u8868\u6027\u9875\u9762\u8986\u76d6\uff1bMaC\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f85\u52a9\u7cfb\u7edf\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u63a8\u7406\u4e3a\u5ba1\u8ba1\u5458\u63d0\u4f9b\u667a\u80fd\u534f\u52a9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u5ba1\u8ba1\u6d41\u7a0b\u7684\u6838\u5fc3\u9636\u6bb5\u8d21\u732e\u4e86\u56db\u4e2a\u65b0\u9896\u7684\u6570\u636e\u96c6\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u7ecf\u8fc7\u5fae\u8c03\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u80dc\u4efb\u4e13\u5bb6\u89d2\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u6a21\u5f0f\u5728Web\u53ef\u8bbf\u95ee\u6027\u5ba1\u8ba1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u53ef\u8bbf\u95ee\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u95e8\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.03206", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03206", "abs": "https://arxiv.org/abs/2511.03206", "authors": ["Kuei-Chun Kao", "Hsu Tzu-Yin", "Yunqi Hong", "Ruochen Wang", "Cho-Jui Hsieh"], "title": "QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models", "comment": "16 pages", "summary": "Recently, Multimodal Large Language Models (MLLMs) encounter two key issues\nin multi-image contexts: (1) a lack of fine-grained perception across disparate\nimages, and (2) a diminished capability to effectively reason over and\nsynthesize information from multiple visual inputs. However, while various\nprompting methods aim to describe visual content, many existing studies focus\nprimarily on single-image settings or specific, constrained scenarios. This\nleaves a critical gap in understanding and addressing how MLLMs tackle more\ngeneral and complex multi-image reasoning tasks. Thus, we first extensively\ninvestigate how current prompting methods perceive fine-grained visual details\nand process visual information when dealing with multiple images. Our findings\nreveal that existing prompting methods fall short in attending to needed clues\nand seamlessly integrating perception and reasoning. Inspired by the findings,\nwe propose a new zero-shot prompting method, Question-Guided Chain-of-Captions\n(QG-CoC), a generalized prompting approach that effectively handles problems\nwith an arbitrary number of images. We evaluate our method on various\nopen-source and closed-source MLLMs for multi-image and single-image\nbenchmarks. Experimental results indicate that QG-CoC demonstrates competitive\nperformance across tasks and exhibits robust improvements in the challenging\nscenarios where existing prompting methods fail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5QG-CoC\uff0c\u901a\u8fc7\u95ee\u9898\u5f15\u5bfc\u7684\u6807\u9898\u94fe\u673a\u5236\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u573a\u666f\u4e0b\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u548c\u8de8\u56fe\u50cf\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u7ade\u4e89\u4f18\u52bf\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u73af\u5883\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u7f3a\u4e4f\u5bf9\u5206\u6563\u56fe\u50cf\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u591a\u89c6\u89c9\u8f93\u5165\u4e0a\u8fdb\u884c\u6709\u6548\u63a8\u7406\u548c\u4fe1\u606f\u5408\u6210\u7684\u80fd\u529b\u4e0b\u964d\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u56fe\u50cf\u8bbe\u7f6e\u6216\u7279\u5b9a\u53d7\u9650\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u901a\u7528\u590d\u6742\u591a\u56fe\u50cf\u63a8\u7406\u4efb\u52a1\u7684\u7406\u89e3\u548c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5QG-CoC\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u95ee\u9898\u5f15\u5bfc\u7684\u6807\u9898\u94fe\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u4efb\u610f\u6570\u91cf\u56fe\u50cf\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u5bf9\u591a\u56fe\u50cf\u573a\u666f\u8fdb\u884c\u7cfb\u7edf\u6027\u8c03\u67e5\uff0c\u53d1\u73b0\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\u5728\u5173\u6ce8\u6240\u9700\u7ebf\u7d22\u548c\u65e0\u7f1d\u6574\u5408\u611f\u77e5\u4e0e\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8fdb\u800c\u8bbe\u8ba1\u51fa\u8fd9\u79cd\u901a\u7528\u7684\u63d0\u793a\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u5f00\u6e90\u548c\u95ed\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6db5\u76d6\u591a\u56fe\u50cf\u548c\u5355\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQG-CoC\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u7ade\u4e89\u4f18\u52bf\uff0c\u5e76\u5728\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\u5931\u6548\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "QG-CoC\u65b9\u6cd5\u4e3a\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u63a8\u7406\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u591a\u56fe\u50cf\u573a\u666f\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u6f5c\u529b\u3002\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u5f3a\u8c03\u4e86\u7ec6\u7c92\u5ea6\u611f\u77e5\u4e0e\u63a8\u7406\u8fc7\u7a0b\u65e0\u7f1d\u6574\u5408\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.03328", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03328", "abs": "https://arxiv.org/abs/2511.03328", "authors": ["Jindong Hong", "Tianjie Chen", "Lingjie Luo", "Chuanyang Zheng", "Ting Xu", "Haibao Yu", "Jianing Qiu", "Qianzhong Chen", "Suning Huang", "Yan Xu", "Yong Gui", "Yijun He", "Jiankai Sun"], "title": "Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks", "comment": null, "summary": "A recent advancement in Multimodal Large Language Models (MLLMs) research is\nthe emergence of \"reasoning MLLMs\" that offer explicit control over their\ninternal thinking processes (normally referred as the \"thinking mode\")\nalongside the standard \"non-thinking mode\". This capability allows these models\nto engage in a step-by-step process of internal deliberation before generating\na final response. With the rapid transition to and adoption of these\n\"dual-state\" MLLMs, this work rigorously evaluated how the enhanced reasoning\nprocesses of these MLLMs impact model performance and reliability in clinical\ntasks. This paper evaluates the active \"thinking mode\" capabilities of two\nleading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We\nassessed their performance on four visual medical tasks using VQA-RAD and\nROCOv2 datasets. Our findings reveal that the improvement from activating the\nthinking mode remains marginal compared to the standard non-thinking mode for\nthe majority of the tasks. Their performance on complex medical tasks such as\nopen-ended VQA and medical image interpretation remains suboptimal,\nhighlighting the need for domain-specific medical data and more advanced\nmethods for medical knowledge integration.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u4efb\u52a1\u4e2d\u6fc0\u6d3b\u601d\u7ef4\u6a21\u5f0f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u601d\u7ef4\u6a21\u5f0f\u76f8\u6bd4\u6807\u51c6\u6a21\u5f0f\u4ec5\u5e26\u6765\u8fb9\u9645\u6539\u8fdb\uff0c\u5728\u590d\u6742\u533b\u7597\u4efb\u52a1\u4e2d\u8868\u73b0\u4ecd\u4e0d\u7406\u60f3\u3002", "motivation": "\u968f\u7740\u5177\u5907'\u53cc\u72b6\u6001'\u80fd\u529b\u7684\u63a8\u7406\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u672c\u7814\u7a76\u65e8\u5728\u4e25\u683c\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u589e\u5f3a\u7684\u63a8\u7406\u8fc7\u7a0b\u5982\u4f55\u5f71\u54cd\u5176\u5728\u4e34\u5e8a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u7279\u522b\u5173\u6ce8\u601d\u7ef4\u6a21\u5f0f\u6fc0\u6d3b\u5bf9\u533b\u7597\u5e94\u7528\u7684\u5b9e\u8d28\u6027\u63d0\u5347\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86Seed1.5-VL\u548cGemini-2.5-Flash\u4e24\u4e2a\u9886\u5148\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e3b\u52a8\u601d\u7ef4\u6a21\u5f0f\u80fd\u529b\uff0c\u5728\u89c6\u89c9\u533b\u7597\u4efb\u52a1\u4e2d\u4f7f\u7528VQA-RAD\u548cROCOv2\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u6db5\u76d6\u56db\u4e2a\u4e0d\u540c\u7684\u89c6\u89c9\u533b\u7597\u4efb\u52a1\u7c7b\u578b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u4e8e\u5927\u591a\u6570\u4efb\u52a1\u800c\u8a00\uff0c\u6fc0\u6d3b\u601d\u7ef4\u6a21\u5f0f\u76f8\u6bd4\u6807\u51c6\u975e\u601d\u7ef4\u6a21\u5f0f\u4ec5\u5e26\u6765\u8fb9\u9645\u6027\u80fd\u6539\u8fdb\uff0c\u5728\u5f00\u653e\u5f0f\u89c6\u89c9\u95ee\u7b54\u548c\u533b\u5b66\u56fe\u50cf\u89e3\u91ca\u7b49\u590d\u6742\u533b\u7597\u4efb\u52a1\u4e2d\u8868\u73b0\u4ecd\u7136\u6b20\u4f73\uff0c\u672a\u80fd\u8fbe\u5230\u7406\u60f3\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u9700\u8981\u9886\u57df\u7279\u5b9a\u7684\u533b\u5b66\u6570\u636e\u548c\u66f4\u5148\u8fdb\u7684\u533b\u5b66\u77e5\u8bc6\u96c6\u6210\u65b9\u6cd5\uff0c\u5f53\u524d\u601d\u7ef4\u6a21\u5f0f\u5728\u590d\u6742\u4e34\u5e8a\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u6709\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6539\u8fdb\u3002"}}
{"id": "2511.03260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03260", "abs": "https://arxiv.org/abs/2511.03260", "authors": ["Rong Wu", "Yim-Sang Yu"], "title": "Enhancing Medical Image Segmentation via Heat Conduction Equation", "comment": null, "summary": "Medical image segmentation has been significantly advanced by deep learning\narchitectures, notably U-Net variants. However, existing models struggle to\nachieve efficient global context modeling and long-range dependency reasoning\nunder practical computational budgets simultaneously. In this work, we propose\na novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation.\nOur model combines Mamba-based state-space modules for efficient long-range\nreasoning with Heat Conduction Operators (HCOs) in the bottleneck layers,\nsimulating frequency-domain thermal diffusion for enhanced semantic\nabstraction. Experimental results on multimodal abdominal CT and MRI datasets\ndemonstrate that the proposed model consistently outperforms strong baselines,\nvalidating its effectiveness and generalizability. It suggest that blending\nstate-space dynamics with heat-based global diffusion offers a scalable and\ninterpretable solution for medical segmentation tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408U-Mamba\u4e0e\u70ed\u4f20\u5bfc\u65b9\u7a0b\u7684\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u957f\u7a0b\u63a8\u7406\uff0c\u5e76\u5728\u74f6\u9888\u5c42\u5f15\u5165\u70ed\u4f20\u5bfc\u7b97\u5b50\u6a21\u62df\u9891\u57df\u70ed\u6269\u6563\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u5b9e\u7528\u8ba1\u7b97\u9884\u7b97\u4e0b\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u957f\u7a0b\u4f9d\u8d56\u63a8\u7406\uff0c\u7279\u522b\u662fU-Net\u53d8\u4f53\u5728\u5168\u5c40\u4fe1\u606f\u6355\u83b7\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u5408Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\u8fdb\u884c\u9ad8\u6548\u957f\u7a0b\u63a8\u7406\uff0c\u5e76\u5728\u74f6\u9888\u5c42\u5f15\u5165\u70ed\u4f20\u5bfc\u7b97\u5b50\u6a21\u62df\u9891\u57df\u70ed\u6269\u6563\u8fc7\u7a0b\uff0c\u589e\u5f3a\u8bed\u4e49\u62bd\u8c61\u80fd\u529b\u3002", "result": "\u5728\u591a\u6a21\u6001\u8179\u90e8CT\u548cMRI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u5c06\u72b6\u6001\u7a7a\u95f4\u52a8\u529b\u5b66\u4e0e\u57fa\u4e8e\u70ed\u529b\u5b66\u7684\u5168\u5c40\u6269\u6563\u76f8\u7ed3\u5408\uff0c\u4e3a\u533b\u5b66\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u6df7\u5408\u5efa\u6a21\u7b56\u7565\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.03601", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.03601", "abs": "https://arxiv.org/abs/2511.03601", "authors": ["Chao Yan", "Boyong Wu", "Peng Yang", "Pengfei Tan", "Guoqiang Hu", "Yuxin Zhang", "Xiangyu", "Zhang", "Fei Tian", "Xuerui Yang", "Xiangyu Zhang", "Daxin Jiang", "Gang Yu"], "title": "Step-Audio-EditX Technical Report", "comment": null, "summary": "We present Step-Audio-EditX, the first open-source LLM-based audio model\nexcelling at expressive and iterative audio editing encompassing emotion,\nspeaking style, and paralinguistics alongside robust zero-shot text-to-speech\n(TTS) capabilities.Our core innovation lies in leveraging only large-margin\nsynthetic data, which circumvents the need for embedding-based priors or\nauxiliary modules. This large-margin learning approach enables both iterative\ncontrol and high expressivity across voices, and represents a fundamental pivot\nfrom the conventional focus on representation-level disentanglement. Evaluation\nresults demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and\nDoubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.", "AI": {"tldr": "Step-Audio-EditX\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u5f00\u6e90\u97f3\u9891\u6a21\u578b\uff0c\u5728\u8868\u8fbe\u6027\u97f3\u9891\u7f16\u8f91\u548c\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u901a\u8fc7\u5927\u8fb9\u754c\u5408\u6210\u6570\u636e\u65b9\u6cd5\u5b9e\u73b0\u4e86\u60c5\u611f\u3001\u8bf4\u8bdd\u98ce\u683c\u548c\u526f\u8bed\u8a00\u7279\u5f81\u7684\u8fed\u4ee3\u63a7\u5236\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u97f3\u9891\u7f16\u8f91\u6a21\u578b\u5728\u8868\u8fbe\u6027\u63a7\u5236\u548c\u8fed\u4ee3\u7f16\u8f91\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u60c5\u611f\u3001\u8bf4\u8bdd\u98ce\u683c\u548c\u526f\u8bed\u8a00\u7279\u5f81\u7b49\u7ec6\u7c92\u5ea6\u63a7\u5236\u80fd\u529b\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u4f20\u7edf\u65b9\u6cd5\u5bf9\u5d4c\u5165\u5148\u9a8c\u6216\u8f85\u52a9\u6a21\u5757\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u91c7\u7528\u4ec5\u4f7f\u7528\u5927\u8fb9\u754c\u5408\u6210\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u57fa\u4e8e\u5d4c\u5165\u7684\u5148\u9a8c\u6216\u8f85\u52a9\u6a21\u5757\u7684\u9700\u6c42\uff0c\u901a\u8fc7\u5927\u8fb9\u754c\u5b66\u4e60\u5b9e\u73b0\u8de8\u58f0\u97f3\u7684\u8fed\u4ee3\u63a7\u5236\u548c\u9ad8\u8868\u8fbe\u6027\uff0c\u4ee3\u8868\u4e86\u4ece\u4f20\u7edf\u8868\u793a\u7ea7\u89e3\u7ea0\u7f20\u65b9\u6cd5\u7684\u6839\u672c\u6027\u8f6c\u53d8\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cStep-Audio-EditX\u5728\u60c5\u611f\u7f16\u8f91\u548c\u5176\u4ed6\u7ec6\u7c92\u5ea6\u63a7\u5236\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86MiniMax-2.6-hd\u548cDoubao-Seed-TTS-2.0\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8868\u8fbe\u6027\u97f3\u9891\u7f16\u8f91\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u5b9e\u73b0\u9ad8\u8d28\u91cf\u97f3\u9891\u7f16\u8f91\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u97f3\u9891\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u8868\u660e\u5927\u8fb9\u754c\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u8868\u793a\u7ea7\u89e3\u7ea0\u7f20\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u97f3\u9891\u7f16\u8f91\u7cfb\u7edf\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.03272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03272", "abs": "https://arxiv.org/abs/2511.03272", "authors": ["Shuangquan Lyu", "Steven Mao", "Yue Ma"], "title": "Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising", "comment": null, "summary": "Generating long videos remains a fundamental challenge, and achieving high\ncontrollability in video inpainting and outpainting is particularly demanding.\nTo address both of these challenges simultaneously and achieve controllable\nvideo inpainting and outpainting for long video clips, we introduce a novel and\nunified approach for long video inpainting and outpainting that extends\ntext-to-video diffusion models to generate arbitrarily long, spatially edited\nvideos with high fidelity. Our method leverages LoRA to efficiently fine-tune a\nlarge pre-trained video diffusion model like Alibaba's Wan 2.1 for masked\nregion video synthesis, and employs an overlap-and-blend temporal co-denoising\nstrategy with high-order solvers to maintain consistency across long sequences.\nIn contrast to prior work that struggles with fixed-length clips or exhibits\nstitching artifacts, our system enables arbitrarily long video generation and\nediting without noticeable seams or drift. We validate our approach on\nchallenging inpainting/outpainting tasks including editing or adding objects\nover hundreds of frames and demonstrate superior performance to baseline\nmethods like Wan 2.1 model and VACE in terms of quality (PSNR/SSIM), and\nperceptual realism (LPIPS). Our method enables practical long-range video\nediting with minimal overhead, achieved a balance between parameter efficient\nand superior performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u957f\u89c6\u9891\u4fee\u590d\u4e0e\u5916\u5ef6\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u4efb\u610f\u957f\u5ea6\u7684\u9ad8\u4fdd\u771f\u7a7a\u95f4\u7f16\u8f91\u89c6\u9891\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u5229\u7528LoRA\u9ad8\u6548\u5fae\u8c03\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u91cd\u53e0\u6df7\u5408\u65f6\u95f4\u534f\u540c\u53bb\u566a\u7b56\u7565\u4fdd\u6301\u957f\u5e8f\u5217\u4e00\u81f4\u6027\u3002", "motivation": "\u957f\u89c6\u9891\u751f\u6210\u5b58\u5728\u6839\u672c\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u89c6\u9891\u4fee\u590d\u548c\u5916\u5ef6\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u53ef\u63a7\u6027\u5c24\u4e3a\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u56fa\u5b9a\u957f\u5ea6\u7247\u6bb5\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bb9\u6613\u51fa\u73b0\u62fc\u63a5\u4f2a\u5f71\u6216\u4e00\u81f4\u6027\u6f02\u79fb\u95ee\u9898\uff0c\u96be\u4ee5\u5b9e\u73b0\u4efb\u610f\u957f\u5ea6\u7684\u65e0\u7f1d\u89c6\u9891\u7f16\u8f91\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528LoRA\u5bf9\u963f\u91cc\u5df4\u5df4Wan 2.1\u7b49\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\uff0c\u4e13\u95e8\u9488\u5bf9\u63a9\u7801\u533a\u57df\u89c6\u9891\u5408\u6210\u4efb\u52a1\u3002\u901a\u8fc7\u91cd\u53e0\u6df7\u5408\u65f6\u95f4\u534f\u540c\u53bb\u566a\u7b56\u7565\u7ed3\u5408\u9ad8\u9636\u6c42\u89e3\u5668\uff0c\u786e\u4fdd\u957f\u5e8f\u5217\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u907f\u514d\u51fa\u73b0\u53ef\u89c1\u7684\u63a5\u7f1d\u6216\u6f02\u79fb\u73b0\u8c61\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4fee\u590d\u548c\u5916\u5ef6\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u5305\u62ec\u6570\u767e\u5e27\u7684\u5bf9\u8c61\u7f16\u8f91\u548c\u6dfb\u52a0\u3002\u5728\u8d28\u91cf\u6307\u6807\uff08PSNR/SSIM\uff09\u548c\u611f\u77e5\u771f\u5b9e\u6027\uff08LPIPS\uff09\u65b9\u9762\u5747\u4f18\u4e8eWan 2.1\u6a21\u578b\u548cVACE\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u6548\u7387\u4e0e\u6027\u80fd\u4f18\u8d8a\u6027\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u9645\u957f\u8303\u56f4\u89c6\u9891\u7f16\u8f91\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u3002\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u6709\u6548\u6269\u5c55\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u4e3a\u4efb\u610f\u957f\u5ea6\u89c6\u9891\u7684\u7a7a\u95f4\u7f16\u8f91\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5728\u53c2\u6570\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2511.03635", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03635", "abs": "https://arxiv.org/abs/2511.03635", "authors": ["Apoorva Upadhyaya", "Wolfgang Nejdl", "Marco Fisichella"], "title": "Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability", "comment": "Accepted in AAAI CONFERENCE ON WEB AND SOCIAL MEDIA (ICWSM 2026)", "summary": "Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward\nunseen targets. Existing research using contrastive, meta-learning, or data\naugmentation suffers from generalizability issues or lack of coherence between\ntext and target. Recent works leveraging large language models (LLMs) for ZSSD\nfocus either on improving unseen target-specific knowledge or generating\nexplanations for stance analysis. However, most of these works are limited by\ntheir over-reliance on explicit reasoning, provide coarse explanations that\nlack nuance, and do not explicitly model the reasoning process, making it\ndifficult to interpret the model's predictions. To address these issues, in our\nstudy, we develop a novel interpretable ZSSD framework, IRIS. We provide an\ninterpretable understanding of the attitude of the input towards the target\nimplicitly based on sequences within the text (implicit rationales) and\nexplicitly based on linguistic measures (explicit rationales). IRIS considers\nstance detection as an information retrieval ranking task, understanding the\nrelevance of implicit rationales for different stances to guide the model\ntowards correct predictions without requiring the ground-truth of rationales,\nthus providing inherent interpretability. In addition, explicit rationales\nbased on communicative features help decode the emotional and cognitive\ndimensions of stance, offering an interpretable understanding of the author's\nattitude towards the given target. Extensive experiments on the benchmark\ndatasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%\ntraining data prove the generalizability of our model, benefiting from the\nproposed architecture and interpretable design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u89e3\u91ca\u96f6\u6837\u672c\u7acb\u573a\u68c0\u6d4b\u6846\u67b6IRIS\uff0c\u901a\u8fc7\u7ed3\u5408\u9690\u5f0f\u63a8\u7406\u4f9d\u636e\u548c\u663e\u5f0f\u8bed\u8a00\u5b66\u7279\u5f81\uff0c\u5728\u65e0\u9700\u771f\u5b9e\u63a8\u7406\u4f9d\u636e\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7acb\u573a\u68c0\u6d4b\uff0c\u540c\u65f6\u63d0\u4f9b\u5185\u5728\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u7acb\u573a\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u6027\u4e0d\u8db3\u3001\u6587\u672c\u4e0e\u76ee\u6807\u4e4b\u95f4\u7f3a\u4e4f\u8fde\u8d2f\u6027\u7b49\u95ee\u9898\uff0c\u4e14\u5927\u591a\u6570\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u663e\u5f0f\u63a8\u7406\u3001\u63d0\u4f9b\u7684\u89e3\u91ca\u7f3a\u4e4f\u7ec6\u5fae\u5dee\u522b\u3001\u672a\u80fd\u663e\u5f0f\u5efa\u6a21\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u6a21\u578b\u9884\u6d4b\u96be\u4ee5\u89e3\u91ca\u3002", "method": "IRIS\u6846\u67b6\u5c06\u7acb\u573a\u68c0\u6d4b\u89c6\u4e3a\u4fe1\u606f\u68c0\u7d22\u6392\u5e8f\u4efb\u52a1\uff0c\u57fa\u4e8e\u6587\u672c\u4e2d\u7684\u5e8f\u5217\uff08\u9690\u5f0f\u63a8\u7406\u4f9d\u636e\uff09\u548c\u8bed\u8a00\u5b66\u5ea6\u91cf\uff08\u663e\u5f0f\u63a8\u7406\u4f9d\u636e\uff09\u5206\u522b\u63d0\u4f9b\u9690\u5f0f\u548c\u663e\u5f0f\u7684\u7acb\u573a\u7406\u89e3\uff0c\u901a\u8fc7\u7406\u89e3\u4e0d\u540c\u7acb\u573a\u9690\u5f0f\u63a8\u7406\u4f9d\u636e\u7684\u76f8\u5173\u6027\u6765\u6307\u5bfc\u6a21\u578b\u9884\u6d4b\uff0c\u65e0\u9700\u771f\u5b9e\u63a8\u7406\u4f9d\u636e\u6807\u6ce8\u5373\u53ef\u63d0\u4f9b\u5185\u5728\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728VAST\u3001EZ-STANCE\u3001P-Stance\u548cRFD\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5373\u4f7f\u4ec5\u4f7f\u752850%\u300130%\u751a\u81f310%\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u8be5\u6a21\u578b\u4ecd\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u5f97\u76ca\u4e8e\u6240\u63d0\u51fa\u7684\u67b6\u6784\u548c\u53ef\u89e3\u91ca\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u9690\u5f0f\u548c\u663e\u5f0f\u63a8\u7406\u4f9d\u636e\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u96f6\u6837\u672c\u7acb\u573a\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u4f5c\u8005\u6001\u5ea6\u60c5\u611f\u548c\u8ba4\u77e5\u7ef4\u5ea6\u7684\u53ef\u89e3\u91ca\u7406\u89e3\uff0c\u4e3a\u53ef\u89e3\u91caAI\u5728\u7acb\u573a\u68c0\u6d4b\u9886\u57df\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.03317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03317", "abs": "https://arxiv.org/abs/2511.03317", "authors": ["Minghao Fu", "Guo-Hua Wang", "Tianyu Cui", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models", "comment": "The code is publicly available at\n  https://github.com/AIDC-AI/Diffusion-SDPO", "summary": "Text-to-image diffusion models deliver high-quality images, yet aligning them\nwith human preferences remains challenging. We revisit diffusion-based Direct\nPreference Optimization (DPO) for these models and identify a critical\npathology: enlarging the preference margin does not necessarily improve\ngeneration quality. In particular, the standard Diffusion-DPO objective can\nincrease the reconstruction error of both winner and loser branches.\nConsequently, degradation of the less-preferred outputs can become sufficiently\nsevere that the preferred branch is also adversely affected even as the margin\ngrows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule\nthat preserves the winner by adaptively scaling the loser gradient according to\nits alignment with the winner gradient. A first-order analysis yields a\nclosed-form scaling coefficient that guarantees the error of the preferred\noutput is non-increasing at each optimization step. Our method is simple,\nmodel-agnostic, broadly compatible with existing DPO-style alignment frameworks\nand adds only marginal computational overhead. Across standard text-to-image\nbenchmarks, Diffusion-SDPO delivers consistent gains over preference-learning\nbaselines on automated preference, aesthetic, and prompt alignment metrics.\nCode is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Diffusion-SDPO\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u76f4\u63a5\u504f\u597d\u4f18\u5316\u4e2d\u7684\u5173\u952e\u75c5\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7f29\u653e\u5931\u8d25\u8005\u68af\u5ea6\u6765\u4fdd\u62a4\u83b7\u80dc\u8005\u8d28\u91cf\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u57fa\u7ebf\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u75c5\u7406\uff1a\u589e\u5927\u504f\u597d\u8fb9\u754c\u5e76\u4e0d\u603b\u80fd\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u6807\u51c6Diffusion-DPO\u76ee\u6807\u4f1a\u589e\u52a0\u83b7\u80dc\u8005\u548c\u5931\u8d25\u8005\u5206\u652f\u7684\u91cd\u6784\u8bef\u5dee\uff0c\u5bfc\u81f4\u5373\u4f7f\u504f\u597d\u8fb9\u754c\u6269\u5927\uff0c\u504f\u597d\u5206\u652f\u4e5f\u4f1a\u53d7\u5230\u4e0d\u5229\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86Diffusion-SDPO\u65b9\u6cd5\uff0c\u91c7\u7528\u4fdd\u62a4\u6027\u66f4\u65b0\u89c4\u5219\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7f29\u653e\u5931\u8d25\u8005\u68af\u5ea6\u6765\u4fdd\u62a4\u83b7\u80dc\u8005\uff0c\u4e00\u9636\u5206\u6790\u5f97\u51fa\u95ed\u5f0f\u7f29\u653e\u7cfb\u6570\uff0c\u786e\u4fdd\u6bcf\u4e2a\u4f18\u5316\u6b65\u9aa4\u4e2d\u504f\u597d\u8f93\u51fa\u7684\u8bef\u5dee\u975e\u9012\u589e\uff0c\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u6a21\u578b\u65e0\u5173\u4e14\u4e0e\u73b0\u6709DPO\u98ce\u683c\u5bf9\u9f50\u6846\u67b6\u5e7f\u6cdb\u517c\u5bb9\u3002", "result": "\u5728\u6807\u51c6\u6587\u672c\u5230\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiffusion-SDPO\u5728\u81ea\u52a8\u5316\u504f\u597d\u3001\u7f8e\u5b66\u548c\u63d0\u793a\u5bf9\u9f50\u6307\u6807\u4e0a\u76f8\u6bd4\u504f\u597d\u5b66\u4e60\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4ec5\u589e\u52a0\u4e86\u8fb9\u9645\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6269\u6563\u6a21\u578b\u504f\u597d\u4f18\u5316\u4e2d\u7684\u5173\u952e\u75c5\u7406\u673a\u5236\uff0c\u63d0\u51fa\u7684\u4fdd\u62a4\u6027\u66f4\u65b0\u65b9\u6cd5\u4e3a\u6269\u6563\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6280\u672f\u8def\u5f84\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2511.03367", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03367", "abs": "https://arxiv.org/abs/2511.03367", "authors": ["Gahyeon Kim", "Sohee Kim", "Seokju Lee"], "title": "Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models", "comment": "Accepted in Pattern Recognition", "summary": "Recent advances in large-scale vision and language models have led to\nsignificant progress in zero-shot learning tasks. Methods such as CoOp and\nCoCoOp have shown that replacing handcrafted prompts with learnable vectors,\nknown as prompt learning, can result in improved performance. However, these\nmodels often struggle to generalize to entirely unseen categories. While\ntraditional zero-shot learning techniques benefit from various data\naugmentation strategies, prompt learning has primarily focused on text-based\nmodifications, leaving the potential of image-based augmentation largely\nunexplored. In this work, we explore how image-level augmentations,\nparticularly those that introduce attribute-specific variations, can support\nand enhance prompt learning. Our analysis examines the interaction between\nthese augmentations and soft prompt frameworks, revealing their potential to\nimprove generalization. We also identify a limitation in existing methods, such\nas CoCoOp, which do not provide explicit guidance for learning prompts that\nfocus on semantically meaningful visual features. To address this, we propose\nAdding Attributes to Prompt Learning, AAPL, a novel method that introduces\nadversarial token embeddings to decouple superficial visual variations\nintroduced by augmentation from class-relevant semantic representations. This\ndecoupling enables the learned prompts to concentrate on visually\ndiscriminative features that align with the target categories. We conduct\ncomprehensive experiments on eleven benchmark datasets, and AAPL consistently\noutperforms existing methods across few-shot, zero-shot, cross-dataset, and\ndomain generalization settings. Our source code is publicly available at:\nhttps://github.com/Gahyeonkim09/AAPL", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAAPL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5bf9\u6297\u6027token\u5d4c\u5165\u6765\u89e3\u8026\u56fe\u50cf\u589e\u5f3a\u5f15\u5165\u7684\u8868\u5c42\u89c6\u89c9\u53d8\u5316\u4e0e\u7c7b\u522b\u76f8\u5173\u8bed\u4e49\u8868\u793a\uff0c\u4ece\u800c\u589e\u5f3a\u63d0\u793a\u5b66\u4e60\u5728\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u572811\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u3001\u96f6\u6837\u672c\u3001\u8de8\u6570\u636e\u96c6\u548c\u9886\u57df\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5982CoOp\u548cCoCoOp\u867d\u7136\u901a\u8fc7\u53ef\u5b66\u4e60\u5411\u91cf\u66ff\u4ee3\u624b\u5de5\u63d0\u793a\u63d0\u5347\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u6027\u80fd\uff0c\u4f46\u5728\u5904\u7406\u5b8c\u5168\u672a\u89c1\u7c7b\u522b\u65f6\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u4f20\u7edf\u96f6\u6837\u672c\u5b66\u4e60\u53d7\u76ca\u4e8e\u591a\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u4f46\u63d0\u793a\u5b66\u4e60\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u5c42\u9762\u7684\u4fee\u6539\uff0c\u56fe\u50cf\u7ea7\u589e\u5f3a\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u5bf9\u8bed\u4e49\u76f8\u5173\u89c6\u89c9\u7279\u5f81\u5b66\u4e60\u7684\u663e\u5f0f\u6307\u5bfc\u3002", "method": "\u63d0\u51faAAPL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5bf9\u6297\u6027token\u5d4c\u5165\u6765\u89e3\u8026\u56fe\u50cf\u589e\u5f3a\u5f15\u5165\u7684\u8868\u5c42\u89c6\u89c9\u53d8\u5316\u4e0e\u7c7b\u522b\u76f8\u5173\u8bed\u4e49\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5c5e\u6027\u7279\u5b9a\u7684\u56fe\u50cf\u7ea7\u589e\u5f3a\uff0c\u4f7f\u5b66\u4e60\u7684\u63d0\u793a\u80fd\u591f\u4e13\u6ce8\u4e8e\u4e0e\u76ee\u6807\u7c7b\u522b\u5bf9\u9f50\u7684\u89c6\u89c9\u5224\u522b\u6027\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u8bed\u4e49\u76f8\u5173\u89c6\u89c9\u7279\u5f81\u663e\u5f0f\u6307\u5bfc\u7684\u5c40\u9650\u6027\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cAAPL\u5728\u5c11\u6837\u672c\u3001\u96f6\u6837\u672c\u3001\u8de8\u6570\u636e\u96c6\u548c\u9886\u57df\u6cdb\u5316\u8bbe\u7f6e\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5404\u79cd\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u56fe\u50cf\u7ea7\u589e\u5f3a\u4e0e\u8f6f\u63d0\u793a\u6846\u67b6\u534f\u540c\u4f5c\u7528\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u56fe\u50cf\u7ea7\u589e\u5f3a\u7279\u522b\u662f\u5c5e\u6027\u7279\u5b9a\u53d8\u5316\u80fd\u591f\u6709\u6548\u652f\u6301\u63d0\u793a\u5b66\u4e60\uff0c\u5bf9\u6297\u6027token\u5d4c\u5165\u673a\u5236\u6210\u529f\u89e3\u8026\u4e86\u8868\u5c42\u53d8\u5316\u4e0e\u8bed\u4e49\u8868\u793a\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u63d0\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u589e\u5f3a\u7b56\u7565\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u89c6\u89c9\u7279\u5f81\u4e0e\u8bed\u4e49\u5bf9\u9f50\u5728\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u589e\u5f3a\u7684\u6df7\u5408\u65b9\u6cd5\u8def\u5f84\u3002"}}
{"id": "2511.03325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03325", "abs": "https://arxiv.org/abs/2511.03325", "authors": ["Mauro Orazio Drago", "Luca Carlini", "Pelinsu Celebi Balyemez", "Dennis Pierantozzi", "Chiara Lena", "Cesare Hassan", "Danail Stoyanov", "Elena De Momi", "Sophia Bano", "Mobarak I. Hoque"], "title": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding", "comment": null, "summary": "Video Question Answering (VideoQA) in the surgical domain aims to enhance\nintraoperative understanding by enabling AI models to reason over temporally\ncoherent events rather than isolated frames. Current approaches are limited to\nstatic image features, and available datasets often lack temporal annotations,\nignoring the dynamics critical for accurate procedural interpretation. We\npropose SurgViVQA, a surgical VideoQA model that extends visual reasoning from\nstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoder\nto fuse video and question features, capturing temporal cues such as motion and\ntool--tissue interactions, which a fine-tuned large language model (LLM) then\ndecodes into coherent answers. To evaluate its performance, we curated\nREAL-Colon-VQA, a colonoscopic video dataset that includes motion-related\nquestions and diagnostic attributes, as well as out-of-template questions with\nrephrased or semantically altered formulations to assess model robustness.\nExperimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset\nshows that SurgViVQA outperforms existing image-based VQA benchmark models,\nparticularly in keyword accuracy, improving over PitVQA by +11\\% on\nREAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions\nfurther confirms improved generalizability and robustness to variations in\nquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework\nfor temporally-aware understanding in surgical VideoQA, enabling AI models to\ninterpret dynamic procedural contexts more effectively. Code and dataset\navailable at https://github.com/madratak/SurgViVQA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SurgViVQA\uff0c\u4e00\u79cd\u7528\u4e8e\u624b\u672f\u89c6\u9891\u95ee\u7b54\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u63a9\u7801\u89c6\u9891-\u6587\u672c\u7f16\u7801\u5668\u878d\u5408\u89c6\u9891\u548c\u95ee\u9898\u7279\u5f81\uff0c\u6355\u6349\u624b\u672f\u573a\u666f\u4e2d\u7684\u65f6\u95f4\u52a8\u6001\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672fVideoQA\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u624b\u672f\u89c6\u9891\u95ee\u7b54\u65b9\u6cd5\u5c40\u9650\u4e8e\u9759\u6001\u56fe\u50cf\u7279\u5f81\uff0c\u53ef\u7528\u6570\u636e\u96c6\u7f3a\u4e4f\u65f6\u95f4\u6807\u6ce8\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u624b\u672f\u8fc7\u7a0b\u4e2d\u5173\u952e\u7684\u8fd0\u52a8\u52a8\u6001\u548c\u5de5\u5177-\u7ec4\u7ec7\u4ea4\u4e92\uff0c\u9650\u5236\u4e86AI\u6a21\u578b\u5bf9\u52a8\u6001\u624b\u672f\u573a\u666f\u7684\u51c6\u786e\u7406\u89e3\u3002", "method": "SurgViVQA\u91c7\u7528\u63a9\u7801\u89c6\u9891-\u6587\u672c\u7f16\u7801\u5668\u878d\u5408\u89c6\u9891\u548c\u95ee\u9898\u7279\u5f81\uff0c\u6355\u6349\u8fd0\u52a8\u52a8\u6001\u548c\u5de5\u5177-\u7ec4\u7ec7\u4ea4\u4e92\u7b49\u65f6\u95f4\u7ebf\u7d22\uff0c\u7136\u540e\u901a\u8fc7\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u751f\u6210\u8fde\u8d2f\u7b54\u6848\uff1b\u540c\u65f6\u6784\u5efa\u4e86REAL-Colon-VQA\u6570\u636e\u96c6\uff0c\u5305\u542b\u8fd0\u52a8\u76f8\u5173\u95ee\u9898\u548c\u8bca\u65ad\u5c5e\u6027\u3002", "result": "\u5728REAL-Colon-VQA\u548cEndoVis18-VQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSurgViVQA\u5728\u5173\u952e\u8bcd\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u57fa\u51c6\u6a21\u578b\uff0c\u5206\u522b\u6bd4PitVQA\u63d0\u534711%\u548c9%\uff1b\u6270\u52a8\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6a21\u578b\u5bf9\u95ee\u9898\u8868\u8ff0\u53d8\u5316\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SurgViVQA\u548cREAL-Colon-VQA\u6570\u636e\u96c6\u4e3a\u624b\u672f\u89c6\u9891\u95ee\u7b54\u63d0\u4f9b\u4e86\u65f6\u95f4\u611f\u77e5\u7406\u89e3\u6846\u67b6\uff0c\u4f7fAI\u6a21\u578b\u80fd\u591f\u66f4\u6709\u6548\u5730\u89e3\u91ca\u52a8\u6001\u624b\u672f\u8fc7\u7a0b\uff1b\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u624b\u672fAI\u4ece\u9759\u6001\u56fe\u50cf\u5206\u6790\u5411\u52a8\u6001\u573a\u666f\u7406\u89e3\u7684\u8f6c\u53d8\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.03332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03332", "abs": "https://arxiv.org/abs/2511.03332", "authors": ["Yi Yang", "Yiming Xu", "Timo Kaiser", "Hao Cheng", "Bodo Rosenhahn", "Michael Ying Yang"], "title": "Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge", "comment": null, "summary": "In this report, we present our solution to the MOT25-Spatiotemporal Action\nGrounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately\nlocalize and track multiple objects that match specific and free-form language\nqueries, using video data of complex real-world scenes as input. We model the\nunderlying task as a video retrieval problem and present a two-stage, zero-shot\napproach, combining the advantages of the SOTA tracking model FastTracker and\nMulti-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our\nmethod achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which\nwon second place in the challenge.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eMOT25\u65f6\u7a7a\u52a8\u4f5c\u5b9a\u4f4d\u6311\u6218\u7684\u4e24\u9636\u6bb5\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u5c06\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u6a21\u578bFastTracker\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578bLLaVA-Video\u76f8\u7ed3\u5408\uff0c\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u57fa\u4e8e\u8bed\u8a00\u67e5\u8be2\u7684\u591a\u76ee\u6807\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\uff0c\u6700\u7ec8\u5728\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e8c\u540d\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3MOT25\u65f6\u7a7a\u52a8\u4f5c\u5b9a\u4f4d\u6311\u6218\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u5373\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7684\u89c6\u9891\u6570\u636e\u4e2d\uff0c\u51c6\u786e\u5730\u5bf9\u7b26\u5408\u7279\u5b9a\u548c\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u67e5\u8be2\u7684\u591a\u4e2a\u76ee\u6807\u8fdb\u884c\u5b9a\u4f4d\u548c\u8ddf\u8e2a\uff0c\u8fd9\u9700\u8981\u540c\u65f6\u5904\u7406\u89c6\u89c9\u8ddf\u8e2a\u548c\u8bed\u8a00\u7406\u89e3\u7684\u591a\u6a21\u6001\u4efb\u52a1\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u89c6\u9891\u68c0\u7d22\u95ee\u9898\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u6a21\u578bFastTracker\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578bLLaVA-Video\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u96c6\u6210\u89c6\u89c9\u8ddf\u8e2a\u80fd\u529b\u548c\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6765\u5b9e\u73b0\u57fa\u4e8e\u67e5\u8be2\u7684\u76ee\u6807\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\u3002", "result": "\u5728MOT25-StAG\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86m-HIoU\u5f97\u520620.68\u548cHOTA\u5f97\u520610.73\u7684\u4f18\u5f02\u8868\u73b0\uff0c\u8fd9\u4e00\u6210\u7ee9\u5728\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e8c\u540d\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u590d\u6742\u591a\u76ee\u6807\u65f6\u7a7a\u52a8\u4f5c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5c06\u5148\u8fdb\u7684\u8ddf\u8e2a\u6a21\u578b\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u4e24\u9636\u6bb5\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u57fa\u4e8e\u8bed\u8a00\u67e5\u8be2\u7684\u591a\u76ee\u6807\u65f6\u7a7a\u5b9a\u4f4d\u95ee\u9898\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u4e0e\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6280\u672f\u8def\u5f84\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u7cbe\u786e\u52a8\u4f5c\u5b9a\u4f4d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.03334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03334", "abs": "https://arxiv.org/abs/2511.03334", "authors": ["Guozhen Zhang", "Zixiang Zhou", "Teng Hu", "Ziqiao Peng", "Youliang Zhang", "Yi Chen", "Yuan Zhou", "Qinglin Lu", "Limin Wang"], "title": "UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions", "comment": null, "summary": "Due to the lack of effective cross-modal modeling, existing open-source\naudio-video generation methods often exhibit compromised lip synchronization\nand insufficient semantic consistency. To mitigate these drawbacks, we propose\nUniAVGen, a unified framework for joint audio and video generation. UniAVGen is\nanchored in a dual-branch joint synthesis architecture, incorporating two\nparallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent\nspace. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which\nenables bidirectional, temporally aligned cross-attention, thus ensuring\nprecise spatiotemporal synchronization and semantic consistency. Furthermore,\nthis cross-modal interaction is augmented by a Face-Aware Modulation module,\nwhich dynamically prioritizes salient regions in the interaction process. To\nenhance generative fidelity during inference, we additionally introduce\nModality-Aware Classifier-Free Guidance, a novel strategy that explicitly\namplifies cross-modal correlation signals. Notably, UniAVGen's robust joint\nsynthesis design enables seamless unification of pivotal audio-video tasks\nwithin a single model, such as joint audio-video generation and continuation,\nvideo-to-audio dubbing, and audio-driven video synthesis. Comprehensive\nexperiments validate that, with far fewer training samples (1.3M vs. 30.1M),\nUniAVGen delivers overall advantages in audio-video synchronization, timbre\nconsistency, and emotion consistency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UniAVGen\uff0c\u4e00\u79cd\u7edf\u4e00\u7684\u97f3\u9891-\u89c6\u9891\u8054\u5408\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u548c\u975e\u5bf9\u79f0\u8de8\u6a21\u6001\u4ea4\u4e92\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5507\u90e8\u540c\u6b65\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5728\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u591a\u4efb\u52a1\u7edf\u4e00\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u97f3\u9891-\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u7531\u4e8e\u7f3a\u4e4f\u6709\u6548\u7684\u8de8\u6a21\u6001\u5efa\u6a21\uff0c\u5f80\u5f80\u5b58\u5728\u5507\u90e8\u540c\u6b65\u6548\u679c\u4e0d\u4f73\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002", "method": "UniAVGen\u91c7\u7528\u53cc\u5206\u652f\u8054\u5408\u5408\u6210\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u5e76\u884c\u7684\u6269\u6563\u53d8\u6362\u5668\u6784\u5efa\u7edf\u4e00\u7684\u8de8\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\uff0c\u6838\u5fc3\u662f\u975e\u5bf9\u79f0\u8de8\u6a21\u6001\u4ea4\u4e92\u673a\u5236\u5b9e\u73b0\u53cc\u5411\u65f6\u95f4\u5bf9\u9f50\u7684\u8de8\u6ce8\u610f\u529b\uff0c\u5e76\u8f85\u4ee5\u9762\u90e8\u611f\u77e5\u8c03\u5236\u6a21\u5757\u52a8\u6001\u4f18\u5316\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u540c\u65f6\u63d0\u51fa\u6a21\u6001\u611f\u77e5\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u7b56\u7565\u589e\u5f3a\u63a8\u7406\u9636\u6bb5\u7684\u751f\u6210\u4fdd\u771f\u5ea6\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cUniAVGen\u5728\u4ec5\u4f7f\u75281.3M\u8bad\u7ec3\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u9700\u898130.1M\u6837\u672c\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u97f3\u9891-\u89c6\u9891\u540c\u6b65\u3001\u97f3\u8272\u4e00\u81f4\u6027\u548c\u60c5\u611f\u4e00\u81f4\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u6574\u4f53\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "UniAVGen\u7684\u9c81\u68d2\u8054\u5408\u5408\u6210\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5173\u952e\u97f3\u9891-\u89c6\u9891\u4efb\u52a1\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u7684\u65e0\u7f1d\u7edf\u4e00\uff0c\u5305\u62ec\u8054\u5408\u97f3\u9891-\u89c6\u9891\u751f\u6210\u4e0e\u5ef6\u7eed\u3001\u89c6\u9891\u5230\u97f3\u9891\u914d\u97f3\u4ee5\u53ca\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u5408\u6210\uff0c\u4e3a\u8de8\u6a21\u6001\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
