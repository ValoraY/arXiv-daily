<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-12-03.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 52]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 5]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 10]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-leveraging-ai-multimodal-geospatial-foundation-models-for-improved-near-real-time-flood-mapping-at-a-global-scale">[1] <a href="https://arxiv.org/abs/2512.02055">Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale</a></h3>
<p><em>Mirela G. Tulbure, Julio Caineta, Mark Broich, Mollie D. Gaines, Philippe Rufin, Leon-Friedrich Thomas, Hamed Alemohammad, Jan Hemmerling, Patrick Hostert</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é€šè¿‡å¾®è°ƒåœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹TerraMindè¿›è¡Œæ´ªæ°´èŒƒå›´åˆ¶å›¾ï¼Œé¦–æ¬¡åœ¨å…¨çƒå°ºåº¦ä¸Šè¯„ä¼°äº†GFMåœ¨æ´ªæ°´åˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†ç»“åˆå¤šæ¨¡æ€å…‰å­¦å’ŒSARæ•°æ®ä»¥åŠå¾®è°ƒç­–ç•¥åœ¨è¿‘å®æ—¶æ´ªæ°´åˆ¶å›¾ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ´ªæ°´æ˜¯æœ€å…·ç ´åæ€§çš„å¤©æ°”ç›¸å…³ç¾å®³ä¹‹ä¸€ï¼Œ2024å¹´æç«¯æ´ªæ°´äº‹ä»¶å½±å“äº†äº”å¤§æ´²çš„ç¤¾åŒºã€‚å°½ç®¡åœ°çƒè§‚æµ‹å«æ˜Ÿä¸ºæ´ªæ°´åˆ¶å›¾æä¾›äº†å…³é”®çš„é¢‘ç¹è¦†ç›–ï¼Œä½†æ“ä½œç²¾åº¦ä¸¥é‡ä¾èµ–æ ‡æ³¨æ•°æ®é›†å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚æœ€è¿‘çš„åœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡è‡ªç›‘ç£é¢„è®­ç»ƒæä¾›äº†æ”¹è¿›çš„æ³›åŒ–æ€§ï¼Œä½†å®ƒä»¬åœ¨å¤šæ ·åŒ–å…¨çƒæ´ªæ°´äº‹ä»¶ä¸Šçš„æ€§èƒ½ä»æœªè¢«å……åˆ†ç†è§£ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶ä½¿ç”¨FloodsNetæ•°æ®é›†å¾®è°ƒESA-IBMçš„TerraMindæ¨¡å‹è¿›è¡Œæ´ªæ°´èŒƒå›´åˆ¶å›¾ï¼Œè¯¥æ•°æ®é›†åŒ…å«85ä¸ªå…¨çƒæ´ªæ°´äº‹ä»¶çš„å…±ä½Sentinel-1 SARå’ŒSentinel-2å…‰å­¦å½±åƒã€‚æµ‹è¯•äº†å››ç§é…ç½®ï¼ˆåŸºç¡€vså¤§å‹æ¨¡å‹ï¼›å†»ç»“vséå†»ç»“éª¨å¹²ç½‘ç»œï¼‰ï¼Œå¹¶ä¸TerraMind Sen1Floods11ç¤ºä¾‹ä»¥åŠåœ¨FloodsNetå’ŒSen1Floods11ä¸Šè®­ç»ƒçš„U-Netè¿›è¡Œæ¯”è¾ƒã€‚</p>
<p><strong>Result:</strong> åŸºç¡€-éå†»ç»“é…ç½®åœ¨å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦å’Œå¬å›ç‡æ–¹é¢æä¾›äº†æœ€ä½³å¹³è¡¡ï¼Œè®¡ç®—æˆæœ¬æ˜¾è‘—ä½äºå¤§å‹æ¨¡å‹ã€‚å¤§å‹éå†»ç»“æ¨¡å‹å®ç°äº†æœ€é«˜å¬å›ç‡ã€‚åœ¨FloodsNetä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å¬å›ç‡ä¸Šä¼˜äºSen1Floods11è®­ç»ƒçš„ç¤ºä¾‹ï¼Œæ€»ä½“å‡†ç¡®æ€§ç›¸ä¼¼ã€‚U-Netæ¯”æ‰€æœ‰GFMé…ç½®å®ç°äº†æ›´é«˜çš„å¬å›ç‡ï¼Œä½†å‡†ç¡®æ€§å’Œç²¾ç¡®åº¦ç•¥ä½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ•´åˆå¤šæ¨¡æ€å…‰å­¦å’ŒSARæ•°æ®å¹¶å¾®è°ƒGFMå¯ä»¥å¢å¼ºè¿‘å®æ—¶æ´ªæ°´åˆ¶å›¾èƒ½åŠ›ã€‚è¿™é¡¹ç ”ç©¶æä¾›äº†GFMåœ¨æ´ªæ°´åˆ†å‰²ä»»åŠ¡ä¸­é¦–æ¬¡å…¨çƒå°ºåº¦è¯„ä¼°ä¹‹ä¸€ï¼Œçªå‡ºäº†å…¶åœ¨æ°”å€™é€‚åº”å’Œç¾å®³æ¢å¤æ–¹é¢çš„æ½œåŠ›å’Œå½“å‰å±€é™æ€§ï¼Œä¸ºæœªæ¥æ”¹è¿›æä¾›äº†é‡è¦åŸºå‡†ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Floods are among the most damaging weather-related hazards, and in 2024, the warmest year on record, extreme flood events affected communities across five continents. Earth observation (EO) satellites provide critical, frequent coverage for mapping inundation, yet operational accuracy depends heavily on labeled datasets and model generalization. Recent Geospatial Foundation Models (GFMs), such as ESA-IBM's TerraMind, offer improved generalizability through large-scale self-supervised pretraining, but their performance on diverse global flood events remains poorly understood.
  We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing co-located Sentinel-1 (Synthetic Aperture Radar, SAR data) and Sentinel-2 (optical) imagery for 85 flood events worldwide. We tested four configurations (base vs. large models; frozen vs. unfrozen backbones) and compared against the TerraMind Sen1Floods11 example and a U-Net trained on both FloodsNet and Sen1Floods11. The base-unfrozen configuration provided the best balance of accuracy, precision, and recall at substantially lower computational cost than the large model. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall with similar overall accuracy. U-Net achieved higher recall than all GFM configurations, though with slightly lower accuracy and precision.
  Our results demonstrate that integrating multimodal optical and SAR data and fine-tuning a GFM can enhance near-real-time flood mapping. This study provides one of the first global-scale evaluations of a GFM for flood segmentation, highlighting both its potential and current limitations for climate adaptation and disaster resilience.</p>
<h3 id="2-context-enriched-contrastive-loss-enhancing-presentation-of-inherent-sample-connections-in-contrastive-learning-framework">[2] <a href="https://arxiv.org/abs/2512.02152">Context-Enriched Contrastive Loss: Enhancing Presentation of Inherent Sample Connections in Contrastive Learning Framework</a></h3>
<p><em>Haojin Deng, Yimin Yang</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸Šä¸‹æ–‡å¢å¼ºçš„å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œé€šè¿‡åŒæ—¶ä¼˜åŒ–ä¸¤ä¸ªæ”¶æ•›ç›®æ ‡æ¥æå‡å¯¹æ¯”å­¦ä¹ æ•ˆæœå¹¶è§£å†³ä¿¡æ¯å¤±çœŸé—®é¢˜ï¼Œåœ¨å¤šä¸ªå¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†16ç§æœ€å…ˆè¿›çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¯¹æ¯”å­¦ä¹ ä¸­çš„å¯¹æ¯”æŸå¤±å‡½æ•°è™½ç„¶èƒ½æœ‰æ•ˆåŒºåˆ†æ ·æœ¬ç›¸ä¼¼æ€§ï¼Œä½†å¯èƒ½å¼•å…¥æ¥è‡ªå¢å¼ºæ ·æœ¬çš„ä¿¡æ¯å¤±çœŸé—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦ä¾èµ–ç›¸åŒæ ‡ç­¾æ ·æœ¬çš„ä¿¡æ¯ï¼ŒåŒæ—¶å¿½è§†æ¥è‡ªåŒä¸€åŸå§‹å›¾åƒçš„æ­£æ ·æœ¬å¯¹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸­è¿™ä¸€é—®é¢˜æ›´ä¸ºæ˜¾è‘—ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸Šä¸‹æ–‡å¢å¼ºçš„å¯¹æ¯”æŸå¤±å‡½æ•°ï¼ŒåŒ…å«ä¸¤ä¸ªæ”¶æ•›ç›®æ ‡ç»„ä»¶ï¼šç¬¬ä¸€ä¸ªç»„ä»¶å¯¹æ ‡ç­¾å¯¹æ¯”æ•æ„Ÿï¼ŒåŒºåˆ†ç›¸åŒç±»åˆ«å’Œä¸åŒç±»åˆ«çš„ç‰¹å¾ä»¥æå‡å¯¹æ¯”è®­ç»ƒæ•ˆç‡ï¼›ç¬¬äºŒä¸ªç»„ä»¶æ‹‰è¿‘æ¥è‡ªåŒä¸€æºå›¾åƒçš„å¢å¼ºæ ·æœ¬è·ç¦»ï¼ŒåŒæ—¶æ¨è¿œæ‰€æœ‰å…¶ä»–æ ·æœ¬ï¼Œä»è€Œè§£å†³ä¿¡æ¯å¤±çœŸé—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨8ä¸ªå¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ˆCIFAR10ã€CIFAR100ã€Caltech-101ã€Caltech-256ã€ImageNetã€BiasedMNISTã€UTKFaceå’ŒCelebAï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ³›åŒ–æ€§èƒ½å’Œå­¦ä¹ æ”¶æ•›é€Ÿåº¦æ–¹é¢å‡ä¼˜äº16ç§æœ€å…ˆè¿›çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨BiasedMNISTæ•°æ®é›†ä¸Šç›¸æ¯”åŸå§‹å¯¹æ¯”æŸå¤±å‡½æ•°å®ç°äº†22.9%çš„æ€§èƒ½æå‡ï¼Œåœ¨ç³»ç»Ÿæ€§å¤±çœŸä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æå‡ºçš„ä¸Šä¸‹æ–‡å¢å¼ºå¯¹æ¯”æŸå¤±å‡½æ•°æœ‰æ•ˆè§£å†³äº†å¯¹æ¯”å­¦ä¹ ä¸­çš„ä¿¡æ¯å¤±çœŸé—®é¢˜ï¼Œä¸ä»…æå‡äº†æ¨¡å‹æ€§èƒ½å’Œå­¦ä¹ æ•ˆç‡ï¼Œè¿˜åœ¨å¤„ç†ç³»ç»Ÿæ€§åå·®ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæ›´é«˜æ•ˆå’Œå…¬å¹³çš„ä¸‹æ¸¸è®­ç»ƒæä¾›äº†æœ‰å‰æ™¯çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Contrastive learning has gained popularity and pushes state-of-the-art performance across numerous large-scale benchmarks. In contrastive learning, the contrastive loss function plays a pivotal role in discerning similarities between samples through techniques such as rotation or cropping. However, this learning mechanism can also introduce information distortion from the augmented samples. This is because the trained model may develop a significant overreliance on information from samples with identical labels, while concurrently neglecting positive pairs that originate from the same initial image, especially in expansive datasets. This paper proposes a context-enriched contrastive loss function that concurrently improves learning effectiveness and addresses the information distortion by encompassing two convergence targets. The first component, which is notably sensitive to label contrast, differentiates between features of identical and distinct classes which boosts the contrastive training efficiency. Meanwhile, the second component draws closer the augmented samples from the same source image and distances all other samples. We evaluate the proposed approach on image classification tasks, which are among the most widely accepted 8 recognition large-scale benchmark datasets: CIFAR10, CIFAR100, Caltech-101, Caltech-256, ImageNet, BiasedMNIST, UTKFace, and CelebA datasets. The experimental results demonstrate that the proposed method achieves improvements over 16 state-of-the-art contrastive learning methods in terms of both generalization performance and learning convergence speed. Interestingly, our technique stands out in addressing systematic distortion tasks. It demonstrates a 22.9% improvement compared to original contrastive loss functions in the downstream BiasedMNIST dataset, highlighting its promise for more efficient and equitable downstream training.</p>
<h3 id="3-finegrain-evaluating-failure-modes-of-text-to-image-models-with-vision-language-model-judges">[3] <a href="https://arxiv.org/abs/2512.02161">FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges</a></h3>
<p><em>Kevin David Hayes, Micah Goldblum, Vikash Sehwag, Gowthami Somepalli, Ashwinee Panda, Tom Goldstein</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è”åˆè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„å±‚æ¬¡åŒ–æ¡†æ¶ï¼Œé€šè¿‡æµ‹è¯•è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¦è¯†åˆ«æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆçš„å›¾åƒä¸­çš„27ç§ç‰¹å®šå¤±è´¥æ¨¡å¼ï¼Œæ­ç¤ºäº†ç°æœ‰è¯„ä¼°æŒ‡æ ‡çš„ä¸è¶³å¹¶åˆ›å»ºäº†åŒ…å«5ç§ç”Ÿæˆæ¨¡å‹å’Œ3ç§è§†è§‰è¯­è¨€æ¨¡å‹æ ‡æ³¨çš„æ•°æ®é›†ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶ç»å¸¸æ— æ³•å‡†ç¡®æ•æ‰ç”¨æˆ·æç¤ºä¸­çš„ç‰¹å®šå±æ€§ï¼Œå¦‚æ­£ç¡®æ•°é‡çš„å¯¹è±¡å’ŒæŒ‡å®šé¢œè‰²ï¼Œè€Œç°æœ‰è¯„ä¼°æ¡†æ¶ç¼ºä¹å¯¹è¿™ç§å¤šæ ·é”™è¯¯çš„å±‚æ¬¡åŒ–åˆ†æèƒ½åŠ›ï¼›åŒæ—¶ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹çš„åŸºå‡†æµ‹è¯•æœªèƒ½è·Ÿä¸Šå…¶ç”¨äºæ ‡æ³¨çš„å¤æ‚åœºæ™¯éœ€æ±‚ï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆè¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„æç¤ºéµå¾ªèƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“æ„åŒ–æ–¹æ³•ï¼Œé€šè¿‡æµ‹è¯•è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¦è¯†åˆ«æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨æŒ‘æˆ˜æ€§æç¤ºä¸‹ç”Ÿæˆçš„å›¾åƒä¸­çš„27ç§ç‰¹å®šå¤±è´¥æ¨¡å¼æ¥è”åˆè¯„ä¼°è¿™ä¸¤ç§æ¨¡å‹ï¼›åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«5ç§æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆçš„å›¾åƒæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨3ç§è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæ ‡æ³¨ï¼Œæœ€åé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹éªŒè¯æ ‡æ³¨çš„æ­£ç¡®æ€§ï¼Œç³»ç»Ÿåˆ†æå±æ€§ä¿çœŸåº¦å’Œå¯¹è±¡è¡¨ç¤ºæ–¹é¢çš„ç³»ç»Ÿæ€§é”™è¯¯ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡å¯¹ç²¾å¿ƒç­–åˆ’çš„æç¤ºé›†è¿›è¡Œå¤±è´¥æ¨¡å¼åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å±æ€§ä¿çœŸåº¦å’Œå¯¹è±¡è¡¨ç¤ºæ–¹é¢çš„ç³»ç»Ÿæ€§é”™è¯¯ï¼›å®éªŒä½¿ç”¨äº†Fluxã€SD3-Mediumã€SD3-Largeã€SD3.5-Mediumã€SD3.5-Largeç­‰5ç§ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠMolmoã€InternVL3ã€Pixtralç­‰3ç§è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶ç”±Llama3éªŒè¯æ ‡æ³¨è´¨é‡ï¼Œç»“æœè¡¨æ˜å½“å‰è¯„ä¼°æŒ‡æ ‡æ— æ³•æ•æ‰è¿™äº›ç»†å¾®é”™è¯¯ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†é’ˆå¯¹æ€§åŸºå‡†æµ‹è¯•å¯¹äºæå‡ç”Ÿæˆæ¨¡å‹å¯é æ€§å’Œå¯è§£é‡Šæ€§çš„é‡è¦æ€§ï¼Œæå‡ºçš„è”åˆè¯„ä¼°æ¡†æ¶èƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸ºæœªæ¥ç”Ÿæˆæ¨¡å‹çš„æ”¹è¿›æä¾›äº†ç³»ç»ŸåŒ–çš„é”™è¯¯åˆ†ææ–¹æ³•å’Œè¯„ä¼°æ ‡å‡†ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸè¯„ä¼°æ–¹æ³•çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Text-to-image (T2I) models are capable of generating visually impressive images, yet they often fail to accurately capture specific attributes in user prompts, such as the correct number of objects with the specified colors. The diversity of such errors underscores the need for a hierarchical evaluation framework that can compare prompt adherence abilities of different image generation models. Simultaneously, benchmarks of vision language models (VLMs) have not kept pace with the complexity of scenes that VLMs are used to annotate. In this work, we propose a structured methodology for jointly evaluating T2I models and VLMs by testing whether VLMs can identify 27 specific failure modes in the images generated by T2I models conditioned on challenging prompts. Our second contribution is a dataset of prompts and images generated by 5 T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large) and the corresponding annotations from VLMs (Molmo, InternVL3, Pixtral) annotated by an LLM (Llama3) to test whether VLMs correctly identify the failure mode in a generated image. By analyzing failure modes on a curated set of prompts, we reveal systematic errors in attribute fidelity and object representation. Our findings suggest that current metrics are insufficient to capture these nuanced errors, highlighting the importance of targeted benchmarks for advancing generative model reliability and interpretability.</p>
<h3 id="4-towards-unified-video-quality-assessment">[4] <a href="https://arxiv.org/abs/2512.02224">Towards Unified Video Quality Assessment</a></h3>
<p><em>Chen Feng, Tianhao Peng, Fan Zhang, David Bull</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºUnified-VQAæ¡†æ¶ï¼Œé€šè¿‡å°†é€šç”¨è§†é¢‘è´¨é‡è¯„ä¼°é‡æ„ä¸ºè¯Šæ–­æ€§ä¸“å®¶æ··åˆé—®é¢˜ï¼Œå®ç°äº†é€‚ç”¨äºå¤šç§è§†é¢‘æ ¼å¼å’Œå¤±çœŸç±»å‹çš„ç»Ÿä¸€è´¨é‡æ¨¡å‹ï¼ŒåŒæ—¶æä¾›å¯è§£é‡Šçš„å¤šç»´ä¼ªå½±å‘é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘è´¨é‡è¯„ä¼°æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šä¸€æ˜¯é‡‡ç”¨å•ä¸€è´¨é‡åˆ†æ•°é¢„æµ‹çš„å•ä¸€æ¨¡å‹æ— æ³•æä¾›è¯Šæ–­æ€§ã€å¯è§£é‡Šçš„åé¦ˆï¼›äºŒæ˜¯å¤§å¤šæ•°æ–¹æ³•æ˜¯ç‰¹å®šæ ¼å¼çš„ä¸“ç”¨æŒ‡æ ‡è€ŒéçœŸæ­£çš„é€šç”¨è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒä»¬ä»ä¸åŒæ„ŸçŸ¥åŸŸå­¦ä¹ æŠ˜è¡·è¡¨ç¤ºã€‚</p>
<p><strong>Method:</strong> Unified-VQAæ¡†æ¶å°†é€šç”¨VQAé‡æ„ä¸ºè¯Šæ–­æ€§ä¸“å®¶æ··åˆé—®é¢˜ï¼Œé‡‡ç”¨å¤šä¸ªä¸“æ³¨äºä¸åŒæ„ŸçŸ¥åŸŸçš„"æ„ŸçŸ¥ä¸“å®¶"ï¼Œè®¾è®¡äº†æ–°é¢–çš„å¤šä»£ç†ä¸“å®¶è®­ç»ƒç­–ç•¥ï¼Œä½¿ç”¨åŸºäºæ’åçš„æŸå¤±å‡½æ•°å¹¶ä»¥æœ€é€‚åˆå…¶é¢†åŸŸçš„ä»£ç†æŒ‡æ ‡ä¸ºæŒ‡å¯¼ä¼˜åŒ–æ¯ä¸ªä¸“å®¶ï¼ŒåŒæ—¶é›†æˆäº†è¯Šæ–­æ€§å¤šä»»åŠ¡å¤´ä»¥ç”Ÿæˆå…¨å±€è´¨é‡åˆ†æ•°å’Œå¯è§£é‡Šçš„å¤šç»´ä¼ªå½±å‘é‡ï¼Œé‡‡ç”¨å¼±ç›‘ç£å­¦ä¹ ç­–ç•¥è¿›è¡Œä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> åœ¨é™æ€æ¨¡å‹å‚æ•°æ¡ä»¶ä¸‹ï¼ˆæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒï¼‰ï¼ŒUnified-VQAåœ¨17ä¸ªåŒ…å«HDã€UHDã€HDRå’ŒHFRæ ¼å¼ä¸­å¤šæ ·åŒ–æµåª’ä½“ä¼ªå½±çš„æ•°æ®åº“ä¸Šï¼Œç›¸æ¯”è¶…è¿‡18ç§åŸºå‡†æ–¹æ³•ï¼Œåœ¨é€šç”¨VQAå’Œè¯Šæ–­æ€§ä¼ªå½±æ£€æµ‹ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæŒç»­ä¸”ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä»£è¡¨äº†å‘å®ç”¨ã€å¯æ“ä½œå’Œå¯è§£é‡Šçš„è§†é¢‘è´¨é‡è¯„ä¼°è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ï¼Œé€šè¿‡ç»Ÿä¸€çš„æ¡†æ¶è§£å†³äº†æ ¼å¼ç‰¹å®šæ€§å’Œç¼ºä¹è¯Šæ–­èƒ½åŠ›çš„é—®é¢˜ï¼Œä¸ºè§†é¢‘è´¨é‡åˆ†ææä¾›äº†æ—¢å…¨é¢åˆå…·è§£é‡Šæ€§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Recent works in video quality assessment (VQA) typically employ monolithic models that typically predict a single quality score for each test video. These approaches cannot provide diagnostic, interpretable feedback, offering little insight into why the video quality is degraded. Most of them are also specialized, format-specific metrics rather than truly <code>generic" solutions, as they are designed to learn a compromised representation from disparate perceptual domains. To address these limitations, this paper proposes Unified-VQA, a framework that provides a single, unified quality model applicable to various distortion types within multiple video formats by recasting generic VQA as a Diagnostic Mixture-of-Experts (MoE) problem. Unified-VQA employs multiple</code>perceptual experts'' dedicated to distinct perceptual domains. A novel multi-proxy expert training strategy is designed to optimize each expert using a ranking-inspired loss, guided by the most suitable proxy metric for its domain. We also integrated a diagnostic multi-task head into this framework to generate a global quality score and an interpretable multi-dimensional artifact vector, which is optimized using a weakly-supervised learning strategy, leveraging the known properties of the large-scale training database generated for this work. With static model parameters (without retraining or fine-tuning), Unified-VQA demonstrates consistent and superior performance compared to over 18 benchmark methods for both generic VQA and diagnostic artifact detection tasks across 17 databases containing diverse streaming artifacts in HD, UHD, HDR and HFR formats. This work represents an important step towards practical, actionable, and interpretable video quality assessment.</p>
<h3 id="5-see-hear-and-understand-benchmarking-audiovisual-human-speech-understanding-in-multimodal-large-language-models">[5] <a href="https://arxiv.org/abs/2512.02231">See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models</a></h3>
<p><em>Le Thien Phuc Nguyen, Zhuoran Yu, Samuel Low Yu Hang, Subin An, Jeongik Lee, Yohan Ban, SeungEun Chung, Thanh-Huy Nguyen, JuWan Maeng, Soochahn Lee, Yong Jae Lee</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†AV-SpeakerBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºè¯´è¯äººä¸­å¿ƒè§†å¬æ¨ç†çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«3,212ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦è¯­éŸ³ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œå¡«è¡¥äº†ç°æœ‰è§†é¢‘åŸºå‡†åœ¨è¯´è¯äººè¯†åˆ«ã€å†…å®¹å¯¹é½å’Œæ—¶é—´å®šä½æ–¹é¢çš„è¯„ä¼°ç©ºç™½ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘åŸºå‡†æµ‹è¯•å¾ˆå°‘è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹äººç±»è¯­éŸ³çš„ç»†ç²’åº¦æ¨ç†èƒ½åŠ›ï¼Œè®¸å¤šä»»åŠ¡è¦ä¹ˆå¯ä»¥é€šè¿‡è§†è§‰å•ç‹¬è§£å†³ï¼Œè¦ä¹ˆä»…å¯¹è¯­éŸ³è¿›è¡Œç²—ç•¥è¯„ä¼°ï¼Œæ— æ³•æ·±å…¥è€ƒå¯Ÿæ¨¡å‹æ˜¯å¦èƒ½å‡†ç¡®å¯¹é½è¯´è¯äººèº«ä»½ã€è¯´è¯å†…å®¹ä»¥åŠè¯´è¯æ—¶é—´è¿™ä¸‰ä¸ªå…³é”®ç»´åº¦ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æ„å»ºäº†AV-SpeakerBenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«3,212ä¸ªç²¾å¿ƒè®¾è®¡çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼Œé‡‡ç”¨è¯´è¯äººä¸­å¿ƒçš„è¡¨è¿°æ–¹å¼ï¼Œå°†è¯´è¯äººè€Œéåœºæ™¯ä½œä¸ºæ ¸å¿ƒæ¨ç†å•å…ƒï¼›é€šè¿‡èåˆåŸºç¡€çš„é—®é¢˜è®¾è®¡å°†è§†å¬ä¾èµ–å…³ç³»åµŒå…¥é—®é¢˜è¯­ä¹‰ä¸­ï¼›å¹¶é‡‡ç”¨ä¸“å®¶æ ‡æ³¨ç¡®ä¿æ—¶é—´ç²¾åº¦å’Œè·¨æ¨¡æ€æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Result:</strong> ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒGeminiç³»åˆ—æ¨¡å‹å§‹ç»ˆä¼˜äºå¼€æºç³»ç»Ÿï¼Œå…¶ä¸­Gemini 2.5 Proå–å¾—æœ€ä½³ç»“æœï¼›åœ¨å¼€æºæ¨¡å‹ä¸­ï¼ŒQwen3-Omni-30Bæ¥è¿‘Gemini 2.0 Flashä½†ä»è¿œè½åäºGemini 2.5 Proï¼Œä¸»è¦å·®è·åœ¨äºè§†å¬èåˆèƒ½åŠ›è€Œéè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºæ¨è¿›æœªæ¥å¤šæ¨¡æ€ç³»ç»Ÿçš„ç»†ç²’åº¦è§†å¬æ¨ç†èƒ½åŠ›å»ºç«‹äº†ä¸¥è°¨çš„è¯„ä¼°åŸºç¡€ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è¯´è¯äººä¸­å¿ƒæ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å¼€æºæ¨¡å‹åœ¨è§†å¬èåˆèƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œä¸ºåç»­ç ”ç©¶æŒ‡æ˜äº†æ”¹è¿›æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.</p>
<h3 id="6-progressive-image-restoration-via-text-conditioned-video-generation">[6] <a href="https://arxiv.org/abs/2512.02273">Progressive Image Restoration via Text-Conditioned Video Generation</a></h3>
<p><em>Peng Kang, Xijun Wang, Yu Yuan</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡é‡æ–°åˆ©ç”¨CogVideoæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹è¿›è¡Œæ¸è¿›å¼è§†è§‰ä¿®å¤ä»»åŠ¡ï¼Œé€šè¿‡å¾®è°ƒä½¿å…¶ç”Ÿæˆä¿®å¤è½¨è¿¹è€Œéè‡ªç„¶è§†é¢‘è¿åŠ¨ï¼Œåœ¨è¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šå’Œä½å…‰å¢å¼ºä»»åŠ¡ä¸Šå®ç°äº†æ—¶ç©ºä¸€è‡´çš„æ¢å¤æ•ˆæœã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹åœ¨æ—¶é—´ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨å›¾åƒä¿®å¤é¢†åŸŸçš„æ½œåŠ›å°šæœªå……åˆ†æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•å°†æ—¶é—´ç”Ÿæˆèƒ½åŠ›é‡æ–°ç”¨äºæ¸è¿›å¼è§†è§‰ä¿®å¤ä»»åŠ¡ï¼Œä»¥äº§ç”Ÿä»é€€åŒ–åˆ°æ¸…æ™°å¸§çš„é€æ­¥æ¢å¤è¿‡ç¨‹ã€‚</p>
<p><strong>Method:</strong> é€šè¿‡å¾®è°ƒCogVideoæ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆä¿®å¤è½¨è¿¹è€Œéè‡ªç„¶è§†é¢‘è¿åŠ¨ï¼›æ„å»ºäº†è¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šå’Œä½å…‰å¢å¼ºçš„åˆæˆæ•°æ®é›†ï¼Œæ¯ä¸ªæ ·æœ¬å±•ç¤ºä»é€€åŒ–åˆ°æ¸…æ™°å¸§çš„æ¸è¿›è¿‡æ¸¡ï¼›æ¯”è¾ƒäº†ä¸¤ç§æç¤ºç­–ç•¥ï¼šè·¨æ‰€æœ‰æ ·æœ¬çš„ç»Ÿä¸€æ–‡æœ¬æç¤ºï¼Œä»¥åŠé€šè¿‡LLaVAå¤šæ¨¡æ€LLMç”Ÿæˆå¹¶ç»ChatGPTä¼˜åŒ–çš„åœºæ™¯ç‰¹å®šæç¤ºæ–¹æ¡ˆã€‚</p>
<p><strong>Result:</strong> å¾®è°ƒåçš„æ¨¡å‹å­¦ä¼šå°†æ—¶é—´è¿›å±•ä¸ä¿®å¤è´¨é‡å…³è”ï¼Œäº§ç”Ÿçš„åºåˆ—åœ¨PSNRã€SSIMå’ŒLPIPSç­‰æ„ŸçŸ¥æŒ‡æ ‡ä¸Šé€å¸§æ”¹å–„ï¼›å®éªŒè¡¨æ˜CogVideoèƒ½æœ‰æ•ˆæ¢å¤ç©ºé—´ç»†èŠ‚å’Œå…‰ç…§ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒæ—¶é—´è¿è´¯æ€§ï¼›æ¨¡å‹åœ¨ReLoBluræ•°æ®é›†ä¸Šæ— éœ€é¢å¤–è®­ç»ƒå³å¯æ³›åŒ–åˆ°çœŸå®åœºæ™¯ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬é²æ£’æ€§å’Œé€šè¿‡æ—¶é—´ä¿®å¤çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹åœ¨è§†è§‰ä¿®å¤ä»»åŠ¡ä¸­çš„é‡æ–°åˆ©ç”¨æ½œåŠ›ï¼Œé€šè¿‡å°†æ—¶é—´ç»´åº¦ä¸ä¿®å¤è´¨é‡å…³è”ï¼Œå®ç°äº†æ¸è¿›å¼æ¢å¤è¿‡ç¨‹ï¼›æ¨¡å‹å±•ç¤ºäº†è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å’Œæ—¶é—´ä¸€è‡´æ€§ä¿æŒï¼Œä¸ºè§†é¢‘ä¿®å¤ä»»åŠ¡æä¾›äº†æ–°çš„èŒƒå¼ï¼ŒåŒæ—¶é€šè¿‡æ—¶é—´è½¨è¿¹æä¾›äº†ä¿®å¤è¿‡ç¨‹çš„å¯è§£é‡Šæ€§æ´å¯Ÿã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Recent text-to-video models have demonstrated strong temporal generation capabilities, yet their potential for image restoration remains underexplored. In this work, we repurpose CogVideo for progressive visual restoration tasks by fine-tuning it to generate restoration trajectories rather than natural video motion. Specifically, we construct synthetic datasets for super-resolution, deblurring, and low-light enhancement, where each sample depicts a gradual transition from degraded to clean frames. Two prompting strategies are compared: a uniform text prompt shared across all samples, and a scene-specific prompting scheme generated via LLaVA multi-modal LLM and refined with ChatGPT. Our fine-tuned model learns to associate temporal progression with restoration quality, producing sequences that improve perceptual metrics such as PSNR, SSIM, and LPIPS across frames. Extensive experiments show that CogVideo effectively restores spatial detail and illumination consistency while maintaining temporal coherence. Moreover, the model generalizes to real-world scenarios on the ReLoBlur dataset without additional training, demonstrating strong zero-shot robustness and interpretability through temporal restoration.</p>
<h3 id="7-understanding-and-harnessing-sparsity-in-unified-multimodal-models">[7] <a href="https://arxiv.org/abs/2512.02351">Understanding and Harnessing Sparsity in Unified Multimodal Models</a></h3>
<p><em>Shwai He, Chaorui Deng, Ang Li, Shen Yan</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„æ¨ç†æ•ˆç‡é—®é¢˜ï¼Œå‘ç°ç”Ÿæˆç»„ä»¶å¯¹å‹ç¼©é«˜åº¦æ•æ„Ÿï¼Œå¹¶æå‡ºåŸºäºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„ç¨€ç–æ¿€æ´»é€‚é…æ–¹æ³•ï¼Œä½¿BAGELæ¨¡å‹ä»…æ¿€æ´»çº¦åŠæ•°å‚æ•°å³å¯è¾¾åˆ°å…¨æ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹è™½ç„¶æ•´åˆäº†ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä½†å¼•å…¥äº†æ¨ç†æ•ˆç‡é—®é¢˜ï¼Œå› ä¸ºç‰¹å®šä»»åŠ¡æˆ–æ ·æœ¬å¯èƒ½ä¸éœ€è¦æ¨¡å‹çš„å…¨éƒ¨çŸ¥è¯†æˆ–å®¹é‡ã€‚ç›®å‰å¯¹äºè¿™äº›æ•ˆç‡é—®é¢˜åœ¨ä¸åŒç»„ä»¶ä¸­å¦‚ä½•è¡¨ç°çš„ç³»ç»Ÿæ€§ç†è§£ä»ç„¶æœ‰é™ï¼Œéœ€è¦æ·±å…¥åˆ†æä»¥ä¼˜åŒ–æ¨¡å‹æ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> é¦–å…ˆé‡‡ç”¨è®­ç»ƒæ— å…³çš„å‰ªæä½œä¸ºæ¢æµ‹æ–¹æ³•ï¼Œç³»ç»Ÿåˆ†æç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ç»„ä»¶ï¼ŒåŒ…æ‹¬æ·±åº¦å‰ªæå’Œå®½åº¦ç¼©å‡ã€‚éšåæå‡ºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰é€‚é…æ–¹æ³•ï¼Œå°†ç”Ÿæˆæ¨¡å—åˆ’åˆ†ä¸ºå¤šä¸ªä¸“å®¶å¹¶å¯ç”¨ç¨€ç–æ¿€æ´»ä»¥æ¢å¤ç”Ÿæˆè´¨é‡ï¼Œé€šè¿‡ä¸“å®¶å†»ç»“è°ƒä¼˜å’Œå®Œå…¨å¯è®­ç»ƒçš„é€‚é…è¿›è¡ŒéªŒè¯ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘ç°ç†è§£ç»„ä»¶åœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºæ˜¾è‘—çš„å¯å‹ç¼©æ€§ï¼Œåœ¨ç”Ÿæˆä»»åŠ¡ä¸­æ›´ä¸ºæ˜æ˜¾ï¼›è€Œç”Ÿæˆç»„ä»¶å¯¹å‹ç¼©é«˜åº¦æ•æ„Ÿï¼Œå³ä½¿åœ¨ä¸­ç­‰å‹ç¼©æ¯”ä¸‹æ€§èƒ½ä¹Ÿä¼šæ€¥å‰§ä¸‹é™ã€‚æå‡ºçš„MoEé€‚é…æ–¹æ³•ä½¿BAGELæ¨¡å‹ä»…æ¿€æ´»çº¦åŠæ•°å‚æ•°å³å¯è¾¾åˆ°ä¸å…¨æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒéªŒè¯äº†ç¨€ç–æ¿€æ´»çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ä¸­ä¸åŒç»„ä»¶å¯¹å‹ç¼©çš„æ•æ„Ÿæ€§å·®å¼‚ï¼Œä¸ºæ¨¡å‹æ•ˆç‡ä¼˜åŒ–æä¾›äº†é‡è¦è§è§£ã€‚æå‡ºçš„MoEé€‚é…æ–¹æ³•é€šè¿‡ç¨€ç–æ¿€æ´»æœ‰æ•ˆè§£å†³äº†ç”Ÿæˆç»„ä»¶çš„æ•ˆç‡ç“¶é¢ˆï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†å¯è¡Œè·¯å¾„ï¼ŒåŒæ—¶å±•ç¤ºäº†åŠ¨æ€æ¿€æ´»æ¨¡å¼åœ¨æ¨¡å‹ä¼˜åŒ–ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at \href{https://github.com/Shwai-He/SparseUnifiedModel}{this link}.</p>
<h3 id="8-worldmm-dynamic-multimodal-memory-agent-for-long-video-reasoning">[8] <a href="https://arxiv.org/abs/2512.02425">WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning</a></h3>
<p><em>Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†WorldMMï¼Œä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€è®°å¿†ä»£ç†ï¼Œé€šè¿‡æ„å»ºå’Œæ£€ç´¢äº’è¡¥çš„æ–‡æœ¬ä¸è§†è§‰è®°å¿†æ¥è§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„ä¸Šä¸‹æ–‡å®¹é‡é™åˆ¶å’Œè§†è§‰ç»†èŠ‚ä¸¢å¤±é—®é¢˜ï¼Œåœ¨å¤šä¸ªé•¿è§†é¢‘é—®ç­”åŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å°æ—¶æˆ–å¤©çº§é•¿è§†é¢‘æ—¶é¢ä¸´ä¸Šä¸‹æ–‡å®¹é‡æœ‰é™å’Œè§†è§‰ç»†èŠ‚ä¸¢å¤±çš„æŒ‘æˆ˜ï¼Œå½“å‰åŸºäºæ–‡æœ¬æ‘˜è¦çš„è®°å¿†å¢å¼ºæ–¹æ³•è¿‡åº¦ä¾èµ–æ–‡æœ¬è¡¨ç¤ºä¸”æ— æ³•æœ‰æ•ˆåˆ©ç”¨è§†è§‰è¯æ®è¿›è¡Œå¤æ‚åœºæ™¯æ¨ç†ï¼ŒåŒæ—¶å›ºå®šæ—¶é—´å°ºåº¦çš„æ£€ç´¢æœºåˆ¶éš¾ä»¥æ•æ‰å¯å˜æŒç»­æ—¶é—´çš„äº‹ä»¶ã€‚</p>
<p><strong>Method:</strong> WorldMMæ„å»ºäº†ä¸‰ç§äº’è¡¥çš„è®°å¿†ç±»å‹ï¼šè·¨å¤šæ—¶é—´å°ºåº¦ç´¢å¼•äº‹å®äº‹ä»¶çš„ç‰‡æ®µè®°å¿†ã€æŒç»­æ›´æ–°é«˜å±‚æ¦‚å¿µçŸ¥è¯†çš„è¯­ä¹‰è®°å¿†ï¼Œä»¥åŠä¿ç•™åœºæ™¯è¯¦ç»†ä¿¡æ¯çš„è§†è§‰è®°å¿†ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè‡ªé€‚åº”æ£€ç´¢ä»£ç†åŸºäºæŸ¥è¯¢è¿­ä»£é€‰æ‹©æœ€ç›¸å…³çš„è®°å¿†æºå¹¶åˆ©ç”¨å¤šç§æ—¶é—´ç²’åº¦ï¼Œç›´åˆ°ç¡®å®šå·²æ”¶é›†è¶³å¤Ÿä¿¡æ¯ã€‚</p>
<p><strong>Result:</strong> WorldMMåœ¨äº”ä¸ªé•¿è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå¹³å‡æ€§èƒ½æ¯”å…ˆå‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†8.4%ï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿è§†é¢‘æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜å¤šæ¨¡æ€è®°å¿†æ¶æ„èƒ½å¤Ÿæœ‰æ•ˆè§£å†³é•¿è§†é¢‘ç†è§£ä¸­çš„ä¸Šä¸‹æ–‡é™åˆ¶é—®é¢˜ï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºä»¥åŠè‡ªé€‚åº”å¤šæ—¶é—´å°ºåº¦æ£€ç´¢ï¼Œä¸ºå¤æ‚é•¿è§†é¢‘æ¨ç†ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨éœ€è¦ç²¾ç»†è§†è§‰ç»†èŠ‚å’Œé•¿æœŸä¾èµ–ç†è§£çš„åº”ç”¨åœºæ™¯ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.</p>
<h3 id="9-multi-domain-enhanced-map-free-trajectory-prediction-with-selective-attention">[9] <a href="https://arxiv.org/abs/2512.02368">Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention</a></h3>
<p><em>Wenyi Xiong, Jian Chen</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— åœ°å›¾è½¨è¿¹é¢„æµ‹ç®—æ³•ï¼Œé€šè¿‡æ—¶åŸŸã€ç©ºåŸŸå’Œé¢‘åŸŸçš„å¤šåŸŸé¢„æµ‹ï¼Œåˆ©ç”¨ä¸“å®¶æ··åˆæœºåˆ¶å’Œé€‰æ‹©æ€§æ³¨æ„åŠ›æ¨¡å—æœ‰æ•ˆå¤„ç†å¤æ‚äº¤äº’åœºæ™¯ä¸­çš„å†—ä½™ä¿¡æ¯ï¼Œæ˜¾è‘—æå‡äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„è½¨è¿¹é¢„æµ‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è½¨è¿¹é¢„æµ‹æ–¹æ³•åœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸­éš¾ä»¥ä»å†—ä½™æ•°æ®ä¸­é«˜æ•ˆæå–æœ‰ä»·å€¼çš„åœºæ™¯ä¿¡æ¯ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡é™ä½å’Œé¢„æµ‹ç²¾åº¦ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„æ™ºèƒ½ä½“äº¤äº’æ—¶è¡¨ç°ä¸ä½³ï¼Œè¿™é™åˆ¶äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§æ— åœ°å›¾è½¨è¿¹é¢„æµ‹ç®—æ³•ï¼Œä½¿ç”¨æ—¶åŸŸã€ç©ºåŸŸå’Œé¢‘åŸŸçš„å¤šåŸŸé¢„æµ‹æ¡†æ¶ï¼›åœ¨æ—¶åŸŸä¿¡æ¯å¤„ç†ä¸­é‡‡ç”¨ä¸“å®¶æ··åˆæœºåˆ¶è‡ªé€‚åº”é€‰æ‹©å…³é”®é¢‘ç‡æˆåˆ†å¹¶é›†æˆå¤šå°ºåº¦æ—¶åŸŸç‰¹å¾ï¼›è®¾è®¡äº†é€‰æ‹©æ€§æ³¨æ„åŠ›æ¨¡å—æ¥è¿‡æ»¤æ—¶åŸŸåºåˆ—å’Œç©ºé—´äº¤äº’ä¸­çš„å†—ä½™ä¿¡æ¯ï¼›æœ€åæ„å»ºäº†å¤šæ¨¡æ€è§£ç å™¨ï¼Œåœ¨è¡¥ä¸çº§å’Œç‚¹çº§æŸå¤±ç›‘ç£ä¸‹ç”Ÿæˆåˆç†çš„è½¨è¿¹ç»“æœã€‚</p>
<p><strong>Result:</strong> åœ¨Nuscencesæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†ç®—æ³•çš„ä¼˜è¶Šæ€§ï¼Œè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚äº¤äº’åœºæ™¯ï¼Œåœ¨è½¨è¿¹é¢„æµ‹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ‰€ææ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œé¢„æµ‹ç²¾åº¦æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡å¤šåŸŸé¢„æµ‹æ¡†æ¶å’Œè‡ªé€‚åº”ä¿¡æ¯ç­›é€‰æœºåˆ¶ï¼Œä¸ºå¤æ‚äº¤äº’åœºæ™¯ä¸‹çš„è½¨è¿¹é¢„æµ‹æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œæ‰€æå‡ºçš„ä¸“å®¶æ··åˆæœºåˆ¶å’Œé€‰æ‹©æ€§æ³¨æ„åŠ›æ¨¡å—èƒ½å¤Ÿæ˜¾è‘—æå‡ä¿¡æ¯å¤„ç†æ•ˆç‡ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¯é æ€§å’Œå®‰å…¨æ€§æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios. Existing methods often struggle to efficiently extract valuable scene information from redundant data, thereby reducing computational efficiency and prediction accuracy, especially when dealing with intricate agent interactions. To address these challenges, we propose a novel map-free trajectory prediction algorithm that achieves trajectory prediction across the temporal, spatial, and frequency domains. Specifically, in temporal information processing, We utilize a Mixture of Experts (MoE) mechanism to adaptively select critical frequency components. Concurrently, we extract these components and integrate multi-scale temporal features. Subsequently, a selective attention module is proposed to filter out redundant information in both temporal sequences and spatial interactions. Finally, we design a multimodal decoder. Under the supervision of patch-level and point-level losses, we obtain reasonable trajectory results. Experiments on Nuscences datasets demonstrate the superiority of our algorithm, validating its effectiveness in handling complex interactive scenarios.</p>
<h3 id="10-see-think-learn-a-self-taught-multimodal-reasoner">[10] <a href="https://arxiv.org/abs/2512.02456">See, Think, Learn: A Self-Taught Multimodal Reasoner</a></h3>
<p><em>Sourabh Sharma, Sonam Gupta, Sadbhawna</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†See-Think-Learnï¼ˆSTLï¼‰è‡ªè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–æ¨ç†æ¨¡æ¿å’Œè´Ÿå‘åŸç†å¢å¼ºï¼Œæ— éœ€ä¾èµ–é«˜è´¨é‡äººå·¥æ ‡æ³¨æˆ–æ˜‚è´µä¸“æœ‰æ¨¡å‹ï¼Œå³å¯åŒæ—¶æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ•´åˆè§†è§‰æ„ŸçŸ¥ä¸è¯­è¨€ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†æœ‰æ•ˆçš„å¤šæ¨¡æ€æ¨ç†éœ€è¦å‡†ç¡®çš„æ„ŸçŸ¥å’Œç¨³å¥çš„æ¨ç†èƒ½åŠ›ï¼Œä»»ä¸€æ–¹é¢çš„å¼±ç‚¹éƒ½ä¼šé™åˆ¶æ¨¡å‹æ€§èƒ½ã€‚å…ˆå‰æå‡æ¨ç†èƒ½åŠ›çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºé«˜è´¨é‡æ€ç»´é“¾æ•°æ®ï¼Œè¿™äº›æ•°æ®éœ€è¦é€šè¿‡åŠ³åŠ¨å¯†é›†å‹äººå·¥æ ‡æ³¨ã€æ˜‚è´µçš„ä¸“æœ‰æ¨¡å‹æˆ–å¿½è§†æ„ŸçŸ¥çš„è‡ªè®­ç»ƒæ–¹æ³•è·å¾—ï¼Œå­˜åœ¨æˆæœ¬é«˜ä¸”å¿½è§†æ„ŸçŸ¥çš„å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†See-Think-Learnï¼ˆSTLï¼‰è‡ªè®­ç»ƒæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯å¼•å…¥ç»“æ„åŒ–æ¨ç†æ¨¡æ¿ï¼Œå¼ºåˆ¶æ¨¡å‹éµå¾ª"å…ˆçœ‹åæƒ³"çš„åŸåˆ™ï¼šé¦–å…ˆæå–è§†è§‰å±æ€§å¹¶è½¬æ¢ä¸ºæ–‡æœ¬å½¢å¼ï¼Œç„¶ååˆ©ç”¨è¿™äº›å±æ€§æŒ‡å¯¼æ¨ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡è®©æ¨¡å‹ç”Ÿæˆå¹¶å­¦ä¹ è‡ªèº«çš„ç»“æ„åŒ–åŸç†ï¼Œåœ¨è‡ªè®­ç»ƒå¾ªç¯ä¸­è”åˆæ”¹è¿›æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ·»åŠ è´Ÿå‘åŸç†ï¼ˆå³è§£é‡Šä¸ºä½•æŸäº›ç­”æ¡ˆé€‰é¡¹æ˜¯é”™è¯¯çš„ï¼‰æ¥å¢å¼ºè®­ç»ƒæ•°æ®ï¼Œæå‡æ¨¡å‹åŒºåˆ†æ­£ç¡®ä¸è¯¯å¯¼æ€§å“åº”çš„èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> è·¨å¤šä¸ªé¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼ŒSTLæ¡†æ¶åœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºä»…åŸºäºç­”æ¡ˆç›´æ¥è®­ç»ƒæˆ–åŸºäºè‡ªç”Ÿæˆæ¨ç†çš„åŸºçº¿æ–¹æ³•ã€‚å®šæ€§åˆ†æè¯å®äº†STLç”ŸæˆåŸç†çš„é«˜è´¨é‡ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿä»¥æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹å¼æ˜¾è‘—å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> STLæ¡†æ¶ä¸ºå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç»“æ„åŒ–æ¨ç†æ¨¡æ¿å’Œè´Ÿå‘åŸç†å¢å¼ºï¼Œå®ç°äº†æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›çš„è”åˆæå‡ã€‚è¯¥æ–¹æ³•é¿å…äº†ä¾èµ–é«˜è´¨é‡äººå·¥æ ‡æ³¨æˆ–æ˜‚è´µä¸“æœ‰æ¨¡å‹çš„é™åˆ¶ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†æ–°çš„è‡ªè®­ç»ƒèŒƒå¼ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼å’Œç ”ç©¶æ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.</p>
<h3 id="11-from-detection-to-association-learning-discriminative-object-embeddings-for-multi-object-tracking">[11] <a href="https://arxiv.org/abs/2512.02392">From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking</a></h3>
<p><em>Yuqing Shao, Yuchen Yang, Rui Yu, Weilong Li, Xu Guo, Huaicheng Yan, Wei Wang, Xiao Sun</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºFDTAæ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼ç‰¹å¾ç»†åŒ–å¢å¼ºç«¯åˆ°ç«¯å¤šç›®æ ‡è·Ÿè¸ªä¸­çš„å…³è”å‡†ç¡®æ€§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å› å…±äº«DETRæ¶æ„äº§ç”Ÿçš„å¯¹è±¡åµŒå…¥ç›¸ä¼¼åº¦è¿‡é«˜é—®é¢˜ï¼Œåœ¨å¤šä¸ªæŒ‘æˆ˜æ€§åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç«¯åˆ°ç«¯å¤šç›®æ ‡è·Ÿè¸ªæ–¹æ³•è™½ç„¶æ£€æµ‹æ€§èƒ½ä¼˜å¼‚ï¼Œä½†å…³è”å‡†ç¡®æ€§ç›¸å¯¹è¾ƒä½ï¼Œä¸»è¦é—®é¢˜åœ¨äºå…±äº«DETRæ¶æ„äº§ç”Ÿçš„å¯¹è±¡åµŒå…¥å…·æœ‰è¿‡é«˜çš„å¯¹è±¡é—´ç›¸ä¼¼åº¦ï¼Œè¯¥æ¶æ„ä»…å…³æ³¨å•å¸§å†…çš„ç±»åˆ«çº§åŒºåˆ†ï¼Œè€Œè·Ÿè¸ªéœ€è¦è·¨å¸§çš„å®ä¾‹çº§åŒºåˆ†ä»¥åŠæ—¶ç©ºè¿ç»­æ€§ï¼Œç°æœ‰æ–¹æ³•å¯¹æ­¤ä¼˜åŒ–ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> æå‡ºFDTAæ˜¾å¼ç‰¹å¾ç»†åŒ–æ¡†æ¶ï¼Œä»ä¸‰ä¸ªäº’è¡¥è§†è§’å¢å¼ºå¯¹è±¡åŒºåˆ†æ€§ï¼šç©ºé—´é€‚é…å™¨é›†æˆæ·±åº¦æ„ŸçŸ¥çº¿ç´¢ä»¥å®ç°ç©ºé—´è¿ç»­æ€§ï¼Œæ—¶é—´é€‚é…å™¨èšåˆå†å²ä¿¡æ¯ä»¥å»ºç«‹æ—¶é—´ä¾èµ–æ€§ï¼Œèº«ä»½é€‚é…å™¨åˆ©ç”¨è´¨é‡æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ å®ç°å®ä¾‹çº§å¯åˆ†ç¦»æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§MOTåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFDTAåœ¨DanceTrackã€SportsMOTå’ŒBFTç­‰æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†æ‰€æå‡ºçš„åŒºåˆ†æ€§åµŒå…¥å¢å¼ºç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†ç«¯åˆ°ç«¯MOTæ–¹æ³•ä¸­å¯¹è±¡åµŒå…¥åŒºåˆ†æ€§ä¸è¶³çš„æ ¸å¿ƒé—®é¢˜ï¼Œæå‡ºçš„å¤šè§†è§’ç‰¹å¾ç»†åŒ–æ¡†æ¶ä¸ºæå‡è·Ÿè¸ªå…³è”å‡†ç¡®æ€§æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œè¡¨æ˜æ˜¾å¼ä¼˜åŒ–æ—¶ç©ºè¿ç»­æ€§å’Œå®ä¾‹çº§åŒºåˆ†æ€§å¯¹å¤šç›®æ ‡è·Ÿè¸ªæ€§èƒ½è‡³å…³é‡è¦ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>End-to-end multi-object tracking (MOT) methods have recently achieved remarkable progress by unifying detection and association within a single framework. Despite their strong detection performance, these methods suffer from relatively low association accuracy. Through detailed analysis, we observe that object embeddings produced by the shared DETR architecture display excessively high inter-object similarity, as it emphasizes only category-level discrimination within single frames. In contrast, tracking requires instance-level distinction across frames with spatial and temporal continuity, for which current end-to-end approaches insufficiently optimize object embeddings. To address this, we introduce FDTA (From Detection to Association), an explicit feature refinement framework that enhances object discriminativeness across three complementary perspectives. Specifically, we introduce a Spatial Adapter (SA) to integrate depth-aware cues for spatial continuity, a Temporal Adapter (TA) to aggregate historical information for temporal dependencies, and an Identity Adapter (IA) to leverage quality-aware contrastive learning for instance-level separability. Extensive experiments demonstrate that FDTA achieves state-of-the-art performance on multiple challenging MOT benchmarks, including DanceTrack, SportsMOT, and BFT, highlighting the effectiveness of our proposed discriminative embedding enhancement strategy. The code is available at https://github.com/Spongebobbbbbbbb/FDTA.</p>
<h3 id="12-contextual-image-attack-how-visual-context-exposes-multimodal-safety-vulnerabilities">[12] <a href="https://arxiv.org/abs/2512.02973">Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities</a></h3>
<p><em>Yuan Xiong, Ziqi Miao, Lijun Li, Chen Qian, Jie Li, Jing Shao</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ–°å‹å›¾åƒä¸­å¿ƒåŒ–è¶Šç‹±æ”»å‡»æ–¹æ³•CIAï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå°†æœ‰å®³æŸ¥è¯¢åµŒå…¥çœ‹ä¼¼è‰¯æ€§çš„è§†è§‰ä¸Šä¸‹æ–‡ä¸­ï¼Œæ˜¾è‘—æå‡äº†æ”»å‡»æˆåŠŸç‡ï¼Œè¯æ˜äº†è§†è§‰æ¨¡æ€æœ¬èº«æ˜¯æ”»å‡»å…ˆè¿›MLLMçš„æœ‰æ•ˆå‘é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¶Šç‹±æ”»å‡»æ–¹æ³•ä¸»è¦å…³æ³¨æ–‡æœ¬-å›¾åƒäº¤äº’ï¼Œå°†è§†è§‰æ¨¡æ€è§†ä¸ºæ¬¡è¦æç¤ºï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å›¾åƒæ‰¿è½½å¤æ‚ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç‹¬ç‰¹æ½œåŠ›ï¼Œå› æ­¤éœ€è¦å¼€å‘æ›´æœ‰æ•ˆçš„å›¾åƒä¸­å¿ƒåŒ–æ”»å‡»æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸Šä¸‹æ–‡å›¾åƒæ”»å‡»æ–¹æ³•ï¼Œé‡‡ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé€šè¿‡å››ç§ä¸åŒçš„å¯è§†åŒ–ç­–ç•¥å°†æœ‰å®³æŸ¥è¯¢å¾®å¦™åœ°åµŒå…¥çœ‹ä¼¼è‰¯æ€§çš„è§†è§‰ä¸Šä¸‹æ–‡ä¸­ï¼Œå¹¶æ•´åˆäº†ä¸Šä¸‹æ–‡å…ƒç´ å¢å¼ºå’Œè‡ªåŠ¨æ¯’æ€§æ··æ·†æŠ€æœ¯æ¥è¿›ä¸€æ­¥æå‡æ”»å‡»æ•ˆæœã€‚</p>
<p><strong>Result:</strong> åœ¨MMSafetyBench-tinyæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCIAæ–¹æ³•å¯¹GPT-4oå’ŒQwen2.5-VL-72Bæ¨¡å‹çš„æ¯’æ€§è¯„åˆ†åˆ†åˆ«è¾¾åˆ°4.73å’Œ4.83ï¼Œæ”»å‡»æˆåŠŸç‡åˆ†åˆ«è¾¾åˆ°86.31%å’Œ91.07%ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰çš„å·¥ä½œã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜è§†è§‰æ¨¡æ€æœ¬èº«æ˜¯æ”»å‡»å…ˆè¿›å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆå‘é‡ï¼Œæå‡ºçš„å›¾åƒä¸­å¿ƒåŒ–æ”»å‡»æ–¹æ³•æ˜¾è‘—æå‡äº†è¶Šç‹±æ”»å‡»çš„æˆåŠŸç‡ï¼Œæ­ç¤ºäº†å½“å‰MLLMå®‰å…¨å¯¹é½æœºåˆ¶åœ¨è§†è§‰æ”»å‡»æ–¹é¢çš„è„†å¼±æ€§ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>While Multimodal Large Language Models (MLLMs) show remarkable capabilities, their safety alignments are susceptible to jailbreak attacks. Existing attack methods typically focus on text-image interplay, treating the visual modality as a secondary prompt. This approach underutilizes the unique potential of images to carry complex, contextual information. To address this gap, we propose a new image-centric attack method, Contextual Image Attack (CIA), which employs a multi-agent system to subtly embeds harmful queries into seemingly benign visual contexts using four distinct visualization strategies. To further enhance the attack's efficacy, the system incorporate contextual element enhancement and automatic toxicity obfuscation techniques. Experimental results on the MMSafetyBench-tiny dataset show that CIA achieves high toxicity scores of 4.73 and 4.83 against the GPT-4o and Qwen2.5-VL-72B models, respectively, with Attack Success Rates (ASR) reaching 86.31\% and 91.07\%. Our method significantly outperforms prior work, demonstrating that the visual modality itself is a potent vector for jailbreaking advanced MLLMs.</p>
<h3 id="13-skywork-r1v4-toward-agentic-multimodal-intelligence-through-interleaved-thinking-with-images-and-deepresearch">[13] <a href="https://arxiv.org/abs/2512.02395">Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch</a></h3>
<p><em>Yifan Zhang, Liang Hu, Haofeng Sun, Peiyu Wang, Yichen Wei, Shukang Yin, Jiangbo Pei, Wei Shen, Peng Xia, Yi Peng, Tianyidan Xie, Eric Li, Yang Liu, Xuchen Song, Yahui Zhou</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Skywork-R1V4ï¼Œä¸€ä¸ª300äº¿å‚æ•°çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€çš„è§„åˆ’æ¡†æ¶å°†å›¾åƒæ“ä½œä¸ç½‘ç»œæœç´¢èƒ½åŠ›ç›¸ç»“åˆï¼Œä»…ä½¿ç”¨ç›‘ç£å¾®è°ƒå°±å®ç°äº†å…ˆè¿›çš„æ™ºèƒ½ä½“æ€§èƒ½ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†Gemini 2.5 Flashã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤šæ¨¡æ€æ™ºèƒ½ä½“ç³»ç»Ÿå­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™ï¼šå°†å›¾åƒæ“ä½œå’Œç½‘ç»œæœç´¢è§†ä¸ºåˆ†ç¦»èƒ½åŠ›ã€è¿‡åº¦ä¾èµ–æ˜‚è´µçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€ä»¥åŠç¼ºä¹åŸºäºçœŸå®å·¥å…·æ‰§è¡Œè½¨è¿¹çš„è§„åˆ’ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†æ™ºèƒ½ä½“åœ¨å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> Skywork-R1V4æ˜¯ä¸€ä¸ª300äº¿å‚æ•°çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¨¡å‹ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„å¤šæ¨¡æ€è§„åˆ’æ¡†æ¶ï¼Œæ•´åˆäº†ä¸»åŠ¨å›¾åƒæ“ä½œã€æ·±åº¦å¤šæ¨¡æ€æœç´¢ä»¥åŠå…³é”®çš„äº¤é”™æ¨ç†æœºåˆ¶ã€‚æ¨¡å‹ä»…é€šè¿‡ç›‘ç£å¾®è°ƒåœ¨å°‘äº30,000ä¸ªé«˜è´¨é‡è§„åˆ’-æ‰§è¡Œä¸€è‡´æ€§è½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡é€æ­¥ä¸€è‡´æ€§è¿‡æ»¤è¿›è¡ŒéªŒè¯ï¼Œå®Œå…¨é¿å…äº†å¼ºåŒ–å­¦ä¹ çš„ä¾èµ–ã€‚</p>
<p><strong>Result:</strong> Skywork-R1V4åœ¨æ„ŸçŸ¥å’Œå¤šæ¨¡æ€æœç´¢åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼šåœ¨MMSearchä¸Šè·å¾—66.1åˆ†ï¼Œåœ¨FVQAä¸Šè·å¾—67.2åˆ†ï¼Œåœ¨æ‰€æœ‰11ä¸ªæŒ‡æ ‡ä¸Šéƒ½è¶…è¶Šäº†Gemini 2.5 Flashã€‚æ¨¡å‹å±•ç°å‡ºæ–°å…´çš„é•¿æ—¶ç¨‹æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤ŸæˆåŠŸåè°ƒè¶…è¿‡10ä¸ªå·¥å…·è°ƒç”¨æ¥è§£å†³å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç²¾å¿ƒç­–åˆ’çš„ç›‘ç£å­¦ä¹ å•ç‹¬è®­ç»ƒï¼Œæ— éœ€ä¾èµ–å¼ºåŒ–å­¦ä¹ ï¼Œå°±èƒ½å®ç°å¤æ‚çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ™ºèƒ½ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†å½“å‰ä¾èµ–å¼ºåŒ–å­¦ä¹ çš„ä¸»æµèŒƒå¼ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆã€å¯æ‰©å±•çš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation ("thinking with images"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.</p>
<h3 id="14-wise-weighted-iterative-society-of-experts-for-robust-multimodal-multi-agent-debate">[14] <a href="https://arxiv.org/abs/2512.02405">WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate</a></h3>
<p><em>Anoop Cherian, River Doyle, Eyal Ben-Dov, Suhas Lohit, Kuan-Chuan Peng</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†WISEï¼ˆåŠ æƒè¿­ä»£ä¸“å®¶ç¤¾ä¼šï¼‰æ¡†æ¶ï¼Œå°†å¤šæ™ºèƒ½ä½“è¾©è®ºæ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œé€šè¿‡å¼‚æ„ä¸“å®¶åˆ†å·¥å’Œä¸¤é˜¶æ®µè¾©è®ºæ¨¡å‹ï¼Œåœ¨å¤šç§è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šå®ç°äº†2-7%çš„å‡†ç¡®ç‡æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ™ºèƒ½ä½“è¾©è®ºä¸»è¦åº”ç”¨äºçº¯è¯­è¨€ä»»åŠ¡ï¼Œå…¶åœ¨å¤šæ¨¡æ€é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§å°šæœªå……åˆ†æ¢ç´¢ï¼Œè€Œç°ä»£å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šæ ·åŒ–è¯­æ–™å’Œä»»åŠ¡è®­ç»ƒä¸­å½¢æˆäº†äº’è¡¥ä¼˜åŠ¿ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæ•´åˆå¼‚æ„ä¸“å®¶èƒ½åŠ›å¹¶å¤„ç†å¤šæ¨¡æ€æ¨ç†çš„è¾©è®ºæ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†WISEæ¡†æ¶ï¼Œå°†æ™ºèƒ½ä½“åˆ’åˆ†ä¸ºç”Ÿæˆè§£å†³æ–¹æ¡ˆçš„æ±‚è§£å™¨å’ŒéªŒè¯æ­£ç¡®æ€§ã€åˆ†é…æƒé‡å¹¶æä¾›è‡ªç„¶è¯­è¨€åé¦ˆçš„åæ€å™¨ï¼›é‡‡ç”¨æ”¹è¿›çš„Dawid-Skeneç®—æ³•è¿›è¡Œåå¤„ç†ï¼Œæ•´åˆäº†ä¸¤é˜¶æ®µè¾©è®ºæ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æ™ºèƒ½ä½“å“åº”å·®å¼‚å’Œåé¦ˆæƒé‡å˜åŒ–ã€‚</p>
<p><strong>Result:</strong> åœ¨SMART-840ã€VisualPuzzlesã€EvoChart-QAå’Œæ–°æ„å»ºçš„SMART-840++æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒWISEæ¡†æ¶åœ¨å¤šæ ·åŒ–å¤šæ¨¡æ€ä»»åŠ¡å’ŒLLMé…ç½®ä¸­ï¼Œç›¸æ¯”æœ€å…ˆè¿›çš„å¤šæ™ºèƒ½ä½“è¾©è®ºè®¾ç½®å’Œèšåˆæ–¹æ³•ï¼Œå‡†ç¡®ç‡æŒç»­æå‡2-7%ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å¤šæ™ºèƒ½ä½“è¾©è®ºå¯æœ‰æ•ˆæ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†é¢†åŸŸï¼Œå¼‚æ„ä¸“å®¶åˆ†å·¥å’Œä¸¤é˜¶æ®µè¾©è®ºæ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—æå‡æ€§èƒ½ï¼Œä¸ºå¤æ‚å¤šæ¨¡æ€é—®é¢˜æ±‚è§£æä¾›äº†æ¨¡å—åŒ–ä¸”å¯æ³›åŒ–çš„æ¡†æ¶ï¼Œå±•ç¤ºäº†æ•´åˆäº’è¡¥æ¨¡å‹ä¼˜åŠ¿çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents' solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations.</p>
<h3 id="15-generalizing-vision-language-models-with-dedicated-prompt-guidance">[15] <a href="https://arxiv.org/abs/2512.02421">Generalizing Vision-Language Models with Dedicated Prompt Guidance</a></h3>
<p><em>Xinyao Li, Yinjie Min, Hongbo Chen, Zhekai Du, Fengling Li, Jingjing Li</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGuiDGçš„é¢†åŸŸä¸“å®¶å¼•å¯¼æ¡†æ¶ï¼Œé€šè¿‡ç†è®ºåˆ†æå‘ç°å¤šä¸“å®¶æ¨¡å‹ä¼˜äºå•ä¸€é€šç”¨æ¨¡å‹ï¼Œä»è€Œåœ¨è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­å®ç°äº†æ›´å¥½çš„é¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†å‚æ•°æ•ˆç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒæ–¹æ³•é¢ä¸´é¢†åŸŸç‰¹å®šæ€§ä¸é¢†åŸŸæ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„å…³é”®æƒè¡¡ï¼Œé€šå¸¸åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šå¾®è°ƒå•ä¸€é€šç”¨æ¨¡å‹ä¼šæŸå®³å¯¹æœªè§é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦è§£å†³è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸¤é˜¶æ®µé¢†åŸŸä¸“å®¶å¼•å¯¼æ¡†æ¶GuiDGï¼Œé¦–å…ˆé€šè¿‡æç¤ºè°ƒä¼˜è·å¾—æºé¢†åŸŸä¸“å®¶æ¨¡å‹ï¼Œç„¶åå¼•å…¥è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—é€šè¿‡è‡ªé€‚åº”ä¸“å®¶é›†æˆæ¥æŒ‡å¯¼è§†è§‰ç¼–ç å™¨çš„å¾®è°ƒï¼ŒåŒæ—¶æ„å»ºäº†ImageNet-DGåŸºå‡†ç”¨äºå°‘æ ·æœ¬é¢†åŸŸæ³›åŒ–è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†é¢†åŸŸæ³›åŒ–åŸºå‡†å’Œæ–°å»ºçš„ImageNet-DGæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGuiDGåœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶è¶…è¶Šäº†æœ€å…ˆè¿›çš„å¾®è°ƒæ–¹æ³•ï¼ŒéªŒè¯äº†å¤šä¸“å®¶ç­–ç•¥åœ¨é¢†åŸŸæ³›åŒ–æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä»ç†è®ºä¸Šè¯æ˜äº†å¤šä¸“å®¶æ¨¡å‹åœ¨é¢†åŸŸæ³›åŒ–æ–¹é¢çš„ä¼˜åŠ¿ï¼Œæå‡ºçš„GuiDGæ¡†æ¶ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹å¾®è°ƒæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æ„å»ºçš„ImageNet-DGåŸºå‡†ä¸ºå°‘æ ·æœ¬é¢†åŸŸæ³›åŒ–ç ”ç©¶æä¾›äº†é‡è¦è¯„ä¼°å·¥å…·ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.</p>
<h3 id="16-boosting-medical-vision-language-pretraining-via-momentum-self-distillation-under-limited-computing-resources">[16] <a href="https://arxiv.org/abs/2512.02438">Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources</a></h3>
<p><em>Phuc Pham, Nhu Pham, Ngoc Quoc Ly</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆåŠ¨é‡è‡ªè’¸é¦å’Œæ¢¯åº¦ç´¯ç§¯çš„é«˜æ•ˆè§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—é¢†åŸŸæ ‡æ³¨æ•°æ®ç¨€ç¼ºä¸”å¯¹æ¯”å­¦ä¹ è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼Œåœ¨ä¿æŒå•GPUè®­ç»ƒæ•ˆç‡çš„åŒæ—¶å®ç°äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨åŒ»ç–—å¥åº·é¢†åŸŸï¼Œè·å–è¯¦ç»†æ ‡æ³¨æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› æ­¤éœ€è¦é²æ£’çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼Œå¯¹æ¯”å­¦ä¹ ä½œä¸ºè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„å…³é”®èŒƒå¼ï¼Œé€šå¸¸éœ€è¦å¤§æ‰¹é‡è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ï¼Œè¿™å¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å¾€å¾€ä»…é™äºèµ„æºå……è¶³çš„æœºæ„ã€‚æ­¤å¤–ï¼Œåœ¨åŒ»ç–—æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œéœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶ä»æ•°æ®å’Œæ¨¡å‹ä¸­æå–çŸ¥è¯†ä»¥æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ç»“åˆåŠ¨é‡æ–¹æ³•å’Œè’¸é¦æŠ€æœ¯ï¼ŒåŒæ—¶è§£å†³è®¡ç®—æ•ˆç‡å’ŒçŸ¥è¯†åˆ©ç”¨é—®é¢˜ã€‚å…·ä½“åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦è´¡çŒ®ï¼šä¸€æ˜¯åˆ©ç”¨åŠ¨é‡è‡ªè’¸é¦æ¥å¢å¼ºå¤šæ¨¡æ€å­¦ä¹ ï¼ŒäºŒæ˜¯å°†åŠ¨é‡æœºåˆ¶ä¸æ¢¯åº¦ç´¯ç§¯ç›¸ç»“åˆï¼Œåœ¨ä¸å¢åŠ èµ„æºæ¶ˆè€—çš„æƒ…å†µä¸‹æ‰©å¤§æœ‰æ•ˆæ‰¹é‡å¤§å°ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨å•GPUä¸Šå®ç°é«˜æ•ˆçš„è®­ç»ƒè¿‡ç¨‹ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸­å–å¾—äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å°‘æ ·æœ¬é€‚åº”æ–¹é¢æä¾›äº†æ˜¾è‘—æå‡ï¼Œå®ç°äº†è¶…è¿‡90%çš„AUC-ROCï¼Œå¹¶å°†æ£€ç´¢ä»»åŠ¡æ€§èƒ½æé«˜äº†2-3%ã€‚é‡è¦çš„æ˜¯ï¼Œè¯¥æ–¹æ³•åœ¨å•GPUä¸Šå®ç°äº†é«˜è®­ç»ƒæ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†åˆç†çš„è®­ç»ƒæ—¶é—´ï¼Œæ˜¾è‘—é™ä½äº†èµ„æºéœ€æ±‚ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡åŠ¨é‡è‡ªè’¸é¦å’Œæ¢¯åº¦ç´¯ç§¯çš„é›†æˆï¼Œä¸ºé«˜æ•ˆå¤šæ¨¡æ€å­¦ä¹ æä¾›äº†åˆ›æ–°è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°é«˜æ€§èƒ½è§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºåŒ»ç–—å¥åº·ç­‰æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é¢†åŸŸï¼Œä¸ºå‡å°‘èµ„æºéœ€æ±‚åŒæ—¶æå‡æ€§èƒ½æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œæ¨åŠ¨äº†é«˜æ•ˆå¤šæ¨¡æ€å­¦ä¹ çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>In medical healthcare, obtaining detailed annotations is challenging, highlighting the need for robust Vision-Language Models (VLMs). Pretrained VLMs enable fine-tuning on small datasets or zero-shot inference, achieving performance comparable to task-specific models. Contrastive learning (CL) is a key paradigm for training VLMs but inherently requires large batch sizes for effective learning, making it computationally demanding and often limited to well-resourced institutions. Moreover, with limited data in healthcare, it is important to prioritize knowledge extraction from both data and models during training to improve performance. Therefore, we focus on leveraging the momentum method combined with distillation to simultaneously address computational efficiency and knowledge exploitation. Our contributions can be summarized as follows: (1) leveraging momentum self-distillation to enhance multimodal learning, and (2) integrating momentum mechanisms with gradient accumulation to enlarge the effective batch size without increasing resource consumption. Our method attains competitive performance with state-of-the-art (SOTA) approaches in zero-shot classification, while providing a substantial boost in the few-shot adaption, achieving over 90% AUC-ROC and improving retrieval tasks by 2-3%. Importantly, our method achieves high training efficiency with a single GPU while maintaining reasonable training time. Our approach aims to advance efficient multimodal learning by reducing resource requirements while improving performance over SOTA methods. The implementation of our method is available at https://github.com/phphuc612/MSD .</p>
<h3 id="17-nuscenes-revisited-progress-and-challenges-in-autonomous-driving">[17] <a href="https://arxiv.org/abs/2512.02448">nuScenes Revisited: Progress and Challenges in Autonomous Driving</a></h3>
<p><em>Whye Kit Fong, Venice Erin Liong, Kok Seang Tan, Holger Caesar</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡å¯¹å¹¿æ³›ä½¿ç”¨çš„è‡ªåŠ¨é©¾é©¶æ•°æ®é›†nuScenesè¿›è¡Œäº†å…¨é¢å›é¡¾ï¼Œæ­ç¤ºäº†å…¶åˆ›å»ºç»†èŠ‚å’ŒæŠ€æœ¯æ ‡å‡†ï¼Œå¹¶åˆ†æäº†è¯¥æ•°æ®é›†å¯¹åç»­è‡ªåŠ¨é©¾é©¶ç ”ç©¶å’Œæ•°æ®é›†å‘å±•çš„æ·±è¿œå½±å“ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªåŠ¨é©¾é©¶å’Œé«˜çº§é©¾é©¶è¾…åŠ©ç³»ç»Ÿä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œè€ŒnuScenesä½œä¸ºæœ€å¹¿æ³›ä½¿ç”¨çš„è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¹‹ä¸€ï¼Œå…¶åˆ›å»ºç»†èŠ‚å’ŒæŠ€æœ¯æ ‡å‡†åœ¨å­¦æœ¯æ–‡çŒ®ä¸­å°šæœªå……åˆ†æŠ«éœ²ï¼Œéœ€è¦ç³»ç»Ÿå›é¡¾å…¶è®¾è®¡ç†å¿µã€æŠ€æœ¯å®ç°ä»¥åŠå¯¹æ•´ä¸ªç ”ç©¶é¢†åŸŸçš„å½±å“ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨å›é¡¾æ€§åˆ†ææ–¹æ³•ï¼Œæ·±å…¥æ¢è®¨nuScenesæ•°æ®é›†çš„åˆ›å»ºè¿‡ç¨‹ã€æŠ€æœ¯ç»†èŠ‚åŠå…¶æ‰©å±•ç‰ˆæœ¬ï¼ˆnuImageså’ŒPanoptic nuScenesï¼‰ï¼Œé€šè¿‡åˆ†ææ•°æ®é›†çš„å¤šæ¨¡æ€ä¼ æ„Ÿå™¨èåˆè®¾è®¡ã€æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•æ¡†æ¶ä»¥åŠæ¶µç›–æ„ŸçŸ¥ã€å®šä½ä¸å»ºå›¾ã€é¢„æµ‹å’Œè§„åˆ’ç­‰ä»»åŠ¡çš„ç»¼åˆç‰¹æ€§ï¼Œæ­ç¤ºå…¶æŠ€æœ¯å®ç°åŸç†ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶é¦–æ¬¡å…¬å¼€äº†nuScenesæ•°æ®é›†åˆ›å»ºçš„è¯¦ç»†æŠ€æœ¯ç»†èŠ‚ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºé¦–ä¸ªåŒ…å«é›·è¾¾æ•°æ®ã€è¦†ç›–ä¸¤å¤§æ´²å¤šæ ·åŒ–åŸå¸‚åœºæ™¯ã€é‡‡ç”¨å®Œå…¨è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨å…¬å…±é“è·¯ä¸Šé‡‡é›†çš„æ•°æ®é›†çš„åˆ›æ–°æ€§ï¼Œå¹¶ç³»ç»Ÿåˆ†æäº†è¯¥æ•°æ®é›†å¯¹åç»­ä¼—å¤šè‡ªåŠ¨é©¾é©¶æ•°æ®é›†çš„æ·±è¿œå½±å“å’Œå®šä¹‰çš„æŠ€æœ¯æ ‡å‡†ã€‚</p>
<p><strong>Conclusion:</strong> nuScenesæ•°æ®é›†ä¸ä»…ä¸ºè‡ªåŠ¨é©¾é©¶ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†å¹³å°ï¼Œè¿˜é€šè¿‡å…¶å¤šæ¨¡æ€ä¼ æ„Ÿå™¨èåˆã€æ ‡å‡†åŒ–è¯„ä¼°å’Œå¤šæ ·åŒ–ä»»åŠ¡è®¾è®¡ï¼Œæ·±åˆ»å½±å“äº†æ•´ä¸ªé¢†åŸŸçš„å‘å±•æ–¹å‘ï¼Œå…¶æŠ€æœ¯æ ‡å‡†å’Œè®¾è®¡ç†å¿µè‡³ä»Šä»è¢«å¹¿æ³›é‡‡ç”¨ï¼Œæˆä¸ºè‡ªåŠ¨é©¾é©¶æ•°æ®é›†å‘å±•çš„é‡Œç¨‹ç¢‘ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Autonomous Vehicles (AV) and Advanced Driver Assistance Systems (ADAS) have been revolutionized by Deep Learning. As a data-driven approach, Deep Learning relies on vast amounts of driving data, typically labeled in great detail. As a result, datasets, alongside hardware and algorithms, are foundational building blocks for the development of AVs. In this work we revisit one of the most widely used autonomous driving datasets: the nuScenes dataset. nuScenes exemplifies key trends in AV development, being the first dataset to include radar data, to feature diverse urban driving scenes from two continents, and to be collected using a fully autonomous vehicle operating on public roads, while also promoting multi-modal sensor fusion, standardized benchmarks, and a broad range of tasks including perception, localization \&amp; mapping, prediction and planning. We provide an unprecedented look into the creation of nuScenes, as well as its extensions nuImages and Panoptic nuScenes, summarizing many technical details that have hitherto not been revealed in academic publications. Furthermore, we trace how the influence of nuScenes impacted a large number of other datasets that were released later and how it defined numerous standards that are used by the community to this day. Finally, we present an overview of both official and unofficial tasks using the nuScenes dataset and review major methodological developments, thereby offering a comprehensive survey of the autonomous driving literature, with a particular focus on nuScenes.</p>
<h3 id="18-ucagents-unidirectional-convergence-for-visual-evidence-anchored-multi-agent-medical-decision-making">[18] <a href="https://arxiv.org/abs/2512.02485">UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making</a></h3>
<p><em>Qianhan Feng, Zhongzhen Huang, Yakun Zhu, Xiaofan Zhang, Qi Dou</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºUCAgentsï¼Œä¸€ç§ç”¨äºåŒ»å­¦è§†è§‰é—®ç­”çš„åˆ†å±‚å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–è¯æ®å®¡è®¡å®ç°å•å‘æ”¶æ•›ï¼Œåœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§çš„åŒæ—¶å¤§å¹…é™ä½è®¡ç®—æˆæœ¬ï¼Œè§£å†³äº†ç°æœ‰å¤šæ™ºèƒ½ä½“æ–¹æ³•ä¸­æ¨ç†è„±ç¦»è§†è§‰è¯æ®å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦è¯Šæ–­ä¸­å­˜åœ¨æ¨ç†è„±ç¦»é—®é¢˜ï¼Œå³è¯­è¨€æµç•…çš„è§£é‡Šä¸å¯éªŒè¯çš„å›¾åƒè¯æ®è„±èŠ‚ï¼ŒæŸå®³ä¸´åºŠä¿¡ä»»ã€‚ç°æœ‰çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿå¤šå­¦ç§‘å›¢é˜Ÿè¾©è®ºæ¥ç¼“è§£å•ä¸€æ¨¡å‹åå·®ï¼Œä½†å¼€æ”¾å¼è®¨è®ºä¼šæ”¾å¤§æ–‡æœ¬å™ªå£°å’Œè®¡ç®—æˆæœ¬ï¼Œä¸”æœªèƒ½å°†æ¨ç†é”šå®šåœ¨è§†è§‰è¯æ®è¿™ä¸€åŒ»å­¦å†³ç­–çš„åŸºçŸ³ä¸Šã€‚</p>
<p><strong>Method:</strong> æå‡ºUCAgentsåˆ†å±‚å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–è¯æ®å®¡è®¡å¼ºåˆ¶æ‰§è¡Œå•å‘æ”¶æ•›ã€‚è¯¥æ¡†æ¶å—ä¸´åºŠå·¥ä½œæµç¨‹å¯å‘ï¼Œç¦æ­¢ç«‹åœºå˜æ›´å¹¶å°†æ™ºèƒ½ä½“äº¤äº’é™åˆ¶åœ¨é’ˆå¯¹æ€§è¯æ®éªŒè¯ï¼ŒæŠ‘åˆ¶ä¿®è¾æ¼‚ç§»åŒæ—¶å¢å¼ºè§†è§‰ä¿¡å·æå–ã€‚å¼•å…¥å•è½®è¯¢é—®è®¨è®ºä»¥æ­ç¤ºè§†è§‰-æ–‡æœ¬é”™ä½çš„æ½œåœ¨é£é™©ï¼Œé€šè¿‡ä¿¡æ¯è®ºå½¢å¼åŒ–åŒé‡å™ªå£°ç“¶é¢ˆï¼Œå…±åŒçº¦æŸè§†è§‰æ¨¡ç³Šæ€§å’Œæ–‡æœ¬å™ªå£°ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªåŒ»å­¦VQAåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒUCAgentsåœ¨PathVQAä¸Šè¾¾åˆ°71.3%çš„å‡†ç¡®ç‡ï¼Œæ¯”æœ€å…ˆè¿›æ–¹æ³•æé«˜6.0%ï¼ŒåŒæ—¶é™ä½87.7%çš„tokenæˆæœ¬ã€‚è¯„ä¼°ç»“æœè¿›ä¸€æ­¥è¯å®UCAgentsåœ¨æ­ç¤ºæ›´å¤šè§†è§‰è¯æ®ä¸é¿å…æ··æ·†æ€§æ–‡æœ¬å¹²æ‰°ä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œå±•ç°äº†è¯Šæ–­å¯é æ€§å’Œè®¡ç®—æ•ˆç‡çš„åŒé‡ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> UCAgentsé€šè¿‡ç»“æ„åŒ–è¯æ®å®¡è®¡å’Œå•å‘æ”¶æ•›æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†è„±ç¦»é—®é¢˜ï¼Œåœ¨ä¿æŒè¯Šæ–­å¯é æ€§çš„åŒæ—¶æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡ï¼Œä¸ºç°å®ä¸–ç•Œä¸´åºŠéƒ¨ç½²æä¾›äº†å…³é”®çš„æŠ€æœ¯åŸºç¡€ã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å—ä¸´åºŠå·¥ä½œæµç¨‹å¯å‘çš„å¤šæ™ºèƒ½ä½“è®¾è®¡æ¥å¹³è¡¡è§†è§‰è¯æ®æå–ä¸æ–‡æœ¬å™ªå£°æ§åˆ¶ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) show promise in medical diagnosis, yet suffer from reasoning detachment, where linguistically fluent explanations drift from verifiable image evidence, undermining clinical trust. Recent multi-agent frameworks simulate Multidisciplinary Team (MDT) debates to mitigate single-model bias, but open-ended discussions amplify textual noise and computational cost while failing to anchor reasoning to visual evidence, the cornerstone of medical decision-making. We propose UCAgents, a hierarchical multi-agent framework enforcing unidirectional convergence through structured evidence auditing. Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, suppressing rhetorical drift while amplifying visual signal extraction. In UCAgents, a one-round inquiry discussion is introduced to uncover potential risks of visual-textual misalignment. This design jointly constrains visual ambiguity and textual noise, a dual-noise bottleneck that we formalize via information theory. Extensive experiments on four medical VQA benchmarks show UCAgents achieves superior accuracy (71.3% on PathVQA, +6.0% over state-of-the-art) with 87.7% lower token cost, the evaluation results further confirm that UCAgents strikes a balance between uncovering more visual evidence and avoiding confusing textual interference. These results demonstrate that UCAgents exhibits both diagnostic reliability and computational efficiency critical for real-world clinical deployment. Code is available at https://github.com/fqhank/UCAgents.</p>
<h3 id="19-masking-matters-unlocking-the-spatial-reasoning-capabilities-of-llms-for-3d-scene-language-understanding">[19] <a href="https://arxiv.org/abs/2512.02487">Masking Matters: Unlocking the Spatial Reasoning Capabilities of LLMs for 3D Scene-Language Understanding</a></h3>
<p><em>Yerim Jeon, Miso Lee, WonJun Moon, Jae-Pil Heo</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†3D-SLIMï¼Œä¸€ç§é’ˆå¯¹3Dåœºæ™¯è¯­è¨€ç†è§£çš„è‡ªé€‚åº”æ³¨æ„åŠ›æ©ç ç­–ç•¥ï¼Œé€šè¿‡æ›¿æ¢å› æœæ³¨æ„åŠ›æ©ç æ¥è§£å†³é¡ºåºåå·®å’Œå—é™æŒ‡ä»¤æ³¨æ„åŠ›é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†3Då¤šæ¨¡æ€æ¨ç†æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰3Dåœºæ™¯è¯­è¨€ç†è§£æ–¹æ³•é€šå¸¸é‡‡ç”¨è¯­è¨€å»ºæ¨¡ä¸­çš„æ ‡å‡†è§£ç å™¨ï¼Œè¿™äº›è§£ç å™¨ä¾èµ–å› æœæ³¨æ„åŠ›æ©ç ï¼Œå¯¼è‡´ä¸¤ä¸ªåŸºæœ¬å†²çªï¼šé¡ºåºæ— å…³3Då¯¹è±¡ä¹‹é—´çš„é¡ºåºåå·®ï¼Œä»¥åŠå—é™çš„å¯¹è±¡-æŒ‡ä»¤æ³¨æ„åŠ›ï¼Œé˜»ç¢äº†ä»»åŠ¡ç‰¹å®šçš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡º3D-SLIMï¼Œä¸€ç§è‡ªé€‚åº”æ³¨æ„åŠ›æ©ç ç­–ç•¥ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå‡ ä½•è‡ªé€‚åº”æ©ç æ ¹æ®ç©ºé—´å¯†åº¦è€Œéæ ‡è®°é¡ºåºçº¦æŸæ³¨æ„åŠ›ï¼ŒæŒ‡ä»¤æ„ŸçŸ¥æ©ç ä½¿å¯¹è±¡æ ‡è®°èƒ½å¤Ÿç›´æ¥è®¿é—®æŒ‡ä»¤ä¸Šä¸‹æ–‡ï¼Œè¯¥æ–¹æ³•æ— éœ€æ¶æ„ä¿®æ”¹æˆ–é¢å¤–å‚æ•°ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’ŒLLMåŸºçº¿ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†3D-SLIMçš„æœ‰æ•ˆæ€§ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–çš„3Dåœºæ™¯è¯­è¨€ä»»åŠ¡ä¸Šå¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¼ºè°ƒäº†è§£ç å™¨è®¾è®¡åœ¨3Då¤šæ¨¡æ€æ¨ç†ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡è®¾è®¡é€‚åº”3Dç©ºé—´ç»“æ„çš„æ³¨æ„åŠ›æ©ç å¯ä»¥å…‹æœä¼ ç»Ÿå› æœæ©ç åœ¨3Dåœºæ™¯ç†è§£ä¸­çš„å±€é™æ€§ï¼Œä¸º3Då¤šæ¨¡æ€æ¨ç†æä¾›äº†ç®€å•è€Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ­ç¤ºäº†è§£ç å™¨è®¾è®¡åœ¨è¯¥é¢†åŸŸçš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Recent advances in 3D scene-language understanding have leveraged Large Language Models (LLMs) for 3D reasoning by transferring their general reasoning ability to 3D multi-modal contexts. However, existing methods typically adopt standard decoders from language modeling, which rely on a causal attention mask. This design introduces two fundamental conflicts in 3D scene understanding: sequential bias among order-agnostic 3D objects and restricted object-instruction attention, hindering task-specific reasoning. To overcome these limitations, we propose 3D Spatial Language Instruction Mask (3D-SLIM), an effective masking strategy that replaces the causal mask with an adaptive attention mask tailored to the spatial structure of 3D scenes. Our 3D-SLIM introduces two key components: a Geometry-adaptive Mask that constrains attention based on spatial density rather than token order, and an Instruction-aware Mask that enables object tokens to directly access instruction context. This design allows the model to process objects based on their spatial relationships while being guided by the user's task. 3D-SLIM is simple, requires no architectural modifications, and adds no extra parameters, yet it yields substantial performance improvements across diverse 3D scene-language tasks. Extensive experiments across multiple benchmarks and LLM baselines validate its effectiveness and underscore the critical role of decoder design in 3D multi-modal reasoning.</p>
<h3 id="20-does-hearing-help-seeing-investigating-audio-video-joint-denoising-for-video-generation">[20] <a href="https://arxiv.org/abs/2512.02457">Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation</a></h3>
<p><em>Jianzong Wu, Hao Lian, Dachao Hao, Ye Tian, Qingyu Shi, Biaolong Chen, Hao Jiang</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯æ˜éŸ³é¢‘-è§†é¢‘è”åˆå»å™ªè®­ç»ƒä¸ä»…èƒ½æå‡è·¨æ¨¡æ€åŒæ­¥æ€§ï¼Œè¿˜èƒ½æ˜¾è‘—æ”¹å–„è§†é¢‘ç”Ÿæˆè´¨é‡ï¼Œå³ä½¿ä»…å…³æ³¨è§†é¢‘æ¨¡æ€æœ¬èº«ã€‚ç ”ç©¶é€šè¿‡å‚æ•°é«˜æ•ˆçš„AVFullDiTæ¶æ„ï¼Œæ­ç¤ºäº†éŸ³é¢‘ä½œä¸ºç‰¹æƒä¿¡å·å¯¹è§†é¢‘åŠ¨æ€ç‰©ç†è§„å¾‹å»ºæ¨¡çš„æ­£åˆ™åŒ–ä½œç”¨ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨éŸ³é¢‘-è§†é¢‘ç”Ÿæˆç³»ç»Ÿçš„è·¨æ¨¡æ€åŒæ­¥æ€§ä¼˜åŠ¿ï¼Œä½†å°šæœªç³»ç»Ÿæ¢ç©¶è”åˆå»å™ªè®­ç»ƒæ˜¯å¦å¯¹å•ä¸€è§†é¢‘æ¨¡æ€çš„ç”Ÿæˆè´¨é‡æœ¬èº«å…·æœ‰æå‡ä½œç”¨ã€‚æœ¬æ–‡æ—¨åœ¨å›ç­”ä¸€ä¸ªåŸºç¡€æ€§é—®é¢˜ï¼šå³ä½¿ä»…å…³æ³¨è§†é¢‘è´¨é‡ï¼ŒéŸ³é¢‘-è§†é¢‘è”åˆå»å™ªè®­ç»ƒèƒ½å¦æ”¹å–„è§†é¢‘ç”Ÿæˆæ•ˆæœï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åŒ…å«å¤æ‚è¿åŠ¨æ¨¡å¼çš„æŒ‘æˆ˜æ€§åœºæ™¯ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†å‚æ•°é«˜æ•ˆçš„Audio-Video Full DiTï¼ˆAVFullDiTï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œæ–‡æœ¬åˆ°éŸ³é¢‘ï¼ˆT2Aï¼‰æ¨¡å—è¿›è¡Œè”åˆå»å™ªè®­ç»ƒã€‚åœ¨ç›¸åŒå®éªŒè®¾ç½®ä¸‹ï¼ŒåŒæ—¶è®­ç»ƒäº†T2AVè”åˆæ¨¡å‹å’Œä»…T2Vçš„å¯¹ç…§æ¨¡å‹ï¼Œä»¥è¿›è¡Œå…¬å¹³æ¯”è¾ƒå¹¶éš”ç¦»éŸ³é¢‘ä¿¡å·çš„å½±å“ã€‚</p>
<p><strong>Result:</strong> å®éªŒé¦–æ¬¡æä¾›äº†ç³»ç»Ÿæ€§è¯æ®ï¼Œè¡¨æ˜éŸ³é¢‘-è§†é¢‘è”åˆå»å™ªè®­ç»ƒç¡®å®èƒ½å¸¦æ¥è¶…è¶ŠåŒæ­¥æ€§çš„è´¨é‡æå‡ã€‚åœ¨åŒ…å«å¤§å¹…åº¦å’Œç‰©ä½“æ¥è§¦è¿åŠ¨çš„æŒ‘æˆ˜æ€§å­é›†ä¸Šï¼Œè”åˆè®­ç»ƒæ¨¡å‹è¡¨ç°å‡ºæŒç»­çš„æ€§èƒ½æ”¹è¿›ï¼Œè§†é¢‘ç”Ÿæˆè´¨é‡æ˜¾è‘—ä¼˜äºä»…è§†é¢‘è®­ç»ƒæ¨¡å‹ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å‡è®¾éŸ³é¢‘é¢„æµ‹ä½œä¸ºç‰¹æƒä¿¡å·ï¼Œä¿ƒä½¿æ¨¡å‹å†…åŒ–è§†è§‰äº‹ä»¶ä¸å…¶å£°å­¦åæœä¹‹é—´çš„å› æœå…³ç³»ï¼Œä»è€Œæ­£åˆ™åŒ–è§†é¢‘åŠ¨æ€å¹¶æå‡ç‰©ç†åˆç†æ€§ã€‚è¿™ä¸€å‘ç°è¡¨æ˜è·¨æ¨¡æ€ååŒè®­ç»ƒæ˜¯å¼€å‘æ›´å¼ºã€æ›´å…·ç‰©ç†åŸºç¡€çš„ä¸–ç•Œæ¨¡å‹çš„æœ‰æ•ˆé€”å¾„ï¼Œä¸ºå¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹è®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision $\times$ impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.</p>
<h3 id="21-from-panel-to-pixel-zoom-in-vision-language-pretraining-from-biomedical-scientific-literature">[21] <a href="https://arxiv.org/abs/2512.02566">From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature</a></h3>
<p><em>Kun Yuan, Min Woo Sun, Zhen Chen, Alejandro Lozano, Xiangteng He, Shi Li, Nassir Navab, Xiaoxiao Sun, Nicolas Padoy, Serena Yeung-Levy</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Panel2Patchï¼Œä¸€ç§ä»ç”Ÿç‰©åŒ»å­¦ç§‘å­¦æ–‡çŒ®ä¸­æŒ–æ˜å±‚æ¬¡ç»“æ„çš„æ–°æ•°æ®ç®¡é“ï¼Œå°†å¤šé¢æ¿ã€æ ‡è®°ä¸°å¯Œçš„å›¾å½¢åŠå…¶å‘¨å›´æ–‡æœ¬è½¬æ¢ä¸ºå¤šç²’åº¦ç›‘ç£ï¼Œç”¨äºè®­ç»ƒç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½å¹¶å‡å°‘äº†é¢„è®­ç»ƒæ•°æ®éœ€æ±‚ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€é¢„è®­ç»ƒé€šå¸¸å°†ä¸°å¯Œçš„ç§‘å­¦å›¾å½¢å’Œæ–‡æœ¬å‹ç¼©ä¸ºç²—ç•¥çš„å›¾å½¢çº§é…å¯¹ï¼Œä¸¢å¼ƒäº†ä¸´åºŠåŒ»ç”Ÿåœ¨å®é™…å…³æ³¨å±€éƒ¨ç»“æ„æ—¶æ‰€ä¾èµ–çš„ç»†ç²’åº¦å¯¹åº”å…³ç³»ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹å¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­ç²¾ç»†è¯­ä¹‰çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> Panel2Patchæ•°æ®ç®¡é“é€šè¿‡è§£æç§‘å­¦å›¾å½¢çš„å¸ƒå±€ã€é¢æ¿å’Œè§†è§‰æ ‡è®°ï¼Œæ„å»ºäº†å›¾å½¢ã€é¢æ¿å’Œè¡¥ä¸ä¸‰ä¸ªå±‚æ¬¡çš„è§†è§‰è¯­è¨€å¯¹é½é…å¯¹ï¼Œå¹¶åŸºäºæ­¤å±‚æ¬¡åŒ–è¯­æ–™å¼€å‘äº†ç²’åº¦æ„ŸçŸ¥çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œç»Ÿä¸€äº†ä»ç²—ç²’åº¦æ•™å­¦æè¿°åˆ°ç»†ç²’åº¦åŒºåŸŸèšç„¦çŸ­è¯­çš„å¼‚æ„ç›®æ ‡ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼ŒPanel2Pipelineä»…éœ€å°‘é‡æ–‡çŒ®å›¾å½¢å³å¯æå–æ¯”å…ˆå‰ç®¡é“æ›´æœ‰æ•ˆçš„ç›‘ç£ä¿¡å·ï¼Œåœ¨å‡å°‘é¢„è®­ç»ƒæ•°æ®é‡çš„åŒæ—¶å®ç°äº†æ˜¾è‘—æ›´å¥½çš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯äº†å±‚æ¬¡åŒ–å¤šç²’åº¦ç›‘ç£çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ä»ç°æœ‰ç§‘å­¦æ–‡çŒ®ä¸­æŒ–æ˜å±‚æ¬¡ç»“æ„ä¿¡æ¯çš„é‡è¦æ€§ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†æ›´ç²¾ç»†çš„ç›‘ç£ä¿¡å·ï¼Œå‡å°‘äº†å¤§è§„æ¨¡æ•°æ®éœ€æ±‚ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„ç”Ÿç‰©åŒ»å­¦å¤šæ¨¡æ€æ¨¡å‹å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>There is a growing interest in developing strong biomedical vision-language models. A popular approach to achieve robust representations is to use web-scale scientific data. However, current biomedical vision-language pretraining typically compresses rich scientific figures and text into coarse figure-level pairs, discarding the fine-grained correspondences that clinicians actually rely on when zooming into local structures. To tackle this issue, we introduce Panel2Patch, a novel data pipeline that mines hierarchical structure from existing biomedical scientific literature, i.e., multi-panel, marker-heavy figures and their surrounding text, and converts them into multi-granular supervision. Given scientific figures and captions, Panel2Patch parses layouts, panels, and visual markers, then constructs hierarchical aligned vision-language pairs at the figure, panel, and patch levels, preserving local semantics instead of treating each figure as a single data sample. Built on this hierarchical corpus, we develop a granularity-aware pretraining strategy that unifies heterogeneous objectives from coarse didactic descriptions to fine region-focused phrases. By applying Panel2Patch to only a small set of the literature figures, we extract far more effective supervision than prior pipelines, enabling substantially better performance with less pretraining data.</p>
<h3 id="22-vision-to-geometry-3d-spatial-memory-for-sequential-embodied-mllm-reasoning-and-exploration">[22] <a href="https://arxiv.org/abs/2512.02458">Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration</a></h3>
<p><em>Zhongyi Cai, Yi Du, Chen Wang, Yu Kong</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†SEER-BenchåŸºå‡†å’Œ3DSPMRæ–¹æ³•ï¼Œé¦–æ¬¡å°†å‡ ä½•ä¿¡æ¯æ˜¾å¼èå…¥å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´ç†è§£ä¸­ï¼Œä»¥è§£å†³é¡ºåºå…·èº«ä»»åŠ¡ä¸­ç©ºé—´çŸ¥è¯†å¤ç”¨å’Œæ¢ç´¢æ¨ç†çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å®¤å†…å…·èº«ä»»åŠ¡ç ”ç©¶é€šå¸¸è¦æ±‚æ™ºèƒ½ä½“ä¸»åŠ¨æ¢ç´¢æœªçŸ¥ç¯å¢ƒå¹¶è¿›è¡Œåœºæ™¯æ¨ç†ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œæ™ºèƒ½ä½“å¸¸é¢ä¸´é¡ºåºä»»åŠ¡åœºæ™¯ï¼Œå…¶ä¸­æ¯ä¸ªæ–°å­ä»»åŠ¡éƒ½ä¾èµ–äºå‰ä¸€ä¸ªä»»åŠ¡çš„å®Œæˆï¼Œä¸”æŸäº›å­ä»»åŠ¡å¯èƒ½ä¸å¯è¡Œã€‚ä¸å•ä»»åŠ¡è®¾ç½®ç›¸æ¯”ï¼Œæ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå¦‚ä½•å¤ç”¨å…ˆå‰æ¢ç´¢ç§¯ç´¯çš„ç©ºé—´çŸ¥è¯†æ¥æ”¯æŒåç»­æ¨ç†å’Œæ¢ç´¢ï¼Œè¿™æ˜¯ä¸€ä¸ªå°šæœªå……åˆ†æ¢ç´¢ä½†å…·æœ‰å®é™…æ„ä¹‰çš„å…·èº«AIæŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†SEER-Benché¡ºåºå…·èº«æ¢ç´¢ä¸æ¨ç†åŸºå‡†ï¼Œæ¶µç›–å…·èº«é—®ç­”å’Œå…·èº«å¤šæ¨¡æ€å¯¼èˆªä¸¤ä¸ªç»å…¸ä»»åŠ¡ã€‚åŸºäºæ­¤åŸºå‡†ï¼Œæå‡ºäº†3DSPMRæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å·²æ¢ç´¢åŒºåŸŸçš„å…³ç³»ã€è§†è§‰å’Œå‡ ä½•çº¿ç´¢æ¥å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é¡ºåºå…·èº«ä»»åŠ¡ä¸­çš„æ¨ç†å’Œæ¢ç´¢èƒ½åŠ›ã€‚è¿™æ˜¯é¦–æ¬¡å°†å‡ ä½•ä¿¡æ¯æ˜¾å¼èå…¥åŸºäºMLLMçš„ç©ºé—´ç†è§£å’Œæ¨ç†ä¸­ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒéªŒè¯è¡¨æ˜ï¼Œ3DSPMRåœ¨é¡ºåºEQAå’ŒEMNä»»åŠ¡ä¸Šå‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥æ–¹æ³•åœ¨SEER-BenchåŸºå‡†ä¸Šçš„è¡¨ç°è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é¡ºåºä»»åŠ¡ä¸­ç©ºé—´çŸ¥è¯†å¤ç”¨å’Œæ¢ç´¢æ¨ç†æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é¦–æ¬¡å°†å‡ ä½•ä¿¡æ¯æ˜¾å¼æ•´åˆåˆ°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´ç†è§£æ¡†æ¶ä¸­ï¼Œä¸ºè§£å†³é¡ºåºå…·èº«ä»»åŠ¡ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜æä¾›äº†æ–°æ€è·¯ã€‚3DSPMRæ–¹æ³•å±•ç¤ºäº†å…³ç³»ã€è§†è§‰å’Œå‡ ä½•çº¿ç´¢èåˆåœ¨å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥å…·èº«AIç³»ç»Ÿåœ¨å¤æ‚é¡ºåºä»»åŠ¡ä¸­çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.</p>
<h3 id="23-reasoning-aware-multimodal-fusion-for-hateful-video-detection">[23] <a href="https://arxiv.org/abs/2512.02743">Reasoning-Aware Multimodal Fusion for Hateful Video Detection</a></h3>
<p><em>Shuonan Yang, Tailin Chen, Jiangbei Yue, Guangliang Cheng, Jianbo Jiao, Zeyu Fu</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨ç†æ„ŸçŸ¥çš„å¤šæ¨¡æ€èåˆï¼ˆRAMFï¼‰æ¡†æ¶ï¼Œé€šè¿‡å±€éƒ¨-å…¨å±€ä¸Šä¸‹æ–‡èåˆå’Œè¯­ä¹‰äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è§£å†³å¤šæ¨¡æ€ä»‡æ¨è§†é¢‘æ£€æµ‹ä¸­çš„è¯­ä¹‰èåˆéš¾é¢˜ï¼Œå¹¶å¼•å…¥å¯¹æŠ—æ€§æ¨ç†è¿‡ç¨‹å¢å¼ºå¯¹å¾®å¦™ä»‡æ¨æ„å›¾çš„ç†è§£ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨çº¿è§†é¢‘ä¸­çš„ä»‡æ¨è¨€è®ºæ—¥ç›Šæˆä¸ºæ•°å­—å¹³å°çš„ä¸¥é‡å¨èƒï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆèåˆå¤šæ¨¡æ€é—´çš„å¤æ‚è¯­ä¹‰å…³ç³»ï¼Œä¸”ç¼ºä¹å¯¹å¾®å¦™ä»‡æ¨å†…å®¹çš„ç†è§£èƒ½åŠ›ï¼Œè¿™æ„æˆäº†å½“å‰ç ”ç©¶çš„ä¸»è¦å±€é™ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†æ¨ç†æ„ŸçŸ¥çš„å¤šæ¨¡æ€èåˆï¼ˆRAMFï¼‰æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå±€éƒ¨-å…¨å±€ä¸Šä¸‹æ–‡èåˆï¼ˆLGCFï¼‰ç”¨äºæ•æ‰å±€éƒ¨æ˜¾è‘—çº¿ç´¢å’Œå…¨å±€æ—¶é—´ç»“æ„ï¼Œè¯­ä¹‰äº¤å‰æ³¨æ„åŠ›ï¼ˆSCAï¼‰å®ç°ç»†ç²’åº¦å¤šæ¨¡æ€è¯­ä¹‰äº¤äº’ï¼›åŒæ—¶å¼•å…¥å¯¹æŠ—æ€§æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆå®¢è§‚æè¿°ã€ä»‡æ¨å‡è®¾æ¨ç†å’Œéä»‡æ¨å‡è®¾æ¨ç†ä¸‰ä¸ªé˜¶æ®µçš„äº’è¡¥è¯­ä¹‰è§†è§’ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œä»‡æ¨è§†é¢‘æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†é²æ£’çš„æ³›åŒ–æ€§èƒ½ï¼Œåœ¨Macro-F1å’Œä»‡æ¨ç±»å¬å›ç‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›æ–¹æ³•æå‡äº†3%å’Œ7%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ç»“åˆç»“æ„åŒ–çš„å¯¹æŠ—æ€§æ¨ç†ä¸ç»†ç²’åº¦å¤šæ¨¡æ€èåˆï¼Œä¸ºä»‡æ¨è§†é¢‘æ£€æµ‹æä¾›äº†æ›´å…¨é¢çš„è¯­ä¹‰ç†è§£æ¡†æ¶ï¼Œè¡¨æ˜åŒæ—¶è€ƒè™‘å±€éƒ¨-å…¨å±€ä¸Šä¸‹æ–‡å’Œå¤šè§†è§’æ¨ç†èƒ½æœ‰æ•ˆæå‡å¯¹å¾®å¦™ä»‡æ¨å†…å®¹çš„è¯†åˆ«èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Hate speech in online videos is posing an increasingly serious threat to digital platforms, especially as video content becomes increasingly multimodal and context-dependent. Existing methods often struggle to effectively fuse the complex semantic relationships between modalities and lack the ability to understand nuanced hateful content. To address these issues, we propose an innovative Reasoning-Aware Multimodal Fusion (RAMF) framework. To tackle the first challenge, we design Local-Global Context Fusion (LGCF) to capture both local salient cues and global temporal structures, and propose Semantic Cross Attention (SCA) to enable fine-grained multimodal semantic interaction. To tackle the second challenge, we introduce adversarial reasoning-a structured three-stage process where a vision-language model generates (i) objective descriptions, (ii) hate-assumed inferences, and (iii) non-hate-assumed inferences-providing complementary semantic perspectives that enrich the model's contextual understanding of nuanced hateful intent. Evaluations on two real-world hateful video datasets demonstrate that our method achieves robust generalisation performance, improving upon state-of-the-art methods by 3% and 7% in Macro-F1 and hate class recall, respectively. We will release the code after the anonymity period ends.</p>
<h3 id="24-mrd-multi-resolution-retrieval-detection-fusion-for-high-resolution-image-understanding">[24] <a href="https://arxiv.org/abs/2512.02906">MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding</a></h3>
<p><em>Fan Yang, Kaihao Zhang</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†å¤šåˆ†è¾¨ç‡æ£€ç´¢-æ£€æµ‹ï¼ˆMRDï¼‰æ¡†æ¶ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£æ–¹æ³•ï¼Œé€šè¿‡å¤šåˆ†è¾¨ç‡è¯­ä¹‰èåˆå’Œå¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹æ¥è§£å†³ç°æœ‰è£å‰ªå¤„ç†æ–¹æ³•ä¸­ç›®æ ‡å¯¹è±¡è¢«åˆ†å‰²å¯¼è‡´çš„è¯­ä¹‰ç›¸ä¼¼æ€§åå·®é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºè£å‰ªçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£æ–¹æ³•å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œå½“ç›®æ ‡å¯¹è±¡è¢«åˆ†å‰²åˆ°å¤šä¸ªå›¾åƒå—æ—¶ï¼Œä¼šç ´åè¯­ä¹‰ç›¸ä¼¼æ€§è®¡ç®—ï¼Œå¯¼è‡´è¯­ä¹‰åå·®ã€‚è¿™ç§è£å‰ªå¤„ç†æ–¹å¼ä¼šç ´åå®Œæ•´å¯¹è±¡çš„å®Œæ•´æ€§ï¼Œå½±å“ç›®æ ‡å®šä½çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºå¤šåˆ†è¾¨ç‡æ£€ç´¢-æ£€æµ‹ï¼ˆMRDï¼‰æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¤šåˆ†è¾¨ç‡è¯­ä¹‰èåˆæ–¹æ³•ï¼Œé€šè¿‡æ•´åˆä¸åŒåˆ†è¾¨ç‡ä¸‹è·å¾—çš„è¯­ä¹‰ç›¸ä¼¼æ€§å›¾æ¥ç”Ÿæˆæ›´å‡†ç¡®çš„è¯­ä¹‰ä¿¡æ¯å¹¶ä¿æŒç›®æ ‡å¯¹è±¡å®Œæ•´æ€§ï¼›å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œé‡‡ç”¨æ»‘åŠ¨çª—å£æ–¹æ³•åœ¨å…¨å±€å°ºåº¦ä¸Šç›´æ¥å®šä½ç›®æ ‡å¯¹è±¡åŒºåŸŸã€‚</p>
<p><strong>Result:</strong> åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨ä¸åŒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚MRDæ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æå‡ç›®æ ‡å®šä½çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¢«åˆ†å‰²åˆ°å¤šä¸ªå›¾åƒå—çš„å¯¹è±¡æ—¶è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜å¤šåˆ†è¾¨ç‡å¤„ç†å¯¹äºé«˜åˆ†è¾¨ç‡å›¾åƒç†è§£è‡³å…³é‡è¦ï¼Œæå‡ºçš„è®­ç»ƒå…è´¹æ¡†æ¶ä¸ºå¤„ç†ä¸åŒå°ºå¯¸å¯¹è±¡æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚å¤šåˆ†è¾¨ç‡è¯­ä¹‰èåˆæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£è¯­ä¹‰ç›¸ä¼¼æ€§åå·®ï¼Œè€Œå¼€æ”¾è¯æ±‡æ£€æµ‹ç»„ä»¶åˆ™å®ç°äº†æ›´ç²¾ç¡®çš„å…¨å±€ç›®æ ‡å®šä½ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.</p>
<h3 id="25-yingvideo-mv-music-driven-multi-stage-video-generation">[25] <a href="https://arxiv.org/abs/2512.02492">YingVideo-MV: Music-Driven Multi-Stage Video Generation</a></h3>
<p><em>Jiahui Chen, Weida Wang, Runhua Shi, Huan Yang, Chaofan Ding, Zihao Chen</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†YingVideo-MVï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºéŸ³ä¹é©±åŠ¨é•¿è§†é¢‘ç”Ÿæˆçš„çº§è”æ¡†æ¶ï¼Œé€šè¿‡é›†æˆéŸ³é¢‘è¯­ä¹‰åˆ†æã€å¯è§£é‡Šé•œå¤´è§„åˆ’æ¨¡å—ã€æ—¶åºæ„ŸçŸ¥æ‰©æ•£Transformeræ¶æ„å’Œé•¿åºåˆ—ä¸€è‡´æ€§å»ºæ¨¡ï¼Œå®ç°äº†ä»éŸ³é¢‘ä¿¡å·è‡ªåŠ¨åˆæˆé«˜è´¨é‡éŸ³ä¹è¡¨æ¼”è§†é¢‘ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘é©±åŠ¨åŒ–èº«è§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†éŸ³ä¹è¡¨æ¼”è§†é¢‘çš„ç”Ÿæˆï¼ˆç‰¹åˆ«æ˜¯åŒ…å«æ‘„åƒæœºè¿åŠ¨ï¼‰ä»æœªè¢«å……åˆ†æ¢ç´¢ï¼Œç°æœ‰é•¿è§†é¢‘ç”Ÿæˆæ–¹æ³•ç¼ºä¹æ˜ç¡®çš„æ‘„åƒæœºè¿åŠ¨æ§åˆ¶ï¼Œä¸”å‰ªè¾‘é—´è¿ç»­æ€§ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> æ–¹æ³•åŒ…æ‹¬éŸ³é¢‘è¯­ä¹‰åˆ†æã€å¯è§£é‡Šé•œå¤´è§„åˆ’æ¨¡å—ï¼ˆMV-Directorï¼‰ã€æ—¶åºæ„ŸçŸ¥æ‰©æ•£Transformeræ¶æ„ã€é•¿åºåˆ—ä¸€è‡´æ€§å»ºæ¨¡ï¼Œå¹¶å¼•å…¥äº†æ‘„åƒæœºé€‚é…å™¨æ¨¡å—å°†æ‘„åƒæœºå§¿æ€åµŒå…¥æ½œåœ¨å™ªå£°ï¼Œä»¥åŠåŸºäºéŸ³é¢‘åµŒå…¥è‡ªé€‚åº”è°ƒæ•´å»å™ªèŒƒå›´çš„æ—¶é—´æ„ŸçŸ¥åŠ¨æ€çª—å£èŒƒå›´ç­–ç•¥ã€‚</p>
<p><strong>Result:</strong> ç»¼åˆåŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒYingVideo-MVåœ¨ç”Ÿæˆè¿è´¯ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„éŸ³ä¹è§†é¢‘æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå®ç°äº†ç²¾ç¡®çš„éŸ³ä¹-åŠ¨ä½œ-æ‘„åƒæœºåŒæ­¥ï¼Œå¹¶é€šè¿‡æ„å»ºå¤§è§„æ¨¡Music-in-the-Wildæ•°æ®é›†æ”¯æŒäº†å¤šæ ·åŒ–çš„é«˜è´¨é‡ç»“æœç”Ÿæˆã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºéŸ³ä¹é©±åŠ¨è§†é¢‘ç”Ÿæˆæä¾›äº†é¦–ä¸ªå®Œæ•´çš„çº§è”æ¡†æ¶ï¼Œå®ç°äº†å¯¹æ‘„åƒæœºè¿åŠ¨çš„ç²¾ç¡®æ§åˆ¶å’Œé•¿åºåˆ—çš„è¿è´¯æ€§ç”Ÿæˆï¼Œä¸ºéŸ³ä¹è¡¨æ¼”è§†é¢‘çš„è‡ªåŠ¨åˆæˆå¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå±•ç¤ºäº†éŸ³é¢‘è¯­ä¹‰åˆ†æä¸è§†è§‰ç”Ÿæˆæ·±åº¦èåˆçš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .</p>
<h3 id="26-benchmarking-scientific-understanding-and-reasoning-for-video-generation-using-videoscience-bench">[26] <a href="https://arxiv.org/abs/2512.02942">Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench</a></h3>
<p><em>Lanxiang Hu, Abhilash Shankarampeta, Yixin Huang, Zilin Dai, Haoyang Yu, Yujie Zhao, Haoqiang Kang, Daniel Zhao, Tajana Rosing, Hao Zhang</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VideoScience-Benchï¼Œè¿™æ˜¯é¦–ä¸ªè¯„ä¼°è§†é¢‘æ¨¡å‹ç§‘å­¦æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«200ä¸ªæ¶µç›–ç‰©ç†å’ŒåŒ–å­¦å¤šæ¦‚å¿µåœºæ™¯çš„æç¤ºï¼Œé€šè¿‡ä¸“å®¶æ ‡æ³¨å’ŒVLM-as-a-Judgeæ–¹æ³•è¯„ä¼°äº†ä¸ƒç§æœ€å…ˆè¿›çš„è§†é¢‘æ¨¡å‹ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘ç”Ÿæˆçš„ä¸‹ä¸€ä¸ªå‰æ²¿æ˜¯å¼€å‘å…·å¤‡é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼Œè¿™éœ€è¦ç†è§£ç°å®ä¸–ç•Œçš„ç§‘å­¦å®šå¾‹ä»¥å‡†ç¡®æ¨¡æ‹Ÿä¸åŒæ¡ä»¶ä¸‹çš„ç‰©ç†ç»“æœã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘åŸºå‡†æµ‹è¯•ä¸»è¦åŸºäºç‰©ç†å¸¸è¯†ï¼Œå¯¹è§†é¢‘æ¨¡å‹çš„ç§‘å­¦æ¨ç†èƒ½åŠ›è¯„ä¼°æœ‰é™ï¼Œå› æ­¤éœ€è¦ä¸“é—¨è¯„ä¼°æœ¬ç§‘æ°´å¹³ç§‘å­¦ç†è§£èƒ½åŠ›çš„åŸºå‡†ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†VideoScience-BenchåŸºå‡†ï¼ŒåŒ…å«200ä¸ªç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œæ¶µç›–14ä¸ªä¸»é¢˜å’Œ103ä¸ªç‰©ç†ä¸åŒ–å­¦æ¦‚å¿µï¼Œæ¯ä¸ªæç¤ºç¼–ç äº†éœ€è¦è·¨å¤šä¸ªç§‘å­¦æ¦‚å¿µç†è§£å’Œæ¨ç†çš„å¤åˆç§‘å­¦åœºæ™¯ã€‚è¯„ä¼°é‡‡ç”¨ä¸“å®¶æ ‡æ³¨æ–¹æ³•ï¼Œåœ¨æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘ä¸¤ç§è®¾ç½®ä¸‹å¯¹ä¸ƒç§æœ€å…ˆè¿›çš„è§†é¢‘æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œè¯„ä¼°ç»´åº¦åŒ…æ‹¬æç¤ºä¸€è‡´æ€§ã€ç°è±¡ä¸€è‡´æ€§ã€æ­£ç¡®åŠ¨æ€æ€§ã€ä¸å˜æ€§å’Œæ—¶ç©ºè¿ç»­æ€§ï¼Œå¹¶é‡‡ç”¨VLM-as-a-Judgeæ–¹æ³•è¯„ä¼°è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨VLM-as-a-Judgeè¯„ä¼°è§†é¢‘ç”Ÿæˆä¸äººç±»è¯„ä¼°ç»“æœå…·æœ‰å¼ºç›¸å…³æ€§ã€‚è¯¥åŸºå‡†é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°äº†è§†é¢‘æ¨¡å‹ä½œä¸ºç”Ÿæˆå™¨å’Œæ¨ç†å™¨çš„åŒé‡èƒ½åŠ›ï¼Œè¦æ±‚ç”Ÿæˆçš„è§†é¢‘åœ¨ç§‘å­¦ç†è§£ä¸Šä¸é¢„æœŸçš„ç‰©ç†å’ŒåŒ–å­¦ç°è±¡ä¿æŒä¸€è‡´ï¼Œä¸ºè§†é¢‘æ¨¡å‹çš„ç§‘å­¦æ¨ç†èƒ½åŠ›æä¾›äº†é‡åŒ–è¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Conclusion:</strong> VideoScience-Benchæ˜¯é¦–ä¸ªä¸“é—¨è¯„ä¼°è§†é¢‘æ¨¡å‹ç§‘å­¦æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨ç§‘å­¦ç†è§£è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚è¯¥ç ”ç©¶ä¸ºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç§‘å­¦æ¨ç†èƒ½åŠ›æä¾›äº†ç³»ç»Ÿè¯„ä¼°æ–¹æ³•ï¼Œæ¨åŠ¨äº†è§†é¢‘æ¨¡å‹ä»å•çº¯ç”Ÿæˆå‘å…·å¤‡ç§‘å­¦ç†è§£èƒ½åŠ›çš„æ–¹å‘å‘å±•ï¼Œç›¸å…³æ•°æ®å’Œè¯„ä¼°ä»£ç å·²å¼€æºä¾›ç ”ç©¶ç¤¾åŒºä½¿ç”¨ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}.</p>
<h3 id="27-a-large-scale-benchmark-for-test-time-adaptation-methods-in-medical-image-segmentation">[27] <a href="https://arxiv.org/abs/2512.02497">A Large Scale Benchmark for Test Time Adaptation Methods in Medical Image Segmentation</a></h3>
<p><em>Wenjing Yu, Shuo Jiang, Yifei Chen, Shuo Chang, Yuanhan Wang, Beining Wu, Jie Dong, Mingxuan Liu, Shenghao Zhu, Feiwei Qin, Changmiao Wang, Qiyuan Tian</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†MedSeg-TTAï¼Œä¸€ä¸ªå…¨é¢çš„åŒ»å­¦å›¾åƒåˆ†å‰²æµ‹è¯•æ—¶é€‚åº”åŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°äº†20ç§ä»£è¡¨æ€§é€‚åº”æ–¹æ³•åœ¨7ç§æˆåƒæ¨¡æ€ä¸‹çš„æ€§èƒ½ï¼Œæ­ç¤ºäº†ä¸åŒé€‚åº”èŒƒå¼çš„é€‚ç”¨æ¡ä»¶ä¸å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŒ»å­¦å›¾åƒåˆ†å‰²çš„æµ‹è¯•æ—¶é€‚åº”ç ”ç©¶å­˜åœ¨æ¨¡æ€è¦†ç›–ä¸è¶³ã€ä»»åŠ¡å¤šæ ·æ€§æœ‰é™å’Œæ–¹æ³•è¯„ä¼°ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œç¼ºä¹ç³»ç»Ÿæ€§çš„è·¨æ¨¡æ€æ¯”è¾ƒï¼Œé˜»ç¢äº†ä¸´åºŠéƒ¨ç½²ä¸­æ–¹æ³•é€‰æ‹©çš„ç§‘å­¦ä¾æ®ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†MedSeg-TTAåŸºå‡†ï¼Œç»Ÿä¸€äº†æ•°æ®é¢„å¤„ç†ã€éª¨å¹²ç½‘ç»œé…ç½®å’Œæµ‹è¯•æ—¶åè®®ï¼Œç³»ç»Ÿè¯„ä¼°äº†20ç§ä»£è¡¨æ€§é€‚åº”æ–¹æ³•ï¼Œæ¶µç›–è¾“å…¥çº§å˜æ¢ã€ç‰¹å¾çº§å¯¹é½ã€è¾“å‡ºçº§æ­£åˆ™åŒ–å’Œå…ˆéªŒä¼°è®¡å››å¤§é€‚åº”èŒƒå¼ï¼Œè¦†ç›–MRIã€CTã€è¶…å£°ã€ç—…ç†ã€çš®è‚¤é•œã€OCTå’Œèƒ¸éƒ¨Xå…‰ä¸ƒç§æˆåƒæ¨¡æ€ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜æ²¡æœ‰å•ä¸€èŒƒå¼åœ¨æ‰€æœ‰æ¡ä»¶ä¸‹è¡¨ç°æœ€ä½³ï¼šè¾“å…¥çº§æ–¹æ³•åœ¨è½»åº¦å¤–è§‚åç§»ä¸‹æ›´ç¨³å®šï¼›ç‰¹å¾çº§å’Œè¾“å‡ºçº§æ–¹æ³•åœ¨è¾¹ç•Œç›¸å…³æŒ‡æ ‡ä¸Šä¼˜åŠ¿æ˜æ˜¾ï¼›å…ˆéªŒæ–¹æ³•è¡¨ç°å‡ºå¼ºçƒˆçš„æ¨¡æ€ä¾èµ–æ€§ï¼›å¤šç§æ–¹æ³•åœ¨å¤§è§„æ¨¡è·¨ä¸­å¿ƒè·¨è®¾å¤‡åç§»ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†æ ¹æ®ä¸´åºŠåœºæ™¯é€‰æ‹©é€‚å½“é€‚åº”èŒƒå¼çš„é‡è¦æ€§ï¼Œæ­ç¤ºäº†ä¸åŒèŒƒå¼çš„é€‚ç”¨è¾¹ç•Œï¼Œæä¾›äº†æ ‡å‡†åŒ–æ•°æ®é›†ã€éªŒè¯å®ç°å’Œå…¬å¼€æ’è¡Œæ¦œï¼Œä¸ºå¼€å‘é²æ£’ä¸”ä¸´åºŠå¯é çš„æµ‹è¯•æ—¶é€‚åº”æ–¹æ³•å¥ å®šäº†ä¸¥æ ¼åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Test time Adaptation is a promising approach for mitigating domain shift in medical image segmentation; however, current evaluations remain limited in terms of modality coverage, task diversity, and methodological consistency. We present MedSeg-TTA, a comprehensive benchmark that examines twenty representative adaptation methods across seven imaging modalities, including MRI, CT, ultrasound, pathology, dermoscopy, OCT, and chest X-ray, under fully unified data preprocessing, backbone configuration, and test time protocols. The benchmark encompasses four significant adaptation paradigms: Input-level Transformation, Feature-level Alignment, Output-level Regularization, and Prior Estimation, enabling the first systematic cross-modality comparison of their reliability and applicability. The results show that no single paradigm performs best in all conditions. Input-level methods are more stable under mild appearance shifts. Feature-level and Output-level methods offer greater advantages in boundary-related metrics, whereas prior-based methods exhibit strong modality dependence. Several methods degrade significantly under large inter-center and inter-device shifts, which highlights the importance of principled method selection for clinical deployment. MedSeg-TTA provides standardized datasets, validated implementations, and a public leaderboard, establishing a rigorous foundation for future research on robust, clinically reliable test-time adaptation. All source codes and open-source datasets are available at https://github.com/wenjing-gg/MedSeg-TTA.</p>
<h3 id="28-dotsocr-multilingual-document-layout-parsing-in-a-single-vision-language-model">[28] <a href="https://arxiv.org/abs/2512.02498">dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model</a></h3>
<p><em>Yumeng Li, Guang Yang, Hao Liu, Bowen Wang, Colin Zhang</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†dots.ocrï¼Œé¦–ä¸ªåœ¨ç»Ÿä¸€ç«¯åˆ°ç«¯æ¡†æ¶ä¸­è”åˆå­¦ä¹ æ–‡æ¡£å¸ƒå±€è§£æä¸‰å¤§æ ¸å¿ƒä»»åŠ¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¯æ‰©å±•æ•°æ®å¼•æ“åˆæˆå¤šè¯­è¨€è¯­æ–™ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æ¡£å¸ƒå±€è§£ææ–¹æ³•ä¾èµ–ç¢ç‰‡åŒ–çš„å¤šé˜¶æ®µæµæ°´çº¿ï¼Œå­˜åœ¨é”™è¯¯ä¼ æ’­é—®é¢˜ä¸”æ— æ³•åˆ©ç”¨è”åˆè®­ç»ƒçš„ä¼˜åŠ¿ï¼Œè¿™é™åˆ¶äº†AIè®¿é—®å’Œè§£é‡Šç»“æ„åŒ–çŸ¥è¯†çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹äºèµ‹èƒ½ä¸‹ä¸€ä»£è§†è§‰è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºdots.ocrï¼Œè¿™æ˜¯ä¸€ä¸ªå•ä¸€çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé¦–æ¬¡åœ¨ç»Ÿä¸€ç«¯åˆ°ç«¯æ¡†æ¶ä¸­è”åˆå­¦ä¹ å¸ƒå±€æ£€æµ‹ã€æ–‡æœ¬è¯†åˆ«å’Œå…³ç³»ç†è§£ä¸‰å¤§æ ¸å¿ƒä»»åŠ¡ï¼Œé€šè¿‡é«˜åº¦å¯æ‰©å±•çš„æ•°æ®å¼•æ“åˆæˆå¤§è§„æ¨¡å¤šè¯­è¨€è¯­æ–™åº“ï¼Œæ”¯æŒæ¨¡å‹åœ¨å¤šæ ·åŒ–è¯­è¨€ã€å¸ƒå±€å’Œé¢†åŸŸä»»åŠ¡ä¸­è¡¨ç°ç¨³å¥ã€‚</p>
<p><strong>Result:</strong> dots.ocråœ¨ç»¼åˆæ€§åŸºå‡†æµ‹è¯•OmniDocBenchä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ–°å¼•å…¥çš„XDocParseåŸºå‡†æµ‹è¯•ï¼ˆæ¶µç›–126ç§è¯­è¨€ï¼‰ä¸Šå»ºç«‹äº†å¼ºå¤§çš„æ–°åŸºçº¿ï¼Œä»¥+7.4åˆ†çš„æ˜¾è‘—ä¼˜åŠ¿è¶…è¶Šæ¬¡ä¼˜ç«äº‰è€…ï¼Œè¯æ˜äº†å…¶å“è¶Šçš„å¤šè¯­è¨€èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»Ÿä¸€ç«¯åˆ°ç«¯æ¡†æ¶åœ¨æ–‡æ¡£å¸ƒå±€è§£æä¸­çš„ä¼˜åŠ¿ï¼Œé€šè¿‡è”åˆå­¦ä¹ æ ¸å¿ƒä»»åŠ¡é¿å…äº†å¤šé˜¶æ®µæµæ°´çº¿çš„é”™è¯¯ä¼ æ’­é—®é¢˜ï¼ŒåŒæ—¶å¼•å…¥çš„XDocParseåŸºå‡†æµ‹è¯•ä¸ºå…¨çƒæ–‡æ¡£æ™ºèƒ½ç ”ç©¶æä¾›äº†æ–°çš„æŒ‘æˆ˜å¹³å°ï¼Œdots.ocrçš„å“è¶Šå¤šè¯­è¨€æ€§èƒ½ä¸ºè·¨è¯­è¨€æ–‡æ¡£ç†è§£è®¾ç«‹äº†æ–°çš„æ ‡å‡†ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce dots.ocr, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this testbed, dots.ocr establishes a powerful new baseline, outperforming the next-best competitor by a remarkable +7.4 point margin and proving its unparalleled multilingual capabilities.</p>
<h3 id="29-geodit-a-diffusion-based-vision-language-model-for-geospatial-understanding">[29] <a href="https://arxiv.org/abs/2512.02505">GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding</a></h3>
<p><em>Jiaqi Liu, Ronghao Fu, Haoran Liu, Lang Sun, Bo Yang</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGeoDiTï¼Œé¦–ä¸ªé¢å‘åœ°ç†ç©ºé—´é¢†åŸŸçš„æ‰©æ•£å¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¹¶è¡Œç»†åŒ–çš„ç”Ÿæˆè¿‡ç¨‹è§£å†³äº†è‡ªå›å½’æ¨¡å‹åœ¨åœ°ç†ç©ºé—´ç†è§£ä¸­çš„ç»“æ„é”™é…é—®é¢˜ï¼Œåœ¨ç»“æ„åŒ–è¾“å‡ºä»»åŠ¡ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªå›å½’æ¨¡å‹çš„ç»“æ„ä¸åœ°ç†ç©ºé—´ç†è§£å›ºæœ‰çš„å¹¶è¡Œæ€§è´¨å­˜åœ¨æ ¹æœ¬æ€§é”™é…ï¼Œå¼ºåˆ¶å°†åˆšæ€§é¡ºåºå™äº‹æ–½åŠ äºåœºæ™¯ä¹‹ä¸Šï¼Œä¸¥é‡é˜»ç¢äº†ç»“æ„åŒ–ã€è¿è´¯è¾“å‡ºçš„ç”Ÿæˆèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦åŒæ—¶è§£æå¤šä¸ªè¯­ä¹‰å…ƒç´ çš„åœ°ç†ç©ºé—´åˆ†æä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å°†åœ°ç†ç©ºé—´ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºå¹¶è¡Œç»†åŒ–è¿‡ç¨‹ï¼Œå®ç°äº†ä»ç²—åˆ°ç»†çš„æ•´ä½“åˆæˆï¼ŒåŒæ—¶è§£ææ‰€æœ‰è¯­ä¹‰å…ƒç´ ï¼Œå¹¶ä¸ºæ­¤å¼•å…¥äº†GeoDiTâ€”â€”é¦–ä¸ªä¸“é—¨ä¸ºåœ°ç†ç©ºé—´é¢†åŸŸè®¾è®¡çš„åŸºäºæ‰©æ•£çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGeoDiTåœ¨éœ€è¦ç»“æ„åŒ–ã€ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒè¾“å‡ºçš„åŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œåœ¨å›¾åƒæè¿°ã€è§†è§‰å®šä½å’Œå¤šå¯¹è±¡æ£€æµ‹ç­‰è‡ªå›å½’æ¨¡å‹è¡¨ç°ä¸ä½³çš„ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶éªŒè¯äº†å°†ç”Ÿæˆè¿‡ç¨‹ä¸æ•°æ®å†…åœ¨ç»“æ„å¯¹é½æ˜¯è§£é”å¤æ‚åœ°ç†ç©ºé—´åˆ†æä¸­å“è¶Šæ€§èƒ½çš„å…³é”®ï¼Œä¸ºåœ°ç†ç©ºé—´äººå·¥æ™ºèƒ½é¢†åŸŸæä¾›äº†æ–°çš„ç”ŸæˆèŒƒå¼ï¼Œå¼ºè°ƒäº†æ¨¡å‹æ¶æ„ä¸é¢†åŸŸç‰¹æ€§åŒ¹é…çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data's intrinsic structure is key to unlocking superior performance in complex geospatial analysis.</p>
<h3 id="30-skymoe-a-vision-language-foundation-model-for-enhancing-geospatial-interpretation-with-mixture-of-experts">[30] <a href="https://arxiv.org/abs/2512.02517">SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts</a></h3>
<p><em>Jiaqi Liu, Ronghao Fu, Lang Sun, Haoran Liu, Xiao Yang, Weipeng Zhang, Xu Na, Zhuoran Duan, Bo Yang</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SkyMoEï¼Œä¸€ç§ä¸“ä¸ºé¥æ„Ÿä»»åŠ¡è®¾è®¡çš„æ··åˆä¸“å®¶è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä»»åŠ¡å’Œç²’åº¦æ„ŸçŸ¥çš„è·¯ç”±æœºåˆ¶ï¼Œå®ç°äº†å¤šæ¨¡æ€ã€å¤šä»»åŠ¡é¥æ„Ÿè§£é‡Šçš„ä¼˜è¶Šæ€§èƒ½ï¼Œåœ¨21ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¥æ„Ÿä»»åŠ¡ä¸Šè¡¨ç°æ¬ ä½³ï¼Œç°æœ‰åœ°ç†ç©ºé—´VLMé‡‡ç”¨ç»Ÿä¸€å»ºæ¨¡ç­–ç•¥ï¼Œéš¾ä»¥åŒºåˆ†ä»»åŠ¡ç±»å‹å’Œè§£é‡Šç²’åº¦ï¼Œé™åˆ¶äº†å…¶åœ¨å±€éƒ¨ç»†èŠ‚æ„ŸçŸ¥å’Œå…¨å±€ä¸Šä¸‹æ–‡ç†è§£ä¹‹é—´çš„å¹³è¡¡èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> SkyMoEé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„ï¼Œè®¾è®¡äº†è‡ªé€‚åº”è·¯ç”±å™¨ç”Ÿæˆä»»åŠ¡å’Œç²’åº¦æ„ŸçŸ¥çš„è·¯ç”±æŒ‡ä»¤ï¼Œä½¿ä¸“ä¸šå¤§è¯­è¨€æ¨¡å‹ä¸“å®¶å¤„ç†ä¸åŒå­ä»»åŠ¡ï¼›å¼•å…¥ä¸Šä¸‹æ–‡è§£è€¦å¢å¼ºç­–ç•¥ï¼Œåˆ›å»ºå±€éƒ¨ä¸å…¨å±€ç‰¹å¾çš„å¯¹æ¯”å¯¹ï¼Œå¼•å¯¼ä¸“å®¶è¿›è¡Œå±‚çº§ç‰¹å®šçš„è¡¨ç¤ºå­¦ä¹ ï¼›æ„å»ºäº†MGRS-BenchåŸºå‡†ï¼Œè¦†ç›–å¤šä¸ªé¥æ„Ÿè§£é‡Šä»»åŠ¡å’Œç²’åº¦çº§åˆ«ã€‚</p>
<p><strong>Result:</strong> åœ¨21ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSkyMoEåœ¨å„é¡¹ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸‹çš„é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œä¼˜è¶Šçš„å¤šç²’åº¦ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ··åˆä¸“å®¶æ¶æ„åœ¨é¥æ„Ÿè§†è§‰è¯­è¨€å»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ä»»åŠ¡å’Œç²’åº¦æ„ŸçŸ¥çš„è·¯ç”±æœºåˆ¶å®ç°äº†ä¸“å®¶è§£è€¦å’Œä¸“ä¸šåŒ–ï¼Œä¸ºå¤šæ¨¡æ€é¥æ„Ÿè§£é‡Šæä¾›äº†å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†åœ°ç†ç©ºé—´AIå‘æ›´ç²¾ç»†ã€æ›´é€‚åº”æ€§çš„æ–¹å‘å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.</p>
<h3 id="31-on-the-problem-of-consistent-anomalies-in-zero-shot-anomaly-detection">[31] <a href="https://arxiv.org/abs/2512.02520">On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection</a></h3>
<p><em>Tai Le-Gia</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬è®ºæ–‡é’ˆå¯¹é›¶æ ·æœ¬å¼‚å¸¸åˆ†ç±»ä¸åˆ†å‰²ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºç†è®ºåˆ†æå’Œç®—æ³•è®¾è®¡çš„ç³»ç»Ÿè§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬è¯†åˆ«ä¸€è‡´å¼‚å¸¸é—®é¢˜ã€å¼€å‘CoDeGraphå›¾æ¡†æ¶ã€æ‰©å±•è‡³3DåŒ»å­¦å½±åƒä»¥åŠæ¡¥æ¥æ‰¹é‡ä¸æ–‡æœ¬æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é›¶æ ·æœ¬å¼‚å¸¸åˆ†ç±»ä¸åˆ†å‰²åœ¨å·¥ä¸šæ£€æµ‹å’ŒåŒ»å­¦æˆåƒä¸­æ—¥ç›Šé‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´ä¸€è‡´å¼‚å¸¸è¿™ä¸€å¤±æ•ˆæ¨¡å¼ï¼Œå³é‡å¤å‡ºç°çš„ç›¸ä¼¼å¼‚å¸¸ä¼šç³»ç»Ÿæ€§åå·®åŸºäºè·ç¦»çš„æ–¹æ³•ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ·±å…¥æ¢ç©¶é›¶æ ·æœ¬AC/ASçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¹¶æä¾›åŸºäºç†è®ºå’Œç®—æ³•è®¾è®¡çš„åŸç†æ€§è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> é¦–å…ˆé€šè¿‡åˆ†æé¢„è®­ç»ƒVision Transformersä¸­è¡¥ä¸è¡¨ç¤ºçš„ç»Ÿè®¡å’Œå‡ ä½•è¡Œä¸ºï¼Œè¯†åˆ«äº†ç›¸ä¼¼æ€§ç¼©æ”¾å’Œé‚»å±…çƒ§æ¯ä¸¤ä¸ªå…³é”®ç°è±¡ã€‚éšåæå‡ºäº†CoDeGraphå›¾æ¡†æ¶ï¼Œé€šè¿‡å¤šé˜¶æ®µå›¾æ„å»ºã€ç¤¾åŒºæ£€æµ‹å’Œç»“æ„åŒ–ç²¾ç‚¼æ¥è¿‡æ»¤ä¸€è‡´å¼‚å¸¸ã€‚è¿›ä¸€æ­¥æ‰©å±•åˆ°3DåŒ»å­¦æˆåƒï¼Œæå‡ºäº†æ— éœ€è®­ç»ƒçš„è®¡ç®—é«˜æ•ˆä½“ç§¯æ ‡è®°åŒ–ç­–ç•¥ã€‚æœ€åå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨CoDeGraphç”Ÿæˆçš„ä¼ªæ©ç ç›‘ç£æç¤ºé©±åŠ¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶æ­ç¤ºäº†åœ¨é«˜åº¦ç›¸ä¼¼ç‰©ä½“åœºæ™¯ä¸­ï¼Œæ­£å¸¸è¡¥ä¸å…³ç³»åœ¨æœ‰/æ— ä¸€è‡´å¼‚å¸¸æ—¶çš„å˜åŒ–è§„å¾‹ã€‚CoDeGraphæ¡†æ¶èƒ½æœ‰æ•ˆæŠ‘åˆ¶ä¸€è‡´å¼‚å¸¸çš„å½±å“ï¼Œè€Œæå‡ºçš„3Dä½“ç§¯æ ‡è®°åŒ–ç­–ç•¥å®ç°äº†çœŸæ­£çš„é›¶æ ·æœ¬3Då¼‚å¸¸æ£€æµ‹æµç¨‹ï¼Œè¯æ˜æ— éœ€ä»»ä½•3Dè®­ç»ƒæ ·æœ¬å³å¯å®ç°ä½“ç§¯å¼‚å¸¸åˆ†å‰²ã€‚å®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½æ¡¥æ¥æ‰¹é‡ä¸æ–‡æœ¬æ–¹æ³•çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶ä¸ºé›¶æ ·æœ¬å¼‚å¸¸åˆ†ç±»ä¸åˆ†å‰²é—®é¢˜æä¾›äº†ç†è®ºç†è§£å’Œå®ç”¨è§£å†³æ–¹æ¡ˆï¼Œç³»ç»Ÿè§£å†³äº†ä»é—®é¢˜å½¢å¼åŒ–åˆ°ç®—æ³•è®¾è®¡å†åˆ°å®é™…åº”ç”¨çš„å®Œæ•´é“¾æ¡ã€‚æå‡ºçš„æ¡†æ¶ä¸ä»…é€‚ç”¨äº2Då·¥ä¸šæ£€æµ‹ï¼Œè¿˜èƒ½æ‰©å±•åˆ°3DåŒ»å­¦æˆåƒï¼Œå¹¶é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹é›†æˆå±•ç¤ºäº†æ–¹æ³•çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä¸ºé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹é¢†åŸŸå¥ å®šäº†é‡è¦åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Zero-shot anomaly classification and segmentation (AC/AS) aim to detect anomalous samples and regions without any training data, a capability increasingly crucial in industrial inspection and medical imaging. This dissertation aims to investigate the core challenges of zero-shot AC/AS and presents principled solutions rooted in theory and algorithmic design.
  We first formalize the problem of consistent anomalies, a failure mode in which recurring similar anomalies systematically bias distance-based methods. By analyzing the statistical and geometric behavior of patch representations from pre-trained Vision Transformers, we identify two key phenomena - similarity scaling and neighbor-burnout - that describe how relationships among normal patches change with and without consistent anomalies in settings characterized by highly similar objects.
  We then introduce CoDeGraph, a graph-based framework for filtering consistent anomalies built on the similarity scaling and neighbor-burnout phenomena. Through multi-stage graph construction, community detection, and structured refinement, CoDeGraph effectively suppresses the influence of consistent anomalies.
  Next, we extend this framework to 3D medical imaging by proposing a training-free, computationally efficient volumetric tokenization strategy for MRI data. This enables a genuinely zero-shot 3D anomaly detection pipeline and shows that volumetric anomaly segmentation is achievable without any 3D training samples.
  Finally, we bridge batch-based and text-based zero-shot methods by demonstrating that CoDeGraph-derived pseudo-masks can supervise prompt-driven vision-language models. Together, this dissertation provides theoretical understanding and practical solutions for the zero-shot AC/AS problem.</p>
<h3 id="32-wemmu-enhanced-bridging-of-vision-language-models-and-diffusion-models-via-noisy-query-tokens">[32] <a href="https://arxiv.org/abs/2512.02536">WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens</a></h3>
<p><em>Jian Yang, Dacheng Yin, Xiaoxuan He, Yong Li, Fengyun Rao, Jing Lyu, Wei Zhai, Yang Cao, Zheng-Jun Zha</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºNoisy Query Tokensæ–¹æ³•ï¼Œé€šè¿‡ç«¯åˆ°ç«¯ä¼˜åŒ–å­¦ä¹ è§†è§‰è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹ä¹‹é—´çš„åˆ†å¸ƒå¼è¡¨ç¤ºç©ºé—´ï¼Œä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­å›ºå®šæŸ¥è¯¢ä»¤ç‰Œå¯¼è‡´çš„æ³›åŒ–å´©æºƒé—®é¢˜ï¼Œå¹¶å¼•å…¥VAEåˆ†æ”¯æ¢å¤ç»†ç²’åº¦å›¾åƒç»†èŠ‚ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œä½¿ç”¨å›ºå®šæ•°é‡å¯å­¦ä¹ æŸ¥è¯¢ä»¤ç‰Œçš„æ–¹æ³•è™½ç„¶è®¡ç®—é«˜æ•ˆï¼Œä½†å­˜åœ¨ä»»åŠ¡æ³›åŒ–å´©æºƒé—®é¢˜ï¼Œæ— æ³•é€‚åº”ä¸é¢„è®­ç»ƒä»»åŠ¡å·®å¼‚è¾ƒå¤§çš„æ–°ä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºNoisy Query Tokensæ–¹æ³•ï¼Œé€šè¿‡ç«¯åˆ°ç«¯ä¼˜åŒ–å­¦ä¹ è§†è§‰è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹ä¹‹é—´çš„åˆ†å¸ƒå¼è¡¨ç¤ºç©ºé—´ï¼Œå¢å¼ºæŒç»­å­¦ä¹ èƒ½åŠ›ï¼›åŒæ—¶å¼•å…¥å¸¦æœ‰çº¿æ€§æŠ•å½±çš„VAEåˆ†æ”¯ï¼Œä»¥æ¢å¤ç»†ç²’åº¦çš„å›¾åƒç»†èŠ‚ä¿¡æ¯ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆç¼“è§£äº†æ³›åŒ–å´©æºƒé—®é¢˜ï¼Œèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„ä»»åŠ¡ä¸Šå®ç°ç¨³å®šçš„æŒç»­å­¦ä¹ ï¼ŒéªŒè¯äº†åˆ†å¸ƒå¼è¡¨ç¤ºç©ºé—´å’ŒVAEåˆ†æ”¯å¯¹æå‡æ¨¡å‹é€‚åº”æ€§å’Œç»†èŠ‚æ¢å¤çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ¡¥æ¥æä¾›äº†æ–°æ€è·¯ï¼Œé€šè¿‡åˆ†å¸ƒå¼è¡¨ç¤ºå­¦ä¹ å’Œç»†èŠ‚æ¢å¤æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ï¼Œä¸ºè·¨æ¨¡æ€ç”Ÿæˆä»»åŠ¡çš„è¿›ä¸€æ­¥å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Recent progress in multimodal large language models (MLLMs) has highlighted the challenge of efficiently bridging pre-trained Vision-Language Models (VLMs) with Diffusion Models. While methods using a fixed number of learnable query tokens offer computational efficiency, they suffer from task generalization collapse, failing to adapt to new tasks that are distant from their pre-training tasks. To overcome this, we propose Noisy Query Tokens, which learn a distributed representation space between the VLM and Diffusion Model via end-to-end optimization, enhancing continual learning. Additionally, we introduce a VAE branch with linear projection to recover fine-grained image details. Experimental results confirm our approach mitigates generalization collapse and enables stable continual learning across diverse tasks.</p>
<h3 id="33-omniperson-unified-identity-preserving-pedestrian-generation">[33] <a href="https://arxiv.org/abs/2512.02554">OmniPerson: Unified Identity-Preserving Pedestrian Generation</a></h3>
<p><em>Changxiao Ma, Chao Yuan, Xincheng Shi, Yuzhuo Ma, Yongfei Zhang, Longkun Zhou, Yujia Zhang, Shangze Li, Yifan Xu</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†OmniPersonï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºå¯è§å…‰/çº¢å¤–å›¾åƒ/è§†é¢‘è¡Œäººé‡è¯†åˆ«ä»»åŠ¡çš„ç»Ÿä¸€èº«ä»½ä¿æŒè¡Œäººç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡å¤šå‚è€ƒèåˆå™¨å’ŒPersonSynæ•°æ®é›†å®ç°äº†é«˜ä¿çœŸã€å¯æ§çš„è¡Œäººç”Ÿæˆï¼Œæœ‰æ•ˆæå‡äº†ReIDæ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¡Œäººé‡è¯†åˆ«é¢†åŸŸé¢ä¸´å¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œä¸»è¦å—é™äºæ•°æ®éšç§å’Œæ ‡æ³¨æˆæœ¬ã€‚ç°æœ‰è¡Œäººç”Ÿæˆæ–¹æ³•åœ¨èº«ä»½ä¸€è‡´æ€§å’Œå¯æ§æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨æ•°æ®é›†å¢å¼ºä¸­çš„æœ‰æ•ˆæ€§ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿä¿æŒèº«ä»½ä¸€è‡´å¹¶æä¾›ç»†ç²’åº¦æ§åˆ¶çš„è¡Œäººç”Ÿæˆæ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†OmniPersonç»Ÿä¸€ç”Ÿæˆæ¨¡å‹ï¼Œæ”¯æŒRGB/IRæ¨¡æ€å›¾åƒ/è§†é¢‘ç”Ÿæˆï¼Œå…·å¤‡å¤šå‚è€ƒå›¾åƒè¾“å…¥ã€ä¸¤ç§è¡Œäººå§¿æ€å’Œæ–‡æœ¬æ§åˆ¶èƒ½åŠ›ï¼ŒåŒæ—¶åŒ…å«RGBåˆ°IRè½¬æ¢å’Œå›¾åƒè¶…åˆ†è¾¨ç‡åŠŸèƒ½ã€‚è®¾è®¡äº†å¤šå‚è€ƒèåˆå™¨ç”¨äºä»å¤šè§†è§’å‚è€ƒå›¾åƒä¸­æå–ç»Ÿä¸€èº«ä»½è¡¨ç¤ºï¼Œç¡®ä¿èº«ä»½ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæ„å»ºäº†PersonSynæ•°æ®é›†åŠå…¶è‡ªåŠ¨åŒ–æ ‡æ³¨æµç¨‹ï¼Œå°†å…¬å¼€çš„ä»…å«IDæ ‡ç­¾çš„ReIDåŸºå‡†è½¬æ¢ä¸ºå…·æœ‰å¯†é›†å¤šæ¨¡æ€ç›‘ç£çš„ä¸°å¯Œæ ‡æ³¨èµ„æºã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniPersonåœ¨è¡Œäººç”Ÿæˆä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è§†è§‰ä¿çœŸåº¦å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚ä½¿ç”¨ç”Ÿæˆæ•°æ®å¢å¼ºç°æœ‰æ•°æ®é›†èƒ½å¤ŸæŒç»­æå‡ReIDæ¨¡å‹çš„æ€§èƒ½ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨æ•°æ®å¢å¼ºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè¡Œäººé‡è¯†åˆ«é¢†åŸŸæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„èº«ä»½ä¿æŒæ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç»Ÿä¸€çš„è¡Œäººç”Ÿæˆæ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨èº«ä»½ä¸€è‡´æ€§å’Œå¯æ§æ€§æ–¹é¢çš„å±€é™æ€§ã€‚OmniPersonçš„å¼€æºå°†ä¿ƒè¿›ç›¸å…³ç ”ç©¶çš„å‘å±•ï¼ŒPersonSynæ•°æ®é›†ä¸ºå¯æ§è¡Œäººç”Ÿæˆä»»åŠ¡æä¾›äº†å®è´µçš„èµ„æºï¼Œä¸ºæœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Person re-identification (ReID) suffers from a lack of large-scale high-quality training data due to challenges in data privacy and annotation costs. While previous approaches have explored pedestrian generation for data augmentation, they often fail to ensure identity consistency and suffer from insufficient controllability, thereby limiting their effectiveness in dataset augmentation. To address this, We introduce OmniPerson, the first unified identity-preserving pedestrian generation pipeline for visible/infrared image/video ReID tasks. Our contributions are threefold: 1) We proposed OmniPerson, a unified generation model, offering holistic and fine-grained control over all key pedestrian attributes. Supporting RGB/IR modality image/video generation with any number of reference images, two kinds of person poses, and text. Also including RGB-to-IR transfer and image super-resolution abilities.2) We designed Multi-Refer Fuser for robust identity preservation with any number of reference images as input, making OmniPerson could distill a unified identity from a set of multi-view reference images, ensuring our generated pedestrians achieve high-fidelity pedestrian generation.3) We introduce PersonSyn, the first large-scale dataset for multi-reference, controllable pedestrian generation, and present its automated curation pipeline which transforms public, ID-only ReID benchmarks into a richly annotated resource with the dense, multi-modal supervision required for this task. Experimental results demonstrate that OmniPerson achieves SoTA in pedestrian generation, excelling in both visual fidelity and identity consistency. Furthermore, augmenting existing datasets with our generated data consistently improves the performance of ReID models. We will open-source the full codebase, pretrained model, and the PersonSyn dataset.</p>
<h3 id="34-ruler-bench-probing-rule-based-reasoning-abilities-of-next-level-video-generation-models-for-vision-foundation-intelligence">[34] <a href="https://arxiv.org/abs/2512.02622">RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence</a></h3>
<p><em>Xuming He, Zehao Fan, Hengjia Li, Fan Zhuo, Hankun Xu, Senlin Cheng, Di Weng, Haifeng Liu, Can Ye, Boxi Wu</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†RULER-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è§„åˆ™æ¨ç†èƒ½åŠ›ï¼Œå‘ç°å½“å‰æœ€å…ˆè¿›æ¨¡å‹åœ¨è§„åˆ™ä¸€è‡´æ€§æŒ‡æ ‡ä¸Šä»…è¾¾åˆ°48.87%ï¼Œæ­ç¤ºäº†è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—ä¸è¶³ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹è¯„ä¼°åŸºå‡†ä¸»è¦å…³æ³¨è§†è§‰æ„ŸçŸ¥å’Œç†è§£å› ç´ ï¼Œå¦‚è§†è§‰ç¾å­¦ã€æŒ‡ä»¤éµå¾ªå’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œä½†æ¨¡å‹çš„è§„åˆ™æ¨ç†èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å°½ç®¡è¿‘æœŸç ”ç©¶å¯¹è§†é¢‘æ¨¡å‹ä½œä¸ºé›¶æ ·æœ¬å­¦ä¹ å™¨è¿›è¡Œäº†åˆæ­¥æ¢ç´¢ï¼Œä½†ä»ç¼ºä¹å¯¹æ¨ç†èƒ½åŠ›çš„ç»†ç²’åº¦åˆ†è§£å’Œå…¨é¢è¯„ä¼°åè®®ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†RULER-BenchåŸºå‡†æµ‹è¯•ï¼ŒåŸºäºæ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘ä¸¤ç§åŸºæœ¬èŒƒå¼ï¼Œæ¶µç›–å…­ä¸ªè§„åˆ™ç±»åˆ«çš„40ä¸ªä»£è¡¨æ€§ä»»åŠ¡ï¼ŒåŒ…å«622ä¸ªé«˜è´¨é‡æ ‡æ³¨å®ä¾‹ã€‚å¯¹äºæ¯ä¸ªç”Ÿæˆè§†é¢‘ï¼Œæ„å»ºäº†æ¶µç›–å››ä¸ªæŒ‡æ ‡çš„æ£€æŸ¥æ¸…å•ï¼Œå¹¶åˆ©ç”¨GPT-4oè¿›è¡Œè¯„åˆ†ï¼Œå®ç°äº†ä¸äººç±»åˆ¤æ–­85%çš„ä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨è§„åˆ™ä¸€è‡´æ€§æŒ‡æ ‡ä¸Šä»…è¾¾åˆ°48.87%çš„å¾—åˆ†ï¼Œçªæ˜¾äº†ä¸‹ä¸€ä»£è§†é¢‘æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—æ”¹è¿›ç©ºé—´ã€‚è¯„ä¼°åè®®é€šè¿‡GPT-4oå®ç°äº†ä¸äººç±»åˆ¤æ–­85%çš„å¯¹é½åº¦ï¼ŒéªŒè¯äº†è‡ªåŠ¨åŒ–è¯„ä¼°çš„å¯é æ€§ã€‚</p>
<p><strong>Conclusion:</strong> RULER-BenchåŸºå‡†æµ‹è¯•æ­ç¤ºäº†å½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨è§„åˆ™æ¨ç†èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæ¨ç†æ„ŸçŸ¥çš„è§†é¢‘ç”Ÿæˆç ”ç©¶æä¾›äº†é‡è¦è¯„ä¼°å·¥å…·ã€‚è¯¥ç ”ç©¶æœ‰æœ›æ¨åŠ¨è§†é¢‘ç”Ÿæˆæ¨¡å‹å‘è§†è§‰åŸºç¡€æ™ºèƒ½å‘å±•ï¼Œä¿ƒè¿›ä¸‹ä¸€ä»£è§†é¢‘æ¨¡å‹åœ¨è®¤çŸ¥è§„åˆ™ç†è§£æ–¹é¢çš„è¿›æ­¥ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.</p>
<h3 id="35-pptbench-towards-holistic-evaluation-of-large-language-models-for-powerpoint-layout-and-design-understanding">[35] <a href="https://arxiv.org/abs/2512.02624">PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding</a></h3>
<p><em>Zheng Huang, Xukai Liu, Tianyu Hu, Kai Zhang, Ye Liu</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†PPTBenchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨PowerPointç›¸å…³ä»»åŠ¡ä¸Šæ€§èƒ½çš„ç»¼åˆåŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è§†è§‰å¸ƒå±€æ¨ç†æ–¹é¢çš„æ˜¾è‘—ä¸è¶³ï¼Œä¸ºè§†è§‰-ç»“æ„æ¨ç†ç ”ç©¶æä¾›äº†æ–°çš„è¯„ä¼°è§†è§’ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºå‡†ä¸»è¦å…³æ³¨ç‹­çª„çš„å­ä»»åŠ¡ï¼Œå¿½è§†äº†å¸ƒå±€ç†è§£è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè€Œå¸ƒå±€ç†è§£å¯¹äºç°å®ä¸–ç•Œçš„å¹»ç¯ç‰‡åˆ›å»ºå’Œç¼–è¾‘è‡³å…³é‡è¦ã€‚PowerPointæ¼”ç¤ºæ–‡ç¨¿ç»“åˆäº†ä¸°å¯Œçš„æ–‡æœ¬å†…å®¹å’Œç»“æ„åŒ–è§†è§‰å¸ƒå±€ï¼Œæ˜¯è¯„ä¼°ç°ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¤šæ¨¡æ€æ¨ç†å’Œå¸ƒå±€ç†è§£èƒ½åŠ›çš„å¤©ç„¶æµ‹è¯•å¹³å°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†PPTBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œåˆ©ç”¨958ä¸ªPPTXæ–‡ä»¶çš„å¤šæ ·åŒ–æ¥æºï¼Œåœ¨å››ä¸ªç±»åˆ«ä¸­è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬æ£€æµ‹ã€ç†è§£ã€ä¿®æ”¹å’Œç”Ÿæˆï¼Œå…±åŒ…å«4,439ä¸ªæ ·æœ¬ã€‚è¯¥åŸºå‡†é€šè¿‡ç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨è§†è§‰å¸ƒå±€æ¨ç†å’Œè¯­ä¹‰ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨æ¨¡å‹ç»“åˆè§†è§‰çº¿ç´¢ä¸JSONå¸ƒå±€ç»“æ„çš„èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å®éªŒæ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œè§†è§‰å¸ƒå±€æ¨ç†ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼šæ¨¡å‹èƒ½å¤Ÿè§£é‡Šå¹»ç¯ç‰‡å†…å®¹ï¼Œä½†æ— æ³•ç”Ÿæˆè¿è´¯çš„ç©ºé—´æ’åˆ—ã€‚æ¶ˆèåˆ†æå’Œè¿›ä¸€æ­¥ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æ¨¡å‹éš¾ä»¥å°†è§†è§‰çº¿ç´¢ä¸JSONå¸ƒå±€ç»“æ„ç›¸ç»“åˆï¼Œä¹Ÿæ— æ³•å°†è§†è§‰ä¿¡æ¯æ•´åˆåˆ°å…¶APIè§„åˆ’èƒ½åŠ›ä¸­ã€‚æ¡ˆä¾‹ç ”ç©¶ç›´è§‚åœ°æš´éœ²äº†ç³»ç»Ÿæ€§å¸ƒå±€é”™è¯¯ï¼Œå¦‚é”™ä½å’Œå…ƒç´ é‡å ã€‚</p>
<p><strong>Conclusion:</strong> è¿™äº›å‘ç°ä¸ºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨PPTåœºæ™¯ä¸­çš„æ€§èƒ½æä¾›äº†æ–°è§†è§’ï¼Œçªæ˜¾äº†è§†è§‰-ç»“æ„æ¨ç†å’Œè¿è´¯å¹»ç¯ç‰‡ç”Ÿæˆæ–¹é¢çš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚ç ”ç©¶å›¢é˜Ÿå®Œå…¨å‘å¸ƒäº†æ‰€æœ‰æ•°æ®é›†å’Œä»£ç ï¼Œä»¥æ”¯æŒå¯é‡å¤æ€§å’Œæœªæ¥ç ”ç©¶ï¼Œå¼ºè°ƒäº†å¼€å‘èƒ½å¤Ÿæœ‰æ•ˆç»“åˆè¯­ä¹‰ç†è§£å’Œç©ºé—´å¸ƒå±€æ¨ç†çš„å¤šæ¨¡æ€æ¨¡å‹çš„å¿…è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>PowerPoint presentations combine rich textual content with structured visual layouts, making them a natural testbed for evaluating the multimodal reasoning and layout understanding abilities of modern MLLMs. However, existing benchmarks focus solely on narrow subtasks while overlooking layout-centric challenges, which are central to real-world slide creation and editing. To bridge this gap, we introduce PPTBench, a comprehensive multimodal benchmark for evaluating LLMs on PowerPoint-related tasks. Leveraging a diverse source of 958 PPTX files, PPTBench evaluates models across four categories with 4,439 samples, including Detection, Understanding, Modification, and Generation. Our experiments reveal a substantial gap between semantic understanding and visual-layout reasoning in current MLLMs: models can interpret slide content but fail to produce coherent spatial arrangements. Ablation and further analysis show that current MLLMs struggle to combine visual cues with JSON-based layout structures and fail to integrate visual information into their API planning ability. And case studies visually expose systematic layout errors such as misalignment and element overlap. These findings provides a new perspective on evaluating VLLMs in PPT scenarios, highlighting challenges and directions for future research on visual-structural reasoning and coherent slide generation. All datasets and code are fully released to support reproducibility and future research.</p>
<h3 id="36-leveraging-large-scale-pretrained-spatial-spectral-priors-for-general-zero-shot-pansharpening">[36] <a href="https://arxiv.org/abs/2512.02643">Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening</a></h3>
<p><em>Yongchuan Cui, Peng Liu, Yi Zeng</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œé¢„è®­ç»ƒçš„æ–°ç­–ç•¥ï¼Œé€šè¿‡åŸºç¡€æ¨¡å‹å­¦ä¹ é²æ£’çš„ç©ºé—´-å…‰è°±å…ˆéªŒçŸ¥è¯†ï¼Œæ˜¾è‘—æå‡äº†é¥æ„Ÿå›¾åƒèåˆæ¨¡å‹çš„è·¨ä¼ æ„Ÿå™¨æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹å‡å–å¾—ä¼˜å¼‚æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é¥æ„Ÿå›¾åƒèåˆæ·±åº¦å­¦ä¹ æ–¹æ³•ç”±äºçœŸå®è®­ç»ƒæ•°æ®æœ‰é™ä»¥åŠä¸åŒå«æ˜Ÿä¼ æ„Ÿå™¨ä¹‹é—´å­˜åœ¨åŸŸå·®è·ï¼Œåœ¨åº”ç”¨äºæœªè§æ•°æ®é›†æ—¶æ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨å®é™…è·¨åŸŸåœºæ™¯ä¸­çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡æ„å»ºå¤šæ ·åŒ–æ¨¡æ‹Ÿæ•°æ®é›†æ¥å­¦ä¹ é²æ£’çš„ç©ºé—´-å…‰è°±å…ˆéªŒçŸ¥è¯†ã€‚å…·ä½“æ–¹æ³•åŒ…æ‹¬å¯¹ImageNetè‡ªç„¶å›¾åƒå’ŒSkyScripté¥æ„Ÿå›¾åƒåº”ç”¨å¤šç§é€€åŒ–æ“ä½œï¼ˆæ¨¡ç³Šã€å™ªå£°ã€ä¸‹é‡‡æ ·ï¼‰å’Œæ•°æ®å¢å¼ºæŠ€æœ¯ï¼ˆæ³¢æ®µç”Ÿæˆã€é€šé“æ··æ´—ã€é«˜é€šæ»¤æ³¢ã€é¢œè‰²æŠ–åŠ¨ç­‰ï¼‰ï¼Œç„¶ååœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šé¢„è®­ç»ƒèåˆæ¨¡å‹ï¼Œæœ€åé‡‡ç”¨é›¶æ ·æœ¬å’Œå•æ ·æœ¬èŒƒå¼åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ¢ç´¢äº†å…¨å¾®è°ƒå’Œå†»ç»“å¾®è°ƒä¸¤ç§å¾®è°ƒæ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> åœ¨å·ç§¯ç¥ç»ç½‘ç»œã€Transformerå’ŒMambaç­‰å¤šç§ç½‘ç»œæ¶æ„ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥é¢„è®­ç»ƒç­–ç•¥æ˜¾è‘—æå‡äº†ä¸åŒå«æ˜Ÿä¼ æ„Ÿå™¨å’Œæˆåƒæ¡ä»¶ä¸‹å„ç§èåˆæ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚é¢„è®­ç»ƒæ¨¡å‹åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­å–å¾—ä¼˜å¼‚ç»“æœï¼Œåœ¨å•æ ·æœ¬è®¾ç½®ä¸­å±•ç°å‡ºä»…éœ€å°‘é‡çœŸå®æ•°æ®çš„æ˜¾è‘—é€‚åº”èƒ½åŠ›ï¼Œåœ¨WorldView-2/3/4ã€IKONOSã€QuickBirdã€GaoFen-2ç­‰å…­ä¸ªæ•°æ®é›†ä¸Šå‡éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè·¨åŸŸå…¨è‰²é”åŒ–æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå»ºç«‹äº†é¥æ„Ÿå›¾åƒèåˆä»»åŠ¡ä¸­æ³›åŒ–èƒ½åŠ›çš„æ–°åŸºå‡†ï¼Œå¹¶é€šè¿‡å…ˆè¿›çš„è®­ç»ƒç­–ç•¥ä¸ºåˆ©ç”¨åŸºç¡€æ¨¡å‹å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå±•ç¤ºäº†æ¨¡æ‹Ÿæ•°æ®é¢„è®­ç»ƒåœ¨æå‡æ¨¡å‹è·¨ä¼ æ„Ÿå™¨é€‚åº”æ€§å’Œå‡å°‘çœŸå®æ•°æ®ä¾èµ–æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.</p>
<h3 id="37-hear-what-matters-text-conditioned-selective-video-to-audio-generation">[37] <a href="https://arxiv.org/abs/2512.02650">Hear What Matters! Text-conditioned Selective Video-to-Audio Generation</a></h3>
<p><em>Junwon Lee, Juhan Nam, Jiyoung Lee</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SelVAæ¨¡å‹ï¼Œç”¨äºè§£å†³æ–‡æœ¬æ¡ä»¶é€‰æ‹©æ€§è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ï¼Œèƒ½å¤Ÿä»å¤šå¯¹è±¡è§†é¢‘ä¸­ä»…ç”Ÿæˆç”¨æˆ·æŒ‡å®šçš„å£°éŸ³æºï¼Œé€šè¿‡æç¤ºè°ƒåˆ¶è§†é¢‘ç¼–ç å™¨å’ŒæŠ‘åˆ¶æ— å…³æ¿€æ´»æ¥å®ç°ç²¾ç¡®çš„éŸ³é¢‘åˆ†ç¦»ä¸ç”Ÿæˆã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆæ–¹æ³•é€šå¸¸ä¸€æ¬¡æ€§ç”Ÿæˆæ··åˆçš„å•ä¸€æºå£°éŸ³ï¼Œä¸»è¦å› ä¸ºè§†è§‰ç‰¹å¾çº ç¼ ä¸”åŒºåŸŸæç¤ºå¾€å¾€æ— æ³•å‡†ç¡®æŒ‡å®šç‰¹å®šå£°æºï¼Œè¿™é™åˆ¶äº†å¤šåª’ä½“åˆ¶ä½œä¸­éœ€è¦å¯¹æ¯ä¸ªå£°æºè¿›è¡Œç‹¬ç«‹ç¼–è¾‘ã€æ··åˆå’Œåˆ›æ„æ§åˆ¶çš„éœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> SelVAæ¨¡å‹å°†æ–‡æœ¬æç¤ºä½œä¸ºç›®æ ‡å£°æºçš„æ˜¾å¼é€‰æ‹©å™¨ï¼Œè°ƒåˆ¶è§†é¢‘ç¼–ç å™¨ä»¥æå–ä¸æç¤ºç›¸å…³çš„è§†é¢‘ç‰¹å¾ï¼›æå‡ºè¡¥å……æ ‡è®°é€šè¿‡æŠ‘åˆ¶æ–‡æœ¬æ— å…³æ¿€æ´»æ¥ä¿ƒè¿›è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°é«˜æ•ˆçš„å‚æ•°è°ƒæ•´ï¼›é‡‡ç”¨è‡ªå¢å¼ºæ–¹æ¡ˆè§£å†³å•å£°é“éŸ³é¢‘ç›‘ç£æ•°æ®ç¼ºä¹çš„é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸“é—¨æ„å»ºçš„VGG-MONOAUDIOåŸºå‡†æµ‹è¯•ä¸Šï¼ŒSelVAåœ¨éŸ³é¢‘è´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œæ—¶é—´åŒæ­¥æ–¹é¢å‡è¡¨ç°å‡ºè‰²ï¼Œå¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶ä¸€è‡´éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œæ¨¡å‹èƒ½å¤Ÿç²¾ç¡®ç”Ÿæˆç”¨æˆ·æŒ‡å®šçš„å£°éŸ³æºã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¤šåª’ä½“åˆ¶ä½œæä¾›äº†ç²¾ç¡®çš„éŸ³é¢‘ç¼–è¾‘å·¥å…·ï¼Œé€šè¿‡æ–‡æœ¬æ¡ä»¶è°ƒåˆ¶å®ç°äº†è§†è§‰ç‰¹å¾çš„è§£çº ç¼ ï¼Œä¸ºé€‰æ‹©æ€§éŸ³é¢‘ç”Ÿæˆä»»åŠ¡å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œå¹¶ä¸ºç¼ºä¹ç›‘ç£æ•°æ®çš„åœºæ™¯æä¾›äº†æœ‰æ•ˆçš„è‡ªå¢å¼ºè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>This work introduces a new task, text-conditioned selective video-to-audio (V2A) generation, which produces only the user-intended sound from a multi-object video. This capability is especially crucial in multimedia production, where audio tracks are handled individually for each sound source for precise editing, mixing, and creative control. However, current approaches generate single source-mixed sounds at once, largely because visual features are entangled, and region cues or prompts often fail to specify the source. We propose SelVA, a novel text-conditioned V2A model that treats the text prompt as an explicit selector of target source and modulates video encoder to distinctly extract prompt-relevant video features. The proposed supplementary tokens promote cross-attention by suppressing text-irrelevant activations with efficient parameter tuning, yielding robust semantic and temporal grounding. SelVA further employs a self-augmentation scheme to overcome the lack of mono audio track supervision. We evaluate SelVA on VGG-MONOAUDIO, a curated benchmark of clean single-source videos for such a task. Extensive experiments and ablations consistently verify its effectiveness across audio quality, semantic alignment, and temporal synchronization. Code and demo are available at https://jnwnlee.github.io/selva-demo/.</p>
<h3 id="38-spatially-grounded-document-retrieval-via-patch-to-region-relevance-propagation">[38] <a href="https://arxiv.org/abs/2512.02660">Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation</a></h3>
<p><em>Agathoklis Georgiou</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆæ¶æ„ï¼Œå°†è§†è§‰è¯­è¨€æ¨¡å‹çš„ç»†ç²’åº¦ç›¸ä¼¼æ€§è¯„åˆ†ä¸OCRæå–çš„åŒºåŸŸç›¸ç»“åˆï¼Œå®ç°äº†æ–‡æ¡£æ£€ç´¢ä¸­ç²¾ç¡®åŒºåŸŸå®šä½ï¼Œè€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚è¯¥ç ”ç©¶é€šè¿‡åæ ‡æ˜ å°„å’Œäº¤é›†åº¦é‡è§£å†³äº†è§†è§‰è¡¥ä¸ä¸OCRè¾¹ç•Œæ¡†ä¹‹é—´çš„å¯¹é½é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹å¦‚ColPaliåœ¨æ–‡æ¡£æ£€ç´¢ä¸­è¿”å›æ•´ä¸ªé¡µé¢è€Œéç‰¹å®šåŒºåŸŸï¼Œé™åˆ¶äº†æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­ç²¾ç¡®ä¸Šä¸‹æ–‡çš„éœ€æ±‚ã€‚åŒæ—¶ï¼ŒåŸºäºOCRçš„ç³»ç»Ÿè™½ç„¶èƒ½æå–å¸¦åæ ‡çš„ç»“æ„åŒ–æ–‡æœ¬ï¼Œä½†ç¼ºä¹è¯­ä¹‰ç›¸å…³æ€§è¯„ä¼°èƒ½åŠ›ã€‚è¿™ä¸¤ç§èŒƒå¼å„è‡ªå­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•æ¥ç»“åˆå®ƒä»¬çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆæ¶æ„ï¼Œåˆ©ç”¨ColPaliçš„è¡¥ä¸çº§ç›¸ä¼¼æ€§è¯„åˆ†ä½œä¸ºç©ºé—´ç›¸å…³æ€§è¿‡æ»¤å™¨ï¼Œåº”ç”¨äºOCRæå–çš„åŒºåŸŸã€‚è¯¥æ–¹æ³•å½¢å¼åŒ–äº†è§†è§‰å˜æ¢å™¨è¡¥ä¸ç½‘æ ¼ä¸OCRè¾¹ç•Œæ¡†ä¹‹é—´çš„åæ ‡æ˜ å°„ï¼Œå¼•å…¥äº†äº¤é›†åº¦é‡è¿›è¡Œç›¸å…³æ€§ä¼ æ’­ï¼Œå¹¶å»ºç«‹äº†æ£€ç´¢ç²¾åº¦çš„ç†è®ºç•Œé™ã€‚æ•´ä¸ªæ–¹æ³•åœ¨æ¨ç†æ—¶è¿è¡Œï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘å¸ƒäº†å¼€æºå®ç°Snappyï¼Œå±•ç¤ºäº†å®é™…åº”ç”¨å¯è¡Œæ€§ã€‚è™½ç„¶ç»éªŒè¯„ä¼°ä»åœ¨è¿›è¡Œä¸­ï¼Œä½†è¯¥æ–¹æ³•é€šè¿‡ç†è®ºåˆ†æå»ºç«‹äº†æ£€ç´¢ç²¾åº¦çš„ç•Œé™ï¼Œå¹¶é€šè¿‡åæ ‡æ˜ å°„å’Œäº¤é›†åº¦é‡è§£å†³äº†è§†è§‰è¡¥ä¸ä¸OCRåŒºåŸŸçš„å¯¹é½é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ä¸OCRç³»ç»Ÿçš„ç©ºé—´å®šä½ç²¾åº¦ï¼Œä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆæä¾›äº†æ›´ç²¾ç¡®çš„ä¸Šä¸‹æ–‡æå–æ–¹æ³•ã€‚æ··åˆæ¶æ„çš„æå‡ºä¸ºæ–‡æ¡£æ£€ç´¢é¢†åŸŸå¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»†ç²’åº¦åŒºåŸŸå®šä½çš„åº”ç”¨åœºæ™¯ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>Vision-language models (VLMs) like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on retrieval precision. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation demonstrating practical applicability, with empirical evaluation ongoing.</p>
<h3 id="39-uautrack-towards-unified-multimodal-anti-uav-visual-tracking">[39] <a href="https://arxiv.org/abs/2512.02668">UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking</a></h3>
<p><em>Qionglin Ren, Dawei Zhang, Chunxu Tian, Dan Zhang</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºUAUTrackï¼Œä¸€ä¸ªç”¨äºåæ— äººæœºè·Ÿè¸ªçš„ç»Ÿä¸€å•ç›®æ ‡è·Ÿè¸ªæ¡†æ¶ï¼Œé€šè¿‡å•æµå•é˜¶æ®µç«¯åˆ°ç«¯æ¶æ„æœ‰æ•ˆæ•´åˆå¤šç§æ¨¡æ€ï¼Œå¹¶å¼•å…¥æ–‡æœ¬å…ˆéªŒæç¤ºç­–ç•¥å¼•å¯¼æ¨¡å‹å…³æ³¨ä¸åŒåœºæ™¯ä¸‹çš„æ— äººæœºç›®æ ‡ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åæ— äººæœºè·Ÿè¸ªç ”ç©¶è™½ç„¶æ¢ç´¢äº†RGBã€TIRå’ŒRGB-Tèåˆç­‰å¤šç§æ¨¡æ€ï¼Œä½†ç¼ºä¹è·¨æ¨¡æ€åä½œçš„ç»Ÿä¸€æ¡†æ¶ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç‹¬ç«‹ä»»åŠ¡çš„ç‹¬ç«‹æ¨¡å‹ï¼Œå¿½è§†äº†è·¨æ¨¡æ€ä¿¡æ¯å…±äº«çš„æ½œåŠ›ï¼Œä¸”å½“å‰åæ— äººæœºè·Ÿè¸ªæŠ€æœ¯ä»å¤„äºèµ·æ­¥é˜¶æ®µï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆéš¾ä»¥å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€æ•°æ®èåˆã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºUAUTrackæ¡†æ¶ï¼Œé‡‡ç”¨å•æµå•é˜¶æ®µç«¯åˆ°ç«¯æ¶æ„ï¼Œæœ‰æ•ˆæ•´åˆå¤šç§æ¨¡æ€ã€‚è¯¥æ¡†æ¶å¼•å…¥å…³é”®ç»„ä»¶â€”â€”æ–‡æœ¬å…ˆéªŒæç¤ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¼•å¯¼æ¨¡å‹å…³æ³¨ä¸åŒåœºæ™¯ä¸‹çš„æ— äººæœºç›®æ ‡ï¼Œå®ç°è·¨æ¨¡æ€ä¿¡æ¯çš„é«˜æ•ˆèåˆä¸åä½œã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒUAUTrackåœ¨Anti-UAVå’ŒDUT Anti-UAVæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨Anti-UAV410æ•°æ®é›†ä¸Šï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¹‹é—´ä¿æŒäº†è‰¯å¥½çš„å¹³è¡¡ï¼Œå±•ç¤ºäº†åœ¨ä¸åŒåæ— äººæœºåœºæ™¯ä¸‹çš„é«˜å‡†ç¡®æ€§å’Œå®é™…æ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè§£å†³åæ— äººæœºè·Ÿè¸ªä¸­å¤šæ¨¡æ€èåˆçš„æŒ‘æˆ˜æä¾›äº†ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡æ–‡æœ¬å…ˆéªŒæç¤ºç­–ç•¥å®ç°äº†è·¨æ¨¡æ€çš„æœ‰æ•ˆåä½œã€‚UAUTrackå±•ç¤ºäº†åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´çš„è‰¯å¥½æƒè¡¡ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„åæ— äººæœºè·Ÿè¸ªç³»ç»Ÿæä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.</p>
<h3 id="40-climaood-improving-anomaly-segmentation-via-physically-realistic-synthetic-data">[40] <a href="https://arxiv.org/abs/2512.02686">ClimaOoD: Improving Anomaly Segmentation via Physically Realistic Synthetic Data</a></h3>
<p><em>Yuxing Liu, Yong Liu</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºClimaDriveæ¡†æ¶ï¼Œä¸€ç§è¯­ä¹‰å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒåˆæˆæ–¹æ³•ï¼Œç”¨äºç”Ÿæˆè¯­ä¹‰è¿è´¯ã€å¤©æ°”å¤šæ ·ä¸”ç‰©ç†åˆç†çš„å¼‚å¸¸é©¾é©¶æ•°æ®ï¼Œå¹¶æ„å»ºäº†å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ClimaOoDï¼Œæ˜¾è‘—æå‡äº†å¼‚å¸¸åˆ†å‰²æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¼‚å¸¸åˆ†å‰²åœ¨è‡ªåŠ¨é©¾é©¶ä¸­è‡³å…³é‡è¦ï¼Œä½†å¼‚å¸¸æ•°æ®çš„ç¨€ç¼ºæ€§å’Œå¤šæ ·æ€§ä¸è¶³ä¸¥é‡é™åˆ¶äº†æ¨¡å‹åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡åˆæˆæ•°æ®ç”Ÿæˆæ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å¾€å¾€ç¼ºä¹ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œç‰©ç†çœŸå®æ€§ï¼Œå¯¼è‡´åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®ä¹‹é—´å­˜åœ¨é¢†åŸŸå·®è·ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºClimaDriveæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§è¯­ä¹‰å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒåˆæˆæ¡†æ¶ï¼Œç»Ÿä¸€äº†ç»“æ„å¼•å¯¼çš„å¤šå¤©æ°”ç”Ÿæˆä¸æç¤ºé©±åŠ¨çš„å¼‚å¸¸ä¿®å¤æŠ€æœ¯ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåˆæˆè¯­ä¹‰è¿è´¯ã€å¤©æ°”å¤šæ ·ä¸”ç‰©ç†åˆç†çš„å¼‚å¸¸é©¾é©¶æ•°æ®ï¼Œå¹¶åŸºäºæ­¤æ„å»ºäº†ClimaOoDåŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–å…­ç§ä»£è¡¨æ€§é©¾é©¶åœºæ™¯åœ¨æ™´æœ—å’Œæ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„æƒ…å†µã€‚</p>
<p><strong>Result:</strong> åœ¨å››ç§æœ€å…ˆè¿›æ–¹æ³•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ClimaOoDè¿›è¡Œè®­ç»ƒæ˜¾è‘—æå‡äº†å¼‚å¸¸åˆ†å‰²æ€§èƒ½ã€‚æ‰€æœ‰æ–¹æ³•çš„AUROCã€APå’ŒFPR95æŒ‡æ ‡å‡æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå…¶ä¸­RbAæ–¹æ³•åœ¨Fishyscapes LAFä¸Šçš„FPR95ä»3.97é™è‡³3.52ï¼Œè¯æ˜äº†æ•°æ®å¢å¼ºçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ClimaOoDæ•°æ®é›†å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œå¼‚å¸¸æ£€æµ‹æä¾›äº†æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ï¼Œä¿ƒè¿›äº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†è¯­ä¹‰å¼•å¯¼çš„åˆæˆæ•°æ®ç”Ÿæˆåœ¨è§£å†³å¼‚å¸¸æ•°æ®ç¨€ç¼ºé—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶å®‰å…¨ç³»ç»Ÿçš„å¼€å‘æä¾›äº†é‡è¦æ”¯æŒã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>Anomaly segmentation seeks to detect and localize unknown or out-of-distribution (OoD) objects that fall outside predefined semantic classes a capability essential for safe autonomous driving. However, the scarcity and limited diversity of anomaly data severely constrain model generalization in open-world environments. Existing approaches mitigate this issue through synthetic data generation, either by copy-pasting external objects into driving scenes or by leveraging text-to-image diffusion models to inpaint anomalous regions. While these methods improve anomaly diversity, they often lack contextual coherence and physical realism, resulting in domain gaps between synthetic and real data. In this paper, we present ClimaDrive, a semantics-guided image-to-image framework for synthesizing semantically coherent, weather-diverse, and physically plausible OoD driving data. ClimaDrive unifies structure-guided multi-weather generation with prompt-driven anomaly inpainting, enabling the creation of visually realistic training data. Based on this framework, we construct ClimaOoD, a large-scale benchmark spanning six representative driving scenarios under both clear and adverse weather conditions. Extensive experiments on four state-of-the-art methods show that training with ClimaOoD leads to robust improvements in anomaly segmentation. Across all methods, AUROC, AP, and FPR95 show notable gains, with FPR95 dropping from 3.97 to 3.52 for RbA on Fishyscapes LAF. These results demonstrate that ClimaOoD enhances model robustness, offering valuable training data for better generalization in open-world anomaly detection.</p>
<h3 id="41-geobridge-a-semantic-anchored-multi-view-foundation-model-bridging-images-and-text-for-geo-localization">[41] <a href="https://arxiv.org/abs/2512.02697">GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization</a></h3>
<p><em>Zixuan Song, Jing Zhang, Di Wang, Zidie Zhou, Wenbin Liu, Haonan Guo, En Wang, Bo Du</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†GeoBridgeï¼Œä¸€ä¸ªç”¨äºè·¨è§†è§’åœ°ç†å®šä½çš„åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡æ–°é¢–çš„è¯­ä¹‰é”šæœºåˆ¶å®ç°å¤šè§†è§’åŒå‘åŒ¹é…å¹¶æ”¯æŒè¯­è¨€åˆ°å›¾åƒæ£€ç´¢ï¼ŒåŒæ—¶æ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡è·¨æ¨¡æ€å¤šè§†è§’å¯¹é½æ•°æ®é›†GeoLocã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿå«æ˜Ÿä¸­å¿ƒçš„åœ°ç†å®šä½èŒƒå¼åœ¨ç¼ºä¹é«˜åˆ†è¾¨ç‡æˆ–æœ€æ–°å«æ˜Ÿå›¾åƒæ—¶é²æ£’æ€§å—é™ï¼Œä¸”æœªèƒ½å……åˆ†åˆ©ç”¨è·¨è§†è§’ï¼ˆå¦‚æ— äººæœºã€å«æ˜Ÿã€è¡—æ™¯ï¼‰å’Œè·¨æ¨¡æ€ï¼ˆå¦‚è¯­è¨€ä¸å›¾åƒï¼‰çš„äº’è¡¥çº¿ç´¢ï¼Œéœ€è¦æ›´çµæ´»ã€é²æ£’çš„å®šä½æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†GeoBridgeåŸºç¡€æ¨¡å‹ï¼Œé‡‡ç”¨æ–°é¢–çš„è¯­ä¹‰é”šæœºåˆ¶é€šè¿‡æ–‡æœ¬æè¿°æ¡¥æ¥å¤šè§†è§’ç‰¹å¾ï¼Œå®ç°åŒå‘è·¨è§†è§’åŒ¹é…å’Œè¯­è¨€åˆ°å›¾åƒæ£€ç´¢ï¼›åŒæ—¶æ„å»ºäº†GeoLocæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª36ä¸ªå›½å®¶çš„è¶…è¿‡50,000å¯¹æ— äººæœºã€è¡—æ™¯å…¨æ™¯å’Œå«æ˜Ÿå›¾åƒåŠå…¶æ–‡æœ¬æè¿°ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼ŒGeoLocé¢„è®­ç»ƒæ˜¾è‘—æå‡äº†GeoBridgeçš„åœ°ç†å®šä½ç²¾åº¦ï¼ŒåŒæ—¶ä¿ƒè¿›äº†è·¨åŸŸæ³›åŒ–èƒ½åŠ›å’Œè·¨æ¨¡æ€çŸ¥è¯†è¿ç§»ï¼›æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶çªç ´äº†ä¼ ç»Ÿå«æ˜Ÿä¸­å¿ƒçš„åœ°ç†å®šä½èŒƒå¼ï¼Œé€šè¿‡è¯­ä¹‰é”šæœºåˆ¶å’Œå¤šæ¨¡æ€å¯¹é½å®ç°äº†æ›´é²æ£’ã€çµæ´»çš„å®šä½ï¼›GeoLocæ•°æ®é›†çš„æ„å»ºä¸ºè·¨è§†è§’è·¨æ¨¡æ€åœ°ç†å®šä½ç ”ç©¶æä¾›äº†é‡è¦èµ„æºï¼Œæ¨åŠ¨äº†åŸºç¡€æ¨¡å‹åœ¨åœ°ç†ç©ºé—´ç†è§£ä¸­çš„åº”ç”¨ã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.</p>
<h3 id="42-vlm-pruner-buffering-for-spatial-sparsity-in-an-efficient-vlm-centrifugal-token-pruning-paradigm">[42] <a href="https://arxiv.org/abs/2512.02700">VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm</a></h3>
<p><em>Zhenkai Wu, Xiaowen Ma, Zhenliang Ni, Dengming Zhang, Han Shu, Xin Jiang, Xinghao Chen</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºVLM-Prunerï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ä»¤ç‰Œå‰ªæç®—æ³•ï¼Œé€šè¿‡æ˜¾å¼å¹³è¡¡å†—ä½™æ€§å’Œç©ºé—´ç¨€ç–æ€§ï¼Œåœ¨ä¿æŒ88.9%å‰ªæç‡çš„åŒæ—¶å®ç°ç«¯åˆ°ç«¯æ¨ç†åŠ é€Ÿï¼Œå¹¶åœ¨äº”ç§VLMä¸Šä¸€è‡´ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¤§é‡è§†è§‰ä»¤ç‰Œå¸¦æ¥äº†æ˜¾è‘—çš„è®¡ç®—æˆæœ¬ï¼Œé˜»ç¢äº†åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚ç°æœ‰å‰ªææ–¹æ³•ä»…ä¾èµ–ä»¤ç‰Œé‡è¦æ€§è€Œå¿½ç•¥äº†ä»¤ç‰Œé—´å†—ä½™æ€§ï¼Œå¯¼è‡´ä¿ç•™å¤§é‡é‡å¤ä»¤ç‰Œæµªè´¹å®¹é‡ï¼›è€Œä¸€äº›å†—ä½™æ„ŸçŸ¥æ–¹æ³•åˆå¿½è§†äº†è§†è§‰ä»¤ç‰Œçš„ç©ºé—´å…³ç³»ï¼Œå¯èƒ½å¯¼è‡´ä¿ç•™ä»¤ç‰Œè¿‡äºç¨€ç–è€Œæ— æ³•å……åˆ†è¦†ç›–ç›®æ ‡å¯¹è±¡åŒºåŸŸã€‚</p>
<p><strong>Method:</strong> æå‡ºVLM-Prunerç®—æ³•ï¼Œé‡‡ç”¨ç¦»å¿ƒå¼ä»¤ç‰Œå‰ªæèŒƒå¼å®ç°ç”±è¿‘åŠè¿œçš„é€‰æ‹©ï¼Œä¼˜å…ˆä¿ç•™ç»†ç²’åº¦ç‰©ä½“ç»†èŠ‚ã€‚è®¾è®¡ç¼“å†²ç©ºé—´ç¨€ç–æ€§å‡†åˆ™æ¨è¿Ÿé€‰æ‹©ç©ºé—´è·ç¦»è¾ƒè¿œçš„ä»¤ç‰Œï¼Œé‡‡ç”¨å¹¶è¡Œè´ªå©ªç­–ç•¥é«˜æ•ˆè¿›è¡Œä»¤ç‰Œé€‰æ‹©ï¼Œå¹¶é€‰æ‹©æ€§èåˆè¢«ä¸¢å¼ƒä»¤ç‰Œä¸­çš„æ˜¾è‘—ä¿¡æ¯åˆ°ä¿ç•™ä»¤ç‰Œä¸­ä»¥å‡è½»ä¿¡æ¯æŸå¤±ã€‚</p>
<p><strong>Result:</strong> åœ¨äº”ç§è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šï¼ŒVLM-Pruneråœ¨88.9%çš„å‰ªæç‡ä¸‹ä¸€è‡´ä¼˜äºå¼ºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶å®ç°äº†ç«¯åˆ°ç«¯æ¨ç†åŠ é€Ÿã€‚ç»¼åˆæ¯”è¾ƒè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¿æŒé«˜å‰ªæç‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ˜¾å¼å¹³è¡¡å†—ä½™æ€§å’Œç©ºé—´ç¨€ç–æ€§åœ¨è§†è§‰ä»¤ç‰Œå‰ªæä¸­çš„é‡è¦æ€§ï¼Œæå‡ºçš„ç¦»å¿ƒå¼å‰ªæèŒƒå¼å’Œç¼“å†²ç©ºé—´ç¨€ç–æ€§å‡†åˆ™ä¸ºè§£å†³ç°æœ‰æ–¹æ³•å±€é™æ€§æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„é«˜æ•ˆéƒ¨ç½²å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\% pruning rate, while delivering an end-to-end inference speedup.</p>
<h3 id="43-geovis-geospatially-rewarded-visual-search-for-remote-sensing-visual-grounding">[43] <a href="https://arxiv.org/abs/2512.02715">GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding</a></h3>
<p><em>Peirong Zhang, Yidan Zhang, Luxiao Xu, Jinliang Lin, Zonghao Guo, Fengxiang Wang, Xue Yang, Kaiwen Wei, Lei Wang</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGeoViSæ¡†æ¶ï¼Œå°†é¥æ„Ÿè§†è§‰å®šä½é‡æ–°æ„å»ºä¸ºæ¸è¿›å¼æœç´¢æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡æ ‘çŠ¶è§†è§‰çº¿ç´¢åºåˆ—ä¸»åŠ¨æ¢ç´¢å…¨å±€å›¾åƒï¼Œåœ¨å¤šä¸ªé¥æ„ŸåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ç²¾ç¡®çš„åœ°ç†ç©ºé—´ç†è§£å¹¶è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰å®šä½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†å…¶èƒ½åŠ›è¿ç§»åˆ°é¥æ„Ÿå›¾åƒä»é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºç›®æ ‡é€šå¸¸åœ¨åƒç±³çº§åœºæ™¯ä¸­æå…¶å¾®å°ï¼Œä¸”æŸ¥è¯¢é€šå¸¸æ¶‰åŠå¤æ‚çš„ç©ºé—´å…³ç³»ï¼Œå¦‚ç›¸å¯¹ä½ç½®ã€ç©ºé—´å±‚æ¬¡ç»“æ„æˆ–è·¨è¿œè·ç¦»å¯¹è±¡çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºGeoViSæ¡†æ¶ï¼Œå°†é¥æ„Ÿè§†è§‰å®šä½é‡æ–°æ„å»ºä¸ºæ¸è¿›å¼æœç´¢æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡æ ‘çŠ¶è§†è§‰çº¿ç´¢åºåˆ—ä¸»åŠ¨æ¢ç´¢å…¨å±€å›¾åƒï¼Œæ•´åˆå¤šæ¨¡æ€æ„ŸçŸ¥ã€ç©ºé—´æ¨ç†å’Œå¥–åŠ±å¼•å¯¼æ¢ç´¢ï¼Œè¿­ä»£ä¼˜åŒ–åœ°ç†ç©ºé—´å‡è®¾ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ£€æµ‹å¾®å°ç›®æ ‡åŒæ—¶ä¿æŒæ•´ä½“åœºæ™¯æ„ŸçŸ¥ã€‚</p>
<p><strong>Result:</strong> åœ¨äº”ä¸ªé¥æ„Ÿå®šä½åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGeoViSå®ç°äº†ç²¾ç¡®çš„åœ°ç†ç©ºé—´ç†è§£ï¼Œåœ¨å…³é”®è§†è§‰å®šä½æŒ‡æ ‡ä¸ŠæŒç»­è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå±•ç°äº†å¼ºå¤§çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†æ¸è¿›å¼æœç´¢æ¨ç†æ¡†æ¶åœ¨è§£å†³é¥æ„Ÿè§†è§‰å®šä½æŒ‘æˆ˜ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤„ç†å¾®å°ç›®æ ‡å’Œå¤æ‚ç©ºé—´å…³ç³»æä¾›äº†æ–°æ–¹æ³•ï¼Œå¼ºè°ƒäº†æ•´åˆå¤šæ¨¡æ€æ„ŸçŸ¥ä¸ç©ºé—´æ¨ç†çš„é‡è¦æ€§ï¼Œä¸ºåœ°ç†ç©ºé—´äººå·¥æ™ºèƒ½åº”ç”¨å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.</p>
<h3 id="44-unicedit-10m-a-dataset-and-benchmark-breaking-the-scale-quality-barrier-via-unified-verification-for-reasoning-enriched-edits">[44] <a href="https://arxiv.org/abs/2512.02790">UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits</a></h3>
<p><em>Keming Ye, Zhipeng Huang, Canmiao Fu, Qingyang Liu, Jiani Cai, Zheqi Lv, Chen Li, Jing Lyu, Zhou Zhao, Shengyu Zhang</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†UnicEdit-10Mæ•°æ®é›†å’ŒUnicBenchåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡è½»é‡çº§æ•°æ®æµæ°´çº¿å’Œ7Bä¸“å®¶æ¨¡å‹Qwen-Verifyè§£å†³å¤šæ¨¡æ€å›¾åƒç¼–è¾‘ä¸­æ•°æ®è´¨é‡ä¸è§„æ¨¡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œä¸ºå¼€æºæ¨¡å‹æä¾›å¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®å’Œç»†ç²’åº¦è¯Šæ–­åŸºå‡†ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å›¾åƒç¼–è¾‘é¢†åŸŸå­˜åœ¨é—­æºä¸å¼€æºæ¨¡å‹æ€§èƒ½å·®è·æ‰©å¤§çš„é—®é¢˜ï¼Œä¸»è¦æºäºå¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºä»¥åŠç¼ºä¹èƒ½å¤Ÿå…¨é¢è¯Šæ–­æ¨¡å‹åœ¨å¤šæ ·åŒ–ç¼–è¾‘è¡Œä¸ºä¸­å¼±ç‚¹çš„åŸºå‡†æµ‹è¯•ã€‚ç°æœ‰æ•°æ®æ„å»ºæ–¹æ³•é¢ä¸´è§„æ¨¡ä¸è´¨é‡çš„æƒè¡¡ï¼šäººå·¥æ ‡æ³¨è´¨é‡é«˜ä½†ä¸å¯æ‰©å±•ï¼Œè€Œè‡ªåŠ¨åŒ–æµæ°´çº¿åˆ™å­˜åœ¨é”™è¯¯ä¼ æ’­å’Œå™ªå£°é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ä¸€ç§è½»é‡çº§æ•°æ®æµæ°´çº¿ï¼Œç”¨ç«¯åˆ°ç«¯æ¨¡å‹æ›¿ä»£å¤šå·¥å…·é“¾ï¼Œå¹¶å¼•å…¥ç»Ÿä¸€çš„åæœŸéªŒè¯é˜¶æ®µã€‚ä¸ºå®ç°å¯æ‰©å±•çš„è´¨é‡æ§åˆ¶ï¼Œè®­ç»ƒäº†ä¸€ä¸ª7BåŒä»»åŠ¡ä¸“å®¶æ¨¡å‹Qwen-Verifyï¼Œç”¨äºé«˜æ•ˆå¤±è´¥æ£€æµ‹å’ŒæŒ‡ä»¤é‡è¿°ã€‚è¯¥æµæ°´çº¿äº§ç”Ÿäº†UnicEdit-10Mæ•°æ®é›†ï¼Œæ¶µç›–å¤šæ ·åŒ–çš„åŸºç¡€ä¸å¤æ‚ç¼–è¾‘ä»»åŠ¡ã€‚åŒæ—¶æå‡ºäº†UnicBenchåŸºå‡†æµ‹è¯•ï¼Œæ‰©å±•äº†åŸºç¡€ç¼–è¾‘è¯„ä¼°ï¼Œæ˜ç¡®è¯„ä¼°ç©ºé—´å’ŒçŸ¥è¯†é©±åŠ¨çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†éç¼–è¾‘ä¸€è‡´æ€§å’Œæ¨ç†å‡†ç¡®æ€§ç­‰æ–°é¢–æŒ‡æ ‡ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡è¯¥æ•°æ®æµæ°´çº¿æ„å»ºäº†UnicEdit-10Mæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1000ä¸‡æ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¦†ç›–äº†å¤šæ ·åŒ–çš„åŸºç¡€ä¸å¤æ‚ç¼–è¾‘ä»»åŠ¡ã€‚åœ¨UnicBenchåŸºå‡†æµ‹è¯•ä¸Šå¯¹ä¸»æµæ¨¡å‹çš„åˆ†ææ­ç¤ºäº†å®ƒä»¬çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬åœ¨ç©ºé—´æ¨ç†å’ŒçŸ¥è¯†é©±åŠ¨ç¼–è¾‘æ–¹é¢çš„ä¸è¶³ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ˜ç¡®æ–¹å‘ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡åˆ›æ–°çš„æ•°æ®æ„å»ºæ–¹æ³•å’Œå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œä¸ºå¤šæ¨¡æ€å›¾åƒç¼–è¾‘é¢†åŸŸæä¾›äº†é‡è¦çš„åŸºç¡€è®¾æ–½ã€‚UnicEdit-10Mæ•°æ®é›†è§£å†³äº†è®­ç»ƒæ•°æ®è§„æ¨¡ä¸è´¨é‡çš„çŸ›ç›¾ï¼Œè€ŒUnicBenchåŸºå‡†æµ‹è¯•åˆ™æä¾›äº†ç»†ç²’åº¦çš„æ¨¡å‹è¯Šæ–­èƒ½åŠ›ï¼Œæœ‰åŠ©äºç¼©å°å¼€æºä¸é—­æºæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†æ¸…æ™°æŒ‡å¼•ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, \textbf{Qwen-Verify}, for efficient failure detection and instruction recaptioning. This pipeline yields \textbf{UnicEdit-10M}, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose \textbf{UnicBench}, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including \textit{Non-edit Consistency} and \textit{Reasoning Accuracy}. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.</p>
<h3 id="45-hud-hierarchical-uncertainty-aware-disambiguation-network-for-composed-video-retrieval">[45] <a href="https://arxiv.org/abs/2512.02792">HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval</a></h3>
<p><em>Zhiwei Chen, Yupeng Hu, Zixu Li, Zhiheng Fu, Haokun Wen, Weili Guan</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†å±‚ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¶ˆæ­§ç½‘ç»œï¼ˆHUDï¼‰ï¼Œç”¨äºè§£å†³ç»„åˆè§†é¢‘æ£€ç´¢ä»»åŠ¡ä¸­è§†é¢‘ä¸æ–‡æœ¬æ¨¡æ€ä¿¡æ¯å¯†åº¦å·®å¼‚å¸¦æ¥çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨æ¨¡æ€é—´ä¿¡æ¯å¯†åº¦å·®å¼‚æ¥å¢å¼ºå¤šæ¨¡æ€æŸ¥è¯¢ç†è§£ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç»„åˆè§†é¢‘æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œå¤šæ¨¡æ€æŸ¥è¯¢åŒ…å«å‚è€ƒè§†é¢‘å’Œä¿®æ”¹æ–‡æœ¬ï¼Œä½†å…ˆå‰ç ”ç©¶å¿½è§†äº†è§†é¢‘ä¸æ–‡æœ¬æ¨¡æ€é—´çš„ä¿¡æ¯å¯†åº¦å·®å¼‚ï¼Œè¿™å¯¼è‡´ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä¿®æ”¹ä¸»ä½“æŒ‡ä»£æ¨¡ç³Šæ€§å’Œæœ‰é™çš„ç»†èŠ‚è¯­ä¹‰å…³æ³¨ï¼Œä»è€Œé™ä½äº†CVRæ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„åˆ†å±‚ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¶ˆæ­§ç½‘ç»œåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ•´ä½“ä»£è¯æ¶ˆæ­§ã€åŸå­ä¸ç¡®å®šæ€§å»ºæ¨¡ä»¥åŠæ•´ä½“åˆ°åŸå­å¯¹é½ï¼Œé€šè¿‡æ•´ä½“è·¨æ¨¡æ€äº¤äº’åˆ©ç”¨é‡å è¯­ä¹‰ï¼Œå¹¶é€šè¿‡åŸå­çº§è·¨æ¨¡æ€äº¤äº’å®ç°ç»†ç²’åº¦è¯­ä¹‰å¯¹é½ï¼Œä»è€Œæœ‰æ•ˆè¿›è¡Œå¯¹è±¡æ¶ˆæ­§å¹¶å¢å¼ºå¯¹ç»†èŠ‚è¯­ä¹‰çš„å…³æ³¨ã€‚</p>
<p><strong>Result:</strong> HUDæ¡†æ¶åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šé’ˆå¯¹ç»„åˆè§†é¢‘æ£€ç´¢å’Œç»„åˆå›¾åƒæ£€ç´¢ä»»åŠ¡å‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯¥æ¡†æ¶ä¸ä»…é€‚ç”¨äºCVRä»»åŠ¡ï¼Œä¹Ÿé€‚ç”¨äºCIRä»»åŠ¡ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é¦–æ¬¡åˆ©ç”¨è§†é¢‘ä¸æ–‡æœ¬é—´çš„ä¿¡æ¯å¯†åº¦å·®å¼‚æ¥å¢å¼ºå¤šæ¨¡æ€æŸ¥è¯¢ç†è§£ï¼Œé€šè¿‡åˆ†å±‚ä¸ç¡®å®šæ€§å»ºæ¨¡å’Œè¯­ä¹‰å¯¹é½æœºåˆ¶è§£å†³äº†æ¨¡æ€é—´ä¿¡æ¯ä¸å¹³è¡¡é—®é¢˜ï¼Œä¸ºç»„åˆæ£€ç´¢ä»»åŠ¡æä¾›äº†æ–°çš„æŠ€æœ¯æ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†è·¨ä»»åŠ¡åº”ç”¨çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>Composed Video Retrieval (CVR) is a challenging video retrieval task that utilizes multi-modal queries, consisting of a reference video and modification text, to retrieve the desired target video. The core of this task lies in understanding the multi-modal composed query and achieving accurate composed feature learning. Within multi-modal queries, the video modality typically carries richer semantic content compared to the textual modality. However, previous works have largely overlooked the disparity in information density between these two modalities. This limitation can lead to two critical issues: 1) modification subject referring ambiguity and 2) limited detailed semantic focus, both of which degrade the performance of CVR models. To address the aforementioned issues, we propose a novel CVR framework, namely the Hierarchical Uncertainty-aware Disambiguation network (HUD). HUD is the first framework that leverages the disparity in information density between video and text to enhance multi-modal query understanding. It comprises three key components: (a) Holistic Pronoun Disambiguation, (b) Atomistic Uncertainty Modeling, and (c) Holistic-to-Atomistic Alignment. By exploiting overlapping semantics through holistic cross-modal interaction and fine-grained semantic alignment via atomistic-level cross-modal interaction, HUD enables effective object disambiguation and enhances the focus on detailed semantics, thereby achieving precise composed feature learning. Moreover, our proposed HUD is also applicable to the Composed Image Retrieval (CIR) task and achieves state-of-the-art performance across three benchmark datasets for both CVR and CIR tasks. The codes are available on https://zivchen-ty.github.io/HUD.github.io/.</p>
<h3 id="46-phycustom-towards-realistic-physical-customization-in-text-to-image-generation">[46] <a href="https://arxiv.org/abs/2512.02794">PhyCustom: Towards Realistic Physical Customization in Text-to-Image Generation</a></h3>
<p><em>Fan Wu, Cheng Chen, Zhoujie Fu, Jiacheng Wei, Yi Xu, Deheng Ye, Guosheng Lin</em></p>
<h4 id="tldr_45">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºPhyCustomæ¡†æ¶ï¼Œé€šè¿‡ä¸¤ç§æ–°é¢–çš„æ­£åˆ™åŒ–æŸå¤±å‡½æ•°æ¿€æ´»æ‰©æ•£æ¨¡å‹è¿›è¡Œç‰©ç†æ¦‚å¿µå®šåˆ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨ç‰©ç†å±æ€§å®šåˆ¶æ–¹é¢çš„ä¸è¶³ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_45">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°å›¾åƒå®šåˆ¶æ–¹æ³•åœ¨ç†è§£å…·ä½“æ¦‚å¿µï¼ˆå¦‚é£æ ¼å’Œå½¢çŠ¶ï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨ç‰©ç†æ¦‚å¿µçš„å®šåˆ¶æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚æ ¸å¿ƒé™åˆ¶åœ¨äºè®­ç»ƒè¿‡ç¨‹ä¸­ç¼ºä¹å¯¹ç‰©ç†çŸ¥è¯†çš„æ˜¾å¼å¼•å…¥ï¼Œå³ä½¿è¾“å…¥æç¤ºä¸­åŒ…å«ç‰©ç†ç›¸å…³è¯æ±‡ï¼Œç°æœ‰æ–¹æ³•ä¹Ÿæ— æ³•å‡†ç¡®åæ˜ ç›¸åº”çš„ç‰©ç†å±æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºPhyCustomå¾®è°ƒæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ç§æ–°é¢–çš„æ­£åˆ™åŒ–æŸå¤±å‡½æ•°ï¼šç­‰è·æŸå¤±æ—¨åœ¨æ¿€æ´»æ‰©æ•£æ¨¡å‹å­¦ä¹ ç‰©ç†æ¦‚å¿µï¼Œè€Œè§£è€¦æŸå¤±åˆ™å¸®åŠ©æ¶ˆé™¤ç‹¬ç«‹æ¦‚å¿µçš„æ··åˆå­¦ä¹ ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜¾å¼å¼•å…¥ç‰©ç†çŸ¥è¯†æ¥å¢å¼ºæ‰©æ•£æ¨¡å‹å¯¹ç‰©ç†å±æ€§çš„ç†è§£å’Œæ§åˆ¶èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPhyCustomåœ¨ç‰©ç†æ¦‚å¿µå®šåˆ¶æ–¹é¢å®šé‡å’Œå®šæ€§å‡ä¼˜äºå…ˆå‰çš„æœ€å…ˆè¿›æ–¹æ³•å’Œæµè¡Œæ–¹æ³•ã€‚åŸºå‡†æµ‹è¯•ç»“æœéªŒè¯äº†è¯¥æ¡†æ¶åœ¨å‡†ç¡®åæ˜ ç‰©ç†å±æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç‰©ç†ç›¸å…³æç¤ºæ—¶è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨æ‰©æ•£æ¨¡å‹ä¸­æ˜¾å¼å¼•å…¥ç‰©ç†çŸ¥è¯†çš„é‡è¦æ€§ï¼Œä¸ºç‰©ç†æ¦‚å¿µå®šåˆ¶æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚PhyCustomæ¡†æ¶çš„æˆåŠŸè¡¨æ˜ï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„æ­£åˆ™åŒ–æŸå¤±å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹å¯¹ç‰©ç†å±æ€§çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä¸ºæœªæ¥åœ¨ç‰©ç†æ„ŸçŸ¥å›¾åƒç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_45">ğŸ“„ Abstract</h4>
<p>Recent diffusion-based text-to-image customization methods have achieved significant success in understanding concrete concepts to control generation processes, such as styles and shapes. However, few efforts dive into the realistic yet challenging customization of physical concepts. The core limitation of current methods arises from the absence of explicitly introducing physical knowledge during training. Even when physics-related words appear in the input text prompts, our experiments consistently demonstrate that these methods fail to accurately reflect the corresponding physical properties in the generated results. In this paper, we propose PhyCustom, a fine-tuning framework comprising two novel regularization losses to activate diffusion model to perform physical customization. Specifically, the proposed isometric loss aims at activating diffusion models to learn physical concepts while decouple loss helps to eliminate the mixture learning of independent concepts. Experiments are conducted on a diverse dataset and our benchmark results demonstrate that PhyCustom outperforms previous state-of-the-art and popular methods in terms of physical customization quantitatively and qualitatively.</p>
<h3 id="47-action-anticipation-at-a-glimpse-to-what-extent-can-multimodal-cues-replace-video">[47] <a href="https://arxiv.org/abs/2512.02846">Action Anticipation at a Glimpse: To What Extent Can Multimodal Cues Replace Video?</a></h3>
<p><em>Manuel Benavent-Lledo, Konstantinos Bacharidis, Victoria Manousaki, Konstantinos Papoutsakis, Antonis Argyros, Jose Garcia-Rodriguez</em></p>
<h4 id="tldr_46">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†AAGæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå•å¸§RGBç‰¹å¾ã€æ·±åº¦çº¿ç´¢å’Œå…ˆéªŒåŠ¨ä½œä¿¡æ¯æ¥å®ç°åŠ¨ä½œé¢„æµ‹ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•™å­¦æ´»åŠ¨ä¸­æ•°æ®é›†ä¸Šèƒ½å¤Ÿä¸åŸºäºè§†é¢‘æ—¶åºèšåˆçš„æ–¹æ³•ç«äº‰ï¼Œè¯æ˜äº†å•å¸§å¤šæ¨¡æ€åŠ¨ä½œé¢„æµ‹çš„å¯è¡Œæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_46">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»ŸåŠ¨ä½œé¢„æµ‹æ–¹æ³•ä¾èµ–ä»è§†é¢‘ä¸­æå–å’Œèšåˆæ—¶åºä¿¡æ¯ï¼Œä½†äººç±»å¾€å¾€é€šè¿‡è§‚å¯Ÿåœºæ™¯ä¸­çš„å•ä¸ªç¬é—´å¹¶ç»“åˆè¶³å¤Ÿä¸Šä¸‹æ–‡å°±èƒ½é¢„æµ‹å³å°†å‘ç”Ÿçš„åŠ¨ä½œï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢è§†é¢‘èšåˆæ˜¯å¦å¯ä»¥è¢«æ›¿ä»£æ¨¡æ€æ‰€å–ä»£ï¼Œå®ç°åŸºäºå•å¸§çš„åŠ¨ä½œé¢„æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†AAGæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆå•å¸§RGBç‰¹å¾å’Œæ·±åº¦çº¿ç´¢ä»¥å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶èå…¥å…ˆéªŒåŠ¨ä½œä¿¡æ¯æä¾›é•¿æœŸä¸Šä¸‹æ–‡ï¼Œè¿™äº›ä¸Šä¸‹æ–‡ä¿¡æ¯é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æ‘˜è¦æˆ–å•å¸§åŠ¨ä½œè¯†åˆ«å™¨çš„é¢„æµ‹ç»“æœè·å¾—ï¼Œå½¢æˆå¤šæ¨¡æ€å•å¸§åŠ¨ä½œé¢„æµ‹æ¡†æ¶ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºAAGçš„å¤šæ¨¡æ€å•å¸§åŠ¨ä½œé¢„æµ‹æ–¹æ³•åœ¨IKEA-ASMã€Meccanoå’ŒAssembly101ä¸‰ä¸ªæ•™å­¦æ´»åŠ¨æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿä¸åŸºäºæ—¶åºèšåˆçš„è§†é¢‘åŸºçº¿æ–¹æ³•å’Œå½“å‰æœ€å…ˆè¿›æ–¹æ³•ç«äº‰ï¼Œè¯æ˜äº†å•å¸§å¤šæ¨¡æ€æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç»“åˆé€‚å½“çš„ç©ºé—´çº¿ç´¢å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå•å¸§å¤šæ¨¡æ€æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ›¿ä»£ä¼ ç»Ÿçš„è§†é¢‘æ—¶åºèšåˆæ–¹æ³•è¿›è¡ŒåŠ¨ä½œé¢„æµ‹ï¼Œè¿™ä¸ºåŠ¨ä½œç†è§£ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ï¼Œå±•ç¤ºäº†å¤šæ¨¡æ€èåˆåœ¨å‡å°‘è®¡ç®—éœ€æ±‚å’Œæé«˜æ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_46">ğŸ“„ Abstract</h4>
<p>Anticipating actions before they occur is a core challenge in action understanding research. While conventional methods rely on extracting and aggregating temporal information from videos, as humans we can often predict upcoming actions by observing a single moment from a scene, when given sufficient context. Can a model achieve this competence? The short answer is yes, although its effectiveness depends on the complexity of the task. In this work, we investigate to what extent video aggregation can be replaced with alternative modalities. To this end, based on recent advances in visual feature extraction and language-based reasoning, we introduce AAG, a method for Action Anticipation at a Glimpse. AAG combines RGB features with depth cues from a single frame for enhanced spatial reasoning, and incorporates prior action information to provide long-term context. This context is obtained either through textual summaries from Vision-Language Models, or from predictions generated by a single-frame action recognizer. Our results demonstrate that multimodal single-frame action anticipation using AAG can perform competitively compared to both temporally aggregated video baselines and state-of-the-art methods across three instructional activity datasets: IKEA-ASM, Meccano, and Assembly101.</p>
<h3 id="48-miccai-stsr-2025-challenge-semi-supervised-teeth-and-pulp-segmentation-and-cbct-ios-registration">[48] <a href="https://arxiv.org/abs/2512.02867">MICCAI STSR 2025 Challenge: Semi-Supervised Teeth and Pulp Segmentation and CBCT-IOS Registration</a></h3>
<p><em>Yaqi Wang, Zhi Li, Chengyu Wu, Jun Liu, Yifan Zhang, Jialuo Chen, Jiaxue Ni, Qian Luo, Jin Liu, Can Han, Changkai Ji, Zhi Qin Tan, Ajo Babu George, Liangyu Chen, Qianni Zhang, Dahong Qian, Shuai Wang, Huiyu Zhou</em></p>
<h4 id="tldr_47">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é€šè¿‡ç»„ç»‡STSR 2025æŒ‘æˆ˜èµ›ï¼Œä¸ºæ•°å­—ç‰™ç§‘ä¸­çš„CBCTå’ŒIOSæ•°æ®å»ºç«‹äº†åŠç›‘ç£å­¦ä¹ åŸºå‡†ï¼Œè§£å†³äº†æ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¹¶æ¨åŠ¨äº†ç‰™é½¿åˆ†å‰²å’Œè·¨æ¨¡æ€é…å‡†çš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆå‘å±•ã€‚</p>
<hr />
<h4 id="detailed-summary_47">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ•°å­—ç‰™ç§‘ä¸­é”¥æŸè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCBCTï¼‰å’Œå£å†…æ‰«æï¼ˆIOSï¼‰å¯¹äºè‡ªåŠ¨åŒ–è¯Šæ–­å’Œæ²»ç–—è§„åˆ’è‡³å…³é‡è¦ï¼Œä½†æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºä¸¥é‡é™åˆ¶äº†ç‰™é½¿å’Œç‰™é«“ç®¡åˆ†å‰²ä»¥åŠè·¨æ¨¡æ€é…å‡†ç­‰ä»»åŠ¡çš„æ·±åº¦å­¦ä¹ è§£å†³æ–¹æ¡ˆå‘å±•ï¼Œå› æ­¤éœ€è¦å»ºç«‹åŠç›‘ç£å­¦ä¹ åŸºå‡†æ¥æ¨åŠ¨è¯¥é¢†åŸŸè¿›å±•ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶ç»„ç»‡äº†åŒ…å«ä¸¤ä¸ªä»»åŠ¡çš„STSR 2025æŒ‘æˆ˜èµ›ï¼šCBCTä¸­ç‰™é½¿å’Œç‰™é«“ç®¡çš„åŠç›‘ç£åˆ†å‰²ï¼Œä»¥åŠCBCTä¸IOSçš„åŠç›‘ç£åˆšæ€§é…å‡†ã€‚æä¾›äº†60ä¸ªæ ‡æ³¨å’Œ640ä¸ªæœªæ ‡æ³¨çš„IOSæ ·æœ¬ï¼Œä»¥åŠ30ä¸ªæ ‡æ³¨å’Œ250ä¸ªæœªæ ‡æ³¨çš„CBCTæ‰«ææ•°æ®ã€‚é¢†å…ˆçš„è§£å†³æ–¹æ¡ˆé‡‡ç”¨äº†nnU-Netå’ŒMamba-likeçŠ¶æ€ç©ºé—´æ¨¡å‹ç»“åˆä¼ªæ ‡ç­¾å’Œä¸€è‡´æ€§æ­£åˆ™åŒ–è¿›è¡Œåˆ†å‰²ï¼Œè€Œé…å‡†ä»»åŠ¡åˆ™ç»“åˆäº†PointNetLKä¸å¯å¾®åˆ†SVDä»¥åŠå‡ ä½•å¢å¼ºæ¥å¤„ç†æ¨¡æ€å·®å¼‚ã€‚</p>
<p><strong>Result:</strong> æŒ‘æˆ˜èµ›å¸å¼•äº†å¹¿æ³›ç¤¾åŒºå‚ä¸ï¼Œæœ€ä½³åˆ†å‰²æ–¹æ³•åœ¨éšè—æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†0.967çš„Diceåˆ†æ•°å’Œ0.738çš„å®ä¾‹äº²å’ŒåŠ›ã€‚é…å‡†ä»»åŠ¡ä¸­ï¼Œæ··åˆç¥ç»-ç»å…¸ä¼˜åŒ–æ–¹æ³•åœ¨æœ‰é™æ ‡æ³¨ä¸‹å®ç°äº†ç²¾ç¡®å¯¹é½ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç å·²å…¬å¼€ï¼Œç¡®ä¿äº†ç ”ç©¶çš„å¯é‡å¤æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŠç›‘ç£å­¦ä¹ åœ¨æ•°å­—ç‰™ç§‘ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ ‡æ³¨ç¨€ç¼ºçš„åŒ»å­¦å›¾åƒåˆ†æä»»åŠ¡ã€‚æŒ‘æˆ˜èµ›çš„æˆåŠŸç»„ç»‡ä¸ºç¤¾åŒºæä¾›äº†æ ‡å‡†åŒ–åŸºå‡†å’Œå¼€æºè§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†CBCTå’ŒIOSæ•°æ®å¤„ç†æŠ€æœ¯çš„å‘å±•ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®é›†å’Œæ–¹æ³•å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract_47">ğŸ“„ Abstract</h4>
<p>Cone-Beam Computed Tomography (CBCT) and Intraoral Scanning (IOS) are essential for digital dentistry, but annotated data scarcity limits automated solutions for pulp canal segmentation and cross-modal registration. To benchmark semi-supervised learning (SSL) in this domain, we organized the STSR 2025 Challenge at MICCAI 2025, featuring two tasks: (1) semi-supervised segmentation of teeth and pulp canals in CBCT, and (2) semi-supervised rigid registration of CBCT and IOS. We provided 60 labeled and 640 unlabeled IOS samples, plus 30 labeled and 250 unlabeled CBCT scans with varying resolutions and fields of view. The challenge attracted strong community participation, with top teams submitting open-source deep learning-based SSL solutions. For segmentation, leading methods used nnU-Net and Mamba-like State Space Models with pseudo-labeling and consistency regularization, achieving a Dice score of 0.967 and Instance Affinity of 0.738 on the hidden test set. For registration, effective approaches combined PointNetLK with differentiable SVD and geometric augmentation to handle modality gaps; hybrid neural-classical refinement enabled accurate alignment despite limited labels. All data and code are publicly available at https://github.com/ricoleehduu/STS-Challenge-2025 to ensure reproducibility.</p>
<h3 id="49-mindgpt-4ov-an-enhanced-mllm-via-a-multi-stage-post-training-paradigm">[49] <a href="https://arxiv.org/abs/2512.02895">MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm</a></h3>
<p><em>Wei Chen, Chaoqun Du, Feng Gu, Wei He, Qizhen Li, Zide Liu, Xuhao Pan, Chang Ren, Xudong Rao, Chenfeng Wang, Tao Wei, Chengjun Yu, Pengfei Yu, Yufei Zheng, Chunpeng Zhou, Pan Zhou, Xuhan Zhu</em></p>
<h4 id="tldr_48">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMindGPT-4ovï¼Œä¸€ç§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ¶µç›–æ•°æ®ç”Ÿäº§ã€æ¨¡å‹è®­ç»ƒå’Œé«˜æ•ˆéƒ¨ç½²çš„é€šç”¨åè®­ç»ƒèŒƒå¼ï¼Œåœ¨ä½æˆæœ¬ä¸‹å®ç°å¤šä¸ªåŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œæœ‰æ•ˆå¢å¼ºäº†MLLMsçš„åŸºç¡€èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_48">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é¢ä¸´é«˜è´¨é‡è·¨é¢†åŸŸæ•°æ®ç”Ÿæˆå›°éš¾ã€é¢†åŸŸç‰¹å®šçŸ¥è¯†ä¸é€šç”¨èƒ½åŠ›å¹³è¡¡ä¸è¶³ã€æ¨ç†èƒ½åŠ›ä¸å¤šç›®æ ‡ä¼˜åŒ–ååŒä¼˜åŒ–æŒ‘æˆ˜ä»¥åŠè®­ç»ƒéƒ¨ç½²æ•ˆç‡ä¸æˆæœ¬é™åˆ¶ç­‰é—®é¢˜ï¼Œéœ€è¦ä¸€ç§ç³»ç»Ÿæ€§çš„åè®­ç»ƒèŒƒå¼æ¥æå‡æ¨¡å‹æ€§èƒ½å¹¶é™ä½åº”ç”¨é—¨æ§›ã€‚</p>
<p><strong>Method:</strong> æå‡ºä¸‰ä¸ªå…³é”®æŠ€æœ¯åˆ›æ–°ï¼šåŸºäºä¿¡æ¯å¯†åº¦çš„æ•°æ®ç”Ÿæˆæ–¹æ¡ˆç»“åˆåŒç»´åº¦æ ‘çŠ¶æ ‡ç­¾ç³»ç»Ÿå®ç°é«˜è´¨é‡è·¨é¢†åŸŸæ•°æ®è‡ªåŠ¨ç”Ÿæˆï¼›åä½œè¯¾ç¨‹ç›‘ç£å¾®è°ƒæ–¹æ³•å¹³è¡¡é¢†åŸŸçŸ¥è¯†æ³¨å…¥ä¸é€šç”¨èƒ½åŠ›ä¿æŒï¼›æ··åˆå¼ºåŒ–å­¦ä¹ èŒƒå¼å¢å¼ºæ¨ç†èƒ½åŠ›åŒæ—¶ä¼˜åŒ–å¤šæ ·æ€§æ¢ç´¢ã€å¤šæ¨¡æ€æ„ŸçŸ¥ä¿æŒå’Œå“åº”ç®€æ´æ€§ç­‰å¤šç›®æ ‡ã€‚æ­¤å¤–å®æ–½5Då¹¶è¡Œè®­ç»ƒã€ç®—å­ä¼˜åŒ–å’Œæ¨ç†é‡åŒ–ç­‰åŸºç¡€è®¾æ–½ä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> MindGPT-4ovåœ¨MMBenchã€MMStarã€MathVisionå’ŒMathVistaç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ï¼ŒåŒæ—¶åœ¨å‚ç›´é¢†åŸŸä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„ç”¨æˆ·ä½“éªŒï¼Œå®ç°äº†ä»å­¦æœ¯ç ”ç©¶åˆ°å·¥ä¸šéƒ¨ç½²çš„æ— ç¼è¿‡æ¸¡ï¼Œå¹¶åœ¨ä½æˆæœ¬ä¸‹è¾¾åˆ°é«˜æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªé€‚ç”¨äºå¹¿æ³›MLLMsçš„é€šç”¨åè®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡ç³»ç»ŸåŒ–çš„æ•°æ®æ„é€ ã€è®­ç»ƒç­–ç•¥å’Œéƒ¨ç½²ä¼˜åŒ–æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€æ¨¡å‹çš„æ€§èƒ½å’Œå®ç”¨æ€§ã€‚åŸºäºQwen3-VLçš„å˜ä½“æ¨¡å‹æƒé‡ã€æ•°æ®é›†å’Œä»£ç å°†å¼€æºï¼Œæ”¯æŒç¤¾åŒºMLLMså‘å±•ï¼Œä¸ºå¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„å®é™…åº”ç”¨æä¾›äº†å¯è¡Œè·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_48">ğŸ“„ Abstract</h4>
<p>We present MindGPT-4ov, a multimodal large language model (MLLM) that introduces a general post-training paradigm spanning data production, model training, and efficient deployment. It achieves state-of-the-art performance across multiple benchmarks at low cost, effectively enhancing the foundational capabilities of MLLMs and the generalization ability. Focusing on data construction, supervised fine-tuning strategies, and multimodal reinforcement learning methods, this work proposes three key innovations: (1) An information density-based data generation scheme, integrated with a dual-dimensional tree-structured label system, enabling automated generation of high-quality cross-domain data. (2) A collaborative curriculum supervised fine-tuning approach that balances the injection of domain-specific knowledge with the preservation of general capabilities. (3) A hybrid reinforcement learning paradigm that enhances reasoning ability while simultaneously addressing multi-objective optimization such as diversity exploration, maintenance of multimodal perception, and response conciseness. Moreover, we implement a series of infrastructure optimizations, such as 5D parallel training, operator optimization, and inference quantization to enhance training and inference efficiency while reducing the cost of domain adaptation. Experimental results demonstrate that the MindGPT-4ov model outperforms state-of-the-art models on benchmarks such as MMBench, MMStar, MathVision, and MathVista. In addition, MindGPT-4ov also demonstrates superior user experience in vertical domain tasks, enabling a seamless transition from academic research to industrial deployment. MindGPT-4ov provides a general post-training paradigm applicable to a wide range of MLLMs. The model weights, datasets, and code for the Qwen3-VL-based variants will be recently open-sourced to support the community's development of MLLMs.</p>
<h3 id="50-layout-anything-one-transformer-for-universal-room-layout-estimation">[50] <a href="https://arxiv.org/abs/2512.02952">Layout Anything: One Transformer for Universal Room Layout Estimation</a></h3>
<p><em>Md Sohag Mia, Muhammad Abdullah Adnan</em></p>
<h4 id="tldr_49">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºLayout Anythingï¼Œä¸€ç§åŸºäºTransformerçš„å®¤å†…å¸ƒå±€ä¼°è®¡æ¡†æ¶ï¼Œé€šè¿‡å°†OneFormerçš„é€šç”¨åˆ†å‰²æ¶æ„é€‚é…åˆ°å‡ ä½•ç»“æ„é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„å¸ƒå±€ä¼°è®¡ï¼Œæ¶ˆé™¤äº†å¤æ‚çš„åå¤„ç†æµç¨‹å¹¶å®ç°äº†114msçš„é«˜é€Ÿæ¨ç†ã€‚</p>
<hr />
<h4 id="detailed-summary_49">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å®¤å†…å¸ƒå±€ä¼°è®¡ä¸­å¤æ‚åå¤„ç†æµç¨‹å’Œå‡ ä½•çº¦æŸæ•´åˆä¸è¶³çš„é—®é¢˜ï¼Œé€šè¿‡å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿç›´æ¥é¢„æµ‹å‡ ä½•ç»“æ„å¹¶ä¿æŒæ›¼å“ˆé¡¿ä¸–ç•Œçº¦æŸçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œä»¥æå‡å¸ƒå±€ä¼°è®¡çš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åŸºäºOneFormerçš„é€šç”¨åˆ†å‰²æ¶æ„è¿›è¡Œé€‚é…ï¼Œé›†æˆäº†ä»»åŠ¡æ¡ä»¶æŸ¥è¯¢å’Œå¯¹æ¯”å­¦ä¹ ï¼Œå¹¶å¼•å…¥äº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šå¸ƒå±€é€€åŒ–ç­–ç•¥é€šè¿‡æ‹“æ‰‘æ„ŸçŸ¥å˜æ¢å¢å¼ºè®­ç»ƒæ•°æ®åŒæ—¶ä¿æŒæ›¼å“ˆé¡¿ä¸–ç•Œçº¦æŸï¼›å¯å¾®åˆ†å‡ ä½•æŸå¤±ç›´æ¥åœ¨è®­ç»ƒä¸­å¼ºåˆ¶å¹³é¢ä¸€è‡´æ€§å’Œé”åˆ©è¾¹ç•Œé¢„æµ‹ï¼Œå½¢æˆç«¯åˆ°ç«¯çš„ç»Ÿä¸€æ¡†æ¶ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼šåœ¨LSUNæ•°æ®é›†ä¸Šåƒç´ è¯¯å·®ä¸º5.43%ï¼Œè§’ç‚¹è¯¯å·®ä¸º4.02%ï¼›åœ¨Hedauæ•°æ®é›†ä¸Šåƒç´ è¯¯å·®ä¸º7.04%ï¼Œè§’ç‚¹è¯¯å·®ä¸º5.17%ï¼›åœ¨Matterport3D-Layoutæ•°æ®é›†ä¸Šåƒç´ è¯¯å·®ä¸º4.03%ï¼Œè§’ç‚¹è¯¯å·®ä¸º3.15%ï¼ŒåŒæ—¶å®ç°äº†114msçš„é«˜é€Ÿæ¨ç†ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å°†é€šç”¨åˆ†å‰²æ¶æ„ä¸å‡ ä½•çº¦æŸç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºå®¤å†…å¸ƒå±€ä¼°è®¡æä¾›äº†å…¼å…·å‡ ä½•æ„ŸçŸ¥å’Œè®¡ç®—æ•ˆç‡çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚ç”¨äºå¢å¼ºç°å®åº”ç”¨å’Œå¤§è§„æ¨¡3Dåœºæ™¯é‡å»ºä»»åŠ¡ï¼Œå±•ç¤ºäº†ç«¯åˆ°ç«¯æ¡†æ¶åœ¨å‡ ä½•ç»“æ„é¢„æµ‹ä¸­çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_49">ğŸ“„ Abstract</h4>
<p>We present Layout Anything, a transformer-based framework for indoor layout estimation that adapts the OneFormer's universal segmentation architecture to geometric structure prediction. Our approach integrates OneFormer's task-conditioned queries and contrastive learning with two key modules: (1) a layout degeneration strategy that augments training data while preserving Manhattan-world constraints through topology-aware transformations, and (2) differentiable geometric losses that directly enforce planar consistency and sharp boundary predictions during training. By unifying these components in an end-to-end framework, the model eliminates complex post-processing pipelines while achieving high-speed inference at 114ms. Extensive experiments demonstrate state-of-the-art performance across standard benchmarks, with pixel error (PE) of 5.43% and corner error (CE) of 4.02% on the LSUN, PE of 7.04% (CE 5.17%) on the Hedau and PE of 4.03% (CE 3.15%) on the Matterport3D-Layout datasets. The framework's combination of geometric awareness and computational efficiency makes it particularly suitable for augmented reality applications and large-scale 3D scene reconstruction tasks.</p>
<h3 id="51-bevdilation-lidar-centric-multi-modal-fusion-for-3d-object-detection">[51] <a href="https://arxiv.org/abs/2512.02972">BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection</a></h3>
<p><em>Guowen Zhang, Chenhang He, Liyi Chen, Lei Zhang</em></p>
<h4 id="tldr_50">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†BEVDilationï¼Œä¸€ç§ä»¥LiDARä¸ºä¸­å¿ƒçš„BEVèåˆæ¡†æ¶ï¼Œé€šè¿‡å°†å›¾åƒBEVç‰¹å¾ä½œä¸ºéšå¼å¼•å¯¼è€Œéç®€å•æ‹¼æ¥ï¼Œæœ‰æ•ˆç¼“è§£äº†å›¾åƒæ·±åº¦ä¼°è®¡è¯¯å·®å¯¼è‡´çš„ç©ºé—´é”™ä½é—®é¢˜ï¼Œå¹¶åœ¨nuScenesåŸºå‡†ä¸Šå®ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_50">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰LiDARä¸ç›¸æœºåœ¨BEVè¡¨ç¤ºä¸­çš„èåˆæ–¹æ³•ç”±äºä¼ æ„Ÿå™¨é—´å‡ ä½•ç²¾åº¦çš„æ ¹æœ¬å·®å¼‚ï¼Œå¾€å¾€å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯å›¾åƒæ·±åº¦ä¼°è®¡è¯¯å·®å¼•èµ·çš„ç©ºé—´é”™ä½é—®é¢˜ï¼Œä»¥åŠç‚¹äº‘å›ºæœ‰çš„ç¨€ç–æ€§å’Œè¯­ä¹‰å±€é™æ€§éœ€è¦å¾—åˆ°æœ‰æ•ˆè§£å†³ã€‚</p>
<p><strong>Method:</strong> BEVDilationé‡‡ç”¨ä»¥LiDARä¸ºä¸­å¿ƒçš„èåˆç­–ç•¥ï¼Œæå‡ºäº†ç¨€ç–ä½“ç´ æ‰©å¼ å—é€šè¿‡å›¾åƒå…ˆéªŒå¯¹å‰æ™¯ä½“ç´ è¿›è¡Œç¨ å¯†åŒ–ä»¥ç¼“è§£ç‚¹äº‘ç¨€ç–æ€§ï¼Œå¹¶å¼•å…¥è¯­ä¹‰å¼•å¯¼çš„BEVæ‰©å¼ å—åˆ©ç”¨å›¾åƒè¯­ä¹‰å¼•å¯¼å’Œé•¿è·ç¦»ä¸Šä¸‹æ–‡æ•è·æ¥å¢å¼ºLiDARç‰¹å¾æ‰©æ•£å¤„ç†ã€‚</p>
<p><strong>Result:</strong> åœ¨nuScenesåŸºå‡†æµ‹è¯•ä¸­ï¼ŒBEVDilationå®ç°äº†ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„è®¡ç®—æ•ˆç‡ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œè¯¥LiDARä¸­å¿ƒç­–ç•¥ç›¸æ¯”æœ´ç´ èåˆæ–¹æ³•å¯¹æ·±åº¦å™ªå£°è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ä»¥LiDARä¸ºä¸­å¿ƒçš„èåˆç­–ç•¥åœ¨å¤„ç†å¤šä¼ æ„Ÿå™¨æ•°æ®æ—¶çš„ä¼˜è¶Šæ€§ï¼Œé€šè¿‡å°†å›¾åƒç‰¹å¾ä½œä¸ºéšå¼å¼•å¯¼è€Œéç›´æ¥æ‹¼æ¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£ç©ºé—´é”™ä½é—®é¢˜ï¼ŒåŒæ—¶åˆ©ç”¨å›¾åƒå…ˆéªŒå¼¥è¡¥ç‚¹äº‘çš„ç¨€ç–æ€§å’Œè¯­ä¹‰å±€é™æ€§ï¼Œä¸º3Dç›®æ ‡æ£€æµ‹ä¸­çš„ä¼ æ„Ÿå™¨èåˆæä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_50">ğŸ“„ Abstract</h4>
<p>Integrating LiDAR and camera information in the bird's eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.</p>
<h3 id="52-inex-hallucination-mitigation-via-introspection-and-cross-modal-multi-agent-collaboration">[52] <a href="https://arxiv.org/abs/2512.02981">InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration</a></h3>
<p><em>Zhongyu Yang, Yingfang Yuan, Xuanming Jiang, Baoyi An, Wei Pang</em></p>
<h4 id="tldr_51">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºInExï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡å†…çœæ¨ç†å’Œå¤–éƒ¨å¤šæ™ºèƒ½ä½“åä½œæ¥è‡ªä¸»ç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_51">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ä¸¥é‡é˜»ç¢äº†å…¶å¯é æ€§å‘å±•ï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸ä¾èµ–äººå·¥å¹²é¢„æˆ–æœªèƒ½å……åˆ†åˆ©ç”¨æ™ºèƒ½ä½“è‡ªä¸»ç¼“è§£å¹»è§‰çš„èƒ½åŠ›ï¼Œéœ€è¦ä¸€ç§æ›´è‡ªä¸»çš„å¹»è§‰ç¼“è§£æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> å—äººç±»è®¤çŸ¥è¿‡ç¨‹å¯å‘ï¼Œæå‡ºInExæ¡†æ¶ï¼ŒåŒ…å«åŸºäºç†µçš„ä¸ç¡®å®šæ€§ä¼°è®¡å¼•å¯¼çš„å†…çœæ¨ç†ï¼Œä»¥åŠé€šè¿‡ç¼–è¾‘æ™ºèƒ½ä½“å’Œè‡ªåæ€æ™ºèƒ½ä½“è¿›è¡Œçš„å¤–éƒ¨è·¨æ¨¡æ€å¤šæ™ºèƒ½ä½“åä½œï¼Œä»¥è¿­ä»£éªŒè¯å’Œç²¾ç‚¼å“åº”ã€‚</p>
<p><strong>Result:</strong> åœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼ŒInExåœ¨é€šç”¨å’Œå¹»è§‰åŸºå‡†æµ‹è¯•ä¸ŠæŒç»­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†4%-27%çš„æ€§èƒ½æå‡ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡æ‹Ÿäººç±»è®¤çŸ¥è¿‡ç¨‹çš„å†…çœæ¨ç†ä¸å¤–éƒ¨éªŒè¯ç›¸ç»“åˆçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè‡ªä¸»ç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œä¸ºæ„å»ºæ›´å¯é çš„AIç³»ç»Ÿæä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_51">ğŸ“„ Abstract</h4>
<p>Hallucination remains a critical challenge in large language models (LLMs), hindering the development of reliable multimodal LLMs (MLLMs). Existing solutions often rely on human intervention or underutilize the agent's ability to autonomously mitigate hallucination. To address these limitations, we draw inspiration from how humans make reliable decisions in the real world. They begin with introspective reasoning to reduce uncertainty and form an initial judgment, then rely on external verification from diverse perspectives to reach a final decision. Motivated by this cognitive paradigm, we propose InEx, a training-free, multi-agent framework designed to autonomously mitigate hallucination. InEx introduces internal introspective reasoning, guided by entropy-based uncertainty estimation, to improve the reliability of the decision agent's reasoning process. The agent first generates a response, which is then iteratively verified and refined through external cross-modal multi-agent collaboration with the editing agent and self-reflection agents, further enhancing reliability and mitigating hallucination. Extensive experiments show that InEx consistently outperforms existing methods, achieving 4%-27% gains on general and hallucination benchmarks, and demonstrating strong robustness.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="53-from-imitation-to-discrimination-toward-a-generalized-curriculum-advantage-mechanism-enhancing-cross-domain-reasoning-tasks">[53] <a href="https://arxiv.org/abs/2512.02580">From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks</a></h3>
<p><em>Changpeng Yang, Jinyang Wu, Yuchen Liu, Shuai Zhang, Yang Li, Qiliang Liang, Hongzhen Wang, Shuai Nie, Jiaming Xu, Runyu Shi, Ying Huang, Guoquan Zhang</em></p>
<h4 id="tldr_52">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CAPOï¼ˆè¯¾ç¨‹ä¼˜åŠ¿ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œä¸€ç§åŸºäºä¼˜åŠ¿ä¿¡å·çš„è‡ªé€‚åº”è¯¾ç¨‹æœºåˆ¶ï¼Œé€šè¿‡åˆ†ç¦»æ­£è´Ÿä¼˜åŠ¿æ ·æœ¬æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒå¼ºåŒ–å­¦ä¹ æ•ˆæœï¼Œåœ¨æ•°å­¦æ¨ç†å’Œå¤šæ¨¡æ€GUIæ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†ç¨³å®šæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_52">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨åè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œé€šå¸¸ä¸åŠ åŒºåˆ†åœ°æ··åˆæ­£è´Ÿä¼˜åŠ¿ä¿¡å·ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ—©æœŸé˜¶æ®µï¼Œè¿™ç§æ··åˆå¯èƒ½å¯¼è‡´æ¨¡ç³Šçš„æŒ‡å¯¼ä¿¡å·å’Œæœ‰é™çš„æ€§èƒ½å¢ç›Šï¼Œé™åˆ¶äº†æ¨¡å‹æ¨ç†èƒ½åŠ›çš„è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†CAPOæ¡†æ¶ï¼Œé‡‡ç”¨åŸºäºä¼˜åŠ¿ä¿¡å·çš„è‡ªé€‚åº”è¯¾ç¨‹æœºåˆ¶ï¼Œé¦–å…ˆä»…ä½¿ç”¨æ­£ä¼˜åŠ¿æ ·æœ¬è¿›è¡Œæ¨¡ä»¿å­¦ä¹ ä»¥å»ºç«‹ç¨³å¥åŸºç¡€ï¼Œéšåé€æ­¥å¼•å…¥è´Ÿä¿¡å·ä»¥åŸ¹å…»åˆ¤åˆ«èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•å…¼å®¹å¤šç§ä¼˜åŒ–æ–¹æ³•åŒ…æ‹¬GRPOã€PPOã€RLOOå’ŒReinforce++ã€‚</p>
<p><strong>Result:</strong> åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼ŒCAPOæ–¹æ³•å®ç°äº†ç¨³å®šä¸”æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹¶è¿›ä¸€æ­¥æœ‰æ•ˆæ³›åŒ–åˆ°å¤šæ¨¡æ€å›¾å½¢ç”¨æˆ·ç•Œé¢æ¨ç†åœºæ™¯ï¼Œè¯æ˜äº†å…¶ä½œä¸ºé€šç”¨ä¼˜åŒ–æ¡†æ¶çš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡è¯¾ç¨‹åŒ–åˆ†ç¦»æ­£è´Ÿä¼˜åŠ¿ä¿¡å·å¯ä»¥æ˜¾è‘—æå‡å¼ºåŒ–å­¦ä¹ åè®­ç»ƒçš„æ•ˆæœï¼Œä¸ºå¤æ‚åœºæ™¯ä¸‹çš„æ¨¡å‹æ³›åŒ–æä¾›äº†æ–°æ€è·¯ï¼ŒCAPOæ¡†æ¶çš„å…¼å®¹æ€§ä½¿å…¶æˆä¸ºå¤šç§ä¼˜åŒ–æ–¹æ³•çš„é€šç”¨å¢å¼ºæ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_52">ğŸ“„ Abstract</h4>
<p>Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose <strong>CAPO</strong> (<strong>C</strong>urriculum <strong>A</strong>dvantage <strong>P</strong>olicy <strong>O</strong>ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.</p>
<h3 id="54-spoken-conversational-agents-with-large-language-models">[54] <a href="https://arxiv.org/abs/2512.02593">Spoken Conversational Agents with Large Language Models</a></h3>
<p><em>Chao-Han Huck Yang, Andreas Stolcke, Larry Heck</em></p>
<h4 id="tldr_53">ğŸ§© TL;DR</h4>
<p>æœ¬æ•™ç¨‹ç³»ç»Ÿæ¢³ç†äº†ä»çº§è”ASR/NLUç³»ç»Ÿåˆ°ç«¯åˆ°ç«¯è¯­éŸ³åŸç”Ÿå¤§è¯­è¨€æ¨¡å‹çš„å‘å±•è·¯å¾„ï¼Œæä¾›äº†ä»å·¥ä¸šçº§åŠ©æ‰‹åˆ°å¼€æ”¾åŸŸå’Œä»»åŠ¡å¯¼å‘ä»£ç†çš„å®ç”¨è·¯çº¿å›¾ï¼Œé‡ç‚¹å…³æ³¨è·¨æ¨¡æ€å¯¹é½ã€è”åˆè®­ç»ƒå’Œé²æ£’æ€§è¯„ä¼°ç­‰å…³é”®æŠ€æœ¯æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_53">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å£è¯­å¯¹è¯ä»£ç†æ­£åœ¨å‘è¯­éŸ³åŸç”Ÿå¤§è¯­è¨€æ¨¡å‹æ¼”è¿›ï¼Œä½†ç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹ä»ä¼ ç»Ÿçº§è”æ¶æ„åˆ°ç«¯åˆ°ç«¯ç³»ç»Ÿçš„ç³»ç»Ÿæ€§æŠ€æœ¯è·¯çº¿æ¢³ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨æ¨¡æ€å¯¹é½ã€è”åˆè¯­éŸ³æ–‡æœ¬è®­ç»ƒã€ä»¥åŠå®é™…éƒ¨ç½²ä¸­çš„é²æ£’æ€§è¯„ä¼°ç­‰æ–¹é¢å­˜åœ¨çŸ¥è¯†ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> æ•™ç¨‹ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹å‘éŸ³é¢‘é¢†åŸŸçš„é€‚é…æ–¹æ³•ï¼ŒåŒ…æ‹¬è·¨æ¨¡æ€å¯¹é½æŠ€æœ¯å’Œè”åˆè¯­éŸ³æ–‡æœ¬è®­ç»ƒç­–ç•¥ï¼›è¯¦ç»†æ¯”è¾ƒäº†çº§è”æ¶æ„ä¸ç«¯åˆ°ç«¯è®¾è®¡çš„ä¼˜åŠ£ï¼Œåˆ†æäº†åASRä¿®æ­£å’Œæµå¼å¤„ç†ç­‰å…³é”®æŠ€æœ¯é€‰æ‹©ï¼›åŒæ—¶å›é¡¾äº†ç›¸å…³æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä»¥åŠé’ˆå¯¹å£éŸ³å¤šæ ·æ€§çš„é²æ£’æ€§å¤„ç†æ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> æ•™ç¨‹æä¾›äº†å¯å¤ç°çš„åŸºçº¿ç³»ç»Ÿå’Œå®ç”¨çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œå»ºç«‹äº†ä»å·¥ä¸šçº§è¯­éŸ³åŠ©æ‰‹åˆ°å½“å‰å¼€æ”¾åŸŸå’Œä»»åŠ¡å¯¼å‘ä»£ç†çš„æŠ€æœ¯è¿æ¥æ¡†æ¶ï¼›ç³»ç»Ÿè¯„ä¼°äº†ä¸åŒè®¾è®¡é€‰æ‹©åœ¨æ€§èƒ½ã€å»¶è¿Ÿå’Œé²æ£’æ€§æ–¹é¢çš„æƒè¡¡ï¼Œä¸ºå®é™…ç³»ç»Ÿå¼€å‘æä¾›äº†æ˜ç¡®çš„å‚è€ƒæ ‡å‡†ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†è¯­éŸ³åŸç”Ÿå¤§è¯­è¨€æ¨¡å‹å‘å±•çš„ç³»ç»Ÿæ€§æŠ€æœ¯è·¯çº¿ï¼Œå¼ºè°ƒäº†éšç§ä¿æŠ¤ã€å®‰å…¨æ€§å’Œè¯„ä¼°æ–¹æ³•ç­‰å¼€æ”¾æ€§é—®é¢˜çš„é‡è¦æ€§ï¼›ä¸ºä»ä¸šè€…æä¾›äº†ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´çŸ¥è¯†ä½“ç³»ï¼ŒæŒ‡æ˜äº†æœªæ¥åœ¨è·¨æ¨¡æ€å­¦ä¹ ã€å®æ—¶å¤„ç†å’Œé²æ£’æ€§ä¼˜åŒ–ç­‰æ–¹é¢çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_53">ğŸ“„ Abstract</h4>
<p>Spoken conversational agents are converging toward voice-native LLMs. This tutorial distills the path from cascaded ASR/NLU to end-to-end, retrieval-and vision-grounded systems. We frame adaptation of text LLMs to audio, cross-modal alignment, and joint speech-text training; review datasets, metrics, and robustness across accents and compare design choices (cascaded vs. E2E, post-ASR correction, streaming). We link industrial assistants to current open-domain and task-oriented agents, highlight reproducible baselines, and outline open problems in privacy, safety, and evaluation. Attendees leave with practical recipes and a clear systems-level roadmap.</p>
<h3 id="55-emergent-bayesian-behaviour-and-optimal-cue-combination-in-llms">[55] <a href="https://arxiv.org/abs/2512.02719">Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs</a></h3>
<p><em>Julian Ma, Jun Wang, Zafeirios Fountas</em></p>
<h4 id="tldr_54">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåä¸ºBayesBenchçš„å¿ƒç†ç‰©ç†å­¦åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ— æ˜¾å¼è®­ç»ƒæˆ–æŒ‡ä»¤æƒ…å†µä¸‹çš„éšå¼è®¡ç®—ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯è´å¶æ–¯ä¸€è‡´çš„å¤šæ¨¡æ€çº¿ç´¢æ•´åˆèƒ½åŠ›ã€‚ç ”ç©¶å‘ç°æ¨¡å‹èƒ½åŠ›ä¸ç­–ç•¥ä¹‹é—´å­˜åœ¨å…³é”®åˆ†ç¦»ï¼Œå‡†ç¡®æ€§å¹¶ä¸èƒ½ä¿è¯é²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_54">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ˜¾å¼æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶éšå¼è®¡ç®—ç­–ç•¥ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚äººç±»åœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸­èƒ½å¤Ÿä½¿ç”¨æ¥è¿‘æœ€ä¼˜çš„è´å¶æ–¯ç­–ç•¥ç›´è§‚å¤„ç†å™ªå£°ä¿¡å·ï¼Œè€Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶LLMsæ˜¯å¦åœ¨æ²¡æœ‰æ˜¾å¼è®­ç»ƒæˆ–æŒ‡ä»¤çš„æƒ…å†µä¸‹è¡¨ç°å‡ºç±»ä¼¼è¡Œä¸ºï¼Œå®ç°æœ€ä¼˜çš„å¤šæ¨¡æ€æ•´åˆã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨å¿ƒç†ç‰©ç†å­¦èŒƒå¼ï¼Œé€šè¿‡ç³»ç»Ÿè¡Œä¸ºç ”ç©¶æ¨æ–­LLMsçš„è®¡ç®—åŸç†ã€‚å¼•å…¥äº†åä¸ºBayesBenchçš„è¡Œä¸ºåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å››ä¸ªåŸºäºæ–‡æœ¬å’Œå›¾åƒçš„å¹…åº¦ä¼°è®¡ä»»åŠ¡ï¼ˆé•¿åº¦ã€ä½ç½®ã€è·ç¦»å’ŒæŒç»­æ—¶é—´ï¼‰ï¼Œå¹¶è¯„ä¼°äº†ä¹ä¸ªä¸åŒçš„LLMsä¸äººç±»åˆ¤æ–­è¿›è¡Œæ ¡å‡†ã€‚é€šè¿‡æ§åˆ¶å™ªå£°ã€ä¸Šä¸‹æ–‡å’ŒæŒ‡ä»¤æç¤ºçš„æ¶ˆèå®éªŒï¼Œæµ‹é‡å¤šæ¨¡æ€çº¿ç´¢æ•´åˆä¸­çš„æ€§èƒ½ã€è¡Œä¸ºå’Œæ•ˆç‡ã€‚é™¤äº†å‡†ç¡®æ€§å’Œæ•ˆç‡æŒ‡æ ‡å¤–ï¼Œè¿˜å¼•å…¥äº†è´å¶æ–¯ä¸€è‡´æ€§åˆ†æ•°ï¼Œå³ä½¿åœ¨å‡†ç¡®æ€§é¥±å’Œæ—¶ä¹Ÿèƒ½æ£€æµ‹è´å¶æ–¯ä¸€è‡´çš„è¡Œä¸ºå˜åŒ–ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶èƒ½åŠ›å¼ºçš„æ¨¡å‹é€šå¸¸ä»¥è´å¶æ–¯ä¸€è‡´çš„æ–¹å¼é€‚åº”ï¼Œä½†å‡†ç¡®æ€§å¹¶ä¸èƒ½ä¿è¯é²æ£’æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒGPT-5 Miniåœ¨æ–‡æœ¬å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å®Œç¾ï¼Œä½†æœªèƒ½æœ‰æ•ˆæ•´åˆè§†è§‰çº¿ç´¢ã€‚è¿™æ­ç¤ºäº†èƒ½åŠ›ä¸ç­–ç•¥ä¹‹é—´çš„å…³é”®åˆ†ç¦»ï¼Œè¡¨æ˜ä»¥å‡†ç¡®æ€§ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•å¯èƒ½è¿‡åº¦å…³æ³¨æ€§èƒ½è€Œå¿½ç•¥äº†è„†å¼±çš„ç¡®å®šæ€§å¤„ç†ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ä¸ç¡®å®šæ€§å¤„ç†çš„åŸåˆ™æ€§æ¶Œç°ï¼Œå¹¶çªå‡ºäº†å‡†ç¡®æ€§ä¸è´å¶æ–¯å€¾å‘ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†æ¨¡å‹èƒ½åŠ›ä¸è®¡ç®—ç­–ç•¥ä¹‹é—´çš„é‡è¦åˆ†ç¦»ï¼Œè¡¨æ˜å‡†ç¡®æ€§å¯¼å‘çš„è¯„ä¼°å¯èƒ½æ©ç›–äº†æ¨¡å‹åœ¨ä¸ç¡®å®šæ€§å¤„ç†æ–¹é¢çš„è„†å¼±æ€§ã€‚ç ”ç©¶å‘å¸ƒçš„å¿ƒç†ç‰©ç†å­¦åŸºå‡†æµ‹è¯•å’Œä¸€è‡´æ€§æŒ‡æ ‡ä¸ºæœªæ¥å¤šæ¨¡æ€æ¶æ„è®¾è®¡æä¾›äº†è¯„ä¼°å·¥å…·å’ŒæŒ‡å¯¼ï¼Œå¼ºè°ƒäº†åœ¨æ¨¡å‹è¯„ä¼°ä¸­è€ƒè™‘éšå¼è®¡ç®—ç­–ç•¥å’Œè´å¶æ–¯ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_54">ğŸ“„ Abstract</h4>
<p>Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.</p>
<h3 id="56-boom-beyond-only-one-modality-kits-multimodal-multilingual-lecture-companion">[56] <a href="https://arxiv.org/abs/2512.02817">BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion</a></h3>
<p><em>Sai Koneru, Fabian Retkowski, Christian Huber, Lukas Hilgert, Seymanur Akti, Enes Yavuz Ugan, Alexander Waibel, Jan Niehues</em></p>
<h4 id="tldr_55">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†BOOMï¼Œä¸€ç§å¤šæ¨¡æ€å¤šè¯­è¨€è®²åº§ä¼´ä¾£ç³»ç»Ÿï¼Œèƒ½å¤Ÿè”åˆç¿»è¯‘è®²åº§éŸ³é¢‘å’Œå¹»ç¯ç‰‡ï¼Œç”Ÿæˆè·¨æ–‡æœ¬ã€è§†è§‰å’Œè¯­éŸ³ä¸‰ç§æ¨¡æ€çš„åŒæ­¥è¾“å‡ºï¼Œä¸ºåœ¨çº¿å­¦ä¹ æä¾›å®Œæ•´çš„æœ¬åœ°åŒ–å­¦ä¹ ä½“éªŒã€‚</p>
<hr />
<h4 id="detailed-summary_55">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ•™è‚²å…¨çƒåŒ–å’Œåœ¨çº¿å­¦ä¹ çš„å¿«é€Ÿå¢é•¿ä½¿å¾—æ•™è‚²å†…å®¹æœ¬åœ°åŒ–æˆä¸ºå…³é”®æŒ‘æˆ˜ï¼Œè®²åº§ææ–™æœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€çš„ï¼Œç»“åˆäº†å£è¯­éŸ³é¢‘å’Œè§†è§‰å¹»ç¯ç‰‡ï¼Œéœ€è¦èƒ½å¤Ÿå¤„ç†å¤šç§è¾“å…¥æ¨¡æ€çš„ç³»ç»Ÿï¼Œä¸ºäº†æä¾›å¯è®¿é—®ä¸”å®Œæ•´çš„å­¦ä¹ ä½“éªŒï¼Œç¿»è¯‘å¿…é¡»ä¿ç•™æ‰€æœ‰æ¨¡æ€ï¼šç”¨äºé˜…è¯»çš„æ–‡æœ¬ã€ç”¨äºè§†è§‰ç†è§£çš„å¹»ç¯ç‰‡ä»¥åŠç”¨äºå¬è§‰å­¦ä¹ çš„è¯­éŸ³ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†BOOMï¼Œä¸€ç§ç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€å¤šè¯­è¨€è®²åº§ä¼´ä¾£ç³»ç»Ÿï¼Œèƒ½å¤Ÿè”åˆç¿»è¯‘è®²åº§éŸ³é¢‘å’Œå¹»ç¯ç‰‡ï¼Œç”ŸæˆåŒæ­¥çš„è·¨æ¨¡æ€è¾“å‡ºï¼ŒåŒ…æ‹¬ç¿»è¯‘æ–‡æœ¬ã€ä¿ç•™è§†è§‰å…ƒç´ çš„æœ¬åœ°åŒ–å¹»ç¯ç‰‡ä»¥åŠåˆæˆè¯­éŸ³ï¼Œç³»ç»Ÿé€šè¿‡å¹»ç¯ç‰‡æ„ŸçŸ¥çš„è½¬å½•æ–¹æ³•å¤„ç†å¤šæ¨¡æ€è¾“å…¥ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œå¹»ç¯ç‰‡æ„ŸçŸ¥çš„è½¬å½•å¯¹ä¸‹æ¸¸ä»»åŠ¡å¦‚æ‘˜è¦å’Œé—®ç­”å…·æœ‰çº§è”æ•ˆç›Šï¼Œç³»ç»Ÿèƒ½å¤Ÿç”ŸæˆåŒæ­¥çš„è·¨æ¨¡æ€è¾“å‡ºï¼ŒåŒ…æ‹¬ç¿»è¯‘æ–‡æœ¬ã€æœ¬åœ°åŒ–å¹»ç¯ç‰‡å’Œåˆæˆè¯­éŸ³ï¼Œæ‰€æœ‰ä»£ç å’Œæ¨¡å‹å·²å¼€æºå‘å¸ƒï¼Œé‡‡ç”¨MITè®¸å¯è¯ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€æ•™è‚²å†…å®¹æœ¬åœ°åŒ–æä¾›äº†ç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½¿å­¦ç”Ÿèƒ½å¤Ÿä»¥æ¯è¯­è®¿é—®è®²åº§å†…å®¹ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å†…å®¹çš„å®Œæ•´æ€§ï¼Œå¹»ç¯ç‰‡æ„ŸçŸ¥çš„è½¬å½•æ–¹æ³•å¯¹ä¸‹æ¸¸ä»»åŠ¡å…·æœ‰ç§¯æå½±å“ï¼Œä¸ºå¤šæ¨¡æ€æœºå™¨ç¿»è¯‘å’Œæ•™è‚²æŠ€æœ¯é¢†åŸŸæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_55">ğŸ“„ Abstract</h4>
<p>The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\footnote{All released code and models are licensed under the MIT License.</p>
<h3 id="57-autoneural-co-designing-vision-language-models-for-npu-inference">[57] <a href="https://arxiv.org/abs/2512.02924">AutoNeural: Co-Designing Vision-Language Models for NPU Inference</a></h3>
<p><em>Wei Chen, Liangmin Wu, Yunhai Hu, Zhiyuan Li, Zhiyuan Cheng, Yicheng Qian, Lingyue Zhu, Zhipeng Hu, Luoyi Liang, Qiang Tang, Zhen Liu, Han Yang</em></p>
<h4 id="tldr_56">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºAutoNeuralï¼Œä¸€ç§ä¸“ä¸ºNPUæ•´æ•°æ¨ç†ååŒè®¾è®¡çš„åŸç”Ÿè§†è§‰-è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œé€šè¿‡æ›¿æ¢æ ‡å‡†ViTç¼–ç å™¨ä¸ºMobileNetV5é£æ ¼éª¨å¹²ç½‘ç»œï¼Œå¹¶ç»“åˆçŠ¶æ€ç©ºé—´æ¨¡å‹ä¸Transformerçš„æ··åˆè®¾è®¡ï¼Œæ˜¾è‘—æå‡äº†è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ¨ç†æ•ˆç‡å’Œé‡åŒ–ç¨³å®šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_56">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰é¢å‘GPUä¼˜åŒ–çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç¥ç»å¤„ç†å•å…ƒä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸»è¦å½’å› äºè§†è§‰Transformerçš„é‡åŒ–è„†å¼±æ€§å’Œè‡ªå›å½’æ³¨æ„åŠ›æœºåˆ¶çš„I/Oå—é™ç‰¹æ€§ï¼Œè¿™äº›å› ç´ æ— æ³•å……åˆ†åˆ©ç”¨NPUçš„é«˜ç®—æœ¯ååé‡ï¼Œå¯¼è‡´ç¡¬ä»¶ä¸æ¨¡å‹ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> AutoNeuralé‡‡ç”¨NPUåŸç”Ÿæ¶æ„è®¾è®¡ï¼Œå°†æ ‡å‡†ViTç¼–ç å™¨æ›¿æ¢ä¸ºåŸºäºæ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„MobileNetV5é£æ ¼éª¨å¹²ç½‘ç»œï¼Œç¡®ä¿æ¿€æ´»åˆ†å¸ƒæœ‰ç•Œä»¥å®ç°ç¨³å®šçš„INT4/8/16é‡åŒ–ï¼›è¯­è¨€éª¨å¹²ç½‘ç»œåˆ™æ•´åˆçŠ¶æ€ç©ºé—´æ¨¡å‹åŸç†ä¸Transformerå±‚ï¼Œé‡‡ç”¨é«˜æ•ˆé—¨æ§å·ç§¯å®ç°çº¿æ€§æ—¶é—´å¤æ‚åº¦ï¼Œæ¶ˆé™¤ç”Ÿæˆè¿‡ç¨‹ä¸­é”®å€¼ç¼“å­˜çš„é‡å†…å­˜I/Oå¼€é”€ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†é‡åŒ–ç¨³å®šæ€§ï¼Œè§†è§‰ç¼–ç å™¨çš„é‡åŒ–è¯¯å·®é™ä½é«˜è¾¾7å€ï¼Œç«¯åˆ°ç«¯å»¶è¿Ÿå‡å°‘14å€ï¼›è§£ç é€Ÿåº¦æå‡3å€ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦å¢åŠ 4å€ï¼›åœ¨Qualcomm SA8295P SoCä¸Šçš„å®é™…æ±½è½¦æ¡ˆä¾‹ç ”ç©¶ä¸­éªŒè¯äº†é©¾é©¶èˆ±åº”ç”¨çš„å®æ—¶æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œé’ˆå¯¹NPUçº¦æŸé‡æ–°è®¾è®¡æ¨¡å‹æ‹“æ‰‘ç»“æ„æ˜¯å®ç°é²æ£’å¤šæ¨¡æ€è¾¹ç¼˜æ™ºèƒ½çš„å‰ææ¡ä»¶ï¼Œç¡¬ä»¶æ„ŸçŸ¥çš„ååŒè®¾è®¡æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£å†³é‡åŒ–è„†å¼±æ€§å’Œå†…å­˜ç“¶é¢ˆé—®é¢˜ï¼Œä¸ºè¾¹ç¼˜AIéƒ¨ç½²æä¾›äº†æ–°çš„æ¶æ„èŒƒå¼ã€‚</p>
<hr />
<h4 id="abstract_56">ğŸ“„ Abstract</h4>
<p>While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="58-flowchart2mermaid-a-vision-language-model-powered-system-for-converting-flowcharts-into-editable-diagram-code">[58] <a href="https://arxiv.org/abs/2512.02170">Flowchart2Mermaid: A Vision-Language Model Powered System for Converting Flowcharts into Editable Diagram Code</a></h3>
<p><em>Pritam Deka, Barry Devereux</em></p>
<h4 id="tldr_57">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Flowchart2Mermaidç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ç½‘ç»œå·¥å…·ï¼Œèƒ½å¤Ÿå°†æµç¨‹å›¾å›¾åƒè½¬æ¢ä¸ºå¯ç¼–è¾‘çš„Mermaid.jsä»£ç ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹å’Œæ··åˆä¸»åŠ¨å¼äº¤äº’ç•Œé¢å®ç°é™æ€æµç¨‹å›¾çš„åŠ¨æ€ç¼–è¾‘ä¸é‡ç”¨ã€‚</p>
<hr />
<h4 id="detailed-summary_57">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æµç¨‹å›¾ä½œä¸ºå¸¸è§çš„æµç¨‹æ²Ÿé€šå·¥å…·ï¼Œé€šå¸¸ä»¥é™æ€å›¾åƒå½¢å¼å…±äº«ï¼Œéš¾ä»¥ç¼–è¾‘å’Œé‡ç”¨ï¼Œç°æœ‰å›¾åƒè½¬å›¾è¡¨å·¥å…·ç¼ºä¹ç»“æ„åŒ–ã€ç‰ˆæœ¬å¯æ§çš„æ–‡æœ¬è¡¨ç¤ºæ–¹æ³•ï¼Œå¯¼è‡´å·¥ä½œæµç¨‹ä¸­æ–­å’Œåä½œæ•ˆç‡ä½ä¸‹ã€‚</p>
<p><strong>Method:</strong> ç³»ç»Ÿé‡‡ç”¨è¯¦ç»†çš„ç³»ç»Ÿæç¤ºå’Œè§†è§‰è¯­è¨€æ¨¡å‹å°†æµç¨‹å›¾å›¾åƒè½¬æ¢ä¸ºMermaid.jsæ ‡è®°è¯­è¨€ï¼Œç•Œé¢æ”¯æŒæ··åˆä¸»åŠ¨å¼ç»†åŒ–ï¼ŒåŒ…æ‹¬å†…è”æ–‡æœ¬ç¼–è¾‘ã€æ‹–æ”¾èŠ‚ç‚¹æ’å…¥ä»¥åŠé›†æˆAIåŠ©æ‰‹è§£é‡Šçš„è‡ªç„¶è¯­è¨€å‘½ä»¤ï¼Œå®ç°æ¸²æŸ“å›¾ä¸ç»“æ„åŒ–æ–‡æœ¬è¡¨ç¤ºçš„åŒæ­¥ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å¼•å…¥äº†è¯„ä¼°ç»“æ„å‡†ç¡®æ€§ã€æµç¨‹æ­£ç¡®æ€§ã€è¯­æ³•æœ‰æ•ˆæ€§å’Œå®Œæ•´æ€§çš„å¤šç»´åº¦è¯„ä»·æŒ‡æ ‡ï¼Œç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆç‰ˆæœ¬å¯æ§çš„æ–‡æœ¬è¡¨ç¤ºï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡å‹ä¸ŠéªŒè¯äº†è½¬æ¢æ•ˆæœï¼Œç›¸æ¯”ç°æœ‰å·¥å…·æä¾›äº†æ›´ç»“æ„åŒ–çš„è¾“å‡ºæ ¼å¼ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å°†é™æ€æµç¨‹å›¾è½¬æ¢ä¸ºå¯ç¼–è¾‘æ–‡æœ¬è¡¨ç¤ºçš„å¯è¡Œæ€§ï¼Œä¸ºè§†è§‰å·¥ä½œæµç¨‹çš„æ•°å­—åŒ–åä½œæä¾›äº†æ–°èŒƒå¼ï¼Œæå‡ºçš„è¯„ä¼°æ¡†æ¶ä¸ºæœªæ¥å›¾åƒåˆ°å›¾è¡¨è½¬æ¢ç³»ç»Ÿçš„æ€§èƒ½æ¯”è¾ƒå»ºç«‹äº†åŸºå‡†ï¼Œæ¨åŠ¨äº†æ–‡æ¡£å·¥ä½œæµç¨‹çš„è‡ªåŠ¨åŒ–å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_57">ğŸ“„ Abstract</h4>
<p>Flowcharts are common tools for communicating processes but are often shared as static images that cannot be easily edited or reused. We present \textsc{Flowchart2Mermaid}, a lightweight web system that converts flowchart images into editable Mermaid.js code which is a markup language for visual workflows, using a detailed system prompt and vision-language models. The interface supports mixed-initiative refinement through inline text editing, drag-and-drop node insertion, and natural-language commands interpreted by an integrated AI assistant. Unlike prior image-to-diagram tools, our approach produces a structured, version-controllable textual representation that remains synchronized with the rendered diagram. We further introduce evaluation metrics to assess structural accuracy, flow correctness, syntax validity, and completeness across multiple models.</p>
<h3 id="59-bridging-the-gap-toward-cognitive-autonomy-in-artificial-intelligence">[59] <a href="https://arxiv.org/abs/2512.02280">Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence</a></h3>
<p><em>Noorbakhsh Amiri Golilarz, Sindhuja Penchala, Shahram Rahimi</em></p>
<h4 id="tldr_58">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†å½“ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ä¸ƒå¤§æ ¸å¿ƒç¼ºé™·ï¼ŒåŒ…æ‹¬ç¼ºä¹å†…åœ¨è‡ªæˆ‘ç›‘æ§ã€å…ƒè®¤çŸ¥æ„è¯†ä¸è¶³ã€å­¦ä¹ æœºåˆ¶å›ºå®šéè‡ªé€‚åº”ç­‰ï¼Œå¹¶æå‡ºäº†åŸºäºç¥ç»è®¤çŸ¥åŸç†çš„è®¤çŸ¥è‡ªä¸»AIæ¶æ„ä½œä¸ºæœªæ¥å‘å±•æ–¹å‘ã€‚</p>
<hr />
<h4 id="detailed-summary_58">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡äººå·¥æ™ºèƒ½åœ¨æ„ŸçŸ¥ã€è¯­è¨€ã€æ¨ç†å’Œå¤šæ¨¡æ€é¢†åŸŸå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†ç°ä»£AIç³»ç»Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­ä»å­˜åœ¨æ ¹æœ¬æ€§é™åˆ¶ï¼Œæ— æ³•å®ç°è‡ªæˆ‘ç›‘æ§ã€è‡ªæˆ‘æ ¡æ­£å’Œè¡Œä¸ºè‡ªä¸»è°ƒèŠ‚ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³å½“ä»£AIæ¨¡å‹çš„ä¸ƒå¤§æ ¸å¿ƒç¼ºé™·ï¼ŒåŒ…æ‹¬å†…åœ¨è‡ªæˆ‘ç›‘æ§ç¼ºå¤±ã€å…ƒè®¤çŸ¥æ„è¯†ä¸è¶³ã€å›ºå®šéè‡ªé€‚åº”å­¦ä¹ æœºåˆ¶ã€ç›®æ ‡é‡æ„èƒ½åŠ›ç¼ºä¹ã€è¡¨å¾ç»´æŠ¤ä¸è¶³ã€å…·èº«åé¦ˆä¸å……åˆ†ä»¥åŠå†…åœ¨èƒ½åŠ¨æ€§ç¼ºå¤±ï¼Œè¿™äº›é™åˆ¶é˜»ç¢äº†ç³»ç»Ÿå®ç°é²æ£’æ³›åŒ–ã€ç»ˆèº«é€‚åº”æ€§å’ŒçœŸå®ä¸–ç•Œè‡ªä¸»æ€§ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é‡‡ç”¨æ¯”è¾ƒåˆ†ææ–¹æ³•ï¼Œå°†äººå·¥ç³»ç»Ÿä¸ç”Ÿç‰©è®¤çŸ¥è¿›è¡Œå¯¹æ¯”ç ”ç©¶ï¼Œå¹¶æ•´åˆAIç ”ç©¶ã€è®¤çŸ¥ç§‘å­¦å’Œç¥ç»ç§‘å­¦çš„è§è§£ã€‚é€šè¿‡ç³»ç»Ÿè¯†åˆ«å’Œåˆ†æä¸ƒå¤§æ ¸å¿ƒç¼ºé™·ï¼Œæå‡ºäº†åŸºäºç¥ç»è®¤çŸ¥åŸç†çš„å‰ç»æ€§æ¶æ„è®¾è®¡æ€è·¯ï¼Œå¼ºè°ƒéœ€è¦è¶…è¶Šå½“å‰æ·±åº¦å­¦ä¹ ä¸åŸºäºTransformerçš„æ¶æ„ï¼Œæ„å»ºèƒ½å¤Ÿå®ç°è‡ªæˆ‘å¯¼å‘é€‚åº”ã€åŠ¨æ€è¡¨å¾ç®¡ç†å’Œæ„å‘æ€§ç›®æ ‡å¯¼å‘è¡Œä¸ºçš„è®¤çŸ¥è‡ªä¸»AIç³»ç»Ÿã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶æ˜ç¡®è¯†åˆ«äº†åˆ¶çº¦å½“ä»£AIæ¨¡å‹çš„ä¸ƒå¤§ç»“æ„æ€§ç¼ºé™·ï¼Œè®ºè¯äº†å•çº¯æ‰©å±•æ¨¡å‹è§„æ¨¡æ— æ³•è§£å†³è¿™äº›æ ¹æœ¬é—®é¢˜ã€‚é€šè¿‡è·¨å­¦ç§‘åˆ†ææ­ç¤ºäº†å½“å‰æ¶æ„åœ¨å®ç°é²æ£’æ³›åŒ–ã€ç»ˆèº«é€‚åº”æ€§å’ŒçœŸå®ä¸–ç•Œè‡ªä¸»æ€§æ–¹é¢çš„å†…åœ¨å±€é™æ€§ï¼Œä¸ºè®¤çŸ¥è‡ªä¸»AIçš„å‘å±•æä¾›äº†ç†è®ºåŸºç¡€å’Œåˆ†ææ¡†æ¶ï¼Œå¼ºè°ƒäº†éœ€è¦èŒƒå¼è½¬å˜è€Œéæ¸è¿›æ”¹è¿›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ä¸»å¼ å‘è®¤çŸ¥åŸºç¡€AIï¼ˆè®¤çŸ¥è‡ªä¸»ï¼‰è¿›è¡ŒèŒƒå¼è½¬å˜ï¼Œè¿™ç§ç³»ç»Ÿèƒ½å¤Ÿå®ç°è‡ªæˆ‘å¯¼å‘é€‚åº”ã€åŠ¨æ€è¡¨å¾ç®¡ç†å’Œæ„å‘æ€§ç›®æ ‡å¯¼å‘è¡Œä¸ºã€‚åŒæ—¶éœ€è¦é…å¥—æ”¹é©æ€§ç›‘ç£æœºåˆ¶ï¼Œç¡®ä¿è‡ªä¸»ç³»ç»Ÿä¿æŒå¯è§£é‡Šæ€§ã€å¯æ²»ç†æ€§å¹¶ä¸äººç±»ä»·å€¼è§‚å¯¹é½ã€‚è¿™ä¸€è½¬å˜å¯¹äºå®ç°çœŸæ­£å…·æœ‰é€‚åº”æ€§ã€è‡ªä¸»æ€§å’Œå¯é æ€§çš„AIç³»ç»Ÿè‡³å…³é‡è¦ï¼Œä¸ºæœªæ¥AIæ¶æ„è®¾è®¡æä¾›äº†æ˜ç¡®çš„æ–¹å‘æ€§æŒ‡å¯¼ã€‚</p>
<hr />
<h4 id="abstract_58">ğŸ“„ Abstract</h4>
<p>Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fun- damentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self- monitoring, lack of meta-cognitive awareness, fixed and non- adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust general- ization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.</p>
<h3 id="60-omniguard-unified-omni-modal-guardrails-with-deliberate-reasoning">[60] <a href="https://arxiv.org/abs/2512.02306">OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning</a></h3>
<p><em>Boyu Zhu, Xiaofei Wen, Wenjie Jacky Mo, Tinghui Zhu, Yanan Xie, Peng Qi, Muhao Chen</em></p>
<h4 id="tldr_59">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†OmniGuardï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æŠ¤æ ç³»ç»Ÿï¼Œé€šè¿‡ç»“æ„åŒ–å®‰å…¨æ ‡æ³¨å’Œä¸“å®¶æ¨¡å‹è’¸é¦ï¼Œå®ç°äº†è·¨æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘çš„ç»Ÿä¸€å®‰å…¨ä¿éšœæ¡†æ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_59">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¤„ç†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘æ—¶å¼•å…¥äº†æ–°çš„å®‰å…¨æŒ‘æˆ˜ï¼Œç°æœ‰æŠ¤æ ç ”ç©¶ä¸»è¦é’ˆå¯¹å•æ¨¡æ€è®¾ç½®ä¸”é€šå¸¸å°†å®‰å…¨ä¿éšœè§†ä¸ºäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¤šæ ·åŒ–æ¨¡æ€å’Œä»»åŠ¡ä¸­çš„é²æ£’æ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æå‡ºäº†OmniGuardå®¶æ—ï¼Œè¿™æ˜¯é¦–ä¸ªå…·å¤‡æ·±æ€ç†Ÿè™‘æ¨ç†èƒ½åŠ›çš„å…¨æ¨¡æ€æŠ¤æ ç³»ç»Ÿï¼›ä¸ºæ”¯æŒè®­ç»ƒï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡21ä¸‡ä¸ªå¤šæ ·åŒ–æ ·æœ¬çš„å¤§å‹å…¨æ¨¡æ€å®‰å…¨æ•°æ®é›†ï¼Œæ¶µç›–æ‰€æœ‰æ¨¡æ€çš„å•æ¨¡æ€å’Œè·¨æ¨¡æ€æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½é€šè¿‡ç›®æ ‡è’¸é¦ä»ä¸“å®¶æ¨¡å‹è·å¾—ç»“æ„åŒ–å®‰å…¨æ ‡ç­¾å’Œç²¾å¿ƒç­–åˆ’çš„å®‰å…¨è¯„æã€‚</p>
<p><strong>Result:</strong> åœ¨15ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒOmniGuardåœ¨å¹¿æ³›çš„å¤šæ¨¡æ€å®‰å…¨åœºæ™¯ä¸­å®ç°äº†å¼ºå¤§çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œç³»ç»Ÿèƒ½å¤Ÿè·¨æ‰€æœ‰æ¨¡æ€æ‰§è¡Œå®‰å…¨ä¿éšœä»»åŠ¡ã€‚</p>
<p><strong>Conclusion:</strong> OmniGuardæä¾›äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å…¨æ¨¡æ€ä¸­æ‰§è¡Œç­–ç•¥å¹¶é™ä½é£é™©ï¼Œä¸ºæ„å»ºæ›´é²æ£’å’Œæ›´å¼ºå¤§çš„å…¨æ¨¡æ€å®‰å…¨ç³»ç»Ÿé“ºå¹³äº†é“è·¯ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="abstract_59">ğŸ“„ Abstract</h4>
<p>Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.</p>
<h3 id="61-reasoning-path-and-latent-state-analysis-for-multi-view-visual-spatial-reasoning-a-cognitive-science-perspective">[61] <a href="https://arxiv.org/abs/2512.02340">Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective</a></h3>
<p><em>Qiyao Xue, Weichen Liu, Shiqi Wang, Haoming Wang, Yuyang Wu, Wei Gao</em></p>
<h4 id="tldr_60">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ReMindView-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè§†è§’ç©ºé—´æ¨ç†ä¸­æ„å»ºã€å¯¹é½å’Œç»´æŠ¤ç©ºé—´å¿ƒç†æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è·¨è§†è§’å¯¹é½å’Œè§†è§’è½¬æ¢æ–¹é¢çš„ç³»ç»Ÿæ€§ç¼ºé™·ã€‚</p>
<hr />
<h4 id="detailed-summary_60">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè§†è§’è®¾ç½®ä¸­è¿›è¡Œç©ºé—´æ¨ç†æ—¶ï¼Œéš¾ä»¥ç»´æŒå‡ ä½•ä¸€è‡´æ€§å’Œè·¨è§†è§’ä¸€è‡´æ€§ï¼Œè¿™ä¸€å·®è·æºäºç¼ºä¹èƒ½å¤Ÿå°†å¤šè§†è§’æ¨ç†ä¸å•è§†è§’æ„ŸçŸ¥å’Œæ—¶é—´å› ç´ åˆ†ç¦»çš„ç»†ç²’åº¦åŸºå‡†æµ‹è¯•ï¼Œå› æ­¤éœ€è¦æ„å»ºè®¤çŸ¥åŸºç¡€æ‰å®çš„è¯„ä¼°æ¡†æ¶æ¥è¯Šæ–­VLMçš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºäº†ReMindView-BenchåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡ç³»ç»Ÿå˜åŒ–è§†è§’ç©ºé—´æ¨¡å¼å’ŒæŸ¥è¯¢ç±»å‹æ¥æ¢æµ‹ç©ºé—´è®¤çŸ¥çš„å…³é”®å› ç´ ï¼Œé‡‡ç”¨æ˜¾å¼åˆ†é˜¶æ®µåˆ†æï¼ˆåŒ…æ‹¬LLM-as-a-judgeå’Œè‡ªä¸€è‡´æ€§æç¤ºï¼‰è¯„ä¼°æ¨ç†è¿‡ç¨‹ï¼Œä»¥åŠéšå¼åˆ†æï¼ˆåŒ…æ‹¬çº¿æ€§æ¢æµ‹å’Œç†µåŠ¨æ€ï¼‰æ¥è¿½è¸ªä»»åŠ¡ç›¸å…³ä¿¡æ¯ä¸¢å¤±å’Œä¸ç¡®å®šæ€§åˆ†ç¦»ã€‚</p>
<p><strong>Result:</strong> å¯¹15ä¸ªå½“å‰VLMçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨å¤šè§†è§’ç©ºé—´æ¨ç†ä¸­æ™®éå­˜åœ¨è·¨è§†è§’å¯¹é½å’Œè§†è§’è½¬æ¢çš„å¤±è´¥ï¼Œæ˜¾å¼åˆ†æè¡¨æ˜æ¨¡å‹åœ¨å¸§å†…æ„ŸçŸ¥è¡¨ç°è‰¯å¥½ä½†åœ¨è·¨è§†è§’ä¿¡æ¯æ•´åˆæ—¶æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œéšå¼åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†ä»»åŠ¡ç›¸å…³ä¿¡æ¯çš„æ¸è¿›æ€§ä¸¢å¤±ä»¥åŠæ­£ç¡®ä¸é”™è¯¯è½¨è¿¹é—´çš„ä¸ç¡®å®šæ€§åˆ†ç¦»ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºVLMç©ºé—´æ¨ç†æä¾›äº†è®¤çŸ¥åŸºç¡€çš„è¯Šæ–­ï¼Œæ­ç¤ºäº†å¤šè§†è§’ç©ºé—´å¿ƒç†æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¦‚ä½•å½¢æˆã€é€€åŒ–å’Œå¤±ç¨³ï¼ŒåŸºå‡†æµ‹è¯•çš„å…¬å¼€å¯ç”¨æ€§å°†ä¿ƒè¿›æœªæ¥åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„æ¨¡å‹æ”¹è¿›å’Œè¯„ä¼°æ–¹æ³•å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_60">ğŸ“„ Abstract</h4>
<p>Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.</p>
<h3 id="62-aetheria-a-multimodal-interpretable-content-safety-framework-based-on-multi-agent-debate-and-collaboration">[62] <a href="https://arxiv.org/abs/2512.02530">Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration</a></h3>
<p><em>Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li</em></p>
<h4 id="tldr_61">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºAetheriaï¼Œä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“è¾©è®ºä¸åä½œçš„å¤šæ¨¡æ€å¯è§£é‡Šå†…å®¹å®‰å…¨æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è¾©è®ºæœºåˆ¶å’ŒRAGçŸ¥è¯†æ£€ç´¢ï¼Œæ˜¾è‘—æå‡å†…å®¹å®‰å…¨å®¡æ ¸çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_61">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ•°å­—å†…å®¹çš„æŒ‡æ•°çº§å¢é•¿ç»™å†…å®¹å®‰å…¨å¸¦æ¥é‡å¤§æŒ‘æˆ˜ï¼Œå½“å‰åŸºäºå•ä¸€æ¨¡å‹æˆ–å›ºå®šæµæ°´çº¿çš„å®¡æ ¸ç³»ç»Ÿåœ¨è¯†åˆ«éšå«é£é™©å’Œæä¾›å¯è§£é‡Šåˆ¤æ–­è¿‡ç¨‹æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ›´é€æ˜å’Œå¯è§£é‡Šçš„å†…å®¹å®¡æ ¸èŒƒå¼ã€‚</p>
<p><strong>Method:</strong> æå‡ºAetheriaå¤šæ¨¡æ€å¯è§£é‡Šå†…å®¹å®‰å…¨æ¡†æ¶ï¼Œé‡‡ç”¨äº”ä¸ªæ ¸å¿ƒæ™ºèƒ½ä½“çš„åä½œæ¶æ„ï¼Œé€šè¿‡åŸºäºRAGçš„çŸ¥è¯†æ£€ç´¢å’ŒåŠ¨æ€ç›¸äº’è¯´æœè¾©è®ºæœºåˆ¶ï¼Œå¯¹å¤šæ¨¡æ€å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æå’Œè£å†³ï¼Œç”Ÿæˆè¯¦ç»†å¯è¿½æº¯çš„å®¡æ ¸æŠ¥å‘Šã€‚</p>
<p><strong>Result:</strong> åœ¨æå‡ºçš„AIR-BenchåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œç»¼åˆå®éªŒéªŒè¯ï¼ŒAetheriaä¸ä»…ç”Ÿæˆè¯¦ç»†å¯è¿½æº¯çš„å®¡æ ¸æŠ¥å‘Šï¼Œè€Œä¸”åœ¨æ•´ä½“å†…å®¹å®‰å…¨å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨éšå«é£é™©è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ¡†æ¶å»ºç«‹äº†é€æ˜å¯è§£é‡Šçš„å†…å®¹å®¡æ ¸èŒƒå¼ï¼Œæ˜¾è‘—æ¨è¿›äº†å¯ä¿¡AIå†…å®¹å®¡æ ¸é¢†åŸŸçš„å‘å±•ï¼Œä¸ºå¤šæ¨¡æ€å†…å®¹å®‰å…¨æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¼ºè°ƒäº†å¯è§£é‡Šæ€§å’Œé€æ˜åº¦åœ¨å†…å®¹å®‰å…¨ç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_61">ğŸ“„ Abstract</h4>
<p>The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.</p>
<h3 id="63-empathy-level-prediction-in-multi-modal-scenario-with-supervisory-documentation-assistance">[63] <a href="https://arxiv.org/abs/2512.02558">Empathy Level Prediction in Multi-Modal Scenario with Supervisory Documentation Assistance</a></h3>
<p><em>Yufei Xiao, Shangfei Wang</em></p>
<h4 id="tldr_62">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…ˆè¿›çš„å¤šæ¨¡æ€å…±æƒ…é¢„æµ‹æ–¹æ³•ï¼Œæ•´åˆè§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œå¹¶å¼•å…¥ç›‘ç£æ–‡æ¡£ä½œä¸ºç‰¹æƒä¿¡æ¯æ¥å¢å¼ºæ–‡æœ¬ç‰¹å¾æå–ï¼Œåœ¨è®­ç»ƒé˜¶æ®µæå‡æ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_62">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å…±æƒ…é¢„æµ‹æŠ€æœ¯ä¸»è¦é›†ä¸­äºå•ä¸€æ¨¡æ€ï¼ˆé€šå¸¸æ˜¯æ–‡æœ¬ï¼‰ï¼Œå¿½è§†äº†å¤šæ¨¡æ€å¤„ç†èƒ½åŠ›ï¼ŒåŒæ—¶å¿½ç•¥äº†æŸäº›ç‰¹æƒä¿¡æ¯çš„åˆ©ç”¨ï¼Œè¿™äº›ä¿¡æ¯å¯èƒ½åŒ…å«é¢å¤–çš„å…±æƒ…å†…å®¹ï¼Œå¯¼è‡´é¢„æµ‹èƒ½åŠ›å—é™ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åŒ…å«å¤šæ¨¡æ€å…±æƒ…é¢„æµ‹å’Œç›‘ç£æ–‡æ¡£è¾…åŠ©è®­ç»ƒä¸¤éƒ¨åˆ†ï¼Œä½¿ç”¨é¢„è®­ç»ƒç½‘ç»œæå–è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬ç‰¹å¾ï¼Œé€šè¿‡è·¨æ¨¡æ€èåˆç”Ÿæˆå¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºæ¥é¢„æµ‹å…±æƒ…æ ‡ç­¾ï¼Œå¹¶åœ¨è¾…åŠ©è®­ç»ƒé˜¶æ®µå¼•å…¥ç›‘ç£æ–‡æ¡£ä½œä¸ºç‰¹æƒä¿¡æ¯ï¼Œåº”ç”¨æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…æ¨¡å‹è¯†åˆ«æ½œåœ¨ä¸»é¢˜åˆ†å¸ƒä»¥çº¦æŸæ–‡æœ¬ç‰¹å¾ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šæ¨¡æ€å’Œå¯¹è¯å…±æƒ…æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€èåˆå’Œç›‘ç£æ–‡æ¡£è¾…åŠ©è®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼Œç‰¹æƒä¿¡æ¯åœ¨è®­ç»ƒé˜¶æ®µçš„å¼•å…¥æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯å’Œåˆ©ç”¨ç‰¹æƒç›‘ç£æ–‡æ¡£çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå…±æƒ…é¢„æµ‹æä¾›äº†æ›´å…¨é¢çš„æ–¹æ³•ï¼ŒåŒæ—¶å±•ç¤ºäº†ä»…åœ¨è®­ç»ƒé˜¶æ®µå¯ç”¨çš„è¾…åŠ©ä¿¡æ¯å¦‚ä½•å¢å¼ºæ¨¡å‹å­¦ä¹ èƒ½åŠ›ï¼Œä¸ºæƒ…æ„Ÿè®¡ç®—å’Œå¿ƒç†å’¨è¯¢åº”ç”¨æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_62">ğŸ“„ Abstract</h4>
<p>Prevalent empathy prediction techniques primarily concentrate on a singular modality, typically textual, thus neglecting multi-modal processing capabilities. They also overlook the utilization of certain privileged information, which may encompass additional empathetic content. In response, we introduce an advanced multi-modal empathy prediction method integrating video, audio, and text information. The method comprises the Multi-Modal Empathy Prediction and Supervisory Documentation Assisted Training. We use pre-trained networks in the empathy prediction network to extract features from various modalities, followed by a cross-modal fusion. This process yields a multi-modal feature representation, which is employed to predict empathy labels. To enhance the extraction of text features, we incorporate supervisory documents as privileged information during the assisted training phase. Specifically, we apply the Latent Dirichlet Allocation model to identify potential topic distributions to constrain text features. These supervisory documents, created by supervisors, focus on the counseling topics and the counselor's display of empathy. Notably, this privileged information is only available during training and is not accessible during the prediction phase. Experimental results on the multi-modal and dialogue empathy datasets demonstrate that our approach is superior to the existing methods.</p>
<h3 id="64-zero-shot-instruction-following-in-rl-via-structured-ltl-representations">[64] <a href="https://arxiv.org/abs/2512.02633">Zero-Shot Instruction Following in RL via Structured LTL Representations</a></h3>
<p><em>Mattia Giuri, Mathias Jackermeier, Alessandro Abate</em></p>
<h4 id="tldr_63">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡å°†çº¿æ€§æ—¶åºé€»è¾‘æŒ‡ä»¤ç¼–ç ä¸ºå¸ƒå°”å…¬å¼åºåˆ—ï¼Œå¹¶åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œç”Ÿæˆç»“æ„åŒ–ä»»åŠ¡è¡¨ç¤ºï¼Œä»è€Œå­¦ä¹ èƒ½å¤Ÿæ‰§è¡Œä»»æ„LTLæŒ‡ä»¤çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤šäº‹ä»¶å¹¶å‘ä¸”å¤æ‚äº¤äº’ç¯å¢ƒä¸­çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_63">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºçº¿æ€§æ—¶åºé€»è¾‘çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å°†LTLæŒ‡ä»¤è§£é‡Šä¸ºæœ‰é™è‡ªåŠ¨æœºï¼Œèƒ½å¤Ÿå­¦ä¹ æ‰§è¡Œä»»æ„æŒ‡ä»¤çš„é€šç”¨ç­–ç•¥ï¼Œä½†åœ¨å¤šä¸ªé«˜å±‚äº‹ä»¶åŒæ—¶ä¸ºçœŸä¸”å¯èƒ½ä»¥å¤æ‚æ–¹å¼äº¤äº’çš„ç¯å¢ƒä¸­è¡¨ç°ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹¶å‘äº‹ä»¶äº¤äº’å¤æ‚çš„åœºæ™¯ä¸­ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šä»»åŠ¡ç­–ç•¥å­¦ä¹ æ–¹æ³•ï¼Œå°†ç­–ç•¥æ¡ä»¶åŒ–äºä¸è‡ªåŠ¨æœºè½¬æ¢ç›´æ¥å¯¹é½çš„ç®€å•å¸ƒå°”å…¬å¼åºåˆ—ã€‚é€šè¿‡å›¾ç¥ç»ç½‘ç»œå¯¹è¿™äº›å¸ƒå°”å…¬å¼è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆç»“æ„åŒ–çš„ä»»åŠ¡è¡¨ç¤ºï¼Œä»è€Œæœ‰æ•ˆå¤„ç†å¤šä¸ªåŸå­å‘½é¢˜åŒæ—¶ä¸ºçœŸä¸”å¤æ‚äº¤äº’çš„æƒ…å†µã€‚</p>
<p><strong>Result:</strong> åœ¨å¤æ‚çš„åŸºäºå›½é™…è±¡æ£‹çš„ç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ç°æœ‰æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨å¤„ç†å¤šäº‹ä»¶å¹¶å‘å’Œå¤æ‚äº¤äº’åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ç»“æ„åŒ–ä»»åŠ¡æ‰§è¡Œæ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§æœ‰æ•ˆå¤„ç†å¤æ‚å¹¶å‘äº‹ä»¶çš„LTLæŒ‡ä»¤æ‰§è¡Œæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–ä»»åŠ¡è¡¨ç¤ºå¢å¼ºäº†ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ºåœ¨å¤æ‚äº¤äº’ç¯å¢ƒä¸­å®ç°é€šç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå¯¹æœºå™¨äººä»»åŠ¡è§„åˆ’å’Œè‡ªä¸»ç³»ç»Ÿæ§åˆ¶å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_63">ğŸ“„ Abstract</h4>
<p>Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach.</p>
<h3 id="65-learning-what-to-attend-first-modality-importance-guided-reasoning-for-reliable-multimodal-emotion-understanding">[65] <a href="https://arxiv.org/abs/2512.02699">Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding</a></h3>
<p><em>Hyeongseop Rha, Jeong Hun Yeo, Junil Won, Se Jin Park, Yong Man Ro</em></p>
<h4 id="tldr_64">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†æ¨¡æ€é‡è¦æ€§å¼•å¯¼æ¨ç†ï¼ˆMIGRï¼‰æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«æƒ…æ„Ÿä¸»å¯¼æ¨¡æ€å¹¶é‡æ–°ç»„ç»‡æ¨ç†åºåˆ—ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿç†è§£ä»»åŠ¡ä¸­çš„æ¨ç†å¯é æ€§ï¼Œå°†æƒ…æ„Ÿä¸ä¸€è‡´è§£é‡Šçš„æ¯”ä¾‹ä»18.10%é™ä½åˆ°7.37%ã€‚</p>
<hr />
<h4 id="detailed-summary_64">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºæ¨ç†çš„å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£æ–¹æ³•å­˜åœ¨æ¨ç†æ¼‚ç§»é—®é¢˜ï¼šæ¨¡å‹é€æ¸ä¾èµ–è‡ªèº«ç”Ÿæˆçš„æ–‡æœ¬è€Œéå¤šæ¨¡æ€è¯æ®ï¼Œä¸”è§£é‡Šè¿‡ç¨‹è¿‡åº¦å—è§†è§‰ä¸»å¯¼çš„æ¨ç†è·¯å¾„å½±å“ï¼Œå¯¼è‡´æƒ…æ„Ÿç†è§£å¯é æ€§ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> æå‡ºæ¨¡æ€é‡è¦æ€§ï¼ˆMIï¼‰æœºåˆ¶è¯†åˆ«æƒ…æ„Ÿä¸»å¯¼æ¨¡æ€ï¼Œå¹¶æ„å»ºMIGRæ¡†æ¶é‡æ–°ç»„ç»‡æ¨ç†åºåˆ—ï¼Œä½¿è§£é‡Šä»å¯¹ç›®æ ‡æƒ…æ„Ÿæœ€å…³é”®çš„æ¨¡æ€å¼€å§‹ï¼›é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬æ¨¡æ€å¯¹é½çš„ç›‘ç£å¾®è°ƒå’Œæ¨¡æ€æ„ŸçŸ¥çš„å¥–åŠ±ä¼˜åŒ–ï¼Œç¡®ä¿ç”Ÿæˆæƒ…æ„ŸåŸºç¡€æ‰å®ã€å› æœç›¸å…³ä¸”ä¿æŒè¿è´¯æ€§çš„è§£é‡Šã€‚</p>
<p><strong>Result:</strong> åœ¨DFEWåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMIGRæ˜¾è‘—æå‡äº†æ¨ç†å¯é æ€§ï¼Œå°†æ­£ç¡®é¢„æµ‹ä½†ä¼´éšæƒ…æ„Ÿä¸ä¸€è‡´è§£é‡Šçš„æ¯”ä¾‹ä»18.10%é™ä½åˆ°7.37%ï¼ŒéªŒè¯äº†ä»æƒ…æ„Ÿä¸»å¯¼æ¨¡æ€å¼€å§‹æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡è¯†åˆ«æƒ…æ„Ÿä¸»å¯¼æ¨¡æ€å¹¶é‡æ–°ç»„ç»‡æ¨ç†åºåˆ—ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢æ—©æœŸæ¨ç†è¢«ä¿¡æ¯é‡è¾ƒå°‘çš„çº¿ç´¢è¯¯å¯¼ï¼Œä»è€Œæå‡å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£çš„å¯é æ€§å’Œè§£é‡Šè´¨é‡ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿè®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_64">ğŸ“„ Abstract</h4>
<p>In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.</p>
<h3 id="66-training-data-attribution-for-image-generation-using-ontology-aligned-knowledge-graphs">[66] <a href="https://arxiv.org/abs/2512.02713">Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs</a></h3>
<p><em>Theodoros Aivalis, Iraklis A. Klampanos, Antonis Troumpoukis, Joemon M. Jose</em></p>
<h4 id="tldr_65">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªé€šè¿‡æ„å»ºæœ¬ä½“å¯¹é½çŸ¥è¯†å›¾è°±æ¥è§£é‡Šç”Ÿæˆæ¨¡å‹è¾“å‡ºçš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»å›¾åƒä¸­æå–ç»“æ„åŒ–ä¸‰å…ƒç»„ï¼Œé€šè¿‡æ¯”è¾ƒç”Ÿæˆå›¾åƒä¸è®­ç»ƒå›¾åƒçš„çŸ¥è¯†å›¾è°±æ¥è¿½è¸ªæ½œåœ¨å½±å“ï¼Œæ”¯æŒç‰ˆæƒåˆ†æå’Œå¯è§£é‡ŠAIã€‚</p>
<hr />
<h4 id="detailed-summary_65">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€ç”Ÿæˆæ¨¡å‹èƒ½åŠ›å¢å¼ºï¼Œé€æ˜åº¦ã€é—®è´£åˆ¶å’Œç‰ˆæƒä¾µæƒé—®é¢˜æ—¥ç›Šçªå‡ºï¼Œç†è§£ç‰¹å®šè®­ç»ƒæ•°æ®å¦‚ä½•å½±å“æ¨¡å‹è¾“å‡ºå˜å¾—è‡³å…³é‡è¦ï¼Œè€Œå½“å‰ä»è§†è§‰å†…å®¹ä¸­æå–ç»“æ„åŒ–ä¸”ç¬¦åˆæœ¬ä½“çš„è¡¨ç¤ºä»é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå›¾åƒå…·æœ‰ä¸°å¯Œæ€§å’Œå¤šå¯¹è±¡ç‰¹æ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»å›¾åƒä¸­æå–ç»“æ„åŒ–ä¸‰å…ƒç»„ï¼Œå¹¶ä¸é¢†åŸŸç‰¹å®šæœ¬ä½“å¯¹é½ï¼Œé€šè¿‡è‡ªåŠ¨æ„å»ºæœ¬ä½“å¯¹é½çŸ¥è¯†å›¾è°±æ¥æ¯”è¾ƒç”Ÿæˆå›¾åƒä¸è®­ç»ƒå›¾åƒçš„å›¾è°±ï¼Œä»è€Œè¿½è¸ªæ½œåœ¨å½±å“ï¼Œæ¡†æ¶æ”¯æŒé€šè¿‡å»å­¦ä¹ å’Œé£æ ¼ç‰¹å®šå®éªŒè¿›è¡ŒéªŒè¯ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•é€šè¿‡æœ¬åœ°è®­ç»ƒæ¨¡å‹çš„å»å­¦ä¹ å®éªŒå’Œå¤§è§„æ¨¡æ¨¡å‹çš„é£æ ¼ç‰¹å®šå®éªŒè¿›è¡Œäº†éªŒè¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¿½è¸ªç”Ÿæˆè¾“å‡ºä¸è®­ç»ƒæ•°æ®ä¹‹é—´çš„æ½œåœ¨å…³è”ï¼Œæ”¯æŒç‰ˆæƒåˆ†æã€æ•°æ®é›†é€æ˜åº¦å’Œå¯è§£é‡ŠAIçš„å®ç°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ¡†æ¶ä¸ºç”Ÿæˆæ¨¡å‹æä¾›äº†å¯è¿½æº¯æ€§å’Œè§£é‡Šæ€§ï¼Œæ”¯æŒç‰ˆæƒåˆ†æã€æ•°æ®é›†é€æ˜åº¦æå‡å’Œå¯è§£é‡ŠAIå‘å±•ï¼Œæœ‰åŠ©äºæ„å»ºä¿ƒè¿›äººç±»åä½œã€åˆ›é€ åŠ›å’Œæ¿€å‘å¥½å¥‡å¿ƒçš„AIç³»ç»Ÿï¼Œä¸ºç”Ÿæˆæ¨¡å‹çš„è´Ÿè´£ä»»ä½¿ç”¨æä¾›äº†æŠ€æœ¯åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_65">ğŸ“„ Abstract</h4>
<p>As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.</p>
<h3 id="67-radiologist-copilot-an-agentic-assistant-with-orchestrated-tools-for-radiology-reporting-with-quality-control">[67] <a href="https://arxiv.org/abs/2512.02814">Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control</a></h3>
<p><em>Yongrui Yu, Zhongzhen Huang, Linjie Mu, Shaoting Zhang, Xiaofan Zhang</em></p>
<h4 id="tldr_66">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Radiologist Copilotï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡ç¼–æ’å¤šç§å·¥å…·å®ç°è‡ªåŠ¨åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸è´¨é‡æ§åˆ¶ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›å…¨é¢æ”¯æŒã€‚</p>
<hr />
<h4 id="detailed-summary_66">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ”¾å°„å­¦æŠ¥å‘Šæ’°å†™æ˜¯ä¸´åºŠæ£€æŸ¥ä¸­è€—æ—¶ä¸”æ˜“å‡ºé”™çš„ä»»åŠ¡ï¼Œç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•ä¸»è¦å…³æ³¨æŠ¥å‘Šç”Ÿæˆé˜¶æ®µè€Œå¿½ç•¥äº†å…³é”®çš„è´¨é‡æ§åˆ¶æµç¨‹ï¼Œæ— æ³•ä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›å…¨é¢æ”¯æŒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠå®è·µä¸­çš„åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºæ¨ç†æ ¸å¿ƒï¼Œæ„å»ºäº†ä¸€ä¸ªæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªä¸»é€‰æ‹©å·¥å…·ã€è§„åˆ’å¹¶æ‰§è¡ŒåŠ¨ä½œï¼Œæ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿåœ¨æ•´ä¸ªæŠ¥å‘Šæµç¨‹ä¸­çš„è¡Œä¸ºã€‚ç¼–æ’çš„å·¥å…·åŒ…æ‹¬åŒºåŸŸå®šä½ã€åŸºäº"think with image"èŒƒå¼çš„åŒºåŸŸåˆ†æè§„åˆ’ã€æˆ˜ç•¥æ¨¡æ¿é€‰æ‹©ã€è´¨é‡è¯„ä¼°ä»¥åŠåé¦ˆé©±åŠ¨çš„è‡ªé€‚åº”ç»†åŒ–ç­‰è´¨é‡æ§åˆ¶æœºåˆ¶ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒRadiologist Copilotåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶Šäº†å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°å‡†ç¡®ã€å®Œæ•´ä¸”é«˜æ•ˆçš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼Œæœ‰æ•ˆååŠ©æ”¾å°„ç§‘åŒ»ç”Ÿå¹¶æå‡ä¸´åºŠæ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨åŒ»å­¦å½±åƒåˆ†æé¢†åŸŸçš„åº”ç”¨æ½œåŠ›ï¼Œé€šè¿‡æ•´åˆç”Ÿæˆä¸è´¨é‡æ§åˆ¶æµç¨‹ï¼Œä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›äº†å…¨é¢çš„è‡ªåŠ¨åŒ–æ”¯æŒã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†æŠ¥å‘Šè´¨é‡ï¼Œè¿˜é€šè¿‡æ¨¡æ‹Ÿäººç±»ä¸“å®¶çš„å·¥ä½œæµç¨‹å¢å¼ºäº†ç³»ç»Ÿçš„ä¸´åºŠå®ç”¨æ€§ï¼Œä¸ºæœªæ¥åŒ»ç–—AIåŠ©æ‰‹çš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract_66">ğŸ“„ Abstract</h4>
<p>Radiology reporting is an essential yet time-consuming and error-prone task for radiologists in clinical examinations, especially for volumetric medical images. Rigorous quality control is also critical but tedious, ensuring that the final report meets clinical standards. Existing automated approaches, including radiology report generation methods and medical vision-language models, focus mainly on the report generation phase and neglect the crucial quality control procedure, limiting their capability to provide comprehensive support to radiologists. We propose Radiologist Copilot, an agentic AI assistant equipped with orchestrated tools designed for automated radiology reporting with quality control. Leveraging large language models as the reasoning backbone, the agentic system autonomously selects tools, plans, and executes actions, emulating the behavior of radiologists throughout the holistic radiology reporting process. The orchestrated tools include region localization, think with image paradigm directed region analysis planning, strategic template selection for report generation, quality assessment and feedback-driven adaptive refinement for quality control. Therefore, Radiologist Copilot facilitates accurate, complete, and efficient radiology reporting, assisting radiologists and improving clinical efficiency. Experimental results demonstrate that Radiologist Copilot significantly surpasses other state-of-the-art methods in radiology reporting. The source code will be released upon acceptance.</p>
  </article>
</body>
</html>
