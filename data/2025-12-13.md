<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang, Zoey Guo, Kaixin Zhu, Ray Zhang, Qizhi Chen, Dongzhi Jiang, Junli Liu, Bohan Zeng, Haoming Song, Delin Qu, Tianyi Bai, Dan Xu, Wentao Zhang, Bin Zhao*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºæ–‡æœ¬åˆ°3Dè‡ªå›å½’ç”Ÿæˆï¼Œæå‡ºäº†Hi-GRPOåˆ†å±‚ä¼˜åŒ–æ–¹æ³•å’ŒAR3D-R1æ¨¡å‹ï¼Œé€šè¿‡å¥–åŠ±è®¾è®¡å’Œç®—æ³•æ”¹è¿›è§£å†³äº†3Dç”Ÿæˆä¸­çš„ç©ºé—´å¤æ‚æ€§å’Œå…¨å±€ä¸€è‡´æ€§æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¼ºåŒ–å­¦ä¹ åœ¨2Då›¾åƒç”Ÿæˆä¸­å·²è¯æ˜æœ‰æ•ˆï¼Œä½†å°†å…¶åº”ç”¨äº3Dç”Ÿæˆä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äº3Då¯¹è±¡å…·æœ‰æ›´é«˜çš„ç©ºé—´å¤æ‚æ€§ï¼Œéœ€è¦å…¨å±€ä¸€è‡´çš„å‡ ä½•ç»“æ„å’Œç»†ç²’åº¦å±€éƒ¨çº¹ç†ï¼Œè¿™ä½¿å¾—3Dç”Ÿæˆå¯¹å¥–åŠ±è®¾è®¡å’ŒRLç®—æ³•æä¸ºæ•æ„Ÿï¼Œç°æœ‰ç ”ç©¶å¯¹æ­¤æ¢ç´¢ä¸è¶³ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨ç³»ç»Ÿæ€§æ–¹æ³•æ¢ç´¢å¤šä¸ªç»´åº¦ï¼šè¯„ä¼°å¥–åŠ±ç»´åº¦è®¾è®¡ï¼Œè¯æ˜ä¸äººç±»åå¥½å¯¹é½çš„é‡è¦æ€§ï¼›ç ”ç©¶GRPOå˜ä½“ï¼Œå¼ºè°ƒä»¤ç‰Œçº§ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ï¼›å¼•å…¥MME-3DRåŸºå‡†æµ‹è¯•ï¼›æå‡ºHi-GRPOåˆ†å±‚å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡ä¸“ç”¨å¥–åŠ±é›†æˆä¼˜åŒ–ä»å…¨å±€åˆ°å±€éƒ¨çš„åˆ†å±‚3Dç”Ÿæˆï¼Œæœ€ç»ˆå¼€å‘äº†AR3D-R1æ¨¡å‹ã€‚

**Result:** å®éªŒè¡¨æ˜é€šç”¨å¤šæ¨¡æ€æ¨¡å‹ä¸º3Då±æ€§æä¾›ç¨³å¥ä¿¡å·ï¼Œä»¤ç‰Œçº§ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼ŒHi-GRPOæ–¹æ³•æœ‰æ•ˆå¤„ç†3Dç”Ÿæˆçš„è‡ªç„¶å±‚æ¬¡ç»“æ„ï¼ŒAR3D-R1æˆä¸ºé¦–ä¸ªRLå¢å¼ºçš„æ–‡æœ¬åˆ°3Dæ¨¡å‹ï¼Œå®ç°äº†ä»ç²—ç³™å½¢çŠ¶åˆ°çº¹ç†ç»†åŒ–çš„ä¸“ä¸šç”Ÿæˆèƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºRLé©±åŠ¨çš„3Dç”Ÿæˆæ¨ç†æä¾›äº†é‡è¦è§è§£ï¼Œè¯æ˜äº†å¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚3Dç”Ÿæˆä»»åŠ¡ä¸­çš„å¯è¡Œæ€§ï¼Œæå‡ºçš„åˆ†å±‚ä¼˜åŒ–æ–¹æ³•å’Œç³»ç»Ÿæ€§åˆ†ææ¡†æ¶ä¸ºæœªæ¥3Dç”Ÿæˆç ”ç©¶å¥ å®šäº†ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ï¼Œæ¨åŠ¨äº†è‡ªå›å½’3Dç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.


### [2] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin, Runsen Xu, Shaohao Zhu, Sihan Yang, Peizhou Cao, Yunlong Ran, Miao Hu, Chenming Zhu, Yiman Xie, Yilin Long, Wenbo Hu, Dahua Lin, Tai Wang, Jiangmiao Pang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MMSI-Video-Benchï¼Œè¿™æ˜¯é¦–ä¸ªå…¨é¢è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è§†é¢‘ç©ºé—´æ™ºèƒ½çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1,106ä¸ªåŸºäº1,278ä¸ªè§†é¢‘ç‰‡æ®µçš„é—®é¢˜ï¼Œæ¶µç›–æ„ŸçŸ¥ã€è§„åˆ’ã€é¢„æµ‹å’Œè·¨è§†é¢‘æ¨ç†å››ä¸ªå±‚æ¬¡ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹ä¸äººç±»èƒ½åŠ›ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰ç¼ºä¹å…¨é¢è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¿ç»­è§†è§‰è¾“å…¥ä¸­ç©ºé—´ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œè¿™å¯¹äºæ¨¡å‹å‘å±•ä¸ºç‰©ç†ç¯å¢ƒä¸­çš„é€šç”¨åŠ©æ‰‹è‡³å…³é‡è¦ã€‚ç°æœ‰è¯„ä¼°ä½“ç³»æœªèƒ½ä»æ•´ä½“ä¸Šè¡¡é‡æ¨¡å‹åœ¨è§†é¢‘ç©ºé—´æ™ºèƒ½æ–¹é¢çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯ç¼ºä¹ç³»ç»Ÿæ€§çš„å¤šå±‚æ¬¡èƒ½åŠ›è¯„ä¼°æ¡†æ¶ã€‚

**Method:** ç ”ç©¶æå‡ºäº†MMSI-Video-BenchåŸºå‡†æµ‹è¯•ï¼Œé‡‡ç”¨å››å±‚æ¬¡æ¡†æ¶ï¼ˆæ„ŸçŸ¥ã€è§„åˆ’ã€é¢„æµ‹ã€è·¨è§†é¢‘æ¨ç†ï¼‰ï¼ŒåŒ…å«1,106ä¸ªäººå·¥æ ‡æ³¨çš„é—®é¢˜ï¼ŒåŸºäºæ¥è‡ª25ä¸ªæ•°æ®é›†å’Œå†…éƒ¨è§†é¢‘çš„1,278ä¸ªç‰‡æ®µã€‚æ¯ä¸ªé¡¹ç›®ç”±3DVä¸“å®¶ç²¾å¿ƒè®¾è®¡å’Œå®¡æŸ¥ï¼Œç¡®ä¿ç²¾ç¡®æ— æ­§ä¹‰çš„æ¥åœ°æ€§ï¼Œå¹¶æ”¯æŒä¸‰ä¸ªé¢†åŸŸå¯¼å‘çš„å­åŸºå‡†ï¼ˆå®¤å†…åœºæ™¯æ„ŸçŸ¥ã€æœºå™¨äººå’Œæ¥åœ°åŸºå‡†ï¼‰è¿›è¡Œé’ˆå¯¹æ€§èƒ½åŠ›è¯„ä¼°ã€‚

**Result:** è¯„ä¼°äº†25ä¸ªå¼€æºå’Œä¸“æœ‰MLLMæ¨¡å‹ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„äººæœºå·®è·ï¼šè®¸å¤šæ¨¡å‹è¡¨ç°æ¥è¿‘éšæœºæ°´å¹³ï¼Œæœ€ä½³æ¨ç†æ¨¡å‹è½åäººç±»è¿‘60%ã€‚ç©ºé—´å¾®è°ƒæ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸Šæ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œç»†ç²’åº¦é”™è¯¯åˆ†ææš´éœ²äº†å‡ ä½•æ¨ç†ã€è¿åŠ¨æ¥åœ°ã€é•¿æ—¶ç¨‹é¢„æµ‹å’Œè·¨è§†é¢‘å¯¹åº”æ–¹é¢çš„ç³»ç»Ÿæ€§å¤±è´¥ã€‚å…¸å‹å¸§é‡‡æ ·ç­–ç•¥åœ¨æ¨ç†å¯†é›†å‹åŸºå‡†ä¸Šè¿ç§»æ•ˆæœå·®ï¼Œ3Dç©ºé—´çº¿ç´¢å’Œæ€ç»´é“¾æç¤ºä¹Ÿæœªå¸¦æ¥æ˜¾è‘—æå‡ã€‚

**Conclusion:** è¯¥åŸºå‡†æµ‹è¯•ä¸ºæ¨è¿›è§†é¢‘ç©ºé—´æ™ºèƒ½ç ”ç©¶æä¾›äº†åšå®çš„æµ‹è¯•å¹³å°ï¼Œæ­ç¤ºäº†å½“å‰MLLMåœ¨ç©ºé—´ç†è§£æ–¹é¢çš„æ ¸å¿ƒå±€é™æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜éœ€è¦å¼€å‘æ›´æœ‰æ•ˆçš„ç©ºé—´è¡¨ç¤ºå­¦ä¹ æ–¹æ³•å’Œæ¨ç†æ¶æ„ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¾èµ–å¾®è°ƒæˆ–æç¤ºå·¥ç¨‹ã€‚åŸºå‡†çš„å¤šå±‚æ¬¡æ¡†æ¶å’Œç»†ç²’åº¦é”™è¯¯åˆ†æä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†å…·ä½“æŒ‡å¯¼ã€‚

---

#### ğŸ“„ Abstract
Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.


### [3] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang, Wenqi Wang, Zecheng Wang, Max Whitton, Michael Wakeham, Arjun Chandra, Joey Huang, Pengyue Zhu, Helen Chen, David Li, Jeffrey Li, Shawn Li, Andrew Zagula, Amy Zhao, Andrew Zhu, Sayaka Nakamura, Yuki Yamamoto, Jerry Jun Yokono, Aaron Mueller, Bryan A. Plummer, Kate Saenko, Venkatesh Saligrama, Boqing Gong*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶æå‡ºäº†BabyVLM-V2æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå„¿ç«¥å‘å±•è½¨è¿¹çš„è§†è§‰è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å¼€å‘è®¤çŸ¥è¯„ä¼°å·¥å…·ç®±DevCVå’Œçºµå‘å¤šæ¨¡æ€é¢„è®­ç»ƒæ•°æ®é›†ï¼Œå®ç°äº†ä»é›¶å¼€å§‹è®­ç»ƒçš„ç´§å‡‘æ¨¡å‹åœ¨å„¿ç«¥è®¤çŸ¥ä»»åŠ¡ä¸Šçš„ç«äº‰æ€§è¡¨ç°ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æ—©æœŸå„¿ç«¥å‘å±•è½¨è¿¹ä¸ºè§†è§‰åŸºç¡€æ¨¡å‹çš„æ ·æœ¬é«˜æ•ˆé¢„è®­ç»ƒæä¾›äº†è‡ªç„¶ç›®æ ‡ï¼Œä½†ç°æœ‰æ–¹æ³•ç¼ºä¹ä¸å„¿ç«¥è®¤çŸ¥èƒ½åŠ›å¯¹é½çš„ç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ï¼Œéœ€è¦å¼€å‘æ›´ç¬¦åˆå‘å±•å¿ƒç†å­¦çš„é¢„è®­ç»ƒå’Œè¯„ä¼°æ–¹æ³•ã€‚

**Method:** ç ”ç©¶æå‡ºäº†BabyVLM-V2æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šçºµå‘å¤šæ–¹é¢çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæœ€å¤§åŒ–è¦†ç›–å©´å„¿ä¸­å¿ƒè§†å¬è¯­æ–™åº“ï¼›é€šç”¨æ¨¡å‹æ¶æ„ï¼›ä»¥åŠDevCVå·¥å…·ç®±ï¼Œå°†NIH Baby Toolboxçš„è§†è§‰ç›¸å…³æµ‹é‡é€‚é…ä¸ºåä¸ªå¤šæ¨¡æ€ä»»åŠ¡çš„åŸºå‡†å¥—ä»¶ï¼Œæ¶µç›–ç©ºé—´æ¨ç†ã€è®°å¿†å’Œè¯æ±‡ç†è§£ç­‰æ—©æœŸå„¿ç«¥èƒ½åŠ›ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œä»é›¶å¼€å§‹é¢„è®­ç»ƒçš„ç´§å‡‘æ¨¡å‹åœ¨DevCVå·¥å…·ç®±ä¸Šèƒ½å¤Ÿè¾¾åˆ°ç«äº‰æ€§æ€§èƒ½ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šç”šè‡³è¶…è¶Šäº†GPT-4oçš„è¡¨ç°ï¼ŒéªŒè¯äº†å‘å±•æ€§é¢„è®­ç»ƒæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** BabyVLM-V2æ¡†æ¶ä¸ºå‘å±•æ€§åˆç†çš„è§†è§‰åŸºç¡€æ¨¡å‹é¢„è®­ç»ƒç ”ç©¶æä¾›äº†åŸåˆ™æ€§ç»Ÿä¸€æ–¹æ³•ï¼Œæœ‰æœ›åŠ é€Ÿè¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œå¹¶ä¸ºç†è§£äººç±»è®¤çŸ¥å‘å±•æä¾›è®¡ç®—æ¨¡å‹æ”¯æŒã€‚

---

#### ğŸ“„ Abstract
Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.


### [4] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray, Ahmed Abdelkader, Chengzhi Mao, Bryan A. Plummer, Kate Saenko, Ranjay Krishna, Leonidas Guibas, Wen-Sheng Chu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºMull-Tokensâ€”â€”ä¸€ç§æ¨¡æ€æ— å…³çš„æ½œåœ¨ä»¤ç‰Œï¼Œé€šè¿‡é¢„è®­ç»ƒåœ¨å›¾åƒæˆ–æ–‡æœ¬æ¨¡æ€ä¸­ä¿å­˜ä¸­é—´ä¿¡æ¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªç”±æ€è€ƒä»¥è·å¾—æ­£ç¡®ç­”æ¡ˆï¼Œä»è€Œåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å®ç°æ˜¾è‘—æ€§èƒ½æå‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨ç©ºé—´ã€æ—¶é—´ã€åŠŸèƒ½ç­‰çœŸå®ä¸–ç•Œæ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨å±€é™æ€§ï¼Œå®ƒä»¬ä¾èµ–è°ƒç”¨ä¸“ä¸šå·¥å…·ã€æ˜‚è´µçš„å›¾åƒç”Ÿæˆæˆ–æ‰‹å·¥åˆ¶ä½œçš„æ¨ç†æ•°æ®åœ¨æ–‡æœ¬å’Œå›¾åƒæ€ç»´é—´åˆ‡æ¢ï¼Œè¿™äº›æ–¹æ³•è„†å¼±ä¸”éš¾ä»¥æ‰©å±•ï¼Œæ— æ³•æœ‰æ•ˆæ”¯æŒè·¨æ¨¡æ€çš„è‡ªç”±æ¨ç†è¿‡ç¨‹ã€‚

**Method:** æœ¬æ–‡æå‡ºMull-Tokensæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡æ€æ— å…³çš„æ½œåœ¨ä»¤ç‰Œï¼Œé€šè¿‡é¢„è®­ç»ƒåœ¨å›¾åƒæˆ–æ–‡æœ¬æ¨¡æ€ä¸­ä¿å­˜ä¸­é—´æ¨ç†ä¿¡æ¯ã€‚è¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨äº¤é”™æ–‡æœ¬-å›¾åƒè½¨è¿¹è¿›è¡Œç›‘ç£è®­ç»ƒï¼Œç„¶åä»…ä½¿ç”¨æœ€ç»ˆç­”æ¡ˆè¿›è¡Œæ— ç›‘ç£å¾®è°ƒï¼Œå€Ÿé‰´äº†æ½œåœ¨æ¨ç†æ¡†æ¶çš„æœ€ä½³å®è·µï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§æ¨¡æ€ä¸­æŠ½è±¡æ€è€ƒã€‚

**Result:** åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŒ…æ‹¬è§£å†³è°œé¢˜å’Œé‡‡å–ä¸åŒè§†è§’ç­‰ä»»åŠ¡ï¼ŒMull-Tokensç›¸æ¯”ä»…ä½¿ç”¨æ–‡æœ¬æ¨ç†æˆ–äº¤é”™å›¾åƒ-æ–‡æœ¬æ¨ç†çš„å¤šä¸ªåŸºçº¿æ–¹æ³•å®ç°äº†å¹³å‡+3%çš„æ€§èƒ½æå‡ï¼Œåœ¨æ¨ç†å¯†é›†çš„è°œé¢˜è§£å†³ä»»åŠ¡ä¸­æœ€é«˜æå‡è¾¾+16%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

**Conclusion:** Mull-Tokensä¸ºæ–‡æœ¬å’Œè§†è§‰æ¨ç†çš„è½åœ°æŒ‘æˆ˜æä¾›äº†ç®€æ´è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ¨¡æ€æ— å…³çš„æ½œåœ¨è¡¨ç¤ºå®ç°äº†è·¨æ¨¡æ€çš„æŠ½è±¡æ€è€ƒèƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿçš„å‘å±•å¼€è¾Ÿäº†æ–°æ–¹å‘ï¼Œå±•ç¤ºäº†åœ¨å¤æ‚ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­è¶…è¶Šä¼ ç»Ÿæ–¹æ³•çš„æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [5] [Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882)
*Hauke Licht*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æƒ…æ„Ÿåˆ†æä¸­çš„æœ‰æ•ˆæ€§ï¼Œå‘ç°åœ¨ç†æƒ³æ¡ä»¶ä¸‹æ¨¡å‹è¡¨ç°å¯é ä¸”æ— äººå£ç»Ÿè®¡åå·®ï¼Œä½†åœ¨çœŸå®è®®ä¼šè¾©è®ºåœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†æŒç»­è¯„ä¼°ç”Ÿæˆå¼AIåœ¨æ”¿æ²»åˆ†æä¸­åº”ç”¨çš„å¿…è¦æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤šæ¨¡æ€ç”Ÿæˆå¼AIåœ¨æƒ…æ„Ÿåˆ†æé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç¼ºä¹å…³äºå…¶åœ¨æ”¿æ²»æ²Ÿé€šæƒ…æ„Ÿåˆ†æä¸­æœ‰æ•ˆæ€§çš„å®è¯è¯æ®ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æƒ…æ„Ÿå”¤èµ·åˆ†æä¸­çš„å®é™…è¡¨ç°ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹ä¸¤ä¸ªäº’è¡¥æ•°æ®é›†è¿›è¡Œè§†é¢‘æƒ…æ„Ÿå”¤èµ·åˆ†æï¼Œè¿™ä¸¤ä¸ªæ•°æ®é›†åŒ…å«äººå·¥æ ‡æ³¨çš„è§†é¢‘è®°å½•ï¼Œé€šè¿‡ç³»ç»Ÿè¯„ä¼°æ¡†æ¶æ¯”è¾ƒæ¨¡å‹è¾“å‡ºä¸äººç±»æ ‡æ³¨ç»“æœï¼Œç‰¹åˆ«å…³æ³¨æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°å·®å¼‚ã€‚

**Result:** åœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æƒ…æ„Ÿå”¤èµ·è¯„åˆ†å…·æœ‰é«˜åº¦å¯é æ€§ï¼Œä¸”æœªæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„äººå£ç»Ÿè®¡åå·®ï¼›ç„¶è€Œåœ¨çœŸå®è®®ä¼šè¾©è®ºåœºæ™¯ä¸­ï¼Œæ¨¡å‹çš„å”¤èµ·è¯„åˆ†è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œå¯èƒ½å¯¹ä¸‹æ¸¸ç»Ÿè®¡æ¨æ–­äº§ç”Ÿè´Ÿé¢å½±å“ã€‚

**Conclusion:** ç ”ç©¶å¼ºè°ƒäº†æŒç»­æ·±å…¥è¯„ä¼°æ–°å…´ç”Ÿæˆå¼AIæ–¹æ³•åœ¨æ”¿æ²»åˆ†æä¸­åº”ç”¨çš„å¿…è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªå¯å¤åˆ¶çš„è¯„ä¼°æ¡†æ¶ï¼Œè¡¨æ˜å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤æ‚ç°å®æ”¿æ²»åœºæ™¯ä¸­çš„è¡¨ç°ä»å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ã€‚

---

#### ğŸ“„ Abstract
Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.
