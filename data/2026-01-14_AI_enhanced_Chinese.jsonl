{"id": "2601.07984", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07984", "abs": "https://arxiv.org/abs/2601.07984", "authors": ["Haorui Yu", "Ramon Ruiz-Dolz", "Xuehang Wen", "Fengrui Zhang", "Qiufeng Yi"], "title": "Cross-Cultural Expert-Level Art Critique Evaluation with Vision-Language Models", "comment": "16 pages, 7 figures, submitted to ACL 2026", "summary": "Vision-Language Models (VLMs) excel at visual perception, yet their ability to interpret cultural meaning in art remains under-validated. We present a tri-tier evaluation framework for cross-cultural art-critique assessment: Tier I computes automated coverage and risk indicators offline; Tier II applies rubric-based scoring using a single primary judge across five dimensions; and Tier III calibrates the Tier II aggregate score to human ratings via isotonic regression, yielding a 5.2% reduction in MAE on a 152-sample held-out set. The framework outputs a calibrated cultural-understanding score for model selection and cultural-gap diagnosis, together with dimension-level diagnostics and risk indicators. We evaluate 15 VLMs on 294 expert anchors spanning six cultural traditions. Key findings are that (i) automated metrics are unreliable proxies for cultural depth, (ii) Western samples score higher than non-Western samples under our sampling and rubric, and (iii) cross-judge scale mismatch makes naive score averaging unreliable, motivating a single primary judge with explicit calibration. Dataset and code are available in the supplementary materials.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u5c42\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6587\u5316\u827a\u672f\u6279\u8bc4\u4e2d\u7684\u6587\u5316\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u6821\u51c6\u8bc4\u5206\u673a\u5236\u663e\u8457\u964d\u4f4e\u4e86\u4e0e\u4eba\u7c7b\u8bc4\u5206\u7684\u8bef\u5dee\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u81ea\u52a8\u6307\u6807\u5728\u8861\u91cf\u6587\u5316\u6df1\u5ea6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u827a\u672f\u4f5c\u54c1\u4e2d\u89e3\u8bfb\u6587\u5316\u610f\u4e49\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u8de8\u6587\u5316\u827a\u672f\u6279\u8bc4\u4e2d\u6587\u5316\u7406\u89e3\u6df1\u5ea6\u7684\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u5c42\u8bc4\u4f30\u6846\u67b6\uff1a\u7b2c\u4e00\u5c42\u79bb\u7ebf\u8ba1\u7b97\u81ea\u52a8\u8986\u76d6\u7387\u548c\u98ce\u9669\u6307\u6807\uff1b\u7b2c\u4e8c\u5c42\u91c7\u7528\u5355\u4e00\u4e3b\u8bc4\u5ba1\u5458\u57fa\u4e8e\u4e94\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u57fa\u4e8e\u91cf\u89c4\u7684\u8bc4\u5206\uff1b\u7b2c\u4e09\u5c42\u901a\u8fc7\u4fdd\u5e8f\u56de\u5f52\u5c06\u7b2c\u4e8c\u5c42\u805a\u5408\u5206\u6570\u6821\u51c6\u5230\u4eba\u7c7b\u8bc4\u5206\uff0c\u4ece\u800c\u4ea7\u751f\u6821\u51c6\u540e\u7684\u6587\u5316\u7406\u89e3\u5206\u6570\u3002", "result": "\u8be5\u6846\u67b6\u5728152\u4e2a\u6837\u672c\u7684\u4fdd\u7559\u96c6\u4e0a\u5b9e\u73b0\u4e865.2%\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e\uff0c\u8bc4\u4f30\u4e8615\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6db5\u76d6\u516d\u79cd\u6587\u5316\u4f20\u7edf\u7684294\u4e2a\u4e13\u5bb6\u951a\u70b9\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u81ea\u52a8\u6307\u6807\u65e0\u6cd5\u53ef\u9760\u4ee3\u7406\u6587\u5316\u6df1\u5ea6\uff0c\u897f\u65b9\u6837\u672c\u5f97\u5206\u9ad8\u4e8e\u975e\u897f\u65b9\u6837\u672c\uff0c\u4e14\u8de8\u8bc4\u5ba1\u5458\u5c3a\u5ea6\u4e0d\u5339\u914d\u4f7f\u5f97\u6734\u7d20\u5e73\u5747\u8bc4\u5206\u4e0d\u53ef\u9760\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9700\u8981\u4e13\u95e8\u7684\u6587\u5316\u7406\u89e3\u8bc4\u4f30\u6846\u67b6\u800c\u975e\u4f9d\u8d56\u901a\u7528\u81ea\u52a8\u6307\u6807\uff0c\u5355\u4e00\u4e3b\u8bc4\u5ba1\u5458\u914d\u5408\u663e\u5f0f\u6821\u51c6\u7684\u65b9\u6cd5\u66f4\u53ef\u9760\uff0c\u6846\u67b6\u8f93\u51fa\u7684\u6821\u51c6\u5206\u6570\u53ef\u7528\u4e8e\u6a21\u578b\u9009\u62e9\u548c\u6587\u5316\u5dee\u8ddd\u8bca\u65ad\uff0c\u4e3a\u8de8\u6587\u5316\u827a\u672fAI\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2601.07985", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.07985", "abs": "https://arxiv.org/abs/2601.07985", "authors": ["Z. Melce H\u00fcs\u00fcnbeyi", "Virginie Mouilleron", "Leonie Uhling", "Daniel Foppe", "Tatjana Scheffler", "Djam\u00e9 Seddah"], "title": "Multilingual, Multimodal Pipeline for Creating Authentic and Structured Fact-Checked Claim Dataset", "comment": null, "summary": "The rapid proliferation of misinformation across online platforms underscores the urgent need for robust, up-to-date, explainable, and multilingual fact-checking resources. However, existing datasets are limited in scope, often lacking multimodal evidence, structured annotations, and detailed links between claims, evidence, and verdicts. This paper introduces a comprehensive data collection and processing pipeline that constructs multimodal fact-checking datasets in French and German languages by aggregating ClaimReview feeds, scraping full debunking articles, normalizing heterogeneous claim verdicts, and enriching them with structured metadata and aligned visual content. We used state-of-the-art large language models (LLMs) and multimodal LLMs for (i) evidence extraction under predefined evidence categories and (ii) justification generation that links evidence to verdicts. Evaluation with G-Eval and human assessment demonstrates that our pipeline enables fine-grained comparison of fact-checking practices across different organizations or media markets, facilitates the development of more interpretable and evidence-grounded fact-checking models, and lays the groundwork for future research on multilingual, multimodal misinformation verification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6784\u5efa\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\u7684\u6570\u636e\u6536\u96c6\u4e0e\u5904\u7406\u6d41\u7a0b\uff0c\u901a\u8fc7\u805a\u5408ClaimReview\u6e90\u3001\u6293\u53d6\u5b8c\u6574\u8f9f\u8c23\u6587\u7ae0\u3001\u89c4\u8303\u5316\u5f02\u6784\u58f0\u660e\u88c1\u51b3\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc1\u636e\u63d0\u53d6\u548c\u7406\u7531\u751f\u6210\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u4e8b\u5b9e\u6838\u67e5\u6a21\u578b\u5f00\u53d1\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u5728\u7ebf\u5e73\u53f0\u4e0a\u7684\u865a\u5047\u4fe1\u606f\u5feb\u901f\u4f20\u64ad\u51f8\u663e\u4e86\u5bf9\u5f3a\u5927\u3001\u6700\u65b0\u3001\u53ef\u89e3\u91ca\u4e14\u591a\u8bed\u8a00\u4e8b\u5b9e\u6838\u67e5\u8d44\u6e90\u7684\u8feb\u5207\u9700\u6c42\uff0c\u7136\u800c\u73b0\u6709\u6570\u636e\u96c6\u5728\u8303\u56f4\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u901a\u5e38\u7f3a\u4e4f\u591a\u6a21\u6001\u8bc1\u636e\u3001\u7ed3\u6784\u5316\u6807\u6ce8\u4ee5\u53ca\u58f0\u660e\u3001\u8bc1\u636e\u4e0e\u88c1\u51b3\u4e4b\u95f4\u7684\u8be6\u7ec6\u5173\u8054\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u6536\u96c6\u4e0e\u5904\u7406\u6d41\u7a0b\uff0c\u901a\u8fc7\u805a\u5408ClaimReview\u6e90\u3001\u6293\u53d6\u5b8c\u6574\u8f9f\u8c23\u6587\u7ae0\u3001\u89c4\u8303\u5316\u5f02\u6784\u58f0\u660e\u88c1\u51b3\uff0c\u5e76\u5229\u7528\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc1\u636e\u63d0\u53d6\u548c\u7406\u7531\u751f\u6210\uff0c\u540c\u65f6\u4e3a\u6570\u636e\u6dfb\u52a0\u7ed3\u6784\u5316\u5143\u6570\u636e\u548c\u5bf9\u9f50\u7684\u89c6\u89c9\u5185\u5bb9\u3002", "result": "\u901a\u8fc7G-Eval\u548c\u4eba\u5de5\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6d41\u7a0b\u80fd\u591f\u5b9e\u73b0\u5bf9\u4e0d\u540c\u7ec4\u7ec7\u6216\u5a92\u4f53\u5e02\u573a\u4e4b\u95f4\u4e8b\u5b9e\u6838\u67e5\u5b9e\u8df5\u7684\u7ec6\u7c92\u5ea6\u6bd4\u8f83\uff0c\u4fc3\u8fdb\u5f00\u53d1\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u8bc1\u636e\u57fa\u7840\u7684\u4e8b\u5b9e\u6838\u67e5\u6a21\u578b\uff0c\u5e76\u4e3a\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u9a8c\u8bc1\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u5168\u9762\u7684\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bc1\u636e\u63d0\u53d6\u548c\u7406\u7531\u751f\u6210\u589e\u5f3a\u4e86\u4e8b\u5b9e\u6838\u67e5\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u548c\u8de8\u8bed\u8a00\u3001\u8de8\u5a92\u4f53\u5e02\u573a\u7684\u6bd4\u8f83\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2601.07986", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.07986", "abs": "https://arxiv.org/abs/2601.07986", "authors": ["Haorui Yu", "Ramon Ruiz-Dolz", "Diji Yang", "Hang He", "Fengrui Zhang", "Qiufeng Yi"], "title": "VULCA-Bench: A Multicultural Vision-Language Benchmark for Evaluating Cultural Understanding", "comment": "8 pages, 4 figures, submitted to ACL 2026 Dataset Track", "summary": "We introduce VULCA-Bench, a multicultural art-critique benchmark for evaluating Vision-Language Models' (VLMs) cultural understanding beyond surface-level visual perception. Existing VLM benchmarks predominantly measure L1-L2 capabilities (object recognition, scene description, and factual question answering) while under-evaluate higher-order cultural interpretation. VULCA-Bench contains 7,410 matched image-critique pairs spanning eight cultural traditions, with Chinese-English bilingual coverage. We operationalise cultural understanding using a five-layer framework (L1-L5, from Visual Perception to Philosophical Aesthetics), instantiated as 225 culture-specific dimensions and supported by expert-written bilingual critiques. Our pilot results indicate that higher-layer reasoning (L3-L5) is consistently more challenging than visual and technical analysis (L1-L2). The dataset, evaluation scripts, and annotation tools are available under CC BY 4.0 in the supplementary materials.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VULCA-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6587\u5316\u7406\u89e3\u80fd\u529b\u7684\u591a\u6587\u5316\u827a\u672f\u8bc4\u8bba\u57fa\u51c6\uff0c\u8d85\u8d8a\u4e86\u8868\u9762\u89c6\u89c9\u611f\u77e5\uff0c\u5305\u542b7,410\u4e2a\u56fe\u50cf-\u8bc4\u8bba\u5bf9\uff0c\u6db5\u76d6\u516b\u79cd\u6587\u5316\u4f20\u7edf\uff0c\u5e76\u91c7\u7528\u4e94\u5c42\u6587\u5316\u7406\u89e3\u6846\u67b6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30L1-L2\u80fd\u529b\uff08\u7269\u4f53\u8bc6\u522b\u3001\u573a\u666f\u63cf\u8ff0\u548c\u4e8b\u5b9e\u95ee\u7b54\uff09\uff0c\u800c\u5ffd\u89c6\u4e86\u66f4\u9ad8\u5c42\u6b21\u7684\u6587\u5316\u89e3\u91ca\u80fd\u529b\uff0c\u5bfc\u81f4\u6a21\u578b\u6587\u5316\u7406\u89e3\u8bc4\u4f30\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5168\u9762\u8861\u91cf\u8de8\u6587\u5316\u573a\u666f\u4e0b\u7684\u6df1\u5ea6\u8ba4\u77e5\u8868\u73b0\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b7,410\u4e2a\u5339\u914d\u56fe\u50cf-\u8bc4\u8bba\u5bf9\u7684\u591a\u6587\u5316\u827a\u672f\u8bc4\u8bba\u57fa\u51c6\uff0c\u6db5\u76d6\u516b\u79cd\u6587\u5316\u4f20\u7edf\u5e76\u652f\u6301\u4e2d\u82f1\u53cc\u8bed\uff1b\u91c7\u7528\u4e94\u5c42\u6587\u5316\u7406\u89e3\u6846\u67b6\uff08L1-L5\uff0c\u4ece\u89c6\u89c9\u611f\u77e5\u5230\u54f2\u5b66\u7f8e\u5b66\uff09\uff0c\u5177\u4f53\u5316\u4e3a225\u4e2a\u6587\u5316\u7279\u5b9a\u7ef4\u5ea6\uff0c\u5e76\u7531\u4e13\u5bb6\u64b0\u5199\u53cc\u8bed\u8bc4\u8bba\u8fdb\u884c\u5b9e\u4f8b\u5316\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9ad8\u5c42\u6b21\u63a8\u7406\uff08L3-L5\uff09\u59cb\u7ec8\u6bd4\u89c6\u89c9\u548c\u6280\u672f\u5206\u6790\uff08L1-L2\uff09\u66f4\u5177\u6311\u6218\u6027\uff0c\u9a8c\u8bc1\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u6df1\u5ea6\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u811a\u672c\u548c\u6807\u6ce8\u5de5\u5177\u5df2\u901a\u8fc7CC BY 4.0\u8bb8\u53ef\u5728\u8865\u5145\u6750\u6599\u4e2d\u516c\u5f00\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6587\u5316\u7406\u89e3\u80fd\u529b\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u4e94\u5c42\u6846\u67b6\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u516c\u5f00\u7684\u6570\u636e\u96c6\u548c\u5de5\u5177\u5c06\u4fc3\u8fdb\u8de8\u6587\u5316AI\u7814\u7a76\uff0c\u63a8\u52a8\u6a21\u578b\u4ece\u8868\u9762\u611f\u77e5\u5411\u6df1\u5ea6\u6587\u5316\u89e3\u91ca\u53d1\u5c55\u3002"}}
{"id": "2601.08209", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08209", "abs": "https://arxiv.org/abs/2601.08209", "authors": ["Rongji Li", "Jian Xu", "Xueqing Chen", "Yisheng Yang", "Jiayi Wang", "Xingyu Chen", "Chunyu Xie", "Dawei Leng", "Xu-Yao Zhang"], "title": "Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models", "comment": null, "summary": "In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining. However, the two dominant paradigms for private knowledge injection each have pronounced drawbacks: fine-tuning is expensive to iterate, and continual updates risk catastrophic forgetting and general-capability regression; retrieval-augmented generation (RAG) keeps the base model intact but is brittle in specialized private corpora due to chunk-induced evidence fragmentation, retrieval drift, and long-context pressure that yields query-dependent prompt inflation. Inspired by how multimodal LLMs align heterogeneous modalities into a shared semantic space, we propose Generation-Augmented Generation (GAG), which treats private expertise as an additional expert modality and injects it via a compact, representation-level interface aligned to the frozen base model, avoiding prompt-time evidence serialization while enabling plug-and-play specialization and scalable multi-domain composition with reliable selective activation. Across two private scientific QA benchmarks (immunology adjuvant and catalytic materials) and mixed-domain evaluations, GAG improves specialist performance over strong RAG baselines by 15.34% and 14.86% on the two benchmarks, respectively, while maintaining performance on six open general benchmarks and enabling near-oracle selective activation for scalable multi-domain deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u751f\u6210\u589e\u5f3a\u751f\u6210\uff08GAG\uff09\u65b9\u6cd5\uff0c\u5c06\u79c1\u6709\u4e13\u4e1a\u77e5\u8bc6\u89c6\u4e3a\u4e13\u5bb6\u6a21\u6001\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u8868\u5f81\u7ea7\u63a5\u53e3\u5bf9\u9f50\u5230\u51bb\u7ed3\u7684\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u79c1\u6709\u77e5\u8bc6\u6ce8\u5165\u4e2d\u5fae\u8c03\u8fed\u4ee3\u6210\u672c\u9ad8\u548cRAG\u5728\u4e13\u4e1a\u8bed\u6599\u4e2d\u8106\u5f31\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u751f\u7269\u533b\u5b66\u3001\u6750\u6599\u548c\u91d1\u878d\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u9700\u8981\u6ce8\u5165\u79c1\u6709\u3001\u9886\u57df\u7279\u5b9a\u7684\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u77e5\u8bc6\u5177\u6709\u4e13\u6709\u6027\u3001\u5feb\u901f\u6f14\u53d8\u6027\u4e14\u5728\u516c\u5f00\u9884\u8bad\u7ec3\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u3002\u7136\u800c\uff0c\u5f53\u524d\u4e24\u79cd\u4e3b\u8981\u7684\u79c1\u6709\u77e5\u8bc6\u6ce8\u5165\u8303\u5f0f\u5404\u6709\u660e\u663e\u7f3a\u9677\uff1a\u5fae\u8c03\u8fed\u4ee3\u6210\u672c\u9ad8\u6602\u4e14\u6301\u7eed\u66f4\u65b0\u53ef\u80fd\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u548c\u901a\u7528\u80fd\u529b\u9000\u5316\uff1b\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u867d\u7136\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u5b8c\u6574\uff0c\u4f46\u5728\u4e13\u4e1a\u79c1\u6709\u8bed\u6599\u4e2d\u56e0\u5206\u5757\u5bfc\u81f4\u7684\u8bc1\u636e\u788e\u7247\u5316\u3001\u68c0\u7d22\u6f02\u79fb\u548c\u957f\u4e0a\u4e0b\u6587\u538b\u529b\u800c\u8868\u73b0\u8106\u5f31\u3002", "method": "\u53d7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u5f02\u6784\u6a21\u6001\u5bf9\u9f50\u5230\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u7684\u542f\u53d1\uff0c\u672c\u6587\u63d0\u51fa\u751f\u6210\u589e\u5f3a\u751f\u6210\uff08GAG\uff09\u65b9\u6cd5\uff0c\u5c06\u79c1\u6709\u4e13\u4e1a\u77e5\u8bc6\u89c6\u4e3a\u989d\u5916\u7684\u4e13\u5bb6\u6a21\u6001\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u8868\u5f81\u7ea7\u63a5\u53e3\u5bf9\u9f50\u5230\u51bb\u7ed3\u7684\u57fa\u7840\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u63d0\u793a\u65f6\u7684\u8bc1\u636e\u5e8f\u5217\u5316\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u5373\u63d2\u5373\u7528\u7684\u4e13\u4e1a\u5316\u4ee5\u53ca\u53ef\u9760\u9009\u62e9\u6027\u6fc0\u6d3b\u7684\u53ef\u6269\u5c55\u591a\u9886\u57df\u7ec4\u5408\u3002", "result": "\u5728\u4e24\u4e2a\u79c1\u6709\u79d1\u5b66\u95ee\u7b54\u57fa\u51c6\uff08\u514d\u75ab\u5b66\u4f50\u5242\u548c\u50ac\u5316\u6750\u6599\uff09\u4ee5\u53ca\u6df7\u5408\u9886\u57df\u8bc4\u4f30\u4e2d\uff0cGAG\u5728\u4e24\u4e2a\u57fa\u51c6\u4e0a\u5206\u522b\u6bd4\u5f3a\u5927\u7684RAG\u57fa\u7ebf\u63d0\u9ad8\u4e8615.34%\u548c14.86%\u7684\u4e13\u4e1a\u6027\u80fd\u3002\u540c\u65f6\uff0c\u5728\u516d\u4e2a\u5f00\u653e\u901a\u7528\u57fa\u51c6\u4e0a\u4fdd\u6301\u4e86\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u63a5\u8fd1oracle\u7684\u9009\u62e9\u6027\u6fc0\u6d3b\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u591a\u9886\u57df\u90e8\u7f72\u3002", "conclusion": "GAG\u65b9\u6cd5\u901a\u8fc7\u5c06\u79c1\u6709\u77e5\u8bc6\u4f5c\u4e3a\u4e13\u5bb6\u6a21\u6001\u6ce8\u5165\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u79c1\u6709\u77e5\u8bc6\u96c6\u6210\u65b9\u6848\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u5373\u63d2\u5373\u7528\u7684\u4e13\u4e1a\u5316\u548c\u53ef\u6269\u5c55\u7684\u591a\u9886\u57df\u7ec4\u5408\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5e73\u8861\u4e86\u4e13\u4e1a\u6027\u80fd\u4e0e\u901a\u7528\u80fd\u529b\u4fdd\u6301\u7684\u9700\u6c42\u3002"}}
{"id": "2601.07855", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07855", "abs": "https://arxiv.org/abs/2601.07855", "authors": ["Subeen Lee", "Siyeong Lee", "Namil Kim", "Jaesik Choi"], "title": "An Empirical Study on Knowledge Transfer under Domain and Label Shifts in 3D LiDAR Point Clouds", "comment": null, "summary": "For 3D perception systems to be practical in real-world applications -- from autonomous driving to embodied AI -- models must adapt to continuously evolving object definitions and sensor domains. Yet, research on continual and transfer learning in 3D point cloud perception remains underexplored compared to 2D vision -- particularly under simultaneous domain and label shifts. To address this gap, we propose the RObust Autonomous driving under Dataset shifts (ROAD) benchmark, a comprehensive evaluation suite for LiDAR-based object classification that explicitly accounts for domain shifts as well as three key forms of label evolution: class split, class expansion, and class insertion. Using large-scale datasets (Waymo, NuScenes, Argoverse2), we evaluate zero-shot transfer, linear probe, and CL, and analyze the impact of backbone architectures, training objectives, and CL methods. Our findings reveal limitations of existing approaches under realistic shifts and establish strong baselines for future research in robust 3D perception.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86ROAD\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LiDAR\u70b9\u4e91\u5206\u7c7b\u5728\u6301\u7eed\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u5173\u6ce8\u540c\u65f6\u53d1\u751f\u7684\u9886\u57df\u504f\u79fb\u548c\u6807\u7b7e\u6f14\u5316\uff0c\u586b\u8865\u4e863D\u611f\u77e5\u7cfb\u7edf\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u9002\u5e94\u6027\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "3D\u611f\u77e5\u7cfb\u7edf\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eabAI\u7b49\u73b0\u5b9e\u5e94\u7528\u4e2d\u9700\u8981\u9002\u5e94\u4e0d\u65ad\u6f14\u5316\u7684\u7269\u4f53\u5b9a\u4e49\u548c\u4f20\u611f\u5668\u9886\u57df\uff0c\u7136\u800c\u4e0e2D\u89c6\u89c9\u76f8\u6bd4\uff0c3D\u70b9\u4e91\u611f\u77e5\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u540c\u65f6\u9762\u4e34\u9886\u57df\u504f\u79fb\u548c\u6807\u7b7e\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u9700\u8981\u88ab\u586b\u8865\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86ROAD\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3aLiDAR\u70b9\u4e91\u5206\u7c7b\u8bbe\u8ba1\u7684\u7efc\u5408\u8bc4\u4f30\u5957\u4ef6\uff0c\u660e\u786e\u8003\u8651\u4e86\u9886\u57df\u504f\u79fb\u4ee5\u53ca\u4e09\u79cd\u5173\u952e\u6807\u7b7e\u6f14\u5316\u5f62\u5f0f\uff1a\u7c7b\u522b\u5206\u88c2\u3001\u7c7b\u522b\u6269\u5c55\u548c\u7c7b\u522b\u63d2\u5165\u3002\u7814\u7a76\u4f7f\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08Waymo\u3001NuScenes\u3001Argoverse2\uff09\u8bc4\u4f30\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u3001\u7ebf\u6027\u63a2\u6d4b\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u9aa8\u5e72\u7f51\u7edc\u67b6\u6784\u3001\u8bad\u7ec3\u76ee\u6807\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u73b0\u5b9e\u504f\u79fb\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u540c\u65f6\u5904\u7406\u9886\u57df\u548c\u6807\u7b7e\u53d8\u5316\u65f6\u7684\u6027\u80fd\u4e0d\u8db3\u3002\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\uff0c\u7814\u7a76\u5efa\u7acb\u4e86\u672a\u6765\u9c81\u68d23D\u611f\u77e5\u7814\u7a76\u7684\u5f3a\u57fa\u7ebf\uff0c\u4e3a\u4e0d\u540c\u67b6\u6784\u548c\u65b9\u6cd5\u5728\u590d\u6742\u6f14\u5316\u573a\u666f\u4e0b\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u91cf\u5316\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e863D\u611f\u77e5\u7cfb\u7edf\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u9002\u5e94\u6301\u7eed\u53d8\u5316\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u5904\u7406\u540c\u65f6\u53d1\u751f\u7684\u9886\u57df\u548c\u6807\u7b7e\u504f\u79fb\u65f6\u7684\u4e0d\u8db3\u3002ROAD\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u9c81\u68d2\u76843D\u611f\u77e5\u6a21\u578b\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u3002"}}
{"id": "2601.08052", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08052", "abs": "https://arxiv.org/abs/2601.08052", "authors": ["Nawazish Alia", "Rachael Shawb", "Karl Mason"], "title": "Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms", "comment": null, "summary": "Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5976\u725b\u573a\u9ad8\u6548\u8d1f\u8377\u8c03\u5ea6\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u77ed\u671f\u9884\u6d4b\u548c\u81ea\u9002\u5e94KL\u6563\u5ea6\u63a7\u5236\uff0c\u5728\u52a8\u6001\u7535\u4ef7\u548c\u53ef\u518d\u751f\u80fd\u6e90\u95f4\u6b47\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0\u6210\u672c\u6700\u5c0f\u5316\u3002", "motivation": "\u5976\u725b\u573a\u4f5c\u4e3a\u9ad8\u80fd\u8017\u884c\u4e1a\u4e25\u91cd\u4f9d\u8d56\u7535\u7f51\u4f9b\u7535\uff0c\u53ef\u518d\u751f\u80fd\u6e90\u7684\u95f4\u6b47\u6027\u7ed9\u5b9e\u65f6\u4f9b\u9700\u5e73\u8861\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u8c03\u5ea6\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5b8c\u5168\u77e5\u6653\u672a\u6765\u7535\u4ef7\u6216\u53d1\u7535\u91cf\uff0c\u8fd9\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4e0d\u5207\u5b9e\u9645\uff0c\u4e14\u6807\u51c6PPO\u53d8\u4f53\u4f9d\u8d56\u56fa\u5b9a\u88c1\u526a\u6216KL\u6563\u5ea6\u9608\u503c\uff0c\u5728\u53ef\u53d8\u7535\u4ef7\u4e0b\u5e38\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u7535\u6c60\u50a8\u80fd\u548c\u70ed\u6c34\u8d1f\u8377\u8c03\u5ea6\u3002\u5176\u4e2dForecast Aware PPO\u6574\u5408\u4e86\u57fa\u4e8e\u5c0f\u65f6\u548c\u6708\u4efd\u7684\u6b8b\u5dee\u6821\u51c6\u77ed\u671f\u9700\u6c42\u4e0e\u53ef\u518d\u751f\u80fd\u6e90\u9884\u6d4b\uff0c\u800cPID KL PPO\u53d8\u4f53\u91c7\u7528\u6bd4\u4f8b\u79ef\u5206\u5fae\u5206\u63a7\u5236\u5668\u81ea\u9002\u5e94\u8c03\u8282KL\u6563\u5ea6\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u5728\u771f\u5b9e\u5976\u725b\u573a\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u6807\u51c6PPO\u964d\u4f4e1%\u7684\u7535\u8d39\u6210\u672c\uff0c\u6bd4DQN\u964d\u4f4e4.8%\uff0c\u6bd4SAC\u964d\u4f4e1.5%\u3002\u5728\u7535\u6c60\u8c03\u5ea6\u65b9\u9762\uff0cPPO\u51cf\u5c11\u4e8613.1%\u7684\u7535\u7f51\u8f93\u5165\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u73b0\u4ee3\u5976\u725b\u573a\u53ef\u6301\u7eed\u80fd\u6e90\u7ba1\u7406\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6574\u5408\u9884\u6d4b\u548c\u81ea\u9002\u5e94KL\u63a7\u5236\u673a\u5236\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e0b\u7684\u80fd\u6e90\u8c03\u5ea6\u95ee\u9898\uff0c\u4e3a\u519c\u4e1a\u9ad8\u80fd\u8017\u884c\u4e1a\u7684\u53ef\u6301\u7eed\u80fd\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u68077\u7684\u5b9e\u73b0\u3002"}}
{"id": "2601.08308", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08308", "abs": "https://arxiv.org/abs/2601.08308", "authors": ["Bo Yang", "Yu Zhang", "Yunkui Chen", "Lanfei Feng", "Xiao Xu", "Nueraili Aierken", "Shijian Li"], "title": "AgriAgent: Contract-Driven Planning and Capability-Aware Tool Orchestration in Real-World Agriculture", "comment": null, "summary": "Intelligent agent systems in real-world agricultural scenarios must handle diverse tasks under multimodal inputs, ranging from lightweight information understanding to complex multi-step execution. However, most existing approaches rely on a unified execution paradigm, which struggles to accommodate large variations in task complexity and incomplete tool availability commonly observed in agricultural environments. To address this challenge, we propose AgriAgent, a two-level agent framework for real-world agriculture. AgriAgent adopts a hierarchical execution strategy based on task complexity: simple tasks are handled through direct reasoning by modality-specific agents, while complex tasks trigger a contract-driven planning mechanism that formulates tasks as capability requirements and performs capability-aware tool orchestration and dynamic tool generation, enabling multi-step and verifiable execution with failure recovery. Experimental results show that AgriAgent achieves higher execution success rates and robustness on complex tasks compared to existing tool-centric agent baselines that rely on unified execution paradigms. All code, data will be released at after our work be accepted to promote reproducible research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AgriAgent\uff0c\u4e00\u4e2a\u7528\u4e8e\u771f\u5b9e\u519c\u4e1a\u573a\u666f\u7684\u4e24\u7ea7\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u6267\u884c\u7b56\u7565\u5904\u7406\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u4efb\u52a1\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u76f8\u6bd4\u73b0\u6709\u7edf\u4e00\u6267\u884c\u8303\u5f0f\u7684\u5de5\u5177\u4e2d\u5fc3\u5316\u667a\u80fd\u4f53\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6267\u884c\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u771f\u5b9e\u519c\u4e1a\u573a\u666f\u4e2d\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u9700\u8981\u5904\u7406\u4ece\u8f7b\u91cf\u7ea7\u4fe1\u606f\u7406\u89e3\u5230\u590d\u6742\u591a\u6b65\u6267\u884c\u7684\u591a\u6a21\u6001\u8f93\u5165\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u7edf\u4e00\u6267\u884c\u8303\u5f0f\uff0c\u96be\u4ee5\u9002\u5e94\u519c\u4e1a\u73af\u5883\u4e2d\u5e38\u89c1\u7684\u4efb\u52a1\u590d\u6742\u5ea6\u5dee\u5f02\u5927\u548c\u5de5\u5177\u53ef\u7528\u6027\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\u3002", "method": "AgriAgent\u91c7\u7528\u57fa\u4e8e\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u5206\u5c42\u6267\u884c\u7b56\u7565\uff1a\u7b80\u5355\u4efb\u52a1\u901a\u8fc7\u7279\u5b9a\u6a21\u6001\u667a\u80fd\u4f53\u76f4\u63a5\u63a8\u7406\u5904\u7406\uff0c\u590d\u6742\u4efb\u52a1\u5219\u89e6\u53d1\u5951\u7ea6\u9a71\u52a8\u7684\u89c4\u5212\u673a\u5236\uff0c\u5c06\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u80fd\u529b\u9700\u6c42\uff0c\u6267\u884c\u80fd\u529b\u611f\u77e5\u7684\u5de5\u5177\u7f16\u6392\u548c\u52a8\u6001\u5de5\u5177\u751f\u6210\uff0c\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u591a\u6b65\u6267\u884c\u548c\u6545\u969c\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAgriAgent\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u76f8\u6bd4\u4f9d\u8d56\u7edf\u4e00\u6267\u884c\u8303\u5f0f\u7684\u73b0\u6709\u5de5\u5177\u4e2d\u5fc3\u5316\u667a\u80fd\u4f53\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6267\u884c\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\uff0c\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5728\u8bba\u6587\u88ab\u63a5\u53d7\u540e\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5206\u5c42\u6267\u884c\u7b56\u7565\u5728\u5904\u7406\u519c\u4e1a\u573a\u666f\u4e2d\u591a\u6837\u5316\u4efb\u52a1\u7684\u6709\u6548\u6027\uff0c\u5951\u7ea6\u9a71\u52a8\u7684\u89c4\u5212\u673a\u5236\u80fd\u591f\u9002\u5e94\u4e0d\u5b8c\u6574\u7684\u5de5\u5177\u53ef\u7528\u6027\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u519c\u4e1a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u519c\u4e1a\u4eba\u5de5\u667a\u80fd\u5411\u66f4\u5b9e\u7528\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2601.08010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08010", "abs": "https://arxiv.org/abs/2601.08010", "authors": ["Chaoyu Li", "Deeparghya Dutta Barua", "Fei Tao", "Pooyan Fazli"], "title": "CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation", "comment": null, "summary": "Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCASHEW\u548cCASHEW-RL\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u805a\u5408\u591a\u4e2a\u5019\u9009\u8f68\u8ff9\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u591a\u6b65\u63a8\u7406\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5bf9\u76f8\u540c\u8f93\u5165\u7684\u91cd\u590d\u91c7\u6837\u4f1a\u4ea7\u751f\u53d1\u6563\u63a8\u7406\u8f68\u8ff9\u548c\u4e0d\u4e00\u81f4\u7684\u6700\u7ec8\u9884\u6d4b\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1aCASHEW\u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u805a\u5408\u591a\u4e2a\u5019\u9009\u63a8\u7406\u8f68\u8ff9\u5f62\u6210\u66f4\u9ad8\u8d28\u91cf\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u5229\u7528\u663e\u5f0f\u89c6\u89c9\u9a8c\u8bc1\u8fc7\u6ee4\u5e7b\u89c9\u6b65\u9aa4\uff1bCASHEW-RL\u5219\u901a\u8fc7Group Sequence Policy Optimization\u8bad\u7ec3\uff0c\u4f7f\u7528\u590d\u5408\u5956\u52b1\u51fd\u6570\u9f13\u52b1\u57fa\u4e8e\u6700\u5c0f\u5145\u5206\u89c6\u89c9\u8bc1\u636e\u7684\u6b63\u786e\u7b54\u6848\uff0c\u5e76\u81ea\u9002\u5e94\u5206\u914d\u63a8\u7406\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u572813\u4e2a\u56fe\u50cf\u7406\u89e3\u3001\u89c6\u9891\u7406\u89e3\u548c\u89c6\u9891\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5176\u4e2dScienceQA\u4e0a\u63d0\u5347\u8fbe23.6\u4e2a\u767e\u5206\u70b9\uff0cEgoSchema\u4e0a\u63d0\u53478.1\u4e2a\u767e\u5206\u70b9\uff0c\u8bc1\u660e\u4e86\u63a8\u7406\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u7684\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u63a8\u7406\u65f6\u8f68\u8ff9\u805a\u5408\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u591a\u6b65\u63a8\u7406\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u7a33\u5065\u7684\u591a\u6a21\u6001\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u81ea\u9002\u5e94\u63a8\u7406\u8ba1\u7b97\u5206\u914d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.08166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08166", "abs": "https://arxiv.org/abs/2601.08166", "authors": ["Mohammad Pivezhandi", "Mahdi Banisharif", "Abusayeed Saifullah", "Ali Jannesari"], "title": "ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms", "comment": "39 pages, 12 figures, 8 tables (including appendix)", "summary": "Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6838\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u70ed\u80fd\u548c\u80fd\u8017\u611f\u77e5\u8c03\u5ea6\u3002\u8be5\u6846\u67b6\u7ed3\u5408LLM\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u548c\u73af\u5883\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u90e8\u7f72\u548c\u5feb\u901f\u51b3\u7b56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u548c\u8c03\u5ea6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u7535\u538b\u9891\u7387\u7f29\u653e\u548c\u4efb\u52a1\u5206\u914d\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u57fa\u4e8e\u5229\u7528\u7387\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u5ffd\u7565\u4e86\u505c\u987f\u65f6\u95f4\uff0c\u800c\u57fa\u4e8e\u79bb\u7ebf\u6027\u80fd\u5206\u6790\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u79bb\u7ebf\u5206\u6790\u751f\u6210\u8868\u683c\uff0c\u65e0\u6cd5\u9002\u5e94\u8fd0\u884c\u65f6\u53d8\u5316\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86\u52a8\u6001\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u9ad8\u6548\u7684\u70ed\u7ba1\u7406\u548c\u80fd\u8017\u6027\u80fd\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\u5206\u89e3\u6307\u6570\u7ea7\u52a8\u4f5c\u7a7a\u95f4\uff0c\u51b3\u7b56\u5ef6\u8fdf\u4ec5\u4e3a358\u6beb\u79d2\u3002\u6846\u67b6\u7ed3\u5408LLM\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u6280\u672f\uff0c\u4eceOpenMP\u7a0b\u5e8f\u4e2d\u63d0\u53d613\u4e2a\u4ee3\u7801\u7ea7\u7279\u5f81\u800c\u65e0\u9700\u6267\u884c\uff0c\u5e76\u5229\u7528\u56de\u5f52\u6280\u672f\u6784\u5efa\u51c6\u786e\u7684\u73af\u5883\u6a21\u578b\u9884\u6d4b\u70ed\u529b\u5b66\u548c\u6027\u80fd\u72b6\u6001\u3002\u91c7\u7528Dyna-Q\u542f\u53d1\u7684\u6846\u67b6\uff0c\u5c06\u76f4\u63a5\u5f3a\u5316\u5b66\u4e60\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u5b9e\u73b0\u96f6\u6837\u672c\u90e8\u7f72\u3002", "result": "\u5b9e\u9a8c\u5728BOTS\u548cPolybenchC\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\uff0c\u8986\u76d6NVIDIA Jetson TX2\u3001Jetson Orin NX\u3001RubikPi\u548cIntel Core i7\u5e73\u53f0\u3002\u7ed3\u679c\u663e\u793a\u76f8\u6bd4Linux ondemand\u8c03\u5ea6\u5668\uff0c\u80fd\u6548\u63d0\u53477.09\u500d\uff0c\u5b8c\u5de5\u65f6\u95f4\u6539\u55844.0\u500d\u3002\u9996\u6b21\u51b3\u7b56\u5ef6\u8fdf\u4e3a3.5\u81f38.0\u79d2\uff08\u542b\u4e00\u6b21\u6027LLM\u7279\u5f81\u63d0\u53d6\uff09\uff0c\u540e\u7eed\u51b3\u7b56\u4ec5\u9700358\u6beb\u79d2\uff0c\u6bd4\u57fa\u4e8e\u8868\u683c\u7684\u5206\u6790\u65b9\u6cd5\u5feb8300\u500d\u3002\u6a21\u578b\u6536\u655b\u901f\u5ea6\u6bd4\u65e0\u6a21\u578b\u65b9\u6cd5\u5feb20\u500d\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408LLM\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u548c\u73af\u5883\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u52a8\u6001\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u90e8\u7f72\u548c\u5feb\u901f\u81ea\u9002\u5e94\u8c03\u5ea6\u3002\u8be5\u65b9\u6cd5\u4e3a\u70ed\u80fd\u548c\u80fd\u8017\u611f\u77e5\u8c03\u5ea6\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5b9e\u65f6\u9002\u5e94\u65b0\u5de5\u4f5c\u8d1f\u8f7d\u7684\u52a8\u6001\u5d4c\u5165\u5f0f\u7cfb\u7edf\u73af\u5883\u3002"}}
{"id": "2601.08342", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08342", "abs": "https://arxiv.org/abs/2601.08342", "authors": ["Run Chen", "Wen Liang", "Ziwei Gong", "Lin Ai", "Julia Hirschberg"], "title": "Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue", "comment": "Accepted to IWSDS 2026", "summary": "Mental manipulation, the strategic use of language to covertly influence or exploit others, is a newly emerging task in computational social reasoning. Prior work has focused exclusively on textual conversations, overlooking how manipulative tactics manifest in speech. We present the first study of mental manipulation detection in spoken dialogues, introducing a synthetic multi-speaker benchmark SPEECHMENTALMANIP that augments a text-based dataset with high-quality, voice-consistent Text-to-Speech rendered audio. Using few-shot large audio-language models and human annotation, we evaluate how modality affects detection accuracy and perception. Our results reveal that models exhibit high specificity but markedly lower recall on speech compared to text, suggesting sensitivity to missing acoustic or prosodic cues in training. Human raters show similar uncertainty in the audio setting, underscoring the inherent ambiguity of manipulative speech. Together, these findings highlight the need for modality-aware evaluation and safety alignment in multimodal dialogue systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86\u8bed\u97f3\u5bf9\u8bdd\u4e2d\u7684\u5fc3\u7406\u64cd\u7eb5\u68c0\u6d4b\uff0c\u6784\u5efa\u4e86\u9996\u4e2a\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u57fa\u51c6SPEECHMENTALMANIP\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u8bed\u97f3\u6a21\u6001\u4e0a\u68c0\u6d4b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u73b0\u8c61\uff0c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u9700\u8981\u6a21\u6001\u611f\u77e5\u7684\u5b89\u5168\u5bf9\u9f50\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u5fc3\u7406\u64cd\u7eb5\u68c0\u6d4b\u7684\u7814\u7a76\u4ec5\u5173\u6ce8\u6587\u672c\u5bf9\u8bdd\uff0c\u5ffd\u89c6\u4e86\u64cd\u7eb5\u7b56\u7565\u5728\u8bed\u97f3\u4e2d\u7684\u8868\u73b0\u5f62\u5f0f\uff0c\u5b58\u5728\u6a21\u6001\u8986\u76d6\u4e0d\u5168\u7684\u7814\u7a76\u7a7a\u767d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8bed\u97f3\u5bf9\u8bdd\u4e2d\u5fc3\u7406\u64cd\u7eb5\u68c0\u6d4b\u8fd9\u4e00\u65b0\u5174\u4efb\u52a1\uff0c\u63a2\u7d22\u6a21\u6001\u5982\u4f55\u5f71\u54cd\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u611f\u77e5\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u9996\u4e2a\u5408\u6210\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u57fa\u51c6SPEECHMENTALMANIP\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u3001\u8bed\u97f3\u4e00\u81f4\u7684\u6587\u672c\u5230\u8bed\u97f3\u6280\u672f\u5c06\u6587\u672c\u6570\u636e\u96c6\u589e\u5f3a\u4e3a\u97f3\u9891\u6570\u636e\u3002\u91c7\u7528\u5c11\u6837\u672c\u5927\u578b\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u5de5\u6807\u6ce8\u65b9\u6cd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6a21\u6001\u5bf9\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5728\u8bed\u97f3\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u9ad8\u7279\u5f02\u6027\u4f46\u53ec\u56de\u7387\u663e\u8457\u4f4e\u4e8e\u6587\u672c\u6570\u636e\uff0c\u8868\u660e\u6a21\u578b\u5bf9\u8bad\u7ec3\u4e2d\u7f3a\u5931\u7684\u58f0\u5b66\u6216\u97f5\u5f8b\u7ebf\u7d22\u654f\u611f\u3002\u4eba\u7c7b\u6807\u6ce8\u8005\u5728\u97f3\u9891\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u7c7b\u4f3c\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u51f8\u663e\u4e86\u64cd\u7eb5\u6027\u8bed\u97f3\u56fa\u6709\u7684\u6a21\u7cca\u6027\u7279\u5f81\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u9700\u8981\u6a21\u6001\u611f\u77e5\u7684\u8bc4\u4f30\u548c\u5b89\u5168\u5bf9\u9f50\u673a\u5236\u3002\u7814\u7a76\u63ed\u793a\u4e86\u8bed\u97f3\u6a21\u6001\u4e2d\u5fc3\u7406\u64cd\u7eb5\u68c0\u6d4b\u7684\u7279\u6b8a\u6311\u6218\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u591a\u6a21\u6001\u793e\u4ea4\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2601.08017", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08017", "abs": "https://arxiv.org/abs/2601.08017", "authors": ["Ev\u017een Wybitul", "Javier Rando", "Florian Tram\u00e8r", "Stanislav Fort"], "title": "Representations of Text and Images Align From Layer One", "comment": null, "summary": "We show that for a variety of concepts in adapter-based vision-language models, the representations of their images and their text descriptions are meaningfully aligned from the very first layer. This contradicts the established view that such image-text alignment only appears in late layers. We show this using a new synthesis-based method inspired by DeepDream: given a textual concept such as \"Jupiter\", we extract its concept vector at a given layer, and then use optimisation to synthesise an image whose representation aligns with that vector. We apply our approach to hundreds of concepts across seven layers in Gemma 3, and find that the synthesised images often depict salient visual features of the targeted textual concepts: for example, already at layer 1, more than 50 % of images depict recognisable features of animals, activities, or seasons. Our method thus provides direct, constructive evidence of image-text alignment on a concept-by-concept and layer-by-layer basis. Unlike previous methods for measuring multimodal alignment, our approach is simple, fast, and does not require auxiliary models or datasets. It also offers a new path towards model interpretability, by providing a way to visualise a model's representation space by backtracing through its image processing components.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u5408\u6210\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u57fa\u4e8e\u9002\u914d\u5668\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u56fe\u50cf\u4e0e\u6587\u672c\u8868\u793a\u5728\u65e9\u671f\u5c42\u5c31\u5df2\u5b58\u5728\u6709\u610f\u4e49\u7684\u5bf9\u9f50\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u8ba4\u4e3a\u6b64\u7c7b\u5bf9\u9f50\u4ec5\u51fa\u73b0\u5728\u6df1\u5c42\u7f51\u7edc\u7684\u89c2\u70b9\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u6311\u6218\u4f20\u7edf\u89c2\u70b9\uff0c\u5373\u57fa\u4e8e\u9002\u914d\u5668\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u56fe\u50cf\u4e0e\u6587\u672c\u7684\u5bf9\u9f50\u4ec5\u51fa\u73b0\u5728\u7f51\u7edc\u6df1\u5c42\u3002\u73b0\u6709\u7814\u7a76\u666e\u904d\u8ba4\u4e3a\u8de8\u6a21\u6001\u5bf9\u9f50\u9700\u8981\u7ecf\u8fc7\u591a\u5c42\u5904\u7406\u624d\u80fd\u5f62\u6210\uff0c\u4f46\u8be5\u7814\u7a76\u8bd5\u56fe\u63a2\u7d22\u8fd9\u79cd\u5bf9\u9f50\u662f\u5426\u5728\u66f4\u65e9\u7684\u7f51\u7edc\u5c42\u4e2d\u5c31\u5df2\u5b58\u5728\uff0c\u4ece\u800c\u63ed\u793a\u6a21\u578b\u5185\u90e8\u8868\u793a\u5f62\u6210\u7684\u52a8\u6001\u8fc7\u7a0b\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7DeepDream\u542f\u53d1\u7684\u5408\u6210\u65b9\u6cd5\uff1a\u7ed9\u5b9a\u6587\u672c\u6982\u5ff5\u5982\"\u6728\u661f\"\uff0c\u5728\u7279\u5b9a\u5c42\u63d0\u53d6\u5176\u6982\u5ff5\u5411\u91cf\uff0c\u7136\u540e\u901a\u8fc7\u4f18\u5316\u8fc7\u7a0b\u5408\u6210\u4e00\u4e2a\u56fe\u50cf\uff0c\u4f7f\u5176\u8868\u793a\u4e0e\u8be5\u5411\u91cf\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u8f85\u52a9\u6a21\u578b\u6216\u6570\u636e\u96c6\uff0c\u76f4\u63a5\u5728Gemma 3\u6a21\u578b\u7684\u4e03\u4e2a\u5c42\u4e0a\u5bf9\u6570\u767e\u4e2a\u6982\u5ff5\u8fdb\u884c\u6d4b\u8bd5\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u6a21\u578b\u7684\u8868\u793a\u7a7a\u95f4\u6765\u9a8c\u8bc1\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728Gemma 3\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\uff0c\u8d85\u8fc750%\u7684\u5408\u6210\u56fe\u50cf\u5df2\u7ecf\u80fd\u591f\u63cf\u7ed8\u76ee\u6807\u6587\u672c\u6982\u5ff5\u7684\u53ef\u8bc6\u522b\u89c6\u89c9\u7279\u5f81\uff0c\u5982\u52a8\u7269\u3001\u6d3b\u52a8\u6216\u5b63\u8282\u7684\u663e\u8457\u7279\u5f81\u3002\u8fd9\u8868\u660e\u56fe\u50cf\u4e0e\u6587\u672c\u8868\u793a\u7684\u5bf9\u9f50\u4ece\u7f51\u7edc\u65e9\u671f\u5c42\u5c31\u5f00\u59cb\u51fa\u73b0\uff0c\u800c\u975e\u4ec5\u9650\u4e8e\u6df1\u5c42\uff0c\u4e3a\u6982\u5ff5\u5c42\u9762\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u63d0\u4f9b\u4e86\u76f4\u63a5\u8bc1\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u76f4\u63a5\u3001\u5efa\u8bbe\u6027\u7684\u8bc1\u636e\uff0c\u8868\u660e\u57fa\u4e8e\u9002\u914d\u5668\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u5728\u65e9\u671f\u5c42\u5c31\u5df2\u5b58\u5728\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u7406\u89e3\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4e3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u8868\u793a\u7a7a\u95f4\u6765\u7406\u89e3\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u800c\u4e14\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u5feb\u901f\u4e14\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2601.08173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08173", "abs": "https://arxiv.org/abs/2601.08173", "authors": ["Daocheng Fu", "Jianbiao Mei", "Rong Wu", "Xuemeng Yang", "Jia Xu", "Ding Wang", "Pinlong Cai", "Yong Liu", "Licheng Wen", "Botian Shi"], "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios", "comment": null, "summary": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \\method{}, a dynamic evaluation environment that simulates a \"trainee\" agent continuously exploring a novel setting. Unlike traditional benchmarks, \\method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EvoEnv\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9c81\u68d2\u6027\u7684\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u73af\u5883\u6027\u80fd\u4e0a\u9650\u800c\u5ffd\u89c6\u5b9e\u9645\u90e8\u7f72\u4e2d\u968f\u673a\u6027\u6311\u6218\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4e0a\u9650\uff0c\u5ffd\u89c6\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u968f\u673a\u6027\u548c\u52a8\u6001\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u4efb\u52a1\u8c03\u5ea6\u3001\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u4e3b\u52a8\u63a2\u7d22\u4ee5\u53ca\u4ece\u7ecf\u9a8c\u4e2d\u6301\u7eed\u5b66\u4e60\u8fd9\u4e09\u4e2a\u5173\u952e\u65b9\u9762\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86EvoEnv\u52a8\u6001\u8bc4\u4f30\u73af\u5883\uff0c\u6a21\u62df\"\u53d7\u8bad\u8005\"\u667a\u80fd\u4f53\u5728\u65b0\u578b\u8bbe\u7f6e\u4e2d\u7684\u6301\u7eed\u63a2\u7d22\u8fc7\u7a0b\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u667a\u80fd\u4f53\uff1a\u9762\u5411\u6d41\u5f0f\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u5ea6\u3001\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u51cf\u5c11\u5e7b\u89c9\u7684\u8c28\u614e\u4fe1\u606f\u83b7\u53d6\uff0c\u4ee5\u53ca\u4ece\u57fa\u4e8e\u89c4\u5219\u52a8\u6001\u751f\u6210\u7684\u4efb\u52a1\u4e2d\u63d0\u70bc\u901a\u7528\u7b56\u7565\u7684\u6301\u7eed\u6f14\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5728\u4e3b\u52a8\u63a2\u7d22\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u8fd9\u63ed\u793a\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u5b9e\u9645\u751f\u4ea7\u573a\u666f\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u8bc4\u4f30\u667a\u80fd\u4f53\u53ef\u9760\u6027\u7684\u6846\u67b6\uff0c\u5c06\u8bc4\u4f30\u91cd\u70b9\u4ece\u9759\u6001\u6d4b\u8bd5\u8f6c\u5411\u73b0\u5b9e\u7684\u751f\u4ea7\u5bfc\u5411\u573a\u666f\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.08621", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08621", "abs": "https://arxiv.org/abs/2601.08621", "authors": ["Jiajin Liu", "Yuanfu Sun", "Dongzhe Fan", "Qiaoyu Tan"], "title": "GraphSearch: Agentic Search-Augmented Reasoning for Zero-Shot Graph Learning", "comment": "16 pages, 5 pages", "summary": "Recent advances in search-augmented large reasoning models (LRMs) enable the retrieval of external knowledge to reduce hallucinations in multistep reasoning. However, their ability to operate on graph-structured data, prevalent in domains such as e-commerce, social networks, and scientific citations, remains underexplored. Unlike plain text corpora, graphs encode rich topological signals that connect related entities and can serve as valuable priors for retrieval, enabling more targeted search and improved reasoning efficiency. Yet, effectively leveraging such structure poses unique challenges, including the difficulty of generating graph-expressive queries and ensuring reliable retrieval that balances structural and semantic relevance. To address this gap, we introduce GraphSearch, the first framework that extends search-augmented reasoning to graph learning, enabling zero-shot graph learning without task-specific fine-tuning. GraphSearch combines a Graph-aware Query Planner, which disentangles search space (e.g., 1-hop, multi-hop, or global neighbors) from semantic queries, with a Graph-aware Retriever, which constructs candidate sets based on topology and ranks them using a hybrid scoring function. We further instantiate two traversal modes: GraphSearch-R, which recursively expands neighborhoods hop by hop, and GraphSearch-F, which flexibly retrieves across local and global neighborhoods without hop constraints. Extensive experiments across diverse benchmarks show that GraphSearch achieves competitive or even superior performance compared to supervised graph learning methods, setting state-of-the-art results in zero-shot node classification and link prediction. These findings position GraphSearch as a flexible and generalizable paradigm for agentic reasoning over graphs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GraphSearch\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u6269\u5c55\u5230\u56fe\u5b66\u4e60\u9886\u57df\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u7684\u96f6\u6837\u672c\u56fe\u5b66\u4e60\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u56fe\u611f\u77e5\u67e5\u8be2\u89c4\u5212\u5668\u548c\u68c0\u7d22\u5668\uff0c\u5728\u591a\u6837\u5316\u7684\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u641c\u7d22\u589e\u5f3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u5904\u7406\u56fe\u7ed3\u6784\u6570\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u56fe\u6570\u636e\u5728\u7535\u5b50\u5546\u52a1\u3001\u793e\u4ea4\u7f51\u7edc\u548c\u79d1\u5b66\u5f15\u7528\u7b49\u9886\u57df\u666e\u904d\u5b58\u5728\u3002\u56fe\u7ed3\u6784\u7f16\u7801\u4e86\u4e30\u5bcc\u7684\u62d3\u6251\u4fe1\u53f7\uff0c\u53ef\u4f5c\u4e3a\u68c0\u7d22\u7684\u5b9d\u8d35\u5148\u9a8c\u77e5\u8bc6\uff0c\u4f46\u6709\u6548\u5229\u7528\u8fd9\u79cd\u7ed3\u6784\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u751f\u6210\u56fe\u8868\u8fbe\u6027\u67e5\u8be2\u7684\u56f0\u96be\u4ee5\u53ca\u5e73\u8861\u7ed3\u6784\u548c\u8bed\u4e49\u76f8\u5173\u6027\u7684\u53ef\u9760\u68c0\u7d22\u95ee\u9898\u3002", "method": "GraphSearch\u6846\u67b6\u5305\u542b\u56fe\u611f\u77e5\u67e5\u8be2\u89c4\u5212\u5668\uff0c\u5c06\u641c\u7d22\u7a7a\u95f4\uff08\u59821\u8df3\u3001\u591a\u8df3\u6216\u5168\u5c40\u90bb\u5c45\uff09\u4e0e\u8bed\u4e49\u67e5\u8be2\u89e3\u8026\uff0c\u4ee5\u53ca\u56fe\u611f\u77e5\u68c0\u7d22\u5668\uff0c\u57fa\u4e8e\u62d3\u6251\u6784\u5efa\u5019\u9009\u96c6\u5e76\u4f7f\u7528\u6df7\u5408\u8bc4\u5206\u51fd\u6570\u8fdb\u884c\u6392\u5e8f\u3002\u6846\u67b6\u5b9e\u4f8b\u5316\u4e86\u4e24\u79cd\u904d\u5386\u6a21\u5f0f\uff1aGraphSearch-R\u9012\u5f52\u6269\u5c55\u90bb\u57df\uff0c\u800cGraphSearch-F\u7075\u6d3b\u68c0\u7d22\u5c40\u90e8\u548c\u5168\u5c40\u90bb\u57df\u800c\u4e0d\u53d7\u8df3\u6570\u7ea6\u675f\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGraphSearch\u5728\u96f6\u6837\u672c\u8282\u70b9\u5206\u7c7b\u548c\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u76d1\u7763\u56fe\u5b66\u4e60\u65b9\u6cd5\u76f8\u7ade\u4e89\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u5e76\u521b\u4e0b\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u56fe\u5b66\u4e60\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002", "conclusion": "GraphSearch\u4f5c\u4e3a\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6cdb\u5316\u7684\u8303\u5f0f\uff0c\u4e3a\u56fe\u4e0a\u7684\u667a\u80fd\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u8bc1\u660e\u4e86\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u5728\u56fe\u7ed3\u6784\u6570\u636e\u4e0a\u7684\u6709\u6548\u6027\u3002\u8be5\u7814\u7a76\u4e3a\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u56fe\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u4e3a\u56fe\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2601.08022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08022", "abs": "https://arxiv.org/abs/2601.08022", "authors": ["Samet Hicsonmez", "Abd El Rahman Shabayek", "Djamila Aouada"], "title": "Training Free Zero-Shot Visual Anomaly Localization via Diffusion Inversion", "comment": null, "summary": "Zero-Shot image Anomaly Detection (ZSAD) aims to detect and localise anomalies without access to any normal training samples of the target data. While recent ZSAD approaches leverage additional modalities such as language to generate fine-grained prompts for localisation, vision-only methods remain limited to image-level classification, lacking spatial precision. In this work, we introduce a simple yet effective training-free vision-only ZSAD framework that circumvents the need for fine-grained prompts by leveraging the inversion of a pretrained Denoising Diffusion Implicit Model (DDIM). Specifically, given an input image and a generic text description (e.g., \"an image of an [object class]\"), we invert the image to obtain latent representations and initiate the denoising process from a fixed intermediate timestep to reconstruct the image. Since the underlying diffusion model is trained solely on normal data, this process yields a normal-looking reconstruction. The discrepancy between the input image and the reconstructed one highlights potential anomalies. Our method achieves state-of-the-art performance on VISA dataset, demonstrating strong localisation capabilities without auxiliary modalities and facilitating a shift away from prompt dependence for zero-shot anomaly detection research. Code is available at https://github.com/giddyyupp/DIVAD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6DIVAD\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u53bb\u566a\u6269\u6563\u9690\u5f0f\u6a21\u578b\u7684\u53cd\u8f6c\u8fc7\u7a0b\uff0c\u5728\u65e0\u9700\u7ec6\u7c92\u5ea6\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff0c\u5728VISA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u96f6\u6837\u672c\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u57fa\u4e8e\u8bed\u8a00\u7684\u65b9\u6cd5\u9700\u8981\u4f9d\u8d56\u7ec6\u7c92\u5ea6\u63d0\u793a\u6765\u5b9e\u73b0\u5b9a\u4f4d\uff0c\u800c\u7eaf\u89c6\u89c9\u65b9\u6cd5\u901a\u5e38\u4ec5\u9650\u4e8e\u56fe\u50cf\u7ea7\u5206\u7c7b\uff0c\u7f3a\u4e4f\u7a7a\u95f4\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u4e0d\u4f9d\u8d56\u8f85\u52a9\u6a21\u6001\u7684\u89c6\u89c9\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u4ee5\u514b\u670d\u5bf9\u63d0\u793a\u7684\u4f9d\u8d56\u5e76\u63d0\u9ad8\u5b9a\u4f4d\u80fd\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u53bb\u566a\u6269\u6563\u9690\u5f0f\u6a21\u578b\u7684\u53cd\u8f6c\u6846\u67b6DIVAD\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7ed9\u5b9a\u8f93\u5165\u56fe\u50cf\u548c\u901a\u7528\u6587\u672c\u63cf\u8ff0\uff0c\u9996\u5148\u5c06\u56fe\u50cf\u53cd\u8f6c\u5230\u6f5c\u5728\u7a7a\u95f4\u83b7\u5f97\u6f5c\u5728\u8868\u793a\uff0c\u7136\u540e\u4ece\u56fa\u5b9a\u7684\u4e2d\u95f4\u65f6\u95f4\u6b65\u5f00\u59cb\u53bb\u566a\u8fc7\u7a0b\u4ee5\u91cd\u5efa\u56fe\u50cf\u3002\u7531\u4e8e\u5e95\u5c42\u6269\u6563\u6a21\u578b\u4ec5\u5728\u6b63\u5e38\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u8be5\u8fc7\u7a0b\u4f1a\u4ea7\u751f\u6b63\u5e38\u5916\u89c2\u7684\u91cd\u5efa\uff0c\u8f93\u5165\u56fe\u50cf\u4e0e\u91cd\u5efa\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\u5219\u7a81\u51fa\u4e86\u6f5c\u5728\u5f02\u5e38\u533a\u57df\u3002", "result": "\u8be5\u65b9\u6cd5\u5728VISA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5f02\u5e38\u5b9a\u4f4d\u80fd\u529b\uff0c\u65e0\u9700\u4efb\u4f55\u8f85\u52a9\u6a21\u6001\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5f02\u5e38\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u7ec6\u7c92\u5ea6\u63d0\u793a\u7684\u4f9d\u8d56\uff0c\u4e3a\u7eaf\u89c6\u89c9\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u53cd\u8f6c\u8fc7\u7a0b\u53ef\u4ee5\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6709\u6548\u7684\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff0c\u4e3a\u51cf\u5c11\u5bf9\u63d0\u793a\u4f9d\u8d56\u7684\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u7eaf\u89c6\u89c9\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u6709\u6548\u7684\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u4ece\u63d0\u793a\u4f9d\u8d56\u5411\u66f4\u901a\u7528\u65b9\u6cd5\u7684\u8f6c\u53d8\u3002"}}
{"id": "2601.08235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08235", "abs": "https://arxiv.org/abs/2601.08235", "authors": ["Shouju Wang", "Haopeng Zhang"], "title": "MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents", "comment": "Submitted to ACL 2026", "summary": "As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MPCI-Bench\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u9690\u79c1\u884c\u4e3a\u7684\u591a\u6a21\u6001\u914d\u5bf9\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u57fa\u51c6\uff0c\u901a\u8fc7\u4e09\u5c42\u8bc4\u4f30\u6846\u67b6\u63ed\u793a\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u5728\u5e73\u8861\u9690\u79c1\u4e0e\u6548\u7528\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u7f3a\u9677\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u4ece\u88ab\u52a8\u804a\u5929\u673a\u5668\u4eba\u6f14\u53d8\u4e3a\u5904\u7406\u4e2a\u4eba\u6570\u636e\u7684\u4e3b\u52a8\u52a9\u624b\uff0c\u8bc4\u4f30\u5176\u5bf9\u793e\u4f1a\u89c4\u8303\u7684\u9075\u5b88\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u573a\u666f\u548c\u8d1f\u9762\u62d2\u7edd\u60c5\u51b5\uff0c\u5ffd\u89c6\u4e86\u591a\u6a21\u6001\u9690\u79c1\u98ce\u9669\u4ee5\u53ca\u9690\u79c1\u4e0e\u6548\u7528\u7684\u57fa\u672c\u6743\u8861\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MPCI-Bench\u591a\u6a21\u6001\u914d\u5bf9\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u57fa\u51c6\uff0c\u5305\u542b\u4ece\u76f8\u540c\u89c6\u89c9\u6e90\u884d\u751f\u7684\u6b63\u8d1f\u914d\u5bf9\u5b9e\u4f8b\uff0c\u5e76\u5b9e\u4f8b\u5316\u4e3a\u4e09\u4e2a\u5c42\u7ea7\uff1a\u89c4\u8303\u6027\u79cd\u5b50\u5224\u65ad\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u6545\u4e8b\u63a8\u7406\u548c\u53ef\u6267\u884c\u7684\u667a\u80fd\u4f53\u884c\u52a8\u8f68\u8ff9\uff0c\u901a\u8fc7\u4e09\u539f\u5219\u8fed\u4ee3\u7cbe\u70bc\u6d41\u7a0b\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u591a\u6a21\u6001\u6a21\u578b\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u5e73\u8861\u9690\u79c1\u4e0e\u6548\u7528\u7684\u5931\u8d25\uff0c\u4ee5\u53ca\u663e\u8457\u7684\u6a21\u6001\u6cc4\u9732\u5dee\u8ddd\uff0c\u5176\u4e2d\u654f\u611f\u89c6\u89c9\u4fe1\u606f\u6bd4\u6587\u672c\u4fe1\u606f\u6cc4\u9732\u66f4\u9891\u7e41\uff0c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5f00\u53d1\u80fd\u591f\u5e73\u8861\u9690\u79c1\u4e0e\u6548\u7528\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7684\u91cd\u8981\u6027\uff0c\u63ed\u793a\u4e86\u6a21\u6001\u6cc4\u9732\u5dee\u8ddd\u8fd9\u4e00\u65b0\u95ee\u9898\uff0cMPCI-Bench\u7684\u5f00\u6e90\u5c06\u4e3a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u57fa\u7840\uff0c\u63a8\u52a8\u66f4\u5168\u9762\u7684\u9690\u79c1\u4fdd\u62a4\u8bc4\u4f30\u6846\u67b6\u53d1\u5c55\u3002"}}
{"id": "2601.08626", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08626", "abs": "https://arxiv.org/abs/2601.08626", "authors": ["Yingjie He", "Zhaolu Kang", "Kehan Jiang", "Qianyuan Zhang", "Jiachen Qian", "Chunlei Meng", "Yujie Feng", "Yuan Wang", "Jiabao Dou", "Aming Wu", "Leqi Zheng", "Pengxiang Zhao", "Jiaxin Liu", "Zeyu Zhang", "Lei Wang", "Guansu Wang", "Qishi Zhan", "Xiaomin He", "Meisheng Zhang", "Jianyuan Ni"], "title": "How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction", "comment": null, "summary": "Large language models (LLMs) excel at semantic understanding, yet their ability to reconstruct internal structure from scrambled inputs remains underexplored. Sentence-level restoration is ill-posed for automated evaluation because multiple valid word orders often exist. We introduce OrderProbe, a deterministic benchmark for structural reconstruction using fixed four-character expressions in Chinese, Japanese, and Korean, which have a unique canonical order and thus support exact-match scoring. We further propose a diagnostic framework that evaluates models beyond recovery accuracy, including semantic fidelity, logical validity, consistency, robustness sensitivity, and information density. Experiments on twelve widely used LLMs show that structural reconstruction remains difficult even for frontier systems: zero-shot recovery frequently falls below 35%. We also observe a consistent dissociation between semantic recall and structural planning, suggesting that structural robustness is not an automatic byproduct of semantic competence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86OrderProbe\u57fa\u51c6\u548c\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u91cd\u5efa\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5373\u4f7f\u524d\u6cbf\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4e5f\u96be\u4ee5\u6062\u590d\u56fa\u5b9a\u56db\u5b57\u7b26\u8868\u8fbe\u5f0f\u7684\u89c4\u8303\u987a\u5e8f\uff0c\u4e14\u8bed\u4e49\u80fd\u529b\u4e0e\u7ed3\u6784\u89c4\u5212\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u5206\u79bb\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u4ece\u4e71\u5e8f\u8f93\u5165\u4e2d\u91cd\u5efa\u5185\u90e8\u7ed3\u6784\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u53e5\u5b50\u7ea7\u6062\u590d\u4efb\u52a1\u7531\u4e8e\u5b58\u5728\u591a\u79cd\u6709\u6548\u8bcd\u5e8f\u800c\u96be\u4ee5\u8fdb\u884c\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u652f\u6301\u7cbe\u786e\u5339\u914d\u8bc4\u5206\u7684\u786e\u5b9a\u6027\u57fa\u51c6\u6765\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u7684\u7ed3\u6784\u91cd\u5efa\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86OrderProbe\u57fa\u51c6\uff0c\u4f7f\u7528\u4e2d\u6587\u3001\u65e5\u6587\u548c\u97e9\u6587\u4e2d\u7684\u56fa\u5b9a\u56db\u5b57\u7b26\u8868\u8fbe\u5f0f\u8fdb\u884c\u7ed3\u6784\u91cd\u5efa\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u8868\u8fbe\u5f0f\u5177\u6709\u552f\u4e00\u7684\u89c4\u8303\u987a\u5e8f\uff0c\u652f\u6301\u7cbe\u786e\u5339\u914d\u8bc4\u5206\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u8d85\u8d8a\u6062\u590d\u51c6\u786e\u7387\uff0c\u8bc4\u4f30\u8bed\u4e49\u4fdd\u771f\u5ea6\u3001\u903b\u8f91\u6709\u6548\u6027\u3001\u4e00\u81f4\u6027\u3001\u9c81\u68d2\u6027\u654f\u611f\u5ea6\u548c\u4fe1\u606f\u5bc6\u5ea6\u7b49\u591a\u4e2a\u7ef4\u5ea6\u3002", "result": "\u5728\u5341\u4e8c\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u6784\u91cd\u5efa\u4efb\u52a1\u5373\u4f7f\u5bf9\u524d\u6cbf\u7cfb\u7edf\u4e5f\u5177\u6709\u6311\u6218\u6027\uff1a\u96f6\u6837\u672c\u6062\u590d\u51c6\u786e\u7387\u7ecf\u5e38\u4f4e\u4e8e35%\u3002\u7814\u7a76\u8fd8\u89c2\u5bdf\u5230\u8bed\u4e49\u56de\u5fc6\u4e0e\u7ed3\u6784\u89c4\u5212\u4e4b\u95f4\u5b58\u5728\u4e00\u81f4\u6027\u7684\u5206\u79bb\u73b0\u8c61\uff0c\u8868\u660e\u7ed3\u6784\u9c81\u68d2\u6027\u5e76\u975e\u8bed\u4e49\u80fd\u529b\u7684\u81ea\u52a8\u526f\u4ea7\u54c1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u8868\u660e\u8bed\u4e49\u7406\u89e3\u4e0e\u7ed3\u6784\u89c4\u5212\u662f\u76f8\u5bf9\u72ec\u7acb\u7684\u8ba4\u77e5\u80fd\u529b\u3002OrderProbe\u57fa\u51c6\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u91cd\u5efa\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u672a\u6765\u7814\u7a76\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u7ed3\u6784\u9c81\u68d2\u6027\u8fdb\u884c\u6a21\u578b\u6539\u8fdb\uff0c\u800c\u975e\u4ec5\u4ec5\u4f9d\u8d56\u8bed\u4e49\u80fd\u529b\u7684\u63d0\u5347\u3002"}}
{"id": "2601.08024", "categories": ["cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.08024", "abs": "https://arxiv.org/abs/2601.08024", "authors": ["Amin Abbasishahkoo", "Mahboubeh Dadkhah", "Lionel Briand"], "title": "A Highly Efficient Diversity-based Input Selection for DNN Improvement Using VLMs", "comment": null, "summary": "Maintaining or improving the performance of Deep Neural Networks (DNNs) through fine-tuning requires labeling newly collected inputs, a process that is often costly and time-consuming. To alleviate this problem, input selection approaches have been developed in recent years to identify small, yet highly informative subsets for labeling. Diversity-based selection is one of the most effective approaches for this purpose. However, they are often computationally intensive and lack scalability for large input sets, limiting their practical applicability. To address this challenge, we introduce Concept-Based Diversity (CBD), a highly efficient metric for image inputs that leverages Vision-Language Models (VLM). Our results show that CBD exhibits a strong correlation with Geometric Diversity (GD), an established diversity metric, while requiring only a fraction of its computation time. Building on this finding, we propose a hybrid input selection approach that combines CBD with Margin, a simple uncertainty metric. We conduct a comprehensive evaluation across a diverse set of DNN models, input sets, selection budgets, and five most effective state-of-the-art selection baselines. The results demonstrate that the CBD-based selection consistently outperforms all baselines at guiding input selection to improve the DNN model. Furthermore, the CBD-based selection approach remains highly efficient, requiring selection times close to those of simple uncertainty-based methods such as Margin, even on larger input sets like ImageNet. These results confirm not only the effectiveness and computational advantage of the CBD-based approach, particularly compared to hybrid baselines, but also its scalability in repetitive and extensive input selection scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u5ff5\u591a\u6837\u6027\u7684\u9ad8\u6548\u56fe\u50cf\u8f93\u5165\u9009\u62e9\u65b9\u6cd5CBD\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u591a\u6837\u6027\u5ea6\u91cf\uff0c\u5e76\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u6784\u5efa\u6df7\u5408\u9009\u62e9\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5fae\u8c03\u4e2d\u6807\u6ce8\u6837\u672c\u9009\u62e9\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5fae\u8c03\u9700\u8981\u6807\u6ce8\u65b0\u6536\u96c6\u7684\u8f93\u5165\u6570\u636e\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u901a\u5e38\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\u3002\u73b0\u6709\u7684\u591a\u6837\u6027\u9009\u62e9\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u8ba1\u7b97\u5bc6\u96c6\u4e14\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u8f93\u5165\u96c6\u4e0a\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u6982\u5ff5\u591a\u6837\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9ad8\u6548\u8ba1\u7b97\u56fe\u50cf\u8f93\u5165\u7684\u591a\u6837\u6027\u7279\u5f81\u3002\u57fa\u4e8eCBD\u4e0e\u51e0\u4f55\u591a\u6837\u6027\u4e4b\u95f4\u7684\u5f3a\u76f8\u5173\u6027\u53d1\u73b0\uff0c\u6784\u5efa\u4e86CBD\u4e0e\u7b80\u5355\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cfMargin\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u8f93\u5165\u9009\u62e9\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u6548\u679c\u7684\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCBD\u4e0e\u51e0\u4f55\u591a\u6837\u6027\u5ea6\u91cf\u5448\u73b0\u5f3a\u76f8\u5173\u6027\uff0c\u540c\u65f6\u8ba1\u7b97\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\u3002\u5728\u591a\u79cdDNN\u6a21\u578b\u3001\u8f93\u5165\u96c6\u548c\u9009\u62e9\u9884\u7b97\u4e0b\uff0cCBD-based\u9009\u62e9\u65b9\u6cd5\u5728\u4e94\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u4e14\u9009\u62e9\u65f6\u95f4\u63a5\u8fd1\u7b80\u5355\u4e0d\u786e\u5b9a\u6027\u65b9\u6cd5\uff0c\u5728ImageNet\u7b49\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u4ecd\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "CBD-based\u65b9\u6cd5\u4e0d\u4ec5\u8bc1\u660e\u4e86\u5176\u76f8\u5bf9\u4e8e\u6df7\u5408\u57fa\u7ebf\u7684\u6709\u6548\u6027\u548c\u8ba1\u7b97\u4f18\u52bf\uff0c\u8fd8\u5c55\u793a\u4e86\u5728\u91cd\u590d\u548c\u5927\u89c4\u6a21\u8f93\u5165\u9009\u62e9\u573a\u666f\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u6807\u6ce8\u6837\u672c\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u9009\u62e9\u8d28\u91cf\u3002"}}
{"id": "2601.08388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08388", "abs": "https://arxiv.org/abs/2601.08388", "authors": ["Corina Chutaux"], "title": "Creativity in AI as Emergence from Domain-Limited Generative Models", "comment": null, "summary": "Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u89c6\u89d2\u4e0b\u7684AI\u521b\u9020\u529b\u6846\u67b6\uff0c\u5c06\u521b\u9020\u529b\u89c6\u4e3a\u6709\u9650\u9886\u57df\u751f\u6210\u6a21\u578b\u5728\u53d7\u9650\u4fe1\u606f\u73af\u5883\u4e2d\u6d8c\u73b0\u7684\u5c5e\u6027\uff0c\u800c\u975e\u4e8b\u540e\u8bc4\u4f30\u6807\u7b7e\uff0c\u5e76\u5efa\u7acb\u4e86\u5305\u542b\u56db\u4e2a\u76f8\u4e92\u4f5c\u7528\u7ec4\u4ef6\u7684\u6982\u5ff5\u5206\u89e3\u3002", "motivation": "\u73b0\u6709AI\u521b\u9020\u529b\u7814\u7a76\u4e3b\u8981\u91c7\u7528\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cf\u751f\u6210\u8f93\u51fa\u7684\u65b0\u9896\u6027\u3001\u591a\u6837\u6027\u6216\u5b9e\u7528\u6027\uff0c\u5c06\u521b\u9020\u529b\u89c6\u4e3a\u5f85\u8bc4\u4f30\u7684\u5c5e\u6027\u800c\u975e\u5f85\u5efa\u6a21\u7684\u73b0\u8c61\uff0c\u8fd9\u5ffd\u7565\u4e86\u521b\u9020\u529b\u4f5c\u4e3a\u6d8c\u73b0\u73b0\u8c61\u7684\u7ed3\u6784\u548c\u60c5\u5883\u6761\u4ef6\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u57fa\u7840\u7684\u6280\u672f\u6846\u67b6\u6765\u7814\u7a76AI\u7cfb\u7edf\u4e2d\u7684\u521b\u9020\u529b\u6d8c\u73b0\u673a\u5236\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u751f\u6210\u89c6\u89d2\u4e0b\u7684\u521b\u9020\u529b\u6846\u67b6\uff0c\u5c06\u521b\u9020\u529b\u89c6\u4e3a\u9886\u57df\u53d7\u9650\u751f\u6210\u6a21\u578b\u5728\u6709\u9650\u4fe1\u606f\u73af\u5883\u4e2d\u7684\u6d8c\u73b0\u5c5e\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u5305\u542b\u56db\u4e2a\u76f8\u4e92\u4f5c\u7528\u7ec4\u4ef6\u7684\u6982\u5ff5\u5206\u89e3\uff1a\u57fa\u4e8e\u6a21\u5f0f\u7684\u751f\u6210\u3001\u8bf1\u5bfc\u4e16\u754c\u6a21\u578b\u3001\u4e0a\u4e0b\u6587\u57fa\u7840\u6027\u548c\u4efb\u610f\u6027\uff0c\u7279\u522b\u5173\u6ce8\u8fd9\u4e9b\u7ec4\u4ef6\u5728\u591a\u6a21\u6001\u751f\u6210\u7cfb\u7edf\u4e2d\u7684\u5177\u4f53\u8868\u73b0\u3002", "result": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u5c06\u521b\u9020\u529b\u89c6\u4e3a\u751f\u6210\u52a8\u6001\u4e0e\u9886\u57df\u7279\u5b9a\u8868\u793a\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u6d8c\u73b0\u73b0\u8c61\u7684\u6280\u672f\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u5ff5\u5206\u89e3\u63ed\u793a\u4e86\u521b\u9020\u529b\u5728\u591a\u6a21\u6001\u751f\u6210\u7cfb\u7edf\u4e2d\u7684\u5177\u4f53\u8868\u73b0\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u5927\u89c4\u6a21\u751f\u6210\u7cfb\u7edf\uff08\u7279\u522b\u662f\u591a\u6a21\u6001\u67b6\u6784\uff09\u4e2d\u65e5\u76ca\u590d\u6742\u7684\u6a21\u5f0f\u91cd\u7ec4\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5c06\u521b\u9020\u529b\u4f5c\u4e3aAI\u7cfb\u7edf\u4e2d\u7684\u6d8c\u73b0\u73b0\u8c61\u800c\u975e\u4e8b\u540e\u8bc4\u4f30\u6807\u7b7e\u8fdb\u884c\u7814\u7a76\u63d0\u4f9b\u4e86\u6280\u672f\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u521b\u9020\u529b\u57fa\u7840\u5efa\u7acb\u5728\u751f\u6210\u52a8\u6001\u4e0e\u9886\u57df\u7279\u5b9a\u8868\u793a\u7684\u76f8\u4e92\u4f5c\u7528\u4e0a\uff0c\u4e3a\u7406\u89e3\u673a\u5668\u521b\u9020\u529b\u7684\u672c\u8d28\u548c\u9650\u5236\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76AI\u7cfb\u7edf\u4e2d\u7684\u521b\u9020\u6027\u884c\u4e3a\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u3002"}}
{"id": "2601.08645", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08645", "abs": "https://arxiv.org/abs/2601.08645", "authors": ["Dilara Toruno\u011flu-Selamet", "Dogukan Arslan", "Rodrigo Wilkens", "Wei He", "Doruk Eryi\u011fit", "Thomas Pickard", "Adriana S. Pagano", "Aline Villavicencio", "G\u00fcl\u015fen Eryi\u011fit", "\u00c1gnes Abuczki", "Aida Cardoso", "Alesia Lazarenka", "Dina Almassova", "Amalia Mendes", "Anna Kanellopoulou", "Antoni Brosa-Rodr\u00edguez", "Baiba Saulite", "Beata Wojtowicz", "Bolette Pedersen", "Carlos Manuel Hidalgo-Ternero", "Chaya Liebeskind", "Danka Joki\u0107", "Diego Alves", "Eleni Triantafyllidi", "Erik Velldal", "Fred Philippy", "Giedre Valunaite Oleskeviciene", "Ieva Rizgeliene", "Inguna Skadina", "Irina Lobzhanidze", "Isabell Stinessen Haugen", "Jauza Akbar Krito", "Jelena M. Markovi\u0107", "Johanna Monti", "Josue Alejandro Sauca", "Kaja Dobrovoljc", "Kingsley O. Ugwuanyi", "Laura Rituma", "Lilja \u00d8vrelid", "Maha Tufail Agro", "Manzura Abjalova", "Maria Chatzigrigoriou", "Mar\u00eda del Mar S\u00e1nchez Ramos", "Marija Pendevska", "Masoumeh Seyyedrezaei", "Mehrnoush Shamsfard", "Momina Ahsan", "Muhammad Ahsan Riaz Khan", "Nathalie Carmen Hau Norman", "Nilay Erdem Ayy\u0131ld\u0131z", "Nina Hosseini-Kivanani", "No\u00e9mi Ligeti-Nagy", "Numaan Naeem", "Olha Kanishcheva", "Olha Yatsyshyna", "Daniil Orel", "Petra Giommarelli", "Petya Osenova", "Radovan Garabik", "Regina E. Semou", "Rozane Rebechi", "Salsabila Zahirah Pranida", "Samia Touileb", "Sanni Nimb", "Sarfraz Ahmad", "Sarvinoz Nematkhonova", "Shahar Golan", "Shaoxiong Ji", "Sopuruchi Christian Aboh", "Srdjan Sucur", "Stella Markantonatou", "Sussi Olsen", "Vahide Tajalli", "Veronika Lipp", "Voula Giouli", "Yelda Ye\u015fildal Erayd\u0131n", "Zahra Saaberi", "Zhuohan Xie"], "title": "A Parallel Cross-Lingual Benchmark for Multimodal Idiomaticity Understanding", "comment": null, "summary": "Potentially idiomatic expressions (PIEs) construe meanings inherently tied to the everyday experience of a given language community. As such, they constitute an interesting challenge for assessing the linguistic (and to some extent cultural) capabilities of NLP systems. In this paper, we present XMPIE, a parallel multilingual and multimodal dataset of potentially idiomatic expressions. The dataset, containing 34 languages and over ten thousand items, allows comparative analyses of idiomatic patterns among language-specific realisations and preferences in order to gather insights about shared cultural aspects. This parallel dataset allows to evaluate model performance for a given PIE in different languages and whether idiomatic understanding in one language can be transferred to another. Moreover, the dataset supports the study of PIEs across textual and visual modalities, to measure to what extent PIE understanding in one modality transfers or implies in understanding in another modality (text vs. image). The data was created by language experts, with both textual and visual components crafted under multilingual guidelines, and each PIE is accompanied by five images representing a spectrum from idiomatic to literal meanings, including semantically related and random distractors. The result is a high-quality benchmark for evaluating multilingual and multimodal idiomatic language understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86XMPIE\uff0c\u4e00\u4e2a\u5305\u542b34\u79cd\u8bed\u8a00\u3001\u8d85\u8fc7\u4e00\u4e07\u4e2a\u9879\u76ee\u7684\u5e73\u884c\u591a\u8bed\u8a00\u591a\u6a21\u6001\u6f5c\u5728\u4e60\u8bed\u8868\u8fbe\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30NLP\u7cfb\u7edf\u5728\u8bed\u8a00\u548c\u6587\u5316\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u652f\u6301\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u7684\u4e60\u8bed\u7406\u89e3\u7814\u7a76\u3002", "motivation": "\u6f5c\u5728\u4e60\u8bed\u8868\u8fbe\u4e0e\u7279\u5b9a\u8bed\u8a00\u793e\u533a\u7684\u65e5\u5e38\u7ecf\u9a8c\u5bc6\u5207\u76f8\u5173\uff0c\u5bf9\u8bc4\u4f30NLP\u7cfb\u7edf\u7684\u8bed\u8a00\u548c\u6587\u5316\u7406\u89e3\u80fd\u529b\u6784\u6210\u6311\u6218\uff0c\u5f53\u524d\u7f3a\u4e4f\u80fd\u591f\u652f\u6301\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u6bd4\u8f83\u5206\u6790\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86XMPIE\u5e73\u884c\u591a\u8bed\u8a00\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b34\u79cd\u8bed\u8a00\u548c\u8d85\u8fc7\u4e00\u4e07\u4e2a\u9879\u76ee\uff0c\u6bcf\u4e2a\u6f5c\u5728\u4e60\u8bed\u8868\u8fbe\u914d\u6709\u4e94\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6\u4ece\u4e60\u8bed\u5230\u5b57\u9762\u610f\u4e49\u7684\u8fde\u7eed\u8c31\uff0c\u5305\u62ec\u8bed\u4e49\u76f8\u5173\u548c\u968f\u673a\u5e72\u6270\u9879\uff0c\u6570\u636e\u7531\u8bed\u8a00\u4e13\u5bb6\u6839\u636e\u591a\u8bed\u8a00\u6307\u5bfc\u539f\u5219\u521b\u5efa\u3002", "result": "XMPIE\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u591a\u8bed\u8a00\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u652f\u6301\u8bed\u8a00\u7279\u5b9a\u5b9e\u73b0\u548c\u504f\u597d\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u80fd\u591f\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u7684\u4e60\u8bed\u7406\u89e3\u6027\u80fd\uff0c\u4ee5\u53ca\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u7684\u7406\u89e3\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u4e60\u8bed\u7406\u89e3\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u7814\u7a76\u4e0d\u540c\u8bed\u8a00\u793e\u533a\u5171\u4eab\u7684\u6587\u5316\u65b9\u9762\uff0c\u5e76\u4e3a\u7406\u89e3\u4e60\u8bed\u77e5\u8bc6\u5728\u8bed\u8a00\u548c\u6a21\u6001\u95f4\u7684\u8fc1\u79fb\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2601.08026", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08026", "abs": "https://arxiv.org/abs/2601.08026", "authors": ["Jifeng Song", "Arun Das", "Pan Wang", "Hui Ji", "Kun Zhao", "Yufei Huang"], "title": "FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures", "comment": null, "summary": "Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFigEx2\uff0c\u4e00\u79cd\u89c6\u89c9\u6761\u4ef6\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u79d1\u5b66\u590d\u5408\u56fe\u4e2d\u5b9a\u4f4d\u9762\u677f\u5e76\u751f\u6210\u9762\u677f\u7ea7\u63cf\u8ff0\uff0c\u901a\u8fc7\u566a\u58f0\u611f\u77e5\u95e8\u63a7\u878d\u5408\u6a21\u5757\u548c\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u548c\u63cf\u8ff0\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u79d1\u5b66\u590d\u5408\u56fe\u5c06\u591a\u4e2a\u5e26\u6807\u7b7e\u9762\u677f\u7ec4\u5408\u6210\u5355\u4e00\u56fe\u50cf\uff0c\u4f46\u5b9e\u9645\u6d41\u7a0b\u4e2d\u7684\u56fe\u6ce8\u7ecf\u5e38\u7f3a\u5931\u6216\u4ec5\u63d0\u4f9b\u56fe\u7ea7\u6458\u8981\uff0c\u8fd9\u4f7f\u5f97\u9762\u677f\u7ea7\u7406\u89e3\u53d8\u5f97\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u76f4\u63a5\u4ece\u590d\u5408\u56fe\u4e2d\u5b9a\u4f4d\u9762\u677f\u5e76\u751f\u6210\u9762\u677f\u7ea7\u63cf\u8ff0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FigEx2\u91c7\u7528\u89c6\u89c9\u6761\u4ef6\u5316\u6846\u67b6\uff0c\u5305\u542b\u566a\u58f0\u611f\u77e5\u95e8\u63a7\u878d\u5408\u6a21\u5757\u4ee5\u81ea\u9002\u5e94\u8fc7\u6ee4\u6807\u8bb0\u7ea7\u7279\u5f81\u6765\u7a33\u5b9a\u68c0\u6d4b\u67e5\u8be2\u7a7a\u95f4\uff0c\u5e76\u91c7\u7528\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff0c\u5229\u7528\u57fa\u4e8eCLIP\u7684\u5bf9\u9f50\u548c\u57fa\u4e8eBERTScore\u7684\u8bed\u4e49\u5956\u52b1\u6765\u5f3a\u5236\u4e25\u683c\u7684\u591a\u6a21\u6001\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aFigEx2\u5728\u68c0\u6d4b\u65b9\u9762\u8fbe\u52300.726 mAP@0.5:0.95\u7684\u4f18\u5f02\u6027\u80fd\uff0c\u5728METEOR\u548cBERTScore\u6307\u6807\u4e0a\u5206\u522b\u663e\u8457\u8d85\u8d8aQwen3-VL-8B\u6a21\u578b0.51\u548c0.24\u5206\uff0c\u5e76\u5728\u672a\u7ecf\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5c55\u73b0\u51fa\u5bf9\u5206\u5e03\u5916\u79d1\u5b66\u9886\u57df\u7684\u5353\u8d8a\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u521b\u65b0\u7684\u566a\u58f0\u611f\u77e5\u878d\u5408\u673a\u5236\u548c\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79d1\u5b66\u590d\u5408\u56fe\u7684\u9762\u677f\u7ea7\u7406\u89e3\u95ee\u9898\uff0c\u6784\u5efa\u7684BioSci-Fig-Cap\u57fa\u51c6\u548c\u8de8\u5b66\u79d1\u6d4b\u8bd5\u5957\u4ef6\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\uff0c\u5c55\u793a\u4e86\u5728\u79d1\u5b66\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.08457", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08457", "abs": "https://arxiv.org/abs/2601.08457", "authors": ["Sargam Yadav", "Abhishek Kaushik", "Kevin Mc Daid"], "title": "An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English", "comment": null, "summary": "Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u53ef\u89e3\u91ca\u7684Web\u5e94\u7528\u7a0b\u5e8f\uff0c\u7528\u4e8e\u68c0\u6d4b\u5370\u5730\u8bed-\u82f1\u8bed\u6df7\u5408\u8bed\u8a00\u6587\u672c\u548c\u8868\u60c5\u5305\u4e2d\u7684\u538c\u5973\u5185\u5bb9\uff0c\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u5148\u8fdb\u7684Transformer\u6a21\u578b\u548cSHAP\u3001LIME\u7b49\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5185\u5bb9\u5ba1\u6838\u5458\u63d0\u4f9b\u900f\u660e\u5316\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u5de5\u5177\u3002", "motivation": "\u6570\u5b57\u5e73\u53f0\u7528\u6237\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u4f46\u540c\u65f6\u4e5f\u52a9\u957f\u4e86\u4ec7\u6068\u8a00\u8bba\u548c\u538c\u5973\u5185\u5bb9\u7684\u4f20\u64ad\uff0c\u73b0\u6709\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6df7\u5408\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5e94\u7528\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7b49\u654f\u611f\u9886\u57df\u4e2d\u5c24\u4e3a\u5173\u952e\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u591a\u8bed\u8a00\u591a\u6a21\u6001\u67b6\u6784\uff0c\u6587\u672c\u68c0\u6d4b\u4f7f\u7528XLM-RoBERTa\u548cmBERT\u6a21\u578b\u5904\u7406\u7ea64,193\u6761\u8bc4\u8bba\uff0c\u591a\u6a21\u6001\u8868\u60c5\u5305\u68c0\u6d4b\u7ed3\u5408mBERT\u4e0eEfficientNet\u3001ResNET\u6a21\u578b\u5904\u7406\u7ea64,218\u4e2a\u8868\u60c5\u5305\uff0c\u5e76\u96c6\u6210SHAP\u548cLIME\u6280\u672f\u63d0\u4f9b\u7279\u5f81\u91cd\u8981\u6027\u89e3\u91ca\u3002", "result": "\u7cfb\u7edf\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u8005\u4f7f\u7528\u804a\u5929\u673a\u5668\u4eba\u53ef\u7528\u6027\u95ee\u5377\u548c\u7528\u6237\u4f53\u9a8c\u95ee\u5377\u8fdb\u884c\u8bc4\u4f30\uff0c\u786e\u5b9a\u4e86\u6574\u4f53\u53ef\u7528\u6027\uff0c\u4f46\u5177\u4f53\u6027\u80fd\u6307\u6807\u672a\u5728\u6458\u8981\u4e2d\u8be6\u7ec6\u8bf4\u660e\uff0c\u8bc4\u4f30\u91cd\u70b9\u5728\u4e8e\u7528\u6237\u4f53\u9a8c\u548c\u7cfb\u7edf\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df7\u5408\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u538c\u5973\u5185\u5bb9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u900f\u660e\u5316\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u5728\u654f\u611f\u9886\u57df\u7684\u5e94\u7528\uff0c\u6709\u52a9\u4e8e\u6253\u51fb\u57fa\u4e8e\u6027\u522b\u7684\u6570\u5b57\u66b4\u529b\uff0c\u786e\u4fdd\u5b89\u5168\u7684\u6570\u5b57\u7a7a\u95f4\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.08741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08741", "abs": "https://arxiv.org/abs/2601.08741", "authors": ["Anmol Gulati", "Sahil Sen", "Waqar Sarguroh", "Kevin Paul"], "title": "From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding", "comment": null, "summary": "Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FRTR-Bench\uff0c\u9996\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u7535\u5b50\u8868\u683c\u63a8\u7406\u57fa\u51c6\uff0c\u4ee5\u53caFRTR\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5d4c\u5165\u5206\u89e3\u3001\u6df7\u5408\u68c0\u7d22\u548c\u89c6\u89c9\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u590d\u6742\u4f01\u4e1a\u7535\u5b50\u8868\u683c\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5305\u542b\u6570\u5343\u884c\u6570\u503c\u3001\u591a\u4e2a\u5173\u8054\u5de5\u4f5c\u8868\u4ee5\u53ca\u56fe\u8868\u3001\u6536\u636e\u7b49\u5d4c\u5165\u89c6\u89c9\u5185\u5bb9\u7684\u4f01\u4e1a\u7ea7\u7535\u5b50\u8868\u683c\u65f6\u5b58\u5728\u63a8\u7406\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u8868\u538b\u7f29\u6216\u5168\u4e0a\u4e0b\u6587\u7f16\u7801\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u4e14\u65e0\u6cd5\u53cd\u6620\u7528\u6237\u4e0e\u590d\u6742\u591a\u6a21\u6001\u5de5\u4f5c\u7c3f\u7684\u771f\u5b9e\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86FRTR-Bench\u57fa\u51c6\uff0c\u5305\u542b30\u4e2a\u4f01\u4e1a\u7ea7Excel\u5de5\u4f5c\u7c3f\uff0c\u6db5\u76d6\u8fd1\u56db\u767e\u4e07\u4e2a\u5355\u5143\u683c\u548c50\u591a\u4e2a\u5d4c\u5165\u56fe\u50cf\u3002\u4e3a\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff0c\u5f00\u53d1\u4e86FRTR\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06Excel\u5de5\u4f5c\u7c3f\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7684\u884c\u3001\u5217\u548c\u5757\u5d4c\u5165\uff0c\u91c7\u7528\u57fa\u4e8e\u4e92\u9006\u6392\u5e8f\u878d\u5408\u7684\u6df7\u5408\u8bcd\u6c47-\u7a20\u5bc6\u68c0\u7d22\uff0c\u5e76\u6574\u5408\u591a\u6a21\u6001\u5d4c\u5165\u4ee5\u540c\u65f6\u63a8\u7406\u6570\u503c\u548c\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u5728\u516d\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u6d4b\u8bd5FRTR\uff0c\u5728FRTR-Bench\u57fa\u51c6\u4e0a\u4f7f\u7528Claude Sonnet 4.5\u8fbe\u523074%\u7684\u7b54\u6848\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u4e4b\u524d\u4ec524%\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002\u5728SpreadsheetLLM\u57fa\u51c6\u4e0a\uff0cFRTR\u4f7f\u7528GPT-5\u8fbe\u523087%\u51c6\u786e\u7387\uff0c\u540c\u65f6\u76f8\u6bd4\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\u51cf\u5c11\u4e86\u7ea650%\u7684\u4ee4\u724c\u4f7f\u7528\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5d4c\u5165\u5206\u89e3\u548c\u6df7\u5408\u68c0\u7d22\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u4fe1\u606f\u6574\u5408\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u590d\u6742\u4f01\u4e1a\u7535\u5b50\u8868\u683c\u7684\u63a8\u7406\u80fd\u529b\u3002FRTR\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u5b9e\u9645\u4f01\u4e1a\u5e94\u7528\u4e2d\u7684\u7535\u5b50\u8868\u683c\u81ea\u52a8\u5316\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.08040", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08040", "abs": "https://arxiv.org/abs/2601.08040", "authors": ["Soumyaroop Nandi", "Prem Natarajan"], "title": "Rescind: Countering Image Misconduct in Biomedical Publications with Vision-Language and State-Space Modeling", "comment": null, "summary": "Scientific image manipulation in biomedical publications poses a growing threat to research integrity and reproducibility. Unlike natural image forensics, biomedical forgery detection is uniquely challenging due to domain-specific artifacts, complex textures, and unstructured figure layouts. We present the first vision-language guided framework for both generating and detecting biomedical image forgeries. By combining diffusion-based synthesis with vision-language prompting, our method enables realistic and semantically controlled manipulations, including duplication, splicing, and region removal, across diverse biomedical modalities. We introduce Rescind, a large-scale benchmark featuring fine-grained annotations and modality-specific splits, and propose Integscan, a structured state space modeling framework that integrates attention-enhanced visual encoding with prompt-conditioned semantic alignment for precise forgery localization. To ensure semantic fidelity, we incorporate a vision-language model based verification loop that filters generated forgeries based on consistency with intended prompts. Extensive experiments on Rescind and existing benchmarks demonstrate that Integscan achieves state of the art performance in both detection and localization, establishing a strong foundation for automated scientific integrity analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u89c6\u89c9\u8bed\u8a00\u5f15\u5bfc\u7684\u751f\u7269\u533b\u5b66\u56fe\u50cf\u4f2a\u9020\u751f\u6210\u4e0e\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u5408\u6210\u4e0e\u89c6\u89c9\u8bed\u8a00\u63d0\u793a\uff0c\u5b9e\u73b0\u4e86\u5bf9\u751f\u7269\u533b\u5b66\u56fe\u50cf\u4e2d\u590d\u5236\u3001\u62fc\u63a5\u548c\u533a\u57df\u79fb\u9664\u7b49\u64cd\u4f5c\u7684\u903c\u771f\u4e14\u8bed\u4e49\u53ef\u63a7\u7684\u4f2a\u9020\uff0c\u5e76\u5efa\u7acb\u4e86\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6Rescind\u548c\u68c0\u6d4b\u6846\u67b6Integscan\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u51fa\u7248\u7269\u4e2d\u7684\u79d1\u5b66\u56fe\u50cf\u7be1\u6539\u5bf9\u7814\u7a76\u5b8c\u6574\u6027\u548c\u53ef\u91cd\u590d\u6027\u6784\u6210\u65e5\u76ca\u4e25\u91cd\u7684\u5a01\u80c1\uff0c\u4e0e\u81ea\u7136\u56fe\u50cf\u53d6\u8bc1\u4e0d\u540c\uff0c\u751f\u7269\u533b\u5b66\u4f2a\u9020\u68c0\u6d4b\u9762\u4e34\u9886\u57df\u7279\u5b9a\u4f2a\u5f71\u3001\u590d\u6742\u7eb9\u7406\u548c\u975e\u7ed3\u6784\u5316\u56fe\u50cf\u5e03\u5c40\u7b49\u72ec\u7279\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u5408\u6210\u4e0e\u89c6\u89c9\u8bed\u8a00\u63d0\u793a\u7684\u751f\u6210\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u5faa\u73af\u786e\u4fdd\u8bed\u4e49\u4fdd\u771f\u5ea6\uff1b\u5efa\u7acb\u4e86\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6Rescind\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u6807\u6ce8\u548c\u6a21\u6001\u7279\u5b9a\u5212\u5206\uff1b\u8bbe\u8ba1\u4e86Integscan\u68c0\u6d4b\u6846\u67b6\uff0c\u91c7\u7528\u6ce8\u610f\u529b\u589e\u5f3a\u7684\u89c6\u89c9\u7f16\u7801\u4e0e\u63d0\u793a\u6761\u4ef6\u8bed\u4e49\u5bf9\u9f50\u7684\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u65b9\u6cd5\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u4f2a\u9020\u5b9a\u4f4d\u3002", "result": "\u5728Rescind\u548c\u73b0\u6709\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cIntegscan\u5728\u68c0\u6d4b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u4e3a\u81ea\u52a8\u5316\u79d1\u5b66\u5b8c\u6574\u6027\u5206\u6790\u5efa\u7acb\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u5728\u591a\u79cd\u751f\u7269\u533b\u5b66\u6a21\u6001\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u7684\u89c6\u89c9\u8bed\u8a00\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u4e0e\u68c0\u6d4b\u7684\u534f\u540c\u8bbe\u8ba1\u89e3\u51b3\u4e86\u9886\u57df\u7279\u5b9a\u6311\u6218\uff0c\u6240\u5efa\u7acb\u7684\u6570\u636e\u96c6\u548c\u68c0\u6d4b\u65b9\u6cd5\u4e3a\u79d1\u5b66\u5b8c\u6574\u6027\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u548c\u7814\u7a76\u610f\u4e49\u3002"}}
{"id": "2601.08509", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08509", "abs": "https://arxiv.org/abs/2601.08509", "authors": ["Jinkwan Jang", "Hyunbin Jin", "Hyungjin Park", "Kyubyung Chae", "Taesup Kim"], "title": "What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting", "comment": "30 pages, 5 figures", "summary": "Time series forecasting is critical to real-world decision making, yet most existing approaches remain unimodal and rely on extrapolating historical patterns. While recent progress in large language models (LLMs) highlights the potential for multimodal forecasting, existing benchmarks largely provide retrospective or misaligned raw context, making it unclear whether such models meaningfully leverage textual inputs. In practice, human experts incorporate what-if scenarios with historical evidence, often producing distinct forecasts from the same observations under different scenarios. Inspired by this, we introduce What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting. The benchmark is available at https://github.com/jinkwan1115/WhatIfTSF.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86What If TSF\uff08WIT\uff09\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u80fd\u591f\u57fa\u4e8e\u4e0a\u4e0b\u6587\u6587\u672c\uff08\u7279\u522b\u662f\u672a\u6765\u573a\u666f\uff09\u8fdb\u884c\u6761\u4ef6\u9884\u6d4b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u5728\u573a\u666f\u5f15\u5bfc\u9884\u6d4b\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5927\u591a\u4e3a\u5355\u6a21\u6001\u4e14\u4f9d\u8d56\u5386\u53f2\u6a21\u5f0f\u5916\u63a8\uff0c\u800c\u5f53\u524d\u591a\u6a21\u6001\u9884\u6d4b\u57fa\u51c6\u4e3b\u8981\u63d0\u4f9b\u56de\u987e\u6027\u6216\u672a\u5bf9\u9f50\u7684\u539f\u59cb\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u660e\u786e\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u771f\u6b63\u5229\u7528\u6587\u672c\u8f93\u5165\u8fdb\u884c\u9884\u6d4b\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u4eba\u7c7b\u4e13\u5bb6\u4f1a\u7ed3\u5408\u5386\u53f2\u8bc1\u636e\u548c\u5047\u8bbe\u573a\u666f\uff0c\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u57fa\u4e8e\u76f8\u540c\u89c2\u6d4b\u4ea7\u751f\u4e0d\u540c\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86What If TSF\uff08WIT\uff09\u591a\u6a21\u6001\u9884\u6d4b\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u901a\u8fc7\u63d0\u4f9b\u4e13\u5bb6\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5408\u7406\u6216\u53cd\u4e8b\u5b9e\u573a\u666f\uff0c\u4e3a\u573a\u666f\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u9884\u6d4b\u63d0\u4f9b\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002\u57fa\u51c6\u8bbe\u8ba1\u65e8\u5728\u8bc4\u4f30\u6a21\u578b\u80fd\u5426\u57fa\u4e8e\u4e0a\u4e0b\u6587\u6587\u672c\uff08\u7279\u522b\u662f\u672a\u6765\u573a\u666f\uff09\u5bf9\u5176\u9884\u6d4b\u8fdb\u884c\u6761\u4ef6\u5316\u5904\u7406\u3002", "result": "WIT\u57fa\u51c6\u63d0\u4f9b\u4e86\u4e13\u5bb6\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5408\u7406\u6216\u53cd\u4e8b\u5b9e\u573a\u666f\uff0c\u4e3a\u573a\u666f\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u9884\u6d4b\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u57fa\u51c6\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u771f\u6b63\u5229\u7528\u6587\u672c\u4e0a\u4e0b\u6587\u8fdb\u884c\u9884\u6d4b\u7684\u6807\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u573a\u666f\u5f15\u5bfc\u9884\u6d4b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u80fd\u529b\u7684\u6807\u51c6\u5316\u57fa\u51c6\u3002WIT\u57fa\u51c6\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5982\u4f55\u6709\u6548\u6574\u5408\u6587\u672c\u573a\u666f\u4fe1\u606f\u8fdb\u884c\u9884\u6d4b\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\u548c\u65b9\u5411\u3002"}}
{"id": "2601.08095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08095", "abs": "https://arxiv.org/abs/2601.08095", "authors": ["Dongsik Yoon", "Jongeun Kim"], "title": "From Prompts to Deployment: Auto-Curated Domain-Specific Dataset Generation via Diffusion Models", "comment": "To appear in the Workshop on Synthetic & Adversarial ForEnsics (SAFE), WACV 2026 (oral presentation)", "summary": "In this paper, we present an automated pipeline for generating domain-specific synthetic datasets with diffusion models, addressing the distribution shift between pre-trained models and real-world deployment environments. Our three-stage framework first synthesizes target objects within domain-specific backgrounds through controlled inpainting. The generated outputs are then validated via a multi-modal assessment that integrates object detection, aesthetic scoring, and vision-language alignment. Finally, a user-preference classifier is employed to capture subjective selection criteria. This pipeline enables the efficient construction of high-quality, deployable datasets while reducing reliance on extensive real-world data collection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u751f\u6210\u9886\u57df\u7279\u5b9a\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6846\u67b6\u89e3\u51b3\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u771f\u5b9e\u90e8\u7f72\u73af\u5883\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u6536\u96c6\u7684\u4f9d\u8d56\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u73af\u5883\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u8db3\u591f\u9886\u57df\u7279\u5b9a\u771f\u5b9e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u771f\u5b9e\u6570\u636e\u6536\u96c6\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u6846\u67b6\uff1a\u9996\u5148\u901a\u8fc7\u53d7\u63a7\u4fee\u590d\u6280\u672f\u5c06\u76ee\u6807\u5bf9\u8c61\u5408\u6210\u5230\u9886\u57df\u7279\u5b9a\u80cc\u666f\u4e2d\uff1b\u7136\u540e\u91c7\u7528\u591a\u6a21\u6001\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5305\u62ec\u5bf9\u8c61\u68c0\u6d4b\u3001\u7f8e\u5b66\u8bc4\u5206\u548c\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\uff1b\u6700\u540e\u4f7f\u7528\u7528\u6237\u504f\u597d\u5206\u7c7b\u5668\u6765\u6355\u6349\u4e3b\u89c2\u9009\u62e9\u6807\u51c6\uff0c\u786e\u4fdd\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\u548c\u9002\u7528\u6027\u3002", "result": "\u8be5\u6d41\u6c34\u7ebf\u80fd\u591f\u9ad8\u6548\u6784\u5efa\u9ad8\u8d28\u91cf\u3001\u53ef\u90e8\u7f72\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6536\u96c6\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8bc4\u4f30\u786e\u4fdd\u4e86\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\uff0c\u7528\u6237\u504f\u597d\u5206\u7c7b\u5668\u7684\u5f15\u5165\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6570\u636e\u7684\u4e3b\u89c2\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u7cfb\u7edf\u5316\u9a8c\u8bc1\u6d41\u7a0b\u7684\u7ed3\u5408\uff0c\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u8fd8\u4e3a\u5b9e\u9645\u90e8\u7f72\u73af\u5883\u4e2d\u7684\u6570\u636e\u9700\u6c42\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u8df5\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.08531", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08531", "abs": "https://arxiv.org/abs/2601.08531", "authors": ["Warissara Booranamaitree", "Xusheng Du", "Yushu Cai", "Zhengyang Wang", "Ye Zhang", "Haoran Xie"], "title": "Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse", "comment": "10 pages, 9 figures, Proceedings of CAADRIA 2026", "summary": "Facade renovation offers a more sustainable alternative to full demolition, yet producing design proposals that preserve existing structures while expressing new intent remains challenging. Current workflows typically require detailed as-built modelling before design, which is time-consuming, labour-intensive, and often involves repeated revisions. To solve this issue, we propose a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) that directly processes rough structural sketch and textual descriptions to produce consistent renovation proposals. First, the input sketch is used by a fine-tuned VLM model to predict bounding boxes specifying where modifications are needed and which components should be added. Next, a stable diffusion model generates detailed sketches of new elements, which are merged with the original outline through a generative inpainting pipeline. Finally, ControlNet is employed to refine the result into a photorealistic image. Experiments on datasets and real industrial buildings indicate that the proposed framework can generate renovation proposals that preserve the original structure while improving facade detail quality. This approach effectively bypasses the need for detailed as-built modelling, enabling architects to rapidly explore design alternatives, iterate on early-stage concepts, and communicate renovation intentions with greater clarity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u80fd\u591f\u76f4\u63a5\u5904\u7406\u7c97\u7565\u7ed3\u6784\u8349\u56fe\u548c\u6587\u672c\u63cf\u8ff0\u6765\u751f\u6210\u4e00\u81f4\u7684\u7acb\u9762\u6539\u9020\u65b9\u6848\uff0c\u6709\u6548\u7ed5\u8fc7\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u9700\u8981\u8be6\u7ec6\u7ae3\u5de5\u5efa\u6a21\u7684\u7e41\u7410\u6d41\u7a0b\u3002", "motivation": "\u7acb\u9762\u6539\u9020\u4f5c\u4e3a\u6bd4\u5b8c\u5168\u62c6\u9664\u66f4\u53ef\u6301\u7eed\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5f53\u524d\u5de5\u4f5c\u6d41\u7a0b\u901a\u5e38\u9700\u8981\u5728\u8bbe\u8ba1\u524d\u8fdb\u884c\u8be6\u7ec6\u7684\u7ae3\u5de5\u5efa\u6a21\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u8017\u65f6\u8017\u529b\u4e14\u7ecf\u5e38\u6d89\u53ca\u91cd\u590d\u4fee\u8ba2\uff0c\u963b\u788d\u4e86\u5efa\u7b51\u5e08\u5feb\u901f\u63a2\u7d22\u8bbe\u8ba1\u65b9\u6848\u548c\u8fed\u4ee3\u65e9\u671f\u6982\u5ff5\u7684\u80fd\u529b\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6839\u636e\u8f93\u5165\u8349\u56fe\u9884\u6d4b\u9700\u8981\u4fee\u6539\u533a\u57df\u7684\u8fb9\u754c\u6846\u548c\u5e94\u6dfb\u52a0\u7684\u7ec4\u4ef6\uff1b\u63a5\u7740\u901a\u8fc7\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u751f\u6210\u65b0\u5143\u7d20\u7684\u8be6\u7ec6\u8349\u56fe\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5f0f\u4fee\u590d\u7ba1\u9053\u5c06\u5176\u4e0e\u539f\u59cb\u8f6e\u5ed3\u5408\u5e76\uff1b\u6700\u540e\u5229\u7528ControlNet\u5c06\u7ed3\u679c\u7ec6\u5316\u4e3a\u903c\u771f\u7684\u56fe\u50cf\u3002", "result": "\u5728\u6570\u636e\u96c6\u548c\u771f\u5b9e\u5de5\u4e1a\u5efa\u7b51\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u65e2\u4fdd\u7559\u539f\u59cb\u7ed3\u6784\u53c8\u63d0\u5347\u7acb\u9762\u7ec6\u8282\u8d28\u91cf\u7684\u6539\u9020\u65b9\u6848\uff0c\u6709\u6548\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u7ed5\u8fc7\u8be6\u7ec6\u7ae3\u5de5\u5efa\u6a21\u9700\u6c42\u65b9\u9762\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5efa\u7b51\u5e08\u63d0\u4f9b\u4e86\u5feb\u901f\u63a2\u7d22\u8bbe\u8ba1\u66ff\u4ee3\u65b9\u6848\u3001\u8fed\u4ee3\u65e9\u671f\u6982\u5ff5\u5e76\u4ee5\u66f4\u6e05\u6670\u65b9\u5f0f\u4f20\u8fbe\u6539\u9020\u610f\u56fe\u7684\u5de5\u5177\uff0c\u4ee3\u8868\u4e86\u751f\u6210\u5f0fAI\u5728\u5efa\u7b51\u6539\u9020\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\u7684\u521b\u65b0\u5e94\u7528\uff0c\u5177\u6709\u663e\u8457\u7684\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2601.08133", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08133", "abs": "https://arxiv.org/abs/2601.08133", "authors": ["Peng Gao", "Yujian Lee", "Yongqi Xu", "Wentao Fan"], "title": "How Do Optical Flow and Textual Prompts Collaborate to Assist in Audio-Visual Semantic Segmentation?", "comment": null, "summary": "Audio-visual semantic segmentation (AVSS) represents an extension of the audio-visual segmentation (AVS) task, necessitating a semantic understanding of audio-visual scenes beyond merely identifying sound-emitting objects at the visual pixel level. Contrary to a previous methodology, by decomposing the AVSS task into two discrete subtasks by initially providing a prompted segmentation mask to facilitate subsequent semantic analysis, our approach innovates on this foundational strategy. We introduce a novel collaborative framework, \\textit{S}tepping \\textit{S}tone \\textit{P}lus (SSP), which integrates optical flow and textual prompts to assist the segmentation process. In scenarios where sound sources frequently coexist with moving objects, our pre-mask technique leverages optical flow to capture motion dynamics, providing essential temporal context for precise segmentation. To address the challenge posed by stationary sound-emitting objects, such as alarm clocks, SSP incorporates two specific textual prompts: one identifies the category of the sound-emitting object, and the other provides a broader description of the scene. Additionally, we implement a visual-textual alignment module (VTA) to facilitate cross-modal integration, delivering more coherent and contextually relevant semantic interpretations. Our training regimen involves a post-mask technique aimed at compelling the model to learn the diagram of the optical flow. Experimental results demonstrate that SSP outperforms existing AVS methods, delivering efficient and precise segmentation results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSSP\uff08Stepping Stone Plus\uff09\u7684\u65b0\u578b\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u97f3\u9891-\u89c6\u89c9\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u5149\u6d41\u548c\u6587\u672c\u63d0\u793a\u6765\u589e\u5f3a\u5206\u5272\u7cbe\u5ea6\uff0c\u5e76\u5728\u590d\u6742\u573a\u666f\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709AVS\u65b9\u6cd5\u3002", "motivation": "\u97f3\u9891-\u89c6\u89c9\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u7684\u53d1\u58f0\u5bf9\u8c61\u8bc6\u522b\uff0c\u5b9e\u73b0\u573a\u666f\u7684\u8bed\u4e49\u7406\u89e3\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u4f46\u9762\u4e34\u8fd0\u52a8\u5bf9\u8c61\u548c\u9759\u6b62\u53d1\u58f0\u5bf9\u8c61\uff08\u5982\u95f9\u949f\uff09\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u65f6\u7a7a\u4e0a\u4e0b\u6587\u548c\u8bed\u4e49\u6574\u5408\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86SSP\u534f\u4f5c\u6846\u67b6\uff0c\u91c7\u7528\u9884\u63a9\u7801\u6280\u672f\u5229\u7528\u5149\u6d41\u6355\u6349\u8fd0\u52a8\u52a8\u6001\uff0c\u4e3a\u7cbe\u786e\u5206\u5272\u63d0\u4f9b\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002\u9488\u5bf9\u9759\u6b62\u53d1\u58f0\u5bf9\u8c61\uff0cSSP\u6574\u5408\u4e86\u4e24\u79cd\u6587\u672c\u63d0\u793a\uff1a\u5bf9\u8c61\u7c7b\u522b\u8bc6\u522b\u548c\u573a\u666f\u63cf\u8ff0\u3002\u6b64\u5916\uff0c\u8fd8\u5b9e\u73b0\u4e86\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u6a21\u5757\u4ee5\u4fc3\u8fdb\u8de8\u6a21\u6001\u6574\u5408\uff0c\u5e76\u91c7\u7528\u540e\u63a9\u7801\u6280\u672f\u8bad\u7ec3\u6a21\u578b\u5b66\u4e60\u5149\u6d41\u56fe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSSP\u6846\u67b6\u5728\u97f3\u9891-\u89c6\u89c9\u5206\u5272\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709AVS\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u5206\u5272\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408\u5149\u6d41\u548c\u6587\u672c\u63d0\u793a\u7684\u534f\u4f5c\u6846\u67b6\u5728\u97f3\u9891-\u89c6\u89c9\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5904\u7406\u8fd0\u52a8\u5bf9\u8c61\u548c\u9759\u6b62\u53d1\u58f0\u5bf9\u8c61\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u573a\u666f\u7406\u89e3\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2601.08620", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08620", "abs": "https://arxiv.org/abs/2601.08620", "authors": ["Ant\u00f3nio Loison", "Quentin Mac\u00e9", "Antoine Edy", "Victor Xing", "Tom Balough", "Gabriel Moreira", "Bo Liu", "Manuel Faysse", "C\u00e9line Hudelot", "Gautier Viaud"], "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ViDoRe v3\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u6a21\u578b\u5904\u7406\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u3001\u591a\u7c7b\u578b\u67e5\u8be2\u548c\u591a\u8bed\u8a00\u573a\u666f\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524dRAG\u7cfb\u7edf\u5728\u89c6\u89c9\u7406\u89e3\u548c\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u6570\u636e\u3001\u5355\u6587\u6863\u7406\u89e3\u6216\u5b64\u7acb\u8bc4\u4f30\u68c0\u7d22\u4e0e\u751f\u6210\u7ec4\u4ef6\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u5e94\u7528\u4e2d\u5904\u7406\u89c6\u89c9\u5143\u7d20\uff08\u8868\u683c\u3001\u56fe\u8868\u3001\u56fe\u50cf\uff09\u3001\u8de8\u6587\u6863\u4fe1\u606f\u5408\u6210\u548c\u51c6\u786e\u6765\u6e90\u5b9a\u4f4d\u7684\u590d\u6742\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u591a\u6a21\u6001RAG\u7cfb\u7edf\u7684\u6709\u6548\u8bc4\u4f30\u4e0e\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86ViDoRe v3\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b10\u4e2a\u4e13\u4e1a\u9886\u57df\u6570\u636e\u96c6\uff0c\u7ea626,000\u4e2a\u6587\u6863\u9875\u9762\u548c3,099\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u67e5\u8be2\uff0c\u652f\u63016\u79cd\u8bed\u8a00\uff0c\u901a\u8fc712,000\u5c0f\u65f6\u4eba\u5de5\u6807\u6ce8\u63d0\u4f9b\u68c0\u7d22\u76f8\u5173\u6027\u3001\u8fb9\u754c\u6846\u5b9a\u4f4d\u548c\u9a8c\u8bc1\u53c2\u8003\u7b54\u6848\u7684\u9ad8\u8d28\u91cf\u6807\u6ce8\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684RAG\u6d41\u6c34\u7ebf\u5728\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u89c6\u89c9\u68c0\u7d22\u5668\u4f18\u4e8e\u6587\u672c\u68c0\u7d22\u5668\uff0c\u665a\u671f\u4ea4\u4e92\u6a21\u578b\u548c\u6587\u672c\u91cd\u6392\u5e8f\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6df7\u5408\u6216\u7eaf\u89c6\u89c9\u4e0a\u4e0b\u6587\u80fd\u6539\u5584\u7b54\u6848\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u975e\u6587\u672c\u5143\u7d20\u3001\u5f00\u653e\u5f0f\u67e5\u8be2\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5b9a\u4f4d\u65b9\u9762\u4ecd\u5b58\u5728\u660e\u663e\u56f0\u96be\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001RAG\u7cfb\u7edf\u5728\u89c6\u89c9\u7406\u89e3\u548c\u8de8\u6587\u6863\u63a8\u7406\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u901a\u8fc7\u5546\u4e1a\u53cb\u597d\u8bb8\u53ef\u53d1\u5e03\u7684\u6570\u636e\u96c6\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u5904\u7406\u548c\u7ec6\u7c92\u5ea6\u4fe1\u606f\u5b9a\u4f4d\u65b9\u9762\u7684\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2601.08139", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08139", "abs": "https://arxiv.org/abs/2601.08139", "authors": ["Zhichen Zeng", "Wenxuan Bao", "Xiao Lin", "Ruizhong Qiu", "Tianxin Wei", "Xuying Ning", "Yuchen Yan", "Chen Luo", "Monica Xiao Cheng", "Jingrui He", "Hanghang Tong"], "title": "Subspace Alignment for Vision-Language Model Test-time Adaptation", "comment": "17 pages, 10 figures", "summary": "Vision-language models (VLMs), despite their extraordinary zero-shot capabilities, are vulnerable to distribution shifts. Test-time adaptation (TTA) emerges as a predominant strategy to adapt VLMs to unlabeled test data on the fly. However, existing TTA methods heavily rely on zero-shot predictions as pseudo-labels for self-training, which can be unreliable under distribution shifts and misguide adaptation due to two fundamental limitations. First (Modality Gap), distribution shifts induce gaps between visual and textual modalities, making cross-modal relations inaccurate. Second (Visual Nuisance), visual embeddings encode rich but task-irrelevant noise that often overwhelms task-specific semantics under distribution shifts. To address these limitations, we propose SubTTA, which aligns the semantic subspaces of both modalities to enhance zero-shot predictions to better guide the TTA process. To bridge the modality gap, SubTTA extracts the principal subspaces of both modalities and aligns the visual manifold to the textual semantic anchor by minimizing their chordal distance. To eliminate visual nuisance, SubTTA projects the aligned visual features onto the task-specific textual subspace, which filters out task-irrelevant noise by constraining visual embeddings within the valid semantic span, and standard TTA is further performed on the purified space to refine the decision boundaries. Extensive experiments on various benchmarks and VLM architectures demonstrate the effectiveness of SubTTA, yielding an average improvement of 2.24% over state-of-the-art TTA methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSubTTA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u7684\u8bed\u4e49\u5b50\u7a7a\u95f4\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5206\u5e03\u504f\u79fb\u4e0b\u4f2a\u6807\u7b7e\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53472.24%\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5177\u6709\u51fa\u8272\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u4f46\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u8106\u5f31\u3002\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u96f6\u6837\u672c\u9884\u6d4b\u4f5c\u4e3a\u4f2a\u6807\u7b7e\u8fdb\u884c\u81ea\u8bad\u7ec3\uff0c\u4f46\u8fd9\u4e9b\u4f2a\u6807\u7b7e\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4e0d\u53ef\u9760\uff0c\u4e3b\u8981\u53d7\u4e24\u4e2a\u57fa\u672c\u9650\u5236\u5f71\u54cd\uff1a\u6a21\u6001\u95f4\u9699\u5bfc\u81f4\u8de8\u6a21\u6001\u5173\u7cfb\u4e0d\u51c6\u786e\uff0c\u4ee5\u53ca\u89c6\u89c9\u5d4c\u5165\u7f16\u7801\u4e86\u4e30\u5bcc\u4f46\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u566a\u58f0\uff0c\u8fd9\u4e9b\u566a\u58f0\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u5e38\u5e38\u6df9\u6ca1\u4efb\u52a1\u7279\u5b9a\u7684\u8bed\u4e49\u3002", "method": "\u672c\u6587\u63d0\u51faSubTTA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u4e24\u79cd\u6a21\u6001\u7684\u8bed\u4e49\u5b50\u7a7a\u95f4\u6765\u589e\u5f3a\u96f6\u6837\u672c\u9884\u6d4b\u4ee5\u66f4\u597d\u5730\u6307\u5bfcTTA\u8fc7\u7a0b\u3002\u4e3a\u5f25\u5408\u6a21\u6001\u95f4\u9699\uff0cSubTTA\u63d0\u53d6\u4e24\u79cd\u6a21\u6001\u7684\u4e3b\u5b50\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u5316\u5f26\u8ddd\u79bb\u5c06\u89c6\u89c9\u6d41\u5f62\u5bf9\u9f50\u5230\u6587\u672c\u8bed\u4e49\u951a\u70b9\u3002\u4e3a\u6d88\u9664\u89c6\u89c9\u566a\u58f0\uff0cSubTTA\u5c06\u5bf9\u9f50\u540e\u7684\u89c6\u89c9\u7279\u5f81\u6295\u5f71\u5230\u4efb\u52a1\u7279\u5b9a\u7684\u6587\u672c\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u5d4c\u5165\u7ea6\u675f\u5728\u6709\u6548\u8bed\u4e49\u8303\u56f4\u5185\u6765\u8fc7\u6ee4\u6389\u4efb\u52a1\u65e0\u5173\u566a\u58f0\uff0c\u7136\u540e\u5728\u7eaf\u5316\u7a7a\u95f4\u4e0a\u6267\u884c\u6807\u51c6TTA\u4ee5\u7ec6\u5316\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548cVLM\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86SubTTA\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684TTA\u65b9\u6cd5\u5e73\u5747\u63d0\u5347\u4e862.24%\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9002\u5e94\u80fd\u529b\uff0c\u901a\u8fc7\u5b50\u7a7a\u95f4\u5bf9\u9f50\u548c\u566a\u58f0\u8fc7\u6ee4\u673a\u5236\u589e\u5f3a\u4e86\u4f2a\u6807\u7b7e\u7684\u53ef\u9760\u6027\u3002", "conclusion": "SubTTA\u901a\u8fc7\u8bed\u4e49\u5b50\u7a7a\u95f4\u5bf9\u9f50\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u9002\u5e94\u4e2d\u7684\u4e24\u4e2a\u6838\u5fc3\u9650\u5236\uff0c\u4e3a\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u53ef\u9760\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u8fd8\u63ed\u793a\u4e86\u6a21\u6001\u5bf9\u9f50\u548c\u566a\u58f0\u8fc7\u6ee4\u5728\u8de8\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2601.08641", "categories": ["cs.AI", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2601.08641", "abs": "https://arxiv.org/abs/2601.08641", "authors": ["Yichen Luo", "Yebo Feng", "Jiahua Xu", "Yang Liu"], "title": "Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning", "comment": null, "summary": "The launch of \\$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.\n  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \\$500,000 across these projects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7528\u4e8e\u6a21\u56e0\u5e01\u8ddf\u5355\u4ea4\u6613\uff0c\u901a\u8fc7\u5206\u89e3\u590d\u6742\u4efb\u52a1\u5e76\u534f\u8c03\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u5728\u8bc6\u522b\u9ad8\u8d28\u91cf\u6a21\u56e0\u5e01\u9879\u76ee\u548c\u5173\u952e\u610f\u89c1\u9886\u8896\u94b1\u5305\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u5355\u4e00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u6a21\u56e0\u5e01\u8ddf\u5355\u4ea4\u6613\u9762\u4e34\u64cd\u7eb5\u6027\u673a\u5668\u4eba\u6cdb\u6ee5\u3001\u88ab\u8ddf\u968f\u94b1\u5305\u672a\u6765\u8868\u73b0\u4e0d\u786e\u5b9a\u4ee5\u53ca\u4ea4\u6613\u6267\u884c\u5ef6\u8fdf\u7b49\u6311\u6218\uff0c\u800c\u5355\u4e00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8d44\u4ea7\u5206\u914d\u7b49\u590d\u6742\u591a\u9762\u4efb\u52a1\u65f6\u80fd\u529b\u6709\u9650\uff0c\u4e14\u5728\u52a0\u5bc6\u8d27\u5e01\u9886\u57df\u7f3a\u4e4f\u8db3\u591f\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u53d7\u8d44\u4ea7\u7ba1\u7406\u56e2\u961f\u7ed3\u6784\u542f\u53d1\uff0c\u63d0\u51fa\u53ef\u89e3\u91ca\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e76\u7531\u4e13\u4e1a\u667a\u80fd\u4f53\u534f\u4f5c\u89e3\u51b3\uff1b\u91c7\u7528\u5c11\u6837\u672c\u601d\u7ef4\u94fe\u63d0\u793a\u6280\u672f\uff0c\u4f7f\u6bcf\u4e2a\u667a\u80fd\u4f53\u83b7\u53d6\u4e13\u4e1a\u6a21\u56e0\u5e01\u4ea4\u6613\u77e5\u8bc6\u3001\u89e3\u91ca\u591a\u6a21\u6001\u6570\u636e\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u3002", "result": "\u5728\u5305\u542b1000\u4e2a\u6a21\u56e0\u5e01\u9879\u76ee\u4ea4\u6613\u6570\u636e\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8bc6\u522b\u9ad8\u8d28\u91cf\u6a21\u56e0\u5e01\u9879\u76ee\u548c\u5173\u952e\u610f\u89c1\u9886\u8896\u94b1\u5305\u65b9\u9762\u5206\u522b\u8fbe\u523073%\u548c70%\u7684\u7cbe\u786e\u7387\uff0c\u6240\u9009\u5173\u952e\u610f\u89c1\u9886\u8896\u5728\u8fd9\u4e9b\u9879\u76ee\u4e2d\u7d2f\u8ba1\u4ea7\u751f50\u4e07\u7f8e\u5143\u7684\u603b\u5229\u6da6\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u6709\u6548\u89e3\u51b3\u5355\u4e00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u91d1\u878d\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u4e13\u4e1a\u667a\u80fd\u4f53\u534f\u4f5c\u663e\u8457\u63d0\u5347\u6a21\u56e0\u5e01\u8ddf\u5355\u4ea4\u6613\u6027\u80fd\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u91d1\u878d\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.08151", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.08151", "abs": "https://arxiv.org/abs/2601.08151", "authors": ["Shezheng Song", "Shasha Li", "Jie Yu"], "title": "Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage \"review\" phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u5c42\u95f4\u63a9\u7801\u5206\u6790\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9-\u6587\u672c\u878d\u5408\u7684\u6f14\u5316\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5bf9\u9f50\u6ce8\u610f\u529b\u6846\u67b6\u6765\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5185\u90e8\u5982\u4f55\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u4ecd\u7136\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\uff0c\u8fd9\u79cd\u9ed1\u7bb1\u6027\u8d28\u9650\u5236\u4e86\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7cfb\u7edf\u6027\u5c42\u95f4\u63a9\u7801\u5206\u6790\u6280\u672f\uff0c\u5bf9\u591a\u79cd\u6a21\u578b\u67b6\u6784\u8fdb\u884c\u9010\u5c42\u5206\u6790\u4ee5\u8ffd\u8e2a\u89c6\u89c9-\u6587\u672c\u878d\u5408\u7684\u6f14\u5316\u8fc7\u7a0b\uff0c\u5e76\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5bf9\u9f50\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5efa\u6a21\u65e9\u671f\u878d\u5408\u5c42\u4e0e\u6700\u7ec8\u5c42\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u53d8\u6362\u6765\u7a81\u51fa\u6709\u610f\u4e49\u7684\u6ce8\u610f\u529b\u8f6c\u79fb\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u878d\u5408\u8fc7\u7a0b\u96c6\u4e2d\u5728\u7279\u5b9a\u5c42\u800c\u975e\u5747\u5300\u5206\u5e03\uff0c\u90e8\u5206\u6a21\u578b\u5728\u8f93\u51fa\u524d\u51fa\u73b0\u89c6\u89c9\u4fe1\u53f7\u91cd\u65b0\u6fc0\u6d3b\u7684\"\u56de\u987e\"\u73b0\u8c61\uff1b\u6ce8\u610f\u529b\u5206\u6790\u663e\u793a\u4e0d\u76f8\u5173\u533a\u57df\u5b58\u5728\u6301\u7eed\u7684\u9ad8\u6ce8\u610f\u529b\u566a\u58f0\uff0c\u800c\u6587\u672c\u5bf9\u9f50\u533a\u57df\u7684\u6ce8\u610f\u529b\u9010\u6e10\u589e\u5f3a\uff1b\u63d0\u51fa\u7684\u5bf9\u9f50\u6ce8\u610f\u529b\u6846\u67b6\u5728\u591a\u79cdMLLM\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86MLLM\u4e2d\u89c6\u89c9-\u6587\u672c\u878d\u5408\u7684\u5c42\u7ea7\u6f14\u5316\u673a\u5236\uff0c\u4e3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff1b\u63d0\u51fa\u7684\u8bad\u7ec3\u65e0\u5173\u6ce8\u610f\u529b\u5bf9\u9f50\u65b9\u6cd5\u5c55\u793a\u4e86\u901a\u8fc7\u7406\u89e3\u5185\u90e8\u8868\u793a\u52a8\u6001\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6a21\u578b\u4f18\u5316\u548c\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.08684", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08684", "abs": "https://arxiv.org/abs/2601.08684", "authors": ["Paolo Italiani", "David Gimeno-Gomez", "Luca Ragazzi", "Gianluca Moro", "Paolo Rosso"], "title": "MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection", "comment": "Accepted at EACL 2026 Findings", "summary": "Women are twice as likely as men to face online harassment due to their gender. Despite recent advances in multimodal content moderation, most approaches still overlook the social dynamics behind this phenomenon, where perpetrators reinforce prejudices and group identity within like-minded communities. Graph-based methods offer a promising way to capture such interactions, yet existing solutions remain limited by heuristic graph construction, shallow modality fusion, and instance-level reasoning. In this work, we present MemeWeaver, an end-to-end trainable multimodal framework for detecting sexism and misogyny through a novel inter-meme graph reasoning mechanism. We systematically evaluate multiple visual--textual fusion strategies and show that our approach consistently outperforms state-of-the-art baselines on the MAMI and EXIST benchmarks, while achieving faster training convergence. Further analyses reveal that the learned graph structure captures semantically meaningful patterns, offering valuable insights into the relational nature of online hate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MemeWeaver\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6a21\u56e0\u95f4\u56fe\u63a8\u7406\u673a\u5236\u68c0\u6d4b\u6027\u522b\u6b67\u89c6\u548c\u538c\u5973\u5185\u5bb9\uff0c\u5728MAMI\u548cEXIST\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u5b9e\u73b0\u66f4\u5feb\u7684\u8bad\u7ec3\u6536\u655b\u3002", "motivation": "\u5973\u6027\u906d\u53d7\u5728\u7ebf\u9a9a\u6270\u7684\u53ef\u80fd\u6027\u662f\u7537\u6027\u7684\u4e24\u500d\uff0c\u4f46\u73b0\u6709\u591a\u6a21\u6001\u5185\u5bb9\u5ba1\u6838\u65b9\u6cd5\u5927\u591a\u5ffd\u89c6\u4e86\u8fd9\u79cd\u73b0\u8c61\u80cc\u540e\u7684\u793e\u4f1a\u52a8\u6001\uff0c\u5373\u65bd\u5bb3\u8005\u5728\u5fd7\u540c\u9053\u5408\u7684\u793e\u533a\u4e2d\u5f3a\u5316\u504f\u89c1\u548c\u7fa4\u4f53\u8ba4\u540c\u3002\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u867d\u7136\u6709\u671b\u6355\u6349\u6b64\u7c7b\u4e92\u52a8\uff0c\u4f46\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4ecd\u53d7\u9650\u4e8e\u542f\u53d1\u5f0f\u56fe\u6784\u5efa\u3001\u6d45\u5c42\u6a21\u6001\u878d\u5408\u548c\u5b9e\u4f8b\u7ea7\u63a8\u7406\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MemeWeaver\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u91c7\u7528\u65b0\u9896\u7684\u6a21\u56e0\u95f4\u56fe\u63a8\u7406\u673a\u5236\u6765\u68c0\u6d4b\u6027\u522b\u6b67\u89c6\u548c\u538c\u5973\u5185\u5bb9\u3002\u8be5\u65b9\u6cd5\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u89c6\u89c9-\u6587\u672c\u878d\u5408\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u56fe\u7ed3\u6784\u5b66\u4e60\u6765\u6355\u6349\u6a21\u56e0\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u542f\u53d1\u5f0f\u56fe\u6784\u5efa\u65b9\u6cd5\u3002", "result": "MemeWeaver\u5728MAMI\u548cEXIST\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0c\u5b66\u4e60\u5230\u7684\u56fe\u7ed3\u6784\u80fd\u591f\u6355\u6349\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u6a21\u5f0f\uff0c\u4e3a\u5728\u7ebf\u4ec7\u6068\u7684\u5173\u7cfb\u6027\u8d28\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u56fe\u63a8\u7406\u673a\u5236\u5728\u6355\u6349\u5728\u7ebf\u6027\u522b\u6b67\u89c6\u7684\u793e\u4f1a\u52a8\u6001\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002\u5b66\u4e60\u5230\u7684\u56fe\u7ed3\u6784\u4e0d\u4ec5\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u8fd8\u63ed\u793a\u4e86\u6a21\u56e0\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u5728\u7ebf\u4ec7\u6068\u7684\u4f20\u64ad\u6a21\u5f0f\u3002"}}
{"id": "2601.08165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08165", "abs": "https://arxiv.org/abs/2601.08165", "authors": ["Phuoc-Nguyen Bui", "Toan Duc Nguyen", "Junghyun Bum", "Duc-Tai Le", "Hyunseung Choo"], "title": "Representation Learning with Semantic-aware Instance and Sparse Token Alignments", "comment": "Under review, 8 pages", "summary": "Medical contrastive vision-language pre-training (VLP) has demonstrated significant potential in improving performance on downstream tasks. Traditional approaches typically employ contrastive learning, treating paired image-report samples as positives and unpaired ones as negatives. However, in medical datasets, there can be substantial similarities between images or reports from different patients. Rigidly treating all unpaired samples as negatives, can disrupt the underlying semantic structure and negatively impact the quality of the learned representations. In this paper, we propose a multi-level alignment framework, Representation Learning with Semantic-aware Instance and Sparse Token Alignments (SISTA) by exploiting the semantic correspondence between medical image and radiology reports at two levels, i.e., image-report and patch-word levels. Specifically, we improve the conventional contrastive learning by incorporating inter-report similarity to eliminate the false negatives and introduce a method to effectively align image patches with relevant word tokens. Experimental results demonstrate the effectiveness of the proposed framework in improving transfer performance across different datasets on three downstream tasks: image classification, image segmentation, and object detection. Notably, our framework achieves significant improvements in fine-grained tasks even with limited labeled data. Codes and pre-trained models will be made available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u5bf9\u9f50\u6846\u67b6SISTA\uff0c\u901a\u8fc7\u5229\u7528\u533b\u5b66\u56fe\u50cf\u4e0e\u653e\u5c04\u5b66\u62a5\u544a\u5728\u56fe\u50cf-\u62a5\u544a\u548c\u8865\u4e01-\u8bcd\u7ea7\u522b\u4e0a\u7684\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\uff0c\u6539\u8fdb\u4e86\u533b\u5b66\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5c06\u6240\u6709\u672a\u914d\u5bf9\u6837\u672c\u89c6\u4e3a\u8d1f\u4f8b\u5bfc\u81f4\u7684\u8bed\u4e49\u7ed3\u6784\u7834\u574f\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u533b\u5b66\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u5c06\u914d\u5bf9\u56fe\u50cf-\u62a5\u544a\u6837\u672c\u89c6\u4e3a\u6b63\u4f8b\uff0c\u672a\u914d\u5bf9\u6837\u672c\u89c6\u4e3a\u8d1f\u4f8b\uff0c\u4f46\u5728\u533b\u5b66\u6570\u636e\u96c6\u4e2d\uff0c\u4e0d\u540c\u60a3\u8005\u7684\u56fe\u50cf\u6216\u62a5\u544a\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u663e\u8457\u76f8\u4f3c\u6027\uff0c\u5c06\u6240\u6709\u672a\u914d\u5bf9\u6837\u672c\u89c6\u4e3a\u8d1f\u4f8b\u4f1a\u7834\u574f\u5e95\u5c42\u8bed\u4e49\u7ed3\u6784\u5e76\u5f71\u54cd\u5b66\u4e60\u8868\u793a\u7684\u8d28\u91cf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u7ea7\u5bf9\u9f50\u6846\u67b6SISTA\uff0c\u901a\u8fc7\u5229\u7528\u533b\u5b66\u56fe\u50cf\u4e0e\u653e\u5c04\u5b66\u62a5\u544a\u5728\u56fe\u50cf-\u62a5\u544a\u548c\u8865\u4e01-\u8bcd\u4e24\u4e2a\u7ea7\u522b\u7684\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\uff0c\u6539\u8fdb\u4e86\u4f20\u7edf\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u4f53\u5305\u62ec\u5f15\u5165\u62a5\u544a\u95f4\u76f8\u4f3c\u6027\u4ee5\u6d88\u9664\u5047\u8d1f\u4f8b\uff0c\u5e76\u5f00\u53d1\u4e86\u6709\u6548\u5bf9\u9f50\u56fe\u50cf\u8865\u4e01\u4e0e\u76f8\u5173\u8bcd\u6807\u8bb0\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff08\u56fe\u50cf\u5206\u7c7b\u3001\u56fe\u50cf\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\uff09\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u8fc1\u79fb\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u7684\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5c06\u516c\u5f00\u63d0\u4f9b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u8003\u8651\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u591a\u7ea7\u5bf9\u9f50\u6846\u67b6\u4e3a\u5904\u7406\u533b\u5b66\u6570\u636e\u4e2d\u7684\u590d\u6742\u8bed\u4e49\u5173\u7cfb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u533b\u5b66AI\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2601.08179", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.08179", "abs": "https://arxiv.org/abs/2601.08179", "authors": ["Anh H. Vo", "Tae-Seok Kim", "Hulin Jin", "Soo-Mi Choi", "Yong-Guk Kim"], "title": "Instruction-Driven 3D Facial Expression Generation and Transition", "comment": null, "summary": "A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6307\u4ee4\u9a71\u52a8\u7684\u4e09\u7ef4\u9762\u90e8\u8868\u60c5\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u6587\u672c\u6307\u4ee4\u5728\u4efb\u610f\u4e24\u79cd\u6307\u5b9a\u8868\u60c5\u4e4b\u95f4\u751f\u6210\u5e73\u6ed1\u7684\u9762\u90e8\u8868\u60c5\u8fc7\u6e21\u5e8f\u5217\uff0c\u663e\u8457\u6269\u5c55\u4e86\u4e09\u7ef4\u865a\u62df\u5316\u8eab\u7684\u8868\u60c5\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u4e09\u7ef4\u865a\u62df\u5316\u8eab\u901a\u5e38\u4ec5\u652f\u6301\u516d\u79cd\u57fa\u672c\u9762\u90e8\u8868\u60c5\uff0c\u7f3a\u4e4f\u6a21\u62df\u771f\u5b9e\u60c5\u611f\u53d8\u5316\u7684\u7075\u6d3b\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u6839\u636e\u6587\u672c\u6307\u4ee4\u5728\u4efb\u610f\u4e24\u79cd\u9762\u90e8\u8868\u60c5\u4e4b\u95f4\u751f\u6210\u5e73\u6ed1\u8fc7\u6e21\u5e8f\u5217\u7684\u95ee\u9898\uff0c\u4ee5\u6269\u5c55\u865a\u62df\u5316\u8eab\u7684\u60c5\u611f\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u6307\u4ee4\u9a71\u52a8\u7684\u9762\u90e8\u8868\u60c5\u5206\u89e3\u5668\u6a21\u5757\u6765\u5b66\u4e60\u591a\u6a21\u6001\u6570\u636e\u5e76\u6355\u6349\u6587\u672c\u63cf\u8ff0\u4e0e\u9762\u90e8\u8868\u60c5\u7279\u5f81\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u968f\u540e\u5f00\u53d1\u4e86\u6307\u4ee4\u5230\u9762\u90e8\u8868\u60c5\u8fc7\u6e21\u65b9\u6cd5\uff0c\u5229\u7528\u8be5\u5206\u89e3\u5668\u548c\u9876\u70b9\u91cd\u5efa\u635f\u5931\u51fd\u6570\u6765\u4f18\u5316\u6f5c\u5728\u5411\u91cf\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u4ece\u800c\u6839\u636e\u7ed9\u5b9a\u6307\u4ee4\u751f\u6210\u9762\u90e8\u8868\u60c5\u5e8f\u5217\u3002\u6700\u540e\u6784\u5efa\u4e86\u9762\u90e8\u8868\u60c5\u8fc7\u6e21\u6a21\u578b\u6765\u751f\u6210\u8868\u60c5\u4e4b\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "result": "\u5728CK+\u548cCelebV-HQ\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u9762\u90e8\u8868\u60c5\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u80fd\u591f\u6839\u636e\u6587\u672c\u6307\u4ee4\u751f\u6210\u51c6\u786e\u7684\u9762\u90e8\u8868\u60c5\u8f68\u8ff9\uff0c\u5e76\u4e14\u901a\u8fc7\u6587\u672c\u63d0\u793a\u53ef\u4ee5\u6781\u5927\u5730\u6269\u5c55\u9762\u90e8\u8868\u60c5\u53ca\u5176\u8fc7\u6e21\u7684\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e09\u7ef4\u865a\u62df\u5316\u8eab\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u8868\u60c5\u63a7\u5236\u673a\u5236\uff0c\u901a\u8fc7\u6587\u672c\u6307\u4ee4\u9a71\u52a8\u7684\u65b9\u5f0f\u663e\u8457\u589e\u5f3a\u4e86\u60c5\u611f\u8868\u8fbe\u7684\u591a\u6837\u6027\u548c\u81ea\u7136\u6027\u3002\u8be5\u6846\u67b6\u5728\u865a\u62df\u73b0\u5b9e\u3001\u6e38\u620f\u89d2\u8272\u52a8\u753b\u548c\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\uff0c\u4e3a\u57fa\u4e8e\u591a\u6a21\u6001\u8f93\u5165\u7684\u9762\u90e8\u52a8\u753b\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2601.08183", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08183", "abs": "https://arxiv.org/abs/2601.08183", "authors": ["Yan Zhu", "Te Luo", "Pei-Yao Fu", "Zhen Zhang", "Zi-Long Wang", "Yi-Fan Qu", "Zi-Han Geng", "Jia-Qi Xu", "Lu Yao", "Li-Yun Ma", "Wei Su", "Wei-Feng Chen", "Quan-Lin Li", "Shuo Wang", "Ping-Hong Zhou"], "title": "GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards", "comment": "45 pages, 17 figures, 6 tables. Leaderboard available at: https://roterdl.github.io/GIBench/ . Includes supplementary material", "summary": "Multimodal Large Language Models (MLLMs) show promise in gastroenterology, yet their performance against comprehensive clinical workflows and human benchmarks remains unverified. To systematically evaluate state-of-the-art MLLMs across a panoramic gastrointestinal endoscopy workflow and determine their clinical utility compared with human endoscopists. We constructed GI-Bench, a benchmark encompassing 20 fine-grained lesion categories. Twelve MLLMs were evaluated across a five-stage clinical workflow: anatomical localization, lesion identification, diagnosis, findings description, and management. Model performance was benchmarked against three junior endoscopists and three residency trainees using Macro-F1, mean Intersection-over-Union (mIoU), and multi-dimensional Likert scale. Gemini-3-Pro achieved state-of-the-art performance. In diagnostic reasoning, top-tier models (Macro-F1 0.641) outperformed trainees (0.492) and rivaled junior endoscopists (0.727; p>0.05). However, a critical \"spatial grounding bottleneck\" persisted; human lesion localization (mIoU >0.506) significantly outperformed the best model (0.345; p<0.05). Furthermore, qualitative analysis revealed a \"fluency-accuracy paradox\": models generated reports with superior linguistic readability compared with humans (p<0.05) but exhibited significantly lower factual correctness (p<0.05) due to \"over-interpretation\" and hallucination of visual features.GI-Bench maintains a dynamic leaderboard that tracks the evolving performance of MLLMs in clinical endoscopy. The current rankings and benchmark results are available at https://roterdl.github.io/GIBench/.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u80c3\u80a0\u5185\u7aa5\u955c\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u8bca\u65ad\u63a8\u7406\u65b9\u9762\u53ef\u5ab2\u7f8e\u521d\u7ea7\u5185\u955c\u533b\u5e08\uff0c\u4f46\u5728\u7a7a\u95f4\u5b9a\u4f4d\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u74f6\u9888\uff0c\u5e76\u63d0\u51fa\u4e86GI-Bench\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u80c3\u80a0\u75c5\u5b66\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u5b8c\u6574\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u57fa\u51c6\u7684\u5bf9\u6bd4\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u9a8c\u8bc1\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u80c3\u80a0\u5185\u7aa5\u955c\u5168\u666f\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u4e34\u5e8a\u6548\u7528\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b20\u4e2a\u7ec6\u7c92\u5ea6\u75c5\u53d8\u7c7b\u522b\u7684GI-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e8612\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e94\u9636\u6bb5\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u89e3\u5256\u5b9a\u4f4d\u3001\u75c5\u53d8\u8bc6\u522b\u3001\u8bca\u65ad\u3001\u53d1\u73b0\u63cf\u8ff0\u548c\u7ba1\u7406\uff0c\u5e76\u4f7f\u7528Macro-F1\u3001\u5e73\u5747\u4ea4\u5e76\u6bd4\u548c\u591a\u7ef4\u5ea6\u674e\u514b\u7279\u91cf\u8868\u5c06\u6a21\u578b\u6027\u80fd\u4e0e\u4e09\u540d\u521d\u7ea7\u5185\u955c\u533b\u5e08\u548c\u4e09\u540d\u4f4f\u9662\u533b\u5e08\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "Gemini-3-Pro\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u8bca\u65ad\u63a8\u7406\u65b9\u9762\uff0c\u9876\u7ea7\u6a21\u578b\u7684Macro-F1\u5206\u6570\u4e3a0.641\uff0c\u4f18\u4e8e\u4f4f\u9662\u533b\u5e08\u76840.492\uff0c\u5e76\u4e0e\u521d\u7ea7\u5185\u955c\u533b\u5e08\u76840.727\u76f8\u5f53\uff0c\u4f46\u5b58\u5728\u5173\u952e\u7684\"\u7a7a\u95f4\u5b9a\u4f4d\u74f6\u9888\"\uff0c\u4eba\u7c7b\u75c5\u53d8\u5b9a\u4f4d\u7684mIoU\u8d85\u8fc70.506\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u4f73\u6a21\u578b\u76840.345\uff0c\u540c\u65f6\u53d1\u73b0\"\u6d41\u7545\u6027-\u51c6\u786e\u6027\u6096\u8bba\"\uff0c\u6a21\u578b\u751f\u6210\u62a5\u544a\u7684\u8bed\u8a00\u53ef\u8bfb\u6027\u4f18\u4e8e\u4eba\u7c7b\u4f46\u4e8b\u5b9e\u51c6\u786e\u6027\u663e\u8457\u8f83\u4f4e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bca\u65ad\u63a8\u7406\u65b9\u9762\u5df2\u8fbe\u5230\u4e34\u5e8a\u53ef\u7528\u6c34\u5e73\uff0c\u4f46\u5728\u7a7a\u95f4\u5b9a\u4f4d\u548c\u89c6\u89c9\u7279\u5f81\u89e3\u91ca\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u5b58\u5728\u7684\"\u8fc7\u5ea6\u89e3\u91ca\"\u548c\u5e7b\u89c9\u95ee\u9898\uff0cGI-Bench\u52a8\u6001\u6392\u884c\u699c\u4e3a\u8ddf\u8e2a\u6a21\u578b\u5728\u4e34\u5e8a\u5185\u7aa5\u955c\u4e2d\u7684\u6f14\u8fdb\u63d0\u4f9b\u4e86\u6301\u7eed\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2601.08192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08192", "abs": "https://arxiv.org/abs/2601.08192", "authors": ["Md. Faiyaz Abdullah Sayeedi", "Rashedur Rahman", "Siam Tahsin Bhuiyan", "Sefatul Wasi", "Ashraful Islam", "Saadia Binte Alam", "AKM Mahbubur Rahman"], "title": "Route, Retrieve, Reflect, Repair: Self-Improving Agentic Framework for Visual Detection and Linguistic Reasoning in Medical Imaging", "comment": null, "summary": "Medical image analysis increasingly relies on large vision-language models (VLMs), yet most systems remain single-pass black boxes that offer limited control over reasoning, safety, and spatial grounding. We propose R^4, an agentic framework that decomposes medical imaging workflows into four coordinated agents: a Router that configures task- and specialization-aware prompts from the image, patient history, and metadata; a Retriever that uses exemplar memory and pass@k sampling to jointly generate free-text reports and bounding boxes; a Reflector that critiques each draft-box pair for key clinical error modes (negation, laterality, unsupported claims, contradictions, missing findings, and localization errors); and a Repairer that iteratively revises both narrative and spatial outputs under targeted constraints while curating high-quality exemplars for future cases. Instantiated on chest X-ray analysis with multiple modern VLM backbones and evaluated on report generation and weakly supervised detection, R^4 consistently boosts LLM-as-a-Judge scores by roughly +1.7-+2.5 points and mAP50 by +2.5-+3.5 absolute points over strong single-VLM baselines, without any gradient-based fine-tuning. These results show that agentic routing, reflection, and repair can turn strong but brittle VLMs into more reliable and better grounded tools for clinical image interpretation. Our code can be found at: https://github.com/faiyazabdullah/MultimodalMedAgent", "AI": {"tldr": "\u672c\u6587\u63d0\u51faR\u2074\u6846\u67b6\uff0c\u4e00\u79cd\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u8def\u7531\u3001\u68c0\u7d22\u3001\u53cd\u601d\u548c\u4fee\u590d\u56db\u4e2a\u534f\u8c03\u667a\u80fd\u4f53\uff0c\u5c06\u5f3a\u4f46\u8106\u5f31\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8f6c\u53d8\u4e3a\u66f4\u53ef\u9760\u3001\u7a7a\u95f4\u57fa\u7840\u66f4\u597d\u7684\u4e34\u5e8a\u56fe\u50cf\u89e3\u91ca\u5de5\u5177\uff0c\u65e0\u9700\u57fa\u4e8e\u68af\u5ea6\u7684\u5fae\u8c03\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e3b\u8981\u4f9d\u8d56\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5927\u591a\u6570\u7cfb\u7edf\u4ecd\u662f\u5355\u6b21\u901a\u8fc7\u7684\u9ed1\u76d2\uff0c\u5728\u63a8\u7406\u63a7\u5236\u3001\u5b89\u5168\u6027\u548c\u7a7a\u95f4\u57fa\u7840\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7f3a\u4e4f\u5bf9\u4e34\u5e8a\u9519\u8bef\u6a21\u5f0f\u7684\u6709\u6548\u5904\u7406\u673a\u5236\u3002", "method": "R\u2074\u6846\u67b6\u5c06\u533b\u5b66\u5f71\u50cf\u5de5\u4f5c\u6d41\u5206\u89e3\u4e3a\u56db\u4e2a\u534f\u8c03\u667a\u80fd\u4f53\uff1a\u8def\u7531\u5668\u6839\u636e\u56fe\u50cf\u3001\u60a3\u8005\u75c5\u53f2\u548c\u5143\u6570\u636e\u914d\u7f6e\u4efb\u52a1\u548c\u4e13\u4e1a\u5316\u63d0\u793a\uff1b\u68c0\u7d22\u5668\u4f7f\u7528\u793a\u4f8b\u8bb0\u5fc6\u548cpass@k\u91c7\u6837\u8054\u5408\u751f\u6210\u81ea\u7531\u6587\u672c\u62a5\u544a\u548c\u8fb9\u754c\u6846\uff1b\u53cd\u601d\u5668\u9488\u5bf9\u6bcf\u4e2a\u8349\u7a3f-\u6846\u5bf9\u6279\u5224\u516d\u79cd\u5173\u952e\u4e34\u5e8a\u9519\u8bef\u6a21\u5f0f\uff1b\u4fee\u590d\u5668\u5728\u9488\u5bf9\u6027\u7ea6\u675f\u4e0b\u8fed\u4ee3\u4fee\u8ba2\u53d9\u4e8b\u548c\u7a7a\u95f4\u8f93\u51fa\uff0c\u540c\u65f6\u4e3a\u672a\u6765\u75c5\u4f8b\u7b56\u5212\u9ad8\u8d28\u91cf\u793a\u4f8b\u3002", "result": "\u5728\u80f8\u90e8X\u5c04\u7ebf\u5206\u6790\u4e2d\uff0cR\u2074\u4f7f\u7528\u591a\u4e2a\u73b0\u4ee3VLM\u9aa8\u5e72\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u62a5\u544a\u751f\u6210\u548c\u5f31\u76d1\u7763\u68c0\u6d4b\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u5f3a\u5927\u7684\u5355VLM\u57fa\u7ebf\uff0c\u6301\u7eed\u63d0\u5347LLM-as-a-Judge\u8bc4\u5206\u7ea61.7-2.5\u5206\uff0cmAP50\u63d0\u53472.5-3.5\u4e2a\u7edd\u5bf9\u767e\u5206\u70b9\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u57fa\u4e8e\u68af\u5ea6\u7684\u5fae\u8c03\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u667a\u80fd\u4f53\u8def\u7531\u3001\u53cd\u601d\u548c\u4fee\u590d\u673a\u5236\u80fd\u591f\u5c06\u5f3a\u5927\u4f46\u8106\u5f31\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8f6c\u53d8\u4e3a\u66f4\u53ef\u9760\u3001\u57fa\u7840\u66f4\u597d\u7684\u4e34\u5e8a\u56fe\u50cf\u89e3\u91ca\u5de5\u5177\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u63a7\u5236\u3001\u53ef\u89e3\u91ca\u4e14\u6027\u80fd\u66f4\u4f18\u7684\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u533b\u7597AI\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.08193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08193", "abs": "https://arxiv.org/abs/2601.08193", "authors": ["Mengqi Wu", "Yongheng Sun", "Qianqian Wang", "Pew-Thian Yap", "Mingxia Liu"], "title": "Unified Multi-Site Multi-Sequence Brain MRI Harmonization Enriched by Biomedical Semantic Style", "comment": "15 pages, 10 figures. Extended version of a paper published at MICCAI 2025 (DOI: 10.1007/978-3-032-04947-6_65)", "summary": "Aggregating multi-site brain MRI data can enhance deep learning model training, but also introduces non-biological heterogeneity caused by site-specific variations (e.g., differences in scanner vendors, acquisition parameters, and imaging protocols) that can undermine generalizability. Recent retrospective MRI harmonization seeks to reduce such site effects by standardizing image style (e.g., intensity, contrast, noise patterns) while preserving anatomical content. However, existing methods often rely on limited paired traveling-subject data or fail to effectively disentangle style from anatomy. Furthermore, most current approaches address only single-sequence harmonization, restricting their use in real-world settings where multi-sequence MRI is routinely acquired. To this end, we introduce MMH, a unified framework for multi-site multi-sequence brain MRI harmonization that leverages biomedical semantic priors for sequence-aware style alignment. MMH operates in two stages: (1) a diffusion-based global harmonizer that maps MR images to a sequence-specific unified domain using style-agnostic gradient conditioning, and (2) a target-specific fine-tuner that adapts globally aligned images to desired target domains. A tri-planar attention BiomedCLIP encoder aggregates multi-view embeddings to characterize volumetric style information, allowing explicit disentanglement of image styles from anatomy without requiring paired data. Evaluations on 4,163 T1- and T2-weighted MRIs demonstrate MMH's superior harmonization over state-of-the-art methods in image feature clustering, voxel-level comparison, tissue segmentation, and downstream age and site classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MMH\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u7ad9\u70b9\u591a\u5e8f\u5217\u8111MRI\u534f\u8c03\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u751f\u7269\u533b\u5b66\u8bed\u4e49\u5148\u9a8c\u8fdb\u884c\u5e8f\u5217\u611f\u77e5\u7684\u98ce\u683c\u5bf9\u9f50\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u65e0\u9700\u914d\u5bf9\u6570\u636e\u7684\u89e3\u5256\u7ed3\u6784\u4fdd\u7559\u534f\u8c03\u3002", "motivation": "\u591a\u7ad9\u70b9\u8111MRI\u6570\u636e\u805a\u5408\u53ef\u589e\u5f3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\uff0c\u4f46\u4f1a\u5f15\u5165\u7531\u7ad9\u70b9\u7279\u5b9a\u53d8\u5f02\uff08\u5982\u626b\u63cf\u4eea\u5382\u5546\u3001\u91c7\u96c6\u53c2\u6570\u548c\u6210\u50cf\u534f\u8bae\u5dee\u5f02\uff09\u5bfc\u81f4\u7684\u975e\u751f\u7269\u5f02\u8d28\u6027\uff0c\u4ece\u800c\u635f\u5bb3\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002\u73b0\u6709\u56de\u987e\u6027MRI\u534f\u8c03\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u6709\u9650\u7684\u914d\u5bf9\u65c5\u884c\u8005\u6570\u636e\uff0c\u6216\u672a\u80fd\u6709\u6548\u89e3\u8026\u98ce\u683c\u4e0e\u89e3\u5256\u7ed3\u6784\uff0c\u4e14\u5927\u591a\u4ec5\u5904\u7406\u5355\u5e8f\u5217\u534f\u8c03\uff0c\u9650\u5236\u4e86\u5728\u5e38\u89c4\u83b7\u53d6\u591a\u5e8f\u5217MRI\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "MMH\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e3a\u57fa\u4e8e\u6269\u6563\u7684\u5168\u5c40\u534f\u8c03\u5668\uff0c\u901a\u8fc7\u98ce\u683c\u65e0\u5173\u7684\u68af\u5ea6\u6761\u4ef6\u5c06MR\u56fe\u50cf\u6620\u5c04\u5230\u5e8f\u5217\u7279\u5b9a\u7684\u7edf\u4e00\u57df\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4e3a\u76ee\u6807\u7279\u5b9a\u5fae\u8c03\u5668\uff0c\u5c06\u5168\u5c40\u5bf9\u9f50\u56fe\u50cf\u9002\u914d\u5230\u671f\u671b\u7684\u76ee\u6807\u57df\u3002\u91c7\u7528\u4e09\u5e73\u9762\u6ce8\u610f\u529bBiomedCLIP\u7f16\u7801\u5668\u805a\u5408\u591a\u89c6\u56fe\u5d4c\u5165\u4ee5\u8868\u5f81\u4f53\u79ef\u98ce\u683c\u4fe1\u606f\uff0c\u5b9e\u73b0\u65e0\u9700\u914d\u5bf9\u6570\u636e\u7684\u56fe\u50cf\u98ce\u683c\u4e0e\u89e3\u5256\u7ed3\u6784\u7684\u663e\u5f0f\u89e3\u8026\u3002", "result": "\u57284,163\u4e2aT1\u548cT2\u52a0\u6743MRI\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cMMH\u5728\u56fe\u50cf\u7279\u5f81\u805a\u7c7b\u3001\u4f53\u7d20\u7ea7\u6bd4\u8f83\u3001\u7ec4\u7ec7\u5206\u5272\u4ee5\u53ca\u4e0b\u6e38\u5e74\u9f84\u548c\u7ad9\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u7ad9\u70b9\u591a\u5e8f\u5217\u8111MRI\u534f\u8c03\u65b9\u9762\u7684\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5229\u7528\u751f\u7269\u533b\u5b66\u8bed\u4e49\u5148\u9a8c\u8fdb\u884c\u5e8f\u5217\u611f\u77e5\u98ce\u683c\u5bf9\u9f50\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u7ad9\u70b9\u591a\u5e8f\u5217\u8111MRI\u534f\u8c03\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002MMH\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u89e3\u8026\u98ce\u683c\u4e0e\u89e3\u5256\u7ed3\u6784\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u534f\u8c03\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u57df\u9002\u5e94\u548c\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.08226", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08226", "abs": "https://arxiv.org/abs/2601.08226", "authors": ["Alexander Shim", "Khalil Saieh", "Samuel Clarke"], "title": "Knowledge-based learning in Text-RAG and Image-RAG", "comment": "9 pages, 10 figures", "summary": "This research analyzed and compared the multi-modal approach in the Vision Transformer(EVA-ViT) based image encoder with the LlaMA or ChatGPT LLM to reduce the hallucination problem and detect diseases in chest x-ray images. In this research, we utilized the NIH Chest X-ray image to train the model and compared it in image-based RAG, text-based RAG, and baseline. [3] [5] In a result, the text-based RAG[2] e!ectively reduces the hallucination problem by using external knowledge information, and the image-based RAG improved the prediction con\"dence and calibration by using the KNN methods. [4] Moreover, the GPT LLM showed better performance, a low hallucination rate, and better Expected Calibration Error(ECE) than Llama Llama-based model. This research shows the challenge of data imbalance, a complex multi-stage structure, but suggests a large experience environment and a balanced example of use.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9\u6bd4\u57fa\u4e8eEVA-ViT\u56fe\u50cf\u7f16\u7801\u5668\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u4e0eLLaMA\u6216ChatGPT LLM\uff0c\u65e8\u5728\u51cf\u5c11\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u5e76\u63d0\u5347\u80f8\u90e8X\u5149\u75be\u75c5\u68c0\u6d4b\u6027\u80fd\uff0c\u53d1\u73b0\u57fa\u4e8e\u6587\u672c\u7684RAG\u80fd\u6709\u6548\u964d\u4f4e\u5e7b\u89c9\u7387\uff0c\u800c\u57fa\u4e8e\u56fe\u50cf\u7684RAG\u901a\u8fc7KNN\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u6821\u51c6\u6548\u679c\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u591a\u6a21\u6001\u65b9\u6cd5\u5b58\u5728\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u80f8\u90e8X\u5149\u75be\u75c5\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u63a2\u7d22\u5982\u4f55\u6709\u6548\u7ed3\u5408\u89c6\u89c9Transformer\u56fe\u50cf\u7f16\u7801\u5668\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u540c\u65f6\u5e94\u5bf9\u6570\u636e\u4e0d\u5e73\u8861\u548c\u590d\u6742\u591a\u9636\u6bb5\u7ed3\u6784\u7684\u6311\u6218\u3002", "method": "\u7814\u7a76\u91c7\u7528\u57fa\u4e8eEVA-ViT\u7684\u56fe\u50cf\u7f16\u7801\u5668\u4e0eLLaMA\u6216ChatGPT LLM\u76f8\u7ed3\u5408\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u4f7f\u7528NIH\u80f8\u90e8X\u5149\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4e09\u79cd\u4e0d\u540c\u914d\u7f6e\uff1a\u57fa\u4e8e\u56fe\u50cf\u7684RAG\uff08\u91c7\u7528KNN\u65b9\u6cd5\uff09\u3001\u57fa\u4e8e\u6587\u672c\u7684RAG\uff08\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u4fe1\u606f\uff09\u4ee5\u53ca\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u7b56\u7565\u5bf9\u5e7b\u89c9\u95ee\u9898\u548c\u75be\u75c5\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6587\u672c\u7684RAG\u80fd\u6709\u6548\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u4fe1\u606f\u663e\u8457\u964d\u4f4e\u5e7b\u89c9\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u56fe\u50cf\u7684RAG\u901a\u8fc7KNN\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u6821\u51c6\u6548\u679c\uff1bGPT LLM\u5728\u6027\u80fd\u8868\u73b0\u3001\u5e7b\u89c9\u7387\u548c\u671f\u671b\u6821\u51c6\u8bef\u5dee\u65b9\u9762\u5747\u4f18\u4e8eLLaMA\u6a21\u578b\uff0c\u663e\u793a\u51fa\u66f4\u597d\u7684\u6574\u4f53\u8868\u73b0\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u548c\u7ed3\u6784\u590d\u6742\u6027\u7684\u6311\u6218\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u7684RAG\u65b9\u6cd5\u548cGPT LLM\u5728\u51cf\u5c11\u5e7b\u89c9\u3001\u63d0\u5347\u6821\u51c6\u6548\u679c\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u533b\u5b66\u8bca\u65ad\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5e76\u5efa\u8bae\u9700\u8981\u5927\u89c4\u6a21\u5b9e\u9a8c\u73af\u5883\u548c\u5e73\u8861\u7684\u7528\u4f8b\u793a\u4f8b\u6765\u8fdb\u4e00\u6b65\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2601.08241", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.08241", "abs": "https://arxiv.org/abs/2601.08241", "authors": ["Michele Fiori", "Gabriele Civitarese", "Marco Colussi", "Claudio Bettini"], "title": "Improving Zero-shot ADL Recognition with Large Language Models through Event-based Context and Confidence", "comment": null, "summary": "Unobtrusive sensor-based recognition of Activities of Daily Living (ADLs) in smart homes by processing data collected from IoT sensing devices supports applications such as healthcare, safety, and energy management. Recent zero-shot methods based on Large Language Models (LLMs) have the advantage of removing the reliance on labeled ADL sensor data. However, existing approaches rely on time-based segmentation, which is poorly aligned with the contextual reasoning capabilities of LLMs. Moreover, existing approaches lack methods for estimating prediction confidence. This paper proposes to improve zero-shot ADL recognition with event-based segmentation and a novel method for estimating prediction confidence. Our experimental evaluation shows that event-based segmentation consistently outperforms time-based LLM approaches on complex, realistic datasets and surpasses supervised data-driven methods, even with relatively small LLMs (e.g., Gemma 3 27B). The proposed confidence measure effectively distinguishes correct from incorrect predictions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u5206\u5272\u548c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u96f6\u6837\u672cADL\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8b\u4ef6\u5206\u5272\u53d6\u4ee3\u4f20\u7edf\u65f6\u95f4\u5206\u5272\uff0c\u5e76\u5f15\u5165\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u667a\u80fd\u5bb6\u5c45\u6d3b\u52a8\u8bc6\u522b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672cADL\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u65f6\u95f4\u5206\u5272\u7b56\u7565\uff0c\u8fd9\u4e0eLLMs\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u4e14\u7f3a\u4e4f\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u673a\u5236\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6548\u679c\u548c\u53ef\u9760\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u4e8b\u4ef6\u5206\u5272\u7b56\u7565\u66ff\u4ee3\u4f20\u7edf\u65f6\u95f4\u5206\u5272\uff0c\u4f7f\u5206\u5272\u8fb9\u754c\u4e0e\u6d3b\u52a8\u4e8b\u4ef6\u7684\u81ea\u7136\u8fb9\u754c\u5bf9\u9f50\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u9884\u6d4b\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e8b\u4ef6\u5206\u5272\u65b9\u6cd5\u5728\u590d\u6742\u73b0\u5b9e\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u65f6\u95f4\u7684LLM\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u76d1\u7763\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5373\u4f7f\u4f7f\u7528\u76f8\u5bf9\u8f83\u5c0f\u7684LLM\u6a21\u578b\uff08\u5982Gemma 3 27B\uff09\u4e5f\u80fd\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u4e14\u63d0\u51fa\u7684\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u80fd\u6709\u6548\u533a\u5206\u9884\u6d4b\u6b63\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u4e8b\u4ef6\u5206\u5272\u7b56\u7565\u80fd\u66f4\u597d\u5730\u5229\u7528LLMs\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u96f6\u6837\u672cADL\u8bc6\u522b\u6027\u80fd\uff0c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u673a\u5236\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u667a\u80fd\u5bb6\u5c45\u4e2d\u7684\u6d3b\u52a8\u8bc6\u522b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.08292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08292", "abs": "https://arxiv.org/abs/2601.08292", "authors": ["Xianfeng Wang", "Kaiwei Zhang", "Qi Jia", "Zijian Chen", "Guangtao Zhai", "Xiongkuo Min"], "title": "KidVis: Do Multimodal Large Language Models Possess the Visual Perceptual Capabilities of a 6-Year-Old?", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) have demonstrated impressive proficiency in high-level reasoning tasks, such as complex diagrammatic interpretation, it remains an open question whether they possess the fundamental visual primitives comparable to human intuition. To investigate this, we introduce KidVis, a novel benchmark grounded in the theory of human visual development. KidVis deconstructs visual intelligence into six atomic capabilities - Concentration, Tracking, Discrimination, Memory, Spatial, and Closure - already possessed by 6-7 year old children, comprising 10 categories of low-semantic-dependent visual tasks. Evaluating 20 state-of-the-art MLLMs against a human physiological baseline reveals a stark performance disparity. Results indicate that while human children achieve a near-perfect average score of 95.32, the state-of-the-art GPT-5 attains only 67.33. Crucially, we observe a \"Scaling Law Paradox\": simply increasing model parameters fails to yield linear improvements in these foundational visual capabilities. This study confirms that current MLLMs, despite their reasoning prowess, lack the essential physiological perceptual primitives required for generalized visual intelligence.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165KidVis\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u4eba\u7c7b\u89c6\u89c9\u53d1\u5c55\u7406\u8bba\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\u89c6\u89c9\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u7684MLLMs\u5728\u513f\u7ae5\u5df2\u638c\u63e1\u7684\u539f\u5b50\u89c6\u89c9\u80fd\u529b\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u4e14\u53c2\u6570\u7f29\u653e\u65e0\u6cd5\u7ebf\u6027\u63d0\u5347\u8fd9\u4e9b\u57fa\u7840\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u7ea7\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5b83\u4eec\u662f\u5426\u5177\u5907\u4e0e\u4eba\u7c7b\u76f4\u89c9\u76f8\u5f53\u7684\u57fa\u7840\u89c6\u89c9\u539f\u8bed\u80fd\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76MLLMs\u662f\u5426\u62e5\u6709\u7c7b\u4f3c6-7\u5c81\u513f\u7ae5\u5df2\u638c\u63e1\u7684\u57fa\u672c\u89c6\u89c9\u80fd\u529b\uff0c\u4ee5\u8bc4\u4f30\u5176\u5e7f\u4e49\u89c6\u89c9\u667a\u80fd\u7684\u751f\u7406\u57fa\u7840\u3002", "method": "\u7814\u7a76\u5f15\u5165KidVis\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u57fa\u51c6\u57fa\u4e8e\u4eba\u7c7b\u89c6\u89c9\u53d1\u5c55\u7406\u8bba\uff0c\u5c06\u89c6\u89c9\u667a\u80fd\u89e3\u6784\u4e3a\u516d\u4e2a\u539f\u5b50\u80fd\u529b\uff1a\u4e13\u6ce8\u529b\u3001\u8ffd\u8e2a\u3001\u8fa8\u522b\u3001\u8bb0\u5fc6\u3001\u7a7a\u95f4\u548c\u95ed\u5408\u80fd\u529b\u3002\u8fd9\u4e9b\u80fd\u529b\u6784\u621010\u4e2a\u4f4e\u8bed\u4e49\u4f9d\u8d56\u7684\u89c6\u89c9\u4efb\u52a1\u7c7b\u522b\uff0c\u7528\u4e8e\u8bc4\u4f3020\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u4e0e\u4eba\u7c7b\u751f\u7406\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff1a\u4eba\u7c7b\u513f\u7ae5\u5e73\u5747\u5f97\u5206\u63a5\u8fd1\u5b8c\u7f8e\uff0895.32\uff09\uff0c\u800c\u6700\u5148\u8fdb\u7684GPT-5\u4ec5\u83b7\u5f9767.33\u5206\u3002\u7814\u7a76\u89c2\u5bdf\u5230\"\u7f29\u653e\u5b9a\u5f8b\u6096\u8bba\"\uff1a\u5355\u7eaf\u589e\u52a0\u6a21\u578b\u53c2\u6570\u65e0\u6cd5\u7ebf\u6027\u63d0\u5347\u8fd9\u4e9b\u57fa\u7840\u89c6\u89c9\u80fd\u529b\u3002\u6240\u670920\u4e2aMLLMs\u5728\u513f\u7ae5\u5df2\u638c\u63e1\u7684\u539f\u5b50\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u5747\u4e0d\u7406\u60f3\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5c3d\u7ba1\u5177\u5907\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u73b0\u5e7f\u4e49\u89c6\u89c9\u667a\u80fd\u6240\u9700\u7684\u57fa\u672c\u751f\u7406\u611f\u77e5\u539f\u8bed\u3002\u8fd9\u4e00\u53d1\u73b0\u6311\u6218\u4e86\u5355\u7eaf\u901a\u8fc7\u53c2\u6570\u6269\u5c55\u5c31\u80fd\u5b9e\u73b0\u5168\u9762\u89c6\u89c9\u667a\u80fd\u7684\u5047\u8bbe\uff0c\u8868\u660e\u9700\u8981\u65b0\u7684\u67b6\u6784\u6216\u8bad\u7ec3\u8303\u5f0f\u6765\u5f25\u8865\u57fa\u7840\u89c6\u89c9\u80fd\u529b\u7684\u4e0d\u8db3\u3002"}}
{"id": "2601.08311", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08311", "abs": "https://arxiv.org/abs/2601.08311", "authors": ["Kang Fu", "Huiyu Duan", "Zicheng Zhang", "Yucheng Zhu", "Jun Zhao", "Xiongkuo Min", "Jia Wang", "Guangtao Zhai"], "title": "Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation", "comment": null, "summary": "Large Multimodal Models (LMMs) have recently shown remarkable promise in low-level visual perception tasks, particularly in Image Quality Assessment (IQA), demonstrating strong zero-shot capability. However, achieving state-of-the-art performance often requires computationally expensive fine-tuning methods, which aim to align the distribution of quality-related token in output with image quality levels. Inspired by recent training-free works for LMM, we introduce IQARAG, a novel, training-free framework that enhances LMMs' IQA ability. IQARAG leverages Retrieval-Augmented Generation (RAG) to retrieve some semantically similar but quality-variant reference images with corresponding Mean Opinion Scores (MOSs) for input image. These retrieved images and input image are integrated into a specific prompt. Retrieved images provide the LMM with a visual perception anchor for IQA task. IQARAG contains three key phases: Retrieval Feature Extraction, Image Retrieval, and Integration & Quality Score Generation. Extensive experiments across multiple diverse IQA datasets, including KADID, KonIQ, LIVE Challenge, and SPAQ, demonstrate that the proposed IQARAG effectively boosts the IQA performance of LMMs, offering a resource-efficient alternative to fine-tuning for quality assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIQARAG\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u4f46\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u901a\u5e38\u9700\u8981\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u65e8\u5728\u5c06\u8d28\u91cf\u76f8\u5173\u6807\u8bb0\u7684\u8f93\u51fa\u5206\u5e03\u4e0e\u56fe\u50cf\u8d28\u91cf\u6c34\u5e73\u5bf9\u9f50\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "IQARAG\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u5305\u542b\u68c0\u7d22\u7279\u5f81\u63d0\u53d6\u3001\u56fe\u50cf\u68c0\u7d22\u4ee5\u53ca\u96c6\u6210\u4e0e\u8d28\u91cf\u5206\u6570\u751f\u6210\u4e09\u4e2a\u5173\u952e\u9636\u6bb5\uff0c\u901a\u8fc7\u68c0\u7d22\u8bed\u4e49\u76f8\u4f3c\u4f46\u8d28\u91cf\u53d8\u5316\u7684\u53c2\u8003\u56fe\u50cf\u53ca\u5176\u5e73\u5747\u610f\u89c1\u5206\u6570\uff0c\u5e76\u5c06\u8fd9\u4e9b\u68c0\u7d22\u5230\u7684\u56fe\u50cf\u4e0e\u8f93\u5165\u56fe\u50cf\u6574\u5408\u5230\u7279\u5b9a\u63d0\u793a\u4e2d\uff0c\u4e3aLMM\u63d0\u4f9b\u89c6\u89c9\u611f\u77e5\u951a\u70b9\u3002", "result": "\u5728KADID\u3001KonIQ\u3001LIVE Challenge\u548cSPAQ\u7b49\u591a\u4e2a\u591a\u6837\u5316\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cIQARAG\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6027\u80fd\uff0c\u4e3a\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u63d0\u4f9b\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u5728\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4f20\u7edf\u8ba1\u7b97\u5bc6\u96c6\u578b\u5fae\u8c03\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u8def\u5f84\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.08321", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08321", "abs": "https://arxiv.org/abs/2601.08321", "authors": ["Lichen Ma", "Xiaolong Fu", "Gaojing Zhou", "Zipeng Guo", "Ting Zhu", "Yichun Liu", "Yu Shi", "Jason Li", "Junshi Huang"], "title": "UM-Text: A Unified Multimodal Model for Image Understanding", "comment": null, "summary": "With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUM-Text\uff0c\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5b9e\u73b0\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u89c6\u89c9\u6587\u672c\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u6587\u672c\u751f\u6210\u4e2d\u98ce\u683c\u4e00\u81f4\u6027\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u6587\u672c\u7f16\u8f91\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u590d\u6742\u6b65\u9aa4\u6307\u5b9a\u6587\u672c\u5185\u5bb9\u548c\u5c5e\u6027\uff08\u5982\u5b57\u4f53\u5927\u5c0f\u3001\u989c\u8272\u3001\u5e03\u5c40\uff09\uff0c\u800c\u672a\u5145\u5206\u8003\u8651\u4e0e\u53c2\u8003\u56fe\u50cf\u7684\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u9a71\u52a8\u7684\u89c6\u89c9\u6587\u672c\u751f\u6210\u6548\u679c\u3002", "method": "\u63d0\u51faUM-Text\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5f15\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6307\u4ee4\u548c\u53c2\u8003\u56fe\u50cf\u4ee5\u7cbe\u5fc3\u8bbe\u8ba1\u6587\u672c\u5185\u5bb9\u548c\u5e03\u5c40\uff0c\u8bbe\u8ba1UM-Encoder\u81ea\u52a8\u914d\u7f6e\u591a\u79cd\u6761\u4ef6\u4fe1\u606f\u7684\u5d4c\u5165\u7ec4\u5408\uff0c\u91c7\u7528\u533a\u57df\u4e00\u81f4\u6027\u635f\u5931\u5728\u6f5c\u5728\u7a7a\u95f4\u548cRGB\u7a7a\u95f4\u63d0\u4f9b\u5b57\u5f62\u751f\u6210\u76d1\u7763\uff0c\u5e76\u5f00\u53d1\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u589e\u5f3a\u6027\u80fd\uff0c\u540c\u65f6\u8d21\u732e\u5305\u542b20\u4e07\u5f20\u591a\u6837\u5316\u573a\u666f\u89c6\u89c9\u6587\u672c\u56fe\u50cf\u7684UM-DATA-200K\u6570\u636e\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751f\u6210\u7684\u89c6\u89c9\u6587\u672c\u56fe\u50cf\u5728\u51c6\u786e\u6027\u548c\u4e0e\u53c2\u8003\u56fe\u50cf\u7684\u548c\u8c10\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u6587\u672c\u7f16\u8f91\u4e2d\u7684\u98ce\u683c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u533a\u57df\u4e00\u81f4\u6027\u635f\u5931\u548c\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u4e3a\u5b57\u5f62\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u76d1\u7763\uff0c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u8d21\u732e\u4e5f\u4e3a\u8be5\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u9a71\u52a8\u7684\u89c6\u89c9\u6587\u672c\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.08464", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08464", "abs": "https://arxiv.org/abs/2601.08464", "authors": ["Evgenii Maslov", "Valentin Khrulkov", "Anastasia Volkova", "Anton Gusarov", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "CoMa: Contextual Massing Generation with Vision-Language Models", "comment": "Code and dataset will be released later", "summary": "The conceptual design phase in architecture and urban planning, particularly building massing, is complex and heavily reliant on designer intuition and manual effort. To address this, we propose an automated framework for generating building massing based on functional requirements and site context. A primary obstacle to such data-driven methods has been the lack of suitable datasets. Consequently, we introduce the CoMa-20K dataset, a comprehensive collection that includes detailed massing geometries, associated economical and programmatic data, and visual representations of the development site within its existing urban context. We benchmark this dataset by formulating massing generation as a conditional task for Vision-Language Models (VLMs), evaluating both fine-tuned and large zero-shot models. Our experiments reveal the inherent complexity of the task while demonstrating the potential of VLMs to produce context-sensitive massing options. The dataset and analysis establish a foundational benchmark and highlight significant opportunities for future research in data-driven architectural design.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u5efa\u7b51\u4f53\u91cf\u751f\u6210\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86CoMa-20K\u6570\u636e\u96c6\u6765\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u5efa\u7b51\u8bbe\u8ba1\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u4f53\u91cf\u751f\u6210\u6784\u5efa\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6761\u4ef6\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u5efa\u7b51\u6982\u5ff5\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5efa\u7b51\u548c\u57ce\u5e02\u89c4\u5212\u4e2d\u7684\u6982\u5ff5\u8bbe\u8ba1\u9636\u6bb5\uff0c\u7279\u522b\u662f\u5efa\u7b51\u4f53\u91cf\u8bbe\u8ba1\uff0c\u5177\u6709\u9ad8\u5ea6\u590d\u6742\u6027\u4e14\u4e25\u91cd\u4f9d\u8d56\u8bbe\u8ba1\u5e08\u7684\u76f4\u89c9\u548c\u624b\u52a8\u5de5\u4f5c\uff0c\u800c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u9762\u4e34\u7684\u4e3b\u8981\u969c\u788d\u662f\u7f3a\u4e4f\u5408\u9002\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u81ea\u52a8\u5316\u8bbe\u8ba1\u6846\u67b6\u7684\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u529f\u80fd\u9700\u6c42\u548c\u573a\u5730\u4e0a\u4e0b\u6587\u7684\u81ea\u52a8\u5316\u5efa\u7b51\u4f53\u91cf\u751f\u6210\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86CoMa-20K\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u8be6\u7ec6\u7684\u4f53\u91cf\u51e0\u4f55\u3001\u7ecf\u6d4e\u548c\u7a0b\u5e8f\u6570\u636e\u4ee5\u53ca\u5f00\u53d1\u573a\u5730\u5728\u73b0\u6709\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u8868\u793a\uff0c\u901a\u8fc7\u5c06\u4f53\u91cf\u751f\u6210\u6784\u5efa\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6761\u4ef6\u4efb\u52a1\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u5fae\u8c03\u6a21\u578b\u548c\u5927\u578b\u96f6\u6837\u672c\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u5efa\u7b51\u4f53\u91cf\u751f\u6210\u4efb\u52a1\u7684\u5185\u5728\u590d\u6742\u6027\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u4f53\u91cf\u9009\u9879\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u6570\u636e\u96c6\u548c\u5206\u6790\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u5efa\u7b51\u8bbe\u8ba1\u5efa\u7acb\u4e86\u57fa\u7840\u57fa\u51c6\uff0c\u5e76\u7a81\u51fa\u4e86\u8be5\u9886\u57df\u672a\u6765\u7814\u7a76\u7684\u91cd\u8981\u673a\u4f1a\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u5efa\u7b51\u8bbe\u8ba1\u5efa\u7acb\u4e86\u91cd\u8981\u7684\u57fa\u51c6\u548c\u8d44\u6e90\uff0c\u5c55\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u5efa\u7b51\u4f53\u91cf\u751f\u6210\u4efb\u52a1\u7684\u6311\u6218\u6027\uff0c\u4e3a\u672a\u6765\u5728\u81ea\u52a8\u5316\u5efa\u7b51\u8bbe\u8ba1\u548c\u57ce\u5e02\u89c4\u5212\u65b9\u9762\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2601.08336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08336", "abs": "https://arxiv.org/abs/2601.08336", "authors": ["Junzhuo Liu", "Xuemei Du", "Daniel Reisenbuchler", "Ye Chen", "Markus Eckstein", "Christian Matek", "Friedrich Feuerhake", "Dorit Merhof"], "title": "Tissue Classification and Whole-Slide Images Analysis via Modeling of the Tumor Microenvironment and Biological Pathways", "comment": "19 pages, 8 figures. This work has been submitted to the IEEE for possible publication", "summary": "Automatic integration of whole slide images (WSIs) and gene expression profiles has demonstrated substantial potential in precision clinical diagnosis and cancer progression studies. However, most existing studies focus on individual gene sequences and slide level classification tasks, with limited attention to spatial transcriptomics and patch level applications. To address this limitation, we propose a multimodal network, BioMorphNet, which automatically integrates tissue morphological features and spatial gene expression to support tissue classification and differential gene analysis. For considering morphological features, BioMorphNet constructs a graph to model the relationships between target patches and their neighbors, and adjusts the response strength based on morphological and molecular level similarity, to better characterize the tumor microenvironment. In terms of multimodal interactions, BioMorphNet derives clinical pathway features from spatial transcriptomic data based on a predefined pathway database, serving as a bridge between tissue morphology and gene expression. In addition, a novel learnable pathway module is designed to automatically simulate the biological pathway formation process, providing a complementary representation to existing clinical pathways. Compared with the latest morphology gene multimodal methods, BioMorphNet's average classification metrics improve by 2.67%, 5.48%, and 6.29% for prostate cancer, colorectal cancer, and breast cancer datasets, respectively. BioMorphNet not only classifies tissue categories within WSIs accurately to support tumor localization, but also analyzes differential gene expression between tissue categories based on prediction confidence, contributing to the discovery of potential tumor biomarkers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BioMorphNet\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u7f51\u7edc\uff0c\u901a\u8fc7\u81ea\u52a8\u6574\u5408\u7ec4\u7ec7\u5f62\u6001\u5b66\u7279\u5f81\u548c\u7a7a\u95f4\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u6765\u652f\u6301\u7ec4\u7ec7\u5206\u7c7b\u548c\u5dee\u5f02\u57fa\u56e0\u5206\u6790\uff0c\u5728\u591a\u79cd\u764c\u75c7\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u57fa\u56e0\u5e8f\u5217\u548c\u5207\u7247\u7ea7\u522b\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u5bf9\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u548c\u6591\u5757\u7ea7\u522b\u5e94\u7528\u5173\u6ce8\u6709\u9650\uff0c\u8fd9\u9650\u5236\u4e86\u5168\u5207\u7247\u56fe\u50cf\u4e0e\u57fa\u56e0\u8868\u8fbe\u8c31\u7684\u6574\u5408\u5728\u7cbe\u51c6\u4e34\u5e8a\u8bca\u65ad\u548c\u764c\u75c7\u8fdb\u5c55\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "method": "BioMorphNet\u6784\u5efa\u56fe\u6a21\u578b\u6765\u5efa\u6a21\u76ee\u6807\u6591\u5757\u4e0e\u5176\u90bb\u57df\u7684\u5173\u7cfb\uff0c\u57fa\u4e8e\u5f62\u6001\u5b66\u548c\u5206\u5b50\u6c34\u5e73\u7684\u76f8\u4f3c\u6027\u8c03\u6574\u54cd\u5e94\u5f3a\u5ea6\u4ee5\u66f4\u597d\u8868\u5f81\u80bf\u7624\u5fae\u73af\u5883\uff1b\u4ece\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6570\u636e\u4e2d\u63d0\u53d6\u4e34\u5e8a\u901a\u8def\u7279\u5f81\u4f5c\u4e3a\u7ec4\u7ec7\u5f62\u6001\u4e0e\u57fa\u56e0\u8868\u8fbe\u7684\u6865\u6881\uff1b\u8bbe\u8ba1\u53ef\u5b66\u4e60\u901a\u8def\u6a21\u5757\u81ea\u52a8\u6a21\u62df\u751f\u7269\u901a\u8def\u5f62\u6210\u8fc7\u7a0b\uff0c\u4e3a\u73b0\u6709\u4e34\u5e8a\u901a\u8def\u63d0\u4f9b\u8865\u5145\u8868\u793a\u3002", "result": "\u4e0e\u6700\u65b0\u7684\u5f62\u6001-\u57fa\u56e0\u591a\u6a21\u6001\u65b9\u6cd5\u76f8\u6bd4\uff0cBioMorphNet\u5728\u524d\u5217\u817a\u764c\u3001\u7ed3\u76f4\u80a0\u764c\u548c\u4e73\u817a\u764c\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747\u5206\u7c7b\u6307\u6807\u5206\u522b\u63d0\u5347\u4e862.67%\u30015.48%\u548c6.29%\uff0c\u4e0d\u4ec5\u51c6\u786e\u5206\u7c7bWSI\u5185\u7684\u7ec4\u7ec7\u7c7b\u522b\u4ee5\u652f\u6301\u80bf\u7624\u5b9a\u4f4d\uff0c\u8fd8\u80fd\u57fa\u4e8e\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u5206\u6790\u7ec4\u7ec7\u7c7b\u522b\u95f4\u7684\u5dee\u5f02\u57fa\u56e0\u8868\u8fbe\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7ec4\u7ec7\u5f62\u6001\u5b66\u4e0e\u7a7a\u95f4\u57fa\u56e0\u8868\u8fbe\u7684\u6574\u5408\u63d0\u4f9b\u4e86\u521b\u65b0\u6846\u67b6\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u764c\u75c7\u7ec4\u7ec7\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u652f\u6301\u5dee\u5f02\u57fa\u56e0\u5206\u6790\u548c\u6f5c\u5728\u80bf\u7624\u751f\u7269\u6807\u5fd7\u7269\u7684\u53d1\u73b0\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u6570\u636e\u5206\u6790\u5728\u7cbe\u51c6\u533b\u7597\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2601.08557", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08557", "abs": "https://arxiv.org/abs/2601.08557", "authors": ["Sushant Gautam", "Cise Midoglu", "Vajira Thambawita", "Michael A. Riegler", "P\u00e5l Halvorsen"], "title": "VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations", "comment": null, "summary": "Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VideoHEDGE\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u89c6\u9891\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u5c55\u57fa\u4e8e\u71b5\u7684\u53ef\u9760\u6027\u4f30\u8ba1\u65b9\u6cd5\u5230\u65f6\u7a7a\u7ed3\u6784\u5316\u8f93\u5165\uff0c\u5e76\u5f15\u5165\u89c6\u89c9\u589e\u5f3a\u8bed\u4e49\u71b5(VASE)\u6307\u6807\uff0c\u5728\u591a\u4e2a7B\u53c2\u6570\u89c6\u9891VLM\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u5e7b\u89c9\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5e7b\u89c9\u73b0\u8c61\u9891\u7e41\u4e14\u7f6e\u4fe1\u5ea6\u9ad8\uff0c\u800c\u73b0\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u4e0e\u6b63\u786e\u6027\u5bf9\u9f50\uff0c\u8fd9\u6784\u6210\u4e86\u5f53\u524d\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u6311\u6218\u548c\u7814\u7a76\u7a7a\u767d\u3002", "method": "VideoHEDGE\u6846\u67b6\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4ece\u539f\u59cb\u89c6\u9891\u7247\u6bb5\u53ca\u5176\u5149\u5ea6\u548c\u65f6\u7a7a\u6270\u52a8\u53d8\u4f53\u4e2d\u751f\u6210\u57fa\u7ebf\u7b54\u6848\u548c\u591a\u4e2a\u9ad8\u6e29\u91c7\u6837\u54cd\u5e94\uff0c\u7136\u540e\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6216\u5d4c\u5165\u65b9\u6cd5\u5c06\u6587\u672c\u8f93\u51fa\u805a\u7c7b\u4e3a\u8bed\u4e49\u5047\u8bbe\uff0c\u6700\u7ec8\u57fa\u4e8e\u805a\u7c7b\u7ea7\u6982\u7387\u8d28\u91cf\u8ba1\u7b97\u4e09\u79cd\u53ef\u9760\u6027\u5206\u6570\uff1a\u8bed\u4e49\u71b5\u3001RadFlag\u548c\u89c6\u89c9\u589e\u5f3a\u8bed\u4e49\u71b5\u3002", "result": "\u5728SoccerChat\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u4e09\u4e2a7B\u53c2\u6570\u89c6\u9891VLM\u8fdb\u884c\u8bc4\u4f30\uff0c\u89c6\u89c9\u589e\u5f3a\u8bed\u4e49\u71b5\u5728\u8f83\u5927\u5931\u771f\u9884\u7b97\u4e0b\u59cb\u7ec8\u83b7\u5f97\u6700\u9ad8\u7684ROC-AUC\u6027\u80fd\uff0c\u800c\u8bed\u4e49\u71b5\u548cRadFlag\u5f80\u5f80\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff1b\u5d4c\u5165\u805a\u7c7b\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u8fbe\u5230\u4e0e\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u805a\u7c7b\u76f8\u5f53\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u89c6\u89c9\u589e\u5f3a\u8bed\u4e49\u71b5\u662f\u68c0\u6d4b\u89c6\u9891VLM\u5e7b\u89c9\u7684\u6709\u6548\u6307\u6807\uff0c\u5d4c\u5165\u805a\u7c7b\u63d0\u4f9b\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9886\u57df\u5fae\u8c03\u867d\u80fd\u51cf\u5c11\u5e7b\u89c9\u9891\u7387\u4f46\u5bf9\u6821\u51c6\u6539\u5584\u6709\u9650\uff0c\u540c\u65f6\u53d1\u5e03\u7684hedge-bench\u5e93\u652f\u6301\u53ef\u590d\u73b0\u548c\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2601.08355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08355", "abs": "https://arxiv.org/abs/2601.08355", "authors": ["Guo Cheng"], "title": "Semantic Misalignment in Vision-Language Models under Perceptual Degradation", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u9000\u5316\u4e0b\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u50cf\u7d20\u7ea7\u9c81\u68d2\u6027\u4e0e\u591a\u6a21\u6001\u8bed\u4e49\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u8131\u8282\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u8bed\u8a00\u7ea7\u9519\u4f4d\u5ea6\u91cf\u6807\u51c6\u6765\u91cf\u5316\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684VLM\u5931\u6548\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u73b0\u5b9e\u611f\u77e5\u9000\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eabAI\u7b49\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\uff0c\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u8bed\u4e49\u63a8\u7406\u548c\u51b3\u7b56\u5931\u8bef\u3002", "method": "\u7814\u7a76\u91c7\u7528Cityscapes\u6570\u636e\u96c6\u4e0a\u7684\u8bed\u4e49\u5206\u5272\u4f5c\u4e3a\u4ee3\u8868\u6027\u611f\u77e5\u6a21\u5757\uff0c\u5f15\u5165\u611f\u77e5\u73b0\u5b9e\u6027\u9000\u5316\uff0c\u8fd9\u4e9b\u9000\u5316\u4ec5\u5bfc\u81f4\u4f20\u7edf\u5206\u5272\u6307\u6807\u9002\u5ea6\u4e0b\u964d\uff0c\u4f46\u4f1a\u5f15\u53d1\u4e0b\u6e38VLM\u884c\u4e3a\u4e25\u91cd\u5931\u6548\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u5957\u8bed\u8a00\u7ea7\u9519\u4f4d\u5ea6\u91cf\u6807\u51c6\uff0c\u7528\u4e8e\u91cf\u5316\u5e7b\u89c9\u3001\u5173\u952e\u9057\u6f0f\u548c\u5b89\u5168\u8bef\u5224\u7b49\u73b0\u8c61\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u611f\u77e5\u9000\u5316\u5728\u4f20\u7edf\u5206\u5272\u6307\u6807\u4e0a\u4ec5\u9020\u6210\u9002\u5ea6\u4e0b\u964d\uff0c\u5374\u5bfc\u81f4\u4e0b\u6e38VLM\u51fa\u73b0\u4e25\u91cd\u5931\u6548\uff0c\u5305\u62ec\u5e7b\u89c9\u5bf9\u8c61\u63d0\u53ca\u3001\u5b89\u5168\u5173\u952e\u5b9e\u4f53\u9057\u6f0f\u4ee5\u53ca\u4e0d\u4e00\u81f4\u7684\u5b89\u5168\u5224\u65ad\uff1b\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u50cf\u7d20\u7ea7\u9c81\u68d2\u6027\u4e0e\u591a\u6a21\u6001\u8bed\u4e49\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u660e\u663e\u8131\u8282\uff0c\u8fd9\u4e00\u73b0\u8c61\u5728\u591a\u4e2a\u5bf9\u6bd4\u6027\u548c\u751f\u6210\u6027VLM\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u57fa\u4e8eVLM\u7684\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u8bc4\u4f30\u6846\u67b6\u9700\u8981\u660e\u786e\u8003\u8651\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u548c\u65b9\u5411\u3002"}}
{"id": "2601.08623", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08623", "abs": "https://arxiv.org/abs/2601.08623", "authors": ["Renyang Liu", "Kangjie Chen", "Han Qiu", "Jie Zhang", "Kwok-Yan Lam", "Tianwei Zhang", "See-Kiong Ng"], "title": "SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models", "comment": "Code at https://github.com/ryliu68/SafeRedir", "summary": "Image generation models (IGMs), while capable of producing impressive and creative content, often memorize a wide range of undesirable concepts from their training data, leading to the reproduction of unsafe content such as NSFW imagery and copyrighted artistic styles. Such behaviors pose persistent safety and compliance risks in real-world deployments and cannot be reliably mitigated by post-hoc filtering, owing to the limited robustness of such mechanisms and a lack of fine-grained semantic control. Recent unlearning methods seek to erase harmful concepts at the model level, which exhibit the limitations of requiring costly retraining, degrading the quality of benign generations, or failing to withstand prompt paraphrasing and adversarial attacks. To address these challenges, we introduce SafeRedir, a lightweight inference-time framework for robust unlearning via prompt embedding redirection. Without modifying the underlying IGMs, SafeRedir adaptively routes unsafe prompts toward safe semantic regions through token-level interventions in the embedding space. The framework comprises two core components: a latent-aware multi-modal safety classifier for identifying unsafe generation trajectories, and a token-level delta generator for precise semantic redirection, equipped with auxiliary predictors for token masking and adaptive scaling to localize and regulate the intervention. Empirical results across multiple representative unlearning tasks demonstrate that SafeRedir achieves effective unlearning capability, high semantic and perceptual preservation, robust image quality, and enhanced resistance to adversarial attacks. Furthermore, SafeRedir generalizes effectively across a variety of diffusion backbones and existing unlearned models, validating its plug-and-play compatibility and broad applicability. Code and data are available at https://github.com/ryliu68/SafeRedir.", "AI": {"tldr": "SafeRedir\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u5d4c\u5165\u91cd\u5b9a\u5411\u5b9e\u73b0\u9c81\u68d2\u6027\u9057\u5fd8\uff0c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5373\u53ef\u6709\u6548\u6d88\u9664\u6709\u5bb3\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u6027\u751f\u6210\u7684\u8d28\u91cf\u548c\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "motivation": "\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5bb9\u6613\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4e0d\u826f\u6982\u5ff5\uff0c\u5bfc\u81f4\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\u548c\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u827a\u672f\u98ce\u683c\uff0c\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u5b58\u5728\u9700\u8981\u6602\u8d35\u91cd\u65b0\u8bad\u7ec3\u3001\u964d\u4f4e\u826f\u6027\u751f\u6210\u8d28\u91cf\u6216\u65e0\u6cd5\u62b5\u6297\u63d0\u793a\u6539\u5199\u548c\u5bf9\u6297\u653b\u51fb\u7b49\u5c40\u9650\u6027\u3002", "method": "SafeRedir\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u7528\u4e8e\u8bc6\u522b\u4e0d\u5b89\u5168\u751f\u6210\u8f68\u8ff9\u7684\u6f5c\u5728\u611f\u77e5\u591a\u6a21\u6001\u5b89\u5168\u5206\u7c7b\u5668\uff0c\u4ee5\u53ca\u7528\u4e8e\u7cbe\u786e\u8bed\u4e49\u91cd\u5b9a\u5411\u7684\u4ee4\u724c\u7ea7\u589e\u91cf\u751f\u6210\u5668\uff0c\u540e\u8005\u914d\u5907\u4ee4\u724c\u63a9\u7801\u548c\u81ea\u9002\u5e94\u7f29\u653e\u8f85\u52a9\u9884\u6d4b\u5668\u4ee5\u5b9a\u4f4d\u548c\u8c03\u8282\u5e72\u9884\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSafeRedir\u5728\u591a\u4e2a\u4ee3\u8868\u6027\u9057\u5fd8\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u9057\u5fd8\u80fd\u529b\u3001\u9ad8\u8bed\u4e49\u548c\u611f\u77e5\u4fdd\u6301\u3001\u9c81\u68d2\u7684\u56fe\u50cf\u8d28\u91cf\u4ee5\u53ca\u589e\u5f3a\u7684\u6297\u5bf9\u6297\u653b\u51fb\u80fd\u529b\uff0c\u5e76\u5728\u591a\u79cd\u6269\u6563\u9aa8\u5e72\u548c\u73b0\u6709\u9057\u5fd8\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u63a8\u7406\u65f6\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5373\u63d2\u5373\u7528\u517c\u5bb9\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e3a\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u548c\u8bed\u4e49\u5b8c\u6574\u6027\u3002"}}
{"id": "2601.08408", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08408", "abs": "https://arxiv.org/abs/2601.08408", "authors": ["Yizhan Feng", "Hichem Snoussi", "Jing Teng", "Jian Liu", "Yuyang Wang", "Abel Cherouat", "Tian Wang"], "title": "Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2", "comment": "The Tenth International Conference on Data Mining and Big Data (DMBD'2025)", "summary": "The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBLIP-2\u7684\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u4efb\u52a1\u5e73\u53f0\uff0c\u901a\u8fc7\u96c6\u6210YOLO-World\u548cYOLOv8-Seg\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u5185\u5bb9\u611f\u77e5\u5173\u952e\u5e27\u91c7\u6837\u673a\u5236\u548c\u7edf\u4e00\u63d0\u793a\u4f18\u5316\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9ad8\u8ba1\u7b97\u6210\u672c\u4e0e\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u590d\u6742\u573a\u666f\u4e2d\u9700\u8981\u5b9e\u65f6\u89c6\u89c9\u7406\u89e3\u4e0e\u4ea4\u4e92\u80fd\u529b\uff0c\u4f46\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u4e0e\u65e0\u4eba\u673a\u8fb9\u7f18\u8bbe\u5907\u7684\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u77db\u76fe\uff0c\u8fd9\u963b\u788d\u4e86\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u4e0e\u5e94\u7528\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u6838\u5fc3\u90e8\u5206\uff1a\u9996\u5148\u5c06BLIP-2\u4e0eYOLO-World\u548cYOLOv8-Seg\u6a21\u578b\u6df1\u5ea6\u878d\u5408\uff0c\u5229\u7528YOLO\u7684\u7cbe\u786e\u611f\u77e5\u7ed3\u679c\u589e\u5f3a\u89c6\u89c9\u6ce8\u610f\u529b\u7406\u89e3\uff1b\u5176\u6b21\u8bbe\u8ba1\u57fa\u4e8eK-Means\u805a\u7c7b\u7684\u5185\u5bb9\u611f\u77e5\u5173\u952e\u5e27\u91c7\u6837\u673a\u5236\uff0c\u7ed3\u5408\u667a\u80fd\u5e27\u9009\u62e9\u548c\u65f6\u5e8f\u7279\u5f81\u62fc\u63a5\uff1b\u6700\u540e\u5b9e\u65bd\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u9002\u5e94\u63d0\u793a\u4f18\u5316\u65b9\u6848\uff0c\u5c06YOLO\u7684\u7ed3\u6784\u5316\u4e8b\u4ef6\u65e5\u5fd7\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u4fe1\u606f\u6ce8\u5165BLIP-2\u8f93\u5165\uff0c\u5e76\u8bbe\u8ba1\u8f93\u51fa\u7ea6\u675f\u8fc7\u6ee4\u6280\u672f\u7ec6\u8282\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u5bf9\u65e0\u4eba\u673a\u6570\u636e\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u6210\u529f\u6269\u5c55\u4e86BLIP-2\u7684\u591a\u4efb\u52a1\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u6709\u6548\u5904\u7406\u89c6\u9891\u7ea7\u4ea4\u4e92\u4efb\u52a1\uff0c\u5e76\u751f\u6210\u51c6\u786e\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8f93\u51fa\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\u540c\u65f6\u4fdd\u6301\u4e86\u591a\u6a21\u6001\u7406\u89e3\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u65e0\u4eba\u673a\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u4efb\u52a1\u5e73\u53f0\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6a21\u578b\u96c6\u6210\u3001\u667a\u80fd\u5e27\u91c7\u6837\u548c\u63d0\u793a\u4f18\u5316\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u4fdd\u6301\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u529b\uff0c\u4e3a\u65e0\u4eba\u673a\u5b9e\u65f6\u89c6\u89c9\u7406\u89e3\u4e0e\u4ea4\u4e92\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.08748", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08748", "abs": "https://arxiv.org/abs/2601.08748", "authors": ["Siqi Li", "Xinyu Cai", "Jianbiao Mei", "Nianchen Deng", "Pinlong Cai", "Licheng Wen", "Yufan Shen", "Xuemeng Yang", "Botian Shi", "Yong Liu"], "title": "UR-Bench: A Benchmark for Multi-Hop Reasoning over Ultra-High-Resolution Images", "comment": "10 pages, 5 figures", "summary": "Recent multimodal large language models (MLLMs) show strong capabilities in visual-language reasoning, yet their performance on ultra-high-resolution imagery remains largely unexplored. Existing visual question answering (VQA) benchmarks typically rely on medium-resolution data, offering limited visual complexity. To bridge this gap, we introduce Ultra-high-resolution Reasoning Benchmark (UR-Bench), a benchmark designed to evaluate the reasoning capabilities of MLLMs under extreme visual information. UR-Bench comprises two major categories, Humanistic Scenes and Natural Scenes, covering four subsets of ultra-high-resolution images with distinct spatial structures and data sources. Each subset contains images ranging from hundreds of megapixels to gigapixels, accompanied by questions organized into three levels, enabling evaluation of models' reasoning capabilities in ultra-high-resolution scenarios. We further propose an agent-based framework in which a language model performs reasoning by invoking external visual tools. In addition, we introduce Semantic Abstraction and Retrieval tools that enable more efficient processing of ultra-high-resolution images. We evaluate state-of-the-art models using both an end-to-end MLLMs and our agent-based framework, demonstrating the effectiveness of our framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UR-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u7528\u5916\u90e8\u89c6\u89c9\u5de5\u5177\u6765\u9ad8\u6548\u5904\u7406\u5343\u5146\u50cf\u7d20\u7ea7\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5f53\u524d\u7684\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u901a\u5e38\u4f7f\u7528\u4e2d\u7b49\u5206\u8fa8\u7387\u6570\u636e\uff0c\u89c6\u89c9\u590d\u6742\u5ea6\u6709\u9650\uff0c\u65e0\u6cd5\u8bc4\u4f30\u6a21\u578b\u5728\u6781\u7aef\u89c6\u89c9\u4fe1\u606f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86UR-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4eba\u6587\u573a\u666f\u548c\u81ea\u7136\u573a\u666f\u4e24\u5927\u7c7b\u522b\uff0c\u6db5\u76d6\u56db\u4e2a\u5177\u6709\u4e0d\u540c\u7a7a\u95f4\u7ed3\u6784\u548c\u6570\u636e\u6e90\u7684\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5b50\u96c6\uff0c\u56fe\u50cf\u5206\u8fa8\u7387\u4ece\u6570\u767e\u5146\u50cf\u7d20\u5230\u5343\u5146\u50cf\u7d20\u4e0d\u7b49\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u5176\u4e2d\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u8c03\u7528\u5916\u90e8\u89c6\u89c9\u5de5\u5177\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u4e86\u8bed\u4e49\u62bd\u8c61\u548c\u68c0\u7d22\u5de5\u5177\u4ee5\u5b9e\u73b0\u5bf9\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u9ad8\u6548\u5904\u7406\u3002", "result": "\u7814\u7a76\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5904\u7406\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u63a8\u7406\u4efb\u52a1\u65f6\u5177\u6709\u663e\u8457\u6709\u6548\u6027\uff0c\u80fd\u591f\u5e94\u5bf9\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u6781\u7aef\u89c6\u89c9\u590d\u6742\u5ea6\u6311\u6218\u3002", "conclusion": "UR-Bench\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u5728\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u63a8\u7406\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u89c6\u89c9\u573a\u666f\u4e0b\u7684\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6d4b\u8bd5\u5e73\u53f0\u3002\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\u5c55\u793a\u4e86\u901a\u8fc7\u6a21\u5757\u5316\u5de5\u5177\u8c03\u7528\u5904\u7406\u6781\u7aef\u89c6\u89c9\u4fe1\u606f\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u8d85\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u65b9\u5411\u3002"}}
{"id": "2601.08420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08420", "abs": "https://arxiv.org/abs/2601.08420", "authors": ["Aditya Chaudhary", "Sneha Barman", "Mainak Singha", "Ankit Jha", "Girish Mishra", "Biplab Banerjee"], "title": "MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP", "comment": "Accepted at InGARSS 2025", "summary": "In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP's training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u8bed\u8a00\u5f15\u5bfc\u7f51\u7edc\uff08MMLGNet\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5229\u7528CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u9ad8\u5149\u8c31\u6210\u50cf\u548cLiDAR\u7b49\u5f02\u6784\u9065\u611f\u6a21\u6001\u4e0e\u81ea\u7136\u8bed\u8a00\u8bed\u4e49\u5bf9\u9f50\uff0c\u901a\u8fc7\u8bed\u8a00\u76d1\u7763\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u6570\u636e\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5730\u7403\u89c2\u6d4b\u6570\u636e\u7684\u65e5\u76ca\u589e\u591a\uff0c\u8feb\u5207\u9700\u8981\u80fd\u591f\u6709\u6548\u878d\u5408\u5149\u8c31\u3001\u7a7a\u95f4\u548c\u51e0\u4f55\u4fe1\u606f\u5e76\u5b9e\u73b0\u8bed\u4e49\u7ea7\u7406\u89e3\u7684\u65b9\u6cd5\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5f02\u6784\u9065\u611f\u6a21\u6001\u4e0e\u8bed\u8a00\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "method": "MMLGNet\u91c7\u7528\u6a21\u6001\u7279\u5b9a\u7684\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u53cc\u5411\u5bf9\u6bd4\u5b66\u4e60\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5c06\u89c6\u89c9\u7279\u5f81\u4e0e\u624b\u5de5\u5236\u4f5c\u7684\u6587\u672c\u5d4c\u5165\u5bf9\u9f50\uff0c\u501f\u9274CLIP\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u4f7f\u7528\u7b80\u5355\u7684CNN\u7f16\u7801\u5668\u5b9e\u73b0\u9ad8\u7ef4\u9065\u611f\u6570\u636e\u4e0e\u8bed\u8a00\u5f15\u5bfc\u89e3\u91ca\u7684\u6865\u63a5\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMMLGNet\u8d85\u8d8a\u4e86\u591a\u79cd\u5df2\u5efa\u7acb\u7684\u591a\u6a21\u6001\u7eaf\u89c6\u89c9\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u5f3a\u52b2\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u8bed\u8a00\u76d1\u7763\u5bf9\u9065\u611f\u6570\u636e\u7406\u89e3\u7684\u663e\u8457\u76ca\u5904\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u8bed\u8a00\u76d1\u7763\u80fd\u591f\u6709\u6548\u63d0\u5347\u9065\u611f\u6570\u636e\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u5730\u7403\u89c2\u6d4b\u6570\u636e\u7684\u89e3\u91ca\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u7b80\u5355CNN\u7f16\u7801\u5668\u7ed3\u5408\u8bed\u8a00\u5f15\u5bfc\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.08807", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08807", "abs": "https://arxiv.org/abs/2601.08807", "authors": ["Tamas Endrei", "Gyorgy Cserey"], "title": "S3-CLIP: Video Super Resolution for Person-ReID", "comment": "Accepted to the 2026 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), VReID-XFD Challenge", "summary": "Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86S3-CLIP\uff0c\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7684CLIP-ReID\u6846\u67b6\uff0c\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6280\u672f\u5982\u4f55\u901a\u8fc7\u63d0\u5347\u8f68\u8ff9\u8d28\u91cf\u6765\u589e\u5f3a\u8de8\u89c6\u89d2\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7a7a\u4e2d\u5230\u5730\u9762\u548c\u5730\u9762\u5230\u7a7a\u4e2d\u573a\u666f\u4e2d\u3002", "motivation": "\u73b0\u6709\u884c\u4eba\u91cd\u8bc6\u522b\u65b9\u6cd5\u5927\u591a\u5c06\u8f68\u8ff9\u8d28\u91cf\u89c6\u4e3a\u6b21\u8981\u95ee\u9898\uff0c\u4e3b\u8981\u5173\u6ce8\u57fa\u7840\u6a21\u578b\u7684\u67b6\u6784\u6539\u8fdb\uff0c\u5ffd\u89c6\u4e86\u8f68\u8ff9\u8d28\u91cf\u5728\u73b0\u5b9e\u4e16\u754c\u56f0\u96be\u573a\u666f\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u8de8\u89c6\u89d2\u6761\u4ef6\u4e0b\u9700\u8981\u5904\u7406\u4f4e\u8d28\u91cf\u8f68\u8ff9\u7684\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86S3-CLIP\u6846\u67b6\uff0c\u5c06\u6700\u65b0\u7684\u8d85\u5206\u8fa8\u7387\u7f51\u7edc\u8fdb\u5c55\u4e0e\u4efb\u52a1\u9a71\u52a8\u7684\u8d85\u5206\u8fa8\u7387\u6d41\u7a0b\u76f8\u7ed3\u5408\uff0c\u4e13\u95e8\u9488\u5bf9\u89c6\u9891\u884c\u4eba\u91cd\u8bc6\u522b\u573a\u666f\u8fdb\u884c\u9002\u914d\uff0c\u901a\u8fc7\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6280\u672f\u63d0\u5347\u8f68\u8ff9\u8d28\u91cf\u6765\u589e\u5f3a\u91cd\u8bc6\u522b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aS3-CIP\u5728VReID-XFD\u6311\u6218\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5728\u7a7a\u5bf9\u5730\u573a\u666f\u8fbe\u523037.52% mAP\uff0c\u5728\u5730\u5bf9\u7a7a\u573a\u666f\u8fbe\u523029.16% mAP\uff0c\u7279\u522b\u662f\u5728\u5730\u5bf9\u7a7a\u8bbe\u7f6e\u4e2d\uff0cRank-1\u3001Rank-5\u548cRank-10\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u4e8611.24%\u300113.48%\u548c17.98%\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u4e86\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4f5c\u4e3a\u63d0\u5347\u884c\u4eba\u91cd\u8bc6\u522b\u8f68\u8ff9\u8d28\u91cf\u7684\u624b\u6bb5\uff0c\u7279\u522b\u662f\u5728\u8de8\u89c6\u89d2\u6761\u4ef6\u4e0b\uff0c\u4e3a\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u56f0\u96be\u573a\u666f\u4e2d\u7684\u4f4e\u8d28\u91cf\u8f68\u8ff9\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u8d85\u5206\u8fa8\u7387\u6280\u672f\u5728\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.08440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08440", "abs": "https://arxiv.org/abs/2601.08440", "authors": ["Yi Qin", "Lehan Wang", "Chenxu Zhao", "Alex P. W. Lee", "Xiaomeng Li"], "title": "Incentivizing Cardiologist-Like Reasoning in MLLMs for Interpretable Echocardiographic Diagnosis", "comment": null, "summary": "Echocardiographic diagnosis is vital for cardiac screening yet remains challenging. Existing echocardiography foundation models do not effectively capture the relationships between quantitative measurements and clinical manifestations, whereas medical reasoning multimodal large language models (MLLMs) require costly construction of detailed reasoning paths and remain ineffective at directly incorporating such echocardiographic priors into their reasoning. To address these limitations, we propose a novel approach comprising Cardiac Reasoning Template (CRT) and CardiacMind to enhance MLLM's echocardiographic reasoning by introducing cardiologist-like mindset. Specifically, CRT provides stepwise canonical diagnostic procedures for complex cardiac diseases to streamline reasoning path construction without the need for costly case-by-case verification. To incentivize reasoning MLLM under CRT, we develop CardiacMind, a new reinforcement learning scheme with three novel rewards: Procedural Quantity Reward (PQtR), Procedural Quality Reward (PQlR), and Echocardiographic Semantic Reward (ESR). PQtR promotes detailed reasoning; PQlR promotes integration of evidence across views and modalities, while ESR grounds stepwise descriptions in visual content. Our methods show a 48% improvement in multiview echocardiographic diagnosis for 15 complex cardiac diseases and a 5% improvement on CardiacNet-PAH over prior methods. The user study on our method's reasoning outputs shows 93.33% clinician agreement with cardiologist-like reasoning logic. Our code will be available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8d85\u58f0\u5fc3\u52a8\u56fe\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5fc3\u810f\u63a8\u7406\u6a21\u677f\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684CardiacMind\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u5fc3\u810f\u75be\u75c5\u7684\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8d85\u58f0\u5fc3\u52a8\u56fe\u57fa\u7840\u6a21\u578b\u672a\u80fd\u6709\u6548\u6355\u6349\u5b9a\u91cf\u6d4b\u91cf\u4e0e\u4e34\u5e8a\u8868\u73b0\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u800c\u533b\u5b66\u63a8\u7406\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6602\u8d35\u7684\u8be6\u7ec6\u63a8\u7406\u8def\u5f84\u6784\u5efa\uff0c\u4e14\u96be\u4ee5\u76f4\u63a5\u878d\u5165\u8d85\u58f0\u5fc3\u52a8\u56fe\u5148\u9a8c\u77e5\u8bc6\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u5fc3\u810f\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u5fc3\u810f\u63a8\u7406\u6a21\u677f\u63d0\u4f9b\u590d\u6742\u5fc3\u810f\u75be\u75c5\u7684\u9010\u6b65\u89c4\u8303\u5316\u8bca\u65ad\u6d41\u7a0b\uff0c\u7b80\u5316\u63a8\u7406\u8def\u5f84\u6784\u5efa\uff1bCardiacMind\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5f15\u5165\u4e09\u79cd\u65b0\u578b\u5956\u52b1\u673a\u5236\u2014\u2014\u8fc7\u7a0b\u6570\u91cf\u5956\u52b1\u4fc3\u8fdb\u8be6\u7ec6\u63a8\u7406\uff0c\u8fc7\u7a0b\u8d28\u91cf\u5956\u52b1\u4fc3\u8fdb\u8de8\u89c6\u56fe\u548c\u6a21\u6001\u7684\u8bc1\u636e\u6574\u5408\uff0c\u8d85\u58f0\u5fc3\u52a8\u56fe\u8bed\u4e49\u5956\u52b1\u786e\u4fdd\u9010\u6b65\u63cf\u8ff0\u4e0e\u89c6\u89c9\u5185\u5bb9\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u572815\u79cd\u590d\u6742\u5fc3\u810f\u75be\u75c5\u7684\u591a\u89c6\u56fe\u8d85\u58f0\u5fc3\u52a8\u56fe\u8bca\u65ad\u4e2d\u5b9e\u73b0\u4e8648%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728CardiacNet-PAH\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u9ad8\u4e865%\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u5176\u63a8\u7406\u8f93\u51fa\u7684\u4e34\u5e8a\u533b\u751f\u540c\u610f\u7387\u8fbe\u523093.33%\uff0c\u663e\u793a\u51fa\u4e0e\u5fc3\u810f\u75c5\u4e13\u5bb6\u76f8\u4f3c\u7684\u63a8\u7406\u903b\u8f91\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u5fc3\u810f\u75c5\u4e13\u5bb6\u601d\u7ef4\u6a21\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d85\u58f0\u5fc3\u52a8\u56fe\u8bca\u65ad\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\u548c\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u673a\u5236\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.08811", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08811", "abs": "https://arxiv.org/abs/2601.08811", "authors": ["Hsiang-Wei Huang", "Kuang-Ming Chen", "Wenhao Chai", "Cheng-Yen Yang", "Jen-Hao Cheng", "Jenq-Neng Hwang"], "title": "Reasoning Matters for 3D Visual Grounding", "comment": "2025 CVPR Workshop on 3D-LLM/VLA: Bridging Language, Vision and Action in 3D Environments", "summary": "The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5408\u62103D\u89c6\u89c9\u5b9a\u4f4d\u6570\u636e\u53ca\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u6570\u636e\u7ba1\u9053\uff0c\u5e76\u57fa\u4e8e\u8be5\u6570\u636e\u5fae\u8c03LLM\u5f97\u5230Reason3DVG-8B\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ec5\u4f7f\u75281.6%\u7684\u8bad\u7ec3\u6570\u636e\u5c31\u57283D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5148\u524d\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u63a8\u7406\u80fd\u529b\u57283D\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f53\u524d3D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u73b0\u6709\u6a21\u578b\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u4e14\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u76843D\u6570\u636e\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\uff1b\u540c\u65f6\uff0c\u73b0\u6709\u57fa\u4e8e\u5408\u6210\u6570\u636e\u6269\u5c55\u7684\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u6709\u9650\u4e14\u4e0e\u6570\u636e\u6536\u96c6\u6210\u672c\u4e0d\u6210\u6bd4\u4f8b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u6765\u63d0\u53473D\u89c6\u89c9\u5b9a\u4f4d\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u5408\u62103D\u89c6\u89c9\u5b9a\u4f4d\u6570\u636e\u53ca\u5176\u5bf9\u5e94\u63a8\u7406\u8fc7\u7a0b\u7684\u6570\u636e\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u89c6\u89c9\u5b9a\u4f4d\u8bad\u7ec3\u6570\u636e\uff1b\u57fa\u4e8e\u751f\u6210\u7684\u6570\u636e\uff0c\u7814\u7a76\u8005\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5f00\u53d1\u4e86Reason3DVG-8B\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf93D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4f18\u5316\u7684\u8bed\u8a00\u6a21\u578b\u3002", "result": "Reason3DVG-8B\u6a21\u578b\u57283D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4ec5\u4f7f\u7528\u5148\u524dLLM-based\u65b9\u6cd53D-GRAND\u6240\u9700\u8bad\u7ec3\u6570\u636e\u76841.6%\u5c31\u5b9e\u73b0\u4e86\u6027\u80fd\u8d85\u8d8a\uff1b\u8fd9\u4e00\u7ed3\u679c\u4e0d\u4ec5\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u6570\u636e\u7ba1\u9053\u7684\u6709\u6548\u6027\uff0c\u4e5f\u51f8\u663e\u4e86\u63a8\u7406\u8fc7\u7a0b\u57283D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u81ea\u52a8\u5408\u6210\u76843D\u89c6\u89c9\u5b9a\u4f4d\u6570\u636e\u53ca\u5176\u63a8\u7406\u8fc7\u7a0b\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6570\u636e\u9700\u6c42\uff1b\u8fd9\u4e00\u53d1\u73b0\u4e3a3D\u89c6\u89c9\u7406\u89e3\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u751f\u6210\u8303\u5f0f\uff0c\u5f3a\u8c03\u4e86\u63a8\u7406\u80fd\u529b\u5728\u8de8\u6a21\u60013D\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u9ad8\u6548\u76843D\u89c6\u89c9\u5b9a\u4f4d\u7cfb\u7edf\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.08458", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08458", "abs": "https://arxiv.org/abs/2601.08458", "authors": ["Chao Tian", "Zikun Zhou", "Chao Yang", "Guoqing Zhu", "Fu'an Zhong", "Zhenyu He"], "title": "Modality-Decoupled RGB-Thermal Object Detector via Query Fusion", "comment": null, "summary": "The advantage of RGB-Thermal (RGB-T) detection lies in its ability to perform modality fusion and integrate cross-modality complementary information, enabling robust detection under diverse illumination and weather conditions. However, under extreme conditions where one modality exhibits poor quality and disturbs detection, modality separation is necessary to mitigate the impact of noise. To address this problem, we propose a Modality-Decoupled RGB-T detection framework with Query Fusion (MDQF) to balance modality complementation and separation. In this framework, DETR-like detectors are employed as separate branches for the RGB and TIR images, with query fusion interspersed between the two branches in each refinement stage. Herein, query fusion is performed by feeding the high-quality queries from one branch to the other one after query selection and adaptation. This design effectively excludes the degraded modality and corrects the predictions using high-quality queries. Moreover, the decoupled framework allows us to optimize each individual branch with unpaired RGB or TIR images, eliminating the need for paired RGB-T data. Extensive experiments demonstrate that our approach delivers superior performance to existing RGB-T detectors and achieves better modality independence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u6001\u89e3\u8026\u7684RGB-T\u68c0\u6d4b\u6846\u67b6MDQF\uff0c\u901a\u8fc7\u67e5\u8be2\u878d\u5408\u673a\u5236\u5e73\u8861\u6a21\u6001\u4e92\u8865\u4e0e\u5206\u79bb\uff0c\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u6392\u9664\u9000\u5316\u6a21\u6001\u5e72\u6270\uff0c\u540c\u65f6\u652f\u6301\u4f7f\u7528\u975e\u914d\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u652f\u4f18\u5316\u3002", "motivation": "RGB-T\u68c0\u6d4b\u867d\u7136\u80fd\u901a\u8fc7\u6a21\u6001\u878d\u5408\u5229\u7528\u8de8\u6a21\u6001\u4e92\u8865\u4fe1\u606f\u5b9e\u73b0\u9c81\u68d2\u68c0\u6d4b\uff0c\u4f46\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u5f53\u4e00\u4e2a\u6a21\u6001\u8d28\u91cf\u8f83\u5dee\u65f6\u4f1a\u5e72\u6270\u68c0\u6d4b\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u6a21\u6001\u5206\u79bb\u6765\u51cf\u8f7b\u566a\u58f0\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5e73\u8861\u6a21\u6001\u4e92\u8865\u4e0e\u5206\u79bb\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u6a21\u6001\u89e3\u8026\u7684RGB-T\u68c0\u6d4b\u6846\u67b6MDQF\uff0c\u91c7\u7528\u7c7b\u4f3cDETR\u7684\u68c0\u6d4b\u5668\u4f5c\u4e3aRGB\u548cTIR\u56fe\u50cf\u7684\u5206\u652f\uff0c\u5728\u6bcf\u4e2a\u7ec6\u5316\u9636\u6bb5\u901a\u8fc7\u67e5\u8be2\u9009\u62e9\u4e0e\u9002\u5e94\u5c06\u9ad8\u8d28\u91cf\u67e5\u8be2\u4ece\u4e00\u4e2a\u5206\u652f\u9988\u9001\u5230\u53e6\u4e00\u4e2a\u5206\u652f\uff0c\u5b9e\u73b0\u67e5\u8be2\u878d\u5408\u5e76\u6392\u9664\u9000\u5316\u6a21\u6001\u5e72\u6270\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728RGB-T\u68c0\u6d4b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6a21\u6001\u72ec\u7acb\u6027\uff0c\u5e76\u4e14\u89e3\u8026\u6846\u67b6\u5141\u8bb8\u4f7f\u7528\u975e\u914d\u5bf9\u7684RGB\u6216TIR\u56fe\u50cf\u5355\u72ec\u4f18\u5316\u6bcf\u4e2a\u5206\u652f\uff0c\u51cf\u5c11\u4e86\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u9700\u6c42\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u5e73\u8861\u6a21\u6001\u4e92\u8865\u4e0e\u5206\u79bb\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u67e5\u8be2\u878d\u5408\u673a\u5236\u80fd\u6709\u6548\u6392\u9664\u9000\u5316\u6a21\u6001\u5e72\u6270\uff0c\u89e3\u8026\u6846\u67b6\u4e3a\u4f7f\u7528\u975e\u914d\u5bf9\u6570\u636e\u4f18\u5316\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u589e\u5f3a\u4e86RGB-T\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.08828", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08828", "abs": "https://arxiv.org/abs/2601.08828", "authors": ["Xindi Wu", "Despoina Paschalidou", "Jun Gao", "Antonio Torralba", "Laura Leal-Taix\u00e9", "Olga Russakovsky", "Sanja Fidler", "Jonathan Lorraine"], "title": "Motion Attribution for Video Generation", "comment": "See the project website at https://research.nvidia.com/labs/sil/projects/MOTIVE/", "summary": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Motive\u6846\u67b6\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8fd0\u52a8\u5f52\u56e0\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u5f52\u56e0\u6280\u672f\u8bc6\u522b\u5f71\u54cd\u65f6\u95f4\u52a8\u6001\u7684\u5173\u952e\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u53d1\u73b0\u6307\u5bfc\u6570\u636e\u7b5b\u9009\u4ee5\u63d0\u5347\u89c6\u9891\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u9891\u751f\u6210\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u6570\u636e\u5982\u4f55\u5f71\u54cd\u8fd0\u52a8\u52a8\u6001\u7684\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u8fd0\u52a8\u800c\u975e\u89c6\u89c9\u5916\u89c2\u7684\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\uff0c\u8fd9\u9650\u5236\u4e86\u901a\u8fc7\u6570\u636e\u7b5b\u9009\u4f18\u5316\u89c6\u9891\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u7684\u80fd\u529b\u3002", "method": "Motive\u662f\u4e00\u4e2a\u57fa\u4e8e\u68af\u5ea6\u7684\u8fd0\u52a8\u4e2d\u5fc3\u6570\u636e\u5f52\u56e0\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u52a8\u52a0\u6743\u635f\u5931\u63a9\u7801\u5c06\u65f6\u95f4\u52a8\u6001\u4e0e\u9759\u6001\u5916\u89c2\u5206\u79bb\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u8fd0\u52a8\u7279\u5b9a\u5f71\u54cd\u8ba1\u7b97\u3002\u8be5\u6846\u67b6\u80fd\u591f\u6269\u5c55\u5230\u73b0\u4ee3\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u89c6\u9891\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u5206\u6790\u54ea\u4e9b\u5fae\u8c03\u7247\u6bb5\u4f1a\u6539\u5584\u6216\u6076\u5316\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u5728\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u4e0a\uff0cMotive\u80fd\u591f\u8bc6\u522b\u5bf9\u8fd0\u52a8\u6709\u5f3a\u70c8\u5f71\u54cd\u7684\u8bad\u7ec3\u7247\u6bb5\uff0c\u6307\u5bfc\u7684\u6570\u636e\u7b5b\u9009\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002\u4f7f\u7528Motive\u9009\u62e9\u7684\u9ad8\u5f71\u54cd\u529b\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728VBench\u57fa\u51c6\u4e0a\u540c\u65f6\u6539\u5584\u4e86\u8fd0\u52a8\u5e73\u6ed1\u5ea6\u548c\u52a8\u6001\u7a0b\u5ea6\uff0c\u76f8\u6bd4\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u83b7\u5f97\u4e8674.1%\u7684\u4eba\u7c7b\u504f\u597d\u80dc\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5b9e\u73b0\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u8fd0\u52a8\u800c\u975e\u89c6\u89c9\u5916\u89c2\u7684\u6570\u636e\u5f52\u56e0\uff0c\u4e3a\u7406\u89e3\u6570\u636e\u5982\u4f55\u5f71\u54cd\u65f6\u95f4\u52a8\u6001\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\u3002Motive\u4e0d\u4ec5\u80fd\u591f\u8bca\u65ad\u73b0\u6709\u6a21\u578b\u7684\u8fd0\u52a8\u95ee\u9898\uff0c\u8fd8\u80fd\u6307\u5bfc\u9ad8\u6548\u7684\u6570\u636e\u7b5b\u9009\u7b56\u7565\uff0c\u4e3a\u63d0\u5347\u89c6\u9891\u751f\u6210\u8d28\u91cf\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2601.08467", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.08467", "abs": "https://arxiv.org/abs/2601.08467", "authors": ["Takamichi Miyata", "Sumiko Miyata", "Andrew Morris"], "title": "Zero-Shot Distracted Driver Detection via Vision Language Models with Double Decoupling", "comment": null, "summary": "Distracted driving is a major cause of traffic collisions, calling for robust and scalable detection methods. Vision-language models (VLMs) enable strong zero-shot image classification, but existing VLM-based distracted driver detectors often underperform in real-world conditions. We identify subject-specific appearance variations (e.g., clothing, age, and gender) as a key bottleneck: VLMs entangle these factors with behavior cues, leading to decisions driven by who the driver is rather than what the driver is doing. To address this, we propose a subject decoupling framework that extracts a driver appearance embedding and removes its influence from the image embedding prior to zero-shot classification, thereby emphasizing distraction-relevant evidence. We further orthogonalize text embeddings via metric projection onto Stiefel manifold to improve separability while staying close to the original semantics. Experiments demonstrate consistent gains over prior baselines, indicating the promise of our approach for practical road-safety applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u4f53\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u9a7e\u9a76\u5458\u5916\u89c2\u5d4c\u5165\u5e76\u5728\u96f6\u6837\u672c\u5206\u7c7b\u524d\u6d88\u9664\u5176\u5bf9\u56fe\u50cf\u5d4c\u5165\u7684\u5f71\u54cd\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9a7e\u9a76\u5458\u5206\u5fc3\u68c0\u6d4b\u4e2d\u4e3b\u4f53\u7279\u5b9a\u5916\u89c2\u53d8\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5206\u5fc3\u9a7e\u9a76\u662f\u4ea4\u901a\u4e8b\u6545\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u9700\u8981\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5206\u5fc3\u9a7e\u9a76\u5458\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u4e3b\u4f53\u7279\u5b9a\u5916\u89c2\u53d8\u5316\uff08\u5982\u670d\u88c5\u3001\u5e74\u9f84\u3001\u6027\u522b\uff09\u4e0e\u884c\u4e3a\u7ebf\u7d22\u7684\u7ea0\u7f20\uff0c\u5bfc\u81f4\u6a21\u578b\u51b3\u7b56\u57fa\u4e8e\u9a7e\u9a76\u5458\u8eab\u4efd\u800c\u975e\u9a7e\u9a76\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e3b\u4f53\u89e3\u8026\u6846\u67b6\uff0c\u63d0\u53d6\u9a7e\u9a76\u5458\u5916\u89c2\u5d4c\u5165\u5e76\u5728\u96f6\u6837\u672c\u5206\u7c7b\u524d\u6d88\u9664\u5176\u5bf9\u56fe\u50cf\u5d4c\u5165\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u5f3a\u8c03\u4e0e\u5206\u5fc3\u76f8\u5173\u7684\u8bc1\u636e\u3002\u8fdb\u4e00\u6b65\u901a\u8fc7\u5ea6\u91cf\u6295\u5f71\u5230Stiefel\u6d41\u5f62\u5bf9\u6587\u672c\u5d4c\u5165\u8fdb\u884c\u6b63\u4ea4\u5316\uff0c\u5728\u4fdd\u6301\u539f\u59cb\u8bed\u4e49\u63a5\u8fd1\u7684\u540c\u65f6\u63d0\u9ad8\u53ef\u5206\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u5148\u524d\u57fa\u7ebf\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u4e3b\u4f53\u89e3\u8026\u548c\u6587\u672c\u5d4c\u5165\u6b63\u4ea4\u5316\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u9053\u8def\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u4e3b\u4f53\u5916\u89c2\u53d8\u5316\u662f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5fc3\u9a7e\u9a76\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u74f6\u9888\uff0c\u63d0\u51fa\u7684\u89e3\u8026\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u5916\u89c2\u4e0e\u884c\u4e3a\u7ebf\u7d22\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5177\u6709\u5b9e\u9645\u9053\u8def\u5b89\u5168\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.08476", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.08476", "abs": "https://arxiv.org/abs/2601.08476", "authors": ["Hao Tang", "Yu Liu", "Shuanglin Yan", "Fei Shen", "Shengfeng He", "Jing Qin"], "title": "Cross-modal Proxy Evolving for OOD Detection with Vision-Language Models", "comment": "Accepted by AAAI 2026", "summary": "Reliable zero-shot detection of out-of-distribution (OOD) inputs is critical for deploying vision-language models in open-world settings. However, the lack of labeled negatives in zero-shot OOD detection necessitates proxy signals that remain effective under distribution shift. Existing negative-label methods rely on a fixed set of textual proxies, which (i) sparsely sample the semantic space beyond in-distribution (ID) classes and (ii) remain static while only visual features drift, leading to cross-modal misalignment and unstable predictions. In this paper, we propose CoEvo, a training- and annotation-free test-time framework that performs bidirectional, sample-conditioned adaptation of both textual and visual proxies. Specifically, CoEvo introduces a proxy-aligned co-evolution mechanism to maintain two evolving proxy caches, which dynamically mines contextual textual negatives guided by test images and iteratively refines visual proxies, progressively realigning cross-modal similarities and enlarging local OOD margins. Finally, we dynamically re-weight the contributions of dual-modal proxies to obtain a calibrated OOD score that is robust to distribution shift. Extensive experiments on standard benchmarks demonstrate that CoEvo achieves state-of-the-art performance, improving AUROC by 1.33% and reducing FPR95 by 45.98% on ImageNet-1K compared to strong negative-label baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoEvo\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u548c\u6807\u6ce8\u7684\u6d4b\u8bd5\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u3001\u6837\u672c\u6761\u4ef6\u5316\u7684\u6587\u672c\u4e0e\u89c6\u89c9\u4ee3\u7406\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u5206\u5e03\u5916\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ee3\u7406\u5bf9\u9f50\u7684\u534f\u540c\u8fdb\u5316\u673a\u5236\u52a8\u6001\u6316\u6398\u4e0a\u4e0b\u6587\u6587\u672c\u8d1f\u4f8b\u5e76\u8fed\u4ee3\u4f18\u5316\u89c6\u89c9\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u90e8\u7f72\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9700\u8981\u53ef\u9760\u7684\u96f6\u6837\u672c\u5206\u5e03\u5916\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u56fa\u5b9a\u6587\u672c\u4ee3\u7406\u7684\u8d1f\u6807\u7b7e\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4e00\u662f\u5bf9\u5206\u5e03\u5916\u8bed\u4e49\u7a7a\u95f4\u7684\u7a00\u758f\u91c7\u6837\u4e0d\u8db3\uff0c\u4e8c\u662f\u6587\u672c\u4ee3\u7406\u9759\u6001\u4e0d\u53d8\u800c\u89c6\u89c9\u7279\u5f81\u6f02\u79fb\u5bfc\u81f4\u8de8\u6a21\u6001\u9519\u4f4d\u548c\u9884\u6d4b\u4e0d\u7a33\u5b9a\uff0c\u8fd9\u9650\u5236\u4e86\u96f6\u6837\u672cOOD\u68c0\u6d4b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6709\u6548\u6027\u3002", "method": "CoEvo\u91c7\u7528\u65e0\u9700\u8bad\u7ec3\u548c\u6807\u6ce8\u7684\u6d4b\u8bd5\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7406\u5bf9\u9f50\u7684\u534f\u540c\u8fdb\u5316\u673a\u5236\u7ef4\u62a4\u4e24\u4e2a\u8fdb\u5316\u7684\u4ee3\u7406\u7f13\u5b58\uff0c\u5b9e\u73b0\u53cc\u5411\u3001\u6837\u672c\u6761\u4ef6\u5316\u7684\u81ea\u9002\u5e94\u3002\u8be5\u65b9\u6cd5\u52a8\u6001\u6316\u6398\u7531\u6d4b\u8bd5\u56fe\u50cf\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u6587\u672c\u8d1f\u4f8b\uff0c\u8fed\u4ee3\u4f18\u5316\u89c6\u89c9\u4ee3\u7406\uff0c\u9010\u6b65\u91cd\u65b0\u5bf9\u9f50\u8de8\u6a21\u6001\u76f8\u4f3c\u6027\u5e76\u6269\u5927\u5c40\u90e8OOD\u8fb9\u754c\uff0c\u6700\u540e\u52a8\u6001\u91cd\u65b0\u52a0\u6743\u53cc\u6a21\u6001\u4ee3\u7406\u7684\u8d21\u732e\u4ee5\u83b7\u5f97\u5bf9\u5206\u5e03\u504f\u79fb\u9c81\u68d2\u7684\u6821\u51c6OOD\u5206\u6570\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCoEvo\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u5f3a\u8d1f\u6807\u7b7e\u57fa\u7ebf\uff0c\u5728ImageNet-1K\u4e0aAUROC\u63d0\u5347\u4e861.33%\uff0cFPR95\u964d\u4f4e\u4e8645.98%\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u96f6\u6837\u672cOOD\u68c0\u6d4b\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u53cc\u5411\u3001\u6837\u672c\u6761\u4ef6\u5316\u7684\u4ee3\u7406\u81ea\u9002\u5e94\u673a\u5236\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u96f6\u6837\u672cOOD\u68c0\u6d4b\u4e2d\u7684\u8de8\u6a21\u6001\u9519\u4f4d\u95ee\u9898\uff0c\u52a8\u6001\u4ee3\u7406\u7f13\u5b58\u548c\u534f\u540c\u8fdb\u5316\u7b56\u7565\u4e3a\u5f00\u653e\u4e16\u754c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u591a\u6a21\u6001\u573a\u666f\u548c\u52a8\u6001\u73af\u5883\u9002\u5e94\u4e2d\u3002"}}
{"id": "2601.08587", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08587", "abs": "https://arxiv.org/abs/2601.08587", "authors": ["Zhengbo Xu", "Jie Ma", "Ziheng Wang", "Zhan Peng", "Jun Liang", "Jing Li"], "title": "End-to-End Video Character Replacement without Structural Guidance", "comment": "10 pages, 9 figures", "summary": "Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MoCha\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u9700\u5355\u5e27\u4efb\u610f\u63a9\u7801\u5b9e\u73b0\u53ef\u63a7\u89c6\u9891\u89d2\u8272\u66ff\u6362\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u9010\u5e27\u5206\u5272\u63a9\u7801\u548c\u663e\u5f0f\u7ed3\u6784\u6307\u5bfc\u7684\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u53ef\u63a7\u89c6\u9891\u89d2\u8272\u66ff\u6362\u9762\u4e34\u7f3a\u4e4f\u914d\u5bf9\u89c6\u9891\u6570\u636e\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u91cd\u5efa\u8303\u5f0f\uff0c\u9700\u8981\u9010\u5e27\u5206\u5272\u63a9\u7801\u548c\u663e\u5f0f\u7ed3\u6784\u6307\u5bfc\uff08\u5982\u9aa8\u67b6\u3001\u6df1\u5ea6\uff09\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5728\u906e\u6321\u3001\u89d2\u8272-\u7269\u4f53\u4ea4\u4e92\u3001\u5f02\u5e38\u59ff\u6001\u6216\u590d\u6742\u5149\u7167\u7b49\u590d\u6742\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e38\u5bfc\u81f4\u89c6\u89c9\u4f2a\u5f71\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u3002", "method": "MoCha\u6846\u67b6\u4ec5\u9700\u5355\u5e27\u4efb\u610f\u63a9\u7801\uff0c\u65e0\u9700\u9010\u5e27\u5206\u5272\u6216\u663e\u5f0f\u7ed3\u6784\u6307\u5bfc\uff1b\u5f15\u5165\u4e86\u6761\u4ef6\u611f\u77e5\u7684RoPE\u673a\u5236\u6765\u9002\u5e94\u591a\u6a21\u6001\u8f93\u5165\u6761\u4ef6\u5e76\u589e\u5f3a\u9762\u90e8\u8eab\u4efd\u7279\u5f81\uff1b\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u9636\u6bb5\uff1b\u4e3a\u89e3\u51b3\u5408\u683c\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7efc\u5408\u6570\u636e\u6784\u5efa\u6d41\u7a0b\uff0c\u5305\u62ec\u57fa\u4e8eUnreal Engine 5\u6784\u5efa\u7684\u9ad8\u4fdd\u771f\u6e32\u67d3\u6570\u636e\u96c6\u3001\u901a\u8fc7\u5f53\u524d\u8096\u50cf\u52a8\u753b\u6280\u672f\u5408\u6210\u7684\u8868\u60c5\u9a71\u52a8\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4ece\u73b0\u6709\u89c6\u9891-\u63a9\u7801\u5bf9\u884d\u751f\u7684\u589e\u5f3a\u6570\u636e\u96c6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u89c6\u89c9\u4f2a\u5f71\u5e76\u63d0\u9ad8\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u89d2\u8272\u66ff\u6362\u6548\u679c\u3002", "conclusion": "MoCha\u901a\u8fc7\u7b80\u5316\u8f93\u5165\u8981\u6c42\u5e76\u6784\u5efa\u7efc\u5408\u8bad\u7ec3\u6570\u636e\uff0c\u4e3a\u53ef\u63a7\u89c6\u9891\u89d2\u8272\u66ff\u6362\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u6761\u4ef6\u611f\u77e5\u673a\u5236\u548c\u540e\u8bad\u7ec3\u7b56\u7565\u4e3a\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4ee3\u7801\u7684\u53d1\u5e03\u5c06\u4fc3\u8fdb\u8be5\u65b9\u5411\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2601.08617", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08617", "abs": "https://arxiv.org/abs/2601.08617", "authors": ["Leo Fillioux", "Omprakash Chakraborty", "Ismail Ben Ayed", "Paul-Henry Courn\u00e8de", "Stergios Christodoulidis", "Maria Vakalopoulou", "Jose Dolz"], "title": "SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning", "comment": null, "summary": "With the increasing adoption of vision-language models (VLMs) in critical decision-making systems such as healthcare or autonomous driving, the calibration of their uncertainty estimates becomes paramount. Yet, this dimension has been largely underexplored in the VLM test-time prompt-tuning (TPT) literature, which has predominantly focused on improving their discriminative performance. Recent state-of-the-art advocates for enforcing full orthogonality over pairs of text prompt embeddings to enhance separability, and therefore calibration. Nevertheless, as we theoretically show in this work, the inherent gradients from fully orthogonal constraints will strongly push semantically related classes away, ultimately making the model overconfident. Based on our findings, we propose Semantic Orthogonal Calibration (SoC), a Huber-based regularizer that enforces smooth prototype separation while preserving semantic proximity, thereby improving calibration compared to prior orthogonality-based approaches. Across a comprehensive empirical validation, we demonstrate that SoC consistently improves calibration performance, while also maintaining competitive discriminative capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8bed\u4e49\u6b63\u4ea4\u6821\u51c6\uff08SoC\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7Huber-based\u6b63\u5219\u5316\u5668\u5b9e\u73b0\u5e73\u6ed1\u7684\u539f\u578b\u5206\u79bb\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u90bb\u8fd1\u6027\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u5224\u522b\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6821\u51c6\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5173\u952e\u51b3\u7b56\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u6821\u51c6\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709VLM\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u63d0\u5347\u5224\u522b\u6027\u80fd\uff0c\u800c\u6821\u51c6\u7ef4\u5ea6\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u5f3a\u5236\u6587\u672c\u63d0\u793a\u5d4c\u5165\u7684\u5b8c\u5168\u6b63\u4ea4\u6027\u4ee5\u589e\u5f3a\u53ef\u5206\u79bb\u6027\uff0c\u4f46\u7406\u8bba\u5206\u6790\u8868\u660e\u8fd9\u79cd\u5b8c\u5168\u6b63\u4ea4\u7ea6\u675f\u4f1a\u8fc7\u5ea6\u63a8\u52a8\u8bed\u4e49\u76f8\u5173\u7c7b\u522b\u5206\u79bb\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u8bed\u4e49\u6b63\u4ea4\u6821\u51c6\uff08SoC\uff09\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8eHuber\u7684\u6b63\u5219\u5316\u5668\uff0c\u65e8\u5728\u5b9e\u73b0\u5e73\u6ed1\u7684\u539f\u578b\u5206\u79bb\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u90bb\u8fd1\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u5b8c\u5168\u6b63\u4ea4\u7ea6\u675f\u7684\u5c40\u9650\u6027\uff0c\u5373\u5176\u68af\u5ea6\u4f1a\u5f3a\u70c8\u63a8\u52a8\u8bed\u4e49\u76f8\u5173\u7c7b\u522b\u5206\u79bb\uff0c\u4ece\u800c\u8bbe\u8ba1\u51fa\u80fd\u591f\u5e73\u8861\u53ef\u5206\u79bb\u6027\u548c\u8bed\u4e49\u4fdd\u6301\u7684\u6b63\u5219\u5316\u7b56\u7565\uff0c\u76f8\u6bd4\u5148\u524d\u57fa\u4e8e\u6b63\u4ea4\u6027\u7684\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6539\u5584\u6821\u51c6\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5168\u9762\u7684\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u7814\u7a76\u8868\u660eSoC\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u5730\u6539\u5584\u4e86\u6821\u51c6\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u6821\u51c6\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u4ecd\u4fdd\u6301\u4e86\u7ade\u4e89\u6027\u7684\u5224\u522b\u80fd\u529b\uff0c\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6821\u51c6\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u4f18\u5316\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5b8c\u5168\u6b63\u4ea4\u7ea6\u675f\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6821\u51c6\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u8bed\u4e49\u611f\u77e5\u6821\u51c6\u65b9\u6cd5\u3002SoC\u65b9\u6cd5\u901a\u8fc7\u5e73\u8861\u539f\u578b\u5206\u79bb\u548c\u8bed\u4e49\u4fdd\u6301\uff0c\u4e3aVLM\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5bf9\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u51b3\u7b56\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.08619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08619", "abs": "https://arxiv.org/abs/2601.08619", "authors": ["Yiming Sun", "Yuan Ruan", "Qinghua Hu", "Pengfei Zhu"], "title": "CtrlFuse: Mask-Prompt Guided Controllable Infrared and Visible Image Fusion", "comment": "18 pages,22 figures,published to AAAI 2026", "summary": "Infrared and visible image fusion generates all-weather perception-capable images by combining complementary modalities, enhancing environmental awareness for intelligent unmanned systems. Existing methods either focus on pixel-level fusion while overlooking downstream task adaptability or implicitly learn rigid semantics through cascaded detection/segmentation models, unable to interactively address diverse semantic target perception needs. We propose CtrlFuse, a controllable image fusion framework that enables interactive dynamic fusion guided by mask prompts. The model integrates a multi-modal feature extractor, a reference prompt encoder (RPE), and a prompt-semantic fusion module (PSFM). The RPE dynamically encodes task-specific semantic prompts by fine-tuning pre-trained segmentation models with input mask guidance, while the PSFM explicitly injects these semantics into fusion features. Through synergistic optimization of parallel segmentation and fusion branches, our method achieves mutual enhancement between task performance and fusion quality. Experiments demonstrate state-of-the-art results in both fusion controllability and segmentation accuracy, with the adapted task branch even outperforming the original segmentation model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCtrlFuse\uff0c\u4e00\u79cd\u53ef\u63a7\u7684\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u63d0\u793a\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u52a8\u6001\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u6027\u548c\u65e0\u6cd5\u4ea4\u4e92\u5904\u7406\u591a\u6837\u5316\u8bed\u4e49\u76ee\u6807\u611f\u77e5\u9700\u6c42\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u4e00\u662f\u4e13\u6ce8\u4e8e\u50cf\u7d20\u7ea7\u878d\u5408\u800c\u5ffd\u89c6\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u4e8c\u662f\u901a\u8fc7\u7ea7\u8054\u68c0\u6d4b/\u5206\u5272\u6a21\u578b\u9690\u5f0f\u5b66\u4e60\u521a\u6027\u8bed\u4e49\uff0c\u65e0\u6cd5\u4ea4\u4e92\u5f0f\u5904\u7406\u591a\u6837\u5316\u7684\u8bed\u4e49\u76ee\u6807\u611f\u77e5\u9700\u6c42\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u53ef\u63a7\u7684\u878d\u5408\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u5177\u4f53\u4efb\u52a1\u9700\u6c42\u52a8\u6001\u8c03\u6574\u878d\u5408\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u7684CtrlFuse\u6846\u67b6\u5305\u542b\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u5668\u3001\u53c2\u8003\u63d0\u793a\u7f16\u7801\u5668\uff08RPE\uff09\u548c\u63d0\u793a-\u8bed\u4e49\u878d\u5408\u6a21\u5757\uff08PSFM\uff09\u3002RPE\u901a\u8fc7\u8f93\u5165\u63a9\u7801\u5f15\u5bfc\u5fae\u8c03\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\uff0c\u52a8\u6001\u7f16\u7801\u4efb\u52a1\u7279\u5b9a\u7684\u8bed\u4e49\u63d0\u793a\uff1bPSFM\u5c06\u8fd9\u4e9b\u8bed\u4e49\u660e\u786e\u6ce8\u5165\u878d\u5408\u7279\u5f81\u4e2d\u3002\u901a\u8fc7\u5e76\u884c\u5206\u5272\u548c\u878d\u5408\u5206\u652f\u7684\u534f\u540c\u4f18\u5316\uff0c\u5b9e\u73b0\u4efb\u52a1\u6027\u80fd\u4e0e\u878d\u5408\u8d28\u91cf\u7684\u76f8\u4e92\u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCtrlFuse\u5728\u878d\u5408\u53ef\u63a7\u6027\u548c\u5206\u5272\u51c6\u786e\u6027\u65b9\u9762\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u7279\u522b\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u7ecf\u8fc7\u9002\u914d\u7684\u4efb\u52a1\u5206\u652f\u751a\u81f3\u8d85\u8d8a\u4e86\u539f\u59cb\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u6846\u67b6\u5728\u589e\u5f3a\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u4ea4\u4e92\u5f0f\u52a8\u6001\u878d\u5408\u6846\u67b6\u5b9e\u73b0\u4efb\u52a1\u611f\u77e5\u578b\u56fe\u50cf\u878d\u5408\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u667a\u80fd\u65e0\u4eba\u7cfb\u7edf\u7684\u73af\u5883\u611f\u77e5\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6846\u67b6\u7684\u53ef\u63a7\u6027\u8bbe\u8ba1\u4e3a\u591a\u6a21\u6001\u878d\u5408\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u80fd\u591f\u6839\u636e\u5177\u4f53\u5e94\u7528\u9700\u6c42\u5b9a\u5236\u878d\u5408\u7b56\u7565\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.08798", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.08798", "abs": "https://arxiv.org/abs/2601.08798", "authors": ["Maayan Yesharim", "R. G. Bina Perl", "Uri Roll", "Sarig Gafny", "Eli Geffen", "Yoav Ram"], "title": "Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching", "comment": "18 pages, 4 figures,", "summary": "Accurate individual identification is essential for monitoring rare amphibians, yet invasive marking is often unsuitable for critically endangered species. We evaluate state-of-the-art computer-vision methods for photographic re-identification of the Hula painted frog (Latonia nigriventer) using 1,233 ventral images from 191 individuals collected during 2013-2020 capture-recapture surveys. We compare deep local-feature matching in a zero-shot setting with deep global-feature embedding models. The local-feature pipeline achieves 98% top-1 closed-set identification accuracy, outperforming all global-feature models; fine-tuning improves the best global-feature model to 60% top-1 (91% top-10) but remains below local matching. To combine scalability with accuracy, we implement a two-stage workflow in which a fine-tuned global-feature model retrieves a short candidate list that is re-ranked by local-feature matching, reducing end-to-end runtime from 6.5-7.8 hours to ~38 minutes while maintaining ~96% top-1 closed-set accuracy on the labeled dataset. Separation of match scores between same- and different-individual pairs supports thresholding for open-set identification, enabling practical handling of novel individuals. We deploy this pipeline as a web application for routine field use, providing rapid, standardized, non-invasive identification to support conservation monitoring and capture-recapture analyses. Overall, in this species, zero-shot deep local-feature matching outperformed global-feature embedding and provides a strong default for photo-identification.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6fd2\u5371\u4e24\u6816\u52a8\u7269\u80e1\u62c9\u5f69\u86d9\u7684\u975e\u4fb5\u5165\u5f0f\u7167\u7247\u91cd\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u5c40\u90e8\u7279\u5f81\u5339\u914d\u4e0e\u5168\u5c40\u7279\u5f81\u5d4c\u5165\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u4e24\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4e14\u53ef\u6269\u5c55\u7684\u4e2a\u4f53\u8bc6\u522b\u3002", "motivation": "\u51c6\u786e\u8bc6\u522b\u4e2a\u4f53\u5bf9\u4e8e\u76d1\u6d4b\u7a00\u6709\u4e24\u6816\u52a8\u7269\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4fb5\u5165\u5f0f\u6807\u8bb0\u65b9\u6cd5\u901a\u5e38\u4e0d\u9002\u7528\u4e8e\u6781\u5ea6\u6fd2\u5371\u7269\u79cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\uff0c\u4e3a\u80e1\u62c9\u5f69\u86d9\u5f00\u53d1\u4e00\u79cd\u975e\u4fb5\u5165\u5f0f\u7684\u7167\u7247\u91cd\u8bc6\u522b\u6280\u672f\uff0c\u4ee5\u652f\u6301\u4fdd\u62a4\u76d1\u6d4b\u548c\u6355\u83b7-\u91cd\u6355\u83b7\u5206\u6790\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6df1\u5ea6\u5c40\u90e8\u7279\u5f81\u5339\u914d\u4e0e\u6df1\u5ea6\u5168\u5c40\u7279\u5f81\u5d4c\u5165\u6a21\u578b\uff0c\u4f7f\u7528\u4e862013-2020\u5e74\u95f4\u91c7\u96c6\u7684191\u53ea\u4e2a\u4f53\u76841,233\u5f20\u8179\u9762\u56fe\u50cf\u3002\u4e3a\u4e86\u7ed3\u5408\u53ef\u6269\u5c55\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff1a\u9996\u5148\u4f7f\u7528\u5fae\u8c03\u540e\u7684\u5168\u5c40\u7279\u5f81\u6a21\u578b\u68c0\u7d22\u5019\u9009\u5217\u8868\uff0c\u7136\u540e\u901a\u8fc7\u5c40\u90e8\u7279\u5f81\u5339\u914d\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\u3002", "result": "\u5c40\u90e8\u7279\u5f81\u7ba1\u9053\u5728\u5c01\u95ed\u96c6\u8bc6\u522b\u4e2d\u8fbe\u5230\u4e8698%\u7684top-1\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u6240\u6709\u5168\u5c40\u7279\u5f81\u6a21\u578b\uff1b\u5fae\u8c03\u540e\u7684\u6700\u4f73\u5168\u5c40\u7279\u5f81\u6a21\u578b\u8fbe\u523060% top-1\u51c6\u786e\u7387\uff0891% top-10\uff09\u3002\u4e24\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\u5c06\u7aef\u5230\u7aef\u8fd0\u884c\u65f6\u95f4\u4ece6.5-7.8\u5c0f\u65f6\u51cf\u5c11\u5230\u7ea638\u5206\u949f\uff0c\u540c\u65f6\u4fdd\u6301\u7ea696%\u7684top-1\u5c01\u95ed\u96c6\u51c6\u786e\u7387\u3002\u76f8\u540c\u4e2a\u4f53\u4e0e\u4e0d\u540c\u4e2a\u4f53\u5bf9\u7684\u5339\u914d\u5206\u6570\u5206\u79bb\u652f\u6301\u5f00\u653e\u96c6\u8bc6\u522b\u7684\u9608\u503c\u8bbe\u7f6e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u8be5\u7269\u79cd\uff0c\u96f6\u6837\u672c\u6df1\u5ea6\u5c40\u90e8\u7279\u5f81\u5339\u914d\u4f18\u4e8e\u5168\u5c40\u7279\u5f81\u5d4c\u5165\uff0c\u53ef\u4f5c\u4e3a\u7167\u7247\u8bc6\u522b\u7684\u5f3a\u5927\u9ed8\u8ba4\u65b9\u6cd5\u3002\u5f00\u53d1\u7684\u4e24\u9636\u6bb5\u7ba1\u9053\u7ed3\u5408\u4e86\u53ef\u6269\u5c55\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u5df2\u90e8\u7f72\u4e3a\u652f\u6301\u5e38\u89c4\u91ce\u5916\u4f7f\u7528\u7684Web\u5e94\u7528\u7a0b\u5e8f\uff0c\u4e3a\u975e\u4fb5\u5165\u5f0f\u4fdd\u62a4\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.08832", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08832", "abs": "https://arxiv.org/abs/2601.08832", "authors": ["Fahad Shamshad", "Nils Lukas", "Karthik Nandakumar"], "title": "RAVEN: Erasing Invisible Watermarks via Novel View Synthesis", "comment": "13 pages", "summary": "Invisible watermarking has become a critical mechanism for authenticating AI-generated image content, with major platforms deploying watermarking schemes at scale. However, evaluating the vulnerability of these schemes against sophisticated removal attacks remains essential to assess their reliability and guide robust design. In this work, we expose a fundamental vulnerability in invisible watermarks by reformulating watermark removal as a view synthesis problem. Our key insight is that generating a perceptually consistent alternative view of the same semantic content, akin to re-observing a scene from a shifted perspective, naturally removes the embedded watermark while preserving visual fidelity. This reveals a critical gap: watermarks robust to pixel-space and frequency-domain attacks remain vulnerable to semantic-preserving viewpoint transformations. We introduce a zero-shot diffusion-based framework that applies controlled geometric transformations in latent space, augmented with view-guided correspondence attention to maintain structural consistency during reconstruction. Operating on frozen pre-trained models without detector access or watermark knowledge, our method achieves state-of-the-art watermark suppression across 15 watermarking methods--outperforming 14 baseline attacks while maintaining superior perceptual quality across multiple datasets.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u4e0d\u53ef\u89c1\u6c34\u5370\u7684\u57fa\u672c\u8106\u5f31\u6027\uff0c\u901a\u8fc7\u5c06\u6c34\u5370\u53bb\u9664\u91cd\u65b0\u8868\u8ff0\u4e3a\u89c6\u56fe\u5408\u6210\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6269\u6563\u6846\u67b6\uff0c\u572815\u79cd\u6c34\u5370\u65b9\u6cd5\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5370\u6291\u5236\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u5353\u8d8a\u7684\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u4e0d\u53ef\u89c1\u6c34\u5370\u5df2\u6210\u4e3a\u8ba4\u8bc1AI\u751f\u6210\u56fe\u50cf\u5185\u5bb9\u7684\u5173\u952e\u673a\u5236\uff0c\u4f46\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6848\u5bf9\u6297\u590d\u6742\u53bb\u9664\u653b\u51fb\u7684\u8106\u5f31\u6027\u5bf9\u4e8e\u8bc4\u4f30\u5176\u53ef\u9760\u6027\u548c\u6307\u5bfc\u7a33\u5065\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u4e0d\u53ef\u89c1\u6c34\u5370\u7684\u57fa\u672c\u8106\u5f31\u6027\uff0c\u7279\u522b\u662f\u53d1\u73b0\u6c34\u5370\u5373\u4f7f\u5bf9\u50cf\u7d20\u7a7a\u95f4\u548c\u9891\u57df\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u8bed\u4e49\u4fdd\u6301\u7684\u89c6\u70b9\u53d8\u6362\u653b\u51fb\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6269\u6563\u6846\u67b6\uff0c\u5c06\u6c34\u5370\u53bb\u9664\u91cd\u65b0\u8868\u8ff0\u4e3a\u89c6\u56fe\u5408\u6210\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u7684\u5173\u952e\u6d1e\u5bdf\u662f\u751f\u6210\u76f8\u540c\u8bed\u4e49\u5185\u5bb9\u7684\u611f\u77e5\u4e00\u81f4\u66ff\u4ee3\u89c6\u56fe\uff0c\u7c7b\u4f3c\u4e8e\u4ece\u504f\u79fb\u89c6\u89d2\u91cd\u65b0\u89c2\u5bdf\u573a\u666f\uff0c\u4ece\u800c\u81ea\u7136\u53bb\u9664\u5d4c\u5165\u6c34\u5370\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002\u6846\u67b6\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5e94\u7528\u53d7\u63a7\u51e0\u4f55\u53d8\u6362\uff0c\u5e76\u901a\u8fc7\u89c6\u56fe\u5f15\u5bfc\u5bf9\u5e94\u6ce8\u610f\u529b\u589e\u5f3a\u4ee5\u5728\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u8bbf\u95ee\u68c0\u6d4b\u5668\u6216\u6c34\u5370\u77e5\u8bc6\u5373\u53ef\u5728\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\u64cd\u4f5c\u3002", "result": "\u8be5\u65b9\u6cd5\u572815\u79cd\u6c34\u5370\u65b9\u6cd5\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5370\u6291\u5236\u6548\u679c\uff0c\u8d85\u8d8a\u4e8614\u79cd\u57fa\u7ebf\u653b\u51fb\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u5353\u8d8a\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u540c\u65f6\u6709\u6548\u53bb\u9664\u6c34\u5370\uff0c\u9a8c\u8bc1\u4e86\u8bed\u4e49\u4fdd\u6301\u89c6\u70b9\u53d8\u6362\u5bf9\u6c34\u5370\u53bb\u9664\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u53ef\u89c1\u6c34\u5370\u7684\u57fa\u672c\u8106\u5f31\u6027\uff0c\u5373\u6c34\u5370\u5373\u4f7f\u5bf9\u4f20\u7edf\u50cf\u7d20\u7a7a\u95f4\u548c\u9891\u57df\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u8bed\u4e49\u4fdd\u6301\u7684\u89c6\u70b9\u53d8\u6362\u653b\u51fb\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u6c34\u5370\u65b9\u6848\u7684\u7a33\u5065\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u8003\u8651\u8bed\u4e49\u7ea7\u653b\u51fb\u5411\u91cf\uff0c\u5e76\u4e3a\u672a\u6765\u6c34\u5370\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
