<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-27.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 28]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 3]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 4]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-kbe-dme-dynamic-multimodal-evaluation-via-knowledge-enhanced-benchmark-evolution">[1] <a href="https://arxiv.org/abs/2510.21182">KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution</a></h3>
<p><em>Junzhe Zhang, Huixuan Zhang, Xiaojun Wan</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†çŸ¥è¯†å¢å¼ºåŸºå‡†æ¼”åŒ–ï¼ˆKBEï¼‰æ¡†æ¶ï¼Œé€šè¿‡å›¾ç»“æ„è¡¨ç¤ºVQAæ ·æœ¬å¹¶æ•´åˆå¤šæ¨¡æ€çŸ¥è¯†ï¼Œå°†é™æ€åŸºå‡†è½¬åŒ–ä¸ºå¯æ§çš„åŠ¨æ€æ¼”åŒ–ç‰ˆæœ¬ï¼Œä»¥è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­çš„æ•°æ®æ±¡æŸ“å’Œé¥±å’Œé—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é™æ€åŸºå‡†å­˜åœ¨æ•°æ®æ±¡æŸ“å’Œé¥±å’Œé£é™©ï¼Œå¯¼è‡´æ€§èƒ½è¯„ä¼°å¤±çœŸæˆ–è¯¯å¯¼ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•éœ€è¦æ›´å¯é çš„è¯„ä¼°åè®®æ¥å‡†ç¡®è¡¡é‡æ¨¡å‹èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨å›¾ç»“æ„è¡¨ç¤ºé™æ€æˆ–åŠ¨æ€VQAæ ·æœ¬ï¼Œæå‡ºçŸ¥è¯†å¢å¼ºåŸºå‡†æ¼”åŒ–æ¡†æ¶ï¼Œé€šè¿‡é‡æ–°é€‰æ‹©åŸå§‹å›¾åƒä¸­çš„è§†è§‰ä¿¡æ¯å’Œæ•´åˆå¤–éƒ¨æ–‡æœ¬çŸ¥è¯†æ¥é‡æ„å’Œæ‰©å±•é—®é¢˜ï¼Œå®ç°éš¾åº¦å¯æ§çš„è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜KBEæœ‰æ•ˆç¼“è§£äº†æ•°æ®æ±¡æŸ“å’Œé¥±å’Œé£é™©ï¼Œæä¾›äº†å¯¹MLLMèƒ½åŠ›æ›´å…¨é¢çš„è¯„ä¼°ï¼Œé€šè¿‡è°ƒæ•´é—®é¢˜æ¢ç´¢ç¨‹åº¦å®ç°éš¾åº¦å¯æ§çš„åŸºå‡†æ¼”åŒ–ã€‚</p>
<p><strong>Conclusion:</strong> KBEæ¡†æ¶ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æä¾›äº†åŠ¨æ€ã€å¯æ§çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹çœŸå®èƒ½åŠ›ï¼Œä¸ºæœªæ¥è¯„ä¼°æ–¹æ³•çš„å‘å±•æä¾›äº†é‡è¦æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>The rapid progress of multimodal large language models (MLLMs) calls for more
reliable evaluation protocols. Existing static benchmarks suffer from the
potential risk of data contamination and saturation, leading to inflated or
misleading performance evaluations. To address these issues, we first apply
Graph formulation to represent a static or dynamic VQA sample. With the
formulation, we propose Knowledge-enhanced Benchmark Evolution(KBE), a dynamic
multimodal evaluation framework. KBE first analyzes the original static
benchmark, then expands it by integrating multimodal knowledge, transforming
the static benchmark into a controllable, dynamic evolving version. Crucially,
KBE can both reconstruct questions by Re-selecting visual information in the
original image and expand existing questions with external textual knowledge.
It enables difficulty-controllable evaluation by adjusting the degree of
question exploration. Extensive experiments demonstrate that KBE alleviates the
risk of data contamination, data saturation, and provides a more comprehensive
assessment of MLLM capabilities.</p>
<h3 id="2-preventing-shortcuts-in-adapter-training-via-providing-the-shortcuts">[2] <a href="https://arxiv.org/abs/2510.20887">Preventing Shortcuts in Adapter Training via Providing the Shortcuts</a></h3>
<p><em>Anujraaj Argo Goyal, Guocheng Gordon Qian, Huseyin Coskun, Aarush Gupta, Himmy Tam, Daniil Ostashev, Ju Hu, Dhritiman Sagar, Sergey Tulyakov, Kfir Aberman, Kuan-Chieh Jackson Wang</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ·å¾„é‡å®šå‘é€‚é…å™¨è®­ç»ƒçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åœ¨é€‚é…å™¨è®­ç»ƒæœŸé—´ä¸ºæ··æ·†å› ç´ å»ºç«‹ä¸“ç”¨è·¯å¾„ï¼Œæœ‰æ•ˆè§£å†³äº†é€‚é…å™¨çº ç¼ ç›®æ ‡å±æ€§ä¸å¶ç„¶å› ç´ çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡ã€å¤šæ ·æ€§å’Œæ–‡æœ¬æç¤ºéµå¾ªèƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŸºäºé€‚é…å™¨çš„è®­ç»ƒåœ¨æ‰©å±•åŸºç¡€å›¾åƒç”Ÿæˆå™¨èƒ½åŠ›æ–¹é¢å‘æŒ¥å…³é”®ä½œç”¨ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ç›®æ ‡å±æ€§ä¸å¶ç„¶å› ç´ ï¼ˆå¦‚å§¿æ€ã€è¡¨æƒ…ã€å…‰ç…§ï¼‰çš„çº ç¼ é—®é¢˜ï¼Œè¿™ç§ä¼ªç›¸å…³æ€§é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¹¶é˜»ç¢äº†å¯¹è¾“å…¥æ–‡æœ¬æç¤ºçš„éµå¾ªã€‚</p>
<p><strong>Method:</strong> æå‡ºæ·å¾„é‡å®šå‘é€‚é…å™¨è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ControlNetæˆ–LoRAç­‰è¾…åŠ©æ¨¡å—ä¸ºæ··æ·†å› ç´ å»ºç«‹ä¸“ç”¨è·¯å¾„ï¼Œæ¶ˆé™¤é€‚é…å™¨å†…éƒ¨åŒ–è¿™äº›å› ç´ çš„åŠ¨æœºï¼Œè®­ç»ƒå®Œæˆååœ¨æ¨ç†é˜¶æ®µç§»é™¤è¾…åŠ©æ¨¡å—ã€‚</p>
<p><strong>Result:</strong> åœ¨é¢éƒ¨å’Œå…¨èº«èº«ä»½æ³¨å…¥ç­‰ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡ã€å¤šæ ·æ€§å’Œæç¤ºéµå¾ªèƒ½åŠ›ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£è€¦ç›®æ ‡å±æ€§ä¸å¶ç„¶å› ç´ ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªé€šç”¨è®¾è®¡åŸåˆ™ï¼šåœ¨è¿½æ±‚è§£è€¦è¡¨ç¤ºæ—¶ï¼Œæœ€æœ‰æ•ˆçš„é€”å¾„å¯èƒ½æ˜¯ä¸ºä¸åº”å­¦ä¹ çš„å†…å®¹å»ºç«‹æ·å¾„ï¼Œè¿™ä¸ºå¤§å‹æ¨¡å‹æ—¶ä»£çš„é€‚é…å™¨è®­ç»ƒæä¾›äº†æ–°çš„è®¾è®¡èŒƒå¼ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Adapter-based training has emerged as a key mechanism for extending the
capabilities of powerful foundation image generators, enabling personalized and
stylized text-to-image synthesis. These adapters are typically trained to
capture a specific target attribute, such as subject identity, using
single-image reconstruction objectives. However, because the input image
inevitably contains a mixture of visual factors, adapters are prone to entangle
the target attribute with incidental ones, such as pose, expression, and
lighting. This spurious correlation problem limits generalization and obstructs
the model's ability to adhere to the input text prompt. In this work, we
uncover a simple yet effective solution: provide the very shortcuts we wish to
eliminate during adapter training. In Shortcut-Rerouted Adapter Training,
confounding factors are routed through auxiliary modules, such as ControlNet or
LoRA, eliminating the incentive for the adapter to internalize them. The
auxiliary modules are then removed during inference. When applied to tasks like
facial and full-body identity injection, our approach improves generation
quality, diversity, and prompt adherence. These results point to a general
design principle in the era of large models: when seeking disentangled
representations, the most effective path may be to establish shortcuts for what
should NOT be learned.</p>
<h3 id="3-head-pursuit-probing-attention-specialization-in-multimodal-transformers">[3] <a href="https://arxiv.org/abs/2510.21518">Head Pursuit: Probing Attention Specialization in Multimodal Transformers</a></h3>
<p><em>Lorenzo Basile, Valentino Maiorca, Diego Doimo, Francesco Locatello, Alberto Cazzaniga</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é€šè¿‡ä¿¡å·å¤„ç†è§†è§’é‡æ–°è§£é‡Šæ³¨æ„åŠ›å¤´åˆ†æï¼Œæ­ç¤ºäº†æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸­ä¸ªä½“æ³¨æ„åŠ›å¤´åœ¨è¯­ä¹‰å’Œè§†è§‰å±æ€§ä¸Šçš„ä¸“ä¸šåŒ–æ¨¡å¼ï¼Œå¹¶è¯æ˜ä»…ç¼–è¾‘1%çš„æ³¨æ„åŠ›å¤´å³å¯å¯é åœ°æŠ‘åˆ¶æˆ–å¢å¼ºç›®æ ‡æ¦‚å¿µã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡è¯­è¨€å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å†…éƒ¨å·¥ä½œæœºåˆ¶ä»æœªè¢«å®Œå…¨ç†è§£ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸­ä¸ªä½“æ³¨æ„åŠ›å¤´å¦‚ä½•ä¸“é—¨åŒ–å¤„ç†ç‰¹å®šè¯­ä¹‰æˆ–è§†è§‰å±æ€§ï¼Œä»¥å¡«è¡¥æ¨¡å‹å¯è§£é‡Šæ€§æ–¹é¢çš„ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> åŸºäºå·²æœ‰çš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œæœ¬ç ”ç©¶ä»ä¿¡å·å¤„ç†è§’åº¦é‡æ–°è§£é‡Šäº†ä½¿ç”¨æœ€ç»ˆè§£ç å±‚æ¢æµ‹ä¸­é—´æ¿€æ´»çš„å®è·µï¼Œå¼€å‘äº†ä¸€ç§åŸåˆ™æ€§æ–¹æ³•æ¥åˆ†æå¤šä¸ªæ ·æœ¬å¹¶æ ¹æ®æ³¨æ„åŠ›å¤´ä¸ç›®æ ‡æ¦‚å¿µçš„ç›¸å…³æ€§è¿›è¡Œæ’åºã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘ç°åœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€Transformerä¸­éƒ½å­˜åœ¨ä¸€è‡´çš„æ³¨æ„åŠ›å¤´çº§ä¸“ä¸šåŒ–æ¨¡å¼ï¼Œé€šè¿‡è¯¥æ–¹æ³•é€‰æ‹©çš„ä»…1%æ³¨æ„åŠ›å¤´ç¼–è¾‘å³å¯å¯é åœ°æŠ‘åˆ¶æˆ–å¢å¼ºæ¨¡å‹è¾“å‡ºä¸­çš„ç›®æ ‡æ¦‚å¿µï¼Œå¹¶åœ¨è¯­è¨€é—®ç­”ã€æ¯’æ€§ç¼“è§£ä»¥åŠè§†è§‰è¯­è¨€å›¾åƒåˆ†ç±»å’Œæè¿°ä»»åŠ¡ä¸­å¾—åˆ°éªŒè¯ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœæ­ç¤ºäº†æ³¨æ„åŠ›å±‚å†…å­˜åœ¨å¯è§£é‡Šå’Œå¯æ§çš„ç»“æ„ï¼Œä¸ºç†è§£å’Œç¼–è¾‘å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹æä¾›äº†ç®€å•æœ‰æ•ˆçš„å·¥å…·ï¼ŒåŒæ—¶è¯æ˜äº†æ¨¡å‹å†…éƒ¨å­˜åœ¨ç³»ç»Ÿæ€§çš„æ¦‚å¿µä¸“ä¸šåŒ–æœºåˆ¶ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Language and vision-language models have shown impressive performance across
a wide range of tasks, but their internal mechanisms remain only partly
understood. In this work, we study how individual attention heads in
text-generative models specialize in specific semantic or visual attributes.
Building on an established interpretability method, we reinterpret the practice
of probing intermediate activations with the final decoding layer through the
lens of signal processing. This lets us analyze multiple samples in a
principled way and rank attention heads based on their relevance to target
concepts. Our results show consistent patterns of specialization at the head
level across both unimodal and multimodal transformers. Remarkably, we find
that editing as few as 1% of the heads, selected using our method, can reliably
suppress or enhance targeted concepts in the model output. We validate our
approach on language tasks such as question answering and toxicity mitigation,
as well as vision-language tasks including image classification and captioning.
Our findings highlight an interpretable and controllable structure within
attention layers, offering simple tools for understanding and editing
large-scale generative models.</p>
<h3 id="4-video-as-prompt-unified-semantic-control-for-video-generation">[4] <a href="https://arxiv.org/abs/2510.20888">Video-As-Prompt: Unified Semantic Control for Video Generation</a></h3>
<p><em>Yuxuan Bian, Xin Chen, Zenan Li, Tiancheng Zhi, Shen Sang, Linjie Luo, Qiang Xu</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Video-As-Prompt (VAP)æ–°èŒƒå¼ï¼Œé€šè¿‡å°†å‚è€ƒè§†é¢‘ä½œä¸ºè¯­ä¹‰æç¤ºæ¥å¼•å¯¼å†»ç»“çš„è§†é¢‘æ‰©æ•£å˜æ¢å™¨ï¼Œå®ç°äº†ç»Ÿä¸€ã€å¯æ³›åŒ–çš„è¯­ä¹‰æ§åˆ¶è§†é¢‘ç”Ÿæˆï¼Œåœ¨å¼€æºæ–¹æ³•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘ç”Ÿæˆä¸­çš„ç»Ÿä¸€è¯­ä¹‰æ§åˆ¶é¢ä¸´å…³é”®æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆé€šè¿‡ç»“æ„æ§åˆ¶æ–½åŠ ä¸é€‚å½“çš„åƒç´ çº§å…ˆéªŒå¼•å…¥ä¼ªå½±ï¼Œè¦ä¹ˆä¾èµ–ä¸å¯æ³›åŒ–çš„æ¡ä»¶ç‰¹å®šå¾®è°ƒæˆ–ä»»åŠ¡ç‰¹å®šæ¶æ„ï¼Œç¼ºä¹é€šç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> VAPé‡‡ç”¨ä¸Šä¸‹æ–‡ç”ŸæˆèŒƒå¼ï¼Œåˆ©ç”¨å‚è€ƒè§†é¢‘ä½œä¸ºç›´æ¥è¯­ä¹‰æç¤ºï¼Œé€šè¿‡å³æ’å³ç”¨çš„æ··åˆå˜æ¢å™¨ä¸“å®¶æ¶æ„å¼•å¯¼å†»ç»“çš„Video Diffusion Transformerï¼Œå¹¶é‡‡ç”¨æ—¶é—´åç½®ä½ç½®åµŒå…¥æ¶ˆé™¤è™šå‡æ˜ å°„å…ˆéªŒä»¥å®ç°é²æ£’çš„ä¸Šä¸‹æ–‡æ£€ç´¢ã€‚</p>
<p><strong>Result:</strong> VAPä½œä¸ºå•ä¸€ç»Ÿä¸€æ¨¡å‹ï¼Œåœ¨å¼€æºæ–¹æ³•ä¸­è¾¾åˆ°æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œè·å¾—38.7%çš„ç”¨æˆ·åå¥½ç‡ï¼Œå¯ä¸é¢†å…ˆçš„æ¡ä»¶ç‰¹å®šå•†ä¸šæ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒæ—¶æ„å»ºäº†åŒ…å«10ä¸‡å¯¹è§†é¢‘çš„VAP-Dataæ•°æ®é›†æ”¯æŒè¯¥æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> VAPçš„å¼ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å’Œå¯¹å„ç§ä¸‹æ¸¸åº”ç”¨çš„æ”¯æŒæ ‡å¿—ç€å‘é€šç”¨å¯æ§è§†é¢‘ç”Ÿæˆè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œå…¶å³æ’å³ç”¨æ¶æ„é˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘å’ŒåŸºå‡†æ•°æ®é›†ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Unified, generalizable semantic control in video generation remains a
critical open challenge. Existing methods either introduce artifacts by
enforcing inappropriate pixel-wise priors from structure-based controls, or
rely on non-generalizable, condition-specific finetuning or task-specific
architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes
this problem as in-context generation. VAP leverages a reference video as a
direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via
a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture
prevents catastrophic forgetting and is guided by a temporally biased position
embedding that eliminates spurious mapping priors for robust context retrieval.
To power this approach and catalyze future research, we built VAP-Data, the
largest dataset for semantic-controlled video generation with over 100K paired
videos across 100 semantic conditions. As a single unified model, VAP sets a
new state-of-the-art for open-source methods, achieving a 38.7% user preference
rate that rivals leading condition-specific commercial models. VAP's strong
zero-shot generalization and support for various downstream applications mark a
significant advance toward general-purpose, controllable video generation.</p>
<h3 id="5-generative-point-tracking-with-flow-matching">[5] <a href="https://arxiv.org/abs/2510.20951">Generative Point Tracking with Flow Matching</a></h3>
<p><em>Mattie Tesfaldet, Adam W. Harley, Konstantinos G. Derpanis, Derek Nowrouzezahrai, Christopher Pal</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ç”Ÿæˆå¼ç‚¹è½¨è¿¹è·Ÿè¸ªå™¨ï¼ˆGenPTï¼‰ï¼Œé€šè¿‡ç”Ÿæˆå¼å»ºæ¨¡æ–¹æ³•è§£å†³è§†é¢‘ç‚¹è·Ÿè¸ªä¸­çš„å¤šæ¨¡æ€ä¸ç¡®å®šæ€§ï¼Œåœ¨é®æŒ¡ç‚¹è·Ÿè¸ªæ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å¯è§ç‚¹è·Ÿè¸ªä¸Šä¿æŒç«äº‰åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æœ€å…ˆè¿›çš„åˆ¤åˆ«å¼ç‚¹è·Ÿè¸ªæ¨¡å‹åœ¨å­˜åœ¨ä¸ç¡®å®šæ€§æ—¶åªèƒ½å›å½’åˆ°å‡å€¼æˆ–ä¼—æ•°ï¼Œæ— æ³•æ•æ‰è½¨è¿¹çš„å¤šæ¨¡æ€ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰é®æŒ¡å’Œå¤–è§‚å˜åŒ–å¯¼è‡´çš„ä¸ç¡®å®šæ€§æƒ…å†µä¸‹å­˜åœ¨å±€é™ã€‚</p>
<p><strong>Method:</strong> GenPTé‡‡ç”¨åŸºäºæµåŒ¹é…çš„ç”Ÿæˆå¼æ¡†æ¶ï¼Œç»“åˆåˆ¤åˆ«å¼è·Ÿè¸ªå™¨çš„è¿­ä»£ä¼˜åŒ–ã€è·¨çª—å£ä¸€è‡´æ€§çš„çª—å£ç›¸å…³å…ˆéªŒï¼Œä»¥åŠä¸“é—¨ä¸ºç‚¹åæ ‡è°ƒæ•´çš„æ–¹å·®è°ƒåº¦ï¼Œå¹¶åœ¨æ¨ç†æ—¶ä½¿ç”¨åŸºäºæ¨¡å‹ç½®ä¿¡åº¦çš„æœ€ä½³ä¼˜å…ˆæœç´¢ç­–ç•¥ã€‚</p>
<p><strong>Result:</strong> åœ¨PointOdysseyã€Dynamic Replicaå’ŒTAP-VidåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGenPTåœ¨é®æŒ¡ç‚¹è·Ÿè¸ªç²¾åº¦ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ï¼ŒåŒæ—¶åœ¨å¯è§ç‚¹è·Ÿè¸ªä¸Šä¿æŒä¸ç°æœ‰åˆ¤åˆ«å¼è·Ÿè¸ªå™¨ç›¸å½“çš„ç«äº‰åŠ›ï¼Œå¹¶åœ¨ä¸“é—¨è®¾è®¡çš„é®æŒ¡å¢å¼ºç‰ˆTAP-Vidä¸Šå±•ç¤ºäº†å¤šæ¨¡æ€æ•æ‰èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ç”Ÿæˆå¼æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå»ºæ¨¡ç‚¹è½¨è¿¹çš„å¤šæ¨¡æ€ä¸ç¡®å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é®æŒ¡åœºæ™¯ä¸‹æ˜¾è‘—æå‡è·Ÿè¸ªæ€§èƒ½ï¼Œä¸ºè§†é¢‘ç‚¹è·Ÿè¸ªé¢†åŸŸæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œè¡¨æ˜ç”Ÿæˆå¼æ¡†æ¶åœ¨è§£å†³è§†è§‰ä¸ç¡®å®šæ€§é—®é¢˜ä¸Šå…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Tracking a point through a video can be a challenging task due to uncertainty
arising from visual obfuscations, such as appearance changes and occlusions.
Although current state-of-the-art discriminative models excel in regressing
long-term point trajectory estimates -- even through occlusions -- they are
limited to regressing to a mean (or mode) in the presence of uncertainty, and
fail to capture multi-modality. To overcome this limitation, we introduce
Generative Point Tracker (GenPT), a generative framework for modelling
multi-modal trajectories. GenPT is trained with a novel flow matching
formulation that combines the iterative refinement of discriminative trackers,
a window-dependent prior for cross-window consistency, and a variance schedule
tuned specifically for point coordinates. We show how our model's generative
capabilities can be leveraged to improve point trajectory estimates by
utilizing a best-first search strategy on generated samples during inference,
guided by the model's own confidence of its predictions. Empirically, we
evaluate GenPT against the current state of the art on the standard
PointOdyssey, Dynamic Replica, and TAP-Vid benchmarks. Further, we introduce a
TAP-Vid variant with additional occlusions to assess occluded point tracking
performance and highlight our model's ability to capture multi-modality. GenPT
is capable of capturing the multi-modality in point trajectories, which
translates to state-of-the-art tracking accuracy on occluded points, while
maintaining competitive tracking accuracy on visible points compared to extant
discriminative point trackers.</p>
<h3 id="6-3dreasonknee-advancing-grounded-reasoning-in-medical-vision-language-models">[6] <a href="https://arxiv.org/abs/2510.20967">3DReasonKnee: Advancing Grounded Reasoning in Medical Vision Language Models</a></h3>
<p><em>Sraavya Sambara, Sung Eun Kim, Xiaoman Zhang, Luyang Luo, Shreya Johri, Mohammed Baharoon, Du Hyun Ro, Pranav Rajpurkar</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†3DReasonKneeï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºåŒ»å­¦å›¾åƒçš„3DåŸºç¡€æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª7,970ä¸ª3Dè†å…³èŠ‚MRIä½“ç§¯çš„494ké«˜è´¨é‡äº”å…ƒç»„ï¼Œæ—¨åœ¨è§£å†³å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨3DåŒ»å­¦å›¾åƒä¸­å®šä½è§£å‰–åŒºåŸŸå¹¶è¿›è¡Œé€æ­¥æ¨ç†çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨3DåŒ»å­¦å›¾åƒä¸­éš¾ä»¥å®šä½è§£å‰–åŒºåŸŸå¹¶è¿›è¡Œé€æ­¥æ¨ç†ï¼Œè¿™æ˜¯çœŸå®ä¸–ç•Œè¯Šæ–­è¯„ä¼°çš„å…³é”®è¦æ±‚ã€‚ç°æœ‰3Dæ•°æ®é›†æä¾›å®šä½æ ‡ç­¾ï¼Œä½†éƒ½ä¸æ”¯æŒè¿™ç§"åŸºç¡€æ¨ç†"èƒ½åŠ›ï¼Œæ— æ³•ä¸ä¸´åºŠåŒ»ç”Ÿçš„è¯Šæ–­å·¥ä½œæµç¨‹å¯¹é½ï¼Œé™åˆ¶äº†å¯ä¿¡èµ–çš„ä¸´åºŠåŒ»ç”Ÿ-AIåä½œã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†3DReasonKneeæ•°æ®é›†ï¼Œæ¯ä¸ªäº”å…ƒç»„åŒ…å«3D MRIä½“ç§¯ã€é’ˆå¯¹ç‰¹å®šè§£å‰–åŒºåŸŸçš„è¯Šæ–­é—®é¢˜ã€å®šä½ç›¸å…³è§£å‰–ç»“æ„çš„3Dè¾¹ç•Œæ¡†ã€ä¸´åºŠåŒ»ç”Ÿç”Ÿæˆçš„è¯¦ç»†3Dæ¨ç†è¿‡ç¨‹çš„è¯Šæ–­æ¨ç†æ­¥éª¤ï¼Œä»¥åŠç›¸å…³è§£å‰–åŒºåŸŸçš„ç»“æ„åŒ–ä¸¥é‡ç¨‹åº¦è¯„ä¼°ã€‚æ•°æ®é›†åˆ›å»ºæ¶‰åŠ450å¤šä¸ªå°æ—¶çš„ä¸“å®¶ä¸´åºŠåŒ»ç”Ÿæ—¶é—´è¿›è¡Œæ‰‹åŠ¨åˆ†å‰²å’Œç”Ÿæˆæ¨ç†é“¾ã€‚</p>
<p><strong>Result:</strong> å»ºç«‹äº†ReasonKnee-Benchè¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°å®šä½å’Œè¯Šæ–­å‡†ç¡®æ€§ï¼Œä¸ºäº”ä¸ªæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†åŸºå‡†æ€§èƒ½ï¼Œä½œä¸ºReasonKnee-Benchçš„åŸºçº¿è¡¨ç°ï¼Œæä¾›äº†å…³äºVLMåœ¨ä¸åŒè§£å‰–åŒºåŸŸå’Œè¯Šæ–­æŸ¥è¯¢ä¸­æ‰§è¡ŒåŸºç¡€å’Œä¸¥é‡ç¨‹åº¦è¯„ä¼°èƒ½åŠ›çš„æ·±å…¥è§è§£ã€‚</p>
<p><strong>Conclusion:</strong> 3DReasonKneeä½œä¸ºéª¨ç§‘å¤–ç§‘åŒ»ç”Ÿè¯Šæ–­ä¸“ä¸šçŸ¥è¯†çš„ç‹¬ç‰¹èµ„æºåº“ï¼Œä¸ºæ¨è¿›å¤šæ¨¡æ€åŒ»å­¦AIç³»ç»Ÿå‘3Dã€ä¸´åºŠå¯¹é½çš„å±€éƒ¨å†³ç­–èƒ½åŠ›æä¾›äº†é‡è¦æµ‹è¯•å¹³å°ï¼Œé€šè¿‡æä¾›ä¸“å®¶æ³¨é‡Šçš„3Dæ¨ç†è·¯å¾„ï¼Œä¿ƒè¿›äº†ä¸´åºŠåŒ»ç”Ÿ-AIåä½œçš„å¯ä¿¡åº¦ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Current Vision-Language Models (VLMs) struggle to ground anatomical regions
in 3D medical images and reason about them in a step-by-step manner, a key
requirement of real-world diagnostic assessment. This ability is essential for
aligning model outputs with the diagnostic workflows clinicians use in
practice, enabling trustworthy clinician-AI collaboration. Existing 3D datasets
provide localization labels, but none support this "grounded reasoning"
ability. To address this gap, we introduce 3DReasonKnee, the first 3D grounded
reasoning dataset for medical images, which provides 494k high-quality
quintuples derived from 7,970 3D knee MRI volumes. Each quintuple includes: (1)
the 3D MRI volume, (2) a diagnostic question targeting a specific anatomical
region (3) a 3D bounding box localizing the relevant anatomical structures, (4)
clinician-generated diagnostic reasoning steps that explicitly detail the 3D
reasoning process, and (5) structured severity assessments for the relevant
anatomical region. The creation and validation of 3DReasonKnee, involving over
450 hours of expert clinician time for manually segmenting MRIs and generating
reasoning chains, ensures its superior quality and clinical relevance. We
establish ReasonKnee-Bench to evaluate localization and diagnostic accuracy,
providing insight into VLM ability to perform grounding and severity assessment
across anatomical regions and diagnostic inquiries. We benchmark five
state-of-the-art VLMs, providing baseline performance for ReasonKnee-Bench. By
providing this unique resource of expert-annotated 3D reasoning pathways,
3DReasonKnee serves as a repository of orthopedic surgeons' diagnostic
expertise and offers a vital testbed for advancing multimodal medical AI
systems towards 3D, clinically aligned, localized decision-making capabilities.
The dataset can be found in:
https://huggingface.co/datasets/rajpurkarlab/3DReasonKnee</p>
<h3 id="7-zing-3d-zero-shot-incremental-3d-scene-graphs-via-vision-language-models">[7] <a href="https://arxiv.org/abs/2510.21069">ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models</a></h3>
<p><em>Pranav Saxena, Jimmy Chiun</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>ZING-3Dæ˜¯ä¸€ä¸ªé›¶æ ·æœ¬3Dåœºæ™¯å›¾ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å®ç°å¼€æ”¾è¯æ±‡è¯†åˆ«ï¼Œæ”¯æŒå¢é‡æ›´æ–°å’Œ3Då‡ ä½•æ¥åœ°ï¼Œé€‚ç”¨äºæœºå™¨äººä¸‹æ¸¸åº”ç”¨ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰3Dåœºæ™¯å›¾ç”Ÿæˆæ–¹æ³•ä¸»è¦å±€é™äºå•è§†è§’è®¾ç½®ï¼Œæ— æ³•æ”¯æŒæ–°è§‚æµ‹çš„å¢é‡æ›´æ–°ï¼Œä¸”ç¼ºä¹3Dç©ºé—´çš„æ˜¾å¼å‡ ä½•æ¥åœ°ï¼Œè¿™äº›é™åˆ¶å¯¹äºå…·èº«åœºæ™¯åº”ç”¨è‡³å…³é‡è¦ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åˆ©ç”¨VLMæ¨ç†ç”Ÿæˆä¸°å¯Œçš„2Dåœºæ™¯å›¾ï¼Œå¹¶ä½¿ç”¨æ·±åº¦ä¿¡æ¯å°†å…¶æ¥åœ°åˆ°3Dç©ºé—´ï¼ŒèŠ‚ç‚¹è¡¨ç¤ºå…·æœ‰ç‰¹å¾ã€3Dä½ç½®å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡çš„å¼€æ”¾è¯æ±‡å¯¹è±¡ï¼Œè¾¹æ•è·ç©ºé—´å’Œè¯­ä¹‰å…³ç³»åŠç‰©ä½“é—´è·ç¦»ã€‚</p>
<p><strong>Result:</strong> åœ¨Replicaå’ŒHM3Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒZING-3Dèƒ½å¤Ÿæœ‰æ•ˆæ•è·ç©ºé—´å’Œå…³ç³»çŸ¥è¯†ï¼Œæ— éœ€ä»»åŠ¡ç‰¹å®šè®­ç»ƒå³å¯å®ç°é«˜è´¨é‡çš„åœºæ™¯ç†è§£ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹åœ¨3Dåœºæ™¯ç†è§£ä¸­çš„å¼ºå¤§æ½œåŠ›ï¼Œä¸ºé›¶æ ·æœ¬3Dåœºæ™¯å›¾ç”Ÿæˆæä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œå¹¶ä¸ºæœºå™¨äººåº”ç”¨ä¸­çš„å®æ—¶ç¯å¢ƒç†è§£å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Understanding and reasoning about complex 3D environments requires structured
scene representations that capture not only objects but also their semantic and
spatial relationships. While recent works on 3D scene graph generation have
leveraged pretrained VLMs without task-specific fine-tuning, they are largely
confined to single-view settings, fail to support incremental updates as new
observations arrive and lack explicit geometric grounding in 3D space, all of
which are essential for embodied scenarios. In this paper, we propose, ZING-3D,
a framework that leverages the vast knowledge of pretrained foundation models
to enable open-vocabulary recognition and generate a rich semantic
representation of the scene in a zero-shot manner while also enabling
incremental updates and geometric grounding in 3D space, making it suitable for
downstream robotics applications. Our approach leverages VLM reasoning to
generate a rich 2D scene graph, which is grounded in 3D using depth
information. Nodes represent open-vocabulary objects with features, 3D
locations, and semantic context, while edges capture spatial and semantic
relations with inter-object distances. Our experiments on scenes from the
Replica and HM3D dataset show that ZING-3D is effective at capturing spatial
and relational knowledge without the need of task-specific training.</p>
<h3 id="8-knowledge-driven-vision-language-model-for-plexus-detection-in-hirschsprungs-disease">[8] <a href="https://arxiv.org/abs/2510.21083">Knowledge-Driven Vision-Language Model for Plexus Detection in Hirschsprung's Disease</a></h3>
<p><em>Youssef Megahed, Atallah Madi, Dina El Demellawy, Adrian D. C. Chan</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œå°†ä¸“å®¶çŸ¥è¯†é©±åŠ¨çš„æ–‡æœ¬æ¦‚å¿µé›†æˆåˆ°åŸºäºå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œç”¨äºæŒ‡å¯¼Hirschsprungç—…è‚ è‚Œå±‚ç¥ç»ä¸›åˆ†ç±»ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåˆ†ç±»æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»ŸCNNæ¨¡å‹ï¼Œå®ç°äº†83.9%çš„å‡†ç¡®ç‡ã€86.6%çš„ç²¾ç¡®ç‡å’Œ87.6%çš„ç‰¹å¼‚æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> Hirschsprungç—…çš„è¯Šæ–­å’Œæ²»ç–—éœ€è¦æ¸…æ™°è¯†åˆ«ç»„ç»‡åˆ‡ç‰‡ä¸­è‚Œå±‚ç¥ç»ä¸›çš„ä¸åŒåŒºåŸŸï¼Œä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•å¦‚å·ç§¯ç¥ç»ç½‘ç»œè™½ç„¶åœ¨æ­¤ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†é€šå¸¸è¢«è§†ä¸ºé»‘ç®±æ¨¡å‹ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ï¼Œä¸”ä¸ç¬¦åˆåŒ»ç”Ÿçš„å†³ç­–æ–¹å¼ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–æ¡†æ¶ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸“å®¶æ¥æºï¼ˆå¦‚åŒ»å­¦æ•™ç§‘ä¹¦å’Œè®ºæ–‡ï¼‰çš„æç¤ºè¯ï¼Œç»å›¢é˜Ÿå®¡æ ¸åä½¿ç”¨QuiltNetç¼–ç ï¼Œå°†ä¸´åºŠç›¸å…³è¯­ä¹‰çº¿ç´¢ä¸è§†è§‰ç‰¹å¾å¯¹é½ï¼Œé›†æˆåˆ°åŸºäºå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æŒ‡å¯¼ç¥ç»ä¸›åˆ†ç±»ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨ä¸åŒåˆ†ç±»æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„åˆ¤åˆ«èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºåŸºäºCNNçš„æ¨¡å‹ï¼ˆåŒ…æ‹¬VGG-19ã€ResNet-18å’ŒResNet-50ï¼‰ï¼Œå®ç°äº†83.9%çš„å‡†ç¡®ç‡ã€86.6%çš„ç²¾ç¡®ç‡å’Œ87.6%çš„ç‰¹å¼‚æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¿™äº›å‘ç°å‡¸æ˜¾äº†å¤šæ¨¡æ€å­¦ä¹ åœ¨ç»„ç»‡ç—…ç†å­¦ä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒäº†æ•´åˆä¸“å®¶çŸ¥è¯†ä»¥è·å¾—æ›´å…·ä¸´åºŠç›¸å…³æ€§çš„æ¨¡å‹è¾“å‡ºçš„ä»·å€¼ï¼Œä¸ºå¼€å‘æ›´ç¬¦åˆä¸´åºŠå®è·µçš„å¯è§£é‡ŠAIç³»ç»Ÿæä¾›äº†é‡è¦æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Hirschsprung's disease is defined as the congenital absence of ganglion cells
in some segment(s) of the colon. The muscle cannot make coordinated movements
to propel stool in that section, most commonly leading to obstruction. The
diagnosis and treatment for this disease require a clear identification of
different region(s) of the myenteric plexus, where ganglion cells should be
present, on the microscopic view of the tissue slide. While deep learning
approaches, such as Convolutional Neural Networks, have performed very well in
this task, they are often treated as black boxes, with minimal understanding
gained from them, and may not conform to how a physician makes decisions. In
this study, we propose a novel framework that integrates expert-derived textual
concepts into a Contrastive Language-Image Pre-training-based vision-language
model to guide plexus classification. Using prompts derived from expert sources
(e.g., medical textbooks and papers) generated by large language models and
reviewed by our team before being encoded with QuiltNet, our approach aligns
clinically relevant semantic cues with visual features. Experimental results
show that the proposed model demonstrated superior discriminative capability
across different classification metrics as it outperformed CNN-based models,
including VGG-19, ResNet-18, and ResNet-50; achieving an accuracy of 83.9%, a
precision of 86.6%, and a specificity of 87.6%. These findings highlight the
potential of multi-modal learning in histopathology and underscore the value of
incorporating expert knowledge for more clinically relevant model outputs.</p>
<h3 id="9-physvlm-avr-active-visual-reasoning-for-multimodal-large-language-models-in-physical-environments">[9] <a href="https://arxiv.org/abs/2510.21111">PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments</a></h3>
<p><em>Weijie Zhou, Xuantang Xiong, Yi Peng, Manli Tao, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸»åŠ¨è§†è§‰æ¨ç†ä»»åŠ¡ï¼Œå°†è§†è§‰æ¨ç†æ‰©å±•åˆ°éƒ¨åˆ†å¯è§‚å¯Ÿçš„äº¤äº’ç¯å¢ƒä¸­ï¼Œå¹¶å¼€å‘äº†PhysVLM-AVRæ¨¡å‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸»åŠ¨è·å–å’Œæ•´åˆä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›ä¸è¶³ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†ç ”ç©¶ä¸»è¦å±€é™äºé™æ€ã€å®Œå…¨å¯è§‚å¯Ÿçš„ç¯å¢ƒï¼Œè¿™ä¸ç°å®ä¸–ç•Œä¸­ä¿¡æ¯å¸¸å› é®æŒ¡æˆ–è§†é‡é™åˆ¶è€Œä¸å®Œæ•´çš„å®é™…æƒ…å†µå­˜åœ¨å·®è·ã€‚äººç±»é€šè¿‡ä¸»åŠ¨æ¢ç´¢å’Œäº¤äº’æ¥æ”¶é›†ä¿¡æ¯çš„é—­ç¯è¿‡ç¨‹å¯å‘äº†æœ¬ç ”ç©¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿç¯å¢ƒä¸­è¿›è¡Œä¸»åŠ¨æ¨ç†çš„èƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†ä¸»åŠ¨è§†è§‰æ¨ç†ä»»åŠ¡ï¼Œå¹¶å¼€å‘äº†CLEVR-AVRä»¿çœŸåŸºå‡†æ¥è¯„ä¼°æ¨ç†æ­£ç¡®æ€§å’Œä¿¡æ¯æ”¶é›†æ•ˆç‡ã€‚æ„å»ºäº†åŒ…å«15.2ä¸‡æ ·æœ¬çš„AVR-152kæ•°æ®é›†ï¼Œæä¾›ä¸°å¯Œçš„æ€ç»´é“¾æ ‡æ³¨ï¼Œæ”¯æŒä¸ç¡®å®šæ€§è¯†åˆ«ã€åŠ¨ä½œæ¡ä»¶ä¿¡æ¯å¢ç›Šé¢„æµ‹å’Œä¿¡æ¯æœ€å¤§åŒ–åŠ¨ä½œé€‰æ‹©ã€‚åŸºäºæ­¤å¼€å‘äº†PhysVLM-AVRæ¨¡å‹ï¼Œåœ¨é«˜çº§é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ä¸­è®­ç»ƒæ™ºèƒ½ä½“ã€‚</p>
<p><strong>Result:</strong> PhysVLM-AVRåœ¨CLEVR-AVRåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å…·èº«æ¨ç†å’Œè¢«åŠ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ã€‚åˆ†æå‘ç°å½“å‰å…·èº«å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è™½ç„¶èƒ½å¤Ÿæ£€æµ‹ä¿¡æ¯ä¸å®Œæ•´æ€§ï¼Œä½†åœ¨é€šè¿‡äº¤äº’ä¸»åŠ¨è·å–å’Œæ•´åˆæ–°ä¿¡æ¯æ–¹é¢å­˜åœ¨æ˜¾è‘—å›°éš¾ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸»åŠ¨æ¨ç†èƒ½åŠ›ä¸Šçš„æ ¹æœ¬æ€§å·®è·ï¼Œå¼ºè°ƒäº†åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿç¯å¢ƒä¸­è¿›è¡Œé—­ç¯æ„ŸçŸ¥-æ¨ç†-è¡ŒåŠ¨é›†æˆçš„é‡è¦æ€§ã€‚æå‡ºçš„ä¸»åŠ¨è§†è§‰æ¨ç†æ¡†æ¶ä¸ºå¼€å‘æ›´æ¥è¿‘äººç±»è®¤çŸ¥èƒ½åŠ›çš„AIç³»ç»Ÿæä¾›äº†æ–°çš„æ–¹å‘å’Œè¯„ä¼°æ ‡å‡†ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Visual reasoning in multimodal large language models (MLLMs) has primarily
been studied in static, fully observable settings, limiting their effectiveness
in real-world environments where information is often incomplete due to
occlusion or limited field of view. Humans, in contrast, actively explore and
interact with their environment-moving, examining, and manipulating objects-to
gather information through a closed-loop process integrating perception,
reasoning, and action. Inspired by this human capability, we introduce the
Active Visual Reasoning (AVR) task, extending visual reasoning to partially
observable, interactive environments. AVR necessitates agents to: (1) actively
acquire information via sequential physical actions, (2) integrate observations
across multiple steps for coherent reasoning, and (3) dynamically adjust
decisions based on evolving visual feedback. To rigorously evaluate AVR, we
introduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive
environments designed to assess both reasoning correctness and
information-gathering efficiency. We present AVR-152k, a large-scale dataset
that offers rich Chain-of-Thought (CoT) annotations detailing iterative
reasoning for uncertainty identification, action-conditioned information gain
prediction, and information-maximizing action selection, crucial for training
agents in a higher-order Markov Decision Process. Building on this, we develop
PhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR,
embodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath,
Geometry30K). Our analysis also reveals that current embodied MLLMs, despite
detecting information incompleteness, struggle to actively acquire and
integrate new information through interaction, highlighting a fundamental gap
in active reasoning capabilities.</p>
<h3 id="10-safetypairs-isolating-safety-critical-image-features-with-counterfactual-image-generation">[10] <a href="https://arxiv.org/abs/2510.21120">SafetyPairs: Isolating Safety Critical Image Features with Counterfactual Image Generation</a></h3>
<p><em>Alec Helbling, Shruti Palaskar, Kundan Krishna, Polo Chau, Leon Gatys, Joseph Yitan Cheng</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SafetyPairsæ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆä»…å®‰å…¨ç›¸å…³ç‰¹å¾ä¸åŒçš„åäº‹å®å›¾åƒå¯¹æ¥ç³»ç»Ÿç ”ç©¶å›¾åƒå®‰å…¨æ€§ï¼Œæ„å»ºäº†åŒ…å«3020å¯¹å›¾åƒçš„å®‰å…¨åŸºå‡†ï¼Œæ­ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç»†å¾®å®‰å…¨åŒºåˆ†ä¸Šçš„å¼±ç‚¹ï¼Œå¹¶å±•ç¤ºäº†è¯¥æ¡†æ¶ä½œä¸ºæ•°æ®å¢å¼ºç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å›¾åƒå®‰å…¨æ•°æ®é›†å­˜åœ¨ç²—ç³™å’Œæ¨¡ç³Šçš„é—®é¢˜ï¼Œä»…æä¾›å¹¿æ³›çš„å®‰å…¨æ ‡ç­¾è€Œæœªéš”ç¦»é©±åŠ¨å®‰å…¨å·®å¼‚çš„å…·ä½“ç‰¹å¾ï¼Œéš¾ä»¥ç³»ç»ŸåŒºåˆ†è‰¯æ€§å›¾åƒå’Œé—®é¢˜å›¾åƒï¼Œç‰¹åˆ«æ˜¯å½“å›¾åƒçš„ç»†å¾®å˜åŒ–ï¼ˆå¦‚ä¾®è¾±æ€§æ‰‹åŠ¿æˆ–ç¬¦å·ï¼‰ä¼šå½»åº•æ”¹å˜å…¶å®‰å…¨å«ä¹‰æ—¶ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†SafetyPairså¯æ‰©å±•æ¡†æ¶ï¼Œåˆ©ç”¨å›¾åƒç¼–è¾‘æ¨¡å‹å¯¹å›¾åƒè¿›è¡Œé’ˆå¯¹æ€§ä¿®æ”¹ï¼Œä»…æ”¹å˜ä¸ç»™å®šå®‰å…¨ç­–ç•¥ç›¸å…³çš„ç‰¹å¾ä»è€Œç¿»è½¬å®‰å…¨æ ‡ç­¾ï¼ŒåŒæ—¶ä¿æŒå®‰å…¨æ— å…³ç»†èŠ‚ä¸å˜ï¼Œæ„å»ºäº†æ¶µç›–9ä¸ªå®‰å…¨ç±»åˆ«çš„å¤šæ ·åŒ–åˆ†ç±»ä½“ç³»ã€‚</p>
<p><strong>Result:</strong> æ„å»ºäº†åŒ…å«3020å¯¹SafetyPairå›¾åƒçš„æ–°å®‰å…¨åŸºå‡†ï¼Œè¯¥åŸºå‡†ä½œä¸ºå¼ºå¤§çš„è¯„ä¼°æ•°æ®æºï¼Œçªæ˜¾äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒºåˆ†ç»†å¾®ä¸åŒå›¾åƒèƒ½åŠ›ä¸Šçš„å¼±ç‚¹ï¼›åŒæ—¶å‘ç°è¯¥æµæ°´çº¿ä½œä¸ºæ•°æ®å¢å¼ºç­–ç•¥èƒ½æœ‰æ•ˆæé«˜è½»é‡çº§é˜²æŠ¤æ¨¡å‹çš„æ ·æœ¬æ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æä¾›äº†é¦–ä¸ªç³»ç»Ÿç ”ç©¶ç»†ç²’åº¦å›¾åƒå®‰å…¨åŒºåˆ†çš„èµ„æºï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨å®‰å…¨æ•æ„Ÿç‰¹å¾è¯†åˆ«ä¸Šçš„å±€é™æ€§ï¼Œæå‡ºçš„æ¡†æ¶ä¸ä»…å¯ç”¨äºæ¨¡å‹è¯„ä¼°ï¼Œè¿˜èƒ½ä½œä¸ºæœ‰æ•ˆçš„æ•°æ®å¢å¼ºæ–¹æ³•æå‡å®‰å…¨é˜²æŠ¤æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>What exactly makes a particular image unsafe? Systematically differentiating
between benign and problematic images is a challenging problem, as subtle
changes to an image, such as an insulting gesture or symbol, can drastically
alter its safety implications. However, existing image safety datasets are
coarse and ambiguous, offering only broad safety labels without isolating the
specific features that drive these differences. We introduce SafetyPairs, a
scalable framework for generating counterfactual pairs of images, that differ
only in the features relevant to the given safety policy, thus flipping their
safety label. By leveraging image editing models, we make targeted changes to
images that alter their safety labels while leaving safety-irrelevant details
unchanged. Using SafetyPairs, we construct a new safety benchmark, which serves
as a powerful source of evaluation data that highlights weaknesses in
vision-language models' abilities to distinguish between subtly different
images. Beyond evaluation, we find our pipeline serves as an effective data
augmentation strategy that improves the sample efficiency of training
lightweight guard models. We release a benchmark containing over 3,020
SafetyPair images spanning a diverse taxonomy of 9 safety categories, providing
the first systematic resource for studying fine-grained image safety
distinctions.</p>
<h3 id="11-noisygrpo-incentivizing-multimodal-cot-reasoning-via-noise-injection-and-bayesian-estimation">[11] <a href="https://arxiv.org/abs/2510.21122">NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation</a></h3>
<p><em>Longtian Qiu, Shan Ning, Jiaxuan Sun, Xuming He</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†NoisyGRPOï¼Œä¸€ç§ç³»ç»Ÿæ€§çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å‘è§†è§‰è¾“å…¥æ³¨å…¥å¯æ§å™ªå£°å¢å¼ºæ¢ç´¢ï¼Œå¹¶é‡‡ç”¨è´å¶æ–¯æ¡†æ¶æ˜¾å¼å»ºæ¨¡ä¼˜åŠ¿ä¼°è®¡è¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨é“¾å¼æ€ç»´æ¨ç†ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é€šç”¨é“¾å¼æ€ç»´æ¨ç†èƒ½åŠ›æ—¶ï¼Œå¾€å¾€éš¾ä»¥è¶…è¶Šè®­ç»ƒåˆ†å¸ƒçš„æ³›åŒ–ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨å®é™…å¤æ‚è§†è§‰åœºæ™¯ä¸­çš„åº”ç”¨æ•ˆæœå’Œç¨³å®šæ€§ã€‚</p>
<p><strong>Method:</strong> NoisyGRPOæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯ï¼šå™ªå£°æ³¨å…¥æ¢ç´¢ç­–ç•¥é€šè¿‡åœ¨è§†è§‰è¾“å…¥ä¸­æ·»åŠ é«˜æ–¯å™ªå£°æ¥é¼“åŠ±æ¨¡å‹åœ¨æ›´å¹¿æ³›çš„è§†è§‰åœºæ™¯ä¸­è¿›è¡Œæ¢ç´¢ï¼›è´å¶æ–¯ä¼˜åŠ¿ä¼°è®¡å°†ä¼˜åŠ¿ä¼°è®¡å»ºæ¨¡ä¸ºè´å¶æ–¯æ¨ç†é—®é¢˜ï¼Œå…¶ä¸­æ³¨å…¥çš„å™ªå£°æ°´å¹³ä½œä¸ºå…ˆéªŒï¼Œè§‚å¯Ÿåˆ°çš„è½¨è¿¹å¥–åŠ±ä½œä¸ºä¼¼ç„¶ï¼Œé€šè¿‡èåˆä¸¤ç§ä¿¡æ¯æºè®¡ç®—è½¨è¿¹ä¼˜åŠ¿çš„ç¨³å¥åéªŒä¼°è®¡ã€‚</p>
<p><strong>Result:</strong> åœ¨æ ‡å‡†é“¾å¼æ€ç»´è´¨é‡ã€é€šç”¨èƒ½åŠ›å’Œå¹»è§‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒNoisyGRPOæ˜¾è‘—æå‡äº†æ³›åŒ–æ€§å’Œé²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å°è§„æ¨¡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Qwen2.5-VL 3Bï¼‰çš„å¼ºåŒ–å­¦ä¹ è®¾ç½®ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡ç³»ç»Ÿæ€§åœ°ç»“åˆå™ªå£°æ³¨å…¥å’Œè´å¶æ–¯å»ºæ¨¡ï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ ä¸­çš„æ³›åŒ–æŒ‘æˆ˜ï¼Œä¸ºæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå®é™…åº”ç”¨æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’Œç†è®ºæ”¯æ’‘ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Reinforcement learning (RL) has shown promise in enhancing the general
Chain-of-Thought (CoT) reasoning capabilities of multimodal large language
models (MLLMs). However, when applied to improve general CoT reasoning,
existing RL frameworks often struggle to generalize beyond the training
distribution. To address this, we propose NoisyGRPO, a systematic multimodal RL
framework that introduces controllable noise into visual inputs for enhanced
exploration and explicitly models the advantage estimation process via a
Bayesian framework. Specifically, NoisyGRPO improves RL training by: (1)
\textbf{Noise-Injected Exploration Policy}: Perturbing visual inputs with
Gaussian noise to encourage exploration across a wider range of visual
scenarios; and (2) \textbf{Bayesian Advantage Estimation}: Formulating
advantage estimation as a principled Bayesian inference problem, where the
injected noise level serves as a prior and the observed trajectory reward as
the likelihood. This Bayesian modeling fuses both sources of information to
compute a robust posterior estimate of trajectory advantage, effectively
guiding MLLMs to prefer visually grounded trajectories over noisy ones.
Experiments on standard CoT quality, general capability, and hallucination
benchmarks demonstrate that NoisyGRPO substantially improves generalization and
robustness, especially in RL settings with small-scale MLLMs such as Qwen2.5-VL
3B. The project page is available at
\href{https://artanic30.github.io/project_pages/NoisyGRPO/}{\texttt{https://artanic30.github.io/project_pages/NoisyGRPO}}.</p>
<h3 id="12-ct-clip-a-multi-modal-fusion-framework-for-robust-apple-leaf-disease-recognition-in-complex-environments">[12] <a href="https://arxiv.org/abs/2510.21346">CT-CLIP: A Multi-modal Fusion Framework for Robust Apple Leaf Disease Recognition in Complex Environments</a></h3>
<p><em>Lemin Liu, Fangchao Hu, Honghua Jiang, Yaru Chen, Limin Liu, Yongliang Qiao</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºCT-CLIPçš„å¤šåˆ†æ”¯è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡ååŒä½¿ç”¨CNNå’ŒVision Transformerå¹¶ç»“åˆCLIPçš„å¤šæ¨¡æ€å­¦ä¹ ï¼Œæœ‰æ•ˆè§£å†³äº†å¤æ‚æœå›­ç¯å¢ƒä¸­è‹¹æœå¶éƒ¨ç—…å®³è¡¨å‹å¼‚è´¨æ€§å¸¦æ¥çš„è¯†åˆ«æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨å¤æ‚æœå›­ç¯å¢ƒä¸­ï¼Œä¸åŒè‹¹æœå¶éƒ¨ç—…å®³çš„è¡¨å‹å¼‚è´¨æ€§è¡¨ç°ä¸ºç—…å˜åŒºåŸŸçš„æ˜¾è‘—å˜å¼‚ï¼Œè¿™å¯¹ä¼ ç»Ÿçš„å¤šå°ºåº¦ç‰¹å¾èåˆæ–¹æ³•æ„æˆäº†æŒ‘æˆ˜ã€‚è¿™äº›æ–¹æ³•ä»…æ•´åˆå·ç§¯ç¥ç»ç½‘ç»œæå–çš„å¤šå±‚ç‰¹å¾ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘å±€éƒ¨ä¸å…¨å±€ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶æå‡ºäº†CNN-Transformer-CLIPï¼ˆCT-CLIPï¼‰å¤šåˆ†æ”¯è¯†åˆ«æ¡†æ¶ï¼ŒååŒä½¿ç”¨CNNæå–å±€éƒ¨ç—…å˜ç»†èŠ‚ç‰¹å¾å’ŒVision Transformeræ•æ‰å…¨å±€ç»“æ„å…³ç³»ã€‚è‡ªé€‚åº”ç‰¹å¾èåˆæ¨¡å—ï¼ˆAFFMï¼‰åŠ¨æ€èåˆè¿™äº›ç‰¹å¾ï¼Œå®ç°å±€éƒ¨ä¸å…¨å±€ä¿¡æ¯çš„æœ€ä¼˜è€¦åˆã€‚åŒæ—¶é‡‡ç”¨å¤šæ¨¡æ€å›¾æ–‡å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„CLIPæƒé‡å®ç°è§†è§‰ç‰¹å¾ä¸ç—…å®³è¯­ä¹‰æè¿°çš„æ·±åº¦å¯¹é½ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCT-CLIPåœ¨å…¬å¼€è‹¹æœç—…å®³æ•°æ®é›†å’Œè‡ªå»ºæ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°97.38%å’Œ96.12%çš„å‡†ç¡®ç‡ï¼Œä¼˜äºå¤šä¸ªåŸºçº¿æ–¹æ³•ã€‚è¯¥æ¡†æ¶åœ¨å¤æ‚ç¯å¢ƒæ¡ä»¶ä¸‹æ˜¾è‘—æå‡äº†è¯†åˆ«ç²¾åº¦ã€‚</p>
<p><strong>Conclusion:</strong> æå‡ºçš„CT-CLIPåœ¨å†œä¸šç—…å®³è¯†åˆ«æ–¹é¢å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†å¤æ‚ç¯å¢ƒæ¡ä»¶ä¸‹çš„è¯†åˆ«ç²¾åº¦ï¼Œä¸ºå†œä¸šåº”ç”¨ä¸­çš„è‡ªåŠ¨åŒ–ç—…å®³è¯†åˆ«æä¾›äº†åˆ›æ–°ä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶ä¸ºå¤„ç†è¡¨å‹å¼‚è´¨æ€§ç—…å®³æä¾›äº†æœ‰æ•ˆçš„å¤šæ¨¡æ€èåˆæ–¹æ³•ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>In complex orchard environments, the phenotypic heterogeneity of different
apple leaf diseases, characterized by significant variation among lesions,
poses a challenge to traditional multi-scale feature fusion methods. These
methods only integrate multi-layer features extracted by convolutional neural
networks (CNNs) and fail to adequately account for the relationships between
local and global features. Therefore, this study proposes a multi-branch
recognition framework named CNN-Transformer-CLIP (CT-CLIP). The framework
synergistically employs a CNN to extract local lesion detail features and a
Vision Transformer to capture global structural relationships. An Adaptive
Feature Fusion Module (AFFM) then dynamically fuses these features, achieving
optimal coupling of local and global information and effectively addressing the
diversity in lesion morphology and distribution. Additionally, to mitigate
interference from complex backgrounds and significantly enhance recognition
accuracy under few-shot conditions, this study proposes a multimodal image-text
learning approach. By leveraging pre-trained CLIP weights, it achieves deep
alignment between visual features and disease semantic descriptions.
Experimental results show that CT-CLIP achieves accuracies of 97.38% and 96.12%
on a publicly available apple disease and a self-built dataset, outperforming
several baseline methods. The proposed CT-CLIP demonstrates strong capabilities
in recognizing agricultural diseases, significantly enhances identification
accuracy under complex environmental conditions, provides an innovative and
practical solution for automated disease recognition in agricultural
applications.</p>
<h3 id="13-towards-physics-informed-spatial-intelligence-with-human-priors-an-autonomous-driving-pilot-study">[13] <a href="https://arxiv.org/abs/2510.21160">Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study</a></h3>
<p><em>Guanlin Wu, Boyan Su, Yang Zhao, Pu Wang, Yichen Lin, Hao Frank Yang</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ç©ºé—´æ™ºèƒ½ç½‘æ ¼ï¼ˆSIGï¼‰ï¼Œä¸€ç§ç»“æ„åŒ–ã€åŸºäºç½‘æ ¼çš„è¡¨ç¤ºæ–¹æ³•ï¼Œç”¨äºåœ¨åŸºç¡€æ¨¡å‹ä¸­æ˜¾å¼ç¼–ç ç©ºé—´å…³ç³»ï¼Œå¹¶å¼€å‘äº†SIG-informedè¯„ä¼°æŒ‡æ ‡æ¥é‡åŒ–æ¨¡å‹çš„å†…åœ¨è§†è§‰ç©ºé—´æ™ºèƒ½ï¼Œæœ‰æ•ˆåˆ†ç¦»ç©ºé—´èƒ½åŠ›ä¸è¯­è¨€å…ˆéªŒã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰åŸºç¡€æ¨¡å‹ä¸­ç©ºé—´æ™ºèƒ½çš„é›†æˆä¸éªŒè¯å­˜åœ¨æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨çº¯æ–‡æœ¬æç¤ºå’ŒVQAå¼è¯„åˆ†æ¥ä»£ç†è§†è§‰ç©ºé—´æ™ºèƒ½ï¼Œè¿™æ©ç›–äº†å‡ ä½•ä¿¡æ¯ã€å¼•å…¥äº†è¯­è¨€æ·å¾„ï¼Œå¹¶å‰Šå¼±äº†å¯¹çœŸæ­£ç©ºé—´æŠ€èƒ½çš„å½’å› ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ç©ºé—´æ™ºèƒ½ç½‘æ ¼ï¼ˆSIGï¼‰ä½œä¸ºæ–‡æœ¬çš„è¡¥å……é€šé“ï¼Œè¿™æ˜¯ä¸€ç§ç»“æ„åŒ–ã€åŸºäºç½‘æ ¼çš„è¡¨ç¤ºæ–¹æ³•ï¼Œæ˜¾å¼ç¼–ç ç‰©ä½“å¸ƒå±€ã€ç‰©ä½“é—´å…³ç³»ä»¥åŠç‰©ç†åŸºç¡€å…ˆéªŒï¼Œä¸ºåŸºäºSIGçš„è¯„ä¼°æŒ‡æ ‡æä¾›äº†å¿ å®ã€ç»„åˆå¼çš„åœºæ™¯ç»“æ„è¡¨ç¤ºã€‚</p>
<p><strong>Result:</strong> åœ¨å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­ï¼ŒSIGç›¸æ¯”çº¯VQAè¡¨ç¤ºåœ¨æ‰€æœ‰VSIæŒ‡æ ‡ä¸Šå‡äº§ç”Ÿæ›´ä¸€è‡´ã€æ›´ç¨³å®šä¸”æ›´å…¨é¢çš„æ€§èƒ½æå‡ï¼Œè¡¨æ˜å…¶ä½œä¸ºæ•°æ®æ ‡æ³¨å’Œè®­ç»ƒæ¨¡å¼ç”¨äºå­¦ä¹ è§†è§‰ç©ºé—´æ™ºèƒ½çš„æ½œåŠ›ï¼ŒåŒæ—¶å‘å¸ƒäº†åŒ…å«1.4Ké©¾é©¶å¸§çš„SIGBenchåŸºå‡†æ•°æ®é›†ã€‚</p>
<p><strong>Conclusion:</strong> SIGä¸ºå­¦ä¹ è§†è§‰ç©ºé—´æ™ºèƒ½æä¾›äº†æœ‰å‰æ™¯çš„æ•°æ®æ ‡æ³¨å’Œè®­ç»ƒæ¨¡å¼ï¼Œå…¶ç»“æ„åŒ–è¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆåˆ†ç¦»æ¨¡å‹çš„ç©ºé—´èƒ½åŠ›ä¸è¯­è¨€å…ˆéªŒï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ç­‰åœºæ™¯ä¸­çš„æœºå™¨VSIä»»åŠ¡å’Œäººç±»æ³¨æ„åŠ›é©±åŠ¨çš„VSIä»»åŠ¡æä¾›äº†ç»Ÿä¸€è¯„ä¼°æ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>How to integrate and verify spatial intelligence in foundation models remains
an open challenge. Current practice often proxies Visual-Spatial Intelligence
(VSI) with purely textual prompts and VQA-style scoring, which obscures
geometry, invites linguistic shortcuts, and weakens attribution to genuinely
spatial skills. We introduce Spatial Intelligence Grid (SIG): a structured,
grid-based schema that explicitly encodes object layouts, inter-object
relations, and physically grounded priors. As a complementary channel to text,
SIG provides a faithful, compositional representation of scene structure for
foundation-model reasoning. Building on SIG, we derive SIG-informed evaluation
metrics that quantify a model's intrinsic VSI, which separates spatial
capability from language priors. In few-shot in-context learning with
state-of-the-art multimodal LLMs (e.g. GPT- and Gemini-family models), SIG
yields consistently larger, more stable, and more comprehensive gains across
all VSI metrics compared to VQA-only representations, indicating its promise as
a data-labeling and training schema for learning VSI. We also release SIGBench,
a benchmark of 1.4K driving frames annotated with ground-truth SIG labels and
human gaze traces, supporting both grid-based machine VSI tasks and
attention-driven, human-like VSI tasks in autonomous-driving scenarios.</p>
<h3 id="14-tokenclip-token-wise-prompt-learning-for-zero-shot-anomaly-detection">[14] <a href="https://arxiv.org/abs/2510.21171">TokenCLIP: Token-wise Prompt Learning for Zero-shot Anomaly Detection</a></h3>
<p><em>Qihang Zhou, Binbin Gao, Guansong Pang, Xin Wang, Jiming Chen, Shibo He</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºTokenCLIPï¼Œä¸€ç§åŸºäºæœ€ä¼˜ä¼ è¾“çš„ä»¤ç‰Œçº§è‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å¯¹é½è§†è§‰ä»¤ç‰Œä¸å¯å­¦ä¹ æ–‡æœ¬å­ç©ºé—´æ¥å®ç°ç»†ç²’åº¦å¼‚å¸¸æ£€æµ‹ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å•ä¸€æ–‡æœ¬ç©ºé—´ä¸­æ— æ³•å‡†ç¡®æ•æ‰å¤šæ ·åŒ–å¼‚å¸¸è¯­ä¹‰çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å•ä¸€æ–‡æœ¬ç©ºé—´æ¥å¯¹é½ä¸åŒç‰©ä½“å’Œé¢†åŸŸçš„è§†è§‰è¯­ä¹‰ï¼Œè¿™ç§æ— å·®åˆ«çš„å¯¹é½æ–¹å¼é˜»ç¢äº†æ¨¡å‹å‡†ç¡®æ•æ‰å¤šæ ·åŒ–çš„å¼‚å¸¸è¯­ä¹‰ï¼Œéœ€è¦æ›´ç»†ç²’åº¦çš„å¯¹é½æœºåˆ¶æ¥æå‡å¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> TokenCLIPå°†ä»¤ç‰Œæ— å…³çš„æ–‡æœ¬ç©ºé—´æ‰©å±•ä¸ºä¸€ç»„æ­£äº¤å­ç©ºé—´ï¼Œé€šè¿‡æœ€ä¼˜ä¼ è¾“é—®é¢˜åŠ¨æ€åˆ†é…æ¯ä¸ªè§†è§‰ä»¤ç‰Œåˆ°è¯­ä¹‰ç›¸å…³çš„å­ç©ºé—´ç»„åˆï¼Œå¹¶åº”ç”¨top-kæ©ç æ¥ç¨€ç–åŒ–ä¼ è¾“è®¡åˆ’ï¼Œä½¿ä¸åŒå­ç©ºé—´ä¸“æ³¨äºä¸åŒçš„è§†è§‰åŒºåŸŸè¯­ä¹‰ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¯æ˜äº†TokenCLIPçš„ä¼˜è¶Šæ€§ï¼Œè¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«å’Œå®šä½å„ç§å¼‚å¸¸æ¨¡å¼ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡ä»¤ç‰Œçº§çš„åŠ¨æ€æ–‡æœ¬-è§†è§‰å¯¹é½å¯ä»¥æ˜¾è‘—æå‡å¼‚å¸¸æ£€æµ‹çš„ç»†ç²’åº¦æ€§èƒ½ï¼Œæœ€ä¼˜ä¼ è¾“æ¡†æ¶ä¸ºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è‡ªé€‚åº”æœºåˆ¶ï¼Œä¸ºæœªæ¥ç»†ç²’åº¦è§†è§‰ç†è§£ä»»åŠ¡å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Adapting CLIP for anomaly detection on unseen objects has shown strong
potential in a zero-shot manner. However, existing methods typically rely on a
single textual space to align with visual semantics across diverse objects and
domains. The indiscriminate alignment hinders the model from accurately
capturing varied anomaly semantics. We propose TokenCLIP, a token-wise
adaptation framework that enables dynamic alignment between visual and
learnable textual spaces for fine-grained anomaly learning. Rather than mapping
all visual tokens to a single, token-agnostic textual space, TokenCLIP aligns
each token with a customized textual subspace that represents its visual
characteristics. Explicitly assigning a unique learnable textual space to each
token is computationally intractable and prone to insufficient optimization. We
instead expand the token-agnostic textual space into a set of orthogonal
subspaces, and then dynamically assign each token to a subspace combination
guided by semantic affinity, which jointly supports customized and efficient
token-wise adaptation. To this end, we formulate dynamic alignment as an
optimal transport problem, where all visual tokens in an image are transported
to textual subspaces based on semantic similarity. The transport constraints of
OT ensure sufficient optimization across subspaces and encourage them to focus
on different semantics. Solving the problem yields a transport plan that
adaptively assigns each token to semantically relevant subspaces. A top-k
masking is then applied to sparsify the plan and specialize subspaces for
distinct visual regions. Extensive experiments demonstrate the superiority of
TokenCLIP.</p>
<h3 id="15-granvit-a-fine-grained-vision-model-with-autoregressive-perception-for-mllms">[15] <a href="https://arxiv.org/abs/2510.21501">GranViT: A Fine-Grained Vision Model With Autoregressive Perception For MLLMs</a></h3>
<p><em>Guanghao Zheng, Bowen Shi, Mingxing Xu, Ruoyu Sun, Peisen Zhao, Zhibo Zhang, Wenrui Dai, Junni Zou, Hongkai Xiong, Xiaopeng Zhang, Qi Tian</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºGranViTï¼Œä¸€ç§æ–°é¢–çš„è§†è§‰Transformerï¼Œé€šè¿‡åŒºåŸŸçº§è‡ªå›å½’è®­ç»ƒå°†ç»†ç²’åº¦ç‰¹å¾æå–ä¸å¤§å‹è¯­è¨€æ¨¡å‹è¯­ä¹‰å¯¹é½ç›¸ç»“åˆï¼Œè§£å†³äº†ç°æœ‰è§†è§‰ç¼–ç å™¨åœ¨ç»†ç²’åº¦æ„ŸçŸ¥æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰ç¼–ç å™¨ä¸»è¦å…³æ³¨å…¨å±€å›¾åƒè¡¨ç¤ºè€Œå¿½è§†ç»†ç²’åº¦åŒºåŸŸåˆ†æï¼Œç”±äºç¼ºä¹ç»†ç²’åº¦æ ‡æ³¨æ•°æ®å’Œä¸“é—¨çš„é¢„è®­ç»ƒèŒƒå¼ï¼Œåœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­é™åˆ¶äº†ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›çš„å‘å±•ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†åŒ…å«200ä¸‡å¼ è‡ªç„¶å’ŒOCRå›¾åƒåŠ1.8äº¿é«˜è´¨é‡åŒºåŸŸçº§æ ‡æ³¨çš„Gran-29Mæ•°æ®é›†ï¼Œå¼€å‘äº†é¢„è®­ç»ƒ-é€‚åº”æ¡†æ¶ç»“åˆè‡ªè’¸é¦æœºåˆ¶ï¼Œé€šè¿‡è¾¹ç•Œæ¡†åˆ°æ ‡é¢˜å›å½’å¢å¼ºè§†è§‰ç¼–ç å™¨çš„å±€éƒ¨è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶é€šè¿‡æ ‡é¢˜åˆ°è¾¹ç•Œæ¡†å›å½’æå‡LLMçš„è§†è§‰ç‰¹å¾åˆ©ç”¨å’Œå®šä½èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›å®éªŒè¡¨æ˜GranViTè¶…è¶Šäº†ç°æœ‰è§†è§‰ç¼–ç å™¨ï¼Œåœ¨ä¸åŒLLMä¸Šå±•ç°å‡ºå¼ºå¤§çš„å¯è¿ç§»æ€§ï¼Œåœ¨ç»†ç²’åº¦è¯†åˆ«ã€å¤šæ¨¡æ€VQAå’ŒOCRç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡å¤§è§„æ¨¡ç»†ç²’åº¦é¢„è®­ç»ƒå’ŒåŒºåŸŸçº§è‡ªå›å½’è®­ç»ƒå¯ä»¥æ˜¾è‘—æå‡è§†è§‰ç¼–ç å™¨çš„ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›äº†æ›´å¼ºå¤§çš„è§†è§‰ç†è§£åŸºç¡€ï¼Œæ¨åŠ¨äº†ç»†ç²’åº¦è§†è§‰è¯­è¨€ä»»åŠ¡çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Vision encoders are indispensable for allowing impressive performance of
Multi-modal Large Language Models (MLLMs) in vision language tasks such as
visual question answering and reasoning. However, existing vision encoders
focus on global image representations but overlook fine-grained regional
analysis. They are limited in fine grained perception due to the scarcity of
fine grained annotated data and the lack of a fine grained pre-training
paradigm. In this paper, we propose GranViT, a novel Vision Transformer that
integrates fine-grained feature extraction with semantic alignment to Large
Language Models (LLMs) via region level autoregressive training. We first
construct Gran-29M, a dataset comprising 2million natural and OCR images paired
with over 180 million high-quality region-level annotations, to enable large
scale fine grained pretraining. Consequently, we develop a
pretraining-adaptation framework along with a self distillation mechanism to
train fine-grained GranViT on Gran-29M. We sufficiently exploit the
fine-grained annotations from Gran-29M to resort to bounding-box-to-caption
regression to enhance localized visual representation of the vision encoder in
the pretraining and caption-to-bounding-box regression to improve vision
feature utilization and localization for LLM in the adaptation. We further
incorporate a self distillation mechanism that imposes explicit localization
constraints on the vision encoder to strengthen its regional reasoning
capability. Extensive experiments show that GranViT surpasses existing vision
encoders and attains strong transferability to varying LLMs. Remarkably, it
achieves state-of-the-art results on fine-grained recognition, multimodal VQA,
and OCR understanding.</p>
<h3 id="16-sample-by-step-optimize-by-chunk-chunk-level-grpo-for-text-to-image-generation">[16] <a href="https://arxiv.org/abs/2510.21583">Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation</a></h3>
<p><em>Yifu Luo, Penghui Du, Bo Li, Sinan Du, Tiantian Zhang, Yongzhe Chang, Kai Wu, Kun Gai, Xueqian Wang</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºChunk-GRPOï¼Œé¦–ä¸ªåŸºäºå—çº§ä¼˜åŒ–çš„GRPOæ–¹æ³•ï¼Œé€šè¿‡å°†ä¼˜åŒ–èŒƒå¼ä»æ­¥çº§è½¬ç§»åˆ°å—çº§æ¥è§£å†³ä¼˜åŠ¿å½’å› ä¸å‡†ç¡®å’Œå¿½ç•¥ç”Ÿæˆæ—¶åºåŠ¨æ€çš„é—®é¢˜ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å®ç°äº†æ›´å¥½çš„åå¥½å¯¹é½å’Œå›¾åƒè´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰Group Relative Policy Optimization (GRPO)æ–¹æ³•åœ¨åŸºäºæµåŒ¹é…çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™ï¼šä¼˜åŠ¿å½’å› ä¸å‡†ç¡®ï¼Œä»¥åŠå¿½ç•¥ç”Ÿæˆçš„æ—¶åºåŠ¨æ€ç‰¹æ€§ï¼Œè¿™é™åˆ¶äº†æ–¹æ³•çš„æ€§èƒ½æå‡å’Œå®é™…åº”ç”¨æ•ˆæœã€‚</p>
<p><strong>Method:</strong> æå‡ºChunk-GRPOæ–¹æ³•ï¼Œå°†è¿ç»­æ­¥éª¤åˆ†ç»„ä¸ºè¿è´¯çš„'å—'ä»¥æ•æ‰æµåŒ¹é…çš„å†…åœ¨æ—¶åºåŠ¨æ€ï¼Œåœ¨å—çº§åˆ«è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œå¹¶å¼•å…¥å¯é€‰çš„åŠ æƒé‡‡æ ·ç­–ç•¥æ¥è¿›ä¸€æ­¥æå‡æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒChunk-GRPOåœ¨åå¥½å¯¹é½å’Œå›¾åƒè´¨é‡æ–¹é¢å‡å–å¾—äº†ä¼˜è¶Šç»“æœï¼ŒéªŒè¯äº†å—çº§ä¼˜åŒ–å¯¹äºGRPOåŸºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯æ˜äº†å°†ä¼˜åŒ–èŒƒå¼ä»æ­¥çº§è½¬ç§»åˆ°å—çº§èƒ½å¤Ÿæœ‰æ•ˆè§£å†³GRPOæ–¹æ³•çš„å±€é™æ€§ï¼Œå—çº§ä¼˜åŒ–ä¸ºGRPOåŸºæ–¹æ³•æä¾›äº†æ–°çš„å‘å±•æ–¹å‘ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Group Relative Policy Optimization (GRPO) has shown strong potential for
flow-matching-based text-to-image (T2I) generation, but it faces two key
limitations: inaccurate advantage attribution, and the neglect of temporal
dynamics of generation. In this work, we argue that shifting the optimization
paradigm from the step level to the chunk level can effectively alleviate these
issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level
GRPO-based approach for T2I generation. The insight is to group consecutive
steps into coherent 'chunk's that capture the intrinsic temporal dynamics of
flow matching, and to optimize policies at the chunk level. In addition, we
introduce an optional weighted sampling strategy to further enhance
performance. Extensive experiments show that ChunkGRPO achieves superior
results in both preference alignment and image quality, highlighting the
promise of chunk-level optimization for GRPO-based methods.</p>
<h3 id="17-towards-physically-executable-3d-gaussian-for-embodied-navigation">[17] <a href="https://arxiv.org/abs/2510.21307">Towards Physically Executable 3D Gaussian for Embodied Navigation</a></h3>
<p><em>Bingchen Miao, Rong Wei, Zhiqi Ge, Xiaoquan sun, Shiqi Gao, Jingzhe Zhu, Renhan Wang, Siliang Tang, Jun Xiao, Rui Tang, Juncheng Li</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSAGE-3Dï¼Œä¸€ç§å°†3Dé«˜æ–¯æ³¼æº…å‡çº§ä¸ºå¯æ‰§è¡Œã€è¯­ä¹‰å’Œç‰©ç†å¯¹é½ç¯å¢ƒçš„æ–°èŒƒå¼ï¼Œé€šè¿‡å¯¹è±¡ä¸­å¿ƒè¯­ä¹‰æ ‡æ³¨å’Œç‰©ç†æ„ŸçŸ¥æ‰§è¡Œè¿æ¥ï¼Œè§£å†³äº†3DGSåœ¨è§†è§‰è¯­è¨€å¯¼èˆªä¸­ç¼ºä¹ç»†ç²’åº¦è¯­ä¹‰å’Œç‰©ç†å¯æ‰§è¡Œæ€§çš„é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> 3Dé«˜æ–¯æ³¼æº…è™½ç„¶å…·æœ‰ç…§ç‰‡çº§å®æ—¶æ¸²æŸ“èƒ½åŠ›ï¼Œä½†åœ¨è§†è§‰è¯­è¨€å¯¼èˆªä»»åŠ¡ä¸­ç¼ºä¹ç»†ç²’åº¦è¯­ä¹‰ç†è§£å’Œç‰©ç†å¯æ‰§è¡Œæ€§ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ç¼©å°ä»¿çœŸä¸ç°å®å·®è·æ–¹é¢çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†SAGE-3Dæ¡†æ¶ï¼ŒåŒ…å«å¯¹è±¡ä¸­å¿ƒè¯­ä¹‰æ ‡æ³¨ä¸º3DGSæ·»åŠ å¯¹è±¡çº§ç»†ç²’åº¦æ³¨é‡Šï¼Œä»¥åŠç‰©ç†æ„ŸçŸ¥æ‰§è¡Œè¿æ¥åœ¨3DGSä¸­åµŒå…¥ç¢°æ’å¯¹è±¡å¹¶æ„å»ºä¸°å¯Œçš„ç‰©ç†æ¥å£ï¼ŒåŒæ—¶å‘å¸ƒäº†åŒ…å«1Kå¯¹è±¡æ ‡æ³¨3DGSå®¤å†…åœºæ™¯æ•°æ®çš„InteriorGSæ•°æ®é›†å’Œé¦–ä¸ªåŸºäº3DGSçš„VLNåŸºå‡†SAGE-Benchã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜3DGSåœºæ™¯æ•°æ®æ”¶æ•›éš¾åº¦æ›´å¤§ä½†è¡¨ç°å‡ºå¼ºæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨VLN-CE Unseenä»»åŠ¡ä¸Šå°†åŸºçº¿æ€§èƒ½æå‡äº†31%ï¼ŒåŒæ—¶æ„å»ºäº†åŒ…å«2M VLNæ•°æ®çš„SAGE-BenchåŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†3DGSåœ¨è¯­ä¹‰å’Œç‰©ç†å¯¹é½æ–¹é¢çš„å¯æ‰©å±•æ€§ï¼Œä¸ºè§†è§‰è¯­è¨€å¯¼èˆªæä¾›äº†æ›´çœŸå®çš„ä»¿çœŸç¯å¢ƒï¼Œæ˜¾è‘—æå‡äº†å¯¼èˆªæ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ï¼Œä¸ºæ„å»ºæ›´é€¼çœŸçš„ä»¿çœŸåˆ°ç°å®è½¬æ¢ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic
real-time rendering capabilities, is regarded as an effective tool for
narrowing the sim-to-real gap. However, it lacks fine-grained semantics and
physical executability for Visual-Language Navigation (VLN). To address this,
we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments
for 3D Navigation), a new paradigm that upgrades 3DGS into an executable,
semantically and physically aligned environment. It comprises two components:
(1) Object-Centric Semantic Grounding, which adds object-level fine-grained
annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds
collision objects into 3DGS and constructs rich physical interfaces. We release
InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and
introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data.
Experiments show that 3DGS scene data is more difficult to converge, while
exhibiting strong generalizability, improving baseline performance by 31% on
the VLN-CE Unseen task. The data and code will be available soon.</p>
<h3 id="18-finers-fine-grained-reasoning-and-segmentation-of-small-objects-with-reinforcement-learning">[18] <a href="https://arxiv.org/abs/2510.21311">FineRS: Fine-grained Reasoning and Segmentation of Small Objects with Reinforcement Learning</a></h3>
<p><em>Lu Zhang, Jiazuo Yu, Haomiao Xiong, Ping Hu, Yunzhi Zhuge, Huchuan Lu, You He</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºFineRSï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å…¨å±€è¯­ä¹‰æ¢ç´¢å’Œå±€éƒ¨æ„ŸçŸ¥ç»†åŒ–çš„ç²—åˆ°ç²¾æµç¨‹ï¼Œè§£å†³äº†é«˜åˆ†è¾¨ç‡å›¾åƒä¸­æå°å‹ç‰©ä½“çš„ç²¾ç¡®ç†è§£å’Œå®šä½é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨æŒ‡ä»¤å¼•å¯¼åˆ†å‰²å’Œè§†è§‰æ¨ç†ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”±äºè¾“å…¥åˆ†è¾¨ç‡å—é™ï¼Œåœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶éš¾ä»¥ç²¾ç¡®ç†è§£å’Œå®šä½è§†è§‰ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åµŒå…¥å¤æ‚èƒŒæ™¯ä¸­çš„æå°å‹ç‰©ä½“æ—¶é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> FineRSé‡‡ç”¨ä¸¤é˜¶æ®µç²—åˆ°ç²¾æµç¨‹ï¼ŒåŒ…æ‹¬å…¨å±€è¯­ä¹‰æ¢ç´¢ï¼ˆGSEï¼‰å’Œå±€éƒ¨æ„ŸçŸ¥ç»†åŒ–ï¼ˆLPRï¼‰ã€‚GSEé€šè¿‡æŒ‡ä»¤å¼•å¯¼æ¨ç†ç”Ÿæˆæ–‡æœ¬å“åº”å’Œç²—ç•¥ç›®æ ‡åŒºåŸŸï¼ŒLPRåˆ™ç»†åŒ–è¯¥åŒºåŸŸä»¥ç”Ÿæˆç²¾ç¡®è¾¹ç•Œæ¡†å’Œåˆ†å‰²æ©ç ã€‚é€šè¿‡å¼•å…¥å®šä½çŸ¥æƒ…çš„å›é¡¾æ€§å¥–åŠ±æœºåˆ¶ï¼Œå°†LPRè¾“å‡ºç”¨äºä¼˜åŒ–GSEä»¥è·å¾—æ›´é²æ£’çš„ç²—ç•¥åŒºåŸŸæ¢ç´¢ã€‚</p>
<p><strong>Result:</strong> åœ¨FineRS-4kæ•°æ®é›†å’Œå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŒ‡ä»¤å¼•å¯¼åˆ†å‰²å’Œè§†è§‰æ¨ç†ä»»åŠ¡ä¸Šä¸€è‡´ä¼˜äºæœ€å…ˆè¿›çš„åŸºäºMLLMçš„æ–¹æ³•ã€‚æ–°æå‡ºçš„FineRS-4kæ•°æ®é›†ä¸“é—¨ç”¨äºè¯„ä¼°MLLMåœ¨å¤æ‚é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸­å¯¹ç»†å¾®ã€å°å°ºåº¦ç›®æ ‡çš„å±æ€§çº§æ¨ç†å’Œåƒç´ çº§åˆ†å‰²èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨è€¦åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œåˆ†å‰²èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè§£å†³é«˜åˆ†è¾¨ç‡å›¾åƒä¸­æå°å‹ç‰©ä½“çš„ç²¾ç¡®å®šä½é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºæœªæ¥åœ¨å¤æ‚è§†è§‰åœºæ™¯ç†è§£æ–¹é¢çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities
across a wide range of vision-language tasks. However, due to the restricted
input resolutions, MLLMs face significant challenges in precisely understanding
and localizing visual details in high-resolution images -- particularly when
dealing with extra-small objects embedded in cluttered contexts. To address
this issue, we propose \textsc{FineRS}, a two-stage MLLM-based reinforcement
learning framework for jointly reasoning and segmenting extremely small objects
within high-resolution scenes. \textsc{FineRS} adopts a coarse-to-fine pipeline
comprising Global Semantic Exploration (GSE) and Localized Perceptual
Refinement (LPR). Specifically, GSE performs instruction-guided reasoning to
generate a textural response and a coarse target region, while LPR refines this
region to produce an accurate bounding box and segmentation mask. To couple the
two stages, we introduce a locate-informed retrospective reward, where LPR's
outputs are used to optimize GSE for more robust coarse region exploration. %
Additionally, we present \textsc{FineRS}-4k, a new dataset for evaluating MLLMs
on attribute-level reasoning and pixel-level segmentation on subtle,
small-scale targets in complex high-resolution scenes. Experimental results on
\textsc{FineRS}-4k and public datasets demonstrate that our method consistently
outperforms state-of-the-art MLLM-based approaches on both instruction-guided
segmentation and visual reasoning tasks.</p>
<h3 id="19-vl-sae-interpreting-and-enhancing-vision-language-alignment-with-a-unified-concept-set">[19] <a href="https://arxiv.org/abs/2510.21323">VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set</a></h3>
<p><em>Shufan Shen, Junshu Sun, Qingming Huang, Shuhui Wang</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºVL-SAEï¼Œä¸€ç§ç¨€ç–è‡ªç¼–ç å™¨ï¼Œé€šè¿‡å°†è§†è§‰-è¯­è¨€è¡¨ç¤ºç¼–ç ä¸ºéšè—å±‚æ¿€æ´»æ¥è§£æ„å¤šæ¨¡æ€å¯¹é½çš„å¯è§£é‡Šæ€§ï¼Œæ¯ä¸ªç¥ç»å…ƒå¯¹åº”ç”±è¯­ä¹‰ç›¸ä¼¼çš„å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºçš„æ¦‚å¿µï¼Œä»è€Œå»ºç«‹ç»Ÿä¸€çš„è§£é‡Šæ¡†æ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å¯¹é½ç»„ä»¶çš„å¯è§£é‡Šæ€§ä»æœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œä¸»è¦å›°éš¾åœ¨äºéš¾ä»¥å°†å¤šæ¨¡æ€è¡¨ç¤ºçš„è¯­ä¹‰æ˜ å°„åˆ°ç»Ÿä¸€çš„æ¦‚å¿µé›†åˆä¸­ã€‚</p>
<p><strong>Method:</strong> æå‡ºVL-SAEç¨€ç–è‡ªç¼–ç å™¨ï¼Œé‡‡ç”¨åŸºäºè·ç¦»çš„ç¼–ç å™¨å’Œä¸¤ä¸ªæ¨¡æ€ç‰¹å®šè§£ç å™¨ï¼Œé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦æ˜¾å¼å¯¹é½å¤šæ¨¡æ€è¡¨ç¤ºï¼Œå¹¶é¼“åŠ±è¯­ä¹‰ç›¸ä¼¼çš„è¡¨ç¤ºåœ¨è‡ªç›‘ç£è®­ç»ƒä¸­è¡¨ç°å‡ºä¸€è‡´çš„ç¥ç»å…ƒæ¿€æ´»æ¨¡å¼ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVL-SAEåœ¨è§£é‡Šå’Œå¢å¼ºè§†è§‰-è¯­è¨€å¯¹é½æ–¹é¢å…·æœ‰å“è¶Šèƒ½åŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡æ¦‚å¿µæ¯”è¾ƒç†è§£è§†è§‰ä¸è¯­è¨€è¡¨ç¤ºçš„å¯¹é½å…³ç³»ï¼Œå¹¶åœ¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œå¹»è§‰æ¶ˆé™¤ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­å¸¦æ¥æ€§èƒ½æå‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€è¡¨ç¤ºå¯¹é½æä¾›äº†å¯è§£é‡Šçš„åˆ†ææ¡†æ¶ï¼Œé€šè¿‡æ¦‚å¿µçº§åˆ«çš„å¯¹é½å¢å¼ºè§†è§‰-è¯­è¨€è¡¨ç¤ºï¼Œä¸ä»…æå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œè¿˜æ”¹å–„äº†å®é™…åº”ç”¨æ€§èƒ½ï¼Œä¸ºå¤šæ¨¡æ€AIç³»ç»Ÿçš„é€æ˜åŒ–å‘å±•æä¾›äº†é‡è¦æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>The alignment of vision-language representations endows current
Vision-Language Models (VLMs) with strong multi-modal reasoning capabilities.
However, the interpretability of the alignment component remains uninvestigated
due to the difficulty in mapping the semantics of multi-modal representations
into a unified concept set. To address this problem, we propose VL-SAE, a
sparse autoencoder that encodes vision-language representations into its hidden
activations. Each neuron in its hidden layer correlates to a concept
represented by semantically similar images and texts, thereby interpreting
these representations with a unified concept set. To establish the
neuron-concept correlation, we encourage semantically similar representations
to exhibit consistent neuron activations during self-supervised training.
First, to measure the semantic similarity of multi-modal representations, we
perform their alignment in an explicit form based on cosine similarity. Second,
we construct the VL-SAE with a distance-based encoder and two modality-specific
decoders to ensure the activation consistency of semantically similar
representations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)
demonstrate the superior capability of VL-SAE in interpreting and enhancing the
vision-language alignment. For interpretation, the alignment between vision and
language representations can be understood by comparing their semantics with
concepts. For enhancement, the alignment can be strengthened by aligning
vision-language representations at the concept level, contributing to
performance improvements in downstream tasks, including zero-shot image
classification and hallucination elimination. Codes are available at
https://github.com/ssfgunner/VL-SAE.</p>
<h3 id="20-muvr-a-multi-modal-untrimmed-video-retrieval-benchmark-with-multi-level-visual-correspondence">[20] <a href="https://arxiv.org/abs/2510.21406">MUVR: A Multi-Modal Untrimmed Video Retrieval Benchmark with Multi-Level Visual Correspondence</a></h3>
<p><em>Yue Feng, Jinwei Hu, Qijia Lu, Jiawei Niu, Li Tan, Shuo Yuan, Ziyi Yan, Yizhen Jia, Qingzhi He, Shiping Ge, Ethan Q. Chen, Wentong Li, Limin Wang, Jie Qin</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€æœªä¿®å‰ªè§†é¢‘æ£€ç´¢ä»»åŠ¡å’ŒMUVRåŸºå‡†ï¼Œæ—¨åœ¨è§£å†³é•¿è§†é¢‘å¹³å°ä¸­å¤šæ¨¡æ€æŸ¥è¯¢çš„æœªä¿®å‰ªè§†é¢‘æ£€ç´¢é—®é¢˜ï¼Œé€šè¿‡æ„å»ºåŒ…å«53Kè§†é¢‘å’Œ1050ä¸ªå¤šæ¨¡æ€æŸ¥è¯¢çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç³»ç»Ÿè¯„ä¼°äº†ç°æœ‰æ£€ç´¢æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ€§èƒ½å±€é™ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘æ£€ç´¢æ–¹æ³•ä¸»è¦é’ˆå¯¹ä¿®å‰ªåçš„çŸ­è§†é¢‘ï¼Œéš¾ä»¥å¤„ç†é•¿è§†é¢‘å¹³å°ä¸­çš„æœªä¿®å‰ªè§†é¢‘å’Œå¤šæ¨¡æ€æŸ¥è¯¢éœ€æ±‚ï¼Œç°æœ‰åŸºå‡†ç¼ºä¹å¯¹å¤šæ¨¡æ€æŸ¥è¯¢æ”¯æŒå’Œæœªä¿®å‰ªè§†é¢‘æ£€ç´¢çš„ä¸“é—¨è¯„ä¼°ï¼Œæ— æ³•æ»¡è¶³å®é™…åº”ç”¨åœºæ™¯çš„éœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> æå‡ºMUVRåŸºå‡†ï¼Œæ”¯æŒè§†é¢‘ä¸­å¿ƒçš„å¤šæ¨¡æ€æŸ¥è¯¢ï¼ˆé•¿æ–‡æœ¬æè¿°ã€è§†é¢‘æ ‡ç­¾æç¤ºã€æ©ç æç¤ºï¼‰ï¼Œé‡‡ç”¨ä¸€å¯¹å¤šæ£€ç´¢èŒƒå¼ï¼Œæ„å»ºåŸºäºæ ¸å¿ƒè§†é¢‘å†…å®¹çš„å¤šå±‚æ¬¡è§†è§‰å¯¹åº”å…³ç³»ï¼ˆå¤åˆ¶ã€äº‹ä»¶ã€åœºæ™¯ã€å®ä¾‹ã€åŠ¨ä½œç­‰å…­ä¸ªçº§åˆ«ï¼‰ï¼Œå¼€å‘ä¸‰ä¸ªç‰ˆæœ¬ï¼ˆBaseã€Filterã€QAï¼‰åˆ†åˆ«è¯„ä¼°æ£€ç´¢æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> åœ¨åŒ…å«53Kæœªä¿®å‰ªè§†é¢‘å’Œ1050ä¸ªå¤šæ¨¡æ€æŸ¥è¯¢çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¯„ä¼°äº†3ä¸ªæœ€å…ˆè¿›çš„è§†é¢‘æ£€ç´¢æ¨¡å‹ã€6ä¸ªåŸºäºå›¾åƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œ10ä¸ªå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œæ­ç¤ºäº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æœªä¿®å‰ªè§†é¢‘å’Œå¤šæ¨¡æ€æŸ¥è¯¢æ–¹é¢çš„å±€é™æ€§ï¼Œä»¥åŠå¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨å¤šè§†é¢‘ç†è§£å’Œé‡æ’åºèƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚</p>
<p><strong>Conclusion:</strong> MUVRåŸºå‡†ç³»ç»Ÿæ­ç¤ºäº†å½“å‰è§†é¢‘æ£€ç´¢æŠ€æœ¯åœ¨å¤„ç†æœªä¿®å‰ªè§†é¢‘å’Œå¤šæ¨¡æ€æŸ¥è¯¢æ–¹é¢çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„è¯„ä¼°æ ‡å‡†å’Œå‘å±•æ–¹å‘ï¼Œå¼ºè°ƒäº†å¤šæ¨¡æ€ç†è§£å’Œé•¿è§†é¢‘å¤„ç†èƒ½åŠ›åœ¨è§†é¢‘æ£€ç´¢ä¸­çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>We propose the Multi-modal Untrimmed Video Retrieval task, along with a new
benchmark (MUVR) to advance video retrieval for long-video platforms. MUVR aims
to retrieve untrimmed videos containing relevant segments using multi-modal
queries. It has the following features: 1) Practical retrieval paradigm: MUVR
supports video-centric multi-modal queries, expressing fine-grained retrieval
needs through long text descriptions, video tag prompts, and mask prompts. It
adopts a one-to-many retrieval paradigm and focuses on untrimmed videos,
tailored for long-video platform applications. 2) Multi-level visual
correspondence: To cover common video categories (e.g., news, travel, dance)
and precisely define retrieval matching criteria, we construct multi-level
visual correspondence based on core video content (e.g., news events, travel
locations, dance moves) which users are interested in and want to retrieve. It
covers six levels: copy, event, scene, instance, action, and others. 3)
Comprehensive evaluation criteria: We develop 3 versions of MUVR (i.e., Base,
Filter, QA). MUVR-Base/Filter evaluates retrieval models, while MUVR-QA
assesses MLLMs in a question-answering format. We also propose a Reranking
Score to evaluate the reranking ability of MLLMs. MUVR consists of 53K
untrimmed videos from the video platform Bilibili, with 1,050 multi-modal
queries and 84K matches. Extensive evaluations of 3 state-of-the-art video
retrieval models, 6 image-based VLMs, and 10 MLLMs are conducted. MUVR reveals
the limitations of retrieval methods in processing untrimmed videos and
multi-modal queries, as well as MLLMs in multi-video understanding and
reranking. Our code and benchmark is available at
https://github.com/debby-0527/MUVR.</p>
<h3 id="21-bridging-the-gap-to-real-world-language-grounded-visual-concept-learning">[21] <a href="https://arxiv.org/abs/2510.21412">Bridging the gap to real-world language-grounded visual concept learning</a></h3>
<p><em>Whie Jung, Semin Kim, Junee Kim, Seunghoon Hong</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„è¯­è¨€å¼•å¯¼è§†è§‰æ¦‚å¿µå­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°è¯†åˆ«å›¾åƒç›¸å…³æ¦‚å¿µè½´å¹¶åœ¨çœŸå®åœºæ™¯ä¸­æ²¿è¿™äº›è½´è¿›è¡Œè§†è§‰æ¦‚å¿µå®šä½ï¼Œæ— éœ€é¢„å®šä¹‰æ¦‚å¿µç±»åˆ«å³å¯å®ç°å¤šæ ·åŒ–çš„è§†è§‰ç¼–è¾‘ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è¯­è¨€å¼•å¯¼çš„è§†è§‰æ¦‚å¿µå­¦ä¹ æ–¹æ³•å±€é™äºå°‘æ•°é¢„å®šä¹‰çš„åŸºæœ¬ç»´åº¦ï¼ˆå¦‚é¢œè‰²å’Œå½¢çŠ¶ï¼‰ï¼Œä¸”ä¸»è¦åœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œæ¢ç´¢ï¼Œæ— æ³•é€‚åº”çœŸå®åœºæ™¯ä¸­ä¸°å¯Œå¤šæ ·çš„è¯­ä¹‰æ¦‚å¿µå­¦ä¹ éœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹å’Œé€šç”¨æç¤ºç­–ç•¥è‡ªé€‚åº”è¯†åˆ«å¤šæ ·åŒ–çš„å›¾åƒç›¸å…³æ¦‚å¿µè½´ï¼Œé€šè¿‡é€šç”¨æ¦‚å¿µç¼–ç å™¨å°†è§†è§‰ç‰¹å¾ç»‘å®šåˆ°å‘ç°çš„æ¦‚å¿µè½´ä¸Šï¼Œæ— éœ€ä¸ºæ¯ä¸ªæ¦‚å¿µå¼•å…¥é¢å¤–æ¨¡å‹å‚æ•°ï¼Œå¹¶ä¼˜åŒ–ç»„åˆé”šå®šç›®æ ‡ç¡®ä¿å„æ¦‚å¿µè½´å¯ç‹¬ç«‹æ“ä½œã€‚</p>
<p><strong>Result:</strong> åœ¨ImageNetã€CelebA-HQå’ŒAFHQæ•°æ®é›†ä¸Šå±•ç¤ºäº†ä¼˜è¶Šçš„ç¼–è¾‘èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†è¿‡äºå¤šæ ·åŒ–è€Œæ— æ³•æ‰‹åŠ¨é¢„å®šä¹‰çš„çœŸå®ä¸–ç•Œæ¦‚å¿µï¼Œå¹¶åœ¨ç»„åˆæ³›åŒ–æ–¹é¢ä¼˜äºç°æœ‰çš„è§†è§‰æ¦‚å¿µå­¦ä¹ å’ŒåŸºäºæ–‡æœ¬çš„ç¼–è¾‘æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ¡†æ¶çªç ´äº†é¢„å®šä¹‰æ¦‚å¿µè½´çš„é™åˆ¶ï¼Œå®ç°äº†çœŸå®åœºæ™¯ä¸­å¤šæ ·åŒ–è§†è§‰æ¦‚å¿µçš„è‡ªé€‚åº”å­¦ä¹ ä¸ç¼–è¾‘ï¼Œä¸ºè¯­è¨€å¼•å¯¼çš„è§†è§‰æ¦‚å¿µå­¦ä¹ æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰è¾ƒå¼ºçš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Human intelligence effortlessly interprets visual scenes along a rich
spectrum of semantic dimensions. However, existing approaches to
language-grounded visual concept learning are limited to a few predefined
primitive axes, such as color and shape, and are typically explored in
synthetic datasets. In this work, we propose a scalable framework that
adaptively identifies image-related concept axes and grounds visual concepts
along these axes in real-world scenes. Leveraging a pretrained vision-language
model and our universal prompting strategy, our framework identifies a diverse
image-related axes without any prior knowledge. Our universal concept encoder
adaptively binds visual features to the discovered axes without introducing
additional model parameters for each concept. To ground visual concepts along
the discovered axes, we optimize a compositional anchoring objective, which
ensures that each axis can be independently manipulated without affecting
others. We demonstrate the effectiveness of our framework on subsets of
ImageNet, CelebA-HQ, and AFHQ, showcasing superior editing capabilities across
diverse real-world concepts that are too varied to be manually predefined. Our
method also exhibits strong compositional generalization, outperforming
existing visual concept learning and text-based editing methods. The code is
available at https://github.com/whieya/Language-grounded-VCL.</p>
<h3 id="22-monitor-exploiting-large-language-models-with-instruction-for-online-video-anomaly-detection">[22] <a href="https://arxiv.org/abs/2510.21449">MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection</a></h3>
<p><em>Shengtian Yang, Yue Feng, Yingshi Liu, Jingrou Zhang, Jie Qin</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMoniTorï¼Œä¸€ç§åŸºäºå†…å­˜çš„åœ¨çº¿è¯„åˆ†é˜Ÿåˆ—æ–¹æ¡ˆï¼Œç”¨äºæ— éœ€è®­ç»ƒçš„è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼Œé€šè¿‡ç»“åˆé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹å’ŒLSTMå¯å‘çš„é¢„æµ‹æœºåˆ¶ï¼Œåœ¨å®æ—¶è§†é¢‘å¼‚å¸¸æ£€æµ‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†é¢‘å¼‚å¸¸æ£€æµ‹ç ”ç©¶ä¸»è¦å…³æ³¨ç¦»çº¿åœºæ™¯ï¼Œè€Œåœ¨çº¿è§†é¢‘å¼‚å¸¸æ£€æµ‹ç”±äºå®æ—¶æ€§çº¦æŸå’Œè®¡ç®—å¼ºåº¦å¾ˆå°‘å—åˆ°å…³æ³¨ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨çº¿è§†é¢‘å¼‚å¸¸æ£€æµ‹ä¸­çš„å›ºæœ‰å¤æ‚æ€§æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> MoniToré‡‡ç”¨æµå¼è¾“å…¥åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç»“åˆLSTMç½‘ç»œå¯å‘çš„é¢„æµ‹æœºåˆ¶æ¥æœ‰æ•ˆå»ºæ¨¡æ—¶é—´ä¾èµ–æ€§ï¼Œå¹¶è®¾è®¡äº†è¯„åˆ†é˜Ÿåˆ—å’Œå¼‚å¸¸å…ˆéªŒæ¥åŠ¨æ€å­˜å‚¨æœ€è¿‘åˆ†æ•°ï¼Œä¸ºLLMsåŒºåˆ†æ­£å¸¸å’Œå¼‚å¸¸è¡Œä¸ºæä¾›æ—¶é—´ç»´åº¦çš„æŒ‡å¯¼ã€‚</p>
<p><strong>Result:</strong> åœ¨UCF-Crimeå’ŒXD-Violenceä¸¤ä¸ªå¤§å‹æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMoniToråœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶ä¸å¼±ç›‘ç£æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é¢„è®­ç»ƒå¤§æ¨¡å‹åœ¨åœ¨çº¿è§†é¢‘å¼‚å¸¸æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡åˆ›æ–°çš„å†…å­˜æœºåˆ¶å’Œæ—¶é—´å»ºæ¨¡æ–¹æ³•ï¼Œä¸ºå®æ—¶å¼‚å¸¸æ£€æµ‹æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒäº†æ— éœ€è®­ç»ƒçš„ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Video Anomaly Detection (VAD) aims to locate unusual activities or behaviors
within videos. Recently, offline VAD has garnered substantial research
attention, which has been invigorated by the progress in large language models
(LLMs) and vision-language models (VLMs), offering the potential for a more
nuanced understanding of anomalies. However, online VAD has seldom received
attention due to real-time constraints and computational intensity. In this
paper, we introduce a novel Memory-based online scoring queue scheme for
Training-free VAD (MoniTor), to address the inherent complexities in online
VAD. Specifically, MoniTor applies a streaming input to VLMs, leveraging the
capabilities of pre-trained large-scale models. To capture temporal
dependencies more effectively, we incorporate a novel prediction mechanism
inspired by Long Short-Term Memory (LSTM) networks. This ensures the model can
effectively model past states and leverage previous predictions to identify
anomalous behaviors. Thereby, it better understands the current frame.
Moreover, we design a scoring queue and an anomaly prior to dynamically store
recent scores and cover all anomalies in the monitoring scenario, providing
guidance for LLMs to distinguish between normal and abnormal behaviors over
time. We evaluate MoniTor on two large datasets (i.e., UCF-Crime and
XD-Violence) containing various surveillance and real-world scenarios. The
results demonstrate that MoniTor outperforms state-of-the-art methods and is
competitive with weakly supervised methods without training. Code is available
at https://github.com/YsTvT/MoniTor.</p>
<h3 id="23-cxr-lanic-language-grounded-interpretable-classifier-for-chest-x-ray-diagnosis">[23] <a href="https://arxiv.org/abs/2510.21464">CXR-LanIC: Language-Grounded Interpretable Classifier for Chest X-Ray Diagnosis</a></h3>
<p><em>Yiming Tang, Wenjia Zhong, Rushi Shah, Dianbo Liu</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†CXR-LanICæ¡†æ¶ï¼Œé€šè¿‡åœ¨BiomedCLIPè¯Šæ–­åˆ†ç±»å™¨ä¸Šè®­ç»ƒåŸºäºè½¬ç å™¨çš„ç¨€ç–è‡ªç¼–ç å™¨ï¼Œå°†åŒ»å­¦å›¾åƒè¡¨å¾åˆ†è§£ä¸ºå¯è§£é‡Šçš„è§†è§‰æ¨¡å¼ï¼Œå®ç°äº†åœ¨ä¿æŒç«äº‰æ€§è¯Šæ–­å‡†ç¡®æ€§çš„åŒæ—¶æä¾›é€æ˜å¯éªŒè¯çš„è§£é‡Šã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨èƒ¸éƒ¨Xå…‰è¯Šæ–­ä¸­å–å¾—äº†æ˜¾è‘—å‡†ç¡®æ€§ï¼Œä½†å…¶é»‘ç›’é¢„æµ‹ç‰¹æ€§é™åˆ¶äº†ä¸´åºŠå¹¿æ³›åº”ç”¨ï¼Œä¸´åºŠåŒ»ç”Ÿéœ€è¦é€æ˜å¯éªŒè¯çš„è§£é‡Šæ¥ä¿¡ä»»è‡ªåŠ¨åŒ–è¯Šæ–­å¹¶è¯†åˆ«æ½œåœ¨å¤±è´¥æ¨¡å¼ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•è®­ç»ƒåŸºäºè½¬ç å™¨çš„ç¨€ç–è‡ªç¼–ç å™¨ï¼Œåœ¨MIMIC-CXRæ•°æ®é›†ä¸Šè®­ç»ƒ100ä¸ªè½¬ç å™¨é›†æˆï¼Œä»å¤šæ¨¡æ€åµŒå…¥ä¸­åˆ†è§£å‡ºçº¦5,000ä¸ªå•ä¹‰æ€§æ¨¡å¼ï¼Œæ¶µç›–å¿ƒè„ã€è‚ºéƒ¨ã€èƒ¸è†œã€ç»“æ„ã€è®¾å¤‡å’Œä¼ªå½±ç­‰ç±»åˆ«ã€‚</p>
<p><strong>Result:</strong> CXR-LanICåœ¨äº”ä¸ªå…³é”®å‘ç°ä¸Šå®ç°äº†ç«äº‰æ€§è¯Šæ–­å‡†ç¡®æ€§ï¼Œé¢„æµ‹å¯åˆ†è§£ä¸º20-50ä¸ªå¯è§£é‡Šæ¨¡å¼ï¼Œæ¯ä¸ªæ¨¡å¼åœ¨å…±äº«ç‰¹å®šæ”¾å°„å­¦ç‰¹å¾çš„å›¾åƒé—´è¡¨ç°å‡ºä¸€è‡´çš„æ¿€æ´»è¡Œä¸ºï¼Œå¹¶å¯é€šè¿‡éªŒè¯æ¿€æ´»å›¾åº“è¿›è¡Œé€æ˜å½’å› ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åœ¨äºä»é’ˆå¯¹ç‰¹å®šè¯Šæ–­ç›®æ ‡è®­ç»ƒçš„åˆ†ç±»å™¨ä¸­æå–å¯è§£é‡Šç‰¹å¾ï¼Œè€Œéé€šç”¨åµŒå…¥ï¼Œç¡®ä¿å‘ç°çš„æ¨¡å¼ä¸ä¸´åºŠå†³ç­–ç›´æ¥ç›¸å…³ï¼Œè¯æ˜åŒ»å­¦AIç³»ç»Ÿå¯ä»¥åŒæ—¶å®ç°å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œé€šè¿‡é€æ˜ã€ä¸´åºŠåŸºç¡€çš„è§£é‡Šæ”¯æŒæ›´å®‰å…¨çš„ä¸´åºŠéƒ¨ç½²ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Deep learning models have achieved remarkable accuracy in chest X-ray
diagnosis, yet their widespread clinical adoption remains limited by the
black-box nature of their predictions. Clinicians require transparent,
verifiable explanations to trust automated diagnoses and identify potential
failure modes. We introduce CXR-LanIC (Language-Grounded Interpretable
Classifier for Chest X-rays), a novel framework that addresses this
interpretability challenge through task-aligned pattern discovery. Our approach
trains transcoder-based sparse autoencoders on a BiomedCLIP diagnostic
classifier to decompose medical image representations into interpretable visual
patterns. By training an ensemble of 100 transcoders on multimodal embeddings
from the MIMIC-CXR dataset, we discover approximately 5,000 monosemantic
patterns spanning cardiac, pulmonary, pleural, structural, device, and artifact
categories. Each pattern exhibits consistent activation behavior across images
sharing specific radiological features, enabling transparent attribution where
predictions decompose into 20-50 interpretable patterns with verifiable
activation galleries. CXR-LanIC achieves competitive diagnostic accuracy on
five key findings while providing the foundation for natural language
explanations through planned large multimodal model annotation. Our key
innovation lies in extracting interpretable features from a classifier trained
on specific diagnostic objectives rather than general-purpose embeddings,
ensuring discovered patterns are directly relevant to clinical decision-making,
demonstrating that medical AI systems can be both accurate and interpretable,
supporting safer clinical deployment through transparent, clinically grounded
explanations.</p>
<h3 id="24-towards-a-golden-classifier-free-guidance-path-via-foresight-fixed-point-iterations">[24] <a href="https://arxiv.org/abs/2510.21512">Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point Iterations</a></h3>
<p><em>Kaibo Wang, Jianda Mao, Tong Wu, Yang Xiang</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†å‰ç»æ€§å¼•å¯¼ï¼ˆFSGï¼‰æ–¹æ³•ï¼Œé€šè¿‡å°†æ¡ä»¶å¼•å¯¼é‡æ–°æ„å»ºä¸ºä¸åŠ¨ç‚¹è¿­ä»£é—®é¢˜ï¼Œè§£å†³äº†ç°æœ‰åˆ†ç±»å™¨æ— å…³å¼•å¯¼ï¼ˆCFGï¼‰æ–¹æ³•çš„æ•ˆç‡é™åˆ¶ã€‚FSGåœ¨æ—©æœŸæ‰©æ•£é˜¶æ®µä¼˜å…ˆè§£å†³é•¿é—´éš”å­é—®é¢˜å¹¶å¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œåœ¨å›¾åƒè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åˆ†ç±»å™¨æ— å…³å¼•å¯¼ï¼ˆCFGï¼‰æ–¹æ³•åŸºäºä¸åŒçš„ç†è®ºè§£é‡Šï¼Œé™åˆ¶äº†è®¾è®¡ç©ºé—´å¹¶æ¨¡ç³Šäº†å…³é”®è®¾è®¡é€‰æ‹©ã€‚è¿™äº›æ–¹æ³•è¢«è¯æ˜æ„æˆå•æ­¥çŸ­é—´éš”è¿­ä»£çš„ç‰¹ä¾‹ï¼Œç†è®ºä¸Šå­˜åœ¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œéœ€è¦ä¸€ç§ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶æ¥æ”¹è¿›æ¡ä»¶å¼•å¯¼æœºåˆ¶ã€‚</p>
<p><strong>Method:</strong> æå‡ºç»Ÿä¸€è§†è§’å°†æ¡ä»¶å¼•å¯¼é‡æ–°æ„å»ºä¸ºä¸åŠ¨ç‚¹è¿­ä»£é—®é¢˜ï¼Œæ—¨åœ¨æ‰¾åˆ°æ½œåœ¨å˜é‡åœ¨æ¡ä»¶ç”Ÿæˆå’Œæ— æ¡ä»¶ç”Ÿæˆä¸‹äº§ç”Ÿä¸€è‡´è¾“å‡ºçš„é»„é‡‘è·¯å¾„ã€‚å¼•å…¥å‰ç»æ€§å¼•å¯¼ï¼ˆFSGï¼‰æ–¹æ³•ï¼Œåœ¨æ—©æœŸæ‰©æ•£é˜¶æ®µä¼˜å…ˆè§£å†³é•¿é—´éš”å­é—®é¢˜å¹¶å¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œçªç ´äº†ä¼ ç»ŸCFGçš„å•æ­¥çŸ­é—´éš”è¿­ä»£é™åˆ¶ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šæ ·åŒ–æ•°æ®é›†å’Œæ¨¡å‹æ¶æ„ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†FSGçš„ä¼˜è¶Šæ€§ï¼Œç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—æå‡ã€‚FSGä¸ä»…æ”¹è¿›äº†ç”Ÿæˆè´¨é‡ï¼Œè¿˜æä¾›äº†æ›´é«˜æ•ˆçš„è®¡ç®—æ¡†æ¶ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶ä¸ºæ¡ä»¶å¼•å¯¼æä¾›äº†æ–°é¢–çš„ç†è®ºè§†è§’ï¼Œæ­ç¤ºäº†è‡ªé€‚åº”è®¾è®¡çš„æ½œåŠ›ã€‚ç»Ÿä¸€æ¡†æ¶ä¸ä»…è§£é‡Šäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œè¿˜ä¸ºæœªæ¥æ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„ä¼˜åŒ–å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œæ¨åŠ¨äº†æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Classifier-Free Guidance (CFG) is an essential component of text-to-image
diffusion models, and understanding and advancing its operational mechanisms
remains a central focus of research. Existing approaches stem from divergent
theoretical interpretations, thereby limiting the design space and obscuring
key design choices. To address this, we propose a unified perspective that
reframes conditional guidance as fixed point iterations, seeking to identify a
golden path where latents produce consistent outputs under both conditional and
unconditional generation. We demonstrate that CFG and its variants constitute a
special case of single-step short-interval iteration, which is theoretically
proven to exhibit inefficiency. To this end, we introduce Foresight Guidance
(FSG), which prioritizes solving longer-interval subproblems in early diffusion
stages with increased iterations. Extensive experiments across diverse datasets
and model architectures validate the superiority of FSG over state-of-the-art
methods in both image quality and computational efficiency. Our work offers
novel perspectives for conditional guidance and unlocks the potential of
adaptive design.</p>
<h3 id="25-foley-control-aligning-a-frozen-latent-text-to-audio-model-to-video">[25] <a href="https://arxiv.org/abs/2510.21581">Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video</a></h3>
<p><em>Ciara Rowles, Varun Jampani, Simon DonnÃ©, Shimon Vainer, Julian Parker, Zach Evans</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>Foley Controlæå‡ºäº†ä¸€ç§è½»é‡çº§çš„è§†é¢‘å¼•å¯¼Foleyæ–¹æ³•ï¼Œé€šè¿‡å†»ç»“é¢„è®­ç»ƒçš„å•æ¨¡æ€æ¨¡å‹å¹¶ä»…å­¦ä¹ å®ƒä»¬ä¹‹é—´çš„å°å‹äº¤å‰æ³¨æ„åŠ›æ¡¥æ¥ï¼Œå°†V-JEPA2è§†é¢‘åµŒå…¥è¿æ¥åˆ°å†»ç»“çš„Stable Audio Open DiTæ–‡æœ¬åˆ°éŸ³é¢‘æ¨¡å‹ï¼Œå®ç°äº†é«˜æ•ˆçš„å¤šæ¨¡æ€éŸ³é¢‘ç”Ÿæˆã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€ç³»ç»Ÿé€šå¸¸éœ€è¦ç«¯åˆ°ç«¯é‡æ–°è®­ç»ƒï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”ç¼ºä¹æ¨¡å—åŒ–ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§è½»é‡çº§æ–¹æ³•ï¼Œåœ¨ä¿æŒé¢„è®­ç»ƒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶å®ç°è§†é¢‘ä¸éŸ³é¢‘çš„æœ‰æ•ˆåŒæ­¥ï¼ŒåŒæ—¶ä¿ç•™æç¤ºé©±åŠ¨çš„å¯æ§æ€§ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åœ¨ç°æœ‰æ–‡æœ¬äº¤å‰æ³¨æ„åŠ›ä¹‹åæ’å…¥ç´§å‡‘çš„è§†é¢‘äº¤å‰æ³¨æ„åŠ›ï¼Œä½¿ç”¨æç¤ºè®¾ç½®å…¨å±€è¯­ä¹‰è€Œè§†é¢‘ç»†åŒ–æ—¶åºå’Œå±€éƒ¨åŠ¨æ€ï¼Œé€šè¿‡æ± åŒ–è§†é¢‘tokenæ¥å‡å°‘å†…å­˜æ¶ˆè€—å¹¶ç¨³å®šè®­ç»ƒï¼Œä¿æŒéŸ³é¢‘å…ˆéªŒä¸å˜ä»…å­¦ä¹ éŸ³é¢‘-è§†é¢‘ä¾èµ–å…³ç³»ã€‚</p>
<p><strong>Result:</strong> åœ¨ç²¾é€‰çš„è§†é¢‘-éŸ³é¢‘åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFoley Controlåœ¨æ—¶é—´å’Œè¯­ä¹‰å¯¹é½æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œè®­ç»ƒå‚æ•°è¿œå°‘äºæœ€è¿‘çš„å¤šæ¨¡æ€ç³»ç»Ÿï¼ŒåŒæ—¶ä¿æŒäº†æç¤ºé©±åŠ¨çš„å¯æ§æ€§å’Œç”Ÿäº§å‹å¥½çš„æ¨¡å—åŒ–ç‰¹æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è½»é‡çº§æ¡¥æ¥è®¾è®¡å¯ä»¥æœ‰æ•ˆè¿æ¥å†»ç»“çš„å•æ¨¡æ€æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€ç”Ÿæˆï¼Œè¿™ç§æ¡¥æ¥è®¾è®¡å…·æœ‰æ‰©å±•åˆ°å…¶ä»–éŸ³é¢‘æ¨¡æ€çš„æ½œåŠ›ï¼Œä¸ºæ¨¡å—åŒ–å¤šæ¨¡æ€ç³»ç»Ÿå¼€å‘æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Foley Control is a lightweight approach to video-guided Foley that keeps
pretrained single-modality models frozen and learns only a small
cross-attention bridge between them. We connect V-JEPA2 video embeddings to a
frozen Stable Audio Open DiT text-to-audio (T2A) model by inserting compact
video cross-attention after the model's existing text cross-attention, so
prompts set global semantics while video refines timing and local dynamics. The
frozen backbones retain strong marginals (video; audio given text) and the
bridge learns the audio-video dependency needed for synchronization -- without
retraining the audio prior. To cut memory and stabilize training, we pool video
tokens before conditioning. On curated video-audio benchmarks, Foley Control
delivers competitive temporal and semantic alignment with far fewer trainable
parameters than recent multi-modal systems, while preserving prompt-driven
controllability and production-friendly modularity (swap/upgrade encoders or
the T2A backbone without end-to-end retraining). Although we focus on
Video-to-Foley, the same bridge design can potentially extend to other audio
modalities (e.g., speech).</p>
<h3 id="26-s3od-towards-generalizable-salient-object-detection-with-synthetic-data">[26] <a href="https://arxiv.org/abs/2510.21605">S3OD: Towards Generalizable Salient Object Detection with Synthetic Data</a></h3>
<p><em>Orest Kupyn, Hirokatsu Kataoka, Christian Rupprecht</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¤§è§„æ¨¡åˆæˆæ•°æ®ç”Ÿæˆå’Œæ¨¡ç³Šæ„ŸçŸ¥æ¶æ„æ˜¾è‘—æå‡æ˜¾è‘—ç›®æ ‡æ£€æµ‹æ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ï¼Œåˆ›å»ºäº†åŒ…å«139,000å¼ é«˜åˆ†è¾¨ç‡å›¾åƒçš„S3ODæ•°æ®é›†ï¼Œåœ¨è·¨æ•°æ®é›†æ³›åŒ–ä¸­å®ç°äº†20-50%çš„é”™è¯¯ç‡é™ä½ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ˜¾è‘—ç›®æ ‡æ£€æµ‹é¢ä¸´æ•°æ®å—é™çš„é—®é¢˜ï¼Œæ˜‚è´µçš„åƒç´ çº§æ ‡æ³¨è¿«ä½¿ç›¸å…³å­ä»»åŠ¡å¦‚DISå’ŒHR-SODéœ€è¦åˆ†åˆ«è®­ç»ƒæ¨¡å‹ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨å¤šæ¨¡æ€æ‰©æ•£ç®¡é“ä»æ‰©æ•£å’ŒDINO-v3ç‰¹å¾ä¸­æå–æ ‡ç­¾ï¼Œæ„å»ºäº†åŒ…å«139,000å¼ é«˜åˆ†è¾¨ç‡å›¾åƒçš„S3ODæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†åŸºäºæ¨¡å‹æ€§èƒ½ä¼˜å…ˆå¤„ç†æŒ‘æˆ˜ç±»åˆ«çš„è¿­ä»£ç”Ÿæˆæ¡†æ¶ï¼Œä»¥åŠèƒ½å¤Ÿè‡ªç„¶å¤„ç†æ˜¾è‘—ç›®æ ‡æ£€æµ‹å›ºæœ‰æ¨¡ç³Šæ€§çš„ç®€åŒ–å¤šæ©ç è§£ç å™¨ã€‚</p>
<p><strong>Result:</strong> ä»…ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨è·¨æ•°æ®é›†æ³›åŒ–ä¸­å®ç°äº†20-50%çš„é”™è¯¯ç‡é™ä½ï¼Œç»è¿‡å¾®è°ƒçš„ç‰ˆæœ¬åœ¨DISå’ŒHR-SODåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Conclusion:</strong> å¤§è§„æ¨¡åˆæˆæ•°æ®ç”Ÿæˆä¸æ¨¡ç³Šæ„ŸçŸ¥æ¶æ„çš„ç»“åˆèƒ½å¤Ÿæ˜¾è‘—æå‡æ˜¾è‘—ç›®æ ‡æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ•°æ®å—é™çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åˆæˆæ•°æ®åœ¨æ¨åŠ¨æ¨¡å‹æ€§èƒ½è¾¹ç•Œæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Salient object detection exemplifies data-bounded tasks where expensive
pixel-precise annotations force separate model training for related subtasks
like DIS and HR-SOD. We present a method that dramatically improves
generalization through large-scale synthetic data generation and
ambiguity-aware architecture. We introduce S3OD, a dataset of over 139,000
high-resolution images created through our multi-modal diffusion pipeline that
extracts labels from diffusion and DINO-v3 features. The iterative generation
framework prioritizes challenging categories based on model performance. We
propose a streamlined multi-mask decoder that naturally handles the inherent
ambiguity in salient object detection by predicting multiple valid
interpretations. Models trained solely on synthetic data achieve 20-50% error
reduction in cross-dataset generalization, while fine-tuned versions reach
state-of-the-art performance across DIS and HR-SOD benchmarks.</p>
<h3 id="27-modest-align-data-efficient-alignment-for-vision-language-models">[27] <a href="https://arxiv.org/abs/2510.21606">Modest-Align: Data-Efficient Alignment for Vision-Language Models</a></h3>
<p><em>Jiaxiang Liu, Yuan Wang, Jiawei Du, Joey Tianyi Zhou, Mingkun Xu, Zuozhu Liu</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Modest-Alignï¼Œä¸€ç§è½»é‡çº§çš„è·¨æ¨¡æ€å¯¹é½æ¡†æ¶ï¼Œé€šè¿‡éšæœºæ‰°åŠ¨å’ŒåµŒå…¥å¹³æ»‘ä¸¤ç§äº’è¡¥ç­–ç•¥æ¥ç¼“è§£èµ„æºå—é™åœºæ™¯ä¸‹çš„è¿‡è‡ªä¿¡é—®é¢˜ï¼Œåœ¨ä»…ä½¿ç”¨1%è®­ç»ƒæ•°æ®å’Œ0.17%GPUæ—¶é—´çš„æƒ…å†µä¸‹å®ç°äº†ä¸CLIPç›¸ç«äº‰çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹ï¼Œå¤§è§„æ¨¡è·¨æ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹å¦‚CLIPé¢ä¸´è¿‡è‡ªä¿¡å’Œæ€§èƒ½é€€åŒ–é—®é¢˜ï¼Œè¿™ä¸»è¦æºäºæ•°æ®è´¨é‡ä½ã€å›¾åƒ-æ–‡æœ¬å¯¹å…³è”æ€§å¼±ä»¥åŠç°æœ‰å¯¹æ¯”å­¦ä¹ æ–¹æ³•å¯¹ä¸ç¡®å®šæ ·æœ¬çš„è¿‡åº¦å¼ºåŒ–ã€‚</p>
<p><strong>Method:</strong> Modest-Alignæ¡†æ¶é‡‡ç”¨ä¸¤ç§äº’è¡¥ç­–ç•¥ï¼šéšæœºæ‰°åŠ¨é€šè¿‡å¼•å…¥å—æ§å™ªå£°æ¨¡æ‹Ÿä¸ç¡®å®šæ€§ï¼ŒåµŒå…¥å¹³æ»‘é€šè¿‡æ ¡å‡†åµŒå…¥ç©ºé—´ä¸­çš„ç›¸ä¼¼åº¦åˆ†å¸ƒæ¥å‡å°‘è¿‡è‡ªä¿¡ï¼Œå…±åŒæå‡å¯¹å™ªå£°å’Œå¼±å¯¹é½æ ·æœ¬çš„é²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒModest-Alignåœ¨æ£€ç´¢ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œä»…ä½¿ç”¨CLIP 1%çš„è®­ç»ƒæ•°æ®å’Œ0.17%çš„GPUæ—¶é—´å°±å®ç°äº†ç«äº‰æ€§æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•ä¸ºç°å®ä¸–ç•Œä½èµ„æºåœºæ™¯ä¸‹çš„è·¨æ¨¡æ€å¯¹é½æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œè¯æ˜äº†é€šè¿‡è½»é‡çº§æ ¡å‡†æœºåˆ¶å¯ä»¥åœ¨æ˜¾è‘—å‡å°‘è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Cross-modal alignment aims to map heterogeneous modalities into a shared
latent space, as exemplified by models like CLIP, which benefit from
large-scale image-text pretraining for strong recognition capabilities.
However, when operating in resource-constrained settings with limited or
low-quality data, these models often suffer from overconfidence and degraded
performance due to the prevalence of ambiguous or weakly correlated image-text
pairs. Current contrastive learning approaches, which rely on single positive
pairs, further exacerbate this issue by reinforcing overconfidence on uncertain
samples. To address these challenges, we propose Modest-Align, a lightweight
alignment framework designed for robustness and efficiency. Our approach
leverages two complementary strategies -- Random Perturbation, which introduces
controlled noise to simulate uncertainty, and Embedding Smoothing, which
calibrates similarity distributions in the embedding space. These mechanisms
collectively reduce overconfidence and improve performance on noisy or weakly
aligned samples. Extensive experiments across multiple benchmark datasets
demonstrate that Modest-Align outperforms state-of-the-art methods in retrieval
tasks, achieving competitive results with over 100x less training data and 600x
less GPU time than CLIP. Our method offers a practical and scalable solution
for cross-modal alignment in real-world, low-resource scenarios.</p>
<h3 id="28-automated-detection-of-visual-attribute-reliance-with-a-self-reflective-agent">[28] <a href="https://arxiv.org/abs/2510.21704">Automated Detection of Visual Attribute Reliance with a Self-Reflective Agent</a></h3>
<p><em>Christy Li, Josep Lopez CamuÃ±as, Jake Thomas Touchet, Jacob Andreas, Agata Lapedriza, Antonio Torralba, Tamar Rott Shaham</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œé€šè¿‡è‡ªåæ€ä»£ç†ç³»ç»Ÿæ€§åœ°ç”Ÿæˆå’Œæµ‹è¯•è§†è§‰å±æ€§ä¾èµ–å‡è®¾ï¼Œç”¨äºæ£€æµ‹è®­ç»ƒå¥½çš„è§†è§‰æ¨¡å‹ä¸­çš„æ„å¤–ä¾èµ–å…³ç³»ã€‚è¯¥æ–¹æ³•åœ¨åŒ…å«130ä¸ªæ¨¡å‹çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºéåæ€åŸºçº¿ï¼Œå¹¶èƒ½è¯†åˆ«CLIPè§†è§‰ç¼–ç å™¨å’ŒYOLOv8ç­‰å…ˆè¿›æ¨¡å‹ä¸­çš„çœŸå®ä¸–ç•Œè§†è§‰å±æ€§ä¾èµ–ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰æ¨¡å‹åœ¨è¿›è¡Œå›¾åƒè¯†åˆ«æ—¶å¯èƒ½æ„å¤–ä¾èµ–ç‰¹å®šçš„è§†è§‰å±æ€§ï¼Œè¿™ç§ä¾èµ–æ€§ä¼šå¨èƒæ¨¡å‹é²æ£’æ€§ã€å¯¼è‡´è¿‡æ‹Ÿåˆå’Œè™šå‡ç›¸å…³æ€§ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹ç³»ç»ŸåŒ–çš„è‡ªåŠ¨åŒ–æ£€æµ‹æœºåˆ¶æ¥è¯†åˆ«è¿™äº›æ½œåœ¨ä¾èµ–å…³ç³»ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿè‡ªåŠ¨å‘ç°å’ŒéªŒè¯æ¨¡å‹ä¾èµ–æ¨¡å¼çš„æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> æ ¸å¿ƒæ–¹æ³•æ˜¯ä¸€ä¸ªè‡ªåæ€ä»£ç†ï¼Œå®ƒç³»ç»Ÿæ€§åœ°ç”Ÿæˆå…³äºæ¨¡å‹å¯èƒ½ä¾èµ–çš„è§†è§‰å±æ€§çš„å‡è®¾å¹¶è¿›è¡Œæµ‹è¯•ã€‚è¯¥è¿‡ç¨‹æ˜¯è¿­ä»£å¼çš„ï¼šä»£ç†æ ¹æ®å®éªŒç»“æœç²¾ç‚¼å‡è®¾ï¼Œå¹¶ä½¿ç”¨è‡ªè¯„ä¼°åè®®æ¥éªŒè¯å…¶å‘ç°æ˜¯å¦å‡†ç¡®è§£é‡Šæ¨¡å‹è¡Œä¸ºã€‚å½“å‡ºç°ä¸ä¸€è‡´æ—¶ï¼Œä»£ç†ä¼šè‡ªæˆ‘åæ€å¹¶è§¦å‘æ–°çš„å®éªŒå¾ªç¯ã€‚</p>
<p><strong>Result:</strong> åœ¨åŒ…å«130ä¸ªè®¾è®¡å…·æœ‰18ä¸ªç±»åˆ«ä¸åŒè§†è§‰å±æ€§ä¾èµ–çš„æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­ï¼Œè‡ªåæ€ä»£ç†çš„æ€§èƒ½éšç€åæ€è¿‡ç¨‹æŒç»­æå‡ï¼Œæ˜¾è‘—ä¼˜äºéåæ€åŸºçº¿ã€‚è¯¥æ–¹æ³•æˆåŠŸè¯†åˆ«äº†CLIPè§†è§‰ç¼–ç å™¨å’ŒYOLOv8ç›®æ ‡æ£€æµ‹å™¨ç­‰æœ€å…ˆè¿›æ¨¡å‹ä¸­çš„çœŸå®ä¸–ç•Œè§†è§‰å±æ€§ä¾èµ–ã€‚</p>
<p><strong>Conclusion:</strong> è‡ªåæ€æœºåˆ¶å¯¹äºç³»ç»ŸåŒ–æ£€æµ‹è§†è§‰æ¨¡å‹ä¸­çš„å±æ€§ä¾èµ–è‡³å…³é‡è¦ï¼Œè¯¥æ–¹æ³•ä¸ºç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹æä¾›äº†å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–å·¥å…·ã€‚ç ”ç©¶ç»“æœè¡¨æ˜è‡ªåæ€èƒ½å¤Ÿæ˜¾è‘—æå‡ä¾èµ–æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œä¸ºæ¨¡å‹å®¡è®¡å’Œé²æ£’æ€§è¯„ä¼°å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>When a vision model performs image recognition, which visual attributes drive
its predictions? Detecting unintended reliance on specific visual features is
critical for ensuring model robustness, preventing overfitting, and avoiding
spurious correlations. We introduce an automated framework for detecting such
dependencies in trained vision models. At the core of our method is a
self-reflective agent that systematically generates and tests hypotheses about
visual attributes that a model may rely on. This process is iterative: the
agent refines its hypotheses based on experimental outcomes and uses a
self-evaluation protocol to assess whether its findings accurately explain
model behavior. When inconsistencies arise, the agent self-reflects over its
findings and triggers a new cycle of experimentation. We evaluate our approach
on a novel benchmark of 130 models designed to exhibit diverse visual attribute
dependencies across 18 categories. Our results show that the agent's
performance consistently improves with self-reflection, with a significant
performance increase over non-reflective baselines. We further demonstrate that
the agent identifies real-world visual attribute dependencies in
state-of-the-art models, including CLIP's vision encoder and the YOLOv8 object
detector.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="29-reasonings-razor-reasoning-improves-accuracy-but-can-hurt-recall-at-critical-operating-points-in-safety-and-hallucination-detection">[29] <a href="https://arxiv.org/abs/2510.21049">Reasoning's Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical Operating Points in Safety and Hallucination Detection</a></h3>
<p><em>Atoosa Chegini, Hamid Kazemi, Garrett Souza, Maria Safi, Yang Song, Samy Bengio, Sinead Williamson, Mehrdad Farajtabar</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†åœ¨ä½è¯¯æŠ¥ç‡è¦æ±‚ä¸‹çš„æ¨ç†å¢å¼ºåˆ†ç±»ä»»åŠ¡ï¼Œå‘ç°æ¨ç†è™½ç„¶æå‡æ•´ä½“å‡†ç¡®ç‡ï¼Œä½†åœ¨ä¸¥æ ¼ç²¾åº¦æ•æ„Ÿåœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œæ­ç¤ºäº†æ¨ç†åœ¨ç²¾åº¦æ•æ„Ÿåº”ç”¨ä¸­çš„åŒåˆƒå‰‘ç‰¹æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡æ¨ç†å·²æˆä¸ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒèŒƒå¼å¹¶æ˜¾è‘—æå‡å¤šç§åŸºå‡†æµ‹è¯•çš„å‡†ç¡®ç‡ï¼Œä½†å…¶åœ¨ç²¾åº¦æ•æ„Ÿä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ä»ä¸æ˜ç¡®ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸¥æ ¼ä½è¯¯æŠ¥ç‡è¦æ±‚ä¸‹çš„åˆ†ç±»ä»»åŠ¡è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶è¦†ç›–å®‰å…¨æ£€æµ‹å’Œå¹»è§‰æ£€æµ‹ä¸¤ä¸ªä»»åŠ¡ï¼Œåœ¨å¾®è°ƒå’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹è¯„ä¼°æ ‡å‡†LLMå’Œå¤§å‹æ¨ç†æ¨¡å‹ï¼Œæ¯”è¾ƒæ¨ç†å¼€å¯ä¸æ¨ç†å…³é—­ä¸¤ç§æ¨¡å¼ï¼Œå¹¶åˆ†æåŸºäºtokençš„è¯„åˆ†ä¸è‡ªæˆ‘å£å¤´åŒ–ç½®ä¿¡åº¦çš„æ€§èƒ½å·®å¼‚ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºæ¨ç†å¼€å¯æ¨¡å¼è™½æå‡æ•´ä½“å‡†ç¡®ç‡ï¼Œä½†åœ¨ä½è¯¯æŠ¥ç‡é˜ˆå€¼ä¸‹è¡¨ç°ä¸ä½³ï¼›æ¨ç†å…³é—­æ¨¡å¼åœ¨ç²¾åº¦æ•æ„Ÿåœºæ™¯ä¸­å ä¼˜ï¼›åŸºäºtokençš„è¯„åˆ†æ˜¾è‘—ä¼˜äºè‡ªæˆ‘å£å¤´åŒ–ç½®ä¿¡åº¦ï¼›ä¸¤ç§æ¨¡å¼çš„ç®€å•é›†æˆå¯æ¢å¤å„è‡ªä¼˜åŠ¿ã€‚</p>
<p><strong>Conclusion:</strong> æ¨ç†åœ¨ç²¾åº¦æ•æ„Ÿåº”ç”¨ä¸­å…·æœ‰åŒåˆƒå‰‘ç‰¹æ€§ï¼šæœ‰ç›Šäºå¹³å‡å‡†ç¡®ç‡ï¼Œä½†å¾€å¾€ä¸é€‚ç”¨äºè¦æ±‚ä¸¥æ ¼ç²¾åº¦çš„å®é™…éƒ¨ç½²åœºæ™¯ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„æ¨¡å‹é€‰æ‹©æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Reasoning has become a central paradigm for large language models (LLMs),
consistently boosting accuracy across diverse benchmarks. Yet its suitability
for precision-sensitive tasks remains unclear. We present the first systematic
study of reasoning for classification tasks under strict low false positive
rate (FPR) regimes. Our analysis covers two tasks--safety detection and
hallucination detection--evaluated in both fine-tuned and zero-shot settings,
using standard LLMs and Large Reasoning Models (LRMs). Our results reveal a
clear trade-off: Think On (reasoning-augmented) generation improves overall
accuracy, but underperforms at the low-FPR thresholds essential for practical
use. In contrast, Think Off (no reasoning during inference) dominates in these
precision-sensitive regimes, with Think On surpassing only when higher FPRs are
acceptable. In addition, we find token-based scoring substantially outperforms
self-verbalized confidence for precision-sensitive deployments. Finally, a
simple ensemble of the two modes recovers the strengths of each. Taken
together, our findings position reasoning as a double-edged tool: beneficial
for average accuracy, but often ill-suited for applications requiring strict
precision.</p>
<h3 id="30-remoni-an-autonomous-system-integrating-wearables-and-multimodal-large-language-models-for-enhanced-remote-health-monitoring">[30] <a href="https://arxiv.org/abs/2510.21445">REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring</a></h3>
<p><em>Thanh Cong Ho, Farah Kharrat, Abderrazek Abid, Fakhri Karray</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºREMONIç³»ç»Ÿï¼Œé€šè¿‡æ•´åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€ç‰©è”ç½‘å’Œå¯ç©¿æˆ´è®¾å¤‡ï¼Œæ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿè‡ªä¸»ç›‘æµ‹æ‚£è€…ç”Ÿå‘½ä½“å¾ã€æ£€æµ‹å¼‚å¸¸çŠ¶æ€å¹¶è¿›è¡Œè‡ªç„¶è¯­è¨€äº¤äº’çš„è¿œç¨‹å¥åº·ç›‘æŠ¤ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿå®æ—¶åˆ†ææ‚£è€…æ´»åŠ¨ã€æƒ…ç»ªçŠ¶æ€ï¼Œå¹¶é€šè¿‡æ™ºèƒ½ä»£ç†ä¸ºåŒ»æŠ¤äººå‘˜æä¾›ç›´è§‚çš„å¥åº·çŠ¶æ€æŸ¥è¯¢ç•Œé¢ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è¿œç¨‹æ‚£è€…ç›‘æŠ¤ç ”ç©¶ä¸»è¦é›†ä¸­äºä¼ æ„Ÿå™¨æ•°æ®æ”¶é›†ã€å¯è§†åŒ–å’Œç‰¹å®šç–¾ç—…å¼‚å¸¸æ£€æµ‹ï¼Œä½†åœ¨äººæœºäº¤äº’æ–¹é¢å­˜åœ¨æ˜¾è‘—ç©ºç™½ã€‚ç°æœ‰ç³»ç»Ÿç¼ºä¹å¯¹æ‚£è€…æ´»åŠ¨çŠ¶æ€ã€æƒ…ç»ªå˜åŒ–çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œä»¥åŠåŒ»æŠ¤äººå‘˜ä¸ç›‘æŠ¤ç³»ç»Ÿä¹‹é—´çš„æ™ºèƒ½äº¤äº’æœºåˆ¶ã€‚</p>
<p><strong>Method:</strong> ç³»ç»Ÿæ•´åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€ç‰©è”ç½‘å’Œå¯ç©¿æˆ´è®¾å¤‡ï¼Œè‡ªåŠ¨æŒç»­æ”¶é›†ç”Ÿå‘½ä½“å¾ã€åŠ é€Ÿåº¦è®¡æ•°æ®å’Œæ‚£è€…è§†é¢‘ç‰‡æ®µã€‚é‡‡ç”¨å¼‚å¸¸æ£€æµ‹æ¨¡å—åŒ…æ‹¬è·Œå€’æ£€æµ‹æ¨¡å‹å’Œç´§æ€¥çŠ¶å†µè¯†åˆ«ç®—æ³•ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹æ— ç¼æ•´åˆæ‰€æœ‰æ‚£è€…ä¿¡æ¯ï¼Œå¹¶å¼€å‘äº†èƒ½å¤Ÿè¯†åˆ«æ‚£è€…æ´»åŠ¨å’Œæƒ…ç»ªçš„è‡ªç„¶è¯­è¨€å¤„ç†ç»„ä»¶ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¯æ˜è¯¥ç³»ç»Ÿåœ¨ç°å®åœºæ™¯ä¸­å…·æœ‰å¯å®æ–½æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¼€å‘äº†å®Œæ•´åŠŸèƒ½åŸå‹å¹¶æ­£åœ¨è¿›è¡Œæµ‹è¯•ï¼ŒéªŒè¯äº†ç³»ç»Ÿå„é¡¹èƒ½åŠ›çš„ç¨³å¥æ€§ã€‚ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘åŒ»æŠ¤äººå‘˜å·¥ä½œè´Ÿæ‹…å’ŒåŒ»ç–—æˆæœ¬ï¼Œé€šè¿‡ç”¨æˆ·å‹å¥½çš„Webåº”ç”¨ä¸ºåŒ»ç”ŸæŠ¤å£«æä¾›å®æ—¶ç”Ÿå‘½ä½“å¾å’Œæ‚£è€…çŠ¶æ€ä¿¡æ¯ã€‚</p>
<p><strong>Conclusion:</strong> REMONIç³»ç»Ÿå±•ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è¿œç¨‹å¥åº·ç›‘æŠ¤ä¸­çš„å®é™…åº”ç”¨ä»·å€¼ï¼Œä¸ºæ™ºèƒ½åŒ»ç–—ç›‘æŠ¤æä¾›äº†æ–°çš„æŠ€æœ¯èŒƒå¼ã€‚è¯¥ç³»ç»Ÿæœ‰æœ›é€šè¿‡è‡ªåŠ¨åŒ–ç›‘æµ‹å’Œæ™ºèƒ½äº¤äº’æ˜¾è‘—æå‡åŒ»ç–—ç›‘æŠ¤æ•ˆç‡ï¼Œä¸ºæœªæ¥æ™ºèƒ½åŒ»ç–—ç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>With the widespread adoption of wearable devices in our daily lives, the
demand and appeal for remote patient monitoring have significantly increased.
Most research in this field has concentrated on collecting sensor data,
visualizing it, and analyzing it to detect anomalies in specific diseases such
as diabetes, heart disease and depression. However, this domain has a notable
gap in the aspect of human-machine interaction. This paper proposes REMONI, an
autonomous REmote health MONItoring system that integrates multimodal large
language models (MLLMs), the Internet of Things (IoT), and wearable devices.
The system automatically and continuously collects vital signs, accelerometer
data from a special wearable (such as a smartwatch), and visual data in patient
video clips collected from cameras. This data is processed by an anomaly
detection module, which includes a fall detection model and algorithms to
identify and alert caregivers of the patient's emergency conditions. A
distinctive feature of our proposed system is the natural language processing
component, developed with MLLMs capable of detecting and recognizing a
patient's activity and emotion while responding to healthcare worker's
inquiries. Additionally, prompt engineering is employed to integrate all
patient information seamlessly. As a result, doctors and nurses can access
real-time vital signs and the patient's current state and mood by interacting
with an intelligent agent through a user-friendly web application. Our
experiments demonstrate that our system is implementable and scalable for
real-life scenarios, potentially reducing the workload of medical professionals
and healthcare costs. A full-fledged prototype illustrating the functionalities
of the system has been developed and being tested to demonstrate the robustness
of its various capabilities.</p>
<h3 id="31-document-understanding-measurement-and-manipulation-using-category-theory">[31] <a href="https://arxiv.org/abs/2510.21553">Document Understanding, Measurement, and Manipulation Using Category Theory</a></h3>
<p><em>Jared Claypoole, Yunye Gong, Noson S. Yanofsky, Ajay Divakaran</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºèŒƒç•´è®ºçš„æ–‡æ¡£ç»“æ„åˆ†ææ–¹æ³•ï¼Œå¼€å‘äº†ä¿¡æ¯ç†è®ºåº¦é‡ã€å†…å®¹æ‘˜è¦ä¸æ‰©å±•æŠ€æœ¯ï¼Œä»¥åŠåˆ©ç”¨RLVRçš„è‡ªç›‘ç£æ–¹æ³•æ¥æ”¹è¿›å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ–‡æ¡£è¡¨ç¤ºä¸ºé—®ç­”å¯¹çš„èŒƒç•´ï¼Œå®ç°äº†æ–‡æ¡£ä¿¡æ¯çš„æ­£äº¤åŒ–åˆ†è§£å’Œæ–°å‹æ‘˜è¦ç”Ÿæˆèƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æ¡£ç†è§£æ–¹æ³•ç¼ºä¹å¯¹æ–‡æ¡£å†…åœ¨ç»“æ„çš„æ•°å­¦å½¢å¼åŒ–è¡¨ç¤ºï¼Œéš¾ä»¥ç³»ç»Ÿæ€§åœ°æå–ã€åº¦é‡å’Œæ“ä½œæ–‡æ¡£ä¿¡æ¯ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡èŒƒç•´è®ºå»ºç«‹æ–‡æ¡£çš„æ•°å­¦è¡¨ç¤ºæ¡†æ¶ï¼Œè§£å†³æ–‡æ¡£ä¿¡æ¯åˆ†è§£ã€æ‘˜è¦ç”Ÿæˆå’Œå†…å®¹æ‰©å±•ç­‰æ ¸å¿ƒé—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºå°†æ–‡æ¡£å»ºæ¨¡ä¸ºé—®ç­”å¯¹èŒƒç•´çš„æ•°å­¦è¡¨ç¤ºï¼Œå¼€å‘äº†ä¿¡æ¯æ­£äº¤åŒ–ç¨‹åºå°†æ–‡æ¡£ä¿¡æ¯åˆ†è§£ä¸ºä¸é‡å çš„éƒ¨åˆ†ã€‚åŸºäºæ­¤æ¡†æ¶æ„å»ºäº†ä¿¡æ¯åº¦é‡æ–¹æ³•ã€æ–°å‹æ‘˜è¦æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨RLVRå¼€å‘äº†è‡ªç›‘ç£æ–¹æ³•ï¼Œé€šè¿‡ç»„åˆæ€§å’Œé—­åŒ…ç­‰ä¸€è‡´æ€§çº¦æŸæ¥æ”¹è¿›é¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> å®ç°äº†æ–‡æ¡£ä¿¡æ¯çš„ç³»ç»Ÿåˆ†è§£å’Œé‡åŒ–åº¦é‡ï¼Œå¼€å‘äº†åŸºäºç‡å¤±çœŸåˆ†æçš„æ‘˜è¦è¯„ä¼°æ¡†æ¶ï¼Œæå‡ºäº†æ–‡æ¡£æ³¨é‡Šæ‰©å±•çš„æ–°è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å¤§å‹é¢„è®­ç»ƒæ¨¡å‹å®ç°äº†æ–¹æ³•çš„å®é™…åº”ç”¨ï¼Œå¹¶æ„å»ºäº†å¤šæ¨¡æ€æ‰©å±•çš„æ•°å­¦æ¡†æ¶ã€‚</p>
<p><strong>Conclusion:</strong> èŒƒç•´è®ºä¸ºæ–‡æ¡£ç†è§£æä¾›äº†å¼ºå¤§çš„æ•°å­¦åŸºç¡€ï¼Œæ”¯æŒç³»ç»ŸåŒ–çš„ä¿¡æ¯æå–å’Œæ“ä½œã€‚é—®ç­”å¯¹è¡¨ç¤ºå’Œæ­£äº¤åŒ–åˆ†è§£ä¸ºæ–‡æ¡£åˆ†æå¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œè‡ªç›‘ç£æ–¹æ³•å±•ç¤ºäº†åˆ©ç”¨ç»“æ„çº¦æŸæ”¹è¿›æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šæ¨¡æ€æ–‡æ¡£å¤„ç†å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>We apply category theory to extract multimodal document structure which leads
us to develop information theoretic measures, content summarization and
extension, and self-supervised improvement of large pretrained models. We first
develop a mathematical representation of a document as a category of
question-answer pairs. Second, we develop an orthogonalization procedure to
divide the information contained in one or more documents into non-overlapping
pieces. The structures extracted in the first and second steps lead us to
develop methods to measure and enumerate the information contained in a
document. We also build on those steps to develop new summarization techniques,
as well as to develop a solution to a new problem viz. exegesis resulting in an
extension of the original document. Our question-answer pair methodology
enables a novel rate distortion analysis of summarization techniques. We
implement our techniques using large pretrained models, and we propose a
multimodal extension of our overall mathematical framework. Finally, we develop
a novel self-supervised method using RLVR to improve large pretrained models
using consistency constraints such as composability and closure under certain
operations that stem naturally from our category theoretic framework.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="32-sketch2bim-a-multi-agent-human-ai-collaborative-pipeline-to-convert-hand-drawn-floor-plans-to-3d-bim">[32] <a href="https://arxiv.org/abs/2510.20838">Sketch2BIM: A Multi-Agent Human-AI Collaborative Pipeline to Convert Hand-Drawn Floor Plans to 3D BIM</a></h3>
<p><em>Abir Khan Ratul, Sanjay Acharjee, Somin Park, Md Nazmus Sakib</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§äººæœºååŒç®¡é“ï¼Œå°†æœªç¼©æ”¾æ‰‹ç»˜å¹³é¢å›¾è½¬æ¢ä¸ºè¯­ä¹‰ä¸€è‡´çš„3D BIMæ¨¡å‹ã€‚è¯¥å·¥ä½œæµç¨‹åœ¨å¤šæ™ºèƒ½ä½“æ¡†æ¶ä¸­åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ„ŸçŸ¥æå–ã€äººå·¥åé¦ˆã€æ¨¡å¼éªŒè¯å’Œè‡ªåŠ¨åŒ–BIMè„šæœ¬ç”Ÿæˆå®ç°é«˜æ•ˆè½¬æ¢ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä¼ ç»ŸBIMåˆ›å»ºè¿‡ç¨‹å¯¹ä¸“ä¸šçŸ¥è¯†å’Œå¤æ‚è½¯ä»¶ä¾èµ–çš„é—®é¢˜ï¼Œä½¿å¾—éä¸“ä¸šäººå£«ä¹Ÿèƒ½é€šè¿‡ç®€å•çš„æ‰‹ç»˜è‰å›¾ç”Ÿæˆé«˜è´¨é‡çš„3Då»ºç­‘ä¿¡æ¯æ¨¡å‹ï¼Œé™ä½BIMæŠ€æœ¯çš„ä½¿ç”¨é—¨æ§›ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨å¤šæ™ºèƒ½ä½“æ¡†æ¶ç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…å«æ„ŸçŸ¥æå–ã€äººå·¥åé¦ˆè¿­ä»£ã€æ¨¡å¼éªŒè¯å’Œè‡ªåŠ¨åŒ–BIMè„šæœ¬ç”Ÿæˆå››ä¸ªå…³é”®é˜¶æ®µã€‚é¦–å…ˆå°†è‰å›¾è¿­ä»£ä¼˜åŒ–ä¸ºç»“æ„åŒ–çš„JSONå¸ƒå±€ï¼Œç„¶åè½¬æ¢ä¸ºå¯æ‰§è¡Œè„šæœ¬ç”Ÿæˆ3D BIMæ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> åœ¨åä¸ªä¸åŒå¹³é¢å›¾ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œé—¨çª—ç­‰å¼€å£å…ƒç´ åœ¨åˆå§‹é˜¶æ®µå³è¾¾åˆ°é«˜å¯é æ€§æ£€æµ‹ï¼Œå¢™ä½“æ£€æµ‹ä»83%å¼€å§‹å¹¶é€šè¿‡å‡ æ¬¡åé¦ˆè¿­ä»£å®ç°è¿‘ä¹å®Œç¾çš„å¯¹é½ã€‚æ‰€æœ‰ç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°å‡ä¿æŒåœ¨0.83ä»¥ä¸Šï¼Œå‡ ä½•è¯¯å·®é€šè¿‡åé¦ˆä¿®æ­£é€æ­¥é™è‡³é›¶ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜MLLMé©±åŠ¨çš„å¤šæ™ºèƒ½ä½“æ¨ç†èƒ½å¤Ÿä½¿BIMåˆ›å»ºå¯¹ä¸“å®¶å’Œéä¸“å®¶éƒ½å˜å¾—å¯è®¿é—®ï¼Œä»…éœ€ä½¿ç”¨æ‰‹ç»˜è‰å›¾å³å¯å®Œæˆã€‚è¿™é¡¹å·¥ä½œä¸ºå»ºç­‘è¡Œä¸šçš„æ•°å­—åŒ–æä¾›äº†æ–°çš„ä½é—¨æ§›è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å¤šæ¨¡æ€AIåœ¨ä¸“ä¸šé¢†åŸŸåº”ç”¨çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>This study introduces a human-in-the-loop pipeline that converts unscaled,
hand-drawn floor plan sketches into semantically consistent 3D BIM models. The
workflow leverages multimodal large language models (MLLMs) within a
multi-agent framework, combining perceptual extraction, human feedback, schema
validation, and automated BIM scripting. Initially, sketches are iteratively
refined into a structured JSON layout of walls, doors, and windows. Later,
these layouts are transformed into executable scripts that generate 3D BIM
models. Experiments on ten diverse floor plans demonstrate strong convergence:
openings (doors, windows) are captured with high reliability in the initial
pass, while wall detection begins around 83% and achieves near-perfect
alignment after a few feedback iterations. Across all categories, precision,
recall, and F1 scores remain above 0.83, and geometric errors (RMSE, MAE)
progressively decrease to zero through feedback corrections. This study
demonstrates how MLLM-driven multi-agent reasoning can make BIM creation
accessible to both experts and non-experts using only freehand sketches.</p>
<h3 id="33-medalign-a-synergistic-framework-of-multimodal-preference-optimization-and-federated-meta-cognitive-reasoning">[33] <a href="https://arxiv.org/abs/2510.21093">MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning</a></h3>
<p><em>Siyong Chen, Jinbo Wen, Jiawen Kang, Tenghui Huang, Xumin Huang, Yuanjia Su, Hudan Pan, Zishao Zhong, Dusit Niyato, Shengli Xie, Dong In Kim</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MedAlignæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€ç›´æ¥åå¥½ä¼˜åŒ–å’Œæ£€ç´¢æ„ŸçŸ¥çš„ä¸“å®¶æ··åˆæ¶æ„æ¥è§£å†³åŒ»ç–—è§†è§‰é—®ç­”ä¸­LVLMçš„å¹»è§‰é—®é¢˜ï¼ŒåŒæ—¶å®ç°è‡ªé€‚åº”æ¨ç†å’Œå¤šæœºæ„åä½œï¼Œåœ¨ä¸‰ä¸ªMed-VQAæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠæœåŠ¡éƒ¨ç½²ä¸­é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šåŸºäºè§†è§‰è¯æ®çš„å¹»è§‰å€¾å‘ã€å›ºå®šæ·±åº¦æ¨ç†çš„ä½æ•ˆæ€§ä»¥åŠå¤šæœºæ„åä½œçš„å›°éš¾ï¼Œè¿™äº›é™åˆ¶äº†LVLMåœ¨æ™ºèƒ½åŒ»ç–—é¢†åŸŸçš„å®é™…åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> æå‡ºå¤šæ¨¡æ€ç›´æ¥åå¥½ä¼˜åŒ–ç›®æ ‡å°†åå¥½å­¦ä¹ ä¸è§†è§‰ä¸Šä¸‹æ–‡æ˜¾å¼å¯¹é½ï¼Œè®¾è®¡æ£€ç´¢æ„ŸçŸ¥ä¸“å®¶æ··åˆæ¶æ„åˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬ç›¸ä¼¼æ€§å°†æŸ¥è¯¢è·¯ç”±åˆ°ä¸“é—¨çš„ä¸Šä¸‹æ–‡å¢å¼ºLVLMä¸“å®¶ï¼Œå¹¶é‡‡ç”¨è”é‚¦æ²»ç†æœºåˆ¶å®ç°åŸºäºæœ¬åœ°å…ƒè®¤çŸ¥ä¸ç¡®å®šæ€§ä¼°è®¡å™¨çš„è‡ªé€‚åº”é“¾å¼æ€ç»´æ¨ç†ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªä»£è¡¨æ€§Med-VQAæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMedAlignå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¯”å¼ºæ£€ç´¢å¢å¼ºåŸºçº¿åœ¨F1åˆ†æ•°ä¸Šæå‡é«˜è¾¾11.85%ï¼ŒåŒæ—¶ä¸å›ºå®šæ·±åº¦CoTæ–¹æ³•ç›¸æ¯”å¹³å‡æ¨ç†é•¿åº¦å‡å°‘51.60%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è§†è§‰ä¸Šä¸‹æ–‡å¯¹é½ã€ä¸“å®¶è·¯ç”±å’Œè”é‚¦æ²»ç†æœºåˆ¶å¯ä»¥æœ‰æ•ˆè§£å†³LVLMåœ¨åŒ»ç–—é¢†åŸŸçš„å¹»è§‰é—®é¢˜ï¼Œä¸ºå¤šæœºæ„ä¸´åºŠåä½œæä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œæ¨åŠ¨äº†æ™ºèƒ½åŒ»ç–—ä¸­å¤§å‹æ¨¡å‹çš„å®é™…éƒ¨ç½²ã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Recently, large models have shown significant potential for smart healthcare.
However, the deployment of Large Vision-Language Models (LVLMs) for clinical
services is currently hindered by three critical challenges: a tendency to
hallucinate answers not grounded in visual evidence, the inefficiency of
fixed-depth reasoning, and the difficulty of multi-institutional collaboration.
To address these challenges, in this paper, we develop MedAlign, a novel
framework to ensure visually accurate LVLM responses for Medical Visual
Question Answering (Med-VQA). Specifically, we first propose a multimodal
Direct Preference Optimization (mDPO) objective to explicitly align preference
learning with visual context. We then design a Retrieval-Aware
Mixture-of-Experts (RA-MoE) architecture that utilizes image and text
similarity to route queries to a specialized and context-augmented LVLM (i.e.,
an expert), thereby mitigating hallucinations in LVLMs. To achieve adaptive
reasoning and facilitate multi-institutional collaboration, we propose a
federated governance mechanism, where the selected expert, fine-tuned on
clinical datasets based on mDPO, locally performs iterative Chain-of-Thought
(CoT) reasoning via the local meta-cognitive uncertainty estimator. Extensive
experiments on three representative Med-VQA datasets demonstrate that MedAlign
achieves state-of-the-art performance, outperforming strong retrieval-augmented
baselines by up to $11.85\%$ in F1-score, and simultaneously reducing the
average reasoning length by $51.60\%$ compared with fixed-depth CoT approaches.</p>
<h3 id="34-memory-free-continual-learning-with-null-space-adaptation-for-zero-shot-vision-language-models">[34] <a href="https://arxiv.org/abs/2510.21175">Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models</a></h3>
<p><em>Yujin Jo, Taesup Kim</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†NuSA-CLæ¡†æ¶ï¼Œä¸€ç§è½»é‡çº§æ— å†…å­˜çš„æŒç»­å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡é›¶ç©ºé—´é€‚åº”ç­–ç•¥åœ¨ä¿æŒé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹é›¶æ ·æœ¬èƒ½åŠ›çš„åŒæ—¶å®ç°æŒç»­å­¦ä¹ ï¼Œè§£å†³äº†å®é™…éƒ¨ç½²ä¸­åˆ†å¸ƒåç§»å’Œæ–°ä»»åŠ¡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç°å®éƒ¨ç½²ä¸­é¢ä¸´ç¯å¢ƒæ¼”å˜å’Œæ–°å…´ç±»åˆ«å¸¦æ¥çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œé™æ€é›¶æ ·æœ¬èƒ½åŠ›ä¸è¶³ï¼Œéœ€è¦æŒç»­å­¦ä¹ æ–¹æ³•åœ¨ä¸å¼•å‘ç¾éš¾æ€§é—å¿˜çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹é€‚åº”ã€‚</p>
<p><strong>Method:</strong> NuSA-CLé‡‡ç”¨ä½ç§©é€‚åº”æŠ€æœ¯ï¼Œå°†ä»»åŠ¡ç‰¹å®šçš„æƒé‡æ›´æ–°çº¦æŸåœ¨æ¨¡å‹å½“å‰å‚æ•°çš„è¿‘ä¼¼é›¶ç©ºé—´å†…ï¼Œè¿™ç§ç­–ç•¥æœ€å°åŒ–å¯¹å·²è·å–çŸ¥è¯†çš„å¹²æ‰°ï¼Œæœ‰æ•ˆä¿ç•™åŸå§‹æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜è¯¥æ¡†æ¶ä¸ä»…æœ‰æ•ˆä¿æŒäº†é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼Œè¿˜åœ¨æŒç»­å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> NuSA-CLä½œä¸ºä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºç°å®åº”ç”¨ä¸­æŒç»­æ¼”åŒ–çš„é›¶æ ·æœ¬è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†æœ‰æ•ˆçš„æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œå…·æœ‰è¾ƒä½çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated
remarkable zero-shot generalization, enabling deployment in a wide range of
real-world tasks without additional task-specific training. However, in real
deployment scenarios with evolving environments or emerging classes, these
models inevitably face distributional shifts and novel tasks. In such contexts,
static zero-shot capabilities are insufficient, and there is a growing need for
continual learning methods that allow models to adapt over time while avoiding
catastrophic forgetting. We introduce NuSA-CL (Null Space Adaptation for
Continual Learning), a lightweight memory-free continual learning framework
designed to address this challenge. NuSA-CL employs low-rank adaptation and
constrains task-specific weight updates to lie within an approximate null space
of the model's current parameters. This strategy minimizes interference with
previously acquired knowledge, effectively preserving the zero-shot
capabilities of the original model. Unlike methods relying on replay buffers or
costly distillation, NuSA-CL imposes minimal computational and memory overhead,
making it practical for deployment in resource-constrained, real-world
continual learning environments. Experiments show that our framework not only
effectively preserves zero-shot transfer capabilities but also achieves highly
competitive performance on continual learning benchmarks. These results
position NuSA-CL as a practical and scalable solution for continually evolving
zero-shot VLMs in real-world applications.</p>
<h3 id="35-a-multimodal-benchmark-for-framing-of-oil-gas-advertising-and-potential-greenwashing-detection">[35] <a href="https://arxiv.org/abs/2510.21679">A Multimodal Benchmark for Framing of Oil &amp; Gas Advertising and Potential Greenwashing Detection</a></h3>
<p><em>Gaku Morio, Harri Rowlands, Dominik Stammbach, Christopher D. Manning, Peter Henderson</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªä¸“å®¶æ ‡æ³¨çš„å¤šæ¨¡æ€è§†é¢‘å¹¿å‘ŠåŸºå‡†æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨èƒ½æºè¡Œä¸šæˆ˜ç•¥ä¼ æ’­åˆ†æä¸­çš„è¡¨ç°ï¼Œå¡«è¡¥äº†ç°æœ‰çº¯æ–‡æœ¬æ¡†æ¶åˆ†ææ•°æ®é›†çš„ç©ºç™½ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ä¼ä¸šå…¬å…³æ´»åŠ¨ä¸­å­˜åœ¨è¨€è¡Œä¸ä¸€çš„"æ¼‚ç»¿"ç°è±¡ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ³æ²¹å¤©ç„¶æ°”è¡Œä¸šï¼Œéœ€è¦å¤§è§„æ¨¡åˆ†ææ¡†æ¶å˜åŒ–æ¥ç†è§£å…¬å…³æ´»åŠ¨çš„ç›®æ ‡å’Œæ€§è´¨ï¼Œè€Œç°æœ‰æ•°æ®é›†å¤šä¸ºçº¯æ–‡æœ¬å½¢å¼ï¼Œæ— æ³•æ»¡è¶³å¤šæ¨¡æ€åˆ†æéœ€æ±‚ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†ä¸€ä¸ªä¸“å®¶æ ‡æ³¨çš„è§†é¢‘å¹¿å‘Šæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªFacebookå’ŒYouTubeçš„å¹¿å‘Šè§†é¢‘ï¼Œæ¶µç›–13ç§æ¡†æ¶ç±»å‹ã€50å¤šå®¶å…¬å¸æˆ–å€¡å¯¼å›¢ä½“ã€20ä¸ªå›½å®¶ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> åŸºçº¿å®éªŒæ˜¾ç¤ºGPT-4.1åœ¨æ£€æµ‹ç¯å¢ƒä¿¡æ¯æ–¹é¢è¾¾åˆ°79%çš„F1åˆ†æ•°ï¼Œè€Œæœ€ä½³æ¨¡å‹åœ¨è¯†åˆ«ç»¿è‰²åˆ›æ–°æ¡†æ¶æ–¹é¢ä»…è¾¾åˆ°46%çš„F1åˆ†æ•°ï¼Œè¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¡†æ¶åˆ†ææ–¹é¢ä»æœ‰è¾ƒå¤§æ”¹è¿›ç©ºé—´ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ•°æ®é›†ä¸ºèƒ½æºè¡Œä¸šæˆ˜ç•¥ä¼ æ’­çš„å¤šæ¨¡æ€åˆ†æç ”ç©¶æä¾›äº†é‡è¦èµ„æºï¼ŒåŒæ—¶æ­ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éšå¼æ¡†æ¶ã€ä¸åŒé•¿åº¦è§†é¢‘å’Œéšå«æ–‡åŒ–èƒŒæ™¯ç­‰æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Companies spend large amounts of money on public relations campaigns to
project a positive brand image. However, sometimes there is a mismatch between
what they say and what they do. Oil &amp; gas companies, for example, are accused
of "greenwashing" with imagery of climate-friendly initiatives. Understanding
the framing, and changes in framing, at scale can help better understand the
goals and nature of public relations campaigns. To address this, we introduce a
benchmark dataset of expert-annotated video ads obtained from Facebook and
YouTube. The dataset provides annotations for 13 framing types for more than 50
companies or advocacy groups across 20 countries. Our dataset is especially
designed for the evaluation of vision-language models (VLMs), distinguishing it
from past text-only framing datasets. Baseline experiments show some promising
results, while leaving room for improvement for future work: GPT-4.1 can detect
environmental messages with 79% F1 score, while our best model only achieves
46% F1 score on identifying framing around green innovation. We also identify
challenges that VLMs must address, such as implicit framing, handling videos of
various lengths, or implicit cultural backgrounds. Our dataset contributes to
research in multimodal analysis of strategic communication in the energy
sector.</p>
  </article>
</body>
</html>
