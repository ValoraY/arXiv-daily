{"id": "2511.16681", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16681", "abs": "https://arxiv.org/abs/2511.16681", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search", "comment": "Accepted to IEEE International Conference on Parallel and Distributed Systems 2025 (ICPADS 2025 Oral)", "summary": "Retrieval-Augmented Generation (RAG) systems have become a dominant approach to augment large language models (LLMs) with external knowledge. However, existing vector database (VecDB) retrieval pipelines rely on flat or single-resolution indexing structures, which cannot adapt to the varying semantic granularity required by diverse user queries. This limitation leads to suboptimal trade-offs between retrieval speed and contextual relevance.\n  To address this, we propose \\textbf{Semantic Pyramid Indexing (SPI)}, a novel multi-resolution vector indexing framework that introduces query-adaptive resolution control for RAG in VecDBs. Unlike existing hierarchical methods that require offline tuning or separate model training, SPI constructs a semantic pyramid over document embeddings and dynamically selects the optimal resolution level per query through a lightweight classifier. This adaptive approach enables progressive retrieval from coarse-to-fine representations, significantly accelerating search while maintaining semantic coverage.\n  We implement SPI as a plugin for both FAISS and Qdrant backends and evaluate it across multiple RAG tasks including MS MARCO, Natural Questions, and multimodal retrieval benchmarks. SPI achieves up to \\textbf{5.7$\\times$} retrieval speedup and \\textbf{1.8$\\times$} memory efficiency gain while improving end-to-end QA F1 scores by up to \\textbf{2.5 points} compared to strong baselines. Our theoretical analysis provides guarantees on retrieval quality and latency bounds, while extensive ablation studies validate the contribution of each component. The framework's compatibility with existing VecDB infrastructures makes it readily deployable in production RAG systems. Code is availabe at \\href{https://github.com/FastLM/SPI_VecDB}{https://github.com/FastLM/SPI\\_VecDB}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8bed\u4e49\u91d1\u5b57\u5854\u7d22\u5f15\uff08SPI\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u5411\u91cf\u6570\u636e\u5e93\u7684\u591a\u5206\u8fa8\u7387\u7d22\u5f15\u6846\u67b6\uff0c\u901a\u8fc7\u67e5\u8be2\u81ea\u9002\u5e94\u5206\u8fa8\u7387\u63a7\u5236\u5b9e\u73b0\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u52a0\u901f\u548c\u4f18\u5316\u3002SPI\u5728\u4fdd\u6301\u8bed\u4e49\u8986\u76d6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u68c0\u7d22\u901f\u5ea6\uff0c\u5e76\u4e0e\u73b0\u6709\u5411\u91cf\u6570\u636e\u5e93\u57fa\u7840\u8bbe\u65bd\u517c\u5bb9\u3002", "motivation": "\u73b0\u6709\u5411\u91cf\u6570\u636e\u5e93\u68c0\u7d22\u7ba1\u9053\u4f9d\u8d56\u6241\u5e73\u6216\u5355\u5206\u8fa8\u7387\u7d22\u5f15\u7ed3\u6784\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u7528\u6237\u67e5\u8be2\u6240\u9700\u7684\u8bed\u4e49\u7c92\u5ea6\u53d8\u5316\uff0c\u5bfc\u81f4\u68c0\u7d22\u901f\u5ea6\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u4e4b\u95f4\u7684\u6b21\u4f18\u6743\u8861\u3002\u4f20\u7edf\u5206\u5c42\u65b9\u6cd5\u9700\u8981\u79bb\u7ebf\u8c03\u4f18\u6216\u5355\u72ec\u6a21\u578b\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u7075\u6d3b\u6027\u3002", "method": "SPI\u6784\u5efa\u6587\u6863\u5d4c\u5165\u7684\u8bed\u4e49\u91d1\u5b57\u5854\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u52a8\u6001\u9009\u62e9\u6700\u4f18\u5206\u8fa8\u7387\u7ea7\u522b\uff0c\u5b9e\u73b0\u4ece\u7c97\u5230\u7ec6\u8868\u793a\u7684\u6e10\u8fdb\u5f0f\u68c0\u7d22\u3002\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u63d2\u4ef6\u5b9e\u73b0\uff0c\u517c\u5bb9FAISS\u548cQdrant\u540e\u7aef\uff0c\u65e0\u9700\u79bb\u7ebf\u8c03\u4f18\u6216\u989d\u5916\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728MS MARCO\u3001Natural Questions\u548c\u591a\u6a21\u6001\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPI\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.7\u500d\u7684\u68c0\u7d22\u52a0\u901f\u548c1.8\u500d\u7684\u5185\u5b58\u6548\u7387\u63d0\u5347\uff0c\u540c\u65f6\u5c06\u7aef\u5230\u7aefQA F1\u5206\u6570\u63d0\u9ad8\u4e86\u6700\u591a2.5\u5206\u3002\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u68c0\u7d22\u8d28\u91cf\u548c\u5ef6\u8fdf\u8fb9\u754c\u4fdd\u8bc1\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u8d21\u732e\u3002", "conclusion": "SPI\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u591a\u5206\u8fa8\u7387\u7d22\u5f15\u89e3\u51b3\u4e86RAG\u7cfb\u7edf\u4e2d\u68c0\u7d22\u6548\u7387\u4e0e\u8d28\u91cf\u7684\u5173\u952e\u6743\u8861\u95ee\u9898\uff0c\u5176\u4e0e\u73b0\u6709\u5411\u91cf\u6570\u636e\u5e93\u57fa\u7840\u8bbe\u65bd\u7684\u517c\u5bb9\u6027\u4f7f\u5176\u53ef\u76f4\u63a5\u90e8\u7f72\u4e8e\u751f\u4ea7\u7cfb\u7edf\u3002\u8be5\u5de5\u4f5c\u4e3a\u5927\u89c4\u6a21\u77e5\u8bc6\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16685", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16685", "abs": "https://arxiv.org/abs/2511.16685", "authors": ["Yuetian Zou", "Hanlei Zhang", "Hua Xu", "Songze Li", "Long Xiao"], "title": "Ellipsoid-Based Decision Boundaries for Open Intent Classification", "comment": null, "summary": "Textual open intent classification is crucial for real-world dialogue systems, enabling robust detection of unknown user intents without prior knowledge and contributing to the robustness of the system. While adaptive decision boundary methods have shown great potential by eliminating manual threshold tuning, existing approaches assume isotropic distributions of known classes, restricting boundaries to balls and overlooking distributional variance along different directions. To address this limitation, we propose EliDecide, a novel method that learns ellipsoid decision boundaries with varying scales along different feature directions. First, we employ supervised contrastive learning to obtain a discriminative feature space for known samples. Second, we apply learnable matrices to parameterize ellipsoids as the boundaries of each known class, offering greater flexibility than spherical boundaries defined solely by centers and radii. Third, we optimize the boundaries via a novelly designed dual loss function that balances empirical and open-space risks: expanding boundaries to cover known samples while contracting them against synthesized pseudo-open samples. Our method achieves state-of-the-art performance on multiple text intent benchmarks and further on a question classification dataset. The flexibility of the ellipsoids demonstrates superior open intent detection capability and strong potential for generalization to more text classification tasks in diverse complex open-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEliDecide\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u692d\u5706\u51b3\u7b56\u8fb9\u754c\u6765\u89e3\u51b3\u6587\u672c\u5f00\u653e\u610f\u56fe\u5206\u7c7b\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u83b7\u5f97\u5224\u522b\u6027\u7279\u5f81\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u53cc\u635f\u5931\u51fd\u6570\u4f18\u5316\u692d\u5706\u8fb9\u754c\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u9002\u5e94\u51b3\u7b56\u8fb9\u754c\u65b9\u6cd5\u5047\u8bbe\u5df2\u77e5\u7c7b\u670d\u4ece\u5404\u5411\u540c\u6027\u5206\u5e03\uff0c\u5c06\u8fb9\u754c\u9650\u5236\u4e3a\u7403\u5f62\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u7279\u5f81\u65b9\u5411\u4e0a\u7684\u5206\u5e03\u65b9\u5dee\u3002\u8fd9\u79cd\u7b80\u5316\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u590d\u6742\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u672a\u77e5\u610f\u56fe\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u8fb9\u754c\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u91c7\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u83b7\u5f97\u5df2\u77e5\u6837\u672c\u7684\u5224\u522b\u6027\u7279\u5f81\u7a7a\u95f4\uff1b\u5176\u6b21\u4f7f\u7528\u53ef\u5b66\u4e60\u77e9\u9635\u53c2\u6570\u5316\u6bcf\u4e2a\u5df2\u77e5\u7c7b\u7684\u692d\u5706\u8fb9\u754c\uff0c\u63d0\u4f9b\u6bd4\u4ec5\u7531\u4e2d\u5fc3\u548c\u534a\u5f84\u5b9a\u4e49\u7684\u7403\u5f62\u8fb9\u754c\u66f4\u5927\u7684\u7075\u6d3b\u6027\uff1b\u6700\u540e\u901a\u8fc7\u65b0\u9896\u8bbe\u8ba1\u7684\u53cc\u635f\u5931\u51fd\u6570\u4f18\u5316\u8fb9\u754c\uff0c\u5e73\u8861\u7ecf\u9a8c\u98ce\u9669\u548c\u5f00\u653e\u7a7a\u95f4\u98ce\u9669\uff0c\u5728\u8986\u76d6\u5df2\u77e5\u6837\u672c\u7684\u540c\u65f6\u6536\u7f29\u8fb9\u754c\u4ee5\u5bf9\u6297\u5408\u6210\u7684\u4f2a\u5f00\u653e\u6837\u672c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6587\u672c\u610f\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u95ee\u9898\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002\u692d\u5706\u8fb9\u754c\u7684\u7075\u6d3b\u6027\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u5f00\u653e\u610f\u56fe\u68c0\u6d4b\u80fd\u529b\uff0c\u5728\u591a\u6837\u590d\u6742\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u6f5c\u529b\u3002", "conclusion": "\u692d\u5706\u51b3\u7b56\u8fb9\u754c\u76f8\u6bd4\u4f20\u7edf\u7403\u5f62\u8fb9\u754c\u80fd\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u5df2\u77e5\u7c7b\u7684\u771f\u5b9e\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u5f00\u653e\u610f\u56fe\u68c0\u6d4b\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u4e3a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u5728\u590d\u6742\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5c55\u793a\u4e86\u5728\u591a\u6837\u5316\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.16830", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16830", "abs": "https://arxiv.org/abs/2511.16830", "authors": ["Oscar Chew", "Po-Yi Lu", "Jayden Lin", "Kuan-Hao Huang", "Hsuan-Tien Lin"], "title": "PEPPER: Perception-Guided Perturbation for Robust Backdoor Defense in Text-to-Image Diffusion Models", "comment": null, "summary": "Recent studies show that text to image (T2I) diffusion models are vulnerable to backdoor attacks, where a trigger in the input prompt can steer generation toward harmful or unintended content. To address this, we introduce PEPPER (PErcePtion Guided PERturbation), a backdoor defense that rewrites the caption into a semantically distant yet visually similar caption while adding unobstructive elements. With this rewriting strategy, PEPPER disrupt the trigger embedded in the input prompt, dilute the influence of trigger tokens and thereby achieve enhanced robustness. Experiments show that PEPPER is particularly effective against text encoder based attacks, substantially reducing attack success while preserving generation quality. Beyond this, PEPPER can be paired with any existing defenses yielding consistently stronger and generalizable robustness than any standalone method. Our code will be released on Github.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PEPPER\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u91cd\u5199\u7b56\u7565\u7834\u574f\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002\u8be5\u65b9\u6cd5\u53ef\u4e0e\u73b0\u6709\u9632\u5fa1\u6280\u672f\u7ed3\u5408\uff0c\u63d0\u4f9b\u6bd4\u4efb\u4f55\u5355\u4e00\u65b9\u6cd5\u66f4\u5f3a\u7684\u901a\u7528\u9c81\u68d2\u6027\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u540e\u95e8\u653b\u51fb\uff0c\u8f93\u5165\u63d0\u793a\u4e2d\u7684\u7279\u5b9a\u89e6\u53d1\u5668\u53ef\u80fd\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u6709\u5bb3\u6216\u975e\u9884\u671f\u5185\u5bb9\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u4fdd\u62a4\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u6b64\u7c7b\u653b\u51fb\u3002", "method": "PEPPER\u91c7\u7528\u611f\u77e5\u5f15\u5bfc\u7684\u6270\u52a8\u7b56\u7565\uff0c\u5c06\u8f93\u5165\u6807\u9898\u91cd\u5199\u4e3a\u8bed\u4e49\u8ddd\u79bb\u8f83\u8fdc\u4f46\u89c6\u89c9\u76f8\u4f3c\u7684\u6807\u9898\uff0c\u540c\u65f6\u6dfb\u52a0\u4e0d\u663e\u773c\u7684\u5143\u7d20\uff0c\u901a\u8fc7\u8fd9\u79cd\u91cd\u5199\u7b56\u7565\u7834\u574f\u8f93\u5165\u63d0\u793a\u4e2d\u5d4c\u5165\u7684\u89e6\u53d1\u5668\u5e76\u7a00\u91ca\u89e6\u53d1\u4ee4\u724c\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePEPPER\u5bf9\u57fa\u4e8e\u6587\u672c\u7f16\u7801\u5668\u7684\u653b\u51fb\u7279\u522b\u6709\u6548\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u653b\u51fb\u6210\u529f\u7387\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4e0e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u80fd\u591f\u63d0\u4f9b\u6bd4\u4efb\u4f55\u5355\u4e00\u65b9\u6cd5\u66f4\u5f3a\u4e14\u66f4\u901a\u7528\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "PEPPER\u8bc1\u660e\u4e86\u901a\u8fc7\u8bed\u4e49\u91cd\u5199\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u9632\u5fa1\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u540e\u95e8\u653b\u51fb\uff0c\u4e3a\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u5176\u80fd\u591f\u4e0e\u73b0\u6709\u9632\u5fa1\u6280\u672f\u534f\u540c\u5de5\u4f5c\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2511.17036", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17036", "abs": "https://arxiv.org/abs/2511.17036", "authors": ["Gyuwon Park"], "title": "Do Vision-Language Models Understand Visual Persuasiveness?", "comment": "8 pages (except for reference and appendix), 5 figures, 7 tables, to be published in NeurIPS 2025 Workshop: VLM4RWD", "summary": "Recent advances in vision-language models (VLMs) have enabled impressive multi-modal reasoning and understanding. Yet, whether these models truly grasp visual persuasion-how visual cues shape human attitudes and decisions-remains unclear. To probe this question, we construct a high-consensus dataset for binary persuasiveness judgment and introduce the taxonomy of Visual Persuasive Factors (VPFs), encompassing low-level perceptual, mid-level compositional, and high-level semantic cues. We also explore cognitive steering and knowledge injection strategies for persuasion-relevant reasoning. Empirical analysis across VLMs reveals a recall-oriented bias-models over-predict high persuasiveness-and weak discriminative power for low/mid-level features. In contrast, high-level semantic alignment between message and object presence emerges as the strongest predictor of human judgment. Among intervention strategies, simple instruction or unguided reasoning scaffolds yield marginal or negative effects, whereas concise, object-grounded rationales significantly improve precision and F1 scores. These results indicate that VLMs core limitation lies not in recognizing persuasive objects but in linking them to communicative intent.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u89c6\u89c9\u8bf4\u670d\u529b\u5224\u65ad\u6570\u636e\u96c6\u5e76\u5f15\u5165\u89c6\u89c9\u8bf4\u670d\u56e0\u7d20\u5206\u7c7b\u6cd5\uff0c\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bf4\u670d\u7406\u89e3\u4e0a\u5b58\u5728\u53ec\u56de\u5bfc\u5411\u504f\u89c1\uff0c\u4e14\u4e3b\u8981\u901a\u8fc7\u9ad8\u5c42\u8bed\u4e49\u5bf9\u9f50\u800c\u975e\u4f4e\u4e2d\u5c42\u7279\u5f81\u8fdb\u884c\u5224\u65ad\uff0c\u5bf9\u8c61\u5bfc\u5411\u7684\u7406\u6027\u63a8\u7406\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u89c6\u89c9\u8bf4\u670d\u2014\u2014\u5373\u89c6\u89c9\u7ebf\u7d22\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u6001\u5ea6\u548c\u51b3\u7b56\u2014\u2014\u4ecd\u4e0d\u6e05\u695a\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7a76VLMs\u5bf9\u89c6\u89c9\u8bf4\u670d\u673a\u5236\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e8c\u5143\u8bf4\u670d\u529b\u5224\u65ad\u7684\u9ad8\u5171\u8bc6\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86\u5305\u542b\u4f4e\u5c42\u611f\u77e5\u3001\u4e2d\u5c42\u6784\u56fe\u548c\u9ad8\u5c42\u8bed\u4e49\u7ebf\u7d22\u7684\u89c6\u89c9\u8bf4\u670d\u56e0\u7d20\u5206\u7c7b\u6cd5\uff0c\u5e76\u63a2\u7d22\u4e86\u8ba4\u77e5\u5f15\u5bfc\u548c\u77e5\u8bc6\u6ce8\u5165\u7b49\u8bf4\u670d\u76f8\u5173\u63a8\u7406\u7b56\u7565\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u663e\u793aVLMs\u5b58\u5728\u53ec\u56de\u5bfc\u5411\u504f\u89c1\u2014\u2014\u6a21\u578b\u8fc7\u5ea6\u9884\u6d4b\u9ad8\u8bf4\u670d\u529b\uff0c\u5bf9\u4f4e\u4e2d\u5c42\u7279\u5f81\u7684\u5224\u522b\u80fd\u529b\u8f83\u5f31\uff0c\u800c\u4fe1\u606f\u4e0e\u5bf9\u8c61\u5b58\u5728\u7684\u9ad8\u5c42\u8bed\u4e49\u5bf9\u9f50\u662f\u9884\u6d4b\u4eba\u7c7b\u5224\u65ad\u7684\u6700\u5f3a\u6307\u6807\uff0c\u5bf9\u8c61\u5bfc\u5411\u7684\u7b80\u6d01\u7406\u6027\u63a8\u7406\u80fd\u663e\u8457\u63d0\u9ad8\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u3002", "conclusion": "VLMs\u7684\u6838\u5fc3\u5c40\u9650\u4e0d\u5728\u4e8e\u8bc6\u522b\u6709\u8bf4\u670d\u529b\u7684\u5bf9\u8c61\uff0c\u800c\u5728\u4e8e\u5c06\u8fd9\u4e9b\u5bf9\u8c61\u4e0e\u6c9f\u901a\u610f\u56fe\u8054\u7cfb\u8d77\u6765\uff0c\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\u5728\u89c6\u89c9\u8bf4\u670d\u5224\u65ad\u4e2d\u8d77\u4e3b\u5bfc\u4f5c\u7528\uff0c\u9488\u5bf9\u6027\u63a8\u7406\u7b56\u7565\u80fd\u6709\u6548\u5f25\u8865\u6a21\u578b\u7f3a\u9677\u3002"}}
{"id": "2511.17056", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17056", "abs": "https://arxiv.org/abs/2511.17056", "authors": ["Paloma Rabaey", "Adrick Tench", "Stefan Heytens", "Thomas Demeester"], "title": "Patient-level Information Extraction by Consistent Integration of Textual and Tabular Evidence with Bayesian Networks", "comment": null, "summary": "Electronic health records (EHRs) form an invaluable resource for training clinical decision support systems. To leverage the potential of such systems in high-risk applications, we need large, structured tabular datasets on which we can build transparent feature-based models. While part of the EHR already contains structured information (e.g. diagnosis codes, medications, and lab results), much of the information is contained within unstructured text (e.g. discharge summaries and nursing notes). In this work, we propose a method for multi-modal patient-level information extraction that leverages both the tabular features available in the patient's EHR (using an expert-informed Bayesian network) as well as clinical notes describing the patient's symptoms (using neural text classifiers). We propose the use of virtual evidence augmented with a consistency node to provide an interpretable, probabilistic fusion of the models' predictions. The consistency node improves the calibration of the final predictions compared to virtual evidence alone, allowing the Bayesian network to better adjust the neural classifier's output to handle missing information and resolve contradictions between the tabular and text data. We show the potential of our method on the SimSUM dataset, a simulated benchmark linking tabular EHRs with clinical notes through expert knowledge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u60a3\u8005\u7ea7\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u548c\u4e34\u5e8a\u6587\u672c\u6570\u636e\uff0c\u4f7f\u7528\u865a\u62df\u8bc1\u636e\u589e\u5f3a\u548c\u4e00\u81f4\u6027\u8282\u70b9\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u6982\u7387\u878d\u5408\uff0c\u6709\u6548\u5904\u7406\u7f3a\u5931\u4fe1\u606f\u5e76\u89e3\u51b3\u6570\u636e\u77db\u76fe\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u5927\u91cf\u5173\u952e\u4fe1\u606f\u5b58\u5728\u4e8e\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u5229\u7528\u7ed3\u6784\u5316\u8868\u683c\u7279\u5f81\u4e0e\u4e34\u5e8a\u7b14\u8bb0\u4e4b\u95f4\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u900f\u660e\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u5904\u7406\u4fe1\u606f\u7f3a\u5931\u548c\u77db\u76fe\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u60a3\u8005\u7ea7\u4fe1\u606f\u63d0\u53d6\u6846\u67b6\uff0c\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u6784\u5efa\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u5904\u7406\u7ed3\u6784\u5316EHR\u7279\u5f81\uff0c\u4f7f\u7528\u795e\u7ecf\u6587\u672c\u5206\u7c7b\u5668\u5206\u6790\u4e34\u5e8a\u7b14\u8bb0\uff0c\u5e76\u901a\u8fc7\u865a\u62df\u8bc1\u636e\u589e\u5f3a\u548c\u4e00\u81f4\u6027\u8282\u70b9\u5b9e\u73b0\u6982\u7387\u878d\u5408\uff0c\u63d0\u9ad8\u9884\u6d4b\u6821\u51c6\u6027\u3002", "result": "\u5728SimSUM\u6a21\u62df\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5355\u72ec\u4f7f\u7528\u865a\u62df\u8bc1\u636e\u80fd\u591f\u663e\u8457\u6539\u5584\u9884\u6d4b\u6821\u51c6\uff0c\u4f7f\u8d1d\u53f6\u65af\u7f51\u7edc\u66f4\u597d\u5730\u8c03\u6574\u795e\u7ecf\u5206\u7c7b\u5668\u8f93\u51fa\uff0c\u6709\u6548\u5904\u7406\u4fe1\u606f\u7f3a\u5931\u548c\u89e3\u51b3\u8868\u683c\u4e0e\u6587\u672c\u6570\u636e\u95f4\u7684\u77db\u76fe\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u6848\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u8282\u70b9\u673a\u5236\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u533b\u7597\u6570\u636e\u65f6\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u9ad8\u98ce\u9669\u533b\u7597\u5e94\u7528\u4e2d\u7684\u900f\u660e\u7279\u5f81\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.16743", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16743", "abs": "https://arxiv.org/abs/2511.16743", "authors": ["Adeel Yousaf", "Joseph Fioresi", "James Beetham", "Amrit Singh Bedi", "Mubarak Shah"], "title": "SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving Pre-Trained Knowledge", "comment": "AAAI 2026 (Main Technical Track)", "summary": "Improving the safety of vision-language models like CLIP via fine-tuning often comes at a steep price, causing significant drops in their generalization performance. We find this trade-off stems from rigid alignment strategies that force unsafe concepts toward single, predefined safe targets, disrupting the model's learned semantic structure. To address this, we propose a proximity-aware approach: redirecting unsafe concepts to their semantically closest safe alternatives to minimize representational change. We introduce SaFeR-CLIP, a fine-tuning framework that applies this principle of minimal intervention. SaFeR-CLIP successfully reconciles safety and performance, recovering up to 8.0% in zero-shot accuracy over prior methods while maintaining robust safety. To support more rigorous evaluation, we also contribute NSFW-Caps, a new benchmark of 1,000 highly-aligned pairs for testing safety under distributional shift. Our work shows that respecting the geometry of pretrained representations is key to achieving safety without sacrificing performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSaFeR-CLIP\uff0c\u4e00\u79cd\u57fa\u4e8e\u90bb\u8fd1\u611f\u77e5\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e0d\u5b89\u5168\u6982\u5ff5\u91cd\u5b9a\u5411\u5230\u8bed\u4e49\u4e0a\u6700\u63a5\u8fd1\u7684\u5b89\u5168\u66ff\u4ee3\u54c1\u6765\u6700\u5c0f\u5316\u8868\u793a\u53d8\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u4e0e\u6cdb\u5316\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5982CLIP\u5728\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u5b89\u5168\u6027\u65f6\u5f80\u5f80\u5bfc\u81f4\u6cdb\u5316\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u8fd9\u79cd\u6743\u8861\u6e90\u4e8e\u50f5\u5316\u7684\u5bf9\u9f50\u7b56\u7565\u5f3a\u5236\u5c06\u4e0d\u5b89\u5168\u6982\u5ff5\u6620\u5c04\u5230\u5355\u4e00\u9884\u5b9a\u4e49\u5b89\u5168\u76ee\u6807\uff0c\u7834\u574f\u4e86\u6a21\u578b\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u90bb\u8fd1\u611f\u77e5\u65b9\u6cd5\uff0c\u5c06\u4e0d\u5b89\u5168\u6982\u5ff5\u91cd\u5b9a\u5411\u5230\u8bed\u4e49\u4e0a\u6700\u63a5\u8fd1\u7684\u5b89\u5168\u66ff\u4ee3\u54c1\u4ee5\u6700\u5c0f\u5316\u8868\u793a\u53d8\u5316\uff0c\u5f00\u53d1\u4e86SaFeR-CLIP\u5fae\u8c03\u6846\u67b6\u5e94\u7528\u8fd9\u79cd\u6700\u5c0f\u5e72\u9884\u539f\u5219\uff0c\u5e76\u8d21\u732e\u4e86NSFW-Caps\u57fa\u51c6\u6570\u636e\u96c6\u7528\u4e8e\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u3002", "result": "SaFeR-CLIP\u6210\u529f\u534f\u8c03\u4e86\u5b89\u5168\u6027\u4e0e\u6027\u80fd\uff0c\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u51c6\u786e\u7387\u4e0a\u6062\u590d\u4e86\u9ad8\u8fbe8.0%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9c81\u68d2\u7684\u5b89\u5168\u6027\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5c0a\u91cd\u9884\u8bad\u7ec3\u8868\u793a\u7684\u51e0\u4f55\u7ed3\u6784\u662f\u5b9e\u73b0\u5b89\u5168\u6027\u800c\u4e0d\u727a\u7272\u6027\u80fd\u7684\u5173\u952e\uff0c\u90bb\u8fd1\u611f\u77e5\u65b9\u6cd5\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8def\u5f84\uff0c\u6700\u5c0f\u5e72\u9884\u539f\u5219\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u9700\u8981\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u6027\u80fd\u7684\u573a\u666f\u3002"}}
{"id": "2511.17129", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17129", "abs": "https://arxiv.org/abs/2511.17129", "authors": ["Yeqin Zhang", "Yizheng Zhao", "Chen Hu", "Binxing Jiao", "Daxin Jiang", "Ruihang Miao", "Cam-Tu Nguyen"], "title": "Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation", "comment": "Accepted by AAAI'26", "summary": "Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLLM2Comp\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u538b\u7f29\u4f5c\u4e3a\u9884\u8bad\u7ec3\u4efb\u52a1\u6765\u6539\u8fdbLLM\u7684\u6587\u672c\u8868\u793a\u80fd\u529b\uff0c\u76f8\u6bd4\u57fa\u4e8etoken\u7ea7\u9884\u6d4b\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\u4e14\u6570\u636e\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u56e0\u679c\u5efa\u6a21\u548c\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8fdb\u884c\u4f18\u5316\uff0c\u96be\u4ee5\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u5168\u5c40\u6587\u672c\u8868\u793a\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u5f15\u5165\u9884\u8bad\u7ec3\u4efb\u52a1\u6765\u9002\u5e94LLM\u7528\u4e8e\u6587\u672c\u8868\u793a\uff0c\u4f46\u5927\u591a\u4f9d\u8d56token\u7ea7\u9884\u6d4b\u76ee\u6807\uff0c\u5982\u63a9\u7801\u4e0b\u4e00\u8bcd\u9884\u6d4b\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6355\u6349\u6574\u4f53\u8bed\u4e49\u8868\u793a\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4e0a\u4e0b\u6587\u538b\u7f29\u4f5c\u4e3a\u65e0\u76d1\u7763\u9002\u5e94LLM\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u5728\u538b\u7f29\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u5b66\u4e60\u751f\u6210\u7d27\u51d1\u7684\u8bb0\u5fc6token\u6765\u66ff\u4ee3\u5b8c\u6574\u4e0a\u4e0b\u6587\u8fdb\u884c\u4e0b\u6e38\u5e8f\u5217\u9884\u6d4b\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u538b\u7f29\u76ee\u6807\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u6784\u5efa\u4e86\u5f3a\u5927\u7684\u6587\u672c\u8868\u793a\u6a21\u578bLLM2Comp\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u538b\u7f29\u76ee\u6807\u80fd\u663e\u8457\u63d0\u5347\u57fa\u4e8eLLM\u7684\u6587\u672c\u8868\u793a\u8d28\u91cf\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u4e8etoken\u7ea7\u9884\u8bad\u7ec3\u4efb\u52a1\u7684\u6a21\u578b\u3002LLM2Comp\u5728\u5e7f\u6cdb\u7684\u6587\u672c\u7f16\u7801\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u5f53\u4ee3\u57fa\u4e8eLLM\u7684\u6587\u672c\u7f16\u7801\u5668\uff0c\u540c\u65f6\u5177\u6709\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\uff0c\u6240\u9700\u8bad\u7ec3\u6570\u636e\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u538b\u7f29\u4f5c\u4e3a\u9884\u8bad\u7ec3\u4efb\u52a1\u4e3aLLM\u6587\u672c\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\uff0c\u76f8\u6bd4\u4f20\u7edftoken\u7ea7\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6355\u6349\u6587\u6863\u7ea7\u8bed\u4e49\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u63d0\u9ad8\u4e86\u6570\u636e\u5229\u7528\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u6587\u672c\u8868\u793a\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.16853", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16853", "abs": "https://arxiv.org/abs/2511.16853", "authors": ["Xizhe Xue", "Xiao Xiang Zhu"], "title": "Towards Unified Vision Language Models for Forest Ecological Analysis in Earth Observation", "comment": "AAAI2026 AI for Environmental Science Workshop", "summary": "Recent progress in vision language models (VLMs) has enabled remarkable perception and reasoning capabilities, yet their potential for scientific regression in Earth Observation (EO) remains largely unexplored. Existing EO datasets mainly emphasize semantic understanding tasks such as captioning or classification, lacking benchmarks that align multimodal perception with measurable biophysical variables. To fill this gap, we present REO-Instruct, the first unified benchmark designed for both descriptive and regression tasks in EO. REO-Instruct establishes a cognitively interpretable logic chain in forest ecological scenario (human activity,land-cover classification, ecological patch counting, above-ground biomass (AGB) regression), bridging qualitative understanding and quantitative prediction. The dataset integrates co-registered Sentinel-2 and ALOS-2 imagery with structured textual annotations generated and validated through a hybrid human AI pipeline. Comprehensive evaluation protocols and baseline results across generic VLMs reveal that current models struggle with numeric reasoning, highlighting an essential challenge for scientific VLMs. REO-Instruct offers a standardized foundation for developing and assessing next-generation geospatial models capable of both description and scientific inference. The project page are publicly available at \\href{https://github.com/zhu-xlab/REO-Instruct}{REO-Instruct}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86REO-Instruct\uff0c\u8fd9\u662f\u9996\u4e2a\u9762\u5411\u5730\u7403\u89c2\u6d4b\u7684\u7edf\u4e00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u540c\u65f6\u5904\u7406\u63cf\u8ff0\u6027\u548c\u56de\u5f52\u4efb\u52a1\uff0c\u586b\u8865\u4e86\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u53ef\u6d4b\u91cf\u751f\u7269\u7269\u7406\u53d8\u91cf\u4e4b\u95f4\u7f3a\u4e4f\u5173\u8054\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u73b0\u6709\u5730\u7403\u89c2\u6d4b\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u7406\u89e3\u4efb\u52a1\u5982\u63cf\u8ff0\u548c\u5206\u7c7b\uff0c\u7f3a\u4e4f\u5c06\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u53ef\u6d4b\u91cf\u751f\u7269\u7269\u7406\u53d8\u91cf\u5bf9\u9f50\u7684\u57fa\u51c6\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86REO-Instruct\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u4e86\u68ee\u6797\u751f\u6001\u573a\u666f\u4e2d\u7684\u8ba4\u77e5\u53ef\u89e3\u91ca\u903b\u8f91\u94fe\uff0c\u6574\u5408\u4e86\u5171\u914d\u51c6\u7684Sentinel-2\u548cALOS-2\u5f71\u50cf\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u4eba\u673a\u6d41\u7a0b\u751f\u6210\u548c\u9a8c\u8bc1\u7ed3\u6784\u5316\u6587\u672c\u6807\u6ce8\u3002", "result": "\u5bf9\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u6a21\u578b\u5728\u6570\u503c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u56f0\u96be\uff0c\u7a81\u663e\u4e86\u79d1\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "REO-Instruct\u4e3a\u5f00\u53d1\u548c\u8bc4\u4f30\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u63cf\u8ff0\u548c\u79d1\u5b66\u63a8\u7406\u7684\u4e0b\u4e00\u4ee3\u5730\u7406\u7a7a\u95f4\u6a21\u578b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u79d1\u5b66\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.17238", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17238", "abs": "https://arxiv.org/abs/2511.17238", "authors": ["Anshul Singh", "Rohan Chaudhary", "Gagneet Singh", "Abhay Kumary"], "title": "Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables", "comment": "Accepted as Spotligh Talk at EurIPS 2025 Workshop on AI For Tabular Data", "summary": "The impressive performance of VLMs is largely measured on benchmarks that fail to capture the complexities of real-world scenarios. Existing datasets for tabular QA, such as WikiTableQuestions and FinQA, are overwhelmingly monolingual (English) and present tables in a digitally perfect, clean format. This creates a significant gap between research and practice. To address this, we present \\textbf{MirageTVQA}, a new benchmark designed to evaluate VLMs on these exact dimensions. Featuring nearly 60,000 QA pairs across 24 languages, MirageTVQA challenges models with tables that are not only multilingual but also visually imperfect, incorporating realistic noise to mimic scanned documents. Our evaluation of the leading VLMs reveals two primary failure points: a severe degradation in performance (over 35\\% drop for the best models) when faced with visual noise and a consistent English-first bias where reasoning abilities fail to transfer to other languages. MirageTVQA provides a benchmark for measuring and driving progress towards more robust VLM models for table reasoning. The dataset and the code are available at: https://github.com/anshulsc/MirageTVQA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MirageTVQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u89c6\u89c9\u566a\u58f0\u73af\u5883\u4e0b\u7684\u8868\u683c\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u4e25\u91cd\u6027\u80fd\u4e0b\u964d\u548c\u8bed\u8a00\u504f\u89c1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u95ee\u7b54\u6570\u636e\u96c6\u5982WikiTableQuestions\u548cFinQA\u4e3b\u8981\u9762\u5411\u82f1\u8bed\u4e14\u5448\u73b0\u5b8c\u7f8e\u6570\u5b57\u683c\u5f0f\uff0c\u65e0\u6cd5\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u4e2d\u591a\u8bed\u8a00\u548c\u89c6\u89c9\u566a\u58f0\u7684\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u7814\u7a76\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86\u5305\u542b\u8fd160,000\u4e2a\u95ee\u7b54\u5bf9\u7684MirageTVQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d624\u79cd\u8bed\u8a00\uff0c\u5e76\u5f15\u5165\u6a21\u62df\u626b\u63cf\u6587\u6863\u7684\u89c6\u89c9\u566a\u58f0\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u89c6\u89c9\u4e0d\u5b8c\u7f8e\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5bf9\u9886\u5148\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u9762\u5bf9\u89c6\u89c9\u566a\u58f0\u65f6\u6700\u4f73\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u8d85\u8fc735%\uff0c\u540c\u65f6\u5b58\u5728\u4e00\u81f4\u7684\u82f1\u8bed\u4f18\u5148\u504f\u89c1\uff0c\u6a21\u578b\u5728\u5176\u4ed6\u8bed\u8a00\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u65e0\u6cd5\u6709\u6548\u8fc1\u79fb\u3002", "conclusion": "MirageTVQA\u4e3a\u8861\u91cf\u548c\u63a8\u52a8\u66f4\u9c81\u68d2\u7684\u8868\u683c\u63a8\u7406\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u5904\u7406\u591a\u8bed\u8a00\u548c\u89c6\u89c9\u566a\u58f0\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.16857", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16857", "abs": "https://arxiv.org/abs/2511.16857", "authors": ["Vineet Bhat", "Sungsu Kim", "Valts Blukis", "Greg Heinrich", "Prashanth Krishnamurthy", "Ramesh Karri", "Stan Birchfield", "Farshad Khorrami", "Jonathan Tremblay"], "title": "BOP-ASK: Object-Interaction Reasoning for Vision-Language Models", "comment": null, "summary": "Vision Language Models (VLMs) have achieved impressive performance on spatial reasoning benchmarks, yet these evaluations mask critical weaknesses in understanding object interactions. Current benchmarks test high level relationships ('left of,' 'behind', etc.) but ignore fine-grained spatial understanding needed for real world applications: precise 3D localization, physical compatibility between objects, object affordances and multi step spatial planning. In this work, we present BOP-ASK, a novel large scale dataset for object interaction reasoning for both training and benchmarking. Our data generation pipeline leverages 6D object poses from the Benchmark for Object Pose Estimation (BOP) datasets from which we derive fine grained annotations such as grasp poses, referred object poses, path planning trajectories, relative spatial and depth relationships, and object-to-object relationships. BOP-ASK comprises over 150k images and 33M question answer pairs spanning six tasks (four novel), providing a rich resource for training and evaluating VLMs. We evaluate proprietary and open sourced VLMs, and conduct human evaluations on BOP-ASK-core, a contributed test benchmark. We also release BOP-ASK-lab, an out-of-distribution benchmark with images not sourced from BOP, enabling testing of generalization. Our experiments demonstrate that models trained on BOP-ASK outperform baselines and exhibit emergent capabilities such as precise object and grasp pose estimation, trajectory planning, and fine-grained object-centric spatial reasoning in cluttered environments. We will publicly release our datasets and dataset generation pipeline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BOP-ASK\uff0c\u4e00\u4e2a\u7528\u4e8e\u7269\u4f53\u4ea4\u4e92\u63a8\u7406\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5229\u75286D\u7269\u4f53\u59ff\u6001\u6570\u636e\u751f\u6210\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u5173\u7cfb\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u786e3D\u5b9a\u4f4d\u3001\u7269\u7406\u517c\u5bb9\u6027\u548c\u591a\u6b65\u7a7a\u95f4\u89c4\u5212\u7b49\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8fd9\u4e9b\u8bc4\u4f30\u63a9\u76d6\u4e86\u5176\u5728\u7406\u89e3\u7269\u4f53\u4ea4\u4e92\u65b9\u9762\u7684\u5173\u952e\u5f31\u70b9\uff0c\u5f53\u524d\u57fa\u51c6\u4e3b\u8981\u6d4b\u8bd5\u9ad8\u5c42\u6b21\u7a7a\u95f4\u5173\u7cfb\u800c\u5ffd\u7565\u4e86\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u6240\u9700\u7684\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7406\u89e3\uff0c\u5305\u62ec\u7cbe\u786e3D\u5b9a\u4f4d\u3001\u7269\u4f53\u95f4\u7269\u7406\u517c\u5bb9\u6027\u3001\u7269\u4f53\u53ef\u4f9b\u6027\u548c\u591a\u6b65\u7a7a\u95f4\u89c4\u5212\u7b49\u80fd\u529b\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u57fa\u4e8eBOP\u6570\u636e\u96c6\u76846D\u7269\u4f53\u59ff\u6001\u751f\u6210\u6570\u636e\u6d41\u6c34\u7ebf\uff0c\u4ece\u4e2d\u63d0\u53d6\u6293\u53d6\u59ff\u6001\u3001\u53c2\u8003\u7269\u4f53\u59ff\u6001\u3001\u8def\u5f84\u89c4\u5212\u8f68\u8ff9\u3001\u76f8\u5bf9\u7a7a\u95f4\u548c\u6df1\u5ea6\u5173\u7cfb\u4ee5\u53ca\u7269\u4f53\u95f4\u5173\u7cfb\u7b49\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u6784\u5efa\u4e86\u5305\u542b\u8d85\u8fc715\u4e07\u5f20\u56fe\u50cf\u548c3300\u4e07\u4e2a\u95ee\u7b54\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u516d\u4e2a\u4efb\u52a1\uff08\u5176\u4e2d\u56db\u4e2a\u4e3a\u65b0\u9896\u4efb\u52a1\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5728BOP-ASK\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u7cbe\u786e\u7269\u4f53\u548c\u6293\u53d6\u59ff\u6001\u4f30\u8ba1\u3001\u8f68\u8ff9\u89c4\u5212\u4ee5\u53ca\u5728\u6742\u4e71\u73af\u5883\u4e2d\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7269\u4f53\u4e2d\u5fc3\u7a7a\u95f4\u63a8\u7406\u7b49\u65b0\u5174\u80fd\u529b\uff0c\u540c\u65f6\u53d1\u5e03\u4e86BOP-ASK-core\u6d4b\u8bd5\u57fa\u51c6\u548cBOP-ASK-lab\u5206\u5e03\u5916\u6cdb\u5316\u57fa\u51c6\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7269\u4f53\u4ea4\u4e92\u63a8\u7406\u65b9\u9762\u7684\u8bc4\u4f30\u7a7a\u767d\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e6D\u59ff\u6001\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bad\u7ec3\u8d44\u6e90\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u540c\u65f6\u5f00\u6e90\u7684\u6570\u636e\u96c6\u548c\u751f\u6210\u6d41\u6c34\u7ebf\u5c06\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2511.17301", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17301", "abs": "https://arxiv.org/abs/2511.17301", "authors": ["Koena Ronny Mabokela", "Tim Schlippe", "Matthias W\u00f6lfel"], "title": "Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages", "comment": "Published in the Proceedings of The Southern African Conference on AI Research (SACAIR 2024), Bloemfontein, South Africa, 2-6 December 2024. ISBN: 978-0-7961-6069-0", "summary": "Sentiment analysis can aid in understanding people's opinions and emotions on social issues. In multilingual communities sentiment analysis systems can be used to quickly identify social challenges in social media posts, enabling government departments to detect and address these issues more precisely and effectively. Recently, large-language models (LLMs) have become available to the wide public and initial analyses have shown that they exhibit magnificent zero-shot sentiment analysis abilities in English. However, there is no work that has investigated to leverage LLMs for sentiment analysis on social media posts in South African languages and detect social challenges. Consequently, in this work, we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 to investigate the sentiment polarities of the 10 most emerging topics in English, Sepedi and Setswana social media posts that fall within the jurisdictional areas of 10 South African government departments. Our results demonstrate that there are big differences between the various LLMs, topics, and languages. In addition, we show that a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%. Consequently, it is now feasible to provide systems that generate reliable information about sentiment analysis to detect social challenges and draw conclusions about possible needs for actions on specific topics and within different language groups.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86GPT-3.5\u3001GPT-4\u3001LlaMa 2\u3001PaLM 2\u548cDolly 2\u7b49\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5357\u975e\u82f1\u8bed\u3001Sepedi\u548cSetswana\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u4e2d\u7684\u96f6\u6837\u672c\u60c5\u611f\u5206\u6790\u6027\u80fd\uff0c\u5e76\u53d1\u73b0\u901a\u8fc7\u591a\u6a21\u578b\u878d\u5408\u53ef\u5c06\u60c5\u611f\u5206\u7c7b\u9519\u8bef\u7387\u964d\u81f31%\u4ee5\u4e0b\uff0c\u4e3a\u68c0\u6d4b\u793e\u4f1a\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u5357\u975e\u591a\u8bed\u8a00\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u60c5\u611f\u5206\u6790\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u793e\u4f1a\u6311\u6218\u65b9\u9762\u5b58\u5728\u7a7a\u767d\uff0c\u800c\u591a\u8bed\u8a00\u793e\u533a\u7684\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u80fd\u591f\u5e2e\u52a9\u653f\u5e9c\u90e8\u95e8\u66f4\u7cbe\u786e\u5730\u8bc6\u522b\u548c\u89e3\u51b3\u793e\u4f1a\u95ee\u9898\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86GPT-3.5\u3001GPT-4\u3001LlaMa 2\u3001PaLM 2\u548cDolly 2\u7b49\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5357\u975e10\u4e2a\u653f\u5e9c\u90e8\u95e8\u7ba1\u8f96\u8303\u56f4\u5185\u768410\u4e2a\u65b0\u5174\u8bdd\u9898\u4e0a\u7684\u96f6\u6837\u672c\u60c5\u611f\u5206\u6790\u80fd\u529b\uff0c\u6db5\u76d6\u82f1\u8bed\u3001Sepedi\u548cSetswana\u4e09\u79cd\u8bed\u8a00\uff0c\u5e76\u91c7\u7528\u4e86\u591a\u6a21\u578b\u7ed3\u679c\u878d\u5408\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u3001\u8bdd\u9898\u548c\u8bed\u8a00\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0c\u901a\u8fc7\u591a\u6a21\u578b\u878d\u5408\u7b56\u7565\u80fd\u591f\u5927\u5e45\u63d0\u5347\u60c5\u611f\u5206\u7c7b\u6027\u80fd\uff0c\u5c06\u5206\u7c7b\u9519\u8bef\u7387\u964d\u81f31%\u4ee5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u53ef\u9760\u7684\u60c5\u611f\u5206\u6790\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u80fd\u591f\u4e3a\u591a\u8bed\u8a00\u793e\u4ea4\u5a92\u4f53\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u653f\u5e9c\u90e8\u95e8\u80fd\u591f\u57fa\u4e8e\u7279\u5b9a\u8bdd\u9898\u548c\u8bed\u8a00\u7fa4\u4f53\u7684\u60c5\u611f\u5206\u6790\u7ed3\u679c\u6765\u68c0\u6d4b\u793e\u4f1a\u6311\u6218\u5e76\u5236\u5b9a\u76f8\u5e94\u884c\u52a8\u7b56\u7565\u3002"}}
{"id": "2511.16870", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16870", "abs": "https://arxiv.org/abs/2511.16870", "authors": ["Loukas Sfountouris", "Giannis Daras", "Paris Giampouras"], "title": "Align & Invert: Solving Inverse Problems with Diffusion and Flow-based Models via Representational Alignment", "comment": null, "summary": "Enforcing alignment between the internal representations of diffusion or flow-based generative models and those of pretrained self-supervised encoders has recently been shown to provide a powerful inductive bias, improving both convergence and sample quality. In this work, we extend this idea to inverse problems, where pretrained generative models are employed as priors. We propose applying representation alignment (REPA) between diffusion or flow-based models and a pretrained self-supervised visual encoder, such as DINOv2, to guide the reconstruction process at inference time. Although ground-truth signals are unavailable in inverse problems, we show that aligning model representations with approximate target features can substantially enhance reconstruction fidelity and perceptual realism. We provide theoretical results showing (a) the relation between the REPA regularization and a divergence measure in the DINOv2 embedding space, and (b) how REPA updates steer the model's internal representations toward those of the clean image. These results offer insights into the role of REPA in improving perceptual fidelity. Finally, we demonstrate the generality of our approach by integrating it into multiple state-of-the-art inverse problem solvers. Extensive experiments on super-resolution, box inpainting, Gaussian deblurring, and motion deblurring confirm that our method consistently improves reconstruction quality across tasks, while also providing substantial efficiency gains by reducing the number of required discretization steps without compromising the performance of the underlying solver.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8868\u793a\u5bf9\u9f50\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u4e0e\u9884\u8bad\u7ec3\u81ea\u76d1\u7763\u7f16\u7801\u5668\u7684\u5185\u90e8\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9006\u95ee\u9898\u6c42\u89e3\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u9006\u95ee\u9898\u4efb\u52a1\u4e2d\u5747\u80fd\u4e00\u81f4\u6539\u5584\u91cd\u5efa\u6548\u679c\uff0c\u540c\u65f6\u51cf\u5c11\u6240\u9700\u7684\u79bb\u6563\u5316\u6b65\u9aa4\u3002", "motivation": "\u73b0\u6709\u9006\u95ee\u9898\u6c42\u89e3\u65b9\u6cd5\u5728\u4f7f\u7528\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u5148\u9a8c\u65f6\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5185\u90e8\u8868\u793a\u4e0e\u76ee\u6807\u7279\u5f81\u5bf9\u9f50\u7684\u6709\u6548\u5f15\u5bfc\u673a\u5236\u3002\u867d\u7136\u8868\u793a\u5bf9\u9f50\u5728\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u4e2d\u5df2\u88ab\u8bc1\u660e\u80fd\u6539\u5584\u6536\u655b\u548c\u6837\u672c\u8d28\u91cf\uff0c\u4f46\u5728\u9006\u95ee\u9898\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5982\u4f55\u5229\u7528\u8fd9\u79cd\u5bf9\u9f50\u6765\u6307\u5bfc\u91cd\u5efa\u8fc7\u7a0b\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u8868\u793a\u5bf9\u9f50\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u5f3a\u5236\u5bf9\u9f50\u6269\u6563\u6216\u6d41\u5f0f\u751f\u6210\u6a21\u578b\u4e0e\u9884\u8bad\u7ec3\u81ea\u76d1\u7763\u89c6\u89c9\u7f16\u7801\u5668\u7684\u5185\u90e8\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u5229\u7528DINOv2\u7b49\u7f16\u7801\u5668\u63d0\u53d6\u8fd1\u4f3c\u76ee\u6807\u7279\u5f81\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86REPA\u6b63\u5219\u5316\u4e0eDINOv2\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6563\u5ea6\u5ea6\u91cf\u7684\u5173\u7cfb\uff0c\u4ee5\u53caREPA\u66f4\u65b0\u5982\u4f55\u5f15\u5bfc\u6a21\u578b\u5185\u90e8\u8868\u793a\u5411\u5e72\u51c0\u56fe\u50cf\u8868\u793a\u6536\u655b\u3002", "result": "\u5728\u8d85\u5206\u8fa8\u7387\u3001\u6846\u5185\u4fee\u590d\u3001\u9ad8\u65af\u53bb\u6a21\u7cca\u548c\u8fd0\u52a8\u53bb\u6a21\u7cca\u7b49\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4e00\u81f4\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u6548\u7387\u2014\u2014\u5728\u4e0d\u5f71\u54cd\u5e95\u5c42\u6c42\u89e3\u5668\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u51cf\u5c11\u4e86\u6240\u9700\u7684\u79bb\u6563\u5316\u6b65\u9aa4\u6570\u91cf\u3002\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u771f\u5b9e\u6027\u5747\u5f97\u5230\u5b9e\u8d28\u6027\u589e\u5f3a\u3002", "conclusion": "\u8868\u793a\u5bf9\u9f50\u4e3a\u9006\u95ee\u9898\u6c42\u89e3\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u901a\u8fc7\u5f15\u5bfc\u6a21\u578b\u5185\u90e8\u8868\u793a\u5411\u76ee\u6807\u7279\u5f81\u6536\u655b\u6765\u6539\u5584\u611f\u77e5\u4fdd\u771f\u5ea6\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u96c6\u6210\u5230\u591a\u79cd\u6700\u5148\u8fdb\u7684\u9006\u95ee\u9898\u6c42\u89e3\u5668\u4e2d\uff0c\u4e3a\u5229\u7528\u9884\u8bad\u7ec3\u8868\u793a\u6307\u5bfc\u751f\u6210\u6a21\u578b\u63a8\u7406\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.17358", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17358", "abs": "https://arxiv.org/abs/2511.17358", "authors": ["Daniil Ignatev", "Ayman Santeer", "Albert Gatt", "Denis Paperno"], "title": "Don't Learn, Ground: A Case for Natural Language Inference with Visual Grounding", "comment": null, "summary": "We propose a zero-shot method for Natural Language Inference (NLI) that leverages multimodal representations by grounding language in visual contexts. Our approach generates visual representations of premises using text-to-image models and performs inference by comparing these representations with textual hypotheses. We evaluate two inference techniques: cosine similarity and visual question answering. Our method achieves high accuracy without task-specific fine-tuning, demonstrating robustness against textual biases and surface heuristics. Additionally, we design a controlled adversarial dataset to validate the robustness of our approach. Our findings suggest that leveraging visual modality as a meaning representation provides a promising direction for robust natural language understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u524d\u63d0\u7684\u89c6\u89c9\u8868\u793a\uff0c\u5e76\u4e0e\u6587\u672c\u5047\u8bbe\u8fdb\u884c\u6bd4\u8f83\u6765\u5b8c\u6210\u63a8\u7406\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u6587\u672c\u504f\u89c1\u548c\u8868\u9762\u542f\u53d1\u5f0f\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u6a21\u6001\uff0c\u5bb9\u6613\u53d7\u5230\u6587\u672c\u504f\u89c1\u548c\u8868\u9762\u542f\u53d1\u5f0f\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u901a\u8fc7\u591a\u6a21\u6001\u8868\u793a\u5c06\u8bed\u8a00\u5728\u89c6\u89c9\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u57fa\u7840\u5316\uff0c\u4ee5\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7cfb\u7edf\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u524d\u63d0\u7684\u89c6\u89c9\u8868\u793a\uff0c\u5e76\u91c7\u7528\u4e24\u79cd\u63a8\u7406\u6280\u672f\u8fdb\u884c\u6bd4\u8f83\uff1a\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u89c6\u89c9\u95ee\u7b54\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53d7\u63a7\u7684\u5bf9\u6297\u6027\u6570\u636e\u96c6\u6765\u9a8c\u8bc1\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u6709\u6548\u62b5\u6297\u4e86\u6587\u672c\u504f\u89c1\u548c\u8868\u9762\u542f\u53d1\u5f0f\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5bf9\u6297\u6027\u6570\u636e\u96c6\u7684\u9a8c\u8bc1\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5229\u7528\u89c6\u89c9\u6a21\u6001\u4f5c\u4e3a\u610f\u4e49\u8868\u793a\u4e3a\u9c81\u68d2\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u591a\u6a21\u6001\u57fa\u7840\u5316\u80fd\u591f\u6709\u6548\u7f13\u89e3\u7eaf\u6587\u672c\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.16937", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16937", "abs": "https://arxiv.org/abs/2511.16937", "authors": ["Hong Gao", "Jingyu Wu", "Xiangkai Xu", "Kangni Xie", "Yunchen Zhang", "Bin Zhong", "Xurui Gao", "Min-Ling Zhang"], "title": "OmniGround: A Comprehensive Spatio-Temporal Grounding Benchmark for Real-World Complex Scenarios", "comment": "20 pages", "summary": "Spatio-Temporal Video Grounding (STVG) aims to localize target objects in videos based on natural language descriptions. Despite recent advances in Multimodal Large Language Models, a significant gap remains between current models and real-world demands involving diverse objects and complex queries. We attribute this to limited benchmark scope, causing models to exhibit category bias, oversimplified reasoning, and poor linguistic robustness. To address these limitations, we introduce OmniGround, a comprehensive benchmark with 3,475 videos spanning 81 categories and complex real-world queries. We propose the Forward-Backward-Refinement annotation pipeline that combines multi-directional tracking with intelligent error correction for high-quality labels. We further introduce DeepSTG, a systematic evaluation framework quantifying dataset quality across four complementary dimensions beyond superficial statistics. Evaluations reveal performance average drop of 10.4% on complex real-world scenes, particularly with small/occluded objects and intricate spatial relations. Motivated by these, we propose PG-TAF, a training-free two-stage framework decomposing STVG into high-level temporal grounding and fine-grained spatio-temporal propagation. Experiments demonstrate PG-TAF achieves 25.6% and 35.6% improvements in m\\_tIoU and m\\_vIoU on OmniGround with consistent gains across four benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOmniGround\u57fa\u51c6\u548cPG-TAF\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u65f6\u7a7a\u89c6\u9891\u5b9a\u4f4d\u4e2d\u7c7b\u522b\u504f\u89c1\u548c\u590d\u6742\u67e5\u8be2\u5904\u7406\u4e0d\u8db3\u7684\u95ee\u9898\u3002PG-TAF\u901a\u8fc7\u4e24\u9636\u6bb5\u5206\u89e3\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u7a7a\u89c6\u9891\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4e0e\u73b0\u5b9e\u9700\u6c42\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e3b\u8981\u7531\u4e8e\u57fa\u51c6\u6570\u636e\u96c6\u8303\u56f4\u6709\u9650\u5bfc\u81f4\u6a21\u578b\u51fa\u73b0\u7c7b\u522b\u504f\u89c1\u3001\u63a8\u7406\u8fc7\u7a0b\u8fc7\u4e8e\u7b80\u5316\u4ee5\u53ca\u8bed\u8a00\u9c81\u68d2\u6027\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86OmniGround\u7efc\u5408\u57fa\u51c6\u6570\u636e\u96c6\u5305\u542b3,475\u4e2a\u89c6\u9891\u548c81\u4e2a\u7c7b\u522b\uff0c\u91c7\u7528\u524d\u5411-\u540e\u5411-\u7cbe\u5316\u6807\u6ce8\u6d41\u7a0b\u7ed3\u5408\u591a\u65b9\u5411\u8ddf\u8e2a\u548c\u667a\u80fd\u7ea0\u9519\uff1b\u5f00\u53d1\u4e86PG-TAF\u8bad\u7ec3\u514d\u8d39\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5c06STVG\u5206\u89e3\u4e3a\u9ad8\u5c42\u65f6\u95f4\u5b9a\u4f4d\u548c\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u4f20\u64ad\u3002", "result": "\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u6027\u80fd\u5e73\u5747\u4e0b\u964d10.4%\uff0c\u7279\u522b\u662f\u5728\u5c0f/\u906e\u6321\u7269\u4f53\u548c\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u4e0a\u8868\u73b0\u8f83\u5dee\uff1bPG-TAF\u5728OmniGround\u4e0am_tIoU\u548cm_vIoU\u5206\u522b\u63d0\u534725.6%\u548c35.6%\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u4e0a\u5747\u83b7\u5f97\u4e00\u81f4\u589e\u76ca\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709STVG\u6a21\u578b\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u7684\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6DeepSTG\u548cPG-TAF\u65b9\u6cd5\u4e3a\u6539\u8fdb\u65f6\u7a7a\u89c6\u9891\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u96c6\u8d28\u91cf\u548c\u4efb\u52a1\u5206\u89e3\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.16901", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16901", "abs": "https://arxiv.org/abs/2511.16901", "authors": ["Lu Zhu", "Tiantian Geng", "Yangye Chen", "Teng Wang", "Ping Lu", "Feng Zheng"], "title": "R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios", "comment": "Accepted by AAAI 2026. Project page: https://github.com/zhlllau/R-AVST", "summary": "Recently, rapid advancements have been made in multimodal large language models (MLLMs), especially in video understanding tasks. However, current research focuses on simple video scenarios, failing to reflect the complex and diverse nature of real-world audio-visual events in videos. To bridge this gap, we firstly introduce R-AVST, a dataset for audio-visual reasoning featuring fine-grained spatio-temporal annotations. In constructing this, we design a pipeline consisting of LLM-based key object extraction, automatic spatial annotation and manual quality inspection, resulting in over 5K untrimmed videos with 27K objects across 100 types of audio-visual events. Building on this dataset, we define three core tasks for spatio-temporal reasoning in audio-visual scenes and generate more than 8K high-quality, evenly distributed question-answer pairs to effectively benchmark model performance. To further enhance reasoning, we propose AVST-Zero, a reinforcement learning-based model that avoids intermediate supervision, directly optimizing behavior via carefully designed multi-dimensional rewards. Extensive experiments validate the effectiveness of our R-AVST in advancing audio-visual spatio-temporal reasoning, upon which AVST-Zero demonstrates competitive performance compared to existing models. To the best of our knowledge, R-AVST is the first dataset designed for real-world audio-visual spatio-temporal reasoning, and AVST-Zero offers a novel perspective for tackling future challenges in this domain.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86R-AVST\u6570\u636e\u96c6\u548cAVST-Zero\u6a21\u578b\uff0c\u524d\u8005\u662f\u9996\u4e2a\u4e13\u4e3a\u771f\u5b9e\u4e16\u754c\u97f3\u9891-\u89c6\u89c9\u65f6\u7a7a\u63a8\u7406\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\uff0c\u540e\u8005\u662f\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u5956\u52b1\u76f4\u63a5\u4f18\u5316\u884c\u4e3a\uff0c\u65e0\u9700\u4e2d\u95f4\u76d1\u7763\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5feb\u901f\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u89c6\u9891\u573a\u666f\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u4e2d\u97f3\u9891-\u89c6\u89c9\u4e8b\u4ef6\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\uff0c\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u9650\u5236\u4e86\u6a21\u578b\u5728\u590d\u6742\u97f3\u9891-\u89c6\u89c9\u65f6\u7a7a\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u9996\u5148\u6784\u5efa\u4e86R-AVST\u6570\u636e\u96c6\uff0c\u91c7\u7528\u57fa\u4e8eLLM\u7684\u5173\u952e\u5bf9\u8c61\u63d0\u53d6\u3001\u81ea\u52a8\u7a7a\u95f4\u6807\u6ce8\u548c\u4eba\u5de5\u8d28\u91cf\u68c0\u67e5\u7684\u6d41\u6c34\u7ebf\uff0c\u5305\u542b\u8d85\u8fc75K\u4e2a\u672a\u4fee\u526a\u89c6\u9891\u548c27K\u4e2a\u5bf9\u8c61\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86AVST-Zero\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u591a\u7ef4\u5ea6\u5956\u52b1\u76f4\u63a5\u4f18\u5316\u884c\u4e3a\uff0c\u907f\u514d\u4e86\u4e2d\u95f4\u76d1\u7763\u7684\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86R-AVST\u6570\u636e\u96c6\u5728\u63a8\u8fdb\u97f3\u9891-\u89c6\u89c9\u65f6\u7a7a\u63a8\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\uff0cAVST-Zero\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u751f\u6210\u4e86\u8d85\u8fc78K\u4e2a\u9ad8\u8d28\u91cf\u3001\u5747\u5300\u5206\u5e03\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\u6765\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "R-AVST\u662f\u9996\u4e2a\u4e13\u4e3a\u771f\u5b9e\u4e16\u754c\u97f3\u9891-\u89c6\u89c9\u65f6\u7a7a\u63a8\u7406\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\uff0cAVST-Zero\u4e3a\u89e3\u51b3\u8be5\u9886\u57df\u672a\u6765\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u9896\u89c6\u89d2\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u590d\u6742\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u5efa\u7acb\u4e86\u91cd\u8981\u7684\u57fa\u51c6\u548c\u89e3\u51b3\u65b9\u6848\u6846\u67b6\u3002"}}
{"id": "2511.17405", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17405", "abs": "https://arxiv.org/abs/2511.17405", "authors": ["Yesheng Liu", "Hao Li", "Haiyu Xu", "Baoqi Pei", "Jiahao Wang", "Mingxuan Zhao", "Jingshu Zheng", "Zheqi He", "JG Yao", "Bowen Qin", "Xi Yang", "Jiajun Zhang"], "title": "Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training", "comment": "Project url: https://flageval-baai.github.io/ReVeL/", "summary": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86ReVeL\u6846\u67b6\uff0c\u5c06\u591a\u9879\u9009\u62e9\u9898\u8f6c\u6362\u4e3a\u5f00\u653e\u5f0f\u95ee\u9898\u4ee5\u89e3\u51b3\u9009\u9879\u6cc4\u9732\u5bfc\u81f4\u7684\u8bc4\u4f30\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7LLM\u91cd\u5199\u548c\u9a8c\u8bc1\u673a\u5236\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u591a\u9879\u9009\u62e9\u9898\uff08MCQA\uff09\u5728\u8bc4\u4f30\u548c\u5f3a\u5316\u5fae\u8c03\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u9009\u9879\u6cc4\u9732\u95ee\u9898\uff0c\u5bfc\u81f4\u51c6\u786e\u7387\u6307\u6807\u4e0d\u53ef\u9760\u5e76\u9f13\u52b1\u6a21\u578b\u731c\u6d4b\u884c\u4e3a\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u80fd\u529b\u3002", "method": "\u63d0\u51faReVeL\u6846\u67b6\uff0c\u901a\u8fc7LLM\u5c06MCQA\u91cd\u5199\u4e3a\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u6839\u636e\u7b54\u6848\u7c7b\u578b\u5206\u7c7b\u5e94\u7528\u4e0d\u540c\u7684\u91cd\u5199\u548c\u9a8c\u8bc1\u65b9\u6848\uff0c\u4f7f\u7528GRPO\u65b9\u6cd5\u5bf9Qwen2.5-VL\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u591a\u9879\u9009\u62e9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReVeL-OpenQA\u8bad\u7ec3\u7684\u6a21\u578b\u4fdd\u6301\u4e86MCQA\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5c06OpenQA\u51c6\u786e\u7387\u63d0\u5347\u4e86\u7ea66\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u63ed\u793a\u4e86MCQA\u57fa\u51c6\u4e2d\u9ad8\u8fbe20\u4e2a\u767e\u5206\u70b9\u7684\u5206\u6570\u81a8\u80c0\u3002", "conclusion": "ReVeL\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u548c\u66f4\u9c81\u68d2\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u6539\u5584\u4e86\u8bc4\u4f30\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u4e3a\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u8bad\u7ec3\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.16979", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16979", "abs": "https://arxiv.org/abs/2511.16979", "authors": ["Yunyun Wang", "Zheng Duan", "Xinyue Liao", "Ke-Jia Chen", "Songcan Chen"], "title": "The Finer the Better: Towards Granular-aware Open-set Domain Generalization", "comment": "9 pages,3 figures,aaai2026", "summary": "Open-Set Domain Generalization (OSDG) tackles the realistic scenario where deployed models encounter both domain shifts and novel object categories. Despite impressive progress with vision-language models like CLIP, existing methods still fall into the dilemma between structural risk of known-classes and open-space risk from unknown-classes, and easily suffers from over-confidence, especially when distinguishing ``hard unknowns\" that share fine-grained visual similarities with known classes. To this end, we propose a Semantic-enhanced CLIP (SeeCLIP) framework that explicitly addresses this dilemma through fine-grained semantic enhancement. In SeeCLIP, we propose a semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens, enabling nuanced vision-language alignment beyond coarse category labels. To position unknown prompts effectively, we introduce duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity. Further, our semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences. These hard negatives force the model to learn finer decision boundaries. Extensive experiments across five benchmarks demonstrate consistent improvements of 3% accuracy and 5% H-score over state-of-the-art methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faSeeCLIP\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u589e\u5f3a\u89e3\u51b3\u5f00\u653e\u96c6\u57df\u6cdb\u5316\u4e2d\u7684\u7ed3\u6784\u98ce\u9669\u4e0e\u5f00\u653e\u7a7a\u95f4\u98ce\u9669\u56f0\u5883\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u9047\u5230\u89c6\u89c9\u76f8\u4f3c\u672a\u77e5\u7c7b\u522b\u65f6\u7684\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u5f00\u653e\u96c6\u57df\u6cdb\u5316\u9762\u4e34\u5df2\u77e5\u7c7b\u522b\u7ed3\u6784\u98ce\u9669\u4e0e\u672a\u77e5\u7c7b\u522b\u5f00\u653e\u7a7a\u95f4\u98ce\u9669\u7684\u6743\u8861\u56f0\u5883\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e0e\u5df2\u77e5\u7c7b\u522b\u5177\u6709\u7ec6\u7c92\u5ea6\u89c6\u89c9\u76f8\u4f3c\u6027\u7684'\u56f0\u96be\u672a\u77e5'\u6837\u672c\u65f6\u5bb9\u6613\u4ea7\u751f\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u611f\u77e5\u63d0\u793a\u589e\u5f3a\u6a21\u5757\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u5224\u522b\u6027\u8bed\u4e49\u6807\u8bb0\uff0c\u5b9e\u73b0\u8d85\u8d8a\u7c97\u7c92\u5ea6\u7c7b\u522b\u6807\u7b7e\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\uff1b\u91c7\u7528\u53cc\u5de5\u5bf9\u6bd4\u5b66\u4e60\u4fdd\u6301\u4e0e\u5df2\u77e5\u7c7b\u522b\u7684\u5206\u79bb\u6027\u548c\u8bed\u4e49\u90bb\u8fd1\u6027\uff1b\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u6269\u6563\u6a21\u5757\u6270\u52a8\u63d0\u53d6\u7684\u8bed\u4e49\u6807\u8bb0\u5408\u6210\u4f2a\u672a\u77e5\u6837\u672c\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u53473%\uff0cH-score\u63d0\u53475%\uff0c\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ec6\u7c92\u5ea6\u8bed\u4e49\u589e\u5f3a\u5728\u7f13\u89e3\u5f00\u653e\u96c6\u57df\u6cdb\u5316\u98ce\u9669\u56f0\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5904\u7406\u89c6\u89c9\u76f8\u4f3c\u672a\u77e5\u7c7b\u522b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u4e86\u5f00\u653e\u73af\u5883\u4e0b\u7684\u7a33\u5065\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2511.16908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16908", "abs": "https://arxiv.org/abs/2511.16908", "authors": ["Shushi Wang", "Zicheng Zhang", "Chunyi Li", "Wei Wang", "Liya Ma", "Fengjiao Chen", "Xiaoyu Li", "Xuezhi Cao", "Guangtao Zhai", "Xiaohong Liu"], "title": "Q-REAL: Towards Realism and Plausibility Evaluation for AI-Generated Content", "comment": null, "summary": "Quality assessment of AI-generated content is crucial for evaluating model capability and guiding model optimization. However, most existing quality assessment datasets and models provide only a single quality score, which is too coarse to offer targeted guidance for improving generative models. In current applications of AI-generated images, realism and plausibility are two critical dimensions, and with the emergence of unified generation-understanding models, fine-grained evaluation along these dimensions becomes especially effective for improving generative performance. Therefore, we introduce Q-Real, a novel dataset for fine-grained evaluation of realism and plausibility in AI-generated images. Q-Real consists of 3,088 images generated by popular text-to-image models. For each image, we annotate the locations of major entities and provide a set of judgment questions and attribution descriptions for these along the dimensions of realism and plausibility. Considering that recent advances in multi-modal large language models (MLLMs) enable fine-grained evaluation of AI-generated images, we construct Q-Real Bench to evaluate them on two tasks: judgment and grounding with reasoning. Finally, to enhance MLLM capabilities, we design a fine-tuning framework and conduct experiments on multiple MLLMs using our dataset. Experimental results demonstrate the high quality and significance of our dataset and the comprehensiveness of the benchmark. Dataset and code will be released upon publication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Q-Real\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u7528\u4e8eAI\u751f\u6210\u56fe\u50cf\u7684\u7ec6\u7c92\u5ea6\u771f\u5b9e\u6027\u548c\u5408\u7406\u6027\u8bc4\u4f30\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5224\u65ad\u548c\u5b9a\u4f4d\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5fae\u8c03\u6846\u67b6\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u5185\u5bb9\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u4ec5\u63d0\u4f9b\u5355\u4e00\u8d28\u91cf\u5206\u6570\uff0c\u8fc7\u4e8e\u7c97\u7cd9\u65e0\u6cd5\u4e3a\u751f\u6210\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u9488\u5bf9\u6027\u6307\u5bfc\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u6027\u548c\u5408\u7406\u6027\u8fd9\u4e24\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b3,088\u5f20\u6587\u672c\u751f\u6210\u56fe\u50cf\u7684Q-Real\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e3b\u8981\u5b9e\u4f53\u7684\u4f4d\u7f6e\u4fe1\u606f\u4ee5\u53ca\u771f\u5b9e\u6027\u548c\u5408\u7406\u6027\u7ef4\u5ea6\u7684\u5224\u65ad\u95ee\u9898\u548c\u5f52\u56e0\u63cf\u8ff0\uff1b\u8bbe\u8ba1\u4e86Q-Real\u57fa\u51c6\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5224\u65ad\u548c\u5b9a\u4f4d\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1b\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u5fae\u8c03\u6846\u67b6\u6765\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6570\u636e\u96c6\u5177\u6709\u9ad8\u8d28\u91cf\u548c\u91cd\u8981\u610f\u4e49\uff0c\u57fa\u51c6\u8bc4\u4f30\u5168\u9762\u6709\u6548\uff0c\u901a\u8fc7\u5fae\u8c03\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aAI\u751f\u6210\u56fe\u50cf\u7684\u7ec6\u7c92\u5ea6\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u7edf\u4e00\u751f\u6210-\u7406\u89e3\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u80fd\u529b\u4e3a\u751f\u6210\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u6307\u5bfc\u65b9\u5411\u3002"}}
{"id": "2511.17432", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17432", "abs": "https://arxiv.org/abs/2511.17432", "authors": ["Shrikant Kendre", "Austin Xu", "Honglu Zhou", "Michael Ryoo", "Shafiq Joty", "Juan Carlos Niebles"], "title": "SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation", "comment": "23 pages, 6 tables, 9 figures", "summary": "Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SMILE\u8bc4\u4f30\u6307\u6807\uff0c\u8be5\u6307\u6807\u901a\u8fc7\u6574\u5408\u53e5\u5b50\u7ea7\u8bed\u4e49\u7406\u89e3\u3001\u5173\u952e\u8bcd\u7ea7\u8bed\u4e49\u7406\u89e3\u548c\u7cbe\u786e\u5173\u952e\u8bcd\u5339\u914d\uff0c\u5728\u6587\u672c\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u8bcd\u6c47\u7cbe\u786e\u6027\u548c\u8bed\u4e49\u76f8\u5173\u6027\u7684\u5e73\u8861\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u548c\u89c6\u89c9\u95ee\u7b54\u8bc4\u4f30\u6307\u6807\u5982ROUGE\u3001METEOR\u548c\u7cbe\u786e\u5339\u914d\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8en-gram\u7684\u8bcd\u6c47\u76f8\u4f3c\u6027\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u9700\u6c42\u3002\u867d\u7136BERTScore\u548cMoverScore\u7b49\u5ea6\u91cf\u65b9\u6cd5\u5229\u7528\u4e0a\u4e0b\u6587\u5d4c\u5165\u89e3\u51b3\u4e86\u8fd9\u4e00\u5c40\u9650\uff0c\u4f46\u5b83\u4eec\u7f3a\u4e4f\u5728\u53e5\u5b50\u7ea7\u548c\u5173\u952e\u8bcd\u7ea7\u8bed\u4e49\u4e4b\u95f4\u5e73\u8861\u7684\u7075\u6d3b\u6027\uff0c\u5e76\u4e14\u5ffd\u7565\u4e86\u4ecd\u7136\u91cd\u8981\u7684\u8bcd\u6c47\u76f8\u4f3c\u6027\u3002\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u5668\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u5b58\u5728\u6210\u672c\u9ad8\u3001\u504f\u89c1\u3001\u4e0d\u4e00\u81f4\u548c\u5e7b\u89c9\u7b49\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86SMILE\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u590d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u53e5\u5b50\u7ea7\u8bed\u4e49\u7406\u89e3\u3001\u5173\u952e\u8bcd\u7ea7\u8bed\u4e49\u7406\u89e3\u548c\u7b80\u5355\u7684\u5173\u952e\u8bcd\u5339\u914d\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u4e0d\u540c\u5c42\u6b21\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5e73\u8861\u4e86\u8bcd\u6c47\u7cbe\u786e\u6027\u548c\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u8bc4\u4f30\u80fd\u529b\u3002SMILE\u8bbe\u8ba1\u4e3a\u8ba1\u7b97\u8f7b\u91cf\u7ea7\uff0c\u907f\u514d\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5668\u7684\u9ad8\u6210\u672c\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "result": "\u5728\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0cSMILE\u4e0e\u4eba\u7c7b\u5224\u65ad\u5177\u6709\u9ad8\u5ea6\u76f8\u5173\u6027\u3002\u8be5\u6307\u6807\u5728\u591a\u4e2a\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5e73\u8861\u8bcd\u6c47\u548c\u8bed\u4e49\u8bc4\u4f30\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aSMILE\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u63d0\u4f9b\u53ef\u9760\u4e14\u4e00\u81f4\u7684\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "SMILE\u6210\u529f\u5f25\u5408\u4e86\u8bcd\u6c47\u8bc4\u4f30\u548c\u8bed\u4e49\u8bc4\u4f30\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u6587\u672c\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u52a0\u5168\u9762\u548c\u51c6\u786e\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u7ed3\u5408\u4e0d\u540c\u5c42\u6b21\u8bed\u4e49\u4fe1\u606f\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u8bc4\u4f30\u6307\u6807\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002SMILE\u7684\u8f7b\u91cf\u7ea7\u7279\u6027\u4f7f\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u8bc4\u4f30\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.17045", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.17045", "abs": "https://arxiv.org/abs/2511.17045", "authors": ["Linfeng Dong", "Yuchen Yang", "Hao Wu", "Wei Wang", "Yuenan HouZhihang Zhong", "Xiao Sun"], "title": "RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis", "comment": "Accepted to AAAI 2026 (Oral)", "summary": "We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision", "AI": {"tldr": "RacketVision\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u7ec6\u7c92\u5ea6\u7403\u62cd\u59ff\u6001\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e52\u4e53\u7403\u3001\u7f51\u7403\u548c\u7fbd\u6bdb\u7403\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u878d\u5408\u7403\u62cd\u59ff\u6001\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u4f53\u80b2\u5206\u6790\u4e2d\u7684\u52a8\u6001\u76ee\u6807\u8ddf\u8e2a\u548c\u591a\u6a21\u6001\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002", "motivation": "\u5f53\u524d\u4f53\u80b2\u5206\u6790\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u7ec6\u7c92\u5ea6\u7403\u62cd\u59ff\u6001\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u590d\u6742\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u7684\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7403\u62cd\u7c7b\u8fd0\u52a8\u7684\u52a8\u6001\u76ee\u6807\u8ddf\u8e2a\u3001\u5173\u8282\u5f0f\u7403\u62cd\u59ff\u6001\u4f30\u8ba1\u548c\u9884\u6d4b\u6027\u8f68\u8ff9\u9884\u6d4b\u7b49\u76f8\u4e92\u5173\u8054\u4efb\u52a1\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u7ec6\u7c92\u5ea6\u7403\u62cd\u59ff\u6001\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e09\u79cd\u7403\u62cd\u8fd0\u52a8\uff0c\u63d0\u51fa\u4f7f\u7528CrossAttention\u673a\u5236\u8fdb\u884c\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\uff0c\u800c\u975e\u7b80\u5355\u7684\u7279\u5f81\u62fc\u63a5\uff0c\u4ee5\u6709\u6548\u6574\u5408\u7403\u62cd\u59ff\u6001\u4fe1\u606f\u4e0e\u7403\u4f53\u4f4d\u7f6e\u6570\u636e\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u7b80\u5355\u62fc\u63a5\u7403\u62cd\u59ff\u6001\u7279\u5f81\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u800cCrossAttention\u673a\u5236\u80fd\u591f\u6709\u6548\u5229\u7528\u7403\u62cd\u59ff\u6001\u4fe1\u606f\uff0c\u5728\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5f3a\u5927\u7684\u5355\u6a21\u6001\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002", "conclusion": "RacketVision\u4e3a\u52a8\u6001\u76ee\u6807\u8ddf\u8e2a\u3001\u6761\u4ef6\u8fd0\u52a8\u9884\u6d4b\u548c\u4f53\u80b2\u591a\u6a21\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u591a\u529f\u80fd\u8d44\u6e90\u548c\u7814\u7a76\u8d77\u70b9\uff0c\u8bc1\u660e\u4e86\u8de8\u6ce8\u610f\u529b\u673a\u5236\u5728\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2511.16917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16917", "abs": "https://arxiv.org/abs/2511.16917", "authors": ["Chi Zhang", "Jiepeng Wang", "Youming Wang", "Yuanzhi Liang", "Xiaoyan Yang", "Zuoxin Li", "Haibin Huang", "Xuelong Li"], "title": "UniModel: A Visual-Only Framework for Unified Multimodal Understanding and Generation", "comment": null, "summary": "We present UniModel, a unified generative model that jointly supports visual understanding and visual generation within a single pixel-to-pixel diffusion framework. Our goal is to achieve unification along three axes: the model, the tasks, and the representations. At the representation level, we eliminate modality discrepancies by mapping both text and images into a shared visual space: textual prompts are rendered as painted text images on a clean canvas, and all inputs and outputs are treated purely as RGB pixels. This yields a fully vision-native formulation of multimodal learning. At the task level, a broad range of vision-language problems are cast as pixel-to-pixel transformations in this visual space. For understanding tasks, the model takes an RGB image and produces a painted text image that visually encodes the semantic prediction. For generation tasks, painted text images serve as visual conditions that guide realistic and semantically aligned image synthesis. Captioning and text-to-image generation thus become different directions of the same underlying visual translation process. At the model level, we instantiate a single Unified Diffusion Transformer trained with rectified flow in pixel space. A shared backbone jointly learns bidirectional mappings between natural images and painted text images, with lightweight task embeddings to specify the desired direction. Experiments on text-to-image synthesis and image-to-text understanding demonstrate strong cross-modal alignment and emergent controllability such as cycle-consistent image-caption-image loops. Our initial exploration suggests that unifying model, tasks, and representations in a single visual space is a promising paradigm for general-purpose multimodal intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUniModel\uff0c\u4e00\u79cd\u5728\u5355\u4e00\u50cf\u7d20\u5230\u50cf\u7d20\u6269\u6563\u6846\u67b6\u5185\u540c\u65f6\u652f\u6301\u89c6\u89c9\u7406\u89e3\u548c\u89c6\u89c9\u751f\u6210\u7684\u7edf\u4e00\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u6587\u672c\u548c\u56fe\u50cf\u6620\u5c04\u5230\u5171\u4eab\u89c6\u89c9\u7a7a\u95f4\u5b9e\u73b0\u591a\u6a21\u6001\u5b66\u4e60\u7684\u5b8c\u5168\u89c6\u89c9\u539f\u751f\u8868\u8ff0\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5b66\u4e60\u5b58\u5728\u6a21\u578b\u3001\u4efb\u52a1\u548c\u8868\u793a\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5206\u79bb\u95ee\u9898\uff0c\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u8868\u793a\u5dee\u5f02\u5bfc\u81f4\u7cfb\u7edf\u590d\u6742\u4e14\u96be\u4ee5\u7edf\u4e00\u5904\u7406\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6d88\u9664\u6a21\u6001\u5dee\u5f02\u5e76\u7edf\u4e00\u5904\u7406\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u50cf\u7d20\u5230\u50cf\u7d20\u6269\u6563\u6846\u67b6\uff0c\u5c06\u6587\u672c\u63d0\u793a\u6e32\u67d3\u4e3a\u7ed8\u5236\u6587\u672c\u56fe\u50cf\uff0c\u6240\u6709\u8f93\u5165\u8f93\u51fa\u5747\u89c6\u4e3aRGB\u50cf\u7d20\uff1b\u4f7f\u7528\u57fa\u4e8e\u6574\u6d41\u6d41\u7684\u7edf\u4e00\u6269\u6563\u53d8\u6362\u5668\u4f5c\u4e3a\u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4efb\u52a1\u5d4c\u5165\u6307\u5b9a\u6620\u5c04\u65b9\u5411\uff0c\u5b9e\u73b0\u81ea\u7136\u56fe\u50cf\u4e0e\u7ed8\u5236\u6587\u672c\u56fe\u50cf\u4e4b\u95f4\u7684\u53cc\u5411\u6620\u5c04\u5b66\u4e60\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u548c\u56fe\u50cf\u5230\u6587\u672c\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u5c55\u73b0\u51fa\u65b0\u5174\u7684\u53ef\u63a7\u6027\u7279\u6027\uff0c\u5982\u56fe\u50cf-\u63cf\u8ff0-\u56fe\u50cf\u7684\u5faa\u73af\u4e00\u81f4\u6027\uff0c\u9a8c\u8bc1\u4e86\u7edf\u4e00\u6846\u67b6\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u6a21\u578b\u3001\u4efb\u52a1\u548c\u8868\u793a\u7edf\u4e00\u5728\u5355\u4e00\u89c6\u89c9\u7a7a\u95f4\u4e2d\u662f\u5b9e\u73b0\u901a\u7528\u591a\u6a21\u6001\u667a\u80fd\u7684\u6709\u524d\u666f\u8303\u5f0f\uff0c\u8fd9\u79cd\u5b8c\u5168\u89c6\u89c9\u539f\u751f\u7684\u65b9\u6cd5\u4e3a\u6784\u5efa\u66f4\u7b80\u6d01\u9ad8\u6548\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5e76\u5c55\u793a\u4e86\u53cc\u5411\u89c6\u89c9\u7ffb\u8bd1\u8fc7\u7a0b\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.17004", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17004", "abs": "https://arxiv.org/abs/2511.17004", "authors": ["Patrick Amadeus Irawan", "Ikhlasul Akmal Hanif", "Muhammad Dehan Al Kautsar", "Genta Indra Winata", "Fajri Koto", "Alham Fikri Aji"], "title": "Vision Language Models are Confused Tourists", "comment": null, "summary": "Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs' stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ConfusedTourist\u8bc4\u4f30\u5957\u4ef6\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9762\u4e34\u6df7\u5408\u6587\u5316\u7ebf\u7d22\u65f6\u7684\u4e25\u91cd\u8106\u5f31\u6027\uff0c\u5373\u4f7f\u7b80\u5355\u7684\u56fe\u50cf\u5806\u53e0\u6270\u52a8\u4e5f\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u7a81\u663e\u4e86\u6784\u5efa\u6587\u5316\u9c81\u68d2\u591a\u6a21\u6001\u7406\u89e3\u7684\u7d27\u8feb\u9700\u6c42\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u7ef4\u5ea6\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u6587\u5316\u6982\u5ff5\u573a\u666f\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u591a\u4e2a\u65e0\u5173\u6587\u5316\u7ebf\u7d22\u5171\u5b58\u7684\u590d\u6742\u60c5\u51b5\uff0c\u8fd9\u79cd\u5c40\u9650\u6027\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u5143\u6587\u5316\u793e\u4f1a\u4e2d\u7684\u5b9e\u9645\u7a33\u5b9a\u6027\u8868\u73b0\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86ConfusedTourist\u6587\u5316\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30\u5957\u4ef6\uff0c\u901a\u8fc7\u56fe\u50cf\u5806\u53e0\u548c\u57fa\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u6270\u52a8\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u6d4b\u8bd5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5730\u7406\u6587\u5316\u7ebf\u7d22\u5e72\u6270\u4e0b\u7684\u7a33\u5b9a\u6027\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u7b80\u5355\u56fe\u50cf\u5806\u53e0\u6270\u52a8\u4e0b\u51c6\u786e\u7387\u6025\u5267\u4e0b\u964d\uff0c\u57fa\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u6270\u52a8\u7248\u672c\u8868\u73b0\u66f4\u5dee\uff0c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u8868\u660e\u5931\u8d25\u6e90\u4e8e\u7cfb\u7edf\u6027\u7684\u6ce8\u610f\u529b\u504f\u79fb\uff0c\u6a21\u578b\u88ab\u5e72\u6270\u7ebf\u7d22\u5206\u6563\u4e86\u539f\u672c\u7684\u7126\u70b9\u3002", "conclusion": "\u89c6\u89c9\u6587\u5316\u6982\u5ff5\u7684\u6df7\u5408\u4f1a\u4e25\u91cd\u635f\u5bb3\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u8fd9\u4e00\u53d1\u73b0\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u5177\u6587\u5316\u9c81\u68d2\u6027\u7684\u591a\u6a21\u6001\u7406\u89e3\u6280\u672f\u7684\u8feb\u5207\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2511.17053", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17053", "abs": "https://arxiv.org/abs/2511.17053", "authors": ["Teng Fu", "Mengyang Zhao", "Ke Niu", "Kaixin Peng", "Bin Li"], "title": "OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding", "comment": "AAAI 2026", "summary": "LVLMs have been shown to perform excellently in image-level tasks such as VQA and caption. However, in many instance-level tasks, such as visual grounding and object detection, LVLMs still show performance gaps compared to previous expert models. Meanwhile, although pedestrian tracking is a classical task, there have been a number of new topics in combining object tracking and natural language, such as Referring MOT, Cross-view Referring MOT, and Semantic MOT. These tasks emphasize that models should understand the tracked object at an advanced semantic level, which is exactly where LVLMs excel. In this paper, we propose a new unified Pedestrian Tracking framework, namely OmniPT, which can track, track based on reference and generate semantic understanding of tracked objects interactively. We address two issues: how to model the tracking task into a task that foundation models can perform, and how to make the model output formatted answers. To this end, we implement a training phase consisting of RL-Mid Training-SFT-RL. Based on the pre-trained weights of the LVLM, we first perform a simple RL phase to enable the model to output fixed and supervisable bounding box format. Subsequently, we conduct a mid-training phase using a large number of pedestrian-related datasets. Finally, we perform supervised fine-tuning on several pedestrian tracking datasets, and then carry out another RL phase to improve the model's tracking performance and enhance its ability to follow instructions. We conduct experiments on tracking benchmarks and the experimental results demonstrate that the proposed method can perform better than the previous methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOmniPT\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u884c\u4eba\u8ddf\u8e2a\u6846\u67b6\uff0c\u80fd\u591f\u57fa\u4e8e\u53c2\u8003\u8fdb\u884c\u8ddf\u8e2a\u5e76\u4e3a\u8ddf\u8e2a\u5bf9\u8c61\u751f\u6210\u8bed\u4e49\u7406\u89e3\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u5728\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LVLMs\u5728\u56fe\u50cf\u7ea7\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b9e\u4f8b\u7ea7\u4efb\u52a1\u5982\u89c6\u89c9\u5b9a\u4f4d\u548c\u76ee\u6807\u68c0\u6d4b\u4e2d\u4ecd\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u540c\u65f6\u884c\u4eba\u8ddf\u8e2a\u9886\u57df\u51fa\u73b0\u4e86\u7ed3\u5408\u76ee\u6807\u8ddf\u8e2a\u4e0e\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7684\u65b0\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u8981\u6c42\u6a21\u578b\u5728\u9ad8\u7ea7\u8bed\u4e49\u5c42\u9762\u7406\u89e3\u8ddf\u8e2a\u5bf9\u8c61\uff0c\u8fd9\u6b63\u662fLVLMs\u7684\u4f18\u52bf\u6240\u5728\u3002", "method": "\u63d0\u51faOmniPT\u6846\u67b6\uff0c\u91c7\u7528RL-Mid Training-SFT-RL\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u901a\u8fc7RL\u9636\u6bb5\u4f7f\u6a21\u578b\u8f93\u51fa\u56fa\u5b9a\u683c\u5f0f\u7684\u8fb9\u754c\u6846\uff0c\u7136\u540e\u4f7f\u7528\u5927\u91cf\u884c\u4eba\u76f8\u5173\u6570\u636e\u96c6\u8fdb\u884c\u4e2d\u95f4\u8bad\u7ec3\uff0c\u63a5\u7740\u5728\u591a\u4e2a\u884c\u4eba\u8ddf\u8e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u6700\u540e\u518d\u6b21\u8fdb\u884cRL\u9636\u6bb5\u4ee5\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002", "result": "\u5728\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6bd4\u5148\u524d\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u884c\u4eba\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u8ddf\u8e2a\u4efb\u52a1\u5efa\u6a21\u4e3a\u57fa\u7840\u6a21\u578b\u53ef\u6267\u884c\u7684\u4efb\u52a1\uff0c\u5e76\u89e3\u51b3\u4e86\u6a21\u578b\u8f93\u51fa\u683c\u5f0f\u5316\u7b54\u6848\u7684\u95ee\u9898\uff0c\u4e3a\u7ed3\u5408LVLMs\u4e0e\u5b9e\u4f8b\u7ea7\u8ddf\u8e2a\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8bed\u4e49\u7406\u89e3\u4e0e\u76ee\u6807\u8ddf\u8e2a\u7684\u878d\u5408\u3002"}}
{"id": "2511.16920", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16920", "abs": "https://arxiv.org/abs/2511.16920", "authors": ["Chaoran Xu", "Chengkan Lv", "Qiyu Chen", "Yunkang Cao", "Feng Zhang", "Zhengtao Zhang"], "title": "DeltaDeno: Zero-Shot Anomaly Generation via Delta-Denoising Attribution", "comment": null, "summary": "Anomaly generation is often framed as few-shot fine-tuning with anomalous samples, which contradicts the scarcity that motivates generation and tends to overfit category priors. We tackle the setting where no real anomaly samples or training are available. We propose Delta-Denoising (DeltaDeno), a training-free zero-shot anomaly generation method that localizes and edits defects by contrasting two diffusion branches driven by a minimal prompt pair under a shared schedule. By accumulating per-step denoising deltas into an image-specific localization map, we obtain a mask to guide the latent inpainting during later diffusion steps and preserve the surrounding context while generating realistic local defects. To improve stability and control, DeltaDeno performs token-level prompt refinement that aligns shared content and strengthens anomaly tokens, and applies a spatial attention bias restricted to anomaly tokens in the predicted region. Experiments on public datasets show that DeltaDeno achieves great generation, realism and consistent gains in downstream detection performance. Code will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDelta-Denoising (DeltaDeno)\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u96f6\u6837\u672c\u7684\u5f02\u5e38\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e24\u4e2a\u6269\u6563\u5206\u652f\u5728\u5171\u4eab\u8c03\u5ea6\u4e0b\u7684\u53bb\u566a\u5dee\u5f02\u6765\u5b9a\u4f4d\u548c\u7f16\u8f91\u7f3a\u9677\uff0c\u89e3\u51b3\u4e86\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5c11\u91cf\u5f02\u5e38\u6837\u672c\u8fdb\u884c\u5fae\u8c03\uff0c\u8fd9\u4e0e\u5f02\u5e38\u7a00\u7f3a\u6027\u7684\u73b0\u5b9e\u76f8\u77db\u76fe\uff0c\u4e14\u5bb9\u6613\u5bfc\u81f4\u7c7b\u522b\u5148\u9a8c\u8fc7\u62df\u5408\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u65e0\u9700\u771f\u5b9e\u5f02\u5e38\u6837\u672c\u6216\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u5f02\u5e38\u751f\u6210\u95ee\u9898\u3002", "method": "DeltaDeno\u901a\u8fc7\u5bf9\u6bd4\u7531\u6700\u5c0f\u63d0\u793a\u5bf9\u9a71\u52a8\u7684\u4e24\u4e2a\u6269\u6563\u5206\u652f\u5728\u5171\u4eab\u8c03\u5ea6\u4e0b\u7684\u53bb\u566a\u5dee\u5f02\uff0c\u5c06\u6bcf\u6b65\u53bb\u566a\u5dee\u5f02\u7d2f\u79ef\u4e3a\u56fe\u50cf\u7279\u5b9a\u5b9a\u4f4d\u56fe\uff0c\u751f\u6210\u63a9\u7801\u6307\u5bfc\u6f5c\u5728\u7a7a\u95f4\u4fee\u590d\uff0c\u540c\u65f6\u4fdd\u7559\u4e0a\u4e0b\u6587\u5e76\u751f\u6210\u771f\u5b9e\u5c40\u90e8\u7f3a\u9677\u3002\u65b9\u6cd5\u8fd8\u5305\u62ec\u4ee4\u724c\u7ea7\u63d0\u793a\u7cbe\u70bc\u4ee5\u5bf9\u9f50\u5171\u4eab\u5185\u5bb9\u5e76\u5f3a\u5316\u5f02\u5e38\u4ee4\u724c\uff0c\u4ee5\u53ca\u5728\u9884\u6d4b\u533a\u57df\u5e94\u7528\u4ec5\u9650\u5f02\u5e38\u4ee4\u724c\u7684\u7a7a\u95f4\u6ce8\u610f\u529b\u504f\u7f6e\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDeltaDeno\u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u751f\u6210\u8d28\u91cf\u3001\u771f\u5b9e\u611f\uff0c\u5e76\u5728\u4e0b\u6e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u65e0\u9700\u771f\u5b9e\u5f02\u5e38\u6837\u672c\u7684\u96f6\u6837\u672c\u5f02\u5e38\u751f\u6210\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5bf9\u6bd4\u548c\u5b9a\u4f4d\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7f3a\u9677\u751f\u6210\uff0c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u7b49\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u589e\u5f3a\u6570\u636e\u6765\u6e90\u3002"}}
{"id": "2511.17450", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17450", "abs": "https://arxiv.org/abs/2511.17450", "authors": ["Yidong Huang", "Zun Wang", "Han Lin", "Dong-Ki Kim", "Shayegan Omidshafiei", "Jaehong Yoon", "Yue Zhang", "Mohit Bansal"], "title": "Planning with Sketch-Guided Verification for Physics-Aware Video Generation", "comment": "website: https://sketchverify.github.io/", "summary": "Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSketchVerify\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u4e8e\u8349\u56fe\u9a8c\u8bc1\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u91c7\u6837\u548c\u9a8c\u8bc1\u5faa\u73af\u751f\u6210\u52a8\u6001\u4e00\u81f4\u7684\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u8fd0\u52a8\u8d28\u91cf\u548c\u7269\u7406\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u6b21\u8fd0\u52a8\u89c4\u5212\uff0c\u901a\u5e38\u53ea\u80fd\u5904\u7406\u7b80\u5355\u8fd0\u52a8\uff0c\u6216\u8005\u9700\u8981\u591a\u6b21\u8c03\u7528\u89c6\u9891\u751f\u6210\u5668\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u8fd9\u9650\u5236\u4e86\u590d\u6742\u52a8\u6001\u573a\u666f\u7684\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9884\u6d4b\u591a\u4e2a\u5019\u9009\u8fd0\u52a8\u8f68\u8ff9\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u9a8c\u8bc1\u5668\u5bf9\u8f68\u8ff9\u8fdb\u884c\u8054\u5408\u8bc4\u4f30\uff0c\u8003\u8651\u8bed\u4e49\u5bf9\u9f50\u548c\u7269\u7406\u5408\u7406\u6027\uff1b\u901a\u8fc7\u5c06\u8f68\u8ff9\u6e32\u67d3\u4e3a\u8f7b\u91cf\u7ea7\u89c6\u9891\u8349\u56fe\u5728\u9759\u6001\u80cc\u666f\u4e0a\u5408\u6210\u5bf9\u8c61\uff0c\u907f\u514d\u6602\u8d35\u7684\u91cd\u590d\u6269\u6563\u5408\u6210\uff1b\u91c7\u7528\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\u76f4\u5230\u8bc6\u522b\u51fa\u6ee1\u610f\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u5728WorldModelBench\u548cPhyWorldBench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd0\u52a8\u8d28\u91cf\u3001\u7269\u7406\u771f\u5b9e\u6027\u548c\u957f\u671f\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u540c\u65f6\u8ba1\u7b97\u6548\u7387\u5927\u5e45\u63d0\u5347\uff1b\u6d88\u878d\u7814\u7a76\u663e\u793a\u589e\u52a0\u8f68\u8ff9\u5019\u9009\u6570\u91cf\u80fd\u6301\u7eed\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6d4b\u8bd5\u65f6\u9a8c\u8bc1\u5faa\u73af\u5728\u63d0\u5347\u8fd0\u52a8\u89c4\u5212\u8d28\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u8f7b\u91cf\u7ea7\u8349\u56fe\u6e32\u67d3\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8fd0\u52a8\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17068", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17068", "abs": "https://arxiv.org/abs/2511.17068", "authors": ["Junming Liu", "Yifei Sun", "Weihua Cheng", "Yujin Kang", "Yirong Chen", "Ding Wang", "Guosun Zeng"], "title": "ReBrain: Brain MRI Reconstruction from Sparse CT Slice via Retrieval-Augmented Diffusion", "comment": "16 pages, 12 figures, 7 tables; Accepted by WACV 2026", "summary": "Magnetic Resonance Imaging (MRI) plays a crucial role in brain disease diagnosis, but it is not always feasible for certain patients due to physical or clinical constraints. Recent studies attempt to synthesize MRI from Computed Tomography (CT) scans; however, low-dose protocols often result in highly sparse CT volumes with poor through-plane resolution, making accurate reconstruction of the full brain MRI volume particularly challenging. To address this, we propose ReBrain, a retrieval-augmented diffusion framework for brain MRI reconstruction. Given any 3D CT scan with limited slices, we first employ a Brownian Bridge Diffusion Model (BBDM) to synthesize MRI slices along the 2D dimension. Simultaneously, we retrieve structurally and pathologically similar CT slices from a comprehensive prior database via a fine-tuned retrieval model. These retrieved slices are used as references, incorporated through a ControlNet branch to guide the generation of intermediate MRI slices and ensure structural continuity. We further account for rare retrieval failures when the database lacks suitable references and apply spherical linear interpolation to provide supplementary guidance. Extensive experiments on SynthRAD2023 and BraTS demonstrate that ReBrain achieves state-of-the-art performance in cross-modal reconstruction under sparse conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReBrain\uff0c\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u6269\u6563\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758fCT\u626b\u63cf\u91cd\u5efa\u5b8c\u6574\u8111\u90e8MRI\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u68c0\u7d22\u76f8\u4f3cCT\u5207\u7247\u4f5c\u4e3a\u53c2\u8003\uff0c\u7ed3\u5408Brownian Bridge\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f4e\u5242\u91cfCT\u534f\u8bae\u5bfc\u81f4\u7684\u7a00\u758f\u4f53\u79ef\u91cd\u5efa\u6311\u6218\u3002", "motivation": "\u8111\u90e8MRI\u5728\u75be\u75c5\u8bca\u65ad\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u67d0\u4e9b\u60a3\u8005\u56e0\u8eab\u4f53\u6216\u4e34\u5e8a\u9650\u5236\u65e0\u6cd5\u8fdb\u884cMRI\u68c0\u67e5\u3002\u73b0\u6709\u4eceCT\u5408\u6210MRI\u7684\u65b9\u6cd5\u9762\u4e34\u4f4e\u5242\u91cf\u534f\u8bae\u5bfc\u81f4\u7684\u7a00\u758fCT\u4f53\u79ef\u548c\u8f83\u5dee\u5e73\u9762\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u4f7f\u5f97\u51c6\u786e\u91cd\u5efa\u5b8c\u6574\u8111\u90e8MRI\u4f53\u79ef\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faReBrain\u6846\u67b6\uff0c\u9996\u5148\u4f7f\u7528Brownian Bridge\u6269\u6563\u6a21\u578b\u57282D\u7ef4\u5ea6\u5408\u6210MRI\u5207\u7247\uff0c\u540c\u65f6\u901a\u8fc7\u5fae\u8c03\u68c0\u7d22\u6a21\u578b\u4ece\u5148\u9a8c\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u7ed3\u6784\u548c\u75c5\u7406\u76f8\u4f3c\u7684CT\u5207\u7247\u4f5c\u4e3a\u53c2\u8003\u3002\u901a\u8fc7ControlNet\u5206\u652f\u6574\u5408\u68c0\u7d22\u5207\u7247\u4ee5\u6307\u5bfc\u4e2d\u95f4MRI\u5207\u7247\u751f\u6210\uff0c\u786e\u4fdd\u7ed3\u6784\u8fde\u7eed\u6027\uff0c\u5e76\u5bf9\u7f55\u89c1\u68c0\u7d22\u5931\u8d25\u60c5\u51b5\u5e94\u7528\u7403\u9762\u7ebf\u6027\u63d2\u503c\u63d0\u4f9b\u8865\u5145\u6307\u5bfc\u3002", "result": "\u5728SynthRAD2023\u548cBraTS\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cReBrain\u5728\u7a00\u758f\u6761\u4ef6\u4e0b\u7684\u8de8\u6a21\u6001\u91cd\u5efa\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u7a00\u758fCT\u5230MRI\u7684\u8f6c\u6362\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u68c0\u7d22\u589e\u5f3a\u6269\u6563\u6846\u67b6\u5728\u89e3\u51b3\u7a00\u758f\u533b\u5b66\u56fe\u50cf\u91cd\u5efa\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4e34\u5e8a\u4e2d\u65e0\u6cd5\u8fdb\u884cMRI\u68c0\u67e5\u7684\u60a3\u8005\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u4e3a\u8de8\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5408\u6210\u5f00\u8f9f\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2511.16940", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.16940", "abs": "https://arxiv.org/abs/2511.16940", "authors": ["Xiongtao Sun", "Hui Li", "Jiaming Zhang", "Yujie Yang", "Kaili Liu", "Ruxin Feng", "Wen Jun Tan", "Wei Yang Bryan Lim"], "title": "MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models", "comment": null, "summary": "Modern Vision-Language Models (VLMs) demonstrate sophisticated reasoning, escalating privacy risks beyond simple attribute perception to individual-level linkage. Current privacy benchmarks are structurally insufficient for this new threat, as they primarily evaluate privacy perception while failing to address the more critical risk of privacy reasoning: a VLM's ability to infer and link distributed information to construct individual profiles. To address this critical gap, we propose \\textbf{MultiPriv}, the first benchmark designed to systematically evaluate individual-level privacy reasoning in VLMs. We introduce the \\textbf{Privacy Perception and Reasoning (PPR)} framework and construct a novel, bilingual multimodal dataset to support it. The dataset uniquely features a core component of synthetic individual profiles where identifiers (e.g., faces, names) are meticulously linked to sensitive attributes. This design enables nine challenging tasks evaluating the full PPR spectrum, from attribute detection to cross-image re-identification and chained inference. We conduct a large-scale evaluation of over 50 foundational and commercial VLMs. Our analysis reveals: (1) Many VLMs possess significant, unmeasured reasoning-based privacy risks. (2) Perception-level metrics are poor predictors of these reasoning risks, revealing a critical evaluation gap. (3) Existing safety alignments are inconsistent and ineffective against such reasoning-based attacks. MultiPriv exposes systemic vulnerabilities and provides the necessary framework for developing robust, privacy-preserving VLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MultiPriv\uff0c\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u4e2a\u4f53\u7ea7\u9690\u79c1\u63a8\u7406\u98ce\u9669\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u6784\u5efa\u53cc\u8bed\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u9690\u79c1\u611f\u77e5\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709VLMs\u5728\u9690\u79c1\u63a8\u7406\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u9690\u79c1\u98ce\u9669\u4ece\u7b80\u5355\u7684\u5c5e\u6027\u611f\u77e5\u5347\u7ea7\u5230\u4e2a\u4f53\u7ea7\u5173\u8054\uff0c\u800c\u73b0\u6709\u9690\u79c1\u57fa\u51c6\u5728\u7ed3\u6784\u4e0a\u65e0\u6cd5\u5e94\u5bf9\u8fd9\u79cd\u65b0\u5a01\u80c1\uff0c\u4e3b\u8981\u8bc4\u4f30\u9690\u79c1\u611f\u77e5\u800c\u672a\u80fd\u89e3\u51b3\u66f4\u5173\u952e\u7684\u9690\u79c1\u63a8\u7406\u98ce\u9669\uff0c\u5373VLMs\u63a8\u65ad\u548c\u94fe\u63a5\u5206\u5e03\u5f0f\u4fe1\u606f\u4ee5\u6784\u5efa\u4e2a\u4f53\u6863\u6848\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u9690\u79c1\u611f\u77e5\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u53cc\u8bed\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5176\u6838\u5fc3\u7279\u70b9\u662f\u5305\u542b\u5408\u6210\u4e2a\u4f53\u6863\u6848\uff0c\u5176\u4e2d\u6807\u8bc6\u7b26\u4e0e\u654f\u611f\u5c5e\u6027\u88ab\u7cbe\u5fc3\u94fe\u63a5\uff0c\u652f\u6301\u4e5d\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u8bc4\u4f30\u5b8c\u6574\u7684PPR\u8c31\u7cfb\uff0c\u4ece\u5c5e\u6027\u68c0\u6d4b\u5230\u8de8\u56fe\u50cf\u91cd\u8bc6\u522b\u548c\u94fe\u5f0f\u63a8\u7406\u3002", "result": "\u5bf9\u8d85\u8fc750\u4e2a\u57fa\u7840\u548c\u5546\u4e1aVLMs\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u53d1\u73b0\u8bb8\u591aVLMs\u5b58\u5728\u663e\u8457\u4e14\u672a\u88ab\u6d4b\u91cf\u7684\u57fa\u4e8e\u63a8\u7406\u7684\u9690\u79c1\u98ce\u9669\uff0c\u611f\u77e5\u7ea7\u6307\u6807\u5bf9\u8fd9\u4e9b\u63a8\u7406\u98ce\u9669\u7684\u9884\u6d4b\u80fd\u529b\u8f83\u5dee\uff0c\u63ed\u793a\u4e86\u5173\u952e\u8bc4\u4f30\u5dee\u8ddd\uff0c\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u5bf9\u6b64\u7c7b\u57fa\u4e8e\u63a8\u7406\u7684\u653b\u51fb\u4e0d\u4e00\u81f4\u4e14\u65e0\u6548\u3002", "conclusion": "MultiPriv\u66b4\u9732\u4e86VLMs\u5728\u9690\u79c1\u63a8\u7406\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u6f0f\u6d1e\uff0c\u4e3a\u5f00\u53d1\u9c81\u68d2\u7684\u9690\u79c1\u4fdd\u62a4VLMs\u63d0\u4f9b\u4e86\u5fc5\u8981\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u9690\u79c1\u611f\u77e5\u8bc4\u4f30\u6765\u5e94\u5bf9\u65b0\u5174\u63a8\u7406\u5a01\u80c1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.16951", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16951", "abs": "https://arxiv.org/abs/2511.16951", "authors": ["Xin Shen", "Rui Zhu", "Lei Shen", "Xinyu Wang", "Kaihao Zhang", "Tianqing Zhu", "Shuchen Wu", "Chenxi Miao", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang", "Xin Yu"], "title": "FingerCap: Fine-grained Finger-level Hand Motion Captioning", "comment": null, "summary": "Understanding fine-grained human hand motion is fundamental to visual perception, embodied intelligence, and multimodal communication. In this work, we propose Fine-grained Finger-level Hand Motion Captioning (FingerCap), which aims to generate textual descriptions that capture detailed finger-level semantics of hand actions. To support this task, we curate FingerCap-40K, a large-scale corpus of 40K paired hand-motion videos and captions spanning two complementary sources: concise instruction-style finger motions and diverse, naturalistic hand-object interactions. To enable effective evaluation, we employ HandJudge, a LLM-based rubric that measures finger-level correctness and motion completeness. Temporal sparsity remains a fundamental bottleneck for current Video-MLLMs, since sparse RGB sampling is insufficient to capture the subtle, high-frequency dynamics underlying fine finger motions. As a simple and compute-friendly remedy, we introduce FiGOP (Finger Group-of-Pictures), which pairs each RGB keyframe with subsequent hand keypoints until the next keyframe. A lightweight temporal encoder converts the keypoints into motion embeddings and integrates them with RGB features. FiGOP adapts the classic GOP concept to finger motion, recovering fine temporal cues without increasing RGB density. Experiments on FingerCap-40K show that strong open- and closed-source Video-MLLMs still struggle with finger-level reasoning, while our FiGOP-augmented model yield consistent gains under HandJudge and human studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FingerCap\u4efb\u52a1\u7528\u4e8e\u751f\u6210\u7ec6\u7c92\u5ea6\u624b\u6307\u7ea7\u624b\u90e8\u8fd0\u52a8\u63cf\u8ff0\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b40K\u914d\u5bf9\u89c6\u9891-\u6587\u672c\u7684FingerCap-40K\u6570\u636e\u96c6\u3002\u4e3a\u89e3\u51b3\u89c6\u9891MLLMs\u5728\u65f6\u95f4\u7a00\u758f\u6027\u4e0a\u7684\u74f6\u9888\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86FiGOP\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408RGB\u5173\u952e\u5e27\u548c\u624b\u90e8\u5173\u952e\u70b9\u6765\u6062\u590d\u7cbe\u7ec6\u65f6\u95f4\u7ebf\u7d22\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u611f\u77e5\u548c\u5177\u8eab\u667a\u80fd\u9886\u57df\u7f3a\u4e4f\u5bf9\u7ec6\u7c92\u5ea6\u624b\u6307\u7ea7\u624b\u90e8\u8fd0\u52a8\u7684\u7406\u89e3\u80fd\u529b\uff0c\u73b0\u6709\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7531\u4e8e\u65f6\u95f4\u7a00\u758f\u91c7\u6837\u65e0\u6cd5\u6355\u6349\u624b\u6307\u8fd0\u52a8\u7684\u9ad8\u9891\u7ec6\u5fae\u52a8\u6001\uff0c\u8fd9\u6210\u4e3a\u624b\u6307\u7ea7\u63a8\u7406\u7684\u57fa\u672c\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86FiGOP\u65b9\u6cd5\uff0c\u5c06\u6bcf\u4e2aRGB\u5173\u952e\u5e27\u4e0e\u540e\u7eed\u624b\u90e8\u5173\u952e\u70b9\u914d\u5bf9\u76f4\u5230\u4e0b\u4e00\u5173\u952e\u5e27\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u65f6\u95f4\u7f16\u7801\u5668\u5c06\u5173\u952e\u70b9\u8f6c\u6362\u4e3a\u8fd0\u52a8\u5d4c\u5165\u5e76\u4e0eRGB\u7279\u5f81\u96c6\u6210\u3002\u8be5\u65b9\u6cd5\u9002\u5e94\u4e86\u7ecf\u5178GOP\u6982\u5ff5\u5230\u624b\u6307\u8fd0\u52a8\uff0c\u5728\u4e0d\u589e\u52a0RGB\u5bc6\u5ea6\u7684\u60c5\u51b5\u4e0b\u6062\u590d\u7cbe\u7ec6\u65f6\u95f4\u7ebf\u7d22\u3002", "result": "\u5728FingerCap-40K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u5f3a\u5927\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u89c6\u9891MLLMs\u5728\u624b\u6307\u7ea7\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u800cFiGOP\u589e\u5f3a\u6a21\u578b\u5728HandJudge\u8bc4\u4f30\u548c\u4eba\u7c7b\u7814\u7a76\u4e2d\u5747\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "FiGOP\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u89c6\u9891MLLMs\u7684\u65f6\u95f4\u7a00\u758f\u6027\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u6355\u6349\u9ad8\u9891\u7ec6\u5fae\u52a8\u6001\u7684\u7ec6\u7c92\u5ea6\u624b\u90e8\u8fd0\u52a8\u7406\u89e3\u4efb\u52a1\uff0c\u4e3a\u624b\u90e8\u52a8\u4f5c\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.17254", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17254", "abs": "https://arxiv.org/abs/2511.17254", "authors": ["Jiaye Qian", "Ge Zheng", "Yuchen Zhu", "Sibei Yang"], "title": "Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats", "comment": "Accepted to NeurIPS 2025, Project Page: https://github.com/SooLab/AllPath", "summary": "Despite their impressive performance across a wide range of tasks, Large Vision-Language Models (LVLMs) remain prone to hallucination. In this study, we propose a comprehensive intervention framework aligned with the transformer's causal architecture in LVLMs, integrating the effects of different intervention paths on hallucination. We find that hallucinations in LVLMs do not arise from a single causal path, but rather from the interplay among image-to-input-text, image-to-output-text, and text-to-text pathways. For the first time, we also find that LVLMs rely on different pathways depending on the question-answer alignment format. Building on these insights, we propose simple yet effective methods to identify and intervene on critical hallucination heads within each pathway, tailored to discriminative and generative formats. Experiments across multiple benchmarks demonstrate that our approach consistently reduces hallucinations across diverse alignment types.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09Transformer\u56e0\u679c\u67b6\u6784\u5bf9\u9f50\u7684\u7efc\u5408\u5e72\u9884\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u56fe\u50cf-\u8f93\u5165\u6587\u672c\u3001\u56fe\u50cf-\u8f93\u51fa\u6587\u672c\u548c\u6587\u672c-\u6587\u672c\u4e09\u79cd\u8def\u5f84\u7684\u4ea4\u4e92\u4f5c\u7528\u6765\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\u3002\u7814\u7a76\u53d1\u73b0LVLM\u7684\u5e7b\u89c9\u6e90\u4e8e\u591a\u8def\u5f84\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u9996\u6b21\u53d1\u73b0\u6a21\u578b\u4f9d\u8d56\u4e0d\u540c\u8def\u5f84\u53d6\u51b3\u4e8e\u95ee\u7b54\u5bf9\u9f50\u683c\u5f0f\uff0c\u636e\u6b64\u63d0\u51fa\u4e86\u9488\u5bf9\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u683c\u5f0f\u7684\u5173\u952e\u5e7b\u89c9\u5934\u8bc6\u522b\u4e0e\u5e72\u9884\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u73b0\u8c61\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3LVLM\u4e2d\u5e7b\u89c9\u95ee\u9898\u7684\u6839\u672c\u539f\u56e0\uff0c\u63a2\u7d22\u4e0d\u540c\u56e0\u679c\u8def\u5f84\u5bf9\u5e7b\u89c9\u4ea7\u751f\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u6709\u6548\u7684\u5e72\u9884\u65b9\u6cd5\u6765\u51cf\u5c11\u8fd9\u79cd\u4e0d\u826f\u73b0\u8c61\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0eTransformer\u56e0\u679c\u67b6\u6784\u5bf9\u9f50\u7684\u7efc\u5408\u5e72\u9884\u6846\u67b6\uff0c\u5206\u6790\u4e86\u56fe\u50cf-\u8f93\u5165\u6587\u672c\u3001\u56fe\u50cf-\u8f93\u51fa\u6587\u672c\u548c\u6587\u672c-\u6587\u672c\u4e09\u79cd\u8def\u5f84\u7684\u4ea4\u4e92\u4f5c\u7528\u3002\u9996\u6b21\u53d1\u73b0LVLM\u6839\u636e\u95ee\u7b54\u5bf9\u9f50\u683c\u5f0f\u4f9d\u8d56\u4e0d\u540c\u8def\u5f84\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u683c\u5f0f\u7684\u5173\u952e\u5e7b\u89c9\u5934\u8bc6\u522b\u4e0e\u5e72\u9884\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7b80\u5355\u800c\u6709\u6548\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6301\u7eed\u51cf\u5c11\u5404\u79cd\u5bf9\u9f50\u7c7b\u578b\u4e0b\u7684\u5e7b\u89c9\u73b0\u8c61\u3002\u901a\u8fc7\u5e72\u9884\u5173\u952e\u5e7b\u89c9\u5934\uff0c\u5728\u4e0d\u540c\u95ee\u7b54\u683c\u5f0f\u4e0b\u90fd\u5b9e\u73b0\u4e86\u5e7b\u89c9\u7684\u663e\u8457\u964d\u4f4e\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LVLM\u5e7b\u89c9\u7684\u591a\u8def\u5f84\u672c\u8d28\uff0c\u8868\u660e\u5e7b\u89c9\u4e0d\u662f\u5355\u4e00\u56e0\u679c\u8def\u5f84\u7684\u7ed3\u679c\uff0c\u800c\u662f\u591a\u79cd\u8def\u5f84\u76f8\u4e92\u4f5c\u7528\u7684\u4ea7\u7269\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u7406\u89e3LVLM\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6240\u63d0\u51fa\u7684\u5e72\u9884\u6846\u67b6\u4e3a\u51cf\u5c11\u5e7b\u89c9\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u63d0\u5347LVLM\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.16955", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.16955", "abs": "https://arxiv.org/abs/2511.16955", "authors": ["Dailan He", "Guanlin Feng", "Xingtong Ge", "Yazhe Niu", "Yi Zhang", "Bingqi Ma", "Guanglu Song", "Yu Liu", "Hongsheng Li"], "title": "Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has shown promise in aligning image and video generative models with human preferences. However, applying it to modern flow matching models is challenging because of its deterministic sampling paradigm. Current methods address this issue by converting Ordinary Differential Equations (ODEs) to Stochastic Differential Equations (SDEs), which introduce stochasticity. However, this SDE-based GRPO suffers from issues of inefficient credit assignment and incompatibility with high-order solvers for fewer-step sampling. In this paper, we first reinterpret existing SDE-based GRPO methods from a distance optimization perspective, revealing their underlying mechanism as a form of contrastive learning. Based on this insight, we propose Neighbor GRPO, a novel alignment algorithm that completely bypasses the need for SDEs. Neighbor GRPO generates a diverse set of candidate trajectories by perturbing the initial noise conditions of the ODE and optimizes the model using a softmax distance-based surrogate leaping policy. We establish a theoretical connection between this distance-based objective and policy gradient optimization, rigorously integrating our approach into the GRPO framework. Our method fully preserves the advantages of deterministic ODE sampling, including efficiency and compatibility with high-order solvers. We further introduce symmetric anchor sampling for computational efficiency and group-wise quasi-norm reweighting to address reward flattening. Extensive experiments demonstrate that Neighbor GRPO significantly outperforms SDE-based counterparts in terms of training cost, convergence speed, and generation quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Neighbor GRPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8ODE\u521d\u59cb\u566a\u58f0\u6761\u4ef6\u751f\u6210\u591a\u6837\u5316\u8f68\u8ff9\uff0c\u5b8c\u5168\u7ed5\u5f00SDE\u8f6c\u6362\u9700\u6c42\uff0c\u89e3\u51b3\u4e86SDE-based GRPO\u5728\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u4f4e\u6548\u548c\u9ad8\u9636\u6c42\u89e3\u5668\u4e0d\u517c\u5bb9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709SDE-based GRPO\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u73b0\u4ee3\u6d41\u5339\u914d\u6a21\u578b\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5176\u786e\u5b9a\u6027\u91c7\u6837\u8303\u5f0f\u4e0eGRPO\u7684\u968f\u673a\u6027\u9700\u6c42\u5b58\u5728\u51b2\u7a81\u3002\u5f53\u524d\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u5c06ODE\u8f6c\u6362\u4e3aSDE\u5f15\u5165\u968f\u673a\u6027\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u4fe1\u7528\u5206\u914d\u4f4e\u6548\u548c\u4e0e\u9ad8\u9636\u6c42\u89e3\u5668\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u91c7\u6837\u8d28\u91cf\u3002", "method": "\u672c\u6587\u9996\u5148\u4ece\u8ddd\u79bb\u4f18\u5316\u89d2\u5ea6\u91cd\u65b0\u89e3\u91ca\u73b0\u6709SDE-based GRPO\u65b9\u6cd5\uff0c\u63ed\u793a\u5176\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\u672c\u8d28\u3002\u57fa\u4e8e\u6b64\u63d0\u51faNeighbor GRPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8ODE\u521d\u59cb\u566a\u58f0\u6761\u4ef6\u751f\u6210\u591a\u6837\u5316\u5019\u9009\u8f68\u8ff9\uff0c\u4f7f\u7528\u57fa\u4e8esoftmax\u8ddd\u79bb\u7684\u4ee3\u7406\u8df3\u8dc3\u7b56\u7565\u8fdb\u884c\u6a21\u578b\u4f18\u5316\u3002\u65b9\u6cd5\u8fd8\u5f15\u5165\u5bf9\u79f0\u951a\u70b9\u91c7\u6837\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u4ee5\u53ca\u7ec4\u95f4\u62df\u8303\u6570\u91cd\u52a0\u6743\u89e3\u51b3\u5956\u52b1\u5e73\u5766\u5316\u95ee\u9898\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cNeighbor GRPO\u5728\u8bad\u7ec3\u6210\u672c\u3001\u6536\u655b\u901f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8eSDE-based\u5bf9\u5e94\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5b8c\u5168\u4fdd\u7559\u4e86\u786e\u5b9a\u6027ODE\u91c7\u6837\u7684\u4f18\u52bf\uff0c\u5305\u62ec\u9ad8\u6548\u6027\u548c\u4e0e\u9ad8\u9636\u6c42\u89e3\u5668\u7684\u517c\u5bb9\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "Neighbor GRPO\u4e3a\u6d41\u5339\u914d\u6a21\u578b\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u89e3\u51b3\u4e86SDE-based\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u8fd8\u5efa\u7acb\u4e86\u8ddd\u79bb\u4f18\u5316\u76ee\u6807\u4e0e\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\u3002\u8be5\u65b9\u6cd5\u4e3a\u786e\u5b9a\u6027\u751f\u6210\u6a21\u578b\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u4ef7\u503c\u548c\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.17282", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17282", "abs": "https://arxiv.org/abs/2511.17282", "authors": ["Chuancheng Shi", "Shangze Li", "Shiming Guo", "Simiao Xie", "Wenhua Wu", "Jingtong Dou", "Chao Wu", "Canran Xiao", "Cong Wang", "Zifeng Cheng", "Fei Shen", "Tat-Seng Chua"], "title": "Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation", "comment": null, "summary": "Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u591a\u8bed\u8a00\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u6587\u5316\u4e00\u81f4\u6027\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4f4d\u6587\u5316\u654f\u611f\u795e\u7ecf\u5143\u5e76\u8bbe\u8ba1\u4e24\u79cd\u5bf9\u9f50\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u6587\u5316\u4e00\u81f4\u6027\u800c\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u591a\u8bed\u8a00\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u63d0\u793a\u4e0b\u5f80\u5f80\u4ea7\u751f\u6587\u5316\u4e2d\u6027\u6216\u82f1\u8bed\u504f\u5411\u7684\u7ed3\u679c\uff0c\u8fd9\u6e90\u4e8e\u8bed\u8a00\u643a\u5e26\u7684\u6587\u5316\u5185\u6db5\u672a\u88ab\u5145\u5206\u6fc0\u6d3b\uff0c\u800c\u975e\u6a21\u578b\u7f3a\u4e4f\u6587\u5316\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63a2\u6d4b\u65b9\u6cd5\u5b9a\u4f4d\u6587\u5316\u654f\u611f\u4fe1\u53f7\u5230\u5c11\u91cf\u56fa\u5b9a\u5c42\u4e2d\u7684\u7279\u5b9a\u795e\u7ecf\u5143\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u4e92\u8865\u5bf9\u9f50\u7b56\u7565\uff1a\u65e0\u9700\u4e3b\u5e72\u5fae\u8c03\u7684\u63a8\u7406\u65f6\u6587\u5316\u6fc0\u6d3b\u548c\u4ec5\u66f4\u65b0\u6587\u5316\u76f8\u5173\u5c42\u7684\u5c42\u5b9a\u5411\u6587\u5316\u589e\u5f3a\u3002", "result": "\u5728CultureBench\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6587\u5316\u4e00\u81f4\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u81f4\u6027\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6587\u5316\u77e5\u8bc6\u5df2\u5b58\u5728\u4e8e\u6a21\u578b\u4e2d\u4f46\u6fc0\u6d3b\u4e0d\u8db3\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u6fc0\u6d3b\u6587\u5316\u76f8\u5173\u795e\u7ecf\u5143\u53ef\u6709\u6548\u63d0\u5347\u8de8\u8bed\u8a00\u6587\u5316\u4e00\u81f4\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u7684\u6587\u5316\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.16991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16991", "abs": "https://arxiv.org/abs/2511.16991", "authors": ["Jonathan Skaza", "Parsa Madinei", "Ziqi Wen", "Miguel Eckstein"], "title": "DReX: Pure Vision Fusion of Self-Supervised and Convolutional Representations for Image Complexity Prediction", "comment": "8 pages", "summary": "Visual complexity prediction is a fundamental problem in computer vision with applications in image compression, retrieval, and classification. Understanding what makes humans perceive an image as complex is also a long-standing question in cognitive science. Recent approaches have leveraged multimodal models that combine visual and linguistic representations, but it remains unclear whether language information is necessary for this task. We propose DReX (DINO-ResNet Fusion), a vision-only model that fuses self-supervised and convolutional representations through a learnable attention mechanism to predict image complexity. Our architecture integrates multi-scale hierarchical features from ResNet-50 with semantically rich representations from DINOv3 ViT-S/16, enabling the model to capture both low-level texture patterns and high-level semantic structure. DReX achieves state-of-the-art performance on the IC9600 benchmark (Pearson r = 0.9581), surpassing previous methods--including those trained on multimodal image-text data--while using approximately 21.5x fewer learnable parameters. Furthermore, DReX generalizes robustly across multiple datasets and metrics, achieving superior results on Pearson and Spearman correlation, Root Mean Square Error (RMSE), and Mean Absolute Error (MAE). Ablation and attention analyses confirm that DReX leverages complementary cues from both backbones, with the DINOv3 [CLS] token enhancing sensitivity to visual complexity. Our findings suggest that visual features alone can be sufficient for human-aligned complexity prediction and that, when properly fused, self-supervised transformers and supervised deep convolutional neural networks offer complementary and synergistic benefits for this task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDReX\uff0c\u4e00\u79cd\u4ec5\u4f7f\u7528\u89c6\u89c9\u4fe1\u606f\u7684\u56fe\u50cf\u590d\u6742\u5ea6\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u81ea\u76d1\u7763\u548c\u5377\u79ef\u8868\u793a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8868\u660e\u89c6\u89c9\u7279\u5f81\u672c\u8eab\u8db3\u4ee5\u8fdb\u884c\u4eba\u7c7b\u5bf9\u9f50\u7684\u590d\u6742\u5ea6\u9884\u6d4b\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u590d\u6742\u5ea6\u9884\u6d4b\u65b9\u6cd5\u591a\u4f9d\u8d56\u591a\u6a21\u6001\u6a21\u578b\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\uff0c\u4f46\u8bed\u8a00\u4fe1\u606f\u5bf9\u6b64\u4efb\u52a1\u662f\u5426\u5fc5\u8981\u5c1a\u4e0d\u660e\u786e\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4ec5\u4f7f\u7528\u89c6\u89c9\u7279\u5f81\u8fdb\u884c\u590d\u6742\u5ea6\u9884\u6d4b\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51faDReX\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408ResNet-50\u7684\u591a\u5c3a\u5ea6\u5c42\u6b21\u7279\u5f81\u548cDINOv3 ViT-S/16\u7684\u8bed\u4e49\u4e30\u5bcc\u8868\u793a\uff0c\u80fd\u591f\u540c\u65f6\u6355\u83b7\u4f4e\u5c42\u7eb9\u7406\u6a21\u5f0f\u548c\u9ad8\u5c42\u8bed\u4e49\u7ed3\u6784\u3002", "result": "\u5728IC9600\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230Pearson\u76f8\u5173\u7cfb\u65700.9581\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u5305\u62ec\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u5185\u7684\u5148\u524d\u65b9\u6cd5\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u51cf\u5c11\u7ea621.5\u500d\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6307\u6807\u4e0a\u5c55\u73b0\u51fa\u7a33\u5065\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f53\u9002\u5f53\u878d\u5408\u65f6\uff0c\u81ea\u76d1\u7763transformer\u548c\u76d1\u7763\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5bf9\u6b64\u4efb\u52a1\u5177\u6709\u4e92\u8865\u548c\u534f\u540c\u6548\u76ca\uff0c\u89c6\u89c9\u7279\u5f81\u672c\u8eab\u8db3\u4ee5\u8fdb\u884c\u4eba\u7c7b\u5bf9\u9f50\u7684\u590d\u6742\u5ea6\u9884\u6d4b\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u8ba4\u77e5\u79d1\u5b66\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2511.17309", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17309", "abs": "https://arxiv.org/abs/2511.17309", "authors": ["David Nordstr\u00f6m", "Johan Edstedt", "Fredrik Kahl", "Georg B\u00f6kman"], "title": "MuM: Multi-View Masked Image Modeling for 3D Vision", "comment": null, "summary": "Self-supervised learning on images seeks to extract meaningful visual representations from unlabeled data. When scaled to large datasets, this paradigm has achieved state-of-the-art performance and the resulting trained models such as DINOv3 have seen widespread adoption. However, most prior efforts are optimized for semantic understanding rather than geometric reasoning. One important exception is Cross-View Completion, CroCo, which is a form of masked autoencoding (MAE) tailored for 3D understanding. In this work, we continue on the path proposed by CroCo and focus on learning features tailored for 3D vision. In a nutshell, we extend MAE to arbitrarily many views of the same scene. By uniformly masking all views and employing a lightweight decoder with inter-frame attention, our approach is inherently simpler and more scalable than CroCo. We evaluate the resulting model, MuM, extensively on downstream tasks including feedforward reconstruction, dense image matching and relative pose estimation, finding that it outperforms the state-of-the-art visual encoders DINOv3 and CroCo v2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMuM\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u63a9\u7801\u81ea\u7f16\u7801\u6269\u5c55\u5230\u4efb\u610f\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u4e13\u95e8\u9488\u5bf93D\u89c6\u89c9\u4efb\u52a1\u5b66\u4e60\u7279\u5f81\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86DINOv3\u548cCroCo v2\u7b49\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u7f16\u7801\u5668\u3002", "motivation": "\u5f53\u524d\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u8bed\u4e49\u7406\u89e3\u800c\u975e\u51e0\u4f55\u63a8\u7406\u4f18\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u5982CroCo\u867d\u7136\u9488\u5bf93D\u7406\u89e3\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u4f46\u5728\u6269\u5c55\u6027\u548c\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e13\u95e8\u9488\u5bf93D\u89c6\u89c9\u4efb\u52a1\u7684\u7279\u5f81\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMuM\u6a21\u578b\uff0c\u5c06\u63a9\u7801\u81ea\u7f16\u7801\u6269\u5c55\u5230\u4efb\u610f\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u901a\u8fc7\u5728\u6240\u6709\u89c6\u89d2\u4e0a\u7edf\u4e00\u5e94\u7528\u63a9\u7801\u7b56\u7565\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u7ed3\u5408\u8de8\u5e27\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u6bd4CroCo\u66f4\u7b80\u5355\u4e14\u66f4\u5177\u6269\u5c55\u6027\u7684\u67b6\u6784\u8bbe\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0c\u5305\u62ec\u524d\u9988\u91cd\u5efa\u3001\u5bc6\u96c6\u56fe\u50cf\u5339\u914d\u548c\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\uff0cMuM\u6a21\u578b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u7f16\u7801\u5668DINOv3\u548cCroCo v2\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4e13\u95e8\u9488\u5bf93D\u89c6\u89c9\u4efb\u52a1\u8bbe\u8ba1\u7684\u591a\u89c6\u89d2\u63a9\u7801\u81ea\u7f16\u7801\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u51e0\u4f55\u611f\u77e5\u7684\u7279\u5f81\u8868\u793a\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u76843D\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u6269\u5c55\u6027\u548c\u6027\u80fd\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.16998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16998", "abs": "https://arxiv.org/abs/2511.16998", "authors": ["Qianyi Shao", "Yuanfan Zhang", "Renxiang Xiao", "Liang Hu"], "title": "VLM-Augmented Degradation Modeling for Image Restoration Under Adverse Weather Conditions", "comment": null, "summary": "Reliable visual perception under adverse weather conditions, such as rain, haze, snow, or a mixture of them, is desirable yet challenging for autonomous driving and outdoor robots. In this paper, we propose a unified Memory-Enhanced Visual-Language Recovery (MVLR) model that restores images from different degradation levels under various weather conditions. MVLR couples a lightweight encoder-decoder backbone with a Visual-Language Model (VLM) and an Implicit Memory Bank (IMB). The VLM performs chain-of-thought inference to encode weather degradation priors and the IMB stores continuous latent representations of degradation patterns. The VLM-generated priors query the IMB to retrieve fine-grained degradation prototypes. These prototypes are then adaptively fused with multi-scale visual features via dynamic cross-attention mechanisms, enhancing restoration accuracy while maintaining computational efficiency. Extensive experiments on four severe-weather benchmarks show that MVLR surpasses single-branch and Mixture-of-Experts baselines in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). These results indicate that MVLR offers a practical balance between model compactness and expressiveness for real-time deployment in diverse outdoor conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5185\u5b58\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6062\u590d\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u94fe\u5f0f\u63a8\u7406\u548c\u9690\u5f0f\u5185\u5b58\u5e93\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5404\u79cd\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u56fe\u50cf\u7684\u6709\u6548\u6062\u590d\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6062\u590d\u7cbe\u5ea6\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u6237\u5916\u673a\u5668\u4eba\u5e94\u7528\u4e2d\uff0c\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u89c6\u89c9\u611f\u77e5\u662f\u81f3\u5173\u91cd\u8981\u4f46\u6781\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u4e0d\u540c\u9000\u5316\u7a0b\u5ea6\u548c\u591a\u79cd\u5929\u6c14\u6761\u4ef6\u7684\u590d\u6742\u9000\u5316\u6a21\u5f0f\u3002", "method": "MVLR\u6a21\u578b\u91c7\u7528\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u4e3b\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u94fe\u5f0f\u63a8\u7406\u4ee5\u7f16\u7801\u5929\u6c14\u9000\u5316\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u9690\u5f0f\u5185\u5b58\u5e93\u5b58\u50a8\u8fde\u7eed\u6f5c\u5728\u9000\u5316\u6a21\u5f0f\u8868\u793a\uff0c\u5229\u7528\u52a8\u6001\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u81ea\u9002\u5e94\u878d\u5408\u591a\u5c3a\u5ea6\u89c6\u89c9\u7279\u5f81\u4e0e\u9000\u5316\u539f\u578b\u3002", "result": "\u5728\u56db\u4e2a\u6076\u52a3\u5929\u6c14\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMVLR\u5728\u5cf0\u503c\u4fe1\u566a\u6bd4\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5355\u5206\u652f\u548c\u4e13\u5bb6\u6df7\u5408\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u7d27\u51d1\u6027\u4e0e\u8868\u8fbe\u80fd\u529b\u7684\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4e0e\u5185\u5b58\u673a\u5236\u80fd\u591f\u6709\u6548\u63d0\u5347\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u6027\u80fd\uff0c\u4e3a\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5728\u591a\u6837\u5316\u6237\u5916\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u89c6\u89c9\u611f\u77e5\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.17442", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17442", "abs": "https://arxiv.org/abs/2511.17442", "authors": ["Binger Chen", "Tacettin Emre B\u00f6k", "Behnood Rasti", "Volker Markl", "Beg\u00fcm Demir"], "title": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing", "comment": "Code and data available at https://github.com/be-chen/REMSA", "summary": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RSFM\u6570\u636e\u5e93\uff08RS-FMD\uff09\u548cREMSA\u667a\u80fd\u4f53\uff0c\u524d\u8005\u662f\u4e00\u4e2a\u5305\u542b150\u591a\u4e2a\u9065\u611f\u57fa\u7840\u6a21\u578b\u7684\u7ed3\u6784\u5316\u8d44\u6e90\u5e93\uff0c\u540e\u8005\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u9065\u611f\u57fa\u7840\u6a21\u578b\u9009\u62e9\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u89e3\u51b3\u6a21\u578b\u9009\u62e9\u96be\u9898\u3002", "motivation": "\u9065\u611f\u9886\u57df\u57fa\u7840\u6a21\u578b\uff08RSFM\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\u9762\u4e34\u6a21\u578b\u9009\u62e9\u56f0\u96be\uff0c\u4e3b\u8981\u7531\u4e8e\u6587\u6863\u5206\u6563\u3001\u683c\u5f0f\u5f02\u6784\u548c\u90e8\u7f72\u7ea6\u675f\u591a\u6837\u7b49\u95ee\u9898\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u6a21\u578b\u9009\u62e9\u5de5\u5177\u6765\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u5feb\u901f\u5b9a\u4f4d\u9002\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u6a21\u578b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b150\u591a\u4e2aRSFM\u7684RS-FMD\u7ed3\u6784\u5316\u6570\u636e\u5e93\uff0c\u6db5\u76d6\u591a\u6a21\u6001\u6570\u636e\u3001\u5206\u8fa8\u7387\u548c\u5b66\u4e60\u8303\u5f0f\uff1b\u5f00\u53d1\u4e86REMSA\u667a\u80fd\u4f53\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u89e3\u6790\u7528\u6237\u9700\u6c42\u3001\u8865\u5168\u7f3a\u5931\u7ea6\u675f\u3001\u6392\u5e8f\u5019\u9009\u6a21\u578b\u5e76\u63d0\u4f9b\u900f\u660e\u89e3\u91ca\uff1b\u5efa\u7acb\u4e86\u5305\u542b75\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u67e5\u8be2\u573a\u666f\u548c900\u79cd\u914d\u7f6e\u7684\u57fa\u51c6\u8bc4\u4f30\u534f\u8bae\u3002", "result": "REMSA\u5728\u4e13\u5bb6\u4e2d\u5fc3\u8bc4\u4f30\u534f\u8bae\u4e0b\u663e\u8457\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u6734\u7d20\u667a\u80fd\u4f53\u3001\u7a20\u5bc6\u68c0\u7d22\u548c\u975e\u7ed3\u6784\u5316RAG-based LLM\uff0c\u4e14\u4ec5\u4f7f\u7528\u516c\u5f00\u5143\u6570\u636e\u8fd0\u884c\uff0c\u4e0d\u8bbf\u95ee\u79c1\u6709\u6216\u654f\u611f\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9065\u611f\u57fa\u7840\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6570\u636e\u5e93\u548cLLM\u667a\u80fd\u4f53\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9009\u62e9\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u9065\u611f\u9886\u57df\u7684\u6a21\u578b\u5e94\u7528\u6807\u51c6\u5316\u548c\u81ea\u52a8\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.17052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17052", "abs": "https://arxiv.org/abs/2511.17052", "authors": ["Jingyun Chen", "Linghan Cai", "Zhikang Wang", "Yi Huang", "Songhan Jiang", "Shenjin Huang", "Hongpeng Wang", "Yongbing Zhang"], "title": "PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning", "comment": "11 pages, 6 figures", "summary": "Analyzing whole-slide images (WSIs) requires an iterative, evidence-driven reasoning process that parallels how pathologists dynamically zoom, refocus, and self-correct while collecting the evidence. However, existing computational pipelines often lack this explicit reasoning trajectory, resulting in inherently opaque and unjustifiable predictions. To bridge this gap, we present PathAgent, a training-free, large language model (LLM)-based agent framework that emulates the reflective, stepwise analytical approach of human experts. PathAgent can autonomously explore WSI, iteratively and precisely locating significant micro-regions using the Navigator module, extracting morphology visual cues using the Perceptor, and integrating these findings into the continuously evolving natural language trajectories in the Executor. The entire sequence of observations and decisions forms an explicit chain-of-thought, yielding fully interpretable predictions. Evaluated across five challenging datasets, PathAgent exhibits strong zero-shot generalization, surpassing task-specific baselines in both open-ended and constrained visual question-answering tasks. Moreover, a collaborative evaluation with human pathologists confirms PathAgent's promise as a transparent and clinically grounded diagnostic assistant.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPathAgent\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u514d\u8d39\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u75c5\u7406\u5b66\u5bb6\u7684\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\u5b9e\u73b0\u5168\u5207\u7247\u56fe\u50cf\u7684\u900f\u660e\u5206\u6790\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5bfc\u822a\u5668\u3001\u611f\u77e5\u5668\u548c\u6267\u884c\u5668\u6a21\u5757\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8f68\u8ff9\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5168\u5207\u7247\u56fe\u50cf\u8ba1\u7b97\u5206\u6790\u6d41\u7a0b\u7f3a\u4e4f\u660e\u786e\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u4e0d\u900f\u660e\u4e14\u96be\u4ee5\u5408\u7406\u89e3\u91ca\uff0c\u65e0\u6cd5\u6a21\u62df\u75c5\u7406\u5b66\u5bb6\u52a8\u6001\u7f29\u653e\u3001\u91cd\u65b0\u805a\u7126\u548c\u81ea\u6211\u4fee\u6b63\u7684\u8fed\u4ee3\u8bc1\u636e\u9a71\u52a8\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "PathAgent\u91c7\u7528\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u514d\u8d39\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u5bfc\u822a\u5668\u8d1f\u8d23\u8fed\u4ee3\u7cbe\u786e\u5b9a\u4f4d\u663e\u8457\u5fae\u533a\u57df\uff0c\u611f\u77e5\u5668\u63d0\u53d6\u5f62\u6001\u5b66\u89c6\u89c9\u7ebf\u7d22\uff0c\u6267\u884c\u5668\u5c06\u8fd9\u4e9b\u53d1\u73b0\u6574\u5408\u5230\u6301\u7eed\u6f14\u5316\u7684\u81ea\u7136\u8bed\u8a00\u8f68\u8ff9\u4e2d\uff0c\u5f62\u6210\u663e\u5f0f\u7684\u601d\u7ef4\u94fe\u3002", "result": "\u5728\u4e94\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cPathAgent\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u5f00\u653e\u6027\u548c\u53d7\u9650\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u5747\u8d85\u8d8a\u4efb\u52a1\u7279\u5b9a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e0e\u4eba\u7c7b\u75c5\u7406\u5b66\u5bb6\u7684\u534f\u4f5c\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u5176\u4f5c\u4e3a\u900f\u660e\u4e14\u4e34\u5e8a\u57fa\u7840\u8bca\u65ad\u52a9\u624b\u7684\u6f5c\u529b\u3002", "conclusion": "PathAgent\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u7684\u53cd\u601d\u6027\u9010\u6b65\u5206\u6790\u65b9\u6cd5\uff0c\u4e3a\u8ba1\u7b97\u75c5\u7406\u5b66\u63d0\u4f9b\u4e86\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u6846\u67b6\uff0c\u5176\u8bad\u7ec3\u514d\u8d39\u7279\u6027\u7ed3\u5408\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u4f7f\u5176\u6210\u4e3a\u4e34\u5e8a\u8bca\u65ad\u4e2d\u900f\u660e\u51b3\u7b56\u652f\u6301\u7684\u6709\u524d\u666f\u5de5\u5177\u3002"}}
{"id": "2511.17094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17094", "abs": "https://arxiv.org/abs/2511.17094", "authors": ["He Huang", "Zixuan Hu", "Dongxiao Li", "Yao Xiao", "Ling-Yu Duan"], "title": "Sparse Reasoning is Enough: Biological-Inspired Framework for Video Anomaly Detection with Large Pre-trained Models", "comment": null, "summary": "Video anomaly detection (VAD) plays a vital role in real-world applications such as security surveillance, autonomous driving, and industrial monitoring. Recent advances in large pre-trained models have opened new opportunities for training-free VAD by leveraging rich prior knowledge and general reasoning capabilities. However, existing studies typically rely on dense frame-level inference, incurring high computational costs and latency. This raises a fundamental question: Is dense reasoning truly necessary when using powerful pre-trained models in VAD systems? To answer this, we propose ReCoVAD, a novel framework inspired by the dual reflex and conscious pathways of the human nervous system, enabling selective frame processing to reduce redundant computation. ReCoVAD consists of two core pathways: (i) a Reflex pathway that uses a lightweight CLIP-based module to fuse visual features with prototype prompts and produce decision vectors, which query a dynamic memory of past frames and anomaly scores for fast response; and (ii) a Conscious pathway that employs a medium-scale vision-language model to generate textual event descriptions and refined anomaly scores for novel frames. It continuously updates the memory and prototype prompts, while an integrated large language model periodically reviews accumulated descriptions to identify unseen anomalies, correct errors, and refine prototypes. Extensive experiments show that ReCoVAD achieves state-of-the-art training-free performance while processing only 28.55\\% and 16.04\\% of the frames used by previous methods on the UCF-Crime and XD-Violence datasets, demonstrating that sparse reasoning is sufficient for effective large-model-based VAD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReCoVAD\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u795e\u7ecf\u7cfb\u7edf\u7684\u53cc\u901a\u8def\u673a\u5236\u5b9e\u73b0\u9009\u62e9\u6027\u5e27\u5904\u7406\uff0c\u5728\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u4ec5\u9700\u5904\u7406\u5c11\u91cf\u5e27\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u8bad\u7ec3\u65e0\u5173\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5bc6\u96c6\u5e27\u7ea7\u63a8\u7406\uff0c\u5bfc\u81f4\u9ad8\u6602\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u5f3a\u5927\u9884\u8bad\u7ec3\u6a21\u578b\u4e0b\u7a00\u758f\u63a8\u7406\u662f\u5426\u8db3\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "method": "ReCoVAD\u91c7\u7528\u53cc\u901a\u8def\u67b6\u6784\uff1a\u53cd\u5c04\u901a\u8def\u4f7f\u7528\u8f7b\u91cf\u7ea7CLIP\u6a21\u5757\u878d\u5408\u89c6\u89c9\u7279\u5f81\u4e0e\u539f\u578b\u63d0\u793a\uff0c\u67e5\u8be2\u52a8\u6001\u8bb0\u5fc6\u5e93\u5b9e\u73b0\u5feb\u901f\u54cd\u5e94\uff1b\u610f\u8bc6\u901a\u8def\u91c7\u7528\u4e2d\u7b49\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u4e8b\u4ef6\u63cf\u8ff0\u548c\u7cbe\u70bc\u5f02\u5e38\u5206\u6570\uff0c\u901a\u8fc7\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u5b9a\u671f\u5ba1\u67e5\u63cf\u8ff0\u4ee5\u8bc6\u522b\u672a\u89c1\u5f02\u5e38\u5e76\u4f18\u5316\u539f\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eReCoVAD\u5728UCF-Crime\u548cXD-Violence\u6570\u636e\u96c6\u4e0a\u5206\u522b\u4ec5\u9700\u5904\u740628.55%\u548c16.04%\u7684\u5e27\u6570\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u8bad\u7ec3\u65e0\u5173\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5728\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u652f\u6301\u4e0b\uff0c\u7a00\u758f\u63a8\u7406\u8db3\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u53cc\u901a\u8def\u673a\u5236\u4e3a\u5176\u4ed6\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u501f\u9274\u7684\u67b6\u6784\u8bbe\u8ba1\u601d\u8def\u3002"}}
{"id": "2511.17103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17103", "abs": "https://arxiv.org/abs/2511.17103", "authors": ["Daiqing Wu", "Dongbao Yang", "Yu Zhou", "Can Ma"], "title": "Bridging Visual Affective Gap: Borrowing Textual Knowledge by Learning from Noisy Image-Text Pairs", "comment": "Accepted by ACM MM 2024", "summary": "Visual emotion recognition (VER) is a longstanding field that has garnered increasing attention with the advancement of deep neural networks. Although recent studies have achieved notable improvements by leveraging the knowledge embedded within pre-trained visual models, the lack of direct association between factual-level features and emotional categories, called the \"affective gap\", limits the applicability of pre-training knowledge for VER tasks. On the contrary, the explicit emotional expression and high information density in textual modality eliminate the \"affective gap\". Therefore, we propose borrowing the knowledge from the pre-trained textual model to enhance the emotional perception of pre-trained visual models. We focus on the factual and emotional connections between images and texts in noisy social media data, and propose Partitioned Adaptive Contrastive Learning (PACL) to leverage these connections. Specifically, we manage to separate different types of samples and devise distinct contrastive learning strategies for each type. By dynamically constructing negative and positive pairs, we fully exploit the potential of noisy samples. Through comprehensive experiments, we demonstrate that bridging the \"affective gap\" significantly improves the performance of various pre-trained visual models in downstream emotion-related tasks. Our code is released on https://github.com/wdqqdw/PACL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5206\u533a\u81ea\u9002\u5e94\u5bf9\u6bd4\u5b66\u4e60\uff08PACL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u501f\u7528\u9884\u8bad\u7ec3\u6587\u672c\u6a21\u578b\u7684\u60c5\u611f\u77e5\u8bc6\u6765\u589e\u5f3a\u89c6\u89c9\u6a21\u578b\u7684\u60c5\u611f\u611f\u77e5\u80fd\u529b\uff0c\u4ece\u800c\u5f25\u5408\u89c6\u89c9\u60c5\u611f\u8bc6\u522b\u4e2d\u7684'\u60c5\u611f\u9e3f\u6c9f'\u95ee\u9898\u3002", "motivation": "\u89c6\u89c9\u60c5\u611f\u8bc6\u522b\u9886\u57df\u5b58\u5728'\u60c5\u611f\u9e3f\u6c9f'\u95ee\u9898\uff0c\u5373\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u4e8b\u5b9e\u7ea7\u7279\u5f81\u4e0e\u60c5\u611f\u7c7b\u522b\u4e4b\u95f4\u7f3a\u4e4f\u76f4\u63a5\u5173\u8054\uff0c\u9650\u5236\u4e86\u9884\u8bad\u7ec3\u77e5\u8bc6\u5728\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6587\u672c\u6a21\u6001\u5177\u6709\u660e\u786e\u7684\u60c5\u611f\u8868\u8fbe\u548c\u9ad8\u4fe1\u606f\u5bc6\u5ea6\uff0c\u80fd\u591f\u6d88\u9664\u8fd9\u79cd\u9e3f\u6c9f\u3002", "method": "\u63d0\u51fa\u5206\u533a\u81ea\u9002\u5e94\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff08PACL\uff09\uff0c\u5173\u6ce8\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e2d\u56fe\u50cf\u4e0e\u6587\u672c\u4e4b\u95f4\u7684\u4e8b\u5b9e\u548c\u60c5\u611f\u8054\u7cfb\uff0c\u901a\u8fc7\u5206\u79bb\u4e0d\u540c\u7c7b\u578b\u7684\u6837\u672c\u5e76\u4e3a\u6bcf\u79cd\u7c7b\u578b\u8bbe\u8ba1\u4e0d\u540c\u7684\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u52a8\u6001\u6784\u5efa\u6b63\u8d1f\u6837\u672c\u5bf9\u4ee5\u5145\u5206\u5229\u7528\u566a\u58f0\u6837\u672c\u7684\u6f5c\u529b\u3002", "result": "\u901a\u8fc7\u5168\u9762\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5f25\u5408'\u60c5\u611f\u9e3f\u6c9f'\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u5728\u4e0b\u6e38\u60c5\u611f\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u501f\u7528\u6587\u672c\u6a21\u6001\u7684\u60c5\u611f\u77e5\u8bc6\u80fd\u591f\u6709\u6548\u589e\u5f3a\u89c6\u89c9\u6a21\u578b\u7684\u60c5\u611f\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u8de8\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u540c\u65f6\u63d0\u51fa\u7684PACL\u65b9\u6cd5\u4e3a\u5904\u7406\u566a\u58f0\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u6846\u67b6\u3002"}}
{"id": "2511.17106", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17106", "abs": "https://arxiv.org/abs/2511.17106", "authors": ["Yuan Zhang", "Ming Lu", "Junwen Pan", "Tao Huang", "Kuan Cheng", "Qi She", "Shanghang Zhang"], "title": "ChainV: Atomic Visual Hints Make Multimodal Reasoning Shorter and Better", "comment": "16 pages", "summary": "Recent advances in multimodal reasoning models have demonstrated impressive capabilities across text and vision. However, even leading models exhibit redundant self-reflection when generating lengthy reasoning chains. While training-free CoT compression methods have emerged in the LLMs domain, they rely on static visual references and thus provide limited gains for multimodal reasoning. Therefore, we propose ChainV, a framework that dynamically integrates visual hints into the reasoning process, thereby making multimodal reasoning shorter and better. Specifically, ChainV first performs a coarse visual patch selection based on the previous reasoning step, then refines it by identifying the most representative atomic visual hint according to the averaged attention intensity. Additionally, ChainV introduces a consistency-based evaluation mechanism to assess the reliability of the chosen hint, guiding the model to adaptively adjust its level of self-reflection. Eventually, the pixel coordinates of the selected visual hint and its reliability are incorporated into thinking with a Bernoulli stochastic process. Experiments indicate that our method significantly improves reasoning accuracy and efficiency, especially on math-intensive benchmarks where visual hints are crucial for multi-step symbolic reasoning. For example, ChainV achieves $2.3\\%$ improvement on the MathVista within MIMO-VL-RL, while reducing inference latency by $51.4\\%$ and shortening output token length by $24.5\\%$.", "AI": {"tldr": "ChainV\u662f\u4e00\u4e2a\u52a8\u6001\u6574\u5408\u89c6\u89c9\u63d0\u793a\u7684\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8865\u4e01\u9009\u62e9\u548c\u6ce8\u610f\u529b\u5f3a\u5ea6\u5206\u6790\u4f7f\u63a8\u7406\u8fc7\u7a0b\u66f4\u77ed\u66f4\u51c6\u786e\uff0c\u5728\u6570\u5b66\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u63a8\u7406\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u5728\u751f\u6210\u957f\u63a8\u7406\u94fe\u65f6\u5b58\u5728\u5197\u4f59\u81ea\u53cd\u601d\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u9759\u6001\u89c6\u89c9\u53c2\u8003\u7684\u65e0\u8bad\u7ec3CoT\u538b\u7f29\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u589e\u76ca\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u52a8\u6001\u6574\u5408\u89c6\u89c9\u63d0\u793a\u7684\u63a8\u7406\u6846\u67b6\u3002", "method": "ChainV\u9996\u5148\u57fa\u4e8e\u524d\u4e00\u6b65\u63a8\u7406\u8fdb\u884c\u7c97\u7565\u89c6\u89c9\u8865\u4e01\u9009\u62e9\uff0c\u7136\u540e\u901a\u8fc7\u5e73\u5747\u6ce8\u610f\u529b\u5f3a\u5ea6\u8bc6\u522b\u6700\u5177\u4ee3\u8868\u6027\u7684\u539f\u5b50\u89c6\u89c9\u63d0\u793a\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u8bc4\u4f30\u673a\u5236\u6765\u8bc4\u4f30\u6240\u9009\u63d0\u793a\u7684\u53ef\u9760\u6027\uff0c\u6700\u7ec8\u901a\u8fc7\u4f2f\u52aa\u5229\u968f\u673a\u8fc7\u7a0b\u5c06\u9009\u5b9a\u89c6\u89c9\u63d0\u793a\u7684\u50cf\u7d20\u5750\u6807\u53ca\u5176\u53ef\u9760\u6027\u6574\u5408\u5230\u601d\u8003\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728MathVista\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cChainV\u5728MIMO-VL-RL\u4e0a\u5b9e\u73b0\u4e862.3%\u7684\u7cbe\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e51.4%\uff0c\u8f93\u51fatoken\u957f\u5ea6\u7f29\u77ed24.5%\uff0c\u7279\u522b\u5728\u9700\u8981\u591a\u6b65\u7b26\u53f7\u63a8\u7406\u7684\u6570\u5b66\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u52a8\u6001\u89c6\u89c9\u63d0\u793a\u6574\u5408\u80fd\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u51cf\u5c11\u6a21\u578b\u5197\u4f59\u81ea\u53cd\u601d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u590d\u6742\u7684\u591a\u6a21\u6001\u63a8\u7406\u573a\u666f\u3002"}}
{"id": "2511.17135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17135", "abs": "https://arxiv.org/abs/2511.17135", "authors": ["Jiaxun Fang", "Li Chen"], "title": "A Multi-Stage Optimization Framework for Deploying Learned Image Compression on FPGAs", "comment": null, "summary": "Deep learning-based image compression (LIC) has achieved state-of-the-art rate-distortion (RD) performance, yet deploying these models on resource-constrained FPGAs remains a major challenge. This work presents a complete, multi-stage optimization framework to bridge the gap between high-performance floating-point models and efficient, hardware-friendly integer-based implementations. First, we address the fundamental problem of quantization-induced performance degradation. We propose a Dynamic Range-Aware Quantization (DRAQ) method that uses statistically-calibrated activation clipping and a novel weight regularization scheme to counteract the effects of extreme data outliers and large dynamic ranges, successfully creating a high-fidelity 8-bit integer model. Second, building on this robust foundation, we introduce two hardware-aware optimization techniques tailored for FPGAs. A progressive mixed-precision search algorithm exploits FPGA flexibility to assign optimal, non-uniform bit-widths to each layer, minimizing complexity while preserving performance. Concurrently, a channel pruning method, adapted to work with the Generalized Divisive Normalization (GDN) layers common in LIC, removes model redundancy by eliminating inactive channels. Our comprehensive experiments show that the foundational DRAQ method reduces the BD-rate overhead of a GDN-based model from $30\\%$ to $6.3\\%$. The subsequent hardware-aware optimizations further reduce computational complexity by over $20\\%$ with negligible impact on RD performance, yielding a final model that is both state-of-the-art in efficiency and superior in quality to existing FPGA-based LIC implementations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u591a\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u9ad8\u6027\u80fd\u6d6e\u70b9\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u8f6c\u5316\u4e3a\u9ad8\u6548\u7684\u786c\u4ef6\u53cb\u597d\u578b\u6574\u6570\u5b9e\u73b0\uff0c\u901a\u8fc7\u52a8\u6001\u8303\u56f4\u611f\u77e5\u91cf\u5316\u3001\u6e10\u8fdb\u6df7\u5408\u7cbe\u5ea6\u641c\u7d22\u548c\u901a\u9053\u526a\u679d\u6280\u672f\uff0c\u5728FPGA\u4e0a\u5b9e\u73b0\u4e86\u65e2\u9ad8\u6548\u53c8\u4fdd\u6301\u4f18\u5f02\u7387\u5931\u771f\u6027\u80fd\u7684\u56fe\u50cf\u538b\u7f29\u7cfb\u7edf\u3002", "motivation": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u867d\u7136\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7387\u5931\u771f\u6027\u80fd\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u7684FPGA\u4e0a\u7684\u90e8\u7f72\u4ecd\u7136\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u91cf\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u4ee5\u53ca\u786c\u4ef6\u8d44\u6e90\u9650\u5236\u4e0e\u6a21\u578b\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u5e73\u8861\u96be\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u52a8\u6001\u8303\u56f4\u611f\u77e5\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u6821\u51c6\u7684\u6fc0\u6d3b\u88c1\u526a\u548c\u65b0\u578b\u6743\u91cd\u6b63\u5219\u5316\u65b9\u6848\u6765\u5e94\u5bf9\u6781\u7aef\u6570\u636e\u5f02\u5e38\u503c\u548c\u5927\u7684\u52a8\u6001\u8303\u56f4\uff1b\u5f00\u53d1\u4e86\u6e10\u8fdb\u6df7\u5408\u7cbe\u5ea6\u641c\u7d22\u7b97\u6cd5\uff0c\u4e3a\u6bcf\u5c42\u5206\u914d\u6700\u4f18\u7684\u975e\u5747\u5300\u4f4d\u5bbd\uff1b\u8bbe\u8ba1\u4e86\u9002\u7528\u4e8eGDN\u5c42\u7684\u901a\u9053\u526a\u679d\u65b9\u6cd5\uff0c\u6d88\u9664\u6a21\u578b\u5197\u4f59\u3002", "result": "\u57fa\u7840DRAQ\u65b9\u6cd5\u5c06\u57fa\u4e8eGDN\u6a21\u578b\u7684BD-rate\u5f00\u9500\u4ece30%\u964d\u4f4e\u52306.3%\uff0c\u540e\u7eed\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u8fdb\u4e00\u6b65\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e\u8d85\u8fc720%\uff0c\u540c\u65f6\u5bf9\u7387\u5931\u771f\u6027\u80fd\u5f71\u54cd\u53ef\u5ffd\u7565\uff0c\u6700\u7ec8\u6a21\u578b\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709FPGA\u56fe\u50cf\u538b\u7f29\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u91cf\u5316\u3001\u7cbe\u5ea6\u4f18\u5316\u548c\u526a\u679d\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u7684\u786c\u4ef6\u5b9e\u73b0\u590d\u6742\u5ea6\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548AI\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2511.17171", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17171", "abs": "https://arxiv.org/abs/2511.17171", "authors": ["Mario Markov", "Stefan Maria Ailuro", "Luc Van Gool", "Konrad Schindler", "Danda Pani Paudel"], "title": "FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle", "comment": null, "summary": "Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FireScope-Bench\u6570\u636e\u96c6\u548cFireScope\u6846\u67b6\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u8bed\u8a00\u63a8\u7406\u7684\u89c6\u89c9\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7406\u89e3\u548c\u56e0\u679c\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u8de8\u5927\u9646\u91ce\u706b\u98ce\u9669\u9884\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u91ce\u706b\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u56e0\u679c\u63a8\u7406\u548c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u80fd\u4e0d\u8db3\uff0c\u65e0\u6cd5\u53ef\u9760\u5730\u5e94\u7528\u4e8e\u8de8\u5927\u9646\u573a\u666f\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5230\u751f\u6210\u6846\u67b6FireScope\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u89c6\u89c9\u76d1\u7763\u5b66\u4e60\u9884\u6d4b\u98ce\u9669\u6805\u683c\u56fe\u5e76\u751f\u6210\u4e92\u8865\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u4f7f\u7528Sentinel-2\u5f71\u50cf\u548c\u6c14\u5019\u6570\u636e\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6FireScope-Bench\u3002", "result": "\u5728\u7f8e\u56fd\u8bad\u7ec3\u5e76\u5728\u6b27\u6d32\u6d4b\u8bd5\u65f6\uff0cFireScope\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e13\u5bb6\u53cd\u9988\u548c\u81ea\u52a8\u5316\u5206\u6790\u8bc1\u5b9e\u5176\u63a8\u7406\u8f68\u8ff9\u5177\u6709\u5fe0\u5b9e\u6027\u548c\u8bed\u4e49\u610f\u4e49\uff0c\u9a8c\u8bc1\u4e86\u8de8\u5927\u9646\u6cdb\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u57fa\u4e8e\u8bed\u8a00\u7684\u63a8\u7406\u80fd\u591f\u6709\u6548\u652f\u6491\u6805\u683c\u9884\u6d4b\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u63a8\u7406\u9a71\u52a8\u7684\u53ef\u89e3\u91ca\u7a7a\u95f4\u5efa\u6a21\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5f00\u521b\u4e86\u8bed\u8a00\u63a8\u7406\u6539\u8fdb\u89c6\u89c9\u751f\u6210\u6cdb\u5316\u80fd\u529b\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.17181", "categories": ["cs.CV", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.17181", "abs": "https://arxiv.org/abs/2511.17181", "authors": ["Dragos-Alexandru Boldisor", "Stefan Smeu", "Dan Oneata", "Elisabeta Oneata"], "title": "Investigating self-supervised representations for audio-visual deepfake detection", "comment": null, "summary": "Self-supervised representations excel at many vision and speech tasks, but their potential for audio-visual deepfake detection remains underexplored. Unlike prior work that uses these features in isolation or buried within complex architectures, we systematically evaluate them across modalities (audio, video, multimodal) and domains (lip movements, generic visual content). We assess three key dimensions: detection effectiveness, interpretability of encoded information, and cross-modal complementarity. We find that most self-supervised features capture deepfake-relevant information, and that this information is complementary. Moreover, models primarily attend to semantically meaningful regions rather than spurious artifacts. Yet none generalize reliably across datasets. This generalization failure likely stems from dataset characteristics, not from the features themselves latching onto superficial patterns. These results expose both the promise and fundamental challenges of self-supervised representations for deepfake detection: while they learn meaningful patterns, achieving robust cross-domain performance remains elusive.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u81ea\u76d1\u7763\u8868\u793a\u5728\u97f3\u9891-\u89c6\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u53d1\u73b0\u8fd9\u4e9b\u7279\u5f81\u80fd\u6355\u83b7\u6709\u610f\u4e49\u7684\u4f2a\u9020\u76f8\u5173\u4fe1\u606f\u4e14\u5177\u6709\u4e92\u8865\u6027\uff0c\u4f46\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002", "motivation": "\u81ea\u76d1\u7763\u8868\u793a\u5728\u89c6\u89c9\u548c\u8bed\u97f3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u97f3\u9891-\u89c6\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u73b0\u6709\u7814\u7a76\u8981\u4e48\u5b64\u7acb\u4f7f\u7528\u8fd9\u4e9b\u7279\u5f81\uff0c\u8981\u4e48\u5c06\u5176\u5d4c\u5165\u590d\u6742\u67b6\u6784\u4e2d\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u81ea\u76d1\u7763\u7279\u5f81\u5728\u97f3\u9891\u3001\u89c6\u9891\u548c\u591a\u6a21\u6001\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u5507\u90e8\u8fd0\u52a8\u548c\u901a\u7528\u89c6\u89c9\u5185\u5bb9\u4e24\u4e2a\u9886\u57df\uff0c\u8bc4\u4f30\u4e86\u68c0\u6d4b\u6548\u679c\u3001\u4fe1\u606f\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u6a21\u6001\u4e92\u8865\u6027\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u5927\u591a\u6570\u81ea\u76d1\u7763\u7279\u5f81\u90fd\u80fd\u6355\u83b7\u4e0e\u6df1\u5ea6\u4f2a\u9020\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u4e14\u8fd9\u4e9b\u4fe1\u606f\u5177\u6709\u4e92\u8865\u6027\uff0c\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u533a\u57df\u800c\u975e\u865a\u5047\u4f2a\u5f71\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4e0d\u53ef\u9760\uff0c\u8fd9\u79cd\u6cdb\u5316\u5931\u8d25\u6e90\u4e8e\u6570\u636e\u96c6\u7279\u6027\u800c\u975e\u7279\u5f81\u672c\u8eab\u5bf9\u8868\u9762\u6a21\u5f0f\u7684\u4f9d\u8d56\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u81ea\u76d1\u7763\u8868\u793a\u5728\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u53cc\u91cd\u6027\uff1a\u867d\u7136\u5b83\u4eec\u80fd\u591f\u5b66\u4e60\u6709\u610f\u4e49\u7684\u6a21\u5f0f\uff0c\u4f46\u5b9e\u73b0\u7a33\u5065\u7684\u8de8\u57df\u6027\u80fd\u4ecd\u7136\u9762\u4e34\u6839\u672c\u6027\u6311\u6218\uff0c\u8fd9\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.17183", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17183", "abs": "https://arxiv.org/abs/2511.17183", "authors": ["Aditya Mishra", "Akshay Agarwal", "Haroon Lone"], "title": "Navigating in the Dark: A Multimodal Framework and Dataset for Nighttime Traffic Sign Recognition", "comment": null, "summary": "Traffic signboards are vital for road safety and intelligent transportation systems, enabling navigation and autonomous driving. Yet, recognizing traffic signs at night remains challenging due to visual noise and scarcity of public nighttime datasets. Despite advances in vision architectures, existing methods struggle with robustness under low illumination and fail to leverage complementary mutlimodal cues effectively. To overcome these limitations, firstly, we introduce INTSD, a large-scale dataset comprising street-level night-time images of traffic signboards collected across diverse regions of India. The dataset spans 41 traffic signboard classes captured under varying lighting and weather conditions, providing a comprehensive benchmark for both detection and classification tasks. To benchmark INTSD for night-time sign recognition, we conduct extensive evaluations using state-of-the-art detection and classification models. Secondly, we propose LENS-Net, which integrates an adaptive image enhancement detector for joint illumination correction and sign localization, followed by a structured multimodal CLIP-GCNN classifier that leverages cross-modal attention and graph-based reasoning for robust and semantically consistent recognition. Our method surpasses existing frameworks, with ablation studies confirming the effectiveness of its key components. The dataset and code for LENS-Net is publicly available for research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86INTSD\u591c\u95f4\u4ea4\u901a\u6807\u5fd7\u6570\u636e\u96c6\u548cLENS-Net\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u56fe\u50cf\u589e\u5f3a\u68c0\u6d4b\u5668\u548c\u591a\u6a21\u6001CLIP-GCNN\u5206\u7c7b\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591c\u95f4\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u4e2d\u5149\u7167\u4e0d\u8db3\u548c\u89c6\u89c9\u566a\u58f0\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u591c\u95f4\u89c6\u89c9\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u591c\u95f4\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u9762\u4e34\u89c6\u89c9\u566a\u58f0\u548c\u516c\u5f00\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4e14\u672a\u80fd\u6709\u6548\u5229\u7528\u4e92\u8865\u7684\u591a\u6a21\u6001\u7ebf\u7d22\u3002\u4e3a\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u591c\u95f4\u6761\u4ef6\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u80fd\u591f\u540c\u65f6\u5904\u7406\u5149\u7167\u6821\u6b63\u4e0e\u8bed\u4e49\u7406\u89e3\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002", "method": "\u9996\u5148\u6784\u5efa\u4e86INTSD\u6570\u636e\u96c6\uff0c\u5305\u542b41\u7c7b\u5370\u5ea6\u5404\u5730\u591c\u95f4\u4ea4\u901a\u6807\u5fd7\u56fe\u50cf\uff1b\u7136\u540e\u63d0\u51faLENS-Net\u6846\u67b6\uff0c\u96c6\u6210\u81ea\u9002\u5e94\u56fe\u50cf\u589e\u5f3a\u68c0\u6d4b\u5668\u8fdb\u884c\u8054\u5408\u5149\u7167\u6821\u6b63\u548c\u6807\u5fd7\u5b9a\u4f4d\uff0c\u540e\u7eed\u91c7\u7528\u7ed3\u6784\u5316\u591a\u6a21\u6001CLIP-GCNN\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u548c\u56fe\u63a8\u7406\u5b9e\u73b0\u9c81\u68d2\u8bc6\u522b\u3002", "result": "LENS-Net\u5728\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u6846\u67b6\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u5176\u5173\u952e\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002\u8be5\u6846\u67b6\u5728INTSD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5728\u591a\u6837\u5316\u5149\u7167\u548c\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591c\u95f4\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u5370\u5ea6\u6570\u636e\u96c6\u548c\u6709\u6548\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u878d\u5408\u548c\u81ea\u9002\u5e94\u56fe\u50cf\u589e\u5f3a\u5728\u4f4e\u5149\u7167\u573a\u666f\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5168\u5929\u5019\u53ef\u9760\u8fd0\u884c\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.17199", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17199", "abs": "https://arxiv.org/abs/2511.17199", "authors": ["Hanyu Zhou", "Chuanhao Ma", "Gim Hee Lee"], "title": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation", "comment": null, "summary": "Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VLA-4D\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u5177\u67094D\u611f\u77e5\u80fd\u529b\u7684\u901a\u7528\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u51654D\u611f\u77e5\u7684\u89c6\u89c9\u8868\u793a\u548c\u65f6\u7a7a\u52a8\u4f5c\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u65f6\u7a7a\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u65f6\u7a7a\u4e00\u81f4\u6027\u64cd\u4f5c\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u867d\u7136\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5c063D\u4f4d\u7f6e\u5d4c\u5165\u89c6\u89c9\u8868\u793a\u6765\u589e\u5f3a\u7a7a\u95f4\u7cbe\u5ea6\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u52a8\u4f5c\u6267\u884c\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a4D\u611f\u77e5\u7684\u89c6\u89c9\u8868\u793a\uff0c\u901a\u8fc7\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\u3001\u5c061D\u65f6\u95f4\u5d4c\u51653D\u4f4d\u7f6e\u5f62\u62104D\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u4e3a\u7edf\u4e00\u89c6\u89c9\u8868\u793a\uff1b\u65f6\u7a7a\u52a8\u4f5c\u8868\u793a\uff0c\u5728\u4f20\u7edf\u7a7a\u95f4\u52a8\u4f5c\u8868\u793a\u57fa\u7840\u4e0a\u6269\u5c55\u65f6\u95f4\u4fe1\u606f\u4ee5\u5b9e\u73b0\u65f6\u7a7a\u89c4\u5212\uff0c\u5e76\u5c06\u591a\u6a21\u6001\u8868\u793a\u5bf9\u9f50\u5230LLM\u4e2d\u8fdb\u884c\u65f6\u7a7a\u52a8\u4f5c\u9884\u6d4b\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\uff0c\u6240\u8bbe\u8ba1\u7684\u89c6\u89c9\u548c\u52a8\u4f5c\u8868\u793a\u5171\u540c\u4f7f\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u7a7a\u95f4\u4e0a\u5e73\u6ed1\u4e14\u5728\u65f6\u95f4\u4e0a\u4e00\u81f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7edf\u4e00\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u6269\u5c55\u7684VLA\u6570\u636e\u96c6\u4e3a\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u4e3a\u7cbe\u7ec6\u5316\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17201", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17201", "abs": "https://arxiv.org/abs/2511.17201", "authors": ["Jiayi Wang", "Wei Dai", "Haoyu Wang", "Sihan Yang", "Haixia Bi", "Jian Sun"], "title": "Continual Alignment for SAM: Rethinking Foundation Models for Medical Image Segmentation in Continual Learning", "comment": null, "summary": "In medical image segmentation, heterogeneous privacy policies across institutions often make joint training on pooled datasets infeasible, motivating continual image segmentation-learning from data streams without catastrophic forgetting. While the Segment Anything Model (SAM) offers strong zero-shot priors and has been widely fine-tuned across downstream tasks, its large parameter count and computational overhead challenge practical deployment. This paper demonstrates that the SAM paradigm is highly promising once its computational efficiency and performance can be balanced. To this end, we introduce the Alignment Layer, a lightweight, plug-and-play module which aligns encoder-decoder feature distributions to efficiently adapt SAM to specific medical images, improving accuracy while reducing computation. Building on SAM and the Alignment Layer, we then propose Continual Alignment for SAM (CA-SAM), a continual learning strategy that automatically adapts the appropriate Alignment Layer to mitigate catastrophic forgetting, while leveraging SAM's zero-shot priors to preserve strong performance on unseen medical datasets. Experimented across nine medical segmentation datasets under continual-learning scenario, CA-SAM achieves state-of-the-art performance. Our code, models and datasets will be released on \\mbox{https://github.com/azzzzyo/Continual-Alignment-for-SAM.}", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCA-SAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5bf9\u9f50\u5c42\u548c\u6301\u7eed\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u4fdd\u6301SAM\u96f6\u6837\u672c\u5148\u9a8c\u7684\u540c\u65f6\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u4e5d\u4e2a\u533b\u5b66\u5206\u5272\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\uff0c\u673a\u6784\u95f4\u7684\u9690\u79c1\u653f\u7b56\u5f02\u8d28\u6027\u4f7f\u5f97\u8054\u5408\u8bad\u7ec3\u4e0d\u53ef\u884c\uff0c\u800cSegment Anything Model\u867d\u7136\u63d0\u4f9b\u5f3a\u5927\u96f6\u6837\u672c\u5148\u9a8c\uff0c\u4f46\u5176\u5927\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u5f00\u9500\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\uff0c\u9700\u8981\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u5bf9\u9f50\u5c42\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u901a\u8fc7\u5bf9\u9f50\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7279\u5f81\u5206\u5e03\u6765\u9ad8\u6548\u9002\u914dSAM\u5230\u7279\u5b9a\u533b\u5b66\u56fe\u50cf\uff1b\u57fa\u4e8e\u6b64\u6784\u5efaCA-SAM\u6301\u7eed\u5b66\u4e60\u7b56\u7565\uff0c\u81ea\u52a8\u9002\u914d\u5408\u9002\u7684\u5bf9\u9f50\u5c42\u4ee5\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u540c\u65f6\u5229\u7528SAM\u7684\u96f6\u6837\u672c\u5148\u9a8c\u4fdd\u6301\u5bf9\u672a\u89c1\u6570\u636e\u96c6\u7684\u5f3a\u6027\u80fd\u3002", "result": "\u5728\u6301\u7eed\u5b66\u4e60\u573a\u666f\u4e0b\u5bf9\u4e5d\u4e2a\u533b\u5b66\u5206\u5272\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCA-SAM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "SAM\u8303\u5f0f\u5728\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u540e\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u5bf9\u9f50\u5c42\u548cCA-SAM\u7b56\u7565\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u6570\u636e\u6d41\u53d8\u5316\u3002"}}
{"id": "2511.17209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17209", "abs": "https://arxiv.org/abs/2511.17209", "authors": ["Cris Claessens", "Christiaan Viviers", "Giacomo D'Amicantonio", "Egor Bondarev", "Fons van der Sommen"], "title": "Scaling Self-Supervised and Cross-Modal Pretraining for Volumetric CT Transformers", "comment": null, "summary": "We introduce SPECTRE, a fully transformer-based foundation model for volumetric computed tomography (CT). Our Self-Supervised & Cross-Modal Pretraining for CT Representation Extraction (SPECTRE) approach utilizes scalable 3D Vision Transformer architectures and modern self-supervised and vision-language pretraining strategies to learn general-purpose CT representations. Volumetric CT poses unique challenges, such as extreme token scaling, geometric anisotropy, and weak or noisy clinical supervision, that make standard transformer and contrastive learning recipes ineffective out of the box. The framework jointly optimizes a local transformer for high-resolution volumetric feature extraction and a global transformer for whole-scan context modeling, making large-scale 3D attention computationally tractable. Notably, SPECTRE is trained exclusively on openly available CT datasets, demonstrating that high-performing, generalizable representations can be achieved without relying on private data. Pretraining combines DINO-style self-distillation with SigLIP-based vision-language alignment using paired radiology reports, yielding features that are both geometrically consistent and clinically meaningful. Across multiple CT benchmarks, SPECTRE consistently outperforms prior CT foundation models in both zero-shot and fine-tuned settings, establishing SPECTRE as a scalable, open, and fully transformer-based foundation model for 3D medical imaging.", "AI": {"tldr": "SPECTRE\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u76843D CT\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u591a\u4e2aCT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u65e0\u9700\u79c1\u6709\u6570\u636e\u5373\u53ef\u83b7\u5f97\u9ad8\u6027\u80fd\u901a\u7528CT\u8868\u793a\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f53\u79efCT\u9762\u4e34\u7684\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u6781\u7aef\u4ee4\u724c\u7f29\u653e\u3001\u51e0\u4f55\u5404\u5411\u5f02\u6027\u4ee5\u53ca\u5f31\u6216\u5608\u6742\u7684\u4e34\u5e8a\u76d1\u7763\uff0c\u8fd9\u4e9b\u6311\u6218\u4f7f\u5f97\u6807\u51c6Transformer\u548c\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u6709\u6548\u5e94\u7528\u3002", "method": "SPECTRE\u91c7\u7528\u53ef\u6269\u5c55\u76843D\u89c6\u89c9Transformer\u67b6\u6784\uff0c\u8054\u5408\u4f18\u5316\u5c40\u90e8Transformer\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u4f53\u79ef\u7279\u5f81\u63d0\u53d6\u548c\u5168\u5c40Transformer\u8fdb\u884c\u5168\u626b\u63cf\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u7ed3\u5408DINO\u98ce\u683c\u7684\u81ea\u84b8\u998f\u548c\u57fa\u4e8eSigLIP\u7684\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u4f7f\u7528\u914d\u5bf9\u653e\u5c04\u5b66\u62a5\u544a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2aCT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPECTRE\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684CT\u57fa\u7840\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u57283D\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u9ad8\u6027\u80fd\u3001\u53ef\u6cdb\u5316\u7684CT\u8868\u793a\u53ef\u4ee5\u901a\u8fc7\u5b8c\u5168\u4f7f\u7528\u516c\u5f00\u53ef\u7528\u6570\u636e\u96c6\u5b9e\u73b0\uff0c\u65e0\u9700\u4f9d\u8d56\u79c1\u6709\u6570\u636e\uff0c\u4e3a3D\u533b\u5b66\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u5f00\u653e\u4e14\u5b8c\u5168\u57fa\u4e8eTransformer\u7684\u57fa\u7840\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17255", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.17255", "abs": "https://arxiv.org/abs/2511.17255", "authors": ["Bulat Khaertdinov", "Mirela Popa", "Nava Tintarev"], "title": "A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback", "comment": "Accepted to WACV'26", "summary": "Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u5173\u6027\u53cd\u9988\u7684\u673a\u5236\u6765\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u63a8\u7406\u65f6\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u56db\u79cd\u53cd\u9988\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u5c0f\u89c4\u6a21\u6a21\u578b\u4e2d\u6548\u679c\u66f4\u4e3a\u660e\u663e\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u652f\u6301\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u89c6\u89c9\u641c\u7d22\uff0c\u4f46\u63d0\u5347\u6027\u80fd\u901a\u5e38\u9700\u8981\u5fae\u8c03\u6216\u6269\u5c55\u6a21\u578b\u89c4\u6a21\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u63a2\u7d22\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u76f8\u5173\u6027\u53cd\u9988\u673a\u5236\u6765\u6539\u8fdb\u68c0\u7d22\u6027\u80fd\uff0c\u907f\u514d\u5bf9\u6a21\u578b\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\u6216\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u56db\u79cd\u76f8\u5173\u6027\u53cd\u9988\u7b56\u7565\uff1a\u7ecf\u5178\u4f2a\u76f8\u5173\u6027\u53cd\u9988\u901a\u8fc7\u57fa\u4e8e\u6392\u540d\u9760\u524d\u7ed3\u679c\u4f18\u5316\u67e5\u8be2\u5d4c\u5165\uff1b\u751f\u6210\u5f0f\u76f8\u5173\u6027\u53cd\u9988\u5229\u7528\u5408\u6210\u5b57\u5e55\u8fdb\u884c\u67e5\u8be2\u4f18\u5316\uff1b\u6ce8\u610f\u529b\u53cd\u9988\u6c47\u603b\u5668\u91c7\u7528\u5b9a\u5236\u5316transformer\u6a21\u578b\u6574\u5408\u591a\u6a21\u6001\u7ec6\u7c92\u5ea6\u7279\u5f81\uff1b\u4ee5\u53ca\u4f7f\u7528\u771f\u5b9e\u5b57\u5e55\u4f5c\u4e3a\u663e\u5f0f\u53cd\u9988\u7684\u4e0a\u754c\u57fa\u51c6\u3002", "result": "\u5728Flickr30k\u548cCOCO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u5f0f\u76f8\u5173\u6027\u53cd\u9988\u3001\u6ce8\u610f\u529b\u53cd\u9988\u6c47\u603b\u5668\u548c\u663e\u5f0f\u53cd\u9988\u76f8\u6bd4\u65e0\u53cd\u9988\u68c0\u7d22\uff0c\u5728\u5c0f\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2dMRR@5\u6307\u6807\u63d0\u53473-5%\uff0c\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u63d0\u53471-3%\u3002\u6ce8\u610f\u529b\u53cd\u9988\u6c47\u603b\u5668\u5728\u8fed\u4ee3\u591a\u8f6e\u68c0\u7d22\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u67e5\u8be2\u6f02\u79fb\u95ee\u9898\u3002", "conclusion": "\u76f8\u5173\u6027\u53cd\u9988\u673a\u5236\u80fd\u591f\u6301\u7eed\u63d0\u5347\u4e0d\u540c\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u548c\u81ea\u9002\u5e94\u89c6\u89c9\u641c\u7d22\u5f00\u8f9f\u4e86\u65b0\u673a\u9047\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u65e2\u53ef\u4ee5\u4f5c\u4e3a\u5fae\u8c03\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e5f\u53ef\u4ee5\u4e0e\u5df2\u5fae\u8c03\u6a21\u578b\u534f\u540c\u4f7f\u7528\uff0c\u5c55\u793a\u4e86\u5728\u63a8\u7406\u9636\u6bb5\u4f18\u5316\u68c0\u7d22\u6548\u679c\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2511.17308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17308", "abs": "https://arxiv.org/abs/2511.17308", "authors": ["Jiajie Guo", "Qingpeng Zhu", "Jin Zeng", "Xiaolong Wu", "Changyong He", "Weida Wang"], "title": "SpatialGeo:Boosting Spatial Reasoning in Multimodal LLMs via Geometry-Semantics Fusion", "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved significant progress in image and language tasks due to the strong reasoning capability of large language models (LLMs). Nevertheless, most MLLMs suffer from limited spatial reasoning ability to interpret and infer spatial arrangements in three-dimensional space. In this work, we propose a novel vision encoder based on hierarchical fusion of geometry and semantics features, generating spatial-aware visual embedding and boosting the spatial grounding capability of MLLMs. Specifically, we first unveil that the spatial ambiguity shortcoming stems from the lossy embedding of the vision encoder utilized in most existing MLLMs (e.g., CLIP), restricted to instance-level semantic features. This motivates us to complement CLIP with the geometry features from vision-only self-supervised learning via a hierarchical adapter, enhancing the spatial awareness in the proposed SpatialGeo. The network is efficiently trained using pretrained LLaVA model and optimized with random feature dropping to avoid trivial solutions relying solely on the CLIP encoder. Experimental results show that SpatialGeo improves the accuracy in spatial reasoning tasks, enhancing state-of-the-art models by at least 8.0% in SpatialRGPT-Bench with approximately 50% less memory cost during inference. The source code is available via https://ricky-plus.github.io/SpatialGeoPages/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSpatialGeo\uff0c\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u4e0e\u8bed\u4e49\u7279\u5f81\u5206\u5c42\u878d\u5408\u7684\u65b0\u578b\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u751f\u6210\u7a7a\u95f4\u611f\u77e5\u7684\u89c6\u89c9\u5d4c\u5165\u6765\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u5185\u5b58\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u7ef4\u7a7a\u95f4\u89e3\u91ca\u548c\u63a8\u7406\u7a7a\u95f4\u5e03\u5c40\u65b9\u9762\u5b58\u5728\u80fd\u529b\u9650\u5236\uff0c\u4e3b\u8981\u95ee\u9898\u6e90\u4e8e\u73b0\u6709\u89c6\u89c9\u7f16\u7801\u5668\uff08\u5982CLIP\uff09\u7684\u635f\u5931\u6027\u5d4c\u5165\u4ec5\u5173\u6ce8\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u7279\u5f81\uff0c\u5bfc\u81f4\u7a7a\u95f4\u6a21\u7cca\u6027\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u51e0\u4f55\u4e0e\u8bed\u4e49\u7279\u5f81\u5206\u5c42\u878d\u5408\u7684\u89c6\u89c9\u7f16\u7801\u5668SpatialGeo\uff0c\u901a\u8fc7\u5206\u5c42\u9002\u914d\u5668\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\u83b7\u5f97\u7684\u51e0\u4f55\u7279\u5f81\u4e0eCLIP\u7684\u8bed\u4e49\u7279\u5f81\u4e92\u8865\uff0c\u91c7\u7528\u968f\u673a\u7279\u5f81\u4e22\u5f03\u7b56\u7565\u907f\u514d\u4ec5\u4f9d\u8d56CLIP\u7f16\u7801\u5668\u7684\u5e73\u51e1\u89e3\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3LLaVA\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSpatialGeo\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\uff0c\u5728SpatialRGPT-Bench\u4e0a\u5c06\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u81f3\u5c118.0%\uff0c\u540c\u65f6\u63a8\u7406\u671f\u95f4\u5185\u5b58\u6210\u672c\u964d\u4f4e\u7ea650%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u7684\u5173\u952e\u5728\u4e8e\u878d\u5408\u51e0\u4f55\u4e0e\u8bed\u4e49\u7279\u5f81\uff0c\u63d0\u51fa\u7684\u5206\u5c42\u878d\u5408\u65b9\u6cd5\u4e3a\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2511.17344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17344", "abs": "https://arxiv.org/abs/2511.17344", "authors": ["Markus Pobitzer", "Chang Liu", "Chenyi Zhuang", "Teng Long", "Bin Ren", "Nicu Sebe"], "title": "Loomis Painter: Reconstructing the Painting Process", "comment": null, "summary": "Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation. This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS, DINO, and CLIP metrics. Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u5a92\u4f53\u7ed8\u753b\u8fc7\u7a0b\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u9a71\u52a8\u7684\u98ce\u683c\u63a7\u5236\u673a\u5236\u548c\u8de8\u5a92\u4f53\u98ce\u683c\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u8de8\u98ce\u683c\u7684\u4e00\u81f4\u7eb9\u7406\u6f14\u5316\u548c\u8fc7\u7a0b\u8fc1\u79fb\u3002\u8be5\u6846\u67b6\u91c7\u7528\u53cd\u5411\u7ed8\u753b\u8bad\u7ec3\u7b56\u7565\u786e\u4fdd\u751f\u6210\u8fc7\u7a0b\u5e73\u6ed1\u4e14\u7b26\u5408\u4eba\u7c7b\u521b\u4f5c\u6d41\u7a0b\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u7ed8\u753b\u8fc7\u7a0b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7ed8\u753b\u6559\u7a0b\u89c6\u9891\u8d44\u6e90\u7f3a\u4e4f\u4ea4\u4e92\u6027\u548c\u4e2a\u6027\u5316\uff0c\u800c\u5f53\u524d\u7684\u751f\u6210\u6a21\u578b\u5728\u8de8\u5a92\u4f53\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7ecf\u5e38\u51fa\u73b0\u65f6\u95f4\u6216\u7ed3\u6784\u4e0a\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u96be\u4ee5\u5fe0\u5b9e\u518d\u73b0\u4eba\u7c7b\u521b\u4f5c\u6d41\u7a0b\u3002\u8fd9\u9650\u5236\u4e86\u827a\u672f\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5bf9\u591a\u6837\u5316\u7ed8\u753b\u5a92\u4ecb\u548c\u98ce\u683c\u7684\u6280\u672f\u638c\u63e1\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u591a\u5a92\u4f53\u7ed8\u753b\u8fc7\u7a0b\u751f\u6210\u6846\u67b6\uff0c\u91c7\u7528\u8bed\u4e49\u9a71\u52a8\u7684\u98ce\u683c\u63a7\u5236\u673a\u5236\u5c06\u591a\u79cd\u5a92\u4f53\u5d4c\u5165\u6269\u6563\u6a21\u578b\u7684\u6761\u4ef6\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u8de8\u5a92\u4f53\u98ce\u683c\u589e\u5f3a\u6280\u672f\u3002\u901a\u8fc7\u53cd\u5411\u7ed8\u753b\u8bad\u7ec3\u7b56\u7565\u786e\u4fdd\u751f\u6210\u8fc7\u7a0b\u7684\u5e73\u6ed1\u6027\u548c\u4eba\u7c7b\u5bf9\u9f50\u6027\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u771f\u5b9e\u7ed8\u753b\u8fc7\u7a0b\u6570\u636e\u96c6\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5728\u8de8\u5a92\u4f53\u4e00\u81f4\u6027\u3001\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u6700\u7ec8\u56fe\u50cf\u4fdd\u771f\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u5728LPIPS\u3001DINO\u548cCLIP\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u63d0\u51fa\u7684\u611f\u77e5\u8ddd\u79bb\u8f6e\u5ed3\u66f2\u7ebf\u80fd\u591f\u5b9a\u91cf\u5efa\u6a21\u521b\u4f5c\u5e8f\u5217\uff0c\u5305\u62ec\u6784\u56fe\u3001\u8272\u5f69\u5206\u5757\u548c\u7ec6\u8282\u7cbe\u70bc\u7b49\u9636\u6bb5\uff0c\u51c6\u786e\u53cd\u6620\u4e86\u4eba\u7c7b\u827a\u672f\u521b\u4f5c\u8fdb\u7a0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u827a\u672f\u6559\u80b2\u63d0\u4f9b\u4e86\u4ea4\u4e92\u5f0f\u548c\u4e2a\u6027\u5316\u7684\u5b66\u4e60\u5de5\u5177\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u751f\u6210\u6846\u67b6\u89e3\u51b3\u4e86\u8de8\u5a92\u4f53\u7ed8\u753b\u8fc7\u7a0b\u751f\u6210\u7684\u6311\u6218\u3002\u611f\u77e5\u8ddd\u79bb\u8f6e\u5ed3\u66f2\u7ebf\u4e3a\u91cf\u5316\u5206\u6790\u521b\u4f5c\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u53cd\u5411\u7ed8\u753b\u8bad\u7ec3\u7b56\u7565\u786e\u4fdd\u4e86\u751f\u6210\u8fc7\u7a0b\u4e0e\u4eba\u7c7b\u521b\u4f5c\u4e60\u60ef\u7684\u4e00\u81f4\u6027\uff0c\u4e3a\u672a\u6765\u827a\u672f\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.17355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17355", "abs": "https://arxiv.org/abs/2511.17355", "authors": ["Taixi Chen", "Jingyun Chen", "Nancy Guo"], "title": "UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification", "comment": null, "summary": "Cell-level radiomics features provide fine-grained insights into tumor phenotypes and have the potential to significantly enhance diagnostic accuracy on hematoxylin and eosin (H&E) images. By capturing micro-level morphological and intensity patterns, these features support more precise tumor identification and improve AI interpretability by highlighting diagnostically relevant cells for pathologist review. However, most existing studies focus on slide-level or patch-level tumor classification, leaving cell-level radiomics analysis largely unexplored. Moreover, there is currently no dedicated backbone specifically designed for radiomics data. Inspired by the recent success of the Mamba architecture in vision and language domains, we introduce a Unified Attention-Mamba (UAM) backbone for cell-level classification using radiomics features. Unlike previous hybrid approaches that integrate Attention and Mamba modules in fixed proportions, our unified design flexibly combines their capabilities within a single cohesive architecture, eliminating the need for manual ratio tuning and improving encode capability. We develop two UAM variants to comprehensively evaluate the benefits of this unified structure. Building on this backbone, we further propose a multimodal UAM framework that jointly performs cell-level classification and image segmentation. Experimental results demonstrate that UAM achieves state-of-the-art performance across both tasks on public benchmarks, surpassing leading image-based foundation models. It improves cell classification accuracy from 74% to 78% ($n$=349,882 cells), and tumor segmentation precision from 75% to 80% ($n$=406 patches). These findings highlight the effectiveness and promise of UAM as a unified and extensible multimodal foundation for radiomics-driven cancer diagnosis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6ce8\u610f\u529b-\u66fc\u5df4\uff08UAM\uff09\u4e3b\u5e72\u7f51\u7edc\uff0c\u4e13\u95e8\u7528\u4e8e\u7ec6\u80de\u7ea7\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u5206\u7c7b\uff0c\u5e76\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5728\u7ec6\u80de\u5206\u7c7b\u548c\u80bf\u7624\u5206\u5272\u4efb\u52a1\u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u8be5\u7814\u7a76\u586b\u8865\u4e86\u7ec6\u80de\u7ea7\u653e\u5c04\u7ec4\u5b66\u5206\u6790\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u57fa\u4e8e\u653e\u5c04\u7ec4\u5b66\u7684\u764c\u75c7\u8bca\u65ad\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5207\u7247\u7ea7\u6216\u6591\u5757\u7ea7\u80bf\u7624\u5206\u7c7b\uff0c\u800c\u7ec6\u80de\u7ea7\u653e\u5c04\u7ec4\u5b66\u5206\u6790\u9886\u57df\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4e14\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u4e3a\u653e\u5c04\u7ec4\u5b66\u6570\u636e\u8bbe\u8ba1\u7684\u4e13\u7528\u4e3b\u5e72\u7f51\u7edc\u3002\u7ec6\u80de\u7ea7\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u80fd\u591f\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u80bf\u7624\u8868\u578b\u6d1e\u5bdf\uff0c\u4f46\u8fd9\u4e00\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u6709\u6548\u6316\u6398\u3002", "method": "\u53d7Mamba\u67b6\u6784\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u9886\u57df\u6210\u529f\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u6ce8\u610f\u529b-\u66fc\u5df4\uff08UAM\uff09\u4e3b\u5e72\u7f51\u7edc\u7528\u4e8e\u7ec6\u80de\u7ea7\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u5206\u7c7b\u3002\u4e0e\u4e4b\u524d\u56fa\u5b9a\u6bd4\u4f8b\u96c6\u6210\u6ce8\u610f\u529b\u548cMamba\u6a21\u5757\u7684\u6df7\u5408\u65b9\u6cd5\u4e0d\u540c\uff0cUAM\u5728\u5355\u4e00\u7edf\u4e00\u67b6\u6784\u4e2d\u7075\u6d3b\u7ed3\u5408\u4e24\u8005\u7684\u80fd\u529b\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u6bd4\u4f8b\u5e76\u63d0\u9ad8\u4e86\u7f16\u7801\u80fd\u529b\u3002\u5f00\u53d1\u4e86\u4e24\u79cdUAM\u53d8\u4f53\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u63d0\u51fa\u591a\u6a21\u6001UAM\u6846\u67b6\uff0c\u8054\u5408\u6267\u884c\u7ec6\u80de\u7ea7\u5206\u7c7b\u548c\u56fe\u50cf\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUAM\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u57fa\u7840\u6a21\u578b\u3002\u7ec6\u80de\u5206\u7c7b\u51c6\u786e\u7387\u4ece74%\u63d0\u5347\u81f378%\uff08n=349,882\u4e2a\u7ec6\u80de\uff09\uff0c\u80bf\u7624\u5206\u5272\u7cbe\u5ea6\u4ece75%\u63d0\u5347\u81f380%\uff08n=406\u4e2a\u6591\u5757\uff09\u3002\u8fd9\u4e9b\u6539\u8fdb\u8bc1\u660e\u4e86UAM\u5728\u653e\u5c04\u7ec4\u5b66\u9a71\u52a8\u8bca\u65ad\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "UAM\u4f5c\u4e3a\u4e00\u79cd\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u57fa\u7840\uff0c\u5c55\u73b0\u4e86\u5728\u653e\u5c04\u7ec4\u5b66\u9a71\u52a8\u764c\u75c7\u8bca\u65ad\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u548c\u6709\u6548\u6027\u3002\u8be5\u7814\u7a76\u4e3a\u7ec6\u80de\u7ea7\u653e\u5c04\u7ec4\u5b66\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u7edf\u4e00\u67b6\u6784\u8bbe\u8ba1\u6d88\u9664\u4e86\u624b\u52a8\u53c2\u6570\u8c03\u6574\u7684\u9700\u6c42\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u7f16\u7801\u80fd\u529b\u548c\u8bca\u65ad\u7cbe\u5ea6\uff0c\u4e3a\u7cbe\u51c6\u533b\u7597AI\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6491\u3002"}}
{"id": "2511.17362", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17362", "abs": "https://arxiv.org/abs/2511.17362", "authors": ["Linxiang Su", "Andr\u00e1s Balogh"], "title": "ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP", "comment": "16 pages", "summary": "Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images. As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness. In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC). Our method operates directly in the embedding space of CLIP, calculating augmentation-induced drift vectors to infer a semantic recovery direction and correcting the embedding based on the angular consistency of these latent drifts. Across a wide range of benchmarks, ATAC consistently achieves remarkably high robustness, surpassing that of previous state-of-the-art methods by nearly 50\\% on average, all while requiring minimal computational overhead. Furthermore, ATAC retains state-of-the-art robustness in unconventional and extreme settings and even achieves nontrivial robustness against adaptive attacks. Our results demonstrate that ATAC is an efficient method in a novel paradigm for test-time adversarial defenses in the embedding space of CLIP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u589e\u5f3a\u7684\u6d4b\u8bd5\u65f6\u5bf9\u6297\u6821\u6b63\u65b9\u6cd5ATAC\uff0c\u901a\u8fc7\u5728CLIP\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u589e\u5f3a\u8bf1\u5bfc\u7684\u6f02\u79fb\u5411\u91cf\u6765\u63a8\u65ad\u8bed\u4e49\u6062\u590d\u65b9\u5411\uff0c\u663e\u8457\u63d0\u5347\u4e86CLIP\u5728\u96f6\u6837\u672c\u56fe\u50cf-\u6587\u672c\u5339\u914d\u4efb\u52a1\u4e2d\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1CLIP\u5728\u96f6\u6837\u672c\u56fe\u50cf-\u6587\u672c\u5339\u914d\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u5bf9\u56fe\u50cf\u4e0a\u7684\u5bf9\u6297\u6270\u52a8\u9ad8\u5ea6\u8106\u5f31\uff0c\u800c\u5bf9\u6297\u5fae\u8c03\u6210\u672c\u8fc7\u9ad8\uff0c\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u9632\u5fa1\u7b56\u7565\u9c81\u68d2\u6027\u4ecd\u7136\u6709\u9650\u3002", "method": "ATAC\u65b9\u6cd5\u76f4\u63a5\u5728CLIP\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u8ba1\u7b97\u589e\u5f3a\u8bf1\u5bfc\u7684\u6f02\u79fb\u5411\u91cf\u6765\u63a8\u65ad\u8bed\u4e49\u6062\u590d\u65b9\u5411\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u6f5c\u5728\u6f02\u79fb\u7684\u89d2\u5ea6\u4e00\u81f4\u6027\u6765\u6821\u6b63\u5d4c\u5165\u8868\u793a\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cATAC\u59cb\u7ec8\u5b9e\u73b0\u4e86\u6781\u9ad8\u7684\u9c81\u68d2\u6027\uff0c\u5e73\u5747\u6bd4\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u9ad8\u51fa\u8fd150%\uff0c\u540c\u65f6\u4ec5\u9700\u6700\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728\u975e\u5e38\u89c4\u548c\u6781\u7aef\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ATAC\u5c55\u793a\u4e86\u4e00\u79cd\u5728CLIP\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u65f6\u5bf9\u6297\u9632\u5fa1\u7684\u65b0\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u8bed\u4e49\u6062\u590d\u65b9\u5411\u7684\u63a8\u65ad\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u5bf9\u6297\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.17397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17397", "abs": "https://arxiv.org/abs/2511.17397", "authors": ["Huangbiao Xu", "Huanqi Wu", "Xiao Ke", "Junyi Wu", "Rui Xu", "Jinglin Xu"], "title": "MCMoE: Completing Missing Modalities with Mixture of Experts for Incomplete Multimodal Action Quality Assessment", "comment": "AAAI 2026", "summary": "Multimodal Action Quality Assessment (AQA) has recently emerged as a promising paradigm. By leveraging complementary information across shared contextual cues, it enhances the discriminative evaluation of subtle intra-class variations in highly similar action sequences. However, partial modalities are frequently unavailable at the inference stage in reality. The absence of any modality often renders existing multimodal models inoperable. Furthermore, it triggers catastrophic performance degradation due to interruptions in cross-modal interactions. To address this issue, we propose a novel Missing Completion Framework with Mixture of Experts (MCMoE) that unifies unimodal and joint representation learning in single-stage training. Specifically, we propose an adaptive gated modality generator that dynamically fuses available information to reconstruct missing modalities. We then design modality experts to learn unimodal knowledge and dynamically mix the knowledge of all experts to extract cross-modal joint representations. With a mixture of experts, missing modalities are further refined and complemented. Finally, in the training phase, we mine the complete multimodal features and unimodal expert knowledge to guide modality generation and generation-based joint representation extraction. Extensive experiments demonstrate that our MCMoE achieves state-of-the-art results in both complete and incomplete multimodal learning on three public AQA benchmarks. Code is available at https://github.com/XuHuangbiao/MCMoE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7f3a\u5931\u6a21\u6001\u8865\u5168\u6846\u67b6MCMoE\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u673a\u5236\u7edf\u4e00\u5355\u6a21\u6001\u548c\u8054\u5408\u8868\u793a\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u4e2d\u6a21\u6001\u7f3a\u5931\u5bfc\u81f4\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u5728\u63a8\u7406\u9636\u6bb5\u7ecf\u5e38\u9762\u4e34\u90e8\u5206\u6a21\u6001\u4e0d\u53ef\u7528\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7f3a\u5931\u4efb\u4f55\u6a21\u6001\u65f6\u65e0\u6cd5\u6b63\u5e38\u5de5\u4f5c\uff0c\u4e14\u7531\u4e8e\u8de8\u6a21\u6001\u4ea4\u4e92\u4e2d\u65ad\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u95e8\u63a7\u6a21\u6001\u751f\u6210\u5668\u52a8\u6001\u878d\u5408\u53ef\u7528\u4fe1\u606f\u91cd\u6784\u7f3a\u5931\u6a21\u6001\uff0c\u8bbe\u8ba1\u6a21\u6001\u4e13\u5bb6\u5b66\u4e60\u5355\u6a21\u6001\u77e5\u8bc6\u5e76\u52a8\u6001\u6df7\u5408\u6240\u6709\u4e13\u5bb6\u77e5\u8bc6\u63d0\u53d6\u8de8\u6a21\u6001\u8054\u5408\u8868\u793a\uff0c\u901a\u8fc7\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u8fdb\u4e00\u6b65\u7ec6\u5316\u548c\u8865\u5145\u7f3a\u5931\u6a21\u6001\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171AQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMCMoE\u5728\u5b8c\u6574\u548c\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u5b66\u4e60\u573a\u666f\u4e0b\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u6a21\u6001\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u8bad\u7ec3\u7edf\u4e00\u4e86\u5355\u6a21\u6001\u548c\u8054\u5408\u8868\u793a\u5b66\u4e60\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u591a\u6a21\u6001\u8bc4\u4f30\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2511.17448", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17448", "abs": "https://arxiv.org/abs/2511.17448", "authors": ["Yuqi Li", "Junhao Dong", "Chuanguang Yang", "Shiping Wen", "Piotr Koniusz", "Tingwen Huang", "Yingli Tian", "Yew-Soon Ong"], "title": "MMT-ARD: Multimodal Multi-Teacher Adversarial Distillation for Robust Vision-Language Models", "comment": "10 pages", "summary": "Vision-Language Models (VLMs) are increasingly deployed in safety-critical applications, making their adversarial robustness a crucial concern. While adversarial knowledge distillation has shown promise in transferring robustness from teacher to student models, traditional single-teacher approaches suffer from limited knowledge diversity, slow convergence, and difficulty in balancing robustness and accuracy. To address these challenges, we propose MMT-ARD: a Multimodal Multi-Teacher Adversarial Robust Distillation framework. Our key innovation is a dual-teacher knowledge fusion architecture that collaboratively optimizes clean feature preservation and robust feature enhancement. To better handle challenging adversarial examples, we introduce a dynamic weight allocation strategy based on teacher confidence, enabling adaptive focus on harder samples. Moreover, to mitigate bias among teachers, we design an adaptive sigmoid-based weighting function that balances the strength of knowledge transfer across modalities. Extensive experiments on ImageNet and zero-shot benchmarks demonstrate that MMT-ARD improves robust accuracy by +4.32% and zero-shot accuracy by +3.5% on the ViT-B-32 model, while achieving a 2.3x increase in training efficiency over traditional single-teacher methods. These results highlight the effectiveness and scalability of MMT-ARD in enhancing the adversarial robustness of multimodal large models. Our codes are available at https://github.com/itsnotacie/MMT-ARD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MMT-ARD\u591a\u6a21\u6001\u591a\u6559\u5e08\u5bf9\u6297\u9c81\u68d2\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6559\u5e08\u77e5\u8bc6\u878d\u5408\u67b6\u6784\u548c\u52a8\u6001\u6743\u91cd\u5206\u914d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5355\u6559\u5e08\u5bf9\u6297\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5b58\u5728\u77e5\u8bc6\u591a\u6837\u6027\u6709\u9650\u3001\u6536\u655b\u901f\u5ea6\u6162\u4ee5\u53ca\u9c81\u68d2\u6027\u4e0e\u51c6\u786e\u6027\u96be\u4ee5\u5e73\u8861\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u591a\u6559\u5e08\u5bf9\u6297\u9c81\u68d2\u84b8\u998f\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u6559\u5e08\u77e5\u8bc6\u878d\u5408\u67b6\u6784\u534f\u540c\u4f18\u5316\u6e05\u6d01\u7279\u5f81\u4fdd\u6301\u548c\u9c81\u68d2\u7279\u5f81\u589e\u5f3a\uff0c\u5f15\u5165\u57fa\u4e8e\u6559\u5e08\u7f6e\u4fe1\u5ea6\u7684\u52a8\u6001\u6743\u91cd\u5206\u914d\u7b56\u7565\u5904\u7406\u56f0\u96be\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94sigmoid\u52a0\u6743\u51fd\u6570\u5e73\u8861\u8de8\u6a21\u6001\u77e5\u8bc6\u4f20\u9012\u5f3a\u5ea6\u3002", "result": "\u5728ImageNet\u548c\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViT-B-32\u6a21\u578b\u7684\u9c81\u68d2\u51c6\u786e\u7387\u63d0\u5347+4.32%\uff0c\u96f6\u6837\u672c\u51c6\u786e\u7387\u63d0\u5347+3.5%\uff0c\u8bad\u7ec3\u6548\u7387\u76f8\u6bd4\u4f20\u7edf\u5355\u6559\u5e08\u65b9\u6cd5\u63d0\u9ad82.3\u500d\u3002", "conclusion": "MMT-ARD\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6559\u5e08\u84b8\u998f\u4e2d\u7684\u77e5\u8bc6\u878d\u5408\u548c\u5e73\u8861\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u591a\u6559\u5e08\u534f\u4f5c\u5728\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17455", "abs": "https://arxiv.org/abs/2511.17455", "authors": ["Bj\u00f6rn Michele", "Alexandre Boulch", "Gilles Puy", "Tuan-Hung Vu", "Renaud Marlet", "Nicolas Courty"], "title": "Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift", "comment": "Accepted at BMVC 2025", "summary": "Semantic segmentation networks trained under full supervision for one type of lidar fail to generalize to unseen lidars without intervention. To reduce the performance gap under domain shifts, a recent trend is to leverage vision foundation models (VFMs) providing robust features across domains. In this work, we conduct an exhaustive study to identify recipes for exploiting VFMs in unsupervised domain adaptation for semantic segmentation of lidar point clouds. Building upon unsupervised image-to-lidar knowledge distillation, our study reveals that: (1) the architecture of the lidar backbone is key to maximize the generalization performance on a target domain; (2) it is possible to pretrain a single backbone once and for all, and use it to address many domain shifts; (3) best results are obtained by keeping the pretrained backbone frozen and training an MLP head for semantic segmentation. The resulting pipeline achieves state-of-the-art results in four widely-recognized and challenging settings. The code will be available at: https://github.com/valeoai/muddos.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u63a2\u7d22\u4e86\u5982\u4f55\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\uff0c\u53d1\u73b0\u9aa8\u5e72\u7f51\u7edc\u67b6\u6784\u5bf9\u6cdb\u5316\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u4e00\u6b21\u6027\u9884\u8bad\u7ec3\u5e76\u9002\u7528\u4e8e\u591a\u79cd\u57df\u504f\u79fb\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u5b8c\u5168\u76d1\u7763\u4e0b\u8bad\u7ec3\u7684\u6fc0\u5149\u96f7\u8fbe\u8bed\u4e49\u5206\u5272\u7f51\u7edc\u65e0\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u6fc0\u5149\u96f7\u8fbe\u7c7b\u578b\u4e0a\u5b9e\u73b0\u826f\u597d\u6cdb\u5316\uff0c\u5b58\u5728\u663e\u8457\u7684\u57df\u504f\u79fb\u6027\u80fd\u5dee\u8ddd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u6709\u6548\u5229\u7528\u8de8\u57df\u9c81\u68d2\u6027\u5f3a\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6765\u63d0\u5347\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u7684\u57df\u81ea\u9002\u5e94\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u65e0\u76d1\u7763\u56fe\u50cf\u5230\u6fc0\u5149\u96f7\u8fbe\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u6fc0\u5149\u96f7\u8fbe\u8bed\u4e49\u5206\u5272\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u5e94\u7528\u65b9\u6cd5\u3002\u5173\u952e\u53d1\u73b0\u5305\u62ec\uff1a\u9aa8\u5e72\u7f51\u7edc\u67b6\u6784\u5bf9\u76ee\u6807\u57df\u6cdb\u5316\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff1b\u53ef\u4ee5\u4e00\u6b21\u6027\u9884\u8bad\u7ec3\u5355\u4e2a\u9aa8\u5e72\u7f51\u7edc\u5e76\u5e94\u7528\u4e8e\u591a\u79cd\u57df\u504f\u79fb\u573a\u666f\uff1b\u6700\u4f73\u7ed3\u679c\u901a\u8fc7\u51bb\u7ed3\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\u5e76\u8bad\u7ec3MLP\u5206\u5272\u5934\u83b7\u5f97\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u56db\u4e2a\u5e7f\u6cdb\u8ba4\u53ef\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u57df\u81ea\u9002\u5e94\u8bbe\u7f6e\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u540c\u6fc0\u5149\u96f7\u8fbe\u7c7b\u578b\u4e4b\u95f4\u7684\u57df\u504f\u79fb\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6fc0\u5149\u96f7\u8fbe\u9aa8\u5e72\u7f51\u7edc\u67b6\u6784\u662f\u6700\u5927\u5316\u57df\u81ea\u9002\u5e94\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e14\u53ef\u4ee5\u901a\u8fc7\u4e00\u6b21\u6027\u9884\u8bad\u7ec3\u5b9e\u73b0\u591a\u57df\u6cdb\u5316\u3002\u51bb\u7ed3\u9884\u8bad\u7ec3\u9aa8\u5e72\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u5272\u5934\u7684\u7b56\u7565\u88ab\u8bc1\u660e\u662f\u6700\u6709\u6548\u7684\uff0c\u4e3a\u6fc0\u5149\u96f7\u8fbe\u8bed\u4e49\u5206\u5272\u7684\u57df\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u548c\u8bbe\u8ba1\u6307\u5bfc\u3002"}}
{"id": "2511.17487", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17487", "abs": "https://arxiv.org/abs/2511.17487", "authors": ["Mark Endo", "Serena Yeung-Levy"], "title": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models", "comment": "Website at https://web.stanford.edu/~markendo/projects/downscaling_intelligence", "summary": "Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aExtract+Think\u7684\u9ad8\u6548\u591a\u6a21\u6001\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u63d0\u53d6\u8c03\u4f18\u548c\u9010\u6b65\u63a8\u7406\u6765\u89e3\u51b3LLM\u964d\u7ef4\u5bf9\u89c6\u89c9\u80fd\u529b\u7684\u4e0d\u6210\u6bd4\u4f8b\u5f71\u54cd\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6548\u7387\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u6a21\u578b\u7684\u89c4\u6a21\u5316\u53d1\u5c55\uff0c\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u8981\u6c42\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u7684\u7cfb\u7edf\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5206\u6790\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u667a\u80fd\u964d\u7ef4\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u63a2\u7a76\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u91cf\u5982\u4f55\u5f71\u54cd\u591a\u6a21\u6001\u80fd\u529b\uff0c\u5e76\u89e3\u51b3LLM\u964d\u7ef4\u5bf9\u89c6\u89c9\u80fd\u529b\u9020\u6210\u7684\u8fc7\u5ea6\u635f\u5bb3\u95ee\u9898\u3002", "method": "\u63d0\u51faExtract+Think\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u89c6\u89c9\u63d0\u53d6\u8c03\u4f18\uff0c\u901a\u8fc7\u663e\u5f0f\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u4e0e\u6307\u4ee4\u76f8\u5173\u7684\u89c6\u89c9\u7ec6\u8282\uff1b\u4ee5\u53ca\u9010\u6b65\u63a8\u7406\u673a\u5236\uff0c\u5229\u7528\u63d0\u53d6\u7684\u89c6\u89c9\u7ec6\u8282\u751f\u6210\u7b54\u6848\u3002\u8be5\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9LLM\u964d\u7ef4\u5bfc\u81f4\u7684\u89c6\u89c9\u80fd\u529b\u74f6\u9888\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u964d\u7ef4\u5bf9\u89c6\u89c9\u80fd\u529b\u7684\u5f71\u54cd\u4e0d\u6210\u6bd4\u4f8b\u5730\u5927\u4e8e\u5bf9LLM\u7ee7\u627f\u80fd\u529b\u7684\u5f71\u54cd\u3002\u89c6\u89c9\u63d0\u53d6\u8c03\u4f18\u663e\u8457\u7f13\u89e3\u4e86\u8fd9\u4e00\u74f6\u9888\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u65b0\u5e73\u8861\uff0c\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u7684\u4e0b\u964d\u5e45\u5ea6\u751a\u81f3\u8d85\u8fc7\u63a8\u7406\u80fd\u529b\u7684\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u89c6\u89c9\u80fd\u529b\u5bf9LLM\u5bb9\u91cf\u7684\u654f\u611f\u6027\uff0c\u63d0\u51fa\u7684Extract+Think\u6846\u67b6\u4e3a\u6784\u5efa\u9ad8\u6548\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5f3a\u8c03\u4e86\u89c6\u89c9\u4fe1\u606f\u63d0\u53d6\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u4e3a\u672a\u6765\u8f7b\u91cf\u5316\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.17490", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17490", "abs": "https://arxiv.org/abs/2511.17490", "authors": ["Yolo Yunlong Tang", "Daiki Shimada", "Hang Hua", "Chao Huang", "Jing Bi", "Rogerio Feris", "Chenliang Xu"], "title": "Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination", "comment": null, "summary": "Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVideo-R4\uff0c\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u53cd\u520d\u673a\u5236\u7684\u89c6\u9891\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u9009\u62e9\u5e27\u3001\u653e\u5927\u5173\u952e\u533a\u57df\u548c\u91cd\u65b0\u7f16\u7801\u50cf\u7d20\u6765\u589e\u5f3a\u6587\u672c\u4e30\u5bcc\u89c6\u9891\u7684\u7406\u89e3\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u8fed\u4ee3\u53cd\u520d\u4f5c\u4e3a\u50cf\u7d20\u57fa\u7840\u591a\u6a21\u6001\u63a8\u7406\u7684\u6709\u6548\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u5355\u6b21\u611f\u77e5\u56fa\u5b9a\u5e27\uff0c\u5bfc\u81f4\u5728\u7ec6\u7c92\u5ea6\u8bc1\u636e\u7406\u89e3\u4e0a\u51fa\u73b0\u5e7b\u89c9\u548c\u5931\u8d25\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u9700\u8981\u91cd\u590d\u68c0\u67e5\u7684\u77ac\u6001\u6587\u672c\u7ebf\u7d22\u3002\u4eba\u7c7b\u89c2\u770b\u6587\u672c\u4e30\u5bcc\u89c6\u9891\u65f6\u4f1a\u6682\u505c\u3001\u653e\u5927\u548c\u91cd\u8bfb\u5173\u952e\u533a\u57df\uff0c\u8fd9\u79cd\u80fd\u529b\u662f\u5f53\u524d\u6a21\u578b\u6240\u7f3a\u4e4f\u7684\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u53cd\u520d\u673a\u5236\uff0c\u901a\u8fc7\u8fed\u4ee3\u9009\u62e9\u5e27\u3001\u653e\u5927\u4fe1\u606f\u533a\u57df\u3001\u91cd\u65b0\u7f16\u7801\u68c0\u7d22\u50cf\u7d20\u548c\u66f4\u65b0\u63a8\u7406\u72b6\u6001\u6765\u589e\u5f3a\u89c6\u9891\u7406\u89e3\u3002\u6784\u5efa\u4e86\u4e24\u4e2a\u5305\u542b\u53ef\u6267\u884c\u53cd\u520d\u8f68\u8ff9\u7684\u6570\u636e\u96c6\uff1aVideo-R4-CoT-17k\u7528\u4e8e\u76d1\u7763\u8bad\u7ec3\uff0cVideo-R4-RL-30k\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u3002\u91c7\u7528\u591a\u9636\u6bb5\u53cd\u520d\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u57fa\u4e8eGRPO\u7684\u5f3a\u5316\u5b66\u4e60\u9010\u6b65\u5fae\u8c037B\u53c2\u6570\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b66\u4e60\u539f\u5b50\u548c\u6df7\u5408\u89c6\u89c9\u64cd\u4f5c\u3002", "result": "Video-R4-7B\u5728M4-ViteVQA\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u8fd8\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5e94\u7528\u4e8e\u591a\u9875\u6587\u6863\u95ee\u7b54\u3001\u5e7b\u706f\u7247\u95ee\u7b54\u548c\u901a\u7528\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fed\u4ee3\u53cd\u520d\u673a\u5236\u5728\u50cf\u7d20\u57fa\u7840\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fed\u4ee3\u89c6\u89c9\u53cd\u520d\u662f\u4e00\u79cd\u6709\u6548\u7684\u50cf\u7d20\u57fa\u7840\u591a\u6a21\u6001\u63a8\u7406\u8303\u5f0f\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u6587\u672c\u4e30\u5bcc\u89c6\u9891\u7684\u7406\u89e3\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u8bc1\u636e\u7406\u89e3\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u8fd8\u5c55\u793a\u4e86\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u548c\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2511.17501", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.17501", "abs": "https://arxiv.org/abs/2511.17501", "authors": ["Weiwei Cai", "Shuangkang Fang", "Weicai Ye", "Xin Dong", "Yunhan Yang", "Xuanyang Zhang", "Wei Cheng", "Yanpei Cao", "Gang Yu", "Tao Chen"], "title": "Native 3D Editing with Full Attention", "comment": null, "summary": "Instruction-guided 3D editing is a rapidly emerging field with the potential to broaden access to 3D content creation. However, existing methods face critical limitations: optimization-based approaches are prohibitively slow, while feed-forward approaches relying on multi-view 2D editing often suffer from inconsistent geometry and degraded visual quality. To address these issues, we propose a novel native 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass. Specifically, we create a large-scale, multi-modal dataset for instruction-guided 3D editing, covering diverse addition, deletion, and modification tasks. This dataset is meticulously curated to ensure that edited objects faithfully adhere to the instructional changes while preserving the consistency of unedited regions with the source object. Building upon this dataset, we explore two distinct conditioning strategies for our model: a conventional cross-attention mechanism and a novel 3D token concatenation approach. Our results demonstrate that token concatenation is more parameter-efficient and achieves superior performance. Extensive evaluations show that our method outperforms existing 2D-lifting approaches, setting a new benchmark in generation quality, 3D consistency, and instruction fidelity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u539f\u751f3D\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u76f4\u63a5\u64cd\u4f5c3D\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u901f\u5ea6\u6162\u548c\u57fa\u4e8e\u591a\u89c6\u56fe2D\u7f16\u8f91\u65b9\u6cd5\u51e0\u4f55\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u521b\u65b0\u76843D\u6807\u8bb0\u62fc\u63a5\u7b56\u7565\uff0c\u5728\u751f\u6210\u8d28\u91cf\u30013D\u4e00\u81f4\u6027\u548c\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6307\u4ee4\u5f15\u5bfc\u76843D\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u8ba1\u7b97\u901f\u5ea6\u6781\u6162\uff0c\u800c\u57fa\u4e8e\u591a\u89c6\u56fe2D\u7f16\u8f91\u7684\u524d\u9988\u65b9\u6cd5\u5e38\u5e38\u5bfc\u81f4\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u89c6\u89c9\u8d28\u91cf\u4e0b\u964d\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e863D\u5185\u5bb9\u521b\u4f5c\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u4e14\u4fdd\u63013D\u4e00\u81f4\u6027\u7684\u7f16\u8f91\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6307\u4ee4\u5f15\u5bfc3D\u7f16\u8f91\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u6dfb\u52a0\u3001\u5220\u9664\u548c\u4fee\u6539\u7b49\u591a\u79cd\u4efb\u52a1\u7c7b\u578b\uff0c\u786e\u4fdd\u7f16\u8f91\u5bf9\u8c61\u5fe0\u5b9e\u9075\u5faa\u6307\u4ee4\u53d8\u5316\u540c\u65f6\u4fdd\u6301\u672a\u7f16\u8f91\u533a\u57df\u4e0e\u6e90\u5bf9\u8c61\u7684\u4e00\u81f4\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63a2\u7d22\u4e86\u4e24\u79cd\u6761\u4ef6\u7b56\u7565\uff1a\u4f20\u7edf\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u521b\u65b0\u76843D\u6807\u8bb0\u62fc\u63a5\u65b9\u6cd5\uff0c\u540e\u8005\u88ab\u8bc1\u660e\u53c2\u6570\u6548\u7387\u66f4\u9ad8\u4e14\u6027\u80fd\u66f4\u4f18\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u8d28\u91cf\u30013D\u4e00\u81f4\u6027\u548c\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u76842D\u63d0\u5347\u65b9\u6cd5\uff0c\u786e\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\u3002\u5b9e\u9a8c\u8bc1\u660e3D\u6807\u8bb0\u62fc\u63a5\u7b56\u7565\u76f8\u6bd4\u4f20\u7edf\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5177\u6709\u66f4\u597d\u7684\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\u8868\u73b0\uff0c\u4e3a3D\u7f16\u8f91\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u539f\u751f3D\u7f16\u8f91\u6846\u67b6\u5728\u6548\u7387\u548c\u4e00\u81f4\u6027\u65b9\u9762\u7684\u663e\u8457\u4f18\u52bf\uff0c\u4e3a3D\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u5de5\u5177\u30023D\u6807\u8bb0\u62fc\u63a5\u7b56\u7565\u7684\u4f18\u8d8a\u6027\u4e3a\u672a\u67653D\u751f\u6210\u6a21\u578b\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\uff0c\u540c\u65f6\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u6784\u5efa\u4e5f\u4e3a\u8be5\u9886\u57df\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
