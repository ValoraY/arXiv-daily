<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-10-31.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 30]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 5]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 2]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-dynamic-vlm-guided-negative-prompting-for-diffusion-models">[1] <a href="https://arxiv.org/abs/2510.26052">Dynamic VLM-Guided Negative Prompting for Diffusion Models</a></h3>
<p><em>Hoyeon Chang, Seungjin Kim, Yoonseok Choi</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„åŠ¨æ€è´Ÿæç¤ºæ–¹æ³•ï¼Œé€šè¿‡åœ¨å»å™ªè¿‡ç¨‹ä¸­ç”Ÿæˆä¸­é—´å›¾åƒé¢„æµ‹å¹¶æŸ¥è¯¢VLMæ¥äº§ç”Ÿä¸Šä¸‹æ–‡ç›¸å…³çš„è´Ÿæç¤ºï¼Œç›¸æ¯”ä¼ ç»Ÿå›ºå®šè´Ÿæç¤ºæ–¹æ³•å®ç°äº†æ›´çµæ´»çš„å›¾åƒç”Ÿæˆæ§åˆ¶ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä¸­çš„è´Ÿæç¤ºæ–¹æ³•é€šå¸¸ä½¿ç”¨å›ºå®šçš„è´Ÿæç¤ºæ–‡æœ¬ï¼Œç¼ºä¹å¯¹ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸Šä¸‹æ–‡å˜åŒ–çš„é€‚åº”æ€§ï¼Œæ— æ³•æ ¹æ®ä¸­é—´ç”ŸæˆçŠ¶æ€åŠ¨æ€è°ƒæ•´è´Ÿå¼•å¯¼ç­–ç•¥ï¼Œé™åˆ¶äº†å›¾åƒç”Ÿæˆè´¨é‡å’Œæ–‡æœ¬å¯¹é½çš„ä¼˜åŒ–æ½œåŠ›ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åœ¨å»å™ªè¿‡ç¨‹çš„ç‰¹å®šæ­¥éª¤ç”Ÿæˆä¸­é—´å›¾åƒé¢„æµ‹ï¼Œç„¶ååˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹åˆ†æè¿™äº›ä¸­é—´ç»“æœå¹¶ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„è´Ÿæç¤ºï¼Œå®ç°äº†è´Ÿæç¤ºçš„åŠ¨æ€è‡ªé€‚åº”ç”Ÿæˆï¼Œè€Œéä¾èµ–é¢„è®¾çš„å›ºå®šæç¤ºæ–‡æœ¬ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è´Ÿå¼•å¯¼å¼ºåº¦ä¸æ–‡æœ¬-å›¾åƒå¯¹é½åº¦ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ï¼Œç›¸æ¯”ä¼ ç»Ÿå›ºå®šè´Ÿæç¤ºæ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Conclusion:</strong> åŠ¨æ€è´Ÿæç¤ºæœºåˆ¶ä¸ºæ‰©æ•£æ¨¡å‹æä¾›äº†æ›´ç²¾ç»†çš„ç”Ÿæˆæ§åˆ¶èƒ½åŠ›ï¼Œè¯æ˜äº†ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå®æ—¶è´Ÿæç¤ºç”Ÿæˆçš„å¯è¡Œæ€§ï¼Œä¸ºæœªæ¥è‡ªé€‚åº”å›¾åƒç”Ÿæˆæ–¹æ³•å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>We propose a novel approach for dynamic negative prompting in diffusion
models that leverages Vision-Language Models (VLMs) to adaptively generate
negative prompts during the denoising process. Unlike traditional Negative
Prompting methods that use fixed negative prompts, our method generates
intermediate image predictions at specific denoising steps and queries a VLM to
produce contextually appropriate negative prompts. We evaluate our approach on
various benchmark datasets and demonstrate the trade-offs between negative
guidance strength and text-image alignment.</p>
<h3 id="2-security-risk-of-misalignment-between-text-and-image-in-multi-modal-model">[2] <a href="https://arxiv.org/abs/2510.26105">Security Risk of Misalignment between Text and Image in Multi-modal Model</a></h3>
<p><em>Xiaosen Wang, Zhijin Ge, Shaokang Wang</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†PReMAæ”»å‡»æ–¹æ³•ï¼Œé¦–æ¬¡é€šè¿‡ä»…åˆ›å»ºå¯¹æŠ—æ€§å›¾åƒæ¥æ“çºµå¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹çš„è¾“å‡ºï¼Œæ­ç¤ºäº†ç°æœ‰æ‰©æ•£æ¨¡å‹ä¸­æ–‡æœ¬ä¸å›¾åƒæ¨¡æ€å¯¹é½ä¸è¶³çš„å®‰å…¨é£é™©ã€‚è¯¥æ–¹æ³•åœ¨å›¾åƒä¿®å¤å’Œé£æ ¼è¿ç§»ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§æ•ˆåŠ›ï¼Œå¯¹å›ºå®šæç¤ºè¯çš„åº”ç”¨åœºæ™¯æ„æˆæ–°å‹å¨èƒã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å¯¹å¯¹æŠ—æ€§è¾“å…¥çš„è„†å¼±æ€§ä»æœªå……åˆ†æ¢ç´¢ã€‚ç ”ç©¶å‘ç°ç°æœ‰æ‰©æ•£æ¨¡å‹ä¸­æ–‡æœ¬ä¸å›¾åƒæ¨¡æ€çš„å¯¹é½å­˜åœ¨ä¸è¶³ï¼Œè¿™ç§é”™ä½åœ¨ç”Ÿæˆä¸å½“å†…å®¹æ—¶å¸¦æ¥æ˜¾è‘—é£é™©ï¼Œç‰¹åˆ«æ˜¯åœ¨NSFWå†…å®¹ç”Ÿæˆæ–¹é¢å­˜åœ¨å®‰å…¨éšæ‚£ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†Prompt-Restricted Multi-modal Attack (PReMA)æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡ä¿®æ”¹è¾“å…¥å›¾åƒæ¥æ“çºµç”Ÿæˆå†…å®¹ï¼ŒåŒæ—¶ä¿æŒæç¤ºè¯ä¸å˜ã€‚è¿™æ˜¯é¦–ä¸ªä»…é€šè¿‡åˆ›å»ºå¯¹æŠ—æ€§å›¾åƒæ¥æ“çºµæ¨¡å‹è¾“å‡ºçš„æ”»å‡»æ–¹æ³•ï¼ŒåŒºåˆ«äºå…ˆå‰ä¸»è¦ç”Ÿæˆå¯¹æŠ—æ€§æç¤ºè¯çš„æ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> åœ¨å›¾åƒä¿®å¤å’Œé£æ ¼è¿ç§»ä»»åŠ¡ä¸Šå¯¹å¤šç§æ¨¡å‹è¿›è¡Œçš„å…¨é¢è¯„ä¼°è¯å®äº†PReMAçš„å¼ºå¤§æ•ˆåŠ›ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ“çºµæ¨¡å‹è¾“å‡ºï¼Œç‰¹åˆ«æ˜¯åœ¨å›ºå®šæç¤ºè¯çš„å›¾åƒç¼–è¾‘åº”ç”¨ä¸­å±•ç°å‡ºæ˜¾è‘—å¨èƒã€‚</p>
<p><strong>Conclusion:</strong> PReMAæ­ç¤ºäº†å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹åœ¨æ¨¡æ€å¯¹é½æ–¹é¢çš„å®‰å…¨æ¼æ´ï¼Œå¯¹å›¾åƒç¼–è¾‘åº”ç”¨çš„å®Œæ•´æ€§æ„æˆæ–°å‹å¨èƒã€‚ç ”ç©¶å¼ºè°ƒäº†éœ€è¦åŠ å¼ºå¤šæ¨¡æ€æ¨¡å‹å®‰å…¨æ€§çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å›ºå®šæç¤ºè¯æ“ä½œåœºæ™¯ä¸‹çš„é˜²æŠ¤æªæ–½ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Despite the notable advancements and versatility of multi-modal diffusion
models, such as text-to-image models, their susceptibility to adversarial
inputs remains underexplored. Contrary to expectations, our investigations
reveal that the alignment between textual and Image modalities in existing
diffusion models is inadequate. This misalignment presents significant risks,
especially in the generation of inappropriate or Not-Safe-For-Work (NSFW)
content. To this end, we propose a novel attack called Prompt-Restricted
Multi-modal Attack (PReMA) to manipulate the generated content by modifying the
input image in conjunction with any specified prompt, without altering the
prompt itself. PReMA is the first attack that manipulates model outputs by
solely creating adversarial images, distinguishing itself from prior methods
that primarily generate adversarial prompts to produce NSFW content.
Consequently, PReMA poses a novel threat to the integrity of multi-modal
diffusion models, particularly in image-editing applications that operate with
fixed prompts. Comprehensive evaluations conducted on image inpainting and
style transfer tasks across various models confirm the potent efficacy of
PReMA.</p>
<h3 id="3-miro-multi-reward-conditioned-pretraining-improves-t2i-quality-and-efficiency">[3] <a href="https://arxiv.org/abs/2510.25897">MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency</a></h3>
<p><em>Nicolas Dufour, Lucas Degeorge, Arijit Ghosh, Vicky Kalogeiton, David Picard</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MIROæ–¹æ³•ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›´æ¥å¯¹å¤šä¸ªå¥–åŠ±æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä½¿æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿç›´æ¥å­¦ä¹ ç”¨æˆ·åå¥½ï¼Œä»è€Œæ˜¾è‘—æé«˜ç”Ÿæˆå›¾åƒè´¨é‡å¹¶åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åœ¨GenEvalç»„åˆåŸºå‡†å’Œç”¨æˆ·åå¥½è¯„åˆ†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤§å‹æœªç­›é€‰æ•°æ®é›†ä¸Šè®­ç»ƒä»¥å®ç°å¤šæ ·åŒ–ç”Ÿæˆèƒ½åŠ›ï¼Œä½†è¿™ä¸ç”¨æˆ·åå¥½å¹¶ä¸ä¸€è‡´ã€‚ç°æœ‰çš„å¥–åŠ±æ¨¡å‹æ–¹æ³•é€šè¿‡åå¤„ç†é€‰æ‹©ç”Ÿæˆå›¾åƒæ¥å¯¹é½å¥–åŠ±ï¼Œä½†ä¼šä¸¢å¼ƒä¿¡æ¯æ•°æ®å¹¶ä¼˜åŒ–å•ä¸€å¥–åŠ±ï¼Œä»è€ŒæŸå®³å¤šæ ·æ€§ã€è¯­ä¹‰ä¿çœŸåº¦å’Œæ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> æå‡ºçš„MIROæ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›´æ¥å¯¹å¤šä¸ªå¥–åŠ±æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç›´æ¥å­¦ä¹ ç”¨æˆ·åå¥½ï¼Œè€Œä¸æ˜¯é‡‡ç”¨åå¤„ç†æ–¹å¼ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä¿¡æ¯æ•°æ®çš„ä¸¢å¼ƒï¼Œå¹¶æ”¯æŒå¤šå¥–åŠ±ä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> MIROæ–¹æ³•ä¸ä»…æ˜¾è‘—æé«˜äº†ç”Ÿæˆå›¾åƒçš„è§†è§‰è´¨é‡ï¼Œè¿˜å¤§å¹…åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ã€‚åœ¨GenEvalç»„åˆåŸºå‡†å’Œç”¨æˆ·åå¥½è¯„åˆ†ï¼ˆPickAScoreã€ImageRewardã€HPSv2ï¼‰ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›´æ¥é›†æˆå¤šå¥–åŠ±æ¡ä»¶åŒ–æ¯”åå¤„ç†é€‰æ‹©æ›´æœ‰æ•ˆï¼Œèƒ½å¤ŸåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€å¤šæ ·æ€§å’Œæ•ˆç‡ã€‚è¿™ç§æ–¹æ³•ä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å¯¹é½é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†ç›´æ¥å­¦ä¹ ç”¨æˆ·åå¥½çš„ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Current text-to-image generative models are trained on large uncurated
datasets to enable diverse generation capabilities. However, this does not
align well with user preferences. Recently, reward models have been
specifically designed to perform post-hoc selection of generated images and
align them to a reward, typically user preference. This discarding of
informative data together with the optimizing for a single reward tend to harm
diversity, semantic fidelity and efficiency. Instead of this post-processing,
we propose to condition the model on multiple reward models during training to
let the model learn user preferences directly. We show that this not only
dramatically improves the visual quality of the generated images but it also
significantly speeds up the training. Our proposed method, called MIRO,
achieves state-of-the-art performances on the GenEval compositional benchmark
and user-preference scores (PickAScore, ImageReward, HPSv2).</p>
<h3 id="4-egoexo-con-exploring-view-invariant-video-temporal-understanding">[4] <a href="https://arxiv.org/abs/2510.26113">EgoExo-Con: Exploring View-Invariant Video Temporal Understanding</a></h3>
<p><em>Minjoon Jung, Junbin Xiao, Junghyun Kim, Byoung-Tak Zhang, Angela Yao</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†EgoExo-ConåŸºå‡†æ¥è¯„ä¼°è§†é¢‘-å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè§†è§’è§†é¢‘ä¸­çš„æ—¶åºç†è§£ä¸€è‡´æ€§ï¼Œå¹¶å¼€å‘äº†View-GRPOå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥æå‡è·¨è§†è§’ä¸€è‡´æ€§æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ç°æœ‰æ¨¡å‹åœ¨å¤šè§†è§’ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œè€Œæå‡ºçš„æ–¹æ³•åœ¨æ”¹å–„è·¨è§†è§’ä¸€è‡´æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘-å¤§è¯­è¨€æ¨¡å‹åœ¨ä»ä¸åŒè§†è§’æ•æ‰åŒä¸€äº‹ä»¶çš„è§†é¢‘ä¸­æ˜¯å¦èƒ½å¤Ÿå®ç°ä¸€è‡´çš„æ—¶åºç†è§£èƒ½åŠ›å°šä¸æ˜ç¡®ã€‚ç ”ç©¶æ—¨åœ¨è§£å†³æ¨¡å‹åœ¨å¤šè§†è§’è§†é¢‘ç†è§£ä¸­çš„ä¸€è‡´æ€§ç¼ºé™·é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å½“è§†é¢‘ä»è‡ªæˆ‘ä¸­å¿ƒè§†è§’å’Œå¤–éƒ¨è§†è§’åŒæ­¥è®°å½•åŒä¸€äº‹ä»¶æ—¶ï¼Œæ¨¡å‹éœ€è¦ä¿æŒè·¨è§†è§’çš„æ—¶åºæ¨ç†ä¸€è‡´æ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†EgoExo-ConåŸºå‡†ï¼ŒåŒ…å«å…¨é¢åŒæ­¥çš„è‡ªæˆ‘ä¸­å¿ƒä¸å¤–éƒ¨ä¸­å¿ƒè§†é¢‘å¯¹åŠäººå·¥ç²¾ç‚¼çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œé‡ç‚¹è¯„ä¼°æ—¶åºéªŒè¯å’Œæ—¶åºå®šä½ä¸¤ä¸ªä»»åŠ¡ã€‚æå‡ºäº†View-GRPOå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåŠ å¼ºäº†è§†è§’ç‰¹å®šçš„æ—¶åºæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ƒè¿›äº†è·¨è§†è§’çš„ä¸€è‡´æ€§ç†è§£ã€‚</p>
<p><strong>Result:</strong> åˆ†ææ­ç¤ºäº†ç°æœ‰è§†é¢‘-å¤§è¯­è¨€æ¨¡å‹çš„ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šæ¨¡å‹å¾€å¾€æ— æ³•ä¿æŒä¸€è‡´æ€§ï¼Œå…¶è¡¨ç°è¿œä½äºå•è§†è§’æ€§èƒ½ï¼›å½“ä½¿ç”¨åŒè§†è§’åŒæ­¥è§†é¢‘è¿›è¡Œç®€å•å¾®è°ƒæ—¶ï¼Œæ¨¡å‹è™½ç„¶ä¸€è‡´æ€§æœ‰æ‰€æ”¹å–„ï¼Œä½†é€šå¸¸è¡¨ç°ä¸å¦‚å•è§†è§’è®­ç»ƒçš„æ¨¡å‹ã€‚View-GRPOæ–¹æ³•åœ¨æ”¹å–„è·¨è§†è§’ä¸€è‡´æ€§æ–¹é¢ä¼˜äºæœ´ç´ SFTå’ŒGRPOæ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜è·¨è§†è§’æ—¶åºç†è§£ä¸€è‡´æ€§æ˜¯è§†é¢‘-å¤§è¯­è¨€æ¨¡å‹çš„é‡è¦æŒ‘æˆ˜ï¼Œéœ€è¦ä¸“é—¨è®¾è®¡çš„è®­ç»ƒæ–¹æ³•ã€‚View-GRPOæ¡†æ¶ä¸ºè§£å†³å¤šè§†è§’ä¸€è‡´æ€§æ¨ç†é—®é¢˜æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œå¼ºè°ƒäº†åœ¨è§†é¢‘ç†è§£ä¸­è€ƒè™‘è§†è§’å·®å¼‚çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€æ¨¡å‹çš„ä¸€è‡´æ€§è¯„ä¼°å’Œæ”¹è¿›æä¾›äº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.</p>
<h3 id="5-generative-image-restoration-and-super-resolution-using-physics-informed-synthetic-data-for-scanning-tunneling-microscopy">[5] <a href="https://arxiv.org/abs/2510.25921">Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy</a></h3>
<p><em>Nikola L. Kolev, Tommaso Rodani, Neil J. Curson, Taylor J. Z. Stock, Alberto Cazzaniga</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„æ‰«æéš§é“æ˜¾å¾®é•œå›¾åƒä¿®å¤ä¸è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œé€šè¿‡ç‰©ç†ä¿¡æ¯åˆæˆæ•°æ®ç”Ÿæˆç®¡é“è®­ç»ƒå…ˆè¿›çš„æµåŒ¹é…å’Œæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿®å¤å›¾åƒè´¨é‡å¹¶å®ç°2-4å€çš„å›¾åƒé‡‡é›†æ—¶é—´å‡å°‘ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ‰«æéš§é“æ˜¾å¾®é•œåœ¨åŸå­åˆ†è¾¨ç‡æˆåƒå’ŒåŸå­æ“çºµæ–¹é¢å…·æœ‰é‡è¦åº”ç”¨ï¼Œä½†å…¶å®é™…æ•ˆç”¨å¸¸å—é™äºé’ˆå°–é€€åŒ–å’Œç¼“æ…¢çš„ä¸²è¡Œæ•°æ®é‡‡é›†è¿‡ç¨‹ï¼ŒåŒæ—¶é’ˆå°–åˆ¶å¤‡è¿‡ç¨‹ä¸­æ–½åŠ çš„é«˜ç”µå‹ä¼šæ”¹å˜é’ˆå°–å°–ç«¯å½¢çŠ¶ï¼Œéœ€è¦é¢‘ç¹è¿›è¡Œé’ˆå°–è°ƒèŠ‚å¤„ç†ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨ç‰©ç†ä¿¡æ¯åˆæˆæ•°æ®ç”Ÿæˆç®¡é“ï¼Œä»…ä½¿ç”¨36å¼ åŸå§‹å®éªŒå›¾åƒä½œä¸ºåŸºç¡€æ•°æ®é›†ï¼Œè®­ç»ƒäº†å¤šç§å…ˆè¿›çš„æµåŒ¹é…å’Œæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡CLIPæœ€å¤§å‡å€¼å·®å¼‚å¾—åˆ†å’Œç»“æ„ç›¸ä¼¼æ€§ç­‰å®šé‡æŒ‡æ ‡è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆä¿®å¤å›¾åƒè´¨é‡ï¼Œé€šè¿‡ä»ç¨€ç–é‡‡æ ·æ•°æ®ä¸­å‡†ç¡®é‡å»ºå›¾åƒï¼Œå®ç°äº†2-4å€çš„å›¾åƒé‡‡é›†æ—¶é—´å‡å°‘ï¼Œæ˜¾è‘—æå‡äº†æ‰«æéš§é“æ˜¾å¾®é•œçš„å®éªŒé€šé‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ¡†æ¶é€šè¿‡å‡å°‘é’ˆå°–è°ƒèŠ‚è¿‡ç¨‹çš„é¢‘ç‡å’Œå¢å¼ºç°æœ‰é«˜é€ŸSTMç³»ç»Ÿçš„å¸§ç‡ï¼Œæœ‰æœ›æ˜¾è‘—æé«˜æ‰«æéš§é“æ˜¾å¾®é•œçš„å®éªŒé€šé‡ï¼Œä¸ºåŸå­å°ºåº¦æˆåƒå’Œæ“çºµæä¾›äº†æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and
atom manipulation, but its utility is often limited by tip degradation and slow
serial data acquisition. Fabrication adds another layer of complexity since the
tip is often subjected to large voltages, which may alter the shape of its
apex, requiring it to be conditioned. Here, we propose a machine learning (ML)
approach for image repair and super-resolution to alleviate both challenges.
Using a dataset of only 36 pristine experimental images of Si(001):H, we
demonstrate that a physics-informed synthetic data generation pipeline can be
used to train several state-of-the-art flow-matching and diffusion models.
Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy
(CMMD) score and structural similarity demonstrates that our models are able to
effectively restore images and offer a two- to fourfold reduction in image
acquisition time by accurately reconstructing images from sparsely sampled
data. Our framework has the potential to significantly increase STM
experimental throughput by offering a route to reducing the frequency of
tip-conditioning procedures and to enhancing frame rates in existing high-speed
STM systems.</p>
<h3 id="6-wod-e2e-waymo-open-dataset-for-end-to-end-driving-in-challenging-long-tail-scenarios">[6] <a href="https://arxiv.org/abs/2510.26125">WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios</a></h3>
<p><em>Runsheng Xu, Hubert Lin, Wonseok Jeon, Hao Feng, Yuliang Zou, Liting Sun, John Gorman, Kate Tolstaya, Sarah Tang, Brandyn White, Ben Sapp, Mingxing Tan, Jyh-Jing Hwang, Drago Anguelov</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†WOD-E2Eæ•°æ®é›†ï¼Œä¸“é—¨é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­ç½•è§çš„é•¿å°¾åœºæ™¯ï¼Œå¹¶å¼•å…¥äº†åŸºäºäººç±»è¯„åˆ†è€…åå¥½çš„æ–°å‹å¼€ç¯è¯„ä¼°æŒ‡æ ‡RFSï¼Œæ—¨åœ¨æ¨åŠ¨ç«¯åˆ°ç«¯é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„é²æ£’æ€§ç ”ç©¶ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç«¯åˆ°ç«¯é©¾é©¶åŸºå‡†ä¸»è¦å…³æ³¨å¸¸è§„åœºæ™¯ï¼Œæ— æ³•å……åˆ†æµ‹è¯•ç³»ç»Ÿåœ¨ç½•è§é•¿å°¾åœºæ™¯ä¸­çš„çœŸå®æ½œåŠ›ï¼Œä¸”ç°æœ‰å¼€ç¯è¯„ä¼°æŒ‡æ ‡éš¾ä»¥æœ‰æ•ˆè¯„ä¼°é©¾é©¶çš„å¤šæ¨¡æ€ç‰¹æ€§æˆ–åœ¨é•¿å°¾åœºæ™¯ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†åŒ…å«4,021ä¸ªé©¾é©¶ç‰‡æ®µï¼ˆçº¦12å°æ—¶ï¼‰çš„WOD-E2Eæ•°æ®é›†ï¼Œä¸“é—¨é’ˆå¯¹å‘ç”Ÿé¢‘ç‡ä½äº0.03%çš„æŒ‘æˆ˜æ€§é•¿å°¾åœºæ™¯ï¼Œæ¯ä¸ªç‰‡æ®µåŒ…å«é«˜çº§è·¯ç”±ä¿¡æ¯ã€è‡ªè½¦çŠ¶æ€å’Œ8ä¸ªç¯è§†æ‘„åƒå¤´æ•°æ®ï¼Œå¹¶æå‡ºäº†åŸºäºè¯„åˆ†è€…è½¨è¿¹åå¥½æ ‡æ³¨çš„æ–°å‹è¯„ä¼°æŒ‡æ ‡RFSã€‚</p>
<p><strong>Result:</strong> WOD-E2Eæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒéªŒè¯é›†çš„è¯„åˆ†è€…åå¥½æ ‡ç­¾ï¼Œæµ‹è¯•é›†æ ‡ç­¾ç”¨äº2025å¹´WOD-E2EæŒ‘æˆ˜èµ›ï¼Œè¯¥æ•°æ®é›†å’Œè¯„ä¼°æ–¹æ³•ä¸ºç«¯åˆ°ç«¯é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚é•¿å°¾åœºæ™¯ä¸­çš„æ€§èƒ½è¯„ä¼°æä¾›äº†æ ‡å‡†åŒ–åŸºå‡†ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ä¸“é—¨çš„é•¿å°¾åœºæ™¯æ•°æ®é›†å’ŒåŸºäºäººç±»åå¥½çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºå¼€å‘é€šç”¨æ€§å¼ºã€é²æ£’æ€§é«˜ä¸”å®‰å…¨çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†é‡è¦åŸºç¡€ï¼Œå°†æ¨åŠ¨è‡ªåŠ¨é©¾é©¶åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„ç ”ç©¶è¿›å±•ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Vision-based end-to-end (E2E) driving has garnered significant interest in
the research community due to its scalability and synergy with multimodal large
language models (MLLMs). However, current E2E driving benchmarks primarily
feature nominal scenarios, failing to adequately test the true potential of
these systems. Furthermore, existing open-loop evaluation metrics often fall
short in capturing the multi-modal nature of driving or effectively evaluating
performance in long-tail scenarios. To address these gaps, we introduce the
Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021
driving segments (approximately 12 hours), specifically curated for challenging
long-tail scenarios that that are rare in daily life with an occurring
frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the
high-level routing information, ego states, and 360-degree camera views from 8
surrounding cameras. To evaluate the E2E driving performance on these long-tail
situations, we propose a novel open-loop evaluation metric: Rater Feedback
Score (RFS). Unlike conventional metrics that measure the distance between
predicted way points and the logs, RFS measures how closely the predicted
trajectory matches rater-annotated trajectory preference labels. We have
released rater preference labels for all WOD-E2E validation set segments, while
the held out test set labels have been used for the 2025 WOD-E2E Challenge.
Through our work, we aim to foster state of the art research into
generalizable, robust, and safe end-to-end autonomous driving agents capable of
handling complex real-world situations.</p>
<h3 id="7-splitflow-flow-decomposition-for-inversion-free-text-to-image-editing">[7] <a href="https://arxiv.org/abs/2510.25970">SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing</a></h3>
<p><em>Sung-Hoon Yoon, Minghan Li, Gaspard Beaudouin, Congcong Wen, Muhammad Rafay Azhar, Mengyu Wang</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæµåˆ†è§£ä¸èšåˆçš„å…åæ¼”å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰åˆ†è§£ç›®æ ‡æç¤ºå¹¶è‡ªé€‚åº”èšåˆå­æµæ¥è§£å†³æ•´æµæµæ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­çš„åæ¼”ä¸å‡†ç¡®å’Œæ¢¯åº¦çº ç¼ é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨è¯­ä¹‰ä¿çœŸåº¦å’Œå±æ€§è§£è€¦æ–¹é¢ä¼˜äºç°æœ‰é›¶æ ·æœ¬ç¼–è¾‘æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ•´æµæµæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­å­˜åœ¨å…³é”®é™åˆ¶ï¼šå°†çœŸå®å›¾åƒæ˜ å°„å›æ½œåœ¨ç©ºé—´çš„åæ¼”è¿‡ç¨‹ä¸å‡†ç¡®ï¼Œä»¥åŠç¼–è¾‘è¿‡ç¨‹ä¸­çš„æ¢¯åº¦çº ç¼ é—®é¢˜å¯¼è‡´è¾“å‡ºæ— æ³•å¿ å®åæ˜ ç›®æ ‡æç¤ºã€‚ç°æœ‰åŸºäºODEçš„æ–¹æ³•è™½ç„¶å°è¯•ç»•è¿‡åæ¼”ç›´æ¥æ˜ å°„æºå’Œç›®æ ‡åˆ†å¸ƒï¼Œä½†ä»äº§ç”Ÿæ¬¡ä¼˜çš„ç¼–è¾‘è´¨é‡ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŸºäºå…åæ¼”å…¬å¼çš„æµåˆ†è§£ä¸èšåˆæ¡†æ¶ï¼Œå°†ç›®æ ‡æç¤ºè¯­ä¹‰åˆ†è§£ä¸ºå¤šä¸ªå­æç¤ºï¼Œä¸ºæ¯ä¸ªå­æç¤ºè®¡ç®—ç‹¬ç«‹æµï¼Œå¹¶é€šè¿‡æŠ•å½±å’Œè½¯èšåˆæœºåˆ¶è‡ªé€‚åº”åŠ æƒå­ç›®æ ‡é€Ÿåº¦åœºã€‚è¯¥æœºåˆ¶å—å¤šä»»åŠ¡å­¦ä¹ ä¸­æ¢¯åº¦å†²çªè§£å†³çš„å¯å‘ï¼ŒæŠ‘åˆ¶è¯­ä¹‰å†—ä½™åŒæ—¶å¼ºè°ƒä¸åŒæ–¹å‘ï¼Œä¿æŒç¼–è¾‘è¾“å‡ºçš„å¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯­ä¹‰ä¿çœŸåº¦å’Œå±æ€§è§£è€¦æ–¹é¢ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬ç¼–è¾‘æ–¹æ³•ã€‚æµåˆ†è§£å¢å¼ºäº†ç›®æ ‡ç©ºé—´çš„å¤šæ ·æ€§ï¼Œè€Œè½¯èšåˆæœºåˆ¶ç¡®ä¿ç”Ÿæˆè¯­ä¹‰å¯¹é½çš„è¾“å‡ºï¼ŒåŒæ—¶ä¿æŒå¯¹å®Œæ•´ç›®æ ‡æç¤ºçš„ä¸€è‡´å¼•å¯¼ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è¯­ä¹‰åˆ†è§£å’Œè‡ªé€‚åº”æµèšåˆå¯ä»¥æœ‰æ•ˆè§£å†³æ•´æµæµæ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­çš„å±€é™æ€§ï¼Œä¸ºå¤æ‚ç¼–è¾‘ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚æ¡†æ¶è®¾è®¡çµæ„Ÿæ¥è‡ªå¤šä»»åŠ¡å­¦ä¹ ï¼Œå±•ç¤ºäº†è·¨é¢†åŸŸæ–¹æ³•åœ¨ç”Ÿæˆæ¨¡å‹ä¼˜åŒ–ä¸­çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥ç¼–è¾‘æ–¹æ³•çš„å‘å±•æä¾›äº†é‡è¦è§è§£ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Rectified flow models have become a de facto standard in image generation due
to their stable sampling trajectories and high-fidelity outputs. Despite their
strong generative capabilities, they face critical limitations in image editing
tasks: inaccurate inversion processes for mapping real images back into the
latent space, and gradient entanglement issues during editing often result in
outputs that do not faithfully reflect the target prompt. Recent efforts have
attempted to directly map source and target distributions via ODE-based
approaches without inversion; however,these methods still yield suboptimal
editing quality. In this work, we propose a flow decomposition-and-aggregation
framework built upon an inversion-free formulation to address these
limitations. Specifically, we semantically decompose the target prompt into
multiple sub-prompts, compute an independent flow for each, and aggregate them
to form a unified editing trajectory. While we empirically observe that
decomposing the original flow enhances diversity in the target space,
generating semantically aligned outputs still requires consistent guidance
toward the full target prompt. To this end, we design a projection and
soft-aggregation mechanism for flow, inspired by gradient conflict resolution
in multi-task learning. This approach adaptively weights the sub-target
velocity fields, suppressing semantic redundancy while emphasizing distinct
directions, thereby preserving both diversity and consistency in the final
edited output. Experimental results demonstrate that our method outperforms
existing zero-shot editing approaches in terms of semantic fidelity and
attribute disentanglement. The code is available at
https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.</p>
<h3 id="8-mv-mlm-bridging-multi-view-mammography-and-language-for-breast-cancer-diagnosis-and-risk-prediction">[8] <a href="https://arxiv.org/abs/2510.26151">MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction</a></h3>
<p><em>Shunjie-Fabian Zheng, Hyeonjun Lee, Thijs Kooi, Ali Diba</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šè§†è§’ä¹³è…ºXçº¿æ‘„å½±ä¸è¯­è¨€æ¨¡å‹ï¼ˆMV-MLMï¼‰ï¼Œé€šè¿‡åˆ©ç”¨é…å¯¹ä¹³è…ºXçº¿å›¾åƒå’Œåˆæˆæ”¾å°„å­¦æŠ¥å‘Šè¿›è¡Œè·¨æ¨¡æ€è‡ªç›‘ç£å­¦ä¹ ï¼Œåœ¨ä¹³è…ºç™Œåˆ†ç±»å’Œé£é™©é¢„æµ‹ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿä¾èµ–å¤§é‡ç²¾ç»†æ ‡æ³¨æ•°æ®ï¼Œä½†è·å–æ­¤ç±»æ•°æ®æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ï¼Œè€ŒåŸºäºå¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬å¯¹é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºè§£å†³åŒ»å­¦å½±åƒä»»åŠ¡ä¸­çš„æ•°æ®æ•ˆç‡é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é‡‡ç”¨å¤šè§†è§’ç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡è·¨æ¨¡æ€è‡ªç›‘ç£åœ¨å›¾åƒ-æ–‡æœ¬å¯¹ä¸Šè¿›è¡Œè”åˆè§†è§‰-æ–‡æœ¬å­¦ä¹ ï¼Œåˆ©ç”¨å¤šä¸ªè§†è§’å’Œç›¸åº”çš„ä¼ªæ”¾å°„å­¦æŠ¥å‘Šæ¥å­¦ä¹ ä¸°å¯Œçš„è¡¨ç¤ºï¼Œä»è€ŒåŒºåˆ†ä¹³è…ºç»„ç»‡æˆ–ç™Œç—‡ç‰¹å¾ï¼ˆé’™åŒ–ã€è‚¿å—ï¼‰å¹¶åˆ©ç”¨è¿™äº›æ¨¡å¼é¢„æµ‹ç™Œç—‡é£é™©ã€‚</p>
<p><strong>Result:</strong> åœ¨ç§æœ‰å’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸‰ä¸ªåˆ†ç±»ä»»åŠ¡ä¸­å‡è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ï¼šæ¶æ€§åˆ†ç±»ã€äºšå‹åˆ†ç±»å’ŒåŸºäºå›¾åƒçš„ç™Œç—‡é£é™©é¢„æµ‹ï¼ŒåŒæ—¶å±•ç°å‡ºå¼ºå¤§çš„æ•°æ®æ•ˆç‡ï¼Œåœ¨ä»…ä½¿ç”¨åˆæˆæ–‡æœ¬æŠ¥å‘Šè®­ç»ƒçš„æƒ…å†µä¸‹è¶…è¶Šäº†ç°æœ‰å…¨ç›‘ç£æˆ–è§†è§‰è¯­è¨€æ¨¡å‹åŸºçº¿ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†å¤šè§†è§’è·¨æ¨¡æ€å­¦ä¹ åœ¨åŒ»å­¦å½±åƒåˆ†æä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå‡å°‘å¯¹æ˜‚è´µäººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼ŒåŒæ—¶å±•ç¤ºäº†åˆæˆæ–‡æœ¬æ•°æ®åœ¨æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œå‡†ç¡®æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.</p>
<h3 id="9-cave-detecting-and-explaining-commonsense-anomalies-in-visual-environments">[9] <a href="https://arxiv.org/abs/2510.26006">CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments</a></h3>
<p><em>Rishika Bhagwatkar, Syrielle Montariol, Angelika Romanou, Beatriz Borges, Irina Rish, Antoine Bosselut</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CAVEï¼Œé¦–ä¸ªçœŸå®ä¸–ç•Œè§†è§‰å¼‚å¸¸åŸºå‡†ï¼Œæ”¯æŒå¼‚å¸¸æè¿°ã€è§£é‡Šå’Œè®ºè¯ä¸‰ä¸ªå¼€æ”¾ä»»åŠ¡ï¼Œä¸ºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹å’Œå¸¸è¯†æ¨ç†èƒ½åŠ›æ–¹é¢æä¾›äº†è®¤çŸ¥ç§‘å­¦å¯å‘çš„ç»¼åˆæ¡†æ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è®¡ç®—æœºè§†è§‰ä¸­çš„å¼‚å¸¸æ£€æµ‹ä¸»è¦å±€é™äºå·¥ä¸šç¼ºé™·æˆ–åˆæˆç”Ÿæˆçš„å¼‚å¸¸ï¼Œæ— æ³•æ•æ‰çœŸå®ä¸–ç•Œå¼‚å¸¸çš„ä¸°å¯Œæ€§å’Œä¸å¯é¢„æµ‹æ€§ï¼Œè€Œäººç±»å´èƒ½è‡ªç„¶åœ°è¯†åˆ«ã€æ¨ç†å’Œè§£é‡Šç¯å¢ƒä¸­çš„å¼‚å¸¸ç°è±¡ã€‚</p>
<p><strong>Method:</strong> CAVEåŸºå‡†å¼•å…¥äº†åŸºäºè®¤çŸ¥ç§‘å­¦ç ”ç©¶å¯å‘çš„ç»†ç²’åº¦æ ‡æ³¨æ¡†æ¶ï¼ŒåŒ…æ‹¬è§†è§‰å®šä½å’ŒåŸºäºè§†è§‰è¡¨ç°ã€å¤æ‚æ€§ã€ä¸¥é‡æ€§å’Œå¸¸è§æ€§çš„å¼‚å¸¸åˆ†ç±»ï¼Œæ”¯æŒå¼‚å¸¸æè¿°ã€è§£é‡Šå’Œè®ºè¯ä¸‰ä¸ªå¼€æ”¾ä»»åŠ¡ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œå³ä½¿é‡‡ç”¨å…ˆè¿›çš„æç¤ºç­–ç•¥ï¼Œæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰å¼‚å¸¸æ„ŸçŸ¥å’Œå¸¸è¯†æ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å›°éš¾ï¼Œçªæ˜¾äº†å½“å‰æ¨¡å‹åœ¨ç†è§£çœŸå®ä¸–ç•Œå¼‚å¸¸æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<p><strong>Conclusion:</strong> CAVEä½œä¸ºç°å®ä¸”è®¤çŸ¥åŸºç¡€æ‰å®çš„åŸºå‡†ï¼Œä¸ºæ¨è¿›å¼‚å¸¸æ£€æµ‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¸¸è¯†æ¨ç†ç ”ç©¶æä¾›äº†å®è´µèµ„æºï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå¼‚å¸¸ç†è§£æ–¹é¢çš„ä¸è¶³å’Œæ”¹è¿›æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>Humans can naturally identify, reason about, and explain anomalies in their
environment. In computer vision, this long-standing challenge remains limited
to industrial defects or unrealistic, synthetically generated anomalies,
failing to capture the richness and unpredictability of real-world anomalies.
In this work, we introduce CAVE, the first benchmark of real-world visual
anomalies. CAVE supports three open-ended tasks: anomaly description,
explanation, and justification; with fine-grained annotations for visual
grounding and categorizing anomalies based on their visual manifestations,
their complexity, severity, and commonness. These annotations draw inspiration
from cognitive science research on how humans identify and resolve anomalies,
providing a comprehensive framework for evaluating Vision-Language Models
(VLMs) in detecting and understanding anomalies. We show that state-of-the-art
VLMs struggle with visual anomaly perception and commonsense reasoning, even
with advanced prompting strategies. By offering a realistic and cognitively
grounded benchmark, CAVE serves as a valuable resource for advancing research
in anomaly detection and commonsense reasoning in VLMs.</p>
<h3 id="10-glyph-sr-can-we-achieve-both-high-quality-image-super-resolution-and-high-fidelity-text-recovery-via-vlm-guided-latent-diffusion-model">[10] <a href="https://arxiv.org/abs/2510.26339">GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?</a></h3>
<p><em>Mingyu Sung, Seungjae Ham, Kangwoo Kim, Yeokyoung Yoon, Sangseok Yun, Il-Min Kim, Jae-Mo Kang</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>GLYPH-SRæå‡ºäº†ä¸€ç§è§†è§‰è¯­è¨€å¼•å¯¼çš„æ‰©æ•£æ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹åœºæ™¯æ–‡æœ¬è¶…åˆ†è¾¨ç‡é—®é¢˜ï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬å¯è¯»æ€§å’Œæ„ŸçŸ¥è´¨é‡ä¼˜åŒ–ï¼Œåœ¨ä¿æŒé«˜è§†è§‰çœŸå®æ€§çš„åŒæ—¶æ˜¾è‘—æå‡OCRæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è¶…åˆ†è¾¨ç‡ç ”ç©¶ä¸»è¦é’ˆå¯¹å¤±çœŸæŒ‡æ ‡ï¼ˆPSNR/SSIMï¼‰æˆ–æ„ŸçŸ¥è´¨é‡æŒ‡æ ‡è¿›è¡Œä¼˜åŒ–ï¼Œä½†è¿™äº›æŒ‡æ ‡å¯¹å­—ç¬¦çº§é”™è¯¯ä¸æ•æ„Ÿï¼Œå¯¼è‡´åœºæ™¯æ–‡æœ¬ï¼ˆå¦‚æ ‡å¿—ã€äº§å“æ ‡ç­¾ä¸­çš„æ–‡å­—ï¼‰åœ¨è¶…åˆ†è¾¨ç‡åä»éš¾ä»¥è¢«OCRç³»ç»Ÿå‡†ç¡®è¯†åˆ«ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨æ•ˆæœã€‚</p>
<p><strong>Method:</strong> GLYPH-SRé‡‡ç”¨åŸºäºOCRæ•°æ®çš„æ–‡æœ¬è¶…åˆ†è¾¨ç‡èåˆæ§åˆ¶ç½‘ç»œï¼ˆTS-ControlNetï¼‰å’Œä¹’ä¹“è°ƒåº¦å™¨ï¼Œåœ¨æ–‡æœ¬å¯¼å‘å’Œåœºæ™¯å¯¼å‘ä¹‹é—´äº¤æ›¿å¼•å¯¼ï¼Œé€šè¿‡åœ¨åˆæˆè¯­æ–™ä¸Šè®­ç»ƒè¿™äº›ç»„ä»¶åŒæ—¶ä¿æŒä¸»è¶…åˆ†è¾¨ç‡åˆ†æ”¯å†»ç»“ï¼Œå®ç°é’ˆå¯¹æ€§æ–‡æœ¬æ¢å¤ã€‚</p>
<p><strong>Result:</strong> åœ¨SVTã€SCUT-CTW1500å’ŒCUTE80æ•°æ®é›†ä¸Šçš„x4å’Œx8è¶…åˆ†è¾¨ç‡å®éªŒä¸­ï¼ŒGLYPH-SRç›¸æ¯”æ‰©æ•£/GANåŸºçº¿å°†OCR F1åˆ†æ•°æå‡äº†æœ€é«˜15.18ä¸ªç™¾åˆ†ç‚¹ï¼ˆSVT x8, OpenOCRï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„MANIQAã€CLIP-IQAå’ŒMUSIQæ„ŸçŸ¥è´¨é‡åˆ†æ•°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜è¶…åˆ†è¾¨ç‡ç³»ç»Ÿéœ€è¦åŒæ—¶ä¼˜åŒ–æ–‡æœ¬å¯è¯»æ€§å’Œè§†è§‰çœŸå®æ€§ï¼ŒGLYPH-SRæ¡†æ¶è¯æ˜äº†é€šè¿‡é’ˆå¯¹æ€§è®¾è®¡å¯ä»¥å®ç°æ—¢çœ‹èµ·æ¥æ­£ç¡®åˆè¯»èµ·æ¥æ­£ç¡®çš„è¶…åˆ†è¾¨ç‡æ•ˆæœï¼Œä¸ºå®é™…éƒ¨ç½²æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.</p>
<h3 id="11-enhancing-temporal-understanding-in-video-llms-through-stacked-temporal-attention-in-vision-encoders">[11] <a href="https://arxiv.org/abs/2510.26027">Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders</a></h3>
<p><em>Ali Rasekh, Erfan Bagheri Soula, Omid Daliran, Simon Gottschalk, Mohsen Fayyaz</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨è§†è§‰ç¼–ç å™¨ä¸­å¼•å…¥å †å æ—¶åºæ³¨æ„åŠ›æ¨¡å—çš„Video-LLMæ¶æ„ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘æ—¶åºç†è§£èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€é«˜+5.5%çš„æ€§èƒ½æå‡ï¼Œè§£å†³äº†å½“å‰è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶åºåŠ¨æ€ç†è§£æ–¹é¢çš„å…³é”®é™åˆ¶ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ—¶åºåŠ¨æ€ç†è§£ä¸Šè¡¨ç°ä¸è¶³ã€‚å®éªŒè¡¨æ˜ç°æœ‰Video-LLMæ¶æ„åœ¨éœ€è¦è¯¦ç»†ç†è§£åŠ¨ä½œåºåˆ—å’Œæ—¶é—´è¿›å±•çš„ä»»åŠ¡ä¸­å­˜åœ¨å…³é”®é™åˆ¶ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰å¸§é—´å…³ç³»å’ŒåŠ¨ä½œæ¼”è¿›è¿‡ç¨‹ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§æ–°é¢–çš„Video-LLMæ¶æ„ï¼Œåœ¨è§†è§‰ç¼–ç å™¨ä¸­ç›´æ¥å¼•å…¥å †å æ—¶åºæ³¨æ„åŠ›æ¨¡å—ã€‚è¯¥è®¾è®¡é€šè¿‡åœ¨è§†è§‰ç¼–ç å™¨ä¸­é›†æˆæ—¶åºæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å°†è§†è§‰ä»¤ç‰Œä¼ é€’ç»™LLMä¹‹å‰æ›´å¥½åœ°æ•æ‰åŠ¨ä½œè¿›å±•å’Œå¸§é—´å…³ç³»ï¼Œä»è€Œå¢å¼ºæ—¶åºæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œåœ¨VITATECSã€MVBenchå’ŒVideo-MMEç­‰åŸºå‡†ä¸Šå®ç°äº†æœ€é«˜+5.5%çš„æ”¹è¿›ã€‚ç‰¹åˆ«æ˜¯åœ¨åŠ¨ä½œè¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰æ¨¡å‹çš„æ—¶åºç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡å¢å¼ºè§†è§‰ç¼–ç å™¨çš„æ—¶åºç»“æ„ï¼Œæœ¬ç ”ç©¶è§£å†³äº†Video-LLMåœ¨è§†é¢‘ç†è§£ä¸­çš„å…³é”®ç©ºç™½ã€‚è¯¥å·¥ä½œè¡¨æ˜ç›´æ¥åœ¨è§†è§‰ç¼–ç å™¨ä¸­é›†æˆæ—¶åºæ³¨æ„åŠ›æ˜¯æå‡è§†é¢‘æ—¶åºæ¨ç†çš„æœ‰æ•ˆé€”å¾„ï¼Œä¸ºæœªæ¥è§†é¢‘ç†è§£æ¨¡å‹çš„è®¾è®¡æä¾›äº†é‡è¦å¯ç¤ºã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Despite significant advances in Multimodal Large Language Models (MLLMs),
understanding complex temporal dynamics in videos remains a major challenge.
Our experiments show that current Video Large Language Model (Video-LLM)
architectures have critical limitations in temporal understanding, struggling
with tasks that require detailed comprehension of action sequences and temporal
progression. In this work, we propose a Video-LLM architecture that introduces
stacked temporal attention modules directly within the vision encoder. This
design incorporates a temporal attention in vision encoder, enabling the model
to better capture the progression of actions and the relationships between
frames before passing visual tokens to the LLM. Our results show that this
approach significantly improves temporal reasoning and outperforms existing
models in video question answering tasks, specifically in action recognition.
We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to
+5.5%. By enhancing the vision encoder with temporal structure, we address a
critical gap in video understanding for Video-LLMs. Project page and code are
available at: https://alirasekh.github.io/STAVEQ2/.</p>
<h3 id="12-which-way-does-time-flow-a-psychophysics-grounded-evaluation-for-vision-language-models">[12] <a href="https://arxiv.org/abs/2510.26241">Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models</a></h3>
<p><em>Shiho Matta, Lis Kanashiro Pereira, Peitao Han, Fei Cheng, Shigeru Kitazawa</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´æ¨ç†æ–¹é¢çš„æ ¹æœ¬ç¼ºé™·ï¼Œé€šè¿‡å¼•å…¥AoT-PsyPhyBENCHåŸºå‡†æµ‹è¯•å‘ç°å¤§å¤šæ•°æ¨¡å‹åœ¨åˆ¤æ–­è§†é¢‘æ—¶é—´æ–¹å‘ä»»åŠ¡ä¸Šè¡¨ç°æ¥è¿‘éšæœºæ°´å¹³ï¼Œè¿œè½åäºäººç±»è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹è§†é¢‘ä¸­æ—¶é—´ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ä»ç„¶è–„å¼±ä¸”ç¼ºä¹å……åˆ†è¯„ä¼°ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œæ¢ç´¢æ¨¡å‹å¯¹æ—¶é—´æ–¹å‘åˆ¤æ–­çš„åŸºæœ¬èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼•å…¥äº†AoT-PsyPhyBENCHåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡å¿ƒç†ç‰©ç†å­¦éªŒè¯çš„è¯„ä¼°æ¡†æ¶ï¼Œä½¿ç”¨ä¸äººç±»è¡Œä¸ºåŸºå‡†ç›¸åŒçš„åˆºæ¿€ææ–™æ¥æµ‹è¯•VLMså¯¹è‡ªç„¶è§†é¢‘ä¸­æ—¶é—´æ–¹å‘çš„æ¨æ–­èƒ½åŠ›ï¼Œå…¨é¢è¯„ä¼°äº†å¼€æºå’Œä¸“æœ‰ã€æ¨ç†å’Œéæ¨ç†ç±»å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºå¤§å¤šæ•°æ¨¡å‹åœ¨æ—¶é—´æ–¹å‘åˆ¤æ–­ä»»åŠ¡ä¸Šè¡¨ç°æ¥è¿‘éšæœºæ°´å¹³ï¼Œå³ä½¿åœ¨ç‰©ç†ä¸å¯é€†è¿‡ç¨‹ï¼ˆå¦‚è‡ªç”±è½ä½“ã€æ‰©æ•£/çˆ†ç‚¸ï¼‰å’Œå› æœæ‰‹åŠ¨åŠ¨ä½œï¼ˆé™¤æ³•/åŠ æ³•ï¼‰ç­‰äººç±»å‡ ä¹èƒ½ç¬é—´è¯†åˆ«çš„ä»»åŠ¡ä¸Šï¼Œæœ€ä½³æ¨¡å‹çš„è¡¨ç°ä¹Ÿè¿œè¿œè½åäºäººç±»å‡†ç¡®ç‡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€ç³»ç»Ÿå­˜åœ¨æ ¹æœ¬æ€§å·®è·ï¼šè™½ç„¶å®ƒä»¬èƒ½å¤Ÿæ•æ‰ä¸°å¯Œçš„è§†è§‰è¯­ä¹‰å…³è”ï¼Œä½†ç¼ºä¹æ—¶é—´è¿ç»­æ€§å’Œå› æœç†è§£æ‰€éœ€çš„å½’çº³åç½®ï¼Œè¿™ä¸ºå¼€å‘å…·æœ‰ç‰©ç†å’Œæ—¶é—´æ¨ç†èƒ½åŠ›çš„ä¸‹ä¸€ä»£VLMsæŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.</p>
<h3 id="13-locot2v-bench-a-benchmark-for-long-form-and-complex-text-to-video-generation">[13] <a href="https://arxiv.org/abs/2510.26412">LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation</a></h3>
<p><em>Xiangqing Zheng, Chengyue Wu, Kehai Chen, Min Zhang</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†LoCoT2V-Benchï¼Œä¸€ä¸ªä¸“é—¨é’ˆå¯¹å¤æ‚è¾“å…¥æ¡ä»¶ä¸‹é•¿è§†é¢‘ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å¼•å…¥å¤šç»´åº¦è¯„ä¼°æ¡†æ¶å’Œæ–°é¢–çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç³»ç»Ÿè¯„ä¼°äº†å½“å‰é•¿è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å™äº‹è¿è´¯æ€§å’Œä¸»é¢˜è¡¨è¾¾ç­‰æŠ½è±¡ç»´åº¦ä¸Šçš„è¡¨ç°ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆè™½ç„¶åœ¨ç”ŸæˆçŸ­é«˜è´¨é‡è§†é¢‘æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†é•¿è§†é¢‘ç”Ÿæˆè¯„ä¼°ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•å¤§å¤šä¾èµ–ç®€åŒ–æç¤ºå¹¶å…³æ³¨ä½å±‚æ¬¡æŒ‡æ ‡ï¼Œå¿½è§†äº†ä¸æç¤ºçš„ç»†ç²’åº¦å¯¹é½ä»¥åŠå™äº‹è¿è´¯æ€§ã€ä¸»é¢˜è¡¨è¾¾ç­‰æŠ½è±¡ç»´åº¦ã€‚</p>
<p><strong>Method:</strong> åŸºäºçœŸå®ä¸–ç•Œè§†é¢‘æ„å»ºäº†åŒ…å«åœºæ™¯è½¬æ¢å’Œäº‹ä»¶åŠ¨æ€ç­‰å…ƒç´ çš„ç°å®å¤æ‚æç¤ºé›†ï¼Œå¹¶å»ºç«‹äº†å¤šç»´åº¦è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬æ–°æå‡ºçš„è¯„ä¼°æŒ‡æ ‡å¦‚äº‹ä»¶çº§å¯¹é½ã€ç»†ç²’åº¦æ—¶é—´ä¸€è‡´æ€§ã€å†…å®¹æ¸…æ™°åº¦ä»¥åŠå…³æ³¨å™äº‹æµç¨‹ã€æƒ…æ„Ÿå“åº”å’Œè§’è‰²å‘å±•ç­‰æŠ½è±¡å±æ€§çš„äººç±»æœŸæœ›å®ç°åº¦æŒ‡æ ‡ã€‚</p>
<p><strong>Result:</strong> å¯¹ä¹ä¸ªä»£è¡¨æ€§é•¿è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œå½“å‰æ–¹æ³•åœ¨åŸºæœ¬è§†è§‰å’Œæ—¶é—´æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨äº‹ä»¶é—´ä¸€è‡´æ€§ã€ç»†ç²’åº¦å¯¹é½å’Œé«˜å±‚æ¬¡ä¸»é¢˜éµå¾ªç­‰æ–¹é¢å­˜åœ¨æ˜¾è‘—å›°éš¾ã€‚</p>
<p><strong>Conclusion:</strong> LoCoT2V-Benchä¸ºé•¿å½¢å¼å¤æ‚æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæä¾›äº†å…¨é¢å¯é çš„è¯„ä¼°å¹³å°ï¼Œæ­ç¤ºäº†å½“å‰æ–¹æ³•åœ¨é«˜çº§è¯­ä¹‰ç†è§£æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥æ–¹æ³•æ”¹è¿›æŒ‡æ˜äº†å…³é”®æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨æå‡å™äº‹è¿è´¯æ€§å’Œä¸»é¢˜è¡¨è¾¾èƒ½åŠ›æ–¹é¢ã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.</p>
<h3 id="14-counteracting-matthew-effect-in-self-improvement-of-lvlms-through-head-tail-re-balancing">[14] <a href="https://arxiv.org/abs/2510.26474">Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing</a></h3>
<p><em>Xin Guo, Zhiheng Xi, Yiwen Ding, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è‡ªæ”¹è¿›è¿‡ç¨‹ä¸­å‡ºç°çš„é©¬å¤ªæ•ˆåº”é—®é¢˜ï¼Œæå‡ºäº†åˆ†å¸ƒé‡å¡‘å’Œè½¨è¿¹é‡é‡‡æ ·ä¸¤ç§è§†è§’çš„å››ç§ç­–ç•¥ï¼Œæœ‰æ•ˆå¹³è¡¡äº†ç®€å•ä¸å¤æ‚æ¨ç†ä»»åŠ¡çš„ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªæ”¹è¿›è¿‡ç¨‹ä¸­å­˜åœ¨é©¬å¤ªæ•ˆåº”é—®é¢˜ï¼Œæ¨¡å‹å€¾å‘äºä¸ºç®€å•æŸ¥è¯¢ç”Ÿæˆé«˜è´¨é‡æ¨ç†è½¨è¿¹ï¼Œè€Œéš¾ä»¥å¤„ç†å¤æ‚æŸ¥è¯¢ï¼Œå¯¼è‡´ä¼˜åŒ–å¤±è¡¡å¹¶é˜»ç¢æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„èƒ½åŠ›æå‡ï¼Œæœ€ç»ˆå½¢æˆæ€§èƒ½ç“¶é¢ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä»åˆ†å¸ƒé‡å¡‘å’Œè½¨è¿¹é‡é‡‡æ ·ä¸¤ä¸ªè§’åº¦çš„å››ç§é«˜æ•ˆç­–ç•¥ï¼Œåœ¨æ¢ç´¢å­¦ä¹ çš„è‡ªæ”¹è¿›è¿‡ç¨‹ä¸­å®ç°å¤´å°¾æ•°æ®çš„é‡æ–°å¹³è¡¡ï¼ŒåŒ…æ‹¬è°ƒæ•´æ•°æ®åˆ†å¸ƒæƒé‡å’Œä¼˜åŒ–æ¨ç†è½¨è¿¹é‡‡æ ·æœºåˆ¶ã€‚</p>
<p><strong>Result:</strong> åœ¨Qwen2-VL-7B-Instructå’ŒInternVL2.5-4Bæ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸ŠæŒç»­æå‡æ¨¡å‹èƒ½åŠ›ï¼Œå¹³å‡æ¯”åŸå§‹è‡ªæ”¹è¿›æ–¹æ³•é«˜å‡º3.86ä¸ªç‚¹ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†è‡ªæ”¹è¿›è¿‡ç¨‹ä¸­çš„ä¼˜åŒ–å¤±è¡¡é—®é¢˜ï¼Œæå‡ºçš„å¹³è¡¡ç­–ç•¥æœ‰æ•ˆç¼“è§£äº†é©¬å¤ªæ•ˆåº”ï¼Œä¸ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æŒç»­æ”¹è¿›æä¾›äº†é‡è¦æ–¹æ³•è®ºï¼Œæ¨åŠ¨äº†å¤æ‚æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Self-improvement has emerged as a mainstream paradigm for advancing the
reasoning capabilities of large vision-language models (LVLMs), where models
explore and learn from successful trajectories iteratively. However, we
identify a critical issue during this process: the model excels at generating
high-quality trajectories for simple queries (i.e., head data) but struggles
with more complex ones (i.e., tail data). This leads to an imbalanced
optimization that drives the model to prioritize simple reasoning skills, while
hindering its ability to tackle more complex reasoning tasks. Over iterations,
this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
effect"--which ultimately hinders further model improvement and leads to
performance bottlenecks. To counteract this challenge, we introduce four
efficient strategies from two perspectives: distribution-reshaping and
trajectory-resampling, to achieve head-tail re-balancing during the
exploration-and-learning self-improvement process. Extensive experiments on
Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
demonstrate that our methods consistently improve visual reasoning
capabilities, outperforming vanilla self-improvement by 3.86 points on average.</p>
<h3 id="15-are-video-models-ready-as-zero-shot-reasoners-an-empirical-study-with-the-mme-cof-benchmark">[15] <a href="https://arxiv.org/abs/2510.26802">Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark</a></h3>
<p><em>Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶å¯¹é¢†å…ˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹Veo-3è¿›è¡Œäº†ç³»ç»Ÿæ€§è¯„ä¼°ï¼Œå‘ç°å½“å‰è§†é¢‘æ¨¡å‹åœ¨çŸ­æ—¶ç©ºé—´ä¸€è‡´æ€§å’Œå±€éƒ¨åŠ¨æ€æ¨ç†æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é•¿æ—¶å› æœæ¨ç†å’Œä¸¥æ ¼å‡ ä½•çº¦æŸæ–¹é¢ä»å­˜åœ¨å±€é™ï¼Œå°šä¸èƒ½ä½œä¸ºç‹¬ç«‹çš„é›¶æ ·æœ¬æ¨ç†å™¨ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å½“å‰è§†é¢‘ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿé«˜ä¿çœŸã€æ—¶é—´è¿è´¯çš„è§†é¢‘ï¼Œè¡¨æ˜å…¶å¯èƒ½ç¼–ç äº†ä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†ï¼Œä½†ä¸€ä¸ªé‡è¦é—®é¢˜ä»æœªè§£å†³ï¼šè¿™äº›æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æ¨ç†åœºæ™¯ä¸­ä½œä¸ºé›¶æ ·æœ¬æ¨ç†å™¨ä½¿ç”¨ï¼Ÿæœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å®è¯ç ”ç©¶å…¨é¢æ¢è®¨è¿™ä¸€é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†MME-CoFåŸºå‡†æµ‹è¯•ï¼Œå¯¹é¢†å…ˆçš„Veo-3æ¨¡å‹åœ¨12ä¸ªç»´åº¦ä¸Šè¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°ï¼ŒåŒ…æ‹¬ç©ºé—´ã€å‡ ä½•ã€ç‰©ç†ã€æ—¶é—´å’Œå…·èº«é€»è¾‘ç­‰æ–¹é¢ï¼Œç³»ç»Ÿæ€§åœ°åˆ»ç”»äº†å…¶ä¼˜åŠ¿å’Œå¤±è´¥æ¨¡å¼ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰è§†é¢‘æ¨¡å‹åœ¨çŸ­æ—¶ç©ºé—´ä¸€è‡´æ€§ã€ç»†ç²’åº¦å®šä½å’Œå±€éƒ¨ä¸€è‡´åŠ¨æ€æ¨ç†æ–¹é¢å±•ç°å‡ºæœ‰å‰æ™¯çš„æ¨¡å¼ï¼Œä½†åœ¨é•¿æ—¶å› æœæ¨ç†ã€ä¸¥æ ¼å‡ ä½•çº¦æŸå’ŒæŠ½è±¡é€»è¾‘æ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å±€é™ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å½“å‰è§†é¢‘æ¨¡å‹å°šä¸èƒ½ä½œä¸ºå¯é çš„ç‹¬ç«‹é›¶æ ·æœ¬æ¨ç†å™¨ï¼Œä½†ä½œä¸ºä¸“ç”¨æ¨ç†æ¨¡å‹çš„è¡¥å……è§†è§‰å¼•æ“å±•ç°å‡ºä»¤äººé¼“èˆçš„æ½œåŠ›ï¼Œä¸ºæœªæ¥è§†é¢‘æ¨ç†æ¨¡å‹çš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Recent video generation models can produce high-fidelity, temporally coherent
videos, indicating that they may encode substantial world knowledge. Beyond
realistic synthesis, they also exhibit emerging behaviors indicative of visual
perception, modeling, and manipulation. Yet, an important question still
remains: Are video models ready to serve as zero-shot reasoners in challenging
visual reasoning scenarios? In this work, we conduct an empirical study to
comprehensively investigate this question, focusing on the leading and popular
Veo-3. We evaluate its reasoning behavior across 12 dimensions, including
spatial, geometric, physical, temporal, and embodied logic, systematically
characterizing both its strengths and failure modes. To standardize this study,
we curate the evaluation data into MME-CoF, a compact benchmark that enables
in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our
findings reveal that while current video models demonstrate promising reasoning
patterns on short-horizon spatial coherence, fine-grained grounding, and
locally consistent dynamics, they remain limited in long-horizon causal
reasoning, strict geometric constraints, and abstract logic. Overall, they are
not yet reliable as standalone zero-shot reasoners, but exhibit encouraging
signs as complementary visual engines alongside dedicated reasoning models.
Project page: https://video-cof.github.io</p>
<h3 id="16-oracleagent-a-multimodal-reasoning-agent-for-oracle-bone-script-research">[16] <a href="https://arxiv.org/abs/2510.26114">OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research</a></h3>
<p><em>Caoshuo Li, Zengmao Ding, Xiaobin Hu, Bang Li, Donghao Luo, Xu Peng, Taisong Jin, Yongge Liu, Shengwei Han, Jing Yang, Xiaoping He, Feng Gao, AndyPian Wu, SevenShu, Chaoyang Wang, Chengjie Wang</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†OracleAgentï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºç”²éª¨æ–‡ç»“æ„åŒ–ç®¡ç†å’Œæ£€ç´¢çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé€šè¿‡é›†æˆå¤šæ¨¡æ€çŸ¥è¯†åº“å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†ç”²éª¨æ–‡ç ”ç©¶çš„æ•ˆç‡å’Œè‡ªåŠ¨åŒ–æ°´å¹³ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç”²éª¨æ–‡ä½œä¸ºæœ€æ—©çš„æ–‡å­—ç³»ç»Ÿä¹‹ä¸€ï¼Œå…¶ç ”ç©¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šç”²éª¨æ–‡è§£è¯»æ¶‰åŠå¤šä¸ªä¸²è¡Œå’Œå¹¶è¡Œå­ä»»åŠ¡çš„å¤æ‚å·¥ä½œæµç¨‹ï¼Œä»¥åŠç”²éª¨æ–‡ä¿¡æ¯ç»„ç»‡å’Œæ£€ç´¢æ•ˆç‡ä½ä¸‹ï¼Œå­¦è€…éœ€è¦èŠ±è´¹å¤§é‡ç²¾åŠ›æœç´¢ã€æ•´ç†å’Œç®¡ç†ç›¸å…³èµ„æºã€‚</p>
<p><strong>Method:</strong> OracleAgenté€šè¿‡é›†æˆå¤šä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç”²éª¨æ–‡åˆ†æå·¥å…·ï¼Œå¹¶çµæ´»ç¼–æ’è¿™äº›ç»„ä»¶æ¥æ„å»ºæ™ºèƒ½ä½“ç³»ç»Ÿï¼›åŒæ—¶æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„é¢†åŸŸç‰¹å®šå¤šæ¨¡æ€çŸ¥è¯†åº“ï¼ŒåŒ…å«è¶…è¿‡140ä¸‡å¼ å•å­—æ‹“ç‰‡å›¾åƒå’Œ8ä¸‡æ¡è§£è¯»æ–‡æœ¬ï¼Œé€šè¿‡å¤šå¹´ä¸¥æ ¼çš„æ•°æ®æ”¶é›†ã€æ¸…æ´—å’Œä¸“å®¶æ ‡æ³¨è¿‡ç¨‹å®Œæˆã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOracleAgentåœ¨ä¸€ç³»åˆ—å¤šæ¨¡æ€æ¨ç†å’Œç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ä¸»æµå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰ï¼›æ¡ˆä¾‹ç ”ç©¶æ˜¾ç¤ºè¯¥ç³»ç»Ÿèƒ½æœ‰æ•ˆååŠ©é¢†åŸŸä¸“å®¶ï¼Œæ˜¾è‘—é™ä½ç”²éª¨æ–‡ç ”ç©¶çš„æ—¶é—´æˆæœ¬ã€‚</p>
<p><strong>Conclusion:</strong> OracleAgentä»£è¡¨äº†ç”²éª¨æ–‡è¾…åŠ©ç ”ç©¶å’Œè‡ªåŠ¨åŒ–è§£è¯»ç³»ç»Ÿå®é™…éƒ¨ç½²çš„é‡è¦è¿›å±•ï¼Œä¸ºæ–‡åŒ–é—äº§ä¿æŠ¤å’Œç ”ç©¶æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨ä¸“ä¸šé¢†åŸŸåº”ç”¨çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>As one of the earliest writing systems, Oracle Bone Script (OBS) preserves
the cultural and intellectual heritage of ancient civilizations. However,
current OBS research faces two major challenges: (1) the interpretation of OBS
involves a complex workflow comprising multiple serial and parallel sub-tasks,
and (2) the efficiency of OBS information organization and retrieval remains a
critical bottleneck, as scholars often spend substantial effort searching for,
compiling, and managing relevant resources. To address these challenges, we
present OracleAgent, the first agent system designed for the structured
management and retrieval of OBS-related information. OracleAgent seamlessly
integrates multiple OBS analysis tools, empowered by large language models
(LLMs), and can flexibly orchestrate these components. Additionally, we
construct a comprehensive domain-specific multimodal knowledge base for OBS,
which is built through a rigorous multi-year process of data collection,
cleaning, and expert annotation. The knowledge base comprises over 1.4M
single-character rubbing images and 80K interpretation texts. OracleAgent
leverages this resource through its multimodal tools to assist experts in
retrieval tasks of character, document, interpretation text, and rubbing image.
Extensive experiments demonstrate that OracleAgent achieves superior
performance across a range of multimodal reasoning and generation tasks,
surpassing leading mainstream multimodal large language models (MLLMs) (e.g.,
GPT-4o). Furthermore, our case study illustrates that OracleAgent can
effectively assist domain experts, significantly reducing the time cost of OBS
research. These results highlight OracleAgent as a significant step toward the
practical deployment of OBS-assisted research and automated interpretation
systems.</p>
<h3 id="17-crag-mm-multi-modal-multi-turn-comprehensive-rag-benchmark">[17] <a href="https://arxiv.org/abs/2510.26160">CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark</a></h3>
<p><em>Jiaqi Wang, Xiao Yang, Kai Sun, Parth Suresh, Sanat Sharma, Adam Czyzewski, Derek Andersen, Surya Appini, Arkav Banerjee, Sajal Choudhary, Shervin Ghasemlou, Ziqiang Guan, Akil Iyer, Haidar Khan, Lingkun Kong, Roy Luo, Tiffany Ma, Zhen Qiao, David Tran, Wenfang Xu, Skyler Yeatman, Chen Zhou, Gunveer Gujral, Yinglong Xia, Shane Moon, Nicolas Scheffer, Nirav Shah, Eun Chang, Yue Liu, Florian Metze, Tammy Stark, Zhaleh Feizollahi, Andrea Jessee, Mangesh Pujari, Ahmed Aly, Babak Damavandi, Rakesh Wanga, Anuj Kumar, Rohit Patel, Wen-tau Yih, Xin Luna Dong</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†CRAG-MMâ€”â€”ä¸€ä¸ªé¢å‘å¤šæ¨¡æ€å¤šè½®å¯¹è¯çš„å…¨é¢æ£€ç´¢å¢å¼ºç”ŸæˆåŸºå‡†ï¼ŒåŒ…å«6.5Kä¸ªå›¾åƒ-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„å’Œ2Kä¸ªå¤šè½®å¯¹è¯ï¼Œä¸“é—¨é’ˆå¯¹å¯ç©¿æˆ´è®¾å¤‡åœºæ™¯è®¾è®¡ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç¼ºä¹ç»¼åˆåŸºå‡†çš„ç©ºç™½ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç¼ºä¹é’ˆå¯¹å¯ç©¿æˆ´è®¾å¤‡åœºæ™¯çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMM-RAGï¼‰ä»»åŠ¡çš„ç»¼åˆåŸºå‡†ï¼Œç‰¹åˆ«æ˜¯èƒ½å¤Ÿåæ˜ çœŸå®ä¸–ç•ŒæŒ‘æˆ˜çš„è¯„ä¼°æ¡†æ¶ï¼Œè¿™é™åˆ¶äº†è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•å’Œå®é™…åº”ç”¨ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†åŒ…å«13ä¸ªé¢†åŸŸã€6.5Kä¸ªå›¾åƒ-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„å’Œ2Kä¸ªå¤šè½®å¯¹è¯çš„åŸºå‡†æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«6.2Kä¸ªæ¨¡æ‹Ÿå¯ç©¿æˆ´è®¾å¤‡æ‹æ‘„çš„è‡ªæˆ‘ä¸­å¿ƒå›¾åƒï¼›è®¾è®¡äº†äº”ç§å›¾åƒè´¨é‡é—®é¢˜ã€å…­ç§é—®é¢˜ç±»å‹ã€ä¸åŒå®ä½“æµè¡Œåº¦ã€ä¿¡æ¯åŠ¨æ€æ€§å’Œå¯¹è¯è½®æ¬¡ï¼›æä¾›äº†ä¸‰ç§ä»»åŠ¡è®¾ç½®å’Œç›¸åº”çš„æ£€ç´¢è¯­æ–™åº“åŠAPIã€‚</p>
<p><strong>Result:</strong> å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œç›´æ¥RAGæ–¹æ³•åœ¨å•è½®å’Œå¤šè½®é—®ç­”ä¸Šçš„çœŸå®æ€§åˆ†åˆ«ä»…ä¸º32%å’Œ43%ï¼Œè€Œä¸šç•Œæœ€å…ˆè¿›è§£å†³æ–¹æ¡ˆçš„è´¨é‡ç›¸ä¼¼ï¼ˆ32%/45%ï¼‰ï¼›è¯¥åŸºå‡†å·²æˆåŠŸä¸¾åŠKDD Cup 2025ç«èµ›ï¼Œè·èƒœæ–¹æ¡ˆå°†åŸºçº¿æ€§èƒ½æå‡äº†28%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰MM-RAGæ–¹æ³•åœ¨å¯ç©¿æˆ´è®¾å¤‡åœºæ™¯ä¸‹çš„æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„è¯„ä¼°æ¡†æ¶å’Œæ–¹å‘æŒ‡å¼•ï¼›åŸºå‡†çš„æˆåŠŸåº”ç”¨è¡¨æ˜å…¶åœ¨æ¨åŠ¨è¯¥é¢†åŸŸå‘å±•æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Wearable devices such as smart glasses are transforming the way people
interact with their surroundings, enabling users to seek information regarding
entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
plays a key role in supporting such questions, yet there is still no
comprehensive benchmark for this task, especially regarding wearables
scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
conversations across 13 domains, including 6.2K egocentric images designed to
mimic captures from wearable devices. We carefully constructed the questions to
reflect real-world scenarios and challenges, including five types of
image-quality issues, six question types, varying entity popularity, differing
information dynamism, and different conversation turns. We design three tasks:
single-source augmentation, multi-source augmentation, and multi-turn
conversations -- each paired with an associated retrieval corpus and APIs for
both image-KG retrieval and webpage retrieval. Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
participants and 5K submissions, with winning solutions improving baseline
performance by 28%, highlighting its early impact on advancing the field.</p>
<h3 id="18-a-tpt-angular-diversity-calibration-properties-for-test-time-prompt-tuning-of-vision-language-models">[18] <a href="https://arxiv.org/abs/2510.26441">A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models</a></h3>
<p><em>Shihab Aaqil Ahamed, Udaya S. K. P. Miriya Thanthrige, Ranga Rodrigo, Muhammad Haris Khan</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºA-TPTæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è§’åº¦å¤šæ ·æ€§æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶æç¤ºè°ƒä¼˜ä¸­çš„æ ¡å‡†æ€§èƒ½ï¼Œè¯¥æ–¹æ³•é€šè¿‡æœ€å¤§åŒ–å½’ä¸€åŒ–æ–‡æœ¬ç‰¹å¾åœ¨å•ä½è¶…çƒé¢ä¸Šçš„æœ€å°æˆå¯¹è§’åº¦è·ç¦»æ¥å®ç°ç‰¹å¾å‡åŒ€åˆ†å¸ƒã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æµ‹è¯•æ—¶æç¤ºè°ƒä¼˜æ–¹æ³•ä¸»è¦å…³æ³¨æœ€å¤§åŒ–å¹³å‡æ–‡æœ¬ç‰¹å¾ç¦»æ•£åº¦æˆ–æ–½åŠ æ­£äº¤çº¦æŸæ¥ä¿ƒè¿›è§’åº¦åˆ†ç¦»ï¼Œä½†è¿™äº›æ–¹æ³•å¯èƒ½æ— æ³•å®ç°ç±»åˆ«é—´æ–‡æœ¬ç‰¹å¾çš„æœ€ä¼˜è§’åº¦åˆ†ç¦»ï¼Œå¿½è§†äº†è§’åº¦å¤šæ ·æ€§çš„å…³é”®ä½œç”¨ï¼Œå¯¼è‡´æ–‡æœ¬ç‰¹å¾ç¼ºä¹åˆ†æ•£æ€§ä»è€ŒæŸå®³æ ¡å‡†æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> æå‡ºA-TPTæ¡†æ¶ï¼Œé€šè¿‡é¼“åŠ±ç”±å¯å­¦ä¹ æç¤ºè¯±å¯¼çš„å½’ä¸€åŒ–æ–‡æœ¬ç‰¹å¾åœ¨å•ä½è¶…çƒé¢ä¸Šå‡åŒ€åˆ†å¸ƒæ¥å®ç°è§’åº¦å¤šæ ·æ€§ï¼Œå…·ä½“é€šè¿‡æœ€å¤§åŒ–ç‰¹å¾é—´æœ€å°æˆå¯¹è§’åº¦è·ç¦»æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªéª¨å¹²ç½‘ç»œå’Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒA-TPTåœ¨é™ä½èšåˆå¹³å‡æ ¡å‡†è¯¯å·®æ–¹é¢æŒç»­è¶…è¶Šæœ€å…ˆè¿›çš„TPTæ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„å‡†ç¡®ç‡ï¼Œåœ¨è‡ªç„¶åˆ†å¸ƒåç§»ä¸‹è¡¨ç°å‡ºä¼˜è¶Šçš„é›¶æ ·æœ¬æ ¡å‡†æ€§èƒ½ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°æ³›åŒ–åˆ°åŒ»å­¦æ•°æ®é›†ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜ä¿ƒè¿›è§’åº¦å¤šæ ·æ€§æ˜¯å®ç°è‰¯å¥½åˆ†æ•£æ–‡æœ¬ç‰¹å¾çš„æœ‰æ•ˆæ–¹æ³•ï¼Œæ˜¾è‘—æ”¹å–„äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶é€‚åº”è¿‡ç¨‹ä¸­çš„æ ¡å‡†æ€§èƒ½ï¼Œä¸ºæå‡æ¨¡å‹å¯é æ€§ã€å¯ä¿¡åº¦å’Œå®‰å…¨æ€§æä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Test-time prompt tuning (TPT) has emerged as a promising technique for
adapting large vision-language models (VLMs) to unseen tasks without relying on
labeled data. However, the lack of dispersion between textual features can hurt
calibration performance, which raises concerns about VLMs' reliability,
trustworthiness, and safety. Current TPT approaches primarily focus on
improving prompt calibration by either maximizing average textual feature
dispersion or enforcing orthogonality constraints to encourage angular
separation. However, these methods may not always have optimal angular
separation between class-wise textual features, which implies overlooking the
critical role of angular diversity. To address this, we propose A-TPT, a novel
TPT framework that introduces angular diversity to encourage uniformity in the
distribution of normalized textual features induced by corresponding learnable
prompts. This uniformity is achieved by maximizing the minimum pairwise angular
distance between features on the unit hypersphere. We show that our approach
consistently surpasses state-of-the-art TPT methods in reducing the aggregate
average calibration error while maintaining comparable accuracy through
extensive experiments with various backbones on different datasets. Notably,
our approach exhibits superior zero-shot calibration performance on natural
distribution shifts and generalizes well to medical datasets. We provide
extensive analyses, including theoretical aspects, to establish the grounding
of A-TPT. These results highlight the potency of promoting angular diversity to
achieve well-dispersed textual features, significantly improving VLM
calibration during test-time adaptation. Our code will be made publicly
available.</p>
<h3 id="19-towards-fine-grained-vision-language-alignment-for-few-shot-anomaly-detection">[19] <a href="https://arxiv.org/abs/2510.26464">Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection</a></h3>
<p><em>Yuanting Fan, Jun Liu, Xiaochen Chen, Bin-Bin Gao, Jian Li, Yong Liu, Jinlong Peng, Chengjie Wang</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FineGrainedADæ¡†æ¶ï¼Œé€šè¿‡å¤šçº§ç»†ç²’åº¦è¯­ä¹‰æè¿°å’Œè¯­ä¹‰å¯¹é½æœºåˆ¶è§£å†³å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä¸­çš„è¯­ä¹‰ä¸å¯¹é½é—®é¢˜ï¼Œåœ¨MVTec-ADå’ŒVisAæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜è¶Šçš„å¼‚å¸¸å®šä½æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä¾èµ–é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ç”±äºç¼ºä¹è¯¦ç»†æ–‡æœ¬æè¿°ï¼Œåªèƒ½é¢„å®šä¹‰å›¾åƒçº§æè¿°æ¥åŒ¹é…è§†è§‰è¡¥ä¸æ ‡è®°ï¼Œå¯¼è‡´å›¾åƒæè¿°ä¸è¡¥ä¸çº§è§†è§‰å¼‚å¸¸ä¹‹é—´çš„è¯­ä¹‰ä¸å¯¹é½ï¼Œä»è€Œè·å¾—æ¬¡ä¼˜çš„å®šä½æ€§èƒ½ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†å¤šçº§ç»†ç²’åº¦è¯­ä¹‰æè¿°ï¼ˆMFSCï¼‰ä¸ºå¼‚å¸¸æ£€æµ‹æ•°æ®é›†æä¾›å¤šçº§ç»†ç²’åº¦æ–‡æœ¬æè¿°ï¼Œå¹¶è®¾è®¡äº†åŒ…å«å¤šçº§å¯å­¦ä¹ æç¤ºï¼ˆMLLPï¼‰å’Œå¤šçº§è¯­ä¹‰å¯¹é½ï¼ˆMLSAï¼‰çš„FineGrainedADæ¡†æ¶ï¼Œå…¶ä¸­MLLPé€šè¿‡è‡ªåŠ¨æ›¿æ¢å’Œè¿æ¥æœºåˆ¶å°†ç»†ç²’åº¦è¯­ä¹‰å¼•å…¥å¤šçº§å¯å­¦ä¹ æç¤ºï¼ŒMLSAè®¾è®¡åŒºåŸŸèšåˆç­–ç•¥å’Œå¤šçº§å¯¹é½è®­ç»ƒæ¥ä¿ƒè¿›å¯å­¦ä¹ æç¤ºä¸ç›¸åº”è§†è§‰åŒºåŸŸçš„å¯¹é½ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„FineGrainedADåœ¨MVTec-ADå’ŒVisAæ•°æ®é›†çš„å°‘æ ·æœ¬è®¾ç½®ä¸­å®ç°äº†ä¼˜è¶Šçš„æ•´ä½“æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†å¼‚å¸¸å®šä½çš„å‡†ç¡®æ€§å’Œæ•ˆæœã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥å¤šçº§ç»†ç²’åº¦è¯­ä¹‰æè¿°å’Œè¯­ä¹‰å¯¹é½æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä¸­çš„è¯­ä¹‰ä¸å¯¹é½é—®é¢˜ï¼Œä¸ºåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’Œæ€§èƒ½æå‡æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Few-shot anomaly detection (FSAD) methods identify anomalous regions with few
known normal samples. Most existing methods rely on the generalization ability
of pre-trained vision-language models (VLMs) to recognize potentially anomalous
regions through feature similarity between text descriptions and images.
However, due to the lack of detailed textual descriptions, these methods can
only pre-define image-level descriptions to match each visual patch token to
identify potential anomalous regions, which leads to the semantic misalignment
between image descriptions and patch-level visual anomalies, achieving
sub-optimal localization performance. To address the above issues, we propose
the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and
fine-grained textual descriptions for existing anomaly detection datasets with
automatic construction pipeline. Based on the MFSC, we propose a novel
framework named FineGrainedAD to improve anomaly localization performance,
which consists of two components: Multi-Level Learnable Prompt (MLLP) and
Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics
into multi-level learnable prompts through automatic replacement and
concatenation mechanism, while MLSA designs region aggregation strategy and
multi-level alignment training to facilitate learnable prompts better align
with corresponding visual regions. Experiments demonstrate that the proposed
FineGrainedAD achieves superior overall performance in few-shot settings on
MVTec-AD and VisA datasets.</p>
<h3 id="20-representation-level-counterfactual-calibration-for-debiased-zero-shot-recognition">[20] <a href="https://arxiv.org/abs/2510.26466">Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition</a></h3>
<p><em>Pei Peng, MingKun Xie, Hang Hao, Tong Jin, ShengJun Huang</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå› æœæ¨ç†çš„è½»é‡çº§è¡¨ç¤ºçº§åäº‹å®æ–¹æ³•ï¼Œé€šè¿‡ä¼°è®¡å¯¹è±¡å’ŒèƒŒæ™¯æœŸæœ›å¹¶åˆæˆåäº‹å®åµŒå…¥ï¼Œæœ‰æ•ˆç¼“è§£è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¯¹è±¡-ä¸Šä¸‹æ–‡æ·å¾„é—®é¢˜ï¼Œåœ¨æ— éœ€é‡æ–°è®­ç»ƒæˆ–æç¤ºè®¾è®¡çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†é›¶æ ·æœ¬å¯é æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨çš„å¯¹è±¡-ä¸Šä¸‹æ–‡æ·å¾„é—®é¢˜ä¸¥é‡å½±å“äº†é›¶æ ·æœ¬å¯é æ€§ï¼Œå½“æµ‹è¯•åœºæ™¯ä¸è®­ç»ƒå…±ç°æ¨¡å¼ä¸åŒæ—¶ï¼Œæ¨¡å‹å®¹æ˜“äº§ç”Ÿé”™è¯¯é¢„æµ‹ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å› æœæ¨ç†æ¡†æ¶è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> åœ¨CLIPè¡¨ç¤ºç©ºé—´ä¸­ä¼°è®¡å¯¹è±¡å’ŒèƒŒæ™¯æœŸæœ›ï¼Œé€šè¿‡ä»å¤–éƒ¨æ•°æ®é›†ã€æ‰¹æ¬¡é‚»å±…æˆ–æ–‡æœ¬æè¿°ä¸­é‡‡æ ·å¤šæ ·æ›¿ä»£ä¸Šä¸‹æ–‡ï¼Œåˆæˆåäº‹å®åµŒå…¥ï¼Œå¹¶åˆ©ç”¨æ€»ç›´æ¥æ•ˆåº”ä¼°è®¡å’Œå¹²é¢„æ¨¡æ‹Ÿæ¥å‡å»ä»…èƒŒæ™¯æ¿€æ´»ï¼Œä¿ç•™æœ‰ç›Šçš„å¯¹è±¡-ä¸Šä¸‹æ–‡äº¤äº’ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨ä¸Šä¸‹æ–‡æ•æ„ŸåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æœ€å·®ç»„å’Œå¹³å‡å‡†ç¡®ç‡ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–æç¤ºè®¾è®¡å³å®ç°äº†æ–°çš„é›¶æ ·æœ¬æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨ç¼“è§£å¹»è§‰åˆ†æ•°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªè½»é‡çº§çš„è¡¨ç¤ºçº§åäº‹å®æ¡†æ¶ï¼Œä¸ºå»åå’Œå¯é çš„å¤šæ¨¡æ€æ¨ç†å¼€è¾Ÿäº†å®ç”¨çš„å› æœé€”å¾„ï¼Œå±•ç¤ºäº†å› æœæ¨ç†åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹é²æ£’æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Object-context shortcuts remain a persistent challenge in vision-language
models, undermining zero-shot reliability when test-time scenes differ from
familiar training co-occurrences. We recast this issue as a causal inference
problem and ask: Would the prediction remain if the object appeared in a
different environment? To answer this at inference time, we estimate object and
background expectations within CLIP's representation space, and synthesize
counterfactual embeddings by recombining object features with diverse
alternative contexts sampled from external datasets, batch neighbors, or
text-derived descriptions. By estimating the Total Direct Effect and simulating
intervention, we further subtract background-only activation, preserving
beneficial object-context interactions while mitigating hallucinated scores.
Without retraining or prompt design, our method substantially improves both
worst-group and average accuracy on context-sensitive benchmarks, establishing
a new zero-shot state of the art. Beyond performance, our framework provides a
lightweight representation-level counterfactual approach, offering a practical
causal avenue for debiased and reliable multimodal reasoning.</p>
<h3 id="21-adsum-two-stream-audio-visual-summarization-for-automated-video-advertisement-clipping">[21] <a href="https://arxiv.org/abs/2510.26569">AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping</a></h3>
<p><em>Wen Xie, Yanjun Zhu, Gijs Overgoor, Yakov Bart, Agata Lapedriza Garcia, Sarah Ostadabbas</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘æ‘˜è¦æŠ€æœ¯çš„è‡ªåŠ¨åŒ–å¹¿å‘Šå‰ªè¾‘æ¡†æ¶ï¼Œé¦–æ¬¡å°†è§†é¢‘å‰ªè¾‘å®šä¹‰ä¸ºé’ˆå¯¹å¹¿å‘Šåœºæ™¯çš„é•œå¤´é€‰æ‹©é—®é¢˜ï¼Œå¹¶é€šè¿‡åŒæµéŸ³è§†é¢‘èåˆæ¨¡å‹æ˜¾è‘—æå‡äº†å¹¿å‘Šå‰ªè¾‘æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¹¿å‘Šå•†é€šå¸¸éœ€è¦ä¸ºåŒä¸€å¹¿å‘Šæ´»åŠ¨åˆ¶ä½œä¸åŒæ—¶é•¿çš„å¤šä¸ªç‰ˆæœ¬ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–äººå·¥ä»é•¿è§†é¢‘å¹¿å‘Šä¸­é€‰æ‹©å’Œé‡æ–°ç¼–è¾‘é•œå¤´æ¥åˆ›å»ºçŸ­ç‰ˆæœ¬ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢è€—æ—¶åˆè´¹åŠ›ï¼Œç°æœ‰é€šç”¨è§†é¢‘æ‘˜è¦æ–¹æ³•ä¸»è¦å…³æ³¨è§†è§‰å†…å®¹è€Œå¿½ç•¥äº†éŸ³é¢‘åœ¨å¹¿å‘Šä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§åŒæµéŸ³è§†é¢‘èåˆæ¨¡å‹æ¥é¢„æµ‹è§†é¢‘å¸§çš„é‡è¦æ€§ï¼Œå…¶ä¸­é‡è¦æ€§å®šä¹‰ä¸ºå¸§è¢«é€‰å…¥ä¼ä¸šåˆ¶ä½œçš„çŸ­å¹¿å‘Šä¸­çš„å¯èƒ½æ€§ï¼Œè¯¥æ–¹æ³•å°†è§†é¢‘å‰ªè¾‘æ„å»ºä¸ºé•œå¤´é€‰æ‹©é—®é¢˜ï¼Œå¹¶ä¸“é—¨é’ˆå¯¹å¹¿å‘Šåœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> åœ¨AdSum204æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¹³å‡ç²¾åº¦ã€æ›²çº¿ä¸‹é¢ç§¯ã€æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°å’Œè‚¯å¾·å°”ç³»æ•°ç­‰å¤šä¸ªæŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†éŸ³é¢‘ä¿¡æ¯å¯¹å¹¿å‘Šå‰ªè¾‘ä»»åŠ¡çš„é‡è¦æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¼ºè°ƒäº†éŸ³é¢‘åœ¨å¹¿å‘Šå‰ªè¾‘ä¸­çš„å…³é”®ä½œç”¨ï¼Œæå‡ºçš„åŒæµéŸ³è§†é¢‘èåˆæ–¹æ³•ä¸ºè‡ªåŠ¨åŒ–å¹¿å‘Šåˆ¶ä½œæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶å‘å¸ƒçš„AdSum204æ•°æ®é›†å¡«è¡¥äº†å¹¿å‘Šç‰¹å®šæ•°æ®é›†çš„ç©ºç™½ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦åŸºå‡†ã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Advertisers commonly need multiple versions of the same advertisement (ad) at
varying durations for a single campaign. The traditional approach involves
manually selecting and re-editing shots from longer video ads to create shorter
versions, which is labor-intensive and time-consuming. In this paper, we
introduce a framework for automated video ad clipping using video summarization
techniques. We are the first to frame video clipping as a shot selection
problem, tailored specifically for advertising. Unlike existing general video
summarization methods that primarily focus on visual content, our approach
emphasizes the critical role of audio in advertising. To achieve this, we
develop a two-stream audio-visual fusion model that predicts the importance of
video frames, where importance is defined as the likelihood of a frame being
selected in the firm-produced short ad. To address the lack of ad-specific
datasets, we present AdSum204, a novel dataset comprising 102 pairs of
30-second and 15-second ads from real advertising campaigns. Extensive
experiments demonstrate that our model outperforms state-of-the-art methods
across various metrics, including Average Precision, Area Under Curve,
Spearman, and Kendall.</p>
<h3 id="22-dynamic-context-aware-scene-reasoning-using-vision-language-alignment-in-zero-shot-real-world-scenarios">[22] <a href="https://arxiv.org/abs/2510.26580">Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios</a></h3>
<p><em>Manjunath Prasad Holenarasipura Rajiv, B. M. Vidyavathi</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€ä¸Šä¸‹æ–‡æ„ŸçŸ¥åœºæ™¯æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰-è¯­è¨€å¯¹é½æŠ€æœ¯è§£å†³é›¶æ ·æœ¬çœŸå®ä¸–ç•Œåœºæ™¯ç†è§£é—®é¢˜ï¼Œåœ¨æœªè§è¿‡çš„å¤æ‚ç¯å¢ƒä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> çœŸå®ç¯å¢ƒä¸­AIç³»ç»Ÿç»å¸¸é¢ä¸´ç¼ºä¹æ ‡æ³¨æ•°æ®çš„é™Œç”Ÿåœºæ™¯ï¼Œä¼ ç»Ÿåœºæ™¯ç†è§£æ¨¡å‹éš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„ä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œè¿™é™åˆ¶äº†è§†è§‰åº”ç”¨åœ¨åŠ¨æ€éç»“æ„åŒ–ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•æ•´åˆé¢„è®­ç»ƒè§†è§‰å˜æ¢å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°†è§†è§‰è¯­ä¹‰ä¸è‡ªç„¶è¯­è¨€æè¿°å¯¹é½ä»¥å¢å¼ºä¸Šä¸‹æ–‡ç†è§£ï¼›åŠ¨æ€æ¨ç†æ¨¡å—é€šè¿‡ç»“åˆå…¨å±€åœºæ™¯çº¿ç´¢å’Œå¯¹è±¡çº§äº¤äº’ï¼Œåœ¨è¯­è¨€å…ˆéªŒæŒ‡å¯¼ä¸‹ä¼˜åŒ–é¢„æµ‹ç»“æœã€‚</p>
<p><strong>Result:</strong> åœ¨COCOã€Visual Genomeå’ŒOpen Imagesç­‰é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚æœªè§ç¯å¢ƒä¸­æ¯”åŸºçº¿æ¨¡å‹æé«˜äº†18%çš„åœºæ™¯ç†è§£å‡†ç¡®ç‡ï¼›åœ¨æ¨¡ç³Šæˆ–æ‚ä¹±åœºæ™¯ä¸­ä¹Ÿè¡¨ç°å‡ºé²æ£’æ€§èƒ½ï¼Œå¾—ç›Šäºè§†è§‰ä¸è¯­è¨€çš„ååŒèåˆã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ¡†æ¶ä¸ºä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†æä¾›äº†å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„æ–¹æ³•ï¼Œæ¨åŠ¨äº†åŠ¨æ€çœŸå®ä¸–ç•Œç¯å¢ƒä¸­é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›çš„å‘å±•ï¼Œä¸ºæ™ºèƒ½ç³»ç»Ÿåœ¨æ— ä»»åŠ¡ç‰¹å®šè®­ç»ƒæ¡ä»¶ä¸‹é€‚åº”æ–°ç¯å¢ƒæä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>In real-world environments, AI systems often face unfamiliar scenarios
without labeled data, creating a major challenge for conventional scene
understanding models. The inability to generalize across unseen contexts limits
the deployment of vision-based applications in dynamic, unstructured settings.
This work introduces a Dynamic Context-Aware Scene Reasoning framework that
leverages Vision-Language Alignment to address zero-shot real-world scenarios.
The goal is to enable intelligent systems to infer and adapt to new
environments without prior task-specific training. The proposed approach
integrates pre-trained vision transformers and large language models to align
visual semantics with natural language descriptions, enhancing contextual
comprehension. A dynamic reasoning module refines predictions by combining
global scene cues and object-level interactions guided by linguistic priors.
Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and
Open Images demonstrate up to 18% improvement in scene understanding accuracy
over baseline models in complex and unseen environments. Results also show
robust performance in ambiguous or cluttered scenes due to the synergistic
fusion of vision and language. This framework offers a scalable and
interpretable approach for context-aware reasoning, advancing zero-shot
generalization in dynamic real-world settings.</p>
<h3 id="23-catch-a-modular-cross-domain-adaptive-template-with-hook">[23] <a href="https://arxiv.org/abs/2510.26582">CATCH: A Modular Cross-domain Adaptive Template with Hook</a></h3>
<p><em>Xinjin Li, Yulie Lu, Jinghan Cao, Yu Ma, Zhenglin Li, Yeyang Zhou</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCATCHæ¡†æ¶ï¼Œä¸€ç§å³æ’å³ç”¨çš„è·¨é¢†åŸŸè§†è§‰é—®ç­”é€‚åº”æ–¹æ³•ï¼Œé€šè¿‡è§£è€¦è§†è§‰å’Œè¯­è¨€é€‚åº”æ¨¡å—ï¼Œåœ¨ä¸é‡æ–°è®­ç»ƒä¸»å¹²æ¨¡å‹çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡å¤šé¢†åŸŸVQAæ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰é—®ç­”æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨é¥æ„Ÿã€åŒ»å­¦å½±åƒã€æ•°å­¦å›¾è¡¨ç­‰è·¨é¢†åŸŸåœºæ™¯ä¸­æ³›åŒ–èƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼Œä¸»è¦ç”±äºåˆ†å¸ƒåç§»å’Œç¼ºä¹æœ‰æ•ˆçš„é¢†åŸŸé€‚åº”æœºåˆ¶ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–é¢†åŸŸç‰¹å®šå¾®è°ƒæˆ–å®šåˆ¶æµç¨‹ï¼Œæˆæœ¬é«˜ã€çµæ´»æ€§å·®ä¸”éš¾ä»¥æ‰©å±•åˆ°å¤šæ ·åŒ–ä»»åŠ¡ã€‚</p>
<p><strong>Method:</strong> CATCHæ¡†æ¶å¼•å…¥ä¸¤ä¸ªè½»é‡çº§æ¨¡å—ï¼šé¢†åŸŸåˆ†ç±»å™¨ç”¨äºè¯†åˆ«è¾“å…¥å›¾åƒç±»å‹ï¼Œä»¥åŠåŒ…å«æç¤ºé€‚é…å™¨å’Œè§†è§‰é€‚é…å™¨çš„åŒé€‚é…æœºåˆ¶ã€‚ä¸¤ä¸ªæ¨¡å—é€šè¿‡ç»Ÿä¸€é’©å­æ¥å£åŠ¨æ€æ³¨å…¥ï¼Œæ— éœ€é‡æ–°è®­ç»ƒä¸»å¹²æ¨¡å‹ï¼Œå®ç°è§†è§‰ç‰¹å¾è°ƒæ•´å’Œè¯­è¨€è°ƒåˆ¶çš„è§£è€¦é€‚åº”ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªé¢†åŸŸç‰¹å®šVQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCATCHæ¡†æ¶åœ¨ä¸é‡æ–°è®­ç»ƒä¸»å¹²æ¨¡å‹çš„æƒ…å†µä¸‹å®ç°ä¸€è‡´æ€§èƒ½æå‡ï¼šMathVQAä¸ŠBLEUæå‡2.3åˆ†ï¼ŒMedVQA-RADä¸ŠVQAå¾—åˆ†æå‡2.6åˆ†ï¼ŒChartQAä¸ŠROUGEæå‡3.1åˆ†ã€‚</p>
<p><strong>Conclusion:</strong> CATCHä¸ºå¤šé¢†åŸŸVQAæä¾›äº†å¯æ‰©å±•å’Œå¯æ‰©å±•çš„æ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§é€‚é…æœºåˆ¶å®ç°è·¨é¢†åŸŸæ³›åŒ–ï¼Œæ”¯æŒåœ¨å®é™…éƒ¨ç½²ä¸­åº”ç”¨äºå¤šæ ·åŒ–åº”ç”¨é¢†åŸŸï¼Œæ˜¾è‘—é™ä½äº†é¢†åŸŸé€‚åº”çš„æˆæœ¬å’Œå¤æ‚æ€§ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Recent advances in Visual Question Answering (VQA) have demonstrated
impressive performance in natural image domains, with models like LLaVA
leveraging large language models (LLMs) for open-ended reasoning. However,
their generalization degrades significantly when transferred to out-of-domain
scenarios such as remote sensing, medical imaging, or math diagrams, due to
large distributional shifts and the lack of effective domain adaptation
mechanisms. Existing approaches typically rely on per-domain fine-tuning or
bespoke pipelines, which are costly, inflexible, and not scalable across
diverse tasks. In this paper, we propose CATCH, a plug-and-play framework for
cross-domain adaptation that improves the generalization of VQA models while
requiring minimal changes to their core architecture. Our key idea is to
decouple visual and linguistic adaptation by introducing two lightweight
modules: a domain classifier to identify the input image type, and a dual
adapter mechanism comprising a Prompt Adapter for language modulation and a
Visual Adapter for vision feature adjustment. Both modules are dynamically
injected via a unified hook interface, requiring no retraining of the backbone
model. Experimental results across four domain-specific VQA benchmarks
demonstrate that our framework achieves consistent performance gains without
retraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on
MedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH
provides a scalable and extensible approach to multi-domain VQA, enabling
practical deployment across diverse application domains.</p>
<h3 id="24-emu35-native-multimodal-models-are-world-learners">[24] <a href="https://arxiv.org/abs/2510.26583">Emu3.5: Native Multimodal Models are World Learners</a></h3>
<p><em>Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, Yueze Wang, Chengyuan Wang, Fan Zhang, Yingli Zhao, Ting Pan, Xianduo Li, Zecheng Hao, Wenxuan Ma, Zhuo Chen, Yulong Ao, Tiejun Huang, Zhongyuan Wang, Xinlong Wang</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Emu3.5ï¼Œä¸€ä¸ªé€šè¿‡ç«¯åˆ°ç«¯é¢„è®­ç»ƒçš„å¤§è§„æ¨¡å¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤ŸåŸç”Ÿé¢„æµ‹è§†è§‰å’Œè¯­è¨€çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œå¹¶å¼•å…¥ç¦»æ•£æ‰©æ•£é€‚é…æŠ€æœ¯å®ç°20å€æ¨ç†åŠ é€Ÿï¼Œåœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šè¾¾åˆ°ä¸Gemini 2.5 Flash Imageç›¸å½“çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€äº¤ç»‡ç”Ÿæˆã€é•¿æ—¶åºä¸€è‡´æ€§ä¿æŒä»¥åŠæ¨ç†æ•ˆç‡æ–¹é¢å­˜åœ¨å±€é™ï¼ŒEmu3.5æ—¨åœ¨æ„å»ºä¸€ä¸ªèƒ½å¤ŸåŸç”Ÿå¤„ç†è§†è§‰è¯­è¨€äº¤ç»‡è¾“å…¥è¾“å‡ºã€å…·å¤‡ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ä¸”æ¨ç†é«˜æ•ˆçš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚</p>
<p><strong>Method:</strong> Emu3.5é‡‡ç”¨ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªtokené¢„æµ‹ç›®æ ‡åœ¨è¶…è¿‡10ä¸‡äº¿tokençš„è§†è§‰è¯­è¨€äº¤ç»‡æ•°æ®ä¸Šè¿›è¡Œç«¯åˆ°ç«¯é¢„è®­ç»ƒï¼Œéšåé€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ å¢å¼ºå¤šæ¨¡æ€æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ç¦»æ•£æ‰©æ•£é€‚é…æŠ€æœ¯å°†é€tokenè§£ç è½¬æ¢ä¸ºåŒå‘å¹¶è¡Œé¢„æµ‹ä»¥æå‡æ¨ç†æ•ˆç‡ã€‚</p>
<p><strong>Result:</strong> Emu3.5åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šè¾¾åˆ°ä¸Gemini 2.5 Flash Imageç›¸å½“çš„æ€§èƒ½ï¼Œåœ¨äº¤ç»‡ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œæ¨ç†åŠ é€Ÿçº¦20å€ï¼ŒåŒæ—¶å±•ç°å‡ºé•¿æ—¶åºè§†è§‰è¯­è¨€ç”Ÿæˆã€ä»»æ„åˆ°å›¾åƒç”Ÿæˆã€å¤æ‚æ–‡æœ¬å›¾åƒç”Ÿæˆä»¥åŠæ—¶ç©ºä¸€è‡´æ€§ä¸–ç•Œæ¢ç´¢ç­‰å¼ºå¤§èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> Emu3.5è¯æ˜äº†é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ å¯ä»¥æ„å»ºå…·å¤‡é€šç”¨ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå…¶å¼€æºå‘å¸ƒå°†æ¨åŠ¨ç¤¾åŒºåœ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½é¢†åŸŸçš„ç ”ç©¶å‘å±•ï¼Œä¸ºå…·èº«æ™ºèƒ½å’Œå¼€æ”¾ä¸–ç•Œäº¤äº’æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>We introduce Emu3.5, a large-scale multimodal world model that natively
predicts the next state across vision and language. Emu3.5 is pre-trained
end-to-end with a unified next-token prediction objective on a corpus of
vision-language interleaved data containing over 10 trillion tokens, primarily
derived from sequential frames and transcripts of internet videos. The model
naturally accepts interleaved vision-language inputs and generates interleaved
vision-language outputs. Emu3.5 is further post-trained with large-scale
reinforcement learning to enhance multimodal reasoning and generation. To
improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),
which converts token-by-token decoding into bidirectional parallel prediction,
accelerating per-image inference by about 20x without sacrificing performance.
Emu3.5 exhibits strong native multimodal capabilities, including long-horizon
vision-language generation, any-to-image (X2I) generation, and complex
text-rich image generation. It also exhibits generalizable world-modeling
abilities, enabling spatiotemporally consistent world exploration and
open-world embodied manipulation across diverse scenarios and tasks. For
comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image
(Nano Banana) on image generation and editing tasks and demonstrates superior
results on a suite of interleaved generation tasks. We open-source Emu3.5 at
https://github.com/baaivision/Emu3.5 to support community research.</p>
<h3 id="25-all-you-need-for-object-detection-from-pixels-points-and-prompts-to-next-gen-fusion-and-multimodal-llmsvlms-in-autonomous-vehicles">[25] <a href="https://arxiv.org/abs/2510.26641">All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles</a></h3>
<p><em>Sayed Pedram Haeri Boroujeni, Niloufar Mehrabi, Hazim Alzorgan, Ahmad Sarlak, Mahlagha Fazeli, Abolfazl Razi</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬ç»¼è¿°ä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†ç‰©ä½“æ£€æµ‹é¢†åŸŸæä¾›äº†å‰ç»æ€§åˆ†æï¼Œé‡ç‚¹å…³æ³¨è§†è§‰è¯­è¨€æ¨¡å‹ã€å¤§è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆå¼AIç­‰æ–°å…´èŒƒå¼ï¼Œé€šè¿‡ç³»ç»Ÿæ¢³ç†ä¼ æ„Ÿå™¨èåˆã€æ•°æ®é›†åˆ†ç±»å’Œå…ˆè¿›æ£€æµ‹æ–¹æ³•ï¼Œä¸ºå½“å‰èƒ½åŠ›ã€å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥æœºé‡ç»˜åˆ¶äº†æ¸…æ™°è·¯çº¿å›¾ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„æˆåŠŸä¾èµ–äºåœ¨å¤æ‚å¤šæ¨¡æ€ç¯å¢ƒä¸­å®ç°å¯é çš„ç‰©ä½“æ£€æµ‹ï¼Œä½†å½“å‰çŸ¥è¯†åœ¨è·¨æ¨¡æ€æ„ŸçŸ¥ã€ä¸Šä¸‹æ–‡æ¨ç†å’ŒååŒæ™ºèƒ½æ–¹é¢ä»ç„¶ç¢ç‰‡åŒ–ï¼Œè¯¥ç ”ç©¶æ—¨åœ¨å¼¥åˆè¿™ä¸€å·®è·ï¼Œé¿å…é‡æ–°å®¡è§†è¿‡æ—¶æŠ€æœ¯ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶ç³»ç»Ÿå›é¡¾äº†è‡ªåŠ¨é©¾é©¶ä¼ æ„Ÿå™¨ï¼ˆç›¸æœºã€è¶…å£°æ³¢ã€æ¿€å…‰é›·è¾¾å’Œé›·è¾¾ï¼‰åŠå…¶èåˆç­–ç•¥ï¼Œå¼•å…¥äº†è¶…è¶Šç®€å•æ”¶é›†çš„ç»“æ„åŒ–æ•°æ®é›†åˆ†ç±»ï¼ŒåŒ…æ‹¬è‡ªè½¦ã€åŸºç¡€è®¾æ–½å’ŒååŒæ•°æ®é›†ï¼Œå¹¶åˆ†æäº†ä»2D/3Dæ£€æµ‹æµç¨‹åˆ°æ··åˆä¼ æ„Ÿå™¨èåˆçš„å‰æ²¿æ–¹æ³•ï¼Œç‰¹åˆ«å…³æ³¨ç”±è§†è§‰å˜æ¢å™¨ã€å¤§å°è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å˜æ¢å™¨æ–¹æ³•ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡ç»¼åˆå¤šè§†è§’åˆ†æï¼Œè¯¥ç ”ç©¶æä¾›äº†å½“å‰èƒ½åŠ›ã€å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥æœºé‡çš„æ¸…æ™°è·¯çº¿å›¾ï¼Œå¼ºè°ƒäº†æ–°å…´èŒƒå¼ä¸ä¼ ç»Ÿä¼ æ„Ÿå™¨èåˆçš„æ•´åˆæ½œåŠ›ï¼Œå¹¶å¯¹ä¸åŒæ•°æ®ç»“æ„å’Œç‰¹å¾è¿›è¡Œäº†äº¤å‰åˆ†æã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç»¼è¿°é€šè¿‡æ•´åˆæ–°å…´AIèŒƒå¼ä¸ä¼ ç»Ÿæ„ŸçŸ¥æ¡†æ¶ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ç‰©ä½“æ£€æµ‹é¢†åŸŸæä¾›äº†ç³»ç»Ÿæ€§çš„å‘å±•è“å›¾ï¼ŒæŒ‡å‡ºäº†å‘å¤šæ¨¡æ€ååŒæ™ºèƒ½å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ç³»ç»Ÿæ¼”è¿›çš„å…³é”®æ–¹å‘ï¼Œä¸ºæœªæ¥ç ”ç©¶å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Autonomous Vehicles (AVs) are transforming the future of transportation
through advances in intelligent perception, decision-making, and control
systems. However, their success is tied to one core capability, reliable object
detection in complex and multimodal environments. While recent breakthroughs in
Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable
progress, the field still faces a critical challenge as knowledge remains
fragmented across multimodal perception, contextual reasoning, and cooperative
intelligence. This survey bridges that gap by delivering a forward-looking
analysis of object detection in AVs, emphasizing emerging paradigms such as
Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI
rather than re-examining outdated techniques. We begin by systematically
reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,
and Radar) and their fusion strategies, highlighting not only their
capabilities and limitations in dynamic driving environments but also their
potential to integrate with recent advances in LLM/VLM-driven perception
frameworks. Next, we introduce a structured categorization of AV datasets that
moves beyond simple collections, positioning ego-vehicle, infrastructure-based,
and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a
cross-analysis of data structures and characteristics. Ultimately, we analyze
cutting-edge detection methodologies, ranging from 2D and 3D pipelines to
hybrid sensor fusion, with particular attention to emerging transformer-driven
approaches powered by Vision Transformers (ViTs), Large and Small Language
Models (SLMs), and VLMs. By synthesizing these perspectives, our survey
delivers a clear roadmap of current capabilities, open challenges, and future
opportunities.</p>
<h3 id="26-steervlm-robust-model-control-through-lightweight-activation-steering-for-vision-language-models">[26] <a href="https://arxiv.org/abs/2510.26769">SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models</a></h3>
<p><em>Anushka Sivakumar, Andrew Zhang, Zaber Hakim, Chris Thomas</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSteerVLMï¼Œä¸€ç§è½»é‡çº§å¼•å¯¼æ¨¡å—ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´è§†è§‰è¯­è¨€æ¨¡å‹ä¸­è¯­è¨€æ¨¡æ€ä¸å›¾åƒä¸Šä¸‹æ–‡ä¹‹é—´çš„æ¿€æ´»è¿æ¥ï¼Œå®ç°æ— éœ€ä¿®æ”¹æ¨¡å‹æƒé‡çš„ç»†ç²’åº¦æ¨ç†æ—¶æ§åˆ¶ã€‚è¯¥æ–¹æ³•ä»…éœ€å­¦ä¹ åŸå§‹VLMå‚æ•°0.14%çš„å‚æ•°é‡ï¼Œåœ¨å¼•å¯¼å’Œå¹»è§‰ç¼“è§£åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰å¹²é¢„æŠ€æœ¯ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¾“å‡ºè¯­ä¹‰æ§åˆ¶æ–¹é¢å­˜åœ¨å±€é™ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿåœ¨æ¨ç†æ—¶å¯¹å¤æ‚è¾“å‡ºè¯­ä¹‰è¿›è¡Œç»†ç²’åº¦æ§åˆ¶çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒéç›®æ ‡ä»»åŠ¡æ€§èƒ½ä¸”ä¸ä¿®æ”¹æ¨¡å‹æƒé‡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢„æå–é™æ€å‘é‡æˆ–æ‰‹åŠ¨è°ƒæ•´å¹²é¢„ç‚¹ï¼Œç¼ºä¹åŠ¨æ€é€‚åº”æ€§ã€‚</p>
<p><strong>Method:</strong> SteerVLMé€šè¿‡å­¦ä¹ ç¼–ç ç›®æ ‡è¡Œä¸ºå’Œç›¸åè¡Œä¸ºçš„é…å¯¹æç¤ºçš„æ½œåœ¨åµŒå…¥ï¼ŒåŠ¨æ€è°ƒæ•´è¿æ¥è¯­è¨€æ¨¡æ€ä¸å›¾åƒä¸Šä¸‹æ–‡çš„æ¿€æ´»è¿æ¥ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ç»´åº¦çº§æ¿€æ´»è°ƒåˆ¶å’Œè·¨å±‚è‡ªé€‚åº”å¼•å¯¼ï¼Œæ— éœ€é¢„æå–é™æ€å‘é‡æˆ–æ‰‹åŠ¨è°ƒæ•´å¹²é¢„ç‚¹ã€‚åŒæ—¶å¼•å…¥äº†VNIAå¤šæ¨¡æ€æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºå¼€å‘å’Œè¯„ä¼°VLMå¼•å¯¼æŠ€æœ¯ã€‚</p>
<p><strong>Result:</strong> SteerVLMåœ¨VLMå¼•å¯¼å’Œå¹»è§‰ç¼“è§£åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰å¹²é¢„æŠ€æœ¯ï¼Œä»…éœ€å­¦ä¹ åŸå§‹VLMå‚æ•°0.14%çš„å‚æ•°é‡ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹å®ç°ç»†ç²’åº¦æ¨ç†æ—¶æ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒéç›®æ ‡ä»»åŠ¡çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡æ¿€æ´»å·¥ç¨‹ä¸ºå¤šæ¨¡æ€æ¨¡å‹æ§åˆ¶æä¾›äº†ç¨³å¥è§£å†³æ–¹æ¡ˆï¼Œè¯æ˜äº†è½»é‡çº§å¼•å¯¼æ¨¡å—åœ¨å®ç°å¤æ‚è¾“å‡ºè¯­ä¹‰æ§åˆ¶æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚SteerVLMçš„æ–¹æ³•ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç²¾ç¡®è¡Œä¸ºè°ƒæ§å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>This work introduces SteerVLM, a lightweight steering module designed to
guide Vision-Language Models (VLMs) towards outputs that better adhere to
desired instructions. Our approach learns from the latent embeddings of paired
prompts encoding target and converse behaviors to dynamically adjust
activations connecting the language modality with image context. This allows
for fine-grained, inference-time control over complex output semantics without
modifying model weights while preserving performance on off-target tasks. Our
steering module requires learning parameters equal to 0.14% of the original
VLM's size. Our steering module gains model control through dimension-wise
activation modulation and adaptive steering across layers without requiring
pre-extracted static vectors or manual tuning of intervention points.
Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a
multimodal dataset specifically created to facilitate the development and
evaluation of VLM steering techniques. Our method outperforms existing
intervention techniques on steering and hallucination mitigation benchmarks for
VLMs and proposes a robust solution for multimodal model control through
activation engineering.</p>
<h3 id="27-chartab-a-benchmark-for-chart-grounding-dense-alignment">[27] <a href="https://arxiv.org/abs/2510.26781">ChartAB: A Benchmark for Chart Grounding &amp; Dense Alignment</a></h3>
<p><em>Aniruddh Bansal, Davit Soselia, Dang Nguyen, Tianyi Zhou</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ChartAlignåŸºå‡†æµ‹è¯•ï¼ˆChartABï¼‰ï¼Œç”¨äºå…¨é¢è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾è¡¨ç†è§£ä»»åŠ¡ä¸­çš„ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨å›¾è¡¨å…ƒç´ å®šä½ã€å±æ€§è¯†åˆ«å’Œå¤šå›¾è¡¨æ¯”è¾ƒæ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾è¡¨ç†è§£ä»»åŠ¡ä¸­ç¼ºä¹å‡†ç¡®çš„ç»†èŠ‚æ„ŸçŸ¥èƒ½åŠ›ï¼Œéš¾ä»¥æå–å›¾è¡¨çš„ç»†ç²’åº¦ç»“æ„ï¼Œè¿™ç§å›¾è¡¨å®šä½èƒ½åŠ›çš„é™åˆ¶è¿›ä¸€æ­¥é˜»ç¢äº†æ¨¡å‹è¿›è¡Œå¤šå›¾è¡¨æ¯”è¾ƒå’Œæ¨ç†çš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶è®¾è®¡äº†ChartAlignåŸºå‡†æµ‹è¯•ï¼Œé‡‡ç”¨JSONæ¨¡æ¿æ¥ä¿ƒè¿›é’ˆå¯¹æ¯ä¸ªå®šä½ä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡è®¡ç®—ï¼Œå¹¶å¼•å…¥æ–°é¢–çš„ä¸¤é˜¶æ®µæ¨ç†å·¥ä½œæµç¨‹æ¥è¯„ä¼°æ¨¡å‹åœ¨å¤šä¸ªå›¾è¡¨é—´å¯¹é½å’Œæ¯”è¾ƒå…ƒç´ /å±æ€§çš„èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å¯¹å¤šä¸ªæœ€æ–°è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°åˆ†ææ­ç¤ºäº†å®ƒä»¬åœ¨å›¾è¡¨ç†è§£ä»»åŠ¡ä¸­çš„æ„ŸçŸ¥åå·®ã€å¼±ç‚¹ã€é²æ£’æ€§å’Œå¹»è§‰é—®é¢˜ï¼Œè¿™äº›å‘ç°çªå‡ºäº†ä¸åŒæ¨¡å‹åœ¨å›¾è¡¨ç†è§£ä»»åŠ¡ä¸­çš„ç»†ç²’åº¦å·®å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœæŒ‡å‡ºäº†å½“å‰æ¨¡å‹éœ€è¦åŠ å¼ºçš„å…·ä½“æŠ€èƒ½ï¼Œä¸ºæ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾è¡¨ç†è§£é¢†åŸŸçš„æ€§èƒ½æä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œå¼ºè°ƒäº†ç»†ç²’åº¦æ„ŸçŸ¥èƒ½åŠ›åœ¨å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Charts play an important role in visualization, reasoning, data analysis, and
the exchange of ideas among humans. However, existing vision-language models
(VLMs) still lack accurate perception of details and struggle to extract
fine-grained structures from charts. Such limitations in chart grounding also
hinder their ability to compare multiple charts and reason over them. In this
paper, we introduce a novel "ChartAlign Benchmark (ChartAB)" to provide a
comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting
tabular data, localizing visualization elements, and recognizing various
attributes from charts of diverse types and complexities. We design a JSON
template to facilitate the calculation of evaluation metrics specifically
tailored for each grounding task. By incorporating a novel two-stage inference
workflow, the benchmark can further evaluate VLMs' capability to align and
compare elements/attributes across two charts. Our analysis of evaluations on
several recent VLMs reveals new insights into their perception biases,
weaknesses, robustness, and hallucinations in chart understanding. These
findings highlight the fine-grained discrepancies among VLMs in chart
understanding tasks and point to specific skills that need to be strengthened
in current models.</p>
<h3 id="28-the-quest-for-generalizable-motion-generation-data-model-and-evaluation">[28] <a href="https://arxiv.org/abs/2510.26794">The Quest for Generalizable Motion Generation: Data, Model, and Evaluation</a></h3>
<p><em>Jing Lin, Ruisi Wang, Junzhe Lu, Ziqi Huang, Guorui Song, Ailing Zeng, Xian Liu, Chen Wei, Wanqi Yin, Qingping Sun, Zhongang Cai, Lei Yang, Ziwei Liu</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä»è§†é¢‘ç”Ÿæˆå‘3Däººä½“è¿åŠ¨ç”Ÿæˆç³»ç»ŸåŒ–è¿ç§»çŸ¥è¯†çš„æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†ViMoGen-228Kã€å¼€å‘åŸºäºæµåŒ¹é…çš„æ‰©æ•£å˜æ¢å™¨æ¨¡å‹ä»¥åŠå»ºç«‹åˆ†å±‚è¯„ä¼°åŸºå‡†MBenchï¼Œæ˜¾è‘—æå‡äº†è¿åŠ¨ç”Ÿæˆçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰3Däººä½“è¿åŠ¨ç”Ÿæˆæ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›æ–¹é¢å­˜åœ¨æ ¹æœ¬æ€§ç“¶é¢ˆï¼Œè€Œç›¸é‚»çš„è§†é¢‘ç”Ÿæˆé¢†åŸŸåœ¨å»ºæ¨¡äººç±»è¡Œä¸ºæ–¹é¢å·²å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–æ€§èƒ½ï¼Œè¿™ä¸ºè¿åŠ¨ç”Ÿæˆæä¾›äº†å¯è¿ç§»çš„æ´å¯Ÿã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åŸºäºæµåŒ¹é…çš„æ‰©æ•£å˜æ¢å™¨ViMoGenï¼Œé€šè¿‡é—¨æ§å¤šæ¨¡æ€æ¡ä»¶æœºåˆ¶ç»Ÿä¸€äº†MoCapæ•°æ®å’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼›åŒæ—¶å¼€å‘äº†ViMoGen-lightè’¸é¦å˜ä½“ï¼Œåœ¨æ¶ˆé™¤è§†é¢‘ç”Ÿæˆä¾èµ–çš„åŒæ—¶ä¿æŒå¼ºæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ„å»ºçš„ViMoGen-228Kæ•°æ®é›†åŒ…å«228,000ä¸ªé«˜è´¨é‡è¿åŠ¨æ ·æœ¬ï¼Œå¤§å¹…æ‰©å±•äº†è¯­ä¹‰å¤šæ ·æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ä»è§†é¢‘ç”Ÿæˆå‘è¿åŠ¨ç”Ÿæˆè¿›è¡ŒçŸ¥è¯†è¿ç§»çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæå‡è¿åŠ¨ç”Ÿæˆæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æä¾›äº†ç³»ç»Ÿæ€§è§£å†³æ–¹æ¡ˆï¼Œæœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢è·¨æ¨¡æ€ç”Ÿæˆä»»åŠ¡çš„ç»Ÿä¸€æ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>Despite recent advances in 3D human motion generation (MoGen) on standard
benchmarks, existing models still face a fundamental bottleneck in their
generalization capability. In contrast, adjacent generative fields, most
notably video generation (ViGen), have demonstrated remarkable generalization
in modeling human behaviors, highlighting transferable insights that MoGen can
leverage. Motivated by this observation, we present a comprehensive framework
that systematically transfers knowledge from ViGen to MoGen across three key
pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a
large-scale dataset comprising 228,000 high-quality motion samples that
integrates high-fidelity optical MoCap data with semantically annotated motions
from web videos and synthesized samples generated by state-of-the-art ViGen
models. The dataset includes both text-motion pairs and text-video-motion
triplets, substantially expanding semantic diversity. Second, we propose
ViMoGen, a flow-matching-based diffusion transformer that unifies priors from
MoCap data and ViGen models through gated multimodal conditioning. To enhance
efficiency, we further develop ViMoGen-light, a distilled variant that
eliminates video generation dependencies while preserving strong
generalization. Finally, we present MBench, a hierarchical benchmark designed
for fine-grained evaluation across motion quality, prompt fidelity, and
generalization ability. Extensive experiments show that our framework
significantly outperforms existing approaches in both automatic and human
evaluations. The code, data, and benchmark will be made publicly available.</p>
<h3 id="29-scaling-image-geo-localization-to-continent-level">[29] <a href="https://arxiv.org/abs/2510.26795">Scaling Image Geo-Localization to Continent Level</a></h3>
<p><em>Philipp Lindenberger, Paul-Edouard Sarlin, Jan Hosang, Matteo Balice, Marc Pollefeys, Simon Lynen, Eduard Trulls</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œé€šè¿‡ä»£ç†åˆ†ç±»ä»»åŠ¡å­¦ä¹ ä¸°å¯Œç‰¹å¾è¡¨ç¤ºï¼Œç»“åˆåœ°é¢å’Œèˆªç©ºå›¾åƒåµŒå…¥ï¼Œå®ç°äº†åœ¨å¤§é™†å°ºåº¦ä¸Šçš„ç»†ç²’åº¦åœ°ç†å®šä½ï¼Œèƒ½å¤Ÿåœ¨æ¬§æ´²åœ°åŒº68%çš„æŸ¥è¯¢ä¸­å®šä½åˆ°200ç±³èŒƒå›´å†…ã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å…¨çƒå°ºåº¦å›¾åƒåœ°ç†å®šä½é¢ä¸´æ ‡å‡†å›¾åƒæ£€ç´¢æ–¹æ³•æ•ˆç‡ä½ä¸‹å’Œè¦†ç›–ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰å¯æ‰©å±•è§£å†³æ–¹æ¡ˆå­˜åœ¨ç²—ç²’åº¦åˆ†ç±»ä¸è·¨è§†å›¾æ£€ç´¢é¢†åŸŸå·®è·çš„æƒè¡¡ï¼Œéœ€è¦åœ¨å¤§åœ°ç†èŒƒå›´å†…å®ç°ç»†ç²’åº¦å®šä½ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨æ··åˆæ–¹æ³•ï¼Œåœ¨è®­ç»ƒæœŸé—´åˆ©ç”¨ä»£ç†åˆ†ç±»ä»»åŠ¡å­¦ä¹ éšå«ç¼–ç ç²¾ç¡®ä½ç½®ä¿¡æ¯çš„ä¸°å¯Œç‰¹å¾è¡¨ç¤ºï¼Œå¹¶å°†å­¦ä¹ åˆ°çš„åŸå‹ä¸èˆªç©ºå›¾åƒåµŒå…¥ç›¸ç»“åˆï¼Œå¢å¼ºå¯¹åœ°é¢æ•°æ®ç¨€ç–æ€§çš„é²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨è¦†ç›–æ¬§æ´²å¤§éƒ¨åˆ†åœ°åŒºçš„æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨68%ä»¥ä¸Šçš„æŸ¥è¯¢ä¸­å®ç°200ç±³èŒƒå›´å†…çš„ç²¾ç¡®å®šä½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•è¯æ˜äº†é€šè¿‡ç»“åˆå­¦ä¹ ç‰¹å¾å’Œè·¨è§†å›¾åµŒå…¥ï¼Œå¯ä»¥åœ¨å¤§é™†å°ºåº¦ä¸Šå®ç°ç»†ç²’åº¦åœ°ç†å®šä½ï¼Œä¸ºå¤§è§„æ¨¡åœ°ç†å®šä½ç³»ç»Ÿæä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Determining the precise geographic location of an image at a global scale
remains an unsolved challenge. Standard image retrieval techniques are
inefficient due to the sheer volume of images (&gt;100M) and fail when coverage is
insufficient. Scalable solutions, however, involve a trade-off: global
classification typically yields coarse results (10+ kilometers), while
cross-view retrieval between ground and aerial imagery suffers from a domain
gap and has been primarily studied on smaller regions. This paper introduces a
hybrid approach that achieves fine-grained geo-localization across a large
geographic expanse the size of a continent. We leverage a proxy classification
task during training to learn rich feature representations that implicitly
encode precise location information. We combine these learned prototypes with
embeddings of aerial imagery to increase robustness to the sparsity of
ground-level data. This enables direct, fine-grained retrieval over areas
spanning multiple countries. Our extensive evaluation demonstrates that our
approach can localize within 200m more than 68\% of queries of a dataset
covering a large part of Europe. The code is publicly available at
https://scaling-geoloc.github.io.</p>
<h3 id="30-omnix-from-unified-panoramic-generation-and-perception-to-graphics-ready-3d-scenes">[30] <a href="https://arxiv.org/abs/2510.26800">OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes</a></h3>
<p><em>Yukun Huang, Jiwen Yu, Yanning Zhou, Jianan Wang, Xintao Wang, Pengfei Wan, Xihui Liu</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†OmniXæ¡†æ¶ï¼Œé€šè¿‡é‡æ–°åˆ©ç”¨2Dç”Ÿæˆå…ˆéªŒå®ç°å…¨æ™¯æ„ŸçŸ¥ï¼Œèƒ½å¤Ÿç”Ÿæˆé€‚ç”¨äºç‰©ç†æ¸²æŸ“ã€é‡å…‰ç…§å’Œä»¿çœŸçš„å›¾å½¢å°±ç»ª3Dåœºæ™¯ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿ2Dæå‡æ–¹æ³•ä»…å…³æ³¨å¤–è§‚ç”Ÿæˆçš„å±€é™ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºå…¨æ™¯å›¾çš„2Dæå‡æ–¹æ³•ä¸»è¦å…³æ³¨å¤–è§‚ç”Ÿæˆï¼Œè€Œå¿½ç•¥äº†å†…åœ¨å±æ€§çš„æ„ŸçŸ¥ï¼Œæ— æ³•ç”Ÿæˆé€‚ç”¨äºç‰©ç†æ¸²æŸ“ã€é‡å…‰ç…§å’Œä»¿çœŸçš„å›¾å½¢å°±ç»ª3Dåœºæ™¯ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œå°†å…¨æ™¯å›¾æŠ€æœ¯æ¨è¿›åˆ°èƒ½å¤Ÿç”Ÿæˆå…·å¤‡å®Œæ•´ç‰©ç†å±æ€§çš„3Dåœºæ™¯ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†OmniXç»Ÿä¸€æ¡†æ¶ï¼ŒåŸºäºè½»é‡é«˜æ•ˆçš„è·¨æ¨¡æ€é€‚é…å™¨ç»“æ„ï¼Œé‡æ–°åˆ©ç”¨2Dç”Ÿæˆå…ˆéªŒè¿›è¡Œå‡ ä½•ã€çº¹ç†å’ŒPBRæè´¨çš„å…¨æ™¯æ„ŸçŸ¥ã€‚åŒæ—¶æ„å»ºäº†å¤§è§„æ¨¡åˆæˆå…¨æ™¯æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªå¤šæ ·åŒ–å®¤å†…å¤–åœºæ™¯çš„é«˜è´¨é‡å¤šæ¨¡æ€å…¨æ™¯å›¾ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¯æ˜äº†æ¨¡å‹åœ¨å…¨æ™¯è§†è§‰æ„ŸçŸ¥å’Œå›¾å½¢å°±ç»ª3Dåœºæ™¯ç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ²‰æµ¸å¼å’Œç‰©ç†çœŸå®çš„è™šæ‹Ÿä¸–ç•Œç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†é‡æ–°åˆ©ç”¨2Dç”Ÿæˆå…ˆéªŒè¿›è¡Œå…¨æ™¯æ„ŸçŸ¥çš„å¯è¡Œæ€§ï¼Œä¸ºç”Ÿæˆå…·å¤‡å®Œæ•´ç‰©ç†å±æ€§çš„3Dåœºæ™¯æä¾›äº†ç»Ÿä¸€æ¡†æ¶ï¼Œæ¨åŠ¨äº†æ²‰æµ¸å¼è™šæ‹Ÿç¯å¢ƒç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>There are two prevalent ways to constructing 3D scenes: procedural generation
and 2D lifting. Among them, panorama-based 2D lifting has emerged as a
promising technique, leveraging powerful 2D generative priors to produce
immersive, realistic, and diverse 3D environments. In this work, we advance
this technique to generate graphics-ready 3D scenes suitable for physically
based rendering (PBR), relighting, and simulation. Our key insight is to
repurpose 2D generative models for panoramic perception of geometry, textures,
and PBR materials. Unlike existing 2D lifting approaches that emphasize
appearance generation and ignore the perception of intrinsic properties, we
present OmniX, a versatile and unified framework. Based on a lightweight and
efficient cross-modal adapter structure, OmniX reuses 2D generative priors for
a broad range of panoramic vision tasks, including panoramic perception,
generation, and completion. Furthermore, we construct a large-scale synthetic
panorama dataset containing high-quality multimodal panoramas from diverse
indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness
of our model in panoramic visual perception and graphics-ready 3D scene
generation, opening new possibilities for immersive and physically realistic
virtual world generation.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="31-listen-to-your-preferences-an-llm-framework-for-multi-objective-selection">[31] <a href="https://arxiv.org/abs/2510.25799">LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection</a></h3>
<p><em>Adam S. Jovine, Tinghan Ye, Francis Bahk, Jingjing Wang, David B. Shmoys, Peter I. Frazier</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†LISTENæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºé›¶æ ·æœ¬åå¥½é¢„æµ‹å™¨ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡å¯¼æ¥ä¼˜åŒ–å¤šç›®æ ‡å†³ç­–é—®é¢˜ï¼Œæ˜¾è‘—é™ä½äº†ä¼ ç»Ÿåå¥½è·å–çš„è®¤çŸ¥è´Ÿæ‹…ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> äººç±»ä¸“å®¶åœ¨é¢å¯¹å¤šç›®æ ‡ç«äº‰çš„å¤§è§„æ¨¡é€‰é¡¹é€‰æ‹©æ—¶å¸¸å¸¸éš¾ä»¥å½¢å¼åŒ–å¤æ‚çš„éšå«åå¥½ï¼Œè¿™ä¸€è¿‡ç¨‹å—åˆ°åå¥½è·å–å›°éš¾çš„ç“¶é¢ˆé™åˆ¶ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿç›´æ¥åˆ©ç”¨è‡ªç„¶è¯­è¨€æŒ‡å¯¼æ¥ç®€åŒ–å¤æ‚å†³ç­–çš„æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†LISTENæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ç§è¿­ä»£ç®—æ³•ï¼šLISTEN-Uä½¿ç”¨LLMä¼˜åŒ–å‚æ•°åŒ–æ•ˆç”¨å‡½æ•°ï¼ŒLISTEN-Té‡‡ç”¨éå‚æ•°æ–¹æ³•åœ¨å°æ‰¹é‡è§£ç©ºé—´ä¸­è¿›è¡Œé”¦æ ‡èµ›å¼é€‰æ‹©ï¼Œä¸¤ç§æ–¹æ³•éƒ½æ—¨åœ¨å…‹æœLLMçš„ä¸Šä¸‹æ–‡çª—å£å’Œæ¨ç†æˆæœ¬é™åˆ¶ã€‚</p>
<p><strong>Result:</strong> åœ¨èˆªç­é¢„è®¢ã€è´­ç‰©å’Œè€ƒè¯•å®‰æ’ç­‰å¤šæ ·åŒ–ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œå½“åå¥½ä¸å‚æ•°åŒ–å¯¹é½æ—¶LISTEN-Uè¡¨ç°ä¼˜å¼‚ï¼Œè€ŒLISTEN-Tæä¾›æ›´ç¨³å¥çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡æ–°é¢–çš„ä¸€è‡´æ€§åº¦é‡æ¥é‡åŒ–åå¥½å¯¹é½ç¨‹åº¦ã€‚</p>
<p><strong>Conclusion:</strong> è¿™é¡¹å·¥ä½œæ¢ç´¢äº†ä½¿ç”¨è‡ªç„¶è¯­è¨€ç›´æ¥æŒ‡å¯¼å¤æ‚å¤šç›®æ ‡å†³ç­–çš„å¯è¡Œæ–¹å‘ï¼Œä¸ºå‡å°‘ä¼ ç»Ÿåå¥½è·å–çš„è®¤çŸ¥è´Ÿæ‹…æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºLLMåœ¨å†³ç­–æ”¯æŒç³»ç»Ÿä¸­çš„å®é™…åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.</p>
<h3 id="32-distilling-multilingual-vision-language-models-when-smaller-models-stay-multilingual">[32] <a href="https://arxiv.org/abs/2510.26271">Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual</a></h3>
<p><em>Sukrit Sriratanawilai, Jhayahgrit Thongwat, Romrawin Chumpu, Patomporn Payoungkhamdee, Sarana Nutanong, Peerat Limkonchotiwat</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é€šè¿‡æ§åˆ¶å®éªŒåˆ†æäº†çŸ¥è¯†è’¸é¦åœ¨å¤šè¯­è¨€è§†è§‰è¯­è¨€æ¨¡å‹å‹ç¼©ä¸­çš„è¡Œä¸ºï¼Œå‘ç°ä¸åŒè’¸é¦é…ç½®åœ¨è·¨è¯­è¨€è¡¨ç¤ºä¸€è‡´æ€§å’Œä¸‹æ¸¸æ€§èƒ½ç¨³å®šæ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œæ­ç¤ºäº†ä»…é èšåˆå‡†ç¡®ç‡æ— æ³•åæ˜ çš„è®¾è®¡æ•æ„Ÿæƒè¡¡ã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­è¡¨ç°å‡ºæ€§èƒ½ä¸å‡çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹å°ºå¯¸å‡å°æ—¶æ›´ä¸ºä¸¥é‡ï¼Œè€ŒçŸ¥è¯†è’¸é¦åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„åº”ç”¨ä»æ˜¯ä¸€ä¸ªæ¢ç´¢ä¸è¶³çš„é¢†åŸŸï¼Œéœ€è¦ç³»ç»Ÿç ”ç©¶è’¸é¦æ–¹æ³•å¯¹è·¨è¯­è¨€è¡¨ç¤ºä¸€è‡´æ€§å’Œæ¨¡å‹å‹ç¼©ä¸‹æ€§èƒ½ç¨³å®šæ€§çš„å½±å“ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨äº”ç§ä¸åŒçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œåœ¨CLIPå’ŒSigLIP2æ¨¡å‹ä¸Šè¿›è¡Œæ§åˆ¶æ€§å®éªŒï¼Œè¯„ä¼°è¿™äº›æ–¹æ³•åœ¨åŸŸå†…æ£€ç´¢å’ŒåŸŸå¤–è§†è§‰é—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨è·¨è¯­è¨€è¡¨ç¤ºä¸€è‡´æ€§å’Œä¸‹æ¸¸æ€§èƒ½ç¨³å®šæ€§ã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°æŸäº›è’¸é¦é…ç½®èƒ½å¤Ÿåœ¨æ¨¡å‹å°ºå¯¸å‡åŠçš„æƒ…å†µä¸‹ä¿æŒç”šè‡³æå‡å¤šè¯­è¨€æ£€ç´¢çš„é²æ£’æ€§ï¼Œä½†å…¶ä»–é…ç½®æ— æ³•ç»´æŒè·¨ä»»åŠ¡ç¨³å®šæ€§ï¼Œæ­ç¤ºäº†ä»…é èšåˆå‡†ç¡®ç‡æ— æ³•æ•æ‰çš„è®¾è®¡æ•æ„Ÿæƒè¡¡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶å¼ºè°ƒäº†çŸ¥è¯†è’¸é¦æ–¹æ³•é€‰æ‹©å¯¹å¤šè¯­è¨€è§†è§‰è¯­è¨€æ¨¡å‹å‹ç¼©æ•ˆæœçš„å…³é”®å½±å“ï¼ŒæŒ‡å‡ºéœ€è¦ç»¼åˆè€ƒè™‘è·¨è¯­è¨€è¡¨ç¤ºä¸€è‡´æ€§å’Œä»»åŠ¡ç¨³å®šæ€§ï¼Œè€Œéä»…å…³æ³¨æ€»ä½“å‡†ç¡®ç‡ï¼Œä¸ºå¤šè¯­è¨€æ¨¡å‹å‹ç¼©æä¾›äº†é‡è¦è®¾è®¡æŒ‡å¯¼ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.</p>
<h3 id="33-missynth-improving-missci-logical-fallacies-classification-with-synthetic-data">[33] <a href="https://arxiv.org/abs/2510.26345">MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data</a></h3>
<p><em>Mykhailo Poliakov, Nadiya Shvai</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMisSynthç®¡é“ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆåˆæˆè°¬è¯¯æ ·æœ¬å¹¶å¾®è°ƒLLMï¼Œæ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹è¯†åˆ«ç§‘å­¦å¥åº·ç›¸å…³é”™è¯¯ä¿¡æ¯çš„èƒ½åŠ›ï¼Œåœ¨æœ‰é™è®¡ç®—èµ„æºä¸‹å®ç°äº†è¶…è¿‡35%çš„F1åˆ†æ•°ç»å¯¹æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¥åº·ç›¸å…³é”™è¯¯ä¿¡æ¯æ™®éå­˜åœ¨ä¸”å…·æœ‰æ½œåœ¨å±å®³æ€§ï¼Œç‰¹åˆ«æ˜¯å½“è¿™äº›å£°æ˜æ‰­æ›²æˆ–æ›²è§£ç§‘å­¦å‘ç°æ—¶éš¾ä»¥è¯†åˆ«ã€‚ç°æœ‰æ ‡æ³¨èµ„æºæœ‰é™ï¼Œéœ€è¦æ¢ç´¢åœ¨è®¡ç®—èµ„æºå—é™æƒ…å†µä¸‹æå‡LLMè¯†åˆ«è°¬è¯¯è®ºè¯èƒ½åŠ›çš„æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æå‡ºMisSynthç®¡é“ï¼Œåº”ç”¨æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ç”Ÿæˆåˆæˆè°¬è¯¯æ ·æœ¬ï¼Œç„¶åä½¿ç”¨è¿™äº›æ ·æœ¬å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¯¥æ–¹æ³•åŸºäºMISSCIæ•°æ®é›†å’Œæ¡†æ¶ï¼Œé€šè¿‡åˆæˆæ•°æ®å¢å¼ºæœ‰é™çš„æ ‡æ³¨èµ„æºã€‚</p>
<p><strong>Result:</strong> å¾®è°ƒæ¨¡å‹ç›¸æ¯”åŸå§‹åŸºçº¿æ¨¡å‹å®ç°äº†æ˜¾è‘—çš„å‡†ç¡®ç‡æå‡ï¼Œå…¶ä¸­LLaMA 3.1 8Bå¾®è°ƒæ¨¡å‹åœ¨MISSCIæµ‹è¯•é›†ä¸Šç›¸æ¯”åŸå§‹åŸºçº¿å®ç°äº†è¶…è¿‡35%çš„F1åˆ†æ•°ç»å¯¹æå‡ã€‚å®éªŒè¯æ˜åˆæˆè°¬è¯¯æ•°æ®èƒ½å¤Ÿæ˜¾è‘—å¢å¼ºé›¶æ ·æœ¬LLMåœ¨çœŸå®ä¸–ç•Œç§‘å­¦é”™è¯¯ä¿¡æ¯ä»»åŠ¡ä¸Šçš„åˆ†ç±»æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å¼•å…¥åˆæˆè°¬è¯¯æ•°æ®å¯ä»¥æœ‰æ•ˆå¢å¼ºæœ‰é™æ ‡æ³¨èµ„æºä¸‹çš„æ¨¡å‹æ€§èƒ½ï¼Œå³ä½¿åœ¨è®¡ç®—èµ„æºå—é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æ˜¾è‘—æå‡LLMè¯†åˆ«ç§‘å­¦é”™è¯¯ä¿¡æ¯çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ºé”™è¯¯ä¿¡æ¯æ£€æµ‹æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œç›¸å…³ä»£ç å’Œåˆæˆæ•°æ®é›†å·²å¼€æºã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.</p>
<h3 id="34-hebrew-diacritics-restoration-using-visual-representation">[34] <a href="https://arxiv.org/abs/2510.26521">Hebrew Diacritics Restoration using Visual Representation</a></h3>
<p><em>Yair Elboher, Yuval Pinter</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDIVRITç³»ç»Ÿï¼Œå°†å¸Œä¼¯æ¥è¯­å˜éŸ³ç¬¦å·æ¢å¤ä»»åŠ¡æ„å»ºä¸ºé›¶æ ·æœ¬åˆ†ç±»é—®é¢˜ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹å°†æœªå˜éŸ³æ–‡æœ¬ä½œä¸ºå›¾åƒå¤„ç†ï¼Œåœ¨å€™é€‰é›†åŒ…å«æ­£ç¡®å½¢å¼æ—¶å®ç°é«˜ç²¾åº¦å˜éŸ³æ¢å¤ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¸Œä¼¯æ¥è¯­å˜éŸ³ç¬¦å·æ¢å¤æ˜¯ç¡®ä¿å‡†ç¡®å‘éŸ³å’Œæ¶ˆé™¤æ–‡æœ¬æ­§ä¹‰çš„å…³é”®ä»»åŠ¡ï¼Œå°½ç®¡æœªå˜éŸ³æ–‡æœ¬å­˜åœ¨é«˜åº¦æ­§ä¹‰æ€§ï¼Œç°æœ‰æ–¹æ³•ä»éœ€è¦æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆæ¥å¤„ç†è¿™ä¸€å¤æ‚çš„è¯­è¨€å¤„ç†é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> DIVRITç³»ç»Ÿå°†å˜éŸ³æ¢å¤æ„å»ºä¸ºåŸºäºä¸Šä¸‹æ–‡æ¡ä»¶çš„é›¶æ ·æœ¬åˆ†ç±»é—®é¢˜ï¼Œä½¿ç”¨å¸Œä¼¯æ¥è§†è§‰è¯­è¨€æ¨¡å‹å°†æœªå˜éŸ³æ–‡æœ¬ä½œä¸ºå›¾åƒå¤„ç†ï¼Œä»åŠ¨æ€ç”Ÿæˆçš„å€™é€‰é›†ä¸­ä¸ºæ¯ä¸ªå•è¯é€‰æ‹©æœ€åˆé€‚çš„å˜éŸ³æ¨¡å¼ã€‚</p>
<p><strong>Result:</strong> åœ¨å…¨é¢è¯„ä¼°ä¸­ï¼Œç³»ç»Ÿåœ¨ä¸ä¾èµ–å¤æ‚æ˜¾å¼è¯­è¨€åˆ†æçš„æƒ…å†µä¸‹æœ‰æ•ˆæ‰§è¡Œå˜éŸ³æ¢å¤ï¼Œåœ¨æ­£ç¡®å˜éŸ³å½¢å¼ä¿è¯å­˜åœ¨äºå€™é€‰é›†çš„oracleè®¾ç½®ä¸‹è¾¾åˆ°é«˜å‡†ç¡®ç‡ï¼Œæ¶æ„ä¼˜åŒ–å’Œè®­ç»ƒæ–¹æ³•æ”¹è¿›æ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜è§†è§‰è¡¨ç¤ºåœ¨å¸Œä¼¯æ¥è¯­è‡ªåŠ¨å˜éŸ³æ¢å¤ä¸­å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œè¯¥æ–¹æ³•é¿å…äº†å¤æ‚çš„è¯­è¨€åˆ†æä¾èµ–ï¼Œä¸ºåŸºäºè§†è§‰çš„è¯­è¨€å¤„ç†ä»»åŠ¡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’Œä¼˜åŒ–æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>Diacritics restoration in Hebrew is a fundamental task for ensuring accurate
word pronunciation and disambiguating textual meaning. Despite the language's
high degree of ambiguity when unvocalized, recent machine learning approaches
have significantly advanced performance on this task.
  In this work, we present DIVRIT, a novel system for Hebrew diacritization
that frames the task as a zero-shot classification problem. Our approach
operates at the word level, selecting the most appropriate diacritization
pattern for each undiacritized word from a dynamically generated candidate set,
conditioned on the surrounding textual context. A key innovation of DIVRIT is
its use of a Hebrew Visual Language Model, which processes undiacritized text
as an image, allowing diacritic information to be embedded directly within the
input's vector representation.
  Through a comprehensive evaluation across various configurations, we
demonstrate that the system effectively performs diacritization without relying
on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting
where the correct diacritized form is guaranteed to be among the provided
candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic
architectural enhancements and optimized training methodologies yield
significant improvements in the system's overall generalization capabilities.
These findings highlight the promising potential of visual representations for
accurate and automated Hebrew diacritization.</p>
<h3 id="35-slideagent-hierarchical-agentic-framework-for-multi-page-visual-document-understanding">[35] <a href="https://arxiv.org/abs/2510.26615">SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding</a></h3>
<p><em>Yiqiao Jin, Rachneet Kaur, Zhen Zeng, Sumitra Ganesh, Srijan Kumar</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SlideAgentï¼Œä¸€ä¸ªç”¨äºç†è§£å¤šæ¨¡æ€ã€å¤šé¡µé¢ã€å¤šå¸ƒå±€æ–‡æ¡£çš„æ™ºèƒ½ä»£ç†æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚ä¸“ä¸šåŒ–ä»£ç†å®ç°ä»å…¨å±€åˆ°å…ƒç´ çš„ç»†ç²’åº¦æ¨ç†ï¼Œåœ¨å¤æ‚è§†è§‰æ–‡æ¡£ç†è§£ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰ä¸“æœ‰å’Œå¼€æºæ¨¡å‹ã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚å¤šé¡µé¢è§†è§‰æ–‡æ¡£æ—¶å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨é¡µé¢å…ƒç´ ç»†ç²’åº¦æ¨ç†æ–¹é¢å­˜åœ¨å±€é™ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶ä¸ºæ–‡æ¡£ç†è§£æä¾›äº†æœºä¼šï¼Œä½†å°šæœªæœ‰æ•ˆè§£å†³å¤šé¡µé¢è§†è§‰æ–‡æ¡£çš„å¤æ‚ç†è§£æŒ‘æˆ˜ã€‚</p>
<p><strong>Method:</strong> SlideAgenté‡‡ç”¨åˆ†å±‚ä¸“ä¸šåŒ–ä»£ç†æ¡†æ¶ï¼Œå°†æ¨ç†åˆ†è§£ä¸ºå…¨å±€ã€é¡µé¢å’Œå…ƒç´ ä¸‰ä¸ªä¸“é—¨å±‚çº§ï¼Œæ„å»ºç»“æ„åŒ–çš„æŸ¥è¯¢æ— å…³è¡¨ç¤ºï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€‰æ‹©æ€§æ¿€æ´»ä¸åŒå±‚çº§ä»£ç†å¹¶æ•´åˆå…¶è¾“å‡ºä»¥ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç­”æ¡ˆã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSlideAgentç›¸æ¯”ä¸“æœ‰æ¨¡å‹å®ç°äº†+7.9çš„æ•´ä½“æ€§èƒ½æå‡ï¼Œç›¸æ¯”å¼€æºæ¨¡å‹å®ç°äº†+9.8çš„æ•´ä½“æ€§èƒ½æå‡ï¼Œåœ¨å¤æ‚å¤šé¡µé¢è§†è§‰æ–‡æ¡£ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åˆ†å±‚ä¸“ä¸šåŒ–ä»£ç†æ¡†æ¶åœ¨å¤æ‚å¤šé¡µé¢è§†è§‰æ–‡æ¡£ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤šæ¨¡æ€æ–‡æ¡£æ™ºèƒ½åˆ†ææä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†ä»£ç†æ¶æ„åœ¨å¤„ç†ç»†ç²’åº¦è·¨é¡µé¢æ¨ç†ä»»åŠ¡ä¸Šçš„ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>Multi-page visual documents such as manuals, brochures, presentations, and
posters convey key information through layout, colors, icons, and cross-slide
references. While large language models (LLMs) offer opportunities in document
understanding, current systems struggle with complex, multi-page visual
documents, particularly in fine-grained reasoning over elements and pages. We
introduce SlideAgent, a versatile agentic framework for understanding
multi-modal, multi-page, and multi-layout documents, especially slide decks.
SlideAgent employs specialized agents and decomposes reasoning into three
specialized levels-global, page, and element-to construct a structured,
query-agnostic representation that captures both overarching themes and
detailed visual or textual cues. During inference, SlideAgent selectively
activates specialized agents for multi-level reasoning and integrates their
outputs into coherent, context-aware answers. Extensive experiments show that
SlideAgent achieves significant improvement over both proprietary (+7.9
overall) and open-source models (+9.8 overall).</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="36-medsae-dissecting-medclip-representations-with-sparse-autoencoders">[36] <a href="https://arxiv.org/abs/2510.26411">MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders</a></h3>
<p><em>Riccardo Renzulli, Colas Lepoutre, Enrico Cassano, Marco Grangetto</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºåŒ»å­¦ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆMedSAEsï¼‰åº”ç”¨äºMedCLIPçš„æ½œåœ¨ç©ºé—´ï¼Œé€šè¿‡ç»“åˆç›¸å…³æ€§æŒ‡æ ‡ã€ç†µåˆ†æå’Œè‡ªåŠ¨ç¥ç»å…ƒå‘½åçš„æ–°è¯„ä¼°æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†åŒ»å­¦è§†è§‰æ¨¡å‹çš„æœºåˆ¶å¯è§£é‡Šæ€§ï¼Œåœ¨CheXpertæ•°æ®é›†ä¸Šå®ç°äº†æ¯”åŸå§‹MedCLIPç‰¹å¾æ›´é«˜çš„å•ä¹‰æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŒ»ç–—äººå·¥æ™ºèƒ½éœ€è¦æ—¢å‡†ç¡®åˆå¯è§£é‡Šçš„æ¨¡å‹ï¼Œå½“å‰ç ”ç©¶æ—¨åœ¨æ¨è¿›åŒ»å­¦è§†è§‰é¢†åŸŸçš„æœºåˆ¶å¯è§£é‡Šæ€§ï¼Œè§£å†³é«˜æ€§èƒ½åŒ»å­¦AIä¸é€æ˜åº¦ä¹‹é—´çš„å·®è·ï¼Œä¸ºä¸´åºŠå¯é è¡¨ç¤ºæä¾›å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å°†åŒ»å­¦ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆMedSAEsï¼‰åº”ç”¨äºMedCLIPè§†è§‰è¯­è¨€æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶æå‡ºç»“åˆç›¸å…³æ€§æŒ‡æ ‡ã€ç†µåˆ†æå’Œé€šè¿‡MedGEMMAåŸºç¡€æ¨¡å‹è¿›è¡Œè‡ªåŠ¨ç¥ç»å…ƒå‘½åçš„ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œä»¥é‡åŒ–å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨CheXpertæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMedSAEç¥ç»å…ƒç›¸æ¯”åŸå§‹MedCLIPç‰¹å¾å®ç°äº†æ›´é«˜çš„å•ä¹‰æ€§å’Œå¯è§£é‡Šæ€§ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨æå‡åŒ»å­¦è§†è§‰æ¨¡å‹é€æ˜åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ¶èµ·äº†é«˜æ€§èƒ½åŒ»å­¦AIä¸é€æ˜åº¦ä¹‹é—´çš„æ¡¥æ¢ï¼Œä¸ºä¸´åºŠå¯é è¡¨ç¤ºæä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ï¼Œæ¨åŠ¨äº†åŒ»å­¦è§†è§‰æ¨¡å‹æœºåˆ¶å¯è§£é‡Šæ€§çš„å‘å±•ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.</p>
<h3 id="37-unveiling-intrinsic-text-bias-in-multimodal-large-language-models-through-attention-key-space-analysis">[37] <a href="https://arxiv.org/abs/2510.26721">Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis</a></h3>
<p><em>Xinhan Zheng, Huyu Wu, Xueting Wang, Haiyun Jiang</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æ­ç¤ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡æœ¬åå¥½æºäºæ³¨æ„åŠ›æœºåˆ¶å†…éƒ¨çš„å…³é”®å‘é‡åˆ†å¸ƒä¸åŒ¹é…ï¼Œè€Œéå¤–éƒ¨æ•°æ®å› ç´ ã€‚ç ”ç©¶å‘ç°è§†è§‰å…³é”®å‘é‡ç›¸å¯¹äºæ–‡æœ¬å…³é”®ç©ºé—´å‘ˆç°åˆ†å¸ƒå¤–ç‰¹æ€§ï¼Œå¯¼è‡´æ³¨æ„åŠ›è®¡ç®—ä¸­è§†è§‰ä¿¡æ¯è¢«ç³»ç»Ÿæ€§ä½ä¼°ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è§†è§‰è¯­è¨€æ•°æ®æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„æ–‡æœ¬åå¥½ï¼Œé™åˆ¶äº†å…¶åŸºäºè§†è§‰è¯æ®è¿›è¡Œæœ‰æ•ˆæ¨ç†çš„èƒ½åŠ›ã€‚ä¸å…ˆå‰å°†è¿™ç§æ–‡æœ¬åè§å½’å› äºæ•°æ®ä¸å¹³è¡¡æˆ–æŒ‡ä»¤è°ƒä¼˜ç­‰å¤–éƒ¨å› ç´ çš„ç ”ç©¶ä¸åŒï¼Œæœ¬ç ”ç©¶æå‡ºè¯¥åè§æºäºæ¨¡å‹å†…éƒ¨æ¶æ„ï¼Œç‰¹åˆ«æ˜¯è§†è§‰å…³é”®å‘é‡åœ¨è¯­è¨€é¢„è®­ç»ƒæœŸé—´å­¦ä¹ çš„æ–‡æœ¬å…³é”®ç©ºé—´ä¸­å‘ˆç°åˆ†å¸ƒå¤–ç‰¹æ€§ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶ä»LLaVAå’ŒQwen2.5-VLæ¨¡å‹ä¸­æå–å…³é”®å‘é‡ï¼Œå¹¶ä½¿ç”¨å®šæ€§ï¼ˆt-SNEï¼‰å’Œå®šé‡ï¼ˆJensen-Shannonæ•£åº¦ï¼‰æ–¹æ³•åˆ†æå…¶åˆ†å¸ƒç»“æ„ã€‚é€šè¿‡æ¯”è¾ƒè§†è§‰å’Œæ–‡æœ¬å…³é”®å‘é‡åœ¨æ³¨æ„åŠ›ç©ºé—´ä¸­çš„åˆ†å¸ƒå·®å¼‚ï¼ŒéªŒè¯è§†è§‰å…³é”®å‘é‡ç›¸å¯¹äºæ–‡æœ¬å…³é”®ç©ºé—´çš„åˆ†å¸ƒå¤–å‡è®¾ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶ç»“æœæä¾›äº†ç›´æ¥è¯æ®è¡¨æ˜è§†è§‰å’Œæ–‡æœ¬å…³é”®å‘é‡åœ¨æ³¨æ„åŠ›ç©ºé—´ä¸­å æ®æ˜æ˜¾ä¸åŒçš„å­ç©ºé—´ã€‚æ¨¡æ€é—´å·®å¼‚åœ¨ç»Ÿè®¡ä¸Šæ˜¾è‘—ï¼Œè¶…è¿‡æ¨¡æ€å†…å˜å¼‚çš„æ•°ä¸ªæ•°é‡çº§ã€‚è§†è§‰å…³é”®å‘é‡åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­è·å¾—çš„ç›¸ä¼¼æ€§å¾—åˆ†ç³»ç»Ÿæ€§è¾ƒä½ï¼Œå¯¼è‡´å…¶åœ¨ä¸Šä¸‹æ–‡è¡¨ç¤ºä¸­çš„åˆ©ç”¨ä¸è¶³ã€‚</p>
<p><strong>Conclusion:</strong> æ–‡æœ¬åè§æºäºæ³¨æ„åŠ›å…³é”®ç©ºé—´å†…éƒ¨çš„å†…åœ¨ä¸å¯¹é½ï¼Œè€Œéä»…æ¥è‡ªå¤–éƒ¨æ•°æ®å› ç´ ã€‚è¿™ä¸€å‘ç°å¯¹å¤šæ¨¡æ€æ¨¡å‹è®¾è®¡å…·æœ‰é‡è¦æ„ä¹‰ï¼Œè¡¨æ˜éœ€è¦é‡æ–°å®¡è§†æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤šæ¨¡æ€èåˆä¸­çš„ä½œç”¨ï¼Œå¹¶å¼€å‘æ›´æœ‰æ•ˆçš„è·¨æ¨¡æ€å¯¹é½æ–¹æ³•ä»¥æ”¹å–„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.</p>
  </article>
</body>
</html>
