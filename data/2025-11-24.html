<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-11-24.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 46]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 10]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-safer-clip-mitigating-nsfw-content-in-vision-language-models-while-preserving-pre-trained-knowledge">[1] <a href="https://arxiv.org/abs/2511.16743">SafeR-CLIP: Mitigating NSFW Content in Vision-Language Models While Preserving Pre-Trained Knowledge</a></h3>
<p><em>Adeel Yousaf, Joseph Fioresi, James Beetham, Amrit Singh Bedi, Mubarak Shah</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSaFeR-CLIPï¼Œä¸€ç§åŸºäºé‚»è¿‘æ„ŸçŸ¥çš„å¾®è°ƒæ¡†æ¶ï¼Œé€šè¿‡å°†ä¸å®‰å…¨æ¦‚å¿µé‡å®šå‘åˆ°è¯­ä¹‰ä¸Šæœ€æ¥è¿‘çš„å®‰å…¨æ›¿ä»£å“æ¥æœ€å°åŒ–è¡¨ç¤ºå˜åŒ–ï¼ŒæˆåŠŸè§£å†³äº†è§†è§‰è¯­è¨€æ¨¡å‹å®‰å…¨æ€§ä¸æ³›åŒ–æ€§èƒ½ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPåœ¨é€šè¿‡å¾®è°ƒæå‡å®‰å…¨æ€§æ—¶å¾€å¾€å¯¼è‡´æ³›åŒ–æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¿™ç§æƒè¡¡æºäºåƒµåŒ–çš„å¯¹é½ç­–ç•¥å¼ºåˆ¶å°†ä¸å®‰å…¨æ¦‚å¿µæ˜ å°„åˆ°å•ä¸€é¢„å®šä¹‰å®‰å…¨ç›®æ ‡ï¼Œç ´åäº†æ¨¡å‹å­¦ä¹ åˆ°çš„è¯­ä¹‰ç»“æ„ã€‚</p>
<p><strong>Method:</strong> æå‡ºé‚»è¿‘æ„ŸçŸ¥æ–¹æ³•ï¼Œå°†ä¸å®‰å…¨æ¦‚å¿µé‡å®šå‘åˆ°è¯­ä¹‰ä¸Šæœ€æ¥è¿‘çš„å®‰å…¨æ›¿ä»£å“ä»¥æœ€å°åŒ–è¡¨ç¤ºå˜åŒ–ï¼Œå¼€å‘äº†SaFeR-CLIPå¾®è°ƒæ¡†æ¶åº”ç”¨è¿™ç§æœ€å°å¹²é¢„åŸåˆ™ï¼Œå¹¶è´¡çŒ®äº†NSFW-CapsåŸºå‡†æ•°æ®é›†ç”¨äºåˆ†å¸ƒåç§»ä¸‹çš„å®‰å…¨æ€§è¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> SaFeR-CLIPæˆåŠŸåè°ƒäº†å®‰å…¨æ€§ä¸æ€§èƒ½ï¼Œç›¸æ¯”å…ˆå‰æ–¹æ³•åœ¨é›¶æ ·æœ¬å‡†ç¡®ç‡ä¸Šæ¢å¤äº†é«˜è¾¾8.0%çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä¿æŒäº†é²æ£’çš„å®‰å…¨æ€§è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å°Šé‡é¢„è®­ç»ƒè¡¨ç¤ºçš„å‡ ä½•ç»“æ„æ˜¯å®ç°å®‰å…¨æ€§è€Œä¸ç‰ºç‰²æ€§èƒ½çš„å…³é”®ï¼Œé‚»è¿‘æ„ŸçŸ¥æ–¹æ³•ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å®‰å…¨å¾®è°ƒæä¾›äº†æ›´æœ‰æ•ˆçš„è·¯å¾„ï¼Œæœ€å°å¹²é¢„åŸåˆ™å¯å¹¿æ³›åº”ç”¨äºå…¶ä»–éœ€è¦å¹³è¡¡å®‰å…¨æ€§ä¸æ€§èƒ½çš„åœºæ™¯ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Improving the safety of vision-language models like CLIP via fine-tuning often comes at a steep price, causing significant drops in their generalization performance. We find this trade-off stems from rigid alignment strategies that force unsafe concepts toward single, predefined safe targets, disrupting the model's learned semantic structure. To address this, we propose a proximity-aware approach: redirecting unsafe concepts to their semantically closest safe alternatives to minimize representational change. We introduce SaFeR-CLIP, a fine-tuning framework that applies this principle of minimal intervention. SaFeR-CLIP successfully reconciles safety and performance, recovering up to 8.0% in zero-shot accuracy over prior methods while maintaining robust safety. To support more rigorous evaluation, we also contribute NSFW-Caps, a new benchmark of 1,000 highly-aligned pairs for testing safety under distributional shift. Our work shows that respecting the geometry of pretrained representations is key to achieving safety without sacrificing performance.</p>
<h3 id="2-towards-unified-vision-language-models-for-forest-ecological-analysis-in-earth-observation">[2] <a href="https://arxiv.org/abs/2511.16853">Towards Unified Vision Language Models for Forest Ecological Analysis in Earth Observation</a></h3>
<p><em>Xizhe Xue, Xiao Xiang Zhu</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†REO-Instructï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘åœ°çƒè§‚æµ‹çš„ç»Ÿä¸€åŸºå‡†æ•°æ®é›†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºåŒæ—¶å¤„ç†æè¿°æ€§å’Œå›å½’ä»»åŠ¡ï¼Œå¡«è¡¥äº†å¤šæ¨¡æ€æ„ŸçŸ¥ä¸å¯æµ‹é‡ç”Ÿç‰©ç‰©ç†å˜é‡ä¹‹é—´ç¼ºä¹å…³è”çš„ç©ºç™½ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦å›å½’ä»»åŠ¡ä¸­çš„æ½œåŠ›å°šæœªè¢«å……åˆ†æ¢ç´¢ï¼Œç°æœ‰åœ°çƒè§‚æµ‹æ•°æ®é›†ä¸»è¦å…³æ³¨è¯­ä¹‰ç†è§£ä»»åŠ¡å¦‚æè¿°å’Œåˆ†ç±»ï¼Œç¼ºä¹å°†å¤šæ¨¡æ€æ„ŸçŸ¥ä¸å¯æµ‹é‡ç”Ÿç‰©ç‰©ç†å˜é‡å¯¹é½çš„åŸºå‡†ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†REO-Instructæ•°æ®é›†ï¼Œå»ºç«‹äº†æ£®æ—ç”Ÿæ€åœºæ™¯ä¸­çš„è®¤çŸ¥å¯è§£é‡Šé€»è¾‘é“¾ï¼Œæ•´åˆäº†å…±é…å‡†çš„Sentinel-2å’ŒALOS-2å½±åƒï¼Œå¹¶é€šè¿‡æ··åˆäººæœºæµç¨‹ç”Ÿæˆå’ŒéªŒè¯ç»“æ„åŒ–æ–‡æœ¬æ ‡æ³¨ã€‚</p>
<p><strong>Result:</strong> å¯¹é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨æ•°å€¼æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å›°éš¾ï¼Œçªæ˜¾äº†ç§‘å­¦è§†è§‰è¯­è¨€æ¨¡å‹é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚</p>
<p><strong>Conclusion:</strong> REO-Instructä¸ºå¼€å‘å’Œè¯„ä¼°èƒ½å¤ŸåŒæ—¶è¿›è¡Œæè¿°å’Œç§‘å­¦æ¨ç†çš„ä¸‹ä¸€ä»£åœ°ç†ç©ºé—´æ¨¡å‹æä¾›äº†æ ‡å‡†åŒ–åŸºç¡€ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ç§‘å­¦å›å½’ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>Recent progress in vision language models (VLMs) has enabled remarkable perception and reasoning capabilities, yet their potential for scientific regression in Earth Observation (EO) remains largely unexplored. Existing EO datasets mainly emphasize semantic understanding tasks such as captioning or classification, lacking benchmarks that align multimodal perception with measurable biophysical variables. To fill this gap, we present REO-Instruct, the first unified benchmark designed for both descriptive and regression tasks in EO. REO-Instruct establishes a cognitively interpretable logic chain in forest ecological scenario (human activity,land-cover classification, ecological patch counting, above-ground biomass (AGB) regression), bridging qualitative understanding and quantitative prediction. The dataset integrates co-registered Sentinel-2 and ALOS-2 imagery with structured textual annotations generated and validated through a hybrid human AI pipeline. Comprehensive evaluation protocols and baseline results across generic VLMs reveal that current models struggle with numeric reasoning, highlighting an essential challenge for scientific VLMs. REO-Instruct offers a standardized foundation for developing and assessing next-generation geospatial models capable of both description and scientific inference. The project page are publicly available at \href{https://github.com/zhu-xlab/REO-Instruct}{REO-Instruct}.</p>
<h3 id="3-bop-ask-object-interaction-reasoning-for-vision-language-models">[3] <a href="https://arxiv.org/abs/2511.16857">BOP-ASK: Object-Interaction Reasoning for Vision-Language Models</a></h3>
<p><em>Vineet Bhat, Sungsu Kim, Valts Blukis, Greg Heinrich, Prashanth Krishnamurthy, Ramesh Karri, Stan Birchfield, Farshad Khorrami, Jonathan Tremblay</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†BOP-ASKï¼Œä¸€ä¸ªç”¨äºç‰©ä½“äº¤äº’æ¨ç†çš„å¤§è§„æ¨¡æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡åˆ©ç”¨6Dç‰©ä½“å§¿æ€æ•°æ®ç”Ÿæˆç»†ç²’åº¦ç©ºé—´å…³ç³»æ ‡æ³¨ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç²¾ç¡®3Då®šä½ã€ç‰©ç†å…¼å®¹æ€§å’Œå¤šæ­¥ç©ºé—´è§„åˆ’ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†è¿™äº›è¯„ä¼°æ©ç›–äº†å…¶åœ¨ç†è§£ç‰©ä½“äº¤äº’æ–¹é¢çš„å…³é”®å¼±ç‚¹ï¼Œå½“å‰åŸºå‡†ä¸»è¦æµ‹è¯•é«˜å±‚æ¬¡ç©ºé—´å…³ç³»è€Œå¿½ç•¥äº†çœŸå®ä¸–ç•Œåº”ç”¨æ‰€éœ€çš„ç»†ç²’åº¦ç©ºé—´ç†è§£ï¼ŒåŒ…æ‹¬ç²¾ç¡®3Då®šä½ã€ç‰©ä½“é—´ç‰©ç†å…¼å®¹æ€§ã€ç‰©ä½“å¯ä¾›æ€§å’Œå¤šæ­¥ç©ºé—´è§„åˆ’ç­‰èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æå‡ºåŸºäºBOPæ•°æ®é›†çš„6Dç‰©ä½“å§¿æ€ç”Ÿæˆæ•°æ®æµæ°´çº¿ï¼Œä»ä¸­æå–æŠ“å–å§¿æ€ã€å‚è€ƒç‰©ä½“å§¿æ€ã€è·¯å¾„è§„åˆ’è½¨è¿¹ã€ç›¸å¯¹ç©ºé—´å’Œæ·±åº¦å…³ç³»ä»¥åŠç‰©ä½“é—´å…³ç³»ç­‰ç»†ç²’åº¦æ ‡æ³¨ï¼Œæ„å»ºäº†åŒ…å«è¶…è¿‡15ä¸‡å¼ å›¾åƒå’Œ3300ä¸‡ä¸ªé—®ç­”å¯¹çš„æ•°æ®é›†ï¼Œæ¶µç›–å…­ä¸ªä»»åŠ¡ï¼ˆå…¶ä¸­å››ä¸ªä¸ºæ–°é¢–ä»»åŠ¡ï¼‰ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜åœ¨BOP-ASKä¸Šè®­ç»ƒçš„æ¨¡å‹ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºç²¾ç¡®ç‰©ä½“å’ŒæŠ“å–å§¿æ€ä¼°è®¡ã€è½¨è¿¹è§„åˆ’ä»¥åŠåœ¨æ‚ä¹±ç¯å¢ƒä¸­è¿›è¡Œç»†ç²’åº¦ç‰©ä½“ä¸­å¿ƒç©ºé—´æ¨ç†ç­‰æ–°å…´èƒ½åŠ›ï¼ŒåŒæ—¶å‘å¸ƒäº†BOP-ASK-coreæµ‹è¯•åŸºå‡†å’ŒBOP-ASK-labåˆ†å¸ƒå¤–æ³›åŒ–åŸºå‡†ç”¨äºå…¨é¢è¯„ä¼°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å¡«è¡¥äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦ç‰©ä½“äº¤äº’æ¨ç†æ–¹é¢çš„è¯„ä¼°ç©ºç™½ï¼Œè¯æ˜äº†åŸºäº6Då§¿æ€çš„æ•°æ®ç”Ÿæˆæ–¹æ³•èƒ½æœ‰æ•ˆæå‡æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œä¸ºçœŸå®ä¸–ç•Œåº”ç”¨æä¾›äº†é‡è¦çš„è®­ç»ƒèµ„æºå’Œè¯„ä¼°æ¡†æ¶ï¼ŒåŒæ—¶å¼€æºçš„æ•°æ®é›†å’Œç”Ÿæˆæµæ°´çº¿å°†æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>Vision Language Models (VLMs) have achieved impressive performance on spatial reasoning benchmarks, yet these evaluations mask critical weaknesses in understanding object interactions. Current benchmarks test high level relationships ('left of,' 'behind', etc.) but ignore fine-grained spatial understanding needed for real world applications: precise 3D localization, physical compatibility between objects, object affordances and multi step spatial planning. In this work, we present BOP-ASK, a novel large scale dataset for object interaction reasoning for both training and benchmarking. Our data generation pipeline leverages 6D object poses from the Benchmark for Object Pose Estimation (BOP) datasets from which we derive fine grained annotations such as grasp poses, referred object poses, path planning trajectories, relative spatial and depth relationships, and object-to-object relationships. BOP-ASK comprises over 150k images and 33M question answer pairs spanning six tasks (four novel), providing a rich resource for training and evaluating VLMs. We evaluate proprietary and open sourced VLMs, and conduct human evaluations on BOP-ASK-core, a contributed test benchmark. We also release BOP-ASK-lab, an out-of-distribution benchmark with images not sourced from BOP, enabling testing of generalization. Our experiments demonstrate that models trained on BOP-ASK outperform baselines and exhibit emergent capabilities such as precise object and grasp pose estimation, trajectory planning, and fine-grained object-centric spatial reasoning in cluttered environments. We will publicly release our datasets and dataset generation pipeline.</p>
<h3 id="4-align-invert-solving-inverse-problems-with-diffusion-and-flow-based-models-via-representational-alignment">[4] <a href="https://arxiv.org/abs/2511.16870">Align &amp; Invert: Solving Inverse Problems with Diffusion and Flow-based Models via Representational Alignment</a></h3>
<p><em>Loukas Sfountouris, Giannis Daras, Paris Giampouras</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è¡¨ç¤ºå¯¹é½æ­£åˆ™åŒ–æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶å¯¹é½æ‰©æ•£æ¨¡å‹ä¸é¢„è®­ç»ƒè‡ªç›‘ç£ç¼–ç å™¨çš„å†…éƒ¨è¡¨ç¤ºï¼Œæ˜¾è‘—æå‡äº†é€†é—®é¢˜æ±‚è§£çš„é‡å»ºè´¨é‡å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•åœ¨å„ç§é€†é—®é¢˜ä»»åŠ¡ä¸­å‡èƒ½ä¸€è‡´æ”¹å–„é‡å»ºæ•ˆæœï¼ŒåŒæ—¶å‡å°‘æ‰€éœ€çš„ç¦»æ•£åŒ–æ­¥éª¤ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é€†é—®é¢˜æ±‚è§£æ–¹æ³•åœ¨ä½¿ç”¨é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹ä½œä¸ºå…ˆéªŒæ—¶ï¼Œç¼ºä¹å¯¹æ¨¡å‹å†…éƒ¨è¡¨ç¤ºä¸ç›®æ ‡ç‰¹å¾å¯¹é½çš„æœ‰æ•ˆå¼•å¯¼æœºåˆ¶ã€‚è™½ç„¶è¡¨ç¤ºå¯¹é½åœ¨ç”Ÿæˆæ¨¡å‹è®­ç»ƒä¸­å·²è¢«è¯æ˜èƒ½æ”¹å–„æ”¶æ•›å’Œæ ·æœ¬è´¨é‡ï¼Œä½†åœ¨é€†é—®é¢˜æ¨ç†è¿‡ç¨‹ä¸­å¦‚ä½•åˆ©ç”¨è¿™ç§å¯¹é½æ¥æŒ‡å¯¼é‡å»ºè¿‡ç¨‹ä»æ˜¯ä¸€ä¸ªæœªå……åˆ†æ¢ç´¢çš„ç ”ç©¶ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†è¡¨ç¤ºå¯¹é½æ­£åˆ™åŒ–æ–¹æ³•ï¼Œåœ¨æ¨ç†æ—¶å¼ºåˆ¶å¯¹é½æ‰©æ•£æˆ–æµå¼ç”Ÿæˆæ¨¡å‹ä¸é¢„è®­ç»ƒè‡ªç›‘ç£è§†è§‰ç¼–ç å™¨çš„å†…éƒ¨è¡¨ç¤ºã€‚è¯¥æ–¹æ³•åˆ©ç”¨DINOv2ç­‰ç¼–ç å™¨æå–è¿‘ä¼¼ç›®æ ‡ç‰¹å¾ï¼Œé€šè¿‡ç†è®ºåˆ†ææ­ç¤ºäº†REPAæ­£åˆ™åŒ–ä¸DINOv2åµŒå…¥ç©ºé—´ä¸­æ•£åº¦åº¦é‡çš„å…³ç³»ï¼Œä»¥åŠREPAæ›´æ–°å¦‚ä½•å¼•å¯¼æ¨¡å‹å†…éƒ¨è¡¨ç¤ºå‘å¹²å‡€å›¾åƒè¡¨ç¤ºæ”¶æ•›ã€‚</p>
<p><strong>Result:</strong> åœ¨è¶…åˆ†è¾¨ç‡ã€æ¡†å†…ä¿®å¤ã€é«˜æ–¯å»æ¨¡ç³Šå’Œè¿åŠ¨å»æ¨¡ç³Šç­‰ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½ä¸€è‡´æå‡é‡å»ºè´¨é‡ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜æ•ˆç‡â€”â€”åœ¨ä¸å½±å“åº•å±‚æ±‚è§£å™¨æ€§èƒ½çš„å‰æä¸‹ï¼Œå‡å°‘äº†æ‰€éœ€çš„ç¦»æ•£åŒ–æ­¥éª¤æ•°é‡ã€‚é‡å»ºä¿çœŸåº¦å’Œæ„ŸçŸ¥çœŸå®æ€§å‡å¾—åˆ°å®è´¨æ€§å¢å¼ºã€‚</p>
<p><strong>Conclusion:</strong> è¡¨ç¤ºå¯¹é½ä¸ºé€†é—®é¢˜æ±‚è§£æä¾›äº†å¼ºå¤§çš„å½’çº³åç½®ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹å†…éƒ¨è¡¨ç¤ºå‘ç›®æ ‡ç‰¹å¾æ”¶æ•›æ¥æ”¹å–„æ„ŸçŸ¥ä¿çœŸåº¦ã€‚è¯¥æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯é›†æˆåˆ°å¤šç§æœ€å…ˆè¿›çš„é€†é—®é¢˜æ±‚è§£å™¨ä¸­ï¼Œä¸ºåˆ©ç”¨é¢„è®­ç»ƒè¡¨ç¤ºæŒ‡å¯¼ç”Ÿæˆæ¨¡å‹æ¨ç†å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Enforcing alignment between the internal representations of diffusion or flow-based generative models and those of pretrained self-supervised encoders has recently been shown to provide a powerful inductive bias, improving both convergence and sample quality. In this work, we extend this idea to inverse problems, where pretrained generative models are employed as priors. We propose applying representation alignment (REPA) between diffusion or flow-based models and a pretrained self-supervised visual encoder, such as DINOv2, to guide the reconstruction process at inference time. Although ground-truth signals are unavailable in inverse problems, we show that aligning model representations with approximate target features can substantially enhance reconstruction fidelity and perceptual realism. We provide theoretical results showing (a) the relation between the REPA regularization and a divergence measure in the DINOv2 embedding space, and (b) how REPA updates steer the model's internal representations toward those of the clean image. These results offer insights into the role of REPA in improving perceptual fidelity. Finally, we demonstrate the generality of our approach by integrating it into multiple state-of-the-art inverse problem solvers. Extensive experiments on super-resolution, box inpainting, Gaussian deblurring, and motion deblurring confirm that our method consistently improves reconstruction quality across tasks, while also providing substantial efficiency gains by reducing the number of required discretization steps without compromising the performance of the underlying solver.</p>
<h3 id="5-omniground-a-comprehensive-spatio-temporal-grounding-benchmark-for-real-world-complex-scenarios">[5] <a href="https://arxiv.org/abs/2511.16937">OmniGround: A Comprehensive Spatio-Temporal Grounding Benchmark for Real-World Complex Scenarios</a></h3>
<p><em>Hong Gao, Jingyu Wu, Xiangkai Xu, Kangni Xie, Yunchen Zhang, Bin Zhong, Xurui Gao, Min-Ling Zhang</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºOmniGroundåŸºå‡†å’ŒPG-TAFæ¡†æ¶ï¼Œè§£å†³äº†æ—¶ç©ºè§†é¢‘å®šä½ä¸­ç±»åˆ«åè§å’Œå¤æ‚æŸ¥è¯¢å¤„ç†ä¸è¶³çš„é—®é¢˜ã€‚PG-TAFé€šè¿‡ä¸¤é˜¶æ®µåˆ†è§£æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†ä¸Šå®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶ç©ºè§†é¢‘å®šä½ä»»åŠ¡ä¸­ä¸ç°å®éœ€æ±‚å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œä¸»è¦ç”±äºåŸºå‡†æ•°æ®é›†èŒƒå›´æœ‰é™å¯¼è‡´æ¨¡å‹å‡ºç°ç±»åˆ«åè§ã€æ¨ç†è¿‡ç¨‹è¿‡äºç®€åŒ–ä»¥åŠè¯­è¨€é²æ£’æ€§å·®ç­‰é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†OmniGroundç»¼åˆåŸºå‡†æ•°æ®é›†åŒ…å«3,475ä¸ªè§†é¢‘å’Œ81ä¸ªç±»åˆ«ï¼Œé‡‡ç”¨å‰å‘-åå‘-ç²¾åŒ–æ ‡æ³¨æµç¨‹ç»“åˆå¤šæ–¹å‘è·Ÿè¸ªå’Œæ™ºèƒ½çº é”™ï¼›å¼€å‘äº†PG-TAFè®­ç»ƒå…è´¹çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå°†STVGåˆ†è§£ä¸ºé«˜å±‚æ—¶é—´å®šä½å’Œç»†ç²’åº¦æ—¶ç©ºä¼ æ’­ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­æ€§èƒ½å¹³å‡ä¸‹é™10.4%ï¼Œç‰¹åˆ«æ˜¯åœ¨å°/é®æŒ¡ç‰©ä½“å’Œå¤æ‚ç©ºé—´å…³ç³»ä¸Šè¡¨ç°è¾ƒå·®ï¼›PG-TAFåœ¨OmniGroundä¸Šm_tIoUå’Œm_vIoUåˆ†åˆ«æå‡25.6%å’Œ35.6%ï¼Œåœ¨å››ä¸ªåŸºå‡†ä¸Šå‡è·å¾—ä¸€è‡´å¢ç›Šã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†ç°æœ‰STVGæ¨¡å‹åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„å±€é™æ€§ï¼Œæå‡ºçš„ç³»ç»Ÿè¯„ä¼°æ¡†æ¶DeepSTGå’ŒPG-TAFæ–¹æ³•ä¸ºæ”¹è¿›æ—¶ç©ºè§†é¢‘å®šä½æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼Œå¼ºè°ƒäº†æ•°æ®é›†è´¨é‡å’Œä»»åŠ¡åˆ†è§£ç­–ç•¥çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Spatio-Temporal Video Grounding (STVG) aims to localize target objects in videos based on natural language descriptions. Despite recent advances in Multimodal Large Language Models, a significant gap remains between current models and real-world demands involving diverse objects and complex queries. We attribute this to limited benchmark scope, causing models to exhibit category bias, oversimplified reasoning, and poor linguistic robustness. To address these limitations, we introduce OmniGround, a comprehensive benchmark with 3,475 videos spanning 81 categories and complex real-world queries. We propose the Forward-Backward-Refinement annotation pipeline that combines multi-directional tracking with intelligent error correction for high-quality labels. We further introduce DeepSTG, a systematic evaluation framework quantifying dataset quality across four complementary dimensions beyond superficial statistics. Evaluations reveal performance average drop of 10.4% on complex real-world scenes, particularly with small/occluded objects and intricate spatial relations. Motivated by these, we propose PG-TAF, a training-free two-stage framework decomposing STVG into high-level temporal grounding and fine-grained spatio-temporal propagation. Experiments demonstrate PG-TAF achieves 25.6% and 35.6% improvements in m_tIoU and m_vIoU on OmniGround with consistent gains across four benchmarks.</p>
<h3 id="6-r-avst-empowering-video-llms-with-fine-grained-spatio-temporal-reasoning-in-complex-audio-visual-scenarios">[6] <a href="https://arxiv.org/abs/2511.16901">R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios</a></h3>
<p><em>Lu Zhu, Tiantian Geng, Yangye Chen, Teng Wang, Ping Lu, Feng Zheng</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†R-AVSTæ•°æ®é›†å’ŒAVST-Zeroæ¨¡å‹ï¼Œå‰è€…æ˜¯é¦–ä¸ªä¸“ä¸ºçœŸå®ä¸–ç•ŒéŸ³é¢‘-è§†è§‰æ—¶ç©ºæ¨ç†è®¾è®¡çš„æ•°æ®é›†ï¼Œåè€…æ˜¯åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨¡å‹ï¼Œé€šè¿‡å¤šç»´åº¦å¥–åŠ±ç›´æ¥ä¼˜åŒ–è¡Œä¸ºï¼Œæ— éœ€ä¸­é—´ç›‘ç£ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨ç®€å•è§†é¢‘åœºæ™¯ï¼Œæ— æ³•åæ˜ çœŸå®ä¸–ç•Œè§†é¢‘ä¸­éŸ³é¢‘-è§†è§‰äº‹ä»¶çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œè¿™ä¸€ç ”ç©¶ç©ºç™½é™åˆ¶äº†æ¨¡å‹åœ¨å¤æ‚éŸ³é¢‘-è§†è§‰æ—¶ç©ºæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é¦–å…ˆæ„å»ºäº†R-AVSTæ•°æ®é›†ï¼Œé‡‡ç”¨åŸºäºLLMçš„å…³é”®å¯¹è±¡æå–ã€è‡ªåŠ¨ç©ºé—´æ ‡æ³¨å’Œäººå·¥è´¨é‡æ£€æŸ¥çš„æµæ°´çº¿ï¼ŒåŒ…å«è¶…è¿‡5Kä¸ªæœªä¿®å‰ªè§†é¢‘å’Œ27Kä¸ªå¯¹è±¡ï¼›åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†AVST-Zeroæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¤šç»´åº¦å¥–åŠ±ç›´æ¥ä¼˜åŒ–è¡Œä¸ºï¼Œé¿å…äº†ä¸­é—´ç›‘ç£çš„éœ€æ±‚ã€‚</p>
<p><strong>Result:</strong> å®éªŒéªŒè¯äº†R-AVSTæ•°æ®é›†åœ¨æ¨è¿›éŸ³é¢‘-è§†è§‰æ—¶ç©ºæ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ŒAVST-Zeroæ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸ç°æœ‰æ¨¡å‹ç›¸ç«äº‰çš„æ€§èƒ½ï¼Œç”Ÿæˆäº†è¶…è¿‡8Kä¸ªé«˜è´¨é‡ã€å‡åŒ€åˆ†å¸ƒçš„é—®é¢˜-ç­”æ¡ˆå¯¹æ¥æœ‰æ•ˆè¯„ä¼°æ¨¡å‹è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> R-AVSTæ˜¯é¦–ä¸ªä¸“ä¸ºçœŸå®ä¸–ç•ŒéŸ³é¢‘-è§†è§‰æ—¶ç©ºæ¨ç†è®¾è®¡çš„æ•°æ®é›†ï¼ŒAVST-Zeroä¸ºè§£å†³è¯¥é¢†åŸŸæœªæ¥æŒ‘æˆ˜æä¾›äº†æ–°é¢–è§†è§’ï¼Œè¿™é¡¹å·¥ä½œä¸ºå¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡å»ºç«‹äº†é‡è¦çš„åŸºå‡†å’Œè§£å†³æ–¹æ¡ˆæ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>Recently, rapid advancements have been made in multimodal large language models (MLLMs), especially in video understanding tasks. However, current research focuses on simple video scenarios, failing to reflect the complex and diverse nature of real-world audio-visual events in videos. To bridge this gap, we firstly introduce R-AVST, a dataset for audio-visual reasoning featuring fine-grained spatio-temporal annotations. In constructing this, we design a pipeline consisting of LLM-based key object extraction, automatic spatial annotation and manual quality inspection, resulting in over 5K untrimmed videos with 27K objects across 100 types of audio-visual events. Building on this dataset, we define three core tasks for spatio-temporal reasoning in audio-visual scenes and generate more than 8K high-quality, evenly distributed question-answer pairs to effectively benchmark model performance. To further enhance reasoning, we propose AVST-Zero, a reinforcement learning-based model that avoids intermediate supervision, directly optimizing behavior via carefully designed multi-dimensional rewards. Extensive experiments validate the effectiveness of our R-AVST in advancing audio-visual spatio-temporal reasoning, upon which AVST-Zero demonstrates competitive performance compared to existing models. To the best of our knowledge, R-AVST is the first dataset designed for real-world audio-visual spatio-temporal reasoning, and AVST-Zero offers a novel perspective for tackling future challenges in this domain.</p>
<h3 id="7-the-finer-the-better-towards-granular-aware-open-set-domain-generalization">[7] <a href="https://arxiv.org/abs/2511.16979">The Finer the Better: Towards Granular-aware Open-set Domain Generalization</a></h3>
<p><em>Yunyun Wang, Zheng Duan, Xinyue Liao, Ke-Jia Chen, Songcan Chen</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºSeeCLIPæ¡†æ¶ï¼Œé€šè¿‡ç»†ç²’åº¦è¯­ä¹‰å¢å¼ºè§£å†³å¼€æ”¾é›†åŸŸæ³›åŒ–ä¸­çš„ç»“æ„é£é™©ä¸å¼€æ”¾ç©ºé—´é£é™©å›°å¢ƒï¼Œæ˜¾è‘—æå‡æ¨¡å‹åœ¨é‡åˆ°è§†è§‰ç›¸ä¼¼æœªçŸ¥ç±»åˆ«æ—¶çš„è¯†åˆ«èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¼€æ”¾é›†åŸŸæ³›åŒ–é¢ä¸´å·²çŸ¥ç±»åˆ«ç»“æ„é£é™©ä¸æœªçŸ¥ç±»åˆ«å¼€æ”¾ç©ºé—´é£é™©çš„æƒè¡¡å›°å¢ƒï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸å·²çŸ¥ç±»åˆ«å…·æœ‰ç»†ç²’åº¦è§†è§‰ç›¸ä¼¼æ€§çš„'å›°éš¾æœªçŸ¥'æ ·æœ¬æ—¶å®¹æ˜“äº§ç”Ÿè¿‡åº¦è‡ªä¿¡é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºè¯­ä¹‰æ„ŸçŸ¥æç¤ºå¢å¼ºæ¨¡å—å°†å›¾åƒåˆ†è§£ä¸ºåˆ¤åˆ«æ€§è¯­ä¹‰æ ‡è®°ï¼Œå®ç°è¶…è¶Šç²—ç²’åº¦ç±»åˆ«æ ‡ç­¾çš„ç»†ç²’åº¦è§†è§‰-è¯­è¨€å¯¹é½ï¼›é‡‡ç”¨åŒå·¥å¯¹æ¯”å­¦ä¹ ä¿æŒä¸å·²çŸ¥ç±»åˆ«çš„åˆ†ç¦»æ€§å’Œè¯­ä¹‰é‚»è¿‘æ€§ï¼›é€šè¿‡è¯­ä¹‰å¼•å¯¼æ‰©æ•£æ¨¡å—æ‰°åŠ¨æå–çš„è¯­ä¹‰æ ‡è®°åˆæˆä¼ªæœªçŸ¥æ ·æœ¬ã€‚</p>
<p><strong>Result:</strong> åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œç›¸æ¯”æœ€å…ˆè¿›æ–¹æ³•ï¼Œå‡†ç¡®ç‡æå‡3%ï¼ŒH-scoreæå‡5%ï¼Œå®ç°äº†æŒç»­çš„æ€§èƒ½æ”¹è¿›ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†ç»†ç²’åº¦è¯­ä¹‰å¢å¼ºåœ¨ç¼“è§£å¼€æ”¾é›†åŸŸæ³›åŒ–é£é™©å›°å¢ƒä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤„ç†è§†è§‰ç›¸ä¼¼æœªçŸ¥ç±»åˆ«æä¾›äº†æ–°æ€è·¯ï¼Œæ¨åŠ¨äº†å¼€æ”¾ç¯å¢ƒä¸‹çš„ç¨³å¥æ¨¡å‹å‘å±•ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Open-Set Domain Generalization (OSDG) tackles the realistic scenario where deployed models encounter both domain shifts and novel object categories. Despite impressive progress with vision-language models like CLIP, existing methods still fall into the dilemma between structural risk of known-classes and open-space risk from unknown-classes, and easily suffers from over-confidence, especially when distinguishing ``hard unknowns" that share fine-grained visual similarities with known classes. To this end, we propose a Semantic-enhanced CLIP (SeeCLIP) framework that explicitly addresses this dilemma through fine-grained semantic enhancement. In SeeCLIP, we propose a semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens, enabling nuanced vision-language alignment beyond coarse category labels. To position unknown prompts effectively, we introduce duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity. Further, our semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences. These hard negatives force the model to learn finer decision boundaries. Extensive experiments across five benchmarks demonstrate consistent improvements of 3% accuracy and 5% H-score over state-of-the-art methods.</p>
<h3 id="8-q-real-towards-realism-and-plausibility-evaluation-for-ai-generated-content">[8] <a href="https://arxiv.org/abs/2511.16908">Q-REAL: Towards Realism and Plausibility Evaluation for AI-Generated Content</a></h3>
<p><em>Shushi Wang, Zicheng Zhang, Chunyi Li, Wei Wang, Liya Ma, Fengjiao Chen, Xiaoyu Li, Xuezhi Cao, Guangtao Zhai, Xiaohong Liu</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Q-Realæ•°æ®é›†å’ŒåŸºå‡†ï¼Œç”¨äºAIç”Ÿæˆå›¾åƒçš„ç»†ç²’åº¦çœŸå®æ€§å’Œåˆç†æ€§è¯„ä¼°ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®ç°åˆ¤æ–­å’Œå®šä½æ¨ç†ä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†å¾®è°ƒæ¡†æ¶æå‡æ¨¡å‹èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰AIç”Ÿæˆå†…å®¹è´¨é‡è¯„ä¼°æ–¹æ³•é€šå¸¸ä»…æä¾›å•ä¸€è´¨é‡åˆ†æ•°ï¼Œè¿‡äºç²—ç³™æ— æ³•ä¸ºç”Ÿæˆæ¨¡å‹ä¼˜åŒ–æä¾›é’ˆå¯¹æ€§æŒ‡å¯¼ï¼Œç‰¹åˆ«æ˜¯åœ¨çœŸå®æ€§å’Œåˆç†æ€§è¿™ä¸¤ä¸ªå…³é”®ç»´åº¦ä¸Šç¼ºä¹ç»†ç²’åº¦è¯„ä¼°èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†åŒ…å«3,088å¼ æ–‡æœ¬ç”Ÿæˆå›¾åƒçš„Q-Realæ•°æ®é›†ï¼Œæ ‡æ³¨ä¸»è¦å®ä½“çš„ä½ç½®ä¿¡æ¯ä»¥åŠçœŸå®æ€§å’Œåˆç†æ€§ç»´åº¦çš„åˆ¤æ–­é—®é¢˜å’Œå½’å› æè¿°ï¼›è®¾è®¡äº†Q-RealåŸºå‡†è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åˆ¤æ–­å’Œå®šä½æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼›å¼€å‘äº†ä¸“é—¨çš„å¾®è°ƒæ¡†æ¶æ¥å¢å¼ºæ¨¡å‹èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜æ•°æ®é›†å…·æœ‰é«˜è´¨é‡å’Œé‡è¦æ„ä¹‰ï¼ŒåŸºå‡†è¯„ä¼°å…¨é¢æœ‰æ•ˆï¼Œé€šè¿‡å¾®è°ƒæ¡†æ¶æ˜¾è‘—æå‡äº†å¤šä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç»†ç²’åº¦å›¾åƒè´¨é‡è¯„ä¼°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºAIç”Ÿæˆå›¾åƒçš„ç»†ç²’åº¦è´¨é‡è¯„ä¼°æä¾›äº†æ ‡å‡†åŒ–æ•°æ®é›†å’ŒåŸºå‡†ï¼Œæ¨åŠ¨äº†ç»Ÿä¸€ç”Ÿæˆ-ç†è§£æ¨¡å‹çš„å‘å±•ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç»†ç²’åº¦è¯„ä¼°èƒ½åŠ›ä¸ºç”Ÿæˆæ¨¡å‹ä¼˜åŒ–æä¾›äº†æœ‰æ•ˆæŒ‡å¯¼æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>Quality assessment of AI-generated content is crucial for evaluating model capability and guiding model optimization. However, most existing quality assessment datasets and models provide only a single quality score, which is too coarse to offer targeted guidance for improving generative models. In current applications of AI-generated images, realism and plausibility are two critical dimensions, and with the emergence of unified generation-understanding models, fine-grained evaluation along these dimensions becomes especially effective for improving generative performance. Therefore, we introduce Q-Real, a novel dataset for fine-grained evaluation of realism and plausibility in AI-generated images. Q-Real consists of 3,088 images generated by popular text-to-image models. For each image, we annotate the locations of major entities and provide a set of judgment questions and attribution descriptions for these along the dimensions of realism and plausibility. Considering that recent advances in multi-modal large language models (MLLMs) enable fine-grained evaluation of AI-generated images, we construct Q-Real Bench to evaluate them on two tasks: judgment and grounding with reasoning. Finally, to enhance MLLM capabilities, we design a fine-tuning framework and conduct experiments on multiple MLLMs using our dataset. Experimental results demonstrate the high quality and significance of our dataset and the comprehensiveness of the benchmark. Dataset and code will be released upon publication.</p>
<h3 id="9-racketvision-a-multiple-racket-sports-benchmark-for-unified-ball-and-racket-analysis">[9] <a href="https://arxiv.org/abs/2511.17045">RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis</a></h3>
<p><em>Linfeng Dong, Yuchen Yang, Hao Wu, Wei Wang, Yuenan HouZhihang Zhong, Xiao Sun</em></p>
<h4 id="tldr_8">ğŸ§© TL;DR</h4>
<p>RacketVisionæå‡ºäº†é¦–ä¸ªå¤§è§„æ¨¡ç»†ç²’åº¦çƒæ‹å§¿æ€æ ‡æ³¨æ•°æ®é›†ï¼Œæ¶µç›–ä¹’ä¹“çƒã€ç½‘çƒå’Œç¾½æ¯›çƒï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆèåˆçƒæ‹å§¿æ€ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†è½¨è¿¹é¢„æµ‹æ€§èƒ½ï¼Œä¸ºä½“è‚²åˆ†æä¸­çš„åŠ¨æ€ç›®æ ‡è·Ÿè¸ªå’Œå¤šæ¨¡æ€ç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚</p>
<hr />
<h4 id="detailed-summary_8">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ä½“è‚²åˆ†æé¢†åŸŸç¼ºä¹å¤§è§„æ¨¡ç»†ç²’åº¦çƒæ‹å§¿æ€æ ‡æ³¨æ•°æ®ï¼Œé™åˆ¶äº†å¤æ‚äººæœºäº¤äº’ç ”ç©¶çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹çƒæ‹ç±»è¿åŠ¨çš„åŠ¨æ€ç›®æ ‡è·Ÿè¸ªã€å…³èŠ‚å¼çƒæ‹å§¿æ€ä¼°è®¡å’Œé¢„æµ‹æ€§è½¨è¿¹é¢„æµ‹ç­‰ç›¸äº’å…³è”ä»»åŠ¡çš„ç ”ç©¶ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡ç»†ç²’åº¦çƒæ‹å§¿æ€æ ‡æ³¨æ•°æ®é›†ï¼Œæ¶µç›–ä¸‰ç§çƒæ‹è¿åŠ¨ï¼Œæå‡ºä½¿ç”¨CrossAttentionæœºåˆ¶è¿›è¡Œå¤šæ¨¡æ€ç‰¹å¾èåˆï¼Œè€Œéç®€å•çš„ç‰¹å¾æ‹¼æ¥ï¼Œä»¥æœ‰æ•ˆæ•´åˆçƒæ‹å§¿æ€ä¿¡æ¯ä¸çƒä½“ä½ç½®æ•°æ®ã€‚</p>
<p><strong>Result:</strong> è¯„ä¼°æ˜¾ç¤ºç®€å•æ‹¼æ¥çƒæ‹å§¿æ€ç‰¹å¾ä¼šé™ä½æ€§èƒ½ï¼Œè€ŒCrossAttentionæœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨çƒæ‹å§¿æ€ä¿¡æ¯ï¼Œåœ¨è½¨è¿¹é¢„æµ‹ä»»åŠ¡ä¸Šè¶…è¶Šäº†å¼ºå¤§çš„å•æ¨¡æ€åŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯äº†å¤šæ¨¡æ€èåˆç­–ç•¥çš„é‡è¦æ€§ã€‚</p>
<p><strong>Conclusion:</strong> RacketVisionä¸ºåŠ¨æ€ç›®æ ‡è·Ÿè¸ªã€æ¡ä»¶è¿åŠ¨é¢„æµ‹å’Œä½“è‚²å¤šæ¨¡æ€åˆ†ææä¾›äº†å¤šåŠŸèƒ½èµ„æºå’Œç ”ç©¶èµ·ç‚¹ï¼Œè¯æ˜äº†è·¨æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤šæ¨¡æ€èåˆä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºæœªæ¥ç›¸å…³ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_8">ğŸ“„ Abstract</h4>
<p>We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision</p>
<h3 id="10-unimodel-a-visual-only-framework-for-unified-multimodal-understanding-and-generation">[10] <a href="https://arxiv.org/abs/2511.16917">UniModel: A Visual-Only Framework for Unified Multimodal Understanding and Generation</a></h3>
<p><em>Chi Zhang, Jiepeng Wang, Youming Wang, Yuanzhi Liang, Xiaoyan Yang, Zuoxin Li, Haibin Huang, Xuelong Li</em></p>
<h4 id="tldr_9">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºUniModelï¼Œä¸€ç§åœ¨å•ä¸€åƒç´ åˆ°åƒç´ æ‰©æ•£æ¡†æ¶å†…åŒæ—¶æ”¯æŒè§†è§‰ç†è§£å’Œè§†è§‰ç”Ÿæˆçš„ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡å°†æ–‡æœ¬å’Œå›¾åƒæ˜ å°„åˆ°å…±äº«è§†è§‰ç©ºé—´å®ç°å¤šæ¨¡æ€å­¦ä¹ çš„å®Œå…¨è§†è§‰åŸç”Ÿè¡¨è¿°ã€‚</p>
<hr />
<h4 id="detailed-summary_9">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å­¦ä¹ å­˜åœ¨æ¨¡å‹ã€ä»»åŠ¡å’Œè¡¨ç¤ºä¸‰ä¸ªç»´åº¦çš„åˆ†ç¦»é—®é¢˜ï¼Œä¸åŒæ¨¡æ€é—´çš„è¡¨ç¤ºå·®å¼‚å¯¼è‡´ç³»ç»Ÿå¤æ‚ä¸”éš¾ä»¥ç»Ÿä¸€å¤„ç†è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œéœ€è¦ä¸€ç§èƒ½å¤Ÿæ¶ˆé™¤æ¨¡æ€å·®å¼‚å¹¶ç»Ÿä¸€å¤„ç†å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡çš„æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> é‡‡ç”¨ç»Ÿä¸€çš„åƒç´ åˆ°åƒç´ æ‰©æ•£æ¡†æ¶ï¼Œå°†æ–‡æœ¬æç¤ºæ¸²æŸ“ä¸ºç»˜åˆ¶æ–‡æœ¬å›¾åƒï¼Œæ‰€æœ‰è¾“å…¥è¾“å‡ºå‡è§†ä¸ºRGBåƒç´ ï¼›ä½¿ç”¨åŸºäºæ•´æµæµçš„ç»Ÿä¸€æ‰©æ•£å˜æ¢å™¨ä½œä¸ºå…±äº«éª¨å¹²ç½‘ç»œï¼Œé€šè¿‡è½»é‡çº§ä»»åŠ¡åµŒå…¥æŒ‡å®šæ˜ å°„æ–¹å‘ï¼Œå®ç°è‡ªç„¶å›¾åƒä¸ç»˜åˆ¶æ–‡æœ¬å›¾åƒä¹‹é—´çš„åŒå‘æ˜ å°„å­¦ä¹ ã€‚</p>
<p><strong>Result:</strong> åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆå’Œå›¾åƒåˆ°æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†å¼ºå¤§çš„è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºæ–°å…´çš„å¯æ§æ€§ç‰¹æ€§ï¼Œå¦‚å›¾åƒ-æè¿°-å›¾åƒçš„å¾ªç¯ä¸€è‡´æ€§ï¼ŒéªŒè¯äº†ç»Ÿä¸€æ¡†æ¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> å°†æ¨¡å‹ã€ä»»åŠ¡å’Œè¡¨ç¤ºç»Ÿä¸€åœ¨å•ä¸€è§†è§‰ç©ºé—´ä¸­æ˜¯å®ç°é€šç”¨å¤šæ¨¡æ€æ™ºèƒ½çš„æœ‰å‰æ™¯èŒƒå¼ï¼Œè¿™ç§å®Œå…¨è§†è§‰åŸç”Ÿçš„æ–¹æ³•ä¸ºæ„å»ºæ›´ç®€æ´é«˜æ•ˆçš„å¤šæ¨¡æ€ç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå¹¶å±•ç¤ºäº†åŒå‘è§†è§‰ç¿»è¯‘è¿‡ç¨‹çš„æ½œåŠ›ã€‚</p>
<hr />
<h4 id="abstract_9">ğŸ“„ Abstract</h4>
<p>We present UniModel, a unified generative model that jointly supports visual understanding and visual generation within a single pixel-to-pixel diffusion framework. Our goal is to achieve unification along three axes: the model, the tasks, and the representations. At the representation level, we eliminate modality discrepancies by mapping both text and images into a shared visual space: textual prompts are rendered as painted text images on a clean canvas, and all inputs and outputs are treated purely as RGB pixels. This yields a fully vision-native formulation of multimodal learning. At the task level, a broad range of vision-language problems are cast as pixel-to-pixel transformations in this visual space. For understanding tasks, the model takes an RGB image and produces a painted text image that visually encodes the semantic prediction. For generation tasks, painted text images serve as visual conditions that guide realistic and semantically aligned image synthesis. Captioning and text-to-image generation thus become different directions of the same underlying visual translation process. At the model level, we instantiate a single Unified Diffusion Transformer trained with rectified flow in pixel space. A shared backbone jointly learns bidirectional mappings between natural images and painted text images, with lightweight task embeddings to specify the desired direction. Experiments on text-to-image synthesis and image-to-text understanding demonstrate strong cross-modal alignment and emergent controllability such as cycle-consistent image-caption-image loops. Our initial exploration suggests that unifying model, tasks, and representations in a single visual space is a promising paradigm for general-purpose multimodal intelligence.</p>
<h3 id="11-vision-language-models-are-confused-tourists">[11] <a href="https://arxiv.org/abs/2511.17004">Vision Language Models are Confused Tourists</a></h3>
<p><em>Patrick Amadeus Irawan, Ikhlasul Akmal Hanif, Muhammad Dehan Al Kautsar, Genta Indra Winata, Fajri Koto, Alham Fikri Aji</em></p>
<h4 id="tldr_10">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ConfusedTouristè¯„ä¼°å¥—ä»¶ï¼Œæ­ç¤ºäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¢ä¸´æ··åˆæ–‡åŒ–çº¿ç´¢æ—¶çš„ä¸¥é‡è„†å¼±æ€§ï¼Œå³ä½¿ç®€å•çš„å›¾åƒå †å æ‰°åŠ¨ä¹Ÿä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œçªæ˜¾äº†æ„å»ºæ–‡åŒ–é²æ£’å¤šæ¨¡æ€ç†è§£çš„ç´§è¿«éœ€æ±‚ã€‚</p>
<hr />
<h4 id="detailed-summary_10">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡åŒ–ç»´åº¦è¯„ä¼°ä¸»è¦å…³æ³¨å•ä¸€æ–‡åŒ–æ¦‚å¿µåœºæ™¯ï¼Œå¿½ç•¥äº†ç°å®ä¸–ç•Œä¸­å¤šä¸ªæ— å…³æ–‡åŒ–çº¿ç´¢å…±å­˜çš„å¤æ‚æƒ…å†µï¼Œè¿™ç§å±€é™æ€§æ— æ³•å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨å¤šå…ƒæ–‡åŒ–ç¤¾ä¼šä¸­çš„å®é™…ç¨³å®šæ€§è¡¨ç°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ConfusedTouristæ–‡åŒ–å¯¹æŠ—é²æ£’æ€§è¯„ä¼°å¥—ä»¶ï¼Œé€šè¿‡å›¾åƒå †å å’ŒåŸºäºå›¾åƒç”Ÿæˆçš„æ‰°åŠ¨æ–¹æ³•ï¼Œç³»ç»Ÿæ€§åœ°æµ‹è¯•è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åœ°ç†æ–‡åŒ–çº¿ç´¢å¹²æ‰°ä¸‹çš„ç¨³å®šæ€§è¡¨ç°ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹åœ¨ç®€å•å›¾åƒå †å æ‰°åŠ¨ä¸‹å‡†ç¡®ç‡æ€¥å‰§ä¸‹é™ï¼ŒåŸºäºå›¾åƒç”Ÿæˆçš„æ‰°åŠ¨ç‰ˆæœ¬è¡¨ç°æ›´å·®ï¼Œå¯è§£é‡Šæ€§åˆ†æè¡¨æ˜å¤±è´¥æºäºç³»ç»Ÿæ€§çš„æ³¨æ„åŠ›åç§»ï¼Œæ¨¡å‹è¢«å¹²æ‰°çº¿ç´¢åˆ†æ•£äº†åŸæœ¬çš„ç„¦ç‚¹ã€‚</p>
<p><strong>Conclusion:</strong> è§†è§‰æ–‡åŒ–æ¦‚å¿µçš„æ··åˆä¼šä¸¥é‡æŸå®³æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½ï¼Œè¿™ä¸€å‘ç°å¼ºè°ƒäº†å¼€å‘æ›´å…·æ–‡åŒ–é²æ£’æ€§çš„å¤šæ¨¡æ€ç†è§£æŠ€æœ¯çš„è¿«åˆ‡æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†é‡è¦æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_10">ğŸ“„ Abstract</h4>
<p>Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs' stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.</p>
<h3 id="12-omnipt-unleashing-the-potential-of-large-vision-language-models-for-pedestrian-tracking-and-understanding">[12] <a href="https://arxiv.org/abs/2511.17053">OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding</a></h3>
<p><em>Teng Fu, Mengyang Zhao, Ke Niu, Kaixin Peng, Bin Li</em></p>
<h4 id="tldr_11">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºOmniPTï¼Œä¸€ä¸ªç»Ÿä¸€çš„è¡Œäººè·Ÿè¸ªæ¡†æ¶ï¼Œèƒ½å¤ŸåŸºäºå‚è€ƒè¿›è¡Œè·Ÿè¸ªå¹¶ä¸ºè·Ÿè¸ªå¯¹è±¡ç”Ÿæˆè¯­ä¹‰ç†è§£ï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•åœ¨è·Ÿè¸ªåŸºå‡†ä¸Šå–å¾—äº†ä¼˜äºå…ˆå‰æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_11">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡LVLMsåœ¨å›¾åƒçº§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å®ä¾‹çº§ä»»åŠ¡å¦‚è§†è§‰å®šä½å’Œç›®æ ‡æ£€æµ‹ä¸­ä»å­˜åœ¨æ€§èƒ½å·®è·ï¼ŒåŒæ—¶è¡Œäººè·Ÿè¸ªé¢†åŸŸå‡ºç°äº†ç»“åˆç›®æ ‡è·Ÿè¸ªä¸è‡ªç„¶è¯­è¨€ç†è§£çš„æ–°ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡è¦æ±‚æ¨¡å‹åœ¨é«˜çº§è¯­ä¹‰å±‚é¢ç†è§£è·Ÿè¸ªå¯¹è±¡ï¼Œè¿™æ­£æ˜¯LVLMsçš„ä¼˜åŠ¿æ‰€åœ¨ã€‚</p>
<p><strong>Method:</strong> æå‡ºOmniPTæ¡†æ¶ï¼Œé‡‡ç”¨RL-Mid Training-SFT-RLå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆé€šè¿‡RLé˜¶æ®µä½¿æ¨¡å‹è¾“å‡ºå›ºå®šæ ¼å¼çš„è¾¹ç•Œæ¡†ï¼Œç„¶åä½¿ç”¨å¤§é‡è¡Œäººç›¸å…³æ•°æ®é›†è¿›è¡Œä¸­é—´è®­ç»ƒï¼Œæ¥ç€åœ¨å¤šä¸ªè¡Œäººè·Ÿè¸ªæ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œæœ€åå†æ¬¡è¿›è¡ŒRLé˜¶æ®µä»¥æå‡è·Ÿè¸ªæ€§èƒ½å’ŒæŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> åœ¨è·Ÿè¸ªåŸºå‡†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæ¯”å…ˆå‰æ–¹æ³•è¡¨ç°æ›´å¥½ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨è¡Œäººè·Ÿè¸ªä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•å°†è·Ÿè¸ªä»»åŠ¡å»ºæ¨¡ä¸ºåŸºç¡€æ¨¡å‹å¯æ‰§è¡Œçš„ä»»åŠ¡ï¼Œå¹¶è§£å†³äº†æ¨¡å‹è¾“å‡ºæ ¼å¼åŒ–ç­”æ¡ˆçš„é—®é¢˜ï¼Œä¸ºç»“åˆLVLMsä¸å®ä¾‹çº§è·Ÿè¸ªä»»åŠ¡æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†è¯­ä¹‰ç†è§£ä¸ç›®æ ‡è·Ÿè¸ªçš„èåˆã€‚</p>
<hr />
<h4 id="abstract_11">ğŸ“„ Abstract</h4>
<p>LVLMs have been shown to perform excellently in image-level tasks such as VQA and caption. However, in many instance-level tasks, such as visual grounding and object detection, LVLMs still show performance gaps compared to previous expert models. Meanwhile, although pedestrian tracking is a classical task, there have been a number of new topics in combining object tracking and natural language, such as Referring MOT, Cross-view Referring MOT, and Semantic MOT. These tasks emphasize that models should understand the tracked object at an advanced semantic level, which is exactly where LVLMs excel. In this paper, we propose a new unified Pedestrian Tracking framework, namely OmniPT, which can track, track based on reference and generate semantic understanding of tracked objects interactively. We address two issues: how to model the tracking task into a task that foundation models can perform, and how to make the model output formatted answers. To this end, we implement a training phase consisting of RL-Mid Training-SFT-RL. Based on the pre-trained weights of the LVLM, we first perform a simple RL phase to enable the model to output fixed and supervisable bounding box format. Subsequently, we conduct a mid-training phase using a large number of pedestrian-related datasets. Finally, we perform supervised fine-tuning on several pedestrian tracking datasets, and then carry out another RL phase to improve the model's tracking performance and enhance its ability to follow instructions. We conduct experiments on tracking benchmarks and the experimental results demonstrate that the proposed method can perform better than the previous methods.</p>
<h3 id="13-deltadeno-zero-shot-anomaly-generation-via-delta-denoising-attribution">[13] <a href="https://arxiv.org/abs/2511.16920">DeltaDeno: Zero-Shot Anomaly Generation via Delta-Denoising Attribution</a></h3>
<p><em>Chaoran Xu, Chengkan Lv, Qiyu Chen, Yunkang Cao, Feng Zhang, Zhengtao Zhang</em></p>
<h4 id="tldr_12">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDelta-Denoising (DeltaDeno)ï¼Œä¸€ç§æ— éœ€è®­ç»ƒã€é›¶æ ·æœ¬çš„å¼‚å¸¸ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”ä¸¤ä¸ªæ‰©æ•£åˆ†æ”¯åœ¨å…±äº«è°ƒåº¦ä¸‹çš„å»å™ªå·®å¼‚æ¥å®šä½å’Œç¼–è¾‘ç¼ºé™·ï¼Œè§£å†³äº†å¼‚å¸¸æ ·æœ¬ç¨€ç¼ºå¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_12">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¼‚å¸¸ç”Ÿæˆæ–¹æ³•é€šå¸¸éœ€è¦å°‘é‡å¼‚å¸¸æ ·æœ¬è¿›è¡Œå¾®è°ƒï¼Œè¿™ä¸å¼‚å¸¸ç¨€ç¼ºæ€§çš„ç°å®ç›¸çŸ›ç›¾ï¼Œä¸”å®¹æ˜“å¯¼è‡´ç±»åˆ«å…ˆéªŒè¿‡æ‹Ÿåˆã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³æ— éœ€çœŸå®å¼‚å¸¸æ ·æœ¬æˆ–è®­ç»ƒçš„é›¶æ ·æœ¬å¼‚å¸¸ç”Ÿæˆé—®é¢˜ã€‚</p>
<p><strong>Method:</strong> DeltaDenoé€šè¿‡å¯¹æ¯”ç”±æœ€å°æç¤ºå¯¹é©±åŠ¨çš„ä¸¤ä¸ªæ‰©æ•£åˆ†æ”¯åœ¨å…±äº«è°ƒåº¦ä¸‹çš„å»å™ªå·®å¼‚ï¼Œå°†æ¯æ­¥å»å™ªå·®å¼‚ç´¯ç§¯ä¸ºå›¾åƒç‰¹å®šå®šä½å›¾ï¼Œç”Ÿæˆæ©ç æŒ‡å¯¼æ½œåœ¨ç©ºé—´ä¿®å¤ï¼ŒåŒæ—¶ä¿ç•™ä¸Šä¸‹æ–‡å¹¶ç”ŸæˆçœŸå®å±€éƒ¨ç¼ºé™·ã€‚æ–¹æ³•è¿˜åŒ…æ‹¬ä»¤ç‰Œçº§æç¤ºç²¾ç‚¼ä»¥å¯¹é½å…±äº«å†…å®¹å¹¶å¼ºåŒ–å¼‚å¸¸ä»¤ç‰Œï¼Œä»¥åŠåœ¨é¢„æµ‹åŒºåŸŸåº”ç”¨ä»…é™å¼‚å¸¸ä»¤ç‰Œçš„ç©ºé—´æ³¨æ„åŠ›åç½®ã€‚</p>
<p><strong>Result:</strong> åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDeltaDenoå®ç°äº†å‡ºè‰²çš„ç”Ÿæˆè´¨é‡ã€çœŸå®æ„Ÿï¼Œå¹¶åœ¨ä¸‹æ¸¸æ£€æµ‹ä»»åŠ¡ä¸­è·å¾—äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ— éœ€çœŸå®å¼‚å¸¸æ ·æœ¬çš„é›¶æ ·æœ¬å¼‚å¸¸ç”Ÿæˆçš„å¯è¡Œæ€§ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å¯¹æ¯”å’Œå®šä½æœºåˆ¶å®ç°äº†é«˜è´¨é‡çš„ç¼ºé™·ç”Ÿæˆï¼Œä¸ºå¼‚å¸¸æ£€æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„å¢å¼ºæ•°æ®æ¥æºã€‚</p>
<hr />
<h4 id="abstract_12">ğŸ“„ Abstract</h4>
<p>Anomaly generation is often framed as few-shot fine-tuning with anomalous samples, which contradicts the scarcity that motivates generation and tends to overfit category priors. We tackle the setting where no real anomaly samples or training are available. We propose Delta-Denoising (DeltaDeno), a training-free zero-shot anomaly generation method that localizes and edits defects by contrasting two diffusion branches driven by a minimal prompt pair under a shared schedule. By accumulating per-step denoising deltas into an image-specific localization map, we obtain a mask to guide the latent inpainting during later diffusion steps and preserve the surrounding context while generating realistic local defects. To improve stability and control, DeltaDeno performs token-level prompt refinement that aligns shared content and strengthens anomaly tokens, and applies a spatial attention bias restricted to anomaly tokens in the predicted region. Experiments on public datasets show that DeltaDeno achieves great generation, realism and consistent gains in downstream detection performance. Code will be made publicly available.</p>
<h3 id="14-planning-with-sketch-guided-verification-for-physics-aware-video-generation">[14] <a href="https://arxiv.org/abs/2511.17450">Planning with Sketch-Guided Verification for Physics-Aware Video Generation</a></h3>
<p><em>Yidong Huang, Zun Wang, Han Lin, Dong-Ki Kim, Shayegan Omidshafiei, Jaehong Yoon, Yue Zhang, Mohit Bansal</em></p>
<h4 id="tldr_13">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSketchVerifyï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„åŸºäºè‰å›¾éªŒè¯çš„è¿åŠ¨è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡æµ‹è¯•æ—¶é‡‡æ ·å’ŒéªŒè¯å¾ªç¯ç”ŸæˆåŠ¨æ€ä¸€è‡´çš„è¿åŠ¨è½¨è¿¹ï¼Œåœ¨ä¿æŒé«˜æ•ˆçš„åŒæ—¶æ˜¾è‘—æå‡è§†é¢‘ç”Ÿæˆçš„è¿åŠ¨è´¨é‡å’Œç‰©ç†çœŸå®æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_13">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•ä¸»è¦ä¾èµ–å•æ¬¡è¿åŠ¨è§„åˆ’ï¼Œé€šå¸¸åªèƒ½å¤„ç†ç®€å•è¿åŠ¨ï¼Œæˆ–è€…éœ€è¦å¤šæ¬¡è°ƒç”¨è§†é¢‘ç”Ÿæˆå™¨è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œè¿™é™åˆ¶äº†å¤æ‚åŠ¨æ€åœºæ™¯çš„ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹å¤šä¸ªå€™é€‰è¿åŠ¨è½¨è¿¹ï¼Œä½¿ç”¨è§†è§‰è¯­è¨€éªŒè¯å™¨å¯¹è½¨è¿¹è¿›è¡Œè”åˆè¯„ä¼°ï¼Œè€ƒè™‘è¯­ä¹‰å¯¹é½å’Œç‰©ç†åˆç†æ€§ï¼›é€šè¿‡å°†è½¨è¿¹æ¸²æŸ“ä¸ºè½»é‡çº§è§†é¢‘è‰å›¾åœ¨é™æ€èƒŒæ™¯ä¸Šåˆæˆå¯¹è±¡ï¼Œé¿å…æ˜‚è´µçš„é‡å¤æ‰©æ•£åˆæˆï¼›é‡‡ç”¨è¿­ä»£ä¼˜åŒ–ç­–ç•¥ç›´åˆ°è¯†åˆ«å‡ºæ»¡æ„çš„è¿åŠ¨è§„åˆ’ã€‚</p>
<p><strong>Result:</strong> åœ¨WorldModelBenchå’ŒPhyWorldBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¿åŠ¨è´¨é‡ã€ç‰©ç†çœŸå®æ€§å’Œé•¿æœŸä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç«äº‰åŸºçº¿ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡å¤§å¹…æå‡ï¼›æ¶ˆèç ”ç©¶æ˜¾ç¤ºå¢åŠ è½¨è¿¹å€™é€‰æ•°é‡èƒ½æŒç»­æå‡æ•´ä½“æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æµ‹è¯•æ—¶éªŒè¯å¾ªç¯åœ¨æå‡è¿åŠ¨è§„åˆ’è´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè½»é‡çº§è‰å›¾æ¸²æŸ“æ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œä¸ºé«˜è´¨é‡è§†é¢‘ç”Ÿæˆæä¾›äº†ä¸€ç§é«˜æ•ˆçš„è¿åŠ¨è§„åˆ’è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_13">ğŸ“„ Abstract</h4>
<p>Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.</p>
<h3 id="15-rebrain-brain-mri-reconstruction-from-sparse-ct-slice-via-retrieval-augmented-diffusion">[15] <a href="https://arxiv.org/abs/2511.17068">ReBrain: Brain MRI Reconstruction from Sparse CT Slice via Retrieval-Augmented Diffusion</a></h3>
<p><em>Junming Liu, Yifei Sun, Weihua Cheng, Yujin Kang, Yirong Chen, Ding Wang, Guosun Zeng</em></p>
<h4 id="tldr_14">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºReBrainï¼Œä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºæ‰©æ•£çš„æ¡†æ¶ï¼Œç”¨äºä»ç¨€ç–CTæ‰«æé‡å»ºå®Œæ•´è„‘éƒ¨MRIã€‚è¯¥æ–¹æ³•é€šè¿‡æ£€ç´¢ç›¸ä¼¼CTåˆ‡ç‰‡ä½œä¸ºå‚è€ƒï¼Œç»“åˆBrownian Bridgeæ‰©æ•£æ¨¡å‹ï¼Œè§£å†³äº†ä½å‰‚é‡CTåè®®å¯¼è‡´çš„ç¨€ç–ä½“ç§¯é‡å»ºæŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_14">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è„‘éƒ¨MRIåœ¨ç–¾ç—…è¯Šæ–­ä¸­è‡³å…³é‡è¦ï¼Œä½†æŸäº›æ‚£è€…å› èº«ä½“æˆ–ä¸´åºŠé™åˆ¶æ— æ³•è¿›è¡ŒMRIæ£€æŸ¥ã€‚ç°æœ‰ä»CTåˆæˆMRIçš„æ–¹æ³•é¢ä¸´ä½å‰‚é‡åè®®å¯¼è‡´çš„ç¨€ç–CTä½“ç§¯å’Œè¾ƒå·®å¹³é¢åˆ†è¾¨ç‡é—®é¢˜ï¼Œä½¿å¾—å‡†ç¡®é‡å»ºå®Œæ•´è„‘éƒ¨MRIä½“ç§¯æå…·æŒ‘æˆ˜æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºReBrainæ¡†æ¶ï¼Œé¦–å…ˆä½¿ç”¨Brownian Bridgeæ‰©æ•£æ¨¡å‹åœ¨2Dç»´åº¦åˆæˆMRIåˆ‡ç‰‡ï¼ŒåŒæ—¶é€šè¿‡å¾®è°ƒæ£€ç´¢æ¨¡å‹ä»å…ˆéªŒæ•°æ®åº“ä¸­æ£€ç´¢ç»“æ„å’Œç—…ç†ç›¸ä¼¼çš„CTåˆ‡ç‰‡ä½œä¸ºå‚è€ƒã€‚é€šè¿‡ControlNetåˆ†æ”¯æ•´åˆæ£€ç´¢åˆ‡ç‰‡ä»¥æŒ‡å¯¼ä¸­é—´MRIåˆ‡ç‰‡ç”Ÿæˆï¼Œç¡®ä¿ç»“æ„è¿ç»­æ€§ï¼Œå¹¶å¯¹ç½•è§æ£€ç´¢å¤±è´¥æƒ…å†µåº”ç”¨çƒé¢çº¿æ€§æ’å€¼æä¾›è¡¥å……æŒ‡å¯¼ã€‚</p>
<p><strong>Result:</strong> åœ¨SynthRAD2023å’ŒBraTSæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒReBrainåœ¨ç¨€ç–æ¡ä»¶ä¸‹çš„è·¨æ¨¡æ€é‡å»ºä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†ä»ç¨€ç–CTåˆ°MRIçš„è½¬æ¢è´¨é‡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†æ£€ç´¢å¢å¼ºæ‰©æ•£æ¡†æ¶åœ¨è§£å†³ç¨€ç–åŒ»å­¦å›¾åƒé‡å»ºé—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¸´åºŠä¸­æ— æ³•è¿›è¡ŒMRIæ£€æŸ¥çš„æ‚£è€…æä¾›äº†å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸ºè·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆå¼€è¾Ÿäº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_14">ğŸ“„ Abstract</h4>
<p>Magnetic Resonance Imaging (MRI) plays a crucial role in brain disease diagnosis, but it is not always feasible for certain patients due to physical or clinical constraints. Recent studies attempt to synthesize MRI from Computed Tomography (CT) scans; however, low-dose protocols often result in highly sparse CT volumes with poor through-plane resolution, making accurate reconstruction of the full brain MRI volume particularly challenging. To address this, we propose ReBrain, a retrieval-augmented diffusion framework for brain MRI reconstruction. Given any 3D CT scan with limited slices, we first employ a Brownian Bridge Diffusion Model (BBDM) to synthesize MRI slices along the 2D dimension. Simultaneously, we retrieve structurally and pathologically similar CT slices from a comprehensive prior database via a fine-tuned retrieval model. These retrieved slices are used as references, incorporated through a ControlNet branch to guide the generation of intermediate MRI slices and ensure structural continuity. We further account for rare retrieval failures when the database lacks suitable references and apply spherical linear interpolation to provide supplementary guidance. Extensive experiments on SynthRAD2023 and BraTS demonstrate that ReBrain achieves state-of-the-art performance in cross-modal reconstruction under sparse conditions.</p>
<h3 id="16-multipriv-benchmarking-individual-level-privacy-reasoning-in-vision-language-models">[16] <a href="https://arxiv.org/abs/2511.16940">MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models</a></h3>
<p><em>Xiongtao Sun, Hui Li, Jiaming Zhang, Yujie Yang, Kaili Liu, Ruxin Feng, Wen Jun Tan, Wei Yang Bryan Lim</em></p>
<h4 id="tldr_15">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MultiPrivï¼Œé¦–ä¸ªç³»ç»Ÿè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ä¸ªä½“çº§éšç§æ¨ç†é£é™©çš„åŸºå‡†ï¼Œé€šè¿‡æ„å»ºåŒè¯­å¤šæ¨¡æ€æ•°æ®é›†å’Œéšç§æ„ŸçŸ¥ä¸æ¨ç†æ¡†æ¶ï¼Œæ­ç¤ºäº†ç°æœ‰VLMsåœ¨éšç§æ¨ç†æ–¹é¢çš„ç³»ç»Ÿæ€§æ¼æ´ã€‚</p>
<hr />
<h4 id="detailed-summary_15">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå¯¼è‡´éšç§é£é™©ä»ç®€å•çš„å±æ€§æ„ŸçŸ¥å‡çº§åˆ°ä¸ªä½“çº§å…³è”ï¼Œè€Œç°æœ‰éšç§åŸºå‡†åœ¨ç»“æ„ä¸Šæ— æ³•åº”å¯¹è¿™ç§æ–°å¨èƒï¼Œä¸»è¦è¯„ä¼°éšç§æ„ŸçŸ¥è€Œæœªèƒ½è§£å†³æ›´å…³é”®çš„éšç§æ¨ç†é£é™©ï¼Œå³VLMsæ¨æ–­å’Œé“¾æ¥åˆ†å¸ƒå¼ä¿¡æ¯ä»¥æ„å»ºä¸ªä½“æ¡£æ¡ˆçš„èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†éšç§æ„ŸçŸ¥ä¸æ¨ç†æ¡†æ¶ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ–°é¢–çš„åŒè¯­å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå…¶æ ¸å¿ƒç‰¹ç‚¹æ˜¯åŒ…å«åˆæˆä¸ªä½“æ¡£æ¡ˆï¼Œå…¶ä¸­æ ‡è¯†ç¬¦ä¸æ•æ„Ÿå±æ€§è¢«ç²¾å¿ƒé“¾æ¥ï¼Œæ”¯æŒä¹ä¸ªæŒ‘æˆ˜æ€§ä»»åŠ¡è¯„ä¼°å®Œæ•´çš„PPRè°±ç³»ï¼Œä»å±æ€§æ£€æµ‹åˆ°è·¨å›¾åƒé‡è¯†åˆ«å’Œé“¾å¼æ¨ç†ã€‚</p>
<p><strong>Result:</strong> å¯¹è¶…è¿‡50ä¸ªåŸºç¡€å’Œå•†ä¸šVLMsè¿›è¡Œå¤§è§„æ¨¡è¯„ä¼°ï¼Œå‘ç°è®¸å¤šVLMså­˜åœ¨æ˜¾è‘—ä¸”æœªè¢«æµ‹é‡çš„åŸºäºæ¨ç†çš„éšç§é£é™©ï¼Œæ„ŸçŸ¥çº§æŒ‡æ ‡å¯¹è¿™äº›æ¨ç†é£é™©çš„é¢„æµ‹èƒ½åŠ›è¾ƒå·®ï¼Œæ­ç¤ºäº†å…³é”®è¯„ä¼°å·®è·ï¼Œç°æœ‰å®‰å…¨å¯¹é½æ–¹æ³•å¯¹æ­¤ç±»åŸºäºæ¨ç†çš„æ”»å‡»ä¸ä¸€è‡´ä¸”æ— æ•ˆã€‚</p>
<p><strong>Conclusion:</strong> MultiPrivæš´éœ²äº†VLMsåœ¨éšç§æ¨ç†æ–¹é¢çš„ç³»ç»Ÿæ€§æ¼æ´ï¼Œä¸ºå¼€å‘é²æ£’çš„éšç§ä¿æŠ¤VLMsæä¾›äº†å¿…è¦æ¡†æ¶ï¼Œå¼ºè°ƒäº†éœ€è¦è¶…è¶Šä¼ ç»Ÿéšç§æ„ŸçŸ¥è¯„ä¼°æ¥åº”å¯¹æ–°å…´æ¨ç†å¨èƒçš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_15">ğŸ“„ Abstract</h4>
<p>Modern Vision-Language Models (VLMs) demonstrate sophisticated reasoning, escalating privacy risks beyond simple attribute perception to individual-level linkage. Current privacy benchmarks are structurally insufficient for this new threat, as they primarily evaluate privacy perception while failing to address the more critical risk of privacy reasoning: a VLM's ability to infer and link distributed information to construct individual profiles. To address this critical gap, we propose \textbf{MultiPriv}, the first benchmark designed to systematically evaluate individual-level privacy reasoning in VLMs. We introduce the \textbf{Privacy Perception and Reasoning (PPR)} framework and construct a novel, bilingual multimodal dataset to support it. The dataset uniquely features a core component of synthetic individual profiles where identifiers (e.g., faces, names) are meticulously linked to sensitive attributes. This design enables nine challenging tasks evaluating the full PPR spectrum, from attribute detection to cross-image re-identification and chained inference. We conduct a large-scale evaluation of over 50 foundational and commercial VLMs. Our analysis reveals: (1) Many VLMs possess significant, unmeasured reasoning-based privacy risks. (2) Perception-level metrics are poor predictors of these reasoning risks, revealing a critical evaluation gap. (3) Existing safety alignments are inconsistent and ineffective against such reasoning-based attacks. MultiPriv exposes systemic vulnerabilities and provides the necessary framework for developing robust, privacy-preserving VLMs.</p>
<h3 id="17-fingercap-fine-grained-finger-level-hand-motion-captioning">[17] <a href="https://arxiv.org/abs/2511.16951">FingerCap: Fine-grained Finger-level Hand Motion Captioning</a></h3>
<p><em>Xin Shen, Rui Zhu, Lei Shen, Xinyu Wang, Kaihao Zhang, Tianqing Zhu, Shuchen Wu, Chenxi Miao, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang, Xin Yu</em></p>
<h4 id="tldr_16">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FingerCapä»»åŠ¡ç”¨äºç”Ÿæˆç»†ç²’åº¦æ‰‹æŒ‡çº§æ‰‹éƒ¨è¿åŠ¨æè¿°ï¼Œå¹¶æ„å»ºäº†åŒ…å«40Ké…å¯¹è§†é¢‘-æ–‡æœ¬çš„FingerCap-40Kæ•°æ®é›†ã€‚ä¸ºè§£å†³è§†é¢‘MLLMsåœ¨æ—¶é—´ç¨€ç–æ€§ä¸Šçš„ç“¶é¢ˆï¼Œä½œè€…æå‡ºäº†FiGOPæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆRGBå…³é”®å¸§å’Œæ‰‹éƒ¨å…³é”®ç‚¹æ¥æ¢å¤ç²¾ç»†æ—¶é—´çº¿ç´¢ã€‚</p>
<hr />
<h4 id="detailed-summary_16">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰æ„ŸçŸ¥å’Œå…·èº«æ™ºèƒ½é¢†åŸŸç¼ºä¹å¯¹ç»†ç²’åº¦æ‰‹æŒ‡çº§æ‰‹éƒ¨è¿åŠ¨çš„ç†è§£èƒ½åŠ›ï¼Œç°æœ‰è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”±äºæ—¶é—´ç¨€ç–é‡‡æ ·æ— æ³•æ•æ‰æ‰‹æŒ‡è¿åŠ¨çš„é«˜é¢‘ç»†å¾®åŠ¨æ€ï¼Œè¿™æˆä¸ºæ‰‹æŒ‡çº§æ¨ç†çš„åŸºæœ¬ç“¶é¢ˆã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†FiGOPæ–¹æ³•ï¼Œå°†æ¯ä¸ªRGBå…³é”®å¸§ä¸åç»­æ‰‹éƒ¨å…³é”®ç‚¹é…å¯¹ç›´åˆ°ä¸‹ä¸€å…³é”®å¸§ï¼Œé€šè¿‡è½»é‡çº§æ—¶é—´ç¼–ç å™¨å°†å…³é”®ç‚¹è½¬æ¢ä¸ºè¿åŠ¨åµŒå…¥å¹¶ä¸RGBç‰¹å¾é›†æˆã€‚è¯¥æ–¹æ³•é€‚åº”äº†ç»å…¸GOPæ¦‚å¿µåˆ°æ‰‹æŒ‡è¿åŠ¨ï¼Œåœ¨ä¸å¢åŠ RGBå¯†åº¦çš„æƒ…å†µä¸‹æ¢å¤ç²¾ç»†æ—¶é—´çº¿ç´¢ã€‚</p>
<p><strong>Result:</strong> åœ¨FingerCap-40Kæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå½“å‰å¼ºå¤§çš„å¼€æºå’Œé—­æºè§†é¢‘MLLMsåœ¨æ‰‹æŒ‡çº§æ¨ç†æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œè€ŒFiGOPå¢å¼ºæ¨¡å‹åœ¨HandJudgeè¯„ä¼°å’Œäººç±»ç ”ç©¶ä¸­å‡å–å¾—ä¸€è‡´æ€§èƒ½æå‡ã€‚</p>
<p><strong>Conclusion:</strong> FiGOPæä¾›äº†ä¸€ç§è®¡ç®—å‹å¥½çš„è§£å†³æ–¹æ¡ˆæ¥å¤„ç†è§†é¢‘MLLMsçš„æ—¶é—´ç¨€ç–æ€§é—®é¢˜ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦æ•æ‰é«˜é¢‘ç»†å¾®åŠ¨æ€çš„ç»†ç²’åº¦æ‰‹éƒ¨è¿åŠ¨ç†è§£ä»»åŠ¡ï¼Œä¸ºæ‰‹éƒ¨åŠ¨ä½œåˆ†æå¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_16">ğŸ“„ Abstract</h4>
<p>Understanding fine-grained human hand motion is fundamental to visual perception, embodied intelligence, and multimodal communication. In this work, we propose Fine-grained Finger-level Hand Motion Captioning (FingerCap), which aims to generate textual descriptions that capture detailed finger-level semantics of hand actions. To support this task, we curate FingerCap-40K, a large-scale corpus of 40K paired hand-motion videos and captions spanning two complementary sources: concise instruction-style finger motions and diverse, naturalistic hand-object interactions. To enable effective evaluation, we employ HandJudge, a LLM-based rubric that measures finger-level correctness and motion completeness. Temporal sparsity remains a fundamental bottleneck for current Video-MLLMs, since sparse RGB sampling is insufficient to capture the subtle, high-frequency dynamics underlying fine finger motions. As a simple and compute-friendly remedy, we introduce FiGOP (Finger Group-of-Pictures), which pairs each RGB keyframe with subsequent hand keypoints until the next keyframe. A lightweight temporal encoder converts the keypoints into motion embeddings and integrates them with RGB features. FiGOP adapts the classic GOP concept to finger motion, recovering fine temporal cues without increasing RGB density. Experiments on FingerCap-40K show that strong open- and closed-source Video-MLLMs still struggle with finger-level reasoning, while our FiGOP-augmented model yield consistent gains under HandJudge and human studies.</p>
<h3 id="18-intervene-all-paths-unified-mitigation-of-lvlm-hallucinations-across-alignment-formats">[18] <a href="https://arxiv.org/abs/2511.17254">Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats</a></h3>
<p><em>Jiaye Qian, Ge Zheng, Yuchen Zhu, Sibei Yang</em></p>
<h4 id="tldr_17">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¸å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰Transformerå› æœæ¶æ„å¯¹é½çš„ç»¼åˆå¹²é¢„æ¡†æ¶ï¼Œé€šè¿‡åˆ†æå›¾åƒ-è¾“å…¥æ–‡æœ¬ã€å›¾åƒ-è¾“å‡ºæ–‡æœ¬å’Œæ–‡æœ¬-æ–‡æœ¬ä¸‰ç§è·¯å¾„çš„äº¤äº’ä½œç”¨æ¥å‡å°‘å¹»è§‰ç°è±¡ã€‚ç ”ç©¶å‘ç°LVLMçš„å¹»è§‰æºäºå¤šè·¯å¾„ç›¸äº’ä½œç”¨ï¼Œå¹¶é¦–æ¬¡å‘ç°æ¨¡å‹ä¾èµ–ä¸åŒè·¯å¾„å–å†³äºé—®ç­”å¯¹é½æ ¼å¼ï¼Œæ®æ­¤æå‡ºäº†é’ˆå¯¹åˆ¤åˆ«å¼å’Œç”Ÿæˆå¼æ ¼å¼çš„å…³é”®å¹»è§‰å¤´è¯†åˆ«ä¸å¹²é¢„æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_17">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“äº§ç”Ÿå¹»è§‰ç°è±¡ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³LVLMä¸­å¹»è§‰é—®é¢˜çš„æ ¹æœ¬åŸå› ï¼Œæ¢ç´¢ä¸åŒå› æœè·¯å¾„å¯¹å¹»è§‰äº§ç”Ÿçš„å½±å“ï¼Œå¹¶å¼€å‘æœ‰æ•ˆçš„å¹²é¢„æ–¹æ³•æ¥å‡å°‘è¿™ç§ä¸è‰¯ç°è±¡ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ä¸ªä¸Transformerå› æœæ¶æ„å¯¹é½çš„ç»¼åˆå¹²é¢„æ¡†æ¶ï¼Œåˆ†æäº†å›¾åƒ-è¾“å…¥æ–‡æœ¬ã€å›¾åƒ-è¾“å‡ºæ–‡æœ¬å’Œæ–‡æœ¬-æ–‡æœ¬ä¸‰ç§è·¯å¾„çš„äº¤äº’ä½œç”¨ã€‚é¦–æ¬¡å‘ç°LVLMæ ¹æ®é—®ç­”å¯¹é½æ ¼å¼ä¾èµ–ä¸åŒè·¯å¾„ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹åˆ¤åˆ«å¼å’Œç”Ÿæˆå¼æ ¼å¼çš„å…³é”®å¹»è§‰å¤´è¯†åˆ«ä¸å¹²é¢„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ç®€å•è€Œæœ‰æ•ˆã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæŒç»­å‡å°‘å„ç§å¯¹é½ç±»å‹ä¸‹çš„å¹»è§‰ç°è±¡ã€‚é€šè¿‡å¹²é¢„å…³é”®å¹»è§‰å¤´ï¼Œåœ¨ä¸åŒé—®ç­”æ ¼å¼ä¸‹éƒ½å®ç°äº†å¹»è§‰çš„æ˜¾è‘—é™ä½ï¼Œè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†LVLMå¹»è§‰çš„å¤šè·¯å¾„æœ¬è´¨ï¼Œè¡¨æ˜å¹»è§‰ä¸æ˜¯å•ä¸€å› æœè·¯å¾„çš„ç»“æœï¼Œè€Œæ˜¯å¤šç§è·¯å¾„ç›¸äº’ä½œç”¨çš„äº§ç‰©ã€‚è¿™ä¸€å‘ç°ä¸ºç†è§£LVLMå·¥ä½œæœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œæ‰€æå‡ºçš„å¹²é¢„æ¡†æ¶ä¸ºå‡å°‘å¹»è§‰æä¾›äº†ç³»ç»Ÿæ€§çš„è§£å†³æ–¹æ¡ˆï¼Œå¯¹æå‡LVLMçš„å¯é æ€§å’Œå®ç”¨æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<hr />
<h4 id="abstract_17">ğŸ“„ Abstract</h4>
<p>Despite their impressive performance across a wide range of tasks, Large Vision-Language Models (LVLMs) remain prone to hallucination. In this study, we propose a comprehensive intervention framework aligned with the transformer's causal architecture in LVLMs, integrating the effects of different intervention paths on hallucination. We find that hallucinations in LVLMs do not arise from a single causal path, but rather from the interplay among image-to-input-text, image-to-output-text, and text-to-text pathways. For the first time, we also find that LVLMs rely on different pathways depending on the question-answer alignment format. Building on these insights, we propose simple yet effective methods to identify and intervene on critical hallucination heads within each pathway, tailored to discriminative and generative formats. Experiments across multiple benchmarks demonstrate that our approach consistently reduces hallucinations across diverse alignment types.</p>
<h3 id="19-neighbor-grpo-contrastive-ode-policy-optimization-aligns-flow-models">[19] <a href="https://arxiv.org/abs/2511.16955">Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models</a></h3>
<p><em>Dailan He, Guanlin Feng, Xingtong Ge, Yazhe Niu, Yi Zhang, Bingqi Ma, Guanglu Song, Yu Liu, Hongsheng Li</em></p>
<h4 id="tldr_18">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†Neighbor GRPOç®—æ³•ï¼Œé€šè¿‡æ‰°åŠ¨ODEåˆå§‹å™ªå£°æ¡ä»¶ç”Ÿæˆå¤šæ ·åŒ–è½¨è¿¹ï¼Œå®Œå…¨ç»•å¼€SDEè½¬æ¢éœ€æ±‚ï¼Œè§£å†³äº†SDE-based GRPOåœ¨æµåŒ¹é…æ¨¡å‹ä¸­çš„ä¿¡ç”¨åˆ†é…ä½æ•ˆå’Œé«˜é˜¶æ±‚è§£å™¨ä¸å…¼å®¹é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_18">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰SDE-based GRPOæ–¹æ³•åœ¨åº”ç”¨äºç°ä»£æµåŒ¹é…æ¨¡å‹æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå…¶ç¡®å®šæ€§é‡‡æ ·èŒƒå¼ä¸GRPOçš„éšæœºæ€§éœ€æ±‚å­˜åœ¨å†²çªã€‚å½“å‰è§£å†³æ–¹æ¡ˆé€šè¿‡å°†ODEè½¬æ¢ä¸ºSDEå¼•å…¥éšæœºæ€§ï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨ä¿¡ç”¨åˆ†é…ä½æ•ˆå’Œä¸é«˜é˜¶æ±‚è§£å™¨ä¸å…¼å®¹çš„é—®é¢˜ï¼Œé™åˆ¶äº†è®­ç»ƒæ•ˆç‡å’Œé‡‡æ ·è´¨é‡ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡é¦–å…ˆä»è·ç¦»ä¼˜åŒ–è§’åº¦é‡æ–°è§£é‡Šç°æœ‰SDE-based GRPOæ–¹æ³•ï¼Œæ­ç¤ºå…¶å¯¹æ¯”å­¦ä¹ æœºåˆ¶æœ¬è´¨ã€‚åŸºäºæ­¤æå‡ºNeighbor GRPOç®—æ³•ï¼Œé€šè¿‡æ‰°åŠ¨ODEåˆå§‹å™ªå£°æ¡ä»¶ç”Ÿæˆå¤šæ ·åŒ–å€™é€‰è½¨è¿¹ï¼Œä½¿ç”¨åŸºäºsoftmaxè·ç¦»çš„ä»£ç†è·³è·ƒç­–ç•¥è¿›è¡Œæ¨¡å‹ä¼˜åŒ–ã€‚æ–¹æ³•è¿˜å¼•å…¥å¯¹ç§°é”šç‚¹é‡‡æ ·æé«˜è®¡ç®—æ•ˆç‡ï¼Œä»¥åŠç»„é—´æ‹ŸèŒƒæ•°é‡åŠ æƒè§£å†³å¥–åŠ±å¹³å¦åŒ–é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> å¤§é‡å®éªŒè¡¨æ˜ï¼ŒNeighbor GRPOåœ¨è®­ç»ƒæˆæœ¬ã€æ”¶æ•›é€Ÿåº¦å’Œç”Ÿæˆè´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºSDE-basedå¯¹åº”æ–¹æ³•ã€‚è¯¥æ–¹æ³•å®Œå…¨ä¿ç•™äº†ç¡®å®šæ€§ODEé‡‡æ ·çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬é«˜æ•ˆæ€§å’Œä¸é«˜é˜¶æ±‚è§£å™¨çš„å…¼å®¹æ€§ï¼ŒåŒæ—¶å®ç°äº†æ›´å¥½çš„å¯¹é½æ•ˆæœã€‚</p>
<p><strong>Conclusion:</strong> Neighbor GRPOä¸ºæµåŒ¹é…æ¨¡å‹çš„å¯¹é½æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ä»…è§£å†³äº†SDE-basedæ–¹æ³•çš„å±€é™æ€§ï¼Œè¿˜å»ºç«‹äº†è·ç¦»ä¼˜åŒ–ç›®æ ‡ä¸ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ä¹‹é—´çš„ç†è®ºè”ç³»ã€‚è¯¥æ–¹æ³•ä¸ºç¡®å®šæ€§ç”Ÿæˆæ¨¡å‹çš„äººç±»åå¥½å¯¹é½å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºä»·å€¼å’Œå®é™…åº”ç”¨å‰æ™¯ã€‚</p>
<hr />
<h4 id="abstract_18">ğŸ“„ Abstract</h4>
<p>Group Relative Policy Optimization (GRPO) has shown promise in aligning image and video generative models with human preferences. However, applying it to modern flow matching models is challenging because of its deterministic sampling paradigm. Current methods address this issue by converting Ordinary Differential Equations (ODEs) to Stochastic Differential Equations (SDEs), which introduce stochasticity. However, this SDE-based GRPO suffers from issues of inefficient credit assignment and incompatibility with high-order solvers for fewer-step sampling. In this paper, we first reinterpret existing SDE-based GRPO methods from a distance optimization perspective, revealing their underlying mechanism as a form of contrastive learning. Based on this insight, we propose Neighbor GRPO, a novel alignment algorithm that completely bypasses the need for SDEs. Neighbor GRPO generates a diverse set of candidate trajectories by perturbing the initial noise conditions of the ODE and optimizes the model using a softmax distance-based surrogate leaping policy. We establish a theoretical connection between this distance-based objective and policy gradient optimization, rigorously integrating our approach into the GRPO framework. Our method fully preserves the advantages of deterministic ODE sampling, including efficiency and compatibility with high-order solvers. We further introduce symmetric anchor sampling for computational efficiency and group-wise quasi-norm reweighting to address reward flattening. Extensive experiments demonstrate that Neighbor GRPO significantly outperforms SDE-based counterparts in terms of training cost, convergence speed, and generation quality.</p>
<h3 id="20-where-culture-fades-revealing-the-cultural-gap-in-text-to-image-generation">[20] <a href="https://arxiv.org/abs/2511.17282">Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation</a></h3>
<p><em>Chuancheng Shi, Shangze Li, Shiming Guo, Simiao Xie, Wenhua Wu, Jingtong Dou, Chao Wu, Canran Xiao, Cong Wang, Zifeng Cheng, Fei Shen, Tat-Seng Chua</em></p>
<h4 id="tldr_19">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è§£å†³å¤šè¯­è¨€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­æ–‡åŒ–ä¸€è‡´æ€§é—®é¢˜çš„æ–¹æ³•ï¼Œé€šè¿‡å®šä½æ–‡åŒ–æ•æ„Ÿç¥ç»å…ƒå¹¶è®¾è®¡ä¸¤ç§å¯¹é½ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†è·¨è¯­è¨€æ–‡åŒ–ä¸€è‡´æ€§è€Œä¿æŒç”Ÿæˆè´¨é‡ã€‚</p>
<hr />
<h4 id="detailed-summary_19">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šè¯­è¨€æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨è·¨è¯­è¨€æç¤ºä¸‹å¾€å¾€äº§ç”Ÿæ–‡åŒ–ä¸­æ€§æˆ–è‹±è¯­åå‘çš„ç»“æœï¼Œè¿™æºäºè¯­è¨€æºå¸¦çš„æ–‡åŒ–å†…æ¶µæœªè¢«å……åˆ†æ¿€æ´»ï¼Œè€Œéæ¨¡å‹ç¼ºä¹æ–‡åŒ–çŸ¥è¯†ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸€ç§æ¢æµ‹æ–¹æ³•å®šä½æ–‡åŒ–æ•æ„Ÿä¿¡å·åˆ°å°‘é‡å›ºå®šå±‚ä¸­çš„ç‰¹å®šç¥ç»å…ƒï¼Œå¹¶è®¾è®¡äº†ä¸¤ç§äº’è¡¥å¯¹é½ç­–ç•¥ï¼šæ— éœ€ä¸»å¹²å¾®è°ƒçš„æ¨ç†æ—¶æ–‡åŒ–æ¿€æ´»å’Œä»…æ›´æ–°æ–‡åŒ–ç›¸å…³å±‚çš„å±‚å®šå‘æ–‡åŒ–å¢å¼ºã€‚</p>
<p><strong>Result:</strong> åœ¨CultureBenchåŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸æ¯”å¼ºåŸºçº¿æ–¹æ³•ï¼Œæ‰€ææ–¹æ³•åœ¨æ–‡åŒ–ä¸€è‡´æ€§æ–¹é¢å–å¾—äº†ä¸€è‡´æ€§æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆä¿çœŸåº¦å’Œå¤šæ ·æ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜æ–‡åŒ–çŸ¥è¯†å·²å­˜åœ¨äºæ¨¡å‹ä¸­ä½†æ¿€æ´»ä¸è¶³ï¼Œé€šè¿‡é’ˆå¯¹æ€§æ¿€æ´»æ–‡åŒ–ç›¸å…³ç¥ç»å…ƒå¯æœ‰æ•ˆæå‡è·¨è¯­è¨€æ–‡åŒ–ä¸€è‡´æ€§ï¼Œä¸ºå¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„æ–‡åŒ–å¯¹é½æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_19">ğŸ“„ Abstract</h4>
<p>Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity.</p>
<h3 id="21-drex-pure-vision-fusion-of-self-supervised-and-convolutional-representations-for-image-complexity-prediction">[21] <a href="https://arxiv.org/abs/2511.16991">DReX: Pure Vision Fusion of Self-Supervised and Convolutional Representations for Image Complexity Prediction</a></h3>
<p><em>Jonathan Skaza, Parsa Madinei, Ziqi Wen, Miguel Eckstein</em></p>
<h4 id="tldr_20">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºDReXï¼Œä¸€ç§ä»…ä½¿ç”¨è§†è§‰ä¿¡æ¯çš„å›¾åƒå¤æ‚åº¦é¢„æµ‹æ¨¡å‹ï¼Œé€šè¿‡èåˆè‡ªç›‘ç£å’Œå·ç§¯è¡¨ç¤ºå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¡¨æ˜è§†è§‰ç‰¹å¾æœ¬èº«è¶³ä»¥è¿›è¡Œäººç±»å¯¹é½çš„å¤æ‚åº¦é¢„æµ‹ã€‚</p>
<hr />
<h4 id="detailed-summary_20">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å›¾åƒå¤æ‚åº¦é¢„æµ‹æ–¹æ³•å¤šä¾èµ–å¤šæ¨¡æ€æ¨¡å‹ç»“åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œä½†è¯­è¨€ä¿¡æ¯å¯¹æ­¤ä»»åŠ¡æ˜¯å¦å¿…è¦å°šä¸æ˜ç¡®ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢ä»…ä½¿ç”¨è§†è§‰ç‰¹å¾è¿›è¡Œå¤æ‚åº¦é¢„æµ‹çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºDReXæ¨¡å‹ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æ³¨æ„åŠ›æœºåˆ¶èåˆResNet-50çš„å¤šå°ºåº¦å±‚æ¬¡ç‰¹å¾å’ŒDINOv3 ViT-S/16çš„è¯­ä¹‰ä¸°å¯Œè¡¨ç¤ºï¼Œèƒ½å¤ŸåŒæ—¶æ•è·ä½å±‚çº¹ç†æ¨¡å¼å’Œé«˜å±‚è¯­ä¹‰ç»“æ„ã€‚</p>
<p><strong>Result:</strong> åœ¨IC9600åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°Pearsonç›¸å…³ç³»æ•°0.9581çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¶…è¶ŠåŒ…æ‹¬å¤šæ¨¡æ€æ–¹æ³•åœ¨å†…çš„å…ˆå‰æ–¹æ³•ï¼ŒåŒæ—¶å‚æ•°é‡å‡å°‘çº¦21.5å€ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šå±•ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å½“é€‚å½“èåˆæ—¶ï¼Œè‡ªç›‘ç£transformerå’Œç›‘ç£æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œå¯¹æ­¤ä»»åŠ¡å…·æœ‰äº’è¡¥å’ŒååŒæ•ˆç›Šï¼Œè§†è§‰ç‰¹å¾æœ¬èº«è¶³ä»¥è¿›è¡Œäººç±»å¯¹é½çš„å¤æ‚åº¦é¢„æµ‹ï¼Œä¸ºè®¡ç®—æœºè§†è§‰å’Œè®¤çŸ¥ç§‘å­¦æä¾›äº†é‡è¦å¯ç¤ºã€‚</p>
<hr />
<h4 id="abstract_20">ğŸ“„ Abstract</h4>
<p>Visual complexity prediction is a fundamental problem in computer vision with applications in image compression, retrieval, and classification. Understanding what makes humans perceive an image as complex is also a long-standing question in cognitive science. Recent approaches have leveraged multimodal models that combine visual and linguistic representations, but it remains unclear whether language information is necessary for this task. We propose DReX (DINO-ResNet Fusion), a vision-only model that fuses self-supervised and convolutional representations through a learnable attention mechanism to predict image complexity. Our architecture integrates multi-scale hierarchical features from ResNet-50 with semantically rich representations from DINOv3 ViT-S/16, enabling the model to capture both low-level texture patterns and high-level semantic structure. DReX achieves state-of-the-art performance on the IC9600 benchmark (Pearson r = 0.9581), surpassing previous methods--including those trained on multimodal image-text data--while using approximately 21.5x fewer learnable parameters. Furthermore, DReX generalizes robustly across multiple datasets and metrics, achieving superior results on Pearson and Spearman correlation, Root Mean Square Error (RMSE), and Mean Absolute Error (MAE). Ablation and attention analyses confirm that DReX leverages complementary cues from both backbones, with the DINOv3 [CLS] token enhancing sensitivity to visual complexity. Our findings suggest that visual features alone can be sufficient for human-aligned complexity prediction and that, when properly fused, self-supervised transformers and supervised deep convolutional neural networks offer complementary and synergistic benefits for this task.</p>
<h3 id="22-mum-multi-view-masked-image-modeling-for-3d-vision">[22] <a href="https://arxiv.org/abs/2511.17309">MuM: Multi-View Masked Image Modeling for 3D Vision</a></h3>
<p><em>David NordstrÃ¶m, Johan Edstedt, Fredrik Kahl, Georg BÃ¶kman</em></p>
<h4 id="tldr_21">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºMuMæ¨¡å‹ï¼Œé€šè¿‡å°†æ©ç è‡ªç¼–ç æ‰©å±•åˆ°ä»»æ„å¤šè§†è§’å›¾åƒï¼Œä¸“é—¨é’ˆå¯¹3Dè§†è§‰ä»»åŠ¡å­¦ä¹ ç‰¹å¾è¡¨ç¤ºã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¶…è¶Šäº†DINOv3å’ŒCroCo v2ç­‰æœ€å…ˆè¿›çš„è§†è§‰ç¼–ç å™¨ã€‚</p>
<hr />
<h4 id="detailed-summary_21">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä¸»è¦é’ˆå¯¹è¯­ä¹‰ç†è§£è€Œéå‡ ä½•æ¨ç†ä¼˜åŒ–ï¼Œç°æœ‰æ–¹æ³•å¦‚CroCoè™½ç„¶é’ˆå¯¹3Dç†è§£è¿›è¡Œäº†æ”¹è¿›ï¼Œä½†åœ¨æ‰©å±•æ€§å’Œå¤æ‚æ€§æ–¹é¢å­˜åœ¨å±€é™ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸“é—¨é’ˆå¯¹3Dè§†è§‰ä»»åŠ¡çš„ç‰¹å¾å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³ç°æœ‰æ–¹æ³•åœ¨å‡ ä½•æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> æå‡ºMuMæ¨¡å‹ï¼Œå°†æ©ç è‡ªç¼–ç æ‰©å±•åˆ°ä»»æ„å¤šè§†è§’å›¾åƒï¼Œé€šè¿‡åœ¨æ‰€æœ‰è§†è§’ä¸Šç»Ÿä¸€åº”ç”¨æ©ç ç­–ç•¥ï¼Œå¹¶é‡‡ç”¨è½»é‡çº§è§£ç å™¨ç»“åˆè·¨å¸§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†æ¯”CroCoæ›´ç®€å•ä¸”æ›´å…·æ‰©å±•æ€§çš„æ¶æ„è®¾è®¡ã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒåŒ…æ‹¬å‰é¦ˆé‡å»ºã€å¯†é›†å›¾åƒåŒ¹é…å’Œç›¸å¯¹å§¿æ€ä¼°è®¡ï¼ŒMuMæ¨¡å‹å‡è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„è§†è§‰ç¼–ç å™¨DINOv3å’ŒCroCo v2çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä¸“é—¨é’ˆå¯¹3Dè§†è§‰ä»»åŠ¡è®¾è®¡çš„å¤šè§†è§’æ©ç è‡ªç¼–ç æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å‡ ä½•æ„ŸçŸ¥çš„ç‰¹å¾è¡¨ç¤ºï¼Œä¸ºè®¡ç®—æœºè§†è§‰ä¸­çš„3Dç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºäº†åœ¨æ‰©å±•æ€§å’Œæ€§èƒ½æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<hr />
<h4 id="abstract_21">ğŸ“„ Abstract</h4>
<p>Self-supervised learning on images seeks to extract meaningful visual representations from unlabeled data. When scaled to large datasets, this paradigm has achieved state-of-the-art performance and the resulting trained models such as DINOv3 have seen widespread adoption. However, most prior efforts are optimized for semantic understanding rather than geometric reasoning. One important exception is Cross-View Completion, CroCo, which is a form of masked autoencoding (MAE) tailored for 3D understanding. In this work, we continue on the path proposed by CroCo and focus on learning features tailored for 3D vision. In a nutshell, we extend MAE to arbitrarily many views of the same scene. By uniformly masking all views and employing a lightweight decoder with inter-frame attention, our approach is inherently simpler and more scalable than CroCo. We evaluate the resulting model, MuM, extensively on downstream tasks including feedforward reconstruction, dense image matching and relative pose estimation, finding that it outperforms the state-of-the-art visual encoders DINOv3 and CroCo v2.</p>
<h3 id="23-vlm-augmented-degradation-modeling-for-image-restoration-under-adverse-weather-conditions">[23] <a href="https://arxiv.org/abs/2511.16998">VLM-Augmented Degradation Modeling for Image Restoration Under Adverse Weather Conditions</a></h3>
<p><em>Qianyi Shao, Yuanfan Zhang, Renxiang Xiao, Liang Hu</em></p>
<h4 id="tldr_22">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å†…å­˜å¢å¼ºè§†è§‰è¯­è¨€æ¢å¤æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹çš„é“¾å¼æ¨ç†å’Œéšå¼å†…å­˜åº“ï¼Œå®ç°äº†å¯¹å„ç§æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹å›¾åƒçš„æœ‰æ•ˆæ¢å¤ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†æ¢å¤ç²¾åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_22">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨è‡ªåŠ¨é©¾é©¶å’Œæˆ·å¤–æœºå™¨äººåº”ç”¨ä¸­ï¼Œæ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„å¯é è§†è§‰æ„ŸçŸ¥æ˜¯è‡³å…³é‡è¦ä½†æå…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå¤„ç†ä¸åŒé€€åŒ–ç¨‹åº¦å’Œå¤šç§å¤©æ°”æ¡ä»¶çš„å¤æ‚é€€åŒ–æ¨¡å¼ã€‚</p>
<p><strong>Method:</strong> MVLRæ¨¡å‹é‡‡ç”¨è½»é‡çº§ç¼–ç å™¨-è§£ç å™¨ä¸»å¹²ç½‘ç»œï¼Œç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œé“¾å¼æ¨ç†ä»¥ç¼–ç å¤©æ°”é€€åŒ–å…ˆéªŒï¼Œå¹¶é€šè¿‡éšå¼å†…å­˜åº“å­˜å‚¨è¿ç»­æ½œåœ¨é€€åŒ–æ¨¡å¼è¡¨ç¤ºï¼Œåˆ©ç”¨åŠ¨æ€äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”èåˆå¤šå°ºåº¦è§†è§‰ç‰¹å¾ä¸é€€åŒ–åŸå‹ã€‚</p>
<p><strong>Result:</strong> åœ¨å››ä¸ªæ¶åŠ£å¤©æ°”åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMVLRåœ¨å³°å€¼ä¿¡å™ªæ¯”å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ ‡ä¸Šå‡ä¼˜äºå•åˆ†æ”¯å’Œä¸“å®¶æ··åˆåŸºçº¿æ–¹æ³•ï¼Œå®ç°äº†æ¨¡å‹ç´§å‡‘æ€§ä¸è¡¨è¾¾èƒ½åŠ›çš„è‰¯å¥½å¹³è¡¡ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜ç»“åˆè§†è§‰è¯­è¨€æ¨ç†ä¸å†…å­˜æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆæå‡æ¶åŠ£å¤©æ°”å›¾åƒæ¢å¤æ€§èƒ½ï¼Œä¸ºå®æ—¶éƒ¨ç½²æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†åœ¨å¤šæ ·åŒ–æˆ·å¤–æ¡ä»¶ä¸‹å®ç°é«˜æ•ˆè§†è§‰æ„ŸçŸ¥çš„å¯è¡Œæ€§ã€‚</p>
<hr />
<h4 id="abstract_22">ğŸ“„ Abstract</h4>
<p>Reliable visual perception under adverse weather conditions, such as rain, haze, snow, or a mixture of them, is desirable yet challenging for autonomous driving and outdoor robots. In this paper, we propose a unified Memory-Enhanced Visual-Language Recovery (MVLR) model that restores images from different degradation levels under various weather conditions. MVLR couples a lightweight encoder-decoder backbone with a Visual-Language Model (VLM) and an Implicit Memory Bank (IMB). The VLM performs chain-of-thought inference to encode weather degradation priors and the IMB stores continuous latent representations of degradation patterns. The VLM-generated priors query the IMB to retrieve fine-grained degradation prototypes. These prototypes are then adaptively fused with multi-scale visual features via dynamic cross-attention mechanisms, enhancing restoration accuracy while maintaining computational efficiency. Extensive experiments on four severe-weather benchmarks show that MVLR surpasses single-branch and Mixture-of-Experts baselines in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). These results indicate that MVLR offers a practical balance between model compactness and expressiveness for real-time deployment in diverse outdoor conditions.</p>
<h3 id="24-remsa-an-llm-agent-for-foundation-model-selection-in-remote-sensing">[24] <a href="https://arxiv.org/abs/2511.17442">REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing</a></h3>
<p><em>Binger Chen, Tacettin Emre BÃ¶k, Behnood Rasti, Volker Markl, BegÃ¼m Demir</em></p>
<h4 id="tldr_23">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†RSFMæ•°æ®åº“ï¼ˆRS-FMDï¼‰å’ŒREMSAæ™ºèƒ½ä½“ï¼Œå‰è€…æ˜¯ä¸€ä¸ªåŒ…å«150å¤šä¸ªé¥æ„ŸåŸºç¡€æ¨¡å‹çš„ç»“æ„åŒ–èµ„æºåº“ï¼Œåè€…æ˜¯é¦–ä¸ªåŸºäºLLMçš„è‡ªåŠ¨åŒ–é¥æ„ŸåŸºç¡€æ¨¡å‹é€‰æ‹©ç³»ç»Ÿï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢è§£å†³æ¨¡å‹é€‰æ‹©éš¾é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_23">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> é¥æ„Ÿé¢†åŸŸåŸºç¡€æ¨¡å‹ï¼ˆRSFMï¼‰çš„å¹¿æ³›åº”ç”¨é¢ä¸´æ¨¡å‹é€‰æ‹©å›°éš¾ï¼Œä¸»è¦ç”±äºæ–‡æ¡£åˆ†æ•£ã€æ ¼å¼å¼‚æ„å’Œéƒ¨ç½²çº¦æŸå¤šæ ·ç­‰é—®é¢˜ï¼Œç¼ºä¹ç³»ç»ŸåŒ–çš„æ¨¡å‹é€‰æ‹©å·¥å…·æ¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…å¿«é€Ÿå®šä½é€‚åˆç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ã€‚</p>
<p><strong>Method:</strong> æ„å»ºäº†åŒ…å«150å¤šä¸ªRSFMçš„RS-FMDç»“æ„åŒ–æ•°æ®åº“ï¼Œæ¶µç›–å¤šæ¨¡æ€æ•°æ®ã€åˆ†è¾¨ç‡å’Œå­¦ä¹ èŒƒå¼ï¼›å¼€å‘äº†REMSAæ™ºèƒ½ä½“ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ è§£æç”¨æˆ·éœ€æ±‚ã€è¡¥å…¨ç¼ºå¤±çº¦æŸã€æ’åºå€™é€‰æ¨¡å‹å¹¶æä¾›é€æ˜è§£é‡Šï¼›å»ºç«‹äº†åŒ…å«75ä¸ªä¸“å®¶éªŒè¯æŸ¥è¯¢åœºæ™¯å’Œ900ç§é…ç½®çš„åŸºå‡†è¯„ä¼°åè®®ã€‚</p>
<p><strong>Result:</strong> REMSAåœ¨ä¸“å®¶ä¸­å¿ƒè¯„ä¼°åè®®ä¸‹æ˜¾è‘—ä¼˜äºå¤šä¸ªåŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬æœ´ç´ æ™ºèƒ½ä½“ã€ç¨ å¯†æ£€ç´¢å’Œéç»“æ„åŒ–RAG-based LLMï¼Œä¸”ä»…ä½¿ç”¨å…¬å¼€å…ƒæ•°æ®è¿è¡Œï¼Œä¸è®¿é—®ç§æœ‰æˆ–æ•æ„Ÿæ•°æ®ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºé¥æ„ŸåŸºç¡€æ¨¡å‹é€‰æ‹©æä¾›äº†é¦–ä¸ªç³»ç»ŸåŒ–è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç»“æ„åŒ–æ•°æ®åº“å’ŒLLMæ™ºèƒ½ä½“çš„ç»“åˆï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹é€‰æ‹©çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œä¸ºé¥æ„Ÿé¢†åŸŸçš„æ¨¡å‹åº”ç”¨æ ‡å‡†åŒ–å’Œè‡ªåŠ¨åŒ–å¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_23">ğŸ“„ Abstract</h4>
<p>Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.</p>
<h3 id="25-pathagent-toward-interpretable-analysis-of-whole-slide-pathology-images-via-large-language-model-based-agentic-reasoning">[25] <a href="https://arxiv.org/abs/2511.17052">PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning</a></h3>
<p><em>Jingyun Chen, Linghan Cai, Zhikang Wang, Yi Huang, Songhan Jiang, Shenjin Huang, Hongpeng Wang, Yongbing Zhang</em></p>
<h4 id="tldr_24">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºPathAgentï¼Œä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå…è´¹ä»£ç†æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿç—…ç†å­¦å®¶çš„é€æ­¥æ¨ç†è¿‡ç¨‹å®ç°å…¨åˆ‡ç‰‡å›¾åƒçš„é€æ˜åˆ†æã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¼èˆªå™¨ã€æ„ŸçŸ¥å™¨å’Œæ‰§è¡Œå™¨æ¨¡å—çš„ååŒå·¥ä½œï¼Œç”Ÿæˆå¯è§£é‡Šçš„å†³ç­–è½¨è¿¹ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="detailed-summary_24">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å…¨åˆ‡ç‰‡å›¾åƒè®¡ç®—åˆ†ææµç¨‹ç¼ºä¹æ˜ç¡®çš„æ¨ç†è½¨è¿¹ï¼Œå¯¼è‡´é¢„æµ‹ç»“æœä¸é€æ˜ä¸”éš¾ä»¥åˆç†è§£é‡Šï¼Œæ— æ³•æ¨¡æ‹Ÿç—…ç†å­¦å®¶åŠ¨æ€ç¼©æ”¾ã€é‡æ–°èšç„¦å’Œè‡ªæˆ‘ä¿®æ­£çš„è¿­ä»£è¯æ®é©±åŠ¨æ¨ç†è¿‡ç¨‹ã€‚</p>
<p><strong>Method:</strong> PathAgenté‡‡ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå…è´¹ä»£ç†æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šå¯¼èˆªå™¨è´Ÿè´£è¿­ä»£ç²¾ç¡®å®šä½æ˜¾è‘—å¾®åŒºåŸŸï¼Œæ„ŸçŸ¥å™¨æå–å½¢æ€å­¦è§†è§‰çº¿ç´¢ï¼Œæ‰§è¡Œå™¨å°†è¿™äº›å‘ç°æ•´åˆåˆ°æŒç»­æ¼”åŒ–çš„è‡ªç„¶è¯­è¨€è½¨è¿¹ä¸­ï¼Œå½¢æˆæ˜¾å¼çš„æ€ç»´é“¾ã€‚</p>
<p><strong>Result:</strong> åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPathAgentå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¼€æ”¾æ€§å’Œå—é™è§†è§‰é—®ç­”ä»»åŠ¡ä¸­å‡è¶…è¶Šä»»åŠ¡ç‰¹å®šåŸºçº¿æ–¹æ³•ï¼Œä¸äººç±»ç—…ç†å­¦å®¶çš„åä½œè¯„ä¼°è¯å®äº†å…¶ä½œä¸ºé€æ˜ä¸”ä¸´åºŠåŸºç¡€è¯Šæ–­åŠ©æ‰‹çš„æ½œåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> PathAgenté€šè¿‡æ¨¡æ‹Ÿäººç±»ä¸“å®¶çš„åæ€æ€§é€æ­¥åˆ†ææ–¹æ³•ï¼Œä¸ºè®¡ç®—ç—…ç†å­¦æä¾›äº†å®Œå…¨å¯è§£é‡Šçš„é¢„æµ‹æ¡†æ¶ï¼Œå…¶è®­ç»ƒå…è´¹ç‰¹æ€§ç»“åˆå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ä½¿å…¶æˆä¸ºä¸´åºŠè¯Šæ–­ä¸­é€æ˜å†³ç­–æ”¯æŒçš„æœ‰å‰æ™¯å·¥å…·ã€‚</p>
<hr />
<h4 id="abstract_24">ğŸ“„ Abstract</h4>
<p>Analyzing whole-slide images (WSIs) requires an iterative, evidence-driven reasoning process that parallels how pathologists dynamically zoom, refocus, and self-correct while collecting the evidence. However, existing computational pipelines often lack this explicit reasoning trajectory, resulting in inherently opaque and unjustifiable predictions. To bridge this gap, we present PathAgent, a training-free, large language model (LLM)-based agent framework that emulates the reflective, stepwise analytical approach of human experts. PathAgent can autonomously explore WSI, iteratively and precisely locating significant micro-regions using the Navigator module, extracting morphology visual cues using the Perceptor, and integrating these findings into the continuously evolving natural language trajectories in the Executor. The entire sequence of observations and decisions forms an explicit chain-of-thought, yielding fully interpretable predictions. Evaluated across five challenging datasets, PathAgent exhibits strong zero-shot generalization, surpassing task-specific baselines in both open-ended and constrained visual question-answering tasks. Moreover, a collaborative evaluation with human pathologists confirms PathAgent's promise as a transparent and clinically grounded diagnostic assistant.</p>
<h3 id="26-sparse-reasoning-is-enough-biological-inspired-framework-for-video-anomaly-detection-with-large-pre-trained-models">[26] <a href="https://arxiv.org/abs/2511.17094">Sparse Reasoning is Enough: Biological-Inspired Framework for Video Anomaly Detection with Large Pre-trained Models</a></h3>
<p><em>He Huang, Zixuan Hu, Dongxiao Li, Yao Xiao, Ling-Yu Duan</em></p>
<h4 id="tldr_25">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºReCoVADæ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»ç¥ç»ç³»ç»Ÿçš„åŒé€šè·¯æœºåˆ¶å®ç°é€‰æ‹©æ€§å¸§å¤„ç†ï¼Œåœ¨è§†é¢‘å¼‚å¸¸æ£€æµ‹ä¸­ä»…éœ€å¤„ç†å°‘é‡å¸§å³å¯è¾¾åˆ°æœ€å…ˆè¿›çš„è®­ç»ƒæ— å…³æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚</p>
<hr />
<h4 id="detailed-summary_25">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºå¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•é€šå¸¸ä¾èµ–å¯†é›†å¸§çº§æ¨ç†ï¼Œå¯¼è‡´é«˜æ˜‚è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢åœ¨å¼ºå¤§é¢„è®­ç»ƒæ¨¡å‹ä¸‹ç¨€ç–æ¨ç†æ˜¯å¦è¶³ä»¥å®ç°æœ‰æ•ˆçš„å¼‚å¸¸æ£€æµ‹ã€‚</p>
<p><strong>Method:</strong> ReCoVADé‡‡ç”¨åŒé€šè·¯æ¶æ„ï¼šåå°„é€šè·¯ä½¿ç”¨è½»é‡çº§CLIPæ¨¡å—èåˆè§†è§‰ç‰¹å¾ä¸åŸå‹æç¤ºï¼ŒæŸ¥è¯¢åŠ¨æ€è®°å¿†åº“å®ç°å¿«é€Ÿå“åº”ï¼›æ„è¯†é€šè·¯é‡‡ç”¨ä¸­ç­‰è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬äº‹ä»¶æè¿°å’Œç²¾ç‚¼å¼‚å¸¸åˆ†æ•°ï¼Œé€šè¿‡é›†æˆå¤§è¯­è¨€æ¨¡å‹å®šæœŸå®¡æŸ¥æè¿°ä»¥è¯†åˆ«æœªè§å¼‚å¸¸å¹¶ä¼˜åŒ–åŸå‹ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ReCoVADåœ¨UCF-Crimeå’ŒXD-Violenceæ•°æ®é›†ä¸Šåˆ†åˆ«ä»…éœ€å¤„ç†28.55%å’Œ16.04%çš„å¸§æ•°å³å¯è¾¾åˆ°æœ€å…ˆè¿›çš„è®­ç»ƒæ— å…³æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰æ–¹æ³•çš„è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜åœ¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹æ”¯æŒä¸‹ï¼Œç¨€ç–æ¨ç†è¶³ä»¥å®ç°æœ‰æ•ˆçš„è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼Œä¸ºå®æ—¶åº”ç”¨æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶åŒé€šè·¯æœºåˆ¶ä¸ºå…¶ä»–è§†é¢‘ç†è§£ä»»åŠ¡æä¾›äº†å¯å€Ÿé‰´çš„æ¶æ„è®¾è®¡æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_25">ğŸ“„ Abstract</h4>
<p>Video anomaly detection (VAD) plays a vital role in real-world applications such as security surveillance, autonomous driving, and industrial monitoring. Recent advances in large pre-trained models have opened new opportunities for training-free VAD by leveraging rich prior knowledge and general reasoning capabilities. However, existing studies typically rely on dense frame-level inference, incurring high computational costs and latency. This raises a fundamental question: Is dense reasoning truly necessary when using powerful pre-trained models in VAD systems? To answer this, we propose ReCoVAD, a novel framework inspired by the dual reflex and conscious pathways of the human nervous system, enabling selective frame processing to reduce redundant computation. ReCoVAD consists of two core pathways: (i) a Reflex pathway that uses a lightweight CLIP-based module to fuse visual features with prototype prompts and produce decision vectors, which query a dynamic memory of past frames and anomaly scores for fast response; and (ii) a Conscious pathway that employs a medium-scale vision-language model to generate textual event descriptions and refined anomaly scores for novel frames. It continuously updates the memory and prototype prompts, while an integrated large language model periodically reviews accumulated descriptions to identify unseen anomalies, correct errors, and refine prototypes. Extensive experiments show that ReCoVAD achieves state-of-the-art training-free performance while processing only 28.55\% and 16.04\% of the frames used by previous methods on the UCF-Crime and XD-Violence datasets, demonstrating that sparse reasoning is sufficient for effective large-model-based VAD.</p>
<h3 id="27-bridging-visual-affective-gap-borrowing-textual-knowledge-by-learning-from-noisy-image-text-pairs">[27] <a href="https://arxiv.org/abs/2511.17103">Bridging Visual Affective Gap: Borrowing Textual Knowledge by Learning from Noisy Image-Text Pairs</a></h3>
<p><em>Daiqing Wu, Dongbao Yang, Yu Zhou, Can Ma</em></p>
<h4 id="tldr_26">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºåˆ†åŒºè‡ªé€‚åº”å¯¹æ¯”å­¦ä¹ ï¼ˆPACLï¼‰æ–¹æ³•ï¼Œé€šè¿‡å€Ÿç”¨é¢„è®­ç»ƒæ–‡æœ¬æ¨¡å‹çš„æƒ…æ„ŸçŸ¥è¯†æ¥å¢å¼ºè§†è§‰æ¨¡å‹çš„æƒ…æ„Ÿæ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œå¼¥åˆè§†è§‰æƒ…æ„Ÿè¯†åˆ«ä¸­çš„'æƒ…æ„Ÿé¸¿æ²Ÿ'é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_26">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è§†è§‰æƒ…æ„Ÿè¯†åˆ«é¢†åŸŸå­˜åœ¨'æƒ…æ„Ÿé¸¿æ²Ÿ'é—®é¢˜ï¼Œå³é¢„è®­ç»ƒè§†è§‰æ¨¡å‹çš„äº‹å®çº§ç‰¹å¾ä¸æƒ…æ„Ÿç±»åˆ«ä¹‹é—´ç¼ºä¹ç›´æ¥å…³è”ï¼Œé™åˆ¶äº†é¢„è®­ç»ƒçŸ¥è¯†åœ¨æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ–‡æœ¬æ¨¡æ€å…·æœ‰æ˜ç¡®çš„æƒ…æ„Ÿè¡¨è¾¾å’Œé«˜ä¿¡æ¯å¯†åº¦ï¼Œèƒ½å¤Ÿæ¶ˆé™¤è¿™ç§é¸¿æ²Ÿã€‚</p>
<p><strong>Method:</strong> æå‡ºåˆ†åŒºè‡ªé€‚åº”å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ˆPACLï¼‰ï¼Œå…³æ³¨ç¤¾äº¤åª’ä½“æ•°æ®ä¸­å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„äº‹å®å’Œæƒ…æ„Ÿè”ç³»ï¼Œé€šè¿‡åˆ†ç¦»ä¸åŒç±»å‹çš„æ ·æœ¬å¹¶ä¸ºæ¯ç§ç±»å‹è®¾è®¡ä¸åŒçš„å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼ŒåŠ¨æ€æ„å»ºæ­£è´Ÿæ ·æœ¬å¯¹ä»¥å……åˆ†åˆ©ç”¨å™ªå£°æ ·æœ¬çš„æ½œåŠ›ã€‚</p>
<p><strong>Result:</strong> é€šè¿‡å…¨é¢å®éªŒè¯æ˜ï¼Œå¼¥åˆ'æƒ…æ„Ÿé¸¿æ²Ÿ'æ˜¾è‘—æå‡äº†å¤šç§é¢„è®­ç»ƒè§†è§‰æ¨¡å‹åœ¨ä¸‹æ¸¸æƒ…æ„Ÿç›¸å…³ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜å€Ÿç”¨æ–‡æœ¬æ¨¡æ€çš„æƒ…æ„ŸçŸ¥è¯†èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºè§†è§‰æ¨¡å‹çš„æƒ…æ„Ÿæ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºè·¨æ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æä¾›äº†æ–°çš„æ€è·¯ï¼ŒåŒæ—¶æå‡ºçš„PACLæ–¹æ³•ä¸ºå¤„ç†å™ªå£°ç¤¾äº¤åª’ä½“æ•°æ®æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ¡†æ¶ã€‚</p>
<hr />
<h4 id="abstract_26">ğŸ“„ Abstract</h4>
<p>Visual emotion recognition (VER) is a longstanding field that has garnered increasing attention with the advancement of deep neural networks. Although recent studies have achieved notable improvements by leveraging the knowledge embedded within pre-trained visual models, the lack of direct association between factual-level features and emotional categories, called the "affective gap", limits the applicability of pre-training knowledge for VER tasks. On the contrary, the explicit emotional expression and high information density in textual modality eliminate the "affective gap". Therefore, we propose borrowing the knowledge from the pre-trained textual model to enhance the emotional perception of pre-trained visual models. We focus on the factual and emotional connections between images and texts in noisy social media data, and propose Partitioned Adaptive Contrastive Learning (PACL) to leverage these connections. Specifically, we manage to separate different types of samples and devise distinct contrastive learning strategies for each type. By dynamically constructing negative and positive pairs, we fully exploit the potential of noisy samples. Through comprehensive experiments, we demonstrate that bridging the "affective gap" significantly improves the performance of various pre-trained visual models in downstream emotion-related tasks. Our code is released on https://github.com/wdqqdw/PACL.</p>
<h3 id="28-chainv-atomic-visual-hints-make-multimodal-reasoning-shorter-and-better">[28] <a href="https://arxiv.org/abs/2511.17106">ChainV: Atomic Visual Hints Make Multimodal Reasoning Shorter and Better</a></h3>
<p><em>Yuan Zhang, Ming Lu, Junwen Pan, Tao Huang, Kuan Cheng, Qi She, Shanghang Zhang</em></p>
<h4 id="tldr_27">ğŸ§© TL;DR</h4>
<p>ChainVæ˜¯ä¸€ä¸ªåŠ¨æ€æ•´åˆè§†è§‰æç¤ºçš„å¤šæ¨¡æ€æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰è¡¥ä¸é€‰æ‹©å’Œæ³¨æ„åŠ›å¼ºåº¦åˆ†æä½¿æ¨ç†è¿‡ç¨‹æ›´çŸ­æ›´å‡†ç¡®ï¼Œåœ¨æ•°å­¦å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡æ¨ç†ç²¾åº¦å’Œæ•ˆç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_27">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€æ¨ç†æ¨¡å‹åœ¨ç”Ÿæˆé•¿æ¨ç†é“¾æ—¶å­˜åœ¨å†—ä½™è‡ªåæ€é—®é¢˜ï¼Œè€ŒåŸºäºé™æ€è§†è§‰å‚è€ƒçš„æ— è®­ç»ƒCoTå‹ç¼©æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­å¢ç›Šæœ‰é™ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤ŸåŠ¨æ€æ•´åˆè§†è§‰æç¤ºçš„æ¨ç†æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> ChainVé¦–å…ˆåŸºäºå‰ä¸€æ­¥æ¨ç†è¿›è¡Œç²—ç•¥è§†è§‰è¡¥ä¸é€‰æ‹©ï¼Œç„¶åé€šè¿‡å¹³å‡æ³¨æ„åŠ›å¼ºåº¦è¯†åˆ«æœ€å…·ä»£è¡¨æ€§çš„åŸå­è§†è§‰æç¤ºï¼Œå¹¶å¼•å…¥åŸºäºä¸€è‡´æ€§çš„è¯„ä¼°æœºåˆ¶æ¥è¯„ä¼°æ‰€é€‰æç¤ºçš„å¯é æ€§ï¼Œæœ€ç»ˆé€šè¿‡ä¼¯åŠªåˆ©éšæœºè¿‡ç¨‹å°†é€‰å®šè§†è§‰æç¤ºçš„åƒç´ åæ ‡åŠå…¶å¯é æ€§æ•´åˆåˆ°æ€è€ƒè¿‡ç¨‹ä¸­ã€‚</p>
<p><strong>Result:</strong> åœ¨MathVistaåŸºå‡†æµ‹è¯•ä¸­ï¼ŒChainVåœ¨MIMO-VL-RLä¸Šå®ç°äº†2.3%çš„ç²¾åº¦æå‡ï¼ŒåŒæ—¶æ¨ç†å»¶è¿Ÿé™ä½51.4%ï¼Œè¾“å‡ºtokené•¿åº¦ç¼©çŸ­24.5%ï¼Œç‰¹åˆ«åœ¨éœ€è¦å¤šæ­¥ç¬¦å·æ¨ç†çš„æ•°å­¦å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜åŠ¨æ€è§†è§‰æç¤ºæ•´åˆèƒ½æœ‰æ•ˆæå‡å¤šæ¨¡æ€æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œä¸ºå‡å°‘æ¨¡å‹å†—ä½™è‡ªåæ€æä¾›äº†æ–°æ€è·¯ï¼Œæœªæ¥å¯æ‰©å±•è‡³æ›´å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†åœºæ™¯ã€‚</p>
<hr />
<h4 id="abstract_27">ğŸ“„ Abstract</h4>
<p>Recent advances in multimodal reasoning models have demonstrated impressive capabilities across text and vision. However, even leading models exhibit redundant self-reflection when generating lengthy reasoning chains. While training-free CoT compression methods have emerged in the LLMs domain, they rely on static visual references and thus provide limited gains for multimodal reasoning. Therefore, we propose ChainV, a framework that dynamically integrates visual hints into the reasoning process, thereby making multimodal reasoning shorter and better. Specifically, ChainV first performs a coarse visual patch selection based on the previous reasoning step, then refines it by identifying the most representative atomic visual hint according to the averaged attention intensity. Additionally, ChainV introduces a consistency-based evaluation mechanism to assess the reliability of the chosen hint, guiding the model to adaptively adjust its level of self-reflection. Eventually, the pixel coordinates of the selected visual hint and its reliability are incorporated into thinking with a Bernoulli stochastic process. Experiments indicate that our method significantly improves reasoning accuracy and efficiency, especially on math-intensive benchmarks where visual hints are crucial for multi-step symbolic reasoning. For example, ChainV achieves $2.3\%$ improvement on the MathVista within MIMO-VL-RL, while reducing inference latency by $51.4\%$ and shortening output token length by $24.5\%$.</p>
<h3 id="29-a-multi-stage-optimization-framework-for-deploying-learned-image-compression-on-fpgas">[29] <a href="https://arxiv.org/abs/2511.17135">A Multi-Stage Optimization Framework for Deploying Learned Image Compression on FPGAs</a></h3>
<p><em>Jiaxun Fang, Li Chen</em></p>
<h4 id="tldr_28">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„å¤šé˜¶æ®µä¼˜åŒ–æ¡†æ¶ï¼Œå°†é«˜æ€§èƒ½æµ®ç‚¹å›¾åƒå‹ç¼©æ¨¡å‹è½¬åŒ–ä¸ºé«˜æ•ˆçš„ç¡¬ä»¶å‹å¥½å‹æ•´æ•°å®ç°ï¼Œé€šè¿‡åŠ¨æ€èŒƒå›´æ„ŸçŸ¥é‡åŒ–ã€æ¸è¿›æ··åˆç²¾åº¦æœç´¢å’Œé€šé“å‰ªææŠ€æœ¯ï¼Œåœ¨FPGAä¸Šå®ç°äº†æ—¢é«˜æ•ˆåˆä¿æŒä¼˜å¼‚ç‡å¤±çœŸæ€§èƒ½çš„å›¾åƒå‹ç¼©ç³»ç»Ÿã€‚</p>
<hr />
<h4 id="detailed-summary_28">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒå‹ç¼©æ¨¡å‹è™½ç„¶å–å¾—äº†æœ€å…ˆè¿›çš„ç‡å¤±çœŸæ€§èƒ½ï¼Œä½†åœ¨èµ„æºå—é™çš„FPGAä¸Šçš„éƒ¨ç½²ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦é—®é¢˜åœ¨äºé‡åŒ–å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ä»¥åŠç¡¬ä»¶èµ„æºé™åˆ¶ä¸æ¨¡å‹å¤æ‚åº¦ä¹‹é—´çš„å¹³è¡¡éš¾é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åŠ¨æ€èŒƒå›´æ„ŸçŸ¥é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡ç»Ÿè®¡æ ¡å‡†çš„æ¿€æ´»è£å‰ªå’Œæ–°å‹æƒé‡æ­£åˆ™åŒ–æ–¹æ¡ˆæ¥åº”å¯¹æç«¯æ•°æ®å¼‚å¸¸å€¼å’Œå¤§çš„åŠ¨æ€èŒƒå›´ï¼›å¼€å‘äº†æ¸è¿›æ··åˆç²¾åº¦æœç´¢ç®—æ³•ï¼Œä¸ºæ¯å±‚åˆ†é…æœ€ä¼˜çš„éå‡åŒ€ä½å®½ï¼›è®¾è®¡äº†é€‚ç”¨äºGDNå±‚çš„é€šé“å‰ªææ–¹æ³•ï¼Œæ¶ˆé™¤æ¨¡å‹å†—ä½™ã€‚</p>
<p><strong>Result:</strong> åŸºç¡€DRAQæ–¹æ³•å°†åŸºäºGDNæ¨¡å‹çš„BD-rateå¼€é”€ä»30%é™ä½åˆ°6.3%ï¼Œåç»­ç¡¬ä»¶æ„ŸçŸ¥ä¼˜åŒ–è¿›ä¸€æ­¥å°†è®¡ç®—å¤æ‚åº¦é™ä½è¶…è¿‡20%ï¼ŒåŒæ—¶å¯¹ç‡å¤±çœŸæ€§èƒ½å½±å“å¯å¿½ç•¥ï¼Œæœ€ç»ˆæ¨¡å‹åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰FPGAå›¾åƒå‹ç¼©å®ç°ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç³»ç»ŸåŒ–çš„é‡åŒ–ã€ç²¾åº¦ä¼˜åŒ–å’Œå‰ªæç­–ç•¥ï¼Œå¯ä»¥åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½æ·±åº¦å­¦ä¹ å›¾åƒå‹ç¼©æ¨¡å‹çš„ç¡¬ä»¶å®ç°å¤æ‚åº¦ï¼Œä¸ºèµ„æºå—é™è®¾å¤‡ä¸Šçš„é«˜æ•ˆAIéƒ¨ç½²æä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_28">ğŸ“„ Abstract</h4>
<p>Deep learning-based image compression (LIC) has achieved state-of-the-art rate-distortion (RD) performance, yet deploying these models on resource-constrained FPGAs remains a major challenge. This work presents a complete, multi-stage optimization framework to bridge the gap between high-performance floating-point models and efficient, hardware-friendly integer-based implementations. First, we address the fundamental problem of quantization-induced performance degradation. We propose a Dynamic Range-Aware Quantization (DRAQ) method that uses statistically-calibrated activation clipping and a novel weight regularization scheme to counteract the effects of extreme data outliers and large dynamic ranges, successfully creating a high-fidelity 8-bit integer model. Second, building on this robust foundation, we introduce two hardware-aware optimization techniques tailored for FPGAs. A progressive mixed-precision search algorithm exploits FPGA flexibility to assign optimal, non-uniform bit-widths to each layer, minimizing complexity while preserving performance. Concurrently, a channel pruning method, adapted to work with the Generalized Divisive Normalization (GDN) layers common in LIC, removes model redundancy by eliminating inactive channels. Our comprehensive experiments show that the foundational DRAQ method reduces the BD-rate overhead of a GDN-based model from $30\%$ to $6.3\%$. The subsequent hardware-aware optimizations further reduce computational complexity by over $20\%$ with negligible impact on RD performance, yielding a final model that is both state-of-the-art in efficiency and superior in quality to existing FPGA-based LIC implementations.</p>
<h3 id="30-firescope-wildfire-risk-prediction-with-a-chain-of-thought-oracle">[30] <a href="https://arxiv.org/abs/2511.17171">FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle</a></h3>
<p><em>Mario Markov, Stefan Maria Ailuro, Luc Van Gool, Konrad Schindler, Danda Pani Paudel</em></p>
<h4 id="tldr_29">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†FireScope-Benchæ•°æ®é›†å’ŒFireScopeæ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºè¯­è¨€æ¨ç†çš„è§†è§‰ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡æ€ç†è§£å’Œå› æœæ¨ç†æ˜¾è‘—æå‡äº†è·¨å¤§é™†é‡ç«é£é™©é¢„æµ‹çš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_29">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰é‡ç«é£é™©é¢„æµ‹æ–¹æ³•ç¼ºä¹å› æœæ¨ç†å’Œå¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œå¯¼è‡´æ³›åŒ–æ€§èƒ½ä¸è¶³ï¼Œæ— æ³•å¯é åœ°åº”ç”¨äºè·¨å¤§é™†åœºæ™¯ï¼Œè¿™é™åˆ¶äº†å®é™…éƒ¨ç½²çš„å¯é æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†åˆ°ç”Ÿæˆæ¡†æ¶FireScopeï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ å’Œè§†è§‰ç›‘ç£å­¦ä¹ é¢„æµ‹é£é™©æ …æ ¼å›¾å¹¶ç”Ÿæˆäº’è¡¥çš„æ¨ç†è½¨è¿¹ï¼Œä½¿ç”¨Sentinel-2å½±åƒå’Œæ°”å€™æ•°æ®æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†FireScope-Benchã€‚</p>
<p><strong>Result:</strong> åœ¨ç¾å›½è®­ç»ƒå¹¶åœ¨æ¬§æ´²æµ‹è¯•æ—¶ï¼ŒFireScopeå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸“å®¶åé¦ˆå’Œè‡ªåŠ¨åŒ–åˆ†æè¯å®å…¶æ¨ç†è½¨è¿¹å…·æœ‰å¿ å®æ€§å’Œè¯­ä¹‰æ„ä¹‰ï¼ŒéªŒè¯äº†è·¨å¤§é™†æ³›åŒ–çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜åŸºäºè¯­è¨€çš„æ¨ç†èƒ½å¤Ÿæœ‰æ•ˆæ”¯æ’‘æ …æ ¼é¢„æµ‹æ¨¡å‹ï¼ŒåŒæ—¶æå‡æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ï¼Œä¸ºæ¨ç†é©±åŠ¨çš„å¯è§£é‡Šç©ºé—´å»ºæ¨¡å¥ å®šäº†åŸºç¡€ï¼Œå¼€åˆ›äº†è¯­è¨€æ¨ç†æ”¹è¿›è§†è§‰ç”Ÿæˆæ³›åŒ–èƒ½åŠ›çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_29">ğŸ“„ Abstract</h4>
<p>Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.</p>
<h3 id="31-investigating-self-supervised-representations-for-audio-visual-deepfake-detection">[31] <a href="https://arxiv.org/abs/2511.17181">Investigating self-supervised representations for audio-visual deepfake detection</a></h3>
<p><em>Dragos-Alexandru Boldisor, Stefan Smeu, Dan Oneata, Elisabeta Oneata</em></p>
<h4 id="tldr_30">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†è‡ªç›‘ç£è¡¨ç¤ºåœ¨éŸ³é¢‘-è§†é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œå‘ç°è¿™äº›ç‰¹å¾èƒ½æ•è·æœ‰æ„ä¹‰çš„ä¼ªé€ ç›¸å…³ä¿¡æ¯ä¸”å…·æœ‰äº’è¡¥æ€§ï¼Œä½†è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚</p>
<hr />
<h4 id="detailed-summary_30">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è‡ªç›‘ç£è¡¨ç¤ºåœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨éŸ³é¢‘-è§†é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œç°æœ‰ç ”ç©¶è¦ä¹ˆå­¤ç«‹ä½¿ç”¨è¿™äº›ç‰¹å¾ï¼Œè¦ä¹ˆå°†å…¶åµŒå…¥å¤æ‚æ¶æ„ä¸­ï¼Œç¼ºä¹ç³»ç»Ÿæ€§è¯„ä¼°ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶ç³»ç»Ÿè¯„ä¼°äº†è‡ªç›‘ç£ç‰¹å¾åœ¨éŸ³é¢‘ã€è§†é¢‘å’Œå¤šæ¨¡æ€ä¸‰ä¸ªç»´åº¦ä¸Šçš„è¡¨ç°ï¼Œé‡ç‚¹å…³æ³¨å”‡éƒ¨è¿åŠ¨å’Œé€šç”¨è§†è§‰å†…å®¹ä¸¤ä¸ªé¢†åŸŸï¼Œè¯„ä¼°äº†æ£€æµ‹æ•ˆæœã€ä¿¡æ¯å¯è§£é‡Šæ€§å’Œè·¨æ¨¡æ€äº’è¡¥æ€§ä¸‰ä¸ªå…³é”®ç»´åº¦ã€‚</p>
<p><strong>Result:</strong> å®éªŒå‘ç°å¤§å¤šæ•°è‡ªç›‘ç£ç‰¹å¾éƒ½èƒ½æ•è·ä¸æ·±åº¦ä¼ªé€ ç›¸å…³çš„ä¿¡æ¯ï¼Œä¸”è¿™äº›ä¿¡æ¯å…·æœ‰äº’è¡¥æ€§ï¼Œæ¨¡å‹ä¸»è¦å…³æ³¨è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŒºåŸŸè€Œéè™šå‡ä¼ªå½±ï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨è·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢è¡¨ç°ä¸å¯é ï¼Œè¿™ç§æ³›åŒ–å¤±è´¥æºäºæ•°æ®é›†ç‰¹æ€§è€Œéç‰¹å¾æœ¬èº«å¯¹è¡¨é¢æ¨¡å¼çš„ä¾èµ–ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†è‡ªç›‘ç£è¡¨ç¤ºåœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„åŒé‡æ€§ï¼šè™½ç„¶å®ƒä»¬èƒ½å¤Ÿå­¦ä¹ æœ‰æ„ä¹‰çš„æ¨¡å¼ï¼Œä½†å®ç°ç¨³å¥çš„è·¨åŸŸæ€§èƒ½ä»ç„¶é¢ä¸´æ ¹æœ¬æ€§æŒ‘æˆ˜ï¼Œè¿™ä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_30">ğŸ“„ Abstract</h4>
<p>Self-supervised representations excel at many vision and speech tasks, but their potential for audio-visual deepfake detection remains underexplored. Unlike prior work that uses these features in isolation or buried within complex architectures, we systematically evaluate them across modalities (audio, video, multimodal) and domains (lip movements, generic visual content). We assess three key dimensions: detection effectiveness, interpretability of encoded information, and cross-modal complementarity. We find that most self-supervised features capture deepfake-relevant information, and that this information is complementary. Moreover, models primarily attend to semantically meaningful regions rather than spurious artifacts. Yet none generalize reliably across datasets. This generalization failure likely stems from dataset characteristics, not from the features themselves latching onto superficial patterns. These results expose both the promise and fundamental challenges of self-supervised representations for deepfake detection: while they learn meaningful patterns, achieving robust cross-domain performance remains elusive.</p>
<h3 id="32-navigating-in-the-dark-a-multimodal-framework-and-dataset-for-nighttime-traffic-sign-recognition">[32] <a href="https://arxiv.org/abs/2511.17183">Navigating in the Dark: A Multimodal Framework and Dataset for Nighttime Traffic Sign Recognition</a></h3>
<p><em>Aditya Mishra, Akshay Agarwal, Haroon Lone</em></p>
<h4 id="tldr_31">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†INTSDå¤œé—´äº¤é€šæ ‡å¿—æ•°æ®é›†å’ŒLENS-Netæ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”å›¾åƒå¢å¼ºæ£€æµ‹å™¨å’Œå¤šæ¨¡æ€CLIP-GCNNåˆ†ç±»å™¨ï¼Œæœ‰æ•ˆè§£å†³äº†å¤œé—´äº¤é€šæ ‡å¿—è¯†åˆ«ä¸­å…‰ç…§ä¸è¶³å’Œè§†è§‰å™ªå£°çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åœ¨æ£€æµ‹å’Œåˆ†ç±»ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æ¡†æ¶ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿæä¾›äº†å¯é çš„å¤œé—´è§†è§‰è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="detailed-summary_31">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤œé—´äº¤é€šæ ‡å¿—è¯†åˆ«é¢ä¸´è§†è§‰å™ªå£°å’Œå…¬å¼€æ•°æ®é›†ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹é²æ£’æ€§ä¸è¶³ï¼Œä¸”æœªèƒ½æœ‰æ•ˆåˆ©ç”¨äº’è¡¥çš„å¤šæ¨¡æ€çº¿ç´¢ã€‚ä¸ºå…‹æœè¿™äº›é™åˆ¶ï¼Œéœ€è¦å¼€å‘ä¸“é—¨é’ˆå¯¹å¤œé—´æ¡ä»¶çš„å¤§è§„æ¨¡æ•°æ®é›†å’Œèƒ½å¤ŸåŒæ—¶å¤„ç†å…‰ç…§æ ¡æ­£ä¸è¯­ä¹‰ç†è§£çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚</p>
<p><strong>Method:</strong> é¦–å…ˆæ„å»ºäº†INTSDæ•°æ®é›†ï¼ŒåŒ…å«41ç±»å°åº¦å„åœ°å¤œé—´äº¤é€šæ ‡å¿—å›¾åƒï¼›ç„¶åæå‡ºLENS-Netæ¡†æ¶ï¼Œé›†æˆè‡ªé€‚åº”å›¾åƒå¢å¼ºæ£€æµ‹å™¨è¿›è¡Œè”åˆå…‰ç…§æ ¡æ­£å’Œæ ‡å¿—å®šä½ï¼Œåç»­é‡‡ç”¨ç»“æ„åŒ–å¤šæ¨¡æ€CLIP-GCNNåˆ†ç±»å™¨ï¼Œé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›å’Œå›¾æ¨ç†å®ç°é²æ£’è¯†åˆ«ã€‚</p>
<p><strong>Result:</strong> LENS-Netåœ¨æ£€æµ‹å’Œåˆ†ç±»ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›æ¡†æ¶ï¼Œæ¶ˆèç ”ç©¶è¯å®äº†å…¶å…³é”®ç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ¡†æ¶åœ¨INTSDæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå±•ç¤ºäº†åœ¨å¤šæ ·åŒ–å…‰ç…§å’Œå¤©æ°”æ¡ä»¶ä¸‹çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¤œé—´äº¤é€šæ ‡å¿—è¯†åˆ«æä¾›äº†é¦–ä¸ªå¤§è§„æ¨¡å°åº¦æ•°æ®é›†å’Œæœ‰æ•ˆçš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œå¼ºè°ƒäº†å¤šæ¨¡æ€èåˆå’Œè‡ªé€‚åº”å›¾åƒå¢å¼ºåœ¨ä½å…‰ç…§åœºæ™¯ä¸­çš„é‡è¦æ€§ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å…¨å¤©å€™å¯é è¿è¡Œå¥ å®šäº†åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_31">ğŸ“„ Abstract</h4>
<p>Traffic signboards are vital for road safety and intelligent transportation systems, enabling navigation and autonomous driving. Yet, recognizing traffic signs at night remains challenging due to visual noise and scarcity of public nighttime datasets. Despite advances in vision architectures, existing methods struggle with robustness under low illumination and fail to leverage complementary mutlimodal cues effectively. To overcome these limitations, firstly, we introduce INTSD, a large-scale dataset comprising street-level night-time images of traffic signboards collected across diverse regions of India. The dataset spans 41 traffic signboard classes captured under varying lighting and weather conditions, providing a comprehensive benchmark for both detection and classification tasks. To benchmark INTSD for night-time sign recognition, we conduct extensive evaluations using state-of-the-art detection and classification models. Secondly, we propose LENS-Net, which integrates an adaptive image enhancement detector for joint illumination correction and sign localization, followed by a structured multimodal CLIP-GCNN classifier that leverages cross-modal attention and graph-based reasoning for robust and semantically consistent recognition. Our method surpasses existing frameworks, with ablation studies confirming the effectiveness of its key components. The dataset and code for LENS-Net is publicly available for research.</p>
<h3 id="33-vla-4d-embedding-4d-awareness-into-vision-language-action-models-for-spatiotemporally-coherent-robotic-manipulation">[33] <a href="https://arxiv.org/abs/2511.17199">VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation</a></h3>
<p><em>Hanyu Zhou, Chuanhao Ma, Gim Hee Lee</em></p>
<h4 id="tldr_32">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†VLA-4Dæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰4Dæ„ŸçŸ¥èƒ½åŠ›çš„é€šç”¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥4Dæ„ŸçŸ¥çš„è§†è§‰è¡¨ç¤ºå’Œæ—¶ç©ºåŠ¨ä½œè¡¨ç¤ºï¼Œè§£å†³äº†æœºå™¨äººæ“ä½œä¸­æ—¶ç©ºä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚</p>
<hr />
<h4 id="detailed-summary_32">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨æ—¶ç©ºä¸€è‡´æ€§æ“ä½œæ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œè™½ç„¶ç°æœ‰æ–¹æ³•é€šè¿‡å°†3Dä½ç½®åµŒå…¥è§†è§‰è¡¨ç¤ºæ¥å¢å¼ºç©ºé—´ç²¾åº¦ï¼Œä½†è¿™äº›æ–¹æ³•éš¾ä»¥å®ç°åŠ¨ä½œæ‰§è¡Œçš„æ—¶é—´ä¸€è‡´æ€§æ§åˆ¶ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†ä¸¤ä¸ªå…³é”®è®¾è®¡ï¼š4Dæ„ŸçŸ¥çš„è§†è§‰è¡¨ç¤ºï¼Œé€šè¿‡æå–è§†è§‰ç‰¹å¾ã€å°†1Dæ—¶é—´åµŒå…¥3Dä½ç½®å½¢æˆ4DåµŒå…¥ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶èåˆä¸ºç»Ÿä¸€è§†è§‰è¡¨ç¤ºï¼›æ—¶ç©ºåŠ¨ä½œè¡¨ç¤ºï¼Œåœ¨ä¼ ç»Ÿç©ºé—´åŠ¨ä½œè¡¨ç¤ºåŸºç¡€ä¸Šæ‰©å±•æ—¶é—´ä¿¡æ¯ä»¥å®ç°æ—¶ç©ºè§„åˆ’ï¼Œå¹¶å°†å¤šæ¨¡æ€è¡¨ç¤ºå¯¹é½åˆ°LLMä¸­è¿›è¡Œæ—¶ç©ºåŠ¨ä½œé¢„æµ‹ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›çš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æœºå™¨äººæ“ä½œä¸åŒä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œæ‰€è®¾è®¡çš„è§†è§‰å’ŒåŠ¨ä½œè¡¨ç¤ºå…±åŒä½¿æœºå™¨äººæ“ä½œåœ¨ç©ºé—´ä¸Šå¹³æ»‘ä¸”åœ¨æ—¶é—´ä¸Šä¸€è‡´ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶é€šè¿‡ç»Ÿä¸€çš„æ¡†æ¶å®ç°äº†æœºå™¨äººæ“ä½œçš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œæ‰©å±•çš„VLAæ•°æ®é›†ä¸ºæ¨¡å‹å¾®è°ƒæä¾›äº†æ”¯æŒï¼Œä¸ºç²¾ç»†åŒ–çš„æœºå™¨äººæ“ä½œæ§åˆ¶æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_32">ğŸ“„ Abstract</h4>
<p>Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.</p>
<h3 id="34-continual-alignment-for-sam-rethinking-foundation-models-for-medical-image-segmentation-in-continual-learning">[34] <a href="https://arxiv.org/abs/2511.17201">Continual Alignment for SAM: Rethinking Foundation Models for Medical Image Segmentation in Continual Learning</a></h3>
<p><em>Jiayi Wang, Wei Dai, Haoyu Wang, Sihan Yang, Haixia Bi, Jian Sun</em></p>
<h4 id="tldr_33">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCA-SAMæ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§å¯¹é½å±‚å’ŒæŒç»­å­¦ä¹ ç­–ç•¥ï¼Œåœ¨ä¿æŒSAMé›¶æ ·æœ¬å…ˆéªŒçš„åŒæ—¶è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„è®¡ç®—æ•ˆç‡å’Œç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œåœ¨ä¹ä¸ªåŒ»å­¦åˆ†å‰²æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_33">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œæœºæ„é—´çš„éšç§æ”¿ç­–å¼‚è´¨æ€§ä½¿å¾—è”åˆè®­ç»ƒä¸å¯è¡Œï¼Œè€ŒSegment Anything Modelè™½ç„¶æä¾›å¼ºå¤§é›¶æ ·æœ¬å…ˆéªŒï¼Œä½†å…¶å¤§å‚æ•°é‡å’Œè®¡ç®—å¼€é”€é™åˆ¶äº†å®é™…éƒ¨ç½²ï¼Œéœ€è¦åœ¨è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</p>
<p><strong>Method:</strong> æå‡ºå¯¹é½å±‚ä½œä¸ºè½»é‡çº§å³æ’å³ç”¨æ¨¡å—ï¼Œé€šè¿‡å¯¹é½ç¼–ç å™¨-è§£ç å™¨ç‰¹å¾åˆ†å¸ƒæ¥é«˜æ•ˆé€‚é…SAMåˆ°ç‰¹å®šåŒ»å­¦å›¾åƒï¼›åŸºäºæ­¤æ„å»ºCA-SAMæŒç»­å­¦ä¹ ç­–ç•¥ï¼Œè‡ªåŠ¨é€‚é…åˆé€‚çš„å¯¹é½å±‚ä»¥ç¼“è§£ç¾éš¾æ€§é—å¿˜ï¼ŒåŒæ—¶åˆ©ç”¨SAMçš„é›¶æ ·æœ¬å…ˆéªŒä¿æŒå¯¹æœªè§æ•°æ®é›†çš„å¼ºæ€§èƒ½ã€‚</p>
<p><strong>Result:</strong> åœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸‹å¯¹ä¹ä¸ªåŒ»å­¦åˆ†å‰²æ•°æ®é›†çš„å®éªŒè¡¨æ˜ï¼ŒCA-SAMå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚</p>
<p><strong>Conclusion:</strong> SAMèŒƒå¼åœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½åå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯¹é½å±‚å’ŒCA-SAMç­–ç•¥ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æŒç»­å­¦ä¹ æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹é€‚åº”æ•°æ®æµå˜åŒ–ã€‚</p>
<hr />
<h4 id="abstract_33">ğŸ“„ Abstract</h4>
<p>In medical image segmentation, heterogeneous privacy policies across institutions often make joint training on pooled datasets infeasible, motivating continual image segmentation-learning from data streams without catastrophic forgetting. While the Segment Anything Model (SAM) offers strong zero-shot priors and has been widely fine-tuned across downstream tasks, its large parameter count and computational overhead challenge practical deployment. This paper demonstrates that the SAM paradigm is highly promising once its computational efficiency and performance can be balanced. To this end, we introduce the Alignment Layer, a lightweight, plug-and-play module which aligns encoder-decoder feature distributions to efficiently adapt SAM to specific medical images, improving accuracy while reducing computation. Building on SAM and the Alignment Layer, we then propose Continual Alignment for SAM (CA-SAM), a continual learning strategy that automatically adapts the appropriate Alignment Layer to mitigate catastrophic forgetting, while leveraging SAM's zero-shot priors to preserve strong performance on unseen medical datasets. Experimented across nine medical segmentation datasets under continual-learning scenario, CA-SAM achieves state-of-the-art performance. Our code, models and datasets will be released on \mbox{https://github.com/azzzzyo/Continual-Alignment-for-SAM.}</p>
<h3 id="35-scaling-self-supervised-and-cross-modal-pretraining-for-volumetric-ct-transformers">[35] <a href="https://arxiv.org/abs/2511.17209">Scaling Self-Supervised and Cross-Modal Pretraining for Volumetric CT Transformers</a></h3>
<p><em>Cris Claessens, Christiaan Viviers, Giacomo D'Amicantonio, Egor Bondarev, Fons van der Sommen</em></p>
<h4 id="tldr_34">ğŸ§© TL;DR</h4>
<p>SPECTREæ˜¯ä¸€ä¸ªåŸºäºTransformerçš„3D CTåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆè‡ªç›‘ç£å­¦ä¹ å’Œè§†è§‰è¯­è¨€é¢„è®­ç»ƒç­–ç•¥ï¼Œåœ¨å¤šä¸ªCTåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†æ— éœ€ç§æœ‰æ•°æ®å³å¯è·å¾—é«˜æ€§èƒ½é€šç”¨CTè¡¨ç¤ºã€‚</p>
<hr />
<h4 id="detailed-summary_34">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ä½“ç§¯CTé¢ä¸´çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æç«¯ä»¤ç‰Œç¼©æ”¾ã€å‡ ä½•å„å‘å¼‚æ€§ä»¥åŠå¼±æˆ–å˜ˆæ‚çš„ä¸´åºŠç›‘ç£ï¼Œè¿™äº›æŒ‘æˆ˜ä½¿å¾—æ ‡å‡†Transformerå’Œå¯¹æ¯”å­¦ä¹ æ–¹æ³•éš¾ä»¥ç›´æ¥æœ‰æ•ˆåº”ç”¨ã€‚</p>
<p><strong>Method:</strong> SPECTREé‡‡ç”¨å¯æ‰©å±•çš„3Dè§†è§‰Transformeræ¶æ„ï¼Œè”åˆä¼˜åŒ–å±€éƒ¨Transformerè¿›è¡Œé«˜åˆ†è¾¨ç‡ä½“ç§¯ç‰¹å¾æå–å’Œå…¨å±€Transformerè¿›è¡Œå…¨æ‰«æä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œç»“åˆDINOé£æ ¼çš„è‡ªè’¸é¦å’ŒåŸºäºSigLIPçš„è§†è§‰è¯­è¨€å¯¹é½ä½¿ç”¨é…å¯¹æ”¾å°„å­¦æŠ¥å‘Šè¿›è¡Œé¢„è®­ç»ƒã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šä¸ªCTåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPECTREåœ¨é›¶æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸‹å§‹ç»ˆä¼˜äºå…ˆå‰çš„CTåŸºç¡€æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨3DåŒ»å­¦æˆåƒä¸­çš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¡¨æ˜é«˜æ€§èƒ½ã€å¯æ³›åŒ–çš„CTè¡¨ç¤ºå¯ä»¥é€šè¿‡å®Œå…¨ä½¿ç”¨å…¬å¼€å¯ç”¨æ•°æ®é›†å®ç°ï¼Œæ— éœ€ä¾èµ–ç§æœ‰æ•°æ®ï¼Œä¸º3DåŒ»å­¦æˆåƒæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€å¼€æ”¾ä¸”å®Œå…¨åŸºäºTransformerçš„åŸºç¡€æ¨¡å‹è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_34">ğŸ“„ Abstract</h4>
<p>We introduce SPECTRE, a fully transformer-based foundation model for volumetric computed tomography (CT). Our Self-Supervised &amp; Cross-Modal Pretraining for CT Representation Extraction (SPECTRE) approach utilizes scalable 3D Vision Transformer architectures and modern self-supervised and vision-language pretraining strategies to learn general-purpose CT representations. Volumetric CT poses unique challenges, such as extreme token scaling, geometric anisotropy, and weak or noisy clinical supervision, that make standard transformer and contrastive learning recipes ineffective out of the box. The framework jointly optimizes a local transformer for high-resolution volumetric feature extraction and a global transformer for whole-scan context modeling, making large-scale 3D attention computationally tractable. Notably, SPECTRE is trained exclusively on openly available CT datasets, demonstrating that high-performing, generalizable representations can be achieved without relying on private data. Pretraining combines DINO-style self-distillation with SigLIP-based vision-language alignment using paired radiology reports, yielding features that are both geometrically consistent and clinically meaningful. Across multiple CT benchmarks, SPECTRE consistently outperforms prior CT foundation models in both zero-shot and fine-tuned settings, establishing SPECTRE as a scalable, open, and fully transformer-based foundation model for 3D medical imaging.</p>
<h3 id="36-a-little-more-like-this-text-to-image-retrieval-with-vision-language-models-using-relevance-feedback">[36] <a href="https://arxiv.org/abs/2511.17255">A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback</a></h3>
<p><em>Bulat Khaertdinov, Mirela Popa, Nava Tintarev</em></p>
<h4 id="tldr_35">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç›¸å…³æ€§åé¦ˆçš„æœºåˆ¶æ¥æ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹çš„æ£€ç´¢æ€§èƒ½ï¼Œæ— éœ€å¾®è°ƒå³å¯åœ¨æ¨ç†æ—¶æå‡æ£€ç´¢æ•ˆæœã€‚è¯¥æ–¹æ³•åŒ…æ‹¬å››ç§åé¦ˆç­–ç•¥ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ£€ç´¢ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å°è§„æ¨¡æ¨¡å‹ä¸­æ•ˆæœæ›´ä¸ºæ˜æ˜¾ã€‚</p>
<hr />
<h4 id="detailed-summary_35">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è™½ç„¶æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„è§†è§‰æœç´¢ï¼Œä½†æå‡æ€§èƒ½é€šå¸¸éœ€è¦å¾®è°ƒæˆ–æ‰©å±•æ¨¡å‹è§„æ¨¡ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæ¢ç´¢åœ¨æ¨ç†æ—¶é€šè¿‡ç›¸å…³æ€§åé¦ˆæœºåˆ¶æ¥æ”¹è¿›æ£€ç´¢æ€§èƒ½ï¼Œé¿å…å¯¹æ¨¡å‹è¿›è¡Œé‡æ–°è®­ç»ƒæˆ–æ‰©å±•ã€‚</p>
<p><strong>Method:</strong> æå‡ºäº†å››ç§ç›¸å…³æ€§åé¦ˆç­–ç•¥ï¼šç»å…¸ä¼ªç›¸å…³æ€§åé¦ˆé€šè¿‡åŸºäºæ’åé å‰ç»“æœä¼˜åŒ–æŸ¥è¯¢åµŒå…¥ï¼›ç”Ÿæˆå¼ç›¸å…³æ€§åé¦ˆåˆ©ç”¨åˆæˆå­—å¹•è¿›è¡ŒæŸ¥è¯¢ä¼˜åŒ–ï¼›æ³¨æ„åŠ›åé¦ˆæ±‡æ€»å™¨é‡‡ç”¨å®šåˆ¶åŒ–transformeræ¨¡å‹æ•´åˆå¤šæ¨¡æ€ç»†ç²’åº¦ç‰¹å¾ï¼›ä»¥åŠä½¿ç”¨çœŸå®å­—å¹•ä½œä¸ºæ˜¾å¼åé¦ˆçš„ä¸Šç•ŒåŸºå‡†ã€‚</p>
<p><strong>Result:</strong> åœ¨Flickr30kå’ŒCOCOæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç”Ÿæˆå¼ç›¸å…³æ€§åé¦ˆã€æ³¨æ„åŠ›åé¦ˆæ±‡æ€»å™¨å’Œæ˜¾å¼åé¦ˆç›¸æ¯”æ— åé¦ˆæ£€ç´¢ï¼Œåœ¨å°è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ä¸­MRR@5æŒ‡æ ‡æå‡3-5%ï¼Œåœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­æå‡1-3%ã€‚æ³¨æ„åŠ›åé¦ˆæ±‡æ€»å™¨åœ¨è¿­ä»£å¤šè½®æ£€ç´¢è®¾ç½®ä¸­è¡¨ç°å‡ºæ›´å¥½çš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£æŸ¥è¯¢æ¼‚ç§»é—®é¢˜ã€‚</p>
<p><strong>Conclusion:</strong> ç›¸å…³æ€§åé¦ˆæœºåˆ¶èƒ½å¤ŸæŒç»­æå‡ä¸åŒè§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ£€ç´¢æ€§èƒ½ï¼Œä¸ºäº¤äº’å¼å’Œè‡ªé€‚åº”è§†è§‰æœç´¢å¼€è¾Ÿäº†æ–°æœºé‡ã€‚è¯¥æ–¹æ³•å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œæ—¢å¯ä»¥ä½œä¸ºå¾®è°ƒçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¹Ÿå¯ä»¥ä¸å·²å¾®è°ƒæ¨¡å‹ååŒä½¿ç”¨ï¼Œå±•ç¤ºäº†åœ¨æ¨ç†é˜¶æ®µä¼˜åŒ–æ£€ç´¢æ•ˆæœçš„æœ‰æ•ˆé€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_35">ğŸ“„ Abstract</h4>
<p>Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search.</p>
<h3 id="37-spatialgeoboosting-spatial-reasoning-in-multimodal-llms-via-geometry-semantics-fusion">[37] <a href="https://arxiv.org/abs/2511.17308">SpatialGeo:Boosting Spatial Reasoning in Multimodal LLMs via Geometry-Semantics Fusion</a></h3>
<p><em>Jiajie Guo, Qingpeng Zhu, Jin Zeng, Xiaolong Wu, Changyong He, Weida Wang</em></p>
<h4 id="tldr_36">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºSpatialGeoï¼Œä¸€ç§åŸºäºå‡ ä½•ä¸è¯­ä¹‰ç‰¹å¾åˆ†å±‚èåˆçš„æ–°å‹è§†è§‰ç¼–ç å™¨ï¼Œé€šè¿‡ç”Ÿæˆç©ºé—´æ„ŸçŸ¥çš„è§†è§‰åµŒå…¥æ¥å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œåœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ€§èƒ½å¹¶é™ä½å†…å­˜æˆæœ¬ã€‚</p>
<hr />
<h4 id="detailed-summary_36">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸‰ç»´ç©ºé—´è§£é‡Šå’Œæ¨ç†ç©ºé—´å¸ƒå±€æ–¹é¢å­˜åœ¨èƒ½åŠ›é™åˆ¶ï¼Œä¸»è¦é—®é¢˜æºäºç°æœ‰è§†è§‰ç¼–ç å™¨ï¼ˆå¦‚CLIPï¼‰çš„æŸå¤±æ€§åµŒå…¥ä»…å…³æ³¨å®ä¾‹çº§è¯­ä¹‰ç‰¹å¾ï¼Œå¯¼è‡´ç©ºé—´æ¨¡ç³Šæ€§ç¼ºé™·ã€‚</p>
<p><strong>Method:</strong> æå‡ºåŸºäºå‡ ä½•ä¸è¯­ä¹‰ç‰¹å¾åˆ†å±‚èåˆçš„è§†è§‰ç¼–ç å™¨SpatialGeoï¼Œé€šè¿‡åˆ†å±‚é€‚é…å™¨å°†è‡ªç›‘ç£å­¦ä¹ è·å¾—çš„å‡ ä½•ç‰¹å¾ä¸CLIPçš„è¯­ä¹‰ç‰¹å¾äº’è¡¥ï¼Œé‡‡ç”¨éšæœºç‰¹å¾ä¸¢å¼ƒç­–ç•¥é¿å…ä»…ä¾èµ–CLIPç¼–ç å™¨çš„å¹³å‡¡è§£ï¼Œä½¿ç”¨é¢„è®­ç»ƒLLaVAæ¨¡å‹è¿›è¡Œé«˜æ•ˆè®­ç»ƒã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒSpatialGeoåœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æå‡å‡†ç¡®ç‡ï¼Œåœ¨SpatialRGPT-Benchä¸Šå°†æœ€å…ˆè¿›æ¨¡å‹çš„æ€§èƒ½æå‡è‡³å°‘8.0%ï¼ŒåŒæ—¶æ¨ç†æœŸé—´å†…å­˜æˆæœ¬é™ä½çº¦50%ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶æ­ç¤ºäº†å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›çš„å…³é”®åœ¨äºèåˆå‡ ä½•ä¸è¯­ä¹‰ç‰¹å¾ï¼Œæå‡ºçš„åˆ†å±‚èåˆæ–¹æ³•ä¸ºæå‡ç©ºé—´æ¨ç†æ€§èƒ½æä¾›äº†æœ‰æ•ˆé€”å¾„ï¼ŒåŒæ—¶å®ç°äº†æ€§èƒ½æå‡ä¸è®¡ç®—æ•ˆç‡çš„å¹³è¡¡ã€‚</p>
<hr />
<h4 id="abstract_36">ğŸ“„ Abstract</h4>
<p>Multimodal large language models (MLLMs) have achieved significant progress in image and language tasks due to the strong reasoning capability of large language models (LLMs). Nevertheless, most MLLMs suffer from limited spatial reasoning ability to interpret and infer spatial arrangements in three-dimensional space. In this work, we propose a novel vision encoder based on hierarchical fusion of geometry and semantics features, generating spatial-aware visual embedding and boosting the spatial grounding capability of MLLMs. Specifically, we first unveil that the spatial ambiguity shortcoming stems from the lossy embedding of the vision encoder utilized in most existing MLLMs (e.g., CLIP), restricted to instance-level semantic features. This motivates us to complement CLIP with the geometry features from vision-only self-supervised learning via a hierarchical adapter, enhancing the spatial awareness in the proposed SpatialGeo. The network is efficiently trained using pretrained LLaVA model and optimized with random feature dropping to avoid trivial solutions relying solely on the CLIP encoder. Experimental results show that SpatialGeo improves the accuracy in spatial reasoning tasks, enhancing state-of-the-art models by at least 8.0% in SpatialRGPT-Bench with approximately 50% less memory cost during inference. The source code is available via https://ricky-plus.github.io/SpatialGeoPages/.</p>
<h3 id="38-loomis-painter-reconstructing-the-painting-process">[38] <a href="https://arxiv.org/abs/2511.17344">Loomis Painter: Reconstructing the Painting Process</a></h3>
<p><em>Markus Pobitzer, Chang Liu, Chenyi Zhuang, Teng Long, Bin Ren, Nicu Sebe</em></p>
<h4 id="tldr_37">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šåª’ä½“ç»˜ç”»è¿‡ç¨‹ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰é©±åŠ¨çš„é£æ ¼æ§åˆ¶æœºåˆ¶å’Œè·¨åª’ä½“é£æ ¼å¢å¼ºï¼Œå®ç°äº†è·¨é£æ ¼çš„ä¸€è‡´çº¹ç†æ¼”åŒ–å’Œè¿‡ç¨‹è¿ç§»ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åå‘ç»˜ç”»è®­ç»ƒç­–ç•¥ç¡®ä¿ç”Ÿæˆè¿‡ç¨‹å¹³æ»‘ä¸”ç¬¦åˆäººç±»åˆ›ä½œæµç¨‹ï¼Œå¹¶åœ¨å¤§è§„æ¨¡çœŸå®ç»˜ç”»è¿‡ç¨‹æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_37">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰çš„ç»˜ç”»æ•™ç¨‹è§†é¢‘èµ„æºç¼ºä¹äº¤äº’æ€§å’Œä¸ªæ€§åŒ–ï¼Œè€Œå½“å‰çš„ç”Ÿæˆæ¨¡å‹åœ¨è·¨åª’ä½“æ³›åŒ–æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç»å¸¸å‡ºç°æ—¶é—´æˆ–ç»“æ„ä¸Šçš„ä¸ä¸€è‡´æ€§ï¼Œéš¾ä»¥å¿ å®å†ç°äººç±»åˆ›ä½œæµç¨‹ã€‚è¿™é™åˆ¶äº†è‰ºæœ¯å­¦ä¹ è¿‡ç¨‹ä¸­å¯¹å¤šæ ·åŒ–ç»˜ç”»åª’ä»‹å’Œé£æ ¼çš„æŠ€æœ¯æŒæ¡ã€‚</p>
<p><strong>Method:</strong> æå‡ºç»Ÿä¸€çš„å¤šåª’ä½“ç»˜ç”»è¿‡ç¨‹ç”Ÿæˆæ¡†æ¶ï¼Œé‡‡ç”¨è¯­ä¹‰é©±åŠ¨çš„é£æ ¼æ§åˆ¶æœºåˆ¶å°†å¤šç§åª’ä½“åµŒå…¥æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ç©ºé—´ï¼Œå¹¶åˆ©ç”¨è·¨åª’ä½“é£æ ¼å¢å¼ºæŠ€æœ¯ã€‚é€šè¿‡åå‘ç»˜ç”»è®­ç»ƒç­–ç•¥ç¡®ä¿ç”Ÿæˆè¿‡ç¨‹çš„å¹³æ»‘æ€§å’Œäººç±»å¯¹é½æ€§ï¼ŒåŒæ—¶æ„å»ºäº†å¤§è§„æ¨¡çœŸå®ç»˜ç”»è¿‡ç¨‹æ•°æ®é›†ç”¨äºæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚</p>
<p><strong>Result:</strong> åœ¨è·¨åª’ä½“ä¸€è‡´æ€§ã€æ—¶é—´è¿è´¯æ€§å’Œæœ€ç»ˆå›¾åƒä¿çœŸåº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œåœ¨LPIPSã€DINOå’ŒCLIPç­‰æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚æå‡ºçš„æ„ŸçŸ¥è·ç¦»è½®å»“æ›²çº¿èƒ½å¤Ÿå®šé‡å»ºæ¨¡åˆ›ä½œåºåˆ—ï¼ŒåŒ…æ‹¬æ„å›¾ã€è‰²å½©åˆ†å—å’Œç»†èŠ‚ç²¾ç‚¼ç­‰é˜¶æ®µï¼Œå‡†ç¡®åæ˜ äº†äººç±»è‰ºæœ¯åˆ›ä½œè¿›ç¨‹ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºè‰ºæœ¯æ•™è‚²æä¾›äº†äº¤äº’å¼å’Œä¸ªæ€§åŒ–çš„å­¦ä¹ å·¥å…·ï¼Œé€šè¿‡ç»Ÿä¸€çš„ç”Ÿæˆæ¡†æ¶è§£å†³äº†è·¨åª’ä½“ç»˜ç”»è¿‡ç¨‹ç”Ÿæˆçš„æŒ‘æˆ˜ã€‚æ„ŸçŸ¥è·ç¦»è½®å»“æ›²çº¿ä¸ºé‡åŒ–åˆ†æåˆ›ä½œè¿‡ç¨‹æä¾›äº†æ–°æ–¹æ³•ï¼Œåå‘ç»˜ç”»è®­ç»ƒç­–ç•¥ç¡®ä¿äº†ç”Ÿæˆè¿‡ç¨‹ä¸äººç±»åˆ›ä½œä¹ æƒ¯çš„ä¸€è‡´æ€§ï¼Œä¸ºæœªæ¥è‰ºæœ¯ç”ŸæˆæŠ€æœ¯çš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_37">ğŸ“„ Abstract</h4>
<p>Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation. This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS, DINO, and CLIP metrics. Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.</p>
<h3 id="39-uam-a-unified-attention-mamba-backbone-of-multimodal-framework-for-tumor-cell-classification">[39] <a href="https://arxiv.org/abs/2511.17355">UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification</a></h3>
<p><em>Taixi Chen, Jingyun Chen, Nancy Guo</em></p>
<h4 id="tldr_38">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ³¨æ„åŠ›-æ›¼å·´ï¼ˆUAMï¼‰ä¸»å¹²ç½‘ç»œï¼Œä¸“é—¨ç”¨äºç»†èƒçº§æ”¾å°„ç»„å­¦ç‰¹å¾åˆ†ç±»ï¼Œå¹¶è¿›ä¸€æ­¥æ‰©å±•ä¸ºå¤šæ¨¡æ€æ¡†æ¶ï¼Œåœ¨ç»†èƒåˆ†ç±»å’Œè‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶å¡«è¡¥äº†ç»†èƒçº§æ”¾å°„ç»„å­¦åˆ†æé¢†åŸŸçš„ç©ºç™½ï¼Œä¸ºåŸºäºæ”¾å°„ç»„å­¦çš„ç™Œç—‡è¯Šæ–­æä¾›äº†ç»Ÿä¸€ä¸”å¯æ‰©å±•çš„å¤šæ¨¡æ€åŸºç¡€ã€‚</p>
<hr />
<h4 id="detailed-summary_38">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ‡ç‰‡çº§æˆ–æ–‘å—çº§è‚¿ç˜¤åˆ†ç±»ï¼Œè€Œç»†èƒçº§æ”¾å°„ç»„å­¦åˆ†æé¢†åŸŸå°šæœªå……åˆ†æ¢ç´¢ï¼Œä¸”ç›®å‰ç¼ºä¹ä¸“é—¨ä¸ºæ”¾å°„ç»„å­¦æ•°æ®è®¾è®¡çš„ä¸“ç”¨ä¸»å¹²ç½‘ç»œã€‚ç»†èƒçº§æ”¾å°„ç»„å­¦ç‰¹å¾èƒ½å¤Ÿæä¾›ç»†ç²’åº¦çš„è‚¿ç˜¤è¡¨å‹æ´å¯Ÿï¼Œä½†è¿™ä¸€æ½œåŠ›å°šæœªå¾—åˆ°æœ‰æ•ˆæŒ–æ˜ã€‚</p>
<p><strong>Method:</strong> å—Mambaæ¶æ„åœ¨è§†è§‰å’Œè¯­è¨€é¢†åŸŸæˆåŠŸçš„å¯å‘ï¼Œæå‡ºäº†ç»Ÿä¸€æ³¨æ„åŠ›-æ›¼å·´ï¼ˆUAMï¼‰ä¸»å¹²ç½‘ç»œç”¨äºç»†èƒçº§æ”¾å°„ç»„å­¦ç‰¹å¾åˆ†ç±»ã€‚ä¸ä¹‹å‰å›ºå®šæ¯”ä¾‹é›†æˆæ³¨æ„åŠ›å’ŒMambaæ¨¡å—çš„æ··åˆæ–¹æ³•ä¸åŒï¼ŒUAMåœ¨å•ä¸€ç»Ÿä¸€æ¶æ„ä¸­çµæ´»ç»“åˆä¸¤è€…çš„èƒ½åŠ›ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒæ•´æ¯”ä¾‹å¹¶æé«˜äº†ç¼–ç èƒ½åŠ›ã€‚å¼€å‘äº†ä¸¤ç§UAMå˜ä½“ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥æå‡ºå¤šæ¨¡æ€UAMæ¡†æ¶ï¼Œè”åˆæ‰§è¡Œç»†èƒçº§åˆ†ç±»å’Œå›¾åƒåˆ†å‰²ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜ï¼ŒUAMåœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†é¢†å…ˆçš„åŸºäºå›¾åƒçš„åŸºç¡€æ¨¡å‹ã€‚ç»†èƒåˆ†ç±»å‡†ç¡®ç‡ä»74%æå‡è‡³78%ï¼ˆn=349,882ä¸ªç»†èƒï¼‰ï¼Œè‚¿ç˜¤åˆ†å‰²ç²¾åº¦ä»75%æå‡è‡³80%ï¼ˆn=406ä¸ªæ–‘å—ï¼‰ã€‚è¿™äº›æ”¹è¿›è¯æ˜äº†UAMåœ¨æ”¾å°„ç»„å­¦é©±åŠ¨è¯Šæ–­ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> UAMä½œä¸ºä¸€ç§ç»Ÿä¸€ä¸”å¯æ‰©å±•çš„å¤šæ¨¡æ€åŸºç¡€ï¼Œå±•ç°äº†åœ¨æ”¾å°„ç»„å­¦é©±åŠ¨ç™Œç—‡è¯Šæ–­ä¸­çš„å·¨å¤§æ½œåŠ›å’Œæœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºç»†èƒçº§æ”¾å°„ç»„å­¦åˆ†ææä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œç»Ÿä¸€æ¶æ„è®¾è®¡æ¶ˆé™¤äº†æ‰‹åŠ¨å‚æ•°è°ƒæ•´çš„éœ€æ±‚ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹çš„ç¼–ç èƒ½åŠ›å’Œè¯Šæ–­ç²¾åº¦ï¼Œä¸ºç²¾å‡†åŒ»ç–—AIç³»ç»Ÿçš„å‘å±•æä¾›äº†é‡è¦æ”¯æ’‘ã€‚</p>
<hr />
<h4 id="abstract_38">ğŸ“„ Abstract</h4>
<p>Cell-level radiomics features provide fine-grained insights into tumor phenotypes and have the potential to significantly enhance diagnostic accuracy on hematoxylin and eosin (H&amp;E) images. By capturing micro-level morphological and intensity patterns, these features support more precise tumor identification and improve AI interpretability by highlighting diagnostically relevant cells for pathologist review. However, most existing studies focus on slide-level or patch-level tumor classification, leaving cell-level radiomics analysis largely unexplored. Moreover, there is currently no dedicated backbone specifically designed for radiomics data. Inspired by the recent success of the Mamba architecture in vision and language domains, we introduce a Unified Attention-Mamba (UAM) backbone for cell-level classification using radiomics features. Unlike previous hybrid approaches that integrate Attention and Mamba modules in fixed proportions, our unified design flexibly combines their capabilities within a single cohesive architecture, eliminating the need for manual ratio tuning and improving encode capability. We develop two UAM variants to comprehensively evaluate the benefits of this unified structure. Building on this backbone, we further propose a multimodal UAM framework that jointly performs cell-level classification and image segmentation. Experimental results demonstrate that UAM achieves state-of-the-art performance across both tasks on public benchmarks, surpassing leading image-based foundation models. It improves cell classification accuracy from 74% to 78% ($n$=349,882 cells), and tumor segmentation precision from 75% to 80% ($n$=406 patches). These findings highlight the effectiveness and promise of UAM as a unified and extensible multimodal foundation for radiomics-driven cancer diagnosis.</p>
<h3 id="40-atac-augmentation-based-test-time-adversarial-correction-for-clip">[40] <a href="https://arxiv.org/abs/2511.17362">ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP</a></h3>
<p><em>Linxiang Su, AndrÃ¡s Balogh</em></p>
<h4 id="tldr_39">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¢å¼ºçš„æµ‹è¯•æ—¶å¯¹æŠ—æ ¡æ­£æ–¹æ³•ATACï¼Œé€šè¿‡åœ¨CLIPçš„åµŒå…¥ç©ºé—´ä¸­è®¡ç®—å¢å¼ºè¯±å¯¼çš„æ¼‚ç§»å‘é‡æ¥æ¨æ–­è¯­ä¹‰æ¢å¤æ–¹å‘ï¼Œæ˜¾è‘—æå‡äº†CLIPåœ¨é›¶æ ·æœ¬å›¾åƒ-æ–‡æœ¬åŒ¹é…ä»»åŠ¡ä¸­çš„å¯¹æŠ—é²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_39">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡CLIPåœ¨é›¶æ ·æœ¬å›¾åƒ-æ–‡æœ¬åŒ¹é…æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶å¯¹å›¾åƒä¸Šçš„å¯¹æŠ—æ‰°åŠ¨é«˜åº¦è„†å¼±ï¼Œè€Œå¯¹æŠ—å¾®è°ƒæˆæœ¬è¿‡é«˜ï¼Œç°æœ‰çš„æµ‹è¯•æ—¶é˜²å¾¡ç­–ç•¥é²æ£’æ€§ä»ç„¶æœ‰é™ã€‚</p>
<p><strong>Method:</strong> ATACæ–¹æ³•ç›´æ¥åœ¨CLIPçš„åµŒå…¥ç©ºé—´ä¸­æ“ä½œï¼Œè®¡ç®—å¢å¼ºè¯±å¯¼çš„æ¼‚ç§»å‘é‡æ¥æ¨æ–­è¯­ä¹‰æ¢å¤æ–¹å‘ï¼Œå¹¶åŸºäºè¿™äº›æ½œåœ¨æ¼‚ç§»çš„è§’åº¦ä¸€è‡´æ€§æ¥æ ¡æ­£åµŒå…¥è¡¨ç¤ºã€‚</p>
<p><strong>Result:</strong> åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒATACå§‹ç»ˆå®ç°äº†æé«˜çš„é²æ£’æ€§ï¼Œå¹³å‡æ¯”å…ˆå‰æœ€å…ˆè¿›æ–¹æ³•çš„é²æ£’æ€§é«˜å‡ºè¿‘50%ï¼ŒåŒæ—¶ä»…éœ€æœ€å°çš„è®¡ç®—å¼€é”€ï¼Œå¹¶åœ¨éå¸¸è§„å’Œæç«¯è®¾ç½®ä¸‹ä¿æŒæœ€å…ˆè¿›çš„é²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> ATACå±•ç¤ºäº†ä¸€ç§åœ¨CLIPåµŒå…¥ç©ºé—´ä¸­è¿›è¡Œæµ‹è¯•æ—¶å¯¹æŠ—é˜²å¾¡çš„æ–°èŒƒå¼ï¼Œè¯æ˜äº†é€šè¿‡è¯­ä¹‰æ¢å¤æ–¹å‘çš„æ¨æ–­å¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹é²æ£’æ€§ï¼Œä¸ºè½»é‡çº§å¯¹æŠ—é˜²å¾¡æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_39">ğŸ“„ Abstract</h4>
<p>Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images. As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness. In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC). Our method operates directly in the embedding space of CLIP, calculating augmentation-induced drift vectors to infer a semantic recovery direction and correcting the embedding based on the angular consistency of these latent drifts. Across a wide range of benchmarks, ATAC consistently achieves remarkably high robustness, surpassing that of previous state-of-the-art methods by nearly 50\% on average, all while requiring minimal computational overhead. Furthermore, ATAC retains state-of-the-art robustness in unconventional and extreme settings and even achieves nontrivial robustness against adaptive attacks. Our results demonstrate that ATAC is an efficient method in a novel paradigm for test-time adversarial defenses in the embedding space of CLIP.</p>
<h3 id="41-mcmoe-completing-missing-modalities-with-mixture-of-experts-for-incomplete-multimodal-action-quality-assessment">[41] <a href="https://arxiv.org/abs/2511.17397">MCMoE: Completing Missing Modalities with Mixture of Experts for Incomplete Multimodal Action Quality Assessment</a></h3>
<p><em>Huangbiao Xu, Huanqi Wu, Xiao Ke, Junyi Wu, Rui Xu, Jinglin Xu</em></p>
<h4 id="tldr_40">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¼ºå¤±æ¨¡æ€è¡¥å…¨æ¡†æ¶MCMoEï¼Œé€šè¿‡æ··åˆä¸“å®¶æœºåˆ¶ç»Ÿä¸€å•æ¨¡æ€å’Œè”åˆè¡¨ç¤ºå­¦ä¹ ï¼Œè§£å†³äº†å¤šæ¨¡æ€åŠ¨ä½œè´¨é‡è¯„ä¼°ä¸­æ¨¡æ€ç¼ºå¤±å¯¼è‡´çš„æ€§èƒ½é€€åŒ–é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_40">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šæ¨¡æ€åŠ¨ä½œè´¨é‡è¯„ä¼°åœ¨æ¨ç†é˜¶æ®µç»å¸¸é¢ä¸´éƒ¨åˆ†æ¨¡æ€ä¸å¯ç”¨çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨ç¼ºå¤±ä»»ä½•æ¨¡æ€æ—¶æ— æ³•æ­£å¸¸å·¥ä½œï¼Œä¸”ç”±äºè·¨æ¨¡æ€äº¤äº’ä¸­æ–­ä¼šå¯¼è‡´ç¾éš¾æ€§çš„æ€§èƒ½ä¸‹é™ï¼Œè¿™é™åˆ¶äº†å®é™…åº”ç”¨ä¸­çš„é²æ£’æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºè‡ªé€‚åº”é—¨æ§æ¨¡æ€ç”Ÿæˆå™¨åŠ¨æ€èåˆå¯ç”¨ä¿¡æ¯é‡æ„ç¼ºå¤±æ¨¡æ€ï¼Œè®¾è®¡æ¨¡æ€ä¸“å®¶å­¦ä¹ å•æ¨¡æ€çŸ¥è¯†å¹¶åŠ¨æ€æ··åˆæ‰€æœ‰ä¸“å®¶çŸ¥è¯†æå–è·¨æ¨¡æ€è”åˆè¡¨ç¤ºï¼Œé€šè¿‡ä¸“å®¶æ··åˆæœºåˆ¶è¿›ä¸€æ­¥ç»†åŒ–å’Œè¡¥å……ç¼ºå¤±æ¨¡æ€ã€‚</p>
<p><strong>Result:</strong> åœ¨ä¸‰ä¸ªå…¬å…±AQAåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMCMoEåœ¨å®Œæ•´å’Œä¸å®Œæ•´å¤šæ¨¡æ€å­¦ä¹ åœºæ™¯ä¸‹å‡å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼ŒéªŒè¯äº†æ–¹æ³•åœ¨æ¨¡æ€ç¼ºå¤±æƒ…å†µä¸‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ¨¡æ€ç¼ºå¤±é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å•é˜¶æ®µè®­ç»ƒç»Ÿä¸€äº†å•æ¨¡æ€å’Œè”åˆè¡¨ç¤ºå­¦ä¹ ï¼Œä¸ºå®é™…åº”ç”¨ä¸­çš„é²æ£’å¤šæ¨¡æ€è¯„ä¼°ç³»ç»Ÿå¼€å‘æä¾›äº†é‡è¦å‚è€ƒã€‚</p>
<hr />
<h4 id="abstract_40">ğŸ“„ Abstract</h4>
<p>Multimodal Action Quality Assessment (AQA) has recently emerged as a promising paradigm. By leveraging complementary information across shared contextual cues, it enhances the discriminative evaluation of subtle intra-class variations in highly similar action sequences. However, partial modalities are frequently unavailable at the inference stage in reality. The absence of any modality often renders existing multimodal models inoperable. Furthermore, it triggers catastrophic performance degradation due to interruptions in cross-modal interactions. To address this issue, we propose a novel Missing Completion Framework with Mixture of Experts (MCMoE) that unifies unimodal and joint representation learning in single-stage training. Specifically, we propose an adaptive gated modality generator that dynamically fuses available information to reconstruct missing modalities. We then design modality experts to learn unimodal knowledge and dynamically mix the knowledge of all experts to extract cross-modal joint representations. With a mixture of experts, missing modalities are further refined and complemented. Finally, in the training phase, we mine the complete multimodal features and unimodal expert knowledge to guide modality generation and generation-based joint representation extraction. Extensive experiments demonstrate that our MCMoE achieves state-of-the-art results in both complete and incomplete multimodal learning on three public AQA benchmarks. Code is available at https://github.com/XuHuangbiao/MCMoE.</p>
<h3 id="42-mmt-ard-multimodal-multi-teacher-adversarial-distillation-for-robust-vision-language-models">[42] <a href="https://arxiv.org/abs/2511.17448">MMT-ARD: Multimodal Multi-Teacher Adversarial Distillation for Robust Vision-Language Models</a></h3>
<p><em>Yuqi Li, Junhao Dong, Chuanguang Yang, Shiping Wen, Piotr Koniusz, Tingwen Huang, Yingli Tian, Yew-Soon Ong</em></p>
<h4 id="tldr_41">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MMT-ARDå¤šæ¨¡æ€å¤šæ•™å¸ˆå¯¹æŠ—é²æ£’è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡åŒæ•™å¸ˆçŸ¥è¯†èåˆæ¶æ„å’ŒåŠ¨æ€æƒé‡åˆ†é…ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§å’Œè®­ç»ƒæ•ˆç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_41">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿå•æ•™å¸ˆå¯¹æŠ—çŸ¥è¯†è’¸é¦æ–¹æ³•å­˜åœ¨çŸ¥è¯†å¤šæ ·æ€§æœ‰é™ã€æ”¶æ•›é€Ÿåº¦æ…¢ä»¥åŠé²æ£’æ€§ä¸å‡†ç¡®æ€§éš¾ä»¥å¹³è¡¡çš„é—®é¢˜ï¼Œé™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„éƒ¨ç½²å¯é æ€§ã€‚</p>
<p><strong>Method:</strong> æå‡ºå¤šæ¨¡æ€å¤šæ•™å¸ˆå¯¹æŠ—é²æ£’è’¸é¦æ¡†æ¶ï¼Œé‡‡ç”¨åŒæ•™å¸ˆçŸ¥è¯†èåˆæ¶æ„ååŒä¼˜åŒ–æ¸…æ´ç‰¹å¾ä¿æŒå’Œé²æ£’ç‰¹å¾å¢å¼ºï¼Œå¼•å…¥åŸºäºæ•™å¸ˆç½®ä¿¡åº¦çš„åŠ¨æ€æƒé‡åˆ†é…ç­–ç•¥å¤„ç†å›°éš¾å¯¹æŠ—æ ·æœ¬ï¼Œå¹¶è®¾è®¡äº†è‡ªé€‚åº”sigmoidåŠ æƒå‡½æ•°å¹³è¡¡è·¨æ¨¡æ€çŸ¥è¯†ä¼ é€’å¼ºåº¦ã€‚</p>
<p><strong>Result:</strong> åœ¨ImageNetå’Œé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­ï¼ŒViT-B-32æ¨¡å‹çš„é²æ£’å‡†ç¡®ç‡æå‡+4.32%ï¼Œé›¶æ ·æœ¬å‡†ç¡®ç‡æå‡+3.5%ï¼Œè®­ç»ƒæ•ˆç‡ç›¸æ¯”ä¼ ç»Ÿå•æ•™å¸ˆæ–¹æ³•æé«˜2.3å€ã€‚</p>
<p><strong>Conclusion:</strong> MMT-ARDæ¡†æ¶æœ‰æ•ˆè§£å†³äº†å¤šæ•™å¸ˆè’¸é¦ä¸­çš„çŸ¥è¯†èåˆå’Œå¹³è¡¡é—®é¢˜ï¼Œè¯æ˜äº†å¤šæ•™å¸ˆåä½œåœ¨æå‡å¤šæ¨¡æ€å¤§æ¨¡å‹å¯¹æŠ—é²æ£’æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ï¼Œä¸ºå®‰å…¨å…³é”®åº”ç”¨æä¾›äº†å¯é è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_41">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) are increasingly deployed in safety-critical applications, making their adversarial robustness a crucial concern. While adversarial knowledge distillation has shown promise in transferring robustness from teacher to student models, traditional single-teacher approaches suffer from limited knowledge diversity, slow convergence, and difficulty in balancing robustness and accuracy. To address these challenges, we propose MMT-ARD: a Multimodal Multi-Teacher Adversarial Robust Distillation framework. Our key innovation is a dual-teacher knowledge fusion architecture that collaboratively optimizes clean feature preservation and robust feature enhancement. To better handle challenging adversarial examples, we introduce a dynamic weight allocation strategy based on teacher confidence, enabling adaptive focus on harder samples. Moreover, to mitigate bias among teachers, we design an adaptive sigmoid-based weighting function that balances the strength of knowledge transfer across modalities. Extensive experiments on ImageNet and zero-shot benchmarks demonstrate that MMT-ARD improves robust accuracy by +4.32% and zero-shot accuracy by +3.5% on the ViT-B-32 model, while achieving a 2.3x increase in training efficiency over traditional single-teacher methods. These results highlight the effectiveness and scalability of MMT-ARD in enhancing the adversarial robustness of multimodal large models. Our codes are available at https://github.com/itsnotacie/MMT-ARD.</p>
<h3 id="43-improving-multimodal-distillation-for-3d-semantic-segmentation-under-domain-shift">[43] <a href="https://arxiv.org/abs/2511.17455">Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift</a></h3>
<p><em>BjÃ¶rn Michele, Alexandre Boulch, Gilles Puy, Tuan-Hung Vu, Renaud Marlet, Nicolas Courty</em></p>
<h4 id="tldr_42">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶ç³»ç»Ÿæ¢ç´¢äº†å¦‚ä½•åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œæ¿€å…‰é›·è¾¾ç‚¹äº‘è¯­ä¹‰åˆ†å‰²çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼Œå‘ç°éª¨å¹²ç½‘ç»œæ¶æ„å¯¹æ³›åŒ–æ€§èƒ½è‡³å…³é‡è¦ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¯ä¸€æ¬¡æ€§é¢„è®­ç»ƒå¹¶é€‚ç”¨äºå¤šç§åŸŸåç§»çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_42">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> åœ¨å®Œå…¨ç›‘ç£ä¸‹è®­ç»ƒçš„æ¿€å…‰é›·è¾¾è¯­ä¹‰åˆ†å‰²ç½‘ç»œæ— æ³•åœ¨æœªè§è¿‡çš„æ¿€å…‰é›·è¾¾ç±»å‹ä¸Šå®ç°è‰¯å¥½æ³›åŒ–ï¼Œå­˜åœ¨æ˜¾è‘—çš„åŸŸåç§»æ€§èƒ½å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æ—¨åœ¨æ¢ç´¢å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨è·¨åŸŸé²æ£’æ€§å¼ºçš„è§†è§‰åŸºç¡€æ¨¡å‹æ¥æå‡æ¿€å…‰é›·è¾¾ç‚¹äº‘è¯­ä¹‰åˆ†å‰²çš„åŸŸè‡ªé€‚åº”èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> åŸºäºæ— ç›‘ç£å›¾åƒåˆ°æ¿€å…‰é›·è¾¾çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œç³»ç»Ÿç ”ç©¶äº†è§†è§‰åŸºç¡€æ¨¡å‹åœ¨æ¿€å…‰é›·è¾¾è¯­ä¹‰åˆ†å‰²åŸŸè‡ªé€‚åº”ä¸­çš„åº”ç”¨æ–¹æ³•ã€‚å…³é”®å‘ç°åŒ…æ‹¬ï¼šéª¨å¹²ç½‘ç»œæ¶æ„å¯¹ç›®æ ‡åŸŸæ³›åŒ–æ€§èƒ½è‡³å…³é‡è¦ï¼›å¯ä»¥ä¸€æ¬¡æ€§é¢„è®­ç»ƒå•ä¸ªéª¨å¹²ç½‘ç»œå¹¶åº”ç”¨äºå¤šç§åŸŸåç§»åœºæ™¯ï¼›æœ€ä½³ç»“æœé€šè¿‡å†»ç»“é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œå¹¶è®­ç»ƒMLPåˆ†å‰²å¤´è·å¾—ã€‚</p>
<p><strong>Result:</strong> æå‡ºçš„æ–¹æ³•åœ¨å››ä¸ªå¹¿æ³›è®¤å¯ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸŸè‡ªé€‚åº”è®¾ç½®ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®éªŒéªŒè¯äº†æ‰€ææ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—å‡å°‘äº†ä¸åŒæ¿€å…‰é›·è¾¾ç±»å‹ä¹‹é—´çš„åŸŸåç§»æ€§èƒ½å·®è·ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜æ¿€å…‰é›·è¾¾éª¨å¹²ç½‘ç»œæ¶æ„æ˜¯æœ€å¤§åŒ–åŸŸè‡ªé€‚åº”æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œä¸”å¯ä»¥é€šè¿‡ä¸€æ¬¡æ€§é¢„è®­ç»ƒå®ç°å¤šåŸŸæ³›åŒ–ã€‚å†»ç»“é¢„è®­ç»ƒéª¨å¹²å¹¶è®­ç»ƒè½»é‡çº§åˆ†å‰²å¤´çš„ç­–ç•¥è¢«è¯æ˜æ˜¯æœ€æœ‰æ•ˆçš„ï¼Œä¸ºæ¿€å…‰é›·è¾¾è¯­ä¹‰åˆ†å‰²çš„åŸŸè‡ªé€‚åº”æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆå’Œè®¾è®¡æŒ‡å¯¼ã€‚</p>
<hr />
<h4 id="abstract_42">ğŸ“„ Abstract</h4>
<p>Semantic segmentation networks trained under full supervision for one type of lidar fail to generalize to unseen lidars without intervention. To reduce the performance gap under domain shifts, a recent trend is to leverage vision foundation models (VFMs) providing robust features across domains. In this work, we conduct an exhaustive study to identify recipes for exploiting VFMs in unsupervised domain adaptation for semantic segmentation of lidar point clouds. Building upon unsupervised image-to-lidar knowledge distillation, our study reveals that: (1) the architecture of the lidar backbone is key to maximize the generalization performance on a target domain; (2) it is possible to pretrain a single backbone once and for all, and use it to address many domain shifts; (3) best results are obtained by keeping the pretrained backbone frozen and training an MLP head for semantic segmentation. The resulting pipeline achieves state-of-the-art results in four widely-recognized and challenging settings. The code will be available at: https://github.com/valeoai/muddos.</p>
<h3 id="44-downscaling-intelligence-exploring-perception-and-reasoning-bottlenecks-in-small-multimodal-models">[44] <a href="https://arxiv.org/abs/2511.17487">Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models</a></h3>
<p><em>Mark Endo, Serena Yeung-Levy</em></p>
<h4 id="tldr_43">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºExtract+Thinkçš„é«˜æ•ˆå¤šæ¨¡æ€æ¨¡å‹æ–¹æ³•ï¼Œé€šè¿‡è§†è§‰æå–è°ƒä¼˜å’Œé€æ­¥æ¨ç†æ¥è§£å†³LLMé™ç»´å¯¹è§†è§‰èƒ½åŠ›çš„ä¸æˆæ¯”ä¾‹å½±å“ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡æ¨¡å‹æ•ˆç‡ã€‚</p>
<hr />
<h4 id="detailed-summary_43">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> éšç€å¤šæ¨¡æ€æ¨¡å‹çš„è§„æ¨¡åŒ–å‘å±•ï¼Œå®é™…åº”ç”¨éœ€æ±‚è¦æ±‚æ›´å°ã€æ›´é«˜æ•ˆçš„ç³»ç»Ÿã€‚æœ¬ç ”ç©¶æ—¨åœ¨ç³»ç»Ÿåˆ†æå¤šæ¨¡æ€æ¨¡å‹ä¸­æ™ºèƒ½é™ç»´çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯æ¢ç©¶å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹å®¹é‡å¦‚ä½•å½±å“å¤šæ¨¡æ€èƒ½åŠ›ï¼Œå¹¶è§£å†³LLMé™ç»´å¯¹è§†è§‰èƒ½åŠ›é€ æˆçš„è¿‡åº¦æŸå®³é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æå‡ºExtract+Thinkæ–¹æ³•ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè§†è§‰æå–è°ƒä¼˜ï¼Œé€šè¿‡æ˜¾å¼è®­ç»ƒæ¨¡å‹æå–ä¸æŒ‡ä»¤ç›¸å…³çš„è§†è§‰ç»†èŠ‚ï¼›ä»¥åŠé€æ­¥æ¨ç†æœºåˆ¶ï¼Œåˆ©ç”¨æå–çš„è§†è§‰ç»†èŠ‚ç”Ÿæˆç­”æ¡ˆã€‚è¯¥æ–¹æ³•ä¸“é—¨é’ˆå¯¹LLMé™ç»´å¯¼è‡´çš„è§†è§‰èƒ½åŠ›ç“¶é¢ˆè¿›è¡Œä¼˜åŒ–ã€‚</p>
<p><strong>Result:</strong> ç ”ç©¶å‘ç°LLMé™ç»´å¯¹è§†è§‰èƒ½åŠ›çš„å½±å“ä¸æˆæ¯”ä¾‹åœ°å¤§äºå¯¹LLMç»§æ‰¿èƒ½åŠ›çš„å½±å“ã€‚è§†è§‰æå–è°ƒä¼˜æ˜¾è‘—ç¼“è§£äº†è¿™ä¸€ç“¶é¢ˆï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸­å®ç°äº†æ•ˆç‡ä¸æ€§èƒ½çš„æ–°å¹³è¡¡ï¼Œè§†è§‰æ„ŸçŸ¥èƒ½åŠ›çš„ä¸‹é™å¹…åº¦ç”šè‡³è¶…è¿‡æ¨ç†èƒ½åŠ›çš„ä¸‹é™ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€æ¨¡å‹ä¸­è§†è§‰èƒ½åŠ›å¯¹LLMå®¹é‡çš„æ•æ„Ÿæ€§ï¼Œæå‡ºçš„Extract+Thinkæ¡†æ¶ä¸ºæ„å»ºé«˜æ•ˆå¤šæ¨¡æ€ç³»ç»Ÿæä¾›äº†æ–°èŒƒå¼ï¼Œå¼ºè°ƒäº†è§†è§‰ä¿¡æ¯æå–åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶ä¸ºæœªæ¥è½»é‡åŒ–å¤šæ¨¡æ€æ¨¡å‹è®¾è®¡æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_43">ğŸ“„ Abstract</h4>
<p>Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.</p>
<h3 id="45-video-r4-reinforcing-text-rich-video-reasoning-with-visual-rumination">[45] <a href="https://arxiv.org/abs/2511.17490">Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination</a></h3>
<p><em>Yolo Yunlong Tang, Daiki Shimada, Hang Hua, Chao Huang, Jing Bi, Rogerio Feris, Chenliang Xu</em></p>
<h4 id="tldr_44">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºVideo-R4ï¼Œä¸€ç§åŸºäºè§†è§‰ååˆæœºåˆ¶çš„è§†é¢‘æ¨ç†å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è¿­ä»£é€‰æ‹©å¸§ã€æ”¾å¤§å…³é”®åŒºåŸŸå’Œé‡æ–°ç¼–ç åƒç´ æ¥å¢å¼ºæ–‡æœ¬ä¸°å¯Œè§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªè§†é¢‘é—®ç­”åŸºå‡†ä¸Šå–å¾—æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†è¿­ä»£ååˆä½œä¸ºåƒç´ åŸºç¡€å¤šæ¨¡æ€æ¨ç†çš„æœ‰æ•ˆèŒƒå¼ã€‚</p>
<hr />
<h4 id="detailed-summary_44">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è§†é¢‘é—®ç­”æ¨¡å‹ä¸»è¦ä¾èµ–å•æ¬¡æ„ŸçŸ¥å›ºå®šå¸§ï¼Œå¯¼è‡´åœ¨ç»†ç²’åº¦è¯æ®ç†è§£ä¸Šå‡ºç°å¹»è§‰å’Œå¤±è´¥é—®é¢˜ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†éœ€è¦é‡å¤æ£€æŸ¥çš„ç¬æ€æ–‡æœ¬çº¿ç´¢ã€‚äººç±»è§‚çœ‹æ–‡æœ¬ä¸°å¯Œè§†é¢‘æ—¶ä¼šæš‚åœã€æ”¾å¤§å’Œé‡è¯»å…³é”®åŒºåŸŸï¼Œè¿™ç§èƒ½åŠ›æ˜¯å½“å‰æ¨¡å‹æ‰€ç¼ºä¹çš„ã€‚</p>
<p><strong>Method:</strong> æå‡ºè§†è§‰ååˆæœºåˆ¶ï¼Œé€šè¿‡è¿­ä»£é€‰æ‹©å¸§ã€æ”¾å¤§ä¿¡æ¯åŒºåŸŸã€é‡æ–°ç¼–ç æ£€ç´¢åƒç´ å’Œæ›´æ–°æ¨ç†çŠ¶æ€æ¥å¢å¼ºè§†é¢‘ç†è§£ã€‚æ„å»ºäº†ä¸¤ä¸ªåŒ…å«å¯æ‰§è¡Œååˆè½¨è¿¹çš„æ•°æ®é›†ï¼šVideo-R4-CoT-17kç”¨äºç›‘ç£è®­ç»ƒï¼ŒVideo-R4-RL-30kç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚é‡‡ç”¨å¤šé˜¶æ®µååˆå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå’ŒåŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ é€æ­¥å¾®è°ƒ7Bå‚æ•°çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå­¦ä¹ åŸå­å’Œæ··åˆè§†è§‰æ“ä½œã€‚</p>
<p><strong>Result:</strong> Video-R4-7Båœ¨M4-ViteVQAåŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ç»“æœã€‚è¯¥æ–¹æ³•è¿˜å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”ç”¨äºå¤šé¡µæ–‡æ¡£é—®ç­”ã€å¹»ç¯ç‰‡é—®ç­”å’Œé€šç”¨è§†é¢‘é—®ç­”ä»»åŠ¡ã€‚å®éªŒéªŒè¯äº†è¿­ä»£ååˆæœºåˆ¶åœ¨åƒç´ åŸºç¡€å¤šæ¨¡æ€æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Conclusion:</strong> è¿­ä»£è§†è§‰ååˆæ˜¯ä¸€ç§æœ‰æ•ˆçš„åƒç´ åŸºç¡€å¤šæ¨¡æ€æ¨ç†èŒƒå¼ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹å¯¹æ–‡æœ¬ä¸°å¯Œè§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ä»…è§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨ç»†ç²’åº¦è¯æ®ç†è§£ä¸Šçš„å±€é™æ€§ï¼Œè¿˜å±•ç¤ºäº†åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ï¼Œä¸ºè§†é¢‘ç†è§£å’Œå¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚</p>
<hr />
<h4 id="abstract_44">ğŸ“„ Abstract</h4>
<p>Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.</p>
<h3 id="46-native-3d-editing-with-full-attention">[46] <a href="https://arxiv.org/abs/2511.17501">Native 3D Editing with Full Attention</a></h3>
<p><em>Weiwei Cai, Shuangkang Fang, Weicai Ye, Xin Dong, Yunhan Yang, Xuanyang Zhang, Wei Cheng, Yanpei Cao, Gang Yu, Tao Chen</em></p>
<h4 id="tldr_45">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸç”Ÿ3Dç¼–è¾‘æ¡†æ¶ï¼Œé€šè¿‡å•æ¬¡å‰å‘ä¼ æ’­ç›´æ¥æ“ä½œ3Dè¡¨ç¤ºï¼Œè§£å†³äº†ç°æœ‰ä¼˜åŒ–æ–¹æ³•é€Ÿåº¦æ…¢å’ŒåŸºäºå¤šè§†å›¾2Dç¼–è¾‘æ–¹æ³•å‡ ä½•ä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†å’Œåˆ›æ–°çš„3Dæ ‡è®°æ‹¼æ¥ç­–ç•¥ï¼Œåœ¨ç”Ÿæˆè´¨é‡ã€3Dä¸€è‡´æ€§å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_45">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰æŒ‡ä»¤å¼•å¯¼çš„3Dç¼–è¾‘æ–¹æ³•å­˜åœ¨å…³é”®å±€é™æ€§ï¼šåŸºäºä¼˜åŒ–çš„æ–¹æ³•è®¡ç®—é€Ÿåº¦ææ…¢ï¼Œè€ŒåŸºäºå¤šè§†å›¾2Dç¼–è¾‘çš„å‰é¦ˆæ–¹æ³•å¸¸å¸¸å¯¼è‡´å‡ ä½•ä¸ä¸€è‡´å’Œè§†è§‰è´¨é‡ä¸‹é™ã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†3Då†…å®¹åˆ›ä½œçš„å¹¿æ³›åº”ç”¨ï¼Œè¿«åˆ‡éœ€è¦å¼€å‘æ›´é«˜æ•ˆä¸”ä¿æŒ3Dä¸€è‡´æ€§çš„ç¼–è¾‘æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€æŒ‡ä»¤å¼•å¯¼3Dç¼–è¾‘æ•°æ®é›†ï¼Œæ¶µç›–æ·»åŠ ã€åˆ é™¤å’Œä¿®æ”¹ç­‰å¤šç§ä»»åŠ¡ç±»å‹ï¼Œç¡®ä¿ç¼–è¾‘å¯¹è±¡å¿ å®éµå¾ªæŒ‡ä»¤å˜åŒ–åŒæ—¶ä¿æŒæœªç¼–è¾‘åŒºåŸŸä¸æºå¯¹è±¡çš„ä¸€è‡´æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ¢ç´¢äº†ä¸¤ç§æ¡ä»¶ç­–ç•¥ï¼šä¼ ç»Ÿçš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œåˆ›æ–°çš„3Dæ ‡è®°æ‹¼æ¥æ–¹æ³•ï¼Œåè€…è¢«è¯æ˜å‚æ•°æ•ˆç‡æ›´é«˜ä¸”æ€§èƒ½æ›´ä¼˜ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡ã€3Dä¸€è‡´æ€§å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢å‡ä¼˜äºç°æœ‰çš„2Dæå‡æ–¹æ³•ï¼Œç¡®ç«‹äº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚å®éªŒè¯æ˜3Dæ ‡è®°æ‹¼æ¥ç­–ç•¥ç›¸æ¯”ä¼ ç»Ÿäº¤å‰æ³¨æ„åŠ›æœºåˆ¶å…·æœ‰æ›´å¥½çš„å‚æ•°æ•ˆç‡å’Œæ€§èƒ½è¡¨ç°ï¼Œä¸º3Dç¼–è¾‘ä»»åŠ¡æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†åŸç”Ÿ3Dç¼–è¾‘æ¡†æ¶åœ¨æ•ˆç‡å’Œä¸€è‡´æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸º3Då†…å®¹åˆ›ä½œæä¾›äº†æ›´å®ç”¨çš„å·¥å…·ã€‚3Dæ ‡è®°æ‹¼æ¥ç­–ç•¥çš„ä¼˜è¶Šæ€§ä¸ºæœªæ¥3Dç”Ÿæˆæ¨¡å‹çš„è®¾è®¡æä¾›äº†é‡è¦å¯ç¤ºï¼ŒåŒæ—¶å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºè¯¥é¢†åŸŸçš„å‘å±•å¥ å®šäº†åšå®åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_45">ğŸ“„ Abstract</h4>
<p>Instruction-guided 3D editing is a rapidly emerging field with the potential to broaden access to 3D content creation. However, existing methods face critical limitations: optimization-based approaches are prohibitively slow, while feed-forward approaches relying on multi-view 2D editing often suffer from inconsistent geometry and degraded visual quality. To address these issues, we propose a novel native 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass. Specifically, we create a large-scale, multi-modal dataset for instruction-guided 3D editing, covering diverse addition, deletion, and modification tasks. This dataset is meticulously curated to ensure that edited objects faithfully adhere to the instructional changes while preserving the consistency of unedited regions with the source object. Building upon this dataset, we explore two distinct conditioning strategies for our model: a conventional cross-attention mechanism and a novel 3D token concatenation approach. Our results demonstrate that token concatenation is more parameter-efficient and achieves superior performance. Extensive evaluations show that our method outperforms existing 2D-lifting approaches, setting a new benchmark in generation quality, 3D consistency, and instruction fidelity.</p>
<div id='cs.CL'></div>

<h1 id="cscl-back">cs.CL <a href="#toc">[Back]</a></h1>
<h3 id="47-towards-hyper-efficient-rag-systems-in-vecdbs-distributed-parallel-multi-resolution-vector-search">[47] <a href="https://arxiv.org/abs/2511.16681">Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search</a></h3>
<p><em>Dong Liu, Yanxuan Yu</em></p>
<h4 id="tldr_46">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†è¯­ä¹‰é‡‘å­—å¡”ç´¢å¼•ï¼ˆSPIï¼‰ï¼Œä¸€ç§ç”¨äºå‘é‡æ•°æ®åº“çš„å¤šåˆ†è¾¨ç‡ç´¢å¼•æ¡†æ¶ï¼Œé€šè¿‡æŸ¥è¯¢è‡ªé€‚åº”åˆ†è¾¨ç‡æ§åˆ¶å®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„åŠ é€Ÿå’Œä¼˜åŒ–ã€‚SPIåœ¨ä¿æŒè¯­ä¹‰è¦†ç›–çš„åŒæ—¶æ˜¾è‘—æå‡æ£€ç´¢é€Ÿåº¦ï¼Œå¹¶ä¸ç°æœ‰å‘é‡æ•°æ®åº“åŸºç¡€è®¾æ–½å…¼å®¹ã€‚</p>
<hr />
<h4 id="detailed-summary_46">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å‘é‡æ•°æ®åº“æ£€ç´¢ç®¡é“ä¾èµ–æ‰å¹³æˆ–å•åˆ†è¾¨ç‡ç´¢å¼•ç»“æ„ï¼Œæ— æ³•é€‚åº”å¤šæ ·åŒ–ç”¨æˆ·æŸ¥è¯¢æ‰€éœ€çš„è¯­ä¹‰ç²’åº¦å˜åŒ–ï¼Œå¯¼è‡´æ£€ç´¢é€Ÿåº¦å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ä¹‹é—´çš„æ¬¡ä¼˜æƒè¡¡ã€‚ä¼ ç»Ÿåˆ†å±‚æ–¹æ³•éœ€è¦ç¦»çº¿è°ƒä¼˜æˆ–å•ç‹¬æ¨¡å‹è®­ç»ƒï¼Œé™åˆ¶äº†å®é™…éƒ¨ç½²çš„çµæ´»æ€§ã€‚</p>
<p><strong>Method:</strong> SPIæ„å»ºæ–‡æ¡£åµŒå…¥çš„è¯­ä¹‰é‡‘å­—å¡”ï¼Œé€šè¿‡è½»é‡çº§åˆ†ç±»å™¨ä¸ºæ¯ä¸ªæŸ¥è¯¢åŠ¨æ€é€‰æ‹©æœ€ä¼˜åˆ†è¾¨ç‡çº§åˆ«ï¼Œå®ç°ä»ç²—åˆ°ç»†è¡¨ç¤ºçš„æ¸è¿›å¼æ£€ç´¢ã€‚è¯¥æ–¹æ³•ä½œä¸ºæ’ä»¶å®ç°ï¼Œå…¼å®¹FAISSå’ŒQdrantåç«¯ï¼Œæ— éœ€ç¦»çº¿è°ƒä¼˜æˆ–é¢å¤–æ¨¡å‹è®­ç»ƒã€‚</p>
<p><strong>Result:</strong> åœ¨MS MARCOã€Natural Questionså’Œå¤šæ¨¡æ€æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPIå®ç°äº†é«˜è¾¾5.7å€çš„æ£€ç´¢åŠ é€Ÿå’Œ1.8å€çš„å†…å­˜æ•ˆç‡æå‡ï¼ŒåŒæ—¶å°†ç«¯åˆ°ç«¯QA F1åˆ†æ•°æé«˜äº†æœ€å¤š2.5åˆ†ã€‚ç†è®ºåˆ†ææä¾›äº†æ£€ç´¢è´¨é‡å’Œå»¶è¿Ÿè¾¹ç•Œä¿è¯ï¼Œæ¶ˆèç ”ç©¶éªŒè¯äº†å„ç»„ä»¶è´¡çŒ®ã€‚</p>
<p><strong>Conclusion:</strong> SPIæ¡†æ¶é€šè¿‡è‡ªé€‚åº”å¤šåˆ†è¾¨ç‡ç´¢å¼•è§£å†³äº†RAGç³»ç»Ÿä¸­æ£€ç´¢æ•ˆç‡ä¸è´¨é‡çš„å…³é”®æƒè¡¡é—®é¢˜ï¼Œå…¶ä¸ç°æœ‰å‘é‡æ•°æ®åº“åŸºç¡€è®¾æ–½çš„å…¼å®¹æ€§ä½¿å…¶å¯ç›´æ¥éƒ¨ç½²äºç”Ÿäº§ç³»ç»Ÿã€‚è¯¥å·¥ä½œä¸ºå¤§è§„æ¨¡çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="abstract_46">ğŸ“„ Abstract</h4>
<p>Retrieval-Augmented Generation (RAG) systems have become a dominant approach to augment large language models (LLMs) with external knowledge. However, existing vector database (VecDB) retrieval pipelines rely on flat or single-resolution indexing structures, which cannot adapt to the varying semantic granularity required by diverse user queries. This limitation leads to suboptimal trade-offs between retrieval speed and contextual relevance.
  To address this, we propose \textbf{Semantic Pyramid Indexing (SPI)}, a novel multi-resolution vector indexing framework that introduces query-adaptive resolution control for RAG in VecDBs. Unlike existing hierarchical methods that require offline tuning or separate model training, SPI constructs a semantic pyramid over document embeddings and dynamically selects the optimal resolution level per query through a lightweight classifier. This adaptive approach enables progressive retrieval from coarse-to-fine representations, significantly accelerating search while maintaining semantic coverage.
  We implement SPI as a plugin for both FAISS and Qdrant backends and evaluate it across multiple RAG tasks including MS MARCO, Natural Questions, and multimodal retrieval benchmarks. SPI achieves up to \textbf{5.7$\times$} retrieval speedup and \textbf{1.8$\times$} memory efficiency gain while improving end-to-end QA F1 scores by up to \textbf{2.5 points} compared to strong baselines. Our theoretical analysis provides guarantees on retrieval quality and latency bounds, while extensive ablation studies validate the contribution of each component. The framework's compatibility with existing VecDB infrastructures makes it readily deployable in production RAG systems. Code is availabe at \href{https://github.com/FastLM/SPI_VecDB}{https://github.com/FastLM/SPI_VecDB}.</p>
<h3 id="48-ellipsoid-based-decision-boundaries-for-open-intent-classification">[48] <a href="https://arxiv.org/abs/2511.16685">Ellipsoid-Based Decision Boundaries for Open Intent Classification</a></h3>
<p><em>Yuetian Zou, Hanlei Zhang, Hua Xu, Songze Li, Long Xiao</em></p>
<h4 id="tldr_47">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºEliDecideæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„æ¤­åœ†å†³ç­–è¾¹ç•Œæ¥è§£å†³æ–‡æœ¬å¼€æ”¾æ„å›¾åˆ†ç±»é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ è·å¾—åˆ¤åˆ«æ€§ç‰¹å¾ç©ºé—´ï¼Œå¹¶é€šè¿‡åŒæŸå¤±å‡½æ•°ä¼˜åŒ–æ¤­åœ†è¾¹ç•Œï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_47">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è‡ªé€‚åº”å†³ç­–è¾¹ç•Œæ–¹æ³•å‡è®¾å·²çŸ¥ç±»æœä»å„å‘åŒæ€§åˆ†å¸ƒï¼Œå°†è¾¹ç•Œé™åˆ¶ä¸ºçƒå½¢ï¼Œå¿½ç•¥äº†ä¸åŒç‰¹å¾æ–¹å‘ä¸Šçš„åˆ†å¸ƒæ–¹å·®ã€‚è¿™ç§ç®€åŒ–é™åˆ¶äº†æ¨¡å‹å¯¹å¤æ‚çœŸå®ä¸–ç•Œåœºæ™¯ä¸­æœªçŸ¥æ„å›¾çš„æ£€æµ‹èƒ½åŠ›ï¼Œéœ€è¦æ›´çµæ´»çš„è¾¹ç•Œè¡¨ç¤ºæ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> é¦–å…ˆé‡‡ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ è·å¾—å·²çŸ¥æ ·æœ¬çš„åˆ¤åˆ«æ€§ç‰¹å¾ç©ºé—´ï¼›å…¶æ¬¡ä½¿ç”¨å¯å­¦ä¹ çŸ©é˜µå‚æ•°åŒ–æ¯ä¸ªå·²çŸ¥ç±»çš„æ¤­åœ†è¾¹ç•Œï¼Œæä¾›æ¯”ä»…ç”±ä¸­å¿ƒå’ŒåŠå¾„å®šä¹‰çš„çƒå½¢è¾¹ç•Œæ›´å¤§çš„çµæ´»æ€§ï¼›æœ€åé€šè¿‡æ–°é¢–è®¾è®¡çš„åŒæŸå¤±å‡½æ•°ä¼˜åŒ–è¾¹ç•Œï¼Œå¹³è¡¡ç»éªŒé£é™©å’Œå¼€æ”¾ç©ºé—´é£é™©ï¼Œåœ¨è¦†ç›–å·²çŸ¥æ ·æœ¬çš„åŒæ—¶æ”¶ç¼©è¾¹ç•Œä»¥å¯¹æŠ—åˆæˆçš„ä¼ªå¼€æ”¾æ ·æœ¬ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ–‡æœ¬æ„å›¾åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨é—®é¢˜åˆ†ç±»æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥éªŒè¯äº†æœ‰æ•ˆæ€§ã€‚æ¤­åœ†è¾¹ç•Œçš„çµæ´»æ€§å±•ç°å‡ºä¼˜è¶Šçš„å¼€æ”¾æ„å›¾æ£€æµ‹èƒ½åŠ›ï¼Œåœ¨å¤šæ ·å¤æ‚å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–æ½œåŠ›ã€‚</p>
<p><strong>Conclusion:</strong> æ¤­åœ†å†³ç­–è¾¹ç•Œç›¸æ¯”ä¼ ç»Ÿçƒå½¢è¾¹ç•Œèƒ½æ›´å‡†ç¡®åœ°å»ºæ¨¡å·²çŸ¥ç±»çš„çœŸå®åˆ†å¸ƒï¼Œæ˜¾è‘—æå‡å¼€æ”¾æ„å›¾æ£€æµ‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡åœ¨å¤æ‚å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå±•ç¤ºäº†åœ¨å¤šæ ·åŒ–çœŸå®ç¯å¢ƒä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</p>
<hr />
<h4 id="abstract_47">ğŸ“„ Abstract</h4>
<p>Textual open intent classification is crucial for real-world dialogue systems, enabling robust detection of unknown user intents without prior knowledge and contributing to the robustness of the system. While adaptive decision boundary methods have shown great potential by eliminating manual threshold tuning, existing approaches assume isotropic distributions of known classes, restricting boundaries to balls and overlooking distributional variance along different directions. To address this limitation, we propose EliDecide, a novel method that learns ellipsoid decision boundaries with varying scales along different feature directions. First, we employ supervised contrastive learning to obtain a discriminative feature space for known samples. Second, we apply learnable matrices to parameterize ellipsoids as the boundaries of each known class, offering greater flexibility than spherical boundaries defined solely by centers and radii. Third, we optimize the boundaries via a novelly designed dual loss function that balances empirical and open-space risks: expanding boundaries to cover known samples while contracting them against synthesized pseudo-open samples. Our method achieves state-of-the-art performance on multiple text intent benchmarks and further on a question classification dataset. The flexibility of the ellipsoids demonstrates superior open intent detection capability and strong potential for generalization to more text classification tasks in diverse complex open-world scenarios.</p>
<h3 id="49-pepper-perception-guided-perturbation-for-robust-backdoor-defense-in-text-to-image-diffusion-models">[49] <a href="https://arxiv.org/abs/2511.16830">PEPPER: Perception-Guided Perturbation for Robust Backdoor Defense in Text-to-Image Diffusion Models</a></h3>
<p><em>Oscar Chew, Po-Yi Lu, Jayden Lin, Kuan-Hao Huang, Hsuan-Tien Lin</em></p>
<h4 id="tldr_48">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†PEPPERé˜²å¾¡æ–¹æ³•ï¼Œé€šè¿‡è¯­ä¹‰é‡å†™ç­–ç•¥ç ´åæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„åé—¨è§¦å‘å™¨ï¼Œæ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡åŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•å¯ä¸ç°æœ‰é˜²å¾¡æŠ€æœ¯ç»“åˆï¼Œæä¾›æ¯”ä»»ä½•å•ä¸€æ–¹æ³•æ›´å¼ºçš„é€šç”¨é²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_48">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å®¹æ˜“å—åˆ°åé—¨æ”»å‡»ï¼Œè¾“å…¥æç¤ºä¸­çš„ç‰¹å®šè§¦å‘å™¨å¯èƒ½å¼•å¯¼æ¨¡å‹ç”Ÿæˆæœ‰å®³æˆ–éé¢„æœŸå†…å®¹ï¼Œç°æœ‰é˜²å¾¡æ–¹æ³•åœ¨ä¿æŠ¤ç”Ÿæˆè´¨é‡çš„åŒæ—¶éš¾ä»¥æœ‰æ•ˆåº”å¯¹æ­¤ç±»æ”»å‡»ã€‚</p>
<p><strong>Method:</strong> PEPPERé‡‡ç”¨æ„ŸçŸ¥å¼•å¯¼çš„æ‰°åŠ¨ç­–ç•¥ï¼Œå°†è¾“å…¥æ ‡é¢˜é‡å†™ä¸ºè¯­ä¹‰è·ç¦»è¾ƒè¿œä½†è§†è§‰ç›¸ä¼¼çš„æ ‡é¢˜ï¼ŒåŒæ—¶æ·»åŠ ä¸æ˜¾çœ¼çš„å…ƒç´ ï¼Œé€šè¿‡è¿™ç§é‡å†™ç­–ç•¥ç ´åè¾“å…¥æç¤ºä¸­åµŒå…¥çš„è§¦å‘å™¨å¹¶ç¨€é‡Šè§¦å‘ä»¤ç‰Œçš„å½±å“ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜PEPPERå¯¹åŸºäºæ–‡æœ¬ç¼–ç å™¨çš„æ”»å‡»ç‰¹åˆ«æœ‰æ•ˆï¼Œæ˜¾è‘—é™ä½äº†æ”»å‡»æˆåŠŸç‡åŒæ—¶ä¿æŒäº†ç”Ÿæˆè´¨é‡ï¼Œä¸ç°æœ‰é˜²å¾¡æ–¹æ³•ç»“åˆä½¿ç”¨æ—¶ï¼Œèƒ½å¤Ÿæä¾›æ¯”ä»»ä½•å•ä¸€æ–¹æ³•æ›´å¼ºä¸”æ›´é€šç”¨çš„é²æ£’æ€§ã€‚</p>
<p><strong>Conclusion:</strong> PEPPERè¯æ˜äº†é€šè¿‡è¯­ä¹‰é‡å†™ç­–ç•¥å¯ä»¥æœ‰æ•ˆé˜²å¾¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„åé—¨æ”»å‡»ï¼Œä¸ºå¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„å®‰å…¨é˜²æŠ¤æä¾›äº†æ–°æ€è·¯ï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶èƒ½å¤Ÿä¸ç°æœ‰é˜²å¾¡æŠ€æœ¯ååŒå·¥ä½œï¼Œå…·æœ‰å®é™…éƒ¨ç½²ä»·å€¼ã€‚</p>
<hr />
<h4 id="abstract_48">ğŸ“„ Abstract</h4>
<p>Recent studies show that text to image (T2I) diffusion models are vulnerable to backdoor attacks, where a trigger in the input prompt can steer generation toward harmful or unintended content. To address this, we introduce PEPPER (PErcePtion Guided PERturbation), a backdoor defense that rewrites the caption into a semantically distant yet visually similar caption while adding unobstructive elements. With this rewriting strategy, PEPPER disrupt the trigger embedded in the input prompt, dilute the influence of trigger tokens and thereby achieve enhanced robustness. Experiments show that PEPPER is particularly effective against text encoder based attacks, substantially reducing attack success while preserving generation quality. Beyond this, PEPPER can be paired with any existing defenses yielding consistently stronger and generalizable robustness than any standalone method. Our code will be released on Github.</p>
<h3 id="50-do-vision-language-models-understand-visual-persuasiveness">[50] <a href="https://arxiv.org/abs/2511.17036">Do Vision-Language Models Understand Visual Persuasiveness?</a></h3>
<p><em>Gyuwon Park</em></p>
<h4 id="tldr_49">ğŸ§© TL;DR</h4>
<p>è¯¥ç ”ç©¶æ„å»ºäº†è§†è§‰è¯´æœåŠ›åˆ¤æ–­æ•°æ®é›†å¹¶å¼•å…¥è§†è§‰è¯´æœå› ç´ åˆ†ç±»æ³•ï¼Œå‘ç°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯´æœç†è§£ä¸Šå­˜åœ¨å¬å›å¯¼å‘åè§ï¼Œä¸”ä¸»è¦é€šè¿‡é«˜å±‚è¯­ä¹‰å¯¹é½è€Œéä½ä¸­å±‚ç‰¹å¾è¿›è¡Œåˆ¤æ–­ï¼Œå¯¹è±¡å¯¼å‘çš„ç†æ€§æ¨ç†èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_49">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹æ˜¯å¦çœŸæ­£ç†è§£è§†è§‰è¯´æœâ€”â€”å³è§†è§‰çº¿ç´¢å¦‚ä½•å½±å“äººç±»æ€åº¦å’Œå†³ç­–â€”â€”ä»ä¸æ¸…æ¥šï¼Œç ”ç©¶æ—¨åœ¨æ¢ç©¶VLMså¯¹è§†è§‰è¯´æœæœºåˆ¶çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶æ„å»ºäº†äºŒå…ƒè¯´æœåŠ›åˆ¤æ–­çš„é«˜å…±è¯†æ•°æ®é›†ï¼Œæå‡ºäº†åŒ…å«ä½å±‚æ„ŸçŸ¥ã€ä¸­å±‚æ„å›¾å’Œé«˜å±‚è¯­ä¹‰çº¿ç´¢çš„è§†è§‰è¯´æœå› ç´ åˆ†ç±»æ³•ï¼Œå¹¶æ¢ç´¢äº†è®¤çŸ¥å¼•å¯¼å’ŒçŸ¥è¯†æ³¨å…¥ç­‰è¯´æœç›¸å…³æ¨ç†ç­–ç•¥ã€‚</p>
<p><strong>Result:</strong> å®è¯åˆ†ææ˜¾ç¤ºVLMså­˜åœ¨å¬å›å¯¼å‘åè§â€”â€”æ¨¡å‹è¿‡åº¦é¢„æµ‹é«˜è¯´æœåŠ›ï¼Œå¯¹ä½ä¸­å±‚ç‰¹å¾çš„åˆ¤åˆ«èƒ½åŠ›è¾ƒå¼±ï¼Œè€Œä¿¡æ¯ä¸å¯¹è±¡å­˜åœ¨çš„é«˜å±‚è¯­ä¹‰å¯¹é½æ˜¯é¢„æµ‹äººç±»åˆ¤æ–­çš„æœ€å¼ºæŒ‡æ ‡ï¼Œå¯¹è±¡å¯¼å‘çš„ç®€æ´ç†æ€§æ¨ç†èƒ½æ˜¾è‘—æé«˜ç²¾ç¡®ç‡å’ŒF1åˆ†æ•°ã€‚</p>
<p><strong>Conclusion:</strong> VLMsçš„æ ¸å¿ƒå±€é™ä¸åœ¨äºè¯†åˆ«æœ‰è¯´æœåŠ›çš„å¯¹è±¡ï¼Œè€Œåœ¨äºå°†è¿™äº›å¯¹è±¡ä¸æ²Ÿé€šæ„å›¾è”ç³»èµ·æ¥ï¼Œé«˜å±‚è¯­ä¹‰ç†è§£åœ¨è§†è§‰è¯´æœåˆ¤æ–­ä¸­èµ·ä¸»å¯¼ä½œç”¨ï¼Œé’ˆå¯¹æ€§æ¨ç†ç­–ç•¥èƒ½æœ‰æ•ˆå¼¥è¡¥æ¨¡å‹ç¼ºé™·ã€‚</p>
<hr />
<h4 id="abstract_49">ğŸ“„ Abstract</h4>
<p>Recent advances in vision-language models (VLMs) have enabled impressive multi-modal reasoning and understanding. Yet, whether these models truly grasp visual persuasion-how visual cues shape human attitudes and decisions-remains unclear. To probe this question, we construct a high-consensus dataset for binary persuasiveness judgment and introduce the taxonomy of Visual Persuasive Factors (VPFs), encompassing low-level perceptual, mid-level compositional, and high-level semantic cues. We also explore cognitive steering and knowledge injection strategies for persuasion-relevant reasoning. Empirical analysis across VLMs reveals a recall-oriented bias-models over-predict high persuasiveness-and weak discriminative power for low/mid-level features. In contrast, high-level semantic alignment between message and object presence emerges as the strongest predictor of human judgment. Among intervention strategies, simple instruction or unguided reasoning scaffolds yield marginal or negative effects, whereas concise, object-grounded rationales significantly improve precision and F1 scores. These results indicate that VLMs core limitation lies not in recognizing persuasive objects but in linking them to communicative intent.</p>
<h3 id="51-learning-to-compress-unlocking-the-potential-of-large-language-models-for-text-representation">[51] <a href="https://arxiv.org/abs/2511.17129">Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation</a></h3>
<p><em>Yeqin Zhang, Yizheng Zhao, Chen Hu, Binxing Jiao, Daxin Jiang, Ruihang Miao, Cam-Tu Nguyen</em></p>
<h4 id="tldr_50">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºLLM2Compï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å‹ç¼©ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡æ¥æ”¹è¿›LLMçš„æ–‡æœ¬è¡¨ç¤ºèƒ½åŠ›ï¼Œç›¸æ¯”åŸºäºtokençº§é¢„æµ‹çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ä¸”æ•°æ®æ•ˆç‡æ›´é«˜ã€‚</p>
<hr />
<h4 id="detailed-summary_50">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦é’ˆå¯¹å› æœå»ºæ¨¡å’Œä¸‹ä¸€è¯é¢„æµ‹è¿›è¡Œä¼˜åŒ–ï¼Œéš¾ä»¥äº§ç”Ÿé«˜è´¨é‡çš„å…¨å±€æ–‡æœ¬è¡¨ç¤ºã€‚è™½ç„¶å·²æœ‰ç ”ç©¶å¼•å…¥é¢„è®­ç»ƒä»»åŠ¡æ¥é€‚åº”LLMç”¨äºæ–‡æœ¬è¡¨ç¤ºï¼Œä½†å¤§å¤šä¾èµ–tokençº§é¢„æµ‹ç›®æ ‡ï¼Œå¦‚æ©ç ä¸‹ä¸€è¯é¢„æµ‹ï¼Œè¿™äº›æ–¹æ³•åœ¨æ•æ‰æ•´ä½“è¯­ä¹‰è¡¨ç¤ºæ–¹é¢å­˜åœ¨å±€é™ã€‚</p>
<p><strong>Method:</strong> æœ¬ç ”ç©¶æ¢ç´¢ä¸Šä¸‹æ–‡å‹ç¼©ä½œä¸ºæ— ç›‘ç£é€‚åº”LLMçš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œåœ¨å‹ç¼©é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å­¦ä¹ ç”Ÿæˆç´§å‡‘çš„è®°å¿†tokenæ¥æ›¿ä»£å®Œæ•´ä¸Šä¸‹æ–‡è¿›è¡Œä¸‹æ¸¸åºåˆ—é¢„æµ‹ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å‹ç¼©ç›®æ ‡ï¼Œç»“åˆå¯¹æ¯”å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œæ„å»ºäº†å¼ºå¤§çš„æ–‡æœ¬è¡¨ç¤ºæ¨¡å‹LLM2Compã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ï¼Œç²¾å¿ƒè®¾è®¡çš„å‹ç¼©ç›®æ ‡èƒ½æ˜¾è‘—æå‡åŸºäºLLMçš„æ–‡æœ¬è¡¨ç¤ºè´¨é‡ï¼Œåœ¨å¤šç§ä»»åŠ¡ä¸Šä¼˜äºåŸºäºtokençº§é¢„è®­ç»ƒä»»åŠ¡çš„æ¨¡å‹ã€‚LLM2Compåœ¨å¹¿æ³›çš„æ–‡æœ¬ç¼–ç ä»»åŠ¡ä¸­è¶…è¶Šäº†å½“ä»£åŸºäºLLMçš„æ–‡æœ¬ç¼–ç å™¨ï¼ŒåŒæ—¶å…·æœ‰æ›´é«˜çš„æ ·æœ¬æ•ˆç‡ï¼Œæ‰€éœ€è®­ç»ƒæ•°æ®æ˜¾è‘—å‡å°‘ã€‚</p>
<p><strong>Conclusion:</strong> ä¸Šä¸‹æ–‡å‹ç¼©ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ä¸ºLLMæ–‡æœ¬è¡¨ç¤ºæä¾›äº†æ–°çš„æœ‰æ•ˆé€”å¾„ï¼Œç›¸æ¯”ä¼ ç»Ÿtokençº§æ–¹æ³•èƒ½æ›´å¥½åœ°æ•æ‰æ–‡æ¡£çº§è¯­ä¹‰ä¿¡æ¯ã€‚è¯¥æ–¹æ³•ä¸ä»…æ€§èƒ½ä¼˜è¶Šï¼Œè¿˜æé«˜äº†æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼Œä¸ºæ„å»ºé«˜æ•ˆæ–‡æœ¬è¡¨ç¤ºæ¨¡å‹å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_50">ğŸ“„ Abstract</h4>
<p>Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data.</p>
<h3 id="52-lost-in-translation-and-noise-a-deep-dive-into-the-failure-modes-of-vlms-on-real-world-tables">[52] <a href="https://arxiv.org/abs/2511.17238">Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables</a></h3>
<p><em>Anshul Singh, Rohan Chaudhary, Gagneet Singh, Abhay Kumary</em></p>
<h4 id="tldr_51">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†MirageTVQAåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€å’Œè§†è§‰å™ªå£°ç¯å¢ƒä¸‹çš„è¡¨æ ¼æ¨ç†èƒ½åŠ›ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„ä¸¥é‡æ€§èƒ½ä¸‹é™å’Œè¯­è¨€åè§é—®é¢˜ã€‚</p>
<hr />
<h4 id="detailed-summary_51">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰è¡¨æ ¼é—®ç­”æ•°æ®é›†å¦‚WikiTableQuestionså’ŒFinQAä¸»è¦é¢å‘è‹±è¯­ä¸”å‘ˆç°å®Œç¾æ•°å­—æ ¼å¼ï¼Œæ— æ³•åæ˜ ç°å®ä¸–ç•Œä¸­å¤šè¯­è¨€å’Œè§†è§‰å™ªå£°çš„å¤æ‚æ€§ï¼Œå¯¼è‡´ç ”ç©¶ä¸å®è·µä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«è¿‘60,000ä¸ªé—®ç­”å¯¹çš„MirageTVQAåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–24ç§è¯­è¨€ï¼Œå¹¶å¼•å…¥æ¨¡æ‹Ÿæ‰«ææ–‡æ¡£çš„è§†è§‰å™ªå£°ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€å’Œè§†è§‰ä¸å®Œç¾æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚</p>
<p><strong>Result:</strong> å¯¹é¢†å…ˆè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œé¢å¯¹è§†è§‰å™ªå£°æ—¶æœ€ä½³æ¨¡å‹æ€§èƒ½ä¸‹é™è¶…è¿‡35%ï¼ŒåŒæ—¶å­˜åœ¨ä¸€è‡´çš„è‹±è¯­ä¼˜å…ˆåè§ï¼Œæ¨¡å‹åœ¨å…¶ä»–è¯­è¨€ä¸Šçš„æ¨ç†èƒ½åŠ›æ— æ³•æœ‰æ•ˆè¿ç§»ã€‚</p>
<p><strong>Conclusion:</strong> MirageTVQAä¸ºè¡¡é‡å’Œæ¨åŠ¨æ›´é²æ£’çš„è¡¨æ ¼æ¨ç†è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†åŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ç°å®åº”ç”¨ä¸­çš„å…³é”®å±€é™æ€§ï¼Œå¼ºè°ƒäº†å¤„ç†å¤šè¯­è¨€å’Œè§†è§‰å™ªå£°èƒ½åŠ›çš„é‡è¦æ€§ã€‚</p>
<hr />
<h4 id="abstract_51">ğŸ“„ Abstract</h4>
<p>The impressive performance of VLMs is largely measured on benchmarks that fail to capture the complexities of real-world scenarios. Existing datasets for tabular QA, such as WikiTableQuestions and FinQA, are overwhelmingly monolingual (English) and present tables in a digitally perfect, clean format. This creates a significant gap between research and practice. To address this, we present \textbf{MirageTVQA}, a new benchmark designed to evaluate VLMs on these exact dimensions. Featuring nearly 60,000 QA pairs across 24 languages, MirageTVQA challenges models with tables that are not only multilingual but also visually imperfect, incorporating realistic noise to mimic scanned documents. Our evaluation of the leading VLMs reveals two primary failure points: a severe degradation in performance (over 35\% drop for the best models) when faced with visual noise and a consistent English-first bias where reasoning abilities fail to transfer to other languages. MirageTVQA provides a benchmark for measuring and driving progress towards more robust VLM models for table reasoning. The dataset and the code are available at: https://github.com/anshulsc/MirageTVQA.</p>
<h3 id="53-large-language-models-for-sentiment-analysis-to-detect-social-challenges-a-use-case-with-south-african-languages">[53] <a href="https://arxiv.org/abs/2511.17301">Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages</a></h3>
<p><em>Koena Ronny Mabokela, Tim Schlippe, Matthias WÃ¶lfel</em></p>
<h4 id="tldr_52">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶åˆ†æäº†GPT-3.5ã€GPT-4ã€LlaMa 2ã€PaLM 2å’ŒDolly 2ç­‰å…ˆè¿›å¤§è¯­è¨€æ¨¡å‹åœ¨å—éè‹±è¯­ã€Sepediå’ŒSetswanaç¤¾äº¤åª’ä½“å¸–å­ä¸­çš„é›¶æ ·æœ¬æƒ…æ„Ÿåˆ†ææ€§èƒ½ï¼Œå¹¶å‘ç°é€šè¿‡å¤šæ¨¡å‹èåˆå¯å°†æƒ…æ„Ÿåˆ†ç±»é”™è¯¯ç‡é™è‡³1%ä»¥ä¸‹ï¼Œä¸ºæ£€æµ‹ç¤¾ä¼šæŒ‘æˆ˜æä¾›äº†å¯é è§£å†³æ–¹æ¡ˆã€‚</p>
<hr />
<h4 id="detailed-summary_52">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰ç¼ºä¹é’ˆå¯¹å—éå¤šè¯­è¨€ç¤¾äº¤åª’ä½“å¸–å­çš„å¤§è¯­è¨€æ¨¡å‹æƒ…æ„Ÿåˆ†æç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹ç¤¾ä¼šæŒ‘æˆ˜æ–¹é¢å­˜åœ¨ç©ºç™½ï¼Œè€Œå¤šè¯­è¨€ç¤¾åŒºçš„æƒ…æ„Ÿåˆ†æç³»ç»Ÿèƒ½å¤Ÿå¸®åŠ©æ”¿åºœéƒ¨é—¨æ›´ç²¾ç¡®åœ°è¯†åˆ«å’Œè§£å†³ç¤¾ä¼šé—®é¢˜ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶è¯„ä¼°äº†GPT-3.5ã€GPT-4ã€LlaMa 2ã€PaLM 2å’ŒDolly 2ç­‰å…ˆè¿›å¤§è¯­è¨€æ¨¡å‹åœ¨å—é10ä¸ªæ”¿åºœéƒ¨é—¨ç®¡è¾–èŒƒå›´å†…çš„10ä¸ªæ–°å…´è¯é¢˜ä¸Šçš„é›¶æ ·æœ¬æƒ…æ„Ÿåˆ†æèƒ½åŠ›ï¼Œæ¶µç›–è‹±è¯­ã€Sepediå’ŒSetswanaä¸‰ç§è¯­è¨€ï¼Œå¹¶é‡‡ç”¨äº†å¤šæ¨¡å‹ç»“æœèåˆç­–ç•¥ã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœæ˜¾ç¤ºä¸åŒå¤§è¯­è¨€æ¨¡å‹ã€è¯é¢˜å’Œè¯­è¨€ä¹‹é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®å¼‚ï¼Œé€šè¿‡å¤šæ¨¡å‹èåˆç­–ç•¥èƒ½å¤Ÿå¤§å¹…æå‡æƒ…æ„Ÿåˆ†ç±»æ€§èƒ½ï¼Œå°†åˆ†ç±»é”™è¯¯ç‡é™è‡³1%ä»¥ä¸‹ï¼Œå®ç°äº†é«˜åº¦å¯é çš„æƒ…æ„Ÿåˆ†ææ•ˆæœã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜å¤§è¯­è¨€æ¨¡å‹èåˆæ–¹æ³•èƒ½å¤Ÿä¸ºå¤šè¯­è¨€ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†ææä¾›å¯é è§£å†³æ–¹æ¡ˆï¼Œä½¿æ”¿åºœéƒ¨é—¨èƒ½å¤ŸåŸºäºç‰¹å®šè¯é¢˜å’Œè¯­è¨€ç¾¤ä½“çš„æƒ…æ„Ÿåˆ†æç»“æœæ¥æ£€æµ‹ç¤¾ä¼šæŒ‘æˆ˜å¹¶åˆ¶å®šç›¸åº”è¡ŒåŠ¨ç­–ç•¥ã€‚</p>
<hr />
<h4 id="abstract_52">ğŸ“„ Abstract</h4>
<p>Sentiment analysis can aid in understanding people's opinions and emotions on social issues. In multilingual communities sentiment analysis systems can be used to quickly identify social challenges in social media posts, enabling government departments to detect and address these issues more precisely and effectively. Recently, large-language models (LLMs) have become available to the wide public and initial analyses have shown that they exhibit magnificent zero-shot sentiment analysis abilities in English. However, there is no work that has investigated to leverage LLMs for sentiment analysis on social media posts in South African languages and detect social challenges. Consequently, in this work, we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 to investigate the sentiment polarities of the 10 most emerging topics in English, Sepedi and Setswana social media posts that fall within the jurisdictional areas of 10 South African government departments. Our results demonstrate that there are big differences between the various LLMs, topics, and languages. In addition, we show that a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%. Consequently, it is now feasible to provide systems that generate reliable information about sentiment analysis to detect social challenges and draw conclusions about possible needs for actions on specific topics and within different language groups.</p>
<h3 id="54-dont-learn-ground-a-case-for-natural-language-inference-with-visual-grounding">[54] <a href="https://arxiv.org/abs/2511.17358">Don't Learn, Ground: A Case for Natural Language Inference with Visual Grounding</a></h3>
<p><em>Daniil Ignatev, Ayman Santeer, Albert Gatt, Denis Paperno</em></p>
<h4 id="tldr_53">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬è‡ªç„¶è¯­è¨€æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆå‰æçš„è§†è§‰è¡¨ç¤ºï¼Œå¹¶ä¸æ–‡æœ¬å‡è®¾è¿›è¡Œæ¯”è¾ƒæ¥å®Œæˆæ¨ç†ã€‚è¯¥æ–¹æ³•æ— éœ€ä»»åŠ¡ç‰¹å®šå¾®è°ƒå³å¯å®ç°é«˜ç²¾åº¦ï¼Œå¹¶å±•ç°å‡ºå¯¹æ–‡æœ¬åè§å’Œè¡¨é¢å¯å‘å¼çš„é²æ£’æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_53">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è‡ªç„¶è¯­è¨€æ¨ç†æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬æ¨¡æ€ï¼Œå®¹æ˜“å—åˆ°æ–‡æœ¬åè§å’Œè¡¨é¢å¯å‘å¼çš„å½±å“ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢é€šè¿‡å¤šæ¨¡æ€è¡¨ç¤ºå°†è¯­è¨€åœ¨è§†è§‰ä¸Šä¸‹æ–‡ä¸­è¿›è¡ŒåŸºç¡€åŒ–ï¼Œä»¥æ„å»ºæ›´é²æ£’çš„è‡ªç„¶è¯­è¨€ç†è§£ç³»ç»Ÿã€‚</p>
<p><strong>Method:</strong> è¯¥æ–¹æ³•åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆå‰æçš„è§†è§‰è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨ä¸¤ç§æ¨ç†æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒï¼šä½™å¼¦ç›¸ä¼¼åº¦å’Œè§†è§‰é—®ç­”ã€‚åŒæ—¶è®¾è®¡äº†ä¸€ä¸ªå—æ§çš„å¯¹æŠ—æ€§æ•°æ®é›†æ¥éªŒè¯æ–¹æ³•çš„é²æ£’æ€§ã€‚</p>
<p><strong>Result:</strong> è¯¥æ–¹æ³•åœ¨æ— éœ€ä»»åŠ¡ç‰¹å®šå¾®è°ƒçš„æƒ…å†µä¸‹å®ç°äº†é«˜å‡†ç¡®ç‡ï¼Œæœ‰æ•ˆæŠµæŠ—äº†æ–‡æœ¬åè§å’Œè¡¨é¢å¯å‘å¼çš„å½±å“ã€‚é€šè¿‡å¯¹æŠ—æ€§æ•°æ®é›†çš„éªŒè¯è¿›ä¸€æ­¥è¯å®äº†è¯¥æ–¹æ³•çš„é²æ£’æ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¡¨æ˜åˆ©ç”¨è§†è§‰æ¨¡æ€ä½œä¸ºæ„ä¹‰è¡¨ç¤ºä¸ºé²æ£’çš„è‡ªç„¶è¯­è¨€ç†è§£æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå¤šæ¨¡æ€åŸºç¡€åŒ–èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£çº¯æ–‡æœ¬æ–¹æ³•çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿè®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_53">ğŸ“„ Abstract</h4>
<p>We propose a zero-shot method for Natural Language Inference (NLI) that leverages multimodal representations by grounding language in visual contexts. Our approach generates visual representations of premises using text-to-image models and performs inference by comparing these representations with textual hypotheses. We evaluate two inference techniques: cosine similarity and visual question answering. Our method achieves high accuracy without task-specific fine-tuning, demonstrating robustness against textual biases and surface heuristics. Additionally, we design a controlled adversarial dataset to validate the robustness of our approach. Our findings suggest that leveraging visual modality as a meaning representation provides a promising direction for robust natural language understanding.</p>
<h3 id="55-beyond-multiple-choice-a-hybrid-framework-for-unifying-robust-evaluation-and-verifiable-reasoning-training">[55] <a href="https://arxiv.org/abs/2511.17405">Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training</a></h3>
<p><em>Yesheng Liu, Hao Li, Haiyu Xu, Baoqi Pei, Jiahao Wang, Mingxuan Zhao, Jingshu Zheng, Zheqi He, JG Yao, Bowen Qin, Xi Yang, Jiajun Zhang</em></p>
<h4 id="tldr_54">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†ReVeLæ¡†æ¶ï¼Œå°†å¤šé¡¹é€‰æ‹©é¢˜è½¬æ¢ä¸ºå¼€æ”¾å¼é—®é¢˜ä»¥è§£å†³é€‰é¡¹æ³„éœ²å¯¼è‡´çš„è¯„ä¼°åå·®é—®é¢˜ï¼Œé€šè¿‡LLMé‡å†™å’ŒéªŒè¯æœºåˆ¶æå‡æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°çš„å¯é æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_54">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQAï¼‰åœ¨è¯„ä¼°å’Œå¼ºåŒ–å¾®è°ƒå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ—¶å­˜åœ¨é€‰é¡¹æ³„éœ²é—®é¢˜ï¼Œå¯¼è‡´å‡†ç¡®ç‡æŒ‡æ ‡ä¸å¯é å¹¶é¼“åŠ±æ¨¡å‹çŒœæµ‹è¡Œä¸ºï¼Œæ— æ³•çœŸå®åæ˜ æ¨¡å‹èƒ½åŠ›ã€‚</p>
<p><strong>Method:</strong> æå‡ºReVeLæ¡†æ¶ï¼Œé€šè¿‡LLMå°†MCQAé‡å†™ä¸ºå¼€æ”¾å¼é—®é¢˜ï¼Œæ ¹æ®ç­”æ¡ˆç±»å‹åˆ†ç±»åº”ç”¨ä¸åŒçš„é‡å†™å’ŒéªŒè¯æ–¹æ¡ˆï¼Œä½¿ç”¨GRPOæ–¹æ³•å¯¹Qwen2.5-VLæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</p>
<p><strong>Result:</strong> åœ¨å¤šé¡¹é€‰æ‹©åŸºå‡†æµ‹è¯•ä¸­ï¼ŒReVeL-OpenQAè®­ç»ƒçš„æ¨¡å‹ä¿æŒäº†MCQAå‡†ç¡®ç‡ï¼ŒåŒæ—¶å°†OpenQAå‡†ç¡®ç‡æå‡äº†çº¦6ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶æ­ç¤ºäº†MCQAåŸºå‡†ä¸­é«˜è¾¾20ä¸ªç™¾åˆ†ç‚¹çš„åˆ†æ•°è†¨èƒ€ã€‚</p>
<p><strong>Conclusion:</strong> ReVeLæ¡†æ¶æä¾›äº†æ›´é«˜æ•ˆçš„æ•°æ®åˆ©ç”¨å’Œæ›´é²æ£’çš„å¥–åŠ±ä¿¡å·ï¼Œæ”¹å–„äº†è¯„ä¼°å‡†ç¡®æ€§å¹¶é™ä½äº†æˆæœ¬å’Œå»¶è¿Ÿï¼Œä¸ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¯é è®­ç»ƒå’Œè¯„ä¼°æä¾›äº†æ–°èŒƒå¼ã€‚</p>
<hr />
<h4 id="abstract_54">ğŸ“„ Abstract</h4>
<p>Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.</p>
<h3 id="56-smile-a-composite-lexical-semantic-metric-for-question-answering-evaluation">[56] <a href="https://arxiv.org/abs/2511.17432">SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation</a></h3>
<p><em>Shrikant Kendre, Austin Xu, Honglu Zhou, Michael Ryoo, Shafiq Joty, Juan Carlos Niebles</em></p>
<h4 id="tldr_55">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†SMILEè¯„ä¼°æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡é€šè¿‡æ•´åˆå¥å­çº§è¯­ä¹‰ç†è§£ã€å…³é”®è¯çº§è¯­ä¹‰ç†è§£å’Œç²¾ç¡®å…³é”®è¯åŒ¹é…ï¼Œåœ¨æ–‡æœ¬å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸­å®ç°äº†è¯æ±‡ç²¾ç¡®æ€§å’Œè¯­ä¹‰ç›¸å…³æ€§çš„å¹³è¡¡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿè¯„ä¼°æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_55">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ä¼ ç»Ÿæ–‡æœ¬å’Œè§†è§‰é—®ç­”è¯„ä¼°æŒ‡æ ‡å¦‚ROUGEã€METEORå’Œç²¾ç¡®åŒ¹é…ä¸»è¦å…³æ³¨åŸºäºn-gramçš„è¯æ±‡ç›¸ä¼¼æ€§ï¼Œå¾€å¾€å¿½ç•¥äº†æ·±å±‚è¯­ä¹‰ç†è§£éœ€æ±‚ã€‚è™½ç„¶BERTScoreå’ŒMoverScoreç­‰åº¦é‡æ–¹æ³•åˆ©ç”¨ä¸Šä¸‹æ–‡åµŒå…¥è§£å†³äº†è¿™ä¸€å±€é™ï¼Œä½†å®ƒä»¬ç¼ºä¹åœ¨å¥å­çº§å’Œå…³é”®è¯çº§è¯­ä¹‰ä¹‹é—´å¹³è¡¡çš„çµæ´»æ€§ï¼Œå¹¶ä¸”å¿½ç•¥äº†ä»ç„¶é‡è¦çš„è¯æ±‡ç›¸ä¼¼æ€§ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°å™¨è™½ç„¶å¼ºå¤§ï¼Œä½†å­˜åœ¨æˆæœ¬é«˜ã€åè§ã€ä¸ä¸€è‡´å’Œå¹»è§‰ç­‰é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†SMILEè¯„ä¼°æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¤åˆæ–¹æ³•ï¼Œç»“åˆäº†å¥å­çº§è¯­ä¹‰ç†è§£ã€å…³é”®è¯çº§è¯­ä¹‰ç†è§£å’Œç®€å•çš„å…³é”®è¯åŒ¹é…ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´åˆä¸åŒå±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹³è¡¡äº†è¯æ±‡ç²¾ç¡®æ€§å’Œè¯­ä¹‰ç›¸å…³æ€§ï¼Œæä¾›å…¨é¢çš„è¯„ä¼°èƒ½åŠ›ã€‚SMILEè®¾è®¡ä¸ºè®¡ç®—è½»é‡çº§ï¼Œé¿å…äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å™¨çš„é«˜æˆæœ¬å’Œå¯é æ€§é—®é¢˜ã€‚</p>
<p><strong>Result:</strong> åœ¨æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘é—®ç­”ä»»åŠ¡ä¸Šçš„å¹¿æ³›åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒSMILEä¸äººç±»åˆ¤æ–­å…·æœ‰é«˜åº¦ç›¸å…³æ€§ã€‚è¯¥æŒ‡æ ‡åœ¨å¤šä¸ªè¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶åœ¨å¹³è¡¡è¯æ±‡å’Œè¯­ä¹‰è¯„ä¼°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºSMILEåœ¨è®¡ç®—æ•ˆç‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæä¾›å¯é ä¸”ä¸€è‡´çš„è¯„ä¼°ç»“æœã€‚</p>
<p><strong>Conclusion:</strong> SMILEæˆåŠŸå¼¥åˆäº†è¯æ±‡è¯„ä¼°å’Œè¯­ä¹‰è¯„ä¼°ä¹‹é—´çš„å·®è·ï¼Œä¸ºæ–‡æœ¬å’Œè§†è§‰é—®ç­”ä»»åŠ¡æä¾›äº†æ›´åŠ å…¨é¢å’Œå‡†ç¡®çš„è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†ç»“åˆä¸åŒå±‚æ¬¡è¯­ä¹‰ä¿¡æ¯çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥è¯„ä¼°æŒ‡æ ‡çš„å‘å±•æä¾›äº†é‡è¦å¯ç¤ºã€‚SMILEçš„è½»é‡çº§ç‰¹æ€§ä½¿å…¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºè¯„ä¼°é¢†åŸŸæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<hr />
<h4 id="abstract_55">ğŸ“„ Abstract</h4>
<p>Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="57-patient-level-information-extraction-by-consistent-integration-of-textual-and-tabular-evidence-with-bayesian-networks">[57] <a href="https://arxiv.org/abs/2511.17056">Patient-level Information Extraction by Consistent Integration of Textual and Tabular Evidence with Bayesian Networks</a></h3>
<p><em>Paloma Rabaey, Adrick Tench, Stefan Heytens, Thomas Demeester</em></p>
<h4 id="tldr_56">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ‚£è€…çº§ä¿¡æ¯æå–æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆç»“æ„åŒ–ç”µå­å¥åº·è®°å½•å’Œä¸´åºŠæ–‡æœ¬æ•°æ®ï¼Œä½¿ç”¨è™šæ‹Ÿè¯æ®å¢å¼ºå’Œä¸€è‡´æ€§èŠ‚ç‚¹å®ç°å¯è§£é‡Šçš„æ¦‚ç‡èåˆï¼Œæœ‰æ•ˆå¤„ç†ç¼ºå¤±ä¿¡æ¯å¹¶è§£å†³æ•°æ®çŸ›ç›¾ã€‚</p>
<hr />
<h4 id="detailed-summary_56">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç”µå­å¥åº·è®°å½•ä¸­å¤§é‡å…³é”®ä¿¡æ¯å­˜åœ¨äºéç»“æ„åŒ–æ–‡æœ¬ä¸­ï¼Œè€Œç°æœ‰æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨ç»“æ„åŒ–è¡¨æ ¼ç‰¹å¾ä¸ä¸´åºŠç¬”è®°ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿé€æ˜èåˆå¤šæ¨¡æ€æ•°æ®å¹¶å¤„ç†ä¿¡æ¯ç¼ºå¤±å’ŒçŸ›ç›¾çš„æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> æå‡ºå¤šæ¨¡æ€æ‚£è€…çº§ä¿¡æ¯æå–æ¡†æ¶ï¼Œç»“åˆä¸“å®¶çŸ¥è¯†æ„å»ºçš„è´å¶æ–¯ç½‘ç»œå¤„ç†ç»“æ„åŒ–EHRç‰¹å¾ï¼Œä½¿ç”¨ç¥ç»æ–‡æœ¬åˆ†ç±»å™¨åˆ†æä¸´åºŠç¬”è®°ï¼Œå¹¶é€šè¿‡è™šæ‹Ÿè¯æ®å¢å¼ºå’Œä¸€è‡´æ€§èŠ‚ç‚¹å®ç°æ¦‚ç‡èåˆï¼Œæé«˜é¢„æµ‹æ ¡å‡†æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨SimSUMæ¨¡æ‹ŸåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”å•ç‹¬ä½¿ç”¨è™šæ‹Ÿè¯æ®èƒ½å¤Ÿæ˜¾è‘—æ”¹å–„é¢„æµ‹æ ¡å‡†ï¼Œä½¿è´å¶æ–¯ç½‘ç»œæ›´å¥½åœ°è°ƒæ•´ç¥ç»åˆ†ç±»å™¨è¾“å‡ºï¼Œæœ‰æ•ˆå¤„ç†ä¿¡æ¯ç¼ºå¤±å’Œè§£å†³è¡¨æ ¼ä¸æ–‡æœ¬æ•°æ®é—´çš„çŸ›ç›¾ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥æ–¹æ³•ä¸ºä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿæä¾›äº†å¯è§£é‡Šçš„å¤šæ¨¡æ€æ•°æ®èåˆæ–¹æ¡ˆï¼Œé€šè¿‡ä¸€è‡´æ€§èŠ‚ç‚¹æœºåˆ¶å¢å¼ºäº†æ¨¡å‹åœ¨å¤„ç†å¤æ‚åŒ»ç–—æ•°æ®æ—¶çš„é²æ£’æ€§å’Œå¯é æ€§ï¼Œä¸ºé«˜é£é™©åŒ»ç–—åº”ç”¨ä¸­çš„é€æ˜ç‰¹å¾å»ºæ¨¡å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<hr />
<h4 id="abstract_56">ğŸ“„ Abstract</h4>
<p>Electronic health records (EHRs) form an invaluable resource for training clinical decision support systems. To leverage the potential of such systems in high-risk applications, we need large, structured tabular datasets on which we can build transparent feature-based models. While part of the EHR already contains structured information (e.g. diagnosis codes, medications, and lab results), much of the information is contained within unstructured text (e.g. discharge summaries and nursing notes). In this work, we propose a method for multi-modal patient-level information extraction that leverages both the tabular features available in the patient's EHR (using an expert-informed Bayesian network) as well as clinical notes describing the patient's symptoms (using neural text classifiers). We propose the use of virtual evidence augmented with a consistency node to provide an interpretable, probabilistic fusion of the models' predictions. The consistency node improves the calibration of the final predictions compared to virtual evidence alone, allowing the Bayesian network to better adjust the neural classifier's output to handle missing information and resolve contradictions between the tabular and text data. We show the potential of our method on the SimSUM dataset, a simulated benchmark linking tabular EHRs with clinical notes through expert knowledge.</p>
  </article>
</body>
</html>
