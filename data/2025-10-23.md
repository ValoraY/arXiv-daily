<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 32]
- [cs.CL](#cs.CL) [Total: 18]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts](https://arxiv.org/abs/2510.19001)
*Seungjun Yu, Junsung Park, Youngsun Lim, Hyunjung Shim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè‡ªåŠ¨é©¾é©¶çš„ä¸¤é˜¶æ®µè§†è§‰è¯­è¨€é—®ç­”ç³»ç»Ÿï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºç­–ç•¥å’Œä¸Šä¸‹æ–‡å¢å¼ºæ˜¾è‘—æå‡äº†é«˜çº§æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’é—®é¢˜çš„å›ç­”å‡†ç¡®æ€§ã€‚è¯¥ç³»ç»Ÿåœ¨nuScenesåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†67.37%çš„æ•´ä½“å‡†ç¡®ç‡ï¼Œå¹¶åœ¨ä¸¥é‡è§†è§‰å¹²æ‰°ä¸‹ä¿æŒ96%çš„é²æ£’æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­é«˜çº§è§†è§‰è¯­è¨€é—®ç­”çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•è®©é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ›´å¥½åœ°ç†è§£é©¾é©¶ç¯å¢ƒä¸­çš„æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤æ‚é©¾é©¶åœºæ™¯çš„æ¨ç†èƒ½åŠ›å’Œä¸Šä¸‹æ–‡ç†è§£æ–¹é¢å­˜åœ¨å±€é™ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„æç¤ºå·¥ç¨‹å’Œä¸Šä¸‹æ–‡å¢å¼ºç­–ç•¥ã€‚

**Method:** ç³»ç»Ÿé‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨Qwen2.5-VL-32Bå¤§æ¨¡å‹ï¼Œè¾“å…¥å…­æ‘„åƒå¤´æ•°æ®ã€å†å²æ—¶åºçª—å£å’Œæ€ç»´é“¾æç¤ºï¼›ç¬¬äºŒé˜¶æ®µå¢å¼ºåœºæ™¯å…ƒæ•°æ®ï¼ˆç‰©ä½“æ ‡æ³¨ã€è‡ªè½¦çŠ¶æ€ç­‰ï¼‰å’Œä»»åŠ¡ç‰¹å®šæŒ‡ä»¤ã€‚å…³é”®æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬è‡ªä¸€è‡´æ€§é›†æˆï¼ˆå¤šé‡‡æ ·æ¨ç†é“¾ï¼‰å’Œåˆ†ç±»åˆ«é—®é¢˜æŒ‡ä»¤è®¾è®¡ï¼Œåˆ†åˆ«é’ˆå¯¹æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’ä»»åŠ¡ä¼˜åŒ–æç¤ºç­–ç•¥ã€‚

**Result:** åœ¨é©¾é©¶é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œç³»ç»Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼šç¬¬ä¸€é˜¶æ®µä½¿ç”¨5å¸§å†å²å’Œ10æ ·æœ¬æç¤ºè¾¾åˆ°65.1%å‡†ç¡®ç‡ï¼ˆé›¶æ ·æœ¬ä¸º62.61%ï¼‰ï¼Œè‡ªä¸€è‡´æ€§é›†æˆæå‡è‡³66.85%ï¼›ç¬¬äºŒé˜¶æ®µè¾¾åˆ°67.37%æ•´ä½“å‡†ç¡®ç‡ã€‚åœ¨ä¸¥é‡è§†è§‰å¹²æ‰°ä¸‹ç³»ç»Ÿä¿æŒ96%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†æ–¹æ³•çš„é²æ£’æ€§ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ç²¾å¿ƒè®¾è®¡çš„æç¤ºå·¥ç¨‹å’Œä¸Šä¸‹æ–‡å¢å¼ºèƒ½å¤Ÿæ˜¾è‘—æå‡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨é«˜çº§é©¾é©¶é—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æƒ…å¢ƒç†è§£å’Œå†³ç­–æ¨ç†æä¾›äº†æœ‰æ•ˆæ¡†æ¶ï¼Œè¯æ˜äº†å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„å®ç”¨ä»·å€¼ï¼Œä¸ºæœªæ¥æ™ºèƒ½é©¾é©¶ç³»ç»Ÿçš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
We present a two-phase vision-language QA system for autonomous driving that
answers high-level perception, prediction, and planning questions. In Phase-1,
a large multimodal LLM (Qwen2.5-VL-32B) is conditioned on six-camera inputs, a
short temporal window of history, and a chain-of-thought prompt with few-shot
exemplars. A self-consistency ensemble (multiple sampled reasoning chains)
further improves answer reliability. In Phase-2, we augment the prompt with
nuScenes scene metadata (object annotations, ego-vehicle state, etc.) and
category-specific question instructions (separate prompts for perception,
prediction, planning tasks). In experiments on a driving QA benchmark, our
approach significantly outperforms the baseline Qwen2.5 models. For example,
using 5 history frames and 10-shot prompting in Phase-1 yields 65.1% overall
accuracy (vs.62.61% with zero-shot); applying self-consistency raises this to
66.85%. Phase-2 achieves 67.37% overall. Notably, the system maintains 96%
accuracy under severe visual corruption. These results demonstrate that
carefully engineered prompts and contextual grounding can greatly enhance
high-level driving QA with pretrained vision-language models.


### [2] [PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions](https://arxiv.org/abs/2510.19060)
*Amith Ananthram, Elias Stengel-Eskin, Lorena A. Bradford, Julia Demarest, Adam Purvis, Keith Krut, Robert Stein, Rina Elster Pantalony, Mohit Bansal, Kathleen McKeown*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PoShï¼Œä¸€ç§åŸºäºåœºæ™¯å›¾ç»“æ„åŒ–è¯„åˆ†æ ‡å‡†å’ŒLLMä½œä¸ºè¯„åˆ¤è€…çš„è¯¦ç»†å›¾åƒæè¿°è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶å¼•å…¥äº†DOCENTåŸºå‡†æ•°æ®é›†ï¼Œåœ¨è‰ºæœ¯å›¾åƒæè¿°ä»»åŠ¡ä¸­å®ç°äº†æ¯”ç°æœ‰æ–¹æ³•æ›´å¥½çš„äººç±»è¯„åˆ†ç›¸å…³æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¯¦ç»†å›¾åƒæè¿°æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†è¯„ä¼°ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œæ ‡å‡†æŒ‡æ ‡å¦‚CIDErå’ŒSPICEæ˜¯ä¸ºçŸ­æ–‡æœ¬è®¾è®¡çš„ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°é•¿æ–‡æœ¬ä¸­çš„å±æ€§å’Œå…³ç³»é”™è¯¯ï¼Œä¸”ç¼ºä¹å¯¹ç‰¹å®šæ–‡æœ¬è·¨åº¦é”™è¯¯çš„å®šä½èƒ½åŠ›ã€‚

**Method:** PoShä½¿ç”¨åœºæ™¯å›¾ä½œä¸ºç»“æ„åŒ–è¯„åˆ†æ ‡å‡†æ¥æŒ‡å¯¼LLMä½œä¸ºè¯„åˆ¤è€…ï¼Œé€šè¿‡ç»†ç²’åº¦é”™è¯¯åˆ†æç”Ÿæˆèšåˆåˆ†æ•°ï¼ŒåŒæ—¶å¼•å…¥äº†DOCENTæ•°æ®é›†ï¼ŒåŒ…å«è‰ºæœ¯ä½œå“ã€ä¸“å®¶æ’°å†™çš„å‚è€ƒæè¿°ä»¥åŠè‰ºæœ¯å²å­¦ç”Ÿæä¾›çš„ç»†ç²’åº¦å’Œç²—ç²’åº¦è´¨é‡è¯„ä¼°ã€‚

**Result:** PoShåœ¨DOCENTæ•°æ®é›†ä¸Šæ¯”æœ€ä½³å¼€æ”¾æƒé‡æ›¿ä»£æ–¹æ³•å®ç°äº†æ›´å¼ºçš„äººç±»è¯„åˆ†ç›¸å…³æ€§ï¼ŒSpearman Ïæé«˜äº†0.05ï¼Œå¯¹å›¾åƒç±»å‹å…·æœ‰é²æ£’æ€§ï¼Œå¹¶ä¸”ä½œä¸ºå¥–åŠ±å‡½æ•°ä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨æè¿°å…·æœ‰ä¸°å¯Œåœºæ™¯åŠ¨æ€çš„å›¾åƒæ—¶çš„æ€§èƒ½å±€é™ã€‚

**Conclusion:** è¯¥ç ”ç©¶é€šè¿‡PoShæŒ‡æ ‡å’ŒDOCENTåŸºå‡†ä¸ºè¯¦ç»†å›¾åƒæè¿°è¯„ä¼°æä¾›äº†æ–°çš„å·¥å…·ï¼Œæ­ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨å¤„ç†å¤æ‚åœºæ™¯åŠ¨æ€æ—¶çš„å±€é™æ€§ï¼Œä¸ºè¾…åŠ©æ–‡æœ¬ç”Ÿæˆç­‰é‡è¦é¢†åŸŸçš„å‘å±•å¥ å®šäº†åŸºç¡€ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªè¡¡é‡VLMè¿›å±•çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–°ä»»åŠ¡ã€‚

---

#### ğŸ“„ Abstract
While vision-language models (VLMs) have advanced into detailed image
description, evaluation remains a challenge. Standard metrics (e.g. CIDEr,
SPICE) were designed for short texts and tuned to recognize errors that are now
uncommon, such as object misidentification. In contrast, long texts require
sensitivity to attribute and relation attachments and scores that localize
errors to particular text spans. In this work, we introduce PoSh, a metric for
detailed image description that uses scene graphs as structured rubrics to
guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained
errors (e.g. mistakes in compositional understanding). PoSh is replicable,
interpretable and a better proxy for human raters than existing metrics
(including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new
dataset, DOCENT. This novel benchmark contains artwork, paired with
expert-written references, and model-generated descriptions, augmented with
granular and coarse judgments of their quality from art history students. Thus,
DOCENT enables evaluating both detailed image description metrics and detailed
image description itself in a challenging new domain. We show that PoSh
achieves stronger correlations (+0.05 Spearman $\rho$) with the human judgments
in DOCENT than the best open-weight alternatives, is robust to image type
(using CapArena, an existing dataset of web imagery) and is a capable reward
function, outperforming standard supervised fine-tuning. Then, using PoSh, we
characterize the performance of open and closed models in describing the
paintings, sketches and statues in DOCENT and find that foundation models
struggle to achieve full, error-free coverage of images with rich scene
dynamics, establishing a demanding new task to gauge VLM progress. Through both
PoSh and DOCENT, we hope to enable advances in important areas such as
assistive text generation.


### [3] [UniHPR: Unified Human Pose Representation via Singular Value Contrastive Learning](https://arxiv.org/abs/2510.19078)
*Zhongyu Jiang, Wenhao Chai, Lei Li, Zhuoran Zhou, Cheng-Yen Yang, Jenq-Neng Hwang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºUniHPRï¼Œä¸€ç§ç»Ÿä¸€çš„äººä½“å§¿æ€è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ–°é¢–çš„åŸºäºå¥‡å¼‚å€¼çš„å¯¹æ¯”å­¦ä¹ æŸå¤±å¯¹é½å›¾åƒã€2Då’Œ3Däººä½“å§¿æ€åµŒå…¥ï¼Œåœ¨äººä½“å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åœ¨å¤šæ¨¡æ€èåˆä¸­ï¼Œäººä½“å§¿æ€è¡¨ç¤ºä½œä¸ºäººæœ¬åº”ç”¨çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œè™½ç„¶å¯ä»¥ä»å›¾åƒã€2Då…³é”®ç‚¹ã€3Déª¨æ¶ç­‰å¤šç§æ¨¡æ€ä¸­æå–ï¼Œä½†ç¼ºä¹å¯¹è¿™äº›è¡¨ç¤ºä¹‹é—´ç›¸å…³æ€§çš„ç³»ç»Ÿç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨å¯¹æ¯”å­¦ä¹ èŒƒå¼æ¥ç»Ÿä¸€å¯¹é½è¿™äº›ä¸åŒæ¨¡æ€çš„è¡¨ç¤ºã€‚

**Method:** æå‡ºäº†UniHPRç»Ÿä¸€äººä½“å§¿æ€è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨åŸºäºå¥‡å¼‚å€¼çš„å¯¹æ¯”å­¦ä¹ æŸå¤±æ¥åŒæ—¶å¯¹é½å›¾åƒã€2Då’Œ3Däººä½“å§¿æ€åµŒå…¥ï¼Œè¯¥æŸå¤±å‡½æ•°èƒ½æ›´å¥½åœ°å¯¹é½ä¸åŒæ¨¡æ€å¹¶æå‡æ€§èƒ½ï¼Œå¹¶ä½¿ç”¨ç®€å•çš„3Däººä½“å§¿æ€è§£ç å™¨è¿›è¡Œè¯„ä¼°ã€‚

**Result:** åœ¨Human3.6Mæ•°æ®é›†ä¸Šè¾¾åˆ°MPJPE 49.9mmï¼Œåœ¨3DPWæ•°æ®é›†ä¸Šè·¨åŸŸè¯„ä¼°è¾¾åˆ°PA-MPJPE 51.6mmï¼ŒåŒæ—¶åœ¨Human3.6Mæ•°æ®é›†ä¸Šå®ç°2Då’Œ3Då§¿æ€æ£€ç´¢ï¼Œæ£€ç´¢è¯¯å·®ä¸º9.24mm MPJPEã€‚

**Conclusion:** UniHPRæ¡†æ¶æˆåŠŸå®ç°äº†å¤šæ¨¡æ€äººä½“å§¿æ€è¡¨ç¤ºçš„ç»Ÿä¸€å¯¹é½ï¼Œè¯æ˜äº†åŸºäºå¥‡å¼‚å€¼çš„å¯¹æ¯”å­¦ä¹ æŸå¤±åœ¨å¤šæ¨¡æ€å¯¹é½ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºäººä½“å§¿æ€ä¼°è®¡å’Œç›¸å…³ä»»åŠ¡æä¾›äº†å¼ºå¤§çš„ç»Ÿä¸€è¡¨ç¤ºåŸºç¡€ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€äººä½“å§¿æ€åˆ†æçš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
In recent years, there has been a growing interest in developing effective
alignment pipelines to generate unified representations from different
modalities for multi-modal fusion and generation. As an important component of
Human-Centric applications, Human Pose representations are critical in many
downstream tasks, such as Human Pose Estimation, Action Recognition,
Human-Computer Interaction, Object tracking, etc. Human Pose representations or
embeddings can be extracted from images, 2D keypoints, 3D skeletons, mesh
models, and lots of other modalities. Yet, there are limited instances where
the correlation among all of those representations has been clearly researched
using a contrastive paradigm. In this paper, we propose UniHPR, a unified Human
Pose Representation learning pipeline, which aligns Human Pose embeddings from
images, 2D and 3D human poses. To align more than two data representations at
the same time, we propose a novel singular value-based contrastive learning
loss, which better aligns different modalities and further boosts performance.
To evaluate the effectiveness of the aligned representation, we choose 2D and
3D Human Pose Estimation (HPE) as our evaluation tasks. In our evaluation, with
a simple 3D human pose decoder, UniHPR achieves remarkable performance metrics:
MPJPE 49.9mm on the Human3.6M dataset and PA-MPJPE 51.6mm on the 3DPW dataset
with cross-domain evaluation. Meanwhile, we are able to achieve 2D and 3D pose
retrieval with our unified human pose representations in Human3.6M dataset,
where the retrieval error is 9.24mm in MPJPE.


### [4] [X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning](https://arxiv.org/abs/2510.19150)
*Yunzhe Wang, Soham Hans, Volkan Ustun*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†X-Ego-CSåŸºå‡†æ•°æ®é›†å’Œè·¨è‡ªæˆ‘å¯¹æ¯”å­¦ä¹ (CECL)æ–¹æ³•ï¼Œç”¨äºè§£å†³å¤šæ™ºèƒ½ä½“å†³ç­–ä¸­åŒæ­¥è‡ªæˆ‘ä¸­å¿ƒè§†è§’å»ºæ¨¡çš„æŒ‘æˆ˜ï¼Œé€šè¿‡å¯¹é½é˜Ÿå‹çš„ç¬¬ä¸€äººç§°è§†è§‰æµæ¥å¢å¼ºå›¢é˜Ÿæˆ˜æœ¯æƒ…å¢ƒæ„ŸçŸ¥èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†é¢‘ç†è§£ç ”ç©¶ä¸»è¦ä¾èµ–ç¬¬ä¸‰äººç§°å¹¿æ’­è§†è§’ï¼Œå¿½è§†äº†å¤šæ™ºèƒ½ä½“å­¦ä¹ ä¸­åŒæ­¥è‡ªæˆ‘ä¸­å¿ƒè§†è§’çš„é‡è¦æ€§ï¼Œæ— æ³•æœ‰æ•ˆå»ºæ¨¡äººç±»å›¢é˜Ÿæˆ˜æœ¯ä¸­ä¸ªä½“è§†è§’ä¸é˜Ÿå‹æ„å›¾é¢„æµ‹ä¹‹é—´çš„å¤æ‚äº¤äº’å…³ç³»ã€‚

**Method:** æ„å»ºäº†X-Ego-CSæ•°æ®é›†ï¼ŒåŒ…å«45åœºèŒä¸šçº§Counter-Strike 2æ¯”èµ›çš„124å°æ—¶æ¸¸æˆå½•åƒï¼Œæä¾›åŒæ­¥çš„ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘æµå’ŒçŠ¶æ€-åŠ¨ä½œè½¨è¿¹ï¼›æå‡ºäº†è·¨è‡ªæˆ‘å¯¹æ¯”å­¦ä¹ (CECL)æ–¹æ³•ï¼Œé€šè¿‡å¯¹é½é˜Ÿå‹çš„è‡ªæˆ‘ä¸­å¿ƒè§†è§‰æµæ¥åŸ¹å…»å›¢é˜Ÿçº§æˆ˜æœ¯æƒ…å¢ƒæ„ŸçŸ¥ã€‚

**Result:** åœ¨é˜Ÿå‹-å¯¹æ‰‹ä½ç½®é¢„æµ‹ä»»åŠ¡ä¸Šè¯„ä¼°CECLï¼Œè¯æ˜å…¶èƒ½æœ‰æ•ˆå¢å¼ºæ™ºèƒ½ä½“ä»å•ä¸€ç¬¬ä¸€äººç§°è§†è§’æ¨æ–­é˜Ÿå‹å’Œå¯¹æ‰‹ä½ç½®çš„èƒ½åŠ›ï¼Œä½¿ç”¨æœ€å…ˆè¿›çš„è§†é¢‘ç¼–ç å™¨å–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚

**Conclusion:** X-Ego-CSå’ŒCECLä¸ºç”µå­ç«æŠ€ä¸­çš„è·¨è‡ªæˆ‘ä¸­å¿ƒå¤šæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•å¥ å®šäº†åŸºç¡€ï¼Œå°†æ¸¸æˆç†è§£å®šä½ä¸ºå¤šæ™ºèƒ½ä½“å»ºæ¨¡å’Œæˆ˜æœ¯å­¦ä¹ çš„æµ‹è¯•å¹³å°ï¼Œå¯¹è™šæ‹Ÿå’Œç°å®é¢†åŸŸä¸­çš„æ—¶ç©ºæ¨ç†å’Œäººæœºåä½œå…·æœ‰é‡è¦å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
Human team tactics emerge from each player's individual perspective and their
ability to anticipate, interpret, and adapt to teammates' intentions. While
advances in video understanding have improved the modeling of team interactions
in sports, most existing work relies on third-person broadcast views and
overlooks the synchronous, egocentric nature of multi-agent learning. We
introduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplay
footage from 45 professional-level matches of the popular e-sports game
Counter-Strike 2, designed to facilitate research on multi-agent
decision-making in complex 3D environments. X-Ego-CS provides cross-egocentric
video streams that synchronously capture all players' first-person perspectives
along with state-action trajectories. Building on this resource, we propose
Cross-Ego Contrastive Learning (CECL), which aligns teammates' egocentric
visual streams to foster team-level tactical situational awareness from an
individual's perspective. We evaluate CECL on a teammate-opponent location
prediction task, demonstrating its effectiveness in enhancing an agent's
ability to infer both teammate and opponent positions from a single
first-person view using state-of-the-art video encoders. Together, X-Ego-CS and
CECL establish a foundation for cross-egocentric multi-agent benchmarking in
esports. More broadly, our work positions gameplay understanding as a testbed
for multi-agent modeling and tactical learning, with implications for
spatiotemporal reasoning and human-AI teaming in both virtual and real-world
domains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.


### [5] [PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning](https://arxiv.org/abs/2510.19183)
*Fengyuan Sun, Hui Chen, Xinhao Xu, Dandan Zheng, Jingdong Chen, Jun Zhou, Jungong Han, Guiguang Ding*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºPruneHalæ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”KVç¼“å­˜å‰ªæå¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹å…³é”®è§†è§‰ä¿¡æ¯çš„å…³æ³¨ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æœ‰æ•ˆç¼“è§£å¹»è§‰é—®é¢˜ï¼Œä¸”å‡ ä¹ä¸å¢åŠ æ¨ç†æˆæœ¬ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆè¦ä¹ˆå¼•å…¥é¢å¤–æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¦ä¹ˆåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ•´åˆå¤–éƒ¨æˆ–å†…éƒ¨ä¿¡æ¯ï¼Œè¿™äº›æ–¹æ³•éƒ½ä¼šå¸¦æ¥é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚ç ”ç©¶å‘ç°å¹»è§‰ä¸è§†è§‰ä»¤ç‰Œæ³¨æ„åŠ›åˆ†é…ä¸è¶³å¯†åˆ‡ç›¸å…³ï¼Œå†—ä½™è§†è§‰ä»¤ç‰Œåˆ†æ•£äº†æ¨¡å‹æ³¨æ„åŠ›ã€‚

**Method:** æå‡ºPruneHalæ–¹æ³•ï¼Œåˆ©ç”¨è‡ªé€‚åº”KVç¼“å­˜å‰ªææ¥å¢å¼ºæ¨¡å‹å¯¹å…³é”®è§†è§‰ä¿¡æ¯çš„å…³æ³¨ï¼Œè¯¥æ–¹æ³•æ— éœ€è®­ç»ƒä¸”æ¨¡å‹æ— å…³ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ä¸åŒçš„è§£ç ç­–ç•¥ä¸­ï¼ŒåŒ…æ‹¬ä¸“é—¨ä¸ºç¼“è§£å¹»è§‰è®¾è®¡çš„ç­–ç•¥ã€‚

**Result:** åœ¨å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„å¹»è§‰è¯„ä¼°åŸºå‡†ä¸Šå¯¹å››ç§ä¸»æµMLLMsè¿›è¡Œè¯„ä¼°ï¼Œå–å¾—äº†ç¨³å¥ä¸”ä¼˜å¼‚çš„ç»“æœï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶é¦–æ¬¡å°†ä»¤ç‰Œå‰ªæåº”ç”¨äºMLLMsçš„å¹»è§‰ç¼“è§£ï¼Œæä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è®­ç»ƒå…è´¹è§£å†³æ–¹æ¡ˆï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„å¹»è§‰é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ€è·¯ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
While multi-modal large language models (MLLMs) have made significant
progress in recent years, the issue of hallucinations remains a major
challenge. To mitigate this phenomenon, existing solutions either introduce
additional data for further training or incorporate external or internal
information during inference. However, these approaches inevitably introduce
extra computational costs. In this paper, we observe that hallucinations in
MLLMs are strongly associated with insufficient attention allocated to visual
tokens. In particular, the presence of redundant visual tokens disperses the
model's attention, preventing it from focusing on the most informative ones. As
a result, critical visual cues are often under-attended, which in turn
exacerbates the occurrence of hallucinations. Building on this observation, we
propose \textbf{PruneHal}, a training-free, simple yet effective method that
leverages adaptive KV cache pruning to enhance the model's focus on critical
visual information, thereby mitigating hallucinations. To the best of our
knowledge, we are the first to apply token pruning for hallucination mitigation
in MLLMs. Notably, our method don't require additional training and incurs
nearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be
seamlessly integrated with different decoding strategies, including those
specifically designed for hallucination mitigation. We evaluate PruneHal on
several widely used hallucination evaluation benchmarks using four mainstream
MLLMs, achieving robust and outstanding results that highlight the
effectiveness and superiority of our method. Our code will be publicly
available.


### [6] [FootFormer: Estimating Stability from Visual Input](https://arxiv.org/abs/2510.19170)
*Keaton Kraiger, Jingjing Li, Skanda Bharadwaj, Jesse Scott, Robert T. Collins, Yanxi Liu*

#### ğŸ§© TL;DR
FootFormeræ˜¯ä¸€ç§è·¨æ¨¡æ€æ–¹æ³•ï¼Œèƒ½å¤Ÿç›´æ¥ä»è§†è§‰è¾“å…¥è”åˆé¢„æµ‹äººä½“è¿åŠ¨åŠ¨åŠ›å­¦ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†è¶³éƒ¨å‹åŠ›åˆ†å¸ƒã€è¶³éƒ¨æ¥è§¦å›¾å’Œè´¨å¿ƒä¼°è®¡çš„æ˜¾è‘—æ”¹è¿›æˆ–ç­‰æ•ˆæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•é€šå¸¸åªèƒ½ç”Ÿæˆäººä½“è¿åŠ¨åŠ¨åŠ›å­¦ä¸­çš„ä¸€æˆ–ä¸¤ä¸ªæµ‹é‡æŒ‡æ ‡ï¼Œå­˜åœ¨é¢„æµ‹èƒ½åŠ›æœ‰é™çš„é—®é¢˜ï¼Œæ— æ³•å…¨é¢æ•æ‰è¿åŠ¨ç¨³å®šæ€§ç›¸å…³çš„å…³é”®è¦ç´ ã€‚

**Method:** è¯¥æ–¹æ³•é‡‡ç”¨è·¨æ¨¡æ€æ¶æ„ï¼Œç›´æ¥ä»è§†è§‰è¾“å…¥è”åˆé¢„æµ‹å¤šä¸ªè¿åŠ¨åŠ¨åŠ›å­¦æŒ‡æ ‡ï¼ŒåŒ…æ‹¬è¶³éƒ¨å‹åŠ›åˆ†å¸ƒã€è¶³éƒ¨æ¥è§¦å›¾å’Œè´¨å¿ƒä½ç½®ã€‚

**Result:** åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒFootFormeråœ¨è¶³éƒ¨å‹åŠ›åˆ†å¸ƒã€è¶³éƒ¨æ¥è§¦å›¾å’Œè´¨å¿ƒä¼°è®¡æ–¹é¢å®ç°äº†ç»Ÿè®¡æ˜¾è‘—æ›´å¥½çš„æ€§èƒ½æˆ–ç­‰æ•ˆç»“æœï¼Œå¹¶åœ¨ç»å…¸è¿åŠ¨å­¦æŒ‡æ ‡ä¸­çš„ç¨³å®šæ€§é¢„æµ‹ç»„ä»¶ï¼ˆå‹åŠ›ä¸­å¿ƒã€è´¨å¿ƒã€æ”¯æ’‘åŸºé¢ï¼‰ä¼°è®¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜ç›´æ¥ä»è§†è§‰è¾“å…¥è”åˆé¢„æµ‹å¤šä¸ªè¿åŠ¨åŠ¨åŠ›å­¦æŒ‡æ ‡æ˜¯å¯è¡Œçš„ï¼Œä¸ºåŸºäºè§†è§‰çš„è¿åŠ¨ç¨³å®šæ€§åˆ†ææä¾›äº†æ–°çš„æŠ€æœ¯é€”å¾„ï¼Œå…·æœ‰åœ¨åº·å¤åŒ»å­¦å’Œè¿åŠ¨ç§‘å­¦é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚

---

#### ğŸ“„ Abstract
We propose FootFormer, a cross-modality approach for jointly predicting human
motion dynamics directly from visual input. On multiple datasets, FootFormer
achieves statistically significantly better or equivalent estimates of foot
pressure distributions, foot contact maps, and center of mass (CoM), as
compared with existing methods that generate one or two of those measures.
Furthermore, FootFormer achieves SOTA performance in estimating
stability-predictive components (CoP, CoM, BoS) used in classic kinesiology
metrics. Code and data are available at
https://github.com/keatonkraiger/Vision-to-Stability.git.


### [7] [Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks](https://arxiv.org/abs/2510.19195)
*Kai Zeng, Zhanqian Wu, Kaixin Xiong, Xiaobao Wei, Xiangyu Guo, Zhenxin Zhu, Kalok Ho, Lijun Zhou, Bohan Zeng, Ming Lu, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Wentao Zhang*

#### ğŸ§© TL;DR
Dream4Driveæ˜¯ä¸€ä¸ªåˆ›æ–°çš„åˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡3Dæ„ŸçŸ¥è§†é¢‘ç¼–è¾‘å¢å¼ºè‡ªåŠ¨é©¾é©¶ä¸‹æ¸¸æ„ŸçŸ¥ä»»åŠ¡ï¼Œèƒ½å¤Ÿå¤§è§„æ¨¡ç”Ÿæˆå¤šè§†è§’æç«¯æ¡ˆä¾‹ï¼Œæ˜¾è‘—æå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æç«¯åœºæ™¯æ„ŸçŸ¥èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰é©¾é©¶ä¸–ç•Œæ¨¡å‹ä¸»è¦å…³æ³¨ç”Ÿæˆè´¨é‡å’Œå¯æ§æ€§æŒ‡æ ‡ï¼Œä½†å¿½è§†äº†å¯¹äºè‡ªåŠ¨é©¾é©¶æ€§èƒ½è‡³å…³é‡è¦çš„ä¸‹æ¸¸æ„ŸçŸ¥ä»»åŠ¡è¯„ä¼°ã€‚ä¼ ç»Ÿæ–¹æ³•é‡‡ç”¨åˆæˆæ•°æ®é¢„è®­ç»ƒåŠ çœŸå®æ•°æ®å¾®è°ƒçš„ç­–ç•¥éœ€è¦ä¸¤å€è®­ç»ƒå‘¨æœŸï¼Œå½“åŸºçº¿æ–¹æ³•åŒæ ·ä½¿ç”¨åŒå€å‘¨æœŸæ—¶ï¼Œåˆæˆæ•°æ®çš„ä¼˜åŠ¿å˜å¾—å¾®ä¸è¶³é“ã€‚

**Method:** Dream4Driveé¦–å…ˆå°†è¾“å…¥è§†é¢‘åˆ†è§£ä¸ºå¤šä¸ª3Dæ„ŸçŸ¥å¼•å¯¼å›¾ï¼Œç„¶åå°†3Dèµ„æºæ¸²æŸ“åˆ°è¿™äº›å¼•å¯¼å›¾ä¸Šï¼Œæœ€åå¾®è°ƒé©¾é©¶ä¸–ç•Œæ¨¡å‹ç”Ÿæˆç»è¿‡ç¼–è¾‘çš„å¤šè§†è§’é€¼çœŸè§†é¢‘ã€‚è¯¥æ¡†æ¶è¿˜è´¡çŒ®äº†å¤§è§„æ¨¡3Dèµ„æºæ•°æ®é›†DriveObj3Dï¼Œè¦†ç›–å…¸å‹é©¾é©¶åœºæ™¯ç±»åˆ«å¹¶æ”¯æŒå¤šæ ·åŒ–3Dæ„ŸçŸ¥è§†é¢‘ç¼–è¾‘ã€‚

**Result:** ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDream4Driveåœ¨å„ç§è®­ç»ƒå‘¨æœŸä¸‹éƒ½èƒ½æœ‰æ•ˆæå‡ä¸‹æ¸¸æ„ŸçŸ¥æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆå¤šè§†è§’æç«¯æ¡ˆä¾‹æ–¹é¢å±•ç°å‡ºå‰æ‰€æœªæœ‰çš„çµæ´»æ€§ï¼Œæ˜¾è‘—å¢å¼ºäº†è‡ªåŠ¨é©¾é©¶ä¸­çš„æç«¯æ¡ˆä¾‹æ„ŸçŸ¥èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åˆæˆæ•°æ®åœ¨è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„å®é™…ä»·å€¼ï¼Œé€šè¿‡åˆ›æ–°çš„3Dæ„ŸçŸ¥è§†é¢‘ç¼–è¾‘æ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å¤§è§„æ¨¡3Dèµ„æºæ•°æ®é›†ï¼Œæ¨åŠ¨äº†è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿåœ¨æç«¯åœºæ™¯ä¸‹çš„æ€§èƒ½æå‡ã€‚

---

#### ğŸ“„ Abstract
Recent advancements in driving world models enable controllable generation of
high-quality RGB videos or multimodal videos. Existing methods primarily focus
on metrics related to generation quality and controllability. However, they
often overlook the evaluation of downstream perception tasks, which are
$\mathbf{really\ crucial}$ for the performance of autonomous driving. Existing
methods usually leverage a training strategy that first pretrains on synthetic
data and finetunes on real data, resulting in twice the epochs compared to the
baseline (real data only). When we double the epochs in the baseline, the
benefit of synthetic data becomes negligible. To thoroughly demonstrate the
benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data
generation framework designed for enhancing the downstream perception tasks.
Dream4Drive first decomposes the input video into several 3D-aware guidance
maps and subsequently renders the 3D assets onto these guidance maps. Finally,
the driving world model is fine-tuned to produce the edited, multi-view
photorealistic videos, which can be used to train the downstream perception
models. Dream4Drive enables unprecedented flexibility in generating multi-view
corner cases at scale, significantly boosting corner case perception in
autonomous driving. To facilitate future research, we also contribute a
large-scale 3D asset dataset named DriveObj3D, covering the typical categories
in driving scenarios and enabling diverse 3D-aware video editing. We conduct
comprehensive experiments to show that Dream4Drive can effectively boost the
performance of downstream perception models under various training epochs.
Project: $\href{https://wm-research.github.io/Dream4Drive/}{this\ https\ URL}$


### [8] [SFGFusion: Surface Fitting Guided 3D Object Detection with 4D Radar and Camera Fusion](https://arxiv.org/abs/2510.19215)
*Xiaozhi Li, Huijun Di, Jian Li, Feng Liu, Wei Liang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºSFGFusionï¼Œä¸€ç§åŸºäºè¡¨é¢æ‹Ÿåˆå¼•å¯¼çš„ç›¸æœº-4Dæˆåƒé›·è¾¾æ£€æµ‹ç½‘ç»œï¼Œé€šè¿‡ä¼°è®¡ç‰©ä½“çš„äºŒæ¬¡æ›²é¢å‚æ•°æ¥å¢å¼ºç©ºé—´è¡¨ç¤ºå’Œè·¨æ¨¡æ€äº¤äº’ï¼Œæœ‰æ•ˆè§£å†³äº†4Dé›·è¾¾ç‚¹äº‘ç¨€ç–æ€§å’Œå¤šæ¨¡æ€èåˆçš„æŒ‘æˆ˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** 4Dæˆåƒé›·è¾¾è™½ç„¶å…·æœ‰ä½æˆæœ¬ã€è¿œè·ç¦»æ£€æµ‹å’Œç²¾ç¡®é€Ÿåº¦æµ‹é‡çš„ä¼˜åŠ¿ï¼Œä½†å…¶ç¨€ç–ç‚¹äº‘å’Œä½åˆ†è¾¨ç‡é™åˆ¶äº†ç‰©ä½“çš„å‡ ä½•è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶é˜»ç¢äº†ä¸ç›¸æœºç­‰å¤šæ¨¡æ€ä¼ æ„Ÿå™¨çš„æœ‰æ•ˆèåˆï¼Œè¿™æ˜¯å½“å‰3Dç›®æ ‡æ£€æµ‹é¢†åŸŸé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚

**Method:** SFGFusioné€šè¿‡ä»å›¾åƒå’Œé›·è¾¾æ•°æ®ä¼°è®¡ç‰©ä½“çš„äºŒæ¬¡æ›²é¢å‚æ•°ï¼Œæ„å»ºæ˜¾å¼è¡¨é¢æ‹Ÿåˆæ¨¡å‹æ¥å¢å¼ºç©ºé—´è¡¨ç¤ºå’Œè·¨æ¨¡æ€äº¤äº’ï¼Œç”Ÿæˆç»†ç²’åº¦å¯†é›†æ·±åº¦é¢„æµ‹ï¼›è¯¥æ·±åº¦é¢„æµ‹ç”¨äºå¼•å¯¼å›¾åƒç‰¹å¾ä»é€è§†è§†å›¾åˆ°é¸Ÿç°å›¾çš„è½¬æ¢ï¼Œå¹¶ç”Ÿæˆå¯†é›†ä¼ªç‚¹äº‘ä»¥ç¼“è§£é›·è¾¾ç‚¹ç¨€ç–æ€§ï¼›æœ€ç»ˆé‡‡ç”¨åŸºäºæŸ±ä½“çš„æ–¹æ³•å¤„ç†ç‚¹äº‘ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ ‡å‡†2Déª¨å¹²ç½‘ç»œå’Œæ£€æµ‹å¤´åœ¨BEVç©ºé—´è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼ŒSFGFusionåœ¨TJ4DRadSetå’Œview-of-delft (VoD) ç›®æ ‡æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¼˜è¶Šæ€§èƒ½ï¼Œæœ‰æ•ˆèåˆäº†ç›¸æœºå’Œ4Dé›·è¾¾ç‰¹å¾ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€èåˆå’Œç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºè¡¨é¢æ‹Ÿåˆçš„è·¨æ¨¡æ€èåˆç­–ç•¥èƒ½å¤Ÿæ˜¾è‘—æå‡4Dé›·è¾¾ä¸ç›¸æœºèåˆçš„æ€§èƒ½ï¼Œä¸ºç¨€ç–ä¼ æ„Ÿå™¨æ•°æ®çš„å‡ ä½•è¡¨ç¤ºå’Œå¤šæ¨¡æ€äº¤äº’æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¯¹è‡ªåŠ¨é©¾é©¶ä¸­çš„3Dç›®æ ‡æ£€æµ‹å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚

---

#### ğŸ“„ Abstract
3D object detection is essential for autonomous driving. As an emerging
sensor, 4D imaging radar offers advantages as low cost, long-range detection,
and accurate velocity measurement, making it highly suitable for object
detection. However, its sparse point clouds and low resolution limit object
geometric representation and hinder multi-modal fusion. In this study, we
introduce SFGFusion, a novel camera-4D imaging radar detection network guided
by surface fitting. By estimating quadratic surface parameters of objects from
image and radar data, the explicit surface fitting model enhances spatial
representation and cross-modal interaction, enabling more reliable prediction
of fine-grained dense depth. The predicted depth serves two purposes: 1) in an
image branch to guide the transformation of image features from perspective
view (PV) to a unified bird's-eye view (BEV) for multi-modal fusion, improving
spatial mapping accuracy; and 2) in a surface pseudo-point branch to generate
dense pseudo-point cloud, mitigating the radar point sparsity. The original
radar point cloud is also encoded in a separate radar branch. These two point
cloud branches adopt a pillar-based method and subsequently transform the
features into the BEV space. Finally, a standard 2D backbone and detection head
are used to predict object labels and bounding boxes from BEV features.
Experimental results show that SFGFusion effectively fuses camera and 4D radar
features, achieving superior performance on the TJ4DRadSet and view-of-delft
(VoD) object detection benchmarks.


### [9] [MobiAct: Efficient MAV Action Recognition Using MobileNetV4 with Contrastive Learning and Knowledge Distillation](https://arxiv.org/abs/2510.19273)
*Zhang Nengbo, Ho Hann Woei*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§MAVåŠ¨ä½œè¯†åˆ«æ¡†æ¶MobiActï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦å’Œå‚æ•°è‡ªç”±æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶å®ç°äº†ä½èƒ½è€—å’Œå¿«é€Ÿæ¨ç†ï¼Œæ˜¾è‘—æå‡äº†å¾®å‹é£è¡Œå™¨åœ¨èµ„æºå—é™å¹³å°ä¸Šçš„åŠ¨ä½œè¯†åˆ«æ•ˆç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¾®å‹é£è¡Œå™¨åŠ¨ä½œè¯†åˆ«æ–¹æ³•å¤§å¤šä¾èµ–è®¡ç®—å¯†é›†å‹å¤§å‹æ¨¡å‹ï¼Œæ— æ³•åœ¨èµ„æºå—é™çš„MAVå¹³å°ä¸Šæœ‰æ•ˆéƒ¨ç½²ï¼Œå¯¼è‡´è¯†åˆ«ç²¾åº¦ä¸æ¨ç†é€Ÿåº¦ä¹‹é—´å­˜åœ¨æ˜¾è‘—æƒè¡¡ï¼ŒäºŸéœ€å¼€å‘å…¼é¡¾é«˜ç²¾åº¦ä¸ä½è®¡ç®—æˆæœ¬çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆã€‚

**Method:** MobiActé‡‡ç”¨MobileNetV4ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œå¼•å…¥é˜¶æ®µå¼æ­£äº¤çŸ¥è¯†è’¸é¦ç­–ç•¥å°†ResNet18æ•™å¸ˆç½‘ç»œçš„MAVè¿åŠ¨ç‰¹å¾æœ‰æ•ˆè¿ç§»è‡³å­¦ç”Ÿç½‘ç»œï¼ŒåŒæ—¶é›†æˆå‚æ•°è‡ªç”±æ³¨æ„åŠ›æœºåˆ¶æå‡è¯†åˆ«ç²¾åº¦è€Œä¸å¢åŠ æ¨¡å‹å¤æ‚åº¦ï¼Œå¹¶å¼€å‘æ··åˆæŸå¤±è®­ç»ƒç­–ç•¥ç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜MobiActåœ¨ä¸‰ä¸ªè‡ªæ”¶é›†æ•°æ®é›†ä¸Šå¹³å‡è¯†åˆ«å‡†ç¡®ç‡è¾¾åˆ°92.12%ï¼Œä»…æ¶ˆè€—136.16çš®ç„¦èƒ½é‡ï¼Œå¤„ç†é€Ÿåº¦ä¸ºæ¯ç§’8.84ä¸ªåŠ¨ä½œï¼ŒåŠ¨ä½œè§£ç é€Ÿåº¦æ¯”é¢†å…ˆæ–¹æ³•å¿«2å€ï¼ŒåŒæ—¶ä¿æŒé«˜åº¦å¯æ¯”çš„è¯†åˆ«ç²¾åº¦ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†è½»é‡çº§æ¶æ„ç»“åˆçŸ¥è¯†è’¸é¦å’Œæ³¨æ„åŠ›æœºåˆ¶å¯åœ¨èµ„æºå—é™å¹³å°ä¸Šå®ç°é«˜æ•ˆMAVåŠ¨ä½œè¯†åˆ«ï¼Œä¸ºè‡ªä¸»ç©ºä¸­èœ‚ç¾¤ç³»ç»Ÿçš„å®æ—¶æ„ŸçŸ¥ä¸åè°ƒæä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ï¼Œå±•ç¤ºäº†åœ¨ç²¾åº¦ä¸æ•ˆç‡ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡çš„å®ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Accurate and efficient recognition of Micro Air Vehicle (MAV) motion is
essential for enabling real-time perception and coordination in autonomous
aerial swarm. However, most existing approaches rely on large, computationally
intensive models that are unsuitable for resource-limited MAV platforms, which
results in a trade-off between recognition accuracy and inference speed. To
address these challenges, this paper proposes a lightweight MAV action
recognition framework, MobiAct, designed to achieve high accuracy with low
computational cost. Specifically, MobiAct adopts MobileNetV4 as the backbone
network and introduces a Stage-wise Orthogonal Knowledge Distillation (SOKD)
strategy to effectively transfer MAV motion features from a teacher network
(ResNet18) to a student network, thereby enhancing knowledge transfer
efficiency. Furthermore, a parameter-free attention mechanism is integrated
into the architecture to improve recognition accuracy without increasing model
complexity. In addition, a hybrid loss training strategy is developed to
combine multiple loss objectives, which ensures stable and robust optimization
during training. Experimental results demonstrate that the proposed MobiAct
achieves low-energy and low-computation MAV action recognition, while
maintaining the fastest action decoding speed among compared methods. Across
all three self-collected datasets, MobiAct achieves an average recognition
accuracy of 92.12%, while consuming only 136.16 pJ of energy and processing
recognition at a rate of 8.84 actions per second. Notably, MobiAct decodes
actions up to 2 times faster than the leading method, with highly comparable
recognition accuracy, highlighting its superior efficiency in MAV action
recognition.


### [10] [CARES: Context-Aware Resolution Selector for VLMs](https://arxiv.org/abs/2510.19496)
*Moshe Kimhi, Nimrod Shabtay, Raja Giryes, Chaim Baskin, Eli Schwartz*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºCARESï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†è¾¨ç‡é€‰æ‹©å™¨ï¼‰ï¼Œä¸€ä¸ªè½»é‡çº§é¢„å¤„ç†æ¨¡å—ï¼Œèƒ½å¤Ÿé¢„æµ‹å›¾åƒ-æŸ¥è¯¢å¯¹æ‰€éœ€çš„æœ€å°è¶³å¤Ÿè¾“å…¥åˆ†è¾¨ç‡ï¼Œåœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶å°†è®¡ç®—é‡å‡å°‘é«˜è¾¾80%ã€‚è¯¥æ–¹æ³•é€šè¿‡ç´§å‡‘è§†è§‰è¯­è¨€æ¨¡å‹æå–ç‰¹å¾å¹¶é¢„æµ‹ç›®æ ‡é¢„è®­ç»ƒVLMå“åº”æ”¶æ•›åˆ°å³°å€¼èƒ½åŠ›æ—¶çš„åˆ†è¾¨ç‡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹é€šå¸¸ä»¥åŸç”Ÿæˆ–é«˜åˆ†è¾¨ç‡å¤„ç†å›¾åƒä»¥ç¡®ä¿è·¨ä»»åŠ¡æœ‰æ•ˆæ€§ï¼Œè¿™å¯¼è‡´è§†è§‰ä»¤ç‰Œå æ€»ä»¤ç‰Œçš„97-99%ï¼Œå³ä½¿åœ¨ä½åˆ†è¾¨ç‡å›¾åƒè¶³å¤Ÿçš„æƒ…å†µä¸‹ä¹Ÿä¼šäº§ç”Ÿé«˜è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹æ™ºèƒ½åˆ†è¾¨ç‡é€‰æ‹©æœºåˆ¶æ¥å¹³è¡¡è®¡ç®—æ•ˆç‡ä¸ä»»åŠ¡æ€§èƒ½ã€‚

**Method:** CARESé‡‡ç”¨ç´§å‡‘VLMï¼ˆ350Må‚æ•°ï¼‰æå–å›¾åƒ-æŸ¥è¯¢å¯¹ç‰¹å¾ï¼Œé¢„æµ‹ç›®æ ‡é¢„è®­ç»ƒVLMå“åº”æ”¶æ•›åˆ°å…¶æ­£ç¡®å›ç­”å³°å€¼èƒ½åŠ›æ—¶çš„æœ€å°è¶³å¤Ÿåˆ†è¾¨ç‡ã€‚è™½ç„¶è®­ç»ƒä¸ºç¦»æ•£åˆ†ç±»å™¨åœ¨å¯é€‰åˆ†è¾¨ç‡é›†åˆä¸Šè¿›è¡Œï¼Œä½†åœ¨æ¨ç†æ—¶å¯æ’å€¼è¿ç»­åˆ†è¾¨ç‡ä»¥å®ç°ç»†ç²’åº¦æ§åˆ¶ã€‚

**Result:** åœ¨æ¶µç›–æ–‡æ¡£å’Œè‡ªç„¶å›¾åƒçš„äº”ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä»¥åŠå¤šæ ·åŒ–ç›®æ ‡VLMä¸Šï¼ŒCARESåœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶å°†è®¡ç®—é‡å‡å°‘é«˜è¾¾80%ã€‚è¯¥æ–¹æ³•è¯æ˜äº†æ™ºèƒ½åˆ†è¾¨ç‡é€‰æ‹©åœ¨ç»´æŒæ¨¡å‹å‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** CARESå±•ç¤ºäº†é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†è¾¨ç‡é€‰æ‹©å¯æ˜¾è‘—ä¼˜åŒ–è§†è§‰è¯­è¨€æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ï¼Œä¸ºå®é™…éƒ¨ç½²æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä¸ºå¤šæ¨¡æ€ç³»ç»Ÿè®¾è®¡æä¾›äº†æ–°æ€è·¯ï¼Œå³é€šè¿‡è½»é‡çº§é¢„å¤„ç†æ¨¡å—åŠ¨æ€è°ƒæ•´è¾“å…¥åˆ†è¾¨ç‡æ¥å¹³è¡¡æ€§èƒ½ä¸æ•ˆç‡ã€‚

---

#### ğŸ“„ Abstract
Large vision-language models (VLMs) commonly process images at native or high
resolution to remain effective across tasks. This inflates visual tokens ofter
to 97-99% of total tokens, resulting in high compute and latency, even when
low-resolution images would suffice. We introduce \emph{CARES}-a
\textbf{C}ontext-\textbf{A}ware \textbf{R}esolution \textbf{S}elector, a
lightweight preprocessing module that, given an image-query pair, predicts the
\emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to
extract features and predict when a target pretrained VLM's response converges
to its peak ability to answer correctly. Though trained as a discrete
classifier over a set of optional resolutions, CARES interpolates continuous
resolutions at inference for fine-grained control. Across five multimodal
benchmarks spanning documents and natural images, as well as diverse target
VLMs, CARES preserves task performance while reducing compute by up to 80%.


### [11] [D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation](https://arxiv.org/abs/2510.19278)
*Nobline Yoo, Olga Russakovsky, Ye Zhu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†D2Dæ¡†æ¶ï¼Œå°†ä¸å¯å¾®çš„æ£€æµ‹æ¨¡å‹è½¬åŒ–ä¸ºå¯å¾®çš„è¯„è®ºå™¨ï¼Œåˆ©ç”¨å…¶ä¼˜è¶Šçš„è®¡æ•°èƒ½åŠ›æŒ‡å¯¼æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆæ­£ç¡®æ•°é‡çš„å¯¹è±¡ï¼Œæ˜¾è‘—æå‡äº†å¯¹è±¡è®¡æ•°å‡†ç¡®æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨è¯­ä¹‰å¯¹é½æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ç”Ÿæˆæç¤ºä¸­æŒ‡å®šæ•°é‡çš„å¯¹è±¡æ—¶ä»å­˜åœ¨å›°éš¾ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨å¯å¾®çš„å›å½’æ¨¡å‹ä½œä¸ºå¤–éƒ¨è¯„è®ºå™¨ï¼Œä½†æ’é™¤äº†å…·æœ‰æ›´ä¼˜è®¡æ•°èƒ½åŠ›ä½†ä¸å¯å¾®çš„æ£€æµ‹å™¨æ¨¡å‹ï¼Œå› ä¸ºå…¶åŸºäºæšä¸¾çš„è®¡æ•°æœ¬è´¨ä¸å¯å¾®åˆ†ã€‚

**Method:** æå‡ºäº†Detector-to-Differentiableæ¡†æ¶ï¼Œé€šè¿‡è®¾è®¡è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°å°†æ£€æµ‹å™¨é€»è¾‘è½¬æ¢ä¸ºè½¯äºŒè¿›åˆ¶æŒ‡ç¤ºå™¨ï¼Œåœ¨æ¨ç†æ—¶ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¼˜åŒ–å™ªå£°å…ˆéªŒï¼Œä»è€Œå°†ä¸å¯å¾®çš„æ£€æµ‹æ¨¡å‹è½¬åŒ–ä¸ºå¯å¾®çš„è¯„è®ºå™¨ã€‚

**Result:** åœ¨SDXL-Turboã€SD-Turboå’ŒPixart-DMDæ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨å››ä¸ªä¸åŒå¤æ‚åº¦çš„åŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†å¯¹è±¡è®¡æ•°å‡†ç¡®æ€§çš„æŒç»­æ˜¾è‘—æå‡ï¼Œä¾‹å¦‚åœ¨D2D-SmallåŸºå‡†ä¸Šæå‡äº†13.7%ï¼ŒåŒæ—¶å›¾åƒè´¨é‡å’Œè®¡ç®—å¼€é”€å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å°†ä¸å¯å¾®æ£€æµ‹æ¨¡å‹è½¬åŒ–ä¸ºå¯å¾®è¯„è®ºå™¨çš„å¯è¡Œæ€§ï¼Œä¸ºæå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è®¡æ•°èƒ½åŠ›æä¾›äº†æ–°é€”å¾„ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ï¼Œä¸ºæœªæ¥ç»“åˆä¸åŒç±»å‹æ¨¡å‹ä¼˜åŠ¿çš„ç ”ç©¶å¼€è¾Ÿäº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Text-to-image (T2I) diffusion models have achieved strong performance in
semantic alignment, yet they still struggle with generating the correct number
of objects specified in prompts. Existing approaches typically incorporate
auxiliary counting networks as external critics to enhance numeracy. However,
since these critics must provide gradient guidance during generation, they are
restricted to regression-based models that are inherently differentiable, thus
excluding detector-based models with superior counting ability, whose
count-via-enumeration nature is non-differentiable. To overcome this
limitation, we propose Detector-to-Differentiable (D2D), a novel framework that
transforms non-differentiable detection models into differentiable critics,
thereby leveraging their superior counting ability to guide numeracy
generation. Specifically, we design custom activation functions to convert
detector logits into soft binary indicators, which are then used to optimize
the noise prior at inference time with pre-trained T2I models. Our extensive
experiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of
varying complexity (low-density, high-density, and multi-object scenarios)
demonstrate consistent and substantial improvements in object counting accuracy
(e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark),
with minimal degradation in overall image quality and computational overhead.


### [12] [A Matter of Time: Revealing the Structure of Time in Vision-Language Models](https://arxiv.org/abs/2510.19559)
*Nidham Tekaya, Manuela Waldner, Matthias Zeppelzauer*

#### ğŸ§© TL;DR
æœ¬æ–‡ç ”ç©¶äº†å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œå‘ç°æ—¶é—´ä¿¡æ¯åœ¨åµŒå…¥ç©ºé—´ä¸­æ²¿ä½ç»´éçº¿æ€§æµå½¢ç»“æ„åŒ–ï¼Œå¹¶æå‡ºä»åµŒå…¥ç©ºé—´æå–æ˜¾å¼æ—¶é—´çº¿è¡¨ç¤ºçš„æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•åœ¨æ—¶é—´æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†ä¸åŸºäºæç¤ºçš„åŸºçº¿æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜çš„å‡†ç¡®æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œè¯„ä¼°å®ƒä»¬å°†è§†è§‰å†…å®¹å®šä½åœ¨æ—¶é—´ä¸­çš„èƒ½åŠ›ã€‚å°½ç®¡VLMsé€šè¿‡å¤§è§„æ¨¡å¤šæ ·åŒ–æ–‡æœ¬å…ƒæ•°æ®è®­ç»ƒè·å¾—äº†å¼€æ”¾è¯æ±‡èƒ½åŠ›ï¼Œä½†å…¶å¯¹æ—¶é—´ä¿¡æ¯çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›å°šæœªå¾—åˆ°ç³»ç»Ÿç ”ç©¶ã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†TIME10kåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10,000å¼ å¸¦æœ‰æ—¶é—´çœŸå®æ ‡ç­¾çš„å›¾åƒï¼Œå¹¶é‡‡ç”¨æ–°é¢–æ–¹æ³•è¯„ä¼°äº†37ä¸ªVLMsçš„æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ã€‚åŸºäºå‘ç°çš„æ—¶é—´ä¿¡æ¯åœ¨åµŒå…¥ç©ºé—´ä¸­çš„ä½ç»´éçº¿æ€§æµå½¢ç»“æ„ï¼Œæå‡ºäº†ä»åµŒå…¥ç©ºé—´æå–æ˜¾å¼æ—¶é—´çº¿è¡¨ç¤ºçš„æ–¹æ³•ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œæ—¶é—´ä¿¡æ¯åœ¨VLMåµŒå…¥ç©ºé—´ä¸­æ²¿ä½ç»´éçº¿æ€§æµå½¢ç»“æ„åŒ–ã€‚æå‡ºçš„æ—¶é—´çº¿è¡¨ç¤ºæ–¹æ³•åœ¨æ—¶é—´æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†ä¸åŸºäºæç¤ºçš„åŸºçº¿æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†VLMsåµŒå…¥ç©ºé—´ä¸­æ—¶é—´ä¿¡æ¯çš„ç»“æ„åŒ–ç‰¹æ€§ï¼Œä¸ºæ—¶é—´æ¨ç†ä»»åŠ¡æä¾›äº†æ–°çš„æ–¹æ³•è§†è§’ã€‚æå‡ºçš„æ—¶é—´çº¿è¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆå»ºæ¨¡æ—¶é—´åŠå…¶æ—¶åºè¿›å±•ï¼Œä¸ºå¤šæ¨¡æ€æ—¶é—´ç†è§£å¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Large-scale vision-language models (VLMs) such as CLIP have gained popularity
for their generalizable and expressive multimodal representations. By
leveraging large-scale training data with diverse textual metadata, VLMs
acquire open-vocabulary capabilities, solving tasks beyond their training
scope. This paper investigates the temporal awareness of VLMs, assessing their
ability to position visual content in time. We introduce TIME10k, a benchmark
dataset of over 10,000 images with temporal ground truth, and evaluate the
time-awareness of 37 VLMs by a novel methodology. Our investigation reveals
that temporal information is structured along a low-dimensional, non-linear
manifold in the VLM embedding space. Based on this insight, we propose methods
to derive an explicit ``timeline'' representation from the embedding space.
These representations model time and its chronological progression and thereby
facilitate temporal reasoning tasks. Our timeline approaches achieve
competitive to superior accuracy compared to a prompt-based baseline while
being computationally efficient. All code and data are available at
https://tekayanidham.github.io/timeline-page/.


### [13] [Unified Reinforcement and Imitation Learning for Vision-Language Models](https://arxiv.org/abs/2510.19307)
*Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ç»Ÿä¸€å¼ºåŒ–ä¸æ¨¡ä»¿å­¦ä¹ ï¼ˆRILï¼‰ï¼Œä¸€ç§é«˜æ•ˆçš„è®­ç»ƒç®—æ³•ï¼Œé€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œä½¿è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ¨¡ä»¿å¤§å‹æ•™å¸ˆæ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›å¹¶ç³»ç»Ÿæ€§åœ°æå‡ç”Ÿæˆæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è™½ç„¶å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å¤§è§„æ¨¡ç‰¹æ€§ä½¿å¾—åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²å˜å¾—ä¸åˆ‡å®é™…ï¼Œå› æ­¤éœ€è¦å¼€å‘èƒ½å¤Ÿåˆ›å»ºå¼ºå¤§ä¸”è½»é‡çº§VLMsçš„é«˜æ•ˆè®­ç»ƒæ–¹æ³•ã€‚

**Method:** RILç®—æ³•ç‹¬ç‰¹åœ°å°†å¼ºåŒ–å­¦ä¹ ä¸å¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ç›¸ç»“åˆï¼Œé‡‡ç”¨åŸºäºLLMçš„åˆ¤åˆ«å™¨æ¥åŒºåˆ†å­¦ç”Ÿå’Œæ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºï¼Œå¹¶åˆ©ç”¨å¤šä¸ªå¤§å‹æ•™å¸ˆVLMæä¾›å¤šæ ·åŒ–æŒ‡å¯¼ï¼Œå®ç°ç»Ÿä¸€çš„å¼ºåŒ–ä¸æ¨¡ä»¿å­¦ä¹ ç­–ç•¥ã€‚

**Result:** åœ¨å¤šç§è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒRILæ˜¾è‘—ç¼©å°äº†ä¸æœ€å…ˆè¿›å¼€æºå’Œé—­æºVLMsçš„æ€§èƒ½å·®è·ï¼Œå¹¶åœ¨å¤šä¸ªå®ä¾‹ä¸­è¶…è¶Šäº†è¿™äº›æ¨¡å‹ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†ç»Ÿä¸€å¼ºåŒ–ä¸æ¨¡ä»¿å­¦ä¹ ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œèƒ½å¤Ÿä½¿è½»é‡çº§å­¦ç”Ÿæ¨¡å‹åœ¨æ€§èƒ½ä¸Šåª²ç¾é¢†å…ˆçš„é—­æºVLMsï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„é«˜æ•ˆè§†è§‰è¯­è¨€æ¨¡å‹éƒ¨ç½²æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Vision-Language Models (VLMs) have achieved remarkable progress, yet their
large scale often renders them impractical for resource-constrained
environments. This paper introduces Unified Reinforcement and Imitation
Learning (RIL), a novel and efficient training algorithm designed to create
powerful, lightweight VLMs. RIL distinctively combines the strengths of
reinforcement learning with adversarial imitation learning. This enables
smaller student VLMs not only to mimic the sophisticated text generation of
large teacher models but also to systematically improve their generative
capabilities through reinforcement signals. Key to our imitation framework is
an LLM-based discriminator that adeptly distinguishes between student and
teacher outputs, complemented by guidance from multiple large teacher VLMs to
ensure diverse learning. This unified learning strategy, leveraging both
reinforcement and imitation, empowers student models to achieve significant
performance gains, making them competitive with leading closed-source VLMs.
Extensive experiments on diverse vision-language benchmarks demonstrate that
RIL significantly narrows the performance gap with state-of-the-art open- and
closed-source VLMs and, in several instances, surpasses them.


### [14] [Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration](https://arxiv.org/abs/2510.19579)
*Francisco Mena, Dino Ienco, Cassio F. Dantas, Roberto Interdonato, Andreas Dengel*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€ååŒå­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸é’ˆå¯¹ç‰¹å®šæ¨ç†æ¨¡æ€çš„æƒ…å†µä¸‹æ³›åŒ–åˆ°å„ç§ä»»åŠ¡ï¼Œé€šè¿‡ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œæ¨¡æ€åˆ¤åˆ«å­¦ä¹ æ¥æŒ‡å¯¼å•æ¨¡æ€æ¨¡å‹æ„å»ºæ¨¡æ€å…±äº«å’Œæ¨¡æ€ç‰¹å®šçš„å†…éƒ¨æ¨¡å‹æµå½¢ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** åœ°çƒè§‚æµ‹é¢†åŸŸä¸­çš„å¤šæ¨¡æ€æ•°æ®åˆ†æé¢ä¸´ç°å®çº¦æŸï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†é˜¶æ®µéš¾ä»¥è®¿é—®ç›¸åŒçš„ä¼ æ„Ÿå™¨æ¨¡æ€ï¼Œç°æœ‰ç ”ç©¶å¤§å¤šé’ˆå¯¹ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡æˆ–æ¨ç†æ¨¡æ€è®¾è®¡å®šåˆ¶åŒ–è§£å†³æ–¹æ¡ˆï¼Œç¼ºä¹é€šç”¨æ€§ã€‚

**Method:** è¯¥æ¡†æ¶ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œæ¨¡æ€åˆ¤åˆ«å­¦ä¹ ï¼Œå¼•å¯¼å•æ¨¡æ€æ¨¡å‹å°†å†…éƒ¨æ¨¡å‹æµå½¢ç»“æ„åŒ–åˆ†ä¸ºæ¨¡æ€å…±äº«å’Œæ¨¡æ€ç‰¹å®šä¿¡æ¯ï¼Œæ”¯æŒåœ¨ä»…æœ‰ä¸€ä¸ªè®­ç»ƒæ—¶å¯ç”¨æ¨¡æ€åœ¨æ¨ç†æ—¶å¯è®¿é—®çš„åœºæ™¯ä¸‹å·¥ä½œã€‚

**Result:** åœ¨å››ä¸ªæ¶µç›–åˆ†ç±»å’Œå›å½’ä»»åŠ¡çš„EOåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”æœ€æ–°çš„æœºå™¨å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰æ–¹æ³•ä»¥åŠEOç‰¹å®šæ–¹æ³•å‡å–å¾—äº†æŒç»­çš„é¢„æµ‹æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†å…¶åœ¨å•æ¨¡æ€æ¨ç†åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€ååŒå­¦ä¹ æ¡†æ¶åœ¨å¤šæ ·åŒ–EOåº”ç”¨ä¸­çš„å•æ¨¡æ€æ¨ç†åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤„ç†ç°å®çº¦æŸä¸‹çš„å¤šæ¨¡æ€æ•°æ®å­¦ä¹ æä¾›äº†é€šç”¨è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

---

#### ğŸ“„ Abstract
Multi-modal co-learning is emerging as an effective paradigm in machine
learning, enabling models to collaboratively learn from different modalities to
enhance single-modality predictions. Earth Observation (EO) represents a
quintessential domain for multi-modal data analysis, wherein diverse remote
sensors collect data to sense our planet. This unprecedented volume of data
introduces novel challenges. Specifically, the access to the same sensor
modalities at both training and inference stages becomes increasingly complex
based on real-world constraints affecting remote sensing platforms. In this
context, multi-modal co-learning presents a promising strategy to leverage the
vast amount of sensor-derived data available at the training stage to improve
single-modality models for inference-time deployment. Most current research
efforts focus on designing customized solutions for either particular
downstream tasks or specific modalities available at the inference stage. To
address this, we propose a novel multi-modal co-learning framework capable of
generalizing across various tasks without targeting a specific modality for
inference. Our approach combines contrastive and modality discriminative
learning together to guide single-modality models to structure the internal
model manifold into modality-shared and modality-specific information. We
evaluate our framework on four EO benchmarks spanning classification and
regression tasks across different sensor modalities, where only one of the
modalities available during training is accessible at inference time. Our
results demonstrate consistent predictive improvements over state-of-the-art
approaches from the recent machine learning and computer vision literature, as
well as EO-specific methods. The obtained findings validate our framework in
the single-modality inference scenarios across a diverse range of EO
applications.


### [15] [BrainMCLIP: Brain Image Decoding with Multi-Layer feature Fusion of CLIP](https://arxiv.org/abs/2510.19332)
*Tian Xia, Zihan Ma, Xinlong Wang, Qing Liu, Xiaowei He, Tianming Liu, Yudan Ren*

#### ğŸ§© TL;DR
BrainMCLIPæå‡ºäº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„ä»fMRIè§£ç å›¾åƒçš„æ–¹æ³•ï¼Œé€šè¿‡å¤šå±‚çº§èåˆç­–ç•¥å°†å¤§è„‘è§†è§‰åŒºåŸŸä¸CLIPä¸­é—´å±‚å¯¹é½ï¼Œæ— éœ€é¢å¤–çš„VAEé€šè·¯å³å¯åœ¨è¯­ä¹‰å‡†ç¡®æ€§å’Œç»†èŠ‚ä¿çœŸåº¦ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰ä»fMRIè§£ç å›¾åƒçš„æ–¹æ³•é€šå¸¸å°†å¤§è„‘æ´»åŠ¨æ˜ å°„åˆ°CLIPçš„æœ€ç»ˆè¯­ä¹‰å±‚ï¼Œä¸ºäº†æ•æ‰æ›´ç²¾ç»†çš„è§†è§‰ç»†èŠ‚è€Œæ·»åŠ å‚æ•°å¯†é›†çš„VAEé€šè·¯ï¼Œä½†è¿™äº›æ–¹æ³•å¿½ç•¥äº†CLIPä¸­é—´å±‚çš„ä¸°å¯Œç‰©ä½“ä¿¡æ¯ï¼Œä¸”ä¸å¤§è„‘åŠŸèƒ½å±‚æ¬¡ç»“æ„ä¸ç¬¦ã€‚

**Method:** BrainMCLIPé‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¤šå±‚çº§èåˆæ–¹æ³•ï¼Œå°†åŠŸèƒ½ä¸Šä¸åŒçš„è§†è§‰åŒºåŸŸï¼ˆä½å±‚/é«˜å±‚ï¼‰çš„fMRIä¿¡å·åˆ†åˆ«ä¸CLIPçš„ä¸­é—´å±‚å’Œæœ€ç»ˆå±‚å¯¹é½ï¼Œå°Šé‡åŠŸèƒ½å±‚æ¬¡ç»“æ„ï¼Œå¹¶å¼•å…¥äº†äº¤å‰é‡å»ºç­–ç•¥å’Œæ–°å‹å¤šç²’åº¦æŸå¤±å‡½æ•°ã€‚

**Result:** BrainMCLIPåœ¨é«˜å±‚è¯­ä¹‰æŒ‡æ ‡ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†åŒ…æ‹¬ä½¿ç”¨VAEé€šè·¯çš„æœ€å…ˆè¿›æ–¹æ³•åœ¨å†…çš„ç«äº‰æ€§èƒ½ï¼ŒåŒæ—¶å‚æ•°æ•°é‡æ˜¾è‘—å‡å°‘71.7%ï¼Œé€šè¿‡åˆ©ç”¨CLIPä¸­é—´ç‰¹å¾æœ‰æ•ˆæ•æ‰äº†CLIP-onlyæ–¹æ³•å¸¸é—æ¼çš„è§†è§‰ç»†èŠ‚ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¡¨æ˜é€šè¿‡åˆç†åˆ©ç”¨CLIPä¸­é—´å±‚ç‰¹å¾å’Œéµå¾ªå¤§è„‘åŠŸèƒ½å±‚æ¬¡ç»“æ„ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–å•ç‹¬VAEé€šè·¯çš„æƒ…å†µä¸‹å®ç°è¯­ä¹‰å‡†ç¡®æ€§å’Œç»†èŠ‚ä¿çœŸåº¦çš„è‰¯å¥½å¹³è¡¡ï¼Œä¸ºfMRIå›¾åƒè§£ç æä¾›äº†æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Decoding images from fMRI often involves mapping brain activity to CLIP's
final semantic layer. To capture finer visual details, many approaches add a
parameter-intensive VAE-based pipeline. However, these approaches overlook rich
object information within CLIP's intermediate layers and contradicts the
brain's functionally hierarchical. We introduce BrainMCLIP, which pioneers a
parameter-efficient, multi-layer fusion approach guided by human visual
system's functional hierarchy, eliminating the need for such a separate VAE
pathway. BrainMCLIP aligns fMRI signals from functionally distinct visual areas
(low-/high-level) to corresponding intermediate and final CLIP layers,
respecting functional hierarchy. We further introduce a Cross-Reconstruction
strategy and a novel multi-granularity loss. Results show BrainMCLIP achieves
highly competitive performance, particularly excelling on high-level semantic
metrics where it matches or surpasses SOTA(state-of-the-art) methods, including
those using VAE pipelines. Crucially, it achieves this with substantially fewer
parameters, demonstrating a reduction of
71.7\%(Table.\ref{tab:compare_clip_vae}) compared to top VAE-based SOTA
methods, by avoiding the VAE pathway. By leveraging intermediate CLIP features,
it effectively captures visual details often missed by CLIP-only approaches,
striking a compelling balance between semantic accuracy and detail fidelity
without requiring a separate VAE pipeline.


### [16] [A Training-Free Framework for Open-Vocabulary Image Segmentation and Recognition with EfficientNet and CLIP](https://arxiv.org/abs/2510.19333)
*Ying Dai, Wei Yu Chen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¼€é›†è¯æ±‡å›¾åƒåˆ†å‰²ä¸è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆEfficientNetB0çš„æ— ç›‘ç£åˆ†å‰²å’ŒCLIPçš„è§†è§‰è¯­è¨€å¯¹é½ï¼Œå®ç°äº†å¼€æ”¾è¯æ±‡çš„è¯­ä¹‰åˆ†å‰²å’Œç›®æ ‡è¯†åˆ«ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å›¾åƒåˆ†å‰²å’Œè¯†åˆ«æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®è¿›è¡Œç›‘ç£è®­ç»ƒï¼Œé™åˆ¶äº†åœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸‹çš„åº”ç”¨ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§æ— éœ€è®­ç»ƒå³å¯å¤„ç†ä»»æ„ç±»åˆ«è¯æ±‡çš„å›¾åƒåˆ†å‰²ä¸è¯†åˆ«æ¡†æ¶ï¼Œè§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨å¼€æ”¾é›†è¯†åˆ«ä¸­çš„å±€é™æ€§ã€‚

**Method:** è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆä½¿ç”¨EfficientNetB0æå–åƒç´ çº§ç‰¹å¾ï¼Œé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£è·å¾—æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶åŸºäºå¥‡å¼‚å€¼åˆ†å¸ƒè‡ªé€‚åº”ç¡®å®šèšç±»æ•°é‡è¿›è¡Œå±‚æ¬¡èšç±»åˆ†å‰²ï¼›ç„¶ååˆ©ç”¨CLIPçš„ViTéª¨å¹²ç½‘ç»œå¯¹åˆ†å‰²åŒºåŸŸè¿›è¡Œç¼–ç ï¼Œä¸é¢„è®¡ç®—çš„æ–‡æœ¬åµŒå…¥åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè·¨æ¨¡æ€å¯¹é½ï¼Œé€šè¿‡ç›¸ä¼¼åº¦è®¡ç®—å®ç°è¯†åˆ«ã€‚

**Result:** åœ¨COCOã€ADE20Kå’ŒPASCAL VOCç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨åŒˆç‰™åˆ©mIoUã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°ç­‰æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šçš„è¯†åˆ«èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†æ— éœ€è®­ç»ƒå³å¯å®ç°é«˜æ•ˆå¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²ä¸è¯†åˆ«çš„å¯è¡Œæ€§ï¼Œæå‡ºçš„æ¡†æ¶å…·æœ‰å‡ºè‰²çš„çµæ´»æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œå®é™…åº”ç”¨ä»·å€¼ï¼Œä¸ºæ— ç›‘ç£è§†è§‰ç†è§£ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
This paper presents a novel training-free framework for open-vocabulary image
segmentation and object recognition (OVSR), which leverages EfficientNetB0, a
convolutional neural network, for unsupervised segmentation and CLIP, a
vision-language model, for open-vocabulary object recognition. The proposed
framework adopts a two stage pipeline: unsupervised image segmentation followed
by segment-level recognition via vision-language alignment. In the first stage,
pixel-wise features extracted from EfficientNetB0 are decomposed using singular
value decomposition to obtain latent representations, which are then clustered
using hierarchical clustering to segment semantically meaningful regions. The
number of clusters is adaptively determined by the distribution of singular
values. In the second stage, the segmented regions are localized and encoded
into image embeddings using the Vision Transformer backbone of CLIP. Text
embeddings are precomputed using CLIP's text encoder from category-specific
prompts, including a generic something else prompt to support open set
recognition. The image and text embeddings are concatenated and projected into
a shared latent feature space via SVD to enhance cross-modal alignment.
Recognition is performed by computing the softmax over the similarities between
the projected image and text embeddings. The proposed method is evaluated on
standard benchmarks, including COCO, ADE20K, and PASCAL VOC, achieving
state-of-the-art performance in terms of Hungarian mIoU, precision, recall, and
F1-score. These results demonstrate the effectiveness, flexibility, and
generalizability of the proposed framework.


### [17] [XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography](https://arxiv.org/abs/2510.19599)
*Haozhe Luo, Shelley Zixin Shu, Ziyu Zhou, Sebastian Otalora, Mauricio Reyes*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†é¦–ä¸ªç³»ç»Ÿæ€§åŸºå‡†XBenchï¼Œç”¨äºè¯„ä¼°ä¸ƒç§CLIPé£æ ¼è§†è§‰è¯­è¨€æ¨¡å‹åœ¨èƒ¸éƒ¨Xå…‰ç‰‡ä¸­çš„è·¨æ¨¡æ€å¯è§£é‡Šæ€§ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ä¸´åºŠå¯é å®šä½èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œå¼ºè°ƒäº†åŒ»ç–—å®è·µä¸­éƒ¨ç½²å‰è¿›è¡Œé’ˆå¯¹æ€§å¯è§£é‡Šæ€§åŸºå‡†æµ‹è¯•çš„å¿…è¦æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç†è§£ä¸­å±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œä½†å…¶å®šä½èƒ½åŠ›ï¼ˆæ–‡æœ¬æ¦‚å¿µä¸è§†è§‰è¯æ®çš„å¯¹é½ç¨‹åº¦ï¼‰åœ¨åŒ»å­¦é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè€Œå¯é çš„å®šä½å¯¹äºæ¨¡å‹å¯è§£é‡Šæ€§å’Œä¸´åºŠé‡‡ç”¨è‡³å…³é‡è¦ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨äº¤å‰æ³¨æ„åŠ›å’Œç›¸ä¼¼æ€§å®šä½å›¾ç”Ÿæˆè§†è§‰è§£é‡Šï¼Œå¹¶å®šé‡è¯„ä¼°å…¶ä¸æ”¾å°„ç§‘åŒ»ç”Ÿæ ‡æ³¨çš„å¤šä¸ªç—…ç†åŒºåŸŸçš„å¯¹é½ç¨‹åº¦ï¼Œç³»ç»Ÿæ€§åœ°æ¯”è¾ƒäº†ä¸ƒç§CLIPé£æ ¼è§†è§‰è¯­è¨€æ¨¡å‹å˜ä½“åœ¨èƒ¸éƒ¨Xå…‰ç‰‡ä¸Šçš„è¡¨ç°ã€‚

**Result:** åˆ†æå‘ç°ï¼šæ‰€æœ‰VLMå˜ä½“å¯¹å¤§å‹æ˜ç¡®ç—…ç†çš„å®šä½è¡¨ç°åˆç†ï¼Œä½†å¯¹å°å‹æˆ–å¼¥æ•£æ€§ç—…å˜çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼›åœ¨èƒ¸éƒ¨Xå…‰ç‰¹å®šæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”é€šç”¨é¢†åŸŸæ•°æ®è®­ç»ƒçš„æ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„å¯¹é½æ€§ï¼›æ¨¡å‹çš„æ•´ä½“è¯†åˆ«èƒ½åŠ›ä¸å®šä½èƒ½åŠ›å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚

**Conclusion:** ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹å…·æœ‰è¾ƒå¼ºçš„è¯†åˆ«èƒ½åŠ›ï¼Œä½†åœ¨ä¸´åºŠå¯é å®šä½æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œè¿™å‡¸æ˜¾äº†åœ¨åŒ»ç–—å®è·µä¸­éƒ¨ç½²å‰è¿›è¡Œé’ˆå¯¹æ€§å¯è§£é‡Šæ€§åŸºå‡†æµ‹è¯•çš„å¿…è¦æ€§ï¼Œä¸ºæœªæ¥åŒ»ç–—AIç³»ç»Ÿçš„å®‰å…¨éƒ¨ç½²æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLMs) have recently shown remarkable zero-shot
performance in medical image understanding, yet their grounding ability, the
extent to which textual concepts align with visual evidence, remains
underexplored. In the medical domain, however, reliable grounding is essential
for interpretability and clinical adoption. In this work, we present the first
systematic benchmark for evaluating cross-modal interpretability in chest
X-rays across seven CLIP-style VLM variants. We generate visual explanations
using cross-attention and similarity-based localization maps, and
quantitatively assess their alignment with radiologist-annotated regions across
multiple pathologies. Our analysis reveals that: (1) while all VLM variants
demonstrate reasonable localization for large and well-defined pathologies,
their performance substantially degrades for small or diffuse lesions; (2)
models that are pretrained on chest X-ray-specific datasets exhibit improved
alignment compared to those trained on general-domain data. (3) The overall
recognition ability and grounding ability of the model are strongly correlated.
These findings underscore that current VLMs, despite their strong recognition
ability, still fall short in clinically reliable grounding, highlighting the
need for targeted interpretability benchmarks before deployment in medical
practice. XBench code is available at
https://github.com/Roypic/Benchmarkingattention


### [18] [DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents](https://arxiv.org/abs/2510.19336)
*Kai Shi, Jun Yang, Ni Yang, Binqiang Pan, Qingsong Xie, Chao Zhang, Zhenyu Yang, Tianhuang Su, Haonan Lu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†DaMoï¼ˆæ•°æ®æ··åˆä¼˜åŒ–å™¨ï¼‰ï¼Œä¸€ç§é€šè¿‡å¯è®­ç»ƒç½‘ç»œé¢„æµ‹æœ€ä¼˜æ•°æ®æ··åˆæ¯”ä¾‹æ¥ä¼˜åŒ–å¤šä»»åŠ¡å­¦ä¹ çš„æ–¹æ³•ï¼Œå¹¶åœ¨ç§»åŠ¨æ‰‹æœºä»£ç†ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥æ–¹æ³•åœ¨PhoneAgentBenchåŸºå‡†æµ‹è¯•ä¸­æ¯”æ›¿ä»£æ–¹æ³•æå‡äº†3.38%çš„æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç§»åŠ¨æ‰‹æœºä»£ç†ï¼ˆMPAsï¼‰ä½œä¸ºå¤šæ¨¡æ€åº”ç”¨çš„é‡è¦ç ”ç©¶æ–¹å‘ï¼Œè™½ç„¶åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ„å»ºï¼Œä½†åœ¨åŒæ—¶å¤„ç†å¤šä¸ªç§»åŠ¨æ‰‹æœºä»»åŠ¡æ—¶æ•ˆæœæœ‰é™ã€‚ç°æœ‰çš„å¤šä»»åŠ¡ç›‘ç£å¾®è°ƒæ–¹æ³•éš¾ä»¥ç¡®å®šæœ€ä¼˜çš„è®­ç»ƒæ•°æ®ç»„åˆä»¥è¾¾åˆ°å³°å€¼æ€§èƒ½ï¼Œè¿™æˆä¸ºåˆ¶çº¦MPAså‘å±•çš„å…³é”®ç“¶é¢ˆã€‚

**Method:** æœ¬æ–‡æå‡ºDaMoæ–¹æ³•ï¼Œé‡‡ç”¨å¯è®­ç»ƒç½‘ç»œé¢„æµ‹æœ€ä¼˜æ•°æ®æ··åˆæ¯”ä¾‹ï¼Œé€šè¿‡é¢„æµ‹ä»»ä½•ç»™å®šæ•°æ®é›†æ¯”ä¾‹ä¸‹çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ¥ç¡®å®šæœ€ä½³é…ç½®ã€‚åŒæ—¶å¼•å…¥äº†PhoneAgentBenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1235ä¸ªQAå¯¹ï¼Œè¦†ç›–å¤šæ ·åŒ–çš„ç°å®å·¥ä¸šç§»åŠ¨åº”ç”¨åœºæ™¯ï¼Œä¸ºå…¨é¢è¯„ä¼°æä¾›æ”¯æŒã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºDaMoåœ¨å°è§„æ¨¡è¯•ç‚¹å®éªŒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é¢„æµ‹èƒ½åŠ›ï¼ˆRÂ²=0.81ï¼‰ï¼Œåœ¨PhoneAgentBenchä¸Šæ¯”æ›¿ä»£æ–¹æ³•æ€§èƒ½æå‡3.38%ã€‚åœ¨BFCL-v3ã€MME-Reasoningã€MME-Perceptionå’ŒOCRBenchç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDaMoå¹³å‡å¾—åˆ†æ¯”å…¶ä»–æ–¹æ³•é«˜2.57%ï¼Œåœ¨BFCL-v3ä»»åŠ¡ä¸Šå•ç‹¬ä¼˜åŒ–MLLMæ—¶æŒ‡æ ‡æå‡12.47%ã€‚

**Conclusion:** DaMoæ–¹æ³•å±•ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’çš„å¯æ‰©å±•æ€§ï¼Œåœ¨å…¶ä»–æ¨¡å‹æ¶æ„ä¸Šä»èƒ½ä¿æŒæœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶ä¸ºå¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æ•°æ®æ··åˆä¼˜åŒ–æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†ç§»åŠ¨æ‰‹æœºä»£ç†æŠ€æœ¯çš„å‘å±•ï¼Œç›¸å…³ä»£ç å’Œæ•°æ®é›†å·²å¼€æºä¾›ç¤¾åŒºä½¿ç”¨ã€‚

---

#### ğŸ“„ Abstract
Mobile Phone Agents (MPAs) have emerged as a promising research direction due
to their broad applicability across diverse scenarios. While Multimodal Large
Language Models (MLLMs) serve as the foundation for MPAs, their effectiveness
in handling multiple mobile phone tasks simultaneously remains limited.
Although multitask supervised fine-tuning (SFT) is widely adopted for multitask
learning, existing approaches struggle to determine optimal training data
compositions for peak performance. To address this challenge, we propose DaMo
(Data Mixture Optimizer) - a novel solution employing a trainable network that
predicts optimal data mixtures by forecasting downstream task performance for
any given dataset ratio. To support comprehensive evaluation, we introduce
PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on
multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse
real-world industrial mobile application scenarios. Demonstrating strong
predictive capability (R^2=0.81) in small-scale pilot experiments, DaMo
efficiently extrapolates optimal data mixing configurations. Our results show
DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to
alternative methods. Furthermore, extensive experiments across established
benchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench
reveal DaMo's superior generalization, outperforming other approaches by 2.57%
in terms of average score. When used solely for MLLM optimization on the
BFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,
DaMo maintains robust scalability, preserving its effectiveness when applied to
other model architectures. The code and dataset are available at
https://github.com/OPPO-Mente-Lab/DaMo.git


### [19] [Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes](https://arxiv.org/abs/2510.19400)
*Zhiyuan Feng, Zhaolu Kang, Qijie Wang, Zhiying Du, Jiongrui Yan, Shubin Shi, Chengbo Yuan, Huizhi Liang, Yu Deng, Qixiu Li, Rushuai Yang, Arctanx An, Leqi Zheng, Weijie Wang, Shawn Chen, Sicheng Xu, Yaobo Liang, Jiaolong Yang, Baining Guo*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MV-RoboBenchåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè§†è§’æœºå™¨äººæ“ä½œä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå‘ç°å½“å‰æœ€å…ˆè¿›æ¨¡å‹ä¸äººç±»æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¹¶æ­ç¤ºäº†å¤šè§†è§’ç©ºé—´æ™ºèƒ½ä¸æœºå™¨äººä»»åŠ¡æ‰§è¡Œä¹‹é—´çš„æ­£ç›¸å…³æ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ä¸»è¦é›†ä¸­äºå•è§†è§’è®¾ç½®ï¼Œè€Œå¿½ç•¥äº†å…¶åœ¨å¤šè§†è§’ä¿¡æ¯æ•´åˆæ–¹é¢çš„èƒ½åŠ›ï¼ŒåŒæ—¶å¤šæ‘„åƒå¤´é…ç½®åœ¨æœºå™¨äººå¹³å°ä¸­æ—¥ç›Šæ™®åŠï¼Œä½†è§†è§‰è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½æœ‰æ•ˆåˆ©ç”¨å¤šè§†è§’è¾“å…¥è¿›è¡Œæœºå™¨äººæ¨ç†ä»æ˜¯ä¸€ä¸ªæœªè§£å†³çš„é—®é¢˜ã€‚

**Method:** ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†MV-RoboBenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1.7kä¸ªæ‰‹åŠ¨ç­–åˆ’çš„é—®ç­”é¡¹ç›®ï¼Œæ¶µç›–å…«ä¸ªå­ä»»åŠ¡ï¼Œåˆ†ä¸ºç©ºé—´ç†è§£å’Œæœºå™¨äººæ‰§è¡Œä¸¤å¤§ç±»åˆ«ï¼Œå¹¶è¯„ä¼°äº†åŒ…æ‹¬å¼€æºå’Œé—­æºæ¨¡å‹åœ¨å†…çš„å¤šç§ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠé‡‡ç”¨æ€ç»´é“¾å¯å‘æŠ€æœ¯çš„å¢å¼ºç‰ˆæœ¬ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½è¿œä½äºäººç±»æ°´å¹³ï¼ŒåŒæ—¶å‘ç°ä¸¤ä¸ªå…³é”®å‘ç°ï¼šå¤šè§†è§’æœºå™¨äººåœºæ™¯ä¸­ç©ºé—´æ™ºèƒ½ä¸æœºå™¨äººä»»åŠ¡æ‰§è¡Œå‘ˆæ­£ç›¸å…³ï¼Œä»¥åŠåœ¨ç°æœ‰é€šç”¨å•è§†è§’ç©ºé—´ç†è§£åŸºå‡†ä¸Šçš„å¼ºæ€§èƒ½å¹¶ä¸èƒ½å¯é åœ°è½¬åŒ–ä¸ºæœºå™¨äººç©ºé—´ä»»åŠ¡çš„æˆåŠŸã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè§†è§’æœºå™¨äººæ„ŸçŸ¥æ–¹é¢é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ï¼ŒMV-RoboBenchä½œä¸ºå¼€æ”¾èµ„æºå‘å¸ƒï¼Œæ—¨åœ¨ä¿ƒè¿›ç©ºé—´åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„è¿›å±•ï¼Œä¸ä»…æä¾›æ•°æ®è¿˜æä¾›äº†å¤šè§†è§’å…·èº«æ¨ç†çš„æ ‡å‡†åŒ–è¯„ä¼°åè®®ã€‚

---

#### ğŸ“„ Abstract
Vision-language models (VLMs) are essential to Embodied AI, enabling robots
to perceive, reason, and act in complex environments. They also serve as the
foundation for the recent Vision-Language-Action (VLA) models. Yet most
evaluations of VLMs focus on single-view settings, leaving their ability to
integrate multi-view information underexplored. At the same time, multi-camera
setups are increasingly standard in robotic platforms, as they provide
complementary perspectives to mitigate occlusion and depth ambiguity. Whether
VLMs can effectively leverage such multi-view inputs for robotic reasoning
therefore remains an open question. To bridge this gap, we introduce
MV-RoboBench, a benchmark specifically designed to evaluate the multi-view
spatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBench
consists of 1.7k manually curated QA items across eight subtasks, divided into
two primary categories: spatial understanding and robotic execution. We
evaluate a diverse set of existing VLMs, including both open-source and
closed-source models, along with enhanced versions incorporating CoT-inspired
techniques. The results show that state-of-the-art models remain far below
human performance, underscoring the substantial challenges VLMs face in
multi-view robotic perception. Additionally, our analysis uncovers two key
findings: (i) spatial intelligence and robotic task execution are positively
correlated in multi-view robotic scenarios; and (ii) strong performance on
existing general-purpose single-view spatial understanding benchmarks does not
reliably translate to success in the robotic spatial tasks assessed by our
benchmark. We release MV-RoboBench as an open resource to foster progress in
spatially grounded VLMs and VLAs, providing not only data but also a
standardized evaluation protocol for multi-view embodied reasoning.


### [20] [From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction](https://arxiv.org/abs/2510.19654)
*Zhida Zhao, Talas Fu, Yifan Wang, Lijun Wang, Huchuan Lu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç­–ç•¥ä¸–ç•Œæ¨¡å‹ï¼ˆPWMï¼‰çš„æ–°å‹é©¾é©¶èŒƒå¼ï¼Œé€šè¿‡ç»Ÿä¸€ä¸–ç•Œå»ºæ¨¡ä¸è½¨è¿¹è§„åˆ’æ¶æ„ï¼Œå¹¶åˆ©ç”¨æ— åŠ¨ä½œæœªæ¥çŠ¶æ€é¢„æµ‹æ–¹æ¡ˆä½¿è§„åˆ’å—ç›Šäºå­¦ä¹ çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œå®ç°äº†ç±»äººçš„é¢„æœŸæ„ŸçŸ¥èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰é©¾é©¶ä¸–ç•Œæ¨¡å‹ä¸»è¦ä¸“æ³¨äºä¸–ç•Œæ¨¡æ‹Ÿå¹¶ä¸è½¨è¿¹è§„åˆ’è§£è€¦ï¼Œå°½ç®¡è¿‘æœŸç ”ç©¶å°è¯•ç»Ÿä¸€ä¸–ç•Œå»ºæ¨¡ä¸è§„åˆ’ï¼Œä½†ä¸–ç•Œå»ºæ¨¡å¯¹è§„åˆ’çš„ååŒä¿ƒè¿›æœºåˆ¶ä»éœ€æ·±å…¥æ¢ç´¢ï¼Œç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨å­¦ä¹ çš„ä¸–ç•ŒçŸ¥è¯†æ¥å¢å¼ºè§„åˆ’æ€§èƒ½ã€‚

**Method:** PWMé‡‡ç”¨ç»Ÿä¸€æ¶æ„æ•´åˆä¸–ç•Œå»ºæ¨¡ä¸è½¨è¿¹è§„åˆ’ï¼Œæå‡ºæ— åŠ¨ä½œæœªæ¥çŠ¶æ€é¢„æµ‹æ–¹æ¡ˆä½¿è§„åˆ’å—ç›Šäºå­¦ä¹ çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œé€šè¿‡åä½œçŠ¶æ€-åŠ¨ä½œé¢„æµ‹å®ç°ç±»äººé¢„æœŸæ„ŸçŸ¥ï¼Œå¹¶å¼•å…¥åŠ¨æ€å¢å¼ºå¹¶è¡Œä»¤ç‰Œç”Ÿæˆæœºåˆ¶ï¼Œé…å¤‡ä¸Šä¸‹æ–‡å¼•å¯¼åˆ†è¯å™¨å’Œè‡ªé€‚åº”åŠ¨æ€ç„¦ç‚¹æŸå¤±ä»¥æé«˜è§†é¢‘é¢„æµ‹æ•ˆç‡ã€‚

**Result:** å°½ç®¡ä»…ä½¿ç”¨å‰è§†æ‘„åƒå¤´è¾“å…¥ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸ŠåŒ¹é…æˆ–è¶…è¶Šäº†ä¾èµ–å¤šè§†è§’å’Œå¤šæ¨¡æ€è¾“å…¥çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶è§„åˆ’å¯é æ€§å’Œé¢„æµ‹æ•ˆç‡çš„æ˜¾è‘—æå‡ã€‚

**Conclusion:** è¯¥ç ”ç©¶å±•ç¤ºäº†ä¸–ç•Œå»ºæ¨¡ä¸è§„åˆ’ç»Ÿä¸€æ¶æ„çš„æ½œåŠ›ï¼Œé€šè¿‡ååŒçŠ¶æ€-åŠ¨ä½œé¢„æµ‹æœºåˆ¶å®ç°äº†æ›´å¯é çš„è§„åˆ’æ€§èƒ½ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†æ–°çš„èŒƒå¼ï¼Œæœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢å¤šæ¨¡æ€è¾“å…¥çš„æ‰©å±•åº”ç”¨ã€‚

---

#### ğŸ“„ Abstract
Despite remarkable progress in driving world models, their potential for
autonomous systems remains largely untapped: the world models are mostly
learned for world simulation and decoupled from trajectory planning. While
recent efforts aim to unify world modeling and planning in a single framework,
the synergistic facilitation mechanism of world modeling for planning still
requires further exploration. In this work, we introduce a new driving paradigm
named Policy World Model (PWM), which not only integrates world modeling and
trajectory planning within a unified architecture, but is also able to benefit
planning using the learned world knowledge through the proposed action-free
future state forecasting scheme. Through collaborative state-action prediction,
PWM can mimic the human-like anticipatory perception, yielding more reliable
planning performance. To facilitate the efficiency of video forecasting, we
further introduce a dynamically enhanced parallel token generation mechanism,
equipped with a context-guided tokenizer and an adaptive dynamic focal loss.
Despite utilizing only front camera input, our method matches or exceeds
state-of-the-art approaches that rely on multi-view and multi-modal inputs.
Code and model weights will be released at
https://github.com/6550Zhao/Policy-World-Model.


### [21] [Reasoning Like Experts: Leveraging Multimodal Large Language Models for Drawing-based Psychoanalysis](https://arxiv.org/abs/2510.19451)
*Xueqi Ma, Yanbei Jiang, Sarah Erfani, James Bailey, Weifeng Liu, Krista A. Ehinger, Jey Han Lau*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†PICKæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®ç°å¿ƒç†å›¾åƒç†è§£ï¼Œä¸“é—¨é’ˆå¯¹æˆ¿æ ‘äººæµ‹è¯•è¿›è¡Œåˆ†å±‚åˆ†æå’ŒçŸ¥è¯†æ³¨å…¥ï¼Œæ˜¾è‘—æå‡äº†MLLMsåœ¨å¿ƒç†åˆ†æé¢†åŸŸçš„èƒ½åŠ›ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å®¢è§‚æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸»è§‚æƒ…æ„Ÿåˆ†æé¢†åŸŸç‰¹åˆ«æ˜¯å¿ƒç†åˆ†æåº”ç”¨æ–¹é¢ä»æœªè¢«å……åˆ†æ¢ç´¢ï¼Œå­˜åœ¨ä¸“ä¸šé¢†åŸŸçŸ¥è¯†èåˆçš„ç©ºç™½ã€‚

**Method:** æå‡ºPICKå¤šæ­¥éª¤æ¡†æ¶ï¼ŒåŒ…æ‹¬å°†å¤æ‚ç»˜å›¾åˆ†è§£ä¸ºè¯­ä¹‰å­å›¾æ„å»ºå±‚æ¬¡è¡¨ç¤ºï¼Œé’ˆå¯¹å•å¯¹è±¡ã€å¤šå¯¹è±¡å’Œæ•´ä½“ä¸‰ä¸ªå±‚æ¬¡è¿›è¡Œé’ˆå¯¹æ€§åˆ†æï¼Œå¼•å…¥HTPçŸ¥è¯†åº“å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„ç‰¹å¾æå–æ¨¡å—ç”Ÿæˆå¿ƒç†ç‰¹å¾ç”»åƒã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜PICKæ¡†æ¶æ˜¾è‘—å¢å¼ºäº†MLLMsåœ¨å¿ƒç†åˆ†ææ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶åœ¨æƒ…æ„Ÿç†è§£ä»»åŠ¡ä¸­éªŒè¯äº†å…¶ä½œä¸ºé€šç”¨æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼¥åˆäº†MLLMsä¸ä¸“ä¸šé¢†åŸŸä¹‹é—´çš„é¸¿æ²Ÿï¼Œæä¾›äº†é€šè¿‡è§†è§‰è¡¨è¾¾ç†è§£äººç±»å¿ƒç†çŠ¶æ€çš„ç»“æ„åŒ–å¯è§£é‡Šæ¡†æ¶ï¼Œä¸ºä¸“ä¸šé¢†åŸŸåº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Language Models (MLLMs) have demonstrated exceptional
performance across various objective multimodal perception tasks, yet their
application to subjective, emotionally nuanced domains, such as psychological
analysis, remains largely unexplored. In this paper, we introduce PICK, a
multi-step framework designed for Psychoanalytical Image Comprehension through
hierarchical analysis and Knowledge injection with MLLMs, specifically focusing
on the House-Tree-Person (HTP) Test, a widely used psychological assessment in
clinical practice. First, we decompose drawings containing multiple instances
into semantically meaningful sub-drawings, constructing a hierarchical
representation that captures spatial structure and content across three levels:
single-object level, multi-object level, and whole level. Next, we analyze
these sub-drawings at each level with a targeted focus, extracting
psychological or emotional insights from their visual cues. We also introduce
an HTP knowledge base and design a feature extraction module, trained with
reinforcement learning, to generate a psychological profile for single-object
level analysis. This profile captures both holistic stylistic features and
dynamic object-specific features (such as those of the house, tree, or person),
correlating them with psychological states. Finally, we integrate these
multi-faceted information to produce a well-informed assessment that aligns
with expert-level reasoning. Our approach bridges the gap between MLLMs and
specialized expert domains, offering a structured and interpretable framework
for understanding human mental states through visual expression. Experimental
results demonstrate that the proposed PICK significantly enhances the
capability of MLLMs in psychological analysis. It is further validated as a
general framework through extensions to emotion understanding tasks.


### [22] [I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs](https://arxiv.org/abs/2510.19678)
*John Burden, Jonathan Prunty, Ben Slater, Matthieu Tehenan, Greg Davis, Lucy Cheke*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶å°†è®¤çŸ¥å¿ƒç†å­¦ä¸­çš„è§†è§‰æœç´¢èŒƒå¼åº”ç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å…ˆè¿›MLLMsåœ¨é¢œè‰²å’Œå°ºå¯¸ç‰¹å¾æœç´¢ä¸­è¡¨ç°å‡ºç±»ä¼¼äººç±»çš„'è·³å‡ºæ•ˆåº”'ï¼Œå¹¶æ­ç¤ºäº†å…¶åœ¨è”åˆæœç´¢ä¸­çš„å®¹é‡é™åˆ¶ï¼Œä¸ºè¯„ä¼°MLLMsçš„æ„ŸçŸ¥èƒ½åŠ›æä¾›äº†è®¤çŸ¥åŸºç¡€è¯Šæ–­å·¥å…·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å…¶è§†è§‰å¤„ç†æœºåˆ¶ä»ä¸é€æ˜ï¼Œå¤§å¤šæ•°é»‘ç›’è¯„ä¼°ä»…å…³æ³¨ä»»åŠ¡å‡†ç¡®æ€§è€Œæ— æ³•æ­ç¤ºåº•å±‚æœºåˆ¶ï¼Œç ”ç©¶æ—¨åœ¨é€šè¿‡è®¤çŸ¥å¿ƒç†å­¦æ–¹æ³•å¡«è¡¥è¿™ä¸€ç†è§£ç©ºç™½ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨ç»å…¸çš„è§†è§‰æœç´¢èŒƒå¼ï¼Œé€šè¿‡æ§åˆ¶å®éªŒæµ‹è¯•é¢œè‰²ã€å°ºå¯¸å’Œå…‰ç…§ç‰¹å¾ï¼Œè¯„ä¼°MLLMsæ˜¯å¦è¡¨ç°å‡º'è·³å‡ºæ•ˆåº”'ï¼Œå¹¶åˆ©ç”¨é’ˆå¯¹æ€§å¾®è°ƒå’Œæœºåˆ¶å¯è§£é‡Šæ€§åˆ†ææ¥éªŒè¯å‘ç°ã€‚

**Result:** å®éªŒå‘ç°å…ˆè¿›MLLMsåœ¨é¢œè‰²æˆ–å°ºå¯¸çš„å•ç‰¹å¾æœç´¢ä¸­è¡¨ç°å‡ºç±»ä¼¼äººç±»çš„è·³å‡ºæ•ˆåº”ï¼Œåœ¨è”åˆç‰¹å¾æœç´¢ä¸­å­˜åœ¨å®¹é‡é™åˆ¶ï¼Œå¹¶ä¸”è¯æ®è¡¨æ˜MLLMsåƒäººç±»ä¸€æ ·å°†è‡ªç„¶åœºæ™¯å…ˆéªŒå¦‚å…‰ç…§æ–¹å‘æ•´åˆåˆ°å¯¹è±¡è¡¨å¾ä¸­ã€‚

**Conclusion:** è§†è§‰æœç´¢å¯ä½œä¸ºè¯„ä¼°MLLMsæ„ŸçŸ¥èƒ½åŠ›çš„è®¤çŸ¥åŸºç¡€è¯Šæ–­å·¥å…·ï¼Œç ”ç©¶æ­ç¤ºäº†MLLMsä¸äººç±»è§†è§‰å¤„ç†çš„ç›¸ä¼¼æ€§ï¼Œä¸ºç†è§£æ¨¡å‹å†…éƒ¨æœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œå¹¶å¼ºè°ƒäº†è®¤çŸ¥å¿ƒç†å­¦æ–¹æ³•åœ¨AIè¯„ä¼°ä¸­çš„ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Multimodal large language models (MLLMs) achieve strong performance on
vision-language tasks, yet their visual processing is opaque. Most black-box
evaluations measure task accuracy, but reveal little about underlying
mechanisms. Drawing on cognitive psychology, we adapt classic visual search
paradigms -- originally developed to study human perception -- to test whether
MLLMs exhibit the ``pop-out'' effect, where salient visual features are
detected independently of distractor set size. Using controlled experiments
targeting colour, size and lighting features, we find that advanced MLLMs
exhibit human-like pop-out effects in colour or size-based disjunctive (single
feature) search, as well as capacity limits for conjunctive (multiple feature)
search. We also find evidence to suggest that MLLMs, like humans, incorporate
natural scene priors such as lighting direction into object representations. We
reinforce our findings using targeted fine-tuning and mechanistic
interpretability analyses. Our work shows how visual search can serve as a
cognitively grounded diagnostic tool for evaluating perceptual capabilities in
MLLMs.


### [23] [[De|Re]constructing VLMs' Reasoning in Counting](https://arxiv.org/abs/2510.19555)
*Simone Alghisi, Gabriel Roccabruna, Massimo Rizzoli, Seyed Mahed Mousavi, Giuseppe Riccardi*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿåˆ†æä¸ƒç§å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è®¡æ•°ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå‘ç°æ¨¡å‹å¯¹ç‰©ä½“æ•°é‡ã€ç©ºé—´æ’åˆ—å’Œå¹²æ‰°ç‰©é«˜åº¦æ•æ„Ÿï¼Œå¹¶æå‡ºä»…å¾®è°ƒè¾“å‡ºå±‚å³å¯æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½21%çš„é’ˆå¯¹æ€§è®­ç»ƒæ–¹æ³•ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§†è§‰æ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«ç‰©ä½“å…³ç³»ã€ç†è§£æ—¶é—´åºåˆ—å’Œè®¡æ•°ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¶…è¶ŠåŸºå‡†è¯„ä¼°ï¼Œæ·±å…¥æ¢ç©¶æ¨¡å‹å¤±è´¥çš„åº•å±‚åŸå› å¹¶æå‡ºé’ˆå¯¹æ€§æ”¹è¿›æ–¹æ³•ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨å—æ§å®éªŒæ¡ä»¶ç³»ç»Ÿè¯„ä¼°ä¸ƒç§å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹çš„è®¡æ•°æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡å±‚é—´åˆ†æè¯†åˆ«é”™è¯¯æ¥æºï¼Œå¹¶è®¾è®¡é’ˆå¯¹æ€§è®­ç»ƒç­–ç•¥ä»…å¾®è°ƒæ¨¡å‹çš„è¾“å‡ºå±‚å‚æ•°ã€‚

**Result:** å®éªŒè¡¨æ˜è§†è§‰è¯­è¨€æ¨¡å‹å¯¹ç‰©ä½“æ•°é‡ã€ç±»å‹ã€ç©ºé—´æ’åˆ—å’Œå¹²æ‰°ç‰©å…±ç°é«˜åº¦æ•æ„Ÿï¼Œå±‚é—´åˆ†ææ­ç¤ºé”™è¯¯ä¸»è¦æºäºæœ€åä¸€å±‚è¡¨ç¤ºåˆ°è¾“å‡ºç©ºé—´çš„æ˜ å°„é—®é¢˜ã€‚ä»…å¾®è°ƒè¾“å‡ºå±‚å³å¯å°†å‡†ç¡®ç‡æå‡é«˜è¾¾21%ï¼Œå¹¶åœ¨çœŸå®æ•°æ®é›†ä¸Šè·å¾—ä¸€è‡´æ”¹è¿›ã€‚

**Conclusion:** è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†é”™è¯¯ä¸»è¦æºäºè¾“å‡ºæ˜ å°„è€Œéç‰¹å¾è¡¨ç¤ºé—®é¢˜ï¼Œé’ˆå¯¹æ€§å¾®è°ƒè¾“å‡ºå±‚æ˜¯é«˜æ•ˆæå‡æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆç­–ç•¥ï¼Œä¸ºæ”¹è¿›è§†è§‰æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Vision-Language Models (VLMs) have recently gained attention due to their
competitive performance on multiple downstream tasks, achieved by following
user-input instructions. However, VLMs still exhibit several limitations in
visual reasoning, such as difficulties in identifying relations (e.g., spatial,
temporal, and among objects), understanding temporal sequences (e.g., frames),
and counting objects. In this work, we go beyond score-level benchmark
evaluations of VLMs by investigating the underlying causes of their failures
and proposing a targeted approach to improve their reasoning capabilities. We
study the reasoning skills of seven state-of-the-art VLMs in the counting task
under controlled experimental conditions. Our experiments show that VLMs are
highly sensitive to the number and type of objects, their spatial arrangement,
and the co-occurrence of distractors. A layer-wise analysis reveals that errors
are due to incorrect mapping of the last-layer representation into the output
space. Our targeted training shows that fine-tuning just the output layer
improves accuracy by up to 21%. We corroborate these findings by achieving
consistent improvements on real-world datasets.


### [24] [The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models](https://arxiv.org/abs/2510.19557)
*Xiaofeng Zhang, Aaron Courville, Michal Drozdzal, Adriana Romero-Soriano*

#### ğŸ§© TL;DR
æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†æç¤ºå¤æ‚åº¦å¯¹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®æ•ˆç”¨çš„å½±å“ï¼Œå‘ç°å¢åŠ æç¤ºå¤æ‚åº¦ä¼šé™ä½æ¡ä»¶å¤šæ ·æ€§å’Œæç¤ºä¸€è‡´æ€§ï¼Œä½†èƒ½å‡å°‘åˆæˆä¸çœŸå®æ•°æ®é—´çš„åˆ†å¸ƒåç§»ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å…·æœ‰ç”Ÿæˆæ— é™åˆæˆæ•°æ®çš„æ½œåŠ›ï¼Œä½†æç¤ºå·¥ç¨‹ä½œä¸ºä¸è¿™äº›æ¨¡å‹äº¤äº’çš„ä¸»è¦æ–¹å¼ï¼Œå…¶å¤æ‚åº¦å¯¹åˆæˆæ•°æ®è´¨é‡ã€å¤šæ ·æ€§å’Œä¸€è‡´æ€§ç­‰å…³é”®æ•ˆç”¨ç»´åº¦çš„ç³»ç»Ÿæ€§å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚

**Method:** ç ”ç©¶é¦–å…ˆé€šè¿‡åˆæˆå®éªŒéªŒè¯æç¤ºå¤æ‚åº¦æ³›åŒ–çš„éš¾åº¦å¹¶è¿›è¡Œç†è®ºæ¨å¯¼ï¼Œç„¶åå¼•å…¥æ–°çš„è¯„ä¼°æ¡†æ¶æ¯”è¾ƒçœŸå®æ•°æ®ä¸åˆæˆæ•°æ®çš„æ•ˆç”¨ï¼Œåœ¨CC12Mã€ImageNet-1kå’ŒDCIç­‰å¤šä¸ªæ•°æ®é›†ä¸Šåˆ†ææç¤ºå¤æ‚åº¦çš„å½±å“ï¼Œå¹¶è¯„ä¼°ä¸åŒçš„æ¨ç†æ—¶å¹²é¢„æ–¹æ³•ã€‚

**Result:** åˆæˆå®éªŒè¡¨æ˜å‘æ›´ä¸€èˆ¬æ¡ä»¶çš„æ³›åŒ–æ¯”åå‘æ³›åŒ–æ›´å›°éš¾ï¼Œå¤§è§„æ¨¡å®è¯å®éªŒæ˜¾ç¤ºå¢åŠ æç¤ºå¤æ‚åº¦å¯¼è‡´æ¡ä»¶å¤šæ ·æ€§å’Œæç¤ºä¸€è‡´æ€§é™ä½ï¼Œä½†å‡å°‘äº†åˆæˆä¸çœŸå®æ•°æ®é—´çš„åˆ†å¸ƒåç§»ï¼Œå…¶ä¸­æç¤ºæ‰©å±•æ–¹æ³•é€šè¿‡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä½œä¸ºä¼¼ç„¶ä¼°è®¡å™¨ï¼Œåœ¨å›¾åƒå¤šæ ·æ€§å’Œç¾å­¦è´¨é‡ä¸Šè¡¨ç°æœ€ä½³ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†æç¤ºå¤æ‚åº¦ä¸åˆæˆæ•°æ®æ•ˆç”¨ä¹‹é—´çš„æƒè¡¡å…³ç³»ï¼Œå½“å‰æ¨ç†æ—¶å¹²é¢„æ–¹æ³•è™½èƒ½å¢å¼ºå¤šæ ·æ€§ä½†ä¼šåç¦»çœŸå®æ•°æ®åˆ†å¸ƒï¼Œæç¤ºæ‰©å±•æ–¹æ³•é€šè¿‡åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„ä¼¼ç„¶ä¼°è®¡èƒ½åŠ›å®ç°äº†æœ€ä¼˜æ€§èƒ½ï¼Œä¸ºä¼˜åŒ–æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åˆæˆæ•°æ®ç”Ÿæˆæä¾›äº†é‡è¦æŒ‡å¯¼ã€‚

---

#### ğŸ“„ Abstract
Text-to-image (T2I) models offer great potential for creating virtually
limitless synthetic data, a valuable resource compared to fixed and finite real
datasets. Previous works evaluate the utility of synthetic data from T2I models
on three key desiderata: quality, diversity, and consistency. While prompt
engineering is the primary means of interacting with T2I models, the systematic
impact of prompt complexity on these critical utility axes remains
underexplored. In this paper, we first conduct synthetic experiments to
motivate the difficulty of generalization w.r.t. prompt complexity and explain
the observed difficulty with theoretical derivations. Then, we introduce a new
evaluation framework that can compare the utility of real data and synthetic
data, and present a comprehensive analysis of how prompt complexity influences
the utility of synthetic data generated by commonly used T2I models. We conduct
our study across diverse datasets, including CC12M, ImageNet-1k, and DCI, and
evaluate different inference-time intervention methods. Our synthetic
experiments show that generalizing to more general conditions is harder than
the other way round, since the former needs an estimated likelihood that is not
learned by diffusion models. Our large-scale empirical experiments reveal that
increasing prompt complexity results in lower conditional diversity and prompt
consistency, while reducing the synthetic-to-real distribution shift, which
aligns with the synthetic experiments. Moreover, current inference-time
interventions can augment the diversity of the generations at the expense of
moving outside the support of real data. Among those interventions, prompt
expansion, by deliberately using a pre-trained language model as a likelihood
estimator, consistently achieves the highest performance in both image
diversity and aesthetics, even higher than that of real data.


### [25] [HAD: Hierarchical Asymmetric Distillation to Bridge Spatio-Temporal Gaps in Event-Based Object Tracking](https://arxiv.org/abs/2510.19560)
*Yao Deng, Xian Zhong, Wenxuan Liu, Zhaofei Yu, Jingling Yuan, Tiejun Huang*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåˆ†å±‚éå¯¹ç§°è’¸é¦ï¼ˆHADï¼‰çš„å¤šæ¨¡æ€çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡å’Œç¼“è§£RGBç›¸æœºä¸äº‹ä»¶ç›¸æœºä¹‹é—´çš„æ—¶ç©ºä¸å¯¹ç§°æ€§ï¼Œæœ‰æ•ˆæå‡äº†åœ¨é«˜é€Ÿè¿åŠ¨ã€é«˜åŠ¨æ€èŒƒå›´ç­‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹çš„ç›®æ ‡è·Ÿè¸ªæ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** RGBç›¸æœºå’Œäº‹ä»¶ç›¸æœºåœ¨æˆåƒæœºåˆ¶ä¸Šå­˜åœ¨æ ¹æœ¬å·®å¼‚ï¼Œå¯¼è‡´æ˜¾è‘—çš„æ—¶ç©ºä¸å¯¹ç§°æ€§ï¼Œé˜»ç¢äº†ä¸¤ç§æ¨¡æ€çš„æœ‰æ•ˆèåˆã€‚è¿™ç§ä¸å¯¹ç§°æ€§é™åˆ¶äº†åœ¨é«˜é€Ÿè¿åŠ¨ã€é«˜åŠ¨æ€èŒƒå›´ç¯å¢ƒå’ŒåŠ¨æ€èƒŒæ™¯å¹²æ‰°ç­‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹ç›®æ ‡è·Ÿè¸ªçš„æ€§èƒ½æå‡ã€‚

**Method:** æå‡ºçš„HADæ¡†æ¶é‡‡ç”¨åˆ†å±‚å¯¹é½ç­–ç•¥ï¼Œåœ¨ä¿æŒå­¦ç”Ÿç½‘ç»œè®¡ç®—æ•ˆç‡å’Œå‚æ•°ç´§å‡‘æ€§çš„åŒæ—¶æœ€å°åŒ–ä¿¡æ¯æŸå¤±ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šæ¨¡æ€çŸ¥è¯†è’¸é¦æ˜¾å¼å»ºæ¨¡å’Œç¼“è§£RGBä¸äº‹ä»¶ç›¸æœºä¹‹é—´çš„æ—¶ç©ºä¸å¯¹ç§°æ€§ã€‚

**Result:** å¤§é‡å®éªŒè¡¨æ˜HADåœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•ï¼Œå…¨é¢çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¯ä¸ªè®¾è®¡ç»„ä»¶çš„æœ‰æ•ˆæ€§å’Œå¿…è¦æ€§ã€‚è¯¥æ–¹æ³•åœ¨æŒ‘æˆ˜æ€§è·Ÿè¸ªåœºæ™¯ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡æ˜¾å¼å»ºæ¨¡å¤šæ¨¡æ€æ—¶ç©ºä¸å¯¹ç§°æ€§å¯ä»¥æœ‰æ•ˆæå‡ç›®æ ‡è·Ÿè¸ªæ€§èƒ½ï¼Œä¸ºå¤šæ¨¡æ€è§†è§‰ä»»åŠ¡ä¸­çš„ä¿¡æ¯èåˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚åˆ†å±‚å¯¹é½ç­–ç•¥åœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶å®ç°äº†ä¿¡æ¯æŸå¤±æœ€å°åŒ–ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
RGB cameras excel at capturing rich texture details with high spatial
resolution, whereas event cameras offer exceptional temporal resolution and a
high dynamic range (HDR). Leveraging their complementary strengths can
substantially enhance object tracking under challenging conditions, such as
high-speed motion, HDR environments, and dynamic background interference.
However, a significant spatio-temporal asymmetry exists between these two
modalities due to their fundamentally different imaging mechanisms, hindering
effective multi-modal integration. To address this issue, we propose
{Hierarchical Asymmetric Distillation} (HAD), a multi-modal knowledge
distillation framework that explicitly models and mitigates spatio-temporal
asymmetries. Specifically, HAD proposes a hierarchical alignment strategy that
minimizes information loss while maintaining the student network's
computational efficiency and parameter compactness. Extensive experiments
demonstrate that HAD consistently outperforms state-of-the-art methods, and
comprehensive ablation studies further validate the effectiveness and necessity
of each designed component. The code will be released soon.


### [26] [Can You Trust What You See? Alpha Channel No-Box Attacks on Video Object Detection](https://arxiv.org/abs/2510.19574)
*Ariana Yi, Ce Zhou, Liyang Xiao, Qiben Yan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Î±-Cloakï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨æ— ç›’è®¾ç½®ä¸‹é€šè¿‡RGBAè§†é¢‘çš„alphaé€šé“å¯¹ç›®æ ‡æ£€æµ‹å™¨è¿›è¡Œå¯¹æŠ—æ”»å‡»çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ¶æ„è§†é¢‘ä¸è‰¯æ€§è§†é¢‘èåˆï¼Œäº§ç”Ÿå¯¹äººç±»è§‚å¯Ÿè€…æ— å®³ä½†èƒ½æŒç»­æ¬ºéª—ç›®æ ‡æ£€æµ‹å™¨çš„èåˆè§†é¢‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€ç›®æ ‡æ£€æµ‹æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶è½¦è¾†å’Œç›‘æ§å¹³å°ç­‰ç½‘ç»œç‰©ç†ç³»ç»Ÿä¸­çš„éƒ¨ç½²æ—¥ç›Šå¢å¤šï¼Œç¡®ä¿å…¶å¯¹æŠ—æ€§å¨èƒçš„å®‰å…¨æ€§è‡³å…³é‡è¦ï¼Œè€Œç°æœ‰ç ”ç©¶ä¸»è¦æ¢ç´¢å›¾åƒé¢†åŸŸçš„å¯¹æŠ—æ”»å‡»ï¼Œè§†é¢‘é¢†åŸŸå°¤å…¶æ˜¯æ— ç›’è®¾ç½®ä¸‹çš„æ”»å‡»ä»ç„¶ç¼ºä¹æ·±å…¥ç ”ç©¶ã€‚

**Method:** Î±-Cloakåˆ©ç”¨alphaé€šé“å°†æ¶æ„ç›®æ ‡è§†é¢‘ä¸è‰¯æ€§è§†é¢‘èåˆï¼Œè®¾è®¡äº†ä¸€ç§èåˆç®—æ³•ç¡®ä¿è§†è§‰éšè”½æ€§å’Œå…¼å®¹æ€§ï¼Œè¯¥æ–¹æ³•æ— éœ€è®¿é—®æ¨¡å‹æ¶æ„ã€å‚æ•°æˆ–è¾“å‡ºï¼Œä¹Ÿä¸ä¼šå¼•å…¥å¯æ„ŸçŸ¥çš„ä¼ªå½±ã€‚

**Result:** åœ¨äº”ä¸ªæœ€å…ˆè¿›çš„ç›®æ ‡æ£€æµ‹å™¨ã€ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹å’Œä¸€ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒÎ±-Cloakåœ¨æ‰€æœ‰åœºæ™¯ä¸‹å‡å®ç°äº†100%çš„æ”»å‡»æˆåŠŸç‡ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒæ¨¡å‹é—´çš„å¹¿æ³›æœ‰æ•ˆæ€§ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†è§†é¢‘æ„ŸçŸ¥ç³»ç»Ÿä¸­å…ˆå‰æœªè¢«æ¢ç´¢çš„æ¼æ´ï¼Œå¼ºè°ƒäº†åœ¨å¯¹æŠ—æ€§è®¾ç½®ä¸­è€ƒè™‘alphaé€šé“çš„é˜²å¾¡æªæ–½çš„ç´§è¿«éœ€æ±‚ï¼Œä¸ºè§†é¢‘å®‰å…¨é¢†åŸŸæä¾›äº†é‡è¦çš„å®‰å…¨å¯ç¤ºã€‚

---

#### ğŸ“„ Abstract
As object detection models are increasingly deployed in cyber-physical
systems such as autonomous vehicles (AVs) and surveillance platforms, ensuring
their security against adversarial threats is essential. While prior work has
explored adversarial attacks in the image domain, those attacks in the video
domain remain largely unexamined, especially in the no-box setting. In this
paper, we present {\alpha}-Cloak, the first no-box adversarial attack on object
detectors that operates entirely through the alpha channel of RGBA videos.
{\alpha}-Cloak exploits the alpha channel to fuse a malicious target video with
a benign video, resulting in a fused video that appears innocuous to human
viewers but consistently fools object detectors. Our attack requires no access
to model architecture, parameters, or outputs, and introduces no perceptible
artifacts. We systematically study the support for alpha channels across common
video formats and playback applications, and design a fusion algorithm that
ensures visual stealth and compatibility. We evaluate {\alpha}-Cloak on five
state-of-the-art object detectors, a vision-language model, and a multi-modal
large language model (Gemini-2.0-Flash), demonstrating a 100% attack success
rate across all scenarios. Our findings reveal a previously unexplored
vulnerability in video-based perception systems, highlighting the urgent need
for defenses that account for the alpha channel in adversarial settings.


### [27] [Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing](https://arxiv.org/abs/2510.19808)
*Yusu Qian, Eli Bocek-Rivele, Liangchen Song, Jialing Tong, Yinfei Yang, Jiasen Lu, Wenze Hu, Zhe Gan*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†Pico-Banana-400Kï¼Œä¸€ä¸ªåŒ…å«40ä¸‡å¼ å›¾åƒçš„å¤§è§„æ¨¡ã€é«˜è´¨é‡æŒ‡ä»¤å›¾åƒç¼–è¾‘æ•°æ®é›†ï¼Œé€šè¿‡ç³»ç»ŸåŒ–çš„è´¨é‡æ§åˆ¶å’Œå¤šæ ·åŒ–ç¼–è¾‘åˆ†ç±»æ³•æ„å»ºï¼Œä¸ºæ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°æä¾›äº†åšå®åŸºç¡€ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç ”ç©¶ç¤¾åŒºçš„å‘å±•å—åˆ°ç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡ã€å¼€æ”¾å¯è®¿é—®çš„çœŸå®å›¾åƒæ•°æ®é›†çš„é™åˆ¶ï¼Œç°æœ‰æ•°æ®é›†å¤šä¸ºåˆæˆç”Ÿæˆï¼Œéš¾ä»¥æ»¡è¶³å¤æ‚ç¼–è¾‘åœºæ™¯çš„ç ”ç©¶éœ€æ±‚ã€‚

**Method:** åˆ©ç”¨Nano-Bananaä»OpenImagesé›†åˆçš„çœŸå®ç…§ç‰‡ç”Ÿæˆå¤šæ ·åŒ–ç¼–è¾‘å¯¹ï¼Œé‡‡ç”¨ç»†ç²’åº¦å›¾åƒç¼–è¾‘åˆ†ç±»æ³•ç¡®ä¿ç¼–è¾‘ç±»å‹çš„å…¨é¢è¦†ç›–ï¼Œé€šè¿‡åŸºäºMLLMçš„è´¨é‡è¯„åˆ†å’Œç²¾å¿ƒç­›é€‰æ¥ä¿æŒå†…å®¹ä¿å­˜å’ŒæŒ‡ä»¤å¿ å®åº¦ï¼Œå¹¶æ„å»ºäº†ä¸‰ä¸ªä¸“é—¨å­é›†ç”¨äºå¤šè½®ç¼–è¾‘ã€åå¥½å¯¹é½å’ŒæŒ‡ä»¤é‡å†™ç ”ç©¶ã€‚

**Result:** æ„å»ºäº†åŒ…å«40ä¸‡å¼ å›¾åƒçš„ç»¼åˆæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«7.2ä¸‡ç¤ºä¾‹çš„å¤šè½®ç¼–è¾‘å­é›†ç”¨äºé¡ºåºç¼–è¾‘ç ”ç©¶ï¼Œ5.6ä¸‡ç¤ºä¾‹çš„åå¥½å­é›†ç”¨äºå¯¹é½ç ”ç©¶ï¼Œä»¥åŠé…å¯¹çš„é•¿åº¦æŒ‡ä»¤ç”¨äºæŒ‡ä»¤é‡å†™èƒ½åŠ›å¼€å‘ï¼Œä¸ºä¸‹ä¸€ä»£æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘æ¨¡å‹æä¾›äº†å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„è®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•èµ„æºã€‚

**Conclusion:** Pico-Banana-400Ké€šè¿‡ç³»ç»ŸåŒ–çš„è´¨é‡æ§åˆ¶å’Œå¤šæ ·åŒ–ç¼–è¾‘åœºæ™¯è¦†ç›–ï¼Œä¸ºæ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ç ”ç©¶æä¾›äº†å…³é”®åŸºç¡€è®¾æ–½ï¼Œä¸ä»…æ”¯æŒå•è½®ç¼–è¾‘ä»»åŠ¡ï¼Œè¿˜æ¨åŠ¨äº†å¤æ‚ç¼–è¾‘åœºæ™¯ã€å¤šè½®æ¨ç†å’ŒæŒ‡ä»¤ä¼˜åŒ–ç­‰å‰æ²¿ç ”ç©¶æ–¹å‘çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Recent advances in multimodal models have demonstrated remarkable text-guided
image editing capabilities, with systems like GPT-4o and Nano-Banana setting
new benchmarks. However, the research community's progress remains constrained
by the absence of large-scale, high-quality, and openly accessible datasets
built from real images. We introduce Pico-Banana-400K, a comprehensive
400K-image dataset for instruction-based image editing. Our dataset is
constructed by leveraging Nano-Banana to generate diverse edit pairs from real
photographs in the OpenImages collection. What distinguishes Pico-Banana-400K
from previous synthetic datasets is our systematic approach to quality and
diversity. We employ a fine-grained image editing taxonomy to ensure
comprehensive coverage of edit types while maintaining precise content
preservation and instruction faithfulness through MLLM-based quality scoring
and careful curation. Beyond single turn editing, Pico-Banana-400K enables
research into complex editing scenarios. The dataset includes three specialized
subsets: (1) a 72K-example multi-turn collection for studying sequential
editing, reasoning, and planning across consecutive modifications; (2) a
56K-example preference subset for alignment research and reward model training;
and (3) paired long-short editing instructions for developing instruction
rewriting and summarization capabilities. By providing this large-scale,
high-quality, and task-rich resource, Pico-Banana-400K establishes a robust
foundation for training and benchmarking the next generation of text-guided
image editing models.


### [28] [Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation](https://arxiv.org/abs/2510.19592)
*Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„åˆ†è§£æ³¨æ„åŠ›èåˆæ–¹æ³•DecAFï¼Œé€šè¿‡å¯¹æ¯”æ€§å¯¹è±¡-èƒŒæ™¯èåˆå’Œäº’è¡¥è§†é¢‘-å¸§èåˆæœºåˆ¶ï¼Œå°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ³¨æ„åŠ›å›¾ç›´æ¥è½¬æ¢ä¸ºè§†é¢‘åˆ†å‰²æ©ç ï¼Œåœ¨æ¨ç†åˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†ä¸åŸºäºè®­ç»ƒæ–¹æ³•ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ–¹æ³•éœ€è¦è”åˆè®­ç»ƒMLLMsä¸SAMæ¥å®ç°è§†é¢‘åˆ†å‰²ï¼Œè€Œæœ¬æ–‡æ—¨åœ¨å¼€å‘æ— éœ€é‡æ–°è®­ç»ƒçš„è§£å†³æ–¹æ¡ˆï¼Œç›´æ¥åˆ©ç”¨MLLMsçš„æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œè§†é¢‘æ¨ç†åˆ†å‰²ï¼Œä½†åŸå§‹æ³¨æ„åŠ›å›¾å­˜åœ¨å™ªå£°ä¸”ä¸å¯¹è±¡åŒºåŸŸå¯¹é½ä¸ä½³çš„é—®é¢˜éœ€è¦è§£å†³ã€‚

**Method:** æå‡ºåˆ†è§£æ³¨æ„åŠ›èåˆæ–¹æ³•DecAFï¼ŒåŒ…å«å¯¹æ¯”æ€§å¯¹è±¡-èƒŒæ™¯èåˆæœºåˆ¶æ¥æŠ‘åˆ¶æ— å…³æ¿€æ´»ï¼Œä»¥åŠäº’è¡¥è§†é¢‘-å¸§èåˆæœºåˆ¶æ¥å¢å¼ºå¯¹è±¡èšç„¦çº¿ç´¢ï¼ŒåŒæ—¶å¼•å…¥æ³¨æ„åŠ›å¼•å¯¼çš„SAM2æç¤ºæœºåˆ¶æ¥è·å–ç»†ç²’åº¦åˆ†å‰²æ©ç ã€‚

**Result:** DecAFåœ¨å‚è€ƒå’Œæ¨ç†è§†é¢‘å¯¹è±¡åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ‰€æœ‰æ— éœ€è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å®ç°äº†ä¸åŸºäºè®­ç»ƒæ–¹æ³•ç›¸å½“çš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¿æŒé›¶è®­ç»ƒä¼˜åŠ¿çš„åŒæ—¶è¾¾åˆ°é«˜æ€§èƒ½çš„èƒ½åŠ›ã€‚

**Conclusion:** è¯¥æ–¹æ³•å±•ç¤ºäº†æ— éœ€é‡æ–°è®­ç»ƒMLLMså³å¯å®ç°é«˜è´¨é‡è§†é¢‘åˆ†å‰²çš„å¯è¡Œæ€§ï¼Œä¸ºå¤šæ¨¡æ€å¤§æ¨¡å‹çš„ç›´æ¥åº”ç”¨å¼€è¾Ÿäº†æ–°é€”å¾„ï¼ŒåŒæ—¶é€šè¿‡æ³¨æ„åŠ›èåˆæœºåˆ¶æœ‰æ•ˆè§£å†³äº†åŸå§‹æ³¨æ„åŠ›å›¾çš„å™ªå£°å’Œå¯¹é½é—®é¢˜ã€‚

---

#### ğŸ“„ Abstract
Multimodal large language models (MLLMs) demonstrate strong video
understanding by attending to visual tokens relevant to textual queries. To
directly adapt this for localization in a training-free manner, we cast video
reasoning segmentation as a video QA task and extract attention maps via
rollout mechanism. However, raw attention maps are noisy and poorly aligned
with object regions. We propose Decomposed Attention Fusion (DecAF), which
refines these maps through two mechanisms: (1) contrastive object-background
fusion and (2) complementary video-frame fusion. This method suppresses
irrelevant activations and enhances object-focused cues, enabling direct
conversion of attention maps into coarse segmentation masks. In addition, we
introduce attention-guided SAM2 prompting for obtaining fine-grained masks.
Unlike existing methods that jointly train MLLMs with SAM, our method operates
entirely without retraining. DecAF outperforms training-free methods and
achieves performance comparable to training-based methods on both referring and
reasoning VOS benchmarks. The code will be available at
https://github.com/HYUNJS/DecAF.


### [29] [MedReason-R1: Learning to Reason for CT Diagnosis with Reinforcement Learning and Local Zoom](https://arxiv.org/abs/2510.19626)
*Yifan Li, Fenghe Tang, Yingtai Li, Shaohua Kevin Zhou*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†MedReason-R1åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ„å»ºCT-RATE-VQAæ•°æ®é›†å’Œå¼•å…¥æ˜¾å¼æ¨ç†è¿‡ç¨‹ï¼Œè§£å†³äº†é€šç”¨VLMsåœ¨åŒ»å­¦é¢†åŸŸè¯Šæ–­æ€§èƒ½ä¸è¶³çš„é—®é¢˜ï¼Œåœ¨CTç–¾ç—…è¯Šæ–­ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** é€šç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒæè¿°ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸçš„æ€§èƒ½ä»ç„¶æ¬ ä½³ï¼Œä¸»è¦ç”±äºç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡çš„ä¸“ä¸šåŒ»å­¦å½±åƒæ•°æ®é›†ï¼Œä»¥åŠå¿½è§†äº†ä»ç²—ç²’åº¦åˆ°ç»†ç²’åº¦çš„è¯Šæ–­è¿‡ç¨‹ã€‚

**Method:** æ„å»ºäº†åŒ…å«84Ké—®ç­”å¯¹çš„CT-RATE-VQAæ•°æ®é›†ï¼Œæå‡ºäº†MedReason-R1åŒ»å­¦VLMï¼Œé‡‡ç”¨å°†ç–¾ç—…æ„Ÿå…´è¶£åŒºåŸŸåµŒå…¥å›¾åƒçš„æ–°ç­–ç•¥ï¼Œå¹¶å¼•å…¥äº†GRPOå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥å®ç°æ— éœ€æ˜‚è´µäººå·¥æ ‡æ³¨çš„æœ‰æ•ˆæ¨ç†ã€‚

**Result:** ä¸æœ€è¿‘çš„é€šç”¨å’ŒåŒ»å­¦VLMsç›¸æ¯”ï¼ŒMedReason-R1åœ¨CTç–¾ç—…è¯Šæ–­ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Conclusion:** è¯¥ç ”ç©¶å¼ºè°ƒäº†å…¨å±€å®šä½å’Œç–¾ç—…ç‰¹å®šç»†èŠ‚åœ¨æå‡æ¨¡å‹è¯Šæ–­æ€§èƒ½ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºåŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹çš„å‘å±•æä¾›äº†æ–°çš„æ¨ç†æ¡†æ¶å’Œæ•°æ®é›†æ”¯æŒã€‚

---

#### ğŸ“„ Abstract
General-purpose large Vision-Language Models (VLMs) demonstrate strong
capabilities in generating detailed descriptions for natural images. However,
their performance in the medical domain remains suboptimal, even for relatively
straightforward tasks, primarily due to the lack of large-scale, high-quality,
specialized medical imaging datasets and the neglect of the diagnostic process
that progresses from coarse to fine-grained. To address the first issue, we
construct the CT-RATE-VQA dataset, which has 84K QA pairs. For the second
issue, we propose MedReason-R1, a medical VLM with explicit reasoning process
for disease diagnosis. MedReason-R1 incorporates a novel strategy that embeds
zoom-in disease region-of-interest areas into the image, highlighting the
crucial role of both global localization and disease-specific details in
enhancing the model's diagnostic performance. Furthermore, we introduce the
GRPO reinforcement learning framework to MedReason-R1, which enables effective
reasoning without relying on costly manual annotations. Compared to recent
general-purpose and medical VLMs, MedReason-R1 achieves state-of-the-art
performance in CT disease diagnosis while retaining generalization. The code,
checkpoints, and dataset are available at:
https://github.com/Leevan001/MedReason-R1


### [30] [Curvilinear Structure-preserving Unpaired Cross-domain Medical Image Translation](https://arxiv.org/abs/2510.19679)
*Zihao Chen, Yi Zhou, Xudong Jiang, Li Chen, Leopold Schmetterer, Bingyao Tan, Jun Cheng*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCurvilinear Structure-preserving Translation (CST)çš„é€šç”¨æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ— é…å¯¹å›¾åƒç¿»è¯‘ä¸­æ˜¾å¼ä¿ç•™ç²¾ç»†æ›²çº¿ç»“æ„æ¥è§£å†³ç°æœ‰æ–¹æ³•åœ¨åŒ»å­¦æˆåƒä¸­æ‰­æ›²å¾®è¡€ç®¡ç­‰ç»†å¾®ç»“æ„çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆç»“æ„ä¸€è‡´æ€§ç›‘ç£ï¼Œåœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ¨¡æ€ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç¿»è¯‘æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰æ— é…å¯¹å›¾åƒç¿»è¯‘æ–¹æ³•åœ¨åŒ»å­¦æˆåƒä¸­ç»å¸¸æ‰­æ›²ç²¾ç»†çš„æ›²çº¿ç»“æ„ï¼Œå¦‚å¾®è¡€ç®¡ç³»ç»Ÿï¼Œè¿™å½±å“äº†è¯Šæ–­å¯é æ€§å’Œå®šé‡åˆ†æã€‚åœ¨çœ¼ç§‘å’Œè¡€ç®¡æˆåƒä¸­ï¼Œç»†å¾®çš„å½¢æ€å˜åŒ–å…·æœ‰é‡è¦çš„ä¸´åºŠæ„ä¹‰ï¼Œå› æ­¤éœ€è¦ä¸€ç§èƒ½å¤Ÿä¿æŒè¿™äº›å…³é”®ç»“æ„å®Œæ•´æ€§çš„ç¿»è¯‘æ–¹æ³•ã€‚

**Method:** CSTæ¡†æ¶é€šè¿‡é›†æˆæ›²çº¿ç»“æ„æå–æ¨¡å—æ¥æä¾›æ‹“æ‰‘ç›‘ç£ï¼Œå°†ç»“æ„ä¸€è‡´æ€§èå…¥è®­ç»ƒè¿‡ç¨‹ã€‚è¯¥æ¡†æ¶å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰æ–¹æ³•ä¸­ï¼Œä½œè€…å°†å…¶åº”ç”¨äºCycleGANå’ŒUNSBä¸¤ä¸ªä»£è¡¨æ€§éª¨å¹²ç½‘ç»œï¼Œé€šè¿‡ç»“æ„æå–æ¨¡å—å¢å¼ºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚

**Result:** åœ¨å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æè¡€ç®¡æˆåƒã€å½©è‰²çœ¼åº•å’ŒXå°„çº¿å† çŠ¶åŠ¨è„‰é€ å½±ä¸‰ç§æˆåƒæ¨¡æ€ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒCSTæé«˜äº†ç¿»è¯‘ä¿çœŸåº¦å¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒå‡ ä½•å®Œæ•´æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿ç•™ç»†å¾®æ›²çº¿ç»“æ„æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

**Conclusion:** é€šè¿‡åœ¨å­¦ä¹ æ˜ å°„ä¸­åŠ å¼ºå‡ ä½•å®Œæ•´æ€§ï¼ŒCSTä¸ºåŒ»å­¦æˆåƒä¸­çš„æ›²çº¿ç»“æ„æ„ŸçŸ¥è·¨åŸŸç¿»è¯‘å»ºç«‹äº†ä¸€ä¸ªåŸåˆ™æ€§é€”å¾„ã€‚è¯¥ç ”ç©¶ä¸ºåŒ»å­¦å›¾åƒç¿»è¯‘ä¸­ä¿æŒå…³é”®è§£å‰–ç»“æ„æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
Unpaired image-to-image translation has emerged as a crucial technique in
medical imaging, enabling cross-modality synthesis, domain adaptation, and data
augmentation without costly paired datasets. Yet, existing approaches often
distort fine curvilinear structures, such as microvasculature, undermining both
diagnostic reliability and quantitative analysis. This limitation is
consequential in ophthalmic and vascular imaging, where subtle morphological
changes carry significant clinical meaning. We propose Curvilinear
Structure-preserving Translation (CST), a general framework that explicitly
preserves fine curvilinear structures during unpaired translation by
integrating structure consistency into the training. Specifically, CST augments
baseline models with a curvilinear extraction module for topological
supervision. It can be seamlessly incorporated into existing methods. We
integrate it into CycleGAN and UNSB as two representative backbones.
Comprehensive evaluation across three imaging modalities: optical coherence
tomography angiography, color fundus and X-ray coronary angiography
demonstrates that CST improves translation fidelity and achieves
state-of-the-art performance. By reinforcing geometric integrity in learned
mappings, CST establishes a principled pathway toward curvilinear
structure-aware cross-domain translation in medical imaging.


### [31] [OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation](https://arxiv.org/abs/2510.19789)
*Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†OmniMotion-Xï¼Œä¸€ä¸ªåŸºäºè‡ªå›å½’æ‰©æ•£å˜æ¢å™¨çš„ç»Ÿä¸€åºåˆ—åˆ°åºåˆ—æ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€å…¨èº«äººä½“è¿åŠ¨ç”Ÿæˆã€‚è¯¥æ¡†æ¶æ”¯æŒæ–‡æœ¬åˆ°è¿åŠ¨ã€éŸ³ä¹åˆ°èˆè¹ˆã€è¯­éŸ³åˆ°æ‰‹åŠ¿ç­‰å¤šç§ä»»åŠ¡ï¼Œå¹¶é€šè¿‡æ¸è¿›å¼å¼±åˆ°å¼ºæ··åˆæ¡ä»¶è®­ç»ƒç­–ç•¥è§£å†³å¤šæ¨¡æ€å†²çªé—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰äººä½“è¿åŠ¨ç”Ÿæˆæ–¹æ³•é€šå¸¸é’ˆå¯¹å•ä¸€æ¨¡æ€ä»»åŠ¡è®¾è®¡ï¼Œç¼ºä¹ç»Ÿä¸€çš„æ¡†æ¶æ¥å¤„ç†å¤šæ ·åŒ–çš„å¤šæ¨¡æ€è¾“å…¥å’Œè¾“å‡ºåœºæ™¯ã€‚ç°æœ‰æ–¹æ³•åœ¨å†…å®¹ä¸€è‡´æ€§ã€é£æ ¼ä¿æŒå’Œæ—¶é—´åŠ¨æ€æ§åˆ¶æ–¹é¢å­˜åœ¨å±€é™ï¼Œéš¾ä»¥æ”¯æŒå¤æ‚çš„äº¤äº’å¼é•¿åºåˆ—è¿åŠ¨ç”Ÿæˆéœ€æ±‚ã€‚

**Method:** æå‡ºåŸºäºè‡ªå›å½’æ‰©æ•£å˜æ¢å™¨çš„ç»Ÿä¸€åºåˆ—åˆ°åºåˆ—æ¡†æ¶ï¼Œå¼•å…¥å‚è€ƒè¿åŠ¨ä½œä¸ºæ–°å‹æ¡ä»¶ä¿¡å·ä»¥å¢å¼ºç”Ÿæˆå†…å®¹çš„ä¸€è‡´æ€§ã€‚é‡‡ç”¨æ¸è¿›å¼å¼±åˆ°å¼ºæ··åˆæ¡ä»¶è®­ç»ƒç­–ç•¥å¤„ç†å¤šæ¨¡æ€å†²çªï¼Œå¹¶æ„å»ºäº†OmniMoCap-Xæ•°æ®é›†ï¼Œæ•´åˆ28ä¸ªå…¬å¼€MoCapæ•°æ®æºï¼Œä½¿ç”¨GPT-4oè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–å±‚æ¬¡åŒ–æ ‡æ³¨ã€‚

**Result:** å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒOmniMotion-Xåœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆé€¼çœŸã€è¿è´¯ä¸”å¯æ§çš„é•¿æ—¶ç¨‹è¿åŠ¨åºåˆ—ï¼Œåœ¨æ–‡æœ¬åˆ°è¿åŠ¨ã€éŸ³ä¹åˆ°èˆè¹ˆç­‰ä»»åŠ¡ä¸­å‡å±•ç°å‡ºä¼˜è¶Šçš„ç”Ÿæˆè´¨é‡ã€‚

**Conclusion:** OmniMotion-Xè¯æ˜äº†ç»Ÿä¸€å¤šæ¨¡æ€è¿åŠ¨ç”Ÿæˆæ¡†æ¶çš„å¯è¡Œæ€§ï¼Œä¸ºå¤æ‚äººä½“åŠ¨ç”»ç”Ÿæˆæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥å¤šæ¨¡æ€äº¤äº’å¼è¿åŠ¨ç”Ÿæˆç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ï¼Œå±•ç¤ºäº†å‚è€ƒè¿åŠ¨æ¡ä»¶ä¿¡å·åœ¨ä¿æŒç”Ÿæˆå†…å®¹ä¸€è‡´æ€§æ–¹é¢çš„é‡è¦ä»·å€¼ã€‚

---

#### ğŸ“„ Abstract
This paper introduces OmniMotion-X, a versatile multimodal framework for
whole-body human motion generation, leveraging an autoregressive diffusion
transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently
supports diverse multimodal tasks, including text-to-motion, music-to-dance,
speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion
prediction, in-betweening, completion, and joint/trajectory-guided synthesis),
as well as flexible combinations of these tasks. Specifically, we propose the
use of reference motion as a novel conditioning signal, substantially enhancing
the consistency of generated content, style, and temporal dynamics crucial for
realistic animations. To handle multimodal conflicts, we introduce a
progressive weak-to-strong mixed-condition training strategy. To enable
high-quality multimodal training, we construct OmniMoCap-X, the largest unified
multimodal motion dataset to date, integrating 28 publicly available MoCap
sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps.
To ensure detailed and consistent annotations, we render sequences into videos
and use GPT-4o to automatically generate structured and hierarchical captions,
capturing both low-level actions and high-level semantics. Extensive
experimental evaluations confirm that OmniMotion-X significantly surpasses
existing methods, demonstrating state-of-the-art performance across multiple
multimodal tasks and enabling the interactive generation of realistic,
coherent, and controllable long-duration motions.


### [32] [Class-Aware Prototype Learning with Negative Contrast for Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2510.19802)
*Xiaozhen Qiao, Jingkai Zhao, Yuqiu Jiang, Xianda Guo, Zhe Sun, Hongyuan Zhang, Xuelong Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†CPL-NCï¼Œä¸€ä¸ªä¸“ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹è®¾è®¡çš„è½»é‡çº§æµ‹è¯•æ—¶é€‚åº”æ¡†æ¶ï¼Œé€šè¿‡ç±»åˆ«æ„ŸçŸ¥åŸå‹ç¼“å­˜å’Œè´Ÿå¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³äº†åˆ†å¸ƒåç§»ä¸‹çš„åŸå‹é€€åŒ–å’Œç±»åˆ«æ··æ·†é—®é¢˜ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éƒ¨ç½²åˆ†å¸ƒä¸è®­ç»ƒåˆ†å¸ƒå‘ç”Ÿåç§»æ—¶æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ç°æœ‰æµ‹è¯•æ—¶é€‚åº”æ–¹æ³•å¿½è§†äº†é•¿å°¾åˆ†å¸ƒä¸­çš„åŸå‹é€€åŒ–é—®é¢˜ä»¥åŠè¯­ä¹‰ç›¸ä¼¼ç±»åˆ«ä¹‹é—´çš„æ··æ·†æŒ‘æˆ˜ï¼Œéœ€è¦ä¸“é—¨é’ˆå¯¹VLMsçš„é€‚åº”æ¡†æ¶æ¥æå‡åˆ†å¸ƒåç§»ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

**Method:** CPL-NCæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šç±»åˆ«æ„ŸçŸ¥åŸå‹ç¼“å­˜æ¨¡å—æ ¹æ®æµ‹è¯•æ—¶é¢‘ç‡å’Œæ¿€æ´»å†å²åŠ¨æ€è°ƒæ•´æ¯ç±»å®¹é‡ï¼Œå¹¶é€šè¿‡å¤æ´»æœºåˆ¶ä¿ç•™ç¨€æœ‰ç±»åˆ«çŸ¥è¯†ï¼›è´Ÿå¯¹æ¯”å­¦ä¹ æœºåˆ¶è¯†åˆ«å¹¶çº¦æŸå›°éš¾çš„è§†è§‰-æ–‡æœ¬è´Ÿæ ·æœ¬ä»¥æé«˜ç±»åˆ«å¯åˆ†æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨éå¯¹ç§°ä¼˜åŒ–ç­–ç•¥ï¼Œä»…ä¼˜åŒ–æ–‡æœ¬åŸå‹è€Œä¿æŒè§†è§‰ç‰¹å¾ç¨³å®šã€‚

**Result:** åœ¨15ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCPL-NCåœ¨ResNet-50å’ŒViT-B/16ä¸¤ç§éª¨å¹²ç½‘ç»œä¸Šéƒ½ä¸€è‡´ä¼˜äºå…ˆå‰çš„æµ‹è¯•æ—¶é€‚åº”æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å„ç§åˆ†å¸ƒåç§»åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

**Conclusion:** CPL-NCé€šè¿‡é’ˆå¯¹æ€§åœ°è§£å†³åŸå‹é€€åŒ–å’Œç±»åˆ«æ··æ·†é—®é¢˜ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåˆ†å¸ƒåç§»ä¸‹çš„éƒ¨ç½²æä¾›äº†æœ‰æ•ˆçš„è½»é‡çº§é€‚åº”æ–¹æ¡ˆã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†åœ¨æµ‹è¯•æ—¶é€‚åº”ä¸­è€ƒè™‘ç±»åˆ«åˆ†å¸ƒç‰¹æ€§å’Œè¯­ä¹‰å…³ç³»çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Vision-Language Models (VLMs) demonstrate impressive zero-shot generalization
through large-scale image-text pretraining, yet their performance can drop once
the deployment distribution diverges from the training distribution. To address
this, Test-Time Adaptation (TTA) methods update models using unlabeled target
data. However, existing approaches often ignore two key challenges: prototype
degradation in long-tailed distributions and confusion between semantically
similar classes. To tackle these issues, we propose \textbf{C}lass-Aware
\textbf{P}rototype \textbf{L}earning with \textbf{N}egative
\textbf{C}ontrast(\textbf{CPL-NC}), a lightweight TTA framework designed
specifically for VLMs to enhance generalization under distribution shifts.
CPL-NC introduces a \textit{Class-Aware Prototype Cache} Module that
dynamically adjusts per-class capacity based on test-time frequency and
activation history, with a rejuvenation mechanism for inactive classes to
retain rare-category knowledge. Additionally, a \textit{Negative Contrastive
Learning} Mechanism identifies and constrains hard visual-textual negatives to
improve class separability. The framework employs asymmetric optimization,
refining only textual prototypes while anchoring on stable visual features.
Experiments on 15 benchmarks show that CPL-NC consistently outperforms prior
TTA methods across both ResNet-50 and ViT-B/16 backbones.


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [33] [Small Language Models Offer Significant Potential for Science Community](https://arxiv.org/abs/2510.18890)
*Jian Zhang*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªåŸºäºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆMiniLMsï¼‰çš„æ¡†æ¶ï¼Œç”¨äºä»åœ°çƒç§‘å­¦æ–‡çŒ®ä¸­è¿›è¡Œç²¾ç¡®ã€å¿«é€Ÿä¸”ç»æµé«˜æ•ˆçš„ä¿¡æ¯æ£€ç´¢ï¼Œç›¸æ¯”å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå–ç»è¿‡ä¸“å®¶éªŒè¯çš„å®šé‡ä¿¡æ¯ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å­˜åœ¨ä¿¡æ¯åè§å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢ä½¿ç”¨å…è´¹çš„å°å‹è¯­è¨€æ¨¡å‹å®ç°ä»å¤§é‡åœ°çƒç§‘å­¦æ–‡çŒ®ä¸­ç²¾ç¡®ã€å¿«é€Ÿä¸”æˆæœ¬å¯æ§çš„ä¿¡æ¯æ£€ç´¢çš„å¯è¡Œæ€§ã€‚

**Method:** æ„å»ºäº†ä¸€ä¸ªåŒ…å«çº¦7700ä¸‡é«˜è´¨é‡å¥å­çš„è¯­æ–™åº“ï¼Œæ¶µç›–2000å¹´è‡³2024å¹´é—´95ç§é¢†å…ˆåœ°çƒç§‘å­¦æœŸåˆŠï¼Œé‡‡ç”¨å°å‹è¯­è¨€æ¨¡å‹é€šè¿‡è¯­ä¹‰æœç´¢æŠ€æœ¯å’Œå¥å­çº§ç´¢å¼•å®ç°è®¡ç®—é«˜æ•ˆçš„ä¿¡æ¯æå–ï¼Œå¹¶ç»“åˆæƒ…æ„Ÿåˆ†æå’Œæ— ç›‘ç£èšç±»åˆ†ææƒ…æ„ŸåŸºè°ƒå’Œä¸»é¢˜æ¼”å˜ã€‚

**Result:** ç›¸æ¯”ChatGPT-4ç­‰å¤§å‹è¯­è¨€æ¨¡å‹äº§ç”Ÿçš„é€šç”¨åŒ–å“åº”ï¼ŒMiniLMæ–¹æ³•åœ¨è¯†åˆ«ç»è¿‡ä¸“å®¶éªŒè¯çš„å¤šå­¦ç§‘æ¥æºä¿¡æ¯æ–¹é¢è¡¨ç°å“è¶Šï¼Œç‰¹åˆ«æ“…é•¿æå–åŒ…å«å®šé‡ç ”ç©¶ç»“æœçš„ä¿¡æ¯ï¼Œå¹¶èƒ½æœ‰æ•ˆè¿½è¸ªåœ°çƒç§‘å­¦ç¤¾åŒºä¸­ç»“è®ºæ¼”å˜ã€ç ”ç©¶é‡ç‚¹å’Œæ–°å…´é—®é¢˜ã€‚

**Conclusion:** MiniLMåœ¨åœ°çƒç§‘å­¦ç¤¾åŒºå…·æœ‰é‡è¦åº”ç”¨æ½œåŠ›ï¼Œå¯ç”¨äºäº‹å®å’Œå›¾åƒæ£€ç´¢ã€è¶‹åŠ¿åˆ†æã€çŸ›ç›¾åˆ†æä»¥åŠæ•™è‚²ç›®çš„ï¼Œä¸ºé¢†åŸŸç‰¹å®šçš„ä¿¡æ¯æ£€ç´¢æä¾›äº†ä¸€ç§è®¡ç®—æ•ˆç‡é«˜ä¸”ç²¾ç¡®çš„æ›¿ä»£æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
Recent advancements in natural language processing, particularly with large
language models (LLMs), are transforming how scientists engage with the
literature. While the adoption of LLMs is increasing, concerns remain regarding
potential information biases and computational costs. Rather than LLMs, I
developed a framework to evaluate the feasibility of precise, rapid, and
cost-effective information retrieval from extensive geoscience literature using
freely available small language models (MiniLMs). A curated corpus of
approximately 77 million high-quality sentences, extracted from 95 leading
peer-reviewed geoscience journals such as Geophysical Research Letters and
Earth and Planetary Science Letters published during years 2000 to 2024, was
constructed. MiniLMs enable a computationally efficient approach for extracting
relevant domain-specific information from these corpora through semantic search
techniques and sentence-level indexing. This approach, unlike LLMs such as
ChatGPT-4 that often produces generalized responses, excels at identifying
substantial amounts of expert-verified information with established,
multi-disciplinary sources, especially for information with quantitative
findings. Furthermore, by analyzing emotional tone via sentiment analysis and
topical clusters through unsupervised clustering within sentences, MiniLM
provides a powerful tool for tracking the evolution of conclusions, research
priorities, advancements, and emerging questions within geoscience communities.
Overall, MiniLM holds significant potential within the geoscience community for
applications such as fact and image retrievals, trend analyses, contradiction
analyses, and educational purposes.


### [34] [DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code](https://arxiv.org/abs/2510.18904)
*Shriyansh Agrawal, Aidan Lau, Sanyam Shah, Ahan M R, Kevin Zhu, Sunishchal Dev, Vasu Sharma*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºé€šè¿‡å¾®è°ƒç¼–ç å™¨ä¸“ç”¨çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰æ¥æ£€æµ‹æœºå™¨ç”Ÿæˆå†…å®¹ï¼Œè¯æ˜åœ¨äºŒå…ƒåˆ†ç±»ä»»åŠ¡ä¸­SLMsåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å¤§å¹…ä¼˜äºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒé«˜æ£€æµ‹ç²¾åº¦çš„åŒæ—¶å®ç°äº†8-12å€çš„å»¶è¿Ÿé™ä½å’Œ3-5å€çš„VRAMå³°å€¼ä½¿ç”¨å‡å°‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰åŸºäºé›¶æ ·æœ¬æ–¹æ³•çš„å¤§è¯­è¨€æ¨¡å‹å†…å®¹æ£€æµ‹å™¨å­˜åœ¨é«˜è®¡ç®—æˆæœ¬ä¸æ£€æµ‹ç²¾åº¦ä¸è¶³çš„é—®é¢˜ï¼Œå¦‚Fast DetectGPTå’ŒGPTZeroç­‰æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸å‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨æ˜æ˜¾æƒè¡¡ï¼ŒäºŸéœ€å¼€å‘æ›´é«˜æ•ˆå‡†ç¡®çš„æ£€æµ‹æ–¹æ¡ˆã€‚

**Method:** ç ”ç©¶é‡‡ç”¨å¾®è°ƒé¢„è®­ç»ƒçš„ç¼–ç å™¨ä¸“ç”¨å°å‹è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯RoBERTAå’ŒCodeBERTaæ¨¡å‹ï¼Œä½¿ç”¨ä¸“é—¨é’ˆå¯¹æºä»£ç å’Œè‡ªç„¶è¯­è¨€çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä¸“æ³¨äºäºŒå…ƒåˆ†ç±»ä»»åŠ¡çš„ä¼˜åŒ–ã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºç¼–ç å™¨æ¨¡å‹åœ¨AUROCæŒ‡æ ‡ä¸Šè¾¾åˆ°0.97-0.99ï¼Œå®F1åˆ†æ•°ä¸º0.89-0.94ï¼Œåœ¨512ä¸ªä»¤ç‰Œè¾“å…¥æ—¶å»¶è¿Ÿé™ä½8-12å€ï¼Œå³°å€¼VRAMä½¿ç”¨å‡å°‘3-5å€ï¼Œåœ¨è·¨ç”Ÿæˆå™¨è¿ç§»å’Œå¯¹æŠ—æ€§è½¬æ¢ä¸‹æ€§èƒ½ä¿æŒä¸ä½äº92%çš„æ¸…æ´AUROCã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯æ˜äº†å°å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨ç”Ÿæˆå†…å®¹æ£€æµ‹ä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ä»…å®ç°äº†è®¡ç®—æ•ˆç‡çš„å¤§å¹…æå‡ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œå¯¹æŠ—é²æ£’æ€§ï¼Œä¸ºå®é™…éƒ¨ç½²æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
The prevalence of Large Language Models (LLMs) for generating multilingual
text and source code has only increased the imperative for machine-generated
content detectors to be accurate and efficient across domains. Current
detectors, predominantly utilizing zero-shot methods, such as Fast DetectGPT or
GPTZero, either incur high computational cost or lack sufficient accuracy,
often with a trade-off between the two, leaving room for further improvement.
To address these gaps, we propose the fine-tuning of encoder-only Small
Language Models (SLMs), in particular, the pre-trained models of RoBERTA and
CodeBERTa using specialized datasets on source code and other natural language
to prove that for the task of binary classification, SLMs outperform LLMs by a
huge margin whilst using a fraction of compute. Our encoders achieve AUROC $=
0.97$ to $0.99$ and macro-F1 $0.89$ to $0.94$ while reducing latency by
$8$-$12\times$ and peak VRAM by $3$-$5\times$ at $512$-token inputs. Under
cross-generator shifts and adversarial transformations (paraphrase,
back-translation; code formatting/renaming), performance retains $\geq 92%$ of
clean AUROC. We release training and evaluation scripts with seeds and configs;
a reproducibility checklist is also included.


### [35] [MMAO-Bench: MultiModal All in One Benchmark Reveals Compositional Law between Uni-modal and Omni-modal in OmniModels](https://arxiv.org/abs/2510.18915)
*Chen Chen, ZeYang Hu, Fengjiao Chen, Liya Ma, Jiaxing Liu, Xiaoyu Li, Xuezhi Cao*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€ç»Ÿä¸€åŸºå‡†æµ‹è¯•MMAO-Benchï¼Œç”¨äºè¯„ä¼°å•æ¨¡æ€å’Œå…¨æ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œæ­ç¤ºäº†è·¨æ¨¡æ€ä¸å•æ¨¡æ€æ€§èƒ½ä¹‹é—´çš„ç»„åˆè§„å¾‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ­£ä»å•æ¨¡æ€ç†è§£å‘è§†è§‰ã€éŸ³é¢‘å’Œè¯­è¨€æ¨¡æ€çš„ç»Ÿä¸€å‘å±•ï¼Œä½†å•æ¨¡æ€ä¸å…¨æ¨¡æ€ä¹‹é—´çš„ç›¸å…³æ€§å°šä¸æ˜ç¡®ï¼Œéœ€è¦å…¨é¢çš„è¯„ä¼°æ¥æ¨åŠ¨å…¨æ¨¡æ€æ¨¡å‹çš„æ™ºèƒ½æ¼”è¿›ã€‚

**Method:** æå‡ºäº†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å…¨æ¨¡æ€åŸºå‡†æµ‹è¯•MMAO-Benchï¼ŒåŒ…å«1880ä¸ªäººå·¥æ ‡æ³¨æ ·æœ¬ï¼Œæ¶µç›–44ç§ä»»åŠ¡ç±»å‹ï¼Œå¹¶åˆ›æ–°æ€§åœ°å¼•å…¥äº†å¤šæ­¥éª¤å¼€æ”¾æ€§é—®é¢˜ç±»å‹ä»¥æ›´å¥½åœ°è¯„ä¼°å¤æ‚æ¨ç†ä»»åŠ¡ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜è·¨æ¨¡æ€ä¸å•æ¨¡æ€æ€§èƒ½ä¹‹é—´å­˜åœ¨ç»„åˆè§„å¾‹ï¼Œå…¨æ¨¡æ€èƒ½åŠ›åœ¨å¼±æ¨¡å‹ä¸Šè¡¨ç°ä¸ºç“¶é¢ˆæ•ˆåº”ï¼Œè€Œåœ¨å¼ºæ¨¡å‹ä¸Šåˆ™è¡¨ç°å‡ºååŒä¿ƒè¿›ä½œç”¨ã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†å…¨æ¨¡æ€èƒ½åŠ›åœ¨ä¸åŒæ¨¡å‹å¼ºåº¦ä¸‹çš„ä¸åŒè¡¨ç°æ¨¡å¼ï¼Œä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„è¯„ä¼°å’Œå‘å±•æä¾›äº†é‡è¦æŒ‡å¯¼ï¼Œè¡¨æ˜å…¨æ¨¡æ€ç†è§£éœ€è¦æ¨¡å‹å…·å¤‡è¶³å¤Ÿçš„åŸºç¡€èƒ½åŠ›æ‰èƒ½å®ç°ååŒæå‡ã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Languages models have been progressing from uni-modal
understanding toward unifying visual, audio and language modalities,
collectively termed omni models. However, the correlation between uni-modal and
omni-modal remains unclear, which requires comprehensive evaluation to drive
omni model's intelligence evolution. In this work, we propose a novel, high
quality and diversity omni model benchmark, MultiModal All in One Benchmark
(MMAO-Bench), which effectively assesses both uni-modal and omni-modal
understanding capabilities. The benchmark consists of 1880 human curated
samples, across 44 task types, and a innovative multi-step open-ended question
type that better assess complex reasoning tasks. Experimental result shows the
compositional law between cross-modal and uni-modal performance and the
omni-modal capability manifests as a bottleneck effect on weak models, while
exhibiting synergistic promotion on strong models.


### [36] [Transformer-Based Low-Resource Language Translation: A Study on Standard Bengali to Sylheti](https://arxiv.org/abs/2510.18898)
*Mangsura Kabir Oni, Tabia Tanzin Prama*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡å¾®è°ƒå¤šè¯­è¨€Transformeræ¨¡å‹å¹¶ä¸é›¶æ ·æœ¬å¤§è¯­è¨€æ¨¡å‹å¯¹æ¯”ï¼Œæ¢ç´¢äº†å­ŸåŠ æ‹‰è¯­åˆ°é”¡å°”èµ«ç‰¹è¯­çš„æœºå™¨ç¿»è¯‘ã€‚å®éªŒè¡¨æ˜å¾®è°ƒæ¨¡å‹æ˜¾è‘—ä¼˜äºLLMsï¼Œå…¶ä¸­mBART-50åœ¨ç¿»è¯‘å……åˆ†æ€§æ–¹é¢è¡¨ç°æœ€ä½³ï¼ŒMarianMTåœ¨å­—ç¬¦çº§ä¿çœŸåº¦æ–¹é¢æœ€å¼ºã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡åŸºäºTransformerçš„ç¥ç»æœºå™¨ç¿»è¯‘æ–¹æ³•åœ¨é«˜èµ„æºè¯­è¨€ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ä½èµ„æºè¯­è¨€å¦‚é”¡å°”èµ«ç‰¹è¯­ä»ç„¶ç ”ç©¶ä¸è¶³ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥å­ŸåŠ æ‹‰è¯­åˆ°é”¡å°”èµ«ç‰¹è¯­ç¿»è¯‘è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œæ¢ç´¢é€‚ç”¨äºä½èµ„æºè¯­è¨€çš„æœºå™¨ç¿»è¯‘æ–¹æ³•ã€‚

**Method:** æœ¬ç ”ç©¶é‡‡ç”¨å¤šè¯­è¨€Transformeræ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¸é›¶æ ·æœ¬å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹æ¯”åˆ†æã€‚å…·ä½“ä½¿ç”¨äº†mBART-50å’ŒMarianMTç­‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œé’ˆå¯¹å­ŸåŠ æ‹‰è¯­åˆ°é”¡å°”èµ«ç‰¹è¯­çš„ç¿»è¯‘ä»»åŠ¡è¿›è¡Œä¸“é—¨çš„é€‚åº”æ€§è®­ç»ƒã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºå¾®è°ƒæ¨¡å‹åœ¨ç¿»è¯‘æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬å¤§è¯­è¨€æ¨¡å‹ã€‚mBART-50æ¨¡å‹åœ¨ç¿»è¯‘å……åˆ†æ€§æ–¹é¢å–å¾—äº†æœ€é«˜å¾—åˆ†ï¼Œè€ŒMarianMTæ¨¡å‹åœ¨å­—ç¬¦çº§ä¿çœŸåº¦æ–¹é¢è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå±•ç°äº†ä¸åŒæ¨¡å‹æ¶æ„åœ¨ç‰¹å®šç¿»è¯‘è´¨é‡ç»´åº¦ä¸Šçš„ä¼˜åŠ¿ã€‚

**Conclusion:** ç ”ç©¶å¼ºè°ƒäº†ä»»åŠ¡ç‰¹å®šé€‚åº”å¯¹äºä½èµ„æºè¯­è¨€æœºå™¨ç¿»è¯‘çš„é‡è¦æ€§ï¼Œè¯æ˜äº†å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹åœ¨æå‡ä½èµ„æºè¯­è¨€ç¿»è¯‘è´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›å‘ç°ä¸ºæ„å»ºåŒ…å®¹æ€§è¯­è¨€æŠ€æœ¯æä¾›äº†é‡è¦å‚è€ƒï¼Œæ¨åŠ¨äº†é¢å‘ä»£è¡¨æ€§ä¸è¶³è¯­è¨€çš„æœºå™¨ç¿»è¯‘ç ”ç©¶è¿›å±•ã€‚

---

#### ğŸ“„ Abstract
Machine Translation (MT) has advanced from rule-based and statistical methods
to neural approaches based on the Transformer architecture. While these methods
have achieved impressive results for high-resource languages, low-resource
varieties such as Sylheti remain underexplored. In this work, we investigate
Bengali-to-Sylheti translation by fine-tuning multilingual Transformer models
and comparing them with zero-shot large language models (LLMs). Experimental
results demonstrate that fine-tuned models significantly outperform LLMs, with
mBART-50 achieving the highest translation adequacy and MarianMT showing the
strongest character-level fidelity. These findings highlight the importance of
task-specific adaptation for underrepresented languages and contribute to
ongoing efforts toward inclusive language technologies.


### [37] [Tibetan Language and AI: A Comprehensive Survey of Resources, Methods and Challenges](https://arxiv.org/abs/2510.19144)
*Cheng Huang, Nyima Tashi, Fan Gao, Yutong Liu, Jiahao Li, Hao Tian, Siyang Jiang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Jin Zhang, Xiao Feng, Hao Wang, Jie Tang, Guojie Tang, Xiangxiang Wang, Jia Zhang, Tsengdar Lee, Yongbin Yu*

#### ğŸ§© TL;DR
æœ¬æ–‡å¯¹è—è¯­äººå·¥æ™ºèƒ½ç ”ç©¶ç°çŠ¶è¿›è¡Œå…¨é¢è°ƒæŸ¥ï¼Œç³»ç»Ÿæ¢³ç†äº†æ–‡æœ¬å’Œè¯­éŸ³æ•°æ®èµ„æºã€NLPä»»åŠ¡ã€æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«åŠå¤§è¯­è¨€æ¨¡å‹ç­‰é¢†åŸŸçš„å‘å±•ï¼Œæ—¨åœ¨ä¸ºä½èµ„æºè¯­è¨€çš„AIç”Ÿæ€ç³»ç»Ÿå»ºè®¾æä¾›åŸºç¡€å‚è€ƒã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è—è¯­ä½œä¸ºäºšæ´²ä¸»è¦ä½èµ„æºè¯­è¨€ï¼Œå…·æœ‰ç‹¬ç‰¹çš„è¯­è¨€å’Œç¤¾ä¼šæ–‡åŒ–ç‰¹å¾ï¼Œä½†ç”±äºç¼ºä¹å¯è®¿é—®çš„æ•°æ®èµ„æºã€æ ‡å‡†åŒ–åŸºå‡†å’Œä¸“ç”¨å·¥å…·ï¼Œåœ¨AIç ”ç©¶ä¸­å—åˆ°æœ‰é™å…³æ³¨ï¼Œæœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚

**Method:** é‡‡ç”¨ç³»ç»Ÿæ€§è°ƒæŸ¥æ–¹æ³•ï¼Œå¯¹ç°æœ‰æ•°æ®é›†å’Œå·¥å…·è¿›è¡Œåˆ†ç±»æ•´ç†ï¼Œè¯„ä¼°ä¸åŒä»»åŠ¡ä¸­ä½¿ç”¨çš„æ–¹æ³•ï¼Œå¹¶åœ¨å¯èƒ½çš„æƒ…å†µä¸‹è¿›è¡Œæ€§èƒ½æ¯”è¾ƒåˆ†æã€‚

**Result:** ç ”ç©¶è¯†åˆ«å‡ºæ•°æ®ç¨€ç–æ€§ã€æ­£å­—æ³•å˜å¼‚å’Œç»Ÿä¸€è¯„ä¼°æŒ‡æ ‡ç¼ºä¹ç­‰æŒç»­ç“¶é¢ˆï¼ŒåŒæ—¶æ¢è®¨äº†è·¨è¯­è¨€è¿ç§»ã€å¤šæ¨¡æ€å­¦ä¹ å’Œç¤¾åŒºé©±åŠ¨èµ„æºåˆ›å»ºçš„æ½œåŠ›ã€‚

**Conclusion:** æœ¬è°ƒæŸ¥ä¸ºæœªæ¥è—è¯­AIç ”ç©¶æä¾›äº†åŸºç¡€æ€§å‚è€ƒï¼Œé¼“åŠ±é€šè¿‡åä½œåŠªåŠ›ä¸ºä½èµ„æºè¯­è¨€æ„å»ºåŒ…å®¹å’Œå¯æŒç»­çš„AIç”Ÿæ€ç³»ç»Ÿï¼Œæ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Tibetan, one of the major low-resource languages in Asia, presents unique
linguistic and sociocultural characteristics that pose both challenges and
opportunities for AI research. Despite increasing interest in developing AI
systems for underrepresented languages, Tibetan has received limited attention
due to a lack of accessible data resources, standardized benchmarks, and
dedicated tools. This paper provides a comprehensive survey of the current
state of Tibetan AI in the AI domain, covering textual and speech data
resources, NLP tasks, machine translation, speech recognition, and recent
developments in LLMs. We systematically categorize existing datasets and tools,
evaluate methods used across different tasks, and compare performance where
possible. We also identify persistent bottlenecks such as data sparsity,
orthographic variation, and the lack of unified evaluation metrics.
Additionally, we discuss the potential of cross-lingual transfer, multi-modal
learning, and community-driven resource creation. This survey aims to serve as
a foundational reference for future work on Tibetan AI research and encourages
collaborative efforts to build an inclusive and sustainable AI ecosystem for
low-resource languages.


### [38] [KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints](https://arxiv.org/abs/2510.19316)
*Kailin Jiang, Hongbo Jiang, Ning Jiang, Zhi Gao, Jinhe Bi, Yuchen Ren, Bin Li, Yuntao Du, Lei Liu, Qing Li*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºKOREæ–¹æ³•ï¼Œé€šè¿‡çŸ¥è¯†å¯¼å‘å¢å¼ºå’Œçº¦æŸçš„ååŒæœºåˆ¶ï¼Œè§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨çŸ¥è¯†æ³¨å…¥è¿‡ç¨‹ä¸­é¢ä¸´çš„æ–°çŸ¥è¯†å­¦ä¹ å›°éš¾ä¸ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå®ç°äº†å‡†ç¡®çš„çŸ¥è¯†é€‚åº”ä¸å¼ºå¤§çš„çŸ¥è¯†ä¿ç•™ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨é¢„è®­ç»ƒæƒé‡ä¸­ç¼–ç äº†å¤§é‡äº‹å®çŸ¥è¯†ï¼Œä½†å…¶çŸ¥è¯†ä¿æŒé™æ€ä¸”æœ‰é™ï¼Œæ— æ³•è·Ÿä¸Šç°å®ä¸–ç•Œçš„å‘å±•ï¼Œè¿™é˜»ç¢äº†æŒç»­çŸ¥è¯†è·å–ã€‚ç°æœ‰æ–¹æ³•åœ¨å­¦ä¹ æ–°çŸ¥è¯†æ—¶å¾€å¾€å›°éš¾ï¼Œå¹¶é­å—ç¾éš¾æ€§é—å¿˜ï¼Œå› æ­¤éœ€è¦è§£å†³çŸ¥è¯†é€‚åº”å’ŒçŸ¥è¯†ä¿ç•™çš„åŒé‡æŒ‘æˆ˜ã€‚

**Method:** KOREæ–¹æ³•åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šçŸ¥è¯†å¯¼å‘å¢å¼ºè‡ªåŠ¨å°†å•ä¸ªçŸ¥è¯†é¡¹è½¬æ¢ä¸ºç»“æ„åŒ–ã€å…¨é¢çš„çŸ¥è¯†è¡¨ç¤ºï¼Œç¡®ä¿æ¨¡å‹å‡†ç¡®å­¦ä¹ æ–°çŸ¥è¯†ï¼›çŸ¥è¯†çº¦æŸæœºåˆ¶å°†å…ˆå‰çŸ¥è¯†å­˜å‚¨åœ¨LMMçº¿æ€§å±‚æ¿€æ´»çš„åæ–¹å·®çŸ©é˜µä¸­ï¼Œå¹¶é€šè¿‡å°†åŸå§‹æƒé‡æŠ•å½±åˆ°çŸ©é˜µé›¶ç©ºé—´æ¥åˆå§‹åŒ–é€‚é…å™¨ï¼Œå®šä¹‰æœ€å°åŒ–ä¸å…ˆå‰çŸ¥è¯†å¹²æ‰°çš„å¾®è°ƒæ–¹å‘ã€‚

**Result:** åœ¨å¤šç§LMMæ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒKOREåœ¨LLaVA-v1.5-7Bã€LLaVA-v1.5-13Bå’ŒQwen2.5-VL-7Bç­‰æ¨¡å‹ä¸Šå®ç°äº†ä¼˜è¶Šçš„æ–°çŸ¥è¯†æ³¨å…¥æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆç¼“è§£äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚

**Conclusion:** KOREé€šè¿‡ååŒçš„çŸ¥è¯†å¢å¼ºå’Œçº¦æŸæœºåˆ¶ï¼Œä¸ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æœ‰æ•ˆçš„æŒç»­å­¦ä¹ è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨æ³¨å…¥æ–°çŸ¥è¯†çš„åŒæ—¶ä¿æŒåŸæœ‰çŸ¥è¯†ï¼Œä¸ºæ¨¡å‹çš„çŸ¥è¯†æ›´æ–°å’Œèƒ½åŠ›æ‰©å±•æä¾›äº†é‡è¦æŠ€æœ¯è·¯å¾„ã€‚

---

#### ğŸ“„ Abstract
Large Multimodal Models encode extensive factual knowledge in their
pre-trained weights. However, its knowledge remains static and limited, unable
to keep pace with real-world developments, which hinders continuous knowledge
acquisition. Effective knowledge injection thus becomes critical, involving two
goals: knowledge adaptation (injecting new knowledge) and knowledge retention
(preserving old knowledge). Existing methods often struggle to learn new
knowledge and suffer from catastrophic forgetting. To address this, we propose
KORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints
for injecting new knowledge into large multimodal models while preserving old
knowledge. Unlike general text or image data augmentation, KORE automatically
converts individual knowledge items into structured and comprehensive knowledge
to ensure that the model accurately learns new knowledge, enabling accurate
adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix
of LMM's linear layer activations and initializes the adapter by projecting the
original weights into the matrix's null space, defining a fine-tuning direction
that minimizes interference with previous knowledge, enabling powerful
retention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B,
LLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new
knowledge injection performance and effectively mitigates catastrophic
forgetting.


### [39] [M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2510.19358)
*Yejin Kwon, Taewoo Kang, Hyunsoo Yoon, Changouk Kim*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†M3-SLUï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤šè¯´è¯äººå¤šè½®å¯¹è¯ç†è§£çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨è¯´è¯äººå½’å±æ¨ç†èƒ½åŠ›ä¸Šçš„å…³é”®ç¼ºé™·ã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡12,000ä¸ªéªŒè¯å®ä¾‹ï¼Œæ¶µç›–ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼šè¯´è¯äººå½’å±é—®ç­”å’Œè¯è¯­åŒ¹é…è¯´è¯äººå½’å±ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å°½ç®¡å½“å‰æ¨¡å‹åœ¨è¯­éŸ³å’Œæ–‡æœ¬ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è¯´è¯äººå½’å±æ¨ç†èƒ½åŠ›ä¸Šä»å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå³éš¾ä»¥ç†è§£è‡ªç„¶å¯¹è¯ä¸­è°åœ¨ä½•æ—¶è¯´äº†ä»€ä¹ˆã€‚ç°æœ‰åŸºå‡†ç¼ºä¹å¯¹å¤šè¯´è¯äººå¤šè½®å¯¹è¯ä¸­è¯´è¯äººæ„ŸçŸ¥ç†è§£èƒ½åŠ›çš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚

**Method:** M3-SLUåŸºäºå››ä¸ªå¼€æ”¾è¯­æ–™åº“æ„å»ºï¼ŒåŒ…å«é…å¯¹éŸ³é¢‘ã€è½¬å½•æ–‡æœ¬å’Œå…ƒæ•°æ®çš„éªŒè¯å®ä¾‹ã€‚åŸºå‡†åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼šè¯´è¯äººå½’å±é—®ç­”å’Œè¯è¯­åŒ¹é…è¯´è¯äººå½’å±ã€‚è¯„ä¼°é‡‡ç”¨çº§è”æµæ°´çº¿å’Œç«¯åˆ°ç«¯MLLMsçš„åŸºçº¿ç»“æœï¼Œä½¿ç”¨LLM-as-Judgeå’Œå‡†ç¡®ç‡æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹èƒ½å¤Ÿè¾ƒå¥½åœ°æ•æ‰å¯¹è¯å†…å®¹ï¼Œä½†åœ¨è¯†åˆ«è¯´è¯äººèº«ä»½æ–¹é¢è¡¨ç°ä¸ä½³ã€‚åŸºå‡†æµ‹è¯•æ­ç¤ºäº†æ¨¡å‹åœ¨è¯´è¯äººæ„ŸçŸ¥å¯¹è¯ç†è§£èƒ½åŠ›ä¸Šçš„å…³é”®å·®è·ï¼Œä¸ºå¤šæ¨¡æ€ç†è§£ç ”ç©¶æä¾›äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯„ä¼°æ ‡å‡†ã€‚

**Conclusion:** M3-SLUä½œä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ï¼Œå°†æ¨åŠ¨è¯´è¯äººæ„ŸçŸ¥å¤šæ¨¡æ€ç†è§£ç ”ç©¶çš„å‘å±•ã€‚ç ”ç©¶å¼ºè°ƒäº†å¼€å‘èƒ½å¤ŸåŒæ—¶ç†è§£å¯¹è¯å†…å®¹å’Œè¯´è¯äººå…³ç³»çš„æ¨¡å‹çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿçš„å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
We present M3-SLU, a new multimodal large language model (MLLM) benchmark for
evaluating multi-speaker, multi-turn spoken language understanding. While
recent models show strong performance in speech and text comprehension, they
still struggle with speaker-attributed reasoning, the ability to understand who
said what and when in natural conversations. M3-SLU is built from four open
corpora (CHiME-6, MELD, MultiDialog, and AMI) and comprises over 12,000
validated instances with paired audio, transcripts, and metadata. It includes
two tasks: (1) Speaker-Attributed Question Answering and (2) Speaker
Attribution via Utterance Matching. We provide baseline results for both
cascaded pipelines and end-to-end MLLMs, evaluated using an LLM-as-Judge and
accuracy metrics. Results show that while models can capture what was said,
they often fail to identify who said it, revealing a key gap in speaker-aware
dialogue understanding. M3-SLU offers as a challenging benchmark to advance
research in speaker-aware multimodal understanding.


### [40] [Local Obfuscation by GLINER for Impartial Context Aware Lineage: Development and evaluation of PII Removal system](https://arxiv.org/abs/2510.19346)
*Prakrithi Shivaprakash, Lekhansh Shukla, Animesh Mukherjee, Prabhat Chand, Pratima Murthy*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†LOGICALç³»ç»Ÿï¼Œä¸€ç§åŸºäºå¾®è°ƒGLiNERæ¨¡å‹çš„é«˜æ•ˆæœ¬åœ°éƒ¨ç½²PIIç§»é™¤æ–¹æ¡ˆï¼Œåœ¨ä¸´åºŠç¬”è®°å»æ ‡è¯†åŒ–ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒæä¾›äº†å‡†ç¡®ã€å®‰å…¨ä¸”è®¡ç®—é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç”µå­å¥åº·è®°å½•ä¸­ä¸ªäººèº«ä»½ä¿¡æ¯çš„ç§»é™¤å¯¹äºç ”ç©¶å’ŒAIå¼€å‘è‡³å…³é‡è¦ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜è®¡ç®—æˆæœ¬å’ŒåŸºäºAPIæœåŠ¡çš„æ•°æ®éšç§é£é™©é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯ä¸´åºŠç¯å¢ƒä¸­çš„éšç§ä¿æŠ¤éœ€æ±‚ã€‚

**Method:** å¼€å‘äº†LOGICALç³»ç»Ÿï¼ŒåŸºäºå¾®è°ƒçš„GLiNERæ¨¡å‹è¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼Œå®šä¹‰äº†ä¹ä¸ªPIIç±»åˆ«ï¼Œä½¿ç”¨2849ä¸ªæ–‡æœ¬å®ä¾‹å¯¹modern-gliner-bi-large-v1.0æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨376ä¸ªæµ‹è¯•å®ä¾‹ä¸Šä½¿ç”¨å­—ç¬¦çº§ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°è¿›è¡Œè¯„ä¼°ã€‚

**Result:** å¾®è°ƒGLiNERæ¨¡å‹å–å¾—äº†å“è¶Šæ€§èƒ½ï¼Œæ€»ä½“å¾®å¹³å‡F1åˆ†æ•°è¾¾åˆ°0.980ï¼Œæ˜¾è‘—ä¼˜äºGemini-Pro-2.5çš„0.845ï¼ŒLOGICALç³»ç»Ÿæ­£ç¡®æ¸…ç†äº†95%çš„æ–‡æ¡£ï¼Œè€Œæ¬¡ä¼˜è§£å†³æ–¹æ¡ˆä»…è¾¾åˆ°64%ï¼Œä¸”æ¨¡å‹åœ¨æ ‡å‡†ç¬”è®°æœ¬ç”µè„‘ä¸Šæ— éœ€ä¸“ç”¨GPUå³å¯é«˜æ•ˆè¿è¡Œã€‚

**Conclusion:** å¾®è°ƒçš„ä¸“ä¸šåŒ–Transformeræ¨¡å‹å¦‚GLiNERä¸ºä¸´åºŠç¬”è®°PIIç§»é™¤æä¾›äº†å‡†ç¡®ã€è®¡ç®—é«˜æ•ˆä¸”å®‰å…¨çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™ç§"æºå¤´æ¸…ç†"æ–¹æ³•æ˜¯èµ„æºå¯†é›†å‹LLMsçš„å®ç”¨æ›¿ä»£æ–¹æ¡ˆï¼Œç‰¹åˆ«é€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒï¼Œä½†2%çš„å®ä½“çº§å‡é˜´æ€§ç‡å¼ºè°ƒäº†åœ¨æ‰€æœ‰æµ‹è¯•ç³»ç»Ÿä¸­éƒ½éœ€è¦äººå·¥éªŒè¯çš„å¿…è¦æ€§ã€‚

---

#### ğŸ“„ Abstract
Removing Personally Identifiable Information (PII) from clinical notes in
Electronic Health Records (EHRs) is essential for research and AI development.
While Large Language Models (LLMs) are powerful, their high computational costs
and the data privacy risks of API-based services limit their use, especially in
low-resource settings. To address this, we developed LOGICAL (Local Obfuscation
by GLINER for Impartial Context-Aware Lineage), an efficient, locally
deployable PII removal system built on a fine-tuned Generalist and Lightweight
Named Entity Recognition (GLiNER) model. We used 1515 clinical documents from a
psychiatric hospital's EHR system. We defined nine PII categories for removal.
A modern-gliner-bi-large-v1.0 model was fine-tuned on 2849 text instances and
evaluated on a test set of 376 instances using character-level precision,
recall, and F1-score. We compared its performance against Microsoft Azure NER,
Microsoft Presidio, and zero-shot prompting with Gemini-Pro-2.5 and
Llama-3.3-70B-Instruct. The fine-tuned GLiNER model achieved superior
performance, with an overall micro-average F1-score of 0.980, significantly
outperforming Gemini-Pro-2.5 (F1-score: 0.845). LOGICAL correctly sanitised 95%
of documents completely, compared to 64% for the next-best solution. The model
operated efficiently on a standard laptop without a dedicated GPU. However, a
2% entity-level false negative rate underscores the need for human-in-the-loop
validation across all tested systems. Fine-tuned, specialised transformer
models like GLiNER offer an accurate, computationally efficient, and secure
solution for PII removal from clinical notes. This "sanitisation at the source"
approach is a practical alternative to resource-intensive LLMs, enabling the
creation of de-identified datasets for research and AI development while
preserving data privacy, particularly in resource-constrained environments.


### [41] [The Massive Legal Embedding Benchmark (MLEB)](https://arxiv.org/abs/2510.19365)
*Umar Butler, Abdur-Rahman Butler, Adrian Lucas Malec*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MLEBï¼ˆå¤§è§„æ¨¡æ³•å¾‹åµŒå…¥åŸºå‡†ï¼‰ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§ã€æœ€å¤šæ ·åŒ–ä¸”æœ€å…¨é¢çš„å¼€æºæ³•å¾‹ä¿¡æ¯æ£€ç´¢åŸºå‡†ï¼ŒåŒ…å«åä¸ªä¸“å®¶æ ‡æ³¨çš„æ•°æ®é›†ï¼Œæ¶µç›–å¤šä¸ªå¸æ³•ç®¡è¾–åŒºã€æ–‡æ¡£ç±»å‹å’Œä»»åŠ¡ç±»å‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¼€æºæ³•å¾‹ä¿¡æ¯æ£€ç´¢é¢†åŸŸå­˜åœ¨é¢†åŸŸå’Œå¸æ³•ç®¡è¾–åŒºè¦†ç›–ä¸è¶³çš„é—®é¢˜ï¼Œç¼ºä¹å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„åŸºå‡†æ•°æ®é›†æ¥æ”¯æŒæ³•å¾‹AIç³»ç»Ÿçš„å…¨é¢è¯„ä¼°å’Œæ¯”è¾ƒã€‚

**Method:** æ„å»ºäº†åŒ…å«åä¸ªä¸“å®¶æ ‡æ³¨æ•°æ®é›†çš„ç»¼åˆåŸºå‡†ï¼Œæ¶µç›–ç¾å›½ã€è‹±å›½ã€æ¬§ç›Ÿã€æ¾³å¤§åˆ©äºšã€çˆ±å°”å…°å’Œæ–°åŠ å¡ç­‰å¤šä¸ªå¸æ³•ç®¡è¾–åŒºï¼Œæ–‡æ¡£ç±»å‹åŒ…æ‹¬æ¡ˆä¾‹ã€ç«‹æ³•ã€ç›‘ç®¡æŒ‡å—ã€åˆåŒå’Œæ–‡çŒ®ï¼Œä»»åŠ¡ç±»å‹åŒ…æ‹¬æœç´¢ã€é›¶æ ·æœ¬åˆ†ç±»å’Œé—®ç­”ã€‚

**Result:** MLEBæ˜¯å½“å‰æœ€å¤§ã€æœ€å¤šæ ·åŒ–çš„æ³•å¾‹ä¿¡æ¯æ£€ç´¢åŸºå‡†ï¼Œå…¶ä¸­ä¸ƒä¸ªæ•°æ®é›†æ˜¯æ–°å»ºçš„ä»¥å¡«è¡¥é¢†åŸŸç©ºç™½ï¼Œæä¾›äº†å¯å¤ç°è¯„ä¼°æ‰€éœ€çš„å®Œæ•´ä»£ç ã€ç»“æœå’Œæ•°æ®ã€‚

**Conclusion:** MLEBä¸ºæ³•å¾‹AIç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œä¿ƒè¿›äº†æ³•å¾‹ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿçš„å¯å¤ç°æ¯”è¾ƒå’Œå‘å±•ï¼Œå¡«è¡¥äº†å¼€æºæ³•å¾‹åŸºå‡†çš„é‡è¦ç©ºç™½ã€‚

---

#### ğŸ“„ Abstract
We present the Massive Legal Embedding Benchmark (MLEB), the largest, most
diverse, and most comprehensive open-source benchmark for legal information
retrieval to date. MLEB consists of ten expert-annotated datasets spanning
multiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore),
document types (cases, legislation, regulatory guidance, contracts, and
literature), and task types (search, zero-shot classification, and question
answering). Seven of the datasets in MLEB were newly constructed in order to
fill domain and jurisdictional gaps in the open-source legal information
retrieval landscape. We document our methodology in building MLEB and creating
the new constituent datasets, and release our code, results, and data openly to
assist with reproducible evaluations.


### [42] [Modeling Turn-Taking with Semantically Informed Gestures](https://arxiv.org/abs/2510.19350)
*Varsha Suresh, M. Hamza Mughal, Christian Theobalt, Vera Demberg*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥DnD Gesture++æ•°æ®é›†å’Œæ··åˆä¸“å®¶æ¡†æ¶ï¼Œè¯æ˜äº†è¯­ä¹‰æ‰‹åŠ¿åœ¨å¤šæ¨¡æ€å¯¹è¯è½®æ¬¡é¢„æµ‹ä¸­çš„è¡¥å……ä½œç”¨ï¼Œæ˜¾è‘—æå‡äº†è½®æ¬¡è½¬æ¢é¢„æµ‹æ€§èƒ½ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** äººç±»å¯¹è¯ä¸­åˆ©ç”¨å¤šæ¨¡æ€çº¿ç´¢ï¼ˆå¦‚è¯­éŸ³ã€æ‰‹åŠ¿å’Œæ³¨è§†ï¼‰æ¥ç®¡ç†è½®æ¬¡è½¬æ¢ï¼Œè™½ç„¶è¯­è¨€å’Œå£°å­¦ç‰¹å¾å…·æœ‰ä¿¡æ¯æ€§ï¼Œä½†æ‰‹åŠ¿æä¾›äº†è¡¥å……æ€§çº¿ç´¢ï¼Œå½“å‰ç ”ç©¶æ—¨åœ¨æ¢ç´¢æ‰‹åŠ¿åœ¨è½®æ¬¡è½¬æ¢å»ºæ¨¡ä¸­çš„å…·ä½“ä½œç”¨ã€‚

**Method:** ç ”ç©¶æ‰©å±•äº†å¤šå‚ä¸è€…DnD Gestureè¯­æ–™åº“ä¸ºDnD Gesture++ï¼Œæ–°å¢2,663ä¸ªè¯­ä¹‰æ‰‹åŠ¿æ ‡æ³¨ï¼Œæ¶µç›–å›¾æ ‡æ€§ã€éšå–»æ€§ã€æŒ‡ç¤ºæ€§å’Œè¯è¯­æ€§æ‰‹åŠ¿ç±»å‹ï¼Œå¹¶é‡‡ç”¨æ··åˆä¸“å®¶æ¡†æ¶æ•´åˆæ–‡æœ¬ã€éŸ³é¢‘å’Œæ‰‹åŠ¿ç‰¹å¾è¿›è¡Œè½®æ¬¡è½¬æ¢é¢„æµ‹å»ºæ¨¡ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥è¯­ä¹‰å¼•å¯¼çš„æ‰‹åŠ¿ç‰¹å¾ç›¸æ¯”åŸºçº¿æ¨¡å‹å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ï¼ŒéªŒè¯äº†æ‰‹åŠ¿åœ¨å¤šæ¨¡æ€è½®æ¬¡è½¬æ¢é¢„æµ‹ä¸­çš„è¡¥å……ä»·å€¼ã€‚

**Conclusion:** è¯¥ç ”ç©¶è¯å®äº†è¯­ä¹‰æ‰‹åŠ¿ä½œä¸ºå¤šæ¨¡æ€å¯¹è¯åˆ†æçš„é‡è¦è¡¥å……çº¿ç´¢ï¼Œä¸ºæ›´è‡ªç„¶çš„äººæœºäº¤äº’ç³»ç»Ÿå¼€å‘æä¾›äº†ç†è®ºä¾æ®ï¼Œæœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢ä¸åŒç±»å‹æ‰‹åŠ¿åœ¨å¯¹è¯ç®¡ç†ä¸­çš„å…·ä½“ä½œç”¨æœºåˆ¶ã€‚

---

#### ğŸ“„ Abstract
In conversation, humans use multimodal cues, such as speech, gestures, and
gaze, to manage turn-taking. While linguistic and acoustic features are
informative, gestures provide complementary cues for modeling these
transitions. To study this, we introduce DnD Gesture++, an extension of the
multi-party DnD Gesture corpus enriched with 2,663 semantic gesture annotations
spanning iconic, metaphoric, deictic, and discourse types. Using this dataset,
we model turn-taking prediction through a Mixture-of-Experts framework
integrating text, audio, and gestures. Experiments show that incorporating
semantically guided gestures yields consistent performance gains over
baselines, demonstrating their complementary role in multimodal turn-taking.


### [43] [ToMMeR -- Efficient Entity Mention Detection from Large Language Models](https://arxiv.org/abs/2510.19410)
*Victor Morand, Nadi Tomeh, Josiane Mothe, Benjamin Piwowarski*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ToMMeRï¼Œä¸€ä¸ªè½»é‡çº§æ¨¡å‹ï¼Œé€šè¿‡æ¢æµ‹å¤§è¯­è¨€æ¨¡å‹æ—©æœŸå±‚æ¥è¯†åˆ«å®ä½“æåŠï¼Œåœ¨13ä¸ªNERåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†93%çš„é›¶æ ·æœ¬å¬å›ç‡ï¼Œè¯æ˜äº†å®ä½“è¡¨ç¤ºè‡ªç„¶å­˜åœ¨äºTransformeræ—©æœŸå±‚ä¸­ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å®ä½“æåŠæ£€æµ‹æ˜¯ä¿¡æ¯æå–çš„åŸºç¡€ä»»åŠ¡ï¼Œä½†å·²çŸ¥æ˜¯æ€§èƒ½ç“¶é¢ˆï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹æ—©æœŸå±‚æ˜¯å¦å·²ç»å…·å¤‡å®ä½“æ£€æµ‹èƒ½åŠ›ï¼Œä»¥åŠèƒ½å¦é€šè¿‡è½»é‡çº§æ–¹æ³•é«˜æ•ˆæå–è¿™äº›ç»“æ„åŒ–è¡¨ç¤ºã€‚

**Method:** æå‡ºäº†ToMMeRæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå‚æ•°é‡å°äº30ä¸‡çš„è½»é‡çº§æ¨¡å‹ï¼Œé€šè¿‡æ¢æµ‹å¤§è¯­è¨€æ¨¡å‹çš„æ—©æœŸå±‚æ¥æå–å®ä½“æåŠæ£€æµ‹èƒ½åŠ›ï¼Œå¹¶å¯ä»¥æ‰©å±•æ·»åŠ è·¨åº¦åˆ†ç±»å¤´ä»¥æ”¯æŒå‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ã€‚

**Result:** åœ¨13ä¸ªNERåŸºå‡†æµ‹è¯•ä¸­ï¼ŒToMMeRå®ç°äº†93%çš„é›¶æ ·æœ¬å¬å›ç‡ï¼Œä½¿ç”¨LLMä½œä¸ºåˆ¤æ–­å™¨æ—¶ç²¾åº¦è¶…è¿‡90%ï¼Œè·¨æ¨¡å‹åˆ†ææ˜¾ç¤ºä¸åŒæ¶æ„çš„æ¨¡å‹åœ¨æåŠè¾¹ç•Œä¸Šé«˜åº¦ä¸€è‡´ï¼Œæ‰©å±•åçš„ToMMeRåœ¨æ ‡å‡†åŸºå‡†ä¸Šè¾¾åˆ°80-87%çš„F1åˆ†æ•°ã€‚

**Conclusion:** ç ”ç©¶è¡¨æ˜ç»“æ„åŒ–å®ä½“è¡¨ç¤ºè‡ªç„¶å­˜åœ¨äºTransformeræ—©æœŸå±‚ä¸­ï¼Œå¯ä»¥é€šè¿‡æå°å‚æ•°é‡é«˜æ•ˆæ¢å¤ï¼Œä¸åŒæ¶æ„çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å®ä½“æåŠæ£€æµ‹ä¸Šè¡¨ç°å‡ºæ”¶æ•›æ€§ï¼Œè¿™ä¸ºè½»é‡çº§ä¿¡æ¯æå–ç³»ç»Ÿæä¾›äº†ç†è®ºåŸºç¡€ã€‚

---

#### ğŸ“„ Abstract
Identifying which text spans refer to entities -- mention detection -- is
both foundational for information extraction and a known performance
bottleneck. We introduce ToMMeR, a lightweight model (<300K parameters) probing
mention detection capabilities from early LLM layers. Across 13 NER benchmarks,
ToMMeR achieves 93\% recall zero-shot, with over 90\% precision using an LLM as
a judge showing that ToMMeR rarely produces spurious predictions despite high
recall. Cross-model analysis reveals that diverse architectures (14M-15B
parameters) converge on similar mention boundaries (DICE >75\%), confirming
that mention detection emerges naturally from language modeling. When extended
with span classification heads, ToMMeR achieves near SOTA NER performance
(80-87\% F1 on standard benchmarks). Our work provides evidence that structured
entity representations exist in early transformer layers and can be efficiently
recovered with minimal parameters.


### [44] [SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision](https://arxiv.org/abs/2510.19398)
*Yasser Hamidullah, Shakib Yazdani, Cennet Oguz, Josef van Genabith, Cristina EspaÃ±a-Bonet*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨è¯­è¨€æ— å…³çš„å¤šæ¨¡æ€åµŒå…¥æ¥ç›‘ç£æ‰‹è¯­ç¿»è¯‘çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¤šè¯­è¨€ç›®æ ‡å¢å¼ºå’Œè§†é¢‘çº§æ‰°åŠ¨æ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå®ç°äº†ç›´æ¥çš„å¤šè¯­è¨€æ‰‹è¯­ç¿»è¯‘ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ä¼ ç»Ÿæ‰‹è¯­ç¿»è¯‘é€šå¸¸ä½¿ç”¨å•ä¸€è¯­è¨€çš„æ–‡æœ¬è¿›è¡Œè®­ç»ƒï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œè·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›ï¼Œç°æœ‰æ–¹æ³•è™½ç„¶ä½¿ç”¨æ–‡æœ¬å¥å­åµŒå…¥æ›¿ä»£glossç›‘ç£ï¼Œä½†ä»å—é™äºç‰¹å®šè¯­è¨€å’Œæ¨¡æ€ã€‚

**Method:** é‡‡ç”¨åœ¨å¤šç§è¯­è¨€çš„æ–‡æœ¬å’Œè¯­éŸ³ä¸Šè®­ç»ƒçš„è¯­è¨€æ— å…³å¤šæ¨¡æ€åµŒå…¥æ¥ç›‘ç£æ‰‹è¯­ç¿»è¯‘ï¼Œå¹¶æå‡ºè€¦åˆå¢å¼ºæ–¹æ³•ï¼Œç»“åˆå¤šè¯­è¨€ç›®æ ‡å¢å¼ºï¼ˆç¿»è¯‘æˆå¤šç§è¯­è¨€ï¼‰å’Œè§†é¢‘çº§æ‰°åŠ¨æ¥æé«˜æ¨¡å‹é²æ£’æ€§ã€‚

**Result:** å®éªŒæ˜¾ç¤ºç›¸æ¯”ä»…ä½¿ç”¨æ–‡æœ¬å¥å­åµŒå…¥ç›‘ç£çš„æ–¹æ³•ï¼Œåœ¨BLEURTæŒ‡æ ‡ä¸Šè·å¾—ä¸€è‡´æå‡ï¼Œåœ¨ä½èµ„æºè®¾ç½®ä¸‹æ”¹è¿›æ›´ä¸ºæ˜¾è‘—ã€‚

**Conclusion:** è¯­è¨€æ— å…³åµŒå…¥ç›‘ç£ä¸è€¦åˆå¢å¼ºç›¸ç»“åˆï¼Œä¸ºä¼ ç»Ÿæ‰‹è¯­ç¿»è¯‘è®­ç»ƒæä¾›äº†å¯æ‰©å±•ä¸”è¯­ä¹‰é²æ£’çš„æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ã€‚

---

#### ğŸ“„ Abstract
Sign language translation (SLT) is typically trained with text in a single
spoken language, which limits scalability and cross-language generalization.
Earlier approaches have replaced gloss supervision with text-based sentence
embeddings, but up to now, these remain tied to a specific language and
modality. In contrast, here we employ language-agnostic, multimodal embeddings
trained on text and speech from multiple languages to supervise SLT, enabling
direct multilingual translation. To address data scarcity, we propose a coupled
augmentation method that combines multilingual target augmentations (i.e.
translations into many languages) with video-level perturbations, improving
model robustness. Experiments show consistent BLEURT gains over text-only
sentence embedding supervision, with larger improvements in low-resource
settings. Our results demonstrate that language-agnostic embedding supervision,
combined with coupled augmentation, provides a scalable and semantically robust
alternative to traditional SLT training.


### [45] [Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark](https://arxiv.org/abs/2510.19585)
*Yu Wu, Ke Shu, Jonas Fischer, Lidia Pivovarova, David Rosson, Eetu MÃ¤kelÃ¤, Mikko Tolonen*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºä»æ··åˆè¯­è¨€å†å²æ–‡æ¡£ä¸­æå–æ‹‰ä¸è¯­ç‰‡æ®µçš„æ–°ä»»åŠ¡ï¼Œé€šè¿‡è¯„ä¼°å¤§å‹åŸºç¡€æ¨¡å‹åœ¨724é¡µå¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å½“ä»£æ¨¡å‹èƒ½å¤Ÿå®ç°å¯é çš„æ‹‰ä¸è¯­æ£€æµ‹ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ä»å¸ƒå±€å¤šæ ·çš„æ··åˆè¯­è¨€å†å²æ–‡æ¡£ä¸­æå–æ‹‰ä¸è¯­ç‰‡æ®µè¿™ä¸€å°šæœªè¢«å……åˆ†æ¢ç´¢çš„ä»»åŠ¡ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ï¼Œä¸ºå†å²æ–‡çŒ®çš„æ•°å­—åŒ–å¤„ç†æä¾›äº†æ–°çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨å¤§å‹åŸºç¡€æ¨¡å‹ä½œä¸ºæ ¸å¿ƒæ–¹æ³•ï¼Œæ„å»ºäº†åŒ…å«724ä¸ªæ ‡æ³¨é¡µé¢çš„å¤šæ¨¡æ€æ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç³»ç»Ÿè¯„ä¼°äº†è¿™äº›æ¨¡å‹åœ¨æ‹‰ä¸è¯­ç‰‡æ®µæ£€æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°èƒ½åŠ›ã€‚

**Result:** å®éªŒç»“æœè¡¨æ˜ï¼Œå½“ä»£æ¨¡å‹èƒ½å¤Ÿå®ç°å¯é çš„æ‹‰ä¸è¯­æ£€æµ‹æ€§èƒ½ï¼Œåœ¨æ··åˆè¯­è¨€å†å²æ–‡æ¡£å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ•ˆæœï¼Œä¸ºç›¸å…³åº”ç”¨æä¾›äº†å®è¯æ”¯æŒã€‚

**Conclusion:** æœ¬ç ”ç©¶é¦–æ¬¡å…¨é¢åˆ†æäº†å¤§å‹åŸºç¡€æ¨¡å‹åœ¨æ‹‰ä¸è¯­ç‰‡æ®µæå–ä»»åŠ¡ä¸­çš„èƒ½åŠ›å’Œå±€é™æ€§ï¼Œä¸ºå†å²æ–‡æ¡£å¤„ç†é¢†åŸŸæä¾›äº†é‡è¦çš„åŸºå‡†å‚è€ƒï¼Œå¹¶è¯æ˜äº†è¯¥æŠ€æœ¯è·¯çº¿çš„å¯è¡Œæ€§ã€‚

---

#### ğŸ“„ Abstract
This paper presents a novel task of extracting Latin fragments from
mixed-language historical documents with varied layouts. We benchmark and
evaluate the performance of large foundation models against a multimodal
dataset of 724 annotated pages. The results demonstrate that reliable Latin
detection with contemporary models is achievable. Our study provides the first
comprehensive analysis of these models' capabilities and limits for this task.


### [46] [MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models](https://arxiv.org/abs/2510.19457)
*Kailin Jiang, Ning Jiang, Yuchen Ren, Yuchen Li, Yifan Gao, Jinhe Bi, Yunpu Ma, Qingqing Liu, Xianhao Wang, Yifan Jia, Hongbo Jiang, Yaocong Hu, Bin Li, Lei Liu, Yuntao Du*

#### ğŸ§© TL;DR
æœ¬æ–‡æå‡ºäº†MINEDåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æ—¶é—´æ•æ„ŸçŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œå‘ç°ç°æœ‰æ¨¡å‹åœ¨æ­¤æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¹¶é€šè¿‡çŸ¥è¯†ç¼–è¾‘æ–¹æ³•éªŒè¯äº†æ›´æ–°æ—¶é—´æ•æ„ŸçŸ¥è¯†çš„å¯è¡Œæ€§ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** ç°æœ‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€é¢„è®­ç»ƒç¼–ç äº†ä¸°å¯Œçš„çŸ¥è¯†ï¼Œä½†å…¶é™æ€è¡¨ç¤ºéš¾ä»¥ç»´æŒå¯¹æ—¶é—´æ•æ„ŸçŸ¥è¯†çš„å‡†ç¡®ç†è§£ï¼Œè€Œç°æœ‰åŸºå‡†æµ‹è¯•å—é™äºé™æ€è®¾è®¡ï¼Œæ— æ³•å……åˆ†è¯„ä¼°æ¨¡å‹çš„æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ã€‚

**Method:** ç ”ç©¶æ„å»ºäº†MINEDç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«6ä¸ªå…³é”®ç»´åº¦å’Œ11ä¸ªæŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œæ¶µç›–è®¤çŸ¥ã€æ„è¯†ã€å¯ä¿¡åº¦ã€ç†è§£ã€æ¨ç†å’Œé²æ£’æ€§ï¼Œä»ç»´åŸºç™¾ç§‘ä¸­ç”±ä¸“ä¸šæ ‡æ³¨è€…æ„å»ºäº†2,104ä¸ªæ—¶é—´æ•æ„ŸçŸ¥è¯†æ ·æœ¬ï¼Œæ¶µç›–å…­ç§çŸ¥è¯†ç±»å‹ã€‚

**Result:** è¯„ä¼°15ä¸ªå¹¿æ³›ä½¿ç”¨çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ˜¾ç¤ºï¼ŒGemini-2.5-Proè·å¾—æœ€é«˜å¹³å‡CEMåˆ†æ•°63.07ï¼Œè€Œå¤§å¤šæ•°å¼€æºæ¨¡å‹ä»ç¼ºä¹æ—¶é—´ç†è§£èƒ½åŠ›ï¼Œæ¨¡å‹åœ¨ç»„ç»‡çŸ¥è¯†ä¸Šè¡¨ç°æœ€ä½³ï¼Œåœ¨ä½“è‚²çŸ¥è¯†ä¸Šè¡¨ç°æœ€å¼±ï¼Œé€šè¿‡çŸ¥è¯†ç¼–è¾‘æ–¹æ³•éªŒè¯äº†æ¨¡å‹åœ¨å•æ¬¡ç¼–è¾‘åœºæ™¯ä¸‹èƒ½æœ‰æ•ˆæ›´æ–°çŸ¥è¯†ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ—¶é—´æ•æ„ŸçŸ¥è¯†ç†è§£æ–¹é¢çš„ç³»ç»Ÿæ€§ä¸è¶³ï¼Œè¯æ˜äº†çŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨æ›´æ–°æ¨¡å‹çŸ¥è¯†æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ”¹è¿›æ¨¡å‹çš„æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›æä¾›äº†é‡è¦åŸºå‡†å’Œæ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal
pre-training, yet their static representations struggle to maintain an accurate
understanding of time-sensitive factual knowledge. Existing benchmarks remain
constrained by static designs, inadequately evaluating LMMs' ability to
understand time-sensitive knowledge. To address this gap, we propose MINED, a
comprehensive benchmark that evaluates temporal awareness along 6 key
dimensions and 11 challenging tasks: cognition, awareness, trustworthiness,
understanding, reasoning, and robustness. MINED is constructed from Wikipedia
by two professional annotators, containing 2,104 time-sensitive knowledge
samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED
shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07,
while most open-source LMMs still lack time understanding ability. Meanwhile,
LMMs perform best on organization knowledge, whereas their performance is
weakest on sport. To address these challenges, we investigate the feasibility
of updating time-sensitive knowledge in LMMs through knowledge editing methods
and observe that LMMs can effectively update knowledge via knowledge editing
methods in single editing scenarios.


### [47] [Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition](https://arxiv.org/abs/2510.19471)
*Yuu Jinnai*

#### ğŸ§© TL;DR
è¯¥ç ”ç©¶éªŒè¯äº†åŸºäºé‡‡æ ·çš„æœ€å°è´å¶æ–¯é£é™©è§£ç åœ¨è¯­éŸ³è½¬æ–‡æœ¬ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå®éªŒè¡¨æ˜MBRè§£ç åœ¨å¤§å¤šæ•°è®¾ç½®ä¸‹ä¼˜äºä¼ ç»Ÿçš„æŸæœç´¢æ–¹æ³•ï¼Œä¸ºç¦»çº¿ASRå’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡æä¾›äº†æ›´é«˜ç²¾åº¦çš„è§£ç æ–¹æ¡ˆã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è™½ç„¶MBRè§£ç åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­å·²è¢«è¯æ˜ä¼˜äºæŸæœç´¢ï¼Œä½†åœ¨è¯­éŸ³è½¬æ–‡æœ¬ä»»åŠ¡ä¸­æŸæœç´¢ä»æ˜¯ä¸»æµå®è·µï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼ŒéªŒè¯MBRè§£ç åœ¨ASRå’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ½œåœ¨ä¼˜åŠ¿ã€‚

**Method:** ç ”ç©¶é‡‡ç”¨WhisperåŠå…¶è¡ç”Ÿæ¨¡å‹ï¼Œåœ¨è‹±è¯­å’Œæ—¥è¯­æ•°æ®é›†ä¸Šç³»ç»Ÿè¯„ä¼°MBRè§£ç åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸ä¼ ç»Ÿçš„æŸæœç´¢æ–¹æ³•è¿›è¡Œå¯¹æ¯”åˆ†æã€‚

**Result:** å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤§å¤šæ•°è¯„ä¼°è®¾ç½®ä¸­ï¼ŒMBRè§£ç çš„å‡†ç¡®ç‡å‡ä¼˜äºæŸæœç´¢ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è¯­éŸ³è½¬æ–‡æœ¬ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜ç²¾åº¦çš„ç¦»çº¿åº”ç”¨åœºæ™¯ä¸­ã€‚

**Conclusion:** MBRè§£ç ä¸ºç¦»çº¿ASRå’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„é«˜ç²¾åº¦è§£ç æ–¹æ³•ï¼Œæœªæ¥å¯è¿›ä¸€æ­¥æ¢ç´¢å…¶åœ¨æ›´å¤šè¯­è¨€å’Œæ¨¡å‹æ¶æ„ä¸­çš„é€‚ç”¨æ€§ï¼Œæ¨åŠ¨è¯­éŸ³å¤„ç†æŠ€æœ¯çš„å‘å±•ã€‚

---

#### ğŸ“„ Abstract
Recent work has shown that sample-based Minimum Bayes Risk (MBR) decoding
outperforms beam search in text-to-text generation tasks, such as machine
translation, text summarization, and image captioning. On the other hand, beam
search is the current practice for speech-to-text tasks such as automatic
speech recognition (ASR) and Speech Translation (ST). Given that MBR decoding
is effective in text-to-text generation tasks, it is reasonable to expect it to
also be effective for speech-to-text tasks. In this paper, we evaluate MBR
decoding for ASR and ST tasks on English and Japanese using Whisper and its
derivative models. We observe that the accuracy of MBR decoding outperforms
that of beam search in most of the experimental settings we have evaluated. The
results show that MBR decoding is a promising method for offline ASR and ST
tasks that require high accuracy. The code is available at
https://github.com/CyberAgentAILab/mbr-for-asr


### [48] [Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent](https://arxiv.org/abs/2510.19641)
*Yangshijie Zhang, Xinda Wang, Jialin Liu, Wenqiang Wang, Zhicong Ma, Xingxing Jia*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºé£æ ¼åŒ–æ–‡æœ¬çš„æ”»å‡»æ–¹æ³•SADï¼Œåˆ©ç”¨äººç±»ä¸NLPæ¨¡å‹åœ¨å­—ä½“æ ·å¼å¤„ç†ä¸Šçš„æ„ŸçŸ¥å·®å¼‚ï¼Œåœ¨æƒ…æ„Ÿåˆ†ç±»å’Œæœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ä¸­å®ç°äº†æœ‰æ•ˆçš„å¯¹æŠ—æ”»å‡»ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** éšç€ç¤¾äº¤åª’ä½“çš„å‘å±•ï¼Œç”¨æˆ·ä½¿ç”¨é£æ ¼åŒ–å­—ä½“å’Œå­—ä½“å¼è¡¨æƒ…ç¬¦å·è¡¨è¾¾ä¸ªæ€§ï¼Œè¿™äº›è§†è§‰ä¸Šå¸å¼•äººçš„æ–‡æœ¬å¯¹äººç±»å¯è¯»ä½†å¯¹NLPæ¨¡å‹æ„æˆæ½œåœ¨å¨èƒã€‚ç ”ç©¶å‘ç°äººç±»ä¸æ¨¡å‹åœ¨å¤„ç†è¿™äº›å­—ç¬¦æ—¶å­˜åœ¨æ„ŸçŸ¥å·®è·ï¼Œæ¨¡å‹å°†é£æ ¼åŒ–å­—ç¬¦è§†ä¸ºç‹¬ç«‹æ ‡è®°ï¼Œä»è€Œå¯¼è‡´å¹²æ‰°ã€‚

**Method:** æå‡ºäº†Style Attack Disguise (SAD)æ”»å‡»æ–¹æ³•ï¼Œè®¾è®¡äº†ä¸¤ç§è§„æ¨¡ï¼šè½»é‡ç‰ˆç”¨äºæŸ¥è¯¢æ•ˆç‡ï¼Œå¼ºåŒ–ç‰ˆç”¨äºå“è¶Šçš„æ”»å‡»æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é£æ ¼åŒ–æ–‡æœ¬åˆ›å»ºå¯¹æŠ—æ ·æœ¬ï¼Œé’ˆå¯¹ä¼ ç»Ÿæ¨¡å‹ã€å¤§è¯­è¨€æ¨¡å‹å’Œå•†ä¸šæœåŠ¡è¿›è¡Œæ”»å‡»ã€‚

**Result:** åœ¨æƒ…æ„Ÿåˆ†ç±»å’Œæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSADåœ¨å„ç§æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºå¼ºå¤§çš„æ”»å‡»æ€§èƒ½ã€‚ç ”ç©¶è¿˜å±•ç¤ºäº†SADå¯¹å¤šæ¨¡æ€ä»»åŠ¡ï¼ˆåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è¯­éŸ³ç”Ÿæˆï¼‰çš„æ½œåœ¨å¨èƒã€‚

**Conclusion:** è¯¥ç ”ç©¶æ­ç¤ºäº†é£æ ¼åŒ–æ–‡æœ¬åœ¨NLPç³»ç»Ÿä¸­çš„å®‰å…¨æ¼æ´ï¼Œå¼ºè°ƒäº†äººç±»ä¸AIç³»ç»Ÿæ„ŸçŸ¥å·®å¼‚å¸¦æ¥çš„å®‰å…¨é£é™©ã€‚SADæ–¹æ³•å±•ç¤ºäº†å¯¹æŠ—æ”»å‡»åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æ‰©å±•æ€§ï¼Œä¸ºæœªæ¥AIå®‰å…¨ç ”ç©¶æä¾›äº†é‡è¦æ–¹å‘ã€‚

---

#### ğŸ“„ Abstract
With social media growth, users employ stylistic fonts and font-like emoji to
express individuality, creating visually appealing text that remains
human-readable. However, these fonts introduce hidden vulnerabilities in NLP
models: while humans easily read stylistic text, models process these
characters as distinct tokens, causing interference. We identify this
human-model perception gap and propose a style-based attack, Style Attack
Disguise (SAD). We design two sizes: light for query efficiency and strong for
superior attack performance. Experiments on sentiment classification and
machine translation across traditional models, LLMs, and commercial services
demonstrate SAD's strong attack performance. We also show SAD's potential
threats to multimodal tasks including text-to-image and text-to-speech
generation.


### [49] [CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation](https://arxiv.org/abs/2510.19670)
*Hasan Akgul, Mari Eplik, Javier Rojas, Aina Binti Abdullah, Pieter van der Merwe*

#### ğŸ§© TL;DR
CoSense-LLMæ˜¯ä¸€ä¸ªè¾¹ç¼˜ä¼˜å…ˆæ¡†æ¶ï¼Œå°†è¿ç»­å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æµè½¬æ¢ä¸ºç´§å‡‘å¯éªŒè¯çš„è¯­ä¹‰ä»¤ç‰Œï¼Œå¹¶åœ¨ä¸¥æ ¼å»¶è¿Ÿã€èƒ½è€—ã€å¸¦å®½å’Œéšç§çº¦æŸä¸‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ååŒå·¥ä½œã€‚è¯¥æ¡†æ¶é€šè¿‡è½»é‡çº§ç¼–ç ã€æœ¬åœ°æ£€ç´¢ã€æ™ºèƒ½è·¯ç”±å’Œå®‰å…¨æ‰§è¡Œå®ç°äº†è¯­ä¹‰ç†è§£ã€éšç§ä¿æŠ¤å’Œå¯é¢„æµ‹å»¶è¿Ÿçš„ååŒä¼˜åŒ–ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³åœ¨å¹²æ‰°ç¯å¢ƒä¸­éƒ¨ç½²å¤§å‹æ¨¡å‹æ—¶é¢ä¸´çš„è¯­ä¹‰ç†è§£ã€éšç§ä¿æŠ¤å’Œå¯é¢„æµ‹å»¶è¿Ÿä¹‹é—´çš„å¹³è¡¡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¿ç»­å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æµæ—¶å¦‚ä½•æ»¡è¶³ä¸¥æ ¼çš„å»¶è¿Ÿã€èƒ½è€—ã€å¸¦å®½å’Œéšç§çº¦æŸã€‚

**Method:** CoSense-LLMåŒ…å«å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šSenseFusionè½»é‡çº§ç¼–ç å™¨å°†ä¼ æ„Ÿå™¨åµŒå…¥ä¸è¯­è¨€å¯¹é½å¹¶å‹ç¼©ä¸ºç¦»æ•£ä»£ç åºåˆ—ï¼›Edge-RAGæœ¬åœ°æ··åˆæ£€ç´¢å±‚åŸºäºç«™ç‚¹ç‰¹å®šç­–ç•¥è¿›è¡Œç”Ÿæˆï¼›PromptRouteræˆæœ¬æ„ŸçŸ¥ç­–ç•¥é€‰æ‹©è¾¹ç¼˜ç”Ÿæˆã€è¾¹ç¼˜æ£€ç´¢æˆ–ç´§å‡‘äº‘å‡çº§ï¼›Secure Executionå¯å®¡è®¡åˆ å‡è·¯å¾„ç¡®ä¿åŸå§‹æ³¢å½¢æ•°æ®ä¸ç¦»å¼€è®¾å¤‡ã€‚

**Result:** åœ¨å®¶åº­ã€åŠå…¬å®¤å’Œè¯Šæ‰€éƒ¨ç½²ä¸­ï¼ŒCoSense-LLMå®ç°äº†äºšç§’çº§ç«¯åˆ°ç«¯å»¶è¿Ÿï¼Œé€šè¿‡åå¥½æœ¬åœ°æ£€ç´¢å“åº”æ˜¾è‘—é™ä½äº†å±‚çº§é—´ä»¤ç‰Œå’Œå¸¦å®½æˆæœ¬ï¼ŒåŒæ—¶é€šè¿‡ä»…ä¼ è¾“ç¦»æ•£ä»£ç å’Œåˆ å‡å…ƒæ•°æ®ä¿æŠ¤éšç§ã€‚æ¶ˆèç ”ç©¶æ˜¾ç¤ºEdge-RAGæé«˜äº†äº‹å®ä¸€è‡´æ€§å¹¶å‡å°‘çŸ›ç›¾ï¼Œæ ¡å‡†ä¸ç¡®å®šæ€§æ”¯æŒé€‰æ‹©æ€§å¼ƒæƒå’Œå—æ§å‡çº§ã€‚

**Conclusion:** ç ”ç©¶ç»“æœæ”¯æŒè¾¹ç¼˜ä¼˜å…ˆè®¾è®¡ç†å¿µï¼Œå°†è¯­ä¹‰ç†è§£ã€éšç§ä¿æŠ¤å’Œå¯é¢„æµ‹å»¶è¿Ÿè§†ä¸ºå¹²æ‰°ç¯å¢ƒä¸­å¤§å‹æ¨¡å‹éƒ¨ç½²çš„åŒç­‰é‡è¦ç›®æ ‡ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ™ºèƒ½æ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†å¯è¡Œçš„æ¶æ„æ–¹æ¡ˆã€‚

---

#### ğŸ“„ Abstract
We present CoSense-LLM, an edge-first framework that turns continuous
multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and
lightweight vision) into compact, verifiable semantic tokens and coordinates
with large language models under explicit latency, energy, bandwidth, and
privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight
encoder that aligns sensor embeddings with language and compresses them into
short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer
that grounds generation in site specific policies and notes; (iii)
PromptRouter, a cost and uncertainty aware policy that selects edge only
generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure
Execution, an auditable redaction path that enforces data minimization so raw
waveforms never leave the device. The system works with modern serving
optimizations, including paged or streaming KV caches, FlashAttention style
kernels, speculative decoding, and quantized LoRA adapters, and supports on
device personalization and federated updates under non IID drift. Across home,
office, and clinic deployments, CoSense-LLM delivers grounded explanations
while meeting tight service level objectives: it sustains sub second (p95) end
to end latency on edge dominant paths, reduces inter tier token and bandwidth
costs by preferring local retrieval grounded responses, and preserves privacy
by transmitting only discrete codes and redacted metadata. Ablations show that
Edge-RAG improves factual consistency and reduces contradictions, calibrated
uncertainty enables selective abstention and controlled escalations, and KV
plus decoding accelerators lower energy per decision. The results support an
edge first design that treats semantics, privacy, and predictable latency as co
equal goals for large model deployments in interference prone environments.


### [50] [Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings](https://arxiv.org/abs/2510.19694)
*Cesar Gonzalez-Gutierrez, Dirk Hovy*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶é€šè¿‡å®è¯åˆ†ææ­ç¤ºäº†æç¤ºå·¥ç¨‹ä¸è¯­è¨€æ¨¡å‹å†…éƒ¨è¡¨å¾è´¨é‡ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼ŒæŒ‘æˆ˜äº†ç›¸å…³æç¤ºå¿…ç„¶äº§ç”Ÿæ›´å¥½è¡¨å¾çš„å¸¸è§å‡è®¾ï¼Œä¸ºç†è§£é›¶æ ·æœ¬å­¦ä¹ æœºåˆ¶æä¾›äº†æ–°è§†è§’ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¯¹äºè¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹æ— éœ€ä»»åŠ¡ç‰¹å®šç›‘ç£å³å¯æ‰§è¡Œå¤šæ ·åŒ–ä»»åŠ¡çš„å†…åœ¨æœºåˆ¶ç†è§£ä¸è¶³ï¼Œç ”ç©¶æç¤ºä¸å†…éƒ¨è¡¨å¾è´¨é‡çš„å…³ç³»æœ‰åŠ©äºæ­ç¤ºé¢„è®­ç»ƒåµŒå…¥å¦‚ä½•æ”¯æŒä¸Šä¸‹æ–‡ä»»åŠ¡è§£å†³ã€‚

**Method:** æœ¬ç ”ç©¶é‡‡ç”¨æ¢æµ‹å®éªŒæ–¹æ³•ï¼Œå¯¹æç¤ºåµŒå…¥è¿›è¡Œç³»ç»Ÿæ€§åˆ†æï¼Œè€ƒå¯Ÿäº†é›¶æ ·æœ¬åˆ†ç±»ä¸­ä¸åŒæç¤ºæ¨¡æ¿ç»„åˆå¯¹è¡¨å¾è´¨é‡çš„å½±å“ï¼Œå¹¶æ·±å…¥æ¢ç©¶äº†å¯èƒ½å¯¼è‡´æ„å¤–è¡Œä¸ºçš„æ½œåœ¨å› ç´ ã€‚

**Result:** å®éªŒå‘ç°æç¤ºç¡®å®å½±å“è¡¨å¾è´¨é‡ï¼Œä½†è¿™äº›å˜åŒ–ä¸æç¤ºå¯¹ç›®æ ‡ä»»åŠ¡çš„ç›¸å…³æ€§å¹¶ä¸ä¸€è‡´ç›¸å…³ï¼Œè¿™ä¸€ç»“æœæŒ‘æˆ˜äº†æ›´ç›¸å…³æç¤ºå¿…ç„¶å¯¼è‡´æ›´å¥½è¡¨å¾çš„æ™®éå‡è®¾ã€‚

**Conclusion:** ç ”ç©¶æ­ç¤ºäº†æç¤ºå·¥ç¨‹ä¸è¡¨å¾è´¨é‡ä¹‹é—´çš„éç›´è§‚å…³ç³»ï¼Œå¼ºè°ƒäº†éœ€è¦æ›´æ·±å…¥ç†è§£æç¤ºå¦‚ä½•å½±å“æ¨¡å‹å†…éƒ¨æœºåˆ¶ï¼Œä¸ºé›¶æ ·æœ¬å­¦ä¹ çš„åŸºç¡€ç†è®ºæä¾›äº†é‡è¦å®è¯ä¾æ®ã€‚

---

#### ğŸ“„ Abstract
Prompting is a common approach for leveraging LMs in zero-shot settings.
However, the underlying mechanisms that enable LMs to perform diverse tasks
without task-specific supervision remain poorly understood. Studying the
relationship between prompting and the quality of internal representations can
shed light on how pre-trained embeddings may support in-context task solving.
In this empirical study, we conduct a series of probing experiments on prompt
embeddings, analyzing various combinations of prompt templates for zero-shot
classification. Our findings show that while prompting affects the quality of
representations, these changes do not consistently correlate with the relevance
of the prompts to the target task. This result challenges the assumption that
more relevant prompts necessarily lead to better representations. We further
analyze potential factors that may contribute to this unexpected behavior.


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS](https://arxiv.org/abs/2510.19055)
*Brandon James Carone, Iran R. Roman, Pablo RipollÃ©s*

#### ğŸ§© TL;DR
æœ¬ç ”ç©¶æå‡ºäº†MUSEåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨éŸ³ä¹ç†è§£ä¸­çš„å…³ç³»æ¨ç†èƒ½åŠ›ï¼Œå‘ç°å½“å‰SOTAæ¨¡å‹å­˜åœ¨æ˜¾è‘—æ„ŸçŸ¥ç¼ºé™·ä¸”ä¸äººç±»ä¸“å®¶å­˜åœ¨æŒç»­å·®è·ã€‚

---

#### ğŸ“˜ Detailed Summary
**Motivation:** å½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘ç†è§£æ–¹é¢å·²å±•ç¤ºèƒ½åŠ›ï¼Œä½†ç°æœ‰è¯„ä¼°å¯èƒ½æ©ç›–äº†å…¶åœ¨å…³ç³»æ¨ç†æ–¹é¢çš„æ ¹æœ¬å¼±ç‚¹ï¼Œéœ€è¦æ›´ç³»ç»Ÿçš„è¯„ä¼°å·¥å…·æ¥æ­ç¤ºè¿™äº›åŸºæœ¬ç¼ºé™·ã€‚

**Method:** ç ”ç©¶å¼•å…¥äº†éŸ³ä¹ç†è§£ä¸ç»“æ„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«10ä¸ªä»»åŠ¡æ¥æ¢æµ‹åŸºç¡€éŸ³ä¹æ„ŸçŸ¥æŠ€èƒ½ï¼Œè¯„ä¼°äº†å››ç§SOTAæ¨¡å‹å¹¶ä¸å¤§è§„æ¨¡äººç±»åŸºçº¿è¿›è¡Œæ¯”è¾ƒï¼ŒåŒæ—¶æµ‹è¯•äº†æ€ç»´é“¾æç¤ºçš„æœ‰æ•ˆæ€§ã€‚

**Result:** ç»“æœæ˜¾ç¤ºSOTAæ¨¡å‹èƒ½åŠ›å·®å¼‚å·¨å¤§ä¸”ä¸äººç±»ä¸“å®¶å­˜åœ¨æŒç»­å·®è·ï¼ŒGemini Proåœ¨åŸºç¡€æ„ŸçŸ¥ä¸Šè¡¨ç°è‰¯å¥½ä½†Qwenå’ŒAudio Flamingo 3æ¥è¿‘éšæœºæ°´å¹³ï¼Œæ€ç»´é“¾æç¤ºäº§ç”Ÿä¸ä¸€è‡´ä¸”é€šå¸¸æœ‰å®³çš„ç»“æœã€‚

**Conclusion:** è¯¥ç ”ç©¶ä¸ºè¯„ä¼°ä¸å˜éŸ³ä¹è¡¨ç¤ºæä¾›äº†å…³é”®å·¥å…·ï¼Œæ­ç¤ºäº†å½“å‰AIç³»ç»Ÿåœ¨éŸ³ä¹ç†è§£æ–¹é¢çš„æ ¹æœ¬å±€é™æ€§ï¼Œå°†æ¨åŠ¨å¼€å‘æ›´é²æ£’çš„AIç³»ç»Ÿã€‚

---

#### ğŸ“„ Abstract
Multimodal Large Language Models (MLLMs) have demonstrated capabilities in
audio understanding, but current evaluations may obscure fundamental weaknesses
in relational reasoning. We introduce the Music Understanding and Structural
Evaluation (MUSE) Benchmark, an open-source resource with 10 tasks designed to
probe fundamental music perception skills. We evaluate four SOTA models (Gemini
Pro and Flash, Qwen2.5-Omni, and Audio-Flamingo 3) against a large human
baseline (N=200). Our results reveal a wide variance in SOTA capabilities and a
persistent gap with human experts. While Gemini Pro succeeds on basic
perception, Qwen and Audio Flamingo 3 perform at or near chance, exposing
severe perceptual deficits. Furthermore, we find Chain-of-Thought (CoT)
prompting provides inconsistent, often detrimental results. Our work provides a
critical tool for evaluating invariant musical representations and driving
development of more robust AI systems.
