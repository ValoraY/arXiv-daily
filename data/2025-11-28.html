<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>2025-11-28.md</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    body {
      background-color: #fafafa;
      font-family: 'Inter', sans-serif;
      padding: 2rem;
    }
    .markdown-body {
      max-width: 900px;
      margin: auto;
      background: white;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
      border-bottom: 1px solid #eaecef;
      padding-bottom: 0.3em;
    }
  </style>
</head>
<body>
  <article class="markdown-body">
    <div id=toc></div>

<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 6]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 2]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cscv-back">cs.CV <a href="#toc">[Back]</a></h1>
<h3 id="1-g2vlm-geometry-grounded-vision-language-model-with-unified-3d-reconstruction-and-spatial-reasoning">[1] <a href="https://arxiv.org/abs/2511.21688">G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning</a></h3>
<p><em>Wenbo Hu, Jingli Lin, Yilin Long, Yunlong Ran, Lihan Jiang, Yifan Wang, Chenming Zhu, Runsen Xu, Tai Wang, Jiangmiao Pang</em></p>
<h4 id="tldr">ğŸ§© TL;DR</h4>
<p>GÂ²VLMæ˜¯ä¸€ä¸ªå‡ ä½•åŸºç¡€çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€3Dç©ºé—´é‡å»ºå’Œç©ºé—´ç†è§£ä¸¤ä¸ªåŸºæœ¬æ–¹é¢ï¼Œè§£å†³äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ™ºèƒ½æ–¹é¢çš„ä¸è¶³ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å­¦ä¹ çš„3Dè§†è§‰å‡ ä½•ç‰¹å¾ç›´æ¥é¢„æµ‹3Då±æ€§ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å’Œäº¤é”™æ¨ç†å¢å¼ºç©ºé—´æ¨ç†ä»»åŠ¡ã€‚</p>
<hr />
<h4 id="detailed-summary">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ™ºèƒ½æ–¹é¢ç¼ºä¹é²æ£’æ€§ï¼Œåœ¨ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸»è¦å½’å› äºç¼ºä¹èƒ½å¤Ÿä»2Då›¾åƒé‡å»º3Dç©ºé—´çš„è§†è§‰å‡ ä½•å­¦ä¹ è¿‡ç¨‹ã€‚</p>
<p><strong>Method:</strong> GÂ²VLMé‡‡ç”¨ç»Ÿä¸€è®¾è®¡ï¼ŒåŸç”Ÿåˆ©ç”¨å­¦ä¹ çš„3Dè§†è§‰å‡ ä½•ç‰¹å¾ç›´æ¥é¢„æµ‹3Då±æ€§ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å’Œäº¤é”™æ¨ç†å¢å¼ºç©ºé—´æ¨ç†ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åœ¨ä¸°å¯Œçš„å¤šè§†è§’å›¾åƒå’Œè§†é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶åˆ©ç”¨é€šå¸¸ä»…ä»éš¾ä»¥æ”¶é›†çš„æ ‡æ³¨ä¸­è·å¾—çš„3Dè§†è§‰å…ˆéªŒã€‚</p>
<p><strong>Result:</strong> å®éªŒç»“æœè¡¨æ˜GÂ²VLMåœ¨ä¸¤é¡¹ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œåœ¨3Dé‡å»ºä»»åŠ¡ä¸Šè¾¾åˆ°ä¸æœ€å…ˆè¿›å‰é¦ˆæ¨¡å‹ç›¸å½“çš„ç»“æœï¼Œåœ¨ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸Šå–å¾—æ›´å¥½æˆ–å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
<p><strong>Conclusion:</strong> é€šè¿‡å°†è¯­ä¹‰å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸ä½å±‚æ¬¡3Dè§†è§‰ä»»åŠ¡ç»Ÿä¸€ï¼ŒGÂ²VLMæœ‰æœ›æˆä¸ºç¤¾åŒºçš„å¼ºå¤§åŸºå‡†ï¼Œå¹¶è§£é”æ›´å¤šæœªæ¥åº”ç”¨ï¼Œå¦‚3Dåœºæ™¯ç¼–è¾‘ã€‚è¯¥ç ”ç©¶ä¸ºç©ºé—´æ™ºèƒ½çš„å‘å±•æä¾›äº†é‡è¦åŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract">ğŸ“„ Abstract</h4>
<p>Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.</p>
<h3 id="2-qwen3-vl-technical-report">[2] <a href="https://arxiv.org/abs/2511.21631">Qwen3-VL Technical Report</a></h3>
<p><em>Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, Ke Zhu</em></p>
<h4 id="tldr_1">ğŸ§© TL;DR</h4>
<p>Qwen3-VLæ˜¯Qwenç³»åˆ—ä¸­æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œæ”¯æŒé«˜è¾¾256K tokençš„äº¤é”™ä¸Šä¸‹æ–‡ï¼Œå¹¶æä¾›å¯†é›†å’ŒMoEæ¶æ„å˜ä½“ä»¥æ»¡è¶³ä¸åŒå»¶è¿Ÿ-è´¨é‡æƒè¡¡éœ€æ±‚ã€‚</p>
<hr />
<h4 id="detailed-summary_1">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çº¯æ–‡æœ¬ç†è§£èƒ½åŠ›ã€é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ä»¥åŠè·¨å›¾åƒå’Œè§†é¢‘çš„é«˜çº§å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿æ–‡æ¡£ã€è§†é¢‘å’Œå¤æ‚å¤šæ¨¡æ€ä»»åŠ¡æ—¶çš„æ€§èƒ½ä¸è¶³é—®é¢˜ã€‚</p>
<p><strong>Method:</strong> è®ºæ–‡æå‡ºäº†ä¸‰ä¸ªå…³é”®æ¶æ„å‡çº§ï¼šå¢å¼ºçš„äº¤é”™MRoPEç”¨äºæ”¹è¿›å›¾åƒå’Œè§†é¢‘çš„æ—¶ç©ºå»ºæ¨¡ï¼›DeepStacké›†æˆæœ‰æ•ˆåˆ©ç”¨å¤šçº§ViTç‰¹å¾ä»¥åŠ å¼ºè§†è§‰-è¯­è¨€å¯¹é½ï¼›åŸºäºæ–‡æœ¬çš„æ—¶é—´å¯¹é½æœºåˆ¶ä»T-RoPEæ¼”è¿›ä¸ºæ˜¾å¼æ—¶é—´æˆ³å¯¹é½ï¼Œå®ç°æ›´ç²¾ç¡®çš„æ—¶é—´å®šä½ã€‚</p>
<p><strong>Result:</strong> Qwen3-VLåœ¨MMMUã€MathVistaå’ŒMathVisionç­‰ç»¼åˆè¯„ä¼°ä¸­è¡¨ç°å‡ºé¢†å…ˆæ€§èƒ½ï¼Œåœ¨å¯æ¯”è¾ƒçš„tokené¢„ç®—å’Œå»¶è¿Ÿçº¦æŸä¸‹ï¼Œæ— è®ºæ˜¯å¯†é›†æ¶æ„è¿˜æ˜¯MoEæ¶æ„å‡å®ç°äº†ä¼˜è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†åŒç±»çº¯æ–‡æœ¬éª¨å¹²æ¨¡å‹ã€‚</p>
<p><strong>Conclusion:</strong> Qwen3-VLå¯ä½œä¸ºå›¾åƒåŸºç¡€æ¨ç†ã€æ™ºèƒ½ä½“å†³ç­–å’Œç°å®å·¥ä½œæµç¨‹ä¸­å¤šæ¨¡æ€ä»£ç æ™ºèƒ½çš„åŸºç¡€å¼•æ“ï¼Œå…¶å¼ºå¤§çš„é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ä¸ºå®é™…åº”ç”¨æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æ’‘ã€‚</p>
<hr />
<h4 id="abstract_1">ğŸ“„ Abstract</h4>
<p>We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.</p>
<h3 id="3-attention-guided-patch-wise-sparse-adversarial-attacks-on-vision-language-action-models">[3] <a href="https://arxiv.org/abs/2511.21663">Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models</a></h3>
<p><em>Naifu Zhang, Wei Tao, Xi Xiao, Qianpu Sun, Yuxin Zheng, Wentao Mo, Peiqiang Wang, Nan Zhang</em></p>
<h4 id="tldr_2">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºADVLAæ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥åœ¨è§†è§‰ç¼–ç å™¨æŠ•å½±åˆ°æ–‡æœ¬ç‰¹å¾ç©ºé—´çš„ç‰¹å¾ä¸Šåº”ç”¨å¯¹æŠ—æ‰°åŠ¨ï¼Œæœ‰æ•ˆç ´åVLAæ¨¡å‹çš„ä¸‹æ¸¸åŠ¨ä½œé¢„æµ‹ã€‚è¯¥æ–¹æ³•åœ¨ä½å¹…åº¦çº¦æŸä¸‹å®ç°é«˜æ•ˆæ”»å‡»ï¼Œæ³¨æ„åŠ›å¼•å¯¼ä½¿æ‰°åŠ¨æ—¢é›†ä¸­åˆç¨€ç–ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºäºè¡¥ä¸çš„æ”»å‡»æ–¹æ³•ã€‚</p>
<hr />
<h4 id="detailed-summary_2">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰VLAæ¨¡å‹çš„å¯¹æŠ—æ”»å‡»æ–¹æ³•éœ€è¦æ˜‚è´µçš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œä¸”é€šå¸¸ç”Ÿæˆæ˜æ˜¾çš„æ‰°åŠ¨è¡¥ä¸ï¼Œè¿™é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œéœ€è¦å¼€å‘æ›´é«˜æ•ˆä¸”ä¸æ˜“å¯Ÿè§‰çš„æ”»å‡»æ–¹æ³•ã€‚</p>
<p><strong>Method:</strong> ADVLAæ¡†æ¶ç›´æ¥åœ¨è§†è§‰ç¼–ç å™¨æŠ•å½±åˆ°æ–‡æœ¬ç‰¹å¾ç©ºé—´çš„ç‰¹å¾ä¸Šåº”ç”¨å¯¹æŠ—æ‰°åŠ¨ï¼Œå¼•å…¥ä¸‰ç§ç­–ç•¥å¢å¼ºæ•æ„Ÿæ€§ã€å¼ºåˆ¶ç¨€ç–æ€§å’Œé›†ä¸­æ‰°åŠ¨ã€‚æ³¨æ„åŠ›å¼•å¯¼æœºåˆ¶ä½¿æ‰°åŠ¨èƒ½å¤Ÿèšç„¦äºå…³é”®åŒºåŸŸå¹¶ä¿æŒç¨€ç–æ€§ã€‚</p>
<p><strong>Result:</strong> åœ¨Lâˆ=4/255çº¦æŸä¸‹ï¼ŒADVLAç»“åˆTop-Kæ©ç ä»…ä¿®æ”¹ä¸åˆ°10%çš„è¡¥ä¸å³å¯å®ç°æ¥è¿‘100%çš„æ”»å‡»æˆåŠŸç‡ã€‚æ‰°åŠ¨é›†ä¸­äºå…³é”®åŒºåŸŸï¼Œåœ¨æ•´ä½“å›¾åƒä¸­å‡ ä¹ä¸å¯å¯Ÿè§‰ï¼Œå•æ­¥è¿­ä»£ä»…éœ€çº¦0.06ç§’ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿè¡¥ä¸æ”»å‡»æ–¹æ³•ã€‚</p>
<p><strong>Conclusion:</strong> ADVLAåœ¨ä½å¹…åº¦å’Œå±€éƒ¨ç¨€ç–æ¡ä»¶ä¸‹æœ‰æ•ˆå‰Šå¼±VLAæ¨¡å‹çš„ä¸‹æ¸¸åŠ¨ä½œé¢„æµ‹ï¼Œé¿å…äº†ä¼ ç»Ÿè¡¥ä¸æ”»å‡»çš„é«˜è®­ç»ƒæˆæœ¬å’Œæ˜æ˜¾æ‰°åŠ¨ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†æ”»å‡»VLAç‰¹å¾ç©ºé—´çš„ç‹¬ç‰¹æœ‰æ•ˆæ€§å’Œå®ç”¨ä»·å€¼ï¼Œä¸ºVLAæ¨¡å‹çš„å®‰å…¨æ€§è¯„ä¼°æä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_2">ğŸ“„ Abstract</h4>
<p>In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.</p>
<h3 id="4-multi-crit-benchmarking-multimodal-judges-on-pluralistic-criteria-following">[4] <a href="https://arxiv.org/abs/2511.21662">Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</a></h3>
<p><em>Tianyi Xiong, Yi Ge, Ming Li, Zuolong Zhang, Pranav Kulkarni, Kaishen Wang, Qi He, Zeying Zhu, Chenxi Liu, Ruibo Chen, Tong Zheng, Yanshuo Chen, Xiyao Wang, Renrui Zhang, Wenhu Chen, Heng Huang</em></p>
<h4 id="tldr_3">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶æå‡ºäº†Multi-CritåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨éµå¾ªå¤šæ ·åŒ–ç»†ç²’åº¦è¯„ä¼°æ ‡å‡†æ–¹é¢çš„èƒ½åŠ›ï¼Œé€šè¿‡å¯¹25ä¸ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„ç»¼åˆåˆ†ææ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤šå…ƒæ ‡å‡†éµå¾ªæ–¹é¢çš„å±€é™æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_3">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å½“å‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä½œä¸ºè¯„ä¼°è€…åœ¨å¤šæ¨¡æ€è¯„ä¼°ç³»ç»Ÿä¸­æ—¥ç›Šæ™®åŠï¼Œä½†å…¶éµå¾ªå¤šæ ·åŒ–ç»†ç²’åº¦è¯„ä¼°æ ‡å‡†çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šå…ƒæ ‡å‡†åˆ¤æ–­å’Œè¯†åˆ«æ ‡å‡†é—´åå¥½å†²çªæ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶å¼€å‘äº†Multi-CritåŸºå‡†ï¼Œé€šè¿‡ä¸¥æ ¼çš„æ•°æ®ç­›é€‰æµç¨‹æ”¶é›†å…·æœ‰å¤šæ ‡å‡†äººå·¥æ ‡æ³¨çš„æŒ‘æˆ˜æ€§å“åº”å¯¹ï¼Œæ¶µç›–å¼€æ”¾å¼ç”Ÿæˆå’Œå¯éªŒè¯æ¨ç†ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†ä¸‰ä¸ªæ–°é¢–æŒ‡æ ‡æ¥ç³»ç»Ÿè¯„ä¼°å¤šå…ƒæ ‡å‡†éµå¾ªã€æ ‡å‡†åˆ‡æ¢çµæ´»æ€§å’Œè¯†åˆ«æ ‡å‡†çº§åå¥½å†²çªçš„èƒ½åŠ›ã€‚</p>
<p><strong>Result:</strong> å¯¹25ä¸ªå¤šæ¨¡æ€æ¨¡å‹çš„ç»¼åˆåˆ†æè¡¨æ˜ï¼Œä¸“æœ‰æ¨¡å‹åœ¨ä¿æŒå¯¹å¤šå…ƒæ ‡å‡†çš„ä¸€è‡´æ€§éµå¾ªæ–¹é¢ä»ç„¶å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾å¼è¯„ä¼°ä¸­ï¼›å¼€æºæ¨¡å‹åœ¨çµæ´»éµå¾ªå¤šæ ·åŒ–æ ‡å‡†æ–¹é¢è¿›ä¸€æ­¥è½åï¼›åŸºäºæ•´ä½“åˆ¤æ–­ä¿¡å·çš„æ‰¹è¯„å¾®è°ƒè™½ç„¶å¢å¼ºäº†è§†è§‰åŸºç¡€èƒ½åŠ›ï¼Œä½†æ— æ³•æ³›åŒ–åˆ°å¤šå…ƒæ ‡å‡†çº§åˆ¤æ–­ã€‚</p>
<p><strong>Conclusion:</strong> Multi-Critä¸ºæ„å»ºå¯é ä¸”å¯æ§çš„å¤šæ¨¡æ€AIè¯„ä¼°å¥ å®šäº†åŸºç¡€ï¼Œæ­ç¤ºäº†å½“å‰å¤šæ¨¡æ€è¯„ä¼°è€…åœ¨å¤šå…ƒæ ‡å‡†éµå¾ªæ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶é€šè¿‡å¯¹æ¨ç†å¾®è°ƒã€æµ‹è¯•æ—¶æ‰©å±•ä»¥åŠå¼€æºä¸ä¸“æœ‰æ¨¡å‹é—´è¾¹ç•Œä¸€è‡´æ€§çš„é¢å¤–åˆ†æè¿›ä¸€æ­¥æ¢ç´¢äº†ç°æœ‰æŠ€æœ¯çš„è¾¹ç•Œã€‚</p>
<hr />
<h4 id="abstract_3">ğŸ“„ Abstract</h4>
<p>Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.</p>
<h3 id="5-seeing-without-pixels-perception-from-camera-trajectories">[5] <a href="https://arxiv.org/abs/2511.21681">Seeing without Pixels: Perception from Camera Trajectories</a></h3>
<p><em>Zihui Xue, Kristen Grauman, Dima Damen, Andrew Zisserman, Tengda Han</em></p>
<h4 id="tldr_4">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†ä»…é€šè¿‡ç›¸æœºè½¨è¿¹æ„ŸçŸ¥è§†é¢‘å†…å®¹çš„å¯è¡Œæ€§ï¼Œæå‡ºäº†CamFormerå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œè¯æ˜ç›¸æœºè½¨è¿¹æ˜¯æ­ç¤ºè§†é¢‘å†…å®¹çš„å¼ºæœ‰åŠ›ä¿¡å·ï¼Œå…¶è¡¨ç¤ºåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„é²æ£’æ€§å’Œé€šç”¨æ€§ã€‚</p>
<hr />
<h4 id="detailed-summary_4">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢ä¸€ä¸ªçœ‹ä¼¼ä¸å¯èƒ½çš„é—®é¢˜ï¼šèƒ½å¦ä»…é€šè¿‡ç›¸æœºè½¨è¿¹ï¼ˆç›¸æœºåœ¨ç©ºé—´ä¸­ç§»åŠ¨çš„è·¯å¾„ï¼‰æ¥æ„ŸçŸ¥è§†é¢‘å†…å®¹ï¼Œè€Œæ— éœ€è§‚å¯Ÿåƒç´ ä¿¡æ¯ã€‚è¯¥ç ”ç©¶å¡«è¡¥äº†ç°æœ‰è§†é¢‘ç†è§£æ–¹æ³•ä¸»è¦ä¾èµ–è§†è§‰å†…å®¹è€Œå¿½è§†ç›¸æœºè¿åŠ¨æ¨¡å¼è¿™ä¸€é‡è¦ä¿¡å·çš„ç©ºç™½ã€‚</p>
<p><strong>Method:</strong> æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¯¹æ¯”å­¦ä¹ æ¡†æ¶æ¥è®­ç»ƒCamFormerç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å°†ç›¸æœºä½å§¿è½¨è¿¹æŠ•å½±åˆ°è”åˆåµŒå…¥ç©ºé—´ä¸­ï¼Œä½¿å…¶ä¸è‡ªç„¶è¯­è¨€å¯¹é½ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†å¤šç§ç›¸æœºä½å§¿ä¼°è®¡æ–¹æ³•ï¼ŒåŒ…æ‹¬é«˜ç²¾åº¦å¤šä¼ æ„Ÿå™¨å’Œæ ‡å‡†RGB-onlyä¼°è®¡å™¨ã€‚</p>
<p><strong>Result:</strong> å®éªŒè¡¨æ˜ç›¸æœºè½¨è¿¹æ˜¯æ­ç¤ºè§†é¢‘å†…å®¹çš„å¼ºæœ‰åŠ›ä¿¡å·ï¼ŒCamFormeråµŒå…¥åœ¨è·¨æ¨¡æ€å¯¹é½ã€åˆ†ç±»å’Œæ—¶é—´åˆ†æç­‰å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥è¡¨ç¤ºå¯¹ä¸åŒç›¸æœºä½å§¿ä¼°è®¡æ–¹æ³•å…·æœ‰é²æ£’æ€§ï¼ŒéªŒè¯äº†å…¶é€šç”¨æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Conclusion:</strong> æœ¬ç ”ç©¶ç¡®ç«‹äº†ç›¸æœºè½¨è¿¹ä½œä¸ºä¸€ç§è½»é‡çº§ã€é²æ£’ä¸”é€šç”¨çš„è§†é¢‘å†…å®¹æ„ŸçŸ¥æ¨¡æ€ï¼Œæ­ç¤ºäº†ç›¸æœºè¿åŠ¨æ¨¡å¼ä¸è§†é¢‘è¯­ä¹‰å†…å®¹ä¹‹é—´çš„æ·±åˆ»è”ç³»ï¼Œä¸ºè§†é¢‘ç†è§£æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•è®ºåŸºç¡€ã€‚</p>
<hr />
<h4 id="abstract_4">ğŸ“„ Abstract</h4>
<p>Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.</p>
<h3 id="6-canvas-to-image-compositional-image-generation-with-multimodal-controls">[6] <a href="https://arxiv.org/abs/2511.21691">Canvas-to-Image: Compositional Image Generation with Multimodal Controls</a></h3>
<p><em>Yusuf Dalva, Guocheng Gordon Qian, Maya Goldenberg, Tsai-Shien Chen, Kfir Aberman, Sergey Tulyakov, Pinar Yanardag, Kuan-Chieh Jackson Wang</em></p>
<h4 id="tldr_5">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºCanvas-to-Imageæ¡†æ¶ï¼Œé€šè¿‡å°†å¤šç§å¼‚æ„æ§åˆ¶ä¿¡å·ç¼–ç ä¸ºå•ä¸€ç”»å¸ƒå›¾åƒï¼Œå®ç°äº†å¯¹æ–‡æœ¬æç¤ºã€ä¸»ä½“å‚è€ƒã€ç©ºé—´å¸ƒå±€å’Œå§¿æ€çº¦æŸçš„ç»Ÿä¸€å¤šæ¨¡æ€æ§åˆ¶ï¼Œæ˜¾è‘—æå‡äº†æ‰©æ•£æ¨¡å‹åœ¨ç»„åˆç”Ÿæˆä»»åŠ¡ä¸­çš„å¿ å®åº¦å’Œæ§åˆ¶ç²¾åº¦ã€‚</p>
<hr />
<h4 id="detailed-summary_5">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°ä»£æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å¤šæ ·åŒ–å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŒæ—¶å¤„ç†æ–‡æœ¬æç¤ºã€ä¸»ä½“å‚è€ƒã€ç©ºé—´æ’åˆ—ã€å§¿æ€çº¦æŸå’Œå¸ƒå±€æ³¨é‡Šç­‰å¤šæ¨¡æ€æ§åˆ¶æ—¶ï¼Œä»éš¾ä»¥å®ç°é«˜ä¿çœŸåº¦çš„ç»„åˆæ§åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”¨æˆ·éœ€è¦åŒæ—¶æŒ‡å®šå¤šç§æ§åˆ¶ä¿¡å·çš„æƒ…å†µä¸‹å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚</p>
<p><strong>Method:</strong> æ ¸å¿ƒæ–¹æ³•æ˜¯å°†å¤šæ ·æ§åˆ¶ä¿¡å·ç¼–ç ä¸ºå•ä¸€å¤åˆç”»å¸ƒå›¾åƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç›´æ¥è¿›è¡Œé›†æˆè§†è§‰ç©ºé—´æ¨ç†ï¼›åŒæ—¶æ„å»ºå¤šä»»åŠ¡æ•°æ®é›†å¹¶æå‡ºå¤šä»»åŠ¡ç”»å¸ƒè®­ç»ƒç­–ç•¥ï¼Œåœ¨ç»Ÿä¸€å­¦ä¹ èŒƒå¼ä¸‹ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ä»¥è”åˆç†è§£å’Œæ•´åˆå¼‚æ„æ§åˆ¶åˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ã€‚</p>
<p><strong>Result:</strong> å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCanvas-to-Imageåœ¨èº«ä»½ä¿æŒå’Œæ§åˆ¶éµå¾ªæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æœ€ä¼˜æ–¹æ³•ï¼Œåœ¨å¤šäººç‰©ç»„åˆã€å§¿æ€æ§åˆ¶ç»„åˆã€å¸ƒå±€çº¦æŸç”Ÿæˆå’Œå¤šæ§åˆ¶ç”Ÿæˆç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Conclusion:</strong> è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç»Ÿä¸€å­¦ä¹ èŒƒå¼å®ç°è·¨å¤šæ§åˆ¶æ¨¡æ€æ¨ç†çš„æœ‰æ•ˆæ€§ï¼Œç›¸æ¯”ä¾èµ–ä»»åŠ¡ç‰¹å®šå¯å‘å¼æ–¹æ³•å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€å¯æ§å›¾åƒç”Ÿæˆæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„å’Œç†è®ºæ´è§ã€‚</p>
<hr />
<h4 id="abstract_5">ğŸ“„ Abstract</h4>
<p>While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.</p>
<div id='cs.AI'></div>

<h1 id="csai-back">cs.AI <a href="#toc">[Back]</a></h1>
<h3 id="7-on-the-limits-of-innate-planning-in-large-language-models">[7] <a href="https://arxiv.org/abs/2511.21591">On the Limits of Innate Planning in Large Language Models</a></h3>
<p><em>Charles Schepanowski, Charles Ling</em></p>
<h4 id="tldr_6">ğŸ§© TL;DR</h4>
<p>æœ¬ç ”ç©¶é€šè¿‡8æ‹¼å›¾ä»»åŠ¡è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„è§„åˆ’å’ŒçŠ¶æ€æ¨ç†èƒ½åŠ›ï¼Œå‘ç°å³ä½¿æä¾›å¤–éƒ¨ç§»åŠ¨éªŒè¯å™¨ï¼Œæ‰€æœ‰æ¨¡å‹ä»æ— æ³•è§£å†³ä»»ä½•è°œé¢˜ï¼Œæ­ç¤ºäº†å½“å‰LLMsåœ¨çŠ¶æ€ç»´æŠ¤å’Œå¯å‘å¼è§„åˆ’æ–¹é¢å­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·ã€‚</p>
<hr />
<h4 id="detailed-summary_6">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¼—å¤šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å…¶è§„åˆ’å’ŒçŠ¶æ€æ¨ç†èƒ½åŠ›ä»ä¸æ˜ç¡®ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨ç›´æ¥è¯„ä¼°è¿™äº›æ ¸å¿ƒè®¤çŸ¥èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰ä»£ç æ‰§è¡Œæˆ–å…¶ä»–å¤–éƒ¨å·¥å…·è¾…åŠ©çš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>Method:</strong> ç ”ç©¶é‡‡ç”¨ç»å…¸çš„8æ‹¼å›¾ä»»åŠ¡ï¼Œæµ‹è¯•äº†å››ç§æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€æ€ç»´é“¾å’Œç®—æ³•æ€ç»´ç­‰å¸¸è§æç¤ºæ¡ä»¶ä¸‹ï¼Œä»¥åŠåˆ†å±‚çº æ­£åé¦ˆæœºåˆ¶ä¸‹çš„è¡¨ç°ï¼Œå¹¶è¿›ä¸€æ­¥ä½¿ç”¨å¤–éƒ¨ç§»åŠ¨éªŒè¯å™¨æä¾›ä»…æœ‰æ•ˆç§»åŠ¨çš„è¾…åŠ©è®¾ç½®ã€‚</p>
<p><strong>Result:</strong> åé¦ˆæœºåˆ¶ä»…å¯¹éƒ¨åˆ†æ¨¡å‹-æç¤ºç»„åˆæœ‰æ‰€æ”¹å–„ï¼Œä½†æˆåŠŸè¿è¡Œé€šå¸¸å†—é•¿ä¸”è®¡ç®—æ˜‚è´µï¼Œå³ä½¿æä¾›å¤–éƒ¨ç§»åŠ¨éªŒè¯å™¨ï¼Œæ‰€æœ‰æ¨¡å‹ä»æ— æ³•è§£å†³ä»»ä½•è°œé¢˜ï¼Œå®šæ€§åˆ†ææ­ç¤ºäº†æ¨¡å‹æ™®éå­˜åœ¨è„†å¼±çš„çŠ¶æ€è¡¨ç¤ºå’Œè–„å¼±çš„å¯å‘å¼è§„åˆ’èƒ½åŠ›ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æ²¡æœ‰ä»£ç è§£é‡Šå™¨ç­‰å¤–éƒ¨å·¥å…·çš„æƒ…å†µä¸‹ï¼Œå½“å‰LLMsåœ¨è§„åˆ’ä»»åŠ¡ä¸­å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œæœªæ¥çš„è¿›å±•å¯èƒ½éœ€è¦å¼€å‘èƒ½å¤Ÿç»´æŠ¤æ˜¾å¼çŠ¶æ€å’Œæ‰§è¡Œç»“æ„åŒ–æœç´¢çš„æœºåˆ¶ã€‚</p>
<hr />
<h4 id="abstract_6">ğŸ“„ Abstract</h4>
<p>Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.</p>
<h3 id="8-agentic-learner-with-grow-and-refine-multimodal-semantic-memory">[8] <a href="https://arxiv.org/abs/2511.21678">Agentic Learner with Grow-and-Refine Multimodal Semantic Memory</a></h3>
<p><em>Weihao Bo, Shan Zhang, Yanpeng Sun, Jingjing Wu, Qunyi Xie, Xiao Tan, Kunbin Chen, Wei He, Xiaofan Li, Na Zhao, Jingdong Wang, Zechao Li</em></p>
<h4 id="tldr_7">ğŸ§© TL;DR</h4>
<p>æœ¬æ–‡æå‡ºäº†ViLoMemï¼Œä¸€ç§åŒæµè®°å¿†æ¡†æ¶ï¼Œé€šè¿‡åˆ†åˆ«ç¼–ç è§†è§‰åˆ†å¿ƒæ¨¡å¼å’Œé€»è¾‘æ¨ç†é”™è¯¯æ¥æ„å»ºç´§å‡‘çš„ã€åŸºäºæ¨¡å¼çš„è®°å¿†ï¼Œä½¿å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿä»æˆåŠŸå’Œå¤±è´¥çš„ç»éªŒä¸­å­¦ä¹ ï¼Œä»è€Œåœ¨å…­ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­æŒç»­æé«˜æ€§èƒ½ã€‚</p>
<hr />
<h4 id="detailed-summary_7">ğŸ“˜ Detailed Summary</h4>
<p><strong>Motivation:</strong> ç°æœ‰åŸºäºè½¨è¿¹çš„è®°å¿†æ–¹æ³•å­˜åœ¨ç®€æ´æ€§åå·®ï¼Œé€æ¸ä¸¢å¤±å…³é”®é¢†åŸŸçŸ¥è¯†ï¼Œå¹¶ä¸”åœ¨å¤šæ¨¡æ€é—®é¢˜è§£å†³ç¯å¢ƒä¸­ä»…è®°å½•å•æ¨¡æ€è¡Œä¸ºè½¨è¿¹ï¼Œæ— æ³•ä¿ç•™è§†è§‰æ³¨æ„å’Œé€»è¾‘æ¨ç†å¦‚ä½•å…±åŒä¿ƒæˆè§£å†³æ–¹æ¡ˆï¼Œè¿™ä¸äººç±»è®¤çŸ¥ä¸­è¯­ä¹‰è®°å¿†æ—¢æ˜¯å¤šæ¨¡æ€åˆæ˜¯æ•´åˆçš„æ ¹æœ¬ç‰¹å¾ä¸ç¬¦ã€‚</p>
<p><strong>Method:</strong> ViLoMemé‡‡ç”¨åŒæµè®°å¿†æ¡†æ¶ï¼Œåˆ†åˆ«ç¼–ç è§†è§‰åˆ†å¿ƒæ¨¡å¼å’Œé€»è¾‘æ¨ç†é”™è¯¯ï¼Œéµå¾ªå¢é•¿-ç²¾ç‚¼åŸåˆ™é€æ­¥ç§¯ç´¯å’Œæ›´æ–°å¤šæ¨¡æ€è¯­ä¹‰çŸ¥è¯†ï¼Œåœ¨ä¿æŒç¨³å®šã€å¯æ³›åŒ–ç­–ç•¥çš„åŒæ—¶é¿å…ç¾éš¾æ€§é—å¿˜ã€‚</p>
<p><strong>Result:</strong> åœ¨å…­ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒViLoMemæŒç»­æé«˜äº†pass@1å‡†ç¡®ç‡ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†é‡å¤çš„è§†è§‰å’Œé€»è¾‘é”™è¯¯ï¼Œæ¶ˆèå®éªŒè¯å®äº†å…·æœ‰æ˜ç¡®åˆ†å¿ƒ-å¹»è§‰åˆ†ç¦»çš„åŒæµè®°å¿†çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Conclusion:</strong> ç ”ç©¶è¯æ˜äº†é”™è¯¯æ„ŸçŸ¥å¤šæ¨¡æ€è®°å¿†å¯¹äºç»ˆèº«å’Œè·¨é¢†åŸŸä»£ç†å­¦ä¹ çš„é‡è¦æ€§ï¼ŒåŒæµè®°å¿†è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¤šæ¨¡æ€è®¤çŸ¥è¿‡ç¨‹ä¸­çš„å…³é”®é”™è¯¯æ¨¡å¼ï¼Œä¸ºæ„å»ºæ›´æ¥è¿‘äººç±»è®¤çŸ¥æœºåˆ¶çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ã€‚</p>
<hr />
<h4 id="abstract_7">ğŸ“„ Abstract</h4>
<p>MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.</p>
  </article>
</body>
</html>
